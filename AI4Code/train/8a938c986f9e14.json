{"cell_type":{"aca940d9":"code","eda409fa":"code","a1ec6ca8":"code","7e55f503":"code","69abfcd8":"code","862860d4":"code","1e724f1a":"code","acdcd5fe":"code","53197ded":"code","d675985c":"code","9cb2b811":"code","ee2498f3":"code","fcb13faf":"code","47e891d4":"code","bc1f4b14":"code","03805c11":"code","02f501a4":"code","a878011e":"code","9c44af9d":"code","e9829d79":"code","e52865ed":"code","366b5b13":"code","8da0af7a":"code","1d881730":"code","755f6107":"code","c5c261ca":"code","d5498fd5":"code","01df21c6":"code","31f5bb8d":"code","3eccc4c4":"code","d4196505":"code","aac66c2e":"code","a9d60935":"code","057652ff":"code","c5d57d06":"code","2ef8bb7b":"code","1b00c084":"code","fc98c302":"code","bcfd80dc":"code","962572d4":"code","faae823d":"code","522c9178":"code","3d46a1a7":"code","8d957e68":"code","6d270db9":"code","717b6528":"code","ce855146":"code","3da222d7":"code","36f717ff":"code","92aebf5b":"code","587f18b1":"code","b93b616e":"code","32f2830d":"code","5663af89":"code","13e202a7":"code","32d34755":"code","64414e9b":"code","5341f228":"code","447f8d5e":"code","194830d0":"code","cc080aaa":"code","60ea62cd":"code","ebc2cf35":"code","69a598b6":"markdown","9eea4c82":"markdown","dab01e44":"markdown","90d3651b":"markdown","e784a210":"markdown"},"source":{"aca940d9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","eda409fa":"from matplotlib import pyplot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n# for beeping\nimport IPython.display as ipd\n# where ar emy models\nfrom sklearn import linear_model\n\n\n\nimport numpy as np\nimport pandas as pd\nimport pickle\n\nfrom matplotlib import pyplot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import PowerTransformer\n\n\nfrom sklearn.model_selection import KFold,cross_val_score,GridSearchCV,StratifiedKFold,RandomizedSearchCV\n\nfrom sklearn import linear_model,tree,svm,model_selection\nfrom sklearn.neighbors import KNeighborsClassifier\nimport xgboost as xgb\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.metrics import roc_curve,make_scorer,confusion_matrix, precision_score, recall_score, f1_score, confusion_matrix, precision_recall_curve, accuracy_score, roc_auc_score,classification_report\n\nfrom sklearn.exceptions import NotFittedError\n\nfrom imblearn import over_sampling\nfrom imblearn.over_sampling import RandomOverSampler\n\nfrom time import time\n\nfrom IPython.lib.display import Audio\nimport warnings\nwarnings.filterwarnings(\"ignore\")","a1ec6ca8":"# Gets the name of the object(using for getting the name of the model)\ndef name_of_obj(obj):\n    for objname, oid in globals().items():\n        if oid is obj:\n            return objname\n# How it works\nget_my_name = linear_model.LogisticRegression()\nprint(name_of_obj(get_my_name))","7e55f503":"# use this for beep\nbeep = np.sin(2*np.pi*400*np.arange(10000*2)\/10000)\n# ipd.Audio(beep, rate=10000, autoplay=True)","69abfcd8":"# print in brick colour\ndef printBrick(message):\n    print('\\x1b[1;31m'+message+'\\x1b[0m')\nprintBrick('Sample Brick Color')","862860d4":"# function to print accuracy, precision, recall, F1-score\ndef print_metrics(actuals, predictions):\n    acc,pre,recall,f1 = get_metrics(actuals,predictions)\n    print(\"Accuracy: {:.5f}\".format(acc))\n    print(\"Precision: {:.5f}\".format(pre))\n    print(\"Recall: {:.5f}\".format(recall))\n    print(\"F1-score: {:.5f}\".format(f1))","1e724f1a":"# function to print accuracy, precision, recall, F1-score\ndef get_metrics(actuals, predictions):\n    acc = accuracy_score(actuals, predictions)\n    pre = precision_score(actuals, predictions)\n    recall = recall_score(actuals, predictions)\n    f1 = f1_score(actuals, predictions)\n    return (acc,pre,recall,f1)","acdcd5fe":"# Draw roc curve print auc values\ndef draw_roc(model_name,testy,pred_probs):\n    ns_probs = [0 for _ in range(len(testy))]\n    pred_probs = pred_probs[:, 1]\n    # calculate scores\n    ns_auc = roc_auc_score(testy, ns_probs)\n    pred_auc = roc_auc_score(testy, pred_probs)\n    # summarize scores\n    print('No Skill: ROC AUC=%.3f' % (ns_auc))\n    print(model_name,': ROC AUC=%.3f' % (pred_auc))\n    # calculate roc curves\n    ns_fpr, ns_tpr, _ = roc_curve(testy, ns_probs)\n    pred_fpr, pred_tpr, _ = roc_curve(testy, pred_probs)\n    # plot the roc curve for the model\n    plt.figure(figsize=(10,6))\n    plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n    plt.plot(pred_fpr, pred_tpr, marker='.', label='Predicted')\n    # axis labels\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    # show the legend\n    plt.legend()\n    # show the plot\n    #plt.show()\n    return pred_auc","53197ded":"df_results = pd.DataFrame(columns=['model','test aoc','train aoc','test f1','train f1','test accuracy','train accuracy','test precision','train precision','test recall','train recall'])\ndef train_and_test_model(model,x_tr,y_tr,x_te,y_te,cv):\n    results = cross_val_score(model, x_tr, y=y_tr, cv=cv,n_jobs=-1,verbose=0,scoring='roc_auc')\n    print(results)\n    print('-' * 100)\n    print(\"Cross Validation AUC score: %.3f%% (%.3f%%)\" % (results.mean()*100.0, results.std()*100.0))\n    print('-' * 100)\n    \"\"\"    try:\n        y_train_pred = model.predict(x_tr)\n    except NotFittedError as e:\n        print(repr(e))\n        model.fit(x_tr,y_tr)\n        y_train_pred = model.predict(x_tr)\n    \"\"\"\n    model.fit(x_tr,y_tr)\n    y_train_pred_proba = model.predict_proba(x_tr)\n    \n    y_train_pred = pd.DataFrame(y_train_pred_proba,columns=model.classes_).idxmax(axis=1)\n    \n    y_test_pred_proba = model.predict_proba(x_te)\n    y_test_pred = pd.DataFrame(y_test_pred_proba,columns=model.classes_).idxmax(axis=1)\n    print('\\x1b[1;31m'+'Train Scores:-'+'\\x1b[0m')\n    print_metrics(y_tr,y_train_pred)\n    print('-' * 100)\n    print('\\x1b[1;31m'+'Test Scores:-'+'\\x1b[0m')\n    print_metrics(y_te,y_test_pred)\n    print('-' * 100)\n    print('\\x1b[1;31m'+'Roc-Auc Curve'+'\\x1b[0m')\n    test_auc = draw_roc(name_of_obj(model),y_te,y_test_pred_proba)\n    #train_auc = draw_roc(type(model).__name__,y_tr,y_train_pred_proba)\n    train_auc = roc_auc_score(y_tr, y_train_pred_proba[:, 1])\n    test_f1 = f1_score(y_te,y_test_pred)\n    train_f1 = f1_score(y_tr,y_train_pred)\n    test_acc = accuracy_score(y_te,y_test_pred)\n    train_acc = accuracy_score(y_tr,y_train_pred)\n    test_pre = precision_score(y_te,y_test_pred)\n    train_pre = precision_score(y_tr,y_train_pred)\n    test_recall = recall_score(y_te,y_test_pred)\n    train_recall = recall_score(y_tr,y_train_pred)\n    print(len(df_results.index))\n    \n    df_results.loc[len(df_results.index)] = [name_of_obj(model), test_auc, train_auc,test_f1,train_f1,test_acc,train_acc,test_pre,train_pre,test_recall,train_recall] \n    return model","d675985c":"#from pandas.core.common import random_state\n\n\npenalty = ['l1', 'l2', 'elasticnet', None]\nC = [0.0001, 0.001, 0.01, 0.05, 0.1, 0.5, 1,10,100]\nclass_weight = ['balanced',None]\nsolver = ['newton-cg','lbfgs','sag','saga']\nmax_iter = [200]\nmulti_class = ['auto']\nl1_ratio = np.linspace(0,1,5).tolist()\n\nparam_grid = dict(\n                  penalty=penalty,\n                  C=C,\n                  class_weight=class_weight,\n                  max_iter=max_iter,\n                  multi_class=multi_class,\n                  l1_ratio=l1_ratio,\n                  solver=solver\n )\ndef run_lr_gridsearchcv(param_grid,kfold,X,y,model_name='temp'):\n  start = time()\n  grid = RandomizedSearchCV(linear_model.LogisticRegression(),\n                      param_grid,\n                      scoring='roc_auc',\n                      verbose=0,\n                      n_jobs = -1,\n                      n_iter = 50,\n                      cv=kfold,\n                      random_state=4\n  )\n  grid_result = grid.fit(X, y)\n  grid_result.best_params_\n  cv_res = pd.DataFrame(grid_result.cv_results_)\n  print(\n      \"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n      % (time() - start, len(grid_result.cv_results_[\"params\"]))\n  )\n  #print('writing to CSV file',model_name)\n  cv_res.to_csv(path_or_buf= model_name +'.csv')\n  print(grid_result.best_params_)\n  \n  #output.eval_js('new Audio(\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/0\/05\/Beep-09.ogg\").play()')\n  return grid_result.best_estimator_","9cb2b811":"df = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\ndf.head()","ee2498f3":"#observe the different feature type present in the data\ndf.info()","fcb13faf":"classes=df['Class'].value_counts()\nnormal_share=classes[0]\/df['Class'].count()*100\nfraud_share=classes[1]\/df['Class'].count()*100","47e891d4":"print('Normal percentage',normal_share)\nprint('Fraud percentage',fraud_share)","bc1f4b14":"classes","03805c11":"df.drop(columns=['Time'],inplace=True)","02f501a4":"y= df['Class']#class variable\nX=df.drop(columns=['Class'])\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X,y,test_size=0.3, random_state=4,stratify=y)\nprint(np.sum(y))\nprint(np.sum(y_train))\nprint(np.sum(y_test))","a878011e":"# kfold\nkfold = StratifiedKFold(n_splits=3, random_state=4, shuffle=True)","9c44af9d":"model_lr_plain_tuned = run_lr_gridsearchcv(param_grid,kfold,X_train,y_train,'model_lr_plain_tuned')","e9829d79":"from sklearn.preprocessing import StandardScaler\n\n# fit scaler on training data\nnorm = StandardScaler().fit(X_train)\nX_train_std = norm.transform(X_train)\nX_test_std = norm.transform(X_test)","e52865ed":"model_lr_std_tuned = run_lr_gridsearchcv(param_grid,kfold,X_train_std,y_train,'model_lr_std_tuned')","366b5b13":"from sklearn.preprocessing import MinMaxScaler\n\n# fit scaler on training data\nnorm = MinMaxScaler().fit(X_train)\nX_train_min = norm.transform(X_train)\nX_test_min = norm.transform(X_test)","8da0af7a":"model_lr_min_tuned = run_lr_gridsearchcv(param_grid,kfold,X_train_min,y_train,'model_lr_min_tuned')","1d881730":"corr = df.corr().abs().sort_values(by='Class',ascending=True)['Class']\ndf_top_corr = df[corr.index[corr>0.1]]\ndf_top_corr.head()","755f6107":"y= df_top_corr['Class']#class variable\nX=df_top_corr.drop(columns=['Class'])\nX_train_corr, X_test_corr, y_train, y_test = model_selection.train_test_split(X,y,test_size=0.3, random_state=4,stratify=y)\nprint(np.sum(y))\nprint(np.sum(y_train))\nprint(np.sum(y_test))","c5c261ca":"model_lr_corr_tuned = run_lr_gridsearchcv(param_grid,kfold,X_train_corr,y_train,'model_lr_corr_tuned')","d5498fd5":"print(model_lr_plain_tuned.get_params())\nprint(model_lr_std_tuned.get_params())\nprint(model_lr_min_tuned.get_params())\nprint(model_lr_corr_tuned.get_params())","01df21c6":"pd.DataFrame(X_train_std,columns = X_train.columns)","31f5bb8d":"from sklearn.preprocessing import PowerTransformer\nptV1 = PowerTransformer()\nptV1.fit(X_train)","3eccc4c4":"skew_df = pd.DataFrame()\nskew_df['skewness']=X_train.skew()\nskew_df['kurtosis']=X_train.kurtosis()\nskew_df.reset_index(inplace=True,drop=True)\nskew_df['transf_skewness'] = pd.DataFrame(ptV1.transform(X_train)).skew()\nskew_df['transf_kurtosis'] = pd.DataFrame(ptV1.transform(X_train)).kurtosis()\nskew_df['skew_decrease'] = abs(skew_df['skewness']) > abs(skew_df['transf_skewness'])\nskew_df","d4196505":"from sklearn.preprocessing import PowerTransformer\nptV1 = PowerTransformer()\nptV1.fit(X_train_std)","aac66c2e":"skew_df = pd.DataFrame()\nX_train_std = pd.DataFrame(X_train_std)\nskew_df['skewness']=X_train_std.skew()\nskew_df['kurtosis']=X_train_std.kurtosis()\nskew_df.reset_index(inplace=True,drop=True)\nskew_df['transf_skewness'] = pd.DataFrame(ptV1.transform(X_train_std)).skew()\nskew_df['transf_kurtosis'] = pd.DataFrame(ptV1.transform(X_train_std)).kurtosis()\nskew_df['skew_decrease'] = abs(skew_df['skewness']) > abs(skew_df['transf_skewness'])\nskew_df","a9d60935":"cols = X_train.columns","057652ff":"trans_cols = ['V1', 'V2', 'V3', 'V5', 'V7', 'V8', 'V11',\n       'V12', 'V14', 'V16', 'V17', 'V19', 'V20', 'V21',\n       'V23', 'V24', 'V25', 'V28', 'Amount']\nX_train_trans = X_train\nX_test_trans = X_test","c5d57d06":"ptV2 = PowerTransformer()\nX_train_trans[trans_cols] = ptV2.fit_transform(X_train_trans[trans_cols])\nX_test_trans[trans_cols] = ptV2.transform(X_test[trans_cols])","2ef8bb7b":"model_lr_trans_tuned = run_lr_gridsearchcv(param_grid,kfold,X_train_trans,y_train,'model_lr_trans_tuned')","1b00c084":"from sklearn.preprocessing import PowerTransformer\nptV1 = PowerTransformer()\nptV1.fit(X_train_std)","fc98c302":"skew_df = pd.DataFrame()\nX_train_std = pd.DataFrame(X_train_std)\nskew_df['skewness']=X_train_std.skew()\nskew_df['kurtosis']=X_train_std.kurtosis()\nskew_df.reset_index(inplace=True,drop=True)\nskew_df['transf_skewness'] = pd.DataFrame(ptV1.transform(X_train_std)).skew()\nskew_df['transf_kurtosis'] = pd.DataFrame(ptV1.transform(X_train_std)).kurtosis()\nskew_df['skew_decrease'] = abs(skew_df['skewness']) > abs(skew_df['transf_skewness'])\nskew_df","bcfd80dc":"trans_cols = ['V1', 'V2', 'V3', 'V5', 'V7', 'V8', 'V11',\n       'V12', 'V14', 'V16', 'V17', 'V19', 'V20', 'V21',\n       'V23', 'V24', 'V25', 'V28', 'Amount']\nX_train_std_trans = X_train_std #pd.DataFrame(X_train_std,columns=cols)","962572d4":"ptV2 = PowerTransformer()\nX_train_std_trans = ptV2.fit_transform(X_train_std_trans)\nX_test_std_trans = ptV2.transform(X_test_std)","faae823d":"model_lr_std_trans_tuned = run_lr_gridsearchcv(param_grid,kfold,X_train_std_trans,y_train,'model_lr_std_trans_tuned')","522c9178":"train_and_test_model(model_lr_plain_tuned,X_train,y_train,X_test,y_test,kfold)","3d46a1a7":"train_and_test_model(model_lr_min_tuned,X_train_min,y_train,X_test_min,y_test,kfold)","8d957e68":"train_and_test_model(model_lr_std_tuned,X_train_std,y_train,X_test_std,y_test,kfold)","6d270db9":"train_and_test_model(model_lr_corr_tuned,X_train_corr,y_train,X_test_corr,y_test,kfold)","717b6528":"df_results","ce855146":"train_and_test_model(model_lr_trans_tuned,X_train_trans,y_train,X_test_trans,y_test,kfold)","3da222d7":"df_results","36f717ff":"train_and_test_model(model_lr_std_trans_tuned,X_train_std_trans,y_train,X_test_std_trans,y_test,kfold)","92aebf5b":"df_results","587f18b1":"ros = RandomOverSampler(random_state=4)\nX_train_std_ro, y_train_std_ro = ros.fit_resample(X_train_std, y_train)","b93b616e":"kfold_oversampled = StratifiedKFold(n_splits=10, random_state=4, shuffle=True)","32f2830d":"model_lr_std_ros_tuned = run_lr_gridsearchcv(param_grid,kfold_oversampled,X_train_std_ro,y_train_std_ro,'model_lr_std_ros_tuned')","5663af89":"train_and_test_model(model_lr_std_ros_tuned,X_train_std_ro,y_train_std_ro,X_test_std,y_test,kfold_oversampled)","13e202a7":"ros = over_sampling.SMOTE(random_state=4)\nX_train_std_smote, y_train_std_smote = ros.fit_resample(X_train_std, y_train)","32d34755":"kfold_oversampled = StratifiedKFold(n_splits=10, random_state=4, shuffle=True)","64414e9b":"model_lr_std_smote_tuned = run_lr_gridsearchcv(param_grid,kfold_oversampled,X_train_std_smote,y_train_std_smote,'model_lr_std_smote_tuned')","5341f228":"train_and_test_model(model_lr_std_smote_tuned,X_train_std_smote,y_train_std_smote,X_test_std,y_test,kfold_oversampled)","447f8d5e":"ros = over_sampling.ADASYN(random_state=4)\nX_train_std_adasyn, y_train_std_adasyn = ros.fit_resample(X_train_std, y_train)","194830d0":"kfold_oversampled = StratifiedKFold(n_splits=10, random_state=4, shuffle=True)","cc080aaa":"model_lr_std_adasyn_tuned = run_lr_gridsearchcv(param_grid,kfold_oversampled,X_train_std_adasyn,y_train_std_adasyn,'model_lr_std_adasyn_tuned')","60ea62cd":"train_and_test_model(model_lr_std_adasyn_tuned,X_train_std_adasyn,y_train_std_adasyn,X_test_std,y_test,kfold_oversampled)","ebc2cf35":"df_results","69a598b6":"# Let the experiments begin!","9eea4c82":"Some Points observed:-\n* The basic model with out any change is performing better than most transformed models.\n* Standardization marginally improved the model.\n* Min max is worst of all.\n* Surprisingly oversampling did not work.","dab01e44":"## Experiment 1:- Does tuning parameters of a model after every change to the data makes sence?\n### Let us tune Logistic regression model before and after changing data(standardization, normalization, transforming etc)","90d3651b":"# The Idea is to Create and test Logistic regression model on the creadit card fraud detection by tuning it with various kind of transformations to data like:-\n* Data as is\n* Standardize\n* Normalize\n* Using only highly correlated variables\n* Transforming Data\n* Random Oversampling\n* SMOTE\n* ADASYN","e784a210":"# Cut Cut Cut"}}