{"cell_type":{"e1866fb1":"code","925db9ff":"code","72288aff":"code","8ff331e1":"code","c6eb6c28":"code","02dbb483":"code","3de2c205":"code","f37ed043":"code","7fb602ca":"code","45cc5620":"code","4890233b":"code","de40bc84":"code","fe5ca8a5":"code","0dcb77a0":"code","bf26368a":"code","38b5a8f2":"code","a12f15ac":"code","9e085aad":"code","dc756e91":"code","b50a1efa":"code","966d621b":"code","668fb654":"code","4f329579":"code","3bf97edd":"code","5ab6d048":"code","c00a484f":"code","31891124":"code","d907d0c9":"code","25238eff":"code","5a42038e":"code","7857d32b":"code","c25725a3":"code","d0890782":"code","c3512562":"code","a140ca9f":"code","c895d9a1":"code","37c97ca2":"code","4372a548":"code","433de561":"code","e5a8487f":"code","5d734ce2":"markdown","be06fa14":"markdown","28c43bd4":"markdown","dc63bf12":"markdown","7432e176":"markdown","dd81efd9":"markdown","e126adad":"markdown"},"source":{"e1866fb1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","925db9ff":"all_tweets = pd.read_json('\/kaggle\/input\/twitter-classification\/random_tweets.json',lines =True)","72288aff":"#total number of tweets in the dataset\nprint(len(all_tweets))\nprint('\\n')\n#columns and features\nprint(all_tweets.columns)\nprint('\\n')\n#text of the first tweet in the dataset\nprint(all_tweets.loc[0]['text'])","8ff331e1":"#feature user is a dictionary\nprint(all_tweets.loc[0]['user'])","c6eb6c28":"#just the location  of user\nprint(all_tweets.loc[0]['user']['location'])","02dbb483":"#checking re-tweet count to determine viral_tweets\nprint(all_tweets['retweet_count'])","3de2c205":"#we will use the median number forretweet count\n\nmedian_retweets = all_tweets.retweet_count.median()\n\n#if retweet count is more than the median count then we will classify them as 1 otherwise 0\n\nall_tweets['is_viral'] = np.where(all_tweets['retweet_count'] > median_retweets , 1 , 0)\nprint(all_tweets['is_viral'])","f37ed043":"### making FEATURES\n# we want to know what makes the tweet viral; it can be the length of the tweet,can be how many\n# hashtags the tweet has\n\nall_tweets['tweet_length'] = all_tweets.apply(lambda tweet: len(tweet['text']),axis=1)\nprint(all_tweets['tweet_length'])","7fb602ca":"#follower_count feature\nall_tweets['followers_count'] = all_tweets.apply(lambda tweet: tweet['user']['followers_count'],axis=1)\n#and same for firends_count\nall_tweets['friends_count'] = all_tweets.apply(lambda tweet: tweet['user']['friends_count'],axis=1)","45cc5620":"# can use number of hashtags in a tweet.  (using '#' count)\n# the number of links in a tweet .  (using 'http' count)\n# the number of words in a tweet . (using split)\n# the avrg number of words in the tweet.","4890233b":"all_tweets['hashtag_count'] = all_tweets.apply(lambda tweet: tweet['text'].count('#'),axis=1)\nall_tweets['http_count'] = all_tweets.apply(lambda tweet: tweet['text'].count('http'),axis=1)","de40bc84":"#get rid of the data which is not relevant\nlabels = all_tweets['is_viral']\ndata = all_tweets[['tweet_length','followers_count','friends_count','hashtag_count','http_count']]","fe5ca8a5":"from sklearn.preprocessing import scale\n\nscaled_data = scale(data , axis=0)","0dcb77a0":"print(scaled_data)","bf26368a":"from sklearn.model_selection import train_test_split","38b5a8f2":"train_data , test_data ,train_labels , test_labels = train_test_split(data , labels , test_size = 0.2 , random_state = 101)","a12f15ac":"from sklearn.neighbors import KNeighborsClassifier\nKnn = KNeighborsClassifier(n_neighbors=5)","9e085aad":"Knn.fit(train_data,train_labels)","dc756e91":"Knn.score(test_data,test_labels)","b50a1efa":"#we gonna chose a valid K which will increase our score","966d621b":"scores = []\nfor k in range(1,200):\n    Knn = KNeighborsClassifier(n_neighbors=k)\n    Knn.fit(train_data,train_labels)\n    scores.append(Knn.score(test_data,test_labels))\nplt.plot(scores)\nplt.show()  ","668fb654":"# we will chose our k somewhere between 30 - 38 to achieve 60% \n# we need to work more on the features to achive more accuracy but the machine is still  better than 50 - 50","4f329579":"# we are going to classify tweets or any sentence wether it came from new_york , london , paris","3bf97edd":"new_york_tweets = pd.read_json('\/kaggle\/input\/twitter-classification\/new_york.json',lines=True)","5ab6d048":"#total number of tweets in the dataset\nprint(len(new_york_tweets))\nprint('\\n')\n#columns and features\nprint(new_york_tweets.columns)\nprint('\\n')\n#text of the 13th tweet in the dataset\nprint(new_york_tweets.loc[12]['text'])","c00a484f":"london_tweets = pd.read_json('\/kaggle\/input\/twitter-classification\/london.json',lines=True) \nparis_tweets = pd.read_json('\/kaggle\/input\/twitter-classification\/paris.json',lines=True)","31891124":"print('New York')\nprint(len(new_york_tweets))\nprint('London')\nprint(len(london_tweets))\nprint('Paris')\nprint(len(paris_tweets))","d907d0c9":"new_york_text = new_york_tweets['text'].tolist()\nlondon_text = london_tweets['text'].tolist()\nparis_text = paris_tweets['text'].tolist()\n\ndf_tweet = new_york_text + london_text + paris_text\ny = [0] * len(new_york_text) + [1] * len(london_text) + [2] * len(paris_text)","25238eff":"X_train ,X_test , y_train , y_test =  train_test_split(df_tweet , y , test_size=0.2, random_state=1)","5a42038e":"print(len(X_train))\nprint(len(X_test))","7857d32b":"# we need CountVector\nfrom sklearn.feature_extraction.text import CountVectorizer\ncounter = CountVectorizer()","c25725a3":"counter.fit(X_train)\ntrain_count = counter.transform(X_train)\ntest_count = counter.transform(X_test)","d0890782":"print(X_train[3])\nprint(train_count[3])","c3512562":"from sklearn.naive_bayes import MultinomialNB\nmnb = MultinomialNB()","a140ca9f":"mnb.fit(train_count,y_train)","c895d9a1":"predictions = mnb.predict(test_count)","37c97ca2":"print(predictions)","4372a548":"from sklearn.metrics import accuracy_score,confusion_matrix\nprint(accuracy_score(y_test,predictions))\nprint('\\n')\nprint(confusion_matrix(y_test,predictions))","433de561":"tweet = 'Hello ! My name is DjSarafO'\nN_tweet = 'I live in Manhattan'","e5a8487f":"z = counter.transform([tweet])\np = counter.transform([N_tweet])\nprint(mnb.predict(z))\nprint(mnb.predict(p))","5d734ce2":"> Evaluating our model","be06fa14":"*normalizing the data*","28c43bd4":"**Train Test Split**","dc63bf12":"> Part 2 Tweet Locations","7432e176":"***TRAIN TEST NAIVE BAYES CLASSIFIER***","dd81efd9":"***Train the classifier***","e126adad":"***Classifying using language - Naive Bayes Classifier***"}}