{"cell_type":{"d6e39c8e":"code","b6e0a1af":"code","772e02fd":"code","9ebc1adc":"code","f2d1eeb5":"code","56a0f716":"code","e7df5d09":"code","5c23a55c":"code","f9cfe97e":"code","3087999c":"code","37a9a0ae":"code","1856307d":"code","c846e7ca":"code","8aae041e":"code","f528f197":"code","5112f882":"code","f861415a":"code","bc8d172e":"code","b22b9101":"code","80e64aa9":"code","713b1a66":"code","5a143e93":"code","157dec0c":"code","3ed17a1b":"code","66e1e928":"code","d6ba8aa5":"code","bc7246df":"code","f4399cc2":"code","0a5085ef":"code","dc2ab3c2":"code","f8a28437":"code","f35ac3f2":"code","25dc5714":"code","ab6ac8d3":"code","b20735d1":"code","bce2092e":"code","c9af02c5":"code","9cd49493":"code","5bd8a34b":"code","a0927db0":"markdown","f18e2692":"markdown","7bb192ec":"markdown","b1e9e620":"markdown","a4a60856":"markdown","b3878533":"markdown","775fb732":"markdown","06068a67":"markdown","e6ebebf3":"markdown","96688f54":"markdown","4054263a":"markdown","a252eda3":"markdown","b933e608":"markdown","e017308d":"markdown","ca99eb37":"markdown","e7b46b18":"markdown"},"source":{"d6e39c8e":"# I don't usually import all the libraries at once, maybe because I can't recall all of those at once.\n# I'll be importing the libraries when required\n\nimport pandas as pd  \nimport numpy as np   ","b6e0a1af":"data=pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/train.csv') # train dataset\ntest=pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/test.csv') # test dataset","772e02fd":"data.head() ","9ebc1adc":"train=data.copy()\ntrain.shape","f2d1eeb5":"test.shape","56a0f716":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\ncat_cols=train.select_dtypes(include=np.object).columns\nfig,ax=plt.subplots(10,1,figsize=(10,20))\nfor i,col in enumerate(cat_cols):\n    train[col].value_counts().plot(kind='bar',ax=ax[i],label=col)\n","e7df5d09":"from sklearn.preprocessing import LabelEncoder\n# LabelEncoding for the first three categorical features\nfor col in cat_cols[0:3]:\n    train[col]=LabelEncoder().fit_transform(train[col])\n    test[col]=LabelEncoder().fit_transform(test[col])","5c23a55c":"from sklearn.preprocessing import OneHotEncoder\nohe=OneHotEncoder()\nohe.fit(train[cat_cols[3:]])\ntrain=pd.concat([train.drop(columns=cat_cols[3:]),pd.DataFrame(ohe.transform(train[cat_cols[3:]]).toarray())],axis=1,join='inner')\ntest=pd.concat([test.drop(columns=cat_cols[3:]),pd.DataFrame(ohe.transform(test[cat_cols[3:]]).toarray())],axis=1,join='inner')","f9cfe97e":"train.head()","3087999c":"test.head()","37a9a0ae":"# If we want to use get_dummies it can be done this way.\n\n#train=pd.get_dummies(train,prefix_sep='_',columns=cat_cols[3:])\n#test=pd.get_dummies(test,prefix_sep='_',columns=cat_cols[3:])\n\n# Here the test and train won't be of same size as there is a missing category G in cat6 feature in the testset.\n# we should insert the missing column seperately\n\n# cat6_G=np.zeros(test.shape[0])\n#test.insert(loc=35,column='cat6_G',value=cat6_G)\n","1856307d":"X=train.drop(columns=['target','id'])\nY=train.target","c846e7ca":"from sklearn.model_selection import train_test_split\n\n# since we have so much data I've used only 0.01% of training data as test data\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,shuffle=True,test_size=0.01,random_state=100)","8aae041e":"print(X_train.shape)\nprint(X_test.shape)","f528f197":"def get_predictions(model,X_train,Y_train,testset):\n    model.fit(X_train,Y_train)\n    return model.predict(testset)","5112f882":"from sklearn.metrics import mean_squared_error\n\ndef rmse(predictions,Y_test):\n    return np.sqrt(mean_squared_error(predictions,Y_test))","f861415a":"from sklearn.linear_model import LinearRegression\nrmse(get_predictions(LinearRegression(),X_train,Y_train,X_test),Y_test)","bc8d172e":"from sklearn.ensemble import RandomForestRegressor\nrmse(get_predictions(RandomForestRegressor(),X_train,Y_train,X_test),Y_test)","b22b9101":"from xgboost import XGBRegressor\nrmse(get_predictions(XGBRegressor(tree_method='gpu_hist'),X_train,Y_train,X_test),Y_test)","80e64aa9":"from lightgbm import LGBMRegressor\nrmse(get_predictions(LGBMRegressor(),X_train,Y_train,X_test),Y_test)","713b1a66":"#!pip install catboost\nfrom catboost import CatBoostRegressor\n\n#for catboost we can directly use categorical features, \n#So splitting the categorical data itself into train test sets.\n\nX_train1,X_test1,Y_train1,Y_test1=train_test_split(data.drop(columns=['id']),data.target,test_size=0.01,random_state=100)\nrmse(get_predictions(CatBoostRegressor(cat_features=cat_cols,task_type='GPU'),X_train1,Y_train1,X_test1),Y_test1)","5a143e93":"from sklearn.model_selection import KFold\n\ndef get_predictions_with_kfold(model,X_train,Y_train,testset,nfolds):\n    kf=KFold(n_splits=nfolds,shuffle=True,random_state=1)\n    preds=np.zeros((testset.shape[0]))\n    for fold,(train_idx,valid_idx) in enumerate(kf.split(X=X_train)):\n        X1,Y1=X_train.iloc[train_idx],Y_train.iloc[train_idx]\n        X2,Y2=X_train.iloc[valid_idx],Y_train.iloc[valid_idx]\n        \n        model.fit(X1,Y1,eval_set=[(X2,Y2)],early_stopping_rounds=500,eval_metric='rmse')\n        preds+=model.predict(testset)\/nfolds\n    return preds","157dec0c":"nfolds=5\nrmse(get_predictions_with_kfold(XGBRegressor(tree_method='gpu_hist'),X_train,Y_train,X_test,nfolds),Y_test)","3ed17a1b":"rmse(get_predictions_with_kfold(LGBMRegressor(),X_train,Y_train,X_test,nfolds),Y_test)","66e1e928":"!pip install optuna","d6ba8aa5":"def objective(trial):\n    param={\n        'tree_method':'gpu_hist',\n        'n_estimators':trial.suggest_int('n_estimators',50,500),\n        'tree_method':'gpu_hist',\n        'max_depth':trial.suggest_int('max_depth',3,20),\n        'learning_rate':trial.suggest_float('learning_rate',0.001,0.1,log=True),\n        'reg_lambda':trial.suggest_float('reg_lambda',0.0,10),\n        'reg_alpha':trial.suggest_float('reg_alpha',0.0,10),\n        'gamma': trial.suggest_float('gamma', 0.0, 10),\n        'subsample': trial.suggest_categorical('subsample', [0.8, 0.9, 1.0]),\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.1, 0.2, 0.3, 0.4, 0.5])\n    }\n    modell=XGBRegressor(**param)\n    modell.fit(X_train,Y_train)\n    return np.sqrt(mean_squared_error(modell.predict(X_test),Y_test))","bc7246df":"import optuna\nstudy=optuna.create_study(direction='minimize')\nstudy.optimize(objective,n_trials=50)","f4399cc2":"study.best_params","0a5085ef":"optuna.visualization.plot_param_importances(study)","dc2ab3c2":"optuna.visualization.plot_optimization_history(study)","f8a28437":"optuna.visualization.plot_slice(study)","f35ac3f2":"def objective1(trial):\n    param={\n      'n_estimators':trial.suggest_int('n_estimators',400,1500),\n      'max_depth':trial.suggest_int('max_depth',10,40),\n      'num_leaves':trial.suggest_int('num_leaves',60,150),\n      'learning_rate':trial.suggest_float('learning_rate',0.001,0.2,log=True),\n      'boosting_type':trial.suggest_categorical('boosting_type',['gbdt']),\n      'class_weight':trial.suggest_categorical('class_weight',['balanced']),\n      'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0),\n      'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0),\n      'colsample_bytree': trial.suggest_float('colsample_bytree', 0.2, 0.9),\n      'min_child_samples': trial.suggest_int('min_child_samples', 40, 250),\n      'subsample_freq': trial.suggest_int('subsample_freq', 1, 10),\n      'subsample': trial.suggest_float('subsample', 0.3, 0.9),\n      'max_bin': trial.suggest_int('max_bin', 128, 1024),\n      'min_data_per_group': trial.suggest_int('min_data_per_group', 50, 200),\n      'cat_smooth': trial.suggest_int('cat_smooth', 50, 100),\n      'cat_l2': trial.suggest_int('cat_l2', 1, 20)\n    }\n    modell=LGBMRegressor(**param)\n    modell.fit(X_train,Y_train)\n\n    return np.sqrt(mean_squared_error(modell.predict(X_test),Y_test))","25dc5714":"study1=optuna.create_study(direction='minimize')\nstudy1.optimize(objective1,n_trials=50)","ab6ac8d3":"study1.best_params","b20735d1":"optuna.visualization.plot_optimization_history(study1)","bce2092e":"optuna.visualization.plot_slice(study1)","c9af02c5":"optuna.visualization.plot_param_importances(study1)","9cd49493":"def csv(model,nfolds):\n    preds=get_predictions_with_kfold(model,X_train,Y_train,test.drop(coluns=['id']),nfolds)\n    pd.DataFrame({\n        'id':test.id,\n        'taget':preds\n    }).to_csv('prediction.csv',index=False)","5bd8a34b":"csv(LGBMRegressor(**study1.best_params),10)","a0927db0":"If we want to we can tune the Hyperparameters even further and also the value of nfolds in kfold should also be considered as hyperparameter as different values give different model predictions. I have tried some values in between 5 to 10.","f18e2692":"Whichever model you feel is performing better convert those predictions into csv file and then submit","7bb192ec":"# Data Preprocessing","b1e9e620":"Before this competition I was using GridSearchCV and RandomSearchCV for Hyperparameter tuning. I got to learn hyperopt and optuna libraries among which optuna is a bit faster, So i will be tuning Hyperparameters using optuna here.","a4a60856":"We can even try pd.get_dummies() for OneHotencoding and this is somewhat easier when compared to using sklearn OneHotEncoder, but the get_dummies method varies the dataset directly and we can't use fit and then transform on the dataset. So if there are different categories for a feature in train and test set the size of dataset will be different which will cause problems, so instead fitting the train dataset to OneHotEncoder and then transforming both train and test set is better for this case","b3878533":"Here as we can see we have the first column id which is of no use for prediction, 9 categorical features(cat0 to cat9) and numeric features(cont1 to cont13) and the target.","775fb732":"**Firstly I'll be using normal models without any hyperparameter tuning or so and not even KFold. We will see how it varies the performance after we make these changes.**","06068a67":"Here as we can see these are the models I have used and worked with to get better accuracy by tuning their hyperparameters. We can't ignore LinearRegression as it is the basic and first thing everyone tries with regression problems. Below Hyperparameter tuning is shown for the three boosting algorithms.","e6ebebf3":"Now we will start using KFold on all the three boosting algorithms and see how the performance increases which is because we are not training the model all at once with the whole dataset.","96688f54":"* In this notebook I am gonna go through what I have learned through the course of participating in this Tabular Playground Series in the month of Febraury.\n* Firstly these Tabular Playground Series are great fun to learn for beginners who want to participate in competitions after going through Titanic or even you can start your Kaggle journey right here.\n* Earlier I used to just join a competition, submit the predictions of basic model and I wasn't at all used to learning new concepts from discussions and notebooks. The best way to learn in Kaggle is to go through notebooks and this is how I got to learn so much during this competition and ended in the top 19%.\n* I have started with basic Linear Regression Model and then while going through a lot of others work, understood the importance of hyperparameter tuning and we will go through all this stuff in this notebook.\n* This notebook will be super useful I guess for beginners who want to try out each model and learn new things such as training model with KFold and how important is hyperparameter tuning.","4054263a":"We can see the difference in rmse when model is trained on the whole dataset at once and by using KFold. This is helpful when the dataset is large and noisy to decrease the variance of the model by training and validating on smaller datasets.","a252eda3":"For the categorical variables I have tried only two types of encoding which are LabelEncoding and OneHotEncoding and from my results OneHotEncoding performed a little better than LabelEncoding. I prefer to use LabelEncoding only when categroical variables have less number of categrories(2 to 4 at max). So for the first three categorical features I have used LabelEncoding and OneHot for others.","b933e608":"LightGBM Hyperparameter tuning.","e017308d":"# Tabular Playground Series Feb2021(Basic Linear Regression Model to LGBM Hyperparameter tuning with optuna)","ca99eb37":"XGboost Hyperparameter tuning.","e7b46b18":"# Model Building"}}