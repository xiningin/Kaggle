{"cell_type":{"ea92fd48":"code","23897c26":"code","4a0bb5fd":"code","bc36e581":"code","998651bc":"code","7aa01152":"code","7354d7c4":"code","5f9d89d0":"code","4bca5aa7":"code","fa79f98f":"code","61e0ad42":"code","49c0059f":"code","e1ba2d5f":"code","195c101b":"code","fb78b5af":"code","f2df26ed":"code","cc8882aa":"code","5c9ec838":"code","3ffaea55":"code","a03a7ff2":"code","afdd394c":"code","5d2f1f61":"code","57522dca":"code","51568d91":"code","74f7eed7":"code","26865a9e":"code","a6d2a2df":"code","989678e0":"code","2f9913e4":"code","ae1d9039":"code","c96db842":"code","460db934":"code","cd1a5c63":"code","86ed6045":"code","90d01d2e":"code","7bae21fd":"code","6f39e0cc":"code","06b78d2e":"code","9e658342":"code","f579d1aa":"code","2094f19b":"code","64e4660d":"code","5a97fddd":"markdown","c952d9a8":"markdown","d9c50f52":"markdown","eb3655af":"markdown","457735bf":"markdown","1ebefdcb":"markdown","2d21924a":"markdown","5d43bb54":"markdown","71ab933b":"markdown","d9aa158d":"markdown","c2c2ecd8":"markdown","054b6771":"markdown","27f5f037":"markdown","204ad2b7":"markdown","fcbfbacf":"markdown","e0bca724":"markdown","2008b420":"markdown"},"source":{"ea92fd48":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","23897c26":"train_df = pd.read_json('\/kaggle\/input\/github-bugs-prediction\/embold_train.json')\ntrain_df[\"text\"] = train_df.title + \" \" + train_df.body\ntrain_df.head()","4a0bb5fd":"print(f'We have {train_df.shape[0]} rows and {train_df.shape[-1]} columns')\nprint(f'\\n')\nprint(f'Remember we combined Title and Body to create new column \"Text\"')\nprint(f'columns {train_df.columns}')","bc36e581":"train_df.dtypes  # check data types ","998651bc":"def label_encode(data, from_numeric= True):\n    \n    '''wrappen function to label to numberic code and vice versa'''\n    \n    if from_numeric:\n        if data== 0:\n            return 'Bug'\n        elif data == 1:\n            return 'Feature'\n        elif data == 2:\n            return 'Question'\n        \n    else:\n        if data== 'Bug':\n            return 0\n        elif data == 'Feature':\n            return 1\n        elif data == 'Question':\n            return 2","7aa01152":"# Lets convert 'label' to its classification label for better visualization \ntrain_df['label'] = train_df.label.apply(label_encode)\n\ntrain_df.label = train_df.label.astype('category')  # convert in category data types\n\ntrain_df.label.head()","7354d7c4":"import seaborn as sns","5f9d89d0":"train_df.label.value_counts()  # .plot(kind= \"bar\")","4bca5aa7":"sns.countplot(x='label',data=train_df)\n            \nprint(f'There are too many request for bug  and feature, less on Questioin \\nClearly our class is imbalanced')","fa79f98f":"train_df['text_len'] = train_df['text'].astype(str).apply(len)\ntrain_df['word_count'] = train_df['text'].apply(lambda x: len(str(x).split()))","61e0ad42":"plt.figure(figsize=[15,5],frameon=True)\n\nplt.subplot(1,2,1)\norder_index = train_df.word_count.value_counts().index\nsns.distplot(train_df.word_count,kde = False)\nplt.title('Overall number of words used')\n\nplt.subplot(1,2,2)\n\norder_index = train_df.text_len.value_counts().index\nsns.distplot(train_df.text_len,kde = False)\nplt.title('Overall number characters used')\n\n\n# plt.close()","49c0059f":"from plotly.offline import iplot\nimport seaborn as sns\n\nimport cufflinks as cf\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)","e1ba2d5f":"train_df['text_len'].iplot(\n    kind='hist',\n    bins=100,\n    xTitle='Text  length',\n    linecolor='black',\n    yTitle='count',\n    title='Text Length Distribution')","195c101b":"train_df['word_count'].iplot(\n    kind='hist',\n    bins=100,\n    xTitle='word count',\n    linecolor='black',\n    yTitle='count',\n    title='word count Distribution')","fb78b5af":"sns.catplot(x=\"label\", y=\"text_len\", data= train_df)\n\ng = sns.FacetGrid(train_df, col=\"label\")\ng.map(sns.distplot, \"text_len\",kde = False)","f2df26ed":"sns.catplot(x=\"label\", y=\"word_count\", data= train_df)\n\ng = sns.FacetGrid(train_df, col=\"label\")\ng.map(sns.distplot, \"word_count\",kde = False)","cc8882aa":"import string","5c9ec838":"# count_Bug_punctuations      = train_df[train_df.label == 'Bug']['text'].apply(lambda z: len([c for c in str(z) if c in string.punctuation]))\n# count_Feature_punctuations  = train_df[train_df.label == 'Feature']['text'].apply(lambda z: len([c for c in str(z) if c in string.punctuation]))\n# count_Question_punctuations = train_df[train_df.label == 'Question']['text'].apply(lambda z: len([c for c in str(z) if c in string.punctuation]))","3ffaea55":"train_df['count_punctuations'] = train_df.text.apply(lambda z: len([c for c in str(z) if c in string.punctuation]))","a03a7ff2":"g = sns.FacetGrid(train_df, col=\"label\" , height = 4, aspect = 1 , sharex = True , sharey = True)\ng.map(sns.distplot, \"count_punctuations\",kde = False)\n\n# print(f'X and Y range are different, so better look figure carefully !!')","afdd394c":"import nltk\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')","5d2f1f61":"train_df['stop_words'] = train_df.text.apply(lambda z : np.mean([len(z) for w in str(z).split()]))\n\ng = sns.FacetGrid(train_df, col=\"label\" , height = 4, aspect = 1 , sharex = True , sharey = True )\ng.map(sns.distplot, \"stop_words\",kde = False)\n\n# print(f'X and Y range are different, so better look figure carefully !!')","57522dca":"train_df['check_url'] = train_df.text.apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n\ng = sns.FacetGrid(train_df, col=\"label\" , height = 4, aspect = 1 , sharex = True , sharey = True)\ng.map(sns.distplot, \"check_url\",kde = False)\n\n# print(f'X and Y range are different, so better look figure carefully !!')","51568d91":"import wordcloud\nfrom wordcloud import WordCloud,STOPWORDS\n\nfrom PIL import Image\n","74f7eed7":"def display_cloud(data,color):\n    plt.subplots(figsize=(15,15))\n    mask = None\n    wc = WordCloud(stopwords=STOPWORDS, \n                   mask=mask, background_color=\"white\", contour_width=2, contour_color=color,\n                   max_words=2000, max_font_size=256,\n                   random_state=42)\n    wc.generate(' '.join(data))\n    plt.imshow(wc, interpolation=\"bilinear\")\n    plt.axis('off')\n    plt.show()","26865a9e":"display_cloud(train_df['text'],'red')","a6d2a2df":"print(f'For Bug class')\ndisplay_cloud(train_df[train_df.label == 'Bug']['text'],'red')","989678e0":"print(f'For Feature class')\ndisplay_cloud(train_df[train_df.label == 'Feature']['text'],'red')","2f9913e4":"print(f'For Question class')\ndisplay_cloud(train_df[train_df.label == 'Question']['text'],'red')","ae1d9039":"#Simplified counter function\n\nfrom collections import Counter\n\ndef create_corpus(word):\n    corpus=[]\n    \n    for x in train_df[train_df['label']==word]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus\n","c96db842":"stops=set(stopwords.words('english'))","460db934":"sns.set(rc={'figure.figsize':(11.7,8.27)})","cd1a5c63":"corpus=create_corpus('Bug')\ncounter=Counter(corpus)\nmost=counter.most_common()\n\nx=[]\ny=[]\nfor word,count in most[:100]:\n    if (word not in stops) :\n        x.append(word)\n        y.append(count)\nsns.barplot(x=y,y=x)","86ed6045":"corpus=create_corpus('Feature')\ncounter=Counter(corpus)\nmost=counter.most_common()\n\nx=[]\ny=[]\nfor word,count in most[:100]:\n    if (word not in stops) :\n        x.append(word)\n        y.append(count)\nsns.barplot(x=y,y=x)","90d01d2e":"corpus=create_corpus('Question')\ncounter=Counter(corpus)\nmost=counter.most_common()\n\nx=[]\ny=[]\nfor word,count in most[:100]:\n    if (word not in stops) :\n        x.append(word)\n        y.append(count)\nsns.barplot(x=y,y=x)","7bae21fd":"# def gram_analysis(data,gram):\n    \n#     token= tokenizer.tokenize(data.lower()) \n#     token = [tok for tok in token if len(tok) > 2 if tok not in stopword_list and not tok.isdigit()]\n#     ngrams=zip(*[token[i:] for i in range(gram)])\n#     final_tokens=[\" \".join(z) for z in ngrams]\n#     return final_tokens\n\n\n# def create_dict(data,grams):\n#     freq_dict=defaultdict(int)\n#     for sentence in data:\n#         for tokens in gram_analysis(sentence,grams):\n#             freq_dict[tokens]+=1\n#     return freq_dict\n\n# def horizontal_bar_chart(df, color):\n#     trace = go.Bar(\n#         y=df[\"n_gram_words\"].values[::-1],\n#         x=df[\"n_gram_frequency\"].values[::-1],\n#         showlegend=False,\n#         orientation = 'h',\n#         marker=dict(\n#             color=color,\n#         ),\n#     )\n#     return trace\n\n# def create_new_df(freq_dict,):\n#     freq_df=pd.DataFrame(sorted(freq_dict.items(),key=lambda z:z[1])[::-1])\n#     freq_df.columns=['n_gram_words','n_gram_frequency']\n#     trace=horizontal_bar_chart(freq_df[:20],'orange')\n#     return trace","6f39e0cc":"# def plot_grams(df1,df2,df3):\n#     fig = tools.make_subplots(rows=1, cols=3, vertical_spacing=0.1,\n#                           subplot_titles=[\"Frequent words of lable 0\", \n#                                           \"Frequent words of lable 1\",\n#                                           \"Frequent words of lable 2\"])\n#     fig.append_trace(df1, 1, 1)\n#     fig.append_trace(df2, 1, 2)\n#     fig.append_trace(df3, 1, 3)\n#     fig['layout'].update(height=800, width=1000, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\n#     py.iplot(fig, filename='word-plots')","06b78d2e":"# for gram in range(2,4):\n    \n#     if(gram == 2):\n#         print(\"Bi-gram analysis\")\n#     else:\n#         print(\"Tri-gram analysis\")\n\n#     freq_label_0_zero=create_dict(label_0_df['text'][:400],gram)\n#     trace_zero=create_new_df(freq_label_0_zero)\n    \n#     freq_label_1_ones=create_dict(label_1_df['text'][:400],gram)\n#     trace_ones=create_new_df(freq_label_1_ones)\n    \n#     freq_label_2_ones=create_dict(label_2_df['text'][:400],gram)\n#     trace_secs=create_new_df(freq_label_2_ones)\n    \n#     plot_grams(trace_zero,trace_ones,trace_secs)","9e658342":"# train_df.columns","f579d1aa":"# count_Bug_punctuations      = train_df[train_df.label == 'Bug']['text'].apply(lambda z: len([c for c in str(z) if c in string.punctuation]))\n# count_Feature_punctuations  = train_df[train_df.label == 'Feature']['text'].apply(lambda z: len([c for c in str(z) if c in string.punctuation]))\n# count_Question_punctuations = train_df[train_df.label == 'Question']['text'].apply(lambda z: len([c for c in str(z) if c in string.punctuation]))","2094f19b":"#Regex cleaning\nimport re\n\ndef remove_punctuations(data):\n    punct_tag=re.compile(r'[^\\w\\s]')\n    data=punct_tag.sub(r'',data)\n    return data\n\ndef remove_html(data):\n    html_tag=re.compile(r'<.*?>')\n    data=html_tag.sub(r'',data)\n    return data\n\ndef remove_url(data):\n    url_clean= re.compile(r\"https:\/\/\\S+|www\\.\\S+\")\n    data=url_clean.sub(r'',data)\n    return data\n\ndef clean_data(data):\n    emoji_clean= re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    data=emoji_clean.sub(r'',data)\n    url_clean= re.compile(r\"https:\/\/\\S+|www\\.\\S+\")\n    data=url_clean.sub(r'',data)\n    return data","64e4660d":"train_df.text = train_df.text.apply(lambda z : remove_url(z))\ntrain_df.text = train_df.text.apply(lambda z: clean_data(z))\ntrain_df.text = train_df.text.apply(lambda z: remove_html(z))\ntrain_df.text = train_df.text.apply(lambda z: remove_punctuations(z))","5a97fddd":"- Majority used words less than 200\n- Number of characters used are fairly in a range\n\nbelow plotly version of above visualization","c952d9a8":"Similar insight for punctuations \n- \"Bug\" level has max stopwods - It make sense as report a bug would certainly contains more stopwords if code or code logs are mention ","d9c50f52":"## Most occuring words distribution ?\n\nHey !! , in below bar plot, what we are seeing so many unknown character ?\n-  Remember we are not cleaning data yet\n-  hmm I just want to show you , text analysis is not a clean job :) \n\nWe`ll clean stopwords later on","eb3655af":"WordCloud Visualizations\n","457735bf":"### Statistical Analysis-I\nOkay lets do some basic descriptive statitical insights on raw training set","1ebefdcb":"This also makes match with our comman sense - People will tend to give reference link for adding feature or for tentative reference solutions to a bug","2d21924a":"### Punctuation used in across class ?","5d43bb54":"## Statistical Analysis-II\nN-gram analysis - to be continue","71ab933b":"### Stopwords usage across class ?","d9aa158d":"## Inference so far - raw data\n\n- Balance class : \"Questions\" class is fairly less compared to \"Bug\" and \"Feature\" counts, where last two class is almost same counts\n- Stopwords : Stopwords contribute a major junk in \"Bug\" and \"Feature\" class . Thought they are the majority among class distribution\n- Unusual usage of long text length\n- Cleaning of text is recommended - Not only stopwords , presence of noise inclusing html ect","c2c2ecd8":"### Data quick glance","054b6771":"#### Lets see how character and words play role in different class","27f5f037":"### Attribute Description:\n\n    -Title - the title of the GitHub bug, feature question\n    -Body - the body of the GitHub bug, feature question\n    -Label - Represents various classes of Labels\n        Bug - 0\n        Feature - 1\n        Question - 2\n        \n    - Text - we combined Title and Body to have whole text feature","204ad2b7":"\"Bug\" class wordcloud\n- make sense to have \"error\" \"issue\" occuring frequently\n- if reported image, most reported in 'png' image format\n- Doe you notice python file is frequently used ? The reason I love python ","fcbfbacf":"\"Feature\" class wordcloud\n- make sense to have \"add\" ad occure tokens. Mostly feature to be added right ? Or are you feature deletion guy :) \n- So you already will give reason for \"github\" words occur too !\n","e0bca724":"### So how they (words used , characater used) do across category ?","2008b420":"\"Bug\" level has max punctuations - It make sense as report a bug would certainly contains more punctuations if code or code logs are mention "}}