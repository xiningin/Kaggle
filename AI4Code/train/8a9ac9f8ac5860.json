{"cell_type":{"47b6a400":"code","d110538e":"code","97b89cb0":"code","a001b548":"code","cae048a9":"code","f5de5ffc":"code","f8f4e6d2":"code","8af11ae3":"code","0cae7447":"code","2dd4f523":"code","6f3960ab":"code","77cbeab8":"code","c3c171af":"code","8bf8a590":"code","5256e0fc":"code","9e3a42ae":"code","84fdb343":"code","53d69933":"code","658c41a6":"code","ddae075f":"code","b1eec5f9":"code","a6a0586b":"code","8c72e263":"code","732d0cdb":"code","8448f968":"code","c7499db3":"code","25317859":"code","72a7ff21":"code","b2fe5bbc":"code","f51f39fe":"code","d7c71b62":"code","081a37d2":"code","fabdaa0c":"code","2537a9f9":"code","874d0dc5":"code","e04fb980":"code","533ec6da":"code","1dedf876":"code","ccbaba63":"code","b0042418":"code","b90c1936":"code","c0415a83":"code","1089bf6a":"code","329ae337":"code","b8a4334a":"code","12b7626a":"code","676af665":"code","0596ad89":"code","8daba54c":"markdown","49cd64e7":"markdown","22c77e50":"markdown","4339a1a9":"markdown","d358e132":"markdown","762eb9dd":"markdown","11797b0b":"markdown","e1556cb0":"markdown","5fbae530":"markdown","8c39c001":"markdown","e335edab":"markdown","83d57fd5":"markdown","554f11de":"markdown","b0ffd288":"markdown","ca91fa68":"markdown","e7994da7":"markdown","8620cafe":"markdown","441026a2":"markdown","a92751fd":"markdown","391701b8":"markdown","5c8dcfa4":"markdown","0d085b9d":"markdown","c83dead4":"markdown","f64adda4":"markdown","8adbacc8":"markdown","90b6603c":"markdown","27ef6323":"markdown","b0fb1bb1":"markdown","6485d483":"markdown","905a9cff":"markdown","56954b38":"markdown","d314f869":"markdown","dced0a1d":"markdown","9910f873":"markdown","d58ffa00":"markdown","6745649b":"markdown"},"source":{"47b6a400":"# Import delle l'analisi esplorativa dei dati\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport datetime as dt\nimport matplotlib.pyplot as plt\n\n# import delle librerie richieste per l'applicazione di algoritmi di clustering\nimport sklearn\nfrom sklearn.cluster import KMeans\nfrom scipy.cluster.hierarchy import linkage\nfrom sklearn.metrics import silhouette_score\nfrom scipy.cluster.hierarchy import cut_tree\nfrom scipy.cluster.hierarchy import dendrogram\nfrom sklearn.preprocessing import StandardScaler","d110538e":"store = pd.read_csv('..\/input\/customer-segmentation-tutorial-in-python\/Mall_Customers.csv')\nstore.head()","97b89cb0":"store.describe()","a001b548":"store.shape","cae048a9":"store.info()","f5de5ffc":"store.describe()","f8f4e6d2":"#Set dello stile dei grafici\nplt.style.use('fivethirtyeight')","8af11ae3":"plt.figure(1 , figsize = (10 , 5))\nn = 0 \nfor x in ['Age' , 'Annual Income (k$)' , 'Spending Score (1-100)']:\n    n += 1\n    plt.subplot(1 , 3 , n)\n    plt.subplots_adjust(hspace =0.5 , wspace = 0.5)\n    sns.distplot(store[x] , bins = 20)\n    plt.title('Distplot of {}'.format(x))\nplt.show()","0cae7447":"plt.figure(1 , figsize = (10 , 5))\nsns.countplot(y = 'Gender' , data = store)\nplt.show()","2dd4f523":"plt.figure(1 , figsize = (10 , 5))\nn = 0 \nfor x in ['Age' , 'Annual Income (k$)' , 'Spending Score (1-100)']:\n    for y in ['Age' , 'Annual Income (k$)' , 'Spending Score (1-100)']:\n        n += 1\n        plt.subplot(3 , 3 , n)\n        plt.subplots_adjust(hspace = 0.5 , wspace = 0.5)\n        sns.regplot(x = x , y = y , data = store)\n        plt.ylabel(y.split()[0]+' '+y.split()[1] if len(y.split()) > 1 else y )\nplt.show()","6f3960ab":"plt.figure(1 , figsize = (10 , 5))\nfor gender in ['Male' , 'Female']:\n    plt.scatter(x = 'Age' , y = 'Annual Income (k$)' , data = store[store['Gender'] == gender] ,\n                s = 200 , alpha = 0.5 , label = gender)\nplt.xlabel('Age'), plt.ylabel('Annual Income (k$)') \nplt.title('Age vs Annual Income suddiviso per Gender')\nplt.legend()\nplt.show()","77cbeab8":"plt.figure(1 , figsize = (10 , 5))\nfor gender in ['Male' , 'Female']:\n    plt.scatter(x = 'Annual Income (k$)',y = 'Spending Score (1-100)' ,\n                data = store[store['Gender'] == gender] ,s = 200 , alpha = 0.5 , label = gender)\nplt.xlabel('Annual Income (k$)'), plt.ylabel('Spending Score (1-100)') \nplt.title('Annual Income vs Spending Score suddiviso per Gender')\nplt.legend()\nplt.show()","c3c171af":"plt.figure(1 , figsize = (15 , 7))\nn = 0 \nfor cols in ['Age' , 'Annual Income (k$)' , 'Spending Score (1-100)']:\n    n += 1 \n    plt.subplot(1 , 3 , n)\n    plt.subplots_adjust(hspace = 0.5 , wspace = 0.5)\n    sns.violinplot(x = cols , y = 'Gender' , data = store , palette = 'vlag')\n    sns.swarmplot(x = cols , y = 'Gender' , data = store)\n    plt.ylabel('Gender' if n == 1 else '')\n    plt.title('Boxplots & Swarmplots' if n == 2 else '')\nplt.show()","8bf8a590":"# Metodo k-Means con un numero di clusters arbitrario.\n# - n_clusters: numero di cluster desiderati - limitazione del k-Means;\n# - n_init: esegue l'algoritmo n volte in modo indipendente, con diversi centroidi casuali per scegliere il modello finale come quello con il SSE pi\u00f9 basso.\n# - max_iter: indica il numero massimo di iterazioni per ogni singola esecuzione. \n\nX1 = store[['Age' , 'Spending Score (1-100)']].iloc[: , :].values\ninertia = []\nfor n in range(1 , 11):\n    method = (KMeans(n_clusters = n, init='k-means++', n_init = 10, max_iter=300, tol=0.0001, random_state= 1))\n    method.fit(X1)\n    inertia.append(method.inertia_)","5256e0fc":"#Stampa delle etichette relative ai cluster\nmethod.labels_","9e3a42ae":"# Plot del valore della somma della radice delle distanze al crescere del numero dei cluster\n\nplt.figure(1 , figsize = (10 ,5))\nplt.plot(np.arange(1 , 11) , inertia , 'o')\nplt.plot(np.arange(1 , 11) , inertia , '-' , alpha = 0.5)\nplt.xlabel('Numero dei cluster') , plt.ylabel('Sum of Squared Distance')\nplt.show()","84fdb343":"method = (KMeans(n_clusters = 4, init='k-means++', n_init = 10, max_iter=300, tol=0.0001, random_state= 1))\nmethod.fit(X1)\nlabels1 = method.labels_\ncentroids1 = method.cluster_centers_","53d69933":"#Stampa delle etichette predette\nmethod.labels_","658c41a6":"h = 0.02\nx_min, x_max = X1[:, 0].min() - 1, X1[:, 0].max() + 1\ny_min, y_max = X1[:, 1].min() - 1, X1[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = method.predict(np.c_[xx.ravel(), yy.ravel()]) ","ddae075f":"plt.figure(1 , figsize = (10 , 5) )\nplt.clf()\nZ = Z.reshape(xx.shape)\nplt.imshow(Z, interpolation='nearest', extent=(xx.min(), xx.max(), yy.min(), yy.max()), cmap = plt.cm.Pastel2, aspect = 'auto', origin='lower')\n\nplt.scatter( x = 'Age', y = 'Spending Score (1-100)', data = store, c = labels1, s = 50 )\nplt.scatter(x = centroids1[: , 0], y =  centroids1[: , 1], s = 50, c = 'red', alpha = 0.5)\nplt.ylabel('Spending Score (1-100)'), plt.xlabel('Age')\nplt.show()","b1eec5f9":"X2 = store[['Annual Income (k$)', 'Spending Score (1-100)']].iloc[: , :].values\ninertia = []\nfor n in range(1 , 11):\n    method = (KMeans(n_clusters = n, init='k-means++', n_init = 10, max_iter=300, tol=0.0001, random_state= 1))\n    method.fit(X2)\n    inertia.append(method.inertia_)","a6a0586b":"# Plot del valore della somma della radice delle distanze al crescere del numero dei cluster\n\nplt.figure(1 , figsize = (10 ,5))\nplt.plot(np.arange(1 , 11) , inertia , 'o')\nplt.plot(np.arange(1 , 11) , inertia , '-' , alpha = 0.5)\nplt.xlabel('Numero dei cluster') , plt.ylabel('Sum of Squared Distance')\nplt.show()","8c72e263":"method = (KMeans(n_clusters = 5, init='k-means++', n_init = 10, max_iter=300, tol=0.0001, random_state= 1))\nmethod.fit(X2)\nlabels2 = method.labels_\ncentroids2 = method.cluster_centers_","732d0cdb":"#Stampa delle etichette predette\nmethod.labels_","8448f968":"h = 0.02\nx_min, x_max = X2[:, 0].min() - 1, X2[:, 0].max() + 1\ny_min, y_max = X2[:, 1].min() - 1, X2[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ2 = method.predict(np.c_[xx.ravel(), yy.ravel()]) ","c7499db3":"plt.figure(1, figsize = (10 , 6))\nplt.clf()\nZ2 = Z2.reshape(xx.shape)\nplt.imshow(Z2 , interpolation='nearest', extent=(xx.min(), xx.max(), yy.min(), yy.max()), cmap = plt.cm.Pastel2, aspect = 'auto', origin='lower')\n\nplt.scatter(x = 'Annual Income (k$)', y = 'Spending Score (1-100)', data = store , c = labels2, s = 50)\nplt.scatter(x = centroids2[: , 0], y =  centroids2[: , 1], s = 50, c = 'red' , alpha = 0.5)\nplt.ylabel('Spending Score (1-100)'), plt.xlabel('Annual Income (k$)')\nplt.show()","25317859":"X3 = store[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']].iloc[: , :].values\ninertia = []\nfor n in range(1 , 11):\n    method = (KMeans(n_clusters = n ,init='k-means++', n_init = 10 ,max_iter=300, tol=0.0001,  random_state= 1))\n    method.fit(X3)\n    inertia.append(method.inertia_)","72a7ff21":"# Plot del valore della somma della radice delle distanze al crescere del numero dei cluster\n\nplt.figure(1 , figsize = (10 ,5))\nplt.plot(np.arange(1 , 11) , inertia , 'o')\nplt.plot(np.arange(1 , 11) , inertia , '-' , alpha = 0.5)\nplt.xlabel('Numero dei cluster') , plt.ylabel('Sum of Squared Distance')\nplt.show()","b2fe5bbc":"# Definizione della lista del numero di cluster da testare\nrange_n_clusters = list(x for x in range (2,10+1))\n\nfor num_clusters in range_n_clusters:\n    method = KMeans(n_clusters = num_clusters ,init='k-means++', n_init = 10 ,max_iter=300, tol=0.0001,  random_state= 1)\n    method.fit(X3)\n    cluster_labels = method.labels_\n    # Calcolo coefficiente di silhouette\n    silhouette_avg = silhouette_score(X3, cluster_labels)\n    print(\"Per n_clusters={0}, il coefficiente di Silhouette \u00e8 pari a {1}\".format(num_clusters, silhouette_avg))","f51f39fe":"method = (KMeans(n_clusters = 6, init='k-means++', n_init = 10, max_iter=300, tol=0.0001, random_state= 1))\nmethod.fit(X3)\nlabels3 = method.labels_\ncentroids3 = method.cluster_centers_","d7c71b62":"#Stampa delle etichette predette\nmethod.labels_","081a37d2":"# Assegnazione delle etichette a ciascun esempio presente nel DataFrame\nstore['Cluster_Id'] = method.labels_\n# Stampa dei primi 5 esempi\nstore.head()","fabdaa0c":"features_list = ['Age', 'Annual Income (k$)', 'Spending Score (1-100)']\n\nfor feature in features_list:\n    sns.boxplot(x='Cluster_Id', y=feature, data=store)\n    plt.show()","2537a9f9":"# Applicazione del metodo Single Linkage\nplt.figure(figsize = (10,5))\nsingle_linkage = linkage(X3, method=\"single\", metric='euclidean')\ndendrogram(single_linkage)\nplt.show()","874d0dc5":"# Applicazione del metodo Complete linkage\nplt.figure(figsize = (10,5))\ncomplete_linkage = linkage(X3, method=\"complete\", metric='euclidean')\ndendrogram(complete_linkage)\nplt.show()","e04fb980":"# Applicazione del metodo Average linkage\nplt.figure(figsize = (10,5))\navg_linkage = linkage(X3, method=\"average\", metric='euclidean')\ndendrogram(avg_linkage)\nplt.show()","533ec6da":"# Desiderando un numero di cluster pari a 4, si inizializza il parametro n_clusters=4\ncluster_labels = cut_tree(complete_linkage, n_clusters=4).reshape(-1, )\n#Stampa delle etichette dei cluster\ncluster_labels","1dedf876":"# Assegnazione delle etichette a ciascun esempio presente nel DataFrame\nstore['Cluster_Labels'] = cluster_labels\n# Stampa dei primi 5 elementi presenti nel DataFrame\nstore.head()","ccbaba63":"#Plot delle Features\n\nfeatures_list = ['Age', 'Annual Income (k$)', 'Spending Score (1-100)']\n\nfor feature in features_list:\n    sns.boxplot(x='Cluster_Labels', y=feature, data=store)\n    plt.show()","b0042418":"## Numero dei clienti in ciascun cluster\nstore['Cluster_Labels'].value_counts(ascending=True)","b90c1936":"from sklearn.cluster import AgglomerativeClustering\n\nac = AgglomerativeClustering(n_clusters=4, affinity='euclidean', linkage='complete')\nagglomerative_cluster_labels = ac.fit_predict(X3)","c0415a83":"# Assegnazione delle etichette a ciascun esempio presente nel DataFrame\nstore['Agglomerative_Clustering'] = agglomerative_cluster_labels\n# Stampa dei primi 5 elementi presenti nel DataFrame\nstore.head()","1089bf6a":"#Plot delle Features\n\nfeatures_list = ['Age', 'Annual Income (k$)', 'Spending Score (1-100)']\n\nfor feature in features_list:\n    sns.boxplot(x='Agglomerative_Clustering', y=feature, data=store)\n    plt.show()","329ae337":"from sklearn.cluster import DBSCAN\nX3 = StandardScaler().fit_transform(X3)\n\ndbscan = DBSCAN(eps=0.3, min_samples=5, metric = 'euclidean')\ndbscan.fit(X3)","b8a4334a":"dbscan_labels = dbscan.labels_\n#Stampa delle etichette dei cluster\ndbscan_labels","12b7626a":"#Identificazione numero di cluster e punti rumorosi\nn_clusters_ = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\nn_noise_ = list(dbscan_labels).count(-1)\n\nprint('Numero di cluster stimati: %d' % n_clusters_)\nprint('Numero di punti rumorosi identificati: %d' % n_noise_)","676af665":"# Assegnazione delle etichette a ciascun esempio presente nel DataFrame\nstore['DensityBased_Labels'] = dbscan_labels\n# Stampa dei primi 5 elementi presenti nel DataFrame\nstore.head()","0596ad89":"#Plot delle Features\n\nfeatures_list = ['Age', 'Annual Income (k$)', 'Spending Score (1-100)']\n\nfor feature in features_list:\n    sns.boxplot(x='DensityBased_Labels', y=feature, data=store)\n    plt.show()","8daba54c":"### BoxPlot ottenuti con k-Means","49cd64e7":"Al fine di ottenere una descrizione complessiva del Dataframe (e dunque del relativo dataset) caricato, mediante il metodo _info()_ si sono ottenute le seguenti informazioni:\n- <b>#<\/b>: numero di feature presente nel DataFrame\n- <b>Column<\/b>: intestazione delle features nel DataFrame\n- <b>Non-Null Count<\/b>: contatore di valori non nulli per ogni feature presente nel DataFrame\n- <b>Dtype<\/b>: tipo di dato memorizzato per ogni feature presente nel DataFrame","22c77e50":"# Esercitazione su k-Means e Hierarchical Clustering","4339a1a9":"## Visualizzazione dei dati","d358e132":"### CountPlot per Genere","762eb9dd":"Ottenuti i risultati visibili dal precedente grafico, si sceglie il valore 5 per il parametro _n_clusters_ e si riesegue nuovamente l'algoritmo k-Means registrando le relative etichette.","11797b0b":"### Studio delle features disponibili","e1556cb0":"### kMeans++\nNel k-means classico, si utilizza un seme casuale per posizionare i centroidi iniziali, che a volte pu\u00f2 provocare cattivi raggruppamenti o una lenta convergenza se i centroidi iniziali sono scelti male. Un modo per risolvere questo problema \u00e8 eseguire l'algoritmo k-mean pi\u00f9 volte su un set di dati e scegliere il modello con le migliori prestazioni in termini di SSE.\n\nUn'altra strategia \u00e8 quella di posizionare i centroidi iniziali molto distanti tra loro tramite l'algoritmo k-means ++, che porta a risultati migliori e pi\u00f9 coerenti rispetto ai classici k-mean \n\nPer utilizzare il k-Means++ baster\u00e0 porre l'attributo init = 'k-means++' (che \u00e8 gi\u00e0 posto di default). Per utilizzare il k-Means classico bisogner\u00e0 porre l'attributo init = 'random'.","5fbae530":"## Descrizione ed analisi del dataset\nIl dataset che verr\u00e0 utilizzato tratta informazioni circa la fedelt\u00e0 dei clienti che hanno sottoscritto una tessera punti. \nTali informazioni raccolte potrebbero ritornare utili, ai fini di business, per pianificare potenziali strategie economiche ed attuare delle promozioni mirate in base alla propria clientela.\n\nLe features presenti nel dataset sono le seguenti:\n- <b>CustomerID<\/b>: Codice univoco identificativo del cliente.\n- <b>Gender<\/b>: Sesso dell'utente.\n- <b>Age<\/b>: Et\u00e0 dell'utente.\n- <b>Annual Income (k$)<\/b>: Spesa annuale dell'utente fatta presso il centro commerciale di riferimento\n- <b>Spending Score (1-100)<\/b>: Punteggio assegnato dal centro commerciale in base al comportamento del cliente e alla natura della spesa","8c39c001":"### Agglomerative Clustering","e335edab":"## Indice contenuti\n- [Obiettivo esercitazione](#Obiettivo-esercitazione)\n- [Descrizione ed analisi del dataset](#Descrizione-ed-analisi-del-dataset)\n- [Analisi esplorativa del dataset](#Analisi-esplorativa-del-dataset)\n    - [Caricamento in memoria del dataset](#Caricamento-in-memoria-del-dataset)\n    - [Pulizia del dataset](#Pulizia-del-dataset)\n- [Visualizzazione dei dati](#Visualizzazione-dei-dati)\n    - [Istogramma](#Istogramma)\n    - [CountPlot per Genere](#CountPlot-per-Genere)\n    - [Studio delle features disponibili](#Studio-delle-features-disponibili)\n- [Clustering via k-Means](#Clustering-via-k-Means)\n    - [k-Means++](#kMeans++)\n    - [Segmentazione dei clienti Age vs Spending Score](#Segmentazione-dei-clienti-Age-vs-Spending-Score)\n    - [Trovare il numero ottimale di clusters](#Trovare-il-numero-ottimale-di-clusters)\n    - [Plot delle regioni identificate](#Plot-delle-regioni-identificate)\n    - [Segmentazione dei clienti Annual Income vs Spending Score](#Segmentazione-dei-clienti-Annual-Income-vs-Spending-Score)\n    - [Plot n2 delle regioni identificate](#Plot-n2-delle-regioni-identificate)\n    - [Segmentazione dei clienti Age vs Annual Income vs Spending Score](#Segmentazione-dei-clienti-Age-vs-Annual-Income-vs-Spending-Score)\n        - [Analisi di Silhouette](#Analisi-di-Silhouette)\n    - [BoxPlot ottenuti con k-Means](BoxPlot-ottenuti-con-k-Means)\n- [Clustering gerarchico](#Clustering-gerarchico)\n    - [Agglomerative clustering](#Agglomerative-Clustering)\n- [DBSCAN](#DBSCAN)\n<hr>","83d57fd5":"**Average Linkage:<br>**\n\nCon il metodo _Average Linkage_, la distanza tra due cluster \u00e8 definita come la distanza media presente tra ciascun punto di un cluster con tutti i punti dell'altro cluster.\n\nPer esempio, la distanza tra i cluster \u201cr\u201d e \u201cs\u201d \u00e8 uguale alla lunghezza mediata dell'arco che connette i punti di un cluster all'altro.\n\n![](https:\/\/www.saedsayad.com\/images\/Clustering_average.png)","554f11de":"## Obiettivo esercitazione\nL'esercitazione ha l'obiettivo di applicare su un dataset reale i differenti algoritmi di clustering, in particolare k-Means e Hierarchical Clustering.\n\nSi effettueranno, inoltre, differenti variazioni all'applicazione standard degli algoritmi per comprendere l'utilizzo dei differenti iper-parametri a seconda delle documentazioni ufficiali dei metodi utilizzati.","b0ffd288":"### Plot delle regioni identificate","ca91fa68":"Successivamente, al fine di comprendere le dimensioni (in termini di esempi e di features a disposizione), mediante apposito attributo si stampano il numero di righe e di colonne del DataFrame:","e7994da7":"**Single Linkage:<br>**\n\nNel clustering che sfrutta la modalit\u00e0 di collegamento _single linkage_, la distanza tra due cluster \u00e8 definita come la pi\u00f9 piccola distanza calcolabile tra due punti in ciascun cluster. Per esempio, la distanza tra il cluster \u201cr\u201d e \u201cs\u201d \u00e8 uguale alla lunghezza dell'arco tra i due punti pi\u00f9 vicini, cos\u00ec come visibile dalla figura riportata.\n\n![](https:\/\/www.saedsayad.com\/images\/Clustering_single.png)","8620cafe":"Ottenuti i risultati visibili dal precedente grafico, si sceglie il valore 6 per il parametro _n_clusters_ e si riesegue nuovamente l'algoritmo k-Means registrando le relative etichette.","441026a2":"## Segmentazione dei clienti Age vs Spending Score","a92751fd":"Per ottenere informazioni statistiche inerenti ciascuna feature a disposizione, mediante il metodo _describe()_ si \u00e8 provveduto al calcolo delle seguenti informazioni:\n- <b>count<\/b>: conteggio del numero di esempi per la feature selezionata\n- <b>mean<\/b>: media aritmetica per la feature selezionata\n- <b>std<\/b>: deviazione standard per la feature selezionata\n- <b>min<\/b>: valore minimo presentato dagli esempi per la feature selezionata\n- <b>25%<\/b>: primo quartile calcolato sugli esempi per la feature selezionata\n- <b>50%<\/b>: secondo quartile calcolato sugli esempi per la feature selezionata\n- <b>75%<\/b>: terzo quartile calcolato sugli esempi per la feature selezionata\n- <b>max<\/b>: valore massimo presentato dagli esempi per la feature selezionata","391701b8":"### Caricamento in memoria del dataset","5c8dcfa4":"## Segmentazione dei clienti Annual Income vs Spending Score","0d085b9d":"## Clustering via k-Means\n\nk-Means \u00e8 un algoritmo di clustering non supervisionato, tra i pi\u00f9 semplici e popolari messi a disposizione dalla libreria _Sci-Kit_.\n\nUn cluster \u00e8 definito come un insieme di punti dati che il clustering \u00e8 uno degli algoritmi di apprendimento automatico non supervisionati pi\u00f9 semplici e popolari.\n\nDefinito il valore del parametro k, che esplica il numero di centroidi da identificare nel dataset. Un centroide \u00e8 la posizione immaginaria o reale che rappresenta il centro di ciascun cluster.\n\nL'algoritmo prevede l'assegnazione di ogni punto dati viene a ciascuno dei cluster utilizzando la nozione di distanza. In altre parole, l'algoritmo k-Means identifica il numero k di centroidi e quindi assegna ogni punto dati al cluster pi\u00f9 vicino, mantenendo i centroidi i pi\u00f9 piccoli possibili.\n\nPer clusterizzare i dati presenti nel dataset, l'algoritmo k-Means identifica randomicamente un primo gruppo di centroidi e tali sono utilizzati come punti iniziali per ogni cluster. Successivamente, si effettua il ricalcolo dei centroidi ogni qualvolta un nuovo esempio \u00e8 assegnato al cluster, al fine di ottimizzare le posizioni dei centroidi.\nIl processo di ottimizzazione termina quando si raggiunge il numero delle iterazioni massime (definite) oppure quando si \u00e8 giunti alla convergenza del metodo.","c83dead4":"### Trovare il numero ottimale di clusters","f64adda4":"### Istogramma","8adbacc8":"Nel clustering agglomerativo, come gi\u00e0 accennato, i singoli punti dati vengono agglomerati iterativamente in cluster di livello superiore.\nNel primo step, ogni singolo punto costituisce un cluster. Successivamente, si agglomerano insieme via via sempre pi\u00f9 punti, andando a costruire cluster sempre pi\u00f9 popolati.\nIl metodo si arresta quando si raggiunge un certo numero di cluster.\nNel metodo seguente vengono utilizzati i seguenti parametri:\n- <b>n_clusters=3<\/b>: si desiderano tre cluster come suggerito dal metodo Elbow\n- <b>affinity<\/b>: metrica utilizzata per computare il linkage. Si utilizza la distanza euclidea.\n    - <b>euclidean<\/b>\n    - <b>l1<\/b>\n    - <b>l2<\/b>\n    - <b>manhattan<\/b>\n    - <b>cosine<\/b>\n    - <b>precomputed<\/b>\n- <b>linkage<\/b>: criterio di collegamento da utilizzare. Ne esistono diversi:\n    - <b>ward<\/b>: minimizza la varianza dei cluster che devono essere fusi insieme\n    - <b>average<\/b>: usa la media delle distanze di ogni osservazione nei due insiemi\n    - <b>complete<\/b>: usa la distanza massima tra due punti negli insiemi\n    - <b>single<\/b>: usa la distanza minima tra due insiemi","90b6603c":"Ottenuti i risultati visibili dal precedente grafico, si sceglie il valore 4 per il parametro _n_clusters_ e si riesegue nuovamente l'algoritmo k-Means registrando le relative etichette.","27ef6323":"## Analisi esplorativa del dataset","b0fb1bb1":"#### Analisi di Silhouette\n\nL'analisi di Silhouette si riferisce a un metodo di interpretazione e convalida della coerenza dei dati rispetto ai cluster identificati.\n\nIl valore dek coefficiente di Silhouette \u00e8 una misura di quanto un oggetto sia simile al proprio cluster (coesione) rispetto ad altri cluster (separazione). Tale valore \u00e8 espresso in un intervallo [-1, +1], dove un valore alto indica che l'esempio \u00e8 ben adattato al proprio cluster e scarsamente abbinato ai cluster vicini. Se la maggior parte degli oggetti ha un valore elevato, la suddivisione degli esempi nei rispettivi cluster \u00e8 appropriata. Se molti punti, invece, hanno un valore basso o negativo, la suddivisione degli esempi nei cluster definiti potrebbe risultare inappropriata.\n\nL'analisi di Silhouette pu\u00f2 essere condotta utilizzando una qualsiasi metrica di distanza, come la distanza euclidea o la distanza di Manhattan.\nIn particolare, pu\u00f2 essere espressa come segue:\n$$\\text{silhouette score}=\\frac{p-q}{max(p,q)}$$\n\n$p$ \u00e8 la distanza media tra il punto e il centroide del cluster pi\u00f9 vicino.\n\n$q$ \u00e8 la distanza media intra-cluster definita su tutti i punti presenti nel proprio cluster.\n","6485d483":"## Clustering gerarchico\n\nGli algoritmi gerarchici in genere clusterizzano i dati usando le misure di distanza. Tuttavia, l'uso delle funzioni di distanza non \u00e8 obbligatorio. Molti algoritmi gerarchici utilizzano altri metodi di clustering, ad esempio metodi density-based o graph-based, come subroutine per la costruzione della gerarchia.\n\nUno dei motivi principali di utiulizzo di tale modalit\u00e0 di clustering \u00e8 che diversi livelli di granularit\u00e0 del clustering forniscono dei dettagli specifici per l'applicazione. Ci\u00f2 fornisce una tassonomia di cluster, che possono essere esplorati sulla base di tali dettagli semantici.\n\nL'organizzazione gerarchica consente la navigazione manuale molto conveniente per un utente, specialmente quando il contenuto dei cluster pu\u00f2 essere descritto in modo semanticamente comprensibile. In altri casi, tali organizzazioni gerarchiche possono essere utilizzate dagli algoritmi di indicizzazione, rispetto alle macroaree di riferimento.\nInoltre, tali metodi possono talvolta essere utilizzati anche per creare cluster \"piatti\" migliori (dove tutte le categorie sono posto allo stesso livello). Alcuni metodi gerarchici agglomerativi e metodi di divisione, possono fornire cluster di qualit\u00e0 migliore rispetto ai metodi di partizionamento come k-Means, sebbene con un costo computazionale pi\u00f9 elevato.\n\nEsistono due tipi di algoritmi gerarchici, a seconda di come viene costruito l'albero gerarchico dei cluster:\n- Metodi bottom-up (agglomerativi): i singoli punti dati vengono successivamente agglomerati in cluster di livello superiore. La principale variazione tra i diversi metodi \u00e8 nella scelta della funzione obiettivo utilizzata per fondere i cluster.\n- Metodi top-down (divisivi): un approccio top-down viene utilizzato per partizionare successivamente i punti in una struttura ad albero. Un algoritmo di clustering piatto pu\u00f2 essere utilizzato per il partizionamento in un determinato passo. Tale approccio offre un'enorme flessibilit\u00e0 in termini di scelta del compromesso tra l'equilibrio nella struttura ad albero e l'equilibrio nel numero di punti in ciascun nodo della struttura ad albero.","905a9cff":"#### Metodo Elbow\nAl fine di identificare il giusto numero per il parametro _n_clusters_ \u00e8 possibile definire un metodo grafico che consenta, variando il parametro mediante una lista di valori espressi, di poter valutare l'attributo _intertia_ (ovvero la somma della radice delle distanze dei campioni dal centro del cluster pi\u00f9 vicino).","56954b38":"#### Taglio del Dendrogramma in base al valore di K","d314f869":"## Segmentazione dei clienti Age vs Annual Income vs Spending Score","dced0a1d":"### Plot n2 delle regioni identificate","9910f873":"Con il seguente comando si effettua il caricamento in memoria di quanto contenuto nel dataset _'OnlineRetail.csv'_.\n\nPer condurre una prima fase di analisi esplorativa e comprendere la natura dei dati a disposizione, si stampano di seguito i primi cinque esempi presenti nel dataset:","d58ffa00":"**Complete Linkage<br>**\n\nNel metodo _Complete Linkage_, la distanza tra due cluster \u00e8 definita come la pi\u00f9 grande distanza tra due punti in ciascun cluster.\n\nPer esempio, la distanza tra i cluster \u201cr\u201d e \u201cs\u201d \u00e8 uguale alla lunghrzza dell'arco tra i due punti pi\u00f9 distanti dei due cluster.\n\n![](https:\/\/www.saedsayad.com\/images\/Clustering_complete.png)","6745649b":"### DBSCAN\n\nDBSCAN \u00e8 un algoritmo di clustering Density-Based utilizzabile su dataset che presentano punti rumorosi. \u00c8 un algoritmo non parametrico di clustering basato sulla densit\u00e0: dato un insieme di punti in uno spazio, raggruppa i punti che sono altamente vicini, contrassegnando come punti anomali i punti che si trovano da soli in regioni a bassa densit\u00e0. \n\nDBSCAN \u00e8 uno degli algoritmi di clustering pi\u00f9 comuni e anche i pi\u00f9 citati nella letteratura scientifica e presenta i seguenti vantaggi:\n- Non richiede la specifica a priori di un numero di cluster, a differenza di k-Means\n- Gestione accurata dei punti rumorosi\n- Robusto in presenza degli outliers\n\nPer DBSCAN, invece, si identificano i seguenti svantaggi:\n- Non deterministico: i punti presenti sulle frontiere possono essere assegnati a cluster differenti, in base all'ordine in cui i dati sono processati\n- La qualit\u00e0 dei risultati restituiti da DBSCAN dipende dalla misura di distanza usata\n- Sensibile al fenomeno della \"Curse of dimensionality\" in presenza di dataset con un numero di features elevato"}}