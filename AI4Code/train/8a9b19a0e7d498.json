{"cell_type":{"b36a15b9":"code","49fe5024":"code","d3d49a64":"code","37359a52":"code","3e671f33":"code","7125f897":"code","5f1c0078":"code","78f36f60":"code","d9d8135e":"code","16cbe2f3":"code","b32fbcd4":"code","adba7da9":"code","337fa939":"code","2576a6dc":"code","b9c6d55c":"code","662ca067":"code","1301d45d":"code","2921df5e":"code","e96ae965":"code","7bf1411f":"code","04a7b44e":"code","3a78f522":"code","d58dd1fa":"code","c8859ca9":"code","a2596e6d":"code","c2501487":"code","d0062f97":"code","a05e89a1":"code","00f91248":"code","644aedd6":"code","ecdabe77":"code","1310ee39":"code","6559c40e":"code","cdcb3c25":"code","20f77088":"code","46685250":"code","cb76de8c":"code","25c1e7f8":"code","7652eef2":"markdown","e59f25b3":"markdown","7a903e82":"markdown","caa49de6":"markdown","6a8ed9f8":"markdown","1e713cd5":"markdown","394dc5d7":"markdown","04f84306":"markdown","d270727d":"markdown","a6b8d925":"markdown","7dcf06e5":"markdown","6037be78":"markdown","cadd7e41":"markdown","903b36cc":"markdown","56663068":"markdown","5e0b796c":"markdown","c58cd4f1":"markdown","d29405fb":"markdown","e2610043":"markdown"},"source":{"b36a15b9":"from IPython.display import Image\nimport os\nImage(\"..\/input\/diamondimages\/Introduction.PNG\")","49fe5024":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d3d49a64":"# Loading Libraries\nimport pandas as pd # for data analysis\nimport numpy as np # for scientific calculation\nimport seaborn as sns # for statistical plotting\nimport matplotlib.pyplot as plt # for plotting\n%matplotlib inline","37359a52":"#Reading Diamond Price Prediction data set.\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/diamond\/diamonds_prediction.csv'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","3e671f33":"diamondprediction_eda=pd.read_csv('\/kaggle\/input\/diamond\/diamonds_prediction.csv')","7125f897":"diamondprediction_eda.describe()","5f1c0078":"print(diamondprediction_eda.shape)\nprint(diamondprediction_eda.head(2))\nprint(diamondprediction_eda.info())","78f36f60":"# Identify Duplicate Records\nduplicate_records = diamondprediction_eda[diamondprediction_eda.duplicated()]\nprint(\"Duplicate Rows except first occurrence based on all columns are :\")\nprint(len(duplicate_records))\nprint(diamondprediction_eda.shape)\nprint(duplicate_records.head(2))","d9d8135e":"# Missing Data Percentage\ntotal = diamondprediction_eda.isnull().sum().sort_values(ascending=False)\npercent = (diamondprediction_eda.isnull().sum()\/diamondprediction_eda.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nf, ax = plt.subplots(figsize=(15, 6))\nplt.xticks(rotation='90')\nsns.barplot(x=missing_data.index, y=missing_data['Percent'])\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of Missing Values', fontsize=15)\nplt.title('Percentage of Missing Data by Feature', fontsize=15)\nmissing_data.head()","16cbe2f3":"diamondprediction_eda.isnull().sum()","b32fbcd4":"# Analysis of Non-numerical columns\ndiamondprediction_eda.describe(include=['O'])","adba7da9":"# Cut Types\n# cut quality of the cut (Fair, Good, Very Good, Premium, Ideal)\nprint(diamondprediction_eda['cut'].unique())\nprint(\"Type of Cuts: %s\" % (diamondprediction_eda['cut'].nunique()))\nprint(diamondprediction_eda['cut'].value_counts())","337fa939":"# Color Types\n# color diamond colour, from J (worst) to D (best)\nprint(diamondprediction_eda['color'].unique())\nprint(\"Type of Color: %s\" % (diamondprediction_eda['color'].nunique()))\nprint(diamondprediction_eda['color'].value_counts())","2576a6dc":"# Clarity Types\n# clarity a measurement of how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))\nprint(diamondprediction_eda['clarity'].unique())\nprint(\"Type of Clarity: %s\" % (diamondprediction_eda['clarity'].nunique()))\nprint(diamondprediction_eda['clarity'].value_counts())","b9c6d55c":"# Remove unwanted column\nprint(diamondprediction_eda.head(2))\ndiamondprediction_eda = diamondprediction_eda.drop(['Unnamed: 0'], axis =1)\nprint(diamondprediction_eda.head(2))","662ca067":"# Count plot for categorical variables.\nplt.rcParams['figure.figsize'] = (10.0, 5.0)\nplt.rcParams['font.family'] = \"serif\"\nfig, ax =plt.subplots(2,2)\nsns.countplot(diamondprediction_eda['cut'], ax=ax[0,0])\nsns.countplot(diamondprediction_eda['color'], ax=ax[0,1])\nsns.countplot(diamondprediction_eda['cut'], ax=ax[1,0])\nsns.countplot(diamondprediction_eda['clarity'], ax=ax[1,1])\nfig.show();","1301d45d":"# Mean Encoding technique has been choosen to ensure better encoding amoung other technique's based on the above plots.\n# Verified the top 2 sample data after mean-encoding technique.\nmean_encode_cut = diamondprediction_eda.groupby('cut')['price'].mean()\nmean_encode_color = diamondprediction_eda.groupby('color')['price'].mean()\nmean_encode_clarity = diamondprediction_eda.groupby('clarity')['price'].mean()\ndiamondprediction_eda.loc[:,'cut_mean_enc']=diamondprediction_eda['cut'].map(mean_encode_cut)\ndiamondprediction_eda.loc[:,'color_mean_enc']=diamondprediction_eda['color'].map(mean_encode_color)\ndiamondprediction_eda.loc[:,'clarity_mean_enc']=diamondprediction_eda['clarity'].map(mean_encode_clarity)\ndiamondprediction_eda.head(2)","2921df5e":"# Drop categorical variables\ndiamondprediction_eda=diamondprediction_eda.drop(['cut','color','clarity'],axis=1)\ndiamondprediction_eda.rename(columns = {'cut_mean_enc':'cut'}, inplace = True)\ndiamondprediction_eda.rename(columns = {'color_mean_enc':'color'}, inplace = True)\ndiamondprediction_eda.rename(columns = {'clarity_mean_enc':'clarity'}, inplace = True)\ndiamondprediction_eda.head(2)","e96ae965":"diamondprediction_eda.describe()","7bf1411f":"# Removing (statistical) outliers for dataset\nprint(diamondprediction_eda.shape)\nQ1 = diamondprediction_eda.quantile(0.25) #(0.19485)\n#print(Q1)\nQ3 = diamondprediction_eda.quantile(0.75) #(0.80515)\n#print(Q3)\nIQR = Q3 - Q1\n#print(IQR)\ndiamondprediction_eda_outliers = diamondprediction_eda[~((diamondprediction_eda < (Q1 - 1.5 * IQR)) |(diamondprediction_eda > (Q3 + 1.5 * IQR))).any(axis=1)]\nprint(diamondprediction_eda_outliers.shape)","04a7b44e":"# Distribution plot for output variable.\nfigure,ax=plt.subplots(nrows=1,ncols=2,figsize=(10,8))\nsns.distplot(diamondprediction_eda.price,ax=ax[0],kde=False)\nsns.distplot(diamondprediction_eda_outliers.price,ax=ax[1],kde=False)\nplt.show()\nplt.tight_layout();","3a78f522":"plt.boxplot(diamondprediction_eda['price'])\nplt.show()\nplt.boxplot(diamondprediction_eda_outliers['price'])\nplt.show();","d58dd1fa":"diamondprediction_eda_outliers.head(2)","c8859ca9":"# Generated HeatMap after removal of outliers and you can see the difference clearly here.\ncorr = diamondprediction_eda_outliers.corr()\nax = sns.heatmap( corr,vmin=-1, vmax=1, center=0,  cmap=sns.diverging_palette(20, 220, n=200),square=True)\nax.set_xticklabels(ax.get_xticklabels(),rotation=45,horizontalalignment='right');","a2596e6d":"X=diamondprediction_eda_outliers.drop(['price'],axis=1)\nY=diamondprediction_eda_outliers['price']\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.3,random_state=42)","c2501487":"from sklearn.linear_model import LinearRegression,Ridge,Lasso,ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor \nfrom sklearn.tree import DecisionTreeRegressor \nfrom sklearn.neighbors import KNeighborsRegressor\n\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.metrics import mean_squared_error, r2_score, accuracy_score\n","d0062f97":"R2_Accuracy = []\nmodels = ['Linear Regression', 'Lasso Regression','Ridge Regression','ElasticNet Regression','DecisionTree Regression','RandomForest Regression','KNeighbours Regression','AdaBoost Regression']","a05e89a1":"linear_reg = LinearRegression()\nlinear_reg.fit(X_train , Y_train)\ncross_validation_accuracy = cross_val_score(estimator = linear_reg, X = X_train, y = Y_train, cv = 5,verbose = 1)\ny_pred = linear_reg.predict(X_test)\nprint('\\nLinear Regression')\nprint('Accuracy Score : %.6f' % linear_reg.score(X_test, Y_test))\n\nprint('\\nCross Validation Accuracy')\nprint(cross_validation_accuracy)\n\nrmse = mean_squared_error(Y_test, y_pred)**0.5\nr2 = r2_score(Y_test, y_pred)\n\nprint('\\nMetrics')\nprint('RMSE   : %0.6f ' % rmse)\nprint('R2     : %0.6f ' % r2)\n\nR2_Accuracy.append(r2);\n\n# Putting together the coefficient and their corrsponding variable names  \nlinear_reg_coefficient = pd.DataFrame() \nlinear_reg_coefficient[\"Columns\"] = X_train.columns \nlinear_reg_coefficient['Coefficient Estimate'] = pd.Series(linear_reg.coef_) \nprint(linear_reg_coefficient) \n\n# Let\u2019s plot a bar chart of above coefficients using matplotlib plotting library.\n# plotting the coefficient score \nfig, ax = plt.subplots(figsize =(10, 5)) \n  \ncolor =['tab:gray', 'tab:blue', 'tab:orange',  \n'tab:green', 'tab:red', 'tab:purple', 'tab:brown',  \n'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan',  \n'tab:orange', 'tab:green', 'tab:blue', 'tab:olive'] \n  \nax.bar(linear_reg_coefficient[\"Columns\"],  \nlinear_reg_coefficient['Coefficient Estimate'],color = color) \n  \nax.spines['bottom'].set_position('zero') \n  \nplt.style.use('ggplot') \nplt.show();\n","00f91248":"lasso_reg = Lasso(alpha=1)\nlasso_reg.fit(X_train , Y_train)\ncross_validation_accuracy = cross_val_score(estimator = lasso_reg, X = X_train, y = Y_train, cv = 5,verbose = 1)\ny_pred = lasso_reg.predict(X_test)\n\nprint('\\nLasso Regression')\nprint('Accuracy Score : %.6f' % lasso_reg.score(X_test, Y_test))\n\nprint('\\nCross Validation Accuracy')\nprint(cross_validation_accuracy)\n\nrmse = mean_squared_error(Y_test, y_pred)**0.5\nr2 = r2_score(Y_test, y_pred)\n\nprint('\\nMetrics')\nprint('RMSE   : %0.6f ' % rmse)\nprint('R2     : %0.6f ' % r2)\n\nR2_Accuracy.append(r2);\n\n# Putting together the coefficient and their corrsponding variable names  \nlasso_reg_coefficient = pd.DataFrame() \nlasso_reg_coefficient[\"Columns\"] = X_train.columns \nlasso_reg_coefficient['Coefficient Estimate'] = pd.Series(lasso_reg.coef_) \nprint(lasso_reg_coefficient) \n\n# Let\u2019s plot a bar chart of above coefficients using matplotlib plotting library.\n# plotting the coefficient score \nfig, ax = plt.subplots(figsize =(10, 5)) \n  \ncolor =['tab:gray', 'tab:blue', 'tab:orange',  \n'tab:green', 'tab:red', 'tab:purple', 'tab:brown',  \n'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan',  \n'tab:orange', 'tab:green', 'tab:blue', 'tab:olive'] \n  \nax.bar(lasso_reg_coefficient[\"Columns\"],  \nlasso_reg_coefficient['Coefficient Estimate'],color = color) \n  \nax.spines['bottom'].set_position('zero') \n  \nplt.style.use('ggplot') \nplt.show(); ","644aedd6":"ridge_reg = Ridge(alpha=1)\nridge_reg.fit(X_train , Y_train)\ncross_validation_accuracy = cross_val_score(estimator = ridge_reg, X = X_train, y = Y_train, cv = 5,verbose = 1)\ny_pred = ridge_reg.predict(X_test)\n\nprint('\\nRidge Regression')\nprint('Accuracy Score : %.6f' % ridge_reg.score(X_test, Y_test))\n\nprint('\\nCross Validation Accuracy')\nprint(cross_validation_accuracy)\n\nrmse = mean_squared_error(Y_test, y_pred)**0.5\nr2 = r2_score(Y_test, y_pred)\n\nprint('\\nMetrics')\nprint('RMSE   : %0.6f ' % rmse)\nprint('R2     : %0.6f ' % r2)\n\nR2_Accuracy.append(r2);\n\n# Putting together the coefficient and their corrsponding variable names  \nridge_reg_coefficient = pd.DataFrame() \nridge_reg_coefficient[\"Columns\"] = X_train.columns \nridge_reg_coefficient['Coefficient Estimate'] = pd.Series(ridge_reg.coef_) \nprint(ridge_reg_coefficient) \n\n# Let\u2019s plot a bar chart of above coefficients using matplotlib plotting library.\n# plotting the coefficient score \nfig, ax = plt.subplots(figsize =(10, 5)) \n  \ncolor =['tab:gray', 'tab:blue', 'tab:orange',  \n'tab:green', 'tab:red', 'tab:purple', 'tab:brown',  \n'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan',  \n'tab:orange', 'tab:green', 'tab:blue', 'tab:olive'] \n  \nax.bar(ridge_reg_coefficient[\"Columns\"],  \nridge_reg_coefficient['Coefficient Estimate'],color = color) \n  \nax.spines['bottom'].set_position('zero') \n  \nplt.style.use('ggplot') \nplt.show(); ","ecdabe77":"elasticnet_reg = ElasticNet(alpha=1)\nelasticnet_reg.fit(X_train , Y_train)\ncross_validation_accuracy = cross_val_score(estimator = elasticnet_reg, X = X_train, y = Y_train, cv = 5,verbose = 1)\ny_pred = elasticnet_reg.predict(X_test)\n\nprint('\\nElasticNet Regression')\nprint('Accuracy Score : %.6f' % elasticnet_reg.score(X_test, Y_test))\n\nprint('\\nCross Validation Accuracy')\nprint(cross_validation_accuracy)\n\nrmse = mean_squared_error(Y_test, y_pred)**0.5\nr2 = r2_score(Y_test, y_pred)\n\nprint('\\nMetrics')\nprint('RMSE   : %0.6f ' % rmse)\nprint('R2     : %0.6f ' % r2)\n\nR2_Accuracy.append(r2);\n\n# Putting together the coefficient and their corrsponding variable names  \nelasticnet_reg_coefficient = pd.DataFrame() \nelasticnet_reg_coefficient[\"Columns\"] = X_train.columns \nelasticnet_reg_coefficient['Coefficient Estimate'] = pd.Series(elasticnet_reg.coef_) \nprint(elasticnet_reg_coefficient) \n\n# Let\u2019s plot a bar chart of above coefficients using matplotlib plotting library.\n# plotting the coefficient score \nfig, ax = plt.subplots(figsize =(10, 5)) \n  \ncolor =['tab:gray', 'tab:blue', 'tab:orange',  \n'tab:green', 'tab:red', 'tab:purple', 'tab:brown',  \n'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan',  \n'tab:orange', 'tab:green', 'tab:blue', 'tab:olive'] \n  \nax.bar(elasticnet_reg_coefficient[\"Columns\"],  \nelasticnet_reg_coefficient['Coefficient Estimate'],color = color) \n  \nax.spines['bottom'].set_position('zero') \n  \nplt.style.use('ggplot') \nplt.show();","1310ee39":"dt_reg = DecisionTreeRegressor(random_state = 0)\ndt_reg.fit(X_train , Y_train)\ncross_validation_accuracy = cross_val_score(estimator = dt_reg, X = X_train, y = Y_train, cv = 5,verbose = 1)\ny_pred = dt_reg.predict(X_test)\n\nprint('\\nDecision Tree Regression')\nprint('Accuracy Score : %.6f' % dt_reg.score(X_test, Y_test))\n\nprint('\\nCross Validation Accuracy')\nprint(cross_validation_accuracy)\n\nrmse = mean_squared_error(Y_test, y_pred)**0.5\nr2 = r2_score(Y_test, y_pred)\n\nprint('\\nMetrics')\nprint('RMSE   : %0.6f ' % rmse)\nprint('R2     : %0.6f ' % r2)\n\nR2_Accuracy.append(r2);\n\n","6559c40e":"rf_reg = RandomForestRegressor()\nrf_reg.fit(X_train , Y_train)\ncross_validation_accuracy = cross_val_score(estimator = rf_reg, X = X_train, y = Y_train, cv = 5,verbose = 1)\ny_pred = rf_reg.predict(X_test)\n\nprint('\\nRandom Forest Regression')\nprint('Accuracy Score : %.6f' % rf_reg.score(X_test, Y_test))\n\nprint('\\nCross Validation Accuracy')\nprint(cross_validation_accuracy)\n\nrmse = mean_squared_error(Y_test, y_pred)**0.5\nr2 = r2_score(Y_test, y_pred)\n\nprint('\\nMetrics')\nprint('RMSE   : %0.6f ' % rmse)\nprint('R2     : %0.6f ' % r2)\n\nR2_Accuracy.append(r2);","cdcb3c25":"knn_reg = KNeighborsRegressor()\nknn_reg.fit(X_train , Y_train)\ncross_validation_accuracy = cross_val_score(estimator = knn_reg, X = X_train, y = Y_train, cv = 5,verbose = 1)\ny_pred = knn_reg.predict(X_test)\n\nprint('\\nKNeighbor Regression')\nprint('Accuracy Score : %.6f' % knn_reg.score(X_test, Y_test))\n\nprint('\\nCross Validation Accuracy')\nprint(cross_validation_accuracy)\n\nrmse = mean_squared_error(Y_test, y_pred)**0.5\nr2 = r2_score(Y_test, y_pred)\n\nprint('\\nMetrics')\nprint('RMSE   : %0.6f ' % rmse)\nprint('R2     : %0.6f ' % r2)\n\nR2_Accuracy.append(r2);","20f77088":"ada_reg = AdaBoostRegressor(n_estimators=500)\nada_reg.fit(X_train , Y_train)\ncross_validation_accuracy = cross_val_score(estimator = ada_reg, X = X_train, y = Y_train, cv = 5,verbose = 1)\ny_pred = ada_reg.predict(X_test)\n\nprint('\\nAdaBoost Regressor')\nprint('Accuracy Score : %.6f' % ada_reg.score(X_test, Y_test))\n\nprint('\\nCross Validation Accuracy')\nprint(cross_validation_accuracy)\n\nrmse = mean_squared_error(Y_test, y_pred)**0.5\nr2 = r2_score(Y_test, y_pred)\n\nprint('\\nMetrics')\nprint('RMSE   : %0.6f ' % rmse)\nprint('R2     : %0.6f ' % r2)\n\nR2_Accuracy.append(r2);","46685250":"compare = pd.DataFrame({'Algorithms' : models , 'R2_Accuracy' : R2_Accuracy})\ncompare.sort_values(by='R2_Accuracy' ,ascending=False)","cb76de8c":"sns.factorplot(x='Algorithms', y='R2_Accuracy' , data=compare, size=6 , aspect=4);","25c1e7f8":"from IPython.display import Image\nimport os\n#!ls ..\/input\/\nImage(\"..\/input\/diamondimages\/Output.PNG\")","7652eef2":"* [Table of Contents](#Tableofcontents)\n<a id=\"AccuracySummaryGraph\"><\/a>\n\n## Accuary Summary Graph for all ML Algorithms","e59f25b3":"* [Table of Contents](#Tableofcontents)\n<a id=\"Conclusion\"><\/a>\n\n## Conclusion\n\n#### When compare to other regression algorithms, Here, RandomForest Regression Algorithms is predict prices with around 98.2% accuracy and best fit for this problem statement. ","7a903e82":"* [Table of Contents](#Tableofcontents)\n<a id=\"KNNRegression\"><\/a>\n\n## KNeighbor Regression","caa49de6":"* [Table of Contents](#Tableofcontents)\n<a id=\"ElasticnetRegression\"><\/a>\n\n## ElasticNet Regression","6a8ed9f8":"* [Table of Contents](#Tableofcontents)\n<a id=\"Importingdatasetandpackages\"><\/a>\n\n## Importing dataset and packages","1e713cd5":"* [Table of Contents](#Tableofcontents)\n<a id=\"RidgeRegression\"><\/a>\n\n## Ridge Regression","394dc5d7":"* [Table of Contents](#Tableofcontents)\n<a id=\"LassoRegression\"><\/a>\n\n## Lasso Regression","04f84306":"* [Table of Contents](#Tableofcontents)\n<a id=\"Introduction\"><\/a>\n\n## Introduction\n\n### Project Name: Diamond price prediction\n\n#### Objective: Predict the price of the diamond using other attributes.\n\nThis dataset contains prices and other attributes of almost 54,000 diamonds.\nThe description of data are as follows:\n1. Price- price in US dollars (\\$326--\\$18,823)\n1. Carat- weight of the diamond (0.2--5.01)\n1. Cut- quality of the cut (Fair, Good, Very Good, Premium, Ideal)\n1. Color- diamond colour, from J (worst) to D (best)\n1. Clarity- a measurement of how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2,\n1. VVS1, IF (best))\n1. x- length in mm (0--10.74)\n1. y- width in mm (0--58.9)\n1. z- depth in mm (0--31.8)\n1. depth- total depth percentage = z \/ mean(x, y) = 2 * z \/ (x + y) (43--79)\n1. table- width of top of diamond relative to widest point (43--95)","d270727d":"<a id=\"Tableofcontents\"><\/a>\n# Table of Contents\n\n* [Introduction](#Introduction)\n* [Importing dataset and packages](#Importingdatasetandpackages)\n* [Exploratory Data Analysis](#ExploratoryDataAnalysis)\n* [Feature Engineering](#FeatureEngineering)\n* [MachineLearning Model Building](#MLModelBuilding)\n* [Linear Regression](#LinearRegression)\n* [Lasso Regression](#LassoRegression)\n* [Ridge Regression](#RidgeRegression)\n* [Elastic Net Regression](#ElasticnetRegression)\n* [Decision Tree Regression](#DTRegression)\n* [Random Forest Regression](#RFRegression)\n* [KNeighbor Regression](#KNNRegression)\n* [AdaBoost Regression](#AdaBoostRegression)\n* [Accuary Summary Table for all ML Algorithms](#AccuracySummaryTable)\n* [Accuary Summary Graph for all ML Algorithms](#AccuracySummaryGraph)\n* [KNeighbor Regression](#KNNRegression)\n* [Conclusion](#Conclusion)\n* [References](#References)","a6b8d925":"* [Table of Contents](#Tableofcontents)\n<a id=\"ExploratoryDataAnalysis\"><\/a>\n\n## Exploratory Data Analysis\n","7dcf06e5":"* [Table of Contents](#Tableofcontents)\n<a id=\"LinearRegression\"><\/a>\n\n## Linear Regression","6037be78":"* [Table of Contents](#Tableofcontents)\n<a id=\"MLModelBuilding\"><\/a>\n\n## MachineLearning Model Building","cadd7e41":"* [Table of Contents](#Tableofcontents)\n<a id=\"AccuracySummaryTable\"><\/a>\n\n## Accuary Summary Table for all ML Algorithms","903b36cc":"* [Table of Contents](#Tableofcontents)\n<a id=\"DTRegression\"><\/a>\n\n## Decision Tree Regression","56663068":"## Main intension of developing this project is to explore ML Regression Algorithms. \n\nHappy Learning!!!","5e0b796c":"* [Table of Contents](#Tableofcontents)\n<a id=\"RFRegression\"><\/a>\n\n## Random Forest Regression","c58cd4f1":"* [Table of Contents](#Tableofcontents)\n<a id=\"References\"><\/a>\n\n## References\n#### Special thanks to Chinmay Rane\n\n*  https:\/\/www.geeksforgeeks.org\/implementation-of-lasso-ridge-and-elastic-net\/\n*  https:\/\/www.geeksforgeeks.org\/python-decision-tree-regression-using-sklearn\/\n*  https:\/\/www.datatechnotes.com\/2019\/08\/elasticnet-regression-example-in-python.html\n*  https:\/\/www.kaggle.com\/fuzzywizard\/diamonds-in-depth-analysis","d29405fb":"* [Table of Contents](#Tableofcontents)\n<a id=\"AdaBoostRegression\"><\/a>\n\n## AdaBoost Regression","e2610043":"* [Table of Contents](#Tableofcontents)\n<a id=\"FeatureEngineering\"><\/a>\n\n## Feature Engineering"}}