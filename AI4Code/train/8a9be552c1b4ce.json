{"cell_type":{"0a68a97f":"code","4d8cb4b3":"code","419ace78":"code","51c8ee9a":"code","bdfecb7b":"code","60a09162":"code","7ba888f0":"code","fe2d4c3f":"code","77bc97fa":"code","4d4ac08e":"code","92056ec4":"code","135260f0":"code","1ccfd75e":"code","f0bea6ba":"code","779ae52f":"code","2fc4e4ec":"code","df397dd4":"code","eb37c8f8":"code","cbe2c11b":"code","7406acf5":"code","4e3efc3f":"code","51808286":"markdown","42fb75c2":"markdown"},"source":{"0a68a97f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4d8cb4b3":"train = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/test.csv')\nsub = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/sample_submission.csv')","419ace78":"# train.head()","51c8ee9a":"# test.head()","bdfecb7b":"# sub.head()","60a09162":"# train_cols = []\n\n# for data in train.columns:\n#     if train[data].dtype == 'float':\n#         # Scale 10\n#         train_cols.append(\n#             pd.DataFrame(columns=['{}_scale_10'.format(data)], \n#                          data=np.array(np.round(train[data] * 10), dtype='int')))\n\n#         # Scale 100\n#         train_cols.append(\n#             pd.DataFrame(columns=['{}_scale_100'.format(data)], \n#                          data=np.array(np.round(train[data] * 100), dtype='int')))","7ba888f0":"# # Concating data train\n# train_data_scale = pd.concat(train_cols, axis=1)","fe2d4c3f":"# # Train scale\n# train_scale = pd.DataFrame(train_data_scale)","77bc97fa":"# train_full = pd.merge(train, train_scale, left_index=True, right_index=True)","4d4ac08e":"# test_cols = []\n\n# for data in test.columns:\n#     if test[data].dtype == 'float':\n#         # Scale 10\n#         test_cols.append(\n#             pd.DataFrame(columns=['{}_scale_10'.format(data)], \n#                          data=np.array(np.round(test[data] * 10), dtype='int')))\n\n#         # Scale 100\n#         test_cols.append(\n#             pd.DataFrame(columns=['{}_scale_100'.format(data)], \n#                          data=np.array(np.round(test[data] * 100), dtype='int')))","92056ec4":"# # Concating data test\n# test_data_scale = pd.concat(test_cols, axis=1)","135260f0":"# # Train scale\n# test_scale = pd.DataFrame(test_data_scale)","1ccfd75e":"# test_full = pd.merge(test, test_scale, left_index=True, right_index=True)","f0bea6ba":"# X = train_full.drop(['id', 'loss'], axis=1)\n# y = train_full['loss']\n\n# testX = test_full.drop(['id'], axis=1)","779ae52f":"X = train.drop(['id', 'loss'], axis=1)\ny = train['loss']\n\ntestX = test.drop(['id'], axis=1)","2fc4e4ec":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error","df397dd4":"SEED = 99\n\nnp.random.seed(SEED)","eb37c8f8":"# Hyperparameter\n# xgb_params = {\n#     'n_estimators': 10000,\n#     'learning_rate': 0.35,\n#     'objective': 'reg:squarederror',\n#     'subsample': 0.8031450486786944,\n#     'colsample_bytree': 0.84,\n#     'max_depth': 3,\n#     'booster': 'gbtree', \n#     'reg_lambda': 35.1,\n#     'reg_alpha': 14.68267919457715,\n#     'random_state': 42,\n#     'n_jobs': 4\n# }\n\n# xgb_params = {\n#     'lambda': 1,\n#     'alpha': 1,\n#     'colsample_bytree': 0,\n#     'subsample': .9,\n#     'learning_rate': 0.17024722721525629,\n#     'n_estimators': 9489,\n#     'objective': 'reg:squarederror',\n#     'max_depth': 3,\n#     'gamma': 2,\n#     'booster': 'gbtree',\n#     'min_child_weight': 155,\n#     'seed' : 38,\n#     'random_state': 42,\n#     'n_jobs': 4,\n#     'sampling_method': 'uniform'\n# }\n\n# xgb_params = {\n#     'lambda': 67.79737006663706,\n#     'alpha': 40.12405005448161,\n#     'colsample_bytree': 0.061613774851329205,\n#     'subsample': 0.9556736521337416,\n#     'learning_rate': 0.17024722721525629,\n#     'n_estimators': 9489,\n#     'max_depth': 3,\n#     'booster': 'gbtree',\n#     'min_child_weight': 155,\n#     'seed' : 38\n# }\n\n# xgb_params = {\n#     'lambda': 67.79737006663706,\n#     'alpha': 40.12405005448161,\n#     'colsample_bytree': 0.061613774851329205,\n#     'subsample': 0.9556736521337416,\n#     'learning_rate': 0.17024722721525629,\n#     'n_estimators': 9489,\n#     'objective': 'reg:squarederror',\n#     'max_depth': 3,\n#     'booster': 'gbtree',\n#     'min_child_weight': 155,\n#     'seed' : SEED,\n#     'random_state': SEED,\n#     'n_jobs': 4\n# }\n\nxgb_params = {'colsample_bytree': 0.31426530319123525,\n            'gamma': 0.45519212869187453,\n            'learning_rate': 0.06211498484648224,\n            'max_depth': 6,\n            'min_child_weight': 1.0,\n            'n_estimators': 10000,\n            'reg_alpha': 21.0,\n            'reg_lambda': 0.9277740406539857,\n            'subsample': 0.6254798229473668,\n            'booster': 'gbtree', \n            'random_state': SEED,\n            'seed': 91,\n            'n_jobs': -1}","cbe2c11b":"#Setting the kfold parameters\nkf = KFold(n_splits = 7, shuffle = True, random_state = SEED)\n\noof_preds = np.zeros((X.shape[0],))\npreds = 0\nmodel_fi = 0\nmean_rmse = 0\n\nfor num, (train_id, valid_id) in enumerate(kf.split(X)):\n    X_train, X_valid = X.loc[train_id], X.loc[valid_id]\n    y_train, y_valid = y.loc[train_id], y.loc[valid_id]\n    \n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    testX = scaler.transform(testX)\n    X_valid = scaler.transform(X_valid)\n    \n    model = XGBRegressor(**xgb_params, tree_method='gpu_hist')\n    model.fit(X_train, y_train,\n             verbose = False,\n             eval_set = [(X_train, y_train), (X_valid, y_valid)],\n             eval_metric = \"rmse\",\n             early_stopping_rounds = 100)\n    \n    #Mean of the predictions\n    preds += model.predict(testX) \/ 7 # Splits\n    \n    #Mean of feature importance\n    model_fi += model.feature_importances_ \/ 7 #splits\n    \n    #Out of Fold predictions\n    oof_preds[valid_id] = model.predict(X_valid)\n    fold_rmse = np.sqrt(mean_squared_error(y_valid, oof_preds[valid_id]))\n    print(f\"Fold {num} | RMSE: {fold_rmse}\")\n    \n    mean_rmse += fold_rmse \/ 7\n    \nprint(f\"\\nOverall RMSE: {mean_rmse}\")","7406acf5":"sub.loss = preds\nsub.head()","4e3efc3f":"sub.to_csv('submission_with_xgbregressor5.csv', index=False)\nprint('sending')","51808286":"# **Divide Data**","42fb75c2":"# Import Module"}}