{"cell_type":{"5fa67bf0":"code","72589ebf":"code","b5a1376f":"code","7dcb1dc5":"code","2b63fcfe":"code","7e3d8cad":"code","14fa2945":"code","60f4bb7f":"code","209d69b1":"markdown","0e53d891":"markdown","e9c8d91d":"markdown","f4e37af7":"markdown"},"source":{"5fa67bf0":"import random","72589ebf":"class GradientDescents:\n  \n  '''\n  Gradient Descent From Scratch\n  '''\n\n  def progress_tracker(self, step: int, cost_function: float) -> None:\n     '''\n    The function allows you to track online progress\n\n    :param step: current step\n    :param cost_function: the value of the cost function at the moment\n\n    '''\n    from IPython.display import clear_output\n    clear_output(wait=True)\n    print('\u0428\u0430\u0433: {}'.format(step))\n    print('\u0424\u0443\u043d\u043a\u0446\u0438\u044f \u043f\u043e\u0442\u0435\u0440\u044c: {:.2f}'.format(cost_function))\n\n  def mse_function(self, y_true: list, y_pred: list) -> float:\n    '''\n    Function that calculates MSE\n\n    :param y_true: the y values we know from the actual data\n    :param y_pred: the y values we got at the moment\n\n    :return mse: MSE value\n    '''\n    # \u041a\u043e\u043b-\u0432\u043e \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439, \u043a\u043e\u0442\u043e\u0440\u043e\u0435 \u043c\u044b \u0441\u0440\u0430\u0432\u043d\u0438\u0432\u0430\u043c\n    n = len(y_true)\n    # \u0421\u0442\u0430\u0440\u0442\u0443\u0435\u043c \u0441 \u043d\u0443\u043b\u044f\n    pre_mse = 0\n    for index, value in enumerate(y_true):\n      pre_mse += (value - y_pred[index])**2\n    mse = pre_mse\/n\n    return mse\n  \n  def gradient_descent(self, X_true: list, y_true: list, \\\n                       start_a: float = 1.0, start_b: float = 1.0, \\\n                       learning_rate: float = 0.003, max_steps: int =30000, \\\n                       save_steps: int = 0) -> dict:\n    '''\n    Simple gradient descent for formulas like y=a*x+b\n\n    :param start_a: first a value in y=a*x+b\n    :param start_b: first b value in y=a*x+b\n    :param learning_rate: learning rate\n    :param max_steps: maximum number of steps\n    :param save_steps: if 0, only result will be saved and returned\n                       if > 0, than every Ns' step will be saved\n   \n    :return return_dict: { \n\n            :return a: a value\n            :return b: b value\n            :return steps: total number of steps made\n            :return mse: MSE value\n            :return mse_list: lisr of MSE values if save_steps > 0\n            :return a_list: list of a values if save_steps > 0\n            :return b_list: list of b values if save_steps > 0\n    \n                        }\n    '''\n    # Initialize first step\n    step = 0\n    a = start_a\n    b = start_b\n    mse = 9999999\n    mse_prev = 0\n\n     # Let's make learning tracking\n    mse_list = []\n    a_list = []\n    b_list = []\n\n    # Predicted ys\n    y_pred = []\n    # Number of y elements in dataset\n    n=len(y_true)\n\n    # Initialize first gradients\n    grad_a=0\n    grad_b=0\n\n    # Model will work until our current step is less than max_steps, \n    # or difference between current MSE and previous MSE is less than 1e-10\n    # or MSE will be less than 1e-5\n    while (step <= max_steps) and (mse >= 1e-10) \\\n           and (abs(mse - mse_prev) >= 1e-5):\n      \n      mse_prev = mse\n      # Calculating moving steps for weights (just like in theory)\n      for i, x in enumerate(X_true):\n        grad_a += -2*(y_true[i] - (a*x + b))* x\n        grad_b += -2*(y_true[i] - (a*x + b))\n      grad_a = grad_a\/n\n      grad_b = grad_b\/n\n      # Make a move, according to lr (-= because we need oposite direction from gradient)\n      a -= learning_rate*grad_a\n      b -= learning_rate*grad_b\n      # New forecast\n      y_pred = [a*x+b for x in X_true]\n      # Check MSE loss\n      mse = self.mse_function(y_true, y_pred)\n\n      step += 1\n\n      # Writing progress\n      if save_steps > 0:\n        if step % save_steps == 0:\n          mse_list.append(mse)\n          a_list.append(a)\n          b_list.append(b)\n      \n      self.progress_tracker(step-1, mse)\n\n    if save_steps > 0:\n      return_dict = {'a': a, 'b': b, 'mse':mse, 'steps': step-1, \\\n            'mse_list': mse_list, 'a_list': a_list, 'b_list': b_list}\n    else:\n      return_dict = {'a': a, 'b': b, 'mse':mse, 'steps': step-1}\n\n    return return_dict","b5a1376f":"# Weigths\ns_w = [5, 12, 4]\n\nX_true = []\n\nfor i in range(100):\n  x_element = []\n  for j in range(3):\n    x_element.append(random.random())\n  X_true.append(x_element)\n\ny_true = []\nfor xt in X_true:\n  y_true.append(sum([w*x for w,x in zip(s_w,xt)]))","7dcb1dc5":"old_gb = GradientDescents()","2b63fcfe":"# Create a random list with number of weights = number of attributes\nweights = [random.random() for f in X_true[0]]\n\nlearning_rate = 0.001\nmse=999\n\nn = len(X_true)\nst = 0\nwhile mse>1e-5:\n  # Calculate gradients\n  gradients = []\n  for wi, w_value in enumerate(weights):\n    current_gradient=0\n    for yi, y_t_val in enumerate(y_true):\n      current_gradient += -2*(y_t_val - sum([w*x for w,x in zip(weights,X_true[yi])]))* X_true[yi][wi]\n    current_gradient = current_gradient\/n\n    gradients.append(current_gradient)\n\n  # Change weights\n  for gi, gr_value in enumerate(gradients):\n    weights[gi] = weights[gi] - learning_rate*gr_value\n\n  # Predict y_pred\n  y_pred = []\n  for X_current in X_true:\n    y_pred.append(sum([w*x for w,x in zip(weights,X_current)]))\n  st +=1\n  mse = old_gb.mse_function(y_true, y_pred)\n  old_gb.progress_tracker(st, mse)","7e3d8cad":"weights","14fa2945":"s_w","60f4bb7f":"class GradientDescents:\n  \n  '''\n  Gradient Descent From Scratch\n  '''\n\n  import random\n\n  def progress_tracker(self, step: int, cost_function: float) -> None:\n    '''\n    The function allows you to track online progress\n\n    :param step: current step\n    :param cost_function: the value of the cost function at the moment\n\n    '''\n    from IPython.display import clear_output\n    clear_output(wait=True)\n    print('\u0428\u0430\u0433: {}'.format(step))\n    print('\u0424\u0443\u043d\u043a\u0446\u0438\u044f \u043f\u043e\u0442\u0435\u0440\u044c: {:.2f}'.format(cost_function))\n\n  def mse_function(self, y_true: list, y_pred: list) -> float:\n    '''\n    Function that calculates MSE\n\n    :param y_true: the y values we know from the actual data\n    :param y_pred: the y values we got at the moment\n\n    :return mse: MSE value\n    '''\n    # Number of values to compare with actual y\n    n = len(y_true)\n    # Starting with 0\n    pre_mse = 0\n    for index, value in enumerate(y_true):\n      pre_mse += (value - y_pred[index])**2\n    mse = pre_mse\/n\n    return mse\n  \n  def gradient_descent(self, X_true: list, y_true: list, \\\n                       start_a: float = 1.0, start_b: float = 1.0, \\\n                       learning_rate: float = 0.003, max_steps: int =30000, \\\n                       save_steps: int = 0) -> dict:\n    '''\n    Simple gradient descent for formulas like y=a*x+b\n\n    :param start_a: first a value in y=a*x+b\n    :param start_b: first b value in y=a*x+b\n    :param learning_rate: learning rate\n    :param max_steps: maximum number of steps\n    :param save_steps: if 0, only result will be saved and returned\n                       if > 0, than every Ns' step will be saved\n   \n    :return return_dict: { \n\n            :return a: a value\n            :return b: b value\n            :return steps: total number of steps made\n            :return mse: MSE value\n            :return mse_list: lisr of MSE values if save_steps > 0\n            :return a_list: list of a values if save_steps > 0\n            :return b_list: list of b values if save_steps > 0\n    \n                        }\n    '''\n    # Initialize first step\n    step = 0\n    a = start_a\n    b = start_b\n    mse = 9999999\n    mse_prev = 0\n\n    # Let's make learning tracking\n    mse_list = []\n    a_list = []\n    b_list = []\n\n    # Predicted ys\n    y_pred = []\n    # Number of y elements in dataset\n    n=len(y_true)\n\n    # Initialize first gradients\n    grad_a=0\n    grad_b=0\n\n    # Model will work until our current step is less than max_steps, \n    # or difference between current MSE and previous MSE is less than 1e-10\n    # or MSE will be less than 1e-5\n    while (step <= max_steps) and (mse >= 1e-10) \\\n           and (abs(mse - mse_prev) >= 1e-5):\n      \n      mse_prev = mse\n      # Calculating moving steps for weights (just like in theory)\n      for i, x in enumerate(X_true):\n        grad_a += -2*(y_true[i] - (a*x + b))* x\n        grad_b += -2*(y_true[i] - (a*x + b))\n      grad_a = grad_a\/n\n      grad_b = grad_b\/n\n      # Make a move, according to lr (-= because we need oposite direction from gradient)\n      a -= learning_rate*grad_a\n      b -= learning_rate*grad_b\n      ## New forecast\n      y_pred = [a*x+b for x in X_true]\n      # Check MSE loss\n      mse = self.mse_function(y_true, y_pred)\n\n      step += 1\n\n      # Writing progress\n      if save_steps > 0:\n        if step % save_steps == 0:\n          mse_list.append(mse)\n          a_list.append(a)\n          b_list.append(b)\n      \n      self.progress_tracker(step-1, mse)\n\n    if save_steps > 0:\n      return_dict = {'a': a, 'b': b, 'mse':mse, 'steps': step-1, \\\n            'mse_list': mse_list, 'a_list': a_list, 'b_list': b_list}\n    else:\n      return_dict = {'a': a, 'b': b, 'mse':mse, 'steps': step-1}\n\n    return return_dict\n\n  def gradient_descent_multi(self, X_true: list, y_true: list, \\\n                              weights: list = None, max_steps: int = 10000, \\\n                              learning_rate: float = 0.003, \\\n                              save_steps: int = 0) -> dict:\n    '''\n    Gradient descent for multiple variables\n\n    :param X_true: actual attributes\n    :param y_true: actual results\n    :param weights: starting weights, if we don't want to start training from random\n    :param learning_rate: learning rate\n    :param max_steps: maximum number of steps at which the algorithm will stop\n    :param save_steps: if 0, only last step will be saved\n                       If not 0, every #save_steps will be saved\n    \n    :return {\n      :return weights: regression weights\n      :return mse: MSE\n      :return steps: # of Steps\n      :return mse_list: list of MSEs if save_steps > 0\n      :return weights_list: list of weigtht lists if save_steps > 0\n    }\n    '''\n    # Initialize random weights\n    if weights == None:\n      weights = [self.random.random() for f in X_true[0]]\n\n    if save_steps > 0:\n      mse_list = []\n      weights_list = []\n    \n    # MSE of the previous state\n    mse_prev = 0\n    mse = 999\n\n    # Nubmer of experiments we've got    \n    n = len(X_true)\n\n    step = 0\n    while (step <= max_steps) and (abs(mse_prev-mse)>1e-5):\n      # Calculate gradients\n      gradients = []\n      for wi, w_value in enumerate(weights):\n        current_gradient=0\n        for yi, y_t_val in enumerate(y_true):\n          current_gradient += -2*(y_t_val - sum([w*x for w,x in \\\n                                                 zip(weights,X_true[yi])]))* X_true[yi][wi]\n        current_gradient = current_gradient\/n\n        gradients.append(current_gradient)\n\n      # Change weights\n      for gi, gr_value in enumerate(gradients):\n        weights[gi] = weights[gi] - learning_rate*gr_value\n\n      # Calculate y_pred\n      y_pred = []\n      for X_current in X_true:\n        y_pred.append(sum([w*x for w,x in zip(weights,X_current)]))\n      \n      step +=1\n      mse_prev = mse\n      mse = self.mse_function(y_true, y_pred)\n      self.progress_tracker(step, mse)\n\n      if save_steps > 0:\n        if step % save_steps == 0:\n          mse_list.append(mse)\n          weights_list.append(weights)\n\n    if save_steps > 0:\n      return_dict = {'weights': weights, 'mse':mse, 'steps': step-1, \\\n                      'mse_list': mse_list, 'weights_list': weights_list}\n    else:\n      return_dict = {'weights': weights, 'mse':mse, 'steps': step-1}\n\n    return return_dict","209d69b1":"\u0421\u043e\u0437\u0434\u0430\u0435\u043c \u0441\u0438\u043d\u0442\u0435\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0435","0e53d891":"\u0410\u043b\u0433\u043e\u0440\u0438\u0442\u043c \u043e\u0442\u0440\u0430\u0431\u043e\u0442\u0430\u043b. \u0412\u0435\u0441\u0430 \u043f\u043e\u0447\u0442\u0438 \u0441\u043e\u0448\u043b\u0438\u0441\u044c","e9c8d91d":"## Theory\nTheory is similar to theory in [Gradient descent for formulas like y = a * x + b](https:\/\/www.kaggle.com\/konstantinsuspitsyn\/gradient-descent-for-formulas-like-y-a-x-b)\n<br><br>\n$MSE = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y_i})^2 $<br>\nPrivate dericatives for $a_i$ = $ \\frac{\\delta f}{\\delta a_i} = \\frac{1}{n} \\sum_{i=1}^n-2*(y-(a_i^0*x_i^0+a_i^1*x_i^1+...+a_i^z*x_i^z))*x_i $, \u0433\u0434\u0435 $x^0=1$, \u0430 $a^0$\u00a0\u2014 free term, and not the degree above, but the ordinal number of the attribute <br><br>","f4e37af7":"# Multivariate Gradient Descent\n## Introduction<br>\n\nThis is a series on Jupiter Notebooks:\n1. [Gradient descent for formulas like y = a * x + b](https:\/\/www.kaggle.com\/konstantinsuspitsyn\/gradient-descent-for-formulas-like-y-a-x-b)\n2. Multivariate Gradient Descent\n3. [Stochastic gradient descent with momentum](https:\/\/www.kaggle.com\/konstantinsuspitsyn\/stochastic-gradient-descent-with-momentum)\n4. [ADAM](https:\/\/www.kaggle.com\/konstantinsuspitsyn\/adam-from-scratch)\n\nThis is second notebok in series, where I create basic algorithms from scratch<br>"}}