{"cell_type":{"fdb5c0a5":"code","52f5866a":"code","5b0df5da":"code","b7de6297":"code","d5b63a50":"code","1d45d0fa":"code","72dcea6f":"code","993b3635":"code","d75f287b":"code","aa94a2fb":"code","39e85829":"code","bd25a161":"code","9964673e":"code","f3f833b0":"code","c164d47a":"code","1ea2b1fa":"code","dc222fd2":"code","6cc73711":"markdown"},"source":{"fdb5c0a5":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sn\nimport skimage.io\nimport keras.backend as K\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout,BatchNormalization ,Activation\nfrom tensorflow.keras.models import Model, Sequential\nfrom keras.applications.nasnet import NASNetLarge\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.optimizers import Adam","52f5866a":"train_datagen = ImageDataGenerator(rescale = 1.\/255,\n                                   validation_split = 0.2,\n                                  \n        rotation_range=5,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        shear_range=0.2,\n        #zoom_range=0.2,\n        horizontal_flip=True,\n        vertical_flip=True,\n        fill_mode='nearest')\n\nvalid_datagen = ImageDataGenerator(rescale = 1.\/255,\n                                  validation_split = 0.2)\n\ntest_datagen  = ImageDataGenerator(rescale = 1.\/255\n                                  )","5b0df5da":"train_dataset  = train_datagen.flow_from_directory(directory = '..\/input\/orchid-genus\/orchid-genus\/train',\n                                                   target_size = (128,128),\n                                                   class_mode = 'categorical',\n                                                   subset = 'training',\n                                                   batch_size = 64)","b7de6297":"valid_dataset = valid_datagen.flow_from_directory(directory = '..\/input\/orchid-genus\/orchid-genus\/train',\n                                                  target_size = (128,128),\n                                                  class_mode = 'categorical',\n                                                  subset = 'validation',\n                                                  batch_size = 64)","d5b63a50":"test_dataset = test_datagen.flow_from_directory(directory = '..\/input\/orchid-genus\/orchid-genus\/test',\n                                                  target_size = (128,128),\n                                                  class_mode = 'categorical',\n                                                  batch_size = 64)","1d45d0fa":"from keras.preprocessing import image\nimg = image.load_img(\"..\/input\/orchid-genus\/orchid-genus\/inet\/cattleya\/C10.jpg\",target_size=(128,128))\nimg = np.array(img)\nplt.imshow(img)\nprint(img.shape)\n\nimg = np.expand_dims(img, axis=0)\nfrom keras.models import load_model\nprint(img.shape)","72dcea6f":"base_model = tf.keras.applications.VGG16(input_shape=(128,128,3),include_top=False,weights=\"imagenet\")","993b3635":"# Freezing Layers\n\nfor layer in base_model.layers[:-4]:\n    layer.trainable=False","d75f287b":"# Building Model\n\nmodel=Sequential()\nmodel.add(base_model)\nmodel.add(Dropout(0.3))\nmodel.add(Flatten())\nmodel.add(BatchNormalization())\nmodel.add(Dense(32,kernel_initializer='he_uniform'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(32,kernel_initializer='he_uniform'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(32,kernel_initializer='he_uniform'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dense(5,activation='softmax'))","aa94a2fb":"# Model Summary\n\nmodel.summary()","39e85829":"from tensorflow.keras.utils import plot_model\nfrom IPython.display import Image\nplot_model(model, to_file='convnet.png', show_shapes=True,show_layer_names=True)\nImage(filename='convnet.png') ","bd25a161":"def f1_score(y_true, y_pred): #taken from old keras source code\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives \/ (predicted_positives + K.epsilon())\n    recall = true_positives \/ (possible_positives + K.epsilon())\n    f1_val = 2*(precision*recall)\/(precision+recall+K.epsilon())\n    return f1_val","9964673e":"METRICS = [\n      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n      tf.keras.metrics.Precision(name='precision'),\n      tf.keras.metrics.Recall(name='recall'),  \n      tf.keras.metrics.AUC(name='auc'),\n        f1_score,\n]","f3f833b0":"lrd = ReduceLROnPlateau(monitor = 'val_loss',patience = 20,verbose = 1,factor = 0.50, min_lr = 1e-10)\n\nmcp = ModelCheckpoint('model.h5')\n\nes = EarlyStopping(verbose=1, patience=20)","c164d47a":"model.compile(optimizer='Adam', loss='categorical_crossentropy',metrics=METRICS)","1ea2b1fa":"history=model.fit(train_dataset,validation_data=valid_dataset,epochs = 5,verbose = 1,callbacks=[lrd,mcp,es])","dc222fd2":"#%% PLOTTING RESULTS (Train vs Validation FOLDER 1)\n\ndef Train_Val_Plot(acc,val_acc,loss,val_loss,auc,val_auc,precision,val_precision,f1,val_f1):\n    \n    fig, (ax1, ax2,ax3,ax4,ax5) = plt.subplots(1,5, figsize= (20,5))\n    fig.suptitle(\" MODEL'S METRICS VISUALIZATION \")\n\n    ax1.plot(range(1, len(acc) + 1), acc)\n    ax1.plot(range(1, len(val_acc) + 1), val_acc)\n    ax1.set_title('History of Accuracy')\n    ax1.set_xlabel('Epochs')\n    ax1.set_ylabel('Accuracy')\n    ax1.legend(['training', 'validation'])\n\n\n    ax2.plot(range(1, len(loss) + 1), loss)\n    ax2.plot(range(1, len(val_loss) + 1), val_loss)\n    ax2.set_title('History of Loss')\n    ax2.set_xlabel('Epochs')\n    ax2.set_ylabel('Loss')\n    ax2.legend(['training', 'validation'])\n    \n    ax3.plot(range(1, len(auc) + 1), auc)\n    ax3.plot(range(1, len(val_auc) + 1), val_auc)\n    ax3.set_title('History of AUC')\n    ax3.set_xlabel('Epochs')\n    ax3.set_ylabel('AUC')\n    ax3.legend(['training', 'validation'])\n    \n    ax4.plot(range(1, len(precision) + 1), precision)\n    ax4.plot(range(1, len(val_precision) + 1), val_precision)\n    ax4.set_title('History of Precision')\n    ax4.set_xlabel('Epochs')\n    ax4.set_ylabel('Precision')\n    ax4.legend(['training', 'validation'])\n    \n    ax5.plot(range(1, len(f1) + 1), f1)\n    ax5.plot(range(1, len(val_f1) + 1), val_f1)\n    ax5.set_title('History of F1-score')\n    ax5.set_xlabel('Epochs')\n    ax5.set_ylabel('F1 score')\n    ax5.legend(['training', 'validation'])\n\n\n    plt.show()\n    \n\nTrain_Val_Plot(history.history['accuracy'],history.history['val_accuracy'],\n               history.history['loss'],history.history['val_loss'],\n               history.history['auc'],history.history['val_auc'],\n               history.history['precision'],history.history['val_precision'],\n               history.history['f1_score'],history.history['val_f1_score']\n              )","6cc73711":"# VVG-16\n\nVGG-16 is a convolutional neural network that is 16 layers deep. You can load a pretrained version of the network trained on more than a million images from the ImageNet database.\n\n\n\n# Dataset used :\n\n* 4000 train data without background + augmentation (800 image\/genus)\n* 2500 test data (500 image\/genus)\n* 185 test data from internet (25 cattleya and 40 other genus)\n\n\n\nDataset in this link  \n\n[https:\/\/www.kaggle.com\/raihanallaam\/orchid-genus](http:\/\/)"}}