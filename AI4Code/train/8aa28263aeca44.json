{"cell_type":{"8c2515ca":"code","b9e59b25":"code","daa68b66":"code","852f15ab":"code","8fdde97a":"code","a8314563":"code","203a3cb1":"code","66eeb5e0":"code","82e5c4b3":"code","049b81f7":"code","1b7f1e29":"code","ae6356c4":"code","cb704513":"code","99b491a6":"code","3f1d1f3d":"code","4cd8778d":"code","cffc51c7":"code","4688d731":"code","db415c35":"code","772c328d":"code","6e06af64":"code","07666bd2":"code","d32a3b4d":"code","fa274834":"code","7d6a5b55":"code","b1fed908":"code","28fa510b":"code","340085b9":"code","5155d664":"code","b7cd59f5":"code","9f9cc845":"code","f7e550d1":"code","51ef68fb":"code","e6da4580":"code","d0f6c0eb":"code","5bff2366":"code","59726abd":"code","ff9c614f":"code","5be2bbd7":"code","9e034057":"code","f3dbdcf8":"code","0d992f89":"code","ad8621e3":"code","c7ceae3a":"code","1e216694":"code","764545c9":"code","ee83347d":"code","bfd187a1":"code","e0652d57":"code","5826af9d":"code","b04807c3":"code","fef26e21":"code","07d4ab02":"code","39eb6461":"code","267a257d":"code","e43d8eb5":"code","1357b41d":"code","e6415eba":"code","c0518aeb":"code","096ec5b3":"code","c85307c2":"code","bf6ff907":"code","f307c5ba":"code","4b887e99":"code","e007f05a":"code","30b86320":"code","df6b2463":"code","36e900c2":"code","bd127c41":"code","fe059ea1":"code","e0041a54":"code","40d01700":"code","1b74e5f7":"code","b8ec762e":"code","f1b97dcd":"code","91999b19":"code","34fe3d9d":"code","ea0e9c35":"code","68d06fbc":"code","32ad0310":"code","72dc4d17":"code","f1fb2322":"code","9112ffb7":"code","233f8752":"code","cb495c1c":"code","370ec85b":"code","eced8a3a":"code","a1809567":"code","d47a8b96":"code","f0015c9c":"code","af1338e2":"code","fea3c178":"code","900adcf9":"markdown","5f4a56b1":"markdown","f84baf93":"markdown","c56c5522":"markdown","902e6d2c":"markdown","a6e3bcd1":"markdown","a0676b29":"markdown","a0e96bde":"markdown","1f4852dc":"markdown","fc812eb0":"markdown","d82e08ab":"markdown","60096600":"markdown","361c9e71":"markdown","ac49fc89":"markdown","24085332":"markdown","c540f284":"markdown","3eb970cc":"markdown","bbbe88ae":"markdown","b1b67012":"markdown","0186eb9a":"markdown","ab65789b":"markdown","f8927658":"markdown","c287a524":"markdown","0a75339b":"markdown","1bb188df":"markdown","e19d70b3":"markdown","7cd548ce":"markdown","d16f60e7":"markdown","c6273ca3":"markdown","a680be5f":"markdown","68d14bf2":"markdown","5ad449ec":"markdown","7dbf541b":"markdown","e36dd139":"markdown"},"source":{"8c2515ca":"import pandas as pd\nimport numpy as np\nimport matplotlib as plt\n%matplotlib inline","b9e59b25":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))## Importing the dataset","daa68b66":"dataset = pd.read_csv(\"\/kaggle\/input\/student-performance-data-set\/student-por.csv\")","852f15ab":"dataset.head()","8fdde97a":"dataset.tail()","a8314563":"grades_pass_fail = []\nfor index, row in dataset.iterrows():\n    if row['G3'] >= 10:\n        grades_pass_fail.append(1) #pass\n    else:\n        grades_pass_fail.append(0) #fail\n        \ngrades_pass_fail_series = pd.Series(grades_pass_fail)\ndataset[\"Pass\/Fail\"] = grades_pass_fail_series","203a3cb1":"grades_erasmus_label_encoded = []\nfor index, row in dataset.iterrows():\n    if row['G3'] >= 16:\n        grades_erasmus_label_encoded.append(1) #A\n    elif row['G3'] == 15 or row['G3'] == 14:\n        grades_erasmus_label_encoded.append(2) #B\n    elif row['G3'] == 12 or row['G3'] == 13:\n        grades_erasmus_label_encoded.append(3) #C\n    elif row['G3'] == 10 or row['G3'] == 11:\n        grades_erasmus_label_encoded.append(4) #D\n    elif row['G3'] <= 9:\n        grades_erasmus_label_encoded.append(5) #F   \n          \ngrades_erasmus_label_encoded_series = pd.Series(grades_erasmus_label_encoded)\ndataset[\"Erasmus Grade Label Encoded\"] = grades_erasmus_label_encoded_series","66eeb5e0":"grades_erasmus = []\nfor index, row in dataset.iterrows():\n    if row['G3'] >= 16:\n        grades_erasmus.append('A') \n    elif row['G3'] == 15 or row['G3'] == 14:\n        grades_erasmus.append('B')\n    elif row['G3'] == 12 or row['G3'] == 13:\n        grades_erasmus.append('C')\n    elif row['G3'] == 10 or row['G3'] == 11:\n        grades_erasmus.append('D')\n    elif row['G3'] <= 9:\n        grades_erasmus.append('F')    \n          \ngrades_erasmus_series = pd.Series(grades_erasmus)\ndataset[\"Erasmus Grade\"] = grades_erasmus_series","82e5c4b3":"dataset.head()","049b81f7":"dataset.tail()","1b7f1e29":"X = dataset.iloc[:, :-4].values #All columns until G3\ny = dataset.iloc[:, -4].values #Column G3","ae6356c4":"print(X)","cb704513":"print(y)","99b491a6":"import matplotlib.pyplot as plt","3f1d1f3d":"plt.style.use('ggplot')\ndataset['G3'].plot.hist(title='Histogram of G3 Grades', bins=20)\nplt.xlabel('Grades - G3')","4cd8778d":"plt.style.use('bmh')\ndataset['G2'].plot.hist(title='Histogram of G2 Grades',bins=20)\nplt.xlabel('Grades - G2')","cffc51c7":"plt.figure(figsize=(4.30,3), dpi=100)\nplt.style.use('seaborn')\ndataset['G1'].plot.hist(title='Histogram of G1 Grades',bins=20)\nplt.xlabel('Grades - G1')","4688d731":"Absences = dataset.iloc[:, -7].values\nG3 = dataset.iloc[:, -4].values","db415c35":"plt.figure(figsize=(10,3), dpi=100)\nplt.style.use('bmh')\nplt.xlabel('Number of Absences')\nplt.ylabel('Grades - G3')\nplt.title('Scatter Plot of Absences and G3 Grades')\nplt.scatter(Absences,G3)","772c328d":"dataset.isnull().values.any()","6e06af64":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nct = ColumnTransformer(transformers = [('encoder', OneHotEncoder(), [0, 1, 3, 4, 5, 8, 9, 10, 11, 15, 16, 17, 18, 19, 20, 21, 22])], remainder=\"passthrough\")\nX = np.array(ct.fit_transform(X))","07666bd2":"print(X)","d32a3b4d":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)","fa274834":"print(X_train)","7d6a5b55":"print(X_test)","b1fed908":"print(y_train)","28fa510b":"print(y_test)","340085b9":"#To print the whole array\n# with np.printoptions(threshold=np.inf):\n#     print(X_test) ","5155d664":"from sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)","b7cd59f5":"importance = regressor.coef_\nfor i,v in enumerate(importance):\n    v = \"{:.2f}\".format(v)\n    print(f'Feature: {i}, Score: {v}')\n# plot feature importance\nplt.bar([i for i in range(len(importance))], importance)\nplt.show()","9f9cc845":"y_pred = regressor.predict(X_test)\nnp.set_printoptions(precision=2)\nprint(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))","f7e550d1":"from sklearn.metrics import r2_score\nr2 = r2_score(y_test, y_pred)\nprint(f'Score (R2): {r2}')","51ef68fb":"from sklearn.model_selection import cross_val_score\nscores = cross_val_score(estimator = regressor, X = X_train, y = y_train, cv = 10, scoring='r2')\nprint(\"Score (R2): {:.2f}\".format(scores.mean()))\nprint(\"Standard Deviation: {:.2f}\".format(scores.std()))","e6da4580":"y = dataset.iloc[:, -3].values #Column Pass\/Fail","d0f6c0eb":"print(y)","5bff2366":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)","59726abd":"from sklearn.tree import DecisionTreeClassifier\nclassifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\nclassifier.fit(X_train, y_train)","ff9c614f":"importance = classifier.feature_importances_\nfor i,v in enumerate(importance):\n    v = \"{:.2f}\".format(v)\n    print(f'Feature: {i}, Score: {v}')\n# plot feature importance\nplt.bar([i for i in range(len(importance))], importance)\nplt.show()","5be2bbd7":"y_pred = classifier.predict(X_test)\nprint(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))","9e034057":"from sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)","f3dbdcf8":"print(\"Accuracy: {:.2f} %\".format(accuracy_score(y_test, y_pred)*100))","0d992f89":"from sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))","ad8621e3":"from sklearn.model_selection import GridSearchCV\nparameters = {'criterion':['gini','entropy'],'max_depth':[4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150]}\ngrid_search = GridSearchCV(estimator = classifier,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search = grid_search.fit(X_train, y_train)\nbest_accuracy = grid_search.best_score_\nbest_parameters = grid_search.best_params_\nprint(\"Best Accuracy: {:.2f} %\".format(best_accuracy * 100))\nprint(\"Best Parameters:\", best_parameters)","c7ceae3a":"y = dataset.iloc[:, -2].values #Column Erasmus Grade Label Encoded","1e216694":"print(y)","764545c9":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)","ee83347d":"from sklearn.tree import DecisionTreeClassifier\nclassifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\nclassifier.fit(X_train, y_train)","bfd187a1":"importance = classifier.feature_importances_\nfor i,v in enumerate(importance):\n    v = \"{:.2f}\".format(v)\n    print(f'Feature: {i}, Score: {v}')\n# plot feature importance\nplt.bar([i for i in range(len(importance))], importance)\nplt.show()","e0652d57":"y_pred = classifier.predict(X_test)\nprint(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))","5826af9d":"from sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)","b04807c3":"print(\"Accuracy: {:.2f} %\".format(accuracy_score(y_test, y_pred)*100))","fef26e21":"from sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))","07d4ab02":"from sklearn.model_selection import GridSearchCV\nparameters = {'criterion':['gini','entropy'],'max_depth':[4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150]}\ngrid_search = GridSearchCV(estimator = classifier,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search = grid_search.fit(X_train, y_train)\nbest_accuracy = grid_search.best_score_\nbest_parameters = grid_search.best_params_\nprint(\"Best Accuracy: {:.2f} %\".format(best_accuracy * 100))\nprint(\"Best Parameters:\", best_parameters)","39eb6461":"y = dataset.iloc[:, -3].values #Column Pass\/Fail","267a257d":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)","e43d8eb5":"from sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\nclassifier.fit(X_train, y_train)","1357b41d":"importance = classifier.feature_importances_\nfor i,v in enumerate(importance):\n    v = \"{:.2f}\".format(v)\n    print(f'Feature: {i}, Score: {v}')\n# plot feature importance\nplt.bar([i for i in range(len(importance))], importance)\nplt.show()","e6415eba":"y_pred = classifier.predict(X_test)\nprint(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))","c0518aeb":"from sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint(\"Accuracy: {:.2f} %\".format(accuracy_score(y_test, y_pred)*100))","096ec5b3":"from sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))","c85307c2":"y = dataset.iloc[:, -2].values #Column Erasmus Grade Label Encoded","bf6ff907":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)","f307c5ba":"from sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\nclassifier.fit(X_train, y_train)","4b887e99":"importance = classifier.feature_importances_\nfor i,v in enumerate(importance):\n    v = \"{:.2f}\".format(v)\n    print(f'Feature: {i}, Score: {v}')\n# plot feature importance\nplt.bar([i for i in range(len(importance))], importance)\nplt.show()","e007f05a":"y_pred = classifier.predict(X_test)\nprint(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))","30b86320":"from sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint(\"Accuracy: {:.2f} %\".format(accuracy_score(y_test, y_pred)*100))","df6b2463":"from sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))","36e900c2":"from sklearn.model_selection import RandomizedSearchCV\nfrom pprint import pprint\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\nmax_features = ['auto', 'sqrt']\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\nmin_samples_split = [2, 5, 10]\nmin_samples_leaf = [1, 2, 4]\nbootstrap = [True, False]\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\npprint(random_grid)","bd127c41":"rf = RandomForestClassifier()\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 10, cv = 3, verbose=2, random_state=42, n_jobs = -1)\nrf_random.fit(X_train, y_train)","fe059ea1":"rf_random.best_params_","e0041a54":"y_pred = rf_random.predict(X_test)\nprint(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))","40d01700":"cm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint(\"Accuracy: {:.2f} %\".format(accuracy_score(y_test, y_pred)* 100))","1b74e5f7":"y = dataset.iloc[:, -3].values #Column Pass\/Fail\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)","b8ec762e":"import sys\n!{sys.executable} -m pip install xgboost\nfrom xgboost import XGBClassifier\nclassifier = XGBClassifier(eval_metric='mlogloss', use_label_encoder=False)\nclassifier.fit(X_train, y_train)","f1b97dcd":"importance = classifier.feature_importances_\nfor i,v in enumerate(importance):\n    v = \"{:.2f}\".format(v)\n    print(f'Feature: {i}, Score: {v}')\n# plot feature importance\nplt.bar([i for i in range(len(importance))], importance)\nplt.show()","91999b19":"y_pred = classifier.predict(X_test)","34fe3d9d":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint(\"Accuracy: {:.2f} %\".format(accuracy_score(y_test, y_pred)*100))","ea0e9c35":"from sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))","68d06fbc":"y = dataset.iloc[:, -2].values #Column Erasmus Grade Label Encoded","32ad0310":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)","72dc4d17":"classifier = XGBClassifier()\nclassifier.fit(X_train, y_train)","f1fb2322":"importance = classifier.feature_importances_\nfor i,v in enumerate(importance):\n    v = \"{:.2f}\".format(v)\n    print(f'Feature: {i}, Score: {v}')\n# plot feature importance\nplt.bar([i for i in range(len(importance))], importance)\nplt.show()","9112ffb7":"y_pred = classifier.predict(X_test)","233f8752":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint(\"Accuracy: {:.2f} %\".format(accuracy_score(y_test, y_pred)*100))","cb495c1c":"accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))","370ec85b":"!{sys.executable} -m pip install delayed\nfrom imblearn.over_sampling import SMOTE\nsm = SMOTE(sampling_strategy = 'auto', random_state=27)\nX_train_smote, y_train_smote = sm.fit_resample(X_train, y_train) #It's important to generate the new samples only in the training set to ensure our model generalizes well to unseen data.\n#Let's now fit our classifiers over our updated dataset!","eced8a3a":"assert len(X_train_smote) !=  len(X_train)\nassert len(y_train_smote) != len(y_train) #confirming that we have a resampled dataset with synthetic values","a1809567":"smote_xgb = classifier.fit(X_train_smote, y_train_smote)\nsmote_pred_xg = smote_xgb.predict(X_test)","d47a8b96":"cm_slr = confusion_matrix(y_test, smote_pred_xg)","f0015c9c":"print(cm_slr)","af1338e2":"print(\"Accuracy: {:.2f} %\".format(accuracy_score(y_test, smote_pred_xg)*100))","fea3c178":"Models = ['DRA', 'DRB','RFA', 'RFB', 'XGBA', 'XGBB']\nScores = [0.9287, 0.7360, 0.9249, 0.7615, 0.9229, 0.7846]\nbarlist = plt.bar(Models, Scores)\nfor i in range(1,6,2):\n    barlist[i].set_color('r')\nplt.xlabel('Models')\nplt.ylabel('Scores')\nplt.title('Model Comparison - Classification')","900adcf9":"## Data Visualizations","5f4a56b1":"## Using SMOTE to mitigate the effects of unbalanced classes ","f84baf93":"## Random Forest Feature Importance 1","c56c5522":"## Model 4A: XGboost to predict pass\/fail","902e6d2c":"## Applying k-Fold Cross Validation","a6e3bcd1":"### Visualizing the Relationship Between the Number of Absences and G3 Grades","a0676b29":"## Applying k-Fold Cross Validation","a0e96bde":"## Model 4B: XGboost to predict encoded Erasmus grade","1f4852dc":"## Random Search Cross Validation in Scikit-Learn","fc812eb0":"## Verifying if there is missing data","d82e08ab":"## Encoding categorical data","60096600":"## XGboost Feature Importance 2 ","361c9e71":"## Importing the libraries","ac49fc89":"## Linear Regression Feature Importance ","24085332":"## Applying k-Fold Cross Validation","c540f284":"## Creating Modified Labels ","3eb970cc":"This notebook contains the code of a prototype ML-based score-predicting feature, designed for the main product of Focus Project as part of my internship. The dataset includes attributes such as past student grades, demographic, social, and school related factors. After performing some basic visualizations, data pre-processing and feature engineering, the data was modelled under binary\/five-level classification and regression tasks. Some models' hyperparameters were fine-tunded to optimize performance, whereas other models with already competitive results did not undergo this procedure to prioritize computational time over performance. Towards the end of this notebook, to tackle class imbalance, the SMOTE (Synthetic Minority Oversampling Technique) was performed, yielding more competitive results.\n\nI am still learning everyday and I am always open to new ideas that can help me improve my code, therefore, if you have any feedback, queries or concerns regarding this notebook, please feel free to email me at aryanmsr@gmail.com.\n\n- Link to dataset: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Student+Performance\n- Link to the original research paper from which this notebook is based on: http:\/\/www3.dsi.uminho.pt\/pcortez\/student.pdf","bbbe88ae":"## Applying Grid Search to find the best model and the best parameters","b1b67012":"## Model 3B: Random-Forest Classification to predict encoded Erasmus grade","0186eb9a":"## Applying k-Fold Cross Validation","ab65789b":"## Model 1: Linear Regression ","f8927658":"## Model 2B: Decision-Tree Based Classification to predict encoded Erasmus grade","c287a524":"# Prototype Score-Predicting Model Based on Portuguese Secondary Student Performance Data","0a75339b":"## Decision Tree Feature Importance 2","1bb188df":"## Random Forest Feature Importance 2","e19d70b3":"## Splitting the Dataset Into a Training Set and Test Set","7cd548ce":"## Applying k-Fold Cross Validation","d16f60e7":"## Model 2A: Decision-Tree Based Classification to predict pass\/fail","c6273ca3":"## Decision Tree Feature Importance","a680be5f":"## Model 3A: Random-Forest Classification to predict pass\/fail","68d14bf2":"## Applying k-Fold Cross Validation","5ad449ec":"## XGboost Feature Importance ","7dbf541b":"## Applying k-Fold Cross Validation","e36dd139":"## Applying Grid Search to find the best model and the best parameters"}}