{"cell_type":{"9c3eb1f8":"code","23be3fa4":"code","ac71d0a1":"code","69d47504":"code","10daabda":"code","937cfd0e":"code","2ce92597":"code","d76d2401":"code","80451838":"code","af2138e3":"code","a5094b39":"code","87d4affa":"code","3860e897":"code","2e0373eb":"code","970028f0":"code","d88a4b30":"code","0739894a":"code","4a677784":"code","7312ebff":"code","4cea8e4f":"code","55da9e02":"code","abb81046":"code","50d5e6df":"code","158862d3":"code","5b93e916":"code","afa641d1":"code","b2af64b0":"code","caee4816":"code","158995bc":"code","c392e3b9":"code","5df59b39":"code","d1c2110e":"code","301256a2":"code","23741816":"code","2a4b0c76":"code","2301261b":"code","47a77728":"code","94820569":"code","921982de":"markdown","bf7ae28d":"markdown","89f38924":"markdown","fa4d19b5":"markdown","50c74f19":"markdown","98d85418":"markdown","81e4c5e4":"markdown","7a165db3":"markdown","05bc1212":"markdown","5918fc96":"markdown","894c2311":"markdown","9498db61":"markdown","fdae7a41":"markdown","5a11ab45":"markdown","265d1ed3":"markdown","89aca072":"markdown","3e28892e":"markdown","37e01e73":"markdown","f8ceafff":"markdown","06a60502":"markdown","22ee0ced":"markdown","a578b30e":"markdown","e65467a1":"markdown","76bab4e4":"markdown","f589be1e":"markdown","ef81980e":"markdown","86e9158c":"markdown","87b3cb1e":"markdown","445df333":"markdown","50a4cd0e":"markdown","20388759":"markdown","1eefa252":"markdown","4d7162a5":"markdown","44343a1a":"markdown","acd443ed":"markdown"},"source":{"9c3eb1f8":"import numpy as np\nnp.random.seed(0)\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"darkgrid\")\n\nimport itertools as it\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.preprocessing import KBinsDiscretizer\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRegressor\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\n\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nfrom pdpbox import pdp, get_dataset, info_plots\nimport shap\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","23be3fa4":"data = pd.read_csv(\"\/kaggle\/input\/yeh-concret-data\/Concrete_Data_Yeh.csv\")\ndata.head()","ac71d0a1":"data.shape","69d47504":"data.dtypes","10daabda":"target_variable = 'csMPa'\nnumerical_variables = ['cement', 'slag', 'flyash', 'water', 'superplasticizer', 'coarseaggregate', 'fineaggregate', 'age']","937cfd0e":"data.info()","2ce92597":"data.describe()","d76d2401":"plt.figure(figsize=(16, 16))\nbar_plot_df = pd.melt(data.iloc[[20]])\nsns.barplot(y=bar_plot_df.value, x=bar_plot_df.variable)","80451838":"fig, axs = plt.subplots(1, 2, figsize=(16, 6))\nsns.boxplot(x=data[target_variable], ax=axs[0])\nsns.distplot(data[target_variable], bins=20, kde=True, ax=axs[1])","af2138e3":"plt.figure(figsize=(12, 8))\ncorr = data.corr()\nax = sns.heatmap(\n    corr, \n    vmin=-1, vmax=1, center=0,\n    annot=True,\n    square=True\n)\nplt.show()","a5094b39":"def num_plots(feature_name):\n    fig, axs = plt.subplots(1, 3, figsize=(16, 6))\n    sns.boxplot(x=data[feature_name], ax=axs[0])\n    sns.distplot(data[feature_name], bins=20, kde=True, ax=axs[1])\n    sns.scatterplot(data=data, x=feature_name, y=target_variable, ax=axs[2])\n    plt.show()","87d4affa":"for feature in numerical_variables:\n    num_plots(feature)","3860e897":"zero_features = ['slag', 'flyash', 'superplasticizer']\n\nzero_counts = []\nfor feature in zero_features:\n    zero_counts.append(data.loc[data[feature]==0, feature].count())\n    print(\"Fraction of zero values for {}: {}\".format(feature, zero_counts[-1]\/data.shape[0]))\n\nsns.barplot(y=zero_counts, x=zero_features)","2e0373eb":"pairs = list(it.combinations(zero_features, r=2))\n\nfor pair in pairs:\n    sns.scatterplot(data=data, x=pair[0], y=pair[1])\n    plt.show()","970028f0":"for feature in zero_features:\n    new_data = data.loc[(data[feature] == 0)]\n    temp_features = list(new_data.columns)\n    temp_features.remove(feature)\n    fig, axs = plt.subplots(3, 3, figsize=(16, 10))\n    i = j = 0\n    for comp_feature in temp_features:\n        \n        try:\n            sns.distplot(new_data[comp_feature], bins=20, kde=True, ax=axs[i][j])\n        except RuntimeError as re:\n            if str(re).startswith(\"Selected KDE bandwidth is 0. Cannot estimate density.\"):\n                sns.distplot(new_data[comp_feature], bins=20, kde=True, kde_kws={'bw': 0.1}, ax=axs[i][j])\n            else:\n                raise re\n#         sns.distplot(new_data[comp_feature], bins=20, kde=True, ax=axs[i][j])\n        i += 1\n        if i == 3:\n            j += 1\n            i = 0\n    plt.show()","d88a4b30":"sub_data = data.loc[(data['superplasticizer'] == 0) & (data['flyash'] > 0)]\nsns.scatterplot(data=sub_data, x='flyash', y='csMPa')","0739894a":"y = data[target_variable]\nX = data.drop(target_variable, axis=1)\n\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2)\n\ncombined = [train_X, test_X]","4a677784":"# 'age' and 'water' had more of an effect than others, but minor compared to 'cement'.\nfeatures_to_square = ['cement']\n\nfor col in features_to_square:\n    for set_X in combined:\n        set_X[col+'_sqrd'] = set_X[col] ** 2","7312ebff":"# Using Yeo-Johnson power transform.\ntransformer = PowerTransformer()\ntrain_X = pd.DataFrame(transformer.fit_transform(train_X), columns=train_X.columns)\ntest_X = pd.DataFrame(transformer.transform(test_X), columns=test_X.columns)","4cea8e4f":"# Scaling between 0 and 1.\nscaler = MinMaxScaler()\ntrain_X = pd.DataFrame(scaler.fit_transform(train_X), columns=train_X.columns)\ntest_X = pd.DataFrame(scaler.transform(test_X), columns=test_X.columns)","55da9e02":"prelim_results = pd.DataFrame(columns=['Model', 'Baseline Score'])\n\ndef score_models(model_name, model, i):\n    score = cross_val_score(model, train_X, train_y, cv=5, scoring='neg_root_mean_squared_error')\n#     print(\"{} average: {}\".format(model_name, score.mean()))\n    prelim_results.loc[i] = [model_name, score.mean()]","abb81046":"models = {\n    \"Linear Regression\": LinearRegression(),\n    \"Ridge Regression\": Ridge(),\n    \"SVM\": SVR(),\n    \"KNN\": KNeighborsRegressor(),\n    \"Random Forest\": RandomForestRegressor(),\n    \"GradientBoost\": GradientBoostingRegressor(),\n    \"XGBoost\": XGBRegressor(),\n}","50d5e6df":"i = 0\nfor name,model in models.items():\n    score_models(name, model, i)\n    i += 1\n\nprelim_results.head(10)","158862d3":"def params_performance(regressor, model_name):\n    print(model_name)\n    print('Best Score: ' + str(regressor.best_score_))\n    print('Best Parameters: ' + str(regressor.best_params_))","5b93e916":"\"\"\"\nparam_grid = {\n    'n_estimators': [200, 400, 600, 800, 1000],\n    'colsample_bytree': [0.5, 0.8, 0.9, 1],\n    'max_depth': [2, 4, 6, 8, 10, None],\n    'reg_alpha': [0, 0.5, 1],\n    'reg_lambda': [1, 1.5, 2],\n    'subsample': [0.6, 0.7, 0.8, 0.9, 1],\n    'learning_rate': [0.01, 0.1, 0.2, 0.3, 0.5],\n    'gamma': [0, 0.01, 0.1, 1, 10],\n    'min_child_weight': [0, 0.01, 0.1, 1, 10],\n    'sampling_method': ['uniform', 'gradient_based']\n}\n\nxgb_rnd_srch = RandomizedSearchCV(XGBRegressor(), param_distributions = param_grid, n_iter =  300, cv = 5, scoring='neg_root_mean_squared_error', verbose = True, n_jobs = -1)\nbest_xgb_rnd_srch = xgb_rnd_srch.fit(train_X, train_y)\nparams_performance(best_xgb_rnd_srch,'XGBoost')\n\"\"\"","afa641d1":"\"\"\"\nparam_grid = {\n    'n_estimators': [900, 1000],\n    'colsample_bytree': [0.95, 1],\n    'max_depth': [3, 4, 5],\n    'reg_alpha': [0.75, 1, 1.25],\n    'reg_lambda': [1.75, 2],\n    'subsample': [0.75, 0.8, 0.85],\n    'learning_rate': [0.05, 0.1, 0.5],\n    'gamma': [0.5, 1, 5],\n    'min_child_weight': [0, 0.01],\n    'sampling_method': ['uniform']\n}\n\nxgb_grd_srch = GridSearchCV(XGBRegressor(), param_grid = param_grid, cv = 5, scoring='neg_root_mean_squared_error', verbose = True, n_jobs = -1)\nbest_xgb_grd_srch = xgb_grd_srch.fit(train_X, train_y)\nparams_performance(best_xgb_grd_srch,'XGBoost')\n\"\"\"","b2af64b0":"# final_model = best_xgb_grd_srch.best_estimator_\nfinal_model = XGBRegressor(colsample_bytree=0.95, gamma=0.5, learning_rate=0.05, max_depth=5, min_child_weight=0, n_estimators=1000, reg_alpha=1.25, reg_lambda=2, sampling_method='uniform', subsample=0.75)\n\nfinal_model.fit(train_X, train_y)\n\npreds = final_model.predict(test_X)\n\nmean_squared_error(test_y, preds, squared=False)","caee4816":"test_y.reset_index(drop=True, inplace=True)\ny_plot = pd.concat([test_y, pd.Series(np.array([1 for i in range(len(test_y))]), name='True Values')], axis=1)\n\npred_plot = pd.concat([pd.Series(preds, name='csMPa'), pd.Series(np.array([0 for i in range(len(test_y))]), name='True Values')], axis=1)\n\npred_y_plot = pd.concat([y_plot, pred_plot], axis=0)\npred_y_plot['index'] = pred_y_plot.index","158995bc":"plt.figure(figsize=(10,10))\nplt.title(\"Predicted and True Target Values\")\nsns.scatterplot(x='index', y='csMPa', hue='True Values', data=pred_y_plot, s=100)\nplt.show()","c392e3b9":"perm = PermutationImportance(final_model).fit(train_X, train_y)\neli5.show_weights(perm, feature_names=train_X.columns.tolist())","5df59b39":"current_feature_names = train_X.columns\ncurrent_features = train_X.columns.tolist()\n\nfor feature_name in current_features:\n    pdp_goals = pdp.pdp_isolate(model=final_model, dataset=test_X, model_features=current_feature_names, feature=feature_name)\n    pdp.pdp_plot(pdp_goals, feature_name)\n    plt.show()","d1c2110e":"feature_paris_to_plot = [['water', 'coarseaggregate'], ['cement', 'cement_sqrd']]\nfor plot_features in feature_paris_to_plot:\n    inter = pdp.pdp_interact(model=final_model, dataset=test_X, model_features=current_feature_names, features=plot_features)\n    pdp.pdp_interact_plot(pdp_interact_out=inter, feature_names=plot_features, plot_type='contour')\n    plt.show()","301256a2":"shap_model = final_model.fit(train_X, train_y)\nexplainer = shap.TreeExplainer(shap_model)\nshap_values = explainer.shap_values(test_X)\nshap.summary_plot(shap_values, test_X)","23741816":"data.loc[data['csMPa'].idxmax()]","2a4b0c76":"data.loc[data['csMPa'] > 78]","2301261b":"recipe = [[500.0, 175.0, 0.0, 143.0, 12.0, 1000.0, 750.0, 360.0]]\nrecipe_columns = data.columns.tolist()\nrecipe_columns.remove('csMPa')\nnew_recipe = pd.DataFrame(data=recipe, columns=recipe_columns)","47a77728":"for col in features_to_square:\n    new_recipe[col+'_sqrd'] = new_recipe[col] ** 2\n\nnew_recipe = pd.DataFrame(transformer.transform(new_recipe), columns=new_recipe.columns)\nnew_recipe = pd.DataFrame(scaler.transform(new_recipe), columns=new_recipe.columns)","94820569":"new_pred = final_model.predict(new_recipe)\n\nprint(new_pred)","921982de":"Considerably better performance in the XGBoost baseline model, compared to the other models. Because of this, only XGBoost will be optimised as it is likely to give the best generalization by itself. From these baselines, it is anticipated that optimising more models for use in an ensemble also won't improve perfromance.","bf7ae28d":"* No highly correlated features.","89f38924":"# 1. Set Up","fa4d19b5":"Engineered new features by multiplying two current fetures and squaring each individual feature.\nThis resulted in:\n* Improved baseline performance on individual models (such as linear regression, KNN and SVM)\n* Decreased ensemble model baseline performance (Random Forest, GradientBoost and XGBoost)\n* Even with these new features, ensemble models continued to outperform individual models. \n\nSubsequently:\n* These new features will not be used, but the code is left in incase these features can help with new models implemented later.\n* 'cement_sqrd' feature was kept as it did not hinder XGBoost model performance (best performing baseline) but did help with other model performance.","50c74f19":"* Not all features are perfectly normally distributed. Should be normalised.\n* All features should be scaled to ensure they are on a similar scale.","98d85418":"# 4. Baseline Modelling","81e4c5e4":"Increases compressive strength:\n* Large amount of cement.\n* Large amount of slag.\n* Medium to large amounts of superplasticizer.\n* Small amounts of fineaggregate.\n* Larger age.\n\nDecreases compressive strength:\n* Large amount of flyash.\n* Large amount of water.\n* Large amount of coarseaggregate.\n* Large amount of fineaggregate.","7a165db3":"Must apply all the same preprocessing steps.","05bc1212":"* Above shows an example recipe for the cement.","5918fc96":"This 0 skewed data may make it harder for us to interpret the model's decision making, thus making it harder for us to properly create a stronger recipe.","894c2311":"Future work:\n* Look into how to effectively use 0-skewed data.\n* Create a function to do a more exhaustive search for a better recipe.","9498db61":"Features:\n* cement - kg of cement (component 1) in a m^3 mixture\n* slag - kg of blast furnace slag (component 2) in a m^3 mixture\n* flyash - kg of fly ash (component 3) in a m63 mixture\n* water - kg of water (component 4) in a m^3 mixture\n* superplasticizer - kg of super plasticizer (component 5) in a m^3 mixture\n* coarseaggregate - kg of coarse aggregate (component 6) in a m^3 mixture\n* fineaggregate - kg of fine aggregate (component 7) in a m^3 mixture\n* age - Day (1~365)\n* csMPa - Concrete compressive strength in MPa","fdae7a41":"* Interesting relationships between model features, which could be inspected further. \n* Could be particularly useful for predicting a new recipe.","5a11ab45":"Have found a recipe that is predicted to have a higher compressive strength than the maximum compressive strenght within the data set. Would hope that the predicted recipe has a compressive strength that is the average error of test set higher than the max, to be more sure that it is a better recipe. Not the case here, but with a more intense search this is very achievable.","265d1ed3":"* Age plays the biggest part in predicting a larger compressive strength.\n* Cement and water play a big part in the prediction also.\n* Other features appear to play amostly minor role.","89aca072":"* 6 cases where superplasticizer = 0 and flyash does not.\n* Above shows the compressive strength of those data points.","3e28892e":"# 9. Conclusions","37e01e73":"**Normalising features:**","f8ceafff":"Preprocessing steps attempted may have helped the baseline model performance but did not help the generalization of the model and thus were not included:\n* Dropping columns, specifically coarseaggregate.\n* Banding some of the continuous, numerical features.\n* Additional features to indicate data points where there were zero values in columns with large amount of 0 values.\n* Engineering new features by multiplying current features together.","06a60502":"# 6. Test Set Accuracy","22ee0ced":"Output:\n\nInitial:\n```\nXGBoost\nBest Score: -4.241429460255205\nBest Parameters: {'colsample_bytree': 0.65, 'gamma': 1, 'learning_rate': 0.1, 'max_depth': None, 'min_child_weight': 10, 'n_estimators': 900, 'reg_alpha': 1, 'reg_lambda': 1, 'sampling_method': 'uniform', 'subsample': 0.9}\n```","a578b30e":"**Scaling features:**","e65467a1":"* Target variable is not skewed, appears normally distributed.","76bab4e4":"# 7. Model Analysis","f589be1e":"* Often when one of these features is zero, the other will be too.\n* In almost every case where superplasticizer is 0, flyash is also 0. However, this relationaship is not reciprocal as data points with flyash = 0  do not also have superplasticizer = 0 almost all the time.","ef81980e":"# 8. Engineering a New Recipe","86e9158c":"# 0. Project Aims\n\n* Predict the compressive strength of concrete form it's mixture ingredients and age.","87b3cb1e":"# 2. EDA","445df333":"* Minimal variety or amount of data available makes this problem difficult to create a very accurate model.\n* It is expected that more data would help performance.\n* Adequate performance considering the small amount of modelling work done and data provided.\n* Managed to use the model to help predict a more effective recipe.","50a4cd0e":"# 3. Preprocessing","20388759":"* Significant amount of time where slag or flyash is 0, or slag and plasticizer is 0.\n* Flyash is much more liekly to be 0 when superplasticizer is not 0, than superplasticizer is to be 0 when flyash is not 0.\n* Can't tell a huge amount form this currently.","1eefa252":"* No missing data.\n* Input variables are all numerical. All are continuous, aside form Age which is a range between 1-365.","4d7162a5":"Output:\n\nInitial:\n```\nXGBoost\nBest Score: -4.304916851823981\nBest Parameters: {'subsample': 0.9, 'sampling_method': 'uniform', 'reg_lambda': 1, 'reg_alpha': 1, 'n_estimators': 1000, 'min_child_weight': 10, 'max_depth': None, 'learning_rate': 0.1, 'gamma': 1, 'colsample_bytree': 0.5}\n```","44343a1a":"# 5. Hyperparameter Optimisation","acd443ed":"Max compressive strength in data set is 82.6MPa. Want to create a recipe that has a larger predicted compressive strength than this."}}