{"cell_type":{"4660550f":"code","1c9e701d":"code","d431a926":"code","25cba8a0":"code","9da3b7f1":"code","b4319937":"code","22c4a770":"code","631e8a4f":"code","cd07efb2":"code","9fc5a73a":"code","0231e4cf":"code","574a8360":"code","9dfe8032":"code","182f4471":"code","b0b9436a":"code","3957c673":"code","f2d8d826":"code","051e7bb1":"code","7050bcb7":"code","fe3b05ab":"code","22ae9e71":"code","2d33940e":"code","60bdaaa0":"code","2b3210f4":"code","30454959":"code","4b090f13":"code","0decb413":"code","0c45a484":"code","9e15297b":"code","50a27b6c":"code","6f707d3d":"code","f28724af":"code","1b9adfc3":"code","37650cb7":"code","08879216":"code","8af96016":"code","706b2a36":"code","a53bbda9":"code","43c5735c":"code","c913aa4b":"code","9713a3c9":"code","e11a30b1":"code","df322917":"code","429fbbeb":"markdown","0adc576c":"markdown","86c355ab":"markdown","8a854a4c":"markdown","0f6cfecc":"markdown","50397776":"markdown","12290def":"markdown","f641c4cd":"markdown","9e14a9be":"markdown","038a7095":"markdown","b1538347":"markdown","39ed6a86":"markdown","29c481d7":"markdown","ad3c5d74":"markdown","61c91e0b":"markdown","10e77eaf":"markdown","3a857ac0":"markdown","53474453":"markdown","6abc21c5":"markdown","ba4d8c3f":"markdown","e323646d":"markdown","dd687080":"markdown","3cc9b47d":"markdown","8c67f26d":"markdown","d9d1cc39":"markdown","04c42191":"markdown","145439e9":"markdown","f79d4d98":"markdown","7cac23ad":"markdown","c8cfc3e2":"markdown","03e94c89":"markdown","bff65c5b":"markdown","a69dae14":"markdown","4b81022b":"markdown","cb28d0cc":"markdown","54761135":"markdown","92e49399":"markdown","0739ec2f":"markdown","0f7ee534":"markdown","3ba1abd3":"markdown","a7c0acfb":"markdown","50a0f735":"markdown","f3fd5a29":"markdown","ac51f04b":"markdown","5eb68477":"markdown","34dd684c":"markdown","b77126c1":"markdown","d418afd8":"markdown"},"source":{"4660550f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns #visualizing\nimport matplotlib.pyplot as plt\nimport joblib\nimport plotly.offline as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected = True)\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1c9e701d":"#Read data\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\n\n#Storing Passenger Id for submision\nId = test.PassengerId","d431a926":"#Train set example\ntrain.head()","25cba8a0":"#Test set example\ntest.head()","9da3b7f1":"print('Train set shape = ' + str(train.shape[0]) +' Rows, ' + str(train.shape[1]) + ' Columns')\nprint('Test set shape = ' + str(test.shape[0]) +' Rows, ' + str(test.shape[1]) + ' Columns')\n\n#Test set has 1 columns less than Train set (Survived columns (predictions) is unknown)","b4319937":"#Let's look at brief summary of train set\nprint(train.info())\nprint(train.describe())","22c4a770":"train['Ticket'].value_counts()","631e8a4f":"cat_col = ['Pclass','Sex','Embarked']\nnum_col = ['Age','SibSp','Parch','Fare']\n","cd07efb2":"train[num_col].describe()","9fc5a73a":"#numerical data\nfig = plt.figure(figsize=(15,10))\nsns.set_style('darkgrid')\nfor index,col in enumerate(num_col):\n    plt.subplot(2,2,index+1)\n    sns.set(font_scale = 1.0)\n    sns.distplot(train[col],kde=False, bins = 20)\nfig.tight_layout(pad=1.0)","0231e4cf":"#numerical data\nfig = plt.figure(figsize=(15,10))\nsns.set_style('darkgrid')\nfor index,col in enumerate(num_col):\n    plt.subplot(2,2,index+1)\n    sns.boxplot(train[col])\nfig.tight_layout(pad=1.0)","574a8360":"#Categorical columns\nfig = plt.figure(figsize=(15,5))\nsns.set_style('darkgrid')\nfor index,col in enumerate(cat_col):\n    plt.subplot(1,3,index+1)\n    sns.set(font_scale = 1.0)\n    sns.countplot(train[col], palette='Set1')\nfig.tight_layout(pad=1.0)","9dfe8032":"fig = plt.figure(figsize=(20,10))\nplt.subplot(2,3,1)\nsns.barplot(x=\"Sex\", y=\"Survived\", data=train, palette = 'Set1')\nplt.subplot(2,3,2)\nsns.barplot(x=\"Pclass\", y=\"Survived\", data=train, palette = 'Set1')\nplt.subplot(2,3,3)\nsns.barplot(x=\"Parch\", y=\"Survived\", data=train, palette = 'Set1')\nplt.subplot(2,3,4)\nsns.barplot(x=\"SibSp\", y=\"Survived\", data=train, palette = 'Set1')\nplt.subplot(2,3,5)\nsns.barplot(x=\"Embarked\", y=\"Survived\", data=train, palette = 'Set1')","182f4471":"bins_fare = np.linspace(0,200,5)\nbins_age = np.linspace(0,80,5)\ntrain['binned_fare'] = pd.cut(train['Fare'],bins_fare)\ntrain['binned_age'] = pd.cut(train['Age'],bins_age)\ntrain.head()","b0b9436a":"fig = plt.figure(figsize=(20,8))\nplt.subplot(1,2,1)\nax = sns.pointplot(x=\"binned_fare\", y=\"Survived\", data=train)\nplt.subplot(1,2,2)\nax = sns.pointplot(x=\"binned_age\", y=\"Survived\", data=train)","3957c673":"df = pd.read_csv('..\/input\/titanic\/train.csv')","f2d8d826":"df.columns","051e7bb1":"#Check amount of missing values by columns\ndf.isnull().mean().sort_values(ascending=False)","7050bcb7":"X_train = df.drop(columns=['Survived']).copy()\ny_train = df['Survived'].copy()","fe3b05ab":"cat_col = ['Pclass','Sex','Embarked']\nnum_col = ['Fare','Age','SibSp','Parch']\nname_col = ['Name']","22ae9e71":"df['Embarked'].value_counts()","2d33940e":"df['Embarked'].value_counts()\nprint('S = ' + str((df['Embarked'].value_counts().S\/df['Embarked'].value_counts().sum()*100).round(2))+\"%\")\nprint('C = ' + str((df['Embarked'].value_counts().C\/df['Embarked'].value_counts().sum()*100).round(2))+\"%\")\nprint('Q = ' + str((df['Embarked'].value_counts().Q\/df['Embarked'].value_counts().sum()*100).round(2))+\"%\")","60bdaaa0":"from sklearn.impute import SimpleImputer\ncat_imputer = SimpleImputer(strategy = 'most_frequent')","2b3210f4":"num_imputer = SimpleImputer(strategy = 'median')","30454959":"df['Title'] = df['Name'].str.extract(' ([A-Za-z]+)\\.', expand = False)\ndf['Title'].unique().tolist()","4b090f13":"#Replace unfamilliar title with most familiar title\n\ntrain['Title'] = train['Name'].str.extract(' ([A-Za-z]+)\\.', expand = False)\ntrain['Title'] = train['Title'].replace(['Lady', 'Countess','Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ntrain['Title'] = train['Title'].replace('Mlle', 'Miss')\ntrain['Title'] = train['Title'].replace('Ms', 'Miss')\ntrain['Title'] = train['Title'].replace('Mme', 'Mrs')\n\nfig = plt.figure(figsize=(20,10))\nplt.subplot(1,2,1)\nsns.set(font_scale=1.5)\nsns.barplot(data=train,x='Title',y='Survived',hue='Pclass')\nplt.subplot(1,2,2)\nsns.set(font_scale=1.0)\nsns.boxplot(data=train,x='Title',y='Age')","0decb413":"from sklearn.base import BaseEstimator, TransformerMixin\n\n#Create custom transformer to extract title information from \"Name\" attribute\nclass TitleAttributeAdder(BaseEstimator,TransformerMixin):\n    def __init__(self):\n        pass\n    \n    def fit(self,X,y=None):\n        return self\n\n    def transform(self,X):\n        X_ = X.copy()\n        X_['Title'] = X_['Name'].str.extract(' ([A-Za-z]+)\\.', expand = False)\n        X_['Title'] = X_['Title'].replace(['Lady', 'Countess','Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n        X_['Title'] = X_['Title'].replace('Mlle', 'Miss')\n        X_['Title'] = X_['Title'].replace('Ms', 'Miss')\n        X_['Title'] = X_['Title'].replace('Mme', 'Mrs')\n        X_ = X_.drop(columns='Name')\n        return X_\n    \n#         title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n\n#         X_['Title'] = X_['Title'].map(title_mapping)\n\n#         # Imputing missing values with 0\n#         X_['Title'] = X_['Title'].fillna(0)\n        \n\n        ","0c45a484":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\n\n#Numerical pipeline\nnum_pipeline = Pipeline([\n    ('imputer', num_imputer), #num_imputer = imputed with median of each features\n    ('std_scaler', StandardScaler()) #z-score scaling\n])\n\n#Categorical pipeline\ncat_pipeline = Pipeline([\n    ('imputer', cat_imputer), #cat_imputer = imputed with most frequent object.\n    ('encoder', OneHotEncoder(handle_unknown='ignore')) #convert categorical columns into one hot vectors\n])\n\n#Name pipeline\nname_pipeline = Pipeline([\n    ('title_adder', TitleAttributeAdder()), #extract title from \"Name\" attribute\n    ('encoder', OneHotEncoder(handle_unknown='ignore')) #convert categorical columns into one hot vectors\n])\n\n#Finally put it into full pipeline to handle all preprocessing steps\nfull_pipeline = ColumnTransformer([\n    ('num', num_pipeline, num_col),\n    ('cat', cat_pipeline, cat_col),\n    ('name', name_pipeline, name_col)\n])","9e15297b":"X_train","50a27b6c":"X_train_prepared = full_pipeline.fit_transform(X_train)\nX_train_prepared = pd.DataFrame(X_train_prepared,columns = full_pipeline.transformers_[0][2]+\n                               full_pipeline.transformers_[1][1][1].get_feature_names().tolist()+\n                               full_pipeline.transformers_[2][1][1].get_feature_names().tolist())\nX_train_prepared.rename(columns = {'x0_1': 'Pclass_1',\n                                  'x0_2' : 'Pclass_2',\n                                  'x0_3' : 'Pclass_3',\n                                  'x1_female': 'female',\n                                  'x1_male': 'male',\n                                  'x2_C': 'Embarked_C',\n                                  'x2_Q': 'Embarked_Q',\n                                  'x2_S': 'Embarked_S'})","6f707d3d":"np.random.seed(1)\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import SGDClassifier, LogisticRegression\nfrom sklearn.svm import LinearSVC , SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\n\n\nmodel_list = [('SGDClassifier', SGDClassifier()),\n              ('LogisticsRegression', LogisticRegression()),\n         ('LinearSVC', LinearSVC()),\n         ('PolyKernelSVC', SVC(kernel = 'poly')),\n         ('RBFKernelSVC', SVC(kernel='rbf')),\n         ('KNN_Classifier', KNeighborsClassifier()),\n         ('RandomForestClassifer', RandomForestClassifier()),\n         ('XGBClassifier', xgb.XGBClassifier())\n        ]\nmodel_score = []\n\nfor m in model_list:\n    model = m[1]\n    score = cross_val_score(model,X_train_prepared,y_train,cv=4, scoring='accuracy')\n    print(f'{m[0]} score = {round(score.mean(),3)}')\n    model_score.append([m[0],score.mean()])\n    ","f28724af":"fig = plt.figure(figsize=(15,10))\nsns.set_style('darkgrid')\nsns.barplot(data = pd.DataFrame(data=model_score,columns = ['Model','Mean_Score']).sort_values(by='Mean_Score',ascending=False),\n           x='Mean_Score', y = 'Model',orient='h',palette='icefire')\nplt.xlim((0.7,0.85));","1b9adfc3":"#%%script false --no-raise-error\nfrom sklearn.model_selection import GridSearchCV\n\n#Hyperparameter to be tweaked for LogisticRegression\nparam_grid1 = [\n    {'C' : [0.1,0.3,1,3,10],\n    },\n    {'penalty':['elasticnet'],\n    'C' : [0.1,0.3,1,3,10],\n    'l1_ratio' : np.linspace(0,1,6)}\n]\n\ngrid_search1 = GridSearchCV(LogisticRegression(),param_grid1, cv=4, scoring = 'accuracy', return_train_score = True)\ngrid_search1.fit(X_train_prepared,y_train)","37650cb7":"#%%script false --no-raise-error\nLogisticRegression = grid_search1.best_estimator_\njoblib.dump(model , 'LogisticRegression.pkl')","08879216":"#%%script false --no-raise-error\n#Hyperparameter to be tweaked\nparam_grid2 = [{\n    'gamma' : [0.01,0.03,0.1,0.3,1],\n    'C' : [0.01,0.03,0.1,0.3,1,],\n}]\n\ngrid_search2 = GridSearchCV(SVC(kernel='rbf',probability=True),param_grid2, cv=4, scoring = 'accuracy', return_train_score = True, n_jobs=-1)\ngrid_search2.fit(X_train_prepared,y_train)","8af96016":"#%%script false --no-raise-error\nSVC_rbf = grid_search2.best_estimator_\njoblib.dump(model , 'SVC_rbf.pkl')","706b2a36":"#%%script false --no-raise-error\n#Hyperparameter to be tweaked\nparam_grid3 = [{\n    ' n_estimators=' : [100,300,500],\n    'max_depth' : [3,5,10],\n    'gamma' : [0.01, 0.03, 0.1],\n    'reg_lambda' : [0.1,0.3,0.5],\n    'reg_alpha': [0.1,0.3,0.5]\n}]\n\ngrid_search3 = GridSearchCV(xgb.XGBClassifier(),param_grid3, cv=4, scoring = 'accuracy', return_train_score = True, n_jobs=-1)\ngrid_search3.fit(X_train_prepared,y_train)","a53bbda9":"#%%script false --no-raise-error\nXGBC = grid_search3.best_estimator_\njoblib.dump(model , 'XGBC.pkl')","43c5735c":"from sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\n\n\ndef plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"\n    Generate 3 plots: the test and training learning curve, the training\n    samples vs fit times curve, the fit times vs score curve.\n\n    Parameters\n    ----------\n    estimator : object type that implements the \"fit\" and \"predict\" methods\n        An object of that type which is cloned for each validation.\n\n    title : string\n        Title for the chart.\n\n    X : array-like, shape (n_samples, n_features)\n        Training vector, where n_samples is the number of samples and\n        n_features is the number of features.\n\n    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n        Target relative to X for classification or regression;\n        None for unsupervised learning.\n\n    axes : array of 3 axes, optional (default=None)\n        Axes to use for plotting the curves.\n\n    ylim : tuple, shape (ymin, ymax), optional\n        Defines minimum and maximum yvalues plotted.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n          - None, to use the default 5-fold cross-validation,\n          - integer, to specify the number of folds.\n          - :term:`CV splitter`,\n          - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer\/None inputs, if ``y`` is binary or multiclass,\n        :class:`StratifiedKFold` used. If the estimator is not a classifier\n        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validators that can be used here.\n\n    n_jobs : int or None, optional (default=None)\n        Number of jobs to run in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    train_sizes : array-like, shape (n_ticks,), dtype float or int\n        Relative or absolute numbers of training examples that will be used to\n        generate the learning curve. If the dtype is float, it is regarded as a\n        fraction of the maximum size of the training set (that is determined\n        by the selected validation method), i.e. it has to be within (0, 1].\n        Otherwise it is interpreted as absolute sizes of the training sets.\n        Note that for classification the number of samples usually have to\n        be big enough to contain at least one sample from each class.\n        (default: np.linspace(0.1, 1.0, 5))\n    \"\"\"\n    if axes is None:\n        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n\n    axes[0].set_title(title)\n    if ylim is not None:\n        axes[0].set_ylim(*ylim)\n    axes[0].set_xlabel(\"Training examples\")\n    axes[0].set_ylabel(\"Score\")\n\n    train_sizes, train_scores, test_scores, fit_times, _ = \\\n        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n                       train_sizes=train_sizes,\n                       return_times=True)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    fit_times_mean = np.mean(fit_times, axis=1)\n    fit_times_std = np.std(fit_times, axis=1)\n\n    # Plot learning curve\n    axes[0].grid()\n    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n                         train_scores_mean + train_scores_std, alpha=0.1,\n                         color=\"r\")\n    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1,\n                         color=\"g\")\n    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n                 label=\"Training score\")\n    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n                 label=\"Cross-validation score\")\n    axes[0].legend(loc=\"best\")\n\n    # Plot n_samples vs fit_times\n    axes[1].grid()\n    axes[1].plot(train_sizes, fit_times_mean, 'o-')\n    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n                         fit_times_mean + fit_times_std, alpha=0.1)\n    axes[1].set_xlabel(\"Training examples\")\n    axes[1].set_ylabel(\"fit_times\")\n    axes[1].set_title(\"Scalability of the model\")\n\n    # Plot fit_time vs score\n    axes[2].grid()\n    axes[2].plot(fit_times_mean, test_scores_mean, 'o-')\n    axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1)\n    axes[2].set_xlabel(\"fit_times\")\n    axes[2].set_ylabel(\"Score\")\n    axes[2].set_title(\"Performance of the model\")\n\n    return plt\n\n\nfig, axes = plt.subplots(3, 3, figsize=(20, 20))\n\ntitle = \"Learning Curves (LogisticRegression)\"\n# Cross validation with 100 iterations to get smoother mean test and train\n# score curves, each time with 20% data randomly selected as a validation set.\ncv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\n\nplot_learning_curve(LogisticRegression, title, X_train_prepared, y_train, axes=axes[:, 0], ylim=(0.7, 1.01),\n                    cv=cv, n_jobs=4)\n\ntitle = r\"Learning Curves (SVM, RBF kernel)\"\n# SVC is more expensive so we do a lower number of CV iterations:\ncv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\nplot_learning_curve(SVC_rbf, title, X_train_prepared, y_train, axes=axes[:, 1], ylim=(0.7, 1.01),\n                    cv=cv, n_jobs=4)\n\ntitle = r\"Learning Curves (XGBClassifier)\"\n# SVC is more expensive so we do a lower number of CV iterations:\ncv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\nplot_learning_curve(XGBC, title, X_train_prepared, y_train, axes=axes[:, 2], ylim=(0.7, 1.01),\n                    cv=cv, n_jobs=4)\nplt.show()","c913aa4b":"from sklearn.ensemble import VotingClassifier\n\nestimators = [('LogisticRegression',LogisticRegression),\n              ('SVC_rbf',SVC_rbf),\n              ('XGBC',XGBC)]\n\nVClassifier = VotingClassifier(estimators,voting='soft')\nVClassifier.fit(X_train_prepared,y_train)","9713a3c9":"#transform test set as same way we did to training set\nX_test_prepared = full_pipeline.transform(test)\n\n#put into dataframe in case we need to look at it\nX_test_prepared = pd.DataFrame(X_test_prepared,columns = full_pipeline.transformers_[0][2]+\n                               full_pipeline.transformers_[1][1][1].get_feature_names().tolist()+\n                               full_pipeline.transformers_[2][1][1].get_feature_names().tolist())\nX_test_prepared.rename(columns = {'x0_1': 'Pclass_1',\n                                  'x0_2' : 'Pclass_2',\n                                  'x0_3' : 'Pclass_3',\n                                  'x1_female': 'female',\n                                  'x1_male': 'male',\n                                  'x2_C': 'Embarked_C',\n                                  'x2_Q': 'Embarked_Q',\n                                  'x2_S': 'Embarked_S'})\n\n#make prediction\nprediction = VClassifier.predict(X_test_prepared)","e11a30b1":"result = pd.DataFrame(prediction,test.PassengerId,columns=['Survived'])\nresult.to_csv('Voting_classifier_prediction.csv')\nresult\n","df322917":"#Prediction of each individual classifier for comparison\n\ny_log = LogisticRegression.predict(X_test_prepared)\ny_svm = SVC_rbf.predict(X_test_prepared)\ny_xgb = XGBC.predict(X_test_prepared)\n\nresult_log = pd.DataFrame(y_log,test.PassengerId,columns=['Survived'])\nresult_svm = pd.DataFrame(y_svm,test.PassengerId,columns=['Survived'])\nresult_xgb = pd.DataFrame(y_xgb,test.PassengerId,columns=['Survived'])\n\nresult_log.to_csv('LogisticRegression_prediction.csv')\nresult_svm.to_csv('SVB_rbf_prediction.csv')\nresult_xgb.to_csv('XGBC.csv')","429fbbeb":"## 3.1 Categorical attributes preprocessing","0adc576c":"## Bivariate analysis","86c355ab":"Data observation :\n* Most passenger were from class 3 \n* There were more male than female onboard\n* Most of passenger embarked at Southampton port following by Cherbourg and Queenstown","8a854a4c":"## 3.3 Feature engineering (extract title from \"Name\" attribute)\nWe also found something interesting. In columns 'Name' There is title embedded inside. so we'll try to extract information about title and see if it have some relation with survival rate.","0f6cfecc":"mainly will consist of 2 steps\n\n1. **Imputing** (filling missing value) by its most frequent. For example, \"Embarked\" attribute consists of 3 differrent port (C = Cherbourg, Q = Queenstown, S = Southampton) \n1. **OneHotEncoding** . Most machine learning algorithm can't intrepete text input, therefore we need to convert it into numerical in order to feed into the model ","50397776":"#  8. Conclusion <a id=8>","12290def":"As we can see here, our features have different characteristics as follow\n\n* Categorical features : Pclass , Sex , Embarked\n* Numerical feafutres : Fare, Age, #SipSp, #Parch\n\nTherefore, different type of attribute need to have different preprocessing steps as follow ","f641c4cd":"# 7. Prediction and Submission <a id=7>","9e14a9be":"Almost 70% of passenger are embarked in S. Therefore, we'll fill missing values with 'S'","038a7095":"Note that we keep out Name attribute for later since at first glance they might seem to be unique but I think that after preprocessing them we might see some pattern in it","b1538347":"### Dataset after transformation\n\nAfter transformantion, \n\n1. Numerical features has been imputed and Standard scaling\n2. Categorical features has been imputed and onehotencoding as shown\n3. Title has been extracted from \"Name\" attribute, and being onehotencoded too","39ed6a86":"Finally, with our voting classifier, we try to predict whether passenger in test set survived the catastrophe or not","29c481d7":"### Dataset before transformation\n\nBefore transformation, dataset is the same as we have imported during Exploratory Data Analysis steps","ad3c5d74":"I decided to drop Cabin feature. Since it contain lots of missing value and there is obscure method to imputes its missing value.\n\nMoreover, ticket attribute seem to have no meaning. It comprised of uninterpretable number and strings, so this will be dropped out too.\n","61c91e0b":"From our top 3 best perform model, I'll try to combine them into soft voting classifier improve overall predicting performance. The reasone behind this is that, the voting classifier often achieves a higher accuracy than individual predictors used to construct its.","10e77eaf":"# Titanics survivor prediction\n> as a part of Titanic: Machine Learning from Disaster competition (https:\/\/www.kaggle.com\/c\/titanic)","3a857ac0":"Title feature review some interesting insight...\n1. As we discovered before, female (Mrs,Miss) have higher chance of survival than male passenger with highest chance of survival belong to 1st class passenger\n1. Master title, which appeared to be children (from right boxplot) always survive for 1st and 2nd class passenger.\n\nfrom my point of view, it's possible that crew and male passengers focus their effort on helping children and woman which result in higher survival chance of children and woman.\n\n1st class passenger have highest chance of survival in all circumstance, it seem that priority had been given to 1st class and 2nd class passenger accordingly.","53474453":"<a class=\"anchor\" id=\"0\"><\/a>\n\n## Table of Contents\n\n1. [Data Exploration](#1)\n1. [Exploratory Data Analysis](#2)\n1. [Data Preprocessing](#3)\n1. [Model Selection](#4)\n1. [Hyperparameters tuning](#5)\n1. [Ensembling (Contruction of voting classifier)](#6)\n1. [Prediction and Submission](#7)\n1. [Conclusion](#8)\n","6abc21c5":"From above guideline, I've selected interesting algorithms as follow\n\n1. Stochastic Gradient Descent Classifier (SGDClassifier)\n1. LogisticRegression\n1. Linear Support Vector Classifier (Linear SVC)\n1. Support Vector Classifier with polynomial kernel\n1. Support Vector Classifier with Gaussian RBF kernel\n1. K-Nearest Neighbour Classifier (KNN)\n1. Random Forest Classifier\n1. Extreme Gradient Boosting Classifier (XGBClassifier)","ba4d8c3f":"Prediction of our ensemble model (Voting classifier) result in 77.99% accuracy, from the learning curves it could be that 2\/3 of our model seem a bit underfit the training set, while XGB Classifier seem overfitting.\n\nSo, in the future I would try several approach to improve model performance :\n\n1. Try to increasing more regularization parameters for XGB Classifier model\n1. Try more feature engineering to add more interesting feature to the model (i.e. combine #Parch and #Sibsp into family size, try to extract information from cabin feature)\n1. Try adding polynomail features\n1. Train a neural network to tackle the problem.","e323646d":"## Univariate analysis","dd687080":"## 5.1 Logistic Regression","3cc9b47d":"# 3.Data preprocessing <a id=3>","8c67f26d":"We'll compare model performance from Scikit-learn model selection guideline as shown below\n\n![Guideline](https:\/\/scikit-learn.org\/stable\/_static\/ml_map.png)\nsee more at : https:\/\/scikit-learn.org\/stable\/tutorial\/machine_learning_map\/index.html","d9d1cc39":"# 4. Model selection <a id=4>","04c42191":"[back to top](#0)","145439e9":"# 6. Ensembling (Construction of Voting classifier) <a id=6>","f79d4d98":"## 5.3 XGB Classifier","7cac23ad":"## Dataset explanation\n\nDataset is provided by Kaggle as a part of \"Titanic: Machine Learning from Disaster\" competition. The goal of this model is to predict whether given passenger will survive the disaster or not based on following features.\n\n1. Survival\t: 0 = No, 1 = Yes\n1. name     : Passenger name including prefix\n1. pclass\t: A proxy for socio-economic status (SES) 1 = 1st, 2 = 2nd, 3 = 3rd\n1. Sex\t    : Male \/ Female\n1. Age\t    : Age in years\t(Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5)\n1. sibsp\t: # of siblings \/ spouses aboard the Titanic\t\n1. parch\t: # of parents \/ children aboard the Titanic\t\n1. ticket\t: Ticket number\t\n1. fare\t    : Passenger fare\t\n1. cabin\t: Cabin number\t\n1. embarked\t: Port of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southampton","c8cfc3e2":"From above outline : Several attribute need to be checked as they contain some missing value\n1. Age\n2. Cabin\n3. Embarked","03e94c89":"## 3.2 Numerical attributes preprocessing","bff65c5b":"**By comparing default algorithm configuration ...** <br>\n\nFrom average cross validation score, we'll select top 3 model to construct voting classifier. \nIn order to get best performance out of voting classifier, predictors shall be trained with different algorithms.\nHowever SVC with RBF and Poly kernel are from the same classifier (with different kernel). Therefore, we'll pick 4th classifier (XGBClassifier) instead of PolyKernelSVC to construct our voting classifier","a69dae14":"Some of the titles above are quite unfamiliar, so we decide to replace these titles with more familiar titles which organizes our categories. After replacing these titles we only end up with 5 distinct titles.","4b81022b":"## 5.2 Support Vector Classifier with RBF kernel","cb28d0cc":"# 2.Exploratory Data Analysis <a id=2>","54761135":"Data observation\n* Age : Traveller on board Titanic have wide range of age from infants to old person with mean and median around 28-30 years old\n* SibSp and Parch : show that most of passengers were not travelling with their family (low # of SibSp and Parch) since friend, mistresses and fianc\u00e9s were ignored. We can not conclude that passengers were travelling alone\n* Fare : ticket fare were widely spread with mean of 32 and median just about 15\n","92e49399":"Data preprocessing steps is very exhaustive and consumed a lot of time. After deciding which preprocessing method needed for each data type, I decide to put preprocessing steps into pipeline to automate the workflow.","0739ec2f":"We'll use exhausive grid seacrh cross validiation method (GridSearchCV) to find best combination of hyperparameters for our algorithms <br>\n\nFor quick committing, I've run GridSearchCV to find optimum hyperparameters and save model for later used in order to recall saved model to train quickly.","0f7ee534":"## 3.4 Combine preprocessing steps into pipeline","3ba1abd3":"Logistic Regression and SVM with RBF kernel seem a bit underfit while XGB classifier is likely to overfit the training set. <br>\n\nIn term of training speed and scalability, LogisticRegression ;with its simplicity; has fastest training speed following by XGB classifier and SVC with rbf kernel","a7c0acfb":"Default setting of Logistic Regression seem to perform best so we keep using it defualt configuration for next step","50a0f735":"![Titanic ship](https:\/\/www.englishclub.com\/efl\/wp-content\/uploads\/2020\/04\/titanic.png)","f3fd5a29":"# 1. Data Exploration <a id=1>","ac51f04b":"Data Observation:\n* Sex : Female has significantly higher rate of survival than male\n* Pclass : 1st class passenger has highest rate of survival follow by 2nd class and 3rd class respectively\n* #Parch : doesn't show strong relationship with survival rate\n* #SibSp : as # of siblings \/ spouses increased, survival rate tend to decrease\n* Embarked port : Passenger embarked from Cherbourg has highest rate of survival\n* Fare : passenger with fare in range [100,150] has highest survival rate\n* age : Elderly people has significant lower rate of survival than adult and younger people","5eb68477":"* Columns 'Cabin' has more than 77% missing so we'll drop out this columns from our consideration\n* Columns 'Age' is missing about 20%. But they're imputable, so we dicide to keep it for further processing\n* Columns 'Embarked' and 'Fare' almost perfect (missing less than 1%). We'll keep these columns for further processing","34dd684c":"For numerical attributes, I decide to create preprocessing steps as follow,\n\n1. **Imputing** by its median. Because some attribute contains outlier, filling with median would have less effect to attribute distribution\n1. **StandardScaling** , Most machine learning algorithm work well when input features having same scale and standard deviation, so I decide to transform numerical data by standard scaling.","b77126c1":"# Introduction\n\n\"The sinking of the Titanic is one of the most infamous shipwrecks in history.On April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. \n\nUnfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.<br>While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\" [1]\n\nThis notebook will try to find answer to the question that which characteristics of passengers most contribute to their chance of survival ann in the end we'll try to predict wheter given passenger will survive the catastrophe or not?\n\n\n","d418afd8":"# 5. Hyperparameter tuning <a id=5>"}}