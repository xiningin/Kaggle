{"cell_type":{"ec023a45":"code","f722fb54":"code","cb420fb5":"code","e00e153b":"code","f16cde25":"code","d05313c6":"code","dd412d42":"code","b2e3df4f":"code","04897bf2":"code","25b28c6d":"code","8958e729":"code","c2f43cc3":"code","ffe76e52":"code","cda40e9e":"code","f1005472":"code","ef9c6b46":"code","f5e3e25d":"code","c31759b1":"code","5bc73701":"code","c42a4bba":"code","7c32f0a0":"code","ab4c86e2":"code","b1db385b":"markdown","a07e5443":"markdown","c3ddee1f":"markdown","896b6923":"markdown","05263007":"markdown","85b60db6":"markdown","aea219b4":"markdown","ff402a61":"markdown","4b7a8205":"markdown","e517b1f9":"markdown","b38922a0":"markdown","f644b664":"markdown","b82e4882":"markdown","bc37e585":"markdown","eabb6d66":"markdown"},"source":{"ec023a45":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f722fb54":"!pip install openpyxl","cb420fb5":"!pip install Lifetimes","e00e153b":"# To start with importing libraries which are essential for the case\nimport datetime as dt\nimport pandas as pd\nfrom sqlalchemy import create_engine\nfrom sklearn.preprocessing import MinMaxScaler","f16cde25":"# Reading the excel format data set by using \"Year 2010-2011\" sheet named.\ndf_ = pd.read_excel(\"..\/input\/online-retail-ii-data-set-from-ml-repository\/online_retail_II.xlsx\", sheet_name=\"Year 2010-2011\")\ndf = df_.copy()","d05313c6":"# Browsing the dataset \ndf.head()\n\n# Examining descriptive statistics\ndf.describe().T\n\n# Detection of missing variables\ndf.isnull().sum()\n\n# Remowing of missing variables\ndf.dropna(inplace=True)\n\n# Removing canceled transactions from the dataset\ndf = df[~df[\"Invoice\"].str.contains(\"C\", na=False)]\ndf.describe().T","dd412d42":"# Let's use the following function to find outliers.\ndef outlier_thresholds(dataframe, variable):\n    quartile1 = dataframe[variable].quantile(0.01)\n    quartile3 = dataframe[variable].quantile(0.99)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit","b2e3df4f":"# Replace function for outliers with thresholds.\ndef replace_with_thresholds(dataframe, variable):\n    low_limit, up_limit = outlier_thresholds(dataframe, variable)\n    dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit\n    dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit","04897bf2":"# Replacing \"Quantity\" and \"Price\"\nreplace_with_thresholds(df, \"Quantity\")\nreplace_with_thresholds(df, \"Price\")","25b28c6d":"# Checking the updated dataset\ndf.describe().T","8958e729":"# Let's add a new variable to the data set to see the total price for each row.\ndf[\"TotalPrice\"] = df[\"Quantity\"] * df[\"Price\"]\ndf.head()","c2f43cc3":"# When was the last time a customer made a purchase?\ndf[\"InvoiceDate\"].max()","ffe76e52":"# In order to get logical results and avoid zero values, we set 2 days later as today's date.\ntoday_date = dt.datetime(2011, 12, 11)","cda40e9e":"# We obtain CLTV metrics by grouping \"Customer IDs\" with aggregation methods that I use specific to each metric.\ncltv_df = df.groupby('Customer ID').agg({'InvoiceDate': [lambda date: (date.max() - date.min()).days,\n                                                         lambda date: (today_date - date.min()).days],\n                                         'Invoice': lambda num: num.nunique(),\n                                         'TotalPrice': lambda TotalPrice: TotalPrice.sum()})\ncltv_df.head()","f1005472":"# Naming columns with the metrics\ncltv_df.columns = cltv_df.columns.droplevel(0)\ncltv_df.columns = ['recency', 'T', 'frequency', 'monetary']\ncltv_df.head()","ef9c6b46":"# Some transformations\ncltv_df[\"monetary\"] = cltv_df[\"monetary\"] \/ cltv_df[\"frequency\"] # Expressing monetary value as average earnings per purchase\n\ncltv_df = cltv_df[cltv_df[\"monetary\"] > 0]# Choosing monetary values greater than zero\n\ncltv_df = cltv_df[(cltv_df['frequency'] > 1)] # Choosing frequency values greater than one\n\ncltv_df[\"recency\"] = cltv_df[\"recency\"] \/ 7 # Expression of recency and T for BGNBD in weekly terms\ncltv_df[\"T\"] = cltv_df[\"T\"] \/ 7","f5e3e25d":"from lifetimes import BetaGeoFitter\n\nbgf = BetaGeoFitter(penalizer_coef=0.001)\n\nbgf.fit(cltv_df['frequency'],\n        cltv_df['recency'],\n        cltv_df['T'])\n\n# Applying the model to the data set and adding as a variable\n# Calculation of expected 1 month of purchase\ncltv_df[\"expected_purc_1_month\"] = bgf.predict(4,\n                                              cltv_df['frequency'],\n                                              cltv_df['recency'],\n                                              cltv_df['T'])\ncltv_df.head()","c31759b1":"from lifetimes import GammaGammaFitter\n\nggf = GammaGammaFitter(penalizer_coef=0.01)\n\nggf.fit(cltv_df['frequency'], cltv_df['monetary'])\n\n# Applying the model to the dataset\nggf.conditional_expected_average_profit(cltv_df['frequency'],\n                                        cltv_df['monetary']).sort_values(ascending=False).head(10)\n\n# Adding model as a variable \ncltv_df[\"expected_average_profit\"] = ggf.conditional_expected_average_profit(cltv_df['frequency'],\n                                                                             cltv_df['monetary'])\ncltv_df.head()","5bc73701":"# Prediction of 1 month CLTV\ncltv_1 = ggf.customer_lifetime_value(bgf,\n                                   cltv_df['frequency'],\n                                   cltv_df['recency'],\n                                   cltv_df['T'],\n                                   cltv_df['monetary'],\n                                   time=1,  \n                                   freq=\"W\",  \n                                   discount_rate=0.01)\n\ncltv_1.sort_values(ascending=False).head(10)","c42a4bba":"# Prediction of 12 months CLTV\ncltv_12 = ggf.customer_lifetime_value(bgf,\n                                   cltv_df['frequency'],\n                                   cltv_df['recency'],\n                                   cltv_df['T'],\n                                   cltv_df['monetary'],\n                                   time=12,  # 12 ayl\u0131k\n                                   freq=\"W\",  # T'nin frekans bilgisi.\n                                   discount_rate=0.01)\n\ncltv_12.sort_values(ascending=False).head(10)","7c32f0a0":"# Prediction of 6 months CLTV\ncltv = ggf.customer_lifetime_value(bgf,\n                                   cltv_df['frequency'],\n                                   cltv_df['recency'],\n                                   cltv_df['T'],\n                                   cltv_df['monetary'],\n                                   time=6,  # 6 months\n                                   freq=\"W\",  # Frequency information of T as a weekly basis\n                                   discount_rate=0.01)\n\ncltv.sort_values(ascending=False).head()\n","ab4c86e2":"# Let's delete the indexes\ncltv = cltv.reset_index()\n\n# Let's merge our cltv and cltv_df dataframes into a new dataframe.\ncltv_final = cltv_df.merge(cltv, on=\"Customer ID\", how=\"left\")\n\n# Let's divide the clv variable into 4 equal parts with the help of qcut and assign it to the variable named segment \n# with the help of labels, with the smallest value \"D\".\ncltv_final[\"segment\"] = pd.qcut(cltv_final[\"clv\"], 4, labels=[\"D\", \"C\", \"B\", \"A\"])\n\n# Let's reduce our \"clv\" variable to 0-1 and assign it to df with a new variable named scaled_clv.\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaler.fit(cltv_final[[\"clv\"]])\ncltv_final[\"scaled_clv\"] = scaler.transform(cltv_final[[\"clv\"]])\n\n# Let's sort our df according to scaled_clv from largest to smallest.\ncltv_final.sort_values(by=\"scaled_clv\", ascending=False).head()\n\n# For advice, let's group our df by segment and look at the mean and total values of the variables.\ncltv_final[[\"segment\", \"recency\",\"frequency\",\"T\",\"monetary\",\"expected_average_profit\",\"scaled_clv\"]].groupby(\"segment\").agg(\n    {\"count\", \"mean\", \"sum\"})","b1db385b":"> What should these functions tell us?\nWhat we call outliers can be called values that are far outside the general distribution that distorts our data set. Since these values appear very few or rare, they prevent us from generalizing about the data set. We can either extract these values directly from the data set or suppress them to threshold values as I did in this example.","a07e5443":"**Combination of BGNBD and Gamma-Gamma Models**\n\nNow let's calculate 1-month and 12-month CLTV for 2010-2011 UK customers, analyze the highest people in 1-month CLTV and 12 months, and interpret if there is a difference between them.","c3ddee1f":"# Customer Lifetime Value","896b6923":"**Preparation of the BGNBD model**","05263007":"# Case Study with CLTV","85b60db6":"> I want to focus on segment A and C. As can be seen, the shopping frequency and monetary value average of the A segment has the highest value. For this segment, monetary increase can be made by offering products similar to the products they bought before through the recommendation systems in a 6-month plan. Or, it can be tried to increase the frequency average by applying special discounts to the products they buy the most.\nAs I said before, losing customers is much more costly than retaining customers. Because finding new customers requires making significant expenses here. For this reason, we see that the C segment has slightly lower averages than the B segment. In order to raise this segment to a higher segment, a 6-month incentive policy can be arranged. In order for customers to shop again, the products they need can be analyzed and discount messages can be sent to customers via e-mail and sms, or discount checks can be sent. We can think that any policy to be made for a segment will also affect other segments and encourage them to shop.","aea219b4":"**Now that we are done with understanding and preparing the data, we can start interpreting the data.**\n\nNow we can find CLTV metrics to make data propered to methods for prediction.","ff402a61":"> When the 1-month and 12-month cltv forecasts are examined, we can see that the results are not much different from each other. So this does not mean that customer value is not variable. In fact, the result we expected was an increase in customer value. The conclusion we need to draw from this is that our data set is not suitable for long-term predictions. For short time intervals, as in our example, we can easily rely on the results for the 1-month period.","4b7a8205":"Now, let's divide all our customers into 4 groups (segments) according to 6-month CLTV for the same customer group and make some action suggestions for 2 groups that I consider important.","e517b1f9":"Customer lifetime value is actually a term used to calculate how much money the customer will leave your company during its existence. CLTV has four metrics which are:\n\n**Recency:** \nIt refers to the time spent by the customer over the last purchase. It is calculated on a weekly basis. The difference from RFM is that it is calculated on a user-specific basis.\n\n**T:**\nIndicates the age of the customer. In other words, it is the difference between the customer's most recent purchase date and the first purchase date. It is calculated on a weekly basis.\n\n**Frequency:**\nIt refers to the total number of repeat purchases of the customer in a certain time period. In order to make a healthy analysis, \"frequency>1\" values should be given weight to.\n\n**Monetary_value:**\nIt represents our average earnings per customer's purchase.\n\n**Why do we need to calculate this?**\nIt is generally said that retaining customers is more profitable than finding new ones. For this reason, we can plan to keep the customer with the right product marketing by analyzing customer needs, and we can go further and predict with the right strategies how much we can retain the customer and how much money the customer will leave to our company. \n\n**What is BGNBD?**\nIt can be used to predict repeat visits for customers to determine the lifetime value of customers. In addition, the data allows us to understand whether the customer will come again.\n\n**What is GG?**\nWith the Gamma-Gamma model, we calculate how much money the customer will leave to the company on average in a certain time period.","b38922a0":"> It can still be seen that the max values of Quantity and Price are well above the average. In addition, even when we look at other percentages, we see that the max values are very high. We should call these values as outliers. I will compress outliers with the replace threshold function to normalize the dataset.","f644b664":"> As you can see, we ran into an unpleasant situation when naming the columns. For this we need to remove the first column naming index and rename the columns.","b82e4882":"I will go through it quickly here as I'm doing the same data preprocessing on my RFM notebook.","bc37e585":"Using the Online-Retail-II dataset, which is the dataset of an e-commerce site. I tried to identify the customers who could bring the most revenue within 1-month or 6-month time periods.\n\nThe dataset named Online-Retail-II, which I also used in my previous RFM notebook, includes the sales of an online store between 01\/12\/2009 - 09\/12\/2011.","eabb6d66":"**Preparation of the Gamma-Gamma model**"}}