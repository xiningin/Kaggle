{"cell_type":{"e0368e38":"code","e4604cd1":"code","85b6b561":"code","ffb8ccc3":"code","677a6b89":"code","5b5635e7":"code","1424adbb":"code","e1b351f9":"code","a605639c":"code","78d22eb3":"code","c27e29e9":"code","1abdf5ba":"code","50f014ca":"code","227fbcfc":"code","89163373":"code","5c134cde":"code","2c386cc5":"code","9c7a407c":"code","75064d2b":"code","0079ccd7":"code","5211325b":"code","f717fae9":"code","972af2d6":"code","f72543e1":"code","7e073433":"code","68fe6b7a":"code","6e7f93a2":"code","156ae8c7":"code","ca78088a":"code","b0dd2f8b":"code","f17a0eb8":"code","2c01ed0e":"code","e2c4db85":"code","235acbc5":"code","65edc363":"code","144a54f9":"code","717a243b":"code","aa561f15":"code","5838ed69":"code","becb032b":"code","d52f89dd":"code","6875639f":"code","ebc46917":"code","e4527101":"code","fbbce80c":"code","68753737":"code","040ced03":"code","56a4b9bf":"code","a301e641":"code","f35f47fe":"code","64e83456":"code","dfca5a19":"code","6b236504":"code","0f261e56":"code","899c8019":"code","65063f6c":"code","1ecc2dc0":"code","c0af439f":"code","61cbd411":"code","664c133f":"code","afa1db9d":"code","c1ab5f2e":"code","0681d956":"code","18c2702f":"code","3b859bef":"code","15701dd6":"code","06150cc1":"code","2b6f0f71":"code","f60b4d71":"code","269ee767":"code","570203e1":"code","83ea5b1c":"code","016eabf9":"code","582ca57c":"code","fa288e36":"code","5551ff94":"markdown","51f925d2":"markdown","2b2fc062":"markdown","69cecf43":"markdown","b29f6ba6":"markdown"},"source":{"e0368e38":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e4604cd1":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nimport pandas as pd\nimport nltk\nimport random\nimport os\nfrom os import path\nfrom PIL import Image\n\n# Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom subprocess import check_output\nfrom wordcloud import WordCloud, STOPWORDS\n\n# Set Plot Theme\nsns.set_palette([\n    \"#30a2da\",\n    \"#fc4f30\",\n    \"#e5ae38\",\n    \"#6d904f\",\n    \"#8b8b8b\",\n])\n\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\nimport re\nfrom nltk.stem import PorterStemmer\n\n# Modeling\nimport statsmodels.api as sm\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.sentiment.util import *\nfrom nltk.util import ngrams\nfrom collections import Counter\nfrom gensim.models import word2vec\n\n# Warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nimport nltk\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dense , Dropout, Bidirectional,SpatialDropout1D,Flatten\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n","85b6b561":"df = pd.read_csv(\"\/kaggle\/input\/womens-ecommerce-clothing-reviews\/Womens Clothing E-Commerce Reviews.csv\",index_col=0)","ffb8ccc3":"df.head()","677a6b89":"plt.figure(figsize=(10,6))\nsns.heatmap(df.isnull(),cmap='viridis')\nplt.show()","5b5635e7":"print(\"Dataframe Dimension: {} Rows, {} Columns\".format(*df.shape))","1424adbb":"df[[\"Title\", \"Division Name\",\"Department Name\",\"Class Name\"]].describe(include=[\"O\"]).T.drop(\"count\",axis=1)","e1b351f9":"sns.countplot(df['Recommended IND'])\nplt.title(\"Count of recommended vs non recommended items\")\nplt.show()","a605639c":"# Continous Distributions\nfig = plt.figure(figsize=(20,14))\nax1 = plt.subplot2grid((2,2),(0,0))\nax1 = sns.distplot(df[\"Positive Feedback Count\"])\nax1 = plt.title(\"Positive Feedback Count Distribution\")\n\nax2 = plt.subplot2grid((2,2),(0,1))\nax2 = sns.distplot(df['Age'])\nax2 = plt.title(\"Age distribution\")\n\nax3 = plt.subplot2grid((2,2),(1,0),colspan=2)\nax3 = sns.distplot(np.log10((df[\"Positive Feedback Count\"][df[\"Positive Feedback Count\"].notnull()]+1)))\nax3 = plt.title(\"Log Positive Feedback count\")\n\nplt.show()","78d22eb3":"def percentage_accumulation(series, percentage):\n    return (series.sort_values(ascending=False)\n            [:round(series.shape[0]*(percentage\/100))]\n     .sum()\/series\n     .sum()*100)\n\n# Gini Coefficient- Inequality Score\n# Source: https:\/\/planspace.org\/2013\/06\/21\/how-to-calculate-gini-coefficient-from-raw-data-in-python\/\ndef gini(list_of_values):\n    sorted_list = sorted(list_of_values)\n    height, area = 0, 0\n    for value in sorted_list:\n        height += value\n        area += height - value \/ 2.\n    fair_area = height * len(list_of_values) \/ 2.\n    return (fair_area - area) \/ fair_area\n","c27e29e9":"inequality = []\nfor x in list(range(100)):\n    inequality.append(percentage_accumulation(df[\"Positive Feedback Count\"], x))","1abdf5ba":"plt.plot(inequality)\nplt.title(\"Percentage of Positive Feedback by Percentage of Reviews\")\nplt.xlabel(\"Review Percentile starting with Feedback\")\nplt.ylabel(\"Percent of Positive Feedback Received\")\nplt.axvline(x=20, c = \"r\")\nplt.axvline(x=53, c = \"g\")\nplt.axhline(y=78, c = \"y\")\nplt.axhline(y=100, c = \"b\", alpha=.3)\nplt.show()\nprint(\"{}% of Positive Feedback belongs to the top 20% of Reviews\".format(\n    round(percentage_accumulation(df[\"Positive Feedback Count\"], 20))))\n\n# Gini\nprint(\"\\nGini Coefficient: {}\".format(round(gini(df[\"Positive Feedback Count\"]),2)))","50f014ca":"fig = plt.figure(figsize=(14,8))\nax1 = plt.subplot2grid((2,2),(0,0))\nax1 = sns.heatmap(pd.crosstab(df['Division Name'],df['Department Name']),cmap='Purples',annot=True, linewidths=.5,fmt='g',\n                cbar_kws={'label': 'Count'})\nax1 = plt.title('Division Name Count by Department Name - Crosstab\\nHeatmap Overall Count Distribution')\n\nax2 = plt.subplot2grid((2,2),(0,1))\nax2 = sns.heatmap(pd.crosstab(df['Division Name'],df['Department Name'],normalize=True).mul(100).round(0),cmap='Purples',annot=True,fmt='g',cbar_kws={'label': 'Percentage %'})\nax2 = plt.title(\"Division Name Count by Department Name - Crosstab\\nHeatmap Overall Percentage Distribution\")\nplt.tight_layout(pad=0)\nplt.show()\n","227fbcfc":"fig = plt.figure(figsize=(14,8))\nax1 = plt.subplot2grid((2,2),(0,0))\nax1 = sns.heatmap(pd.crosstab(df['Division Name'], df[\"Department Name\"], normalize='columns').mul(100).round(0),\n            annot=True, linewidths=.5,fmt='g', cmap=\"Purples\",cbar_kws={'label': 'Percentage %'})\nax1 = plt.title('Division Name Count by Department Name - Crosstab\\nHeatmap % Distribution by Columns')\n\nax2 = plt.subplot2grid((2,2),(0,1))\nax2 = sns.heatmap(pd.crosstab(df['Division Name'], df[\"Department Name\"], normalize='index').mul(100).round(0),\n            annot=True, linewidths=.5,fmt='g', cmap=\"Purples\",cbar_kws={'label': 'Percentage %'})\nax2 = plt.title(\"Division Name Count by Department Name - Crosstab\\nHeatmap % Distribution by Index\")\nplt.tight_layout(pad=0)\nplt.show()\n","89163373":"fig = plt.figure(figsize=(14,8))\nax1 = plt.subplot2grid((2,2),(0,0))\nax1 = sns.heatmap(pd.crosstab(df['Class Name'],df['Department Name']),cmap='inferno_r',annot=True, linewidths=.5,fmt='g',\n                cbar_kws={'label': 'Count'})\nax1 = plt.title('Class Name Count by Department Name - Crosstab\\nHeatmap Overall Count Distribution')\n\nax2 = plt.subplot2grid((2,2),(0,1))\nax2 = sns.heatmap(pd.crosstab(df['Class Name'],df['Department Name'],normalize=True).mul(100).round(0),cmap='inferno_r',annot=True,fmt='g',cbar_kws={'label': 'Percentage %'})\nax2 = plt.title(\"Class Name Count by Department Name - Crosstab\\nHeatmap Overall Percentage Distribution\")\nplt.tight_layout(pad=0)\nplt.show()","5c134cde":"fig = plt.figure(figsize=(14,8))\nax1 = plt.subplot2grid((2,2),(0,0))\nax1 = sns.heatmap(pd.crosstab(df['Class Name'], df[\"Department Name\"], normalize='columns').mul(100).round(0),\n            annot=True, linewidths=.5,fmt='g', cmap=\"nipy_spectral_r\",cbar_kws={'label': 'Percentage %'})\nax1 = plt.title('Class Name Count by Department Name - Crosstab\\nHeatmap % Distribution by Columns')\n\nax2 = plt.subplot2grid((2,2),(0,1))\nax2 = sns.heatmap(pd.crosstab(df['Class Name'], df[\"Department Name\"], normalize='index').mul(100).round(0),\n            annot=True, linewidths=.5,fmt='g', cmap=\"nipy_spectral_r\",cbar_kws={'label': 'Percentage %'})\nax2 = plt.title(\"Class Name Count by Department Name - Crosstab\\nHeatmap % Distribution by Index\")\nplt.tight_layout(pad=0)\nplt.show()\n","2c386cc5":"fig = plt.figure(figsize=(14,8))\nax1 = plt.subplot2grid((2,2),(0,0))\nax1 = sns.heatmap(pd.crosstab(df['Class Name'],df['Division Name']),cmap='cubehelix_r',annot=True, linewidths=.5,fmt='g',\n                cbar_kws={'label': 'Count'})\nax1 = plt.title('Class Name Count by Division Name - Crosstab\\nHeatmap Overall Count Distribution')\n\nax2 = plt.subplot2grid((2,2),(0,1))\nax2 = sns.heatmap(pd.crosstab(df['Class Name'],df['Division Name'],normalize=True).mul(100).round(0),cmap='cubehelix_r',annot=True,fmt='g',cbar_kws={'label': 'Percentage %'})\nax2 = plt.title(\"Class Name Count by Division Name - Crosstab\\nHeatmap Overall Percentage Distribution\")\nplt.tight_layout(pad=0)\nplt.show()","9c7a407c":"fig = plt.figure(figsize=(14,8))\nax1 = plt.subplot2grid((2,2),(0,0))\nax1 = sns.heatmap(pd.crosstab(df['Class Name'], df[\"Division Name\"], normalize='columns').mul(100).round(0),\n            annot=True, linewidths=.5,fmt='g', cmap=\"ocean_r\",cbar_kws={'label': 'Percentage %'})\nax1 = plt.title('Class Name Count by Division Name - Crosstab\\nHeatmap % Distribution by Columns')\n\nax2 = plt.subplot2grid((2,2),(0,1))\nax2 = sns.heatmap(pd.crosstab(df['Class Name'], df[\"Division Name\"], normalize='index').mul(100).round(0),\n            annot=True, linewidths=.5,fmt='g', cmap=\"ocean_r\",cbar_kws={'label': 'Percentage %'})\nax2 = plt.title(\"Class Name Count by Division Name - Crosstab\\nHeatmap % Distribution by Index\")\nplt.tight_layout(pad=0)\nplt.show()","75064d2b":"def minmaxscaler(df):\n    return (df-df.min())\/(df.max()-df.min())\ndef zscorenomalize(df):\n    return (df - df.mean())\/df.std()\n\n","0079ccd7":"df.describe()","5211325b":"print(df.info())","f717fae9":"sns.set(rc={'figure.figsize':(11,4)})\npd.isnull(df).sum().plot(kind='bar',color='royalblue')\nplt.ylabel('count')\nplt.title('Missing values')\nplt.show()","972af2d6":"sns.set(rc={'figure.figsize':(16,6)})\nplt.hist(df['Age'],bins=50,color='royalblue')\nplt.xlabel(\"Age\")\nplt.ylabel(\"Count\")\nplt.title(\"Age Distribution\")\nplt.show()","f72543e1":"sns.set(rc={\"figure.figsize\":(14,6)})\nsns.boxplot(x='Rating',y='Age',data=df)\nplt.title('Rating Distribution per Age')\nplt.show()","7e073433":"sns.countplot(df['Rating'])\nplt.title(\"Rating count\")\nplt.show()","68fe6b7a":"fig = plt.figure(figsize=(20,14))\nax1 = plt.subplot2grid((2,2),(0,0))\n# ax1 = plt.xticks(rotation=90)\nax1 = sns.countplot(df['Division Name'])\nax1 = plt.title(\"Review in each division\")\n\nax2 = plt.subplot2grid((2,2),(0,1))\nax2 = plt.xticks(rotation=90)\nax2 = sns.countplot(df['Department Name'])\nax2 = plt.title(\"Review in each department\")\n\nax3 = plt.subplot2grid((2,2),(1,0),colspan=2)\nax3 = plt.xticks(rotation=90)\nax3 = sns.countplot(df['Class Name'])\nax3 = plt.title(\"Reviews in each Class\")\n","6e7f93a2":"df.head(2)","156ae8c7":"recommended = df[df['Recommended IND']==1]\nnot_recommended = df[df['Recommended IND']==0]","ca78088a":"recommended","b0dd2f8b":"fig = plt.figure(figsize=(20, 14))\nax1 = plt.subplot2grid((2, 2), (0, 0))\nax1 = sns.countplot(recommended['Division Name'],color='red',alpha=1,label=\"recommended\")\nax1 = sns.countplot(not_recommended['Division Name'],color='black',alpha=1,label=\"non-recommended\")\nax1 = plt.title(\"Recommended Items in each Division\")\nax1 = plt.legend(loc='best')\n\nax2 = plt.subplot2grid((2, 2), (0, 1))\nax2 = sns.countplot(recommended['Department Name'],color='blue',alpha=1,label=\"recommended\")\nax2 = sns.countplot(not_recommended['Department Name'],color='black',alpha=1,label=\"non-recommended\")\nax2 = plt.title(\"Recommended Items in each Department\")\nax2 = plt.legend(loc='best')\n\nax3 = plt.subplot2grid((2, 2), (1, 0),colspan=2)\nax3 = sns.countplot(recommended['Class Name'],color='orange',alpha=1,label=\"recommended\")\nax3 = sns.countplot(not_recommended['Class Name'],color='black',alpha=1,label=\"non-recommended\")\nax3 = plt.title(\"Recommended Items for each class\")\nax3 = plt.legend(loc='best')","f17a0eb8":"def percentstandardize_barplot(x,y,hue, data, ax=None, order= None):\n    sns.barplot(x= x, y=y, hue=hue, ax=ax, order=order,\n    data=(data[[x, hue]]\n     .reset_index(drop=True)\n     .groupby([x])[hue]\n     .value_counts(normalize=True)\n     .rename('Percentage').mul(100)\n     .reset_index()\n     .sort_values(hue)))\n    plt.title(\"Percentage Frequency of {} by {}\".format(hue,x))\n    plt.ylabel(\"Percentage %\")","2c01ed0e":"hue = \"Recommended IND\"\nfig = plt.figure(figsize=(20, 14))\n\nax1 = plt.subplot2grid((2, 2), (0, 0))\nax1 = percentstandardize_barplot(x=\"Department Name\",y=\"Percentage\",hue=hue,data=df)\nax1 = plt.title(\"Recommended Items in each Department\")\nax1 = plt.legend(loc='best')\n\nax2 = plt.subplot2grid((2, 2), (0, 1))\nax2 = percentstandardize_barplot(x=\"Division Name\",y=\"Percentage\", hue=hue,data=df)\nax2 = plt.title(\"Recommended Items in each Division\")\nax2 = plt.legend(loc='best')\n\nax3 = plt.subplot2grid((2, 2), (1, 0),colspan=2)\nax3 = percentstandardize_barplot(x=\"Class Name\",y=\"Percentage\", hue=hue,data=df)\nax3 = plt.title(\"Recommended Items for each class\")\nax3 = plt.legend(loc='best')\nplt.show()","e2c4db85":"xvar = [\"Department Name\",\"Division Name\",\"Class Name\"]\nhue = \"Rating\"\nfig = plt.figure(figsize=(20, 14))\n\nax1 = plt.subplot2grid((2, 2), (0, 0))\nax1 = percentstandardize_barplot(x=xvar[0],y=\"Percentage\", hue=hue,data=df)\nax1 = plt.title(\"Percentage Frequency of {}\\nby {}\".format(hue, xvar[0]))\nax1 = plt.ylabel(\"Percentage %\")\n\nax2 = plt.subplot2grid((2, 2), (0,1))\nax2 = percentstandardize_barplot(x=xvar[1],y=\"Percentage\", hue=\"Rating\",data=df)\nax2 = plt.title(\"Percentage Frequency of {}\\nby {}\".format(hue, xvar[1]))\nax2 = plt.ylabel(\"Percentage %\")\n\nax3 = plt.subplot2grid((2, 2), (1,0),colspan=2)\nax2 = plt.xticks(rotation=45)\nax3 = percentstandardize_barplot(x=xvar[2],y=\"Percentage\", hue=\"Rating\",data=df)\nax3 = plt.title(\"Percentage Frequency of {}\\nby {}\".format(hue, xvar[2]))\nax3 = plt.ylabel(\"Percentage %\")\n\nplt.show()","235acbc5":"hue = \"Rating\"\nfig = plt.figure(figsize=(20, 14))\n\nax1 = plt.subplot2grid((2, 2), (0, 0))\nax1 = sns.countplot(x=\"Rating\", hue=\"Recommended IND\",data=df)\nax1 = plt.title(\"Occurrence of {}\\nby {}\".format(hue, \"Recommended IND\"))\nax1 = plt.ylabel(\"Count\")\n\nax2 = plt.subplot2grid((2,2),(0,1))\nax2 = percentstandardize_barplot(x=\"Rating\",y=\"Percentage\", hue=\"Recommended IND\",data=df)\nax2 = plt.title(\"Percentage Normalized Occurrence of {}\\nby {}\".format(hue, \"Recommended IND\"))\nax1 = plt.ylabel(\"% Percentage by Rating\")\nplt.show()","65edc363":"fig = plt.figure(figsize=(18,8))\nplt.xticks(rotation=90)\nplt.xlabel('Clothing ID')\nplt.ylabel(\"Popularity\")\nplt.title(\"ID of Top 50 Clothing Items\")\ndf['Clothing ID'].value_counts()[:30].plot(kind='bar',color='royalblue')\nplt.show()","144a54f9":"g = sns.jointplot(x= df[\"Positive Feedback Count\"], y=df[\"Age\"], kind='reg', color='royalblue')\ng.fig.suptitle(\"Scatter Plot for Age and Positive Feedback Count\")\nplt.show()","717a243b":"fig = plt.figure(figsize=(20,14))\nax1 = plt.subplot2grid((2,2),(0,0))\nax1 = sns.boxplot(x=\"Division Name\",y='Rating',data=df)\nax1 = plt.title('Rating Distribution per Division')\n\nax2 = plt.subplot2grid((2,2),(0,1))\nax2 = sns.boxplot(x=\"Department Name\",y='Rating',data=df)\nax2 = plt.title('Rating Distribution per Department')\n\nax3 = plt.subplot2grid((2,2),(1,0),colspan=2)\nax3 = plt.xticks(rotation=45)\nax3 = sns.boxplot(x=\"Class Name\",y='Rating',data=df)\nax3 = plt.title('Rating Distribution per Class')","aa561f15":"df.head()","5838ed69":"df.columns","becb032b":"df['Review Text'][0]","d52f89dd":"data = df[['Review Text','Rating']]\ndata.head()","6875639f":"data.isnull().sum()","ebc46917":"data.dropna(inplace=True)\ndata.isnull().sum()","e4527101":"data.shape","fbbce80c":"X = data['Review Text']\ny = pd.get_dummies(data['Rating']).values\n# y = data['Rating']\nmessages = X.copy()\nmessages = list(messages)","68753737":"type(messages)","040ced03":"voc_size = len(X)\n\nfrom nltk.stem.porter import PorterStemmer\nps = PorterStemmer()\ncorpus = []\nfor i in range(0, len(messages)):\n    review = re.sub('[^a-zA-Z]', ' ', messages[i])\n    review = review.lower()\n    review = review.split()\n    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n    review = ' '.join(review)\n    corpus.append(review)\n","56a4b9bf":"corpus[1]","a301e641":"onehot_repr=[one_hot(words,voc_size)for words in corpus] \nsent_length=40\nembedded_docs=pad_sequences(onehot_repr,padding='post',maxlen=sent_length)\nprint(embedded_docs[0:5])\n","f35f47fe":"X = np.array(embedded_docs)\ny = np.array(y)\nprint(X.shape,y.shape)","64e83456":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\nprint(X_train.shape,y_train.shape,X_test.shape,y_test.shape)","dfca5a19":"EMBEDDING_DIM=100\nmodel = Sequential()\nmodel.add(Embedding(voc_size, EMBEDDING_DIM, input_length=sent_length))\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2,return_sequences=True))\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(100,dropout=0.2,recurrent_dropout=0.2))\nmodel.add(Dense(5, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(\"\\n\")\nprint(\"-------------MODEL SUMMARY--------------\")\nprint(\"\\n\")\nmodel.summary()\nepochs = 100\nbatch_size = 64\nfrom tensorflow.keras.callbacks import EarlyStopping\nes = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto',restore_best_weights=True)\nprint(\"\\n\")\nprint(\"-------------STARTING TRAINING--------------\")\nprint(\"\\n\")\nhistory = model.fit(X_train,y_train,validation_split=0.2,epochs=epochs,batch_size=batch_size,callbacks=[es])\nprint(\"\\n\")\nprint(\"-------------TRAINING COMPLETED--------------\")\nprint(\"\\n\")\naccr = model.evaluate(X_test,y_test)\nprint('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n\n","6b236504":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,classification_report\n# Import and evaluate each test batch using Matthew's correlation coefficient\nfrom sklearn.metrics import accuracy_score,matthews_corrcoef\n\nfrom tqdm import tqdm, trange,tnrange,tqdm_notebook\nimport random\nimport os\nimport io\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader,RandomSampler,TensorDataset,SequentialSampler\nfrom transformers import BertTokenizer, BertConfig,AdamW, BertForSequenceClassification,get_linear_schedule_with_warmup","0f261e56":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nn_gpu = torch.cuda.device_count()\ntorch.cuda.get_device_name(0)\nSEED = 10\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif device == torch.device(\"cuda\"):\n    torch.cuda.manual_seed_all(SEED)","899c8019":"device = torch.device(\"cuda\")","65063f6c":"data.head()","1ecc2dc0":"sentences = data['Review Text'].values\nMAX_LEN = 256\n# importing bert tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased',do_lower_case=True)\ninput_ids = [tokenizer.encode(sent, add_special_tokens=True,max_length=MAX_LEN,pad_to_max_length=True) for sent in sentences]\nlabels = data['Rating'].values\n\nprint(\"Actual sentence before tokenization: \",sentences[1])\nprint(\"Encoded Input from dataset: \",input_ids[1])","c0af439f":"attention_mask=[]\nattention_mask = [[float(i>0) for i in seq] for seq in input_ids]\nprint(attention_mask[1])","61cbd411":"X_train,X_test,y_train,y_test = train_test_split(input_ids,labels,random_state=41,test_size=0.1)","664c133f":"train_masks, test_masks,_,_  = train_test_split(attention_mask,input_ids,random_state=41,test_size=0.1)","afa1db9d":"X_train = torch.tensor(X_train)\nX_test = torch.tensor(X_test)\ny_train = torch.tensor(y_train)\ny_test = torch.tensor(y_test)\ntrain_masks = torch.tensor(train_masks)\ntest_masks = torch.tensor(test_masks)","c1ab5f2e":"X_train.shape, y_train.shape, X_test.shape,y_test.shape, train_masks.shape,test_masks.shape","0681d956":"batch_size=32\ntrain_data = TensorDataset(X_train,train_masks,y_train)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data , sampler=train_sampler, batch_size=batch_size)","18c2702f":"test_data = TensorDataset(X_test,test_masks,y_test)\ntest_sampler = RandomSampler(test_data)\ntest_dataloader = DataLoader(test_data,sampler=test_sampler,batch_size=batch_size)\n","3b859bef":"train_data[0]","15701dd6":"len(train_dataloader)","06150cc1":"model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=6).to(device)\n# Parameters:\nlr = 2e-5\nadam_epsilon = 1e-8\n# Number of training epochs (authors recommend between 2 and 4)\nepochs = 3\nnum_warmup_steps = 0\nnum_training_steps = len(train_dataloader)*epochs\noptimizer = AdamW(model.parameters(),lr=lr,eps=adam_epsilon,correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\nscheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)","2b6f0f71":"## Store our loss and accuracy for plotting\ntrain_loss_set = []\nlearning_rate = []\n\n# Gradients gets accumulated by default\nmodel.zero_grad()\n\n# tnrange is a tqdm wrapper around the normal python range\nfor _ in tnrange(1,epochs+1,desc='Epoch'):\n  print(\"<\" + \"=\"*22 + F\" Epoch {_} \"+ \"=\"*22 + \">\")\n  # Calculate total loss for this epoch\n  batch_loss = 0\n\n  for step, batch in enumerate(train_dataloader):\n    # Set our model to training mode (as opposed to evaluation mode)\n    model.train()\n    \n    # Add batch to GPU\n    batch = tuple(t.to(device) for t in batch)\n    # Unpack the inputs from our dataloader\n    b_input_ids, b_input_mask, b_labels = batch\n\n    # Forward pass\n    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n    loss = outputs[0]\n    \n    # Backward pass\n    loss.backward()\n    \n    # Clip the norm of the gradients to 1.0\n    # Gradient clipping is not in AdamW anymore\n    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n    \n    # Update parameters and take a step using the computed gradient\n    optimizer.step()\n    \n    # Update learning rate schedule\n    scheduler.step()\n\n    # Clear the previous accumulated gradients\n    optimizer.zero_grad()\n    \n    # Update tracking variables\n    batch_loss += loss.item()\n\n  # Calculate the average loss over the training data.\n  avg_train_loss = batch_loss \/ len(train_dataloader)\n\n  #store the current learning rate\n  for param_group in optimizer.param_groups:\n    print(\"\\n\\tCurrent Learning rate: \",param_group['lr'])\n    learning_rate.append(param_group['lr'])\n    \n  train_loss_set.append(avg_train_loss)\n  print(F'\\n\\tAverage Training loss: {avg_train_loss}')\n    \n  # Validation\n\n  # Put model in evaluation mode to evaluate loss on the validation set\n  model.eval()\n\n  # Tracking variables \n  eval_accuracy,eval_mcc_accuracy,nb_eval_steps = 0, 0, 0\n\n  # Evaluate data for one epoch\n  for batch in test_dataloader:\n    # Add batch to GPU\n    batch = tuple(t.to(device) for t in batch)\n    # Unpack the inputs from our dataloader\n    b_input_ids, b_input_mask, b_labels = batch\n    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n    with torch.no_grad():\n      # Forward pass, calculate logit predictions\n      logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n    \n    # Move logits and labels to CPU\n    logits = logits[0].to('cpu').numpy()\n    label_ids = b_labels.to('cpu').numpy()\n\n    pred_flat = np.argmax(logits, axis=1).flatten()\n    labels_flat = label_ids.flatten()\n    \n    df_metrics=pd.DataFrame({'Epoch':epochs,'Actual_class':labels_flat,'Predicted_class':pred_flat})\n    \n    tmp_eval_accuracy = accuracy_score(labels_flat,pred_flat)\n    tmp_eval_mcc_accuracy = matthews_corrcoef(labels_flat, pred_flat)\n    \n    eval_accuracy += tmp_eval_accuracy\n    eval_mcc_accuracy += tmp_eval_mcc_accuracy\n    nb_eval_steps += 1\n\n  print(F'\\n\\tValidation Accuracy: {eval_accuracy\/nb_eval_steps}')\n  print(F'\\n\\tValidation MCC Accuracy: {eval_mcc_accuracy\/nb_eval_steps}')","f60b4d71":"df_metrics","269ee767":"df_metrics['Actual_class'].unique() , df_metrics['Predicted_class'].unique() ","570203e1":"data[['Review Text','Rating']].drop_duplicates(keep='first')","83ea5b1c":"df.head()","016eabf9":"## emotion labels\nlabel2int = {\n  \"bad\": 2,\n  \"neutral\": 3,\n  \"good\": 4,\n    \"excellent\":5\n}","582ca57c":"print(classification_report(df_metrics['Actual_class'].values, df_metrics['Predicted_class'].values, target_names=label2int.keys(), digits=len(label2int)))","fa288e36":"# saving the model\n# model_save_folder = 'model\/'\n# tokenizer_save_folder = 'tokenizer\/'\n\n# path_model = F'\/kaggle\/working\/{model_save_folder}'\n# path_tokenizer = F'\/kaggle\/working\/{tokenizer_save_folder}'\n\n# #create the dir\n\n# !mkdir -p {path_model}\n# !mkdir -p {path_tokenizer}\n\n# ## Now let's save our model and tokenizer to a directory\n# model.save_pretrained(path_model)\n# tokenizer.save_pretrained(path_tokenizer)\n\n# model_save_name = 'fineTuneModel.pt'\n# path = path_model = F'\/kaggle\/working\/{model_save_folder}\/{model_save_name}'\n# torch.save(model.state_dict(),path);","5551ff94":"# we get a validation accuracy of almost 70 percent using BERT \n\n# more models will be added ","51f925d2":"# BERT ","2b2fc062":"# MORE MODELS WILL BE ADDED","69cecf43":"# DEEP LEARNING ","b29f6ba6":"# UNSTACKED and Percentage"}}