{"cell_type":{"719e1e48":"code","254041e5":"code","fa2d76a7":"code","d64c986c":"code","aff3095b":"code","dea2ea4c":"code","bb4a86f0":"code","fe60a31d":"code","71ab8d53":"code","2fa070f8":"code","014d4441":"code","a1181bc2":"code","a040b532":"code","06c034f7":"markdown","032831a2":"markdown","6add57bd":"markdown","7b5c15ab":"markdown","a784aace":"markdown","3c9908ee":"markdown","bf3d250b":"markdown","31ab0743":"markdown","aac6531b":"markdown","91dac0ff":"markdown","a2e7279d":"markdown","ea6a78c4":"markdown","42ac0bed":"markdown"},"source":{"719e1e48":"import pandas as pd\nimport matplotlib.pyplot as plt","254041e5":"# Read the data\nX_full = pd.read_csv('..\/input\/home-data-for-ml-course\/train.csv', index_col='Id')\nX_test = pd.read_csv('..\/input\/home-data-for-ml-course\/test.csv', index_col='Id')\n\n# Drop X_full with missing target (if any)\nX_full.dropna(axis=0, subset=['SalePrice'], inplace=True)\n\n# Specify target variable\ny = X_full['SalePrice']\n\n# Remove target variable from X_full\nX_full.drop(['SalePrice'], axis=1, inplace=True)\n","fa2d76a7":"# Create a list of categorical and numerical variables in X_full\ncat_all =  [col for col in X_full.columns if X_full[col].dtype == 'object']\nnum_all = [col for col in X_full.columns if X_full[col].dtype in ['int64', 'float64']]\n\n\n# Get to know the data\nprint('Shape of X_full:', X_full.shape)\nprint('Total number of categorical variables:', len(cat_all))\nprint('Total number of numerical variables:', len(num_all))","d64c986c":"# Obtain info for missing values for each categorical variable\ncat_null = X_full[cat_all].isnull().sum()\n\n# Create a list of categorical columns with more than 20 percent of missing values (arbitrary choose)\ncat_lownull = [col for col in cat_null.index if cat_null[col] < int(len(X_full.index)*0.2)]\n\n# Check cardinality of categorical columns\n# Create a list of categorical columns with low cardinality (< 10 arbitrary choose)\ncat_lownull_lowcard = [col for col in X_full[cat_lownull].columns if\n                        X_full[col].nunique() < 10]","aff3095b":"# Obtain info for missing values for each numerical variable\nnum_null = X_full[num_all].isnull().sum()\n\n# Create a list of numerical columns with more than 60 percent of missing values (arbitrary choose)\nnum_lownull = [col for col in num_null.index if num_null[col] < int(len(X_full.index)*0.6)]","dea2ea4c":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\n\ndef build_pipeline(model, categorical_cols, numerical_cols):\n    # Preprocessing for categorical data\n    categorical_transformer = Pipeline(steps=[\n        ('imputer', SimpleImputer(strategy='most_frequent')),\n        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n        ])\n\n    # Preprocessing for numerical data\n    numerical_transformer = SimpleImputer(strategy='median')\n        \n    # Bundle preprocessing for numerical and categorical data\n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('cat', categorical_transformer, categorical_cols),\n            ('num', numerical_transformer, numerical_cols)\n            ])\n            \n    return Pipeline(steps=[('preprocessor', preprocessor),('model', model)])","bb4a86f0":"from sklearn.model_selection import cross_val_score\n\n# Create a helper function to score model perfomance\ndef get_score(model, X, y, categorical_cols, numerical_cols):\n    my_pipeline = build_pipeline(model, categorical_cols, numerical_cols)\n\n    scores = -1 * cross_val_score(my_pipeline, X, y,\n                                  cv=5,\n                                  scoring='neg_mean_absolute_error')\n    return scores.mean()","fe60a31d":"# Specify columns\/features of interest\ncategorical_cols = cat_all\nnumerical_cols = num_all\nmy_cols =  categorical_cols + numerical_cols","71ab8d53":"# import the models\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor","2fa070f8":"RF_results = {\n    50: 17787.59220547945,\n    100: 17610.124061643837,\n    150: 17595.117739726025,\n    200: 17565.44338013699,\n    250: 17586.754210958905,\n    300: 17556.510452054797,\n    350: 17542.81497260274,\n    400: 17549.64762328767,\n    450: 17534.629689497717,\n    500: 17539.583256164384,\n    550: 17548.770433374844,\n    600: 17520.606503424657,\n    650: 17525.15801053741,\n    700: 17527.700497064583,\n    750: 17536.033297716895,\n    800: 17537.59766267123\n}\n\n# Get best n_estimators\nn_estimators_best = min(RF_results, key=RF_results.get)\nprint(f'Best n_estimators: {n_estimators_best} gives MAE of {RF_results[n_estimators_best]}')\n\n# Plot MAE as a funciton of n_estimators\nplt.plot(list(RF_results.keys()), list(RF_results.values()), label='RandomForest')\nplt.legend(loc='upper right')\nplt.xlabel('n_estimators')\nplt.ylabel('Mean Absolute Error (MAE)')\nplt.show()","014d4441":"XGB_results = {\n    50: 16459.692302547093,\n    100: 16115.321120505138,\n    150: 16095.46531464041,\n    200: 16078.901434075344,\n    250: 16074.888337435788,\n    300: 16079.34586098031,\n    350: 16081.235710081335,\n    400: 16082.553673480308,\n    450: 16078.698579302227,\n    500: 16082.503855415238,\n    550: 16085.968113227742,\n    600: 16087.704510916094,\n    650: 16091.130195847601,\n    700: 16093.248817422946,\n    750: 16094.12746949914,\n    800: 16094.797193386132\n}\n\n# Get best n_estimators\nn_estimators_best = min(XGB_results, key=XGB_results.get)\nprint(f'Best n_estimators: {n_estimators_best} gives MAE of {XGB_results[n_estimators_best]} (learning_rate=0.1)')\n\n# Plot MAE as a funciton of n_estimators\nplt.plot(list(XGB_results.keys()), list(XGB_results.values()), label='XGBoost (learning_rate=0.1)')\nplt.legend(loc='upper right')\nplt.xlabel('n_estimators')\nplt.ylabel('Mean Absolute Error (MAE)')\nplt.show()","a1181bc2":"# 1. Build pipeline\n# model = RandomForestRegressor(n_estimators=min(RF_results, key=RF_results.get), random_state=0)\nmodel = XGBRegressor(n_estimators=min(XGB_results, key=XGB_results.get), learning_rate=0.1, random_state=0)\nmy_pipeline = build_pipeline(model, categorical_cols, numerical_cols)\n\n# 2. Preprocess testing data and fit model using the pipeline \nmy_pipeline.fit(X_full[my_cols], y)\n\n# 3. Predict results for X_test\npreds_test = my_pipeline.predict(X_test[my_cols])\npreds_test","a040b532":"# Save test predictions to file\noutput = pd.DataFrame({'Id': X_test.index,\n                       'SalePrice': preds_test})\noutput.to_csv('submission.csv', index=False)","06c034f7":"## 2. Creating a Pipeline\nI find it useful to build the pipeline in a function. Doing so offers great flexibility with testing different ideas for model optimization. The build_pipeline function below creates the pipeline that will be used in preprocessing categorical variables, numerical variables, and it defines the model to be used.\n\nNote that although the SimpleImputer strategy was hardcoded, it could easily be made an argument into the build_pipeline function for testing the effect of different imputation strategies on model performance.  ","032831a2":"### Making predictions\nMaking predictions is now as simple as:\n1. Build pipeline. Make sure that model = the model used during parameter calibration and that model parameters = the best parameter obtained during the parameter calibration step.\n2. Preprocess testing data and fit model using the pipeline.\n3. Predict results for X_test.","6add57bd":"## 3. Cross-Validation\nI find it easier to use cross-validation instead of splitting data into training and testing sets. Also, cross-validation gives a more accurate measure of model quality because it reduces randomness in scoring model performance. A drawback is that cross-validation is computationally expensive for larger datasets. \u00a0Check out [Cross-Validation](http:\/\/www.kaggle.com\/alexisbcook\/cross-validation) for more detail.\n\nDefining a helper function to score model performance makes it easier to use the same parameters and criteria for scoring:","7b5c15ab":"### 4.1 RandomForest: Calibrating n_estimators\nTo get the optimal n_estimators for the RandomForest model, the following code snippet was used:\n```\nRF_results = {}\nfor n_estimators in range(50,850,50):\n    print(f'Running n_estimators = {n_estimators}')\n    X = X_full[my_cols].copy()\n    model = RandomForestRegressor(n_estimators=n_estimators, random_state=0)\n    RF_results[n_estimators] = get_score(model, X, y, categorical_cols, numerical_cols)\n```\nNote: I have decided to put the code snippet in the markdown section because it takes a long time to complete. RF_results from the code is shown below: ","a784aace":"## Conclusion\nThis notebook attempts to summarize the Intermediate Machine Learning micro-course. It covers: Categorical and Numerical grouping of variables, Creating a Pipeline, Cross-Validation, and RandomForest Vs. XGBoost.\n\nYou are encouraged to try different ideas and check the performance on the Leaderboard.\n<br>\nTips for improving the overall workflow and other suggestions will be appreciated.\n<br>\nAlso, feel free to let me know of any mistakes.","3c9908ee":"### 4.2 XGBoost: Calibrating n_estimators and learning_rate\nTo get the optimal n_estimators for the XGBoost model, the following code snippet was used:\n```\nXGB_results = {}\nfor n_estimators in range(50,850,50):\n    print(f'Running n_estimators = {n_estimators}')\n    X = X_full[my_cols].copy()\n    model = XGBRegressor(n_estimators=n_estimators, learning_rate=0.1, random_state=0)\n    XGB_results[n_estimators] = get_score(model, X, y, categorical_cols, numerical_cols)    \n```\nHow did I choose learning_rate=0.1? I used the time-tested method of trial and error. However, using\u00a0GridSearchCV\u00a0is a better approach.\n<br>\nNote: I have decided to put the code snippet in the markdown section because it takes a long time to complete. XGB_results from the code is shown below:","bf3d250b":"# <center> Intermediate Machine Learning in One Notebook <\/center>\n\nThe aim of this notebook is to summarize the [Intermediate Machine Learning](http:\/\/www.kaggle.com\/learn\/intermediate-machine-learning) micro-course in one notebook. To serve as a reference for myself and any budding Kaggler who might find it useful. This will be a high-level overview and I strongly encourage checking out the [micro-course](http:\/\/www.kaggle.com\/learn\/intermediate-machine-learning) if you have any questions. [Housing Prices Competition for Kaggle Learn Users](http:\/\/www.kaggle.com\/c\/home-data-for-ml-course) will be used as an example.\n\nThis notebook will cover:\n* Categorical and Numerical grouping of variables\n* Creating a Pipeline\n* Cross-Validation\n* RandomForest Vs. XGBoost","31ab0743":"## Output file for submission","aac6531b":"## 4. RandomForest Vs. XGBoost\nNow that we have designed the pipeline let's test some models. This section will compare the performance of RandomForest Vs. XGBoost machine learning model. To keep it simple, all features in X_full was used by specifying columns of interest as:\n```\ncategorical_cols = cat_all \u00a0 #or cat_lownull, or cat_lownull_lowcard columns defined earlier \u00a0\nnumerical_cols = num_all \u00a0 \u00a0 #or num_lownull column defined earlier\nmy_cols = \u00a0categorical_cols + numerical_cols\n```\nThis highlights the need for defining different columns or features that you might want to use in testing model performance. I presume this might come in handy when one starts performing more complex feature engineering that adds new features to the data.\u00a0 ","91dac0ff":"### 1.2 Get to know the numerical variables\nAt this level, getting to know the numerical variables can be as simple as obtaining the fraction of missing data for each feature. You could decide to drop features with too many missing data later on.","a2e7279d":"## Acknowledgement\nSome code snippets were taken from [Intermediate Machine Learning](http:\/\/www.kaggle.com\/learn\/intermediate-machine-learning) micro-course.","ea6a78c4":"## 1. Categorical and Numerical grouping of variables\nGroup features into categorical and numerical features. Doing so allows for custom feature engineering on each group in the pipeline that will be built later.","42ac0bed":"### 1.1 Get to know the categorical variables\nGetting to know your categorical variables, for the intermediate level, can be divided into two steps:\n1. Obtain the fraction of missing data for each feature. You could decide to drop features with too many missing data later on.\n2. For categorical variables, obtain the cardinality of each feature. You could decide to drop features with high cardinality later on. "}}