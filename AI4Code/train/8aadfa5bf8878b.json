{"cell_type":{"e387bdcf":"code","602543ea":"code","0a8a90d5":"code","ee23af39":"code","ae569882":"code","c32f0c93":"code","d3a0242f":"code","854584bd":"code","8016c348":"code","c8a39a78":"code","6b26291a":"code","52d5c0b4":"code","d9876f1e":"code","673162fb":"code","e250e2a6":"code","172704bb":"code","8626741a":"markdown","fcfcee2b":"markdown","cc51cf6f":"markdown","4244eb32":"markdown","7fcfd6f4":"markdown"},"source":{"e387bdcf":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.externals import joblib\n\nimport xgboost as xgb\nimport lightgbm as lgb\n","602543ea":"train_file = '..\/input\/train.csv'\ntest_file = '..\/input\/test.csv'","0a8a90d5":"train = pd.read_csv(train_file)\ntest = pd.read_csv(test_file)","ee23af39":"test_ID = test['ID']\ny_train = train['target']\ny_train = np.log1p(y_train)","ae569882":"train.drop(\"ID\", axis = 1, inplace = True)\ntrain.drop(\"target\", axis = 1, inplace = True)\ntest.drop(\"ID\", axis = 1, inplace = True)","c32f0c93":"NUM_OF_DECIMALS = 32\ntrain = train.round(NUM_OF_DECIMALS)\ntest = test.round(NUM_OF_DECIMALS)","d3a0242f":"train_zeros = pd.DataFrame({'Percent_zero':((train.values)==0).mean(axis=0),\n                           'Column' : train.columns})\n\nhigh_vol_columns = train_zeros['Column'][train_zeros['Percent_zero'] < 0.70].values\nlow_vol_columns = train_zeros['Column'][train_zeros['Percent_zero'] >= 0.70].values\n","854584bd":"train = train.replace({0:np.nan})\ntest = test.replace({0:np.nan})","8016c348":"cluster_sets = {\"low\":low_vol_columns, \"high\":high_vol_columns}\nfor cluster_key in cluster_sets:\n    for df in [train,test]:\n        df[\"count_not0_\"+cluster_key] = df[cluster_sets[cluster_key]].count(axis=1)\n        df[\"sum_\"+cluster_key] = df[cluster_sets[cluster_key]].sum(axis=1)\n        df[\"var_\"+cluster_key] = df[cluster_sets[cluster_key]].var(axis=1)\n        df[\"median_\"+cluster_key] = df[cluster_sets[cluster_key]].median(axis=1)\n        df[\"mean_\"+cluster_key] = df[cluster_sets[cluster_key]].mean(axis=1)\n        df[\"std_\"+cluster_key] = df[cluster_sets[cluster_key]].std(axis=1)\n        df[\"max_\"+cluster_key] = df[cluster_sets[cluster_key]].max(axis=1)\n        df[\"min_\"+cluster_key] = df[cluster_sets[cluster_key]].min(axis=1)\n        df[\"skew_\"+cluster_key] = df[cluster_sets[cluster_key]].skew(axis=1)\n        df[\"kurtosis_\"+cluster_key] = df[cluster_sets[cluster_key]].kurtosis(axis=1)\n","c8a39a78":"train_more_simplified = train.drop(high_vol_columns,axis=1).drop(low_vol_columns,axis=1)\ntest_more_simplified = test.drop(high_vol_columns,axis=1).drop(low_vol_columns,axis=1)","6b26291a":"train_more_simplified.head()","52d5c0b4":"def run_xgb(train_X, train_y, val_X, val_y, test_X):\n    params = {'objective': 'reg:linear', \n          'eval_metric': 'rmse',\n          'eta': 0.001,\n          'max_depth': 6, \n          'subsample': 0.6, \n          'colsample_bytree': 0.6,\n          'alpha':0.001,\n          'random_state': 42, \n          'silent': True}\n    print(\"Load matrices\")\n    tr_data = xgb.DMatrix(train_X, train_y)\n    va_data = xgb.DMatrix(val_X, val_y)\n    \n    print(\"Set watchlist\")\n    watchlist = [(tr_data, 'train'), (va_data, 'valid')]\n\n    print(\"Train model\")\n    model_xgb = xgb.train(params, tr_data, 20000, watchlist, maximize=False, early_stopping_rounds = 100, verbose_eval=100)\n    \n    dtest = xgb.DMatrix(test_X)\n    xgb_pred_y = np.expm1(model_xgb.predict(dtest, ntree_limit=model_xgb.best_ntree_limit))\n    \n    return xgb_pred_y, model_xgb","d9876f1e":"dev_X, val_X, dev_y, val_y = train_test_split(train_more_simplified, y_train, test_size = 0.2, random_state = 40)","673162fb":"pred_test_xgb, model_xgb = run_xgb(dev_X, dev_y, val_X, val_y, test_more_simplified)\nprint(\"Finished!\")","e250e2a6":"sub = pd.read_csv('..\/input\/sample_submission.csv')\nsub[\"target\"] = pred_test_xgb","172704bb":"sub.to_csv('sub_XGB_Aggregate_v2.csv', index=False)","8626741a":"# Create submission file","fcfcee2b":"#### Define XGBoost model.  Note this is taken (almost) straight from another public kernel, although I think I changed the max depth to try to control overfitting. No grid-search performed, could help.","cc51cf6f":"### Split out the dense\/sparse clusters here","4244eb32":"# Santander - Separate Aggregates for the dense and sparse feature clusters, applied to simple XGBoost model\n\n\nNote: most of the code in this kernel is taken from a number of other public kernels, including the pre-processing, aggregate feature types and XGBoost code & settings.  Thank you everyone for being so generous with sharing Kernels!!\n\nMy own work is primarily the creation of aggregate features for the most dense columns.  I did this because they standout so much from the other columns that they must come from a different distribution - they hold a different type of transctional or limit information.\nIn other public kernels it can be seen that there are a number of other variable clusters, which suggests 3-4 groups of variables could be created, with aggregate features for each.\n","7fcfd6f4":"#### I had another version that only dropped the sparse columns and kept the dense columns, but dropping those as well improved LB score."}}