{"cell_type":{"83479975":"code","f065cbf5":"code","054ad5d3":"code","9770532b":"code","8aa07e4d":"code","90f2abb3":"code","4110f044":"code","48d65e56":"code","bea36d2c":"code","97156a73":"code","7698b6ee":"code","9ab0ba70":"code","7de6de77":"code","4273315d":"code","551b6bec":"code","07d7649c":"code","a4f16134":"code","f373bdd8":"code","7044657f":"code","f4bb947a":"code","220d4dd6":"code","21bf99e3":"markdown","89e88baa":"markdown","7eb3926b":"markdown","f7b51dbf":"markdown","fbfb9277":"markdown","19be2b7c":"markdown","51c944e8":"markdown","265a7521":"markdown","217e8408":"markdown","44a61f58":"markdown","2f3c0f6b":"markdown","705e6c70":"markdown","6c5c415f":"markdown","6eecb34c":"markdown","6a27f650":"markdown","181f3bd8":"markdown","5ebb0b1f":"markdown","d7f1e5d4":"markdown","51f5d9fa":"markdown","7684df2e":"markdown","f154baab":"markdown","4b854810":"markdown","015358c4":"markdown","f9372c23":"markdown","5f438727":"markdown","64929b41":"markdown","e5dbfe19":"markdown","5e4cc991":"markdown"},"source":{"83479975":"import os\nimport seaborn as sns\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport tensorflow_hub as hub \nimport matplotlib.pyplot as plt\nimport sklearn\nimport pandas as pd\nfrom sklearn.metrics import classification_report, confusion_matrix","f065cbf5":"def show_image(file_path):\n    img = tf.keras.preprocessing.image.load_img(file_path, target_size=(228, 228))\n    plt.imshow(img)\n    plt.show()","054ad5d3":"def preprocess_image(item):\n    image_string = tf.io.read_file(item[0])\n    image_decoded = tf.image.decode_jpeg(image_string, channels=3)\n    image_resized = tf.image.resize(image_decoded, (IMAGE_SIZE, IMAGE_SIZE))\n    image_resized = tf.cast(image_resized, tf.float32) \/ 255.0\n    return image_resized, tf.strings.to_number(item[1], tf.int64)","9770532b":"def get_dataset(edible_fungies, poisonous_fungies, mode, batch_size):\n    x = list(edible_fungies) + list(poisonous_fungies)\n    y = [0] * len(edible_fungies) + [1] * len(poisonous_fungies)\n    items = [(a, b) for (a, b) in zip(x, y)]\n    dataset = tf.data.Dataset.from_tensor_slices(np.array(items)).shuffle(len(x))\n    dataset = dataset.map(preprocess_image).batch(batch_size)\n    return dataset","8aa07e4d":"def get_balanced_dataset(edible_fungies, poisonous_fungies, batch_count, batch_size, mode=\"train\"):\n    length_per_category = batch_size * batch_count \/\/ 2\n    edible_indices = np.random.choice(len(edible_fungies), length_per_category)\n    poisonous_indices = np.random.choice(len(poisonous_fungies), length_per_category)\n    samle_count = 2 * length_per_category\n    return get_dataset(\n        edible_fungies[edible_indices], \n        poisonous_fungies[poisonous_indices], \n        mode, \n        batch_size\n    ), samle_count","90f2abb3":"base_path = \"\/kaggle\/input\/edible-and-poisonous-fungi\/\"\nlables = [\"edible\", \"poisonous\"]\ndirectory_group = [\n    ['edible mushroom sporocarp', 'edible sporocarp'], \n    ['poisonous mushroom sporocarp', 'poisonous sporocarp']\n]\nedible_fungies = []\npoisonous_fungies = []\nfor (label, directories) in zip(lables, directory_group):\n    for directory in directories:\n        items = os.listdir(base_path + directory)\n        for item in items:\n            file_path = base_path + directory + \"\/\" + item\n            if label == \"edible\":\n                edible_fungies.append(file_path)\n            else:\n                poisonous_fungies.append(file_path)\nedible_fungies = list(set(edible_fungies))\npoisonous_fungies = list(set(poisonous_fungies))","4110f044":"batch_size = 32\nvalidation_split = 0.2\nedible_fungies_split_index = int((1 - validation_split) * len(edible_fungies))\npoisonous_fungies_split_index = int((1 - validation_split) * len(poisonous_fungies))\ntrain_edible_fungies, valid_edible_fungies = edible_fungies[:edible_fungies_split_index],  edible_fungies[edible_fungies_split_index:] \ntrain_poisonous_fungies, valid_poisonous_fungies = poisonous_fungies[:poisonous_fungies_split_index],  poisonous_fungies[poisonous_fungies_split_index:] \nprint(len(train_edible_fungies), len(valid_edible_fungies))\nprint(len(train_poisonous_fungies), len(valid_poisonous_fungies))\nnum_batch_per_epoch = min(len(train_edible_fungies), len(train_poisonous_fungies)) \/\/ batch_size\nprint(num_batch_per_epoch)\nnum_epochs = 50\ntrain_edible_fungies = np.array(train_edible_fungies)\nvalid_edible_fungies = np.array(valid_edible_fungies)\ntrain_poisonous_fungies = np.array(train_poisonous_fungies)\nvalid_poisonous_fungies = np.array(valid_poisonous_fungies)\ntotal_valid_count = len(valid_edible_fungies) + len(valid_poisonous_fungies)","48d65e56":"len(edible_fungies)","bea36d2c":"len(poisonous_fungies)","97156a73":"edible_fungies[:10]","7698b6ee":"poisonous_fungies[:10]","9ab0ba70":"for i in range(10):\n    show_image(poisonous_fungies[np.random.randint(len(poisonous_fungies))])","7de6de77":"for i in range(10):\n    show_image(edible_fungies[np.random.randint(len(edible_fungies))])","4273315d":"IMAGE_SIZE = 224\nhandle_base = \"mobilenet_v2\"\nMODULE_HANDLE =\"https:\/\/tfhub.dev\/google\/tf2-preview\/{}\/feature_vector\/4\".format(handle_base)\nfeature_extractor = hub.KerasLayer(MODULE_HANDLE,\n                                    input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\nfeature_extractor.trainable = False  ","551b6bec":"tf.keras.backend.clear_session()\nmodel = tf.keras.Sequential([\n    feature_extractor,\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.Dense(32, activation='relu'),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.Dense(2, activation='softmax')\n])\nmodel.summary()","07d7649c":"valid_dataset = get_dataset(valid_edible_fungies, valid_poisonous_fungies, \"valid\", batch_size)\noptimizer = tf.keras.optimizers.Adam()\nloss = tf.keras.losses.SparseCategoricalCrossentropy()\nhistory = {\n    \"train_loss\": [],\n    \"valid_loss\": [],\n    \"train_accuracy\": [],\n    \"valid_accuracy\": []\n}\nfor epoch in range(num_epochs):\n    begin_time = time.time()\n    train_dataset, total_train_count = get_balanced_dataset(train_edible_fungies, train_poisonous_fungies, num_batch_per_epoch, batch_size, mode=\"train\")\n    train_losses = []\n    valid_losses = []\n    correct_count = 0\n    total_count = 0\n    for (x_batch, y_true) in train_dataset:\n        with tf.GradientTape() as tape:\n            y_pred = model(x_batch)\n            predict_labels = tf.argmax(y_pred, axis=-1)\n            loss_value = loss(y_true, y_pred)\n        gradients = tape.gradient(loss_value, model.trainable_weights)\n        optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n        train_losses.append(loss_value)\n        correct_count += tf.reduce_sum(tf.cast(y_true == predict_labels, tf.int64))\n        total_count += y_true.shape[0]\n    train_loss = tf.reduce_mean(train_losses)\n    train_accuracy = correct_count \/ total_train_count\n    history[\"train_loss\"].append(train_loss)\n    history[\"train_accuracy\"].append(train_accuracy)\n    correct_count = 0\n    total_count = 0\n    for (x_batch, y_true) in valid_dataset:\n        y_pred = model(x_batch)\n        predict_labels = tf.argmax(y_pred, axis=-1)\n        loss_value = loss(y_true, y_pred)\n        valid_losses.append(loss_value)\n        correct_count += tf.reduce_sum(tf.cast(y_true == predict_labels, tf.int64))\n        total_count += y_true.shape[0]\n    valid_loss = tf.reduce_mean(valid_losses)\n    valid_accuracy = correct_count \/ total_valid_count\n    history[\"valid_loss\"].append(valid_loss)\n    history[\"valid_accuracy\"].append(valid_accuracy)\n    elapsed_time = time.time() -  begin_time\n    print(\"Epoch: %d \/ %d\"%(epoch + 1, num_epochs))\n    print(\"%.2fs Loss: %.2f Accuracy: %.2f Validation Loss: %.2f Validation Accuracy: %.2f\"%(elapsed_time, train_loss, train_accuracy, valid_loss, valid_accuracy))\nfor key in history:\n    history[key] = list(np.array(history[key]))","a4f16134":"pd.DataFrame(history).plot()","f373bdd8":"predicted_labels = []\nactual_labels = []\nfor (x_batch, y_true) in valid_dataset:\n    y_pred = model(x_batch)\n    predicted_labels += list(np.array(tf.argmax(y_pred, axis=-1)))\n    actual_labels += list(np.array(y_true))","7044657f":"matrix = confusion_matrix(actual_labels, predicted_labels)\nprint(matrix)\nsns.heatmap(matrix)","f4bb947a":"cls_report = classification_report(predicted_labels, actual_labels)\nprint(cls_report)","220d4dd6":"model.save(\"model.h5\")","21bf99e3":"### Classification Report","89e88baa":"**Number of Edible Fungies:**","7eb3926b":"**File path of edible fungies images:**","f7b51dbf":"## Overview\n\nIn this notebook, I will apply following ideas & techniques to solve this **Fungi Classification** problem:\n- Exploratory Data Analysis\n- Dataset Balancing\n- Transfer Learning\n- Train Validation Split\n- Image Preprocessing\n- Custom Training Loop with TensorFlow GradientTape\n- Model Evaluation","fbfb9277":"## Model Development","19be2b7c":"**Preprocess image**","51c944e8":"**Show image**","265a7521":"## Import Dataset","217e8408":"### Loss & Accuracy over time","44a61f58":"### Model Building with Transfer Learning","2f3c0f6b":"### Prediction with validation dataset","705e6c70":"### Confusion Matrix","6c5c415f":"## Save Model","6eecb34c":"**Get Balanced Dataset**","6a27f650":"## Import Packages","181f3bd8":"**File path of edible fungies images:**","5ebb0b1f":"## Common Functions","d7f1e5d4":"# Fungi Classification\n## Table of Contents\n- Overview\n- Import Packages\n- Import Dataset\n- Common Functions\n- Exploratory Data Analysis\n- Model Development\n    - Model Building with Transfer Learning\n    - Model Training with Balanced Dataset and Custom Training Loop\n- Model Evaluation\n    - Loss & Accuracy over time\n    - Prediction with validation dataset\n    - Confusion Matrix\n    - Classification Report\n- Save Model\n- Concusion\n- Todos","51f5d9fa":"## Exploratory Data Analysis","7684df2e":"**Poisonous fungies**","f154baab":"## Model Evaluation","4b854810":"There is a label imblance problem in this dataset. I am planing to provide same number of edible and poisonous fungies in each training epoch.","015358c4":"## Todos\n- Data Augmentation\n- Better image preprocessing\n- More EDA\n","f9372c23":"## Conclusion\nNow the dataset can achive 79% accuracy on validation dataset. The result is not bad ,yet there's still a lot to improve. For Example, the size of images varies a lot, I just scale it to the same size for simplicity, how about applying different image preprocessing strategies such as random crop, rotation?","5f438727":"**Number of poisonous Fungies:**","64929b41":"**Edible fungies**","e5dbfe19":"**Get TensorFlow Dataset for training and evaluation**","5e4cc991":"### Model Training with Balanced Dataset and Custom Training Loop"}}