{"cell_type":{"f76b2f10":"code","13431b3d":"code","e0313b76":"code","9c333934":"code","160dcc07":"code","9e6ae8b0":"code","9a9d3b7a":"code","251e23b7":"code","3b7e7679":"code","aa387856":"code","c347da49":"code","07dfe260":"code","2b15793a":"code","b0d2eaf7":"code","768e9637":"markdown","f594edf7":"markdown","e5798918":"markdown","655a8299":"markdown","7e23419d":"markdown","ad800e21":"markdown","3dd4ae32":"markdown","c33aade5":"markdown","e22c6d05":"markdown","ab8bdbcf":"markdown","ee923522":"markdown","5ca5f2ea":"markdown","aad269d2":"markdown","dc2f7361":"markdown","56a3e2e4":"markdown"},"source":{"f76b2f10":"#%tensorflow_version 2.x\nimport tensorflow as tf\nprint(\"Tensorflow version \" + tf.__version__)\n\ntry:\n  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\nexcept ValueError:\n  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","13431b3d":"import re\nimport numpy as np\nfrom matplotlib import pyplot as plt","e0313b76":"AUTO = tf.data.experimental.AUTOTUNE\n\nIMAGE_SIZE = [331, 331]\n\nbatch_size = 16 * tpu_strategy.num_replicas_in_sync\n\ngcs_pattern = 'gs:\/\/flowers-public\/tfrecords-jpeg-331x331\/*.tfrec'\nvalidation_split = 0.19\nfilenames = tf.io.gfile.glob(gcs_pattern)\nsplit = len(filenames) - int(len(filenames) * validation_split)\ntrain_fns = filenames[:split]\nvalidation_fns = filenames[split:]\n        \ndef parse_tfrecord(example):\n  features = {\n    \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n    \"class\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means scalar\n    \"one_hot_class\": tf.io.VarLenFeature(tf.float32),\n  }\n  example = tf.io.parse_single_example(example, features)\n  decoded = tf.image.decode_jpeg(example['image'], channels=3)\n  normalized = tf.cast(decoded, tf.float32) \/ 255.0 # convert each 0-255 value to floats in [0, 1] range\n  image_tensor = tf.reshape(normalized, [*IMAGE_SIZE, 3])\n  one_hot_class = tf.reshape(tf.sparse.to_dense(example['one_hot_class']), [5])\n  return image_tensor, one_hot_class\n\ndef load_dataset(filenames):\n  # Read from TFRecords. For optimal performance, we interleave reads from multiple files.\n  records = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\n  return records.map(parse_tfrecord, num_parallel_calls=AUTO)\n\ndef get_training_dataset():\n  dataset = load_dataset(train_fns)\n\n  # Create some additional training images by randomly flipping and\n  # increasing\/decreasing the saturation of images in the training set. \n  def data_augment(image, one_hot_class):\n    modified = tf.image.random_flip_left_right(image)\n    modified = tf.image.random_saturation(modified, 0, 2)\n    return modified, one_hot_class\n  augmented = dataset.map(data_augment, num_parallel_calls=AUTO)\n\n  # Prefetch the next batch while training (autotune prefetch buffer size).\n  return augmented.repeat().shuffle(2048).batch(batch_size).prefetch(AUTO) \n\ntraining_dataset = get_training_dataset()\nvalidation_dataset = load_dataset(validation_fns).batch(batch_size).prefetch(AUTO)","9c333934":"CLASSES = ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']\n\ndef display_one_flower(image, title, subplot, color):\n  plt.subplot(subplot)\n  plt.axis('off')\n  plt.imshow(image)\n  plt.title(title, fontsize=16, color=color)\n  \n# If model is provided, use it to generate predictions.\ndef display_nine_flowers(images, titles, title_colors=None):\n  subplot = 331\n  plt.figure(figsize=(13,13))\n  for i in range(9):\n    color = 'black' if title_colors is None else title_colors[i]\n    display_one_flower(images[i], titles[i], 331+i, color)\n  plt.tight_layout()\n  plt.subplots_adjust(wspace=0.1, hspace=0.1)\n  plt.show()\n\ndef get_dataset_iterator(dataset, n_examples):\n  return dataset.unbatch().batch(n_examples).as_numpy_iterator()\n\ntraining_viz_iterator = get_dataset_iterator(training_dataset, 9)","160dcc07":"# Re-run this cell to show a new batch of images\nimages, classes = next(training_viz_iterator)\nclass_idxs = np.argmax(classes, axis=-1) # transform from one-hot array to class number\nlabels = [CLASSES[idx] for idx in class_idxs]\ndisplay_nine_flowers(images, labels)","9e6ae8b0":"def create_model():\n  pretrained_model = tf.keras.applications.Xception(input_shape=[*IMAGE_SIZE, 3], include_top=False)\n  pretrained_model.trainable = True\n  model = tf.keras.Sequential([\n    pretrained_model,\n    tf.keras.layers.GlobalAveragePooling2D(),\n    tf.keras.layers.Dense(5, activation='softmax')\n  ])\n  model.compile(\n    optimizer='adam',\n    loss = 'categorical_crossentropy',\n    metrics=['accuracy']\n  )\n  return model\n\nwith tpu_strategy.scope(): # creating the model in the TPUStrategy scope means we will train the model on the TPU\n  model = create_model()\nmodel.summary()","9a9d3b7a":"def count_data_items(filenames):\n  # The number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n  n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n  return np.sum(n)\n\nn_train = count_data_items(train_fns)\nn_valid = count_data_items(validation_fns)\ntrain_steps = count_data_items(train_fns) \/\/ batch_size\nprint(\"TRAINING IMAGES: \", n_train, \", STEPS PER EPOCH: \", train_steps)\nprint(\"VALIDATION IMAGES: \", n_valid)","251e23b7":"EPOCHS = 12\n\nstart_lr = 0.00001\nmin_lr = 0.00001\nmax_lr = 0.00005 * tpu_strategy.num_replicas_in_sync\nrampup_epochs = 5\nsustain_epochs = 0\nexp_decay = .8\n\ndef lrfn(epoch):\n  if epoch < rampup_epochs:\n    return (max_lr - start_lr)\/rampup_epochs * epoch + start_lr\n  elif epoch < rampup_epochs + sustain_epochs:\n    return max_lr\n  else:\n    return (max_lr - min_lr) * exp_decay**(epoch-rampup_epochs-sustain_epochs) + min_lr\n    \nlr_callback = tf.keras.callbacks.LearningRateScheduler(lambda epoch: lrfn(epoch), verbose=True)\n\nrang = np.arange(EPOCHS)\ny = [lrfn(x) for x in rang]\nplt.plot(rang, y)\nprint('Learning rate per epoch:')","3b7e7679":"history = model.fit(training_dataset, validation_data=validation_dataset,\n                    steps_per_epoch=train_steps, epochs=EPOCHS, callbacks=[lr_callback])\n\nfinal_accuracy = history.history[\"val_accuracy\"][-5:]\nprint(\"FINAL ACCURACY MEAN-5: \", np.mean(final_accuracy))","aa387856":"def display_training_curves(training, validation, title, subplot):\n  ax = plt.subplot(subplot)\n  ax.plot(training)\n  ax.plot(validation)\n  ax.set_title('model '+ title)\n  ax.set_ylabel(title)\n  ax.set_xlabel('epoch')\n  ax.legend(['training', 'validation'])\n\nplt.subplots(figsize=(10,10))\nplt.tight_layout()\ndisplay_training_curves(history.history['accuracy'], history.history['val_accuracy'], 'accuracy', 211)\ndisplay_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 212)","c347da49":"def flower_title(label, prediction):\n  # Both prediction (probabilities) and label (one-hot) are arrays with one item per class.\n  class_idx = np.argmax(label, axis=-1)\n  prediction_idx = np.argmax(prediction, axis=-1)\n  if class_idx == prediction_idx:\n    return f'{CLASSES[prediction_idx]} [correct]', 'black'\n  else:\n    return f'{CLASSES[prediction_idx]} [incorrect, should be {CLASSES[class_idx]}]', 'red'\n\ndef get_titles(images, labels, model):\n  predictions = model.predict(images)\n  titles, colors = [], []\n  for label, prediction in zip(classes, predictions):\n    title, color = flower_title(label, prediction)\n    titles.append(title)\n    colors.append(color)\n  return titles, colors\n\nvalidation_viz_iterator = get_dataset_iterator(validation_dataset, 9)","07dfe260":"# Re-run this cell to show a new batch of images\nimages, classes = next(validation_viz_iterator)\ntitles, colors = get_titles(images, classes, model)\ndisplay_nine_flowers(images, titles, colors)","2b15793a":"# We can save our model with:\nmodel.save('model.h5')\n# and reload it with:\nreloaded_model = tf.keras.models.load_model('model.h5')","b0d2eaf7":"# Re-run this cell to show a new batch of images\nimages, classes = next(validation_viz_iterator)\ntitles, colors = get_titles(images, classes, reloaded_model)\ndisplay_nine_flowers(images, titles, colors)","768e9637":"## Training","f594edf7":"## Save and re-loading our trained model","e5798918":"## Predictions\n\nLet's look at some our model's predictions next to the original images. We'll show 9 images from the validation set.","655a8299":"Accuracy goes up and loss goes down. Looks good!","7e23419d":"Actually train the model. While the first epoch will be quite a bit slower as we must XLA-compile the execution graph and load the data, later epochs should complete in ~5s.","ad800e21":"# TPUs in Colab&nbsp; <a href=\"https:\/\/cloud.google.com\/tpu\/\"><img valign=\"middle\" src=\"https:\/\/raw.githubusercontent.com\/GoogleCloudPlatform\/tensorflow-without-a-phd\/master\/tensorflow-rl-pong\/images\/tpu-hexagon.png\" width=\"50\"><\/a>\nIn this example, we'll work through training a model to classify images of\nflowers on Google's lightning-fast Cloud TPUs. Our model will take as input a photo of a flower and return whether it is a daisy, dandelion, rose, sunflower, or tulip.\n\nWe use the Keras framework, new to TPUs in TF 2.1.0. Adapted from [this notebook](https:\/\/colab.research.google.com\/github\/GoogleCloudPlatform\/training-data-analyst\/blob\/master\/courses\/fast-and-lean-data-science\/07_Keras_Flowers_TPU_xception_fine_tuned_best.ipynb) by [Martin Gorner](https:\/\/twitter.com\/martin_gorner).","3dd4ae32":"Let's take a peek at the training dataset we've created:","c33aade5":"## Enabling and testing the TPU\n\nFirst, you'll need to enable TPUs for the notebook:\n\n- Navigate to Edit\u2192Notebook Settings\n- select TPU from the Hardware Accelerator drop-down\n\nNext, we'll check that we can connect to the TPU:","e22c6d05":"\n## Input data\n\nOur input data is stored on Google Cloud Storage. To more fully use the parallelism TPUs offer us, and to avoid bottlenecking on data transfer, we've stored our input data in TFRecord files, 230 images per file.\n\nBelow, we make heavy use of `tf.data.experimental.AUTOTUNE` to optimize different parts of input loading.\n\nAll of these techniques are a bit overkill for our (small) dataset, but demonstrate best practices for using TPUs.\n","ab8bdbcf":"## Next steps\n\nMore TPU\/Keras examples include:\n- [Shakespeare in 5 minutes with Cloud TPUs and Keras](https:\/\/colab.research.google.com\/github\/tensorflow\/tpu\/blob\/master\/tools\/colab\/shakespeare_with_tpu_and_keras.ipynb)\n- [Fashion MNIST with Keras and TPUs](https:\/\/colab.research.google.com\/github\/tensorflow\/tpu\/blob\/master\/tools\/colab\/fashion_mnist.ipynb)\n\nWe'll be sharing more examples of TPU use in Colab over time, so be sure to check back for additional example links, or [follow us on Twitter @GoogleColab](https:\/\/twitter.com\/googlecolab).","ee923522":"Calculate the number of images in each dataset. Rather than actually load the data to do so (expensive), we rely on hints in the filename. This is used to calculate the number of batches per epoch.\n","5ca5f2ea":"## Model\nTo get maxmimum accuracy, we leverage a pretrained image recognition model (here, [Xception](http:\/\/openaccess.thecvf.com\/content_cvpr_2017\/papers\/Chollet_Xception_Deep_Learning_CVPR_2017_paper.pdf)). We drop the ImageNet-specific top layers (`include_top=false`), and add a max pooling and a softmax layer to predict our 5 classes.","aad269d2":"#### License","dc2f7361":"Calculate and show a learning rate schedule. We start with a fairly low rate, as we're using a pre-trained model and don't want to undo all the fine work put into training it.","56a3e2e4":"Copyright 2019-2020 Google LLC\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n\n---\n\n\nThis is not an official Google product but sample code provided for an educational purpose.\n"}}