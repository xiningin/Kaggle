{"cell_type":{"76849b52":"code","f95e3afd":"code","ec822691":"code","4aa2c785":"code","943ec78b":"code","32245731":"code","5f75dcc4":"code","04c4641e":"code","be7a1611":"code","53ea2fd3":"code","3597830c":"code","aac3fa5c":"code","7233ee60":"code","90ccbd66":"code","d837f4c4":"code","0d64dd7a":"code","1fb21a57":"code","950ff712":"code","d40100ce":"code","7a6b6210":"code","45b5b7a5":"code","8484d187":"markdown","5e08c10a":"markdown","47ab168c":"markdown","09652252":"markdown","2d769222":"markdown","cda322f1":"markdown","1c97e56c":"markdown","83ca7ccf":"markdown","b64a0556":"markdown","495f079e":"markdown","640e4742":"markdown","15a9b539":"markdown","adf578fa":"markdown","8e3a6c6c":"markdown","ae5e9880":"markdown","96292d6e":"markdown"},"source":{"76849b52":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nfrom sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f95e3afd":"train_data = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('..\/input\/titanic\/test.csv')\n\ntrain_data.head()","ec822691":"del train_data['Name']\ndel train_data['PassengerId']","4aa2c785":"train_data.isna().sum()","943ec78b":"train_data = train_data[train_data[\"Embarked\"].notna()]","32245731":"for col in train_data.columns:\n    print(col+\": \",len(pd.unique(train_data[col])), \" (\"+str(train_data[col].dtype)+\")\")","5f75dcc4":"train_data['Cabin'][train_data['Cabin'].isna()] = 'NaN'\nord_enc = OrdinalEncoder()\nord_enc = ord_enc.fit(train_data[['Ticket', 'Cabin']])\ntrain_data[['Ticket', 'Cabin']] = ord_enc.transform(train_data[['Ticket', 'Cabin']])","04c4641e":"train_data = pd.get_dummies(train_data, columns=['Sex', 'Embarked'])\ntrain_data.head()","be7a1611":"knn_imputer = KNNImputer(n_neighbors=5)\ntrain_data = pd.DataFrame(knn_imputer.fit_transform(train_data), columns=train_data.columns)\ntrain_data.head()","53ea2fd3":"train_data.isna().sum()","3597830c":"train_data.describe()","aac3fa5c":"train_data.median()","7233ee60":"train_data = train_data[(np.abs(stats.zscore(train_data['Fare'])) < 2.4)]\ntrain_data.describe()","90ccbd66":"variance = np.var(train_data)\nprint(variance)\nhighvar_cols = [col for col in train_data.columns if variance[col] > 2]\nprint(highvar_cols)","d837f4c4":"train_data_scaled = train_data.copy()\nminmax_scal = MinMaxScaler(feature_range=(0.0,1.0))\nminmax_scal = minmax_scal.fit(train_data_scaled[highvar_cols])\ntrain_data_scaled[highvar_cols] = minmax_scal.transform(train_data_scaled[highvar_cols])\ntrain_data_scaled.head()","0d64dd7a":"sns.pairplot(train_data_scaled)\nplt.show()","1fb21a57":"train_data_scaled.corr()","950ff712":"plt.figure(figsize=(12,8))\nsns.heatmap(train_data_scaled.corr())\nplt.show()","d40100ce":"def preprocess_titanic(data):\n    from sklearn.impute import KNNImputer\n    from sklearn.preprocessing import OrdinalEncoder\n    from sklearn.preprocessing import MinMaxScaler\n    from scipy import stats\n    \n    del data['Name']\n    del data['PassengerId']\n    \n    data = data[data[\"Embarked\"].notna()]\n    data['Cabin'][data['Cabin'].isna()] = 'NaN'\n    \n    ord_enc = OrdinalEncoder()\n    ord_enc = ord_enc.fit(data[['Ticket', 'Cabin']])\n    data[['Ticket', 'Cabin']] = ord_enc.transform(data[['Ticket', 'Cabin']])\n    \n    data = pd.get_dummies(data, columns=['Sex', 'Embarked'])\n    \n    knn_imputer = KNNImputer(n_neighbors=5)\n    data = pd.DataFrame(knn_imputer.fit_transform(data), columns=data.columns)\n    \n    data = data[(np.abs(stats.zscore(data['Fare'])) < 2.4)]\n    \n    variance = np.var(data)\n    highvar_cols = [col for col in data.columns if variance[col] > 2]\n\n    minmax_scal = MinMaxScaler(feature_range=(0.0,1.0))\n    minmax_scal = minmax_scal.fit(data[highvar_cols])\n    data[highvar_cols] = minmax_scal.transform(data[highvar_cols])\n    return data","7a6b6210":"teste = pd.read_csv('..\/input\/titanic\/train.csv')\nteste = preprocess_titanic(teste)\nteste.head()","45b5b7a5":"train_data_scaled.head()","8484d187":"For the embarked and sex features we'll be using one hot encoding because the problem with the ordinal encoding is that the model could learn a order relationship between the values, and as we know, there isn't this kind of relation on these features values.","5e08c10a":"# Feature scaling\n\nFor a lot of machine learning algorithms is important for the data to have the same scale, so we'll be applying the MinMax encoding on the features with high variance.","47ab168c":"# Correlation analysis\n\nAs we can see above, there are no features with high correlation.","09652252":"# Handling missing data II","2d769222":"For the classification models predictions some features will not give useful information, so we'll be removing the features 'PassengerId' and 'Name' that arent relevant information to know if a passenger survived or not.","cda322f1":"# Removing irrelevant features","1c97e56c":"One way to check if a attribute has outliers is to check the statistical summary of the data. If the feature has a high discrepance between the mean and the median, its likely that it has outliers.","83ca7ccf":"# Textual data encoding","b64a0556":"Most of the available machine learning algorithms can't handle textual data, so we'll be transforming them into numercical data.\n\nFirst we'll see how many unique values each of the textual features has to see the right encoding for each of them. If we aren't careful with the encoding, the model can suffer from the curse of dimensionality or it'll learn the features in the wrong way.","495f079e":"As can be seen, only the fare feature has a significant difference between the mean and median, so we'll be removing all rows where the fare is higher than 2.4 standard deviations. ","640e4742":"# Libraries importing and configuration","15a9b539":"As can be seen, some of the features would lead to high dimensional data if we applied the one hot encoding. So, we'll apply the Ordinal encoding for the ticket and cabin features.","adf578fa":"As the feature Embarked has a small number of missing data, we can remove the rows containing them.","8e3a6c6c":"# Checking for outliers","ae5e9880":"# Handling missing data I","96292d6e":"For the rest, we'll be using the kNN imputer to fill the missing data."}}