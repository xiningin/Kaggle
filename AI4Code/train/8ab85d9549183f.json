{"cell_type":{"39b62cd9":"code","f7940980":"code","e85eded9":"code","0cfc17e2":"code","9fa3abc7":"code","40693f6e":"code","808106f9":"code","4a692c78":"code","f00ad48f":"code","fcd138e7":"code","abf8ae4b":"code","72141307":"code","165ae8c2":"code","eb569b51":"code","72074628":"code","8ee83eb5":"code","1b8ea824":"markdown"},"source":{"39b62cd9":"!pip install -U kaggle_environments cpprb","f7940980":"import gc\nfrom multiprocessing import set_start_method, cpu_count, Process, Event, SimpleQueue\nimport time\n\nimport numpy as np\nimport tensorflow as tf\nimport cpprb # Replay Buffer Library: https:\/\/ymd_h.gitlab.io\/cpprb\/\nfrom tqdm.notebook import tqdm\n\nfrom kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, row_col\nfrom kaggle_environments import make\n\n# %load_ext tensorboard\n# %tensorboard --logdir logs","e85eded9":"# Global config\n#RIGHT = 0\n#GO = 1\n#LEFT = 2\n\nGOOSE = -1\nFOOD = 1\n\nact_shape = 4\n\nWIDTH = 11\nHEIGHT = 7\n\nxc = WIDTH\/\/2 + 1\nyc = HEIGHT\/\/2 + 1\n\nEAST_idx  = (xc+1,yc  )\nNORTH_idx = (xc  ,yc-1)\nWEST_idx  = (xc-1,yc  )\nSOUTH_idx = (xc  ,yc+1)\n\nAROUND = ([xc+1,xc  ,xc-1,xc  ],\n          [yc  ,yc-1,yc  ,yc+1])\n\ncode2dir = {0:'EAST', 1:'NORTH', 2:'WEST', 3:'SOUTH'}\n\ndir2code = {\"EAST\":0, \"NORTH\": 1, \"WEST\":2, \"SOUTH\": 3}","0cfc17e2":"def create_model():\n    model = tf.keras.Sequential([tf.keras.layers.Dense(100,activation=\"relu\",input_shape=(WIDTH*HEIGHT,)),\n                                 tf.keras.layers.Dense(100,activation=\"relu\"),\n                                 tf.keras.layers.Dense(100,activation=\"relu\"),\n                                 tf.keras.layers.Dense(act_shape)])\n    return model","9fa3abc7":"def Q_func(model,obs,act):\n    return tf.reduce_sum(model(obs) * tf.one_hot(act,depth=act_shape), axis=1)\n\ndef Q1_func(model,next_obs,rew,done):\n    gamma = 0.99\n    return gamma*tf.reduce_max(model(next_obs),axis=1)*(1.0-done) + rew\n\n#@tf.function\ndef train_then_absTD(model,target,obs,act,rew,next_obs,done,weights):\n    with tf.GradientTape() as tape:\n        tape.watch(model.trainable_weights)\n        Q = Q_func(model,obs,act)\n        yQ1_r = Q1_func(target,next_obs,rew,done)\n        TD_square = tf.square(Q - yQ1_r)\n        weighted_loss = tf.reduce_mean(TD_square * weights)\n\n    grad = tape.gradient(weighted_loss,model.trainable_weights)\n    opt.apply_gradients(zip(grad,model.trainable_weights))\n\n    Qnew = Q_func(model,obs,act)\n    return tf.abs(Qnew - yQ1_r)\n\n#@tf.function\ndef abs_TD(model,target,obs,act,rew,next_obs,done):\n    Q = Q_func(model,obs,act)\n    yQ1_r = Q1_func(target,next_obs,rew,done)\n    return tf.abs(Q - yQ1_r)   ","40693f6e":"def pos(index):\n    return index%WIDTH, index\/\/WIDTH\n\ndef centering(z,dz,Z):\n    z += dz\n    if z < 0:\n        z += Z\n    elif Z >= Z:\n        z -= Z\n    return z\n    \n\ndef encode_board(obs,act=\"NORTH\",idx=0):\n    \"\"\"\n    Player goose is always set at the center\n    \"\"\"\n    board = np.zeros((WIDTH,HEIGHT))\n\n    if len(obs[\"geese\"][idx]) == 0:\n        return board\n        \n    x0, y0 = pos(obs[\"geese\"][idx][0])\n    dx = xc - x0\n    dy = yc - y0\n    \n    for goose in obs[\"geese\"]:\n        for g in goose[:-1]: # the last tail is safe\n            x, y = pos(g)\n            \n            x = centering(x,dx,WIDTH)\n            y = centering(y,dy,HEIGHT)\n                \n            board[x,y] = GOOSE\n            \n    for food in obs[\"food\"]:\n        x, y = pos(food)\n        \n        x = centering(x,dx,WIDTH)\n        y = centering(y,dy,HEIGHT)\n        \n        board[x,y] = FOOD\n        \n    board[xc,yc] = dir2code[act]\n\n    # Avoid Body Hit add psudo GOOSE\n    if act == \"EAST\":\n        board[WEST_idx] = GOOSE\n    elif act == \"NORTH\":\n        board[SOUTH_idx] = GOOSE\n    elif act == \"WEST\":\n        board[EAST_idx] = GOOSE\n    elif act == \"SOUTH\":\n        board[NORTH_idx] = GOOSE\n    else:\n        raise\n    \n    return board","808106f9":"def get_obs_action(model,states,idx=0, train=False):\n    act = states[idx][\"action\"]\n\n    if states[idx][\"status\"] != \"ACTIVE\":\n        return None, act\n    \n    board = encode_board(states[0][\"observation\"],act=act,idx=idx)\n    \n    # e-greedy\n    if train:\n        if np.random.random() < 0.1:\n            new_act = np.random.randint(4)\n        else:\n            new_act = int(tf.math.argmax(tf.squeeze(model(board.reshape(1,-1)))))    \n    else:\n        Q = tf.squeeze(model(board.reshape(1,-1))).numpy()\n        OK = (board[AROUND] != GOOSE)\n        \n        new_act = 0\n        max_v = -99999\n        for i, (q,ok) in enumerate(zip(Q,OK)):\n            if (q > max_v) and ok:\n                new_act = i\n                max_v = q\n\n    return board, code2dir[new_act]","4a692c78":"def get_obs_action_greedy(states,idx=0):\n    act = states[idx][\"action\"]\n    \n    if states[idx][\"status\"] != \"ACTIVE\":\n        return None, act\n    \n    board = encode_board(states[0][\"observation\"],act=act,idx=idx)\n    \n    obs = states[0][\"observation\"]\n\n    if len(obs[\"geese\"][idx]) == 0 or len(obs[\"food\"]) == 0:\n        return board, act\n    \n    x0, y0 = pos(obs[\"geese\"][idx][0])\n    \n    min_len = WIDTH + HEIGHT\n    min_i = 0\n    NG = (board[AROUND] == GOOSE)\n    for i, food in enumerate(obs[\"food\"]):\n        x, y = pos(food)\n        \n        dx = x - x0\n        dy = y - y0\n        L = abs(dx) + abs(dy)\n        \n        if dx == 0:\n            if (dy > 0) and NG[dir2code[\"SOUTH\"]]:\n                L += 2\n            elif (dy < 0) and NG[dir2code[\"NORTH\"]]:\n                L += 2\n        if dy == 0:\n            if (dx > 0) and NG[dir2code[\"EAST\"]]:\n                L += 2\n            elif (dx < 0) and NG[dir2code[\"WEST\"]]:\n                L += 2\n            \n        if L < min_len:\n            min_len = L\n            min_i = i\n\n    food = obs[\"food\"][min_i]\n    x, y = pos(food)\n\n    if (x > x0):\n        return board, \"EAST\"\n    \n    if (x < x0):\n        return board, \"WEST\"\n    \n    if (y > y0):\n        return board, \"SOUTH\"\n    \n    if (y < y0):\n        return board, \"NORTH\"\n    \n    return board, act","f00ad48f":"def create_buffer(buffer_size,env_dict,alpha):\n    return cpprb.MPPrioritizedReplayBuffer(buffer_size,env_dict,alpha=alpha)","fcd138e7":"def explorer(global_rb,env_dict,is_training_done,queue):\n    local_buffer_size = int(1e+2)\n    local_rb = cpprb.ReplayBuffer(local_buffer_size+4,env_dict)\n\n    model = create_model()\n    target = tf.keras.models.clone_model(model)\n    env = make(\"hungry_geese\", debug=False)\n    \n    states = env.reset(4)\n    while not is_training_done.is_set():\n        if not queue.empty():\n            w,wt = queue.get()\n            model.set_weights(w)\n            target.set_weights(wt)\n\n        board_act = [get_obs_action(model,states,i,train=True) if i < 2 else get_obs_action_greedy(states,i)\n                     for i in range(4)]\n\n        states = env.step([a for b,a in board_act])\n\n        for i, (b, a) in enumerate(board_act):\n            if b is None:\n                continue\n\n            local_rb.add(obs=b.ravel(),\n                         act=dir2code[a],\n                         next_obs=encode_board(states[0][\"observation\"],act=a,idx=i).ravel(),\n                         rew=states[i][\"reward\"],\n                         done=(states[i][\"status\"] != \"ACTIVE\"))\n\n        if all(s[\"status\"] != \"ACTIVE\" for s in states):\n            states = env.reset(4)\n            local_rb.on_episode_end()\n\n        if local_rb.get_stored_size() >= local_buffer_size:\n            sample = local_rb.get_all_transitions()\n            global_rb.add(**sample,\n                          priorities=abs_TD(model,target,\n                                            tf.constant(sample[\"obs\"]),\n                                            tf.constant(sample[\"act\"].ravel()),\n                                            tf.constant(sample[\"rew\"].ravel()),\n                                            tf.constant(sample[\"next_obs\"]),\n                                            tf.constant(sample[\"done\"].ravel())))\n            local_rb.clear()            ","abf8ae4b":"%%time\n\n# Training\nn_warming = 100\nn_train_step = int(1e+4)\nbatch_size = 64\n\nwriter = tf.summary.create_file_writer(\".\/logs\")\n\n# Replay Buffer \nbuffer_size = 10e+5\nenv_dict = {\"obs\": {\"shape\": (WIDTH*HEIGHT)},\n            \"act\": {\"dtype\": int},\n            \"next_obs\": {\"shape\": (WIDTH*HEIGHT)},\n            \"rew\": {},\n            \"done\": {}}\nalpha = 0.5\nrb = create_buffer(buffer_size, env_dict,alpha)\n\n# Model\ntarget_update = 50\n\n\nmodel = create_model()\ntarget = tf.keras.models.clone_model(model)\n\nopt = tf.keras.optimizers.Adam()\n\n# Ape-X\nexplorer_update_freq = 100\nn_explorer = cpu_count() - 1\n\n\nis_training_done = Event()\nis_training_done.clear()\n\nqs = [SimpleQueue() for _ in range(n_explorer)]\nps = [Process(target=explorer,\n              args=[rb,env_dict,is_training_done,q])\n      for q in qs]\n\nfor p in ps:\n    p.start()\n\nprint(\"warm-up\")\nwhile rb.get_stored_size() < n_warming:\n    time.sleep(1)\n\n\nprint(\"training\")\n    \nepoch = 0\nfor i in tqdm(range(n_train_step)):        \n    sample = rb.sample(batch_size,beta=0.4)\n    \n    absTD = train_then_absTD(model,target,\n                             tf.constant(sample[\"obs\"]),\n                             tf.constant(sample[\"act\"].ravel()),\n                             tf.constant(sample[\"rew\"].ravel()),\n                             tf.constant(sample[\"next_obs\"]),\n                             tf.constant(sample[\"done\"].ravel()),\n                             tf.constant(sample[\"weights\"].ravel()))\n    rb.update_priorities(sample[\"indexes\"],absTD)\n        \n    if i % target_update == 0:\n        target.set_weights(model.get_weights())\n        \n    if i % explorer_update_freq == 0:\n        w = model.get_weights()\n        wt = target.get_weights()\n        for q in qs:\n            q.put((w,wt))\n\n    \nis_training_done.set()\n\n!mkdir -p sub\nmodel.save(\"sub\/model\")\n\nfor p in ps:\n    p.join()","72141307":"test_env = make(\"hungry_geese\", debug=True)\n\nfor _ in range(4):\n    states = test_env.reset(4)\n    while any(s[\"status\"] == \"ACTIVE\" for s in states):\n        board_act = [get_obs_action(model,states,i) for i in range(4)]\n        #board_act = [get_obs_action_greedy(states,i) for i in range(4)]\n        states = test_env.step([a for b,a in board_act])\n\n    test_env.render(mode='ipython')","165ae8c2":"%%writefile sub\/main.py\n\nimport sys\nimport os\n\nsys.path.append(\"\/kaggle_simulations\/agent\")\nworking_dir = \"\/kaggle_simulations\/agent\"\n\nif os.path.exists(\"sub\/model\"):\n    model_f = \"sub\/model\"\nelif os.path.exists(os.path.join(working_dir,\"model\")):\n    model_f = os.path.join(working_dir,\"model\")\nelse:\n    raise ValueError(\"No model file\")\n    \nprint(model_f)\n\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, row_col\n\nGOOSE = -1\nFOOD = 1\n\nact_shape = 4\n\nWIDTH = 11\nHEIGHT = 7\n\nxc = WIDTH\/\/2 + 1\nyc = HEIGHT\/\/2 + 1\n\nEAST_idx  = (xc+1,yc  )\nNORTH_idx = (xc  ,yc-1)\nWEST_idx  = (xc-1,yc  )\nSOUTH_idx = (xc  ,yc+1)\n\n\nAROUND = ([xc+1,xc  ,xc-1,xc  ],\n          [yc  ,yc-1,yc  ,yc+1])\n\n\ncode2dir = {0:'EAST', 1:'NORTH', 2:'WEST', 3:'SOUTH'}\ndir2code = {\"EAST\":0, \"NORTH\": 1, \"WEST\":2, \"SOUTH\": 3}\n\n\npolicy = tf.keras.models.load_model(model_f)\nLAST_ACT = \"NORTH\"\n\ndef pos(index):\n    return index%WIDTH, index\/\/WIDTH\n\ndef centering(z,dz,Z):\n    z += dz\n    if z < 0:\n        z += Z\n    elif Z >= Z:\n        z -= Z\n    return z\n    \n\ndef encode_board(obs,idx=0):\n    \"\"\"\n    Player goose is always set at the center\n    \"\"\"\n    global LAST_ACT\n    act = LAST_ACT\n\n    board = np.zeros((WIDTH,HEIGHT))\n\n    if len(obs[\"geese\"][idx]) == 0:\n        return board\n        \n    x0, y0 = pos(obs[\"geese\"][idx][0])\n    dx = xc - x0\n    dy = yc - y0\n    \n    for goose in obs[\"geese\"]:\n        for g in goose[:-1]: # the last tail is safe\n            x, y = pos(g)\n            \n            x = centering(x,dx,WIDTH)\n            y = centering(y,dy,HEIGHT)\n                \n            board[x,y] = GOOSE\n            \n    for food in obs[\"food\"]:\n        x, y = pos(food)\n        \n        x = centering(x,dx,WIDTH)\n        y = centering(y,dy,HEIGHT)\n        \n        board[x,y] = FOOD\n        \n    board[xc,yc] = dir2code[act]\n\n    # Avoid Body Hit add psudo GOOSE\n    if act == \"EAST\":\n        board[WEST_idx] = GOOSE\n    elif act == \"NORTH\":\n        board[SOUTH_idx] = GOOSE\n    elif act == \"WEST\":\n        board[EAST_idx] = GOOSE\n    elif act == \"SOUTH\":\n        board[NORTH_idx] = GOOSE\n    else:\n        raise\n\n    return board\n\n\ndef get_action(obs_dict,config_dict):\n    global policy\n    global LAST_ACT\n    \n    idx = Observation(obs_dict).index\n    board = encode_board(obs_dict,idx)\n\n    Q = tf.squeeze(policy(board.reshape(1,-1))).numpy()\n    OK = (board[AROUND] != GOOSE)\n\n    new_act = 0\n    max_v = -99999\n    for i, (q,ok) in enumerate(zip(Q,OK)):\n        if (q > max_v) and ok:\n            new_act = i\n            max_v = q\n    \n    LAST_ACT = code2dir[new_act]\n\n    return LAST_ACT","eb569b51":"# Test with self\n\ntest_env.run([\"sub\/main.py\",\"sub\/main.py\",\"sub\/main.py\",\"sub\/main.py\"])\ntest_env.render(mode='ipython')","72074628":"# Test with sample agent\n\ntest_env.run([\"sub\/main.py\",\"sub\/main.py\",\"..\/input\/hungry-geese\/agent.py\",\"..\/input\/hungry-geese\/agent.py\"])\ntest_env.render(mode='ipython')","8ee83eb5":"import tarfile\nimport os.path\n\ndef make_tarfile(output_filename, source_dir):\n    with tarfile.open(output_filename, \"w:gz\") as tar:\n        tar.add(source_dir, arcname=os.path.basename(source_dir))\n\nmake_tarfile('submission.tar.gz', '.\/sub\/')","1b8ea824":"* Learn Q function by DQN with Ape-X (3 explorers + 1 learner)\n  * Encode board such as player position is always center\n  * Each explorer has 2 NN actors and 2 rule based greedy actors\n* Rule based Safe Guard to avoid Body Hit and Collision\n* Submit multiple files (code + model file) under this instruction https:\/\/www.kaggle.com\/c\/google-football\/discussion\/191257"}}