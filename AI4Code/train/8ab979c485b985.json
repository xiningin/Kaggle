{"cell_type":{"195016b1":"code","d0ad2341":"code","767e9432":"code","b42158fe":"code","2259269b":"code","ae8a3a81":"code","a60c538b":"code","e034aa78":"code","dfaa3a07":"code","c5593826":"code","37cb1cb4":"code","faf9668f":"code","be377141":"code","efae0866":"code","bbd48313":"markdown","fe47d9c3":"markdown","6f7bee62":"markdown","5d04fe3a":"markdown","17e07c08":"markdown","ad01d5aa":"markdown","90cd56e2":"markdown","ab41230e":"markdown","4f15dcc3":"markdown","ec8220da":"markdown","69dbbcdc":"markdown","1357d1e3":"markdown","3dcfc42c":"markdown","d06c7599":"markdown","5f60e5cb":"markdown","253c8e8f":"markdown","8b37ccaa":"markdown"},"source":{"195016b1":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import recall_score","d0ad2341":"df = pd.read_csv(\"..\/input\/heart-disease-uci\/heart.csv\")\ndf.head(3)","767e9432":"df.info()","b42158fe":"X, Y = df.drop(\"target\", axis=1), df.target","2259269b":"steps = [\n    (\"feature_selection\", VarianceThreshold()),\n    (\"model\", LogisticRegression(solver='lbfgs', max_iter=1000))\n]","ae8a3a81":"exec_pipeline = Pipeline(steps)","a60c538b":"class MyInputer(object):\n    def __init__(self, fill_value):\n        self.fill_value = fill_value\n    \n    def fit(self, X, y):\n        return self\n    \n    def transform(self, X):\n        if isinstance(X, pd.DataFrame):\n            return X.fillna(self.fill_value)\n        else:\n            for i in range(X.shape[0]):\n                for j in range(X.shape[1]):\n                    if np.isnan(X[i][j]):\n                        X[i][j] = self.fill_value\n        \n        return X","e034aa78":"my_inputer = MyInputer(0)","dfaa3a07":"exec_pipeline.steps.insert(0, (\"inputer\", my_inputer))","c5593826":"n_folds = 10\nrecall_sum = 0\n\nkf = StratifiedKFold(n_splits=n_folds)\nfor train_idx, test_idx in kf.split(X, Y):\n    exec_pipeline.fit(X.loc[train_idx], Y[train_idx])\n    \n    Y_preds = exec_pipeline.predict(X.loc[test_idx])\n    recall_sum += recall_score(Y[test_idx], Y_preds)\n    \nprint(f\"The average recall score was {recall_sum\/n_folds:.2f}\")","37cb1cb4":"exec_pipeline.score(X.loc[test_idx], Y[test_idx])","faf9668f":"exec_pipeline[\"inputer\"]","be377141":"exec_pipeline[:2]","efae0866":"exec_pipeline.set_params(feature_selection__threshold=0.1)","bbd48313":"---\n\n## Read the Data","fe47d9c3":"### Add Processing Nodes\n\nDynammically adding processing nodes to the pipeline is also not an issue. We could, for example, pre-define a base pipeline, and increment on demand before executing it.","6f7bee62":"## Introduction\n\nExploratory data science projects are often very dynamic and potentially messy since the vast majority of them are developed with the aid of notebooks. Due to its very nature, notebooks usually favor bad coding practices and, consequently, makes it difficult to maintain - or even understand - the solution.\n\nConsidering these data science pipelines have pretty decoupled components, which usually have an organic connection between the processing steps, it makes sense to centralize them in a succint and organized way so debugging and maintanance procedures become easier.\n\nBigger projects may require more elaborated interactions among their components, while also keeping a well defined execution flow. For such scenarios, there is an awesome Python framework called [Kedro](https:\/\/kedro.readthedocs.io\/en\/stable\/) that sublimely manages different data sources, and code pieces. This notebook, on the other hand, will be mainly focused on relatively small pipelines with relatively simple underlying structure. For this matter we'll be using [SciKit-Learn's Pipeline Module](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.pipeline.Pipeline.html).","5d04fe3a":"Since our focus here is on the pipeline resources, we'll use a simple dataset about [Heart Disease Classification](https:\/\/www.kaggle.com\/ronitf\/heart-disease-uci)","17e07c08":"---\n\n## Thanks you for the reading.\n### Please give your feedback. It will be more than welcome.","ad01d5aa":"---\n\n## Executing the Pipeline\n\nSince every node from the pipeline implements a `fit` function, the fitting procedure can easily be done by simply calling a `fit` function out of the pipeline object, with the respective features (X), and target (Y) variables.\n\nOur pipeline can be composed only of transformer methods, what would imply on the use of the `transform` function to get its final output. However, it can also comprise a predictor as the final node. Hence, to get our final output we need to actually call the `predict` function.\n\n**Note: the input used to fit the `i+1-th` node, is always the output from the `i-th`.**","90cd56e2":"---\n\n## Building a Simple Pipeline\n\nTaking into consideration that the dataset is pretty small - and pretty much ready to use - there is not a lot of preprocessing steps to be done. Below we employ a simple feature selection method called Variance Threshold which removes features with little to no variation in value (a possible indicator of irrelevance to the prediction process depending on the feature nature).\n\nNote that we also use a Logistic Regression model as the predictor once its learning procedure is a bit simpler, and have bigger chances to reach better performance with smaller datasets.\n\n","ab41230e":"If the pipeline estimator implements the `score` function, it will also be available to the pipeline object.","4f15dcc3":"## Retrieving Nodes\nEven though we're dealing with pipelines, there may be situations in which we are required to execute a single step, or even a subset of them. In such cases, we can access the nodes in two different ways, namely, by name, and by index.\n\n### By Name\nAccessing by name is nothing different from a dictionary. In this case, the key is the informed string corresponding to the wanted node.","ec8220da":"As a matter of fact, we can expect a pipeline to ingest a dataflow, and output a transformed version of the same data. With this in mind, SKLearn's pipeline requires all intermediate nodes to follow the **\"transformer\"** interface, i.e. they must implement the `fit` and `transform` methods. The final node, on the other hand, is only required to implement the `fit` method.\n\nThe creation of a pipeline consists of simply presenting a list of processing steps, where each item comprises a name for the node, and an instantiated object that will be used on the process.","69dbbcdc":"### By Index\n\nThe access by index is slight different from accessing by name in the fact that we can either access a single node, or an entire subset of nodes. Since the derived slice is a new pipeline, it has all properties described here in this notebook.","1357d1e3":"The insertion of a new processing step can be done in any place of the execution pipeline, only requiring a bit more of attention when manipulating the last node because of its different interface requirements.","3dcfc42c":"### Implementing You Own Node\n\nPython is a widely used language, what increases the probability of finding already implemented functions and classes that best fits our needs. However, in some cases we still need to apply some changes, or even implement some functions from scratch. Fortunately, this won't be a problem for SKLearn's Pipeline once the required interface is implemented on the class.\n\nFor illustration's sake, below we implement a simple inputer class (even though there is not missing values in our data :D)","d06c7599":"---\n\n## Conclusion","5f60e5cb":"---\n\n## Accessing Parameters\n\nAn important aspect of a pipeline, is the wide variety of parameters that are available for tunning. With SKLearn pipelines we can redefine the parameters values by simply using the `set_params` function. Considering the possibility of different nodes having parameters with the same name, we need to specify the parameters we wish to change in a particular way. It is necessary to append a prefix with the name of the node to which the feature belongs to followed by a `__`. For example, `feature_selection__` (node's name) + `threshold` (the parameter).","253c8e8f":"In this notebook we have discussed the use of a simple resource capable of improving the readability, development speed, and maintainability of data science code. Even though more suited for small projects, this can also be incorporated in many bigger projects with equal relevance. For more information about Scikit-Learn's pipelines, follow the [official user guide](https:\/\/scikit-learn.org\/stable\/modules\/compose.html#pipeline).","8b37ccaa":"As we can see below, every feature in our dataset is numerical and not null. Consequently, it doesn't require lots of preprocessing steps"}}