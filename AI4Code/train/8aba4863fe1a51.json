{"cell_type":{"55b8a86f":"code","9b9924b3":"code","22c24b5c":"code","01453756":"code","08d2e708":"code","f114c914":"code","cdb4bb57":"code","64037431":"code","f6895ce9":"code","7392a362":"code","ad966dfa":"code","49986642":"code","5a7801ae":"code","63b06531":"code","20f5e240":"code","b053a3d8":"code","85947856":"code","ae503d81":"code","22d4f46b":"code","5e613dff":"code","bb649236":"code","c89bebf6":"code","a53b2616":"code","4cac7e09":"code","b95ec09e":"code","de7f54cb":"code","67994f08":"code","f99875f1":"code","f9ecf176":"code","23bc78f2":"code","cafce1f3":"code","472ab960":"code","c2d59bf7":"markdown","1ca356b6":"markdown","2f14090a":"markdown","1dbc46ac":"markdown","2f0aec9a":"markdown","e991fd97":"markdown","81f579b4":"markdown","76f9522e":"markdown","e8bbb219":"markdown","184c87b0":"markdown","da36faea":"markdown","9ab10cb0":"markdown","ebead4e0":"markdown","9f2d3159":"markdown","5349a053":"markdown"},"source":{"55b8a86f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","9b9924b3":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\ndata_train = pd.read_csv('..\/input\/train.csv')\ndata_test = pd.read_csv('..\/input\/test.csv')\n\ndata_train.sample(3)\n","22c24b5c":"data_test.sample(3)","01453756":"#Let's look at data description\n\ndata_train.describe() \n\n","08d2e708":"#let's check missing values in each columns\ndata_train.isna().sum()\n# Age 177 , and Cabin 687 Values are missing ","f114c914":"# Female should have higher change of survival - lets see if thats right-\nsns.barplot(x=\"Sex\",y=\"Survived\",data=data_train).set_title(\"Female percentage of survival {0}\".format(data_train['Survived'][data_train.Sex=='female'].value_counts(normalize=True)[1]*100))\n#over 74 of female passengers survived. Clearly a higher chance of survival.","cdb4bb57":"#Let Draw a barplot between 'Embarked' and 'Survived' and compare it with 'Sex'\nsns.barplot(x=\"Embarked\", y=\"Survived\", hue=\"Sex\", data=data_train)\n\n#Looking at the Bar Plot , we can say male survival rate is less in each Embarked.","64037431":"#Lets Draw a Bar plot between Pclass & Survived and Hue it on 'Sex'\nsns.barplot(x='Pclass',y=\"Survived\",hue=\"Sex\",data=data_train)\n# Here also Male survival rate is less in each Pclass.","f6895ce9":"#draw a bar plot for SibSp vs. survival\nsns.barplot(x=\"SibSp\", y=\"Survived\", data=data_train)","7392a362":"#draw a bar plot for Parch vs. survival\nsns.barplot(x=\"Parch\", y=\"Survived\", data=data_train)","ad966dfa":"data_train.head()","49986642":"# Here i will impute missing values in Age and Cabin features \n# Ill use Knn for the imputation , coz data set is quite small and it will give me better imputation values.\n\n\n#digitize age\n#import Knn from fancyimpute\n#Compare both values\n\nfrom fancyimpute import KNN\n#data_train['Age']=np.digitize(data_train['Age'],bins=[0,5,12,18,25,35,60,120])\nfeatures=['Age','Pclass','SibSp','Parch','Fare']\ndata_train_numeric=data_train[features].as_matrix()\ndata_test_numeric=data_test[features].as_matrix()\ndata_train_imputed=pd.DataFrame(KNN(6).complete(data_train_numeric),index=data_train.index)\ndata_test_imputed=pd.DataFrame(KNN(6).complete(data_test_numeric),index=data_test.index)\ndata_train['Age_imputed']=data_train_imputed.iloc[:,0]\ndata_test['Age_imputed']=data_train_imputed.iloc[:,0]\n\n","5a7801ae":"#Lets compare Age imputed values with Age column \n\ndata_train[data_train['Age'].isnull()].head()\n#Let's drop 'Age' column \ndata_train=data_train.drop('Age',axis=1)\ndata_test=data_test.drop('Age',axis=1)\n\n","63b06531":"data_train['Age']=data_train['Age_imputed']\ndata_test['Age']=data_test['Age_imputed']\ndata_train=data_train.drop('Age_imputed',axis=1)\ndata_test=data_test.drop('Age_imputed',axis=1)\ndata_train.head()\n","20f5e240":"#Funtion definiton for Age categorization \ndef simplify_ages(df):\n    \n    bins = (-1, 0, 5, 12, 18, 25, 35, 60, 120)\n    group_names = ['Unknown', 'Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Senior']\n    categories = pd.cut(df.Age, bins, labels=group_names)\n    df.Age = categories\n    return df\n\n\n\ndef simplify_fares(df):\n    df.Fare = df.Fare.fillna(-0.5)\n    bins = (-1, 0, 8, 15, 31, 1000)\n    group_names = ['Unknown', '1_quartile', '2_quartile', '3_quartile', '4_quartile']\n    categories = pd.cut(df.Fare, bins, labels=group_names)\n    df.Fare = categories\n    return df\n\ndef format_name(df):\n    df['Lname'] = df.Name.apply(lambda x: x.split(' ')[0])\n    df['Title'] = df.Name.apply(lambda x: x.split(' ')[1])\n\n    \n    return df    \n    \ndef drop_features(df):\n    return df.drop(['Ticket', 'Name', 'Embarked'], axis=1)\n\ndef transform_features(df):\n    df = simplify_ages(df)\n    df = simplify_fares(df)\n    df = format_name(df)\n    df = drop_features(df)\n    return df\n\ndata_train = transform_features(data_train)\ndata_test = transform_features(data_test)\ndata_train.head()","b053a3d8":"#lets plot graph for Age,Cabin& fare \n\n#male- Bay and all female seems to have survival ratio","85947856":"plt.figure(figsize=(15,8))\nsns.barplot(x=\"Age\", y=\"Survived\", data=data_train)","ae503d81":"sns.barplot(x=\"Fare\", y=\"Survived\", hue=\"Sex\", data=data_train);\n#Survival rate of males are increasing with Fare.","22d4f46b":"from sklearn import preprocessing\ndef encode_features(df_train, df_test):\n    features = [ 'Sex', 'Lname', 'Title','Age','Fare']\n    df_combined = pd.concat([df_train[features], df_test[features]])\n    \n    for feature in features:\n        le = preprocessing.LabelEncoder()\n        le = le.fit(df_combined[feature])\n        df_train[feature] = le.transform(df_train[feature])\n        df_test[feature] = le.transform(df_test[feature])\n    return df_train, df_test\n    \ndata_train, data_test = encode_features(data_train, data_test)\ndata_train.head()\n","5e613dff":"from sklearn.model_selection import train_test_split\n\nX_all = data_train.drop(['Survived','Cabin', 'PassengerId'], axis=1)\n\ny_all = data_train['Survived']\n\nnum_test = 0.20\nX_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=num_test, random_state=23)","bb649236":"#import GridSearchCv \nfrom sklearn.model_selection import GridSearchCV","c89bebf6":"#Logestic regression \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nparam_grid={'C':[0.001,0.01,0.1,1,10,100]}\nclf=LogisticRegression()\ngrid=GridSearchCV(clf,param_grid,cv=10,scoring='accuracy').fit(X_train,y_train)\ny_pred=grid.predict(X_test)\nacc_lreg=round(accuracy_score(y_pred,y_test)*100,2)\nprint(\"Accuracy Score for Logestic Regression {0}\".format(acc_lreg))\n","a53b2616":"#Support Vector Machine\nfrom sklearn.svm import SVC\nclf=SVC()\nCs=[0.001,0.01,0.1,1,10,]\ngammas=[0.001,0.01,0.1,1]\nparam_grid={'C':Cs,'gamma':gammas}\ngrid=GridSearchCV(clf,param_grid,cv=10,scoring='accuracy').fit(X_train,y_train)\ny_pred=grid.predict(X_test)\nacc_svc=round(accuracy_score(y_pred,y_test)*100,2)\nprint(\"Accuracy Score for Support Vector Machine{0}\".format(acc_svc))\n","4cac7e09":"#Decision Tree classifier\nfrom sklearn.tree import DecisionTreeClassifier\nclf=DecisionTreeClassifier()\nparam_grid={'max_depth':[2,4,6,8,10],'max_features':[2,3,4,5,6,7]}\ngrid=GridSearchCV(clf,param_grid,cv=10,scoring='accuracy').fit(X_train,y_train)\ny_pred=grid.predict(X_test)\nacc_dtree=round(accuracy_score(y_pred,y_test)*100,2)\nprint(\"Accuracy score for decision tree{0}\".format(acc_dtree))\n","b95ec09e":"#Random Forest Classifier\nfrom sklearn.ensemble import RandomForestClassifier\nclf=RandomForestClassifier()\nparam_grid={'max_depth':[2,4,6,8,10],'max_features':[2,3,4,5,6,7]}\ngrid=GridSearchCV(clf,param_grid,cv=10,scoring='accuracy').fit(X_train,y_train)\ny_pred=grid.predict(X_test)\nacc_rforest=round(accuracy_score(y_pred,y_test)*100,2)\nprint(\"RandomForestAccuracy Score{0} \".format(acc_rforest))","de7f54cb":"#KNN or KNearestNeighbors\nfrom sklearn.neighbors import KNeighborsClassifier\nclf=KNeighborsClassifier()\nparam_grid={'n_neighbors':[2,4,6,8,10],'weights':['uniform','distance']}\ngrid=GridSearchCV(clf,param_grid,cv=10,scoring='accuracy').fit(X_train,y_train)\ny_pred=grid.predict(X_test)\nacc_knn=round(accuracy_score(y_pred,y_test)*100,2)\nprint(\"KnearrestNeighbors Accuracy Score{0} \".format(acc_knn))\n","67994f08":"#Gradient Boost decision Tree\nfrom sklearn.ensemble import GradientBoostingClassifier\nclf=GradientBoostingClassifier()\nparam_grid={'max_depth':[2,4,6,8,10],'max_features':[2,3,4,5,6,7]}\ngrid=GridSearchCV(clf,param_grid,cv=10,scoring='accuracy').fit(X_train,y_train)\ny_pred=grid.predict(X_test)\nacc_gbdtree=round(accuracy_score(y_pred,y_test)*100,2)\nprint(\"GradientboostingClassifier accuracy Score {0}\".format(acc_gbdtree))\n","f99875f1":"model=pd.DataFrame({'model':['GradientboostingClassifier','KnearrestNeighbors','RandomForest','DecisionTreeClassifier','SupportVectormachine','LogisticRegression'] ,'Acc_Score':[acc_gbdtree,acc_knn,acc_rforest,acc_dtree,acc_svc,acc_lreg]})\nmodel.sort_values('Acc_Score',ascending=False)","f9ecf176":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import make_scorer, accuracy_score\nfrom sklearn.model_selection import GridSearchCV\n\n# Choose the type of classifier. \nclf = RandomForestClassifier()\n\n# Choose some parameter combinations to try\nparameters = {'n_estimators': [4, 6, 9,12], \n              'max_features': ['log2', 'sqrt','auto'], \n              'criterion': ['entropy', 'gini'],\n              'max_depth': [2, 3, 5, 10], \n              'min_samples_split': [2, 3, 5],\n              'min_samples_leaf': [1,5,8]\n             }\n\n# Type of scoring used to compare parameter combinations\nacc_scorer = make_scorer(accuracy_score)\n\n# Run the grid search\ngrid_obj = GridSearchCV(clf, parameters, scoring=acc_scorer)\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set the clf to the best combination of parameters\nclf = grid_obj.best_estimator_\n\n# Fit the best algorithm to the data. \nclf.fit(X_train, y_train)\n\n","23bc78f2":"predictions = clf.predict(X_test)\nprint(accuracy_score(y_test, predictions))","cafce1f3":"from sklearn.cross_validation import KFold\n\ndef run_kfold(clf):\n    kf = KFold(891, n_folds=10)\n    outcomes = []\n    fold = 0\n    for train_index, test_index in kf:\n        fold += 1\n        X_train, X_test = X_all.values[train_index], X_all.values[test_index]\n        y_train, y_test = y_all.values[train_index], y_all.values[test_index]\n        clf.fit(X_train, y_train)\n        predictions = clf.predict(X_test)\n        accuracy = accuracy_score(y_test, predictions)\n        outcomes.append(accuracy)\n        print(\"Fold {0} accuracy: {1}\".format(fold, accuracy))     \n    mean_outcome = np.mean(outcomes)\n    print(\"Mean Accuracy: {0}\".format(mean_outcome)) \n\nrun_kfold(clf)\n","472ab960":"ids = data_test['PassengerId']\npredictions = clf.predict(data_test.drop(['PassengerId','Cabin'], axis=1))\n\n\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions })\noutput.to_csv('titanic-predictions.csv', index = False)\noutput.head()","c2d59bf7":"Let's Compare accuracy score of each model ","1ca356b6":"## Predict the Actual Test Data\n\nAnd now for the moment of truth. Make the predictions, export the CSV file, and upload them to Kaggle.","2f14090a":"## Fitting and Tuning an Algorithm\n\nNow it's time to figure out which algorithm is going to deliver the best model. I'm going to test accuracy for below machine learning algorithm and pic best model based on the results.\n\n*  Logistic Regression\n* Support Vector Machines\n* Decision Tree Classifier\n* Random Forest Classifier\n* KNN or k-Nearest Neighbors\n* Gradient Boosting Classifier","1dbc46ac":"People with less than four parents or children aboard are more likely to survive than those with four or more. Again, people traveling alone are less likely to survive than those with 1-3 parents or children.","2f0aec9a":"As we can see Gradientboost and RandomForest  are our top two performer. I will take RandomForest for final predictions ","e991fd97":"## Visualizing Data\n\nVisualizing data is crucial for recognizing underlying patterns to exploit in the model. ","81f579b4":"## Some Final Encoding\n\nThe last part of the preprocessing phase is to normalize labels. The LabelEncoder in Scikit-learn will convert each unique string value into a number, making out data more flexible for various algorithms. \n\nThe result is a table of numbers that looks scary to humans, but beautiful to machines. ","76f9522e":"## Validate with KFold\n\nIs this model actually any good? It helps to verify the effectiveness of the algorithm using KFold. This will split our data into 10 buckets, then run the algorithm using a different bucket as the test set for each iteration. ","e8bbb219":"**Some Observation after looking data:-**\nlooking at data set , close to 77% of data is missing for cabin and  19.86% for Age.\n\nNumerical Features: Age (Continuous), Fare (Continuous), SibSp (Discrete), Parch (Discrete)\n\nCategorical Features: Survived, Sex, Embarked, Pclass\n\nAlphanumeric Features: Ticket, Cabin\n\n\n","184c87b0":"## CSV to DataFrame\n\nCSV files can be loaded into a dataframe by calling `pd.read_csv` . After loading the training and test files, print a `sample` to see what you're working with.","da36faea":"# Machine Learning from Start to Finish with Scikit-Learn\n\nThis notebook covers my attempt to predict Survival of Titanic passengers. \nAny sugestions or coments are welcome.\n\n### Steps Covered\n\n\n1. Importing  a DataFrame\n2. Visualize the Data\n3. Cleanup and Transform the Data\n4. Encode the Data ( Use of Knn in pre-processing)\n5. Split Training and Test Sets\n6. Fit and Predict on Multiple model\n7. Fine Tune Algorithms\n8. Cross Validate with KFold\n9. Upload to Kaggle","9ab10cb0":"## Splitting up the Training Data\n\nNow its time for some Machine Learning. \n\nFirst, separate the features(X) from the labels(y). \n\n**X_all:** All features minus the value we want to predict (Survived).\n\n**y_all:** Only the value we want to predict. \n\nSecond, use Scikit-learn to randomly shuffle this data into four variables. In this case, I'm training 80% of the data, then testing against the other 20%.  \n\nLater, this data will be reorganized into a KFold pattern to validate the effectiveness of a trained algorithm. ","ebead4e0":"In general, it's clear that people with more siblings or spouses aboard were less likely to survive. However, contrary to expectations, people with no siblings or spouses were less to likely to survive than those with one or two. (34.5% vs 53.4% vs. 46.4%)","9f2d3159":"## Transforming Features\n\n1. Aside from 'Sex', the 'Age' feature is second in importance. To avoid overfitting, I'm grouping people into logical human age groups. \n2. Each Cabin starts with a letter. I bet this letter is much more important than the number that follows, let's slice it off. \n3. Fare is another continuous value that should be simplified. I ran `data_train.Fare.describe()` to get the distribution of the feature, then placed them into quartile bins accordingly. \n4. Extract information from the 'Name' feature. Rather than use the full name, I extracted the last name and name prefix (Mr. Mrs. Etc.), then appended them as their own features. \n5. Lastly, drop useless features. (Ticket and Name)","5349a053":"Babies are more likely to survive."}}