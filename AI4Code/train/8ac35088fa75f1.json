{"cell_type":{"515e6530":"code","81fed2ad":"code","648b8c61":"code","38795489":"code","a1dc3315":"code","261afca9":"code","ed9d97da":"code","c4986655":"code","4be24c73":"code","7075ce59":"code","fe7ec3e3":"code","0b3b34ea":"code","9e9c85b8":"code","6ee275fd":"code","27e03d09":"code","3054e4d1":"code","03a809b9":"code","7971859c":"code","02fee07b":"markdown","ebabda49":"markdown","fdb8e342":"markdown","e922e945":"markdown","d0a7f558":"markdown","508e5817":"markdown","05c1c33e":"markdown"},"source":{"515e6530":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","81fed2ad":"import itertools\nimport os \nimport matplotlib.pyplot as plt \nimport numpy as np \nimport tensorflow as tf \nimport tensorflow_hub as hub","648b8c61":"print('TF version: ',tf.__version__)\nprint('Hub version: ',hub.__version__)\nprint('GPU is','available' if tf.config.list_logical_devices('GPU')else 'NOT AVAILABLE' )","38795489":"# model_name = \"mobilenet_v3_small_100_224\" # @param ['efficientnetv2-s', 'efficientnetv2-m', 'efficientnetv2-l', 'efficientnetv2-s-21k', 'efficientnetv2-m-21k', 'efficientnetv2-l-21k', 'efficientnetv2-xl-21k', 'efficientnetv2-b0-21k', 'efficientnetv2-b1-21k', 'efficientnetv2-b2-21k', 'efficientnetv2-b3-21k', 'efficientnetv2-s-21k-ft1k', 'efficientnetv2-m-21k-ft1k', 'efficientnetv2-l-21k-ft1k', 'efficientnetv2-xl-21k-ft1k', 'efficientnetv2-b0-21k-ft1k', 'efficientnetv2-b1-21k-ft1k', 'efficientnetv2-b2-21k-ft1k', 'efficientnetv2-b3-21k-ft1k', 'efficientnetv2-b0', 'efficientnetv2-b1', 'efficientnetv2-b2', 'efficientnetv2-b3', 'efficientnet_b0', 'efficientnet_b1', 'efficientnet_b2', 'efficientnet_b3', 'efficientnet_b4', 'efficientnet_b5', 'efficientnet_b6', 'efficientnet_b7', 'bit_s-r50x1', 'inception_v3', 'inception_resnet_v2', 'resnet_v1_50', 'resnet_v1_101', 'resnet_v1_152', 'resnet_v2_50', 'resnet_v2_101', 'resnet_v2_152', 'nasnet_large', 'nasnet_mobile', 'pnasnet_large', 'mobilenet_v2_100_224', 'mobilenet_v2_130_224', 'mobilenet_v2_140_224', 'mobilenet_v3_small_100_224', 'mobilenet_v3_small_075_224', 'mobilenet_v3_large_100_224', 'mobilenet_v3_large_075_224']\nmodel_name = \"mobilenet_v3_small_100_224\"\nmodel_handle_map = {\n  \"efficientnetv2-s\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/efficientnet_v2_imagenet1k_s\/feature_vector\/2\",\n  \"efficientnetv2-m\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/efficientnet_v2_imagenet1k_m\/feature_vector\/2\",\n  \"efficientnetv2-l\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/efficientnet_v2_imagenet1k_l\/feature_vector\/2\",\n  \"efficientnetv2-s-21k\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/efficientnet_v2_imagenet21k_s\/feature_vector\/2\",\n  \"efficientnetv2-m-21k\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/efficientnet_v2_imagenet21k_m\/feature_vector\/2\",\n  \"efficientnetv2-l-21k\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/efficientnet_v2_imagenet21k_l\/feature_vector\/2\",\n  \"efficientnetv2-xl-21k\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/efficientnet_v2_imagenet21k_xl\/feature_vector\/2\",\n  \"efficientnetv2-b0-21k\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/efficientnet_v2_imagenet21k_b0\/feature_vector\/2\",\n  \"efficientnetv2-b1-21k\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/efficientnet_v2_imagenet21k_b1\/feature_vector\/2\",\n  \"efficientnetv2-b2-21k\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/efficientnet_v2_imagenet21k_b2\/feature_vector\/2\",\n  \"efficientnetv2-b3-21k\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/efficientnet_v2_imagenet21k_b3\/feature_vector\/2\",\n  \"efficientnetv2-s-21k-ft1k\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/efficientnet_v2_imagenet21k_ft1k_s\/feature_vector\/2\",\n  \"efficientnetv2-m-21k-ft1k\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/efficientnet_v2_imagenet21k_ft1k_m\/feature_vector\/2\",\n  \"efficientnetv2-l-21k-ft1k\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/efficientnet_v2_imagenet21k_ft1k_l\/feature_vector\/2\",\n  \"efficientnetv2-xl-21k-ft1k\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/efficientnet_v2_imagenet21k_ft1k_xl\/feature_vector\/2\",\n  \"efficientnetv2-b0-21k-ft1k\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/efficientnet_v2_imagenet21k_ft1k_b0\/feature_vector\/2\",\n  \"efficientnetv2-b1-21k-ft1k\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/efficientnet_v2_imagenet21k_ft1k_b1\/feature_vector\/2\",\n  \"efficientnetv2-b2-21k-ft1k\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/efficientnet_v2_imagenet21k_ft1k_b2\/feature_vector\/2\",\n  \"efficientnetv2-b3-21k-ft1k\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/efficientnet_v2_imagenet21k_ft1k_b3\/feature_vector\/2\",\n  \"efficientnetv2-b0\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/efficientnet_v2_imagenet1k_b0\/feature_vector\/2\",\n  \"efficientnetv2-b1\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/efficientnet_v2_imagenet1k_b1\/feature_vector\/2\",\n  \"efficientnetv2-b2\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/efficientnet_v2_imagenet1k_b2\/feature_vector\/2\",\n  \"efficientnetv2-b3\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/efficientnet_v2_imagenet1k_b3\/feature_vector\/2\",\n  \"efficientnet_b0\": \"https:\/\/hub.tensorflow.google.cn\/tensorflow\/efficientnet\/b0\/feature-vector\/1\",\n  \"efficientnet_b1\": \"https:\/\/hub.tensorflow.google.cn\/tensorflow\/efficientnet\/b1\/feature-vector\/1\",\n  \"efficientnet_b2\": \"https:\/\/hub.tensorflow.google.cn\/tensorflow\/efficientnet\/b2\/feature-vector\/1\",\n  \"efficientnet_b3\": \"https:\/\/hub.tensorflow.google.cn\/tensorflow\/efficientnet\/b3\/feature-vector\/1\",\n  \"efficientnet_b4\": \"https:\/\/hub.tensorflow.google.cn\/tensorflow\/efficientnet\/b4\/feature-vector\/1\",\n  \"efficientnet_b5\": \"https:\/\/hub.tensorflow.google.cn\/tensorflow\/efficientnet\/b5\/feature-vector\/1\",\n  \"efficientnet_b6\": \"https:\/\/hub.tensorflow.google.cn\/tensorflow\/efficientnet\/b6\/feature-vector\/1\",\n  \"efficientnet_b7\": \"https:\/\/hub.tensorflow.google.cn\/tensorflow\/efficientnet\/b7\/feature-vector\/1\",\n  \"bit_s-r50x1\": \"https:\/\/hub.tensorflow.google.cn\/google\/bit\/s-r50x1\/1\",\n  \"inception_v3\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/inception_v3\/feature-vector\/4\",\n  \"inception_resnet_v2\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/inception_resnet_v2\/feature-vector\/4\",\n  \"resnet_v1_50\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/resnet_v1_50\/feature-vector\/4\",\n  \"resnet_v1_101\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/resnet_v1_101\/feature-vector\/4\",\n  \"resnet_v1_152\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/resnet_v1_152\/feature-vector\/4\",\n  \"resnet_v2_50\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/resnet_v2_50\/feature-vector\/4\",\n  \"resnet_v2_101\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/resnet_v2_101\/feature-vector\/4\",\n  \"resnet_v2_152\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/resnet_v2_152\/feature-vector\/4\",\n  \"nasnet_large\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/nasnet_large\/feature_vector\/4\",\n  \"nasnet_mobile\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/nasnet_mobile\/feature_vector\/4\",\n  \"pnasnet_large\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/pnasnet_large\/feature_vector\/4\",\n  \"mobilenet_v2_100_224\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/mobilenet_v2_100_224\/feature_vector\/4\",\n  \"mobilenet_v2_130_224\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/mobilenet_v2_130_224\/feature_vector\/4\",\n  \"mobilenet_v2_140_224\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/mobilenet_v2_140_224\/feature_vector\/4\",\n  \"mobilenet_v3_small_100_224\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/mobilenet_v3_small_100_224\/feature_vector\/5\",\n  \"mobilenet_v3_small_075_224\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/mobilenet_v3_small_075_224\/feature_vector\/5\",\n  \"mobilenet_v3_large_100_224\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/mobilenet_v3_large_100_224\/feature_vector\/5\",\n  \"mobilenet_v3_large_075_224\": \"https:\/\/hub.tensorflow.google.cn\/google\/imagenet\/mobilenet_v3_large_075_224\/feature_vector\/5\",\n}\n\nmodel_image_size_map = {\n  \"efficientnetv2-s\": 384,\n  \"efficientnetv2-m\": 480,\n  \"efficientnetv2-l\": 480,\n  \"efficientnetv2-b0\": 224,\n  \"efficientnetv2-b1\": 240,\n  \"efficientnetv2-b2\": 260,\n  \"efficientnetv2-b3\": 300,\n  \"efficientnetv2-s-21k\": 384,\n  \"efficientnetv2-m-21k\": 480,\n  \"efficientnetv2-l-21k\": 480,\n  \"efficientnetv2-xl-21k\": 512,\n  \"efficientnetv2-b0-21k\": 224,\n  \"efficientnetv2-b1-21k\": 240,\n  \"efficientnetv2-b2-21k\": 260,\n  \"efficientnetv2-b3-21k\": 300,\n  \"efficientnetv2-s-21k-ft1k\": 384,\n  \"efficientnetv2-m-21k-ft1k\": 480,\n  \"efficientnetv2-l-21k-ft1k\": 480,\n  \"efficientnetv2-xl-21k-ft1k\": 512,\n  \"efficientnetv2-b0-21k-ft1k\": 224,\n  \"efficientnetv2-b1-21k-ft1k\": 240,\n  \"efficientnetv2-b2-21k-ft1k\": 260,\n  \"efficientnetv2-b3-21k-ft1k\": 300, \n  \"efficientnet_b0\": 224,\n  \"efficientnet_b1\": 240,\n  \"efficientnet_b2\": 260,\n  \"efficientnet_b3\": 300,\n  \"efficientnet_b4\": 380,\n  \"efficientnet_b5\": 456,\n  \"efficientnet_b6\": 528,\n  \"efficientnet_b7\": 600,\n  \"inception_v3\": 299,\n  \"inception_resnet_v2\": 299,\n  \"nasnet_large\": 331,\n  \"pnasnet_large\": 331,\n}\n\nmodel_handle = model_handle_map.get(model_name)\npixels = model_image_size_map.get(model_name, 224)\n\nprint(f\"Selected model: {model_name} : {model_handle}\")\n\nIMAGE_SIZE = (pixels, pixels)\nprint(f\"Input size {IMAGE_SIZE}\")\n\nBATCH_SIZE = 16","a1dc3315":"data_dir = tf.keras.utils.get_file(\n    'flower_photos',#\u538b\u7f29\u5305\n    'https:\/\/storage.googleapis.com\/download.tensorflow.org\/example_images\/flower_photos.tgz',#\u538b\u7f29\u5305\u7684\u94fe\u63a5\n    untar=True#\u8868\u793a\u538b\u7f29\u5305\n)\nprint(data_dir)","261afca9":"do_fine_tuning=False#\u8fc1\u79fb\u5b66\u4e60\u6cd5\nprint('Build model with',model_handle)\nfrom tensorflow.keras import layers,Sequential,losses,optimizers","ed9d97da":"def build_dataset(subset):\n    return tf.keras.preprocessing.image_dataset_from_directory(\n        data_dir,\n        validation_split=.20,\n        subset=subset,\n        label_mode=\"categorical\",\n        # Seed needs to provided when using validation_split and shuffle = True.\n        # A fixed seed is used so that the validation set is stable across runs.\n        seed=123,\n        image_size=IMAGE_SIZE,\n        batch_size=1\n    )\n\ntrain_ds = build_dataset(\"training\")\nclass_names = tuple(train_ds.class_names)\n#\u8fd4\u56de\u6570\u636e\u96c6\u7684\u5927\u5c0f,\u6570\u91cf\u786e\u5b9a\u8fd4\u56de\u6570\u5b57,\u65e0\u9650\u91cf\uff0c\u8fd4\u56detf.data.INFINITE_CARDINALITY,\u672a\u77e5\uff0c\u8fd4\u56detf.data.UNKNOWN_CARDINALITY\ntrain_size = train_ds.cardinality().numpy()\n#\u5c06\u6570\u636e\u96c6\u7684\u5143\u7d20\u62c6\u5206\u4e3a\u591a\u4e2a\u5143\u7d20\u3002\u5176\u4e2dbatch\u5c06\u6b64\u6570\u636e\u96c6\u7684\u8fde\u7eed\u5143\u7d20\u5408\u5e76\u4e3a\u6279\ntrain_ds = train_ds.unbatch().batch(BATCH_SIZE)\n#\u91cd\u590d\u6b64\u6570\u636e\u96c6\ntrain_ds = train_ds.repeat()\n\n#tf.keras.layers.Rescaling(scale, offset=0.0, **kwargs)\n#To rescale an input in the [0, 255] range to be in the [0, 1] range, you would pass scale=1.\/255.\n#To rescale an input in the [0, 255] range to be in the [-1, 1] range, you would pass scale=1.\/127.5, offset=-1.\nnormalization_layer = tf.keras.layers.Rescaling(1. \/ 255)\npreprocessing_model = tf.keras.Sequential([normalization_layer])\ndo_data_augmentation = False\nif do_data_augmentation:\n    preprocessing_model.add(\n        #random rotations are only applied during training. \n        #If you need to apply random rotations at inference time, set training to True when calling the layer.\n      tf.keras.layers.RandomRotation(40))\n    preprocessing_model.add(\n        #Randomly translate each image during training.\n      tf.keras.layers.RandomTranslation(0, 0.2))\n    preprocessing_model.add(\n      tf.keras.layers.RandomTranslation(0.2, 0))\n  # Like the old tf.keras.preprocessing.image.ImageDataGenerator(),\n  # image sizes are fixed when reading, and then a random zoom is applied.\n  # If all training inputs are larger than image_size, one could also use\n  # RandomCrop with a batch size of 1 and rebatch later.\n    preprocessing_model.add(\n        #\u5728\u8bad\u7ec3\u671f\u95f4\u968f\u673a\u7f29\u653e\u6bcf\u4e2a\u56fe\u50cf\u3002\n      tf.keras.layers.RandomZoom(0.2, 0.2))\n    preprocessing_model.add(\n        #\u5728\u8bad\u7ec3\u671f\u95f4\u968f\u673a\u7ffb\u8f6c\u6bcf\u4e2a\u56fe\u50cf\u3002\n      tf.keras.layers.RandomFlip(mode=\"horizontal\"))\ntrain_ds = train_ds.map(lambda images, labels:\n                        (preprocessing_model(images), labels))\n\nval_ds = build_dataset(\"validation\")\nvalid_size = val_ds.cardinality().numpy()\nval_ds = val_ds.unbatch().batch(BATCH_SIZE)\nval_ds = val_ds.map(lambda images, labels:\n                    (normalization_layer(images), labels))","c4986655":"model=Sequential([\n    layers.InputLayer(input_shape=IMAGE_SIZE+(3,)),#\u8f93\u5165\u5c42\n    hub.KerasLayer(model_handle,trainable=do_fine_tuning),#237\u5c42\n    layers.Dropout(rate=0.2),\n    layers.Dense(len(class_names),\n                kernel_regularizer=tf.keras.regularizers.l2(0.0001))\n])\n# model.build((None,)+IMAGE_SIZE+(3,))\nmodel.build(input_shape=(None,IMAGE_SIZE,3))\nmodel.summary()","4be24c73":"#tf.keras.optimizers.SGD(\n#    learning_rate=0.01, momentum=0.0, nesterov=False, name='SGD', **kwargs\n#)\n#Update rule for parameter w with gradient g when momentum is 0:w = w - learning_rate * g\n#Update rule when momentum is larger than 0:velocity = momentum * velocity - learning_rate * g\uff0cw = w + velocity\n#When nesterov=True, this rule becomes:velocity = momentum * velocity - learning_rate * g\uff0cw = w + momentum * velocity - learning_rate * g\nmodel.compile(\n   optimizer=optimizers.SGD(learning_rate=0.005,momentum=0.9),\n    loss=losses.CategoricalCrossentropy(from_logits=True,label_smoothing=0.1),\n    metrics=['accuracy']\n)","7075ce59":"class_names","fe7ec3e3":"steps_per_epoch=train_size\/\/BATCH_SIZE\nvalidation_steps=valid_size\/\/BATCH_SIZE\nhist=model.fit(\n    train_ds,\n    epochs=5,\n    steps_per_epoch=steps_per_epoch,\n    validation_data=val_ds,\n    validation_steps=validation_steps\n).history","0b3b34ea":"plt.figure()\nplt.ylabel('Loss (training and validation)')\nplt.xlabel('Training Steps')\nplt.ylim([0,2])\nplt.plot(hist['loss'])\nplt.plot(hist['val_loss'])\nplt.legend()\nplt.show()\n\nplt.figure()\nplt.ylabel('Accuracy (training and Validation)')\nplt.xlabel('Training steps')\nplt.ylim([0,1])\nplt.plot(hist['accuracy'])\nplt.plot(hist['val_accuracy'])\nplt.legend()\nplt.show()","9e9c85b8":"x,y=next(iter(val_ds))\nimage=x[0,:,:,:]\ntrue_index=np.argmax(y[0])\nplt.imshow(image)\nplt.axis('off')\nplt.show()\n\n\nprediction_scores=model.predict(np.expand_dims(image,axis=0))\npredicted_index=np.argmax(prediction_scores)\nprint('True label: ',class_names[true_index])\nprint('Predicted label: '+class_names[predicted_index])","6ee275fd":"save_model_path=f\"saved_flowers_model_{model_name}\"","27e03d09":"tf.saved_model.save(model,save_model_path)","3054e4d1":"optimize_lite_model = False \nnum_calibration_examples = 60 \nrepresentative_dataset = None\nif optimize_lite_model and num_calibration_examples:\n  # Use a bounded number of training examples without labels for calibration.\n  # TFLiteConverter expects a list of input tensors, each with batch size 1.\n    representative_dataset = lambda: itertools.islice(\n      ([image[None, ...]] for batch, _ in train_ds for image in batch),\n      num_calibration_examples)\n\nconverter = tf.lite.TFLiteConverter.from_saved_model(save_model_path)\nif optimize_lite_model:\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n    if representative_dataset:  # This is optional, see above.\n        converter.representative_dataset = representative_dataset\nlite_model_content = converter.convert()\n\nwith open(f\"lite_flowers_model_{model_name}.tflite\", \"wb\") as f:\n    f.write(lite_model_content)\nprint(\"Wrote %sTFLite model of %d bytes.\" %\n      (\"optimized \" if optimize_lite_model else \"\", len(lite_model_content)))","03a809b9":"interpreter = tf.lite.Interpreter(model_content=lite_model_content)\n# This little helper wraps the TFLite Interpreter as a numpy-to-numpy function.\ndef lite_model(images):\n    interpreter.allocate_tensors()\n    interpreter.set_tensor(interpreter.get_input_details()[0]['index'], images)\n    interpreter.invoke()\n    return interpreter.get_tensor(interpreter.get_output_details()[0]['index'])","7971859c":"num_eval_examples = 50 \neval_dataset = ((image, label)  # TFLite expects batch size 1.\n                for batch in train_ds\n                for (image, label) in zip(*batch))\ncount = 0\ncount_lite_tf_agree = 0\ncount_lite_correct = 0\nfor image, label in eval_dataset:\n    probs_lite = lite_model(image[None, ...])[0]\n    probs_tf = model(image[None, ...]).numpy()[0]\n    y_lite = np.argmax(probs_lite)\n    y_tf = np.argmax(probs_tf)\n    y_true = np.argmax(label)\n    count +=1\n    if y_lite == y_tf: count_lite_tf_agree += 1\n    if y_lite == y_true: count_lite_correct += 1\n    if count >= num_eval_examples: break\nprint(\"TFLite model agrees with original model on %d of %d examples (%g%%).\" %\n      (count_lite_tf_agree, count, 100.0 * count_lite_tf_agree \/ count))\nprint(\"TFLite model is accurate on %d of %d examples (%g%%).\" %\n      (count_lite_correct, count, 100.0 * count_lite_correct \/ count))","02fee07b":"### \u7535\u8111\u4e0a\u6a21\u62dfLite\u7248\u7684\u7684\u6a21\u578b","ebabda49":"### \u4e0b\u8f7d\u9c9c\u82b1\u6570\u636e\u96c6\uff0c\u83b7\u5f97\u6570\u636e\u96c6\u8def\u5f84","fdb8e342":"### \u6570\u636e\u96c6\u7684\u9884\u5904\u7406\u76f8\u5173\u5de5\u4f5c","e922e945":"### \u8bad\u7ec3\u597d\u7684\u6a21\u578b\u8f6c\u5316\u4e3aLite\u7248","d0a7f558":"### \u642d\u5efa\u76f8\u5e94\u7684\u5c42\uff0c\u6700\u540e\u7684\u5c42\u8f93\u51fa\u7c7b\u522b\u4e3a4","508e5817":"### \u8fdb\u884c\u8bad\u7ec3\u8bad\u7ec3\u96c6\u6570\u636e\u548c\u9a8c\u8bc1\u96c6\u6570\u636e","05c1c33e":"### \u753b\u51fa\u8bad\u7ec3\u96c6\uff0c\u9a8c\u8bc1\u96c6\u7684\u8bad\u7ec3\u8bef\u5dee\u548c\u51c6\u786e\u7387"}}