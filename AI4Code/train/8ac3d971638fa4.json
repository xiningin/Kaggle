{"cell_type":{"d3c67c61":"code","020b5bf3":"code","4f5a00c6":"code","d232fa18":"code","802ac9a3":"code","9f29cf1a":"code","24f0233f":"code","f8a81622":"code","fc282741":"code","17d1bb17":"code","39c77250":"code","ce10bd03":"code","a00ca420":"code","0992f272":"code","a52202dd":"code","f89f6927":"code","ce40d3ca":"code","1b6be957":"code","d4a550e0":"code","3992d2cf":"code","3dcf2b89":"code","aa4c89c5":"code","753ef5b0":"code","36a4eb74":"code","ba3e9492":"code","8c25bb50":"code","ebec85f9":"code","fe3c8c47":"code","8cd99229":"code","15d4f690":"code","0ce9c9d1":"code","6ca3cd22":"code","06184e73":"code","c80e0dca":"code","1552ca35":"code","5760ba5e":"code","56f9f2cc":"code","14b588d9":"code","b7ec8b46":"code","75b2bbc6":"code","f74b7f29":"code","c63de6c4":"markdown","c9601491":"markdown","bc24d718":"markdown","6e06270a":"markdown","abb641f4":"markdown","c2db8c75":"markdown","977bf6d5":"markdown","ca2edbff":"markdown","c1e5cc5d":"markdown","3833ce84":"markdown","37502dc7":"markdown","caac899a":"markdown","873749a8":"markdown","ffa222d0":"markdown","e3f48c6f":"markdown","e7ea3930":"markdown","9a60c2d4":"markdown","da263c87":"markdown"},"source":{"d3c67c61":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nimport scipy.stats\nnp.random.seed(0)\nsns.set_style(\"darkgrid\")\nimport os\nos.listdir(\"..\/input\")\nimport warnings\nwarnings.filterwarnings(\"ignore\")","020b5bf3":"df = pd.read_csv(\"..\/input\/students-performance-in-exams\/StudentsPerformance.csv\")","4f5a00c6":"df.head()","d232fa18":"df.shape","802ac9a3":"df.info()","9f29cf1a":"df.describe()","24f0233f":"# Dividing the dataset into numerical and categorical data\ncat_data = [cname for cname in df.columns if df[cname].dtype == \"object\"]\nnum_data = [cname for cname in df.columns if df[cname].dtype == 'int64']","f8a81622":"# Getting the number of unique entries of each columns in categorical data\ncat_entries = list(map(lambda col: df[col].nunique(), cat_data))\nd = dict(zip(cat_data,cat_entries))\n# printing number of unique entries of columns in ascending order\nsorted(d.items(),key=lambda x : x[1])","fc282741":"df['parental level of education'].value_counts()","17d1bb17":"df['race\/ethnicity'].value_counts()","39c77250":"# for i in cat_num\nsns.countplot(x = 'gender', data=df)","ce10bd03":"sns.countplot(x = 'lunch', data=df)","a00ca420":"sns.countplot(x = 'race\/ethnicity', data=df)","0992f272":"sns.countplot(x = 'test preparation course', data=df,hue='gender')","a52202dd":"sns.countplot(x = 'parental level of education', data=df)\nplt.xticks(rotation = 70)\nplt.show()","f89f6927":"sns.catplot(x='gender',data=df,kind = 'count',hue='test preparation course',row='race\/ethnicity',)","ce40d3ca":"df.corr()","1b6be957":"sns.scatterplot(x='writing score',y='math score',hue='gender', data=df)","d4a550e0":"# Using the normal.cdf package in scipy.stats, finding the probability value\nmath_prob = scipy.stats.norm.cdf((90-df['writing score'].mean())\/df['writing score'].std())\nprint(1-math_prob)","3992d2cf":"sns.pairplot(df)","3dcf2b89":"sns.heatmap(df.corr(), annot=True)\nplt.yticks(rotation = 45)","aa4c89c5":"math_reading = scipy.stats.pearsonr(df['math score'],df['reading score'])\nwriting_reading = scipy.stats.pearsonr(df['writing score'],df['reading score'])\nmath_writing = scipy.stats.pearsonr(df['math score'],df['writing score'])\nprint(\"Test Statistic and p-value for correlation between math and reading score is\", math_reading)\nprint('\\n')\nprint(\"Test Statistic and p-value for correlation between writing and reading score is\", writing_reading)\nprint('\\n')\nprint(\"Test Statistic and p-value for correlation between math and writing score is\", math_reading)","753ef5b0":"cont_table  = pd.crosstab(df['lunch'], df['gender'])\ncont_table","36a4eb74":"scipy.stats.chi2_contingency(cont_table,correction=False)","ba3e9492":"# Probability value of student who scored less than 30 out of 100 in reading\nscipy.stats.norm.cdf((30-df['reading score'].mean())\/df['math score'].std())","8c25bb50":"# Lets check more about outliers \nsns.boxplot(data=df,x = 'reading score')","ebec85f9":"# Probability value of student who scored less than 30 out of 100 in writing\nscipy.stats.norm.cdf((30-df['writing score'].mean())\/df['writing score'].std())","fe3c8c47":"sns.boxplot(data=df,x = 'writing score')","8cd99229":"# Probability value of student who scored less than 30 out of 100 in maths\nscipy.stats.norm.cdf((30-df['math score'].mean())\/df['math score'].std())","15d4f690":"sns.boxplot(data=df,x = 'math score')","0ce9c9d1":"from scipy.stats import norm\n\n# Plot between -4 and 4 with 0.1 steps.\nx_axis = np.arange(-4, 4, 0.1)\n# Mean = 0, SD = 1.\nplt.plot(x_axis, norm.pdf(x_axis, 0, 1))\nplt.show()","6ca3cd22":"ax = sns.distplot(df['writing score'],\n                  bins=20,\n                  kde=True,\n                  color='red',\n                  hist_kws={\"linewidth\": 15,'alpha':1})\nax.set(xlabel='Normal Distribution', ylabel='Frequency')\n## we can assume it is normal","06184e73":"ax = sns.distplot(df['math score'],\n                  bins=20,\n                  kde=True,\n                  color='green',\n                  hist_kws={\"linewidth\": 15,'alpha':1})\nax.set(xlabel='Normal Distribution', ylabel='Frequency')","c80e0dca":"ax = sns.distplot(df['reading score'],\n                  bins=20,\n                  kde=True,\n                  color='blue',\n                  hist_kws={\"linewidth\": 15,'alpha':1})\nax.set(xlabel='Normal Distribution', ylabel='Frequency')","1552ca35":"# Levene's Test - Equality of variance test\n# Ho : Female and male have equal variance\n# H1 : Female and male do not have equal variance\nmath_var = scipy.stats.levene(df[df['gender']=='female']['math score'],\n                  df[df['gender']=='male']['math score'], center = 'mean')\nreading_var = scipy.stats.levene(df[df['gender']=='female']['reading score'],\n                  df[df['gender']=='male']['reading score'], center = 'mean')\nwriting_var = scipy.stats.levene(df[df['gender']=='female']['writing score'],\n                  df[df['gender']=='male']['writing score'], center = 'mean')\nprint(\"Test Statistic and p-value for math  is\", math_var)\nprint('\\n')\nprint(\"Test Statistic and p-value for writing is\", writing_var)\nprint('\\n')\nprint(\"Test Statistic and p-value for reading is\", reading_var)","5760ba5e":"# t test\nmath_score = scipy.stats.ttest_ind(df[df['gender']=='female']['math score'],df[df['gender']=='male']['math score'], equal_var = True)\nwriting_score  = scipy.stats.ttest_ind(df[df['gender']=='female']['writing score'],df[df['gender']=='male']['writing score'], equal_var = True)\nreading_score = scipy.stats.ttest_ind(df[df['gender']=='female']['reading score'],df[df['gender']=='male']['reading score'], equal_var = True)\nprint(\"Test Statistic and p-value for math score is\", math_score)\nprint('\\n')\nprint(\"Test Statistic and p-value for writing score is\", writing_score)\nprint('\\n')\nprint(\"Test Statistic and p-value for reading score is\", reading_score)","56f9f2cc":"math_mean = round(df['math score'].mean(),4)\nmath_sd = round(df['math score'].std(),4)\nprint(\" Mean is\",math_mean,\"and\",'\\n',\"Standard Deviation is\",math_sd)","14b588d9":"## because it is a two-tailed test we multiply by 2\n## Z- test\n2*round(scipy.stats.norm.cdf((10.7 - 12)\/(5.5\/np.sqrt(36))), 3)","b7ec8b46":"# Levene's Test - Equality of variance test\n# Ho : Female and male have equal variance\n# H1 : Female and male do not have equal variance\nmath_var = scipy.stats.levene(df[df['test preparation course']=='none']['math score'],\n                  df[df['test preparation course']=='completed']['math score'], center = 'mean')\nreading_var = scipy.stats.levene(df[df['test preparation course']=='none']['reading score'],\n                  df[df['test preparation course']=='completed']['reading score'], center = 'mean')\nwriting_var = scipy.stats.levene(df[df['test preparation course']=='none']['writing score'],\n                  df[df['test preparation course']=='completed']['writing score'], center = 'mean')\nprint(\"Test Statistic and p-value for math  is\", math_var)\nprint(\"Test Statistic and p-value for writing is\", writing_var)\nprint(\"Test Statistic and p-value for reading is\", reading_var)","75b2bbc6":"## z-test\nmath_z_test = 2*round(scipy.stats.norm.cdf((10.7 - 12)\/(5.5\/np.sqrt(36))), 3)","f74b7f29":"# One Way ANOVA\n# The one-way ANOVA tests the null hypothesis that two or more groups have the same population mean. \n# The test is applied to samples from two or more groups, possibly with differing sizes.\nf, p = scipy.stats.f_oneway(df['math score'], df['reading score'], df['writing score'])\nprint(\"F-Statistic = \",f, \"and P-value = \",p)\n# f_oneway accepts multidimensional input arrays. \n# When the inputs are multidimensional and axis is not given, the test is performed along the first\n# axis of the input arrays. For the following data, the test is performed three times, once for each column.","c63de6c4":"From the distribution plots above all three have almost normal distribution.","c9601491":"**The table shows expected value at 5% level of significance.**","bc24d718":"# Hypothesis Testing\n*Hypothesis testing is a statistical method that is used in making statistical decisions using experimental data. Hypothesis Testing is basically an assumption that we make about the population parameter.Hypothesis testing is an essential procedure in statistics. A hypothesis test evaluates two mutually exclusive statements about a population to determine which statement is best supported by the sample data. When we say that a finding is statistically significant, it\u2019s thanks to a hypothesis test.*\n\n**Which are important parameter of hypothesis testing ?**\n\n**Null hypothesis** :- In inferential statistics, the null hypothesis is a general statement or default position that there is no relationship between two measured phenomena, or no association among groups\n**Alternative hypothesis** :-\nThe alternative hypothesis is the hypothesis used in hypothesis testing that is contrary to the null hypothesis. It is usually taken to be that the observations are the result of a real effect (with some amount of chance variation superposed)\n\n**Normal Distribution -**\nA variable is said to be normally distributed or have a normal distribution if its distribution has the shape of a normal curve \u2014 a special bell-shaped curve. \u2026 The graph of a normal distribution is called the normal curve, which has all of the following properties: 1. The mean, median, and mode are equal.\n\n\n**Level of significance:** Refers to the degree of significance in which we accept or reject the null-hypothesis. 100% accuracy is not possible for accepting or rejecting a hypothesis, so we therefore select a level of significance that is usually 5%.\nThis is normally denoted with alpha(maths symbol ) and generally it is 0.05 or 5% , which means your output should be 95% confident to give similar kind of result in each sample.\n\n**Type I error:** When we reject the null hypothesis, although that hypothesis was true. Type I error is denoted by alpha. In hypothesis testing, the normal curve that shows the critical region is called the alpha region\n\n**Type II errors:** When we accept the null hypothesis but it is false. Type II errors are denoted by beta. In Hypothesis testing, the normal curve that shows the acceptance region is called the beta region.\n\n**One tailed test :-** A test of a statistical hypothesis , where the region of rejection is on only one side of the sampling distribution , is called a one-tailed test.\nExample :- a college has \u2265 4000 student or data science \u2264 80% org adopted.\n\n**Two-tailed test :-** A two-tailed test is a statistical test in which the critical area of a distribution is two-sided and tests whether a sample is greater than or less than a certain range of values. If the sample being tested falls into either of the critical areas, the alternative hypothesis is accepted instead of the null hypothesis.\n\n\n![image.png](attachment:899634dc-f14b-4718-911e-08d3814bf0f4.png)\n\n\n\n\n\n\n\n\n\n\n\n\n\n**P-value :-** The P value, or calculated probability, is the probability of finding the observed, or more extreme, results when the null hypothesis (H 0) of a study question is true \u2014 the definition of \u2018extreme\u2019 depends on how the hypothesis is being tested.\nIf your P value is less than the chosen significance level then you reject the null hypothesis i.e. accept that your sample gives reasonable evidence to support the alternative hypothesis. It does NOT imply a \u201cmeaningful\u201d or \u201cimportant\u201d difference; that is for you to decide when considering the real-world relevance of your result.\nExample : you have a coin and you don\u2019t know whether that is fair or tricky so let\u2019s decide null and alternate hypothesis\n\nExample : you have a coin and you don\u2019t know whether that is fair or tricky so let\u2019s decide null and alternate hypothesis\n\n![image.png](attachment:41dcbddd-3094-4718-b6bf-8949f6963c08.png)\n\nH0 : a coin is a fair coin.\n\nH1 : a coin is a tricky coin. and alpha = 5% or 0.05\n\nNow let\u2019s toss the coin and calculate p- value ( probability value).\n\nToss a coin 1st time and result is tail- P-value = 50% (as head and tail have equal probability)\n\nToss a coin 2nd time and result is tail, now p-value = 50\/2 = 25%\nand similarly we Toss 6 consecutive time and got result as P-value = 1.5% but we set our significance level as 95% means 5% error rate we allow and here we see we are beyond that level i.e. our null- hypothesis does not hold good so we need to reject and propose that this coin is a tricky coin which is actually.\n\n**Degree of freedom :-** Now imagine you\u2019re not into hats. You\u2019re into data analysis.You have a data set with 10 values. If you\u2019re not estimating anything, each value can take on any number, right? Each value is completely free to vary.But suppose you want to test the population mean with a sample of 10 values, using a 1-sample t test. You now have a constraint \u2014 the estimation of the mean. What is that constraint, exactly? By definition of the mean, the following relationship must hold: The sum of all values in the data must equal n x mean, where n is the number of values in the data set.\n\nSo if a data set has 10 values, the sum of the 10 values must equal the mean x 10. If the mean of the 10 values is 3.5 (you could pick any number), this constraint requires that the sum of the 10 values must equal 10 x 3.5 = 35.\n\nWith that constraint, the first value in the data set is free to vary. Whatever value it is, it\u2019s still possible for the sum of all 10 numbers to have a value of 35. The second value is also free to vary, because whatever value you choose, it still allows for the possibility that the sum of all the values is 35.\n\n\n![image.png](attachment:6e7b86a5-8feb-42d5-b53f-3349441cc4e2.png)\n\n**Now Let\u2019s see some of widely used hypothesis testing type :-**\n\n**T Test ( Student T test)**\n\n**Z Test**\n\n**ANOVA Test**\n\n**Chi-Square Test**","6e06270a":"### Result\n**At 5% level of significance, p value for maths and writing score is greater than 0.05 and for writing it is less than 0.05.**\n\n**Therefore, for maths and writing, studens who with preparation course and students with no preparation course both have equal variance.**\n**But in reading it is not same.**\n","abb641f4":"### Pearson Correlation test(for numerical variable)\n#### Ho = have correlation\n#### H1 = do not have correlation","c2db8c75":"### Result\n**At 5% level of significance**\n\n**From above test,**\n\n**since all the three p-values are less than 0.05.** \n\n**Conclusion:** We will reject the null hypothesis and accept the alternate hypothesis.\n\n**Inference:** The scores have a correlation between them.","977bf6d5":"**At 5% level of significance**\n\n**From above test,**\n\n**since all the three p-values are less than 0.05.** \n\n**Conclusion: We will reject the null hypothesis and accept the alternate hypothesis.**\n\n**Inference: The scores of male and female are not equal.**","ca2edbff":"## From the plot above we can notice:\n1) the outliers, as there are many data points which lie far away from maximum points.\n\n2) All of the pairplots seems to have a linear relationship with the other variable. To clarify that we'll plot the correlation map.\n","c1e5cc5d":"## Visualizing the data","3833ce84":"## Descriptive Statistics","37502dc7":"Standard Normal Table\nYou can use the z-table to find a set of \u201cless-than\u201d probabilities for a wide range of z-values. To use the z-table to find probabilities::\n\nGo to the row that represents the first digit and the first digit after the decimal point of your z-value. (e.g 0.9 in 0.93)\nGo to the column that represents the second digit after the decimal point of your z-value.   (e.g 3 in 0.93, this will be 0.03 in the column)\nIntersect the row and column from Steps 1 and 2.\nFor example, suppose you want to find the probability of z-score less than 1.44 denoted as p(Z < 1.44). Using the second Z-table below, find the row for 1.4 and the column for 0.04. Intersect that row and column to find the probability: 0.92507. Therefore p(Z < 1.44) = 0.92507.\n\nThe area under any normal curve (including the standardized normal curve) is 1, that means that, p(Z < 1.44) + p(Z > 1.44) =1. Therefore, the probability of z-score greater than 1.44 i.e. p(Z > 1.44) = 1 \u2013 p(Z < 1.44) which equals 1 \u2013 0.92507 which equals 0.07493.\n\nSuppose you want to look for p(Z < \u20131.44). You find the row for \u20131.14 and the column for 0.04. Intersect the row and column and you find 0.07493. That means p(Z < \u20131.44) = 0.07493. This happens to be the same as the value of p(Z > +1.44). This is because the normal distribution is symmetric. So the tail of the curve below \u20131.44 representing p(Z < \u20131.44) looks exactly like the tail above 1.44 representing p(Z > +1.44).\n\nYou can also do a reverse lookup, assuming you are told that the age of grade 1 kindergarten pupils in Kampala city is normally distributed with a mean of 6 years. If only 0.71% (same as 0.0071) of the pupils are 8 years and above, what is the z-score?\n\nWe can look within the negative box for the z-score closest to 0.0071. In this case -2.45, but because we are looking at the right-tailed test, we get rid of the negative sign i.e.z-value  = 2.45 (Note: if it was a left-tailed test e.g. 0.71% of them were less than 3 years old, we will use the value as-is) OR\n\nWe can look within the positive table, for the value that corresponds to 1-0.0071 = 0.99286 (we do this because the standard normal table always gives you values to the left, so to get values to the right, you will have to remove from 1). The value that corresponds to this will be 2.45.\n\nTo make things easier, you can also use a \"z-score from p-value\" or a \"p-value from z-score\" calculator online.\n\n\n","caac899a":"**Lets perform t test to check whether there is a significant difference between scores regards to gender.**\n\nHo : The score of males is equal to score of females.\n\nH1 : The score of males is not equal to score of females.","873749a8":"### Result\n**From the p-value we can state that all three subjects have different population mean.**","ffa222d0":"From above description of numerical data, the mean and median(50%) are almost same for all three columns, hence we may say that our data is normally distributed rest we may confirm after plotting the columns.","e3f48c6f":"### Correlation Test\n#### Chi square test(For categorical variable)\n##### The formula for Chi-square is given as follows.The summation of the observed value, i.e the counts in each group minus the expected value,all squared, divided by the expected value. Expected values are based on the given totals.","e7ea3930":"**Conclusion :** Since, at 5% level of significance, P(calculated) > P(0.05).\n\nTherefore we will accept the null hypothesis\n\n**Inference :** All the scores have equal variance with respect to gender.","9a60c2d4":"## Importing the data","da263c87":"## Loading required libraries"}}