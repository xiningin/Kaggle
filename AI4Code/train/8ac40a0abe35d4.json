{"cell_type":{"97f54cec":"code","38d5e7fd":"code","44ad73c8":"code","d97779ee":"code","1c757dea":"code","d3ab80b6":"code","bd4d4252":"code","bdc24dd3":"code","edae94f8":"code","4dfcec7f":"code","268ce09d":"code","548abcf8":"code","58bf9264":"code","db6b9215":"code","89524f51":"code","63510017":"code","ce52138a":"code","49bb9720":"code","b8249856":"code","2be1af91":"code","db579dd7":"code","82fbbe03":"code","db54a420":"code","4c61914e":"code","a7efe1bd":"code","e289e9bb":"code","f59a16e5":"code","70b30d40":"code","9b597bc1":"code","53eb2a12":"code","d319d621":"code","be909bce":"code","237fd913":"code","d07d4134":"code","7b31752c":"code","8d969941":"code","82f66096":"code","2963d030":"code","8b7fe176":"code","8f266160":"code","a2062cf2":"code","1d4ed0a5":"code","6a034ff7":"code","ec518051":"code","04d03076":"code","1ddad6d1":"code","a6e5d6f8":"code","628fb762":"code","f706cf78":"code","31c764ef":"code","9b29187d":"code","40de75c4":"code","31ce9061":"code","3ceaffa7":"code","8a2626de":"code","2ed63674":"code","23683820":"code","315e1217":"code","bebc5be7":"code","4644cb1d":"code","240f2335":"code","5320de02":"code","42092e21":"code","616b5f9b":"code","ef65b31a":"code","ea760749":"code","7fc6e1a9":"code","8e52e728":"code","70c021a8":"code","fcf7e5a2":"code","91718a48":"code","20a1f65d":"code","462086cb":"code","c6bdab25":"code","7d67fa63":"code","39a0b20e":"code","cd9c01aa":"code","e92d50b9":"code","19c923ca":"code","7f949c45":"markdown","0ad9955b":"markdown","105e9456":"markdown","bf88deb2":"markdown","b2ab252d":"markdown","587e06c4":"markdown","64b493ae":"markdown","347d36e9":"markdown","49b88711":"markdown","ce76f839":"markdown","88a9a3c9":"markdown","57ef67c3":"markdown","1ccb3d9f":"markdown","fbafa4b2":"markdown","6bcf865f":"markdown"},"source":{"97f54cec":"#importing libraries\nimport pandas as pd\nimport numpy as np\nfrom warnings import simplefilter\nsimplefilter(\"ignore\")\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport chardet #evaluate encoding of csv\n! pip install keybert\nfrom keybert import KeyBERT\nimport re\nimport string\nimport nltk\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords        \nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.stem import PorterStemmer\nnltk.download(\"vader_lexicon\") # load the Lexicon that quantifies polar sentiment (positive\/negative)\n\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\nfrom sklearn.pipeline import Pipeline, FeatureUnion\n# from sklearn.compose import ColumnTransformer\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split","38d5e7fd":"# look at the first ten thousand bytes to guess the character encoding\nwith open(\"\/kaggle\/input\/nlp-getting-started\/train.csv\", 'rb') as rawdata:\n    result = chardet.detect(rawdata.read(100000))\n\n# check what the character encoding might be\nprint(result)","44ad73c8":"df_train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\nX_test = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")","d97779ee":"df_train.head()","1c757dea":"df_train.describe(include=\"all\").T","d3ab80b6":"df_train.info()","bd4d4252":"#visualize the missing values\nax, fig = plt.subplots(figsize=(10,10))\n\nax = sns.heatmap(df_train.isna(),yticklabels=False,cbar=False,cmap='BuGn_r', alpha = 0.9)\n\nplt.xticks(rotation = 45, ha=\"right\")\n\nplt.tight_layout()\n\n#note that low number of missings won't show in the chart","bdc24dd3":"df_train[\"id\"]\n#not apporting any info we can drop it","edae94f8":"df_train[\"keyword\"].describe()","4dfcec7f":"#has some nan values\ndf_train[\"keyword\"].isna().value_counts(normalize=True)\n# 0.8% (61) are missing - we will proceed imputing these values using KeyBERT","268ce09d":"df_train[~df_train[\"keyword\"].isna()][\"keyword\"].unique().tolist()[:30] #remove this slicing if you want to explore all the list","548abcf8":"df_train[\"text\"].values.tolist()[:30] #remove this slicing if you want to explore all the list","58bf9264":"df_train[\"location\"]\n#lots of values missing","db6b9215":"df_train[\"location\"].isna().value_counts(normalize=True)*100\n#33%  missing locations","89524f51":"df_train[~df_train[\"location\"].isna()][\"location\"].values.tolist()[:30] #remove this slicing if you want to explore all the list","63510017":"df_train[\"target\"].describe()","ce52138a":"df_train[\"target\"].value_counts(normalize=True)*100\n#42% are disaster","49bb9720":"#splitting data\ny = df_train[\"target\"]\n# X.drop(\"target\", axis=1, inplace=True)\nX_train, X_valid, y_train, y_valid = train_test_split(df_train.drop(\"target\", axis=1), y, test_size=0.25, random_state=42)","b8249856":"def keyword_extractor(text):\n    \"\"\"\n    This function extracts keywords from text using KeyBERT\n    \n    It ueses a list keywords candidates where to chose from.\n    Maximal Marginal Relevance (MMR) set to True\n    Diversity is set to 0.2\n    Top n keywords\/keyphrases is set to 6\n    \n    Args:\n        text (str): text to porcess\n\n    Returns:\n        list: list of keywords\n    \"\"\"\n    \n\n    keywords = kw_model.extract_keywords(\n        text, keyphrase_ngram_range=(1,1),\n        stop_words=\"english\",\n        candidates=kwd_list,\n        use_mmr=True,\n        diversity=0.2,\n        top_n=1\n        )\n    \n    return \"\".join([str(i[0]) for i in keywords])","2be1af91":"def location_transformer(df):\n    \"\"\"\n    Creating new feature \"has_location\"\n    \"\"\"\n    \n    df[\"has_location\"] = np.where(df[\"location\"].isna(), 0,1)\n    ","db579dd7":"    #to avoid leaking we'll use X_train to make the unique keywords list\n    global kwd_list\n    kwd_list = X_train[\"keyword\"].unique().tolist() \n    kwd_list.remove(np.nan) #removing nan value from the list\n    \n    #removing %20 from the list and substituting with a space\n    pattern=\"%20\"\n    regex = re.compile(pattern)\n    kwd_list = [regex.sub(\" \", i) if \"%20\" in i else i for i in kwd_list]","82fbbe03":"def kwd_transformer(df):\n    \"\"\"\n    1. Removing %20 from keywords\n    2. Imputing missing keywords with keyBERT\n    3. Changing wild fire to wildfires\n    \"\"\"\n\n    \n    #removing space encoding\n    df[\"keyword\"] = df[\"keyword\"].str.replace(\"%20\", \" \")\n    print(\"removed space encoding\")\n    \n    #imputing missing keywords\n    \n    global kw_model\n    kw_model = KeyBERT()\n    \n    df[\"keyword\"] = df.apply(lambda x: keyword_extractor(x[\"text\"]) if x[\"keyword\"] is np.nan else x[\"keyword\"], axis=1)\n    print(\"Imputed missing Keywords\")\n    \n    #making wildfires an unique word\n    df[\"keyword\"] = df[\"keyword\"].apply(lambda x: \"wildfires\" if x==\"wild fires\" else x)\n    print(\"made 'wild fires'=='wildfires' \")\n    \n","db54a420":"def polarity(row):\n    \"\"\"\n    Creating Polarity score feature\n    \"\"\"\n    \n    pol = analyzer.polarity_scores(row)\n    compound = pol[\"compound\"]\n    \n    return pd.Series([compound])","4c61914e":"def text_transformer(df):\n    \"\"\"\n    1. Removing links form text and creating a feature \"has_link\"\n    2. Creating a text length feature and binning it in 2 chunks - \n        this is because short text tweets are more prone to have target 1\n    3. Creating a \"has_hastag\" feature. We will then remove \"#\" from text and merge the ashtags\n        in a unique feature with keywords and stem it\n    4. Removing all the tags from the text and creating a feature \"has_tag\"\n    5. Removing all tabs characters from the thext\n    6. Creating a compound sentiment feature\n    \"\"\"\n    \n    # 1------LINKS\n    #creating new feature has_link\n    #creating the regex link pattern\n    link_pattern = r\"https?:\\\/\\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&\\\/\\\/=]*)\"\n    link = re.compile(link_pattern)\n    \n    #creating feature\n    df[\"has_link\"] = df[\"text\"].str.contains(link, regex=True).astype(int)\n    \n    #removing links from the text\n    df[\"text\"] = df[\"text\"].str.replace(link,\"\", regex=True)\n    \n    print(\"links done\")\n    \n    # 2--------LENGHT\n    #creating new feature text_len\n    df[\"text_len\"] = df[\"text\"].apply(lambda x: len(x))\n    \n    #creating 5 bins for the lenght\n    df[\"text_len_bins\"] = pd.cut(df['text_len'].astype(int), 2) #140chr \/ 2\n    #encoding the bins\n    label = LabelEncoder()\n    df[\"text_len_bins\"] = label.fit_transform(df[\"text_len_bins\"])\n    \n    print(\"len done\")\n    \n    # 3-------ASHTAGS\n    #creating ashtag has ashtag feature and ashtags feature\n    #ashtags pattern\n    ashtag_pattern = r\"#(\\w+)\"\n    ashtag = re.compile(ashtag_pattern)\n    \n    #create feature if it has hashtags\n    df[\"has_ashtags\"] = df[\"text\"].str.contains(ashtag, regex=True).astype(int)\n    \n    #create ashtags feature\n    df[\"ashtags\"] = df[\"text\"].apply(lambda x: re.findall(ashtag, x.lower()))\n    \n    #merging keywords and ashtags\n    df[\"keyword+\"] = df.apply(lambda x: [x[\"keyword\"]]+x[\"ashtags\"] if x[\"keyword\"] not in x[\"ashtags\"] else x[\"ashtags\"],axis=1) #removing duplicates and merging columns\n    \n    df[\"keyword+\"] = df[\"keyword+\"].apply(lambda x: \" \".join(x))\n    \n    #stemming keywords\n    global stemmer\n    stemmer = PorterStemmer()\n    df[\"keyword+\"] = df[\"keyword\"].apply(lambda x : stemmer.stem(x))\n    \n    #replace ashtag with word without #\n    df[\"text\"] = df[\"text\"].str.replace(\"#\",\"\", regex=False)\n    \n    print(\"ashtags done\")\n    \n    # 4-------TAGS\n    #creating has_tag feature\n    \n    #tag pattern\n    tag_pattern = r\"@(\\w+)\"\n    tag = re.compile(tag_pattern)\n    \n    #add has tag\n    df[\"has_tag\"] = df[\"text\"].str.contains(tag, regex=True).astype(int)\n    \n    #remove all @person from the text feature\n    df[\"text\"] = df[\"text\"].str.replace(tag,\"\", regex=True)\n    \n    print(\"tags done\")\n    \n    # 5------\\n\n    #replacing \\n\n    df[\"text\"] = df[\"text\"].str.replace(\"\\n\",\" \", regex=False)\n    \n    # 6-------SENTIMENT SCORE\n    #add sentiment compound feature\n    global analyzer\n    analyzer = SentimentIntensityAnalyzer()\n    \n    df[\"compound\"] = df.apply(lambda row: polarity(row[\"text\"]), axis=1)\n    \n    print(\"sentiment done\")\n    ","a7efe1bd":"kwd_transformer(X_train)\ntext_transformer(X_train)\nlocation_transformer(X_train)","e289e9bb":"df=pd.concat([X_train,y_train], axis=1)\ndf.head()","f59a16e5":"sns.histplot(data=df, x=\"text_len\", hue=\"target\", bins=14) \n#we can see a distinction between text shorter than 60 chars being less frequent","70b30d40":"sns.boxplot(data=df, x=\"target\", y=\"text_len\") # 1 are slightly longer","9b597bc1":"sns.barplot(data=df, x=\"text_len_bins\",y=\"text_len\",hue=\"target\", orient=\"v\") \n#distaster tweets are longer for bin one we created","53eb2a12":"sns.boxplot(data=df, x=\"target\", y=\"compound\") #disaster tweets are clearly more proce to ha a negative compound sentiment","d319d621":"#visualizing keywords with most disaster labels\norder = pd.crosstab(df[\"keyword+\"], df.target).sort_values(1, ascending=False).index","be909bce":"fig, ax = plt.subplots(figsize=(10,35)\n                       )\nax = sns.countplot(data=df, y=\"keyword+\", hue=\"target\", orient=\"v\", order = order)\n\nax.xaxis.tick_top()\nax.xaxis.set_label_position('top')\n\nplt.legend(loc='upper right')\nplt.tight_layout()","237fd913":"sns.countplot(data=df, x=\"has_ashtags\", hue=\"target\")\n# not having an hashtag could be a discriminant for a disaster tweet","d07d4134":"sns.countplot(data=df, x=\"has_tag\", hue=\"target\")\n# a tag could be a discriminant eve tough we must take into account that the disaster tweets are less than non disaster","7b31752c":"sns.countplot(data=df, x=\"has_location\", hue=\"target\")\n# same as before","8d969941":"sns.countplot(data=df, x=\"has_link\", hue=\"target\")\n# here we can see how having a link could be indicative of a disaster","82f66096":"kwd_transformer(X_valid)\ntext_transformer(X_valid)\nlocation_transformer(X_valid)","2963d030":"kwd_transformer(X_test)\ntext_transformer(X_test)\nlocation_transformer(X_test)","8b7fe176":"print(X_train.shape,y_train.shape, X_valid.shape, y_valid.shape, X_test.shape)","8f266160":"encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n\n#TRAIN\ntrain_kwd_dummies = pd.DataFrame(encoder.fit_transform(X_train[\"keyword+\"].values.reshape(-1,1)))\nX_train.reset_index(inplace=True) #needs index to be resetted with concat\n\n#concat\nX_train = pd.concat([X_train, train_kwd_dummies], axis=1)\n\nX_train.index = X_train[\"index\"]\nX_train.drop(\"index\", axis=1, inplace=True)\n","a2062cf2":"valid_kwd_dummies = pd.DataFrame(encoder.transform(X_valid[\"keyword+\"].values.reshape(-1,1)))\n\n#VALID\nX_valid.reset_index(inplace=True) #needs index to be resetted\n\n#concat\nX_valid = pd.concat([X_valid, valid_kwd_dummies], axis=1)\n\nX_valid.index = X_valid[\"index\"]\nX_valid.drop(\"index\", axis=1, inplace=True)","1d4ed0a5":"test_kwd_dummies = pd.DataFrame(encoder.transform(X_test[\"keyword+\"].values.reshape(-1,1)))\n\n#TEST\nX_test.reset_index(inplace=True) #needs index to be resetted\n\n#concat\nX_test = pd.concat([X_test, test_kwd_dummies], axis=1)\n\nX_test.index = X_test[\"index\"]\nX_test.drop(\"index\", axis=1, inplace=True)","6a034ff7":"print(X_train.shape,y_train.shape, X_valid.shape, y_valid.shape, X_test.shape)","ec518051":"def text_process(text):\n    \"\"\"\n    Takes in a string of text, then performs the following:\n    1. Remove all punctuation\n    2. Remove all stopwords\n    3. Stem the text\n    4. Returns a list of the cleaned text\n    \"\"\"\n    # Check characters to see if they are in punctuation\n    nopunc = [char for char in text if char not in string.punctuation]\n\n    # Join the characters again to form the string.\n    nopunc = ''.join(nopunc)\n    \n    #Stem text\n    stemmer = PorterStemmer()\n    \n    # Now just remove any stopwords\n    return [stemmer.stem(word) for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n","04d03076":"#TRAINING\n#vectorizing text columns\nvectorizer = TfidfVectorizer(analyzer=text_process, min_df=5, max_df=0.5, ngram_range=(1,2))\n\ntrain_text_tfidf = vectorizer.fit_transform(X_train['text'])\n\n#making a dataframe to concatenate\ntrain_text_tfidf = pd.DataFrame(train_text_tfidf.toarray())\n\nX_train.reset_index(inplace=True) #needs index to be resetted\n\n#concat\nX_train = pd.concat([X_train, train_text_tfidf], axis=1)\n\nX_train.index = X_train[\"index\"]\nX_train.drop(\"index\", axis=1, inplace=True)","1ddad6d1":"#VALIDATION\nvalid_text_tfidf = vectorizer.transform(X_valid['text']) #avoid data leakage\n\n#making a dataframe to concatenate\nvalid_text_tfidf = pd.DataFrame(valid_text_tfidf.toarray())\n\nX_valid.reset_index(inplace=True) #needs index to be resetted\n\n#concat\nX_valid = pd.concat([X_valid, valid_text_tfidf], axis=1)\n\nX_valid.index = X_valid[\"index\"]\nX_valid.drop(\"index\", axis=1, inplace=True)","a6e5d6f8":"#TEST\ntest_text_tfidf = vectorizer.transform(X_test['text']) #avoid data leakage\n\n#making a dataframe to concatenate\ntest_text_tfidf = pd.DataFrame(test_text_tfidf.toarray())\n\nX_test.reset_index(inplace=True) #needs index to be resetted\n\n#concat\nX_test = pd.concat([X_test, test_text_tfidf], axis=1)\n\nX_test.index = X_test[\"index\"]\nX_test.drop(\"index\", axis=1, inplace=True)","628fb762":"#scaling the compound feature\nfrom sklearn.preprocessing import MinMaxScaler\nscaler=MinMaxScaler()","f706cf78":"scaler.fit(X_train[\"compound\"].values.reshape(-1,1))\n# scaler.data_max_","31c764ef":"X_train[\"compound\"] = scaler.transform(X_train[\"compound\"].values.reshape(-1,1))\nX_valid[\"compound\"] = scaler.transform(X_valid[\"compound\"].values.reshape(-1,1))\nX_test[\"compound\"] = scaler.transform(X_test[\"compound\"].values.reshape(-1,1))","9b29187d":"def drop_columns(df):\n   return df.drop([\"keyword\", \"id\", \"ashtags\", \"location\", \"text_len\", \"keyword+\", \"text\"], axis=1, inplace=True)","40de75c4":"#dropping non necessary features\ndrop_columns(X_train)\ndrop_columns(X_valid)\ndrop_columns(X_test)","31ce9061":"print(X_train.shape,y_train.shape, X_valid.shape, y_valid.shape, X_test.shape)","3ceaffa7":"#pca didn't give any improvement\n# pca = PCA(n_components=6, random_state=42)","8a2626de":"\n# X_train_pca = pca.fit_transform(X_train)\n# X_valid_pca=pca.transform(X_valid)","2ed63674":"# pca.explained_variance_\n#only first 6 explain variance","23683820":"# print(X_train_pca.shape,y_train.shape, X_valid_pca.shape, y_valid.shape)","315e1217":"from sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression(C=1,random_state=42).fit(X_train, y_train)","bebc5be7":"prediction = clf.predict(X_valid)","4644cb1d":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n\nprint(confusion_matrix(y_valid, prediction))\nprint(accuracy_score(y_valid, prediction))\nprint(classification_report(y_valid, prediction))\nprint(f1_score(y_valid, prediction))\n","240f2335":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=100, random_state=42)\n","5320de02":"rfc.fit(X_train,y_train)","42092e21":"prediction = rfc.predict(X_valid)","616b5f9b":"print(confusion_matrix(y_valid, prediction))\nprint(accuracy_score(y_valid, prediction))\nprint(classification_report(y_valid, prediction))\nprint(f1_score(y_valid, prediction))","ef65b31a":"from sklearn.svm import SVC\nsvm=SVC()","ea760749":"svm.fit(X_train,y_train)","7fc6e1a9":"prediction = svm.predict(X_valid)","8e52e728":"print(confusion_matrix(y_valid, prediction))\nprint(accuracy_score(y_valid, prediction))\nprint(classification_report(y_valid, prediction))\nprint(f1_score(y_valid, prediction))","70c021a8":"from sklearn.naive_bayes import MultinomialNB\nnb = MultinomialNB()","fcf7e5a2":"nb.fit(X_train,y_train)","91718a48":"prediction = nb.predict(X_valid)\n\nprint(confusion_matrix(y_valid, prediction))\nprint(accuracy_score(y_valid, prediction))\nprint(classification_report(y_valid, prediction))\nprint(f1_score(y_valid, prediction))","20a1f65d":"from sklearn.model_selection import GridSearchCV, cross_validate, KFold\nfrom sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score","462086cb":"scoring = {'accuracy' : make_scorer(accuracy_score), \n           'precision' : make_scorer(precision_score),\n           'recall' : make_scorer(recall_score), \n           'f1_score' : make_scorer(f1_score)}\n\nkfold = KFold(n_splits=10)\n\nresults = cross_validate(estimator=clf,\n                            X=X_train,\n                            y=y_train,\n                            cv=kfold,\n                            scoring=scoring)\n\n\nprint(\"Accuracy: {:.2f} %\".format(results[\"test_accuracy\"].mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(results[\"test_accuracy\"].std()*100))\nprint(\"F1_score: {:.2f} %\".format(results[\"test_f1_score\"].mean()*100))\nprint(\"Precision: {:.2f} %\".format(results[\"test_precision\"].mean()*100))\nprint(\"Recall: {:.2f} %\".format(results[\"test_recall\"].mean()*100))\n","c6bdab25":"parameters = [{'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000] }]\ngrid_search = GridSearchCV(estimator = clf,\n                           param_grid = parameters,\n                           scoring = 'f1',\n                           cv = 5,\n                           n_jobs = -1,\n                           verbose=2)\ngrid_search.fit(X_train, y_train)\nbest_accuracy = grid_search.best_score_\nbest_parameters = grid_search.best_params_\nprint(\"Best f1: {:.2f} %\".format(best_accuracy*100))\nprint(\"Best Parameters:\", best_parameters)","7d67fa63":"test_prediction = clf.predict(X_test)","39a0b20e":"test_prediction","cd9c01aa":"submission = pd.DataFrame({'id': pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\").id.values,'target': test_prediction})\n","e92d50b9":"submission.to_csv(\"submission.csv\", index=False)","19c923ca":"len(submission)","7f949c45":"## Keyword","0ad9955b":"Let's go forwars keeping analyzing the data and then we will build a function to tranform the Keyword column. Let's have a look at the kind of keywords we are dealing with:","105e9456":"# Feature Transformation and Engineering","bf88deb2":"Changing categorical to numerical features:\n1. One Hot Encode Keywords+\n2. TD-IDF Vectorization fo the text feature to apply machine learning algorithm","b2ab252d":"To avoid data leakage we need to train test split the data first and then create the list from the train set of unique keywords","587e06c4":"## Location","64b493ae":"# EDA","347d36e9":"Random Forest performed slightly better than the onthers classifiers. We will fine tune this model:","49b88711":"more than 15% of values missing and not very useful info from this feature - we will drop it and create a new featur \"has_location\"","ce76f839":"## ID","88a9a3c9":"## Target","57ef67c3":"# Model Evaluation","1ccb3d9f":"We can see that most of the spaces are wrlngly encoded with %20. Also, there are words that are similar, we will take care of them when dealing with the text feature.","fbafa4b2":"## Text","6bcf865f":"As we can see here we have different porblems:\n1. There are a multitude of links. We don't need them but we will create a new feature \"has_link\"\n2. Some words have been encoded incorrectly eg. \\x89\u00db\u00aat\n3. Tabs and escape need to be removed\n4. People tags are not useful. We will take them out and add a new feature \"has_tag\"\n5. Ashtags will be removed. We will create a new feature \"ashtags\" that we will then merge with the keyword feature creating \"keyword+\". Also we will normalize the keywords using stemming\n6. We will create in the end a new feature based on the sentiment score using the compound value\n7. After we will normalize the text applying punctuation and stopwrods removal and stemming\n"}}