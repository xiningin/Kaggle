{"cell_type":{"54b85cb3":"code","01b7f603":"code","8e46bec5":"code","9154719f":"code","aa24fcf4":"code","8429103c":"code","bf5ab516":"code","406cc0cf":"code","3b4030f9":"code","11057b0b":"code","6a342d1f":"code","b43d75cf":"code","b64afc04":"code","5cc1c2bd":"code","7c1a80d9":"code","fae6a71b":"code","07175298":"code","bd204fd8":"code","7136249c":"code","997731a8":"code","249e6609":"code","4c4ccd1b":"code","160fae8b":"code","5836dc05":"code","af3b69ea":"code","3b7c3a7f":"code","8d24f880":"code","ccbf8b13":"code","ad86e0f9":"code","ccd3fbe2":"code","0f38684e":"code","5bba4e91":"code","3889d72f":"markdown"},"source":{"54b85cb3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","01b7f603":"import numpy as np\nimport pandas as pd\nfrom IPython.display import display, HTML\n# plotting\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nfrom ipywidgets import widgets\nimport plotly.express as px","8e46bec5":"train_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_df =  pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","9154719f":"train_df.head()","aa24fcf4":"train_df.describe()","8429103c":"clean_df = train_df.copy()","bf5ab516":"clean_df.drop(columns = ['Street', 'LandContour', 'Utilities', 'LandSlope', 'Condition1', 'Condition2', 'BldgType', 'RoofMatl', 'ExterCond', 'BsmtCond', 'BsmtFinType2', 'Heating', 'CentralAir', 'Electrical', 'LowQualFinSF', 'BsmtHalfBath', 'KitchenAbvGr', 'Functional', 'GarageQual', 'GarageCond', 'PavedDrive', '3SsnPorch', 'PoolArea', 'MiscVal', 'SaleType', 'SaleCondition','Alley', 'PoolQC', 'Fence', 'MiscFeature'], inplace = True)","406cc0cf":"clean_df.head()","3b4030f9":"discret_columns = ['MasVnrType','BsmtQual','BsmtExposure','BsmtFinType1','FireplaceQu','GarageType','GarageFinish']\nfor col in discret_columns:\n    most_frequent_value = clean_df[col].value_counts().index[0]\n    clean_df[col].fillna(most_frequent_value, inplace = True)","11057b0b":"continuous_columns = ['LotFrontage','MasVnrArea','GarageYrBlt']\nfor col in continuous_columns:\n    most_frequent_value = clean_df[col].value_counts().index[0]\n    clean_df[col].fillna(clean_df[col].mean(), inplace = True)","6a342d1f":"def get_list_of_low_corr_columns(df):\n    corr_df = df.corr()\n    columns = corr_df[abs(corr_df[\"SalePrice\"])<0.1][\"SalePrice\"].index\n    columns = [c for c in columns if c in df.columns]\n    return list(columns)\n\nclean_df.drop(columns=get_list_of_low_corr_columns(clean_df), inplace = True)","b43d75cf":"clean_df.head()","b64afc04":"#Normallising\nnormalizing_columns = ['LotFrontage','LotArea','OverallQual','YearBuilt','YearRemodAdd','MasVnrArea','BsmtFinSF1','BsmtUnfSF','TotalBsmtSF','1stFlrSF','2ndFlrSF','GrLivArea','BsmtFullBath','FullBath','HalfBath','BedroomAbvGr','TotRmsAbvGrd','Fireplaces','GarageYrBlt','GarageCars','GarageArea','WoodDeckSF','OpenPorchSF','EnclosedPorch','ScreenPorch']\nfor col in normalizing_columns:\n    clean_df[col] = (clean_df[col]-clean_df[col].mean())\/clean_df[col].std()","5cc1c2bd":"column_informations = {}\nnum_values = len(train_df)\nfor col in train_df.columns:\n    num_unique = train_df[col].nunique()\n    num_nulls = round(train_df[col].isna().sum()\/num_values,2)\n    d_type = train_df.dtypes[col]\n    \n    if (num_unique < 30):\n        # discrete column\n        info_str = \"[\"\n        value_counts = train_df[col].value_counts()\n        single_value_weight = round(value_counts.iloc[0] \/ num_values, 2)\n        for index, value in value_counts.items():\n            info_str += f\"{value} X {index}, \"\n        column_informations[col] = {\"d_type\":d_type, \"discret\": True, \"percentage_of_missing_values\": num_nulls, \"single_value_weight\": single_value_weight,\n                                    \"min\": 0.0, \"max\": 0.0, \"mean\": 0.0, \"median\": 0.0, \"info_str\": info_str[:-2] + \"]\"} \n    else:\n        # continuous column\n        if d_type == \"int64\" or d_type == \"float64\":\n            column_informations[col] = {\"d_type\":d_type, \"discret\": False, \"percentage_of_missing_values\": num_nulls, \"single_value_weight\": 0.0,\n                                        \"min\": train_df[col].min(), \"max\": train_df[col].max(), \"mean\": round(train_df[col].mean(), 2),\n                                        \"median\": round(train_df[col].median(), 2), \"info_str\": \"\"}\n        else:\n            column_informations[col] = {\"d_type\":d_type, \"discret\": False, \"percentage_of_missing_values\": num_nulls, \"min\": \"-\", \"max\": \"-\",\n                                        \"mean\": \"-\", \"median\": \"-\", \"info_str\": \"\"}\n\n# build DataFrame from dictionary\ninfo_df = pd.DataFrame.from_dict(column_informations, orient='index')","7c1a80d9":"display(HTML(info_df[info_df[\"discret\"]==True][[\"d_type\", \"percentage_of_missing_values\", \"single_value_weight\", \"info_str\"]].to_html()))\nprint(len(info_df[info_df[\"discret\"]==True]))","fae6a71b":"def get_list_of_numeric_columns(df):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    return [c for c in df.select_dtypes(include=numerics).columns if c in df.columns]\n\ndef get_list_of_non_numeric_cat_cols(df, info_df=info_df):\n    list_of_categorical_cols = list(info_df.loc[info_df[\"discret\"]].index)\n    list_of_categorical_cols = [c for c in list_of_categorical_cols if c in df.columns]\n    list_of_numerical_cols = get_list_of_numeric_columns(df)\n    return [c for c in list_of_categorical_cols if c not in list_of_numerical_cols]\n\n\ncategorical_columns = get_list_of_non_numeric_cat_cols(clean_df)","07175298":"for col in categorical_columns:\n    clean_df[col]=clean_df[col].astype('category').cat.codes","bd204fd8":"clean_df.head()","7136249c":"y = clean_df['SalePrice']\nclean_df.drop(columns=['SalePrice'], inplace = True)","997731a8":"import xgboost as xgb\nimport warnings\nfrom xgboost import plot_tree\nwarnings.filterwarnings(action='ignore', category=UserWarning) \n# sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV","249e6609":"X_train, X_test, y_train, y_test = train_test_split(clean_df, y)","4c4ccd1b":"xgb_model = xgb.XGBRegressor(subsample = 0.2, \n                             learning_rate=0.01,\n                             max_depth=3, \n                             n_estimators=500).fit(X_train, y_train)\nprint(\"Performance on train data:\", xgb_model.score(X_train, y_train))\nprint(\"Performance on test data:\", xgb_model.score(X_test, y_test))","160fae8b":"summission_df = test_df.copy()","5836dc05":"low_correlation_for_test = get_list_of_low_corr_columns(train_df)\nlow_correlation_for_test.remove('Id')\nlow_correlation_for_test","af3b69ea":"summission_df.drop(columns = ['Street', 'LandContour', 'Utilities', 'LandSlope', 'Condition1', 'Condition2', 'BldgType', 'RoofMatl', 'ExterCond', 'BsmtCond', 'BsmtFinType2', 'Heating', 'CentralAir', 'Electrical', 'LowQualFinSF', 'BsmtHalfBath', 'KitchenAbvGr', 'Functional', 'GarageQual', 'GarageCond', 'PavedDrive', '3SsnPorch', 'PoolArea', 'MiscVal', 'SaleType', 'SaleCondition','Alley', 'PoolQC', 'Fence', 'MiscFeature'], inplace = True)\n\nsummission_df.drop(columns = ['MSSubClass',\n 'OverallCond',\n 'BsmtFinSF2',\n 'MoSold',\n 'YrSold'], inplace = True)\n\nfor col in discret_columns:\n    most_frequent_value = summission_df[col].value_counts().index[0]\n    summission_df[col].fillna(most_frequent_value, inplace = True)\n\nfor col in continuous_columns:\n    summission_df[col].fillna(summission_df[col].mean(), inplace = True)\n\n\n\nfor col in normalizing_columns:\n    summission_df[col] = (summission_df[col]-summission_df[col].mean())\/summission_df[col].std()\n\nfor col in categorical_columns:\n    summission_df[col]=summission_df[col].astype('category').cat.codes","3b7c3a7f":"clean_df.columns","8d24f880":"summission_df.columns","ccbf8b13":"id_col = summission_df['Id']\nsummission_df.drop(columns=['Id'],  inplace = True)","ad86e0f9":"predictions = xgb_model.predict(summission_df)","ccd3fbe2":"predictions","0f38684e":"submission = pd.DataFrame(columns=['Id', 'SalePrice'])\nsubmission['Id'] = id_col\nsubmission['SalePrice'] = predictions","5bba4e91":"submission.to_csv('submission.csv', index=False)","3889d72f":"Modelling"}}