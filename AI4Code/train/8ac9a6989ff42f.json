{"cell_type":{"c1a70896":"code","36e0b33c":"code","a4e4793f":"code","34a3d6ce":"code","794fcfd9":"code","20063ad0":"code","63ffbde4":"code","4668a2cc":"code","8022477b":"code","85fd86e5":"code","515109a6":"code","74b58ae0":"code","4b8a4c81":"code","baac43e7":"code","e3e919cc":"code","e3d59916":"code","3b21eb35":"code","11a0d133":"code","5c477f53":"code","c0bf3197":"code","76e84c53":"code","679be9c7":"code","614b75de":"code","6792fd10":"code","5957a169":"code","fc2d5fa9":"code","e49b4f4b":"code","e4d78eba":"code","464af894":"code","912ab7a9":"code","d022295e":"code","663f7919":"code","70d59544":"code","88f4851a":"code","6feb339a":"code","94e3590c":"code","dea234a3":"code","b18cb079":"code","e8d3f7d2":"code","29e7aded":"code","7c600d21":"code","b1e6676c":"code","30b69849":"code","05bb6d1f":"code","31fece60":"code","a70bfe6e":"code","42ab377c":"code","c00b587e":"code","988a7c38":"code","ee5ed483":"code","546762f5":"code","73418bd8":"code","fff92da7":"code","fce723c6":"code","c52190a3":"code","a595d162":"code","1929c4cc":"code","6d0c59d6":"code","04cc3564":"code","714b36bc":"code","3b2d0ea9":"code","fbfba38c":"code","52a40b37":"markdown","5fd7264f":"markdown","e8f7e2e7":"markdown","550baca2":"markdown","af9a2521":"markdown","ed078afa":"markdown","72a7ad18":"markdown","b87ea8e8":"markdown","199f070b":"markdown","8a8f137f":"markdown","6c28b4ab":"markdown","10144eda":"markdown","35c01ab9":"markdown","5a6cc336":"markdown","fc0294aa":"markdown","476debd8":"markdown","ca139f17":"markdown","635c083c":"markdown","0b0413b3":"markdown","6c5648af":"markdown","c7a06cf6":"markdown","8adf6286":"markdown","75c14337":"markdown","073a6b8b":"markdown","6698f60e":"markdown","b8959e46":"markdown","d0a1c218":"markdown","2772b046":"markdown","37b690c8":"markdown","9e2dac92":"markdown","ef3eb8ed":"markdown","66b01e7c":"markdown","69f1d210":"markdown","053c97d2":"markdown","60ea2092":"markdown","f8e84176":"markdown","7f3ccc8a":"markdown","2ebb3e5d":"markdown","71740e5b":"markdown","ba47e0f0":"markdown","5b4e9487":"markdown","210df825":"markdown","2b8b84c3":"markdown","216a1ecb":"markdown","16ad6dbe":"markdown","e5c00b6f":"markdown","ea4b74e0":"markdown","21850ced":"markdown","49b8ed73":"markdown","d2cceb0e":"markdown","d97bc4b0":"markdown","9cc258fb":"markdown","51e8fded":"markdown","556243b3":"markdown","3aa6dbbb":"markdown","07336fa0":"markdown","81b7a0ad":"markdown","0b865969":"markdown","8d6db366":"markdown","e2be45a7":"markdown","9bf37331":"markdown","97c70a61":"markdown","b96bc590":"markdown","8ac12887":"markdown","67d1aabb":"markdown","24749df2":"markdown","cd2bf432":"markdown","4a54fd67":"markdown","7470e203":"markdown","1cc5110d":"markdown","49d8bc95":"markdown","38069a6d":"markdown","4035f740":"markdown"},"source":{"c1a70896":"#!pip install pandas\n!pip install tqdm \n!pip install snorkel\n!pip install scispacy\n!pip install bert_score\n!pip install scispacy scipy\n!pip install bert-score\n!pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.4\/en_core_sci_sm-0.2.4.tar.gz","36e0b33c":"import pandas as pd\nimport numpy as np\nimport re\nfrom tqdm import tqdm \nimport os \nimport warnings\nwarnings.filterwarnings('ignore')\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport logging\n\nfrom snorkel.labeling import PandasLFApplier,LFAnalysis,LabelingFunction\nfrom snorkel.labeling.model.label_model import LabelModel\n\nimport nltk\nfrom nltk.tokenize import sent_tokenize,word_tokenize\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.metrics import roc_auc_score\n\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader\nimport torch.nn as nn\nfrom torch.optim import Adam\nfrom torch.autograd import Variable\n\nimport spacy\nfrom spacy.matcher import PhraseMatcher\nimport scispacy\nfrom scispacy.umls_linking import UmlsEntityLinker\n\nimport transformers\nfrom transformers import BertTokenizer,BertModel, BertConfig, AdamW,BertForQuestionAnswering\ntransformers.tokenization_utils.logger.setLevel(logging.ERROR)\ntransformers.configuration_utils.logger.setLevel(logging.ERROR)\ntransformers.modeling_utils.logger.setLevel(logging.ERROR)\nfrom bert_score import score,scorer\n\n\nfrom os import path\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\nimport matplotlib.pyplot as plt\nget_ipython().run_line_magic('matplotlib', 'inline')\n","a4e4793f":"def build_raw_data (file):\n    def retunsb(sentlist,i,lennu):\n        sent=sentlist[i]\n        if i-1<0:\n            present=''\n        else:\n            present=sentlist[i-1]\n        if i+1>=lennu:\n            aftsent=''\n        else:\n            aftsent=sentlist[i+1]\n        tempsent=''\n        tempsent=tempsent.join( [present, sent, aftsent])\n        return tempsent\n    allfile=file\n    #get alldoucment\n    allfile['abstract']=allfile.abstract.astype(str)\n    #get allsentence\n    allsent=[]\n    allid=[]\n    allab=[]\n    for i in tqdm(range(len(allfile))):\n        temp=allfile.abstract.iloc[i]\n        temp=sent_tokenize(temp)\n        for j in range(len(temp)):\n            tempab=retunsb(temp,j,len(temp))\n            allsent.append(temp[j])\n            allid.append(allfile.pid.iloc[i])\n            allab.append(tempab)\n            \n    allsent=pd.DataFrame(allsent,columns=['sent'])\n    allsent['pid']=allid\n    allsent['abstract']=allab\n    return allfile, allsent\n\ndef loop_labing(keylist,valuelist,virus):\n    def keyword_lookup(x, keywords, virus ,label):\n        y=x.sent.lower()\n        print(y)\n        print(type(y))\n        if any(word in y for word in keywords) and any(word in y for word in virus) :\n            return label\n        return Norelevent\n\n    def make_keyword_lf(keywords, virus,name,label=None):\n        return LabelingFunction(\n            name=f\"keyword_{name}\",\n            f=keyword_lookup,\n            resources=dict(keywords=keywords,virus=virus ,label=label),\n        )\n    \n    def keyword_lookup1(x, keywords, virus ,label):\n        y=x.sent.lower()\n        if not any (word in y for word in keywords) and any(word in y for word in virus):\n            return label\n        return Norelevent\n\n    def make_keyword_lf1(keywords, virus,name,label=None):\n        return LabelingFunction(\n            name=f\"keyword_{name}\",\n            f=keyword_lookup1,\n            resources=dict(keywords=keywords,virus=virus ,label=label),\n        )\n    #This function has some drawback because I am writing the function to combine previous and latter sentence for a given sentence\n    def abstract_lookup(x, keywords,virus,label):\n        y=x.abstract.lower()\n        if any(word in y for word in keywords)and any(word in y for word in virus):\n            return label\n        return Norelevent\n\n    def make_abstract_lf(keywords,virus,name,label=None):\n        return LabelingFunction(\n            name=f\"abstract_{name}\",\n            f=abstract_lookup,\n            resources=dict(keywords=keywords,virus=virus,label=label),\n        )\n    \n    Norelevent = -1\n    allweaklabf=[]\n    viruselist=virus\n    for i in range(len(keylist)):\n        #labelvalue=i\n        labelvalue=1\n        vbname=keylist[i]\n        vbnameab=vbname+'su'\n        globals()[vbname] = make_keyword_lf(keywords=valuelist[i],virus=viruselist,name=vbnameab,label=labelvalue)\n        \n        vbname1=keylist[i]+'ab'\n        vbnameab1=vbname+'su1'\n        globals()[vbname1] = make_abstract_lf(keywords=valuelist[i],virus=viruselist,name=vbnameab1,label=labelvalue)\n        \n        vbname2=keylist[i]+'sent'\n        vbnameab2=vbname+'sentno'\n        globals()[vbname2] = make_keyword_lf1(keywords=valuelist[i],virus=viruselist,name=vbnameab2,label=0)\n     \n        allweaklabf.append(globals()[vbname])\n        allweaklabf.append(globals()[vbname1])\n        allweaklabf.append(globals()[vbname2])\n    \n    \n    return allweaklabf\n\ndef snorkel_process (keylist,dataframe,allweaklabf):\n    \n    cardinalitynu=2\n    applier = PandasLFApplier(lfs=allweaklabf)\n    all_train_l = applier.apply(df=dataframe)\n    report=LFAnalysis(L=all_train_l, lfs=allweaklabf).lf_summary()\n    print(report)\n    label_model = LabelModel(cardinality=cardinalitynu,verbose=False)\n    label_model.fit(all_train_l)\n    predt=label_model.predict(all_train_l)\n    dataframe['L_label']=predt\n    dataframe=dataframe[dataframe.L_label>=0]\n    \n    train,test=train_test_split(dataframe,test_size=0.2,random_state=234)#123)\n    \n    trainsent=train.sent.values\n    \n    trainlabel=train.L_label.values\n    \n    testsent=test.sent.values\n    testlabel=test.L_label.values\n    \n    return trainsent,trainlabel,testsent,testlabel,keylist,report\n\n","34a3d6ce":"def inputid (inputsent,tokenizername):\n    tokenizer = BertTokenizer.from_pretrained(tokenizername)\n    input_ids = []\n    for sent in tqdm(inputsent):\n        sent= word_tokenize(sent)[0:500]\n        encoded_sent = tokenizer.encode(sent,add_special_tokens = True)\n        input_ids.append(encoded_sent)\n    return input_ids\n\ndef maxwordnum(allsec):\n    allsentlen=[]\n    for i in tqdm(allsec):\n        wordnu=len(i)\n        allsentlen.append(wordnu)\n    maxnum=max(np.array(allsentlen))\n    return maxnum\n\ndef dxseqpadding (seq,maxnu):\n    seq2=[]\n    for i in tqdm(seq):\n        stamp=len(i)\n        i=np.pad(i,((0,maxnu-stamp)),'constant',constant_values=0)\n        seq2.append(i)\n    return seq2\n\ndef attid (inputsent):\n    attention_masks = []\n    for sent in tqdm(inputsent):\n        att_mask = [int(token_id > 0) for token_id in sent]\n        attention_masks.append(att_mask)\n    return attention_masks\ndef dataloader (trainval, test,args):\n    train_inputs=trainval[0]\n    train_inputs = torch.tensor(train_inputs)\n    train_labels=trainval[1]\n    train_labels = torch.tensor(train_labels)\n    train_masks=trainval[2]\n    train_masks = torch.tensor(train_masks)\n    \n    val_inputs=trainval[3]\n    val_inputs = torch.tensor(val_inputs)\n    val_labels=trainval[4]\n    val_labels = torch.tensor(val_labels)\n    val_masks=trainval[5]\n    val_masks = torch.tensor(val_masks)\n    \n    test_inputs=test[0]\n    test_inputs = torch.tensor(test_inputs)\n    test_labels=test[1]\n    test_labels = torch.tensor(test_labels)\n    test_masks=test[2]\n    test_masks = torch.tensor(test_masks)\n    train_data = TensorDataset(train_inputs, train_masks, train_labels)    \n    train_dataloader = DataLoader(train_data, batch_size=args, shuffle=True)\n    \n    validation_data = TensorDataset(val_inputs, val_masks, val_labels)    \n    validation_dataloader = DataLoader(validation_data, batch_size=args, shuffle=True)\n        \n    test_data = TensorDataset(test_inputs, test_masks, test_labels)\n    test_dataloader = DataLoader(test_data, batch_size=args, shuffle=True)\n    \n    return (train_dataloader,validation_dataloader,test_dataloader)\ndef bert_process (args,trainsent,valsent,testsent,trainlabel,vallabel,testlabel):\n    if args['science']==True:       \n        trainsci=inputid(trainsent,args['modelname2'])\n        valsci=inputid(valsent,args['modelname2'])\n        testsci=inputid(testsent,args['modelname2'])\n        trainnor=inputid(trainsent,args['modelname1'])\n        valnor=inputid(valsent,args['modelname1'])\n        testnor=inputid(testsent,args['modelname1'])\n        maxnum=maxwordnum(testnor)\n        trainsci=dxseqpadding(trainsci,maxnum)\n        valsci=dxseqpadding(valsci,maxnum)\n        testsci=dxseqpadding(testsci,maxnum)\n        trainnor=dxseqpadding(trainnor,maxnum)\n        valnor=dxseqpadding(valnor,maxnum)\n        testnor=dxseqpadding(testnor,maxnum)\n        trainsciatt=attid(trainsci)\n        valsciatt=attid(valsci)\n        testsciatt=attid(testsci)\n        trainnoratt=attid(trainnor)\n        valnoratt=attid(valnor)\n        testnoratt=attid(testnor)\n        nortrainval=(trainnor,trainlabel,trainnoratt,valnor,vallabel,valnoratt)\n        scitrainval=(trainsci,trainlabel,trainsciatt,valsci,vallabel,valsciatt)\n        scitest=(testsci,testlabel,testsciatt)\n        nortest=(testnor,testlabel,testnoratt)\n        norloder=dataloader(nortrainval, nortest,int(args['batch_size']))\n        sciloder=dataloader(scitrainval, scitest,int(args['batch_size']))\n    else : \n            \n        trainnor=inputid(trainsent,args['modelname1'])\n        valnor=inputid(valsent,args['modelname1'])\n        testnor=inputid(testsent,args['modelname1'])\n        maxnum=maxwordnum(testnor)\n        trainnor=dxseqpadding(trainnor,maxnum)\n        valnor=dxseqpadding(valnor,maxnum)\n        testnor=dxseqpadding(testnor,maxnum)\n        trainnoratt=attid(trainnor)\n        valnoratt=attid(valnor)\n        testnoratt=attid(testnor)\n        nortrainval=(trainnor,trainlabel,trainnoratt,valnor,vallabel,valnoratt)        \n        nortest=(testnor,testlabel,testnoratt)\n        norloder=dataloader(nortrainval, nortest,int(args['batch_size']))       \n        sciloder=[]\n    return norloder,sciloder\n\n\n\nclass bert(nn.Module):\n    def __init__(self, args):\n        super(bert, self).__init__()\n        self.args = args\n        self.emb1=BertModel.from_pretrained(self.args['modelname1'],num_labels = 1,output_attentions = False,output_hidden_states = False)#.cuda(3)\n        self.emb1_size=self.emb1.config.hidden_size\n        if self.args['science']==True:\n            self.emb2=BertModel.from_pretrained(self.args['modelname2'],num_labels = 1,output_attentions = False,output_hidden_states = False)#.cuda(3)\n            self.emb2_size=self.emb2.config.hidden_size\n            self.emb_size=self.emb1_size+self.emb2_size\n        else:\n            self.emb_size=self.emb1_size\n        #self.pool = nn.MaxPool1d(args['pool_kernel_size'])\n        #self.bn = nn.BatchNorm1d(self.emb_size)\n        self.lin1 = nn.Linear(self.emb_size, self.args['hidden_size'])\n        self.dropout = nn.Dropout(args['dropout'])\n        self.lin2 = nn.Linear(self.args['hidden_size'], 1)\n        \n    def forward(self,data1,mask1,data2=None,mask2=None):\n        if self.args['science']==True:\n            emb1=self.emb1(data1,attention_mask=mask1)\n            #pooler_output1=torch.mean(emb1[0], 1)\n            #pooler_output1=torch.mean(self.pool(emb1[0].transpose(2,1)),2)\n            pooler_output1 = emb1[1]\n            emb2=self.emb2(data2,attention_mask=mask2)\n            #pooler_output2=torch.mean(emb2[0], 1)\n            #pooler_output2=torch.mean(self.pool(emb2[0].transpose(2,1)),2)\n            pooler_output2 = emb2[1]\n            pooler_output = self.dropout(torch.cat((pooler_output1, pooler_output2), 1))\n            pooler_output = nn.functional.relu(self.lin1(pooler_output))\n        else:\n            emb1 =self.emb1(data1)\n            #pooler_output1=torch.mean(emb1[0], 1)\n            #pooler_output1=torch.mean(self.pool(emb1[0].transpose(2,1)),2)\n            pooler_output1 = self.dropout(emb1[1])\n            pooler_output = nn.functional.relu(self.lin1(pooler_output1))\n\n        out = self.lin2(pooler_output)\n        return out\n\ndef train_(model,loss,optimizer,dataloaders1,epoch,dataloaders2=None):\n    model.train()\n    allloss=[]\n    allbatch=[]\n    print('Train')\n    if args['science']==True:\n        for batch_idx, batch in tqdm(enumerate(zip(dataloaders1, dataloaders2))):\n            data1, mask1, target1 = batch[0]\n            data2, mask2, target2 = batch[1]\n            target1 = Variable(target1).to(args['device'])\n            data1 = Variable(data1).to(args['device'])\n            mask1=Variable(mask1).to(args['device'])\n            data2 = Variable(data2).to(args['device'])\n            mask2=Variable(mask2).to(args['device'])\n            optimizer.zero_grad()\n            out = model.forward(data1,mask1,data2,mask2)\n            lossall = loss(out.squeeze(),target1.float().squeeze())            \n            lossall = torch.sum(lossall)\n            lossall.backward()\n            optimizer.step()\n            loss1=lossall.item()\n            allloss.append(loss1)\n            allbatch.append(batch_idx*epoch)\n           # pdb.set_trace()\n\n    else:\n        for batch_idx, batch in tqdm(enumerate(dataloaders1)):\n            data1, mask1, target1 = batch\n            target1 = Variable(target1).to(args['device'])\n            data1 = Variable(data1).to(args['device'])\n            mask1=Variable(mask1).to(args['device'])\n            optimizer.zero_grad()\n            out = model.forward(data1,mask1)\n            lossall = loss(out.squeeze(),target1.float().squeeze())            \n            lossall = torch.sum(lossall)\n            lossall.backward()\n            optimizer.step()\n            allloss.append(lossall)\n            allbatch.append(batch_idx*epoch)\n    return allloss, allbatch\n\ndef val_(model,dataloaders1,dataloaders2=None):\n    model.eval()\n    allout=[]\n    alltarget=[]\n    print('Validation')\n    if args['science']==True:\n        for batch_idx, batch in tqdm(enumerate(zip(dataloaders1, dataloaders2))):\n            data1, mask1, target1 = batch[0]\n            data2, mask2, target2 = batch[1]\n            data1 = Variable(data1).to(args['device'])\n            mask1=Variable(mask1).to(args['device'])\n            data2 = Variable(data2).to(args['device'])\n            mask2=Variable(mask2).to(args['device'])\n            with torch.no_grad(): \n                out = torch.nn.functional.sigmoid(model.forward(data1,mask1,data2,mask2))\n            out=out.cpu().detach().numpy()\n            #target=target1.to('cpu').numpy().astype(int)\n            target=target1.numpy().astype(int)\n            \n            allout.append(list(out))\n            alltarget.append(list(target))\n    else:\n        for batch_idx, batch in tqdm(enumerate(dataloaders1)):\n            data1, mask1, target1 = batch\n            #target1 = Variable(target1).cuda()\n            data1 = Variable(data1).to(args['device'])\n            mask1=Variable(mask1).to(args['device'])\n            with torch.no_grad(): \n                out = torch.nn.functional.igmoid(model.forward(data1,mask1,target1))\n            out=out.cpu().detach().numpy()\n            target=target1.numpy().astype(int)\n            \n            allout.append(list(out))\n            alltarget.append(list(target))\n    return allout,alltarget\ndef test_(model,dataloaders1,dataloaders2=None):\n    model.eval()\n    allout=[]\n    alltarget=[]\n    allattn=[]\n    print('test')\n    if args['science']==True:\n        for batch_idx, batch in tqdm(enumerate(zip(dataloaders1, dataloaders2))):\n            data1, mask1, target1 = batch[0]\n            data2, mask2, target2 = batch[1]\n            #target1 = Variable(target1).cuda()\n            data1 = Variable(data1).to(args['device'])\n            mask1=Variable(mask1).to(args['device'])\n            #target2 = Variable(target2).cuda()\n            data2 = Variable(data2).to(args['device'])\n            mask2=Variable(mask2).to(args['device'])\n            with torch.no_grad(): \n                out = torch.nn.functional.sigmoid(model.forward(data1,mask1,data2,mask2))\n            out=out.cpu().detach().numpy()\n            target=target1.numpy().astype(int)\n            \n            allout.append(out)\n            alltarget.append(target)\n            #allattn.append(attn)\n    else:\n        for batch_idx, batch in tqdm(enumerate(dataloaders1)):\n            data1, mask1, target1 = batch\n            #target1 = Variable(target1).cuda()\n            data1 = Variable(data1).to(args['device'])\n            mask1=Variable(mask1).to(args['device'])\n            with torch.no_grad(): \n                out = torch.nn.functional.sigmoid(model.forward(data1,mask1))\n            out=out.cpu().detach().numpy()\n            target=target1.numpy().astype(int)\n            \n            allout.append(out)\n            alltarget.append(target)\n            #allattn.append(attn)\n    return allout,alltarget,\ndef result(result):\n    alllist=[]\n    for i in result:\n        for j in i:\n            alllist.append(j)\n    return alllist\n\ndef matcher (nlp, terms):\n    patterns = [nlp(text) for text in terms]\n    matcher = PhraseMatcher(nlp.vocab)\n    matcher.add(\"TerminologyList\", None, *patterns)\n    return matcher\n\ndef reportfu(dataframe, pidlist,matcher,nlp):\n    allmatchnu=[]\n    for doc in dataframe.sent.values:\n        doc=nlp(str(doc))\n        matches = matcher(doc)\n        allmatch=[]\n        for match_id, start, end in matches:\n            rule_id = nlp.vocab.strings[match_id]\n            span = doc[start : end]\n            allmatch.append(span)\n        matchnu=len(allmatch)\n        allmatchnu.append(matchnu)\n    return allmatchnu\n\ndef retrain_sentece_classification(keylist,valuelist,viruslist,allsent):\n    print('retrain')\n    allweaklabf=loop_labing(keylist,valuelist,viruslist)\n    trainsent,trainlabel,valsent,vallabel,keylist,report=snorkel_process (keylist,allsent,allweaklabf)\n    testsent=allsent.sent.values\n    testlabel=allsent.newpid.values\n    num_labels=1\n    norloder,sciloder=bert_process(args,trainsent,valsent,testsent,trainlabel,vallabel,testlabel)\n    #torch.cuda.set_device(args[\"device\"])\n    model= bert(args)\n    model=model.to(args[\"device\"])\n    params=model.parameters()\n    optimizer = AdamW(params,lr = 2e-5, eps = 1e-8 )\n    loss = nn.BCEWithLogitsLoss()\n    alloss=[]\n    allbatch=[]\n    dev_lrl=0\n    vallrl=[]\n    testpred=[]\n    testpid=[]\n    current_early_stop_times=0\n    for epoch in range(1, args['epochs'] + 1):\n        epochloss, epochbatch=train_(model,loss,optimizer,norloder[0],epoch,dataloaders2=sciloder[0])\n        alloss.append(epochloss)\n        allbatch.append(epochbatch)\n        #print('BCE training loss: ', np.mean(alloss))\n        allout,alltarget=val_(model,norloder[1],sciloder[1])\n        allout1=result(allout)\n        alltarget1=result(alltarget)\n        epochlrl=average_precision_score(alltarget1,allout1)\n        vallrl.append(epochlrl)\n        if epochlrl >= dev_lrl:\n            print(\"- new best auc{}\".format(epochlrl))\n            allout,alltarget=test_(model,norloder[2],sciloder[2])\n            allout=result(allout)\n            alltarget=result(alltarget)\n            #allattn=result(allattn)\n            allpd=pd.DataFrame(allout,columns=keylist)\n            allpd['newpid']=alltarget\n            dev_lrl = epochlrl\n            current_early_stop_times = 0\n        else:\n            current_early_stop_times += 1\n            print(current_early_stop_times)\n        if current_early_stop_times >= args['early_stop_times'] :\n            break;\n    print (\"- early stopping {} epochs without improvement\".format(epoch))\n    return allpd\n\n","794fcfd9":"def retrain_reranking(allsent, input_file_dir,gold):\n    print('rerank')\n    import en_core_sci_sm\n    nlp = en_core_sci_sm.load()\n    #nlp = spacy.load('en_core_sci_sm')\n    linker = UmlsEntityLinker(resolve_abbreviations=True)\n    nlp.add_pipe(linker)\n    allpd=allpd.merge(allsent)\n    alllabel1=pd.DataFrame()\n    for i in tqdm(range(len(keylist))) :\n        newdata=allpd[['newpid',keylist[i],'sent','pid']].sort_values(by=keylist[i],ascending =False)\n        matchers=matcher(nlp, valuelist[i])\n        pidlist=newdata.newpid.values  \n        allmatchnu=reportfu(newdata, pidlist,matchers,nlp)\n        allmatchnu1=np.where(np.array(allmatchnu) >0, 1, 0)\n        newdata['label']=allmatchnu1\n        newdata=newdata.rename(columns={keylist[i]:'predict_prob'})\n        newdata['category']=[keylist[i]]*len(newdata)\n        alllabel1=pd.concat([newdata,alllabel1])\n    refg=alllabel1[alllabel1.pid.isin(gold.pid.values)]\n    refg=alllabel1\n    refg=refg.sort_values(['predict_prob','label'], ascending=[False, True])\n    refg=refg[(refg.label==1) & (refg.predict_prob>=0.8)]\n    refg=refg[['newpid','sent','category']].drop_duplicates()\n    cand=alllabel1[['newpid','sent','category']].drop_duplicates()\n    categorylist=list(cand.category.drop_duplicates())\n    allframe=pd.DataFrame()\n    for i in tqdm(range(len(categorylist))):\n        refg1=refg[refg.category==categorylist[i]]\n        refgsent=list(refg1.sent.values)\n        refgsentn=len(refgsent)\n        cand1=cand[cand.category==categorylist[i]]\n        candsent=list(cand1.sent.values)\n        candsentn=len(candsent)\n        if refgsentn==0:\n            sentence=candsent\n            score1=[0]*candsentn\n            category=[categorylist[i]]*candsentn\n            tempframe=pd.DataFrame(sentence,columns=['sent'])\n            tempframe['score']=score1\n            tempframe['category']=category\n            allframe=pd.concat([allframe,tempframe])\n        else :\n            testrefsent=[]\n            testcandsent=[]\n            for j in range(len(candsent)):\n                for k in range(len(refgsent)):\n                    testrefsent.append(refgsent[k])\n                    testcandsent.append(candsent[j])\n            P, R, F1 = score(testcandsent, testrefsent, lang='en-sci', verbose=False)\n            sentence_candidates=testcandsent\n            score1=list(F1.numpy())\n            tempframe=pd.DataFrame(sentence,columns=['sent'])\n            tempframe['score']=score1\n            tempframe=tempframe.groupby('sent').mean().reset_index()\n            category=[categorylist[i]]*len(tempframe)\n            tempframe['category']=category\n            allframe=pd.concat([allframe,tempframe])\n        \n    allframe=allframe.sort_values(by=['category','score'],ascending=False).drop_duplicates()\n    allframe1=allframe.merge(alllabel1)\n    allframe1=allframe1[['newpid','pid','category','score','predict_prob','label']]\n    allframe1=allframe1.sort_values(['score','label','predict_prob' ], ascending=[False,False, False])\n    goldframe=pd.DataFrame()\n    for i in categorylist:\n        tempframe1=allframe1[allframe1.category==i].head(n=50)    \n        tempframe1=tempframe1[tempframe1.label==1]\n        tempframe1['real_label']=[1]*len(tempframe1)\n        tempframe2=allframe1[allframe1.category==i].tail(n=50)    \n        tempframe2=tempframe2[tempframe2.label==0]\n        tempframe2['real_label']=[0]*len(tempframe2)\n        tempframe=pd.concat([tempframe1,tempframe2])\n        goldframe=pd.concat([goldframe,tempframe])\n    allframe2=pd.DataFrame()\n    for i in categorylist:\n        tempframe=allframe1[allframe1.category==i]\n        goldframe1=goldframe[goldframe.category==i]\n        allx=tempframe[['score','predict_prob','label']].values\n        goldx=goldframe1[['score','predict_prob','label']].values\n        goldy=goldframe1[['real_label']].values\n        fit=SGDRegressor()\n        fit.fit(X=goldx ,y=goldy)\n        ally=fit.predict(allx)\n        tempframe['newscore']=ally\n        tempframe=tempframe.sort_values(by='newscore',ascending=False)\n        allframe2=pd.concat([tempframe,allframe2])\n    allframe2=allframe2[['newpid','pid','category','newscore']]\n    alllabel12=alllabel1[['pid','newpid','category','sent']]\n    alllabel12=alllabel12.merge(allframe2).sort_values(by=['category','newscore'],ascending=False)\n    sentence_candidates=alllabel12[:1000]\n    #alllabel12.to_csv(input_directory+'ALLRanking_TASK4_Q1.csv')\n    #alllabel13=alllabel12[0:1000]\n    #alllabel14=allfile[allfile.pid.isin(alllabel13.pid.values)]\n    #alllfile4.to_csv(input_directory+'ALLRanking_TASK4_Q7_1000_abstract.csv')\n    return sentence_candidates","20063ad0":"def load_pretrained_qa_model(model_str=None, use_cuda=True):\n    if model_str is None:\n        model_str = 'bert-large-uncased-whole-word-masking-finetuned-squad'\n        device = torch.device('cuda' if torch.cuda.is_available() and use_cuda else 'cpu')\n    \n    tokenizer = BertTokenizer.from_pretrained(model_str)\n    model = BertForQuestionAnswering.from_pretrained(model_str).to(device)\n\n    model.eval()\n    return tokenizer, model\n\ndef answer_question(question, document, model, tokenizer):\n    device = model.device\n    \n    encoded = tokenizer.encode_plus(question, document, return_tensors='pt', max_length=512)\n    start_scores, end_scores = model(encoded['input_ids'].to(device),\n                                     token_type_ids=encoded['token_type_ids'].to(device))\n\n    tokens = tokenizer.convert_ids_to_tokens(encoded['input_ids'].squeeze())\n    ans_start, ans_end = torch.argmax(start_scores), torch.argmax(end_scores)\n    \n    ans_tokens = tokens[ans_start: ans_end+1]\n    if '[SEP]' in ans_tokens:\n        ans_tokens = ans_tokens[ans_tokens.index('[SEP]')+1:]\n    ans = tokenizer.convert_tokens_to_string(ans_tokens)\n    ans = ans.replace(' - ', '-').replace('[CLS]', '')\n    ans_score = start_scores.max() + end_scores.max()\n\n    return ans, ans_score.item()\n\ndef ask_all_possible_questions(question_list, keyword_list, sentence):\n    answers = []\n    for keyword, question in zip(keyword_list, question_list):\n        ans, score = answer_question(question, sentence, model, tokenizer)\n        if ans != '':\n            answers.append((ans, score, keyword, question))\n    if len(answers) == 0:\n        return '', '', ''\n    answers, scores, keywords, questions = zip(*answers)\n    ans_idx = np.argmax(scores)\n    return answers[ans_idx], keywords[ans_idx], questions[ans_idx]\n\ndef answer_all_possible_questions(question_file, sentences_df, output_file):\n#     sentences_df = pd.read_csv(sentence_file, usecols=range(1, 6))\n    \n    keyword_question_df = pd.read_csv(question_file, \n                                      header=None, names=['keyword', 'question'])\n    keyword_list = keyword_question_df['keyword'].tolist()\n    question_list = keyword_question_df['question'].tolist()\n        \n    sentences = sentences_df['sent'].tolist()\n    answers = []\n    for i in tqdm(range(len(sentences))):\n        sent = sentences[i]\n        ans = ask_all_possible_questions(question_list, keyword_list, sent)\n        answers.append(ans)\n    \n    ans_lst, val_lst, ques_lst = zip(*answers)\n    sentences_df = sentences_df.assign(answer=ans_lst)\n    sentences_df = sentences_df.assign(keyword=val_lst)\n    sentences_df = sentences_df.assign(question=ques_lst)\n    \n    sentences_df.to_csv(output_file)\n    \n    return sentences_df","63ffbde4":"tokenizer, model = load_pretrained_qa_model()","4668a2cc":"def load_keywords_list(input_file_dir):\n    keylist=[]\n    with open(input_file_dir+'keylist.txt', \"r\") as f:\n        alist =f.read().splitlines()\n        for line in alist:\n            keylist=line.split(',')\n    valuelist = []\n    with open(input_file_dir+'valuelist.txt', \"r\") as f:\n        alist =f.read().splitlines()\n        alist =[x.lower() for x in alist ]\n        for line in alist:\n            valuelist.append(line.split(','))\n    viruslist = []\n    with open(input_file_dir+'viruslist.txt', \"r\") as f:\n        alist =f.read().splitlines()\n        for line in alist:\n            viruslist=line.split(',')\n            viruslist =[x.lower() for x in viruslist]\n    return keylist, valuelist, viruslist","8022477b":"#Configuration \nargs={#retrain options\n      \"retrain-sentence-classification\": False, # retrain sentence classification model\n      \"retrain-reranking\":False, # rerun the reranking model\n      \"retrain-qa\": False, #retrain question answering model\n      \"results_dir\": '\/kaggle\/input\/kagglecovid19literature\/results\/',\n    \n      #saved intermediate files\n      'original_abstract_file': 'NIH_ab.csv', #abstracts containing the keywords\n      'sentence_ranking_file': 'Bert_NIH_task4.csv', #retrieved sentences\n      'sentence_reranking_file': 'ALLRanking_TASK4_1000.csv', #reranked retrieved sentences\n      'ranked_abstract_file': 'ALLRanking_TASK4_1000_abstract.csv', #abstract containing the retrieved sentences\n      'specific_questions_file': 'question_list.csv', #fine-graind specific questions for general question\n      'answer_file': 'answers_top1000.csv', #answers to questions with given retrieved sentences\n      \"device\": torch.device('cuda' if torch.cuda.is_available() and use_cuda else 'cpu'),\n    \n      #sentence classification model's parameters\n      \"modelname1\":'bert-base-uncased', \n      \"modelname2\":'allenai\/scibert_scivocab_uncased',\n      \"hidden_size\":256,\n      \"dropout\":0.2,\n      \"batch_size\":10,\n      \"epochs\":100,\n      \"lr\":0.001,\n      \"seed\": 1,\n      \"early_stop_times\":5,\n      \"science\":True\n}","85fd86e5":"question='Q1' \nquestion_dir=question+'\/'\n\ninput_file_dir=args[\"results_dir\"]+question_dir","515109a6":"keylist, valuelist, viruslist = load_keywords_list(input_file_dir)","74b58ae0":"valuelist[0][:10]","4b8a4c81":"fullab=pd.read_csv(input_file_dir+args['original_abstract_file'])\nallfile, allsent=build_raw_data(fullab)\nallsent['newpid']=range(len(allsent))","baac43e7":"allsent.sent[0].lower()","e3e919cc":"if args['retrain-sentence-classification']:\n    allpd=retrain_sentece_classification(keylist, valuelist, viruslist, allsent)\nelse:\n    allpd=pd.read_csv(input_file_dir+args['sentence_ranking_file'])\n","e3d59916":"allpd.merge(allsent)[['newpid', 'sent']]","3b21eb35":"gold=None","11a0d133":"if args['retrain-reranking']:\n    if gold is None:\n        gold=pd.read_csv(input_file_dir+'NIH_ab_COVID.csv')\n    sentence_candidates=retrain_reranking(allsent, input_file_dir,gold)\nelse:\n    sentence_candidates=pd.read_csv(input_file_dir+args['sentence_reranking_file'])","5c477f53":"sentence_candidates[['newpid', 'sent', 'newscore']].head()","c0bf3197":"if args['retrain-qa']:\n    output_file = '\/kaggle\/working\/' + question + '_answers_top1000.csv'\n    answer_df = answer_all_possible_questions(input_file_dir+args['specific_questions_file'], \n                                              sentence_candidates, \n                                              output_file)\nelse:\n    answer_df = pd.read_csv(input_file_dir+args['answer_file'])","76e84c53":"answer_df[['newpid','question', 'answer']].head()","679be9c7":"reliable_answers=[627, 684, 816, 829, 543, 752, 1096, 626, 571, 590]\ngold=answer_df[answer_df['newpid'].isin(reliable_answers)]\n","614b75de":"text= sentence_candidates.sent[0]\nwordcloud = WordCloud().generate(text)\nwordcloud = WordCloud(max_font_size=50, max_words=1000, background_color=\"white\").generate(text)\ntext = \" \".join(review for review in sentence_candidates.sent)\nprint (\"There are {} words in the combination of all review.\".format(len(text)))\nstopwords = set(STOPWORDS)\nwordcloud = WordCloud(stopwords=stopwords, max_font_size=50, max_words=1000,background_color=\"white\").generate(text)\n\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()\n\n","6792fd10":"question='Q2' \nquestion_dir=question+'\/'\n\ninput_file_dir=args[\"results_dir\"]+question_dir","5957a169":"keylist, valuelist, viruslist = load_keywords_list(input_file_dir)","fc2d5fa9":"fullab=pd.read_csv(input_file_dir+args['original_abstract_file'])\nallfile, allsent=build_raw_data(fullab)\nallsent['newpid']=range(len(allsent))","e49b4f4b":"allsent.sent[0].lower()","e4d78eba":"if args['retrain-sentence-classification']:\n    allpd=retrain_sentece_classification(keylist, valuelist, viruslist, allsent)\nelse:\n    allpd=pd.read_csv(input_file_dir+args['sentence_ranking_file'])\n","464af894":"allpd.merge(allsent)[['newpid', 'sent']]","912ab7a9":"gold=None","d022295e":"if args['retrain-reranking']:\n    if gold is None:\n        gold=pd.read_csv(input_file_dir+'NIH_ab_COVID.csv')\n        sentence_candidates=retrain_reranking(allsent, input_file_dir,gold)\nelse:\n    sentence_candidates=pd.read_csv(input_file_dir+args['sentence_reranking_file'])","663f7919":"sentence_candidates[['newpid', 'sent', 'newscore']].head()","70d59544":"if args['retrain-qa']:\n    output_file = '\/kaggle\/working\/' + question + '_answers_top1000.csv'\n    answer_df = answer_all_possible_questions(input_file_dir+args['specific_questions_file'], \n                                              sentence_candidates, \n                                              output_file)\nelse:\n    answer_df = pd.read_csv(input_file_dir+args['answer_file'])","88f4851a":"answer_df[['newpid','question', 'answer']].head()","6feb339a":"reliable_answers=[1399, 39101, 2780, 7160, 8116, 603, 2045, 3018, 12996, 15990]\ngold=answer_df[answer_df['newpid'].isin(reliable_answers)]\n","94e3590c":"text= sentence_candidates.sent[0]\nwordcloud = WordCloud().generate(text)\nwordcloud = WordCloud(max_font_size=50, max_words=1000, background_color=\"white\").generate(text)\ntext = \" \".join(review for review in sentence_candidates.sent)\nprint (\"There are {} words in the combination of all review.\".format(len(text)))\nstopwords = set(STOPWORDS)\nwordcloud = WordCloud(stopwords=stopwords, max_font_size=50, max_words=1000,background_color=\"white\").generate(text)\n\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()\n\n","dea234a3":"question='Q3' \nquestion_dir=question+'\/'\n\ninput_file_dir=args[\"results_dir\"]+question_dir","b18cb079":"keylist, valuelist, viruslist = load_keywords_list(input_file_dir)","e8d3f7d2":"fullab=pd.read_csv(input_file_dir+args['original_abstract_file'])\nallfile, allsent=build_raw_data(fullab)\nallsent['newpid']=range(len(allsent))","29e7aded":"allsent.sent[0].lower()","7c600d21":"if args['retrain-sentence-classification']:\n    allpd=retrain_sentece_classification(keylist, valuelist, viruslist, allsent)\nelse:\n    allpd=pd.read_csv(input_file_dir+args['sentence_ranking_file'])\n","b1e6676c":"allpd.merge(allsent)[['newpid', 'sent']]","30b69849":"gold=None","05bb6d1f":"if args['retrain-reranking']:\n    if gold is None:\n        gold=pd.read_csv(input_file_dir+'NIH_ab_COVID.csv')\n    sentence_candidates=retrain_reranking(allsent, input_file_dir,gold)\nelse:\n    sentence_candidates=pd.read_csv(input_file_dir+args['sentence_reranking_file'])","31fece60":"sentence_candidates[['newpid', 'sent', 'newscore']].head()","a70bfe6e":"if args['retrain-qa']:\n    output_file = '\/kaggle\/working\/' + question + '_answers_top1000.csv'\n    answer_df = answer_all_possible_questions(input_file_dir+args['specific_questions_file'], \n                                              sentence_candidates, \n                                              output_file)\nelse:\n    answer_df = pd.read_csv(input_file_dir+args['answer_file'])","42ab377c":"answer_df[['newpid','question', 'answer']].head()","c00b587e":"reliable_answers=[24397, 23799, 22874, 5207, 4015, 4797, 13057, 35129, 13470, 18611] \ngold=answer_df[answer_df['newpid'].isin(reliable_answers)]\n","988a7c38":"text= sentence_candidates.sent[0]\nwordcloud = WordCloud().generate(text)\nwordcloud = WordCloud(max_font_size=50, max_words=1000, background_color=\"white\").generate(text)\ntext = \" \".join(review for review in sentence_candidates.sent)\nprint (\"There are {} words in the combination of all review.\".format(len(text)))\nstopwords = set(STOPWORDS)\nwordcloud = WordCloud(stopwords=stopwords, max_font_size=50, max_words=1000,background_color=\"white\").generate(text)\n\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()\n\n","ee5ed483":"question='Q7' \nquestion_dir=question+'\/'\n\ninput_file_dir=args[\"results_dir\"]+question_dir","546762f5":"keylist, valuelist, viruslist = load_keywords_list(input_file_dir)","73418bd8":"fullab=pd.read_csv(input_file_dir+args['original_abstract_file'])\nallfile, allsent=build_raw_data(fullab)\nallsent['newpid']=range(len(allsent))","fff92da7":"allsent.sent[0].lower()","fce723c6":"if args['retrain-sentence-classification']:\n    allpd=retrain_sentece_classification(keylist, valuelist, viruslist, allsent)\nelse:\n    allpd=pd.read_csv(input_file_dir+args['sentence_ranking_file'])\n","c52190a3":"allpd.merge(allsent)[['newpid', 'sent']]","a595d162":"gold=None","1929c4cc":"if args['retrain-reranking']:\n    if gold is None:\n        gold=pd.read_csv(input_file_dir+'NIH_ab_COVID.csv')\n    sentence_candidates=retrain_reranking(allsent, input_file_dir,gold)\nelse:\n    sentence_candidates=pd.read_csv(input_file_dir+args['sentence_reranking_file'])","6d0c59d6":"sentence_candidates[['newpid', 'sent', 'newscore']].head()","04cc3564":"if args['retrain-qa']:\n    output_file = '\/kaggle\/working\/' + question + '_answers_top1000.csv'\n    answer_df = answer_all_possible_questions(input_file_dir+args['specific_questions_file'], \n                                              sentence_candidates, \n                                              output_file)\nelse:\n    answer_df = pd.read_csv(input_file_dir+args['answer_file'])","714b36bc":"answer_df[['newpid','question', 'answer']].head()","3b2d0ea9":"reliable_answers= [5041, 13724, 4688, 7153, 9964, 11546, 10794, 12358, 3191, 12086]\ngold=answer_df[answer_df['newpid'].isin(reliable_answers)]\n","fbfba38c":"text= sentence_candidates.sent[0]\nwordcloud = WordCloud().generate(text)\nwordcloud = WordCloud(max_font_size=50, max_words=1000, background_color=\"white\").generate(text)\ntext = \" \".join(review for review in sentence_candidates.sent)\nprint (\"There are {} words in the combination of all review.\".format(len(text)))\nstopwords = set(STOPWORDS)\nwordcloud = WordCloud(stopwords=stopwords, max_font_size=50, max_words=1000,background_color=\"white\").generate(text)\n\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()\n\n","52a40b37":"Load original abstract files containing the keywords. This will be the subset of data we will focus.","5fd7264f":"Given the retrieved sentences, we rerank them based on human feedback on the relevance of the sentences. \n\n`gold` is a list of `pid` that human experts think reliable. For the first initial learning, we don't have `gold` yet. Just set it as `None`. If `gold` is provided, we will rerank the sentences as prioritizing the `gold` and sentences similar to `gold`.","e8f7e2e7":"Now we are going back to `retrain_reranking` and repeat until our models answer correctly and human experts satify with the answers. Just a few iteration would be enough:)","550baca2":"Ranked list of relevant sentences","af9a2521":"Human can review the answers and select the most reliable answers","ed078afa":"Re-ranked list of relevant sentences","72a7ad18":"## Question Answering","b87ea8e8":"# Question 7\nRephrase the general question:\n\n> Efforts to develop animal models and standardize challenge studies\n\ninto specific questions:\n\n> Are mice and rats used to evaluate the lethal challenge\n\n> Are young and senescent mice associated with the homologous and heterologous challenge\n\n> Are standardized intranasal challenges and DNA immunization associated with mice models\n\n> Are SARS-CoV mice models associate with heterologous challenges\n\n> Are rodents and primates used to test robust protection from the lethal dose of virus\n\n","199f070b":"Load original abstract files containing the keywords","8a8f137f":"Load necessary keywords input","6c28b4ab":"Given the retrieved sentences, we rerank them based on human feedback on the relevance of the sentences. \n\n`gold` is a list of `pid` that human experts think reliable. For the first initial learning, we don't have `gold` yet. Just set it as `None`. If `gold` is provided, we will rerank the sentences as prioritizing the `gold` and sentences similar to `gold`.","10144eda":"Given the retrieved sentences, we rerank them based on human feedback on the relevance of the sentences. \n\n`gold` is a list of `pid` that human experts think reliable. For the first initial learning, we don't have `gold` yet. Just set it as `None`. If `gold` is provided, we will rerank the sentences as prioritizing the `gold` and sentences similar to `gold`.","35c01ab9":"Here is example of `valuelist`. We tried to seek a specific concept to mine specific and accurate sentences.","5a6cc336":"## Reranking","fc0294aa":"Ranked list of relevant sentences","476debd8":"## Reranking","ca139f17":"Set input directory","635c083c":"## Question Answering","0b0413b3":"Ranked list of relevant sentences","6c5648af":"![summary.png](attachment:summary.png)","c7a06cf6":"## Pseudo labeling and Sentence classification","8adf6286":"Based on the keywords, we will i) generate pseudo label for the relevance of the sentences, ii) learn a classification model to classify the sentence is relevent to the question or not, iii) retrieved the top 1000 most confident sentences.","75c14337":"## Pseudo labeling and Sentence classification","073a6b8b":"Based on the keywords, we will i) generate pseudo label for the relevance of the sentences, ii) learn a classification model to classify the sentence is relevent to the question or not, iii) retrieved the top 1000 most confident sentences.","6698f60e":"\n# Question 3\nRephrase the general question:\n>  Exploration of use of best animal models and their predictive value for a human vaccine.\n\ninto specific questions:\n> What animal models are used to evaluate SARS-CoV\n\n> are mice models used to evaluate the SL-CoV vaccine\n\n> Do mice models increase IFN-\u0152\u2265 by DNA vaccines\n\n> Can vaccinated mice prevent MERS-Cov\n\n> Are mice and monkeys used to test vaccine\n\nand 7 other questions.\n","b8959e46":"Human can review the answers and select the most reliable answers","d0a1c218":"## Pseudo labeling and Sentence classification","2772b046":"Re-ranked list of relevant sentences","37b690c8":"Set input directory","9e2dac92":"Configurate models, saving directory, loading directory, etc","ef3eb8ed":"## Reranking","66b01e7c":"Import packages","69f1d210":"If you finish the human-machine feedback loops, let us summarize the literature using word cloud.","053c97d2":"## Question Answering","60ea2092":"Human can review the answers and select the most reliable answers. Our subject matter experts reviewed and selected the following sentences as listed in `reliable_answers`. \n\nNote that we have `qgrid` to interactively check the relevance of sentences, but we failed to load them in Kaggle kernel.","f8e84176":"Load original abstract files containing the keywords","7f3ccc8a":"Load necessary keywords input","2ebb3e5d":"Human can review the answers and select the most reliable answers","71740e5b":"Now we are going back to `retrain_reranking` and repeat until the human experts satify with the answers","ba47e0f0":"If you finish the human-machine feedback loops, let us summarize the literature using word cloud.","5b4e9487":"Define functions for reranking","210df825":"## Question Answering","2b8b84c3":"Re-ranked list of relevant sentences","216a1ecb":"Example of sentence","16ad6dbe":"## Reranking","e5c00b6f":"Example of sentence","ea4b74e0":"Define function for loading keywords","21850ced":"Install requirements","49b8ed73":"Now we are going back to `retrain_reranking` and repeat until the human experts satify with the answers","d2cceb0e":"Re-ranked list of relevant sentences","d97bc4b0":"Based on the keywords, we will \n1. generate pseudo label for the relevance of the sentences\n2. learn a classification model to classify the sentence is relevent to the question or not\n3. retrieve the top 1000 most confident sentences.","9cc258fb":"# Question 2\nRephrase the general question:\n> Methods evaluating potential complication of Antibody-Dependent Enhancement (ADE) in vaccine recipients.\n\ninto specific questions:\n> Are epitopes and antibodies involved in antibody dependent enhancement\n\n> What immune response are involved in ADE\n\n> Are sustained inflammation, lymphopenia, and\/or cytokine storm involved in ADE\n\n> Are SARS-CoV vaccines involved in immunopathology\n\n> What mouse models are involved in SARS-CoV vaccines\n\nand 13 other questions.\n","51e8fded":"If you finish the human-machine feedback loops, let us summarize the literature using word cloud.","556243b3":"Aggregated visualization is here\nhttps:\/\/github.com\/yejinjkim\/kaggle-covid19-literature\/blob\/master\/kaggle_scattertext.html","3aa6dbbb":"Example of sentence","07336fa0":"Set input directory","81b7a0ad":"Define functions for pseudo label","0b865969":"Load original abstract files containing the keywords","8d6db366":"Now we are going back to `retrain_reranking` and repeat until the human experts satify with the answers","e2be45a7":"# A Human-in-loop Literature Digest for COVID-19 Treatments","9bf37331":"Preload the question answering model","97c70a61":"\n\n## Summary\nThis notebook presents the literature review for COVID-19 vaccines and therapeutics. The main strengths are as follows:\n- Integrate human feedback loop in the pipeline to seamlessly integrate human expert\u2019s knowledge and machine learning\u2019s computation\n- Enhance relevance labels using labeling function in Snorkel\n\n\n\n## Objective\nTo integrate state-of-the-art natural language processing (NLP) techniques and human expert knowledge in a harmonized pipeline for automated extraction of relevant literature for COVID-19 vaccines and therapeutics. \n\n### Pros\n- Rank the relevant sentences by three granularity criteria: keywords inclusion, pseudo labels on relevance, similarity to gold standard sentence \n- Integrate human feedback in the loop\n- Enhance relevance labels by combining weak knowledges\n\n\n### Cons: \n- Manual keywords generation requires subject matter expert's knowledge\n- Inclusion of keywords might not be the best way to select relevant documents or sentences\n- Sometimes the retrieved sentences can be less accurate depending on the difficulty of defining keywords\n\n## Approach\nThe pipeline consists of five modules\n1. Keywords generation\n2. Pseudo labeling of sentences\n3. Relevant sentence retrieval and reranking\n4. Question Answering\n\n### 1. Keywords generation\nWe formulated this information retrieval as a semi-supervised learning. We provided an initial small set of gold standards (a set of relevant sentences by reviewing a few papers that accurately answer the given questions). We also provided silver standards via label modeling using [Snorkel](https:\/\/www.snorkel.org\/). For the ten questions in Task 4, we listed relevant keywords that describe the question in fine-grained concepts (For example, the first question is related to \u201cdrugs being developed\u201d, the human experts will find actual drugs that are being developed, such as Remdesivir, hydroxycholoroquine,  Lopinavir, and other under developing COVID-2019 drugs). We used those keywords to search relevant papers within the dataset and generated pseudo labels for the relevance of the sentences to the Task4's questions. To extend the set of limited keywords and find unnoticed drugs, we used [UMLS](https:\/\/www.nlm.nih.gov\/research\/umls\/index.html) and applied entity recognition technology to COVID-2019 related documents. Other keywords can came from experts to provide additional informaiton.\n\n### 2. Pseudo labeling of sentences\nThen we used those keywords to build combined labeling function by combining drug names and other related information (such as virus name). For the selected documents we annotated each sentence whether relevant or not using the inclusion of the keywords combinations as a silver standard label. These partial and incomplete weak labels were used as initial input to the labeling function implemented in [Snorkel](https:\/\/www.snorkel.org\/) to derive pseudo labels of remaining sentences without annotations. Human experts further evaluated those labeled sentences that were relevant to the concepts we define or not. The selected sentences are used to train models. Other unknown sentences are predicted by our model. The total number of selected documents for this preliminary pipeline was 3,269. There were 25,739 sentences in this dataset. \n\n\n### 3. Relevant sentence retrieval\nOnce we have a pseudo label on some sentence's relevance, we built a sentence classification model to classify the sentences without a label. Using the pseudo labels, we extracted relevant sentences out of all 25,739 sentences. We used pre-trained BERT) and Sci-BERT for the sentence classification.\n\n### 4. Question Answering\nWe implemented the question-answering framework to extract key information from the sentences retrieved by previous steps that best address each question. This step uses a publicly available pre-trained BERT model that was fine-tuned with SQuAD dataset for question-answering purposes.\n\nOnce we ranked the most relevant sentences to answers for the questions, human experts examined and gave feedback on the validity of each sentence. The human feedback was injected back to the pseudo labeling modules to prioritize the human validated sentences as a gold standard. We repeated the module 2-4 until the extracted sentences converged to human expert\u2019s knowledge. \n\n\n## Acknowledgements\n[Sci-BERT]()\n\n[BERT]()\n\n[BERT-SQuAD]()\n\n[Snorkel](https:\/\/www.snorkel.org\/)\n\n[Drug list from Frenkel-Morgenstern Lab](http:\/\/mfm-lab.md.biu.ac.il\/research\/covid19\/)\n\n","b96bc590":"Our results are here - ranked list of relevant sentences.","8ac12887":"## Pseudo labeling and Sentence classification","67d1aabb":"Define functions for sentence classification","24749df2":"If you finish the human-machine feedback loops, let us summarize the literature using word cloud.","cd2bf432":"Load necessary keywords input","4a54fd67":"Load necessary keywords input","7470e203":"Example of sentence","1cc5110d":"Set input directory","49d8bc95":"Given the retrieved sentences, we rerank them based on human feedback on the relevance of the sentences. \n\n`gold` is a list of `pid` that human experts think reliable. For the first initial learning, we don't have `gold` yet. Just set it as `None`. If `gold` is provided, we will rerank the sentences as prioritizing the `gold` and sentences similar to `gold`.","38069a6d":"# Question 1\nRephrase the general question:\n> Effectiveness of drugs being developed and tried to treat COVID-19 patients\n\ninto specific questions:\n> Can ritonavir treat COVID-19 \n\n> Can lopinavir treat COVID-19\n\n> Can darunavir treat COVID-19\n\n> Can AZT treat COVID-19\n\n> Can nelfinavir treat COVID-19\n\nand 80 other questions.","4035f740":"Define functions for question answering"}}