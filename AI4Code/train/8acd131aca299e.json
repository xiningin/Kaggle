{"cell_type":{"5cc71d87":"code","5e4ff2c4":"code","d26e74e6":"code","1666bdd0":"code","34ed7102":"code","ea789f52":"code","a6bd5f33":"code","ecf6709c":"code","6d50a0da":"code","978b6a88":"code","fb470d9f":"code","f897825a":"code","385e62d6":"code","9d536e51":"code","3ff9732d":"code","6359d68d":"code","4af200bc":"code","8cf13fe6":"code","da7c6ca7":"code","51d06688":"code","feb7211e":"code","7ea0f64c":"code","04ca941e":"code","5962ae6a":"code","855e3b18":"code","830fa7ee":"code","2d355029":"code","d2c204a3":"code","98187e53":"code","0f66c020":"code","430970cc":"code","584204eb":"code","6755f44c":"code","b5684b98":"code","8ceb9727":"code","06328fb6":"code","dca4fbb6":"code","bda51954":"code","25288b66":"code","3a5674de":"code","da53768c":"code","1af22285":"code","4dd95aa2":"code","83dc0f70":"code","a2c8f8f0":"code","a0735083":"code","6962bd95":"code","8af036d9":"code","6e6e3d7a":"code","058c03f2":"code","facb6bd4":"code","76108101":"code","4578918d":"code","290276bd":"code","df6d5092":"code","26bcff72":"code","3655ad35":"markdown","c3fe5c0d":"markdown","d1bde5a3":"markdown"},"source":{"5cc71d87":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5e4ff2c4":"# Using MNIST dataset\nfrom sklearn.datasets import fetch_openml","d26e74e6":"df = fetch_openml('mnist_784')\ndf.keys() # We will use mainly two keys, data(X) and target(y)","1666bdd0":"df['target'] # This is our label.","34ed7102":"X, y = df['data'], df['target']\nX[:1].shape, X[:1].ndim\n# Observing the shape and dimension of first element of X \n# because you need to manipulate shape for visualisation  \n","ea789f52":"X.shape, y.shape, y[:5] \n# Now observe the shape of whole X, y\n# and look at the labels which is in string form","a6bd5f33":"some_image = np.array(X[:1]).reshape(28, 28)\n# Reshaping first element of X to visualise because you can't \n# visualise it without changing shape","ecf6709c":"# Importing necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nplt.imshow(some_image, cmap=\"binary\") # without cmap, your image will be colorful\nplt.axis(\"off\") # If you don't use this line, you will see the axis","6d50a0da":"# 70% train, 30% test\nX_train, X_test, y_train, y_test = X[:48999], X[49000:], y[:48999], y[49000:]","978b6a88":"# Looking at the shape and dimension(if ndim > 2, it will throw error in training)\nX_train.shape, X_train.ndim","fb470d9f":"# Now let's try SGDclassifier\nfrom sklearn.linear_model import SGDClassifier\nsgd = SGDClassifier(max_iter=2000, random_state=42) # Increased max_iter from 1000 to 2000 to avoid max iteration reached limit error\nsgd.fit(X_train, y_train)\n","f897825a":"sgd.score(X_train, y_train)","385e62d6":"# Now let's predict for our test data\nsgd_pred = sgd.predict(X_test)\nsgd_pred","9d536e51":"# Check the score how well it is performing\nsgd.score(X_test, y_test)","3ff9732d":"# Let's try stratifiedkfold to observe accuracy\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.base import clone # use clone to copy sgd model\n\nsk_fold = StratifiedKFold(random_state=42, n_splits=3, shuffle=True)\nfor train_index, test_index in sk_fold.split(X_train, y_train):\n    \n    X_train_fold, X_test_fold = X_train.loc[train_index], X_train.loc[test_index]\n    y_train_fold, y_test_fold = y_train.loc[train_index], y_train.loc[test_index]\n    \n    sgd_clone = clone(sgd)\n    sgd_clone.fit(X_train_fold, y_train_fold)\n    y_pred = sgd_clone.predict(X_test_fold)\n    correct = sum(y_pred == y_test_fold)\n    print(correct\/len(y_pred))","6359d68d":"from sklearn.model_selection import cross_val_score\ncross_val_score(sgd_clone, X_train, y_train, cv=3, scoring='accuracy')\n# observe cross_val_score for sgd_clone model for each fold(cv=3 fold)","4af200bc":"# Now let's try random forest classifier which has good accuracy\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier()\nrf.fit(X_train, y_train)","8cf13fe6":"# wow it's 1\nrf.score(X_train, y_train)","da7c6ca7":"rf_pred = rf.predict(X_test)\nrf_pred","51d06688":"# Now it's better than sgd classifier\nrf.score(X_test, y_test)","feb7211e":"# observe the confusion matrix for each class(there are 10 from 0-9)\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test, rf_pred)","7ea0f64c":"# Now trying SVC classifier\nfrom sklearn.svm import SVC\nsvc_clf = SVC()\nsvc_clf.fit(X_train, y_train)","04ca941e":"# let's predict first element which we used in the beginning(X[0]=5)\nsome_image = np.array(X[:1])\nsvc_clf.predict(some_image)","5962ae6a":"# Observe the decision function of X[0] which tells the chances of being\n# 5 in the form of score out of 10. Observe the highest number 9.312 \n# at the fifth position. The number 8.30 at the 3rd position shows close\n# similarity towards 5.\ndec_func = svc_clf.decision_function(some_image)\ndec_func","855e3b18":"# It shows outcome after passing decision function\nnp.argmax(dec_func)","830fa7ee":"# shows total no of classes\nsvc_clf.classes_","2d355029":"# Try standard scaler to scale the input X, it can increase the cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train.astype(np.float64))","d2c204a3":"# try increasing cv from 3 to 5 and observe the difference\ncross_val_score(sgd, X_train_scaled, y_train, cv=3, scoring=\"accuracy\")","98187e53":"# Loading the dataset\ntrain = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')","0f66c020":"# 42000*785 size, I'm printing first 5 rows\ntrain.head()","430970cc":"# Local data details\nY_train_loc = train['label']\nY_train_loc[:5]","584204eb":"# Removing Labels from the input data\nX_train_loc = train.drop(labels=['label'], axis=1)\nX_train_loc.head()","6755f44c":"# Let's see count of each class using countplot\nimport seaborn as sns\ng = sns.countplot(x=Y_train_loc)\nY_train_loc.value_counts()","b5684b98":"# Observing the shape and dimension of data \n#( you need  ndim=4 for model building using keras )\nX_train_loc.shape, X_train_loc.ndim,  Y_train_loc.shape","8ceb9727":"# check for null\nX_train.isnull().any().describe()","06328fb6":"test.isnull().any().describe()","dca4fbb6":"# Converting dataframe X_train_loc and test into array \n# so that you can reshape data into ndim=4 \nX_train_loc = np.array(X_train_loc)","bda51954":"# Reshape image\nX_train_loc = X_train_loc.reshape(-1, 28, 28, 1)\ntest = test.values.reshape(-1, 28, 28, 1)","25288b66":"X_train_loc.shape, Y_train_loc.shape, test.shape","3a5674de":"# Defining categories of label\nfrom keras.utils.np_utils import to_categorical\nY_train_loc = to_categorical(Y_train_loc, num_classes=10)\nY_train_loc.shape","da53768c":"# Split training and validation set\nfrom sklearn.model_selection import train_test_split\nX_train_loc, X_val, Y_train_loc, Y_val = train_test_split(X_train_loc, Y_train_loc, test_size=0.1, random_state=2)","1af22285":"# plotting one instance with axis(use plt.axis(\"off\") to remove axis)\nplt.imshow(np.array(X_train_loc)[2], cmap=\"binary\")","4dd95aa2":"# Importing necessary libraries of keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau","83dc0f70":"# Using convolution layer\nmodel = Sequential()\nmodel.add(Conv2D(filters=64, kernel_size=(5,5), padding='Same', activation='relu', input_shape=(28,28,1)))\nmodel.add(BatchNormalization())","a2c8f8f0":"model.add(Conv2D(filters=64, kernel_size=(5,5), padding='Same', activation='relu'))\nmodel.add(BatchNormalization())","a0735083":"# Pooling layer acts as downsampling filter\n#  It looks at the 2 neighboring pixels and picks the maximal value.\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))","6962bd95":"model.add(Conv2D(filters=64, kernel_size=(3,3), padding='Same', activation='relu'))\nmodel.add(BatchNormalization())","8af036d9":"model.add(Conv2D(filters=64, kernel_size=(3,3), padding='Same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(0.25))\n# Dropout is a regularization method, where a proportion \n# of nodes in the layer are randomly ignored \n# (setting their weights to zero) for each training sample).","6e6e3d7a":"model.add(Conv2D(filters=64, kernel_size=(3,3), padding='Same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.25))","058c03f2":"model.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.25))\n\nmodel.add(Dense(10, activation='softmax'))\nmodel.summary()","facb6bd4":"from keras.utils import plot_model\nplot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)\nfrom IPython.display import Image\nImage('model.png')","76108101":"optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)","4578918d":"model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])","290276bd":"# Set annealing method of learning rate\n#  to make the optimizer converge faster and \n# closest to the global minimum of the loss function\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', patience=3, verbose=1, factor=0.5, min_lr=0.0)\n","df6d5092":"# Increase epochs for better accuracy, here I'm posting for epoch = 1 \n# because It will take time to run for more number epochs\nepochs=1\nbatch_size=32","26bcff72":"# without data augmentation, and with epoch=1, accuracy=99.7%\nhistory = model.fit(X_train_loc, Y_train_loc, batch_size= batch_size, epochs = epochs, validation_data=(X_val, Y_val), verbose=2)\n","3655ad35":"# Handling MNIST dataset using Keras","c3fe5c0d":"# I took deep learning section of MNIST from Abdelwahed Ashraf Notebook. You can look at his notebook for further reference\n<a href=\"https:\/\/www.kaggle.com\/abdelwahed43\/handwritten-digits-recognizer-0-999-simple-model\">CLICK HERE<\/a>","d1bde5a3":"<img src=\"https:\/\/www.researchgate.net\/profile\/Hiromichi-Fujisawa\/publication\/222834590\/figure\/fig3\/AS:305192978403329@1449775084162\/Sample-images-of-MNIST-data.png\" width=\"500\" height=\"200\">"}}