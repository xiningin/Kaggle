{"cell_type":{"ef9b096a":"code","716a19c1":"code","927b90c0":"code","083a7466":"code","4eff1f61":"code","498c4cf4":"code","873f5b61":"code","6802104a":"code","abb34497":"code","69a8195e":"code","32f58e13":"code","c193f510":"code","0e71aa29":"code","9b7050be":"code","b4e5e621":"markdown"},"source":{"ef9b096a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","716a19c1":"from typing import Tuple\n\nimport albumentations as A\nimport pandas as pd\nimport torch\nfrom albumentations.pytorch import ToTensorV2\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nimport cv2\nfrom torch.nn import functional as F\n\n\ndef get_image(item: pd.Series, image_dir: str):\n    image_path = os.path.join(image_dir, item.filename)\n    image = cv2.imread(image_path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    return image\n\n\ndef get_transform(image_size, normalize=True, train=True):\n    if train:\n        transforms = [\n            A.Resize(height=image_size, width=image_size),\n            A.Transpose(p=0.5),\n            A.VerticalFlip(p=0.5),\n            A.HorizontalFlip(p=0.5),\n        ]\n    else:\n        transforms = [\n        ]\n\n    if normalize:\n        transforms.append(A.Normalize())\n\n    transforms.extend([\n        ToTensorV2(),\n    ])\n    return A.Compose(transforms)\n\n\nclass ShopeeDataset(Dataset):\n    def __init__(\n            self, df: pd.DataFrame, image_dir: str,\n            transform=None,\n    ):\n        self.df = df\n        self.image_dir = image_dir\n        self.transform = transform\n\n    def __len__(self) -> int:\n        return len(self.df)\n\n    def __getitem__(self, idx) -> Tuple:\n        item = self.df.iloc[idx]\n        image = get_image(item, self.image_dir)\n        image_id = item.filename\n        # get label\n        label = item.category\n        data = {\n            'image': image,\n            'labels': label,\n        }\n\n        if self.transform is not None:\n            data = self.transform(**data)\n\n        image = data['image']\n        label = torch.tensor(data['labels'], dtype=torch.long)\n\n        return image, label\n","927b90c0":"import torch\nfrom torch import nn\n\nBACKBONE='resnext101_32x8d_swsl'\n\n\nclass ResNetHead(nn.Module):\n    def __init__(self, in_features: int, n_classes: int, use_neck=0):\n        super().__init__()\n\n        self.pooling = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc1 = nn.Linear(in_features, n_classes)\n        self.use_neck = use_neck\n\n    def forward(self, x):\n        if not self.use_neck:\n            x = self.pooling(x)\n            x = torch.flatten(x, start_dim=1)\n        x = self.apply_fc_out(x)\n        return x\n\n    def apply_fc_out(self, x):\n        return self.fc1(x)\n    \n    \nclass ResNetBase(nn.Module):\n    def __init__(self, backbone='resnext50_32x4d_ssl'):\n        super(ResNetBase, self).__init__()\n        if backbone.endswith('l'):\n            self.backbone = torch.hub.load(\n                'facebookresearch\/semi-supervised-ImageNet1K-models',\n                backbone,\n            )\n        else:\n            self.backbone = getattr(models, backbone)(pretrained=True)\n        self.out_features = self.backbone.fc.in_features\n\n    def forward(self, x):\n        base = self.backbone\n        x = base.conv1(x)\n        x = base.bn1(x)\n        x = base.relu(x)\n        x = base.maxpool(x)\n\n        x = base.layer1(x)\n        x = base.layer2(x)\n        x = base.layer3(x)\n        x = base.layer4(x)\n        return x\n\n\ndef build_model(backbone: str, n_classes: int, **kwargs) -> nn.Module:\n    return Model(backbone=backbone, n_classes=n_classes, **kwargs)\n\n\nclass Model(nn.Module):\n    def __init__(self, *, backbone: str, n_classes: int, use_neck=0):\n        super().__init__()\n\n        self.backbone = ResNetBase(backbone)\n        self.in_features = self.backbone.out_features\n        self.use_neck = use_neck\n        if self.use_neck:\n            self.hidden_dim = 1024\n            self.neck = Neck(self.in_features, 1024)\n            self.in_features = self.hidden_dim\n        self.head = ResNetHead(self.in_features, n_classes, self.use_neck)\n\n    def forward(self, x):\n        x = self.backbone(x)\n        if self.use_neck:\n            x = self.neck(x)\n        x = self.head(x)\n        return x","083a7466":"class WrappedModel(nn.Module):\n    def __init__(self, module):\n        super(WrappedModel, self).__init__()\n        self.module = module # that I actually define.\n    def forward(self, x):\n        return self.module(x)","4eff1f61":"def load_models(model_files):\n    models = []\n    for model_f in model_files:\n        model_f = os.path.join(model_dir, model_f)\n        model = build_model(n_classes=42, backbone=BACKBONE)\n        model = WrappedModel(model)\n        model.load_state_dict(torch.load(model_f, map_location=lambda storage, loc: storage), strict=True)\n        model.eval()\n        model.to(device)\n        models.append(model)\n        print(f'{model_f} loaded!')\n    return models","498c4cf4":"image_test_dir = '..\/input\/shopeetestcentercrop\/test_256x256\/test_256x256\/'\nimage_test_dir_2 = '..\/input\/shopeetestcentercrop\/test_center_crop_128x128\/test_center_crop_128x128\/'\n# model_dir = '..\/input\/shopeemodels\/'\nmodel_dir = '..\/input\/shopeeresnext101cutmix\/'\n\ndevice = torch.device('cuda')\n\ndf_test = pd.read_csv('..\/input\/shopee-product-detection-open\/test.csv')\n\nmodel_files = [\n    'best_fold0.pth',\n    'best_fold1.pth',\n    'best_fold2_839.pth',\n    'best_fold3.pth',\n#     'best_fold2_837.pth',\n]\nmodels = load_models(model_files)","873f5b61":"IMAGE_SIZE = 128\nBATCH_SIZE = 64","6802104a":"test_dataset = ShopeeDataset(\n        df_test, image_dir=image_test_dir,\n        transform=get_transform(image_size=IMAGE_SIZE, train=False),\n)\ndata_test_loader = DataLoader(\n        test_dataset, batch_size=BATCH_SIZE,\n        shuffle=False\n)","abb34497":"FINAL_PRED = []\nwith torch.no_grad():\n    for (data, targets) in tqdm(data_test_loader):\n        data = data.to(device, dtype=torch.float)\n\n        preds = []\n        for model in models:\n            model = model.to(device)\n            output = model(data)\n            preds.append(output)\n\n        # ----ensemble models----#\n        proba = torch.mean(torch.stack([pred for pred in preds], dim=0), dim=0)\n\n\n        pred = F.softmax(proba, dim=1).data.cpu().numpy().argmax(axis=1)\n        for i in range(len(pred)):\n            FINAL_PRED.append(pred[i])","69a8195e":"# LOGITS_FINAL = []\n# for i, model in enumerate(models):\n#     LOGITS = []\n#     LOGITS2 = []\n#     test_dataset = ShopeeDataset(\n#         df_test, image_dir=image_test_dir,\n#         transform=get_transform(image_size=IMAGE_SIZE, train=False),\n#     )\n#     data_test_loader = DataLoader(\n#         test_dataset, batch_size=BATCH_SIZE,\n#         shuffle=False\n#     )\n\n#     test_dataset2 = ShopeeDataset(\n#         df_test, image_dir=image_test_dir_2,\n#         transform=get_transform(image_size=IMAGE_SIZE, train=False),\n#     )\n#     data_test_loader2 = DataLoader(\n#         test_dataset2, batch_size=BATCH_SIZE,\n#         shuffle=False\n#     )\n                            \n#     with torch.no_grad():\n#         for (data, targets) in tqdm(data_test_loader):\n#             data = data.to(device)\n#             logits = model(data)\n#             LOGITS.append(logits)\n\n#         for (data, targets) in tqdm(data_test_loader2):\n#             data = data.to(device)\n#             logits = model(data)\n#             LOGITS2.append(logits)\n\n#     LOGITS_TMP = (F.softmax(torch.cat(LOGITS), dim=1).cpu() + F.softmax(torch.cat(LOGITS2), dim=1).cpu()) \/ 2\n#     LOGITS_FINAL.append(LOGITS_TMP)\n\n# model_num = len(LOGITS_FINAL)\n# PREDS = 0\n\n# for ans in LOGITS_FINAL:\n#     PREDS += ans\/model_num\n# PREDS = PREDS.numpy().argmax(axis=1)\n# FINAL_PRED = PREDS","32f58e13":"df_test['category'] = FINAL_PRED","c193f510":"df_test['category'] = df_test['category'].apply(lambda x: f'{x:02}')","0e71aa29":"df_test.to_csv('submission.csv', index=False)","9b7050be":"df_test[:20]","b4e5e621":"# TTA"}}