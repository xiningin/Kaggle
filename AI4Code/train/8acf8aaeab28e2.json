{"cell_type":{"e1ab4ad5":"code","e1aafe28":"code","c0077164":"code","f83293c3":"code","4100d8d0":"markdown","f14ea7d1":"markdown","42bed47f":"markdown","233a2a54":"markdown","7110e878":"markdown","7ecef57a":"markdown"},"source":{"e1ab4ad5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e1aafe28":"# test classification dataset\nfrom sklearn.datasets import make_classification\n# define dataset\nX, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5,n_classes=2,\nweights=None,random_state=7)\n# summarize the dataset\nprint(X.shape, y.shape)","c0077164":"#Converting into dataframes\nX_dataframe,y_dataframe=pd.DataFrame(X),pd.DataFrame(y)\nX_dataframe","f83293c3":"y_dataframe","4100d8d0":"# So,what does the parameters of **make_classification** mean?","f14ea7d1":"**n_samples** - total number of training rows, examples that match the parameters. That's why in the shape of the returned design matrix, X, it is (n_samples, n_features)\n\n**n_features** - number of columns\/features of dataset.\n\n**n_informative** - number of features that will be useful in helping to classify your test dataset. In other words, if you perform PCA or another dimensionality reduction algorithm, you should be able to explain nearly 100 % of the variance in the problem with just n_informative (in this case 15) features.\n\n**n_redundant** -number of features that can be left out after dimensionality reduction\n\n**n_classes** - The number of classes (or labels) of the classification problem.\n\n**weights** - basically this is a ratio of class balance. For your case, since it's set to None and number of samples = 1000, you'll get 500 classes of each label.\n\n**random_state** - Determines random number generation for dataset creation. Pass an int for reproducible output across multiple function calls\n","42bed47f":"# Running the code creates the dataset and confirms that the input part of the dataset has \n* 1,000 rows and \n* 20 columns\n\nfor the 20 input variables and that the output variable has \n\n* 1,000 rows with two classes. \n\nThe input features are stored in X and output target classes are in y. \n\nThey are in numpy format. Thus we need to convert it in pandas dataframe.","233a2a54":"# In this notebook , I will explain all the terms associated with creating the dataset.","7110e878":"# If you want to know more about other terms involved,\n\n**Reference:**  https:\/\/docs.w3cub.com\/scikit_learn\/modules\/generated\/sklearn.datasets.make_classification.html","7ecef57a":"# In order to create a dummy dataset, the **make_classification()** function from **sklearn.datasets** can be used. We can create our dataset as per our requirements and then evaluate our machine learning models."}}