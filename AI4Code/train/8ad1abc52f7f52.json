{"cell_type":{"5dcc2a23":"code","2dc47c6e":"code","8e094e8f":"code","deda19ee":"code","da671ccc":"code","10e2bdff":"code","3715998f":"code","656688c5":"code","3326dc9e":"code","9367ab8b":"code","c776edd3":"code","6335221d":"code","09e7450d":"code","7ad6043b":"code","29467a6c":"code","493fbf75":"code","4528df8a":"code","c6fb3f81":"code","fac99b3e":"code","8ff8877b":"code","d3931175":"code","ee2198b4":"code","776504d0":"code","fa151696":"code","0f0185f3":"code","d6f7f3ae":"code","fe1d454c":"code","e2369cd9":"code","9a8c8b17":"code","b505cc10":"code","b060b3f5":"code","7c09b050":"code","d6490baf":"code","f8b1f8aa":"code","03b0de5a":"code","5561b01d":"code","0398f671":"code","6b6e060e":"code","5228b1e4":"code","549be454":"code","0fa81688":"code","a1fdbea6":"code","8f93d0d2":"code","2a304488":"markdown","178805db":"markdown","eb0be705":"markdown","a27c559b":"markdown","34a6a2cc":"markdown","9c7bc709":"markdown","ef2279f8":"markdown","2bc3fdd7":"markdown","1acb03dd":"markdown","9d8ac4ab":"markdown","35e80f30":"markdown","8df76f24":"markdown","dcb84ce9":"markdown","7ae7f39e":"markdown","e7e21bbc":"markdown","d7d21e89":"markdown","68049ac0":"markdown","19c589f0":"markdown","9007f211":"markdown","bf3425e3":"markdown","d30fbeaf":"markdown","4d629095":"markdown","df82601a":"markdown","97ec5e13":"markdown","8abb93a4":"markdown","7a938c24":"markdown","21744a55":"markdown","09c4fc36":"markdown","b5069c79":"markdown","0e95a42a":"markdown","f2ee93aa":"markdown","5ef3ded1":"markdown","0907ed03":"markdown"},"source":{"5dcc2a23":"import pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom collections import Counter\nimport re\n\nimport nltk as nl\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer \n\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,precision_score, recall_score, f1_score, accuracy_score\n\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.neural_network import MLPClassifier","2dc47c6e":"nl.download('wordnet')\nnl.download('omw-1.4')\nnl.download('stopwords')","8e094e8f":"stopwords = stopwords.words('english')\nstemmer = SnowballStemmer(\"english\")\nlemmatizer = WordNetLemmatizer()","deda19ee":"dataset = pd.read_csv(\"\/kaggle\/input\/spam-mails-dataset\/spam_ham_dataset.csv\")","da671ccc":"dataset.head()","10e2bdff":"dataset.info()","3715998f":"dataset_mails = dataset.drop(dataset.columns[0:2],axis=1)","656688c5":"dataset_mails","3326dc9e":"def clean_text(text):\n    new_text=text.lower()\n    clean_text= re.sub(\"[^a-z]+\",\" \",new_text)\n    clean_text_stopwords = \"\"\n    for i in clean_text.split(\" \")[1:]:\n        if not i in stopwords and len(i) > 3:\n            clean_text_stopwords += i\n            clean_text_stopwords += \" \"\n            clean_text_stopwords=lemmatizer.lemmatize(clean_text_stopwords)\n            clean_text_stopwords=stemmer.stem(clean_text_stopwords)\n    return clean_text_stopwords","9367ab8b":"dataset_mails[\"text_clean\"] = dataset_mails.text.apply(clean_text)","c776edd3":"dataset_mails","6335221d":"dataset_mails_clean = dataset_mails.drop(dataset_mails.columns[0:1],axis=1)","09e7450d":"dataset_mails_clean['len'] = dataset_mails_clean['text_clean'].str.len()","7ad6043b":"dataset_mails_clean","29467a6c":"plt.rcParams['figure.figsize'] = (10, 7)\nsns.boxenplot(x = dataset_mails_clean['label_num'], y = dataset_mails_clean['len'])\nplt.title('relation entre les spams et la longueur du text')\nplt.show()\n","493fbf75":"count_Class=pd.value_counts(dataset_mails_clean[\"label_num\"], sort= True)\ncount_Class.plot(kind= 'bar', color= [\"blue\", \"orange\"])\nplt.title('Bar chart')\nplt.show()","4528df8a":"count_Class.plot(kind = 'pie',  autopct='%1.0f%%')\nplt.title('Pie chart')\nplt.ylabel('')\nplt.show()","c6fb3f81":"count1 = Counter(\" \".join(dataset_mails_clean[dataset_mails_clean['label_num']==0][\"text_clean\"]).split()).most_common(20)\ndf1 = pd.DataFrame.from_dict(count1)\ndf1 = df1.rename(columns={0: \"words in non-spam\", 1 : \"count\"})\ncount2 = Counter(\" \".join(dataset_mails_clean[dataset_mails_clean['label_num']==1][\"text_clean\"]).split()).most_common(20)\ndf2 = pd.DataFrame.from_dict(count2)\ndf2 = df2.rename(columns={0: \"words in spam\", 1 : \"count_\"})","fac99b3e":"df1.plot.bar(legend = False)\ny_pos = np.arange(len(df1[\"words in non-spam\"]))\nplt.xticks(y_pos, df1[\"words in non-spam\"])\nplt.title('Mots plus fr\u00e9quents dans les messages non spam')\nplt.xlabel('words')\nplt.ylabel('number')\nplt.show()","8ff8877b":"df2.plot.bar(legend = False, color = 'orange')\ny_pos = np.arange(len(df2[\"words in spam\"]))\nplt.xticks(y_pos, df2[\"words in spam\"])\nplt.title('Mots plus fr\u00e9quents dans les spams')\nplt.xlabel('words')\nplt.ylabel('number')\nplt.show()","d3931175":"    from wordcloud import WordCloud\n    text = ' '.join(dataset_mails_clean.text_clean.values)\n    wordcloud = WordCloud(max_font_size=50, max_words=1000, background_color=\"white\",  colormap='BuGn').generate(text.lower())\n    plt.figure(figsize=(15,15))\n    plt.imshow(wordcloud, interpolation=\"bilinear\")\n    plt.axis(\"off\")\n    plt.show()","ee2198b4":"dataset_mails_clean.groupby('label_num').describe()","776504d0":"# dataset_mails_clean.groupby('text_clean').describe()","fa151696":"x = dataset_mails_clean['text_clean']\ny = dataset_mails_clean['label_num']","0f0185f3":"x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2)","d6f7f3ae":"print(x_train.shape, x_test.shape)","fe1d454c":"print(\"Samples per class in train {}\".format(np.bincount(y_train)))\nprint(\"Samples per class in test {}\".format(np.bincount(y_test)))","e2369cd9":"bow_vec = CountVectorizer()","9a8c8b17":"train_bow = bow_vec.fit_transform(x_train)\ntest_bow = bow_vec.transform(x_test)","b505cc10":"cv_df = pd.DataFrame(train_bow.toarray(),columns = bow_vec.get_feature_names())\ncv_df.head()","b060b3f5":"# le nombre des mots dans le vocabulaire bag of words\n\nfeature_names = bow_vec.get_feature_names()\nprint(\"Number of features: {}\".format(len(feature_names)))","7c09b050":"# Use multiple classifiers and grid search for prediction\ndef ML_modeling(models, params, X_train, X_test, y_train, y_test, performance_metrics):    \n    \n    if not set(models.keys()).issubset(set(params.keys())):\n        raise ValueError('Some estimators are missing parameters')\n\n    for key in models.keys():\n    \n        model = models[key]\n        param = params[key]\n        gs = GridSearchCV(model, param, cv=10, error_score=0, refit=True)\n        gs.fit(X_train, y_train)\n        y_pred = gs.predict(X_test)\n        \n        # Print scores for the classifier\n        accuracy_sc = accuracy_score(y_test, y_pred)\n        precision_sc= precision_score(y_test, y_pred, average='macro')\n        recall_sc = recall_score(y_test, y_pred, average='macro')\n        f1_sc =  f1_score(y_test, y_pred, average='macro')\n        \n        performance_metrics.append([key,accuracy_sc,precision_sc,recall_sc,f1_sc])\n        print(key, ':', gs.best_params_)\n        print(\"Accuracy: %1.3f \\tPrecision: %1.3f \\tRecall: %1.3f \\t\\tF1: %1.3f\\n\" % (accuracy_sc, precision_sc, recall_sc, f1_sc))\n        \n    return ","d6490baf":"## Preparing to make a pipeline \nmodels = {\n    'SVM': SVC(),\n    'Naive Bayes': MultinomialNB(),\n    'Perceptron': MLPClassifier()\n}\n\nparams = {\n    'SVM': { 'kernel': ['linear', 'rbf'] },\n    'Naive Bayes': { 'alpha': [0.5, 1], 'fit_prior': [True, False] },\n    'Perceptron': { 'activation': ['tanh', 'relu'] }\n}","f8b1f8aa":"%%time\nperformance_metrics_bow = []\nprint(\"==============Bag of Words==============\\n\")\nML_modeling(models, params, train_bow, test_bow, y_train, y_test, performance_metrics_bow)","03b0de5a":"metrics_bow_df = pd.DataFrame(performance_metrics_bow,columns=['Model' , 'Accuracy', 'Precision' , 'Recall', \"F1 Score\"])","5561b01d":"tfidf = TfidfVectorizer()","0398f671":"train_tfidf = tfidf.fit_transform(x_train)\ntest_tfidf = tfidf.transform(x_test)","6b6e060e":"tfidf_df = pd.DataFrame(train_tfidf.toarray(), columns = tfidf.get_feature_names())\ntfidf_df.head()","5228b1e4":"print(\"==============TF-IDF==============\\n\")\nperformance_metrics_tfidf = []\nML_modeling(models, params, train_tfidf, test_tfidf, y_train, y_test, performance_metrics_tfidf)","549be454":"metrics_tfidf_df = pd.DataFrame(performance_metrics_tfidf,columns=['Model' , 'Accuracy', 'Precision' , 'Recall', \"F1 Score\"])","0fa81688":"metrics_bow_df","a1fdbea6":"metrics_tfidf_df","8f93d0d2":"f, axes = plt.subplots(2, 2, figsize=(12,8), sharey = True, sharex = True)\n\naxes[0, 0].plot(metrics_bow_df['Model'], metrics_bow_df['Accuracy'], linestyle = 'solid', linewidth = 2, label = 'Bag-Of-Words')\naxes[0, 0].plot(metrics_tfidf_df['Model'], metrics_tfidf_df['Accuracy'], linestyle = 'solid', linewidth = 2, label = 'TF-IDF')\naxes[0, 0].set_title('Accuracy')\n\naxes[0, 1].plot(metrics_bow_df['Model'], metrics_bow_df['F1 Score'], linestyle = 'solid', linewidth = 2)\naxes[0, 1].plot(metrics_tfidf_df['Model'], metrics_tfidf_df['F1 Score'], linestyle = 'solid', linewidth = 2)\naxes[0, 1].set_title('F1-mesure')\n\naxes[1, 0].plot(metrics_bow_df['Model'], metrics_bow_df['Precision'], linestyle = 'solid', linewidth = 2)\naxes[1, 0].plot(metrics_tfidf_df['Model'], metrics_tfidf_df['Precision'], linestyle = 'solid', linewidth = 2)\naxes[1, 0].set_title('Precision')\n\naxes[1, 1].plot(metrics_bow_df['Model'], metrics_bow_df['Recall'], linestyle = 'solid', linewidth = 2)\naxes[1, 1].plot(metrics_tfidf_df['Model'], metrics_tfidf_df['Recall'], linestyle = 'solid', linewidth = 2)\naxes[1, 1].set_title('Recall')\n\nf.legend()\nf.show()","2a304488":"**Objectif de projet**\n\nD\u00e9tecter les spams\n\n**Vous trouverez ci-dessous les \u00e9tapes que nous suivrons essentiellement:**\n\n> 1. \u00c9tape 1: Comprendre les donn\u00e9es \n>\n> 2. \u00c9tape 2: Pr\u00e9traitement\n>\n>    * Stop-Word Removal \n>    * Lower Casing \n>    * Stemming \n>    * Lemmatization \n>    * Tokenization \n>\n>\n> 3. \u00c9tape 3: BAG OF WORDS\n>\n> 4. \u00c9tape 4: TF-IDF\n>\n> 5. \u00c9tape 5: Evalution ","178805db":"Savoir les meilleurs param\u00e8tres. ","eb0be705":"### Bag of Words","a27c559b":"### La description de la dataset","34a6a2cc":"#### Num\u00e9risation du texte \u00e0 l'aide de TfidfVectorizer","9c7bc709":"### Enlever le champ text avant le pr\u00e9traitement","ef2279f8":"#### L'entrainement des mod\u00e8les et l'affichage des scores","2bc3fdd7":"# Classify emails and detect spams","1acb03dd":"### TF-IDF","9d8ac4ab":"#### **nous constatons que les messages avec une grande longueur sont des spams**","35e80f30":"#### Cr\u00e9ation de la nouvelle dataset qui va contenir notre texte sous format num\u00e9rique","8df76f24":"## Pr\u00e9traitement","dcb84ce9":"#### Cr\u00e9ation de la nouvelle dataset qui va contenir notre texte sous format num\u00e9rique","7ae7f39e":"Pour choisir les meilleurs param\u00e8tres de ces mod\u00e8les, on va travailler avec la biblioth\u00e8que\nGridSearchCV qui est une m\u00e9thode d'optimisation (hyperparameter optimization) qui va nous\npermettre de tester une s\u00e9rie de param\u00e8tres et de comparer les performances pour en\nd\u00e9duire le meilleur param\u00e9trage.\nOn va d\u2019abord cr\u00e9er la fonction d'entra\u00eenement du mod\u00e8le avec diff\u00e9rents param\u00e8tres.","e7e21bbc":"### Visualisation du nombre des mots les plus fr\u00e9quents dans les non-spams et spams","d7d21e89":"## Biblioth\u00e8ques","68049ac0":"### CountVectorizer (Bag of Word Vector)","19c589f0":"## L'apprentissage automatique des mod\u00e8les","9007f211":"### Visualisation","bf3425e3":"### Visualisation de la longueur du texte dans les spams et non spams","d30fbeaf":"### TfidfVectorizer (TF-IDF)","4d629095":"#### L'entrainement du mod\u00e8le SVM, Naive Bayes et MLPClassifier(PMC)","df82601a":"### Application de la fonction sur toute la dataset","97ec5e13":"## Visualisation","8abb93a4":"### L'ajout d'un nouveau champ (Len) qui est la taille de chaque texte","7a938c24":"#### Division de la dataset","21744a55":"# Evaluation","09c4fc36":"## Enlever les champs secondaires","b5069c79":"#### L'affichage des scores\n\nDans l\u2019affichage ci-dessous on voit chaque mod\u00e8les avec les meilleurs param\u00e8tres et les\nm\u00e9triques d'\u00e9valuation (Accuracy, precision, recall, f1_score)\n","0e95a42a":"### Visualisation du nombre de spams et de non-spam","f2ee93aa":"## Explorer l'ensemble de donn\u00e9es","5ef3ded1":"### La fonction clean text permet de convertir toutes les lettres en minuscules, supprimer les num\u00e9ros, la ponctuation et tous les mots qui ont moins de 3 caract\u00e8res","0907ed03":"#### Num\u00e9risation du texte \u00e0 l'aide de CountVectorizer"}}