{"cell_type":{"aeb50bf2":"code","48cec969":"code","053a7bce":"code","1b5a1ea1":"code","3cff26fb":"code","1bdbdb9a":"code","7a6dbc12":"code","1c41c568":"code","9e665c89":"code","e7f300a1":"code","19482ca3":"code","2f81fb15":"code","4911a0c4":"code","02195537":"code","ac6a0b79":"code","09b54c6b":"code","17bbdac0":"code","3f047128":"code","64886890":"code","7fb5e791":"code","ce3fe471":"code","f98862fd":"code","c4f54176":"code","a7104817":"code","a9eaad70":"code","21a727fd":"code","d5aa3486":"code","fcf30c7c":"code","26f7e58c":"code","7966b318":"code","ff90295b":"code","a42e6cf7":"markdown","123d18d7":"markdown","5504e0c9":"markdown","b98a900e":"markdown","401a6881":"markdown","1d0d47ee":"markdown","f09f1bad":"markdown","f2671749":"markdown","b8db0086":"markdown"},"source":{"aeb50bf2":"%load_ext tensorboard\nimport os\nfrom glob import glob\nimport time\nimport random\n\nimport IPython.display as display\nimport matplotlib.pyplot as plt # Matplotlib is used to generate plots of data.\nimport matplotlib.image as mpimg\nimport PIL\nfrom PIL import Image\nimport imageio\nimport numpy as np # Numpy is an efficient linear algebra library.\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n%matplotlib inline","48cec969":"# Experiment paths\n# Save the model for further use\nEXPERIMENT_ID = \"train_1\"\nMODEL_SAVE_PATH = os.path.join(\"\/content\/cell_images_32\/results\/\", EXPERIMENT_ID)\nif not os.path.exists(MODEL_SAVE_PATH):\n    os.makedirs(MODEL_SAVE_PATH)\nCHECKPOINT_DIR = os.path.join(MODEL_SAVE_PATH, 'training_checkpoints')\n\n# Data path\nDATA_PATH = \"..\/input\/cell-images32\"\n\n# Model parameters\nBATCH_SIZE = 64\nEPOCHS = 20000\nLATENT_DEPTH = 100\nIMAGE_SHAPE = [32,32]\nn_ch = 3\nLR = 1e-4\n\nseed = random.seed(30)","053a7bce":"image_count = len(list(glob(str( DATA_PATH + '*.png'))))\nimage_count","1b5a1ea1":"cell_images_path = list(glob(str(DATA_PATH + '*.png')))\nfor image_path in cell_images_path[:10]:\n    display.display(Image.open(str(image_path)))","3cff26fb":"# Read in the image\nimage = mpimg.imread(cell_images_path[20])\n\nplt.axis(\"off\")\nplt.imshow(image)","1bdbdb9a":"# Isolate RGB channels\nr = image[:,:,0]\ng = image[:,:,1]\nb = image[:,:,2]\n\n# Visualize the individual color channels\nf, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20,10))\nax1.set_title('R channel')\nax1.imshow(r, cmap='gray')\nax2.set_title('G channel')\nax2.imshow(g, cmap='gray')\nax3.set_title('B channel')\nax3.imshow(b, cmap='gray')","7a6dbc12":"@tf.function\ndef preprocessing_data(path):\n    image = tf.io.read_file(path)\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.image.resize(image, [IMAGE_SHAPE[0],IMAGE_SHAPE[1]])\n    image = image \/ 255.0\n    return image","1c41c568":"def dataloader(paths):\n    dataset = tf.data.Dataset.from_tensor_slices(paths)\n    dataset = dataset.shuffle(10* BATCH_SIZE)\n    dataset = dataset.map(preprocessing_data)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(1)\n    return dataset","9e665c89":"dataset = dataloader(cell_images_path)\nfor batch in dataset.take(1):\n    for img in batch:\n        img_np = img.numpy()\n        plt.figure()\n        plt.axis('off')\n        plt.imshow((img_np-img_np.min())\/(img_np.max()-img_np.min()))","e7f300a1":"image_list = []\nfor filename in cell_images_path:\n    im=Image.open(filename)\n    im =im.resize((IMAGE_SHAPE[0],IMAGE_SHAPE[1]))\n    image_list.append(im)","19482ca3":"images_processed = []\nfor image in image_list:\n    gan_train_images = np.asarray(image)\n    gan_train_images = gan_train_images.astype('float32') \/255.0\n    images_processed.append(gan_train_images)","2f81fb15":"dataset = tf.data.Dataset.from_tensor_slices(images_processed).shuffle(10* BATCH_SIZE).batch(BATCH_SIZE)\nfor batch in dataset.take(1):\n    for img in batch:\n        img_np = img.numpy()\n        plt.figure()\n        plt.axis('off')\n        plt.imshow((img_np-img_np.min())\/(img_np.max()-img_np.min()))","4911a0c4":"# Let's make genrator model function\n\ndef make_generator_model():\n\n    model = tf.keras.Sequential()\n    model.add(layers.Dense(4*4*256, use_bias=False, input_shape=(100,)))\n    model.add(layers.LeakyReLU())\n\n\n    model.add(layers.Reshape((4, 4, 256)))\n    assert model.output_shape == (None, 4, 4, 256) \n\n\n    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same', use_bias=False))\n    assert model.output_shape == (None, 8, 8, 128) \n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same', use_bias=False))\n    assert model.output_shape == (None, 16, 16, 64) \n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n\n    model.add(layers.Conv2DTranspose(32, (4, 4), strides=(2, 2), padding='same', use_bias=False))\n    assert model.output_shape == (None, 32, 32, 32)\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n\n    model.add(layers.Conv2DTranspose(3, (3, 3), strides=(1, 1), padding='same', \n                                     use_bias=False, activation='tanh')) \n    assert model.output_shape == (None, 32, 32, 3) \n\n    return model","02195537":"generator = make_generator_model()\n\nnoise = tf.random.normal([1, 100])\ngenerated_image = generator(noise, training=False)\n\nplt.imshow(generated_image[0, :, :, 0], cmap='gray')","ac6a0b79":"def make_discriminator_model():\n    model = tf.keras.Sequential()\n    model.add(layers.Conv2D(64, (4, 4), strides=(2, 2), padding='same',\n                                     input_shape=[32, 32, 3]))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n \n    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n \n    model.add(layers.Flatten())\n \n    model.add(layers.Dense(1))\n\n    return model","09b54c6b":"discriminator = make_discriminator_model()\ndecision = discriminator(generated_image)\nprint (decision)","17bbdac0":"# This method returns a helper function to compute the cross entropy loss. (for Binary Classification)\ncross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)","3f047128":"def discriminator_loss(real_output, fake_output):\n    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n    total_loss = real_loss + fake_loss\n    return total_loss","64886890":"def generator_loss(fake_output):\n    return cross_entropy(tf.ones_like(fake_output), fake_output)","7fb5e791":"# Use Adam as the Optimizer\ngenerator_optimizer = tf.keras.optimizers.Adam(1e-4)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(1e-4)","ce3fe471":"checkpoint_dir = '.\/training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\ncheckpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n                                 discriminator_optimizer=discriminator_optimizer,\n                                 generator=generator,\n                                 discriminator=discriminator)","f98862fd":"# Define the training loop\n\nEPOCHS = 10000\nnoise_dim = 100\nnum_examples_to_generate = 16\n\nseed = tf.random.normal([num_examples_to_generate, noise_dim])","c4f54176":"@tf.function\ndef train_step(images):\n    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n      generated_images = generator(noise, training=True)\n\n      real_output = discriminator(images, training=True)\n      fake_output = discriminator(generated_images, training=True)\n\n      gen_loss = generator_loss(fake_output)\n      disc_loss = discriminator_loss(real_output, fake_output)\n\n    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n\n    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))","a7104817":"# Create and save images\n\ndef generate_and_save_images(model, epoch, test_input):\n  predictions = model(test_input, training=False)\n\n  fig = plt.figure(figsize=(4,4))\n\n  for i in range(predictions.shape[0]):\n      plt.subplot(4, 4, i+1)\n      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n      plt.axis('off')\n\n  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n  plt.show()\n","a9eaad70":"def train(dataset, epochs):\n  for epoch in range(epochs):\n    start = time.time()\n\n    for image_batch in dataset:\n      train_step(image_batch)\n\n# Instantly create images for GIF.\n    display.clear_output(wait=True)\n    generate_and_save_images(generator,\n                             epoch + 1,\n                             seed)\n\n\n# It saves the model every 15 epoch passes\n    if (epoch + 1) % 15 == 0:\n      checkpoint.save(file_prefix = checkpoint_prefix)\n    \n\n # print the report on how much time it takes for each epoch\n    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n\n# Generate after the last epoch is over.\n  display.clear_output(wait=True)\n  generate_and_save_images(generator,\n                           epochs,\n                           seed)\n","21a727fd":"# Model training\n%%time\ntrain(dataset, EPOCHS)","d5aa3486":"# Restore the last checkpoint.\ncheckpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))","fcf30c7c":"# Create GIF\n# Displays a single image using epoch numbers.\n\ndef display_image(epoch_no):\n  return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))","26f7e58c":"display_image(EPOCHS)","7966b318":"anim_file = 'dcgan.gif'\n\nwith imageio.get_writer(anim_file, mode='I') as writer:\n    filenames = glob(os.path.join(MODEL_SAVE_PATH, 'image*.png'))\n    filenames = sorted(filenames)\n    last = -1\n    for i,filename in enumerate(filenames):\n        frame = 2*(i**0.5)\n        if round(frame) > round(last):\n            last = frame\n        else:\n            continue\n        image = imageio.imread(filename)\n        writer.append_data(image)\n    image = imageio.imread(filename)\n    writer.append_data(image)\n\nimport IPython\nif IPython.version_info > (6,2,0,''):\n    display.Image(filename=anim_file)","ff90295b":"# If you are working at Colab, you can download the animation from the code below:\n\ntry:\n  from google.colab import files\nexcept ImportError:\n  pass\nelse:\n  files.download(anim_file)","a42e6cf7":"Acknowledgement: Portions of this page are reproduced and modified from work created and shared by Google and used according to terms described in the Creative Commons 4.0 Attribution License.","123d18d7":"The data ","5504e0c9":"### Experiment utils (RUN ME!)","b98a900e":"Let's create an image using a generator that hasn't been trained yet.","401a6881":"### Save checkpoint\nThis notebook shows you how to save and restore models that can be useful in cases where long-running training is disrupted.","1d0d47ee":"### Model Traning","f09f1bad":"# DCGAN with Parasitized Images","f2671749":"## Creating the model\nGenerator and Discriminator are defined using [Keras Sequential API] \n","b8db0086":"# Please give an upvoe if you feel it is useful\n\n### plaese check other notebooks \n\n* [DCGAN with MNIST ](https:\/\/www.kaggle.com\/joshuajhchoi\/wgan-gp-with-mnist-for-absolute-beginners)\n\n* [DCGAN with Cifar10](https:\/\/www.kaggle.com\/joshuajhchoi\/dcgan-with-cifar10-for-absolute-beginners)\n\n* [DCGAN with fashion MNIST](https:\/\/www.kaggle.com\/joshuajhchoi\/dcgan-with-fashion-mnist-for-absolute-beginners)\n\n* [CGAN with MNIST](https:\/\/www.kaggle.com\/joshuajhchoi\/cgan-with-mnist-for-absolute-beginners)\n\n* [WGAN with MNIST](http:\/\/https:\/\/www.kaggle.com\/joshuajhchoi\/wgan-with-fashion-mnist-for-absolute-beginners)\n\n* [WGAN-GP with MNIST](https:\/\/www.kaggle.com\/joshuajhchoi\/wgan-gp-with-mnist-for-absolute-beginners)"}}