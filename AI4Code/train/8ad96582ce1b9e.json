{"cell_type":{"04d58771":"code","cc5f51ae":"code","bb5a11f1":"code","229250c9":"code","72bc3597":"code","04543026":"code","b55a8f19":"code","b5694062":"code","7ed79e7b":"code","83994c0d":"code","99b52988":"code","7fa697d6":"code","f453d116":"code","6fdf4adc":"code","f1a96682":"code","b458d3f7":"code","b28abcbb":"code","b8bbe06b":"code","6547d5f0":"code","9684dfc3":"code","e2477f34":"code","dae6989d":"code","422c837b":"code","7638e707":"code","e2df72e9":"code","730da930":"code","9b5a201b":"code","95775273":"code","e7f5f570":"code","49449ac5":"code","1910e66f":"code","01fcabce":"code","5914c8e9":"code","e42e1949":"code","b4756b8d":"code","89eb4c09":"code","e51b3a17":"code","c931116f":"code","917c6404":"code","1e312be0":"code","3034f17f":"code","073af90f":"code","9798104c":"code","13a14f99":"code","d50a3178":"code","28e76724":"code","d767b2d9":"code","8e728a60":"code","81bc5547":"code","ca762cb0":"code","9ce6283c":"code","a3eadf1f":"code","dbe72c45":"code","215c6fa6":"code","7284200b":"code","2c17821a":"code","57f37037":"code","b5877f4c":"code","8b50aec8":"code","25c57757":"code","8d69994c":"code","97d901e6":"code","a280c5c6":"code","ce5a15fe":"code","f7a8424e":"code","13679dbd":"code","c98d33bc":"code","448c9b42":"code","ec52b2a4":"code","ca16a543":"code","f094b91f":"code","419d7a6a":"code","683d5a3f":"code","9ec29129":"code","81372ea5":"code","50aeaa02":"markdown","83341733":"markdown","3742f1c0":"markdown","bdb490ad":"markdown","c6e0d5a7":"markdown","968fd6b8":"markdown","bff092df":"markdown","29e0f66a":"markdown","3ed38904":"markdown","2c2c90d2":"markdown","94da52b7":"markdown","3a7e0779":"markdown","13fb62f1":"markdown","85e94f29":"markdown","b4ff5475":"markdown","843765da":"markdown","257f21db":"markdown","d29849dd":"markdown","f9051f2e":"markdown","b8980e83":"markdown","2c34e223":"markdown","1319cfae":"markdown","c7d04103":"markdown","e033a75b":"markdown","7ccf50be":"markdown","49b7832a":"markdown","e9d3b78d":"markdown","db8b7064":"markdown","598be303":"markdown","d71d96fb":"markdown","d7952028":"markdown","4c45bf6b":"markdown","2b8f8cbc":"markdown","bc795f00":"markdown","22bd7a41":"markdown","608bded0":"markdown","4e90562e":"markdown","d283f23b":"markdown","e99428db":"markdown","3bb2ca01":"markdown","bd51a5de":"markdown","5fc7e7de":"markdown","61a8fd43":"markdown","6d9017e8":"markdown","e25135fe":"markdown","6829bbf7":"markdown","a1fd6a54":"markdown","56c03c06":"markdown","ddfab543":"markdown","68f46d04":"markdown","4f713924":"markdown","04a195a7":"markdown","ced96d57":"markdown","6b8f4278":"markdown","9be2ee2a":"markdown","391614f5":"markdown","a2f3966e":"markdown","f2752abe":"markdown","daf18725":"markdown","60ebdad1":"markdown","73f9cd42":"markdown","7a730f3e":"markdown","8b05c722":"markdown","55b0ef47":"markdown","b2575964":"markdown","b939bbd3":"markdown","728b120c":"markdown","18187554":"markdown","2bb4f99c":"markdown","b9759e05":"markdown"},"source":{"04d58771":"import numpy as np\nfrom numpy import array\nfrom numpy import argmax\nimport pandas as pd\nfrom sklearn.preprocessing import scale, MinMaxScaler\nfrom keras.utils import to_categorical\n\nimport seaborn as sb\nimport matplotlib.pyplot as plt\nimport pandas_profiling\nfrom PIL import Image\n\nimport keras.backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras.optimizers import Adam, SGD\n\nfrom sklearn.metrics import accuracy_score, log_loss, average_precision_score, f1_score, plot_confusion_matrix, plot_precision_recall_curve, plot_roc_curve\nfrom sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier\nfrom sklearn.svm import SVC, LinearSVC, NuSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier","cc5f51ae":"data_frame_train = pd.read_csv('..\/input\/titanic\/train.csv')\ndata_frame_test = pd.read_csv('..\/input\/titanic\/test.csv')","bb5a11f1":"data_frame_train.head()","229250c9":"print('Total row col of train data : ',data_frame_train.shape[0],' ',data_frame_train.shape[1])\nprint('Total row col of test data  : ',data_frame_test.shape[0],' ',data_frame_test.shape[1])","72bc3597":"data_frame_train.isnull().sum()","04543026":"data_frame_test.isnull().sum()","b55a8f19":"data_frame_train.info()","b5694062":"data_frame_train.describe()","7ed79e7b":"data_frame_train['Age'] = data_frame_train['Age'].fillna(data_frame_train['Age'].median())\ndata_frame_test['Age'] = data_frame_test['Age'].fillna(data_frame_test['Age'].median())\ndata_frame_test['Fare'] = data_frame_test['Fare'].fillna(0)","83994c0d":"data_frame_train.Embarked.value_counts()","99b52988":"data_frame_train['Embarked'] = data_frame_train['Embarked'].fillna('S')\ndata_frame_test['Embarked'] = data_frame_test['Embarked'].fillna('S')","7fa697d6":"data_frame_train['Sex'] = pd.factorize(data_frame_train.Sex)[0]\ndata_frame_test['Sex'] = pd.factorize(data_frame_test.Sex)[0]\n\ndata_frame_train['Embarked'] = pd.factorize(data_frame_train.Embarked)[0]\ndata_frame_test['Embarked'] = pd.factorize(data_frame_test.Embarked)[0]","f453d116":"print('After converting categorical Sex data to numerical      :',data_frame_train.Sex.unique())\nprint('After converting categorical Embarked data to numerical :',data_frame_train.Embarked.unique())","6fdf4adc":"data_frame_train.head()","f1a96682":"data_frame_train.isnull().sum()","b458d3f7":"data_frame_train['Cabin'].describe()","b28abcbb":"data_frame_train['Ticket'].describe()","b8bbe06b":"print('Total unique feature in Cabin column :',data_frame_train['Cabin'].nunique())\nprint('Total unique feature in Ticket column :',data_frame_train['Ticket'].nunique())","6547d5f0":"data_frame_train = data_frame_train.drop(['Ticket'], axis=1)\ndata_frame_train = data_frame_train.drop(['Cabin'], axis=1)\ndata_frame_train = data_frame_train.drop(['Name'], axis=1)","9684dfc3":"data_frame_test = data_frame_test.drop(['Ticket'], axis=1)\ndata_frame_test = data_frame_test.drop(['Cabin'], axis=1)\ndata_frame_test = data_frame_test.drop(['Name'], axis=1)","e2477f34":"data_frame_train.info()","dae6989d":"data_frame_test['Age'] = data_frame_test['Age'].astype(int)\ndata_frame_test['Fare'] = data_frame_test['Fare'].astype(int)\n\ndata_frame_train['Age'] = data_frame_train['Age'].astype(int)\ndata_frame_train['Fare'] = data_frame_train['Fare'].astype(int)","422c837b":"data_frame_train.info()","7638e707":"sb.set(rc={'figure.figsize':(10,10)})\nsb.heatmap(data_frame_train.corr(), annot = True)","e2df72e9":"data_frame_test.head()","730da930":"data_frame_train.head()","9b5a201b":"sb.set_style(\"whitegrid\");\nsb.pairplot(data_frame_train, hue=\"Survived\")\n\nimage = Image.open('..\/input\/images\/Coorelation_1.png')\nimage","95775273":"plt.figure(figsize=(10,8), dpi= 80)\nsb.kdeplot(data_frame_train.loc[data_frame_train['Sex'] == 0, \"Survived\"], shade=True, color=\"g\", label=\"Male=0\")\nsb.kdeplot(data_frame_train.loc[data_frame_train['Sex'] == 1, \"Survived\"], shade=True, color=\"deeppink\", label=\"Female=1\")\n# Decoration\nplt.title('Survived', fontsize=22)\nplt.legend()\nplt.show()\nimage = Image.open('..\/input\/images\/kde.png')\nimage","e7f5f570":"plt.figure(figsize=(15,15), dpi= 80)\nsb.factorplot('Survived', 'Age', data=data_frame_train, hue='Sex')\nplt.show()","49449ac5":"plt.figure(figsize=(10,8), dpi= 80)\nsb.barplot('Survived','Age',data=data_frame_train,hue='Sex')\nplt.show()","1910e66f":"sb.factorplot('Survived',data=data_frame_train,kind='count',hue='Sex')\nplt.show()","01fcabce":"sb.factorplot(\"Pclass\", \"Survived\", hue = \"Sex\", data = data_frame_train)\nplt.show()","5914c8e9":"pd.crosstab([data_frame_train[\"Sex\"], data_frame_train[\"Survived\"]], data_frame_train[\"Pclass\"], \n            margins = True).style.background_gradient(cmap = \"summer_r\")","e42e1949":"sb.barplot(x = \"Sex\", y = \"Survived\", hue = \"Pclass\", data = data_frame_train)\nplt.show()","b4756b8d":"sb.barplot(x = \"Embarked\", y = \"Survived\", hue = \"Pclass\", data = data_frame_train)\nplt.show()","89eb4c09":"grid = sb.FacetGrid(data_frame_train, col='Survived', row='Pclass', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend();\n#plt.xkcd()","e51b3a17":"data_frame_train.head()","c931116f":"data_frame_test.head()","917c6404":"drop_feature = ['PassengerId','Survived']\nx_train = data_frame_train.drop(drop_feature, axis=1)\ny_train = data_frame_train['Survived']\nx_test = data_frame_test.drop('PassengerId', axis=1)","1e312be0":"x_train.head()","3034f17f":"x_test.head()","073af90f":"scaler = MinMaxScaler()\nx_train_scaled = scaler.fit_transform(x_train)\nx_test_scaled = scaler.fit_transform(x_test)","9798104c":"x_train_scaled[0] # Age 80 SibSp 8 Fare 511.999","13a14f99":"clf = LogisticRegression()\n\nclf.fit(x_train_scaled, y_train)\ny_pred_log_reg = clf.predict(x_test_scaled)\nacc_log_reg = round( clf.score(x_train_scaled, y_train) * 100, 2)\nprint (str(acc_log_reg) + ' %')","d50a3178":"clf = SVC()\n\nclf.fit(x_train_scaled, y_train)\ny_pred_svc = clf.predict(x_test_scaled)\nacc_svc = round(clf.score(x_train_scaled, y_train) * 100, 2)\nprint (str(acc_svc) + '%')","28e76724":"clf = LinearSVC()\n\nclf.fit(x_train_scaled, y_train)\ny_pred_linear_svc = clf.predict(x_test_scaled)\nacc_linear_svc = round(clf.score(x_train_scaled, y_train) * 100, 2)\nprint (str(acc_linear_svc) + '%')","d767b2d9":"from sklearn.linear_model import SGDClassifier\n# sgd = linear_model.SGDClassifier()\nsgd = SGDClassifier()\n\nsgd.fit(x_train_scaled, y_train)\nY_pred = sgd.predict(x_test_scaled)\nsgd.score(x_train_scaled, y_train)\nacc_sgd = round(sgd.score(x_train_scaled, y_train) * 100, 2)\nprint(str(acc_sgd)+'%')","8e728a60":"clf = RandomForestClassifier()\n\nclf.fit(x_train_scaled, y_train)\nY_prediction_randomforest = clf.predict(x_test_scaled)\nclf.score(x_train_scaled, y_train)\nacc_random_forest = round(clf.score(x_train_scaled, y_train) * 100, 2)\nprint(str(acc_random_forest) + '%')","81bc5547":"clf = KNeighborsClassifier()\n\nclf.fit(x_train_scaled, y_train)\ny_pred_knn = clf.predict(x_test_scaled)\nacc_knn = round(clf.score(x_train_scaled, y_train) * 100, 2)\nprint (str(acc_knn)+'%')","ca762cb0":"clf = DecisionTreeClassifier()\n\nclf.fit(x_train_scaled, y_train)\ny_pred_decision_tree = clf.predict(x_test_scaled)\nacc_decision_tree = round(clf.score(x_train_scaled, y_train) * 100, 2)\nprint (str(acc_decision_tree) + '%')","9ce6283c":"clf = GaussianNB()\n\nclf.fit(x_train_scaled, y_train)\ny_pred_gnb = clf.predict(x_test_scaled)\nacc_gnb = round(clf.score(x_train_scaled, y_train) * 100, 2)\nprint (str(acc_gnb) + '%')","a3eadf1f":"clf = DecisionTreeClassifier()\n\nclassifier = clf.fit(x_train_scaled, y_train)\nY_prediction_randomforest = clf.predict(x_train_scaled)\nacc_random_forest = round(clf.score(x_train_scaled, y_train) * 100, 2)\nprint(str(acc_random_forest) + '%')\n\nclass_names = ['Survived', 'Not Survived']\ntitle = 'Confusion Matrix'\nnp.set_printoptions(precision=3)\n\ndisp = plot_confusion_matrix(classifier, x_train_scaled, y_train, display_labels=class_names, cmap=plt.cm.Blues)\nplt.grid(False)\ndisp.ax_.set_title(title)\nprint(title)\nprint(disp.confusion_matrix)\nplt.show()","dbe72c45":"model = Sequential()\nmodel.add(Dense(4, input_shape=(7,), activation='relu'))\nmodel.add(Dense(1,  activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer=SGD(lr=.001), metrics=['accuracy'])\nmodel.summary()\nhistory = model.fit(x_train_scaled, y_train, epochs=1000) # y_test_bin\n#history = model.fit(x_train, y_train_bin, epochs=1000, validation_data=[x_test, y_test_bin])\n#result = model.evaluate(x_test, y_test_bin)","215c6fa6":"model = Sequential()\nmodel.add(Dense(1, input_shape=(7,), activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer=SGD(lr=.001), metrics=['accuracy'])\nmodel.summary()\nhistory = model.fit(x_train_scaled, y_train, epochs=1000) # y_test_bin\n#result = model.evaluate(x_test, y_test_bin)","7284200b":"classifiers_set_1 = [\n    LinearSVC(),\n    SGDClassifier(),\n    ]\n\nfor clf in classifiers_set_1:\n\n  name = clf.__class__.__name__\n  clf.fit(x_train_scaled, y_train)\n\n  y_pred_decision_tree = clf.predict(x_train_scaled)\n  acc = accuracy_score(y_train, y_pred_decision_tree)\n  print('{:<25}'.format(name),\": \", \" Accuracy: {:.2%}\".format(acc))","2c17821a":"classifiers_set_2 = [\n    LogisticRegression(),\n    SVC(kernel=\"rbf\", C=0.025, probability=True),\n    NuSVC(probability=True),\n    RandomForestClassifier(),\n    KNeighborsClassifier(),\n    DecisionTreeClassifier(),\n    GaussianNB(),\n    ]\n\nacc_loss = pd.DataFrame(columns=[\"Classifier\", \"Accuracy\", \"Log Loss\"])\n\nprint('Accuracy and Loss for Train data in different Classifier : \\n')\n\nfor clf in classifiers_set_2:\n  name = clf.__class__.__name__\n  clf.fit(x_train_scaled, y_train)\n\n  y_pred_decision_tree = clf.predict(x_train_scaled)\n  acc = accuracy_score(y_train, y_pred_decision_tree)\n\n  y_pred_decision_tree = clf.predict_proba(x_train_scaled)\n  loss = log_loss(y_train, y_pred_decision_tree)\n\n  print('{:<25}'.format(name),\": \", \" Accuracy: {:.2%}\".format(acc),\" Loss: {:.1}\".format(loss))\n  \n  temp = pd.DataFrame([[name, acc*100, loss]], columns=[\"Classifier\", \"Accuracy\", \"Log Loss\"])\n  acc_loss = acc_loss.append(temp)\n\n# For ANN\nann_res = model.evaluate(x_train_scaled, y_train,steps=None)\nprint('{:<25}'.format('ANN'),\": \", \" Accuracy: {:.2%}\".format(ann_res[1]),\" Loss: {:.1}\".format(ann_res[0]))\ntemp = pd.DataFrame([['ANN', ann_res[1]*100, ann_res[0]]], columns=[\"Classifier\", \"Accuracy\", \"Log Loss\"])\nacc_loss = acc_loss.append(temp)","57f37037":"acc_loss","b5877f4c":"plt.figure(figsize=(8,5), dpi= 80)\nsb.barplot(x='Accuracy', y='Classifier', data=acc_loss, color=\"g\")\n\nplt.xlabel('Accuracy %')\nplt.title('Classifier Accuracy of Train')\nplt.show()","8b50aec8":"plt.figure(figsize=(8,5), dpi= 80)\nsb.barplot(x='Log Loss', y='Classifier', data=acc_loss, color=\"r\")\n\nplt.xlabel('Log Loss')\nplt.title('Classifier Log Loss of Train')\nplt.show()","25c57757":"pred_res = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\ny_test = pred_res['Survived']\nall_id = pred_res['PassengerId']","8d69994c":"classifiers_set_3 = [\n    LogisticRegression(),\n    SVC(kernel=\"rbf\", C=0.025, probability=True),\n    NuSVC(probability=True),\n    RandomForestClassifier(),\n    KNeighborsClassifier(),\n    DecisionTreeClassifier(),\n    GaussianNB(),\n    ]\n\nacc_loss_2 = pd.DataFrame(columns=[\"Classifier\", \"Accuracy\", \"Log Loss\"])\nprint('Accuracy and Loss for Test data in different Classifier : \\n')\nfor clf in classifiers_set_3:\n  name = clf.__class__.__name__\n  clf.fit(x_train_scaled, y_train)\n\n  y_pred_decision_tree = clf.predict(x_test_scaled)\n  acc = accuracy_score(y_test, y_pred_decision_tree)\n\n  y_pred_decision_tree = clf.predict_proba(x_test_scaled)\n  loss = log_loss(y_test, y_pred_decision_tree)\n\n  print('{:<25}'.format(name),\": \", \" Accuracy: {:.2%}\".format(acc),\" Loss: {:.1}\".format(loss))\n  \n  temp = pd.DataFrame([[name, acc*100, loss]], columns=[\"Classifier\", \"Accuracy\", \"Log Loss\"])\n  acc_loss_2 = acc_loss_2.append(temp)\n\n# For ANN\n#ann_res = model.evaluate(x_train_scaled, y_train,steps=None)\n#temp = pd.DataFrame([['ANN', ann_res[1]*100, ann_res[0]]], columns=[\"Classifier\", \"Accuracy\", \"Log Loss\"])\n#acc_loss = acc_loss.append(temp)","97d901e6":"plt.figure(figsize=(8,5), dpi= 80)\nsb.barplot(x='Accuracy', y='Classifier', data=acc_loss_2, color=\"g\")\n\nplt.xlabel('Accuracy %')\nplt.title('Classifier Accuracy of Test')\nplt.show()","a280c5c6":"plt.figure(figsize=(8,5), dpi= 80)\nsb.barplot(x='Log Loss', y='Classifier', data=acc_loss_2, color=\"r\")\n\nplt.xlabel('Log Loss')\nplt.title('Classifier Log Loss of Test')\nplt.show()","ce5a15fe":"clf = SVC(kernel=\"rbf\", C=0.025, probability=True)\n\nclassifier = clf.fit(x_train_scaled, y_train)\ny_pred_decision_tree = clf.predict(x_test_scaled)\nacc_decision_tree = accuracy_score(y_test, y_pred_decision_tree)\nprint (\"Accuracy : {:.3%}\".format(acc_decision_tree))\n\nclass_names = ['Survived', 'Not Survived']\ntitle = 'Confusion Matrix'\n#np.set_printoptions(precision=2)\n\ndisp = plot_confusion_matrix(classifier, x_test_scaled, y_test, display_labels=class_names, cmap=plt.cm.Blues)\nplt.grid(False)\ndisp.ax_.set_title(title)\nprint(title)\nprint(disp.confusion_matrix)","f7a8424e":"clf = SVC(kernel=\"rbf\", C=0.025, probability=True)\n\nclassifier = clf.fit(x_train_scaled, y_train)\ny_pred_decision_tree = clf.predict(x_test_scaled)\nacc_decision_tree = accuracy_score(y_test, y_pred_decision_tree)\nprint (\"Accuracy : {:.3%}\".format(acc_decision_tree))","13679dbd":"#cd \/content\/drive\/My Drive\/Google Colab\/Data_Science\/Titanic","c98d33bc":"import pickle\n# now you can save it to a file\nwith open('..\/input\/titanic-99\/Titanic_99.pkl', 'wb') as f:\n    pickle.dump(clf, f)","448c9b42":"# and later you can load it\nwith open('..\/input\/titanic-99\/Titanic_99.pkl', 'rb') as f:\n    save_model = pickle.load(f)","ec52b2a4":"save_model.predict(x_test_scaled)","ca16a543":"print('F1 macro    : ',f1_score(y_test, y_pred_decision_tree, average='macro'))\nprint('F1 micro    : ',f1_score(y_test, y_pred_decision_tree, average='micro'))\nprint('F1 weighted : ',f1_score(y_test, y_pred_decision_tree, average='weighted'))\nprint('F1 None     : ',f1_score(y_test, y_pred_decision_tree, average=None))","f094b91f":"average_precision = average_precision_score(y_test, y_pred_decision_tree)\nprint('Average precision-recall score: {0:0.2f}'.format(average_precision))","419d7a6a":"disp = plot_precision_recall_curve(classifier, x_test_scaled, y_test)\ndisp.ax_.set_title('2-class Precision-Recall curve: ' 'AP={0:0.2f}'.format(average_precision))","683d5a3f":"Final_Result = pd.DataFrame(list(zip(all_id, y_pred_decision_tree)),columns =['PassengerId', 'Survived']) ","9ec29129":"Final_Result.head()","81372ea5":"Final_Result.to_csv('..\/input\/output\/Final_Result.csv',index=False)","50aeaa02":"#\ud83d\udd2a Split the data as train and test for our algorithm...","83341733":"Logistic regression is a classification process where the threshold value are involved. Logistic regression used sigmoid function.\n![Imgur](https:\/\/i.imgur.com\/y35zaKt.jpg) \n![Imgur](https:\/\/i.imgur.com\/qun2DpM.jpg)","3742f1c0":"#\ud83d\udcdd Feature selection...","bdb490ad":"<h3> 1. Coorelation with entire feature of train data.","c6e0d5a7":"# &#128209; Titanic Problem:\nIn this titanic problem we need to predict the passenger is survived or not.\n\n* LINK : https:\/\/www.kaggle.com\/c\/titanic\/","968fd6b8":"<h3>2. v2 all algorithm including ANN.","bff092df":"<h3>4. Visulization of loss with test data.","29e0f66a":"Random Forest Classifier classify classes based on majority voting system.\n![Imgur](https:\/\/i.imgur.com\/trUUiO8.jpg)","3ed38904":"#<h2>\ud83d\udc51 Confusion matrix of DecisionTreeClassifier which gives us best accuracy...","2c2c90d2":"<h3> 6. Sex, Survived.","94da52b7":"Gaussian NB follow this equation to classify different class.\n![Imgur](https:\/\/i.imgur.com\/z1TLOj3.jpg)","3a7e0779":"#\ud83d\udc89 Import train & test data...","13fb62f1":"K-nearest neighbors algorithm classify classes using calculating distance with selecting neighbors.\n![Imgur](https:\/\/i.imgur.com\/E5Qnexh.png)","85e94f29":"<h3>5. Visualization of Loss of different classifier.","b4ff5475":"See some animation , Just enjoy.\n<a href=\"https:\/\/imgur.com\/UKk7AGb\"><img src=\"https:\/\/i.imgur.com\/UKk7AGb.gif\" title=\"source: imgur.com\" \/><\/a><br>\nThis is the Human neuron architecture from where modern Artificial Neural Network get the concept.\n![Imgur](https:\/\/i.imgur.com\/Z8bbVYd.jpg)<br>\nThis the conversion form of human neuron, where designed a neuron in artificial way. Or you can say mimic the human neuron.\n![Imgur](https:\/\/i.imgur.com\/F6XO0Mx.jpg)<br>\nThis is the basic ANN building block, where hidden layer input and output are shown.\n![Imgur](https:\/\/i.imgur.com\/WWx45SV.jpg)","843765da":"<h3> 5. Age, Sex.","257f21db":"<h3>1. v1","d29849dd":"#\ud83d\udcc4 Analyze train data set...","f9051f2e":"Stochastic Gradient Descent classifier worked with finding the global minimum.\n![Imgur](https:\/\/i.imgur.com\/PlyGL3y.jpg)","b8980e83":"#<h2>\u2699  Lets Check the test data set with those algorithms...","2c34e223":"<h3>3. Visulization of accuracy with test data.","1319cfae":"#<h2>\ud83d\udc95 Comparison between different ML algorithm...","c7d04103":"#<h2>\ud83d\udcc0 Save the model...","e033a75b":"<h3> 11. More visualization of generalized the coorelation between Sex, Survived, Pclass.","7ccf50be":"<h2>5. Cabin and Ticket data exploit :\n<ul>\n<li> Cabin and Ticket is categorical data.\n<li> Cabin column has 147 uniq value and Ticket has 681 uniq data.\n<li> Which is hard to convert in numerical value (except strong statistical calculation).","49b7832a":"<h2> Droping Ticket, Cabin, Name column.","e9d3b78d":"<h3>1. Version one of ANN which has two layer.","db8b7064":"In Decision Tree Classifier to classify class according with some particular conditions.\n![Imgur](https:\/\/i.imgur.com\/wFSXq7M.jpg)","598be303":"<h3> 3. Sex and Survived.","d71d96fb":"Support Vector Machine classify data based on Maximum Margin and Support vectors.\n![Imgur](https:\/\/i.imgur.com\/BYVuAUG.jpg)","d7952028":"<h2>4. Categorical convertion :\n<ul>\n<li>Value of Sex convert to male = 0, female = 1.\n<li>Value of Embarked S = 0, C = 1, Q = 2","4c45bf6b":"# &#127865; Purpose Of This Notebook:\n* Learn uses basic sk-learn classification algorithm.\n* Learn bacic ANN coding with keras api.\n* This NoteBook shows you to uses of various sklearn regression models and compare between them.\n* Also shows some different performance on same algorithm on train and test set.\n* For better view visit : https:\/\/github.com\/shahinvx\/Titanic\/blob\/master\/Titanic_Kaggle.ipynb ","2b8f8cbc":"<h2>6. Converting float64 Age, float64 Fare to int64.","bc795f00":"<h3>4. SGDClassifier","22bd7a41":"<h3>1. Importing submission data of test.","608bded0":"#<h2>\ud83d\udcbf Load the model...","4e90562e":"<h3>4. Visualization of Accuracy of different classifier.","d283f23b":"<h2> After convertion of data its look like as below.","e99428db":"<h3>* Here we see that SVC and NuSVC are performed better then DecisionTreeClassifier and RandomForestClassifier.","3bb2ca01":"# \ud83d\udd17 Important library and function...","bd51a5de":"<h3>1. LogisticRegression","5fc7e7de":"#<h2>\ud83d\udc9a Precision-Recall score of the model...","61a8fd43":"<h3> 10. Visualization of the coorelation between Sex, Survived, Embarked.","6d9017e8":"<h3> 7. Sex, Survived and Pclass.","e25135fe":"<h3>2. SVC","6829bbf7":"<h3>8. GaussianNB","a1fd6a54":"<h2>2. NULL value of Age column fill with it's median value:","56c03c06":"# \ud83d\udcd1 Data Pre-processing...","ddfab543":"<h3> 2. Coorelation with entire feature of train data in seaborn.","68f46d04":"#<h2>\ud83e\udde4 Lets try Artificial Neural Network for our training...","4f713924":"#\ud83d\udd0c Train Titanic data in different ML algorithms...","04a195a7":"#<h2>\ud83c\udfc1 Save our prediction as .csv format to submit the prediction to competition...","ced96d57":"<h3>3. Accuracy and Loss according to Classifiers.","6b8f4278":"<h2>1. NULL or Missing data checking:<h2>\n<ul>\n<li>Age column has 177 null data in train and 86 in test data.\n<li>Cabin column has 687 null data in train and 327 in test data.\n<li>Embarked has 2 missing value.<\/li>\n<\/ul>","9be2ee2a":"#<h2>\ud83d\udc97 F1 score of the model...","391614f5":"#\ud83d\udcc8 Data Visualization...","a2f3966e":"<h3>2. Run those alogorithm with test data and check the accuracy and loss difference.","f2752abe":"<h2>3. Missing value of Embarked column fill with 'S' because the frequency of 'S' is more higher then other two 'C', 'Q':","daf18725":"<h3>5. RandomForestClassifier","60ebdad1":"#<h2>\ud83d\udda8 Normalize the data with minmax scaler...","73f9cd42":"#<h2>\ud83d\udc9c Precision-Recall Curve of the model...","7a730f3e":"#<h2>\ud83d\udc51 Confusion matrix of SVC which gives us best accuracy...","8b05c722":"<h3>7. DecisionTreeClassifier","55b0ef47":"<h3>6. KNeighborsClassifier","b2575964":"<h3>2. Version two of ANN which has one layer.","b939bbd3":"<h3>3. LinearSVC","728b120c":"<h3> 9. Visualization of generalized the coorelation between Sex, Survived, Pclass.","18187554":"Titanic Dataset Download link:\n* Train : \"https:\/\/www.kaggle.com\/c\/titanic\/download\/train.csv\"\n* Test  : \"https:\/\/www.kaggle.com\/c\/titanic\/download\/test.csv\"","2bb4f99c":"<h3> 8. More generalize the coorelation between Sex, Survived, Pclass.","b9759e05":"<h3> 4. Age, Sex, Survived."}}