{"cell_type":{"6ccfe06a":"code","8c720cb7":"code","cc88b9b4":"code","a4eb86dc":"code","df4968ba":"code","4af84069":"code","3e6b3a79":"code","8f466978":"code","f333f27d":"code","7eadeb51":"code","07e69959":"code","de286529":"code","b5fce28d":"code","89767771":"code","735dc08a":"code","4604cc1e":"code","30051aa6":"code","bbf1e97b":"code","970943e0":"markdown","a0520bf1":"markdown","35901d5c":"markdown","0c8e738c":"markdown","1e52d53f":"markdown","4800f384":"markdown","1a0f7d2e":"markdown","cb2f56b9":"markdown","fe30fa27":"markdown","9da75a3e":"markdown"},"source":{"6ccfe06a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport joblib\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier","8c720cb7":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\nprint(train.shape) ","cc88b9b4":"train.head()","a4eb86dc":"train.isnull().sum().to_frame().T","df4968ba":"(train.Survived.value_counts() \/ len(train)).to_frame()","4af84069":"y_train = train.Survived.values\ntrain.drop(['Survived'], axis=1, inplace=True)\ntrain.head()","3e6b3a79":"train['FamSize'] = train['SibSp'] + train['Parch']\n\ndef set_deck(cabin):\n    if str(cabin) == 'nan':\n        return 'Missing'\n    return cabin[0]\n\ntrain['Deck'] = train['Cabin'].map(set_deck)\n\ntrain.head()","8f466978":"num_features = ['Age', 'FamSize', 'Fare']\ncat_features = ['Sex', 'Pclass', 'Deck', 'Embarked']\n\nfeatures = num_features + cat_features\n\n\nnum_transformer = Pipeline(\n    steps = [\n        ('imputer', SimpleImputer(strategy='mean')),\n        ('scaler', StandardScaler())  \n    ]\n)\n\n\ncat_transformer = Pipeline(\n    steps = [\n        ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),\n        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n    ]\n)\n\n\npreprocessor = ColumnTransformer(\n    transformers = [\n        ('num', num_transformer, num_features),\n        ('cat', cat_transformer, cat_features)\n    ]\n)","f333f27d":"preprocessor.fit(train)\n\nX_train = preprocessor.transform(train)\n\nprint(X_train.shape)\nprint(y_train.shape)","7eadeb51":"%%time \n\nlr_clf = LogisticRegression(max_iter=1000, solver='saga', penalty='elasticnet')\n\nlr_parameters = {\n    'l1_ratio':[0, 0.5, 1],\n    'C': [0.01, 0.1, 1, 10]\n}\n\nlr_grid = GridSearchCV(lr_clf, lr_parameters, cv=10, refit='True', n_jobs=-1, verbose=10, scoring='accuracy')\nlr_grid.fit(X_train, y_train)\n\nlr_model = lr_grid.best_estimator_\n\nprint('Best Parameters:', lr_grid.best_params_)\nprint('Best CV Score:  ', lr_grid.best_score_)\nprint('Training Acc:   ', lr_model.score(X_train, y_train))","07e69959":"lr_summary = pd.DataFrame(lr_grid.cv_results_['params'])\nlr_summary['cv_score'] = lr_grid.cv_results_['mean_test_score']\n\nfor r in lr_parameters['l1_ratio']:\n    temp = lr_summary.query(f'l1_ratio == {r}')\n    plt.plot(temp.C, temp.cv_score, label=r)\nplt.xscale('log')\nplt.ylim([0.75, 0.82])\nplt.xlabel('Regularization Parameter (C)')\nplt.ylabel('CV Score')\nplt.legend(title='L1 Ratio', loc='lower right')\nplt.grid()\nplt.show()\n\nprint(lr_summary.to_string(index=False))","de286529":"%%time\n \ndt_clf = DecisionTreeClassifier(random_state=1)\n\ndt_parameters = {\n    'max_depth': [2, 4, 6, 8, 10, 12, 14, 16],\n    'min_samples_leaf': [2, 4, 8, 16]\n}\n\ndt_grid = GridSearchCV(dt_clf, dt_parameters, cv=10, refit='True', n_jobs=-1, verbose=0, scoring='accuracy')\ndt_grid.fit(X_train, y_train)\n\ndt_model = dt_grid.best_estimator_\n\nprint('Best Parameters:', dt_grid.best_params_)\nprint('Best CV Score:  ', dt_grid.best_score_)\nprint('Training Acc:   ', dt_model.score(X_train, y_train))","b5fce28d":"dt_summary = pd.DataFrame(dt_grid.cv_results_['params'])\ndt_summary['cv_score'] = dt_grid.cv_results_['mean_test_score']\n\nfor ms in dt_parameters['min_samples_leaf']:\n    temp = dt_summary.query(f'min_samples_leaf == {ms}')\n    plt.plot(temp.max_depth, temp.cv_score, label=ms)\nplt.xlabel('Maximum Depth')\nplt.ylabel('CV Score')\nplt.legend(title='Min Samples')\nplt.grid()\nplt.show()\n\nprint(dt_summary.to_string(index=False))","89767771":"%%time \n\nrf_clf = RandomForestClassifier(random_state=1, n_estimators=50)\n\nrf_parameters = {\n    'max_depth': [4, 8, 16, 20, 24, 28, 32],\n    'min_samples_leaf': [1, 2, 4]\n}\n\nrf_grid = GridSearchCV(rf_clf, rf_parameters, cv=10, refit='True', n_jobs=-1, verbose=0, scoring='accuracy')\nrf_grid.fit(X_train, y_train)\n\nrf_model = rf_grid.best_estimator_\n\nprint('Best Parameters:', rf_grid.best_params_)\nprint('Best CV Score:  ', rf_grid.best_score_)\nprint('Training Acc:   ', rf_model.score(X_train, y_train))","735dc08a":"rf_summary = pd.DataFrame(rf_grid.cv_results_['params'])\nrf_summary['cv_score'] = rf_grid.cv_results_['mean_test_score']\n\nfor ms in rf_parameters['min_samples_leaf']:\n    temp = rf_summary.query(f'min_samples_leaf == {ms}')\n    plt.plot(temp.max_depth, temp.cv_score, label=ms)\nplt.xlabel('Maximum Depth')\nplt.ylabel('CV Score')\nplt.legend(title='Min Samples')\nplt.grid()\nplt.show()\n\nprint(rf_summary.to_string(index=False))","4604cc1e":"print(rf_grid.best_params_)","30051aa6":"final_model = RandomForestClassifier(random_state=1, n_estimators=50, max_depth=20, min_samples_leaf=4)\nfinal_model.fit(X_train, y_train)\n\nprint(final_model.score(X_train, y_train))","bbf1e97b":"joblib.dump(preprocessor, 'titanic_preprocessor_1.joblib')\njoblib.dump(final_model, 'titanic_model_1.joblib')\nprint('Model written to file.')","970943e0":"# Import Statements","a0520bf1":"# Load Training Data","35901d5c":"## Logistic Regression","0c8e738c":"# Model Selection","1e52d53f":"# Check for Missing Values","4800f384":"# Preprocessing","1a0f7d2e":"## Random Forests","cb2f56b9":"## Decision Trees","fe30fa27":"# Save Pipeline and Model","9da75a3e":"# Check Label Distribution"}}