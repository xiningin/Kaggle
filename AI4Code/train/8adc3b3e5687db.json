{"cell_type":{"eb2fc3b6":"code","4a48db83":"code","95bcd99a":"code","b9ec3d76":"code","a43d937a":"code","f03abf05":"code","1d162176":"code","246a7b68":"code","e1342d13":"code","b69fcef2":"code","0586a1b4":"code","c90aa93c":"code","bd5e5047":"code","9b700941":"code","21e04944":"code","f9fe5f51":"code","1d08b168":"code","90c2688e":"code","c6408625":"code","dd087ba2":"code","c5cadb44":"code","d051fa21":"code","0476c028":"markdown","bca907b7":"markdown","77aa21c8":"markdown","db6e8832":"markdown","a0486305":"markdown","b471ec23":"markdown","2dc8c7ea":"markdown"},"source":{"eb2fc3b6":"import numpy as np\nimport pandas as pd\nimport random\nimport os\nimport string\nimport re\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.set_cmap('PiYG_r')\nsns.set_palette(\"PiYG_r\")\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import TweetTokenizer\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score\n\nimport warnings\nwarnings.filterwarnings('ignore')","4a48db83":"# Download stopwords\nnltk.download('stopwords')","95bcd99a":"def set_seed(seed = 42):\n    \"\"\"For reproducibility.\"\"\"\n    np.random.seed(seed)\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nset_seed()","b9ec3d76":"train_raw = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntest_raw = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\ntrain_raw.head()","a43d937a":"train_raw.shape, test_raw.shape","f03abf05":"train_raw.info()","1d162176":"# Look at the distribution of the target variable\nsns.countplot(train_raw.target)\nplt.title(\"#Disaster Tweets is Less Than Non-Disaster Tweets\")\nplt.show()","246a7b68":"train_raw.target.mean()","e1342d13":"# Randomly sample 5 disaster tweets from training data.\nsample_1 = train_raw[train_raw.target == 1].sample(n=5, random_state=42)\nfor i in range(5):\n    print(str(i+1),'.', sample_1.iloc[i].text, '\\n')","b69fcef2":"# Randomly sample 5 non-disaster tweets from training data.\nsample_0 = train_raw[train_raw.target == 0].sample(n=5, random_state=42)\nfor i in range(5):\n    print(str(i+1),'.', sample_0.iloc[i].text, '\\n')","0586a1b4":"text_raw = train_raw.text\ntarget_raw = train_raw.target.values\n\nX_train, X_test, y_train, y_test = train_test_split(text_raw, target_raw, test_size = 0.2, \n                                                    stratify = target_raw, random_state = 2020)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","c90aa93c":"def process_tweet(tweet):\n    \"\"\"Process tweet function.\n    Input:\n        tweet: a string containing a tweet\n    Output:\n        tweets_clean: a list of words (unigram) containing the processed tweet\n\n    \"\"\"\n    stemmer = PorterStemmer()\n    stopwords_english = stopwords.words('english')\n    tweet = re.sub(r'\\$\\w*', '', tweet)\n    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n    tweet = re.sub(r'https?:\\\/\\\/.*[\\r\\n]*', '', tweet)\n    tweet = re.sub(r'#', '', tweet)\n    \n    # tokenize tweets\n    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n                               reduce_len=True)\n    tweet_tokens = tokenizer.tokenize(tweet)\n\n    tweets_clean = []\n    for word in tweet_tokens:\n        if (word not in stopwords_english and  # remove stopwords\n                word not in string.punctuation):  # remove punctuation\n            stem_word = stemmer.stem(word)  # stemming word\n            tweets_clean.append(stem_word)\n\n    return tweets_clean\n\n\ndef build_freqs(tweets, ys):\n    \"\"\"Build frequencies.\n    Input:\n        tweets: a list of tweets\n        ys: an m x 1 array with the sentiment label of each tweet\n            (either 0 or 1)\n    Output:\n        freqs: a dictionary mapping each (word, disaster_status) pair to its\n        frequency\n    \"\"\"\n\n    yslist = np.squeeze(ys).tolist()\n\n    freqs = {}\n    for y, tweet in zip(yslist, tweets):\n        for word in process_tweet(tweet):\n            pair = (word, y)\n            freqs[pair] = freqs.get(pair, 0) + 1\n\n    return freqs\n","bd5e5047":"# create frequency dictionary\nfreqs = build_freqs(X_train, y_train)","9b700941":"len(freqs)","21e04944":"def train_naive_bayes(freqs, train_x, train_y):\n    '''\n    Input:\n        freqs: dictionary from (word, label) to how often the word appears\n        train_x: a list of tweets\n        train_y: a list of labels correponding to the tweets (0,1)\n    Output:\n        logprior: the log prior. \n        loglikelihood: the log likelihood of you Naive bayes equation. \n    '''\n    loglikelihood = {}\n    logprior = 0\n\n    vocab = set([pair[0] for pair in freqs.keys()])\n    V = len(vocab) # the number of unique words in the vocabulary\n\n    N_pos = N_neg = V_pos = V_neg = 0\n    for pair in freqs.keys():\n        # if the label is disaster (greater than zero)\n        if pair[1] > 0:\n            V_pos += 1\n            N_pos += freqs[pair]\n\n        # else, the label is not disaster\n        else:\n            V_neg += 1\n            N_neg += freqs[pair]\n\n    D = train_y.shape[0]  # the number of tweets\n\n    D_pos = sum(train_y)  # the number of disaster tweets\n    D_neg = D - D_pos  # # the number of non-disaster tweets\n\n    logprior = np.log(D_pos) - np.log(D_neg)\n\n    for word in vocab:\n        # get the disaster and non-disaster frequency of the word\n        freq_pos = freqs.get((word, 1), 0)\n        freq_neg = freqs.get((word, 0), 0)\n\n        # calculate the probability that each word is disaster, and non-disaster\n        p_w_pos = (freq_pos + 1) \/ (N_pos + V)\n        p_w_neg = (freq_neg + 1) \/ (N_neg + V)\n\n        # calculate the log likelihood of the word\n        loglikelihood[word] = np.log(p_w_pos) - np.log(p_w_neg)\n\n\n    return logprior, loglikelihood\n","f9fe5f51":"logprior, loglikelihood = train_naive_bayes(freqs, X_train, y_train)\nprint(logprior)\nprint(len(loglikelihood))","1d08b168":"def naive_bayes_predict(tweets, logprior, loglikelihood):\n    '''\n    Input:\n        tweet: a string\n        logprior: a number\n        loglikelihood: a dictionary of words mapping to numbers\n    Output:\n        p: the sum of all the logliklihoods of each word in the tweet (if found in the dictionary) + logprior (a number)\n\n    '''\n    res = []\n    for tweet in tweets:\n        # process the tweet to get a list of words\n        word_l = process_tweet(tweet)\n    \n        # initialize probability to zero\n        p = 0\n\n        # add the logprior\n        p += logprior\n\n        for word in word_l:\n\n            # check if the word exists in the loglikelihood dictionary\n            if word in loglikelihood:\n                # add the log likelihood of that word to the probability\n                p += loglikelihood[word]\n    \n        if p > 0:\n            res.append(1)\n        else:\n            res.append(0)\n\n\n    return res","90c2688e":"train_preds = naive_bayes_predict(X_train, logprior, loglikelihood)\ntest_preds = naive_bayes_predict(X_test, logprior, loglikelihood)","c6408625":"print('TRAINING SET: accuracy score: {}, F1 score: {}'.format(accuracy_score(y_train, train_preds), \n                                                              f1_score(y_train, train_preds)))\nprint('TEST SET: accuracy score: {}, F1 score: {}'.format(accuracy_score(y_test, test_preds), \n                                                         f1_score(y_test, test_preds)))","dd087ba2":"# Predict test data\ntest = test_raw.text\ntest_res = naive_bayes_predict(test, logprior, loglikelihood)","c5cadb44":"res = pd.DataFrame({'id': test_raw.id, 'target': test_res})\nres.head()","d051fa21":"# res.to_csv(\"naive_bayes_submission.csv\", header=True, index=False)","0476c028":"<span style=\"color:DODGERBLUE\">\n    1. There's handles and urls everywhere in tweets, which can be removed in the text preprocessing.<br \/>\n    2. Emoticons are useful for sentiment classifiction (positive or negative), however, they may not that indicative under 'disaster or non-disaster' senario. A negative emoticon can be used in both negative daily life tweets and disaster tweets.\n<\/span>","bca907b7":"Kaggle result: 0.78976","77aa21c8":"## Split training tweets to training set and test set","db6e8832":"<span style=\"color:DODGERBLUE\">\n    In this notebook, I will only use text data to make predictions.\n<\/span>","a0486305":"## Simple EDA","b471ec23":"## Text Preprocessing","2dc8c7ea":" <font color='DODGERBLUE'> The training set is not perfectly balanced.<\/font>"}}