{"cell_type":{"afd559fa":"code","982a6e91":"code","467a2025":"code","3576380e":"code","1c631969":"code","0d8157e5":"code","ce5f93c7":"code","af090f55":"code","c37a253f":"code","5f0c38d6":"code","80a287ed":"code","f562f617":"code","6707419c":"code","93d14fd3":"code","70b6d395":"code","7711e51e":"code","48c8ba41":"code","4ef9bb09":"code","416dda26":"code","d8319653":"code","23bc5699":"code","f275aacb":"code","87bdf63b":"code","709309b6":"code","7d83f19d":"code","4f28c16d":"code","abc62f8b":"code","7da6d32f":"code","2ab93b22":"code","c2b8838b":"code","bc34804e":"code","e39a3f8b":"code","ba4027e1":"code","16ea0863":"code","b7e7e8b0":"code","723bbb7e":"code","277fc5ad":"code","39f3bf68":"markdown","e7461946":"markdown","3187f4ac":"markdown","6c4c2347":"markdown","e0335e0e":"markdown","5dcff82d":"markdown","409d9bdd":"markdown","ce6f412f":"markdown","59dc3809":"markdown","3d4eb165":"markdown","4b6a3f85":"markdown","63a294c4":"markdown","d7cf63df":"markdown","0ac9047f":"markdown","cd136167":"markdown","0a56ba9e":"markdown","17c71afc":"markdown","48198333":"markdown","24e1a969":"markdown"},"source":{"afd559fa":"import tensorflow as tf\nfrom tensorflow import keras\n\nimport os\nimport tempfile\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nimport sklearn\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler,RobustScaler\n\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten\nfrom keras.optimizers import SGD, Adam\nfrom keras.utils import np_utils","982a6e91":"file = tf.keras.utils\nraw_df = pd.read_csv('https:\/\/storage.googleapis.com\/download.tensorflow.org\/data\/creditcard.csv')\nraw_df.head()","467a2025":"raw_df.describe()","3576380e":"raw_df.isnull().sum().max()","1c631969":"neg, pos = np.bincount(raw_df['Class'])\ntotal = neg + pos\nprint('Total Examples are {}\\n Positives are {:.2f}% of total\\n Negatives are {:.2f}% of total'.format(total, 100*pos\/total, 100*neg\/total))","0d8157e5":"amount_val = raw_df['Amount'].values\nfig = sns.distplot(amount_val)\nfig.set_title('Distribution of Transection Amount')\nfig.set_xlim([min(amount_val), max(amount_val)])","ce5f93c7":"time_val = raw_df['Time'].values\nfig = sns.distplot(time_val,  color = 'b')\nfig.set_title('Distribution of Transection Time')\nfig.set_xlim([min(time_val), max(time_val)])","af090f55":"# sklearn StandardScaler will set the mean to 0 and standard deviation to 1.  \n# RobustScaler is less prone to outliers.\nraw_df['scaled_amount'] = RobustScaler().fit_transform(raw_df['Amount'].values.reshape(-1,1))\nraw_df['scaled_time'] = RobustScaler().fit_transform(raw_df['Time'].values.reshape(-1,1))\n\nraw_df.drop(['Time','Amount'], axis = 1, inplace = True)","c37a253f":"fig = sns.distplot(raw_df['scaled_time'].values)\nfig.set_title('Distribution of Scaled Transection Time')","5f0c38d6":"fig = sns.distplot(raw_df['scaled_amount'].values,  color = 'b')\nfig.set_title('Distribution of Scaled Transection Amount')","80a287ed":"pos","f562f617":"new_df = raw_df.copy()\nnew_df = new_df.sample(frac = 1)\nnew_pos = new_df.loc[new_df['Class'] == 1]\nnew_neg = new_df.loc[new_df['Class'] == 0][:492]\nnorm_dis_df = pd.concat([new_pos, new_neg])\nnorm_df = norm_dis_df.sample(frac = 1, random_state=42)\nnorm_df.head()","6707419c":"print('Distribution of the Classes in the subsample dataset')\nprint(norm_df['Class'].value_counts()\/len(norm_df))\nsns.countplot('Class', data = norm_df)","93d14fd3":"plt.figure(figsize=(12,10))\nsns.heatmap(norm_df.corr(),cmap='coolwarm_r')","70b6d395":"fig, axes = plt.subplots(ncols=4, figsize=(20,4))\nsns.boxplot(x = 'Class', y = 'V1', data = norm_df, ax = axes[0])\naxes[0].set_title('V1 Negative Correlation with Class')\n\nsns.boxplot(x = 'Class', y = 'V3', data = norm_df, ax = axes[1])\naxes[1].set_title('V3 Negative Correlation with Class')\n\nsns.boxplot(x = 'Class', y = 'V5', data = norm_df, ax = axes[2])\naxes[2].set_title('V5 Negative Correlation with Class')\n\nsns.boxplot(x = 'Class', y = 'V6', data = norm_df, ax = axes[3])\naxes[3].set_title('V6 Negative Correlation with Class')","7711e51e":"fig, axes = plt.subplots(ncols=4, figsize=(20,4))\nsns.boxplot(x = 'Class', y = 'V7', data = norm_df, ax = axes[0])\naxes[0].set_title('V7 Negative Correlation with Class')\n\nsns.boxplot(x = 'Class', y = 'V9', data = norm_df, ax = axes[1])\naxes[1].set_title('V9 Negative Correlation with Class')\n\nsns.boxplot(x = 'Class', y = 'V10', data = norm_df, ax = axes[2])\naxes[2].set_title('V10 Negative Correlation with Class')\n\nsns.boxplot(x = 'Class', y = 'V12', data = norm_df, ax = axes[3])\naxes[3].set_title('V12 Negative Correlation with Class')","48c8ba41":"fig, axes = plt.subplots(ncols=4, figsize=(20,4))\nsns.boxplot(x = 'Class', y = 'V14', data = norm_df, ax = axes[0])\naxes[0].set_title('V14 Negative Correlation with Class')\n\nsns.boxplot(x = 'Class', y = 'V16', data = norm_df, ax = axes[1])\naxes[1].set_title('V16 Negative Correlation with Class')\n\nsns.boxplot(x = 'Class', y = 'V17', data = norm_df, ax = axes[2])\naxes[2].set_title('V17 Negative Correlation with Class')\n\nsns.boxplot(x = 'Class', y = 'V18', data = norm_df, ax = axes[3])\naxes[3].set_title('V18 Negative Correlation with Class')","4ef9bb09":"v1_pos = norm_df['V1'].loc[norm_df['Class'] == 1].values\nq25, q75 = np.percentile(v1_pos, 25), np.percentile(v1_pos, 75)\n\nv1_iqr = q75 - q25\n\nv1_lower, v1_upper = q25 - v1_iqr * 1.5, q75 + v1_iqr * 1.5\n\nnorm_df = norm_df.drop(norm_df[(norm_df['V1'] > v1_upper)|(norm_df['V1'] < v1_lower)].index)","416dda26":"v3_pos = norm_df['V3'].loc[norm_df['Class'] == 1].values\nq25, q75 = np.percentile(v3_pos, 25), np.percentile(v3_pos, 75)\n\nv3_iqr = q75 - q25\n\nv3_lower, v3_upper = q25 - v3_iqr * 1.5, q75 + v3_iqr * 1.5\n\nnorm_df = norm_df.drop(norm_df[(norm_df['V3'] > v3_upper)|(norm_df['V3'] < v3_lower)].index)","d8319653":"v5_pos = norm_df['V5'].loc[norm_df['Class'] == 1].values\nq25, q75 = np.percentile(v5_pos, 25), np.percentile(v5_pos, 75)\n\nv5_iqr = q75 - q25\n\nv5_lower, v5_upper = q25 - v5_iqr * 1.5, q75 + v5_iqr * 1.5\n\nnorm_df = norm_df.drop(norm_df[(norm_df['V5'] > v5_upper)|(norm_df['V5'] < v5_lower)].index)","23bc5699":"v6_pos = norm_df['V6'].loc[norm_df['Class'] == 1].values\nq25, q75 = np.percentile(v6_pos, 25), np.percentile(v6_pos, 75)\n\nv6_iqr = q75 - q25\n\nv6_lower, v6_upper = q25 - v6_iqr * 1.5, q75 + v6_iqr * 1.5\n\nnorm_df = norm_df.drop(norm_df[(norm_df['V6'] > v6_upper)|(norm_df['V6'] < v6_lower)].index)","f275aacb":"v7_pos = norm_df['V7'].loc[norm_df['Class'] == 1].values\nq25, q75 = np.percentile(v7_pos, 25), np.percentile(v7_pos, 75)\n\nv7_iqr = q75 - q25\n\nv7_lower, v7_upper = q25 - v7_iqr * 1.5, q75 + v7_iqr * 1.5\n\nnorm_df = norm_df.drop(norm_df[(norm_df['V7'] > v7_upper)|(norm_df['V7'] < v7_lower)].index)","87bdf63b":"v9_pos = norm_df['V9'].loc[norm_df['Class'] == 1].values\nq25, q75 = np.percentile(v9_pos, 25), np.percentile(v9_pos, 75)\n\nv9_iqr = q75 - q25\n\nv9_lower, v9_upper = q25 - v9_iqr * 1.5, q75 + v9_iqr * 1.5\n\nnorm_df = norm_df.drop(norm_df[(norm_df['V9'] > v9_upper)|(norm_df['V9'] < v9_lower)].index)","709309b6":"v10_pos = norm_df['V10'].loc[norm_df['Class'] == 1].values\nq25, q75 = np.percentile(v10_pos, 25), np.percentile(v10_pos, 75)\n\nv10_iqr = q75 - q25\n\nv10_lower, v10_upper = q25 - v10_iqr * 1.5, q75 + v10_iqr * 1.5\n\nnorm_df = norm_df.drop(norm_df[(norm_df['V10'] > v10_upper)|(norm_df['V10'] < v10_lower)].index)","7d83f19d":"v12_pos = norm_df['V12'].loc[norm_df['Class'] == 1].values\nq25, q75 = np.percentile(v12_pos, 25), np.percentile(v12_pos, 75)\n\nv12_iqr = q75 - q25\n\nv12_lower, v12_upper = q25 - v12_iqr * 1.5, q75 + v12_iqr * 1.5\n\nnorm_df = norm_df.drop(norm_df[(norm_df['V12'] > v12_upper)|(norm_df['V12'] < v12_lower)].index)","4f28c16d":"v14_pos = norm_df['V14'].loc[norm_df['Class'] == 1].values\nq25, q75 = np.percentile(v14_pos, 25), np.percentile(v14_pos, 75)\n\nv14_iqr = q75 - q25\n\nv14_lower, v14_upper = q25 - v14_iqr * 1.5, q75 + v14_iqr * 1.5\n\nnorm_df = norm_df.drop(norm_df[(norm_df['V14'] > v14_upper)|(norm_df['V14'] < v14_lower)].index)","abc62f8b":"fig, axes = plt.subplots(ncols=4, figsize=(20,4))\nsns.boxplot(x = 'Class', y = 'V1', data = norm_df, ax = axes[0])\naxes[0].set_title('V1 Negative Correlation with Class')\n\nsns.boxplot(x = 'Class', y = 'V3', data = norm_df, ax = axes[1])\naxes[1].set_title('V3 Negative Correlation with Class')\n\nsns.boxplot(x = 'Class', y = 'V5', data = norm_df, ax = axes[2])\naxes[2].set_title('V5 Negative Correlation with Class')\n\nsns.boxplot(x = 'Class', y = 'V6', data = norm_df, ax = axes[3])\naxes[3].set_title('V6 Negative Correlation with Class')","7da6d32f":"fig, axes = plt.subplots(ncols=4, figsize=(20,4))\nsns.boxplot(x = 'Class', y = 'V7', data = norm_df, ax = axes[0])\naxes[0].set_title('V7 Negative Correlation with Class')\n\nsns.boxplot(x = 'Class', y = 'V9', data = norm_df, ax = axes[1])\naxes[1].set_title('V9 Negative Correlation with Class')\n\nsns.boxplot(x = 'Class', y = 'V10', data = norm_df, ax = axes[2])\naxes[2].set_title('V10 Negative Correlation with Class')\n\nsns.boxplot(x = 'Class', y = 'V12', data = norm_df, ax = axes[3])\naxes[3].set_title('V12 Negative Correlation with Class')","2ab93b22":"fig, axes = plt.subplots(ncols=4, figsize=(20,4))\nsns.boxplot(x = 'Class', y = 'V14', data = norm_df, ax = axes[0])\naxes[0].set_title('V14 Negative Correlation with Class')\n\nsns.boxplot(x = 'Class', y = 'V16', data = norm_df, ax = axes[1])\naxes[1].set_title('V16 Negative Correlation with Class')\n\nsns.boxplot(x = 'Class', y = 'V17', data = norm_df, ax = axes[2])\naxes[2].set_title('V17 Negative Correlation with Class')\n\nsns.boxplot(x = 'Class', y = 'V18', data = norm_df, ax = axes[3])\naxes[3].set_title('V18 Negative Correlation with Class')","c2b8838b":"# Use a utility from sklearn to split and shuffle the dataset\nlabels = norm_df['Class']\nfeatures = norm_df.drop('Class', axis = 1)\ntrain_feature, test_feature, train_label, test_label = train_test_split(features, labels, test_size = 0.2, random_state=42)\ntrain_feature, val_feature, train_label, val_label = train_test_split(features, labels, test_size = 0.2)\n\ntrain_label = train_label.values\ntest_label = test_label.values\nval_label = val_label.values\ntrain_feature = train_feature.values\ntest_feature = test_feature.values\nval_feature = val_feature.values","bc34804e":"from keras.utils import to_categorical\n\ntrain_label = to_categorical(train_label)\ntest_label = to_categorical(test_label)\nval_label = to_categorical(val_label)","e39a3f8b":"print('training label shape:', train_label.shape)\nprint('training feature shape', train_feature.shape)\nprint('testing label shape', test_label.shape)\nprint('testing feature shape', test_feature.shape)\nprint('validation label shape', val_label.shape)\nprint('validation feature shape', val_feature.shape)","ba4027e1":"model = Sequential()\n\nmodel.add(Dense(input_dim = train_feature.shape[-1], units = 200, activation = 'relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(units = 200, activation = 'relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(units = 2, activation = 'sigmoid'))\n\nmodel.summary()\n\nmodel.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])","16ea0863":"model.fit(x = train_feature, y = train_label, batch_size = 30, epochs = 30)","b7e7e8b0":"train_result = model.evaluate(x = train_feature, y = train_label)\n","723bbb7e":"validate_result = model.evaluate(x = val_feature, y = val_label)","277fc5ad":"predict_result = model.evaluate(x = test_feature, y = test_label)","39f3bf68":"# 2. Feature analysis","e7461946":"- Remove \"extreme outliers\" from features that have a high negative correlation with our classes (the lower our feature value the more likely it will be a fraud transaction).  \n`V1,V3, V5, V6, V7, V9, V10, V12, V14, V16, V17, V18`  \n- This will have a positive impact on the accuracy of our models.","3187f4ac":"- The transactions data are highly skewed on non-fraud that we need sub-sample on following analysis.","6c4c2347":"- Time and amount should be scaled as the other columns. ","e0335e0e":"- check the outliers","5dcff82d":"## 2.5 Dataset Splitting","409d9bdd":"There are 492 cases of fraud in our dataset so we can randomly get 492 cases of non-fraud to create our new sub dataframe.  \nWe concat the 492 cases of fraud and non fraud, creating a new sub-sample.","ce6f412f":"## 2.2 Scaling","59dc3809":"# 1. Load and check the data\nThis dataset has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group of ULB (Universit\u00e9 Libre de Bruxelles) on big data mining and fraud detection.","3d4eb165":"- Check the distribution again","4b6a3f85":"## 2.1 Examine the features' distribution","63a294c4":"- we'll drop off the outliers from `V1, V3, V5, V6, V7, V9, V10, V12, V14` ","d7cf63df":"# 3. Model building","0ac9047f":"## 1.1 Load data","cd136167":"## 2.3 Sub-Sample","0a56ba9e":"## 0. References \nhttps:\/\/www.kaggle.com\/janiobachmann\/credit-fraud-dealing-with-imbalanced-datasets by Janio Martinez.\nhttps:\/\/colab.research.google.com\/github\/tensorflow\/docs\/blob\/master\/site\/en\/tutorials\/structured_data\/imbalanced_data.ipynb#scrollTo=dUeKVCYTbcyT Copyright 2019 The TensorFlow Authors.  \nThank you :)","17c71afc":"## 1.2 Check feature and missing values","48198333":"## 2.4 Anomaly detection","24e1a969":"- PCA Transformation: The description of the data says that all the V features went through a PCA transformation (Dimensionality Reduction technique) (Except for time and amount)."}}