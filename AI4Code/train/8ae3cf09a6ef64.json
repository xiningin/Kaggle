{"cell_type":{"957a7a86":"code","ae099f53":"code","5b88dd49":"code","a34bc49b":"code","f97ec182":"code","40a14961":"code","ff35ed39":"code","527ce8c6":"code","3bf34dd7":"code","27fb89bb":"code","9e5373b5":"markdown","7fdb34a8":"markdown","698915fb":"markdown","c5bc9915":"markdown","a2cd21a9":"markdown","e8fbfdfe":"markdown"},"source":{"957a7a86":"#loading dataset\nimport pandas as pd\nimport numpy as np\n#visualisation\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n# data preprocessing\nfrom sklearn.preprocessing import StandardScaler\n# data splitting\nfrom sklearn.model_selection import train_test_split\n# data modeling\nfrom sklearn.metrics import confusion_matrix,accuracy_score,roc_curve,classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n","ae099f53":"data = pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')\ndata.head()","5b88dd49":"data.info()","a34bc49b":"y = data[\"Outcome\"]\nX = data.drop('Outcome',axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state = 0)","f97ec182":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","40a14961":"m1 = 'Logistic Regression'\nlr = LogisticRegression()\nmodel = lr.fit(X_train, y_train)\nlr_predict = lr.predict(X_test)\nlr_conf_matrix = confusion_matrix(y_test, lr_predict)\nlr_acc_score = accuracy_score(y_test, lr_predict)\nprint(\"confussion matrix\")\nprint(lr_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Logistic Regression:\",lr_acc_score*100,'\\n')\nprint(classification_report(y_test,lr_predict))","ff35ed39":"m3 = 'Random Forest Classfier'\nrf = RandomForestClassifier(n_estimators=20, random_state=12,max_depth=5)\nrf.fit(X_train,y_train)\nrf_predicted = rf.predict(X_test)\nrf_conf_matrix = confusion_matrix(y_test, rf_predicted)\nrf_acc_score = accuracy_score(y_test, rf_predicted)\nprint(\"confussion matrix\")\nprint(rf_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Random Forest:\",rf_acc_score*100,'\\n')\nprint(classification_report(y_test,rf_predicted))","527ce8c6":"m7 = 'Support Vector Classifier'\nsvc =  SVC(kernel='rbf', C=2)\nsvc.fit(X_train, y_train)\nsvc_predicted = svc.predict(X_test)\nsvc_conf_matrix = confusion_matrix(y_test, svc_predicted)\nsvc_acc_score = accuracy_score(y_test, svc_predicted)\nprint(\"confussion matrix\")\nprint(svc_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Support Vector Classifier:\",svc_acc_score*100,'\\n')\nprint(classification_report(y_test,svc_predicted))","3bf34dd7":"model_ev = pd.DataFrame({'Model': ['Logistic Regression','Random Forest','Support Vector Machine'], 'Accuracy': [lr_acc_score*100,\n                    rf_acc_score*100,svc_acc_score*100]})\nmodel_ev","27fb89bb":"colors = ['red','green','blue','gold','silver','yellow','orange',]\nplt.figure(figsize=(12,5))\nplt.title(\"barplot Represent Accuracy of different models\")\nplt.xlabel(\"Accuracy %\")\nplt.ylabel(\"Algorithms\")\nplt.bar(model_ev['Model'],model_ev['Accuracy'],color = colors)\nplt.show()","9e5373b5":"## **ML models**\n\nHere I take different machine learning algorithm and try to find algorithm which predict accurately.\n\n1. Logistic Regression\n2. Naive Bayes\n3. Random Forest Classifier\n4. Extreme Gradient Boost\n5. K-Nearest Neighbour\n6. Decision Tree\n7. Support Vector Machine\n","7fdb34a8":"# **Model Evaluation**","698915fb":"# **Conclusion**\n\n**Logistic Regression gives the best Accuracy compared to other models**.\n\n**If you like my work don't hesitate to upvote.**\n","c5bc9915":"# Loading Dataset","a2cd21a9":"## **Model prepration**","e8fbfdfe":"## Packages Required"}}