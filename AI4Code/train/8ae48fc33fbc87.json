{"cell_type":{"e3f0325a":"code","2c496757":"code","98568bfe":"code","c76fd7bf":"code","6a19e9d6":"code","11ad7da7":"code","3852181e":"code","557a1a69":"code","7d5ebf3a":"code","a79fec46":"code","3edc222e":"code","47dd9086":"code","50bed1a7":"code","d29d1ebe":"code","49cefa21":"code","551241e0":"code","d2ead15f":"code","679d5fcb":"code","c26fba49":"code","645f3125":"code","1f15e66d":"code","20cb42e8":"code","d99b3631":"code","9c5d4d4c":"markdown","f37e57b9":"markdown","5dab91f0":"markdown","8c178870":"markdown","ba65394c":"markdown","b34dff62":"markdown","e6a84e26":"markdown","b4dec150":"markdown"},"source":{"e3f0325a":"import csv\nfrom sklearn.metrics import classification_report\nfrom itertools import islice, chain\nimport pandas as pd\nimport os\nimport numpy as np\nimport nltk\nfrom nltk.corpus import stopwords\nimport collections\nfrom IPython.display import Image\nimport random\nimport zipfile\nfrom nltk.stem import SnowballStemmer\nfrom string import punctuation\nimport spacy\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom gensim.models.doc2vec import TaggedDocument\nfrom gensim.models import Doc2Vec\nfrom tqdm import tqdm\nfrom six.moves import range\nfrom six.moves.urllib.request import urlretrieve\nfrom sklearn import utils\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n%matplotlib inline","2c496757":"TRAIN_PATH = os.path.join(\"..\/input\/\", \"BBC News Train.csv\")\nTEST_PATH = os.path.join(\"..\/input\/\", \"BBC News Test.csv\")","98568bfe":"bbc_train = pd.read_csv(TRAIN_PATH, encoding='latin-1')\nbbc_test = pd.read_csv(TEST_PATH, encoding='latin-1')","c76fd7bf":"bbc_train.head(10)","6a19e9d6":"bbc_test.head(10)","11ad7da7":"topic = bbc_train['Category'].value_counts()\nplt.figure(figsize=(12,4))\nsns.barplot(topic.index, topic.values, alpha=0.8)\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Topic', fontsize=12)\nplt.xticks(rotation=90)\nplt.show();","3852181e":"def read_data(filename):\n    \"\"\"\n      Extrai artigos at\u00e9 um determinado limite em um arquivo zip como uma lista de palavras\n      e pr\u00e9-processa usando a biblioteca nltk python\n      \"\"\"\n        \n    data = [[],[]]\n    train_data = {}\n    for i in range(filename.shape[0]):\n        text_string = filename[filename.columns[1]][i]\n        text_string = text_string.lower()\n        text_string = nltk.word_tokenize(text_string)\n        # Atribui a classe aos arquivos\n        data[0].append(text_string)\n        data[1].append(filename[filename.columns[2]][i])\n        \"\"\" Atribui o t\u00f3pico ao documento \"\"\"\n        train_data[filename[filename.columns[2]][i]+'-'+filename[filename.columns[0]][i].astype(str)] = text_string\n        print('\\tConclu\u00edda a leitura de dados para o t\u00f3pico: ',filename[filename.columns[2]][i]) \n               \n    return data, train_data\n\ndef read_test_data(filename):\n    \"\"\"\n      Extrai artigos at\u00e9 um determinado limite em um arquivo zip como uma lista de palavras\n      e pr\u00e9-processa usando a biblioteca nltk python\n      \"\"\"\n        \n    test_data = {}\n    for i in range(filename.shape[0]):\n        text_string = filename[filename.columns[1]][i]\n        text_string = text_string.lower()\n        text_string = nltk.word_tokenize(text_string)\n        # Atribui a classe aos arquivos\n        \"\"\" Atribui o t\u00f3pico ao documento \"\"\"\n        test_data[filename[filename.columns[0]][i].astype(str)] = text_string\n        print('\\tConclu\u00edda a leitura de dados para o t\u00f3pico: ',filename[filename.columns[0]][i].astype(str)) \n               \n    return test_data\n\nprint('Processando dados de treinamento...\\n')\nwords, train_words = read_data(bbc_train)\n\nprint('\\nProcessando dados de teste...\\n')\n\ntest_words = read_test_data(bbc_test)\n\n#test_words = read_test_data(filename)\n\n#list_test_words = list(map(tuple, test_words.items()))\n#random.shuffle(list_test_words)\n#test_words = dict(list_test_words)\n","557a1a69":"vocabulary_size = 25000\nWords = []\ndef build_dataset(words):\n    for word in words[0]:\n        Words.extend(word)    \n    count = [['UNK', -1]]\n    count.extend(collections.Counter(Words).most_common(vocabulary_size - 1))\n\n    # Dicion\u00e1rio\n    dictionary = dict()\n    for word, _ in count:\n        dictionary[word] = len(dictionary)\n    \n    data = list()\n    unk_count = 0\n    \n    for word in Words:\n        if word in dictionary:\n            index = dictionary[word]\n        else:\n            index = 0  # dictionary['UNK']            \n            unk_count = unk_count + 1\n            \n        data.append(word)\n\n    count[0][1] = unk_count\n    \n    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n    assert len(dictionary) == vocabulary_size\n\n    return data, count, dictionary, reverse_dictionary\n\ndef build_dataset_with_existing_dictionary(words, dictionary):\n    '''\n    Aqui usamos essa fun\u00e7\u00e3o para converter strings de palavras em IDs com um determinado dicion\u00e1rio\n    '''\n    data = list()\n    for word in words:\n        if word in dictionary:\n            index = dictionary[word]\n        else:\n            index = 0  # dictionary['UNK']\n        data.append(word)\n    return data\n\n# Processando dados de treino\ndata, count, dictionary, reverse_dictionary = build_dataset(words)\n\ntrain_data = {}\n\nfor k,v in train_words.items():\n    print('Construindo o dataset de treino para o documento ', k)\n    train_data[k] = build_dataset_with_existing_dictionary(train_words[k],dictionary)\n\n# Processando dados de teste\n\ntest_data = {}\n\nfor k,v in test_words.items():\n    print('Construindo o dataset de teste para o documento ', k)\n    test_data[k] = build_dataset_with_existing_dictionary(test_words[k],dictionary)\n    \nprint('\\nPalavras mais comuns (+UNK)', count[:5])\nprint('\\nAmostra de dados', data[:10])\nprint('\\nChaves: ', test_data.keys())\nprint('\\nItems: ', test_data.items())\n\n# Removemos para liberar mem\u00f3ria no computador. N\u00e3o precisamos mais desses objetos.\ndel words  \n#del test_words","7d5ebf3a":"# Converte de Dicion\u00e1rio para lista\ndata_train = [ [k,v] for k, v in train_data.items() ]\ndata_train = np.array(data_train)\ndata_test = [ [k,v] for k, v in test_data.items() ]\ndata_test = np.array(data_test)","a79fec46":"# fun\u00e7\u00e3o para Identificar as classes dos documentos e preparando os dados para o algoritimo\ndef prepara_dados(data):\n    datax = [[],[]]\n    for x in range(data.shape[0]):\n        s = data[x][0]\n        s = s.split(\"-\")\n        datax[0].append(s[0])\n        datax[1].append(data[x][1])                \n    return datax\n\ndata_train = prepara_dados(data_train)\ndata_test = prepara_dados(data_test)","3edc222e":"def label_sentences(corpus, topics):\n    \"\"\"\n    A implementa\u00e7\u00e3o do Doc2Vec da Gensim exige que cada documento \/ par\u00e1grafo tenha um r\u00f3tulo associado a ele.\n\u00a0\u00a0\u00a0\u00a0Fiz isso usando o m\u00e9todo TaggedDocument, etiquetando com a pr\u00f3pria classe do documento.\n    \"\"\"\n   \n    labeled = []\n    tags = np.unique(topics, return_counts=False)\n    for i, v in enumerate(corpus):\n        label = [s for s in tags if topics[i] in s]\n        doc =  \" \".join(str(x) for x in v)\n        labeled.append(TaggedDocument(doc.split(), label))\n    return labeled\nX_train = label_sentences(np.array(data_train[1]), data_train[0])\nX_test  = label_sentences(np.array(data_test[1]), data_test[0])","47dd9086":"len(X_train)","50bed1a7":"len(X_test)","d29d1ebe":"Image(url = 'Doc2Vec.png')","49cefa21":"# Instanciando um modelo Doc2Vec com um vetor de 128 palavras\n\nmodel_dbow = Doc2Vec(dm=0, vector_size=128, window=10, negative=5, cbow_mean=1, min_count=1, alpha=0.1, min_alpha=0.005)\nmodel_dbow.build_vocab([x for x in tqdm(X_train)])\n\n\n# Alicando 50 itera\u00e7\u00f5es sobre o corpus de treinamento.\n\nfor epoch in range(50):\n    model_dbow.train(utils.shuffle([x for x in tqdm(X_train)]), total_examples=len(X_train), epochs=10)\n    model_dbow.alpha -= 0.002\n    model_dbow.min_alpha = model_dbow.alpha","551241e0":"%%time\n\n# Dados de treino\n\ntrain_targets, train_regressors = zip(\n    *[(doc.tags[0], model_dbow.infer_vector(doc.words, alpha=0.1, min_alpha=0.005, epochs=100)) for doc in X_train])","d2ead15f":"%%time\n\n# Dados de teste\n\ntest_targets, test_regressors = zip(\n    *[(doc.tags[0], model_dbow.infer_vector(doc.words, alpha=0.1, min_alpha=0.005, epochs=100)) for doc in X_test])","679d5fcb":"# Aplicando de Regress\u00e3o Logistica\nlogreg = LogisticRegression(n_jobs=1, C=1e5)\nlogreg.fit(train_regressors, train_targets)\ny_pred = logreg.predict(test_regressors)\n","c26fba49":"y_pred.shape[0]","645f3125":"len(test_targets)","1f15e66d":"bbc_test['Category'] = y_pred","20cb42e8":"bbc_test.drop(['Text'], axis = 1, inplace = True)","d99b3631":"# Saving the dataset with the transformations\n\nbbc_test.to_csv('.\/BBC News Sample Solution.csv', index=False)","9c5d4d4c":"## Classifica\u00e7\u00e3o de Documentos com doc2Vec\n\n\nDoc2vec \u00e9 um algoritmo n\u00e3o supervisionado para gerar vetores para frases, par\u00e1grafos ou documentos (Representa\u00e7\u00f5es distribu\u00eddas de senten\u00e7as e documento). Trata-se de um conceito que foi apresentado em 2014 por Le & Mikilov, veja neste [artigo](https:\/\/arxiv.org\/abs\/1405.4053). Este algoritmo \u00e9 uma adapta\u00e7\u00e3o do word2vec, sendo que os vetores gerados pelo doc2vec podem ser usados para tarefas como encontrar semelhan\u00e7as entre senten\u00e7as , par\u00e1grafos ou documentos.\n\nUm vetor de documento \u00e9 uma representa\u00e7\u00e3o abstrata de comprimento vari\u00e1vel do significado contextual de um determinado tipo de documento. Assim como um vetor de palavras, \u00e9 o produto do processo de treinamento para uma rede neural, onde a entrada \u00e9 tipicamente um termo one-hot encoded codificado a partir do vocabul\u00e1rio do modelo e a sa\u00edda \u00e9 uma distribui\u00e7\u00e3o de probabilidade para palavras na pr\u00f3xima janela de contexto. \n\n\nNeste mini-projeto utilizaremos o conjunto de dados da BBC [dataset](http:\/\/mlg.ucd.ie\/datasets\/bbc.html). Este conjunto de dados \u00e9 composto de 2225 documentos do site de not\u00edcias da BBC correspondendo a hist\u00f3rias em cinco \u00e1reas distintas: (neg\u00f3cios, entretenimento, pol\u00edtica, esporte, tecnologia).\n\n\n## Dataset\n\nPara essa tarefa, usaremos um conjunto de arquivos de texto j\u00e1 organizado. Este conjunto de dados \u00e9 composto de 2225 documentos que correspondem a artigos de not\u00edcias da BBC, representando 5 t\u00f3picos distintos:\n\n    * Neg\u00f3cios\n    * Entretenimento\n    * Pol\u00edtica\n    * Esportes\n    * Tecnologia\n    \nUm exemplo de documento da categoria Tecnologia:\n\n'UK net users leading TV downloads British TV viewers lead the trend of illegally downloading US shows from the net, according to research. New episodes of 24, Desperate Housewives and Six Feet Under, appear on the web hours after they are shown in the US, said a report. Web tracking company Envisional said 18% of downloaders were from within the UK and that downloads of TV programmers had increased by 150% in the last year....'\n\nUsaremos 1490 documentos de cada categoria. Nosso vocabul\u00e1rio ser\u00e1 de tamanho 25.000. Al\u00e9m disso, cada documento ser\u00e1 representado por uma tag. Por exemplo, o 50\u00ba documento da se\u00e7\u00e3o Entretenimento ser\u00e1 representado como entretenimento-50. Deve-se notar que este \u00e9 um conjunto de dados muito pequeno comparado ao grande corpora de texto que est\u00e1 sendo analisado em aplica\u00e7\u00f5es do mundo real. No entanto, este pequeno exemplo \u00e9 adequado no momento para ver o poder das Word embeddings.\n","f37e57b9":"## Treinando o Classificador de Regress\u00e3o Log\u00edstica.","5dab91f0":"Usaremos a seguinte estrat\u00e9gia:\n\n1. Extrair os dados de todos os arquivos de texto e aprender as word embeddings.\n2. Extrair conjuntos rand\u00f4micos de documentos dos word embeddings j\u00e1 treinados.\n3. Preparar os dados para os modelos de treinamento e avalia\u00e7\u00e3o doc2Vec\n4. Contruir o vocabul\u00e1rio atrav\u00e9s de um modelo doc2vec - Distributed Bag of Words (DBOW)\n5. Aplicar um modelo de Regress\u00e3o Log\u00edstica para classifica\u00e7\u00e3o dos documentos","8c178870":"## Lendo os dados com pr\u00e9-processamento utilizando a biblioteca NLTK\n\nEfetua a leitura dos dados, depois converte o texto para letras min\u00fascula e o converte em tokens usando a biblioteca nltk. Temos duas fun\u00e7\u00f5es `read_data`, que l\u00ea os documentos de treino e `read_test_data` para ler os documentos de teste. ","ba65394c":"## Modelos de Treinamento e Avalia\u00e7\u00e3o doc2Vec\n\nPrimeiramente instanciamos um modelo doc2vec - Distributed Bag of Words (DBOW). Na arquitetura word2vec, n\u00f3s temos os algoritmos  \u201ccontinuous bag of words\u201d (CBOW) e \u201cskip-gram\u201d (SG), j\u00e1 na arquitetura doc2vec, os algoritmos correspondentes s\u00e3o \u201cdistributed memory\u201d (DM) e \u201cdistributed bag of words\u201d (DBOW).\n\nO DBOW \u00e9 o modelo doc2vec an\u00e1logo ao modelo Skip-gram do word2vec. Os vetores de par\u00e1grafos s\u00e3o obtidos pelo treinamento de uma rede neural na tarefa de prever uma distribui\u00e7\u00e3o de probabilidade de palavras em um par\u00e1grafo, dada uma palavra aleatoriamente amostrada do par\u00e1grafo.","b34dff62":"## Construindo os Vetores de Recurso para o Classificador","e6a84e26":"## Prepara\u00e7\u00e3o dos dados para cria\u00e7\u00e3o do vocabul\u00e1rio no doc2vec","b4dec150":"## Construindo os Dicion\u00e1rios\nPara entender cada um desses elementos, vamos tamb\u00e9m assumir o texto \"Eu gosto de ir \u00e0 escola\"\n\n* `dictionary`: mapeia uma palavra para um ID (i.e. {Eu:0, gosto:1, de:2, ir:3, \u00e0:4, escola:5})\n* `reverse_dictionary`: mapeia um ID para uma palavra (i.e. {0:Eu, 1:gosto, 2:de, 3:ir, 4:\u00e0, 5:escola}\n* `count`: Lista de elementos (palavra, frequ\u00eancia) (i.e. [(Eu,1),(gosto,1),(de,2),(ir,1),(\u00e0,1),(escola,1)]\n* `data` : Cont\u00e9m a string de texto que lemos, onde palavras s\u00e3o substitu\u00eddas por IDs de palavras (i.e. [0, 1, 2, 3, 2, 4])\n\nTamb\u00e9m introduzimos um token especial adicional chamado `UNK` para indicar que palavras raras s\u00e3o muito raras para serem usadas."}}