{"cell_type":{"21555d88":"code","d9c58ec6":"code","8f25704f":"code","666bc33e":"code","0d32bbce":"code","34cff7bb":"code","65d4440e":"code","525528db":"code","22b23ffb":"code","1e6cf0c3":"code","dce4f0e5":"code","0f01926e":"code","bb2237fa":"code","deaf175b":"code","59e8a49c":"code","b79c208e":"code","83b4ad81":"code","d7ca8c5e":"code","f3c81f13":"code","64f9d637":"code","321fff0d":"code","9906f5cd":"code","8c8a1329":"code","65b42990":"code","3ea8ac92":"code","6729878e":"markdown","75e19dcd":"markdown","21224fbf":"markdown","0125b985":"markdown","553f5707":"markdown","8d0a61f9":"markdown","1567c4f8":"markdown","9c6bf9a2":"markdown","f975e079":"markdown","ff6ce05b":"markdown","ba499bf4":"markdown","8e57fef1":"markdown"},"source":{"21555d88":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom torchtext import data\nfrom torchtext import datasets\nfrom torchtext import vocab\n\nimport random\nimport time\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.metrics import f1_score\n\n# To run experiments deterministically\nSEED = 42\n\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True","d9c58ec6":"# For kaggle\nimport os\nprint(os.listdir(\"..\/input\"))\nTRAIN_PATH = '..\/input\/train.csv'\nTEST_PATH = '..\/input\/test.csv'\nGLOVE_PATH = '..\/input\/embeddings\/glove.840B.300d\/glove.840B.300d.txt'\nGLOVE = 'glove.840B.300d.txt'","8f25704f":"print(os.listdir(\"..\/input\/embeddings\/glove.840B.300d\"))","666bc33e":"# Pretrained embedding to use\nEMBEDDING_PATH = GLOVE_PATH\n# Should we limit the vocab size?\n# (A) 120000, 95000\nMAX_SIZE = 120000\n# (A) Should we limit number of words in a sentence?\nMAX_LEN = 70\n\n# Split ratio for test\/valid\nSPLIT_RATIO = 0.9\n\n# (A)\nBATCH_SIZE = 512\n\n# Model parameters\n# (A) Could be lesser I think.\nHIDDEN_DIM = 32\n# (A)\nN_LAYERS = 2\nBIDIRECTIONAL = True\n# (C)\nDROPOUT = 0.5","0d32bbce":"# Defining the Fields for our dataset\n# Skipping the id column\nID = data.Field()\nTEXT = data.Field(tokenize='spacy')\nTARGET = data.LabelField(dtype=torch.float)\n\ntrain_fields = [('id', None), ('text', TEXT), ('target', TARGET)]\ntest_fields = [('id', ID), ('text', TEXT)]\n\n# Creating our train and test data\ntrain_data = data.TabularDataset(\n    path=TRAIN_PATH,\n    format='csv',\n    skip_header=True,\n    fields=train_fields\n)\n\ntest_data = data.TabularDataset(\n    path=TEST_PATH,\n    format='csv',\n    skip_header=True,\n    fields=test_fields\n)\n\n# Create validation dataset (default 70:30 split)\ntrain_data, valid_data = train_data.split(split_ratio=SPLIT_RATIO, random_state=random.seed(SEED))","34cff7bb":"print(f'Number of training examples: {len(train_data)}')\nprint(f'Number of validation examples: {len(valid_data)}')\nprint(f'Number of test examples: {len(test_data)}')","65d4440e":"# One training example\nvars(train_data.examples[0])","525528db":"# Importing the pretrained embedding\nvec = vocab.Vectors(EMBEDDING_PATH)\n\n# Build the vocabulary using only the train dataset?,\n# and also by specifying the pretrained embedding\nTEXT.build_vocab(train_data, vectors=vec, max_size=MAX_SIZE)\nTARGET.build_vocab(train_data)\nID.build_vocab(test_data)","22b23ffb":"print(f'Unique tokens in TEXT vocab: {len(TEXT.vocab)}')\nprint(f'Unique tokens in TARGET vocab: {len(TARGET.vocab)}')","1e6cf0c3":"TEXT.vocab.vectors.shape","dce4f0e5":"# Might have some confusion as to how the batch iterators are defined\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Automatically shuffles and buckets the input sequences into\n# sequences of similar length\ntrain_iter, valid_iter = data.BucketIterator.splits(\n    (train_data, valid_data),\n    sort_key=lambda x: len(x.text), # what function\/field to use to group the data\n    batch_size=BATCH_SIZE,\n    device=device\n)\n\n# Don't want to shuffle test data, so use a standard iterator\ntest_iter = data.Iterator(\n    test_data,\n    batch_size=BATCH_SIZE,\n    device=device,\n    train=False,\n    sort=False,\n    sort_within_batch=False\n)","0f01926e":"class RNN(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim,\n                 n_layers, bidirectional, dropout):\n        super().__init__()\n        \n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers,\n                           bidirectional=bidirectional, dropout=dropout)\n        # Final hidden state has both forward and backward components\n        self.fc = nn.Linear(hidden_dim*2, output_dim)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x):\n        # x: [seq_length, batch_size]\n        embedded = self.dropout(self.embedding(x))\n        # embedded: [seq_length, batch_size, emb_dim]\n        \n        output, (hidden, cell) = self.rnn(embedded)\n        # output: [seq_length, batch_size, hid_dim * num_directions]\n        # hidden: [num_layers * num_directions, batch_size, hid_dim]\n        # cell:\n        \n        # Concat the final forward and backward hidden layers\n        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n        # hidden: [batch_size, hid_dim * num_directions]\n        \n        return self.fc(hidden.squeeze(0))\n        # return: [batch_size, 1]","bb2237fa":"emb_shape = TEXT.vocab.vectors.shape\nINPUT_DIM = emb_shape[0]\nEMBEDDING_DIM = emb_shape[1]\nOUTPUT_DIM = 1\n\nmodel = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)","deaf175b":"pretrained_embeddings = TEXT.vocab.vectors\npretrained_embeddings.shape","59e8a49c":"model.embedding.weight.data.copy_(pretrained_embeddings)","b79c208e":"optimizer = optim.Adam(model.parameters())\ncriterion = nn.BCEWithLogitsLoss()\n\nmodel = model.to(device)\ncriterion = criterion.to(device)","83b4ad81":"def train(model, iterator, optimizer, criterion):\n    # Track the loss\n    epoch_loss = 0\n    \n    model.train()\n    \n    for batch in iterator:\n        optimizer.zero_grad()\n        \n        predictions = model(batch.text).squeeze(1)\n        loss = criterion(predictions, batch.target)\n        \n        loss.backward()\n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        \n    return epoch_loss \/ len(iterator)","d7ca8c5e":"def evaluate(model, iterator, criterion):\n    epoch_loss = 0\n    \n    model.eval()\n    \n    with torch.no_grad():\n        for batch in iterator:\n            predictions = model(batch.text).squeeze(1)\n            loss = criterion(predictions, batch.target)\n\n            epoch_loss += loss.item()\n        \n    return epoch_loss \/ len(iterator)","f3c81f13":"N_EPOCHS = 6\n\n# Track time taken\nstart_time = time.time()\n\nfor epoch in range(N_EPOCHS):\n    epoch_start_time = time.time()\n    \n    train_loss = train(model, train_iter, optimizer, criterion)\n    valid_loss = evaluate(model, valid_iter, criterion)\n    \n    print(f'| Epoch: {epoch+1:02} '\n          f'| Train Loss: {train_loss:.3f} '\n          f'| Val. Loss: {valid_loss:.3f} '\n          f'| Time taken: {time.time() - epoch_start_time:.2f}s'\n          f'| Time elapsed: {time.time() - start_time:.2f}s')","64f9d637":"# Use validation dataset\nvalid_pred = []\nvalid_truth = []\n    \nmodel.eval()\n    \nwith torch.no_grad():\n    for batch in valid_iter:\n        valid_truth += batch.target.cpu().numpy().tolist()\n        predictions = model(batch.text).squeeze(1)\n        valid_pred += torch.sigmoid(predictions).cpu().data.numpy().tolist()","321fff0d":"tmp = [0,0,0] # idx, cur, max\ndelta = 0\nfor tmp[0] in np.arange(0.1, 0.501, 0.01):\n    tmp[1] = f1_score(valid_truth, np.array(valid_pred)>tmp[0])\n    if tmp[1] > tmp[2]:\n        delta = tmp[0]\n        tmp[2] = tmp[1]\nprint('best threshold is {:.4f} with F1 score: {:.4f}'.format(delta, tmp[2]))","9906f5cd":"test_pred = []\ntest_id = []\n\nmodel.eval()\n\nwith torch.no_grad():\n    for batch in test_iter:\n        predictions = model(batch.text).squeeze(1)\n        test_pred += torch.sigmoid(predictions).cpu().data.numpy().tolist()\n        test_id += batch.id.view(-1).cpu().numpy().tolist()","8c8a1329":"test_pred = (np.array(test_pred) >= delta).astype(int)\ntest_id = [ID.vocab.itos[i] for i in test_id]","65b42990":"submission = pd.DataFrame({'qid': test_id, 'prediction': test_pred})","3ea8ac92":"submission.to_csv('submission.csv', index=False)","6729878e":"**Hyperparameters**  \nLet's keep track of the various hyperparameters here.","75e19dcd":"## Prediction and submission","21224fbf":"## Building the Vocabulary and Embeddings","0125b985":"## Transfering the pre-trained word embeddings","553f5707":"# Data Preprocessing","8d0a61f9":"# Defining the Model\nThe RNN architecture will be\n- LSTM\n- Bidirectional\n- Multi-layer\n- With dropout","1567c4f8":"## Creating Train \/ Validation \/ Test Datasets","9c6bf9a2":"## Constructing the Iterator \/ Batching","f975e079":"# Prediction","ff6ce05b":"## Determining probability threshold","ba499bf4":"## Training loop here","8e57fef1":"# Training the Model"}}