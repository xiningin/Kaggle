{"cell_type":{"6a4a88ad":"code","e3545a7b":"code","21d56910":"code","45456d02":"code","f70b1a5a":"code","0ba23d27":"code","0ca12dfd":"code","9c85d97c":"code","5557fcfb":"code","eb74a76b":"code","d6d53fb7":"code","98f8d116":"code","d8d0ec77":"code","7321f3bd":"code","d0f23fbe":"code","33676b2e":"code","f18c15be":"code","8d688cc8":"code","8e3b58ad":"code","7d215bb6":"code","e4330574":"code","cee652d4":"code","90101467":"code","895b1448":"code","f8d05bc0":"code","38a1bbdf":"code","762bd3a0":"code","0fb86089":"code","193feb79":"code","1e301932":"code","491e8d8b":"code","44398d90":"code","fb25f042":"code","43f33f40":"markdown","8f13b67a":"markdown","7d1ce551":"markdown","b9cd41b9":"markdown","9ca7084d":"markdown","76f23c3a":"markdown","562ee834":"markdown","67f86d10":"markdown","65bb69c0":"markdown","03d03193":"markdown","c824f3ac":"markdown","a2c250c2":"markdown","4e622b47":"markdown","fdda389f":"markdown","a3ef78b5":"markdown"},"source":{"6a4a88ad":"import numpy as np\nimport random\nimport pandas as pd\nimport os\n\nfrom sklearn.metrics import mean_absolute_error\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as Layers\nimport tensorflow.keras.models as Models\nimport warnings\nwarnings.filterwarnings('ignore') #Ignore \"future\" warnings and Data-Frame-Slicing warnings.\n","e3545a7b":"def seed_everything(seed): \n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\nseed_everything(2000)","21d56910":"ROOT = '..\/input\/osic-pulmonary-fibrosis-progression'\n\ntrain_df = pd.read_csv(f'{ROOT}\/train.csv')\nprint(f'Train data has {train_df.shape[0]} rows and {train_df.shape[1]} columnns and looks like this:')","45456d02":"train_unique_df = train_df.drop_duplicates(subset = ['Patient'], keep = 'first')\n#train_unique_df.head()","f70b1a5a":"# CHECK FOR DUPLICATES & DEAL WITH THEM\n# keep = False: All duplicates will be shown\ndupRows_df = train_df[train_df.duplicated(subset = ['Patient', 'Weeks'], keep = False )]\n# dupRows_df.head()","0ba23d27":"train_df.drop_duplicates(subset=['Patient','Weeks'], keep = False, inplace = True)","0ca12dfd":"print(f'So there are {dupRows_df.shape[0]} (= {dupRows_df.shape[0] \/ train_df.shape[0] * 100:.2f}%) duplicates.')","9c85d97c":"test_df = pd.read_csv(f'{ROOT}\/test.csv')\nprint(f'Test data has {test_df.shape[0]} rows and {test_df.shape[1]} columnns, has no duplicates and looks like this:')\ntest_df.head()","5557fcfb":"## CHECK SUBMISSION FORMAT\nsub_df = pd.read_csv(f\"{ROOT}\/sample_submission.csv\")\n\nprint(f\"The sample submission contains: {sub_df.shape[0]} rows and {sub_df.shape[1]} columns.\")","eb74a76b":"# split Patient_Week Column and re-arrage columns\nsub_df[['Patient','Weeks']] = sub_df.Patient_Week.str.split(\"_\",expand = True)\nsub_df =  sub_df[['Patient','Weeks','Confidence', 'Patient_Week']]","d6d53fb7":"sub_df = sub_df.merge(test_df.drop('Weeks', axis = 1), on = \"Patient\")","98f8d116":"# introduce a column to indicate the source (train\/test) for the data\ntrain_df['Source'] = 'train'\nsub_df['Source'] = 'test'\n\ndata_df = train_df.append([sub_df])\ndata_df.reset_index(inplace = True)\n#data_df.head()","d8d0ec77":"def get_baseline_week(df):\n    # make a copy to not change original df    \n    _df = df.copy()\n    # ensure all Weeks values are INT and not accidentaly saved as string\n    _df['Weeks'] = _df['Weeks'].astype(int)\n    # as test data is containing all weeks, \n    _df.loc[_df.Source == 'test','min_week'] = np.nan\n    _df[\"min_week\"] = _df.groupby('Patient')['Weeks'].transform('min')\n    _df['baselined_week'] = _df['Weeks'] - _df['min_week']\n    \n    return _df   ","7321f3bd":"data_df = get_baseline_week(data_df)\n#data_df.head()","d0f23fbe":"def get_baseline_FVC(df):\n    # same as above\n    _df = df.copy()\n    base = _df.loc[_df.Weeks == _df.min_week]\n    base = base[['Patient','FVC']].copy()\n    base.columns = ['Patient','base_FVC']\n    \n    # add a row which contains the cumulated sum of rows for each patient\n    base['nb'] = 1\n    base['nb'] = base.groupby('Patient')['nb'].transform('cumsum')\n    \n    # drop all except the first row for each patient (= unique rows!), containing the min_week\n    base = base[base.nb == 1]\n    base.drop('nb', axis = 1, inplace = True)\n    \n    # merge the rows containing the base_FVC on the original _df\n    _df = _df.merge(base, on = 'Patient', how = 'left')    \n    _df.drop(['min_week'], axis = 1)\n    \n    return _df","33676b2e":"data_df = get_baseline_FVC(data_df)\ndata_df.head()","f18c15be":"def own_MinMaxColumnScaler(df, columns):\n    \"\"\"Adds columns with scaled numeric values to range [0, 1]\n    using the formula X_scld = (X - X.min) \/ (X.max - X.min)\"\"\"\n    for col in columns:\n        new_col_name = col + '_scld'\n        col_min = df[col].min()\n        col_max = df[col].max()        \n        df[new_col_name] = (df[col] - col_min) \/ ( col_max - col_min )","8d688cc8":"def own_OneHotColumnCreator(df, columns):\n    \"\"\"OneHot Encodes categorical features. Adds a column for each unique value per column\"\"\"\n    for col in cat_attribs:\n        for value in df[col].unique():\n            df[value] = (df[col] == value).astype(int)","8e3b58ad":"## APPLY DEFINED TRANSFORMATIONS\n# define which attributes shall not be transformed, are numeric or categorical\nno_transform_attribs = ['Patient', 'Weeks', 'min_week']\nnum_attribs = ['FVC', 'Percent', 'Age', 'baselined_week', 'base_FVC']\ncat_attribs = ['Sex', 'SmokingStatus']\n\nown_MinMaxColumnScaler(data_df, num_attribs)\nown_OneHotColumnCreator(data_df, cat_attribs)\n\ndata_df[data_df.Source != \"train\"].head()","7d215bb6":"# get back original data split\ntrain_df = data_df.loc[data_df.Source == 'train']\nsub = data_df.loc[data_df.Source == 'test']","e4330574":"# create constants for the loss function\nC1, C2 = tf.constant(70, dtype='float32'), tf.constant(1000, dtype=\"float32\")\n\n# define competition metric\ndef score(y_true, y_pred):\n    \"\"\"Calculate the competition metric\"\"\"\n    tf.dtypes.cast(y_true, tf.float32)\n    tf.dtypes.cast(y_pred, tf.float32)\n    sigma = y_pred[:, 2] - y_pred[:, 0]\n    fvc_pred = y_pred[:, 1]\n    \n    sigma_clip = tf.maximum(sigma, C1)\n    delta = tf.abs(y_true[:, 0] - fvc_pred)\n    delta = tf.minimum(delta, C2)\n    sq2 = tf.sqrt( tf.dtypes.cast(2, dtype = tf.float32) )\n    metric = (delta \/ sigma_clip) * sq2 + tf.math.log(sigma_clip * sq2)\n    return K.mean(metric)\n\n# define pinball loss\ndef qloss(y_true, y_pred):\n    \"\"\"Calculate Pinball loss\"\"\"\n    # IMPORTANT: define quartiles, feel free to change here!\n    qs = [0.2, 0.50, 0.8]\n    q = tf.constant(np.array([qs]), dtype = tf.float32)\n    e = y_true - y_pred\n    v = tf.maximum(q * e, (q-1) * e)\n    return K.mean(v)\n\n# combine competition metric and pinball loss to a joint loss function\ndef mloss(_lambda):\n    \"\"\"Combine Score and qloss\"\"\"\n    def loss(y_true, y_pred):\n        return _lambda * qloss(y_true, y_pred) + (1 - _lambda) * score(y_true, y_pred)\n    return loss","cee652d4":"## GET TRAINING DATA AND TARGET VALUE\nseed_everything(1950)\n\n### Features: choose which features you want to use\nfeatures_list = ['baselined_week_scld', 'Percent_scld', 'Age_scld', 'base_FVC_scld', 'Male', 'Female', 'Ex-smoker', 'Never smoked', 'Currently smokes']\n\n# get target value\ny = train_df['FVC'].values.astype(float)\n\n# get training & test data\nX_train = train_df[features_list].values\nX_test = sub[features_list].values","90101467":"import tensorflow_addons as tfa\n\ndef get_model():\n    \"Creates and returns a model\"\n   \n    inp = Layers.Input((len(features_list),), name = \"Patient\")\n    x = Layers.BatchNormalization()(inp)\n    x = tfa.layers.WeightNormalization(Layers.Dense(196, activation = \"elu\", name = \"d1\"))(x)\n    x = Layers.BatchNormalization()(x)\n    x = Layers.Dropout(0.3)(x)\n    x = tfa.layers.WeightNormalization(Layers.Dense(128, activation = \"elu\", name = \"d2\"))(x)\n    x = Layers.BatchNormalization()(x)\n    x = Layers.Dropout(0.2)(x)\n    # predicting the quantiles\n    p1 = Layers.Dense(3, activation = \"relu\", name = \"p1\")(x)\n    # quantile adjusting p1 predictions\n    p2 = Layers.Dense(3, activation = \"relu\", name = \"p2\")(x)\n    preds = Layers.Lambda(lambda x: x[0] + tf.cumsum(x[1], axis = 1), \n                     name = \"preds\")([p1, p2])\n    \n    model = Models.Model(inputs = inp, outputs = preds, name = \"NeuralNet\")\n    model.compile(loss = mloss(0.8), optimizer = tf.optimizers.Adam(), metrics = [score])\n        \n    return model","895b1448":"neuralNet = get_model()","f8d05bc0":"# create list with the all pathes to all models\nmodels_weights_list = []\nmodels_root = '..\/input\/osic-basic-models-public'\n\nfor filename in os.listdir(models_root):\n    if filename.endswith('.h5'):\n        file_path = os.path.join(models_root, filename)\n        models_weights_list.append(file_path)\n        print(file_path)        \n    else:\n        continue","38a1bbdf":"# instantiate target arrays\ntrain_preds = np.zeros((X_train.shape[0], 3))\ntest_preds = np.zeros((X_test.shape[0], 3))\nval_scores = np.zeros((len(models_weights_list)))\n\n# Ensemble: Average predictions of all models\nfor counter, model_weights in enumerate(models_weights_list):\n    neuralNet.load_weights(model_weights)\n    \n    # predict and evaluate training-data\n    train_preds = neuralNet.predict(X_train, verbose = 0)        \n    val_scores[counter] = neuralNet.evaluate(X_train, y, verbose = 0)[1]    \n    print(f\"Based on training data, this model has a val_score of: {val_scores[counter]}\")\n        \n    # predict on test set and average the predictions over all folds\n    test_preds += neuralNet.predict(X_test, verbose = 0)\n\nprint(f\"\\nAverage val_score of all ensembled\/averaged models: {np.mean(val_scores)}\")    \ntest_preds \/= len(models_weights_list)","762bd3a0":"## FIND OPTIMIZED STANDARD-DEVIATION\nsigma_opt = mean_absolute_error(y, train_preds[:,1])\nsigma_uncertain = train_preds[:,2] - train_preds[:,0]\nsigma_mean = np.mean(sigma_uncertain)\nprint(sigma_opt, sigma_mean)","0fb86089":"## PREPARE SUBMISSION FILE WITH OUR PREDICTIONS\nsub['FVC1'] = test_preds[:, 1]\nsub['Confidence1'] = test_preds[:,2] - test_preds[:,0]\n\n# get rid of unused data and show some non-empty data\nsubmission = sub[['Patient_Week','FVC','Confidence','FVC1','Confidence1']].copy()\nsubmission.loc[~submission.FVC1.isnull()].head(10)","193feb79":"submission.loc[~submission.FVC1.isnull(),'FVC'] = submission.loc[~submission.FVC1.isnull(),'FVC1']\n\nif sigma_mean < 70:\n    submission['Confidence'] = sigma_opt\nelse:\n    submission.loc[~submission.FVC1.isnull(),'Confidence'] = submission.loc[~submission.FVC1.isnull(),'Confidence1']","1e301932":"submission.head()","491e8d8b":"submission.describe().T","44398d90":"org_test = pd.read_csv('..\/input\/osic-pulmonary-fibrosis-progression\/test.csv')\n\nfor i in range(len(org_test)):\n    submission.loc[submission['Patient_Week']==org_test.Patient[i]+'_'+str(org_test.Weeks[i]), 'FVC'] = org_test.FVC[i]\n    submission.loc[submission['Patient_Week']==org_test.Patient[i]+'_'+str(org_test.Weeks[i]), 'Confidence'] = 70","fb25f042":"submission[[\"Patient_Week\",\"FVC\",\"Confidence\"]].to_csv(\"submission.csv\", index = False)","43f33f40":"Now the data has the format we need to work with.","8f13b67a":"## Load all dependencies you need\n<span style=\"color:darkgreen ;font-family: Impact; font-size:13;\"> from  <\/span> coffee  <span style=\"color:darkgreen ;font-family: Impact; font-size:13;\"> import  <\/span> ***** ","7d1ce551":"### Let's do the Data-Prep and all related legwork ourselfs, w\/o sklearn. How to use sklearn, you can read here: \nhttps:\/\/www.kaggle.com\/chrisden\/6-82-quantile-reg-lr-schedulers-checkpoints  \n\n","b9cd41b9":"Okay, we made it! Let's finally check our stats and submit it!","9ca7084d":"## Getting the format right\nIn this section we are going to do all the Data-Wrangling and pre-processing which we need later to create our folds and evaluate our models.","76f23c3a":"# Quick intro\n\n## This notebook shall show how to easily ensemble various models and improve the robustness for the private LB.\n","562ee834":"In the following last step we overwrite our predictions with the known data from the orginal submission file to not waste known data.","67f86d10":"# Data Wrangling","65bb69c0":"### Prepare submission file\nIn the next section we are going to use the ```train_preds``` to calculate the optimized sigma, which is a measure for certainty or rather uncertainty. We can do that, as we have both: the model's estimate and the real data. We subtract the lower quartile from the upper quartile (defined in the loss function) and average it.","03d03193":"# Loss function & Model\nIn this section we are going to define the loss & a first model.\nFirst we are taking care of the loss. We are trying to minimize the following:\n\n![image.png](attachment:image.png)\n\nThe global minimum of this function is achieved for delta = 0 and sigma = 70, which [results in a loss of roughly -4.59.](https:\/\/www.kaggle.com\/c\/osic-pulmonary-fibrosis-progression\/discussion\/168469).\n\nGetting our model to predict the Confidence and FVC values (which is what we need!) is not working fine so far, as you can read [here](https:\/\/www.kaggle.com\/c\/osic-pulmonary-fibrosis-progression\/discussion\/167764).\nCurrently the way to go seems to be pinball loss. \n","c824f3ac":"## Preparing the data for the Neural Network","a2c250c2":"### Model","4e622b47":"# Evaluation & Ensemble\n\n\n","fdda389f":"## <font color='blue'>Thanks a lot for reading, I hope you could gain as much insights from reading this as I got from writing it. If you liked it, an upvote is highly appreciated. If you are interested in more content like this, feel free to follow me! ;)<\/font>","a3ef78b5":"## A good way to get many models and do some averaging can be found here: [TabData:Find best Hyperparameter & ENSEMBLE](https:\/\/www.kaggle.com\/ChristianDenich\/6-82x-tabdata-find-best-hyperparameter-ensemble\/edit\/run\/42823401). In fact, you can simply run the before mentioned notebook and upload the best models to do a submission with this notebook."}}