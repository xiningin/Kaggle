{"cell_type":{"e1d5d08d":"code","54ec267e":"code","a6a99c34":"code","19130155":"code","774e5f6f":"code","65fc6af9":"code","e63d0946":"code","b8473c34":"code","51f8d117":"code","6a7817a0":"code","cd52ae17":"code","e29dc400":"code","9453f080":"code","00e2e06a":"code","d4545a55":"code","ab5f69c3":"code","3e918630":"code","eb713e5c":"code","66f2a166":"code","e127d03e":"markdown","f8d9488d":"markdown","dd7f9255":"markdown","8742e18b":"markdown","28974f16":"markdown","bddd84ca":"markdown","191de2ad":"markdown","29455e9c":"markdown","bbb6d393":"markdown","a4f27ea8":"markdown","a87b85e2":"markdown","0f2ca5e9":"markdown","a9a1da54":"markdown","a06f484d":"markdown","157172a8":"markdown","dc9e1cdb":"markdown","6928fc7f":"markdown","52f86d80":"markdown","a36c42b7":"markdown","0a416be9":"markdown","5437440e":"markdown","7cc9e732":"markdown","a1b1b6c0":"markdown","3e256a29":"markdown"},"source":{"e1d5d08d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        path=os.path.join(dirname, filename)\n        if 'train' in path:\n            __training_path=path\n        elif 'test' in path:\n            __test_path=path","54ec267e":"#loaded files\nprint(f'Training path:{__training_path}\\nTest path:{__test_path}')","a6a99c34":"# Kaggle Environment Prepration\n#update kaggle env\nimport sys\n#you may update the environment that allow you to run the whole code\n!{sys.executable} -m pip install --upgrade scikit-learn==\"0.24.2\"","19130155":"#record this information if you need to run the Kernel internally\nimport sklearn; sklearn.show_versions()","774e5f6f":"def __load__data(__training_path, __test_path, concat=False):\n\t\"\"\"load data as input dataset\n\tparams: __training_path: the training path of input dataset\n\tparams: __test_path: the path of test dataset\n\tparams: if it is True, then it will concatinate the training and test dataset as output\n\treturns: generate final loaded dataset as dataset, input and test\n\t\"\"\"\n\t# LOAD DATA\n\timport pandas as pd\n\t__train_dataset = pd.read_csv(__training_path, delimiter=',')\n\t__test_dataset = pd.read_csv(__test_path, delimiter=',')\n\treturn __train_dataset, __test_dataset\n__train_dataset, __test_dataset = __load__data(__training_path, __test_path, concat=True)\n__train_dataset.head()","65fc6af9":"# STORE SUBMISSION RELEVANT COLUMNS\n__test_dataset_submission_columns = __test_dataset['PassengerId']","e63d0946":"# DISCARD IRRELEVANT COLUMNS\n__train_dataset.drop(['PassengerId'], axis=1, inplace=True)\n__test_dataset.drop(['PassengerId'], axis=1, inplace=True)","b8473c34":"# PREPROCESSING-1\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\n_NUMERIC_COLS_WITH_MISSING_VALUES = ['Age', 'Fare', 'Parch', 'Pclass', 'SibSp']\nfor _col in _NUMERIC_COLS_WITH_MISSING_VALUES:\n    __simple_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n    __train_dataset[_col] = __simple_imputer.fit_transform(__train_dataset[_col].values.reshape(-1,1))[:,0]\n    if _col in __test_dataset:\n        __test_dataset[_col] = __simple_imputer.transform(__test_dataset[_col].astype(__train_dataset[_col].dtypes).values.reshape(-1,1))[:,0]","51f8d117":"# PREPROCESSING-2\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\n_STRING_COLS_WITH_MISSING_VALUES = ['Cabin', 'Ticket', 'Sex', 'Name', 'Embarked']\nfor _col in _STRING_COLS_WITH_MISSING_VALUES:\n    __simple_imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n    __train_dataset[_col] = __simple_imputer.fit_transform(__train_dataset[_col].values.reshape(-1,1))[:,0]\n    if _col in __test_dataset:\n        __test_dataset[_col] = __simple_imputer.transform(__test_dataset[_col].astype(__train_dataset[_col].dtypes).values.reshape(-1,1))[:,0]","6a7817a0":"# PREPROCESSING-3\nfrom sklearn.preprocessing import OrdinalEncoder\n_CATEGORICAL_COLS = ['Sex', 'Embarked']\n_ohe = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n__train_dataset[_CATEGORICAL_COLS] = pd.DataFrame(_ohe.fit_transform(__train_dataset[_CATEGORICAL_COLS]), columns=_CATEGORICAL_COLS)\n__test_dataset[_CATEGORICAL_COLS] = pd.DataFrame(_ohe.transform(__test_dataset[_CATEGORICAL_COLS]), columns=_CATEGORICAL_COLS)","cd52ae17":"# PREPROCESSING-4\nimport nltk\nimport re\nimport string\n_TEXT_COLUMNS = ['Name', 'Ticket', 'Cabin']\ndef process_text(__dataset):\n    for _col in _TEXT_COLUMNS:\n        process_text = [t.lower() for t in __dataset[_col]]\n        # strip all punctuation\n        table = str.maketrans('', '', string.punctuation)\n        process_text = [t.translate(table) for t in process_text]\n        # convert all numbers in text to 'num'\n        process_text = [re.sub(r'\\d+', 'num', t) for t in process_text]\n        __dataset[_col] = process_text\n    return __dataset\n__train_dataset = process_text(__train_dataset)\n__test_dataset = process_text(__test_dataset)","e29dc400":"# DETACH TARGET\n__feature_train = __train_dataset.drop(['Survived'], axis=1)\n__target_train =__train_dataset['Survived']\n__feature_test = __test_dataset","9453f080":"# PREPROCESSING-5\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport scipy.sparse as sparse\nfrom scipy.sparse import hstack, csr_matrix\n_TEXT_COLUMNS = ['Cabin', 'Ticket', 'Name']\n__temp_train_data = __feature_train[_TEXT_COLUMNS]\n__feature_train.drop(_TEXT_COLUMNS, axis=1, inplace=True)\n__feature_train_object_array = []\n__temp_test_data = __feature_test[_TEXT_COLUMNS]\n__feature_test.drop(_TEXT_COLUMNS, axis=1, inplace=True)\n__feature_test_object_array = []\nfor _col in _TEXT_COLUMNS:\n    __tfidfvectorizer = TfidfVectorizer(max_features=3000)\n    vector_train = __tfidfvectorizer.fit_transform(__temp_train_data[_col])\n    __feature_train_object_array.append(vector_train)\n    vector_test = __tfidfvectorizer.transform(__temp_test_data[_col])\n    __feature_test_object_array.append(vector_test)\n__feature_train = sparse.hstack([__feature_train] + __feature_train_object_array).tocsr()\n__feature_test = sparse.hstack([__feature_test] + __feature_test_object_array).tocsr()","00e2e06a":"# MODEL\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\n__model = RandomForestClassifier()\n__model.fit(__feature_train, __target_train) \n__y_pred = __model.predict(__feature_test)","d4545a55":"# SUBMISSION\nsubmission = pd.DataFrame(columns=['PassengerId'], data=__test_dataset_submission_columns)\nsubmission = pd.concat([submission, pd.DataFrame(__y_pred, columns=['Survived'])], axis=1)\nsubmission.head()","ab5f69c3":"# save submission file\nsubmission.to_csv(\"kaggle_submission.csv\", index=False)","3e918630":"#from sklearn.metrics import accuracy_score","eb713e5c":"#__accuracy = accuracy_score(__target_train, __y_pred)","66f2a166":"#print('RESULT: Accuracy: ' + str(__accuracy))","e127d03e":"# Text Processing\nThe dataset has <b>3<\/b> text values as follows: <b>Name,Ticket,Cabin<\/b>.\nNow, let's covert the text as follows.\n\n- First, convert text to lowercase;\n\n- Second, strip all punctuations;\n\n- Finally, convert all numbers in text to 'num'; therefore, in the next step our model will use a single token instead of valriety of tokens of numbers.","f8d9488d":"## Encoding Ordinal Categorical Features\nLet's transfer categorical features as an integer array.\nWe will use Ordinal Encoder as explained [here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.OrdinalEncoder.html).\n\nIn the given input dataset there are <b>2<\/b> columns that can be transfered to integer and it includes:* Sex,Embarked *.","dd7f9255":"## Remove Missing Values in Numerical Columns\n\nIn the given input dataset there are <b>5 columns <\/b> with  missing data as follows:<b>Cabin, Ticket, Sex, Name, Embarked<\/b>\n\nThe following code removes the missing values from those columns. We use average value (median) of each column to replace the null values.","8742e18b":"# Finding Intresting Datapoints\nLet's process each field by their histogram frequency and check if there is any intresting data point.\n\nThere are <b>4<\/b> number of intresting values in the following columns.\nThe below table shows each <b>Value<\/b> of each <b>Field<\/b>(column) with their total frequencies, <b>Lower<\/b> shows the lower frequency of normal distribution, <b>Upper<\/b> shows the upper bound frequency of normal distribution, and <b>Criteria<\/b> shows if the frequnecy passed <b>Upper bound<\/b> or <b>Lower bound<\/b>.\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th><\/th>\n      <th>Field<\/th>\n      <th>Value<\/th>\n      <th>Frequency<\/th>\n      <th>Lower<\/th>\n      <th>Upper<\/th>\n      <th>Criteria<\/th>\n    <\/tr>\n  <\/thead>\n  <tbody>\n    <tr>\n      <th>0<\/th>\n      <td>Age<\/td>\n      <td>24.00<\/td>\n      <td>30<\/td>\n      <td>0.0088<\/td>\n      <td>29.9736<\/td>\n      <td>Upper<\/td>\n    <\/tr>\n    <tr>\n      <th>1<\/th>\n      <td>SibSp<\/td>\n      <td>0.00<\/td>\n      <td>608<\/td>\n      <td>5.0012<\/td>\n      <td>607.7606<\/td>\n      <td>Upper<\/td>\n    <\/tr>\n    <tr>\n      <th>2<\/th>\n      <td>Parch<\/td>\n      <td>0.00<\/td>\n      <td>678<\/td>\n      <td>1.0018<\/td>\n      <td>677.6640<\/td>\n      <td>Upper<\/td>\n    <\/tr>\n    <tr>\n      <th>3<\/th>\n      <td>Fare<\/td>\n      <td>8.05<\/td>\n      <td>43<\/td>\n      <td>1.0000<\/td>\n      <td>42.9753<\/td>\n      <td>Upper<\/td>\n    <\/tr>\n  <\/tbody>\n<\/table>\n\n\n\nFor example, in the <b>Age<\/b> column the value of <b>24.0<\/b> has <b>30<\/b> repeatation but this number is not between Lower bound(0.0088) and Upper bound(29.97360000000002).\n\n\nLet     $C_0=24.0$   and   $Freq(C_0)=30$     ,   $Upper(C_0)=29.97360000000002$     ,   $Lower(C_0)=0.0088$\n\n$Freq(C_0) > Upper(C_0)$.","28974f16":"## Remove Missing Values in Categorical Columns","bddd84ca":"## Input Dataset","191de2ad":"# Training Model and Prediction\nFirst, we will train a model based on preprocessed values of training data set.\nSecond, let's predict test values based on the trained model.","29455e9c":"In the given input dataset there are <b>5 columns <\/b> with  missing data as follows:<b>Age, Fare, Parch, Pclass, SibSp<\/b>\nThe following code removes the missing values from those columns. We use average value (median) of each column to replace the null values.","bbb6d393":"# Submission File\nWe have to maintain the target columns in \"submission.csv\" which will be submitted as our prediction results.","a4f27ea8":"# About Competition\nTitanic ML competition is the best first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\n Machine learning can predict which passengers survived the Titanic shipwreck. The competition is simple: use machine learning to create a model that predicts which survivors survived the wreck.Competition file is available [here](https:\/\/www.kaggle.com\/c\/titanic).","a87b85e2":"# Load Competition Dataset","0f2ca5e9":"## Imputation Transformer\nWe will use which is an imputation transformer for completing missing values.\nWe can use out-of-the-box imputation transformer from Scikit-Learn packages. The detail and the list of complete parameters can be found [here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.impute.SimpleImputer.html)","a9a1da54":"## Random Forest Classifier\nWe will use *RandomForestClassifier* which is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\nThe sub-sample size is controlled with the max_samples parameter if bootstrap=True (default), otherwise the whole dataset is used to build each tree.\nMore detail about *RandomForestClassifier* can be found [here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html).","a06f484d":"# Input Dataset","157172a8":"There is <b>2<\/b> unique value in <b>Survived<\/b> column which is a target column.\nlet's see frequency of values for the target column of {col}:\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th><\/th>\n      <th>Survived<\/th>\n    <\/tr>\n  <\/thead>\n  <tbody>\n    <tr>\n      <th>0<\/th>\n      <td>549<\/td>\n    <\/tr>\n    <tr>\n      <th>1<\/th>\n      <td>342<\/td>\n    <\/tr>\n  <\/tbody>\n<\/table>","dc9e1cdb":"### Discard Irrelevant Columns\nIn the given input dataset there are <b>1<\/b> column that can be removed as follows:* PassengerId *.","6928fc7f":"<b> Is there any null value?<\/b> \nThe answer is <b>Yes<\/b>; let's review top 3 of those columns with the number of Null values.\n\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th><\/th>\n      <th>Column<\/th>\n      <th>#Null<\/th>\n    <\/tr>\n  <\/thead>\n  <tbody>\n    <tr>\n      <th>0<\/th>\n      <td>Cabin<\/td>\n      <td>687<\/td>\n    <\/tr>\n    <tr>\n      <th>1<\/th>\n      <td>Age<\/td>\n      <td>177<\/td>\n    <\/tr>\n    <tr>\n      <th>2<\/th>\n      <td>Embarked<\/td>\n      <td>2<\/td>\n    <\/tr>\n  <\/tbody>\n<\/table>\n\nAs partial of the results shown above, there are total <b>3<\/b> columns with Null values.","52f86d80":"Competition dataset located in \"\/kaggle\/input\"; This path defined by Kaggle to access the competition file. We will list two files from this path as input files.","a36c42b7":"### Target Column\nThe target column is the value which we need to predict.\nTherefore, we need to detach the target columns in prediction.\nNote that if we don't drop this fields, it will generate a model with high accuracy on training and worst accuracy on test (because the value in test dataset is Null).\nHere is the list of *target column*: <b>Survived<\/b>","0a416be9":"# Text Vectorizer\nIn the next step, we will transfer pre-processed text columns to a vector representation. The vector representations allows us to train a model based on numerical representations.\nWe will use TfidfVectorizer and more detail can be found [here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.TfidfVectorizer.html).","5437440e":"## Skewness\nIn probability theory and statistics, skewness is a measure of the asymmetry of the probability distribution of a real-valued random variable about its mean. The skewness value can be positive, zero, negative, or undefined.\n\nMore detail can be found [here](https:\/\/en.wikipedia.org\/wiki\/Skewness) and the [Probability and Statistics Tables and Formulae](http:\/\/tomlr.free.fr\/Math%E9matiques\/Math%20Complete\/Probability%20and%20statistics\/CRC%20-%20standard%20probability%20and%20Statistics%20tables%20and%20formulae%20-%20DANIEL%20ZWILLINGER.pdf) by Zwillinger and Kokoska.\n\nHere are two samples Skewness data for positive and negative skew data.<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/f\/f8\/Negative_and_positive_skew_diagrams_%28English%29.svg\" alt=\"source:Wikimedia\">\n\nFirst, we we will calculate the skewness for each columns in Titanic Dataset if each selected column has positive float\/integer values.\n\nSecond, we will review the following conditions.\n\n- if $Skewness>1$ or $Skewness<-1$, we will consider it as highly skewed;\n\n- if $0.5<Skewness<=1$ or $-0.5<Skewness<=-1$, we will consider it as moderate skewed;\n\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th><\/th>\n      <th>Column<\/th>\n      <th>Skewness<\/th>\n      <th>Skewness_Type<\/th>\n    <\/tr>\n  <\/thead>\n  <tbody>\n    <tr>\n      <th>0<\/th>\n      <td>Pclass<\/td>\n      <td>-0.630548<\/td>\n      <td>moderately skewed<\/td>\n    <\/tr>\n    <tr>\n      <th>1<\/th>\n      <td>SibSp<\/td>\n      <td>3.695352<\/td>\n      <td>highly skewed<\/td>\n    <\/tr>\n    <tr>\n      <th>2<\/th>\n      <td>Parch<\/td>\n      <td>2.749117<\/td>\n      <td>highly skewed<\/td>\n    <\/tr>\n    <tr>\n      <th>3<\/th>\n      <td>Fare<\/td>\n      <td>4.787317<\/td>\n      <td>highly skewed<\/td>\n    <\/tr>\n  <\/tbody>\n<\/table>","7cc9e732":"<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/8\/81\/Titanic_in_color.png\/320px-Titanic_in_color.png\"\/>\n<center>Image Source: Fidodog14 and SandyShores03, Public domain, via Wikimedia Commons<\/center>","a1b1b6c0":"Let's review the dataset description:\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th><\/th>\n      <th>count<\/th>\n      <th>mean<\/th>\n      <th>std<\/th>\n      <th>min<\/th>\n      <th>25%<\/th>\n      <th>50%<\/th>\n      <th>75%<\/th>\n      <th>max<\/th>\n    <\/tr>\n  <\/thead>\n  <tbody>\n    <tr>\n      <th>PassengerId<\/th>\n      <td>891.0<\/td>\n      <td>446.000000<\/td>\n      <td>257.353842<\/td>\n      <td>1.00<\/td>\n      <td>223.5000<\/td>\n      <td>446.0000<\/td>\n      <td>668.5<\/td>\n      <td>891.0000<\/td>\n    <\/tr>\n    <tr>\n      <th>Survived<\/th>\n      <td>891.0<\/td>\n      <td>0.383838<\/td>\n      <td>0.486592<\/td>\n      <td>0.00<\/td>\n      <td>0.0000<\/td>\n      <td>0.0000<\/td>\n      <td>1.0<\/td>\n      <td>1.0000<\/td>\n    <\/tr>\n    <tr>\n      <th>Pclass<\/th>\n      <td>891.0<\/td>\n      <td>2.308642<\/td>\n      <td>0.836071<\/td>\n      <td>1.00<\/td>\n      <td>2.0000<\/td>\n      <td>3.0000<\/td>\n      <td>3.0<\/td>\n      <td>3.0000<\/td>\n    <\/tr>\n    <tr>\n      <th>Age<\/th>\n      <td>714.0<\/td>\n      <td>29.699118<\/td>\n      <td>14.526497<\/td>\n      <td>0.42<\/td>\n      <td>20.1250<\/td>\n      <td>28.0000<\/td>\n      <td>38.0<\/td>\n      <td>80.0000<\/td>\n    <\/tr>\n    <tr>\n      <th>SibSp<\/th>\n      <td>891.0<\/td>\n      <td>0.523008<\/td>\n      <td>1.102743<\/td>\n      <td>0.00<\/td>\n      <td>0.0000<\/td>\n      <td>0.0000<\/td>\n      <td>1.0<\/td>\n      <td>8.0000<\/td>\n    <\/tr>\n    <tr>\n      <th>Parch<\/th>\n      <td>891.0<\/td>\n      <td>0.381594<\/td>\n      <td>0.806057<\/td>\n      <td>0.00<\/td>\n      <td>0.0000<\/td>\n      <td>0.0000<\/td>\n      <td>0.0<\/td>\n      <td>6.0000<\/td>\n    <\/tr>\n    <tr>\n      <th>Fare<\/th>\n      <td>891.0<\/td>\n      <td>32.204208<\/td>\n      <td>49.693429<\/td>\n      <td>0.00<\/td>\n      <td>7.9104<\/td>\n      <td>14.4542<\/td>\n      <td>31.0<\/td>\n      <td>512.3292<\/td>\n    <\/tr>\n  <\/tbody>\n<\/table>","3e256a29":"# Exploratory Data Analysis (EDA)\n## General Structure\nTitanic Dataset includes <b>12<\/b> columns and <b>891<\/b> rows.\nThere are <b>3<\/b> different data types as follows: *int64, object, float64*."}}