{"cell_type":{"ab4297f9":"code","234aee66":"code","b749761e":"code","78bcf1f7":"code","94821ba8":"code","d87f7e67":"code","2a6d211a":"code","443d30e0":"code","9cbf38ae":"code","1501b9f5":"code","16f74cfd":"code","1c3daaa1":"code","f3b46673":"code","6d391f71":"code","d469fb62":"markdown","32b51b0f":"markdown","ee52bfff":"markdown","b017da3b":"markdown","e791f89e":"markdown","cd6bd768":"markdown","e63582ef":"markdown","f8573e72":"markdown","daa042e9":"markdown","9b650c90":"markdown","4906ad45":"markdown","fe485dc5":"markdown"},"source":{"ab4297f9":"# Download data and preparing to prediction (including FE) \n# Thanks to: https:\/\/www.kaggle.com\/mauricef\/titanic\nimport pandas as pd\nimport numpy as np \ntraindf = pd.read_csv('..\/input\/titanic\/train.csv').set_index('PassengerId')\ntestdf = pd.read_csv('..\/input\/titanic\/test.csv').set_index('PassengerId')\ndf = pd.concat([traindf, testdf], axis=0, sort=False)\ndf['Title'] = df.Name.str.split(',').str[1].str.split('.').str[0].str.strip()\ndf['IsWomanOrBoy'] = ((df.Title == 'Master') | (df.Sex == 'female'))\ndf['LastName'] = df.Name.str.split(',').str[0]\nfamily = df.groupby(df.LastName).Survived\ndf['WomanOrBoyCount'] = family.transform(lambda s: s[df.IsWomanOrBoy].fillna(0).count())\ndf['WomanOrBoyCount'] = df.mask(df.IsWomanOrBoy, df.WomanOrBoyCount - 1, axis=0)\ndf['FamilySurvivedCount'] = family.transform(lambda s: s[df.IsWomanOrBoy].fillna(0).sum())\ndf['FamilySurvivedCount'] = df.mask(df.IsWomanOrBoy, df.FamilySurvivedCount - df.Survived.fillna(0), axis=0)\ndf['WomanOrBoySurvived'] = df.FamilySurvivedCount \/ df.WomanOrBoyCount.replace(0, np.nan)\ndf['Alone'] = (df.WomanOrBoyCount == 0)\ntrain_y = df.Survived.loc[traindf.index]\ndf = pd.concat([df.WomanOrBoySurvived.fillna(0), df.Alone, df.Sex.replace({'male': 0, 'female': 1})], axis=1)\n\ntest_x = df.loc[testdf.index]","234aee66":"# My upgrade - the one line of the code for prediction : LB = 0.83253 (Titanic Top 3%) \ntest_x['Survived'] = (((test_x.WomanOrBoySurvived <= 0.238) & (test_x.Sex > 0.5) & (test_x.Alone > 0.5)) | \\\n          ((test_x.WomanOrBoySurvived > 0.238) & \\\n           ~((test_x.WomanOrBoySurvived > 0.55) & (test_x.WomanOrBoySurvived <= 0.633))))","b749761e":"# Saving the result\npd.DataFrame({'Survived': test_x['Survived'].astype(int)}, \\\n             index=testdf.index).reset_index().to_csv('survived.csv', index=False)","78bcf1f7":"import pandas as pd\nimport numpy as np \nimport graphviz\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","94821ba8":"# Download data and preparing to prediction (including FE) \ntraindf = pd.read_csv('..\/input\/titanic\/train.csv').set_index('PassengerId')\ntestdf = pd.read_csv('..\/input\/titanic\/test.csv').set_index('PassengerId')\ndf = pd.concat([traindf, testdf], axis=0, sort=False)","d87f7e67":"# FE\ndf['Title'] = df.Name.str.split(',').str[1].str.split('.').str[0].str.strip()\ndf['IsWomanOrBoy'] = ((df.Title == 'Master') | (df.Sex == 'female'))\ndf['LastName'] = df.Name.str.split(',').str[0]\nfamily = df.groupby(df.LastName).Survived\ndf['WomanOrBoyCount'] = family.transform(lambda s: s[df.IsWomanOrBoy].fillna(0).count())\ndf['WomanOrBoyCount'] = df.mask(df.IsWomanOrBoy, df.WomanOrBoyCount - 1, axis=0)\ndf['FamilySurvivedCount'] = family.transform(lambda s: s[df.IsWomanOrBoy].fillna(0).sum())\ndf['FamilySurvivedCount'] = df.mask(df.IsWomanOrBoy, df.FamilySurvivedCount - df.Survived.fillna(0), axis=0)\ndf['WomanOrBoySurvived'] = df.FamilySurvivedCount \/ df.WomanOrBoyCount.replace(0, np.nan)\ndf['Alone'] = (df.WomanOrBoyCount == 0)","2a6d211a":"df","443d30e0":"train_y = df.Survived.loc[traindf.index]\ndata = pd.concat([df.WomanOrBoySurvived.fillna(0), df.Alone, \\\n                  df.Sex.replace({'male': 0, 'female': 1})], axis=1)\ntrain_x, test_x = data.loc[traindf.index], data.loc[testdf.index]\ntrain_x.head(5)","9cbf38ae":"# Tuning the DecisionTreeClassifier by the GridSearchCV\nparameters = {'max_depth' : np.arange(2, 5, dtype=int),\n              'min_samples_leaf' :  np.arange(2, 5, dtype=int)}\nclassifier = DecisionTreeClassifier(random_state=1000)\nmodel = GridSearchCV(estimator=classifier, param_grid=parameters, scoring='accuracy', cv=2, n_jobs=-1)\nmodel.fit(train_x, train_y)\nbest_parameters = model.best_params_\nprint(best_parameters)","1501b9f5":"model=DecisionTreeClassifier(max_depth = best_parameters['max_depth'], \n                             random_state = 1000)\nmodel.fit(train_x, train_y)","16f74cfd":"# plot tree\ndot_data = export_graphviz(model, out_file=None, feature_names=train_x.columns, class_names=['0', '1'], \n                           filled=True, rounded=False,special_characters=True, precision=3) \ngraph = graphviz.Source(dot_data)\ngraph ","1c3daaa1":"# # Prediction by the DecisionTreeClassifier\ny_pred = model.predict(test_x).astype(int)\nprint('Mean =', y_pred.mean(), ' Std =', y_pred.std())","f3b46673":"# The one line of the code for prediction : LB = 0.80382 (Titanic Top 6%) \ntest_x['Survived'] = (((test_x.WomanOrBoySurvived <= 0.238) & (test_x.Sex > 0.5) & (test_x.Alone > 0.5)) | \\\n                      ((test_x.WomanOrBoySurvived > 0.238) & \\\n                       ~((test_x.WomanOrBoySurvived > 0.55) & (test_x.WomanOrBoySurvived <= 0.633))))\ny_pred = test_x['Survived'].astype(int)\nprint('Mean =', y_pred.mean(), ' Std =', y_pred.std())","6d391f71":"# Saving the result\npd.DataFrame({'Survived': y_pred}, index=testdf.index).reset_index().to_csv('submission.csv', index=False)","d469fb62":"### Prediction","32b51b0f":"As you can see there is a slight difference in std, possibly related to the fact that the rules on the decision tree are shown with rounding. But this did not affect the accuracy of the solution, at least the first 5 decimal places in the Kaggle leaderboard","ee52bfff":"Now I will give a code with forecasting not in the context of the classes of cabins and ports, but in the context of the surnames of passengers (Thanks to: https:\/\/www.kaggle.com\/mauricef\/titanic). \n\n\nIt is interesting that there are obvious data errors in the dataset (for example, with respect to the Andersson (see https:\/\/www.kaggle.com\/c\/titanic\/discussion\/14904#latest-208058), who are not all the same family, and lived in several different cabins), but if they are corrected, the forecast will worsen!\n\nAfter the code, I will show how this one line of prediction code was developed.","b017da3b":"# Tuning the model","e791f89e":"### Saving the result","cd6bd768":"# [Titanic: Machine Learning from Disaster](https:\/\/www.kaggle.com\/c\/titanic)","e63582ef":"### Tuning model","f8573e72":"Thanks to:\n\n* [Automatic selection from 20 classifier models](https:\/\/www.kaggle.com\/vbmokin\/automatic-selection-from-20-classifier-models)\n* [Titanic (0.83253) - Comparison 20 popular models](https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models)\n* [Three lines of code for Titanic Top 15%](https:\/\/www.kaggle.com\/vbmokin\/three-lines-of-code-for-titanic-top-15)\n* [Three lines of code for Titanic Top 20%](https:\/\/www.kaggle.com\/vbmokin\/three-lines-of-code-for-titanic-top-20)\n* [Titanic Top 3% : cluster analysis](https:\/\/www.kaggle.com\/vbmokin\/titanic-top-3-cluster-analysis)\n* [Feature importance - xgb, lgbm, logreg, linreg](https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg)","daa042e9":"![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/6\/6e\/St%C3%B6wer_Titanic.jpg)","9b650c90":"## Titanic : one lines of the prediction code for LB = 0.80382","4906ad45":"### Plot tree","fe485dc5":"### Download data and preparing to prediction (including FE) "}}