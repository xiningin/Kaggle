{"cell_type":{"f475aeab":"code","324ad01d":"code","0732328d":"code","416fdeb2":"code","9a986f95":"code","117c540f":"code","cf219a48":"code","518edd7e":"code","ddc558ec":"code","a81c2cbd":"code","e5b6728f":"code","4a210379":"code","b566239a":"code","4398352d":"code","b84135dc":"code","c6bf85f3":"code","e8d33839":"code","4b4ee44f":"code","5a0f2a4d":"code","ab7f0954":"code","9358ae35":"code","05ef8973":"code","7b46bbca":"code","a3057169":"code","17271165":"code","365e01f8":"code","33aadc40":"code","83f3e9f7":"code","5eb29838":"code","c3e1c845":"code","c8b77b30":"code","f08fa647":"code","8d262793":"code","18c8f340":"code","4e8187bf":"code","ecc90863":"code","ae7bdd0e":"code","fad3af18":"code","3869fb56":"code","d035ad55":"code","4745b4b0":"code","773822a8":"code","316f04a3":"code","f79b1a41":"code","93dba615":"code","02e2c6fc":"code","e72d22b3":"code","4f53ba1f":"code","f83d04d4":"code","71fc32bd":"code","a93d387c":"code","131cc68f":"code","616a403c":"code","aba52525":"code","7e565560":"code","9523d8cc":"code","223a27fe":"code","ca25badb":"code","8bbc20ca":"code","ca7adfcf":"code","8aefd8f0":"code","f55b596c":"code","0249a8c6":"code","8c42a6f0":"code","ed557817":"code","b6a997ea":"code","d0dbb40e":"code","64b85721":"code","bf5a7210":"markdown","e652cb3b":"markdown","610b9074":"markdown","b43cba52":"markdown","c19802fa":"markdown","d88664b4":"markdown","23fb4bac":"markdown","ece35d52":"markdown","95855c99":"markdown","27cc9be9":"markdown","96806f11":"markdown","9476b98b":"markdown","94b5730c":"markdown","7435a15c":"markdown","03bf4b84":"markdown","41222b6e":"markdown","9ddc02fa":"markdown","27fb0e6c":"markdown","52bc8e25":"markdown","2ff9c901":"markdown","3d075a69":"markdown","dcf4c09e":"markdown","59283bcd":"markdown","3950144a":"markdown","f2940c31":"markdown","defb4c2c":"markdown","5b51e446":"markdown","51ca26cc":"markdown","bb542f5f":"markdown","e93227b8":"markdown","8a36e4db":"markdown","11d67bc5":"markdown","123bfb19":"markdown","22af77d4":"markdown","31a6643e":"markdown","e1ffcb35":"markdown","088b84a1":"markdown","0f60d229":"markdown","2d1a6e8b":"markdown","58555c63":"markdown","bda05a6f":"markdown","e1a13cdb":"markdown","75aa6882":"markdown","2803dc82":"markdown","74c65aae":"markdown","378bcb7d":"markdown","e0e51be1":"markdown","5d81e395":"markdown","5d1c1273":"markdown","a2d92254":"markdown","0e881bae":"markdown","77618794":"markdown","a2f24894":"markdown","ef6cfdf8":"markdown","20294de6":"markdown","02ae474a":"markdown","b5fd6d67":"markdown","110eb2ea":"markdown","d3e74aad":"markdown","b5dc73be":"markdown","8c4956a7":"markdown","10a9fadd":"markdown","4de7c3fd":"markdown","552c7350":"markdown","6b7ab235":"markdown","9e77ec72":"markdown","fd754a02":"markdown","90c8583b":"markdown","6cef8693":"markdown","852249d2":"markdown","90d55724":"markdown","0cc81ce5":"markdown","6f84817e":"markdown","af99ea8e":"markdown","2c3d650e":"markdown","9e145415":"markdown","95578fe9":"markdown","44d2dd46":"markdown","dd08b555":"markdown","b5112e5e":"markdown","dc8cb6fe":"markdown","666c51d4":"markdown","e654bae7":"markdown","6ea97cc6":"markdown","f397e1d8":"markdown","f8fab998":"markdown","2c507051":"markdown","72a11c0c":"markdown","598f77a7":"markdown","18e52841":"markdown","7188edb6":"markdown","09509335":"markdown","48ada495":"markdown","50dea46e":"markdown","48bb8528":"markdown","a43733b5":"markdown","ca66c2b6":"markdown"},"source":{"f475aeab":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib\nfrom mpl_toolkits.mplot3d import Axes3D\nimport sklearn\nimport sys\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import ParameterGrid\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import tree\nfrom sklearn.metrics import f1_score, fbeta_score, accuracy_score, confusion_matrix, roc_curve, auc\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom imblearn.combine import SMOTEENN\nfrom imblearn.under_sampling import EditedNearestNeighbours\nfrom imblearn.over_sampling import ADASYN, SMOTE\n\n\nfrom pandas.api.types import is_numeric_dtype\n\npalette = [\"#9b59b6\", \"#5497db\", \"#95a5a6\", \"#e74c3c\", \"#34495e\", \"#2ecc71\"]\nsns.set_palette(palette)\n","324ad01d":"# Columns of the dataset (data comes without an header)\ncolumns = [\"existing_account\", \"month_duration\", \"credit_history\",\\\n           \"purpose\", \"credit_amount\", \"saving_bonds\",\\\n           \"employment_status\", \"installment_rate\", \"status_sex\", \\\n           \"debts_status\", \"resident_since\", \"property\", \\\n           \"age\", \"installment_plans\", \"housing_status\", \\\n           \"credit_number\", \"job\", \"people_liability\", \\\n           \"telephone\", \"foreign\", \"result\"]\n\nnumerical_attributes = [\"month_duration\", \"credit_amount\", \"installment_rate\", \"resident_since\", \"age\",\\\n                        \"credit_number\", \"people_liability\"]","0732328d":"df = pd.read_csv(\"..\/input\/german-credit-data\/german.data\", sep=\" \", header=None, names=columns)","416fdeb2":"df.head()","9a986f95":"df.info()","117c540f":"df.describe()","cf219a48":"\n# Positive labels\neligible= df[\"result\"] == 1\nprint(f\"The proportion of individuals which are eligible for credit (good risk) is: {df[eligible].shape[0]\/df.shape[0]}\")\n\n# Negative labels\nnot_eligible= df[\"result\"] == 2\nprint(f\"The proportion of individuals which are not eligible for credit (bad risk) is: {df[not_eligible].shape[0]\/df.shape[0]}\")\n\n# We can note a sligthly umbalance","518edd7e":"df_numerical = df.copy()\n\ndummy_columns = [\"credit_history\", \"purpose\", \"status_sex\", \"debts_status\", \"property\", \"installment_plans\", \"housing_status\", \\\n            \"foreign\", \"existing_account\", \"saving_bonds\", \"telephone\", \"job\", \"employment_status\"]\n\ndf_numerical = pd.get_dummies(df_numerical, columns=dummy_columns, drop_first=True)","ddc558ec":"df_numerical.head()","a81c2cbd":"corr = df.corr()\n\nfig, ax = plt.subplots(figsize=(10,10)) \nax = sns.heatmap(corr, annot=True, linewidth=0.5, xticklabels=numerical_attributes + [\"result\"])\nax.set_yticklabels(labels=numerical_attributes + [\"result\"], rotation=0)\nax.set_title(\"Correlation matrix for the numerical features\")\nplt.show()","e5b6728f":"corr = df_numerical.corr()\n\nfig, ax = plt.subplots(figsize=(10,10)) \nax = sns.heatmap(corr, xticklabels=corr.columns)\nax.set_yticklabels(labels=corr.columns, rotation=0) \nax.set_title(\"Correlation Matrix considering also dummy encoded variable\")\nplt.show()","4a210379":"corr_unstacked = corr.unstack().abs()\nranked_corr = corr_unstacked.sort_values(kind=\"quicksort\", ascending=False)\nranked_corr = ranked_corr[ranked_corr != 1]\n\nranked_corr.head(10)","b566239a":"pairplot_df = df.loc[:, numerical_attributes + [\"result\"]].replace({\"result\": {1:\"good\", 2:\"bad\"}})\nax = sns.pairplot(pairplot_df, hue=\"result\", plot_kws={\"alpha\":0.5}, diag_kind=\"hist\")\nax.fig.suptitle(\"Pairplots for the numberical variables\", y=1.08)","4398352d":"# Utility function that given the dataset prints a table pivoted for two values of interest with the counts of each one\ndef entries_per_couple_variables(df, first_var, second_var, first_level_indexes=None, second_level_indexes=None):\n    # perform the renaming on the column values\n    renamed_df = df.loc[:,[first_var, second_var]]\n    if first_level_indexes is not None and second_level_indexes is not None:\n        renamed_df = renamed_df.replace({first_var:first_level_indexes, second_var:second_level_indexes})\n    # group for the two columns\n    grouped_df = renamed_df.groupby([first_var, second_var]).size().reset_index()\n    \n    # pivot the result\n\n    pivoted_df = pd.pivot_table(grouped_df, index=first_var, columns=second_var, values=0)\n    pivoted_df[\"Total Column Sum\"] = pivoted_df.sum(axis=1)\n    for index in second_level_indexes.values():\n        pivoted_df[f\"proportion_{index}\"] = pivoted_df[index]\/pivoted_df[\"Total Column Sum\"]\n    \n    df_total_rows = pivoted_df.sum(axis=0)\n    df_total_rows = df_total_rows.rename(\"Total Row Sum\")\n    \n    \n    pivoted_df = pivoted_df.append(df_total_rows)\n    for index in first_level_indexes.values():\n        prop_df = pivoted_df.loc[index]\/df_total_rows\n        prop_df = prop_df.rename(f\"proportion_{index}\")\n        prop_df[len(second_level_indexes):] = \"-\"\n        pivoted_df = pivoted_df.append(prop_df)\n            \n    df_total_rows[len(second_level_indexes)+1:] = \"-\"\n    pivoted_df.loc[\"Total Row Sum\"] = df_total_rows\n    \n    return pivoted_df","b84135dc":"saving_bonds_level_indexes = {\"A61\": \"A61: < 100DM\",\n                       \"A62\":\"A62: 100 DM<=...< 500DM\",\n                       \"A63\":\"A63: 500 DM <= ... < 1000 DM\",\n                       \"A64\":\"A64: >= 1000 DM\",\n                       \"A65\":\"A65: unknown\/ no savings account\"}\nsecond_level_indexes = {1:\"good\", 2:\"bad\"}\nentries_per_couple_variables(df, \"saving_bonds\", \"result\", saving_bonds_level_indexes, second_level_indexes)","c6bf85f3":"existing_account_level_indexes = {\"A11\": \"A11: .. < 0 DM\",\n                       \"A12\": \"A12: 0 <= ... < 200 DM\",\n                       \"A13\": \"A13: ... >= 200 DM \/ salary assignments for at least 1 year\",\n                       \"A14\": \"A14: no checking account\"}\n\nentries_per_couple_variables(df, \"existing_account\", \"result\", existing_account_level_indexes, second_level_indexes)","e8d33839":"entries_per_couple_variables(df, \"existing_account\", \"saving_bonds\", existing_account_level_indexes, saving_bonds_level_indexes)","4b4ee44f":"housing_level_indexes = {\"A151\":\"A151: rent\",\\\n                       \"A152\":\"A152: own\", \\\n                       \"A153\":\"A153: for free\"}\nentries_per_couple_variables(df, \"housing_status\", \"result\", housing_level_indexes, second_level_indexes)","5a0f2a4d":"property_level_indexes = {\"A121\": \"A121: real estate\",\\\n                          \"A122\": \"A122: if not A121 : building society savings agreement\/ life insurance\",\\\n                          \"A123\": \"A123: if not A121\/A122 : car or other, not in attribute 6\",\\\n                          \"A124\": \"A124: unknown \/ no property\"}\n\nentries_per_couple_variables(df, \"housing_status\", \"property\", housing_level_indexes, property_level_indexes)","ab7f0954":"first_level_indexes = {\"A91\":\"A91: male:divorced\/separated\",\\\n                       \"A92\":\"A92: female:divorced\/separated\/married\",\\\n                       \"A93\":\"A93: male:single\",\\\n                       \"A94\":\"A94: male:married\/widowed\"}\nentries_per_couple_variables(df, \"status_sex\", \"result\", first_level_indexes, second_level_indexes)","9358ae35":"first_level_indexes = {\"A40\":\"A40: car (new)\",\\\n                       \"A41\":\"A41: car (used)\",\\\n                       \"A42\":\"A42: furniture\/equipment\",\\\n                       \"A43\":\"A43: radio\/television\",\\\n                       \"A44\":\"A44: domestic appliances\",\\\n                       \"A45\":\"A45: repairs\",\\\n                       \"A46\":\"A46: education\",\\\n                       \"A48\" : \"A48: retraining\",\\\n                       \"A49\":\"A49: business\",\\\n                       \"A410\" : \"A410: others\"}\nentries_per_couple_variables(df, \"purpose\", \"result\", first_level_indexes, second_level_indexes)","05ef8973":"job_level_indexes = {\"A171\": \"unemployed\/ unskilled - non-resident\",\\\n                     \"A172\": \"A172: unskilled - resident\",\\\n                     \"A173\": \"A173: skilled employee \/ official\",\\\n                     \"A174\": \"A174: management\/ self-employed\/highly qualified employee\/ officer\"}\nentries_per_couple_variables(df, \"job\", \"result\", job_level_indexes, second_level_indexes)","7b46bbca":"fig, axs = plt.subplots(3, 3, figsize=(15, 15))\n\ni = 0\nj = 0\n\nfor category in numerical_attributes:\n    sns.boxplot(y=df[category], x=df[\"result\"].replace({1:\"good\", 2:\"bad\"}), ax=axs[i, j], orient=\"v\", showmeans=True)\n    j += 1\n    if j%3 == 0:\n        j = 0\n        i += 1\n        \naxs[2, 1].set_visible(True)\nfig.delaxes(axs[2, 1])\naxs[2, 2].set_visible(True)\nfig.delaxes(axs[2, 2])","a3057169":"X = np.array(df_numerical.loc[:, df_numerical.columns != \"result\"])\nprint(f\"Shape of the features of the dataset: {X.shape}\")\n\ny = np.array(df_numerical.loc[:, \"result\"].replace({1:0, 2:1}))\nprint(f\"Shape of the labels of the dataset: {y.shape}\")\n\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.3, stratify=y)\n\nprint(f\"Shape of the training set: {X_train.shape}\")\nprint(f\"Shape of the test set: {X_test.shape}\")","17271165":"# Standardize data\nscaler = StandardScaler()\nX_train_raw = X_train\nX_test_raw = X_test\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","365e01f8":"# Default beta=1 and default k=5\nada = ADASYN()\n\nX_train_ada, y_train_ada = ada.fit_resample(X_train, y_train)","33aadc40":"print(f\"Shape of the balanced ADASYN dataset is: {X_train_ada.shape}\")\n\npos_ratio = y_train_ada[y_train_ada==1].shape[0]\/y_train_ada.shape[0]\nneg_ratio = y_train_ada[y_train_ada==0].shape[0]\/y_train_ada.shape[0]\n\nprint(f\"Proportion of positive samples in the balanced training set: {pos_ratio:.2f}\")\nprint(f\"Proportion of negative samples in the balanced training set: {neg_ratio:.2f}\")\n\npos_ratio_test = y_test[y_test==1].shape[0]\/y_test.shape[0]\nneg_ratio_test = y_test[y_test==0].shape[0]\/y_test.shape[0]\n\nprint(f\"Proportion of positive samples in the test set: {pos_ratio_test}\")\nprint(f\"Proportion of negative samples in the test set: {neg_ratio_test}\")","83f3e9f7":"smoteenn = SMOTEENN(smote=SMOTE(), enn=EditedNearestNeighbours())\n\nX_train_smoteenn, y_train_smoteenn = smoteenn.fit_resample(X_train, y_train)","5eb29838":"print(f\"Shape of the balanced SMOTEEN dataset is: {X_train_smoteenn.shape}\")\n\npos_ratio = y_train_smoteenn[y_train_smoteenn==1].shape[0]\/y_train_smoteenn.shape[0]\nneg_ratio = y_train_smoteenn[y_train_smoteenn==0].shape[0]\/y_train_smoteenn.shape[0]\n\nprint(f\"Proportion of positive samples in the balanced training set: {pos_ratio:.2f}\")\nprint(f\"Proportion of negative samples in the balanced training set: {neg_ratio:.2f}\")\n\npos_ratio_test = y_test[y_test==1].shape[0]\/y_test.shape[0]\nneg_ratio_test = y_test[y_test==0].shape[0]\/y_test.shape[0]\n\nprint(f\"Proportion of positive samples in the test set: {pos_ratio_test}\")\nprint(f\"Proportion of negative samples in the test set: {neg_ratio_test}\")","c3e1c845":"pca = PCA(n_components=2)\n\nX_pca_visualization = pca.fit_transform(X)\n\nfig, ax = plt.subplots(figsize=(10, 10))\nsns.scatterplot(x=X_pca_visualization[:, 0], y=X_pca_visualization[:, 1], hue=df[\"result\"].replace({1:\"good\", 2:\"bad\"}))\nax.set_title(\"Original Dataset reduced to two components with PCA\")\nax.set_xlabel(\"First PC\")\nax.set_ylabel(\"Second PC\")\nplt.show()\n\nprint(f\"The percentage of variance explained by each components is: {pca.explained_variance_ratio_}\")\n\npca = PCA(n_components=2)\nX_ada, y_ada =  ada.fit_resample(scaler.fit_transform(X), y)\nX_pca_visualization_ada = pca.fit_transform(X_ada)\n\nfig, ax = plt.subplots(figsize=(10, 10))\nsns.scatterplot(x=X_pca_visualization_ada[:, 0], y=X_pca_visualization_ada[:, 1], \\\n                hue=pd.Series(y_ada).replace({0:\"good\", 1:\"bad\"}))\nax.set_title(\"ADASYN rebalanced Dataset reduced to two components with PCA\")\nax.set_xlabel(\"First PC\")\nax.set_ylabel(\"Second PC\")\nplt.show()\n\nprint(f\"The percentage of variance explained by each components for the ADASYN dataset is: {pca.explained_variance_ratio_}\")\n\npca = PCA(n_components=2)\nX_smoteenn, y_smoteenn =  smoteenn.fit_resample(scaler.fit_transform(X), y)\nX_pca_visualization_smoteenn = pca.fit_transform(X_smoteenn)\n\nfig, ax = plt.subplots(figsize=(10, 10))\nsns.scatterplot(x=X_pca_visualization_smoteenn[:, 0], y=X_pca_visualization_smoteenn[:, 1], \\\n                hue=pd.Series(y_smoteenn).replace({0:\"good\", 1:\"bad\"}))\nax.set_title(\"SMOTEENN rebalanced Dataset reduced to two components with PCA\")\nax.set_xlabel(\"First PC\")\nax.set_ylabel(\"Second PC\")\nplt.show()\n\nprint(f\"The percentage of variance explained by each components for the SMOTEENN dataset is: {pca.explained_variance_ratio_}\")","c8b77b30":"pca = PCA(n_components=3)\nX_pca_visualization = pca.fit_transform(scaler.fit_transform(X))\n\nfig, ax = plt.subplots(figsize=(10, 10))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X_pca_visualization[df[\"result\"] == 1, 0], \\\n           X_pca_visualization[df[\"result\"] == 1, 1], \\\n           X_pca_visualization[df[\"result\"] == 1, 2], label=\"bad\")\n\nax.scatter(X_pca_visualization[df[\"result\"] == 2, 0], \\\n           X_pca_visualization[df[\"result\"] == 2, 1], \\\n           X_pca_visualization[df[\"result\"] == 2, 2], label=\"good\")\nax.legend()\nax.set_title(\"Original Dataset reduced to three components with PCA\")\nax.set_xlabel(\"First PC\")\nax.set_ylabel(\"Second PC\")\nax.set_zlabel(\"Third PC\")\nplt.show()\n\nprint(f\"The percentage of variance explained by each components is: {pca.explained_variance_ratio_}\")\n\nX_ada, y_ada =  ada.fit_resample(scaler.fit_transform(X), y)\nX_pca_visualization_ada = pca.fit_transform(X_ada)\n\nfig, ax = plt.subplots(figsize=(10, 10))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X_pca_visualization_ada[y_ada==1, 0], \\\n           X_pca_visualization_ada[y_ada==1, 1], \\\n           X_pca_visualization_ada[y_ada==1, 2], label=\"bad\")\n\nax.scatter(X_pca_visualization_ada[y_ada==0, 0], \\\n           X_pca_visualization_ada[y_ada==0, 1], \\\n           X_pca_visualization_ada[y_ada==0, 2], label=\"good\")\nax.legend()\nax.set_title(\"ADASYN Dataset reduced to three components with PCA\")\nax.set_xlabel(\"First PC\")\nax.set_ylabel(\"Second PC\")\nax.set_zlabel(\"Third PC\")\nplt.show()\n\nprint(f\"The percentage of variance explained by each components for the ADASYN dataset is: {pca.explained_variance_ratio_}\")\n\nX_smoteenn, y_smoteenn =  smoteenn.fit_resample(scaler.fit_transform(X), y)\nX_pca_visualization_smoteenn = pca.fit_transform(X_smoteenn)\n\nfig, ax = plt.subplots(figsize=(10, 10))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X_pca_visualization_smoteenn[y_smoteenn==1, 0], \\\n           X_pca_visualization_smoteenn[y_smoteenn==1, 1], \\\n           X_pca_visualization_smoteenn[y_smoteenn==1, 2], label=\"bad\")\n\nax.scatter(X_pca_visualization_smoteenn[y_smoteenn==0, 0], \\\n           X_pca_visualization_smoteenn[y_smoteenn==0, 1], \\\n           X_pca_visualization_smoteenn[y_smoteenn==0, 2], label=\"good\")\nax.legend()\nax.set_title(\"SMOTEENN Dataset reduced to three components with PCA\")\nax.set_xlabel(\"First PC\")\nax.set_ylabel(\"Second PC\")\nax.set_zlabel(\"Third PC\")\nplt.show()\n\nprint(f\"The percentage of variance explained by each components for the SMOTEENN dataset is: {pca.explained_variance_ratio_}\")","f08fa647":"pca = PCA(n_components=X_train.shape[1])\nc_analyzed = 30\n\npca.fit(X_train)\nfig, ax = plt.subplots(figsize=(10, 10))\nax = sns.lineplot(x=range(1, X_train.shape[1]+1), y=pca.explained_variance_ratio_, label=\"Variance explained by each component\")\nax = sns.lineplot(x=range(1, X_train.shape[1]+1), y=np.cumsum(pca.explained_variance_ratio_), label=\"Cumulative variance explained\")\nax = sns.scatterplot(x=[c_analyzed], y=[np.cumsum(pca.explained_variance_ratio_)[c_analyzed-1]], s=100)\nax.text(c_analyzed+1, np.cumsum(pca.explained_variance_ratio_)[c_analyzed-1], f\"{np.cumsum(pca.explained_variance_ratio_)[c_analyzed-1]:.2f}\",\\\n       horizontalalignment='left')\n\nax = sns.scatterplot(x=[c_analyzed], y=[pca.explained_variance_ratio_[c_analyzed-1]], s=100)\nax.text(c_analyzed+1, pca.explained_variance_ratio_[c_analyzed-1], f\"{pca.explained_variance_ratio_[c_analyzed-1]:.2f}\",\\\n       horizontalalignment='left')\nplt.axvline(c_analyzed, color=\"black\", ls=\"--\")\nax.set_title(\"number of componet used against PVE for the original Dataset\")\nax.set_xlabel(\"Principal component\")\nax.set_ylabel(\"PVE\")\nplt.show()","8d262793":"pca = PCA(n_components=X_train_ada.shape[1])\n\npca.fit(X_train_ada)\nfig, ax = plt.subplots(figsize=(10, 10))\nax = sns.lineplot(x=range(1, X_train_ada.shape[1]+1), y=pca.explained_variance_ratio_, label=\"Variance explained by each component\")\nax = sns.lineplot(x=range(1, X_train_ada.shape[1]+1), y=np.cumsum(pca.explained_variance_ratio_), label=\"Cumulative variance explained\")\nax = sns.scatterplot(x=[c_analyzed], y=[np.cumsum(pca.explained_variance_ratio_)[c_analyzed-1]], s=100)\nax.text(c_analyzed+1, np.cumsum(pca.explained_variance_ratio_)[c_analyzed-1], f\"{np.cumsum(pca.explained_variance_ratio_)[c_analyzed-1]:.2f}\",\\\n       horizontalalignment='left')\n\nax = sns.scatterplot(x=[c_analyzed], y=[pca.explained_variance_ratio_[c_analyzed-1]], s=100)\nax.text(c_analyzed+1, pca.explained_variance_ratio_[c_analyzed-1], f\"{pca.explained_variance_ratio_[c_analyzed-1]:.2f}\",\\\n       horizontalalignment='left')\nplt.axvline(c_analyzed, color=\"black\", ls=\"--\")\nax.set_title(\"number of componet used against PVE for the ADASYN re-balanced Dataset\")\nax.set_xlabel(\"Principal component\")\nax.set_ylabel(\"PVE\")\nplt.show()","18c8f340":"pca = PCA(n_components=X_train_smoteenn.shape[1])\n\npca.fit(X_train_smoteenn)\nfig, ax = plt.subplots(figsize=(10, 10))\nax = sns.lineplot(x=range(1, X_train_smoteenn.shape[1]+1), y=pca.explained_variance_ratio_, label=\"Variance explained by each component\")\nax = sns.lineplot(x=range(1, X_train_smoteenn.shape[1]+1), y=np.cumsum(pca.explained_variance_ratio_), label=\"Cumulative variance explained\")\nax = sns.scatterplot(x=[c_analyzed], y=[np.cumsum(pca.explained_variance_ratio_)[c_analyzed-1]], s=100)\nax.text(c_analyzed+1, np.cumsum(pca.explained_variance_ratio_)[c_analyzed-1], f\"{np.cumsum(pca.explained_variance_ratio_)[c_analyzed-1]:.2f}\",\\\n       horizontalalignment='left')\n\nax = sns.scatterplot(x=[c_analyzed], y=[pca.explained_variance_ratio_[c_analyzed-1]], s=100)\nax.text(c_analyzed+1, pca.explained_variance_ratio_[c_analyzed-1], f\"{pca.explained_variance_ratio_[c_analyzed-1]:.2f}\",\\\n       horizontalalignment='left')\nplt.axvline(c_analyzed, color=\"black\", ls=\"--\")\nax.set_title(\"number of componet used against PVE for the SMOTEENN re-balanced Dataset\")\nax.set_xlabel(\"Principal component\")\nax.set_ylabel(\"PVE\")\nplt.show()","4e8187bf":"pca = PCA(c_analyzed)\n\nX_train_pca = pca.fit_transform(X_train)\nX_test_pca = pca.transform(X_test)","ecc90863":"default_weighting = {\"TP\":0,\\\n                        \"TN\":0,\\\n                        \"FP\":1,\\\n                        \"FN\":5}\n\nadjusted_weighting = {\"TP\":0,\\\n                        \"TN\":-1,\\\n                        \"FP\":1,\\\n                        \"FN\":10}                        \n\ndef compute_penalty(y, y_hat, weight_matrix):\n    \n    penalty = 0\n    count = 0\n    for y_i, y_hat_i in zip(y, y_hat):\n        \n        # True Positive Case\n        if(y_i == 1 and y_hat_i == 1):\n            penalty += weight_matrix[\"TP\"]\n        \n        # True Negative Case\n        if(y_i == 0 and y_hat_i == 0):\n            penalty += weight_matrix[\"TN\"]\n            \n        # False Positive Case\n        if(y_i == 0 and y_hat_i == 1):\n            penalty += weight_matrix[\"FP\"]        \n            \n        # False Negative Case\n        if(y_i == 1 and y_hat_i == 0):\n            penalty += weight_matrix[\"FN\"]\n        \n        count += 1\n            \n    return penalty \/ count       ","ae7bdd0e":"results_df = pd.DataFrame(columns=[\"model_name\", \"parameters\", \"training_score (f2)\",\"f1_test\", \"f2_test\", \"accuracy_test\", \\\n                                   \"penalty_default_matrix\", \"penalty_adj_matrix\"])","fad3af18":"y_hat_baseline = np.ones((300, 1))\n\n# Train score\nf2_train = fbeta_score(y_train, np.ones((700, 1)), beta=2)\n\n# Train score\nf2_test = fbeta_score(y_test, y_hat_baseline, beta=2)\n\n# f1 score\nf1_test = f1_score(y_test, y_hat_baseline)\n\n# Accuracy\naccuracy_test = accuracy_score(y_test, y_hat_baseline)\n\n# Penalty default weight\npenalty_default = compute_penalty(y_test, y_hat_baseline, weight_matrix=default_weighting)\n\n# Penalty adjusted weight \npenalty_adjusted = compute_penalty(y_test, y_hat_baseline, weight_matrix=adjusted_weighting)\n\noutput_df = pd.DataFrame(data=[[f1_test, f2_test, accuracy_test, penalty_default, penalty_adjusted]],\\\n        columns=[\"f1_test\", \"f2_test\", \"accuracy_test\", \"penalty_default_matrix\", \"penalty_adj_matrix\"])\n\nconf_matrix = confusion_matrix(y_test, y_hat_baseline)\n\nfig, ax = plt.subplots(figsize=(7, 5))\nsns.heatmap(conf_matrix, annot=True, linewidth=0.5, xticklabels=[\"good\", \"bad\"], fmt=\"g\")\nax.set_yticklabels(labels=[\"good\", \"bad\"], rotation=0) \nax.set_xlabel(\"predicted\")\nax.set_ylabel(\"actual\")\nax.set_title(\"Confusion matrix for the baseline model\")\nplt.show()","3869fb56":"classifier_df = pd.DataFrame(data = [[\"Baseline model\", \"N\/A\" , f2_train]], \\\n        columns=[\"model_name\", \"parameters\", \"training_score (f2)\"])\nresults_df = results_df.append(pd.concat([classifier_df, output_df], axis=1, sort=False))","d035ad55":"results_df","4745b4b0":"summary_df = results_df.copy()","773822a8":"def trainClassifier(X_train, y_train, model_name, classifier, params, score, score_parameters=None,\\\n                    greater_is_better=True, verbose=False, adasyn=None, smoteenn=None, pca=None, standardize=None):\n\n    kf = StratifiedKFold(10)\n    \n    \n    train_scores = []\n\n    if greater_is_better:\n        best_score = 0\n    else:\n        best_score = sys.float_info.max\n        \n    for config in ParameterGrid(params):\n        train_scores_run = []\n        counts = []\n        for train_indices, valid_indices in kf.split(X_train, y_train):\n            counts.append(len(train_indices))\n            X_train_kf = X_train[train_indices]\n            y_train_kf = y_train[train_indices]\n            X_valid_kf = X_train[valid_indices]\n            y_valid_kf = y_train[valid_indices]\n                       \n            if standardize is not None: \n                X_train_kf = standardize.fit_transform(X_train_kf)\n                X_valid_kf = standardize.transform(X_valid_kf)\n            \n            if adasyn is not None:\n                X_train_kf, y_train_kf = adasyn.fit_resample(X_train_kf, y_train_kf)\n                \n            if smoteenn is not None:\n                X_train_kf, y_train_kf = smoteenn.fit_resample(X_train_kf, y_train_kf)\n            \n            if pca is not None:\n                X_train_kf = pca.fit_transform(X_train_kf)\n                X_valid_kf = pca.transform(X_valid_kf)\n\n            # keep track of the number of elements in each split\n            model = classifier(**config)\n            model.fit(X_train_kf, y_train_kf)\n            y_hat = model.predict(X_valid_kf)\n            train_score = score(y_valid_kf, y_hat, **score_parameters)\n            train_scores_run.append(train_score)\n\n        if(greater_is_better):\n            if np.average(train_scores_run, weights=counts) > best_score:\n                best_score = np.average(train_scores_run, weights=counts)\n                best_config = config\n                if(verbose):\n                    print(\"New best score obtained\")\n                    print(f\"Training with: {config}\")\n                    print(f\"Total Score obtained with cross validation: {best_score}\\n\")\n        else:\n            if np.average(train_scores_run, weights=counts) < best_score:\n                best_score = np.average(train_scores_run, weights=counts)\n                if(verbose):\n                    print(\"New best score obtained\")\n                    print(f\"Training with: {config}\")\n                    print(f\"Total Score obtained with cross validation: {best_score}\\n\")\n\n        train_scores.append(np.average(train_scores_run, weights=counts))\n\n    output_df = pd.DataFrame(data = [[model_name, best_config ,best_score]], \\\n        columns=[\"model_name\", \"parameters\", \"training_score (f2)\"])\n\n    return output_df\n\ndef testClassifier(X_train, y_train, X_test, y_test, classifier, best_params, adasyn=None, smoteenn=None, pca=None, standardize=None, proba=False):\n    # Train model obtained with best hyperparameters              \n    if standardize is not None: \n        X_train = standardize.fit_transform(X_train)\n        X_test = standardize.transform(X_test)\n    \n    if adasyn is not None:\n        X_train, y_train = ada.fit_resample(X_train, y_train)\n        \n    if smoteenn is not None:\n        X_train, y_train = smoteenn.fit_resample(X_train, y_train)\n        \n    if pca is not None:\n        X_train = pca.fit_transform(X_train)\n        X_test = pca.transform(X_test)\n    \n    best_model = classifier(**best_params)\n    best_model.fit(X_train, y_train)\n    y_hat = best_model.predict(X_test)\n\n    # f1 score\n    f1_test = f1_score(y_test, y_hat)\n    \n    # f2 score\n    f2_test = fbeta_score(y_test, y_hat, beta=2)\n\n    # Accuracy\n    accuracy_test = accuracy_score(y_test, y_hat)# Train model obtained with best hyperparameters\n\n    # Penalty default weight\n    penalty_default = compute_penalty(y_test, y_hat, weight_matrix=default_weighting)\n\n    # Penalty adjusted weight \n    penalty_adjusted = compute_penalty(y_test, y_hat, weight_matrix=adjusted_weighting)\n\n    output_df = pd.DataFrame(data=[[f1_test, f2_test, accuracy_test, penalty_default, penalty_adjusted]],\\\n        columns=[\"f1_test\", \"f2_test\", \"accuracy_test\", \"penalty_default_matrix\", \"penalty_adj_matrix\"])\n\n    conf_matrix = confusion_matrix(y_test, y_hat)\n    \n    if proba == False:\n        y_score = best_model.decision_function(X_test)\n    else:\n        y_score = best_model.predict_proba(X_test)[:, 1]\n        \n    fpr, tpr, thresholds = roc_curve(y_test, y_score, pos_label=1)\n    roc_auc = auc(fpr, tpr)\n    roc = [fpr, tpr]  \n    \n    return output_df, conf_matrix, roc, roc_auc\n\ndef performAnalysis(train_Xs, y_train, test_Xs, y_test, model_names, classifier,\\\n                    params, score, score_parameters, greater_is_better=True, verbose=False,\\\n                    adasyn=None, smoteenn=None, pca=None, standardize=None, proba=False):\n    \n    # Find best parameters with cross validation\n    conf_matrices = []\n    rocs = []\n    roc_aucs = []\n    \n    results_df = pd.DataFrame(columns=[\"model_name\", \"parameters\", \"training_score (f2)\", \"f1_test\", \"f2_test\",\\\n                                       \"accuracy_test\", \"penalty_default_matrix\", \"penalty_adj_matrix\"])\n    \n    for X_train, X_test, model_name in zip(train_Xs, test_Xs, model_names):\n        classifier_df = trainClassifier(X_train, y_train, model_name, classifier, params=params, score=score,\\\n             score_parameters=score_parameters, greater_is_better=True,verbose=False, adasyn=adasyn, pca=pca, smoteenn=smoteenn, standardize=standardize)\n        \n        tests_df, conf_matrix, roc, auc = testClassifier(X_train, y_train, X_test, y_test, classifier, \\\n                                                classifier_df[\"parameters\"].to_dict()[0], adasyn=adasyn, smoteenn=smoteenn, pca=pca, standardize=standardize, proba=proba)\n        conf_matrices.append(conf_matrix)\n        rocs.append(roc)\n        roc_aucs.append(auc)\n                                                                         \n        results_df = results_df.append(pd.concat([classifier_df, tests_df], axis=1, sort=False))\n                                                                \n    return results_df, conf_matrices, rocs, roc_aucs\n\ndef draw_rocs(rocs, roc_aucs, roc_titles):\n    fig, axs = plt.subplots(2, 3, figsize=(25, 10))\n    i = 0\n    j = 0\n\n    for roc, roc_auc, roc_title in zip(rocs, roc_aucs, roc_titles):\n        lw=2\n        sns.lineplot(roc[0], roc[1], color='darkorange',\n             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc, ax=axs[j, i])\n        sns.lineplot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--', ax=axs[j, i])\n        plt.xlim([0.0, 1.0])\n        plt.ylim([0.0, 1.05])\n        axs[j, i].set_xlabel('False Positive Rate')\n        axs[j, i].set_ylabel('True Positive Rate')\n        axs[j, i].set_title(roc_title)\n        axs[j, i].legend(loc=\"lower right\")\n\n        i += 1\n        if i%3 == 0 and i != 0:\n            j += 1\n            i = 0\n\n    plt.show()   \n    \ndef draw_conf_matrices(conf_matrices, plot_titles):\n    fig, axs = plt.subplots(2, 3, figsize=(25, 10))\n    i = 0\n    j = 0\n\n    for conf_matrix, plot_title in zip(conf_matrices, plot_titles):\n        sns.heatmap(conf_matrix, annot=True, linewidth=0.5, xticklabels=[\"good\", \"bad\"], ax=axs[j, i], fmt=\"g\")\n        axs[j, i].set_yticklabels(labels=[\"good\", \"bad\"], rotation=0) \n        axs[j, i].set_xlabel(\"predicted\")\n        axs[j, i].set_ylabel(\"actual\")\n        axs[j, i].set_title(plot_title)\n\n        i += 1\n        if i%3 == 0 and i != 0:\n            j += 1\n            i = 0\n    plt.show()\n\ndef run_classifier(proba=True):\n    results_df, conf_matrices, rocs, roc_aucs = performAnalysis(\\\n                                                [X_train_raw], y_train, [X_test_raw], y_test, model_names=[model_names[0]], \\\n                                                classifier=classifier, params=params, score=fbeta_score, score_parameters={\"beta\":2},\\\n                                                greater_is_better=True, verbose=False, proba=proba, standardize=scaler)\n\n    results_df_pca, conf_matrices_pca, rocs_pca, roc_aucs_pca, = performAnalysis(\\\n                                                [X_train_raw], y_train, [X_test_raw], y_test, model_names=[model_names[1]], \\\n                                                classifier=classifier, params=params, score=fbeta_score, score_parameters={\"beta\":2},\\\n                                                greater_is_better=True, verbose=False, proba=proba, pca=pca, standardize=scaler)\n\n    rocs.append(rocs_pca[0])\n    roc_aucs.append(roc_aucs_pca[0])\n    results_df = results_df.append(results_df_pca)\n    conf_matrices.append(conf_matrices_pca[0])\n\n    results_df_ada, conf_matrices_ada, rocs_ada, roc_aucs_ada = performAnalysis(\\\n                                                [X_train_raw], y_train, [X_test_raw], y_test, model_names=[model_names[2]],\\\n                                                classifier=classifier, params=params, score=fbeta_score, score_parameters={\"beta\":2},\\\n                                                greater_is_better=True, verbose=False, proba=proba, adasyn=ada, pca=None, standardize=scaler)\n\n    results_df = results_df.append(results_df_ada)\n    conf_matrices.append(conf_matrices_ada[0])\n    rocs.append(rocs_ada[0])\n    roc_aucs.append(roc_aucs_ada[0])\n\n    results_df_ada_pca, conf_matrices_ada_pca, rocs_ada_pca, roc_aucs_ada_pca = performAnalysis(\\\n                                                [X_train_raw], y_train, [X_test_raw], y_test, model_names=[model_names[3]],\\\n                                                classifier=classifier, params=params, score=fbeta_score, score_parameters={\"beta\":2}, \\\n                                                greater_is_better=True, verbose=False, proba=proba, adasyn=ada, pca=pca, standardize=scaler)\n\n    results_df = results_df.append(results_df_ada_pca)\n    conf_matrices.append(conf_matrices_ada_pca[0])\n    rocs.append(rocs_ada_pca[0])\n    roc_aucs.append(roc_aucs_ada_pca[0])\n\n    results_df_smoteenn, conf_matrices_smoteenn, rocs_smoteenn, roc_aucs_smoteenn = performAnalysis(\\\n                                                [X_train_raw], y_train, [X_test_raw], y_test, model_names=[model_names[4]],\\\n                                                classifier=classifier, params=params, score=fbeta_score, score_parameters={\"beta\":2}, \\\n                                                greater_is_better=True, verbose=False, proba=proba, smoteenn=smoteenn, pca=None, standardize=scaler)\n\n    results_df = results_df.append(results_df_smoteenn)\n    conf_matrices.append(conf_matrices_smoteenn[0])\n    rocs.append(rocs_smoteenn[0])\n    roc_aucs.append(roc_aucs_smoteenn[0])\n\n    results_df_smoteenn_pca, conf_matrices_smoteenn_pca, rocs_smoteenn_pca, roc_aucs_smoteenn_pca = performAnalysis(\\\n                                                [X_train_raw], y_train, [X_test_raw], y_test, model_names=[model_names[5]],\\\n                                                classifier=classifier, params=params, score=fbeta_score, score_parameters={\"beta\":2}, \\\n                                                greater_is_better=True, verbose=False, proba=proba, smoteenn=smoteenn, pca=pca, standardize=scaler)\n\n    results_df = results_df.append(results_df_smoteenn_pca)\n    conf_matrices.append(conf_matrices_smoteenn_pca[0])\n    rocs.append(rocs_smoteenn_pca[0])\n    roc_aucs.append(roc_aucs_smoteenn_pca[0])\n    \n    return results_df, conf_matrices, rocs, roc_aucs","316f04a3":"params = {\n    \"n_neighbors\": [1, 3, 5, 7, 9, 11, 13, 15]\n}\nclassifier = KNeighborsClassifier\nmodel_names = [\"K-NN\", \"K-NN PCA\", \"K-NN ADASYN\", \"K-NN ADASYN PCA\", \"K-NN SMOTEENN\", \"K-NN SMOTEENN PCA\"]\nconf_matr_titles = [\"Confusion Matrix K-NN not reduced dimension\", \"Confusion Matrix K-NN PCA\",\\\n               \"Confusion Matrix K-NN ADASYN\", \"Confusion Matrix K-NN ADASYN PCA\",\\\n               \"Confusion Matrix K-NN SMOTEENN\", \"Confusion Matrix K-NN SMOTEENN PCA\"]\n\nroc_titles = [\"ROC K-NN not reduced dimension\", \"ROC K-NN PCA\",\\\n              \"ROC K-NN ADASYN\", \"ROC K-NN ADASYN PCA\",\\\n              \"ROC K-NN SMOTEENN\", \"ROC K-NN SMOTEENN PCA\"]\n              ","f79b1a41":"results_df, conf_matrices, rocs, roc_aucs = run_classifier(proba=True)\n\nsummary_df = summary_df.append(results_df)\n\nresults_df","93dba615":"draw_conf_matrices(conf_matrices, conf_matr_titles)","02e2c6fc":"draw_rocs(rocs, roc_aucs, roc_titles)","e72d22b3":"params = {\n    \"C\": [1e-3, 1e-2, 1e-1, 1],\n    \"max_iter\": [30000]\n}\nclassifier = LinearSVC\nmodel_names = [\"Linear SVM\", \"Linear SVM PCA\",\\\n               \"Linear SVM ADASYN\", \"Linear SVM ADASYN PCA\",\\\n               \"Linear SVM SMOTEENN\", \"Linear SVM SMOTEENN PCA\"]\nconf_matr_titles = [\"Confusion Matrix Linear SVM not reduced dimension\", \"Confusion Matrix Linear SVM PCA\",\\\n                    \"Confusion Matrix Linear SVM ADASYN\", \"Confusion Matrix Linear SVM ADASYN PCA\",\\\n                    \"Confusion Matrix Linear SVM SMOTEENN\", \"Confusion Matrix Linear SVM SMOTEENN PCA\"]\n\nroc_titles = [\"ROC Linear SVM not reduced dimension\", \"ROC Linear SVM PCA\",\\\n              \"ROC Linear SVM ADASYN\", \"ROC Linear SVM ADASYN PCA\",\\\n              \"ROC Linear SVM SMOTEENN\", \"ROC Linear SVM SMOTEENN PCA\"]","4f53ba1f":"results_df, conf_matrices, rocs, roc_aucs = run_classifier(proba=False)\n\nsummary_df = summary_df.append(results_df)\n\nresults_df","f83d04d4":"draw_conf_matrices(conf_matrices, conf_matr_titles)","71fc32bd":"draw_rocs(rocs, roc_aucs, roc_titles)","a93d387c":"params = {\n    \"kernel\" : [\"rbf\"],\n    \"C\": [1e-4, 1e-3, 1e-2, 1e-1, 1, 10],\n    \"gamma\": [1e-3, 1e-2, 1e-1, 1, 10]\n}\nclassifier = SVC\nmodel_names = [\"RBF SVM\", \"RBF SVM PCA\",\\\n               \"RBF SVM ADASYN\", \"RBF SVM ADASYN PCA\",\\\n               \"RBF SVM SMOTEENN\", \"RBF SVM SMOTEENN PCA\"]\nconf_matr_titles = [\"Confusion Matrix RBF SVM not reduced dimension\", \"Confusion Matrix RBF SVM PCA\",\\\n                    \"Confusion Matrix RBF SVM ADASYN\", \"Confusion Matrix RBF SVM ADASYN PCA\",\\\n                    \"Confusion Matrix RBF SVM SMOTEENN\", \"Confusion Matrix RBF SVM SMOTEENN PCA\"]\n\nroc_titles = [\"ROC RBF SVM not reduced dimension\", \"ROC RBF SVM PCA\",\\\n              \"ROC RBF SVM ADASYN\", \"ROC RBF SVM ADASYN PCA\",\\\n              \"ROC RBF SVM SMOTEENN\", \"ROC RBF SVM SMOTEENN PCA\"]","131cc68f":"results_df, conf_matrices, rocs, roc_aucs = run_classifier(proba=False)\n\nsummary_df = summary_df.append(results_df)\n\nresults_df","616a403c":"draw_conf_matrices(conf_matrices, conf_matr_titles)","aba52525":"draw_rocs(rocs, roc_aucs, roc_titles)","7e565560":"params = {\n    \"C\": [1e-3, 1e-2, 1e-1, 1, 10]\n}\nclassifier = LogisticRegression\nmodel_names = [\"Logistic Regression\", \"Logistic Regression PCA\",\\\n               \"Logistic Regression ADASYN\", \"Logistic Regression ADASYN PCA\",\\\n               \"Logistic Regression SMOTEENN\", \"Logistic Regression SMOTEENN PCA\"]\nconf_matr_titles = [\"Confusion Matrix Logistic Regression not reduced dimension\", \"Confusion Matrix Logistic Regression PCA\",\\\n               \"Confusion Matrix Logistic Regression ADASYN\", \"Confusion Matrix Logistic Regression ADASYN PCA\",\\\n                   \"Confusion Matrix Logistic Regression SMOTEENN\", \"Confusion Matrix Logistic Regression SMOTEENN PCA\"]\n\nroc_titles = [\"ROC Logistic Regression not reduced dimension\", \"ROC Logistic Regression not reduced dimension PCA\",\\\n              \"ROC Logistic Regression ADASYN\", \"ROC Logistic Regression ADASYN PCA\",\\\n              \"ROC Logistic Regression SMOTEENN\", \"ROC Logistic Regression SMOTEENN PCA\"]\n","9523d8cc":"results_df, conf_matrices, rocs, roc_aucs = run_classifier(proba=True)\n\nsummary_df = summary_df.append(results_df)\n\nresults_df","223a27fe":"draw_conf_matrices(conf_matrices, conf_matr_titles)","ca25badb":"draw_rocs(rocs, roc_aucs, roc_titles)","8bbc20ca":"params = {\n    \"max_depth\": [None, 4, 6, 8, 10],\n    \"min_samples_split\": [2, 0.2, 0.4],\n    \"min_samples_leaf\": [1, 0.2]\n}\nclassifier = DecisionTreeClassifier\nmodel_names = [\"Decision Tree\", \"Decision Tree PCA\",\\\n               \"Decision Tree ADASYN\", \"Decision Tree ADASYN PCA\",\\\n               \"Decision Tree SMOTEENN\", \"Decision Tree SMOTEENN PCA\"]\nconf_matr_titles = [\"Confusion Matrix Decision Tree not reduced dimension\", \"Confusion Matrix Decision Tree PCA\",\\\n                    \"Confusion Matrix Decision Tree ADASYN\", \"Confusion Matrix Decision Tree ADASYN PCA\",\\\n                    \"Confusion Matrix Decision Tree SMOTEENN\", \"Confusion Matrix Decision Tree SMOTEENN PCA\"]\nroc_titles = [\"ROC Decision Tree not reduced dimension\", \"ROC Decision Tree PCA\",\\\n              \"ROC Decision Tree ADASYN\", \"ROC Decision Tree ADASYN PCA\",\\\n              \"ROC Decision Tree SMOTEENN\", \"ROC Decision Tree SMOTEENN PCA\"]","ca7adfcf":"results_df, conf_matrices, rocs, roc_aucs = run_classifier(proba=True)\n\nsummary_df = summary_df.append(results_df)\n\nresults_df","8aefd8f0":"draw_conf_matrices(conf_matrices, conf_matr_titles)","f55b596c":"draw_rocs(rocs, roc_aucs, roc_titles)","0249a8c6":"params = {\"max_depth\": [3,5, 7, 10,None],\n          \"n_estimators\":[3,5,10,25,50],\n          \"max_features\": [4,7,15,20, \"auto\"]}\nclassifier = RandomForestClassifier\nmodel_names = [\"Random Forest\", \"Random Forest PCA\",\\\n               \"Random Forest ADASYN\", \"Random Forest ADASYN PCA\",\\\n               \"Random Forest SMOTEENN\", \"Random Forest SMOTEENN PCA\"]\nconf_matr_titles = [\"Confusion Matrix Random Forest not reduced dimension\", \"Confusion Matrix Random Forest PCA\",\\\n                    \"Confusion Matrix Random Forest ADASYN\", \"Confusion Matrix Random Forest ADASYN PCA\",\\\n                    \"Confusion Matrix Random Forest SMOTEENN\", \"Confusion Matrix Random Forest SMOTEENN PCA\"]\nroc_titles = [\"ROC Random Forest not reduced dimension\", \"ROC Random Forest PCA\",\\\n              \"ROC Random Forest ADASYN\", \"ROC Random Forest ADASYN PCA\",\\\n              \"ROC Random Forest SMOTEENN\", \"ROC Random Forest SMOTEENN PCA\"]","8c42a6f0":"results_df, conf_matrices, rocs, roc_aucs = run_classifier(proba=True)\n\nsummary_df = summary_df.append(results_df)\n\nresults_df","ed557817":"draw_conf_matrices(conf_matrices, conf_matr_titles)","b6a997ea":"draw_rocs(rocs, roc_aucs, roc_titles)","d0dbb40e":"summary_df = summary_df.reset_index(drop=True)\nf2_df = summary_df[[\"f2_test\", \"model_name\"]].copy()\nf2_df.loc[0, \"model_type\"] = \"Baseline model\"\nf2_df.loc[1:len(model_names)+1, \"model_type\"] = \"K-NN\"\nf2_df.loc[len(model_names)+1:2*len(model_names)+1, \"model_type\"] = \"Linear SVM\"\nf2_df.loc[2*len(model_names)+1:3*len(model_names)+1, \"model_type\"] = \"RBF SVM\"\nf2_df.loc[3*len(model_names)+1:4*len(model_names)+1, \"model_type\"] = \"Logistic Regression\"\nf2_df.loc[4*len(model_names)+1:5*len(model_names)+1, \"model_type\"] = \"Decision Tree\"\nf2_df.loc[5*len(model_names)+1:6*len(model_names)+1, \"model_type\"] = \"Random Forest\"\n\nf2_df = f2_df.sort_values(by=[\"f2_test\"], ascending=False)\n\nfig, ax = plt.subplots(figsize=(15, 15))\n\nax = sns.barplot(y=f2_df[\"model_name\"], x=f2_df[\"f2_test\"], hue=f2_df[\"model_type\"])\nax.set_title(\"Final results for the different classification models\")\nplt.show()","64b85721":"summary_df","bf5a7210":"After the label encoding we end up with $49$ columns instead of the original $21$ (so there are $48$ features).","e652cb3b":"### Dimensionality reduction","610b9074":"A beatiful way to look at the relationships between the numerical variables is through pairplots. On the diagonal we can see the normalized histograms with the distribution of the quantitative features, while on the other cells we can see the scatter plot of one numerical variable against another one.","b43cba52":"### K-Nearest Neighbors","c19802fa":"As we can see the highest value of correlation between two dummy encoded variables is of $0.78$. The variable housing_status_153 assumes value $1$ when the applicant lives for free at the expenses of another person, while the variable property_A124 assumes value $1$ when the individual does not own a property. So this correlation value is quite explicable.\n\nA common threshold used in machine learning for eliminating variables due to high correlations is $0.85$, this threshold is quite arbitrary and can be problem dependent. In this situation I decided to leave both the variables (we will use PCA in order to get uncorrelated features).","d88664b4":"### Oversampling and Undersampling","23fb4bac":"We said earlier that undersampling may not be the best solution in such a small dataset, but what if we apply first an oversampling tecnique and then an undersampling one? This is what SMOTEENN does: it applies first the SMOTE oversampling algorithm on the minority class to balance the dataset, then the ENN algorithm is applied, this is an undersampling algorithm which has the goal of cleaning the dataset from the misleading examples.","ece35d52":"<div class=\"cite2c-biblio\"><\/div>","95855c99":"### Code for performing Cross Validation","27cc9be9":"With the describe method we can see some basics statistics about the quantitative features","96806f11":"PCA aims at finding a lower dimensional subspace in which the original data is projected. The basis of this new subspace with lower dimension is given by the principal components. The first principal component found is that direction which preserve most of the variance of the data. The second principal component is the one which preserves most of the variance of the data while being ortogonal to the first and so on.\nLet's define a vector $\\mathbf{x} \\in \\mathbb{R}^d$ containing one sample of our data. Now we can define matrix $W\\in \\mathbb{R}^{n, d}$ which projects our vector $\\mathbf{x}$ into a lower dimensional subspace $\\mathbb{R}^n$ with $n\\leq d$, we call $W$ projection matrix.\nLet's define another matrix $U\\in \\mathbb{R}^{d, n}$ which projects the reduced vector $W\\mathbf{x}$ back to $\\mathbf{R}^d$, the original dimension; we call $U$ the reconstruction matrix. A reasonable way to choose $U$ and $W$ is in such a way that the reconstructed vector $\\tilde{\\mathbf{x}}=UW\\mathbf{x}$ is similar to the original vector $\\mathbf{x}$. A perfect reconstruction is in general impossible, this is a consequence of the Rouch\u00e8-Capelli theorem, a foundamental result in linear algebra. In fact in general if we have a matrix $A\\in \\mathbb{R}^{n, d}$ with $n<d-1$ and two vectors $\\mathbf{u},\\mathbf{v}$ with $\\mathbf{u}\\neq \\mathbf{v}$ we can find $\\mathbf{u},\\mathbf{v}$ such that $A\\mathbf{u}=A\\mathbf{v}$. That's because  by definition $A$ has more columns than rows, then its null space won't be the zero vector. The solution of the system $A\\mathbf{u}=\\mathbf{b}$ will have a particular solution plus the null space of $A$.\n\nSince perfect reconstruction is in general not achievable we can formulate the PCA problem as finding the matrices $U, V$ which minimize the reconstruction error. In math terms:\n\n$\\arg\\min_{W\\in \\mathbb{R}^{n, d}, U\\in \\mathbb{R}^{d, n}}\\sum_{i=1}^m\\|\\mathbf{x}_i-UW\\mathbf{x}_i\\|_2^2$\n\nwhere $m$ is the number of samples present in the dataset.\n\nIt can be shown that at the minimum of the problem the columns of $U$ are orthonormal (namely $U^TU=I_{n\\times n}$) and that $W=U^T$.\n\nThen we can write the optimization problem as follows:\n$\\arg\\min_{U\\in \\mathbb{R}^{d, n}, U^TU=I}\\sum_{i=1}^m\\|\\mathbf{x}_i-UU^T\\mathbf{x}_i\\|_2^2$.\nUsing elementary algebraic operations we can rewrite the problem as:\n$\\|\\mathbf{x}_i-UU^T\\mathbf{x}_i\\|_2^2=...=\\|\\mathbf{x}\\|^2-trace(U^T\\mathbf{x}\\mathbf{x}^TU)$,\nwhere the trace is the sum of the diagonal entries of a matrix. Since the trace is linear we can rewrite the optimization problem as follows:\n\n$\\arg\\max_{U\\in \\mathbb{R}^{d, n}, U^TU=I}trace(U^T\\sum_{i=1}^n\\mathbf{x}_i\\mathbf{x}_i^TU)$\n\nWe can easily solve this problem and we get as result that the matrix which satisfies this condition is the orthonormal matrix $U\\in\\mathbb{R}^{d, n}$  which has in it's columns the first $n$ eigenvectors $\\mathbf{u}_1,...\\mathbf{u}_n$ of the matrix $X^TX$ sorted according to the largests eigenvalues.\n\nWe can also see the problem as a variance maximization problem. In fact if we first center our data (we remove the sample mean of each column) so that it has mean zero and we call $X$ the matrix containing all of our data, $X\\in\\mathbb{R}^{m, d}$ then we can define the covariance matrix as $\\frac{1}{n-1}\\mathbf{X}^T\\mathbf{X}=\\Sigma, s.t.\\bar{\\mu}_X=\\bar{0}$.\nNow if we define the first projected principal component as $\\mathbf{z}_1=a_{1,1}x_{i}+...+a_{1,d}x_{d}$, where the $a_{i,j}$ are the so called loadings we can write the following optimization problem:\n\n\\begin{equation*}\\max_a Var(\\mathbf{z}_1)\\end{equation*}\n<center>s.t.<\/center>\n\n\\begin{equation*}\\sum_{i=1}^{d}a_{1,i}^2=1\\end{equation*}\nwhere the constraint on the loadings is to ensure compactness of the dominium. This problem is equivalent of writing:\n\n\\begin{equation*}\\max_{\\mathbf{z}_1} \\mathbf{z}_1^T\\Sigma\\mathbf{z}_1\\end{equation*}\n<center>s.t.<\/center>\n\n\\begin{equation*}\\mathbf{z}_1^T\\mathbf{z}_1=1\\end{equation*}\n\nthis is a quadratic optimization problem, we can solve it using the lagrangian function:\n$\\mathcal{L}(\\mathbf{z}, \\lambda) = \\mathbf{z}_1^T\\Sigma\\mathbf{z}_1 - \\lambda(\\mathbf{z}_1^T\\mathbf{z}_1-1)$\n\nIf we differentiate it and set the partial derivative with respect to $\\mathbf{z}_1$ to $0$ we get the exact solution of the other proof. For getting the other principal components we proceed in the same way but we add as a constraint the orthogonality from the previous principal components: $\\mathbf{z}_i\\Sigma\\mathbf{z}_j=0, \\forall j\\leq i$","9476b98b":"We will now perform a grid search for:\n- $\\text{max_dept} = \\text{None}, 4, 6, 8, 10$\n- $\\text{min samples split} = 2, 0.2\\%, 0.4\\%$\n- $\\text{min samples leaf} = 1, 0.2\\%$","94b5730c":"### Logistic Regression","7435a15c":"### A baseline","03bf4b84":"We will now fit a Decision Tree classifier for various hyperparameters. In particular we will tune the maximum depth of the tree (in order to not overfit we could decide to stop earlier and assign the label in the leaf to the majority class), the minum number of samples needed for performing a split on an internal node and the minum number of samples required to be a leaf node.","41222b6e":"Before introducing this tecnique I should make a clarification: PCA is not built to be applied to datasets containing categorical variables, but should only be used in dataset containing continuous variables. In the case of categorical variables there are more advanced tecniques such as MCA (Multiple Correspondence Analysis) and for mixed datasets there exist tecniques such as FAMD (Factor Analysis of Mixed Data) which combines both PCA and MCA or a generalization of PCA for mixed dataset called PCAMixed. Nevertheless in the following section we will apply PCA to the dataset which has continuous and dummy encoded variables, in order to check the performance and the validity of this method, knowing that there are ad hoc solutions not explored here. I should cite some papers which apply PCA to dummy encoded variables in particular in [1] we find this tecnique applied. In [2] there is a comparison between the tecnique of using dummy variable described in [1] for PCA and other more advanced instruments.","9ddc02fa":"**SMOTE**\n\nThe Synthetic Minority Over-sampling Technique (SMOTE) is an algorithm very similar to ADASYN, in fact we could say that ADASYN is a generalization of SMOTE. The only difference is that with SMOTE the number of synthetic samples generated is the same for each neighborhood. The synthetic generation process is exactly the same as in ADASYN\n\n**ENN**\n\nThe Edited Nearest Neighbors (ENN) on the other hand is a undersampling tecnique, it works by computing for each point of the class in exam its $k$-nearest neighbors (typically $k=3$), the point considered is removed if the number of neighbors of the other classes is higher than the number of neighbors belonging to the point's class. In other words the point is removed if the sample is misclassified by the $k$-NN algorithm in the dataset. This helps to remove noisy and borderline examples; the idea is to facilitate the learning of the algorithm. The problem here is that the discarded data can have some useful information. Interestingly ENN can be used both in in the majority class and in the minority one.","27fb0e6c":"As we can see the boxplot reveals some outliers, but they are not wrong or out of scale values so the decision to keep them has been made","52bc8e25":"A common problem in machine learning is that the models need specific hyperparameters and this set of parameters is higly problem dependent. So every time we want to apply some ML algorithm we must perform a tuning phase in which we fit different models with different parameters and we keep the best model. A common error is to perform this tuning on the training set. This would cause major overfitting. In fact it could happen that the model achieves fantastic performances on the training set, but very poor on the test set. For this reason there are various validation tecniques that we can use.\n\n**Holdout**\nThis is the most simple validation technique. It consists in splitting the data in three sets, training, validation and testing. The training set is used for training the model, the validation set to evaluate the model in order to find the best hyperparameters and finally the test set for testing the model performances on data which was never seen by the algorithm during training.\n\n**k-Fold Cross Validation**\nThe problem with the holdout tecnique is that if the dataset has not so many samples we can incur in the situation of not using enough data for training, and this could be a problem, because machine learning algorithms need lots of data.\nTo cope with this problem we can use cross validation.\nThe goal of this method is to give an accurate estimate of the metrics that we are evaluating without wasting too much data.\nIn this case we do not split our dataset into training, validation and testing, but only into training and testing. The training set is then partitioned in $k$ folds of size $\\frac{m}{k}$ ($m$ is the size of the training set). Then we leave one fold out and we train on the remainng $k-1$ folds. We evaluate on the fold which was left out from the training. We repeat this procedure $k$ times, one from each fold, obtaining $k$ cross validation scores. We then compute the average of these scores. Usually $k$-Fold CV is used for small and medium datasets in order to have more accurate scores with respect to holdout. When the dataset is big usually the holdout tecnique is used, because of course is faster than $k$-fold CV and the data is enough to ensure convergence of the model on the training set.\n<figure>\n<img src=\"https:\/\/gialbo.github.io\/Analysis-on-the-German-Credit-Risk-Dataset\/media\/KfoldCV.gif\" style=\"width: 400px;\">\n    <figcaption><center><i>$k$-Fold Cross validation in action. <a href=\"https:\/\/en.wikipedia.org\/wiki\/Cross-validation_(statistics)\"> Source<\/a><\/i><\/center><\/figcaption>\n<\/figure>\n\n**Leave One Out**\nA particular case of cross validation is the Leave One Out CV. In this case if we call $m$ the number of samples of the dataset we use $m$ folds. Then at each step we use only one sample for validation and the others for training, then we average the reult. This is particularly useful to squeeze the most from the dataset and it is done only for very small datasets. Of course this is a very expensive tecnique and can require a lot of time on bigger datasets.\n\n**Stratified CV**\nIt is important to point out that the cross validation is performend in such a way to preserve the label distribution of the dataset. This tecnique is called Stratified Cross Validation.\n<figure>\n<img src=\"https:\/\/gialbo.github.io\/Analysis-on-the-German-Credit-Risk-Dataset\/media\/stratCV.png\" style=\"width: 400px;\">\n    <figcaption><center><i>Stratified $k$-Fold Cross validation in action. <a href=\"https:\/\/stats.stackexchange.com\/questions\/49540\/understanding-stratified-cross-validation\"> Source<\/a><\/i><\/center><\/figcaption>\n<\/figure>\n\nIn the following sections we will use a $10$-Fold Stratified Cross Validation on the training set. It is important to note that in order to make a better tuning the ADASYN and SMOTEENN techniques must only be applied on the indices used for training, so at each CV step we apply to the train data ADASYN or SMOTEENN (in the case of ADASYN+PCA we apply PCA here after ADASYN) in order to not alterate the validation process. The data used for validation in fact must belong to the underlying data distribution, without any alteration.","2ff9c901":"We are going to apply ADASYN to our training set with $k=5$ and $\\beta=1$","3d075a69":"As we saw the cost of a false negative for the bank is higher than the cost of a false positive. We are then interested in a F-measure that will summarize a model\u2019s ability to minimize misclassification errors for the positive class, but we want to favor models that are better are minimizing false negatives over false positives.\nWe can obtain this by using a measure which computes the <i>weighted<\/i> harmonic mean between precision and recall and favors high recall scores over precision ones. This measure is a generalization of the $F_1$ measures and is called $F_{\\beta}$, where $\\beta$ is the parameter that defines the weighting of the two scores.\n\\begin{equation*}\nF_{\\beta}=\\frac{(1+\\beta^2)\\times Precision\\times Recall}{\\beta^2\\times Precision+Recall}\n\\end{equation*}\n\nA $\\beta$ of $2$ will give more weight to recall over precision, this is the measure that we will use in the dataset and it is calleD the $F_2$ measure:\n\\begin{equation*}\nF_2=\\frac{5\\times Precision\\times Recall}{4\\times Precision+Recall}\n\\end{equation*}\n\nThe $F_2$ measure will be used for training, all of the other introduced measures will be reported for testing as well in order to make a comparison.","dcf4c09e":"### Decision Trees","59283bcd":"Random forests is an example of an ensemble learning method in which different machine learning models are combined in order to get a better predictive performance. So random forests work by building $B$ different decision trees $f_B$ and making decisions by taking majority voting.\nThe term <i>random<\/i> in random forests is related to two key concepts:\n\n- A random sample (with replacement) of the training set is used for building each tree. This tecnique is called bootstrap: all of the $B$ decision trees in the forest are fitted using a bootstrapped set of the original training set.\n- At each splitting step not all the features of the dataset are considered, but only a random sample of them. The number of features selected $d$ can vary and it is an hyperparameter. The most common choice is to use $d=\\sqrt{p})$, where $p$ is the original number of features. In the particular case in which $d=p$ (we are using all the features at each step) the tecnique is not called anymore random forests but bagging. The advantage of using random forests over bagging is that using less features at each step will help to decorrelate the various different trees in the forest.","3950144a":"### Data Preprocessing","f2940c31":"We can now see the overall performances of the algorithms on the fitted dataset.","defb4c2c":"We will now plot the correlation matrix between the numerical variables in order to check whether there are some higly correlated features. The idea here is that if two features are highly correlated this might imply that one variables is described by the other, so one of the two correlated variables could be discarded simplifying the problem.","5b51e446":"#### Data Standardization","51ca26cc":"We choose to use $30$ components which preserve a PVE of around $85\\%$.","bb542f5f":"In the case of the german credit dataset accuracy may not be the best metric to assess the performances of our model. That's basically for two reasons: first the dataset is imbalanced (at least for the models fitted without ADASYN or SMOTEENN), second for the bank issuing the credit a false negative is way more problematic than a false positive. In fact the bank wants to maximize its profit so it is worse to give money to an applicant which has a bad credit score (False negative) than not giving money to an applicant that has a good credit score (False positive). In the first case in fact the bank incurs in the risk of losing all the amount of money lended, while in the second case if the bank denies a loan to an applicant which has an high likelihood of repaying it risks of losing only the interests.\nFor this reason attached to the dataset description is present a penalty matrix indicating how each outcome of the classification should be penalized. In particular the matrix suggested is:\n\n<figure>\n<img src=\"https:\/\/gialbo.github.io\/Analysis-on-the-German-Credit-Risk-Dataset\/media\/penalty_matrix.jpg\" style=\"width: 400px;\">\n    <figcaption><center><i>Penalty matrix<\/i><\/center><\/figcaption>\n<\/figure>\n\nAs a score measure we could use the average penalty of the dataset, of course in this case lower is better.\n\nwe could argue that a true negative should have a negative penalty, while a false negative an higher penalty. In the literature other penalty matrices have been proposed to deal with this dataset (other matrices are reported in [3]). In order to compare different way of penalize our models we introduce an adjusted penalty matrix defined as:\n<figure>\n<img src=\"https:\/\/gialbo.github.io\/Analysis-on-the-German-Credit-Risk-Dataset\/media\/penalty_matrix_adj.jpg\" style=\"width: 400px;\">\n    <figcaption><center><i>Penalty matrix adjusted<\/i><\/center><\/figcaption>\n<\/figure>","e93227b8":"Among all the various machine learning algorithms the $k$-Nearest Neighbors based techniques are the simplest ones. The main idea behind this technique is to assign the label of a given test sample $\\mathbf{x}$ by looking at the labels of the nearest $k$ samples in the feature space and choosing as class for the sample the most rapresented label in its neighborhood.  So we can say that k-NN is a family of algorithms, one for each value of k. It is importat to note that in order to use\n$k$-NN we must define a distance in the feature space to evaluate how far is any point from the given input set.\n$k$-NN is a very lazy technique, there is not a real model built from input data in the training phase, but\nthe training samples are stored in a lookup table and in the test phase the distances between the training\nsamples and the test input are computed. Finally a majority vote is taken in the neighborhood of the\nsample to assign its label, so we can see $k$-NN as a searching problem.\n\n<figure>\n    <table><tr><td bgcolor=\"white\"><img src=\"https:\/\/gialbo.github.io\/Analysis-on-the-German-Credit-Risk-Dataset\/media\/knn-tc.PNG\" style=\"width: 200px;\"\/><\/td><td bgcolor=\"white\"><img src=\"https:\/\/gialbo.github.io\/Analysis-on-the-German-Credit-Risk-Dataset\/media\/knn-class.PNG\" style=\"width: 200px;\"\/><\/td><\/tr><\/table>\n    <figcaption><center><i>$k$-NN in action with $k=3$ at test time<\/i><\/center><\/figcaption>\n<\/figure>\n\nOne of the main problem of $k$-NN is that the algorithms needs to compute the distance from every point in the dataset at test time. So even if the training is very fast (just store the training data), the testing phase can take a longer time. This problem is explained in detail in the this GIF:\n<figure>\n    <img src=\"https:\/\/gialbo.github.io\/Analysis-on-the-German-Credit-Risk-Dataset\/media\/KNN-Classification.gif\", style=\"width: 400px;\">\n    <figcaption><center><i>$k$-NN for $k=5$. <a href=\"https:\/\/machinelearningknowledge.ai\/k-nearest-neighbor-classification-simple-explanation-beginners\/\">Source<\/a><\/i><\/center><\/figcaption>\n<\/figure>\n\nA common choice for the distance is to use the Euclidean Distance, therefore we will use it, but $k$-NN can also be applied with other distances too.\n\nWe will now perform a grid search for:\n- $K = 1, 3, 5, 7, 9, 11, 13, 15$","8a36e4db":"When a bank (or another credit institute) receives an application for a loan it must perform a risk assessment in order to evaluate whether or not the applicant is eligible for credit. In the case the specific individual has a good credit risk it is more likely that he or she will repay the debt. Viceversa if an individual has a bad credit risk then there is an highly likelihood that the bank will lose money if it accepts to give a loan to the applicant. In this analysis I will focus on trying to predict the risk score of individuals using socio-economical factors. I will use the <a href=\"https:\/\/archive.ics.uci.edu\/ml\/datasets\/Statlog+%28German+Credit+Data%29\">German Credit Dataset<\/a> (Statlog), which contains 1000 samples: 20 features and 1 response variable. The features describe the demographic and socio-economical situation of every sample, while the response variable indicates the risk score for the individual using two values: good risk score (numerical value $1$ in the dataset) and bad risk score (numerical value $2$ in the dataset). This problem is then a supervised binary classification problem.","11d67bc5":"#### Dummy Encoding","123bfb19":"#### SVM with RBF Kernel","22af77d4":"We will now perform a grid search for:\n- $C =  1e-3, 1e-2, 1e-1, 1, 10$  ","31a6643e":"Note how we oversample only the training set. This will be useful to compare the PCA results in the next section. During training we will perform cross validation so we will **NOT** use the data fitted in the cell above, but we will apply ADASYN and SMOTEENN only on the train indices of cross validation at each fold.","e1ffcb35":"#### Train-test split","088b84a1":"We will now explore the data in order to understand it and find some patterns. Let's start by loading the german.data file into a Pandas DataFrame","0f60d229":"#### Contingency Tables","2d1a6e8b":"### Data exploration","58555c63":"### Classification Task","bda05a6f":"Decision trees are algorithms that use a tree-like structure to make predictions. In particolar each internal node of the tree performs a test on a predictor (for example attribute $foreign=yes$) while each branch represents the outcome of the test. Each leaf node represent the outcome of the classification (label). Then the various paths from the root to the leaves are decision rules on the data.\n\n<figure>\n<img src=\"https:\/\/gialbo.github.io\/Analysis-on-the-German-Credit-Risk-Dataset\/media\/tree_credit.PNG\" style=\"width: 400px;\">\n    <figcaption><center><i>Example of decision tree for a generic credit risk analysis. <a href=\"https:\/\/medium.com\/@ryotennis0503\/what-is-decision-tree-345d8d27c350\">Source<\/a><\/i><\/center><\/figcaption>\n<\/figure>\n\nOne great advantage of Decision Trees is that they are very interpretable, furthermore decision trees can naturally handle categorical variables. Of course the decision tests can also be generalized to continuous data. This is done basically in two ways:\n\n- Binary decision: split according to $A<v$ or $A>v$ where $A$ is a continuous attribute and $v$ is an arbitrary number.\n- Discretization: form and ordinal categorical attribute from a continuous one. This can be done in various ways, for example by equal interval bucketing, equal frequency bucketing (percentiles) or by clustering. The discretization can be done at the beginning of the tree or at each split. So at each step, we sort the observations by the continuous attribute and we discretize according to the threshold defined (C4.5 and CART use this tecnique).\n\nIn general a split can be of two types:\n- Binary: only two branches as result of a test.\n- Multiway: more than two branches as result of a test.\n\nIn order to build a decision tree we need an algorithm which chooses the best possible rules. Unfortunately find an optimal decision tree is an NP-Hard problem, so it is computationally intractable. Nevertheless various algorithms have emerged during the years for the tree-building process, these are greedy algorithms, this means that they build the tree by considering the best possible split <i>at each step<\/i>. The resulting tree may not be optimal, but usually provides good results.\n\nBut how can we measure if a split is better than another? How can we find the best possible rule at each step?\nThe main idea is that splits in which there is  <i>homogeneity<\/i> between classes should be preferred. At each node of the tree we want to have the class distribution as homogeneus as possible, this means that most of the elements should belong to one class and only a little fraction of samples to the other class. Having said so we can see that the maximum possible homogenity is when in a split there are only samples for one class and the other is not represented. Viceversa the lowest possible value of homogeneity is when the classes are balanced in the split. \nThus homogeneity in the context of decision tree can be seen as a measure of node purity: the goal of the decision tree is to choose among all possible splits at each step the one which minimizes the node impurity, or maximizes the homogeneity.\n\nThere are several indexes for measuring homogeneity, some of the most important are:\n\n- **GINI Index**: Defined as \\begin{equation*}GINI(t)=1-\\sum_i[p(i|t)]^2\\end{equation*} where $p(i|t)$ is the relative frequency of class $i$ at the node $t$ of the tree. The GINI index assume it's maximums value $\\frac{1}{\\text{num classes}}$ when the classes are equally represented, this means that there is an high impurity at the node. Viceversa it assumes its minimum values $0$ when all elements belong to one class and the others are not represented. We can define the quality of a split using the GINI index in the case of a multiway splitting of a parent node $p$ with $k$ partitions by computing the impurity measure of the particular split: \\begin{equation*}GINI_{split}=\\sum_{i=1}^{k}\\frac{n_i}{n_p}GINI(i)\\end{equation*}. Where $n_i$ is the number of records at child $i$, $n_p$ is the number of record at the parent node $p$ and $GINI(i)$ is the GINI index at the child node $i$. When building the tree the split which at each step has the minimum value of $GINI_{split}$ is choosen.This is the strategy used in the CART algorithm (also used in SLIQ, SPRINT). \n\n- **Entropy Index**: Defined as \\begin{equation*}Entropy(t)=\\sum_i-p(i|t)\\log_2p(i|t)\\end{equation*}. In this case too the maximum for entropy is for equally distributed classes at split $t$ and Entropy assumes value $log_2(\\text{num_classes})$ implying and high level of impurity. The minimum is at $0$ when only one class is represented and implies a low level of impurity. The algorithms that use this type of index usually want to perform the splits by looking at the gain in information that each splits provides, thus they look at an index derived from Entropy called Information Gain defined in the case of multiway splitting of a parent node $p$ into $k$ child nodes as: \\begin{equation*}GAIN_{split}=Entropy(p)-\\sum_{i=1}^{k}\\frac{n_i}{n_p}Entropy(i)\\end{equation*}. At each node, the split which maximizes the IG index is choosen. This is the strategy adopted by ID3, C4.5. Information gain has a problem: it tends to prefer splits yielding a smaller number of partitions, each small, but pure. To overcome this problem another measure has been proposed, Gain Ratio: \\begin{equation*}GainRATIO_{split}=\\frac{GAIN_{split}}{SplitINFO}\\end{equation*}, where $SplitINFO$ is defined as: $SplitINFO=-\\sum_{i=1}^k\\frac{n_i}{n_p}\\log(\\frac{n_i}{n_p})$. The idea is to adjust the gain according to the Entropy of the partitioning SplitINFO. Higher entropy partitioning (high number of small partitions) is penalized. This strategy is used in C4.5.\n\nIn general the tree building process continues until all samples have been classified correctly(impurity is minimized). This means that the accuracy computed by evaluating the algorithm on the training set is $100\\%$. This of course is not true for the test set.\n<figure>\n<img src=\"https:\/\/gialbo.github.io\/Analysis-on-the-German-Credit-Risk-Dataset\/media\/tree_classification.gif\" style=\"width: 600px;\">\n    <figcaption><center><i>How the classification is performed for decision trees. <a href=\"http:\/\/www.r2d3.us\/visual-intro-to-machine-learning-part-1\/\">Source<\/a><\/i><\/center><\/figcaption>\n<\/figure>\n\nThere are different algorithms for solving decision trees, for example ID3, C4.5, CART and each one uses different impurity measures, different way for performing the splits and so on. In our analysis we will use an optimized version of the CART algorithm which uses the GINI index as impurity measure (ID3 is similar to CART but uses Entropy instead of GINI, and support only classification while CART can also be used for regression). For a complete list of the difference betweens the various algorithms see [3].\n\nOne of the main problems of these algorithms is that the returned tree can be very big and complex and this can lead to overfitting. There are various solutions, for example we could limit the depth of the three. This can lead to leaf nodes with a certain level of residual impurity. In this case a majority vote in the leaf is taken to output a classification model.\nAnother strategy could be to prune the tree after the tree is built. The hope is that by eliminating some sub-branches or substituting them with leaves the increase in the training error will be limited and the true risk of the pruned tree will be smaller. The pruning process usually is performed in a bottom up approach: starting from the leaves the model is modified in order to substitute each node with a leaf or with one of its subtrees. At each step the pruned tree which optimizes the validation score or the $k$-Fold cross validation score is kept. The process continues for all the nodes.\n\nThere is also another method we can try: random forests. This tecnique will be analyzed in the next section.","e1a13cdb":"We will now divide our dataset into a training part and a test part. The training set will be fed to the cross validation for tuning the model, while the test set is used to test the performance of our model (hyperparameter tuning is not done on the test set of course). The training set contains the $70\\%$ of the original observations, while the test set the $30\\%$. The split is done in a stratified way, this means that the we keep the label distribution between the sets.","75aa6882":"#### Linear SVM","2803dc82":"As with every classification task we must start by defining a baseline model. This is the starting point for performing model selection and evaluation. The simpler model we can make here is the one which classifies every sample as positive, thus obtaining a base value for $F_2$.","74c65aae":"We are dealing with a mixed dataset in which there are qualitative and quantitative variables. In order to fed the data to a machine learning algorithm we need to convert the categorical features in numerical one. This is done via label encoding. In this work I used the dummy encoding technique.","378bcb7d":"In the previous demonstrations we obtained a way to compute the directions which preserve the greatest amount of variance in the data. But how can we choose how many principal components we should keep? To do that we can consider the proportion of variance explained by each component, this can be computed by the formula:\n\n$Var(\\mathbf{z}_j) = \\frac{1}{m}\\sum_{i=1}^{d}\\mathbf{z}_{i,j}^2$\nwhere $\\mathbf{z}_j$ is the $j$-th principal component.\nSo we can choose the number of components $n$ by looking at the cumulative variance explained by the firsts $n$ components, divided the total variance explained:\n\n$\\frac{\\sum_{j=1}^{n}\\sum_{i=1}^{m}z_{i,j}^2}{\\sum_{j=1}^{d}\\sum_{i=1}^{m}x_{i,j}^2}$\n\nwhere at the denominator we have the total variance present in the data (assuming of course that the variables have been centered)","e0e51be1":"In order to perform the classification task we will use several different algorithms:\n\n- K-Nearest Neighbors\n- Linear Support Vector Machines\n- RBF Support Vector Machines\n- Logistic Regression\n- Decision Trees\n- Random Forest","5d81e395":"### Random Forests","5d1c1273":"We will now fit a Random Forest Classifier performing a grid search for:\n- $\\text{max depth} = \\text{None}, 3, 5, 7, 10$\n- $\\text{number of trees} = 3, 5, 10, 25, 50$\n- $\\text{number of features to consider at each split} = 4, 7, 15, 20, \\sqrt{p}$","a2d92254":"### References","0e881bae":"We will now check the class distribution in the dataset to see whether there is some imbalance","77618794":"Receiver Operating Characterisc curves are a method for measuring performances in the case of binary classification problems. In this curve are plotted the False Positive Rate (x-axis) (specificity $\\frac{TN}{FP+TN}$) and the True Positive Rate (y-axis) (a.k.a. recall or sensitivity) at the varying of certain thresholds on the probability that a particular algorithm outputs a the positive value. The idea here is that it can be more flexible to predict the probability that a sample belong to a certain class rather than predicting only the labels. Thus ROC curves provides a diagnostic tool to help the interpration of the probabilistic forecast of an algorithm. When predicting the probability that a sample belong to the positive class we can use a threshold to output a label, for example we could say that if the probability of class $1$ is greater than $\\frac{1}{2}$ then we output the label $1$. At the varying of this threshold we could compute the true positive and false positive rate and plot it on a graph.\nThe shape of this curve contains a lot of information, for example lower values for the x-axis indicate that there are less false positives and a lot of true negatives, while high y-axis values indicate an high number of true positive and consequently a lower number of false negatives. It is easy to see that the ideal ROC curve is a curve which passes for the point $(0,1)$ in the ROC space. On the other hand a curve which is simply a $45^{\\circ}$ degree line is like a random guessing (the true positive rate is equal to the false positive rate for every threshold); we want our classifiers to be better than this line.\n\n<figure>\n<img src=\"https:\/\/gialbo.github.io\/Analysis-on-the-German-Credit-Risk-Dataset\/media\/roc-curve-v2.png\" style=\"width: 400px;\">\n    <figcaption><center><i>Roc Curves. <a href=\"https:\/\/glassboxmedicine.com\/2019\/02\/23\/measuring-performance-auc-auroc\/\"> Source<\/a><\/i><\/center><\/figcaption>\n<\/figure>\n\nAn important value to consider while looking at a ROC is the Area Under The Curve (AUC). This is a number between $0$ and $1$ which tells us how much the model is capable of distinguishing between the classes. The higher the AUC the better the model.\n\nIt is very important to note that ROC curves are not useful in case of extreme imbalance in the data. That's because the TPR only depends on the positives in the data. The AUC paramenter assesses the overall performance of the model, not its ability at correctly classifying the minority class. For this reason the ROC curve is not recomended in such scenarios. In the case of the German Credit Dataset the imbalancement between the classes is not so high, so the ROC curve could be used also for the non re-balanced sets","a2f24894":"### Support Vector Machines","ef6cfdf8":"### The dataset","20294de6":"As we can see there is not an high correlation between the numerical features, the highest value of correlation is between the variables credit amount and month duration, but these can be simply explained saying that it is natural that loans with an higher price will have an higher duration in month. However this relation is not so prevalent as we will see in the scatterplot between these two variables.","02ae474a":"Unfortunately this matrix is not so interpretable like the one with only the quantitative variables, for this reason the couples of variables with highest absolute correlation have been reported here.","b5fd6d67":"As we have already said ADASYN will generate some synthetic samples from the under represented class in order to re-balance the dataset. First of all we must define the ratio of majority to minority examples $d$:\n\\begin{equation*}\nd=\\frac{m_s}{m_l}\n\\end{equation*}\nwhere $m_s$ and $m_l$ are respectively the number of majority and minority class examples. If there is an umbalance we can proceed with the algorithm.\nThen we can compute the number of samples that must be generated $G$ with:\n\\begin{equation*}\nG=(m_s-m_l)\\beta\n\\end{equation*}\n$\\beta$ here is the ratio of majority\/minority, $\\beta=1$ means a perfect balanced dataset after ADASYN.\n\nThe next step consists in finding the $k$-Nearest Neighbors of each minority sample ($k$ of course is an hyperparameter to be tuned, default value is $5$). This implies defining a distance, the Euclidean one is used most of the times. \nFor each minority point in the feature space then we must compute the number $r_i$:\n\\begin{equation*}\nr_i=\\frac{m_{ns,i}}{k}\n\\end{equation*}\nwhere $m_{ns,i}$ indicates the number of majority points in the neighborhood of the $i$-th minority point.\n$r_i$ indicates the dominance of the majority class in the neighborhood.\nNow we can normalize the $r_i$'s so that they sum to one:\n\\begin{equation*}\n\\hat{r}_i = \\frac{r_i}{\\sum_ir_i}\n\\end{equation*}\n\nThen for each neighborhood we calculate the amount $G_i$ of synthetic samples to generate:\n\\begin{equation*}\nG_i=G\\hat{r}_i\n\\end{equation*}\n\nThe number of syntetic samples per neighborhood is higher in the neighborhoods in which there is a stronger prevalence of the majority class. This explains the word Adaptive in ADASYN: we generate more samples in the neighborhoods which are harder to learn, because it is more difficult to discriminate the data here. In this way we are making the problem more difficult and we hope that this will give a more robust model.\n\nHaving defined the number of samples to generate per neighborhood we can generate $G_i$ samples for each neighborhood. This is done firstly by taking a minority point in the neighborhood $x_i$, secondly by taking another point in the same neighborhood $x_{zi}$, then generating a new synthetic example $s_i$ with the formula:\n\\begin{equation*}\ns_i=x_i+\\lambda(x_i-x_{zi})\n\\end{equation*}\n$\\lambda$ is a random number between $0$ and $1$.\n\nVisually this consists in generating synthetic points on the line which connects the minority samples.\n<figure>\n<img src=\"https:\/\/gialbo.github.io\/Analysis-on-the-German-Credit-Risk-Dataset\/media\/synthetic.png\" syle= \"width: 200px;\">\n    <figcaption><center><i>Synthetic points generation. <a href=\"https:\/\/www.datasciencecentral.com\/profiles\/blogs\/handling-imbalanced-data-sets-in-supervised-learning-using-family\">Source<\/a><\/i><\/center><\/figcaption>\n<\/figure>\n\nSome white gaussian noise can be added in order to make new data more realistic.\n\nThis method is very useful, but has some drawbacks, in particular:\n- There can be neighborhoods with only $1$ minority example. The problem can be solved by copying the minority sample $G_i$ times or simply ignoring such neighborhoods.<\/il>\n- In the case a lot of data is generated in neighborhoods where the density of majority samples is high the synthetic samples could be too similar to the majority one, this could generate a lot of false positives or false negatives.\n","110eb2ea":"We will now fit a Soft Margin Support Vector Machine, and we will perform a grid search for:\n- $C = 1e-3, 1e-2, 1e-1, 1$","d3e74aad":"### Metrics used","b5dc73be":"#### Boxplots","8c4956a7":"#### Correlation analysis","10a9fadd":"#### Receiver Operating Characteristic","4de7c3fd":"### Loading packages","552c7350":"#### Pairplots","6b7ab235":"Welcome to my Notebook! If you like it or have any advice on how to improve it let me know in the comments. For other articles about machine learning and deep learning I invite you to check my blog: <a href=\"https:\/\/Gialbo.github.io\">Gialbo's Blog<\/a>. Have fun!","9e77ec72":"- [1] https:\/\/pubmed.ncbi.nlm.nih.gov\/11227840\/ Use of PCA for dummy encoded data\n- [2] http:\/\/staskolenikov.net\/talks\/Gustavo-Stas-PCA-generic.pdf Analysis of PCA for categorical data\n- [3] https:\/\/bayesian-intelligence.com\/publications\/TR2010_1_zonneveldt_korb_nicholson_bn_class_credit_data.pdf Use of different penalty matrices\n- [4] https:\/\/dl.acm.org\/doi\/10.1145\/1007730.1007735 On the use of SMOTEENN with ENN applied both to the majority and minority class","fd754a02":"### Validation tecniques","90c8583b":"#### Label encoding","6cef8693":"As we can see the value used for describing the response are $1 \\implies good$ result and $2 \\implies bad$ result. We will say in the rest of the work that a negative result means a good risk score (eligibility) while a positive result to a bad score (no eligibility). Further in the classification pipeline we will change the numerical labels of the response in order to have values of $0 \\implies \\text{negative (good risk)}$ and $1 \\implies \\text{positive (bad risk)}$.","852249d2":"As already said we choose the number of components by looking at the proportion of variance explained by each one of these. We can do that by plotting the cumulative proportion of variance explained and the proportion of variance explained in a graph as the number of components increase. The rule of thumb is to find an elbow in the graph or to consider a PVE of around $85\\%$","90d55724":"When there is not a clear ordinal relationship between the categories we can apply a dummy encoding strategy. Given one categorical variables with $k$ categories, a dummy encoder creates $k-1$ binary values which assume the value $1$ if the object belong to that category, $0$ otherwise. We will now perform dummy encoding on the categorical variables.","0cc81ce5":"As we can see from the cell above the dataset is unbalanced, in fact the $70\\%$ of observation belong to the negative class while only the $30\\%$ to the positive one. This is not an high imbalance but should be considered. We will use some oversampling methods in the next section in order to cope with this problem","6f84817e":"# Analysis on the German Credit Dataset","af99ea8e":"We can inspect some of the first rows of the DataFrame using the head method of Pandas","2c3d650e":"As a preprocessing step is important to normalize the data. This is useful to bring all the features on the same scale. Most of the machine learning algorithm work better for normalized data. There are various tecnique for normalization, like for example min-max normalization or standardization. Here I have decided to adopt the latter. With the operation of data standardization we want each feature to have mean zero and variance one, this is done by subtracting from each column of the dataset the mean and dividing by the column variance. In mathematical formulas:\n\n$x_{i, standardized} = \\frac{x_i-\\bar{x}}{\\hat{\\sigma_{x}}}$\n\nThis operation is done for every column of the dataset.","9e145415":"**Hard SVM**\nIn the linear case we want to find an hyperplane which separates the data and has the highest possible margin. we can define this hyperplane as  $f(x)=\\langle \\mathbf{w},\\mathbf{x}\\rangle+b$, where $\\mathbf{w}$ is a vector orthogonal to the hyperplane and $b$ an intercept term.\n\n<figure>\n    <img src=\"https:\/\/gialbo.github.io\/Analysis-on-the-German-Credit-Risk-Dataset\/media\/svm.PNG\", style=\"width: 400px;\">\n    <figcaption><center><i>SVM, hyperplanes and margin. <a href=\"https:\/\/towardsdatascience.com\/support-vector-machine-vs-logistic-regression-94cc2975433f\">Source<\/a><\/i><\/center><\/figcaption>\n<\/figure>\n\nWhen we define hyperplanes there is an hidden degree of fredoom in the scalar product. In fact the function associated to \n$(\\mathbf{w},b)$ does not change if we rescale the hyperplane to $(\\lambda\\mathbf{w},{\\lambda}b)$, for $\\lambda\\in\\mathbb{R}^+$.\nThere will however be a change in the margin as measured by the function output, as opposed to the purely geometrical margin.\nHaving said so we can impose that a point belongs to the margin if $\\langle \\mathbf{w},\\mathbf{x}\\rangle+b=\\pm 1$.\nSo we will perform our classification task by saying that a point belongs to a specific class if $\\langle \\mathbf{w},\\mathbf{x}\\rangle+b\\geq1$\nand belongs to the other class if $\\langle \\mathbf{w},\\mathbf{x}\\rangle+b\\leq1$.\nNow we will try to model an optimization form for this problem. Let's call a point on the margin $\\mathbf{x}_+$ if it belongs to a class which is in the same direction respect to the hyperplane as the vector $\\mathbf{w}$\nand $\\mathbf{x}_-$ if the point is in the opposite direction (belongs to the other class). So we can define the margin as:\n\\begin{equation*}\n        \\frac{\\langle\\mathbf{x}_+-\\mathbf{x}_-,\\mathbf{w}\\rangle}{2\\|\\mathbf{w}\\|}=\n        \\frac{1}{2\\|\\mathbf{w}\\|}[\\langle\\mathbf{x}_+,\\mathbf{w}\\rangle-\\langle\\mathbf{x}_-,\\mathbf{w}\\rangle]\n\\end{equation*}\n\nWe can then sum and subtract the same quantity $\\pm b$:\n\\begin{equation*}\n        \\frac{1}{2\\|\\mathbf{w}\\|}\\left\\{[\\langle\\mathbf{x}_+,\\mathbf{w}\\rangle+b]-[\\langle\\mathbf{x}_-,\\mathbf{w}\\rangle+b]\\right\\}=\n        \\frac{1}{\\|\\mathbf{w}\\|}\n\\end{equation*}\nThe value we just obtained is the margin. In SVM we have said that we want to maximize this quantity:\n\\begin{equation*}\n        \\max \\frac{1}{\\|\\mathbf{w}\\|}\n\\end{equation*}\n<center>s.t.<\/center>\n\\begin{equation*}\n        y_i[\\langle \\mathbf{w},\\mathbf{x}\\rangle+b]\\geq1\n\\end{equation*}\n\nand this is equivalent to minimize:\n\\begin{equation*}\n        \\min \\frac{1}{2}\\|\\mathbf{w}\\|^2\n\\end{equation*}\n<center>s.t.<\/center>\n\\begin{equation*}\n        y_i[\\langle \\mathbf{w},\\mathbf{x}\\rangle+b]\\geq1\n\\end{equation*}\nWhere we wrote the problem in this way in order to be sure to have a differentiable twice function.\nThe problem we just stated is called the Primal Optimization Problem and using the Lagrangian multipliers we can derive the dual:\n\\begin{equation*}\n        \\mathcal{L}(\\mathbf{w}, b, \\alpha)=\\frac{1}{2}\\|\\mathbf{w}\\|^2-\\sum_i\\alpha_i[ y_i(\\langle \\mathbf{w},\\mathbf{x}_i\\rangle+b)-1]\n\\end{equation*}\n\nWhere we introduced $\\alpha_i$ Lagrangian multipiers, $\\alpha_i\\geq 0$.\nThis means that optimality in $\\mathbf{w}$, $b$ is at saddle point with $\\alpha$ and that derivatives in $\\mathbf{w}$, $b$ need to vanish.\nThe derivatives of the Lagrangian function with respect to $\\mathbf{w}$ and $b$ are:\n\\begin{equation*}\n        \\partial_w\\mathcal{L}(\\mathbf{w}, b, \\alpha)=\\mathbf{w}-\\sum_i\\alpha_iy_i\\mathbf{x_i}=0\n\\end{equation*}\n\\begin{equation*}\n        \\partial_b\\mathcal{L}(\\mathbf{w}, b, \\alpha)=\\sum_i\\alpha_iy_i=0\n\\end{equation*}\nso we can plug back the obtained terms into $\\mathcal{L}$ and we obtain:\n\\begin{equation*}\n        \\max_\\alpha-\\frac{1}{2}\\sum_{i,j}\\alpha_i\\alpha_jy_iy_j\\langle\\mathbf{x}_i,\\mathbf{x}_j\\rangle+\\sum_i\\alpha_i\n\\end{equation*}\n<center>s.t.<\/center>\n\\begin{align*}\n        \\sum_i\\alpha_iy_i=0,\\\\\n        \\alpha_i\\geq0\n\\end{align*}\nThis is the dual optimization problem. We will found out solving this problem that most of the $\\alpha$ are zero.\nThe only non zero Lagrangian multipliers are those associatied with points on the margin, so the optimal hyperplane can be written as\na linear combination of points on the margin. These vector support the decision, so they are called support vectors.\nThe SVM as described until now is called Hard SVM and it's working can be visualized in the animation below:\n<figure>\n    <img src=\"https:\/\/gialbo.github.io\/Analysis-on-the-German-Credit-Risk-Dataset\/media\/svm_gif.gif\", style=\"width: 400px;\">\n    <figcaption><center><i>SVM in action. <a href=\"https:\/\/jeremykun.com\/2017\/06\/05\/formulating-the-support-vector-machine-optimization-problem\/\">Source<\/a><\/i><\/center><\/figcaption>\n<\/figure>\n\n**Soft SVM**\n\nThe main problem of the linear SVM that we have found is that it does not allow for any mistake, for this reason is called Hard SVM. But sometimes the data is noisy, or not completely linearly separable. We need to find a method to allow for some mistakes, this will bring to a model more robust to noise and outliers. In order to achieve this goal we can add some slack variables to the optimization problem, which will become:\n\\begin{equation*}\n        \\min_{\\mathbf{w}, b} \\frac{1}{2}\\|\\mathbf{w}\\|^2+C\\sum_i\\xi_i\n\\end{equation*}\n<center>s.t.<\/center>\n\\begin{align*}\n        y_i\\langle\\mathbf{w},\\mathbf{x}\\rangle+b&\\geq 1-\\xi_i, \\\\\n        \\xi_i&\\geq 0\n\\end{align*}\n\nWith this trick we get that violations are possible (the problem is always feasible), but penalized, so the goal of the optimization problem is to minimize that amount of slack. In this case too we can derive the dual problem:\n\n\\begin{equation*}\n        \\mathcal{L}(\\mathbf{w}, b, \\alpha)=\\frac{1}{2}\\|\\mathbf{w}\\|^2+C\\sum_i\\xi_i\n        -\\sum_i\\alpha_i[ y_i(\\langle \\mathbf{w},\\mathbf{x}_i\\rangle+b)-1]\n        -\\sum_i\\eta_i\\xi_i\n\\end{equation*}\nThen as in the previous case the optimality in $\\mathbf{w}$, $b$, $\\mathbf{\\xi}$ is at saddle point with $\\alpha$, $\\eta$, so\nthe derivatives need to vanish.\n\\begin{equation*}\n        \\partial_{\\mathbf{w}}\\mathcal{L}(\\mathbf{w}, b, \\xi, \\alpha, \\eta)=\n        \\mathbf{w}-\\sum_i\\alpha_iy_i\\mathbf{x}_i=0\n\\end{equation*}\n\\begin{equation*}\n        \\partial_{b}\\mathcal{L}(\\mathbf{w}, b, \\xi, \\alpha, \\eta)=\n        \\sum_i\\alpha_iy_i=0\n\\end{equation*}\n\\begin{equation*}\n        \\partial_{\\xi_i}\\mathcal{L}(\\mathbf{w}, b, \\xi, \\alpha, \\eta)=\n        C-\\alpha_i-\\eta_i=0\n\\end{equation*}\nSo plugging everything again in the Lagrangian function yields:\n\\begin{equation*}\n        \\max_\\alpha-\\frac{1}{2}\\sum_{i,j}\\alpha_i\\alpha_jy_iy_j\\langle\\mathbf{x}_i,\\mathbf{x}_j\\rangle+\\sum_i\\alpha_i\n\\end{equation*}\ns.t.\n\\begin{align*}\n        &\\sum_i\\alpha_iy_i=0,\\\\\n        &\\alpha_i\\in[0,C]\n\\end{align*}\nWhat is changing with respect to the hard margin case is that now we are limiting the value of each $\\alpha_i$ to be not greater than $C$.\nThe effect of $C$ which is the constant determining the number of slack variables, determines which values $\\alpha$ can take.\n$C$ is proportional to the mistake I make and therefore it determines how soft or hard the margin is, the larger the $C$ the softer the margin,\nso this parameters acts as a regularization term.\n\n**Solving Linear SVM with Stochastic Gradient Descent**\n\nWe can rewrite the optimization problem of soft margin SVM as follows:\n\\begin{equation*}\n        \\min_{\\mathbf{w}, b} \\frac{1}{2}\\|\\mathbf{w}\\|^2+C\\sum_i\\max(0, 1-y_i\\mathbf{w}^T\\mathbf{x}_i)\n\\end{equation*}\n\nThe second term in the minimization function is the hinge loss: it penalizes vectors that correspond to mistakes (soft constraint). Note that if the distance between the point and the plane is higher than the margin then the loss assumes value $0$. The function we just defined is convex in $\\mathbf{w}$ so there exist a global minimum.\nWe can minimize the function with the Gradient Descent algorithm: for an arbitrary number of iterations (epochs) at each step we compute the gradient w.r.t. to $\\mathbf{w}$ (we know that the gradient of a function is the direction of maximum increase). We can then take a step in the opposite direction towards the minimum.\nThe problem here is that the max function is not differentiable, so we can not compute the gradient (the limit of the difference quotient from the left is different from the one from the right). A solution is to compute the sub-gradient. A sub-tangent at a point is any hyperplane that lies below the function at the point. A sub-gradient is the slope of that line. So if we call the function to minimize $J$ we can compute the gradient of it:\n\\begin{equation*}\n\\nabla_{\\mathbf{w}}J=\\begin{cases}\\mathbf{w}\\;\\text{if} \\max(0, 1-y_i\\mathbf{w}^T\\mathbf{x}_i)=0\\\\\n\\mathbf{w}-Cy_i\\mathbf{x}_i\\;\\text{otherwise}\\end{cases}\n\\end{equation*}\nThe general Gradient Descent algorithm given the training set has the following steps:\n1. Initialize $\\mathbf{w}=0\\in\\mathbf{R}^d$\n2. For each epoch $t = 1,...,T$:\n     Update $\\mathbf{w}\\leftarrow\\mathbf{w}-\\gamma_t\\nabla_{\\mathbf{w}}^tJ$\n3. Return $\\mathbf{w}$\n\n$\\gamma_t$ is called the learning rate and it is a parameter useful in order to not make too big steps towards the minumum (there is the risk of overshooting it).\nIf we compute the gradient of all the training set at each step the algorithm may end up taking too much time to converge. For this reason we use the Stochastic Gradient Descent. This algorithms is like the GD, but computes the gradient only of a random sample of the data at each step. The SGD algorithm is guaranteed to converge if the value of $\\gamma_t$ is little enough (we do not make too big steps in a wrong direction).","95578fe9":"#### SMOTEENN","44d2dd46":"Now we can look at the correlation matrix considering also the dummy encoded variables","dd08b555":"Another very common and powerful family of machine learning algorithms is the Support Vector Machine (SVM) family. Given a binary classification problem the goal of SVMs is to find an hyperplane in a N-dimensional subspace which performs the best split of the labeled training data points into the two categories. With best split we intend the split which maximizes the margin between the classes; with margin we intend the distance between the hyperplane and its closest\nsamples in the feature space. In order to find this best hyperplane SVM sees the classification task as a\nconvex optimization problem that can be efficiently solved using Stochastic Gradient Descent. Furthermore SVM can be kernelized as we will see in detail in the next sections and be able to solve also non-linearly separable problems.\nOne of the main advantages of SVM besides it's flexibility is that the algorithm only cares about the points on the margin (the so-called support vectors), so this can yield to a small sample complexity even if the dimensionality of the feature space is high or even infinite.","b5112e5e":"Logistic Regression can be seen as a particular case of the Generalized Linear Model (GLM). Such model in general has basically three components:\n\n1. A response vector $Y_1,\\dots,Y_m$ of indipendent r.v. with a mean $\\mu_i=\\mathbb{E}(Y_i)$. The data will be the realizations of these random variables $y_1,\\dots, y_m$.\n2. A linear combinations $\\mathbf{\\eta}$ of predictors $\\mathbf{\\beta}$  and data $X$ with values varying with $i=1,\\dots,n$: $\\eta_i = \\sum_j\\beta_jx_{i,j}$ or in matricial form $\\mathbf{\\eta}=X\\mathbf{\\beta}$.\n3. A link function connecting the means $\\mu_i$ and the linear combinations $\\eta_i$: $g(\\mu_i)=\\eta_i$\n\nHaving defined the GLM in this way we can consider in particular the case in which the response variables are Bernoulli variables with paramenter $p_i$.\n\\begin{equation*}\nY_i\\sim Bernoulli(p_i)\n\\end{equation*}\n\nWe know that: $\\mathbb{E}(Y_i)=0(1-p_i)+1(p_i)=p_i$, so now to apply the GLM we only need to define a proper link function $g(\\mu_i)$. A simple choice would be to use the identity function, in this way: $p_i=\\sum_j\\beta_jx_{i,j}$.\nThis is definitely not a good idea, because it would be like fitting a regression model to solve a classification problem. The values returned by the linear combination of data and predictor would be over an unbounded straight line, but we know that $p_i$ could only assume values between $0$ and $1$. \n\nWe need something else, an intelligent solution could be to use the logit link function, defined as:\n\\begin{equation*}\nlogit(p)=\\log(\\frac{p}{1-p})\n\\end{equation*}\n\nBut why this could be a good choice? Let's start considering the odds function: $\\frac{p}{1-p}$. the problem with this function is that it is not symmetrical. We can simply symmetrize it by taking the log, thus getting the log-odds function or logit.\n<figure>\n<img src=\"https:\/\/gialbo.github.io\/Analysis-on-the-German-Credit-Risk-Dataset\/media\/350px-Logit.png\" style=\"width: 400px;\">\n    <figcaption><center><i>The logit function.<\/i><\/center><\/figcaption>\n<\/figure>\n\nThe logit function stretches the entire real line to the range $[0, 1]$. We can use it as a link function, thus getting: $logit(p_i)=\\sum_j\\beta_jx_{i, j}$.\n\nWith a little algebra we can simply compute the inverse of the logit: $p_i=\\frac{e^{\\sum_j\\beta_jx_{i, j}}}{1+e^{\\sum_j\\beta_jx_{i, j}}}$\n\nLets call this function $\\sigma(\\sum_j\\beta_jx_{i, j})=\\frac{1}{1+e^{\\sum_j\\beta_jx_{i, j}}}$ the <i>sigmoid<\/i> function, we know then that $1-\\sigma(\\sum_j\\beta_jx_{i, j})=\\frac{e^{\\sum_j\\beta_jx_{i, j}}}{1+e^{\\sum_j\\beta_jx_{i, j}}}$\n\nSo in our classification problem the likelihood that we want to maximize is:\n\\begin{equation*}\n        L(y|X\\mathbf{\\beta})=\\prod_i(1-\\sigma(\\sum_j\\beta_jx_{i, j}))^{y_i}\\sigma(\\sum_j\\beta_jx_{i, j})^{1-y_i}\n\\end{equation*}\nwe can as usual take the logarithm, compute the derivative with respect to the $\\beta$'s and get:\n\\begin{equation*}\n \\frac{\\partial}{\\partial\\mathbf{\\beta_j}}L(\\mathbf{\\beta})=\\dots=\\sum_{j}x_{i,j}(y_i-(1-\\sigma(\\sum_i\\beta_ix_{i, j})))\n\\end{equation*}\n\nThe expression we have found is the update rule of the coefficients and is equal to the sample multiplied by the true label minus the prediction of the label; this function has not a close solution, so we can not find a formula to compute the parameters,\nbut on the other hand it is the derivative of a convex function; so we can optimize it with stochastic gradient descent.\n\nIn order to reduce overfitting we can regularize the model by imposing additional constraints on the parameters. For example we can assume $\\mathbf{w}_i$ comes from a Gaussian distribution with mean $0$ and variance $\\sigma^2_0$ (prior on the parameters), $\\sigma^2_0$ is user defined.\nIn that case we have a prior on the parameter and so:\n\\begin{equation*}\n        \\mathbb{P}(y=1, \\theta|X)\\propto \\mathbb{P}(y=1|X;\\theta)\\mathbb{P}(\\theta)\n\\end{equation*}\n\nSo if we take into account this prior will have a posterior which will be the goal of optimization.\nSo the log likelihood becomes:\n\\begin{equation*}\n        \\ln L(y|X;\\mathbf{w})=\\sum_{i=1}^N\\ln(1-\\sigma(\\sum_j\\beta_jx_{i, j}))+(1-y_i)\\ln(\\sigma(\\sum_j\\beta_jx_{i, j}))-\\sum_j\\frac{\\mathbf{\n        \\beta_j^2}}{2\\sigma^2_0}\n\\end{equation*}\n\nIn this case we perform a MAP (Maximum A priori Probability) estimate.\nThe regularization studied is called $L_2$ regularization, we are trying to minimize the sum of square value of the $\\beta_i$s. Another regularization tecqnique is $L_1$ regularization which tries to\nminimize $|\\mathbf{\\beta}|$.\n\nIn our anaylisis we will perform Logistic Regression by tuning the parameter $C$ which is proportional to the inverse of the variance of the prior $\\sigma_0^2$. This parameters tunes the strength of the regularization, like in SVM smaller values imply stronger regularization.","dc8cb6fe":"#### Principal Component Analysis","666c51d4":"## Conclusions","e654bae7":"#### PCA for visualization","6ea97cc6":"We will now use some contingency tables to see if there are relationships between two categorical variables or between one categorical variable and the response. I have reported here only some of the most important","f397e1d8":"Dimensionality reduction consists mapping data in a lower dimensionality space. The goal is to keep the loss of information as little as possible. There are various reasons for performing dimensionality reduction: first of all high dimensional data imposes high computational challenges, secondly as the number of dimensions grows data become more sparse and some algorithms (expecially the distance based ones) fail in high dimension spaces. This is the curse of dimensionality. In this work we will explore one of the most used tecniques in dimensionality reduction: PCA (Principal Component Analysis).","f8fab998":"One of the main advantages of SVM is that they can be kernelized. Linear SVMs work by finding the best hyperplane which splits the data, this is also called halfspace. The power of such method is rather restricted, there are problems which are not linearly separable. A simple solution to this problem is to map the input dimension to an higher dimensional space in which we hope the data is linearly separable. In this case we would apply SVM to the data in the new space, also called the feature space.\n<figure>\n<img src=\"https:\/\/gialbo.github.io\/Analysis-on-the-German-Credit-Risk-Dataset\/media\/feature_space.png\" style=\"width: 400px;\">\n    <figcaption><center><i>In this higher dimensional space data is linearly separable. <a href=\"https:\/\/towardsdatascience.com\/the-kernel-trick-c98cdbcaeb3f\"> Source<\/a><\/i><\/center><\/figcaption>\n<\/figure>\n\nMore formally we can say that this technique consists in $4$ steps:\n1. Given a domain set $\\mathcal{X}$, choose a mapping $\\Phi:\\mathcal{X}\\to\\mathcal{F}$ for some feature space $\\mathcal{F}$. This feature space can be any Hilbert space of finite dimension $n$ or infinite dimension.\n2. Given a sequence of training examples $S=(x_1, y_1),\\dots,(x_m, y_m)$ map the training data to $\\hat{S}=(\\Phi(x_1),y_1),\\dots,(\\Phi(x_m),y_m)$\n3. Train a linear prediction on the mapped data (can be an SVM but also another algorithm)\n4. Perform prediction on the learned hyperplane.\n\nThis method has two major problems: computing the mapped data in the feature space can be very computationally expensive. Secondly finding good feature mappings could be difficult and need some prior knowledge. A solution to this problem is to use the kernel trick. With the term <i>kernels<\/i> we intend inner products in the feature space. So given a domain space $\\mathcal{X}$, some feature mapping $\\Phi$ in some Hilbert space we define the kernel function $K(\\mathbf{x},\\mathbf{x'})=\\langle\\Phi(\\mathbf{x}),\\Phi(\\mathbf{x'})\\rangle$. Having defined the kernel means that if we are able to write all the operations that we need to carry out as inner products between the data, then we do not care of finding an ad hoc feature mapping, all we need to do is substiting the inner products with the kernel. This means that we can also use kernels that are related to a mapping to a space with an infinite number of dimension. This is the so called Kernel Trick and permits of solving higly non linear problem in an efficient way.\n\nAs we saw in the Linear SVM case, we can rewrite the optimization problem by getting the dual. In this new case all the operations that involve the data is constituted of inner products. This means that SVM can be kernelized. This result is formalized better in the Representer Theorem.\n\nIn this section we are going to use a particular Kernel: the Radial Basis Function kernel Let the original data space be $\\mathbb{R}$ and consider the mapping $\\Phi$, where for each non-negative integer $n$ there exists an element $\\Phi(x)_n$ that is equal to: $\\frac{1}{\\sqrt{n}!}e^{-\\frac{x^2}{2}}x^n$, then:\n\\begin{align*}\n\\langle\\Phi(x),\\Phi(x')\\rangle &= \\sum_{n=0}^{\\infty}\\left(\\frac{1}{\\sqrt{n}!}e^{-\\frac{x^2}{2}}x^n\\right)\\left(\\frac{1}{\\sqrt{n}!}e^{-\\frac{(x)'^2}{2}}(x')^n\\right)\\\\\n&=e^{-\\frac{x^2+(x')^2}{2}}\\sum_{n=0}^{\\infty}\\frac{(xx')^n}{n!}\\\\\n&=e^{-\\frac{\\|x-x'\\|^2}{2}}\n\\end{align*}\n\nmore generally, given a scalar $\\sigma>0$ we could define the RBF kernel as:\n\\begin{equation*}\nK(\\mathbf{x}, \\mathbf{x'})=e^{-\\frac{\\|\\mathbf{x}-\\mathbf{x'}\\|^2}{2\\sigma}}\n\\end{equation*}\n\nIntuitively the Gaussian kernel sets the inner product in the feature space to be near zero if the two points a far away, viceversa near one if they are close. With $\\sigma$ we determine the scale so what do we mean by \"close\".\nIn the literature (and also in sklearn) instead of $\\sigma$ we rather find the parameter $\\gamma=\\frac{1}{2\\sigma}$. Thus it indicates how far the influence of a single training sample reaches, low values mean far, and high values mean close. $\\gamma$ can be seen as the inverse of the radius of influence of samples selected by the model as support vectors.","2c507051":"#### Choosing the best number of components","72a11c0c":"We will now apply the SMOTEENN algorithm to the dataset, in particular, SMOTE will be applied only to the minority class, while ENN will be applied <i>after<\/i> SMOTE on both the classes. Thus we will undersample the majority class and the minority class (we try to correct the noise introduced by SMOTE and undersample the majority). This process is presented in [4].","598f77a7":"The German Credit Dataset as we saw is slightly imbalanced. A situation in which one class is represented at the $70\\%$ and the other at the $30\\%$ is not a bad imbalance and most of the algorithms available can deal with this situation. However it could be interesting in this situation to try some tecniques to re-balance the dataset and see if this improves the performances compared to algorithms trained with the original dataset. When dealing with umbalanced data there are mainly two strategies that can be adopted to re-balance it: undersampling and oversampling. Undersampling consists in down-sampling the most rapresented class in order to re-balance the data. In this case the dataset we are using is not so big ($1000$ instances) so there could be the risk that with undersampling some useful information is lost. The other strategy is oversampling, which consists in copying the observations of the under represented class until the dataset is balanced. This of course can cause major overfitting problems.\n\n<figure>\n<img src=\"https:\/\/gialbo.github.io\/Analysis-on-the-German-Credit-Risk-Dataset\/media\/over_under_sampling.png\" style=\"width: 400px;\">\n    <figcaption><center><i>Oversampling and Undersampling. <a href=\"https:\/\/www.kaggle.com\/rafjaa\/resampling-strategies-for-imbalanced-datasets#t1\">Source<\/a><\/i><\/center><\/figcaption>\n<\/figure>\n\nA solution to the mentioned problem of oversampling is creating new synthetic observations for the under-represented class in order to balance the data. This is the strategy we are going to follow, with ADASYN (Adaptive Synthetic sampling approach for imbalanced learning).\nIt is important to point out that every oversampling (but also undersampling) technique must be applied only on training data, not on the data used for validation or testing, that's because we want to keep our validation and test set the most similar as possible to the underlying data distribution in order to get accurate results.","18e52841":"Boxplots are a great tool for exploring the data distrubution and look for outliers. With boxplots we can summarize the data distributions in a nice visualization. We have that the lower part of the wisker is the minumum computed considering the interquartile range: $Q1-1.5\\times IQR$, where $Q1$ is the first quartile ($25\\%$ percentile) and $IQR$ is the interquartile range, which is the difference between $Q3$ ($3$rd quartile) and $Q1$. The lower edge of the rectangle is $Q1$ while the line in the middle is the median $Q2$; the upper edge of the box is $Q3$. The upper whisker is the maximum computed considering $Q3+1.5\\times IQR$. The points that are outside the whiskers are labeled as outliers. The green triangle in the plot indicates the mean of the distribution.\n\n<figure>\n<img src=\"https:\/\/gialbo.github.io\/Analysis-on-the-German-Credit-Risk-Dataset\/media\/box_plot.png\" style=\"width: 400px;\">\n    <figcaption><center><i>Boxplot Legend. <a href=\"https:\/\/towardsdatascience.com\/understanding-boxplots-5e2df7bcbd51\">Source<\/a><\/i><\/center><\/figcaption>\n<\/figure>","7188edb6":"The distributions of the quantitative variables are quite different from each other. For example the distribution of the age seems a bit skewed to the left and assumes its maximum around the age of $35$ years. The variable credit amount on the other hand decreases as the amount requested increases. Other variables, like for example people_liability or resident_since assume only a limited number of values in the plot.\nBy looking at the scatter plot between credit_amount and month_duration we can see a mild linear relationships. This was already spotted in the correlation value of $0.62$ found in the previous section between these two variables.","09509335":"For measuring the performances of a classification model different metrics can be used depending on the particular problem and situation. In fact a sample can be classified in several ways and depending on the outcome of the classification and on the true label of the sample we can have different configurations. In particular for a binary classification problem in which we could have two possible values for the label (let's say a positive and a negative value) we can define the following outcomes :\n\n- **True Positive TP**: this outcome occurs when the classifier correctly predicts the positive label of the sample.\n- **True Negative TN**: this outcome occurs when the classifier correctly predicts the negative label of the sample.\n- **False Positive FP**: this outcome occurs when the classifier wrongly predicts a positive label for the sample, but the true label was negative\n- **False Positive FN**: this outcome occurs when the classifier wrongly predicts a negative label for the sample, but the true label was positive\n\nA good visualization for examining the number of TP, TN, FP and FN is the confusion matrix. In the particular case of a binary classification problem this is a $2\\times 2$ matrix containg on each row the instance of the true class and on each column the instance of the predicted class (or viceversa), thus obtaining:\n<figure>\n<img src=\"https:\/\/gialbo.github.io\/Analysis-on-the-German-Credit-Risk-Dataset\/media\/conf_matrix.jpeg\" style=\"width: 400px;\">\n    <figcaption><center><i>Confusion matrix, note how 0 indicates negative and 1 positive. <a href=\"https:\/\/towardsdatascience.com\/demystifying-confusion-matrix-29f3037b0cfa\">Source<\/a><\/i><\/center><\/figcaption>\n<\/figure>\n\n\nAccording to these definitions we can introduce several metrics for defining how well our model predicts the labels of a particular set of features. some of these are:\n\n- Accuracy:\n    This is the simplest and most known one, it is the ratio of correctly classified samples in the dataset:\n    $Accuracy = \\frac{\\text{Number of correctly classified predictions}}{\\text{Total number of predictions}}$\n    \n    accuracy can also be written as:\n    \n    $Accuracy = \\frac{\\text{TP}+\\text{TN}}{\\text{TP}+\\text{TN}+\\text{FP}+\\text{FN}}$\n    \n    \n- Precision:\n    Sometimes accuracy is not enough, it could happen that we want to know what is the proportion of true positives among all samples classifed as positives, this is precision:\n    \n    $Precision = \\frac{\\text{TP}}{\\text{TP}+\\text{FP}}$\n    \n    \n- Recall:\n    Recall on the other hand wants to measure the proportion of actual positives that were classified correclty:\n    \n    $Recall = \\frac{\\text{TP}}{\\text{TP}+\\text{FN}}$\n    \nIn the following figure we could see graphically the logic behind precision and recall:\n<figure>\n<img src=\"https:\/\/gialbo.github.io\/Analysis-on-the-German-Credit-Risk-Dataset\/media\/Precisionrecall.png\" style=\"width: 400px;\">\n    <figcaption><center><i>Precision and Recall. <a href=\"https:\/\/en.wikipedia.org\/wiki\/Precision_and_recall\">Source<\/a><\/i><\/center><\/figcaption>\n<\/figure>\n    \nTo evaluate a model we have to both look at precision and recall, but unfortunately these measures are sometimes in conflict. This means that improving precision typically reduces recall and viceversa. A tradeoff between these two measures is needed. A measures which tries to do that is:\n\n- $F_1$ score:\n    This is the armonic mean between precision and recall, defined as:\n    $F_1 = 2\\times\\frac{Precision\\times Recall}{Precision + Recall}$","48ada495":"The dataset is composed of both categorical and numerical features which are used to describe the socio-economic situation of the credit applicants, the categorical ones are codified with alpha numerical codes, a feature conversion strategy is needed (more on that later). As an example I report the meaning of some of the features of the dataset\n\n**Existing Checking Account (Qualitative)**\n\nStatus of existing checking account i.e. range of money in existing checking account (if present)\n\n| Value | Description|\n| --- | --- |\n| A11 | .. < 0 DM | \n| A12 | 0 <= ... < 200 DM | \n| A13 | ... >= 200 DM \/ salary assignments for at least 1 year |\n| A14 | no checking account|\n\nNote: DM stands for Deutsche Marks.\n\n**Month Duration (Numerical)**\n\nDuration in month of the asked loan\n\n**Purpose (Qualitative)**\n\nPurpose of the requested loan\n\n| Value | Description|\n| --- | --- |\n| A40 | car (new)\n| A41 | car (used)\n| A42 | furniture\/equipment\n| A43 | radio\/television\n| A44 | domestic appliances\n| A45 | repairs\n| A46 | education\n| A47 | vacation\n| A48 | retraining\n| A49 | business\n| A410 | others\n\n**Credit Amount (Numerical)**\n\nAmount of money requested for the credit\n\n**Personal Status and Sex (Qualitative)**\n\n| Value | Description|\n| --- | --- |\n|A91 | male : divorced\/separated\n|A92 | female : divorced\/separated\/married\n|A93 | male : single\n|A94 | male : married\/widowed\n|A95 | female : single\n\n**Property (Qualitative)**\n\nType of property owned by the applicant\n\n| Value | Description|\n| --- | --- |\n|A121 | real estate\n|A122 | if not A121 : building society savings agreement\/ life insurance\n|A123 | if not A121\/A122 : car or other, not in attribute 6\n|A124 | unknown \/ no property\n\n**Age (Numerical)**\n\nAge in years\n\n**Housing (Qualitative)**\n\n| Value | Description|\n| --- | --- |\n|A151 | rent\n|A152 | own\n|A153 | for free\n\n**Existing credit number (Numerical)**\n\nNumber of existing credit at this bank\n\n**Result (Response)**\n\n| Value | Description|\n| --- | --- |\n|1 | good (applicant eligible for credit)\n|2 | bad (applicant not eligible for credit)","50dea46e":"We will now perform a grid search for:\n- $C = 1e-4, 1e-3, 1e-2, 1e-1, 1, 10$\n- $\\gamma = 1e-3, 1e-2, 1e-1, 1, 10$","48bb8528":"#### ADASYN","a43733b5":"PCA can also be used for projecting the data in a lower dimensional space in order to visualize it. In the following example we project the data to a $2$D and $3$D space. Note how the goal of PCA is not to find the projection which best splits the data distributions, but to preserve most of the variance.","ca66c2b6":"With df.info() we can find if some data are missing"}}