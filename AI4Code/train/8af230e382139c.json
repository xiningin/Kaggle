{"cell_type":{"232a0ed0":"code","8d567d14":"code","15ded2c4":"code","24ae65f9":"code","14831ec7":"code","67f5c4d1":"code","5ea5699a":"code","1e6fcb15":"code","2031062e":"code","42a1302e":"code","3120d114":"code","af421388":"code","cc8a9c17":"code","ed2428e4":"code","fdf7c435":"code","52a973aa":"code","0a7c2b8e":"code","4e58406b":"code","070c5ed1":"markdown","75c5c968":"markdown","56d6b674":"markdown","4f37b17e":"markdown","14abfceb":"markdown","1eb73c6a":"markdown","ff718668":"markdown","b2e6b10d":"markdown","4501455e":"markdown"},"source":{"232a0ed0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn import preprocessing\n\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Convolution2D, MaxPooling2D,BatchNormalization\n\nimport nltk \nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nimport re\n \nimport random\nimport cv2\nimport matplotlib.pyplot as plt\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import  Embedding,LSTM,SpatialDropout1D\n \n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n \n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8d567d14":"data = pd.read_csv('..\/input\/my-filecsv\/framingham.csv');\ndata.info();","15ded2c4":"avg_education = data[\"education\"].astype(\"float\").mean(axis=0); #\u0440\u0430\u0445\u0443\u0454\u043c\u043e \u0441\u0435\u0440\u0435\u0434\u043d\u0454 \u0437\u043d\u0430\u0447\u0435\u043d\u043d\u044f \ndata[\"education\"].replace(np.nan, avg_education, inplace=True); # \u0437\u0430\u043f\u043e\u0432\u043d\u044e\u0454\u043c\u043e \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u0438 \n\navg_cigsPerDay = data[\"cigsPerDay\"].astype(\"float\").mean(axis=0);\ndata[\"cigsPerDay\"].replace(np.nan, avg_cigsPerDay, inplace=True);\n\navg_BPMeds = data[\"BPMeds\"].astype(\"float\").mean(axis=0);\ndata[\"BPMeds\"].replace(np.nan, avg_BPMeds, inplace=True);\n\navg_total_cholesterol= data[\"totChol\"].astype(\"float\").mean(axis=0);\ndata[\"totChol\"].replace(np.nan, avg_total_cholesterol, inplace=True);\n\navg_glucose= data[\"BMI\"].astype(\"float\").mean(axis=0);\ndata[\"BMI\"].replace(np.nan, avg_glucose, inplace=True);\n\navg_heartRate= data[\"heartRate\"].astype(\"float\").mean(axis=0);\ndata[\"heartRate\"].replace(np.nan, avg_heartRate, inplace=True);\n\navg_glucose= data[\"glucose\"].astype(\"float\").mean(axis=0);\ndata[\"glucose\"].replace(np.nan, avg_glucose, inplace=True);\n\n#convert categorical variable into indicator variables\ndata = pd.get_dummies(data,columns=['male'],drop_first=True);\ndata = pd.get_dummies(data,columns=['currentSmoker'],drop_first=True);\ndata = pd.get_dummies(data,columns=['prevalentStroke'],drop_first=True);\ndata = pd.get_dummies(data,columns=['prevalentHyp'],drop_first=True);\ndata = pd.get_dummies(data,columns=['diabetes'],drop_first=True);\ndata = pd.get_dummies(data,columns=['TenYearCHD'],drop_first=True);\n\n#\u043d\u043e\u0440\u043c\u0430\u043b\u0456\u0437\u0430\u0446\u0456\u044f \u0434\u0430\u043d\u0438\u0445  \nx = data.drop('prevalentHyp_1', axis=1); #features\ny = data['prevalentHyp_1']; #\u0446\u0456\u043b\u044c\u043e\u0432\u0430 \u0437\u043c\u0456\u043d\u043d\u0430 \nnames = x.columns; \nx = preprocessing.normalize(x, axis=0); #\u043d\u043e\u0440\u043c\u0430\u043b\u0456\u0437\u0430\u0446\u0456\u044f \u0434\u0430\u043d\u0438\u0445 \u043f\u043e \u0441\u0442\u043e\u0432\u0431\u0446\u044f\u043c \nx = pd.DataFrame(x, columns=names)","24ae65f9":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=17); #\u043f\u043e\u0434\u0456\u043b \u043d\u0430 \u0442\u0440\u0435\u043d\u0443\u0432\u0430\u043b\u044c\u043d\u0443 \u0442\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u0443 \u0432\u0438\u0431\u0456\u0440\u043a\u0438\n\ny_train = keras.utils.to_categorical(y_train, 2); #\u043f\u0435\u0440\u0435\u0442\u0432\u043e\u0440\u0435\u043d\u043d\u044f \u0432\u0435\u043a\u0442\u043e\u0440\u0430 \u043a\u043b\u0430\u0441\u0443 \u0443 \u0434\u0432\u0456\u0439\u043a\u043e\u0432\u0443 \u043c\u0430\u0442\u0440\u0438\u0446\u044e \u043a\u043b\u0430\u0441\u0456\u0432.\ny_test = keras.utils.to_categorical(y_test, 2); \n\n\nmodel =  keras.Sequential();#\u043d\u0430\u0448\u0430 \u043c\u0435\u0440\u0435\u0436\u0430\nmodel.add(layers.Dense(10, activation='relu', input_shape=(x_train.shape[1],)));#\u0434\u043e\u0434\u0430\u0454\u043c\u043e \u0448\u0430\u0440 \nmodel.add(layers.Dense(20, activation='relu'));#\u0434\u043e\u0434\u0430\u0454\u043c\u043e \u0448\u0430\u0440 \nmodel.add(layers.Dense(20, activation='softmax'));#\u0434\u043e\u0434\u0430\u0454\u043c\u043e \u0448\u0430\u0440 \nmodel.add(layers.Dense(2, activation='sigmoid'));#\u0434\u043e\u0434\u0430\u0454\u043c\u043e \u0448\u0430\u0440 \n\nmodel.summary();\nmodel.compile(loss='binary_crossentropy', metrics=['accuracy']);#\u0437\u0431\u0438\u0440\u0430\u0454\u043c\u043e \u043c\u043e\u0434\u0435\u043b\u044c\u043a\u0443 \u0434\u043b\u044f \u043d\u0430\u0432\u0447\u0430\u043d\u043d\u044f ","14831ec7":" model.fit(x_train, y_train, epochs=60, validation_split=0.2);#\u043d\u0430\u0432\u0447\u0430\u0454\u043c\u043e","67f5c4d1":"score_test = model.evaluate(x_test, y_test, verbose=0); #\u043e\u0446\u0456\u043d\u043a\u0438 \u0432\u0436\u0435 \u043d\u0430\u0432\u0447\u0435\u043d\u043d\u043e\u0457 \u043c\u043e\u0434\u0435\u043b\u0456 \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u0438\u0445 \u0434\u0430\u043d\u043d\u0438\u0445 \nprint('Test loss:', score_test[0]);# \u0432\u0442\u0440\u0430\u0442\u0438 \nprint('Test accuracy:', score_test[1]);#\u0442\u043e\u0447\u043d\u0456\u0441\u0442\u044c(\u043e\u0431\u0440\u0430\u043d\u0430 \u043c\u0435\u0442\u0440\u0438\u043a\u0430 accuracy)","5ea5699a":"path = '\/kaggle\/input\/animal-image-datasetdog-cat-and-panda\/animals\/';\nlabel = ['cats', 'dogs', 'panda'];\n\ndata = [];\nlabels = [];\npath_images = [];\n\nfor i, l in enumerate(label): # \u0437\u0431\u0435\u0440\u0456\u0433\u0430\u0454\u043c\u043e \u0448\u043b\u044f\u0445\u0438 \u0434\u043e \u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u044c\n    for j in os.listdir(path+l):\n        path_images.append([path+l+'\/'+j, i]) # i=0 : 'cat', i=1 : 'dog', i=2 : 'panda'\n\n\nrandom.shuffle(path_images); #\u043f\u0435\u0440\u0435\u043c\u0456\u0448\u0443\u0454\u043c\u043e \u0448\u043b\u044f\u0445\u0438 \u0434\u0430\u043d\u043d\u0438\u0445\n\nfor i in path_images:#\n    image = cv2.imread(i[0]);\n    image = cv2.resize(image, (55, 32));\n    data.append(image);\n    label = i[1];\n    labels.append(label);\n    \ndata = np.array(data, dtype=\"float\") \/ 255;\nlabels = np.array(labels);","1e6fcb15":"x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=17); #\u043f\u043e\u0434\u0456\u043b \u043d\u0430 \u0442\u0440\u0435\u043d\u0443\u0432\u0430\u043b\u044c\u043d\u0443 \u0442\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u0443 \u0432\u0438\u0431\u0456\u0440\u043a\u0438","2031062e":"y_train = keras.utils.to_categorical(y_train, 3); #\u043f\u0435\u0440\u0435\u0442\u0432\u043e\u0440\u0435\u043d\u043d\u044f \u0432\u0435\u043a\u0442\u043e\u0440\u0430 \u043a\u043b\u0430\u0441\u0443 \u0443 \u0434\u0432\u0456\u0439\u043a\u043e\u0432\u0443 \u043c\u0430\u0442\u0440\u0438\u0446\u044e \u043a\u043b\u0430\u0441\u0456\u0432.\ny_test = keras.utils.to_categorical(y_test, 3); \n\n\nmodel =  keras.Sequential(); # \u043d\u0430\u0448\u0430 \u043d\u0435\u0439\u0440\u043e\u043c\u0435\u0440\u0435\u0436\u0430\n \nmodel.add(Convolution2D(32, (2, 2), activation='relu', input_shape=(32, 55, 3)));\nmodel.add(MaxPooling2D(pool_size=(2, 2)));\nmodel.add(Convolution2D(32, (2, 2), activation='relu'));\nmodel.add(MaxPooling2D(pool_size=(2, 2)));\nmodel.add(BatchNormalization());\nmodel.add(Dropout(0.25));\nmodel.add(Flatten());\nmodel.add(Dense(128, activation='relu'));\nmodel.add(Dropout(0.5));\nmodel.add(Dense(3, activation='softmax'));\n\nmodel.summary();\nmodel.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy']);","42a1302e":"model.fit(x_train, y_train, epochs=30);#\u043d\u0430\u0432\u0447\u0430\u0454\u043c\u043e","3120d114":"score = model.evaluate(x_test, y_test, verbose=0);\nprint('Test loss:', score[0]);\nprint('Test accuracy:', score[1]);","af421388":"text = pd.read_csv('..\/input\/training1600000processednoemoticoncsv\/training.1600000.processed.noemoticon.csv', encoding='latin'); # dataset from lab3\ntext = text.drop(columns=['1467810369', 'Mon Apr 06 22:19:45 PDT 2009', '_TheSpecialOne_', 'NO_QUERY'], axis = 1);\ntext.columns = ['sentiment', 'text'];\ntext = text.drop(np.append(np.array(range(6000,800000)),np.array(range(806000,text.shape[0]))), axis=0);\n\nstop_words = stopwords.words('english');\nstemmer = SnowballStemmer('english');\ntext_cleaning_re = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\";\n\ndef preprocess(text, stem=False):\n    text = re.sub(text_cleaning_re, ' ', str(text).lower()).strip();\n    tokens = [];\n    for token in text.split():\n        if token not in stop_words:\n            if stem:\n                tokens.append(stemmer.stem(token))\n            else:\n                tokens.append(token)\n    return \" \".join(tokens)\n\ntext.text = text.text.apply(lambda t: preprocess(t));\n\nlabel = {0: 0, 4: 1};\n\ndef label_decoder(l):\n    return label[l];\n\ntext.sentiment = text.sentiment.apply(lambda l: label_decoder(l))\ntext.head()","cc8a9c17":"x_train, x_test, y_train, y_test = train_test_split(text.text, text.sentiment, test_size=0.2, random_state=17);","ed2428e4":"def max_length():\n    twitt_length = [];\n    for t in x_train:\n        twitt_length.append(len(t));\n    return int(np.ceil(np.mean(twitt_length)));","fdf7c435":"token = Tokenizer(lower=False);\ntoken.fit_on_texts(x_train);\nx_train = token.texts_to_sequences(x_train); #\u043f\u0435\u0440\u0435\u0432\u043e\u0434\u0438\u043c\u043e \u0442\u0435\u043a\u0441\u0442 \u0443 \u0447\u0438\u0441\u043b\u0430\nx_test = token.texts_to_sequences(x_test);#\n\nmaxlen = max_length(); # \u0434\u043b\u044f \u0432\u0438\u0440\u0456\u0432\u043d\u0435\u043d\u043d\u044f \u0434\u043e\u0432\u0436\u0438\u043d\u0438 \nx_train = pad_sequences(x_train, maxlen=maxlen); #\u0432\u0438\u0440\u0456\u0432\u043d\u044e\u0454\u043c\u043e \u0434\u043e\u0432\u0436\u0438\u043d\u0443 \u0442\u0432\u0456\u0442\u0442\u0456\u0432\nx_test = pad_sequences(x_test, maxlen=maxlen);# \n\ntotal_words = len(token.word_index) + 1;   \nprint('\u0417\u0430\u043a\u043e\u0434\u043e\u0432\u0430\u043d\u0430 x_train: \\n', x_train, '\\n');\nprint('\u041c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u0430 \u0434\u043e\u0432\u0436\u0438\u043d\u0430 \u0442\u0432\u0456\u0442\u0442\u0430: ',maxlen);\nprint('\u0412\u0441\u044c\u043e\u0433\u043e \u0441\u043b\u0456\u0432: ', total_words);","52a973aa":"model = keras.Sequential();\nmodel.add(Embedding(total_words, 32, input_length= maxlen));\nmodel.add(SpatialDropout1D(0.1));\nmodel.add(LSTM(8, dropout=0.2, recurrent_dropout=0.3));\nmodel.add(Dense(1, activation='sigmoid'));\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']); ","0a7c2b8e":" model.fit(x_train, y_train, epochs=10,batch_size=32);","4e58406b":"score = model.evaluate(x_test, y_test);\nprint('Test loss:', score[0]);\nprint('Test accuracy:', score[1]);","070c5ed1":"### 2. \u041a\u043b\u0430\u0441\u0438\u0444\u0456\u043a\u0430\u0446\u0456\u044f \u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u044c","75c5c968":"#### \u041a\u043b\u0430\u0441\u0438\u0444\u0456\u043a\u0430\u0446\u0456\u044f:","56d6b674":"## \u041b\u0430\u0431\u043e\u0440\u0430\u0442\u043e\u0440\u043d\u0430 \u0440\u043e\u0431\u043e\u0442\u0430 \u21164 \u041c\u0430\u0437\u0443\u0440 \u0410. \u0424\u0406-12\u043c\u043d\n1. \u0412\u0438\u0440\u0456\u0448\u0456\u0442\u044c \u0437\u0430\u0432\u0434\u0430\u043d\u043d\u044f \u043a\u043b\u0430\u0441\u0438\u0444\u0456\u043a\u0430\u0446\u0456\u0457 \u0434\u0430\u043d\u0438\u0445, \u0437 \u044f\u043a\u0438\u043c\u0438 \u0432\u0438 \u043f\u0440\u0430\u0446\u044e\u0432\u0430\u043b\u0438 \u0432 \u043b\u0430\u0431\u043e\u0440\u0430\u0442\u043e\u0440\u043d\u0456\u0439 \u21162 \u0437\u0430 \u0434\u043e\u043f\u043e\u043c\u043e\u0433\u043e\u044e \n\u043f\u043e\u0432\u043d\u043e\u0437\u0432\u2019\u044f\u0437\u0430\u043d\u043e\u0457 \u043d\u0435\u0439\u0440\u043e\u043c\u0435\u0440\u0435\u0436\u0456 \u043f\u0440\u044f\u043c\u043e\u0433\u043e \u043f\u043e\u0448\u0438\u0440\u0435\u043d\u043d\u044f (fully connected feed-forward network);\n2. \u0412\u0438\u0440\u0456\u0448\u0456\u0442\u044c \u0437\u0430\u0432\u0434\u0430\u043d\u043d\u044f \u043a\u043b\u0430\u0441\u0438\u0444\u0456\u043a\u0430\u0446\u0456\u0457 \u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u044c \u0437\u0430 \u0434\u043e\u043f\u043e\u043c\u043e\u0433\u043e\u044e \u0437\u0433\u043e\u0440\u0442\u043a\u043e\u0432\u043e\u0457 (convolutional) \u043d\u0435\u0439\u0440\u043e\u043c\u0435\u0440\u0435\u0436\u0456 \n(\u044f\u043a\u0449\u043e \u0432 \u043e\u0431\u0440\u0430\u043d\u043e\u043c\u0443 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0456 \u043a\u043b\u0430\u0441\u0456\u0432 \u0437\u0430\u0431\u0430\u0433\u0430\u0442\u043e, \u0434\u043e\u0441\u0442\u0430\u0442\u043d\u044c\u043e \u0437\u0430\u043b\u0438\u0448\u0438\u0442\u0438 3-5);\n3. \u0412\u0438\u0440\u0456\u0448\u0456\u0442\u044c \u043e\u0434\u043d\u0443 \u0437 \u0442\u0440\u044c\u043e\u0445 \u0437\u0430\u0434\u0430\u0447 \u0437\u0430 \u0432\u0430\u0448\u0438\u043c \u0432\u0438\u0431\u043e\u0440\u043e\u043c: \u043a\u043b\u0430\u0441\u0438\u0444\u0456\u043a\u0430\u0446\u0456\u0457 \u0442\u0435\u043a\u0441\u0442\u0456\u0432, \u043a\u043b\u0430\u0441\u0438\u0444\u0456\u043a\u0430\u0446\u0456\u0457 \u0447\u0430\u0441\u043e\u0432\u0438\u0445 \u0440\u044f\u0434\u0456\u0432 \n\u0430\u0431\u043e \u043f\u0440\u043e\u0433\u043d\u043e\u0437\u0443\u0432\u0430\u043d\u043d\u044f \u0437\u043d\u0430\u0447\u0435\u043d\u044c \u0447\u0430\u0441\u043e\u0432\u043e\u0433\u043e \u0440\u044f\u0434\u0443 \u0437\u0430 \u0434\u043e\u043f\u043e\u043c\u043e\u0433\u043e\u044e \u0440\u0435\u043a\u0443\u0440\u0435\u043d\u0442\u043d\u043e\u0457 \u043d\u0435\u0439\u0440\u043e\u043c\u0435\u0440\u0435\u0436\u0456.","4f37b17e":"#### \u0417\u0430\u0432\u0430\u043d\u0442\u0430\u0436\u0435\u043d\u043d\u044f \u0442\u0430 \u043e\u0431\u0440\u043e\u0431\u043a\u0430 \u0434\u0430\u043d\u0438\u0445:","14abfceb":"#### \u041f\u043e\u0431\u0443\u0434\u043e\u0432\u0430 \u043c\u043e\u0434\u0435\u043b\u0456:","1eb73c6a":"#### \u041f\u0456\u0434\u0433\u043e\u0442\u043e\u0432\u043a\u0430 \u0434\u0430\u043d\u0438\u0445 \u0434\u043b\u044f \u043d\u0435\u0439\u0440\u043e\u043c\u0435\u0440\u0435\u0436\u0456:","ff718668":"### 3. \u041a\u043b\u0430\u0441\u0438\u0444\u0456\u043a\u0430\u0446\u0456\u0457 \u0442\u0435\u043a\u0441\u0442\u0456\u0432","b2e6b10d":"#### \u041f\u0435\u0440\u0435\u0434\u043e\u0431\u0440\u043e\u0431\u043a\u0430 \u0442\u0435\u043a\u0441\u0442\u0443:","4501455e":"### 1. \u041a\u043b\u0430\u0441\u0438\u0444\u0456\u043a\u0430\u0446\u0456\u0457 \u0434\u0430\u043d\u0438\u0445"}}