{"cell_type":{"83df8244":"code","df8a3969":"code","fe261381":"code","bd3a2c4f":"code","0f114f3c":"code","0aa5e011":"code","8f73e81a":"code","cc0ed5bb":"code","17441c4c":"code","0d5c19ad":"code","4efbd328":"code","bfa39707":"code","1976d371":"code","0af7086b":"code","16f72aec":"code","0b176f35":"code","78e0c869":"code","f48b41ad":"code","863fbee6":"code","60ab3e21":"code","128bedb3":"code","ba0364e0":"code","ef00d75a":"code","972f0e27":"code","5561455c":"code","0f23faab":"code","ace44a0c":"code","ccd5bdb0":"markdown","221896fa":"markdown","7b1ef3c7":"markdown","66231388":"markdown","842ef4b0":"markdown","b4a6f120":"markdown","8da8345a":"markdown","1080d70f":"markdown","b919cd40":"markdown","855de7d0":"markdown","b831fe30":"markdown","b6df172b":"markdown"},"source":{"83df8244":"# pandas to open data files & processing it.\nimport pandas as pd\n# to see all columns\npd.set_option('display.max_columns', None)\n# To see whole text\npd.set_option('max_colwidth', -1)\n\n# numpy for numeric data processing\nimport numpy as np\n\n# keras for deep learning model creation\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Flatten, Bidirectional, GlobalMaxPool1D\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.utils import plot_model\n\n# to fix random seeds\nimport random\nimport tensorflow as tf\nimport torch\nimport os\n\n# Regular Expression for text cleaning\nimport re\n\n# to track the progress - progress bar\nfrom tqdm.notebook import tqdm","df8a3969":"sarcasm_data = pd.read_csv(\"..\/input\/sarcasm\/train-balanced-sarcasm.csv\")\nprint(sarcasm_data.shape)\nsarcasm_data.head()","fe261381":"sarcasm_data.drop(['author', 'subreddit', 'score', 'ups', 'downs', 'date', 'created_utc', 'parent_comment'], axis=1, inplace=True)\n# remove empty rows\nsarcasm_data.dropna(inplace=True)\nsarcasm_data.head()","bd3a2c4f":"sarcasm_data['label'].value_counts()","0f114f3c":"mispell_dict = {\"ain't\": \"is not\", \"cannot\": \"can not\", \"aren't\": \"are not\", \"can't\": \"can not\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",\n                \"doesn't\": \"does not\",\n                \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\", \"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\",\n                \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\", \"I'm\": \"I am\",\n                \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \"i'll've\": \"i will have\", \"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n                \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\", \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\n                \"mightn't\": \"might not\", \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n                \"needn't've\": \"need not have\", \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\",\n                \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n                \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\", \"so's\": \"so as\", \"this's\": \"this is\", \"that'd\": \"that would\",\n                \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\", \"they'd\": \"they would\",\n                \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\",\n                \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\",\n                \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\",\n                \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n                \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"wont\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\",\n                \"wouldn't\": \"would not\",\n                \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\", \"y'all'd've\": \"you all would have\", \"y'all're\": \"you all are\", \"y'all've\": \"you all have\",\n                \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", 'colour': 'color',\n                'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor',\n                'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What',\n                'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I',\n                'theBest': 'the best', 'howdoes': 'how does', 'Etherium': 'Ethereum',\n                'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what',\n                'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization'}\n\nmispell_dict = {k.lower(): v.lower() for k, v in mispell_dict.items()}","0aa5e011":"import nltk\nimport re\n\nstop_words = nltk.corpus.stopwords.words('english')\n\ndef normalize_document(doc):\n    # lower case and remove special characters\\whitespaces\n    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n    doc = doc.lower()\n    doc = doc.strip()\n    # tokenize document\n    tokens = nltk.word_tokenize(doc)\n    # filter stopwords out of document\n    filtered_tokens = [token for token in tokens if token not in stop_words]\n    # re-create document from filtered tokens\n    doc = ' '.join(filtered_tokens)\n    return doc","8f73e81a":"normalize_document('I am a girl!#12 342')","cc0ed5bb":"# apply preprocessing_text function\nsarcasm_data['comment'] = sarcasm_data['comment'].apply(normalize_document)\nsarcasm_data.head()","17441c4c":"# total unique words we are going to use.\nTOTAL_WORDS = 40000\n\n# max number of words one sentence can have\nMAX_LEN = 50\n\n# width of of 1D embedding vector\nEMBEDDING_SIZE = 300","0d5c19ad":"%%time\ntokenizer = Tokenizer(num_words=TOTAL_WORDS)\ntokenizer.fit_on_texts(list(sarcasm_data['comment']))\n\ntrain_data = tokenizer.texts_to_sequences(sarcasm_data['comment'])\ntrain_data = pad_sequences(train_data, maxlen = MAX_LEN)\ntarget = sarcasm_data['label']","4efbd328":"open(EMBEDDING_FILE)","bfa39707":"%%time\nEMBEDDING_FILE = '..\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec'\n\ndef get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n\nembeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in tqdm(open(EMBEDDING_FILE)))\n\nword_index = tokenizer.word_index\nnb_words = min(TOTAL_WORDS, len(word_index))\nembedding_matrix = np.zeros((nb_words, EMBEDDING_SIZE))","1976d371":"for word, i in tqdm(word_index.items()):\n    if i >= TOTAL_WORDS: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","0af7086b":"embedding_matrix.shape","16f72aec":"def seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    tf.random.set_seed(seed)\n\n# We fix all the random seed so that, we can reproduce the results.\nseed_everything(2020)","0b176f35":"input_layer = Input(shape=(MAX_LEN,))\n\nembedding_layer = Embedding(TOTAL_WORDS, EMBEDDING_SIZE, weights = [embedding_matrix])(input_layer)\n\nLSTM_layer = Bidirectional(LSTM(128, return_sequences = True))(embedding_layer)\nmaxpool_layer = GlobalMaxPool1D()(LSTM_layer)\n\ndense_layer_1 = Dense(64, activation=\"relu\")(maxpool_layer)\ndropout_1 = Dropout(0.5)(dense_layer_1)\n\ndense_layer_2 = Dense(32, activation=\"relu\")(dropout_1)\ndropout_2 = Dropout(0.5)(dense_layer_2)\n\noutput_layer = Dense(1, activation=\"sigmoid\")(dropout_2)\n\nmodel = Model(input_layer, output_layer)\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()","78e0c869":"plot_model(model, show_shapes=True)","f48b41ad":"BATCH_SIZE = 512\nEPOCHS = 2\n\nhistory = model.fit(\n    train_data, target,\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    # We are using randomly selected 20% sentences as validation data.\n    validation_split=0.2\n)","863fbee6":"model.save('model.h5')","60ab3e21":"model = load_model('model.h5')","128bedb3":"sarcasm_data[sarcasm_data['label']==1].sample(20)","ba0364e0":"sentence = \"sun rises from the east\"\nsentence = normalize_document(sentence)\nprint(sentence)\n\nsentence = tokenizer.texts_to_sequences([sentence])\nsentence = pad_sequences(sentence, maxlen = MAX_LEN)\nsentence","ef00d75a":"# Make the prediction.\nprediction = model.predict(sentence)\nprediction[0][0]","972f0e27":"print(\"So, it's saying sentence have probability of %.3f percent\"%(prediction[0][0]*100))","5561455c":"sentence = \"Isn't it great that, your girlfriend dumped you?\"\nsentence = normalize_document(sentence)\nprint(sentence)\n\nsentence = tokenizer.texts_to_sequences([sentence])\nsentence = pad_sequences(sentence, maxlen = MAX_LEN)\nsentence","0f23faab":"# Make the prediction.\nprediction = model.predict(sentence)\nprediction[0][0]","ace44a0c":"print(\"So, it's saying sentence have probability of %.3f percent\"%(prediction[0][0]*100))","ccd5bdb0":"Let's open embedding file now & store in a matrix.","221896fa":"## Let's start the training of our model","7b1ef3c7":"### Cool !!\n\nTraining is over.<br>\nWe can see validation accuracy above.<br>\n\n### Let's test our model on some random input now.","66231388":"Let's build our NLP deep learning model now..","842ef4b0":"So, there are `505368` sentences with sarcastic news headlines !!<br>\n\nLet's do some pre-processing on our text data.<br>\nThese are the common practices which can improve performance in almost any NLP task.<br><br>\n\nOne common thing we can do is to remove `contractions`.<br>\n### Like, \"ain't\" to \"is not\", \"can't\" to \"can not\" etc.","b4a6f120":"## Before training the model first, let's understand our model first.\n\n`input_layer` : Input layer with which we will get text sentence.<br><br>\n`embedding_layer` : Embedding layer with which we will map each word with it's corresponding embedding vector.<br><br>\n`LSTM_layer` : LSTM layer with 128 LSTM cells.\n* We are using Bidirectional to run LSTM from both side of the text sentence.\n    1. Left to Right\n    2. Right to Left\n* Purpose of this is to give our model both side context.\n* It's also possible to not use this. But using this have provided good results.\n<br><br>\n`maxpool_layer` : Max pool layer is used to minimize the image size by pooling maximum number out of 2x2 grid.\n![maxpool](https:\/\/distilledai.com\/wp-content\/uploads\/2020\/04\/2x2-max-pool-CNN.png)\n<br><br>\n`dense_layer_1` : Feed-forward dense layer to classify the features captured by LSTM layer.\n<br><br>\n`dropout_1` : Dropout is interesting trick. In Dropout, we randomly turn off some percentage of our neurons so that their's output can't go to next layer. Here we are turning off 20% of our total neurons.\n* Purpose of doing this is again to make our training robust.\n* Network should not depend some specific neurons to make predictions. And random turn will allow us to do that.\n* Picture below help us to understand it. \n![dropout](https:\/\/distilledai.com\/wp-content\/uploads\/2020\/04\/dropout-in-deep-learning.png)\n<br><br>\n`dense_layer_2` & `dropout_2` are same as above.\n<br><br>\n`output_layer` : To get the output prediction from the neural network.\n","8da8345a":"We just need `comment` & `label` column.<br>\nSo, let's remove others.","1080d70f":"Let's open the data files now.","b919cd40":"Let's make our preprocessing function.","855de7d0":"We can see, how our sentence got converted into numbers.<br>\n\n## We are doing padding to keep the final length same for every sentence no matter the sentence length.\n## Our Neural network have learned to ignore 0 in the training itself.","b831fe30":"Today in this lesson, we will learn the basics of Natural Language Processing(NLP) by building a model on \"Sarcasm Detection\".<br><br>\n![sarcasm](https:\/\/distilledai.com\/wp-content\/uploads\/2020\/04\/sarcasm.png)\n\nWe will use LSTM (Long Short Term Memory) layers to create our deep learning model who can predict whether a statement is sarcastic or not.<br><br>\n\nBut before going to start the coding part of the lesson, let's first understand basics of few things that we are going to use.<br>\n\n## Tokenize\nIn NLP, we work with text data.<br>\nBut, machines \/ computers can't read text. They just deal with numbers.<br>\nSo, to convert text data into numbers, we give a unique ID number to each word & replace word it's corresponding number.\n### That's what Tokenizer does.\nIt converts our text data to numeric format.<br>\nLike, sentence `\"machine learning is cool\"` will convert into something like `\"2315 834 12 4510\"`.<br><br>\n\nAfter converting our text data into numbers, now, we can use that data to train our ML models.<br>\nBut, there's one thing we are missing.<br><br>\n\nAfter converting, each word into numbers, `king` word will converted into something like 1678, `queen` to something like 56832, `man` to something like 4285, `woman` to something like 6387 etc. (These numbers I have choosen randomly. Point being, it'll converted to numbers).<br>\n\n* How, we will give our model information like, `king` & `queen` are related same as `man` & `woman`.<br>\n* Or, `italy`, `spain`, `brazil`, `india`, `japan` etc. words have one thing in common. (They all are name of countris)<br>\n* Or, `cricket`, `football`, `tennis` etc. words are also similar. (names of games)<br>\n* Or, the way words `china` & `beijing` are related, words `russia` & `moscow` are also related in the same way.<br><br>\n\nAll these are knowledge related things.<br>\nHow can we give this knowledge to our NLP models so that, it can use that knowledge to perform better at detecting sarcasm?<br>\n\nAnswer is `word embeddings`.\n\n## Word embeddings\n\nWord embeddings are nothing but 1D vector of length 200-500, containing floating point numbers.<br>\nEach word have corresponding 1D array for it.<br><br>\n\nFor e.g.<br>\n`king` : `[-0.34, 0.98, -0.04, 0.32]`<br>\n`queen` : `[0.25, -0.23, -0.10, -0.72]`<br>\n.<br>\n.<br>\n.<br>\n\nHere it's just 4 numbers in a 1D vector, but in actual there will be 200-500 numbers in a 1D vector for each word.<br>\n\n### Why we use word embeddings?\nThese word embeddings captures intrinsic meaning in some way for each words.<br>\nIt kind of stores \u201cmeaning\u201d of words or \u201cinsights\u201d of the words inside them.<br>\nFor e.g.<br>\n![gender](https:\/\/distilledai.com\/wp-content\/uploads\/2020\/04\/word-embeddings-gender.png)\n<center>The way man & woman are related, king & queen, uncle & aunt are related the same way. <\/center><br>\n\nWe can even perform mathematical operations on word embeddings & get expected results.<br>\n![gender maths](https:\/\/distilledai.com\/wp-content\/uploads\/2020\/04\/word-embeddings-king-queen-man-woman.png)\n<center>If we subtract word embeddings of `man` from `king` & add word embedding of `woman`, we get word embedding of `queen` ([source](http:\/\/jalammar.github.io\/illustrated-word2vec\/))<\/center><br>\n\n![word embeddings country](https:\/\/distilledai.com\/wp-content\/uploads\/2020\/04\/word-embeddings-country-capital.png)\n<center>Relation between country & capitals ([source](https:\/\/blog.acolyer.org\/2016\/04\/21\/the-amazing-power-of-word-vectors\/))<\/center><br>\n\n### We use word embeddings to transfer the knowledge of words to model so that it can perform better.\n\n### How do we get these word embeddings?\n* There's something called [Language Models](https:\/\/en.wikipedia.org\/wiki\/Language_model).<br>\n* They basically learn to predict next word of the sentence given all the past words.<br>\n* We will not go there as of now. If you want to learn more about it, I recommend reading [this blog](https:\/\/www.analyticsvidhya.com\/blog\/2019\/08\/comprehensive-guide-language-model-nlp-python-code\/).\n\nSo, now we have understood tokenizer & word embeddings.<br>\nLet's also quickly understand LSTM.<br>\n\n## LSTM (Long Short Term Memory)\n\nLSTM is type of RNN (Recurrent Neural Network) which \"remembers\" text to perform better in the task we are doing.<br>\nFor e.g. <br>\nTo answer the question, `\"Bob is from France. Alice is from Japan. So, what's the mother-tongue of Bob?\"`<br>\nWe need to remember the first sentence, `\"Bob is from France\"`.<br>\nWithout knowing \/ remembering that, we can't answer the question `\"French\"`<br><br>\n\nSame way, LSTM remembers, past text to perform better at the task we are doing.<br>\n\nI highly recommend to check out [this blog](https:\/\/medium.com\/mlreview\/understanding-lstm-and-its-diagrams-37e2f46f1714) to understand LSTM. <br>That blog contains a simple explanation of LSTM that almost everyone can understand.<br>\n\nHere's what a cell of LSTM looks like.<br>\n![LSTM](https:\/\/distilledai.com\/wp-content\/uploads\/2020\/04\/LSTM-cell-RNN.png)\n<center>An LSTM cell ([source](https:\/\/medium.com\/mlreview\/understanding-lstm-and-its-diagrams-37e2f46f1714))<\/center>\n<br>\nThere are 3 valves in a single LSTM cell.<br>\n1. **Forget valve**\n    * This valve decides what should be forgetted from the past text. What things are irrelevant to remember.\n    * First part from left is forget valve.\n2. **Memory valve**\n    * This valve decides how much new memory should influence the old memory.\n    * New memory is generated by a single layer neural network with pentagon shaped activation function(tanh).\n    * Output will be element-wise multiple the new memory valve, and add to the old memory to form the new memory.\n    * Memory valve is in the middle of the above figure.\n3. **Output valve**\n    * This valve generates output of this perticular LSTM cell.\n    * It's in the right part of above LSTM cell figure.\n    * It's controlled by new memory & previous output.\n    * This valve controls how much new memory should output to the next LSTM unit.\n<br>\n\n### In one LSTM layer, there will be a chain of this type of LSTM cells. \n\nSo, that's enough of theoratical talking.<br>\nLet's do it by code.","b6df172b":"## Sarcasm is a subjective matter.<br> One person can call a sentence sarcastic & another don't.\n### So, results may be subjective. But our goal was to understand some basic concepts of NLP.\n\n# Summary\n\n* We saw what Tokenization is.\n* We saw what word embedings are.\n* We understood how a LSTM cell works in a nutshell.\n* We cleaned the text data to use it in training\n* We trained a deep learning model"}}