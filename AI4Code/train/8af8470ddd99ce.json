{"cell_type":{"d4a79795":"code","181645f4":"code","1c2eb904":"code","c2d349f7":"code","eaa8ba90":"code","61641c8c":"code","290d8559":"code","c870a313":"code","70bd56cc":"code","80e094ae":"code","7e938194":"code","8e5b7dcc":"code","df15900c":"code","db202613":"code","8422bcd2":"code","84f1956f":"code","c218b106":"code","cd4bfa39":"code","c13049aa":"code","ccf11abc":"code","f33c9649":"markdown","95899003":"markdown","221d52da":"markdown","b49a5365":"markdown","03cb3bb8":"markdown","0bedda7f":"markdown","e205faa7":"markdown","01f78d4c":"markdown","c2faf388":"markdown","693fd2e8":"markdown","c13df12d":"markdown","bf1ac1d3":"markdown","4dcee062":"markdown","83a59eea":"markdown","49bff506":"markdown","0f11476d":"markdown","446d0d1f":"markdown","c2148ba6":"markdown"},"source":{"d4a79795":"import keras\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.interpolate import interp1d","181645f4":"from __future__ import print_function, division\n\nfrom keras.datasets import mnist\nfrom keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply\nfrom keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.layers.convolutional import UpSampling2D, Conv2D\nfrom keras.models import Sequential, Model\nfrom keras.optimizers import Adam\n\nimport matplotlib.pyplot as plt\n\nimport numpy as np\n\nclass ACGAN():\n    def __init__(self, input_rows, input_cols, input_channels, input_classes, latent_dim=100):\n        # Input shape\n        self.img_rows = input_rows\n        self.img_cols = input_cols\n        self.channels = input_channels\n        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n        self.num_classes = input_classes\n        self.latent_dim = latent_dim\n\n        optimizer = Adam(0.0002, 0.5)\n        losses = ['binary_crossentropy', 'sparse_categorical_crossentropy']\n\n        # Build and compile the discriminator\n        self.discriminator = self.build_discriminator()\n        self.discriminator.compile(loss=losses,\n            optimizer=optimizer,\n            metrics=['accuracy'])\n\n        # Build the generator\n        self.generator = self.build_generator()\n\n        # The generator takes noise and the target label as input\n        # and generates the corresponding digit of that label\n        noise = Input(shape=(self.latent_dim,))\n        label = Input(shape=(1,))\n        img = self.generator([noise, label])\n\n        # For the combined model we will only train the generator\n        self.discriminator.trainable = False\n\n        # The discriminator takes generated image as input and determines validity\n        # and the label of that image\n        valid, target_label = self.discriminator(img)\n\n        # The combined model  (stacked generator and discriminator)\n        # Trains the generator to fool the discriminator\n        self.combined = Model([noise, label], [valid, target_label])\n        self.combined.compile(loss=losses,\n            optimizer=optimizer)\n\n    def build_generator(self):\n\n        model = Sequential()\n\n        model.add(Dense(128 * 7 * 7, activation=\"relu\", input_dim=self.latent_dim))\n        model.add(Reshape((7, 7, 128)))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(UpSampling2D())\n        model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n        model.add(Activation(\"relu\"))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(UpSampling2D())\n        model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n        model.add(Activation(\"relu\"))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Conv2D(self.channels, kernel_size=3, padding='same'))\n        model.add(Activation(\"tanh\"))\n\n        model.summary()\n\n        noise = Input(shape=(self.latent_dim,))\n        label = Input(shape=(1,), dtype='int32')\n        label_embedding = Flatten()(Embedding(self.num_classes, 100)(label))\n\n        model_input = multiply([noise, label_embedding])\n        img = model(model_input)\n\n        return Model([noise, label], img)\n\n    def build_discriminator(self):\n\n        model = Sequential()\n\n        model.add(Conv2D(16, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.25))\n        model.add(Conv2D(32, kernel_size=3, strides=2, padding=\"same\"))\n        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.25))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.25))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Conv2D(128, kernel_size=3, strides=1, padding=\"same\"))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.25))\n\n        model.add(Flatten())\n        model.summary()\n\n        img = Input(shape=self.img_shape)\n\n        # Extract feature representation\n        features = model(img)\n\n        # Determine validity and label of the image\n        validity = Dense(1, activation=\"sigmoid\")(features)\n        label = Dense(self.num_classes+1, activation=\"softmax\")(features)\n\n        return Model(img, [validity, label])\n\n    def train(self, X_train, y_train, epochs, batch_size=128, sample_interval=50):\n\n        # Load the dataset\n        #(X_train, y_train), (_, _) = mnist.load_data()\n        #X_train, y_train = self.X_train, self.y_train\n\n        # Configure inputs\n        X_train = (X_train.astype(np.float32) - 127.5) \/ 127.5\n        X_train = np.expand_dims(X_train, axis=3)\n        y_train = y_train.reshape(-1, 1)\n\n        # Adversarial ground truths\n        valid = np.ones((batch_size, 1))\n        fake = np.zeros((batch_size, 1))\n\n        # Loss output\n        g_loss_epochs = np.zeros((epochs, 1))\n        d_loss_epochs = np.zeros((epochs, 1))\n\n        for epoch in range(epochs):\n\n            # ---------------------\n            #  Train Discriminator\n            # ---------------------\n\n            # Select a random batch of images\n            idx = np.random.randint(0, X_train.shape[0], batch_size)\n            imgs = X_train[idx]\n\n            # Sample noise as generator input\n            noise = np.random.normal(0, 1, (batch_size, 100))\n\n            # The labels of the digits that the generator tries to create an\n            # image representation of\n            sampled_labels = np.random.randint(0, 10, (batch_size, 1))\n\n            # Generate a half batch of new images\n            gen_imgs = self.generator.predict([noise, sampled_labels])\n\n            # Image labels. 0-9 if image is valid or 10 if it is generated (fake)\n            img_labels = y_train[idx]\n            fake_labels = 10 * np.ones(img_labels.shape)\n\n            # Train the discriminator\n            d_loss_real = self.discriminator.train_on_batch(imgs, [valid, img_labels])\n            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, [fake, fake_labels])\n            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n\n            # ---------------------\n            #  Train Generator\n            # ---------------------\n\n            # Train the generator\n            g_loss = self.combined.train_on_batch([noise, sampled_labels], [valid, sampled_labels])\n\n            #show the final losses\n            g_loss_epochs[epoch] = g_loss[0]\n            d_loss_epochs[epoch] = d_loss[0]\n\n            # If at save interval => save generated image samples\n            if epoch % sample_interval == 0:\n                # Plot the progress\n                print (\"Epoch: %d [D loss: %f, acc.: %.2f%%, op_acc: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[3], 100*d_loss[4], g_loss[0]))\n                #do not save model\n                #self.save_model()\n                self.sample_images(epoch, smp_rows=2, smp_cols=10, save_img=False)\n\n        return g_loss_epochs, d_loss_epochs\n \n\n    #row, cols to be sampled\n    def sample_images(self, epoch, smp_rows=5, smp_cols=10, save_img=True, fig_size=(8, 3)):\n        r, c = smp_rows, smp_cols\n        noise = np.random.normal(0, 1, (r * c, 100))\n        sampled_labels = np.array([num for _ in range(r) for num in range(c)])\n        gen_imgs = self.generator.predict([noise, sampled_labels])\n        # Rescale images 0 - 1\n        gen_imgs = 0.5 * gen_imgs + 0.5\n\n        #plt.figure(figsize=fig_size)\n        fig, axs = plt.subplots(r, c, figsize=fig_size)\n        cnt = 0\n        for i in range(r):\n            for j in range(c):\n                axs[i,j].imshow(gen_imgs[cnt,:,:,0], cmap='gray')\n                axs[i,j].axis('off')\n                cnt += 1\n        if save_img:\n            fig.savefig(\"..\/images\/%d.png\" % epoch)\n        else:\n            #plt.figure(figsize=fig_size)\n            plt.show()\n        plt.close()\n\n    def sample_single_image(self, noise, label):\n        gen_imgs = self.generator.predict([noise, np.array(label).reshape((1, ))])\n        # Rescale images 0 - 1\n        gen_imgs = 0.5 * gen_imgs + 0.5\n        plt.imshow(gen_imgs[0, :, :, 0], cmap='gray')\n\n    def save_model(self):\n\n        def save(model, model_name):\n            model_path = \"..\/saved_model\/%s.json\" % model_name\n            weights_path = \"..\/saved_model\/%s_weights.hdf5\" % model_name\n            options = {\"file_arch\": model_path,\n                        \"file_weight\": weights_path}\n            json_string = model.to_json()\n            #\n            open(options['file_arch'], 'w').write(json_string)\n            model.save_weights(options['file_weight'])\n\n        save(self.generator, \"generator\")\n        save(self.discriminator, \"discriminator\")","1c2eb904":"#label dictionary\nlabel_dict = {0: 'tshirt',\n              1: 'trouser',\n              2: 'pullover',\n              3: 'dress',\n              4: 'coat',\n              5: 'sandal',\n              6: 'shirt',\n              7: 'sneaker',\n              8: 'bag',\n              9: 'boot'}","c2d349f7":"#input dimensions\ninput_rows = 28\ninput_cols = 28\ninput_channels = 1","eaa8ba90":"#load the data with the format needed in the AC-GAN implementation\ndef load_fashion_mnist(input_rows, input_cols, path='..\/input\/fashion-mnist_train.csv'):\n    #read the csv data\n    df = pd.read_csv(path)\n    #extract the image pixels\n    X_train = df.drop(columns = ['label'])\n    X_train = X_train.as_matrix()\n    X_train = X_train.reshape(X_train.shape[0], input_rows, input_cols)\n    #extract the labels\n    y_train = df['label'].as_matrix()\n    \n    return X_train, y_train","61641c8c":"X_train, y_train = load_fashion_mnist(input_rows, input_cols)","290d8559":"X_train.shape, y_train.shape, type(X_train), type(y_train)","c870a313":"input_classes = pd.Series(y_train).nunique()","70bd56cc":"#the instance of the ACGAN class\n#we can see the Discriminator and Generator architecture\nfashion_acgan = ACGAN(input_rows, input_cols, input_channels, input_classes)","80e094ae":"#Train the AC-GAN with the fashion-mnist data\ng_loss, d_loss = fashion_acgan.train(X_train, y_train, epochs=2000, batch_size=100, sample_interval=200)","7e938194":"#Show the Generator-Descriminator loss for every epoch\ndef plot_gan_losses(g_loss, d_loss):\n    plt.plot(g_loss)\n    plt.plot(d_loss)\n    plt.title('GAN Loss evolution')\n    plt.ylabel('')\n    plt.xlabel('epoch')\n    plt.legend(['Generator', 'Discriminator'], loc='upper left')\n    plt.show()","8e5b7dcc":"plot_gan_losses(g_loss, d_loss)","df15900c":"fashion_acgan.sample_images(0, smp_rows=5, smp_cols=10, save_img=False, fig_size=(10, 10))","db202613":"# draw interpolated samples between two arbitrary points\ndef show_interp_samples(point1, point2, N_samples_interp, input_classes):\n    #pick N_samples_interp + 2 points (one is the start, noise_1, and the other the end, noise_2)\n    N_samples_interp_all = N_samples_interp + 2\n    N_labs = input_classes\n    plt.figure(figsize=(11,11))\n    plt.plot([N_labs, N_samples_interp_all, (N_samples_interp_all * N_labs) + 1])\n    \n    #fit a line between point1 and point2\n    line = interp1d([1, N_samples_interp_all], np.vstack([point1, point2]), axis=0)\n    \n    #for every label\n    for lab in range(input_classes):\n        #draw N_samples_interp_all samples from the fitted line\n        for i in range(N_samples_interp_all):\n            ax = plt.subplot(N_labs, N_samples_interp_all, 1 + (i + (lab*N_samples_interp_all)))\n            plt.axis('off')\n            fashion_acgan.sample_single_image(line(i + 1).reshape((1, 100)), lab)","8422bcd2":"np.random.seed(seed=42)\n#get a random point\nnoise_1 = np.random.normal(0, 1, (1, 100))\n#draw samples between that point, and that same point with all coordinates scaled by -1\nshow_interp_samples(noise_1, -noise_1, 4, input_classes)","84f1956f":"#draw another arbitrary point\nnoise_2 = np.random.normal(0, 1, (1, 100))\n#draw samples between these two arbitrary points\nshow_interp_samples(noise_1, noise_2, 4, input_classes)","c218b106":"id_label_sample = 8\n#yet another noise sample\nyan = np.random.normal(0, 1, (1, 100))\nimg_smp = fashion_acgan.generator.predict([yan, np.array(id_label_sample).reshape(1, )])[0, :, :, 0]\nimg_smp = img_smp*0.5 + 0.5","cd4bfa39":"def plot_n_closest(img_smp, id_label_sample, X_train, N_closest, fig_size=(8, 3)):\n    #get the images that belong to id_label_sample\n    idx_lab = np.where(y_train==id_label_sample)\n    X_lab = (X_train[idx_lab, :, :].astype(np.float32) \/ 255)[0, :, :, :]\n    \n    #apply the norm between X_lab and the sampled image across all images\n    L1d = np.sum(np.apply_along_axis(np.linalg.norm, -1, X_lab - img_smp, ord=1), axis=1)\n    idx_l1_sort = L1d.argsort()\n    #plt.imshow(X_lab[idx_l1_sort[0], :, :], cmap='gray')\n    fig, axs = plt.subplots(1, N_closest, figsize=fig_size)\n    for i in range(N_closest):\n        axs[i].imshow(X_lab[idx_l1_sort[i], :, :], cmap='gray')\n        axs[i].axis('off')\n    plt.show()","c13049aa":"#original image\nplt.imshow(img_smp, cmap='gray')","ccf11abc":"plot_n_closest(img_smp, 8, X_train, 5, fig_size=(8, 8))","f33c9649":"### Samples between interpolated points","95899003":"Two different tests will be done to the AC-GAN to asses the quality of the generated images:\n*    Draw samples from interpolated points between two arbitrary points\n*    Check the distance between generated images and training images\n\nGANs that have 'overfit' show abrupt changes when there are small changes in the input noise. Well trained GANs should have more smooth output changes when are fed with slightly different input noise. The second check tries to asses if the AC-GAN has overfit to the training data, meaning that it will output a copy of input images. ","221d52da":"## Sample images from the final trained AC-GAN","b49a5365":"\n<img src=\"https:\/\/s3-ap-south-1.amazonaws.com\/av-blog-media\/wp-content\/uploads\/2017\/06\/11000153\/g1.jpg\" width=\"600\"><\/img>\n<figcaption>GAN diagram, taken from https:\/\/www.analyticsvidhya.com\/blog\/2017\/06\/introductory-generative-adversarial-networks-gans\/<\/figcaption>\n","03cb3bb8":"Most classes we get interesting outputs, while for the 6th column (sandals) we get not so nice generated images. In the good side, it can also be seen intra-class variability, which is a good sign that the ac-gan is performing well.","0bedda7f":"After some reading I decided to use an 'AC-GAN', that stand for Auxiliar Classifier GAN. The main difference between the AC-GAN and a plain GAN, is that to train the AC-GAN we need both training samples, and training labels. These Training labels are fed to the generator, that will generate fake images using both the random noise, and the input label. The discriminator also has to predict the source of the image (i.e. if it is a fake image or not), and in the AC-GAN scenario, it also has to predict the label of the image.","e205faa7":"## Generated images analysis","01f78d4c":"## **Introduction**","c2faf388":"Keras currently has no AC-GAN implementation out of the shelf, so after some googling, I came across [this](https:\/\/github.com\/eriklindernoren\/Keras-GAN\/) nice repo, where there many GAN architectures implemented in Keras. One of the implemented architectures is the [AC-GAN](https:\/\/github.com\/eriklindernoren\/Keras-GAN\/blob\/master\/acgan\/acgan.py).\nI made some changes in that implementation, this is a short summary of what these changes intended to do:\n*   being able to train with more datasets than just standard mnist\n*   showing samples of generated images during training\n*   sampling images can be shown in cells, not only into files","693fd2e8":"### L1 distance evaluation","c13df12d":"Now, let's get another generated image, and see whether if it is a copy from any training image.","bf1ac1d3":"For more detailed information, I recommend reading these papers:\n*     Generative Adversarial Networks: https:\/\/arxiv.org\/pdf\/1406.2661.pdf\n*     Conditional Image Synthesis with Auxiliary Classifier GANs: https:\/\/arxiv.org\/pdf\/1610.09585.pdf","4dcee062":"It can be seen that in the first epochs (first 200) the generator has huge loss values, while the discriminator has lower loss values, meaning that the generator is not being able to trick the discriminator yet. The loss for the generator decreases after some epochs, arriving to a stable point after epoch 700.","83a59eea":"In the previous cell we train our AC-GAN with the fashion-mnist data. We can see that in the first 200 epochs the generator is doing a poor job, and outputs mainly noise. The output gets richer iteration after iteration, getting convincing results in the last training iterations.","49bff506":"## AC-GAN implementation","0f11476d":"GANs are an interesting topic I have always wanted to play with. There are already many types of GANs, in general, a GAN consists of a discriminator and a generator, in which the generator makes new (fake) samples, and feed them to the discriminator, that has to detect whether if the sample is fake, or if it comes from the training dataset. ","446d0d1f":"In the previous image we have 10 rows (one per class), and 6 columns (2 for the extreme points, and 4 interpolated points in the middle). \nThe output for the sandal class is quite poor, and the output or the third and fourth columns is also very poor, probably because for these two colums many coordinates will be around 0, and during training the AC-GAN is not fed with points following that distribution. It can also be seen better quality outputs for the first, second, fifth and sixth columns. There are no abrupt changes at first sight either. To fix the issue around the 0s region, let's interpolate intermediate inputs from two arbitrary points.","c2148ba6":"We can visually asses, that the generated images similar to the closest ones, without being an exact copy as well."}}