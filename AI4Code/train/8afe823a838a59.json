{"cell_type":{"f7edc773":"code","564ffdff":"code","858f2a38":"code","0926ec18":"code","b4b7557f":"code","e7b476c1":"markdown","07baabf7":"markdown","5f5ac4b7":"markdown","d4859509":"markdown","8c69e04b":"markdown"},"source":{"f7edc773":"import os, string\nfrom __future__ import print_function\nfrom gensim.models import KeyedVectors\nimport numpy as np\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt","564ffdff":"def plot_with_labels(low_dim_embs, labels, filename='tsne.png'):\n    assert low_dim_embs.shape[0] >= len(labels), \"More labels than embeddings\"\n    plt.figure(figsize=(18, 18))  # in inches\n    for i, label in enumerate(labels):\n        x, y = low_dim_embs[i, :]\n        plt.scatter(x, y)\n        plt.annotate(label,\n                 xy=(x, y),\n                 xytext=(5, 2),\n                 textcoords='offset points',\n                 ha='right',\n                 va='bottom')\n    plt.savefig(filename)","858f2a38":"def clean_str(text):\n    \n    text = text.translate(string.punctuation)\n\n    return text","0926ec18":"# Limita o n\u00famero de tokens que ser\u00e3o visualizados\nlimit = 200\n# dimensionalidade do word vector\nvector_dim = 50\n\n#filename = \"..\/input\/word2vec-google\/GoogleNews-vectors-negative{0}.bin\".format(vector_dim)\nfilename = \"..\/input\/word2vec-cbow-50\/cbow_s50.txt\"\n\nmodel = KeyedVectors.load_word2vec_format(filename, binary=False)\n\n\n# Obtendo Tokens e vetores\nwords = []\nembedding = np.array([])\ni = 0\nfor word in model.vocab:\n    # Interrompe o loop se o limite exceder\n    if i == limit: break\n        \n    words.append(clean_str(word))\n    embedding = np.append(embedding, model[word])\n    i += 1\n    \nembedding = embedding.reshape(limit, vector_dim)","b4b7557f":"tsne = TSNE(n_components=2)\nlow_dim_embedding = tsne.fit_transform(embedding)\n\n# plota o gr\u00e1fico\nplot_with_labels(low_dim_embedding, words)","e7b476c1":"## Visualizando gr\u00e1ficos de Word Embeddings\n\nO c\u00f3digo abaixo ajuada a importar um word vector previamente treinado em conte\u00fados do Google News, usando a abordagem Word2Vec, e plotar o gr\u00e1fico que exprime os relacionamentos sem\u00e2nticos entre as palavras codificadas nos vetores.","07baabf7":"## Definindo uma fun\u00e7\u00e3o para plotar o gr\u00e1fico de Word Embeddings","5f5ac4b7":"## Prepara a visualiza\u00e7\u00e3o dos dados usando **TSNE**  (t-distributed Stochastic Neighbor Embedding)\nPara maiores detalhes sobre como usar o TSNE no Scikit-learn, veja este link: [http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.manifold.TSNE.html](http:\/\/)","d4859509":"## Importando o word vector previamente treinado\n \n Estou usando o GoogleNews-vectors-negative**300**.bin (de dimensionalidade 300), mas voc\u00ea pode usar qualquer outro.","8c69e04b":"## Representando palavras em espa\u00e7os vetoriais\n\nFerramentas de processamento de linguagem natural (NLP\u200a\u2014\u200aNatural Language Processing) baseadas em machine learning precisam que as palavras de um texto estejam representadas em formato vetorial e num\u00e9rico, considerando que s\u00edmbolos discretos n\u00e3o carregam informa\u00e7\u00e3o \u00fatil quando o interesse \u00e9 usar o computador para interpretar linguagem natural.\n\nAo criar uma representa\u00e7\u00e3o das palavras em um espa\u00e7o vetorial cont\u00ednuo, n\u00f3s podemos codificar diversas regularidades lingu\u00edsticas, tais como os relacionamentos sem\u00e2nticos presentes nos textos, de maneira que as palavras que possuem um significado similar s\u00e3o mapeadas em pontos muito pr\u00f3ximos dentro deste espa\u00e7o de vetor.\n\nEsta proximidade, que pode ser calculada usando aritm\u00e9tica vetorial, reflete a similaridade entre duas ou mais palavras. Normalmente se utiliza a dist\u00e2ncia cosseno ou euclidiana entre dois vetores para calcular esta similaridade. No caso da dist\u00e2ncia cosseno, quanto mais pr\u00f3ximo de 1 for o resultado, mais similares s\u00e3o duas palavras dentro deste espa\u00e7o.\n\n![C\u00e1lculo da dist\u00e2ncia cosseno](https:\/\/cdn-images-1.medium.com\/max\/800\/1*kQiCiH39yc09V6ZaSjC7QQ.png)\n\nUma abordagem que se tornou bastante comum para computar este tipo de representa\u00e7\u00e3o \u00e9 a Word2vec ([Mikolov et al, 2013](https:\/\/arxiv.org\/pdf\/1301.3781.pdf)). Na figura abaixo, h\u00e1 uma plotagem de um word embedding desses, que ilustra a utilidade desta t\u00e9cnica. Veja que, palavras que possuem um significado similar est\u00e3o agrupadas muito pr\u00f3ximas dentro do espa\u00e7o de vetor. Para mais detalhes sobre como utilizar este conceito em um exemplo pr\u00e1tico, [veja este post aqui](https:\/\/luisfred.com.br\/deep-learning\/analise-de-sentimentos-usando-redes-neurais)."}}