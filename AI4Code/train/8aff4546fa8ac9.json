{"cell_type":{"87db8b54":"code","ebc01cc9":"code","8e9aa66f":"code","ca95f9ee":"code","79233677":"code","fc0f0b2c":"code","67909690":"code","a62ba68b":"code","d15b1f27":"code","072605d5":"code","07fad83e":"code","50120563":"code","c549bfa5":"code","b53b274c":"code","65fc829f":"markdown","d6bdfb23":"markdown","27b99969":"markdown","b965baae":"markdown","a80801e3":"markdown","feb4b479":"markdown","49ce4235":"markdown","ec4433a0":"markdown","77b39b99":"markdown","cd0bba0d":"markdown","911ecf88":"markdown"},"source":{"87db8b54":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\ndata=pd.read_csv(r\"..\/input\/amazon-fine-food-reviews\/Reviews.csv\",encoding=\"latin1\")","ebc01cc9":"\ndata=pd.concat([data.Score,data.Text],axis=1)\ndata = data.iloc[:30000]\ndata.dropna(axis=0,inplace=True)\ndata.head(10)","8e9aa66f":"data.Score.value_counts()\n","ca95f9ee":"import re\nfirst_description=data.Text[0]\ndescription=re.sub(\"[^a-zA-Z]\",\" \",first_description) \ndescription=description.lower()","79233677":"print(data.Text[0])\nprint(description)","fc0f0b2c":"import nltk\n#\u263bnltk.download(\"stopwords\")      \n#from nltk.corpus import stopwords\n#description = [ word for word in description if not word in set(stopwords.words(\"english\"))]\ndescription= nltk.word_tokenize(description)","67909690":"import nltk as nlp\n\n\nlemma = nlp.WordNetLemmatizer()\ndescription = [ lemma.lemmatize(word) for word in description] \n\ndescription =\" \".join(description)","a62ba68b":"#%%\nprint(first_description)\nprint(description)","d15b1f27":"description_list = []\nfor description in data.Text:\n    description = re.sub(\"[^a-zA-Z]\",\" \",description)\n    description = description.lower()   \n    description = nltk.word_tokenize(description)\n    lemma = nlp.WordNetLemmatizer()\n    description = [ lemma.lemmatize(word) for word in description]\n    description = \" \".join(description)\n    description_list.append(description)","072605d5":"from sklearn.feature_extraction.text import CountVectorizer \nmax_features = 15000\n\ncount_vectorizer = CountVectorizer(max_features=max_features,stop_words = \"english\")\n\nsparce_matrix = count_vectorizer.fit_transform(description_list).toarray()  # x\n\nwords=count_vectorizer.get_feature_names()\nprint(\"Most used words: \",words[50:100])","07fad83e":"y = data.iloc[:,0].values   \nx = sparce_matrix\n# train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.1, random_state = 42)","50120563":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train)","c549bfa5":"y_pred = nb.predict(x_test)\nprint(\"accuracy: \",nb.score(y_pred.reshape(-1,1),y_test))","b53b274c":"from sklearn.linear_model import LogisticRegression\nlr=LogisticRegression()\nlr.fit(x_train,y_train)\nprint(\"lr accuracy: \",lr.score(x_test,y_test))","65fc829f":"**4**\n                        \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/**Split Words Part**\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n                        \nLets split our values. here we can use split() too but if we use split() it wont split \"isn't\" to \"is not\". And drop insignificant words like is,the,have these are not important for classificaiton we dont need them but i can do that in next steps(in bag of words part).","d6bdfb23":"**9**\n\n   \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/**Bayesian Classification**\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n\nLets try with bayesian classification","27b99969":"**10**\n\n   \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/**Logistic Regression**\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n\n\nwe get bad score rate with bayesian classification.. Lets try again with logistic regression","b965baae":"**5**\n\n\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/**Lemmatization Part**\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n\nLemmazite our values. For example products will be product","a80801e3":"\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/**Introduction**\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n\nIt is a kernel calculate Score acording to user texts. I used first 30000 rows because of memory error..","feb4b479":"**6**\n                 \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/ **Every Datas Part** \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n                 \nNow lets do that for all of our values with a loop.","49ce4235":"**2**\n                             \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/**Data Part**\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n\nSelecting texts and score of texts. And preparing our data","ec4433a0":"**7**\n\n\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/**Bag of Word Part**\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n\nWe are removing irrelavent words(i talked about that in split part) and creating matrix form in order to make them in order. Also after matrix form we will have our sentences with numbers. This means that now our computer can understand human language","77b39b99":"**3**\n                            \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/**Regular Expression Part**\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n                            \nWe will do every data cleaning parts with our first text.When finished data cleaning session we will do for every datas.\nNow we will drop some marks(like !,?,:),:D) from our text values and make all off them lower case","cd0bba0d":"\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/**Conclusion**\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n\n*We get only %70 rate.. I think it's because we could not use all of dataset and we can select only 15000 max_features value because of memory error.It will give better results if we use all of dataset it will give better results.\n\n* If you have any question or any suggestions feel free to write me\n","911ecf88":"**8**\n\n   \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/**Test and Trains Part**\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n\nsplit our tests and trains"}}