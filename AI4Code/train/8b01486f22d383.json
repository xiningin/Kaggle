{"cell_type":{"67f99a5a":"code","e1e0962b":"code","a4d16b73":"code","16943a88":"code","59c4198d":"code","ccb2092b":"code","aade45d3":"code","175cbb5c":"code","870081e9":"code","eefefa63":"code","6cbca91e":"code","be9a1f7f":"code","622cbaa2":"code","ccd24a4d":"code","939ec87f":"code","50ad28df":"code","1ad2c6e7":"code","bde9db93":"code","358654c0":"code","b714070c":"markdown","ae8c7f50":"markdown","0c2dd2b7":"markdown","ea15a975":"markdown","eaa6ca3c":"markdown","40c74634":"markdown","8b90b683":"markdown","e4c3a100":"markdown","f1110b88":"markdown","2aaa0f16":"markdown","67eb1f8a":"markdown","a626bea2":"markdown","31bc15d3":"markdown","4b06460d":"markdown","a64e47fb":"markdown"},"source":{"67f99a5a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","e1e0962b":"import pandas as pd\nimport numpy as np\nimport re\nfrom tqdm import tqdm_notebook\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\nimport pickle\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold, KFold\n\nfrom sklearn.metrics import (confusion_matrix, precision_recall_curve, auc,\n                             roc_curve, recall_score, classification_report, f1_score,\n                             precision_recall_fscore_support, roc_auc_score)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline ","a4d16b73":"import warnings\nwarnings.filterwarnings('ignore')","16943a88":"df = pd.read_csv(\"\/kaggle\/input\/widsdatathon2020\/training_v2.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/widsdatathon2020\/unlabeled.csv\")\nprint(\"Shape of the data is {}\".format(df.shape))\nprint(\"Shape of the test data is {}\".format(test.shape))","59c4198d":"target_column = \"hospital_death\"\n\ndoubtful_columns = [\n    \"cirrhosis\",\n    \"diabetes_mellitus\",\n    \"immunosuppression\",\n    \"hepatic_failure\",\n    \"leukemia\",\n    \"lymphoma\",\n    \"solid_tumor_with_metastasis\",\n    \"gcs_unable_apache\"\n]\n\ncols_with_around_70_percent_zeros = [\n    \"intubated_apache\", \"ventilated_apache\",\n]\n\ncols_with_diff_dist_in_test = [\n    \"hospital_id\", \"icu_id\"\n]\n\nselected_columns = [\n    'd1_spo2_max', 'd1_diasbp_max', 'd1_temp_min', 'h1_sysbp_max', 'gender', 'heart_rate_apache', \n    'weight', 'icu_stay_type', 'd1_mbp_max', 'h1_resprate_max', 'd1_heartrate_min', 'apache_post_operative', 'apache_4a_hospital_death_prob', \n    'd1_mbp_min', 'apache_4a_icu_death_prob', 'd1_sysbp_max', 'icu_type', 'apache_3j_bodysystem', 'h1_sysbp_min', 'h1_resprate_min', 'd1_resprate_max', \n    'h1_mbp_min', 'ethnicity', 'arf_apache', 'resprate_apache', 'map_apache', 'temp_apache', 'icu_admit_source', 'h1_spo2_min', \n    'd1_spo2_min', 'd1_resprate_min', 'h1_mbp_max', 'height', 'age', 'h1_diasbp_max', 'd1_sysbp_min',\n    'pre_icu_los_days', 'd1_heartrate_max', 'd1_diasbp_min', 'apache_2_bodysystem', 'gcs_eyes_apache', 'apache_2_diagnosis', \n    'gcs_motor_apache', 'd1_temp_max', 'h1_spo2_max', 'h1_heartrate_max', 'bmi', 'd1_glucose_min', \n    'h1_heartrate_min', 'gcs_verbal_apache', 'apache_3j_diagnosis', 'd1_glucose_max', 'h1_diasbp_min'\n]\nprint(f'Total number of diff. dist.  columns are {len(cols_with_diff_dist_in_test)}')\nprint(f'Total number of columns with 70% 0s are {len(cols_with_around_70_percent_zeros)}')\nprint(f'Total number of doubtful columns are {len(doubtful_columns)}')\nprint(f'Total number of selected columns are {len(selected_columns)}')","ccb2092b":"len(selected_columns) + len(cols_with_around_70_percent_zeros) + len(cols_with_diff_dist_in_test) + len(doubtful_columns)","aade45d3":"continuous_columns = [\n    'd1_spo2_max', 'd1_diasbp_max', 'd1_temp_min', 'h1_sysbp_max', 'heart_rate_apache', \n    'weight', 'd1_mbp_max', 'h1_resprate_max', 'd1_heartrate_min', 'apache_4a_hospital_death_prob', \n    'd1_mbp_min', 'apache_4a_icu_death_prob', 'd1_sysbp_max', 'h1_sysbp_min', 'h1_resprate_min', 'd1_resprate_max', \n    'h1_mbp_min', 'resprate_apache', 'map_apache', 'temp_apache', 'h1_spo2_min', \n    'd1_spo2_min', 'd1_resprate_min', 'h1_mbp_max', 'height', 'age', 'h1_diasbp_max', 'd1_sysbp_min',\n    'pre_icu_los_days', 'd1_heartrate_max', 'd1_diasbp_min', 'gcs_eyes_apache', \n    'gcs_motor_apache', 'd1_temp_max', 'h1_spo2_max', 'h1_heartrate_max', 'bmi', 'd1_glucose_min', \n    'h1_heartrate_min', 'gcs_verbal_apache', 'd1_glucose_max', 'h1_diasbp_min'\n]\nbinary_columns = [\n    \"apache_post_operative\", \"arf_apache\", \"cirrhosis\", \"diabetes_mellitus\", \"immunosuppression\",\n    \"hepatic_failure\", \"leukemia\", \"lymphoma\", \"solid_tumor_with_metastasis\", \"gcs_unable_apache\",\n    \"intubated_apache\", \"ventilated_apache\"\n\n]\ncategorical_columns = [\n    'icu_stay_type', 'icu_type', \"apache_3j_bodysystem\", 'ethnicity', \"gender\",\n    'icu_admit_source', \"apache_2_bodysystem\", 'apache_2_diagnosis', 'apache_3j_diagnosis', \n]\nhigh_cardinality_columns = [\n    \"hospital_id\", \"icu_id\"\n]\n\nprint(len(continuous_columns) + len(binary_columns) + len(categorical_columns) + len(high_cardinality_columns))","175cbb5c":"columns_to_be_used = list(set(\n    doubtful_columns + cols_with_around_70_percent_zeros + cols_with_diff_dist_in_test + selected_columns))\nprint(f'Total columns to be used initially are {len(columns_to_be_used)}')\n\ncategorical_columns = list(set(categorical_columns))\ncontinuous_columns = list(set(continuous_columns))\nbinary_columns = list(set(binary_columns))\nhigh_cardinality_columns = list(set(high_cardinality_columns))\nprint(f'Total categorical columns to be used initially are {len(categorical_columns)}')\nprint(f'Total continuous columns to be used initially are {len(continuous_columns)}')\nprint(f'Total binary_columns to be used initially are {len(binary_columns)}')\nprint(f'Total high_cardinality_columns to be used initially are {len(high_cardinality_columns)}')","870081e9":"df_train, Y_tr = df[columns_to_be_used], df[target_column]\ndf_test = test[columns_to_be_used]\nprint(df_train.shape, Y_tr.shape, df_test.shape)","eefefa63":"# for categorical label encoding\ncat_labenc_mapping = {\n    col: LabelEncoder()\n    for col in categorical_columns\n}\n\nfor col in tqdm_notebook(categorical_columns):\n    df_train[col] = df_train[col].astype('str')\n    cat_labenc_mapping[col] = cat_labenc_mapping[col].fit(\n        np.unique(df_train[col].unique().tolist() + df_test[col].unique().tolist())\n    )\n    df_train[col] = cat_labenc_mapping[col].transform(df_train[col]) \n        \n\nfor col in tqdm_notebook(categorical_columns):\n    print()\n    df_test[col] = df_test[col].astype('str')\n    df_test[col] = cat_labenc_mapping[col].transform(df_test[col])","6cbca91e":"# imputing\n\n# for categorical\ncat_col2imputer_mapping = {\n    col: SimpleImputer(strategy='most_frequent')\n    for col in categorical_columns\n}\n\n# for continuous\ncont_col2imputer_mapping = {\n    col: SimpleImputer(strategy='median')\n    for col in continuous_columns\n}\n\n# for binary \nbin_col2imputer_mapping = {\n    col: SimpleImputer(strategy='most_frequent')\n    for col in binary_columns\n}\n\n# for high cardinality \nhicard_col2imputer_mapping = {\n    col: SimpleImputer(strategy='median')\n    for col in high_cardinality_columns\n}\n\nall_imp_dicts = [cat_col2imputer_mapping, cont_col2imputer_mapping, bin_col2imputer_mapping,  hicard_col2imputer_mapping]\n\n# fitting imputers\nfor imp_mapping_obj in tqdm_notebook(all_imp_dicts):\n    for col, imp_object in imp_mapping_obj.items():\n        data = df_train[col].values.reshape(-1, 1)\n        imp_object.fit(data)\n\n# transofrming imputed columns\n# fitting imputers\nfor imp_mapping_obj in tqdm_notebook(all_imp_dicts):\n    for col, imp_object in imp_mapping_obj.items():\n        data = df_train[col].values.reshape(-1, 1)\n        data = imp_object.transform(data)\n        df_train[col] = list(data.reshape(-1,))\n\n# inputing on test \nfor imp_mapping_obj in tqdm_notebook(all_imp_dicts):\n    for col, imp_object in imp_mapping_obj.items():\n        data = df_test[col].values.reshape(-1, 1)\n        data = imp_object.transform(data)\n        df_test[col] = list(data.reshape(-1,))","be9a1f7f":"# train_test split\nX_train, X_eval, Y_train, Y_eval = train_test_split(df_train, Y_tr, test_size=0.15, stratify=Y_tr)\nX_train.shape, X_eval.shape, Y_train.shape, Y_eval.shape","622cbaa2":"# tuning tree specific features\ngkf = KFold(n_splits=3, shuffle=True, random_state=42).split(X=X_train, y=Y_train)\n\nfit_params_of_xgb = {\n    \"early_stopping_rounds\":100, \n    \"eval_metric\" : 'auc', \n    \"eval_set\" : [(X_eval,Y_eval)],\n    # 'callbacks': [lgb.reset_parameter(learning_rate=learning_rate_010_decay_power_099)],\n    'verbose': 100,\n}\n\n\n# A parameter grid for XGBoost\nparams = {\n    'booster': [\"gbtree\"],\n    'learning_rate': [0.1],\n    'n_estimators': range(100, 500, 100),\n    'min_child_weight': [1],\n    'gamma': [0],\n    'subsample': [0.8],\n    'colsample_bytree': [0.8],\n    'max_depth': [5],\n    \"scale_pos_weight\": [1]\n}\n\nxgb_estimator = XGBClassifier(\n    objective='binary:logistic',\n    # silent=True,\n)\n\ngsearch = GridSearchCV(\n    estimator=xgb_estimator,\n    param_grid=params,\n    scoring='roc_auc',\n    n_jobs=-1,\n    cv=gkf, verbose=3\n)\n\n# gsearch = RandomizedSearchCV(\n#     estimator=xgb_estimator,\n#     param_distributions=params,\n#     scoring='roc_auc',\n#     n_jobs=-1,\n#     cv=gkf, verbose=3\n# )\n\nxgb_model = gsearch.fit(X=X_train, y=Y_train, **fit_params_of_xgb)\ngsearch.best_params_, gsearch.best_score_","ccd24a4d":"gkf = KFold(n_splits=3, shuffle=True, random_state=42).split(X=X_train, y=Y_train)\n\nfit_params_of_xgb = {\n    \"early_stopping_rounds\":100, \n    \"eval_metric\" : 'auc', \n    \"eval_set\" : [(X_eval,Y_eval)],\n    # 'callbacks': [lgb.reset_parameter(learning_rate=learning_rate_010_decay_power_099)],\n    'verbose': 100,\n}\n\n\n# A parameter grid for XGBoost\nparams = {\n    'booster': [\"gbtree\"],\n    'learning_rate': [0.1],\n    'n_estimators': [300],\n    'gamma': [0],\n    'subsample': [0.8],\n    'colsample_bytree': [0.8],\n    \"scale_pos_weight\": [1],\n    'max_depth':range(2, 7, 2),\n    'min_child_weight':range(2, 8, 2)\n}\n\nxgb_estimator = XGBClassifier(\n    objective='binary:logistic',\n    silent=True,\n)\n\ngsearch = GridSearchCV(\n    estimator=xgb_estimator,\n    param_grid=params,\n    scoring='roc_auc',\n    n_jobs=-1,\n    cv=gkf, verbose=3\n)\n\nxgb_model = gsearch.fit(X=X_train, y=Y_train, **fit_params_of_xgb)\ngsearch.best_params_, gsearch.best_score_","939ec87f":"gkf = KFold(n_splits=3, shuffle=True, random_state=42).split(X=X_train, y=Y_train)\n\nfit_params_of_xgb = {\n    \"early_stopping_rounds\":100, \n    \"eval_metric\" : 'auc', \n    \"eval_set\" : [(X_eval,Y_eval)],\n    # 'callbacks': [lgb.reset_parameter(learning_rate=learning_rate_010_decay_power_099)],\n    'verbose': 100,\n}\n\n\n# A parameter grid for XGBoost\nparams = {\n    'booster': [\"gbtree\"],\n    'learning_rate': [0.1],\n    'n_estimators': [300],\n    'subsample': [0.8],\n    'colsample_bytree': [0.8],\n    \"scale_pos_weight\": [1],\n    'max_depth':[4],\n    'min_child_weight': [6],\n    'gamma': [0, 0.01, 0.01]\n}\n\nxgb_estimator = XGBClassifier(\n    objective='binary:logistic',\n    silent=True,\n)\n\ngsearch = GridSearchCV(\n    estimator=xgb_estimator,\n    param_grid=params,\n    scoring='roc_auc',\n    n_jobs=-1,\n    cv=gkf, verbose=3\n)\n\n# gsearch = RandomizedSearchCV(\n#     estimator=xgb_estimator,\n#     param_distributions=params,\n#     scoring='roc_auc',\n#     n_jobs=-1,\n#     cv=gkf, verbose=3\n# )\n\nxgb_model = gsearch.fit(X=X_train, y=Y_train, **fit_params_of_xgb)\ngsearch.best_params_, gsearch.best_score_","50ad28df":"gkf = KFold(n_splits=3, shuffle=True, random_state=42).split(X=X_train, y=Y_train)\n\nfit_params_of_xgb = {\n    \"early_stopping_rounds\":100, \n    \"eval_metric\" : 'auc', \n    \"eval_set\" : [(X_eval,Y_eval)],\n    # 'callbacks': [lgb.reset_parameter(learning_rate=learning_rate_010_decay_power_099)],\n    'verbose': 100,\n}\n\n\n# A parameter grid for XGBoost\nparams = {\n    'booster': [\"gbtree\"],\n    'learning_rate': [0.1],\n    'n_estimators': [300],\n    \"scale_pos_weight\": [1],\n    'max_depth':[4],\n    'min_child_weight': [6],\n    'gamma': [0],\n    'subsample': [i\/ 10.0 for i in range(2, 5)],\n    'colsample_bytree': [i\/ 10.0 for i in range(8, 10)]\n}\n\nxgb_estimator = XGBClassifier(\n    objective='binary:logistic',\n    silent=True,\n)\n\ngsearch = GridSearchCV(\n    estimator=xgb_estimator,\n    param_grid=params,\n    scoring='roc_auc',\n    n_jobs=-1,\n    cv=gkf, verbose=3\n)\n\n# gsearch = RandomizedSearchCV(\n#     estimator=xgb_estimator,\n#     param_distributions=params,\n#     scoring='roc_auc',\n#     n_jobs=-1,\n#     cv=gkf, verbose=3\n# )\n\nxgb_model = gsearch.fit(X=X_train, y=Y_train, **fit_params_of_xgb)\ngsearch.best_params_, gsearch.best_score_","1ad2c6e7":"gkf = KFold(n_splits=3, shuffle=True, random_state=42).split(X=X_train, y=Y_train)\n\nfit_params_of_xgb = {\n    \"early_stopping_rounds\":100, \n    \"eval_metric\" : 'auc', \n    \"eval_set\" : [(X_eval,Y_eval)],\n    # 'callbacks': [lgb.reset_parameter(learning_rate=learning_rate_010_decay_power_099)],\n    'verbose': 100,\n}\n\n\n# A parameter grid for XGBoost\nparams = {\n    'booster': [\"gbtree\"],\n    'learning_rate': [0.1],\n    'n_estimators': [300],\n    \"scale_pos_weight\": [1],\n    'max_depth':[4],\n    'min_child_weight': [6],\n    'gamma': [0],\n    'subsample': [0.4],\n    'colsample_bytree': [0.8],\n    'reg_alpha': [1, 0.5, 0.1, 0.08]\n}\n\nxgb_estimator = XGBClassifier(\n    objective='binary:logistic',\n    silent=True,\n)\n\ngsearch = GridSearchCV(\n    estimator=xgb_estimator,\n    param_grid=params,\n    scoring='roc_auc',\n    n_jobs=-1,\n    cv=gkf, verbose=3\n)\n\n# gsearch = RandomizedSearchCV(\n#     estimator=xgb_estimator,\n#     param_distributions=params,\n#     scoring='roc_auc',\n#     n_jobs=-1,\n#     cv=gkf, verbose=3\n# )\n\nxgb_model = gsearch.fit(X=X_train, y=Y_train, **fit_params_of_xgb)\ngsearch.best_params_, gsearch.best_score_","bde9db93":"gkf = KFold(n_splits=3, shuffle=True, random_state=42).split(X=X_train, y=Y_train)\n\nfit_params_of_xgb = {\n    \"early_stopping_rounds\":100, \n    \"eval_metric\" : 'auc', \n    \"eval_set\" : [(X_eval,Y_eval)],\n    # 'callbacks': [lgb.reset_parameter(learning_rate=learning_rate_010_decay_power_099)],\n    'verbose': 100,\n}\n\n# A parameter grid for XGBoost\nparams = {\n    'booster': [\"gbtree\"],\n    'learning_rate': [0.01],\n    'n_estimators': range(1000, 6000, 1000),\n    \"scale_pos_weight\": [1],\n    'max_depth':[4],\n    'min_child_weight': [6],\n    'gamma': [0],\n    'subsample': [0.4],\n    'colsample_bytree': [0.8],\n    'reg_alpha': [0.08]\n}\n\nxgb_estimator = XGBClassifier(\n    objective='binary:logistic',\n    silent=True,\n)\n\ngsearch = GridSearchCV(\n    estimator=xgb_estimator,\n    param_grid=params,\n    scoring='roc_auc',\n    n_jobs=-1,\n    cv=gkf, verbose=3\n)\n\nxgb_model = gsearch.fit(X=X_train, y=Y_train, **fit_params_of_xgb)\ngsearch.best_params_, gsearch.best_score_","358654c0":"params_for_fit = {\n    \"eval_metric\":\"auc\", \"eval_set\": [(X_eval, Y_eval)],\n    'early_stopping_rounds':500, 'verbose': 100\n}\nxgb_estimator = XGBClassifier(\n    n_estimators=3000,\n    objective='binary:logistic',\n    booster=\"gbtree\",\n    learning_rate=0.01,\n    scale_pos_weight=1,\n    max_depth=4,\n    min_child_weight=6,\n    gamma=0,\n    subsample=0.4,\n    colsample_bytree=0.8,\n    reg_alpha=0.08,\n#         n_jobs=-1\n)\nxgb_estimator.fit(X=X_train, y=Y_train, **params_for_fit)","b714070c":"## Step 2. Finding min_child_weight and max_depth","ae8c7f50":"Taking subset of the database with the above selected columns","0c2dd2b7":"Dividing Columns into Categories","ea15a975":"Using sklearn's train test split to create validation set","eaa6ca3c":"- Now we will fix this n_estimator=200 and learning_rate=0.1 value and find out others.","40c74634":"## Hyper Parameter Tuning","8b90b683":"## Tuning Gamma","e4c3a100":"## Tuning reg_alpha","f1110b88":"## Training Final Model","2aaa0f16":"## Imputation\n1. Imputing missing values in continuous columns by Median\n    * I am considering high cardinality columns as continuous columns only    \n2. Imputing missing values in categorical columns by Mode","67eb1f8a":"Using Label Encoder to encode text into integer classes","a626bea2":"## Tuning subsample and colsample_bytree","31bc15d3":"* I dropped the columns which had more than 20 % missing values\n* After analysing the data I divided columns into following sections","4b06460d":"## Reducing learning Rate and adding more Trees","a64e47fb":"### Step 1. Finding n_estimators after fixing other parameters\n- max_depth = 5 : This should be between 3-10. I\u2019ve started with 5 but you can choose a different number as well. 4-6 can be good starting points.\n- min_child_weight = 1 : A smaller value is chosen because it is a highly imbalanced class problem and leaf nodes can have smaller size groups.\n- gamma = 0 : A smaller value like 0.1-0.2 can also be chosen for starting. This will anyways be tuned later.\n> - subsample, colsample_bytree = 0.8 : This is a commonly used used start value. Typical values range between 0.5-0.9.\n- scale_pos_weight = 1: Because of high class imbalance.\n"}}