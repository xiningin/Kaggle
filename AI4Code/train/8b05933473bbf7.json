{"cell_type":{"01265239":"code","5325f67a":"code","cddcda4a":"code","6f878c02":"code","a19377f7":"code","e1db46cf":"code","5369c1b5":"code","cf35dc8b":"code","652939c7":"code","f202e4ef":"code","d969ec99":"code","f5dd34f3":"code","b4f03526":"code","e65bba0f":"code","2c2934b6":"code","03f2de9a":"code","74ef9d03":"code","86f81fea":"code","b181a2e4":"code","17a4e14b":"code","ef67c8dd":"code","d1ee4644":"code","4dfc7e29":"code","8a2a9fcb":"code","d43073cd":"code","daba4113":"code","dad380cf":"code","59dafeea":"code","6a738f1c":"code","7e3e51d6":"code","c7749b45":"code","626e782d":"code","d6ab096c":"code","ba74dd54":"code","02ddd0cd":"code","05bca806":"code","601989a3":"code","d6be72de":"code","7611b44d":"code","1170fa54":"markdown","c7fd3394":"markdown","21997c6d":"markdown","c083ea5e":"markdown","8fd0394d":"markdown","9155a25e":"markdown","6f764084":"markdown","342e5743":"markdown","eaf67aec":"markdown","ff0f00da":"markdown","0d99a624":"markdown","f9e827b9":"markdown","53442d6b":"markdown","7db2a140":"markdown","d496b950":"markdown","64ab0636":"markdown","f2f8beed":"markdown","8dc0d5ae":"markdown","f8866a09":"markdown","6c41bcca":"markdown"},"source":{"01265239":"import os\nimport gc\nimport re\nimport string\nimport operator\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom wordcloud import STOPWORDS\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\nimport nltk\nfrom nltk.util import ngrams\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\n\nfrom collections import defaultdict\nfrom collections import Counter\n\nfrom wordcloud import WordCloud\n\nimport warnings\nwarnings.filterwarnings(action='ignore')\n#plt.style.use('fivethirtyeight')\\","5325f67a":"train_df = pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv')\ntest_df = pd.read_csv('..\/input\/tweet-sentiment-extraction\/test.csv')","cddcda4a":"train_df.head()","6f878c02":"test_df.head()","a19377f7":"print('There are {} rows and {} columns in train'.format(train_df.shape[0],train_df.shape[1]))\nprint('There are {} rows and {} columns in test'.format(test_df.shape[0],test_df.shape[1]))","e1db46cf":"train_df.isnull().sum().sort_values(ascending=False)","5369c1b5":"test_df.isnull().sum().sort_values(ascending=False)","cf35dc8b":"train_df = train_df.dropna()\ntest_df = test_df.dropna() # Although there is no missing value in test dataset, perform the dropna","652939c7":"train_df.isnull().sum().sort_values(ascending=False)","f202e4ef":"x=train_df.sentiment.value_counts()\nax = sns.barplot(x.index,x)\nfor i, v in enumerate(x.iteritems()):        \n    ax.text(i ,v[1], \"{:,}\".format(v[1]), ha='center', va ='bottom', fontsize=10, color='black', rotation=0)\nplt.gca().set_ylabel('tweets')","d969ec99":"fig,(ax1,ax2,ax3) = plt.subplots(1,3,figsize=(20,5))\ntrain_len = train_df[train_df['sentiment']=='neutral']['text'].str.len()\nax1.hist(train_len,color='red')\nax1.set_title('Neutral tweets')\n\ntrain_len = train_df[train_df['sentiment']=='positive']['text'].str.len()\nax2.hist(train_len,color='blue')\nax2.set_title('Positive tweets')\n\ntrain_len = train_df[train_df['sentiment']=='negative']['text'].str.len()\nax3.hist(train_len, color='green')\nax3.set_title('Negative tweets')\nfig.suptitle('Characters in tweets')\nplt.show()","f5dd34f3":"fig, ax = plt.subplots(figsize=(10,5))\nsns.kdeplot(train_df[train_df['sentiment']=='neutral']['text'].str.len())\nsns.kdeplot(train_df[train_df['sentiment']=='positive']['text'].str.len())\nsns.kdeplot(train_df[train_df['sentiment']=='negative']['text'].str.len())\nplt.title(\"Distribution of Tweets\")\nax.legend(labels=[\"Neutral\",\"Positive\",\"Negative\"])","b4f03526":"fig,(ax1,ax2,ax3) = plt.subplots(1,3,figsize=(20,5))\ntrain_len = train_df[train_df['sentiment']=='neutral']['text'].str.split().map(lambda x: len(x))\nax1.hist(train_len,color='red')\nax1.set_title('Neutral tweets')\n\ntrain_len = train_df[train_df['sentiment']=='positive']['text'].str.split().map(lambda x: len(x))\nax2.hist(train_len,color='blue')\nax2.set_title('Positive tweets')\n\ntrain_len = train_df[train_df['sentiment']=='negative']['text'].str.split().map(lambda x: len(x))\nax3.hist(train_len, color='green')\nax3.set_title('Negative tweets')\nfig.suptitle('Words in tweets')\nplt.show()","e65bba0f":"fig, ax = plt.subplots(figsize=(10,5))\nsns.kdeplot(train_df[train_df['sentiment']=='neutral']['text'].str.split().map(lambda x: len(x)))\nsns.kdeplot(train_df[train_df['sentiment']=='positive']['text'].str.split().map(lambda x: len(x)))\nsns.kdeplot(train_df[train_df['sentiment']=='negative']['text'].str.split().map(lambda x: len(x)))\nplt.title(\"Distribution of Tweets\")\nax.legend(labels=[\"Neutral\",\"Positive\",\"Negative\"])","2c2934b6":"# word_count\ntrain_df['word_count'] = train_df['text'].apply(lambda x: len(str(x).split()))\ntest_df['word_count'] = test_df['text'].apply(lambda x: len(str(x).split()))\n\n# unique_word_count\ntrain_df['unique_word_count'] = train_df['text'].apply(lambda x: len(set(str(x).split())))\ntest_df['unique_word_count'] = test_df['text'].apply(lambda x: len(set(str(x).split())))\n\n# stop_word_count\ntrain_df['stop_word_count'] = train_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\ntest_df['stop_word_count'] = test_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n\n# url_count\ntrain_df['url_count'] = train_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\ntest_df['url_count'] = test_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n\n# mean_word_length\ntrain_df['mean_word_length'] = train_df['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ntest_df['mean_word_length'] = test_df['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n\n# char_count\ntrain_df['char_count'] = train_df['text'].apply(lambda x: len(str(x)))\ntest_df['char_count'] = test_df['text'].apply(lambda x: len(str(x)))\n\n# punctuation_count\ntrain_df['punctuation_count'] = train_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\ntest_df['punctuation_count'] = test_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n\n# hashtag_count\ntrain_df['hashtag_count'] = train_df['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\ntest_df['hashtag_count'] = test_df['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\n\n# mention_count\ntrain_df['mention_count'] = train_df['text'].apply(lambda x: len([c for c in str(x) if c == '@']))\ntest_df['mention_count'] = test_df['text'].apply(lambda x: len([c for c in str(x) if c == '@']))","03f2de9a":"METAFEATURES = ['word_count', 'unique_word_count', 'stop_word_count', 'url_count', 'mean_word_length',\n                'char_count', 'punctuation_count', 'hashtag_count', 'mention_count']\n\nneu = train_df['sentiment'] == 'neutral'\npos = train_df['sentiment'] == 'positive'\nneg = train_df['sentiment'] == 'negative'\n\nfig, axes = plt.subplots(ncols=2, nrows=len(METAFEATURES), figsize=(30, 50))\n\nfor i, feature in enumerate(METAFEATURES):\n    sns.distplot(train_df.loc[neu][feature], label='Neutral', ax=axes[i][0], color='green')\n    sns.distplot(train_df.loc[pos][feature], label='Positive', ax=axes[i][0], color='blue')\n    sns.distplot(train_df.loc[neg][feature], label='Negative', ax=axes[i][0], color='red')\n\n    sns.distplot(train_df[feature], label='Training', ax=axes[i][1], color='blue')\n    sns.distplot(test_df[feature], label='Test', ax=axes[i][1], color='yellow')\n    \n    for j in range(2):\n        axes[i][j].set_xlabel('')\n        axes[i][j].tick_params(axis='x', labelsize=10)\n        axes[i][j].tick_params(axis='y', labelsize=10)\n        axes[i][j].legend()\n    \n    axes[i][0].set_title(f'{feature} Target Distribution in Training Set', fontsize=13)\n    axes[i][1].set_title(f'{feature} Training & Test Set Distribution', fontsize=13)\n\nplt.show()","74ef9d03":"def generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(' ') if token != '' if token not in STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [' '.join(ngram) for ngram in ngrams]\n\nN = 30","86f81fea":"def ngram_cal(df, n_gram):\n    neutral_ngrams = defaultdict(int)\n    positive_ngrams = defaultdict(int)\n    negative_ngrams = defaultdict(int)\n    \n    for tweet in df[neu]['text']:\n        for word in generate_ngrams(tweet, n_gram=n_gram):\n            neutral_ngrams[word] += 1\n        \n    for tweet in df[pos]['text']:\n        for word in generate_ngrams(tweet, n_gram=n_gram):\n            positive_ngrams[word] += 1\n            \n    for tweet in df[neg]['text']:\n        for word in generate_ngrams(tweet, n_gram=n_gram):\n            negative_ngrams[word] += 1\n    \n    df_neutral_ngrams = pd.DataFrame(sorted(neutral_ngrams.items(), key=lambda x: x[1])[::-1])\n    df_positive_ngrams = pd.DataFrame(sorted(positive_ngrams.items(), key=lambda x: x[1])[::-1])\n    df_negative_ngrams = pd.DataFrame(sorted(negative_ngrams.items(), key=lambda x: x[1])[::-1])\n    \n    return df_neutral_ngrams, df_positive_ngrams, df_negative_ngrams","b181a2e4":"def ngram_plot(df_gram1, df_gram2, df_gram3, U):\n    fig, axes = plt.subplots(ncols=3, figsize=(30, 10))\n    plt.tight_layout()\n\n    sns.barplot(y=df_gram1[0].values[:N], x=df_gram1[1].values[:N], ax=axes[0], color='green')\n    sns.barplot(y=df_gram2[0].values[:N], x=df_gram2[1].values[:N], ax=axes[1], color='blue')\n    sns.barplot(y=df_gram3[0].values[:N], x=df_gram3[1].values[:N], ax=axes[2], color='red')\n\n    for i in range(3):\n        axes[i].spines['right'].set_visible(False)\n        axes[i].set_xlabel('')\n        axes[i].set_ylabel('')\n        axes[i].tick_params(axis='x', labelsize=10)\n        axes[i].tick_params(axis='y', labelsize=10)\n\n    axes[0].set_title(f'Top {N} most common {U}-grams in Neutral Tweets', fontsize=15)\n    axes[1].set_title(f'Top {N} most common {U}-grams in Positive Tweets', fontsize=15)\n    axes[2].set_title(f'Top {N} most common {U}-grams in Negative Tweets', fontsize=15)\n\n    plt.show()","17a4e14b":"a, b, c = ngram_cal(train_df, 1)\nngram_plot(a, b, c, 1)\na, b, c = ngram_cal(train_df, 2)\nngram_plot(a, b, c, 2)\na, b, c = ngram_cal(train_df, 3)\nngram_plot(a, b, c, 3)","ef67c8dd":"neu = test_df['sentiment'] == 'neutral'\npos = test_df['sentiment'] == 'positive'\nneg = test_df['sentiment'] == 'negative'\n\na, b, c = ngram_cal(test_df, 1)\nngram_plot(a, b, c, 1)\na, b, c = ngram_cal(test_df, 2)\nngram_plot(a, b, c, 2)\na, b, c = ngram_cal(test_df, 3)\nngram_plot(a, b, c, 3)","d1ee4644":"train_df.head()","4dfc7e29":"# Seperate Label\nlabel = train_df['selected_text'].values\ntrain_df = train_df.drop(['selected_text'], axis=1)\n\n# Drop useless data from ngrams\n#METAFEATURES = ['word_count', 'unique_word_count', 'stop_word_count', 'url_count', 'mean_word_length',\n#                'char_count', 'punctuation_count', 'hashtag_count', 'mention_count']\ntrain_df = train_df.drop(METAFEATURES, axis=1)\ntest_df = test_df.drop(METAFEATURES, axis=1)\n\n# Drop the irrevelant parameter\ntrain_df = train_df.drop(['textID'], axis=1)\ntest_df = test_df.drop(['textID'], axis=1)","8a2a9fcb":"train_df.head()","d43073cd":"# Combine dataset\ndf = pd.concat([train_df, test_df], axis=0)\nprint('There are {} rows and {} columns in train'.format(train_df.shape[0],train_df.shape[1]))\nprint('There are {} rows and {} columns in test'.format(test_df.shape[0],test_df.shape[1]))\nprint('There are {} rows and {} columns in total'.format(df.shape[0],df.shape[1]))\n\n# Save the size of train and test dataset\ntrain_size = train_df.shape[0]\ntest_size = test_df.shape[0]\n\ndf.head(5)","daba4113":"def remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ndef remove_punct(text):\n    table = str.maketrans('','',string.punctuation)\n    return text.translate(table)","dad380cf":"df['text'] = df['text'].apply(lambda x : remove_URL(x))\ndf['text'] = df['text'].apply(lambda x : remove_html(x))\ndf['text'] = df['text'].apply(lambda x : remove_emoji(x))\ndf['text'] = df['text'].apply(lambda x : remove_punct(x))","59dafeea":"def remove_tags(text):\n    tag_pattern = re.compile(r'[@|#][^\\s]+')\n    return tag_pattern.sub(r'',text)\n#    return text + ' ' + ' '.join(tags) + ' '+ ' '.join(tags) + ' ' + ' '.join(tags)","6a738f1c":"df['text'] = df['text'].apply(lambda x : remove_tags(x))","7e3e51d6":"nltk.download('punkt')\nnltk.download('stopwords')\nstemmer  = SnowballStemmer('english')\nstopword = stopwords.words('english')\n\ndef Remove_StopAndStem(text):\n    string_list = text.split()\n    return ' '.join([stemmer.stem(i) for i in string_list if i not in stopword])","c7749b45":"df['text'] = df['text'].apply(Remove_StopAndStem)","626e782d":"df.head()","d6ab096c":"dict_of_words = {}\nfor row in  df.itertuples():\n    for i in row[1].split():\n        try:\n            dict_of_words[i] += 1\n        except:\n            dict_of_words[i] = 1\n\n#Initializing  WordCloud\nwordcloud = WordCloud(background_color = 'black', width=1000, height=500).generate_from_frequencies(dict_of_words)\nfig = plt.figure(figsize=(10,5))\nplt.imshow(wordcloud)\nplt.tight_layout(pad=1)\nplt.show()","ba74dd54":"re_train = df[:train_size]\nre_test = df[train_size:]\n\nprint('There are {} rows and {} columns in train'.format(re_train.shape[0],re_train.shape[1]))\nprint('There are {} rows and {} columns in test'.format(re_test.shape[0],re_test.shape[1]))\nprint('Original train dataset : {} \\nOriginal test dataset: {}'.format(train_size, test_size))","02ddd0cd":"neu = re_train['sentiment'] == 'neutral'\npos = re_train['sentiment'] == 'positive'\nneg = re_train['sentiment'] == 'negative'\n\na, b, c = ngram_cal(re_train, 1)\nngram_plot(a, b, c, 1)\na, b, c = ngram_cal(re_train, 2)\nngram_plot(a, b, c, 2)\na, b, c = ngram_cal(re_train, 3)\nngram_plot(a, b, c, 3)","05bca806":"neu = re_test['sentiment'] == 'neutral'\npos = re_test['sentiment'] == 'positive'\nneg = re_test['sentiment'] == 'negative'\n\na, b, c = ngram_cal(re_test, 1)\nngram_plot(a, b, c, 1)\na, b, c = ngram_cal(re_test, 2)\nngram_plot(a, b, c, 2)\na, b, c = ngram_cal(re_test, 3)\nngram_plot(a, b, c, 3)","601989a3":"X_train, X_valid, y_train, y_valid = train_test_split(re_train, label, test_size=0.2, random_state=42)","d6be72de":"vectorizer = TfidfVectorizer(min_df = 0.0005, \n                             max_features = 100000, \n                             tokenizer = lambda x: x.split(),\n                             ngram_range = (1,4))\n\n\nX_train = vectorizer.fit_transform(X_train['text'])\nX_valid = vectorizer.transform(X_valid['text'])","7611b44d":"print(\"Training Points: \", len(X_train.toarray()),\"| Training Features:\" , len(X_train.toarray()[0]))\nprint(\"Testing Points: \", len(X_valid.toarray()),\"| Testing Features:\" , len(X_valid.toarray()[0]))\nprint()\nprint(\"Training Points: \", len(y_train))\nprint(\"Testing Points: \", len(y_valid))","1170fa54":"* #### Number of words","c7fd3394":"N-grams in Train dataset","21997c6d":"* Split Train and Validation dataset","c083ea5e":"* Remove Url \/ Html \/ Emoji \/ Punct","8fd0394d":"* Remove Stopwords","9155a25e":"# 3. Data cleaning","6f764084":"* Check Missing value","342e5743":"* Test dataset","eaf67aec":"# 4. Cleaning result","ff0f00da":"* Word Cloud","0d99a624":"* Remove Tags","f9e827b9":"* Number of Labels","53442d6b":"* Number of characters","7db2a140":"* Combine Dataset <br>\nBefore data cleaning, to perform at once, train dataset and test dataset are combined.<br>\nThe label in train dataset is seperated to <code>label<\/code>.","d496b950":"* Meta data","64ab0636":"* Train dataset","f2f8beed":"* N-gram","8dc0d5ae":"N-grams in Test dataset","f8866a09":"# 1. Load data","6c41bcca":"# 2. Meta data analysis\nThe dataset is needed to be cleaned. To check the effect of cleaning, we perform the data analysis before.<br>\n[Reference: NLP with Disaster Tweets](https:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-cleaning-and-bert)"}}