{"cell_type":{"49349d70":"code","e41ffc73":"code","4608c9c3":"code","e559894d":"code","2c65d497":"code","b6c84b3e":"code","77f54905":"code","d0c94aed":"code","60b66f9e":"code","e5cb8f35":"code","d131ef26":"code","2372e22d":"code","62cab53c":"markdown","cc2c6d5f":"markdown"},"source":{"49349d70":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e41ffc73":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom transformers import TFAutoModel,AutoTokenizer\nimport tensorflow as tf","4608c9c3":"checkpoint = 'joeddav\/xlm-roberta-large-xnli'\n\ntokenizer=AutoTokenizer.from_pretrained(checkpoint)","e559894d":"train=pd.read_csv('..\/input\/contradictory-my-dear-watson\/train.csv')\ntest=pd.read_csv('..\/input\/contradictory-my-dear-watson\/test.csv')\nprint(train.shape)\nprint(test.shape)","2c65d497":"MAX_LEN=100","b6c84b3e":"train_encoded = tokenizer.batch_encode_plus(train[['premise','hypothesis']].values.tolist(),padding='max_length',max_length=MAX_LEN,truncation=True,return_attention_mask=True)\ntest_encoded = tokenizer.batch_encode_plus(test[['premise','hypothesis']].values.tolist(),padding='max_length',max_length=MAX_LEN,truncation=True,return_attention_mask=True)\n\ntrain_ids=tf.convert_to_tensor(train_encoded['input_ids'],dtype=tf.int32)\ntrain_mask=tf.convert_to_tensor(train_encoded['attention_mask'],dtype=tf.int32)\ntrain_input={'input_ids':train_ids,'input_mask':train_mask}\n\ntest_ids=tf.convert_to_tensor(test_encoded['input_ids'],dtype=tf.int32)\ntest_mask=tf.convert_to_tensor(test_encoded['attention_mask'],dtype=tf.int32)\ntest_input={'input_ids':test_ids,'input_mask':test_mask}","77f54905":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\n\nprint('Number of replicas:', strategy.num_replicas_in_sync)\n","d0c94aed":"EPOCHS=20\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nVAL_SPLIT = 0.2","60b66f9e":"with strategy.scope():\n    input_ids = tf.keras.Input(shape = (MAX_LEN,), dtype = tf.int32,name='input_ids') \n    input_mask=tf.keras.Input(shape=(MAX_LEN,),dtype=tf.int32,name='input_mask')   \n    \n    pretrained_model = TFAutoModel.from_pretrained(checkpoint)\n    logits = pretrained_model([input_ids,input_mask])[0]\n\n    output = tf.keras.layers.GlobalAveragePooling1D()(logits)\n    output = tf.keras.layers.Dense(3, activation = 'softmax')(output)\n    model = tf.keras.Model(inputs = [input_ids,input_mask], outputs = output)\n\n    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-5), \n                  loss = 'sparse_categorical_crossentropy', \n                  metrics = ['accuracy']) \n    model.summary()","e5cb8f35":"early_stop = tf.keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True)\nmodel.fit(train_input, train.label, validation_split = VAL_SPLIT, epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[early_stop], verbose=1)","d131ef26":"predictions=[np.argmax(i) for i in model.predict(test_input)]\n\nsubmission = test.id.copy().to_frame()\nsubmission['prediction'] = predictions\nsubmission.head()\nid\t","2372e22d":"submission.to_csv(\"submission.csv\", index = False)","62cab53c":"#### Did it work?\nKaggle provides all users TPU Quota at no cost, which we can use to explore this competition. The most common approaches to NLI problems include using embeddings and transformers like BERT. In this competition, Kaggle provids a starter notebook to try our hand at this problem using the power of Tensor Processing Units (TPUs). Yes, it works because of the TPUs, TPUs are powerful hardware accelerators specialized in deep learning tasks, including Natural Language Processing. \n\n#### What did you not understand about this process?\nWell, everything provides in the competition data page. I've no problem while working on it. If you guys don't understand the thing that I'll do in this notebook then please comment on this notebook. \n\n#### What else do you think you can try as part of this approach?\nWell, everything is in its place. If I feel like i need to add something to it then i'll definitely do this.\n\n\n\n\n\n### PLEASE UPVOTE if you like this notebook. It will keep me motivated to update my notebook. :)","cc2c6d5f":"#### What are you trying to do in this notebook?\nDetecting contradiction and entailment in multilingual text using TPUs.\nTo create an NLI(Natural Language Inferencing) model that assigns labels of 0, 1, or 2 (corresponding to entailment, neutral, and contradiction) to pairs of premises and hypotheses. To make things more interesting, the train and test set.\n\n#### Why are you trying it?\nIn this Competition, I wanna classify pairs of sentences (consisting of a premise and a hypothesis) into three categories - entailment, contradiction, or neutral.\n\nThere are three hypotheses :-\n\nHypothesis 1:\nJust by the look on his face when he came through the door I just knew that he was let down.\n\n**We know that this is true based on the information in the premise. So, this pair is related by *entailment*.**\n\nHypothesis 2:\nHe was trying not to make us feel guilty but we knew we had caused him trouble.\n\n**This very well might be true, but we can\u2019t conclude this based on the information in the premise. So, this relationship is *neutral*.**\n\nHypothesis 3:\nHe was so excited and bursting with joy that he practically knocked the door off it's frame.\n\n**We know this isn\u2019t true, because it is the complete opposite of what the premise says. So, this pair is related by *contradiction*.**\n\n### Special thanks to Tensorflow Datasets (TFDS) for providing this and many other useful datasets! \n"}}