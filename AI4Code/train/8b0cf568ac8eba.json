{"cell_type":{"05bad6bd":"code","498dcf38":"code","f1359902":"code","bd7f2224":"code","86a40e41":"code","f013df6a":"code","4cea568f":"code","acfd0edd":"code","35eb17ca":"code","1becf2f7":"code","c3e884e7":"code","be6f711d":"code","604c94db":"code","c1e456ca":"code","1e47e577":"code","b4c9f36f":"code","f88f8f7f":"code","ebc15492":"code","cfab1541":"code","8e0939c4":"code","55df05aa":"code","4e7f103c":"code","113815f8":"code","7b18b1ec":"code","3be1987e":"code","8fa87280":"code","46a4cb6d":"code","8b1e6fc0":"code","66339733":"code","b2e05827":"code","874f604f":"code","b8a3e063":"code","489c6623":"code","7f57a25f":"code","74294375":"code","4632b348":"code","94da6279":"markdown","1b227e0e":"markdown","0c3856ec":"markdown","04cca102":"markdown","7b88f2f7":"markdown","a088249f":"markdown","8333c984":"markdown","919a88c2":"markdown","5bdad85d":"markdown","53dc5861":"markdown","292e2ad2":"markdown"},"source":{"05bad6bd":"import pandas as pd\nimport numpy as np\nimport os\nimport re\n\nimport json\nfrom pathlib import Path\nfrom collections import OrderedDict\n\nfrom typing import List, Dict, Set\nfrom thinc.api import Model\n\nimport spacy\nfrom spacy.tokens import Doc\nfrom spacy.matcher import Matcher\nfrom spacy import displacy\n\nfrom nltk import tokenize\nfrom tqdm import tqdm\nimport gc","498dcf38":"TRAIN_PATH = '..\/input\/coleridgeinitiative-show-us-the-data\/train\/'\nTEST_PATH = '..\/input\/coleridgeinitiative-show-us-the-data\/test\/'","f1359902":"df = pd.read_csv('..\/input\/coleridgeinitiative-show-us-the-data\/train.csv')\ndf.info()","bd7f2224":"df.head()","86a40e41":"df.select_dtypes(include=[object]).describe().T","f013df6a":"# Get all labels for each Id \ndef get_labels(df: pd.DataFrame) -> Dict[str, Set[str]]:\n    ids = df.Id.unique().tolist()\n    output = {}\n    for i in ids:\n        labels = df.loc[df.Id == i]['cleaned_label'].unique().tolist()\n        output[i] = labels\n\n    return output","4cea568f":"%%time\nid_labels = get_labels(df)","acfd0edd":"len(id_labels)","35eb17ca":"id_labels['d0fa7568-7d8e-4db9-870f-f9c6f668c17b']","1becf2f7":"BasePattern = List[Dict[str, str]]","c3e884e7":"# cleaned label to label abbreviation \ndef get_abbreviation(words: List[str]) -> str:\n    return ''.join([i[0].upper() for i in words])","be6f711d":"def fetch_pattern(label: str) -> Dict[str, BasePattern]:\n    words = label.split()\n    pattern = []\n    for word in words:\n        lower = {}\n        lower['LOWER'] = word.lower()\n        pattern.append(lower)\n    return {label: pattern}","604c94db":"fetch_pattern('Hello World')","c1e456ca":"def create_patterns(labels: List[str]) -> List[Dict[str, BasePattern]]:\n    patterns = []\n    for label in labels:\n        pattern = fetch_pattern(label)\n        patterns.append(pattern)        \n    return patterns","1e47e577":"labels = df.cleaned_label.unique().tolist()\nlen(labels)","b4c9f36f":"%%time\nPATTERNS = create_patterns(labels)","f88f8f7f":"PATTERNS[:5]","ebc15492":"model_name = \"en_core_web_sm\"\nnlp = spacy.load(model_name, disable=[\"tagger\", \"parser\", \"ner\"])","cfab1541":"# Add created patterns to Matcher\ndef add_patterns(model: Model) -> Matcher:\n    matcher = Matcher(model.vocab, validate=True)\n    for name_pattern in PATTERNS:\n        for name, pattern in name_pattern.items():\n            matcher.add(name, [pattern], on_match=None)\n    return matcher","8e0939c4":"matcher = add_patterns(nlp)","55df05aa":"# https:\/\/www.kaggle.com\/c\/coleridgeinitiative-show-us-the-data\/discussion\/230091\n# https:\/\/www.kaggle.com\/c\/coleridgeinitiative-show-us-the-data\/overview\/evaluation\ndef compute_fbeta(y_true: List[List[str]],\n                  y_pred: List[List[str]],\n                  beta: float = 0.5) -> float:\n    \"\"\"Compute the Jaccard-based micro FBeta score.\n    \"\"\"\n\n    def _jaccard_similarity(str1: str, str2: str) -> float:\n        a = set(str1.split()) \n        b = set(str2.split())\n        c = a.intersection(b)\n        return float(len(c)) \/ (len(a) + len(b) - len(c))\n\n    tp = 0  # true positive\n    fp = 0  # false positive\n    fn = 0  # false negative\n    for ground_truth_list, predicted_string_list in zip(y_true, y_pred):\n        predicted_string_list_sorted = sorted(predicted_string_list)\n        for ground_truth in sorted(ground_truth_list):            \n            if len(predicted_string_list_sorted) == 0:\n                fn += 1\n            else:\n                similarity_scores = [\n                    _jaccard_similarity(ground_truth, predicted_string)\n                    for predicted_string in predicted_string_list_sorted\n                ]\n                matched_idx = np.argmax(similarity_scores)\n                if similarity_scores[matched_idx] >= 0.5:\n                    predicted_string_list_sorted.pop(matched_idx)\n                    tp += 1\n                else:\n                    fn += 1\n        fp += len(predicted_string_list_sorted)\n\n    tp *= (1 + beta ** 2)\n    fn *= beta ** 2\n    fbeta_score = tp \/ (tp + fp + fn)\n    return fbeta_score","4e7f103c":"# Check metric sanity\ncompute_fbeta(\n    [['national education longitudinal study','slosh model']],\n    [['education', 'slosh model']]\n)","113815f8":"def read_json(path: str, pub_id: str) -> None:\n    path = Path(path + pub_id + '.json')\n    with open(path, 'rt') as json_file:\n        return json.load(json_file, object_hook=OrderedDict)","7b18b1ec":"def clean_text(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower())","3be1987e":"def find_patterns(docs: Doc) -> List[str]:\n    labels = set()\n    for doc in docs:\n        for match_id, start, end in matcher(doc):\n            label = doc.vocab.strings[match_id]\n            labels.add(label)\n    if not labels:\n        labels.add('')\n    return list(labels)","8fa87280":"def get_preds(ids: List[str], path: str, model: Model) -> Dict[str, List[str]]:\n    \n    output = {}\n    for pub_id in tqdm(ids):\n        json_file = read_json(path, pub_id)\n        \n        # JSON data to the list of lists [sentences of each section of the publication]\n        sentences = [tokenize.sent_tokenize(passage['text']) for passage in json_file]\n        \n        # Flattening the list to create a list on all sentences for each publication Id\n        flatten = lambda lst: [clean_text(item) for sublist in lst for item in sublist]\n        sentences = flatten(sentences)\n        \n        # Get all Doc objects to operate on sentence level\n        docs = []\n        for sent in sentences:\n            doc = model(sent)\n            docs.append(doc) \n            \n        # Rule-based matching\n        output[pub_id] = find_patterns(docs)\n    gc.collect()\n        \n    return output","46a4cb6d":"ids = df.Id.unique().tolist()\nlen(ids)","8b1e6fc0":"# Check the output format of the model\nget_preds(ids[:2], TRAIN_PATH, nlp)","66339733":"# Predictions from pattern matching model\npreds = get_preds(ids[:1000], TRAIN_PATH, nlp)","b2e05827":"def filter_ids(id_labels: Dict[str, List[str]], ids: List[str]) -> Dict[str, List[str]]:\n    return {pub_id: id_labels[pub_id] for pub_id in ids}","874f604f":"# Check the filtering output\nfilter_ids(id_labels, ids[:2])","b8a3e063":"# References for the evaluation\nrefs = filter_ids(id_labels, ids[:1000])\nlen(preds), len(refs)","489c6623":"ref_list = [v for k,v in refs.items()]\npred_list = [v for k,v in preds.items()]\nfbeta = compute_fbeta(ref_list, pred_list)\nprint(f'FBeta Score for sample of {len(preds)} publications: {fbeta:.4f}')","7f57a25f":"test = pd.read_csv('..\/input\/coleridgeinitiative-show-us-the-data\/sample_submission.csv')\ntest_ids = test.Id.to_list()\ntest_preds = get_preds(test_ids, TEST_PATH, nlp)","74294375":"data = {'Id': [], 'PredictionString': []}\nfor i, labels in test_preds.items():\n    data['Id'].append(i)\n    labels = '|'.join(labels)\n    data['PredictionString'].append(labels)\n    \nsub = pd.DataFrame.from_dict(data)\nsub.to_csv('submission.csv', index=False)","4632b348":"pd.read_csv('.\/submission.csv')","94da6279":"```python\npattern_sample = [\n    {'LOWER': 'national'},\n    {'LOWER': 'education'},\n    {'LOWER': 'longitudinal'},\n    {'LOWER': 'study'}\n]\n```","1b227e0e":"# Notes\n\n## Research\n\n- [ ] Study the inconsistencies to expand the pattern types.\n- [ ] The publications in the training dataset are poorly labeled, the test set seems to have many more labels than there are in the train set. Find external public data to extract more labels to better generalize to datasets so improve the rule-based and statistical models.\n- [ ] Review possible ML approaches for automatic data labeling.\n\n## Code\n\n- [x] Basic Patterns\n- [x] Spacy Matcher\n- [x] Improve Metric\n- [x] Review annotations\n- [ ] Add docs","0c3856ec":"# About\n\nBrif data exploration. Rule-based Spacy model (pattern matching) as a baseline.\n","04cca102":"# Patterns","7b88f2f7":"# Id & Unique Labels","a088249f":"# Test","8333c984":"# Metric - Micro FBeta Score","919a88c2":"# Evaluation ","5bdad85d":"# Matcher","53dc5861":"# Data\n\nPublications are provided in JSON format, broken up into sections with section titles. The goal in [this competition](https:\/\/www.kaggle.com\/c\/coleridgeinitiative-show-us-the-data\/overview) is not just to match known dataset strings but to generalize to datasets that have never been seen before using NLP and statistical techniques. A percentage of the public test set publications are drawn from the training set - not all datasets have been identified in train, so these unidentified datasets have been used as a portion of the public test labels. These should serve as guides for the difficult task of labeling the private test set. [Details](https:\/\/www.kaggle.com\/c\/coleridgeinitiative-show-us-the-data\/data)","292e2ad2":"# Predictions"}}