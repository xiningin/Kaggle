{"cell_type":{"30cd7376":"code","9487ba46":"code","a6736657":"code","a44b6eec":"code","f7bb8edd":"code","41773009":"code","16d4a722":"code","9a466f1a":"code","f9c8e231":"code","0db3824c":"code","c8ee7762":"code","804d1f00":"code","bf26ae75":"code","b1dda9a4":"code","81418e22":"code","a17aceb4":"code","89151e2a":"code","5233cda9":"code","aed2c08b":"code","424bb9a1":"code","3d23ed33":"code","5de86d4e":"code","0945fb3d":"code","8820aa84":"code","67778c93":"code","a808bac9":"code","91d16ee2":"code","356ff14a":"code","32e51708":"code","ae2779e9":"code","bf50dcb4":"code","316745e6":"code","9a9075dc":"markdown","78ddf8ba":"markdown","07062553":"markdown","cc64093c":"markdown","be538b91":"markdown","eb414486":"markdown","39b968c3":"markdown","a2103702":"markdown","eb87a124":"markdown","7fab3282":"markdown","11d30dff":"markdown","777003ce":"markdown","4e1062f8":"markdown","b9989415":"markdown","d63f4bcf":"markdown","cee98393":"markdown","916bbea6":"markdown","833639de":"markdown","c9de6e3d":"markdown","31d0a997":"markdown","12b8da42":"markdown"},"source":{"30cd7376":"# list input folder\n! ls \/kaggle\/input\/siim-covid19-detection -l","9487ba46":"# ! pip install -qU \"numpy>=1.20\" --no-binary numpy --no-build-isolation\n! pip install -q python-gdcm\n# ! pip install -q pylibjpeg-libjpeg pylibjpeg-openjpeg\n# ! pip install -qU \"pylibjpeg==1.2\" --no-binary :all:\n! pip install -qU pydicom opencv-python-headless pycocotools # \"torchvision==0.8\" \"torch==1.7\"\n# ! pip install -q https:\/\/github.com\/PyTorchLightning\/lightning-flash\/archive\/master.zip\n! pip list | grep torch\n! pip list | grep lightning\n! pip list | grep dicom\n! pip list | grep jpeg\n! nvidia-smi\n\n%load_ext autoreload\n%autoreload 2\n\nimport pydicom  # , pylibjpeg, openjpeg, libjpeg\nprint(getattr(pydicom.config, \"gdcm_handler\").is_available())\nprint(getattr(pydicom.config, \"pylibjpeg_handler\").is_available())","a6736657":"%matplotlib inline\n\nimport os\nimport glob\nimport json\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nBASE_PATH = '\/kaggle\/input\/siim-covid19-detection'\nLABELS = (\"Negative for Pneumonia\", \"Typical Appearance\", \"Indeterminate Appearance\", \"Atypical Appearance\")\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)","a44b6eec":"path_csv_image = os.path.join(BASE_PATH, 'train_image_level.csv')\ntrain_images = pd.read_csv(path_csv_image, index_col=\"id\").sort_values(\"StudyInstanceUID\")\ntrain_images[\"name\"] = [v.split('_')[0] for v in train_images.index]\ntrain_images[\"boxes\"] = train_images[\"boxes\"].apply(lambda v: eval(v) if not isinstance(v, float) else None)\ntrain_images[\"#boxes\"] = train_images[\"boxes\"].apply(lambda v: len(v) if v else 0)\n\nimgs_paths = [glob.glob(os.path.join(BASE_PATH, 'train', row['StudyInstanceUID'], '*', f\"{row['name']}.*\")) for _, row in train_images.iterrows()]\nprint(f\"max images shall be one and is: {max([len(p) for p in imgs_paths])}\")\ntrain_images[\"path\"] = [os.path.sep.join(p[0].split(os.path.sep)[-4:]) for p in imgs_paths]\n\ndisplay(train_images.head())\nprint(len(train_images))","f7bb8edd":"train_images[\"label_\"] = train_images[\"label\"].apply(lambda lb: lb.split()[::6])\ntrain_images[\"label__\"] = train_images[\"label_\"].apply(lambda lb: set(lb))\nax = train_images[\"label__\"].value_counts().plot.pie(ylabel=\"\", autopct=\"%.1f%%\")","41773009":"labels_none = [lb for lb in train_images[\"label\"] if lb.startswith(\"none\")]\nprint(set(labels_none))","16d4a722":"labels_1 = [tuple(lb.split()[1::6]) for lb in train_images[\"label\"]]\nprint(set(labels_1))","9a466f1a":"path_csv_study = os.path.join(BASE_PATH, 'train_study_level.csv')\ntrain_study = pd.read_csv(path_csv_study, index_col=\"id\").sort_values(\"id\")\ntrain_study[\"id_\"] = [v.split('_')[0] for v in train_study.index]\ntrain_study[\"class\"] = [np.argmax(row.values) for _, row in train_study[list(LABELS)].iterrows()]\ndisplay(train_study.head())\nprint(len(train_study))","f9c8e231":"counts = train_images[\"StudyInstanceUID\"].value_counts()\ndisplay(dict(enumerate(np.bincount(counts))))\nax = counts.hist(bins=2*max(counts))","0db3824c":"train_study_ids = set(train_study[\"id_\"])\nmiss = [id_ for id_ in train_images[\"StudyInstanceUID\"] if id_ not in train_study_ids]\nprint(f\"Missed: {len(miss)}\")\nprint(f\"{len(train_study)} == {train_study[list(LABELS)].sum().sum()}\")\n\nax = train_study[list(LABELS)].sum().plot.pie(ylabel=\"\", autopct=\"%.1f%%\")","c8ee7762":"train_images = pd.merge(train_images, train_study, how=\"left\", left_on=\"StudyInstanceUID\", right_on=\"id_\")\ndisplay(train_images.head())","804d1f00":"fig, axarr = plt.subplots(ncols=2, figsize=(7, 3))\n\ntrain_images_none = train_images[train_images[\"label\"].str.startswith(\"none\")]\naxarr[0].set_title(\"classes with labels None\")\nax = train_images_none[\"class\"].value_counts().plot.pie(ax=axarr[0], ylabel=\"\", autopct=\"%.1f%%\")\n\ntrain_images_other = train_images[~ train_images[\"label\"].str.startswith(\"none\")]\naxarr[1].set_title(\"classes with any labels\")\nax = train_images_other[\"class\"].value_counts().plot.pie(ax=axarr[1], ylabel=\"\", autopct=\"%.1f%%\")","bf26ae75":"import pydicom\nfrom pydicom.pixel_data_handlers import apply_voi_lut\n\nidx_ = 0\ndicom_path = os.path.join(BASE_PATH, train_images[\"path\"][idx_])\ndicom = pydicom.dcmread(dicom_path)\nprint(vars(dicom).keys())\nprint(dicom)\nimg = apply_voi_lut(dicom.pixel_array, dicom)\n\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import patches\n\nfig, ax = plt.subplots()\nax_im = ax.imshow(img, cmap=\"gray\")\nfor bbox in train_images[\"boxes\"][idx_]:\n    # Create a Rectangle patch\n    rect = patches.Rectangle((bbox['x'], bbox['y']), bbox['width'], bbox['height'], linewidth=1, edgecolor='r', facecolor='none')\n    # Add the patch to the Axes\n    ax.add_patch(rect)\n\n_= plt.colorbar(ax_im)","b1dda9a4":"import cv2\nfrom copy import deepcopy\n\ndef load_image(path_file: str, meta: dict, spacing: float = 1.0, percentile: bool = True):\n    dicom = pydicom.dcmread(path_file)\n    try:\n        img = apply_voi_lut(dicom.pixel_array, dicom)\n    except RuntimeError as err:\n        print(err)\n        return None, {}\n    if dicom.PhotometricInterpretation == 'MONOCHROME1':\n        img = img.max() - img\n    p_low = np.percentile(img, 1) if percentile else img.min()\n    p_high = np.percentile(img, 99) if percentile else img.max()\n    # normalize\n    img = (img.astype(float) - p_low) \/ (p_high - p_low)\n    meta.update(dict(\n        boxes=deepcopy(meta.get(\"boxes\")) or [],\n        body=dicom.BodyPartExamined,\n        interpret=dicom.PhotometricInterpretation,\n        spacing=dicom.ImagerPixelSpacing,\n    ))\n    if spacing:\n        factor = np.array(meta['spacing']) \/ spacing\n        dims = tuple((np.array(img.shape[::-1]) * factor).astype(int))\n        img = cv2.resize(img, dsize=dims, interpolation=cv2.INTER_LINEAR)\n        for bbox in meta[\"boxes\"]:\n            bbox['x'] *= factor[0]\n            bbox['y'] *= factor[1]\n            bbox['width'] *= factor[0]\n            bbox['height'] *= factor[1]\n        meta.update(dict(spacing=(spacing, spacing)))\n    return img, meta","81418e22":"NB_SAMPLES = 5\nNB_CLASSES = max(train_images[\"class\"]) + 1\nrect_property = dict(linewidth=1, edgecolor='r', facecolor='none')\n\nfig, axarr = plt.subplots(nrows=NB_CLASSES, ncols=NB_SAMPLES, figsize=(NB_SAMPLES * 4, NB_CLASSES * 5))\nfor cls, df in train_images.groupby(\"class\"):\n    for i, (_, row) in enumerate(df[:NB_SAMPLES].iterrows()):\n        img, meta = load_image(os.path.join(BASE_PATH, row[\"path\"]), dict(row), spacing=1.)\n        axarr[cls, i].set_title(f\"label: {cls}; body: {meta['body']}\\n interpret: {meta['interpret']}\\n spacing: {meta['spacing']}\")\n        if img is None:\n            continue\n        _ = axarr[cls, i].imshow(img.astype(float) \/ img.max(), cmap=\"gray\")\n        if not meta[\"boxes\"]:\n            continue\n        for bbox in meta[\"boxes\"]:\n            rect = patches.Rectangle((bbox['x'], bbox['y']), bbox['width'], bbox['height'], **rect_property)\n            axarr[cls, i].add_patch(rect)","a17aceb4":"counts, cases = [], []\nfor cls, df in train_images.groupby(\"class\"):\n    counts.append(dict(df[\"#boxes\"].value_counts()))\n    cases.append(LABELS[cls])\n\ndf = pd.DataFrame(counts, index=cases)\ndisplay(df)\nax = df[sorted(df.columns)].T.plot.bar(grid=True, xlabel=\"#boxes per image\", ylabel=\"#images per class\", figsize=(7, 3))","89151e2a":"# from tqdm.autonotebook import tqdm\n\n# train_images['BodyPart'] = [pydicom.dcmread(os.path.join(BASE_PATH, p)).BodyPartExamined for p in tqdm(train_images['path'])]\n\n# counts, cases = [], []\n# for cls, df in train_images.groupby(\"class\"):\n#     counts.append(dict(df[\"BodyPart\"].value_counts()))\n#     cases.append(LABELS[cls])\n\n# ax = pd.DataFrame(counts, index=cases).T.plot.bar(grid=True, xlabel=\"BodyPart\", ylabel=\"#images\", figsize=(8, 3))","5233cda9":"def convert_boxes_to_coco(meta, image_hw):\n    # ih, iw = img.shape[:2]\n    ih, iw = image_hw\n    bboxes = []\n    for bbox in meta[\"boxes\"]:\n        # cls, x_center, y_center, width, height\n        bboxes.append({\n            \"cls\": meta[\"class\"],\n            \"x_center\": float(bbox['x'] + bbox['width'] \/ 2) \/ iw,\n            \"y_center\": float(bbox['y'] + bbox['height'] \/ 2) \/ ih,\n            \"width\":  bbox['width'] \/ iw,\n            \"height\": bbox['height'] \/ ih\n        })\n    return bboxes","aed2c08b":"PATH_OUT = \"\/kaggle\/working\"\nPATH_OUT_IMAGE = \"\/kaggle\/working\/images\"\nPATH_OUT_LABEL = \"\/kaggle\/working\/labels\"\nSPACING = 1.0\n\nfor d in (PATH_OUT_IMAGE, PATH_OUT_LABEL):\n    os.makedirs(d, exist_ok=True)\n    for dd in (\"train\", \"test\"):\n        os.makedirs(os.path.join(d, dd), exist_ok=True)","424bb9a1":"from tqdm.autonotebook import tqdm\nfrom multiprocessing import Pool\nfrom functools import partial\n\n\ndef conver_image(id_row, dir_name):\n    _, row = id_row\n    # phase = \"train\" if np.random.random() < 0.8 else \"valid\"\n    img, meta = load_image(os.path.join(BASE_PATH, row['path']), dict(row), spacing=SPACING)\n    plt.imsave(os.path.join(PATH_OUT_IMAGE, dir_name, f\"{row['name']}.jpg\"), img, cmap='gray')\n    bboxes = convert_boxes_to_coco(meta, image_hw=img.shape[:2])\n    df = pd.DataFrame(bboxes)[[\"cls\", \"x_center\", \"y_center\", \"width\", \"height\"]] if bboxes else pd.DataFrame(bboxes)\n    df.to_csv(os.path.join(PATH_OUT_LABEL, dir_name, f\"{row['name']}.txt\"), sep=\" \", index=None, header=None)\n    meta.update({\"bboxes\": bboxes, \"image_size\": img.shape})\n    return meta\n\nmetas = []\npool = Pool(os.cpu_count())\nfor meta in pool.map(partial(conver_image, dir_name=\"train\"), tqdm(train_images.iterrows(), total=len(train_images))):\n    metas.append(meta)\npool.close()\npool.join()","3d23ed33":"import json\n\nannots = []\nrunning_id = 0\nfor idx, meta in enumerate(metas):\n    ih, iw = meta[\"image_size\"]\n    for i, box in enumerate(meta[\"bboxes\"]):\n        w = int(box['width'] * iw)\n        h = int(box['height'] * ih)\n        x = int(box['x_center'] * iw) - np.ceil(w \/ 2.)\n        y = int(box['y_center'] * ih) - np.ceil(h \/ 2.)\n        rec = {\n            \"id\": running_id,\n            \"image_id\": idx,\n            \"category_id\": meta['class'],\n            \"area\": w * h,\n            \"bbox\": [max(0, x), max(0, y), w, h],\n            \"segmentation\": [],\n            \"iscrowd\": 0,\n        }\n        annots.append(rec)\n        running_id += 1\n\ncoco = {\n    \"annotations\": annots,\n    \"categories\": [{\"id\": i, \"name\": n, \"supercategory\": \"\"} for i, n in enumerate(LABELS)],\n    \"images\": [{\"id\": idx, \"file_name\": f\"{meta['name']}.jpg\", \"height\": meta[\"image_size\"][0], \"width\": meta[\"image_size\"][1]} for idx, meta in enumerate(metas)],\n}\n\npath_json = os.path.join(PATH_OUT, \"covid_train.json\")\nwith open(path_json, \"w\") as fp:\n    json.dump(coco, fp)","5de86d4e":"found_images = glob.glob(os.path.join(BASE_PATH, 'test', '*', '*', '*.dcm'))\n\ntest_images = pd.DataFrame({\n    \"name\": os.path.splitext(os.path.basename(p))[0],\n    \"path\": os.path.sep.join(p.split(os.path.sep)[-4:])\n} for p in found_images)\ndisplay(test_images.head())\n\npool = Pool(os.cpu_count())\nlist(pool.imap_unordered(partial(conver_image, dir_name=\"test\"), tqdm(test_images.iterrows(), total=len(test_images))))\npool.close()\npool.join()","0945fb3d":"! cd \/kaggle\/working\n! zip covid-dataset.zip -q -r *","8820aa84":"! rm -rf lightning-flash\n! pip uninstall -y lightning-flash\n! git clone https:\/\/github.com\/PyTorchLightning\/lightning-flash.git\n! cd lightning-flash && git checkout feature\/icevision && pip install -q .[image]\n# ! pip install -q https:\/\/github.com\/PyTorchLightning\/lightning-flash\/archive\/refs\/heads\/feature\/icevision.zip#egg=lightning-flash[image]\n! pip uninstall -y fiftyone wandb\n# ! pip install -q effdet","67778c93":"import flash\nfrom flash.image import ObjectDetectionData, ObjectDetector\n\n# 1. Create the DataModule\ndm = ObjectDetectionData.from_coco(\n    train_folder=os.path.join(PATH_OUT_IMAGE, 'train'),\n    train_ann_file=os.path.join(PATH_OUT, \"covid_train.json\"),\n    val_split=0.1,\n    batch_size=6,\n    image_size=640,\n)","a808bac9":"# 2. Build the task\nmodel = ObjectDetector(\n    head=\"efficientdet\",\n    backbone=\"tf_d3_ap\",\n    learning_rate=1.5e-5,\n    num_classes=dm.num_classes,\n    image_size=640,\n)","91d16ee2":"import pytorch_lightning as pl\nprint(pl.__version__)\nlogger = pl.loggers.CSVLogger(save_dir='logs\/')\n                              \n# 3. Create the trainer and finetune the model\ntrainer = flash.Trainer(\n    max_epochs=20,\n    gpus=1,\n    precision=16,\n    accumulate_grad_batches=12,\n    logger=logger,\n    val_check_interval=0.5,\n)\ntrainer.finetune(model, datamodule=dm, strategy=\"freeze_unfreeze\")\n\n# 3. Save the model!\ntrainer.save_checkpoint(\"object_detection_model.pt\")","356ff14a":"metrics = pd.read_csv(f'{trainer.logger.log_dir}\/metrics.csv')\ndisplay(metrics.head())\n\naggreg_metrics = []\nagg_col = \"epoch\"\nfor i, dfg in metrics.groupby(agg_col):\n    agg = dict(dfg.mean())\n    agg[agg_col] = i\n    aggreg_metrics.append(agg)\n\ndf_metrics = pd.DataFrame(aggreg_metrics)\ndf_metrics[['loss', 'class_loss', 'box_loss']].plot(grid=True, legend=True, xlabel=agg_col)\ndf_metrics[['Precision (IoU=0.50:0.95,area=all)', 'Recall (IoU=0.50:0.95,area=all,maxDets=100)']].plot(grid=True, legend=True, xlabel=agg_col)","32e51708":"# 4. Detect objects in a few images!\npredictions = []\nmodel.to(\"cuda\")\nfor _, row in tqdm(test_images.iterrows(), total=len(test_images)):\n    p_img = os.path.join(PATH_OUT_IMAGE, \"test\", f\"{row['name']}.jpg\")\n    preds = model.predict([p_img])\n    rec = {**dict(row), \"predictions\": preds[0]}\n    predictions.append(rec)","ae2779e9":"print(predictions[0])\ndisplay(predictions[0]['predictions'].as_dict)","bf50dcb4":"pred_boxes = [len(p['predictions'].as_dict()['detection']['bboxes']) for p in predictions]\nprint(dict(enumerate(np.bincount(pred_boxes))))\nprint(pred_boxes)","316745e6":"# ! ls \/kaggle\/working\/images\/test\/\n# ! ls \/kaggle\/working\/images\/test\/","9a9075dc":"## Training with Flash","78ddf8ba":"conver train images and save metadata to the overview table","07062553":"From previous we ca see nb images is larger nb studies...","cc64093c":"Converting the test images","be538b91":"See sanity chek that sumof labels is equls to nb samples and show case\/label distibution","eb414486":"### Show training charts","39b968c3":"## Setup environment\n\nInstalling needed packages and list versions for reproducibility","a2103702":"### Samples per class\n\ngroup images per class and show a few sample images per class together with detection bounding boxes","eb87a124":"### Fuse the two tables\n\nlets trasfer the labels to the images","7fab3282":"### Number of detections per case\n\nHistgram of number of detections in subject depending on type","11d30dff":"# SIIM-FISABIO-RSNA COVID-19 Detection\n\nIdentify and localize COVID-19 abnormalities on chest radiographs","777003ce":"prepare the new dataset folders","4e1062f8":"**train_image_level.csv** - the train image-level metadata, with one row for each image, including both correct labels and any bounding boxes in a dictionary format. Some images in both test and train have multiple bounding boxes.\n\n- `id` - unique image identifier\n- `boxes` - bounding boxes in easily-readable dictionary format\n- `label` - the correct prediction label for the provided bounding boxes\n\nEnrich table with some countings and parsing the base name","b9989415":"**train_study_level.csv** - the train study-level metadata, with one row for each study, including correct labels.\n\n- `id` - unique study identifier\n- `Negative for Pneumonia` - 1 if the study is negative for pneumonia, 0 otherwise\n- `Typical Appearance` - 1 if the study has this appearance, 0 otherwise\n- `Indeterminate Appearance`  - 1 if the study has this appearance, 0 otherwise\n- `Atypical Appearance` - if the study has this appearance, 0 otherwise\n","d63f4bcf":"### Predict labels for test images","cee98393":"## Data exploration\n\nChecking what data do we have available and what is the labels distribution...\n\nWe start with:\n- naive loading tables\n- see distributions\n- visualuze images","916bbea6":"### Overview & Annotations\n\nStarting with checking what is the provided tables...","833639de":"### Run traning","c9de6e3d":"Creating the COCO coordinate file which contains:\n\n- bounding boxes\n- images with dimensions\n- class describtion","31d0a997":"### Show sample image\n\nloading the mages from DICOM format and show then in standard figures...","12b8da42":"## Convert to COCO\n\nThe *.txt file specifications are:\n\n- One row per object - class x_center y_center width height\n- Box coordinates must be in normalized xywh format (from 0 - 1)\n- Class numbers are zero-indexed\n\n<img src=\"https:\/\/user-images.githubusercontent.com\/26833433\/91506361-c7965000-e886-11ea-8291-c72b98c25eec.jpg\" width=\"480\">"}}