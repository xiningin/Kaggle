{"cell_type":{"8a494df4":"code","99771c3a":"code","032324be":"code","e99633b3":"code","fec6647c":"code","a633f0a3":"code","1f1ea68b":"code","f4d6b1c9":"code","73e428df":"code","86701d9c":"code","6fe9857c":"code","0218d769":"code","419f9250":"code","f8109cf3":"code","e9b7b6b8":"code","0088d408":"code","ba30234b":"code","36815d55":"code","dba7d70f":"code","2c866a15":"code","1731a2c7":"code","11623be4":"code","629e5170":"code","d1c47f0d":"code","bb78875d":"code","dcb2adde":"code","8d375e78":"code","9bf7970d":"code","aff9b57f":"code","31464e6c":"code","97f9c109":"code","70755074":"code","403b4724":"code","28ffc53c":"code","521f32c5":"code","18433517":"code","bf520b7a":"code","c228415c":"code","59b4740f":"code","e27e334c":"code","c0c92c69":"code","1848c806":"code","16bc0b2e":"code","50b4e1c2":"markdown","bcc12bae":"markdown","098b07e2":"markdown","4bfae573":"markdown","30454238":"markdown","10cc48a6":"markdown","3e686721":"markdown","6e401411":"markdown","0ede69b7":"markdown","203c622a":"markdown","ade6131d":"markdown","e842f465":"markdown","8d47a7ad":"markdown","6f6dfdd9":"markdown","cde53d7b":"markdown","b50ce78e":"markdown","d394cea0":"markdown","29cd4c4d":"markdown","d9845095":"markdown","8faebd58":"markdown"},"source":{"8a494df4":"import sys\nsys.path.append('..\/input\/pytorch-image-models\/pytorch-image-models-master')","99771c3a":"# Asthetics\nimport warnings\nimport sklearn.exceptions\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n\n# General\nfrom tqdm import tqdm\nfrom collections import defaultdict\nimport pandas as pd\nimport numpy as np\nimport os\nimport random\nimport glob\npd.set_option('display.max_columns', None)\n\n# Visualizations\nfrom PIL import Image\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import iplot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nimport plotly.express as px\n%matplotlib inline\nsns.set(style=\"whitegrid\")\n\n# Image Aug\nimport albumentations\nfrom albumentations.pytorch.transforms import ToTensorV2\n\n# Machine Learning\n# Utils\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\n# Deep Learning\nimport torch\nimport torchvision\nimport timm\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\n#Metrics\nfrom sklearn.metrics import roc_auc_score\n\n# Random Seed Initialize\nRANDOM_SEED = 42\n\ndef seed_everything(seed=RANDOM_SEED):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \nseed_everything()\n\n# Device Optimization\nif torch.cuda.is_available():\n    device = torch.device('cuda')\nelse:\n    device = torch.device('cpu')\n    \nprint(f'Using device: {device}')","032324be":"csv_dir = '..\/input\/seti-breakthrough-listen'\ntrain_dir = '..\/input\/seti-breakthrough-listen\/train'\ntest_dir = '..\/input\/seti-breakthrough-listen\/test'\n\ntrain_file_path = os.path.join(csv_dir, 'train_labels.csv')\nsample_sub_file_path = os.path.join(csv_dir, 'sample_submission.csv')\n\nprint(f'Train file: {train_file_path}')\nprint(f'Train file: {sample_sub_file_path}')","e99633b3":"train_df = pd.read_csv(train_file_path)\ntest_df = pd.read_csv(sample_sub_file_path)","fec6647c":"train_df.sample(10)","a633f0a3":"test_df.sample(10)","1f1ea68b":"ax = plt.subplots(figsize=(12, 6))\nsns.set_style(\"whitegrid\")\nsns.countplot(x='target', data=train_df);\nplt.ylabel(\"No. of Observations\", size=20);\nplt.xlabel(\"Target\", size=20);","f4d6b1c9":"def return_filpath(name, folder=train_dir):\n    path = os.path.join(folder, name[0], f'{name}.npy')\n    return path","73e428df":"# Inspired from https:\/\/www.kaggle.com\/ihelon\/signal-search-exploratory-data-analysis\ndef show_cadence(filename, label=None, folder=train_dir):\n    plt.figure(figsize=(16, 10))\n    arr = np.load(return_filpath(filename, folder=folder))\n    for i in range(6):\n        plt.subplot(6, 1, i + 1)\n        if i == 0:\n            if label is not None:\n                plt.title(f'ID: {os.path.basename(filename)} TARGET: {label}', fontsize=18)\n        plt.imshow(arr[i].astype(float), interpolation='nearest', aspect='auto')\n        plt.text(5, 100, ['ON', 'OFF'][i % 2], bbox={'facecolor': 'white'})\n        plt.xticks([])\n    plt.show()","86701d9c":"# Inspired from https:\/\/www.kaggle.com\/ihelon\/signal-search-exploratory-data-analysis\ndef show_channels(filename, label=None, folder=train_dir):\n    plt.figure(figsize=(16, 10))\n    if label is not None:\n        plt.suptitle(f'ID: {os.path.basename(filename)} TARGET: {label}', fontsize=18)\n    arr = np.load(return_filpath(filename, folder=folder))\n    for i in range(6):\n        plt.subplot(2, 3, i + 1)\n        plt.imshow(arr[i].astype(float))\n        plt.grid(b=None)\n    plt.show()","6fe9857c":"df_tmp = train_df[train_df['target'] == 0].sample(3)\nfor _, row in df_tmp.iterrows():\n    show_cadence(filename = row['id'],\n                 label = row['target'])","0218d769":"for _, row in df_tmp.iterrows():\n    show_channels(filename = row['id'],\n                 label = row['target'])","419f9250":"df_tmp = train_df[train_df['target'] == 1].sample(3)\nfor _, row in df_tmp.iterrows():\n    show_cadence(filename = row['id'],\n                 label = row['target'])","f8109cf3":"for _, row in df_tmp.iterrows():\n    show_channels(filename = row['id'],\n                 label = row['target'])","e9b7b6b8":"def get_train_transforms():\n    return albumentations.Compose(\n        [\n            albumentations.Resize(256,256),\n            albumentations.HorizontalFlip(p=0.5),\n            albumentations.VerticalFlip(p=0.5),\n            albumentations.Rotate(limit=180, p=0.7),\n            albumentations.RandomBrightness(limit=0.6, p=0.5),\n            albumentations.Cutout(\n                num_holes=10, max_h_size=12, max_w_size=12,\n                fill_value=0, always_apply=False, p=0.5\n            ),\n            albumentations.ShiftScaleRotate(\n                shift_limit=0.25, scale_limit=0.1, rotate_limit=0\n            ),\n            ToTensorV2(p=1.0),\n        ]\n    )\n\ndef get_valid_transforms():\n    return albumentations.Compose(\n        [\n            albumentations.Resize(256,256),\n            ToTensorV2(p=1.0)\n        ]\n    )\n\ndef get_test_transforms(TTA):\n    if TTA > 1:\n        return albumentations.Compose(\n            [\n                albumentations.Resize(256,256),\n                albumentations.HorizontalFlip(p=0.5),\n                albumentations.VerticalFlip(p=0.5),\n                albumentations.Rotate(limit=180, p=0.7),\n                albumentations.RandomBrightness(limit=0.6, p=0.5),\n                albumentations.ShiftScaleRotate(\n                    shift_limit=0.25, scale_limit=0.1, rotate_limit=0\n                ),\n                ToTensorV2(p=1.0)\n            ]\n        )\n    else:\n        return albumentations.Compose(\n            [\n                albumentations.Resize(256,256),\n                ToTensorV2(p=1.0)\n            ]\n        )","0088d408":"train_df['image_path'] = train_df['id'].apply(lambda x: return_filpath(x))\ntest_df['image_path'] = test_df['id'].apply(lambda x: return_filpath(x, folder=test_dir))","ba30234b":"(X_train, X_valid, y_train, y_valid) = train_test_split(train_df['image_path'],\n                                                        train_df['target'],\n                                                        test_size=0.2,\n                                                        stratify=train_df['target'],\n                                                        shuffle=True,\n                                                        random_state=RANDOM_SEED)","36815d55":"class SETIDataset(Dataset):\n    def __init__(self, images_filepaths, targets, transform=None):\n        self.images_filepaths = images_filepaths\n        self.targets = targets\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.images_filepaths)\n\n    def __getitem__(self, idx):\n        image_filepath = self.images_filepaths[idx]\n        image = np.load(image_filepath)\n        image = image.astype(np.float32)\n        image = np.vstack(image).transpose((1, 0))\n            \n        if self.transform is not None:\n            image = self.transform(image=image)[\"image\"]\n        else:\n            image = image[np.newaxis,:,:]\n            image = torch.from_numpy(image).float()\n        \n        label = torch.tensor(self.targets[idx]).float()\n        return image, label","dba7d70f":"train_dataset = SETIDataset(\n    images_filepaths=X_train.values,\n    targets=y_train.values,\n    transform=get_train_transforms()\n)\n\nvalid_dataset = SETIDataset(\n    images_filepaths=X_valid.values,\n    targets=y_valid.values,\n    transform=get_valid_transforms()\n)","2c866a15":"def usr_roc_score(output, target):\n    try:\n        y_pred = torch.sigmoid(output).cpu()\n        y_pred = y_pred.detach().numpy()\n        target = target.cpu()\n\n        return roc_auc_score(target, y_pred)\n    except:\n        return 0.5","1731a2c7":"class MetricMonitor:\n    def __init__(self, float_precision=3):\n        self.float_precision = float_precision\n        self.reset()\n\n    def reset(self):\n        self.metrics = defaultdict(lambda: {\"val\": 0, \"count\": 0, \"avg\": 0})\n\n    def update(self, metric_name, val):\n        metric = self.metrics[metric_name]\n\n        metric[\"val\"] += val\n        metric[\"count\"] += 1\n        metric[\"avg\"] = metric[\"val\"] \/ metric[\"count\"]\n\n    def __str__(self):\n        return \" | \".join(\n            [\n                \"{metric_name}: {avg:.{float_precision}f}\".format(\n                    metric_name=metric_name, avg=metric[\"avg\"],\n                    float_precision=self.float_precision\n                )\n                for (metric_name, metric) in self.metrics.items()\n            ]\n        )","11623be4":"scheduler_params = {\n    'name': 'CosineAnnealingLR',\n    'T_max': 10,\n    'min_lr': 1e-6,\n}","629e5170":"def get_scheduler(optimizer, scheduler_params=scheduler_params):\n    scheduler = CosineAnnealingLR(optimizer,\n                                  T_max=scheduler_params['T_max'],\n                                  eta_min=scheduler_params['min_lr'],\n                                  last_epoch=-1)\n    return scheduler","d1c47f0d":"class_counts = y_train.value_counts().to_list()\nnum_samples = sum(class_counts)\nlabels = y_train.to_list()\n\nclass_weights = [num_samples\/class_counts[i] for i in range(len(class_counts))]\nweights = [class_weights[labels[i]] for i in range(int(num_samples))]\nsampler = WeightedRandomSampler(torch.DoubleTensor(weights), int(num_samples))","bb78875d":"focal_loss_params = {\n    'alpha': 1,\n    'gamma': 2,\n    'xent': 0.1,\n    'device': device\n}","dcb2adde":"class FocalCosineLoss(nn.Module):\n    def __init__(self, focal_loss_params=focal_loss_params):\n        super(FocalCosineLoss, self).__init__()\n        self.alpha = focal_loss_params['alpha']\n        self.gamma = focal_loss_params['gamma']\n        self.xent = focal_loss_params['xent']\n        if focal_loss_params['device'] == 'cuda':\n            self.y = torch.Tensor([1]).cuda()\n        else:\n            self.y = torch.Tensor([1]).cpu()\n\n    def forward(self, input, target, reduction='mean'):\n        cosine_loss = F.cosine_embedding_loss(input, F.one_hot(target, num_classes=input.size(-1)), self.y, reduction=reduction)\n        cent_loss = F.cross_entropy(F.normalize(input), target, reduce=False)\n        pt = torch.exp(-cent_loss)\n        focal_loss = self.alpha * (1-pt)**self.gamma * cent_loss\n\n        if reduction == 'mean':\n            focal_loss = torch.mean(focal_loss)\n\n        return cosine_loss + self.xent * focal_loss","8d375e78":"label_smoothing_params = {\n    'smoothing': 1,\n    'classes': 2\n}","9bf7970d":"class LabelSmoothingLoss(nn.Module):\n    def __init__(self, classes=label_smoothing_params['classes'],\n                 smoothing=label_smoothing_params['smoothing']):\n        super(LabelSmoothingLoss, self).__init__()\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.cls = classes\n    \n    def forward(self, pred, target):\n        pred = pred.log_softmax(dim=-1)\n        with torch.no_grad():\n            true_dist = torch.zeros_like(pred)\n            true_dist.fill_(self.smoothing \/ (self.cls - 1))\n            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))","aff9b57f":"params = {\n    'model': 'nfnet_l0',\n    'inp_channels': 1,\n    'device': device,\n    'lr': 1e-4,\n    'weight_decay': 1e-6,\n    'batch_size': 32,\n    'num_workers' : 0,\n    'epochs': 10,\n    'out_features': 1,\n    'Balance_Dataset': True\n}","31464e6c":"if params['Balance_Dataset']:\n    train_loader = DataLoader(\n        train_dataset, batch_size=params['batch_size'], sampler = sampler,\n        num_workers=params['num_workers'], pin_memory=True\n    )\nelse:\n    train_loader = DataLoader(\n        train_dataset, batch_size=params['batch_size'], shuffle=True,\n        num_workers=params['num_workers'], pin_memory=True\n    )\n\nval_loader = DataLoader(\n    valid_dataset, batch_size=params['batch_size'], shuffle=False,\n    num_workers=params['num_workers'], pin_memory=True\n)","97f9c109":"class AlienNet(nn.Module):\n    def __init__(self, model_name=params['model'], out_features=params['out_features'],\n                 inp_channels=params['inp_channels'], pretrained=True):\n        super().__init__()\n        self.model = timm.create_model(model_name, pretrained=pretrained,\n                                       in_chans=inp_channels)\n        if model_name.split('_')[0] == 'efficientnet':\n            n_features = self.model.classifier.in_features\n            self.model.conv_stem = nn.Conv2d(inp_channels, 40, kernel_size=(3, 3),\n                                             stride=(2, 2), padding=(1, 1), bias=False)\n            self.model.classifier = nn.Linear(n_features, out_features)\n        \n        elif model_name.split('_')[0] == 'nfnet':\n            n_features = self.model.head.fc.in_features\n            self.model.head.fc = nn.Linear(n_features, out_features)\n    \n    def forward(self, x):\n        x = self.model(x)\n        return x","70755074":"model = AlienNet()\nmodel = model.to(params['device'])\ncriterion = nn.BCEWithLogitsLoss().to(params['device'])\noptimizer = torch.optim.Adam(model.parameters(), lr=params['lr'],\n                             weight_decay=params['weight_decay'],\n                             amsgrad=False)\nscheduler = get_scheduler(optimizer)","403b4724":"def train(train_loader, model, criterion, optimizer, epoch, params):\n    metric_monitor = MetricMonitor()\n    model.train()\n    stream = tqdm(train_loader)\n    max_grad_norm = 1000\n    for i, (images, target) in enumerate(stream, start=1):\n        images = images.to(params['device'], non_blocking=True)\n        target = target.to(params['device'], non_blocking=True).float().view(-1, 1)\n        output = model(images)\n        loss = criterion(output, target)\n        roc_score = usr_roc_score(output, target)\n        metric_monitor.update('Loss', loss.item())\n        metric_monitor.update('ROC', roc_score)\n        loss.backward()\n        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n        optimizer.step()\n        optimizer.zero_grad()\n        stream.set_description(\n            \"Epoch: {epoch}. Train.      {metric_monitor}\".format(\n                epoch=epoch,\n                metric_monitor=metric_monitor)\n        )","28ffc53c":"def validate(val_loader, model, criterion, epoch, params):\n    metric_monitor = MetricMonitor()\n    model.eval()\n    stream = tqdm(val_loader)\n    final_targets = []\n    final_outputs = []\n    with torch.no_grad():\n        for i, (images, target) in enumerate(stream, start=1):\n            images = images.to(params['device'], non_blocking=True)\n            target = target.to(params['device'], non_blocking=True).float().view(-1, 1)\n            output = model(images)\n            loss = criterion(output, target)\n            roc_score = usr_roc_score(output, target)\n            metric_monitor.update('Loss', loss.item())\n            metric_monitor.update('ROC', roc_score)\n            stream.set_description(\n                \"Epoch: {epoch}. Validation. {metric_monitor}\".format(\n                    epoch=epoch,\n                    metric_monitor=metric_monitor)\n            )\n            \n            targets = target.detach().cpu().numpy().tolist()\n            outputs = output.detach().cpu().numpy().tolist()\n            \n            final_targets.extend(targets)\n            final_outputs.extend(outputs)\n    return final_outputs, final_targets","521f32c5":"best_roc = -np.inf\nbest_epoch = -np.inf\nbest_model_name = None\nfor epoch in range(1, params['epochs'] + 1):\n    train(train_loader, model, criterion, optimizer, epoch, params)\n    predictions, valid_targets = validate(val_loader, model, criterion, epoch, params)\n    roc_auc = round(roc_auc_score(valid_targets, predictions), 3)\n    torch.save(model.state_dict(),f\"{params['model']}_{epoch}_epoch_{roc_auc}_roc_auc.pth\")\n    if roc_auc > best_roc:\n        best_roc = roc_auc\n        best_epoch = epoch\n        best_model_name = f\"{params['model']}_{epoch}_epoch_{roc_auc}_roc_auc.pth\"\n    scheduler.step()","18433517":"print(f'The best ROC: {best_roc} was achieved on epoch: {best_epoch}.')\nprint(f'The Best saved model is: {best_model_name}')","bf520b7a":"NUM_TTA = 1","c228415c":"model = AlienNet()\nmodel.load_state_dict(torch.load(best_model_name))\nmodel = model.to(params['device'])","59b4740f":"model.eval()\npredicted_labels = None\nfor i in range(NUM_TTA):\n    test_dataset = SETIDataset(\n        images_filepaths = test_df['image_path'].values,\n        targets = test_df['target'].values,\n        transform = get_test_transforms(NUM_TTA)\n    )\n    test_loader = DataLoader(\n        test_dataset, batch_size=params['batch_size'],\n        shuffle=False, num_workers=params['num_workers'],\n        pin_memory=True\n    )\n    \n    temp_preds = None\n    with torch.no_grad():\n        for (images, target) in tqdm(test_loader):\n            images = images.to(params['device'], non_blocking=True)\n            output = model(images)\n            predictions = torch.sigmoid(output).cpu().numpy()\n            if temp_preds is None:\n                temp_preds = predictions\n            else:\n                temp_preds = np.vstack((temp_preds, predictions))\n    \n    if predicted_labels is None:\n        predicted_labels = temp_preds\n    else:\n        predicted_labels += temp_preds\n        \npredicted_labels \/= NUM_TTA","e27e334c":"sub_df = pd.DataFrame()\nsub_df['id'] = test_df['id']\nsub_df['target'] = predicted_labels","c0c92c69":"sub_df.head()","1848c806":"sub_df.to_csv('submission.csv', index=False)","16bc0b2e":"torch.save(model.state_dict(), f\"{params['model']}_{best_epoch}epochs_weights.pth\")","50b4e1c2":"## Data Sampler","bcc12bae":"## Scheduler","098b07e2":"Let's see some examples of the negative class:-","4bfae573":"This is a simple starter kernel on implementation of Transfer Learning using Pytorch for this problem. Pytorch has many SOTA Image models which you can try out using the guidelines in this notebook.\n\nI hope you have learnt something from this notebook. I have created this notebook as a baseline model, which you can easily fork and paly-around with to get much better results. I might update parts of it down the line when I get more GPU hours and some interesting ideas.\n\n**If you liked this notebook and use parts of it in you code, please show some support by upvoting this kernel. It keeps me inspired to come-up with such starter kernels and share it with the community.**\n\nThanks and happy kaggling!","30454238":"### 2. Label Smoothing","10cc48a6":"## Final Save Model","3e686721":"Now looking at some data of positive class:-","6e401411":"## CNN Model","0ede69b7":"## Loss Functions\n\n### 1. Focal Cosine Loss","203c622a":"Let's observe the class imbalance in the target variable...","ade6131d":"## Image Augmentation","e842f465":"As we can see (and sort of also hoped for), this is a pretty imbalanced dataset. We need to take care of that while building our models...","8d47a7ad":"# Model","6f6dfdd9":"## Dataset","cde53d7b":"## Metrics","b50ce78e":"# EDA","d394cea0":"# Prediction","29cd4c4d":"Loading the best model from training step...","d9845095":"![SETI](https:\/\/earthsky.org\/upl\/2020\/02\/Earth-transit-zone-Breakthrough-Listen.jpg)\n# About This Notebook\nThis is a first run through the competition data to try and understand the datatset and realise the problem at hand with some quick EDA and a baseline Model.  \n**If you found this notebook useful and use parts of it in your work, please don't forget to show your appreciation by upvoting this kernel. That keeps me motivated and inspires me to write and share these public kernels.** \ud83d\ude0a\n\n# Problem Statement\n* The Breakthrough Listen team at the University of California, Berkeley, employs the world\u2019s most powerful telescopes to scan millions of stars for signs of technology.\n* It\u2019s hard to search for a faint needle of alien transmission in the huge haystack of detections from modern technology.\n* Current methods use two filters to search through the haystack.\n    * First, the Listen team intersperses scans of the target stars with scans of other regions of sky. Any signal that appears in both sets of scans probably isn\u2019t coming from the direction of the target star.\n    * Second, the pipeline discards signals that don\u2019t change their frequency, because this means that they are probably nearby the telescope.\n* Use data science skills to help identify anomalous signals in scans of Breakthrough Listen targets.\n* Because there are no confirmed examples of alien signals to use to train machine learning algorithms, the team included some simulated signals.\n\n# Why this competition?\nAs evident from the problem statement, this competition presents an interesting challenge straight out of a Sci-Fi movie stuff!  \nAlso (if successful) this model should be able to answer one of the biggest questions in science.\n\n# Expected Outcome\nGiven a numpy array of signal, we should be able to identify it as a positive class (signal from an alien lifeform) or negative class (signal from one of our devices).\n\n# Data Description\nData is stored in a numpy float16 format in training folder and the labes are mentioned in the `train_labels.csv` file where the first letter of the file name indicates the subfolder the `.npy` file is placed inside the train directory.  \nThe data consist of two-dimensional arrays {shape = (6, 273, 256)}, so there may be approaches from computer vision that are promising, as well as digital signal processing, anomaly detection, and more.\n\n# Grading Metric\nSubmissions are evaluated on **area under the ROC curve** between the predicted probability and the observed target.\n\n# Problem Category\nFrom the data and objective its is evident that this is a **Classification Problem**. But we have an option for the approach starting with vanilla ML methods to Computer Vision to Anomaly detection etc.\n\nSo without further ado, let's now start with some basic imports to take us through this:-","8faebd58":"## Training and Validation"}}