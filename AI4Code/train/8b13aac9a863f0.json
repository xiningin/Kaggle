{"cell_type":{"a04dc604":"code","170ff3b7":"code","0cebdf62":"code","8539e029":"code","752af382":"code","ef9631c1":"code","1aeca16c":"code","1134893e":"code","eb2f9f68":"code","d038f963":"code","4b11f3e5":"code","83d03679":"code","7d2ee85e":"code","7731975f":"code","04eefa19":"code","0f138f66":"code","6dac0a77":"code","8592f10a":"code","da01a89d":"code","1c9f0961":"code","e5846119":"markdown","6ca2e836":"markdown","c15f62e1":"markdown","0c7c9fca":"markdown","46a3a977":"markdown"},"source":{"a04dc604":"import numpy as np\nimport pandas as pd\n\nreal = pd.read_csv('..\/input\/fake-and-real-news-dataset\/True.csv')\nfake = pd.read_csv('..\/input\/fake-and-real-news-dataset\/Fake.csv')","170ff3b7":"### If you want to run the notebook faster at the cost of accuracy you can uncomment out the two lines below to use only a sample of 40k\n\n# real = real.sample(20000)\n# fake = fake.sample(20000)\nreal.shape, fake.shape","0cebdf62":"real.head()","8539e029":"num = 100 # Selects an article to preview fromt the real dataset\n\nprint('Title: ', real.title[num],'\\n')\nprint('Article:\\n', real.text[num])","752af382":"### Based on the differences in this column we cannot use this feature without data leakage.\nprint('subjects of fake news articles:',fake['subject'].unique())\nprint('subjects of real news articles:',real['subject'].unique())","ef9631c1":"### Since the real news articles and fake news articles are in two different data sets we can add a label column easily\nreal['is_real'] = 1\nfake['is_real'] = 0","1aeca16c":"data = real.append(fake)\ndata.index = range(data.shape[0])\ndata.sample(10)","1134893e":"data.isnull().sum()","eb2f9f68":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import stopwords\nstopwords = stopwords.words('english')","d038f963":"countvec = CountVectorizer(strip_accents='ascii', stop_words=stopwords, ngram_range=(1,2), max_features=1000)\ntfidf = TfidfVectorizer(strip_accents='ascii', stop_words=stopwords, ngram_range=(1,2), max_features=1000)\n\ntext_column = 'title' # use 'text' to train on the full articles or 'title' to only use the titles.\n                     # 'text' will take alot longer for the vectorizers to run","4b11f3e5":"%%time\ncount_dat = countvec.fit_transform(data[text_column])\ncount_dat = pd.DataFrame(count_dat.toarray())\ncount_dat.sample(10)","83d03679":"### Unhide the output to see the vocabulary dictionary generated by the vectorizer\n\ncountvec.vocabulary_","7d2ee85e":"%%time\ntfidf_dat = tfidf.fit_transform(data[text_column])\ntfidf_dat = pd.DataFrame(tfidf_dat.toarray())\ntfidf_dat.sample(10)","7731975f":"from sklearn.model_selection import train_test_split\ny = data.is_real\n\ntrain_x1, test_x1, train_y1, test_y1 = train_test_split(count_dat, y, test_size=.3, random_state=42)\ntrain_x2, test_x2, train_y2, test_y2 = train_test_split(tfidf_dat, y, test_size=.3, random_state=42)","04eefa19":"%%time\n\nfrom sklearn.svm import LinearSVC\nfrom sklearn import metrics\n\nsvm = LinearSVC()\nsvm.fit(train_x1, train_y1)\npreds = svm.predict(test_x1)\nprint('Accuracy with count vectorizer:', metrics.accuracy_score(preds, test_y1), '\\n\\n')","0f138f66":"%%time\n\nsvm = LinearSVC()\nsvm.fit(train_x2, train_y2)\npreds = svm.predict(test_x2)\nprint('Accuracy with tfidf vectorizer:', metrics.accuracy_score(preds, test_y2), '\\n\\n')","6dac0a77":"%%time\nfrom sklearn.ensemble import RandomForestClassifier\n\nrfc = RandomForestClassifier()\nrfc.fit(train_x1, train_y1)\nprint('Accuracy with count vectorizer:', metrics.accuracy_score(rfc.predict(test_x1), test_y1), '\\n\\n')","8592f10a":"%%time\nfrom sklearn.ensemble import RandomForestClassifier\n\nrfc = RandomForestClassifier()\nrfc.fit(train_x2, train_y2)\nprint('accuracy with tfidf vectorizer:', metrics.accuracy_score(preds, test_y2), '\\n\\n')","da01a89d":"%%time\nfrom sklearn.naive_bayes import GaussianNB\n\ngnb = GaussianNB()\ngnb.fit(train_x1, train_y1)\nprint('Accuracy with count vectorizer:', metrics.accuracy_score(gnb.predict(test_x1), test_y1), '\\n\\n')","1c9f0961":"%%time\nfrom sklearn.naive_bayes import GaussianNB\n\ngnb = GaussianNB()\ngnb.fit(train_x2, train_y2)\nprint('Accuracy with tfidf vectorizer:', metrics.accuracy_score(preds, test_y2), '\\n\\n')","e5846119":"## Predicting whether a new article is Fake or Real\nIn this notebook we will be using two bag of words techniques; count vectorization and tfidf vectorization. \n\nDepending on whether one runs this notebook using just the titles, or using the full articles the accuracy can vary between 92-99%. However, the full article version does have some data leakage (see the notebook https:\/\/www.kaggle.com\/mosewintner\/5-data-leaks-100-acc-1-word-99-6-acc for why everyone can easily exceed 99%). \n\nThis notbook was originally put together for an NLP study group session. To see some of the context around this notebook you can view the recording on youtube of the study group session: https:\/\/youtu.be\/HlmmXrA4FUU","6ca2e836":"The sample from the count vectorizer below may appear to contain only zeroes, this is because the output is a sparse matrix where the vast majority of columns in any one row will be zero. Below this cell we can also veiw the vocabulary key used to build the vectors.","c15f62e1":"Below we use 3 different models, a support vector machine, a random forest, and a naive bayes model. The %%time magic is used to view the time each model takes to complete training\/inference. It is worth pointing out how fast the SVM is able to train and perform inference compaired the other two models\n\nDepending on the model used the difference between count vectorizer and tfidf is either trivial or signifigant. The most interesting of these changes is the naive bayes model, which not only becomes much more accurate, but also cuts a signifigant amount of time off training\/inference. One explanation of this is that tfidf essentially has weighted values instead of a strightforward count, which help to start the model off with coeficients (or equivilant model parameters) closer to the optimal value. SVMs seem to be able to reach convergence optimally without the extra help, but it would seem that NB\/RF models benifit from initiallizing closer to the optimum values.","0c7c9fca":"Here we create the count vectorizer and tfidf-vectorizer, we use the optional arguments to strip accents, remove n-grams, filter out stop words, and set the vector length to 1k elements. These two vectorizers will perform tokenization on their own so that step has been skipped. One can update the `text_column` variable to select whether to vectorize the articles by title or by the full article text. Using only the title results in models that only achieve 92% accuracy, but the vectorizer will complete in less than 3 seconds (for the entire corpus). When using the full articles the accuracy will exceed 99% on test data but the vectorization time jumps to almost a minute (for the entire corpus).","46a3a977":"Here we split the data into training and test sets, the two sets of training data reprsent the two different vectorization methods, performed side by side for compairison. Since this is a balanced dataset we will use accuracy as the metric."}}