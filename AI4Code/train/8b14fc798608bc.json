{"cell_type":{"b5137e95":"code","b7221af9":"code","7f2db795":"code","9120d512":"code","2c0e8d17":"code","f471002c":"code","7580d0d1":"code","396d057a":"code","06edeb2a":"code","b218f99f":"code","37a3239f":"code","714c5479":"code","4b5cbb67":"code","0af295a3":"code","35a5abeb":"code","665b327d":"code","8f94ee36":"code","043c977c":"code","6c792f2d":"code","2c3fb831":"code","6ceeddbc":"code","768035da":"code","d8ac3072":"code","780ca227":"code","c3a9ae59":"markdown","698377a7":"markdown","f6a9d36e":"markdown","44d4dc18":"markdown","9ff33bc9":"markdown","035014a6":"markdown","f8b87827":"markdown","605665b0":"markdown","23294abb":"markdown","b020108e":"markdown","a897e9b3":"markdown","86a5c4aa":"markdown","e806a4a9":"markdown","48f26f01":"markdown","d052b01c":"markdown","4490b591":"markdown","a18334da":"markdown","3308a7e4":"markdown","1945d120":"markdown","624c7fe9":"markdown","89c031f1":"markdown","eb4c5cbe":"markdown","a3e0b6dd":"markdown","687099d2":"markdown","365ff17a":"markdown","6f084801":"markdown","616b02a0":"markdown","7c61871b":"markdown","b7cee215":"markdown","a33d6c01":"markdown","6f348615":"markdown","910d3c29":"markdown","4a86b0c6":"markdown","b6b81992":"markdown","1e95315f":"markdown","83fee325":"markdown","d8f85c13":"markdown","ca821007":"markdown"},"source":{"b5137e95":"import numpy as np\nimport pandas as pd \nimport seaborn as sns","b7221af9":"data = pd.read_csv('..\/input\/loan-prediction\/train.csv')\ncat_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')","7f2db795":"data.head()","9120d512":"data.info()","2c0e8d17":"import missingno as msno\nmsno.matrix(data)","f471002c":"msno.bar(data, color = 'y', figsize = (10,8))","7580d0d1":"msno.heatmap(data)","396d057a":"ax = msno.dendrogram(data)","06edeb2a":"data.describe()","b218f99f":"data.isnull()","37a3239f":"total = data.isnull().sum().sort_values(ascending=False)\npercent = ((data.isnull().sum()\/data.isnull().count())*100).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data","714c5479":"data.notnull().sum().sort_values(ascending=False)","4b5cbb67":"data.dropna(subset = ['Loan_Amount_Term'], axis = 0, how = 'any', inplace = True)","0af295a3":"# data.drop(['column_name'], axis = 1, inplace = True)","35a5abeb":"columns = ['LoanAmount','ApplicantIncome','CoapplicantIncome']\nsns.pairplot(data[columns])","665b327d":"data['LoanAmount'].fillna(data['LoanAmount'].median(), inplace = True)","8f94ee36":"data['Dependents'].fillna((data['Dependents'].mode()[0]),inplace=True)","043c977c":"data['Gender'].value_counts()","6c792f2d":"data['Gender'].fillna('Male', inplace = True)","2c3fb831":"data['Gender'].fillna(data['Gender'].value_counts().index[0], inplace = True)","6ceeddbc":"data['Self_Employed'].fillna(method='ffill',inplace=True)","768035da":"cat_data.head()","d8ac3072":"total = cat_data.isnull().sum().sort_values(ascending=False)\npercent = ((cat_data.isnull().sum()\/cat_data.isnull().count())*100).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","780ca227":"cat_data['PoolQC'].fillna(\"none\", inplace = True)","c3a9ae59":"# 1.Loading the libraries and data # ","698377a7":"The other way to handle numeric data is to fill the columns. We can use a test static like mean\/median\/mode to fill in, depending on the kind of value that the column holds. \n\nBy checking the skewness and presence of outliers in the data we can decide whether to to fill with mean or median. \n\nWhenever a graph falls on a normal distribution, using the mean is a good choice. But if our data has extreme values, we will need to look at median, because it gives a better representative number for our sample.\n\nAlso, presence of outliers has a major effect on the mean, so in that case using median is a better choice.","f6a9d36e":"# 3.Checking for Missing values numerically","44d4dc18":"Categorical values need to be treated differently. One way is to check the most frequent values in a particular column and fill the column with that value.","9ff33bc9":"Sometimes it may happen that the datatype of a column seems inconsistent with the kind of data it holds. For example, the 'dependents' column refers to the number of dependents of the applicant. It makes more sense for it to have an integer datatype rather an object. Such problems can be solved by typecasting the datatype.","035014a6":"Another way to fill categorical values is to use ffill or bfill. Forward-fill propagates the last observed non-null value forward until another non-null value is encountered.  Backward-fill propagates the first observed non-null value backward until another non-null value is met.\n\nWhen we have a large dataset, I prefer filling the values using ffill\/bfill as it keeps the data distributed. \n\nBelow is an example.","f8b87827":"We can import the missingno library that can be used for graphical analysis of missing values and it is compatible with pandas.\n\nUsing the matrix, we can quickly find the pattern of missingness in the dataset. The sparkline at right summarizes the general shape of the data completeness and points out the rows with the maximum and minimum nullity in the dataset.","605665b0":"Taking a first look at our data gives us a rough idea about the variables and the kind of data it holds. ","23294abb":"Using mode returns a series, so to avoid errors we should use .mode()[0] to get the value.","b020108e":"The two lines can be merged into a single line of code as:-","a897e9b3":"Heatmap shows the correlation of missingness between every 2 column.\n\nA value near -1 means if one variable appears then the other variable is very likely to be missing.\nA value near 0 means there is no dependence between the occurrence of missing values of two variables.\nA value near 1 means if one variable appears then the other variable is very likely to be present.","86a5c4aa":"**4.3 Predict the Missing values**","e806a4a9":"**4.1.2 Imputation**","48f26f01":"A dendogram plot is a tree diagram of missingness that reveals trends deeper than the pairwise ones visible in the correlation heatmap.\n\nFor detailed explanation you can refer to the link at the end of the notebook.","d052b01c":"# MISSING VALUES","4490b591":"We are going to use the House prices: Advanced regression dataset to point out a mistake that poeple might make while handling missing values. Let us have a quick look at the data and the number of missing values that it holds.","a18334da":"This bar chart gives you an idea about how many missing values are there in each column. We can also change the colour and size of the figure as per our wish.","3308a7e4":"When we look at the table, we notice that the column PoolQC has 99% of the values missing and we might tend to delete that column altogether. But if we have a look at the description of the data, PoolQC corresponds to the quality of the pool and any null value indicates absence of a pool which is a factor in determining the price of a house. Hence, before dropping any such data, we should always keep a check.\n\nFilling the missing values with \"none\" means that presence of \"none\" will indicate absence of a pool. ","1945d120":"Missing values are frequently indicated by out-of-range entries; perhaps a negative number in a numeric field that is normally only positive, or a 0 in a numeric field that can never normally be 0. Our current dataset does not show an error of that sort, but this function comes in handy if we want to check the inconsistency in the numeric values.","624c7fe9":"# 2.Checking for Missing values visually # ","89c031f1":"Just as isnull() returns the number of null values, notnull() helps in finding the number of non-null values.","eb4c5cbe":"# 5. A word of caution","a3e0b6dd":"If we find that most of the values of a column is missing, we might want to go ahead and drop the column altogether. We need to keep in mind that dropping data is not a good habit as we might lose information so we should drop the columns only if that column is insignificant and most of the values are missing. Below is an example.","687099d2":"Let us go ahead and look at the various ways how we can detect and handle missing data.","365ff17a":"**4.2 CATEGORICAL VALUES**","6f084801":"Reading the two datasets that are going to be used to demonstrate various methods of handling missing values.","616b02a0":"References: https:\/\/github.com\/ResidentMario\/missingno","7c61871b":"The .info() functions gives us an idea about the total number of non-null datapoints in each column and their datatype.","b7cee215":"# 4.Handling Missing values","a33d6c01":"Hi everyone. \n\nI was literally overwhelmed by the responses I got on my previous [notebook](https:\/\/www.kaggle.com\/twinkle0705\/an-interactive-eda-of-electricity-consumption) and the [dataset](https:\/\/www.kaggle.com\/twinkle0705\/state-wise-power-consumption-in-india) I uploaded last week. All your support and suggestions keeps me going on so keep them coming.\n\nAs it is said - data science is not only about model fitting and great accuracy scores through tuning, it's more about what goes on behind the curtains. Data preprocessing is the backstage manager which makes your model work best when you've tuned it to the best parameters. \n\nSo, here is my first take on data preprocessing where I deal with various methods of filling missing values in your data. Usually, this is the first step you should take towards data preprocessing and hence it's my first step too to explain the various underlying methods you can take to fill missing values. I have tried to cover all the steps as per my knowledge and I hope to improvise it with advanced techniques in future versions. \n\nComment below for suggestions on any other methods you know of to help me and others alike. \nDo upvote if you find my effort worth it.","6f348615":"**4.1.1 Deletion**","910d3c29":"The isnull function returns a dataframe that shows True for a missing value. This type of analysis is not good for huge datasets. But, with little tweaks in the code we can create a table that shows us the total number of missing values and also the percentage of missing values.","4a86b0c6":"By using the columns or features that doesn\u2019t have missing values, we can predict the null values in other columns using Machine Learning Algorithms. In this case we divide our data into 2 sets. One that doesn't have any missing value and the other with the missing value. The former becomes our training data whereas the latter becomes the testing data where the variable with missing data is our target variable. \n\nOne drawback of this method is that if there is no relationship between attributes in the data set and the attribute with missing values, then our model will not be precise for estimating missing values.","b6b81992":"In this notebook, I have tried to demonstrate numerous ways of handling missing data. We need to keep experimenting and draw insights from our data to know which method will give best results. ","1e95315f":"# 6. Conclusion","83fee325":"The difference between data found in many tutorials and data in the real world is that real-world data is rarely clean and homogeneous. In particular, many interesting datasets will have some amount of data missing. To make matters even more complicated, different data sources may indicate missing data in different ways. \n\n**Why do we need to treat missing data?**\n\nMissing data in the training data set can reduce the power \/ fit of a model or can lead to a biased model because we have not analysed the behavior and relationship with other variables correctly. It can lead to wrong prediction or classification.\n\n\n**Why does the data have missing values?**\n\n* Missing Completely At Random (MCAR) - The reason for missingness is totally independent of the predictors and response i.e., the probability of missingness is the same for each unit in your sample.\n\n* Missing At Random (MAR) - Missingness depends only on other available information (e.g., other predictors). For example, we are collecting data for age and female has higher missing value compare to male.\n\n* Missing Not At Random (MNAR): depends on unobserved predictors -  This is a case when the missing values are not random and are related to the unobserved input variable. For example: In a medical study, if a particular diagnostic causes discomfort, then there is higher chance of drop out from the study. Here \"discomfort\" is not an input variable.\n\n* Missingn Not At Random II (MNAR): depends on the missing value itself - Missingness depends on the (potentially missing) variable itself. For example,  people with higher earnings are less likely to reveal them.","d8f85c13":"One way to handle missing values is to drop them. We can drop all rows with 'any' NAs in a particular column (should be done only when proportion of missing values is very small or else we can lose information). Below is an example of how to do it.","ca821007":"**4.1 NUMERICAL VALUES :-**"}}