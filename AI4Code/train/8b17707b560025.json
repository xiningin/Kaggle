{"cell_type":{"858a96b3":"code","14ef715f":"code","137415a3":"code","14d6dca0":"code","5684fc5e":"code","f0b3448f":"code","f778f8bb":"code","807c4c45":"code","f36b1ef5":"code","14dc7aa8":"code","2226036a":"code","01cf87c7":"code","fddcad98":"code","0ad9dc63":"code","cf31dcfb":"code","08731b81":"code","0e8c37f5":"code","aa2d05b4":"code","7e5ed8d4":"code","d9400871":"code","1c3d1d86":"markdown","1773840c":"markdown","00bc3aa7":"markdown","6d6375fa":"markdown","02fc62b6":"markdown","fa285b5c":"markdown","5496642d":"markdown","abe440cc":"markdown","fb93aa9e":"markdown","468faf5b":"markdown"},"source":{"858a96b3":"import numpy as np, pandas as pd, gc\nimport cv2, matplotlib.pyplot as plt\nimport cudf, cuml, cupy\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml.neighbors import NearestNeighbors\nimport tensorflow as tf\nfrom tensorflow.keras.applications import EfficientNetB0\nprint('RAPIDS',cuml.__version__)\nprint('TF',tf.__version__)","14ef715f":"# RESTRICT TENSORFLOW TO 1GB OF GPU RAM\n# SO THAT WE HAVE 15GB RAM FOR RAPIDS\nLIMIT = 1\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n  try:\n    tf.config.experimental.set_virtual_device_configuration(\n        gpus[0],\n        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024*LIMIT)])\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n    #print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n  except RuntimeError as e:\n    print(e)\nprint('We will restrict TensorFlow to max %iGB GPU RAM'%LIMIT)\nprint('then RAPIDS can use %iGB GPU RAM'%(16-LIMIT))","137415a3":"COMPUTE_CV = True\n\ntest = pd.read_csv('..\/input\/shopee-product-matching\/test.csv')\nif len(test)>3: COMPUTE_CV = False\nelse: print('this submission notebook will compute CV score, but commit notebook will not')","14d6dca0":"train = pd.read_csv('..\/input\/shopee-product-matching\/train.csv')\ntmp = train.groupby('label_group').posting_id.agg('unique').to_dict()\ntrain['target'] = train.label_group.map(tmp)\nprint('train shape is', train.shape )\ntrain.head()","5684fc5e":"tmp = train.groupby('image_phash').posting_id.agg('unique').to_dict()\ntrain['oof'] = train.image_phash.map(tmp)","f0b3448f":"def getMetric(col):\n    def f1score(row):\n        n = len( np.intersect1d(row.target,row[col]) )\n        return 2*n \/ (len(row.target)+len(row[col]))\n    return f1score","f778f8bb":"train['f1'] = train.apply(getMetric('oof'),axis=1)\nprint('CV score for baseline =',train.f1.mean())","807c4c45":"if COMPUTE_CV:\n    test = pd.read_csv('..\/input\/shopee-product-matching\/train.csv')\n    test_gf = cudf.DataFrame(test)\n    print('Using train as test to compute CV (since commit notebook). Shape is', test_gf.shape )\nelse:\n    test = pd.read_csv('..\/input\/shopee-product-matching\/test.csv')\n    test_gf = cudf.read_csv('..\/input\/shopee-product-matching\/test.csv')\n    print('Test shape is', test_gf.shape )\ntest_gf.head()","f36b1ef5":"class DataGenerator(tf.keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, df, img_size=256, batch_size=32, path=''): \n        self.df = df\n        self.img_size = img_size\n        self.batch_size = batch_size\n        self.path = path\n        self.indexes = np.arange( len(self.df) )\n        \n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        ct = len(self.df) \/\/ self.batch_size\n        ct += int(( (len(self.df)) % self.batch_size)!=0)\n        return ct\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        X = self.__data_generation(indexes)\n        return X\n            \n    def __data_generation(self, indexes):\n        'Generates data containing batch_size samples' \n        X = np.zeros((len(indexes),self.img_size,self.img_size,3),dtype='float32')\n        df = self.df.iloc[indexes]\n        for i,(index,row) in enumerate(df.iterrows()):\n            img = cv2.imread(self.path+row.image)\n            X[i,] = cv2.resize(img,(self.img_size,self.img_size)) #\/128.0 - 1.0\n        return X","14dc7aa8":"BASE = '..\/input\/shopee-product-matching\/test_images\/'\nif COMPUTE_CV: BASE = '..\/input\/shopee-product-matching\/train_images\/'\n\nWGT = '..\/input\/effnetb0\/efficientnetb0_notop.h5'\nmodel = EfficientNetB0(weights=WGT,include_top=False, pooling='avg', input_shape=None)\n\nembeds = []\nCHUNK = 1024*4\n\nprint('Computing image embeddings...')\nCTS = len(test)\/\/CHUNK\nif len(test)%CHUNK!=0: CTS += 1\nfor i,j in enumerate( range( CTS ) ):\n    \n    a = j*CHUNK\n    b = (j+1)*CHUNK\n    b = min(b,len(test))\n    print('chunk',a,'to',b)\n    \n    test_gen = DataGenerator(test.iloc[a:b], batch_size=32, path=BASE)\n    image_embeddings = model.predict(test_gen,verbose=1,use_multiprocessing=True, workers=4)\n    embeds.append(image_embeddings)\n\n    #if i>=1: break\n    \ndel model\n_ = gc.collect()\nimage_embeddings = np.concatenate(embeds)\nprint('image embeddings shape',image_embeddings.shape)","2226036a":"KNN = 50\nif len(test)==3: KNN = 2\nmodel = NearestNeighbors(n_neighbors=KNN)\nmodel.fit(image_embeddings)","01cf87c7":"preds = []\nCHUNK = 1024*4\n\nprint('Finding similar images...')\nCTS = len(image_embeddings)\/\/CHUNK\nif len(image_embeddings)%CHUNK!=0: CTS += 1\nfor j in range( CTS ):\n    \n    a = j*CHUNK\n    b = (j+1)*CHUNK\n    b = min(b,len(image_embeddings))\n    print('chunk',a,'to',b)\n    distances, indices = model.kneighbors(image_embeddings[a:b,])\n    \n    for k in range(b-a):\n        IDX = np.where(distances[k,]<6.0)[0]\n        IDS = indices[k,IDX]\n        o = test.iloc[IDS].posting_id.values\n        preds.append(o)\n        \ndel model, distances, indices, image_embeddings, embeds\n_ = gc.collect()","fddcad98":"test['preds2'] = preds\ntest.head()","0ad9dc63":"print('Computing text embeddings...')\nmodel = TfidfVectorizer(stop_words='english', binary=True, max_features=25_000)\ntext_embeddings = model.fit_transform(test_gf.title).toarray()\nprint('text embeddings shape',text_embeddings.shape)","cf31dcfb":"preds = []\nCHUNK = 1024*4\n\nprint('Finding similar titles...')\nCTS = len(test)\/\/CHUNK\nif len(test)%CHUNK!=0: CTS += 1\nfor j in range( CTS ):\n    \n    a = j*CHUNK\n    b = (j+1)*CHUNK\n    b = min(b,len(test))\n    print('chunk',a,'to',b)\n    \n    # COSINE SIMILARITY DISTANCE\n    cts = cupy.matmul( text_embeddings, text_embeddings[a:b].T).T\n    \n    for k in range(b-a):\n        IDX = cupy.where(cts[k,]>0.7)[0]\n        o = test.iloc[cupy.asnumpy(IDX)].posting_id.values\n        preds.append(o)\n        \ndel model, text_embeddings\n_ = gc.collect()","08731b81":"test['preds'] = preds\ntest.head()","0e8c37f5":"tmp = test.groupby('image_phash').posting_id.agg('unique').to_dict()\ntest['preds3'] = test.image_phash.map(tmp)\ntest.head()","aa2d05b4":"def combine_for_sub(row):\n    x = np.concatenate([row.preds,row.preds2, row.preds3])\n    return ' '.join( np.unique(x) )\n\ndef combine_for_cv(row):\n    x = np.concatenate([row.preds,row.preds2, row.preds3])\n    return np.unique(x)","7e5ed8d4":"if COMPUTE_CV:\n    tmp = test.groupby('label_group').posting_id.agg('unique').to_dict()\n    test['target'] = test.label_group.map(tmp)\n    test['oof'] = test.apply(combine_for_cv,axis=1)\n    test['f1'] = test.apply(getMetric('oof'),axis=1)\n    print('CV Score =', test.f1.mean() )\n\ntest['matches'] = test.apply(combine_for_sub,axis=1)","d9400871":"test[['posting_id','matches']].to_csv('submission.csv',index=False)\nsub = pd.read_csv('submission.csv')\nsub.head()","1c3d1d86":"# Use Phash Feature\nWe will predict all items with the same phash as duplicates","1773840c":"# Compute Baseline CV Score\nA baseline is to predict all items with the same `image_phash` as being duplicate. Let's calcuate the CV score for this submission.","00bc3aa7":"# Compute RAPIDS Model CV and Infer Submission\nWe will now use image embeddings, text embeddings, and phash to create a better model with better CV. We will also infer submission csv.\n\nNote how the variable `COMPUTE_CV` is only `True` when we **commit** this notebook. Right now you are reading a **commit** notebook, so we see test replaced with train and computed CV score. When we **submit** this notebook, the variable `COMPUTE_CV` will be `False` and the **submit** notebook will **not** compute CV. Instead it will load the real test dataset with 70,000 rows and find duplicates in the real test dataset.","6d6375fa":"# Compute CV Score\nThis simple model scores a high CV of 0.700+!","02fc62b6":"# Write Submission CSV\nIn this notebook, the submission file below looks funny containing train information. But when we submit this notebook, the size of `test.csv` dataframe will be longer than 3 rows and the variable `COMPUTE_CV` will subsequently set to `False`. Then our submission notebook will compute the correct matches using the real test dataset and our submission csv for LB will be ok.","fa285b5c":"# Use Image Embeddings\nTo prevent memory errors, we will compute image embeddings in chunks. And we will find similar images with RAPIDS cuML KNN in chunks.","5496642d":"# Load Train Data\nFirst we load the train data and create a target column of ground truths to help us compute CV score. Note how the variable `COMPUTE_CV` will change to `False` when we **submit** this notebook but it is `True` now because you are reading a **commit** notebook.","abe440cc":"# Use Text Embeddings\nTo prevent memory errors, we will find similar titles in chunks. To faciliate this, we will use cosine similarity between text embeddings instead of KNN.","fb93aa9e":"# Load Libraries","468faf5b":"# SUBMISSION with RAPIDS TfidfVectorizer\n![Screen%20Shot%202021-03-10%20at%203.46.44%20PM.png](attachment:Screen%20Shot%202021-03-10%20at%203.46.44%20PM.png)\n\nIn my first notebook [here][1], we do some EDA and explain our solution approach which is to use both images and text with RAPIDS to find similar items. This notebook optimizes GPU memory so submission notebook will be successful. We also calculate the CV score (if the notebook is committed but not when submitted). If you haven't read my first [notebook, read][1] that first. You can learn more about RAPIDS [here][2] and [here][3]\n\n**Note**: This **committed** notebook computes CV score but when we **submit** this notebook it does **not** compute CV. Instead it will load the 70,000 row `test.csv` file and compute matches in the test dataset. Because the variable `COMPUTE_CV = True` when we **commit** this notebook. But when we **submit** this notebook to Kaggle then the length of `test.csv` will be longer than 3 and the `if-statement` below will change to `COMPUTE_CV=False`. \n\n[1]: https:\/\/www.kaggle.com\/cdeotte\/rapids-cuml-tfidfvectorizer-and-knn\n[2]: https:\/\/rapids.ai\/start.html\n[3]: https:\/\/docs.rapids.ai\/api"}}