{"cell_type":{"df4c5086":"code","41d94e52":"code","fd116542":"code","ed4b0482":"code","8eb7d85b":"code","454da35e":"code","cbd048d8":"code","e909dd89":"code","f6125a2b":"code","31858f5e":"code","d02c76a7":"code","9d4f86be":"code","fe86df80":"code","50b2e879":"code","3d50acc5":"code","47940720":"code","38533253":"code","2dfbfc7a":"code","1d10128d":"code","27900ea5":"markdown","a0d9ca0f":"markdown","e482a2c8":"markdown","0a8ed1e0":"markdown","e042efc3":"markdown","034876a1":"markdown","83f36d8a":"markdown","c57fa798":"markdown","61f4851e":"markdown","955ce617":"markdown","8bdf7f66":"markdown","0e20c88c":"markdown","3942771d":"markdown","63330c12":"markdown"},"source":{"df4c5086":"\n## We will be importing the major libraries that we will need for our EDA\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\n\n\nfrom sklearn import metrics\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\n\ndata_df = pd.read_csv('..\/input\/Iris.csv')","41d94e52":"## We will first have a look at the top five records. Note that since the first 50 records belong to Iris-sentosa\n## so will will not be seeing any other records apart from those belonging to Iris-sentosa\ndata_df.head()","fd116542":"## Next we are interested in finding out a little more about the dataset like the data columns\/features,their data types \n## and also total number of values for each feature.\ndata_df.info()","ed4b0482":"## Next we will be exploring the count,mean,standard deviation,min\/max and the percentiles off the numeric values.\n## On observation it is found that the mean of the features lie within the 50th percentile.\n\ndata_df.describe()","8eb7d85b":"## Next we will have a look at our label column\n## using the function value_counts(), we will see the total unique count of records\n## So we see that there are 50 labels for Iris-setosa, 50 fir Iris-versicolor and 50 for Iris-virginica\n\ndata_df['Species'].value_counts()","454da35e":"data = data_df.drop('Id', axis = 1)\n## axis = 1 means, drop column\n## axis = 0 means, drop labels","cbd048d8":"data.corr(method = 'pearson')","e909dd89":"correlation = data.corr(method = 'pearson')\nheat_map = sns.heatmap(correlation,annot = True, cmap = 'coolwarm', linewidth = .5)\nplt.show()","f6125a2b":"## Also we will be demonstrating the pair-wise feature correlation using pairplot from the seaborn library\n## We are giving the parameter hue = 'Species' so that for each species, the colour marker will be different.\n\nsns.pairplot(data, hue = 'Species')","31858f5e":"g = sns.violinplot(y='Species', x='SepalLengthCm', data=data, inner='quartile')\nplt.show()\ng = sns.violinplot(y='Species', x='SepalWidthCm', data=data, inner='quartile')\nplt.show()\ng = sns.violinplot(y='Species', x='PetalLengthCm', data=data, inner='quartile')\nplt.show()\ng = sns.violinplot(y='Species', x='PetalWidthCm', data=data, inner='quartile')\nplt.show()","d02c76a7":"## In X, we are storing all the features while in y we are storing the lables\nX = data.drop(['Species'], axis=1)\ny = data['Species']\n\n## 50% training date, 50% test data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=5)\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","9d4f86be":"# experimenting with different n values\nk_range = list(range(1,51))\nscores = []\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train, y_train)\n    y_pred = knn.predict(X_test)\n    scores.append(metrics.accuracy_score(y_test, y_pred))\n\nplt.plot(k_range, scores)\nplt.xlabel('Value of k for KNN')\nplt.ylabel('Accuracy Score')\nplt.title('Accuracy Scores for Values of k of k-Nearest-Neighbors')\nplt.grid(True)\nplt.show()\n","fe86df80":"## K Nearest Neighbour\nknn = KNeighborsClassifier(n_neighbors=20)\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\nmetrics.accuracy_score(y_test, y_pred)","50b2e879":"## Logistic Regression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\nmetrics.accuracy_score(y_test, y_pred)","3d50acc5":"## Decision Tree\ntree = DecisionTreeClassifier(max_depth = 3)\ntree.fit(X_train,y_train)\ny_pred = tree.predict(X_test)\nmetrics.accuracy_score(y_test,y_pred)","47940720":"## Now lets split the train and test data into 80% training data and 20% testing data \n## And run our tests again\n\nX_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.3, random_state=5)\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(X_test.shape)","38533253":"## K Nearest Neighbour\nknn = KNeighborsClassifier(n_neighbors=20)\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\nmetrics.accuracy_score(y_test, y_pred)","2dfbfc7a":"## Logistic Regression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\nmetrics.accuracy_score(y_test, y_pred)","1d10128d":"## Decision Tree\ntree = DecisionTreeClassifier(max_depth = 3)\ntree.fit(X_train,y_train)\ny_pred = tree.predict(X_test)\nmetrics.accuracy_score(y_test,y_pred)","27900ea5":"Next we will create a heatmap to represent the above correlation in form of a graph.\nTo draw a heatmap we need to import the seaborn package.\nTHis is a good website to refer: https:\/\/www.absentdata.com\/python-graphs\/create-a-heat-map-with-seaborn\/","a0d9ca0f":"## Split the dataset into a training set and a testing set\n\n### Advantages\n- By splitting the dataset pseudo-randomly into a two separate sets, we can train using one set and test using another.\n- This ensures that we won't use the same observations in both sets.\n- More flexible and faster than creating a model using all of the dataset for training.\n\n### Disadvantages\n- The accuracy scores for the testing set can vary depending on what observations are in the set. \n- This disadvantage can be countered using k-fold cross-validation.\n\n### Notes\n- The accuracy score of the models depends on the observations in the testing set, which is determined by the seed of the pseudo-random number generator (random_state parameter).\n- As a model's complexity increases, the training accuracy (accuracy you get when you train and test the model on the same data) increases.\n- If a model is too complex or not complex enough, the testing accuracy is lower.\n- For KNN models, the value of k determines the level of complexity. A lower value of k means that the model is more complex.","e482a2c8":"Using the various data model packages avialable in Scikit-Learn, we will train our Iris dataset to find the best \nmodel that has the highest accuracy.","0a8ed1e0":"##### We will be investigating the value of k for which the accuracy of prediction is the highest.Since the training set has 90 rows, we can have a maximum value of k = 90. However here we will test till k = 50","e042efc3":"## Choosing KNN to Model Iris Species Prediction with k = 20\nAfter seeing that a value of k = 20 is a pretty good number of neighbors for this model, I used it to fit the model for the entire dataset instead of just the training set.","034876a1":"# Preview of Data\n\nThe Iris dataset is a cleaned dataset. By cleaned I mean there are no Null values or any garbage data.\n\n- There are 150 observations with 4 features each (sepal length, sepal width, petal length, petal width).\n- There are no null values, so we don't have to worry about that.\n- There are 50 observations of each species (setosa, versicolor, virginica).","83f36d8a":"# Data Analysis And Visualization\n\nNext we will try to find out if there is any correlation between the  four attributes SepalLengthCm,SepalWidthCm,PetalLengthCm and PetalWidthCm. We will be implementing the pearson method for this purpose.\n\nWhat we find is that SepalLengthCm has strong positive correlation with PetalLengthCm and PetalWidthCm.\nAlso PetalWidthCm has strong positive correlation with SepalLengthCm and PetalLengthCm.","c57fa798":"So from the graph we can see that iris-sentosa is distinctly different from the other two groups namely iris-versicolor and iris-virginica. Iris-sentosa can be very well classified based on PetalLengthCm and PetalWidthCm. There is a small amount of overlap between iris-versicolor and iris-virginica.","61f4851e":"Next we will be exploring the range of values for SepalLengthCm,SepalWidthCm ,PetalLengthCm, PetalWidthCm.\nWe will be plotting one graph for each.\nWe will be using violinplot from seaborn to visually represent them.","955ce617":"# Data Cleansing\n\nSince the Iris data set is already a clean dataset we do not need to spend any time for data cleansing activity.\nHowever while executing the data.info() command , we found that there is a column called 'id' which does not add any value \nto our analysis activity. Hence we will be dropping that column from our dataset.","8bdf7f66":"If you found this notebook useful, give me a upvote. Thanks!!","0e20c88c":"#  Background of the very popular Iris Dataset\n\nThe Iris data set also known as 'Fisher's Iris data' is one of the most popular multivariate dataset.\nThe data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). \n\nFour features were measured from each sample: the length and the width of the sepals and petals, in centimeters. Based on the combination of these four features, Fisher developed a linear discriminant model to distinguish the species from each other.","3942771d":"# Modeling with scikit-learn","63330c12":"# Re-Doing All the tests once again after splitting the data into 70% training and 30% testing set"}}