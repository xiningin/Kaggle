{"cell_type":{"b2d7849f":"code","2e1f27aa":"code","7407a78a":"code","fc0bcf8d":"code","21146fdb":"code","b6787f3d":"code","b8691980":"code","c678f569":"code","74c4d0d6":"code","60f5a37b":"code","7842f6c1":"code","7995227f":"code","0b0db38b":"code","dcf76baf":"code","1b807b70":"code","75a46025":"code","f8921d09":"code","b57582c3":"code","d92135f4":"code","ba620b66":"code","1fc0fafa":"code","47d2428e":"code","df9dd419":"code","b567fc10":"code","b48a6b95":"code","5130c1d4":"code","e7065cab":"code","0672f85d":"code","dac61c5e":"code","fdbe4f11":"code","dd0bf3c5":"code","2c6ec1b1":"code","3d434902":"code","6c800a80":"code","eb0e97dc":"code","e6ca7a16":"code","6c724bb2":"code","d6f586ca":"code","1d95cbc2":"code","86ad3493":"code","eec853e7":"code","1f871c34":"code","06514b09":"code","6164fbc8":"code","fed25058":"code","5c99a9db":"code","b675a655":"code","14250967":"code","25da9c4c":"markdown","27151d3b":"markdown","38de6ff9":"markdown","debf8ade":"markdown","b75d3b2e":"markdown","7a58f441":"markdown","41610400":"markdown","add698cc":"markdown","e853ea9a":"markdown","ebd04d6d":"markdown","6ab4d7d0":"markdown","768af0a6":"markdown","3241df8d":"markdown","b077a4c5":"markdown","cddaf905":"markdown","41fb5924":"markdown","9ef55a32":"markdown","8e9c950a":"markdown","b5f2862a":"markdown","91a41d9a":"markdown","652cc161":"markdown","27e029f4":"markdown","b63a456c":"markdown","b904ccaf":"markdown","56d5ca5b":"markdown","54140c54":"markdown","0d464cf4":"markdown","aa5d61e0":"markdown","f360e091":"markdown","750e543a":"markdown","73cf590a":"markdown","50ea07c0":"markdown","bad83f4f":"markdown","da90db76":"markdown","706157bf":"markdown","f794c567":"markdown","0fb5b41c":"markdown","8ccee13c":"markdown"},"source":{"b2d7849f":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sb\n%matplotlib inline","2e1f27aa":"titanic_train = pd.read_csv('..\/input\/titanic\/train.csv')\ntitanic_train.head()","7407a78a":"titanic_train.describe()","fc0bcf8d":"titanic_train.info()","21146fdb":"titanic_train.Age.value_counts()","b6787f3d":"titanic_train.Age.isna().sum()","b8691980":"## Since 'Age' is continuous, we will plot the frequencies a, nd check ether we can fill them using either the \n## 'median' or the 'mean' age\n\nplt.figure(figsize = [7, 5])\n\nplt.hist(titanic_train.Age, bins = 20);\nplt.axvline(x = titanic_train.Age.mean(), color = 'red', ls = '--', lw = 2)\nplt.axvline(x = titanic_train.Age.median(), color = 'orange', ls = '-.', lw = 2)\nplt.xlabel('Age')\nplt.ylabel('Count')\nplt.show()\n\nprint('\\nMedian Age: ', titanic_train.Age.median())\nprint('Mean Age: ', titanic_train.Age.mean())","c678f569":"titanic_train.Age.fillna(titanic_train.Age.mean(), inplace = True)\n\ntitanic_train.Age.isna().sum()","74c4d0d6":"titanic_train.info()","60f5a37b":"titanic_train.Embarked.value_counts()","7842f6c1":"titanic_train.Embarked.isna().sum()","7995227f":"titanic_train.Embarked.fillna(method = 'ffill', inplace = True)\n\ntitanic_train.Embarked.isna().sum()","0b0db38b":"titanic_train.info()","dcf76baf":"titanic_train.Cabin.value_counts()","1b807b70":"titanic_train.Pclass.value_counts()","75a46025":"titanic_train.Cabin.isna().sum()","f8921d09":"titanic_train.Cabin.fillna(titanic_train.Cabin.mode()[0], inplace = True)\n\ntitanic_train.Cabin.isna().sum()","b57582c3":"titanic_train.info()","d92135f4":"sb.countplot(titanic_train.Survived)\n\n\ntitanic_train.Survived.value_counts()","ba620b66":"titanic_train.head()","1fc0fafa":"plt.figure(figsize = [7, 5])\n\nplt.hist(titanic_train.Age, bins = 20);\nplt.axvline(x = titanic_train.Age.mean(), color = 'red', ls = '--', lw = 2)\nplt.axvline(x = titanic_train.Age.median(), color = 'orange', ls = '-.', lw = 2)\nplt.xlabel('Age')\nplt.ylabel('Count')\nplt.show()\n\nprint('\\nMedian Age: ', titanic_train.Age.median())\nprint('Mean Age: ', titanic_train.Age.mean())","47d2428e":"data_df = titanic_train.drop(['PassengerId', 'Ticket', 'Cabin', 'Name'], axis = 1)\ndata_df.head()","df9dd419":"data_df.Sex.value_counts()","b567fc10":"data_df.Parch.value_counts()","b48a6b95":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ndata_df['Embarked'] = le.fit_transform(data_df['Embarked'])\ndata_df['Sex'] = le.fit_transform(data_df['Sex'])\n\ndata_df.head()","5130c1d4":"for i in data_df.columns[1:]:\n    sb.boxplot(data_df[i]);\n    plt.show()","e7065cab":"## Capping the `Age` outliers\n\nlower = data_df['Age'].quantile(0.25) - 1.5 * (data_df['Age'].quantile(0.75) - data_df['Age'].quantile(0.25))\nupper = data_df['Age'].quantile(0.75) + 1.5 * (data_df['Age'].quantile(0.75) - data_df['Age'].quantile(0.25))\n\ndata_df['Age'] = np.where(data_df['Age'] > upper, upper, np.where(data_df['Age'] < lower, lower, data_df['Age']))\n\nsb.boxplot(data_df['Age']);\nplt.show()","0672f85d":"## Check the distribution of `Fare`\n\nplt.hist(data_df['Fare'], bins = 20)\nplt.axvline(x = data_df.Fare.mean(), color = 'red', ls = '--', lw = 2)\nplt.axvline(x = data_df.Fare.median(), color = 'orange', ls = '-.', lw = 2)\nplt.xlabel('Fare')\nplt.ylabel('Count')\nplt.show()","dac61c5e":"data_df['Fare'] = np.log(data_df['Fare'] + 0.1)\n\nplt.hist(data_df['Fare'], bins = 20)\nplt.xlabel('Log(Fare)')\nplt.ylabel('Count')\nplt.show()\n\nsb.boxplot(data_df['Fare']);","fdbe4f11":"## Now cap the outliers for `Fare`\n\nlower = data_df['Fare'].quantile(0.25) - 1.5 * (data_df['Fare'].quantile(0.75) - data_df['Fare'].quantile(0.25))\nupper = data_df['Fare'].quantile(0.75) + 1.5 * (data_df['Fare'].quantile(0.75) - data_df['Fare'].quantile(0.25))\n\ndata_df['Fare'] = np.where(data_df['Fare'] > upper, upper, np.where(data_df['Fare'] < lower, \n                                                                    lower, data_df['Fare']))\n\nsb.boxplot(data_df['Fare']);\nplt.show()","dd0bf3c5":"data_df.head()","2c6ec1b1":"data_df.corr()","3d434902":"## Plot the Heatmap along with the annotated correlations\n\nplt.figure(figsize = [9, 7])\nsb.heatmap(data_df.corr(), cmap = 'viridis_r', annot = True);\nplt.show()","6c800a80":"## Plot a pairplot to visualize the relationships among the attributes based on the Target variable\n\nsb.pairplot(data = data_df, hue = 'Survived')","eb0e97dc":"from sklearn.model_selection import train_test_split\n\nx_train, x_val, Y_train, y_val = train_test_split(data_df.iloc[:, 1:].values, data_df.iloc[:, 0].values, \n                                                  test_size = 0.3, random_state = 123)","e6ca7a16":"x_train.shape, Y_train.shape, x_val.shape, y_val.shape","6c724bb2":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nsc_x_train = scaler.fit_transform(x_train)\nsc_x_val = scaler.transform(x_val)\n\nsc_x_train, x_train, sc_x_val","d6f586ca":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report\n\nlr_model = LogisticRegression(solver = 'newton-cg', penalty = 'l2', C = 0.1, random_state = 123)\n\nY_pred = lr_model.fit(sc_x_train, Y_train).predict(sc_x_val)\n\nprint('The Accuracy score for the LogisticRegression model: ', accuracy_score(y_val, Y_pred))\nprint('The corresponding F1-score: ', f1_score(y_val, Y_pred))\nprint('\\n\\n')\nprint('The corresponding Classification Report: \\n', classification_report(y_val, Y_pred))","1d95cbc2":"from sklearn.tree import DecisionTreeClassifier\n\ndt_model = DecisionTreeClassifier(criterion = 'entropy', max_depth = 3, min_samples_leaf = 3, \n                                  min_samples_split = 2, random_state = 123)\n\nY_pred = dt_model.fit(sc_x_train, Y_train).predict(sc_x_val)\n\nprint('The Accuracy score for the DecisionTree model: ', accuracy_score(y_val, Y_pred))\nprint('The corresponding F1-score: ', f1_score(y_val, Y_pred))\nprint('\\n\\n')\nprint('The corresponding Classification Report: \\n', classification_report(y_val, Y_pred))","86ad3493":"from sklearn.ensemble import RandomForestClassifier\n\nrf_model = RandomForestClassifier(criterion = 'entropy', max_depth = None, min_samples_leaf = 3, \n                                  min_samples_split = 2, n_estimators = 10, n_jobs = 1, random_state = 123)\n\nY_pred = rf_model.fit(sc_x_train, Y_train).predict(sc_x_val)\n\nprint('The Accuracy score for the RandomForest model: ', accuracy_score(y_val, Y_pred))\nprint('The corresponding F1-score: ', f1_score(y_val, Y_pred))\nprint('\\n\\n')\nprint('The corresponding Classification Report: \\n', classification_report(y_val, Y_pred))","eec853e7":"from sklearn.ensemble import AdaBoostClassifier\n\nab_model = AdaBoostClassifier(learning_rate = 0.1, n_estimators = 500, random_state = 123)\n\nY_pred = ab_model.fit(sc_x_train, Y_train).predict(sc_x_val)\n\nprint('The Accuracy score for the Adaboost model: ', accuracy_score(y_val, Y_pred))\nprint('The corresponding F1-score: ', f1_score(y_val, Y_pred))\nprint('\\n\\n')\nprint('The corresponding Classification Report: \\n', classification_report(y_val, Y_pred))","1f871c34":"import xgboost\nfrom xgboost.sklearn import XGBClassifier\n\nxg_model = XGBClassifier(booster = 'gbtree', learning_rate = 0.01, max_depth = 3, n_estimators = 200, n_jobs = 1,\n                         random_state = 123)\n\nY_pred = xg_model.fit(sc_x_train, Y_train).predict(sc_x_val)\n\nprint('The Accuracy score for the XGBoost model: ', accuracy_score(y_val, Y_pred))\nprint('The corresponding F1-score: ', f1_score(y_val, Y_pred))\nprint('\\n\\n')\nprint('The corresponding Classification Report: \\n', classification_report(y_val, Y_pred))","06514b09":"from sklearn.svm import SVC\n\nsvc_model = SVC(C = 1, gamma = 0.1, random_state = 123)\n\nY_pred = svc_model.fit(sc_x_train, Y_train).predict(sc_x_val)\n\nprint('The Accuracy score for the SVM model: ', accuracy_score(y_val, Y_pred))\nprint('The corresponding F1-score: ', f1_score(y_val, Y_pred))\nprint('\\n\\n')\nprint('The corresponding Classification Report: \\n', classification_report(y_val, Y_pred))","6164fbc8":"test_set = pd.read_csv('..\/input\/titanic\/test.csv')\ntest_set.head()","fed25058":"## Perform the same pre-processing steps on the `test_set`\n\npassenger_id = test_set['PassengerId'].values\n\n\n## Drop the unrequired attributes\n\ntest_set.drop(['PassengerId', 'Ticket', 'Cabin', 'Name'], axis = 1, inplace = True)\n\n\n## Fill in the NaN values\n\ntest_set.Age.fillna(test_set.Age.mean(), inplace = True)\ntest_set.Fare.fillna(test_set.Fare.mode()[0], inplace = True)\n\n\n## Label Encode the 'Sex' & 'Embarked' attributes\n\ntest_set['Sex'] = le.fit_transform(test_set['Sex'])\ntest_set['Embarked'] = le.fit_transform(test_set['Embarked'])\n\n\n## Capping the `Age` outliers\n\nlower = test_set['Age'].quantile(0.25) - 1.5 * (test_set['Age'].quantile(0.75) - test_set['Age'].quantile(0.25))\nupper = test_set['Age'].quantile(0.75) + 1.5 * (test_set['Age'].quantile(0.75) - test_set['Age'].quantile(0.25))\n\ntest_set['Age'] = np.where(test_set['Age'] > upper, upper, \n                           np.where(test_set['Age'] < lower, lower, test_set['Age']))\n\n\n## Log transformation of 'Fare' and capping the outliers\n\ntest_set['Fare'] = np.log(test_set['Fare'] + 0.1)\n\nlower = test_set['Fare'].quantile(0.25) - 1.5 * (test_set['Fare'].quantile(0.75) - test_set['Fare'].quantile(0.25))\nupper = test_set['Fare'].quantile(0.75) + 1.5 * (test_set['Fare'].quantile(0.75) - test_set['Fare'].quantile(0.25))\n\ntest_set['Fare'] = np.where(test_set['Fare'] > upper, upper, \n                            np.where(test_set['Fare'] < lower, lower, test_set['Fare']))\n\n\ntest_set.head()","5c99a9db":"## Assign values to `x_test`\n\nx_test = test_set.iloc[:, :].values\n\nsc_x_test = scaler.transform(x_test)\n\nsc_x_test, x_test","b675a655":"## Predict using the DT model created earlier\n\nY_pred_test = dt_model.fit(sc_x_train, Y_train).predict(sc_x_test)\nY_pred_test","14250967":"import csv\n\nFinal_csv = pd.DataFrame({'PassengerId': passenger_id, 'Survived': Y_pred_test})\nFinal_csv.to_csv(\"Titanic_Survival_Analysis.csv\", index = False)\nFinal_csv.head(10)","25da9c4c":"#### Check for the class imbalance of the Target Variable","27151d3b":"#### Normalize the data to bring them all on the same scale","38de6ff9":"### Explore the data","debf8ade":"### Building the Models\n\nUse GridSearch to figure out the best hyperparameters to use for highest accuracy\n\n - **Grid Search** has already been performed in an earlier notebook, this notebook only contains the models built from the best parameters returned from the Grid Search. It takes much time, so we have avoided it here.","b75d3b2e":"Since the mean & the median Age is quite near to each other, we can use any of them as the fill method to fill the missing entries....","7a58f441":"#### Check for the dataset description, info, presence of NaN, and work them out:","41610400":"#### Variable Notes","add698cc":"### EDA:","e853ea9a":"### Necessary Imports and Data Loading:","ebd04d6d":"The sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nUsing this notebook, we will try to model the survival predictions for its passengers based on their passenger data (ie name, age, gender, socio-economic class, etc).","6ab4d7d0":"#### Decision Trees","768af0a6":"##### Cabin::","3241df8d":"Since all the attributes in `data_df` is numerical, we can directly check through them","b077a4c5":"We can have a look at the data and intuitively try to drop out some insignificant variables and would later check if our intuition was right:\n\n - **PassengerId:** Does not seem to matter here; each and every detail is contained in the other variables.\n - **Ticket:** We already have `Fare` which we are going to consider in model building. This is just another representation.\n - **Cabin:** Since we are going by the intuition that most survivors could be based on `Pclass` on top of `Sex` attributes, and also `Cabin` allotments are done by the basis of `Pclass`, we can just use `Pclass` and this attribute is rather redundant to it, so we can drop the same.\n - **Name:** For obvious reasons....\n ","cddaf905":"#### Data Dictionary:","41fb5924":"#### Drop out insignificant columns","9ef55a32":"Looking at the above models, it seems that `Decision Trees`, `XGBoost` and `SVM` produced the best Accuracy scores. But looking at the F1-score, it seems `Decision Trees` slightly take the upper hand.\n\nSo for this dataset, we will use `Decision Tree` as our model with the referred hyperparameters.","8e9c950a":"##### Age::","b5f2862a":"##### Embarked::","91a41d9a":"Since the data is sorted through `PassengerId`, intuitively we can say that consecutive passengers would embark from the same port, hence, a forward filling for the missing data could be the best way forward. ","652cc161":"#### Split the data into training & validation sets","27e029f4":"Not much of a class imbalance as we can see here, so we do not need to employ sampling methods.","b63a456c":"#### Random Forest","b904ccaf":"#### Check the correlation among  the attributes, and that with the Target variable","56d5ca5b":"## Survival Prediction?","54140c54":"#### Checking for outliers for numerical attributes","0d464cf4":"|   Variable  | \tDefinition  |\tKey   |\n| ----------- | --------------- | ------- |\n|   survival  | \tSurvival    |\t0 = No, 1 = Yes   |\n|   pclass\t  |     Ticket class |\t1 = 1st, 2 = 2nd, 3 = 3rd   |\n|   sex       |  \tSex\t        |       | \n|   Age       | \tAge in years |\t    |\n|   sibsp     |  \t# of siblings \/ spouses aboard the Titanic   |    |\t\n|   parch     | \t# of parents \/ children aboard the Titanic\t |    |\n|   ticket    | \tTicket number     |      |\t\n|   fare      |\t    Passenger fare    |      |\t\n|   cabin     | \tCabin number\t  |      |\n|   embarked  | \tPort of Embarkation |   C = Cherbourg, Q = Queenstown, S = Southampton     |","aa5d61e0":"#### Adaboost","f360e091":"`Age` & `Fare` attributes contain most of the outliers, so lets cap it in accordance to their upper and lower bound whiskers.","750e543a":"**pclass:** A proxy for socio-economic status (SES)\n - 1st = Upper\n - 2nd = Middle\n - 3rd = Lower\n\n**age:** Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\n**sibsp:** The dataset defines family relations in this way...\n - Sibling = brother, sister, stepbrother, stepsister\n - Spouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\n**parch:** The dataset defines family relations in this way...\n - Parent = mother, father\n - Child = daughter, son, stepdaughter, stepson\n - Some children travelled only with a nanny, therefore parch=0 for them.","73cf590a":"#### XGBoost","50ea07c0":"`Cabin` contains the most of the missing data. Intuitively, since most of the passengers aboard belong to the lower class, we can assume the most allotted cabins to fill these missing values. `mode` of the set....","bad83f4f":"#### Logistic Regression","da90db76":"We see the `Embarked` variable, which contains the Port in 'char' format; we would do best to encode them to numerical variables.","706157bf":"The data has been split into two groups:\n\n - training set (train.csv)\n - test set (test.csv)\n\nThe training set should be used to build the models. We van further divide the Training set into training & validation sets to try and check for the accuracies.","f794c567":"#### Support Vector","0fb5b41c":"##### From the above info  it is clear that `Age`, `Cabin` and `Embarked` have missing entries. We will see and fill them individually:","8ccee13c":"As seen above, `Fare` has a highly skewed distribution -- so a log transformation of that data is recommended."}}