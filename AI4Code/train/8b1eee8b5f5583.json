{"cell_type":{"e4fee473":"code","c995d6bc":"code","d944bbf1":"code","3d041663":"code","c76ebbc9":"code","2fd14a0a":"code","fae9b4cd":"code","0013a5d7":"code","f11b55e9":"code","41e24dbe":"code","dafda892":"code","b76adbe8":"code","8417308a":"markdown","e4ca7ff6":"markdown","3521ee75":"markdown","03e7b90a":"markdown","1187f919":"markdown"},"source":{"e4fee473":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","c995d6bc":"t140 = pd.read_csv('\/kaggle\/input\/sentiment140\/training.1600000.processed.noemoticon.csv',\n                   sep=',',\n                   header=None,\n                   encoding='latin')\n\nlabel_text = t140[[0, 5]]\n\n# Convert labels to range 0-1                                        \nlabel_text[0] = label_text[0].apply(lambda x: 0 if x == 0 else 1)\n\n# Assign proper column names to labels\nlabel_text.columns = ['label', 'text']\n\n# Assign proper column names to labels\nlabel_text.head()","d944bbf1":"import re\n\nhashtags = re.compile(r\"^#\\S+|\\s#\\S+\")\nmentions = re.compile(r\"^@\\S+|\\s@\\S+\")\nurls = re.compile(r\"https?:\/\/\\S+\")\n\ndef process_text(text):\n  text = hashtags.sub(' hashtag', text)\n  text = mentions.sub(' entity', text)\n  return text.strip().lower()\n  \ndef match_expr(pattern, string):\n  return not pattern.search(string) == None\n\ndef get_data_wo_urls(dataset):\n    link_with_urls = dataset.text.apply(lambda x: match_expr(urls, x))\n    return dataset[[not e for e in link_with_urls]]","3d041663":"label_text.text = label_text.text.apply(process_text)","c76ebbc9":"from sklearn.model_selection import train_test_split\nTRAIN_SIZE = 0.75\nVAL_SIZE = 0.05\ndataset_count = len(label_text)\n\ndf_train_val, df_test = train_test_split(label_text, test_size=1-TRAIN_SIZE-VAL_SIZE, random_state=42)\ndf_train, df_val = train_test_split(df_train_val, test_size=VAL_SIZE \/ (VAL_SIZE + TRAIN_SIZE), random_state=42)\n\nprint(\"TRAIN size:\", len(df_train))\nprint(\"VAL size:\", len(df_val))\nprint(\"TEST size:\", len(df_test))","2fd14a0a":"df_train = get_data_wo_urls(df_train)\ndf_train.head()","fae9b4cd":"!mkdir dataset\ndf_train.sample(frac=1.0).reset_index(drop=True).to_csv('dataset\/train.tsv', sep='\\t', index=None, header=None)\ndf_val.to_csv('dataset\/dev.tsv', sep='\\t', index=None, header=None)\ndf_test.to_csv('dataset\/test.tsv', sep='\\t', index=None, header=None)\n! cd dataset && ls","0013a5d7":"#from BertLibrary import BertFTModel\n#import numpy as np","f11b55e9":"# !wget https:\/\/storage.googleapis.com\/bert_models\/2018_10_18\/uncased_L-12_H-768_A-12.zip\n# !unzip uncased_L-12_H-768_A-12.zip","41e24dbe":"# !mkdir output\n# ft_model = BertFTModel( model_dir='uncased_L-12_H-768_A-12',\n#                         ckpt_name=\"bert_model.ckpt\",\n#                         labels=['0','1'],\n#                         lr=1e-05,\n#                         num_train_steps=30000,\n#                         num_warmup_steps=1000,\n#                         ckpt_output_dir='output',\n#                         save_check_steps=1000,\n#                         do_lower_case=False,\n#                         max_seq_len=50,\n#                         batch_size=32,\n#                         )\n\n\n# ft_trainer =  ft_model.get_trainer()\n# ft_evaluator = ft_model.get_evaluator()","dafda892":"# ft_trainer.train_from_file('dataset', 35000)","b76adbe8":"# ft_evaluator.evaluate_from_file('dataset', checkpoint=\"output\/model.ckpt-35000\") ","8417308a":"Then we preprocess the text replacing hashtags and mentions with custom tokens, ensuring that the model won't overfit on them","e4ca7ff6":"## Model Training\nWe cannot run this on Kaggle if we want to share this kernel as a notebook\n","3521ee75":"# Twitter Sentiment Analysis with Bert\nIn this notebook we show how to finetune BERT to Sentiment140 dataset to do sentiment classification on twitter.\nWe first prepare the data removing urls from the training set and replacing hashtag and mentions with custom tokens.\nThen we fit Bert on the dataset and present the results. \n\nBecause of kaggle's kernel limitations, we move the computation on Colab Gpu environment which you can follow here\n\n","03e7b90a":"We split the dataset and remove tweets with urls because we do not want them to change the meaning of the text, leading the model to learn wrong patterns. Then we store the datasets, because BERT need to load them from disk","1187f919":"We load the dataset and select only the columns for text and label"}}