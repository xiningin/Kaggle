{"cell_type":{"f244c8d6":"code","17738005":"code","2507ce3e":"code","b79ce915":"code","c7ba3cad":"code","1484ec02":"code","19698483":"code","f1a1f799":"code","60520bbb":"code","578aae6c":"code","a736bb43":"code","07f48d1b":"code","f2cfe524":"code","fa6bccf7":"code","5a70b595":"code","53352be7":"code","073941e2":"code","24b999eb":"code","77920d92":"code","1a097a5c":"code","9f575fa6":"code","fd740564":"code","b0435cd4":"code","bd57f792":"code","2c2d82bd":"code","f32a5bd0":"code","775d48cb":"code","362f0c0d":"code","3c03bcd6":"code","863bc024":"code","00d8d213":"code","5b386c0c":"code","31362bb8":"code","cf53dec1":"code","59b36676":"code","6c349152":"code","90d2f1aa":"code","bc9389fa":"markdown","c3a776c6":"markdown","6be54908":"markdown","bc76d897":"markdown","e1472239":"markdown","0cd2bf3a":"markdown","f067c5a3":"markdown","e43a8f6e":"markdown","121673d8":"markdown","8fb6da20":"markdown","ad45b0e6":"markdown","aa99db17":"markdown","9b65578f":"markdown","113d717e":"markdown"},"source":{"f244c8d6":"import numpy as np\nimport pandas as pd \nimport json\nimport math\n\n# for data visualization\nimport seaborn as sns\nimport plotly.express as px \nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\nfrom sklearn import preprocessing\nfrom sklearn import model_selection\nfrom sklearn import linear_model\nfrom sklearn import svm\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import ensemble\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","17738005":"tracks = pd.read_csv('\/kaggle\/input\/spotify-dataset-19212020-160k-tracks\/tracks.csv')\nartists = pd.read_csv('\/kaggle\/input\/spotify-dataset-19212020-160k-tracks\/artists.csv')\ngenres = pd.read_csv('\/kaggle\/input\/spotify-dataset-19212020-160k-tracks\/data_by_genres_o.csv')\nyears = pd.read_csv('\/kaggle\/input\/spotify-dataset-19212020-160k-tracks\/data_by_year_o.csv')","2507ce3e":"with open('\/kaggle\/input\/spotify-dataset-19212020-160k-tracks\/dict_artists.json') as f:\n    artists_related = json.load(f)","b79ce915":"tracks.info()","c7ba3cad":"artists.info()","1484ec02":"genres.info()","19698483":"years.info()","f1a1f799":"tracks.describe() ","60520bbb":"fig = px.histogram(tracks, x=\"popularity\", nbins=40, title=\"Histogram of Tracks' popularity\")\nfig.show()","578aae6c":"tracks[\"release_year\"] = pd.to_datetime(tracks[\"release_date\"]).dt.year\ntracks.columns","a736bb43":"# Removing Redundant variables\ntracks_ml = tracks[['popularity', 'duration_ms', 'explicit','release_year', 'danceability', 'energy',\n                   'key', 'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness',\n                   'liveness', 'valence', 'tempo', 'time_signature']]","07f48d1b":"tracks_2020_ml = tracks_ml.query(\"release_year == 2020\").drop([\"release_year\"], axis=1)","f2cfe524":"sns.set(rc={'figure.figsize':(22,14)})\nsns.heatmap(tracks_2020_ml.corr(), linewidths=.5, annot=True, cmap=\"YlGnBu\",\n           mask=np.triu(np.ones_like(tracks_2020_ml.corr(), dtype=np.bool)))\\\n    .set_title(\"Correlations Heatmap between Audio Features, Based on 2020's Tracks\")","fa6bccf7":"cols = tracks_2020_ml.columns\nfor col in cols:\n    fig = px.histogram(tracks_2020_ml, x=col, title=\"Histogram of Tracks' \" + col + \", 2020\")\n    fig.show()","5a70b595":"X, y = tracks_2020_ml[cols[1:]], tracks_2020_ml[cols[0]]\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.33, random_state=0)","53352be7":"# define models\nmodels = {\n    \"OLS\": linear_model.LinearRegression(),\n    \"Ridge\": linear_model.Ridge(),\n    \"Lasso\": linear_model.Lasso(),\n    \"Bayesian\": linear_model.BayesianRidge(), \n    \"SVM\": svm.SVR(),\n    \"RandomForestReg\": ensemble.RandomForestRegressor(),\n    \"AdaBoostReg\": ensemble.AdaBoostRegressor(),\n    \"GradientBoostingReg\": ensemble.GradientBoostingRegressor()\n}","073941e2":"# cross validate to fit and score\ncv_results = pd.DataFrame(columns=['model', 'train_score', 'test_score'])\nfor key in models.keys():\n    cv_res = cross_validate(models[key], X_train, y_train, \n                             return_train_score=True,\n                             scoring='neg_root_mean_squared_error',\n                             cv=5, n_jobs=-1)\n    res = {\n        'model': key, \n        'train_score': cv_res[\"train_score\"].mean(), \n        'test_score': cv_res[\"test_score\"].mean(),\n        'fit_time': cv_res[\"fit_time\"].mean(),\n        'score_time': cv_res[\"score_time\"].mean(),\n        }\n    cv_results = cv_results.append(res, ignore_index=True)\n    print(\"CV for model:\", key, \"done.\")\n    \n# show and compare results\ncv_results[\"train_score\"] = cv_results[\"train_score\"].apply(abs)\ncv_results[\"test_score\"] = cv_results[\"test_score\"].apply(abs)\nsorted_cv_results = cv_results.sort_values(['train_score', 'test_score'])\nsorted_cv_results","24b999eb":"best_model_key = sorted_cv_results.head(1).model.values[0]\nprint(\"best_model_choosed:\", best_model_key)\nbest_model = models[best_model_key].fit(X_train, y_train)\nprint('root_mean_squared_error:', np.sqrt(mean_squared_error(y_test, best_model.predict(X_test))))","77920d92":"test_set = y_test.reset_index()\ntest_set[\"predicted\"] = best_model.predict(X_test)\ntest_set[\"abs_error\"] = abs(test_set[\"predicted\"] - test_set[\"popularity\"])\ntest_set = test_set.sort_values(\"abs_error\", ascending=False)\ntest_set","1a097a5c":"test_set.describe()","9f575fa6":"test_set.abs_error.plot(kind='hist', title=\"Testing Set Prediction Error Distribution\", figsize=(10,6));","fd740564":"print(\"abs_error over 50 prediction percentage:\", end=\" \")\nprint(test_set.query(\"abs_error > 50\").abs_error.count() \/ test_set.abs_error.count())","b0435cd4":"test_set.predicted.plot(kind='hist', title=\"Testing Set Prediction Distribution\", figsize=(10,6));","bd57f792":"test_set.popularity.plot(kind='hist', title=\"Testing Set True Value Distribution\", figsize=(10,6));","2c2d82bd":"print(\"nagetive prediction percentage:\", end=\" \")\nprint(test_set.query(\"predicted < 0\").predicted.count() \/ test_set.predicted.count())","f32a5bd0":"tracks_2020_ml_n = tracks_2020_ml.query(\"popularity > 20\")\nX_n, y_n = tracks_2020_ml_n[cols[1:]], tracks_2020_ml_n[cols[0]]\nX_n_train, X_n_test, y_n_train, y_n_test = model_selection.train_test_split(X_n, y_n, test_size=0.33, random_state=1)","775d48cb":"# cross validate to fit and score\ncv_results = pd.DataFrame(columns=['model', 'train_score', 'test_score'])\nfor key in models.keys():\n    cv_res = cross_validate(models[key], X_n_train, y_n_train, \n                             return_train_score=True,\n                             scoring='neg_root_mean_squared_error',\n                             cv=5, n_jobs=-1)\n    res = {\n        'model': key, \n        'train_score': cv_res[\"train_score\"].mean(), \n        'test_score': cv_res[\"test_score\"].mean(),\n        'fit_time': cv_res[\"fit_time\"].mean(),\n        'score_time': cv_res[\"score_time\"].mean(),\n        }\n    cv_results = cv_results.append(res, ignore_index=True)\n    print(\"CV for model:\", key, \"done.\")\n    \n# show and compare results\ncv_results[\"train_score\"] = cv_results[\"train_score\"].apply(abs)\ncv_results[\"test_score\"] = cv_results[\"test_score\"].apply(abs)\nsorted_cv_results = cv_results.sort_values(['train_score', 'test_score'])\nsorted_cv_results","362f0c0d":"best_model_key = sorted_cv_results.head(1).model.values[0]\nprint(\"best_model_choosed:\", best_model_key)\nn_best_model_rf = models[best_model_key].fit(X_n_train, y_n_train)\nprint('root_mean_squared_error:', np.sqrt(mean_squared_error(y_n_test, n_best_model_rf.predict(X_n_test))))","3c03bcd6":"print(\"model_choosed: GradientBoostingReg\")\nn_best_model_gb = models[\"GradientBoostingReg\"].fit(X_n_train, y_n_train)\nprint('root_mean_squared_error:', np.sqrt(mean_squared_error(y_n_test, n_best_model_gb.predict(X_n_test))))","863bc024":"test_set_n = y_n_test.reset_index()\ntest_set_n[\"predicted\"] = n_best_model_gb.predict(X_n_test)\ntest_set_n[\"abs_error\"] = abs(test_set_n[\"predicted\"] - test_set_n[\"popularity\"])\ntest_set_n = test_set_n.sort_values(\"abs_error\", ascending=False)\ntest_set_n","00d8d213":"pd.concat([test_set.describe().add_suffix('_o'), test_set_n.describe().add_suffix('_n')], axis=1)","5b386c0c":"# # Create the parameter grid based on the results of random search \n# param_grid_rf = {\n#     'bootstrap': [True],\n#     'max_depth': range(50, 101, 50),\n#     'max_features': range(5, 10),\n#     'min_samples_leaf': range(5, 10),\n# #     'min_samples_split': range(2, 7),\n#     'n_estimators':  range(10, 101, 20)\n# }\n\n# # Instantiate the grid search model\n# grid_search_rf = model_selection.GridSearchCV(estimator = models[\"RandomForestReg\"], param_grid = param_grid_rf, \n#                               cv = 2, n_jobs = -1, scoring='neg_root_mean_squared_error')\n\n# # Fit the grid search to the data\n# grid_search_rf.fit(X_n_train, y_n_train)\n# print(\"best_params:\", grid_search_rf.best_params_)","31362bb8":"# best_grid_rf = grid_search_rf.best_estimator_\n# print('root_mean_squared_error: ', np.sqrt(mean_squared_error(y_n_test, best_grid_rf.predict(X_n_test))))","cf53dec1":"# # Create the parameter grid based on the results of random search \n# param_grid_gb = {\n#     \"loss\": [\"ls\", \"lad\", \"huber\", \"quantile\"],\n#     \"learning_rate\": [0.01, 0.05, 0.1, 0.15, 0.2],\n#     \"n_estimators\": range(10, 201, 50),\n# #     \"min_samples_leaf\": range(5, 10),\n# #     \"max_depth\": range(50, 101, 10),\n#     \"max_features\": range(5, 10)\n# }\n\n# # Instantiate the grid search model\n# grid_search_gb = model_selection.GridSearchCV(estimator = models[\"GradientBoostingReg\"], param_grid = param_grid_gb, \n#                               cv = 2, n_jobs = -1, scoring='neg_root_mean_squared_error')\n\n# # Fit the grid search to the data\n# grid_search_gb.fit(X_n_train, y_n_train)\n# print(\"best_params:\", grid_search_gb.best_params_)","59b36676":"# best_grid_gb = grid_search_gb.best_estimator_\n# print('root_mean_squared_error: ', np.sqrt(mean_squared_error(y_n_test, best_grid_gb.predict(X_n_test))))","6c349152":"tracks_2021_ml = tracks_ml.query(\"release_year == 2021 & popularity > 20\").drop([\"release_year\"], axis=1)\nX_2021, y_2021 = tracks_2021_ml[cols[1:]], tracks_2021_ml[cols[0]]\nprint('root_mean_squared_error on 2021 songs:', np.sqrt(mean_squared_error(y_2021, n_best_model_gb.predict(X_2021))))","90d2f1aa":"tracks_2019_ml = tracks_ml.query(\"release_year == 2019 & popularity > 20\").drop([\"release_year\"], axis=1)\nX_2019, y_2019 = tracks_2019_ml[cols[1:]], tracks_2019_ml[cols[0]]\nprint('root_mean_squared_error on 2019 songs:', np.sqrt(mean_squared_error(y_2019, n_best_model_gb.predict(X_2019))))","bc9389fa":"Thus, 2020 Model have the similar prediction error on 2019 and 2021 data set.","c3a776c6":"## Modeling","6be54908":"After removing outliers, the max abs_error is reduced from 69 to 43, and the mean abs_error is reduced from 12.87 to 9.34.  \n","bc76d897":"## Error Analysis One More Time","e1472239":"## Evaluate NEW Best Model on Testing Set Again\nMade a tradeoff the fit time and train\/test score of Random Forest and Gradient Boosting regressor, Gradient Boosting regressor is not over fitting and saved more time, so this section will evaluate Gradient Boosting regressor on testing set.","0cd2bf3a":"## Improved Modeling\nSince one of the assumptions of the regression is normally distributed, remove outliers which's popularity <= 20 and re-modeling.","f067c5a3":"## Can 2020 Model Predict 2019 and 2021 Songs' Popularity?","e43a8f6e":"## Model Error Analysis","121673d8":"## EDA and Pre-Processing","8fb6da20":"## Hyperparameter Tuning","ad45b0e6":"**Analysis:**    \nIn the prediction, only 1.26% of testing data have abs_root_mean_squared_error > 50, most of the error is bewteen 0 to 20.   \nThe mean abs_root_mean_squared_error is 12.897846 and the median is 9.95.    \nIn the dataset, part (popularity over 20) of the popularity true value is normally distributed, but another part (popularity under 20) is not. Since one of the assumptions of the regression is normally distributed, the predicted value is normally distributed. This is one of the causes of the error.","aa99db17":"## Evaluate  Best Model on Testing Set\nConsidering the difference between the train and test score in the cross validatation on training set, the RandomForestReg model is a little over fitting. However, this model gives the best test score, so we use it as our best model and evaluate it on testing set.","9b65578f":"**Analysis:**     \n1. From the histogram of popularity, most of song and rated in 0 to 1 range, and the range over 20 shows a normal distribution.      \n2. The distribution of duration_ms, danceability, energy, loudness, liveness and valence are normal.    \n3. The most of songs' time_signature is 4.            \n4. The distribution of speechiness, acousticness and instrumentalness are right-skewed.","113d717e":"**Analysis:**     \n1. Acousticness is highly negative correlated to energy and loudness.      \n2. energy and loudness are highly positve correlated to each other.\n3. Instrumentalness is highly negative correlated to loudness. \n4. Danceability and valence have a highly positve correlation.\n5. Popularity is highly positve correlated to explicit and danceability, and it is highly negative correlated to instrumentalness."}}