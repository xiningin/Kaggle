{"cell_type":{"75b01561":"code","2abe933a":"code","68a0218f":"code","86b22f51":"code","67cb9308":"code","eaaaeb0d":"code","acbcde49":"code","10c92131":"code","2f6cb595":"code","d3574d11":"code","f989a2bc":"code","bc258aac":"code","6669a9a4":"code","ca038ad6":"code","7e5ce35e":"code","0e023deb":"code","9ca4ee8b":"code","9d1e1e17":"code","c232ef26":"code","9ffd34ac":"code","8737fb58":"code","09cd90c1":"code","84ab8415":"code","57e69fc4":"code","b15f5e82":"code","03e150c3":"code","bc8b5ebc":"code","bf47e87a":"code","2b213efd":"code","92f2f2d9":"code","8f66c04d":"code","66517e76":"code","3997c031":"code","b8a6d910":"code","7ac2ec33":"code","56b337e4":"code","7f9421b0":"code","4d3107ab":"code","51bb9127":"code","a6f3ee79":"code","1b8ec746":"code","760d65e8":"code","a7c26d18":"code","6631dbac":"code","37ec99fd":"code","6c884d37":"code","4713ebc3":"code","d712ac6c":"code","ab3e2e4f":"code","5e1c0974":"code","56ba7df2":"code","59de52b3":"code","0818573b":"code","05c6a3da":"code","e07e46fa":"code","08f762ff":"code","e1bb4c24":"code","2a407d8b":"code","f898b7e5":"code","68b15227":"code","455fd4d9":"code","4c95c0d0":"code","6e663981":"code","37f28e6c":"code","b530c867":"code","956f3560":"code","bb59f0b9":"code","35260440":"code","9396cc88":"code","20404dab":"code","77272c15":"code","2e34a938":"code","12a209ab":"code","37f2f765":"code","8007d324":"code","51d1d368":"code","8f1c944a":"markdown","ce196b23":"markdown","2cb9ce6b":"markdown","198fafd6":"markdown","4fdc02e3":"markdown","4d9f79bb":"markdown","0355cebf":"markdown","63b1ff6f":"markdown","7d65b36f":"markdown","be8c982c":"markdown","5797635f":"markdown","78743556":"markdown","be46be9e":"markdown","a9a1403a":"markdown","6994ed6c":"markdown","44df531d":"markdown","5492eb4a":"markdown","98a933f9":"markdown","d0b6b20f":"markdown","47e55f29":"markdown","1b582008":"markdown","0cb8309c":"markdown"},"source":{"75b01561":"# Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime as dt\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_log_error, mean_absolute_error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer","2abe933a":"# Load training data\ndata_train = pd.read_csv(\"..\/input\/restaurant-revenue-prediction\/train.csv.zip\")","68a0218f":"data_train.head()","86b22f51":"data_train.info()","67cb9308":"# Check for null values\ndata_train.isna().sum()","eaaaeb0d":"# Check all the cities and provinces we are dealing with\ndata_train.City.unique()","acbcde49":"# The number of provinces and cities in our data\nlen(data_train.City.unique())","10c92131":"# Types of cities we are dealing with\ndata_train['City Group'].unique()","2f6cb595":"bigCities = len(data_train[data_train['City Group'] == \"Big Cities\"])\notherCount = len(data_train[data_train['City Group'] == \"Other\"])\ndic_1 = {\"Big Cities\": bigCities, \"Other\": otherCount}\n\nfig, ax = plt.subplots(figsize=(5, 5))\nax.bar(dic_1.keys(), \n       dic_1.values(), \n       width=0.8, \n       color=['skyblue', 'orange'])\nax.set(xlabel= \"City Group\", \n       ylabel='Count',\n       title='Training Examples of the City Groups');\ndata_train['City Group'].value_counts()","d3574d11":"data_train['Type'].value_counts()","f989a2bc":"fc = len(data_train[data_train['Type'] == \"FC\"])\nil = len(data_train[data_train['Type'] == \"IL\"])\ndt = len(data_train[data_train['Type'] == \"DT\"])\ndic_2 = {'Food Court': fc ,\"Inline\": il , \"Drive Thru\": dt}\n\nfig, ax = plt.subplots(figsize=(8, 5))\nax.bar(dic_2.keys(), \n       dic_2.values(), \n       width=0.8, \n       color=['darkorange', 'bisque', 'moccasin'])\nax.set(xlabel='Type of Restaurant', \n       ylabel='Count',\n       title='Training Examples of the Types of Restaurants');","bc258aac":"data_train['Open Date'].dtype","6669a9a4":"# Convert the Open Date column to the datetime data type\ndata_train['Open Date'] = pd.to_datetime(data_train['Open Date'])","ca038ad6":"data_train['Open Date'].dtype","7e5ce35e":"# Sort the values by year in ascending order\ndata_train.sort_values(by=['Open Date'], inplace=True, ascending=True, ignore_index=True)","0e023deb":"data_train = data_train.drop('Id', axis=1)","9ca4ee8b":"data_train.head()","9d1e1e17":"# Add seperate columns for the Open date values\ndata_train['Sale Day'] = data_train['Open Date'].dt.day\ndata_train['Sale Year'] = data_train['Open Date'].dt.year\ndata_train['Sale Month'] = data_train['Open Date'].dt.month","c232ef26":"data_train.head()","9ffd34ac":"data_train['Sale Year'].value_counts()","8737fb58":"# Store categorical variable names in a list\nctg_vars = []\n\nfor col in data_train:\n    if len(data_train[col].unique()) <= 30:\n        ctg_vars.append(col)","09cd90c1":"# Remove the P variables from categorical variables' list\ni = 1\nfor k in range(1, 43):\n    for p in ctg_vars:\n        if p == \"P\" + str(i):\n            ctg_vars.remove(\"P\" + str(i))\n            i += 1","84ab8415":"print(ctg_vars)","57e69fc4":"len(ctg_vars)","b15f5e82":"#Plot histograms for all the P columns and the revenue column\nhist_cols = list(data_train.columns[4:42])\ndata_train[hist_cols].hist(figsize= (12,60), layout=(19,2), bins=15);","03e150c3":"sns.distplot(data_train['revenue']);","bc8b5ebc":"fig, ax = plt.subplots(figsize=(10,5))\nax.scatter(data_train['Open Date'], data_train['revenue'])\nax.set(ylabel=\"Revenue \/ 10^-7\",\n       xlabel='Year',\n       title='Annual Restaurant Revenue');","bf47e87a":"# Median Revenue of big cities and other cities\nax_wp_1 = sns.boxplot(x='revenue', y='City Group', data=data_train)\nax_wp_1.set(title='Whisker plot');\n\nbc_median = data_train[data_train['City Group'] == 'Big Cities']['revenue'].median()\noc_median = data_train[data_train['City Group'] == 'Other']['revenue'].median()\nprint(\"Median Revenue of Big cities:\", bc_median)\nprint(\"Median Revenue of Other cities:\", oc_median)","2b213efd":"data_train['revenue'].max()","92f2f2d9":"# Median revenue for the types of restaurants\nrt_median = data_train.groupby('Type')['revenue'].aggregate(np.median)\nprint(\"Median Revenue of the types of restaurants per annum: \\n\", rt_median[1:])","8f66c04d":"data_train[data_train['Type'] == 'FC']['revenue'].cumsum().plot()\ndata_train[data_train['Type'] == 'IL']['revenue'].cumsum().plot()\nplt.ylabel('Cumulative Sum of Revenue')\nplt.xlabel('Number of examples')\nplt.legend(['Food Court', 'Inline'])\nplt.title('Cumulative Revenue Graph');","66517e76":"# Type of restaurant with the most revenue\ndata_train[data_train['revenue'] == data_train['revenue'].max()]['Type']","3997c031":"plt.figure(figsize=(45,25))\nsns.heatmap(data_train.corr(),annot=True)\nsns.set(font_scale=1.4)","b8a6d910":"# P variables will be considered as continous variables rather than categorical variables\nimp_train = IterativeImputer(max_iter=30, random_state=0, missing_values=0, sample_posterior = True, min_value=1)\np_vals = [\"P\" + str(i) for i in range(1, 38)]\ndata_train[p_vals] = np.round(imp_train.fit_transform(data_train[p_vals]))","7ac2ec33":"data_temp = data_train.copy()","56b337e4":"data_temp.drop('Open Date', axis=1, inplace=True)\ndata_temp.drop('City', axis=1, inplace=True)","7f9421b0":"data_temp['revenue'] = np.log1p(data_temp['revenue'])","4d3107ab":"data_temp.to_csv('train_data_modified.csv', index=False)","51bb9127":"# Load the temp data\ndata = pd.read_csv('train_data_modified.csv')","a6f3ee79":"data = pd.get_dummies(data, columns=ctg_vars)","1b8ec746":"# Add new columns to our dataset to match our input features\ndata['Sale Year_1995'] = pd.DataFrame(np.zeros((137, 1)), dtype='uint8')\ndata['Sale Year_2001'] = pd.DataFrame(np.zeros((137, 1)), dtype='uint8')\ndata['Sale Year_2003'] = pd.DataFrame(np.zeros((137, 1)), dtype='uint8')\ndata['Sale Day_19'] = pd.DataFrame(np.zeros((137, 1)), dtype='uint8')","760d65e8":"X = data.drop('revenue', axis=1)\ny = data['revenue']","a7c26d18":"# Split data\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, random_state=0)","6631dbac":"# Create Random Forest Regressor model\nmodel = RandomForestRegressor(n_estimators=1000 ,random_state=0)\nmodel.fit(X_train, y_train)","37ec99fd":"# Evaluation Function\n\ndef rmse(y_test, y_preds):\n    return np.sqrt(mean_squared_error(y_test, y_preds))\n\ndef show_scores(model):\n    train_preds = model.predict(X_train)\n    val_preds = model.predict(X_valid)\n    scores = {\"Training MAE\": mean_absolute_error(y_train, train_preds),\n              \"Validating MAE\": mean_absolute_error(y_valid, val_preds),\n              \"Training RMSE\": rmse(y_train, train_preds),\n              \"Validating RMSE\": rmse(y_valid, val_preds),\n              \"Training R^2\": model.score(X_train, y_train),\n              \"Validating R^2\": model.score(X_valid, y_valid)}\n    return scores","6c884d37":"show_scores(model)","4713ebc3":"# Model 2 - CatBoost\nfrom catboost import CatBoostRegressor\nmodel_2 = CatBoostRegressor(verbose=False)\nmodel_2.fit(X_train, y_train);","d712ac6c":"cat_pred = model_2.predict(X_valid)","ab3e2e4f":"show_scores(model_2)","5e1c0974":"# Number of trees\ntrees = np.arange(100, 1000, 100)\n\nfor i in trees:\n    print(\"Number of Trees: {}\".format(i))\n    rf_test_model = RandomForestRegressor(n_estimators=i, random_state=0, criterion='mae')\n    rf_test_model.fit(X_train, y_train)\n    train_preds = rf_test_model.predict(X_train)\n    val_preds = rf_test_model.predict(X_valid)\n    print('RMSE for training set: {}'.format(rmse(y_train, train_preds)))\n    print('RMSE for validation set: {} \\n'.format(rmse(y_valid, val_preds)))","56ba7df2":"# Parameter dictionary for GridSearch\nrf_grid = {'n_estimators': [200, 600, 800],\n           'criterion': ['mse', 'mae'],\n           'max_features': [0.33, 0.5, 'auto', 'sqrt'],       \n           }","59de52b3":"rf_gs = GridSearchCV(estimator = RandomForestRegressor(),\n                     param_grid = rf_grid,\n                     cv = 5,\n                     verbose = True)\n\nrf_gs.fit(X_train, y_train)","0818573b":"rf_gs.score(X_valid, y_valid)","05c6a3da":"rf_gs.score(X_train, y_train)","e07e46fa":"rf_gs.best_params_","08f762ff":"rf_gs.best_params_['n_estimators']","e1bb4c24":"rf_test_model = RandomForestRegressor(n_estimators=rf_gs.best_params_['n_estimators'], random_state=0, \n                                      criterion=rf_gs.best_params_['criterion'], max_features = rf_gs.best_params_['max_features'])\nrf_test_model.fit(X_train, y_train)\ntrain_preds = rf_test_model.predict(X_train)\nval_preds = rf_test_model.predict(X_valid)\nprint('RMSE for training set: {}'.format(rmse(y_train, train_preds)))\nprint('RMSE for validation set: {}'.format(rmse(y_valid, val_preds)))","2a407d8b":"data_test = pd.read_csv(\"..\/input\/restaurant-revenue-prediction\/test.csv.zip\")","f898b7e5":"data_test.head()","68b15227":"data_test.isna().sum()","455fd4d9":"len(data_test.City.unique())","4c95c0d0":"data_test['Type'].unique()","6e663981":"data_test['Open Date'] = pd.to_datetime(data_test['Open Date'])\ndata_test.sort_values(by=['Open Date'], inplace=True, ascending=True, ignore_index=True)","37f28e6c":"data_test['Open Date'].dtype","b530c867":"data_test['Sale Day'] = data_test['Open Date'].dt.day\ndata_test['Sale Year'] = data_test['Open Date'].dt.year\ndata_test['Sale Month'] = data_test['Open Date'].dt.month","956f3560":"data_test.drop('Open Date', axis=1, inplace=True)\ndata_test.drop('City', axis=1, inplace=True)","bb59f0b9":"ctg_vars_test = []\n\nfor col in data_test:\n    if len(data_test[col].unique()) <= 31:\n        ctg_vars_test.append(col)","35260440":"# Remove the P variables\ni = 1\nfor k in range(1, 43):\n    for p in ctg_vars_test:\n        if p == \"P\" + str(i):\n            ctg_vars_test.remove(\"P\" + str(i))\n            i += 1","9396cc88":"print(ctg_vars_test)","20404dab":"data_temp_test = data_test.copy()","77272c15":"data_temp_test.loc[data_temp_test['Type'] == 'MB', 'Type'] = 'DT'","2e34a938":"imp_test = IterativeImputer(max_iter=30, random_state=0, missing_values=0, sample_posterior = True, min_value=1)\np_vals_test = [\"P\" + str(i) for i in range(1, 38)]\ndata_temp_test[p_vals_test] = np.round(imp_test.fit_transform(data_temp_test[p_vals_test]))","12a209ab":"data_temp_test = pd.get_dummies(data_temp_test, columns=ctg_vars_test)","37f2f765":"data_temp_test.to_csv('test_data_modified.csv', index=False)","8007d324":"test_data = pd.read_csv('test_data_modified.csv')","51d1d368":"submission = pd.DataFrame(columns=[\"Id\", \"Prediction\"])\nsubmission[\"Id\"] = test_data['Id']\n\n# Random Forest Model predictions\nrf_pred_sub = rf_test_model.predict(test_data.drop('Id', axis=1))\nsubmission['Prediction'] = np.expm1(rf_pred_sub)\nsubmission.to_csv('submission_random_forest.csv', index=False)\n\n# CatBoost Model predictions\ncb_pred_sub = model_2.predict(test_data.drop('Id', axis=1))\nsubmission['Prediction'] = np.expm1(cb_pred_sub)\nsubmission.to_csv('submission_cat_boost.csv', index=False)","8f1c944a":"### Save changes made to the test data in another file","ce196b23":"## Data Source\n\n* Kaggle: https:\/\/www.kaggle.com\/c\/restaurant-revenue-prediction\/overview","2cb9ce6b":"> **Regression models**:\n*  Random Forest\n* CatBoost ","198fafd6":"## Imputing Null P values","4fdc02e3":" These columns are missing from the training data which will become an issue when the input features from our test set will not match the input features from our training set\n- Sale Year_1995\n- Sale Year_2001\n- Sale Year_2003\n- Sale Day_19","4d9f79bb":"MB type of restaurants will be replaced with DT","0355cebf":"## Exploratory Data Analysis","63b1ff6f":"## Hyperparameter Tuning","7d65b36f":"### Making Predictions on the test set","be8c982c":"## Data Fields","5797635f":"## Modelling","78743556":"### About the company\n\n* TFI has over 1,200 quick service restaurants across the globe.\n* They employ over 20,000 people in Europe and Asia.\n* They make significant investments in their niche.\n* When the wrong location for a restaurant brand is chosen, the site closes within 18 months and operating losses are incurred.\n* Their goal is to increase effectiveness in their investments.","be46be9e":"# Restaurant Revenue Prediction","a9a1403a":"> You have the opening dates, cities, types of cities, types of restaurants and obfuscated data to predict the revenue","6994ed6c":"1. **Id**: Restaurant id. \n2. **Open Date**: opening date for a restaurant\n3. **City**: City that the restaurant is in. Note that there are unicode in the names. \n4. **City Group**: Type of the city. Big cities, or Other. \n5. **Type**: Type of the restaurant. FC: Food Court, IL: Inline, DT: Drive Thru, MB: Mobile\n6. **P1, P2 - P37**: There are three categories of these obfuscated data. Demographic data are gathered from third party providers with GIS systems. These include population in any given area, age and gender distribution, development scales. Real estate data mainly relate to the m2 of the location, front facade of the location, car park availability. Commercial data mainly include the existence of points of interest including schools, banks, other QSR operators.\n7. **Revenue**: The revenue column indicates a (transformed) revenue of the restaurant in a given year and is the target of predictive analysis. Please note that the values are transformed so they don't mean real dollar values. \n","44df531d":"## Test set","5492eb4a":"> We can observe that the revenue independent variable is rightly skewed","98a933f9":"## References\n1. https:\/\/towardsdatascience.com\/restaurant-revenue-prediction-467f0990403e\n2. https:\/\/towardsdatascience.com\/random-forest-hyperparameters-and-how-to-fine-tune-them-17aee785ee0d","d0b6b20f":"### Random Forest","47e55f29":"### Save changes made to the data in another file","1b582008":"> **Goal**: Using demographic, real estate, and commercial data, this competition challenges you to predict the annual restaurant sales of 100,000 regional locations.\n\n> **Evaluaton Metric**: RMSE(Root Mean Squared Error)","0cb8309c":"## Data Preparation "}}