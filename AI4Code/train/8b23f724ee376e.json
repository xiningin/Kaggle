{"cell_type":{"a3822086":"code","a4cf9f04":"code","a2d77fdd":"code","85772eeb":"code","c2816b2b":"code","3c9512e6":"code","d2757faf":"code","420e29d9":"code","7785cc9a":"code","68c9b64b":"code","430afc41":"code","cf709e01":"code","2d4871bb":"code","d5d6863e":"code","7958f98e":"code","657e2cb6":"code","45693254":"code","eee0f81b":"code","1951c442":"code","77ef6946":"code","99108447":"code","0b96553e":"code","53241606":"code","f13408e0":"code","3e1ac380":"code","5504dd33":"markdown","64b39659":"markdown","85559b37":"markdown","9e0ad368":"markdown","6d7a6b71":"markdown","66c3fd4d":"markdown","5c48983f":"markdown","16657305":"markdown","a2aa0743":"markdown","ea8398d9":"markdown","b2ea63ee":"markdown","b83a9288":"markdown","5fc2af25":"markdown","b2a5473a":"markdown","b638faf3":"markdown","056c78c6":"markdown","4449c6e6":"markdown","69f310cf":"markdown","087472f5":"markdown","1ef32663":"markdown","b6d583d8":"markdown","5a3fc2fc":"markdown","abbb027b":"markdown","1649e015":"markdown"},"source":{"a3822086":"import pandas as pd\nimport numpy as np\nimport string\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport plotly.tools as tls\nimport plotly.offline as py\nfrom plotly.offline import init_notebook_mode, iplot, plot\nimport plotly.graph_objs as go\ninit_notebook_mode(connected=True)\nimport warnings\nfrom collections import Counter\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,accuracy_score, roc_curve, auc\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\n\n# Load data\nprint('Use my extended dataset containing complexity analysis (NLP) of the NAME feature.')\n#df = pd.read_csv('..\/input\/ks-projects-201801.csv',encoding ='latin1')\ndf = pd.read_csv('..\/input\/kickstarter2018nlp\/ks-projects-201801-extra.csv',encoding ='latin1')\nprint('done')\n","a4cf9f04":"df.head(5)\ndf.describe()\nprint(df.shape)\nprint(df.info())\nprint(df.nunique())\nprint('done')","a2d77fdd":"percentual_success = round(df[\"state\"].value_counts() \/ len(df[\"state\"]) * 100,2)\n\nprint(\"State Percentual in %: \")\nprint(percentual_success)\n\nstate = round(df[\"state\"].value_counts() \/ len(df[\"state\"]) * 100,2)\n\nlabels = list(state.index)\nvalues = list(state.values)\n\ntrace1 = go.Pie(labels=labels, values=values, marker=dict(colors=['red']))\n\nlayout = go.Layout(title='Distribuition of States', legend=dict(orientation=\"h\"));\n\nfig = go.Figure(data=[trace1], layout=layout)\niplot(fig)\nprint('done')","85772eeb":"# Exploring the distribution logarithm of these values\ndf_failed = df[df[\"state\"].isin([\"failed\",\"canceled\",\"suspended\"])] #antonio\ndf_sucess = df[df[\"state\"] == \"successful\"]\n\n#First plot\ntrace0 = go.Histogram(\n    x= np.log(df.usd_goal_real + 1).head(100000),\n    histnorm='probability', showlegend=False,\n    xbins=dict(\n        start=-5.0,\n        end=19.0,\n        size=1),\n    autobiny=True)\n\n#Second plot\ntrace1 = go.Histogram(\n    x = np.log(df.usd_pledged_real + 1).head(100000),\n    histnorm='probability', showlegend=False,\n    xbins=dict(\n        start=-1.0,\n        end=17.0,\n        size=1))\n\n# Add histogram data\nx1 = np.log(df_failed['usd_goal_real']+1).head(100000)\nx2 = np.log(df_sucess[\"usd_goal_real\"]+1).head(100000)\n\ntrace3 = go.Histogram(\n    x=x1,\n    opacity=0.60, nbinsx=30, name='Goals Failed', histnorm='probability'\n)\ntrace4 = go.Histogram(\n    x=x2,\n    opacity=0.60, nbinsx=30, name='Goals Successful', histnorm='probability'\n)\n\n\ndata = [trace0, trace1, trace3, trace4]\nlayout = go.Layout(barmode='overlay')\n\n#Creating the grid\nfig = tls.make_subplots(rows=2, cols=2, specs=[ [{'colspan': 2}, None], [{}, {}]],\n                          #subplot_titles=('Failed and Successful Projects',\n                          subplot_titles=('Successful and Non-Successful Projects',\n                                          'Goal','Pledged'))\n\n#setting the figs\nfig.append_trace(trace0, 2, 1)\nfig.append_trace(trace1, 2, 2)\nfig.append_trace(trace3, 1, 1)\nfig.append_trace(trace4, 1, 1)\n\nfig['layout'].update(title=\"Distributions\",\n                     height=500, width=900, barmode='overlay')\niplot(fig)\nprint('done')","c2816b2b":"# group instances by state: failed\/successful\nmain_cats = df[\"main_category\"].value_counts()\nmain_cats_failed = df[df[\"state\"].isin([\"failed\",\"canceled\",\"suspended\"])][\"main_category\"].value_counts() #antonio\nmain_cats_sucess = df[df[\"state\"] == \"successful\"][\"main_category\"].value_counts()\n\n# plots\ntrace0 = go.Bar(\n    x=main_cats_failed.index,\n    y=main_cats_failed.values,\n    name=\"Failed\"\n)\n\ntrace1 = go.Bar(\n    x=main_cats_sucess.index,\n    y=main_cats_sucess.values,\n    name=\"Success\"\n)\n\ntrace2 = go.Bar(\n    x=main_cats.index,\n    y=main_cats.values,\n    name=\"Distribution\"\n)\n\n\nfig = tls.make_subplots(rows=2, cols=2, specs=[[{}, {}], [{'colspan': 2}, None]],\n                          subplot_titles=('Non-Successful','Successful', \"General Category's\"))\n\n#setting the figs\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig.append_trace(trace2, 2, 1)\n\nfig['layout'].update(showlegend=True, title=\"Main Category's Distribuition\",bargap=0.05)\niplot(fig)\n\ndf['main_category'].value_counts().plot.bar()\nplt.show()\n\ndf['currency'].value_counts().plot.bar()\nplt.show()\n\ndf['country'].value_counts().plot.bar()\nplt.show()\n\ndf['state'].value_counts().plot.bar()\nplt.show()\nprint('done')","3c9512e6":"df = df[df[\"state\"].isin([\"failed\",\"canceled\",\"suspended\",\"successful\"])].copy() \n\nprint(df.shape)\n\nprint(\"Delete columns 'ID', 'name', 'usd_pledged', 'usd_pledged_real'. name is string. usd_pledged only contains N\/A. usd_pledged_real is only set when the target variable is set.\")\ndf = df.drop('ID', 1)\ndf = df.drop('name', 1)\ndf = df.drop('usd pledged', 1)\ndf = df.drop('usd_pledged_real', 1)\ndf = df.drop('backers', 1)\n\nprint(df.shape)\n\n\nprint(\"Create new column 'duration_days' = 'deadline' - 'launched'\")\ndf['launched'] = pd.to_datetime(df['launched'])\ndf['deadline'] = pd.to_datetime(df['deadline'])\ndf['duration_days'] = df['deadline'].subtract(df['launched'])\ndf['duration_days'] = df['duration_days'].astype('timedelta64[D]')\n\nprint('drop columns: launched, deadline and pledged as they are only set once the state is final.')\ndf = df.drop('launched', 1)\ndf = df.drop('deadline', 1)\ndf = df.drop('pledged', 1)\n\ndf = df[(df['goal'] <= 100000) & (df['goal'] >= 1000)].copy()\ndf.shape\n\ndf['state'] = df['state'].map({\n        'failed': 0,\n        'canceled': 0, \n        'suspended':0, \n        'successful': 1         \n})\n\nprint('use one-hot-codding for category, main_category and currency')\ndf = pd.get_dummies(df, columns = ['category'])\ndf = pd.get_dummies(df, columns = ['main_category'])\ndf = pd.get_dummies(df, columns = ['currency'])\ndf = pd.get_dummies(df, columns=['country'])\n\nprint(\"Rename 'main_category_Film & Video' to 'main_category_Film' to avoid character encoding issues\")\ndf.rename(columns={\"main_category_Film & Video\": \"main_category_Film\"}, inplace=True)\nprint('done')","d2757faf":"print('drop target variable from train\/test datasets')\nprint(df.shape)\ndf.head()\ny = df['state']\nprint(y.shape)\ny.head(5)\ndf = df.drop('state', 1)\n\nprint('Split dataframe into random train and test subsets')\nX_train, X_test, Y_train, Y_test = train_test_split(\n    df,\n    y, \n    test_size = 0.1,\n    random_state = 42\n)\n\nprint('train data shape')\nprint(X_train.shape, Y_train.shape)\nprint('test data shape')\nprint(X_test.shape, Y_test.shape)\nprint('done')","420e29d9":"# LGBM\nclf_lgbm = LGBMClassifier(\n        n_estimators=300,\n        num_leaves=30,\n        colsample_bytree=.8,\n        subsample=.8,\n        max_depth=10,\n        reg_alpha=.1,\n        reg_lambda=.05,\n        min_split_gain=.005\n    )\n\nclf_lgbm.fit(X_train, \n        Y_train,\n        eval_set= [(X_train, Y_train), (X_test, Y_test)], \n        eval_metric='auc', \n        verbose=0, \n        early_stopping_rounds=30\n       )\n\nacc_clf_lgbm = round(clf_lgbm.score(X_test, Y_test) * 100, 2)\nacc_clf_lgbm","7785cc9a":"# catboost (70.8)\nfrom catboost import CatBoostClassifier\ncatmodel = CatBoostClassifier(\n    custom_loss=['Accuracy'],\n    random_seed=42,\n    logging_level='Silent'\n)\ncatmodel.fit(\n    X_train, Y_train,\n    eval_set=(X_test, Y_test),\n    plot=False\n);\ncatmodel = round(catmodel.score(X_test, Y_test) * 100, 2)\ncatmodel","68c9b64b":"# adaboost (69.92)\nbdt = AdaBoostClassifier(DecisionTreeClassifier(max_depth=8), #1\n                         algorithm=\"SAMME\",\n                         n_estimators=200)\nbdt.fit(X_train, Y_train)\nacc_bdt = round(bdt.score(X_test, Y_test) * 100, 2)\nacc_bdt","430afc41":"# Logistic Regression (64.7)\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nacc_log = round(logreg.score(X_test, Y_test) * 100, 2)\n\ncoeff_df = pd.DataFrame(df.columns.delete(0))\ncoeff_df.columns = ['Feature']\ncoeff_df[\"Correlation\"] = pd.Series(logreg.coef_[0])\ncoeff_df.sort_values(by='Correlation', ascending=False)\n\nacc_log","cf709e01":"# KNN (65.04)\nknn = KNeighborsClassifier()\nknn.fit(X_train, Y_train)\nacc_knn = round(knn.score(X_test, Y_test) * 100, 2)\nacc_knn","2d4871bb":"# Linear SVC\/SupportVectorMachine (66.11 failed to converge)\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\nacc_linear_svc = round(linear_svc.score(X_test, Y_test) * 100, 2)\nacc_linear_svc","d5d6863e":"# Decision Tree (66.02)\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nacc_decision_tree = round(decision_tree.score(X_test, Y_test) * 100, 2)\nacc_decision_tree","7958f98e":"# random forest (67.13)\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_test, Y_test) * 100, 2)\nacc_random_forest","657e2cb6":"# gradient boosting (67.16)\nclf_gb = GradientBoostingClassifier(n_estimators=100, \n                                 max_depth=1, \n                                 random_state=0)\nclf_gb.fit(X_train, Y_train)\nacc_clf_gb = round(clf_gb.score(X_test, Y_test) * 100, 2)\nacc_clf_gb","45693254":"# multi-layer perceptron (64.71)\nmlp = MLPClassifier(solver='lbfgs', \n                    alpha=1e-5, \n                    hidden_layer_sizes=(21, 2), \n                    random_state=1)\nmlp.fit(X_train, Y_train)\nacc_mlp = round(mlp.score(X_test, Y_test) * 100, 2)\nacc_mlp","eee0f81b":"# bagging classifier (63.7)\nbagging = BaggingClassifier(\n    KNeighborsClassifier(\n        n_neighbors=8,\n        weights='distance'\n        ),\n    oob_score=True,\n    max_samples=0.5,\n    max_features=1.0\n    )\nclf_bag = bagging.fit(X_train,Y_train)\nacc_clf_bag = round(clf_bag.score(X_test, Y_test) * 100, 2)\nacc_clf_bag","1951c442":"# Feature selection and PCA - Simple variance baseline approach\n\nfrom sklearn.feature_selection import VarianceThreshold, SelectKBest, chi2\n\nprint('there is a high number of variables in the dataset:')\nprint(df.shape)\n\n#print(\"feature selection: remove all features whose variance doesn\u2019t meet some threshold.\")\nsel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n\nprint(\"standardize the remaining features\")\ndfpca = pd.DataFrame(sel.fit_transform(df))\n\nfrom sklearn.decomposition import PCA\nprint('apply PCA, 5 components')\npca = PCA(n_components = 5) # arbitrary number\nprincipalComponents = pca.fit_transform(dfpca)\nprincipalDf = pd.DataFrame(data = principalComponents, columns = ['pc1','pc2','pc3','pc4','pc5'])\n\n# Split dataframe into random train and test subsets\nXpca_train, Xpca_test, Ypca_train, Ypca_test = train_test_split(\n    principalDf,\n    y, \n    test_size = 0.1,\n    random_state=42\n)\n\nprint('PCA reduces the number of variables to:')\nprint(Xpca_train.shape, Ypca_train.shape)\nprint(Xpca_test.shape, Ypca_test.shape)\nprint('done')","77ef6946":"# LGBM with pca \n\nclfpca_lgbm = LGBMClassifier(\n        n_estimators=300,\n        num_leaves=30,\n        colsample_bytree=.8,\n        subsample=.8,\n        max_depth=10,\n        reg_alpha=.1,\n        reg_lambda=.05,\n        min_split_gain=.005\n    )\n\nclfpca_lgbm.fit(Xpca_train, \n        Ypca_train,\n        eval_set= [(Xpca_train, Ypca_train), (Xpca_test, Ypca_test)], \n        eval_metric='auc', \n        verbose=0, \n        early_stopping_rounds=30\n       )\n\nclfpca_lgbm = round(clfpca_lgbm.score(Xpca_test, Ypca_test) * 100, 2)\nclfpca_lgbm","99108447":"# catboost with PCA\nfrom catboost import CatBoostClassifier\ncatmodelpca = CatBoostClassifier(\n    custom_loss=['Accuracy'],\n    random_seed=42,\n    logging_level='Silent'\n)\ncatmodelpca.fit(\n    Xpca_train, Ypca_train,\n    eval_set=(Xpca_test, Ypca_test),\n    plot=False\n);\ncatmodelpca = round(catmodelpca.score(Xpca_test, Ypca_test) * 100, 2)\ncatmodelpca","0b96553e":"# bagging classifier with PCA\nbaggingpca = BaggingClassifier(\n    KNeighborsClassifier(\n        n_neighbors=8,\n        weights='distance'\n        ),\n    oob_score=True,\n    max_samples=0.5,\n    max_features=1.0\n    )\nclf_bagpca = baggingpca.fit(Xpca_train,Ypca_train)\nacc_clf_bagpca = round(clf_bagpca.score(Xpca_test, Ypca_test) * 100, 2)\nacc_clf_bagpca","53241606":"print('LGBM: model selection (best hyperparameters) using gridsearchcv.')\n\nfrom sklearn.model_selection import GridSearchCV\n\ntrain_data = lgb.Dataset(X_test, label = Y_test)\n\nlgbm_mdl = LGBMClassifier(\n    n_estimators=300,\n    num_leaves=30,\n    colsample_bytree=.8,\n    subsample=.8,\n    max_depth=10,\n    reg_alpha=.1,\n    reg_lambda=.05,\n    min_split_gain=.005\n)\n\ngridParams = {\n    'learning_rate': [0.07, 0.1],\n    'n_estimators': [100, 300],\n    'num_leaves': [20, 50],\n    'random_state' : [501, 42], \n    'colsample_bytree' : [0.6, 0.8],\n    'subsample' : [0.6, 0.8],\n    'max_depth' : [10, 20]\n}\ngrid = GridSearchCV(lgbm_mdl, gridParams, verbose = 2, cv = 4, n_jobs = -1)\ngrid.fit(X_test, Y_test)\nprint(grid.best_params_)\nprint(grid.best_score_)\n\nparams = {\n    'colsample_bytree': grid.best_params_['colsample_bytree'],\n    'learning_rate': grid.best_params_['learning_rate'],\n    'max_depth': grid.best_params_['max_depth'],\n    'n_estimators': grid.best_params_['n_estimators'],\n    'num_leaves': grid.best_params_['num_leaves'],\n    'random_state': grid.best_params_['random_state'],\n    'subsample': grid.best_params_['subsample'],\n    'reg_alpha': .1,\n    'reg_lambda': .05,\n    'min_split_gain': .005,\n    'colsample_bytree': .8\n}\n\nprint('train model using the best hyperparameters')\nlgbm = lgb.train(params, train_data, verbose_eval = 4)\n\nprint('predict using the test set')\npredictions_lgbm_prob = lgbm.predict(X_test) # X_train\npredictions_lgbm_01 = np.where(predictions_lgbm_prob > 0.5, 1, 0)\n\nacc_lgbm_v2 = round(accuracy_score(Y_test, predictions_lgbm_01)  * 100, 2) # Y_train\nprint(acc_lgbm_v2)\nprint('done')","f13408e0":"print('Feature importance')\nlgb.plot_importance(lgbm, max_num_features=21, importance_type='split')","3e1ac380":"models = pd.DataFrame({\n    'Model': ['KNN', \n              'Logistic Regression', \n              'Random Forest',   \n              'Linear SVC (SVM)', \n              'Decision Tree', \n              'BaggingClassifier',\n              'AdaBoostClassifier', \n              'GradientBoostingClassifier',\n              'LGBMClassifier',\n              'CatBoost',\n              'LGBMClassifier PCA',\n              'CatBoost PCA',\n              'Bagging PCA',\n              'Light GBM with gridsearchcv'\n             ],\n    'Score': [acc_knn, \n              acc_log, \n              acc_random_forest,   \n              acc_linear_svc, \n              acc_decision_tree,\n              acc_clf_bag, \n              acc_bdt, \n              acc_clf_gb, \n              acc_clf_lgbm,\n              catmodel,\n              clfpca_lgbm,\n              catmodelpca,\n              acc_clf_bagpca,\n              acc_lgbm_v2\n             ]})\nmodels.sort_values(by='Score', ascending=False)","5504dd33":"###### Summary\n1. Load Modules and Data\n2. Initial Exploration\n3. Descriptive Statistics\n4. Data Preparation\n5. Model Development\n6. Principal Component Analysis\n7. Light GBM v2: Model selection (best hyperparameters) using gridsearchcv\n8. Light GBM version 3\n9. Model Evaluation","64b39659":"# 2. Initial Exploration","85559b37":"## Multi-layer perceptron","9e0ad368":"# 5. Model Development","6d7a6b71":"## Light GBM","66c3fd4d":"## Support Vector Machines","5c48983f":"# 4. Data Preparation","16657305":"## Catboost with PCA","a2aa0743":"## Light GBM with PCA","ea8398d9":"## Light GBM","b2ea63ee":"## Logistic Regression","b83a9288":"## Gradient Boosting","5fc2af25":"## Bagging classsifier with PCA","b2a5473a":"## KNN","b638faf3":"## Random Forest","056c78c6":"## Catboost","4449c6e6":"## Bagging classifier","69f310cf":"# **1. Load Modules and Data**","087472f5":"## Decision Tree","1ef32663":"# 9. Model Evaluation","b6d583d8":"# 6. Principal Component Analysis","5a3fc2fc":"# 7. Cross-validation using gridsearchcv","abbb027b":"# 3. Descriptive Statistics","1649e015":"## Adaboost"}}