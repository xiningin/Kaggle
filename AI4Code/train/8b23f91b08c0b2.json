{"cell_type":{"9141bea7":"code","f674408b":"code","88ecb35e":"code","6879a98f":"code","fdc4147a":"code","17b2c0b9":"code","7a8d98d1":"code","0dcc0d64":"code","ed15eeb8":"code","69fe3aae":"code","16eea52d":"code","4f45ad55":"code","62601399":"code","b113a3ca":"code","cbcfbc29":"code","956a958a":"code","03c03303":"code","e9040342":"code","f8738a00":"code","cd13e64d":"code","6d8453a6":"code","a72035f4":"code","833d0ae6":"code","0607304e":"code","d6eb474f":"code","fc68cde5":"code","84f2ae0e":"code","8bdd8dad":"code","aa7a1f1c":"code","8f92b285":"code","347b24ef":"code","5db2d031":"code","eac6221a":"code","b2c888e0":"code","5dd6a3d9":"code","6c1d5abb":"code","cee3bfaa":"code","c4f4a3b5":"code","b77af268":"code","508ecd9e":"code","b724dea9":"code","61f59ccf":"code","39384bd3":"code","1e985c41":"code","e57f1b42":"code","30648dd5":"code","80e7ba39":"code","ce0d94c4":"code","4312b52a":"code","32480974":"code","b697a529":"code","94346c88":"code","2bd70ca8":"code","ef658c7d":"code","437cb0ef":"code","fd0cad8d":"code","b6be307d":"code","bc03659f":"code","680ff132":"code","06bc0200":"code","30af6d4b":"code","747384a0":"code","1e04a008":"markdown","01916702":"markdown","7d569dd5":"markdown","3ab4026b":"markdown","a9c6bf99":"markdown","6a9e7926":"markdown","bf11a49e":"markdown","4c1c0e83":"markdown","36c70cf9":"markdown","ecfd9db2":"markdown","a602846a":"markdown","7e48396d":"markdown","af7a33fb":"markdown","e753cdbf":"markdown","5a24576b":"markdown","4faa1c74":"markdown","f31791ae":"markdown","3257d595":"markdown"},"source":{"9141bea7":"import random\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport plotly.figure_factory as ff\nimport plotly.graph_objects as goT\nimport seaborn as sns\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier, CatBoostRegressor\nfrom sklearn import metrics\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.ensemble import (AdaBoostClassifier, ExtraTreesClassifier,\n                              GradientBoostingClassifier,\n                              RandomForestClassifier, VotingClassifier)\nfrom sklearn.linear_model import (ElasticNet, Lasso, LinearRegression,\n                                  LogisticRegression, Ridge)\nfrom sklearn.metrics import (classification_report, log_loss,\n                             mean_squared_error, mean_squared_log_error)\nfrom sklearn.model_selection import (GridSearchCV, StratifiedKFold,\n                                     cross_val_score, learning_curve,\n                                     train_test_split)\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier","f674408b":"# Config \nTRAIN_PATH = \"..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv\"","88ecb35e":"train_df = pd.read_csv(TRAIN_PATH)","6879a98f":"train_df.head()","fdc4147a":"train_df.describe()","17b2c0b9":"train_df.info()","7a8d98d1":"# Let us look at the different values of quality\ntrain_df.quality.unique()","0dcc0d64":"def correlation_matrix(dataframe, col_list_to_drop):\n    \"\"\"This plots a correlation matrix in a dataframe\n    @param dataframe: The dataframe\n    @param col_list_to_drop: The columns which we should skip.\n    NOTE : We should only have numerical columns in the correlations plot.\n    \"\"\"\n    f, ax = plt.subplots(figsize=[20, 15])\n    sns.heatmap(dataframe.drop(col_list_to_drop, axis=1).corr(),\n                annot=True,\n                fmt=\".2f\",\n                ax=ax,\n                cbar_kws={'label': 'Correlation Coefficient'},\n                cmap='viridis')\n    ax.set_title(\"Correlation Matrix\", fontsize=18)\n    plt.show()","ed15eeb8":"correlation_matrix(train_df, [])","69fe3aae":"# Modify quality attribute\n\ndef compute_quality_label(x):\n    if x <= 5:\n        return 'good'\n    else:\n        return 'better'\n\ntrain_df['num quality'] = train_df['quality']\ntrain_df['quality'] = train_df['quality'].apply(lambda x : compute_quality_label(x))","16eea52d":"train_df.head()","4f45ad55":"def continuous_var_distribution_vs_single_cat_var(dataframe, numerical_col_1,\n                                                  cat_col_name, cat_col_list,\n                                                  plot_title):\n    \"\"\"This method plots multiple box plots for a single\n    continuous variable distribution vs multiple categorical\n    variable distribution.\n    @param dataframe: The dataframe\n    @param numerical_col_1: The numerical col name\n    @param cat_col_name: The categorical col name\n    @param cat_col_list: The different values of the categorical variable\n    that it takes\n    @param plot_title: The plot title name\n    \"\"\"\n    hex_colors_names = []\n    for name, hex in matplotlib.colors.cnames.items():\n        hex_colors_names.append(name)\n\n    dataframe_list = []\n    for col_name in cat_col_list:\n        dataframe_list.append(dataframe[dataframe[cat_col_name] == col_name][numerical_col_1])\n\n    fig = go.Figure()\n    for i in range(len(dataframe_list)):\n        df = dataframe_list[i]\n        fig.add_trace(go.Box(y=df,\n                             jitter=0.3,\n                             pointpos=-1.8,\n                             boxpoints='all',  # Display all points in plot\n                             marker_color=hex_colors_names[i+30],\n                             name=cat_col_list[i]))\n    fig.update_layout(title=plot_title)\n    fig.show()","62601399":"def continous_var_vs_single_cat_vars_waves(dataframe, numerical_col_1,\n                                           cat_col_name, cat_col_list,\n                                           binsize, show_hist=False):\n    \"\"\"This method plots distplot and also waves without\n    the histogram in the background when there is a single\n    continuous variable vs a single categorical vairable\n    @param dataframe: The dataframe\n    @param numerical_col_1: The numerical col name\n    @param cat_col_name: The categorical col name\n    @param cat_col_list: The different values the categorical\n    variable takes\n    @param binsize: The bin size for the single categorical variable\n    @param show_hist: Whether to show the histogram or not\n    \"\"\"\n    hex_colors_names = []\n    for name, hex in matplotlib.colors.cnames.items():\n        hex_colors_names.append(name)\n    random.shuffle(hex_colors_names)\n    hex_colors_names = hex_colors_names[:len(cat_col_list)]\n\n    dataframe_list = []\n    for col_name in cat_col_list:\n        dataframe_list.append(dataframe[dataframe[cat_col_name] == col_name][numerical_col_1])\n\n    binsize_list = []\n    for i in range(len(cat_col_list)):\n        binsize_list.append(binsize)\n\n    fig = ff.create_distplot(dataframe_list,\n                             cat_col_list,\n                             show_hist=show_hist,\n                             colors=hex_colors_names,\n                             bin_size=binsize_list)\n    fig.show()","b113a3ca":"continous_var_vs_single_cat_vars_waves(train_df, \n                                      'fixed acidity',\n                                      'quality',\n                                      ['good', 'better'],\n                                      [10, 10])","cbcfbc29":"## A new dataframe for storing the newly formed columns\ntemp_df = pd.DataFrame()\ntemp_df['quality'] = train_df['quality']","956a958a":"# Modify fixed acidity attribute\n\ndef compute_fixed_acidity_label(x):\n    if x <= 6:\n        return 0\n    elif x <= 10:\n        return 1\n    else:\n        return 2\n\ntemp_df['num fixed acidity'] = train_df['fixed acidity'].apply(lambda x : compute_fixed_acidity_label(x))","03c03303":"# Plot after modification and assigning the different groups\ncontinous_var_vs_single_cat_vars_waves(temp_df, \n                                      'num fixed acidity',\n                                      'quality',\n                                      ['good', 'better'],\n                                      [10, 10])","e9040342":"continous_var_vs_single_cat_vars_waves(train_df, \n                                      'volatile acidity',\n                                      'quality',\n                                      ['good', 'better'],\n                                      [10, 10])","f8738a00":"def compute_volatile_acidity_label(x):\n    if x <= 0.53:\n        return 0\n    else:\n        return 1\n\ntemp_df['num volatile acidity'] = train_df['volatile acidity'].apply(lambda x : compute_volatile_acidity_label(x))","cd13e64d":"continous_var_vs_single_cat_vars_waves(temp_df, \n                                      'num volatile acidity',\n                                      'quality',\n                                      ['good', 'better'],\n                                      [10, 10])","6d8453a6":"def numerical_vs_numerical_or_categorical(dataframe, numerical_col_1,\n                                          numerical_col_2, title_of_plot,\n                                          x_axis_title, y_axis_title,\n                                          categorical_col=None, numerical_col_3=None):\n    \"\"\"This method plots the a scatter plot between a numerical value,\n    and a categorical value. It also supports when there are 1 or 2 numerical\n    values along with a categorical value.\n    @param dataframe: The dataframe\n    @param numerical_col_1: The first numerical value\n    @param numerical_col_2: The second numerical value\n    @param title_of_plot: Title of the plot\n    @param x_axis_title: X axis title\n    @param y_axis_title: Y axis title\n    @param categorical_col: Categorical column name (optional)\n    @param numerical_col_3: The third numerical value (optional)\n    \"\"\"\n    fig = px.scatter(dataframe, x=numerical_col_1, y=numerical_col_2, color=categorical_col, size=numerical_col_3)\n    fig.update_layout(title=title_of_plot, xaxis_title=x_axis_title, yaxis_title=y_axis_title)\n    fig.show()","a72035f4":"## Let us first find out the bound sulphur in the wine. \n## We do this be subtracting the free sulphur dioxide from the total sulhpur dioxide\n## We will drop this column later\n\ntrain_df['bound sulfur dioxide'] = train_df['total sulfur dioxide'] - train_df['free sulfur dioxide']","833d0ae6":"numerical_vs_numerical_or_categorical(train_df, \n                                     'free sulfur dioxide', 'bound sulfur dioxide', 'Relationship between free and bound sulfur dioxide',\n                                     'free sulfur dioxide', 'bound sulfur dioxide')","0607304e":"numerical_vs_numerical_or_categorical(train_df, \n                                     'free sulfur dioxide', 'sulphates', 'Relationship between free sulfur dioxide and sulphates',\n                                     'free sulfur dioxide', 'sulphates')","d6eb474f":"numerical_vs_numerical_or_categorical(train_df, \n                                     'bound sulfur dioxide', 'sulphates', 'Relationship between bound sulphur dioxide and sulphates',\n                                     'bound sulfur dioxide', 'sulphates')","fc68cde5":"train_df = train_df.drop(['bound sulfur dioxide'], axis=1)","84f2ae0e":"train_df.columns","8bdd8dad":"for col_name in train_df.columns:\n    if col_name == 'quality' or col_name == 'fixed acidity' or col_name == 'volatile acidity' or col_name == 'num quality':\n        continue\n    continous_var_vs_single_cat_vars_waves(train_df, \n                                           col_name,\n                                           'quality',\n                                           ['good', 'better'],\n                                           [5, 5])","aa7a1f1c":"def compute_citric_acid(x):\n    if x <= 0.3:\n        return 0\n    else:\n        return 1\n\ndef compute_chlorides(x):\n    if x <= 0.1:\n        return 0\n    else:\n        return 1\n    \ndef compute_free_sulfur_dioxide(x):\n    if x <= 20:\n        return 0\n    else:\n        return 1\n    \ndef compute_total_sulfur_dioxide(x):\n    if x <= 57:\n        return 0\n    else:\n        return 1\n    \ndef compute_density(x):\n    if x <= 0.9957:\n        return 0\n    elif x <= 0.9988:\n        return 1\n    else:\n        return 2\n    \ndef compute_sulphates(x):\n    if x <= 0.62:\n        return 0\n    elif x <= 1.07:\n        return 1\n    else:\n        return 2\n    \ndef compute_alcohol(x):\n    if x <= 10.23:\n        return 0\n    else:\n        return 1\n\n\ntemp_df['num citric acid'] = train_df['citric acid'].apply(lambda x : compute_citric_acid(x))\ntemp_df['num chlorides'] = train_df['chlorides'].apply(lambda x : compute_chlorides(x))\ntemp_df['num free sulfur dioxide'] = train_df['free sulfur dioxide'].apply(lambda x : compute_free_sulfur_dioxide(x))\ntemp_df['num total sulfur dioxide'] = train_df['total sulfur dioxide'].apply(lambda x : compute_total_sulfur_dioxide(x))\ntemp_df['num density'] = train_df['density'].apply(lambda x : compute_density(x))\ntemp_df['num sulphates'] = train_df['sulphates'].apply(lambda x : compute_sulphates(x))\ntemp_df['num alcohol'] = train_df['alcohol'].apply(lambda x : compute_alcohol(x))\n","8f92b285":"train_df = train_df.drop(['residual sugar', 'pH'], axis=1)","347b24ef":"temp_df.columns","5db2d031":"for col_name in temp_df.columns:\n    if col_name == 'quality':\n        continue\n    continous_var_vs_single_cat_vars_waves(temp_df, \n                                           col_name,\n                                           'quality',\n                                           ['good', 'better'],\n                                           [5, 5])","eac6221a":"train_df = train_df.drop(['quality'], axis=1)\ntemp_df = temp_df.drop(['quality'], axis=1)","b2c888e0":"num_quality = train_df['num quality']\ntrain_df = train_df.drop(['num quality'], axis=1)\n\nscaled_features = StandardScaler().fit_transform(train_df.values)\ntrain_df = pd.DataFrame(scaled_features, index=train_df.index, columns=train_df.columns)\n\ntrain_df['num quality'] = num_quality.values","5dd6a3d9":"temp_df['quality'] = train_df['num quality']\nX = temp_df.drop(['quality'], axis=1)\ny = temp_df['quality']","6c1d5abb":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","cee3bfaa":"random_state = 2\nclassifiers = [SVC(random_state=random_state),\n               DecisionTreeClassifier(random_state=random_state),\n               AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state), random_state=random_state,\n                                  learning_rate=0.1),\n               RandomForestClassifier(random_state=random_state),\n               ExtraTreesClassifier(random_state=random_state),\n               GradientBoostingClassifier(random_state=random_state),\n               MLPClassifier(random_state=random_state),\n               KNeighborsClassifier(),\n               LogisticRegression(random_state=random_state),\n               LinearDiscriminantAnalysis()]","c4f4a3b5":"def ensembling_cross_val_classification_first_step(X_train, Y_train):\n    kfold = StratifiedKFold(n_splits=10)\n\n    cv_results = []\n    for classifier in classifiers:\n        cv_results.append(cross_val_score(classifier,\n                                          X_train, y=Y_train,\n                                          scoring=\"accuracy\",\n                                          cv=kfold,\n                                          n_jobs=4))\n\n    cv_means = []\n    cv_std = []\n    for cv_result in cv_results:\n        cv_means.append(cv_result.mean())\n        cv_std.append(cv_result.std())\n\n    cv_res = pd.DataFrame(\n        {\n            \"CrossValMeans\": cv_means,\n            \"CrossValerrors\": cv_std,\n            \"Algorithm\": [\"SVC\", \"DecisionTree\", \"AdaBoost\",\n                          \"RandomForest\", \"ExtraTrees\", \"GradientBoosting\",\n                          \"MultipleLayerPerceptron\", \"KNeighboors\", \"LogisticRegression\",\n                          \"LinearDiscriminantAnalysis\"]\n        }\n    )\n\n    g = sns.barplot(\"CrossValMeans\",\n                    \"Algorithm\",\n                    data=cv_res,\n                    palette=\"Set3\",\n                    orient=\"h\",\n                    **{'xerr': cv_std})\n    g.set_xlabel(\"Mean Accuracy\")\n    g.set_title(\"Cross validation scores\")","b77af268":"ensembling_cross_val_classification_first_step(X_train, y_train)","508ecd9e":"classifiers_for_ensembling = [\n    ExtraTreesClassifier(),\n    RandomForestClassifier()\n    #GradientBoostingClassifier(),\n    #SVC(probability=True)\n]\n\nparameters_for_ensembling_models = [\n    {\n        # For ExtraTreeClassifier\n        \"max_depth\": [None],\n        \"max_features\": [1, 3, 10],\n        \"min_samples_split\": [2, 3, 10],\n        \"min_samples_leaf\": [1, 3, 10],\n        \"bootstrap\": [False],\n        \"n_estimators\": [100, 300],\n        \"criterion\": [\"gini\"]\n    },\n    {\n        # Random forest classifier\n        \"max_depth\": [None],\n        \"max_features\": [1, 3, 10],\n        \"min_samples_split\": [2, 3, 10],\n        \"min_samples_leaf\": [1, 3, 10],\n        \"bootstrap\": [False],\n        \"n_estimators\": [100, 300],\n        \"criterion\": [\"gini\"]\n    }\n#     {\n#         # Gradient boosting\n#         'loss': [\"deviance\"],\n#         'n_estimators': [100, 200, 300],\n#         'learning_rate': [0.1, 0.05, 0.01],\n#         'max_depth': [4, 8],\n#         'min_samples_leaf': [100, 150],\n#         'max_features': [0.3, 0.1]\n#     },\n#     {\n#         # SVM Classifier\n#         'kernel': ['rbf'],\n#         'gamma': [0.001, 0.01, 0.1, 1],\n#         'C': [1, 10, 50, 100, 200, 300, 1000]\n#     }\n]","b724dea9":"def plot_learning_curve(estimator, title,\n                        X, y, ylim=None,\n                        cv=None, n_jobs=-1,\n                        train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n\n    train_sizes, train_scores, test_scores = learning_curve(estimator,\n                                                            X,\n                                                            y,\n                                                            cv=cv,\n                                                            n_jobs=n_jobs,\n                                                            train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n\n    plt.grid()\n    plt.fill_between(train_sizes,\n                     train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std,\n                     alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes,\n                     test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std,\n                     alpha=0.1,\n                     color=\"g\")\n    plt.plot(train_sizes,\n             train_scores_mean,\n             'o-',\n             color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes,\n             test_scores_mean,\n             'o-',\n             color=\"g\",\n             label=\"Cross-validation score\")\n    plt.legend(loc=\"best\")\n    return plt\n\n\ndef grid_search_find_best_models(X_train, Y_train,\n                                 kfold):\n    best_models = []\n\n    for i in range(len(classifiers_for_ensembling)):\n        model = classifiers_for_ensembling[i]\n        params = parameters_for_ensembling_models[i]\n        grid_search_model = GridSearchCV(model,\n                                         param_grid=params,\n                                         cv=kfold,\n                                         scoring=\"accuracy\",\n                                         n_jobs=4,\n                                         verbose=1)\n        print(model)\n        grid_search_model.fit(X_train, Y_train)\n        best_models.append(grid_search_model.best_estimator_)\n        plot_learning_curve(grid_search_model.best_estimator_,\n                            \"Learning curve for best model\",\n                            X_train,\n                            Y_train,\n                            cv=kfold)\n\n    return best_models","61f59ccf":"best_models = grid_search_find_best_models(X_train, y_train, StratifiedKFold(n_splits=10))","39384bd3":"def plot_feature_importance_of_tree_based_models(names_classifiers, X_train):\n    # names_classifiers = [(\"AdaBoosting\", ada_best), (\"ExtraTrees\", ExtC_best), (\"RandomForest\", RFC_best),\n    #                     (\"GradientBoosting\", GBC_best)]\n\n    nrows = ncols = 4\n    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, sharex=\"all\", figsize=(15, 15))\n\n    nclassifier = 0\n    for row in range(nrows):\n        for col in range(ncols):\n            name = names_classifiers[nclassifier][0]\n            classifier = names_classifiers[nclassifier][1]\n            indices = np.argsort(classifier.feature_importances_)[::-1][:40]\n            g = sns.barplot(y=X_train.columns[indices][:40],\n                            x=classifier.feature_importances_[indices][:40],\n                            orient='h',\n                            ax=axes[row][col])\n            g.set_xlabel(\"Relative importance\", fontsize=12)\n            g.set_ylabel(\"Features\", fontsize=12)\n            g.tick_params(labelsize=9)\n            g.set_title(name + \" feature importance\")\n            nclassifier += 1","1e985c41":"best_models","e57f1b42":"names_classifier = [('ExtraTreeClassifier', best_models[0]),\n                   ('RandomForestClassifier', best_models[1])]\nplot_feature_importance_of_tree_based_models(names_classifier, X_train)","30648dd5":"def plot_ensemble_classifier_results(test, classifiers,\n                                     X_train, Y_train):\n    # [('rfc', RFC_best), ('extc', ExtC_best),\n    #  ('svc', SVMC_best), ('adac', ada_best), ('gbc', GBC_best)]\n\n    class_res = []\n    for classifier in classifiers:\n        class_res.append(pd.Series(classifier[1].predict(test), name=classifier[0]))\n    ensemble_results = pd.concat(class_res, axis=1)\n    sns.heatmap(ensemble_results.corr(), annot=True)\n    votingC = VotingClassifier(estimators=classifiers,\n                               voting='soft',\n                               n_jobs=4)\n    return votingC.fit(X_train, Y_train)","80e7ba39":"voting_classifier = plot_ensemble_classifier_results(X_test, names_classifier, X_train, y_train)","ce0d94c4":"print(classification_report(y_test.values, voting_classifier.predict(X_test)))","4312b52a":"train_df = train_df.drop(['chlorides', 'free sulfur dioxide', 'fixed acidity'], axis=1)\ntemp_df = temp_df.drop(['num chlorides', 'num free sulfur dioxide', 'num fixed acidity'], axis=1)","32480974":"temp_df = temp_df.drop(['quality'], axis=1)","b697a529":"dataset = pd.concat([temp_df, train_df], axis=1)","94346c88":"X = dataset.iloc[:, :-1]\ny = dataset.iloc[:, -1]","2bd70ca8":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","ef658c7d":"cat_features = [0, 1, 2, 3, 4, 5]","437cb0ef":"clf = CatBoostClassifier(iterations=100) #verbose=5)\n\nclf.fit(X_train, y_train, cat_features=cat_features, eval_set=(X_test, y_test))","fd0cad8d":"print(classification_report(y_test.values, clf.predict(X_test)))","b6be307d":"def cat_boost_regressor(X_train, y_train,\n                        X_val, y_val,\n                        categorical_features_indices):\n    \"\"\"This trains the cat boost regressor on a training data set\n    @param X_train: Training dataset input\n    @param y_train: Training dataset output\n    @param X_val: Validation dataset input\n    @param y_val: Validation dataset output\n    @param categorical_features_indices: List of indices\n    which are categorical features in the X_train\n    @return: The trained model\n    \"\"\"\n    model = CatBoostRegressor(iterations=60,\n                              depth=3,\n                              learning_rate=0.1,\n                              loss_function='RMSE')\n    model.fit(X_train, y_train,\n              cat_features=categorical_features_indices,\n              eval_set=(X_val, y_val),\n              plot=True)\n\n    return model","bc03659f":"cb_model = cat_boost_regressor(X_train, y_train, X_test, y_test, cat_features)","680ff132":"mean_squared_error(y_test.values, cb_model.predict(X_test))","06bc0200":"def ridge_regression(X_train, y_train):\n    \"\"\"This trains the ridge regressor\n    @param X_train: Training dataset input\n    @param y_train: Training dataset output\n    @return: Trained model, the best parameters after grid search and the\n    best score of the model\n    \"\"\"\n    ridge = Ridge()\n    parameters = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3, 1e-2, 1, 5, 10, 20]}\n    ridge_regressor = GridSearchCV(ridge,\n                                   parameters,\n                                   scoring='neg_mean_squared_error',\n                                   cv=5)\n    ridge_regressor.fit(X_train, y_train)\n\n    return ridge_regressor.best_estimator_ , ridge_regressor.best_params_, ridge_regressor.best_score_","30af6d4b":"ridge_model, _, _ = ridge_regression(X_train, y_train)","747384a0":"mean_squared_error(y_test.values, ridge_model.predict(X_test))","1e04a008":"## A look at the different contributions to quality","01916702":"### Analysis\n\nIn case of volatile acidity, we can clearly see that all the samples of the wine having volatile acidity above **0.53** are good and not better.\n\nThe better wines all have volatile acidity less than that of **0.53**. We can then break this down as well into tow groups.\n\nOne having volatile acidity less than **0.53** and other having volatile acidity above **0.53**.","7d569dd5":"## Cat boost classifier\n\nLet us apply the cat boost classifier first to see the result","3ab4026b":"## Different regression models \n\n","a9c6bf99":"## Analysis and results\n\nWe did not get a good accuracy from this. However, we got to see some very interesting things from the feature importances.\n\nWe saw that the features:\n* chlorides\n* free sulfur dioxide\n* fixed acidity\n\nare not that much importance. So, we shall now remove them and train a model with both these generated and original scaled features.","6a9e7926":"## Training the model\n\nLet us now train the model. First, we will train the model only on the newly created features and not the old features to see the results.","bf11a49e":"## Analysis of sulphur\n\nWe have the following different columns in our dataset related to sulphur. One is **free sulfur dioxide**, the other is **total sulfur dioxide**, and the third is **sulphates**.\n\nAt first glance, it seems that these are somehow related to eachother. Let us look at how this might be so.","4c1c0e83":"## Volatile acidity","36c70cf9":"## Fixed acidity\n\nLet us now look at how fixed acidity contributes to the quality","ecfd9db2":"## Import the training dataset","a602846a":"## Other columns as well","7e48396d":"## Preprocessing \n\nNow, before we train the model, let us go through some preprocessing.\n\nWe will try to preprocess the dataset and do some scaling.","af7a33fb":"## Quality distribution\n\nWe can modify the quality distribution in such a way that the wines having quality 3,4,5 are good : label ```0```\n\nand the wines having quality 6,7,8 are better : label ```1```\n\nHowever, let us first look at the different contributions to quality to geta rough ides\n","e753cdbf":"Hmm..these two look somewhat similar right? And both of them have the common **free sulfur dioxide** in them.\n\nLet's try to plot the **sulphates** and **bound sulfur dioxide** now and see what we get.","5a24576b":"### Analysis\n\nHere we can see that the quality of wine is the best at a fixed acidity range. This range of acidity is between **6 and 10** after which the quality of the wine decreases down. So, we can break down the fixed acidity into categorical values as well.\n\nThe quality of the wine can be understood to be between the followign ranges:\n* **0 to 6** : 0\n* **6 to 10** : 1\n* **10 and above** : 2\n\nAll the wines above 10 and below 6 don't have that good quality. They are of poor quality and not the best as can be seen in the above distribution.","4faa1c74":"Woah!! We can see here that these two have a strong relationship between eachother. At times when the bound sulfur dioxide is less, the sulphates are also very less. And there are very less cases when the bound sulphur dioxide is less that the sulphates are more.\n\nAlso, we can clearly see that when the sulphur dioxide is bound more, the sulphates are either not present or very few of them are present in high concentrations.\n\nKeeping this in mind, we can now drop the sulphates as well here since it can be derived easily.","f31791ae":"## Column analysis\n\nAs can be seen in the above diagrams and plots, there are certain plots which do not\ncontribute significantly in any manner to the quality of the wine. Two of them are quite visible.\nThese are **residual sugar** and **pH**.\n\nWe will be dropping these two to reduce the dimensionality of our dataset.","3257d595":"## Generated columns\n\nNow, let us have a look at all the columsn which we generated using the analysis."}}