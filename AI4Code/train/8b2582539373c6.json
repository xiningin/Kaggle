{"cell_type":{"5df660f7":"code","2ba47020":"code","48b38c28":"code","d40e3e15":"code","476f0171":"code","b6b2eddb":"code","a85c2a5c":"code","3c7b1b90":"code","e249db75":"code","20ef7f45":"code","f3dee6b1":"code","931b4083":"code","0c1a92a5":"code","6e57a6a7":"code","2221bc9f":"code","242b800c":"code","f6b462e7":"code","40e4dcad":"code","d64d54da":"code","81d8c7d3":"code","c4c88eca":"code","9d0b7ddd":"code","78c97b1c":"code","7a954541":"code","3f45683d":"markdown"},"source":{"5df660f7":"#SET THE VALUES FOR THIS RUN\nTHIS_DROPOUT=0.3\nTHIS_BATCHSIZE=32\nTHIS_FOLD=1","2ba47020":"import pickle\nimport pandas as pd\nhpa_df = pd.read_pickle(\"..\/input\/partial-dataframes-with-paths-for-training\/dataset_partial_224\")","48b38c28":"hpa_df","d40e3e15":"LABELS= {\n0: \"Nucleoplasm\",\n1: \"Nuclear membrane\",\n2: \"Nucleoli\",\n3: \"Nucleoli fibrillar center\",\n4: \"Nuclear speckles\",\n5: \"Nuclear bodies\",\n6: \"Endoplasmic reticulum\",\n7: \"Golgi apparatus\",\n8: \"Intermediate filaments\",\n9: \"Actin filaments\",\n10: \"Microtubules\",\n11: \"Mitotic spindle\",\n12: \"Centrosome\",\n13: \"Plasma membrane\",\n14: \"Mitochondria\",\n15: \"Aggresome\",\n16: \"Cytosol\",\n17: \"Vesicles and punctate cytosolic patterns\",\n18: \"Negative\"\n}","476f0171":"import tensorflow as tf\nprint(tf.__version__)\n\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *\nimport tensorflow_addons as tfa\n\nimport os\nimport re\nimport cv2\nimport glob\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom functools import partial\nimport matplotlib.pyplot as plt","b6b2eddb":"IMG_WIDTH = 224\nIMG_HEIGHT = 224\nBATCH_SIZE = THIS_BATCHSIZE\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE","a85c2a5c":"#%%script echo skipping\n#set an amount of folds to split dataframe into --> k-fold cross validation\n# explanation: https:\/\/towardsdatascience.com\/cross-validation-explained-evaluating-estimator-performance-e51e5430ff85\nN_FOLDS=5\n#choose which one of the 5 folds will be used as validation set this time\ni_VAL_FOLD=THIS_FOLD\nhpa_df=np.array_split(hpa_df, N_FOLDS+1) #add one extra part for testing set\ndf_test_split=hpa_df[-1]\nhpa_df=hpa_df[:-1]\ni_training = [i for i in range(N_FOLDS)]\ni_training.pop(i_VAL_FOLD-1)\ni_validation=i_VAL_FOLD-1\ndf_train_split=list()\nfor i in i_training:\n    df_train_split.append(hpa_df[i])\ndf_train_split=pd.concat(df_train_split)\ndf_val_split=hpa_df[i_validation]","3c7b1b90":"print(len(df_train_split))\nprint(len(df_val_split))\nprint(len(df_test_split))","e249db75":"#%%script echo skipping\n#analyze class imbalance and set up class weights here\n#https:\/\/www.analyticsvidhya.com\/blog\/2020\/10\/improve-class-imbalance-class-weights\/\ny_train=df_train_split[\"Label\"].apply(lambda x:list(map(int, x.split(\"|\"))))\ny_train=y_train.values\ny_train=np.concatenate(y_train)\nfrom sklearn.utils import class_weight\nclass_weights = class_weight.compute_class_weight('balanced',\n                                                 np.unique(y_train),\n                                                 y_train)","20ef7f45":"#%%script echo skipping\ntmp_dict={}\nfor i in range(len(LABELS)):\n    tmp_dict[i]=class_weights[i]\nclass_weights=tmp_dict\nclass_weights","f3dee6b1":"#%%script echo skipping\n@tf.function\ndef multiple_one_hot(cat_tensor, depth_list):\n    \"\"\"Creates one-hot-encodings for multiple categorical attributes and\n    concatenates the resulting encodings\n\n    Args:\n        cat_tensor (tf.Tensor): tensor with mutiple columns containing categorical features\n        depth_list (list): list of the no. of values (depth) for each categorical\n\n    Returns:\n        one_hot_enc_tensor (tf.Tensor): concatenated one-hot-encodings of cat_tensor\n    \"\"\"\n    one_hot_enc_tensor = tf.one_hot(cat_int_tensor[:,0], depth_list[0], axis=1)\n    for col in range(1, len(depth_list)):\n        add = tf.one_hot(cat_int_tensor[:,col], depth_list[col], axis=1)\n        one_hot_enc_tensor = tf.concat([one_hot_enc_tensor, add], axis=1)\n    return one_hot_enc_tensor\n\n@tf.function\ndef load_image(df_dict):\n    # Load image\n    rgb = tf.io.read_file(df_dict['path'])\n    image = tf.image.decode_png(rgb, channels=3)\n    #https:\/\/medium.com\/@kyawsawhtoon\/a-tutorial-to-histogram-equalization-497600f270e2\n    #image=tf.image.per_image_standardization(image)\n    \n    # Parse label\n    label = tf.strings.split(df_dict['Label'], sep='|')\n    label = tf.strings.to_number(label, out_type=tf.int32)\n    label = tf.reduce_sum(tf.one_hot(indices=label, depth=19), axis=0)\n    \n    return image, label","931b4083":"#%%script echo skipping\ntrain_ds = tf.data.Dataset.from_tensor_slices(dict(df_train_split))\nval_ds = tf.data.Dataset.from_tensor_slices(dict(df_val_split))\n\n# Training Dataset\ntrain_ds = (\n    train_ds\n    .shuffle(1024)\n    .map(load_image, num_parallel_calls=AUTOTUNE)\n    .batch(BATCH_SIZE)\n    .prefetch(tf.data.experimental.AUTOTUNE)\n)\n\n# Validation Dataset\nval_ds = (\n    val_ds\n    .shuffle(1024)\n    .map(load_image, num_parallel_calls=AUTOTUNE)\n    .batch(BATCH_SIZE)\n    .prefetch(tf.data.experimental.AUTOTUNE)\n)","0c1a92a5":"#%%script echo skipping\ndef get_label_name(labels):\n    l = np.where(labels == 1.)[0]\n    label_names = []\n    for label in l:\n        label_names.append(LABELS[label])\n        \n    return '-'.join(str(label_name) for label_name in label_names)\n\ndef show_batch(image_batch, label_batch):\n  plt.figure(figsize=(20,20))\n  for n in range(10):\n      ax = plt.subplot(5,5,n+1)\n      plt.imshow(image_batch[n])\n      plt.title(get_label_name(label_batch[n].numpy()))\n      plt.axis('off')","6e57a6a7":"#%%script echo skipping\n# Training batch\nimage_batch, label_batch = next(iter(train_ds))\nshow_batch(image_batch, label_batch)\n#print(label_batch)","2221bc9f":"#%%script echo skipping\n\ndef get_model():\n    base_model = tf.keras.applications.EfficientNetB0(include_top=False, weights='imagenet')\n    base_model.trainable = True\n\n    inputs = Input((IMG_HEIGHT, IMG_WIDTH, 3))\n    x = base_model(inputs, training=True)\n    x = GlobalAveragePooling2D()(x)\n    x = Dropout(THIS_DROPOUT)(x)\n    outputs = Dense(len(LABELS), activation='sigmoid')(x)\n    \n    return Model(inputs, outputs)\n\ntf.keras.backend.clear_session()\nmodel = get_model()\nmodel.summary()","242b800c":"#%%script echo skipping\ntime_stopping_callback = tfa.callbacks.TimeStopping(seconds=int(round(60*60*7.5)), verbose=1) #7.5h\n\nearlystopper = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', patience=5, verbose=0, mode='min',\n    restore_best_weights=True\n)","f6b462e7":"#%%script echo skipping\n#set up checkpoint save\n#source:https:\/\/www.tensorflow.org\/tutorials\/keras\/save_and_load\n!pip install -q pyyaml h5py\nimport os\ncheckpoint_path = \".\/cp.ckpt\"\ncheckpoint_dir = os.path.dirname(checkpoint_path)\n\n# Create a callback that saves the model's weights\ncp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n                                                 save_weights_only=True,\n                                                 verbose=1)","40e4dcad":"#%%script echo skipping\nimport keras.backend as K\nimport math\nK_epsilon = K.epsilon()\ndef f1(y_true, y_pred):\n    y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), 0.5), K.floatx())\n    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n\n    p = tp \/ (tp + fp + K_epsilon)\n    r = tp \/ (tp + fn + K_epsilon)\n\n    f1 = 2*p*r \/ (p+r+K_epsilon)\n    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n    return K.mean(f1)\ndef f1_loss(y_true, y_pred):\n    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n\n    p = tp \/ (tp + fp + K_epsilon)\n    r = tp \/ (tp + fn + K_epsilon)\n\n    f1 = 2*p*r \/ (p+r+K_epsilon)\n    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n    return 1-K.mean(f1)","d64d54da":"#%%script echo skipping\nimport tensorflow as tf\nimport timeit\n\ndevice_name = tf.test.gpu_device_name()\nif \"GPU\" not in device_name:\n    print(\"GPU device not found\")\nprint('Found GPU at: {}'.format(device_name))","81d8c7d3":"tf.keras.backend.clear_session()\nmodel = get_model()\n\nbce = tf.keras.losses.BinaryCrossentropy(from_logits=False) #not used\n\n#model.load_weights(checkpoint_path)\n\nmodel.compile(optimizer=tf.keras.optimizers.Adam(),\n              loss=f1_loss,\n              metrics=['accuracy',f1,tf.keras.metrics.AUC(multi_label=True),'binary_crossentropy'])\nfrom keras.callbacks import LearningRateScheduler\ndef decay_schedule(epoch, lr): #not used\n    # '% 5' to decay by 0.25 every 5 epochs; use `% 1` to decay after each epoch\n    if (epoch % 10 == 0) and (epoch != 0):\n        lr = lr * 0.25\n    print(\"LEARNING RATE=\"+str(lr))\n    return lr\n\ndef fixed_decay(epoch, lr):\n    if epoch<=20:\n        lr = 0.001\n    if epoch>20 and epoch <=40:\n        lr = 0.00075\n    if epoch>40 and epoch <=60:\n        lr = 0.0005\n    if epoch>60 and epoch <=80:\n        lr = 0.00025\n    if epoch>80 and epoch <=100:\n        lr = 0.0001\n    if epoch>100 and epoch <=120:\n        lr = 0.000075\n    if epoch>120 and epoch <=140:\n        lr = 0.00005\n    if epoch>140:\n        lr = 0.000025\n    print(\"LEARNING RATE=\"+str(lr))\n    return lr\n\nlr_scheduler = LearningRateScheduler(fixed_decay)\nhistory=model.fit(train_ds,\n                  epochs=1000,\n                  validation_data=val_ds,\n                  class_weight=class_weights,\n                  callbacks=[cp_callback,time_stopping_callback,lr_scheduler])","c4c88eca":"#%%script echo skipping\nhistory.history","9d0b7ddd":"#%%script echo skipping\n#source: https:\/\/machinelearningmastery.com\/display-deep-learning-model-training-history-in-keras\/\n# list all data in history\nprint(history.history.keys())\n\ndef plot_metric(name):\n    val_name='val_'+name\n    plt.plot(history.history[name])\n    if name!=\"lr\":\n        plt.plot(history.history[val_name])\n    plt.title(name)\n    plt.ylabel(name)\n    plt.xlabel('epoch')\n    if name!=\"lr\":\n        plt.legend(['train', 'val'], loc='upper left')\n    else:\n        plt.legend(['train'], loc='upper left')\n    plt.show()\n    \nplot_metric('loss')\nplot_metric('accuracy')\nplot_metric('f1')\nplot_metric('auc')\nplot_metric('binary_crossentropy')\nplot_metric('lr')","78c97b1c":"with open('.\/historyhistory.pkl', 'wb') as handle:\n    pickle.dump(history.history, handle, protocol=pickle.HIGHEST_PROTOCOL)","7a954541":"with open('.\/history.pkl', 'wb') as handle:\n    pickle.dump(history, handle, protocol=pickle.HIGHEST_PROTOCOL)","3f45683d":"#### this notebook is part of the documentation on my HPA approach  \n    -> main notebook: https:\/\/www.kaggle.com\/philipjamessullivan\/0-hpa-approach-summary"}}