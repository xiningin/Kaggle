{"cell_type":{"e0e3eee7":"code","dc94503e":"code","fdd00690":"code","b1361082":"code","df7a6ec9":"code","bec228a6":"code","96056fa5":"code","ea733692":"code","e27d505b":"code","36bb8064":"code","f9efb0f9":"code","ec671dd7":"code","87917c38":"markdown","59b95866":"markdown","6fc88b08":"markdown","77ece1b9":"markdown","c7c803c5":"markdown","e4b243a5":"markdown","2184ea86":"markdown","32d468f0":"markdown","0b16f589":"markdown","af557a07":"markdown","2aabaa07":"markdown","30f0b5ae":"markdown","ed110ec0":"markdown","98dd383a":"markdown","ae168b20":"markdown","6cccf02e":"markdown","aac7a930":"markdown","cb04a31e":"markdown","c18bd9c5":"markdown"},"source":{"e0e3eee7":"import numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom xgboost import XGBRegressor\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.preprocessing import PolynomialFeatures\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-whitegrid')\n# Set Matplotlib defaults\nplt.rc('figure', autolayout=True)\nplt.rc('axes', labelweight='bold', labelsize='large',\n       titleweight='bold', titlesize=18, titlepad=10)\nplt.rc('animation', html='html5')\n\nimport os","dc94503e":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","fdd00690":"# Read the data\nX = pd.read_csv('..\/input\/30-days-of-ml\/train.csv', index_col='id')\nX_test_full = pd.read_csv('..\/input\/30-days-of-ml\/test.csv', index_col='id')\n\n# Remove rows with missing target, separate target from predictors\nX.dropna(axis=0, subset=['target'], inplace=True)\ny = X.target             \nX.drop(['target'], axis=1, inplace=True)\n\n# Break off validation set from training data\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                                random_state=0)\n\n# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\nlow_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n                        X_train_full[cname].dtype == \"object\"]\n\n# Select numeric columns\nnumeric_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n\n# Keep selected columns only\nmy_cols = low_cardinality_cols + numeric_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\nX_test = X_test_full[my_cols].copy()\n\n# One-hot encode the data (to shorten the code, we use pandas)\nX_train = pd.get_dummies(X_train)\nX_valid = pd.get_dummies(X_valid)\nX_test = pd.get_dummies(X_test)\nX_train, X_valid = X_train.align(X_valid, join='left', axis=1)\nX_train, X_test = X_train.align(X_test, join='left', axis=1)\n","b1361082":"from sklearn.preprocessing import StandardScaler\nsc= StandardScaler()\nX_train=sc.fit_transform(X_train)\nX_valid = sc.transform(X_valid)\n\nX_train = pd.DataFrame(X_train)\nX_valid = pd.DataFrame(X_valid)","df7a6ec9":"def score_dataset(X_train, X_valid, y_train, y_valid):\n    model = RandomForestRegressor(n_estimators=100, random_state=0)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_valid)\n    return mean_absolute_error(y_valid, preds)","bec228a6":"# Preprocessing for numerical data\nnumerical_transformer = SimpleImputer(strategy=\"constant\") # Your code here\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n]) \n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numeric_cols),\n        ('cat', categorical_transformer, low_cardinality_cols)\n    ])\n","96056fa5":"\nmodel = RandomForestRegressor(n_estimators=100, random_state=0) # Your code here\nmodel0 = XGBRegressor(n_estimators=100, n_jobs=4, learning_rate=0.05, random_state=0)\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nGradientBoostingRegressorModel = GradientBoostingRegressor(n_estimators=10,\n                                                   max_depth=9, learning_rate=0.05,\n                                                   random_state=9)\n\n","ea733692":"# Bundle preprocessing and modeling code in a pipeline\nclf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', GradientBoostingRegressorModel)\n                     ])","e27d505b":"#clf.fit(X_train, y_train)\nmodel0.fit(X_train, y_train)","36bb8064":"# Preprocessing of validation data, get predictions\npreds = model0.predict(X_valid)\n\n#plt.figure(figsize(20,10))\nsns.scatterplot( x=preds, y=y_valid)\nplt.show()","f9efb0f9":"print('MAE:', mean_absolute_error(y_valid, preds))\n","ec671dd7":"preds0 = model0.predict(X_test)\n# Save test predictions to file\noutput = pd.DataFrame({'Id': X_test.index,\n                       'target': preds0})\noutput.to_csv('submission.csv', index=False)\n","87917c38":"# Trying Polynomial Feaatures","59b95866":"# let see the mean absalute error for our model","6fc88b08":"# Preprocessing of training data, fit model ","77ece1b9":"* # Then Load the Data using pandas ","c7c803c5":"**this may take a while because it had 400 estimators**","e4b243a5":"# Remove rows with missing target, separate target from predictors\n","2184ea86":"# another regression moodel ","32d468f0":"# Now er predict","0b16f589":"# Break off validation set from training data","af557a07":"**Read the data and have a look**","2aabaa07":"# Function for comparing different approaches","30f0b5ae":" **Still cleaning using simple imputer from sklearn**","ed110ec0":"# To keep things simple, we'll use only numerical predictors\n**Cleaning data is very important step**","98dd383a":"# lets scale the data","ae168b20":"# Select numerical columns","6cccf02e":"**lets try three regression model and vote for the best**","aac7a930":"# Define model","cb04a31e":"# **This is a simple Regression model for the 30 ML month**\n# importing Liberaries ","c18bd9c5":"# The Output"}}