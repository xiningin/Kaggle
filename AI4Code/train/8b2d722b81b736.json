{"cell_type":{"8d5a344d":"code","f0a16c19":"code","66bebda3":"code","0684872f":"code","c0dfed01":"code","063a0e60":"code","60727669":"code","1ec03fb5":"code","95f011f1":"code","46596e5b":"code","8675ae69":"code","6997e65d":"code","d9354644":"code","ddad63cb":"code","fd899192":"code","82da6757":"code","10252f1d":"code","c0f3ac42":"code","20886512":"code","24c0eae2":"code","583766d2":"code","d00e24a1":"code","2c1eda4c":"code","b85f5b06":"code","73f7b11a":"code","97f7cf88":"code","81fe960d":"code","a14255b6":"code","88231f33":"code","8afd0572":"code","07cb0c27":"code","887f8261":"code","465e7a5d":"code","5273c22f":"code","5f2aa027":"code","62c6d3a8":"code","78650aba":"code","ed591e93":"code","142e4f1f":"code","60b66bf7":"code","853c255d":"code","23b82416":"code","15c78f78":"code","09bd1b1d":"code","cedcf7d4":"code","0a6f1411":"code","91fbc8d4":"code","9e8496ea":"code","b45d8027":"code","a4969d9e":"code","e59935a2":"code","24b65d3a":"code","2fab05cc":"code","4d202670":"code","8c912c42":"code","6edb9e26":"code","5a8c7183":"code","3e38e0f4":"code","11a68904":"code","615800d4":"code","4bf8dc20":"code","8309f30f":"code","3fa9cf1b":"markdown","cadad9b7":"markdown","4a8487d5":"markdown","8a9e7440":"markdown","0db8a6df":"markdown","d8cd0a38":"markdown","a13874b2":"markdown","ba7c993b":"markdown","09157e87":"markdown","08f667a5":"markdown","cd7a56a3":"markdown","691c797b":"markdown","ffe05ae1":"markdown","91611529":"markdown","0d86cd7c":"markdown","9fa69923":"markdown","13012e97":"markdown","9dd40837":"markdown","c8240335":"markdown","66a0e7b6":"markdown","04909965":"markdown","e48e49ce":"markdown","c085a99a":"markdown","e67e9979":"markdown","9446564f":"markdown","4df5b71e":"markdown","5dff778c":"markdown","6358ab16":"markdown"},"source":{"8d5a344d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sns\nimport datetime\nfrom kaggle.competitions import nflrush\nimport tqdm\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler\nimport keras\n\nsns.set_style('darkgrid')\nmpl.rcParams['figure.figsize'] = [15,10]","f0a16c19":"train = pd.read_csv('..\/input\/nfl-big-data-bowl-2020\/train.csv', dtype={'WindSpeed': 'object'})","66bebda3":"train.head()","0684872f":"train['PlayId'].value_counts()","c0dfed01":"train['Yards'].describe()","063a0e60":"ax = sns.distplot(train['Yards'])\nplt.vlines(train['Yards'].mean(), plt.ylim()[0], plt.ylim()[1], color='r', linestyles='--');\nplt.text(train['Yards'].mean()-8, plt.ylim()[1]-0.005, \"Mean yards travaled\", size=15, color='r')\nplt.xlabel(\"\")\nplt.title(\"Yards travaled distribution\", size=20);","60727669":"cat_features = []\nfor col in train.columns:\n    if train[col].dtype =='object':\n        cat_features.append((col, len(train[col].unique())))","1ec03fb5":"off_form = train['OffenseFormation'].unique()\ntrain['OffenseFormation'].value_counts()","95f011f1":"train = pd.concat([train.drop(['OffenseFormation'], axis=1), pd.get_dummies(train['OffenseFormation'], prefix='Formation')], axis=1)\ndummy_col = train.columns","46596e5b":"train['GameClock'].value_counts()","8675ae69":"def strtoseconds(txt):\n    txt = txt.split(':')\n    ans = int(txt[0])*60 + int(txt[1]) + int(txt[2])\/60\n    return ans","6997e65d":"train['GameClock'] = train['GameClock'].apply(strtoseconds)","d9354644":"sns.distplot(train['GameClock'])","ddad63cb":"train['PlayerHeight']","fd899192":"train['PlayerHeight'] = train['PlayerHeight'].apply(lambda x: 12*int(x.split('-')[0])+int(x.split('-')[1]))","82da6757":"train['TimeHandoff']","10252f1d":"train['TimeHandoff'] = train['TimeHandoff'].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%fZ\"))\ntrain['TimeSnap'] = train['TimeSnap'].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%fZ\"))","c0f3ac42":"train['TimeDelta'] = train.apply(lambda row: (row['TimeHandoff'] - row['TimeSnap']).total_seconds(), axis=1)","20886512":"train['PlayerBirthDate'] = train['PlayerBirthDate'].apply(lambda x: datetime.datetime.strptime(x, \"%m\/%d\/%Y\"))","24c0eae2":"seconds_in_year = 60*60*24*365.25\ntrain['PlayerAge'] = train.apply(lambda row: (row['TimeHandoff']-row['PlayerBirthDate']).total_seconds()\/seconds_in_year, axis=1)","583766d2":"train = train.drop(['TimeHandoff', 'TimeSnap', 'PlayerBirthDate'], axis=1)","d00e24a1":"train['WindSpeed'].value_counts()","2c1eda4c":"train['WindSpeed'] = train['WindSpeed'].apply(lambda x: x.lower().replace('mph', '').strip() if not pd.isna(x) else x)","b85f5b06":"train['WindSpeed'].value_counts()","73f7b11a":"#let's replace the ones that has x-y by (x+y)\/2\n# and also the ones with x gusts up to y\ntrain['WindSpeed'] = train['WindSpeed'].apply(lambda x: (int(x.split('-')[0])+int(x.split('-')[1]))\/2 if not pd.isna(x) and '-' in x else x)\ntrain['WindSpeed'] = train['WindSpeed'].apply(lambda x: (int(x.split()[0])+int(x.split()[-1]))\/2 if not pd.isna(x) and type(x)!=float and 'gusts up to' in x else x)","97f7cf88":"def str_to_float(txt):\n    try:\n        return float(txt)\n    except:\n        return -1","81fe960d":"train['WindSpeed'] = train['WindSpeed'].apply(str_to_float)","a14255b6":"train['WindDirection'].value_counts()","88231f33":"train.drop('WindDirection', axis=1, inplace=True)","8afd0572":"train['PlayDirection'].value_counts()","07cb0c27":"train['PlayDirection'] = train['PlayDirection'].apply(lambda x: x is 'right')","887f8261":"train['Team'] = train['Team'].apply(lambda x: x.strip()=='home')","465e7a5d":"train['GameWeather'].unique()","5273c22f":"train['GameWeather'] = train['GameWeather'].str.lower()\nindoor = \"indoor\"\ntrain['GameWeather'] = train['GameWeather'].apply(lambda x: indoor if not pd.isna(x) and indoor in x else x)\ntrain['GameWeather'] = train['GameWeather'].apply(lambda x: x.replace('coudy', 'cloudy').replace('clouidy', 'cloudy').replace('party', 'partly') if not pd.isna(x) else x)\ntrain['GameWeather'] = train['GameWeather'].apply(lambda x: x.replace('clear and sunny', 'sunny and clear') if not pd.isna(x) else x)\ntrain['GameWeather'] = train['GameWeather'].apply(lambda x: x.replace('skies', '').replace(\"mostly\", \"\").strip() if not pd.isna(x) else x)","5f2aa027":"train['GameWeather'].unique()","62c6d3a8":"from collections import Counter\nweather_count = Counter()\nfor weather in train['GameWeather']:\n    if pd.isna(weather):\n        continue\n    for word in weather.split():\n        weather_count[word]+=1\n        \nweather_count.most_common()[:15]","78650aba":"def map_weather(txt):\n    ans = 1\n    if pd.isna(txt):\n        return 0\n    if 'partly' in txt:\n        ans*=0.5\n    if 'climate controlled' in txt or 'indoor' in txt:\n        return ans*3\n    if 'sunny' in txt or 'sun' in txt:\n        return ans*2\n    if 'clear' in txt:\n        return ans\n    if 'cloudy' in txt:\n        return -ans\n    if 'rain' in txt or 'rainy' in txt:\n        return -2*ans\n    if 'snow' in txt:\n        return -3*ans\n    return 0","ed591e93":"train['GameWeather'] = train['GameWeather'].apply(map_weather)","142e4f1f":"train['IsRusher'] = train['NflId'] == train['NflIdRusher']","60b66bf7":"train.drop(['NflId', 'NflIdRusher'], axis=1, inplace=True)","853c255d":"train = train.sort_values(by=['PlayId', 'Team', 'IsRusher']).reset_index()","23b82416":"train.drop(['GameId', 'PlayId', 'index', 'IsRusher', 'Team', 'Season'], axis=1, inplace=True)","15c78f78":"cat_features = []\nfor col in train.columns:\n    if train[col].dtype =='object':\n        cat_features.append(col)\n        \ntrain = train.drop(cat_features, axis=1)","09bd1b1d":"train.fillna(-999, inplace=True)","cedcf7d4":"players_col = []\nfor col in train.columns:\n    if train[col][:22].std()!=0:\n        players_col.append(col)","0a6f1411":"X_train = np.array(train[players_col]).reshape(-1, 11*22)","91fbc8d4":"play_col = train.drop(players_col+['Yards'], axis=1).columns\nX_play_col = np.zeros(shape=(X_train.shape[0], len(play_col)))\nfor i, col in enumerate(play_col):\n    X_play_col[:, i] = train[col][::22]","9e8496ea":"X_train = np.concatenate([X_train, X_play_col], axis=1)\ny_train = np.zeros(shape=(X_train.shape[0], 199))\nfor i,yard in enumerate(train['Yards'][::22]):\n    y_train[i, yard+99:] = np.ones(shape=(1, 100-yard))","b45d8027":"from keras.callbacks import EarlyStopping","a4969d9e":"from keras import backend as K\n\n\n__all__ = ['RAdam']\n\n\nclass RAdam(keras.optimizers.Optimizer):\n    \"\"\"RAdam optimizer.\n    # Arguments\n        learning_rate: float >= 0. Learning rate.\n        beta_1: float, 0 < beta < 1. Generally close to 1.\n        beta_2: float, 0 < beta < 1. Generally close to 1.\n        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n        decay: float >= 0. Learning rate decay over each update.\n        weight_decay: float >= 0. Weight decay for each param.\n        amsgrad: boolean. Whether to apply the AMSGrad variant of this\n            algorithm from the paper \"On the Convergence of Adam and\n            Beyond\".\n        total_steps: int >= 0. Total number of training steps. Enable warmup by setting a positive value.\n        warmup_proportion: 0 < warmup_proportion < 1. The proportion of increasing steps.\n        min_lr: float >= 0. Minimum learning rate after warmup.\n    # References\n        - [Adam - A Method for Stochastic Optimization](https:\/\/arxiv.org\/abs\/1412.6980v8)\n        - [On the Convergence of Adam and Beyond](https:\/\/openreview.net\/forum?id=ryQu7f-RZ)\n        - [On The Variance Of The Adaptive Learning Rate And Beyond](https:\/\/arxiv.org\/pdf\/1908.03265v1.pdf)\n    \"\"\"\n\n    def __init__(self, learning_rate=0.001, beta_1=0.9, beta_2=0.999,\n                 epsilon=None, decay=0., weight_decay=0., amsgrad=False,\n                 total_steps=0, warmup_proportion=0.1, min_lr=0., **kwargs):\n        learning_rate = kwargs.pop('lr', learning_rate)\n        super(RAdam, self).__init__(**kwargs)\n        with K.name_scope(self.__class__.__name__):\n            self.iterations = K.variable(0, dtype='int64', name='iterations')\n            self.learning_rate = K.variable(learning_rate, name='learning_rate')\n            self.beta_1 = K.variable(beta_1, name='beta_1')\n            self.beta_2 = K.variable(beta_2, name='beta_2')\n            self.decay = K.variable(decay, name='decay')\n            self.weight_decay = K.variable(weight_decay, name='weight_decay')\n            self.total_steps = K.variable(total_steps, name='total_steps')\n            self.warmup_proportion = K.variable(warmup_proportion, name='warmup_proportion')\n            self.min_lr = K.variable(min_lr, name='min_lr')\n        if epsilon is None:\n            epsilon = K.epsilon()\n        self.epsilon = epsilon\n        self.initial_decay = decay\n        self.initial_weight_decay = weight_decay\n        self.initial_total_steps = total_steps\n        self.amsgrad = amsgrad\n\n    def get_updates(self, loss, params):\n        grads = self.get_gradients(loss, params)\n        self.updates = [K.update_add(self.iterations, 1)]\n\n        lr = self.lr\n\n        if self.initial_decay > 0:\n            lr = lr * (1. \/ (1. + self.decay * K.cast(self.iterations, K.dtype(self.decay))))\n\n        t = K.cast(self.iterations, K.floatx()) + 1\n\n        if self.initial_total_steps > 0:\n            warmup_steps = self.total_steps * self.warmup_proportion\n            decay_steps = K.maximum(self.total_steps - warmup_steps, 1)\n            decay_rate = (self.min_lr - lr) \/ decay_steps\n            lr = K.switch(\n                t <= warmup_steps,\n                lr * (t \/ warmup_steps),\n                lr + decay_rate * K.minimum(t - warmup_steps, decay_steps),\n            )\n\n        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p), name='m_' + str(i)) for (i, p) in enumerate(params)]\n        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p), name='v_' + str(i)) for (i, p) in enumerate(params)]\n\n        if self.amsgrad:\n            vhats = [K.zeros(K.int_shape(p), dtype=K.dtype(p), name='vhat_' + str(i)) for (i, p) in enumerate(params)]\n        else:\n            vhats = [K.zeros(1, name='vhat_' + str(i)) for i in range(len(params))]\n\n        self.weights = [self.iterations] + ms + vs + vhats\n\n        beta_1_t = K.pow(self.beta_1, t)\n        beta_2_t = K.pow(self.beta_2, t)\n\n        sma_inf = 2.0 \/ (1.0 - self.beta_2) - 1.0\n        sma_t = sma_inf - 2.0 * t * beta_2_t \/ (1.0 - beta_2_t)\n\n        for p, g, m, v, vhat in zip(params, grads, ms, vs, vhats):\n            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n\n            m_corr_t = m_t \/ (1.0 - beta_1_t)\n            if self.amsgrad:\n                vhat_t = K.maximum(vhat, v_t)\n                v_corr_t = K.sqrt(vhat_t \/ (1.0 - beta_2_t))\n                self.updates.append(K.update(vhat, vhat_t))\n            else:\n                v_corr_t = K.sqrt(v_t \/ (1.0 - beta_2_t))\n\n            r_t = K.sqrt((sma_t - 4.0) \/ (sma_inf - 4.0) *\n                         (sma_t - 2.0) \/ (sma_inf - 2.0) *\n                         sma_inf \/ sma_t)\n\n            p_t = K.switch(sma_t >= 5, r_t * m_corr_t \/ (v_corr_t + self.epsilon), m_corr_t)\n\n            if self.initial_weight_decay > 0:\n                p_t += self.weight_decay * p\n\n            p_t = p - lr * p_t\n\n            self.updates.append(K.update(m, m_t))\n            self.updates.append(K.update(v, v_t))\n            new_p = p_t\n\n            # Apply constraints.\n            if getattr(p, 'constraint', None) is not None:\n                new_p = p.constraint(new_p)\n\n            self.updates.append(K.update(p, new_p))\n        return self.updates\n\n    @property\n    def lr(self):\n        return self.learning_rate\n\n    @lr.setter\n    def lr(self, learning_rate):\n        self.learning_rate = learning_rate\n\n    def get_config(self):\n        config = {\n            'learning_rate': float(K.get_value(self.learning_rate)),\n            'beta_1': float(K.get_value(self.beta_1)),\n            'beta_2': float(K.get_value(self.beta_2)),\n            'decay': float(K.get_value(self.decay)),\n            'weight_decay': float(K.get_value(self.weight_decay)),\n            'epsilon': self.epsilon,\n            'amsgrad': self.amsgrad,\n            'total_steps': float(K.get_value(self.total_steps)),\n            'warmup_proportion': float(K.get_value(self.warmup_proportion)),\n            'min_lr': float(K.get_value(self.min_lr)),\n        }\n        base_config = super(RAdam, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))","e59935a2":"from keras.layers import Layer\n\nclass Mish(Layer):\n    def __init__(self, **kwargs):\n        super(Mish, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        super(Mish, self).build(input_shape)\n\n    def call(self, x):\n        return x * K.tanh(K.softplus(x))\n\n    def compute_output_shape(self, input_shape):\n        return input_shape","24b65d3a":"from keras.callbacks import *\n\nclass CyclicLR(Callback):\n    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n    The method cycles the learning rate between two boundaries with\n    some constant frequency, as detailed in this paper (https:\/\/arxiv.org\/abs\/1506.01186).\n    The amplitude of the cycle can be scaled on a per-iteration or \n    per-cycle basis.\n    This class has three built-in policies, as put forth in the paper.\n    \"triangular\":\n        A basic triangular cycle w\/ no amplitude scaling.\n    \"triangular2\":\n        A basic triangular cycle that scales initial amplitude by half each cycle.\n    \"exp_range\":\n        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n        cycle iteration.\n    For more detail, please see paper.\n    \n    # Example\n        ```python\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., mode='triangular')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```\n    \n    Class also supports custom scaling functions:\n        ```python\n            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi\/2.))\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., scale_fn=clr_fn,\n                                scale_mode='cycle')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```    \n    # Arguments\n        base_lr: initial learning rate which is the\n            lower boundary in the cycle.\n        max_lr: upper boundary in the cycle. Functionally,\n            it defines the cycle amplitude (max_lr - base_lr).\n            The lr at any cycle is the sum of base_lr\n            and some scaling of the amplitude; therefore \n            max_lr may not actually be reached depending on\n            scaling function.\n        step_size: number of training iterations per\n            half cycle. Authors suggest setting step_size\n            2-8 x training iterations in epoch.\n        mode: one of {triangular, triangular2, exp_range}.\n            Default 'triangular'.\n            Values correspond to policies detailed above.\n            If scale_fn is not None, this argument is ignored.\n        gamma: constant in 'exp_range' scaling function:\n            gamma**(cycle iterations)\n        scale_fn: Custom scaling policy defined by a single\n            argument lambda function, where \n            0 <= scale_fn(x) <= 1 for all x >= 0.\n            mode paramater is ignored \n        scale_mode: {'cycle', 'iterations'}.\n            Defines whether scale_fn is evaluated on \n            cycle number or cycle iterations (training\n            iterations since start of cycle). Default is 'cycle'.\n    \"\"\"\n\n    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n                 gamma=1., scale_fn=None, scale_mode='cycle'):\n        super(CyclicLR, self).__init__()\n\n        self.base_lr = base_lr\n        self.max_lr = max_lr\n        self.step_size = step_size\n        self.mode = mode\n        self.gamma = gamma\n        if scale_fn == None:\n            if self.mode == 'triangular':\n                self.scale_fn = lambda x: 1.\n                self.scale_mode = 'cycle'\n            elif self.mode == 'triangular2':\n                self.scale_fn = lambda x: 1\/(2.**(x-1))\n                self.scale_mode = 'cycle'\n            elif self.mode == 'exp_range':\n                self.scale_fn = lambda x: gamma**(x)\n                self.scale_mode = 'iterations'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n        self.clr_iterations = 0.\n        self.trn_iterations = 0.\n        self.history = {}\n\n        self._reset()\n\n    def _reset(self, new_base_lr=None, new_max_lr=None,\n               new_step_size=None):\n        \"\"\"Resets cycle iterations.\n        Optional boundary\/step size adjustment.\n        \"\"\"\n        if new_base_lr != None:\n            self.base_lr = new_base_lr\n        if new_max_lr != None:\n            self.max_lr = new_max_lr\n        if new_step_size != None:\n            self.step_size = new_step_size\n        self.clr_iterations = 0.\n        \n    def clr(self):\n        cycle = np.floor(1+self.clr_iterations\/(2*self.step_size))\n        x = np.abs(self.clr_iterations\/self.step_size - 2*cycle + 1)\n        if self.scale_mode == 'cycle':\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n        else:\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n        \n    def on_train_begin(self, logs={}):\n        logs = logs or {}\n\n        if self.clr_iterations == 0:\n            K.set_value(self.model.optimizer.lr, self.base_lr)\n        else:\n            K.set_value(self.model.optimizer.lr, self.clr())        \n            \n    def on_batch_end(self, epoch, logs=None):\n        \n        logs = logs or {}\n        self.trn_iterations += 1\n        self.clr_iterations += 1\n\n        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n        self.history.setdefault('iterations', []).append(self.trn_iterations)\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n        \n        K.set_value(self.model.optimizer.lr, self.clr())","2fab05cc":"import tensorflow as tf","4d202670":"def train_model(x_tr, y_tr, x_vl, y_vl):\n    inp = keras.layers.Input([X_train.shape[1]])\n    x = keras.layers.Dense(units=324)(inp)\n    x = keras.layers.BatchNormalization(momentum=0.8, axis=1)(x) \n    x = Mish()(x) \n    x = keras.layers.Dropout(0.35)(x) \n        \n    x = keras.layers.Dense(units=512)(x) \n    x = keras.layers.BatchNormalization(momentum=0.8, axis=1)(x) \n    x = Mish()(x) \n    x = keras.layers.Dropout(0.5)(x) \n        \n    x = keras.layers.Dense(units=324)(x) \n    x = keras.layers.BatchNormalization(momentum=0.8, axis=1)(x) \n    x = Mish()(x) \n    x = keras.layers.Dropout(0.35)(x)\n    \n    x = keras.layers.Dense(units=199, activation='sigmoid')(x) \n    \n    model = keras.Model(inp, x) \n    er = EarlyStopping(patience=15, min_delta=1e-4, restore_best_weights=True, monitor='val_loss')\n    clr = CyclicLR(base_lr=1e-3, max_lr=5*1e-3, step_size = 1000, gamma = 0.99)\n    model.compile(optimizer=RAdam(warmup_proportion=0.1, min_lr=1e-5), \n                  loss='mse', \n                  metrics = ['mse']\n                 )\n    model.fit(x_tr, y_tr, epochs=100, callbacks=[clr, er], validation_data=[x_vl, y_vl])\n    return model","8c912c42":"scaler = StandardScaler() \nX_train = scaler.fit_transform(X_train) ","6edb9e26":"def make_pred(df, sample, env, models):\n    df['OffenseFormation'] = df['OffenseFormation'].apply(lambda x: x if x in off_form else np.nan)\n    df = pd.concat([df.drop(['OffenseFormation'], axis=1), pd.get_dummies(df['OffenseFormation'], prefix='Formation')], axis=1)\n    missing_cols = set( dummy_col ) - set( test.columns )-set('Yards')\n    for c in missing_cols:\n        df[c] = 0\n    df = df[dummy_col]\n    df.drop(['Yards'], axis=1, inplace=True)\n    df['GameClock'] = df['GameClock'].apply(strtoseconds)\n    df['PlayerHeight'] = df['PlayerHeight'].apply(lambda x: 12*int(x.split('-')[0])+int(x.split('-')[1]))\n    df['TimeHandoff'] = df['TimeHandoff'].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%fZ\"))\n    df['TimeSnap'] = df['TimeSnap'].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%fZ\"))\n    df['TimeDelta'] = df.apply(lambda row: (row['TimeHandoff'] - row['TimeSnap']).total_seconds(), axis=1)\n    df['PlayerBirthDate'] = df['PlayerBirthDate'].apply(lambda x: datetime.datetime.strptime(x, \"%m\/%d\/%Y\"))\n    seconds_in_year = 60*60*24*365.25\n    df['PlayerAge'] = df.apply(lambda row: (row['TimeHandoff']-row['PlayerBirthDate']).total_seconds()\/seconds_in_year, axis=1)\n    df['WindSpeed'] = df['WindSpeed'].apply(lambda x: x.lower().replace('mph', '').strip() if not pd.isna(x) else x)\n    df['WindSpeed'] = df['WindSpeed'].apply(lambda x: (int(x.split('-')[0])+int(x.split('-')[1]))\/2 if not pd.isna(x) and '-' in x else x)\n    df['WindSpeed'] = df['WindSpeed'].apply(lambda x: (int(x.split()[0])+int(x.split()[-1]))\/2 if not pd.isna(x) and type(x)!=float and 'gusts up to' in x else x)\n    df['WindSpeed'] = df['WindSpeed'].apply(str_to_float)\n    df['PlayDirection'] = train['PlayDirection'].apply(lambda x: x is 'right')\n    df['Team'] = df['Team'].apply(lambda x: x.strip()=='home')\n    indoor = \"indoor\"\n    df['GameWeather'] = df['GameWeather'].apply(lambda x: indoor if not pd.isna(x) and indoor in x else x)\n    df['GameWeather'] = df['GameWeather'].apply(lambda x: x.lower().replace('coudy', 'cloudy').replace('clouidy', 'cloudy').replace('party', 'partly').replace('clear and sunny', 'sunny and clear').replace('skies', '').replace(\"mostly\", \"\").strip() if not pd.isna(x) else x)\n    df['GameWeather'] = df['GameWeather'].apply(map_weather)\n    df['IsRusher'] = df['NflId'] == df['NflIdRusher']\n    \n    df = df.sort_values(by=['PlayId', 'Team', 'IsRusher']).reset_index()\n    df = df.drop(['TimeHandoff', 'TimeSnap', 'PlayerBirthDate', 'WindDirection', 'NflId', 'NflIdRusher', 'GameId', 'PlayId', 'index', 'IsRusher', 'Team', 'Season'], axis=1)\n    cat_features = []\n    for col in df.columns:\n        if df[col].dtype =='object':\n            cat_features.append(col)\n\n    df = df.drop(cat_features, axis=1)\n    df.fillna(-999, inplace=True)\n    X = np.array(df[players_col]).reshape(-1, 11*22)\n    play_col = df.drop(players_col, axis=1).columns\n    X_play_col = np.zeros(shape=(X.shape[0], len(play_col)))\n    for i, col in enumerate(play_col):\n        X_play_col[:, i] = df[col][::22]\n    X = scaler.transform(np.concatenate([X, X_play_col], axis=1))\n    y_pred = np.mean([model.predict(X) for model in models], axis=0)\n    for pred in y_pred:\n        prev = 0\n        for i in range(len(pred)):\n            if pred[i]<prev:\n                pred[i]=prev\n            prev=pred[i]\n    \n    env.predict(pd.DataFrame(data=y_pred,columns=sample.columns))\n    return y_pred","5a8c7183":"from sklearn.model_selection import RepeatedKFold\n\nrkf = RepeatedKFold(n_splits=5, n_repeats=5)","3e38e0f4":"from keras import backend as K","11a68904":"models = []\n\nfor tr_idx, vl_idx in rkf.split(X_train, y_train):\n    \n    x_tr, y_tr = X_train[tr_idx], y_train[tr_idx]\n    x_vl, y_vl = X_train[vl_idx], y_train[vl_idx]\n    \n    model = train_model(x_tr, y_tr, x_vl, y_vl)\n    models.append(model)","615800d4":"env = nflrush.make_env()","4bf8dc20":"for test, sample in tqdm.tqdm(env.iter_test()):\n    make_pred(test, sample, env, models)","8309f30f":"env.write_submission_file()","3fa9cf1b":"Since we already have the quarter feature, we can just divide the Game Clock by 15 minutes so we can get the normalized time left in the quarter.","cadad9b7":"Let's preprocess some of those features.","4a8487d5":"We can see there are some values that are not standardized(e.g. 12mph), we are going to remove mph from all our values.","8a9e7440":"We know that 1ft=12in, thus:","0db8a6df":"We are going to apply the following preprocessing:\n \n- Lower case\n- N\/A Indoor, N\/A (Indoors) and Indoor => indoor Let's try to cluster those together.\n- coudy and clouidy => cloudy\n- party => partly\n- sunny and clear => clear and sunny\n- skies and mostly => \"\"","d8cd0a38":"# End\n\nIf you reached this far please comment and upvote this kernel, feel free to make improvements on the kernel and please share if you found anything useful!","a13874b2":"## Team","ba7c993b":"# Overall analysis","09157e87":"## Offense formation","08f667a5":"# Baseline model","cd7a56a3":"We are now going to make one big row for each play where the rusher is the last one","691c797b":"## NflId NflIdRusher","ffe05ae1":"## Wind Speed and Direction","91611529":"Let's now look at the most common words we have in the weather description","0d86cd7c":"As expected, we have 22 of each playid since we have 22 players.\n\nLet's look at our target variable(Yards).","9fa69923":"Game clock is supposed to be a numerical feature.","13012e97":"## Game Weather","9dd40837":"The wind direction won't affect our model much because we are analyzing running plays so we are just going to drop it.","c8240335":"- Let's see how PlayId is distribuited","66a0e7b6":"## Player height","04909965":"Let's use the time handoff to calculate the players age","e48e49ce":"# Categorical features","c085a99a":"Let's drop the categorical features and run a simple random forest in our model","e67e9979":"To encode our weather we are going to do the following map:\n \n- climate controlled or indoor => 3, sunny or sun => 2, clear => 1, cloudy => -1, rain => -2, snow => -3, others => 0\n- partly => multiply by 0.5\n\nI don't have any expercience with american football so I don't know if playing in a climate controlled or indoor stadium is good or not, if someone has a good idea on how to encode this it would be nice to leave it in the comments :)","9446564f":"Since I don't have any knowledge about formations, I am just goig to one-hot encode this feature","4df5b71e":"## Game Clock","5dff778c":"## PlayDirection","6358ab16":"## Time handoff and snap and Player BirthDate"}}