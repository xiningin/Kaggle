{"cell_type":{"d832a6d3":"code","c74242e4":"code","66a87a4f":"code","800bdda4":"code","a816d874":"code","1f232dde":"code","0f7b3551":"code","7f0dc036":"code","95d52557":"code","a81e2d94":"code","efe3dba4":"code","9d923f2d":"code","55b56016":"code","49cea7a3":"code","a4decb38":"code","3096dfc7":"code","65bebeb8":"code","9b2beeb1":"code","d8faec1c":"code","b5ba0add":"code","7b76f8f0":"code","f343f651":"markdown","0154fe8e":"markdown","2fff3574":"markdown","539a83c8":"markdown"},"source":{"d832a6d3":"import sys\n# Pytorch\u306eBERT\uff08github\u304b\u3089\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u305f\u30b3\u30fc\u30c9\uff09\u3092\u8aad\u307f\u8fbc\u3093\u3067\u3044\u308b\npackage_dir = \"..\/input\/ppbert\/pytorch-pretrained-bert\/pytorch-pretrained-BERT\"\nsys.path.append(package_dir)","c74242e4":"# \u5225\u306e\u5834\u6240\u304b\u3089import\u3057\u3066\u3044\u308b\u30d5\u30a1\u30a4\u30eb\uff08Pytorch\u306eBERT\u5b9f\u88c5\u30d5\u30a1\u30a4\u30eb\uff09\u3092\u81ea\u52d5\u8aad\u307f\u8fbc\u307f\n%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n\nimport fastai\nfrom fastai.train import Learner\nfrom fastai.train import DataBunch\nfrom fastai.callbacks import *\nfrom fastai.basic_data import DatasetType\nimport fastprogress\nfrom fastprogress import force_console_behavior\nimport numpy as np\nfrom pprint import pprint\nimport pandas as pd\nimport os\nimport time\nimport gc\nimport random\nfrom tqdm._tqdm_notebook import tqdm_notebook as tqdm\nfrom keras.preprocessing import text, sequence\nimport torch\nfrom torch import nn\nfrom torch.utils import data\nfrom torch.nn import functional as F\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport torch.utils.data\nfrom tqdm import tqdm\nimport warnings\nfrom pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification, BertAdam\nfrom pytorch_pretrained_bert import BertConfig\nfrom nltk.tokenize.treebank import TreebankWordTokenizer\nfrom scipy.stats import rankdata\n\nfrom gensim.models import KeyedVectors","66a87a4f":"# \u30c7\u30fc\u30bf\u3092\u5b66\u7fd2\u7528\u306b\u5206\u5272\ndef convert_lines(example, max_seq_length,tokenizer):\n    # \u4e00\u56de\u306e\u5b66\u7fd2\u3067\u4f7f\u7528\u3059\u308b\u6587\u7ae0\u306e\u5358\u8a9e\u6570\n    max_seq_length -=2\n    all_tokens = []\n    longer = 0\n    for text in tqdm(example):\n        # BERT\u30bb\u30eb\u306b\u5358\u8a9e\u3092\u5165\u529b\uff1f\n        tokens_a = tokenizer.tokenize(text)\n        if len(tokens_a)>max_seq_length:\n            # \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u304c\u9577\u3059\u304e\u305f\u3089\u3001\u9014\u4e2d\u3067\u5207\u308b\uff1f\n            tokens_a = tokens_a[:max_seq_length]\n            longer += 1\n        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n        all_tokens.append(one_token)\n    return np.array(all_tokens)\n\ndef is_interactive():\n    return 'SHLVL' not in os.environ\n\ndef seed_everything(seed=123):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \ndef get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\n# embedding\u30ec\u30a4\u30e4\u3092\u4f5c\u6210\ndef load_embeddings(path):\n    #with open(path,'rb') as f:\n    emb_arr = KeyedVectors.load(path)\n    return emb_arr\n\n# \u5358\u8a9e\u306e\u5206\u6563\u8868\u73fe\uff08\u30d9\u30af\u30c8\u30eb\uff09\u3092\u53d6\u5f97\ndef build_matrix(word_index, path):\n    # embedding\u30ec\u30a4\u30e4\u3092\u4f5c\u6210\uff08\u30cd\u30c3\u30c8\u3067\u516c\u958b\u3055\u308c\u3066\u3044\u308b\u7269\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\uff09\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((max_features + 1, 300))\n    unknown_words = []\n    \n    for word, i in word_index.items():\n        if i <= max_features:\n            try:\n                # embedding\u30ec\u30a4\u30e4\u3067\u30c7\u30fc\u30bf\u5185\u306e\u5358\u8a9e\u3092\u30d9\u30af\u30c8\u30eb\u5316\n                embedding_matrix[i] = embedding_index[word]\n            except KeyError:\n                try:\n                    # embedding\u30ec\u30a4\u30e4\u3067\u30c7\u30fc\u30bf\u5185\u306e\u5358\u8a9e\uff08\u5c0f\u6587\u5b57\uff09\u3092\u30d9\u30af\u30c8\u30eb\u5316\n                    embedding_matrix[i] = embedding_index[word.lower()]\n                except KeyError:\n                    try:\n                        # embedding\u30ec\u30a4\u30e4\u3067\u30c7\u30fc\u30bf\u5185\u306e\u5358\u8a9e\uff08\u5148\u982d\u306e\u6587\u5b57\u304c\u5927\u6587\u5b57\u3001\u4e8c\u756a\u76ee\u4ee5\u964d\u306e\u6587\u5b57\u304c\u5c0f\u6587\u5b57\uff09\u3092\u30d9\u30af\u30c8\u30eb\u5316\n                        embedding_matrix[i] = embedding_index[word.title()]\n                    except KeyError:\n                        # embedding\u30ec\u30a4\u30e4\u306b\u8a18\u9332\u3055\u308c\u3066\u3044\u306a\u304b\u3063\u305f\u3089unknown_word\u3068\u3057\u3066\u51e6\u7406\n                        unknown_words.append(word)\n    return embedding_matrix, unknown_words\n\ndef sigmoid(x):\n    return 1 \/ (1 + np.exp(-x))\n\nclass SpatialDropout(nn.Dropout2d):\n    def forward(self, x):\n        x = x.unsqueeze(2)    # (N, T, 1, K)\n        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n        x = x.squeeze(2)  # (N, T, K)\n        return x\n\ndef train_model(learn,test,output_dim,lr=0.001,\n                batch_size=512, n_epochs=3,\n                enable_checkpoint_ensemble=True):\n    \n    all_test_preds = []\n    checkpoint_weights = [2 ** epoch for epoch in range(n_epochs)]\n    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n    n = len(learn.data.train_dl)\n    phases = [(TrainingPhase(n).schedule_hp('lr', lr * (0.6**(i)))) for i in range(n_epochs)]\n    sched = GeneralScheduler(learn, phases)\n    learn.callbacks.append(sched)\n    for epoch in range(n_epochs):\n        learn.fit(1)\n        test_preds = np.zeros((len(test), output_dim))    \n        for i, x_batch in enumerate(test_loader):\n            X = x_batch[0].cuda()\n            y_pred = sigmoid(learn.model(X).detach().cpu().numpy())\n            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred\n\n        all_test_preds.append(test_preds)\n\n\n    if enable_checkpoint_ensemble:\n        test_preds = np.average(all_test_preds, weights=checkpoint_weights, axis=0)    \n    else:\n        test_preds = all_test_preds[-1]\n        \n    return test_preds\n\n# \u7279\u6b8a\u6587\u5b57\u306e\u53d6\u5f97\u3068\u8981\u3089\u306a\u3044\u6587\u5b57\u306e\u524a\u9664\ndef handle_punctuation(x):\n    x = x.translate(remove_dict)\n    x = x.translate(isolate_dict)\n    return x\n\n# Treebank\u306b\u8a13\u7df4\u30c7\u30fc\u30bf\u3092\u30bb\u30c3\u30c8\ndef handle_contractions(x):\n    x = tokenizer.tokenize(x)\n    return x\n\n# \u30c7\u30fc\u30bf\u306e\u6700\u521d\u306e\u30b7\u30f3\u30b0\u30eb\u30af\u30a9\u30fc\u30c8\u3092\u30b9\u30da\u30fc\u30b9\u306b\u7f6e\u63db\ndef fix_quote(x):\n    x = [x_[1:] if x_.startswith(\"'\") else x_ for x_ in x]\n    x = ' '.join(x)\n    return x\n\n# \u30c7\u30fc\u30bf\u306e\u6574\u5f62\ndef preprocess(x):\n    x = handle_punctuation(x)\n    x = handle_contractions(x)\n    x = fix_quote(x)\n    return x\n\n# \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092minibatch\u6bce\u306b\u914d\u308b\u30c7\u30fc\u30bf\u306b\u5206\u5272\n# \u6b63\u76f4\u3001\u826f\u304f\u5206\u304b\u3089\u3093\u2026\nclass SequenceBucketCollator():\n    def __init__(self, choose_length, sequence_index, length_index, label_index=None):\n        self.choose_length = choose_length\n        self.sequence_index = sequence_index\n        self.length_index = length_index\n        self.label_index = label_index\n        \n    def __call__(self, batch):\n        batch = [torch.stack(x) for x in list(zip(*batch))]\n        \n        sequences = batch[self.sequence_index]\n        lengths = batch[self.length_index]\n        \n        length = self.choose_length(lengths)\n        mask = torch.arange(start=maxlen, end=0, step=-1) < length\n        padded_sequences = sequences[:, mask]\n        \n        batch[self.sequence_index] = padded_sequences\n        \n        if self.label_index is not None:\n            return [x for i, x in enumerate(batch) if i != self.label_index], batch[self.label_index]\n    \n        return batch\n\n# \u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u3092\u4f5c\u6210\nclass NeuralNet(nn.Module):\n    def __init__(self, embedding_matrix, num_aux_targets):\n        super(NeuralNet, self).__init__()\n        embed_size = embedding_matrix.shape[1]\n        \n        self.embedding = nn.Embedding(max_features, embed_size)\n        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.embedding.weight.requires_grad = False\n        self.embedding_dropout = SpatialDropout(0.3)\n        \n        self.lstm1 = nn.LSTM(embed_size, LSTM_UNITS, bidirectional=True, batch_first=True)\n        self.lstm2 = nn.LSTM(LSTM_UNITS * 2, LSTM_UNITS, bidirectional=True, batch_first=True)\n    \n        self.linear1 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n        self.linear2 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n        \n        self.linear_out = nn.Linear(DENSE_HIDDEN_UNITS, 1)\n        self.linear_aux_out = nn.Linear(DENSE_HIDDEN_UNITS, num_aux_targets)\n        \n    def forward(self, x, lengths=None):\n        h_embedding = self.embedding(x.long())\n        h_embedding = self.embedding_dropout(h_embedding)\n        \n        h_lstm1, _ = self.lstm1(h_embedding)\n        h_lstm2, _ = self.lstm2(h_lstm1)\n        \n        # global average pooling\n        avg_pool = torch.mean(h_lstm2, 1)\n        # global max pooling\n        max_pool, _ = torch.max(h_lstm2, 1)\n        \n        h_conc = torch.cat((max_pool, avg_pool), 1)\n        h_conc_linear1  = F.relu(self.linear1(h_conc))\n        h_conc_linear2  = F.relu(self.linear2(h_conc))\n        \n        hidden = h_conc + h_conc_linear1 + h_conc_linear2\n        \n        result = self.linear_out(hidden)\n        aux_result = self.linear_aux_out(hidden)\n        out = torch.cat([result, aux_result], 1)\n        \n        return out\n    \ndef custom_loss(data, targets):\n    bce_loss_1 = nn.BCEWithLogitsLoss(weight=targets[:,1:2])(data[:,:1],targets[:,:1])\n    bce_loss_2 = nn.BCEWithLogitsLoss()(data[:,1:],targets[:,2:])\n    return (bce_loss_1 * loss_weight) + bce_loss_2\n\n# \u30c7\u30fc\u30bf\u5bb9\u91cf\u3092\u6700\u5c0f\u5316\ndef reduce_mem_usage(df):\n    # \u73fe\u5728\u306e\u30c7\u30fc\u30bf\u5bb9\u91cf\u3092\u8a18\u9332\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    # \u30c7\u30fc\u30bf\u5bb9\u91cf\u3092\u30ab\u30e9\u30e0\u6bce\u306b\u5727\u7e2e\n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        # \u30ab\u30e9\u30e0\u304cobject\uff08\u6587\u5b57\u5217\u578b\uff09\u4ee5\u5916\u306e\u30c7\u30fc\u30bf\u3092\u5727\u7e2e\n        if col_type != object:\n            # \u30ab\u30e9\u30e0\uff08\u6570\u5024\u578b\uff09\u306e\u6700\u5c0f\u5024\u3001\u6700\u5927\u5024\u3092\u8a18\u9332\n            c_min = df[col].min()\n            c_max = df[col].max()\n            # \u30ab\u30e9\u30e0\u306e\u578b\u304cint\u578b\u306e\u7269\u3092\u5727\u7e2e\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df\n\ndef ensemble_predictions(predictions, weights, type_=\"linear\"):\n    assert np.isclose(np.sum(weights), 1.0)\n    if type_ == \"linear\":\n        res = np.average(predictions, weights=weights, axis=0)\n    elif type_ == \"harmonic\":\n        res = np.average([1 \/ p for p in predictions], weights=weights, axis=0)\n        return 1 \/ res\n    elif type_ == \"geometric\":\n        numerator = np.average(\n            [np.log(p) for p in predictions], weights=weights, axis=0\n        )\n        res = np.exp(numerator \/ sum(weights))\n        return res\n    elif type_ == \"rank\":\n        res = np.average([rankdata(p) for p in predictions], weights=weights, axis=0)\n        return res \/ (len(res) + 1)\n    return res","800bdda4":"# \u8a2d\u5b9a\u3055\u308c\u305f\u30a8\u30e9\u30fc\u306e\u5185\u3001\u6700\u521d\u306b\u691c\u51fa\u3055\u308c\u305f\u7269\u3060\u3051\u3092\u8868\u793a\u3002\nwarnings.filterwarnings(action='once')\ndevice = torch.device('cuda')\nMAX_SEQUENCE_LENGTH = 300\nSEED = 1234\nBATCH_SIZE = 512\nBERT_MODEL_PATH = '..\/input\/bert-pretrained-models\/uncased_l-12_h-768_a-12\/uncased_L-12_H-768_A-12\/'\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\n# \u4e88\u6e2c\u65b9\u6cd5\u3092\u6c7a\u5b9a\u7406\u8ad6\uff08\u5b66\u7fd2\u3055\u308c\u305f\u4e8b\u5f8c\u78ba\u7387P(A|B)\u304b\u3089\u4e88\u6e2c\u3092\u7b97\u51fa\uff09\u306b\u6307\u5b9a\ntorch.backends.cudnn.deterministic = True\n# Bert\u30e2\u30c7\u30eb\u3092\u5b9a\u7fa9\nbert_config = BertConfig('..\/input\/bert-inference\/bert\/bert_config.json')\ntokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None,do_lower_case=True)\n\n# \u30d7\u30ed\u30b0\u30ec\u30b9\u30d0\u30fc\u306e\u8a2d\u5b9a\ntqdm.pandas()\nCRAWL_EMBEDDING_PATH = '..\/input\/gensim-embeddings-dataset\/crawl-300d-2M.gensim'\nGLOVE_EMBEDDING_PATH = '..\/input\/gensim-embeddings-dataset\/glove.840B.300d.gensim'\nNUM_MODELS = 2\nLSTM_UNITS = 128\nDENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\nMAX_LEN = 220\n# \u30d7\u30ed\u30b0\u30ec\u30b9\u30d0\u30fc\u306e\u8868\u793a\u8a2d\u5b9a\nif not is_interactive():\n    def nop(it, *a, **k):\n        return it\n\n    tqdm = nop\n\n    fastprogress.fastprogress.NO_BAR = True\n    master_bar, progress_bar = force_console_behavior()\n    fastai.basic_train.master_bar, fastai.basic_train.progress_bar = master_bar, progress_bar\n\nseed_everything()","a816d874":"# \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f\ntest_df = pd.read_csv(\"..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv\")\n# \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3092str\u578b\u306b\u5909\u63db\ntest_df['comment_text'] = test_df['comment_text'].astype(str) \n# \u6b20\u640d\u5024\u306b\u5024\u3092\u633f\u5165\uff08DUMMY_VARUE\u304c\u4f55\u306e\u30aa\u30d7\u30b7\u30e7\u30f3\u304b\u5206\u304b\u3089\u306a\u3044\uff09\n# \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3092\u4e00\u56de\u306e\u5b66\u7fd2\u6bce\u306e\u30b5\u30a4\u30ba\uff08\u6587\u7ae0\uff09\u306b\u5206\u5272\uff1f\nX_test = convert_lines(test_df[\"comment_text\"].fillna(\"DUMMY_VALUE\"), MAX_SEQUENCE_LENGTH, tokenizer)","1f232dde":"# Bert\u30e2\u30c7\u30eb\u3092\u4f5c\u6210\nmodel = BertForSequenceClassification(bert_config, num_labels=1)\nmodel.load_state_dict(torch.load(\"..\/input\/bert-inference\/bert\/bert_pytorch.bin\"))\n# cuda\u30e1\u30e2\u30ea\u306b\u4f5c\u6210\u3057\u305fBert\u30e2\u30c7\u30eb\u3092\u4fdd\u5b58\nmodel.to(device)\n# \u304d\u3081\u7d30\u304b\u3044\u6570\u5024\u6574\u5f62\uff08requires_grad\uff09\u3092\u884c\u308f\u306a\u3044\uff1f\n# \u6301\u3063\u3066\u304d\u305fBert\u30e2\u30c7\u30eb\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u5b66\u7fd2\u3057\u306a\u3044\uff08\u305d\u306e\u307e\u307e\u4f7f\u3046\uff09\nfor param in model.parameters():\n    param.requires_grad = False\nmodel.eval()","0f7b3551":"# \u4e88\u6e2c\u30c7\u30fc\u30bf\u3092\u5165\u308c\u308b\u5909\u6570\u3092\u4f5c\u6210\ntest_preds = np.zeros((len(X_test)))\n# Tensor\u578b\u306e\u4e00\u6b21\u5143\u30c7\u30fc\u30bf\uff08\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\uff09\u306b\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3092\u4ed8\u4e0e\ntest = torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.long))\n# \u30df\u30cb\u30d0\u30c3\u30c1\u6bce\u306b\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3092\u518d\u914d\u5e03\ntest_loader = torch.utils.data.DataLoader(test, batch_size=512, shuffle=False)\n# \u5b66\u7fd2\u4e2d\u306e\u30d7\u30ed\u30b0\u30ec\u30b9\u30d0\u30fc\u306e\u8868\u793a\ntk0 = tqdm(test_loader)\nfor i, (x_batch,) in enumerate(tk0):\n    # cuda\u30e1\u30e2\u30ea\u4e0a\u306eBert\u30e2\u30c7\u30eb\u3067\u30df\u30cb\u30d0\u30c3\u30c1\u6bce\u306b\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3092\u5b66\u7fd2\n    pred = model(x_batch.to(device), attention_mask=(x_batch > 0).to(device), labels=None)\n    # \u5b66\u7fd2\u7d50\u679c\u3092\u4fdd\u5b58\n    test_preds[i * 512:(i + 1) * 512] = pred[:, 0].detach().cpu().squeeze().numpy()\n\n# \u5b66\u7fd2\u7d50\u679c\u3092\u6d3b\u6027\u5316\u95a2\u6570(\u30b7\u30b0\u30e2\u30a4\u30c9)\u306b\u639b\u3051\u308b\ntest_pred = torch.sigmoid(torch.tensor(test_preds)).numpy().ravel()","7f0dc036":"submission_bert = pd.DataFrame.from_dict({\n    'id': test_df['id'],\n    'prediction': test_pred\n})","95d52557":"# \u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u5bb9\u91cf\u3092\u6700\u5c0f\u5316\ntrain_df = reduce_mem_usage(pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv'))","a81e2d94":"symbols_to_isolate = '.,?!-;*\"\u2026:\u2014()%#$&_\/@\uff3c\u30fb\u03c9+=\u201d\u201c[]^\u2013>\\\\\u00b0<~\u2022\u2260\u2122\u02c8\u028a\u0252\u221e\u00a7{}\u00b7\u03c4\u03b1\u2764\u263a\u0261|\u00a2\u2192\u0336`\u2765\u2501\u2523\u252b\u2517\uff2f\u25ba\u2605\u00a9\u2015\u026a\u2714\u00ae\\x96\\x92\u25cf\u00a3\u2665\u27a4\u00b4\u00b9\u2615\u2248\u00f7\u2661\u25d0\u2551\u25ac\u2032\u0254\u02d0\u20ac\u06e9\u06de\u2020\u03bc\u2712\u27a5\u2550\u2606\u02cc\u25c4\u00bd\u02bb\u03c0\u03b4\u03b7\u03bb\u03c3\u03b5\u03c1\u03bd\u0283\u272c\uff33\uff35\uff30\uff25\uff32\uff29\uff34\u263b\u00b1\u264d\u00b5\u00ba\u00be\u2713\u25fe\u061f\uff0e\u2b05\u2105\u00bb\u0412\u0430\u0432\u2763\u22c5\u00bf\u00ac\u266b\uff23\uff2d\u03b2\u2588\u2593\u2592\u2591\u21d2\u2b50\u203a\u00a1\u2082\u2083\u2767\u25b0\u2594\u25de\u2580\u2582\u2583\u2584\u2585\u2586\u2587\u2199\u03b3\u0304\u2033\u2639\u27a1\u00ab\u03c6\u2153\u201e\u270b\uff1a\u00a5\u0332\u0305\u0301\u2219\u201b\u25c7\u270f\u25b7\u2753\u2757\u00b6\u02da\u02d9\uff09\u0441\u0438\u02bf\u2728\u3002\u0251\\x80\u25d5\uff01\uff05\u00af\u2212\ufb02\ufb01\u2081\u00b2\u028c\u00bc\u2074\u2044\u2084\u2320\u266d\u2718\u256a\u25b6\u262d\u272d\u266a\u2614\u2620\u2642\u2603\u260e\u2708\u270c\u2730\u2746\u2619\u25cb\u2023\u2693\u5e74\u220e\u2112\u25aa\u2599\u260f\u215b\uff43\uff41\uff53\u01c0\u212e\u00b8\uff57\u201a\u223c\u2016\u2133\u2744\u2190\u263c\u22c6\u0292\u2282\u3001\u2154\u00a8\u0361\u0e4f\u26be\u26bd\u03a6\u00d7\u03b8\uffe6\uff1f\uff08\u2103\u23e9\u262e\u26a0\u6708\u270a\u274c\u2b55\u25b8\u25a0\u21cc\u2610\u2611\u26a1\u2604\u01eb\u256d\u2229\u256e\uff0c\u4f8b\uff1e\u0295\u0250\u0323\u0394\u2080\u271e\u2508\u2571\u2572\u258f\u2595\u2503\u2570\u258a\u258b\u256f\u2533\u250a\u2265\u2612\u2191\u261d\u0279\u2705\u261b\u2669\u261e\uff21\uff2a\uff22\u25d4\u25e1\u2193\u2640\u2b06\u0331\u210f\\x91\u2800\u02e4\u255a\u21ba\u21e4\u220f\u273e\u25e6\u266c\u00b3\u306e\uff5c\uff0f\u2235\u2234\u221a\u03a9\u00a4\u261c\u25b2\u21b3\u25ab\u203f\u2b07\u2727\uff4f\uff56\uff4d\uff0d\uff12\uff10\uff18\uff07\u2030\u2264\u2215\u02c6\u269c\u2601'\nsymbols_to_delete = '\\n\ud83c\udf55\\r\ud83d\udc35\ud83d\ude11\\xa0\\ue014\\t\\uf818\\uf04a\\xad\ud83d\ude22\ud83d\udc36\ufe0f\\uf0e0\ud83d\ude1c\ud83d\ude0e\ud83d\udc4a\\u200b\\u200e\ud83d\ude01\u0639\u062f\u0648\u064a\u0647\u0635\u0642\u0623\u0646\u0627\u062e\u0644\u0649\u0628\u0645\u063a\u0631\ud83d\ude0d\ud83d\udc96\ud83d\udcb5\u0415\ud83d\udc4e\ud83d\ude00\ud83d\ude02\\u202a\\u202c\ud83d\udd25\ud83d\ude04\ud83c\udffb\ud83d\udca5\u1d0d\u028f\u0280\u1d07\u0274\u1d05\u1d0f\u1d00\u1d0b\u029c\u1d1c\u029f\u1d1b\u1d04\u1d18\u0299\u0493\u1d0a\u1d21\u0262\ud83d\ude0b\ud83d\udc4f\u05e9\u05dc\u05d5\u05dd\u05d1\u05d9\ud83d\ude31\u203c\\x81\u30a8\u30f3\u30b8\u6545\u969c\\u2009\ud83d\ude8c\u1d35\u035e\ud83c\udf1f\ud83d\ude0a\ud83d\ude33\ud83d\ude27\ud83d\ude40\ud83d\ude10\ud83d\ude15\\u200f\ud83d\udc4d\ud83d\ude2e\ud83d\ude03\ud83d\ude18\u05d0\u05e2\u05db\u05d7\ud83d\udca9\ud83d\udcaf\u26fd\ud83d\ude84\ud83c\udffc\u0b9c\ud83d\ude16\u1d20\ud83d\udeb2\u2010\ud83d\ude1f\ud83d\ude08\ud83d\udcaa\ud83d\ude4f\ud83c\udfaf\ud83c\udf39\ud83d\ude07\ud83d\udc94\ud83d\ude21\\x7f\ud83d\udc4c\u1f10\u1f76\u03ae\u03b9\u1f72\u03ba\u1f00\u03af\u1fc3\u1f34\u03be\ud83d\ude44\uff28\ud83d\ude20\\ufeff\\u2028\ud83d\ude09\ud83d\ude24\u26fa\ud83d\ude42\\u3000\u062a\u062d\u0643\u0633\u0629\ud83d\udc6e\ud83d\udc99\u0641\u0632\u0637\ud83d\ude0f\ud83c\udf7e\ud83c\udf89\ud83d\ude1e\\u2008\ud83c\udffe\ud83d\ude05\ud83d\ude2d\ud83d\udc7b\ud83d\ude25\ud83d\ude14\ud83d\ude13\ud83c\udffd\ud83c\udf86\ud83c\udf7b\ud83c\udf7d\ud83c\udfb6\ud83c\udf3a\ud83e\udd14\ud83d\ude2a\\x08\u2011\ud83d\udc30\ud83d\udc07\ud83d\udc31\ud83d\ude46\ud83d\ude28\ud83d\ude43\ud83d\udc95\ud835\ude0a\ud835\ude26\ud835\ude33\ud835\ude22\ud835\ude35\ud835\ude30\ud835\ude24\ud835\ude3a\ud835\ude34\ud835\ude2a\ud835\ude27\ud835\ude2e\ud835\ude23\ud83d\udc97\ud83d\udc9a\u5730\u7344\u8c37\u0443\u043b\u043a\u043d\u041f\u043e\u0410\u041d\ud83d\udc3e\ud83d\udc15\ud83d\ude06\u05d4\ud83d\udd17\ud83d\udebd\u6b4c\u821e\u4f0e\ud83d\ude48\ud83d\ude34\ud83c\udfff\ud83e\udd17\ud83c\uddfa\ud83c\uddf8\u043c\u03c5\u0442\u0455\u2935\ud83c\udfc6\ud83c\udf83\ud83d\ude29\\u200a\ud83c\udf20\ud83d\udc1f\ud83d\udcab\ud83d\udcb0\ud83d\udc8e\u044d\u043f\u0440\u0434\\x95\ud83d\udd90\ud83d\ude45\u26f2\ud83c\udf70\ud83e\udd10\ud83d\udc46\ud83d\ude4c\\u2002\ud83d\udc9b\ud83d\ude41\ud83d\udc40\ud83d\ude4a\ud83d\ude49\\u2004\u02e2\u1d52\u02b3\u02b8\u1d3c\u1d37\u1d3a\u02b7\u1d57\u02b0\u1d49\u1d58\\x13\ud83d\udeac\ud83e\udd13\\ue602\ud83d\ude35\u03ac\u03bf\u03cc\u03c2\u03ad\u1f78\u05ea\u05de\u05d3\u05e3\u05e0\u05e8\u05da\u05e6\u05d8\ud83d\ude12\u035d\ud83c\udd95\ud83d\udc45\ud83d\udc65\ud83d\udc44\ud83d\udd04\ud83d\udd24\ud83d\udc49\ud83d\udc64\ud83d\udc76\ud83d\udc72\ud83d\udd1b\ud83c\udf93\\uf0b7\\uf04c\\x9f\\x10\u6210\u90fd\ud83d\ude23\u23fa\ud83d\ude0c\ud83e\udd11\ud83c\udf0f\ud83d\ude2f\u0435\u0445\ud83d\ude32\u1f38\u1fb6\u1f41\ud83d\udc9e\ud83d\ude93\ud83d\udd14\ud83d\udcda\ud83c\udfc0\ud83d\udc50\\u202d\ud83d\udca4\ud83c\udf47\\ue613\u5c0f\u571f\u8c46\ud83c\udfe1\u2754\u2049\\u202f\ud83d\udc60\u300b\u0915\u0930\u094d\u092e\u093e\ud83c\uddf9\ud83c\uddfc\ud83c\udf38\u8521\u82f1\u6587\ud83c\udf1e\ud83c\udfb2\u30ec\u30af\u30b5\u30b9\ud83d\ude1b\u5916\u56fd\u4eba\u5173\u7cfb\u0421\u0431\ud83d\udc8b\ud83d\udc80\ud83c\udf84\ud83d\udc9c\ud83e\udd22\u0650\u064e\u044c\u044b\u0433\u044f\u4e0d\u662f\\x9c\\x9d\ud83d\uddd1\\u2005\ud83d\udc83\ud83d\udce3\ud83d\udc7f\u0f3c\u3064\u0f3d\ud83d\ude30\u1e37\u0417\u0437\u25b1\u0446\ufffc\ud83e\udd23\u5356\u6e29\u54e5\u534e\u8bae\u4f1a\u4e0b\u964d\u4f60\u5931\u53bb\u6240\u6709\u7684\u94b1\u52a0\u62ff\u5927\u574f\u7a0e\u9a97\u5b50\ud83d\udc1d\u30c4\ud83c\udf85\\x85\ud83c\udf7a\u0622\u0625\u0634\u0621\ud83c\udfb5\ud83c\udf0e\u035f\u1f14\u6cb9\u522b\u514b\ud83e\udd21\ud83e\udd25\ud83d\ude2c\ud83e\udd27\u0439\\u2003\ud83d\ude80\ud83e\udd34\u02b2\u0448\u0447\u0418\u041e\u0420\u0424\u0414\u042f\u041c\u044e\u0436\ud83d\ude1d\ud83d\udd91\u1f50\u1f7b\u03cd\u7279\u6b8a\u4f5c\u6226\u7fa4\u0449\ud83d\udca8\u5706\u660e\u56ed\u05e7\u2110\ud83c\udfc8\ud83d\ude3a\ud83c\udf0d\u23cf\u1ec7\ud83c\udf54\ud83d\udc2e\ud83c\udf41\ud83c\udf46\ud83c\udf51\ud83c\udf2e\ud83c\udf2f\ud83e\udd26\\u200d\ud835\udcd2\ud835\udcf2\ud835\udcff\ud835\udcf5\uc548\uc601\ud558\uc138\uc694\u0416\u0459\u041a\u045b\ud83c\udf40\ud83d\ude2b\ud83e\udd24\u1fe6\u6211\u51fa\u751f\u5728\u4e86\u53ef\u4ee5\u8bf4\u666e\u901a\u8bdd\u6c49\u8bed\u597d\u6781\ud83c\udfbc\ud83d\udd7a\ud83c\udf78\ud83e\udd42\ud83d\uddfd\ud83c\udf87\ud83c\udf8a\ud83c\udd98\ud83e\udd20\ud83d\udc69\ud83d\udd92\ud83d\udeaa\u5929\u4e00\u5bb6\u26b2\\u2006\u26ad\u2686\u2b2d\u2b2f\u23d6\u65b0\u2700\u254c\ud83c\uddeb\ud83c\uddf7\ud83c\udde9\ud83c\uddea\ud83c\uddee\ud83c\uddec\ud83c\udde7\ud83d\ude37\ud83c\udde8\ud83c\udde6\u0425\u0428\ud83c\udf10\\x1f\u6740\u9e21\u7ed9\u7334\u770b\u0281\ud835\uddea\ud835\uddf5\ud835\uddf2\ud835\uddfb\ud835\ude06\ud835\uddfc\ud835\ude02\ud835\uddff\ud835\uddee\ud835\uddf9\ud835\uddf6\ud835\ude07\ud835\uddef\ud835\ude01\ud835\uddf0\ud835\ude00\ud835\ude05\ud835\uddfd\ud835\ude04\ud835\uddf1\ud83d\udcfa\u03d6\\u2000\u04af\u057d\u1d26\u13a5\u04bb\u037a\\u2007\u0570\\u2001\u0269\uff59\uff45\u0d66\uff4c\u01bd\uff48\ud835\udc13\ud835\udc21\ud835\udc1e\ud835\udc2b\ud835\udc2e\ud835\udc1d\ud835\udc1a\ud835\udc03\ud835\udc1c\ud835\udc29\ud835\udc2d\ud835\udc22\ud835\udc28\ud835\udc27\u0184\u1d28\u05df\u146f\u0ed0\u03a4\u13e7\u0be6\u0406\u1d11\u0701\ud835\udc2c\ud835\udc30\ud835\udc32\ud835\udc1b\ud835\udc26\ud835\udc2f\ud835\udc11\ud835\udc19\ud835\udc23\ud835\udc07\ud835\udc02\ud835\udc18\ud835\udfce\u051c\u0422\u15de\u0c66\u3014\u13ab\ud835\udc33\ud835\udc14\ud835\udc31\ud835\udfd4\ud835\udfd3\ud835\udc05\ud83d\udc0b\ufb03\ud83d\udc98\ud83d\udc93\u0451\ud835\ude25\ud835\ude2f\ud835\ude36\ud83d\udc90\ud83c\udf0b\ud83c\udf04\ud83c\udf05\ud835\ude6c\ud835\ude56\ud835\ude68\ud835\ude64\ud835\ude63\ud835\ude61\ud835\ude6e\ud835\ude58\ud835\ude60\ud835\ude5a\ud835\ude59\ud835\ude5c\ud835\ude67\ud835\ude65\ud835\ude69\ud835\ude6a\ud835\ude57\ud835\ude5e\ud835\ude5d\ud835\ude5b\ud83d\udc7a\ud83d\udc37\u210b\ud835\udc00\ud835\udc25\ud835\udc2a\ud83d\udeb6\ud835\ude62\u1f39\ud83e\udd18\u0366\ud83d\udcb8\u062c\ud328\ud2f0\uff37\ud835\ude47\u1d7b\ud83d\udc42\ud83d\udc43\u025c\ud83c\udfab\\uf0a7\u0411\u0423\u0456\ud83d\udea2\ud83d\ude82\u0a97\u0ac1\u0a9c\u0ab0\u0abe\u0aa4\u0ac0\u1fc6\ud83c\udfc3\ud835\udcec\ud835\udcfb\ud835\udcf4\ud835\udcee\ud835\udcfd\ud835\udcfc\u2618\ufd3e\u032f\ufd3f\u20bd\\ue807\ud835\udc7b\ud835\udc86\ud835\udc8d\ud835\udc95\ud835\udc89\ud835\udc93\ud835\udc96\ud835\udc82\ud835\udc8f\ud835\udc85\ud835\udc94\ud835\udc8e\ud835\udc97\ud835\udc8a\ud83d\udc7d\ud83d\ude19\\u200c\u041b\u2012\ud83c\udfbe\ud83d\udc79\u238c\ud83c\udfd2\u26f8\u516c\u5bd3\u517b\u5ba0\u7269\u5417\ud83c\udfc4\ud83d\udc00\ud83d\ude91\ud83e\udd37\u64cd\u7f8e\ud835\udc91\ud835\udc9a\ud835\udc90\ud835\udc74\ud83e\udd19\ud83d\udc12\u6b22\u8fce\u6765\u5230\u963f\u62c9\u65af\u05e1\u05e4\ud835\ude6b\ud83d\udc08\ud835\udc8c\ud835\ude4a\ud835\ude6d\ud835\ude46\ud835\ude4b\ud835\ude4d\ud835\ude3c\ud835\ude45\ufdfb\ud83e\udd84\u5de8\u6536\u8d62\u5f97\u767d\u9b3c\u6124\u6012\u8981\u4e70\u989d\u1ebd\ud83d\ude97\ud83d\udc33\ud835\udfcf\ud835\udc1f\ud835\udfd6\ud835\udfd1\ud835\udfd5\ud835\udc84\ud835\udfd7\ud835\udc20\ud835\ude44\ud835\ude43\ud83d\udc47\u951f\u65a4\u62f7\ud835\udde2\ud835\udff3\ud835\udff1\ud835\udfec\u2981\u30de\u30eb\u30cf\u30cb\u30c1\u30ed\u682a\u5f0f\u793e\u26f7\ud55c\uad6d\uc5b4\u3138\u3153\ub2c8\u035c\u0296\ud835\ude3f\ud835\ude54\u20b5\ud835\udca9\u212f\ud835\udcbe\ud835\udcc1\ud835\udcb6\ud835\udcc9\ud835\udcc7\ud835\udcca\ud835\udcc3\ud835\udcc8\ud835\udcc5\u2134\ud835\udcbb\ud835\udcbd\ud835\udcc0\ud835\udccc\ud835\udcb8\ud835\udcce\ud835\ude4f\u03b6\ud835\ude5f\ud835\ude03\ud835\uddfa\ud835\udfee\ud835\udfed\ud835\udfef\ud835\udff2\ud83d\udc4b\ud83e\udd8a\u591a\u4f26\ud83d\udc3d\ud83c\udfbb\ud83c\udfb9\u26d3\ud83c\udff9\ud83c\udf77\ud83e\udd86\u4e3a\u548c\u4e2d\u53cb\u8c0a\u795d\u8d3a\u4e0e\u5176\u60f3\u8c61\u5bf9\u6cd5\u5982\u76f4\u63a5\u95ee\u7528\u81ea\u5df1\u731c\u672c\u4f20\u6559\u58eb\u6ca1\u79ef\u552f\u8ba4\u8bc6\u57fa\u7763\u5f92\u66fe\u7ecf\u8ba9\u76f8\u4fe1\u8036\u7a23\u590d\u6d3b\u6b7b\u602a\u4ed6\u4f46\u5f53\u4eec\u804a\u4e9b\u653f\u6cbb\u9898\u65f6\u5019\u6218\u80dc\u56e0\u5723\u628a\u5168\u5802\u7ed3\u5a5a\u5b69\u6050\u60e7\u4e14\u6817\u8c13\u8fd9\u6837\u8fd8\u267e\ud83c\udfb8\ud83e\udd15\ud83e\udd12\u26d1\ud83c\udf81\u6279\u5224\u68c0\u8ba8\ud83c\udfdd\ud83e\udd81\ud83d\ude4b\ud83d\ude36\uc950\uc2a4\ud0f1\ud2b8\ub93c\ub3c4\uc11d\uc720\uac00\uaca9\uc778\uc0c1\uc774\uacbd\uc81c\ud669\uc744\ub835\uac8c\ub9cc\ub4e4\uc9c0\uc54a\ub85d\uc798\uad00\ub9ac\ud574\uc57c\ud569\ub2e4\uce90\ub098\uc5d0\uc11c\ub300\ub9c8\ucd08\uc640\ud654\uc57d\uae08\uc758\ud488\ub7f0\uc131\ubd84\uac08\ub54c\ub294\ubc18\ub4dc\uc2dc\ud5c8\ub41c\uc0ac\uc6a9\ud83d\udd2b\ud83d\udc41\u51f8\u1f70\ud83d\udcb2\ud83d\uddef\ud835\ude48\u1f0c\ud835\udc87\ud835\udc88\ud835\udc98\ud835\udc83\ud835\udc6c\ud835\udc76\ud835\udd7e\ud835\udd99\ud835\udd97\ud835\udd86\ud835\udd8e\ud835\udd8c\ud835\udd8d\ud835\udd95\ud835\udd8a\ud835\udd94\ud835\udd91\ud835\udd89\ud835\udd93\ud835\udd90\ud835\udd9c\ud835\udd9e\ud835\udd9a\ud835\udd87\ud835\udd7f\ud835\udd98\ud835\udd84\ud835\udd9b\ud835\udd92\ud835\udd8b\ud835\udd82\ud835\udd74\ud835\udd9f\ud835\udd88\ud835\udd78\ud83d\udc51\ud83d\udebf\ud83d\udca1\u77e5\u5f7c\u767e\\uf005\ud835\ude40\ud835\udc9b\ud835\udc72\ud835\udc73\ud835\udc7e\ud835\udc8b\ud835\udfd2\ud83d\ude26\ud835\ude52\ud835\ude3e\ud835\ude3d\ud83c\udfd0\ud835\ude29\ud835\ude28\u1f7c\u1e51\ud835\udc71\ud835\udc79\ud835\udc6b\ud835\udc75\ud835\udc6a\ud83c\uddf0\ud83c\uddf5\ud83d\udc7e\u14c7\u14a7\u152d\u1403\u1427\u1426\u1473\u1428\u14c3\u14c2\u1472\u1438\u146d\u144e\u14c0\u1423\ud83d\udc04\ud83c\udf88\ud83d\udd28\ud83d\udc0e\ud83e\udd1e\ud83d\udc38\ud83d\udc9f\ud83c\udfb0\ud83c\udf1d\ud83d\udef3\u70b9\u51fb\u67e5\u7248\ud83c\udf6d\ud835\udc65\ud835\udc66\ud835\udc67\uff2e\uff27\ud83d\udc63\\uf020\u3063\ud83c\udfc9\u0444\ud83d\udcad\ud83c\udfa5\u039e\ud83d\udc34\ud83d\udc68\ud83e\udd33\ud83e\udd8d\\x0b\ud83c\udf69\ud835\udc6f\ud835\udc92\ud83d\ude17\ud835\udfd0\ud83c\udfc2\ud83d\udc73\ud83c\udf57\ud83d\udd49\ud83d\udc32\u0686\u06cc\ud835\udc6e\ud835\uddd5\ud835\uddf4\ud83c\udf52\ua725\u2ca3\u2c8f\ud83d\udc11\u23f0\u9244\u30ea\u4e8b\u4ef6\u0457\ud83d\udc8a\u300c\u300d\\uf203\\uf09a\\uf222\\ue608\\uf202\\uf099\\uf469\\ue607\\uf410\\ue600\u71fb\u88fd\u30b7\u865a\u507d\u5c41\u7406\u5c48\u0413\ud835\udc69\ud835\udc70\ud835\udc80\ud835\udc7a\ud83c\udf24\ud835\uddf3\ud835\udddc\ud835\uddd9\ud835\udde6\ud835\udde7\ud83c\udf4a\u1f7a\u1f08\u1f21\u03c7\u1fd6\u039b\u290f\ud83c\uddf3\ud835\udc99\u03c8\u0541\u0574\u0565\u057c\u0561\u0575\u056b\u0576\u0580\u0582\u0564\u0571\u51ac\u81f3\u1f40\ud835\udc81\ud83d\udd39\ud83e\udd1a\ud83c\udf4e\ud835\udc77\ud83d\udc02\ud83d\udc85\ud835\ude2c\ud835\ude31\ud835\ude38\ud835\ude37\ud835\ude10\ud835\ude2d\ud835\ude13\ud835\ude16\ud835\ude39\ud835\ude32\ud835\ude2b\u06a9\u0392\u03ce\ud83d\udca2\u039c\u039f\u039d\u0391\u0395\ud83c\uddf1\u2672\ud835\udf48\u21b4\ud83d\udc92\u2298\u023b\ud83d\udeb4\ud83d\udd95\ud83d\udda4\ud83e\udd58\ud83d\udccd\ud83d\udc48\u2795\ud83d\udeab\ud83c\udfa8\ud83c\udf11\ud83d\udc3b\ud835\udc0e\ud835\udc0d\ud835\udc0a\ud835\udc6d\ud83e\udd16\ud83c\udf8e\ud83d\ude3c\ud83d\udd77\uff47\uff52\uff4e\uff54\uff49\uff44\uff55\uff46\uff42\uff4b\ud835\udff0\ud83c\uddf4\ud83c\udded\ud83c\uddfb\ud83c\uddf2\ud835\uddde\ud835\udded\ud835\uddd8\ud835\udde4\ud83d\udc7c\ud83d\udcc9\ud83c\udf5f\ud83c\udf66\ud83c\udf08\ud83d\udd2d\u300a\ud83d\udc0a\ud83d\udc0d\\uf10a\u10da\u06a1\ud83d\udc26\\U0001f92f\\U0001f92a\ud83d\udc21\ud83d\udcb3\u1f31\ud83d\ude47\ud835\uddf8\ud835\udddf\ud835\udde0\ud835\uddf7\ud83e\udd5c\u3055\u3088\u3046\u306a\u3089\ud83d\udd3c'","efe3dba4":"# \u30c4\u30ea\u30fc\u30d0\u30f3\u30af\u3092\u5b9a\u7fa9\ntokenizer = TreebankWordTokenizer()\n\n# \u30c7\u30b3\u30fc\u30c9\u51fa\u6765\u306a\u3044\u6587\u5b57\u3001\u8981\u3089\u306a\u3044\u6587\u5b57\u306e\u8f9e\u66f8\nisolate_dict = {ord(c):f' {c} ' for c in symbols_to_isolate}\nremove_dict = {ord(c):f'' for c in symbols_to_delete}","9d923f2d":"# \u8a13\u7df4\u30c7\u30fc\u30bf\u4e2d\u306e\u8981\u3089\u306a\u3044\u30c7\u30fc\u30bf\uff08\u6587\u5b57\u5217\uff09\u3092\u524a\u9664\u3057\u3001\u30c7\u30b3\u30fc\u30c9\u51fa\u6765\u306a\u304b\u3063\u305f\u30c7\u30fc\u30bf\uff08\u6587\u5b57\u5217\uff09\u3092\u6587\u5b57\u306b\u5909\u63db\nx_train = train_df['comment_text'].progress_apply(lambda x:preprocess(x))\n# \u4e88\u6e2c\u3057\u305f\u3044\u76ee\u7684\u5909\u6570\ny_aux_train = train_df[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]\n# \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u4e2d\u306e\u8981\u3089\u306a\u3044\u30c7\u30fc\u30bf\uff08\u6587\u5b57\u5217\uff09\u3092\u524a\u9664\u3057\u3001\u30c7\u30b3\u30fc\u30c9\u51fa\u6765\u306a\u304b\u3063\u305f\u30c7\u30fc\u30bf\uff08\u6587\u5b57\u5217\uff09\u3092\u6587\u5b57\u306b\u5909\u63db\nx_test = test_df['comment_text'].progress_apply(lambda x:preprocess(x))\n\n# \u8a13\u7df4\u30c7\u30fc\u30bf\u4e2d\u306e\u8aac\u660e\u5909\u6570\nidentity_columns = [\n    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n# Overall\nweights = np.ones((len(x_train),)) \/ 4\n# Subgroup(\u5404\u8a13\u7df4\u30c7\u30fc\u30bf\u6bce\u306b\u6709\u52b9\u306a\u8aac\u660e\u5909\u6570\u306e\u500b\u6570\u5206\u3001\u91cd\u308a\u3092\u52a0\u7b97)\nweights += (train_df[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) \/ 4\n# Background Positive, Subgroup Negative(\u8a13\u7df4\u30c7\u30fc\u30bf\u6bce\u306b\u6709\u52b9\u306atarget\u30ab\u30e9\u30e0\u30921\u3068\u3057\u3001\u6709\u52b9\u3067\u306a\u3044\u8aac\u660e\u5909\u6570\u306e\u500b\u6570\u3068\u8db3\u3057\u5408\u308f\u305b\u305f\u6570\u3092\u91cd\u308a\u306b\u52a0\u7b97)\nweights += (( (train_df['target'].values>=0.5).astype(bool).astype(np.int) +\n   (train_df[identity_columns].fillna(0).values<0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) \/ 4\n# Background Negative, Subgroup Positive(\u8a13\u7df4\u30c7\u30fc\u30bf\u6bce\u306b\u6709\u52b9\u3067\u306a\u3044target\u30ab\u30e9\u30e0\u30921\u3068\u3057\u3001\u6709\u52b9\u306a\u8aac\u660e\u5909\u6570\u306e\u500b\u6570\u3068\u8db3\u3057\u5408\u308f\u305b\u305f\u6570\u3092\u91cd\u308a\u306b\u52a0\u7b97)\nweights += (( (train_df['target'].values<0.5).astype(bool).astype(np.int) +\n   (train_df[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) \/ 4\nloss_weight = 1.0 \/ weights.mean()\n\n# \u5404\u8a13\u7df4\u30c7\u30fc\u30bf\u3092\u3001\u6709\u52b9\u306atarget\u306b\u306f1\u3001\u305d\u3046\u3067\u306a\u3044target\u306b\u306f0\u3092\u5272\u308a\u632f\u3063\u305f\u30ab\u30e9\u30e0\u3068\u3001weights\u3092\u9023\u7d50\u3057\u305f\u914d\u5217\u306b\u5909\u63db\u3002\ny_train = np.vstack([(train_df['target'].values>=0.5).astype(np.int),weights]).T\n\nmax_features = 410047","55b56016":"# \u6587\u5b57\u5217\u3092\u30d9\u30af\u30c8\u30eb\u5316\u3059\u308b\u5909\u63db\u5668\u3092\u5b9a\u7fa9\ntokenizer = text.Tokenizer(num_words = max_features, filters='',lower=False)","49cea7a3":"# \u8a13\u7df4\u30c7\u30fc\u30bf\u3068\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\uff08\u6587\u7ae0\uff09\u3092\u5358\u8a9e\u6bce\u306b\u30d9\u30af\u30c8\u30eb\u5316\ntokenizer.fit_on_texts(list(x_train) + list(x_test))\n\n# \u8a13\u7df4\u30c7\u30fc\u30bf\u4e2d\u306e\u5358\u8a9e\u306e\u5206\u6563\u8868\u73fe\u3092\u4f5c\u6210\ncrawl_matrix, unknown_words_crawl = build_matrix(tokenizer.word_index, CRAWL_EMBEDDING_PATH)\nprint('n unknown words (crawl): ', len(unknown_words_crawl))\n\nglove_matrix, unknown_words_glove = build_matrix(tokenizer.word_index, GLOVE_EMBEDDING_PATH)\nprint('n unknown words (glove): ', len(unknown_words_glove))\n\nmax_features = max_features or len(tokenizer.word_index) + 1\nmax_features\n\n# \u8868\u73fe\u65b9\u6cd5\u304c\u9055\u3046\u8a13\u7df4\u30c7\u30fc\u30bf\u3068\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3092\u7d50\u5408\nembedding_matrix = np.concatenate([crawl_matrix, glove_matrix], axis=-1)\nembedding_matrix.shape\n\ndel crawl_matrix\ndel glove_matrix\ngc.collect()\n\n# \u8a13\u7df4\u30c7\u30fc\u30bf\u3068\u76ee\u7684\u5909\u6570\u3092\u9023\u7d50\ny_train_torch = torch.tensor(np.hstack([y_train, y_aux_train]), dtype=torch.float32)","a4decb38":"# \u8a13\u7df4\u30c7\u30fc\u30bf\u3001\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u4e0a\u306e\u5358\u8a9e\u3092TreeBank\u4e0a\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u306b\u5909\u63db\nx_train = tokenizer.texts_to_sequences(x_train)\nx_test = tokenizer.texts_to_sequences(x_test)","3096dfc7":"# \u5404\u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u5358\u8a9e\u6570\u3092\u53d6\u5f97\nlengths = torch.from_numpy(np.array([len(x) for x in x_train]))\n\n# \u5404\u30c7\u30fc\u30bf\u306e\u5358\u8a9e\u6570\u304c300\u306b\u306a\u308b\u3088\u3046\u306b\u3001padding\u3092\u884c\u3046\n# padding\u30aa\u30d7\u30b7\u30e7\u30f3\u3067\u30c7\u30fc\u30bf\u306e\u524d\u5f8c('pre' or 'post')\u3069\u3061\u3089\u3092padding\u3059\u308b\u304b\u3092\u6c7a\u3081\u3089\u308c\u3001\u30c7\u30d5\u30a9\u30eb\u30c8\u306f\u524d'pre'\nmaxlen = 300\nx_train_padded = torch.from_numpy(sequence.pad_sequences(x_train, maxlen=maxlen))","65bebeb8":"test_lengths = torch.from_numpy(np.array([len(x) for x in x_test]))\n\nx_test_padded = torch.from_numpy(sequence.pad_sequences(x_test, maxlen=maxlen))","9b2beeb1":"batch_size = 512\n# padding\u3057\u305f\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3068\u5404\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306e\u9577\u3055\u3092\u540c\u3058\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3067\u53d6\u308a\u51fa\u305b\u308bTensor\u578b\u4e8c\u6b21\u5143\u914d\u5217\u306b\u52a0\u5de5\ntest_dataset = data.TensorDataset(x_test_padded, test_lengths)\n# padding\u3057\u305f\u8a13\u7df4\u30c7\u30fc\u30bf\u3001\u305d\u306e\u30c7\u30fc\u30bf\u306epadding\u3059\u308b\u524d\u306e\u9577\u3055\u3068\u76ee\u7684\u5909\u6570\u3092Tensor\u578b\u4e09\u6b21\u5143\u914d\u5217\u306b\u52a0\u5de5\ntrain_dataset = data.TensorDataset(x_train_padded, lengths, y_train_torch)\nvalid_dataset = data.Subset(train_dataset, indices=[0, 1])\n\ntrain_collator = SequenceBucketCollator(lambda lenghts: lenghts.max(), \n                                        sequence_index=0, \n                                        length_index=1, \n                                        label_index=2)\ntest_collator = SequenceBucketCollator(lambda lenghts: lenghts.max(), sequence_index=0, length_index=1)\n\n# \u5404\u7a2e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092minibatch\u6bce\u306b\u6574\u5f62\ntrain_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=train_collator)\nvalid_loader = data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=train_collator)\ntest_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=test_collator)\n\n# minibatch\u6bce\u306b\u8a13\u7df4\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3068\u6b63\u89e3\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4f5c\u6210\ndatabunch = DataBunch(train_dl=train_loader, valid_dl=valid_loader, collate_fn=train_collator)","d8faec1c":"all_test_preds = []\n\nfor model_idx in range(NUM_MODELS):\n    print('Model ', model_idx)\n    seed_everything(1 + model_idx)\n    model = NeuralNet(embedding_matrix, y_aux_train.shape[-1])\n    learn = Learner(databunch, model, loss_func=custom_loss)\n    test_preds = train_model(learn,test_dataset,output_dim=7)    \n    all_test_preds.append(test_preds)","b5ba0add":"submission_lstm = pd.DataFrame.from_dict({\n    'id': test_df['id'],\n    'prediction': np.mean(all_test_preds, axis=0)[:, 0]\n})","7b76f8f0":"submission = pd.read_csv(\n    \"..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/sample_submission.csv\"\n)\n\nweights = [0.333, 0.667]\nsubmission[\"prediction\"] = ensemble_predictions(\n    [submission_bert.prediction.values, submission_lstm.prediction.values],\n    weights,\n    type_=\"rank\",\n)\nsubmission.to_csv(\"submission.csv\", index=False)\n\n\n","f343f651":"**Blending part**","0154fe8e":"**LSTM Part**","2fff3574":"Thanks for @christofhenkel @abhishek @iezepov for their great work:\n\nhttps:\/\/www.kaggle.com\/christofhenkel\/how-to-preprocessing-for-glove-part2-usage\nhttps:\/\/www.kaggle.com\/abhishek\/pytorch-bert-inference\nhttps:\/\/www.kaggle.com\/iezepov\/starter-gensim-word-embeddings","539a83c8":"**BERT Part**"}}