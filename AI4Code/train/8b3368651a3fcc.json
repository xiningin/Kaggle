{"cell_type":{"f4772791":"code","4a846fc2":"code","976f6c0e":"code","09ff3677":"code","cefe59c9":"code","ea5966ef":"code","b262862d":"code","8dfbbc1c":"code","5baad360":"code","e2e36f4e":"code","bcde16c1":"code","f795fc64":"code","880d3121":"code","62bdd7fe":"code","d54995d2":"code","86d16679":"code","02ed72c0":"code","d7c31892":"code","8e67edfb":"code","e337b27c":"code","39911825":"code","3a0924a0":"code","32da7609":"code","dd437d20":"code","f70bbb57":"code","2295994e":"code","8ded9de5":"code","68eb3d48":"code","0538cb10":"code","361736fd":"code","cd501f88":"code","1da9bd17":"code","26e4e5f6":"code","d995e708":"code","5b4403f8":"code","31388a38":"code","829c169f":"code","3ea29ba9":"code","77d055c8":"code","da18cfe0":"code","8f1ceba9":"code","5c99aebd":"code","9dea5765":"code","a2eff449":"code","1256bbb8":"code","0a323e10":"code","fb186e1a":"markdown","8bde6fc9":"markdown","82585516":"markdown","704511ce":"markdown","3e3ac6de":"markdown"},"source":{"f4772791":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n'''for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))'''\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4a846fc2":"import pandas as pd\ntrain_csv = pd.read_csv('\/kaggle\/input\/landmark-recognition-2020\/train.csv')\ntrain_csv.head(10)","976f6c0e":"len(train_csv)","09ff3677":"len(train_csv['landmark_id'].unique())","cefe59c9":"import matplotlib.image as img\nimport matplotlib.pyplot as plt\ndef show_img(file_name):\n    image = img.imread('\/kaggle\/input\/landmark-recognition-2020\/train\/'+file_name[0]+'\/'+file_name[1]+'\/'+file_name[2]+'\/'+file_name)\n    plt.imshow(image)\n    plt.show()","ea5966ef":"show_img('0000059611c7d079.jpg')","b262862d":"train_csv = pd.read_csv('\/kaggle\/input\/landmark-recognition-2020\/train.csv')\ntrain_csv.head(10)\n\n# put .jpg into the file name\ndef add_txt(fn):\n    return fn+'.jpg'\n\ntrain_csv['id'] = train_csv['id'].apply(add_txt)\n\n\n\n# choose those labels with more than 200 images, and choose the first 200 images of each label\n# move every training files to the same folder\n%cd \/kaggle\/working\nif not os.path.exists('training'):\n    os.mkdir('training')\nif not os.path.exists('validation'):\n    os.mkdir('validation')\nif not os.path.exists('testing'):\n    os.mkdir('testing')    \n\nimport shutil\nimport random\n\nlabel_list = train_csv['landmark_id'].unique()\ncnt = 0\nfinal_label_list = []\n\nfor label in list(label_list): # label order by random\n    file_list = list(train_csv['id'][train_csv['landmark_id']==label])\n    if len(file_list) >= 200:\n        final_label_list.append(label)\n        if not os.path.exists('\/kaggle\/working\/training\/'+str(label)):\n            os.mkdir('\/kaggle\/working\/training\/'+str(label))\n        if not os.path.exists('\/kaggle\/working\/validation\/'+str(label)):\n            os.mkdir('\/kaggle\/working\/validation\/'+str(label))\n        if not os.path.exists('\/kaggle\/working\/testing\/'+str(label)):\n            os.mkdir('\/kaggle\/working\/testing\/'+str(label))\n        for file in file_list[:120]:  # 120 files for training\n            src = '\/kaggle\/input\/landmark-recognition-2020\/train\/'+file[0]+'\/'+file[1]+'\/'+file[2]+'\/'+file\n            dst = '\/kaggle\/working\/training\/'+str(label)+'\/'+file\n            if not os.path.exists(dst):\n                shutil.copyfile(src, dst)\n        for file in file_list[120:160]: # 40 files for validation\n            src = '\/kaggle\/input\/landmark-recognition-2020\/train\/'+file[0]+'\/'+file[1]+'\/'+file[2]+'\/'+file\n            dst = '\/kaggle\/working\/validation\/'+str(label)+'\/'+file\n            if not os.path.exists(dst):\n                shutil.copyfile(src, dst)\n        for file in file_list[160:200]: # 40 files for testing\n            src = '\/kaggle\/input\/landmark-recognition-2020\/train\/'+file[0]+'\/'+file[1]+'\/'+file[2]+'\/'+file\n            dst = '\/kaggle\/working\/testing\/'+str(label)+'\/'+file\n            if not os.path.exists(dst):\n                shutil.copyfile(src, dst)\n        cnt += 1\n    if cnt == 100: # only need 500 labels\n        break\n# 20,000 files in total","8dfbbc1c":"len(final_label_list)","5baad360":"print(len(os.listdir('.\/training')), len(os.listdir('.\/validation')), len(os.listdir('.\/testing')))","e2e36f4e":"from keras.preprocessing.image import ImageDataGenerator\n\ntrain_datagen = ImageDataGenerator(\n    rescale=1.\/255,\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True)\n\ntest_datagen = ImageDataGenerator(rescale=1.\/255)\n\ntrain_dir = '\/kaggle\/working\/training'\nvalidation_dir = '\/kaggle\/working\/validation'\ntest_dir = '\/kaggle\/working\/testing'\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=(256, 256),\n    batch_size = 32,\n    class_mode='categorical',\n    seed=42)\n\nvalidation_generator = test_datagen.flow_from_directory(\n    validation_dir,\n    target_size=(256, 256),\n    batch_size = 32,\n    class_mode='categorical',\n    seed=42)\n\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=(256, 256),\n    batch_size = 1,\n    class_mode='categorical',\n    seed=42)","bcde16c1":"test_generator.class_indices.keys()","f795fc64":"from tensorflow.keras.applications import MobileNetV2\n\nconv_base = MobileNetV2(include_top=False,\n                        weights=\"imagenet\",\n                        input_shape=(256, 256, 3)\n)\n\nconv_base.trainable = True\n\nconv_base.summary()","880d3121":"from keras.layers import Dense, Dropout, MaxPooling2D, GlobalAveragePooling2D, Flatten, Conv2D, Input\nfrom keras.models import Sequential\nfrom keras import optimizers\nimport tensorflow as tf\n\nmodel = Sequential()\nmodel.add(conv_base)\nmodel.add(GlobalAveragePooling2D())\nmodel.add(Dense(100, activation='softmax'))\nmodel.compile(optimizer=optimizers.RMSprop(lr=2e-5),\n              loss = 'categorical_crossentropy',\n              metrics=['accuracy'])\n\nmodel.summary()","62bdd7fe":"with tf.device(\"gpu\"):\n    history = model.fit(\n        train_generator,\n        epochs=100, \n        validation_data=validation_generator,\n        verbose=2\n    )","d54995d2":"model.save('.\/final_model.hdf5')","86d16679":"# plot the training results\nimport matplotlib.pyplot as plt\n\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc)+1)\n\nplt.plot(epochs, acc, '#21466C', label='Training acc')\nplt.plot(epochs, val_acc, '#ff0051', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, '#21466C', label='Training loss')\nplt.plot(epochs, val_loss, '#ff0051', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","02ed72c0":"scores = model.evaluate(test_generator)\nprint('loss:', scores[0])\nprint('accuracy:', scores[1])","d7c31892":"label_list = list(test_generator.class_indices.keys())\nlabel_list[:10]","8e67edfb":"y_pred = model.predict(test_generator)","e337b27c":"y_pred.shape","39911825":"max(y_pred[0])","3a0924a0":"y_pred_ls = y_pred.tolist()","32da7609":"y = []\n\nfor i in range(len(y_pred_ls)):\n    max_value = max(y_pred_ls[i])\n    max_index = y_pred_ls[i].index(max_value)\n    y.append(max_index)\n    \ny = np.array(y)","dd437d20":"scores = []\nfor i in range(len(y_pred_ls)):\n    scores.append(max(y_pred[i]))\n    \nscores = np.array(scores)","f70bbb57":"y","2295994e":"scores","8ded9de5":"from sklearn.metrics import roc_curve, roc_auc_score\nfpr, tpr, thresholds = roc_curve(y, scores, pos_label=0)","68eb3d48":"y_0_1 = []\nauc=[]\nfor j in y:\n    if j != 10:\n        y_0_1.append(0)\n    else:\n        y_0_1.append(1)\ny_0_1 = np.array(y_0_1)\nauc_of_the_label = roc_auc_score(y_0_1, scores)\nauc.append(auc_of_the_label)\nauc","0538cb10":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, roc_auc_score\n\nauc=[]\n\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\nplt.title('ROC')\n\nfor i in range(100):\n    fpr, tpr, _ = roc_curve(y, scores, pos_label=i)\n    plt.plot(fpr, tpr, label=i)\n    \n    y_0_1 = []\n    for j in y:\n        if j != i:\n            y_0_1.append(0)\n        else:\n            y_0_1.append(1)\n    y_0_1 = np.array(y_0_1)\n    auc_of_the_label = roc_auc_score(y_0_1, scores)\n    auc.append(auc_of_the_label)\n\n\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\nplt.title('ROC')\n\nplt.show()\n\n","361736fd":"scores","cd501f88":"max(auc)","1da9bd17":"min(auc)","26e4e5f6":"label_list = list(test_generator.class_indices.keys())","d995e708":"label_list[:10]","5b4403f8":"def to_original_label(new_label): # \u5f9e\u65b0\u7684label\u5c0d\u61c9\u56de\u539f\u672c\u8cc7\u6599\u7684label\n    return int(label_list[new_label])","31388a38":"max_auc_label = to_original_label(auc.index(max(auc))) # label with the highest AUC\nmin_auc_label = to_original_label(auc.index(min(auc))) # label with the lowest AUC\nprint('max auc label', max_auc_label)\nprint('min auc label', min_auc_label)","829c169f":"file_list = train_csv[train_csv['landmark_id']==max_auc_label]['id']\nfile_list","3ea29ba9":"for file in file_list[:20]:\n    show_img(file)","77d055c8":"file_list = train_csv[train_csv['landmark_id']==min_auc_label]['id']\nfile_list","da18cfe0":"for file in file_list[:20]:\n    show_img(file)","8f1ceba9":"image_index = 0 # choose a image (0-3999)\nimage = test_generator[image_index][0] # [0][0]: the first 0 stands for the 0th image, the second 0 stands for the image array\nimage = image.reshape((256,256,3))\nplt.imshow(image)\nplt.show()\nprint(test_generator[0][1][0]) # array of the image's label","5c99aebd":"def get_label(arr):\n    for i in range(len(arr)):\n        if arr[i] == max(arr):\n            return label_list[i]","9dea5765":"# label from original data\nget_label(test_generator[image_index][1][0])","a2eff449":"# label by prediction\nget_label(model.predict(test_generator[image_index][0])[0])","1256bbb8":"label = get_label(model.predict(test_generator[image_index][0])[0])\nimage_list = [] # image with the same label\nfor i in range(4000):\n    if get_label(test_generator[i][1][0]) == label:\n        image_list.append(i)\n","0a323e10":"if len(image_list) > 10:\n    for i in range(10):\n        if i != image_index:\n            image = test_generator[image_list[i]][0]\n            image = image.reshape((256,256,3))\n            plt.imshow(image)\n            plt.show()\n        \nelse:\n    for i in range(len(image_list)):\n        if i != image_index:\n            image = test_generator[image_list[i]][0]\n            image = image.reshape((256,256,3))\n            plt.imshow(image)\n            plt.show()\n","fb186e1a":"Then, images with the lowest AUC score","8bde6fc9":"Just view some beautiful images","82585516":"### Test by eyes","704511ce":"First, images with the highest AUC score","3e3ac6de":"# Training"}}