{"cell_type":{"c5fb7b71":"code","6e7a4512":"code","febcbbce":"code","305b9d5b":"code","96b2fd5b":"code","7ac7b46d":"code","7d810b00":"code","baecb34d":"code","a1cf1dbf":"code","96ed4606":"code","b8f23935":"code","5a6645d6":"code","9f2fdf09":"code","23678ac9":"code","cbf5ddef":"code","da6f608b":"code","ba728726":"code","4abb364e":"code","1373f3dc":"code","3c57d3f9":"code","8d2c6203":"code","3bf92743":"code","3345e442":"code","68ec4a74":"code","ea886934":"code","74ad1c79":"code","84401ac4":"code","916343a6":"code","fdb75a1d":"code","93164606":"code","73768e65":"code","a24d1fb5":"code","c8d61414":"code","1fc46faf":"code","062ff4fe":"code","f8258645":"code","e005a9ca":"code","fbfbd2f0":"code","f746cfdc":"code","2afe295e":"code","3b6e15ce":"code","0198b7ed":"code","762c82a8":"code","4b10fc4a":"code","51ec406c":"code","a8386448":"code","93406e65":"code","1d8e261d":"code","9f0afcbe":"code","40f577fc":"code","19db4160":"code","e83f661f":"code","b39edcf7":"code","c78ca25b":"code","5776e5c1":"code","cb128184":"code","3d80ff45":"code","0e200354":"code","1be1a92a":"code","a01d51a8":"code","2f3dac27":"code","2bcc60fa":"code","4de4b576":"code","d668506b":"code","72c28b64":"code","19f7cb96":"code","5504e3c6":"code","f8f1e776":"code","8f20d011":"code","aac18231":"code","369f770e":"code","bdd39330":"code","f7bc372b":"code","1233fb53":"code","d7546f03":"code","bc10a5eb":"code","fddb0c74":"code","42865207":"code","fca84c18":"code","e2a24627":"code","3a21b8b4":"code","75b677a7":"code","c543c376":"code","1513bbb6":"code","6c022880":"code","986ac5a7":"code","a79c47f1":"code","30fbdbea":"code","b245d2fb":"code","9f61cdb9":"code","ef02f16b":"code","ede0dace":"code","fda7adb1":"code","d2bbed03":"code","d5ac5d8b":"code","fbb36906":"code","77a3d8c1":"code","ef8e46a6":"code","d9173b28":"code","f1506838":"code","1d250433":"code","b26f9a5c":"code","c660d025":"code","1ad1f9c7":"code","fc8a0ca2":"code","690aaa62":"code","a3b0257e":"code","495559fe":"code","16dc9912":"code","f42e17eb":"code","2f4f0ceb":"code","c86c0157":"code","b6b7f831":"code","9e17e877":"code","febb4249":"code","59a68e55":"code","99c5f64f":"code","58404b3e":"code","9f6d4d3c":"code","44dd78c5":"code","0fa6f7e5":"code","2f9e0586":"code","e3695588":"code","8dd02ca5":"code","5cc9ffbe":"code","63808da7":"code","7d8ed0a6":"code","c5803bd0":"code","6f8d2c3c":"code","b4a6c0db":"code","fb45e4e1":"code","180f2a60":"code","8ca7d455":"code","077fb4b4":"code","f2fe8ccf":"code","14007a5b":"code","93f3cae3":"code","74973d18":"code","fe05b5c5":"code","ee537af5":"code","bcf861a8":"code","4f275844":"markdown","523ad35a":"markdown","deda91a9":"markdown","e12fcc07":"markdown","0bfa473a":"markdown","b364c409":"markdown","4ebc4a8d":"markdown","39232837":"markdown","6341ed3b":"markdown","f20848e9":"markdown","d366081c":"markdown","3b1815b7":"markdown","8da9705a":"markdown","516015b8":"markdown","685cb2b3":"markdown","ccff5d47":"markdown","2ea5d15b":"markdown","a3c2b2d8":"markdown","b8e90e15":"markdown","fc7390a9":"markdown","308e2402":"markdown","8643adb9":"markdown","ed1afb70":"markdown","fdc4f7cc":"markdown","c8446765":"markdown","e71ace4f":"markdown","4c496d30":"markdown","afa2416b":"markdown","67830565":"markdown","cdb019f9":"markdown","0c020ea3":"markdown","4b2b910a":"markdown","50d470e8":"markdown","d9fea496":"markdown","fd6a4918":"markdown","f3b9012c":"markdown","7e43d5ef":"markdown","943f9e4a":"markdown","c2c127d1":"markdown","876af80c":"markdown","4fe4fd4d":"markdown","8be11ced":"markdown","c8f7e092":"markdown","25374acb":"markdown","fdfc4ad3":"markdown","505e845f":"markdown"},"source":{"c5fb7b71":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","6e7a4512":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns","febcbbce":"data = pd.read_csv(\"..\/input\/mushrooms.csv\")\ndata.head(6)","305b9d5b":"data.isnull().sum()","96b2fd5b":"data['class'].unique()","7ac7b46d":"data.shape","7d810b00":"from sklearn.preprocessing import LabelEncoder\nlabelencoder=LabelEncoder()\nfor col in data.columns:\n    data[col] = labelencoder.fit_transform(data[col])\n \ndata.head()","baecb34d":"data['stalk-color-above-ring'].unique()","a1cf1dbf":"print(data.groupby('class').size())","96ed4606":"'''\n# Create a figure instance\nfig, axes = plt.subplots(nrows=2 ,ncols=2 ,figsize=(9, 9))\n\n# Create an axes instance and the boxplot\nbp1 = axes[0,0].boxplot(data['stalk-color-above-ring'],patch_artist=True)\n\nbp2 = axes[0,1].boxplot(data['stalk-color-below-ring'],patch_artist=True)\n\nbp3 = axes[1,0].boxplot(data['stalk-surface-below-ring'],patch_artist=True)\n\nbp4 = axes[1,1].boxplot(data['stalk-surface-above-ring'],patch_artist=True)\n'''\nax = sns.boxplot(x='class', y='stalk-color-above-ring', \n                data=data)\nax = sns.stripplot(x=\"class\", y='stalk-color-above-ring',\n                   data=data, jitter=True,\n                   edgecolor=\"gray\")\nsns.plt.title(\"Class w.r.t stalkcolor above ring\",fontsize=12)","b8f23935":"X = data.iloc[:,1:23]  # all rows, all the features and no labels\ny = data.iloc[:, 0]  # all rows, label only\nX.head()\ny.head()","5a6645d6":"X.describe()","9f2fdf09":"y.head()","23678ac9":"data.corr()","cbf5ddef":"# Scale the data to be between -1 and 1\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX=scaler.fit_transform(X)\nX","da6f608b":"from sklearn.decomposition import PCA\npca = PCA()\npca.fit_transform(X)","ba728726":"covariance=pca.get_covariance()\n#covariance","4abb364e":"explained_variance=pca.explained_variance_\nexplained_variance","1373f3dc":"with plt.style.context('dark_background'):\n    plt.figure(figsize=(6, 4))\n    \n    plt.bar(range(22), explained_variance, alpha=0.5, align='center',\n            label='individual explained variance')\n    plt.ylabel('Explained variance ratio')\n    plt.xlabel('Principal components')\n    plt.legend(loc='best')\n    plt.tight_layout()","3c57d3f9":"N=data.values\npca = PCA(n_components=2)\nx = pca.fit_transform(N)\nplt.figure(figsize = (5,5))\nplt.scatter(x[:,0],x[:,1])\nplt.show()","8d2c6203":"from sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=2, random_state=5)\nX_clustered = kmeans.fit_predict(N)\n\nLABEL_COLOR_MAP = {0 : 'g',\n                   1 : 'y'\n                  }\n\nlabel_color = [LABEL_COLOR_MAP[l] for l in X_clustered]\nplt.figure(figsize = (5,5))\nplt.scatter(x[:,0],x[:,1], c= label_color)\nplt.show()","3bf92743":"pca_modified=PCA(n_components=17)\npca_modified.fit_transform(X)","3345e442":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=4)","68ec4a74":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import metrics\n\nmodel_LR= LogisticRegression()","ea886934":"model_LR.fit(X_train,y_train)","74ad1c79":"y_prob = model_LR.predict_proba(X_test)[:,1] # This will give you positive class prediction probabilities  \ny_pred = np.where(y_prob > 0.5, 1, 0) # This will threshold the probabilities to give class predictions.\nmodel_LR.score(X_test, y_pred)","84401ac4":"confusion_matrix=metrics.confusion_matrix(y_test,y_pred)\nconfusion_matrix","916343a6":"auc_roc=metrics.roc_auc_score(y_test,y_pred)\nauc_roc","fdb75a1d":"from sklearn.metrics import roc_curve, auc\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob)\nroc_auc = auc(false_positive_rate, true_positive_rate)\nroc_auc","93164606":"import matplotlib.pyplot as plt\nplt.figure(figsize=(10,10))\nplt.title('Receiver Operating Characteristic')\nplt.plot(false_positive_rate,true_positive_rate, color='red',label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],linestyle='--')\nplt.axis('tight')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')","73768e65":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import metrics\n\nLR_model= LogisticRegression()\n\ntuned_parameters = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000] ,\n              'penalty':['l1','l2']\n                   }","a24d1fb5":"data.corr()","c8d61414":"from sklearn.model_selection import GridSearchCV\n\nLR= GridSearchCV(LR_model, tuned_parameters,cv=10)","1fc46faf":"LR.fit(X_train,y_train)","062ff4fe":"print(LR.best_params_)","f8258645":"y_prob = LR.predict_proba(X_test)[:,1] # This will give you positive class prediction probabilities  \ny_pred = np.where(y_prob > 0.5, 1, 0) # This will threshold the probabilities to give class predictions.\nLR.score(X_test, y_pred)","e005a9ca":"confusion_matrix=metrics.confusion_matrix(y_test,y_pred)\nconfusion_matrix","fbfbd2f0":"auc_roc=metrics.classification_report(y_test,y_pred)\nauc_roc","f746cfdc":"auc_roc=metrics.roc_auc_score(y_test,y_pred)\nauc_roc","2afe295e":"from sklearn.metrics import roc_curve, auc\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob)\nroc_auc = auc(false_positive_rate, true_positive_rate)\nroc_auc","3b6e15ce":"\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10,10))\nplt.title('Receiver Operating Characteristic')\nplt.plot(false_positive_rate,true_positive_rate, color='red',label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],linestyle='--')\nplt.axis('tight')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')","0198b7ed":"LR_ridge= LogisticRegression(penalty='l2')\nLR_ridge.fit(X_train,y_train)","762c82a8":"y_prob = LR_ridge.predict_proba(X_test)[:,1] # This will give you positive class prediction probabilities  \ny_pred = np.where(y_prob > 0.5, 1, 0) # This will threshold the probabilities to give class predictions.\nLR_ridge.score(X_test, y_pred)","4b10fc4a":"confusion_matrix=metrics.confusion_matrix(y_test,y_pred)\nconfusion_matrix","51ec406c":"auc_roc=metrics.classification_report(y_test,y_pred)\nauc_roc","a8386448":"auc_roc=metrics.roc_auc_score(y_test,y_pred)\nauc_roc","93406e65":"\nfrom sklearn.metrics import roc_curve, auc\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob)\nroc_auc = auc(false_positive_rate, true_positive_rate)\nroc_auc","1d8e261d":"\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10,10))\nplt.title('Receiver Operating Characteristic')\nplt.plot(false_positive_rate,true_positive_rate, color='red',label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],linestyle='--')\nplt.axis('tight')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')","9f0afcbe":"from sklearn.naive_bayes import GaussianNB\nmodel_naive = GaussianNB()\nmodel_naive.fit(X_train, y_train)","40f577fc":"y_prob = model_naive.predict_proba(X_test)[:,1] # This will give you positive class prediction probabilities  \ny_pred = np.where(y_prob > 0.5, 1, 0) # This will threshold the probabilities to give class predictions.\nmodel_naive.score(X_test, y_pred)","19db4160":"print(\"Number of mislabeled points from %d points : %d\"\n      % (X_test.shape[0],(y_test!= y_pred).sum()))","e83f661f":"scores = cross_val_score(model_naive, X, y, cv=10, scoring='accuracy')\nprint(scores)","b39edcf7":"scores.mean()","c78ca25b":"confusion_matrix=metrics.confusion_matrix(y_test,y_pred)\nconfusion_matrix","5776e5c1":"auc_roc=metrics.classification_report(y_test,y_pred)\nauc_roc","cb128184":"auc_roc=metrics.roc_auc_score(y_test,y_pred)\nauc_roc","3d80ff45":"from sklearn.metrics import roc_curve, auc\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob)\nroc_auc = auc(false_positive_rate, true_positive_rate)\nroc_auc","0e200354":"import matplotlib.pyplot as plt\nplt.figure(figsize=(10,10))\nplt.title('Receiver Operating Characteristic')\nplt.plot(false_positive_rate,true_positive_rate, color='red',label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],linestyle='--')\nplt.axis('tight')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')","1be1a92a":"from sklearn.svm import SVC\nsvm_model= SVC()","a01d51a8":"tuned_parameters = {\n 'C': [1, 10, 100,500, 1000], 'kernel': ['linear','rbf'],\n 'C': [1, 10, 100,500, 1000], 'gamma': [1,0.1,0.01,0.001, 0.0001], 'kernel': ['rbf'],\n #'degree': [2,3,4,5,6] , 'C':[1,10,100,500,1000] , 'kernel':['poly']\n    }","2f3dac27":"from sklearn.grid_search import RandomizedSearchCV\n\nmodel_svm = RandomizedSearchCV(svm_model, tuned_parameters,cv=10,scoring='accuracy',n_iter=20)","2bcc60fa":"model_svm.fit(X_train, y_train)\nprint(model_svm.best_score_)","4de4b576":"print(model_svm.grid_scores_)","d668506b":"print(model_svm.best_params_)","72c28b64":"\ny_pred= model_svm.predict(X_test)\nprint(metrics.accuracy_score(y_pred,y_test))","19f7cb96":"confusion_matrix=metrics.confusion_matrix(y_test,y_pred)\nconfusion_matrix","5504e3c6":"auc_roc=metrics.classification_report(y_test,y_pred)\nauc_roc","f8f1e776":"auc_roc=metrics.roc_auc_score(y_test,y_pred)\nauc_roc","8f20d011":"from sklearn.metrics import roc_curve, auc\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\nroc_auc = auc(false_positive_rate, true_positive_rate)\nroc_auc","aac18231":"import matplotlib.pyplot as plt\nplt.figure(figsize=(10,10))\nplt.title('Receiver Operating Characteristic')\nplt.plot(false_positive_rate,true_positive_rate, color='red',label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],linestyle='--')\nplt.axis('tight')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')","369f770e":"tuned_parameters = {\n 'C': [1, 10, 100,500, 1000], 'kernel': ['linear','rbf'],\n 'C': [1, 10, 100,500, 1000], 'gamma': [1,0.1,0.01,0.001, 0.0001], 'kernel': ['rbf'],\n 'degree': [2,3,4,5,6] , 'C':[1,10,100,500,1000] , 'kernel':['poly']\n    }","bdd39330":"from sklearn.grid_search import RandomizedSearchCV\n\nmodel_svm = RandomizedSearchCV(svm_model, tuned_parameters,cv=10,scoring='accuracy',n_iter=20)","f7bc372b":"model_svm.fit(X_train, y_train)\nprint(model_svm.best_score_)","1233fb53":"print(model_svm.grid_scores_)","d7546f03":"print(model_svm.best_params_)","bc10a5eb":"y_pred= model_svm.predict(X_test)\nprint(metrics.accuracy_score(y_pred,y_test))","fddb0c74":"confusion_matrix=metrics.confusion_matrix(y_test,y_pred)\nconfusion_matrix","42865207":"auc_roc=metrics.classification_report(y_test,y_pred)\nauc_roc","fca84c18":"auc_roc=metrics.roc_auc_score(y_test,y_pred)\nauc_roc","e2a24627":"from sklearn.metrics import roc_curve, auc\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\nroc_auc = auc(false_positive_rate, true_positive_rate)\nroc_auc","3a21b8b4":"import matplotlib.pyplot as plt\nplt.figure(figsize=(10,10))\nplt.title('Receiver Operating Characteristic')\nplt.plot(false_positive_rate,true_positive_rate, color='red',label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],linestyle='--')\nplt.axis('tight')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')","75b677a7":"from sklearn.ensemble import RandomForestClassifier\n\nmodel_RR=RandomForestClassifier()\n\n#tuned_parameters = {'min_samples_leaf': range(5,10,5), 'n_estimators' : range(50,200,50),\n                    #'max_depth': range(5,15,5), 'max_features':range(5,20,5)\n                    #}\n               ","c543c376":"model_RR.fit(X_train,y_train)","1513bbb6":"y_prob = model_RR.predict_proba(X_test)[:,1] # This will give you positive class prediction probabilities  \ny_pred = np.where(y_prob > 0.5, 1, 0) # This will threshold the probabilities to give class predictions.\nmodel_RR.score(X_test, y_pred)","6c022880":"confusion_matrix=metrics.confusion_matrix(y_test,y_pred)\nconfusion_matrix","986ac5a7":"auc_roc=metrics.classification_report(y_test,y_pred)\nauc_roc","a79c47f1":"auc_roc=metrics.roc_auc_score(y_test,y_pred)\nauc_roc","30fbdbea":"from sklearn.metrics import roc_curve, auc\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob)\nroc_auc = auc(false_positive_rate, true_positive_rate)","b245d2fb":"import matplotlib.pyplot as plt\nplt.figure(figsize=(10,10))\nplt.title('Receiver Operating Characteristic')\nplt.plot(false_positive_rate,true_positive_rate, color='red',label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],linestyle='--')\nplt.axis('tight')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')","9f61cdb9":"from sklearn.ensemble import RandomForestClassifier\n\nmodel_RR=RandomForestClassifier()\n\ntuned_parameters = {'min_samples_leaf': range(10,100,10), 'n_estimators' : range(10,100,10),\n                    'max_features':['auto','sqrt','log2']\n                    }\n    ","ef02f16b":"from sklearn.grid_search import RandomizedSearchCV\n\nRR_model= RandomizedSearchCV(model_RR, tuned_parameters,cv=10,scoring='accuracy',n_iter=20,n_jobs= -1)","ede0dace":"RR_model.fit(X_train,y_train)","fda7adb1":"print(RR_model.grid_scores_)","d2bbed03":"print(RR_model.best_score_)","d5ac5d8b":"print(RR_model.best_params_)","fbb36906":"y_prob = RR_model.predict_proba(X_test)[:,1] # This will give you positive class prediction probabilities  \ny_pred = np.where(y_prob > 0.5, 1, 0) # This will threshold the probabilities to give class predictions.\nRR_model.score(X_test, y_pred)","77a3d8c1":"confusion_matrix=metrics.confusion_matrix(y_test,y_pred)\nconfusion_matrix","ef8e46a6":"auc_roc=metrics.classification_report(y_test,y_pred)\nauc_roc","d9173b28":"auc_roc=metrics.roc_auc_score(y_test,y_pred)\nauc_roc","f1506838":"\nfrom sklearn.metrics import roc_curve, auc\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob)\nroc_auc = auc(false_positive_rate, true_positive_rate)\nroc_auc","1d250433":"import matplotlib.pyplot as plt\nplt.figure(figsize=(10,10))\nplt.title('Receiver Operating Characteristic')\nplt.plot(false_positive_rate,true_positive_rate, color='red',label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],linestyle='--')\nplt.axis('tight')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')","b26f9a5c":"from sklearn.tree import DecisionTreeClassifier\n\nmodel_tree = DecisionTreeClassifier()","c660d025":"model_tree.fit(X_train, y_train)","1ad1f9c7":"y_prob = model_tree.predict_proba(X_test)[:,1] # This will give you positive class prediction probabilities  \ny_pred = np.where(y_prob > 0.5, 1, 0) # This will threshold the probabilities to give class predictions.\nmodel_tree.score(X_test, y_pred)","fc8a0ca2":"confusion_matrix=metrics.confusion_matrix(y_test,y_pred)\nconfusion_matrix","690aaa62":"auc_roc=metrics.classification_report(y_test,y_pred)\nauc_roc","a3b0257e":"auc_roc=metrics.roc_auc_score(y_test,y_pred)\nauc_roc","495559fe":"from sklearn.metrics import roc_curve, auc\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob)\nroc_auc = auc(false_positive_rate, true_positive_rate)","16dc9912":"import matplotlib.pyplot as plt\nplt.figure(figsize=(10,10))\nplt.title('Receiver Operating Characteristic')\nplt.plot(false_positive_rate,true_positive_rate, color='red',label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],linestyle='--')\nplt.axis('tight')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')","f42e17eb":"from sklearn.tree import DecisionTreeClassifier\n\nmodel_DD = DecisionTreeClassifier()\n\n\ntuned_parameters= {'criterion': ['gini','entropy'], 'max_features': [\"auto\",\"sqrt\",\"log2\"],\n                   'min_samples_leaf': range(1,100,1) , 'max_depth': range(1,50,1)\n                  }\n           ","2f4f0ceb":"from sklearn.grid_search import RandomizedSearchCV\nDD_model= RandomizedSearchCV(model_DD, tuned_parameters,cv=10,scoring='accuracy',n_iter=20,n_jobs= -1,random_state=5)","c86c0157":"DD_model.fit(X_train, y_train)","b6b7f831":"print(DD_model.grid_scores_)","9e17e877":"print(DD_model.best_score_)","febb4249":"print(DD_model.best_params_)","59a68e55":"y_prob = DD_model.predict_proba(X_test)[:,1] # This will give you positive class prediction probabilities  \ny_pred = np.where(y_prob > 0.5, 1, 0) # This will threshold the probabilities to give class predictions.\nDD_model.score(X_test, y_pred)","99c5f64f":"confusion_matrix=metrics.confusion_matrix(y_test,y_pred)\nconfusion_matrix","58404b3e":"auc_roc=metrics.classification_report(y_test,y_pred)\nauc_roc","9f6d4d3c":"auc_roc=metrics.roc_auc_score(y_test,y_pred)\nauc_roc","44dd78c5":"from sklearn.metrics import roc_curve, auc\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob)\nroc_auc = auc(false_positive_rate, true_positive_rate)\nroc_auc","0fa6f7e5":"import matplotlib.pyplot as plt\nplt.figure(figsize=(10,10))\nplt.title('Receiver Operating Characteristic')\nplt.plot(false_positive_rate,true_positive_rate, color='red',label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],linestyle='--')\nplt.axis('tight')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')","2f9e0586":"from sklearn.neural_network import MLPClassifier","e3695588":"mlp = MLPClassifier()\nmlp.fit(X_train,y_train)","8dd02ca5":"y_prob = mlp.predict_proba(X_test)[:,1] # This will give you positive class prediction probabilities  \ny_pred = np.where(y_prob > 0.5, 1, 0) # This will threshold the probabilities to give class predictions.\nmlp.score(X_test, y_pred)","5cc9ffbe":"confusion_matrix=metrics.confusion_matrix(y_test,y_pred)\nconfusion_matrix","63808da7":"auc_roc=metrics.classification_report(y_test,y_pred)\nauc_roc","7d8ed0a6":"auc_roc=metrics.roc_auc_score(y_test,y_pred)\nauc_roc","c5803bd0":"from sklearn.metrics import roc_curve, auc\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob)\nroc_auc = auc(false_positive_rate, true_positive_rate)\nroc_auc","6f8d2c3c":"import matplotlib.pyplot as plt\nplt.figure(figsize=(10,10))\nplt.title('Receiver Operating Characteristic')\nplt.plot(false_positive_rate,true_positive_rate, color='red',label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],linestyle='--')\nplt.axis('tight')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')","b4a6c0db":"'''\nfrom sklearn.neural_network import MLPClassifier\n\nmlp = MLPClassifier()\n\ntuned_parameters={'hidden_layer_sizes': range(1,200,10) , 'activation': ['tanh','logistic','relu'],\n                  'alpha':[0.0001,0.001,0.01,0.1,1,10], 'max_iter': range(50,200,50)\n    \n}\n'''","fb45e4e1":"#from sklearn.grid_search import RandomizedSearchCV\n#model_mlp= RandomizedSearchCV(mlp_model, tuned_parameters,cv=10,scoring='accuracy',n_iter=5,n_jobs= -1,random_state=5)","180f2a60":"#model_mlp.fit(X_train, y_train)","8ca7d455":"#print(model_mlp.grid_scores_)","077fb4b4":"#print(model_mlp.best_score_)","f2fe8ccf":"#print(model_mlp.best_params_)","14007a5b":"'''\ny_prob = model_LR.predict_proba(X_test)[:,1] # This will give you positive class prediction probabilities  \ny_pred = np.where(y_prob > 0.5, 1, 0) # This will threshold the probabilities to give class predictions.\nmodel_LR.score(X_test, y_pred)\n'''","93f3cae3":"#confusion_matrix=metrics.confusion_matrix(y_test,y_pred)\n#confusion_matrix","74973d18":"#auc_roc=metrics.classification_report(y_test,y_pred)\n#auc_roc","fe05b5c5":"#auc_roc=metrics.roc_auc_score(y_test,y_pred)\n#auc_roc","ee537af5":"'''\nfrom sklearn.metrics import roc_curve, auc\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob)\nroc_auc = auc(false_positive_rate, true_positive_rate)\n'''","bcf861a8":"'''\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10,10))\nplt.title('Receiver Operating Characteristic')\nplt.plot(false_positive_rate,true_positive_rate, color='red',label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],linestyle='--')\nplt.axis('tight')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n'''","4f275844":"### Let us take only first two principal components and visualise it using K-means clustering","523ad35a":"### n_jobs\n**This parameter tells the engine how many processors is it allowed to use. A value of \u201c-1\u201d means there is no restriction whereas a value of \u201c1\u201d means it can only use one processor.**","deda91a9":"# Principal Component Analysis","e12fcc07":"**We can see that the last 4 components has less amount of variance  of the data.The 1st 17 components retains more than 90% of the data.**","0bfa473a":"# Support Vector machine with polynomial Kernel","b364c409":"**Note**: We can avoid PCA here since the dataset is very small.","4ebc4a8d":"### Thus default decision tree model is giving us best accuracy score","39232837":"### Plotting boxplot to see the distribution of the data","6341ed3b":"**Separating features and label**","f20848e9":"### Taking a look at the correlation ","d366081c":"# Standardising the data","3b1815b7":"### Checking the encoded values","8da9705a":"**1)Criterion:** Decision trees use multiple algorithms to decide to split a node in two or more sub-nodes.Decision tree splits the nodes on all available variables and then selects the split which results in most homogeneous sub-nodes. The details of Gini and entropy needs detail explanation.\n\n2)**max_depth(Maximum depth of tree (vertical depth)):**\nUsed to control over-fitting as higher depth will allow model to learn relations very specific to a particular sample.\n\n**max_features** and **min_samples_leaf** is same as Random Forest classifier","516015b8":"### Default Decision Tree model","685cb2b3":"**B) n_estimators** : This is the number of trees you want to build before taking the maximum voting or averages of predictions. Higher number of trees give you better performance but makes your code slower. You should choose as high value as your processor can handle because this makes your predictions stronger and more stable.","ccff5d47":"The **gamma** parameter defines how far the influence of a single training example reaches, with low values meaning \u2018far\u2019 and high values meaning \u2018close\u2019. The **gamma** parameters can be seen as the inverse of the radius of influence of samples selected by the model as support vectors.\n\nThe **C** parameter trades off misclassification of training examples against simplicity of the decision surface. A low **C** makes the decision surface smooth, while a high **C** aims at classifying all training examples correctly by giving the model freedom to select more samples as support vectors.","2ea5d15b":"### Let us tune the hyperparameters of the Decision tree model","a3c2b2d8":"### Let us check if there is any null values","b8e90e15":"# Support Vector Machine","fc7390a9":"**We can see that the dataset has values in strings.We need to convert all the unique values to integers. Thus we perform label encoding on the data.**","308e2402":"### Reading the file ","8643adb9":"## Neural Network","ed1afb70":" **A)max_features**: These are the maximum number of features Random Forest is allowed to try in individual tree.\n**1)Auto** : This will simply take all the features which make sense in every tree.Here we simply do not put any restrictions on the individual tree.\n**2)sqrt** : This option will take square root of the total number of features in individual run. For instance, if the total number of variables are 100, we can only take 10 of them in individual tree.\n**3)log2**:It  is another option which takes log to the base 2 of the features input.\n\n**Increasing max_features generally improves the performance of the model as at each node now we have a higher number of options to be considered.But, for sure, you decrease the speed of algorithm by increasing the max_features. Hence, you need to strike the right balance and choose the optimal max_features.**","fdc4f7cc":"**1) hidden_layer_sizes**  : Number of hidden layers in the network.(default is 100).Large number may overfit the data.\n\n**2)activation**: Activation function for the hidden layer.\nA)\u2018logistic\u2019, the logistic sigmoid function, returns f(x) = 1 \/ (1 + exp(-x)).\nB)\u2018tanh\u2019, the hyperbolic tan function, returns f(x) = tanh(x).\nC)\u2018relu\u2019, the rectified linear unit function, returns f(x) = max(0, x)\n\n**3)alpha:** L2 penalty (regularization term) parameter.(default 0.0001)\n\n**4)max_iter:** Maximum number of iterations. The solver iterates until convergence (determined by \u2018tol\u2019) or this number of iterations.(default 200)","c8446765":"# Default Logistic Regression","e71ace4f":"### Applying default Neural Network model","4c496d30":"**C)min_sample_leaf**:  Leaf is the end node of a decision tree. A smaller leaf makes the model more prone to capturing noise in train data. Hence it is important to try different values to get good estimate.","afa2416b":"# Gaussian Naive Bayes","67830565":"**L1 and L2 are regularization parameters.They're used to avoid overfiting.Both L1 and L2 regularization prevents overfitting by shrinking (imposing a penalty) on the coefficients.**  \n    **L1 is the first moment norm |x1-x2| (|w| for regularization case) that is simply the absolute d\u0131stance between two points where L2 is second moment norm corresponding to Eucledian Distance that is  |x1-x2|^2 (|w|^2 for regularization case).**     \n       **In simple words,L2 (Ridge) shrinks all the coefficient by the same proportions but eliminates none, while L1 (Lasso) can shrink some coefficients to zero, performing variable selection.\nIf all the features are correlated with the label, ridge outperforms lasso, as the coefficients are never zero in ridge. If only a subset of features are correlated with the label, lasso outperforms ridge as in lasso model some coefficient can be shrunken to zero.**","cdb019f9":"It is turning out to be computationally expensive for me with tuned model. Hence I am not running this. Also any suggestion to improvise it is welcome. :)","0c020ea3":"**Thus we have two claasification. Either the mushroom is poisonous or edible**","4b2b910a":"# Logistic Regression(Tuned model)","50d470e8":"# Performing PCA by taking 17 components with maximum Variance","d9fea496":"### Thus default Random forest model is giving us best accuracy.","fd6a4918":"### Let us tuned the parameters of Random Forest just for the purpose of knowledge","f3b9012c":"### Splitting the data into training and testing dataset","7e43d5ef":"**The grid search provided by GridSearchCV exhaustively generates candidates from a grid of parameter values specified with the tuned_parameter.The GridSearchCV instance implements the usual estimator API: when \u201cfitting\u201d it on a dataset all the possible combinations of parameter values are evaluated and the best combination is retained.**","943f9e4a":"**There are 3 features which can be tuned to improve the performance of Random Forest**    \n\n**1) max_features 2) n_estimators  3) min_sample_leaf**","c2c127d1":"### Thus using K-means we are able segregate 2 classes well using the first two components with maximum variance.","876af80c":"###  Importing all the libraries","4fe4fd4d":"### Trying default model","8be11ced":"# Support Vector Machine without polynomial kernel","c8f7e092":"**Thus we have 22 features(1st one is label)  and 8124 instances.Now let us check which features constitutes maximum information.** ","25374acb":"The grid search provided by GridSearchCV exhaustively generates candidates from a grid of parameter values specified with the tuned_parameter**.The GridSearchCV instance implements the usual estimator API: when \u201cfitting\u201d it on a dataset all the possible combinations of parameter values are evaluated and the best combination is retained.\nBut it is proving computationally expensive here.So I am opting for RandomizedSearchCV.\n\nRandomizedSearchCV implements a randomized search over parameters, where each setting is sampled from a distribution over possible parameter values. This has two main benefits over an exhaustive search:\n1)A budget can be chosen independent of the number of parameters and possible values.\n2)Adding parameters that do not influence the performance does not decrease efficiency.","fdfc4ad3":"I am  going to apply 6 Supervised machine learning models on the given dataset.The strategy is to apply default model first with no tuning of the hyperparameter and then tuned them with different hyperparameter values and then I'll plot ROC curve to select the best machine learning model.The models used are as follows:\n**1) Principal Component Analysis\n**2)Logistic Regression**\n **3)Gaussian Naive Bayes**\n  **4)Support Vector Machine**\n  **5)Random Forest Classifier** \n  **6)Decision trees**\n   **7)Simple neural network**","505e845f":"### Tuning the hyperparameters of the neural network"}}