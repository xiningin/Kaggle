{"cell_type":{"a3c650c4":"code","ead7b755":"code","77e564d3":"code","fc4b0337":"code","85ce3286":"code","49fc26be":"code","13614fde":"code","eff00721":"code","941c2c7b":"code","92375c74":"code","ca1a1d9a":"code","8101192a":"code","529588df":"code","e7174f15":"code","7f7587c9":"code","ef10c367":"code","b622288d":"code","efeaaaac":"code","373e9f38":"code","5fe63c77":"code","10db26da":"code","9b213d03":"code","3d5e95d7":"code","cc843567":"code","098d6981":"code","7fde6015":"code","995c4baa":"code","d37ce2fc":"code","473dcf85":"code","c27ea0f5":"code","4df94328":"code","94bd54b8":"code","91670ec9":"code","c977471c":"code","1805bc4d":"code","0221f0ac":"markdown","ec408763":"markdown","d8f579bd":"markdown","55a29821":"markdown","aec2c19f":"markdown","16cae46f":"markdown","37ba5e2f":"markdown","a3f82d5a":"markdown","046e7517":"markdown","6a498bbb":"markdown","6ef907f5":"markdown","dbef43f4":"markdown","fdbc0a79":"markdown","210054c9":"markdown","1f4ef8f8":"markdown","da142c60":"markdown","3db1c284":"markdown","3ebce61d":"markdown","4d8d3747":"markdown","95387426":"markdown","a5374608":"markdown","a8c2cdfa":"markdown","2db1192f":"markdown","588d7f14":"markdown","6d498e55":"markdown","780d7384":"markdown","4a46f9d0":"markdown","29be9dd3":"markdown","dbfbd6c0":"markdown","da92710e":"markdown","4c35d919":"markdown"},"source":{"a3c650c4":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set()\n\nimport warnings\nwarnings.filterwarnings('ignore')","ead7b755":"train = pd.read_csv('\/kaggle\/input\/customer\/Train.csv')\ntest = pd.read_csv('\/kaggle\/input\/customer\/Test.csv')\nsample_submission = pd.read_csv('\/kaggle\/input\/customer\/sample_submission.csv')","77e564d3":"train.head()","fc4b0337":"#combine train and test dataset\ncombined_data = pd.concat([train, test], ignore_index=True)\n\ncombined_data.info()","85ce3286":"combined_data.describe(include='all')","49fc26be":"print('Percentage of missing values:')\nprint('-----------------------------')\nprint(combined_data.isnull().sum().sort_values(ascending=False)[1:] \/ 10695 * 100)","13614fde":"#check for imbalance\nsns.countplot(train['Segmentation'], order=['A','B','C','D']);","eff00721":"#plot the distribution of numerical features\ntrain.hist(bins=50,figsize=(10,10),grid=False)\nplt.tight_layout()\nplt.show()","941c2c7b":"#check for duplicate\nprint('Duplicated value(s) on the train dataset : ', train.duplicated().sum())\nprint('Duplicated value(s) on the test dataset  : ', test.duplicated().sum())","92375c74":"sns.countplot('Family_Size', hue='Segmentation', data=train, hue_order=['A','B','C','D']);","ca1a1d9a":"#fill with mode\ntrain['Family_Size'].fillna(train['Family_Size'].mode()[0], inplace=True)\ntest['Family_Size'].fillna(test['Family_Size'].mode()[0], inplace=True)","8101192a":"sns.countplot('Ever_Married', hue='Segmentation', data=train, hue_order=['A','B','C','D']);","529588df":"#fill with mode\ntrain['Ever_Married'].fillna(train['Ever_Married'].mode()[0], inplace=True)\ntest['Ever_Married'].fillna(test['Ever_Married'].mode()[0], inplace=True)","e7174f15":"sns.countplot(y='Profession', hue='Segmentation', data=train, hue_order=['A','B','C','D']);","7f7587c9":"train.loc[(train['Profession'].isnull() & (train['Segmentation']=='C')),['Profession']] = 'Artist'\ntrain.loc[(train['Profession'].isnull() & (train['Segmentation']=='D')),['Profession']] = 'Healthcare'","ef10c367":"#fill with mode\ntrain['Profession'].fillna(train['Profession'].mode()[0], inplace=True)\ntest['Profession'].fillna(test['Profession'].mode()[0], inplace=True)","b622288d":"sns.countplot('Var_1', hue='Segmentation', data=train, hue_order=['A','B','C','D']);","efeaaaac":"#fill with mode\ntrain['Var_1'].fillna(train['Var_1'].mode()[0], inplace=True)\ntest['Var_1'].fillna(test['Var_1'].mode()[0], inplace=True)","373e9f38":"sns.countplot('Graduated', hue='Segmentation', data=train, hue_order=['A','B','C','D']);","5fe63c77":"train.loc[(train['Graduated'].isnull() & (train['Segmentation']=='D')), ['Graduated']] = 'No'","10db26da":"train['Graduated'].fillna(train['Graduated'].mode()[0], inplace=True)\ntest['Graduated'].fillna(test['Graduated'].mode()[0], inplace=True)","9b213d03":"#check for missing values\ntrain.isnull().sum().sort_values(ascending=False)","3d5e95d7":"#copy features that are needed later\ntarget_array = train['Segmentation'].copy()\ntest_id = test['ID'].copy()\n\n#drop features\ntrain.drop(['Segmentation'], axis=1, inplace=True)\n\nprint('train shape: ', train.shape)\nprint('test shape: ', test.shape)","cc843567":"#create Work_Experience_given feature\ntrain['Work_Experience_is_given']=train['Work_Experience'].notnull()*1\ntest['Work_Experience_is_given']=train['Work_Experience'].notnull()*1\n\n#fill missing values\ntrain['Work_Experience'].fillna(train['Work_Experience'].mode()[0], inplace=True)\ntest['Work_Experience'].fillna(test['Work_Experience'].mode()[0], inplace=True)","098d6981":"#convert age in bins\n#train['Age']=pd.cut(train['Age'],bins=[10,20,30,40,50,60,70,80,90],labels=[15,25,35,45,55,65,75,85])\n#test['Age']=pd.cut(test['Age'],bins=[10,20,30,40,50,60,70,80,90],labels=[15,25,35,45,55,65,75,85])\n\n#train['Age']=train['Age'].astype('int')\n#test['Age']=test['Age'].astype('int')","7fde6015":"from sklearn import preprocessing\n\nle = preprocessing.LabelEncoder()\n\nfor feature in ['Gender','Ever_Married','Graduated','Profession','Spending_Score','Var_1']:\n    train[feature]=le.fit_transform(train[feature])\n    test[feature]=le.transform(test[feature])","995c4baa":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\ntrain = pd.DataFrame(scaler.fit_transform(train), columns=train.columns)\ntest = pd.DataFrame(scaler.transform(test), columns=test.columns)","d37ce2fc":"#define a normality test function\ndef normalityTest(data, alpha=0.05):\n    \"\"\"data (array)   : The array containing the sample to be tested.\n\t   alpha (float)  : Significance level.\n\t   return True if data is normal distributed\"\"\"\n    \n    from scipy import stats\n    \n    statistic, p_value = stats.normaltest(data)\n    \n    #null hypothesis: array comes from a normal distribution\n    if p_value < alpha:  \n        #The null hypothesis can be rejected\n        is_normal_dist = False\n    else:\n        #The null hypothesis cannot be rejected\n        is_normal_dist = True\n    \n    return is_normal_dist","473dcf85":"#check normality of all numericaal features and transform it if not normal distributed\nfor feature in train.columns:\n    if (train[feature].dtype != 'object'):\n        if normalityTest(train[feature]) == False:\n            train[feature] = np.log1p(train[feature])\n            test[feature] = np.log1p(test[feature])","c27ea0f5":"X = train\ny = target_array\n\nX_to_be_predicted = test","4df94328":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7)","94bd54b8":"from lightgbm import LGBMClassifier\n\n#tuning the model\nmodel = LGBMClassifier(learning_rate=0.1,\n                       n_estimators=1200,\n                       max_depth=5,\n                       min_child_weight=1,\n                       gamma=0,\n                       subsample=0.8,\n                       colsample_bytree=0.8,\n                       nthread=4,\n                       scale_pos_weight=3,\n                       seed=27)\n\n#fitting\nmodel.fit(X_train, y_train)","91670ec9":"from sklearn.metrics import classification_report\n\n#print a classification report\nprint(classification_report(y_test, model.predict(X_test)))","c977471c":"#make a prediction\ny_predict = model.predict(X_to_be_predicted)\ny_predict","1805bc4d":"#sava results to a file\nresults = pd.DataFrame({'ID': test_id, 'Segmentation': y_predict})\nresults.to_csv('my_submission.csv', index=False)\nresults.head()","0221f0ac":"### Check for Any Missing Values","ec408763":"- Family_Size","d8f579bd":"### Plot The Distribution of Numerical Features","55a29821":"Now, let's take a quick look at the train and test datasets to gain some initial insight.","aec2c19f":"Fill the rest with mode","16cae46f":"This kernel is going to dealing in a problem statement on [Hackathon Competition: Customer Segmentation](https:\/\/datahack.analyticsvidhya.com\/contest\/janatahack-customer-segmentation\/). In this competition, we will divide a customer base into groups of individuals that are similar in specific ways relevant to marketing, such as age, gender, interests and spending habits.\n\n**Competition Description**:\n\nAn automobile company has plans to enter new markets with their existing products (P1, P2, P3, P4 and P5). After intensive market research, they\u2019ve deduced that the behavior of new market is similar to their existing market. \n\nIn their existing market, the sales team has classified all customers into 4 segments (A, B, C, D). Then, they performed segmented outreach and communication for different segment of customers. This strategy has work exceptionally well for them. They plan to use the same strategy on new markets and have identified 2627 new potential customers. \n\nYou are required to help the manager to predict the right group of the new customers.","37ba5e2f":"Then, import the data","a3f82d5a":"# Import Libraries & The Data\nFirst, we import necessary libraries, such as:","046e7517":"### Check for Duplicate","6a498bbb":"Check for percentage of missing values in each feature","6ef907f5":"### Creating features matrix (X) and target array (y)","dbef43f4":"The distribution of numerical features are right skewed. So, we need to transform it later.","fdbc0a79":"Model training : LGBMClassifier","210054c9":"The target array is balance, so it is a balance classifiction problem.","1f4ef8f8":"We get 90% accuracy.","da142c60":"- Graduated","3db1c284":"# Feature Engineering\n### Copy and Drop","3ebce61d":"- Profession","4d8d3747":"### Check for Imbalance of Target Array","95387426":"# Creating a Model\nWe begin by splitting data into two subsets: for training data and for testing data.","a5374608":"We will fill missing data in Work_Experience later.","a8c2cdfa":"### Labelling Data","2db1192f":"# Exploratory Data Analysis\n### Train and Test Data Exploration\nDisplay the ```head()``` to familiarize ourself with the data.","588d7f14":"- Var_1","6d498e55":"### Fill Missing Data\nWe will start with the most missing values, and then continuing our way to the least.","780d7384":"### Creating New Features","4a46f9d0":"### Normality Test","29be9dd3":"- Work_Experience\n\nThe missing values will be filled in the Feature Engineering Section, because we will need the missing values there.","dbfbd6c0":"- Ever_Married","da92710e":"We can see that most of the artists are in segment C and most of the Healthcare are in segment D.","4c35d919":"### Standardization"}}