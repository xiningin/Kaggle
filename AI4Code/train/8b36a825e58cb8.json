{"cell_type":{"aa61da39":"code","f6964ddd":"code","bedf3fb2":"code","6f8bef59":"code","096b4f65":"code","b43f627f":"code","39f4fc81":"code","a22f12c2":"code","b8a633b8":"code","fd05e5e2":"code","15c645c7":"code","25ccf447":"code","60616849":"code","85c15563":"code","689eeffc":"code","79fdc71c":"code","cac92c67":"code","30170449":"code","e5b105f6":"code","40f43fba":"code","42329c86":"code","85481643":"code","853328e3":"markdown","e7099b72":"markdown","2c669fdf":"markdown","cb8eb174":"markdown","6bfff825":"markdown","9796b8f2":"markdown","b04569a8":"markdown","c63b52dd":"markdown","1bcf8856":"markdown"},"source":{"aa61da39":"import pandas as pd\nimport plotly as px\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport itertools\nfrom itertools import chain\nfrom sklearn.feature_selection import RFE\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, learning_curve, train_test_split\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix, roc_curve, precision_recall_curve, accuracy_score\nimport warnings\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport plotly.figure_factory as ff\n\nplt.rcParams['figure.figsize'] = 8, 5\nplt.style.use('ggplot')\n\nwarnings.filterwarnings('ignore') #ignore warning messages ","f6964ddd":"import pandas as pd \n\nraw_df = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')\nraw_df","bedf3fb2":"raw_df.info()","6f8bef59":"raw_df.describe()","096b4f65":"#Drop the column with all missing values\nraw_df = raw_df.dropna(axis=1)","b43f627f":"#get the new count of the number of rows\nraw_df.shape","39f4fc81":"#get a count of the number of Malignant (M) or Benign(B) cells\nraw_df['diagnosis'].value_counts()\n","a22f12c2":"#Visualize the count through seaborn library\nsns.countplot(raw_df['diagnosis'], label='count')","b8a633b8":"features = ['radius','texture','perimeter','area','smoothness','compactness','concavity','concave points','symmetry','fractal_dimension']\n\nfor feature in features:\n    print(\"{} distribution\".format(feature))\n    sns.boxplot(data=raw_df[['{}_mean'.format(feature), '{}_se'.format(feature), '{}_worst'.format(feature)]])\n    plt.title('Distribution of {}'.format(feature))\n    plt.show()","fd05e5e2":"#print the first 5 rows of the new data\nraw_df.head(5)","15c645c7":"#get the correlation of the columns\nraw_df.iloc[:,1:12].corr()","25ccf447":"print('Pairplot')\nsns.pairplot(data=raw_df[['diagnosis','area_mean','texture_mean','smoothness_mean','concavity_mean','symmetry_mean']], hue=\"diagnosis\", height=3, diag_kind=\"hist\")\nplt.show()","60616849":"#Visualize the correlation\nplt.figure(figsize=(12,10))\nsns.heatmap(raw_df.iloc[:,1:12].corr(), annot=True, fmt='.0%')","85c15563":"#Encode the categorical data values\nfrom sklearn.preprocessing import LabelEncoder\nlabelencoder_Y = LabelEncoder()\nraw_df.iloc[:,1]= labelencoder_Y.fit_transform(raw_df.iloc[:,1].values)\n\nraw_df.iloc[:,1]","689eeffc":"#split the data set into independent(X) and dependent (Y) data sets \nX= raw_df.iloc[:,2:31].values\nY= raw_df.iloc[:,1].values\n\n","79fdc71c":"#split the dataset into 75% training and 25% testing\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.25, random_state =0)","cac92c67":"#scale the data(feature scaling)\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)","30170449":"#create a function for thr models\ndef models(X_train, Y_train):\n    #Logistic Regression \n    from sklearn.linear_model import LogisticRegression\n    log = LogisticRegression(random_state=0)\n    log.fit(X_train, Y_train)\n    \n    #Decision Tree\n    from sklearn.tree import DecisionTreeClassifier\n    tree = DecisionTreeClassifier(criterion = 'entropy', random_state=0)\n    tree.fit(X_train, Y_train)\n    \n    #Random Forest Claasifier\n    from sklearn.ensemble import RandomForestClassifier\n    forest = RandomForestClassifier(n_estimators= 10, criterion ='entropy', random_state = 0)\n    forest.fit(X_train, Y_train)\n    \n    #Print the models accuracy on the training data\n    print('[0]Logistc Regreesion Training Accuracy: ', log.score(X_train, Y_train))\n    print('[1]Decision Tree Classifier Training Accuracy: ', tree.score(X_train, Y_train))\n    print('[2]Random Forest Train Training Accuracy: ', forest.score(X_train, Y_train))\n    \n    return log, tree, forest","e5b105f6":"#Getting all of the models\nmodel = models(X_train, Y_train)","40f43fba":"#Test model sccuracy on test data on confusion matrix\nfrom sklearn.metrics import confusion_matrix \n\nfor i in range( len(model)):\n    print('MODEL', i)\n    \n    cm = confusion_matrix(Y_test, model[i].predict(X_test))\n    TP = cm[0][0]\n    TN = cm[1][1]\n    FN = cm[1][0]\n    FP = cm[0][1]\n\n    print(cm)\n    print('Testing Accuracy = ', (TP+TN)\/(TP+TN+FN+FP))\n    print()","42329c86":"#show another way to get metrics of the models\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nfor i in range( len(model)):\n    print('MODEL', i)\n    print(classification_report(Y_test, model[i].predict(X_test)))\n    print(accuracy_score(Y_test, model[i].predict(X_test)))\n","85481643":"#Print the prediction of Random Forest Classifier Model\npred = model[2].predict(X_test)\nprint(pred)\nprint()\nprint(Y_test)","853328e3":"The dataset contains  569 rows and 33 columns.The dataset contains both numeric and categorical columns. Our objective is to create a model to predict the value in the column 'dignosis'.","e7099b72":"# Correlation of the variables","2c669fdf":"Let's check the data types and missing values in the various columns.","cb8eb174":"# Distribution of the features","6bfff825":"# Introduction\n## The dataset\nThe Breast Cancer (Wisconsin) Diagnosis dataset contains the diagnosis and a set of 30 features describing the characteristics of the cell nuclei present in the digitized image of a fine needle aspirate (FNA) of a breast mass.\n\nTen real-valued features are computed for each cell nucleus:\n\nradius (mean of distances from center to points on the perimeter);\n\ntexture (standard deviation of gray-scale values);\n\nperimeter;\n\narea;\n\nsmoothness (local variation in radius lengths);\n\ncompactness (perimeter^2 \/ area - 1.0);\n\nconcavity (severity of concave portions of the contour);\n\nconcave points (number of concave portions of the contour);\n\nsymmetry;\n\nfractal dimension (\u201ccoastline approximation\u201d - 1).\n\nThe mean, standard error (SE) and \u201cworst\u201d or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. We will analyze the features to understand the predictive value for diagnosis. We will then create models using two different algorithms and use the models to predict the diagnosis.\n\n## Fine needle aspiration\nFine-needle aspiration (FNA) is a diagnostic procedure used to investigate lumps or masses. In this technique, a thin (23\u201325 gauge), hollow needle is inserted into the mass for sampling of cells that, after being stained, will be examined under a microscope (biopsy). Fine-needle aspiration biopsies are very safe minor surgical procedures.\n","9796b8f2":"## Create Model for prediction ","b04569a8":"In this notebook, I will train a ***logistic regression*** model using the Breast Cancer dataset to predict whether tumor is belign or malingnent. this is  a *binary classification problem*.","c63b52dd":"## Missing value","1bcf8856":"## Read the data"}}