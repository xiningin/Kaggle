{"cell_type":{"ed7be939":"code","45b3c4e2":"code","67e8b319":"code","9a1d47cb":"code","6ef2b17a":"code","7c1a0a6d":"code","4846f516":"code","279a18cf":"code","24c34ea5":"code","fce0dcd4":"code","6c814eae":"code","b0658b38":"code","61578882":"code","58bc5c5a":"code","e3e322b1":"markdown","22c32f33":"markdown","ecbb9c1e":"markdown","766845d1":"markdown","8f8dcf18":"markdown","4cd4bb71":"markdown","836342e2":"markdown","c35e065b":"markdown","1e39c1e7":"markdown","958b59c0":"markdown","e43c4b49":"markdown","bb1d32e5":"markdown"},"source":{"ed7be939":"import keras\nfrom keras.datasets import cifar10\nfrom keras.models import Sequential\nfrom keras import datasets, layers, models\nfrom keras.utils import np_utils\nfrom keras import regularizers\nfrom keras.layers import Dense, Dropout, BatchNormalization\nimport matplotlib.pyplot as plt\nimport numpy as np","45b3c4e2":"(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()","67e8b319":"# Checking the number of rows (records) and columns (features)\nprint(train_images.shape)\nprint(train_labels.shape)\nprint(test_images.shape)\nprint(test_labels.shape)","9a1d47cb":"# Checking the number of unique classes \nprint(np.unique(train_labels))\nprint(np.unique(test_labels))","6ef2b17a":"# Creating a list of all the class labels\nclass_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n               'dog', 'frog', 'horse', 'ship', 'truck']","7c1a0a6d":"# Visualizing some of the images from the training dataset\nplt.figure(figsize=[10,10])\nfor i in range (25):    # for first 25 images\n  plt.subplot(5, 5, i+1)\n  plt.xticks([])\n  plt.yticks([])\n  plt.grid(False)\n  plt.imshow(train_images[i], cmap=plt.cm.binary)\n  plt.xlabel(class_names[train_labels[i][0]])\n\nplt.show()","4846f516":"# Converting the pixels data to float type\ntrain_images = train_images.astype('float32')\ntest_images = test_images.astype('float32')\n \n# Standardizing (255 is the total number of pixels an image can have)\ntrain_images = train_images \/ 255\ntest_images = test_images \/ 255 \n\n# One hot encoding the target class (labels)\nnum_classes = 10\ntrain_labels = np_utils.to_categorical(train_labels, num_classes)\ntest_labels = np_utils.to_categorical(test_labels, num_classes)","279a18cf":"# Creating a sequential model and adding layers to it\n\nmodel = Sequential()\n\nmodel.add(layers.Conv2D(32, (3,3), padding='same', activation='relu', input_shape=(32,32,3)))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.Conv2D(32, (3,3), padding='same', activation='relu'))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.MaxPooling2D(pool_size=(2,2)))\nmodel.add(layers.Dropout(0.3))\n\nmodel.add(layers.Conv2D(64, (3,3), padding='same', activation='relu'))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.Conv2D(64, (3,3), padding='same', activation='relu'))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.MaxPooling2D(pool_size=(2,2)))\nmodel.add(layers.Dropout(0.5))\n\nmodel.add(layers.Conv2D(128, (3,3), padding='same', activation='relu'))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.Conv2D(128, (3,3), padding='same', activation='relu'))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.MaxPooling2D(pool_size=(2,2)))\nmodel.add(layers.Dropout(0.5))\n\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(128, activation='relu'))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(num_classes, activation='softmax'))    # num_classes = 10\n\n# Checking the model summary\nmodel.summary()","24c34ea5":"model.compile(optimizer='adam', loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])","fce0dcd4":"history = model.fit(train_images, train_labels, batch_size=64, epochs=100,\n                    validation_data=(test_images, test_labels))","6c814eae":"# Loss curve\nplt.figure(figsize=[6,4])\nplt.plot(history.history['loss'], 'black', linewidth=2.0)\nplt.plot(history.history['val_loss'], 'green', linewidth=2.0)\nplt.legend(['Training Loss', 'Validation Loss'], fontsize=14)\nplt.xlabel('Epochs', fontsize=10)\nplt.ylabel('Loss', fontsize=10)\nplt.title('Loss Curves', fontsize=12)","b0658b38":"# Accuracy curve\nplt.figure(figsize=[6,4])\nplt.plot(history.history['accuracy'], 'black', linewidth=2.0)\nplt.plot(history.history['val_accuracy'], 'blue', linewidth=2.0)\nplt.legend(['Training Accuracy', 'Validation Accuracy'], fontsize=14)\nplt.xlabel('Epochs', fontsize=10)\nplt.ylabel('Accuracy', fontsize=10)\nplt.title('Accuracy Curves', fontsize=12)","61578882":"# Making the Predictions\npred = model.predict(test_images)\nprint(pred)\n\n# Converting the predictions into label index \npred_classes = np.argmax(pred, axis=1)\nprint(pred_classes)","58bc5c5a":"# Plotting the Actual vs. Predicted results\n\nfig, axes = plt.subplots(5, 5, figsize=(15,15))\naxes = axes.ravel()\n\nfor i in np.arange(0, 25):\n    axes[i].imshow(test_images[i])\n    axes[i].set_title(\"True: %s \\nPredict: %s\" % (class_names[np.argmax(test_labels[i])], class_names[pred_classes[i]]))\n    axes[i].axis('off')\n    plt.subplots_adjust(wspace=1)","e3e322b1":"<a id=\"section-two\"><\/a>\n## Reading the CIFAR-10 dataset from Keras datasets & setting train and test data","22c32f33":"<a id=\"section-ten\"><\/a>\n## Predicting the Result\n\nHere is the fun part. Let's take 25 images from the testing data and see how many of it we predicted correctly. ","ecbb9c1e":"<a id=\"section-nine\"><\/a>\n## Visualizing the Evaluation\n\n* Loss Curve - Comparing the Training Loss with the Testing Loss over increasing Epochs.\n* Accuracy Curve - Comparing the Training Accuracy with the Testing Accuracy over increasing Epochs.","766845d1":"### Table of Content\n   1. [Importing Dependencies](#section-one)\n   2. [Reading the cifar10 dataset from Keras datasets & setting train and test data](#section-two)\n   3. [Some EDA](#section-three)\n   4. [Data Preprocessing](#section-four)\n   5. [Building the CNN Model using Keras](#section-five) \n*       5.1 [Setting up Layers](#section-six)\n*       5.2 [Compiling the Model](#section-seven)\n*       5.3 [Fitting the Model](#section-eight)\n   6. [Visualizing the Evaluation](#section-nine)\n   7. [Predicting the Results](#section-ten)\n","8f8dcf18":"<a id=\"section-four\"><\/a>\n## Data Preprocessing\n\n* The reason for Standardizing\/Normalizing is to convert all pixel values to values between 0 and 1.\n* The reason for converting type to float is that to_categorical (one hot encoding) needs the data to be of type float by default.\n* The reason for using to_categorical is that the loss function that we will be using in this code (categorical_crossentropy) when compiling the model needs data to be one hot encoded.\n","4cd4bb71":"## A Simple Keras CNN trained on CIFAR-10 dataset with over 88% accuracy (Without Data Augmentation)\n\nCIFAR-10 definition taken from Wikipedia: \n \nThe CIFAR-10 dataset (Canadian Institute For Advanced Research) is a collection of images that are commonly used to train machine learning and computer vision algorithms. It is one of the most widely used datasets for machine learning research. The CIFAR-10 dataset contains 60,000 32x32 color images in 10 different classes. The 10 different classes represent airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. There are 6,000 images of each class.\n\nIn the code below, I have used Keras to build an image cassification model trained on the CIFAR-10 dataset. It uses the following layers\/functions:\n* **For building the Model** - CNN, Maxpooling and Dense Layers.\n* **For Activation Function** - ReLU (in CNN layers for handling image pixels) and Softmax (for final classification).\n* **For handling Overfitting (Regularizing)** - DropOut Layer.\n* **For normalizing\/standardizing the inputs between the layers (within the network)** and hence accelerating the training, providing regularization and reducing the generalization error - Batch Normalization Layer.","836342e2":"<a id=\"section-five\"><\/a>\n## Building the CNN Model using Keras","c35e065b":"<a id=\"section-one\"><\/a>\n## Importing Dependencies","1e39c1e7":"<a id=\"section-eight\"><\/a>\n### Fitting the Model \n* Batch Size is used for Adam optimizer.\n* Epochs - One epoch is one complete cycle (forward pass + backward pass).","958b59c0":"<a id=\"section-three\"><\/a>\n## EDA (Exploratory Data Analysis)","e43c4b49":"<a id=\"section-six\"><\/a>\n### Setting up the Layers","bb1d32e5":"<a id=\"section-seven\"><\/a>\n### Compiling the Model \n* Optimizer used during Back Propagation for weight and bias adjustment - Adam (adjusts the learning rate adaptively).\n* Loss Function used - Categorical Crossentropy (used when multiple categories\/classes are present).\n* Metrics used for evaluation - Accuracy."}}