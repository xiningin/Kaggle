{"cell_type":{"d4bcf64b":"code","1e6b8a5f":"code","ce702352":"code","b1f959ae":"code","080b41d9":"code","2aeced2e":"code","1ce6536c":"code","778cd2f5":"code","9f9254a8":"code","7b43d8c9":"code","a508dfa3":"code","0d755d16":"code","0cfd31b7":"code","1fbc86e7":"code","34c8855e":"code","065ead6d":"code","63f3c1bf":"code","f676b219":"code","bfa9a2d1":"code","fbac1bef":"code","d4007f08":"code","1dceb535":"code","22c20a67":"markdown","560dea30":"markdown","9313ee60":"markdown","0a9dcb6b":"markdown","045145b8":"markdown"},"source":{"d4bcf64b":"%matplotlib inline\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom glob import glob\nimport gc\n\nimport random\n\nfrom sklearn.model_selection import KFold, train_test_split, StratifiedKFold\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display\n\nfrom tqdm import tqdm, tqdm_notebook\ntqdm.pandas()\n\nplt.rcParams[\"figure.figsize\"] = (12, 8)\nplt.rcParams['axes.titlesize'] = 16\nplt.style.use('seaborn-whitegrid')\nsns.set_palette('Set2')\n\nimport tensorflow as tf\nfrom tensorflow.keras import Model, Input\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import Adam\n\nimport tensorflow.keras.backend as K\n\nimport os\nprint(os.listdir('..\/input\/'))\n\nimport warnings\nwarnings.simplefilter('ignore')\n\nfrom time import time, strftime, gmtime\nstart = time()\nimport datetime\nprint(str(datetime.datetime.now()))","1e6b8a5f":"def seed_everything(seed = 0):\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n\nseed = 777\nseed_everything(seed)","ce702352":"base_dir = '..\/input\/commonlitreadabilityprize\/'","b1f959ae":"train = pd.read_csv(base_dir + 'train.csv')\nprint(train.shape)\ntrain.head()","080b41d9":"test = pd.read_csv(base_dir + 'test.csv')\nprint(test.shape)\ntest.head()","2aeced2e":"sub = pd.read_csv(base_dir + 'sample_submission.csv')\nprint(sub.shape)\nsub.head()","1ce6536c":"print(f\"Number of unique id in trainset: {train['id'].nunique()}\")\nprint(f\"Number of unique id in testset: {test['id'].nunique()}\")","778cd2f5":"sns.kdeplot(train['target'], shade = True, color = 'green')\nplt.axvline(train['target'].mean(), label = 'Mean', color = 'r', linewidth = 1, linestyle = '--')\nplt.axvline(train['target'].median(), label = 'Median', color = 'b', linewidth = 1, linestyle = '--')\nplt.legend();","9f9254a8":"sns.kdeplot(train['standard_error'], shade = True, color = 'grey')\nplt.axvline(train['standard_error'].mean(), label = 'Mean', color = 'r', linewidth = 1, linestyle = '--')\nplt.axvline(train['standard_error'].median(), label = 'Median', color = 'b', linewidth = 1, linestyle = '--')\nplt.legend();","7b43d8c9":"train['excerpt_len'] = train['excerpt'].apply(lambda x: len(str(x)))\ntrain['excerpt_wordlen'] = train['excerpt'].apply(lambda x: len(str(x).split(' ')))\n\ntest['excerpt_len'] = test['excerpt'].apply(lambda x: len(str(x)))\ntest['excerpt_wordlen'] = test['excerpt'].apply(lambda x: len(str(x).split(' ')))","a508dfa3":"print(f\"Max. word length in train - Excerpt: {train['excerpt_wordlen'].max()}\")\nprint(f\"Min. word length in train - Excerpt: {train['excerpt_wordlen'].min()}\")\nprint()\nprint(f\"Max. word length in train - Excerpt: {test['excerpt_wordlen'].max()}\")\nprint(f\"Min. word length in train - Excerpt: {test['excerpt_wordlen'].min()}\")","0d755d16":"plt.subplot(1, 2, 1)\nsns.distplot(train['excerpt_len'], bins = 50)\nplt.title('Train Character Length')\n\nplt.subplot(1, 2, 2)\nsns.distplot(train['excerpt_wordlen'], bins = 50)\nplt.title('Train Word Length');","0cfd31b7":"plt.subplot(1, 2, 1)\nsns.distplot(test['excerpt_len'], bins = 50)\nplt.title('Test Character Length')\n\nplt.subplot(1, 2, 2)\nsns.distplot(test['excerpt_wordlen'], bins = 50)\nplt.title('Test Word Length');","1fbc86e7":"train['folds'] = -1\n\ntrain['bins'] = pd.cut(train['target'], bins = 6, labels = False)\n\nskf = StratifiedKFold(n_splits = 5)\n\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(X = train['excerpt'], y = train['bins'].values)):\n    train.loc[val_idx, 'folds'] = fold\n        \ntrain.drop('bins', axis = 1, inplace = True)\ntrain.head()","34c8855e":"import tokenizers\nfrom transformers import RobertaConfig, TFRobertaModel\nfrom transformers import RobertaTokenizer, TFRobertaForSequenceClassification\n\nmax_len = 256\n\nroberta_path = '..\/input\/tf-roberta\/'","065ead6d":"tok = RobertaTokenizer.from_pretrained('..\/input\/roberta-base')\ntok.vocab_size","63f3c1bf":"def roberta_encode(texts, tokenizer, max_len = max_len):\n    all_tokens = np.ones((len(texts), max_len), dtype = 'int32')\n    all_masks = np.zeros((len(texts), max_len), dtype = 'int32')\n    \n    for k, text in enumerate(texts):\n        encoded = tok.encode_plus(\n            text,                \n            add_special_tokens = True,\n            max_length = max_len,     \n            pad_to_max_length = True,\n            return_attention_mask = True,\n       )\n        #print(encoded['input_ids'])\n        #print(encoded['attention_mask'])\n        #For one sentence as input:\n        # <s> ...word tokens... <\/s\n        \n        # bos_token_id <s>: 0\n        # eos_token_id <\/s>: 2\n        # sep_token_id <\/s>: 2\n        # pad_token_id <pad>: 1\n        \n        #Roberta does not use token_type_ids like BERT does.\n        #So there's no need to create token_type_ids.\n        \n        all_tokens[k, :max_len] = encoded['input_ids']\n        all_masks[k, :max_len] = encoded['attention_mask']\n    return all_tokens, all_masks","f676b219":"def build_roberta(max_len = max_len):\n    input_word_ids = Input(shape = (max_len,), dtype = tf.int32, name = \"input_word_ids\")\n    input_mask = Input(shape = (max_len,), dtype = tf.int32, name = \"input_mask\")\n    \n    config = RobertaConfig.from_pretrained(roberta_path + 'config-roberta-base.json')\n    \n    roberta_model = TFRobertaModel.from_pretrained(roberta_path + 'pretrained-roberta-base.h5', \n                                                                       config = config)\n    \n    x = roberta_model([input_word_ids, input_mask])[0]\n    \n    x = tf.keras.layers.Dropout(0.2)(x)\n    \n    out = tf.keras.layers.Dense(1, activation = 'linear')(x)\n    \n    model = Model(inputs = [input_word_ids, input_mask], outputs = out)\n    \n    model.compile(Adam(lr = 1e-5), loss = tf.keras.losses.MeanSquaredError(), \n                  metrics = tf.keras.metrics.RootMeanSquaredError())\n    \n    return model","bfa9a2d1":"Xtrain = roberta_encode(train['excerpt'].values, tok, max_len = max_len)\nytrain = train['target'].values\n\nprint(Xtrain[0].shape, ytrain.shape)\n\nK.clear_session()\nmodel = build_roberta(max_len = max_len)\n\n#model.trainable = True\n\ncheck = ModelCheckpoint(f'roberta_model.h5', monitor = 'val_loss', verbose = 1, save_best_only = True,\n    save_weights_only = True, mode = 'auto', save_freq = 'epoch')\n\nhistory = model.fit(Xtrain, ytrain, epochs = 4, batch_size = 8, \n          verbose = 1, callbacks = [check], \n          validation_split = 0.2\n         )\n\nprint('Loading model...')\nmodel.load_weights(f'roberta_model.h5')\n\nXtest = roberta_encode(test['excerpt'].values, tok, max_len = max_len)\nprint(Xtest[0].shape)\n\nprint('Predicting Test...')\npreds = model.predict(Xtest, verbose = 1)\n","fbac1bef":"sub['target'] = np.mean(preds, axis = 1)\nsub.to_csv('.\/submission.csv', index = False)\nsub","d4007f08":"plt.subplot(1, 2, 1)\nsns.kdeplot(train['target'], shade = True, color = 'green')\nplt.axvline(train['target'].mean(), label = 'Mean', color = 'r', linewidth = 1, linestyle = '--')\nplt.axvline(train['target'].median(), label = 'Median', color = 'b', linewidth = 1, linestyle = '--')\nplt.legend()\nplt.title('Train Target')\n\nplt.subplot(1, 2, 2)\nsns.kdeplot(sub['target'], shade = True, color = 'blue')\nplt.axvline(sub['target'].mean(), label = 'Mean', color = 'r', linewidth = 1, linestyle = '--')\nplt.axvline(sub['target'].median(), label = 'Median', color = 'b', linewidth = 1, linestyle = '--')\nplt.legend()\nplt.title('Predicted Target');","1dceb535":"finish = time()\nprint(strftime(\"%H:%M:%S\", gmtime(finish - start)))","22c20a67":"# RoBERTa","560dea30":"<code>__Distribution of Text Lengths__<\/code>","9313ee60":"<code>__Distribution of Standard Error__<\/code>\n\n- Not provided for testset","0a9dcb6b":"- The max word length is useful to determine the tokenizer's max_len ","045145b8":"<code>__Distribution of Target__<\/code>"}}