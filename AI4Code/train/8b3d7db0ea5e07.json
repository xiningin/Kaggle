{"cell_type":{"b6eb9707":"code","abebd965":"code","18fac4e6":"code","66b3d855":"code","2537e95d":"code","4c347f95":"code","b31fd4a2":"code","4d2586ee":"code","9287fc99":"code","fe35be53":"code","0d0b3114":"code","99a45373":"code","237ff402":"code","7a6abd4f":"code","a520a84e":"code","45775006":"code","66183b40":"code","b9ea1f5f":"code","10dc663c":"code","89f13573":"code","da710808":"code","3356c5de":"code","c0d7bfb2":"code","f7d757bf":"code","581e9b99":"code","af1efa96":"code","66e0a0ac":"code","3a702cab":"code","ec5b956f":"code","c9c523a7":"code","f1da7259":"code","3a2569c9":"code","07a3873c":"code","8d293ce6":"code","7ac593e2":"code","960c2a83":"code","8eceb74a":"code","8157b2dc":"code","b7f16a9d":"code","800b7c27":"code","75853827":"code","6cb7fe7e":"code","5f59abcf":"code","23e59a9f":"code","8f10dedb":"code","3964b76e":"code","96f25e90":"code","429fa530":"code","0656ccad":"code","e24b7c2a":"code","c4e60081":"code","cea7dc4e":"code","279f7fd5":"code","5764e893":"code","1724478e":"code","33285adb":"code","0fbcc33c":"code","50feebaa":"code","c131b9f2":"code","5b0b7076":"code","e626f539":"code","ddfee320":"code","b8daa5a0":"code","8cd97dff":"code","7fe0c13c":"code","cb1c3a6f":"code","797fcb6c":"code","1af05285":"code","e1e15959":"code","5dc283e7":"code","0092451a":"code","28d07184":"code","71d78992":"code","f312608f":"code","70324396":"code","ef9d13b6":"code","aa54a0d0":"code","b93dedb6":"code","1fa6ebd5":"code","43398ed3":"code","fec3d639":"code","57e6e97b":"code","83542461":"markdown","9be47ed7":"markdown","6909e1c8":"markdown","2b197cb0":"markdown","7a730adb":"markdown","0bc7ba1e":"markdown","33cd396d":"markdown","0e6e987b":"markdown","e80b60a6":"markdown","d0fe44bc":"markdown","6754c359":"markdown","a6be25b7":"markdown","305c2c7b":"markdown","51b4129f":"markdown","4ca47cce":"markdown","f5b8ab00":"markdown","133f090d":"markdown","08bde59c":"markdown","5a8411ad":"markdown","8c38a91e":"markdown","bede38e3":"markdown","361607bd":"markdown","d7c6194c":"markdown","220778be":"markdown","d84e8050":"markdown","dde15a89":"markdown","6401d697":"markdown","6afbfd19":"markdown","e16718d4":"markdown","693698db":"markdown","e413b0c4":"markdown","9d4786d4":"markdown","4f4c826c":"markdown","fe339b9f":"markdown","3428b42b":"markdown","dad34ec8":"markdown","1dd08a09":"markdown","f4df2857":"markdown","94d80c2e":"markdown","f1be02f3":"markdown","f2d20708":"markdown"},"source":{"b6eb9707":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom scipy.stats import kurtosis, skew\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom scipy.stats import mstats\nimport operator\n\nimport missingno\nfrom sklearn.preprocessing import FunctionTransformer\npd.set_option('display.max_columns', 90)","abebd965":"data=pd.read_csv(\"..\/input\/home-data-for-ml-course\/train.csv\")","18fac4e6":"data.head()","66b3d855":"print(\"Number of observations {}\".format(data.shape[0]))\nprint(\"Number of features {} -> 79 unique features + Id + SalePrice(target)\".format(data.shape[1]))","2537e95d":"non_obj_col=[]# list to store cloums which are not of object(Nominal\/Ordinal) type.\nobj_col=[]# list to store cloumns of object type.\n\nfor col in data.columns:\n    if(data[col].dtype != 'object'):\n        non_obj_col.append(col)\n        \nfor col in data.columns:\n    if(data[col].dtype == 'object'):\n        obj_col.append(col)\n        \n\n","4c347f95":"data['SalePrice'].hist();","b31fd4a2":"print( 'excess kurtosis of distribution (should be 0 for normal distribution): {}'.format( kurtosis(data['SalePrice']) ))\n\nprint( 'skewness of distribution (should be 0 for normal distribution): {}'.format( skew(data['SalePrice']) ))","4d2586ee":"crim_sqrt=np.log(data['SalePrice'])\ncrim_sqrt.hist();","9287fc99":"plt.figure(figsize=(15,8))\n\ng=sns.boxplot(x='Neighborhood',y='SalePrice',data=data)\ng.set_title(\"Sales Price around different household\")\ng.set_xticklabels(g.get_xticklabels(), rotation=30);","fe35be53":"plt.figure(figsize=(15,8))\nsns.boxplot(x='OverallCond',y='SalePrice',data=data);","0d0b3114":"plt.figure(figsize=(15,8))\nsns.regplot(x='GrLivArea',y='SalePrice',data=data);","99a45373":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\ncorr_matrix = data.corr()#try different color pallets\nplt.figure(figsize=(20, 10))\n\nax=sns.heatmap(corr_matrix, \n    vmin=-1, vmax= .8, center=0,\n    square=True)\n\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=90,\n    horizontalalignment='right'\n);\n\n","237ff402":"data['TotRmsAbvGrd'].corr(data['GrLivArea'])# very high corelation 0.8254893743088431","7a6abd4f":"data['GarageYrBlt'].corr(data['YearBuilt'])# very high corelation 0.825667484174342","a520a84e":"data['TotalBsmtSF'].corr(data['1stFlrSF'])","45775006":"data['GarageCars'].corr(data['GarageArea'])","66183b40":"corr_saleprice=corr_matrix[\"SalePrice\"].sort_values(ascending=False)[:10]\ncorr_saleprice","b9ea1f5f":"c=corr_matrix[\"SalePrice\"].sort_values(ascending=False)[:10].index\n\nplt.figure(figsize=(20, 10))\n\nsns.heatmap(data[c].corr(),\n           vmin=-1, vmax= .8, center=0,\n            square=True, annot=True);","10dc663c":"import matplotlib.pyplot as plt\nfig, axes = plt.subplots(nrows = 2, ncols = 2,figsize=(15,8))\n\n\ndata.plot(kind='scatter', x='SalePrice', y='OverallQual',ax=axes[0,0])\ndata.plot(kind='scatter', x='GarageArea', y='GarageCars', ax=axes[0,1])\ndata.plot(kind='scatter', x='TotalBsmtSF', y='1stFlrSF', ax=axes[1,0])\ndata.plot(kind='scatter', x='GrLivArea', y='TotRmsAbvGrd', ax=axes[1,1]);","89f13573":"print(\"Number of categorical variable in data = {} \".format(len(obj_col)))","da710808":"var=['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\n\n\ndef kruskawallis_test(column):\n    k_test=data.loc[:,[column,'SalePrice']]\n    x=pd.pivot_table(k_test,index=k_test.index, values='SalePrice',columns=column)\n    \n    for i in range(x.shape[1]):\n        var[i]=x.iloc[:,i]\n        var[i]=var[i][~var[i].isnull()].tolist()\n        \n#     s=\"\"\n#     for i in range(x.shape[1]):\n#         s=s+'var{}'.format(i)+','\n\n    m=()\n    for i in range(x.shape[1]):\n        m=m+(var[i],)\n    \n    \n    args=(m)\n    \n    \n    H, pval = mstats.kruskalwallis(*args)\n\n    return pval\n\n\neffective_col={}\n\nfor col in obj_col:\n    pval=kruskawallis_test(col)\n    if(pval<0.05):\n        effective_col[col]=pval","3356c5de":"sorted_x = sorted(effective_col.items(), key=operator.itemgetter(1))\n\neff_cat_col=[]\n\nfor i in range(len(sorted_x)):\n    key,value = sorted_x[i]\n    eff_cat_col.append(key)","c0d7bfb2":"fig, axes = plt.subplots(nrows = 3, ncols = 3,figsize=(25,20))\n\nsns.boxplot(x='SalePrice', y=eff_cat_col[0], data=data,ax=axes[0,0])\nsns.boxplot(x='SalePrice', y=eff_cat_col[1], data=data,ax=axes[0,1])\nsns.boxplot(x='SalePrice', y=eff_cat_col[2], data=data,ax=axes[0,2])\nsns.boxplot(x='SalePrice', y=eff_cat_col[3], data=data,ax=axes[1,0])\nsns.boxplot(x='SalePrice', y=eff_cat_col[4], data=data,ax=axes[1,1])\nsns.boxplot(x='SalePrice', y=eff_cat_col[5], data=data,ax=axes[1,2])\nsns.boxplot(x='SalePrice', y=eff_cat_col[6], data=data,ax=axes[2,0])\nsns.boxplot(x='SalePrice', y=eff_cat_col[7], data=data,ax=axes[2,1])\nsns.boxplot(x='SalePrice', y=eff_cat_col[8], data=data,ax=axes[2,2]);","f7d757bf":"from sklearn.model_selection import train_test_split\n\ndata, some_data = train_test_split(data, test_size = 0.2, random_state = 42)","581e9b99":"data.shape","af1efa96":"some_data.shape","66e0a0ac":"data=pd.read_csv(\"..\/input\/home-data-for-ml-course\/train.csv\")\ndata, some_data = train_test_split(data, test_size = 0.2, random_state = 42)","3a702cab":"MSSubClass_dict={20 : '1-STORY 1946 & NEWER ALL STYLES',\n                30 : '1-STORY 1945 & OLDER',\n                40 : '1-STORY 1945 & OLDER',\n                45 : '1-1\/2 STORY - UNFINISHED ALL AGES',\n                50 : '1-1\/2 STORY FINISHED ALL AGES',\n                60 : '2-STORY 1946 & NEWER',\n                70 : '2-STORY 1945 & OLDER',\n                75 : '2-1\/2 STORY ALL AGES',\n                80 : 'SPLIT OR MULTI-LEVEL',\n                85 : 'SPLIT FOYER',\n                90 : 'DUPLEX - ALL STYLES AND AGES',\n                120 : '1-STORY PUD (Planned Unit Development) - 1946 & NEWER',\n                150 : '1-1\/2 STORY PUD - ALL AGES',\n                160 : '2-STORY PUD - 1946 & NEWER',\n                180 : 'PUD - MULTILEVEL - INCL SPLIT LEV\/FOYER',\n                190 : '2 FAMILY CONVERSION - ALL STYLES AND AGES'}\n\n\nExterQual_dict = {'Po': 1,\n                 'Fa' : 2,\n                 'TA' : 3,\n                 'Gd' : 4,\n                 'Ex' : 5,\n                 'Not Available' : 0}\n\nBsmtExposure_dict = {'Gd' : 4 , \n                    'Av' : 3 ,\n                    'Mn' : 2 ,\n                    'No' : 1,\n                    'Not Available' :0}\n\nBsmtFinType1_dict = {'Unf' : 1,\n                    'LwQ' : 2,\n                    'Rec' : 3,\n                    'BLQ' : 4,\n                    'ALQ' : 5,\n                    'GLQ' : 6,\n                    'Not Available' : 0}\n\nbinary_dict = {'Yes' : 1 , 'No' : 0}\n\n\ndef datatype_corrector(col, dict):\n    transformed_col = col.replace(dict)\n    return transformed_col\n\ndef CombinedDataCorrector(data):\n    data['MSSubClass'] = datatype_corrector(data.loc[:, ['MSSubClass']], MSSubClass_dict)\n            \n    data['ExterQual'] = datatype_corrector(data.loc[:, ['ExterQual']], ExterQual_dict)\n    data['ExterCond'] = datatype_corrector(data.loc[:, ['ExterCond']], ExterQual_dict)\n    data['BsmtCond'] = datatype_corrector(data.loc[:, ['BsmtCond']], ExterQual_dict)\n    data['BsmtQual'] = datatype_corrector(data.loc[:, ['BsmtQual']], ExterQual_dict)\n            \n    data['BsmtExposure'] = datatype_corrector(data.loc[:, ['BsmtExposure']], BsmtExposure_dict)\n            \n    data['BsmtFinType1'] = datatype_corrector(data.loc[:, ['BsmtFinType1']], BsmtFinType1_dict)\n    data['BsmtFinType2'] = datatype_corrector(data.loc[:, ['BsmtFinType2']], BsmtFinType1_dict)\n            \n    data['HeatingQC'] = datatype_corrector(data.loc[:, ['HeatingQC']], ExterQual_dict)\n    data['GarageCond'] = datatype_corrector(data.loc[:, ['GarageCond']], ExterQual_dict)\n    data['PoolQC'] = datatype_corrector(data.loc[:, ['PoolQC']], ExterQual_dict)\n    data['GarageQual'] = datatype_corrector(data.loc[:, ['GarageQual']], ExterQual_dict)\n                                                    \n    data['CentralAir'] = datatype_corrector(data.loc[:, ['CentralAir']], binary_dict)\n    return data\n","ec5b956f":"# plt.figure(figsize=(15,8))\n# sns.scatterplot(x='age_house',y='SalePrice',hue = 'OverallQual',palette =sns.color_palette(\"hls\", 10), data=data).set_title(\"Age of House and Overall Quality against Sale Price\");","c9c523a7":"def age_creator(yr_built, yr_sold):\n    return yr_sold-yr_built\n\ndef remodel_checker(year,remod_year):\n    if(abs(remod_year-year)==0):\n        return 'no'\n    elif((abs(remod_year-year)>0) & (abs(remod_year-year)<2) ):\n        return '1 year'\n    elif((abs(remod_year-year)>=2) & (abs(remod_year-year)<20) ):\n        return '2 - 20 year'\n    elif((abs(remod_year-year)>=20) & (abs(remod_year-year)<40)):\n        return '20 - 40 years'\n    elif((abs(remod_year-year)>=40) & (abs(remod_year-year)<70)):\n        return '40 - 70 years'\n    elif(abs(remod_year-year)>=70):\n        return '70+'\n\n\ndef Attribute_Engineered(data):\n    #lets make some simplified score quality*condition\n        data['ExterQualScore'] = data['ExterQual']*data['ExterCond']\n\n        data['BsmtQualScore'] = data['BsmtCond']*data['BsmtQual']\n\n        data['GarageQualScore'] = data['GarageCond']*data['GarageQual']\n\n        data['OverallQualScore'] = data['OverallCond']*data['OverallQual']\n        \n        data['age_house'] = age_creator(data['YearBuilt'],data['YrSold']) #will remove year_built high correlation with age\n        \n        data['remod_flag']=data.apply(lambda x : remodel_checker(x['YearBuilt'],x['YearRemodAdd']), axis=1)\n        \n        coo_saleprice = corr_saleprice.index\n        coo_saleprice = coo_saleprice.tolist()\n        coo_saleprice = coo_saleprice[1:]\n\n        for col in coo_saleprice:\n            col_name = col + 'sq'\n            data[col_name]=data[col]**2\n        \n        return data","f1da7259":"missingno.matrix(data,figsize = (30,10),labels=True,fontsize=10);","3a2569c9":"nulls = data.isnull().sum()\nnulls = nulls[nulls.values>0]\npd.DataFrame({'col' : nulls.index, 'values' : (nulls.values\/data.shape[0])*100})","07a3873c":"data['Electrical'].value_counts()","8d293ce6":"print(\"To decide for lot frontage(Quantitative variable) if NA value present is < 5% of the data i will remove the observations having NA  values {}\".format(data['LotFrontage'].isnull().sum()\/data.shape[1]))","7ac593e2":"def treat_missing_val(data):\n    cols_to_treat = ['PoolQC', 'MiscFeature', 'Fence', 'FireplaceQu',\n                    'BsmtCond', 'BsmtQual', 'BsmtExposure', 'BsmtFinType1',\n                    'BsmtFinType2', 'Alley', 'GarageType', 'GarageFinish',\n                    'GarageQual', 'GarageCond']\n    for col in cols_to_treat:\n        data[col] = data[col].fillna('Not Available')\n        \n    return data\n\n# please review 'MiscFeature'\n#data['RoofStyle'].value_counts()\n#, 'MSSubClass'\n# feature removal random forest feature importance\n# 'RoofMatl', 'Condition2', 'Utilities', 'Exterior1st', 'Exterior2nd', 'SaleType'\n        \ndef feature_cleaner(data):\n#     data = data.dropna()\n        \n    remove_col=['TotRmsAbvGrd','GarageYrBlt','1stFlrSF','GarageArea', 'YearBuilt', 'MiscFeature', 'Fence',\n               'PoolQC', 'Alley', 'RoofStyle', 'MSSubClass']\n    data = data.drop(remove_col, axis=1)\n\n#     data = data.drop('Id', axis=1)\n    return data\n\ndef drop_excess(data):\n    data = data.drop(['Id', 'index'], axis=1)\n    return data\n\ndef reset_index(data):\n    data = data.reset_index(inplace = False)\n    return data\n\n\nmissing_val = FunctionTransformer(treat_missing_val, validate = False)\nfeature_clean = FunctionTransformer(feature_cleaner, validate = False)\nreset = FunctionTransformer(reset_index, validate = False)\ncorrect_data = FunctionTransformer(CombinedDataCorrector, validate = False)\nengineered_features = FunctionTransformer(Attribute_Engineered, validate = False)","960c2a83":"drop_rest = FunctionTransformer(drop_excess, validate = False)","8eceb74a":"\ndef impute_num_values(data):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    num_col = data.select_dtypes(include=numerics).columns.to_list()\n    \n    for col in num_col:\n        data[col] = data[col].fillna(data[col].mode()[0])\n#     data['LotFrontage'] = data['LotFrontage'].fillna(data['LotFrontage'].median())\n#     data['MasVnrArea'] = data['MasVnrArea'].fillna(data['MasVnrArea'].median())\n    return data\n    \ndef impute_cat_values(data):\n    cat_col = data.select_dtypes(include='object').columns.to_list()\n    \n    for col in cat_col:\n        data[col] = data[col].fillna(data[col].mode()[0])\n                                                            \n#     data['Electrical'] = data['Electrical'].fillna(data['Electrical'].mode()[0])\n#     data['MasVnrType'] = data['MasVnrType'].fillna(data['MasVnrType'].mode()[0])\n    return data\n\n    \nimpute_num = FunctionTransformer(impute_num_values, validate = False)\nimpute_cat = FunctionTransformer(impute_cat_values, validate = False)","8157b2dc":"data.shape","b7f16a9d":"from sklearn.pipeline import Pipeline\n\ndata_prep_pipeline = Pipeline(\n    steps=[\n        ('2', missing_val),\n        ('num_imp', impute_num),\n        ('cat_imp', impute_cat),\n        ('one', correct_data),\n        ('two', engineered_features),\n        ('1', feature_clean),\n        ('3', reset),\n#         ('clean', drop_rest)\n    ], verbose = False)\n\ndata = data_prep_pipeline.fit_transform(data)","800b7c27":"data.shape","75853827":"data_labels = data['SalePrice'].copy()\ndata = data.drop('SalePrice', axis = 1)","6cb7fe7e":"cat_col = data.select_dtypes(include='object').columns.to_list()\n# cat_col","5f59abcf":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n\nnum_col = data.select_dtypes(include=numerics).columns.to_list()\n","23e59a9f":"len(cat_col)\n# cat_col","8f10dedb":"len(num_col)\n# num_col","3964b76e":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom sklearn.impute import SimpleImputer\n\ndata_col = ['Id', 'index']\n# num_col = []\n# cat_col = []\n\n\nfrom sklearn.compose import ColumnTransformer\n\nnum_pipeline = Pipeline(\n    steps = [\n#         ('imp', SimpleImputer(strategy = 'median')),\n        ('scaler', MinMaxScaler())\n    ]\n)\n\ncat_pipeline = Pipeline(\n    steps = [\n#         ('imp', SimpleImputer(strategy = 'most_frequent')),\n        ('encoder', OneHotEncoder())\n    ]\n)\n\nmy_pipe = Pipeline(\n    steps = [\n        ('clean', drop_rest)\n    ]\n)\n\nfull_pipeline = ColumnTransformer(\n    [('first', my_pipe, data_col),\n     ('second', num_pipeline, num_col),\n     ('third', cat_pipeline, cat_col)]\n)","96f25e90":"data_prepared = full_pipeline.fit_transform(data)\ndata_prepared","429fa530":"from sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(data_prepared, data_labels)","0656ccad":"# test['MiscFeature'].value_counts()","e24b7c2a":"some_label = some_data.loc[:, ['SalePrice']]\n\nsome_data = some_data.drop(columns = ['SalePrice'], axis = 1)\n\ns_data = some_data.iloc[:5]\nsome_data_1 = data_prep_pipeline.transform(s_data)\nsome_data_2 = full_pipeline.transform(some_data_1)\n# lin_reg.predict(some_data_2)","c4e60081":"lin_reg.predict(some_data_2)","cea7dc4e":"some_label.iloc[:5]","279f7fd5":"data_prepared.shape","5764e893":"from sklearn.model_selection import GridSearchCV\n\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import cross_val_score","1724478e":"\n\n\ndef find_best_model_gridsearchCV(X,y):\n    algos = { \n        'linear_regression' : {\n            'model' : LinearRegression(),\n            'parmas' : {\n                'normalize' : [True, False]\n            }\n        },\n        'lasso' : {\n            'model' : Lasso(),\n            'parmas' : {\n                'alpha' : [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03],\n                'selection' : ['random', 'cyclic']\n            }\n        },\n        'Ridge' : {\n            'model' : Ridge(),\n            'parmas' : {\n                'alpha' : [0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6, 10, 30, 60],\n                'solver' : ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']\n            }\n        },\n        'Decision_Tree' : {\n            'model' : DecisionTreeRegressor(random_state=0),\n            'parmas' : {\n                'splitter' : ['best', 'random'],\n                'max_depth' : [2, 3, 5, 7, 9, 10, 20, 50]\n            }\n        },\n        'Random_Forest' : {\n            'model' : RandomForestRegressor(random_state=0),\n            'parmas' : {\n                'max_depth' : [2, 3, 5, 7, 9, 10, 20, 50]\n            }\n        }\n        }\n    scores = []\n    cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n    for algo_names, config in algos.items():\n        gs = GridSearchCV(config['model'], config['parmas'], cv=cv, return_train_score=False)\n        gs.fit(X,y)\n        scores.append({\n            'model' : algo_names,\n            'best_score' : gs.best_score_,\n            'best_parma' : gs.best_params_\n        })\n        \n    return pd.DataFrame(scores, columns = ['model', 'best_score', 'best_parma'])\n    ","33285adb":"find_best_model_gridsearchCV(data_prepared, np.log(data_labels))","0fbcc33c":"from sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(LinearRegression(), data_prepared, data_labels, scoring = \"neg_mean_squared_error\", cv = 10)\n\nlr_rms_score = np.sqrt(-scores)\n\ndef display_scores(scores):\n    print(\"scores : \", scores)\n    print(\"Mean : \", scores.mean())\n    print(\"sd : \", scores.std())\n    \ndisplay_scores(lr_rms_score)","50feebaa":"from sklearn.metrics import mean_squared_error\n\nlr = LinearRegression()\nlr.fit(data_prepared, data_labels)\nlr_pred = lr.predict(data_prepared)\nlr_mse = mean_squared_error(data_labels ,lr_pred)\nlr_rmse = np.sqrt(lr_mse)\nlr_rmse","c131b9f2":"from sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\n\nscores = cross_val_score(Lasso(alpha = 0.0006, selection = 'random'), data_prepared, data_labels, scoring = \"neg_mean_squared_error\", cv = 10)\n\nlr_lasso_score = np.sqrt(-scores)\n\ndisplay_scores(lr_lasso_score)","5b0b7076":"lasso = Lasso(alpha = 0.0006, selection = 'random')\nlasso.fit(data_prepared, data_labels)\nlasso_pred = lasso.predict(data_prepared)\nlasso_mse = mean_squared_error(data_labels ,lasso_pred)\nlasso_rmse = np.sqrt(lasso_mse)\nlasso_rmse","e626f539":"scores = cross_val_score(Ridge(alpha = 10, solver = 'saga'), data_prepared, data_labels, scoring = \"neg_mean_squared_error\", cv = 10)\n\nlr_ridge_score = np.sqrt(-scores)\n\ndisplay_scores(lr_ridge_score)","ddfee320":"ridge = Ridge(alpha = 10, solver = 'saga')\nridge.fit(data_prepared, data_labels)\nridge_pred = ridge.predict(data_prepared)\nridge_mse = mean_squared_error(data_labels ,ridge_pred)\nridge_rmse = np.sqrt(ridge_mse)\nridge_rmse","b8daa5a0":"scores = cross_val_score(DecisionTreeRegressor(random_state=0, max_depth = 7, splitter = 'random'), data_prepared, np.log(data_labels), scoring = \"neg_mean_squared_error\", cv = 10)\n\ndt_score = np.sqrt(-scores)\n\ndisplay_scores(dt_score)","8cd97dff":"dt = DecisionTreeRegressor(random_state=0, max_depth = 7, splitter = 'random')\ndt.fit(data_prepared, np.log(data_labels))\ndt_pred = dt.predict(data_prepared)\ndt_mse = mean_squared_error(data_labels ,np.exp(dt_pred))\ndt_rmse = np.sqrt(dt_mse)\ndt_rmse","7fe0c13c":"scores = cross_val_score(RandomForestRegressor(random_state=0, max_depth = 50), data_prepared, data_labels, scoring = \"neg_mean_squared_error\", cv = 10)\n\nrnad_f_score = np.sqrt(-scores)\n\ndisplay_scores(rnad_f_score)","cb1c3a6f":"rf = RandomForestRegressor(random_state=0, max_depth = 50)\nrf.fit(data_prepared, data_labels)\nrf_pred = rf.predict(data_prepared)\nrf_mse = mean_squared_error(data_labels ,rf_pred)\nrf_rmse = np.sqrt(rf_mse)\nrf_rmse","797fcb6c":"# rf.feature_importances_","1af05285":"test=pd.read_csv(\"..\/input\/home-data-for-ml-course\/test.csv\")","e1e15959":"data_prepared.shape","5dc283e7":"test.shape","0092451a":"ridge = Ridge(alpha = 10, solver = 'saga')\nridge.fit(data_prepared, np.log(data_labels))\n\n# DecisionTreeRegressor(random_state=0, max_depth = 7, splitter = 'random')\n# ridge.fit(data_prepared, np.log(data_labels))","28d07184":"test1 = data_prep_pipeline.transform(test)\n# test1.shape","71d78992":"ids = test1.loc[:, 'Id']","f312608f":"test2 = full_pipeline.transform(test1)\nfinal_pred = ridge.predict(test2)","70324396":"result = pd.DataFrame({'Id' : ids, 'SalePrice' : np.exp(final_pred)})\n\nresult.to_csv(\"submission6.csv\", index=False)\n\n#Your submission scored 17249.40585, which is an improvement of your previous score of 18468.08879. Great job!\n#31700 approx --> 31000","ef9d13b6":"data.columns","aa54a0d0":"cat_col = data.select_dtypes(include='object').columns.to_list()\nnum_col = data.select_dtypes(include=numerics).columns.to_list()","b93dedb6":"data_1 = pd.get_dummies(data, columns = cat_col)","1fa6ebd5":"from sklearn.metrics import mean_squared_error\n\nrf = RandomForestRegressor(random_state=0, max_depth = 50)\nrf.fit(data_1, np.log(data_labels))\nrf_pred = rf.predict(data_1)\nrf_mse = mean_squared_error(data_labels ,np.exp(rf_pred))\nrf_rmse = np.sqrt(rf_mse)\nrf_rmse\n#11444.09555191875","43398ed3":"mm = pd.DataFrame({'col' : data_1.columns, 'score' : rf.feature_importances_})\nmm = mm.sort_values(by = ['score'], ascending = False)\nmm = mm.reset_index()\nmm.iloc[-20:]","fec3d639":"# feature removal random forest feature importance\n# 'RoofMatl', 'Condition2', 'Utilities', 'Exterior1st', 'Exterior2nd', 'SaleType'","57e6e97b":"######################### END ##################### ROUGH AFTER THIS","83542461":" Looking at `SalePrice` distribution. It's not hard to notice that the distribution is positively skewed. Transforming your target to normal disribution linearizes the target. Which is useful for many models which expect linear targets. This is tackled in the coming section for EDA, target variable is not transformed.","9be47ed7":"#### EXPLANATION\n\nLooking at the graph we can say that the assumption made earlier that `Neighbourhood` is one important factor effecting `SalePrice` holds true. As some neighbourhoods have higher sale price than others. ","6909e1c8":"Indeed there is strong relationship among them.\n\nAn interesting relationship can be observed between 1stFlrSF and TotalBsmtSF in this graph, a strong linear relationship can be observed between variables with the datapoints making a boundary which makes sense as a house with x square feet of basement area will have more or at most equal First Floor area in square feet.\n","2b197cb0":"Just for demonstratio purpose the positive skewness in the data can be mitigated by using a `log(x)` transform. It's effect can be seen on the target variable. After applying log transform the data is more close to normal which will later help in improving performance of the model.","7a730adb":"# EXPLORATORY DATA ANALYSIS\n\nFundamentally variables in the dataset can be subdivided into:\n1. Qualitative \n2. Quantitative\n\nThese variables can't be analysed using same set of tools. Thefore, by making a group of each type we will try to give a basic structure to this analysis.\n\n","0bc7ba1e":"To get a grip over data and kick off the analysis let's see the relation of `SalePrice` with some variables that are prominent influencing factors while buying a house:\n\n1. Neighborhood\n\n2. OverallCond\n\n3. GrLivArea\n\n","33cd396d":"#### EXPLANATION\n\nThese are some categorical variable which came up as a result of kruskawallis test which conclude that these are some variables(Neighbourhood, ExterQual, KitchenQual) whose groups have a stistically significant difference in values.","0e6e987b":"### SUMMARY OF VISUALIZATION\n\n1-saleprice and overall quality is strongly corelated.\n\n2-garagearea and garagecars(Size of garage in square feet & Size of garage in car capacity)\n\n3-totalbsmtSF and 1stFlrSF(Total square feet of basement area & First Floor square feet)\n\n4-GrLivArea and TotRmsAbvGrd(Above grade (ground) living area square feet & Total rooms above grade (does not include bathrooms))","e80b60a6":"### GrLivArea\n\nWith this graph i am going to analyse `SalePrice` relation with `GrLivArea` : Above grade (ground) living area square feet","d0fe44bc":"# BUILDING MODELS\n\nIn this section i am going to build regression models on the dataset prepared. I am going to demonstrate how using regulerization techiniques with linear regression can improve performance of model significantely. \n\nRegulerization techniques:\n\n1. Ridge regression\n \n2. Lasso regression","6754c359":"Exploring all 43 variable is not practical. To move forward we need some kind of priority list to choose only the top variables from. To make a list of variables with priority is to limit the scope of our investigation. So, we can derive maximum information using minimum real state.\n\nFor this purpose we are going to use **kruskawallis test** which is also known as (\"One way ANOVA with Ranks\") which is a non parametic test use to determine if there is statistical difference between two or more groups.\n","a6be25b7":"#### EXPLANATION\nThis graph shows the evidence for the presence of a **strong linear relationship** between `SalePrice` and `GrLivArea`","305c2c7b":"## ENCODING\n\nThis secting is dedicated to encoding categorical variables present in the data. Regression models used in the analysis are not efficient in handling catehorical variable to solve this problem every categorical column will be encoded using one hot encoding.","51b4129f":"### OverallCond\n\nOverallCond: Rates the overall condition of the house\n\n       10\tVery Excellent\n       9\tExcellent\n       8\tVery Good\n       7\tGood\n       6\tAbove Average\t\n       5\tAverage\n       4\tBelow Average\t\n       3\tFair\n       2\tPoor\n       1\tVery Poor","4ca47cce":"In Regression analysis it is expected that independent variables are independent. The regression coefficient we get are measure of change in dependent variable when independent variable is changed by 1 unit, holding all other variables constant. Having high correlation between variable can make it difficult to change one variable keeping other constant.\n\nThe variables above shows high correlation with each other and should be removed to avoid the problem of **Multicollinearity**. We can conclude that the variables represent same information. \n\nFor example:\n\n1. **TotRmsAbvGrd**: Total rooms above grade (does not include bathrooms)-->above grade floor is completely above ground \n   is same as **GrLivArea**: Above grade (ground) living area square feet.\n   \n2. **GarageYrBlt**: Year garage was built --> **YearBuilt**: Original construction date\n","f5b8ab00":"# PROBLEM STATEMENT\n\nThe dataset used for this project contains 79 unique features of different houses across Ames, Iowa and the prices they are sold at. The features in the dataset takes into account quality and physical attributes of the properties across Iowa. \n\nNumber of variables:\n\n1. 20 continuous variables\n2. 46 categorical variable(23 ordinal, 23 nominal)\n3. 14 discrete variable\n\nThe data was derived from Kaggle Competition Dataset with Heeding **House Price : Advanced Regression Techniques**([Data Source](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/data)).\n\nIn this project i am going to demonstrate performance of few simple regression models:\n1. Multiple Linear Regression \n2. Lasso Regression\n3. Ridge Regression\n4. Elastic net Regression\n\nI will try to evaluate the performance on the model on the basis of\n $$ MSE = {\\frac{1}{n}\\Sigma_{i=1}^{n}{\\Big(\\frac{d_i -f_i}{\\sigma_i}\\Big)^2}}$$\n\n","133f090d":"### ZOOMED CORELATION MATRIX","08bde59c":"This section i am going to zoom at variables which are corelated to target variable `SalePrice`. ","5a8411ad":"# LOADING LIBRARIES","8c38a91e":"looking at some of these columns containing null values, some attributes have only few entries. Even thou as mentioned above an NA means aminity not present but if an ammenity is not present in majority (PoolQC contains 99% NA). There is no point in keeping that attribute as it will not provide useful information to our model.","bede38e3":"## DATATYPE CORRECTION\n\nThere are many variables in the dataset which are ordinal but defined as nominal. In this sub section we are going to investigate and correct all ordinal variable which are present as nominal variable.\n\nStarting our data type correction with nominal variable which is present as ordinal.\n\nMSSubClass : Identifies the type of dwelling involved in the sale. It is a variable in which order is not appropriate and is better of as a nominal variable.\n\n\nMoving forward we can observe many features which can be better described as ordinal are present as nominal.\n\nfor example: ExterQual - Evaluates the quality of the material on the exterior \n\t\t\n       Ex\tExcellent\n       Gd\tGood\n       TA\tAverage\/Typical\n       Fa\tFair\n       Po\tPoor\n       \nThere is an order present in variable values **Excellent > Good > Average\/Typical > Fair > Poor** these were incoded to hold an order with the best group having highest number.\n","361607bd":"Looking at this visualization some variables come up having vary less data for example:\n\n1. Alley\n2. PoolQC\n3. Fence\n4. MiscFeature\n\ncontains NA in most of the observation. But, looking at data description most of nominal variables that contains NA can be interepreted as no having feature specified by column.\n\nfor example:\n\nPoolQC: NA -> No Pool\n\nMiscFeature: NA -> None\n\nand a similar intrepretation can be drawn for NA in **Fence, FireplaceQu, BsmtCond, BsmtCond, BsmtQual, BsmtExposure, BsmtFinType1, BsmtFinType2**. To deal with all these NA values a simple imputation is enough.","d7c6194c":"# DATA","220778be":"## CREATING TEST SET","d84e8050":"#### SUMMARY FROM GRAPH\n\nLooking at the corelation matrix 4 points grabs my attention most with a bright red color:\n\n1. TotRmsAbvGrd - GrLivArea\n2. GarageYrBlt - YearBuilt\n3. TotalBsmtSF - 1stFlrSF\n4. GarageCars - GarageArea","dde15a89":"## MISSING DATA\n\nMissing value is defined as the data value that is not present for the vaiable of interest. The problem with missing value is that it can generate inconsistency in our result and many machine learning algorithms don't accept missing values.\n\nThere are various methods for treating missing values some of the popular ones are:\n\n1. Value imputation\n2. Removing the observation\n\nTo measure the completeness of the data present, python provides a **missingno** function which help us visualize the complements of each variable present.  \n","6401d697":"### EXPLANATION\n\nThe group with lesser age and higher overquality have a higher sale price.","6afbfd19":"# IMPORT DATA\n\nThe data is already devided into train and test set.","e16718d4":"#### EXPLANATION\n\nThis graph shows a clear evidence towards the initial assumption that overall quality of house effects sales price. As to be noted houses with higher rating for `OverallCond` have higher sale price.","693698db":"## EXPLORING QUANTITATIVE VARIABLE","e413b0c4":"### CHECKING CORELATIONS","9d4786d4":"### REMOVING REDUNDENT FEATURES\n\nAs mentioned in the previous section some features are redundent in the datset and does not add any value to our regression model hence elemination of such features are important.\n'TotRmsAbvGrd','GarageYrBlt','1stFlrSF','GarageArea', 'YearBuilt'","4f4c826c":"# PREPROCESSING","fe339b9f":"scatterplots are a good way to visualize replationships between quantitative variables. In these scatterplot i am going to dig a little deeper inside these 4 corelated variables.","3428b42b":"### Neighborhood","dad34ec8":"## FEATURE ENGINEERING \n\nThis section is aimed at engineering some meaningful features out of existing one. \n\n**By defination** `Feature engineering is the process of using domain knowledge to extract features from raw data via data mining\ntechniques. These features can be used to improve the performance of machine learning algorithms.`\n\n* The features created\/extracted from existing ones provide more meaningful data for model to train on which help the model to explain the variation in target variable better.\n\n* The features extraced from combination of features can help reduce number of features in model making them redundant.\n\n\n1. **Age of house sold**\n\nthis feature describes the age of the house sold which can be calculated using the features describing year when the house was built `YearBuilt` and the year when house was sold `YrSold`.\n\n\n2. **Remodling of house**:\n\nThis feature is engineered using the assumption that when a house was remodled can effect sales price saleprice. This feature can be easily calculated using year \n\nThe 6 bins used are :\n1. no -> not remodeled\n2. 1 year -> remodeled after 1 year of building\n3. 2 - 20 years -> remodeled between 2 -20 after built.\n4. 20 -40 years -> remodeled between 20 - 40 after built.\n5. 40 - 70 years -> remodeled between 40 -70 after built.\n6. 70+ years -> remodeled 70+ years after built.\n\n\n3. **Quality Scores**\n\nIn this dataset there are heaps of variable which rates condition and quality of different features of house rating them from Excellent <--> poor. Earlier we converted these variables to an ordinal form. In this section we are going to engineer a feature based on assumption that a house with a higher quality rating and higher condition rating of a particular feature(Basement, Garage) will have higher sale price. \n\nmultiplying Quality and Condition score will make the difference more pronounced between houses with low and high rating.  \n\n4. **TACKLING NON LENIARITY**\n\nLooking at the huge number of variables it is estimated that a simple linear model will not be able to capture target variable, i am going to introduce some features which are strongly related to sale price raised to a power which will help fight non linearity.","1dd08a09":"### VISUALIZING CORELATED RELATIONSHIPS","f4df2857":"## DISTRIBUTION OF TARGET VARIABLE\n\nAs mentioned above `SalePrice` is the target variable for the problem set of the analysis. ","94d80c2e":"## SCALING VARIABLES\n\nScaling is considered an important part of preprocessing as having different scale in different features can make it difficult for loss founction to convergence. In this part i am going to scale all the quantitative features i am going to train my model on.","f1be02f3":"Moving forward i am going to check standard correlation coefficient (also called Pearson\u2019s r) between every pair of attributes using the corr() method and try to decipher some relationships between variables.","f2d20708":"## EXPLORING QUALITATIVE VARIABLE\n\nIn this section of the EDA the focus of the analysis is going to shift towards qualitaive variables(Nomial\/ordinal)"}}