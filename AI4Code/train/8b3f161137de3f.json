{"cell_type":{"210a2b20":"code","30134bd2":"code","0c555a84":"code","ee251cfd":"code","6f342f02":"code","f5df9901":"code","da106c3f":"code","f94d2286":"code","1565dcb1":"code","e54c17d4":"code","7bf49364":"code","429ea12e":"code","54c24cf0":"code","944a06de":"code","00416363":"code","2f15be17":"code","32ca47d8":"code","d1c6462c":"code","4ff1131d":"code","e914519d":"code","b1d16eac":"code","c1bc1b0d":"code","f09262ec":"code","5776aeec":"code","fdecae7e":"code","ed3ba89c":"code","7787b382":"code","9b2a557c":"code","38fd78c9":"markdown","578c6c26":"markdown","89fb6e54":"markdown","e8cb9517":"markdown","fc6877ad":"markdown","8b544611":"markdown","8b5761c3":"markdown","1948120a":"markdown","8b2e4ca9":"markdown","440dd32c":"markdown","a2c74e5c":"markdown","9b52bdb6":"markdown","9db48ebd":"markdown"},"source":{"210a2b20":"from IPython.display import HTML\n\nHTML('<center><iframe  width=\"850\" height=\"450\" src=\"https:\/\/www.youtube.com\/embed\/K0H43N-Hx7w\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen><\/iframe><\/center>')","30134bd2":"!pip install pymap3d==2.1.0\n!pip install -U l5kit","0c555a84":"import os, gc\nimport zarr\nimport numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport seaborn as sns\nsns.set()\n%config InlineBackend.figure_format = 'retina'\n\nimport matplotlib.pyplot as plt\n\nfrom l5kit.data import ChunkedDataset, LocalDataManager\nfrom l5kit.dataset import EgoDataset, AgentDataset\n\nfrom l5kit.rasterization import build_rasterizer\nfrom l5kit.configs import load_config_data\nfrom l5kit.visualization import draw_trajectory, TARGET_POINTS_COLOR\nfrom l5kit.geometry import transform_points\nfrom tqdm import tqdm\nfrom collections import Counter\nfrom l5kit.data import PERCEPTION_LABELS\nfrom prettytable import PrettyTable\n\nfrom matplotlib import animation, rc\nfrom IPython.display import HTML\n\nrc('animation', html='jshtml')","ee251cfd":"def animate_solution(images):\n\n    def animate(i):\n        im.set_data(images[i])\n \n    fig, ax = plt.subplots()\n    im = ax.imshow(images[0])\n    \n    return animation.FuncAnimation(fig, animate, frames=len(images), interval=60)","6f342f02":"# set env variable for data\nos.environ[\"L5KIT_DATA_FOLDER\"] = \"..\/input\/lyft-motion-prediction-autonomous-vehicles\"\n# get config\ncfg = load_config_data(\"..\/input\/lyft-config-files\/visualisation_config.yaml\")\nprint(cfg)","f5df9901":"dm = LocalDataManager()\ndataset_path = dm.require(cfg[\"val_data_loader\"][\"key\"])\nzarr_dataset = ChunkedDataset(dataset_path)\nzarr_dataset.open()\nprint(zarr_dataset)","da106c3f":"agents = zarr_dataset.agents\nprobabilities = agents[\"label_probabilities\"]\nlabels_indexes = np.argmax(probabilities, axis=1)\ncounts = []\nfor idx_label, label in enumerate(PERCEPTION_LABELS):\n    counts.append(np.sum(labels_indexes == idx_label))\n    \ntable = PrettyTable(field_names=[\"label\", \"counts\"])\nfor count, label in zip(counts, PERCEPTION_LABELS):\n    table.add_row([label, count])\nprint(table)","f94d2286":"rast = build_rasterizer(cfg, dm)\ndataset = EgoDataset(cfg, zarr_dataset, rast)","1565dcb1":"data = dataset[80]\n\nim = data[\"image\"].transpose(1, 2, 0)\nim = dataset.rasterizer.to_rgb(im)\ntarget_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\ndraw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n\nplt.imshow(im[::-1])\nplt.show()","e54c17d4":"cfg[\"raster_params\"][\"map_type\"] = \"py_satellite\"\nrast = build_rasterizer(cfg, dm)\ndataset = EgoDataset(cfg, zarr_dataset, rast)\ndata = dataset[80]\n\nim = data[\"image\"].transpose(1, 2, 0)\nim = dataset.rasterizer.to_rgb(im)\ntarget_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\ndraw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n\nfig = plt.subplots(figsize=(10,10))\nplt.imshow(im[::-1])\nplt.show()","7bf49364":"dataset = AgentDataset(cfg, zarr_dataset, rast)\ndata = dataset[80]\n\nim = data[\"image\"].transpose(1, 2, 0)\nim = dataset.rasterizer.to_rgb(im)\ntarget_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\ndraw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n\nfig = plt.subplots(figsize=(10,10))\nplt.imshow(im[::-1])\nplt.show()","429ea12e":"from IPython.display import display, clear_output\nimport PIL\n \ncfg[\"raster_params\"][\"map_type\"] = \"py_semantic\"\nrast = build_rasterizer(cfg, dm)\ndataset = EgoDataset(cfg, zarr_dataset, rast)\nscene_idx = 34\nindexes = dataset.get_scene_indices(scene_idx)\nimages = []\n\nfor idx in indexes:\n    \n    data = dataset[idx]\n    im = data[\"image\"].transpose(1, 2, 0)\n    im = dataset.rasterizer.to_rgb(im)\n    target_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\n    center_in_pixels = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n    draw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n    clear_output(wait=True)\n    images.append(PIL.Image.fromarray(im[::-1]))","54c24cf0":"anim = animate_solution(images)\nHTML(anim.to_jshtml())","944a06de":"anim = animate_solution(images)\nHTML(anim.to_jshtml())","00416363":"z = zarr.open(\".\/dataset.zarr\", mode=\"w\", shape=(500,), dtype=np.float32, chunks=(100,))\n\n# We can write to it by assigning to it. This gets persisted on disk.\nz[0:150] = np.arange(150)\nprint(z.info)\n\n# Reading from a zarr array is as easy as slicing from it like you would any numpy array. \n# The return value is an ordinary numpy array. Zarr takes care of determining which chunks to read from.\nprint(z[:10])\nprint(z[::20]) # Read every 20th value","2f15be17":"SCENE_DTYPE = [\n    (\"frame_index_interval\", np.int64, (2,)),\n    (\"host\", \"<U16\"),  # Unicode string up to 16 chars\n    (\"start_time\", np.int64),\n    (\"end_time\", np.int64),\n]","32ca47d8":"FRAME_DTYPE = [\n    (\"timestamp\", np.int64),\n    (\"agent_index_interval\", np.int64, (2,)),\n    (\"traffic_light_faces_index_interval\", np.int64, (2,)),\n    (\"ego_translation\", np.float64, (3,)),\n    (\"ego_rotation\", np.float64, (3, 3)),\n]","d1c6462c":"HTML('<center><iframe width=\"700\" height=\"400\" src=\"https:\/\/www.youtube.com\/embed\/tlThdr3O5Qo?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen><\/iframe><\/center>')","4ff1131d":"os.environ[\"L5KIT_DATA_FOLDER\"] = \"..\/input\/lyft-motion-prediction-autonomous-vehicles\"\ndm = LocalDataManager()\nsample_path = '..\/input\/lyft-motion-prediction-autonomous-vehicles\/scenes\/sample.zarr'\nsample_dataset = ChunkedDataset(sample_path)\nsample_dataset.open()\n\nsample_agents = sample_dataset.agents\nsample_agents = pd.DataFrame(sample_agents)\nsample_agents.columns = [\"data\"]; features = ['centroid', 'extent', 'yaw', 'velocity', 'track_id', 'label_probabilities']\n\nfor i, feature in enumerate(features):\n    sample_agents[feature] = sample_agents['data'].apply(lambda x: x[i])\nsample_agents.drop(columns=[\"data\"],inplace=True)\nsample_agents.head()","e914519d":"from l5kit.data.map_api import MapAPI\nfrom l5kit.rasterization.rasterizer_builder import _load_metadata\n\nsemantic_map_filepath = dm.require(cfg[\"raster_params\"][\"semantic_map_key\"])\ndataset_meta = _load_metadata(cfg[\"raster_params\"][\"dataset_meta_key\"], dm)\nworld_to_ecef = np.array(dataset_meta[\"world_to_ecef\"], dtype=np.float64)\n\nmap_api = MapAPI(semantic_map_filepath, world_to_ecef)\nMAP_LAYERS = [\"junction\", \"node\", \"segment\", \"lane\"]\n\n\ndef element_of_type(elem, layer_name):\n    return elem.element.HasField(layer_name)\n\n\ndef get_elements_from_layer(map_api, layer_name):\n    return [elem for elem in map_api.elements if element_of_type(elem, layer_name)]\n\n\nclass MapRenderer:\n    \n    def __init__(self, map_api):\n        self._color_map = dict(drivable_area='#a6cee3',\n                               road_segment='#1f78b4',\n                               road_block='#b2df8a',\n                               lane='#474747')\n        self._map_api = map_api\n    \n    def render_layer(self, layer_name):\n        fig = plt.figure(figsize=(10, 10))\n        ax = fig.add_axes([0, 0, 1, 1])\n        \n    def render_lanes(self):\n        all_lanes = get_elements_from_layer(self._map_api, \"lane\")\n        fig = plt.figure(figsize=(10, 10))\n        ax = fig.add_axes([0, 0, 1, 1])\n        for lane in all_lanes:\n            self.render_lane(ax, lane)\n        return fig, ax\n        \n    def render_lane(self, ax, lane):\n        coords = self._map_api.get_lane_coords(MapAPI.id_as_str(lane.id))\n        self.render_boundary(ax, coords[\"xyz_left\"])\n        self.render_boundary(ax, coords[\"xyz_right\"])\n        \n    def render_boundary(self, ax, boundary):\n        xs = boundary[:, 0]\n        ys = boundary[:, 1] \n        ax.plot(xs, ys, color=self._color_map[\"lane\"], label=\"lane\")\n        \n        \nrenderer = MapRenderer(map_api)\nfig, ax = renderer.render_lanes()","b1d16eac":"import pandas as pd, numpy as np\npd.options.display.max_columns=305","c1bc1b0d":"paths = [\n    \"..\/input\/lyft-best-performing-public-kernels\/lyft-ensembling-raster-sizes.csv\", \n    \"..\/input\/lyft-best-performing-public-kernels\/lyft-prediction-with-multi-mode-confidence.csv\",\n]\nweights = [0.4, 0.6]","f09262ec":"conf_cols = np.array([\"conf_0\", \"conf_1\", \"conf_2\"])","5776aeec":"xy_cols = [[],[],[]]\nfor i in range(50):\n    for j in range(3):\n        xy_cols[j].append(f\"coord_x{j}{i}\")\n        xy_cols[j].append(f\"coord_y{j}{i}\")\nxy_cols[0][:10]","fdecae7e":"COLUMNS = [\"timestamp\", \"track_id\"] + list(conf_cols) + xy_cols[0] + xy_cols[1] + xy_cols[2]","ed3ba89c":"def sort_df(df, sort_timestamp_track_id=True):\n    \n    conf_orders = np.argsort(-df[conf_cols].values,1)\n    XY = np.stack([df[xy_cols[0]].values,df[xy_cols[1]].values, df[xy_cols[2]].values], axis=1)\n    XY = XY[np.arange(len(XY))[:, None], conf_orders]\n\n    df2 = pd.DataFrame(columns = COLUMNS)\n    df2[\"timestamp\"] = df[\"timestamp\"].values\n    df2[\"track_id\"] = df[\"track_id\"].values\n    df2[xy_cols[0] + xy_cols[1] + xy_cols[2]] = XY.reshape(-1,300)\n    df2[conf_cols] = df[conf_cols].values[np.arange(len(df))[:, None], conf_orders]\n    \n    if sort_timestamp_track_id:\n        df2.sort_values([\"timestamp\", \"track_id\"], inplace=True)\n        df2.reset_index(inplace=True, drop=True)\n    return df2","7787b382":"%%time\n\ndf = None\nfor path,w in zip(paths,weights):\n    print(w, path)\n    temp = pd.read_csv(path)\n    temp = sort_df(temp)\n    temp[COLUMNS[5:]] *= w\n    if df is None:\n        df = temp\n    else:\n        df[COLUMNS[2:]] += temp[COLUMNS[2:]]\ndf[conf_cols] \/= df[conf_cols].sum(1).values[:, None]\n\nsample = pd.read_csv(\"..\/input\/lyft-motion-prediction-autonomous-vehicles\/multi_mode_sample_submission.csv\")\n\ndf = sample[[\"timestamp\", \"track_id\"]].merge(df, on=[\"timestamp\", \"track_id\"])\nsample.shape, df.shape","9b2a557c":"df.to_csv(\"submission.csv\", index=False, float_format='%.6f')","38fd78c9":"# Submission","578c6c26":"# Frames \n\n\nA frame captures all information that was observed at a time. This includes\n\nthe timestamp, which the frame describes;\n\n* data about the ego vehicle itself such as rotation and position;\n* a reference to the other agents (vehicles, cyclists and pedestrians) that were captured by the ego's sensors;\n* a reference to all traffic light faces (see below) for all visible lanes.\n\n\nThe properties for both agents and traffic light faces are stored in their two respective arrays. The frame contains only pointers to these stored objects given by a start and an end index in these arrays (again, start is included while end excluded).","89fb6e54":"# DATA FORMAT and Data Overview\n\nThe dataset is provided in zarr format. The zarr files are flat, compact, and highly performant for loading. To read the dataset please use our new Python software kit.\nThe dataset consists of frames and agent states. A frame is a snapshot in time which consists of ego pose, time, and multiple agent states. Each agent state describes the position, orientation, bounds, and type.","e8cb9517":"# Combining","fc6877ad":"# Load the data","8b544611":"# Data Format\n\nThe data in this competition is packed into .zarr files, which can be loaded with the Python zarr module. Taken from the competition Data description, each .zarr file contains a set of:\n\n* scenes: driving episodes acquired from a given vehicle.\n* frames: snapshots in time of the pose of the vehicle.\n* agents: a generic entity captured by the vehicle's sensors. Note that only 4 of the 17 possible agent label_probabilities are present in this dataset.\n* agents_mask: a mask that (for train and validation) masks out objects that aren't useful for training. In test, the mask (provided in files as mask.npz) masks out any test object for which predictions are NOT required.\n* traffic_light_faces: traffic light information.","8b5761c3":"# Autonomous Vehicle with Trajectory","1948120a":"# Semantic","8b2e4ca9":"# Please upvote if you find it useful","440dd32c":"# Satellite","a2c74e5c":"Combining Lyft : https:\/\/www.kaggle.com\/kneroma\/combining-lyft-multimode-models\/","9b52bdb6":"# Scenes\n\nA scene is identified by the host (i.e. which car was used to collect it) and a start and end time. It consists of multiple frames (=snapshots at discretized time intervals). The scene datatype stores references to its corresponding frames in terms of the start and end index within the frames array (described below). The frames in between these indices all correspond to the scene (including start index, excluding end index).","9db48ebd":"# Satellite"}}