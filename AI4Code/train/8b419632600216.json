{"cell_type":{"3870cf44":"code","db2604dc":"code","5f284410":"code","96ea4260":"code","41e3980f":"code","268f0773":"code","e995ba3e":"code","076df790":"code","aeed4554":"code","ffede4c1":"code","72a8c7bd":"code","70fa1dc8":"code","084c80bc":"code","90e41dbd":"code","27548343":"code","1f5ddb0e":"code","afb36518":"code","7ca99698":"code","b48a121a":"code","f978ea4f":"code","ffe47480":"code","f4e5e3ca":"code","06465564":"code","f0129529":"markdown","0db94cc0":"markdown","b189d66a":"markdown","637e8a5a":"markdown"},"source":{"3870cf44":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom wordcloud import WordCloud,STOPWORDS\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","db2604dc":"df = pd.read_csv('\/kaggle\/input\/eksi-sozluk-entries\/real.csv')\ndf.head()","5f284410":"df.label.value_counts()","96ea4260":"import seaborn as sns\n\nsns.countplot(df.label)","41e3980f":"df['text'] = df['text'].str.replace(\"\\r\", \" \")\ndf['text'] = df['text'].str.replace(\"\\n\", \" \")\ndf['text'] = df['text'].str.replace(\"    \", \" \")\ndf['text'] = df['text'].str.replace('\"', '')\ndf['text'] = df['text'].str.replace('\"', '')\n\npunctuation_signs = list(\")(?:!.,;\")\n\nfor punct_sign in punctuation_signs:\n    df['text'] = df['text'].str.replace(punct_sign, ' ')","268f0773":"df['text'][0]","e995ba3e":"import nltk\nfrom nltk.corpus import stopwords\nSTOP_WORDS = list(stopwords.words('turkish'))","076df790":"for stop_word in STOP_WORDS:\n    regex_stopword = r\"\\b\" + stop_word + r\"\\b\"\n    df['text'] = df['text'].str.replace(regex_stopword, '')","aeed4554":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder","ffede4c1":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","72a8c7bd":"encoder = LabelEncoder()\ndf['label_num']  = encoder.fit_transform(df['label'])\ndf['label_num'].value_counts()","70fa1dc8":"X_train, X_test, y_train, y_test = train_test_split(df['text'], \n                                                    df['label_num'], \n                                                    test_size=0.10, \n                                                    random_state=8)","084c80bc":"y_train =  tf.keras.utils.to_categorical(y_train, num_classes=5)\ny_test  = tf.keras.utils.to_categorical(y_test, num_classes=5)\n\nprint(y_train.shape)","90e41dbd":"vocab_size = 2000\nembedding_dim = 16\nmax_length = 120\ntrunc_type='post'\npadding_type='post'\noov_tok = \"<OOV>\"","27548343":"tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(X_train)\nword_index = tokenizer.word_index\n\ntrain_sequences = tokenizer.texts_to_sequences(X_train)\ntrain_padded = pad_sequences(train_sequences, padding=padding_type, maxlen=max_length)","1f5ddb0e":"reversed_word_index = dict([(value, key) for key, value in word_index.items()])","afb36518":"sentence=\"\"\nfor i in train_padded[0]:\n    if i != 0:\n        sentence+=reversed_word_index[i]\n    else:\n        break\nprint(sentence)","7ca99698":"validation_sequences = tokenizer.texts_to_sequences(X_test)\nvalidation_padded = pad_sequences(validation_sequences, padding=padding_type, maxlen=max_length)","b48a121a":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(24, activation='relu'),\n    tf.keras.layers.Dense(5, activation='softmax')\n])\nmodel.compile(loss=\"categorical_crossentropy\",optimizer='adam',metrics=['accuracy'])\nmodel.summary()","f978ea4f":"num_epochs = 20\nhistory = model.fit(train_padded, y_train, epochs=num_epochs, validation_data=(validation_padded, y_test), verbose=2)","ffe47480":"import matplotlib.pyplot as plt\n\n\ndef plot_graphs(history, string):\n  plt.plot(history.history[string])\n  plt.plot(history.history['val_'+string])\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(string)\n  plt.legend([string, 'val_'+string])\n  plt.show()\n  \nplot_graphs(history, \"accuracy\")\nplot_graphs(history, \"loss\")","f4e5e3ca":"e = model.layers[0]\nweights = e.get_weights()[0]\nprint(weights.shape) # shape: (vocab_size, embedding_dim)","06465564":"import io\n\nout_v = io.open('vecs.tsv', 'w', encoding='utf-8')\nout_m = io.open('meta.tsv', 'w', encoding='utf-8')\nfor word_num in range(1, vocab_size):\n  word = reversed_word_index[word_num]\n  embeddings = weights[word_num]\n  out_m.write(word + \"\\n\")\n  out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\nout_v.close()\nout_m.close()","f0129529":"## DATA CLEANING","0db94cc0":"**For a starting point 80 percent accuracy is good. I will add more data and try other models such as SVM even neural networks.**\n\n**If you like this simple kernel, please upvote.**","b189d66a":"To see the projection go to [tensorflow projector](https:\/\/projector.tensorflow.org\/) upload these meta.tsv and vecs.tsv files and see the embeddings.","637e8a5a":"There is clearly a overfitting."}}