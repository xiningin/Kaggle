{"cell_type":{"4a4c0e1a":"code","c4cd0e2b":"code","bd2b8842":"code","a68311db":"code","006d3849":"code","6c053341":"code","8ba75f3a":"code","b665d30f":"code","aba6e8f3":"code","4f3076a2":"code","19bd7bc4":"code","69ab6969":"code","f4e1d8ff":"code","d5277eaa":"code","1dcca96f":"code","7608fd1c":"code","42db8383":"code","2bf40ffc":"code","05e18fca":"code","f554b891":"code","3d510ea1":"code","b41f3650":"code","ef6c5b1a":"code","6e33689b":"code","a814d5a4":"code","fc6a10dc":"code","6b1d129d":"code","a46b453d":"code","f7799643":"code","23c47e60":"code","3cd72547":"code","3d47fb5f":"code","75472dea":"code","f594e66a":"code","cca5c84c":"code","9c19da0a":"code","7bdf29fa":"code","135fc4a9":"markdown","780754c6":"markdown","8297a064":"markdown","b3dad7e5":"markdown","9ad44bf5":"markdown","79606a48":"markdown","f0a32b31":"markdown","93a0bb82":"markdown","098c79e4":"markdown","6109569f":"markdown"},"source":{"4a4c0e1a":"!python -m spacy download en_core_web_md | grep -v 'already satisfied'","c4cd0e2b":"import numpy as np\nimport pandas as pd\nimport re\n\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nimport string\nimport spacy\nnlp = spacy.load(\"en_core_web_md\")\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch.nn import functional as F\n\nfrom sklearn.decomposition import NMF\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn import metrics\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import LinearSVC\nfrom sklearn.feature_selection import chi2","bd2b8842":"df = pd.read_csv('..\/input\/reddit-proana-dataset\/proED_full_dataset.csv')\ndf.head()","a68311db":"df['Flair'] = df['Flair'].apply(lambda x: x.lower())\ndf['Flair'] = df['Flair'].apply(lambda x: \"rant\/rave\" if re.search(r\"(?:rant|rave)\", x) else x)\ndf['Flair'] = df['Flair'].apply(lambda x: \"discussion\" if re.search(r\"discussion\", x) else x)\ndf['Flair'] = df['Flair'].apply(lambda x: \"humor\" if re.search(r\"humor\", x) else x)","006d3849":"#select 10 largest categories\nflair_counts = df['Flair'].value_counts().to_frame()\ntop10 = flair_counts[:10].index.tolist()\n\ndf = df.loc[df['Flair'].isin(top10) == True,]\nlen(df)","6c053341":"data = {'Text': df['Title'] + ' ' + df['Text'], 'Flair': df['Flair']}\ndf_sm = pd.DataFrame(data=data)\ndf_sm.head()","8ba75f3a":"stpwrd = stopwords.words('english')\nstpwrd.extend(['[deleted]', '[removed]'])","b665d30f":"def process(text):\n    nopunc = [char for char in text if char not in string.punctuation]\n    nopunc = ''.join(nopunc)\n    clean = ' '.join(word.lower() for word in nopunc.split() if word.lower() not in stopwords.words('english'))\n    return clean\n\ndf_sm['Text'] = df_sm['Text'].map(lambda x: x if str(x) == 'nan' else process(x))","aba6e8f3":"#save the cleaned dataset\ndf_sm.to_csv('proED_full_dataset_clean_sm.csv')","4f3076a2":"df = pd.read_csv('..\/input\/proedclean\/proED_full_dataset_clean_sm.csv')\ndf = df.iloc[:, 1:]\ndf.head()","19bd7bc4":"#removing rows with either NA `Text` value or the value that's too long\ndf = df[df['Text'].isna()==False]\ndf = df[df['Text'].map(len) < 1000000]","69ab6969":"#subset rows based on selected flairs\nflairs_selected = ['rant\/rave', 'discussion', 'help', 'goal', 'thinspo']\ndf = df[df['Flair'].isin(flairs_selected)]\nlen(df)","f4e1d8ff":"tfidf = TfidfVectorizer()\ndtm = tfidf.fit_transform(df['Text'])","d5277eaa":"nmf_model = NMF(n_components=8,random_state=42)\nnmf_model.fit(dtm)","1dcca96f":"for index,topic in enumerate(nmf_model.components_):\n    print(f'THE TOP 10 WORDS FOR TOPIC #{index}')\n    print([tfidf.get_feature_names()[i] for i in topic.argsort()[-10:]])\n    print('\\n')","7608fd1c":"mapping_dict = {}\nfor i, f in enumerate(flairs_selected):\n    mapping_dict[f] = i\n\nX = df['Text']\ny = df['Flair']\n\ny = y.map(mapping_dict).values","42db8383":"tfidf = TfidfVectorizer()\nfeat = tfidf.fit_transform(X).toarray()","2bf40ffc":"# chisq2 statistical test\nN = 5    # Number of examples to be listed\nfor f, i in sorted(mapping_dict.items()):\n    chi2_feat = chi2(feat, y == i)\n    indices = np.argsort(chi2_feat[0])\n    feat_names = np.array(tfidf.get_feature_names())[indices]\n    unigrams = [w for w in feat_names if len(w.split(' ')) == 1]\n    print(\"\\nFlair '{}':\".format(f))\n    print(\"Most correlated words:\\n\\t. {}\".format('\\n\\t. '.join(unigrams[-N:])))","05e18fca":"#X = df['Text']\n#y = df['Flair']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)\nprint(len(X_train), len(X_test))","f554b891":"clf_tfidf_lsvc = Pipeline([('tfidf', TfidfVectorizer()),\n                     ('clf', LinearSVC())])\n\n# Feed the training data through the pipeline\nclf_tfidf_lsvc.fit(X_train, y_train)\n\n# Form a prediction set\npredictions = clf_tfidf_lsvc.predict(X_test)\n# Print the overall accuracy\nprint('{}% correct'.format(round(metrics.accuracy_score(y_test,predictions)*100, 2)))","3d510ea1":"clf_count_lsvc = Pipeline([('cv', CountVectorizer()),\n                     ('clf', LinearSVC())])\n\nclf_count_lsvc.fit(X_train, y_train)\npredictions = clf_count_lsvc.predict(X_test)\nprint('{}% correct'.format(round(metrics.accuracy_score(y_test,predictions)*100, 2)))","b41f3650":"def vectorize(text):\n    text = nlp(text)\n    vec = [word.vector for word in text]\n    return torch.tensor(sum(vec) \/ len(vec))","ef6c5b1a":"X_train_tensor = torch.stack([vectorize(text) for text in X_train])\nX_test_tensor = torch.stack([vectorize(text) for text in X_test])\n\nmapping_dict = {}\nfor i, f in enumerate(list(set(df['Flair']))):\n    mapping_dict[f] = i\n\ny_train_tensor = torch.LongTensor(y_train)\ny_test_tensor = torch.LongTensor(y_test)\ny_train_tensor = torch.LongTensor(y_train.map(mapping_dict).values)\ny_test_tensor = torch.LongTensor(y_test.map(mapping_dict).values)","6e33689b":"torch.save(X_train_tensor, 'X_train.pt')\ntorch.save(X_test_tensor, 'X_test.pt')\ntorch.save(y_train_tensor, 'y_train.pt')\ntorch.save(y_test_tensor, 'y_test.pt')","a814d5a4":"class SingleNN(nn.Module):\n    def __init__(self, input_size, output_size):\n        super().__init__()\n        self.fc = nn.Linear(input_size, output_size, bias=False)\n        nn.init.normal_(self.fc.weight, 0.0, 1.0)\n  \n    def forward(self,x):\n        x = self.fc(x)\n        return x","fc6a10dc":"model = SingleNN(input_size=X_train_tensor.size()[1], output_size=len(set(df['Flair'])))\ncriterion = nn.CrossEntropyLoss()\nmodel","6b1d129d":"optimizer = torch.optim.Adam(model.parameters(), lr=0.01)","a46b453d":"epochs = 300\nlosses = []\n\nfor i in range(epochs):\n    i+=1\n    y_pred = model(X_train_tensor)\n    loss = criterion(y_pred, y_train_tensor)\n    losses.append(loss)\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    if i%10 == 1:\n        print(f'epoch: {i:3}  train_loss: {loss.item():10.8f}')","f7799643":"rows = len(y_test_tensor)\ncorrect = 0\n\nwith torch.no_grad():\n    y_val = model(X_test_tensor)\n\nfor i in range(rows):\n    if y_val[i].argmax().item() == y_test_tensor[i]:\n        correct += 1\n        \nprint('{}% correct'.format(round(100*correct\/rows), 2))","23c47e60":"class RedditDataset(Dataset):\n    def __init__(self, X, y):  # define dataset components\n        self.X = X\n        self.y = y\n\n    def __len__(self):  # define value returned by len(dataset)\n        return len(self.y)\n\n    def __getitem__(self, idx):  # define value returned by dataset[idx]\n        return [self.X[idx], self.y[idx]]","3cd72547":"dataset_train = RedditDataset(X_train_tensor, y_train_tensor)\ndataset_test = RedditDataset(X_test_tensor, y_test_tensor)","3d47fb5f":"dataloader_train = DataLoader(dataset_train, batch_size=1, shuffle=True)\ndataloader_test = DataLoader(dataset_test, batch_size=len(dataset_test), shuffle=False)","75472dea":"class MLPNet(nn.Module):\n    def __init__(self, input_size, mid_size, output_size, mid_layers):\n        super().__init__()\n        self.mid_layers = mid_layers\n        self.fc = nn.Linear(input_size, mid_size)\n        self.fc_mid = nn.Linear(mid_size, mid_size)\n        self.fc_out = nn.Linear(mid_size, output_size) \n        self.bn = nn.BatchNorm1d(mid_size)\n\n    def forward(self, x):\n        x = F.relu(self.fc(x))\n        for _ in range(self.mid_layers):\n            x = F.relu(self.bn(self.fc_mid(x)))\n            x = F.relu(self.fc_out(x))\n        return x","f594e66a":"def train_model(dataset_train, batch_size, model, criterion, optimizer, num_epochs, device=None):\n    model.to(device)\n    \n    dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n    \n    log_train = []\n    for epoch in range(num_epochs):\n        model.train()\n        for inputs, labels in dataloader_train:\n            optimizer.zero_grad()\n            \n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            outputs = model.forward(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n        \n        model.eval()\n        loss = 0.0\n        with torch.no_grad():\n            for inputs, labels in dataloader_train:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                outputs = model(inputs)\n                loss += criterion(outputs, labels).item()\n        \n        if epoch%10 == 1:\n            print(f'epoch: {epoch:3}  train_loss: {loss \/ len(dataloader_train):10.8f}')","cca5c84c":"model = MLPNet(300, 200, 5, 1)\n\ncriterion = nn.CrossEntropyLoss()\n\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n\ndevice = torch.device('cuda')\n\ntrain_model(dataset_train, 64, model, criterion, optimizer, 300, device)","9c19da0a":"def calculate_accuracy(model, loader, device):\n    model.eval()\n    total = 0\n    correct = 0\n    with torch.no_grad():\n        for inputs, labels in loader:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            outputs = model(inputs)\n            pred = torch.argmax(outputs, dim=-1)\n            total += len(inputs)\n            correct += (pred == labels).sum().item()\n    return correct \/ total","7bdf29fa":"train_accuracy = calculate_accuracy(model, dataloader_train, device)\ntest_accuracy = calculate_accuracy(model, dataloader_test, device)\nprint('{}% correct'.format(round(100*test_accuracy, 2)))","135fc4a9":"### topic modeling","780754c6":"### linearSVC with count vectorizer","8297a064":"### single-layer neural network","b3dad7e5":"### linearSVC with tf-idf vectorizer","9ad44bf5":"I will make a smaller dataframe consisting of `Text` (titles and main contents combined) and `Flair`.","79606a48":"### most characteristic words for each flair","f0a32b31":"### splitting into train and test data","93a0bb82":"## flair prediction and topic modeling from r\/proED\/ subreddit","098c79e4":"### multi-layer neural network","6109569f":"### pre-processing\nI'll rename some of the flairs with similar concepts to in order to reduce the number of categories. I will also remove stopwords and make the letters lower case."}}