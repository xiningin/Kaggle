{"cell_type":{"758e5afb":"code","03394f0b":"code","cb23f5a7":"code","294efb8a":"code","c0fae5b0":"code","5ee033ea":"code","32130c63":"code","b55a0136":"code","4d6e02dd":"code","1b5931ff":"code","61d7aa85":"code","42801def":"code","cde618fa":"code","a2e44d37":"code","76a03b66":"code","fa037f4a":"code","2a2a6a9a":"code","46a3c840":"code","c106459d":"code","9266b601":"markdown","db5e2ed3":"markdown","5e1e6984":"markdown","3bdfd145":"markdown"},"source":{"758e5afb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","03394f0b":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")","cb23f5a7":"train.head()","294efb8a":"import seaborn as sns\nsns.set_theme(style=\"whitegrid\")\ng = sns.catplot(x=\"Sex\", y=\"Survived\",\n                col=\"Pclass\",\n                data=train, kind=\"bar\",\n                height=4, aspect=.7);","c0fae5b0":"sns.set_theme(style=\"whitegrid\")\ng = sns.catplot(x=\"Parch\", y=\"Survived\",\n                data=train, kind=\"bar\",\n                height=4, aspect=2);","5ee033ea":"sns.set_theme(style=\"whitegrid\")\ng = sns.catplot(x=\"SibSp\", y=\"Survived\",\n                data=train, kind=\"bar\",\n                height=4, aspect=2);","32130c63":"age_survived_data = pd.DataFrame()\nage_survived_data['Age'] = train['Age']\nage_survived_data['Survived'] = train['Survived']\nbins = [0, 10, 20, 30, 40, 50, 60, 70, 80]\nlabels = ['0-10', '11-20', '21-30', '31-40', '41-50', '51-60', '61-70', '71-80']\nage_survived_data['AgeGroup'] = pd.cut(age_survived_data['Age'], bins=bins, labels=labels, right=False)\nage_survived_data['AgeGroup'] = age_survived_data.AgeGroup.cat.add_categories('unknown').fillna('unknown')\ntrain['AgeGroup'] = age_survived_data.AgeGroup","b55a0136":"sns.catplot(x='AgeGroup', y='Survived', kind='bar', data=age_survived_data, aspect=2)","4d6e02dd":"train.isna().sum()\n## Missing values: Age(177), Cabin(687), Embarked(2)\n## Age which cannot be predicted, but Cabin has 687 null values which we can try to predict using \"Fair\"(ticket cost) and \"PClass\"(class).","1b5931ff":"train.head()\n#train.groupby('Cabin')['Fare'].mean().tail(40)","61d7aa85":"## Lets drop rows where Age and Embarked values are null and Cabin column\n## Now the training data has 712 rows\ndata = train.dropna(subset=['Age', 'Embarked', 'Cabin'])\nlen(data.index)\ndel data['Cabin']\ndel data['Name']","42801def":"#Normalizing Age\ndata['NormalizedFare'] = (data['Fare'] - data['Fare'].min()\/data['Fare'].max()-data['Fare'].min())\ntest['NormalizedFare'] = (test['Fare'] - test['Fare'].min()\/test['Fare'].max()-test['Fare'].min())\n#Standardize Age\ndata['StandardizedFare'] = data['Fare'] - data['Fare'].mean()\/data['Fare'].std()\ntest['StandardizedFare'] = test['Fare'] - test['Fare'].mean()\/test['Fare'].std()\ntest","cde618fa":"train_features = data[['Age', 'NormalizedFare', 'Sex', 'Pclass','SibSp', 'Parch']]\ntrain_target = data['Survived']\ntest_features = test[['Age', 'NormalizedFare', 'Sex', 'Pclass', 'SibSp', 'Parch']]","a2e44d37":"from sklearn import preprocessing\n\nencoder = preprocessing.LabelEncoder()\ntrain_features = train_features.apply(encoder.fit_transform)\ntest_features = test_features.apply(encoder.fit_transform)\ntrain_features","76a03b66":"# Score: 0.76315 Rank: 47219\nfrom sklearn.linear_model import LogisticRegression\nreg = LogisticRegression().fit(train_features, train_target)\nreg.score(train_features, train_target)\nsurvived = reg.predict(test_features)","fa037f4a":"# Score: 0.76555 Rank: 46726\nfrom sklearn.naive_bayes import GaussianNB\nreg = GaussianNB().fit(train_features, train_target)\nreg.score(train_features, train_target)\nsurvived = reg.predict(test_features)","2a2a6a9a":"# Score: 0.64114\nfrom sklearn.neighbors import KNeighborsClassifier\nneigh = KNeighborsClassifier(n_neighbors=2)\nreg = neigh.fit(train_features, train_target)\nreg.score(train_features, train_target)\nsurvived = reg.predict(test_features)","46a3c840":"submission = pd.DataFrame({'PassengerId':test.PassengerId, 'Survived': survived})\nsubmission.to_csv('submission.csv', index=False)","c106459d":"# Score: 0.64354\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nclf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\nreg = clf.fit(train_features, train_target)\nsurvived = reg.predict(test_features)","9266b601":"Theory: Lets try to understand if Age was a deciding factor\n\nInsights:\n1. Kids in age group 0-10 survived the most, probably because women and children are evacuated first.\n\nConslusion: Age group is definitely a factor.","db5e2ed3":"Theory: Lets also check the relationship of SibSp(Sibling\/Spouse) and Parch(parent\/child\/other relationship). I don't think these play a deciding role, but let the data speak!\n\nInsights:\n1. People having 2\/3 siblings survived more than others\n2. Indivduals with their spouses or single parent(1) survived more than others.\n\nConslusion: The relation is not clear.","5e1e6984":"Theory: From the variable notes we understand that Pclass represents socio economic status, and I have theory that people with a higher socio economic status had a greater probability of survival. In addition to that, Sex is another variable that could be a deciding factor, as during a calamity, women and children are always evacuated first.\n\nInsights:\n1. Females survived way more than the Males.\n2. First and Second class people survived more than Third class\n\nConclusion: The socio economic status and sex was a determintal factor in deciding who got on the availaible life boats.","3bdfd145":"***The Challenge: The sinking of the Titanic is one of the most infamous shipwrecks in history.\n\n*On April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew. While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others. In this challenge, we ask you to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).*\n\n\n***Variable Notes\n\npclass: A proxy for socio-economic status (SES)\n1st = Upper\n2nd = Middle\n3rd = Lower\n\nage: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\nsibsp: The dataset defines family relations in this way...\nSibling = brother, sister, stepbrother, stepsister\nSpouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\nparch: The dataset defines family relations in this way...\nParent = mother, father\nChild = daughter, son, stepdaughter, stepson\nSome children travelled only with a nanny, therefore parch=0 for them."}}