{"cell_type":{"171a36cc":"code","f150f77a":"code","53c0b4bb":"code","6df098bd":"code","6f62949f":"code","fb718c27":"code","3711549f":"code","a87879fd":"code","285988f3":"code","826a467e":"code","b0da2d78":"code","523d455c":"code","2f173432":"code","e212c157":"code","a6908848":"code","fc26976b":"code","0ce49b0e":"code","8c337147":"code","429c73ec":"code","effcb67c":"code","ba71ddd0":"code","1239cd17":"code","0095a235":"code","2bc1b918":"code","1e542bc9":"code","4f067fc6":"code","a1d90f4a":"code","417eea0b":"code","dae4c79b":"code","df143de7":"code","ee0c9175":"code","d6895d25":"code","ae5e4879":"code","7f712653":"code","08fbdf5e":"code","2ca41096":"code","5aa1ccbd":"code","7cd2b6a2":"code","3f37cf27":"code","4de0e77d":"code","8d8c8f5a":"code","a78e16f9":"code","ef6c9220":"code","cb0dc139":"code","33566ca7":"code","de42f6e9":"code","a68c51ab":"code","b84d0ca4":"code","d9f77a7e":"code","1fc77d39":"code","26e0e177":"code","02a71c57":"code","fd4a3056":"code","ca47693b":"code","dd8823e3":"code","d0732556":"code","e90b7767":"code","cd5c0288":"code","d677d3d9":"code","05574f9b":"code","29e287a3":"code","a6894942":"code","ba107183":"code","12474424":"code","f763485e":"code","3fc8bdbe":"code","8251d195":"code","43104205":"code","2c3f5efc":"code","32eeab36":"code","1599a554":"code","b8581b65":"code","004ad65b":"code","859e50d7":"code","fbf91032":"code","f623a3da":"code","800ba87b":"code","17b3cdc1":"code","ef05127e":"code","6acbc4a7":"code","0c4aadef":"code","11009feb":"markdown"},"source":{"171a36cc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f150f77a":"# Caricare i dati di training, i dati di test, e verificare il # di osservazioni e il # di caratteristiche per ciascuno dei due\napp_train = pd.read_csv('..\/input\/home-credit-default-risk\/application_train.csv')\nprint('Shape di addestramento: ', app_train.shape)\napp_test = pd.read_csv('..\/input\/home-credit-default-risk\/application_test.csv')\nprint('Shape di addestramento: ', app_test.shape)","53c0b4bb":"# Create a label encoder object\nfrom sklearn import preprocessing\nle = preprocessing.LabelEncoder()\n\ndef mylabelencoder(df):\n    le_count = 0\n    # Iterate through the columns\n    for col in df:\n        if df[col].dtype == 'object':\n            # If 2 or fewer unique categories\n            if len(list(df[col].unique())) <= 3:\n                df[col] = df[col].fillna(\"\")\n                # Train on the training data\n                le.fit(df[col])\n                # Transform both training and testing data\n                df[col] = le.transform(df[col])\n\n                # Keep track of how many columns were label encoded\n                le_count += 1            \n    print('%d columns were label encoded.' % le_count)","6df098bd":"# Controlla che codifichi lo stesso numero di colonne su train e su test\nmylabelencoder(app_train)\nmylabelencoder(app_test)","6f62949f":"# One Hot Encoding delle altre feature categoriche\napp_train = pd.get_dummies(app_train)\napp_test = pd.get_dummies(app_test)","fb718c27":"# Riallineamento delle colonne\n# Cerca le colonne differenti tra i due set, togliendo dal train la target per cercare le differenze\ncolset_train = set(app_train.columns)\ncolset_test = set(app_test.columns)\ncolset_train.discard('TARGET')\ncolset_train.difference(colset_test)","3711549f":"for k in list(colset_train.difference(colset_test)):\n    app_test[k] = 0 # Default a 0 perch\u00e9 se le colonne non appaiono in test sono sempre non-hot\n    \nprint(app_train.shape)\nprint(app_test.shape)\nset(app_train.columns).difference(set(app_test.columns))\n\n# Test ha una colonna in meno perch\u00e9 non contiene TARGET","a87879fd":"# Analisi degli outlier\napp_train.dtypes.value_counts()","285988f3":"(app_train[\"DAYS_BIRTH\"]\/-365).describe()","826a467e":"(app_train[\"DAYS_EMPLOYED\"]\/365).describe()","b0da2d78":"app_train[\"DAYS_EMPLOYED\"].plot(kind=\"hist\")","523d455c":" app_train[\"DAYS_EMPLOYED\"] == 365243","2f173432":"# Identificare e filtrare le osservazioni che hanno la caratt. DAYS EMPLOYED con un valore outlier\n# Per caso, l'outlier \u00e8 legato al fatto che il prestito \u00e8 andato in default? In che proporzione di occasioni?\nanomalies = app_train[\"DAYS_EMPLOYED\"] == 365243\napp_train['DAYS_EMPLOYED_ANOM'] = anomalies\napp_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n(app_train['DAYS_EMPLOYED']\/365).plot.hist(title = 'Days Employment Histogram');\n","e212c157":"app_train[\"DAYS_EMPLOYED_ANOM\"].value_counts()","a6908848":"import seaborn as sns\n#sns.pairplot(app_train.iloc[:,:10])","fc26976b":"# Plotting as desired\n# app_train.iloc[:,27:70].plot.hist(subplots=True, legend=False)","0ce49b0e":"# Analisi delle collinearit\u00e0\n#correlations = app_train.corr()","8c337147":"#correlations","429c73ec":"#correlations[\"TARGET\"].sort_values().head(10)","effcb67c":"#correlations[\"TARGET\"].sort_values().tail(10)","ba71ddd0":"app_train['DAYS_BIRTH'].corr(app_train['TARGET'])","1239cd17":"(app_train['DAYS_BIRTH']\/-365).corr(app_train['TARGET'])","0095a235":"app_train[\"EXT_SOURCE_3\"].plot(kind=\"hist\")","2bc1b918":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.style.use('fivethirtyeight')\nplt.figure(figsize = (10, 8))\n\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 0, 'DAYS_BIRTH'] \/ -365, label = 'target == 0' )\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 1, 'DAYS_BIRTH'] \/ -365, label = 'target == 1' )\n\n# Metti la legenda\nplt.legend()\n","1e542bc9":"# Creare dei kernel plot su EXT_SOURCE(1,2,3) rispetto a TARGET per valutarne la \"quantit\u00e0 di segnale\"\n\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 0, 'EXT_SOURCE_1'], label = 'target == 0' )\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 1, 'EXT_SOURCE_1'], label = 'target == 1' )\n\n# Metti la legenda\nplt.legend()","4f067fc6":"# Creare dei kernel plot su EXT_SOURCE(1,2,3) rispetto a TARGET per valutarne la \"quantit\u00e0 di segnale\"\n\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 0, 'EXT_SOURCE_2'], label = 'target == 0' )\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 1, 'EXT_SOURCE_2'], label = 'target == 1' )\n\n# Metti la legenda\nplt.legend()","a1d90f4a":"# Creare dei kernel plot su EXT_SOURCE(1,2,3) rispetto a TARGET per valutarne la \"quantit\u00e0 di segnale\"\n\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 0, 'EXT_SOURCE_3'], label = 'target == 0' )\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 1, 'EXT_SOURCE_3'], label = 'target == 1' )\n\n# Metti la legenda\nplt.legend()","417eea0b":"# Extract the EXT_SOURCE variables and show correlations\next_data = app_train[['TARGET', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\next_data_corrs = ext_data.corr()\next_data_corrs","dae4c79b":"plt.figure(figsize = (8, 6))\n\n# Heatmap of correlations\nsns.heatmap(ext_data_corrs, cmap = plt.cm.RdYlBu_r, vmin = -0.25, annot = True, vmax = 0.6)\nplt.title('Correlation Heatmap');","df143de7":"# Feature Engineering\n# Combinazioni polinomiali delle caratteristiche\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.PolynomialFeatures.html\npoly_features = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\npoly_features_test = app_test[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\npoly_target = app_train['TARGET']","ee0c9175":"# Imputazione dei valori mancanti\n# https:\/\/scikit-learn.org\/stable\/modules\/impute.html\npoly_features['EXT_SOURCE_1'].plot(kind=\"hist\")","d6895d25":"poly_features['EXT_SOURCE_1'].isnull().value_counts()","ae5e4879":"# Inseriamo NaN al posto di null\npoly_features['EXT_SOURCE_1'] = poly_features['EXT_SOURCE_1'].fillna(np.nan)","7f712653":"poly_features","08fbdf5e":"# Uso di SimpleImputer per imputazione automatica di valori tramite una funzione nelle celle vuote\nimport numpy as np\nfrom sklearn.impute import SimpleImputer\nimp = SimpleImputer(missing_values=np.nan, strategy='median')\npoly_features_imputed = imp.fit_transform(poly_features)\npoly_features_imputed_df = pd.DataFrame(poly_features_imputed, columns=['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH'])","2ca41096":"poly_features_test_imputed = imp.fit_transform(poly_features_test)\npoly_features_test_imputed_df = pd.DataFrame(poly_features_test_imputed, columns=['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH'])","5aa1ccbd":"# Tappati tutti i buchi con le mediane\npoly_features_imputed_df.isnull().value_counts()\npoly_features_test_imputed_df.isnull().value_counts()","7cd2b6a2":"# Calcola combinazioni polinomiali delle feature del dataframe Df\nfrom sklearn.preprocessing import PolynomialFeatures\ndef poly_transformer(df):\n    poly_transformer = PolynomialFeatures(degree = 3)\n    poly_transformer.fit(df)\n    poly_features_df = pd.DataFrame(poly_transformer.transform(df), columns=poly_transformer.get_feature_names(df.columns))\n    poly_features_df = poly_features_df.drop(labels='1', axis=1)\n    return poly_features_df","3f37cf27":"# Ora finalmente posso usare le polynomial features\nfrom sklearn.preprocessing import PolynomialFeatures\npoly_transformer = PolynomialFeatures(degree = 3)\npoly_transformer.fit(poly_features_imputed_df)","4de0e77d":"poly_transformer.transform(poly_features_imputed_df)","8d8c8f5a":"poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH'])","a78e16f9":"poly_features_df = pd.DataFrame(poly_transformer.transform(poly_features_imputed_df), columns=poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']))","ef6c9220":"poly_features_df","cb0dc139":"poly_features_df = poly_features_df.drop(labels='1', axis=1)","33566ca7":"poly_features_df['TARGET'] = poly_target","de42f6e9":"# Vediamo ora se rispetto alle sole features predominanti di base, quelle costruite in polinomiali risultano maggiormente correlate al target\npoly_collinears = poly_features_df.corr()","a68c51ab":"poly_collinears['TARGET'].sort_values().head(5)","b84d0ca4":"poly_collinears['TARGET'].sort_values().tail(5)","d9f77a7e":"poly_features_test_df = pd.DataFrame(poly_transformer.transform(poly_features_test_imputed_df), columns=poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']))","1fc77d39":"poly_features_test_df = poly_features_test_df.drop(labels='1', axis=1)","26e0e177":"#prova_poly_features","02a71c57":"# Provo la mia funzione\n#prova_poly_features = poly_features_imputed_df.copy() # copia per separare i df nella trasformazione\n#pippo = poly_transformer(prova_poly_features)","fd4a3056":"# Sistemare la poly transformation in una funzione\n# Creare delle caratteristiche derivate (\"in modo business\")\n","ca47693b":"# Creo una copia per valore e non per riferimento dei dati di train e di test\napp_train_domain = app_train.copy()\napp_test_domain = app_test.copy()","dd8823e3":"# Funzione che calcola nuove caratteristiche di dominio\ndef calculate_domain_data(df):    \n    df['CREDIT_INCOME_PERCENT'] = df['AMT_CREDIT'] \/ df['AMT_INCOME_TOTAL']\n    df['ANNUITY_INCOME_PERCENT'] = df['AMT_ANNUITY'] \/ df['AMT_INCOME_TOTAL']\n    df['CREDIT_TERM'] = df['AMT_ANNUITY'] \/ df['AMT_CREDIT']\n    df['DAYS_EMPLOYED_PERCENT'] = df['DAYS_EMPLOYED'] \/ df['DAYS_BIRTH']","d0732556":"calculate_domain_data(app_train_domain)","e90b7767":"app_train_domain","cd5c0288":"# Infine, provare con la visualizzazione kernel density a dimostrare se qualcuna di queste nuove caratteristiche create (sia polinominali, sia di business)\n# ha un particolare potere \"separatorio\" rispetto al target\ndef mykdeplot(df, field):\n    sns.kdeplot(df.loc[app_train['TARGET'] == 0, field], label = 'target == 0' )\n    sns.kdeplot(df.loc[app_train['TARGET'] == 1, field], label = 'target == 1' )\n    \n    # Metti la legenda\n    plt.legend()","d677d3d9":"mykdeplot(poly_features_df, 'EXT_SOURCE_2 EXT_SOURCE_3')","05574f9b":"app_train","29e287a3":"import numpy as np\nfrom sklearn.impute import SimpleImputer\nimp = SimpleImputer(missing_values=np.nan, strategy='median')\napp_train_imputed = imp.fit_transform(app_train)\napp_train_imputed_df = pd.DataFrame(app_train_imputed, columns=app_train.columns)","a6894942":"app_test_imputed = imp.fit_transform(app_test)\napp_test_imputed_df = pd.DataFrame(app_test_imputed, columns=app_test.columns)","ba107183":"# Modello di \"Baseline\", fatto schiaffando tutto dentro senza alcuna remora\nfrom sklearn.linear_model import LogisticRegression\nlog_reg = LogisticRegression(C = 0.0001, solver='liblinear')\n\ntrain = app_train_imputed_df.copy()\ntrain = train.drop(columns = ['TARGET'])\ntrain_labels = app_train['TARGET']\n\n","12474424":"print(train.shape)\nprint(app_test.shape)","f763485e":"set(train.columns).difference(set(app_test.columns))","3fc8bdbe":"train = train.drop('DAYS_EMPLOYED_ANOM', axis=1)","8251d195":"train.shape","43104205":"log_reg.fit(train, train_labels)","2c3f5efc":"# uso il modello fittato per fare la previsione\n#log_reg.predict_proba(app_train_imputed_df.drop('TARGET', axis=1))\nlog_reg.predict_proba(app_test_imputed_df)[:,1]","32eeab36":"submit = app_test[['SK_ID_CURR']]","1599a554":"submit['TARGET'] = log_reg.predict_proba(app_test_imputed_df)[:,1]","b8581b65":"submit","004ad65b":"submit.to_csv('submission.csv', index=False)","859e50d7":"#from sklearn import svm\n#clf = svm.SVC(kernel='linear', C=1, probability=True).fit(X_train, y_train)\n#clf.score(X_test, y_test)","fbf91032":"# 1) Ripulire tutto il foglio in modo che si esegua completamente dall'inizio alla fine e scriva il file \"submission\"\n# 2) Anzich\u00e9 fittare il modello su tutto il df \"app_train\", usare train_test_split per splittare app_train in app_train_train e app_train_test\n# 3) Rifittare LogisticRegression usando app_train_train\n# 4) Usare score() per trovare lo score passando a score i dati app_train_test (e le relative label di TARGET)\n# 5) Ripetere usando SVC (Support Vector Classifier)\n# 6) RIpetere usando RandomForestClassifier\n# 7) Ripetere cambiando qualcuno degli iperparametri di SVC o di RFC (es. \"C\" in SVC, o max_depth in RFC)","f623a3da":"# Provo usando la random_forest\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Feature names\nfeatures = list(train.columns)\n\n# Make the random forest classifier\nrandom_forest = RandomForestClassifier(max_depth=4, n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)\n\n# Train on the training data\nrandom_forest.fit(train, train_labels)\n\n# Extract feature importances\nfeature_importance_values = random_forest.feature_importances_\nfeature_importances = pd.DataFrame({'feature': features, 'importance': feature_importance_values})\n\n# Make predictions on the test data\npredictions = random_forest.predict_proba(app_test_imputed_df)[:, 1]","800ba87b":"# Make a submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = predictions\n\n# Save the submission dataframe\nsubmit.to_csv('random_forest_baseline.csv', index = False)\nsubmit.to_csv('submission.csv', index = False)","17b3cdc1":"# Extract feature importances\nfeature_importance_values = random_forest.feature_importances_\nfeature_importances = pd.DataFrame({'feature': features, 'importance': feature_importance_values})\n# Uso delle feature importance\nfeature_importances = feature_importances.sort_values('importance', ascending=False)\nfeature_importances= feature_importances.set_index('feature')","ef05127e":"feature_importances","6acbc4a7":"feature_importances[:20].plot(kind=\"barh\", )","0c4aadef":"# 1) Rispetto allo scoring di baseline, provare con un modello con meno caratteristiche, nell'idea di privilegiare le caratteristiche con pi\u00f9 segnale\n# 2) Mergiare altri dati che sembrano promettenti, ripulirli se necessario, fittare e sottomettere il nuovo dataset per verificare lo score\n\n# 3) Ricchi premi e cotillions ai primi 3 score","11009feb":"* CREDIT_INCOME_PERCENT: the percentage of the credit amount relative to a client's income\n* ANNUITY_INCOME_PERCENT: the percentage of the loan annuity relative to a client's income\n* CREDIT_TERM: the length of the payment in months (since the annuity is the monthly amount due\n* DAYS_EMPLOYED_PERCENT: the percentage of the days employed relative to the client's age"}}