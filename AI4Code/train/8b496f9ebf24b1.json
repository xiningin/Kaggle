{"cell_type":{"91198387":"code","f69d8bd6":"code","82fd5443":"code","eaf623d4":"code","5caf862d":"code","03516dab":"code","5e4e9de0":"code","1dd6e494":"code","0bd3d81b":"code","a9843154":"code","b82cec0e":"code","5c24ec97":"code","129b402c":"code","c5eecbd8":"code","b63404a0":"code","21281115":"code","5898a056":"code","c89b3961":"code","8ac5069d":"markdown","f51d6e28":"markdown","29823d56":"markdown","1c26235c":"markdown","bf67896c":"markdown","74b8bacb":"markdown","702536d3":"markdown","6fa2f50e":"markdown","53b3e08f":"markdown","49333969":"markdown","0bb4edc2":"markdown","25d141e4":"markdown","1fed9c08":"markdown","ee52c1f8":"markdown","5337d336":"markdown","74046bdf":"markdown","729db1f1":"markdown","84790409":"markdown","681c113c":"markdown","db12ca29":"markdown","fd6be7d4":"markdown"},"source":{"91198387":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sn\nfrom sklearn.preprocessing import StandardScaler\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.utils import shuffle\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Reshape, Dense, Dropout, Conv1D, MaxPooling1D, Flatten\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import balanced_accuracy_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping\nfrom keras.optimizers.schedules import ExponentialDecay\nfrom keras.models import load_model\nfrom itertools import chain\n\npd.options.mode.chained_assignment = None  # default='warn'","f69d8bd6":"exoTrain = pd.read_csv('\/kaggle\/input\/kepler-labelled-time-series-data\/exoTrain.csv')\nexoTest = pd.read_csv('\/kaggle\/input\/kepler-labelled-time-series-data\/exoTest.csv')\n\nexoTrain.head(5)","82fd5443":"print(exoTrain['LABEL'].value_counts())\nprint(exoTest['LABEL'].value_counts())","eaf623d4":"def flux_graph(dataset, row, dataframe, planet):\n    if dataframe:\n        fig = plt.figure(figsize=(20,5), facecolor=(.18, .31, .31))\n        ax = fig.add_subplot()\n        ax.set_facecolor('#004d4d')\n        ax.set_title(planet, color='white', fontsize=22)\n        ax.set_xlabel('time', color='white', fontsize=18)\n        ax.set_ylabel('flux_' + str(row), color='white', fontsize=18)\n        ax.grid(False)\n        flux_time = list(dataset.columns)\n        flux_values = dataset[flux_time].iloc[row]\n        ax.plot([i + 1 for i in range(dataset.shape[1])], flux_values, '#00ffff')\n        ax.tick_params(colors = 'black', labelcolor='#00ffff', labelsize=14)\n        plt.show()\n    else:\n        fig = plt.figure(figsize=(20,5), facecolor=(.18, .31, .31))\n        ax = fig.add_subplot()\n        ax.set_facecolor('#004d4d')\n        ax.set_title(planet, color='white', fontsize=22)\n        ax.set_xlabel('time', color='white', fontsize=18)\n        ax.set_ylabel('flux_' + str(row), color='white', fontsize=18)\n        ax.grid(False)\n        flux_values = dataset[row]\n        ax.plot([i + 1 for i in range(dataset.shape[1])], flux_values, '#00ffff')\n        ax.tick_params(colors = 'black', labelcolor='#00ffff', labelsize=14)\n        plt.show()","5caf862d":"def show_graph(dataframe, dataset):\n    with_planet = exoTrain[exoTrain['LABEL'] == 2].head(3).index\n    wo_planet = exoTrain[exoTrain['LABEL'] == 1].head(3).index\n\n    for row in with_planet:\n        flux_graph(dataset, row, dataframe, planet = 'with_planet')\n    for row in wo_planet:\n        flux_graph(dataset, row, dataframe, planet = 'wo_planet')","03516dab":"show_graph(True, dataset = exoTrain.loc[:, exoTrain.columns != 'LABEL'])","5e4e9de0":"scaler = StandardScaler()\nscaled_data = scaler.fit_transform(exoTrain.loc[:, exoTrain.columns != 'LABEL'])\nshow_graph(False, scaled_data)","1dd6e494":"def handle_outliers(dataset, num_iterations):\n    \n    #threshold = None\n    dataset_handled = dataset\n\n    for n in range(num_iterations):\n        #for column in range(dataset_handled.shape[0]):\n        for index, row in dataset_handled.iterrows():\n            row_values = row.values\n            row_max, row_min = row_values.max(), row_values.min()\n            row_maxidx, row_minidx = row_values.argmax(), row_values.argmin()\n            row_mean = row_values.mean()\n\n            #if np.abs(column_max\/column_mean) >= threshold:\n            dataset_handled.iloc[index][row_maxidx] = row_mean\n\n            #if np.abs(column_min\/column_mean) >= threshold:\n            dataset_handled.iloc[index][row_minidx] = row_mean\n\n    return dataset_handled","0bd3d81b":"handled_dataset = handle_outliers(exoTrain.loc[:, exoTrain.columns != 'LABEL'], 2)\nshow_graph(True, handled_dataset)","a9843154":"def lable_change(y_train, y_test):\n    labler = lambda x: 1 if x == 2 else 0\n    y_train_01, y_test_01 = y_train.apply(labler), y_test.apply(labler)\n\n    return y_train_01, y_test_01","b82cec0e":"def smote(x_train, y_train):\n    #smote = SMOTE(random_state=17, sampling_strategy='minority')\n    over = SMOTE(sampling_strategy=0.2)\n    under = RandomUnderSampler(sampling_strategy=0.3)\n    steps = [('o', over), ('u', under)]\n    pipeline = Pipeline(steps=steps)\n    x_train_res, y_train_res = pipeline.fit_resample(x_train, y_train)\n\n    return x_train_res, y_train_res","5c24ec97":"y_smote_test = exoTrain.loc[:, 'LABEL']\nprint(y_smote_test.value_counts())\n_, y_smote_test = smote(handled_dataset, y_smote_test)\nprint(y_smote_test.value_counts())","129b402c":"# Define training and testing datasets\ndef datasets():\n    x_train, y_train = exoTrain.loc[:, exoTrain.columns != 'LABEL'], exoTrain.loc[:, 'LABEL']\n    x_test, y_test = exoTest.loc[:, exoTest.columns != 'LABEL'], exoTest.loc[:, 'LABEL']\n    \n    #fill NaNs with mean (no NaNs)\n    #for column in x_train:\n        #x_train[column] = x_train[column].fillna(round(x_train[column].mean(), 2))\n\n    #x_train, x_test = scale(x_train, x_test)\n    x_train = handle_outliers(x_train, 2)\n    x_train, y_train = smote(x_train, y_train)\n    y_train, y_test = lable_change(y_train, y_test)\n\n    n_features = x_train.shape[1]\n\n    return x_train, y_train, x_test, y_test, n_features","c5eecbd8":"# Graph train and test accuracy\ndef graph_acc(history):\n    # Plot loss during training\n    plt.subplot(211)\n    plt.title('Loss')\n    plt.plot(history.history['loss'], label='train')\n    plt.plot(history.history['val_loss'], label='test')\n    plt.legend()\n\n    # Plot accuracy during training\n    plt.subplot(212)\n    plt.title('Accuracy')\n    plt.plot(history.history['accuracy'], label='train')\n    plt.plot(history.history['val_accuracy'], label='test')\n    plt.legend()\n    plt.show()","b63404a0":"# Confusion matrix\ndef conf_matrix(y_test, y_pred):\n\n    matrix = confusion_matrix(y_test, y_pred)\n    df_cm = pd.DataFrame(matrix, columns=[0, 1], index = [0, 1])\n    df_cm.index.name = 'Truth'\n    df_cm.columns.name = 'Predicted'\n    plt.figure(figsize = (10,7))\n    sn.set(font_scale=1.4) \n    sn.heatmap(df_cm, cmap=\"BuGn\", annot=True, annot_kws={\"size\": 16})\n    plt.show()\n    \n    return matrix","21281115":"# Print prediction metrics\ndef prediction_metrics(y_test, y_pred, y_class_pred, matrix):\n    FP = matrix[0][1] \n    FN = matrix[1][0]\n    TP = matrix[1][1]\n    TN = matrix[0][0]\n\n    sens = TP\/(TP+FN)\n    spec = TN\/(TN+FP) \n    g_mean = np.sqrt(sens * spec)\n\n    accuracy = accuracy_score(y_test, y_class_pred)\n    balanced_accuracy = balanced_accuracy_score(y_test, y_class_pred)\n    precision = precision_score(y_test, y_class_pred)\n    recall = recall_score(y_test, y_class_pred)\n    f1 = f1_score(y_test, y_class_pred)\n    auc = roc_auc_score(y_test, y_pred)\n\n    print('\\t\\t Prediction Metrics\\n')\n    print(\"Accuracy:\\t\", \"{:0.3f}\".format(accuracy))\n    print(\"Precision:\\t\", \"{:0.3f}\".format(precision))\n    print(\"Recall:\\t\\t\", \"{:0.3f}\".format(recall))\n    print(\"\\nF1 Score:\\t\", \"{:0.3f}\".format(f1))\n    print(\"ROC AUC:\\t\", \"{:0.3f}\".format(auc))\n    print(\"Balanced\\nAccuracy:\\t\", \"{:0.3f}\".format(balanced_accuracy))\n    print(\"\\nSensitivity:\\t\", \"{:0.3f}\".format(sens))\n    print(\"Specificity:\\t\", \"{:0.3f}\".format(spec))\n    print(\"Geometric Mean:\\t\", \"{:0.3f}\".format(g_mean))","5898a056":"def cnn_model():\n\n    # Data preparation\n    x_train, y_train, x_test, y_test, n_features = datasets()\n    x_train, y_train = shuffle(x_train, y_train) # shuffle the data to avoid stagnant 0.0000e+00 val_accuracy\n\n    # Architecture\n    model = Sequential()\n    model.add(Reshape((3197, 1), input_shape=(3197,)))\n    model.add(Conv1D(filters=10, kernel_size=2, activation='relu', input_shape=(n_features, 1), kernel_regularizer='l2'))\n    model.add(MaxPooling1D(pool_size=2, strides=2))\n    model.add(Dropout(0.2))\n    model.add(Flatten())\n    model.add(Dense(48, activation=\"relu\"))\n    model.add(Dropout(0.4))\n    model.add(Dense(18, activation=\"relu\"))\n    model.add(Dense(1, activation=\"sigmoid\"))\n\n    # Representation of architecture\n    print(model.summary())\n\n    # Compile model\n    lr_schedule = ExponentialDecay(initial_learning_rate=1e-2, decay_steps=10000, decay_rate=0.94)\n\n    model.compile(optimizer = Adam(learning_rate=lr_schedule), loss='binary_crossentropy', metrics=['accuracy'])\n\n    # Fit model\n    early_stop = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)\n\n    history = model.fit(x_train, y_train, validation_split = 0.2, batch_size=64, callbacks=[early_stop], epochs=30, verbose=2)\n\n    # Evaluate the model\n    _, train_acc = model.evaluate(x_train, y_train, verbose=2)\n    _, test_acc = model.evaluate(x_test, y_test, verbose=2)\n    print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n\n    # Prediction\n    y_class_pred = (model.predict(x_test) > 0.5).astype(\"int32\")\n    y_pred = model.predict(x_test)\n\n    # Accuracy graph\n    graph_acc(history)\n\n    # Confustion matrix\n    matrix = conf_matrix(y_test, y_class_pred)\n\n    # Metrics\n    prediction_metrics(y_test, y_pred, y_class_pred, matrix)\n","c89b3961":"cnn_model()","8ac5069d":"### Function Capsule and Results","f51d6e28":"#### General","29823d56":"The score isn't perfect, but it's possible to achieve perfect score after multiple simulations. You can download pre-trained weights and code from my [Github repository](https:\/\/github.com\/ztsv-av\/exoplanet) and check out results yourself.\n\nHere's the result from the model that used pre-trained weights that I mentioned above:\n\n![image.png](attachment:ec4757a5-36e2-42d3-86b6-0b6e24a22a47.png)\n\nIf you have any comments or questions, let me know.","1c26235c":"### Model Structure","bf67896c":"Below you may see the head of the data and how unbalanced the data are, and also plots of FLUX values of stars with exoplanets and without them. These plots describe stars in terms of their characteristics (w\/ or without exoplanets), provide insight on data and illustrative information on outliers. Specifically, we may differentiate whether a star has exoplanets or not - the systems with exoplanets are sinusoid-looking whereas stars without exoplanets are more stable and condensed","74b8bacb":"\n### Data Study","702536d3":"Finally, let's define our datasets, confusion matrix and metrics functions, and create our model.","6fa2f50e":"#### Outliers","53b3e08f":"___","49333969":"## Main","0bb4edc2":"The peculiarity of this problem is that it has highly imbalanced training and test datasets. To deal with the problem, the SMOTE (oversampling) technique combined with random undersampling is used as per ([Chawla et al., 2002](https:\/\/arxiv.org\/pdf\/1106.1813.pdf)). As for the model, the following CNN architecture is used to achieve **perfect score (100%)**:\n\n* **Input layer**;\n* **1D convolutional layer**, consisting of 10 2x2 filters, L2 regularization and RELU activation function;\n* **1D max pooling layer**, window size - 2x2, stride - 2;\n* **Dropout** with 20% probability;\n* **Fully connected layer** with 32 neurons and RELU activation function;\n* **Dropout** with 40% probability;\n* **Fully connected layer** with 18 neurons and RELU activation function;\n* **Output layer** with sigmoid function.\n\nAs it is suggested in papers ([Hinton et al., 2021](https:\/\/arxiv.org\/pdf\/1207.0580.pdf), [Park & Kwak, 2016](http:\/\/mipal.snu.ac.kr\/images\/1\/16\/Dropout_ACCV2016.pdf)), we use 20% dropout after 1D CONV layers and 40-50% dropout after fully connected layers. In our model, we use **Adam** optimizer, **binary-crossentropy** loss function, **batch size of 64** and **30 epochs**. Also, we use **exponential decay** and **early stopping** to prevent non-convergence and overfitting.\n\nTraining on GPU involves a certain degree of randomness. On average, this model achieves a perfect score on $y=1$ (star has an exoplanet) around 20 times for every 200 simulations.","25d141e4":"The study on data and its FLUX values led us to conclude that the scaled data were less efficient than the raw data. Here, scaling affects slight differences observed on plots between FLUX values of stars with exoplanets and without them. We have trained the model on both scaled and unscaled data - the metrics were generally higher with the unscaled version. For educational purposes, we plot the same graphs using the data after scaling. As you can see below, scaling the data leads to degradation of distinction between stars with and without exoplanets","1fed9c08":"### Datasets and Metrics","ee52c1f8":"* **Input layer**;\n* **1D convolutional layer**, consisting of 10 2x2 filters, L2 regularization and RELU activation function;\n* **1D max pooling layer**, window size - 2x2, stride - 2;\n* **Dropout** with 20% probability;\n* **Fully connected layer** with 48 neurons and RELU activation function;\n* **Dropout** with 40% probability;\n* **Fully connected layer** with 18 neurons and RELU activation function;\n* **Output layer** with sigmoid function.\n\nIn our model, we use **Adam** optimizer, **binary-crossentropy** loss function, **batch size of 64** and **30 epochs**. Also, we use **exponential decay** and **early stopping**.","5337d336":"#### Scaling","74046bdf":"# Exoplanet Hunting","729db1f1":"#### SMOTE","84790409":"We begin by relabling the classes to a 0-1 (0 - with exoplanet(s), 1- without exoplanet(s)) convention and defining the function for SMOTE. We oversample only the minor part of data ($y=1$) and undersample the major part of the data ($y=0$). In numbers, ~2k observations of $y=0$ were removed and ~1k observations of $y=1$ were added","681c113c":"### Library Import\n\n* **numpy** and **pandas** are for data handling\n* **matplotlib** and **seaborn** are for plotting\n* **sklearn**, **imblearn** and **itertools** are for data processing and metric evaluation\n* **keras** and **tensorflow** are for fitting and predicting","db12ca29":"## Abstract","fd6be7d4":"The data is very rich in outliers so this fact must be treated appropriately. Since we have not applied scaling it is a delicate task to come up with a treshold for truncation. After numerous tests a good approach is to substitute the maximum FLUX value of each star with its mean. Below you may see a plot comparsion of data before and after outlier handling"}}