{"cell_type":{"4de07a64":"code","2e3347ae":"code","45bcab6e":"code","3fbaaf93":"code","fed6cbed":"code","d1c6a9d9":"code","3bdb627f":"code","e24bc217":"code","f7aa04d3":"code","1a9978e6":"code","c1c95433":"code","1dca2cdb":"code","9a3b8e17":"code","28a425ab":"code","0d074a1b":"code","9f0655b8":"code","8561c12a":"code","e28f90cb":"code","b6f3af86":"code","70a21faf":"code","328ebfdd":"code","bb219513":"code","e3c87aca":"code","e53b72bc":"code","b9e33cd1":"code","836f8326":"code","c34bf40a":"code","a6e6b40e":"code","505ea7ff":"code","d7d27140":"code","917ca8fe":"code","b518fdf2":"code","b4ac96d6":"code","344abcb2":"code","62a0e734":"code","f4cad7cf":"code","426e6381":"code","7ec2c930":"code","959cafe5":"code","11dd3bcb":"markdown","2c6c2cd6":"markdown","6a8e3332":"markdown","e36e6b7d":"markdown","2b575385":"markdown","f68ce1e6":"markdown","264b3a00":"markdown","6a855eed":"markdown","9ea7e905":"markdown","f5690744":"markdown","2882a493":"markdown","bfa737c5":"markdown","c793b8b8":"markdown","e5c9aa54":"markdown","b4f75bf3":"markdown","216de7a1":"markdown","fc7b64a5":"markdown","b91ed6bc":"markdown","19f73f7e":"markdown","ddbb7a16":"markdown","75b52ad9":"markdown","f4b848f8":"markdown","c770bcda":"markdown"},"source":{"4de07a64":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"your-secret-label\")","2e3347ae":"!pip install textfeatures","45bcab6e":"!pip install textstat","3fbaaf93":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn import datasets\nfrom sklearn import model_selection\nfrom pandas_profiling import ProfileReport\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport missingno as msno\nimport re\nimport string\nfrom pprint import pprint\nimport textstat\nimport textfeatures\nimport nltk\nfrom nltk.tokenize import sent_tokenize,word_tokenize\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.sparse import hstack, csr_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import LinearSVR\nfrom sklearn.metrics import mean_absolute_error\nfrom math import sqrt","fed6cbed":"train = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\nsample_submision = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')\n","d1c6a9d9":"train.head()","3bdb627f":"test.head()","e24bc217":"sample_submision.head()","f7aa04d3":"train.dtypes","1a9978e6":"#train = train.drop([\"url_legal\",\"license\"], axis=1)\n#test = test.drop([\"url_legal\",\"license\"], axis=1)","c1c95433":"msno.bar(train, sort=\"ascending\", figsize=(10,5), fontsize=12)\nplt.show()","1dca2cdb":"plt.figure(figsize=(12, 5))\nsns.histplot(train['target'])\nplt.title('Target distribution')\nplt.show()\nprint(train.target.describe())","9a3b8e17":"plt.figure(figsize=(12,5))\nplt.xlim(-5,2)\nplt.ylabel('target')\nsns.boxplot(x=train['target'])\nplt.show()\n","28a425ab":"plt.figure(figsize=(12, 5))\nsns.histplot(train['standard_error'])\nplt.title('Target distribution')\nplt.show()\nprint(train.standard_error.describe())","0d074a1b":"plt.figure(figsize=(12,5))\nplt.xlim(-0.05,0.7)\nplt.ylabel('standard_error')\nsns.boxplot(x=train['standard_error'])\nplt.show()","9f0655b8":"sns.jointplot(x=train['target'], y=train['standard_error'], kind='hex',height=8)\nplt.suptitle(\"Target vs Standard error\")\nplt.subplots_adjust(top=0.94)\nplt.show()","8561c12a":"sort_by_target = train.sort_values(['target'])\nsort_by_target_lowest = sort_by_target[['excerpt','target']].head(5)\nfor label, row in sort_by_target_lowest.iterrows():\n    print(row[\"excerpt\"][:400])\n    print(row[\"target\"])","e28f90cb":"sort_by_target_highest = sort_by_target[['excerpt','target']].tail(5)\nfor label, row in sort_by_target_highest.iterrows():\n    print(row[\"excerpt\"][:400])\n    print(row[\"target\"])","b6f3af86":"text = \" \".join(excerpt for excerpt in sort_by_target_lowest.excerpt)\n# Create stopword list:\nstopwords = set(STOPWORDS)\n\n\n# Generate a word cloud image\nwordcloud = WordCloud(stopwords=stopwords, background_color=\"white\").generate(text)\n\n# Display the generated image:\n# the matplotlib way:\nplt.figure(figsize=(10,10))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","70a21faf":"text = \" \".join(excerpt for excerpt in sort_by_target_highest.excerpt)\n# Create stopword list:\nstopwords = set(STOPWORDS)\n\n\n# Generate a word cloud image\nwordcloud = WordCloud(stopwords=stopwords, background_color=\"white\").generate(text)\n\n# Display the generated image:\n# the matplotlib way:\nplt.figure(figsize=(10,10))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","328ebfdd":"df_train = train.copy()\ndf_test = test.copy()","bb219513":"def feature_engineering(df):\n    df['character_count'] = df['excerpt'].apply(lambda x: len(str(x)))\n    df['digit_count'] = df['excerpt'].apply(lambda x: np.sum(([int(word.isdigit()) for word in str(x).split()])))\n    df['word_count'] = df['excerpt'].apply(textstat.lexicon_count)\n    df['unique_word_count'] = df['excerpt'].apply(lambda x: len(set(str(x).split())))\n    df['mean_word_length'] = df['excerpt'].apply(lambda x: np.mean([len(word) for word in str(x).split()]))\n    df['syllable_count'] = df['excerpt'].apply(textstat.syllable_count)\n    df['sentence_count'] = df['excerpt'].apply(textstat.sentence_count)\n    df['flesch_reading_ease'] = df['excerpt'].apply(textstat.flesch_reading_ease)\n    df['flesch_kincaid_grade'] = df['excerpt'].apply(textstat.flesch_kincaid_grade)\n    df['automated_readability_index'] = df['excerpt'].apply(textstat.automated_readability_index)\n    df['coleman_liau_index'] = df['excerpt'].apply(textstat.coleman_liau_index)\n    df['linsear_write_formula'] = df['excerpt'].apply(textstat.linsear_write_formula)\n    df['difficult_words']= df['excerpt'].apply(lambda x: textstat.difficult_words(x))\n    df['avg_sentence_length']= df['excerpt'].apply(lambda x: textstat.avg_sentence_length(x))\n    df['reading_time']=df['excerpt'].apply(lambda x: textstat.reading_time(x))\n    df['dc_readability_score'] = df['excerpt'].apply(lambda x: textstat.dale_chall_readability_score(x))\n\n    return df\n    \n    ","e3c87aca":"df_train = feature_engineering(df_train)\ndf_test = feature_engineering(df_test)","e53b72bc":"df_train.head()","b9e33cd1":"columns = ['character_count', 'digit_count', 'word_count', 'unique_word_count',\n       'mean_word_length', 'syllable_count', 'sentence_count',\n       'flesch_reading_ease', 'flesch_kincaid_grade',\n       'automated_readability_index', 'coleman_liau_index',\n       'linsear_write_formula', 'difficult_words', 'avg_sentence_length',\n       'reading_time', 'dc_readability_score']\n\ndf_temp = df_train[columns]","836f8326":"fig = plt.figure(figsize=(15, 15), dpi=100)\nmatrix = np.triu(df_train[columns + ['target']].corr())\nsns.heatmap(df_train[columns + ['target']].corr(), annot=True, mask=matrix)\nplt.title('New Features and Target Correlations', size=20, pad=20)","c34bf40a":"def plot_feature(feature):\n\n    fig, axes = plt.subplots(ncols=2, figsize=(32, 6))\n\n    sns.regplot(x=df_train['target'], y=df_train[feature], line_kws={'color': 'red'}, ax=axes[0])\n    sns.kdeplot(df_train[feature], fill=True, ax=axes[1])\n\n    axes[0].set_xlabel(f'target', size=18)\n    axes[0].set_ylabel(feature, size=18)\n    axes[1].set_xlabel('')\n    axes[1].set_ylabel('')\n    axes[1].legend(prop={'size': 15})\n    for i in range(2):\n        axes[i].tick_params(axis='x', labelsize=15)\n        axes[i].tick_params(axis='y', labelsize=15)\n    axes[0].set_title(f'target vs {feature}', size=20, pad=20)\n    axes[1].set_title(f'{feature} Distribution', size=20, pad=20)\n\n    plt.show()\n    \nfor feature in columns:\n    plot_feature(feature)","a6e6b40e":"textfeatures.clean(df_train,\"excerpt\",\"clean_excerpt\")\ntextfeatures.clean(df_test,\"excerpt\",\"clean_excerpt\")","505ea7ff":"#df_train['clean_excerpt']=df_train['clean_excerpt'].apply(lambda x: clean(x))\n#df_test['clean_excerpt']=df_test['clean_excerpt'].apply(lambda x: clean(x))","d7d27140":"df_train['clean_excerpt'] = df_train['clean_excerpt'].astype(str)\ndf_test['clean_excerpt'] = df_test['clean_excerpt'].astype(str)","917ca8fe":"# Stemming\nfrom nltk.stem.porter import PorterStemmer\n\ndef get_stemmed_text(corpus):\n    stemmer = PorterStemmer()\n    return [' '.join([stemmer.stem(word) for word in review.split()]) for review in corpus]\n\ndf_train['clean_excerpt'] = get_stemmed_text(df_train['clean_excerpt'])\ndf_test['clean_excerpt'] = get_stemmed_text(df_test['clean_excerpt'])","b518fdf2":"# Lemmatization\nfrom nltk.stem import WordNetLemmatizer\ndef get_lemmatized_text(corpus):\n    lemmatizer = WordNetLemmatizer()\n    return [' '.join([lemmatizer.lemmatize(word) for word in review.split()]) for review in corpus]\n\ndf_train['clean_excerpt'] = get_lemmatized_text(df_train['clean_excerpt'])\ndf_test['clean_excerpt'] = get_lemmatized_text(df_test['clean_excerpt'])","b4ac96d6":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(use_idf=True)\n\ntrain_v = vectorizer.fit_transform(df_train['clean_excerpt'])\ntest_v = vectorizer.transform(df_test['clean_excerpt'])","344abcb2":"\nscaler = StandardScaler()\n\n\nfor col in columns:\n    df_train[col] = scaler.fit_transform(df_train[col].values.reshape(-1, 1))\n    df_test[col] = scaler.transform(df_test[col].values.reshape(-1, 1))","62a0e734":"scaler = StandardScaler()\n\nfor col in columns:\n    df_train[col] = scaler.fit_transform(df_train[col].values.reshape(-1, 1))\n    df_test[col] = scaler.transform(df_test[col].values.reshape(-1, 1))","f4cad7cf":"train_X1 = df_train[columns]\ntest_X1 = df_test[columns]","426e6381":"train_X_Title = hstack([train_v, csr_matrix(train_X1.values)])\ntest_X_Title = hstack([test_v, csr_matrix(test_X1.values)])\ny1 = train['target']","7ec2c930":"# LinearSVR model\n\nX_train, X_test, y_train, y_test = train_test_split(train_X_Title, y1, test_size=0.20, random_state=42)\n\nclf1 = LinearSVR(C=0.2)\nclf1.fit(X_train, y_train)\n\ny_pred1 = clf1.predict(X_test)\nmae1 = mean_absolute_error(y_pred1, y_test)\nprint('MAE:', sqrt(mae1))","959cafe5":"pred = clf1.predict(test_X_Title)\ntest['target'] = pred\ntest.drop(['url_legal','license'],axis=1,inplace=True)\ntest.drop(['excerpt'],axis=1,inplace=True)\ntest.reset_index(drop=True, inplace=True)\ntest.head()\ntest.to_csv('output.csv')","11dd3bcb":"<font size=\"3\">I am using [missingno](http:\/\/pypi.org\/project\/missingno\/) library for visualizing missing values.<\/font>\n","2c6c2cd6":"<font size=\"3\">We can infer from the above graphs that :<\/font>\n1. Most of the values are less then 0.\n2. There are no outliers in the column.","6a8e3332":"<font size=\"3\">We will try to check the values for excerpt when the target is lowest and highest.<\/font>","e36e6b7d":"# Missing values","2b575385":"# Pre-Processing and Feature Engineering","f68ce1e6":"<font size=\"3\">Wordcloud for target lowest till 5 rows each<\/font>","264b3a00":"# Pre-Processing","6a855eed":"Tokenization","9ea7e905":"Excerpt Values for Highest Target Values","f5690744":"Normalization","2882a493":"# Data Exploration","bfa737c5":"Excerpt Values for Lowest Target Values","c793b8b8":"We will add a new feature called [Dale\u2013Chall readability formula](http:\/\/https:\/\/en.wikipedia.org\/wiki\/Dale%E2%80%93Chall_readability_formula).\nThe Dale\u2013Chall readability formula is a readability test that provides a numeric gauge of the comprehension difficulty that readers come upon when reading a text. It uses a list of 3000 words that groups of fourth-grade American students could reliably understand, considering any word not on that list to be difficult.\n\nYou will be able to find more about the formula in the hyperlink.\n\nWe will add some other features also.","e5c9aa54":"# Visualization","b4f75bf3":"<h1 id=\"heading\">\n\n<a class=\"anchor-link\" href=\"https:\/\/www.kaggle.com\/deb009\/commonlit-readability-prize-eda\/notebook#heading\">\u00b6<\/a>\n<\/h1>","216de7a1":"We will try to figure out the collinearity of the columns which we have created now.\n\n","fc7b64a5":"<font size=\"3\"> So, from the above two cells we are able to figure out that the target is inversly propotional to complexity of the sentences<\/font>","b91ed6bc":"clean() function of textfeatures can be used to clean the document.","19f73f7e":"<font size=\"3\">It looks like standard error column has outliers around 0 and more than 0.5.<\/font>\n","ddbb7a16":"# Wordcloud","75b52ad9":"Only flesch_reading_grade and dc_readability_score has more than 50 percent coorelation with target.We will try to plot each column with target and see there distribution.","f4b848f8":"Standard Scalar","c770bcda":"<font size=\"3\">Wordcloud for highest target till 5 rows each<\/font>"}}