{"cell_type":{"4a62973a":"code","04ebb0c6":"code","ec9303cc":"code","4ee8c236":"markdown","317cc7ff":"markdown","da971b82":"markdown","abe4094b":"markdown","73216b2e":"markdown","670fdafd":"markdown","abad2a8e":"markdown"},"source":{"4a62973a":"import pandas as pd\nimport numpy as np\nimport warnings\nimport time\nwarnings.filterwarnings(\"ignore\")\nimport lightgbm as lgb\nfrom bayes_opt import BayesianOptimization\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split","04ebb0c6":"train = pd.read_csv('..\/input\/train.csv')\ntrain, extratrain = train_test_split(train, test_size=0.2, random_state=0)","ec9303cc":"X = train.drop('target', axis=1).drop('ID_code', axis=1)\ny = train.target\ndef bayes_parameter_opt_lgb(X, y, init_round=15, opt_round=25, n_folds=5, random_seed=6, n_estimators=10000, output_process=False):\n    # prepare data\n    train_data = lgb.Dataset(data=X, label=y, free_raw_data=False)\n    # parameters\n    def lgb_eval(num_leaves, feature_fraction, bagging_fraction, min_split_gain, min_child_weight, learning_rate, num_threads, min_data_in_leaf, min_sum_hessian_in_leaf):\n        # fixed parameters\n        params = {'application':'binary',\n                  'num_iterations': n_estimators,\n                  'learning_rate':learning_rate,\n                  'early_stopping_round':100,\n                  'metric':'auc',\n                  'max_depth':-1,\n                  'bagging_freq':7,\n                  'verbosity':-1}\n        # variables\n        params[\"num_leaves\"] = int(round(num_leaves))\n        params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n        params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n        params['learning_rate'] = learning_rate\n        params['min_split_gain'] = min_split_gain\n        params['min_child_weight'] = min_child_weight\n        params['num_threads'] = int(num_threads)\n        params['min_data_in_leaf'] = int(min_data_in_leaf)\n        params['min_sum_hessian_in_leaf'] = min_sum_hessian_in_leaf\n        cv_result = lgb.cv(params, train_data, nfold=n_folds, seed=random_seed, stratified=True, verbose_eval =200, metrics=['auc'])\n        return max(cv_result['auc-mean'])\n    # range of variables\n    lgbBO = BayesianOptimization(lgb_eval, {'num_leaves': (22, 50),\n                                            'feature_fraction': (0.01, 0.9),\n                                            'bagging_fraction': (0.8, 1),\n                                            'min_split_gain': (0.001, 0.1),\n                                            'min_child_weight': (5, 50),\n                                            'learning_rate': (0.001, 0.01),\n                                            'num_threads': (6, 10),\n                                            'min_data_in_leaf': (60, 100),\n                                            'min_sum_hessian_in_leaf': (5.0 , 15.0)},\n                                             random_state=0)\n    # optimize!\n    lgbBO.maximize(init_points=init_round, n_iter=opt_round)\n    \n    # output optimization process\n    if output_process==True: lgbBO.points_to_csv(\"bayes_opt_result.csv\")\n    \n    # return best parameters\n    return lgbBO.res\nopt_params = bayes_parameter_opt_lgb(X, y, init_round=15, opt_round=25, n_folds=5, random_seed=6, n_estimators=100) #15&25","4ee8c236":"Remember, if you choose bigger numbers for init_round, opt_round, n_estimators, you will get better results though it takes more time. You also can set other params easily.","317cc7ff":"If there are any problems, I appreciate it if you inform me of it. Thanks!","da971b82":"Thank you for seeing my humble kernel! Hope your success!","abe4094b":"Because Santandar competition welcomes many beginners including myself, I share this kernel. As I referred to this kernel (https:\/\/www.kaggle.com\/sz8416\/simple-bayesian-optimization-for-lightgbm), you may get more info there.","73216b2e":"## **Tuning hyperparameters for LightGBM by Bayesian Optimization**","670fdafd":"If you split data, you can calculate faster. Accuracy would not get so worse.","abad2a8e":"Basically, purple params indicate the best score (you can see target score is strengthened compared to past ones). Therefore under the given condition, purple param which have largest iter number is the best."}}