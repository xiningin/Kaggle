{"cell_type":{"a63459a1":"code","23b86b96":"code","504257ec":"code","c5eb5aa7":"code","cbc8c951":"code","5d7e3e6e":"code","cbd1a1d6":"code","a1f7656e":"code","709646ec":"code","3b61002b":"code","daedc148":"code","bb4aad98":"code","4a73c5d5":"code","1ecddb38":"code","9515082e":"code","c98421c6":"code","5d4c7278":"code","1e2b27ed":"code","b06b1a86":"code","22d5d037":"code","d117f7cc":"code","50f107c5":"code","c9131011":"code","08118867":"markdown","420bd18c":"markdown","11f01536":"markdown","df3ba886":"markdown","fa5defa4":"markdown","03e7bb60":"markdown","929a727a":"markdown","73665dfd":"markdown","28e5a35c":"markdown","0c2c8f71":"markdown","eacd519e":"markdown","7385061c":"markdown","58a28421":"markdown","b31d3b3d":"markdown","94c99023":"markdown","f6372e33":"markdown","18b89a2d":"markdown","cdc9fbda":"markdown","9b402204":"markdown","009abf29":"markdown","c7bc6856":"markdown","e6b42edd":"markdown","389310b4":"markdown","e96d860b":"markdown","4d386151":"markdown","d91d806a":"markdown","6aa080b6":"markdown"},"source":{"a63459a1":"\nimport seaborn as sns\nimport random\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport cv2 as cv\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\n\npaths_normal = []\npaths_pneumonia = []\n\nimport os\nfor dirname, _, filenames in os.walk(\"..\/input\/chest-xray-pneumonia\/chest_xray\/train\/NORMAL\/\"):\n    for filename in filenames:\n        paths_normal.append(os.path.join(dirname, filename))\n    \nimport os\nfor dirname, _, filenames in os.walk(\"..\/input\/chest-xray-pneumonia\/chest_xray\/train\/PNEUMONIA\/\"):\n    for filename in filenames:\n        paths_pneumonia.append(os.path.join(dirname, filename))\n    \n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\nimport tensorflow as tf","23b86b96":"label_normal = np.zeros((500,1))\nlabel_pneumonia = np.ones((500,1))\nlabel = list(np.concatenate((label_normal,label_pneumonia),axis = 0));\npaths = paths_normal[0:500] + paths_pneumonia[0:500]\n","504257ec":"d = {'paths': paths, 'label': label\n    }\ndf = pd.DataFrame(data=d)","c5eb5aa7":"\n\nX = np.zeros((1,100*100),np.uint8)\ny = np.zeros((1,1),np.uint8)\nfor count,ele in enumerate (df.iloc[:,0],0): \n    y_temp = df.iloc[count,1]\n    y = np.vstack((y,y_temp))\n    X_temp = cv.imread(ele,cv.IMREAD_GRAYSCALE) \n    X_temp = cv.resize(X_temp,(100,100)).reshape(1,100*100)\n    X = np.vstack((X,X_temp))\n    print(\"progression : %{}\".format((count\/10)))\n    if count\/10 >= 99.9:\n        print(\"Done\")\nX = X[1:,:]\ny = y[1:,:]\n        \n\n","cbc8c951":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n","5d7e3e6e":"plt.figure(figsize=(15,15))\nfor count,i in enumerate(range(0,6),231):\n    \n    plt.subplot(count)\n    if y_train[i]==1:\n        plt.title(\"Pneumonia\")\n        plt.imshow(X_train[i,:].reshape(100,100),'gray')\n        \n    elif y_train[i]==0:\n        plt.title(\"Normal\")\n        plt.imshow(X_train[i,:].reshape(100,100),'gray')\nplt.show()        ","cbd1a1d6":"isnan_train = np.isnan(X_train).all()\nisnan_test = np.isnan(X_val).all()\nprint(isnan_train,isnan_test)\n\n","a1f7656e":"X_train,X_val = X_train[:,:]\/255, X_val[:,:]\/255\n\n","709646ec":"#Model\nmodel = tf.keras.Sequential()\nmodel.add(Dense(units = 784\/2, activation = 'relu', input_dim=X_train.shape[1]))\nmodel.add(Dense(units = 784\/4, activation = 'relu'))\nmodel.add(Dense(units = 784\/8, activation = 'relu'))\nmodel.add(Dense(units = 784\/16, activation = 'relu'))\nmodel.add(Dense(units = 784\/32, activation = 'relu'))\nmodel.add(Dense(units = 1, activation = 'sigmoid'))","3b61002b":"model.compile(loss=\"binary_crossentropy\",optimizer=\"sgd\", metrics = ['accuracy'])\n","daedc148":"model.fit(X_train, y_train, batch_size=20, epochs=90)","bb4aad98":"#Making Predictions on Test data\npredicted = model.predict(X_val)\ny_head_ann = [0 if i<0.5 else 1 for i in predicted]","4a73c5d5":"\nprint(accuracy_score(y_val, y_head_ann))\ncm_ann = confusion_matrix(y_val,y_head_ann)\nsns.heatmap(cm_ann, annot=True) ;","1ecddb38":" \n\n#Initialising the CNN\ncnn = Sequential()\ncnn.add(layers.Conv2D(filters=64,kernel_size=3,activation='relu',input_shape=[100,100,1]))  # 1 is our canal number it is just 1 because we use grayscale data\ncnn.add(layers.Conv2D(filters=64,kernel_size=3,activation='relu'))\n\n#Pooling\ncnn.add(layers.MaxPool2D(pool_size=2,strides=2)) #I preffered Max Pooling for this model\ncnn.add(Dropout(0.2))\n\n#Second Layer\ncnn.add(layers.Conv2D(filters=64,kernel_size=3,activation='relu'))\ncnn.add(layers.Conv2D(filters=64,kernel_size=3,activation='relu'))\ncnn.add(layers.MaxPool2D(pool_size=2,strides=2))\ncnn.add(Dropout(0.2))\n\n\n\n#Flattening and bulding ANN\n\ncnn.add(Flatten())\ncnn.add(Dense(64, activation = \"relu\"))\ncnn.add(Dense(32, activation = \"relu\"))\ncnn.add(Dropout(0.5))\ncnn.add(Dense(1, activation = \"sigmoid\")) \n","9515082e":"# Now we need to choose loss function, optimizer and compile the model\ncnn.compile(optimizer=\"adam\",loss='binary_crossentropy',metrics=['accuracy'])\n","c98421c6":"X_train = X_train.reshape(-1,100,100,1)\nX_val = X_val.reshape(-1,100,100,1)","5d4c7278":"\n\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  \n        samplewise_center=False, \n        featurewise_std_normalization=False,  \n        samplewise_std_normalization=False,  \n        zca_whitening=False,  \n        rotation_range=10, \n        zoom_range = 0.1, \n        width_shift_range=0.1,  \n        height_shift_range=0.1,  \n        horizontal_flip=False,  \n        vertical_flip=False)  \n\n\ndatagen.fit(X_train)","1e2b27ed":"cnn.fit_generator(datagen.flow(X_train,y_train, batch_size=20),epochs = 90, validation_data = (X_val,y_val),verbose = 1,steps_per_epoch=len(X_train) \/\/ 20)","b06b1a86":"predicted = cnn.predict(X_val)\n\ny_head_cnn = [0 if i<0.5 else 1 for i in predicted]","22d5d037":"\nprint(accuracy_score(y_val, y_head_cnn))\ncm_cnn = confusion_matrix(y_val,y_head_cnn)\nsns.heatmap(cm_cnn, annot=True) ;","d117f7cc":"plt.figure(figsize=(5, 5))\nplt.subplot(221)\nplt.title(\"ANN Confusion Matrix\")\nsns.heatmap(cm_ann, annot=True) ;\n\nplt.subplot(222)\nplt.title(\"CNN Confusion Matrix\")\nsns.heatmap(cm_cnn, annot=True) ;\nplt.show()","50f107c5":"paths_normal_test = []\npaths_pneumonia_test = []\n\nimport os\nfor dirname, _, filenames in os.walk(\"..\/input\/chest-xray-pneumonia\/chest_xray\/train\/NORMAL\/\"):\n    for filename in filenames:\n        paths_normal_test.append(os.path.join(dirname, filename))\n    \nimport os\nfor dirname, _, filenames in os.walk(\"..\/input\/chest-xray-pneumonia\/chest_xray\/train\/PNEUMONIA\/\"):\n    for filename in filenames:\n        paths_pneumonia_test.append(os.path.join(dirname, filename))\n\nlabel_normal_test = np.zeros((500,1))\nlabel_pneumonia_test = np.ones((500,1))\nlabel_test = list(np.concatenate((label_normal_test,label_pneumonia_test),axis = 0));\npaths_test = paths_normal_test[0:500] + paths_pneumonia_test[0:500]\n\nd = {'paths': paths_test, 'label': label_test\n    }\ndf_test = pd.DataFrame(data=d)\n\nX = np.zeros((1,100*100),np.uint8)\ny = np.zeros((1,1),np.uint8)\nfor count,ele in enumerate (df.iloc[:,0],0): \n    y_temp = df.iloc[count,1]\n    y = np.vstack((y,y_temp))\n    X_temp = cv.cvtColor(cv.imread(ele),cv.COLOR_BGR2GRAY)  \n    X_temp = cv.resize(X_temp,(100,100)).reshape(1,100*100)\n    X = np.vstack((X,X_temp))\n    print(\"progression : %{}\".format((count\/10)))\n    if count\/10 >= 99.9:\n        print(\"Done\")\nX_test = X[1:,:]\ny_test = y[1:,:]\n        \nX_test = X_test[:,:]\/255\nX_test = X_test.reshape(-1,100,100,1)","c9131011":"predicted = cnn.predict(X_test)\ny_head = [0 if i<0.5 else 1 for i in predicted]\n\nprint(accuracy_score(y_test, y_head))\ncm = confusion_matrix(y_test,y_head)\nsns.heatmap(cm, annot=True) ;","08118867":"<a id=\"12\"><\/a> <br>\n## Predictions\n**\"predict\" method will be use  to make predictions. This method takes  the data we want to test as parameter. And will return float numbers between 0 and 1 which means \"normal\" and \" pneumonia\". Therefore,  we need to round the returned value to closest integer.**\n","420bd18c":"<a id=\"7\"><\/a> <br>\n## Scaling\n**We need  to normalize the data otherwise some of them  will perform superiority on others. This is something we don't want.**","11f01536":"<a id=\"20\"><\/a> <br>\n# Conculison\n**As a result, we can see that our CNN model was more succesful to classify X-ray chest images. So we should use the CNN model to predict the rest of the data.**","df3ba886":"<a id=\"16\"><\/a> <br>\n## Compiling\n","fa5defa4":"<a id=\"2\"><\/a> <br>\n  # Data Pre-Processing\n**Since the data unlabeled we need to label them by ourselves. So I will use 0 for normal images and 1 for Pneumonia. 1000 image will be enough to train.\nHere, I created (500,1) shaped zeros and ones and concateneted the by rows.** **And also stored the first 500 paths for each label. \nNow I have 500-500 normal-pnemonia images and labels one after another.**\n  ","03e7bb60":"<a id=\"19\"><\/a> <br>\n## Evaluation","929a727a":"<a id=\"3\"><\/a> <br>\n## Resizing\n**We have multi various sized images. We can't process them without resizing. \nI resized them into 100x100 so we will have 10000 pixels for each image.**\n**I also flattened them into lines and stacked them vertically so I can use it for my ANN model.\nLater that I will reshape it for my CNN model.**","73665dfd":"<a id=\"21\"><\/a> <br>\n## Compare The Results\nWe stored our confusion matrices in different variables so we can compare eachother.","28e5a35c":"<a id=\"6\"><\/a> <br>\n## Nan Check\n**This is the part we check whether we have Nan data.**","0c2c8f71":"<a id=\"15\"><\/a> <br>\n## Building\n**Structure of CNN model will be like  (Conv2D->relu -> MaxPool2D -> Dropout)x2 -> Flatten -> Dense -> Dropout -> Out**\n\n\n**Again, CNN will start with \"Sequential\". Conv2D takes 3D array as input shape so we need to reshape our data. Since I resized my data into (1,10000), 100x100 will be fine as new shape. I will do that reshaping later. I will choose the hyper paramaters such as optimizer,loss function,activation,same as in the ann so we can compare.**","eacd519e":"<a id=\"18\"><\/a> <br>\n## Predicton","7385061c":"<a id=\"5\"><\/a> <br>\n## Visualization\n**Let's make some visualization**","58a28421":"<a id=\"13\"><\/a> <br>\n## Evaluation\n**Accuracy is a good metric for evaluation but, will not be enough to understand  whether our model overfitted or not. We can use confusion matrix to understand it well.**","b31d3b3d":"<a id=\"4\"><\/a> <br>\n## Splitting\n**So far, we have 500 normal and 500 pneumonia images and labels one after another. We need to mix them to avoid overfitting. Also we will be splitted the data into test and validation.**","94c99023":"<a id=\"22\"><\/a> <br>\n## Prediction On Test Data\n**We need to pre process the test data in same way we did before.**","f6372e33":"<a id=\"16\"><\/a> <br>\n## Reshaping\n **We need to reshape our 10000 pixels to (100,100 for each) image.**","18b89a2d":" #                 Introduction\n    \n**Pneumonia is  the filling of air vesicles in the lung with an inflamed fluid. Viruses, bacteria, and rarely fungal infections cause it. Pneumonia can be diagnosed by examinening the X-Ray chest radiography by doctors. We will do it instead of doctors this time.**\n     \n<font color = 'red'>   \n   ## Content\n   \n1. [Importing The Necessary Libraries](#1)\n    \n2. [Data Pre-Processing](#2)\n    *     [Resizing](#3)\n    *     [Splitting](#4)\n    *     [Visualisation](#5)\n    *     [Nan Check](#6)\n    *     [Scaling](#7)\n    \n    \n3. [The ANN Model](#8)\n \n    *     [Building](#9)\n    *     [Compiling](#10)\n    *     [Fitting](#11)\n    *     [Predictions](#12)\n    *     [Evaluation](#13)\n    \n    \n4. [The CNN Model](#14)\n    \n    *     [Building](#15)\n    *     [Compiling](#16)\n    *     [Fitting](#17)\n    *     [Predictions](#18)\n    *     [Evaluation](#19)\n    \n    \n    \n5. [Conclusion](#20)\n    \n    *     [Compare The Results](#21)\n    *     [Prediction On Test Data](#22)\n    *     [Evaluation](#23)\n    \n    \n    ","cdc9fbda":"<a id=\"1\"><\/a> <br>\n## Importing The Necessary Libraries\n<br>\n\n**Also I will create two lists for keeping the paths.**","9b402204":"<a id=\"23\"><\/a> <br>\n## Evaluation","009abf29":"<a id=\"8\"><\/a> <br>\n# The ANN Model\n\n**We start our model with Sequential. I added lots of layers to increase the accuracy. Numbers of neurons pretty intuitive so, you need to try for your own model. Since our output is 0 or 1, that means our output binary. So I chose 1 neuron and sigmoid activation function as an activator.** ","c7bc6856":"<a id=\"17\"><\/a> <br>\n## Fitting\n**We will use fit_generator method since we did data augmentation**","e6b42edd":"<a id=\"16\"><\/a> <br>\n## Data Augmentation\n**Data Augmentation is a process we can make some manipulation on images such as rotating, zoom in,zoom out,shifting etc. It is important to avoid overfitting**","389310b4":"<a id=\"14\"><\/a> <br>\n# The CNN Model\n**I want to compare the results between ANN and CNN so let's start to building the CNN model.**","e96d860b":"<a id=\"9\"><\/a> <br>\n## Building","4d386151":"**I created a data frame  and stored my paths and labels there. Reason I did it, I can reach the correct label for each image by using same index.**","d91d806a":"<a id=\"10\"><\/a> <br>\n## Compiling\n**Now, we need to choose a loss function, an optmizer function and a evaluation metric.\nOutput is binary which is 0 or 1 so our loss function should be \"binary_crossentropy\". \"Adam\" and \"sgd\" are the most used optimizers for binary classification. I will use \"sgd\" since our data is not complex. You can read some extra documents in order to understand how to choose optimizer. \"accuracy\" is a good way to evaluate the models mostly.**","6aa080b6":"<a id=\"11\"><\/a> <br>\n## Fitting\n**We use \"fit\" method except we make \"Data Augmentation\". We need to specify the batch size and epochs here. Again, these are hyper parameters so, you should find the best values by trying them or you can use gridsearchcv.**"}}