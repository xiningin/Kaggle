{"cell_type":{"58d33a13":"code","f40522f9":"code","e974c5b9":"code","5036e0ac":"code","82cfe1f2":"code","543f029a":"code","e285dc52":"code","445a48eb":"code","d44c2d2f":"code","987c1c8d":"code","0879f070":"code","341f1775":"code","06b2297e":"code","f274dca6":"code","b9e8344a":"code","213c80ef":"code","67296707":"code","17d7b988":"code","278e95bc":"code","90880b3e":"code","5d41a986":"code","c2f80fa8":"code","57d2ac44":"code","4f2c8b94":"code","10273ce4":"code","b33a807e":"code","1ba026ba":"code","4f0aa2f5":"code","0e054942":"code","58f5bc6e":"code","b567a01e":"code","f2c27923":"code","f932f972":"code","3404bae6":"markdown","a207db33":"markdown","3585be76":"markdown","570dba68":"markdown","f66bae83":"markdown","c44ffc50":"markdown","7cd2f4d5":"markdown","a99391d9":"markdown","ca770ebb":"markdown","edae3f1e":"markdown","9d358fbf":"markdown","7d484398":"markdown","4228a336":"markdown","6d8c549b":"markdown","352111e8":"markdown","c9a10902":"markdown","b74d8348":"markdown","6418463f":"markdown","de9db18a":"markdown","46df5c1f":"markdown","a26e533e":"markdown","bad72609":"markdown","9f41424c":"markdown","6b7b1c06":"markdown","81a81077":"markdown","cbca85ad":"markdown","ec08a2e1":"markdown","128b8d46":"markdown","fdbe97b7":"markdown","7b9b2480":"markdown","f8a8a3ca":"markdown","6db78aa2":"markdown","9f5e3659":"markdown","61010d73":"markdown","9367e9fe":"markdown","76a85f6c":"markdown","8e045c66":"markdown","d67a14c3":"markdown","96f2e6db":"markdown","4eae4999":"markdown","c0551911":"markdown","5e41be4a":"markdown","d5bf5a85":"markdown","934e64a9":"markdown","f90aeb60":"markdown","fc618556":"markdown"},"source":{"58d33a13":"!pip install bs4\nimport os\nimport time\nimport sys\nimport numpy as np\nimport pandas as pd\nimport regex as re\nimport lxml\nimport numbers\nfrom bs4 import BeautifulSoup\nimport requests\nimport json\n\nreq_headers = {\n    'accept': 'text\/html,application\/xhtml+xml,application\/xml;q=0.9,image\/webp,image\/apng,*\/*;q=0.8',\n    'accept-encoding': 'gzip, deflate, br',\n    'accept-language': 'en-US,en;q=0.8',\n    'upgrade-insecure-requests': '1',\n    'user-agent': 'Mozilla\/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/61.0.3163.100 Safari\/537.36'\n}","f40522f9":"page_url = 'https:\/\/content.fortune.com\/wp-json\/irving\/v1\/data\/franchise-search-results?list_id=2814606&token=Zm9ydHVuZTpCcHNyZmtNZCN5SndjWkkhNHFqMndEOTM='","e974c5b9":"# request the URL and parse the JSON\nresponse = requests.get(page_url)\nresponse.raise_for_status() # raise exception if invalid response\ncomps = response.json()\nprint(len(comps))","5036e0ac":"comp = comps[1]\nprint(len(comp))","82cfe1f2":"# prettify json\nformat_json = json.dumps(comp, sort_keys=True, indent=5)\nprint(format_json[:1000])","543f029a":"# select the third company\nn_comp = comp['items'][2]\nprint(n_comp)","e285dc52":"n_comp.keys()","445a48eb":"new_comp = n_comp['permalink']\nprint(new_comp)","d44c2d2f":"my_comp = n_comp['fields']\nprint(my_comp)","987c1c8d":"fields = []\n\nfor j in comp['items']:\n    \n    #for key, value in comp:\n    field = j['fields']\n    fields.append(field)\n        \nprint(len(fields))","0879f070":"df = pd.DataFrame()","341f1775":"def gather_data(name, col):\n\n    lst = []\n    val = \"\"\n    for i in fields:\n        for j in i:\n            if j['key'] == name:\n                val = j['value']\n            else:\n                pass\n        \n        lst.append(val)\n        \n    df[col] = lst\n    \n    return df","06b2297e":"keys = ['name', 'rank', 'rankchange', 'f500_revenues', 'f500_profits', \n        'f500_ employees', 'sector', 'hqcity', 'hqstate', 'newcomer', \n        'ceofounder', 'ceowoman', 'profitable']\n\ncols = ['company', 'rank', 'rank_change', 'revenue', 'profit',\n        'num. of employees', 'sector', 'city', 'state', 'newcomer', \n        'ceo_founder', 'ceo_woman', 'profitable']\n\n\nfor x,y in zip(keys, cols):\n    f500_df = gather_data(x, y)","f274dca6":"print(f500_df.shape)\nf500_df.head()","b9e8344a":"f500_df[['rank', 'rank_change', 'revenue', 'profit','num. of employees']] = f500_df[['rank', 'rank_change', 'revenue','profit','num. of employees']].apply(pd.to_numeric)","213c80ef":"# fill missing values with zero\nf500_df['rank_change'] = f500_df['rank_change'].fillna(0)","67296707":"print(f500_df.dtypes)","17d7b988":"# Create new features\nf500_df['prev_rank'] = (f500_df['rank'] + f500_df['rank_change'])\nf500_df.head()","278e95bc":"# newcommer prev_rank\nf500_df.loc[(f500_df['newcomer'] == 'yes')| (f500_df['rank']>499), 'prev_rank'] = \" \"         ","90880b3e":"f500_df.head()","5d41a986":"urls = []\n\nfor i in comp['items']:\n    \n    #for key, value in comp:\n    url = i['permalink']\n    urls.append(url)\n        \nprint(len(urls))","c2f80fa8":"def soups(data):\n    r = requests.get(data).text\n    # select the the JSON from the html\n    html = r[r.find('__PRELOADED_STATE__ = ') + len('__PRELOADED_STATE__ = '):]\n    html = html[:html.find('};') + 1]\n    \n    # find company-information\n    good_data = html[html.find('\"company-information\",\"config\":') + len('\"company-information\",\"config\":'):]\n    good_data = good_data[:good_data.find('},') + 1]\n    \n    # convert to json\n    j = json.loads(good_data)\n    \n    return j","57d2ac44":"# Call soup function and store output in a list\nweb = []\n\nfor url in urls:\n    htmls = soups(url)\n    web.append(htmls)\nprint(len(web))","4f2c8b94":"print(web[1])","10273ce4":"m_df = pd.DataFrame()","b33a807e":"def get_data(name, col):\n\n    lst = []\n    val = \"\"\n    for i in web: \n        for key, value in i.items():\n            if key == name:\n                val = value\n            else:\n                pass\n        \n        lst.append(val)\n        \n    m_df[col] = lst\n    \n    return m_df","1ba026ba":"keys = ['ceo','website', 'ticker','marketValue']\n\ncols = ['CEO', 'Website', 'Ticker','Market Cap']\n\n\nfor x,y in zip(keys, cols):\n    m_df = get_data(x, y)\n    #n_df = get_data(x, y)","4f0aa2f5":"m_df.head()","0e054942":"# join\nf1000_df = f500_df.join(m_df, how='left')","58f5bc6e":"print(f1000_df.shape)\nf1000_df.head()","b567a01e":"f1000_df = f1000_df.sort_values(by='rank', ignore_index=True)","f2c27923":"f1000_df.head()","f932f972":"f1000_df.to_csv(\"Fortune_1000.csv\", index=False)","3404bae6":"# 3. Crawl Company Urls","a207db33":"### Another Roadblock\n\nInterestingly, the data we want cannot be scraped by simply looking at the developer window and calling the elements representing the corresponding values. You can see in the photo below, the CEO name is represented by a \"div\" element and in the \"info__value--2AHH7\" class, but when we call those elements it returns an empty dataset. ","3585be76":"# 1. Network Analysis","570dba68":"![fortune500_photo.jpeg](attachment:fortune500_photo.jpeg) ","f66bae83":"![html_search.png](attachment:html_search.png)","c44ffc50":"Now we can join our two datasets and sort the data so it shows the rankings from 1 to 1000. ","7cd2f4d5":"I want to create a dataset of all the companies in the Fortune 500 with some specific information about each. I figure the best way to do that is to go to the actual Fortune Magazine website. \n\nI planned on scraping information from the list of companies, but unfortunately this is a webpage heavy with javascript where the list changes without reloading the page. This prevents me from just scraping all the list contents direcly from the html. \n","a99391d9":"**Sort**","ca770ebb":"Our comp (companies) variable contains a list of dictionaries for each company. Each company has two parts: 1) a list of dictionaries which contain information about the company, and 2) a dictionary with a link to the company's Fortune page. \n\n","edae3f1e":"![Fortune_Table.png](attachment:Fortune_Table.png)","9d358fbf":"## Overview\n\nI started this notebook as a way to create a Fortune 500 Company dataset. Originally, I planned on simply scraping the list of companies on the Fortune website, but this website would not be tamed by my usual web scraping skills. I couldn't simply use Beautiful Soup to parse the data and select my desired elements because this web page is dynamic. So this notebook outlines my process for finding a way to get the data.  \n\nThe notebook is broken down as follows:\n1.  **Network Analysis**\n    * Explore the network and data requests used to render our webpage\n    * Call the API that supplies the data for the company table   \n2.  **JSON Data**\n    * Parse the JSON response from the API and select the data we want\n    * Create a dataframe and organize data  \n3.  **Scraping URLs**\n    * Crawl all the individual company Fortune webpages\n    * Find the html that contains the data we want and extract it\n    * Scrape our data and create a dataframe   \n4.  **Joining and Sorting Data**\n    * Join our two datasets and sort the rows  \n5.  **Conclusion**\n    * Convert dataframe to csv file","7d484398":"![Network_tab.png](attachment:Network_tab.png)","4228a336":"### DataFrame\n\nNow we're going to pull the data we want from each company's dictionary and put it into a dataframe. ","6d8c549b":"We send a request to the url and it provides us with a JSON response that contains all sorts of data on each company in the list! ","352111e8":"There are 1000 companies in this list.","c9a10902":"Now that we know how our dataset looks, we're going to select info about each company it contains. \n\nFirst, grab all of the \"field\" dictionaries from comp dataset and put them in list. ","b74d8348":"### Let's give an example of one of the companies below","6418463f":"### Scrape Data and Store in Dataframe\n\nNext, we scrape the specific datsa we want from each dictionary in our list and put the data into a new dataframe. ","de9db18a":"And this makes up our second dataset.","46df5c1f":"The permalink dictionary returns a link to the company's Fortune webpage.","a26e533e":"Fill missing rank change values with zero because they either didn't change ranks or they are newcommers. ","bad72609":"But we need to account for companies that are newcommers to the 500 and any companies outside of the top 500 (their rank change is not given). If we don't we will get duplicate values. ","9f41424c":"Now we can address the \"permalink\" varibale in our dictionaries. \n\nThis dictionary contains the link to each company's Fortune webpage.\n\nGrab those links and put them in a list. ","6b7b1c06":"### Data Types and Features\n\nChange our data types to numeric form in columns that show numbers.","81a81077":"The \"fields\" dictionary returns a list of other dictionaries containing items which populate the main Fortune web page. ","cbca85ad":"Therefore, we create our function to make our webpage request and then we extract our variable \"__PRELOADED_STATE__\". We can further narrow our response to the actual dictionary, **\"company-information\"**, that holds the data we seek. And finally, we convert that data to json format.","ec08a2e1":"# 2. JSON Data","128b8d46":"**Example:** Second company in the list web page response.","fdbe97b7":"Looks like I need to be a little more creative, so I pulled up the page source and did a keyword search on the the html. I found the name of the CEO in the html and going backwareds I was able to determine it was defined inside of the javascript variable:\n\n    \"__PRELOADED_STATE__ = \"","7b9b2480":"And that is our first dataset.","f8a8a3ca":"# 4. Join and Sort","6db78aa2":"Create a dataframe and select the features we want from the json output. ","9f5e3659":"Once there, we will look at the the requests being sent from our browser to different API's. This [article](http:\/\/www.gregreda.com\/2015\/02\/15\/web-scraping-finding-the-api\/) will give you a better understanding of how this works and how to select the API you are looking for.","61010d73":"![ceo_dev_shot.png](attachment:ceo_dev_shot.png)","9367e9fe":"**Now we can begine exploring and parsing the data we want.** \n\n- Above we see what we actually got back from our request was a list that contained 2 items.\n\n- The second item in the list contains the data we want so we call that one.","76a85f6c":"Let's see if we can get a better view of the json data for this company.","8e045c66":"Gather urls:","d67a14c3":"Since traditional web scraping won't work, I suspect there is another source producing the information on this web page. It is likely an API that my browser is calling to get some sort of JSON response and render the information on the page. So I decided to try to find the API that responds with the company data listed in this table. \n\nTo do this we need to right-click on the web page and select \"inspect\", which will pull up the developer window on our webpage. Then go to the network tab, which will look something like this:","96f2e6db":"**New Feature: Previous Rank** \n\nNow we're going to create a new feature that shows the previous rank (2020) of each company in the Forune 500. Does not apply to companies outside of top 500. ","4eae4999":"We want to look at the XHR responses in the network tab and try to find any response from a url name that mentions \"fortune\" or \"lists\". We poke around a little and find a GET request to the url below that provides us with a JSON repsone. ","c0551911":"Check data types","5e41be4a":"The third company in the list is Xcel Energy and it's dictionary has two parts:\n\n1. A \"fields\" dictionary\n2. A \"permalink\" dictionary","d5bf5a85":"# Fortune 500 Data Collection\n\n","934e64a9":"Crawl each of these urls and pull the infromation from the page for:\n\n- website url\n- CEO\n- Ticker\n- Market cap\n","f90aeb60":"# Conclusion\n\nFinally, we have our complete Fortune 500 dataset and we can create our csv file. ","fc618556":"For each company in the list, we will crawl their Fortune 500 company page and scrape the information we desire.  "}}