{"cell_type":{"3da7aa9b":"code","b199e073":"code","b367cbed":"code","47187518":"code","39a7d840":"code","b9602ffb":"code","acb4776b":"code","41617f27":"code","86bd5034":"code","b74a16aa":"code","983da5a2":"code","72f53bd5":"code","f2cd7337":"code","eeeb6234":"code","e4ce1979":"code","7228a685":"code","3e70b2b2":"code","4046e7c6":"code","77f217a9":"code","27922637":"code","da5fdc18":"code","77a0f97b":"code","d46ea177":"code","a9146faf":"code","0eabb1a9":"markdown","c1d1570f":"markdown","c6a0879e":"markdown","deccc72c":"markdown","3780bcbc":"markdown","4d8d7001":"markdown","99e3f6fd":"markdown","4b87271f":"markdown","2919e3cb":"markdown","8ef6c594":"markdown","b89736a4":"markdown","0c4c7728":"markdown","2776b63c":"markdown","aef18ef5":"markdown","005ba9b7":"markdown","97994d6a":"markdown","822829c0":"markdown","f2e66259":"markdown","4a2a10b4":"markdown","c34f635c":"markdown","f9ad57ee":"markdown","b5de0fe5":"markdown","f6021a6d":"markdown","bdb35eee":"markdown","bb332834":"markdown","c35e1ad9":"markdown","c86536f1":"markdown","3f9e9d91":"markdown","739bb923":"markdown","ab5babcf":"markdown","fedb6c4c":"markdown","8dd9e02f":"markdown","93c78379":"markdown","bebaf877":"markdown","29124aa8":"markdown","2f25b032":"markdown","bf1e3727":"markdown","bbbe5ec1":"markdown","c421e37b":"markdown","51338131":"markdown","950544ee":"markdown","b4e43a14":"markdown","1b4705ce":"markdown","041000f3":"markdown","26efe354":"markdown","e77b3be7":"markdown","2f3bfc16":"markdown","83082d8b":"markdown","f9a37e85":"markdown","47232e52":"markdown","b8264053":"markdown"},"source":{"3da7aa9b":"import pandas as pd\nimport numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import r2_score\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom keras.optimizers import Adam\nfrom keras.layers import LSTM\nprint(\"All libraries have been imported\")","b199e073":"df = pd.read_csv('..\/input\/crude-oil-stock-price\/CrudeOil.csv')\ndf.head(10)","b367cbed":"df.dropna(inplace=True)\ndf.head(10)","47187518":"df.info()","39a7d840":"df.drop(['Open', 'High', 'Low', 'Close', 'Volume'], axis=1, inplace=True)","b9602ffb":"df.info()","acb4776b":"df['Date'] = pd.to_datetime(df['Date'])\ndf_chg= df.set_index(['Date'], drop=True)\ndf_chg.head()","41617f27":"plt.figure(figsize=(15,8))\ndf_chg['Adj Close'].plot();","86bd5034":"split_date = pd.Timestamp('2017-12-27')\ndf1 = df_chg['Adj Close']\ntrain = df1.loc[:split_date]\ntest = df1.loc[split_date:]\nplt.figure(figsize=(15,8))\nax = train.plot()\ntest.plot(ax=ax)\nplt.legend(['train', 'test']);","b74a16aa":"print(\"We have\", len(train), \"train values\")\nprint(\"We have\", len(test), \"test values\")","983da5a2":"train_processed = df_chg.iloc[:, 0:1].values\ntrain_processed = train_processed[0:1996:1]\ntest_processed = df_chg.iloc[:, 0:1].values\ntest_processed = test_processed[1995:2495:1]","72f53bd5":"scaler = MinMaxScaler(feature_range=(-1, 1))\ntrain_sc = scaler.fit_transform(train_processed)\ntest_sc = scaler.transform(test_processed)","f2cd7337":"X_train = train_sc[:-1]\ny_train = train_sc[1:]","eeeb6234":"X_test = test_sc[:-1]\ny_test = test_sc[1:]","e4ce1979":"nn_model = Sequential()\nnn_model.add(Dense(12, input_dim=1, activation='relu'))\nnn_model.add(Dense(1))\nnn_model.compile(loss='mean_squared_error', optimizer='adam')\nearly_stop = EarlyStopping(monitor='loss', patience=2, verbose=1)\nhistory = nn_model.fit(X_train, y_train, epochs=100, batch_size=1, verbose=1, callbacks=[early_stop], shuffle=False)","7228a685":"y_pred_test_nn = nn_model.predict(X_test)\ny_train_pred_nn = nn_model.predict(X_train)\nprint(\"The R2 score on the Train set is:\\t{:0.3f}\".format(r2_score(y_train, y_train_pred_nn)))\nprint(\"The R2 score on the Test set is:\\t{:0.3f}\".format(r2_score(y_test, y_pred_test_nn)))","3e70b2b2":"train_sc_df = pd.DataFrame(train_sc, columns=['Y'], index=train.index)\ntest_sc_df = pd.DataFrame(test_sc, columns=['Y'], index=test.index)\n\n\nfor s in range(1,2):\n    train_sc_df['X_{}'.format(s)] = train_sc_df['Y'].shift(s)\n    test_sc_df['X_{}'.format(s)] = test_sc_df['Y'].shift(s)\n\nX_train = train_sc_df.dropna().drop('Y', axis=1)\ny_train = train_sc_df.dropna().drop('X_1', axis=1)\n\nX_test = test_sc_df.dropna().drop('Y', axis=1)\ny_test = test_sc_df.dropna().drop('X_1', axis=1)\n\nX_train = X_train['X_1']\ny_train = y_train['Y']\nX_test = X_test['X_1']\ny_test = y_test['Y']\n\nX_train = X_train.values\ny_train = y_train.values\n\nX_test = X_test.values\ny_test = y_test.values","4046e7c6":"X_train_lmse = X_train.reshape(X_train.shape[0], 1, 1)\nX_test_lmse = X_test.reshape(X_test.shape[0], 1, 1)\n\nprint('Train shape: ', X_train_lmse.shape)\nprint('Test shape: ', X_test_lmse.shape)","77f217a9":"lstm_model = Sequential()\nlstm_model.add(LSTM(7, input_shape=(1, X_train_lmse.shape[1]), activation='relu', kernel_initializer='lecun_uniform', return_sequences=False))\nlstm_model.add(Dense(1))\nlstm_model.compile(loss='mean_squared_error', optimizer='adam')\nearly_stop = EarlyStopping(monitor='loss', patience=2, verbose=1)\nhistory_lstm_model = lstm_model.fit(X_train_lmse, y_train, epochs=100, batch_size=1, verbose=1, shuffle=False, callbacks=[early_stop])","27922637":"y_pred_test_lstm = lstm_model.predict(X_test_lmse)\ny_train_pred_lstm = lstm_model.predict(X_train_lmse)\nprint(\"The R2 score on the Train set is:\\t{:0.3f}\".format(r2_score(y_train, y_train_pred_lstm)))\nprint(\"The R2 score on the Test set is:\\t{:0.3f}\".format(r2_score(y_test, y_pred_test_lstm)))","da5fdc18":"nn_test_mse = nn_model.evaluate(X_test, y_test, batch_size=1)\nlstm_test_mse = lstm_model.evaluate(X_test_lmse, y_test, batch_size=1)\nprint('NN: %f'%nn_test_mse)\nprint('LSTM: %f'%lstm_test_mse)","77a0f97b":"nn_y_pred_test = nn_model.predict(X_test)\nlstm_y_pred_test = lstm_model.predict(X_test_lmse)","d46ea177":"plt.figure(figsize=(15, 8))\nplt.plot(y_test, label='True')\nplt.plot(y_pred_test_nn, label='NN')\nplt.title(\"ANN's Prediction\")\nplt.xlabel('Observation')\nplt.ylabel('Adj Close Scaled')\nplt.legend()\nplt.show();","a9146faf":"plt.figure(figsize=(15, 8))\nplt.plot(y_test, label='True')\nplt.plot(lstm_y_pred_test, label='LSTM')\nplt.title(\"LSTM's Prediction\")\nplt.xlabel('Observation')\nplt.ylabel('Adj Close scaled')\nplt.legend()\nplt.show();","0eabb1a9":"<img src=\"https:\/\/camo.githubusercontent.com\/623dc99da4b11773ac862bc61844994da77b34ce\/68747470733a2f2f692e6962622e636f2f3452354a53714b2f526573696d342e706e67\" style=\"width: 700px;\"\/> ","c1d1570f":"### References","c6a0879e":"and scale datas","deccc72c":"We scale train and test data to [-1, 1]","3780bcbc":"We use shift function that shifts the entire column by 1.","4d8d7001":"As can be seen, the \"Adj Close\" data are quite erratic.","99e3f6fd":"### The Data","4b87271f":"* We create a Sequantial model\n* add layers via the .add() method\n* Pass an input_dim argument to the first layer.\n* The activation function is the Rectified Linear Unit- Relu.\n* Configure the learning process, which is done via the compile method.\n* A loss function is mean_squared_error , and An optimizer is adam.\n* Stop training when a monitored loss has stopped improving.\n* patience=2, indicate number of epochs with no improvement after which training will be stopped.\n* The ANN is trained for 100 epochs and a batch size of 1 is used.","2919e3cb":"### The Plotting","8ef6c594":"<br>\nWe split train and test again ","b89736a4":"### Forecasting","0c4c7728":"### Scale The Data","2776b63c":"## __A Study on Stock Price of Crude Oil Time Series Forecasting with Simple Neural Networks and LSTM__","aef18ef5":"<br>\nIt had an early stopping at Epoch 35\/100","005ba9b7":"We predict our model and get R2 scores for train and test.","97994d6a":"The simplest networks contain no hidden layers and are equivalent to linear regressions. This figure shows the neural network version of a linear regression with four predictors. The coefficients attached to these predictors are called \u201cweights\u201d. The forecasts are obtained by a linear combination of the inputs. The weights are selected in the neural network framework using a \u201clearning algorithm\u201d that minimises a \u201ccost function\u201d such as the MSE. ","822829c0":"The line passing horizontally from the top along the diagram in the LSTM represents cell state. Data processed in layers contained in repeating modules has a certain effect on the values of cell state.\n<br> ![cellstate](https:\/\/camo.githubusercontent.com\/3cc728a04a43b4970bf862473b5680ed10453ef4\/68747470733a2f2f692e6962622e636f2f564c5058364d772f526573696d332e706e67)","f2e66259":"* LSTM has a visible layer with 1 input.\n* A hidden layer with LSTM neurons.\n* We used relu activation function for the LSTM neurons. \n* A loss function is mean_squared_error , and An optimizer is adam.\n* Stop training when a monitored loss has stopped improving.\n* The LSTM is trained for 100 epoch and a batch size of 1 is used.","4a2a10b4":"Artificial neural networks are forecasting methods that are based on simple mathematical models of the brain. They allow complex nonlinear relationships between the response variable and its predictors.","c34f635c":"At the last step, it  should be decided that what the output will be. Cell state for ouput is applied to tanh function in the sigmoid layer and multiplication with output value of sigmoid gate.(the value is set to -1 to 1) Thus, the value for decided part of the output is obtained. \n<br> ![](https:\/\/camo.githubusercontent.com\/aca69a19f65aa774e2986e10b900f4fc4e067a6d\/68747470733a2f2f692e6962622e636f2f7658787a5671742f526573696d392e706e67)","f9ad57ee":"Get traning and test data","b5de0fe5":"LSTM is a special type of RNN that can learn long-term dependencies. It was first introduced in 1997 by Hochreiter and Schmindhuber. LSTMs are designed to prevent the problem of long-term dependence and are widely used. \n","f6021a6d":"### LSTM - Traning Model","bdb35eee":"### Simple ANN - Traning Model","bb332834":"We have NaN values and we drop these values.","c35e1ad9":"* Forecasting is the estimation of the future values of a variable.\n* We make forecasting about future events by drawing meaning from the models we obtained by using information from earlier periods.\n* Strategies, plans and targets for the future are determined by forecasting. \n* Forecasting studies were initially realized with simple modeling experiments and then more complex models were established by using the advantages of techonology, and better results were obtanied.","c86536f1":"Load the data into a Pandas dataframe and have a quick peek of the head rows.","3f9e9d91":"### Import Libraries","739bb923":"### Forecasting","ab5babcf":"Compare test MSE of both models.","fedb6c4c":"<br>\nIt had an early stopping at Epoch 19\/100","8dd9e02f":"### Forecasting Method: Artificial Neural Network (ANN)","93c78379":"We splited train and test with Date values. We have to convert datas.","bebaf877":"![Rnn](https:\/\/camo.githubusercontent.com\/a44d19bc6f7d2d8ef95452127f3d7e61dfd6379b\/68747470733a2f2f692e6962622e636f2f7938546d3837312f526573696d312e706e67) \n$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$ A standart RNN has a single layer \n![LSTM](https:\/\/camo.githubusercontent.com\/70fec97625524a11d1b7296ec18c13ac4b16dd37\/68747470733a2f2f692e6962622e636f2f7a354d485368502f526573696d322e706e67)\n$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$ LSTM has four specially placed layers","29124aa8":"We split the data by nearly %80 and %20 to train and test. Split the data to train and test set by date \"2017-12-27\". That is, the data prior to this date is the traning data and the data from this data onward is the test data and we plotting it again.","2f25b032":"A neural network can be thought of as a network of \u201cneurons\u201d which are organised in layers. The predictors (or inputs) form the bottom layer and the forecasts (or outputs) form the top layer. There may also be intermediate layers containing \u201chidden neurons\u201d.","bf1e3727":"<img src=\"https:\/\/otexts.com\/fpp2\/nnet2.png\" style=\"width: 600px;\"\/> <br>\n_A neural network with four inputs and one hidden layer with three hidden neurons._ <br>","bbbe5ec1":"We will need to convert all our input variables in a 3D vector form.","c421e37b":"<br>\nWe will drop the columns we don't need then convert \"Date\" column to datatime data type and set \"Date\" column to index.","51338131":"### Forecasting Method: Long Short Term Memory (LSTM)","950544ee":"Crude Oil WTI CL=F Stock Price\n<br> I get the data from \"finance.yahoo.com\". Dataset can be downloaded from [here](https:\/\/ca.finance.yahoo.com\/quote\/CL%3DF\/history?period1=1262304000&period2=1577836800&interval=1d&filter=history&frequency=1d). I set the date range from Jan 01, 2010 to Jan 01, 2020 . ","b4e43a14":"The next step is to decide which new information is stored in cell state. This happens in two steps. Firstly, the sigmoid layer called input gate layer (update gate) decides which value to refresh.","1b4705ce":"LSTM has ability to add or remove information to the cell state through structures called gates. The gate in the figure consists of a sigmoid network layer and point multiplication. \n<br> ![gate](https:\/\/camo.githubusercontent.com\/0ed236e6916c452367c01cf8a1d1a6db47911b88\/68747470733a2f2f692e6962622e636f2f784d665a6851772f526573696d332d352e706e67)","041000f3":"We plot a time series line plot","26efe354":"We predict our LSTM model and get R2 scores for train and test.","e77b3be7":"![](https:\/\/camo.githubusercontent.com\/49bc070ed5fc3183fe134fa9ccbda98a5c465656\/68747470733a2f2f692e6962622e636f2f4c3534744a72442f526573696d372e706e67) ![](https:\/\/camo.githubusercontent.com\/7b094617e63e11993b628e3fdd641676362b18ef\/68747470733a2f2f692e6962622e636f2f355474785671332f526573696d382e706e67)","2f3bfc16":"### Split The Data","83082d8b":"<br>\n<br>\nLSTM firstly determines which information passes through the cell state. This decision is made by sigmoid layer which is called forget gate layer. ","f9a37e85":"![](https:\/\/camo.githubusercontent.com\/8ba9e82854ade2c8646edd3b3589c8fd4f54ec29\/68747470733a2f2f692e6962622e636f2f4d7044527162772f526573696d352e706e67) ![](https:\/\/camo.githubusercontent.com\/27d1a141e78dc5e90ccb54e4215c30cf53d14447\/68747470733a2f2f692e6962622e636f2f39477a337674422f526573696d362e706e67)","47232e52":"The tanh layer creates new candidate values. Cell state is refreshed by combining these two generated values. This newly obtained values is the value stored for each state value to be transferred to next block. ","b8264053":"* https:\/\/mc.ai\/an-introduction-on-time-series-forecasting-with-simple-neura-networks-lstm\/\n* https:\/\/otexts.com\/fpp2\/nnetar.html\n* J. Schmidhuber, 2015\n* Olah, Understanding LSTM Networks, 2015\n* S. Hochreiter & J. Schmidhuber, Long Short-Term Memory, Neural Computation 9(8):1735-1780, 1997\n* https:\/\/machinelearningmastery.com\/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping\/#:~:text=the%20validation%20dataset.-,Early%20Stopping%20in%20Keras,configured%20when%20instantiated%20via%20arguments"}}