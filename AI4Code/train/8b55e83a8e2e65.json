{"cell_type":{"b094244f":"code","15232285":"code","2826895f":"code","6723f626":"code","a5a5c8d8":"code","524bd776":"code","e13298fa":"code","8e73f723":"markdown","26e8bec1":"markdown","87850760":"markdown","2873685f":"markdown"},"source":{"b094244f":"## importing auxiliary libraries\nimport numpy as np \nimport pandas as pd\n\n## vectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n## similarity metric\nfrom sklearn.metrics.pairwise import cosine_similarity","15232285":"## inspecting the data\n\ndf = pd.read_csv('\/kaggle\/input\/poems-in-portuguese\/portuguese-poems.csv')\n\n## lets drop any NA content\ndf.dropna(subset=['Content'],inplace=True)\n\n## reset index for organization purposes\ndf.reset_index(drop=True,inplace=True)\ndf.head()","2826895f":"%%time\ntfvec = TfidfVectorizer(max_features=10000)\nx = tfvec.fit_transform(df['Content'])\nx","6723f626":"## creating the not so optimized function that calculates and finds 10 most similar poems\n\ndef find_similar(poem):\n    \n    simi = []\n\n    for i in range(x.shape[0]):\n        simi.append((i,cosine_similarity(x[0],x[i])[0][0]))\n\n    simi.sort(key = lambda x: x[1],reverse=True)\n    \n    df_ret = df.iloc[np.array(simi[:10])[:,0],[0,1]]\n    df_ret['similarity'] = np.array(simi[:10])[:,1]\n    \n    return df_ret","a5a5c8d8":"%%time\nfind_similar(x[0])","524bd776":"print(df[(df.Author == 'Cec\u00edlia Meireles') & (df.Title == 'Retrato')].Content[0])","e13298fa":"print(df[(df.Author == 'Fernanda Benevides') & (df.Title == 'Flagrante')].Content[12643])","8e73f723":"Let's take a look and see if they're really similar.","26e8bec1":"## TF-IDF\n---\n\nTF-IDF stands for Term Frquency Inverse Document Frequency. Huge name, right? Maybe the formula can help us understand more about it.\n\n![formula](https:\/\/lh3.googleusercontent.com\/proxy\/xdso5BjffybsMdD89sPanw1_izob5t-08zorCwxLErC4NbNRWX1Xj-lwfrXTx4Zq63zPzXww-tPsHIKsXQX5xqEpsjDTtKMC5YrmR7En4rwJEVJjEtSAPmYOC8ZwptKl9Xle9JjcffI)\n\nWhere i == a word and j == document.\n\nBut why do we need TF-IDF? This is a vectorization technique, it means that we are taking a document and representing it as a vector. This vector now is a numeric representation of the whole document and it can be used for a number of purposes including finding vectors that are spatially  close to one another.","87850760":"In the code cell above I'm vectorizing the 'content' serie of the dataframe using [TF-IDF](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.TfidfVectorizer.html) with a parameter called max_features, this parameter returns only the N features that represent the most frequent words. The output is a [sparse matrix](https:\/\/machinelearningmastery.com\/sparse-matrices-for-machine-learning\/) with 15541 documents and 10000 features.","2873685f":"# Similar Poems\n___\n\n![poems](https:\/\/p1.pxfuel.com\/preview\/1017\/419\/763\/library-books-shelves-bookshelf.jpg)\n\nThe goal of this notebook is to find similar poems using a vetorization technique called TF-IDF and similarity measured called cosine_similarity."}}