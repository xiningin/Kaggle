{"cell_type":{"5a9c1397":"code","0cf129d3":"code","9a4ef67a":"code","067d4478":"code","0f9a976f":"code","e1bddc89":"code","513c5940":"code","9e6a9f7a":"code","30420276":"code","4e213380":"code","0d5340dc":"code","88672bd9":"code","b5c6c970":"code","abbc3d04":"code","2d0f0943":"code","58791581":"code","b8c9e81e":"code","6a3e3461":"code","79df4eaa":"code","f5c391fe":"code","d54c0270":"code","e3ec6b6e":"code","01bc65e2":"code","44d2c6f0":"code","36095759":"code","a99a1483":"code","91da98f5":"code","05d780f4":"code","86b45606":"code","bf4e6450":"code","39ecaab7":"code","ba463e99":"code","d33e21b6":"code","bdcbf481":"code","aab8bc60":"code","2f2bb459":"code","bd5f92ad":"code","b43a4774":"code","9037843f":"code","30aa85d2":"code","08b34952":"code","76ca111f":"code","fc50e705":"code","849d2ae7":"code","c32ac969":"code","dba48f1e":"code","86434ca0":"code","fb03a77e":"code","e452e2ae":"code","f5a1c5b1":"code","e2811ad0":"code","c2240337":"code","c3b70a5f":"code","d44d2aa0":"code","0d093a56":"code","0f09363c":"code","187b1bb8":"code","6a897ff5":"code","48a55c27":"code","f0f25b87":"code","05aa792b":"code","ddd524b7":"code","abac85e6":"code","1ccb73e2":"code","72a18596":"code","69639894":"code","ff046fc7":"code","a379a7da":"code","d6a3778f":"code","2fb46385":"code","cc59c5aa":"code","116125d8":"code","6a6f0ce3":"code","02df13c4":"code","049847c4":"code","e47dcc67":"code","c27833ef":"code","b44d0e2f":"code","eb4d7cc9":"code","2ca9a1ef":"code","96b1171c":"code","e50cf1b6":"code","a2cf3e35":"code","8274b633":"code","6bbb2236":"code","feddc2ee":"code","868a2c2d":"code","78ca20b6":"code","eba73e35":"code","311187b0":"code","86bb7785":"code","1d9a6205":"code","d4d240b8":"code","91d9f8fc":"code","9e5fb5e6":"code","0e2af6a0":"markdown","7335b20e":"markdown","f62fb38c":"markdown","9937590a":"markdown","a2ad144f":"markdown","83538402":"markdown","9a187797":"markdown","cbade515":"markdown","30205224":"markdown","ca35ae22":"markdown","048dd5a6":"markdown","22424618":"markdown","30c5e2ec":"markdown","7ba9b8ba":"markdown","b66a4003":"markdown","8dc4a188":"markdown","bfd97d07":"markdown","de476047":"markdown","ef1f67c3":"markdown","5f6df034":"markdown","37b9fb1a":"markdown","6c3d1bb3":"markdown","ea282897":"markdown","1087e2be":"markdown","2f226f42":"markdown","2a8f4535":"markdown","d78196fa":"markdown","a1d115a9":"markdown","06e4752a":"markdown","6c914758":"markdown","4aa11ddd":"markdown","c4978181":"markdown","677e5b9c":"markdown","9458f902":"markdown","420f8622":"markdown","f192702e":"markdown","3ef1fe23":"markdown","b2600772":"markdown","0e02645e":"markdown","49d46e9b":"markdown","e4ae6131":"markdown","5a659ded":"markdown","e54eecfd":"markdown","a5571083":"markdown","71b8b264":"markdown","d9c649ca":"markdown","e4b25a02":"markdown","c0f9097f":"markdown","90a992ab":"markdown","0dda4539":"markdown","f3ec3836":"markdown","3bcf1d83":"markdown","3d3d09c4":"markdown"},"source":{"5a9c1397":"import numpy as np \nimport pandas as pd \nimport unicodedata\nimport re\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 12, 10\n\nfrom collections import Counter\n\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom catboost import Pool, CatBoostRegressor\n\nfrom sklearn import metrics\nfrom sklearn import model_selection\n\nimport gc\nimport time\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","0cf129d3":"# Read train and test dataset\n\ntrain_df = pd.read_excel(\"..\/input\/Final_Train.xlsx\")\ntest_df = pd.read_excel(\"..\/input\/Final_Test.xlsx\")\ndf_test = test_df.copy()","9a4ef67a":"train_df.head()","067d4478":"test_df.head()","0f9a976f":"# Check shape of dataset\n\ntrain_df.shape, test_df.shape","e1bddc89":"# check train column types\n\nctype = train_df.dtypes.reset_index()\nctype.columns = [\"Count\", \"Column Type\"]\nctype.groupby(\"Column Type\").aggregate('count').reset_index()","513c5940":"# check test column types\n\nctype = test_df.dtypes.reset_index()\nctype.columns = [\"Count\", \"Column Type\"]\nctype.groupby(\"Column Type\").aggregate('count').reset_index()","9e6a9f7a":"# Check the Maximum and Minimum number of qualifications\n\n# Train set\ndat_train = train_df.Qualification.apply(lambda x: len(x.split(',')))\nprint(\"Maximum qualifications of a doctor in the Train dataset is {}\\n\".format(dat_train.max()))\nprint(\"And the qualifications is --> {}\\n\\n\".format(train_df.Qualification[dat_train.idxmax()]))\nprint(\"Minimum qualification of a doctor in the Train dataset is {}\\n\".format(dat_train.min()))\nprint(\"And the qualifications is --> {}\\n\\n\".format(train_df.Qualification[dat_train.idxmin()]))\n\n# Test set\ndat_test = test_df.Qualification.apply(lambda x: len(x.split(',')))\nprint(\"Maximum qualifications of a doctor in the Test dataset is {}\\n\".format(dat_test.max()))\nprint(\"And the qualifications is --> {}\\n\\n\".format(test_df.Qualification[dat_test.idxmax()]))\nprint(\"Minimum qualification of a doctor in the Test dataset is {}\\n\".format(dat_test.min()))\nprint(\"And the qualifications is --> {}\".format(test_df.Qualification[dat_test.idxmin()]))","30420276":"sorted(test_df.Qualification[test_df.Qualification.apply(lambda x: len(x.split(','))).idxmax()].split(\",\"))","4e213380":"# Define function to remove inconsistencies in the data\ndef sortQual(text):\n    arr = re.sub(r'\\([^()]+\\)', lambda x: x.group().replace(\",\",\"-\"), text) # to replace ',' with '-' inside brackets only\n    return ','.join(sorted(arr.lower().replace(\" \",\"\").split(\",\")))","0d5340dc":"# Apply the function on the Qualification set\n\n# Train Set\ntrain_df.Qualification = train_df.Qualification.apply(lambda x: sortQual(x))\n\n# Test Set\ntest_df.Qualification = test_df.Qualification.apply(lambda x: sortQual(x))","88672bd9":"# Define a function to create a doc of all Qualifications seprataed by ','\n\ndef doc(series):\n    text = ''\n    for i in series:\n        text += i + ','\n    return text","b5c6c970":"# List of top 10 unique Qualifications along with there occurence in Train Set\n\ntext = doc(train_df.Qualification)\ndf = pd.DataFrame.from_dict(dict(Counter(text.split(',')).most_common()), orient='index').reset_index()\ndf.columns=['Qualification','Count']\ndf.head(10)","abbc3d04":"# List of top 10 unique Qualifications along with there occurence in Test Set\n\ntext = doc(test_df.Qualification)\ndf = pd.DataFrame.from_dict(dict(Counter(text.split(',')).most_common()), orient='index').reset_index()\ndf.columns=['Qualification','Count']\ndf.head(10)","2d0f0943":"text = doc(test_df.Qualification)\ndf = pd.DataFrame.from_dict(dict(Counter(text.split(',')).most_common()), orient='index').reset_index()\ndf.columns=['Qualification','Count']\ndf['code'] = df.Qualification.astype('category').cat.codes\ndf.head(10)","58791581":"qual_dict = dict(zip(df.Qualification, df.code))","b8c9e81e":"def qual_col(dataframe, col, col_num):\n    return dataframe[col].str.split(',').str[col_num]","6a3e3461":"# for training set\nfor i in range(0,dat_train.max()):\n    qual = \"Qual_\"+ str(i+1)\n    train_df[qual] = qual_col(train_df,'Qualification', i)\n\n    \n# for test set\nfor i in range(0,dat_test.max()):\n    qual = \"Qual_\"+ str(i+1)\n    test_df[qual] = qual_col(test_df,'Qualification', i)\n","79df4eaa":"# Select Qualification categorical columns to be encoded\n\ncolumn_test = ['Qual_1', 'Qual_2', 'Qual_3', 'Qual_4',\n           'Qual_5', 'Qual_6', 'Qual_7', 'Qual_8', 'Qual_9', 'Qual_10', 'Qual_11',\n           'Qual_12', 'Qual_13', 'Qual_14', 'Qual_15', 'Qual_16', 'Qual_17']\n\ncolumn_train = ['Qual_1', 'Qual_2', 'Qual_3', 'Qual_4',\n           'Qual_5', 'Qual_6', 'Qual_7', 'Qual_8', 'Qual_9', 'Qual_10']","f5c391fe":"# Encode categorical columns for Test and Train set\n\nfor i in column_train:\n    train_df.replace({i: qual_dict}, inplace=True)\n    \n    \nfor i in column_test:\n    test_df.replace({i: qual_dict}, inplace=True)","d54c0270":"train_df.head()","e3ec6b6e":"test_df.head()","01bc65e2":"train_df['Qual_count'] = train_df.Qualification.apply(lambda x: len(x.split(',')))\ntest_df['Qual_count'] = test_df.Qualification.apply(lambda x: len(x.split(',')))","44d2c6f0":"# Train set\ntrain_df['years_exp'] = train_df['Experience'].str.slice(stop=2).astype(int)\n\n# Test set\ntest_df['years_exp'] = test_df['Experience'].str.slice(stop=2).astype(int)","36095759":"train_df.head()","a99a1483":"# Train set\ntrain_df['Rating'].fillna('0%',inplace = True)\ntrain_df['Rating'] = train_df['Rating'].str.slice(stop=-1).astype(int)\n\n# Test set\ntest_df['Rating'].fillna('0%',inplace = True)\ntest_df['Rating'] = test_df['Rating'].str.slice(stop=-1).astype(int)","91da98f5":"train_df.head()","05d780f4":"train_df.Place = train_df.Place.apply(lambda x: ','.join(str(x).lower().replace(\" \",\"\").split(\",\")))\ntest_df.Place = train_df.Place.apply(lambda x: ','.join(str(x).lower().replace(\" \",\"\").split(\",\")))","86b45606":"# Train Set\ntrain_df['City'] = train_df['Place'].apply(lambda x: str(x).replace(' ','').split(',')[-1])\ntrain_df['Locality'] = train_df['Place'].apply(lambda x: str(x).rsplit(',', 1)[0])\n\n\n# Test Set\ntest_df['City'] = test_df['Place'].apply(lambda x: str(x).replace(' ','').split(',')[-1])\ntest_df['Locality'] = test_df['Place'].apply(lambda x: str(x).rsplit(',', 1)[0])","bf4e6450":"# Lets Check Unique Cities in test set\n\ntest_df.City.value_counts()","39ecaab7":"# Lets Check Unique Cities in train set\n\ntrain_df.City.value_counts()","ba463e99":"train_df[train_df.City == 'e']","d33e21b6":"train_df.loc[3980, 'Place'] = np.nan\ntrain_df.loc[3980, 'City'] = np.nan\ntrain_df.loc[3980, 'Locality'] = np.nan","bdcbf481":"# Define function to dummify feature\n\ndef get_dummies(dataframe,feature_name):\n  dummy = pd.get_dummies(dataframe[feature_name], prefix=feature_name)\n  dummy.drop(dummy.columns[0], axis=1, inplace=True) #avoid dummy trap\n  return pd.concat([dataframe,dummy], axis = 1)","aab8bc60":"train_df = get_dummies(train_df, 'City')\ntest_df = get_dummies(test_df, 'City')","2f2bb459":"# Checkout dataframe after dummification of City\n\ntrain_df.head()","bd5f92ad":"train_df.Profile.value_counts()","b43a4774":"train_df.Profile = train_df.Profile.apply(lambda x: str(x).lower().replace(\" \",\"\"))\ntest_df.Profile = train_df.Profile.apply(lambda x: str(x).lower().replace(\" \",\"\"))","9037843f":"train_df = get_dummies(train_df, 'Profile')\ntest_df = get_dummies(test_df, 'Profile')","30aa85d2":"train_df.head()","08b34952":"# train_df.Locality.value_counts()","76ca111f":"# List of top 10 Localities along with there occurence in Train Set\ntrain_df['Locality'] = train_df['Locality'].apply(str) # Convert int64 dtype to str type first\ntext = doc(train_df.Locality)\ndf = pd.DataFrame.from_dict(dict(Counter(text.split(',')).most_common()), orient='index').reset_index()\ndf.columns=['Locality','Count']\ndf.head(10)","fc50e705":"# List of top 10 unique Localities along with there occurence in Test Set\ntest_df['Locality'] = test_df['Locality'].apply(str) # Convert int64 dtype to str type first\ntext = doc(test_df.Locality)\ndf = pd.DataFrame.from_dict(dict(Counter(text.split(',')).most_common()), orient='index').reset_index()\ndf.columns=['Locality','Count']\ndf.head(10)","849d2ae7":"# Define function to label encode the selected categorical variable for modeling\n\ndef encode(data):\n    return data.astype('category').cat.codes","c32ac969":"# Encode Locality column of test data\n\ncolumns = ['Locality']\n\nfor i in columns:\n    col = i+\"_code\"\n    test_df[col] = encode(test_df[i])","dba48f1e":"# Check test dataset after encoding locality\ntest_df.head()","86434ca0":"# Create unique lists of [variable, variable code] combination and drop duplicate pairs.\n\ndf_test_merge = test_df[['Locality','Locality_code']].drop_duplicates()","fb03a77e":"# Pull the respective encoded variables list in the train data (Using a left join) to avoid any merging issue.\n\ntrain_df = pd.merge(train_df,df_test_merge[['Locality','Locality_code']],on='Locality', how='left')","e452e2ae":"# Train set after merging encoded Locality\n\ntrain_df.head()","f5a1c5b1":"list(train_df.Miscellaneous_Info[0:10])","e2811ad0":"train_df.Miscellaneous_Info = train_df.Miscellaneous_Info.str.replace(\",\",\"\")\ntest_df.Miscellaneous_Info = test_df.Miscellaneous_Info.str.replace(\",\",\"\")","c2240337":"# Train set\ntrain_df.Miscellaneous_Info = train_df.Miscellaneous_Info.str.replace(unicodedata.lookup('Indian Rupee Sign'), 'INR ')\n\n# Test set\ntest_df.Miscellaneous_Info = test_df.Miscellaneous_Info.str.replace(unicodedata.lookup('Indian Rupee Sign'), 'INR ')","c3b70a5f":"list(train_df.Miscellaneous_Info[0:10])","d44d2aa0":"# Define function to return the Feedback numbers\n\ndef find_feedback(data):\n    result = re.search(r' (.*?) Feedback',data)\n    if result:\n        return int(result.group(1))\n    else:\n        return 0","0d093a56":"# Fetch out the feedback numbers in different records. \n\n# Train set\ntrain_df['feedack_num'] = train_df.Miscellaneous_Info.apply(lambda x: find_feedback(x) if '%' in str(x) else 0)\n\n# Test set\ntest_df['feedack_num'] = test_df.Miscellaneous_Info.apply(lambda x: find_feedback(x) if '%' in str(x) else 0)","0f09363c":"train_df.head()","187b1bb8":"# Let us have a look at the different Fee value in the records.\n\nlist(train_df.Miscellaneous_Info[train_df.Miscellaneous_Info.str.contains('INR', na = False)].sample(10))","6a897ff5":"# Define function to return the Fees Value\n\ndef find_fees(data):\n    result = re.search(r'INR (\\d*)',data)\n    if result:\n        return int(result.group(1))\n    else:\n        return 0\n","48a55c27":"# Fetch out the Fees value in different records. \n\n# Train set\ntrain_df['fees_val'] = train_df.Miscellaneous_Info.apply(lambda x: find_fees(x) if 'INR' in str(x) else 0)\n\n# Test set\ntest_df['fees_val'] = test_df.Miscellaneous_Info.apply(lambda x: find_fees(x) if 'INR' in str(x) else 0)","f0f25b87":"train_df.head()","05aa792b":"train_df.Fees.value_counts().reset_index().sort_values(by='index')","ddd524b7":"train_df[train_df.Fees < 50]","abac85e6":"train_df.years_exp.describe()","1ccb73e2":"test_df.years_exp.describe()","72a18596":"train_df[train_df.years_exp == 0]","69639894":"test_df[test_df.Qualification.str.contains('getinspiredbyremarkablestoriesofpeoplelikeyou')]","ff046fc7":"train_df.fees_val.describe()","a379a7da":"train_df.fees_val.value_counts()","d6a3778f":"train_df.loc[train_df.fees_val > 999, ['Miscellaneous_Info',\"Fees\"]]","2fb46385":"test_df.loc[test_df.fees_val > 999, ['Miscellaneous_Info']]","cc59c5aa":"# Define function as per above requirement.\n\ndef mark_100(data):\n    data.Fees = np.where(data.Qualification.str.contains('getinspiredbyremarkablestoriesofpeoplelikeyou', na = False),\n                      100,\n                      data.Fees)\n    return data","116125d8":"cols_to_use = ['Rating', 'Qual_1', 'Qual_2', 'Qual_3', 'Qual_4',\n       'Qual_5', 'Qual_6', 'Qual_7', 'Qual_8', 'Qual_9', 'Qual_10',\n       'Qual_count', 'years_exp', 'City_chennai', 'City_coimbatore', 'City_delhi', 'City_ernakulam', 'City_hyderabad',\n       'City_mumbai', 'City_nan', 'City_thiruvananthapuram', 'Profile_dentist', 'Profile_dermatologists', 'Profile_entspecialist', \n       'Profile_generalmedicine', 'Profile_homeopath', 'Locality_code', 'feedack_num', 'fees_val']\n\ntarget_col = 'Fees'","6a6f0ce3":"train = train_df[cols_to_use].copy()\n# train['Fees'] = train_df.Fees.copy()\ntest = test_df[cols_to_use].copy()","02df13c4":"train.head()","049847c4":"test.head()","e47dcc67":"def missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns\n\nmissing_values_table(train)","c27833ef":"for i in cols_to_use:\n    train[i] = pd.to_numeric(train[i].astype(str), errors='coerce').fillna(-1).astype(int)\n    test[i] = pd.to_numeric(test[i].astype(str), errors='coerce').fillna(-1).astype(int)","b44d0e2f":"# Define Train and test set\ntrain_X = train.copy()\ntest_X = test.copy()\ntrain_y = train_df.Fees.copy()\n\ntrain_X.shape, train_y.shape, test_X.shape","eb4d7cc9":"n_fold = 5\nfolds = KFold(n_splits=n_fold, shuffle=True, random_state=4)","2ca9a1ef":"def train_model(X=train_X,\n                X_test=test_X,\n                y=train_y,\n                params=None,\n                folds=folds,\n                model_type='lgb',\n                plot_feature_importance=False,\n                averaging='usual',\n                make_oof=False,\n                num_class=0\n               ):\n    result_dict = {}\n    \n    if make_oof:\n        if num_class:\n            oof = np.zeros((len(X), num_class))\n        else:\n            oof = np.zeros((len(X)))\n    \n    if num_class:\n        prediction = np.zeros((len(X_test), num_class))\n    else:\n        prediction = np.zeros((len(X_test)))\n    \n    scores = []\n    \n    feature_importance = pd.DataFrame()\n    \n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n        gc.collect()\n        print(\"\")\n        print('Fold', fold_n + 1, 'started at', time.ctime())\n        \n        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n        \n        \n        if model_type == 'lgb':\n            train_data = lgb.Dataset(X_train, label=y_train,\n#                                      categorical_feature = cat_cols\n                                    )\n            valid_data = lgb.Dataset(X_valid, label=y_valid,\n#                                      categorical_feature = cat_cols\n                                    )\n            \n            model = lgb.train(\n                params,\n                train_data,\n                num_boost_round = 1000,\n                valid_sets = [valid_data],\n                verbose_eval = 100,\n                early_stopping_rounds = 100\n            )\n\n            del train_data, valid_data\n            \n            y_pred_valid = model.predict(X_valid, num_iteration=model.best_iteration)\n            y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n            \n            del X_valid\n            gc.collect()            \n            \n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid)\n\n            model = xgb.train(\n                params = params,\n                dtrain = train_data,\n                num_boost_round = 1500,\n                evals=[(valid_data, \"Validation\")],\n                early_stopping_rounds = 100,\n                verbose_eval = 100,\n                )\n            \n            del train_data, valid_data\n            \n            y_pred_valid = model.predict(xgb.DMatrix(X_valid), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_test), ntree_limit=model.best_ntree_limit)\n            \n            del X_valid\n            gc.collect()\n            \n        if model_type == 'cat':\n            train_pool = Pool(X_train, y_train) \n            valid_pool = Pool(X_valid, y_valid)\n            \n            model =  CatBoostRegressor(iterations=5000,\n                                        learning_rate=0.01,\n                                        eval_metric='RMSE',\n                                        random_seed = 4,\n                                        metric_period = 200,\n                                      )\n            \n            model.fit(train_pool,\n                      eval_set=(valid_pool),\n#                       cat_features=cat_cols,\n                      use_best_model=True\n                     )\n            \n            del train_pool, valid_pool\n                    \n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test).reshape(-1,)\n            \n            del X_valid\n            gc.collect()            \n\n        if make_oof:\n            oof[valid_index] = y_pred_valid\n                   \n#         scores.append(kappa(y_valid, y_pred_valid.argmax(1)))        \n#         print('Fold kappa:', kappa(y_valid, y_pred_valid.argmax(1)))\n#         print('')\n        \n        if averaging == 'usual':\n            prediction += y_pred\n        elif averaging == 'rank':\n            prediction += pd.Series(y_pred).rank().values\n        \n        # feature importance\n        if model_type == 'lgb' or model_type == 'xgb' or model_type == 'cat':\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = X.columns\n            if model_type == 'lgb':\n                fold_importance[\"importance\"] = model.feature_importance()\n            elif model_type == 'xgb':\n                fold_importance[\"importance\"] = fold_importance.feature.map(model.get_score()).fillna(0)\n            else:\n                fold_importance[\"importance\"] = model.feature_importances_            \n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction \/= n_fold\n    result_dict['prediction'] = prediction\n    \n#     print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    if plot_feature_importance:\n        plt.figure(figsize=(15, 15));\n        feature_importance = pd.DataFrame(feature_importance.groupby(\"feature\")[\"importance\"].mean().sort_values(ascending=False).reset_index())[:50]\n        sns.barplot(x=\"importance\", y=\"feature\", data=feature_importance);\n        plt.title('Feature Importance (avg over folds)');\n\n        result_dict['feature_importance'] = feature_importance\n            \n    if make_oof:\n        result_dict['oof'] = oof\n    \n    return result_dict","96b1171c":"lgb_params = {\n    'metric': 'rmse',\n    \"objective\" : \"regression\",\n    'min_data_in_leaf': 149, \n    'max_depth': 9,\n    \"boosting\": \"gbdt\",\n    \"lambda_l1\": 0.2634,\n    \"random_state\": 133,\n    \"num_leaves\" : 30,\n    \"min_child_samples\" : 100,\n    \"learning_rate\" : 0.1,\n    \"bagging_fraction\" : 0.7,\n    \"feature_fraction\" : 0.5,\n    \"bagging_frequency\" : 5,\n    \"bagging_seed\" : 4,\n    \"verbosity\" : -1\n    }\n\nresult_dict_lgb = train_model(X=train_X,\n                              X_test=test_X,\n                              y=train_y,\n                              params=lgb_params,\n                              model_type='lgb',\n                              plot_feature_importance=True,\n                              make_oof=True,\n                              num_class=0                           \n                             )","e50cf1b6":"test_lgb = test_df[['Qualification', 'Experience', 'Rating', 'Place', 'Profile','Miscellaneous_Info']].copy()\ntest_lgb['Fees'] = result_dict_lgb['prediction']","a2cf3e35":"xgb_params = {\n    \"learning_rate\" : 0.01,\n    \"n_estimators\" : 3400,\n    \"max_depth\" : 7,\n    \"min_child_weight\" : 0,\n    \"gamma\": 0,\n    \"subsample\" : 0.7,\n    \"colsample_bytree\" : 0.7,\n    \"objective\" : 'reg:linear',\n    \"nthread\" : -1,\n    \"scale_pos_weight\" : 1,\n    \"seed\" : 4,\n    \"reg_alpha\" : 0.00006\n}\n\nresult_dict_xgb = train_model(X=train_X,\n                              X_test=test_X,\n                              y=train_y,\n                              params=xgb_params,\n                              model_type='xgb',\n                              plot_feature_importance=True,\n                              make_oof=True,\n                              num_class=0\n                             )","8274b633":"# Create XGB prediction\n\ntest_xgb = test_df[['Qualification', 'Experience', 'Rating', 'Place', 'Profile','Miscellaneous_Info']].copy()\ntest_xgb['Fees'] = result_dict_xgb['prediction']","6bbb2236":"result_dict_cat = train_model(X=train_X,\n                              X_test=test_X,\n                              y=train_y,\n                              model_type='cat',\n                              plot_feature_importance=True,\n                              make_oof=True,\n                              num_class=0                           \n                             )","feddc2ee":"# Create CAT prediction\n\ntest_cat = test_df[['Qualification', 'Experience', 'Rating', 'Place', 'Profile','Miscellaneous_Info']].copy()\ntest_cat['Fees'] = result_dict_cat['prediction']","868a2c2d":"# Create X and Y dataset\n\nY = train_y.copy()\nX = train[cols_to_use]","78ca20b6":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout, BatchNormalization\nfrom keras.optimizers import Adam\n\n# Function to create model\ndef baseline_model(learn_rate=0.01, init_mode='normal', activation = 'relu', dropout_rate=0.1, weight_constraint=1, neurons = 100):\n    model = Sequential()\n    model.add(Dense(units = neurons, kernel_initializer = init_mode, activation = activation, input_dim = 29))\n    model.add(BatchNormalization())\n#     model.add(Dropout(rate = dropout_rate))\n    model.add(Dense(units = int(neurons \/ 2), kernel_initializer = init_mode, activation = activation))\n#     model.add(Dropout(rate = dropout_rate))\n    model.add(Dense(units = int(neurons \/ 4), kernel_initializer = init_mode, activation = activation))\n#     model.add(Dropout(rate = dropout_rate))\n    model.add(Dense(units = 1, kernel_initializer = init_mode, activation = 'linear'))\n    optimizer = Adam(lr=learn_rate)\n    model.compile(optimizer = optimizer, loss = 'mean_squared_logarithmic_error', metrics = ['mse'])\n    return model\n\n# fix random seed for reproducibility\nseed = 4\nnp.random.seed(seed)","eba73e35":"from keras.callbacks import ModelCheckpoint\n\ncheckpoint_name = 'weights.best.hdf5' \ncheckpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\ncallbacks_list = [checkpoint]\n\n# Train the model\nmodel = baseline_model()\nmodel.summary()\nmodel.fit(X, Y, epochs=100, batch_size=58, validation_split = 0.2, callbacks=callbacks_list)","311187b0":"weights_file = checkpoint_name # choose the best checkpoint \nmodel.load_weights(weights_file) # load it\nmodel.compile(loss='mean_absolute_error', optimizer=Adam(lr=0.01), metrics=['mean_absolute_error'])","86bb7785":"# Predictions\npred_dnn = model.predict(test[cols_to_use])","1d9a6205":"# Create submission\ntest_dnn = test_df[['Qualification', 'Experience', 'Rating', 'Place', 'Profile','Miscellaneous_Info']].copy()\ntest_dnn['Fees'] = pred_dnn","d4d240b8":"# Create average of LGB, XGB and DNN\n\ndf_test = test_df[['Qualification', 'Experience', 'Rating', 'Place', 'Profile','Miscellaneous_Info']].copy()\ndf_test[\"Fees\"] = (test_xgb[\"Fees\"] + test_lgb[\"Fees\"] + test_cat[\"Fees\"] + test_dnn[\"Fees\"])\/4","91d9f8fc":"test_lgb = mark_100(test_lgb.copy())\ntest_xgb = mark_100(test_xgb.copy())\ntest_cat = mark_100(test_cat.copy())\ntest_dnn = mark_100(test_dnn.copy())\ndf_test = mark_100(df_test.copy())","9e5fb5e6":"test_lgb.to_csv('submission_lgb.csv', index=False)\ntest_xgb.to_csv('submission_xgb.csv', index=False)\ntest_cat.to_csv('submission_cat.csv', index=False)\ntest_dnn.to_csv('submission_dnn.csv', index=False)\ndf_test.to_csv('submission_average.csv', index=False)","0e2af6a0":"While most of the records seems legit, but there are some recods with some unusual qualification text as **getinspiredbyremarkablestoriesofpeoplelikeyou** and all of them has Profile marked as **Dermatologists** with the Fees value as **100**. Lets check whether test set also contains such records?","7335b20e":"#### ANN for Regression","f62fb38c":"#### Define function to Separate multiple Qualifications into individual qualification columns","9937590a":"#### Lets cleanup inconsistencies of the **place** variable to fetch out City and localities","a2ad144f":"#### Let's check the Fees Distribution in Train Dataset","83538402":"**It can be observed that:**\n\n1. Few rows have NaN values\n1. Few rows give only the profile related info along with the fees of the Doctor ;)\n1. Other rows give info about the doctors rating followed by number of people rated then the address\n1. There are lot of commas in the value, Lets remove them to avoid any discrepancy","9a187797":"This Kernel is majorly inspired from [this](https:\/\/medium.com\/@supreetdeshpande95\/how-to-ace-your-first-hackathon-tutorial-in-python-e40b3d0204e8) article.","cbade515":"### Create a dictinary of Qualification with there respective codes","30205224":"#### Let's encode Locality feature of the test set and then merge into train set","ca35ae22":"#### Let's have a look at both dataset after encoding qualification columns","048dd5a6":"#### Let's limit the test data set to the features in the train set only. so that model would predict only on the trained features.","22424618":"#### Create the output files for the result submission","30c5e2ec":"#### Let's checkout the dataset after dummification of profile variable","7ba9b8ba":"### Let's assign category codes to the unique qualifications in the test dataset.","b66a4003":"It can be seen that there are records where years of experience is greater than 10 or 20 but the Fees is less than 50. Whereas some of the records have less than 1 years of experience even then the fees is more than 50. We cannot deny the fact that different qualification and profile demands different fees. But there are always exception in terms of charity or other services, so therefore we can ignore thee records and let them remain intact for the sake of model training","8dc4a188":"# Please **UPVOTE** if this kernel helped you in anyway :)","bfd97d07":"#### Capture CAT Predictions","de476047":"Lets check the years of experience distribution","ef1f67c3":"* It is observed that the Fees value is not consistent as some records contains more than one fee value.\n* It is also observed that the second fee value is just 120% value of the first fee value. \n* So we will take only the first fee value wherever available.","5f6df034":"#### Lets create a new column depicting the number of qualifications per doctor.","37b9fb1a":"#### We have to fill missing ratings with NaN values as 0% and convert the string type into integer to fetch actual value","6c3d1bb3":"### This is a practise work for one of the [MachineHack](https:\/\/www.machinehack.com\/course\/predict-a-doctors-consultation-fees-hackathon\/) Hackathons. \nIt required lot of string manipulation techniques to engineer the data before we feed into the model to predict the Doctor's consultation fees.\n\nThis tutorial doesn't cover exhaustive EDA . If you are looking for some EDA practise , Please refer to my other [kernel](https:\/\/www.kaggle.com\/nitin194\/nyc-taxi-trip-duration-prediction) ","ea282897":"#### Model training and prediction using K-Fold technique","1087e2be":"Let's prune up the profile variable and dummify them in both the datasets","2f226f42":"#### Let's check both datasets now for consistencies","2a8f4535":"#### We will encode qualifications only on the test data to ignore extra qualifications that are present in the train data but are not present in the test data. As the extra qualification in train data won't help model to predict fees in the test data. This is done to avoid any misfit while predicting test data after fitting on the train data due to uncommon category values.","d78196fa":"#### Since the number of Cities are less, We can dummify the city names. Lets do it","a1d115a9":"#### Let's default mark the Fees value as **100** for the selected records as decided","06e4752a":"It is observable that many records have Fees mentioned as less than 50. Lets check those records.","6c914758":"* **We observe that qualifications are not sorted due to leading whitespace**. \n* **We also need to remove other inconsistensies in the data like**\n    1. replace comma inside bracket by hyphen\n    2. lowercase all the words\n    3. remove spaces within qualification\n* **Lets work on them and sort the variables character wise.**","4aa11ddd":"It looks like almost similar distribution in the test set from the experience perspective. So we cannot ignore the records with minimum and maximum years of experience in the Train set.\nLet's check the records with 0 years of experience in the Train set","c4978181":"We can observe that min exp is being shown as 0 and max is 66. Both of them seems to be unusual. Lets check the experience distribution in Test set","677e5b9c":"**Let's check and count total number of unique qualifications in Train and test set**","9458f902":"#### Both dataset seems to be consistent. Let's replace all string and missing values in the selected columns to -1.","420f8622":"#### Lets define features to be used for model training and prediction.","f192702e":"### Lets sort and check for any unusual characters in Qualification","3ef1fe23":"We can observe that there are some records showing fees greater than or equal to 1000. These may pose as outliers, so lets expand them and have a look","b2600772":"#### Let's check that final record with **e** as the city name","0e02645e":"#### Very unusual observation. All the records in the Train set have Fees marked as default **100** for which the fees value inside misc column is more than or equal to **1000**. There can be two possible reasons here:\n1. The Fees value of 100 is wrongly\/mistakenly marked for all such records and we should ignore it\n2. The Fees value default of 100 is genuine marked for all such records for any presumed reason.\n\n#### Let's assume the first possibility for the sake of model and ignore it.So we have following thing to do.\n\n1. Mark the Fees default as **100** for all the predicted records which contains **getinspiredbyremarkablestoriesofpeoplelikeyou** as qualification. \n\nThis way we will be slight closer to the actual results which will lead to much better RMSLE for this hackathon.","49d46e9b":"Really strange distribution. Lets expland it to see unique fees values extracted from the misclaneous column","e4ae6131":"Let's check missing values in each column","5a659ded":"#### Capture XGB Results","e54eecfd":"#### Let's now check the unique Profile's in the dataset","a5571083":"#### Split the qualifications into different columns","71b8b264":"#### Now we will convert experience into integer value","d9c649ca":"#### Define a function to train different models","e4b25a02":"#### Coming to the messy column i.e Miscellaneous_Info Lets make the most out of this.","c0f9097f":"#### Separate City and Locality from Place variable","90a992ab":"### Now let's convert the Indian Rupee symbol to readable INR String","0dda4539":"#### Clearly its a one off record. Lets mark the Place, city and locality as missing instead of deleting the record.","f3ec3836":"Let's check whether we have such records in the Test set as well?","3bcf1d83":"Yes there are some records with similar qualification and profile. Lets see if we can get some more insight from the Fees values fetched from the Misclaneous column.","3d3d09c4":"#### Capture LGB results."}}