{"cell_type":{"8246c9f9":"code","89cb62d5":"code","3e589be1":"code","bdb55b82":"code","0d1cc7f7":"code","5fbeec2f":"code","c281f658":"code","9143387a":"code","5dd81e92":"code","c0b99a04":"code","7aa037ca":"code","b947542d":"code","9858109b":"code","21a4e21e":"code","cbbe8faf":"code","12e2db2d":"code","0d558f3a":"code","d6a352cd":"code","5b383f3b":"markdown","caba50bf":"markdown","5e28dab5":"markdown","728a4d3d":"markdown","3ee72d77":"markdown","cfd878c6":"markdown","aa2a9e0a":"markdown","a8b77a89":"markdown","2af61c00":"markdown","2129736e":"markdown","90c9c34e":"markdown","ee0284c2":"markdown","03f68aa7":"markdown"},"source":{"8246c9f9":"%matplotlib inline\n\nimport os\nimport shutil\nimport random\nimport torch\nimport torchvision\nimport numpy as np\n\nfrom PIL import Image\nfrom matplotlib import pyplot as plt\n\ntorch.manual_seed(0)","89cb62d5":"from distutils.dir_util import copy_tree\n\ndirectory ='..\/input\/covid19-radiography-database\/COVID-19_Radiography_Dataset'\n\nsource_dirs = ['Normal', 'Viral Pneumonia', 'COVID']\ninput_dir = '.\/train'\noutput_dir = '.\/test'\n\nnormal_dir = '..\/input\/covid19-radiography-database\/COVID-19_Radiography_Dataset\/Normal'\nviral_dir = '..\/input\/covid19-radiography-database\/COVID-19_Radiography_Dataset\/Viral Pneumonia'\ncovid_dir = '..\/input\/covid19-radiography-database\/COVID-19_Radiography_Dataset\/COVID'\n    \nif os.path.isdir(os.path.join(directory, source_dirs[1])):\n    if not os.path.isdir(input_dir):\n        os.mkdir(input_dir)             # Create Train Dir\n        \n        # Copy Classes to Train\n        copy_tree(normal_dir, input_dir + '\/Normal')\n        copy_tree(viral_dir, input_dir + '\/Viral Pneumonia')\n        copy_tree(covid_dir, input_dir + '\/COVID')\n        \n    if not os.path.isdir(output_dir):\n        os.mkdir(output_dir)            # Create Test Dir\n\n    for c in source_dirs:\n        if not os.path.isdir(os.path.join(output_dir, c)):\n            os.mkdir(os.path.join(output_dir, c))# Create Test Dir Classes\n            \n    for c in source_dirs:\n        images = [x for x in os.listdir(os.path.join(input_dir, c)) if x.lower().endswith('png')]\n        selected_images = random.sample(images, 30)\n        for image in selected_images:\n            source_path = os.path.join(input_dir, c, image)\n            target_path = os.path.join(output_dir, c, image)\n            shutil.move(source_path, target_path)\n","3e589be1":"class ChestXRayDataset(torch.utils.data.Dataset):\n    def __init__(self, image_dirs, transform):\n        def get_images(class_name):\n            images = [x for x in os.listdir(image_dirs[class_name]) if x.lower().endswith('png')]\n            print(f'Found {len(images)} {class_name} examples')\n            return images\n        \n        self.images = {}\n        self.class_names = ['Normal', 'Viral', 'COVID-19']\n        \n        for class_name in self.class_names:\n            self.images[class_name] = get_images(class_name)\n            \n        self.image_dirs = image_dirs\n        self.transform = transform\n        \n    \n    def __len__(self):\n        return sum([len(self.images[class_name]) for class_name in self.class_names])\n    \n    \n    def __getitem__(self, index):\n        class_name = random.choice(self.class_names)\n        index = index % len(self.images[class_name])\n        image_name = self.images[class_name][index]\n        image_path = os.path.join(self.image_dirs[class_name], image_name)\n        image = Image.open(image_path).convert('RGB')\n        return self.transform(image), self.class_names.index(class_name)","bdb55b82":"#Creating a Transformation Object\ntrain_transform = torchvision.transforms.Compose([\n    #Converting images to the size that the model expects\n    torchvision.transforms.Resize(size=(224,224)),\n    torchvision.transforms.RandomHorizontalFlip(), #A RandomHorizontalFlip to augment our data\n    torchvision.transforms.ToTensor(), #Converting to tensor\n    torchvision.transforms.Normalize(mean = [0.485, 0.456, 0.406],\n                                    std = [0.229, 0.224, 0.225]) #Normalizing the data to the data that the ResNet18 was trained on\n    \n])","0d1cc7f7":"#Creating a Transformation Object\ntest_transform = torchvision.transforms.Compose([\n    #Converting images to the size that the model expects\n    torchvision.transforms.Resize(size=(224,224)),\n# We don't do data augmentation in the test\/val set    \n    torchvision.transforms.ToTensor(), #Converting to tensor\n    torchvision.transforms.Normalize(mean = [0.485, 0.456, 0.406],\n                                    std = [0.229, 0.224, 0.225]) #Normalizing the data to the data that the ResNet18 was trained on\n    \n])","5fbeec2f":"train_dirs = {\n    'Normal': input_dir + '\/Normal',\n    'Viral': input_dir + '\/Viral Pneumonia',\n    'COVID-19': input_dir + '\/COVID'\n}\n\ntrain_dataset = ChestXRayDataset(train_dirs, train_transform)","c281f658":"test_dirs = {\n    'Normal': output_dir + '\/Normal',\n    'Viral': output_dir + '\/Viral Pneumonia',\n    'COVID-19': output_dir + '\/COVID'\n}\n\ntest_dataset = ChestXRayDataset(test_dirs, test_transform)","9143387a":"batch_size = 6\n\ndl_train = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\ndl_test = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n\nprint('Num of training batches', len(dl_train))\nprint('Num of test batches', len(dl_test))","5dd81e92":"class_names = train_dataset.class_names\n\ndef show_images(images,labels, preds):\n    plt.figure(figsize=(8,4))\n    \n    for i, image in enumerate(images):\n        plt.subplot(1, 6, i + 1, xticks = [], yticks =[]) # x & y ticks are set to blank\n        image = image.numpy().transpose((1, 2, 0)) # Channel first then height and width\n        mean = np.array([0.485, 0.456, 0.406])\n        std = np.array([0.229, 0.224, 0.225])\n        image = image * std + mean\n        image = np.clip(image, 0., 1.)\n        plt.imshow(image)\n        \n        col = 'green' if preds[i] == labels[i] else 'red'\n        \n        plt.xlabel(f'{class_names[int(labels[i].numpy())]}')\n        plt.ylabel(f'{class_names[int(preds[i].numpy())]}', color=col)\n    plt.tight_layout()\n    plt.show()","c0b99a04":"images, labels = next(iter(dl_train)) #Fetch the next batch of images\nshow_images(images, labels, labels)","7aa037ca":"images, labels = next(iter(dl_test))\nshow_images(images, labels, labels)","b947542d":"resnet18 = torchvision.models.resnet18(pretrained=True) #resnet18 is a small CNN that we can train quickly giving us decent results\n\n","9858109b":"#Changing the last fc to 3 output features\n\nresnet18.fc = torch.nn.Linear(in_features=512, out_features=3)\nloss_fn = torch.nn.CrossEntropyLoss()\n\noptimizer = torch.optim.Adam(resnet18.parameters(), lr=3e-5) #To find optimum learning rate 1. Trial\/Error 2. Hyperparameter search","21a4e21e":"def show_preds():\n    resnet18.eval()  #Setting the model to evaluation mode\n    images, labels = next(iter(dl_test))\n    outputs = resnet18(images)\n    _, preds = torch.max(outputs, 1)\n    show_images(images, labels, preds)","cbbe8faf":"show_preds()","12e2db2d":"def train(epochs):\n    print('Starting training..')\n    for e in range(0, epochs):\n        print('='*20)\n        print(f'Starting epoch {e + 1}\/{epochs}')\n        print('='*20)\n\n        train_loss = 0.\n        val_loss = 0.  #Not computing val_loss since we'll be evaluating the model multiple times within one epoch\n        \n        resnet18.train() # set model to training phase\n        \n        for train_step, (images, labels) in enumerate(dl_train):\n            optimizer.zero_grad()\n            outputs = resnet18(images)\n            loss = loss_fn(outputs, labels)\n            #Once we get the loss we need to take a gradient step\n            loss.backward() #Back propogation\n            optimizer.step() #Completes the gradient step by updating all the parameter values(We are using all parameters)\n            train_loss += loss.item() #Loss is a tensor which can't be added to train_loss so .item() converts it to float\n            \n            #Evaluating the model every 20th step\n            if train_step % 20 == 0:\n                print('Evaluating at step', train_step)\n\n                accuracy = 0\n\n                resnet18.eval() # set model to eval phase\n\n                for val_step, (images, labels) in enumerate(dl_test):\n                    outputs = resnet18(images)\n                    loss = loss_fn(outputs, labels)\n                    val_loss += loss.item()\n\n                    _, preds = torch.max(outputs, 1) # 1 corresponds to the values and ) corresponds to the no of examples\n                    accuracy += sum((preds == labels).numpy()) #adding correct preds to acc\n\n                val_loss \/= (val_step + 1) # 15 test batches so this logic gives the value for each step\n                accuracy = accuracy\/len(test_dataset)\n                print(f'Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}')\n\n                show_preds()\n\n                resnet18.train()\n\n                if accuracy >= 0.95:\n                    print('Performance condition satisfied, stopping..')\n                    return\n\n        train_loss \/= (train_step + 1)\n\n        print(f'Training Loss: {train_loss:.4f}')\n    print('Training complete..')\n","0d558f3a":"%%time\n\ntrain(epochs=1)","d6a352cd":"show_preds()","5b383f3b":"## Did you find this Notebook useful \ud83e\udd14?     Show your appreciation with an upvote \u263a\ufe0f","caba50bf":"### Final Results","5e28dab5":"![](https:\/\/media.makeameme.org\/created\/mmmmmmmm-good-results.jpg)\n# What did we find?\n* Pooled results showed that chest X-ray correctly diagnosed COVID-19 in 80.6% of people who had COVID-19. \n* However, it incorrectly identified COVID-19 in 28.5% of people who did not have COVID-19.\n\n[Read more](https:\/\/www.cochrane.org\/CD013639\/INFECTN_how-accurate-chest-imaging-diagnosing-covid-19)","728a4d3d":"### Prepare DataLoader\n","3ee72d77":"### Image Transformations","cfd878c6":"# How Accurate is Chest Imaging for Diagnosing COVID-19?\n\n![](https:\/\/www.iaea.org\/sites\/default\/files\/styles\/third_page_width_portrait_2_3\/public\/chestxray.png?itok=z07bhz5h&c=b64d5f5d9a54f1cfd815edc18198844a)\n\n* People with suspected COVID-19 need to know quickly whether they are infected, so they can receive appropriate treatment, self-isolate, and inform close contacts.\n* Currently, a formal diagnosis of COVID-19 requires a laboratory test (RT-PCR) of nose and throat samples. RT-PCR requires specialist equipment and takes at least 24 hours to produce a result. It is not completely accurate, and may require a second RT-PCR or a different test to confirm diagnosis.\n* COVID-19 is a respiratory disease. Clinicians may use chest imaging to diagnose people who have COVID-19 symptoms, while awaiting RT-PCR results or when RT-PCR results are negative, and the person has COVID-19 symptoms.","aa2a9e0a":"### Creating Custom Dataset","a8b77a89":"### Importing Libraries","2af61c00":"### Preparing Training and Test Sets","2129736e":"#### We wanted to know whether chest imaging is accurate enough to diagnose COVID-19 in people with suspected infection. ","90c9c34e":"### Data Visualization","ee0284c2":"### Training the Model","03f68aa7":"### Creating the Model"}}