{"cell_type":{"70f44f99":"code","94c5a325":"code","2399f81f":"code","e79bf936":"code","abb6118b":"code","89a89e79":"code","a5b6e46d":"code","e606eac9":"code","31bfdbe9":"code","df799db5":"code","c0bb2b5b":"code","d9ff3af4":"code","c8ed5da4":"code","c17a2f1d":"code","665e360d":"code","c84787b3":"code","4cd1c95d":"code","c367f1d3":"code","e030805b":"code","7c80cf85":"code","3ed4f5dc":"code","a321fcf1":"code","3227a2b9":"code","e62644a6":"code","5d9dae21":"code","21a0384d":"code","0b2e361f":"code","e5ff135a":"code","bc8052b2":"code","05236713":"code","b95d3c3d":"code","01a3dec4":"code","37f58169":"code","e49b4c96":"code","2a784bbf":"code","389cddf0":"code","962d2bf6":"code","f8745996":"code","69ff56fb":"code","80130164":"code","161711ca":"code","de03c8f4":"code","2262b9e9":"code","bd807325":"code","c443f008":"code","35fe548a":"code","41650fd3":"code","b11839ef":"code","d4479752":"code","587fd3fb":"markdown","f288fc7a":"markdown","7329da98":"markdown","6628f895":"markdown","fff30b8c":"markdown","ddcd008c":"markdown","094c2ba8":"markdown","ec3e2cb7":"markdown","138c1b3f":"markdown","4461f39e":"markdown","220f2356":"markdown","578d1943":"markdown","53503edc":"markdown","a0d0d00b":"markdown","da05c53d":"markdown","b6892ca0":"markdown","ade76d60":"markdown","45b7995d":"markdown","11731922":"markdown","63b37262":"markdown","91208990":"markdown","b4b97379":"markdown","389afe55":"markdown","db11ab19":"markdown","1ff81b49":"markdown","4e13a628":"markdown","2ee43ccf":"markdown","e5757e9d":"markdown","87fc6e96":"markdown","f73e8a17":"markdown","bb1514a5":"markdown","332bb662":"markdown"},"source":{"70f44f99":"'''Import basic modules'''\nimport pandas as pd\nimport numpy as np\nimport string\n\n'''import visualization'''\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n%matplotlib inline\n\n'''Plotly visualization .'''\nimport plotly.offline as py\nimport plotly.graph_objs as go\nimport plotly.tools as tls\npy.init_notebook_mode(connected=True)\n\n'''Display markdown formatted output like bold, italic bold etc.'''\nfrom IPython.display import Markdown\ndef bold(string):\n    display(Markdown(string))\n\n'''Ignore deprecation and future, and user warnings.'''\nimport warnings as wrn\nwrn.filterwarnings('ignore', category = DeprecationWarning) \nwrn.filterwarnings('ignore', category = FutureWarning) \nwrn.filterwarnings('ignore', category = UserWarning) ","94c5a325":"'''Read in train and test data from csv files'''\ntrain = pd.read_csv('..\/input\/cat-in-the-dat\/train.csv')\ntest = pd.read_csv('..\/input\/cat-in-the-dat\/test.csv')","2399f81f":"'''Train and test data at a glance.'''\nbold('**Preview of Train Data:**')\ndisplay(train.head(3))\nbold('**Preview of Test Data:**')\ndisplay(test.head(3))","e79bf936":"'''Dimension of train and test data'''\nbold('**Shape of our train and test data**')\nprint('Dimension of train:',train.shape) \nprint('Dimension of test:',test.shape)","abb6118b":"'''Variable Description'''\ndef description(df):\n    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name','dtypes']]\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n    summary['First Value'] = df.iloc[0].values\n    summary['Second Value'] = df.iloc[1].values\n    summary['Third Value'] = df.iloc[2].values\n    return summary","89a89e79":"bold('**Variable Description of  train Data:**')\ndescription(train)","a5b6e46d":"bold('**Variable Description of  train Data:**')\ndescription(test)","e606eac9":"''' #1.Function for displaying bar labels in absolute scale.'''\ndef abs_bar_labels():\n    font_size = 15\n    plt.ylabel('Absolute Frequency', fontsize = font_size)\n    plt.xticks(rotation = 0, fontsize = font_size)\n    plt.yticks([])\n    \n    # Set individual bar lebels in absolute number\n    for x in ax.patches:\n        ax.annotate(x.get_height(), \n        (x.get_x() + x.get_width()\/2., x.get_height()), ha = 'center', va = 'center', xytext = (0, 7), \n        textcoords = 'offset points', fontsize = font_size, color = 'black')\n    \n'''#2.Function for displaying bar lebels in relative scale.'''\ndef pct_bar_labels():\n    font_size = 15\n    plt.ylabel('Relative Frequency (%)', fontsize = font_size)\n    plt.xticks(rotation = 0, fontsize = font_size)\n    plt.yticks([]) \n    \n    # Set individual bar lebels in proportional scale\n    for x in ax1.patches:\n        ax1.annotate(str(x.get_height()) + '%', \n        (x.get_x() + x.get_width()\/2., x.get_height()), ha = 'center', va = 'center', xytext = (0, 7), \n        textcoords = 'offset points', fontsize = font_size, color = 'black')\n         \n'''#3.Function to create a dataframe of absolute and relative frequency of each variable. And plot absolute and relative frequency.'''\ndef absolute_and_relative_freq(variable):\n    global  ax, ax1 \n    # Dataframe of absolute and relative frequency\n    absolute_frequency = variable.value_counts()\n    relative_frequency = round(variable.value_counts(normalize = True)*100, 2)\n    # Was multiplied by 100 and rounded to 2 decimal points for percentage.\n    df = pd.DataFrame({'Absolute Frequency':absolute_frequency, 'Relative Frequency(%)':relative_frequency})\n    print('Absolute & Relative Frequency of',variable.name,':')\n    display(df)\n    \n    # This portion plots absolute frequency with bar labeled.\n    fig_size = (18,5)\n    font_size = 15\n    title_size = 18\n    ax =  absolute_frequency.plot.bar(title = 'Absolute Frequency of %s' %variable.name, figsize = fig_size)\n    ax.title.set_size(title_size)\n    abs_bar_labels()  # Displays bar labels in abs scale.\n    plt.show()\n    \n    # This portion plots relative frequency with bar labeled.\n    ax1 = relative_frequency.plot.bar(title = 'Relative Frequency of %s' %variable.name, figsize = fig_size)\n    ax1.title.set_size(title_size)\n    pct_bar_labels() # Displays bar labels in relative scale.\n    plt.show()","31bfdbe9":"'''Plot and count the target variable in absolute and relative scale'''\nabsolute_and_relative_freq(train.target)","df799db5":"'''#4.Create a function that relative frequency of Target variable by a categorical variable. And then plots the relative frequency of target by a categorical variable.'''\ndef crosstab(cat, cat_target, color):\n    '''cat = categorical variable, cat_target = our target categorical variable.'''\n    global ax1\n    fig_size = (18, 5)\n    title_size = 18\n    font_size = 15\n    cat_grouped_by_cat_target = pd.crosstab(index = cat, columns = cat_target)\n    pct_cat_grouped_by_cat_target = round(pd.crosstab(index = cat, columns = cat_target, normalize = 'index')*100, 2)\n       \n    # Plot relative frequrncy of Target by a categorical variable\n    ax1 = pct_cat_grouped_by_cat_target.plot.bar(color = color, title = 'Percentage Count of target by %s' %cat.name, figsize = fig_size)\n    ax1.title.set_size(fontsize = title_size)\n    pct_bar_labels()\n    plt.xlabel(cat.name, fontsize = font_size)\n    plt.show()","c0bb2b5b":"'''Plot the binary variables in relative scale'''\n\nbold('**Percentage Count of target by bin_0:**')\ncrosstab(train.bin_0, train.target, color = ['r', 'g'])\n\nbold('**Percentage Count of target by bin_1:**')\ncrosstab(train.bin_1, train.target, color = ['r', 'g'])\n\nbold('**Percentage Count of target by bin_2:**')\ncrosstab(train.bin_2, train.target, color = ['r', 'g'])\n\nbold('**Percentage Count of target by bin_3:**')\ncrosstab(train.bin_3, train.target, color = ['r', 'g'])\n\nbold('**Percentage Count of target by bin_4:**')\ncrosstab(train.bin_4, train.target, color = ['r', 'g'])","d9ff3af4":"'''Plot the nominal variables in relative scale'''\n\nbold('**Percentage Count of target by nom_0:**')\ncrosstab(train.nom_0, train.target, color = ['b', 'y'])\n\nbold('**Percentage Count of target by nom_1:**')\ncrosstab(train.nom_1, train.target, color = ['b', 'y'])\n\nbold('**Percentage Count of target by nom_2:**')\ncrosstab(train.nom_2, train.target, color = ['b', 'y'])\n\nbold('**Percentage Count of target by nom_3:**')\ncrosstab(train.nom_3, train.target, color = ['b', 'y'])\n\nbold('**Percentage Count of target by nom_4:**')\ncrosstab(train.nom_4, train.target, color = ['b', 'y'])","c8ed5da4":"'''Plot the Ordinal variables in relative scale'''\n\nbold('**Percentage Count of target by ord_0:**')\ncrosstab(train.ord_0, train.target, color = ['c', 'm'])\n\nbold('**Percentage Count of target by ord_1:**')\ncrosstab(train.ord_1, train.target, color = ['c', 'm'])\n\nbold('**Percentage Count of target by ord_2:**')\ncrosstab(train.ord_2, train.target, color = ['c', 'm'])\n\nbold('**Percentage Count of target by ord_3:**')\ncrosstab(train.ord_3, train.target, color = ['c', 'm'])","c17a2f1d":"def crosstab(cat, cat_target, color):\n    '''cat = categorical variable, cat_target = our target categorical variable.'''\n    global ax1\n    fig_size = (28, 6)\n    title_size = 18\n    font_size = 15\n    cat_grouped_by_cat_target = pd.crosstab(index = cat, columns = cat_target)\n    pct_cat_grouped_by_cat_target = round(pd.crosstab(index = cat, columns = cat_target, normalize = 'index')*100, 2)\n       \n    # Plot relative frequrncy of target by a categorical variable\n    ax1 = pct_cat_grouped_by_cat_target.plot.bar(color = color, title = 'Percentage Count of target by %s' %cat.name, figsize = fig_size)\n    ax1.title.set_size(fontsize = title_size)\n    pct_bar_labels()\n    plt.xlabel(cat.name, fontsize = font_size)\n    plt.show()","665e360d":"'''Plot the Ord_4 and ord_5 variables in relative scale'''\nbold('**Percentage Count of target by ord_4:**')\ncrosstab(train.ord_4, train.target, color = ['c', 'm'])\n\nbold('**Percentage Count of target by ord_5:**')\ncrosstab(train['ord_5'].head(26), train.target, color = ['c', 'm'])","c84787b3":"'''Distribution plot of ord_5'''\nord_5_count = train['ord_5'].value_counts().reset_index()['ord_5'].values\nplt.figure(figsize=(12,6))\n\nax = sns.distplot(ord_5_count, bins= 50, color = 'k')\nax.set_title(\"Distribution of ord_5 category values\", fontsize=22)\nax.set_xlabel(\"Total of entries in ord_5 category's\", fontsize=18)\nax.set_ylabel(\"Density\", fontsize=18)\n\nplt.show()","4cd1c95d":"'''Distribution plot of day'''\nplt.figure(figsize=(12,6))\n\nax = sns.distplot(train.day, color = 'g', kde = False)\nax.set_title(\"Distribution of day category values\", fontsize=22)\nax.set_xlabel(\"Total of entries in day category's\", fontsize=18)\nax.set_ylabel(\"Density\", fontsize=18)\n\nplt.show()","c367f1d3":"'''Distribution plot of month'''\nplt.figure(figsize=(12,6))\n\nax = sns.distplot(train.month, color = 'r', kde = False)\nax.set_title(\"Distribution of month category values\", fontsize=22)\nax.set_xlabel(\"Total of entries in month category's\", fontsize=18)\nax.set_ylabel(\"Density\", fontsize=18)\n\nplt.show()","e030805b":"'''Binary encoding of bin_3 and bin_4 of train data and test data'''\ntrain['bin_3'] = train['bin_3'].apply(lambda x: 0 if x == 'F' else 1)\ntrain['bin_4'] = train['bin_4'].apply(lambda x: 0 if x == 'N' else 1)\n\ntest['bin_3'] = test['bin_3'].apply(lambda x: 0 if x == 'F' else 1)\ntest['bin_4'] = test['bin_4'].apply(lambda x: 0 if x == 'N' else 1)","7c80cf85":"'''Now extract the nominal variables for one hot encoding of train and test data.'''\none_hot_train = pd.get_dummies(train[['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4']], drop_first = True)\n\none_hot_test = pd.get_dummies(test[['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4']], drop_first = True)\n\n'''Droping the variables'''\ntrain.drop(['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4'], axis=1, inplace=True)\ntest.drop(['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4'], axis=1, inplace=True)","3ed4f5dc":"#'''Using sklearn bulit funtion for One Hot encoding'''\n#from sklearn.preprocessing import OneHotEncoder\n\n#for f in ['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4']:\n#    if train[f].dtype == 'object' or test[f].dtype == 'object':\n#        ohe = OneHotEncoder()\n#        ohe.fit(train[f].values.reshape(-1, 1)) #fiting the data\n#        ohe_train = ohe.transform(train[f].values.reshape(-1, 1)) #transforming the data\n#        ohe_train = pd.DataFrame(ohe_train.toarray()) #convert sparse matrix to dataframe\n#        \n#        ohe.fit(test[f].values.reshape(-1, 1))\n#        ohe_test = ohe.transform(test[f].values.reshape(-1, 1))\n#        ohe_test = pd.DataFrame(ohe_test.toarray())\n        ","a321fcf1":"#'''Using sklearn's FeatureHasher to encode High Cardinality Features'''\n#from sklearn.feature_extraction import FeatureHasher\n#for f in ['nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']:\n#    if train[f].dtype == 'object' or test[f].dtype == 'object':\n#        h = FeatureHasher(input_type='string', n_features=1000)\n#        h.fit(train[f].values) #fiting the data\n#        hash_train = h.transform(train[f].values) #transforming the data\n#        hash_train = pd.DataFrame(hash_train.toarray()) #convert sparse matrix to dataframe\n        \n#        h.fit(test[f].values) #fiting the data\n#       hash_test = h.transform(test[f].values) #transforming the data\n#       hash_test = pd.DataFrame(hash_test.toarray())","3227a2b9":"#'''Another hashing trick'''\n#I got this solution from @kabure and @Giba\n#high_card = ['nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']\n#for col in high_card:\n#    train[f'hash_{col}'] = train[col].apply( lambda x: hash(str(x)) % 5000 )\n#    test[f'hash_{col}'] = test[col].apply( lambda x: hash(str(x)) % 5000 )  ","e62644a6":"high_card = ['nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9'] \nfor col in high_card:\n    enc_nom_1 = (train.groupby(col).size()) \/ len(train)\n    train[f'freq_{col}'] = train[col].apply(lambda x : enc_nom_1[x])\n    enc_nom_2 = (test.groupby(col).size()) \/ len(test)\n    test[f'freq_{col}'] = test[col].apply(lambda x : enc_nom_2[x])\n\n    '''Droping the variables'''\ntrain.drop(['nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9'], axis=1, inplace=True)\ntest.drop(['nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9'], axis=1, inplace=True)","5d9dae21":"#from sklearn.preprocessing import LabelEncoder\n\n#for f in ['nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']:\n#    if train[f].dtype=='object' or test[f].dtype=='object': \n#        lbl = LabelEncoder()\n#        lbl.fit(list(train[f].values) + list(test[f].values))\n#        train[f'le_{f}'] = lbl.transform(list(train[f].values))\n#        test[f'le_{f}'] = lbl.transform(list(test[f].values))","21a0384d":"\"\"\"let's begin the manual process of ordinal encoding.\"\"\"\n# ordinal encoding on train data\ntrain.ord_1.replace(to_replace = ['Novice', 'Contributor','Expert', 'Master', 'Grandmaster'],\n                         value = [0, 1, 2, 3, 4], inplace = True)\n\ntrain.ord_2.replace(to_replace = ['Freezing', 'Cold', 'Warm', 'Hot','Boiling Hot', 'Lava Hot'],\n                         value = [0, 1, 2, 3, 4, 5], inplace = True)\n\ntrain.ord_3.replace(to_replace = ['a', 'b', 'c', 'd', 'e', 'f', 'g','h', 'i', 'j', 'k', 'l', 'm', 'n', 'o'],\n                         value = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], inplace = True)\n\ntrain.ord_4.replace(to_replace = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I','J', 'K', 'L', 'M', 'N', 'O', \n                                     'P', 'Q', 'R','S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'],\n                         value = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, \n                                  22, 23, 24, 25], inplace = True)\n\n# ordinal encoding on test data\ntest.ord_1.replace(to_replace = ['Novice', 'Contributor','Expert', 'Master', 'Grandmaster'],\n                         value = [0, 1, 2, 3, 4], inplace = True)\n\ntest.ord_2.replace(to_replace = ['Freezing', 'Cold', 'Warm', 'Hot','Boiling Hot', 'Lava Hot'],\n                         value = [0, 1, 2, 3, 4, 5], inplace = True)\n\ntest.ord_3.replace(to_replace = ['a', 'b', 'c', 'd', 'e', 'f', 'g','h', 'i', 'j', 'k', 'l', 'm', 'n', 'o'],\n                         value = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], inplace = True)\n\ntest.ord_4.replace(to_replace = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I','J', 'K', 'L', 'M', 'N', 'O', \n                                       'P', 'Q', 'R','S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'],\n                         value = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, \n                                  22, 23, 24, 25], inplace = True)","0b2e361f":"#'''Using sklearn's OrdinalEncoder to encode High Cardinality Features'''\n#from sklearn.preprocessing import OrdinalEncoder\n#oe = OrdinalEncoder(categories='auto')\n#oe.fit(train.ord_5.values.reshape(-1, 1))\n#train.ord_5 = oe.transform(train.ord_5.values.reshape(-1, 1))\n#test.ord_5 = oe.transform(test.ord_5.values.reshape(-1, 1))","e5ff135a":"'''Then encode 'ord_5' using ACSII values'''\n# Source:- https:\/\/www.kaggle.com\/c\/cat-in-the-dat\/discussion\/105702#latest-607652\n\n'''Option 1: Add up the indices of two letters in string.ascii_letters'''\n# train['ord_5_oe_add'] = train['ord_5'].apply(lambda x:sum([(string.ascii_letters.find(letter)+1) for letter in x]))\n# test['ord_5_oe_add'] = test['ord_5'].apply(lambda x:sum([(string.ascii_letters.find(letter)+1) for letter in x]))\n\n'''Option 2: Join the indices of two letters in string.ascii_letters'''\n#train['ord_5_oe_join'] = train['ord_5'].apply(lambda x:float(''.join(str(string.ascii_letters.find(letter)+1) for letter in x)))\n# test['ord_5_oe_join'] = test['ord_5'].apply(lambda x:float(''.join(str(string.ascii_letters.find(letter)+1) for letter in x)))\n\n'''Option 3: Split 'ord_5' into two new columns using the indices of two letters in string.ascii_letters, separately'''\ntrain['ord_5_oe1'] = train['ord_5'].apply(lambda x:(string.ascii_letters.find(x[0])+1))\ntest['ord_5_oe1'] = test['ord_5'].apply(lambda x:(string.ascii_letters.find(x[0])+1))\n\ntrain['ord_5_oe2'] = train['ord_5'].apply(lambda x:(string.ascii_letters.find(x[1])+1))\ntest['ord_5_oe2'] = test['ord_5'].apply(lambda x:(string.ascii_letters.find(x[1])+1))\n\n'''Option 4: Simply sort their values by string of ord_5'''\nord_5 = sorted(list(set(train['ord_5'].values)))\nord_5 = dict(zip(ord_5, range(len(ord_5))))\ntrain.loc[:, 'ord_5'] = train['ord_5'].apply(lambda x: ord_5[x]).astype(float)\ntest.loc[:, 'ord_5'] = test['ord_5'].apply(lambda x: ord_5[x]).astype(float)","bc8052b2":"'''Transfer the cyclical features into two dimensional sin-cos features'''\n# source:  https:\/\/www.kaggle.com\/avanwyk\/encoding-cyclical-features-for-deep-learning\ndef cyclical_encode(data, col, max_val):\n    data[col + '_sin'] = np.sin(2 * np.pi * data[col]\/max_val)\n    data[col + '_cos'] = np.cos(2 * np.pi * data[col]\/max_val)\n    return data\n\ntrain = cyclical_encode(train, 'day', 7)\ntest = cyclical_encode(test, 'day', 7) \n\ntrain = cyclical_encode(train, 'month', 12)\ntest = cyclical_encode(test, 'month', 12)","05236713":"'''Transfer the dtypes of encoded ordinal features into float64'''\nfor col in ['ord_0', 'ord_1', 'ord_2', 'ord_3', 'ord_4', 'ord_5_oe1', 'ord_5_oe2', 'ord_5']:\n    train[col]= train[col].astype('float64')\n    test[col]= test[col].astype('float64')","b95d3c3d":"\"\"\"Let's concate one hot encoded, cyclic encoded variables together.\"\"\"\ntrain_processed = pd.concat([one_hot_train, train], axis = 1)\ntest_processed = pd.concat([one_hot_test, test], axis = 1)","01a3dec4":"\n\"\"\"Let's look at our final train and test data for modelling.\"\"\"\nbold('**Updated train data for modelling:**')\ndisplay(train_processed.head(3))\nbold('**Updated test data for modelling:**')\ndisplay(test_processed.head(3))","37f58169":"'''Function to reduce the DF size'''\n# source: https:\/\/www.kaggle.com\/kernels\/scriptcontent\/3684066\/download\n\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","e49b4c96":"train_processed = reduce_mem_usage(train_processed)\ntest_processed = reduce_mem_usage(test_processed)","2a784bbf":"'''Seting X and Y'''\nX_train = train_processed.drop(['id', 'target'],axis = 1)\ny_train = train_processed['target']\nX_test = test_processed.drop(['id'], axis = 1)\n","389cddf0":"\"\"\"Let's have a final look at our data\"\"\"\nbold('**Data Dimension for Model Building:**')\nprint('Input matrix dimension:', X_train.shape)\nprint('Output vector dimension:',y_train.shape)\nprint('Test data dimension:', X_test.shape)","962d2bf6":"'''Importing the auxiliar and preprocessing librarys'''\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import GridSearchCV, KFold, StratifiedKFold, cross_val_score\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.pipeline import Pipeline\n\n'''Initialize all the regression models object we are interested in.'''\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom xgboost import XGBClassifier","f8745996":"'''Spot-Check Algorithms'''\ndef GetBasedModel():\n    basedModels = []\n    \n    basedModels.append(('LR',\n                        Pipeline([('Scaler', StandardScaler()), \n                                   ('LR', LogisticRegression())])))\n    basedModels.append(('RC',\n                        Pipeline([('Scaler', StandardScaler()),\n                                   ('RC', RidgeClassifier())])))\n    basedModels.append(('LDA', \n                        Pipeline([('Scaler', StandardScaler()),\n                                   ('LDA', LinearDiscriminantAnalysis())])))\n    #basedModels.append(('KNN'  , KNeighborsClassifier()))\n    #basedModels.append(('DT' , DecisionTreeClassifier()))\n    #basedModels.append(('NB'   , GaussianNB()))\n    basedModels.append(('AB'   , AdaBoostClassifier()))\n    basedModels.append(('GBM'  , GradientBoostingClassifier()))\n    basedModels.append(('RF'   , RandomForestClassifier()))\n    basedModels.append(('ET'   , ExtraTreesClassifier()))\n    basedModels.append(('XGB'   , XGBClassifier()))\n    \n    return basedModels\n\n\n'''Test options and evaluation metric'''\ndef BasedLine(X_train, y_train, models):\n    num_folds = 7\n    scoring = 'accuracy'\n\n    results, names = [], []\n\n    for name, model in models:\n        kfold = StratifiedKFold(n_splits = num_folds, random_state = 44)\n        cv_results = cross_val_score(model, X_train, y_train, cv = kfold, \n                                scoring = scoring, n_jobs = -1)\n        names.append(name)\n        results.append(cv_results)\n        msg = \"%s: %f (+\/- %f)\" % (name, cv_results.mean(),  \n                                   cv_results.std())\n        print(msg)\n    \n    return names, results\n    \n    \n'''Visualization of results'''\nclass PlotBoxR(object):\n    \n    \n    def __Trace(self,nameOfFeature,value): \n    \n        trace = go.Box(\n            y=value,\n            name = nameOfFeature,\n            marker = dict(\n                color = 'rgb(0, 128, 128)',\n            )\n        )\n        return trace\n\n    def PlotResult(self,names,results):\n        \n        data = []\n\n        for i in range(len(names)):\n            data.append(self.__Trace(names[i],results[i]))\n\n\n        py.iplot(data)","69ff56fb":"models = GetBasedModel()\nnames,results = BasedLine(X_train, y_train,models)\nPlotBoxR().PlotResult(names,results)","80130164":"'''Create a function to tune hyperparameters of the selected models.'''\nseed = 44\ndef grid_search_cv(model, params):\n    global best_params, best_score\n    grid_search = GridSearchCV(estimator = model, param_grid = params, cv = 10, verbose = 3,\n                             scoring = 'accuracy', n_jobs = -1)\n    grid_search.fit(X_train, y_train)\n    best_params = grid_search.best_params_\n    best_score = grid_search.best_score_\n    \n    return best_params, best_score","161711ca":"'''Define hyperparameters of Logistic Regression.'''\nLR_model = LogisticRegression()\n\nLR_params = {'penalty':['l1', 'l2'],\n             'C': np.logspace(0, 2, 4, 8 ,10)}\n\ngrid_search_cv(LR_model, LR_params)\nLR_best_params, LR_best_score = best_params, best_score\nprint('LR best params:{} & best_score:{:0.5f}' .format(LR_best_params, LR_best_score))","de03c8f4":"'''Define hyperparameters of Ridge Classifier.'''\nRC_model = RidgeClassifier()\n\nRC_params = {'alpha':[ 9, 9.2, 9.4, 9.5, 9.52, 9.54, 9.56, 9.58, 9.6, 9.62, 9.64, 9.66, 9.68, 9.7,  9.8],\n             'random_state':[seed]}\n\ngrid_search_cv(RC_model, RC_params)\nRC_best_params, RC_best_score = best_params, best_score\nprint('Ridge best params:{} & best_score:{:0.5f}' .format(RC_best_params, RC_best_score))","2262b9e9":"'''Define hyperparameters of Linear Discriminant Analysis.'''\nLDA_model = LinearDiscriminantAnalysis()\n\nLDA_params = {'solver': ['svd', 'lsqr', 'eigen'],\n              }\n\ngrid_search_cv(LDA_model, LDA_params)\nLDA_best_params, LDA_best_score = best_params, best_score\nprint('LDA best params:{} & best_score:{:0.5f}' .format(LDA_best_params, LDA_best_score))","bd807325":"'''Define hyperparameters of AdaBoostClassifier.'''\nAB_model = AdaBoostClassifier()\n\nAB_params = {'n_estimators': (50 , 100),\n             'learning_rate': (1.0, 3.0),\n             'algorithm': ['SAMME', 'SAMME.R']}\n\ngrid_search_cv(AB_model, AB_params)\nAB_best_params, AB_best_score = best_params, best_score\nprint('AB best params:{} & best_score:{:0.5f}' .format(AB_best_params, AB_best_score))","c443f008":"#'''Define hyperparameters of Gradient Boosting Classifier'''\n#GBC_model = GradientBoostingClassifier()\n\n#GBC_params = {'learning_rate': [0.02, 0.05, 0.1],\n#              'max_depth': [4, 6],\n#              'max_features': [1.0, 0.3], \n#              'min_samples_split': [ 2, 3],\n#              'random_state':[seed]}\n\n#grid_search_cv(GBC_model, GBC_params)\n#GBC_best_params, GBC_best_score = best_params, best_score\n#print('GBC best params:{} & best_score:{:0.5f}' .format(GBC_best_params, GBC_best_score))","35fe548a":"#'''For XGBC, the following hyperparameters are usually tunned.'''\n#'''https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html'''\n\n#XGB_model = XGBClassifier(\n#            n_estimators=500,\n#            verbose = True)\n\n\n#XGB_params = {'max_depth': (2, 5),\n#               'reg_alpha':  (0.01, 0.4),\n#               'reg_lambda': (0.01, 0.4),\n#               'learning_rate': (0.1, 0.4),\n#               'colsample_bytree': (0.3, 1),\n#               'gamma': (0.01, 0.7),\n#               'num_leaves': (2, 5),\n#               'min_child_samples': (1, 5),\n#              'subsample': [0.5, 0.8],\n#              'random_state':[seed]}\n\n#grid_search_cv(XGB_model, XGB_params)\n#XGB_best_params, XGB_best_score = best_params, best_score\n#print('XGB best params:{} & best_score:{:0.5f}' .format(XGB_best_params, XGB_best_score))","41650fd3":"'''Instantiate the models with optimized hyperparameters.'''\nlr = LogisticRegression(**LR_best_params)\nrc = RidgeClassifier(**RC_best_params)\nlda = LinearDiscriminantAnalysis(**LDA_best_params)\nab = AdaBoostClassifier(**AB_best_params)","b11839ef":"'''Instantiate the models with optimized hyperparameters.'''\nmodels = {'LR': lr, 'RC': rc, 'LDA': lda, 'AB': ab}\n\n# 10-fold Cross Validation after Optimization\nscore = []\nfor x, (keys, items) in enumerate(models.items()):\n    # Train the models with optimized parameters using cross validation.\n    # No need to fit the data. cross_val_score does that for us.\n    # But we need to fit train data for prediction in the follow session.\n    items.fit(X_train, y_train)\n    scores = cross_val_score(items, X_train, y_train, cv = 10, scoring = 'accuracy')\n    score.append(scores.mean())\n    print('Mean Accuracy: %0.4f (+\/- %0.4f) [%s]'  % (scores.mean(), scores.std(), keys))","d4479752":"'''Submission with the most accurate logistic regression'''\nsubmission = pd.DataFrame({'id': test['id'],\n                           'target': lr.predict_proba(X_test)[:,1] })\nsubmission.to_csv('submission_lr.csv', index = False)\n\n'''Submission with the most accurate Ridge Classifier'''\nsubmission  = pd.DataFrame({'id': test['id'],\n                            'target': rc.predict(X_test)})\nsubmission.to_csv('submission_rc.csv', index = False)\n\n'''Submission with the most accurate Linear Discriminant Analysis'''\nsubmission  = pd.DataFrame({'id': test['id'],\n                            'target': lda.predict(X_test)})\nsubmission.to_csv('submission_lda.csv', index = False)\n\n'''Submission with the most accurate AdaBoostClassifier'''\nsubmission  = pd.DataFrame({'id': test['id'],\n                            'target': ab.predict(X_test)})\nsubmission.to_csv('submission_ab.csv', index = False)","587fd3fb":"**Great!! As we can see that data don't have any missing values but in target has missing values (due to concatenation of train and test set) that we would predict learning from the train dataset.**","f288fc7a":"# 5. Feature Engineering ","7329da98":"#### Optimizing our best model performed","6628f895":"## 4.3. Ordinal Variable","fff30b8c":"## 6.8 Retrain and Predict Using Optimized Hyperparameters","ddcd008c":"# 2. Variable Description, Identification, and Correction\nDescribe what each of the variable indicates and identify our response and predictor variables. Then seperate the categorical variables from numerical variables (i.e., pandas object, float64 or int64 data types).","094c2ba8":"**I am not optimizing XGBoost but you can try optimization on all the models for better score.**\n\n**Note: Though optimizing hyperparameters is time consuming.**","ec3e2cb7":"## 5.2 One Hot Encoding - Nominal Features\nNominal variables contain two or more categories without a natural ordering of the categories. For variable likes nom_0, nom_1, nom_2, nom_3, nom_4 we using One Hot Encoding. \n\nCategorical variables without any inherent order will be converted into numerical for our model using pandas get_dummies method.","138c1b3f":"**It's seems that ord_5 is normally distributed.**","4461f39e":"# 1.  Importing Packages and Collecting Data\nAfter importing required modules, read train and test data from csv files.","220f2356":"# Seting X and Y","578d1943":"## 5.4 Label Encoding\n### Nominal Features: High Cardinality Features\nwe use sklearn's label encoder method, it will randomly encode variables. Variables like nom_5, nom_6, nom_7, nom_8, nom_9, ord_5.\n","53503edc":"## 4.2 Nominal variables","a0d0d00b":"## 6.2 Creating Pipeline and Evaluation Models\n\n#### Standardization\n\nNumerical features preprocessing is different for tree and non tree model.\n\n1) Usually:\n   * Tree based models does not depend on scaling\n   * Non-tree based models hugely depend on scaling\n\n2) Most Often used preprocening are:\n    * MinMax scaler to (0,1)\n    It can formulated as\n    x <- (x - min(x)) \/ (max(x) - min(x))\n    \n    * Standard Scaler to mean = 0 and std =1\n    It can formulated as\n    x <- (x - mean(x)) \/ sd(x)","da05c53d":"**We can see that target variable is a binary and It's seems that balanced in the ratio of 7:3 approximately.**","b6892ca0":"## 4.1 Binary Features","ade76d60":"## 6.1 Importing Libraries","45b7995d":"# Competition description:\n\nIs there a cat in your dat?\n\nA common task in machine learning pipelines is encoding categorical variables for a given algorithm in a format that allows as much useful signal as possible to be captured.\n\nBecause this is such a common task and important skill to master, we've put together a dataset that contains only categorical features, and includes:\n\n- binary features\n- low- and high-cardinality nominal features\n- low- and high-cardinality ordinal features\n- (potentially) cyclical features\n\nThis Playground competition will give you the opportunity to try different encoding schemes for different algorithms to compare how they perform. We encourage you to share what you find with the community.\n\nIf you're not sure how to get started, you can check out the Categorical Variables section of Kaggle's Intermediate Machine Learning course.\n\n\nHave Fun!","11731922":"# ***<font color=\"green\">Give me your feedback and if you find  my kernel helpful please <b>UPVOTE will be appreciated.<\/b> <br><\/font>***\n# --Thank You for Reading","63b37262":"## 5.5 Encoding cyclic features\nEncoding cyclical continuous features - 24-hour time. Some data is inherently cyclical. Time is a rich example of this: minutes, hours, seconds, day of week, week of month, month, season, and so on all follow cycles.","91208990":"# Objective:\n**Objective of the kernel  is to show complete exploration to understand the categorical data, technique of handing categorical variables and after it I will build a Machine Learning Model.**","b4b97379":"## 5.5 Ordinal Encoding \n### Ordinal Feature\nThen, we will use pandas replace method to manually encode ordninal variables, because we don't want to be lost ordinality. Variables are ord_1, ord_2, ord_3, ord_4. ord_0 contains numerical values so we do not need to encode it again.\n","389afe55":"**I will keep only the frequency encoding to evaluate the results.**\n\n**Feel free to change if you would;**","db11ab19":"# About This Kernel\n\nHello, kagglers! In this kernel, we are going to fight with categorical variables with some exciting feature encoding techniques and modeling.\n\nI hope you enjoy my work and bring me your feedback. If this kernel is useful for you, please don't forget to upvote the kernel and You may ask any question if you have, Let's get started:\n\nVikas Singh,\n\nHappy Learning!!","1ff81b49":"## 5.3 Feature Hashing\n### Nominal Features: High Cardinality Features\n\nImplementing feature hashing, aka the hashing trick on Variables like nom_5, nom_6, nom_7, nom_8, nom_9 due to High Cardinality Features.\n\nFeature hashing is a very cool technique to represent categories in a \u201cone hot encoding style\u201d as a sparse matrix but with a much lower dimensions. In feature hashing we apply a hashing function to the category and then represent it by its indices. ","4e13a628":"## 6.7 Optimizing Hyperparameters\nNow let's add Grid Search to all the models with the hopes of optimizing their hyperparameters and thus improving their accuracy. Are the default model parameters the best bet? Let's find out.\n\nNote: Though optimizing hyperparameters is time consuming, hyperparameters should be tuned for all the models you try because only then you will be able to tell what is the best you can get out of that particular model.","2ee43ccf":"# 3. Target Feature: Balanced or Imbalanced\n\nImbalanced data typically refers to a problem with classification problems where the classes are not represented equally.\n\nFor example, you may have a 2-class (binary) classification problem with 100 instances (rows). A total of 80 instances are labeled with Class-1 and the remaining 20 instances are labeled with Class-2.\n\nThis is an imbalanced dataset and the ratio of Class-1 to Class-2 instances is 80:20 or more concisely 4:1.","e5757e9d":"# 4. Distribution of categorical variables.","87fc6e96":"# 6. Model Building & Evaluation \nWith all the preprocessings done and dusted, we're ready to train our regression models with the processed data.**","f73e8a17":"## 5.1 Binary Encoding - Binary Features\n\nA binary variable is a variable with only two values. For example:\n* 1 \/ 0.\n* Yes \/ No.\n* Success \/ Failure.\n* Male \/ Female.\n* Black \/ White\n\nFor machine learning models, we need numerical feature. so, we convert bin_3 and bin_4 in 1 and 0 form. In Binary Encoding \u2014 convert each integer to binary digits. Each binary digit gets one column. Some info loss but fewer dimensions.","bb1514a5":"## 5.4 Encoding with the Frequency\n### Nominal Features: High Cardinality Features","332bb662":"### Ordinal Feature :High Cardinality Features"}}