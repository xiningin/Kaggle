{"cell_type":{"df47b1e3":"code","ea2f9bcd":"code","ecf58b11":"code","e897da9d":"code","bd2e118d":"code","11819e9a":"code","daf57800":"code","7bc3f510":"code","2320a092":"code","e2400fd1":"code","a6e3e1fc":"code","752ddd5d":"code","18c1eaca":"code","5612116d":"code","b3634ecd":"code","62e0f29c":"code","446a5485":"code","57d57313":"code","157d38f3":"code","edd18fcb":"code","580cf5fa":"code","963e2a3c":"code","3d846942":"code","19ded108":"code","5d85d9f2":"code","91e6f683":"code","acc7ac58":"code","96c57ed2":"code","b55ed780":"code","7ad4e0b4":"code","c6e9c1d3":"code","aa22506e":"code","f9867132":"markdown","294ad15f":"markdown","53222918":"markdown","2e4ecc27":"markdown","41bec585":"markdown","5d1d68ee":"markdown","48c692f2":"markdown","e9382436":"markdown","90069251":"markdown","b9521bd7":"markdown","ce7d0489":"markdown","d5c1b2db":"markdown","6ba970a6":"markdown","2ffe0bcc":"markdown","f44ae098":"markdown","ae9728f7":"markdown","e32ddf79":"markdown","9f62e145":"markdown"},"source":{"df47b1e3":"import os\nimport json\nimport string\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import *\nfrom wordcloud import *\nfrom tqdm import tqdm\n\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nfrom sklearn import model_selection, preprocessing, metrics, ensemble, naive_bayes, linear_model\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import sent_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers\n\n\nimport os\nprint(os.listdir(\"..\/input\"))","ea2f9bcd":"train_df = pd.read_csv(\"..\/input\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/test.csv\")\nprint(\"Train shape : \", train_df.shape)\nprint(\"Test shape : \", test_df.shape)","ecf58b11":"train_df.head()","e897da9d":"train_df['target'].value_counts().plot(kind='bar')\ntarget_table = pd.crosstab(index = train_df['target'], columns='count')\nprint(target_table\/target_table.sum())","bd2e118d":"def generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]","11819e9a":"train0_df = train_df[train_df['target']==0]\ntrain1_df = train_df[train_df['target']==1]\n\n#For all sincere questions\nfrom collections import defaultdict\nsinc_dict = defaultdict(int)\nfor sent in train0_df[\"question_text\"]:\n    for word in generate_ngrams(sent):\n        sinc_dict[word] += 1\nsinc_sorted = pd.DataFrame(sorted(sinc_dict.items(), key=lambda x: x[1])[::-1])\nsinc_sorted.columns = [\"word\", \"wordcount\"]\n\n#For all insincere questions\nfrom collections import defaultdict\ninsinc_dict = defaultdict(int)\nfor sent in train1_df[\"question_text\"]:\n    for word in generate_ngrams(sent):\n        insinc_dict[word] += 1\ninsinc_sorted = pd.DataFrame(sorted(insinc_dict.items(), key=lambda x: x[1])[::-1])\ninsinc_sorted.columns = [\"word\", \"wordcount\"]","daf57800":"gram1_0 = go.Bar(y = sinc_sorted[\"word\"].head(20),x = sinc_sorted[\"wordcount\"].head(20),orientation=\"h\")\ngram1_1 = go.Bar(y = insinc_sorted[\"word\"].head(20),x = insinc_sorted[\"wordcount\"].head(20),orientation=\"h\")\n\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,\n                          subplot_titles=[\"Frequent words of sincere questions\", \n                                          \"Frequent words of insincere questions\"])\nfig.append_trace(gram1_0, 1, 1)\nfig.append_trace(gram1_1, 1, 2)\nfig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\npy.iplot(fig, filename='word-plots')","7bc3f510":"sinc_dict2 = defaultdict(int)\nfor sent in train0_df[\"question_text\"]:\n    for word in generate_ngrams(sent,n_gram=2):\n        sinc_dict2[word] += 1\nsinc_sorted_2 = pd.DataFrame(sorted(sinc_dict2.items(), key=lambda x: x[1])[::-1])\nsinc_sorted_2.columns = [\"word\", \"wordcount\"]\n\n#For all insincere questions\nfrom collections import defaultdict\ninsinc_dict_2 = defaultdict(int)\nfor sent in train1_df[\"question_text\"]:\n    for word in generate_ngrams(sent,n_gram=2):\n        insinc_dict_2[word] += 1\ninsinc_sorted_2 = pd.DataFrame(sorted(insinc_dict_2.items(), key=lambda x: x[1])[::-1])\ninsinc_sorted_2.columns = [\"word\", \"wordcount\"]","2320a092":"gram2_0 = go.Bar(y = sinc_sorted_2[\"word\"].head(20),x = sinc_sorted_2[\"wordcount\"].head(20),orientation=\"h\")\ngram2_1 = go.Bar(y = insinc_sorted_2[\"word\"].head(20),x = insinc_sorted_2[\"wordcount\"].head(20),orientation=\"h\")\n\nfig2 = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,\n                          subplot_titles=[\"Frequent words of sincere questions\", \n                                          \"Frequent words of insincere questions\"])\nfig2.append_trace(gram2_0, 1, 1)\nfig2.append_trace(gram2_1, 1, 2)\nfig2['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\npy.iplot(fig2, filename='word-plots')","e2400fd1":"from wordcloud import WordCloud, STOPWORDS\n\n## Number of words in the text ##\ntrain_df[\"num_words\"] = train_df[\"question_text\"].apply(lambda x: len(str(x).split()))\ntest_df[\"num_words\"] = test_df[\"question_text\"].apply(lambda x: len(str(x).split()))\n\n## Number of unique words in the text ##\ntrain_df[\"num_unique_words\"] = train_df[\"question_text\"].apply(lambda x: len(set(str(x).split())))\ntest_df[\"num_unique_words\"] = test_df[\"question_text\"].apply(lambda x: len(set(str(x).split())))\n\n## Number of characters in the text ##\ntrain_df[\"num_chars\"] = train_df[\"question_text\"].apply(lambda x: len(str(x)))\ntest_df[\"num_chars\"] = test_df[\"question_text\"].apply(lambda x: len(str(x)))\n\n## Number of stopwords in the text ##\ntrain_df[\"num_stopwords\"] = train_df[\"question_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\ntest_df[\"num_stopwords\"] = test_df[\"question_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n\n## Number of punctuations in the text ##\ntrain_df[\"num_punctuations\"] =train_df['question_text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\ntest_df[\"num_punctuations\"] =test_df['question_text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n\n## Number of title case words in the text ##\ntrain_df[\"num_words_upper\"] = train_df[\"question_text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\ntest_df[\"num_words_upper\"] = test_df[\"question_text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n\n## Number of title case words in the text ##\ntrain_df[\"num_words_title\"] = train_df[\"question_text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\ntest_df[\"num_words_title\"] = test_df[\"question_text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n\n## Average length of the words in the text ##\ntrain_df[\"mean_word_len\"] = train_df[\"question_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ntest_df[\"mean_word_len\"] = test_df[\"question_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))","a6e3e1fc":"train_df.head()","752ddd5d":"## Truncate some extreme values for better visuals ##\ntrain_df['num_words'].loc[train_df['num_words']>60] = 60 #truncation for better visuals\ntrain_df['num_punctuations'].loc[train_df['num_punctuations']>10] = 10 #truncation for better visuals\ntrain_df['num_chars'].loc[train_df['num_chars']>350] = 350 #truncation for better visuals\n\nf, axes = plt.subplots(3, 1, figsize=(10,20))\nsns.boxplot(x='target', y='num_words', data=train_df, ax=axes[0])\naxes[0].set_xlabel('Target', fontsize=12)\naxes[0].set_title(\"Number of words in each class\", fontsize=15)\n\nsns.boxplot(x='target', y='num_chars', data=train_df, ax=axes[1])\naxes[1].set_xlabel('Target', fontsize=12)\naxes[1].set_title(\"Number of characters in each class\", fontsize=15)\n\nsns.boxplot(x='target', y='num_punctuations', data=train_df, ax=axes[2])\naxes[2].set_xlabel('Target', fontsize=12)\n#plt.ylabel('Number of punctuations in text', fontsize=12)\naxes[2].set_title(\"Number of punctuations in each class\", fontsize=15)\nplt.show()","18c1eaca":"#def tokenize(data):\n#    tokenized_docs = [word_tokenize(doc.lower()) for doc in data]\n#    alpha_tokens = [[t for t in doc if t.isalpha() == True] for doc in tokenized_docs]\n#    stemmer = PorterStemmer ()\n#    stemmed_tokens = [[stemmer.stem(alpha) for alpha in doc] for doc in alpha_tokens]\n#    X_stem_as_string = [\" \".join(x_t) for x_t in stemmed_tokens]\n#    return X_stem_as_string","5612116d":"X = train_df['question_text']\ny = train_df['target']\nX_test = test_df['question_text']\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=.2, random_state=42, stratify=y)\nX_train.shape, y_train.shape, X_val.shape, y_val.shape","b3634ecd":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.pipeline import Pipeline","62e0f29c":"rf = ensemble.RandomForestClassifier(class_weight='balanced_subsample')\ntfvec = TfidfVectorizer(stop_words='english', lowercase=False)\npipe = Pipeline([\n    ('vectorizer', tfvec),\n    ('rf', rf )\n])","446a5485":"pipe.fit(X_train, y_train)","57d57313":"y_pred = pipe.predict(X_val)","157d38f3":"cm = metrics.confusion_matrix(y_val, y_pred)\n\nax = plt.gca()\nsns.heatmap(cm, cmap='Blues', cbar=False, annot=True, xticklabels=y_val.unique(), yticklabels=y_val.unique(), ax=ax);\nax.set_xlabel('y_pred');\nax.set_ylabel('y_true');\nax.set_title('Confusion Matrix');\n\ncr = metrics.classification_report(y_val, y_pred)\nprint(cr)","edd18fcb":"lr = linear_model.LogisticRegression()\npipe_lr = Pipeline([\n    ('vectorizer', tfvec),\n    ('lr', lr )\n])","580cf5fa":"pipe_lr.fit(X_train, y_train)","963e2a3c":"y_pred_lr = pipe_lr.predict(X_val)","3d846942":"cm_lr = metrics.confusion_matrix(y_val, y_pred_lr)\n\nax = plt.gca()\nsns.heatmap(cm_lr, cmap='Blues', cbar=False, annot=True, xticklabels=y_val.unique(), yticklabels=y_val.unique(), ax=ax);\nax.set_xlabel('y_pred');\nax.set_ylabel('y_true');\nax.set_title('Confusion Matrix');\n\ncr = metrics.classification_report(y_val, y_pred_lr)\nprint(cr)","19ded108":"metrics.f1_score(y_pred=y_pred_lr,y_true=y_val)","5d85d9f2":"y_prob_lr = pipe_lr.predict_proba(X_val)\nbest_threshold = 0\nf1=0\nfor i in np.arange(.1, .51, 0.01):\n    y_pred2_lr = [1 if proba>i else 0 for proba in y_prob_lr[:, 1]]\n    f1score = metrics.f1_score(y_pred=y_pred2_lr, y_true=y_val)\n    if f1score>f1:\n        best_threshold = i\n        f1=f1score\n        \ny_pred2_lr = [1 if proba>best_threshold else 0 for proba in y_prob_lr[:, 1]]\nf1 = metrics.f1_score(y_pred2_lr, y_val)\nprint('The best threshold is {}, with an f1_score of {}'.format(best_threshold, f1))","91e6f683":"y_pred_sub = pipe_lr.predict(X_test) \n\nsub = pd.read_csv('..\/input\/sample_submission.csv')\nsub.prediction = (y_pred_sub > best_threshold).astype(int)\nsub.to_csv(\"submission.csv\", index=False)","acc7ac58":"lr = linear_model.LogisticRegression(penalty='l2',solver='sag')\npipe_cv = Pipeline([\n    ('vectorizer', tfvec),\n    ('lr', lr )\n])","96c57ed2":"param_grid = {'lr__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000] }\n\nclf = model_selection.GridSearchCV(pipe_cv, param_grid,cv=5)","b55ed780":"clf.fit(X_train,y_train)","7ad4e0b4":"y_pred_lrcv = clf.best_estimator_.predict(X_val)\nprint(metrics.f1_score(y_val, y_pred_lrcv))\nprint(metrics.classification_report(y_val, y_pred_lrcv))","c6e9c1d3":"### Lets find the best threshold for cut-off\ny_prob_lrcv = clf.best_estimator_.predict_proba(X_val)\nbest_threshold = 0\nf1=0\nfor i in np.arange(.1, .51, 0.01):\n    y_pred2_lrcv = [1 if proba>i else 0 for proba in y_prob_lrcv[:, 1]]\n    f1score = metrics.f1_score(y_pred=y_pred2_lrcv, y_true=y_val)\n    if f1score>f1:\n        best_threshold = i\n        f1=f1score\n        \ny_pred2_lrcv = [1 if proba>best_threshold else 0 for proba in y_prob_lrcv[:, 1]]\nf1 = metrics.f1_score(y_pred2_lrcv, y_val)\nprint('The best threshold is {}, with an f1_score of {}'.format(best_threshold, f1))","aa22506e":"y_pred_sub = pipe_lr.predict(X_test) \n\nsub = pd.read_csv('..\/input\/sample_submission.csv')\nsub.prediction = (y_pred_sub > best_threshold).astype(int)\nsub.to_csv(\"submission.csv\", index=False)","f9867132":"### BASE MODEL","294ad15f":"Logistic regression seems to do a better job here. We can further optimize the f1-score by tuning the threshold.","53222918":"#### 1 gram - Most common words in both the types of questions","2e4ecc27":"#### Target Variable distribution:\nCheck if the data set is imbalanced","41bec585":"# Tune Logistic regression - CV","5d1d68ee":"### Play around with the Threshold to see if f1_score can be increased","48c692f2":"Looks like we have a decent model with F1 of 0.607","e9382436":"#### 2 gram - Most common words in both the types of questions","90069251":"In general, looks like insincere questions have higher number of punctuations, words and charecters","b9521bd7":"Let's explore this dataset. ","ce7d0489":"#### Meta Features:\n\nNow let us create some meta features and then look at how they are distributed between the classes. The ones that we will create are\n\nNumber of words in the text\nNumber of unique words in the text\nNumber of characters in the text\nNumber of stopwords\nNumber of punctuations\nNumber of upper case words\nNumber of title case words\nAverage length of the words\n\nReference: SRK's code","d5c1b2db":"#### Read the files","6ba970a6":"Words like best, people, good are highlighted in sincere questions.\nWords like trump, women, people, white, muslims are highlighed in insincere questions\nSome words are stop words and also some words are common between them","2ffe0bcc":"### Logistic Regression Model","f44ae098":"Again we see some common words popping up, somewords that stand out though might be able to help classify the sincere ones from the insincere ones.","ae9728f7":"#### Random Forest Model","e32ddf79":"##### Looks like we have Sincere:Insincere == 93%:7%\n\n## Explore the data\n\n#### N-gram function from the text\n\nReference: http:\/\/www.albertauyeung.com\/post\/generating-ngrams-python\/","9f62e145":"#### Define a tokenize function"}}