{"cell_type":{"4740d902":"code","daff14b1":"code","11e95ccd":"code","e8642971":"code","328a772b":"code","969c29f2":"code","bc71ec1f":"code","d37c0055":"code","d7099702":"code","83e3149a":"code","74dcca23":"code","71d67083":"code","65934bf7":"markdown","95a09e05":"markdown","65bf3653":"markdown","878b8b16":"markdown","2e3f700e":"markdown","be4d7cfc":"markdown","fd2a8be9":"markdown"},"source":{"4740d902":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfrom tqdm import tqdm\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in tqdm(filenames):\n        pass\n        #print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","daff14b1":"import os\nfrom os.path import join\nimport cv2\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nfrom glob import glob\nfrom PIL import Image\nfrom torchvision import models\nfrom tqdm import tqdm\n\nbatch_size = 32\ntotal_epoch = 5\nmethod = 'GoogLeNet' # (ResNet | VGG | GoogLeNet)","11e95ccd":"if torch.cuda.is_available():\n    print(\"CUDA is available\")\n    device = 'cuda'\nelse:\n    print(\"CUDA is NOT available\")\n    device = 'cpu'\n\ntorch.manual_seed(777)\nif device == 'cuda':\n    torch.cuda.manual_seed_all(777)","e8642971":"train_mother_path = '\/kaggle\/input\/2021-ai-w10-p2\/images\/images\/'\ntest_mother_path = '\/kaggle\/input\/2021-ai-w10-p2\/test_data\/test_data'\ntrain_image_path = glob(join(train_mother_path, '*', '*'))\ntest_image_path = glob(join(test_mother_path, '*'))","328a772b":"# (ResNet | VGG | GoogLeNet)\n\"\"\"\npretrained=True \uc635\uc158\uc744 \ud1b5\ud574 pretrain weight\ub97c \ubd88\ub7ec\uc628\ub2e4.\n\"\"\"\nmethod = 'VGG'\nif method is \"ResNet\":\n    model = models.resnet18(pretrained=True)\n    # output layer \ucd9c\ub825 \ud615\ud0dc \ubcc0\ud658\n    model_classifier = torch.nn.Linear(1024, 10)\n    torch.nn.init.xavier_uniform_(model_classifier.weight)\n    model.fc = model_classifier\n    model.to(device)             \nif method is \"VGG\":\n    model = models.vgg16(pretrained=True)\n    # output layer \ucd9c\ub825 \ud615\ud0dc \ubcc0\ud658\n    model_classifier = torch.nn.Linear(4096, 10)\n    torch.nn.init.xavier_uniform_(model_classifier.weight)\n    model.classifier[6] = model_classifier\n    model.to(device)\nif method is \"GoogLeNet\":\n    model = models.googlenet(pretrained=True)\n    # output layer \ucd9c\ub825 \ud615\ud0dc \ubcc0\ud658\n    model_classifier = torch.nn.Linear(1024, 10)\n    torch.nn.init.xavier_uniform_(model_classifier.weight)\n    model.fc = model_classifier\n    model.to(device)","969c29f2":"model","bc71ec1f":"import torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nimport time\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nfrom torchvision import models\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)","d37c0055":"# training function\nimport random \n\ndef train(model, train_image_path):\n    model.train()\n    train_running_loss = 0.0\n    train_running_correct = 0\n    random.shuffle(train_image_path)\n    transform = transforms.Compose(\n        [transforms.Resize((224, 224)),\n         transforms.ToTensor(),\n         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n    for i in range(len(train_image_path)\/\/3200):\n        x_train=[]\n        y_train=[]\n        for ii in range(3200):\n            path_i = train_image_path[(i*3200)+ii]\n            image = Image.open(path_i, mode='r')\n            image = image.convert('RGB')\n            data = transform(image)\n            data = torch.reshape(data, (1,3,224,224))\n            target = int(path_i.split('\/')[-2])\n            x_train.append(data)\n            y_train.append(target)\n        trainset = torch.utils.data.TensorDataset(torch.cat(x_train, dim=0), torch.tensor(y_train))\n        trainloader =  torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n        del x_train\n        for data in tqdm(trainloader):\n            data, target = data[0].to(device), data[1].to(device)\n            \n            optimizer.zero_grad()\n            output = model(data)\n\n            loss = criterion(output, target).cuda()\n\n            train_running_loss += loss.item()\n            _, preds = torch.max(output.data, 1)\n            train_running_correct += (preds == target).sum().item()\n            loss.backward()\n            optimizer.step()\n        del trainloader\n        del trainset\n    train_loss = train_running_loss\/len(train_image_path)\n    train_accuracy = 100. * train_running_correct\/len(train_image_path)\n    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}')\n    \n    return train_loss, train_accuracy","d7099702":"train_loss , train_accuracy = [], []\nval_loss , val_accuracy = [], []\nstart = time.time()\nfor epoch in range(total_epoch):\n    train_epoch_loss, train_epoch_accuracy = train(model, train_image_path)\n    train_loss.append(train_epoch_loss)\n    train_accuracy.append(train_epoch_accuracy)\n    print(total_epoch, train_accuracy, val_accuracy)\n    \nend = time.time()\n \nprint((end-start)\/60, 'minutes')","83e3149a":"# training function\ndef test(model, test_image_path):\n    model.eval()\n    preds=[]\n    transform = transforms.Compose(\n        [transforms.Resize((224, 224)),\n         transforms.ToTensor(),\n         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n    for path_i in tqdm(range(0, 10000)):\n        image = Image.open(test_image_path+str(path_i)+'.png', mode='r')\n        image = image.convert('RGB')\n        data = transform(image)\n        data = torch.reshape(data, (1,3,224,224))\n        data = data.to(device)\n        output = model(data)\n        _, pred = torch.max(output.data, 1)\n        preds.extend(pred.detach().cpu().tolist())\n    \n    return preds","74dcca23":"sample_submit_csv = pd.read_csv('\/kaggle\/input\/2021-ai-w10-p2\/format.csv')\npreds = test(model, \"\/kaggle\/input\/2021-ai-w10-p2\/test_data\/test_data\/\")","71d67083":"sample_submit_csv['label'] = preds\nsample_submit_csv.to_csv(\"{}_pred.csv\".format(method), index=False)","65934bf7":"# \ud3c9\uac00","95a09e05":"# \uc0ac\uc804\ud559\uc2b5 \ubaa8\ub378 \uc0ac\uc6a9 (LeNet, VGG, ResNet)","65bf3653":"# \ud559\uc2b5 \uae30\ubcf8 \ubcc0\uc218 \uc14b\ud305","878b8b16":"# \ub370\uc774\ud130 \ub85c\ub354","2e3f700e":"# \ubaa8\ub378 \ud559\uc2b5","be4d7cfc":"# GPU \uc0ac\uc6a9 \uac00\ub2a5 \uc5ec\ubd80 \ud655\uc778 \ubc0f \ub80c\ub364 seed \uace0\uc815","fd2a8be9":"```python\n# TrainSet\uc758 \ud3f4\ub354 \uad6c\uc131\n\n\u251c\u2500 images  \n\u2502 \u251c\u2500 0 # (label name)  \n\u2502 \u2502 \u251c\u2500 aeroplane_s_000004.png # (image name)  \n\u2502 \u2502 \u251c\u2500 aeroplane_s_0000021.png  \n\u2502 \u2502 \u2514\u2500 aeroplane_s_000022.png  \n\u2502 \u251c\u2500 1  \n\u2502 \u2502 \u251c\u2500 ambulance_s_000101.png  \n\u2502 \u2502 \u251c\u2500 ambulance_s_000204.png  \n\u2502 \u2502 \u2514\u2500 ambulance_s_000266.png  \n\u2502 \u2502  \n\u2502 . . .  \n\u2502 \u251c\u2500 9  \n\u2502 \u2502 \u251c\u2500 . . .  \n\u2502 \u2502 \u2514\u2500 aerial_ladder_truck_s_000042.png  \n\u2502 \u2514\u2500  \n\u2514\u2500    \n```"}}