{"cell_type":{"13f8fcc5":"code","88ce3107":"code","a9166865":"code","33422334":"code","1778f9c2":"markdown","2f4c1371":"markdown","155b9df8":"markdown","98199579":"markdown","d879904d":"markdown"},"source":{"13f8fcc5":"import pandas as pd\nimport numpy  as np","88ce3107":"LogisticRegression        = pd.read_csv(\"..\/input\/logistic-regression-classifier-minimalist-script\/submission.csv\")\nRandomForestClassifier    = pd.read_csv(\"..\/input\/random-forest-classifier-minimalist-script\/submission.csv\")\nneural_network            = pd.read_csv(\"..\/input\/very-simple-neural-network-for-classification\/submission.csv\")\nGaussianProcessClassifier = pd.read_csv(\"..\/input\/gaussian-process-classification-sample-script\/submission.csv\")\nSupportVectorClassifier   = pd.read_csv(\"..\/input\/support-vector-classifier-minimalist-script\/submission.csv\")      ","a9166865":"all_data = [ LogisticRegression['Survived'] , \n             RandomForestClassifier['Survived'], \n             neural_network['Survived'], \n             GaussianProcessClassifier['Survived'], \n             SupportVectorClassifier['Survived'] ]\n\nvotes       = pd.concat(all_data, axis='columns')\n\npredictions = votes.mode(axis='columns').to_numpy()","33422334":"output = pd.DataFrame({'PassengerId': neural_network.PassengerId, \n                       'Survived'   : predictions.flatten()})\noutput.to_csv('submission.csv', index=False)","1778f9c2":"We shall read in the predictions, *i.e.* the `submission.csv` files,  from an odd number of estimators. For this example we shall use predictions from \na [logistic regression](https:\/\/www.kaggle.com\/carlmcbrideellis\/logistic-regression-classifier-minimalist-script), \na [random forest](https:\/\/www.kaggle.com\/carlmcbrideellis\/random-forest-classifier-minimalist-script).\na [neural network](https:\/\/www.kaggle.com\/carlmcbrideellis\/very-simple-neural-network-for-classification),\na [Gaussian process classifier](https:\/\/www.kaggle.com\/carlmcbrideellis\/gaussian-process-classification-sample-script),\nand finally \na [Support Vector Machine classifier](https:\/\/www.kaggle.com\/carlmcbrideellis\/support-vector-classifier-minimalist-script):","2f4c1371":"### Related reading:\n* [Dymitr Ruta and Bogdan Gabrys \"Classifier selection for majority voting\", Information Fusion,\nVolume 6 Pages 63-81 (2005)](https:\/\/www.sciencedirect.com\/science\/article\/abs\/pii\/S1566253504000417)","155b9df8":"### Ensemble methods: classifiers and majority voting\nThe goal of ensemble methods is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability \/ robustness over a single estimator [[1]](https:\/\/scikit-learn.org\/stable\/modules\/ensemble.html).\nHere we shall look at an averaging method known as **majority voting**. In majority voting, the predicted class label for a particular sample is the class label that represents the majority \n([mode](https:\/\/en.wikipedia.org\/wiki\/Mode_&#40;statistics&#41;))\nof the class labels predicted by each individual classifier [[2]](https:\/\/scikit-learn.org\/stable\/modules\/ensemble.html#voting-classifier). \n\n\n","98199579":"we shall now calculate the mode, using [pandas.DataFrame.mode](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.mode.html):","d879904d":"and finally we shall produce a new `submission.csv` file whose predictions are now the mode of all of the above estimators:"}}