{"cell_type":{"59a62b8b":"code","f4bd77be":"code","f8274dc5":"code","87a7afab":"code","5e8feec2":"code","1e37fb24":"code","9b2af9af":"code","5227f767":"code","006b6d25":"code","156a74d6":"code","1839f03e":"code","7d078def":"code","d5963290":"code","9471099d":"code","e7d93405":"code","0c260059":"code","9d8641dc":"code","2e1e877d":"code","67a06431":"code","2b0ced57":"code","499913a7":"code","528e12bd":"code","91475f7a":"code","0dc581f8":"code","2c7a50bf":"code","3bf98d8c":"code","0aeb334d":"code","83751ddf":"markdown","f14e772a":"markdown","88a2ee12":"markdown","df26ff2b":"markdown","e74034b9":"markdown","c8e53ae6":"markdown","28e0fcf6":"markdown","e52597b2":"markdown","389f0432":"markdown","8a18233f":"markdown"},"source":{"59a62b8b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f4bd77be":"import cv2\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom imblearn.under_sampling import NearMiss\n\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix\n\nimport keras\nfrom keras.optimizers import Adam\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout , BatchNormalization\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau","f8274dc5":"alpha_data = pd.read_csv('\/kaggle\/input\/az-handwritten-alphabets-in-csv-format\/A_Z Handwritten Data.csv')","87a7afab":"alpha_data.head()","5e8feec2":"# Seperating dependent variable from independent variables\ny = alpha_data['0']\ndel alpha_data['0']","1e37fb24":"plt.figure(figsize = (10,5))\nsns.countplot(y)","9b2af9af":"nM = NearMiss()\nX_data, y_data = nM.fit_sample(alpha_data, y)","5227f767":"plt.figure(figsize = (10,5))\nsns.countplot(y_data)","006b6d25":"print(X_data.head())\nprint(' ')\nprint(y_data.head())","156a74d6":"lB = LabelBinarizer()\ny = lB.fit_transform(y_data)\ny","1839f03e":"X_data = X_data \/ 255\nX_data","7d078def":"X_data = np.array(X_data)","d5963290":"X_data = X_data.reshape(-1,28,28,1)","9471099d":"# Showing few images\n\nf, ax = plt.subplots(2,5)\nf.set_size_inches(10,10)\nk = 0\nfor i in range(2):\n    for j in range(5):\n        ax[i,j].imshow(X_data[k].reshape(28,28), cmap='gray')\n        k += 1\n    plt.tight_layout()","e7d93405":"X_train, X_test, y_train, y_test = train_test_split(X_data, y, test_size=0.2)\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2)\nX_train.shape, X_test.shape, X_valid.shape, y_train.shape, y_test.shape, y_valid.shape","0c260059":"dataGen = ImageDataGenerator(rotation_range=10,\n                             zoom_range=0.1,\n                             width_shift_range=0.1,\n                             height_shift_range=0.1)\ndataGen.fit(X_train)","9d8641dc":"learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 2, \n                                            verbose=1,factor=0.5, min_lr=0.00001)","2e1e877d":"model = Sequential()\nmodel.add(Conv2D(75 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu' , input_shape = (28,28,1)))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\nmodel.add(Conv2D(50 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\nmodel.add(Conv2D(25 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\nmodel.add(Flatten())\nmodel.add(Dense(units = 512 , activation = 'relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(units = 26 , activation = 'softmax'))\nmodel.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\nmodel.summary()","67a06431":"print(model.summary())","2b0ced57":"history = model.fit(dataGen.flow(X_train,y_train, batch_size = 128) ,epochs = 5 , \n                    validation_data = (X_valid, y_valid) , \n                    callbacks = [learning_rate_reduction])","499913a7":"plt.figure(1)\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.legend(['training','validation'])\nplt.title('Loss')\nplt.xlabel('epoch')\nplt.figure(2)\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.legend(['training','validation'])\nplt.title('Accuracy')\nplt.xlabel('epoch')\nplt.show()","528e12bd":"score = model.evaluate(X_test,y_test,verbose=0)\nprint('Test Score = ',score[0])\nprint('Test Accuracy =', score[1])","91475f7a":"className = {0:'A', 1:'B', 2:'C', 3:'D', 4:'E',\n             5:'F', 6:'G', 7:'H', 8:'I', 9:'J',\n             10:'K', 11:'L', 12:'M', 13:'N', 14:'O',\n             15:'P', 16:'Q', 17:'R', 18:'S', 19:'T', \n             20:'U', 21:'V', 22:'W', 23:'X', 24:'Y',\n             25:'Z'}","0dc581f8":"predictions = model.predict_classes(X_test)\npredictions[:5] ","2c7a50bf":"y_test[0:5]","3bf98d8c":"plt.figure(figsize=(10,10))\nfor i in range(25):\n    plt.subplot(5,7,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(X_test[i].reshape(28,28), cmap='gray')\n    plt.title(className[np.argmax(y_test[i])])\n    plt.xlabel(className[predictions[i]])\nplt.show()","0aeb334d":"model.save('Alpha.model')","83751ddf":"# Data Augmentation\n- with data augmentation we can save us from overfitting","f14e772a":"## Importing Required Libraries","88a2ee12":"## One hot encoding 'Labels'","df26ff2b":"Now  each class of our data is completly balanced so we can proceed further.","e74034b9":"Here we are using under sampling to balance our data before sending it to model.","c8e53ae6":"## Data Balancing","28e0fcf6":"## Importing Data","e52597b2":"As We can clearly see our data is highly imbalanced. Firstly we have to balance our data so that our model won't get biased for perticular class during prediction.","389f0432":"## Testing our Model","8a18233f":"## Data Insights"}}