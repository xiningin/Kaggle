{"cell_type":{"fd05a34d":"code","2a75f808":"code","70d467bd":"code","96b23e41":"code","6da74e14":"code","a07e7788":"code","a2f906cb":"code","a70c2a53":"code","6cc37384":"code","bd21e324":"code","37e4c551":"code","66b5b7d6":"code","11b4ac9d":"code","274c5a1b":"code","9f0a9609":"code","6a626be3":"code","ef946b96":"code","14a6931a":"code","d085c126":"code","a9870b5f":"code","f2c830cc":"code","161fea6f":"code","f18e6e06":"code","41587bb8":"code","bccb9dd1":"code","92166fb5":"code","dfb86ced":"code","c62c7771":"code","64553ed5":"code","0c15f053":"code","9d158e43":"code","0940da67":"code","7f2e483c":"code","86807e4f":"code","48fd9315":"markdown","d5aba3ce":"markdown","d7098a61":"markdown","71e9348e":"markdown","b13c80fc":"markdown","ca3c0787":"markdown","6757506c":"markdown","d0ee41b5":"markdown","88d9bed7":"markdown","ae919a0b":"markdown","2ac455cb":"markdown","543d3cd8":"markdown","9eb3efdd":"markdown","5cb66248":"markdown","56405046":"markdown","22b3987a":"markdown","4417582e":"markdown","18085c67":"markdown","ef62822a":"markdown","252cdc99":"markdown","2ff983bd":"markdown","3c3ae354":"markdown","8ca6443d":"markdown","71b96d85":"markdown","05ab47ee":"markdown","591e0a19":"markdown","e99c33ae":"markdown","68568ac2":"markdown","c1ce91d3":"markdown","b4fe5e4d":"markdown","73023ceb":"markdown","c2b30432":"markdown","fe18caca":"markdown","7a663706":"markdown","f7a47839":"markdown","f860c9b1":"markdown","6747250c":"markdown"},"source":{"fd05a34d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)    \nimport matplotlib as mat\nimport matplotlib.pyplot as plt    \nimport seaborn as sns\nimport random\nimport os\nimport gc\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import  LabelEncoder, RobustScaler , MinMaxScaler ,StandardScaler\nimport tensorflow as tf \nfrom tensorflow import keras\nimport tensorflow_addons as tfa\nimport tensorflow.keras.backend as K \nfrom tensorflow.keras import layers\nfrom tensorflow.keras.utils import to_categorical ,plot_model\nfrom tensorflow.keras import callbacks\nfrom tensorflow.keras.layers import Dense, Dropout, Input, InputLayer, Flatten \nfrom tensorflow.random import set_seed\nset_seed(42)\nimport warnings\nwarnings.filterwarnings('ignore')\nsns.set_style(\"whitegrid\")","2a75f808":"train_data = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/train.csv\")\ntest_data= pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/test.csv\")\nsample = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/sample_submission.csv\")\ntrain= train_data.drop('Id', axis=1) # drop unused id \ntest= test_data.drop('Id', axis=1)","70d467bd":"train.head(3)","96b23e41":"train.describe().style.background_gradient(cmap='RdPu')","6da74e14":"# variables variaition   \ndf_var=train.var().reset_index()\ndf_var.columns =['feature', 'variation']\ndf_var.sort_values(\"variation\",ascending = False).T","a07e7788":"# Correlation matrix\ncorrMatrix =train.corr(method='pearson', min_periods=1)\ncorrMatrix.style.background_gradient(axis=None)","a2f906cb":"cor_targ = train.corrwith(train[\"Cover_Type\"]).reset_index()\ncor_targ.columns =['feature', 'CorrelatioWithTarget']\ncor_targ.sort_values('CorrelatioWithTarget',ascending = False).T","a70c2a53":"ax = plt.figure(figsize=(12, 6))\ncover_type= train['Cover_Type'].value_counts().sort_index()\nsns.barplot(x=cover_type.index, y=cover_type,palette=\"BuPu_r\")\nplt.show()\n","6cc37384":"test.head(3)\n","bd21e324":"test.describe().style.background_gradient(axis =1)","37e4c551":"plt.figure(figsize=(15,8))\nfeatures = train.columns.values[0:54]\nsns.distplot(train[features].mean(axis=1),color=\"red\", kde=True,bins=120, label='train')\nsns.distplot(test[features].mean(axis=1),color=\"darkblue\", kde=True,bins=120, label='test')\nplt.title(\"Distribution of mean values per row in the train and test data\")\nplt.legend()\nplt.show()\n","66b5b7d6":"plt.figure(figsize=(15,5))\nsns.distplot(train[features].mean(axis=0),color=\"orange\",kde=True,bins=120, label='train')\nsns.distplot(test[features].mean(axis=0),color=\"blue\", kde=True,bins=120, label='test')\nplt.title(\"Distribution of mean values per column in the train and test set\")\nplt.legend()\nplt.show()","11b4ac9d":"plt.figure(figsize=(15,5))\nsns.distplot(train[features].std(axis=1),color=\"#2F4F4F\", kde=True,bins=120, label='train')\nsns.distplot(test[features].std(axis=1),color=\"#FF6347\", kde=True,bins=120, label='test')\nplt.title(\"Distribution of std per row in the train and test data \")\nplt.legend()\nplt.show()","274c5a1b":"plt.figure(figsize=(15,5))\nsns.distplot(train[features].std(axis=0),color=\"#778899\",kde=True,bins=120, label='train')\nsns.distplot(test[features].std(axis=0),color=\"#800080\", kde=True,bins=120, label='test')\nplt.title(\"Distribution of std per column in the train and test data\")\nplt.legend()\nplt.show()","9f0a9609":"num_cols = [\n    \"Elevation\",\n    \"Aspect\",\n    \"Slope\",\n    \"Horizontal_Distance_To_Hydrology\",\n    \"Vertical_Distance_To_Hydrology\",\n     \"Horizontal_Distance_To_Roadways\",\n    \"Hillshade_9am\",\n    \"Hillshade_Noon\",\n    \"Hillshade_3pm\",\n    \"Horizontal_Distance_To_Fire_Points\",\n]\ni = 1\nplt.figure()\nfig, ax = plt.subplots(figsize=(18, 15))\nfor col in num_cols:\n    plt.subplot(5,3,i)\n    sns.distplot(train[col],color=\"yellow\", kde=True,bins=100, label='train')\n    sns.distplot(test[col],color=\"Darkblue\", kde=True,bins=100, label='test')\n    i += 1\nplt.legend()\nplt.title(\" numirical features Distribution in both train and test data\")  \nplt.show()","6a626be3":"# remove unuseful features\ntrain = train.drop([ 'Soil_Type7', 'Soil_Type15','Soil_Type1'], axis=1)\ntest= test.drop(['Soil_Type7', 'Soil_Type15','Soil_Type1'], axis=1)\n##### drop class 5 \ntrain =train[train.Cover_Type != 5] ","ef946b96":"y_target = train[\"Cover_Type\"].copy() ##target variable \nX_train = train.copy().drop(\"Cover_Type\",axis = 1) ##train data ","14a6931a":"ax = plt.figure(figsize=(12, 6))\ncover_type=y_target.value_counts().reset_index()\nsns.barplot(x=cover_type.index,data=cover_type,y=\"Cover_Type\",color=\"salmon\")\nplt.show()\n","d085c126":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2\n\n    for col in df.columns:\n        col_type = df[col].dtypes\n\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n\n    if verbose:\n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n \n    return df","a9870b5f":"%%time\nX_train = reduce_mem_usage(X_train)\ntest = reduce_mem_usage(test)","f2c830cc":"# i expriment it with skew abd sum\u00f9 variable but it hurted the performance by -0.0003\n#you can expriment with others using the same model or other models \n\ndef stat_features(df):\n    #df['f_mean'] = df.mean(axis=1)\n    #df['f_std']  = df.std(axis=1)\n    df['r_skew'] = df.skew(axis=1)\n    df['r_sum'] = df.sum(axis=1)\n    return df\n\n#X_train = stat_features(train_df)\n#test = stat_features(test_df)","161fea6f":"### delet it \ndel train_data \ndel test_data ","f18e6e06":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\ntest= scaler.transform(test)","41587bb8":"### label encoder for target \nlabel_encod = LabelEncoder()\ny_encoded =label_encod.fit_transform(y_target)\n# categorical transofrm for target\n#y_cat =to_categorical(y_encoded)","bccb9dd1":"print(y_encoded.shape,y_target.shape,X_train.shape,test.shape )","92166fb5":"n_classes = 6\ndef get_model(X_train):\n    inputs = layers.Input(shape = (X_train.shape[1],))\n    \n    hidden = layers.Dense(units=508, kernel_initializer=\"lecun_normal\", activation=\"selu\")(inputs)\n    dropout = layers.Dropout(0.3)(hidden)\n    hidden1 = tfa.layers.WeightNormalization(layers.Dense(units=256, activation='selu', kernel_initializer=\"lecun_normal\"))(dropout)\n    dropout1 = layers.Dropout(0.3)(layers.Concatenate()([hidden1, hidden]))\n    hidden2 = tfa.layers.WeightNormalization(layers.Dense(units=128, activation='selu', kernel_initializer=\"lecun_normal\"))(dropout1) \n    dropout2 = layers.Dropout(0.3)(layers.Concatenate()([hidden1,hidden, hidden2]))\n    hidden3 = tfa.layers.WeightNormalization(layers.Dense(units=64, activation='selu', kernel_initializer=\"lecun_normal\"))(dropout2) \n    dropout3 = layers.Dropout(0.3)(layers.Concatenate()([hidden3,dropout2, hidden2]))\n    hidden4 = tfa.layers.WeightNormalization(layers.Dense(units=32, activation='selu', kernel_initializer=\"lecun_normal\"))(dropout3) \n\n    # output layer \n    output = layers.Dense(n_classes, activation = 'softmax')(hidden4)\n    #model \n    model = keras.Model(inputs=inputs, outputs=output, name=\"resnet_baseline\")\n    \n    return model\n","dfb86ced":"##model parameters\nearly_stopping = callbacks.EarlyStopping(patience=10, min_delta=1e-5, restore_best_weights=True)\nreduce_lr = callbacks.ReduceLROnPlateau(factor = 0.6, patience = 5, verbose = 0) \noptimizer = keras.optimizers.Adam()\nmetrics=['acc']\nloss= \"sparse_categorical_crossentropy\"","c62c7771":"model = get_model(X_train)\nmodel.compile(loss=loss, optimizer = optimizer, metrics=metrics)","64553ed5":"plot_model(\n    model,\n    show_shapes=True,\n    show_layer_names=True\n)","0c15f053":"model.summary()","9d158e43":"epoch = 70\nbatch_size = 2048\nval_score = []\ntest_pred = np.zeros((1, 1))\nN_F = 10  #use 5 folds\nSKF= StratifiedKFold(n_splits=N_F, shuffle=True, random_state=42)\nfor fold, (idx_train, idx_valid) in enumerate(SKF.split(X_train,y_encoded)):\n        X_tr, y_tr = X_train[idx_train], y_encoded[idx_train]\n        X_val, y_val = X_train[idx_valid], y_encoded[idx_valid]\n        K.clear_session()\n        model = get_model(X_tr)\n        model.compile(loss=loss, optimizer = optimizer, metrics=metrics)\n        model.fit(X_tr, y_tr,\n              batch_size = batch_size, epochs =epoch,\n              validation_data=(X_val, y_val),\n              callbacks=[early_stopping, reduce_lr], verbose=0)\n        val_pred = np.argmax(model.predict(X_val), axis=1)\n        score = accuracy_score(y_val, val_pred)\n        val_score.append(score)\n        test_pred = test_pred + model.predict(test)\n        print(f\"FOLD {fold:d}: validation accuracy is {score:.6f}\")\n        _ = gc.collect()\nprint (\"**************************************************\")\nprint(f\"Mean Validation Accuracy is : {np.mean(val_score)}\")","0940da67":"predictions = np.argmax(test_pred, axis=1)\npredictions = label_encod.inverse_transform(predictions)","7f2e483c":"sample['Cover_Type'] = predictions \nsample.to_csv('resnet_baseline.csv', index=False)\nsample","86807e4f":"ax = plt.figure(figsize=(12, 6))\ncover_type=sample['Cover_Type'].value_counts().sort_index(ascending = False )\nsns.barplot(x=cover_type.index,y=cover_type,color=\"salmon\")\nplt.show()","48fd9315":" #### Basic summary statistics ","d5aba3ce":"##### Convert Prediction","d7098a61":"#### Train and Evaluation ","71e9348e":"As we can see the train and test data have similar distribution with slight differences.","b13c80fc":"##### Reduce memory size and clear space","ca3c0787":"Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, kaggle team have launched many Playground competitions that are more approachable than Featured competitions and thus, more beginner-friendly.\n\nFor this competition, you will be predicting a categorical target based on a number of feature columns given in the data. The data is synthetically generated by a GAN that was trained on a the data from [the Forest Cover Type Prediction](https:\/\/www.kaggle.com\/c\/forest-cover-type-prediction\/overview). This dataset is much larger, and may or may not have the same relationship to the target as the original data.","6757506c":"<h2 style='background:#ce67e0; border:0; color:white'><center>I hope you find this useful , Thank you <\/center><\/h2>","d0ee41b5":"**Scale data.**","88d9bed7":"#### Correlation with the target ","ae919a0b":"After we did some data exploration , now it's time to train a baseline in order to make prediction , for that we are going to use a residuel network.","2ac455cb":"#### Feature variation ","543d3cd8":"##### label encoder for target ","9eb3efdd":"<h2 style='background:#ce67e0; border:0; color:white'><center>Notebook Set up<\/center><\/h2>","5cb66248":"#### Feature Distribution\nNow let's check the distribution of numerical features in both train and test data ( binary features don't need visualization )","56405046":"The dataset as usel consist of three files :\n\n* train.csv - the training data with the target Cover_Type column\n* test.csv - the test set; you will be predicting the Cover_Type for each row in this file (the target integer class)\n* sample_submission.csv - a sample submission file in the correct format","22b3987a":"##### Correaltion matrix ","4417582e":"### How could you improve the score : \nResNet stand as a strong baseline but if well tuned it can outperform standard DNN,There are many ways that could improve the score including but not limited to :\n* Feature engineering and feature selection ( like the ones implimented by [[1]](https:\/\/www.kaggle.com\/chryzal\/features-engineering-for-you) ,[[2]](https:\/\/www.kaggle.com\/gulshanmishra\/tps-dec-21-tensorflow-nn-feature-engineering)) \n* [ Pseudolabeling](https:\/\/www.kaggle.com\/remekkinas\/tps-12-nn-tpu-pseudolabeling-0-95690)\n* hyperparameter tuning ( adding more layers , adjusting number of unites and layers connections.. )\n* [ensembling NN predictions](https:\/\/machinelearningmastery.com\/ensemble-methods-for-deep-learning-neural-networks\/)","18085c67":"**check model summary**","ef62822a":"#### check target values again ","252cdc99":"#### Submission","2ff983bd":"<h2 style='background:#ce67e0; border:0; color:white'><center>About the data<\/center><\/h2>","3c3ae354":"We got 347,190 params to train , let's go ! ","8ca6443d":"##### Check predictions distribution ","71b96d85":"### Test Data \nAfter checking the train data  it's time to check the test data.","05ab47ee":"#### Distribution of Mean and Standard deviation\nAs in previous tabular playgrounds , we want to see the distribution of the mean and standard deviation values per row and column in both train and test data.","591e0a19":"#### Build Model \nThe model architecture is inspired by Alexander's [Notebook](https:\/\/www.kaggle.com\/alexryzhkov\/python-keras-nn-residual\/notebook) with some changes.If you want to know more about ResNet and why it works check this tow youtube videos [what's resnet](https:\/\/www.youtube.com\/watch?v=ZILIbUvp5lk) and [Why resnet work](https:\/\/www.youtube.com\/watch?v=RYth6EbBUqM)","e99c33ae":"#### Data preparation ####","68568ac2":"<h2 style='background:#ce67e0; border:0; color:white'><center>Overview<\/center><\/h2>","c1ce91d3":"#### Basic summary statistic ","b4fe5e4d":"<h2 style='background:#ce67e0; border:0; color:white'><center>Load Data <\/center><\/h2>","73023ceb":"As we can see , some classes are more represented than others , which make it an imbalanced multi-class classification problem , we should be more carefull when modeling this kind of problem later on we will try to work on that.","c2b30432":"### Target Variable\nThe target for this competition is a multiclass variable (ranging from 1 to 7 )","fe18caca":"<h2 style='background:#ce67e0; border:0; color:white'><center> Implimenting a ResNet baseline <\/center><\/h2>\n","7a663706":"Based on feature variation and feature correlation , we can see that some features are more related to target than others and some of them are less variant than others, which gives ideas about feature selection.","f7a47839":"<h1> <center> Tabular Playground Series - Dec 2021<\/center><\/h1>\n\n\n![ccccccccccc.JPG](attachment:0719f09d-daf1-4939-8588-c15021e7478f.JPG)","f860c9b1":"<h2 style='background:#ce67e0; border:0; color:white'><center> Exploratory data analysis  <\/center><\/h2>\nAs in most compettion and Ml projects we first take a quick look at our train and test data in order to better understand what we are dealing with this month.\n\n\n\n#### Train Data ","6747250c":"##### Plot model "}}