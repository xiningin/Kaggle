{"cell_type":{"025b5625":"code","ec37f0c2":"code","a4de70b0":"code","2faad120":"code","6cda799c":"code","f05982b5":"code","10286e6e":"code","bc2a85af":"code","11a82da5":"code","4ae952a2":"code","ed5a978c":"code","0b1c7533":"code","f7d69681":"code","c467d8d4":"code","e31bfede":"code","bd88c985":"code","7ed6b5b4":"code","bb70b13b":"code","67c9ee54":"code","33c5be35":"code","9a3bd453":"markdown","ab876414":"markdown","a3bf9e6f":"markdown"},"source":{"025b5625":"import numpy as np\nimport pandas as pd\n\nimport os\nimport math\nimport copy\nfrom itertools import starmap\n\nimport cv2\nimport matplotlib.pyplot as plt\nimport PIL\nfrom PIL import Image\n\nimport dlib\nimport torch.nn.functional as F\nimport torch\nfrom torch import nn\nfrom torch import optim\nimport torchvision\nfrom torchvision.transforms import functional as FT","ec37f0c2":"PIC_SIZE = 48\nBATCH_SIZE = 5\n\nCROPS_PATH = '..\/input\/face-landmarks-dlib-crops\/'\nORIGS_PATH = '..\/input\/face-landmarks\/'\nONET_WTS_PATH = '..\/input\/face-landmarks-best-onet-weights\/'","a4de70b0":"def get_avg_norm_dist(image, target, predicted_landmarks):\n    \"\"\"\n    Calculate average RMSE for all 68 predicted points, then normalize by dividing it by sqrt(w * h),\n    where w - DLIB crop's width, h - DLIB crop's height.\n    \"\"\"\n    w, h = image.shape[1], image.shape[0]\n    n_points = 68\n    \n    x_gt = target[:,0]\n    y_gt = target[:,1]\n    x_pred = predicted_landmarks[:,0]\n    y_pred = predicted_landmarks[:,1]\n    diff_x = [x_gt[i] - x_pred[i] for i in range(n_points)]\n    diff_y = [y_gt[i] - y_pred[i] for i in range(n_points)]\n    \n    dist = np.sqrt(np.square(diff_x) + np.square(diff_y))\n    avg_norm_dist = np.sum(dist) \/ (n_points * np.sqrt(w * h))\n    \n    return avg_norm_dist","2faad120":"def visualize_samples(images, output, rows, columns):\n    fig = plt.figure(figsize=(15, 15))\n        \n    for i, (image, landmarks) in enumerate(zip(images, output)):\n        for point in landmarks:\n            x, y = int(point[0]), int(point[1])\n            image = cv2.circle(image, (x, y), radius=0, color=(0, 0, 255), thickness=1)\n            \n        fig.add_subplot(rows, columns, i + 1)\n        plt.imshow(image)\n            \n    plt.show()","6cda799c":"def shape_to_np(shape):\n    coords = np.zeros((68, 2))\n    for i in range(0, 68):\n        coords[i] = (shape.part(i).x, shape.part(i).y)\n    return coords","f05982b5":"def dlib_inference(images_dir, targets_csv, old_coords_csv):\n    predictor = dlib.shape_predictor('..\/input\/shape-predictor\/shape_predictor_68_face_landmarks.dat')\n    \n    images = sorted(os.listdir(images_dir))\n    targets = pd.read_csv(targets_csv, header=None)\n    targets = targets.iloc[:,1:].to_numpy().tolist()\n    \n    old_coords = pd.read_csv(old_coords_csv, header=None)\n    old_coords = old_coords.iloc[:,1:].to_numpy().tolist()\n    \n    # Some samples to visualize model output.\n    samples = []\n    \n    ced = []\n    \n    for i, (image_name, target, old_coord) in enumerate(zip(images, targets, old_coords)):\n        image = dlib.load_rgb_image(images_dir + image_name)\n        target = np.array(target).reshape((68, 2))\n        rect = dlib.rectangle(left=0, top=0, right=image.shape[1], bottom=image.shape[0])\n        output = shape_to_np(predictor(image, rect))\n        ced.append(get_avg_norm_dist(image, target + old_coord, output + old_coord))\n        \n        if i < 25:\n            samples.append((image, output))\n        \n    return ced, samples","10286e6e":"class LandmarksDataset(torch.utils.data.Dataset):\n    def __init__(self, crops_dir, crops_csv, origs_dir, origs_csv, old_coords_csv):\n        \"\"\"\n        Args:\n            crops_dir (string): Directory with all the cropped images.\n            crops_csv (string): Path to the csv file with annotations for cropped images.\n            origs_dir (string): Directory with all the original images.\n            origs_csv (string): Path to the csv file with annotations for original images.\n            old_coords_csv (string): Path to the csv file with upper left corners of face bboxes on original images.\n            train_flag (boolean): Indicates training phase (if True, then do augmentations).\n        \"\"\"\n        self.crops_landmarks = pd.read_csv(crops_csv)\n        self.crops_dir = crops_dir\n        \n        self.origs_landmarks = pd.read_csv(origs_csv)\n        self.origs_dir = origs_dir\n        self.old_coords = pd.read_csv(old_coords_csv)\n\n    def __len__(self):\n        return len(self.crops_landmarks)\n\n    def __getitem__(self, idx):\n        img_name = os.path.join(self.crops_dir, self.crops_landmarks.iloc[idx, 0])\n        image = dlib.load_rgb_image(img_name)\n        landmarks = self.crops_landmarks.iloc[idx, 1:]\n        landmarks = np.array([landmarks])\n        landmarks = landmarks.astype('float').reshape(-1, 2)\n\n        original_image = dlib.load_rgb_image(self.origs_dir + '\/' + os.path.basename(img_name))\n        # Resizing too big images the same way they were resized while creating crops dataset.\n        w, h = original_image.shape[1], original_image.shape[0]\n        if w > 1000 or h > 1000:\n            original_image = cv2.resize(original_image, (300, int(h * 300 \/ w)))\n            \n        old_coords = self.old_coords.iloc[idx, 1:]\n        old_coords = np.array(old_coords)\n        old_coords = old_coords.astype('float').flatten()\n        \n        return original_image, old_coords, image, landmarks","bc2a85af":"class ONet(nn.Module):\n    def __init__(self):\n        super(ONet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3)\n        torch.nn.init.xavier_uniform_(self.conv1.weight)\n        \n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n        torch.nn.init.xavier_uniform_(self.conv2.weight)\n        \n        self.conv3 = nn.Conv2d(64, 64, kernel_size=3)\n        torch.nn.init.xavier_uniform_(self.conv3.weight)\n        \n        self.conv4 = nn.Conv2d(64, 128, kernel_size=2)\n        torch.nn.init.xavier_uniform_(self.conv4.weight)\n        \n        self.pool1 = nn.MaxPool2d(2, 2)\n        self.pool2 = nn.MaxPool2d(2, 2)\n        self.pool3 = nn.MaxPool2d(2, 2)\n        self.fc1 = nn.Linear(1152, 256)\n        self.fc2 = nn.Linear(256, 136)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.pool1(x)\n        x = F.relu(x)\n        \n        x = self.conv2(x)\n        x = self.pool2(x)\n        x = F.relu(x)\n        \n        x = self.conv3(x)\n        x = self.pool3(x)\n        x = F.relu(x)\n        \n        x = self.conv4(x)\n        x = F.relu(x)\n        \n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.fc2(x)\n        \n        return x","11a82da5":"def get_full_size_coords(image, landmarks, old_coords):\n    \"\"\"\n    Get coordinates predicted by model and recalculate them for the original image (not cropped).\n    Args:\n        output_landmarks (ndarray): predicted face landmarks (for cropped image resized to 48x48).\n        old_coords (tuple of two floats): upper left corner of the face bounding box on the original image.\n    Return:\n        landmarks (ndarray): landmarks for a full size image, shape (68, 2).\n    \"\"\"\n    w, h = image.shape[1], image.shape[0]\n    landmarks = torch.Tensor(landmarks).reshape((68, 2))\n    rescaled_landmarks = torch.zeros(landmarks.shape)\n    rescaled_landmarks[:,0] = landmarks[:,0] \/ (48 \/ w)\n    rescaled_landmarks[:,1] = landmarks[:,1] \/ (48 \/ h)\n    rescaled_landmarks += torch.Tensor(old_coords).float()\n    return rescaled_landmarks","4ae952a2":"def resize_image(image, target):\n    w, h = image.shape[1], image.shape[0]\n    image = cv2.resize(image, (PIC_SIZE, PIC_SIZE))\n    target[:,0] =  target[:,0] * (PIC_SIZE \/ w)\n    target[:,1] =  target[:,1] * (PIC_SIZE \/ h)\n    return image, target\n\n\ndef prepare_data(images, targets, device):\n    \"\"\"\n    Resizes images to 48x48, recalculates landmarks and sends tensors to CUDA.\n    \"\"\"\n    resized = list(starmap(resize_image, zip(images, targets)))\n    images, targets = map(list, zip(*resized))\n    \n    images = [FT.to_tensor(image) for image in images]\n    images = torch.stack(list(images)).float().to(device) \n    \n    targets = torch.from_numpy(np.stack(targets))\n    targets = targets.reshape(targets.shape[0], targets.shape[1] * targets.shape[2]).float().to(device)\n    return images, targets","ed5a978c":"def collate_fn(batch):\n    \"\"\"\n    Describes how to combine tensors of different sizes.\n    \"\"\"\n    return tuple(zip(*batch))","0b1c7533":"def onet_inference(best_weights):\n    model = torch.load(best_weights)\n    model.eval()\n    model.to(device)\n    \n    ced = []\n    \n    criterion = torch.nn.MSELoss(reduction='none')\n    \n    test_loader = torch.utils.data.DataLoader(test, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n    \n    for j, (original_images, old_coords, images, targets) in enumerate(test_loader):\n        rescaled_images, rescaled_targets = prepare_data(images, targets, device)\n        \n        output = model(rescaled_images)\n        output = [landmarks.cpu() for landmarks in output]\n        \n        # Visualize results for 5 first batchs.\n        if j < 5:\n            test_images = [np.asarray(FT.to_pil_image(image.cpu().detach())) for image in rescaled_images]\n            test_output = [landmarks.detach().numpy().reshape((68, 2)) for landmarks in output]\n            visualize_samples(test_images, test_output, rows=1, columns=BATCH_SIZE)\n        \n        # Get model's output in scale of the original image.\n        full_size_landmarks = list(starmap(get_full_size_coords, zip(images, output, old_coords)))\n        landmarks = torch.stack(full_size_landmarks, axis=0)\n        \n        # Get target in scale of the original image.\n        full_size_targets = list(starmap(get_full_size_coords, zip(images, targets, old_coords)))\n        targets = torch.stack(full_size_targets, axis=0)\n\n        for image, target, predicted_landmarks in zip(images, full_size_targets, full_size_landmarks):\n            ced.append(get_avg_norm_dist(image, target.cpu().detach().numpy(), \n                                         predicted_landmarks.cpu().detach().numpy()))\n        \n    return ced","f7d69681":"def plot_ced(ced, model_name):\n    data_sorted = sorted(ced)\n    p = 1. * np.arange(len(ced)) \/ (len(ced) - 1)\n    \n    data = [(x, y) for x, y in zip(data_sorted, p) if x <= 0.08]\n    data_sorted, p = list(zip(*data))\n\n    fig = plt.figure(figsize=(10,5))\n    ax = fig.add_subplot()\n    ax.plot(data_sorted, p)\n    ax.set_xlabel('Normalized RMSE')\n    ax.set_ylabel('Percentage')\n    area = np.trapz(p, data_sorted)\n    plt.title(model_name + ': AUC = {}'.format(area))\n    plt.show()","c467d8d4":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","e31bfede":"dlib_ced, dlib_samples = dlib_inference(CROPS_PATH + 'menpo_test_crops\/menpo_test_crops\/menpo_test_crops\/', \\\n                                        CROPS_PATH + 'menpo_test_crop_labels.csv', \\\n                                        CROPS_PATH + 'menpo_test_old_coords.csv')","bd88c985":"dlib_images, dlib_output = list(map(list, zip(*dlib_samples)))\nresized_imgs = list(starmap(resize_image, zip(copy.deepcopy(dlib_images), copy.deepcopy(dlib_output))))\ndlib_img, dlib_out = map(list, zip(*resized_imgs))\nvisualize_samples(dlib_img, dlib_out, 5, 5)","7ed6b5b4":"plot_ced(dlib_ced, 'DLIB')","bb70b13b":"test = LandmarksDataset(CROPS_PATH + 'menpo_test_crops\/menpo_test_crops\/menpo_test_crops', CROPS_PATH + 'menpo_test_crop_labels.csv', \\\n                        ORIGS_PATH + 'menpo_test\/menpo_test', ORIGS_PATH + 'menpo_test_labels.csv', \\\n                        CROPS_PATH + 'menpo_test_old_coords.csv')","67c9ee54":"onet_ced = onet_inference(ONET_WTS_PATH + 'best_wts_aug.pt')","33c5be35":"plot_ced(onet_ced, 'O-Net')","9a3bd453":"# DLIB C++ Library\n\nFirst of all let's try one of the most popular tools for face recognition \u2014 DLIB C++ Library.\nTo save some time I used already prepared dataset of face crops that I got also using DLIB's face detector. Here we'll try DLIB's shape predictor that predicts 68 points for single face (IBUG format). We're going to test the predictor using only Menpo dataset since it was trained on 300W dataset. \n\n### Metric\n\nThe final metric for comparison is going to be AUC CED \u2014 area under graph of cumulative error distribution. For error we'll take Root Square Mean Error (RMSE), normalized by DLIB's crop width and height, i.e. divided by square root of their product.","ab876414":"# Conclusion\n\nWe achieved same results as DLIB's shape predictor in terms of used metric. However, if we print the whole testing dataset we can see that O-Net has less precision in context of just one image, but it has less complete misses compared to DLIB. Also both models aren't immune to highly rotated pictures so they require more training with more augmentations.","a3bf9e6f":"# O-Net\n\nHere we'll test convolutional neural network O-Net from the paper **Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks**. The original network had 16 output neurons: 2 for face classification, 4 for bounding box regression and 10 for face landmarks and it was modified to have 136 outputs (68 points) to match IBUG format. \n\n### Training\n\nIt was trained for 600 epochs with learning rates 0.01, 0.005 abd 0.001 (changing every 200 epochs) on Menpo and 300W datasets. We're also going to test it only using test part of Menpo dataset."}}