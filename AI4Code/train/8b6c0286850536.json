{"cell_type":{"ed166c27":"code","c2a2f9b7":"code","a43fbe6b":"code","95f8c32b":"code","f2aa0314":"code","bf9befd7":"code","c77ea8c7":"code","08060d1b":"code","7e3a2604":"code","d8af1ab4":"code","3efc7ad2":"code","c13f7a45":"code","93fbc35f":"code","7345ff38":"code","ebca9475":"code","c1a6368f":"code","d1bf4eb7":"code","f911760e":"code","ebb52c96":"code","64ba59be":"code","7479d277":"code","d26af339":"code","61c5191c":"code","c60d97e4":"code","152c62b7":"code","9c810b92":"code","d815b186":"code","a6969de0":"code","eacc77ad":"code","4554e0dc":"code","7bd7a050":"code","b39d5765":"code","734e8040":"markdown","ddd4c59d":"markdown","0251913a":"markdown","c6928775":"markdown"},"source":{"ed166c27":" # This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS\nimport string\nimport nltk\nimport seaborn as sns\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c2a2f9b7":"fake = pd.read_csv(\"\/kaggle\/input\/fake-and-real-news-dataset\/Fake.csv\")\ntrue=  pd.read_csv(\"\/kaggle\/input\/fake-and-real-news-dataset\/True.csv\")","a43fbe6b":"fake.head()","95f8c32b":"true.head()","f2aa0314":"#Counting by Subjects \nfor key,count in fake.subject.value_counts().iteritems():\n    print(f\"{key}:\\t{count}\")\n    \n#Getting Total Rows\nprint(f\"Total Records:\\t{fake.shape[0]}\")","bf9befd7":"#Counting by Subjects \nfor key,count in true.subject.value_counts().iteritems():\n    print(f\"{key}:\\t{count}\")\n    \n#Getting Total Rows\nprint(f\"Total Records:\\t{fake.shape[0]}\")","c77ea8c7":" emptyt=[index for index,text in enumerate(true.text.values) if str(text).strip() == '']\n print(f\"No of empty rows: {len(emptyt)}\")\n print(f\"No of  total rows: {len(true.text.values)}\")\n \n","08060d1b":"true=true[true[\"text\"].str.strip() !=''] \nprint(f\"No of  total rows: {len(true.text.values)}\")\ntrue","7e3a2604":" emptyf=[index for index,text in enumerate(fake.text.values) if str(text).strip() == '']\n print(f\"No of empty rows: {len(emptyf)}\")\n print(f\"No of total rows: {len(fake.text.values)}\")","d8af1ab4":"true['From'] = true['text'].str.split('-' , 1).str[0]\ntrue['text'] = true['text'].str.split('-' , 1).str[1]\ntrue","3efc7ad2":"true['num'] = 1\nfake['num'] = 0\n\n\ndata = pd.concat([fake, true])\n\ndata\n\n# data = pd.DataFrame()\n# data = true.append(fake)\n# data\n# \u0395\u03bd\u03ce\u03bd\u03c9 \u03c4\u03b1 \u03b4\u03b5\u03b4\u03bf\u03bc\u03ad\u03bd\u03b1 \u03ba\u03b1\u03b9 \u03c4\u03b1 \u03ba\u03ac\u03bd\u03c9 \u03c9\u03c2 \u03ad\u03bd\u03b1.\u0398\u03ad\u03c4\u03c9 \u03c9\u03c2 fake news =0\u03ba\u03b1\u03b9 true new = 1.","c13f7a45":"\nsns.set_style(\"darkgrid\")\nsns.countplot(data.num)\n# by_tf = data.num.value_counts()\n# by_tf.plot(kind='bar')","93fbc35f":"# data.isna().sum()\n# data.title.count()\ndata.subject.value_counts()","7345ff38":"patternDel = \"http\"\nfilter1 = data['date'].str.contains(patternDel)\ndata = data[~filter1]\n# \u0394\u03b9\u03b1\u03b3\u03c1\u03ac\u03c6\u03c9 \u03bf,\u03c4\u03b9 \u03b5\u03af\u03bd\u03b1\u03b9 link.","ebca9475":"pattern = \"Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec\"\nfilter2 = data['date'].str.contains(pattern)\ndata = data[filter2]","c1a6368f":"data['date'] = pd.to_datetime(data['date'])","d1bf4eb7":"data = data.fillna(\"\")\ndata","f911760e":"datasub=data.groupby(['subject', 'num'])['text'].count()\ndatasub = datasub.unstack().fillna(0)\ndatasub","ebb52c96":"ax = datasub.plot(kind = 'bar', figsize = (12,8), grid = True)\nplt.show()","64ba59be":"data_ = data.copy()\ndata_ = data_.sort_values(by = ['date'])\ndata_ = data_.reset_index(drop=True)\ndata_\n\n# \u03a3\u03bf\u03c1\u03c4\u03ac\u03c1\u03b9\u03c3\u03bc\u03b1 \u03bc\u03b5 \u03b2\u03ac\u03c3\u03b7 \u03b7\u03bc\u03b5\u03c1\u03bf\u03bc\u03b7\u03bd\u03af\u03b1","7479d277":"# \u039c\u03b5\u03c4\u03c1\u03ac\u03c9 \u03c4\u03b1 fake news \u03b1\u03bd\u03b1 \u03b7\u03bc\u03ad\u03c1\u03b1\ndataf = data_[data_['num'] == 0]\ndataf = dataf.groupby(['date'])['num'].count()\nf = pd.DataFrame(dataf)\nf","d26af339":"# \u039c\u03b5\u03c4\u03c1\u03ac\u03c9 \u03c4\u03b1 real news \u03b1\u03bd\u03b1 \u03b7\u03bc\u03ad\u03c1\u03b1\ndatat = data_[data_['num'] == 1]\ndatat = datat.groupby(['date'])['num'].count()\nt = pd.DataFrame(datat)\nt","61c5191c":"# plt.figure(figsize = (20,20)) # Text that is not Real\n# wc = WordCloud(max_words = 2000 , width = 1600 , height = 800 , stopwords = STOPWORDS).generate(\" \".join(data[data.num == 1].text))\n# plt.axis(\"off\")\n# plt.imshow(wc , interpolation = 'bilinear')\n# plt.show()","c60d97e4":"# plt.figure(figsize = (20,20)) # Text that is not Real\n# wc = WordCloud(max_words = 2000 , width = 1600 , height = 800 , stopwords = STOPWORDS).generate(\" \".join(data[data.num == 0].text))\n# plt.axis(\"off\")\n# plt.imshow(wc , interpolation = 'bilinear')\n# plt.show()","152c62b7":"stopwords = nltk.corpus.stopwords.words(\"english\")\n\n# \u039a\u03b1\u03b8\u03b1\u03c1\u03af\u03b6\u03b5\u03b9 \u03c4\u03b1 \u03ba\u03b5\u03af\u03bc\u03b5\u03bd\u03b1 \u03c3\u03c4\u03bf 'text' \u03b1\u03c0\u03cc \u03c4\u03b1 \u03c3\u03b7\u03bc\u03b5\u03af\u03b1 \u03c3\u03c4\u03af\u03be\u03b7\u03c2,\u03c0\u03b1\u03c1\u03b5\u03bd\u03b8\u03ad\u03c3\u03b5\u03b9\u03c2 \u03ba\u03c4\u03bb\ndef precleaning(x):\n    f = x\n    f = f.lower()\n    f = re.sub('\\[.*?\\]', '', f) \n    f = re.sub(r'[^\\w\\s]','',f) \n    f = re.sub('\\w*\\d\\w*', '', f) \n    f = re.sub(r'http\\S+', '', f)\n    f = re.sub('\\n', '', f)\n    return f\n\n# \u0391\u03c6\u03b1\u03b9\u03c1\u03b5\u03af \u03c4\u03b9\u03c2 stopwords\ndef remove_stopwords(text):\n    token_text = nltk.word_tokenize(text)\n    removed = [word for word in token_text if word not in stopwords]\n    joinedtext = ' '.join(removed)\n    return joinedtext\n\n# \u03a3\u03c5\u03bd\u03bf\u03bb\u03b9\u03ba\u03cc \u03ba\u03b1\u03b8\u03ac\u03c1\u03b9\u03c3\u03bc\u03b1\ndef cleaning(text):\n    text = precleaning(text)\n    text = remove_stopwords(text)\n    return text\n\nnew_data = data_.copy()\nnew_data['text'] = data_.text.apply(lambda x : cleaning(x))\nnew_data.head()","9c810b92":"# stopwords = set(STOPWORDS)\n\ndef common_tokens_title(data, feature, name):\n    column = data[feature].str.lower() \n    text = ' '.join(column)\n    exclude = set(string.punctuation)\n    words = ''.join(char for char in text if char not in exclude)\n    words_splitted = words.split()\n    words_stopped = [word for word in words_splitted if not word in stopwords]\n    print(f'{name}:\\n{pd.DataFrame(nltk.FreqDist(words_stopped).most_common(10))[0]}')\n    \ncommon_tokens_title(true, 'title', 'Most common descriptive words in Real News Titles')\nprint('\\n')\ncommon_tokens_title(fake, 'title', 'Most common descriptive words in Fake News Titles')\nprint('\\n')\ncommon_tokens_title(new_data, 'title', 'Most common descriptive words in Combined News Titles')","d815b186":"plt.figure(figsize = (20,20)) # Text that is not Real\nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800 , stopwords = STOPWORDS).generate(\" \".join(new_data[new_data.num == 1].text))\nplt.axis(\"off\")\nplt.imshow(wc , interpolation = 'bilinear')\nplt.show()","a6969de0":"plt.figure(figsize = (20,20)) # Text that is not Real\nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800 , stopwords = STOPWORDS).generate(\" \".join(new_data[new_data.num == 0].text))\nplt.axis(\"off\")\nplt.imshow(wc , interpolation = 'bilinear')\nplt.show()","eacc77ad":"def get_ngrams(corpus, n, g):\n    vec = CountVectorizer(ngram_range=(g, g)).fit(corpus)\n    words = vec.transform(corpus)\n    total_words = words.sum(axis=0) \n    words_freq = [(word, total_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\n# def graph_ngrams(data_set , ngrams):\n#     plt.figure(figsize = (16,9))\n#     gram = get_ngrams(data_set.text,10,ngrams)\n#     gram = dict(gram)\n#     sns.barplot(x=list(gram.values()),y=list(gram.keys()))\n    \n# graph_ngrams(new_data, 1)","4554e0dc":"# \u0395\u03bc\u03c6\u03ac\u03bd\u03b9\u03b6\u03b5\u03b9 unigrams\nplt.figure(figsize = (16,9))\nuni = get_ngrams(new_data.text,10,1)  #\u0395\u03bc\u03c6\u03b1\u03bd\u03af\u03b6\u03b5\u03b9 \u03c4\u03b1 \u03b4\u03ad\u03ba\u03b1 \u03c0\u03c1\u03ce\u03c4\u03b1 \u03c0\u03b9\u03bf \u03c3\u03c5\u03c7\u03bd\u03ac unigrams\nuni = dict(uni)\nsns.barplot(x=list(uni.values()),y=list(uni.keys()))","7bd7a050":"# \u0395\u03bc\u03c6\u03b1\u03bd\u03af\u03b6\u03b5\u03b9 bigrams\nplt.figure(figsize = (16,9))\nbi = get_ngrams(new_data.text,10,2)  #\u0395\u03bc\u03c6\u03b1\u03bd\u03af\u03b6\u03b5\u03b9 \u03c4\u03b1 \u03b4\u03ad\u03ba\u03b1 \u03c0\u03c1\u03ce\u03c4\u03b1 \u03c0\u03b9\u03bf \u03c3\u03c5\u03c7\u03bd\u03ac bigrams\nbi = dict(bi)\nsns.barplot(x=list(bi.values()),y=list(bi.keys()))","b39d5765":"plt.figure(figsize = (16,9))\ntri = get_ngrams(new_data.text,10,3) #\u0395\u03bc\u03c6\u03b1\u03bd\u03af\u03b6\u03b5\u03b9 \u03c4\u03b1 \u03b4\u03ad\u03ba\u03b1 \u03c0\u03c1\u03ce\u03c4\u03b1 \u03c0\u03b9\u03bf \u03c3\u03c5\u03c7\u03bd\u03ac trigrams\ntri = dict(tri)\nsns.barplot(x=list(tri.values()),y=list(tri.keys()))","734e8040":" **\u038a\u03c3\u03c9\u03c2 \u03bd\u03b1 \u03bc\u03b7\u03bd \u03c7\u03c1\u03b5\u03b9\u03ac\u03b6\u03bf\u03bd\u03c4\u03b1\u03b9 \u03c4\u03b1 \u03b4\u03c5\u03bf \u03c0\u03ac\u03bd\u03c9 wordclouds \u03b3\u03b9\u03b1\u03c4\u03af \u03b1\u03c0\u03bf \u03b5\u03b4\u03ce \u03ba\u03b1\u03b9 \u03bc\u03b5\u03c4\u03ac \u03ba\u03b1\u03b8\u03b1\u03c1\u03af\u03b6\u03bf\u03bd\u03c4\u03b1\u03b9 \u03c4\u03b1 \u03b4\u03b5\u03b4\u03bf\u03bc\u03ad\u03bd\u03b1**","ddd4c59d":"**\u039c\u03c0\u03bf\u03c1\u03bf\u03c5\u03bc\u03b5 \u03bd\u03b1 \u03b4\u03b7\u03bc\u03b9\u03bf\u03c5\u03c1\u03b3\u03ae\u03c3\u03bf\u03c5\u03bc\u03b5 \u03bc\u03b9\u03b1 \u03c3\u03c5\u03bd\u03b1\u03c1\u03c4\u03b7\u03c3\u03b7(graph_ngrams) \u03b3\u03b9\u03b1 \u03bd\u03b1 \u03c0\u03bb\u03bf\u03c4\u03ac\u03c1\u03b5\u03b9 \u03c4\u03b1 \u03b4\u03b5\u03b4\u03bf\u03bc\u03ad\u03bd\u03b1, \u03b1\u03bd\u03c4\u03b9 \u03bd\u03b1 \u03ba\u03ac\u03bd\u03bf\u03c5\u03bc\u03b5 copy-paste.**","0251913a":"**\u0391\u03c0\u03bf \u03b5\u03b4\u03ce \u03ba\u03b1\u03b9 \u03bc\u03b5\u03c4\u03b1 \u03c7\u03c1\u03b7\u03c3\u03b9\u03bc\u03bf\u03c0\u03bf\u03b9\u03ce \u03c4\u03bf new_data: \u03c4\u03b1 \u03b4\u03b5\u03b4\u03bf\u03bc\u03ad\u03bd\u03b1 \u03b1\u03c6\u03bf\u03cd \u03ad\u03c7\u03bf\u03c5\u03bd \u03ba\u03b1\u03b8\u03b1\u03c1\u03b9\u03c3\u03c4\u03b5\u03af.**","c6928775":"**\u039c\u03c0\u03bf\u03c1\u03bf\u03cd\u03bc\u03b5 \u03bd\u03b1 \u03b2\u03ac\u03bb\u03bf\u03c5\u03bc\u03b5 \u03ba\u03b1\u03b9 unigram, bigram, trigram \u03ba\u03b1\u03b9 \u03b3\u03b9\u03b1 \u03c4\u03b1 True News(datat) \u03ba\u03b1\u03b9 Fake News(dataf) \u03be\u03b5\u03c7\u03c9\u03c1\u03b9\u03c3\u03c4\u03ac**"}}