{"cell_type":{"5c48f262":"code","609120d2":"code","31a743e3":"code","c91494ca":"code","e381c02a":"code","38fcb83d":"code","1718283e":"code","35093733":"code","70434ae4":"code","3725c463":"code","cfd6cd0f":"code","007fbef2":"code","b1d2a78d":"code","283b531e":"code","19ca6c9c":"code","d306928a":"code","64fd7b06":"code","79de5560":"code","02ca6d90":"code","59fc7710":"code","8d42a8e4":"code","1851566a":"code","cb75735a":"code","b524c09b":"code","614d7d8d":"code","58d57e4c":"code","cef27a6a":"code","7d22f616":"code","250188da":"code","33e85a82":"code","a4eb73b6":"code","f6196363":"code","ada80a69":"code","ce5e237c":"code","35b0b26c":"code","18739e99":"code","e6f16084":"code","4940d515":"code","e29c3481":"code","5f69d081":"code","91ed8555":"code","534147f8":"code","ba88e0d2":"code","22fe200e":"code","6f455478":"code","410e3789":"code","98a0704c":"code","df45fc7e":"code","c4aee074":"code","a3b4aa19":"code","ecf7f03d":"code","1f78d602":"code","00fac496":"code","13b52bb2":"code","b885868c":"code","a95bbe83":"code","e87b2ecf":"code","8b5dabe6":"code","cf86b1b3":"code","22ba8537":"code","0b3b166c":"code","c17e6138":"code","40873683":"markdown","27af1069":"markdown","0dce3c72":"markdown","baf0a1eb":"markdown","c51d9cca":"markdown","b4762a88":"markdown","c0104857":"markdown","326b4a33":"markdown","c55b3183":"markdown","1c856df8":"markdown","f55f7334":"markdown","eefbfc46":"markdown","3c0b2f70":"markdown","1161e850":"markdown","0cfc2926":"markdown","e137e94d":"markdown","7dd346df":"markdown","14900a33":"markdown","b2045a7b":"markdown","6b9d68c3":"markdown","992f558e":"markdown","a71f0cd3":"markdown","4b87c09f":"markdown","befc852b":"markdown","81f5423c":"markdown","ab33b3e5":"markdown","a16e7548":"markdown","a7580d00":"markdown","9e393440":"markdown","cf54cb45":"markdown","096d442d":"markdown","79fc4113":"markdown"},"source":{"5c48f262":"import pandas as pd\nimport numpy as np","609120d2":"#set date as index\ndf = pd.read_csv('..\/input\/learn-time-series-forecasting-from-gold-price\/gold_price_data.csv',index_col='Date', parse_dates=['Date'], dayfirst=False)","31a743e3":"#gold price is not collected in daily basis at the beginning\ndf.head(10)","c91494ca":"#gold price almost collected in daily basis later\ndf.tail(10)","e381c02a":"df.shape","38fcb83d":"df.isnull().sum()","1718283e":"df.describe()","35093733":"df.plot()","70434ae4":"import matplotlib.pyplot as plt\nfrom pylab import rcParams","3725c463":"#convert time series to a frequency of year. Original frequency is quarterly\ndf.asfreq('Y').plot()","cfd6cd0f":"df.asfreq('M').plot()","007fbef2":"#compare time series with a past of itself\nplt.plot(df, label='Original')\nplt.plot(df.shift(300), label='Lagged')\nplt.legend()\nplt.show()","b1d2a78d":"#change in absolute no\ndf.diff().plot()","283b531e":"#% change\ndf.pct_change().plot()","19ca6c9c":"#approx. interval of first 50 rows is ranged from month to quartar\ndf[:50].pct_change().plot()","d306928a":"#data from around 1979 is more completed (daily)\n#periods: shift for forming percent change to have briefly monthly %change\ndf[51:].pct_change(periods=30).plot()","64fd7b06":"df[60:]","79de5560":"from statsmodels.tsa.seasonal import seasonal_decompose","02ca6d90":"rcParams['figure.figsize'] = 6,5\n\nresult = seasonal_decompose(df, model='multiplicative', period=120)\nresult.plot()\nplt.show()","59fc7710":"#original df isn't in fixed frequency, so '365 days' is used instead\nrcParams['figure.figsize'] = 10,5\nplt.plot(df, label='Original')\nplt.plot(df.rolling('365D').mean(), label='Rolling mean')\nplt.legend()\nplt.show()","8d42a8e4":"rcParams['figure.figsize'] = 10,5\nplt.plot(df, label='Original')\nplt.plot(df.expanding().mean(), label='Expanding mean')\nplt.plot(df.expanding().std(), label='Expanding std')\nplt.legend()\nplt.show()","1851566a":"from statsmodels.tsa.stattools import adfuller\n\ndef adf(df):\n    rcParams['figure.figsize'] = 10,5\n    dftest = adfuller(df, autolag='AIC')\n    adf = pd.Series(dftest[0:4], index=['Test statistics','p-value','number of lags used','number of observations'])\n    #critical values are stored in dictionary format\n    for key,value in dftest[4].items():\n        adf['Critical values({})'.format(key)] = value\n    return adf, df.plot()","cb75735a":"#original series\nadf(df)","b524c09b":"#1st order differencing\nadf(df.diff().dropna())","614d7d8d":"#2nd order differencing\nadf(df.diff().diff().dropna())","58d57e4c":"from statsmodels.graphics.tsaplots import plot_acf, plot_pacf","cef27a6a":"plt.rcParams.update({'figure.figsize':(10,8), 'figure.dpi':120})\nfig, axes = plt.subplots(3, 1, sharex=True)\n\nplot_acf(df, ax=axes[0])\naxes[0].set_title('Original series')\n\nplot_acf(df.diff().dropna(), ax=axes[1])\naxes[1].set_title('1st order differencing')\n\nplot_acf(df.diff().diff().dropna(), ax=axes[2])\naxes[2].set_title('2nd order differencing')\n\nplt.show()","7d22f616":"pip install pmdarima","250188da":"from statsmodels.tsa.arima_model import ARIMA\nfrom pmdarima.arima.utils import ndiffs","33e85a82":"# ADF Test\nndiffs(df, test='adf')  # 2\n\n# KPSS test\nndiffs(df, test='kpss')  # 0\n\n# PP test:\nndiffs(df, test='pp')  # 2","a4eb73b6":"rcParams['figure.figsize'] = 6,3\nplot_pacf(df.diff().dropna())\nplt.show()","f6196363":"rcParams['figure.figsize'] = 6,3\nplot_acf(df.diff().dropna())\nplt.show()","ada80a69":"model = ARIMA(df, order=(1,1,1))\nmodel_fit = model.fit(disp=0)\nprint(model_fit.summary())","ce5e237c":"rcParams['figure.figsize'] = 12,5\nresiduals = pd.DataFrame(model_fit.resid)\nfig, ax = plt.subplots(1,2)\nresiduals.plot(title=\"Residuals\", ax=ax[0])\nresiduals.plot(kind='kde', title='Density', ax=ax[1])\nplt.show()","35b0b26c":"rcParams['figure.figsize'] = 12,5\nmodel_fit.plot_predict(dynamic=False)\nplt.show()","18739e99":"df.shape","e6f16084":"#want to verify the recent 3 month-trend\n#simply create train-test split that set test size around 90 rows\ntrain = df[:10700]\ntest = df[10700:]","4940d515":"#arima(1,1,1)\nmodel_val = ARIMA(train, order=(1,1,1))\nmodel_val_fit = model_val.fit(disp=-1)","e29c3481":"# alpha=0.05 --> 95% conf\nfc, se, conf = model_val_fit.forecast(87, alpha=0.05)","5f69d081":"fc_series = pd.Series(fc, index=test.index)\nlower_series = pd.Series(conf[:, 0], index=test.index)\nupper_series = pd.Series(conf[:, 1], index=test.index)","91ed8555":"plt.figure(figsize=(12,5), dpi=100)\nplt.plot(train[10000:], label='Train')\nplt.plot(test, label='Actual')\nplt.plot(fc_series, label='Forecast')\nplt.fill_between(lower_series.index, lower_series, upper_series, color='k', alpha=.15)\nplt.title('Forecast vs Actual')\nplt.legend(loc='upper left', fontsize=8)\nplt.show()","534147f8":"from sklearn.metrics import mean_squared_error\nfrom statsmodels.tools.eval_measures import rmse","ba88e0d2":"def arima(d,p,q):\n    train = df[:10700]\n    test = df[10700:]\n\n    model_val = ARIMA(train, order=(d,p,q))\n    model_val_fit = model_val.fit(disp=-1)\n    print(model_val_fit.summary())\n\n    fc, se, conf = model_val_fit.forecast(87, alpha=0.05)\n    fc_series = pd.Series(fc, index=test.index)\n    lower_series = pd.Series(conf[:, 0], index=test.index)\n    upper_series = pd.Series(conf[:, 1], index=test.index)\n\n    plt.figure(figsize=(12,5), dpi=100)\n    plt.plot(train[10000:], label='Train')\n    plt.plot(test, label='Actual')\n    plt.plot(fc_series, label='Forecast')\n    plt.fill_between(lower_series.index, lower_series, upper_series, color='k', alpha=.15)\n    plt.title('Forecast vs Actual')\n    plt.legend(loc='upper left', fontsize=8)\n    print('RSME:{}'.format(round(rmse(test, fc_series).mean(),4)))\n    return plt.show()","22fe200e":"arima(3,2,1)","6f455478":"def arima_size(day,d,p,q):\n    n = 10787-day\n    train = df[:n]\n    test = df[n:]\n\n    model_val = ARIMA(train, order=(d,p,q))\n    model_val_fit = model_val.fit(disp=-1)\n\n    fc, se, conf = model_val_fit.forecast(day, alpha=0.05)\n    fc_series = pd.Series(fc, index=test.index)\n    lower_series = pd.Series(conf[:, 0], index=test.index)\n    upper_series = pd.Series(conf[:, 1], index=test.index)\n\n    plt.figure(figsize=(12,5), dpi=100)\n    plt.plot(train[10400:], label='Train')\n    plt.plot(test, label='Actual')\n    plt.plot(fc_series, label='Forecast')\n    plt.fill_between(lower_series.index, lower_series, upper_series, color='k', alpha=.15)\n    plt.title('Forecast vs Actual')\n    plt.legend(loc='upper left', fontsize=8)\n    print('RSME:{}'.format(round(rmse(test, fc_series).mean(),4)))\n    return plt.show()","410e3789":"#predict next 30 days\narima_size(day=30,d=1,p=1,q=1)","98a0704c":"#predict next 10 days\narima_size(day=10,d=1,p=1,q=1)","df45fc7e":"import pmdarima as pm","c4aee074":"train = df[:10757]\ntest = df[10757:]","a3b4aa19":"model_auto_val = pm.auto_arima(train, start_p=1, start_q=1,\n                      test='adf',       # adf test help find optimal 'd'\n                      max_p=3, max_q=3, # maximum p and q\n                      m=1,              # frequency of series\n                      d=None,           # let model determine 'd'\n                      seasonal=False,   # No Seasonality\n                      start_P=0, \n                      D=0, \n                      trace=True,\n                      error_action='ignore',  \n                      suppress_warnings=True, \n                      stepwise=True)\n\nprint(model_auto_val.summary())","ecf7f03d":"plt.figure(figsize=(12,5), dpi=100)\n\nfitted, confint = model_auto_val.predict(30, return_conf_int=True)\n\n# make series for plotting purpose\nauto_series = pd.Series(fitted, index=test.index)\nlower_series = pd.Series(confint[:, 0], index=test.index)\nupper_series = pd.Series(confint[:, 1], index=test.index)\n\nplt.plot(train[10400:], label='Train')\nplt.plot(test, label='Actual')\nplt.plot(auto_series, label='Forecast')\nplt.fill_between(lower_series.index, lower_series, upper_series, color='k', alpha=.15)\nplt.title('Forecast vs Actual in next 30 days')\nplt.legend(loc='upper left', fontsize=12)\nprint('RSME:{}'.format(round(rmse(test, auto_series).mean(),4)))\nplt.show()","1f78d602":"conda install -c conda-forge fbprophet -y","00fac496":"from fbprophet import Prophet","13b52bb2":"#DataFrame must have a specific format\n#1st column is \u2018ds\u2018 and contain the date-times\n#2nd column is \u2018y\u2018 and contain the observations\n\ndf_fb = pd.read_csv('..\/input\/learn-time-series-forecasting-from-gold-price\/gold_price_data.csv')\ndf_fb = df_fb.rename(columns={'Date':'ds','Value':'y'})\n\ntrain_fb = df_fb[:10757]\ntest_fb = df_fb[10757:]\n\nmodel2 = Prophet()\nmodel2.fit(train_fb)","b885868c":"future = model2.make_future_dataframe(periods=300)\nforecast_fb = model2.predict(future)","a95bbe83":"train_fb['ds'] = pd.to_datetime(train_fb['ds'], format='%Y-%m-%d')\ntest_fb['ds'] = pd.to_datetime(test_fb['ds'], format='%Y-%m-%d')","e87b2ecf":"forecast_fb_plot = forecast_fb[10757:]\n\nplt.figure(figsize=(15,5))\nplt.plot(train_fb.ds, train_fb['y'], label = 'Train')\nplt.plot(test_fb.ds, test_fb['y'], label='Test')\nplt.plot(forecast_fb_plot.ds, forecast_fb_plot['yhat'], label='Prophet Forecast')\nplt.legend(loc='best')\n\nplt.show()","8b5dabe6":"print('RSME:{}'.format(round(rmse(test_fb.y, forecast_fb_plot['yhat'][:30]).mean(),4)))","cf86b1b3":"from statsmodels.tsa.holtwinters import SimpleExpSmoothing\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing","22ba8537":"train_hw = df[:10757]\ntest_hw = df[10757:]\n\nfuture2 = ExponentialSmoothing(train_hw, trend='mul').fit()\nforecast_hw = future2.forecast(30)\n\ndate_index = pd.date_range('2020-02-03', periods=30, freq='D')\nforecast_hw.index = date_index\n\nplt.figure(figsize=(16,8))\nplt.plot(train_hw[10000:], label='Train')\nplt.plot(test_hw, label='Test')\nplt.plot(forecast_hw, label='Holt_Winter Forecast')\nplt.legend(loc='best')\n\nplt.show()","0b3b166c":"print('RSME:{}'.format(round(rmse(test_hw, forecast_hw).mean(),4)))","c17e6138":"Summary = {'Method':['ARIMA(1,1,1) (manual)','ARIMA(2,1,2) (auto)','Prophet','Holt-Winters'],\n              'RSME':[46.1067,round(rmse(test, auto_series).mean(),4),round(rmse(test_fb.y, forecast_fb_plot['yhat'][:30]).mean(),4),round(rmse(test_hw, forecast_hw).mean(),4)]}\n\npd.DataFrame(Summary)","40873683":"# Auto Arima Forecast","27af1069":"There is another method to determine the order of differencing: ndiffs\n\n3 tests can be choosen: \u2018kpss\u2019(default), \u2018adf\u2019, \u2018pp\u2019\n\nhttp:\/\/alkaline-ml.com\/pmdarima\/1.5.1\/modules\/generated\/pmdarima.arima.ndiffs.html\n\nAll tests show the same result: 1","0dce3c72":"# Order of differencing (d)","baf0a1eb":"Although residual errors not mainly around zero, but the variance is uniform.\n\nNote: dynamic=False --> in-sample lagged values are used for prediction","c51d9cca":"PACF lag 1 is significant (above the significance line) and lag 2 is insignificant, so p=1 should be chosen.","b4762a88":"# Shifting and lags","c0104857":"# Out-of-Time Cross validation","326b4a33":"# Time series forecast by ARIMA, Prophet, Holt-Winters\n\nData source: https:\/\/www.kaggle.com\/arashnic\/learn-time-series-forecasting-from-gold-price\n\nDescription: gold price from 1970-01-01 to 2020-03-13","c55b3183":"https:\/\/i.stack.imgur.com\/DjEKh.png","1c856df8":"# Stationarity\n- constant mean\n- constant variance\n- autocovariance thta does not depend on time\n\ni.e. mean, variance, and autocorrelation do not change with time\n\n2 popular test:\n1. Rolling statistics\n2. ADCF test","f55f7334":"Both 1st & 2nd order differencing are able to make the seies stationary with p<0.05.\n\nHowever, it should be careful not to over-difference the series as it may affect the model parameters. \n\n- Method of determining the right order of differencing\n    1. Autocorrelation function plot (ACF plot)\n        - Terminalogy\n            - autocorrelation: correlation of observations and previous values in the same series\n            - lag: observations with previous time steps\n        - ACF plot reaches to zero fairly quick --> ideal order of differencing\n        - +ve for many lags (>=10), needs further differencing\n        - lag 1's autocorrelation too -ve --> probably over-differenced\n    - Interpretation\n        - confidence intervals (95%) are drawn as a cone\n        - correlation values outside the code are likely a correlation, instead of statistical fluke (by luck)\n    2. ndiffs() under pmdarima\n ","eefbfc46":"# 3. Holt-Winters\n- uses exponential smoothing to encode lots of values from the past by computing the combined effects of value, trend and seasonality","3c0b2f70":"# 2. Prophet\n- open source software released by Facebook\n- based on an additive model\n- non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects\n- works best with time series having strong seasonal effects and several seasons of historical data","1161e850":"# Time Series Components","0cfc2926":"- Definition of MA term (q):\n    -  error of the lagged forecast\n- Method of finding q\n    - ACF\n        - show no of MA terms required to remove autocorrelation in a stationarized series","e137e94d":"- Method of finding p\n    - Partial Autocorrelation (PACF) plot\n        - correlation between the series and its lag excluding the contributions from the intermediate lags","7dd346df":"ARIMA(3,2,1) makes no difference. Since gold price is flutuating, similar to stock, it may be difficult to forcast a 3-month trend.\nLet's predict a shorter period.","14900a33":"# ","b2045a7b":"search multiple combinations of p,d,q parameters in a stepwise approach \n--> choose the best model having the least AIC","6b9d68c3":"The P Values of the AR1 and MA1 terms have improved and are highly significant (< 0.05).","992f558e":"ARIMA(2,1,2) is suggested as it has smallest AIC (Akaike information criterio) value.\n\nAIC) is an estimator of prediction error and balance the trade-off between the goodness of fit and simplicity of the model.","a71f0cd3":"ARIMA(1,1,1) model predict the gold price will drop in next 3 months and the actual price almost lie within the 95% confidence band. However, forecasts are consistently below the actual value. \n\nSo, other parameters should be tried.","4b87c09f":"Lags in 2nd differencing fall into negative zone very quickly, indicating the series may have been over differenced.\nThus, 1st differencing should be chosen.","befc852b":"# Order of  AR term (p)","81f5423c":"- 3 systematic components: level, trend, seasonality\n- 1 non-systematic component: noise\n\n    - Level: average value\n    - Trend: increasing\/ decreasing value \n    - Seasonality: repeating short-term cycle\n    - Noise: random variation","ab33b3e5":"rmse of ARIMA(2,1,2) is 46.018, while that of ARIMA(1,1,1) is 46.1067","a16e7548":"# Order of MA term (q)","a7580d00":"# 1. ARIMA model\n- Auto Regressive Integrated Moving Average\n- forecast based on past values\n- suitable for non-seasonal time series & not a random white noise \n- characterized by 3 terms: d, p, q\n    - d: number of differencing required to make the time series stationary\n    - p: order of the AR term\n    - q: order of the MA term\n- time series with seasonal patterns --> need to add seasonal terms --> SARIMA (Seasonal ARIMA)\n\n","9e393440":"Similar with the result of AR, lag 1 is also far above the significance line than others.\n\nq=1","cf54cb45":"![image.png](attachment:image.png)","096d442d":"Prophet predict the gold price will have a great drop(green line), rather than increase.\nHowever, the actual gold price keep increasing instead. Incorrect prediction leads to big rsme.","79fc4113":"- Objective of differencing: convert the time series into stationary \n\n(ie. if the time series is already stationary, d=0)\n\n- Method of stationarity determination\n\nADF test (Augmented Dickey Fuller test) is useful in determining if the series is stationary. ADF is in default of a null hypothesis that series is non-stationary, so if the p-value <0.05 and null hypothesis can be rejected.\n\npython tools for ADF test: https:\/\/www.statsmodels.org\/stable\/generated\/statsmodels.tsa.stattools.adfuller.html\n\n- Method of converting the stationarity\n    - Differencing \n        - stabilise the mean of a time series by removing changes in the level of a time series\n        - eliminate\/reduce the trend and seasonality\n    - Transformations\n        logarithms stabilise the variance of a time series"}}