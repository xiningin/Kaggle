{"cell_type":{"e44908ab":"code","f5391f3d":"code","9d5f79a4":"code","cf657492":"code","9b124f29":"code","695d7eee":"code","b8ff0d6a":"code","5aa49f9d":"code","6da326ec":"code","5af270de":"markdown","74383041":"markdown","053a3087":"markdown","9ef75d9f":"markdown","e378fb19":"markdown","35026c1d":"markdown","735dd836":"markdown","a2dc0000":"markdown","c84617f1":"markdown","1151c581":"markdown","1c6e5127":"markdown","9f207036":"markdown","85a55cc0":"markdown","0cbc67eb":"markdown","4f0554ed":"markdown","3e691db8":"markdown","c34dc79a":"markdown","1f9b0ac4":"markdown","d94780c5":"markdown","8e59e96c":"markdown","0ad74471":"markdown","c3063d45":"markdown","d8a037e6":"markdown"},"source":{"e44908ab":"import librosa\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.gridspec import GridSpec\nfrom sklearn.model_selection import train_test_split\nimport os\nfrom os.path import join\n\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Dropout, Input, BatchNormalization, LeakyReLU \nfrom tensorflow.keras.losses import CategoricalCrossentropy\nfrom tensorflow.keras.optimizers import Adam\n\n\n# ========================================================\n# Hyper parameters\n# ========================================================\n\n# Hyper Parameters - Preprocessing (for mfcc coefficients)\nhop_length = 512\nn_fft = 2048\nn_mfcc = 64\n\n# Hyper Parameters - for FC neural network model\nlearning_rate=1e-3\nbatch_size = 32\nn_epoch = 200\ndrop_rate = 0.3\ndrop_seed = 1\nval_split = 0.2\noptimizer = Adam(learning_rate=learning_rate)\nfunc_loss = CategoricalCrossentropy(from_logits=True)\n\n# Paths\ndata_path = r'..\/input\/eating-sound-collection\/clips_rd'\n\nextracted_features_path = \"_\".join([r'hop_length', str(hop_length),\n                                    'n_fft', str(n_fft),\n                                    'n_mfcc', str(n_mfcc)])","f5391f3d":"# ========================================================\n# Define the dataset\n# l = List\n# df = Dataframe\n# ========================================================\n\nclasses = os.listdir(data_path)\n\n# Define files list\nl_files = [[join(data_path, cls, file), cls]\n           for cls in classes\n           for file in os.listdir(join(data_path, cls))]\n\n# Add Columns names\ndf_files = pd.DataFrame(l_files, columns=('path', 'label'))\n\n# Calculate duration\nfor i, row in tqdm(df_files.iterrows()):\n    df_files.at[i, 'duration'] = librosa.get_duration(filename=row['path'])","9d5f79a4":"def dataset_overview(df):\n    \"\"\"Dataset Total Instances\"\"\"\n    class_dist = df.groupby(['label'])['duration'].count()\n\n    plt.figure(1, figsize=[16, 8])\n    gs = GridSpec(1, 14, wspace=0.5, hspace=0.25)\n\n    plt.subplot(gs[:, :7])\n    plt.barh(class_dist.index, class_dist, color='b')\n    plt.yticks(rotation=0, fontsize=12)\n    plt.title('Number of Samples Per Class', pad=10, loc='left')\n\n    plt.subplot(gs[:, 8:10])\n    df_files.boxplot(rot=0)\n    plt.yticks(rotation=0, fontsize=12)\n    plt.suptitle('')\n    plt.show()\n\n\ndataset_overview(df_files)\n","cf657492":"# ========================================================\n# f = features\n# l = List\n# ========================================================\n\nextracted_features_path = \"_\".join(['hop_length', str(hop_length),\n                                    'n_fft', str(n_fft),\n                                    'n_mfcc', str(n_mfcc)])\n\ndef extract_features(dataset):\n    l_features_scale = []\n    l_labels = np.array(pd.get_dummies(dataset['label']))\n\n    '''Create mfcc matrix'''\n\n    for index, file in tqdm(dataset.iterrows()):\n\n        # Load audio files\n        signal, sample_rate = librosa.load(file['path'])\n\n        # Extract MFCC Coefficients\n        f_mfcc = librosa.feature.mfcc(signal,\n                                      sr=sample_rate,\n                                      n_mfcc=n_mfcc,\n                                      hop_length=hop_length)\n\n        # Scale MFCC\n        f_mfcc_scaled = np.mean(f_mfcc.T, axis=0)\n        l_features_scale.append(f_mfcc_scaled)\n            \n    np.save('.\/features_mfcc_scaled_' + extracted_features_path, l_features_scale)\n    np.save('.\/labels_mfcc_scaled_' + extracted_features_path, l_labels)\n\n    return np.array(l_features_scale), np.array(l_labels)\n\n\nfeatures_1D, labels = extract_features(df_files)\n","9b124f29":"scaled_features = np.load( '..\/input\/eating-sound-collection-extracted-mfcc-features\/features_mfcc_scaled_' + extracted_features_path + '.npy')\n\nlabels = np.load( '..\/input\/eating-sound-collection-extracted-mfcc-features\/labels_mfcc_scaled_' + extracted_features_path + '.npy')\n\nx_train, x_test, y_train, y_test = train_test_split(scaled_features, labels, test_size=0.2, random_state=10)","695d7eee":"def build_fc_audio_model():\n    i = Input(shape=(n_mfcc,), name=\"mfcc\")\n    x = Dense(512, name=\"FC_1\")(i)\n    x = BatchNormalization()(x)\n    x = LeakyReLU()(x)\n    x = Dropout(rate=drop_rate, seed=drop_seed)(x)\n\n    x = Dense(256, name=\"FC_2\")(x)\n    x = BatchNormalization()(x)\n    x = LeakyReLU()(x)\n    x = Dropout(rate=drop_rate, seed=drop_seed)(x)\n\n    x = Dense(len(classes), name=\"FC_3\")(x)\n\n    model = Model(inputs=i, outputs=x, name='Audio-Class-Model')\n    model.summary()\n\n    return model\n\n\n'''building the model:'''\nmodel_1D = build_fc_audio_model()\n\nmodel_1D.compile(optimizer=optimizer,\n                 loss=func_loss,\n                 metrics='accuracy')\n\n'''fitting the model:'''\nhistory = model_1D.fit(x_train,\n                       y_train,\n                       batch_size=batch_size,\n                       epochs=n_epoch,\n                       validation_split=val_split)\n","b8ff0d6a":"# summarize history for accuracy\nplt.figure(1, figsize=[16, 6])\nplt.subplot(121)\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\n\n# summarize history for loss\nplt.subplot(122)\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper right')\nplt.show()","5aa49f9d":"model_1D.evaluate(x_test, y_test, batch_size=batch_size)","6da326ec":"model_1D.save(extracted_features_path +  '_FC_Model')","5af270de":"The pre-processing is a long time process (around 40 minutes).\n\nTo save time, I uploaded the mfcc NumPy files as a dataset - you can load them directly in section 04","74383041":"The best results were with the MFCC 64 coefficients-  the results are below.\n\n**You can create your mfcc matrix using the function in section 03.**","053a3087":"# #06 - Analysing Loss & Accuracy","9ef75d9f":"To analyze the dataset, we'll create a data frame that include:\n* Files paths, \n* Class of each file\n* Duration (in seconds) of each audio signal","e378fb19":"# #02 - Dataset Overview","35026c1d":"# #07 - Testing","735dd836":"# #01 - Loading the Dataset","a2dc0000":"**Background:** in simple words (and not academic) [MFCC](https:\/\/en.wikipedia.org\/wiki\/Mel-frequency_cepstrum) is a kind of \"smart\" spectrogram, analyzing audio signals regards to the human ears biology & capabilities.   \n\nI calculated the **MEAN** of the MFCC coefficients to be the **features space** to accomplish the architecture.\n\n[Librosa](https:\/\/librosa.org\/doc\/latest\/index.htmlhttps:\/\/librosa.org\/doc\/latest\/index.html) is a package music and audio analysis, and I used it to calculate the mfcc. \n\n**Advantages:**\n* The system won't be affected by time duration.\n* The network can be a simple, fully connected network\n* Low-time training.\n* Minimum parameters\\wehigths.\n\n* Minimum parameters\\wehigths.","c84617f1":"* The system will handle **any signal length** as an input.\n* The system **won't** need any padding in the pre-processing.","1151c581":"Loading the pre-saved MFCC matrix and labels vector, and split them to train\/test datasets","1c6e5127":"# Introduction","9f207036":"* The classes aren't balanced.\n* Most of the signals are 5-6 seconds long\n* There are few signals longer than 10 seconds","85a55cc0":"#  Dataset Insights:","0cbc67eb":"# My Architecture Considerations:\n","4f0554ed":"# #00 - Hyper Parameters","3e691db8":"The standard way to classify audio is based on **Spectogram & CNN Models**. \n\n[Spectogram](https:\/\/en.wikipedia.org\/wiki\/Spectrogram) is a **Pre-processing step**, where every audio signal convert from a time domain to a frequency\/time domain using [FFT Methods](https:\/\/en.wikipedia.org\/wiki\/Fast_Fourier_transform).\n\n* In practice, the time\/frequency domain is a two-dimension matrix: the y-axis is the frequency, and the x-axis is the time, and it's a kind of \"picture.\"\n* Because it's a picture, it can be trained & classified using **CNN Models**.\n\n\n**Note:** The x-axis length is changing via the time duration of the sample - the spectrogram picture WIDTH has no constraints and is needed to resize.\n\n\n**Summary:** The typical way to classify audio is by transforming it into a picture (spectrogram) in the pre-processing step and training it using a CNN model.","c34dc79a":"# #05 - Build & Train a Simple FC Model:","1f9b0ac4":"# Common Audio Classification Process:","d94780c5":"# Features Engineering:","8e59e96c":"# #03 - Pre-Processing: Extracting Scaled MFCC Features","0ad74471":"# #04 - Train\/Test Split ","c3063d45":"This notebook will describe a SIMPLE  **audio classification** problem using a **Fully-Connected** neural network.\n\n**Food identification** technology potentially benefits both food and media industries and improves human-computer interaction.\n\nThis food classification dataset is based on 246 YouTube videos of 20 food types.","d8a037e6":"# #08 - Save the Model"}}