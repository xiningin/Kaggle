{"cell_type":{"fc0e2f79":"code","424586f6":"code","3abb209d":"code","9536ab07":"code","1ff566ac":"code","55b0e9dc":"code","3ba28714":"code","43527186":"code","9ff3912d":"code","e25b8a4b":"code","2499e3e1":"code","eda5b5a4":"code","f7ab0e3c":"code","bfa0de30":"code","b2b62474":"code","ef0982f8":"code","33e7d167":"code","49629849":"code","5cc67f82":"code","55a60d28":"code","f4516d37":"code","31c26213":"markdown","506e97b4":"markdown","1a152f61":"markdown","ce712cd0":"markdown","28506903":"markdown","672312ad":"markdown","76d19346":"markdown","9264e93c":"markdown","c817f7ba":"markdown","f82832d3":"markdown","71618ec2":"markdown","e74af681":"markdown","271a2d78":"markdown","23b76973":"markdown","4ec1b6bb":"markdown","530666d1":"markdown","96f8dff4":"markdown"},"source":{"fc0e2f79":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL']= '3'\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom skimage.transform import resize\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\nimport tensorflow as tf\nfrom tensorflow.keras import *\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.callbacks import *\nfrom tensorflow.keras.optimizers import *\n\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory\nfrom tensorflow.keras.layers.experimental.preprocessing import *","424586f6":"batch_size = 32\nimage_size = (224, 224)\nclasses = 43\ndirectory = '..\/input\/directions\/Direction'\n\ntrain_dataset = image_dataset_from_directory(directory,\n                                             shuffle = True,\n                                             batch_size = batch_size,\n                                             image_size = image_size,\n                                             validation_split = 0.2,\n                                             subset = 'training',\n                                             seed = 42)\nvalidation_dataset = image_dataset_from_directory(directory,\n                                             shuffle = True,\n                                             batch_size = batch_size,\n                                             image_size = image_size,\n                                             validation_split = 0.2,\n                                             subset = 'validation',\n                                             seed = 42)","3abb209d":"class_names = train_dataset.class_names\n\nplt.figure(figsize=(10, 10))\nfor images, labels in train_dataset.take(1):\n    for i in range(9):\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        plt.title(class_names[labels[i]])\n        plt.axis(\"off\")","9536ab07":"AUTOTUNE = tf.data.experimental.AUTOTUNE\ntrain_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\nvalidation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)","1ff566ac":"\ndef data_augmenter():\n    '''\n    Create a simple function to augment training images\n    Returns:\n        tf.keras.Sequential\n    '''\n    \n    data_augmentation = tf.keras.Sequential()\n    data_augmentation.add(RandomContrast(0.4, seed=None))\n    \n    return data_augmentation","55b0e9dc":"data_augmentation = data_augmenter()\n\nfor image, _ in train_dataset.take(1):\n    plt.figure(figsize=(10, 10))\n    first_image = image[0]\n    for i in range(9):\n        ax = plt.subplot(3, 3, i + 1)\n        augmented_image = data_augmentation(tf.expand_dims(first_image, 0))\n        plt.imshow(augmented_image[0] \/ 255)\n        plt.axis('off')","3ba28714":"preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input","43527186":"def make_model(image_size, num_classes, data_augmentation = data_augmenter()):\n    \n    input_shape = image_size + (3,)\n    \n    base_model = tf.keras.applications.MobileNetV2(input_shape=input_shape,\n                                                   include_top=False, # Do not include the dense prediction layer\n                                                   weights=\"imagenet\") # Load imageNet parameters\n    \n    # Freeze the base model by making it non trainable\n    base_model.trainable = False \n\n    # create the input layer (Same as the imageNetv2 input size)\n    inputs = tf.keras.Input(shape=input_shape) \n    \n    # apply data augmentation to the inputs\n    x = data_augmentation(inputs)\n   \n    # data preprocessing using the same weights the model was trained on\n    x = preprocess_input(x) \n    \n    # set training to False to avoid keeping track of statistics in the batch norm layer\n    x = base_model(x, training=False) \n    \n    # Add the new Binary classification layers\n    # use global avg pooling to summarize the info in each channel\n    x = GlobalAveragePooling2D()(x) \n    #include dropout with probability of 0.2 to avoid overfitting\n    x = Dropout(0.2)(x)\n        \n    # create a prediction layer\n    if num_classes == 2:\n        activation = \"sigmoid\"\n        units = 1\n    else:\n        activation = \"softmax\"\n        units = num_classes\n\n    x = layers.Dropout(0.5)(x)\n    \n    prediction_layer = Dense(units, activation=activation)\n    \n    outputs = prediction_layer(x)\n    \n    model = Model(inputs, outputs)\n    \n    return model","9ff3912d":"# Define a model using the make_model function\nmodel = make_model(image_size, num_classes = 4)\n\n# Preview the Model Architecture\ntf.keras.utils.plot_model(model, show_shapes=True)","e25b8a4b":"# Preview the MobileNetV2 architecture\ntf.keras.utils.plot_model(model.layers[4], show_shapes=True)","2499e3e1":"base_learning_rate = 0.001\noptimizer = Adam(learning_rate = base_learning_rate)\ninitial_epochs = 20\nbatch_size = 64\nloss = 'sparse_categorical_crossentropy'\nmetrics = ['accuracy']\ncallback = EarlyStopping(monitor='val_accuracy', patience=20, restore_best_weights=True)\nreduce_lr = ReduceLROnPlateau(monitor='val_accuracy',factor=1e-1, patience=8, verbose=1, min_lr = 2e-6)\n\nmodel.compile(optimizer = optimizer,\n              loss = loss,\n              metrics = metrics)","eda5b5a4":"history_freeze = model.fit(train_dataset, \n                          batch_size = batch_size, \n                          epochs = initial_epochs, \n                          validation_data = validation_dataset, \n                          callbacks = [callback, reduce_lr], \n                          shuffle = True)","f7ab0e3c":"acc = [0.] + history_freeze.history['accuracy']\nval_acc = [0.] + history_freeze.history['val_accuracy']\n\nloss = history_freeze.history['loss']\nval_loss = history_freeze.history['val_loss']\n\nplt.figure(figsize=(8, 8))\nplt.subplot(2, 1, 1)\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.ylabel('Accuracy')\nplt.ylim([min(plt.ylim()),1])\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.ylabel('Cross Entropy')\nplt.ylim([0,1.0])\nplt.title('Training and Validation Loss')\nplt.xlabel('epoch')\nplt.show()","bfa0de30":"base_model = model.layers[4] # MobileNetV2 Architecture\nbase_model.trainable = True\n# Let's take a look to see how many layers are in the base model\nprint(\"Number of layers in the base model: \", len(base_model.layers))\n\n# Fine-tune from this layer onwards\nfine_tune_at = 120\n\n# Freeze all the layers before the `fine_tune_at` layer\nfor layer in base_model.layers[:fine_tune_at]:\n    layer.trainable = True\n\n\noptimizer = Adam(learning_rate = 0.1 * base_learning_rate)\nbatch_size = 64\nloss = 'sparse_categorical_crossentropy'\nmetrics = ['accuracy']\ncallback = EarlyStopping(monitor='val_accuracy', patience=20, restore_best_weights=True)\nreduce_lr = ReduceLROnPlateau(monitor='val_accuracy',factor=1e-1, patience=8, verbose=1, min_lr = 2e-6)\n\nmodel.compile(optimizer = optimizer,\n              loss = loss,\n              metrics = metrics)","b2b62474":"fine_tune_epochs = 10\ntotal_epochs =  initial_epochs + fine_tune_epochs\n\nhistory_fine = model.fit(train_dataset,\n                         epochs = total_epochs,\n                         batch_size = batch_size,\n                         initial_epoch = history_freeze.epoch[-1],\n                         callbacks = [callback, reduce_lr], \n                         shuffle = True,\n                         validation_data = validation_dataset)","ef0982f8":"acc = [0.] + history_fine.history['accuracy']\nval_acc = [0.] + history_fine.history['val_accuracy']\n\nloss = history_fine.history['loss']\nval_loss = history_fine.history['val_loss']\n\nplt.figure(figsize=(8, 8))\nplt.subplot(2, 1, 1)\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.ylabel('Accuracy')\nplt.ylim([min(plt.ylim()),1])\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.ylabel('Cross Entropy')\nplt.ylim([0,1.0])\nplt.title('Training and Validation Loss')\nplt.xlabel('epoch')\nplt.show()","33e7d167":"train_accuracy = round(history_fine.history['accuracy'][-1] * 100, 2)\nvalidation_accuracy = round(history_fine.history['val_accuracy'][-1] * 100, 2)\n\nprint(\"Train Accuracy: {}%\".format(train_accuracy))\nprint(\"Validation Accuracy: {}%\".format(validation_accuracy))","49629849":"model.save('model.h5')","5cc67f82":"# Import an image into TF array\nimage = tf.keras.preprocessing.image.load_img('..\/input\/directions\/Direction\/Up\/109.jpg', target_size=image_size)\nimage_array = tf.keras.preprocessing.image.img_to_array(image)\nimage_array = tf.expand_dims(image_array, 0)  # Create batch axis","55a60d28":"# Predict the class of the image\n\ndictionary = {0 : 'Down', 1 : 'Left', 2 : 'Right', 3 : 'Up'} \npredictions = model.predict(image_array)\nscore = predictions[0]\npredicted_class = np.argmax(score)\nprediction = dictionary[predicted_class]","f4516d37":"# Dispolay the selected Image and its class \nplt.imshow(image)\nplt.title(prediction)\nplt.axis(\"off\")\nplt.show()\n","31c26213":"# **Traffic Sign Direction Classification: Transfer Learning with MobileNetV2**","506e97b4":"### 2.3. Import the mobilenet preprocessing function <a class=\"anchor\" id=\"2-3\"><\/a>","1a152f61":"### 4. Predict Selected Images using the trained Model <a class=\"anchor\" id=\"4\"><\/a>","ce712cd0":"### 2.2. Define a function to augment the dataset <a class=\"anchor\" id=\"2-2\"><\/a>\n\nThe RandomContrast method will be used to augmnet the training images. I did not attempt using the RandomVertical and RandomHorizontal methods because I do not have much insight about the images and their orientations. Some of the images could be slant and using a small rotation angle could shift their orientation to another class.","28506903":"## **1. Import Required Packages** <a class=\"anchor\" id=\"1\"><\/a>","672312ad":"MobileNetV2 was designed to provide fast and computationally efficient performance. It's been pre-trained on ImageNet, a dataset containing over 14 million images and 1000 classes. \n\n**Objectives of the project:**\n\n- Create a dataset from a directory\n- Preprocess and augment data using the Sequential API\n- Adapt a pretrained model to new data and train a classifier using the Functional API and MobileNet\n- Fine-tune a classifier's final layers to improve accuracy ","76d19346":"### 3.2. Fine-tuning the Model by re-running the optimizer in the last layers to improve accuracy <a class=\"anchor\" id=\"3-2\"><\/a>\n\nWe could try fine-tuning the model by re-running the optimizer in the last layers to improve accuracy. In transfer learning, the way we achieve this is by unfreezing the layers at the end of the network, and then re-training our model on the final layers with a very low learning rate. Adapting the learning rate to go over these layers in smaller steps can yield more fine details - and higher accuracy.\n\n**The intuition for what's happening**: when the network is in its earlier stages, it trains on low-level features, like edges. In the later layers, more complex, high-level features like arrows begin to emerge. For transfer learning, the low-level features can be kept the same, as they have common features for most images. When adding new data, we generally want the high-level features to adapt to it, which is rather like letting the network learn to detect features more related to our data. \n\nTo achieve this, just unfreeze the final layers and re-run the optimizer with a smaller learning rate, while keeping all the other layers frozen. After then, we will run it again for another few epochs, and see if our accuracy improved!","9264e93c":"## 2. Preprocess and Augment Training Images <a class=\"anchor\" id=\"2\"><\/a>","c817f7ba":"### 2.1. Extend the data generator pipelines to improve performance <a class=\"anchor\" id=\"2-1\"><\/a>","f82832d3":"#### 3.3. Save Model <a class=\"anchor\" id=\"3-3\"><\/a>","71618ec2":"### 3.1. Transfer Learning with MobileNetV2: Layer freezing with Functional API <a class=\"anchor\" id=\"3-1\"><\/a>","e74af681":"We'll use the MobileNetV2's pre-trained parameters on ImageNet to modify the classifier task  so that it's able to recognize traffic directions. We can achieve this in three steps: \n\n1. Delete the top layer (the classification layer)\n    * Set `include_top` in `base_model` as False\n2. Add a new classifier layer\n    * Train only one layer by freezing the rest of the network\n3. Freeze the base model and train the newly-created classifier layer\n    * Set `base model.trainable=False` to avoid changing the weights and train *only* the new layer\n    * Set training in `base_model` to False to avoid keeping track of statistics in the batch norm layer","271a2d78":"## **3. Model Architecture & Training (Using MobileNetV2 for Transfer Learning)** <a class=\"anchor\" id=\"3\"><\/a>","23b76973":"### 1.3. Preview some of the images from the training set <a class=\"anchor\" id=\"1-3\"><\/a>","4ec1b6bb":"## Table of Content\n\n- [1 - Import Required Packages](#1)\n    - [1.1. Import Required Libraries](#1-1)\n    - [1.2. Create pipelines to load and the training and validation images](#1-2)\n    - [1.3. Preview some of the images from the Training Set](#1-3)\n    \n- [2 - Preprocess and Augment Training Data](#2)\n    - [2.1. - Extend the data generator pipelines to improve performance](#2-1)\n    - [2.1. - Augment Dataset](#2-2)\n    \n- [3 - Model Architecture & Training (Using MobileNetV2 for Transfer Learning)](#3)\n    - [3.1. - Layer Freezing with the Functional API](#3-1)\n    - [3.2. - Fine-tuning the Model](#3-2)    \n    - [3.3. - Save the Model](#3-3)\n    \n- [4 - Predict selected images using the trained Model)](#4)\n","530666d1":"### 1.2. Create pipelines to load and the training and validation images <a class=\"anchor\" id=\"1-2\"><\/a>","96f8dff4":"### 1.1. Import Required Libraries <a class=\"anchor\" id=\"1-1\"><\/a>"}}