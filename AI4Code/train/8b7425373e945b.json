{"cell_type":{"4c2b7895":"code","e5fada3e":"code","e332d84a":"code","0a578569":"code","af0ee889":"code","be8559a6":"code","11f27925":"code","5a061cac":"code","c426c1aa":"code","c1e9a80e":"code","a60a24a5":"code","0d0d1489":"code","985519c7":"code","25c291c6":"code","fd006a12":"code","b968d842":"code","768f5b92":"code","c4adbecf":"code","e561838f":"code","8d7d05d1":"code","4131eb17":"code","f6137ee8":"code","b97a2cbc":"code","ea828615":"code","f259756b":"code","38430d30":"code","f0a0309b":"code","b41c65be":"code","f62a17cd":"code","9ea8c406":"code","4ef8e2ae":"code","2dda201d":"code","7225dea5":"code","ca8b32ab":"code","6d5d9fff":"code","5f37811a":"code","388b4566":"code","baa59ce1":"code","0707ae64":"code","aad57a99":"code","43d4366f":"code","4059a837":"code","82293d6d":"code","3d9205ea":"code","ee3768bc":"code","2e98672b":"code","5149e547":"code","389495d6":"code","df82d803":"code","8d5ff859":"code","bc6b1bf3":"code","a6368e5d":"code","18008a54":"code","8e40c537":"code","2fb18a2c":"code","2c9a0628":"code","55b3fc32":"code","990dfd1e":"code","6809e753":"code","30be2dcf":"markdown","b2936dcd":"markdown","2afaf5c7":"markdown","162fa7f8":"markdown","78c310b5":"markdown","b3bfaa46":"markdown","c6b2b673":"markdown","78c52527":"markdown","88acde70":"markdown","062d90f5":"markdown","2999aa34":"markdown","bff54485":"markdown","b69d8b88":"markdown","c24a8ccd":"markdown","0c8ab413":"markdown","7cb4d955":"markdown","9108cede":"markdown","ebe86301":"markdown","44b15f36":"markdown","8b24984d":"markdown","c170b811":"markdown","6ac67ef6":"markdown","590f535b":"markdown","b009deda":"markdown","de546b2a":"markdown","f2256d32":"markdown","bef99cd4":"markdown","86599ae3":"markdown","7fbf7546":"markdown","e3f590bd":"markdown","08d297b6":"markdown","97b5cff6":"markdown","9e40c7c8":"markdown","7540cf6b":"markdown","9afe15f3":"markdown","0424e6a6":"markdown","22eab09d":"markdown","6d042adb":"markdown","1e8cf830":"markdown","4c3161aa":"markdown","589d7642":"markdown","ebdb29c8":"markdown","278b69d8":"markdown","7b056361":"markdown","ef34b2c5":"markdown","de8cd992":"markdown","43c847c8":"markdown","3077d475":"markdown","25456625":"markdown","06d4f8c4":"markdown"},"source":{"4c2b7895":"import pandas as pd\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport glob\nimport os\nimport gc\nfrom tqdm.notebook import tqdm\nfrom bs4 import BeautifulSoup\nfrom nltk.stem import WordNetLemmatizer\nimport warnings\nwarnings.filterwarnings('ignore')\ntrain =False","e5fada3e":"    comments = pd.read_csv('..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv')\n    comments","e332d84a":"if train:\n    validation = pd.read_csv('..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv')\n    validation","0a578569":"sample = pd.read_csv('..\/input\/jigsaw-toxic-severity-rating\/sample_submission.csv')\nsample","af0ee889":"if train:\n    train1 =pd.read_csv('..\/input\/old-competitaion\/train.csv\/train.csv')\n    train1","be8559a6":"if train:\n    check1 =train1[['comment_text','toxic','severe_toxic']]\n    check1\n    del train1","11f27925":"if train:\n    train2 = pd.read_csv('..\/input\/old-competitaion\/compe2\/compe2\/train.csv')\n    train2","5a061cac":"if train:\n    anno = pd.read_csv('..\/input\/old-competitaion\/compe2\/compe2\/toxicity_individual_annotations.csv')\n    anno","c426c1aa":"if train:\n    anno.toxic.unique()","c1e9a80e":"if train:\n    anno_ =anno[anno.duplicated(subset='id')!=True]\n    anno_","a60a24a5":"if train:\n    train2 = train2.merge(anno_,on='id')\n    train2","0d0d1489":"if train:\n    check2 =train2[['comment_text','severe_toxicity','toxicity_annotator_count','toxic']]\n    check2","985519c7":"if train:\n    anno2 = pd.read_csv('..\/input\/old-competitaion\/compe2\/compe2\/identity_individual_annotations.csv')\n    anno2\n    del train2\n    del anno\n    del anno2\n    del anno_","25c291c6":"if train:\n    all_ = pd.read_csv('..\/input\/old-competitaion\/compe2\/compe2\/all_data.csv')\n    all_","fd006a12":"if train:\n    all_.columns","b968d842":"if train:\n    all_.toxicity.hist()","768f5b92":"if train:\n    check3 =all_[['comment_text','toxicity','severe_toxicity','toxicity_annotator_count']]\n    check3","c4adbecf":"if train:\n    del all_","e561838f":"if train:\n    jig1 = pd.read_csv('..\/input\/old-competitaion\/compe3\/compe3\/jigsaw-toxic-comment-train-processed-seqlen128.csv')\n    jig1","8d7d05d1":"if train:\n    jig1.toxic.unique()","4131eb17":"if train:\n    check4 =jig1[['comment_text','severe_toxic','toxic']]\n    del jig1\n    check4","f6137ee8":"if train:\n    jig2 = pd.read_csv('..\/input\/old-competitaion\/compe3\/compe3\/jigsaw-toxic-comment-train.csv')\n    jig2","b97a2cbc":"if train:\n    jig2.toxic.unique()","ea828615":"if train:\n    check4 =jig2[['comment_text','toxic','severe_toxic']]\n    del jig2\n    check4","f259756b":"if train:\n    jig3 = pd.read_csv('..\/input\/old-competitaion\/compe3\/compe3\/jigsaw-unintended-bias-train.csv')\n    jig3","38430d30":"if train:\n    jig3.toxic.hist()","f0a0309b":"if train:\n    jig3.severe_toxicity.hist()","b41c65be":"if train:\n    jig3.columns","f62a17cd":"if train:\n    check5 =jig3[['comment_text','toxic', 'severe_toxicity','toxicity_annotator_count']]\n    del jig3\n    check5","9ea8c406":"if train:\n    jig4 = pd.read_csv('..\/input\/old-competitaion\/compe3\/compe3\/validation.csv')\n    jig4","4ef8e2ae":"if train:\n    jig4.toxic.unique()","2dda201d":"if train:\n    check6 =jig4[['comment_text','toxic']]\n    del jig4\n    check6","7225dea5":"if train:\n    data1 = pd.concat([check1,check4]).reset_index(drop=True)\n    del check1\n    del check4\n    data1","ca8b32ab":"if train:\n    check3 =check3.rename(columns={'toxicity':'toxic'})","6d5d9fff":"if train:\n    data2 = pd.concat([check3,check5]).reset_index(drop=True)\n    del check3\n    del check5\n    data2","5f37811a":"if train:\n    data3 =check2\n    data3","388b4566":"if train:\n    data4= check6\n    data4","baa59ce1":"if train:\n    data1 = data1[['comment_text','toxic']]\n    data1","0707ae64":"if train:\n    data2 = data2[['comment_text','toxic']]\n    data2","aad57a99":"if train:\n    data3 =data3[['comment_text','toxic']]\n    data3","43d4366f":"if train:\n    data4","4059a837":"if train:\n    data1= pd.concat([data1,data2,data3]).reset_index(drop=True)\n    del data2\n    del data3\n    del data4","82293d6d":"def clean(data, col):\n    \n    data[col] = data[col].str.replace('https?:\/\/\\S+|www\\.\\S+', ' social medium ')      \n        \n    data[col] = data[col].str.lower()\n    data[col] = data[col].str.replace(\"4\", \"a\") \n    data[col] = data[col].str.replace(\"2\", \"l\")\n    data[col] = data[col].str.replace(\"5\", \"s\") \n    data[col] = data[col].str.replace(\"1\", \"i\") \n    data[col] = data[col].str.replace(\"!\", \"i\") \n    data[col] = data[col].str.replace(\"|\", \"i\") \n    data[col] = data[col].str.replace(\"0\", \"o\") \n    data[col] = data[col].str.replace(\"l3\", \"b\") \n    data[col] = data[col].str.replace(\"7\", \"t\") \n    data[col] = data[col].str.replace(\"7\", \"+\") \n    data[col] = data[col].str.replace(\"8\", \"ate\") \n    data[col] = data[col].str.replace(\"3\", \"e\") \n    data[col] = data[col].str.replace(\"9\", \"g\")\n    data[col] = data[col].str.replace(\"6\", \"g\")\n    data[col] = data[col].str.replace(\"@\", \"a\")\n    data[col] = data[col].str.replace(\"$\", \"s\")\n    data[col] = data[col].str.replace(\"#ofc\", \" of fuckin course \")\n    data[col] = data[col].str.replace(\"fggt\", \" faggot \")\n    data[col] = data[col].str.replace(\"your\", \" your \")\n    data[col] = data[col].str.replace(\"self\", \" self \")\n    data[col] = data[col].str.replace(\"cuntbag\", \" cunt bag \")\n    data[col] = data[col].str.replace(\"fartchina\", \" fart china \")    \n    data[col] = data[col].str.replace(\"youi\", \" you i \")\n    data[col] = data[col].str.replace(\"cunti\", \" cunt i \")\n    data[col] = data[col].str.replace(\"sucki\", \" suck i \")\n    data[col] = data[col].str.replace(\"pagedelete\", \" page delete \")\n    data[col] = data[col].str.replace(\"cuntsi\", \" cuntsi \")\n    data[col] = data[col].str.replace(\"i'm\", \" i am \")\n    data[col] = data[col].str.replace(\"offuck\", \" of fuck \")\n    data[col] = data[col].str.replace(\"centraliststupid\", \" central ist stupid \")\n    data[col] = data[col].str.replace(\"hitleri\", \" hitler i \")\n    data[col] = data[col].str.replace(\"i've\", \" i have \")\n    data[col] = data[col].str.replace(\"i'll\", \" sick \")\n    data[col] = data[col].str.replace(\"fuck\", \" fuck \")\n    data[col] = data[col].str.replace(\"f u c k\", \" fuck \")\n    data[col] = data[col].str.replace(\"shit\", \" shit \")\n    data[col] = data[col].str.replace(\"bunksteve\", \" bunk steve \")\n    data[col] = data[col].str.replace('wikipedia', ' social medium ')\n    data[col] = data[col].str.replace(\"faggot\", \" faggot \")\n    data[col] = data[col].str.replace(\"delanoy\", \" delanoy \")\n    data[col] = data[col].str.replace(\"jewish\", \" jewish \")\n    data[col] = data[col].str.replace(\"sexsex\", \" sex \")\n    data[col] = data[col].str.replace(\"allii\", \" all ii \")\n    data[col] = data[col].str.replace(\"i'd\", \" i had \")\n    data[col] = data[col].str.replace(\"'s\", \" is \")\n    data[col] = data[col].str.replace(\"youbollocks\", \" you bollocks \")\n    data[col] = data[col].str.replace(\"dick\", \" dick \")\n    data[col] = data[col].str.replace(\"cuntsi\", \" cuntsi \")\n    data[col] = data[col].str.replace(\"mothjer\", \" mother \")\n    data[col] = data[col].str.replace(\"cuntfranks\", \" cunt \")\n    data[col] = data[col].str.replace(\"ullmann\", \" jewish \")\n    data[col] = data[col].str.replace(\"mr.\", \" mister \")\n    data[col] = data[col].str.replace(\"aidsaids\", \" aids \")\n    data[col] = data[col].str.replace(\"njgw\", \" nigger \")\n    data[col] = data[col].str.replace(\"wiki\", \" social medium \")\n    data[col] = data[col].str.replace(\"administrator\", \" admin \")\n    data[col] = data[col].str.replace(\"gamaliel\", \" jewish \")\n    data[col] = data[col].str.replace(\"rvv\", \" vanadalism \")\n    data[col] = data[col].str.replace(\"admins\", \" admin \")\n    data[col] = data[col].str.replace(\"pensnsnniensnsn\", \" penis \")\n    data[col] = data[col].str.replace(\"pneis\", \" penis \")\n    data[col] = data[col].str.replace(\"pennnis\", \" penis \")\n    data[col] = data[col].str.replace(\"pov.\", \" point of view \")\n    data[col] = data[col].str.replace(\"vandalising\", \" vandalism \")\n    data[col] = data[col].str.replace(\"cock\", \" dick \")\n    data[col] = data[col].str.replace(\"asshole\", \" asshole \")\n    data[col] = data[col].str.replace(\"youi\", \" you \")\n    data[col] = data[col].str.replace(\"afd\", \" all fucking day \")\n    data[col] = data[col].str.replace(\"sockpuppets\", \" sockpuppetry \")\n    data[col] = data[col].str.replace(\"iiprick\", \" iprick \")\n    data[col] = data[col].str.replace(\"penisi\", \" penis \")\n    data[col] = data[col].str.replace(\"warrior\", \" warrior \")\n    data[col] = data[col].str.replace(\"loil\", \" laughing out insanely loud \")\n    data[col] = data[col].str.replace(\"vandalise\", \" vanadalism \")\n    data[col] = data[col].str.replace(\"helli\", \" helli \")\n    data[col] = data[col].str.replace(\"lunchablesi\", \" lunchablesi \")\n    data[col] = data[col].str.replace(\"special\", \" special \")\n    data[col] = data[col].str.replace(\"ilol\", \" i lol \")\n    data[col] = data[col].str.replace(\"'\", \"\")\n    data[col] = data[col].str.replace('\"', '')\n    data[col] = data[col].str.replace(',', ' ')\n    data[col] = data[col].str.replace(r'\\b[uU]\\b', 'you')\n    data[col] = data[col].str.replace(r\"what's\", \"what is \")\n    data[col] = data[col].str.replace(r\"\\'s\", \" is \")\n    data[col] = data[col].str.replace(r\"\\'ve\", \" have \")\n    data[col] = data[col].str.replace(r\"can't\", \"cannot \")\n    data[col] = data[col].str.replace(r\"n't\", \" not \")\n    data[col] = data[col].str.replace(r\"i'm\", \"i am \")\n    data[col] = data[col].str.replace(r\"\\'re\", \" are \")\n    data[col] = data[col].str.replace(r\"\\'d\", \" would \")\n    data[col] = data[col].str.replace(r\"\\'ll\", \" will \")\n    data[col] = data[col].str.replace(r\"\\'scuse\", \" excuse \")\n    data[col] = data[col].str.replace('\\s+', ' ')  # will remove more than one whitespace character\n#     text = re.sub(r'\\b([^\\W\\d_]+)(\\s+\\1)+\\b', r'\\1', re.sub(r'\\W+', ' ', text).strip(), flags=re.I)  # remove repeating words coming immediately one after another\n    data[col] = data[col].str.replace(r'(.)\\1+', r'\\1\\1') # 2 or more characters are replaced by 2 characters\n#     text = re.sub(r'((\\b\\w+\\b.{1,2}\\w+\\b)+).+\\1', r'\\1', text, flags = re.I)\n    data[col] = data[col].str.replace(\"[:|\u2663|'|\u00a7|\u2660|*|\/|?|=|%|&|-|#|\u2022|~|^|>|<|\u25ba|_]\", '')\n    \n    \n    data[col] = data[col].str.replace(r\"what's\", \"what is \")    \n    data[col] = data[col].str.replace(r\"\\'ve\", \" have \")\n    data[col] = data[col].str.replace(r\"can't\", \"cannot \")\n    data[col] = data[col].str.replace(r\"n't\", \" not \")\n    data[col] = data[col].str.replace(r\"i'm\", \"i am \")\n    data[col] = data[col].str.replace(r\"\\'re\", \" are \")\n    data[col] = data[col].str.replace(r\"\\'d\", \" would \")\n    data[col] = data[col].str.replace(r\"\\'ll\", \" will \")\n    data[col] = data[col].str.replace(r\"\\'scuse\", \" excuse \")\n    data[col] = data[col].str.replace(r\"\\'s\", \" \")\n    # Clean some punctutations\n    data[col] = data[col].str.replace('\\n', ' \\n ')\n    data[col] = data[col].str.replace(r'([a-zA-Z]+)([\/!?.])([a-zA-Z]+)',r'\\1 \\2 \\3')\n    # Replace repeating characters more than 3 times to length of 3\n    data[col] = data[col].str.replace(r'([*!?\\'])\\1\\1{2,}',r'\\1\\1\\1')    \n    # Add space around repeating characters\n    data[col] = data[col].str.replace(r'([*!?\\']+)',r' \\1 ')    \n    # patterns with repeating characters \n    data[col] = data[col].str.replace(r'([a-zA-Z])\\1{2,}\\b',r'\\1\\1')\n    data[col] = data[col].str.replace(r'([a-zA-Z])\\1\\1{2,}\\B',r'\\1\\1\\1')\n    data[col] = data[col].str.replace(r'[ ]{2,}',' ').str.strip()   \n    data[col] = data[col].str.replace(r'[ ]{2,}',' ').str.strip()   \n    #data[col] = data[col].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n    \n    return data","3d9205ea":"    def text_cleaning(text):\n        #template = re.compile(r'https?:\/\/\\S+|www\\.\\S+')  # Removes website links\n        #text = template.sub(r'', text)\n    \n        #soup = BeautifulSoup(text, 'lxml')  # Removes HTML tags\n        #only_text = soup.get_text()\n        #text = only_text\n        \n        text = re.sub('[^A-Za-z0-9]+', ' ', str(text).lower()).strip()\n        text = re.sub(' +', ' ', text)\n        emoji_pattern = re.compile(\"[\"\n                                   u\"\\U0001F600-\\U0001F64F\"  # \u9854\u6587\u5b57\n                                   u\"\\U0001F300-\\U0001F5FF\"  # \u8a18\u53f7\u3068\u7d75\u6587\u5b57\n                                   u\"\\U0001F680-\\U0001F6FF\"  # \u4ea4\u901a\u53ca\u3073\u5730\u56f3\u8a18\u53f7\n                                   u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                                   \"]+\", flags=re.UNICODE)\n        text = emoji_pattern.sub(r'', text)\n        text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) \n        text = re.sub(' +', ' ', text) \n        text = text.strip().lower() \n    \n        text = ' '.join([lemmatizer.lemmatize(word) for word in text.split(' ')])\n        text = ' '.join([word for word in text.split(' ') if word not in stop])\n\n        return text","ee3768bc":"import re\nfrom nltk.corpus import stopwords\n\nlemmatizer = WordNetLemmatizer()\nstop = stopwords.words(\"english\")","2e98672b":"if train:\n    del data1\n    del check2\n    del check6\n    del validation\n    gc.collect()","5149e547":"if train:\n    test = clean(data1, 'comment_text')\n    temp = test.comment_text.apply(text_cleaning)\n    test.comment_text =temp.values\n    test.toxic = data1.toxic\n\n    test.to_csv('train_data.csv', index=False)\n","389495d6":"if train==False:\n    test = pd.read_csv('..\/input\/jigsaw\/train_data.csv')","df82d803":"test","8d5ff859":"if train==False:\n    test = test.dropna()\n    test = test[:len(test)\/\/6]","bc6b1bf3":"if train==False:\n    from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n    vec = TfidfVectorizer(min_df= 3, max_df=0.5, analyzer = 'char_wb', ngram_range = (3,5))\n    X = vec.fit_transform(test['comment_text'])\n        ","a6368e5d":"if train==False:\n    from sklearn.linear_model import Ridge, Lasso, BayesianRidge\n    model = Ridge(alpha=0.5)\n    model.fit(X, test['toxic'])","18008a54":"if train==False:\n    import re\n    pred = pd.DataFrame()\n    pred['comment_text'] = comments.text.apply(text_cleaning)\n","8e40c537":"if train==False:\n    pred = clean(pred, 'comment_text')\n    pred.comment_text = pred.comment_text.apply(text_cleaning)\n    pred","2fb18a2c":"if train==False:\n    X_test = vec.transform(pred.comment_text.values)\n    toxic = model.predict(X_test)\n","2c9a0628":"if train==False:\n    sample.score = toxic \n    sample","55b3fc32":"if train==False:\n    sample.to_csv(\"submission.csv\", index=False)","990dfd1e":"import sys\nprint(\"{}{:>25}{}{:>10}{}\".format('|','Variable Name','|','memory','|'))\nfor var_name in dir():\n    if not var_name.startswith(\"_\") and sys.getsizeof(eval(var_name)) > 10000:\n        print(\"{}{:>25}{}{:>10}{}\".format('|',var_name,'|',sys.getsizeof(eval(var_name)),'|'))","6809e753":"#model, tokenizer, \\\n#        test_loader, scaler = config(fold=1, model_name='..\/input\/roberta\/', load_model_path='..\/input\/jigsaw-roberta-base-i\/')","30be2dcf":"----------------------------------------------------------","b2936dcd":"### It is not the training data, but the data that verifies the trained model.","2afaf5c7":"\n\ntoxic is represented by 0,1.","162fa7f8":"\n\ntoxic is represented by 0,1.","78c310b5":"### You need to look at this text to determine if it is harmful.","b3bfaa46":"### \u30e1\u30e2\u30ea\u30ea\u30fc\u30af\u3092\u89e3\u6c7a\u3067\u304d\u306a\u304b\u3063\u305f\u306e\u3067\u3001train\u30d5\u30e9\u30b0\u3092\u4ed8\u3051\u307e\u3057\u305f\u3002\n\n### I couldn't resolve the memory leak, so I flagged it as train.","c6b2b673":"### First, let's take a look at the data provided","78c52527":"### comments pred","88acde70":"\n\nSevere_toxicity is expressed in the range of 0-1 but it is most often around 0.1.","062d90f5":"### Train","2999aa34":"<span style=\"color: orange; font-family: Segoe UI; font-size: 1.9em; font-weight: 300;\">sample_submission.csv<\/span>","bff54485":"<span style=\"color: orange; font-family: Segoe UI; font-size: 1.9em; font-weight: 300;\">comments_to_score.csv<\/span>","b69d8b88":"\n\nThis data is represented by 0,1 whether it is harmful to the text.","c24a8ccd":"\n\ntoxic is represented by 0,1.","0c8ab413":"\n\ntoxicity is expressed in the range of 0-1 but it is most often around 0.1.","7cb4d955":"---------------------","9108cede":"![image.png](attachment:611674da-6970-40c8-86aa-05a4394fc55f.png)","ebe86301":"\n\n### <font color='orange'><u> There is nothing we can do without learning data, so we will collect the data. <\/u><\/font>","44b15f36":"# Data Merge","8b24984d":"\n\n### check2 and check6 cannot be combined, so leave them as they are.","c170b811":"\n\nSince toxicity_annotator_count is the number of characters, it is difficult to evaluate it, but for the time being, it seems that 500 or less harmful characters are included in this data.","6ac67ef6":"![image.png](attachment:a7f30a1e-5b26-434d-98cd-cd568cc2e8f5.png)","590f535b":"\n\nI'm not sure if this can be used at this time.","b009deda":"\n\nThere is a lot of data that needs to be removed. \\ n!? \"And so on.","de546b2a":"# TF-IDF","f2256d32":"\u82f1\u8a9e\u3067\u306f\u306a\u3044\n\nIt's not English.","bef99cd4":"\n\n### check1 and check4 can be combined","86599ae3":"\n\n### Since you will run out of memory, leave only the necessary data and erase the rest","7fbf7546":"![image.png](attachment:198825b3-eb27-4b7f-ab04-7d79e895a4fe.png)","e3f590bd":"<span style=\"color: orange; font-family: Segoe UI; font-size: 1.9em; font-weight: 300;\">Import<\/span>","08d297b6":"\n\nSevere_toxicity is expressed in the range of 0-1 but it is most often around 0.1.","97b5cff6":"\n\n### The feature of this competition is a natural language task for which <font color='orange'>no learning data is provided.<\/font>\n### For the learning data, see the following three competitions.\n### Toxic Comment Classification Challenge, Jigsaw Unintended Bias in Toxicity Classification, Jigsaw Multilingual Toxic Comment Classification\n### Somehow unfriendly competition.","9e40c7c8":"\n\n### data1-data4 is ready.","7540cf6b":"<span style=\"color: orange; font-family: Segoe UI; font-size: 1.9em; font-weight: 300;\">validation_data.csv<\/span>","9afe15f3":"\n\ntoxic is expressed in the range of 0-1 but it is most often around 0.1.","0424e6a6":"------------------------------","22eab09d":"\n\ntoxic is represented by 0,1.","6d042adb":"### This is the submission file. Whether it is harmful or not will be entered in this score column.","1e8cf830":"![image.png](attachment:9a98652f-7b91-41e1-95b2-14c5bdc1abae.png)","4c3161aa":"\n\nSevere_toxicity is expressed in the range of 0-1 but it is most often around 0.1.","589d7642":"\n### I came to this point and found that there are three types of data.\n### Whether it is harmful or not It is expressed by 0,1, in the range of 0-1 and by the number of characters.\n### It is necessary to put together as learning data.","ebdb29c8":"# stopwords","278b69d8":"![image.png](attachment:633ca5fc-50eb-44e6-b211-906c36719466.png)","7b056361":"![image.png](attachment:5420af5a-fe79-4e06-be5f-739698de70e4.png)","ef34b2c5":"\n\n### check3 and check5 can be combined. The line names are different, so you need to change them.","de8cd992":"\n\nMerge train and anno.","43c847c8":"\n\n### I feel that comment_text and toxic of this learning data can be used.","3077d475":"-------------------------------------","25456625":"\u82f1\u8a9e\u3067\u306f\u306a\u3044data4\u306f\u5916\u3059\u3002\n\nRemove data4 which is not in English.","06d4f8c4":"\n\nThere is still a lot of garbage. I think there was a good way, but I forgot. I will investigate this time. First of all, I will erase it manually"}}