{"cell_type":{"5a96de0e":"code","2d5397ca":"code","bfdf699f":"code","0d85e8f9":"code","a11b283c":"code","df760589":"code","2c384cb2":"code","e6142fe1":"code","590342b6":"code","60f039b1":"code","c629517e":"code","b11c62e6":"code","4a6cb8e1":"code","a965f5c6":"code","6a39cead":"code","13c0ec0a":"code","b240b305":"code","90bc5908":"markdown"},"source":{"5a96de0e":"import pandas as pd\nimport numpy as np\n\nimport torch\nimport torchvision.datasets as data\nimport torchvision.transforms as transforms\nimport random\n\nfrom sklearn import preprocessing","2d5397ca":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nrandom.seed(777)\ntorch.manual_seed(777)\nif device == 'cuda':\n  torch.cuda.manual_seed_all(777)","bfdf699f":"# \ud559\uc2b5 \ud30c\ub77c\ubbf8\ud130 \uc124\uc815\nlearning_rate = 0.001\ntraining_epochs = 163\nbatch_size = 10\n#dropout \ucd94\uac00\ndrop_prob=0.5\nScaler = preprocessing.StandardScaler()","0d85e8f9":"train_data=pd.read_csv('train_unemployment_rate.csv',header=None)\ntest_data=pd.read_csv('test_unemployment_rate.csv',header=None)","a11b283c":"#\ub144,\uc6d4\uac12 \uc218\uc815\ntrain_data[[1]]=train_data[[1]]%10000\/100\nx_train_data=train_data.loc[1:,1:3]\ny_train_data=train_data.loc[1:,4]\nx_train_data","df760589":"x_train_data=np.array(x_train_data)\ny_train_data=np.array(y_train_data)\n\n# \uc2a4\ucf00\uc77c\ub7ec\ub97c \ud1b5\ud574 preprocessing\nx_train_data = Scaler.fit_transform(x_train_data)\n\nx_train_data=torch.FloatTensor(x_train_data)\ny_train_data=torch.FloatTensor(y_train_data)","2c384cb2":"train_dataset = torch.utils.data.TensorDataset(x_train_data, y_train_data)","e6142fe1":"data_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n                                          batch_size=batch_size,\n                                          shuffle=True,\n                                          drop_last=True)","590342b6":"#layer1->layer3\nlinear1 = torch.nn.Linear(3,3,bias=True)\nlinear2 = torch.nn.Linear(3,3,bias=True)\nlinear3 = torch.nn.Linear(3,1,bias=True)\nrelu=torch.nn.Dropout(p=drop_prob)\ndropout = torch.nn.Dropout(p=drop_prob)","60f039b1":"torch.nn.init.xavier_normal_(linear1.weight)\ntorch.nn.init.xavier_normal_(linear2.weight)\ntorch.nn.init.xavier_normal_(linear3.weight)","c629517e":"model = torch.nn.Sequential(linear1,relu,dropout,\n                            linear2,relu,dropout,\n                            linear3).to(device)","b11c62e6":"loss = torch.nn.MSELoss().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) ","4a6cb8e1":"total_batch = len(data_loader)\nfor epoch in range(training_epochs):\n    avg_cost = 0\n\n    for X, Y in data_loader:\n\n        X = X.to(device)\n        Y = Y.to(device)\n\n        # \uadf8\ub798\ub514\uc5b8\ud2b8 \ucd08\uae30\ud654\n        optimizer.zero_grad()\n        # Forward \uacc4\uc0b0\n        hypothesis = model(X)\n        # Error \uacc4\uc0b0\n        cost = loss(hypothesis, Y)\n        # Backparopagation\n        cost.backward()\n        # \uac00\uc911\uce58 \uac31\uc2e0\n        optimizer.step()\n\n        # \ud3c9\uade0 Error \uacc4\uc0b0\n        avg_cost += cost \/ total_batch\n\n    print('Epoch:', '%03d' % (epoch + 1), 'rate =', '{:.1f}'.format(avg_cost))\n\nprint('Learning finished')","a965f5c6":"with torch.no_grad():\n  test_data[[1]]=test_data[[1]]%10000\/100\n  x_test_data=test_data.loc[1:,1:3]\n  x_test_data=np.array(x_test_data)\n  x_test_data = Scaler.transform(x_test_data)\n  x_test_data=torch.from_numpy(x_test_data).float().to(device)\n\n  prediction = model(x_test_data)","6a39cead":"correct_prediction = prediction.cpu().numpy().reshape(-1,1)\ncorrect_prediction","13c0ec0a":"submit=pd.read_csv('submission.csv')","b240b305":"for i in range(len(correct_prediction)):\n  submit['Expected'][i]=float(correct_prediction[i])\n  submit['Expected']=submit['Expected'].astype(float)\nsubmit","90bc5908":"\ubca0\uc774\uc2a4\ub77c\uc778 \ucf54\ub4dc\uc640\uc758 \ucc28\uc774\n\ndropout\ucd94\uac00\n\n\uc5f0,\uc6d4\uac12 \uc218\uc815\n\nxdata 2\uac1c \uc0ac\uc6a9->3\uac1c \uc0ac\uc6a9\n\n\nlayer1->layer3\uc73c\ub85c \uc218\uc815\n"}}