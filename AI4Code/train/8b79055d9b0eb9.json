{"cell_type":{"2b767055":"code","b111aca0":"code","aba9ff68":"code","5844e1a8":"code","56919792":"code","a77917e7":"code","2ff5db30":"code","853d2cbe":"code","807e4eb7":"code","b6962e27":"code","6c4afe02":"code","631419d7":"code","e8210e13":"code","5b94a614":"code","a8ac56d0":"code","e0f898cf":"code","0e5f9f81":"code","12642e4a":"code","03a5d1f1":"code","4978b8fb":"markdown","9cd46d7e":"markdown","194d715b":"markdown","e51db60d":"markdown","b995bf8c":"markdown","ea06a3cf":"markdown","a222d610":"markdown","467b2c11":"markdown","703c46ad":"markdown"},"source":{"2b767055":"from sklearn.metrics import precision_score, roc_auc_score, recall_score, confusion_matrix, roc_curve, precision_recall_curve, accuracy_score\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split, RandomizedSearchCV, learning_curve, GridSearchCV, StratifiedKFold\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.metrics import (accuracy_score, log_loss, classification_report)\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor  \n\n\nfrom sklearn import model_selection, preprocessing, ensemble\nfrom sklearn.model_selection import train_test_split\nfrom matplotlib.ticker import StrMethodFormatter\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.kernel_ridge import KernelRidge\n\n\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.pipeline import make_pipeline\nimport statsmodels.formula.api as smf\nfrom sklearn.metrics import log_loss\nimport matplotlib.pyplot as plt\nfrom sklearn import neighbors\nfrom datetime import datetime\nfrom sklearn import ensemble\nfrom scipy.stats import norm\nfrom sklearn import metrics\nfrom scipy import stats\nimport lightgbm as lgb\nimport seaborn as sns\nimport xgboost as xgb\n\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport pandas_profiling\nfrom pathlib import Path\n\n%matplotlib inline\n\n# Import statements required for Plotly \nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport plotly.figure_factory as ff\n\n\nfrom sklearn.linear_model import LogisticRegression\n\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import confusion_matrix\nimport xgboost\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nfrom sklearn import tree\nfrom IPython.display import Image as PImage\nfrom subprocess import check_call\nfrom PIL import Image, ImageDraw, ImageFont\nimport re\n\n\nimport h2o\nimport pandas as pd\nimport numpy as np \nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV \nfrom h2o.automl import H2OAutoML\nh2o.init()","b111aca0":"def model_performance_plot(model) : \n    #conf matrix\n    conf_matrix = confusion_matrix(valid_y, y_pred)\n    trace1 = go.Heatmap(z = conf_matrix  ,x = [\"0 (pred)\",\"1 (pred)\"],\n                        y = [\"0 (true)\",\"1 (true)\"],xgap = 2, ygap = 2, \n                        colorscale = 'Viridis', showscale  = False)\n\n    #show metrics\n    tp = conf_matrix[1,1]\n    fn = conf_matrix[1,0]\n    fp = conf_matrix[0,1]\n    tn = conf_matrix[0,0]\n    Accuracy  =  ((tp+tn)\/(tp+tn+fp+fn))\n    Precision =  (tp\/(tp+fp))\n    Recall    =  (tp\/(tp+fn))\n    F1_score  =  (2*(((tp\/(tp+fp))*(tp\/(tp+fn)))\/((tp\/(tp+fp))+(tp\/(tp+fn)))))\n\n    show_metrics = pd.DataFrame(data=[[Accuracy , Precision, Recall, F1_score]])\n    show_metrics = show_metrics.T\n\n    colors = ['gold', 'lightgreen', 'lightcoral', 'lightskyblue']\n    trace2 = go.Bar(x = (show_metrics[0].values), \n                   y = ['Accuracy', 'Precision', 'Recall', 'F1_score'], text = np.round_(show_metrics[0].values,4),\n                    textposition = 'auto',\n                   orientation = 'h', opacity = 0.8,marker=dict(\n            color=colors,\n            line=dict(color='#000000',width=1.5)))\n    \n    #plot roc curve\n    model_roc_auc = round(roc_auc_score(valid_y, y_score) , 3)\n    fpr, tpr, t = roc_curve(valid_y, y_score)\n    trace3 = go.Scatter(x = fpr,y = tpr,\n                        name = \"Roc : \",\n                        line = dict(color = ('rgb(22, 96, 167)'),width = 2), fill='tozeroy')\n    trace4 = go.Scatter(x = [0,1],y = [0,1],\n                        line = dict(color = ('black'),width = 1.5,\n                        dash = 'dot'))\n    \n    # Precision-recall curve\n    precision, recall, thresholds = precision_recall_curve(valid_y, y_score)\n    trace5 = go.Scatter(x = recall, y = precision,\n                        name = \"Precision\" + str(precision),\n                        line = dict(color = ('lightcoral'),width = 2), fill='tozeroy')\n    \n    #subplots\n    fig = tls.make_subplots(rows=2, cols=2, print_grid=False, \n                        subplot_titles=('Confusion Matrix',\n                                        'Metrics',\n                                        'ROC curve'+\" \"+ '('+ str(model_roc_auc)+')',\n                                        'Precision - Recall curve'))\n    \n    fig.append_trace(trace1,1,1)\n    fig.append_trace(trace2,1,2)\n    fig.append_trace(trace3,2,1)\n    fig.append_trace(trace4,2,1)\n    fig.append_trace(trace5,2,2)\n    \n    fig['layout'].update(showlegend = False, title = '<b>Model performance<\/b><br>'+str(model),\n                        autosize = False, height = 900,width = 830,\n                        plot_bgcolor = 'rgba(240,240,240, 0.95)',\n                        paper_bgcolor = 'rgba(240,240,240, 0.95)',\n                        margin = dict(b = 195))\n    fig[\"layout\"][\"xaxis2\"].update((dict(range=[0, 1])))\n    fig[\"layout\"][\"xaxis3\"].update(dict(title = \"false positive rate\"))\n    fig[\"layout\"][\"yaxis3\"].update(dict(title = \"true positive rate\"))\n    fig[\"layout\"][\"xaxis4\"].update(dict(title = \"recall\"), range = [0,1.05])\n    fig[\"layout\"][\"yaxis4\"].update(dict(title = \"precision\"), range = [0,1.05])\n    fig.layout.titlefont.size = 14\n    \n    py.iplot(fig)","aba9ff68":"def features_imp(model, cf) : \n\n    coefficients  = pd.DataFrame(model.feature_importances_)\n    column_data     = pd.DataFrame(list(data))\n    coef_sumry    = (pd.merge(coefficients,column_data,left_index= True,\n                              right_index= True, how = \"left\"))\n    coef_sumry.columns = [\"coefficients\",\"features\"]\n    coef_sumry    = coef_sumry.sort_values(by = \"coefficients\",ascending = False)\n    coef_sumry = coef_sumry[coef_sumry[\"coefficients\"] !=0]\n    trace = go.Bar(x = coef_sumry[\"features\"],y = coef_sumry[\"coefficients\"],\n                    name = \"coefficients\",\n                    marker = dict(color = coef_sumry[\"coefficients\"],\n                                  colorscale = \"Viridis\",\n                                  line = dict(width = .6,color = \"black\")))\n    layout = dict(title =  'Feature Importances xgb_cfl')\n                    \n    fig = dict(data = [trace], layout=layout)\n    py.iplot(fig)","5844e1a8":"#cumulative gain curve\ndef cum_gains_curve(model):\n    pos = pd.get_dummies(y_test).as_matrix()\n    pos = pos[:,1] \n    npos = np.sum(pos)\n    index = np.argsort(y_score) \n    index = index[::-1] \n    sort_pos = pos[index]\n    #cumulative sum\n    cpos = np.cumsum(sort_pos) \n    #recall\n    recall = cpos\/npos \n    #size obs test\n    n = y_test.shape[0] \n    size = np.arange(start=1,stop=369,step=1) \n    #proportion\n    size = size \/ n \n    #plots\n    model = 'xgb_cfl'\n    trace1 = go.Scatter(x = size,y = recall,\n                        name = \"Lift curve\",\n                        line = dict(color = ('rgb(22, 96, 167)'),width = 2))\n    trace2 = go.Scatter(x = size,y = size,\n                        name = \"Baseline\",\n                        showlegend=False,\n                        line = dict(color = ('black'),width = 1.5,\n                        dash = 'dot'))\n\n    layout = dict(title = 'Cumulative gains curve'+' '+str(model),\n                  yaxis = dict(title = 'Percentage positive targeted',zeroline = False),\n                  xaxis = dict(title = 'Percentage contacted', zeroline = False)\n                 )\n\n    fig  = go.Figure(data = [trace1,trace2], layout = layout)\n    py.iplot(fig)","56919792":"# Cross val metric\ndef cross_val_metrics(model) :\n    scores = ['accuracy', 'precision', 'recall']\n    for sc in scores:\n        scores = cross_val_score(model, X, y, cv = 5, scoring = sc)\n        print('[%s] : %0.5f (+\/- %0.5f)'%(sc, scores.mean(), scores.std()))","a77917e7":"df = pd.read_csv('..\/input\/ibm-hr-analytics-attrition-dataset\/WA_Fn-UseC_-HR-Employee-Attrition.csv', encoding = \"ISO-8859-1\",\n                 low_memory = False)\n\n# Let's look at a sample of the dataset to understand the format and type of features\ndf.sample(5)","2ff5db30":"pandas_profiling.ProfileReport(df)","853d2cbe":"print('We have %d features' %len(df.columns))\n\nprint('The name of the features are as below :')\n\nprint(df.columns)","807e4eb7":"df = df.drop(['EmployeeCount','Over18','StandardHours'], axis = 1)","b6962e27":"cols = ['DailyRate','DistanceFromHome','Age','HourlyRate','MonthlyRate','MonthlyIncome','TotalWorkingYears','YearsAtCompany']\n\nn_bins = 7\nfor i in cols:\n    lower, higher = df[i].min(), df[i].max()\n    edges = range(lower, higher, round((higher - lower)\/n_bins))\n    lbs = ['(%d, %d]'%(edges[i], edges[i+1]) for i in range(len(edges)-1)]\n    df[i] = pd.cut( x = df[i], bins=edges, labels=lbs, include_lowest=True)","6c4afe02":"df.head(5)","631419d7":"total_records= len(df)\ncolumns = ['Age', 'BusinessTravel', 'DailyRate', 'Department',\n       'DistanceFromHome', 'Education', 'EducationField',\n        'EnvironmentSatisfaction', 'Gender', 'HourlyRate',\n       'JobInvolvement', 'JobLevel', 'JobRole', 'JobSatisfaction',\n       'MaritalStatus', 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked',\n       'Over18', 'OverTime', 'PercentSalaryHike', 'PerformanceRating',\n       'RelationshipSatisfaction', 'StandardHours', 'StockOptionLevel',\n       'TotalWorkingYears', 'TrainingTimesLastYear', 'WorkLifeBalance',\n       'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion',\n       'YearsWithCurrManager']\nplt.figure(figsize=(20,20))\nj=0\nfor i in columns:\n    j +=1\n    plt.subplot(19,2,j)\n    #sns.countplot(hrdata[i])\n    ax1 = sns.countplot(data=df,x= i,hue=\"Attrition\")\n    if(j==8 or j== 7):\n        plt.xticks( rotation=90)\n    for p in ax1.patches:\n        height = p.get_height()\n        ax1.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}'.format(height,0),\n                ha=\"center\",rotation=0) \n\n# Custom the subplot layout\nplt.subplots_adjust(bottom=-0.9, top=2)\nplt.show()\n","e8210e13":"#Let us look at the correlation between the numerical variables\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64','uint8']\nnumerical = df.select_dtypes(include = numerics)\nnumerical.sample(5)","5b94a614":"# Correlation between variables helps us understand which of these variables could make a change in other variables and we could remove one of them if their correlation coefficient is more than a threshold we could specify\n# we are using spearman instead of Pearson is because our data is related to human behavior and it is not linear\n\n%matplotlib notebook\nimport plotly.figure_factory as ff\n\ndfcorr = numerical.corr(method = 'spearman')\nnum_cols = dfcorr.columns.tolist()\nnum_array = np.array(dfcorr)\n\n# Let us plot the data\n\ntrace = go.Heatmap(z= num_array,\n                   x = num_cols,\n                   y = num_cols,\n                   colorscale = 'Electric',\n                   colorbar = dict(),)\n\nlayout = go.Layout(dict(title = 'Correlation Matrix for variables',\n                        autosize = False,\n                        height  = 1400,\n                        width   = 1600,\n                        margin  = dict(r = 0 ,l = 210,\n                                       t = 25,b = 210,\n                                     ),\n                        yaxis   = dict(tickfont = dict(size = 9)),\n                        xaxis   = dict(tickfont = dict(size = 9)),\n                       )\n                  )\n\nfig = go.Figure(data=[trace], layout = layout)\npy.iplot(fig)\n","a8ac56d0":"# Remove highly correlated variables\n\nthreshold = 0.85\n\ndatacorrs = numerical.corr(method ='spearman').abs()\n\nupper = datacorrs.where(np.triu(np.ones(datacorrs.shape),k=1).astype(np.bool))\n\nto_drop = [column for column in upper.columns if any (upper[column] >= threshold)]\n\ndf = df.drop(columns = to_drop)\nprint(to_drop)","e0f898cf":"# Let's see how the distribution of data when it comes to active and inactive employees\ndata = [go.Bar(\n            x=df[\"Attrition\"].value_counts().index.values,\n            y= df[\"Attrition\"].value_counts().values\n    )]\n\npy.iplot(data, filename='basic-bar')","0e5f9f81":"# Let us start with our Target variable which is attrition and it is binary, So let us convert attrition = yes to 1 and attrition = No to 0\n\ntarget_map = {'Yes':1, 'No':0}\ndf['Attrition'] = df['Attrition'].apply(lambda x : target_map[x])\n\n# Store the target variable Attrition separate and remove it from the dataset\n\ntarget = df['Attrition']\ndf = df.drop('Attrition', 1)\ndf = df.drop('EmployeeNumber', 1)\n\n# Now begins the one hot encoding for the remaining variables\ncategory = []\nfor cols, val in df.iteritems():\n    if val.dtype !='int64':\n        category.append(cols)\n        \n\nnumeric = df.columns.difference(category)\n\ndf_categor = df[category]\ndf_cat = pd.get_dummies(df_categor)\ndf_num = df[numeric]\ndf = pd.concat([df_num, df_cat, target], axis = 1)\ndf.sample(5)\n","12642e4a":"data = df.copy()\n# Shuffle the dataset\ndf.sample(frac=0.1)\n# Split them into train and test\ntraindata = df.sample(frac=0.8, random_state = 200)\ntestdata = df.drop(traindata.index)\n# Converting the datasets into H2o frame\ntrain_data = h2o.H2OFrame(traindata)\ntest_data = h2o.H2OFrame(testdata)\nx = train_data.columns\ny = \"Attrition\"\nx.remove(y)\n# For binary classification, response should be factor\ntrain_data[y] = train_data[y].asfactor()\ntest_data[y] = test_data[y].asfactor()\n# Let the game begin\naml = H2OAutoML(max_models=5, seed=100, nfolds = 5, max_runtime_secs = 200, max_runtime_secs_per_model = 240)\naml.train(x=x, y=y , training_frame = train_data)","03a5d1f1":"lb = aml.leaderboard\nlb.head(rows=lb.nrows)","4978b8fb":"Now that all the packages are loaded. Let's define some functions like Model performance plot, Feature importance, ROC\/AUC curve and cross validation.\n\n*****These modules & some coding were taken from another Kernel, Thank you to the OP!!*****","9cd46d7e":"To get a good understanding of the data we must look at a descriptive report of the data.\n\nWe can use the pandas profiling for this purpose which gives a detailed analyis  of the data","194d715b":"**VOILA!!! We have an accuracy of 83%**","e51db60d":"We are going to be doing some basic feature engineering","b995bf8c":"****Into to Data****\n\nFirst let us try to understand what is employee attrition.\nAttrition is a term used most often in the case when an employee resigns or retires, sometimes they are also called employee churn which again means losing someone from a company. In this case we are trying to predict employees who will resign voluntarily and not involuntarily(also called layoff,this depends on the budget for the team and many other factors)\n\n****Why is predicting attrition important****\n\nAttrition affects a lot of factors in an organization and some of them are as follows:\n\n1. A lot of cost is involved when an employee is hired, trained and during knowledge transfer in the new team. So when the employee leaves in 6 months or 1 year the entire cost spent on this is lost.\n2. To replace the employee who has quit there again needs to be money spent on job posting, recruiting and knowledge transfer.\n3. Meanwhile the team who has lost the team member will suffer due to shortage of resources and the team's productivity will go down.\n4. The task handled by the employee who quit will have to be managed by existing team member on top of his\/her regular task, making his\/her life a pain.\n5. Often a job role takes atleast 3 to 4 months to be filled and sometimes more if the candidate is on work visa and needs his visa transferred. Also not to mention the extra visa costs will be added to the department.\n\nSo according to a research the total attrition cost which involves hiring, job posting, training and knowledge transfer will all amount to the annual salary of the person who quit and in some cases 2x annual salary if the skills sets are rare.","ea06a3cf":"Below is a plot to understand the distribution of the data","a222d610":"We are going to use pandas to load our csv source file into a dataframe","467b2c11":"This is my first kernel in Kaggle and I am going to use this kernel as a start for getting to gain knowledge in Kaggle","703c46ad":"**Now lets get started with our Analysis**"}}