{"cell_type":{"e6ff7fab":"code","ea5685c8":"code","0a01762b":"code","f66f6393":"code","29bf25bb":"code","f7e8cae5":"code","c17620ea":"code","f5f0e907":"code","917f7b9b":"code","2df86363":"code","21cd8735":"code","3816e751":"code","62870dec":"code","c4b2fdb8":"code","18080906":"code","7b5916b7":"code","b4fb6ba7":"code","61a3d5e9":"code","229ad49e":"code","1f55e279":"code","616bff27":"markdown","a7da250f":"markdown","8e32b607":"markdown","35b3adba":"markdown","a69191ca":"markdown","24ebb2a6":"markdown","1fe4eaf9":"markdown","40e3cb2a":"markdown","6e966ab5":"markdown","24fe1d0c":"markdown","d0b9ec1f":"markdown","cecba226":"markdown","730c02b4":"markdown","9cebd9ce":"markdown","cf366380":"markdown"},"source":{"e6ff7fab":"import numpy as np\nimport pandas as pd\nimport os\nprint(os.listdir(\"..\/input\"))","ea5685c8":"# load data from csv files\ntrain_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')\nprint(train_df.shape, test_df.shape)","0a01762b":"train_df['label'].value_counts(sort=False)","f66f6393":"# create arrays from dataframes\ntrain_X = train_df.drop(['label'], axis=1).values\ntrain_Y = train_df['label'].values\ntest_X = test_df.values\nprint(train_X.shape, train_Y.shape, test_X.shape)","29bf25bb":"import matplotlib.pyplot as plt\n\n# look at some of the digits from train_X\nplt.figure(figsize=(15,6))\nfor i in range(40):  \n    plt.subplot(4, 10, i+1)\n    plt.imshow(train_X[i].reshape((28,28)),cmap=plt.cm.binary)\n    plt.title(\"label=%d\" % train_Y[i],y=0.9)\n    plt.axis('off')\nplt.subplots_adjust(wspace=0.3, hspace=-0.1)\nplt.show()","f7e8cae5":"# prepare the data for CNN\n\n# reshape flattened data into 3D tensor\nn_x = 28\ntrain_X_digit = train_X.reshape((-1, n_x, n_x, 1))  \ntest_X_digit = test_X.reshape((-1, n_x, n_x, 1))    # similarly for test set\nprint(train_X_digit.shape, test_X_digit.shape)\n\n# standardize the values in the datasets by dividing by 255\ntrain_X_digit = train_X_digit \/ 255.\ntest_X_digit = test_X_digit \/ 255.\n\n# one-hot encode the labels in train_Y\nfrom keras.utils.np_utils import to_categorical\nonehot_labels = to_categorical(train_Y)\nprint(onehot_labels.shape)\nprint(train_Y[181], onehot_labels[181])\nplt.figure(figsize=(1,1))\nplt.imshow(train_X[181].reshape((28,28)),cmap=plt.cm.binary)\nplt.show()","c17620ea":"# use Keras data generator to augment the training set\n\nfrom keras_preprocessing.image import ImageDataGenerator\ndata_augment = ImageDataGenerator(rotation_range=10, zoom_range=0.1, \n                                 width_shift_range=0.1, height_shift_range=0.1)","f5f0e907":"# build the CNN from keras\nfrom keras import models\nfrom keras import layers\n\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, kernel_size=5, padding='same', activation='relu', input_shape=(28, 28, 1)))\nmodel.add(layers.MaxPooling2D(pool_size=(2,2)))\nmodel.add(layers.Dropout(rate=0.4))\nmodel.add(layers.Conv2D(64, kernel_size=5, activation='relu'))\nmodel.add(layers.MaxPooling2D(pool_size=(2,2)))\nmodel.add(layers.Dropout(rate=0.4))\nmodel.add(layers.Conv2D(128, kernel_size=3, activation='relu'))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(256, activation='relu'))\nmodel.add(layers.Dropout(rate=0.4))\nmodel.add(layers.Dense(10, activation='softmax'))\n\nmodel.summary()","917f7b9b":"# compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', \n              metrics=['accuracy'])","2df86363":"# set a learning rate annealer\nfrom keras.callbacks import ReduceLROnPlateau\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy',patience=3,factor=0.5,min_lr=0.00001,\n                                           verbose=1)","21cd8735":"# set up a dev set (5000 samples) to check the performance of the CNN\nX_dev = train_X_digit[:5000]\nrem_X_train = train_X_digit[5000:]\nprint(X_dev.shape, rem_X_train.shape)\n\nY_dev = onehot_labels[:5000]\nrem_Y_train = onehot_labels[5000:]\nprint(Y_dev.shape, rem_Y_train.shape)","3816e751":"# Train and validate the model\nepochs = 30\nbatch_size = 128\nhistory = model.fit_generator(data_augment.flow(rem_X_train, rem_Y_train, batch_size=batch_size), \n                              epochs=epochs, steps_per_epoch=rem_X_train.shape[0]\/\/batch_size, \n                              validation_data=(X_dev, Y_dev), callbacks=[learning_rate_reduction])","62870dec":"# plot and visualise the training and validation losses\nloss = history.history['loss']\ndev_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\n\nfrom matplotlib import pyplot as plt\nplt.plot(epochs, loss, 'bo', label='training loss')\nplt.plot(epochs, dev_loss, 'b', label='validation loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","c4b2fdb8":"# do error analysis on the predictions for X_dev\npred_dev = model.predict(X_dev)\npred_dev_labels = np.argmax(pred_dev, axis=1)","18080906":"# look at those that were classified wrongly in X_dev\nresult = pd.DataFrame(train_Y[:5000], columns=['Y_dev'])\nresult['Y_pred'] = pred_dev_labels\nresult['correct'] = result['Y_dev'] - result['Y_pred']\nerrors = result[result['correct'] != 0]\nerror_list = errors.index\nprint('Number of errors is ', len(errors))\nprint('The indices are ', error_list)","7b5916b7":"# plot the image of the wrong in predictions for X_dev\nplt.figure(figsize=(15,10))\nfor i in range(len(error_list)):\n    plt.subplot(6, 10, i+1)\n    plt.imshow(X_dev[error_list[i]].reshape((28,28)),cmap=plt.cm.binary)\n    plt.title(\"true={}\\npredict={}\".format(train_Y[error_list[i]], \n                                           pred_dev_labels[error_list[i]]), y=0.9)\n    plt.axis('off')\nplt.subplots_adjust(wspace=0.3, hspace=-0.1)\nplt.show()","b4fb6ba7":"# predict on test set\npredictions = model.predict(test_X_digit)\nprint(predictions.shape)","61a3d5e9":"# set the predicted labels to be the one with the highest probability\npredicted_labels = np.argmax(predictions, axis=1)","229ad49e":"# look at some of the predictions for test_X\nplt.figure(figsize=(15,6))\nfor i in range(40):  \n    plt.subplot(4, 10, i+1)\n    plt.imshow(test_X[i].reshape((28,28)),cmap=plt.cm.binary)\n    plt.title(\"predict=%d\" % predicted_labels[i],y=0.9)\n    plt.axis('off')\nplt.subplots_adjust(wspace=0.3, hspace=-0.1)\nplt.show()","1f55e279":"# create submission file\nresult = pd.read_csv('..\/input\/sample_submission.csv')\nresult['Label'] = predicted_labels\n# generate submission file in csv format\nresult.to_csv('submission.csv', index=False)","616bff27":"# Create CNN Model","a7da250f":"# Prepare the data for use in CNN","8e32b607":"#### Here are some examples of the predictions made:","35b3adba":"* This kernel uses Convolutional Neural Network (CNN) to classify hand-drawn digits, from 1 through 9. The datasets are essentially from the famous MNIST data, which comprises 70,000 labelled images of digits.\n* In this 'Digit Recognizer' competition, there are 42,000 labelled images in the train data and 28,000 unlabelled images in the test data.\n* Before working on this prediction problem, I found it useful to read [Chris Deotte's kernels.][1] The 'Digit Recognizer' data are basically taken entirely from the 70,000 MNIST data. Hence, it is advisable not to bring in other sources of MNIST data, to avoid having your CNN model trained on the test data. The 28,000 test data should be kept unseen as much as possible.\n[1]: https:\/\/www.kaggle.com\/cdeotte\/mnist-perfect-100-using-knn","a69191ca":"* Thanks to [Shay Guterman][1] for providing inputs on improving the model.\n[1]: https:\/\/www.kaggle.com\/shaygu\/fast-cnn-for-beginners-0-9945","24ebb2a6":"#### Here are some examples of the hand-drawn digits from the train dataset:","1fe4eaf9":"Started on 22 May 2019","40e3cb2a":"# Error Analysis","6e966ab5":"# Make Predictions","24fe1d0c":"Looking at those that were predicted wrongly, I see that there are quite several difficult and ambiguous ones. If the validation set is also representative of those in the test set (28,000), I would think it improbable that an accuracy of 100% can be attained.","d0b9ec1f":"* set up a validation set to check the performance of the CNN.","cecba226":"* Looks good....","730c02b4":"# Introduction","9cebd9ce":"#### Run the model using the train and validation datasets, and capture histories to visualise the performance.","cf366380":"# Examine the data"}}