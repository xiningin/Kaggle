{"cell_type":{"73bea98a":"code","f9f468a0":"code","9fe14f53":"code","07e05782":"code","0406d525":"code","0622ca20":"code","a6ff2e37":"code","34519014":"code","4ebb4edf":"code","64107b96":"code","4c2442d2":"code","226feb6b":"code","7b82dba0":"code","5421b311":"code","c158c928":"markdown","eede5934":"markdown","c0638cb1":"markdown","55aeef2c":"markdown","3dcd6ca8":"markdown","f9684744":"markdown","a3d072e9":"markdown"},"source":{"73bea98a":"!pip --quiet install ..\/input\/treelite\/treelite-0.93-py3-none-manylinux2010_x86_64.whl","f9f468a0":"!pip --quiet install ..\/input\/treelite\/treelite_runtime-0.93-py3-none-manylinux2010_x86_64.whl","9fe14f53":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n# treelite\nimport treelite\nimport treelite_runtime \n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nimport pathlib\nfrom tqdm import tqdm\nfrom random import choices\n\nimport operator\nimport xgboost as xgb\n        \n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","07e05782":"# tf setup\nprint(\"Tensorflow version \" + tf.__version__)\nAUTO = tf.data.experimental.AUTOTUNE\n\nMIXED_PRECISION = False\nXLA_ACCELERATE = True\n\nif MIXED_PRECISION:\n    from tensorflow.keras.mixed_precision import experimental as mixed_precision\n    if tpu: policy = tf.keras.mixed_precision.experimental.Policy('mixed_bfloat16')\n    else: policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n    mixed_precision.set_policy(policy)\n    print('Mixed precision enabled')\n\nif XLA_ACCELERATE:\n    tf.config.optimizer.set_jit(True)\n    print('Accelerated Linear Algebra enabled')","0406d525":"SEED = 2021\nSTART_DATE = 86\nFOLDS = 5\nNN_NAME = 'mlp' # 1dcnn, resnet, mlp","0622ca20":"%%time\n\n# train = pd.read_csv('..\/input\/jane-street-market-prediction\/train.csv')\ntrain = pd.read_feather('..\/input\/janestreet-save-as-feather\/train.feather')\ntrain = train.query(f'date >= {START_DATE}').reset_index(drop = True) \ntrain = train.astype({c: np.float32 for c in train.select_dtypes(include='float64').columns}) #limit memory use\ntrain.fillna(train.mean(),inplace=True)\ntrain = train.query('weight > 0').reset_index(drop = True)\n#train['action'] = (train['resp'] > 0).astype('int')\ntrain['action'] =  (  (train['resp_1'] > 0 ) & (train['resp_2'] > 0 ) & (train['resp_3'] > 0 ) & (train['resp_4'] > 0 ) &  (train['resp'] > 0 )   ).astype('int')\nfeatures = [c for c in train.columns if 'feature' in c]\n\nresp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']\n\nX = train[features].values\ny = np.stack([(train[c] > 0).astype('int') for c in resp_cols]).T #Multitarget\n\nf_mean = np.mean(train[features[1:]].values,axis=0)","a6ff2e37":"y.shape[-1]","34519014":"predictor = treelite_runtime.Predictor('..\/input\/janestreet-faster-inference-by-xgb-with-treelite\/mymodel.so', verbose=True)","4ebb4edf":"def create_autoencoder(input_dim,output_dim,noise=0.05):\n    i = tf.keras.layers.Input(input_dim)\n    encoded = tf.keras.layers.BatchNormalization()(i)\n    encoded = tf.keras.layers.GaussianNoise(noise)(encoded)\n    encoded = tf.keras.layers.Dense(64,activation='relu')(encoded)\n    decoded = tf.keras.layers.Dropout(0.2)(encoded)\n    decoded = tf.keras.layers.Dense(input_dim,name='decoded')(decoded)\n    x = tf.keras.layers.Dense(32,activation='relu')(decoded)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(0.2)(x)\n    x = tf.keras.layers.Dense(output_dim,activation='sigmoid',name='label_output')(x)\n    \n    encoder = tf.keras.models.Model(inputs=i,outputs=encoded)\n    autoencoder = tf.keras.models.Model(inputs=i,outputs=[decoded,x])\n    \n    autoencoder.compile(optimizer=tf.keras.optimizers.Adam(0.001), \n                        loss={'decoded':'mse','label_output':'binary_crossentropy'})\n    return autoencoder, encoder","64107b96":"autoencoder, encoder = create_autoencoder(X.shape[-1],y.shape[-1],noise=0.1)\nencoder.load_weights('..\/input\/janestreet-1dcnn-for-feature-extraction-train\/encoder.hdf5') \nencoder.trainable = False","4c2442d2":"def create_resnet(n_features, n_labels, encoder, label_smoothing = 0.0005):    \n    input_1 = tf.keras.layers.Input(shape = (n_features,))\n    input_2 = encoder(input_1)\n\n    head_1 = tf.keras.Sequential([\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(512, activation=\"elu\"), \n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.4),\n        tf.keras.layers.Dense(256, activation = \"elu\")\n        ],name='Head1') \n\n    input_3 = head_1(input_1)\n    input_3_concat = tf.keras.layers.Concatenate()([input_2, input_3])\n\n    head_2 = tf.keras.Sequential([\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(512, \"relu\"),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.3),\n        tf.keras.layers.Dense(512, \"elu\"),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.3),\n        tf.keras.layers.Dense(256, \"relu\"),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.3),\n        tf.keras.layers.Dense(256, \"elu\")\n        ],name='Head2')\n\n    input_4 = head_2(input_3_concat)\n    input_4_avg = tf.keras.layers.Average()([input_3, input_4]) \n\n    head_3 = tf.keras.Sequential([\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(256, kernel_initializer='lecun_normal', activation='selu'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Dense(128, kernel_initializer='lecun_normal', activation='selu'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1), name='l2_norm'),\n        tf.keras.layers.Dense(n_labels, activation=\"sigmoid\")\n        ],name='Head3')\n\n    output = head_3(input_4_avg)\n\n    model = tf.keras.models.Model(inputs = [input_1, ], outputs = output)\n    opt = tfa.optimizers.RectifiedAdam(learning_rate=1e-03)\n    opt = tfa.optimizers.SWA(opt)\n    model.compile(optimizer=opt, \n                  loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing), \n                  metrics=['AUC'])\n    \n    return model","226feb6b":"%%time\n\nif NN_NAME == 'resnet':\n    models = []\n\n    for fold in range(FOLDS):\n        tf.keras.backend.clear_session()\n        model = create_resnet(X.shape[-1], y.shape[-1], encoder)\n        model.load_weights(pathlib.Path(f'..\/input\/janestreet-resnet-with-autoencoder-train\/model_{SEED}_{fold}.hdf5'))\n        models.append(model)\n        \n    models = [models[-1]]","7b82dba0":"%%time\n\nif NN_NAME == 'mlp':\n    model = tf.keras.models.load_model('..\/input\/jane-street-with-keras-nn-overfit\/model.h5')\n    models = [model]","5421b311":"f = np.median\nth = 0.500\n\nimport janestreet\nenv = janestreet.make_env()\nfor (test_df, pred_df) in tqdm(env.iter_test()):\n    if test_df['weight'].item() > 0:\n        x_tt = test_df.loc[:, features].values\n        \n        # GBDT inference with treelite\n        batch = treelite_runtime.Batch.from_npy2d(x_tt)\n        xgb_pred = predictor.predict(batch)\n    \n        # NN inference\n        if np.isnan(x_tt[:, 1:].sum()):\n            x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * f_mean\n        \n        pred = np.mean([model(x_tt, training = False).numpy() for model in models],axis=0)\n        pred = f(pred)\n        \n        # ensemble\n        pred_df.action = np.where(0.85*pred + 0.15*xgb_pred >= th, 1, 0).astype(int)\n    else:\n        pred_df.action = 0\n    env.predict(pred_df)","c158c928":"# Load NN","eede5934":"# Treelite","c0638cb1":"## Submission","55aeef2c":"# Loading the training data","3dcd6ca8":"# Libraries","f9684744":"# Load XGB with Treelite","a3d072e9":"Thought ensembling is not possible in this competiton due to the limited inference time?\n\nYes, it is! (But at most 2x shallow NN + 1x GBDT, in my experiment)\n\nHere I demonstrate it by simply ensembling the following public notebooks of mine:\n\nGBDT:\n\n- [[JaneStreet] Faster Inference by XGB with Treelite](https:\/\/www.kaggle.com\/code1110\/janestreet-faster-inference-by-xgb-with-treelite)\n\nNN (one of the following):\n\n- [[janestreet] 1DCNN for Feature Extraction (infer)](https:\/\/www.kaggle.com\/code1110\/janestreet-1dcnn-for-feature-extraction-infer)\n- [[janestreet] ResNet with AutoEncoder (infer)](https:\/\/www.kaggle.com\/code1110\/janestreet-resnet-with-autoencoder-infer)\n- [Jane Street with Keras NN overfit](https:\/\/www.kaggle.com\/code1110\/jane-street-with-keras-nn-overfit)\n\n\nThe key is to use [Treelite](https:\/\/treelite.readthedocs.io\/en\/latest\/) for a GBDT model to accelerate the inference speed."}}