{"cell_type":{"8b2bff5d":"code","2052513e":"code","d32bf604":"code","b2394586":"code","795e3a9a":"code","705a4902":"code","9a943043":"code","992ae010":"code","74106053":"code","e59b9daf":"code","725544bf":"code","7d8cf535":"code","9e78f679":"code","1577480a":"code","07d652d3":"code","e66e3214":"code","4b2caaa2":"code","322f5297":"code","ba793a9b":"code","adadf676":"code","10bfc065":"code","8d1e720b":"code","07558f81":"code","d8785538":"code","eb07260d":"code","31730671":"code","11415b8a":"code","d11e87f6":"code","6f24de4f":"code","5f1cf546":"code","435c0f24":"code","05b1ffe9":"code","3207eeb6":"code","fab63839":"code","8274f2fe":"code","52a05b1c":"code","e437642e":"code","0f8beae4":"code","e0dae871":"code","a05b51c6":"code","0dd9cca8":"code","265f96ce":"code","a9ca118b":"code","219ffcb3":"code","ed035a00":"code","e510f2de":"code","bfe25623":"code","0999cca4":"code","ce9408ab":"code","e16a57e6":"code","45d18c78":"code","2c722867":"code","b2286ba3":"code","86786a18":"code","f08fc282":"code","745e3a3f":"code","804b3103":"code","e0c31fe4":"code","907c5ef1":"code","18d99c39":"code","2eafec22":"code","fbc0b5b7":"code","e5e9ccf2":"code","ff133d9e":"code","d8c431c0":"code","d2902038":"code","043ce2fc":"code","6eaf8de4":"code","cd4ef495":"code","263ea117":"code","e53966d2":"code","dcd78de2":"code","0565907e":"code","d416b6ef":"code","f8e78b70":"code","136d7e65":"code","9f4422ba":"code","cfabc8a2":"code","3867da48":"code","187850cb":"code","b9c4c24c":"code","e3a6feeb":"code","d9261120":"code","638dc896":"code","60dedf98":"code","08f3a812":"code","63afe521":"code","8a791674":"code","d037bea1":"code","06d4188c":"code","be1ea470":"code","f1a8701e":"code","663e568e":"code","369a6b36":"code","dc40f75b":"code","69f6596e":"code","1681cc33":"code","5367ebf1":"code","324fc62a":"code","d79a6502":"code","963f349e":"code","aca45d43":"code","4215a29f":"code","4d38cfba":"code","e0012fe4":"code","e0dd6d76":"code","41da65b1":"code","41dd7682":"code","3d7d2d25":"code","47a437a7":"code","c76f9fb9":"code","30bc18bf":"code","899d5710":"code","79e3d74f":"code","99f9dfe9":"markdown","3aac4ae3":"markdown","caf8a0e9":"markdown","aafa0beb":"markdown","e23b1193":"markdown","f561a376":"markdown","763694fd":"markdown","0abaa8ca":"markdown","174f0b15":"markdown","18e5821d":"markdown","a6b2f39b":"markdown","c2672227":"markdown","bd03df00":"markdown","7c5d9e27":"markdown","b03f01b6":"markdown","058b198c":"markdown","7f24489f":"markdown","4fd6b5d2":"markdown","318abf27":"markdown","b9627ece":"markdown","5274fa89":"markdown","c3cf6e6b":"markdown","c4a3be59":"markdown","613cab00":"markdown","628addca":"markdown","c2e0b3af":"markdown","37d6c9c9":"markdown","cf8d7312":"markdown","0f244258":"markdown","f5441e87":"markdown","a054b242":"markdown","bb7129a4":"markdown","d85efc6b":"markdown","78b65440":"markdown","f1b1677a":"markdown","4e49eb65":"markdown","f2059fe6":"markdown","f76b5a12":"markdown","04ebe392":"markdown","31000b48":"markdown","84a90146":"markdown","071671ee":"markdown","f801fe94":"markdown","1e540359":"markdown","4d004246":"markdown","f307cb12":"markdown","2ea70c82":"markdown","66f3448f":"markdown","061f3cc4":"markdown","a8025c17":"markdown","44a4a367":"markdown","35209aea":"markdown","d89081a2":"markdown","e038ac35":"markdown","065945f8":"markdown","89eae134":"markdown","8af7ce93":"markdown","6428dc59":"markdown","529b263b":"markdown","1f70bae9":"markdown","b74c624b":"markdown","a31e4f8a":"markdown","b58192b1":"markdown","6a27a72e":"markdown","6d5cc4c7":"markdown","4857bbee":"markdown","9aed665b":"markdown","eec424ee":"markdown","7031e847":"markdown","82876186":"markdown","7cf171c5":"markdown","c8d5781c":"markdown","12d3b59c":"markdown","71cc270b":"markdown","e37417f8":"markdown","c964106f":"markdown","aab529df":"markdown"},"source":{"8b2bff5d":"import numpy as np\nimport pandas as pd\nimport sklearn\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport matplotlib.animation as animation\nfrom IPython.core.display import HTML","2052513e":"hf = pd.read_csv(\"..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv\")","d32bf604":"hf.platelets = hf.platelets\/1000","b2394586":"hf.head()","795e3a9a":"hf[\"CPK\"] = hf[\"creatinine_phosphokinase\"]\nhf = hf.drop(\"creatinine_phosphokinase\", axis=1)","705a4902":"# PALETTE\n\nfrom matplotlib.colors import LinearSegmentedColormap\nimport numpy as np\nimport matplotlib\n\ndef colorFader(c1,c2,mix=0): #fade (linear interpolate) from color c1 (at mix=0) to c2 (mix=1)\n    c1=np.array(matplotlib.colors.to_rgb(c1))\n    c2=np.array(matplotlib.colors.to_rgb(c2))\n    return matplotlib.colors.to_hex((1-mix)*c1 + mix*c2)\n\nmeshPalette = []\nmeshPalette_rev = []\nnCol = 50\n\nfor i in range(nCol):\n    meshPalette.append(colorFader(\"#71706E\", \"#990303\", i\/nCol))\n    meshPalette_rev.append(colorFader(\"#990303\",\"#9C9999\", i\/nCol))\n\ncm = LinearSegmentedColormap.from_list(\"cmap_name\", meshPalette, N=nCol)\ncm_rev = LinearSegmentedColormap.from_list(\"cmap_name\", meshPalette_rev, N=nCol)\n\n#sns.palplot(meshPalette)\n#sns.palplot([\"#990303\", \"#9C9999\", \"#71706E\", \"#292323\", \"#FFFFFF\"]);","9a943043":"numerical_features = [\"age\", \"CPK\", \"ejection_fraction\", \"platelets\", \"serum_creatinine\", \"serum_sodium\"]\ncategorical_features = [\"anaemia\", \"diabetes\", \"high_blood_pressure\", \"sex\", \"smoking\"]","992ae010":"plt.figure(figsize=(18, 27))\n\nfor i, col in enumerate(numerical_features):\n    plt.subplot(6, 4, i*2+1)\n    plt.subplots_adjust(hspace =.25, wspace=.3)\n    \n    plt.grid(True)\n    plt.title(col)\n    sns.kdeplot(hf.loc[hf[\"DEATH_EVENT\"]==0, col], label=\"alive\", color = \"#990303\", shade=True, kernel='gau', cut=0)\n    sns.kdeplot(hf.loc[hf[\"DEATH_EVENT\"]==1, col], label=\"dead\",  color = \"#292323\", shade=True, kernel='gau', cut=0)\n    plt.subplot(6, 4, i*2+2) \n    sns.boxplot(y = col, data = hf, x=\"DEATH_EVENT\", palette = [\"#990303\", \"#9C9999\"])   ","74106053":"plt.figure(figsize=(12, 8))\n\nfor i, col in enumerate(categorical_features):\n    plt.subplot(2, 3, i+1)\n    plt.title(col)\n    plt.subplots_adjust(hspace =.5, wspace=.3)\n    sns.countplot(data=hf, x=col, hue=\"DEATH_EVENT\", palette = [\"#990303\", \"#9C9999\"], alpha=0.8, edgecolor=\"k\", linewidth=1)","e59b9daf":"from sklearn.preprocessing import StandardScaler","725544bf":"import matplotlib.image as mpimg\nfrom matplotlib import gridspec\n\nhf_norm = hf.copy()\n\nfor i, col in enumerate(numerical_features):\n    hf_norm[[col]] = StandardScaler(with_mean=True, with_std=True).fit_transform(hf_norm[[col]])\n    \nplt.figure(figsize=(16, 4))\ngs  = gridspec.GridSpec(1, 5, width_ratios=[1, 1 ,0.1, 1, 1])\nplt.subplot(gs[0])   \nplt.grid(True)\nplt.title(\"ejection fraction\")\nsns.kdeplot(hf.loc[hf[\"DEATH_EVENT\"]==0, \"ejection_fraction\"], label=\"alive\", color = \"#990303\", shade=True, kernel='gau', cut=0)\nsns.kdeplot(hf.loc[hf[\"DEATH_EVENT\"]==1, \"ejection_fraction\"], label=\"dead\",  color = \"#292323\", shade=True, kernel='gau', cut=0)\nplt.subplot(gs[1]) \nsns.boxplot(y = \"ejection_fraction\", data = hf, x=\"DEATH_EVENT\", palette = [\"#990303\", \"#9C9999\"])  \nplt.subplot(gs[2])\nplt.imshow(mpimg.imread(\"..\/input\/heart-failure-clinical-records-images\/right_arrow.png\"))\nplt.axis('off')\nplt.subplot(gs[3])\nplt.grid(True)\nplt.title(\"ejection fraction\")\nsns.kdeplot(hf_norm.loc[hf[\"DEATH_EVENT\"]==0, \"ejection_fraction\"], label=\"alive\", color = \"#990303\", shade=True, kernel='gau', cut=0)\nsns.kdeplot(hf_norm.loc[hf[\"DEATH_EVENT\"]==1, \"ejection_fraction\"], label=\"dead\",  color = \"#292323\", shade=True, kernel='gau', cut=0)\nplt.subplot(gs[4])\nsns.boxplot(y = \"ejection_fraction\", data = hf_norm, x=\"DEATH_EVENT\", palette = [\"#990303\", \"#9C9999\"]);  \nplt.tight_layout()","7d8cf535":"all_features = categorical_features.copy()\nall_features.extend(numerical_features)","9e78f679":"plt.figure(figsize=(8, 7))\nsns.heatmap(hf_norm[all_features].corr(method='pearson'), vmin=-1, vmax=1, cmap='viridis', annot=True, fmt='.2f');","1577480a":"from scipy.stats import shapiro\n\n#sw_df = pd.DataFrame(columns=[\"DEATH_EVENT=0\", \"DEATH_EVENT=1\", \"Both\"])\nindex = [(feat, \"statistic\") for feat in numerical_features]\nindex.extend([(feat, \"p-value\") for feat in numerical_features])\n\nindex = pd.MultiIndex.from_tuples(index)\n\nsw_df = pd.DataFrame(index=index,  columns = [\"Both Classes\", \"DEATH_EVENT=0\", \"DEATH_EVENT=1\"])\n\nfor feat in numerical_features:\n    x = hf_norm[feat]\n    stat, p = shapiro(x)\n    \n    sw_df[\"Both Classes\"].loc[(feat, \"statistic\")] = stat\n    sw_df[\"Both Classes\"].loc[(feat, \"p-value\")] = p\n    \n    x = hf_norm.loc[hf[\"DEATH_EVENT\"]==0, feat]\n    stat, p = shapiro(x)\n    sw_df[\"DEATH_EVENT=0\"].loc[(feat, \"statistic\")] = stat\n    sw_df[\"DEATH_EVENT=0\"].loc[(feat, \"p-value\")] = p\n    \n    x = hf_norm.loc[hf[\"DEATH_EVENT\"]==1, feat]\n    stat, p = shapiro(x)\n    sw_df[\"DEATH_EVENT=1\"].loc[(feat, \"statistic\")] = stat\n    sw_df[\"DEATH_EVENT=1\"].loc[(feat, \"p-value\")] = p\n    \nsw_df = sw_df.unstack()","07d652d3":"pd.set_option('display.float_format', '{:.3g}'.format)\nsw_df","e66e3214":"from sklearn.model_selection import train_test_split, StratifiedKFold","4b2caaa2":"train_ratio = 0.75\nval_ratio = 0.25\n\nho_train_df, ho_val_df = train_test_split(hf_norm, train_size = train_ratio, random_state=42)\nunnorm_ho_train_df, unnorm_ho_val_df = train_test_split(hf, train_size = train_ratio, random_state=42)\n\nprint(\"Holdout split:\")\nprint(f\"Train samples: {len(ho_train_df)}\")\nprint(f\"Validation\/Test samples: {len(ho_val_df)}\")","322f5297":"from sklearn.feature_selection import mutual_info_classif\n\n    \nMI = (mutual_info_classif(ho_train_df[all_features],\n                             ho_train_df[\"DEATH_EVENT\"], n_neighbors=20,\n                             discrete_features=[True, True, True, True, True, False, False, False, False, False, False],\n                             random_state=42))\n\nplt.figure(figsize=(5.4, 4))\nplt.barh(y=all_features, width=MI, color=\"#990303\")\nplt.title(\"Mutual information w.r.t. DEATH_EVENT (whole training set)\");\nplt.xlabel(\"Mutual information\")\nplt.gca().xaxis.grid(True, linestyle=':');\nplt.tight_layout();","ba793a9b":"from scipy.stats import chi2_contingency\nfrom scipy.stats import chi2\n\nprint(\"Observed\")\nctable = pd.crosstab(ho_train_df[\"anaemia\"], ho_train_df[\"DEATH_EVENT\"])\nctable.columns = [\"DEATH_EVENT=0\", \"DEATH_EVENT=1\"]\nctable.index = [\"anaemia=0\", \"anaemia=1\"]\nctable.loc[\"Total\"] = ctable.sum()\nctable[\"Total\"] = ctable.sum(axis=1)\nctable","adadf676":"print(\"Expected\")\ncontingency_table = pd.crosstab(ho_train_df[\"anaemia\"], ho_train_df[\"DEATH_EVENT\"])\nstat, p, dof, expected = chi2_contingency(contingency_table)\nexpected = pd.DataFrame(expected)\nexpected.columns = [\"DEATH_EVENT=0\", \"DEATH_EVENT=1\"]\nexpected.index = [\"anaemia=0\", \"anaemia=1\"]\nexpected","10bfc065":"def chi2_test(df, feat1, feat2):\n\n    contingency_table = pd.crosstab(df[feat1], df[feat2])\n    \n    stat, p, dof, expected = chi2_contingency(contingency_table)\n    prob = 0.95 # alpha=0.05\n    critical = chi2.ppf(prob, dof)\n    #print('alpha=%.3f, critical_value=%.3f,\\nstat=%.3f' % (1 - prob, critical, stat))\n\n    # interpret p-value\n    alpha = 1.0 - prob\n    \n    return stat, p","8d1e720b":"stats = []\np_values = []\n\nfor feat in categorical_features:\n        \n    stat, p = chi2_test(ho_train_df, feat, \"DEATH_EVENT\")\n    stats.append(stat)\n    p_values.append(p)        \n\nfig, axes = plt.subplots(ncols=2, sharey=True)\nfig.set_size_inches(8.5, 4)\naxes[0].barh(y=categorical_features, width=stats, color=\"#990303\", label=\"test statistic\", height=0.5)\naxes[0].set_title(\"Chi squared test statistics\")\n\naxes[1].barh(y=categorical_features, width=p_values, color=\"#9C9999\", label=\"p-value\", height=0.5)\naxes[1].set_title(\"Chi squared test p-values\")\n\naxes[0].xaxis.grid(True, linestyle=':');\naxes[1].xaxis.grid(True, linestyle=':');\n\naxes[0].legend(loc=1)\naxes[1].legend(loc=4)\n\nfig.subplots_adjust(wspace=0.06)\nplt.tight_layout()","07558f81":"kfold_train_df, kfold_val_df = ho_train_df.copy(), ho_val_df.copy()\n\nall_features = ['anaemia', 'sex', 'age', 'CPK',\n                'ejection_fraction', 'serum_creatinine', 'serum_sodium']\n\n#ho_train_df, ho_val_df = ho_train_df[all_features+[\"DEATH_EVENT\"]], ho_val_df[all_features+[\"DEATH_EVENT\"]]\n#unnorm_ho_train_df, unnorm_ho_val_df = unnorm_ho_train_df[all_features+[\"DEATH_EVENT\"]], unnorm_ho_val_df[all_features+[\"DEATH_EVENT\"]]","d8785538":"kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nplt.figure(figsize=(12.5, 2.5))\n\nprint(\"Mutual information with respect to DEATH_EVENT (5 folds)\")\nfor i, (train_idx, val_idx) in enumerate(kf.split(kfold_train_df, kfold_train_df[\"DEATH_EVENT\"])):\n    MI=(mutual_info_classif(kfold_train_df.iloc[train_idx][categorical_features+numerical_features],\n                             kfold_train_df.iloc[train_idx][\"DEATH_EVENT\"], n_neighbors=20,\n                             discrete_features=[True, True, True, True, True, False, False, False, False, False, False],\n                             random_state=42))\n    plt.subplot(1, 5, i+1)\n    plt.title(f\"Iteration {i+1}\")\n    plt.barh(y=['anaemia', 'diabetes', 'h.b.p.','sex', 'smoking', 'age', 'CPK', 'e.f.', 'platelets',\n         's.c.', 's.s.'], width=MI, color=\"#990303\", label=\"test statistic\")\n    \nplt.tight_layout()  ","eb07260d":"kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nplt.figure(figsize=(12.5, 2.5))\nprint(\"Chi squared test statistics (5 folds)\")\nfor i, (train_idx, val_idx) in enumerate(kf.split(kfold_train_df, kfold_train_df[\"DEATH_EVENT\"])):\n    stats = []\n    p_values = []\n\n    for feat in categorical_features:\n\n        stat, p = chi2_test(kfold_train_df.iloc[train_idx], feat, \"DEATH_EVENT\")\n        stats.append(stat)\n        p_values.append(p)\n    \n    plt.subplot(1, 5, i+1)\n    plt.title(f\"Iteration {i+1}\")\n    plt.barh(y=['anaemia', 'diabetes', 'h.b.p.', 'sex', 'smoking'], width=stats, color=\"#990303\", label=\"test statistic\")\nplt.tight_layout()","31730671":"numerical_features = ['age', 'CPK', 'ejection_fraction', 'serum_creatinine', 'serum_sodium']\nall_features_kfold = [\n    [\"anaemia\", \"diabetes\"]+numerical_features,\n    [\"sex\", \"smoking\"]+numerical_features,\n    [\"anaemia\", \"sex\"]+numerical_features,\n    [\"anaemia\", \"smoking\"]+numerical_features,\n    [\"sex\", \"smoking\"]+numerical_features,\n]","11415b8a":"plt.figure(figsize=(3, 3))\nplt.pie(hf[\"DEATH_EVENT\"].value_counts(),\n        labels = [\"alive\", \"dead\"],\n        colors = [\"#990303\", \"#9C9999\"], \n        wedgeprops={'edgecolor':'black', 'linewidth': 2}, \n        autopct = lambda y: str(round(y))+\"%\",\n        startangle=90);","d11e87f6":"n_to_sample = len(ho_train_df[ho_train_df.DEATH_EVENT==0]) - len(ho_train_df[ho_train_df.DEATH_EVENT==1])\nnew_samples = ho_train_df[ho_train_df.DEATH_EVENT==1].sample(n_to_sample, replace=True, random_state=42)\n\nho_train_df_rs = ho_train_df.append(new_samples)\n\nnew_samples = unnorm_ho_train_df[unnorm_ho_train_df.DEATH_EVENT==1].sample(n_to_sample, replace=True, random_state=42)\nunnorm_ho_train_df_rs = unnorm_ho_train_df.append(new_samples)","6f24de4f":"nbins = 10\nplt.figure(figsize = (8, 4))\n\nplt.subplot(1, 2, 1)\nplt.ylim(0, 1)\nplt.title(\"Distribution for class 1 before oversampling\")\nsns.distplot(ho_train_df[ho_train_df.DEATH_EVENT==1].ejection_fraction, bins=nbins)\nplt.subplot(1, 2, 2)\nplt.ylim(0, 1)\nplt.title(\"Distribution for class 1 after oversampling\")\nsns.distplot(ho_train_df_rs[ho_train_df_rs.DEATH_EVENT==1].ejection_fraction, bins=nbins);\nplt.tight_layout()","5f1cf546":"def rand_jitter(arr):\n    np.random.seed(42)\n    stdev = .01*(max(arr)-min(arr))\n    return arr + np.random.randn(len(arr)) * stdev","435c0f24":"norm = np.linalg.norm\nimport random\nfrom random import sample \n\ndef SMOTE(data, sampling_rate, n_neigh, random_state=42):\n    random.seed(random_state)\n    new_samples = []\n    \n    if sampling_rate==0:\n        return\n    \n    if sampling_rate>n_neigh: return      \n    data = data.reset_index(drop=True)\n\n    n_samples = data.count()[0]\n\n    for i in range(n_samples):\n        dists = []\n        for j in range(n_samples):\n            if i==j: continue\n            dists.append((j, norm(data.loc[i]-data.loc[j])))    \n        \n        topk = sorted(dists, key=lambda s: s[1])[:n_neigh]\n        neighs = sample(topk, sampling_rate)\n\n        for neigh in neighs:\n            alpha = random.random()\n            new_samples.append(data.loc[i] + alpha * (data.loc[neigh[0]]-data.loc[i]))\n            \n    return new_samples","05b1ffe9":"%matplotlib notebook\nplt.ioff()\nfig, ax = plt.subplots(1, 1);\n%matplotlib inline \nplt.ion()","3207eeb6":"fig.set_size_inches(4, 4);\nfig.set_dpi(100)\n\ndef animate_func(i):\n    new_samples = SMOTE(hf[hf[\"DEATH_EVENT\"]==1][[\"ejection_fraction\", \"serum_creatinine\"]], sampling_rate = i, n_neigh = 20)\n    sm_hf = hf.copy()\n\n    if new_samples:\n        sm_hf = hf.append(new_samples)\n        \n    sm_hf[\"DEATH_EVENT\"].fillna(1, inplace=True)\n    \n    ax.clear();\n\n    ax.set_ylim(0, 17.5);\n    ax.set_xlim(10, 90);\n    \n    ax.scatter((sm_hf[sm_hf[\"DEATH_EVENT\"]==1][\"ejection_fraction\"]),\n            sm_hf[sm_hf[\"DEATH_EVENT\"]==1][\"serum_creatinine\"], s=5, label=\"DEAD\", color=\"#71706E\", alpha=1)\n    ax.scatter(rand_jitter(sm_hf[sm_hf[\"DEATH_EVENT\"]==0][\"ejection_fraction\"]),\n            sm_hf[sm_hf[\"DEATH_EVENT\"]==0][\"serum_creatinine\"], s=5, label=\"ALIVE\", color=\"#990303\", alpha=1)\n    ax.legend()\n    \n    ax.set_xlabel(\"ejection_fraction\")\n    ax.set_ylabel(\"serum_creatinine\")\n    ax.set_title(f\"Sampling rate: {i}, ALIVE = {sm_hf[sm_hf.DEATH_EVENT==0].count()[3]}, DEAD = {sm_hf[sm_hf.DEATH_EVENT==1].count()[3]}\")\n\n    return [fig]\n    \nanim = animation.FuncAnimation(\n                               fig, \n                               animate_func, \n                               frames = 10,\n                               interval = 100, # in ms\n                               );\n\nHTML(anim.to_jshtml())","fab63839":"n_to_sample = len(ho_train_df[ho_train_df.DEATH_EVENT==0]) - len(ho_train_df[ho_train_df.DEATH_EVENT==1])\nnew_samples = SMOTE(ho_train_df[ho_train_df[\"DEATH_EVENT\"]==1][all_features],\n                    sampling_rate = 1, n_neigh = 50)\n\n# categorical attributes need to be fixed\nfor s in new_samples:\n    s[\"anaemia\"] = np.round(s[\"anaemia\"])\n    s[\"sex\"] = np.round(s[\"sex\"])\n\nho_train_df_sm = ho_train_df.append(new_samples)\nho_train_df_sm[\"DEATH_EVENT\"].fillna(1, inplace=True)\n\n\nnew_samples = SMOTE(unnorm_ho_train_df[unnorm_ho_train_df[\"DEATH_EVENT\"]==1][all_features],\n                    sampling_rate = 1, n_neigh = 50)\n\n# categorical attributes need to be fixed\nfor s in new_samples:\n    s[\"anaemia\"] = np.round(s[\"anaemia\"])\n    s[\"sex\"] = np.round(s[\"sex\"])\n\nunnorm_ho_train_df_sm = unnorm_ho_train_df.append(new_samples)\nunnorm_ho_train_df_sm[\"DEATH_EVENT\"].fillna(1, inplace=True)","8274f2fe":"nbins = 10\nplt.figure(figsize = (8, 4))\n\nplt.subplot(1, 2, 1)\nplt.ylim(0, 1)\nplt.title(\"Distribution for class 1 before SMOTE\")\nsns.distplot(ho_train_df[ho_train_df.DEATH_EVENT==1].ejection_fraction, bins=nbins)\nplt.subplot(1, 2, 2)\nplt.ylim(0, 1)\nplt.title(\"Distribution for class 1 after SMOTE\")\nsns.distplot(ho_train_df_sm[ho_train_df_sm.DEATH_EVENT==1].ejection_fraction, bins=nbins)\nplt.tight_layout();","52a05b1c":"w = len(ho_train_df) \/ (2 * np.bincount(ho_train_df.DEATH_EVENT))\nprint(f\"class 0: {w[0]} \\nclass 1: {w[1]}\")","e437642e":"# KFOLD\n\nclass KFold():\n    \n    # caching smote results \n    smote_folds = None\n    smote_labels = None\n    \n    def __init__(self, all_features_kfold, random_state=42):\n        self.smote_folds = []\n        self.smote_labels = []\n        self.features = all_features_kfold\n        \n        self.kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    \n    def fit_predict(self, model, X, y, threshold = None, resampling = None, cached = False):\n        acc, rec, pre, f1 = 0, 0, 0, 0\n        \n        if resampling==\"SMOTE\" and not cached:\n            self.smote_folds = []\n            self.smote_labels = []\n        \n        for i, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n           \n            X_fold = X[self.features[i]] # for each fold we have different features\n        \n            X_train, X_val = X_fold.iloc[train_idx], X_fold.iloc[val_idx]\n            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n\n            if resampling==\"oversampling\":\n                n_to_sample = len(X_train[y_train==0]) - len(X_train[y_train==1])\n                new_samples = X_train[y_train==1].sample(n_to_sample, replace=True, random_state=42)\n                \n                X_train = X_train.append(new_samples)\n                y_train = y_train.append(pd.Series([1]*len(new_samples)))\n                \n                X_train = X_train.reset_index(drop=True)\n                y_train = y_train.reset_index(drop=True)\n\n            if resampling==\"SMOTE\": \n                if not cached or len(self.smote_folds)<5 or len(self.smote_labels)<5:\n                    n_to_sample = len(X_train[y_train==0]) - len(X_train[y_train==1])\n                    new_samples = SMOTE(X_train[y_train==1], sampling_rate = 1, n_neigh = 50)\n\n                    # categorical attributes need to be fixed\n                    for s in new_samples:\n                        if \"anaemia\" in  s.index:\n                            s[\"anaemia\"] = np.round(s[\"anaemia\"])\n                        if \"high_blood_pressure\" in s.index:\n                            s[\"high_blood_pressure\"] = np.round(s[\"high_blood_pressure\"])\n                        if \"sex\" in  s.index:\n                            s[\"sex\"] = np.round(s[\"sex\"])\n                        if \"smoking\" in  s.index:\n                            s[\"smoking\"] = np.round(s[\"smoking\"])\n                        if \"diabetes\" in  s.index:\n                            s[\"diabetes\"] = np.round(s[\"diabetes\"])\n                                            \n                    X_train = X_train.append(new_samples, ignore_index=True)\n                    y_train = y_train.append(pd.Series([1]*len(new_samples)))\n                    \n                    X_train = X_train.reset_index(drop=True)\n                    y_train = y_train.reset_index(drop=True)\n                    \n                    # cache smoted folds\n                    self.smote_folds.append(X_train)\n                    self.smote_labels.append(y_train)\n                    \n                else:\n                    # use cached folds\n                    X_train = self.smote_folds[i]\n                    y_train = self.smote_labels[i]\n\n            model.fit(X_train, y_train)\n            preds = model.predict(X_val)\n\n            if threshold:\n                preds[preds>=threshold] = 1\n                preds[preds<threshold] = 0\n\n            acc += accuracy_score(y_val, preds)\n            pre += precision_score(y_val, preds)\n            rec += recall_score(y_val, preds)\n            f1 += f1_score(y_val, preds)\n\n        acc \/= 5\n        pre \/= 5\n        rec \/= 5\n        f1 \/= 5\n        return acc, pre, rec, f1","0f8beae4":"# initialize kfold object\nkfold = KFold(all_features_kfold, random_state=42)","e0dae871":"from sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.svm import SVC, SVR\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\n\nfrom sklearn.metrics import roc_curve","a05b51c6":"final_results = pd.DataFrame(columns=[\"Model\", \"Holdout Original\",\"Holdout Oversampling\", \"Holdout SMOTE\", \n                   \"Holdout class-weight=balanced\", \"KFold Original\", \"KFold Oversampling\",\n                   \"KFold SMOTE\", \"KFold class-weight=balanced\"])","0dd9cca8":"p = np.linspace(1e-2, 1-1e-2, 100)\n\nplt.figure(figsize=(3, 3))\nplt.plot(p, [1-pj**2-(1-pj)**2 for pj in p], label=\"Gini Index\")\nplt.plot(p, [-((pj*np.log(pj))+((1-pj)*np.log(1-pj))) for pj in p], label=\"Shannon Index\")\nplt.grid()\nplt.legend();\nplt.xlabel(\"p0\")\nplt.title(\"Gini and shannon indexes for binary data\");","265f96ce":"%matplotlib notebook\nplt.ioff()\nfig, (ax1, ax2) = plt.subplots(1, 2);\n%matplotlib inline \nplt.ion()\n\ndepth = range(1, 6)\nh = 0.03\nx_min, x_max = hf[\"ejection_fraction\"].min() - 1, hf[\"ejection_fraction\"].max() + 1\ny_min, y_max = hf[\"serum_creatinine\"].min() - .5, hf[\"serum_creatinine\"].max() + .5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                        np.arange(y_min, y_max, h))\n\nfig.set_size_inches(14, 7);\ndef animate_func(i):\n    \n    dt = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=depth[i], random_state=42, )\n    dt.fit(hf[[\"ejection_fraction\", \"serum_creatinine\"]], hf.DEATH_EVENT)\n    Z = dt.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    \n    ax1.clear()\n    ax1.contourf(xx, yy, Z, alpha=0.5, cmap=cm_rev)\n    ax1.scatter(hf[hf[\"DEATH_EVENT\"]==0][\"ejection_fraction\"],\n                hf[hf[\"DEATH_EVENT\"]==0][\"serum_creatinine\"], label=\"alive\", color=\"#990303\", edgecolor='BLACK')\n    ax1.scatter(hf[hf[\"DEATH_EVENT\"]==1][\"ejection_fraction\"],\n                hf[hf[\"DEATH_EVENT\"]==1][\"serum_creatinine\"], label=\"dead\", color=\"#9C9999\", edgecolor='BLACK')\n\n    ax1.set_xlabel(\"ejection_fraction\")\n    ax1.set_ylabel(\"serum_creatinine\")\n    ax1.legend();\n    \n    tree.plot_tree(dt,  feature_names=[\"e.f.\", \"s.c.\"], filled=True,\n                   label='none', ax=ax2, rounded=True, proportion=True, impurity=False);\n    ax2.set_title(f\"Decision tree (depth: {i+1})\")\n    \n    fig.tight_layout()\n    return [fig]\n\nanim = animation.FuncAnimation(\n                               fig, \n                               animate_func, \n                               frames = len(depth),\n                               interval = 200, # in ms\n                               );\n\nHTML(anim.to_jshtml())","a9ca118b":"depth = range(1, 12)\n\nrs_acc, rs_rec, rs_pre, rs_f1 = [], [], [], []\nsm_acc, sm_rec, sm_pre, sm_f1 = [], [], [], []\nno_rs_acc, no_rs_rec, no_rs_pre, no_rs_f1 = [], [], [], []\nw_acc, w_rec, w_pre, w_f1 = [], [], [], []\n\nfor d in depth: \n    # random oversampling \n    dt = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=d, random_state=42, class_weight=None)\n    dt.fit(unnorm_ho_train_df_rs[all_features], unnorm_ho_train_df_rs['DEATH_EVENT'])\n    pred = dt.predict(unnorm_ho_val_df[all_features])\n    rs_acc.append(accuracy_score(unnorm_ho_val_df[\"DEATH_EVENT\"], pred))\n    rs_rec.append(recall_score(unnorm_ho_val_df[\"DEATH_EVENT\"], pred))\n    rs_pre.append(precision_score(unnorm_ho_val_df[\"DEATH_EVENT\"], pred))\n    rs_f1.append(f1_score(unnorm_ho_val_df[\"DEATH_EVENT\"], pred))\n    \n    # SMOTE \n    dt = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=d, random_state=42, class_weight=None)\n    dt.fit(unnorm_ho_train_df_sm[all_features], unnorm_ho_train_df_sm['DEATH_EVENT'])\n    pred = dt.predict(unnorm_ho_val_df[all_features])\n    sm_acc.append(accuracy_score(unnorm_ho_val_df[\"DEATH_EVENT\"], pred))\n    sm_rec.append(recall_score(unnorm_ho_val_df[\"DEATH_EVENT\"], pred))\n    sm_pre.append(precision_score(unnorm_ho_val_df[\"DEATH_EVENT\"], pred))\n    sm_f1.append(f1_score(unnorm_ho_val_df[\"DEATH_EVENT\"], pred))\n    \n    # not resampled\n    dt = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=d, random_state=42, class_weight=None)\n    dt.fit(unnorm_ho_train_df[all_features], unnorm_ho_train_df['DEATH_EVENT'])\n    pred = dt.predict(unnorm_ho_val_df[all_features])\n    no_rs_acc.append(accuracy_score(unnorm_ho_val_df[\"DEATH_EVENT\"], pred))\n    no_rs_rec.append(recall_score(unnorm_ho_val_df[\"DEATH_EVENT\"], pred))\n    no_rs_pre.append(precision_score(unnorm_ho_val_df[\"DEATH_EVENT\"], pred))\n    no_rs_f1.append(f1_score(unnorm_ho_val_df[\"DEATH_EVENT\"], pred))\n \n    # weighted classes\n    dt = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=d, random_state=42, class_weight='balanced')\n    dt.fit(unnorm_ho_train_df[all_features], unnorm_ho_train_df['DEATH_EVENT'])\n    pred = dt.predict(unnorm_ho_val_df[all_features])\n    w_acc.append(accuracy_score(unnorm_ho_val_df[\"DEATH_EVENT\"], pred))\n    w_rec.append(recall_score(unnorm_ho_val_df[\"DEATH_EVENT\"], pred))\n    w_pre.append(precision_score(unnorm_ho_val_df[\"DEATH_EVENT\"], pred))\n    w_f1.append(f1_score(unnorm_ho_val_df[\"DEATH_EVENT\"], pred))","219ffcb3":"#sorted([(a, b) for a, b in zip(no_rs_acc, no_rs_f1)], key= lambda a: a[0]+a[1])[-1];","ed035a00":"plt.figure(figsize=(10, 10))\n\n# accuracy\nplt.subplot(2, 2, 1)    \nplt.plot(list(depth), rs_acc, label=\"random oversampling\", marker='o', linewidth=2, markersize=4)\nplt.plot(list(depth), sm_acc, label=\"SMOTE\", marker='o', linewidth=2, markersize=4)\nplt.plot(list(depth), no_rs_acc, label = \"original dataset\", marker='o', linewidth=2, markersize=4)\nplt.plot(list(depth), w_acc, label = \"class_weight balanced\", marker='o', linewidth=2, markersize=4)\n\nplt.legend(loc=4)\nplt.grid(True)\nplt.ylim(0, 1)\nplt.ylabel(\"Validation accuracy\")\nplt.xlabel(\"Tree depth\")\nplt.title(\"Accuracy\")\n\n# precision\nplt.subplot(2, 2, 2)    \nplt.plot(list(depth), rs_pre, label=\"random oversampling\", marker='o', linewidth=2, markersize=4)\nplt.plot(list(depth), sm_pre, label=\"SMOTE\", marker='o', linewidth=2, markersize=4)\nplt.plot(list(depth), no_rs_pre, label = \"original dataset\", marker='o', linewidth=2, markersize=4)\nplt.plot(list(depth), w_pre, label = \"class_weight balanced\", marker='o', linewidth=2, markersize=4)\n\nplt.legend(loc=4)\nplt.grid(True)\nplt.ylim(0, 1)\nplt.ylabel(\"Validation precision\")\nplt.xlabel(\"Tree depth\")\nplt.title(\"Precision\")\n\n# recall\nplt.subplot(2, 2, 3)    \nplt.plot(list(depth), rs_rec, label=\"random oversampling\", marker='o', linewidth=2, markersize=4)\nplt.plot(list(depth), sm_rec, label=\"SMOTE\", marker='o', linewidth=2, markersize=4)\nplt.plot(list(depth), no_rs_rec, label = \"original dataset\", marker='o', linewidth=2, markersize=4)\nplt.plot(list(depth), w_rec, label = \"class_weight balanced\", marker='o', linewidth=2, markersize=4)\n\nplt.legend(loc=4)\nplt.grid(True)\nplt.ylim(0, 1)\nplt.ylabel(\"Validation recall\")\nplt.xlabel(\"Tree depth\")\nplt.title(\"Recall\")\n\n# f1 score\nplt.subplot(2, 2, 4)    \nplt.plot(list(depth), rs_f1, label=\"random oversampling\", marker='o', linewidth=2, markersize=4)\nplt.plot(list(depth), sm_f1, label=\"SMOTE\", marker='o', linewidth=2, markersize=4)\nplt.plot(list(depth), no_rs_f1, label = \"original dataset\", marker='o', linewidth=2, markersize=4)\nplt.plot(list(depth), w_f1, label = \"class_weight balanced\", marker='o', linewidth=2, markersize=4)\n\nplt.legend(loc=4)\nplt.grid(True)\nplt.ylim(0, 1)\nplt.ylabel(\"Validation f1 score\")\nplt.xlabel(\"Tree depth\")\nplt.title(\"F1 score\");","e510f2de":"# KFOLD\n\nrs_acc, rs_rec, rs_pre, rs_f1 = [], [], [], []\nsm_acc, sm_rec, sm_pre, sm_f1 = [], [], [], []\nno_rs_acc, no_rs_rec, no_rs_pre, no_rs_f1 = [], [], [], []\nw_acc, w_rec, w_pre, w_f1 = [], [], [], []\n\nfor d in depth: \n    # random oversampling \n    dt = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=d, random_state=42, class_weight=None)\n    acc, rec, pre, f1 = kfold.fit_predict(dt, ho_train_df, ho_train_df['DEATH_EVENT'],\n                                                          resampling=\"oversampling\")\n    rs_acc.append(acc)\n    rs_rec.append(rec)\n    rs_pre.append(pre)\n    rs_f1.append(f1)\n    \n    # SMOTE \n    dt = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=d, random_state=42, class_weight=None)\n    acc, rec, pre, f1 = kfold.fit_predict(dt, ho_train_df, ho_train_df['DEATH_EVENT'],\n                                                          cached=True, resampling=\"SMOTE\")\n    sm_acc.append(acc)\n    sm_rec.append(rec)\n    sm_pre.append(pre)\n    sm_f1.append(f1)\n    \n    # not resampled\n    dt = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=d, random_state=42, class_weight=None)\n    acc, rec, pre, f1 = kfold.fit_predict(dt, ho_train_df, ho_train_df['DEATH_EVENT'])\n    no_rs_acc.append(acc)\n    no_rs_rec.append(rec)\n    no_rs_pre.append(pre)\n    no_rs_f1.append(f1)\n \n    # weighted classes\n    dt = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=d, random_state=42, class_weight='balanced')\n    acc, rec, pre, f1 = kfold.fit_predict(dt, ho_train_df, ho_train_df['DEATH_EVENT'])\n    w_acc.append(acc)\n    w_rec.append(rec)\n    w_pre.append(pre)\n    w_f1.append(f1)","bfe25623":"#sorted([(a, b) for a, b in zip(w_acc, w_f1)], key= lambda a: a[0]+a[1]);","0999cca4":"%matplotlib notebook\nplt.ioff()\nfig, ax = plt.subplots(1, 1);\n%matplotlib inline \nplt.ion()\n\nfig.set_size_inches(20, 15);\ndef animate_func(i):\n    dt = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=i+1, random_state=42, class_weight='balanced')\n    dt.fit(unnorm_ho_train_df[all_features], unnorm_ho_train_df['DEATH_EVENT'])\n    pred = dt.predict(unnorm_ho_val_df[all_features])\n    \n    tree.plot_tree(dt,  feature_names=all_features, filled=True, label='all', ax=ax, rounded=True, proportion=True);\n    ax.set_title(f\"Decision tree (depth: {i+1})\", fontsize=25)\n    # value are float variables due to class_weight=\"balanced\"\n    return [fig]\n\nanim = animation.FuncAnimation(\n                               fig, \n                               animate_func, \n                               frames = 11,\n                               interval = 200, # in ms\n                               );\n\nHTML(anim.to_jshtml())","ce9408ab":"# oob_score: Whether to use out-of-bag samples to estimate the generalization accuracy\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\nmax_features='sqrt' # square root of number of features       \nmax_depth = 14\ndepth = range(1, max_depth)\nn_trees = [5, 10, 20, 50, 100]\nn_vals = len(n_trees)\n\nrs_acc, rs_oob_acc, rs_rec, rs_pre, rs_f1= [None]*n_vals, [None]*n_vals, [None]*n_vals, [None]*n_vals, [None]*n_vals\nsm_acc, sm_oob_acc, sm_rec, sm_pre, sm_f1 = [None]*n_vals, [None]*n_vals, [None]*n_vals, [None]*n_vals, [None]*n_vals\nno_rs_acc, no_rs_oob_acc, no_rs_rec, no_rs_pre, no_rs_f1= [None]*n_vals, [None]*n_vals, [None]*n_vals, [None]*n_vals, [None]*n_vals\nw_acc, w_oob_acc, w_rec, w_pre, w_f1 = [None]*n_vals, [None]*n_vals, [None]*n_vals, [None]*n_vals, [None]*n_vals\n\nfor i,n in enumerate(n_trees):\n    \n    rs_acc[i], rs_oob_acc[i], rs_rec[i], rs_pre[i], rs_f1[i] = [], [], [], [], []\n    sm_acc[i], sm_oob_acc[i], sm_rec[i], sm_pre[i], sm_f1[i] = [], [], [], [], []\n    no_rs_acc[i], no_rs_oob_acc[i], no_rs_rec[i], no_rs_pre[i], no_rs_f1[i] = [], [], [], [], []\n    w_acc[i], w_oob_acc[i], w_rec[i], w_pre[i], w_f1[i] = [], [], [], [], []\n\n    for d in depth: \n        # random oversampling\n        rf = RandomForestClassifier(n_estimators=n, criterion='gini', max_depth=d, max_features=max_features,\n                                    oob_score=True, random_state=42, class_weight=None)\n        \n        rf.fit(ho_train_df_rs[all_features], ho_train_df_rs['DEATH_EVENT']);\n        pred = rf.predict(ho_val_df[all_features]);\n        rs_acc[i].append(accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred))\n        rs_oob_acc[i].append(rf.oob_score_)\n        rs_rec[i].append(recall_score(ho_val_df[\"DEATH_EVENT\"], pred))\n        rs_pre[i].append(precision_score(ho_val_df[\"DEATH_EVENT\"], pred))\n        rs_f1[i].append(f1_score(ho_val_df[\"DEATH_EVENT\"], pred))\n\n        # SMOTE\n        rf = RandomForestClassifier(n_estimators=n, criterion='gini', max_depth=d, max_features=max_features,\n                                    oob_score=True, random_state=42, class_weight=None)\n\n        rf.fit(ho_train_df_sm[all_features], ho_train_df_sm['DEATH_EVENT']);\n        pred = rf.predict(ho_val_df[all_features]);\n        sm_acc[i].append(accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred))\n        sm_oob_acc[i].append(rf.oob_score_)\n        sm_rec[i].append(recall_score(ho_val_df[\"DEATH_EVENT\"], pred))\n        sm_pre[i].append(precision_score(ho_val_df[\"DEATH_EVENT\"], pred))\n        sm_f1[i].append(f1_score(ho_val_df[\"DEATH_EVENT\"], pred))\n        \n        # no random oversampling\n        rf = RandomForestClassifier(n_estimators=n, criterion='gini', max_depth=d, max_features=max_features,\n                                    oob_score=True, random_state=42, class_weight=None)\n\n        rf.fit(ho_train_df[all_features], ho_train_df['DEATH_EVENT']);\n        pred = rf.predict(ho_val_df[all_features]);\n        no_rs_acc[i].append(accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred))\n        no_rs_oob_acc[i].append(rf.oob_score_)\n        no_rs_rec[i].append(recall_score(ho_val_df[\"DEATH_EVENT\"], pred))\n        no_rs_pre[i].append(precision_score(ho_val_df[\"DEATH_EVENT\"], pred))\n        no_rs_f1[i].append(f1_score(ho_val_df[\"DEATH_EVENT\"], pred))\n\n        # class weight\n        rf = RandomForestClassifier(n_estimators=n, criterion='gini', max_depth=d, max_features=max_features,\n                                    oob_score=True, random_state=42, class_weight='balanced')\n\n        rf.fit(ho_train_df[all_features], ho_train_df['DEATH_EVENT']);\n        pred = rf.predict(ho_val_df[all_features]);\n        w_acc[i].append(accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred))\n        w_oob_acc[i].append(rf.oob_score_)\n        w_rec[i].append(recall_score(ho_val_df[\"DEATH_EVENT\"], pred))\n        w_pre[i].append(precision_score(ho_val_df[\"DEATH_EVENT\"], pred))\n        w_f1[i].append(f1_score(ho_val_df[\"DEATH_EVENT\"], pred))","e16a57e6":"%matplotlib notebook\nplt.ioff()\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2);\n%matplotlib inline \nplt.ion()","45d18c78":"fig.set_size_inches(7, 7);\ndef animate_func(i):  \n    # accuracy\n    ax1.clear();\n    ax1.plot(list(depth), rs_acc[i], label=\"random oversampling\", linewidth=1.5, marker='o', markersize=3);\n    ax1.plot(list(depth), sm_acc[i], label=\"SMOTE\", linewidth=1.5, marker='o', markersize=3);\n    ax1.plot(list(depth), no_rs_acc[i], label = \"original dataset\", linewidth=1.5, marker='o', markersize=3);\n    ax1.plot(list(depth), w_acc[i], label = \"class_weight balanced\", linewidth=1.5, marker='o', markersize=3);\n\n    ax1.plot(list(depth), rs_oob_acc[i], label=\"random oversampling OOB\", linewidth=1, linestyle='dashed', alpha=0.8, color = \"#287CB7\");\n    ax1.plot(list(depth), sm_oob_acc[i], label=\"SMOTE OOB\", linewidth=1, linestyle='dashed', alpha=0.8, color = \"#FE8417\");\n    ax1.plot(list(depth), no_rs_oob_acc[i], label = \"original dataset OOB\", linewidth=1, linestyle='dashed', alpha=0.8, color = \"#34A334\");\n    ax1.plot(list(depth), w_oob_acc[i], label = \"class_weight balanced OOB\", linewidth=1, linestyle='dashed', alpha=0.8, color = \"#D62728\");\n\n    ax1.legend(fontsize=7, loc=3);\n    ax1.grid(True);\n    ax1.set_ylim(0, 1);\n    ax1.set_ylabel(\"Validation accuracy\");\n    ax1.set_xlabel(\"Single tree depth\");\n    ax1.set_title(f\"Accuracy (n_trees: {n_trees[i]})\");\n\n    # precision\n    ax2.clear();\n    ax2.plot(list(depth), rs_pre[i], label=\"random oversampling\", marker='o', linewidth=1.5, markersize=3);\n    ax2.plot(list(depth), sm_pre[i], label=\"SMOTE\", marker='o', linewidth=1.5, markersize=3);\n    ax2.plot(list(depth), no_rs_pre[i], label = \"original dataset\", marker='o', linewidth=1.5, markersize=3);\n    ax2.plot(list(depth), w_pre[i], label = \"class_weight balanced\", marker='o', linewidth=1.5, markersize=3);\n\n    ax2.legend(fontsize=7, loc=3);\n    ax2.grid(True);\n    ax2.set_ylim(0, 1);\n    ax2.set_ylabel(\"Validation precision\");\n    ax2.set_xlabel(\"Single tree depth\");\n    ax2.set_title(f\"Precision (n_trees: {n_trees[i]})\");\n\n    # recall\n    ax3.clear();\n    ax3.plot(list(depth), rs_rec[i], label=\"random oversampling\", marker='o', linewidth=1.5, markersize=3);\n    ax3.plot(list(depth), sm_rec[i], label=\"SMOTE\", marker='o', linewidth=1.5, markersize=3);\n    ax3.plot(list(depth), no_rs_rec[i], label = \"original dataset\", marker='o', linewidth=1.5, markersize=3);\n    ax3.plot(list(depth), w_rec[i], label = \"class_weight balanced\", marker='o', linewidth=1.5, markersize=3);\n\n    ax3.legend(fontsize=7, loc=4);\n    ax3.grid(True);\n    ax3.set_ylim(0, 1);\n    ax3.set_ylabel(\"Validation recall\");\n    ax3.set_xlabel(\"Single tree depth\");\n    ax3.set_title(f\"Recall (n_trees: {n_trees[i]})\");\n\n    # f1 score\n    ax4.clear();\n    ax4.plot(list(depth), rs_f1[i], label=\"random oversampling\", marker='o', linewidth=1.5, markersize=3);\n    ax4.plot(list(depth), sm_f1[i], label=\"SMOTE\", marker='o', linewidth=1.5, markersize=3);\n    ax4.plot(list(depth), no_rs_f1[i], label = \"original dataset\", marker='o', linewidth=1.5, markersize=3);\n    ax4.plot(list(depth), w_f1[i], label = \"class_weight balanced\", marker='o', linewidth=1.5, markersize=3);\n\n    ax4.legend(fontsize=7, loc=4);\n    ax4.grid(True);\n    ax4.set_ylim(0, 1);\n    ax4.set_ylabel(\"Validation f1 score\");\n    ax4.set_xlabel(\"Single tree depth\");\n    ax4.set_title(f\"F1 score (n_trees: {n_trees[i]})\");\n    \n    fig.tight_layout()\n    return [fig];\n\nanim = animation.FuncAnimation(\n                               fig, \n                               animate_func, \n                               frames = n_vals,\n                               interval = 100, # in ms\n                               );\nHTML(anim.to_jshtml())","2c722867":"# KFOLD \n\nrs_acc, rs_oob_acc, rs_rec, rs_pre, rs_f1= [None]*n_vals, [None]*n_vals, [None]*n_vals, [None]*n_vals, [None]*n_vals\nsm_acc, sm_oob_acc, sm_rec, sm_pre, sm_f1 = [None]*n_vals, [None]*n_vals, [None]*n_vals, [None]*n_vals, [None]*n_vals\nw_acc, w_oob_acc, w_rec, w_pre, w_f1 = [None]*n_vals, [None]*n_vals, [None]*n_vals, [None]*n_vals, [None]*n_vals\n\nfor i,n in enumerate(n_trees):\n    \n    rs_acc[i], rs_oob_acc[i], rs_rec[i], rs_pre[i], rs_f1[i] = [], [], [], [], []\n    sm_acc[i], sm_oob_acc[i], sm_rec[i], sm_pre[i], sm_f1[i] = [], [], [], [], []\n    w_acc[i], w_oob_acc[i], w_rec[i], w_pre[i], w_f1[i] = [], [], [], [], []\n\n    for d in depth: \n        # random oversampling\n        rf = RandomForestClassifier(n_estimators=n, criterion='gini', max_depth=d, max_features=max_features,\n                                     random_state=42, class_weight=None)\n        \n        acc, rec, pre, f1 = kfold.fit_predict(rf, ho_train_df, ho_train_df['DEATH_EVENT'],\n                                                          resampling=\"oversampling\")\n        rf.fit(ho_train_df_rs[all_features], ho_train_df_rs['DEATH_EVENT']);\n        rs_acc[i].append(acc)\n        \n        rs_rec[i].append(rec)\n        rs_pre[i].append(pre)\n        rs_f1[i].append(f1)\n\n        # SMOTE\n        rf = RandomForestClassifier(n_estimators=n, criterion='gini', max_depth=d, max_features=max_features,\n                                    random_state=42, class_weight=None)\n\n        acc, rec, pre, f1 = kfold.fit_predict(rf, ho_train_df, ho_train_df['DEATH_EVENT'], \n                                               resampling=\"SMOTE\", cached=True)\n        sm_acc[i].append(acc)\n        sm_rec[i].append(rec)\n        sm_pre[i].append(pre)\n        sm_f1[i].append(f1)\n        \n        # no random oversampling\n        rf = RandomForestClassifier(n_estimators=n, criterion='gini', max_depth=d, max_features=max_features,\n                                     random_state=42, class_weight=None)\n\n        acc, rec, pre, f1 = kfold.fit_predict(rf, ho_train_df, ho_train_df['DEATH_EVENT'])\n        no_rs_acc[i].append(acc)\n        no_rs_rec[i].append(rec)\n        no_rs_pre[i].append(pre)\n        no_rs_f1[i].append(f1)\n\n        # class weight\n        rf = RandomForestClassifier(n_estimators=n, criterion='gini', max_depth=d, max_features=max_features,\n                                     random_state=42, class_weight='balanced')\n\n        acc, rec, pre, f1 = kfold.fit_predict(rf, ho_train_df, ho_train_df['DEATH_EVENT'])\n        w_acc[i].append(acc)\n        w_rec[i].append(rec)\n        w_pre[i].append(pre)\n        w_f1[i].append(f1)","b2286ba3":"#acc = [a for b in w_acc for a in b]\n#f1 =  [a for b in w_f1 for a in b]\n#sorted([(a, b) for a, b in zip(acc, f1)], key=lambda a: a[0]+a[1])","86786a18":"# holdout\n\nthreshold=0.5\n\n# oversampled \nlr = LinearRegression(fit_intercept=True)\nlr.fit(ho_train_df_rs[all_features], ho_train_df_rs['DEATH_EVENT'])\npred = lr.predict(ho_val_df[all_features])\nraw_pred_rs = pred.copy()\npred[pred>=0.5]=1\npred[pred<0.5]=0\nrs_acc=(accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred))\nrs_rec=(recall_score(ho_val_df[\"DEATH_EVENT\"], pred))\nrs_pre=(precision_score(ho_val_df[\"DEATH_EVENT\"], pred))\nrs_f1=(f1_score(ho_val_df[\"DEATH_EVENT\"], pred))\n    \n# SMOTE \nlr = LinearRegression(fit_intercept=True)\nlr.fit(ho_train_df_sm[all_features], ho_train_df_sm['DEATH_EVENT'])\npred = lr.predict(ho_val_df[all_features])\nraw_pred_sm = pred.copy()\npred[pred>=0.5]=1\npred[pred<0.5]=0\nsm_acc=(accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred))\nsm_rec=(recall_score(ho_val_df[\"DEATH_EVENT\"], pred))\nsm_pre=(precision_score(ho_val_df[\"DEATH_EVENT\"], pred))\nsm_f1=(f1_score(ho_val_df[\"DEATH_EVENT\"], pred))\n    \n# not oversampled\nlr = LinearRegression(fit_intercept=True)\nlr.fit(ho_train_df[all_features], ho_train_df['DEATH_EVENT'])\npred = lr.predict(ho_val_df[all_features])\nraw_pred_no_rs = pred.copy()\npred[pred>=0.5]=1\npred[pred<0.5]=0\nno_rs_acc=(accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred))\nno_rs_rec=(recall_score(ho_val_df[\"DEATH_EVENT\"], pred))\nno_rs_pre=(precision_score(ho_val_df[\"DEATH_EVENT\"], pred))\nno_rs_f1=(f1_score(ho_val_df[\"DEATH_EVENT\"], pred))","f08fc282":"# oversampled \nlr = LinearRegression(fit_intercept=True)\nrs_acc, rs_rec, rs_pre, rs_f1 = kfold.fit_predict(lr, ho_train_df, ho_train_df['DEATH_EVENT'], threshold=0.5, resampling=\"oversampling\")\n    \n# SMOTE \nlr = LinearRegression(fit_intercept=True)\nsm_acc, sm_rec, sm_pre, sm_f1 = kfold.fit_predict(lr, ho_train_df, ho_train_df['DEATH_EVENT'], threshold=0.5, resampling=\"SMOTE\", cached=True)\n    \n# not oversampled\nlr = LinearRegression(fit_intercept=True)\nno_rs_acc, no_rs_rec, no_rs_pre, no_rs_f1= kfold.fit_predict(lr, ho_train_df, ho_train_df['DEATH_EVENT'], threshold=0.5)","745e3a3f":"# https:\/\/python-graph-gallery.com\/11-grouped-barplot\/\n\nfig = plt.figure(figsize=(6, 4))\nbarWidth = 0.25\nspace=0.0\n \nbars1 = [no_rs_f1, no_rs_rec, no_rs_pre, no_rs_acc]\nbars2 = [rs_f1, rs_rec, rs_pre, rs_acc]\nbars3 = [sm_f1, sm_rec, sm_pre, sm_acc]\n\nr1 = np.arange(len(bars1))\nr2 = [x + barWidth + space for x in r1]\nr3 = [x + barWidth + space for x in r2]\n\nplt.barh(r3, bars1, label=\"Original\",height=barWidth, edgecolor='white')\nplt.barh(r2, bars2, label=\"Oversampling\", height=barWidth, edgecolor='white')\nplt.barh(r1, bars3, label=\"SMOTE\", height=barWidth, edgecolor='white')\n\nplt.title(\"Mean values on 5-Fold CV\")\nplt.yticks([r + barWidth for r in range(len(bars1))], [\"F1 score\", \"Recall\", \"Precision\", \"Accuracy\"])\nplt.xlim(0, 1)\nplt.gca().xaxis.grid(True, linestyle=':')\nplt.legend();","804b3103":"# area under the curve evaluation\ndef roc_area(tpr, fpr):\n    area = 0\n    for i in range(len(tpr)-1):\n        base = fpr[i+1]-fpr[i]\n        h = tpr[i]\n        area += base*h\n        \n    return round(area, 3)","e0c31fe4":"fpr_rs, tpr_rs, thresholds = roc_curve(ho_val_df[\"DEATH_EVENT\"], raw_pred_rs)\nfpr_no_rs, tpr_no_rs, thresholds = roc_curve(ho_val_df[\"DEATH_EVENT\"], raw_pred_no_rs)\nfpr_sm, tpr_sm, thresholds = roc_curve(ho_val_df[\"DEATH_EVENT\"], raw_pred_sm)\n\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 3, 1)\nplt.plot(fpr_no_rs, tpr_no_rs, label=f\"Original (area={roc_area(tpr_no_rs, fpr_no_rs)})\", linewidth=3, color='#990303')\nplt.grid()\nplt.ylim(0, 1)\nplt.xlim(0, 1)\nplt.legend(loc=4)\nplt.xlabel(\"False positive rate\")\nplt.ylabel(\"True positive rate\")\nplt.title(\"ROC curve [Original]\")\nplt.fill_between(fpr_no_rs, 0, tpr_no_rs, alpha=0.05, color='#990303')\n\nplt.subplot(1, 3, 2)\nplt.plot(fpr_rs, tpr_rs, label=f\"Oversampling (area={roc_area(tpr_rs, fpr_rs)})\", linewidth=3, color='#990303')\nplt.grid()\nplt.ylim(0, 1)\nplt.xlim(0, 1)\nplt.legend(loc=4)\nplt.xlabel(\"False positive rate\")\nplt.ylabel(\"True positive rate\")\nplt.title(\"ROC curve [Oversampling]\")\nplt.fill_between(fpr_rs, 0, tpr_rs, alpha=0.05, color='#990303')\n\nplt.subplot(1, 3, 3)\nplt.plot(fpr_sm, tpr_sm, label=f\"SMOTE (area={roc_area(tpr_sm, fpr_sm)})\", linewidth=3, color='#990303')\nplt.grid()\nplt.ylim(0, 1)\nplt.xlim(0, 1)\nplt.legend(loc=4)\nplt.xlabel(\"False positive rate\")\nplt.ylabel(\"True positive rate\")\nplt.title(\"ROC curve [SMOTE]\")\nplt.fill_between(fpr_sm, 0, tpr_sm, alpha=0.05, color='#990303');\n\nplt.tight_layout()","907c5ef1":"c = 1\npenalty = \"l2\"\nsolver = \"liblinear\"\nmulti_class = \"auto\"\n\n# oversampled\nlr = LogisticRegression(penalty=penalty, C=c, fit_intercept=True, class_weight=None,\n                   random_state=42, solver=solver, multi_class=multi_class)\n\nlr.fit(ho_train_df_rs[all_features], ho_train_df_rs['DEATH_EVENT'])\npred = lr.predict(ho_val_df[all_features])\nraw_pred_rs = lr.predict_proba(ho_val_df[all_features])[:, 1]\n\nrs_acc=(accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred))\nrs_rec=(recall_score(ho_val_df[\"DEATH_EVENT\"], pred))\nrs_pre=(precision_score(ho_val_df[\"DEATH_EVENT\"], pred))\nrs_f1=(f1_score(ho_val_df[\"DEATH_EVENT\"], pred))\n    \n# SMOTE \nlr = LogisticRegression(penalty=penalty, C=c, fit_intercept=True, class_weight=None,\n                   random_state=42, solver=solver, multi_class=multi_class)\n\nlr.fit(ho_train_df_sm[all_features], ho_train_df_sm['DEATH_EVENT'])\npred = lr.predict(ho_val_df[all_features])\nraw_pred_sm = lr.predict_proba(ho_val_df[all_features])[:, 1]\n\nsm_acc=(accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred))\nsm_rec=(recall_score(ho_val_df[\"DEATH_EVENT\"], pred))\nsm_pre=(precision_score(ho_val_df[\"DEATH_EVENT\"], pred))\nsm_f1=(f1_score(ho_val_df[\"DEATH_EVENT\"], pred))\n    \n# not oversampled\nlr = LogisticRegression(penalty=penalty, C=c, fit_intercept=True, class_weight=None,\n                   random_state=42, solver=solver, multi_class=multi_class)\n\nlr.fit(ho_train_df[all_features], ho_train_df['DEATH_EVENT'])\npred = lr.predict(ho_val_df[all_features])\nraw_pred_no_rs = lr.predict_proba(ho_val_df[all_features])[:, 1]\n\nno_rs_acc=(accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred))\nno_rs_rec=(recall_score(ho_val_df[\"DEATH_EVENT\"], pred))\nno_rs_pre=(precision_score(ho_val_df[\"DEATH_EVENT\"], pred))\nno_rs_f1=(f1_score(ho_val_df[\"DEATH_EVENT\"], pred))\n\n# class-weight\nlr = LogisticRegression(penalty=penalty, C=c, fit_intercept=True, class_weight=\"balanced\",\n                   random_state=42, solver=solver, multi_class=multi_class)\n\nlr.fit(ho_train_df[all_features], ho_train_df['DEATH_EVENT'])\npred = lr.predict(ho_val_df[all_features])\nraw_pred_w = lr.predict_proba(ho_val_df[all_features])[:, 1]\n\nw_acc=(accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred))\nw_rec=(recall_score(ho_val_df[\"DEATH_EVENT\"], pred))\nw_pre=(precision_score(ho_val_df[\"DEATH_EVENT\"], pred))\nw_f1=(f1_score(ho_val_df[\"DEATH_EVENT\"], pred))","18d99c39":"# oversampled\nlr = LogisticRegression(penalty=penalty, C=c, fit_intercept=True, class_weight=None,\n                   random_state=42, solver=solver, multi_class=multi_class)\n\nrs_acc, rs_rec, rs_pre, rs_f1 = kfold.fit_predict(lr, ho_train_df, ho_train_df['DEATH_EVENT'],\n                                                  threshold=0.5, resampling=\"oversampling\")\n    \n# SMOTE \nlr = LogisticRegression(penalty=penalty, C=c, fit_intercept=True, class_weight=None,\n                   random_state=42, solver=solver, multi_class=multi_class)\n\nsm_acc, sm_rec, sm_pre, sm_f1= kfold.fit_predict(lr, ho_train_df, ho_train_df['DEATH_EVENT'],\n                                                 threshold=0.5, resampling=\"SMOTE\", cached=True)\n    \n# not oversampled\nlr = LogisticRegression(penalty=penalty, C=c, fit_intercept=True, class_weight=None,\n                   random_state=42, solver=solver, multi_class=multi_class)\n\nno_rs_acc, no_rs_rec, no_rs_pre, no_rs_f1= kfold.fit_predict(lr, ho_train_df, ho_train_df['DEATH_EVENT'], threshold=0.5)\n\n# class-weight\nlr = LogisticRegression(penalty=penalty, C=c, fit_intercept=True, class_weight=\"balanced\",\n                   random_state=42, solver=solver, multi_class=multi_class)\n\nw_acc, w_rec, w_pre, w_f1= kfold.fit_predict(lr, ho_train_df, ho_train_df['DEATH_EVENT'], threshold=0.5)","2eafec22":"'''\nprint(f\"no_rs:{no_rs_acc},{no_rs_f1}\")\nprint(f\"rs:{rs_acc},{rs_f1}\")\nprint(f\"sm:{sm_acc},{sm_f1}\")\nprint(f\"w:{w_acc},{w_f1}\")\n''';","fbc0b5b7":"# https:\/\/python-graph-gallery.com\/11-grouped-barplot\/\n\nfig = plt.figure(figsize=(6, 4))\nbarWidth = 0.2\nspace=0.01\n \nbars1 = [no_rs_f1, no_rs_rec, no_rs_pre, no_rs_acc]\nbars2 = [rs_f1, rs_rec, rs_pre, rs_acc]\nbars3 = [sm_f1, sm_rec, sm_pre, sm_acc]\nbars4 = [w_f1, w_rec, w_pre, w_acc]\n\nr1 = np.arange(len(bars1))\nr2 = [x + barWidth + space for x in r1]\nr3 = [x + barWidth + space for x in r2]\nr4 = [x + barWidth + space for x in r3]\n\nplt.barh(r4, bars1, label=\"Original\",height=barWidth, edgecolor='white', )\nplt.barh(r3, bars2, label=\"Oversampling\", height=barWidth, edgecolor='white',)\nplt.barh(r2, bars3, label=\"SMOTE\", height=barWidth, edgecolor='white', )\nplt.barh(r1, bars4, label=\"class-weight\", height=barWidth, edgecolor='white', )\n\nplt.title(\"Mean values on 5-Fold CV\")\nplt.yticks([r + barWidth*1.5 for r in range(len(bars1))], [\"F1 score\", \"Recall\", \"Precision\", \"Accuracy\", ])\nplt.xlim(0, 1)\nplt.gca().xaxis.grid(True, linestyle=':')\nplt.legend();","e5e9ccf2":"fpr_rs, tpr_rs, thresholds = roc_curve(ho_val_df[\"DEATH_EVENT\"], raw_pred_rs)\nfpr_no_rs, tpr_no_rs, thresholds = roc_curve(ho_val_df[\"DEATH_EVENT\"], raw_pred_no_rs)\nfpr_sm, tpr_sm, thresholds = roc_curve(ho_val_df[\"DEATH_EVENT\"], raw_pred_sm)\nfpr_w, tpr_w, thresholds = roc_curve(ho_val_df[\"DEATH_EVENT\"], raw_pred_w)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(2, 2, 1)\nplt.plot(fpr_no_rs, tpr_no_rs, label=f\"Original (area={roc_area(tpr_no_rs, fpr_no_rs)})\", linewidth=3, color='#990303')\nplt.grid()\nplt.ylim(0, 1)\nplt.xlim(0, 1)\nplt.legend(loc=4)\nplt.xlabel(\"False positive rate\")\nplt.ylabel(\"True positive rate\")\nplt.title(\"ROC curve [Original]\")\nplt.fill_between(fpr_no_rs, 0, tpr_no_rs, alpha=0.05, color='#990303')\n\nplt.subplot(2, 2, 2)\nplt.plot(fpr_rs, tpr_rs, label=f\"Oversampling (area={roc_area(tpr_rs, fpr_rs)})\", linewidth=3, color='#990303')\nplt.grid()\nplt.ylim(0, 1)\nplt.xlim(0, 1)\nplt.legend(loc=4)\nplt.xlabel(\"False positive rate\")\nplt.ylabel(\"True positive rate\")\nplt.title(\"ROC curve [Oversampling]\")\nplt.fill_between(fpr_rs, 0, tpr_rs, alpha=0.05, color='#990303')\n\nplt.subplot(2, 2, 3)\nplt.plot(fpr_sm, tpr_sm, label=f\"SMOTE (area={roc_area(tpr_sm, fpr_sm)})\", linewidth=3, color='#990303')\nplt.grid()\nplt.ylim(0, 1)\nplt.xlim(0, 1)\nplt.legend(loc=4)\nplt.xlabel(\"False positive rate\")\nplt.ylabel(\"True positive rate\")\nplt.title(\"ROC curve [SMOTE]\")\nplt.fill_between(fpr_sm, 0, tpr_sm, alpha=0.05, color='#990303')\n\nplt.subplot(2, 2, 4)\nplt.plot(fpr_w, tpr_w, label=f\"Class-weight (area={roc_area(tpr_w, fpr_w)})\", linewidth=3, color='#990303')\nplt.grid()\nplt.ylim(0, 1)\nplt.xlim(0, 1)\nplt.legend(loc=4)\nplt.xlabel(\"False positive rate\")\nplt.ylabel(\"True positive rate\")\nplt.title(\"ROC curve [class-weight balanced]\")\nplt.fill_between(fpr_w, 0, tpr_w, alpha=0.05, color='#990303');\n\nplt.tight_layout()","ff133d9e":"def make_meshgrid(x, y, h=.02):\n    x_min, x_max = x.min() - 1, x.max() + 1\n    y_min, y_max = y.min() - 1, y.max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    return xx, yy\n\ndef plot_contours(ax, clf, xx, yy, **params):\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    out = ax.contourf(xx, yy, Z, **params)\n    return out","d8c431c0":"%matplotlib notebook\nplt.ioff()\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3);\n%matplotlib inline \nplt.ion()\n\nC = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\nh = 0.03\nx_min, x_max = hf_norm[\"ejection_fraction\"].min() - .5, hf_norm[\"ejection_fraction\"].max() + .5\ny_min, y_max = hf_norm[\"serum_creatinine\"].min() - .5, hf_norm[\"serum_creatinine\"].max() + .5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                        np.arange(y_min, y_max, h))\n\nfig.set_size_inches(11, 4);\ndef animate_func(i):  \n    # linear\n    svm = SVC(C=C[i], kernel='linear', random_state=42, class_weight='balanced', probability=True)\n    svm.fit(hf_norm[[\"ejection_fraction\", \"serum_creatinine\"]], hf_norm['DEATH_EVENT'])\n\n    Z = svm.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z[:, 1].reshape(xx.shape)\n    \n    ax1.clear()\n    ax1.contourf(xx, yy, Z, alpha=0.7, cmap=cm_rev, antialiased=True)\n    ax1.scatter(hf_norm[hf_norm[\"DEATH_EVENT\"]==0][\"ejection_fraction\"],\n                hf_norm[hf_norm[\"DEATH_EVENT\"]==0][\"serum_creatinine\"], label=\"alive\", color=\"#990303\", edgecolor='BLACK')\n    ax1.scatter(hf_norm[hf_norm[\"DEATH_EVENT\"]==1][\"ejection_fraction\"],\n                hf_norm[hf_norm[\"DEATH_EVENT\"]==1][\"serum_creatinine\"], label=\"dead\", color=\"#9C9999\", edgecolor='BLACK')\n\n    ax1.set_xlabel(\"ejection_fraction\")\n    ax1.set_ylabel(\"serum_creatinine\")\n    ax1.legend();\n    ax1.set_title(f\"Linear (C={C[i]})\")\n    \n    # poly\n    svm = SVC(C=C[i], kernel='poly', random_state=42, gamma='auto', degree=3, class_weight='balanced', probability=True)\n    svm.fit(hf_norm[[\"ejection_fraction\", \"serum_creatinine\"]], hf_norm['DEATH_EVENT'])\n\n    Z = svm.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z[:, 1].reshape(xx.shape)\n\n    ax2.clear()\n    ax2.contourf(xx, yy, Z, alpha=0.7, cmap=cm_rev, antialiased=True)\n    ax2.scatter(hf_norm[hf_norm[\"DEATH_EVENT\"]==0][\"ejection_fraction\"],\n                hf_norm[hf_norm[\"DEATH_EVENT\"]==0][\"serum_creatinine\"], label=\"alive\", color=\"#990303\", edgecolor='BLACK')\n    ax2.scatter(hf_norm[hf_norm[\"DEATH_EVENT\"]==1][\"ejection_fraction\"],\n                hf_norm[hf_norm[\"DEATH_EVENT\"]==1][\"serum_creatinine\"], label=\"dead\", color=\"#9C9999\", edgecolor='BLACK')\n\n    ax2.set_xlabel(\"ejection_fraction\")\n    ax2.set_ylabel(\"serum_creatinine\")\n    ax2.legend();\n    ax2.set_title(f\"Polynomial (C={C[i]})\")\n\n    # rbf\n    svm = SVC(C=C[i], kernel='rbf', random_state=42, gamma='auto', class_weight='balanced', probability=True)\n    svm.fit(hf_norm[[\"ejection_fraction\", \"serum_creatinine\"]], hf_norm['DEATH_EVENT'])\n\n    Z = svm.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z[:, 1].reshape(xx.shape)\n\n    ax3.clear()\n    ax3.contourf(xx, yy, Z, alpha=0.7, cmap=cm_rev, antialiased=True)\n    \n    ax3.scatter(hf_norm[hf_norm[\"DEATH_EVENT\"]==0][\"ejection_fraction\"],\n                hf_norm[hf_norm[\"DEATH_EVENT\"]==0][\"serum_creatinine\"], label=\"alive\", color=\"#990303\", edgecolor='BLACK')\n    ax3.scatter(hf_norm[hf_norm[\"DEATH_EVENT\"]==1][\"ejection_fraction\"],\n                hf_norm[hf_norm[\"DEATH_EVENT\"]==1][\"serum_creatinine\"], label=\"dead\", color=\"#9C9999\", edgecolor='BLACK')\n\n    ax3.set_xlabel(\"ejection_fraction\")\n    ax3.set_ylabel(\"serum_creatinine\")\n    ax3.legend();\n    ax3.set_title(f\"Radial Basis Function (C={C[i]})\")\n    \n    fig.tight_layout()\n    return [fig]\n\nanim = animation.FuncAnimation(\n                               fig, \n                               animate_func, \n                               frames = len(C),\n                               interval = 500, # in ms\n                               );\nHTML(anim.to_jshtml())","d2902038":"C = 1\n\n# original\nsvm = SVC(kernel=\"linear\", random_state=42, probability=True, C = C)\nsvm.fit(ho_train_df[all_features], ho_train_df['DEATH_EVENT'])\n\npred = svm.predict(ho_val_df[all_features])\nlin_raw_pred_no_rs = svm.predict_proba(ho_val_df[all_features])[:, 1]\n\nlin_no_rs_acc = accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred)\nlin_no_rs_pre = recall_score(ho_val_df[\"DEATH_EVENT\"], pred)\nlin_no_rs_rec = precision_score(ho_val_df[\"DEATH_EVENT\"], pred)\nlin_no_rs_f1 = f1_score(ho_val_df[\"DEATH_EVENT\"], pred)\n\n# oversampled\nsvm = SVC(kernel=\"linear\", random_state=42, probability=True, C = C)\nsvm.fit(ho_train_df_rs[all_features], ho_train_df_rs['DEATH_EVENT'])\n\npred = svm.predict(ho_val_df[all_features])\nlin_raw_pred_rs = svm.predict_proba(ho_val_df[all_features])[:, 1]\n\nlin_rs_acc = accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred)\nlin_rs_pre = recall_score(ho_val_df[\"DEATH_EVENT\"], pred)\nlin_rs_rec = precision_score(ho_val_df[\"DEATH_EVENT\"], pred)\nlin_rs_f1 = f1_score(ho_val_df[\"DEATH_EVENT\"], pred)\n\n# SMOTE\nsvm = SVC(kernel=\"linear\", random_state=42, probability=True, C = C)\nsvm.fit(ho_train_df_sm[all_features], ho_train_df_sm['DEATH_EVENT'])\n\npred = svm.predict(ho_val_df[all_features])\nlin_raw_pred_sm = svm.predict_proba(ho_val_df[all_features])[:, 1]\n\nlin_sm_acc = accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred)\nlin_sm_pre = recall_score(ho_val_df[\"DEATH_EVENT\"], pred)\nlin_sm_rec = precision_score(ho_val_df[\"DEATH_EVENT\"], pred)\nlin_sm_f1 = f1_score(ho_val_df[\"DEATH_EVENT\"], pred)\n\n# class-weight balanced\nsvm = SVC(kernel=\"linear\", random_state=42, probability=True, class_weight=\"balanced\", C = C)\nsvm.fit(ho_train_df[all_features], ho_train_df['DEATH_EVENT'])\n\npred = svm.predict(ho_val_df[all_features])\nlin_raw_pred_w = svm.predict_proba(ho_val_df[all_features])[:, 1]\n\nlin_w_acc = accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred)\nlin_w_pre = recall_score(ho_val_df[\"DEATH_EVENT\"], pred)\nlin_w_rec = precision_score(ho_val_df[\"DEATH_EVENT\"], pred)\nlin_w_f1 = f1_score(ho_val_df[\"DEATH_EVENT\"], pred)","043ce2fc":"'''\nprint(f\"no_rs:{lin_no_rs_acc},{lin_no_rs_f1}\")\nprint(f\"rs:{lin_rs_acc},{lin_rs_f1}\")\nprint(f\"sm:{lin_sm_acc},{lin_sm_f1}\")\nprint(f\"w:{lin_w_acc},{lin_w_f1}\")\n''';","6eaf8de4":"# original\nsvm = SVC(kernel=\"linear\", random_state=42, probability=True, C = C)\nlin_no_rs_acc, lin_no_rs_pre, lin_no_rs_rec, lin_no_rs_f1 = kfold.fit_predict(svm, ho_train_df, ho_train_df['DEATH_EVENT'], threshold=0.5, resampling=\"oversampling\")\n\n# oversampled\nsvm = SVC(kernel=\"linear\", random_state=42, probability=True, C = C)\nlin_rs_acc, lin_rs_pre, lin_rs_rec, lin_rs_f1 = kfold.fit_predict(svm, ho_train_df, ho_train_df['DEATH_EVENT'], threshold=0.5, resampling=\"SMOTE\", cached=True)\n\n# SMOTE\nsvm = SVC(kernel=\"linear\", random_state=42, probability=True, C = C)\nlin_sm_acc, lin_sm_pre, lin_sm_rec, lin_sm_f1 = kfold.fit_predict(svm, ho_train_df, ho_train_df['DEATH_EVENT'], threshold=0.5)\n\n# class-weight balanced\nsvm = SVC(kernel=\"linear\", random_state=42, probability=True, class_weight=\"balanced\", C = C)\nlin_w_acc, lin_w_pre, lin_w_rec, lin_w_f1 = kfold.fit_predict(svm, ho_train_df, ho_train_df['DEATH_EVENT'], threshold=0.5)","cd4ef495":"'''\nprint(f\"no_rs:{lin_no_rs_acc},{lin_no_rs_f1}\")\nprint(f\"rs:{lin_rs_acc},{lin_rs_f1}\")\nprint(f\"sm:{lin_sm_acc},{lin_sm_f1}\")\nprint(f\"w:{lin_w_acc},{lin_w_f1}\")\n''';","263ea117":"# https:\/\/python-graph-gallery.com\/11-grouped-barplot\/\n\nfig = plt.figure(figsize=(6, 4))\nbarWidth = 0.2\nspace=0.01\n \nbars1 = [lin_no_rs_f1, lin_no_rs_rec, lin_no_rs_pre, lin_no_rs_acc]\nbars2 = [lin_rs_f1, lin_rs_rec, lin_rs_pre, lin_rs_acc]\nbars3 = [lin_sm_f1, lin_sm_rec, lin_sm_pre, lin_sm_acc]\nbars4 = [lin_w_f1, lin_w_rec, lin_w_pre, lin_w_acc]\n\nr1 = np.arange(len(bars1))\nr2 = [x + barWidth + space for x in r1]\nr3 = [x + barWidth + space for x in r2]\nr4 = [x + barWidth + space for x in r3]\n\nplt.barh(r4, bars1, label=\"Original\",height=barWidth, edgecolor='white')\nplt.barh(r3, bars2, label=\"Oversampling\", height=barWidth, edgecolor='white')\nplt.barh(r2, bars3, label=\"SMOTE\", height=barWidth, edgecolor='white')\nplt.barh(r1, bars4, label=\"class-weight\", height=barWidth, edgecolor='white')\n\nplt.title(\"Mean values on 5-Fold CV\")\nplt.yticks([r + barWidth*1.5 for r in range(len(bars1))], [\"F1 score\", \"Recall\", \"Precision\", \"Accuracy\", ])\nplt.xlim(0, 1)\nplt.gca().xaxis.grid(True, linestyle=':')\nplt.legend();","e53966d2":"from sklearn.metrics import roc_curve\n\nfpr_rs, tpr_rs, thresholds = roc_curve(ho_val_df[\"DEATH_EVENT\"], lin_raw_pred_rs)\nfpr_no_rs, tpr_no_rs, thresholds = roc_curve(ho_val_df[\"DEATH_EVENT\"], lin_raw_pred_no_rs)\nfpr_sm, tpr_sm, thresholds = roc_curve(ho_val_df[\"DEATH_EVENT\"], lin_raw_pred_sm)\nfpr_w, tpr_w, thresholds = roc_curve(ho_val_df[\"DEATH_EVENT\"], lin_raw_pred_w)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(2, 2, 1)\nplt.plot(fpr_no_rs, tpr_no_rs, label=f\"Original (area={roc_area(tpr_no_rs, fpr_no_rs)})\", linewidth=3, color='#990303')\nplt.grid()\nplt.ylim(0, 1)\nplt.xlim(0, 1)\nplt.legend(loc=4)\nplt.xlabel(\"False positive rate\")\nplt.ylabel(\"True positive rate\")\nplt.title(\"ROC curve [Original]\")\nplt.fill_between(fpr_no_rs, 0, tpr_no_rs, alpha=0.05, color='#990303')\n\nplt.subplot(2, 2, 2)\nplt.plot(fpr_rs, tpr_rs, label=f\"Oversampling (area={roc_area(tpr_rs, fpr_rs)})\", linewidth=3, color='#990303')\nplt.grid()\nplt.ylim(0, 1)\nplt.xlim(0, 1)\nplt.legend(loc=4)\nplt.xlabel(\"False positive rate\")\nplt.ylabel(\"True positive rate\")\nplt.title(\"ROC curve [Oversampling]\")\nplt.fill_between(fpr_rs, 0, tpr_rs, alpha=0.05, color='#990303')\n\nplt.subplot(2, 2, 3)\nplt.plot(fpr_sm, tpr_sm, label=f\"SMOTE (area={roc_area(tpr_sm, fpr_sm)})\", linewidth=3, color='#990303')\nplt.grid()\nplt.ylim(0, 1)\nplt.xlim(0, 1)\nplt.legend(loc=4)\nplt.xlabel(\"False positive rate\")\nplt.ylabel(\"True positive rate\")\nplt.title(\"ROC curve [SMOTE]\")\nplt.fill_between(fpr_sm, 0, tpr_sm, alpha=0.05, color='#990303')\n\nplt.subplot(2, 2, 4)\nplt.plot(fpr_sm, tpr_sm, label=f\"class-weight (area={roc_area(tpr_w, fpr_w)})\", linewidth=3, color='#990303')\nplt.grid()\nplt.ylim(0, 1)\nplt.xlim(0, 1)\nplt.legend(loc=4)\nplt.xlabel(\"False positive rate\")\nplt.ylabel(\"True positive rate\")\nplt.title(\"ROC curve [class-weight='balanced']\")\nplt.fill_between(fpr_w, 0, tpr_w, alpha=0.05, color='#990303')\n\nplt.tight_layout()","dcd78de2":"def _make_combinations(lenghts, n, comb, combs):\n    if len(lenghts)==n:\n        combs.append(comb.copy())\n    else:\n        for i in range(lenghts[n]):\n            combs = _make_combinations(lenghts, n+1, comb+[i], combs)\n    return combs\n\ndef make_combinations(lenghts):\n    combs = []\n    combs = _make_combinations(lenghts, 0, [], combs)\n    return combs","0565907e":"def gridsearch(model, train_set, train_lab, params, starting_conf, resampling = None):\n    \n    results = []\n    lenghts = [len(params[l]) for l in params.keys()]\n    combs = make_combinations(lenghts)\n    n_params = len(params.keys())\n\n    for comb in combs:\n        conf = starting_conf.copy()\n        for i, param in enumerate(params.keys()):\n            conf[param]=params[param][comb[i]]\n        \n        m = model(**conf)\n        \n        acc, pre, rec, f1 = kfold.fit_predict(model=m, X = train_set, y = train_lab, resampling=resampling, cached=True)\n        results.append([acc, f1, conf])\n    return np.asarray(results)  ","d416b6ef":"variable_params = {\n    \"C\": [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n    \"gamma\": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n}","f8e78b70":"# Original dataset\nplt.figure(figsize=(9, 8))\n\n# poly 3 degree\nplt.subplot(2, 2, 1)\n\nstarting_params = {\"class_weight\":None, \"random_state\": 42, \"kernel\":\"poly\" , \"degree\": 3, 'max_iter':100000}\nresults = gridsearch(SVC, ho_train_df, ho_train_df['DEATH_EVENT'], variable_params, starting_params)\n\nmatrix_res = results[:, 0].reshape(len(variable_params[\"C\"]), len(variable_params[\"gamma\"]))\nmatrix_res = matrix_res.astype(np.float64)\n\nplt.tight_layout()\nax = sns.heatmap(matrix_res, annot=True, cmap=\"viridis\")\nplt.xlabel(\"gamma\")\nplt.ylabel(\"C\")\nplt.title(\"Polynomial (degree: 3) [Original] accuracy\")\nax.set_xticklabels(variable_params[\"gamma\"])\nax.set_yticklabels(variable_params[\"C\"])\n\nplt.subplot(2, 2, 2)\n\nmatrix_res = results[:, 1].reshape(len(variable_params[\"C\"]), len(variable_params[\"gamma\"]))\nmatrix_res = matrix_res.astype(np.float64)\n\nplt.tight_layout()\nax = sns.heatmap(matrix_res, annot=True, cmap=\"inferno\")\nplt.xlabel(\"gamma\")\nplt.ylabel(\"C\")\nplt.title(\"Polynomial (degree: 3) [Original] f1 score\")\nax.set_xticklabels(variable_params[\"gamma\"])\nax.set_yticklabels(variable_params[\"C\"])\n\n# rbf\nplt.subplot(2, 2, 3)\n\nstarting_params = {\"class_weight\":None, \"random_state\": 42, \"kernel\":\"rbf\", 'max_iter':100000}\nresults = gridsearch(SVC, ho_train_df, ho_train_df['DEATH_EVENT'], variable_params, starting_params)\n\nmatrix_res = results[:, 0].reshape(len(variable_params[\"C\"]), len(variable_params[\"gamma\"]))\nmatrix_res = matrix_res.astype(np.float64)\n\nplt.tight_layout()\nax = sns.heatmap(matrix_res, annot=True, cmap=\"viridis\")\nplt.xlabel(\"gamma\")\nplt.ylabel(\"C\")\nplt.title(\"RBF [Original] accuracy\")\nax.set_xticklabels(variable_params[\"gamma\"])\nax.set_yticklabels(variable_params[\"C\"]);\n\nplt.subplot(2, 2, 4)\n\nmatrix_res = results[:, 1].reshape(len(variable_params[\"C\"]), len(variable_params[\"gamma\"]))\nmatrix_res = matrix_res.astype(np.float64)\n\nplt.tight_layout()\nax = sns.heatmap(matrix_res, annot=True, cmap=\"inferno\")\nplt.xlabel(\"gamma\")\nplt.ylabel(\"C\")\nplt.title(\"RBF [Original] f1 score\")\nax.set_xticklabels(variable_params[\"gamma\"])\nax.set_yticklabels(variable_params[\"C\"]);","136d7e65":"# random oversampling dataset\nplt.figure(figsize=(9, 8))\n\n# poly 3 degree\nplt.subplot(2, 2, 1)\n\nstarting_params = {\"class_weight\":None, \"random_state\": 42, \"kernel\":\"poly\" , \"degree\": 3, 'max_iter':100000}\nresults = gridsearch(SVC, ho_train_df, ho_train_df['DEATH_EVENT'], variable_params, \n                     starting_params, resampling=\"oversampling\")\n\nmatrix_res = results[:, 0].reshape(len(variable_params[\"C\"]), len(variable_params[\"gamma\"]))\nmatrix_res = matrix_res.astype(np.float64)\n\nplt.tight_layout()\nax = sns.heatmap(matrix_res, annot=True, cmap=\"viridis\")\nplt.xlabel(\"gamma\")\nplt.ylabel(\"C\")\nplt.title(\"Polynomial (degree: 3) [Oversampling] accuracy\")\nax.set_xticklabels(variable_params[\"gamma\"])\nax.set_yticklabels(variable_params[\"C\"])\n\nplt.subplot(2, 2, 2)\n\nmatrix_res = results[:, 1].reshape(len(variable_params[\"C\"]), len(variable_params[\"gamma\"]))\nmatrix_res = matrix_res.astype(np.float64)\n\nplt.tight_layout()\nax = sns.heatmap(matrix_res, annot=True, cmap=\"inferno\")\nplt.xlabel(\"gamma\")\nplt.ylabel(\"C\")\nplt.title(\"Polynomial (degree: 3) [Oversampling] f1 score\")\nax.set_xticklabels(variable_params[\"gamma\"])\nax.set_yticklabels(variable_params[\"C\"])\n\n# rbf\nplt.subplot(2, 2, 3)\n\nstarting_params = {\"class_weight\":None, \"random_state\": 42, \"kernel\":\"rbf\", 'max_iter':100000}\nresults = gridsearch(SVC, ho_train_df, ho_train_df['DEATH_EVENT'], variable_params,\n                     starting_params, resampling=\"oversampling\")\n\nmatrix_res = results[:, 0].reshape(len(variable_params[\"C\"]), len(variable_params[\"gamma\"]))\nmatrix_res = matrix_res.astype(np.float64)\n\nplt.tight_layout()\nax = sns.heatmap(matrix_res, annot=True, cmap=\"viridis\")\nplt.xlabel(\"gamma\")\nplt.ylabel(\"C\")\nplt.title(\"RBF [Oversampling] accuracy\")\nax.set_xticklabels(variable_params[\"gamma\"])\nax.set_yticklabels(variable_params[\"C\"]);\n\nplt.subplot(2, 2, 4)\n\nmatrix_res = results[:, 1].reshape(len(variable_params[\"C\"]), len(variable_params[\"gamma\"]))\nmatrix_res = matrix_res.astype(np.float64)\n\nplt.tight_layout()\nax = sns.heatmap(matrix_res, annot=True, cmap=\"inferno\")\nplt.xlabel(\"gamma\")\nplt.ylabel(\"C\")\nplt.title(\"RBF [Oversampling] f1 score\")\nax.set_xticklabels(variable_params[\"gamma\"])\nax.set_yticklabels(variable_params[\"C\"]);","9f4422ba":"# SMOTE dataset\nplt.figure(figsize=(9, 8))\n\n# poly 3 degree\nplt.subplot(2, 2, 1)\n\nstarting_params = {\"class_weight\":None, \"random_state\": 42, \"kernel\":\"poly\" , \"degree\": 3, 'max_iter':100000}\n\nresults = gridsearch(SVC, ho_train_df, ho_train_df['DEATH_EVENT'], variable_params, \n                     starting_params, resampling=\"SMOTE\")\n\nmatrix_res = results[:, 0].reshape(len(variable_params[\"C\"]), len(variable_params[\"gamma\"]))\nmatrix_res = matrix_res.astype(np.float64)\n\nplt.tight_layout()\nax = sns.heatmap(matrix_res, annot=True, cmap=\"viridis\")\nplt.xlabel(\"gamma\")\nplt.ylabel(\"C\")\nplt.title(\"Polynomial (degree: 3) [SMOTE] accuracy\")\nax.set_xticklabels(variable_params[\"gamma\"])\nax.set_yticklabels(variable_params[\"C\"])\n\nplt.subplot(2, 2, 2)\n\nmatrix_res = results[:, 1].reshape(len(variable_params[\"C\"]), len(variable_params[\"gamma\"]))\nmatrix_res = matrix_res.astype(np.float64)\n\nplt.tight_layout()\nax = sns.heatmap(matrix_res, annot=True, cmap=\"inferno\")\nplt.xlabel(\"gamma\")\nplt.ylabel(\"C\")\nplt.title(\"Polynomial (degree: 3) [SMOTE] f1 score\")\nax.set_xticklabels(variable_params[\"gamma\"])\nax.set_yticklabels(variable_params[\"C\"])\n\n# rbf\nplt.subplot(2, 2, 3)\n\nstarting_params = {\"class_weight\":None, \"random_state\": 42, \"kernel\":\"rbf\", 'max_iter':100000}\nresults = gridsearch(SVC, ho_train_df, ho_train_df['DEATH_EVENT'], variable_params,\n                     starting_params, resampling=\"SMOTE\")\n\nmatrix_res = results[:, 0].reshape(len(variable_params[\"C\"]), len(variable_params[\"gamma\"]))\nmatrix_res = matrix_res.astype(np.float64)\n\nplt.tight_layout()\nax = sns.heatmap(matrix_res, annot=True, cmap=\"viridis\")\nplt.xlabel(\"gamma\")\nplt.ylabel(\"C\")\nplt.title(\"RBF [SMOTE] accuracy\")\nax.set_xticklabels(variable_params[\"gamma\"])\nax.set_yticklabels(variable_params[\"C\"]);\n\nplt.subplot(2, 2, 4)\n\nmatrix_res = results[:, 1].reshape(len(variable_params[\"C\"]), len(variable_params[\"gamma\"]))\nmatrix_res = matrix_res.astype(np.float64)\n\nplt.tight_layout()\nax = sns.heatmap(matrix_res, annot=True, cmap=\"inferno\")\nplt.xlabel(\"gamma\")\nplt.ylabel(\"C\")\nplt.title(\"RBF [SMOTE] f1 score\")\nax.set_xticklabels(variable_params[\"gamma\"])\nax.set_yticklabels(variable_params[\"C\"]);","cfabc8a2":"# class weight\nplt.figure(figsize=(9, 8))\n\n# poly 3 degree\nplt.subplot(2, 2, 1)\n\nstarting_params = {\"class_weight\":\"balanced\", \"random_state\": 42, \"kernel\":\"poly\" , \"degree\": 3, 'max_iter':100000}\nresults = gridsearch(SVC, ho_train_df, ho_train_df['DEATH_EVENT'], variable_params, \n                     starting_params)\n\nmatrix_res = results[:, 0].reshape(len(variable_params[\"C\"]), len(variable_params[\"gamma\"]))\nmatrix_res = matrix_res.astype(np.float64)\n\nplt.tight_layout()\nax = sns.heatmap(matrix_res, annot=True, cmap=\"viridis\")\nplt.xlabel(\"gamma\")\nplt.ylabel(\"C\")\nplt.title(\"Polynomial (degree: 3)\\n [class-weight balanced] accuracy\")\nax.set_xticklabels(variable_params[\"gamma\"])\nax.set_yticklabels(variable_params[\"C\"])\n\nplt.subplot(2, 2, 2)\n\nmatrix_res = results[:, 1].reshape(len(variable_params[\"C\"]), len(variable_params[\"gamma\"]))\nmatrix_res = matrix_res.astype(np.float64)\n\nplt.tight_layout()\nax = sns.heatmap(matrix_res, annot=True, cmap=\"inferno\")\nplt.xlabel(\"gamma\")\nplt.ylabel(\"C\")\nplt.title(\"Polynomial (degree: 3)\\n [class-weight balanced] f1 score\")\nax.set_xticklabels(variable_params[\"gamma\"])\nax.set_yticklabels(variable_params[\"C\"])\n\n# rbf\nplt.subplot(2, 2, 3)\n\nstarting_params = {\"class_weight\":\"balanced\", \"random_state\": 42, \"kernel\":\"rbf\", 'max_iter':100000}\nresults = gridsearch(SVC, ho_train_df, ho_train_df['DEATH_EVENT'], variable_params,\n                     starting_params)\n\nmatrix_res = results[:, 0].reshape(len(variable_params[\"C\"]), len(variable_params[\"gamma\"]))\nmatrix_res = matrix_res.astype(np.float64)\n\nplt.tight_layout()\nax = sns.heatmap(matrix_res, annot=True, cmap=\"viridis\")\nplt.xlabel(\"gamma\")\nplt.ylabel(\"C\")\nplt.title(\"RBF [class-weight balanced] accuracy\")\nax.set_xticklabels(variable_params[\"gamma\"])\nax.set_yticklabels(variable_params[\"C\"]);\n\nplt.subplot(2, 2, 4)\n\nmatrix_res = results[:, 1].reshape(len(variable_params[\"C\"]), len(variable_params[\"gamma\"]))\nmatrix_res = matrix_res.astype(np.float64)\n\nplt.tight_layout()\nax = sns.heatmap(matrix_res, annot=True, cmap=\"inferno\")\nplt.xlabel(\"gamma\")\nplt.ylabel(\"C\")\nplt.title(\"RBF [class-weight balanced] f1 score\")\nax.set_xticklabels(variable_params[\"gamma\"])\nax.set_yticklabels(variable_params[\"C\"]);","3867da48":"best_poly_no_rs = {\"class_weight\":None, \"random_state\": 42, \"kernel\":\"poly\" , \"degree\": 3, \"C\":0.1, \"gamma\":1, \"probability\":True}\nbest_rbf_no_rs = {\"class_weight\":None, \"random_state\": 42, \"kernel\":\"rbf\" , \"C\":10, \"gamma\":0.1, \"probability\":True}\n\nbest_poly_rs = {\"class_weight\":None, \"random_state\": 42, \"kernel\":\"poly\" , \"degree\": 3, \"C\":0.1, \"gamma\":1, \"probability\":True}\nbest_rbf_rs = {\"class_weight\":None, \"random_state\": 42, \"kernel\":\"rbf\" , \"C\":0.1, \"gamma\":1, \"probability\":True}\n\nbest_poly_sm = {\"class_weight\":None, \"random_state\": 42, \"kernel\":\"poly\" , \"degree\": 3, \"C\":100, \"gamma\":0.1, \"probability\":True}\nbest_rbf_sm = {\"class_weight\":None, \"random_state\": 42, \"kernel\":\"rbf\" , \"C\":1, \"gamma\":0.1, \"probability\":True}\n\nbest_poly_w = {\"class_weight\":\"balanced\", \"random_state\": 42, \"kernel\":\"poly\" , \"degree\": 3, \"C\":10, \"gamma\":0.1, \"probability\":True}\nbest_rbf_w = {\"class_weight\":\"balanced\", \"random_state\": 42, \"kernel\":\"rbf\" , \"C\":1, \"gamma\":0.1, \"probability\":True}","187850cb":"# original\nsvm = SVC(**best_poly_no_rs)\nsvm.fit(ho_train_df[all_features], ho_train_df['DEATH_EVENT'])\n\npred = svm.predict(ho_val_df[all_features])\npoly_raw_pred_no_rs = svm.predict_proba(ho_val_df[all_features])[:, 1]\n\npoly_no_rs_acc = accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred)\npoly_no_rs_pre = recall_score(ho_val_df[\"DEATH_EVENT\"], pred)\npoly_no_rs_rec = precision_score(ho_val_df[\"DEATH_EVENT\"], pred)\npoly_no_rs_f1 = f1_score(ho_val_df[\"DEATH_EVENT\"], pred)\n\n# oversampling\nsvm = SVC(**best_poly_rs)\nsvm.fit(ho_train_df_rs[all_features], ho_train_df_rs['DEATH_EVENT'])\n\npred = svm.predict(ho_val_df[all_features])\npoly_raw_pred_rs = svm.predict_proba(ho_val_df[all_features])[:, 1]\n\npoly_rs_acc = accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred)\npoly_rs_pre = recall_score(ho_val_df[\"DEATH_EVENT\"], pred)\npoly_rs_rec = precision_score(ho_val_df[\"DEATH_EVENT\"], pred)\npoly_rs_f1 = f1_score(ho_val_df[\"DEATH_EVENT\"], pred)\n\n# SMOTE\nsvm = SVC(**best_poly_sm)\nsvm.fit(ho_train_df_sm[all_features], ho_train_df_sm['DEATH_EVENT'])\n\npred = svm.predict(ho_val_df[all_features])\npoly_raw_pred_sm = svm.predict_proba(ho_val_df[all_features])[:, 1]\n\npoly_sm_acc = accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred)\npoly_sm_pre = recall_score(ho_val_df[\"DEATH_EVENT\"], pred)\npoly_sm_rec = precision_score(ho_val_df[\"DEATH_EVENT\"], pred)\npoly_sm_f1 = f1_score(ho_val_df[\"DEATH_EVENT\"], pred)\n\n# class-weight balanced\nsvm = SVC(**best_poly_w)\nsvm.fit(ho_train_df[all_features], ho_train_df['DEATH_EVENT'])\n\npred = svm.predict(ho_val_df[all_features])\npoly_raw_pred_w = svm.predict_proba(ho_val_df[all_features])[:, 1]\n\npoly_w_acc = accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred)\npoly_w_pre = recall_score(ho_val_df[\"DEATH_EVENT\"], pred)\npoly_w_rec = precision_score(ho_val_df[\"DEATH_EVENT\"], pred)\npoly_w_f1 = f1_score(ho_val_df[\"DEATH_EVENT\"], pred)","b9c4c24c":"'''\nprint(f\"no_rs:{poly_no_rs_acc},{poly_no_rs_f1}\")\nprint(f\"rs:{poly_rs_acc},{poly_rs_f1}\")\nprint(f\"sm:{poly_sm_acc},{poly_sm_f1}\")\nprint(f\"w:{poly_w_acc},{poly_w_f1}\")\n''';","e3a6feeb":"# https:\/\/python-graph-gallery.com\/11-grouped-barplot\/\n\nfig = plt.figure(figsize=(6, 4))\nbarWidth = 0.2\nspace=0.01\n \nbars1 = [poly_no_rs_f1, poly_no_rs_rec, poly_no_rs_pre, poly_no_rs_acc]\nbars2 = [poly_rs_f1, poly_rs_rec, poly_rs_pre, poly_rs_acc]\nbars3 = [poly_sm_f1, poly_sm_rec, poly_sm_pre, poly_sm_acc]\nbars4 = [poly_w_f1, poly_w_rec, poly_w_pre, poly_w_acc]\n\nr1 = np.arange(len(bars1))\nr2 = [x + barWidth + space for x in r1]\nr3 = [x + barWidth + space for x in r2]\nr4 = [x + barWidth + space for x in r3]\n\nplt.barh(r4, bars1, label=\"Original [C:0.1, gamma:1]\",height=barWidth, edgecolor='white', )\nplt.barh(r3, bars2, label=\"Oversampling [C:0.1, gamma:1]\", height=barWidth, edgecolor='white',)\nplt.barh(r2, bars3, label=\"SMOTE [C:100, gamma:0.1]\", height=barWidth, edgecolor='white', )\nplt.barh(r1, bars4, label=\"class-weight [C:10, gamma:0.1]\", height=barWidth, edgecolor='white', )\n\nplt.yticks([r + barWidth*1.5 for r in range(len(bars1))], [\"F1 score\", \"Recall\", \"Precision\", \"Accuracy\", ])\nplt.xlim(0, 1)\nplt.gca().xaxis.grid(True, linestyle=':')\nplt.legend(bbox_to_anchor=(1, 1));","d9261120":"from sklearn.metrics import roc_curve\n\nfpr_rs, tpr_rs, thresholds = roc_curve(ho_val_df[\"DEATH_EVENT\"], poly_raw_pred_rs)\nfpr_no_rs, tpr_no_rs, thresholds = roc_curve(ho_val_df[\"DEATH_EVENT\"], poly_raw_pred_no_rs)\nfpr_sm, tpr_sm, thresholds = roc_curve(ho_val_df[\"DEATH_EVENT\"], poly_raw_pred_sm)\nfpr_w, tpr_w, thresholds = roc_curve(ho_val_df[\"DEATH_EVENT\"], poly_raw_pred_w)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(2, 2, 1)\nplt.plot(fpr_no_rs, tpr_no_rs, label=f\"Original (area={roc_area(tpr_no_rs, fpr_no_rs)})\", linewidth=3, color='#990303')\nplt.grid()\nplt.ylim(0, 1)\nplt.xlim(0, 1)\nplt.legend(loc=4)\nplt.xlabel(\"False positive rate\")\nplt.ylabel(\"True positive rate\")\nplt.title(\"ROC curve [Original]\")\nplt.fill_between(fpr_no_rs, 0, tpr_no_rs, alpha=0.05, color='#990303')\n\nplt.subplot(2, 2, 2)\nplt.plot(fpr_rs, tpr_rs, label=f\"Oversampling (area={roc_area(tpr_rs, fpr_rs)})\", linewidth=3, color='#990303')\nplt.grid()\nplt.ylim(0, 1)\nplt.xlim(0, 1)\nplt.legend(loc=4)\nplt.xlabel(\"False positive rate\")\nplt.ylabel(\"True positive rate\")\nplt.title(\"ROC curve [Oversampling]\")\nplt.fill_between(fpr_rs, 0, tpr_rs, alpha=0.05, color='#990303')\n\nplt.subplot(2, 2, 3)\nplt.plot(fpr_sm, tpr_sm, label=f\"SMOTE (area={roc_area(tpr_sm, fpr_sm)})\", linewidth=3, color='#990303')\nplt.grid()\nplt.ylim(0, 1)\nplt.xlim(0, 1)\nplt.legend(loc=4)\nplt.xlabel(\"False positive rate\")\nplt.ylabel(\"True positive rate\")\nplt.title(\"ROC curve [SMOTE]\")\nplt.fill_between(fpr_sm, 0, tpr_sm, alpha=0.05, color='#990303')\n\nplt.subplot(2, 2, 4)\nplt.plot(fpr_w, tpr_w, label=f\"class-weight (area={roc_area(tpr_w, fpr_w)})\", linewidth=3, color='#990303')\nplt.grid()\nplt.ylim(0, 1)\nplt.xlim(0, 1)\nplt.legend(loc=4)\nplt.xlabel(\"False positive rate\")\nplt.ylabel(\"True positive rate\")\nplt.title(\"ROC curve [class-weight balanced]\")\nplt.fill_between(fpr_w, 0, tpr_w, alpha=0.05, color='#990303')\n\nplt.tight_layout()","638dc896":"# original\nsvm = SVC(**best_rbf_no_rs)\nsvm.fit(ho_train_df[all_features], ho_train_df['DEATH_EVENT'])\n\npred = svm.predict(ho_val_df[all_features])\nrbf_raw_pred_no_rs = svm.predict_proba(ho_val_df[all_features])[:, 1]\n\nrbf_no_rs_acc = accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred)\nrbf_no_rs_pre = recall_score(ho_val_df[\"DEATH_EVENT\"], pred)\nrbf_no_rs_rec = precision_score(ho_val_df[\"DEATH_EVENT\"], pred)\nrbf_no_rs_f1 = f1_score(ho_val_df[\"DEATH_EVENT\"], pred)\n\n# oversampling\nsvm = SVC(**best_rbf_rs)\nsvm.fit(ho_train_df_rs[all_features], ho_train_df_rs['DEATH_EVENT'])\n\npred = svm.predict(ho_val_df[all_features])\nrbf_raw_pred_rs = svm.predict_proba(ho_val_df[all_features])[:, 1]\n\nrbf_rs_acc = accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred)\nrbf_rs_pre = recall_score(ho_val_df[\"DEATH_EVENT\"], pred)\nrbf_rs_rec = precision_score(ho_val_df[\"DEATH_EVENT\"], pred)\nrbf_rs_f1 = f1_score(ho_val_df[\"DEATH_EVENT\"], pred)\n\n# SMOTE\nsvm = SVC(**best_rbf_sm)\nsvm.fit(ho_train_df_sm[all_features], ho_train_df_sm['DEATH_EVENT'])\n\npred = svm.predict(ho_val_df[all_features])\nrbf_raw_pred_sm = svm.predict_proba(ho_val_df[all_features])[:, 1]\n\nrbf_sm_acc = accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred)\nrbf_sm_pre = recall_score(ho_val_df[\"DEATH_EVENT\"], pred)\nrbf_sm_rec = precision_score(ho_val_df[\"DEATH_EVENT\"], pred)\nrbf_sm_f1 = f1_score(ho_val_df[\"DEATH_EVENT\"], pred)\n\n# class-weight balanced\nsvm = SVC(**best_rbf_w)\nsvm.fit(ho_train_df[all_features], ho_train_df['DEATH_EVENT'])\n\npred = svm.predict(ho_val_df[all_features])\nrbf_raw_pred_w = svm.predict_proba(ho_val_df[all_features])[:, 1]\n\nrbf_w_acc = accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred)\nrbf_w_pre = recall_score(ho_val_df[\"DEATH_EVENT\"], pred)\nrbf_w_rec = precision_score(ho_val_df[\"DEATH_EVENT\"], pred)\nrbf_w_f1 = f1_score(ho_val_df[\"DEATH_EVENT\"], pred)","60dedf98":"'''\nprint(f\"no_rs:{rbf_no_rs_acc},{rbf_no_rs_f1}\")\nprint(f\"rs:{rbf_rs_acc},{rbf_rs_f1}\")\nprint(f\"sm:{rbf_sm_acc},{rbf_sm_f1}\")\nprint(f\"w:{rbf_w_acc},{rbf_w_f1}\")\n''';","08f3a812":"# https:\/\/python-graph-gallery.com\/11-grouped-barplot\/\n\nfig = plt.figure(figsize=(6, 4))\nbarWidth = 0.2\nspace=0.01\n \nbars1 = [rbf_no_rs_f1, rbf_no_rs_rec, rbf_no_rs_pre, rbf_no_rs_acc]\nbars2 = [rbf_rs_f1, rbf_rs_rec, rbf_rs_pre, rbf_rs_acc]\nbars3 = [rbf_sm_f1, rbf_sm_rec, rbf_sm_pre, rbf_sm_acc]\nbars4 = [rbf_w_f1, rbf_w_rec, rbf_w_pre, rbf_w_acc]\n\nr1 = np.arange(len(bars1))\nr2 = [x + barWidth + space for x in r1]\nr3 = [x + barWidth + space for x in r2]\nr4 = [x + barWidth + space for x in r3]\n\nplt.barh(r4, bars1, label=\"Original [C:10, gamma:0.1]\",height=barWidth, edgecolor='white', )\nplt.barh(r3, bars2, label=\"Oversampling [C:0.1, gamma:1]\", height=barWidth, edgecolor='white',)\nplt.barh(r2, bars3, label=\"SMOTE [C:1, gamma:0.1]\", height=barWidth, edgecolor='white', )\nplt.barh(r1, bars4, label=\"class-weight [C:1, gamma:0.1]\", height=barWidth, edgecolor='white', )\n\nplt.yticks([r + barWidth*1.5 for r in range(len(bars1))], [\"F1 score\", \"Recall\", \"Precision\", \"Accuracy\"])\nplt.xlim(0, 1)\nplt.gca().xaxis.grid(True, linestyle=':')\nplt.legend(bbox_to_anchor=(1, 1));","63afe521":"fpr_rs, tpr_rs, thresholds = roc_curve(ho_val_df[\"DEATH_EVENT\"], rbf_raw_pred_rs)\nfpr_no_rs, tpr_no_rs, thresholds = roc_curve(ho_val_df[\"DEATH_EVENT\"], rbf_raw_pred_no_rs)\nfpr_sm, tpr_sm, thresholds = roc_curve(ho_val_df[\"DEATH_EVENT\"], rbf_raw_pred_sm)\nfpr_w, tpr_w, thresholds = roc_curve(ho_val_df[\"DEATH_EVENT\"], rbf_raw_pred_w)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(2, 2, 1)\nplt.plot(fpr_no_rs, tpr_no_rs, label=f\"Original (area={roc_area(tpr_no_rs, fpr_no_rs)})\", linewidth=3, color='#990303')\nplt.grid()\nplt.ylim(0, 1)\nplt.xlim(0, 1)\nplt.legend(loc=4)\nplt.xlabel(\"False positive rate\")\nplt.ylabel(\"True positive rate\")\nplt.title(\"ROC curve [Original]\")\nplt.fill_between(fpr_no_rs, 0, tpr_no_rs, alpha=0.05, color='#990303')\n\nplt.subplot(2, 2, 2)\nplt.plot(fpr_rs, tpr_rs, label=f\"Oversampling (area={roc_area(tpr_rs, fpr_rs)})\", linewidth=3, color='#990303')\nplt.grid()\nplt.ylim(0, 1)\nplt.xlim(0, 1)\nplt.legend(loc=4)\nplt.xlabel(\"False positive rate\")\nplt.ylabel(\"True positive rate\")\nplt.title(\"ROC curve [Oversampling]\")\nplt.fill_between(fpr_rs, 0, tpr_rs, alpha=0.05, color='#990303')\n\nplt.subplot(2, 2, 3)\nplt.plot(fpr_sm, tpr_sm, label=f\"SMOTE (area={roc_area(tpr_sm, fpr_sm)})\", linewidth=3, color='#990303')\nplt.grid()\nplt.ylim(0, 1)\nplt.xlim(0, 1)\nplt.legend(loc=4)\nplt.xlabel(\"False positive rate\")\nplt.ylabel(\"True positive rate\")\nplt.title(\"ROC curve [SMOTE]\")\nplt.fill_between(fpr_sm, 0, tpr_sm, alpha=0.05, color='#990303')\n\nplt.subplot(2, 2, 4)\nplt.plot(fpr_w, tpr_w, label=f\"class-weight (area={roc_area(tpr_w, fpr_w)})\", linewidth=3, color='#990303')\nplt.grid()\nplt.ylim(0, 1)\nplt.xlim(0, 1)\nplt.legend(loc=4)\nplt.xlabel(\"False positive rate\")\nplt.ylabel(\"True positive rate\")\nplt.title(\"ROC curve [class-weight balanced]\")\nplt.fill_between(fpr_w, 0, tpr_w, alpha=0.05, color='#990303')\n\nplt.tight_layout()","8a791674":"n_neigh = [1, 5, 10, 20, 50]\nmetric = \"minkowski\"\np = [2, 3, 4, 5]\nh = 0.03\n\nx_min, x_max = hf_norm[\"ejection_fraction\"].min() - .5, hf_norm[\"ejection_fraction\"].max() + .5\ny_min, y_max = hf_norm[\"serum_creatinine\"].min() - .5, hf_norm[\"serum_creatinine\"].max() + .5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))","d037bea1":"print(\"Evaluting different number of nearest neighbors\")\n%matplotlib notebook\nplt.ioff()\nfig, (ax1, ax2) = plt.subplots(1, 2);\n%matplotlib inline \nplt.ion()\n\nfig.set_size_inches(8, 4);\ndef animate_func(i):\n    knn = KNeighborsClassifier(n_neigh[i], weights=\"uniform\", metric=metric, p=2)\n    knn.fit(hf_norm[[\"ejection_fraction\", \"serum_creatinine\"]], hf_norm['DEATH_EVENT'])\n    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    \n    ax1.clear()\n    ax1.contourf(xx, yy, Z, alpha=0.5, cmap=cm_rev)\n\n    ax1.scatter(hf_norm[hf_norm[\"DEATH_EVENT\"]==0][\"ejection_fraction\"],\n                    hf_norm[hf_norm[\"DEATH_EVENT\"]==0][\"serum_creatinine\"], label=\"alive\", color=\"#990303\", edgecolor='BLACK')\n    ax1.scatter(hf_norm[hf_norm[\"DEATH_EVENT\"]==1][\"ejection_fraction\"],\n                    hf_norm[hf_norm[\"DEATH_EVENT\"]==1][\"serum_creatinine\"], label=\"dead\", color=\"#9C9999\", edgecolor='BLACK')\n\n    ax1.set_xlabel(\"ejection_fraction\")\n    ax1.set_ylabel(\"serum_creatinine\")\n    ax1.legend();\n    ax1.set_title(f\"KNN (weights=uniform) n_neigh={n_neigh[i]}\")\n    \n    # distance\n    knn = KNeighborsClassifier(n_neigh[i], weights=\"distance\", metric=metric, p=2)\n    knn.fit(hf_norm[[\"ejection_fraction\", \"serum_creatinine\"]], hf_norm['DEATH_EVENT'])\n    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    \n    ax2.clear()\n    ax2.contourf(xx, yy, Z, alpha=0.5, cmap=cm_rev)\n\n    ax2.scatter(hf_norm[hf_norm[\"DEATH_EVENT\"]==0][\"ejection_fraction\"],\n                    hf_norm[hf_norm[\"DEATH_EVENT\"]==0][\"serum_creatinine\"], label=\"alive\", color=\"#990303\", edgecolor='BLACK')\n    ax2.scatter(hf_norm[hf_norm[\"DEATH_EVENT\"]==1][\"ejection_fraction\"],\n                    hf_norm[hf_norm[\"DEATH_EVENT\"]==1][\"serum_creatinine\"], label=\"dead\", color=\"#9C9999\", edgecolor='BLACK')\n\n    ax2.set_xlabel(\"ejection_fraction\")\n    ax2.set_ylabel(\"serum_creatinine\")\n    ax2.legend();\n    ax2.set_title(f\"KNN (weights=distance) n_neigh={n_neigh[i]}\")\n    \n    fig.tight_layout()\n    \nanim = animation.FuncAnimation(\n                               fig, \n                               animate_func, \n                               frames = len(n_neigh),\n                               interval = 500, # in ms\n                               );\nHTML(anim.to_jshtml())","06d4188c":"print(\"Evaluating different values of p for minkowski distance metric\")\n%matplotlib notebook\nplt.ioff()\nfig, (ax1, ax2) = plt.subplots(1, 2);\n%matplotlib inline \nplt.ion()\n\nfig.set_size_inches(8, 4);\ndef animate_func(i):\n    knn = KNeighborsClassifier(10, weights=\"uniform\", metric=metric, p=p[i])\n    knn.fit(hf_norm[[\"ejection_fraction\", \"serum_creatinine\"]], hf_norm['DEATH_EVENT'])\n    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    \n    ax1.clear()\n    ax1.contourf(xx, yy, Z, alpha=0.5, cmap=cm_rev)\n\n    ax1.scatter(hf_norm[hf_norm[\"DEATH_EVENT\"]==0][\"ejection_fraction\"],\n                    hf_norm[hf_norm[\"DEATH_EVENT\"]==0][\"serum_creatinine\"], label=\"alive\", color=\"#990303\", edgecolor='BLACK')\n    ax1.scatter(hf_norm[hf_norm[\"DEATH_EVENT\"]==1][\"ejection_fraction\"],\n                    hf_norm[hf_norm[\"DEATH_EVENT\"]==1][\"serum_creatinine\"], label=\"dead\", color=\"#9C9999\", edgecolor='BLACK')\n\n    ax1.set_xlabel(\"ejection_fraction\")\n    ax1.set_ylabel(\"serum_creatinine\")\n    ax1.legend();\n    ax1.set_title(f\"KNN (weights=uniform) p={p[i]}\")\n    \n    # distance\n    knn = KNeighborsClassifier(10, weights=\"distance\", metric=metric, p=p[i])\n    knn.fit(hf_norm[[\"ejection_fraction\", \"serum_creatinine\"]], hf_norm['DEATH_EVENT'])\n    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    \n    ax2.clear()\n    ax2.contourf(xx, yy, Z, alpha=0.5, cmap=cm_rev)\n\n    ax2.scatter(hf_norm[hf_norm[\"DEATH_EVENT\"]==0][\"ejection_fraction\"],\n                    hf_norm[hf_norm[\"DEATH_EVENT\"]==0][\"serum_creatinine\"], label=\"alive\", color=\"#990303\", edgecolor='BLACK')\n    ax2.scatter(hf_norm[hf_norm[\"DEATH_EVENT\"]==1][\"ejection_fraction\"],\n                    hf_norm[hf_norm[\"DEATH_EVENT\"]==1][\"serum_creatinine\"], label=\"dead\", color=\"#9C9999\", edgecolor='BLACK')\n\n    ax2.set_xlabel(\"ejection_fraction\")\n    ax2.set_ylabel(\"serum_creatinine\")\n    ax2.legend();\n    ax2.set_title(f\"KNN (weights=distance) p={p[i]}\")\n    \n    fig.tight_layout()\n    \nanim = animation.FuncAnimation(\n                               fig, \n                               animate_func, \n                               frames = len(p),\n                               interval = 500, # in ms\n                               );\nHTML(anim.to_jshtml())","be1ea470":"n_neigh = range(1, 21, 2)\nn = len(n_neigh)\n\nrs_acc, rs_rec, rs_pre, rs_f1 = [], [], [], []\nsm_acc, sm_rec, sm_pre, sm_f1 = [], [], [], []\nno_rs_acc, no_rs_rec, no_rs_pre, no_rs_f1 = [], [], [], []\nrs_raw_pred, sm_raw_pred, no_rs_raw_pred = [], [], []\n\nfor i,n in enumerate(n_neigh):\n       \n    # oversampling\n    knn = KNeighborsClassifier(n, weights=\"uniform\", metric=metric, p=2)\n    knn.fit(ho_train_df_rs[all_features], ho_train_df_rs['DEATH_EVENT']);\n    pred = knn.predict_proba(ho_val_df[all_features]);\n    pred=pred[:, 1]\n    rs_raw_pred.append(pred.copy())\n    pred[pred>=0.5]=1\n    pred[pred<0.5]=0\n        \n    rs_acc.append(accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred))\n    rs_rec.append(recall_score(ho_val_df[\"DEATH_EVENT\"], pred))\n    rs_pre.append(precision_score(ho_val_df[\"DEATH_EVENT\"], pred))\n    rs_f1.append(f1_score(ho_val_df[\"DEATH_EVENT\"], pred))\n\n    # SMOTE\n    knn = KNeighborsClassifier(n, weights=\"uniform\", metric=metric, p=2)\n    knn.fit(ho_train_df_sm[all_features], ho_train_df_sm['DEATH_EVENT']);\n    pred = knn.predict_proba(ho_val_df[all_features]);\n    pred=pred[:, 1]\n    sm_raw_pred.append(pred.copy())\n    pred[pred>=0.5]=1\n    pred[pred<0.5]=0\n        \n    sm_acc.append(accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred))\n    sm_rec.append(recall_score(ho_val_df[\"DEATH_EVENT\"], pred))\n    sm_pre.append(precision_score(ho_val_df[\"DEATH_EVENT\"], pred))\n    sm_f1.append(f1_score(ho_val_df[\"DEATH_EVENT\"], pred))\n        \n    #no oversampling\n    knn = KNeighborsClassifier(n, weights=\"uniform\", metric=metric, p=2)\n    knn.fit(ho_train_df[all_features], ho_train_df['DEATH_EVENT']);\n    pred = knn.predict_proba(ho_val_df[all_features]);\n    pred=pred[:, 1]\n    no_rs_raw_pred.append(pred.copy())\n    pred[pred>=0.5]=1\n    pred[pred<0.5]=0\n        \n    no_rs_acc.append(accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred))\n    no_rs_rec.append(recall_score(ho_val_df[\"DEATH_EVENT\"], pred))\n    no_rs_pre.append(precision_score(ho_val_df[\"DEATH_EVENT\"], pred))\n    no_rs_f1.append(f1_score(ho_val_df[\"DEATH_EVENT\"], pred))","f1a8701e":"#sorted([(a, b) for a, b in zip(sm_acc, sm_f1)], key=lambda a:a[0]+a[1])","663e568e":"# k fold\nrs_acc, rs_rec, rs_pre, rs_f1= [], [], [], []\nsm_acc, sm_rec, sm_pre, sm_f1= [], [], [], []\nno_rs_acc, no_rs_rec, no_rs_pre, no_rs_f1= [], [], [], []\n\nfor i,n in enumerate(n_neigh):\n       \n    # oversampling\n    knn = KNeighborsClassifier(n, weights=\"uniform\", metric=metric, p=2)\n    acc, pre, rec, f1 = kfold.fit_predict(knn, ho_train_df, ho_train_df['DEATH_EVENT'],\n                                          threshold=0.5, resampling=\"oversampling\")\n    rs_acc.append(acc)\n    rs_rec.append(rec)\n    rs_pre.append(pre)\n    rs_f1.append(f1)\n\n    # SMOTE\n    knn = KNeighborsClassifier(n, weights=\"uniform\", metric=metric, p=2)\n    acc, pre, rec, f1 = kfold.fit_predict(knn, ho_train_df, ho_train_df['DEATH_EVENT'],\n                                          threshold=0.5, resampling=\"SMOTE\", cached=\"True\")\n    sm_acc.append(acc)\n    sm_rec.append(rec)\n    sm_pre.append(pre)\n    sm_f1.append(f1)\n        \n    #no oversampling\n    knn = KNeighborsClassifier(n, weights=\"uniform\", metric=metric, p=2)\n    acc, pre, rec, f1 = kfold.fit_predict(knn, ho_train_df, ho_train_df['DEATH_EVENT'],\n                                          threshold=0.5, resampling=None)\n    no_rs_acc.append(acc)\n    no_rs_rec.append(rec)\n    no_rs_pre.append(pre)\n    no_rs_f1.append(f1)","369a6b36":"#sorted([(a, b) for a, b in zip(sm_acc, sm_f1)], key=lambda a:a[0]+a[1])","dc40f75b":"print(\"K-Fold cross-validation metrics (5 folds)\")\nplt.figure(figsize=(10, 10))\n\n# accuracy\nplt.subplot(2, 2, 1)    \nplt.plot(list(n_neigh), rs_acc, label=\"Oversampling\", marker='o', linewidth=3)\nplt.plot(list(n_neigh), sm_acc, label=\"SMOTE\", marker='o', linewidth=3)\nplt.plot(list(n_neigh), no_rs_acc, label = \"original dataset\", marker='o', linewidth=3)\nplt.xticks(n_neigh)\n\nplt.legend()\nplt.grid(True)\nplt.ylim(0, 1)\nplt.ylabel(\"Validation accuracy\")\nplt.xlabel(\"n neighbors\")\nplt.title(\"Accuracy\")\n\n# precision\nplt.subplot(2, 2, 2)   \nplt.plot(list(n_neigh), rs_pre, label=\"Oversampling\", marker='o', linewidth=3)\nplt.plot(list(n_neigh), sm_pre, label=\"SMOTE\", marker='o', linewidth=3)\nplt.plot(list(n_neigh), no_rs_pre, label = \"original dataset\", marker='o', linewidth=3)\nplt.xticks(n_neigh)\n\nplt.legend()\nplt.grid(True)\nplt.ylim(0, 1)\nplt.ylabel(\"Validation precision\")\nplt.xlabel(\"n neighbors\")\nplt.title(\"Precision\")\n\n# recall\nplt.subplot(2, 2, 3)    \nplt.plot(list(n_neigh), rs_rec, label=\"Oversampling\", marker='o', linewidth=3)\nplt.plot(list(n_neigh), sm_rec, label=\"SMOTE\", marker='o', linewidth=3)\nplt.plot(list(n_neigh), no_rs_rec, label = \"original dataset\", marker='o', linewidth=3)\nplt.xticks(n_neigh)\n\nplt.legend()\nplt.grid(True)\nplt.ylim(0, 1)\nplt.ylabel(\"Validation recall\")\nplt.xlabel(\"n neighbors\")\nplt.title(\"Recall\")\n\n# f1 score\nplt.subplot(2, 2, 4)    \nplt.plot(list(n_neigh), rs_f1, label=\"Oversampling\", marker='o', linewidth=3)\nplt.plot(list(n_neigh), sm_f1, label=\"SMOTE\", marker='o', linewidth=3)\nplt.plot(list(n_neigh), no_rs_f1, label = \"original dataset\", marker='o', linewidth=3)\nplt.xticks(n_neigh)\n\nplt.legend()\nplt.grid(True)\nplt.ylim(0, 1)\nplt.ylabel(\"Validation f1 score\")\nplt.xlabel(\"n neighbors\")\nplt.title(\"F1 score\");","69f6596e":"n_neigh = range(1, 21, 2)\nn = len(n_neigh)\n\nrs_acc, rs_rec, rs_pre, rs_f1= [], [], [], []\nsm_acc, sm_rec, sm_pre, sm_f1= [], [], [], []\nno_rs_acc, no_rs_rec, no_rs_pre, no_rs_f1= [], [], [], []\nrs_raw_pred, sm_raw_pred, no_rs_raw_pred = [], [], []\n\nfor i,n in enumerate(n_neigh):\n         \n    # oversampling\n    knn = KNeighborsClassifier(n, weights=\"distance\", metric=metric, p=2)\n    knn.fit(ho_train_df_rs[all_features], ho_train_df_rs['DEATH_EVENT']);\n    pred = knn.predict_proba(ho_val_df[all_features]);\n    pred=pred[:, 1]\n    rs_raw_pred.append(pred.copy())\n    pred[pred>=0.5]=1\n    pred[pred<0.5]=0\n        \n    rs_acc.append(accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred))\n    rs_rec.append(recall_score(ho_val_df[\"DEATH_EVENT\"], pred))\n    rs_pre.append(precision_score(ho_val_df[\"DEATH_EVENT\"], pred))\n    rs_f1.append(f1_score(ho_val_df[\"DEATH_EVENT\"], pred))\n\n    # SMOTE\n    knn = KNeighborsClassifier(n, weights=\"distance\", metric=metric, p=2)\n    knn.fit(ho_train_df_sm[all_features], ho_train_df_sm['DEATH_EVENT']);\n    pred = knn.predict_proba(ho_val_df[all_features]);\n    pred=pred[:, 1]\n    sm_raw_pred.append(pred.copy())\n    pred[pred>=0.5]=1\n    pred[pred<0.5]=0\n        \n    sm_acc.append(accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred))\n    sm_rec.append(recall_score(ho_val_df[\"DEATH_EVENT\"], pred))\n    sm_pre.append(precision_score(ho_val_df[\"DEATH_EVENT\"], pred))\n    sm_f1.append(f1_score(ho_val_df[\"DEATH_EVENT\"], pred))\n        \n    # no oversampling\n    knn = KNeighborsClassifier(n, weights=\"distance\", metric=metric, p=2)\n    knn.fit(ho_train_df[all_features], ho_train_df['DEATH_EVENT']);\n    pred = knn.predict_proba(ho_val_df[all_features]);\n    pred=pred[:, 1]\n    no_rs_raw_pred.append(pred.copy())\n    pred[pred>=0.5]=1\n    pred[pred<0.5]=0\n        \n    no_rs_acc.append(accuracy_score(ho_val_df[\"DEATH_EVENT\"], pred))\n    no_rs_rec.append(recall_score(ho_val_df[\"DEATH_EVENT\"], pred))\n    no_rs_pre.append(precision_score(ho_val_df[\"DEATH_EVENT\"], pred))\n    no_rs_f1.append(f1_score(ho_val_df[\"DEATH_EVENT\"], pred))","1681cc33":"#sorted([(a, b) for a, b in zip(rs_acc, rs_f1)], key=lambda a:a[0]+a[1])","5367ebf1":"# k fold\n\nrs_acc, rs_rec, rs_pre, rs_f1= [], [], [], []\nsm_acc, sm_rec, sm_pre, sm_f1= [], [], [], []\nno_rs_acc, no_rs_rec, no_rs_pre, no_rs_f1= [], [], [], []\n\nfor i,n in enumerate(n_neigh):\n         \n    # oversampling\n    knn = KNeighborsClassifier(n, weights=\"distance\", metric=metric, p=2)\n    acc, pre, rec, f1 = kfold.fit_predict(knn, ho_train_df, ho_train_df['DEATH_EVENT'],\n                                          threshold=0.5, resampling=\"oversampling\")\n    rs_acc.append(acc)\n    rs_rec.append(rec)\n    rs_pre.append(pre)\n    rs_f1.append(f1)\n    \n    # SMOTE\n    knn = KNeighborsClassifier(n, weights=\"distance\", metric=metric, p=2)\n    acc, pre, rec, f1 = kfold.fit_predict(knn, ho_train_df, ho_train_df['DEATH_EVENT'],\n                                          threshold=0.5, resampling=\"SMOTE\", cached=\"True\")\n    sm_acc.append(acc)\n    sm_rec.append(rec)\n    sm_pre.append(pre)\n    sm_f1.append(f1)  \n    \n    # no oversampling\n    knn = KNeighborsClassifier(n, weights=\"distance\", metric=metric, p=2)\n    acc, pre, rec, f1 = kfold.fit_predict(knn, ho_train_df, ho_train_df['DEATH_EVENT'],\n                                          threshold=0.5, resampling=None)\n    no_rs_acc.append(acc)\n    no_rs_rec.append(rec)\n    no_rs_pre.append(pre)\n    no_rs_f1.append(f1)","324fc62a":"#sorted([(a, b) for a, b in zip(sm_acc, sm_f1)], key=lambda a:a[0]+a[1])","d79a6502":"print(\"K-Fold cross-validation metrics (5 folds)\")\nplt.figure(figsize=(10, 10))\n\n# accuracy\nplt.subplot(2, 2, 1)    \nplt.plot(list(n_neigh), rs_acc, label=\"Oversampling\", marker='o', linewidth=3)\nplt.plot(list(n_neigh), sm_acc, label=\"SMOTE\", marker='o', linewidth=3)\nplt.plot(list(n_neigh), no_rs_acc, label = \"original dataset\", marker='o', linewidth=3)\nplt.xticks(n_neigh)\n\nplt.legend()\nplt.grid(True)\nplt.ylim(0, 1)\nplt.ylabel(\"Validation accuracy\")\nplt.xlabel(\"n neighbors\")\nplt.title(\"Accuracy\")\n\n# precision\nplt.subplot(2, 2, 2)   \nplt.plot(list(n_neigh), rs_pre, label=\"Oversampling\", marker='o', linewidth=3)\nplt.plot(list(n_neigh), sm_pre, label=\"SMOTE\", marker='o', linewidth=3)\nplt.plot(list(n_neigh), no_rs_pre, label = \"original dataset\", marker='o', linewidth=3)\nplt.xticks(n_neigh)\n\nplt.legend()\nplt.grid(True)\nplt.ylim(0, 1)\nplt.ylabel(\"Validation precision\")\nplt.xlabel(\"n neighbors\")\nplt.title(\"Precision\")\n\n# recall\nplt.subplot(2, 2, 3)    \nplt.plot(list(n_neigh), rs_rec, label=\"Oversampling\", marker='o', linewidth=3)\nplt.plot(list(n_neigh), sm_rec, label=\"SMOTE\", marker='o', linewidth=3)\nplt.plot(list(n_neigh), no_rs_rec, label = \"original dataset\", marker='o', linewidth=3)\nplt.xticks(n_neigh)\n\nplt.legend()\nplt.grid(True)\nplt.ylim(0, 1)\nplt.ylabel(\"Validation recall\")\nplt.xlabel(\"n neighbors\")\nplt.title(\"Recall\")\n\n# f1 score\nplt.subplot(2, 2, 4)    \nplt.plot(list(n_neigh), rs_f1, label=\"Oversampling\", marker='o', linewidth=3)\nplt.plot(list(n_neigh), sm_f1, label=\"SMOTE\", marker='o', linewidth=3)\nplt.plot(list(n_neigh), no_rs_f1, label = \"original dataset\", marker='o', linewidth=3)\nplt.xticks(n_neigh)\n\nplt.legend()\nplt.grid(True)\nplt.ylim(0, 1)\nplt.ylabel(\"Validation f1 score\")\nplt.xlabel(\"n neighbors\")\nplt.title(\"F1 score\");","963f349e":"dead = hf_norm[hf_norm[\"DEATH_EVENT\"]==1][all_features].corr(method='pearson')\nalive = hf_norm[hf_norm[\"DEATH_EVENT\"]==0][all_features].corr(method='pearson')\n\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplt.title(\"Mutual correlation given DEATH_EVENT=1\")\nsns.heatmap(dead, vmin=-1, vmax=1, annot=True, xticklabels=False, cmap='viridis', fmt='.2f')\n\nplt.subplot(1, 2, 2)\nplt.title(\"Mutual correlation given DEATH_EVENT=0\")\nsns.heatmap(alive, vmin=-1, vmax=1, annot=True, xticklabels=False, cmap='viridis', fmt='.2f')\n\nplt.tight_layout()","aca45d43":"feats = [\"age\", \"ejection_fraction\", \"serum_creatinine\", \"serum_sodium\"]\n\nfig, axs = plt.subplots(1, 4)\nfig.set_size_inches(14, 4);\n\nfor j, ax in enumerate(axs):\n    ax.set_title(feats[j], fontsize=13)\n    sns.distplot(hf_norm.loc[hf[\"DEATH_EVENT\"]==0, feats[j]], ax=ax, label=\"alive\", color = \"#990303\", kde=False, rug=True)\n    sns.distplot(hf_norm.loc[hf[\"DEATH_EVENT\"]==1, feats[j]], ax=ax, label=\"dead\", color = \"#292323\", kde=False, rug=True)\n    ax.legend(prop={'size': 13})\n    \nfig.tight_layout()","4215a29f":"%matplotlib notebook\nplt.ioff()\nfig, axs = plt.subplots(1, 4);\n%matplotlib inline \nplt.ion()\n\nbws = [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8]\nfeats = [\"age\", \"ejection_fraction\", \"serum_creatinine\", \"serum_sodium\"]\nylims = [0.8, 1, 1.75, 0.9]\n\nfig.set_size_inches(14, 4);\ndef animate_func(i):    \n    for j, ax in enumerate(axs):        \n        ax.clear()        \n        ax.set_ylim(0, ylims[j])\n        sns.kdeplot(hf_norm.loc[hf[\"DEATH_EVENT\"]==0, feats[j]], ax=ax,\n                label=\"alive\", color = \"#990303\", shade=True, kernel=\"gau\", cut=0, bw=bws[i])\n        sns.kdeplot(hf_norm.loc[hf[\"DEATH_EVENT\"]==1, feats[j]], ax=ax,\n                label=\"dead\",  color = \"#292323\", shade=True, kernel=\"gau\", cut=0, bw=bws[i])\n        ax.set_xlabel(f\"bandwidth: {bws[i]}\")\n        ax.set_title(feats[j])\n        \n    fig.tight_layout()\n    \nanim = animation.FuncAnimation(\n                               fig, \n                               animate_func, \n                               frames = len(bws),\n                               interval = 500, # in ms\n                               );\nHTML(anim.to_jshtml())","4d38cfba":"rot = lambda s: 1.06*np.std(s)*len(s)**(-0.2)\n\nfeats = [\"age\", \"ejection_fraction\", \"serum_creatinine\", \"serum_sodium\", \"CPK\"]\n\nfor feat in feats:\n    \n    h = rot(hf_norm.loc[hf[\"DEATH_EVENT\"]==0, feat])\n    print(f\"{feat}: {np.round(h, 2)}\")","e0012fe4":"from scipy.stats import gaussian_kde, norm\n\nclass bernoulli:  \n    prob0, prob1 = None, None\n    \n    def __init__(self, samples):\n        if isinstance(samples, list):\n            if len(set(samples))!=2:\n                print(\"not binary\")\n                return None\n            \n            counts = np.unique(samples, return_counts=True)\n            self.prob0 = counts[1][0]\/len(samples)\n            self.prob1 = 1-self.prob0            \n        else:\n            if isinstance(samples, float):     \n                self.prob1 = samples\n                self.prob0 = 1-self.prob1\n\n    def evaluate(self, x):\n        if x==0:\n            return self.prob0\n        if x==1:\n            return self.prob1\n        return None\n    \nclass MyNaiveBayes:  \n    prior = None\n    categorical = []\n    distributions = {}\n    columns = []\n    \n    def __init__(self, prior, categorical, bw=None):\n        self.prior=bernoulli(prior)\n        self.categorical = categorical\n        \n        if bw is not None:\n            self.bw = bw\n        else:\n            self.bw=\"scott\"\n        \n    def fit(self, X, y):\n        self.columns = list(X.columns)\n        \n        for i, col in enumerate(self.columns):\n            data = X[col]\n            \n            if(self.categorical[i]):\n                distr0 = bernoulli(list(data[y==0]))\n                distr1 = bernoulli(list(data[y==1]))\n            else:\n                distr0 = gaussian_kde(data[y==0], bw_method=self.bw)\n                distr1 = gaussian_kde(data[y==1], bw_method=self.bw)\n            \n            self.distributions[col] = (distr0, distr1)\n    \n    def predict_proba(self, X):\n        probs = []\n        \n        for _, sample in X.iterrows():\n            score0 = 1\n            score1 = 1\n            \n            for col in self.columns:\n                score0 *= self.distributions[col][0].evaluate(sample[col])\n                score1 *= self.distributions[col][1].evaluate(sample[col])\n\n            score0 *= self.prior.evaluate(0)\n            score1 *= self.prior.evaluate(1)\n            \n            prob1 = score1\/(score0+score1)        \n            probs.append(prob1)\n            \n        return np.array(probs)\n     \n    def predict(self, X):\n        probs = self.predict_proba(X)\n        probs[probs>=0.5] = 1\n        probs[probs<0.5] = 0\n        return probs","e0dd6d76":"gauss = norm(0, 1)\nprior = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n\nno_rs_acc, no_rs_pre, no_rs_rec, no_rs_f1= [], [], [], []\nrs_acc, rs_pre, rs_rec, rs_f1= [], [], [], []\nsm_acc, sm_pre, sm_rec, sm_f1= [], [], [], []\n\nfor i in range(len(bws)):\n    # original\n    for p in prior:\n        nb = MyNaiveBayes(prior = p, categorical=(True, True, False, False, False, False, False, False), bw=bws[i])\n        nb.fit(ho_train_df[all_features], ho_train_df['DEATH_EVENT'])\n        preds = nb.predict(ho_val_df[all_features])\n\n        no_rs_acc.append(accuracy_score(preds, ho_val_df['DEATH_EVENT']))\n        no_rs_pre.append(precision_score(preds, ho_val_df['DEATH_EVENT']))\n        no_rs_rec.append(recall_score(preds, ho_val_df['DEATH_EVENT']))\n        no_rs_f1.append(f1_score(preds, ho_val_df['DEATH_EVENT']))\n\n    # oversampling\n    for p in prior:\n        nb = MyNaiveBayes(prior = p, categorical=(True, True, False, False, False, False, False, False), bw=bws[i])\n        nb.fit(ho_train_df_rs[all_features], ho_train_df_rs['DEATH_EVENT'])\n        preds = nb.predict(ho_val_df[all_features])\n\n        rs_acc.append(accuracy_score(preds, ho_val_df['DEATH_EVENT']))\n        rs_pre.append(precision_score(preds, ho_val_df['DEATH_EVENT']))\n        rs_rec.append(recall_score(preds, ho_val_df['DEATH_EVENT']))\n        rs_f1.append(f1_score(preds, ho_val_df['DEATH_EVENT']))\n\n    # SMOTE\n    for p in prior:\n        nb = MyNaiveBayes(prior = p, categorical=(True, True, False, False, False, False, False, False), bw=bws[i])\n        nb.fit(ho_train_df_sm[all_features], ho_train_df_sm['DEATH_EVENT'])\n        preds = nb.predict(ho_val_df[all_features])\n\n        sm_acc.append(accuracy_score(preds, ho_val_df['DEATH_EVENT']))\n        sm_pre.append(precision_score(preds, ho_val_df['DEATH_EVENT']))\n        sm_rec.append(recall_score(preds, ho_val_df['DEATH_EVENT']))\n        sm_f1.append(f1_score(preds, ho_val_df['DEATH_EVENT']))     ","41da65b1":"#sorted([(a, b) for a,b in zip(sm_acc, sm_f1)], key=lambda a: a[0]+a[1])","41dd7682":"print(\"K-Fold cross-validation metrics (5 folds)\")\n%matplotlib notebook\nplt.ioff()\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2);\n%matplotlib inline \nplt.ion()\n\nfig.set_size_inches(7, 7);\ndef animate_func(i):\n\n    x = np.linspace(-3, 3, 1000)\n    y = (gauss.pdf(x\/bws[i]))\/bws[i]\n    \n    ax1.clear()\n    ax1.set_title(\"Gaussian kernel\")\n    ax1.plot(x, y, color='#990303', linewidth=1.5, label=f\"bandwidth={bws[i]}\")\n    ax1.fill_between(x, 0, y, alpha=0.15, color='#990303')\n    ax1.set_ylim(0, 4.1)\n    ax1.set_xlim(-3.5, 3.5)\n    ax1.legend(loc=1, fontsize=8)\n    \n    # original\n    acc, pre, rec, f1= [], [], [], []\n    for p in prior:\n        nb = MyNaiveBayes(prior = p, categorical=(True, True, False, False, False, False, False), bw=bws[i])\n        accuracy, precision, recall, f1_s = kfold.fit_predict(nb, ho_train_df, ho_train_df['DEATH_EVENT'],\n                                          threshold=0.5, resampling=None)\n        acc.append(accuracy)\n        pre.append(precision)\n        rec.append(recall)\n        f1.append(f1_s)\n        \n    ax2.clear()\n    ax2.set_ylim(0, 1)\n    ax2.set_title(\"Original\")\n    ax2.set_xlabel(\"Prior class probability P(DEATH_EVENT=1)\")\n    \n    ax2.plot(list(prior), acc, label=\"accuracy\", marker='o', linewidth=2)\n    ax2.plot(list(prior), pre, label=\"precision\", marker='o', linewidth=1, alpha=0.5)\n    ax2.plot(list(prior), rec, label=\"recall\", marker='o', linewidth=1, alpha=0.5)\n    ax2.plot(list(prior), f1, label=\"f1 score\", marker='o', linewidth=1)\n    ax2.legend(loc=4, fontsize=8)\n    ax2.grid()\n\n    # oversampling\n    acc, pre, rec, f1= [], [], [], []\n    for p in prior:\n        nb = MyNaiveBayes(prior = p, categorical=(True, True, False, False, False, False, False), bw=bws[i])\n        \n        accuracy, precision, recall, f1_s = kfold.fit_predict(nb, ho_train_df, ho_train_df['DEATH_EVENT'],\n                                          threshold=0.5, resampling=\"oversampling\")\n        acc.append(accuracy)\n        pre.append(precision)\n        rec.append(recall)\n        f1.append(f1_s)\n        \n    ax3.clear()\n    ax3.set_ylim(0, 1)\n    ax3.set_title(\"Random oversampling\")\n    ax3.set_xlabel(\"Prior class probability P(DEATH_EVENT=1)\")\n    \n    ax3.plot(list(prior), acc, label=\"accuracy\", marker='o', linewidth=2)\n    ax3.plot(list(prior), pre, label=\"precision\", marker='o', linewidth=1, alpha=0.5)\n    ax3.plot(list(prior), rec, label=\"recall\", marker='o', linewidth=1, alpha=0.5)\n    ax3.plot(list(prior), f1, label=\"f1 score\", marker='o', linewidth=1)\n    ax3.legend(loc=4, fontsize=8)\n    ax3.grid()\n\n    # smote\n    acc, pre, rec, f1= [], [], [], []\n    for p in prior:\n        nb = MyNaiveBayes(prior = p, categorical=(True, True, False, False, False, False, False), bw=bws[i])\n        \n        accuracy, precision, recall, f1_s = kfold.fit_predict(nb, ho_train_df, ho_train_df['DEATH_EVENT'],\n                                          threshold=0.5, resampling=\"SMOTE\", cached=\"True\")\n        acc.append(accuracy)\n        pre.append(precision)\n        rec.append(recall)\n        f1.append(f1_s)\n        \n    ax4.clear()\n    ax4.set_ylim(0, 1)\n    ax4.set_title(\"SMOTE\")\n    ax4.set_xlabel(\"Prior class probability P(DEATH_EVENT=1)\")\n    \n    ax4.plot(list(prior), acc, label=\"accuracy\", marker='o', linewidth=2)\n    ax4.plot(list(prior), pre, label=\"precision\", marker='o', linewidth=1, alpha=0.5)\n    ax4.plot(list(prior), rec, label=\"recall\", marker='o', linewidth=1, alpha=0.5)\n    ax4.plot(list(prior), f1, label=\"f1 score\", marker='o', linewidth=1)\n    ax4.legend(loc=4, fontsize=8)\n    ax4.grid()\n    \n    fig.tight_layout()\n    return [fig]\n    \nanim = animation.FuncAnimation(\n                               fig, \n                               animate_func, \n                               frames = len(bws),\n                               interval = 500, # in ms\n                               );\nHTML(anim.to_jshtml())","3d7d2d25":"#sorted([(a, b) for a,b in zip(sm_acc, sm_f1)], key=lambda a: a[0]+a[1])","47a437a7":"# original\nno_rs_acc, no_rs_pre, no_rs_rec, no_rs_f1= [], [], [], []\nfor p in prior:\n    nb = GaussianNB(priors=[1-p, p])\n    nb.fit(ho_train_df[all_features], ho_train_df['DEATH_EVENT'])\n    preds = nb.predict(ho_val_df[all_features])\n    no_rs_acc.append(accuracy_score(preds, ho_val_df['DEATH_EVENT']))\n    no_rs_pre.append(precision_score(preds, ho_val_df['DEATH_EVENT']))\n    no_rs_rec.append(recall_score(preds, ho_val_df['DEATH_EVENT']))\n    no_rs_f1.append(f1_score(preds, ho_val_df['DEATH_EVENT']))\n    \n\n# oversampling\nrs_acc, rs_pre, rs_rec, rs_f1= [], [], [], []\nfor p in prior:\n    nb = GaussianNB(priors=[1-p, p])\n    nb.fit(ho_train_df_rs[all_features], ho_train_df_rs['DEATH_EVENT'])\n    preds = nb.predict(ho_val_df[all_features])\n    rs_acc.append(accuracy_score(preds, ho_val_df['DEATH_EVENT']))\n    rs_pre.append(precision_score(preds, ho_val_df['DEATH_EVENT']))\n    rs_rec.append(recall_score(preds, ho_val_df['DEATH_EVENT']))\n    rs_f1.append(f1_score(preds, ho_val_df['DEATH_EVENT']))\n\n# smote\nsm_acc, sm_pre, sm_rec, sm_f1= [], [], [], []\nfor p in prior:\n    nb = GaussianNB(priors=[1-p, p])\n    nb.fit(ho_train_df_sm[all_features], ho_train_df_sm['DEATH_EVENT'])\n    preds = nb.predict(ho_val_df[all_features])\n    sm_acc.append(accuracy_score(preds, ho_val_df['DEATH_EVENT']))\n    sm_pre.append(precision_score(preds, ho_val_df['DEATH_EVENT']))\n    sm_rec.append(recall_score(preds, ho_val_df['DEATH_EVENT']))\n    sm_f1.append(f1_score(preds, ho_val_df['DEATH_EVENT']))","c76f9fb9":"#sorted([(a, b) for a,b in zip(sm_acc, sm_f1)], key=lambda a: a[0]+a[1])","30bc18bf":"# k fold\nprint(\"K-Fold cross-validation metrics (5 folds)\")\n\nplt.figure(figsize=(12, 4))\n\n# original\nacc, pre, rec, f1= [], [], [], []\n    \nfor p in prior:\n    nb = GaussianNB(priors=[1-p, p])\n    accuracy, precision, recall, f1_s = kfold.fit_predict(nb, ho_train_df, ho_train_df['DEATH_EVENT'],\n                                          threshold=0.5, resampling=None)\n    acc.append(accuracy)\n    pre.append(precision)\n    rec.append(recall)\n    f1.append(f1_s)\n    \nplt.subplot(1, 3, 1)\nplt.ylim(0, 1)\nplt.title(\"Original\")\nplt.xlabel(\"Prior class probability P(DEATH_EVENT=1)\")\n    \nplt.plot(list(prior), acc, label=\"accuracy\", marker='o', linewidth=2)\nplt.plot(list(prior), pre, label=\"precision\", marker='o', linewidth=1, alpha=0.5)\nplt.plot(list(prior), rec, label=\"recall\", marker='o', linewidth=1, alpha=0.5)\nplt.plot(list(prior), f1, label=\"f1 score\", marker='o', linewidth=1)\nplt.legend()\nplt.grid()\n\n# oversampling\nacc, pre, rec, f1= [], [], [], []\n    \nfor p in prior:\n    nb = GaussianNB(priors=[1-p, p])\n    accuracy, precision, recall, f1_s = kfold.fit_predict(nb, ho_train_df, ho_train_df['DEATH_EVENT'],\n                                          threshold=0.5, resampling=\"oversampling\")\n    acc.append(accuracy)\n    pre.append(precision)\n    rec.append(recall)\n    f1.append(f1_s)\n    \nplt.subplot(1, 3, 2)\nplt.ylim(0, 1)\nplt.title(\"Random oversampling\")\nplt.xlabel(\"Prior class probability P(DEATH_EVENT=1)\")\n    \nplt.plot(list(prior), acc, label=\"accuracy\", marker='o', linewidth=2)\nplt.plot(list(prior), pre, label=\"precision\", marker='o', linewidth=1, alpha=0.5)\nplt.plot(list(prior), rec, label=\"recall\", marker='o', linewidth=1, alpha=0.5)\nplt.plot(list(prior), f1, label=\"f1 score\", marker='o', linewidth=1)\nplt.legend()\nplt.grid()\n\n# smote\nacc, pre, rec, f1= [], [], [], []\n    \nfor p in prior:\n    nb = GaussianNB(priors=[1-p, p])\n    accuracy, precision, recall, f1_s = kfold.fit_predict(nb, ho_train_df, ho_train_df['DEATH_EVENT'],\n                                          threshold=0.5, resampling=\"SMOTE\", cached=True)\n    acc.append(accuracy)\n    pre.append(precision)\n    rec.append(recall)\n    f1.append(f1_s)\n\nplt.subplot(1, 3, 3)\nplt.ylim(0, 1)\nplt.title(\"SMOTE\")\nplt.xlabel(\"Prior class probability P(DEATH_EVENT=1)\")\n    \nplt.plot(list(prior), acc, label=\"accuracy\", marker='o', linewidth=2)\nplt.plot(list(prior), pre, label=\"precision\", marker='o', linewidth=1, alpha=0.5)\nplt.plot(list(prior), rec, label=\"recall\", marker='o', linewidth=1, alpha=0.5)\nplt.plot(list(prior), f1, label=\"f1 score\", marker='o', linewidth=1)\nplt.legend()\nplt.grid()\n\nplt.tight_layout()","899d5710":"no_rs_acc, no_rs_pre, no_rs_rec, no_rs_f1= [], [], [], []\nrs_acc, rs_pre, rs_rec, rs_f1= [], [], [], []\nsm_acc, sm_pre, sm_rec, sm_f1= [], [], [], []\n\n\n# original\nfor p in prior:\n    nb = GaussianNB(priors=[1-p, p])\n    accuracy, precision, recall, f1_s = kfold.fit_predict(nb, ho_train_df, ho_train_df['DEATH_EVENT'],\n                                              threshold=0.5, resampling=None)\n    no_rs_acc.append(accuracy)\n    no_rs_pre.append(precision)\n    no_rs_rec.append(recall)\n    no_rs_f1.append(f1_s)\n\n    # oversampling\nfor p in prior:\n        nb = GaussianNB(priors=[1-p, p])\n\n        accuracy, precision, recall, f1_s = kfold.fit_predict(nb, ho_train_df, ho_train_df['DEATH_EVENT'],\n                                              threshold=0.5, resampling=\"oversampling\")\n        rs_acc.append(accuracy)\n        rs_pre.append(precision)\n        rs_rec.append(recall)\n        rs_f1.append(f1_s)\n\n        # smote\nfor p in prior:\n        nb = GaussianNB(priors=[1-p, p])\n\n        accuracy, precision, recall, f1_s = kfold.fit_predict(nb, ho_train_df, ho_train_df['DEATH_EVENT'],\n                                              threshold=0.5, resampling=\"SMOTE\", cached=\"True\")\n        sm_acc.append(accuracy)\n        sm_pre.append(precision)\n        sm_rec.append(recall)\n        sm_f1.append(f1_s)\n","79e3d74f":"#sorted([(a, b) for a,b in zip(sm_acc, sm_f1)], key=lambda a: a[0]+a[1])","99f9dfe9":"Then we know that:\n\n\\begin{align}\n{X^2 = \\sum_{i=1}^k \\frac{(o_i - e_i)^2}{e_i}}\n\\end{align}\n\nwhere $o_i$ is the observed value and $e_i$ is the expected value of the i-th combination of features.<br>\nAs the number of samples goes to infinity, $X^2$ tends to a $\\chi^2$ distribution with (columns-1)\\*(rows-1) degrees of freedom (so in this case 1).<br>\n\nIn the `scipy` implementation, the statistic is further corrected with **Yates correction** for continuity that, according to the [documentation](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.stats.chi2_contingency.html) (and [definition](https:\/\/en.wikipedia.org\/wiki\/Chi-squared_test#Yates's_correction_for_continuity)), consists in subtracting 0.5 from the absolute difference, in the numerator.<br>\nIn this way, the statistic for `anaemia` is 0.617.\n<br>\n\nNow that we have the statistic we could compare it with a **chi2 distribution** with those given degrees of freedom, fixing an alpha value and keeping only the features that produce a lower p-value (rejected). \n\n","3aac4ae3":"The best model seems to be the decision tree on the original dataset with `class_weight = balanced` with a `depth` of 6.","caf8a0e9":"Let's take a look to the dataset","aafa0beb":"##### Out Of Bag score (OOB)\nOut of bag score is equal to 1 - the **out of bag error**.<br>\nConsidering a sample ${z_i=(x_i, y_i)}$ the out of bag error is the average error for each ${z_i}$ evaluated using only trees that **do not** contain ${z_i}$ in their training set.<br>\nSo, for each sample, we compute the error considering only trees not trained on that sample.<br>\nThis allows us to do a sort of vaidation during training.","e23b1193":"Despite the rule of thumb, different bandwidths will be evaluated.","f561a376":"<a id='Standardization'><\/a>\n## Standardization <a style=\"text-decoration:none\" href=\"#Exploratory_data_analisys\">\u2934<\/a>\nOne important step is standardization for numerical features that is performed via the standard `sklearn` function, removing the mean and scaling to unit variance.\n\n\\begin{align}\n{z = \\frac{(x\u2212\\bar{x})}{\\hat{\\sigma}}}\n\\end{align}\n\nwhere $\\bar{x}$ is the mean of the training samples and ${\\hat{\\sigma}}$ is the sample standard deviation.","763694fd":"Now let's apply it on our actual classification task.","0abaa8ca":"To show how SMOTE works let's plot the samples according to `ejection_fraction` and `serum_creatinine`.\nIn the visualization below, SMOTE is performed on the whole dataset while then, when we use it during classification, we perform it only on training samples (as stated for random oversampling, using it on the whole dataset leads to data leakage).<br>The same happens in the kfold case in which resampling methods are applied inside each iteration and not before.\n<br>\n<br>\nA jittering is added on the x axis to better see the distribution.","174f0b15":"We can see that for the mutual information, the vast majority of the cases the features are kept or dropped analogously to the holdout feature election so we stick with the original feature selection removing `platelets`. <br>\n\nFor the chi-squared test the results are quite different, so it's decided to keep the top 2 for each iteration.","18e5821d":"that indicates how samples are distributed among these two features. ","a6b2f39b":"Here are listed the main libraries used:\n* **Numpy**: standard library for math operations\n* **Scipy**: used to compute test statistics and distributions\n* **Pandas**: used to manipulate data inside dataframes and for basic computations\n* **Sklearn**: used to apply different ML models to the data\n* **Pyplot** to plot visualizations\n* **Seaborn** built on top of pyplot (nicer visualizations)\n\nOther libraries:\n* **random**: used to generate random numbers\n* **HTML and matplotlib.animation**: used for the animations","c2672227":"Best models are re-trained on the whole training set and then evaluated on the validation set.\n\n<a id='Polynomial_kernel'><\/a>\n### Polynomial kernel - best models","bd03df00":"<a id='Support_vector_machine'><\/a>\n## Support Vector Machine <a style=\"text-decoration:none\" href=\"#Classification_models\">\u2934<\/a>\n\nSupport vector machine is a powerful model used for both classification and regression.<br>\nIt consists in trying to fit an hyperplane that best divides the dataset into the two classes by maximizing the margin (the distance between the hyperplane and the closest points).\n\n![svm.png](attachment:svm.png)\n#### Hard margin\nThe simplest implementation is the hard margin SVM in which data needs to be linearly separable to allow the algorithm to converge.<br>\nConsidering the hyperplane described by the vector **w** such that:<br>\n\n\\begin{align}\n{L=\\{v:\\langle w,v\\rangle+ b= 0\\}}\\space , \\space \u2016w\u2016=1\n\\end{align}\n\nthe distance of a point fom the hyperplane L can be evaluated in that way:<br>\n\n\\begin{align}\n{d(x,L) =| \\langle w,x \\rangle+ b |}\n\\end{align}\n\nwhile the distance between two points of two different classes on the margin is:\n\n\\begin{align}\n\\frac{<x_{+} - x_{-}, W>}{\\left \\| W \\right \\|} = \\frac{(<x_{+}, W> + \\space b) - (<x_{-}, W> + \\space b)}{\\left \\| W \\right \\|} = \\frac{2}{\\left \\| W \\right \\|}\n\\end{align}\n\nSo we need to find w and b such that that distance is **maximized** (considering the whole training set) and at the same time all the points are **classified correctly**.<br>\nEquivalently:<br>\n\n\\begin{align}\n{min_{w,b} \\frac{1}{2}\u2016w\u2016^2 \\space\\space s.t. \\space\\space \\forall i,\\space\\space y_i \\big(\\langle w,x_i\\rangle+b \\big)>1}\n\\end{align}\n\nwhere $y_i$ is the true label and the prediction is the evaluated distance of the sample from the hyperplane.\nBasically if the label and the prediction have the same sign it means that the prediction is on the correct side of the margin.<br>\nWhen the algorithm has converged, then the model can be described using only the points on the margin (called **support vectors**). This means that SVM can scale well.<br>\nBeing able to respect this constraint means having a linearly separable problem.<br>\nThe main drawback is that, in the real world, the vast majority of the problems are not linearly separable, and an algorithm like this one would not converge.<br>\nFor this reason we can add a term on the constraints to relax them.\n\n#### Soft margin\nIn this implementation, a \n<span style=\"color:red\">relaxation<\/span>\nis added to the constraint.<br>\n\n\\begin{align}\n{\nmin_{w,b} \\Big(\\frac{1}{2} \u2016W\u2016^2 + {\\color{red} {C  \\sum_{i=1}^{m} {\\xi_i}}}\\Big)\n\\space \\space\ns.t.\n\\space \\space\n\\forall i,\\space\\space y_i \\big(\\langle w,x_i\\rangle+b \\big)\u2265 1 \\color{red}{\u2212\\xi_i\\space\\space and \\space\\space \\xi_i\u22650}\n} \n\\end{align}\n\n$\\xi_i$ is called **slack variable**, it is the distance of $x_i$ from the corresponding class's margin if $x_i$ is on the wrong side of the margin and 0 otherwise.<br>\nSmaller values of C allows more errors in exchange of a bigger margin, while higher values can be used where it's needed to be less permissive regarding misclassificatons, with an higher risk of overfitting.\n\nIn some cases, problems are not well separable on the original feature space but they are separable on another space.<br>\nTherefore it's possible to map our features in another higher dimensional space in which hopefully it will be easier to learn.<br>\nOne simple example is the polynomial mapping:<br>\n\nA degree k polynomial is \n$p(x)=\\sum_{j=0}^{k}w_jx^j\\space$\n\nthat can also be seen as:\n$\\space \\langle w,\\psi(x)\\rangle \\space \\space where \\space \\space \\psi(x) = (1,x,x^2,...,x^k)$<br>\n\n$\\psi(x)$ is the polynomial mapping function\n\nThe problems arise in the case of multiple predictors.<br>\nIn fact in a polynomial mapping on a $\\rm I\\!R^2$ space:\n\n$ \\psi(\\textbf{x}) = (x_1^2, x_2^2, \\sqrt2x_1x_2)$ <br>\n(considering only the quadratic features)\n\nThis means that, in the cases in which there are a lot of features, mapping every time from the original to the new space could be costly (second order features in 1000 dimensions are around $5\\cdot10^5$ numbers)<br>\nThe solution is using kernel functions.<br>\n\n#### Kernel trick\nTo solve the svm optimization problem, computing inner products it's needed.<br>\nIn fact, according to **representer theorem**, $w$ can be written as:\n\n${w = \\sum_{i} \\alpha_i\\psi(x_i)}\\space$\n\nso it follows that: <br>\n\n$\\space \u2016w\u2016^2 = \\langle \\sum_j{\\alpha_j\\psi(x_j)}, \\sum_j{\\alpha_j\\psi(x_j)}\\rangle = \\sum_{i,j=1}^{m}\\alpha_i\\alpha_j\\langle\\psi(x_i),\\psi(x_j)\\rangle $\n\nand the problem now consists in maximizing the margin over $\\alpha$.<br>\nOnce $\\alpha$ vector is obtained, **support vectors** correspond to samples that have an alpha value that is greater than zero (so to describe the model, only those samples are needed).\n\nConsidering the mapping, this operation could be unfeasible.<br>\nA **kernel function** is a function that implements the inner product in the new feature space (given $\\psi$):<br>\n\n${ K(\\textbf{x},\\textbf{x}^\\prime) =\\langle\\psi(\\textbf{x}),\\psi(\\textbf{x}^\\prime)\\rangle}$\n\nIn this way we don't have to explicitly apply ${\\psi}$ on our data and then compute the inner product.<br>\n\nA symmetric function $K: X\\times X \\rightarrow \\rm I\\!R$ can be a kernel function if and only if it respects the **Mercer theorem** that says that the Gram matrix, that is the matrix such that:\n${G_{i,j} = K (\\textbf{x}_i, \\textbf{x}_j)}$ needs to be **positive semidefinite**.\n\nA matrix is semidefinite if and only if:\n\n${\\textbf{x}^T G \\textbf{x} > 0\\space \\space\\forall \\textbf{x}\\in R^n \\backslash 0}$<br>\nand the aigenvalues are **non-negative**.\n\n<br><br>\nThe most used kernels are:\n* the **polynomial kernel** (degree k)<br>\n${(\\langle \\textbf{x}, \\textbf{x}^\\prime \\rangle + 1)^k }$\n\n\n* the **radial basis function** (gaussian kernel)<br>\n${e^{-\\gamma \u2016\\textbf{x} - \\textbf{x}^\\prime\u2016^2}}$\n\nLinear kernel means that no mapping is done and the kernel is simply ${\\langle \\textbf{x}, \\textbf{x}^\\prime\\rangle}$","7c5d9e27":"My implementation of **SMOTE** is based on the original paper by N. V. Chawla *et al.* [[2]](#references).<br>\n\nWe can see how it works when the sample rate is higher. <br>\nIn this case the imbalance is not so strong so, just with a sample rate of 1 we obtain a good balance between classes.","b03f01b6":"Here we can see how the tree grows according to the depth constraint.<br>\nHaving trained the tree with unnormalized features, from the visualization we can see the split policies on the original feature values, providing so a clearer explaination.","058b198c":"<a id='Random_forest'><\/a>\n## Random forest <a style=\"text-decoration:none\" href=\"#Classification_models\">\u2934<\/a>\n\nRandom forest is an ensemble model based on a number of decision trees.<br>\nThe decision trees are trained with data, sampled **with repetition** from the original dataset (bagging).<br>\n**Bootstrapping** training data in fact allow us to decrease the variance of the model without increasing the bias (average of many trees is less sensitive to noise with respect to single tree) and helps us to train trees that are less correlated.<br>\n(Each tree will be trained on average on 63.2% of training data)<br>\n**Feature bagging** is also performed: this means that at each candidate split, only a subset of features (traditionally the square root or the log2 of the total number) is considered.<br>\nThis forces the models to select different features, increasing uncorrelation.<br>\nPrediction is then done by majority voting.","7f24489f":"<a id='weights_original'><\/a>\n### weights = \"original\"","4fd6b5d2":"When the number on samples for each part is 1, the method is called **leave-1-out**.","318abf27":"<a id='RBF_kernel'><\/a>\n### RBF kernel - best models","b9627ece":"<a id='Linear_kernel'><\/a>\n### Linear kernel","5274fa89":"To better see how soft margin svm works with different kernels, we can see the classification according to `serum_creatinine` and `ejection_fraction`.<br>\nThen the same algorithm will be tested on all the 7 features of the original, oversampled and SMOTE dataset.","c3cf6e6b":"<a id='K_nearest_neighbors'><\/a>\n## K Nearest Neighbors <a style=\"text-decoration:none\" href=\"#Classification_models\">\u2934<\/a>\nKNN model tries to classify new points according to the class of the nearest neighbors.<br>\nNearest neighbors are evaluated according to a **distance metric** function and for each new point, only a fixed number of neighbors are taken into account.<br>\nThis model is quite simple but it doesn't scale well.<br>\nIn fact, to predict a new point is necessary to store the entire dataset, so when the number of features or the number of records is very high, the computation could be heavy.<br>\nConsidering the way KNN learns, it is possible to re-train an already trained model on new data.<br>\n<br>\nThe parameters taken into account are, of course, the number of neighbors but also the distance metric that is the **minkowski distance**:<br>\n\n${\\Big(\\sum_{i=1}^{n}{|x_i-y_i|^p}\\Big)^{1\/p}}$\n\nin which the parameter $p$ is changed.<br>\nThen is also possible to **weight** the neighbors according to their distance from the new point.(`weights=distance`)<br>","c4a3be59":"Before applying Naive Bayes we need to test that all the features are mutually independent, conditional on the target.<br>\nWe already tested the linear independence of the features with the correlation matrix, now we can perform the same correlation matrix but considering the samples **given the target label**.","613cab00":"${\\displaystyle \\rho _{X_1,X_2}={\\frac {\\operatorname {cov} (X_1,X_2)}{\\sigma _{X_1}\\sigma _{X_2}}}}$ \n\nWhere $cov$ stands for the covariance measure:\n\n${\\displaystyle \\operatorname {cov} (X_1,X_2)=\\operatorname {E}{{\\big [}(X_1-\\operatorname {E} [X_1])(X_2-\\operatorname {E} [X_2]){\\big ]}},}$\n\nThat is computed for every pair of features ${X_1}$ and ${X_2}$","628addca":"For brevity `creatinine_phosphokinase` will be renamed `CPK`.","c2e0b3af":"<a id='Gaussian_naive_bayes'><\/a>\n### Gaussian Naive Bayes\nAnyway, even if the hypothesis seems to be not respected, a Gaussian Naive Bayes is performed.<br>\nIn the Gaussian Naive Bayes we assume that our continouos values associated with the class are distributed according to a normal distribution.\n\n\\begin{align}\np(X_i=x_i|C = k) = \\frac{1}{\\sqrt{2\\pi\\sigma_{k}^{2}}} e^{-\\frac{(x_i-\\mu_k)^2}{2\\sigma_k^2}}\n\\end{align}\nwhere $x_i$ is the observation value and $\\mu_k$ and $\\sigma_k^2$ are respectively the sample mean and sample variance of the values in $X_i$ associated with class $k$.\n\nAs we can see, the results are still quite comparable with the other models.<br>\nIn fact one very nice positive aspect of Naive Bayes in general is the good performances even in cases in which some of the hypothesis are not respected.","37d6c9c9":"The **null hypothesis** is that `anaemia` and `DEATH_EVENT` are independent, so we compute the expected values considering them as under **the null hypothesis**, so:\n\n\\begin{align}\n{P(anaemia=0, DEATH\\_EVENT=0) = P(anaemia=0)\\times P(DEATH\\_EVENT=0) = \\frac{128 \\times 159}{224 \\times 224} = 0.406}\n\\end{align}\n\n\n\\begin{align}\n{E(anaemia=0, DEATH\\_EVENT=0) = P(anaemia=0, DEATH\\_EVENT=0) \\times N = 0.406 \\times 224 = 90.9}\n\\end{align}","cf8d7312":"#### Bandwidth rule of thumb\nThere is also a rule of thumb for the selection of the best bandwidth in the case of **gaussian kernel** and **gaussian estimated distribution**.<br>\n\n\\begin{align}\nh = \\bigg(\\frac{4 \\hat\\sigma^5}{3n}\\bigg)^{\\frac{1}{5}} \\approx 1.06\\hat\\sigma n ^ {-\\frac{1}{n}}\n\\end{align}\n\nEven if gaussian assumptions seems to be [not respected](#normality_assumptions) we can see the results for our features:","0f244258":"<a id='Decision_tree'><\/a>\n## Decision tree <a style=\"text-decoration:none\" href=\"#Classification_models\">\u2934<\/a>\n\nDecision trees are one of the most widely known machine learning models.<br>\nThey are non-parametric models that learn by recursively split the predictor space (and so the train samples) according to the **best** feature (greedy approach) until the tree reaches a constrained depth, the subsets contain elements of only one class or it meets another stopping criterion (e.g. less than 5 samples in the subset).<br>\nThe **best** feature is the feature that, if used to discriminate samples, allows us to obtain the best possible split according to a measure.<br>\nSuch measure in this case is the **Gini Index** (in this sklearn implementation the unnormalized gini index is used)<br>\n\n${G = 1-\\sum_{j} p_j^2}$  \nwhere $p$ is the ratio between number of samples of class $j$ and total number of samples.\n\nGini index is evaluated on both the splits, weighted by the number of samples in each split and the feature that gives us the lower overall Gini index is chosen.<br>\n\nDue to the nature of this model, the explainability is quite high. Is in fact possible to plot the tree to see how the dataset is split at every step and it's easy to describe how the prediction works.<br>\nThe main drawback of decision tree is the fact that sometimes is a too simple model, that provides lower values of accuracy and it's easy to overfit.<br>\nTo tackle this problem it's possible to train an *ensemble* of decision trees, called **random forest**.","f5441e87":"Usually, **linear regression is not suggested for binary classification** because it can output values below 0 or above 1 (nonsense, assuming probabilistic meaning).<br>\nIn this case I decided to perform it anyway to **compare the results** with another regression technique called **logistic regression** that instead is very suitable for binary classification.","a054b242":"<a id='references'><\/a>\n# References  <a style=\"text-decoration:none\" href=\"#index\">\u2934<\/a>\n\n[[1]](https:\/\/bmcmedinformdecismak.biomedcentral.com\/articles\/10.1186\/s12911-020-1023-5)  D. Chicco, G. Jurman. \"Machine learning can predict survival of patients with heart failure from serum creatinine and ejection fraction alone\", 2020\n\n\n[[2]](https:\/\/arxiv.org\/abs\/1106.1813) N. V. Chawla, K. W. Bowyer, L. O. Hall, W. P. Kegelmeyer. SMOTE: Synthetic Minority Over-sampling Technique, 2002\n\n\n[[3]](https:\/\/dl.acm.org\/doi\/10.5555\/2074158.2074196) G. H. John, P. Langley. Estimating Continuous Distributions in Bayesian  Classifiers, 1995","bb7129a4":"<a id='KFold_case'><\/a>\n### KFold case\n\nWhen kfold crossvalidation is done, feature selection needs to be performed inside every iteration, in fact, performing it before would mean considering the whole training set and this can lead to an overestimation of the accuracy during cross validation.<br>\nIn this case, feature selection consists in dropping features that have a very low mutual information and chi squared statistic so it's possible to evaluate those for each iteration of KFold.","d85efc6b":"[An online version of this notebook is available here](https:\/\/nbviewer.jupyter.org\/github\/lorenzodenisi\/Heart-Failure-Clinical-Records\/blob\/master\/Heart%20Failures.ipynb) <br>","78b65440":"To be consistent with the feature description let's represent the `platelets` as kiloplatelets\/mL","f1b1677a":"To evaluate the probability of a sample given the class, it's important to know the feature distributions.\nThis is a fundamental prior knowledge on features, picking the wrong distributions can leads to wrong results.\n\nOur dataset is composed by a mix of binary and continouos features.<br>\nFor the first ones, a **bernoulli distribution** is the easy pick, while for the others an analisys has to be done.<br>\nNaive bayes can achieve good results with categorical features so one approach could be **binning** the numerical features into bins and then treat those bins as categories, but the problem is that in that way you risk to lose some important information.<br>\nAnother approach consists in estimating the continouos distributions.\nAltough some of them seems to be gaussians, others are not (`ejection_fractions` seems to be bimodal).<br>\nTo tackle this problem it's possible to estimate the distributions without trying to fit an already known one, using **kernel density estimation**.\n\nThis technique has been proposed in 1995 by **John and Langley** [[3](#references)] under the name of **Flexible Bayes**.\n\nSo, to evaluate Naive Bayes classifier, a model based on bernoulli and kde distributions is implemented.<br>\nThen, to compare the results, also the classic **Gaussian Naive Bayes** is tested.","4e49eb65":"We can clearly see how using some rebalancing techniques the **f1 score** increase substancially.<br>\nIn some cases SMOTE performs better with respect to random oversampling, and the opposite in others.<br>\nFurthermore, where is possible to apply it, also the use of the `class-weight` parameter increases the performances, sometimes outperforming the other techniques.\n\nThen we noticed how using Gaussian Naive Bayes, even without respecting the hypothesis, leads to good results and also with a Bayes Classifier with KDE, the results are in line.\n\nBest overall model seems to be the **random forest** trained on the oversampled dataset, that delivers the best results in terms of accuracy and f1 score.<br>\nAlso **RBF-SVM** with `class-weights=balanced` provides some good results on KFold.\n\nFor the models that allow it, it's possible to evaluate the **ROC curve** to select a threshold according to the main goal (minimize false positives or maximize true positives) but the results in the table are obtained by fixing the threshold at 0.5.\n\nThe overall results seem in line with the ones obtained in the reference paper [[1]](#references) but it's needed to keep in mind that the metrics are highly influenced by the small dimension of the dataset (75 samples in holdout validation set).","f2059fe6":"#### Results\nFrom the results we can say that `smoking`, `high_blood_pressure`, `diabetes` and `platelets` can be easily dropped.<br>\nAltough `sex`and `anaemia` produce still very low values of the statistic, they are kept.\n(In this way we can see how models handle a mix of continouos and binary features)","f76b5a12":"<a id='Class_imbalance'><\/a>\n## Class imbalance <a style=\"text-decoration:none\" href=\"#Exploratory_data_analisys\">\u2934<\/a>\n\nOne thing to take into account is the possible class imbalance.<br>","04ebe392":"<a id='Exploratory_data_analisys'><\/a>\n\n# **Exploratory data analisys** <a style=\"text-decoration:none\" href=\"#index\">\u2934<\/a>","31000b48":"<a id='Smote'><\/a>\n### Smote  \n\nSynthetic Minority Oversampling Technique or SMOTE is a useful technique used to deal with unbalanced datasets.\nIt consists in taking for each sample of the minority class the k-nearest neighbours an then synthesize new samples starting from the sample and one of the nearest neighbours (chosen randomly)","84a90146":"# Heart Failure clinical records \n\n\nHeart failure occurs when the heart is not able to pump enough blood to the body.<br>\nHF are only a subgroup of all the **cardiovascular diseases** that comprehend also coronary heart diseases (heart attacks), cerebrovascular diseases (strokes) and other pathologies that altogether kill every year  approximately **17 million people** around the world.<br>\n\n\n\nMachine learning applied to medical records can be useful to  predict the survival of a patient, highlighting patterns and even ranking the features to understand which are **risk factors**, possibly undetectable by doctors.<br>\nIn this notebook the analisys will be done starting from an **EDA** to understand the dataset and applying some **preprocessing** to be able to learn properly from it.<br>\nThen will follow a number of **machine learning models** trained on the preprocessed dataset, aiming to predict the **survival** of patients that suffered HF.<br>\nThe results are presented at the end of the notebook. ([click here to go to the end](#Results_and_conclusions))","071671ee":"<a id='Feature_distributions'><\/a>\n## Feature distributions <a style=\"text-decoration:none\" href=\"#Exploratory_data_analisys\">\u2934<\/a>\nWe can see how features are distributed according to label.","f801fe94":"<a id='index'><\/a>\n### Index\n*  [Exploratory Data analisys](#Exploratory_data_analisys)\n\n    * [Imports](#Imports)\n    * [Dataset](#Dataset)\n    * [Feature distributions](#Feature_distributions)\n    * [Standardization](#Standardization)\n    * [Correlation matrix](#Correlation_matrix)\n    * [Normality Assumptions](#normality_assumptions)\n    * [Validate the models](#validate_the_models)\n    * [Feature selection](#Feature_selection)\n        * [Mutual information](#Mutual_information)\n        * [Chi-squared test](#Chi_squared_test)\n        * [KFold case](#KFold_case)\n    * [Class imbalance](#Class_imbalance)\n        * [Random oversampling](#oversampling)\n        * [Smote](#Smote)\n        * [Class-weight parameter](#Class_weight)\n        \n    \n*  [Classification models](#Classification_models)\n    * [Decision tree](#Decision_tree)\n    * [Random forest](#Random_forest)\n    * [Linear regression](#Linear_regression)\n    * [Logistic regression](#Logistic_regression)\n    * [Support vector machine](#Support_vector_machine)\n        * [Linear kernel](#Linear_kernel)\n        * [Polynomial and RBF kernels](#Polynomial_and_RBF_kernels)\n        * [Polynomial kernel](#Polynomial_kernel)\n        * [RBF kernel](#RBF_kernel)\n    * [K nearest neighbors](#K_nearest_neighbors)\n        * [weights=\"original\"](#weights_original)\n        * [weights=\"distance\"](#weights_distance)\n    * [Naive bayes](#Naive_bayes)\n        * [Kernel density estimation](#Kernel_density_estimation)\n        * [Naive bayes with KDE and bernoulli (Flexible bayes)](#naive_bayes_with_KDE_and_bernoulli)\n        * [Gaussian Naive Bayes](#Gaussian_naive_bayes)\n    \n    \n*  [Results and conclusions](#Results_and_conclusions)\n*  [References](#references)","1e540359":"<a id='Polynomial_and_RBF_kernels'><\/a>\n### Polynomial and RBF kernels\n<a id='Gridsearch on C and gamma'><\/a>\n#### Gridsearch on C and gamma\n\nGamma is a kernel coefficient used in both polynomial and RBF kernels.<br>\nFor RBF kernels, gamma is the coefficient multiplied to the squared norm of $x-x^\\prime$ while for polynomial kernel, in the `libsvm` implementation (the one used by sklearn library), the kernel function is ${(\\gamma \\langle \\textbf{x}, \\textbf{x}^\\prime \\rangle + 1)^k }$\n\nGridsearch model selection is done with KFold crossvalidation.","4d004246":"From the correlation matrices we can see that, apart from some pairs of features (`anaemia`-`CPK`, `sex`-`ejection_fraction`), the vast majority seems to be strongly uncorrelated.","f307cb12":"Here are reported the **categorical** features","2ea70c82":"<a id='Kernel_density_estimation'><\/a>\n### Kernel density estimation\n\nKDE is a technique used to estimate the probability distribution of a continouos random variable starting from a finite set of observations.<br>\nA KDE weights a defined density around each observation.<br>\n\n\\begin{align}\n{\n\\hat{f_h}(x)=\\frac{1}{n} \\sum_{i=1}^{n}K_h(x-x_i) = \\frac{1}{nh} \\sum_{i=1}^{n}\\frac{K(x-x_i)}{h} \n}\n\\end{align}\n\nPractically, it consists in summing, for each observation, the same function K called kernel, centered on that observation.<br>\nA further parameter $h$ called **bandwidth** is used to control the *smothing effect* of the kde.\n\n![kde.png](attachment:kde.png)\n\nOne common example of kernel is a normal kernel but also other functions can be used such as triangular, Epanechnikov or uniform.<br>\n\nHere we can see how the *bandwidth* works.","66f3448f":"<a id='Chi_squared_test'><\/a>\n### Chi-squared test\n\nFor the categorical features we can further test the dependence w.r.t. the target with a **chi-squared test**.<br>\nA lower value of the statistic means a stronger independence.\n\nChi-squared test is performed starting from the contingency table, for instance:","061f3cc4":"Here we can see the results on our dataset:","a8025c17":"<a id='Correlation_matrix'><\/a>\n## Correlation matrix <a style=\"text-decoration:none\" href=\"#Exploratory_data_analisys\">\u2934<\/a>\n\nTo see how features are correlated is useful to look at the correlation matrix that is a matrix in which are showed the correlation values of each couple of features according to the **Pearson's correlation coefficient**:","44a4a367":"| Model  | Holdout Original | Holdout Oversampling | Holdout SMOTE | Holdout class-weight=balanced | KFold Original | KFold Oversampling | KFold SMOTE | KFold class-weight=balanced| \n| :------------- | :-------------: | :-------------: | :-------------: | :-------------: | :-------------: | :-------------: | :-------------: | :-------------: |\n| **Decision Tree** | 0.706 (0.607) | 0.733 (0.655) | 0.706 (0.645) | 0.747 (0.698) | 0.790 (0.560) | 0.794 (0.687) | 0.754 (0.598) | 0.785 (0.664) | \n| **Random Forest** | 0.707 (0.577) | **0.787 (0.733)** | 0.747 (0.667) | 0.733 (0.642) | 0.803 (0.625) | **0.808 (0.683)** | 0.794 (0.654) | 0.799 (0.617) | \n| **Linear Regression** | 0.667 (0.444) | 0.693 (0.635) | 0.707 (0.633) | - | 0.776 (0.508) | 0.727 (0.611) | 0.750 (0.614) | - | \n| **Logistic Regression** | 0.667 (0.444) | 0.707 (0.645) | 0.733 (0.667) | 0.707 (0.645) | 0.772 (0.501) | 0.736 (0.607) | 0.759 (0.630) | 0.740 (0.619) | \n| **Linear SVM** | 0.653 (0.458) | 0.720 (0.667) | 0.720 (0.644) | 0.706 (0.656) | 0.736 (0.594) | 0.759 (0.606) | 0.781 (0.519) | 0.718 (0.574) | \n| **Poly SVM** | 0.680 (0.538) | 0.693 (0.582) | 0.640 (0.542) | 0.706 (0.607) | 0.759 (0.503) | 0.759 (0.562) | 0.763 (0.589) | 0.754 (0.536) | \n| **RBF SVM** | 0.680 (0.571) | 0.680 (0.657) | 0.720 (0.657) | 0.747 (0.698) | 0.790 (0.542) | 0.781 (0.669) | 0.794 (0.680) | **0.799 (0.693)** | \n| **KNN original** | 0.640 (0.501) | 0.720 (0.644) | 0.680 (0.586) | - | 0.772 (0.471) | 0.737 (0.603) | 0.763 (0.604) | - | \n| **KNN distance** | 0.667 (0.510) | 0.733 (0.667) | 0.793 (0.610) | - | 0.776 (0.485) | 0.737 (0.599) | 0.759 (0.601) | - | \n| **Flexible Bayes** | 0.733 (0.730) | 0.733 (0.714) | 0.747 (0.716) | - | 0.799 (0.631) | 0.772 (0.616) | 0.785 (0.611) | - | \n| **Gaussian Naive Bayes** | 0.693 (0.667) | 0.707 (0.686) | 0.733 (0.688) | - | 0.781 (0.653) | 0.763 (0.619) | 0.727 (0.606) | - | ","35209aea":"Firstly we plot the **numerical** features (omitting `time` because is not used in the prediction).<br>\nIn this case we plot the kernel density estimation with a **kdeplot** to better see the distribution along with the **boxplot**.","d89081a2":"<a id='Naive_bayes'><\/a>\n## Naive Bayes <a style=\"text-decoration:none\" href=\"#Classification_models\">\u2934<\/a>\n\nNaive bayes classifier is a probabilistic model based on the Bayes theorem in which is assumed a strong independence between the features.<br>\nThe concept is that every feature independently contribute to the final prediction.<br>\nOur goal is to find:\n\n\\begin{align}\n{p(C=k|x_1, \\dots, x_n)}\n\\end{align}\n\nthat is the probability of having the class k, given the sample features (considered independent).<br>\n**Bayes theorem** tells us that:\n\n\\begin{align}\n{p(C=k|X=x)=\\frac{p(C=k)p(X=x|C=k)}{p(X=x)}}\n\\end{align}\n\nBasically we have to compute the **feature distributions given the class** $p(X=x|C=k)$, while $p(X=x)$ in this case is called **evidence** and is evaluated in this way: $p(X=x)=\\sum_k p(C=k)p(X=x|C=k)$.<br>\n$p(C=k)$ is the **prior**.\nThis represents our prior knowledge on the class distribution. We will see that, changing the prior, the results may be very different.\n\nOnce the class probabilities given the sample are computed, one common strategy is to pick the most probable one:\n\n\\begin{align}\n{\\hat{y} = argmax_{k \\in {1, \\dots, K}} \\space p(C=k) \\prod_{i=1}^{n}p(X_i=x_i|C=k)}\n\\end{align}\nwhere  the **evidence** is ignored because is the same for every class.<br>\nSince we assume feature independence:\n\n${p(X=x|C=k})=\\prod_{i=1}^{n}p(X_i=x_i| C=k)$","e038ac35":"In this case, applying an hypotethical alpha value equal to 0.05, all the numerical features could be considered non normal.<br>\nThis holds for the samples conditional on the class but also considering all of them, as we can see from the table.","065945f8":"|Iteration|Feature 1| Feature 2|\n|:---:|---|---|\n|**Iteration 1**|anaemia|diabetes|\n|**Iteration 2**|sex|smoking|\n|**Iteration 3**|anaemia|sex|\n|**Iteration 4**|anaemia|smoking|\n|**Iteration 5**|sex|smoking|","89eae134":"This is an example of how a two feature decisison tree work.<br>\ns.c stands for `serum creatinine` while e.j. stands for `ejection fraction`.","8af7ce93":"| Feature  | Explanation | Measurement | Range |\n| :------------- | :------------- | :------------- | :------------- |\n| `age`   | Age of the patient | Years | (40, ..., 95) |\n| `anaemia`  | Decrease of red blood cells or hemoglobin<br> (haematocrit levels were lower than 36%)  | Boolean  |  0, 1 |\n| `creatinine_phosphokinase` | Level of the CPK enzyme in the blood | mcg\/L | (23, ..., 7861) |\n| `diabetes` | If the patient has diabetes | Boolean | 0, 1 |\n| `ejection_fraction` | Percentage of blood leaving the heart<br> at each contraction | Percentage | (14, ..., 80) |\n| `high_blood_pressure`  | If a patient has hypertension | Boolean | 0, 1 |\n| `platelets` | Platelets in the blood | kiloplatelets\/mL | (25.01, ..., 850.00) |\n| `serum_creatinine` | Level of creatinine in the blood | mg\/dL | (0.50, ..., 9.40) |\n| `serum_sodium` | Level of sodium in the blood |  mEq\/L | 114, ..., 148 |\n| `sex` | Woman or man | Binary | 0, 1 |\n| `smoking` | If the patient smokes | Boolean | 0, 1 |\n| `time` | Follow-up period | Days | (4, ..., 285) |\n| `DEATH_EVENT` | If the patient died during the follow-up period |  Boolean | 0, 1 |","6428dc59":"As we can see, even if not so strong, there is a class imbalance.<br>\nThis can leads to biased results that can be noticed by measures such as `recall`, `precision` or `f1`.<br>\nTo handle class inbalance it's possible to *re-balance* the dataset with different techniques.\n\n<a id='oversampling'><\/a>\n### Random oversampling\n\nRandom oversampling is a resample technique that consists in taking the under-represented class samples and sampling new samples from them until the classes are balanced.<br>\nDuring training, random oversampling needs to be done after the subdivision into train, validation and test to avoid **data leakage**.<br>\nThe idea is that, not being able to sample more samples from the true distribution, we sample them from the empirical distribution coming from the samples that we already have.","529b263b":"As we can see already from the unnormalized distribution plot of the features, the most informative ones seem to be `ejection_fraction` and `serum_creatinine`.<br>\nThis is confirmed by the fact that in the original paper by Chicco and Jurman [[1](#references)] the analisys has been conducted taking into account only these two features.","1f70bae9":"<a id='Feature_selection'><\/a>\n## Feature selection <a style=\"text-decoration:none\" href=\"#Exploratory_data_analisys\">\u2934<\/a>\n<a id='Mutual_information'><\/a>\n### Mutual information\n\nMutual information can be useful when it's needed to assess the dependence of a feature with respect to the target.<br>\n**MI** is always > 0  and higher values indicate stronger dependence.<br>\nFormally:\n\n\\begin{align}\n{I(X;Y) =\\sum_{y \\in Y}\\sum_{x \\in X}p_{X, Y}(x, y)log\\Bigg( \\frac {p_{X, Y}(x, y)}{p_X(x)p_Y(y)} \\Bigg)}\n\\end{align}\n\n\\begin{align}\n{I(X;Y) =\\int_{y}\\int_{x}p_{X, Y}(x, y)log\\Bigg( \\frac {p_{X, Y}(x, y)}{p_X(x)p_Y(y)} \\Bigg)dx dy}\n\\end{align}\n\n\nIn this case the mutual information is estimated for each feature with respect to the class label `DEATH_EVENT`.<br>\nMutual information is estimated with the function `mutual_info_classif` from `sklearn` that, according to the  [documentation](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.mutual_info_regression.html), exploits entropy estimation from k-nearest neighbors distances. ","b74c624b":"<a id='weights_distance'><\/a>\n### weights = \"distance\"","a31e4f8a":"<a id='Dataset'><\/a>\n##  Dataset <a style=\"text-decoration:none\" href=\"#Exploratory_data_analisys\">\u2934<\/a>\n\n**Heart failure clinical records Data Set** contains the medical records of 299 patients who had heart failure.<br>\nThe dataset contains 11 clinical features (some of them are binary, others are numerical), the *follow-up* period and the label `DEATH_EVENT` that indicates whether or not the patient has died.<br>\nWe can find some features strictly related to medical aspects like levels of enzymes, sodium, creatinine and platelets in the blood and others that are more common like age, sex or smoking.\n\nThe dataset is collected in 2015 at the Allied Hospital in Faisalabad (Punjab, Pakistan)","b58192b1":"<a id='naive_bayes_with_KDE_and_bernoulli'><\/a>\n### Naive bayes with KDE and bernoulli (Flexible bayes)\n\nAs stated before, the conditional distributions are a mix of **bernoulli and kde estimations**.<br>\nThe probabilities are then multiplied to obtain the class probability given the sample.\n\nHere it's possible to see how the metrics change according the prior (on X axis) and bandwidth (on the slider).<br>\nOn the top left plot there is an example of a normal kernel (centered in zero) with the given bandwidth.","6a27a72e":"<a id='Logistic_regression'><\/a>\n## Logistic Regression <a style=\"text-decoration:none\" href=\"#Classification_models\">\u2934<\/a>\n\nLogistic regression is a generalized linear model in which the link function is not the identity (as in the case of linear regression) but is the **logit**.<br>\n\n\\begin{align}\n{logit(p) = ln(\\frac {p}{1-p})}\n\\end{align}\n\nThe **link function** is a function connecting the expected value of the response and the linear combination of predictors.<br>\nPractically:<br>\n\n\\begin{align}\n{logit(p(X)) = X\\cdot\\beta}\n\\end{align}\n\n\\begin{align}\n{p(X) = logit^{-1}(X\\cdot\\beta)}\n\\end{align}\n\n\\begin{align}\n{p(X) = S(X\\cdot\\beta)}\n\\end{align}\n\nwhere $p$ is the expected value of the prediction that in this case (binary) can be modeled as a bernoulli.<br>\n\nThe logit \"stretches\" the interval (0, 1) into the whole real line.\nThe inverse of the logit is called **sigmoid**:\n\n\\begin{align}\n{\\displaystyle S(x)={\\frac {1}{1+e^{-x}}}={\\frac {e^{x}}{e^{x}+1}}.}\n\\end{align}\nso:\n\\begin{align}\n{p(X) = \\frac{e^{X \\beta}}{1+e^{X \\beta}}}\n\\end{align}\n\n![regression.png](attachment:regression.png)\n\nIn this way every prediction is bounded between 0 and 1, assuming a probabilistic meaning.<br>\nFor this reason, logistic regression is very suitable for binary classification.","6d5cc4c7":"Here we can see the distribution of `ejection_fraction` for only under-represented class samples before and after oversampling. (the line is a kde)","4857bbee":"We can train decision trees considering the rebalanced datasets and the original one. One further model can be trained with `class_weight=\"balanced\"`<br>\nFor decision trees and random forests, normalization is not necessary, so for the sake of visualization I use the unnormalized features.","9aed665b":"<a id='Classification_models'><\/a>\n# Classification models <a style=\"text-decoration:none\" href=\"#index\">\u2934<\/a>\n\nNow will follow a series of different models used to perform classification of the `DEATH_EVENT`\n\n* [Decision tree](#Decision_tree)\n* [Random forest](#Random_forest)\n* [Linear regression](#Linear_regression)\n* [Logistic regression](#Logistic_regression)\n* [Support vector machine (linear, poly, rbf)](#Support_vector_machine)\n* [K nearest neighbors](#K_nearest_neighbors)\n* [Naive bayes](#Naive_bayes)\n        \nAll models are evaluated considering the following metrics\n\n* ${accuracy = \\frac {TP+TN}{TP+TN+FP+FN}\\quad }$\n\n\n* ${precision = \\frac {TP}{TP+FP}\\quad}$\n\n\n* ${recall = \\frac {TP}{TP+FN}\\quad}$ \n\n\n* ${F_1 = 2 \\times \\frac{precision \\times recall}{precision+recall}}$\n\n\n\nMoreover, **ROC curve** is also evaluated.<br>\nReceiver operating characteristic is a plot that shows the True Positive and False positive rates applying different thresholds on the prediction (that needs to be a number between 0 and 1).<br>\nThen, model selection can be also performed according to the Area Under the Curve (**AUC**) that is the area under the roc curve. (the bigger the better)","eec424ee":"<a id='Class_weight'><\/a>\n### Class-weight parameter\n\nAnother way to handle the class imbalance is class weighting.<br>\nThis is a parameter present in many models that allows to weight samples during training according to the imbalance.<br>\nThe configuration tested is `class-weight=\"balanced\"` that according to the [sklearn documentation](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html) sets the weights in this way:<br>\n\n${w_i = \\frac {N} {m\\times n_i}}$\nwhere ${N}$ is the total number of samples, ${m}$ is the number of classes and ***${n_i}$*** is the number of samples belonging to class i.\n\nIn our case the weights will be:","7031e847":"Now let's apply it on our training set considering all features ","82876186":"<a id='normality_assumptions'><\/a>\n\n## Normality assumptions <a style=\"text-decoration:none\" href=\"#Exploratory_data_analisys\">\u2934<\/a>\n\nSpeaking of the numerical features, it's interesting to notice whether or not they come from a normal distribution.<br>\nThis could be helpful for some models ([Gaussian Naive Bayes](#Gaussian_naive_bayes)) in which is assumed normality conditional to the class.<br>\nTo test it, a **Shapiro-Wilk test** is performed on all numerical features.<br>\nThe test statistic is:\n\n\\begin{align}\n{\nW = \\frac\n{\\Big( \\sum_{i=1}^{n} a_i x_{(i)}  \\Big)^2}\n{\\sum_{i=1}^{n} \\big( x_i - \\bar{x} \\big)^2}\n}\n\\end{align}\n\nwhere $x_{(i)}$ is the i-th smallest number in the sample, $\\bar{x}$ is the sample mean and $a_i$ are coefficients derived from a normal distributions.\n\nThe **null hypothesis** is that the samples are taken from a normal distribution, so with a sufficiently low value of the p-value we can consider the features as **not normal**.","7cf171c5":"<a id='validate_the_models'><\/a>\n\n## Validate the models <a style=\"text-decoration:none\" href=\"#Exploratory_data_analisys\">\u2934<\/a>\nTo validate and test our models we can have different approaches.<br>\nThe first one is **holdout**, that consists in dividing the dataset into subsets dedicated to training, validation and test.\nThis is useful when we have lots of data.<br>\nIn this case we have 299 records so the same subset is used both for validation and testing.<br>\nAnother approach is **K-Fold** that consists in diving the dataset into K parts with an equal number of samples, using K-1 for training and the last one as validation.<br> \nThis is done K times and each part is used as validation exactly once.<br>\nThen the results are combined (averaged).<br>\n\n![kfold.png](attachment:kfold.png)","c8d5781c":"Our goal is to find $\\beta$ such that ${|| Y -  X \\beta ||^2}$ is minimized. <br>\n\n$X$ has as many rows as the number of training samples and $p+1$ columns where $p$ is the number of predictiors, that in this case, after feature selection is 7 plus one first column that is an all one column that is used for the intercept (${\\beta_{0}}$).<br>\n\nBasically, each prediction in  $X \\beta$ is made by a linear combination of the features, weighted by $\\beta$<br>\n${y_i = \\sum_{j=0}^p{x_j\\beta_j}}$\n<br>\nthat is the prediction for the i-th observation according to the $p$ predictors.","12d3b59c":"\\begin{align}\n{Y = \\begin{bmatrix}\n    y_{0}        \\\\\n    y_{1}       \\\\\n    \\vdots  \\\\\n    y_{n}   \n\\end{bmatrix}} \\space \\space \\space \\space\n{X = \\begin{bmatrix}\n    x_{11}       & x_{12} & x_{13} & \\dots & x_{1p} \\\\\n    x_{21}       & x_{22} & x_{23} & \\dots & x_{2p} \\\\\n    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    x_{n1}       & x_{n2} & x_{n3} & \\dots & x_{np}\n\\end{bmatrix}} \\space \\space \\space \\space\n{\\beta = \\begin{bmatrix}\n    \\beta_{0}        \\\\\n    \\beta_{1}       \\\\\n    \\vdots  \\\\\n    \\beta_{p}   \n\\end{bmatrix}} \\space \\space \\space \\space\n{\\epsilon = \\begin{bmatrix}\n    \\epsilon_{0}        \\\\\n    \\epsilon_{1}       \\\\\n    \\vdots  \\\\\n    \\epsilon_{n}   \n\\end{bmatrix}}\n\\end{align}","71cc270b":"From the heatmap we can see that in general, features are quite uncorrelated with the exception of `sex` and `smoking` that seems to be slightly positively correlated. <br>\nAs we will see during feature selection, this is not a problem since one of the two will be dropped.","e37417f8":"<a id='Results_and_conclusions'><\/a>\n# Results and conclusions <a style=\"text-decoration:none\" href=\"#index\">\u2934<\/a>\n\nHere we can see the results obtained with different models and different rebalancing techniques for the **Hearth Disease** dataset.<br>\nBoth **accuracy** and **f1 score** (inside parenthesis) are showed.","c964106f":"<a id='Linear_regression'><\/a>\n## Linear Regression <a style=\"text-decoration:none\" href=\"#Classification_models\">\u2934<\/a>\n\nLinear regression is one of the simplest models in machine learning.<br>\nIt is a generalized linear model capable of fitting a linear equation to observed data.<br>\n\n![simple_linear_regression_scaled2.png](attachment:simple_linear_regression_scaled2.png)\n\n**Generalized Linear Models** are based on the following equation:<br>\n${Y=X \\beta + \\epsilon}$ \n<br>\nin which $Y$ is the response vector, $X$ is the matrix of predictors, $\\beta$ is a set of unknown parameters and $\\epsilon$ is a set of unobservable random variables called errors.","aab529df":"<a id='Imports'><\/a>\n## Imports <a style=\"text-decoration:none\" href=\"#Exploratory_data_analisys\">\u2934<\/a>"}}