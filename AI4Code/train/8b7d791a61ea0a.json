{"cell_type":{"88ecfdd8":"code","4dfd6fe8":"code","cfbaea57":"code","23c4f8eb":"code","11673a11":"code","d61e53b0":"code","9eefc46d":"code","4b73e432":"code","f1922417":"code","99b77fb3":"code","1e3fd8e4":"code","7eb8f890":"code","c39ead3d":"code","ee9d9975":"code","36eb3817":"code","36fda577":"code","a0772cd9":"code","19acded1":"code","676fab96":"code","08b2404e":"code","5a63045b":"code","6c46f958":"code","2285679f":"code","fb36e892":"code","a7af8c3f":"code","8b298c63":"code","b6d66208":"code","c22a810d":"code","2788d326":"code","09b98bd2":"code","592644b1":"code","e644cfb3":"code","719d6a3a":"code","e106bcde":"code","a3ae08ef":"code","51ded11d":"code","938554ad":"code","abf25a5d":"code","129cf332":"code","0a80b6f3":"code","464e5b2c":"code","d1a6cc36":"code","0535fe7f":"code","2e37f777":"code","5140c41e":"code","d592a64b":"code","c7b18760":"code","0ac3f135":"code","24500e51":"code","84ca0ca5":"code","17a1f0ab":"code","257abbf1":"markdown","3709f798":"markdown","0f2791f5":"markdown","c222071c":"markdown","00aa99c5":"markdown","3a3a700e":"markdown","ffbfc467":"markdown","aa72fde6":"markdown","0baf6099":"markdown","94c982fb":"markdown","e3b7a26e":"markdown","cf4f34fe":"markdown","daab5bf5":"markdown","9e130b1d":"markdown","952ab6b2":"markdown","2485db16":"markdown","785931bd":"markdown","affb432b":"markdown","3f543862":"markdown","a5cf8bea":"markdown","28816425":"markdown","d7d56997":"markdown","edb3a6d9":"markdown","7148a359":"markdown","9502c5c4":"markdown","5e953875":"markdown","c04f805e":"markdown","1bceb01b":"markdown","d274f4e5":"markdown","2205a384":"markdown","d8a52ae1":"markdown","27d7891c":"markdown","130889fd":"markdown","6d09323d":"markdown","c8e33d91":"markdown","aea86124":"markdown","6a987d6f":"markdown","e1185aba":"markdown","1458fcd4":"markdown"},"source":{"88ecfdd8":"import os\nimport torch\nfrom torchvision import datasets, models, transforms","4dfd6fe8":"mean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]","cfbaea57":"train_transform = transforms.Compose([\n                                transforms.Resize(256),\n                                transforms.RandomResizedCrop(224),\n                                transforms.RandomHorizontalFlip(),\n                                transforms.ToTensor(),\n                                transforms.Normalize(mean, std)])\n\ntest_transform = transforms.Compose([\n                                transforms.Resize(256),\n                                transforms.CenterCrop(224),\n                                transforms.ToTensor(),\n                                transforms.Normalize(mean, std)])","23c4f8eb":"basePath = '..\/input\/flowers\/flowers_'\ntrainPath = basePath + '\/train'\nos.listdir(trainPath)\n","11673a11":"data_dir = '..\/input\/flowers\/flowers_'","d61e53b0":"image_datasets ={}","9eefc46d":"image_datasets['train']= datasets.ImageFolder(data_dir + '\/train', train_transform)","4b73e432":"image_datasets['test']= datasets.ImageFolder(data_dir + '\/test', test_transform)","f1922417":"print(\"Training data size - %d\" %  len(image_datasets['train']))\nprint(\"Test data size - %d\" %  len(image_datasets['test']))","99b77fb3":"class_names = image_datasets['train'].classes\nclass_names","1e3fd8e4":"image_datasets","7eb8f890":"dataloaders ={}","c39ead3d":"dataloaders['train'] = torch.utils.data.DataLoader(image_datasets['train'],\n                                                   batch_size=8,\n                                                   shuffle=True,\n                                                   num_workers=4)","ee9d9975":"dataloaders['test'] = torch.utils.data.DataLoader(image_datasets['test'],\n                                                  batch_size=8,\n                                                  shuffle=True,\n                                                  num_workers=4)","36eb3817":"dataloaders","36fda577":"inputs, labels = next(iter(dataloaders['train']))","a0772cd9":"inputs.shape","19acded1":"labels","676fab96":"import torchvision\n\ninp = torchvision.utils.make_grid(inputs)","08b2404e":"inp.shape","5a63045b":"inp.max()","6c46f958":"import numpy as np\n\nnp.clip(inp, 0, 1).max()","2285679f":"inp.numpy().transpose((1, 2, 0)).shape","fb36e892":"import matplotlib.pyplot as plt\n\nplt.ion()","a7af8c3f":"def img_show(inp, title=None):\n    \n    inp = inp.numpy().transpose((1, 2, 0))\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    \n    plt.figure(figsize=(16,4))\n    plt.axis('off')\n    plt.imshow(inp)\n    \n    if title is not None:\n        plt.title(title)","8b298c63":"img_show(inp, title=[class_names[x] for x in labels])","b6d66208":"model = models.resnet18(pretrained=True)","c22a810d":"num_ftrs = model.fc.in_features\nnum_ftrs","2788d326":"import torch.nn as nn","09b98bd2":"model.fc = nn.Linear(num_ftrs, 5)","592644b1":"criterion = nn.CrossEntropyLoss()","e644cfb3":"import torch.optim as optim\n\noptimizer = optim.SGD(model.parameters(), \n                      lr=0.001, \n                      momentum=0.9)","719d6a3a":"from torch.optim import lr_scheduler\n\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer, \n                                       step_size=7, \n                                       gamma=0.1)","e106bcde":"def calculate_accuracy(phase, running_loss, running_corrects):\n\n    epoch_loss = running_loss \/ len(image_datasets[phase])\n    epoch_acc = running_corrects.double() \/ len(image_datasets[phase])\n\n    print('{} Loss: {:.4f} Acc: {:.4f}'.format( phase, epoch_loss, epoch_acc))\n    \n    return (epoch_loss, epoch_acc)","a3ae08ef":"def phase_train(model, criterion, optimizer, scheduler):\n    \n    scheduler.step()\n    model.train()\n    running_loss = 0.0\n    running_corrects = 0\n    \n    \n    for inputs, labels in dataloaders['train']:\n\n        optimizer.zero_grad()\n        \n        with torch.set_grad_enabled(True):\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            loss = criterion(outputs, labels)\n            \n            loss.backward()\n            optimizer.step()\n        \n        running_loss += loss.item() * inputs.size(0)\n        running_corrects += torch.sum(preds == labels.data)\n    \n    calculate_accuracy('train', running_loss, running_corrects) ","51ded11d":"import copy","938554ad":"best_acc = 0.0","abf25a5d":"def phase_test(model, criterion, optimizer):\n    \n    model.eval()\n    running_loss = 0.0\n    running_corrects = 0\n    global best_acc\n    \n    for inputs, labels in dataloaders['test']:\n\n        optimizer.zero_grad()\n        \n        with torch.no_grad():\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            loss = criterion(outputs, labels)\n        \n        running_loss += loss.item() * inputs.size(0)\n        running_corrects += torch.sum(preds == labels.data)\n        \n    epoch_loss, epoch_acc = calculate_accuracy('test', running_loss, running_corrects)\n    \n    if epoch_acc > best_acc:\n        best_acc = epoch_acc\n        best_model_wts = copy.deepcopy(model.state_dict())\n        \n    return best_model_wts","129cf332":"def build_model(model, criterion, optimizer, scheduler, num_epochs=10):\n    \n    best_model_wts = copy.deepcopy(model.state_dict())\n\n    for epoch in range(num_epochs):\n        print('Epoch {}\/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n\n        phase_train(model, criterion, optimizer, scheduler)\n        best_model_wts = phase_test(model, criterion, optimizer)\n        print()\n    \n    print('Best test Acc: {:4f}'.format(best_acc))\n\n    model.load_state_dict(best_model_wts)\n    return model","0a80b6f3":"model = build_model(model, \n                    criterion, \n                    optimizer, \n                    exp_lr_scheduler, \n                    num_epochs=1)","464e5b2c":"with torch.no_grad():\n    \n    inputs, labels = iter(dataloaders['test']).next()\n    inp = torchvision.utils.make_grid(inputs)\n    \n    outputs = model(inputs)\n    _, preds = torch.max(outputs, 1)\n    \n    for j in range(len(inputs)):\n        inp = inputs.data[j]\n        img_show(inp, 'predicted:' + class_names[preds[j]])","d1a6cc36":"# freeze the lower layers of our model and train just the higher layers\nfrozen_model = models.resnet18(pretrained=True)","0535fe7f":"# iterate over all of the model parameters and set requires_grad to false\nfor param in frozen_model.parameters():\n    # freeze those model weights and they will be updated during training\n    param.requires_grad = False","2e37f777":"# train the model parameters of the last linear layer \n# that replaces the existing last linear layer of the model and this has trainable parameters\nfrozen_model.fc = nn.Linear(num_ftrs, 5)","5140c41e":"optimizer = optim.SGD(frozen_model.fc.parameters(), \n                      lr=0.001, \n                      momentum=0.9)","d592a64b":"exp_lr_scheduler = lr_scheduler.StepLR(optimizer, \n                                       step_size=7, \n                                       gamma=0.1)","c7b18760":"criterion = nn.CrossEntropyLoss()","0ac3f135":"best_acc = 0.0","24500e51":"# instantiate classes - the stochastic gradient decent optimizer \n# the step learning rate scheduler \n# set up the CrossEntropyLoss function\n# track the best accuracy of our model\n\nfrozen_model = build_model(frozen_model, \n                           criterion, \n                           optimizer, \n                           exp_lr_scheduler, \n                           num_epochs=1)","84ca0ca5":"with torch.no_grad():\n    \n    inputs, labels = iter(dataloaders['test']).next()\n    inp = torchvision.utils.make_grid(inputs)\n    \n    outputs = frozen_model(inputs)\n    _, preds = torch.max(outputs, 1)\n    \n    for j in range(len(inputs)):\n        inp = inputs.data[j]\n        img_show(inp, 'predicted:' + class_names[preds[j]])","17a1f0ab":"#End of Code","257abbf1":"#### Create a scheduler for this model\nJust like with the earlier model, we attach a scheduler to this optimizer in order to slow down the learning rate","3709f798":"#### Define the training phase\nThis will be the training operation performed at each epoch\n\n* <b>scheduler.step()<\/b> will set up the scheduler for each step in order to decay the learning rate\n* <b>model.train()<\/b> will set the pre-trained model into training mode. This is only available for pre-trained models\n* <b>running_loss<\/b> will keep track of the loss at each iteration\n* <b>running_corrects<\/b> keeps a count of the number of correct predictions which will be used to calculate the accuracy of the model\n<br \/>\n* By setting <b>torch.set_grad_enabled(True)<\/b> we are enabling Autograd\n* <b>outputs<\/b> is the list probabilities for each possible label for the batch of images (which are the inputs). We use torch.max() to get the index of the highest probability label for each image in the batch","0f2791f5":"#### Create a grid to display the batch of images\ntorchvision is a module which contains datasets, model architectures and image transformation tools. Here, we use one of its utilities to create a grid of images from our batch ","c222071c":"#### Reset the best_acc score\nThis is so that the new frozen model can start from scratch","00aa99c5":"#### Transposing the image\nWe will use np.transpose to change the shape of image tensor<br>\n<b>.imshow()<\/b> needs a 2D array, or a 3D array with the third dimension being of size 3 or 4 only (For RGB or RGBA), so we will shift first axis to last<br>\nRef - https:\/\/matplotlib.org\/api\/pyplot_api.html#matplotlib.pyplot.imshow","3a3a700e":"#### We add the final layer which is not frozen","ffbfc467":"#### Call the build_model function\nWe set the number of epochs to 1 here, as it takes about 30 minutes for this model to be built. When running on a cloud platform VM with a GPU, we were able to run about 6 epochs and got accuracy levels upwards of 90%","aa72fde6":"#### Initialize the value of accuracy to 0\nThis will be used in the test phase to calculate the accuracy of the model","0baf6099":"### Freezing a model's layers\nHere, we freeze all the layers in the network except the final layer.<br> \nWe need to set requires_grad == False to freeze the parameters so that the gradients are not computed in backward().","94c982fb":"### Data Loaders\nData loader combines a dataset and provides single or multi-process iterators over the dataset\n* <b>batch_size<\/b> = how many samples per batch to load (default: 1).<br>\n* <b>shuffle = True<\/b>, to have the data reshuffled at every epoch <br>\n* <b>num_workers= 4<\/b>, 4 subprocesses will be used for data loading.\n0 means that the data will be loaded in the main process. (default: 0)","e3b7a26e":"#### Get a batch of training data\nWhich we will go on to examine","cf4f34fe":"Image augmentation and normalization <br>\n\n* Transforms can be chained together using Compose\n\n* In image augmentation we randomly flip images, so that our model can detect wrongly oriented images too\n\n* All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n\n* We first Resize the image to 256 then crop it to 224, so that it doesnt cut important features\n\nref-https:\/\/pytorch.org\/docs\/stable\/torchvision\/models.html#id3","daab5bf5":"#### Build the new model with the frozen layers\nThis is similar to what we did previously. Except this time, the gradient calculation is turned off for the parameters when a call is made to backward()","9e130b1d":"#### Unzip the images\nThis will unwrap the contents of flowers\\_.zip to the datasets directory. This contains two subdirectories - train and test - which contain the images for training and testing","952ab6b2":"#### Examine the maximum pixel value in the batch","2485db16":"#### The criterion to minimize in the loss function\nGiven this is a classification model, we will look to minimize the cross-entropy loss","785931bd":"#### Examine the shape\nThe grid effectively contains all the 8 images placed side by side with padding of 2 pixels between the images and at the edges of the grid","affb432b":"#### We define our custome function to display the images\nHere, we perform the transpose and clip operations discussed above. <br \/>\nThe line <i>inp = std * inp + mean<\/i> is meant to normalize the pixel values. <br \/>\n","3f543862":"### Set up the optimizer\nWe define a simple SGD optimizer with momentum which accelerate gradients vectors in the right directions and hence leads to faster converging","a5cf8bea":"#### We test a batch of flowers on this new model","28816425":"#### Decay Learning Rate by a factor of 0.1 every 7 epochs\nThis will speed up the computation as we don't need the same learning rate while getting closer to convergence","d7d56997":"#### Collecting datasets","edb3a6d9":"#### Take a look at the labels for this image batch","7148a359":"#### Check the input feature size for our linear layer\nThe final layer of our pre-trained model will be the input to a final Linear layer we need to add. To get the number of input features for our linear layer, we check the value of model.fc.in_features","9502c5c4":"#### Load the Resnet pretrained model","5e953875":"#### A function to calculate the loss and accuracy of our model\ncalculate_accuracy will calculate loss and accuracy per epoch and will be used in training as well as while testing","c04f805e":"#### We build upon our pre-trained model\n* we initialize the <b>best_model_wts<\/b> from the weights of the pre-trained model\n* perform the training and testing and update the model weights if it has supplied improved accuracy in the epoch","1bceb01b":"#### The loss calculation is the same CrossEntropyLoss as earlier\nThis is just included here as a reminder as we are not changing the definition","d274f4e5":"<b>Download link:<\/b> https:\/\/www.kaggle.com\/alxmamaev\/flowers-recognition\/home\n\n<b>Summary:<\/b> Classifier built using resnet pretrained model, requires flowers.zip (public) as dataset\n\n<b>ref: <\/b>https:\/\/pytorch.org\/tutorials\/beginner\/transfer_learning_tutorial.html#load-data","2205a384":"#### Scale the RGB values to be in the [0,1] range\nIn maltplotlib floating point image RGB values must be in the 0-1 range. To do that we will use np.clip() method","d8a52ae1":"#### Add an optimizer - for only the final layer\nThere is no gradient calculation for the other layers, so we cannot use an optimizer for those","27d7891c":"#### Adding last layer,\nWe have 5 discrete output labels, hence last layer must 5 output neurons(nodes)","130889fd":"#### Define the test phase\nThis will be the test operation performed at each epoch\n\n* <b>model.eval()<\/b> will set the pre-trained model into evaluation mode. This is only available for pre-trained models\n* <b>running_loss<\/b> will keep track of the loss at each iteration\n* <b>running_corrects<\/b> keeps a count of the number of correct predictions which will be used to calculate the accuracy of the model\n<br \/>\n* By setting <b>torch.no_grad()<\/b> we are disabling Autograd\n* <b>outputs<\/b> is the list probabilities for each possible label for the batch of images (which are the inputs). We use torch.max() to get the index of the highest probability label for each image in the batch\n<br \/>\n\nOnce the accuracy is calculated, we check to see if the accuracy in this has improved since the previous epoch. If not, we do not adjust the weights of the model. If so, we set the weights of the model to be the ones calculated in this epoch and return those values","6d09323d":"#### Lets check how our model performs\nWe will take one batch from test datasets (and we have batches of 8 images) and we will predict the correct label\n","c8e33d91":"#### Check out the shape of this batch of images\nThe first dimension gives the number of images. The next dimension represents the number of channels. The last two give the image size","aea86124":"#### Use Matplotlib to plot the image grid","6a987d6f":"#### What does our dataset look like?","e1185aba":"#### Use a generic data loader available in pytorch\nA call to ImageFolder(Path, Transform) applies our transformations to all the images in the specified directory","1458fcd4":"#### Examine the types of flowers in the dataset"}}