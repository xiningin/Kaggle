{"cell_type":{"fe090f18":"code","94ec8a99":"code","1992df8c":"code","ae05bf97":"code","bf7be9e0":"code","057ca7d0":"code","5d44ea5a":"code","23af2ab1":"code","ec77976b":"code","3119e444":"code","c4be0654":"code","ec10ce97":"code","e83a1278":"code","6cce5d35":"code","4f7c97ef":"code","52026bf8":"code","0e0aa06a":"code","f66ae357":"code","994a01ea":"code","09ac8191":"code","6dfd614b":"code","13906c87":"code","49918095":"code","1d5b8ba9":"code","84f04ddc":"code","89e16ea2":"code","2e46c644":"code","981e5737":"code","f1fc2fd8":"code","548353a5":"code","d559b7b1":"code","b9bce133":"code","0c7c10a8":"code","272c5f68":"code","58dab4e4":"code","f9ab661b":"code","62356cba":"code","8491d54d":"code","fce92dd4":"code","6e0cb77b":"code","60d40782":"markdown","1fa712a1":"markdown","f6fded38":"markdown","a24af69b":"markdown","d1ca02b6":"markdown","82df211e":"markdown","88829a35":"markdown","de542a1c":"markdown","c6a7ad90":"markdown","4564eb09":"markdown","1b57522b":"markdown","79ed96d2":"markdown","f472cc4b":"markdown","89690677":"markdown","4fb8a5d4":"markdown","b95163a6":"markdown","3ed9f1c5":"markdown","503a49e9":"markdown","6157845a":"markdown","010d20c7":"markdown","e4cab80f":"markdown","793c29d6":"markdown"},"source":{"fe090f18":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","94ec8a99":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\", index_col = \"PassengerId\")\nprint(train.shape)\ntrain.head","1992df8c":"test = pd.read_csv(\"..\/input\/titanic\/test.csv\", index_col = \"PassengerId\")\nprint(test.shape)\ntest.head()","ae05bf97":"%matplotlib inline\nimport seaborn as sns\nimport matplotlib.pyplot as plt","bf7be9e0":"# 1. by using plot\nsns.countplot(data = train, x = \"Sex\", hue = \"Survived\")","057ca7d0":"# 2. by using pivot\npd.pivot_table(train, index = \"Sex\", values = \"Survived\")","5d44ea5a":"sns.countplot(data = train, x = \"Pclass\", hue = \"Survived\")","23af2ab1":"pd.pivot_table(train, index = \"Pclass\", values = \"Survived\")","ec77976b":"sns.countplot(data = train, x = \"Embarked\", hue = \"Survived\")","3119e444":"pd.pivot_table(train, index = \"Embarked\", values = \"Survived\")","c4be0654":"sns.lmplot(data = train, x = \"Age\", y = \"Fare\", hue = \"Survived\", fit_reg = False)","ec10ce97":"# delete three outlier data\nlow_fare = train[train[\"Fare\"] < 500]\ntrain.shape, low_fare.shape","e83a1278":"sns.lmplot(data = low_fare, x = \"Age\", y = \"Fare\", hue = \"Survived\", fit_reg = False)","6cce5d35":"low_fare = train[train[\"Fare\"] < 100]\nsns.lmplot(data = low_fare, x = \"Age\", y = \"Fare\", hue = \"Survived\", fit_reg = False)","4f7c97ef":"train.loc[train[\"Sex\"] == \"male\", \"Sex_encode\"] = 0\ntrain.loc[train[\"Sex\"] == \"female\", \"Sex_encode\"] = 1\ntrain.head()","52026bf8":"test.loc[test[\"Sex\"] == \"male\", \"Sex_encode\"] = 0\ntest.loc[test[\"Sex\"] == \"female\", \"Sex_encode\"] = 1\ntest.head()","0e0aa06a":"train[train[\"Fare\"].isnull()]","f66ae357":"test[test[\"Fare\"].isnull()]","994a01ea":"train[\"Fare_fillin\"] = train[\"Fare\"]\ntrain[[\"Fare\", \"Fare_fillin\"]].head()","09ac8191":"test[\"Fare_fillin\"] = test[\"Fare\"]\ntest[[\"Fare\", \"Fare_fillin\"]].head()","6dfd614b":"test.loc[test[\"Fare\"].isnull(), \"Fare_fillin\"] = 0\n\ntest.loc[test[\"Fare\"].isnull(), [\"Fare\", \"Fare_fillin\"]]","13906c87":"train[\"Embarked_C\"] = train[\"Embarked\"] == \"C\"\ntrain[\"Embarked_S\"] = train[\"Embarked\"] == \"S\"\ntrain[\"Embarked_Q\"] = train[\"Embarked\"] == \"Q\"\ntrain[[\"Embarked_C\", \"Embarked_S\", \"Embarked_Q\"]].head()","49918095":"test[\"Embarked_C\"] = test[\"Embarked\"] == \"C\"\ntest[\"Embarked_S\"] = test[\"Embarked\"] == \"S\"\ntest[\"Embarked_Q\"] = test[\"Embarked\"] == \"Q\"\ntest[[\"Embarked_C\", \"Embarked_S\", \"Embarked_Q\"]].head()","1d5b8ba9":"feature_names = [\"Pclass\", \"Sex_encode\", \"Fare_fillin\", \"Embarked_C\", \"Embarked_S\", \"Embarked_Q\"]\nfeature_names","84f04ddc":"label_name = \"Survived\"","89e16ea2":"X_train = train[feature_names]\nprint(X_train.shape)\nX_train.head()","2e46c644":"X_test = test[feature_names]\nprint(X_test.shape)\nX_test.head()","981e5737":"y_train = train[label_name]\nprint(y_train.shape)\ny_train.head()","f1fc2fd8":"from sklearn.tree import DecisionTreeClassifier\n\nmodel = DecisionTreeClassifier(max_depth = 5)\nmodel","548353a5":"model.fit(X_train, y_train)","d559b7b1":"import graphviz\nfrom sklearn.tree import export_graphviz\n\ndot_tree = export_graphviz(model,\n                           feature_names = feature_names,\n                           class_names = [\"Perish\", \"Survived\"],\n                           out_file = None)\n\n\ngraphviz.Source(dot_tree)","b9bce133":"predictions = model.predict(X_test)\nprint(predictions.shape)\npredictions[0:10]","0c7c10a8":"submission = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\", index_col = \"PassengerId\")\nprint(submission.shape)\nsubmission.head()","272c5f68":"submission[\"Survived\"] = predictions\nsubmission.head()","58dab4e4":"# submission.to_csv(\".\/decision_tree.csv\")\n","f9ab661b":"from sklearn.utils.testing import all_estimators\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.warn(\"FutureWarning\")","62356cba":"allAlgorithms = all_estimators(type_filter = \"classifier\")\nallAlgorithms\nkfold_cv = KFold(n_splits = 5, shuffle = True)\nresult = []\nfor (name, algorithm) in allAlgorithms:\n    if(name == 'CheckingClassifier' or name == 'ClassifierChain' or \n       name == 'MultiOutputClassifier' or name == 'OneVsOneClassifier' or \n       name =='OneVsRestClassifier' or name == 'OutputCodeClassifier' or\n       name =='VotingClassifier' or name == 'RadiusNeighborsClassifier'): continue\n        \n    model = algorithm()\n    if hasattr(model, \"score\"):\n        scores = cross_val_score(model, X_train, y_train, cv = kfold_cv)\n        result.append({\"name\": name, \"mean\": np.mean(scores)})\n\nresult","8491d54d":"import operator\nsorted(result, key = operator.itemgetter(\"mean\", \"name\"))[-5:]","fce92dd4":"from sklearn.ensemble import GradientBoostingClassifier\nbestModel = GradientBoostingClassifier(max_depth = 5)\nbestModel.fit(X_train, y_train)","6e0cb77b":"bestPrediction = bestModel.predict(X_test)\nprint(bestPrediction.shape)\n\nbestSubmission = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\", index_col = \"PassengerId\")\nbestSubmission[\"Survived\"] = bestPrediction\nbestSubmission.to_csv(\".\/GradientBoostingClassifier.csv\")","60d40782":"## 3rd column : Embarked\n* There are three types of docks: 1) Cherbourg (C) 2) Queenstown (Q) and 3) Southampton (S).","1fa712a1":"# 5. Submit","f6fded38":"## 2) Encode Embarked\n\n* C == 0\n* S == 1\n* Q == 2       ---> One Hot Encoding","a24af69b":" * ### Fill in missing fare\n","d1ca02b6":"# 3. Train (by using Decision Tree)\n\n* Feature : Values that help you to get label\n        1) Ticket Class (Pclass), 2) Gender (Sex_encode), 3) Fare_fillin, and 4) Embarked.\n\n* Label : Survived or not","82df211e":"* The data visualization is not very specific (ex: what percentage of male passengers are likely to survive?) But is very intuitive because it shows a picture.\n\n\n* On the other hand, the pivot table shows a specific figure (ex: 18.9% chance of a male passenger surviving), but it's hard to intuitively understand what that figure means. (ex: So how many times different is the survival rate of female passengers compared to males?)","88829a35":"# 1. Load Dataset","de542a1c":"* The analysis shows that the more likely to survive in Sherbourg, C, the more likely to die in Southampton, S. \n\n\n\n1. To be more specific,The largest number of passengers are in Southampton, but many are killed. Nearly twice as many survivors appear to die.\n\n\n2. The number of passengers in Cherbourg is relatively small compared to Southampton, but the number of survivors is higher than that of the dead.\n\n\n3. People boarding in Queenstown have a slightly higher chance of dying, but initially have fewer passengers.","c6a7ad90":"* The higher the Pclass, the higher the chance of survival.","4564eb09":"## 4th column : Age & Fare\n","1b57522b":"# 4. Predict\n* If the Decision Tree has been trained successfully, all that remains is to use this Decision Tree to predict the survival \/ death of the passengers in the test data.","79ed96d2":"# 2. Preprocessing","f472cc4b":"### Training by using DecisionTreeClassifier","89690677":"## 2nd column : Pclass\n\n* The room class is divided into 1 class (= first class), 2 class (= business) and 3 class (= economy).","4fb8a5d4":"## 1) Encode Sex","b95163a6":"* The basic conditions for putting data in the machine learning algorithm provided by scikit-learn are:\n        1. All data should consist of numbers (integer, decimal, etc.).\n        \n        2. There should be no empty values in the data.","3ed9f1c5":"# Cross validation","503a49e9":"## 1st column : Sex","6157845a":"## os.getwcd() --> '\/kaggle\/working'","010d20c7":"* A closer look at the results shows that 1) the age is under 15 years old, and 2) passengers paying less than $20 for the fare have a relatively high survival rate.","e4cab80f":"### Visualize\n#### When the learning finished, we can visualize it to see how well it learned.","793c29d6":"* Outlier Detection -> Delete Data"}}