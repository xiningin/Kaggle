{"cell_type":{"2aaff92a":"code","664b4adb":"code","9e082fd4":"code","cd157ca1":"code","773f7b2c":"code","94e361d1":"code","c531389e":"code","da2981f4":"code","19a19c14":"code","8a41c29a":"code","f4c2101e":"code","2e14b12f":"code","cc21162e":"code","918c5d58":"code","da985c4d":"code","6d2f1ae6":"code","7fb4ba0b":"code","efc8c606":"code","4f0add1e":"code","2a24e42e":"code","224389ab":"code","9cb542a3":"code","851f5a0e":"code","4aa4f3dd":"code","33696b5b":"code","d43aa37f":"code","359d1268":"code","37235847":"code","b41dfe16":"code","c30eeb77":"code","fda6b20f":"code","a8965913":"code","728d5df6":"code","4ffaa506":"code","e10d5b50":"code","6605d91f":"code","c2ecf5a7":"code","88b2b191":"code","4e5b0fcc":"code","e94ee82a":"code","630a16c4":"code","e6042257":"code","1fc85926":"markdown","d7c79e82":"markdown","900cc55b":"markdown","1e7dd7e2":"markdown","5298d864":"markdown","f415de40":"markdown","85037a95":"markdown","e55e3dff":"markdown","c1adacda":"markdown","3f3f6aca":"markdown","59afa4d9":"markdown","8832ecba":"markdown","12540b2a":"markdown","f58241e8":"markdown","b9116818":"markdown","1aca2969":"markdown","69e8379a":"markdown","f3156fff":"markdown","66d0342b":"markdown"},"source":{"2aaff92a":"import numpy as np \nimport pandas as pd","664b4adb":"df = pd.read_csv(\"..\/input\/nlp-topic-modelling\/Reviews.csv\")","9e082fd4":"df.head(10)","cd157ca1":"df.shape","773f7b2c":"new_df = pd.DataFrame(df['Text'],columns=['Text'])\nnew_df.head(10)","94e361d1":"import string  \nimport nltk\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('punkt')\nfrom nltk.corpus import stopwords \nfrom nltk.stem import WordNetLemmatizer","c531389e":"sent = []\nfor sentence in new_df['Text']:\n    sentence_tokens = word_tokenize(sentence)\n    sent.append(sentence_tokens)","da2981f4":"sentence = sent[0]\nprint(sentence)","19a19c14":"stopwords_english = stopwords.words('english') \npunctuation = string.punctuation\n\nformatted_sent = []\nfor sentence in sent:\n    formatted_words = []\n    for word in sentence:\n        if word not in stopwords_english and word not in string.punctuation and len(word)>2:  #Removes word with less than 2 characters, present in english stop words or is a punctuation\n            formatted_words.append(word.lower())\n    formatted_sent.append(formatted_words)","8a41c29a":"sentence = formatted_sent[0]\nprint(sentence)","f4c2101e":"lemma_sent = []\n\nlemma = WordNetLemmatizer()\n\nfor sentence in formatted_sent:\n    lemma_words = []\n    for word in sentence:\n        lemma_word = lemma.lemmatize(word)\n        lemma_words.append(lemma_word)\n    lemma_sent.append(lemma_words)","2e14b12f":"sentence = lemma_sent[0]\nprint(sentence)","cc21162e":"del sent\ndel formatted_sent","918c5d58":"final_sentence = []\n\nfor sentence in lemma_sent:\n    words = ' '.join([str(word) for word in sentence])\n    final_sentence.append(words)","da985c4d":"sentence = final_sentence[0]\nprint(sentence)","6d2f1ae6":"new_df['New_Text'] = final_sentence\nnew_df = new_df.drop(['Text'], axis = 1)\nnew_df.head(10)","7fb4ba0b":"#pip install wordcloud","efc8c606":"import matplotlib.pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS","4f0add1e":"wordcloud_sentence = \"\"\n\nfor sentence in new_df['New_Text']:\n    wordcloud_sentence += \" \"+sentence+\" \"\n\nwordcloud = WordCloud(width = 2000, height = 2000,background_color ='black',min_font_size = 10).generate(wordcloud_sentence)\n\nplt.figure(figsize=(10,10))\nplt.imshow(wordcloud)\nplt.show()\n","2a24e42e":"del wordcloud_sentence\ndel wordcloud","224389ab":"from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer","9cb542a3":"train_dataset = new_df['New_Text'].to_numpy()","851f5a0e":"train_dataset[0]","4aa4f3dd":"tfidf = TfidfVectorizer()\ntrain_dataset = tfidf.fit_transform(train_dataset)","33696b5b":"from sklearn.decomposition import NMF","d43aa37f":"model = NMF(n_components=10, init='random', random_state=0) #Note n_components is the number of sentences for which we need to find topics","359d1268":"model.fit(train_dataset)","37235847":"for index, topic in enumerate(model.components_):\n    print(\"The top 5 words for sentence \"+str(index)+\" which suggests the topics are : \")\n    print([tfidf.get_feature_names()[i] for i in topic.argsort()[-5:]])","b41dfe16":"train_dataset = new_df['New_Text'].to_numpy()","c30eeb77":"train_dataset[0]","fda6b20f":"vectors = CountVectorizer()\ntrain_dataset = vectors.fit_transform(train_dataset)","a8965913":"from sklearn.decomposition import LatentDirichletAllocation","728d5df6":"lda = LatentDirichletAllocation(n_components = 10,max_iter=5,random_state = 0,n_jobs = -1)","4ffaa506":"# lda.fit(train_dataset)","e10d5b50":"# for index, topic in enumerate(lda.components_):\n#     print(\"The top 5 words for sentence \"+str(index)+\" which suggests the topics are : \")\n#     print([vectors.get_feature_names()[i] for i in topic.argsort()[-5:]])","6605d91f":"import gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS","c2ecf5a7":"lemma_sent[0]","88b2b191":"dictionary = gensim.corpora.Dictionary(lemma_sent)","4e5b0fcc":"dictionary.filter_extremes(no_below=15, no_above=0.3, keep_n=10000)","e94ee82a":"bow_corpus = [dictionary.doc2bow(doc) for doc in lemma_sent]","630a16c4":"lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)","e6042257":"for idx, topic in lda_model.print_topics(-1):\n    print('Topic: {} \\nWords: {}'.format(idx, topic))","1fc85926":"**Note** : Before passing the dataset to Sklearn NMF function, we need to vectorize it like in case of any text based modelling techniques.","d7c79e82":"**Note** : Do not pass the array of sentences (train_dataset) to the dictionary function. Pass only the multidimensional array of words.","900cc55b":"# 3. NMF (Non-negative Matrix Factorization) :","1e7dd7e2":"Here, we will need only the **Text** column since **topic modelling** is a **unsupervised learning** use case.","5298d864":"# 1. Extracting data and preprocessing :","f415de40":"# 2. Word Cloud :","85037a95":"**We can observe that most of the frequently occuring words are related to food and groceries.** \n<\/br>There are some words like href which probably indicates an hyperlink. This can be removed manually using regex functions.","e55e3dff":"**Non-Negative Matrix Factorization** is a statistical method to **reduce the dimension of the input corpora**. It uses **factor analysis** method to provide comparatively less weightage to the words with less coherence.","c1adacda":"# 4.1 Using Scikit-Learn: ","3f3f6aca":"Here, we will filter out words that occur in less than 15 documents\/sentences, in atleast 30% of all the documents, and consider only 10000 tokens for modelling:","59afa4d9":"**Training** the **LDA gensim model** on the **doc2bow output** :","8832ecba":"Lets first visualise the distribution using **WordCloud** : ","12540b2a":"# 4.2 Using Gensim :","f58241e8":"# 4. LDA (Latent Dirichlet Allocation) :","b9116818":"# 5. Finally, please upvote this notebook if you find it helpful :)","1aca2969":"**Note** : **LDA on sklearn is very slow on Kaggle Kernel**. I have tested the same on **colab notebook** and it works fine. It might take upto a day sometimes to execute on Kaggle Kernel unlike NMF which executes in a few minutes. It is better to use an alternative version of **LDA provided by the gensim library:**","69e8379a":"**doc2bow** is used to count the occurance of each word in a document:","f3156fff":"Let's **preprocess** the textual data first :","66d0342b":"In natural language processing, the **latent Dirichlet allocation (LDA)** is a **generative statistical model** that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word's presence is attributable to one of the document's topics."}}