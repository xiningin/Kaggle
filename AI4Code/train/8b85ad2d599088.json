{"cell_type":{"398a73c6":"code","71ab5508":"code","c0364c88":"code","babedea6":"code","0563013f":"code","dd9f2661":"code","9be5d9c0":"code","ffdf597e":"code","9c15f960":"code","2ff9bc7f":"code","01ab932a":"code","9b8b6425":"code","d43aaf63":"code","5e00ca00":"code","2b01b53d":"code","ff9799c9":"code","d66dadc1":"code","52b995df":"code","7964bb49":"code","003e0962":"code","0e922cf8":"code","01831d7a":"code","825c9cc7":"code","1d27af74":"code","aefb7f6c":"code","50f56a59":"code","62ec7878":"code","c1260e99":"code","90bcb4f7":"code","fa1ef0d9":"code","f2b65e6e":"markdown","285e4e24":"markdown","2d565ccc":"markdown","8cb94190":"markdown","bfbaa429":"markdown","1755827e":"markdown","d996de7f":"markdown","30af4366":"markdown","82a63c2b":"markdown","b8a53660":"markdown","b7a9a4b0":"markdown","48f99175":"markdown","0247767c":"markdown","f1823c43":"markdown","71496517":"markdown","173f10ba":"markdown","54f1b0ca":"markdown","826e825a":"markdown","7d06dbd0":"markdown","8b8c8f76":"markdown","63d11102":"markdown","6f71b689":"markdown"},"source":{"398a73c6":"import numpy as np\nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","71ab5508":"training = pd.read_csv('..\/input\/titanic\/train.csv')","c0364c88":"training.head()","babedea6":"training.loc[training['Sex'] == 'male', 'Sex_Numeric'] = 0\ntraining.loc[training['Sex'] == 'female', 'Sex_Numeric'] = 1\n\ntraining.head()","0563013f":"training['Title'] = training.Name.apply(lambda x: x.split(',')[1].split('.')[0].replace(' ',''))","dd9f2661":"training.Title.unique()","9be5d9c0":"training.loc[training['Title'] == 'Mr', 'Title_Numeric'] = 0\ntraining.loc[training['Title'] == 'Mrs', 'Title_Numeric'] = 1\ntraining.loc[training['Title'] == 'Miss', 'Title_Numeric'] = 2\ntraining.loc[training['Title'] == 'Master', 'Title_Numeric'] = 3\ntraining.loc[training['Title'] == 'Don', 'Title_Numeric'] = 4\ntraining.loc[training['Title'] == 'Rev', 'Title_Numeric'] = 5\ntraining.loc[training['Title'] == 'Dr', 'Title_Numeric'] = 6\ntraining.loc[training['Title'] == 'Mme', 'Title_Numeric'] = 7\ntraining.loc[training['Title'] == 'Ms', 'Title_Numeric'] = 8\ntraining.loc[training['Title'] == 'Major', 'Title_Numeric'] = 9\ntraining.loc[training['Title'] == 'Lady', 'Title_Numeric'] = 10\ntraining.loc[training['Title'] == 'Sir', 'Title_Numeric'] = 11\ntraining.loc[training['Title'] == 'Mlle', 'Title_Numeric'] = 12\ntraining.loc[training['Title'] == 'Col', 'Title_Numeric'] = 13\ntraining.loc[training['Title'] == 'Capt', 'Title_Numeric'] = 14\ntraining.loc[training['Title'] == 'theCountess', 'Title_Numeric'] = 15\ntraining.loc[training['Title'] == 'Jonkheer', 'Title_Numeric'] = 16\ntraining.loc[training['Title'] == 'Dona', 'Title_Numeric'] = 17","ffdf597e":"training.head()","9c15f960":"training.describe()","2ff9bc7f":"import missingno as msno\nmsno.matrix(training)","01ab932a":"training.Age = training.Age.fillna(training.Age.median())","9b8b6425":"training.groupby('Embarked')['Ticket'].nunique()","d43aaf63":"training.Embarked = training.Embarked.fillna('S')","5e00ca00":"training.loc[training['Embarked'] == 'S', 'Embarked_Numeric'] = 0\ntraining.loc[training['Embarked'] == 'Q', 'Embarked_Numeric'] = 1\ntraining.loc[training['Embarked'] == 'C', 'Embarked_Numeric'] = 2","2b01b53d":"msno.matrix(training)","ff9799c9":"training = training.drop(columns = ['Cabin'])","d66dadc1":"numeric_training = training[['Age', 'SibSp', 'Parch', 'Fare']]\ncategorical_training = training[['Survived', 'Pclass', 'Sex_Numeric', 'Title_Numeric', 'Embarked_Numeric']]","52b995df":"import seaborn as sns\nsns.heatmap(numeric_training.corr(), cmap=\"YlGnBu\")","7964bb49":"df_training = training[['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Sex_Numeric', 'Title_Numeric', 'Embarked_Numeric', 'Survived']]","003e0962":"X = df_training[['Pclass', 'Age', 'SibSp', 'Fare', 'Sex_Numeric', 'Title_Numeric', 'Embarked_Numeric']]\ny = df_training[['Survived']]","0e922cf8":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasClassifier","01831d7a":"seed = 404\nnp.random.seed(seed)","825c9cc7":"gnb = GaussianNB()\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\ncv_score = cross_val_score(gnb, X, y.values.ravel(), cv=kfold)\nprint('Gaussian Naive Bayes K-fold Scores:')\nprint(cv_score)\nprint()\nprint('Gaussian Naive Bayes Average Score:')\nprint(cv_score.mean())\nprint()","1d27af74":"lr = LogisticRegression(max_iter = 2000)\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\ncv_score = cross_val_score(lr, X, y.values.ravel(), cv=kfold)\nprint('Logistic Regression K-fold Scores (training):')\nprint(cv_score)\nprint()\nprint('Logistic Regression Average Score:')\nprint(cv_score.mean())","aefb7f6c":"dt = tree.DecisionTreeClassifier(random_state = 1)\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\ncv_score = cross_val_score(dt, X, y.values.ravel(), cv=kfold)\nprint('Decision Tree K-fold Scores:')\nprint(cv_score)\nprint()\nprint('Decision Tree Average Score:')\nprint(cv_score.mean())","50f56a59":"knn = KNeighborsClassifier()\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\ncv_score = cross_val_score(knn, X, y.values.ravel(), cv=kfold)\nprint('KNN K-fold Scores):')\nprint(cv_score)\nprint()\nprint('KNN Average Score:')\nprint(cv_score.mean())","62ec7878":"rf = RandomForestClassifier(random_state = 1)\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\ncv_score = cross_val_score(rf, X, y.values.ravel(), cv=kfold)\nprint('Random Forest K-fold Scores:')\nprint(cv_score)\nprint()\nprint('Random Forest Average Score:')\nprint(cv_score.mean())","c1260e99":"svc = SVC(probability = True)\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\ncv_score = cross_val_score(svc, X, y.values.ravel(), cv=kfold)\nprint('Support Vector Classification K-fold Scores:')\nprint(cv_score)\nprint()\nprint('Support Vector Classification Average Score:')\nprint(cv_score.mean())","90bcb4f7":"xgb = XGBClassifier(random_state =1)\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\ncv_score = cross_val_score(xgb, X, y.values.ravel(), cv=kfold)\nprint('XGBoost Classifier K-fold Scores:')\nprint(cv_score)\nprint()\nprint('XGBoost Classifier Average Score:')\nprint(cv_score.mean())","fa1ef0d9":"def create_model():\n    model = Sequential()\n    \n    model.add(Dense(7, input_dim=7, activation='relu'))\n    model.add(Dense(14, activation='relu'))\n    model.add(Dense(21, activation='relu'))\n    model.add(Dense(1, activation='sigmoid'))\n\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\nseed = 7\nnp.random.seed(seed)\n\nmodel = KerasClassifier(build_fn=create_model, epochs=150, batch_size=10, verbose=0)\n\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\ncv_score = cross_val_score(model, X, y, cv=kfold)\nprint('Neural Network K-fold Scores:')\nprint(cv_score)\nprint()\nprint('Neural Network Average Score:')\nprint(cv_score.mean())","f2b65e6e":"# Decision Tree","285e4e24":"Now that we have all the titles, we can create a column where they are converted to a numeric categorical variable.","2d565ccc":"# Building the models\n\nImport all the necessary libraries for all the models we will be testing.\nWe'll use k-fold cross validation to ensure our models are not overfitting.\n\nhttps:\/\/en.wikipedia.org\/wiki\/Cross-validation_(statistics)","8cb94190":"# Random Forest","bfbaa429":"Taking a look at all the titles in the training and testing sets.","1755827e":"# Applying multiple machine learning models to the Titanic data set\n\nThe goal of this notebook is primarily to apply multiple different machine learning models to the titanic data set. A little bit of data exploration and manipulation is done as well. Machine learning models that were applied:\n\n\nGaussian Naive Bayes: https:\/\/en.wikipedia.org\/wiki\/Naive_Bayes_classifier\n\nLogistic Regression: https:\/\/en.wikipedia.org\/wiki\/Logistic_regression\n\nDecision Tree: https:\/\/en.wikipedia.org\/wiki\/Decision_tree\n\nKNN: https:\/\/en.wikipedia.org\/wiki\/K-nearest_neighbors_algorithm\n\nRandom Forest: https:\/\/en.wikipedia.org\/wiki\/Random_forest\n\nSupport Vector: https:\/\/en.wikipedia.org\/wiki\/Support_vector_machine\n\nXGBoost: https:\/\/en.wikipedia.org\/wiki\/XGBoost\n\nNeural Networks: https:\/\/en.wikipedia.org\/wiki\/Neural_network\n\nI used the following libraries:\n\nscikit-learn: https:\/\/scikit-learn.org\/\n\nkeras: https:\/\/keras.io\/","d996de7f":"Set a random seed for reproducibility.","30af4366":"Add a column where the categorical variable 'Sex' has been converted to a numerical variable.","82a63c2b":"Take a look at the data one more time to see we covered all our bases.","b8a53660":"Possibly the title of an individual could have something to do with them surviving (Maybe Dr or Col were more or less likely to survive). Thus a column was created containing the individuals title.","b7a9a4b0":"Now we can look at the data more in detail. We'll look at each column in detail and observe any missing values.","48f99175":"It seems Random Forest and XGBoost performed the best.\n\nIf you wanted to improve performance of the models you could play with the hyperparameters for each model. If you have the time and computational power, scikit learn has a Grid Search functionality to do just that:\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html\n\nIf you enjoyed this notebook please upvote and leave a comment.\n\nThanks for reading!","0247767c":"# Neural Network","f1823c43":"# Support Vector","71496517":"# XGBoost","173f10ba":"# Gaussian Naive Bayes","54f1b0ca":"# Model preparation","826e825a":"# KNN","7d06dbd0":"# Logistic Regression","8b8c8f76":"# Data Importing, processing and visualizing","63d11102":"Clearly the 'Age', 'Cabin' and 'Embarked' columns are missing a few data points. With age we'll just fill it in with the median. For embarked, we'll identify the most commonly occuring value and use that to fill the missing values. The cabin number is missing so many and probably correlates to the price\/class of the passenger so we'll just drop that all together. Lastly we'll convert the embarked locations to numeric categorical values as we did with sex and title before.","6f71b689":"Now we'll split into our X train (predictor\/independent variables) and y train (predicted\/dependent variable)."}}