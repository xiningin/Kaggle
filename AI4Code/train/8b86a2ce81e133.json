{"cell_type":{"a18445d3":"code","14487469":"code","73e1563b":"code","262c1256":"code","e239faa4":"code","6c5623d4":"code","3761b830":"code","40ddb274":"code","238f7839":"code","64f86141":"code","50ca05ee":"code","d538f497":"code","43a85fd6":"code","8353421c":"code","dda1dae7":"code","62b64d0b":"code","c166dd65":"code","83e213e3":"code","49b0c8f4":"code","599c227a":"code","b49cf4d7":"code","7901eb68":"code","6be096bd":"code","cbe6f588":"code","b91a2d5b":"code","59effd39":"code","6a4b1d77":"code","94baa16a":"code","56dd0526":"code","72117cde":"code","24302452":"code","06e170a7":"code","16ec7f2e":"code","972fc690":"code","8e1d87cc":"code","90378303":"code","4fa68b8c":"code","aeb3bd5b":"code","5ecd910e":"code","3b2b45a3":"code","fe413085":"code","3d08b9b2":"code","1de6975c":"code","a9b926d1":"code","71a005ef":"code","6c0a75ed":"code","e61207f1":"code","8aa09af4":"code","7b161b9c":"code","75d67c29":"code","1aeb8610":"code","fc8d95b3":"code","9cd764f6":"code","81dc6b59":"code","d77f3ed0":"code","d8f5007c":"code","9751d919":"code","3fe5cb0f":"code","f7490222":"code","6a66035e":"code","9523c3ab":"code","7fb0eeaf":"markdown","057ec454":"markdown","b710382b":"markdown","5cf6b277":"markdown","5e82c7f3":"markdown","b277a031":"markdown","1f24fed1":"markdown","ec5c87b9":"markdown","7b6da04c":"markdown","d0d8467a":"markdown","e3e6612e":"markdown","237b0333":"markdown","85419132":"markdown","1746a197":"markdown","af2d0290":"markdown","b2dec424":"markdown","2ff19ba3":"markdown","94a26174":"markdown","217aa4d3":"markdown","c5503f7d":"markdown","954cf26a":"markdown","e7b6a95d":"markdown","ef3df2ef":"markdown","2af43100":"markdown","ffc5786f":"markdown","368720db":"markdown","51076f74":"markdown","839df178":"markdown","9fd4e0e2":"markdown","6d6629e7":"markdown","4e5f5419":"markdown","298ce2d7":"markdown","961c1d89":"markdown","1f45a1c6":"markdown","d1ff5e07":"markdown","8d8ea557":"markdown","dbdbe718":"markdown","1ec797a7":"markdown","41f79dee":"markdown","360b45b6":"markdown","f2e4e028":"markdown","2d6cc4d8":"markdown","0dee11aa":"markdown","11d33608":"markdown","98aa0541":"markdown","d94a01ba":"markdown","47fb508c":"markdown","b11f893b":"markdown"},"source":{"a18445d3":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom nltk.tokenize import TweetTokenizer\nimport datetime\nimport lightgbm as lgb\nfrom scipy import stats\nfrom scipy.sparse import hstack, csr_matrix\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn import metrics\nfrom wordcloud import WordCloud\nfrom collections import Counter\nimport gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\n#from nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom nltk.stem.porter import *\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.multiclass import OneVsRestClassifier\npd.set_option('max_colwidth',400)\npd.set_option('max_columns', 50)\nimport json\nimport altair as alt\nfrom  altair.vega import v3\nfrom IPython.display import HTML\nimport gc\nimport os","14487469":"# Preparing altair. I use code from this great kernel: https:\/\/www.kaggle.com\/notslush\/altair-visualization-2018-stackoverflow-survey\n\nvega_url = 'https:\/\/cdn.jsdelivr.net\/npm\/vega@' + v3.SCHEMA_VERSION\nvega_lib_url = 'https:\/\/cdn.jsdelivr.net\/npm\/vega-lib'\nvega_lite_url = 'https:\/\/cdn.jsdelivr.net\/npm\/vega-lite@' + alt.SCHEMA_VERSION\nvega_embed_url = 'https:\/\/cdn.jsdelivr.net\/npm\/vega-embed@3'\nnoext = \"?noext\"\n\npaths = {\n    'vega': vega_url + noext,\n    'vega-lib': vega_lib_url + noext,\n    'vega-lite': vega_lite_url + noext,\n    'vega-embed': vega_embed_url + noext\n}\n\nworkaround = \"\"\"\nrequirejs.config({{\n    baseUrl: 'https:\/\/cdn.jsdelivr.net\/npm\/',\n    paths: {}\n}});\n\"\"\"\n\n#------------------------------------------------ Defs for future rendering\ndef add_autoincrement(render_func):\n    # Keep track of unique <div\/> IDs\n    cache = {}\n    def wrapped(chart, id=\"vega-chart\", autoincrement=True):\n        if autoincrement:\n            if id in cache:\n                counter = 1 + cache[id]\n                cache[id] = counter\n            else:\n                cache[id] = 0\n            actual_id = id if cache[id] == 0 else id + '-' + str(cache[id])\n        else:\n            if id not in cache:\n                cache[id] = 0\n            actual_id = id\n        return render_func(chart, id=actual_id)\n    # Cache will stay outside and \n    return wrapped\n            \n@add_autoincrement\ndef render(chart, id=\"vega-chart\"):\n    chart_str = \"\"\"\n    <div id=\"{id}\"><\/div><script>\n    require([\"vega-embed\"], function(vg_embed) {{\n        const spec = {chart};     \n        vg_embed(\"#{id}\", spec, {{defaultStyle: true}}).catch(console.warn);\n        console.log(\"anything?\");\n    }});\n    console.log(\"really...anything?\");\n    <\/script>\n    \"\"\"\n    return HTML(\n        chart_str.format(\n            id=id,\n            chart=json.dumps(chart) if isinstance(chart, dict) else chart.to_json(indent=None)\n        )\n    )\n\nHTML(\"\".join((\n    \"<script>\",\n    workaround.format(json.dumps(paths)),\n    \"<\/script>\",\n)))","73e1563b":"print(os.listdir(\"..\/input\"))","262c1256":"print(os.listdir(\"..\/input\"))","e239faa4":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\nsub = pd.read_csv('..\/input\/sample_submission.csv')","6c5623d4":"train.head()","3761b830":"test.head()","40ddb274":"#We may need Time series analisys because train-data has created_date.\n#What is \"intellectual_or_learning_disability\",\"rating\",\"sexual_explicit\",\"identity_annotator_count\",\"toxicity_annotator_count\",\"publication_id\"? I have to research them.","238f7839":"train.shape","64f86141":"test.shape","50ca05ee":"train.describe()","d538f497":"test.describe()","43a85fd6":"train[\"id\"].value_counts().head()","8353421c":"train_id = train[\"id\"].astype(int)\nplt.subplots(figsize=(10, 7)) \nsns.set_context(\"poster\")\nsns.distplot(train_id,kde =False)\nplt.title('Train ID Histogram')","dda1dae7":"train[\"id\"].min()","62b64d0b":"train[\"id\"].max()","c166dd65":"test_id = test[\"id\"].astype(int)\nplt.subplots(figsize=(10, 7)) \nsns.set_context(\"poster\")\nsns.distplot(test_id,kde =False)\nplt.title('Test ID Histogram');","83e213e3":"sns.set_context(\"poster\")\ntrain['created_date'] = pd.to_datetime(train['created_date']).values.astype('datetime64[M]')\nplt.subplots(figsize=(15, 6))\nax1 = sns.lineplot(x=train['created_date'], y=train['id'],label='Train ID',color = \"red\");\nax2 = ax1.twinx()\nsns.lineplot(x=train['created_date'], y=train['target'], label='target', ax=ax2);\nh1, l1 = ax1.get_legend_handles_labels()\nh2, l2 = ax2.get_legend_handles_labels()\nax1.legend(h1+h2, l1+l2, loc='lower right');","49b0c8f4":"hist_df = pd.cut(train['target'], 20).value_counts().sort_index().reset_index().rename(columns={'index': 'bins'})\nhist_df['bins'] = hist_df['bins'].astype(str)\nrender(alt.Chart(hist_df).mark_bar().encode(\n    x=alt.X(\"bins:O\", axis=alt.Axis(title='Target')),\n    y=alt.Y('target:Q', axis=alt.Axis(title='Count')),\n    tooltip=['target', 'bins']\n).properties(title=\"Counts of target bins\", width=1000, height=500).interactive())\n","599c227a":"print(\"(target=0)\/all_target = \",(train['target'] >= 0).sum() \/ train.shape[0])\nprint(\"(target>0)\/all_target = \",(train['target'] > 0).sum() \/ train.shape[0])\nprint(\"(target>0.25)\/all_target = \",(train['target'] >= 0.25).sum() \/ train.shape[0])\nprint(\"(target>=0.5)\/all_target \",(train['target'] >= 0.5).sum() \/ train.shape[0])","b49cf4d7":"train['target'].value_counts().head(20)","7901eb68":"train['comment_text'].value_counts().head(50)","6be096bd":"test['comment_text'].value_counts().head(50)","cbe6f588":"train_plus = train\ntrain_plus['train_comment_length'] = train.comment_text.apply(len)\ntest_plus = test\ntest_plus['test_comment_length'] = test.comment_text.apply(len)","b91a2d5b":"sns.set_context(\"poster\");\nplt.figure(figsize=(15, 8));\nsns.distplot(train_plus['train_comment_length'] ,kde=False, rug=False,bins=100,norm_hist=True,label = \"train\");\nsns.distplot(test_plus['test_comment_length'] ,kde=False, rug=False,bins=100,norm_hist=True,label = \"test\",axlabel = \"comment_length\");\nplt.legend();\nplt.title(\"Histgram of train adn test comment_length with normalize\");","59effd39":"plt.figure(figsize=(8, 8));\nsns.scatterplot(x=train_plus['train_comment_length'], y=train_plus['target'],alpha = 0.5);\nplt.title('Target & Train_comment_length');","6a4b1d77":"print(\"correlation coefficients is \",np.corrcoef(train_plus['train_comment_length'],train_plus['target'])[1,0])","94baa16a":"plt.figure(figsize=(3, 8));\nsns.boxplot(y=train_plus['train_comment_length'] )\nplt.title(\"Boxplot of train_comment_length\");","56dd0526":"#Check the Number of 75% point\nq25,q50,q75= np.percentile(train_plus['train_comment_length'], [25,50,75])\niqr = q75- q25\nq100 = q75 + iqr*1.5\nprint(\"Nuber of 25% point = \",q25)\nprint(\"Nuber of 50% point = \",q50)\nprint(\"Nuber of 75% point = \",q75)\nprint(\"Nuber of 100% point = \",q100)","72117cde":"train_plus[train_plus['train_comment_length'] > 894].head()","24302452":"train_plus['comment_text'][train_plus['train_comment_length'] > 414].value_counts().head()","06e170a7":"plt.figure(figsize=(8, 8));\nsns.scatterplot(x=train_plus['train_comment_length'][train_plus['train_comment_length'] > 894], y=train_plus['target'][train_plus['train_comment_length'] > 894],alpha = 0.5);\nplt.title('Target & Train_comment_length');","16ec7f2e":"print(\"correlation coefficients is \",np.corrcoef(train_plus['train_comment_length'][train_plus['train_comment_length'] > 894],train_plus['target'][train_plus['train_comment_length'] > 894])[1,0])","972fc690":"train_plus['num_words'] = train['comment_text'].apply(lambda comment: len(comment.split()))","8e1d87cc":"plt.figure(figsize=(8, 8));\nsns.scatterplot(x=train_plus['num_words'], y=train_plus['target'],alpha = 0.5);\nplt.title('Target & Number of words');","90378303":"print(\"correlation coefficients is \",np.corrcoef(train_plus['num_words'],train_plus['target'])[1,0])","4fa68b8c":"plt.figure(figsize=(3, 8));\nsns.boxplot(y=train_plus['num_words'] )\nplt.title(\"Boxplot of num_words\");","aeb3bd5b":"#Check the Number of 100% point\nq25,q50,q75= np.percentile(train_plus['num_words'], [25,50,75])\niqr = q75- q25\nq100 = q75 + iqr*1.5\nprint(\"Nuber of 25% point = \",q25)\nprint(\"Nuber of 50% point = \",q50)\nprint(\"Nuber of 75% point = \",q75)\nprint(\"Nuber of 100% point = \",q100)","5ecd910e":"print(\"correlation coefficients is \",np.corrcoef(train_plus['num_words'][train_plus['num_words'] > 156],train_plus['target'][train_plus['num_words'] > 156])[1,0])","3b2b45a3":"train_plus['capitals'] = train['comment_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))","fe413085":"plt.figure(figsize=(8, 8));\nsns.scatterplot(x=train_plus['capitals'], y=train_plus['target'],alpha = 0.5);\nplt.title('Target & Number of capitals');","3d08b9b2":"print(\"correlation coefficients is \",np.corrcoef(train_plus['capitals'],train_plus['target'])[1,0])","1de6975c":"plt.figure(figsize=(3, 8));\nsns.boxplot(y=train_plus['capitals'] )\nplt.title(\"Boxplot of capitals\");","a9b926d1":"#Check the Number of 100% point\nq25,q50,q75= np.percentile(train_plus['capitals'], [25,50,75])\niqr = q75- q25\nq100 = q75 + iqr*1.5\nprint(\"Nuber of 25% point = \",q25)\nprint(\"Nuber of 50% point = \",q50)\nprint(\"Nuber of 75% point = \",q75)\nprint(\"Nuber of 100% point = \",q100)","71a005ef":"print(\"correlation coefficients is \",np.corrcoef(train_plus['capitals'][train_plus['capitals'] > 23],train_plus['target'][train_plus['capitals'] > 23])[1,0])","6c0a75ed":"train_plus['num_exclamation_marks'] = train['comment_text'].apply(lambda comment: comment.count('!'))","e61207f1":"plt.figure(figsize=(8, 8));\nsns.scatterplot(x=train_plus['num_exclamation_marks'], y=train_plus['target'],alpha = 0.5);\nplt.title('Target & Number of num_exclamation_marks');","8aa09af4":"print(\"correlation coefficients is \",np.corrcoef(train_plus['num_exclamation_marks'],train_plus['target'])[1,0])","7b161b9c":"plt.figure(figsize=(3, 8));\nsns.boxplot(y=train_plus['num_exclamation_marks'] )\nplt.title(\"Boxplot of num_exclamation_marks\");","75d67c29":"#Check the Number of 100% point\nq25,q50,q75= np.percentile(train_plus['num_exclamation_marks'], [25,50,75])\niqr = q75- q25\nq100 = q75 + iqr*1.5\nprint(\"Nuber of 25% point = \",q25)\nprint(\"Nuber of 50% point = \",q50)\nprint(\"Nuber of 75% point = \",q75)\nprint(\"Nuber of 100% point = \",q100)","1aeb8610":"stopwords = set(STOPWORDS)\n\ndef show_wordcloud(data, title = None):\n    wordcloud = WordCloud(\n        background_color='white',\n        stopwords=stopwords,\n        max_words=50,\n        max_font_size=40, \n        scale=5,\n        random_state=1\n    ).generate(str(data))\n\n    fig = plt.figure(1, figsize=(10,10))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize=20)\n        fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()","fc8d95b3":"show_wordcloud(train['comment_text'].sample(20000), title = 'Prevalent words in comments - train data')","9cd764f6":"show_wordcloud(test['comment_text'].sample(20000), title = 'Prevalent words in comments - test data')","81dc6b59":"show_wordcloud(train.loc[train['insult'] < 0.25]['comment_text'].sample(20000), \n               title = 'Prevalent comments with insult score < 0.25')","d77f3ed0":"show_wordcloud(train.loc[train['insult'] > 0.75]['comment_text'].sample(20000), \n               title = 'Prevalent comments with insult score > 0.75')","d8f5007c":"show_wordcloud(train.loc[train['threat'] < 0.25]['comment_text'], \n               title = 'Prevalent words in comments with threat score < 0.25')","9751d919":"show_wordcloud(train.loc[train['threat'] > 0.75]['comment_text'], \n               title = 'Prevalent words in comments with threat score > 0.75')","3fe5cb0f":"show_wordcloud(train.loc[train['obscene']< 0.25]['comment_text'], \n               title = 'Prevalent words in comments with obscene score < 0.25')","f7490222":"show_wordcloud(train.loc[train['obscene'] > 0.75]['comment_text'], \n               title = 'Prevalent words in comments with obscene score > 0.75')","6a66035e":"show_wordcloud(train.loc[train['target'] < 0.25]['comment_text'], \n               title = 'Prevalent words in comments with target score < 0.25')","9523c3ab":"show_wordcloud(train.loc[train['target'] > 0.75]['comment_text'], \n               title = 'Prevalent words in comments with target score > 0.75')","7fb0eeaf":"## Discussion\nWe understood \"target\" distribution.\n\n\u30fbThere are many 0 target.\n\n\u30fbWhy are there so many 0.15-0.20 target? We must research it later.","057ec454":"## <a id='24'>comment_text : value_count<\/a>  ","b710382b":"\u30fb\u3000ID has unique number.","5cf6b277":"# <a id='#1'>Introduction<\/a>   \n\n*Work still progress.*\n\nFirstly,I used  below kernel as a reference. Thanks!\n\n\u30fbhttps:\/\/www.kaggle.com\/gpreda\/jigsaw-eda\n\n\u30fbhttps:\/\/www.kaggle.com\/nz0722\/simple-eda-text-preprocessing-jigsaw\n\n## Backgorund\n\nIn the past competition, we made the model for perdicting \"target\" from Test data, and minimize this type of unintended bias.<br>\n\ne.g.  \"This is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!\"<br>\n \u2192terget is 0.000000<br>\n \ne.g.  \"haha you guys are a bunch of losers.\"<br>\n \u2192terget is 0.89617, because \"loser\" may be toxicity comment.  <br>\n \n&emsp;\n\nBut,that'model has unintende bias, and we have to remove this.  <br>\n\ne.g.  \"I'm a gay woman.\"<br>\n \n \nThis sentence is not toxic, but old model may be deciding toxicity comment.<br>\n\n&emsp;\n\nSo, let' make better model for removing unintend bias!<br>\n \n## Feature of my kernel\n\nIn this kernel, I conclude whether all variables are useful for making new variable or not as far as possible. \n\n\n\n## Conclusion\n\nIn this paragraph, I summarizes what I found in this kernel.\nIt will be updated as appropriate.\n\n\n\u30fb \"test.csv\" has only comment text. So I think we can make new variables only about comment_text.(I'm not confident\u30fb\u30fb\u30fb)\n\n\u30fbNew variable\u3000\"Number of words\"\u3000\"Number of Capitals\"\u3000\"Number of exclamation marks\" may be useful.\n\n\u30fbIn the comment data,there is something frong.For example \"Trump\" at highly insult, \"Damn\" at highly obscene.\n\n\u30fb\n","5e82c7f3":"## Discussion\n\nCorrelation between traget and \"capitals\" is 0.026, and Cor between target and \"capitals > 23\" is 0.064.\n\nI think the longer the number of capitals, the more likely it is that the comment is toxic.","b277a031":"# <a id='22'>target<\/a>  ","1f24fed1":"Firstly, train_data has **id** and **target**. \n* **id**: unique number for comment.\n* **target**: express toxicity determined by rater\n\nSecondly, the **comment_text** are stored in `train` and `test`.  \n* **comment_text**\n\nIn `train`, following feature expresses aggression of comment.\n* **severe_toxicity**\n* **obsence**\n* **identity_attack**\n* **insult**\n\nThe topic is also related to five categories: race or ethnicity, gender, sexual orientation, religion, disability, as following:\n* **race or ethnicity**: asian, black, jewish, latino, other_race_or_ethnicity, white  \n* **gender**: female, male, transgender, other_gender  \n* **sexual orientation**: bisexual, heterosexual, homosexual_gay_or_lesbian, other_sexual_orientation  \n* **religion**: atheist,buddhist,  christian, hindu, muslim, other_religion  \n* **disability**: intellectual_or_learning_disability, other_disability, physical_disability, psychiatric_or_mental_illness  \n\nWe also have few article\/comment identification information:\n* **created_date**  \n* **publication_id**   \n* **parent_id**  \n* **article_id** \n\nSeveral user feedback information associated with the comments are provided:\n* **rating**  \n* **funny**  \n* **wow**  \n* **sad**  \n* **likes**  \n* **disagree**  \n* **sexual_explicit**  \n\nIn the datasets are also 2 fields relative to annotations:\n* **identity_annotator_count**  \n* **toxicity_annotator_count**","ec5c87b9":"## <a id='23'>comment_text<\/a>  ","7b6da04c":"Let's check the train_data more than 100% point","d0d8467a":"Many words of direct threat are found. For kill,shoot,head.\n\nBut, dog is also threat word? I have to research it later.","e3e6612e":"# Conclusion\n\nlater.","237b0333":"## Data exploration","85419132":"Let's have a quick look at the data first","1746a197":"Many words of direct obscene are found. ass.\n\nAlso, I was surprised that the following words would become obscene\nDamn,fucking,shit,etc\u30fb\u30fb\u30fb.","af2d0290":"# <a id='20'>Data Overview<\/a>  ","b2dec424":"# Discussion\n\nFirstly, there is many Politically related words.\n\nSecondly,We found many word along each feature,\nbut there is something frong,for example \"Trump\" at highly insult, \"Damn\" at highly obscene.","2ff19ba3":"Many words of direct insult are found. For example,stupid,idot.\n\nBut, We can also find politically related words .for example Trump,President.","94a26174":"*Work still progress*","217aa4d3":"## <a id='28'>comment_text : Number of exclamation marks<\/a>","c5503f7d":"## <a id='25'>comment_text : comment_length<\/a>  ","954cf26a":"Import\n------","e7b6a95d":"## Discussion\n\nCorrelation between traget and \"num_exclamation_marks\" is 0.055.\n\nI think the longer the number of num_exclamation_marks, the more likely it is that the comment is toxic.","ef3df2ef":"*Politically related words are found*. For example Trump,Republician.","2af43100":"## Conclusion\nWe can see  defined correlation between \"target\" and \"comment_length\".\n\nSo I think this variable may be useful.","ffc5786f":"## Data exploration\nLet's have a look at the data first","368720db":"## Conclusion\n\nI think we can't make new valuable from target, because target is objective variable.","51076f74":"We can see a little correlation between target and New Variable.","839df178":"## Discussion\nHum\u30fb\u30fb\u30fb.Why so many same comment\u3000\"START WORKING \u30fb\u30fb\u30fb\" have train and test data? May this be commercial comment\uff1f \n\nWe have to research these later.","9fd4e0e2":"Next , we check outlier using boxplot.","6d6629e7":"## <a id='27'>comment_text : Number of Capitals<\/a>","4e5f5419":"## Discussion\n\nCorrelation of comment_length is 0.006, and Cor of comment_length > 894 is 0.011.\n\nI think the longer the comment, the more likely it is that the comment is toxic.","298ce2d7":"# <a id='2'>Data exploration<\/a>  ","961c1d89":"## Conclusion\nWe can see  defined correlation between \"target\" and \"capitals\".\n\nSo I think this variable may be useful stongly.","1f45a1c6":"**Let's check the 'comment_text' about each new variable!**","d1ff5e07":"\u30fb\u3000Train ID has nearly 500000 and 5500000","8d8ea557":"## Conclusion\nWe can see  defined correlation between \"target\" and \"num_words\".\n\nSo I think this variable may be useful.","dbdbe718":"## Conclusion\n\nI think we can't make new valuable from ID.","1ec797a7":"## Data exploration\nLet's have a look at the data first","41f79dee":"# <a id='21'>ID<\/a>  ","360b45b6":"## Data exploration","f2e4e028":"## Conclusion\nWe can see  defined correlation between \"target\" and \"num_exclamation_marks\".\n\nSo I think this variable may be useful.","2d6cc4d8":"## Discussion\nID increase with time, and target also increase, but I think they has no corelation between ID and target.","0dee11aa":"## Discussion\n\nCorrelation between traget and \"num_words\" is 0.0096, and Cor between target and \"num_words > 156\" is 0.017.\n\nI think the longer the num_words, the more likely it is that the comment is toxic.","11d33608":"## Conclusion\n\nlater.","98aa0541":"## <a id='29'>comment_text : Wordcloud<\/a>","d94a01ba":"This is histogram of train_comment length.\n\nWe can see rise around 1000.","47fb508c":"## <a id='26'>comment_text : Number of words<\/a>  ","b11f893b":"<h1><center><font size=\"6\">Jigsaw Unintended Bias in Toxicity Classification<\/font><\/center><\/h1>\n<h1><center><font size=\"6\">Simple EDA<\/font><\/center><\/h1>\n\n<center><img src=\"https:\/\/d1f5hsy4d47upe.cloudfront.net\/36\/36f0885cfc92b2d8abf1073f229c0047_w.jpg\" width=\"300\"><\/img><\/center>\n\n<br>\n\n# <a id='0'>Content<\/a>\n\n- <a href='#1'>Introduction<\/a>  \n- <a href='#2'>Data exploration<\/a>  \n    - <a href='#20'>Data Overview<\/a>  \n    - <a href='#21'>ID<\/a>  \n    - <a href='#22'>target<\/a>  \n    - <a href='#23'>comment_text<\/a>  \n    - <a href='#24'>comment_text : value_count<\/a>  \n    - <a href='#25'>comment_text : comment_length<\/a>  \n    - <a href='#26'>comment_text : Number of words<\/a>  \n    - <a href='#27'>comment_text : Number of Capitals<\/a>\n    - <a href='#28'>comment_text : Number of exclamation marks<\/a>\n    - <a href='#29'>comment_text : Wordcloud<\/a>"}}