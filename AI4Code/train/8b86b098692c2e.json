{"cell_type":{"c6a4882e":"code","1fdfab86":"code","a3c5037e":"code","75a407ed":"code","a17aaaff":"code","448a5fdb":"code","0d2c0efb":"code","1fd26f1d":"code","6d4479a1":"code","80a219fe":"code","3b45b15e":"code","dcd1c568":"code","0925ea41":"code","41cea1c0":"code","cb13e429":"code","705cb788":"code","29f7a8b0":"code","6078d8fc":"code","55596eac":"code","987f9a29":"code","c37eb433":"code","a794ed3c":"code","f8809612":"code","d8e9a56c":"code","43b066a9":"code","166c2e3a":"code","0b0de155":"code","b45cbc20":"code","4dc8302c":"code","7a7be03b":"code","8eb9ed04":"code","82dde7db":"code","69266a1e":"code","dadfbafc":"code","fd6d96bf":"code","5e5d3e95":"code","f5b8ad80":"code","d6fb8f65":"code","861e6007":"code","f1361f0c":"code","a56b0a6e":"code","1f8246e3":"code","64acef29":"code","f3d7a4f2":"code","5f7cf604":"code","846cfeb8":"code","23dc3895":"code","9f93ea2f":"code","6a85de88":"code","7721f5f8":"code","31e3bb35":"code","9b517723":"code","c167bd4d":"code","14cf1819":"code","438e6b33":"code","9d26ef33":"code","669e0a29":"code","bc68e5f9":"code","2e65793e":"code","cd67cbe8":"code","c7f0ebca":"code","4eab46a1":"code","169907d9":"code","8108ed59":"code","1444a395":"markdown","90a74bad":"markdown","b838cfbe":"markdown","50bd14d1":"markdown","56e67db6":"markdown","b9817f9f":"markdown","74036e50":"markdown","1ef6bde2":"markdown","a8a556bb":"markdown","ed35ba46":"markdown","39f5293c":"markdown","a38a3822":"markdown","2c8e71c6":"markdown","121ac43b":"markdown","f4c80038":"markdown","c3d1ca33":"markdown","70d666e6":"markdown","c3d7670f":"markdown","1099f33d":"markdown"},"source":{"c6a4882e":"!pip install funpymodeling","1fdfab86":"import importlib\n\nimport pandas as pd\nimport math\nimport pyarrow.feather as feather\n\nfrom sklearn.feature_selection import mutual_info_classif\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.preprocessing import StandardScaler\nfrom funpymodeling.exploratory import status\nfrom sklearn.impute import KNNImputer\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import KBinsDiscretizer\nfrom sklearn.preprocessing import PowerTransformer, MinMaxScaler\nfrom sklearn.pipeline import Pipeline\npd.options.mode.chained_assignment = None\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.cluster import KMeans\nimport torch.optim as optim\n\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import silhouette_score","a3c5037e":"# Se cargan los datasets\ntrain = pd.read_csv(\"..\/input\/airline-passenger-satisfaction\/train.csv\")\ntest = pd.read_csv(\"..\/input\/airline-passenger-satisfaction\/test.csv\")\ndataset = pd.concat([train, test], ignore_index=True)","75a407ed":"train.head()","a17aaaff":"dataset.info()","448a5fdb":"status(dataset)","0d2c0efb":"dataset.describe()","1fd26f1d":"# Define useful columns, remove IDs\nfeatures = ['Gender', 'Customer Type', 'Age', 'Type of Travel', 'Class',\n       'Flight Distance', 'Inflight wifi service',\n       'Departure\/Arrival time convenient', 'Ease of Online booking',\n       'Gate location', 'Food and drink', 'Online boarding', 'Seat comfort',\n       'Inflight entertainment', 'On-board service', 'Leg room service',\n       'Baggage handling', 'Checkin service', 'Inflight service',\n       'Cleanliness', 'Departure Delay in Minutes', 'Arrival Delay in Minutes']\ntarget = ['satisfaction']\n\n# Obtain target variable\nX_train = train[features]\ny_train = train[target]\nX_test = test[features]\ny_test = test[target]","6d4479a1":"# Convert variables to float to plot\ndef convert_float(dataset, cols):\n    dataset[cols] = dataset[cols].astype(float)","80a219fe":"cols = [\"Flight Distance\", \"Departure Delay in Minutes\", \"Age\"]\nfor data in [X_train, X_test]:\n    convert_float(data, cols)","3b45b15e":"def display_dataset_distributions(dataset, figsize=(22, 10), unique=False):\n    if unique:\n        params = {\"bins\": np.unique(dataset)}\n    else:\n        params = {}\n    fig = dataset.hist(xlabelsize=12, ylabelsize=12, figsize=figsize, **params)\n    [x.title.set_size(14) for x in fig.ravel()]\n    plt.tight_layout()\n    plt.show()\n\n\ndef display_dataset_categorical(dataset, n_rows=1, figsize=(20, 10)):\n    fig, ax = plt.subplots(\n        n_rows, int(len(dataset.columns) \/ n_rows) + 1, figsize=figsize\n    )\n    fig.tight_layout(pad=5.0)\n    for i, categorical_feature in enumerate(dataset):\n        dataset[categorical_feature].value_counts().plot(\n            kind=\"bar\", ax=ax[i \/\/ (n_rows + 1)][i % (n_rows + 1)], grid=True\n        ).set_title(categorical_feature)\n    plt.grid()","dcd1c568":"X = pd.concat([X_train, X_test], ignore_index=True)\ndisplay_dataset_distributions(X.select_dtypes(include=[\"float64\"]))","0925ea41":"display_dataset_categorical(X.select_dtypes(include=[\"object\", \"int64\"]), n_rows=4, figsize=(20, 20))","41cea1c0":"# Plot target variable\ny = pd.concat([y_train, y_test], ignore_index=True)\ny.value_counts().plot(kind=\"bar\", figsize=(5, 5), grid=True)","cb13e429":"# Check missing data in target variable\ny.isna().sum()","705cb788":"# Obtain Nans in output dataset\nX.isna().sum()","29f7a8b0":"# Percentage\nX.isna().sum() \/ X.shape[0] * 100","6078d8fc":"# Transform class\ndef transform_class(x):\n    if x == 'Business':\n        return 2\n    elif x == 'Eco Plus':\n        return 1\n    elif x == 'Eco':\n        return 0    \n    else:\n        return -1\n\n# Transform strings into 1 and 0\ndef boolean_encoding(df, column_name, mapping={\"Yes\": 1, \"No\": 0}, default=-1):\n    df[column_name] = getattr(df, column_name).map(mapping).fillna(default).astype(int)\n    return df\n\n# Transform categorical values\nfeatures = ['Gender', 'Customer Type', 'Type of Travel']\nfor data in [X_train, X_test]:\n    data = boolean_encoding(data, 'Gender', mapping={\"Male\": 1, \"Female\": 0})\n    data = boolean_encoding(data, 'Customer Type', mapping={\"Loyal Customer\": 1, \"disloyal Customer\": 0})\n    data = boolean_encoding(data, 'Type of Travel', mapping={\"Business travel\": 1, \"Personal Travel\": 0})\n    data['Class'] = data['Class'].apply(transform_class)\n\nfor data in [y_train, y_test]:\n    boolean_encoding(data, 'satisfaction', mapping={\"satisfied\": 1, \"neutral or dissatisfied\": 0})","55596eac":"status(X_train)","987f9a29":"# KNN imputation\nimputer = KNNImputer(n_neighbors=2)\nimputer.fit_transform(X_train)\nfor data in [X_train, X_test]:\n    data[:] = imputer.transform(data)\n\nstatus(X_train)","c37eb433":"def plot_box_whiskers(df, variables, row_num=4, figsize=(20, 30)):\n    rows = int(len(variables) \/ row_num)\n    cols = int(math.ceil(len(variables) \/ rows))\n    _, axes = plt.subplots(rows, cols, figsize=figsize)\n    for i in range(len(variables)):\n        row = i % rows\n        col = i \/\/ rows\n        sns.boxplot(y=df[variables[i]], ax=axes[row][col])\n        axes[row][col].set_title(variables[i])","a794ed3c":"features = ['Age', 'Flight Distance', 'Departure Delay in Minutes', 'Arrival Delay in Minutes']\nplot_box_whiskers(X_train, features, row_num=2, figsize=(10, 15))","f8809612":"from sklearn.preprocessing import KBinsDiscretizer\n\ndef discretize_variable(df, variable, n_bins=4, strategy=\"kmeans\"):\n    print\n    kbins = KBinsDiscretizer(n_bins=n_bins, encode=\"ordinal\", strategy=strategy)\n    df[variable] = kbins.fit_transform(df[variable].to_numpy().reshape(-1, 1))\n    return df\n\n# Discretize variables with high oblicuity\ndiscretize_list = ['Departure Delay in Minutes', 'Arrival Delay in Minutes']\n\nfor data in [X_train, X_test]:\n    for var in discretize_list:\n        discretize_variable(data, var, n_bins=20, strategy=\"quantile\")\n\ndisplay_dataset_distributions(X_train[discretize_list], unique=True, figsize=(15, 6))","d8e9a56c":"def discretize_variable(df, variable, n_bins=4, strategy=\"kmeans\"):\n    kbins = KBinsDiscretizer(n_bins=n_bins, encode=\"ordinal\", strategy=strategy)\n    df[variable] = kbins.fit_transform(df[variable].to_numpy().reshape(-1, 1))\n    return df\n\n\ndef eliminate_outliers_capping(df, variables):\n    for variable in variables:\n        upper_limit, lower_limit = find_skewed_boundaries(df, variable, 1.5)\n        df[variable] = np.where(\n            df[variable] > upper_limit,\n            upper_limit,\n            np.where(df[variable] < lower_limit, lower_limit, df[variable]),\n        )\n    return df\n\n\ndef find_skewed_boundaries(df, variable, distance=1.5):\n    IQR = df[variable].quantile(0.75) - df[variable].quantile(0.25)\n    lower_boundary = df[variable].quantile(0.25) - (IQR * distance)\n    upper_boundary = df[variable].quantile(0.75) + (IQR * distance)\n    return upper_boundary, lower_boundary\n\n\ndef eliminate_high_corr_columns(df, max_corr=0.95):\n    corr = df.corr()\n    indexes = np.where(np.abs(corr) > max_corr)\n    indexes = [\n        (corr.index[x], corr.columns[y]) for x, y in zip(*indexes) if x != y and x < y\n    ]\n    drop_cols = [pair[0] for pair in indexes]\n    print(f\"Eliminated columns: {drop_cols}\")\n    return df.drop(drop_cols, axis=1)\n\n\ndef get_higher_corr(df, variable, method=\"pearson\"):\n    corr = df.corr(method=method)[variable]\n    corr = corr.reindex(corr.abs().sort_values(ascending=False).index)\n    return corr","43b066a9":"# Eliminate outliers using capping\nfeatures = ['Age', 'Flight Distance', 'Departure Delay in Minutes', 'Arrival Delay in Minutes']\nfor data in [X_train, X_test]:\n    data[:] = eliminate_outliers_capping(data.copy(), features)\n\nplot_box_whiskers(X_train, features, row_num=2, figsize=(10, 15))","166c2e3a":"def plot_correlation_heatmap(df):\n    plt.subplots(figsize=(20, 20))\n    corr = df.corr()\n    mask = np.zeros_like(corr, dtype=np.bool)\n    mask[np.triu_indices_from(mask)] = True\n    sns.heatmap(corr, cmap=\"YlGnBu\", mask=mask, square=True, annot=True)\n    sns.set(font_scale=0.9)","0b0de155":"plot_correlation_heatmap(X_train)","b45cbc20":"# Eliminate highly correlated varaibles\nX_train = eliminate_high_corr_columns(X_train, max_corr=0.8)\nX_test = eliminate_high_corr_columns(X_test, max_corr=0.8)","4dc8302c":"# Get mutual information\nmutual_info = pd.DataFrame(mutual_info_classif(X_train, y_train.to_numpy().ravel()).reshape(-1, 1))\nmutual_info = mutual_info.set_index(X_train.columns)","7a7be03b":"mutual_info.sort_values(by=[0], ascending=False)","8eb9ed04":"# Eliminate variables not related to the target varaible\n#drop_cols = [\"Gender\", \"Departure\/Arrival time convenient\"]\n#X_train = X_train.drop(drop_cols, axis=1)\n#X_test = X_test.drop(drop_cols, axis=1)","82dde7db":"def label_percentage_plot(df, col_x, col_y, y_val, y_label=\"\", x_label=\"\"):\n    percentage = (\n        df[[col_x, col_y]][df[col_y] == y_val].groupby(col_x).count()\n        \/ df[[col_x, col_y]].groupby(col_x).count()\n    )\n    _, ax = plt.subplots(figsize=(18, 6))\n    plt.xticks(rotation=\"vertical\")\n    ax.bar(df[col_x].value_counts().index, df[col_x].value_counts(), color=\"lightgrey\")\n    ax2 = ax.twinx()\n    ax2.plot(percentage.index, percentage, color=\"red\")\n    ax.set_ylabel(y_label)\n    ax.set_xlabel(x_label)\n    ax2.set_ylabel(\"Percentage\")\n","69266a1e":"label_percentage_plot(pd.concat([X_train, y_train], axis=1), \"Online boarding\", \"satisfaction\", 1)","dadfbafc":"label_percentage_plot(pd.concat([X_train, y_train], axis=1), \"Inflight wifi service\", \"satisfaction\", 1)","fd6d96bf":"oversample = RandomOverSampler(sampling_strategy='minority')\nX_train, y_train = oversample.fit_resample(X_train, y_train)\ny_train.value_counts().plot(kind=\"bar\", figsize=(5, 5), grid=True)","5e5d3e95":"scaler = StandardScaler()\nX_train[:] = scaler.fit_transform(X_train)\nX_test[:] = scaler.fit_transform(X_test)","f5b8ad80":"from sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import classification_report\nfrom functools import wraps\nfrom time import time\nfrom timeit import default_timer as timer\nimport matplotlib.pyplot as plt\nimport torch\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom torch import nn\n\ndef plot_roc_curve(y_val, y_pred):\n    fpr, tpr, _ = roc_curve(y_val, y_pred)\n    auc_read = auc(fpr, tpr)\n    plt.figure()\n    plt.plot(fpr, tpr, label=f\"ROC curve (area={auc_read}')\")\n    plt.plot([0, 1], [0, 1], \"k--\")\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.title(\"ROC curve\")\n    plt.legend(loc=\"lower right\")\n    plt.show()\n    \ndef evaluate_model(model, X_train, y_train, X_test, y_test):\n    model.train(X_train, y_train)\n    model.classification_report(X_test, y_test)\n    print(\"Training time: {} s\".format(model.train_time))\n    model.plot_curve(X_test, y_test)\n\n\nclass Model:\n    def __init__(self, model_class, params):\n        self.model = model_class(**params)\n        self.train_time = None\n\n    def train(self, X, y):\n        start = timer()\n        self.model.fit(X, y)\n        end = timer()\n        self.train_time = end - start\n\n    def predict(self, X):\n        return self.model.predict(X)\n\n    def classification_report(self, X, y):\n        y_pred = self.predict(X)\n        roc_auc = roc_auc_score(y, y_pred)\n        self.roc_auc = roc_auc\n        print(\"ROC_AUC = {}\\n\".format(roc_auc))\n        print(classification_report(y, y_pred, digits=5))\n        self.report = classification_report(y, y_pred, digits=5, output_dict=True)\n        return self.report\n\n    def confusion_matrix(self, X, y):\n        y_pred = self.predict(X)\n        cm = confusion_matrix(y, y_pred)\n        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n        disp.plot()\n        plt.grid(None)\n\n    def plot_curve(self, X, y):\n        y_pred = self.predict(X)\n        plot_roc_curve(y, y_pred)\n\n\ndef run_model(model, X_train, y_train, X_test, y_test):\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    roc_auc = roc_auc_score(y_test, y_pred)\n    print(\"ROC_AUC = {}\\n\".format(roc_auc))\n    print(classification_report(y_test, y_pred, digits=5))\n    plot_confusion_matrix(model, X_test, y_test, cmap=plt.cm.Blues, normalize=\"all\")\n    plt.grid(None)\n    return model, roc_auc\n\n\ndef fit_model(model, criterion, optimizer, dataloaders, train_len, test_len, epochs=50):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(device)\n    if torch.cuda.is_available():\n        model.cuda()\n    losses_train = []\n    losses_val = []\n    running_loss = 0.0\n    start = timer()\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        correct = 0\n        correct_val = 0\n        model = model.train()\n        for i, data in enumerate(dataloaders[\"train\"]):\n            inputs, labels = data\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            preds = torch.round(torch.sigmoid(outputs))\n            optimizer.step()\n            running_loss += loss.item()\n            correct += (preds == labels).float().sum()\n        model = model.eval()\n        with torch.no_grad():\n            valid_loss = 0.0\n            for i, data in enumerate(dataloaders[\"val\"]):\n                inputs, labels = data\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                valid_loss += loss.item()\n                preds = torch.round(torch.sigmoid(outputs))\n                correct_val += (preds == labels).float().sum()\n\n        print(\n            \"[%d] training loss: %.5f, validation loss: %.5f\"\n            % (\n                epoch + 1,\n                running_loss \/ len(dataloaders[\"train\"]),\n                valid_loss \/ len(dataloaders[\"val\"]),\n            ),\n            end=\"\",\n        )\n        print(\n            \", train accuracy {}, val accuracy {}\".format(\n                100 * correct \/ train_len, 100 * correct_val \/ test_len\n            )\n        )\n        losses_train.append(running_loss \/ len(dataloaders[\"train\"]))\n        losses_val.append(valid_loss \/ len(dataloaders[\"val\"]))\n    end = timer()\n    train_time = start - end\n    return losses_train, losses_val, train_time\n","d6fb8f65":"models = {}","861e6007":"params = {\n    \"n_estimators\": 100,\n    \"random_state\": 0,\n}\n\nmodel = Model(AdaBoostClassifier, params)\nevaluate_model(model, X_train, y_train, X_test, y_test)\nmodels[\"model_ad\"] = model","f1361f0c":"model.confusion_matrix(X_test, y_test)","a56b0a6e":"params = {}\n\nmodel = Model(SVC, params)\nevaluate_model(model, X_train, y_train, X_test, y_test)\nmodels[\"model_svm\"] = model","1f8246e3":"model.confusion_matrix(X_test, y_test)","64acef29":"params = {}\n\nmodel = Model(DecisionTreeClassifier, params)\nevaluate_model(model, X_train, y_train, X_test, y_test)\nmodels[\"model_dt\"] = model","f3d7a4f2":"model.confusion_matrix(X_test, y_test)","5f7cf604":"params = {\n    'random_state': 42,\n    \"base_estimator\": DecisionTreeClassifier(),\n    \"bootstrap_features\": False,\n    \"n_estimators\": 100, \n}\n\nmodel = Model(BaggingClassifier, params)\nevaluate_model(model, X_train, y_train, X_test, y_test)\nmodels[\"model_bg\"] = model","846cfeb8":"model.confusion_matrix(X_test, y_test)","23dc3895":"params = {\n    'max_depth': 25,\n    'min_samples_leaf': 1,\n    'min_samples_split': 2,\n    'n_estimators': 1200,\n    'random_state': 42\n}\n\nmodel = Model(RandomForestClassifier, params)\nevaluate_model(model, X_train, y_train, X_test, y_test)\nmodels[\"model_rf\"] = model","9f93ea2f":"model.confusion_matrix(X_test, y_test)","6a85de88":"params = {\n    'random_state': 42\n}\n\nmodel = Model(LogisticRegression, params)\nevaluate_model(model, X_train, y_train, X_test, y_test)\nmodels[\"model_lr\"] = model","7721f5f8":"model.confusion_matrix(X_test, y_test)","31e3bb35":"params = {\n    'random_state': 0,\n    'n_clusters': 2,\n}\n\nmodel = Model(KMeans, params)\nevaluate_model(model, X_train, y_train, X_test, y_test)\nmodels[\"model_km\"] = model","9b517723":"model.confusion_matrix(X_test, y_test)","c167bd4d":"kmeans_silhouette = silhouette_score(X_train, model.model.labels_).round(2)\nprint(f\"Kmeans silhouette score: {kmeans_silhouette}\")","14cf1819":"\nclass Net(nn.Module, Model):\n    def __init__(self):\n        super(Net, self).__init__()\n\n        self.layer_1 = nn.Linear(19, 64) \n        self.layer_2 = nn.Linear(64, 64)\n        self.layer_out = nn.Linear(64, 1) \n        \n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(p=0.1)\n        self.batchnorm1 = nn.BatchNorm1d(64)\n        self.batchnorm2 = nn.BatchNorm1d(64)\n        \n    def forward(self, inputs):\n        x = self.relu(self.layer_1(inputs))\n        x = self.batchnorm1(x)\n        x = self.relu(self.layer_2(x))\n        x = self.batchnorm2(x)\n        x = self.dropout(x)\n        x = self.layer_out(x)\n        return x\n    \n    def predict(self, X):\n        X = torch.tensor(X.values).float().detach()\n        y_pred = torch.round(torch.sigmoid(self.forward(X))).detach().numpy()\n        return y_pred\n        \nmodel = Net()","438e6b33":"class CustomDataset(Dataset):\n  def __init__(self, X, Y):\n    super().__init__()\n    self.X = torch.tensor(X.values).float()\n    self.Y = torch.tensor(Y.values).float()\n\n  def __len__(self):\n    return len(self.X)\n  \n  def __getitem__(self, idx):\n    x = self.X[idx]\n    y = self.Y[idx]\n    return x, y","9d26ef33":"params = {\n    'batch_size': 64,\n    'shuffle': True,\n}\n\ntrain_set = CustomDataset(X_train, y_train)\ntest_set = CustomDataset(X_test, y_test)\n\ntrain_loader = torch.utils.data.DataLoader(train_set, **params)\ntest_loader = torch.utils.data.DataLoader(test_set, **params)","669e0a29":"dataloaders = {\"train\": train_loader, \"val\": test_loader,}\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.01,)","bc68e5f9":"losses_train, losses_val, train_time = fit_model(model, criterion, optimizer, dataloaders, len(train_set), len(test_set), epochs=5)","2e65793e":"model.classification_report(X_test, y_test)\nmodel.confusion_matrix(X_test, y_test)\nmodel.train_time = train_time\nmodels[\"model_nn\"] = model","cd67cbe8":"model.plot_curve(X_test, y_test)","c7f0ebca":"import random","4eab46a1":"model_list = [\"model_ad\", \"model_bg\", \"model_dt\", \"model_lr\", \"model_svm\", \"model_km\"]\n# Metr\u00edcas\nfor i, metric in enumerate([\"precision\", \"recall\", \"f1-score\"]):\n    plt.figure()\n    values = {model: models[model].report[\"weighted avg\"][metric] for model in model_list}\n    values = {k: v for k, v in sorted(values.items(), key=lambda item: item[1])}\n    plt.barh(list(values.keys()), list(values.values()))\n    plt.title(metric)\n    plt.xlim([0.7, 1])","169907d9":"# AUC\nplt.figure()\nvalues = {model: models[model].roc_auc for model in model_list}\nvalues = {k: v for k, v in sorted(values.items(), key=lambda item: item[1])}\nplt.barh(list(values.keys()), list(values.values()))\nplt.xlim([0.7, 1])\nplt.title(\"Training time [s]\")","8108ed59":"plt.figure()\nvalues = {model: models[model].train_time for model in model_list}\nvalues = {k: v for k, v in sorted(values.items(), key=lambda item: item[1])}\nplt.barh(list(values.keys()), list(values.values()))\nplt.title(\"Training time [s]\")","1444a395":"### Variables correlation","90a74bad":"### Feature engineering","b838cfbe":"#### Decision tree","50bd14d1":"### Clean dataset","56e67db6":"### Obtain dataset ","b9817f9f":"### Training","74036e50":"### Evaluation","1ef6bde2":"### Importing libraries","a8a556bb":"### Normalization","ed35ba46":"#### SVM","39f5293c":"### Exploratory analysis","a38a3822":"#### Random forest","2c8e71c6":"#### Adaboost","121ac43b":"#### Bagging","f4c80038":"#### Logistic regression","c3d1ca33":"#### Kmeans","70d666e6":"#### Neural network","c3d7670f":"### Outliers","1099f33d":"### Oversampling"}}