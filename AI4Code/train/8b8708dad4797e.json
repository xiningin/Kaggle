{"cell_type":{"f9f73d97":"code","ad55d794":"code","aba87ade":"code","6aa8d10f":"code","34a9438e":"code","8bca4ce8":"code","6b60cf55":"code","0685cc02":"code","d0c8d9e2":"code","915b845b":"code","5afe9e54":"code","318801c5":"code","ba827d7d":"code","b008e957":"code","edf99e53":"markdown","251b1c31":"markdown"},"source":{"f9f73d97":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ad55d794":"import re\nfrom nltk.tokenize import word_tokenize\nfrom collections import defaultdict, Counter\nimport random\nimport string\n\nimport bs4 as bs\nimport urllib.request\nimport re","aba87ade":"raw_html = urllib.request.urlopen('https:\/\/en.wikipedia.org\/wiki\/World_War_II')\nraw_html = raw_html.read()\n\narticle_html = bs.BeautifulSoup(raw_html, 'lxml')\narticle_paragraphs = article_html.find_all('p')\narticle_text = ''\n\nfor para in article_paragraphs:\n    article_text += para.text\n\narticle_text = article_text.lower()","6aa8d10f":"class MarkovChain:\n    def __init__(self):\n        self.lookup_dict = defaultdict(list)\n\n    def _preprocess(self, string):\n        cleaned = re.sub(r'\\W+', ' ', string).lower()\n        tokenized = word_tokenize(cleaned)\n        return tokenized\n\n    def add_document(self, string):\n        preprocessed_list = self._preprocess(string)\n        pairs = self.__generate_tuple_keys(preprocessed_list)\n        for pair in pairs:\n            self.lookup_dict[pair[0]].append(pair[1])\n\n    def __generate_tuple_keys(self, data):\n        if len(data) < 1:\n            return\n        for i in range(len(data) - 1):\n            yield [ data[i], data[i + 1] ]\n\n    def generate_text(self, string):\n        if len(self.lookup_dict) > 0:\n            print(\"Next word suggestions:\", Counter(self.lookup_dict[string]).most_common()[:3])\n        return","34a9438e":"#training_1 = \"How are you? How many people attend? How did it go?\"\ntraining_1 = article_text\nmy_next_word = MarkovChain()\nmy_next_word.add_document(training_1)","8bca4ce8":"#my_next_word.generate_text(input().lower())\nmy_next_word.generate_text(\"world\")","6b60cf55":"class MarkovChain:\n    def __init__(self):\n        self.lookup_dict = defaultdict(list)\n\n    def _preprocess(self, string):\n        cleaned = re.sub(r'\\W+', ' ', string).lower()\n        tokenized = word_tokenize(cleaned)\n        return tokenized\n    \n    def add_document(self, string):\n        preprocessed_list = self._preprocess(string)\n        pairs = self.__generate_tuple_keys(preprocessed_list)\n        for pair in pairs:\n            self.lookup_dict[pair[0]].append(pair[1])\n        pairs2 = self.__generate_2tuple_keys(preprocessed_list)\n        for pair in pairs2:\n            self.lookup_dict[tuple([pair[0], pair[1]])].append(pair[2])\n        pairs3 = self.__generate_3tuple_keys(preprocessed_list)\n        for pair in pairs3:\n            self.lookup_dict[tuple([pair[0], pair[1], pair[2]])].append(pair[3])\n\n    def __generate_tuple_keys(self, data):\n        if len(data) < 1:\n            return \n        for i in range(len(data) - 1):\n            yield [ data[i], data[i + 1] ]\n\n    #to add two words tuple as key and the next word as value\n    def __generate_2tuple_keys(self, data):\n        if len(data) < 2:\n            return\n        for i in range(len(data) - 2):\n            yield [ data[i], data[i + 1], data[i+2] ]\n  \n    #to add three words tuple as key and the next word as value \n    def __generate_3tuple_keys(self, data):\n        if len(data) < 3:\n            return\n        for i in range(len(data) - 3):\n            yield [ data[i], data[i + 1], data[i+2], data[i+3] ]\n\n    def oneword(self, string):\n        return Counter(self.lookup_dict[string]).most_common()[:3]\n\n    def twowords(self, string):\n        suggest = Counter(self.lookup_dict[tuple(string)]).most_common()[:3]\n        if len(suggest)==0:\n            return self.oneword(string[-1])\n        return suggest\n    \n    def threewords(self, string):\n        suggest = Counter(self.lookup_dict[tuple(string)]).most_common()[:3]\n        if len(suggest)==0:\n            return self.twowords(string[-2:])\n        return suggest\n    \n    def morewords(self, string):\n        return self.threewords(string[-3:])\n\n    def generate_text(self, string):\n        if len(self.lookup_dict) > 0:\n            tokens = string.split(\" \")\n            if len(tokens)==1:\n                print(\"Next word suggestions:\", self.oneword(string))\n            elif len(tokens)==2:\n                print(\"Next word suggestions:\", self.twowords(string.split(\" \")))\n            elif len(tokens)==3:\n                print(\"Next word suggestions:\", self.threewords(string.split(\" \")))\n            elif len(tokens)>3:\n                print(\"Next word suggestions:\", self.morewords(string.split(\" \")))\n        return","0685cc02":"#training_1 = \"How are you? How many people attend? How did it go?\"\ntraining_1 = article_text\nmy_next_word = MarkovChain()\nmy_next_word.add_document(training_1)","d0c8d9e2":"#my_next_word.generate_text(input().lower())\nmy_next_word.generate_text(\"world war\")","915b845b":"#pip install python-docx\n#pip install doc3","5afe9e54":"#pip install docx","318801c5":"from keras.preprocessing.text import Tokenizer\nimport nltk\nfrom nltk.tokenize import word_tokenize\nimport numpy as np\nimport re\nfrom keras.utils import to_categorical\n#from doc3 import training_doc3\n\n#training_doc3 = 'Hello, this is a sample test file for our next word prediction model. How are you doing? How many people have attended the session? When will you attend the class? What time is the class scheduled at? What is the agenda of the class?'\ntraining_doc3 = article_text\n\ncleaned = re.sub(r'\\W+', ' ', training_doc3).lower()\ntokens = word_tokenize(cleaned)\ntrain_len = 4\ntext_sequences = []\nfor i in range(train_len,len(tokens)):\n    seq = tokens[i-train_len:i]\n    text_sequences.append(seq)\n\nsequences = {}\ncount = 1\nfor i in range(len(tokens)):\n    if tokens[i] not in sequences:\n        sequences[tokens[i]] = count\n        count += 1\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(text_sequences)\nsequences = tokenizer.texts_to_sequences(text_sequences)\n\n#vocabulary size increased by 1 for the cause of padding\nvocabulary_size = len(tokenizer.word_counts)+1\nn_sequences = np.empty([len(sequences),train_len], dtype='int32')\n\nfor i in range(len(sequences)):\n    n_sequences[i] = sequences[i]\n\ntrain_inputs = n_sequences[:,:-1]\ntrain_targets = n_sequences[:,-1]\ntrain_targets = to_categorical(train_targets, num_classes=vocabulary_size)\nseq_len = train_inputs.shape[1]","ba827d7d":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Embedding\nmodel = Sequential()\nmodel.add(Embedding(vocabulary_size, seq_len, input_length=seq_len))\nmodel.add(LSTM(50,return_sequences=True))\nmodel.add(LSTM(50))\nmodel.add(Dense(50,activation='relu'))\nmodel.add(Dense(vocabulary_size, activation='softmax'))\n# compiling the network\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.fit(train_inputs,train_targets,epochs=100,verbose=0)","b008e957":"from keras.preprocessing.sequence import pad_sequences\n#input_text = input().strip().lower()\ninput_text = \"hiroshima and\".strip().lower()\nencoded_text = tokenizer.texts_to_sequences([input_text])[0]\npad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\nprint(encoded_text, pad_encoded)\nfor i in (model.predict(pad_encoded)[0]).argsort()[-3:][::-1]:\n  pred_word = tokenizer.index_word[i]\n  print(\"Next word suggestion:\",pred_word)","edf99e53":"Further, in the above method, we can have a sequence length of 2 or 3 or more using the following code","251b1c31":"In this approach, the sequence length of one is taken for predicting the next word. This means we will predict the next word given in the previous word."}}