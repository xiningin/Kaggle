{"cell_type":{"277620c6":"code","64e93837":"code","92ca11c8":"code","f4c9a551":"code","15ef96b5":"code","b55f7d3a":"code","7a19732f":"code","5ab68c82":"code","f4fdd5aa":"code","1efd30b2":"code","569eaf86":"code","07051b80":"code","bac978df":"code","fb513fe8":"code","29d8fb74":"code","e3ff7428":"code","fceb3b0c":"code","b974e89c":"markdown","a8b91322":"markdown","40cbe764":"markdown","16913268":"markdown","b0b0e7ab":"markdown","b30a4653":"markdown","8f6580fa":"markdown","3b67d89c":"markdown","8d4f5660":"markdown"},"source":{"277620c6":"import numpy as np \nimport pandas as pd \nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import roc_auc_score\nimport random\nrandom.seed(123)\nsns.set_style(\"darkgrid\")\ntrain = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\nlen_train = len(train)\ntrain_y = train[\"Survived\"]","64e93837":"plt.figure(figsize=(20,6))\ntrain_group = train.groupby(['Pclass','Sex'])['Fare'].mean()\nplt.subplot(121)\nsns.heatmap(train_group.unstack(\"Pclass\"), cmap='Blues', annot=True)\nplt.title(\"Fare (Pclass vs Sex)\")\ntrain_group = train.groupby(['Pclass','Sex'])['Age'].mean()\nplt.subplot(122)\nsns.heatmap(train_group.unstack(\"Pclass\"), cmap='Blues', annot=True)\nplt.title(\"Age (Pclass vs Sex)\")","92ca11c8":"from statsmodels.formula.api import ols\nfrom statsmodels.stats.anova import anova_lm\nlm = ols('Age~ C(Pclass) + C(Sex) * C(Embarked) + C(SibSp) + C(Parch)', pd.concat([train,test],axis=0)).fit()\nanova_lm(lm,typ=2)","f4c9a551":"lm = ols('Fare~ C(Pclass) + C(Sex) + C(Embarked) + C(SibSp) + C(Parch)', pd.concat([train,test],axis=0)).fit()\nanova_lm(lm,typ=2)","15ef96b5":"mapping = {'Mlle': 'Miss', 'Major': 'Mr', 'Col': 'Mr', 'Sir': 'Mr', 'Don': 'Mr', 'Mme': 'Miss',\n          'Jonkheer': 'Mr', 'Lady': 'Mrs', 'Capt': 'Mr', 'the Countess': 'Mrs', 'Ms': 'Miss', 'Dona': 'Mrs'}\n\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X[self.attribute_names]\n\nclass FirstTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, name):\n        self.name = name\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        value = [i[0][0] for i in X[self.name]]\n        X[self.name] = value\n        return X\n    \nclass ValueImputer(BaseEstimator, TransformerMixin):\n    def __init__(self, name, value):\n        self.value = value\n        self.name = name\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        X[self.name].fillna(self.value,inplace=True)\n        return X\n\nclass GroupbyImputer(BaseEstimator, TransformerMixin):\n    def __init__(self, name, group_by):\n        self.name = name\n        self.group_by = group_by\n    def fit(self, X, y=None):\n        self.value = X.groupby(self.group_by)[self.name]\n        return self\n    def transform(self, X):\n        X[self.name] = self.value.apply(lambda x: x.fillna(x.median()))\n        i=1\n        while(X[self.name].isnull().sum()>0):\n            k = X.groupby(self.group_by[:-i])[self.name]\n            X[self.name] = (k.apply(lambda x: x.fillna(x.median())))\n            i+=1\n            if i == len(self.group_by):\n                break\n        return X\n\nclass MostTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, names,mapping):\n        self.names = names\n        self.mapping = mapping\n    def fit(self, X, y=None):\n        self.freq = X[self.names].str.split(', ', expand=True)[1].str.split('.', expand=True)[0]\n        return self\n    def transform(self, X, y=None):\n        X[self.names + \"_map\"] = self.freq\n        X.replace({self.names+\"_map\": self.mapping}, inplace=True)\n        return X.drop(\"Name\",axis=1)\n\n\nclass FrequentTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, names):\n        self.names = names\n    def fit(self, X, y=None):\n        self.freq =  X.groupby(self.names)[self.names].transform('count')\n        return self\n    def transform(self, X, y=None):\n        X[self.names + \"_freq\"] = self.freq\n        return X.drop(self.names, axis=1)\n\nimputer_pipeline = Pipeline([\n        ('Cabin_imputer', ValueImputer(name = \"Cabin\",value=\"None\")),\n        ('Embarked_imputer', ValueImputer(name= \"Embarked\",value=\"S\")),\n        ('Fare_imputer', GroupbyImputer(name= \"Fare\",group_by = ['Pclass', 'Parch','SibSp'])),\n        ('Age_imputer', GroupbyImputer(name= \"Age\",group_by = ['Pclass','Parch','Embarked'])),\n    ])\n\ntransform_pipeline = Pipeline([\n        ('Cabin', FirstTransformer(name= \"Cabin\")),\n        ('Name_transformer', MostTransformer(names = \"Name\", mapping = mapping)),\n        ('Ticket_transformer', FrequentTransformer(names= \"Ticket\")),\n    ])","b55f7d3a":"class TaitanicProcessing(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        self.y = y\n        return self\n    def transform(self, X):\n        X[\"Sex\"] = X[\"Sex\"].map({\"male\":1, \"female\":0})\n        X[\"Family\"] = X[\"SibSp\"] + X[\"Parch\"] + 1\n        X[\"Family\"] = X[\"Family\"].astype(str)\n        X['Family'] = X['Family'].replace(\"1\", 'Alone')\n        X['Family'] = X['Family'].replace([\"2\",\"3\",\"4\"], 'Normal')\n        X['Family'] = X['Family'].replace([\"5\",\"6\"], 'Mid')\n        X['Family'] = X['Family'].replace([\"7\",\"8\",\"11\"], 'Big')\n        X[\"Fare\"] = np.log1p(X[\"Fare\"])\n        X[\"Fare\"] = MinMaxScaler().fit_transform(X['Fare'].values.reshape(-1, 1))\n        X['Cabin'] = X['Cabin'].replace(['A', 'B', 'C','T'], 'ABCT')\n        X['Cabin'] = X['Cabin'].replace(['D', 'E'], 'DE')\n        X['Cabin'] = X['Cabin'].replace(['F', 'G'], 'FG')\n        X['Age'] = pd.qcut(X['Age'], 10)\n        X['Age'] = LabelEncoder().fit_transform(X['Age'])\n        return X\n\nclass DummyCategory(BaseEstimator, TransformerMixin):\n    def __init__(self, names):\n        self.names = names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        encoded_fea = []\n        for c in self.names:\n            encoded = OneHotEncoder().fit_transform(X[c].values.reshape(-1, 1)).toarray()\n            n = X[c].nunique()\n            cols = ['{}_{}'.format(c, n) for n in range(1, n + 1)]\n            encoded_df = pd.DataFrame(encoded, columns=cols)\n            encoded_df.index = X.index\n            encoded_fea.append(encoded_df)\n            \n        return pd.concat([X, *encoded_fea[:5]], axis=1)\n\nprocess_pipeline = Pipeline([\n        (\"process\", TaitanicProcessing()),\n        (\"cat_pipeline\", DummyCategory([\"Embarked\", \"Name_map\",\"Cabin\",\"Family\"])),\n    ])\n\nfull_pipeline =  Pipeline([\n        (\"imputer\", imputer_pipeline),\n        (\"transform\", transform_pipeline),\n        (\"process\", process_pipeline),\n    ])","7a19732f":"df = pd.concat([train.drop(\"Survived\",axis=1),test],axis=0,sort=False)\ndf.set_index(\"PassengerId\",drop=True, inplace=True)\ndf = full_pipeline.fit_transform(df,train_y)\ndf.reset_index(drop=True,inplace=True)\ndf.info()","5ab68c82":"df.drop([\"Embarked\", \"Name_map\",\"Cabin\",\"Family\",'SibSp','Parch'],axis=1,inplace=True)\ndf.head()","f4fdd5aa":"train = df.iloc[:len_train,:].reset_index(drop=True)\ntest = df.iloc[len_train:,:].reset_index(drop=True)","1efd30b2":"def rf_cv(n_estimators, max_depth, min_samples_split, min_samples_leaf, ccp_alpha, x_data=None, y_data=None, n_splits=5, output='score'):\n    score = 0\n    kf = StratifiedKFold(n_splits=n_splits, random_state=5, shuffle=True)\n    models = []\n    for train_index, valid_index in kf.split(x_data, y_data):\n        x_train, y_train = x_data.iloc[train_index], y_data[train_index]\n        x_valid, y_valid = x_data.iloc[valid_index], y_data[valid_index]\n                                           \n        model = RandomForestClassifier(\n            criterion='gini',\n            max_features='auto',\n            n_estimators = int(n_estimators), \n            max_depth = int(max_depth),\n            min_samples_split = int(min_samples_split),\n            min_samples_leaf = int(min_samples_leaf),\n            ccp_alpha = ccp_alpha,\n            random_state = 123,\n            oob_score=True,\n            n_jobs=-1\n        )\n        \n        model.fit(x_train, y_train)\n        models.append(model)\n        \n        pred = model.predict_proba(x_valid)[:, 1]\n        true = y_valid\n        score += roc_auc_score(true, pred)\/n_splits\n    \n    if output == 'score':\n        return score\n    if output == 'model':\n        return models\n\nfrom functools import partial \nfrom bayes_opt import BayesianOptimization\nfunc_fixed = partial(rf_cv, x_data=train, y_data=train_y, n_splits=5, output='score')\nrf_ba = BayesianOptimization(\n    func_fixed, \n    {\n        'n_estimators': (1000, 2000),                        \n        'max_depth' : (5,13),\n        'min_samples_split' : (4,8),\n        'min_samples_leaf' : (4,8),\n        'ccp_alpha' : (0.0001, 0.01)\n    }, \n    random_state=4321            \n)\nrf_ba.maximize(init_points=5, n_iter=20)","569eaf86":"params = rf_ba.max['params']\nrf_model = rf_cv(\n    params['n_estimators'],\n    params['max_depth'],\n    params['min_samples_split'],\n    params['min_samples_leaf'],\n    params['ccp_alpha'],\n    x_data=train, y_data=train_y, n_splits=5, output='model') \n\nimportances = pd.DataFrame(np.zeros((train.shape[1], 5)), columns=['Fold_{}'.format(i) for i in range(1, 6)], index=train.columns)\n\npreds = []\n\nfor i, model in enumerate(rf_model):\n    importances.iloc[:, i] = model.feature_importances_\n    pred = model.predict(test)\n    preds.append(pred)\npred = np.mean(preds, axis=0)","07051b80":"importances['Mean_Importance'] = importances.mean(axis=1)\nimportances.sort_values(by='Mean_Importance', inplace=True, ascending=False)\n\nplt.figure(figsize=(15, 15))\ns = sns.barplot(x=importances.index, y='Mean_Importance', data=importances)\ns.set_xticklabels(s.get_xticklabels(),rotation=90)\nplt.title('Random Forest Classifier Feature Importance', size=15)","bac978df":"y_pred = pred\ny_pred[y_pred >= 0.5] = 1\ny_pred = y_pred.astype(int)\nsubmission = pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")\nsubmission[\"Survived\"] = y_pred\nsubmission.to_csv(\"submission.csv\",index = False)","fb513fe8":"rfc_model = RandomForestClassifier(criterion='gini',n_estimators=1800,max_depth=7, min_samples_split=6,min_samples_leaf=6,\n                                           max_features='auto', oob_score=True, random_state=123,n_jobs=-1) \n\noob = 0\nprobs = pd.DataFrame(np.zeros((len(test),10)), columns=['Fold_{}_Sur_{}'.format(i, j) for i in range(1, 6) for j in range(2)])\n\n\nkf = StratifiedKFold(n_splits=5, random_state=5, shuffle=True)\n\nfor fold, (trn_idx, val_idx) in enumerate(kf.split(train, train_y), 1):\n    rfc_model.fit(train.iloc[trn_idx,:], train_y[trn_idx])\n    probs.loc[:, ['Fold_{}_Sur_0'.format(fold),'Fold_{}_Sur_1'.format(fold)]] = rfc_model.predict_proba(test)\n    oob += rfc_model.oob_score_ \/ 5\n    print('Fold {} OOB : {}'.format(fold, rfc_model.oob_score_))\nprint('Average Score: {}'.format(oob))","29d8fb74":"survived =[col for col in probs.columns if col.endswith('Sur_1')]\nprobs['survived'] = probs[survived].mean(axis=1)\nprobs['unsurvived'] = probs.drop(columns=survived).mean(axis=1)\nprobs['pred'] = 0\nsub = probs[probs['survived'] >= 0.5].index\nprobs.loc[sub, 'pred'] = 1\n\ny_pred = probs['pred'].astype(int)","e3ff7428":"submission = pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")\nsubmission[\"Survived\"] = y_pred\nsubmission.to_csv(\"submission.csv\",index = False)","fceb3b0c":"submission.head()","b974e89c":"## Age, Fare","a8b91322":"# Anova analysis\n\nIn the picture above, although there are differences depending on Pclass, I want to see more factors.\nBut it is limited to graphically express several factors. So I decided to do anova analysis.","40cbe764":"## feature importance","16913268":"- When I submitted the score I got 0.78947\n\n- using other model apply other parameters and evaluate by oob score.","b0b0e7ab":"# Introduction\nFollowing the previous EDA process, I will start working based on the results and additional work like processing, modeling, etc.\n\n- Taitanic EDA\nhttps:\/\/www.kaggle.com\/rbud613\/taitanic-eda\n\n## Load packages and datasets","b30a4653":"\n- When I submitted the score I got 0.80861\n\n## Reference\n\nhttps:\/\/github.com\/rickiepark\/handson-ml\/blob\/master\/02_end_to_end_machine_learning_project.ipynb","8f6580fa":"- Family : In previous notebook, frequency and category are different depending on the size. So divide categories by number. \n- Cabin : Like Family, divide categories by letter.\n- Fare : perform log transform\n- Age : It was divided into 10 sections.\n\n- process_pipeline : After processing the variables, encoding as label and make dummy variables. \n- full_pipeline : all pipeline is connected.","3b67d89c":"- In above results, Embarked's statistic and Sex's statistic are small compared to other factors.\n- In First result, The interaction between Sex and Pclass is not significant, but it's statistic is similar to Embarked's statistic\n\n\n\n## Additional information\n\n- I found embarked information about Embarked's value is NA in this page\nhttps:\/\/www.encyclopedia-titanica.org\/titanic-survivors\/ \nTheir's embarked is Southampton.\n\n\n\n# Data Processing(using Pipeline)\n\nI tried to do it using a Pipeline in scikit-learn.\n\n- imputer_pipeline : process missing value.\n    - Age and Fare are grouped by Pclass, Parch and SibSp, and fill missing values.\n    - In SibSp category, there are cases where all are missing values(SibSp==8), so the variable is subtracted and grouped again.\n- transform_pipeline : transform value\n    - Change variables as in the previous notebook.","8d4f5660":"# Modeling\n\n## RandomForest + BayesianOptimization"}}