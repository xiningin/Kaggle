{"cell_type":{"82d63583":"code","98871581":"code","bfab6704":"code","8c504ec2":"code","7c514857":"code","7147c83b":"code","cfee2e02":"code","6c545545":"code","a484e4a0":"code","41c7a162":"code","80af4dc5":"code","4a11b520":"code","703a30a6":"code","144f0f4a":"code","c291bd65":"code","e2d81fac":"code","d36bd5c6":"code","0ada6878":"code","682ddb01":"code","345513be":"code","f0ba314b":"code","6f63f2eb":"code","b17a1e77":"code","0ad46288":"code","04742f48":"code","c609397a":"code","93a4a01b":"code","56f7a73b":"code","476924f3":"code","52b3054e":"code","0b07125a":"code","50b08cf0":"code","48c5c36b":"code","b770fd07":"code","55fc8867":"code","546da51b":"code","4e87232c":"code","9756b7e6":"code","02d96c93":"code","782f2a94":"code","29deee5e":"code","f7d23a90":"code","653727ae":"code","36a69b7f":"code","387dad22":"code","d855b7c3":"code","2e7a2cbf":"code","2ae1aac8":"code","b5a04073":"code","c24d69d8":"code","a81522e3":"code","147093fa":"code","2f830798":"code","9761db22":"code","7caad18f":"code","6067c80d":"code","1970f6e6":"code","645207e6":"code","978e1135":"markdown","17a18c9b":"markdown","cc24dbfc":"markdown","812565b7":"markdown","8807285f":"markdown","5cd99e5a":"markdown","0e43da0c":"markdown","129cfd90":"markdown","502522c9":"markdown","1b8aa83f":"markdown","0b391077":"markdown","1a4b30f4":"markdown","cedea24b":"markdown","b7c8c80c":"markdown","b79db445":"markdown"},"source":{"82d63583":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nprint('Priyatama is ready!')","98871581":"train = pd.read_csv('..\/input\/credit-default-prediction-ai-big-data\/train.csv')\ntest = pd.read_csv('..\/input\/credit-default-prediction-ai-big-data\/test.csv')\ntrain.head()","bfab6704":"test.head()","8c504ec2":"# Strip column names of all spaces and add underscore wherever required.\n\ntrain.columns = ['_'.join(col.split(' ')).lower() for col in train.columns]\ntest.columns = ['_'.join(col.split(' ')).lower() for col in test.columns]","7c514857":"print('Columns in Train dataset are as follows:' '\\n')\nprint(train.columns)\nprint('o' * 80, '\\n')\nprint('Columns in Test dataset are as follows:' '\\n')\nprint(test.columns)","7147c83b":"print('Data types in Train dataset as follows:' '\\n')\nprint(train.dtypes)\nprint('\\n','o' * 80, '\\n')\nprint('Data types in Test dataset as follows:' '\\n')\nprint(test.dtypes)","cfee2e02":"print('Unique home_ownership in Train dataset as follows:' '\\n')\na_list = train.home_ownership.unique()\nprint(*a_list, sep = \", \")\nprint('\\n','o' * 80, '\\n')\nprint('Unique home_ownership in Test dataset as follows:' '\\n')\nb_list = test.home_ownership.unique()\nprint(*b_list, sep = \", \")","6c545545":"print('Unique Year in current job values in Train dataset as follows:' '\\n')\na_list = train.years_in_current_job.unique()\nprint(*a_list, sep = \", \")\nprint('\\n','o' * 80, '\\n')\nprint('Unique Year in current job values in Test dataset as follows:' '\\n')\nb_list = test.years_in_current_job.unique()\nprint(*b_list, sep = \", \")","a484e4a0":"years_in_current_job_map = {'10+ years': 10,'9 years': 9,'8 years': 8,'7 years':7,'6 years':6,\n                            '5 years': 5, '4 years': 4, '3 years':3, '2 years': 2,'1 year': 1, '< 1 year':0 }","41c7a162":"train.years_in_current_job = train.years_in_current_job.map(years_in_current_job_map)","80af4dc5":"test.years_in_current_job = test.years_in_current_job.map(years_in_current_job_map)","4a11b520":"train.years_in_current_job.value_counts(dropna=False).plot(kind='barh')","703a30a6":"test.years_in_current_job.value_counts(dropna=False).plot(kind='barh')","144f0f4a":"train.years_in_current_job.unique()","c291bd65":"test.years_in_current_job.unique()","e2d81fac":"train.years_in_current_job= train.years_in_current_job.agg(lambda x : x.fillna( x.median()))\ntest.years_in_current_job = test.years_in_current_job.agg(lambda x : x.fillna( x.median()))","d36bd5c6":"print('Intotal unique purpose values Train dataset are ' +(str(train.purpose.nunique())))\nprint('\\n','o' * 80, '\\n')\nprint('Intotal unique purpose values Test dataset are ' +(str(test.purpose.nunique())))","0ada6878":"#Create a list of all values in Purpose column of train dataset\ntrain_purpose = [i for i in train.purpose]\n\n#Create a list of all values in Purpose column of test dataset\ntest_purpose = [i for i in test.purpose]\n\n#Substract the values in test from values in train.\n\nbad_label_cols = list(set(train_purpose)-set(test_purpose))\n\nprint(*bad_label_cols, sep = \", \")","682ddb01":"train = train[train.purpose != 'renewable energy']","345513be":"print('Different terms for loan in Train dataset are as follows:' '\\n')\na_list = train.term.unique()\nprint(*a_list, sep = \", \")\nprint('\\n','o' * 80, '\\n')\nprint('Different terms for loan in Train dataset are as follows:' '\\n')\nb_list = test.term.unique()\nprint(*b_list, sep = \", \")","f0ba314b":"print('Null values in Train dataset are as follows:' '\\n')\na = train.isnull().sum()\nprint(a[a>0])\nprint('\\n','o' * 80, '\\n')\nprint('Null values in Train dataset are as follows:' '\\n')\nb = test.isnull().sum()\nprint(b[b>0])","6f63f2eb":"train.bankruptcies =  train.bankruptcies.agg(lambda x : x.fillna(x.median()))\ntrain.years_in_current_job =  train.years_in_current_job.agg(lambda x : x.fillna(x.median()))\ntrain.months_since_last_delinquent  =  train.months_since_last_delinquent.agg(lambda x : x.fillna(x.median()))\ntrain.credit_score =  train.credit_score.agg(lambda x : x.fillna(x.median()))\ntrain['annual_income'] =  train['annual_income'].agg(lambda x : x.fillna(x.mean()))","b17a1e77":"test.bankruptcies =  test.bankruptcies.agg(lambda x : x.fillna(x.median()))\ntest.years_in_current_job =  test.years_in_current_job.agg(lambda x : x.fillna(x.median()))\ntest.months_since_last_delinquent  =  test.months_since_last_delinquent.agg(lambda x : x.fillna(x.median()))\ntest.credit_score =  test.credit_score.agg(lambda x : x.fillna(x.median()))\ntest['annual_income'] =  test['annual_income'].agg(lambda x : x.fillna(x.mean()))","0ad46288":"print('Are there any Null values in Train dataset?' '\\n')\nprint(train.isnull().sum().any())\nprint('\\n','o' * 80, '\\n')\nprint('Are there any Null values in test dataset?' '\\n')\nprint(test.isnull().sum().any())","04742f48":"corr = train.corr()\nsns.heatmap(train.corr(), \n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values)","c609397a":"train2 = train.drop('id', axis=1)\ntest2 = test.copy()","93a4a01b":"train2  = pd.get_dummies(train2, drop_first = True)\ntrain2.shape","56f7a73b":"test2  = pd.get_dummies(test2, drop_first = True)\ntest2.shape","476924f3":"from sklearn.model_selection import train_test_split\n\nfrom sklearn.model_selection import cross_val_score","52b3054e":"X = train2.drop(['credit_default'], axis = 1)\ny =train2.credit_default","0b07125a":"train2.shape","50b08cf0":"X_train, X_valid, y_train, y_valid = train_test_split(X, y,\n                                                      train_size=0.8, test_size=0.2,\n                                                      random_state=0)","48c5c36b":"X_valid.shape","b770fd07":"from sklearn.metrics import f1_score,confusion_matrix\nfrom sklearn.metrics import accuracy_score","55fc8867":"from sklearn.ensemble import RandomForestClassifier","546da51b":"clf_rf = RandomForestClassifier(n_estimators=20 ,random_state=43)      \nclr_rf = clf_rf.fit(X_train,y_train)\n\nac = accuracy_score(y_valid,clf_rf.predict(X_valid))\nprint('Accuracy is: ',int(ac*100),\" %\")\ncm = confusion_matrix(y_valid,clf_rf.predict(X_valid))\nsns.heatmap(cm,annot=True,fmt=\"d\")","4e87232c":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n# find best scored 5 features\nselect_feature = SelectKBest(chi2, k=5).fit(X_valid, y_valid)\na = select_feature.scores_\nb = X_train.columns\ndf = pd.DataFrame(list(zip(b, a)),\n               columns =['Column', 'Score'])\ndf.dtypes","9756b7e6":"df['Score'] = df['Score'].replace(np.nan, 0)","02d96c93":"df['Score'] = df['Score'].astype(int)","782f2a94":"df.sort_values(by='Score',ascending=False)","29deee5e":"X_train_2 = select_feature.transform(X_train)\nX_valid_2 = select_feature.transform(X_valid)\n#random forest classifier with n_estimators=10 (default)\nclf_rf_2 = RandomForestClassifier()      \nclr_rf_2 = clf_rf_2.fit(X_train_2,y_train)\nac_2 = accuracy_score(y_valid,clf_rf_2.predict(X_valid_2))\nprint('Accuracy is: ',int(ac_2*100), ' %')\ncm_2 = confusion_matrix(y_valid,clf_rf_2.predict(X_valid_2))\nsns.heatmap(cm_2,annot=True,fmt=\"d\")","f7d23a90":"from sklearn.feature_selection import RFE\n# Create the RFE object and rank each pixel\nclf_rf_3 = RandomForestClassifier()      \nrfe = RFE(estimator=clf_rf_3, n_features_to_select=7, step=1)\nrfe = rfe.fit(X_train, y_train)","653727ae":"print('Chosen best 7 feature by rfe:',X_valid.columns[rfe.support_])","36a69b7f":"rfe.score(X_valid, y_valid)","387dad22":"from sklearn.feature_selection import RFECV\n\n# The \"accuracy\" scoring is proportional to the number of correct classifications\nclf_rf_4 = RandomForestClassifier() \nrfecv = RFECV(estimator=clf_rf_4, step=1, cv=5,scoring='accuracy')   #5-fold cross-validation\nrfecv = rfecv.fit(X_train, y_train)\n\nprint('Optimal number of features :', rfecv.n_features_)\nprint('Best features :', X_train.columns[rfecv.support_])","d855b7c3":"rfecv.score(X_valid, y_valid)","2e7a2cbf":"filtered_column = (X_train.columns[rfecv.support_]).tolist()\nfiltered_column","2ae1aac8":"clf_rf_5 = RandomForestClassifier()      \nclr_rf_5 = clf_rf_5.fit(X_train,y_train)\nimportances = clr_rf_5.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in clf_rf.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(X_train.shape[1]):\n    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n\n# Plot the feature importances of the forest\n\nplt.figure(1, figsize=(15, 7))\nplt.title(\"Feature importances\")\nplt.bar(range(X_train.shape[1]), importances[indices],\n       color=\"b\", yerr=std[indices], align=\"center\")\nplt.xticks(range(X_train.shape[1]), X_train.columns[indices],rotation=90)\nplt.xlim([-1, X_train.shape[1]])\nplt.show()","b5a04073":"rfecv.score(X_valid, y_valid)","c24d69d8":"X_train.filter(filtered_column)\nX_valid.filter(filtered_column)","a81522e3":"rfecv.score(X_valid, y_valid)","147093fa":"test3 =test2.filter(filtered_column)\ntest3.shape","2f830798":"X_train_0 =X_train.filter(filtered_column)\nX_train_0.shape","9761db22":"X_valid_0 =X_valid.filter(filtered_column)\nX_valid_0.shape","7caad18f":"rfecv.fit(X_train_0, y_train)\nrfecv.score(X_valid_0, y_valid)","6067c80d":"y_pred = rfecv.predict(test3)","1970f6e6":"submission = pd.DataFrame({\n        \"Id\": test2[\"id\"],\n        \"Credit Default\": y_pred\n    })\n\nsubmission.to_csv('submission.csv', index=False)","645207e6":"submission.shape","978e1135":"## 5.3. Top 7 features using Recursive Feature Elimination.","17a18c9b":"# 4.Model Designing. ","cc24dbfc":"## 4.3. Scoring Random Forest Classifier using Confusion Matrix.","812565b7":"# 3. Feature Engineering.","8807285f":"# 2. Data Cleaning.","5cd99e5a":"## 5.4. Finding optimal features using Recursive Feature Elimination and Cross-Validation Selection.","0e43da0c":"    There are only four columns that have categorical values.\n    THe first task is to check them.","129cfd90":"## 5.2. Random Forest classifier with best 5 features.","502522c9":"# 6. Preparing Submission.","1b8aa83f":"# 5. Feature Selection.","0b391077":"    Home Ownership column in both the dataset is usable as it is.","1a4b30f4":"## 4.1. Train & Valid Split. ","cedea24b":"## 5.5. Tree based feature selection and random forest classification.","b7c8c80c":"# 1. Read the data.","b79db445":"## 5.1. Selecting best 5 features."}}