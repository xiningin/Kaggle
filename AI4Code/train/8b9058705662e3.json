{"cell_type":{"7ab4d863":"code","ca242e7d":"code","8a57bcbf":"code","79207f57":"code","ac994cbc":"code","ee734ea1":"code","4f690ad0":"code","fc48a0d2":"code","44905eac":"code","da05e467":"code","f4f2e44d":"code","2273edc7":"code","b5d996e8":"markdown","d8dc34c3":"markdown","01f1e5de":"markdown","8502fd1f":"markdown","7bfa9550":"markdown","db9c89eb":"markdown","297f9846":"markdown","c1d1127c":"markdown","c13aefba":"markdown","cd2ab1ec":"markdown"},"source":{"7ab4d863":"import numpy as np\nimport pandas as pd\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom bs4 import BeautifulSoup\nimport re\nfrom nltk.stem.wordnet import WordNetLemmatizer","ca242e7d":"train_docs = [\n    \"He is a lazy boy. She is also lazy.\",\n    \"Edwin is a lazy person.\"\n]\n\ntest_docs = [\n    \"She is extremely lazy.\"\n]","8a57bcbf":"lemmatizer = WordNetLemmatizer() # Instantiate lemmatizer\n\ndef proc_text(messy): #input is a single string\n    first = BeautifulSoup(messy, \"lxml\").get_text() #gets text without tags or markup, remove html\n    second = re.sub(\"[^a-zA-Z]\",\" \",first) #obtain only letters\n    third = second.lower() #obtains a list of words in lower case\n    fourth = word_tokenize(third) # tokenize\n    fifth = [lemmatizer.lemmatize(str(x)) for x in fourth] #lemmatizing\n    stops = set(stopwords.words(\"english\")) #faster to search through a set than a list\n    final = [w for w in fifth if not w in stops] #remove stop words\n    return final","79207f57":"train = [proc_text(text) for text in train_docs] # preprocess training set\ntest = [proc_text(text) for text in test_docs] # preprocess test set\n\nprint(\"Training Documents:\\n{}\\n\\n\".format(train))\nprint(\"Test Documents:\\n{}\\n\\n\".format(test))","ac994cbc":"# Construct Vocabulary-Index Dictionary\n\nvocab_idx = dict()\nnum_vocab = len(vocab_idx)\n\nfor doc in train:\n    for token in set(doc):\n        if token not in vocab_idx.keys():\n            vocab_idx[token] = num_vocab\n            num_vocab += 1","ee734ea1":"print(\"A Peek at Vocabulary-Index Dictionary\\n\\n\")\nprint(vocab_idx)","4f690ad0":"# Construct Matrix of Zeroes\n\ntrain_matrix = np.zeros((len(train), num_vocab)) # Document Term Matrix for training set","fc48a0d2":"# Update Counts of Terms in Each Document for Training Set\n\nfor idx, doc in enumerate(train):\n    for token in doc:\n        train_matrix[idx, vocab_idx[token]] += 1","44905eac":"print(\"A Peek at Train Matrix\\n\\n\")\nprint(train_matrix)","da05e467":"# Construct a matrix of zeros\n\ntest_matrix = np.zeros((len(test), num_vocab))","f4f2e44d":"# Update Counts of Terms in Each Document for Test Set\n\nfor idx, doc in enumerate(test):\n    for token in doc:\n        if token in vocab_idx.keys():\n            test_matrix[idx, vocab_idx[token]] += 1","2273edc7":"print(\"A Peek at Test Matrix\\n\\n\")\nprint(test_matrix)","b5d996e8":"## Table of Contents\n\n1. [Import Packages](#1)\n2. [Import Data](#2)\n3. [Preprocessing Steps](#3)\n4. [Preprocess Texts](#4)\n5. [Bag of Words Representation](#5)\n6. [Applying Bag-of-Words on Test Set](#6)\n7. [Closing Remarks](#7)","d8dc34c3":"# NLP 101 - Bag of Words Explained\n\nThis notebook aims to explain and perform the bag of words approach from scratch. This approach helps convert documents containing texts into representations which can be comprehended by machine learning models. Enjoy reading! :)","01f1e5de":"# Preprocessing Steps <a class=\"anchor\" id=\"3\"><\/a>\n\n1. Remove markup\n2. Obtain only alphabets\n3. Convert all to lowercase\n4. Tokenize (A blunt defintion would be extracting words from dcuments and each word would be an element of the list which represents a single document)\n5. Lemmatize\n6. Remove stopwords","8502fd1f":"Let's reiterate what the test document is:\n\n- She is extremely lazy.\n\nThe test document has been processed and the tokens extracted were:\n\n- 'extremely', 'lazy'\n\nNote that the word 'extremely' was not found in the vocabulary of tokens extracted from the training documents, hence it will not be represented in the document-term matrix constructed for the test set. Furthermore, for the test set, we can see that:\n\n- The word 'boy' appeared 0 time.\n- The word 'also' appeared 0 time.\n- The word 'lazy' appeared 1 time.\n- The word 'person' appeared 0 times.\n- The word 'edwin' appeared 0 times.\n\nThus the vector representation for the first document would be (0, 0, 1, 0, 0).","7bfa9550":"# Closing Remarks <a class=\"anchor\" id=\"7\"><\/a>\n\nBag of Words approach is a frequency-based approach to convert each document into a vector which can be used to train a machine learning model. However, this approach does not capture the syntactic and semantic relationships present within documents and there exist many other more effective ways of representation. Just to name a few:\n\n1. Tfidf\n2. Word Vector Averaging\n3. Sentence Encoding\n\nHope this kernel has been helpful in some ways :)","db9c89eb":"# Import Packages <a class=\"anchor\" id=\"1\"><\/a>","297f9846":"# Applying Bag-of-Words on Test Document <a class=\"anchor\" id=\"6\"><\/a>\n\nNow, we would have to apply similar steps on the test document, but there are some differences to take note of:\n\n- Vocabulary-index dictionary is only built using the tokens from training set as the model would be built using distinct tokens seen in the training set only. This is because there may be new tokens found in the test set, which are not seen in the training set. Since the number of features used to train the model is fixed, the building of the vocabulary-index dictionary would be restricted to just those tokens from the training set.\n\n- As a result of the pervious pointer, the number of rows and columns in the document-term matrix for the test set would be the numbers of documents and distinct tokens in the test set respectively.\n\nIn summary, the steps are as follows:\n\n1. Construct a matrix of zeros for the test set where the numbers of rows and columns would be determined by the number of documents in test set and the number of distinct tokens found in training set.\n2. Iterate through the tokens of each document and update the counts of tokens for that partcular document. ","c1d1127c":"# Bag of Words Representation <a class=\"anchor\" id=\"4\"><\/a>\n\nNow that the documents are preprocessed, we shall proceed to implementing the bag of words approach. Note that each of train and test is a list of lists, where each element in each list is a token. \n\nWe have to convert all each document into a list or a vector and when you do this to all documents, they together will form a matrix. This matrix is also called\n\n## Document-Term Matrix\n\nLet $d_i$ and $w_j$ denote i-th row and j-th column of this matrix respectively. Each entry of this matrix, which can be expressed as $ ({d_i}, {w_j}) $, would be represented by a non-negative interger which is greater than equal to zero. This integer would also mean the number of times word $j$ appears in document $i$. \n\nTo elaborate further, the numbers of rows and columns of this matrix will be determined by the number of documents and number of distinct tokens (or words) in your training set respectively. All the tokens (or words) extracted from all training documents make up what we usually call, vocabulary.\n\n## Procedure\n\nNote that these steps are only performed on the training set.\n\nThe steps are as follows:\n\n1. Construct vocabulary-index dictionary, i.e. create a Python dictionary with distinct tokens as keys and corresponding values would be indices ranging from 0 to number of distinct tokens.\n2. Construct a matrix of zeros, where the numbers of rows and columns correspond to the numbers of training documents and distinct tokens respectively.\n3. Iterate through the tokens of each document and update the counts of tokens for that partcular document.","c13aefba":"Lets reiterate what the training documents were, they are: \n\n1. He is a lazy boy. She is also lazy.\n2. Edwin is a lazy person.\n\nThey have been processed and the tokens extracted were:\n\n1. 'lazy', 'boy', 'also', 'lazy'\n2. 'edwin', 'lazy', 'person'\n\nThe distinct tokens obtained from training documents are:\n\n1. boy\n2. also\n3. lazy\n4. person\n5. edwin\n\nand their corresponding indices are from 0 to 4 respectively.\n\nLooking at the document-term matrix constructed based on training set, the first row corresponds to the first document and the columns represent the distinct tokens mentioned previously and the order is preserved. To elaborate further, for the first document, we can see that:\n\n- The word 'boy' appeared 1 time.\n- The word 'also' appeared 1 time.\n- The word 'lazy' appeared 2 times.\n- The word 'person' appeared 0 times.\n- The word 'edwin' appeared 0 times.\n\nThus the vector representation for the first document would be (1, 1, 2, 0, 0).","cd2ab1ec":"# Import Data <a class=\"anchor\" id=\"2\"><\/a>"}}