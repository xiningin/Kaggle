{"cell_type":{"26f4301b":"code","416eb118":"code","40d832e8":"code","6ed5387d":"code","00fc71a4":"code","b69a152c":"code","bcdac096":"code","91e1237f":"code","d246a964":"code","d30868b4":"code","b1a3d1ef":"code","f327d0ef":"code","5d2bb6e6":"markdown","7f35ec6f":"markdown","76506255":"markdown","fbc2fc6e":"markdown","62e6a66f":"markdown"},"source":{"26f4301b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport seaborn as sb\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.figure_factory as ff\n\nfrom sklearn.model_selection import train_test_split,GridSearchCV,cross_val_score,KFold\nfrom sklearn.preprocessing import StandardScaler  \nfrom sklearn.metrics import accuracy_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,VotingClassifier,BaggingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom lightgbm import LGBMClassifier","416eb118":"df = pd.read_csv('..\/input\/churn-modelling\/Churn_Modelling.csv')\ndf =df.drop(['RowNumber','Surname'],axis=1)\ndf.head()\n","40d832e8":"# df_1 = df.select_dtypes(exclude='object')\n# df_1.hist(figsize=(10,8))\n\ndf.select_dtypes(exclude='object').hist(figsize=(14,10),bins=20)\nsb.countplot(df['Exited'])\ndf['Exited'].value_counts().unique()\ncorr = df.corr()\ncorr.style.background_gradient(cmap='Greens').set_precision(2)","6ed5387d":"\n# Seperate the target or response variable\n\ny = df['Exited']\n\n#Remove response from dataset\n\nX = df.drop(['Exited'],axis =1)\nX.head()\n\n# apply train test split\n\ntrain_X,valid_X,train_y,valid_y = train_test_split(X,y,test_size=0.20,random_state=1)\n\n#LabelEncoding for categorical variables\n\nfrom sklearn.preprocessing import LabelEncoder\nlbenc = LabelEncoder()\n\ncols = ['Geography','Gender']\n\nfor col in df[cols]:\n    train_X[col] = lbenc.fit_transform(train_X[col])\n    valid_X[col] = lbenc.transform(valid_X[col])\n","00fc71a4":"# import the class\nfrom sklearn.linear_model import LogisticRegression\n\n# instantiate the model (using the default parameters)\nlogreg = LogisticRegression(solver='liblinear',random_state=1,C=10.0)\n\n# fit the model with data\nlogreg.fit(train_X,train_y)\n\n#\ny_pred=logreg.predict(valid_X)\n\n\n# import the metrics class\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\ncnf_matrix = metrics.confusion_matrix(valid_y, y_pred)\ncnf_matrix\nprint(classification_report(valid_y, y_pred,digits=5))","b69a152c":"y_pred = logreg.predict_proba(valid_X)[::,1]\nfpr, tpr, _ = metrics.roc_curve(valid_y,  y_pred)\nauc = metrics.roc_auc_score(valid_y, y_pred)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.show()\n\n","bcdac096":"rs=1\nmodels = [LogisticRegression(random_state=rs), LGBMClassifier(),KNeighborsClassifier(),\n          BaggingClassifier(random_state=rs),DecisionTreeClassifier(random_state=rs),\n          RandomForestClassifier(random_state=rs), GradientBoostingClassifier(random_state=rs),\n          XGBClassifier(random_state=rs), MLPClassifier(random_state=rs),\n          CatBoostClassifier(random_state=rs,verbose = False)]\nalias = [\"LogisticRegression\",\"LGBM\",\"KNN\",\"Bagging\",\n             \"DecisionTree\",\"Random_Forest\",\"GBM\",\"XGBoost\",\"Art.Neural_Network\",\"CatBoost\"]","91e1237f":"print('Default model validation accuracies for the train data:', end = \"\\n\\n\")\nfor name, model in zip(alias, models):\n    model.fit(train_X, train_y)\n    y_pred = model.predict(valid_X) \n    print(name,':',\"%.3f\" % accuracy_score(y_pred, valid_y))","d246a964":"predictors=pd.concat([train_X,valid_X])\noutcomes = []\nprint('10 fold Cross validation accuracy and std of the default models for the train data:', end = \"\\n\\n\")\nfor name, model in zip(alias, models):\n    kfold = KFold(n_splits=10, random_state=2003)\n    cv_results = cross_val_score(model, predictors, y, cv = kfold, scoring = \"accuracy\")\n    outcomes.append(cv_results)\n    print(\"{}: {} ({})\".format(name, \"%.3f\" % cv_results.mean() ,\"%.3f\" %  cv_results.std()))","d30868b4":"cv_models = [LGBMClassifier(),LogisticRegression(random_state=rs),\n          RandomForestClassifier(random_state=rs), GradientBoostingClassifier(random_state=rs),\n          ]\n\n\nparams=[]\nlgbm_params = {\"learning_rate\" : [0.001, 0.01, 0.1, 0.05],\n             \"n_estimators\": [1000,500,100],\n             \"max_depth\": [3,5,10]}\nlog_params = {\"penalty\": [\"l1\",\"l2\"],\n               \"C\":[1, 3, 5],\n            \"solver\":['lbfgs', 'liblinear', 'sag', 'saga'], \"max_iter\":[1000]}\n\nrf_params = {\"max_features\": [\"log2\",\"auto\",\"sqrt\"],\n                \"min_samples_split\":[2,3,5],\n                \"min_samples_leaf\":[1,3,5],\n                \"bootstrap\":[True,False],\n                \"n_estimators\":[50,100,150],\n                \"criterion\":[\"gini\",\"entropy\"]}\ngbm_params = {\"learning_rate\" : [0.001, 0.01, 0.1, 0.05],\n             \"n_estimators\": [100,500,100],\n             \"max_depth\": [3,5,10],\n             \"min_samples_split\": [2,5,10]}\n\nparams = [lgbm_params,log_params,rf_params,gbm_params]\nnew_alias = [\"LGBM\",\"LogisticRegression\",\"RandomForest\",\"GBM\"]\ncv_result = {}\nbest_estimators = {}\nfor name, model,param in zip(new_alias,cv_models,params):\n        clf = GridSearchCV(model, param_grid=param, cv =10, scoring = \"accuracy\", n_jobs = -1,verbose = False)\n        clf.fit(train_X,train_y)\n        cv_result[name]=clf.best_score_\n        best_estimators[name]=clf.best_estimator_\n        print(clf.best_estimator_)\n        print(name,'cross validation accuracy : %.3f'%cv_result[name])\n        print(clf.best_score_)","b1a3d1ef":"accuracies={}\nprint('Validation accuracies of the tuned models for the train data:', end = \"\\n\\n\")\nfor name, model_tuned in zip(best_estimators.keys(),best_estimators.values()):\n    y_pred =  model_tuned.fit(train_X,train_y).predict(valid_X)\n    accuracy=accuracy_score(y_pred, valid_y)\n    print(name,':', \"%.3f\" %accuracy)\n    accuracies[name]=accuracy","f327d0ef":"n=3\naccu=sorted(accuracies, reverse=True, key= lambda k:accuracies[k])[:n]\nfirstn=[[k,v] for k,v in best_estimators.items() if k in accu]\nprint(firstn)\n\nvotingC = VotingClassifier(estimators = firstn, voting = \"soft\", n_jobs = -1)\nmodel= votingC.fit(train_X, train_y)\nprint(\"\\nAccuracy_score is:\",accuracy_score(model.predict(valid_X),valid_y))","5d2bb6e6":"There are no missing values.\n\nLet's move on to see how is the data distributed.\n","7f35ec6f":"LightGBM fetches the highest accuracy.\n\nMoving ahead , we will first use cross validation for accuracy and then perform hyperparamter tuning to further improve the accuracy","76506255":"This gives 79.25 % accuracy.Let's check AUC","fbc2fc6e":"#1. Logistic Regression ","62e6a66f":"Let's create train and test data using train test split "}}