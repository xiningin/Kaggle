{"cell_type":{"ef533d57":"code","509dec23":"code","fcf06662":"code","5de51693":"code","acb25a48":"code","d9d04f18":"code","b31f304e":"code","96813b73":"code","e1be8363":"code","e87aa20b":"code","4117f613":"code","e29899cf":"code","d0d30be7":"code","e1c36efb":"code","db26e315":"code","a117cabf":"code","e8e6dbc4":"code","99dd426e":"code","be447e05":"code","ded666c9":"code","8d3772fa":"code","2c27ae69":"code","620911e4":"code","d4a66417":"code","3f17d846":"code","f0d8f961":"code","58d53750":"code","fcf6019f":"code","4fad9ecc":"code","0a4a1996":"code","dae70469":"code","4d76bd16":"code","c89afcec":"code","d98914fe":"code","878a87d5":"code","b6247995":"code","8dcaf505":"code","c7434309":"code","1d90de47":"code","640d6b44":"code","bfd17825":"code","5fff56f7":"code","a06d1cf4":"code","e24d828f":"code","f2f87d0d":"code","4ca2351c":"code","fc017a49":"code","443c442f":"code","7ee82547":"code","57b5d491":"code","e1d674d2":"code","2156e8e0":"code","0d7d2f07":"code","96dc4564":"code","2e072d39":"code","952af390":"code","0caa27e1":"code","004c6014":"code","e5d0ca0e":"code","2a87b150":"code","c790adf7":"code","907a2c31":"code","857981fd":"code","0f08ad16":"code","2950ddd8":"code","35c6ce8e":"code","eaaa95a6":"code","4480f5f5":"code","d0916a53":"code","a42c97e1":"markdown","67c7b19a":"markdown","a1ed5886":"markdown","d6c75c0e":"markdown","0c09df23":"markdown","507ac69a":"markdown","8423c196":"markdown","5240204b":"markdown","8f45ae88":"markdown","d5560a28":"markdown","d00e381a":"markdown","be97084c":"markdown","5fcde360":"markdown","97bacc03":"markdown","618b1af9":"markdown","1d5ce0d0":"markdown","d5aecc16":"markdown","4f0f9851":"markdown","beca535d":"markdown","2ac8343b":"markdown","a321ea73":"markdown"},"source":{"ef533d57":"!pip install hvplot","509dec23":"import hvplot","fcf06662":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport hvplot.pandas\n\n%matplotlib inline\nsns.set_style(\"whitegrid\")\nplt.style.use(\"fivethirtyeight\")\n\npd.set_option(\"display.float_format\", \"{:.2f}\".format)\npd.set_option(\"display.max_columns\", 80)\npd.set_option(\"display.max_rows\", 80)","5de51693":"df = pd.read_csv(\"\/kaggle\/input\/ibm-hr-analytics-attrition-dataset\/WA_Fn-UseC_-HR-Employee-Attrition.csv\")\ndf.head()","acb25a48":"df.info()","d9d04f18":"df.describe()","b31f304e":"for column in df.columns:\n    print(f\"{column}: Number of unique values {df[column].nunique()}\")\n    print(\"==========================================================\")","96813b73":"df.drop(['EmployeeCount', 'EmployeeNumber', 'Over18', 'StandardHours'], axis=\"columns\", inplace=True)","e1be8363":"object_col = []\nfor column in df.columns:\n    if df[column].dtype == object and len(df[column].unique()) <= 30:\n        object_col.append(column)\n        print(f\"{column} : {df[column].unique()}\")\n        print(df[column].value_counts())\n        print(\"====================================\")\nobject_col.remove('Attrition')","e87aa20b":"len(object_col)","4117f613":"from sklearn.preprocessing import LabelEncoder\n\nlabel = LabelEncoder()\ndf[\"Attrition\"] = label.fit_transform(df.Attrition)","e29899cf":"disc_col = []\nfor column in df.columns:\n    if df[column].dtypes != object and df[column].nunique() < 30:\n        print(f\"{column} : {df[column].unique()}\")\n        disc_col.append(column)\n        print(\"====================================\")\ndisc_col.remove('Attrition')","d0d30be7":"cont_col = []\nfor column in df.columns:\n    if df[column].dtypes != object and df[column].nunique() > 30:\n        print(f\"{column} : Minimum: {df[column].min()}, Maximum: {df[column].max()}\")\n        cont_col.append(column)\n        print(\"====================================\")","e1c36efb":"df.hvplot.hist(y='DistanceFromHome', by='Attrition', subplots=False, width=600, height=300, bins=30)","db26e315":"df.hvplot.hist(y='Education', by='Attrition', subplots=False, width=600, height=300)","a117cabf":"df.hvplot.hist(y='RelationshipSatisfaction', by='Attrition', subplots=False, width=600, height=300)","e8e6dbc4":"df.hvplot.hist(y='EnvironmentSatisfaction', by='Attrition', subplots=False, width=600, height=300)","99dd426e":"df.hvplot.hist(y='JobInvolvement', by='Attrition', subplots=False, width=600, height=300)","be447e05":"df.hvplot.hist(y='JobLevel', by='Attrition', subplots=False, width=600, height=300)","ded666c9":"df.hvplot.hist(y='JobSatisfaction', by='Attrition', subplots=False, width=600, height=300)","8d3772fa":"df.hvplot.hist(y='NumCompaniesWorked', by='Attrition', subplots=False, width=600, height=300)","2c27ae69":"df.hvplot.hist(y='PercentSalaryHike', by='Attrition', subplots=False, width=600, height=300)","620911e4":"df.hvplot.hist(y='StockOptionLevel', by='Attrition', subplots=False, width=600, height=300)","d4a66417":"df.hvplot.hist(y='TrainingTimesLastYear', by='Attrition', subplots=False, width=600, height=300)","3f17d846":"df.hvplot.hist(y='Age', by='Attrition', subplots=False, width=600, height=300, bins=35)","f0d8f961":"df.hvplot.hist(y='MonthlyIncome', by='Attrition', subplots=False, width=600, height=300, bins=50)","58d53750":"df.hvplot.hist(y='YearsAtCompany', by='Attrition', subplots=False, width=600, height=300, bins=35)","fcf6019f":"df.hvplot.hist(y='TotalWorkingYears', by='Attrition', subplots=False, width=600, height=300, bins=35)","4fad9ecc":"plt.figure(figsize=(30, 30))\nsns.heatmap(df.corr(), annot=True, cmap=\"RdYlGn\", annot_kws={\"size\":15})","0a4a1996":"col = df.corr().nlargest(20, \"Attrition\").Attrition.index\nplt.figure(figsize=(15, 15))\nsns.heatmap(df[col].corr(), annot=True, cmap=\"RdYlGn\", annot_kws={\"size\":10})","dae70469":"df.drop('Attrition', axis=1).corrwith(df.Attrition).hvplot.barh()","4d76bd16":"# Transform categorical data into dummies\ndummy_col = [column for column in df.drop('Attrition', axis=1).columns if df[column].nunique() < 20]\ndata = pd.get_dummies(df, columns=dummy_col, drop_first=True, dtype='uint8')\ndata.info()","c89afcec":"print(data.shape)\n\n# Remove duplicate Features\ndata = data.T.drop_duplicates()\ndata = data.T\n\n# Remove Duplicate Rows\ndata.drop_duplicates(inplace=True)\n\nprint(data.shape)","d98914fe":"data.shape","878a87d5":"data.drop('Attrition', axis=1).corrwith(data.Attrition).sort_values().plot(kind='barh', figsize=(10, 30))","b6247995":"feature_correlation = data.drop('Attrition', axis=1).corrwith(data.Attrition).sort_values()\nmodel_col = feature_correlation[np.abs(feature_correlation) > 0.02].index\nlen(model_col)","8dcaf505":"from sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\n\nX = data.drop('Attrition', axis=1)\ny = data.Attrition\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42,\n                                                    stratify=y)\n\nscaler = StandardScaler()\nX_train_std = scaler.fit_transform(X_train)\nX_test_std = scaler.transform(X_test)\nX_std = scaler.transform(X)","c7434309":"def feature_imp(df, model):\n    fi = pd.DataFrame()\n    fi[\"feature\"] = df.columns\n    fi[\"importance\"] = model.feature_importances_\n    return fi.sort_values(by=\"importance\", ascending=False)","1d90de47":"y_test.value_counts()[0] \/ y_test.shape[0]","640d6b44":"stay = (y_train.value_counts()[0] \/ y_train.shape)[0]\nleave = (y_train.value_counts()[1] \/ y_train.shape)[0]\n\nprint(\"===============TRAIN=================\")\nprint(f\"Staying Rate: {stay * 100:.2f}%\")\nprint(f\"Leaving Rate: {leave * 100 :.2f}%\")\n\nstay = (y_test.value_counts()[0] \/ y_test.shape)[0]\nleave = (y_test.value_counts()[1] \/ y_test.shape)[0]\n\nprint(\"===============TEST=================\")\nprint(f\"Staying Rate: {stay * 100:.2f}%\")\nprint(f\"Leaving Rate: {leave * 100 :.2f}%\")","bfd17825":"from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_auc_score\n\ndef evaluate(model, X_train, X_test, y_train, y_test):\n    y_test_pred = model.predict(X_test)\n    y_train_pred = model.predict(X_train)\n\n    print(\"TRAINIG RESULTS: \\n===============================\")\n    clf_report = pd.DataFrame(classification_report(y_train, y_train_pred, output_dict=True))\n    print(f\"CONFUSION MATRIX:\\n{confusion_matrix(y_train, y_train_pred)}\")\n    print(f\"ACCURACY SCORE:\\n{accuracy_score(y_train, y_train_pred):.4f}\")\n    print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n\n    print(\"TESTING RESULTS: \\n===============================\")\n    clf_report = pd.DataFrame(classification_report(y_test, y_test_pred, output_dict=True))\n    print(f\"CONFUSION MATRIX:\\n{confusion_matrix(y_test, y_test_pred)}\")\n    print(f\"ACCURACY SCORE:\\n{accuracy_score(y_test, y_test_pred):.4f}\")\n    print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")","5fff56f7":"from sklearn.linear_model import LogisticRegression\n\nlr_clf = LogisticRegression(solver='liblinear', penalty='l1')\nlr_clf.fit(X_train_std, y_train)\n\nevaluate(lr_clf, X_train_std, X_test_std, y_train, y_test)","a06d1cf4":"from sklearn.metrics import precision_recall_curve, roc_curve\n\ndef plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n    plt.plot(thresholds, recalls[:-1], \"g--\", label=\"Recall\")\n    plt.xlabel(\"Threshold\")\n    plt.legend(loc=\"upper left\")\n    plt.title(\"Precision\/Recall Tradeoff\")\n    \n\ndef plot_roc_curve(fpr, tpr, label=None):\n    plt.plot(fpr, tpr, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], \"k--\")\n    plt.axis([0, 1, 0, 1])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve')\n    \n    \nprecisions, recalls, thresholds = precision_recall_curve(y_test, lr_clf.predict(X_test_std))\nplt.figure(figsize=(14, 25))\nplt.subplot(4, 2, 1)\nplot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n\nplt.subplot(4, 2, 2)\nplt.plot(precisions, recalls)\nplt.xlabel(\"Precision\")\nplt.ylabel(\"Recall\")\nplt.title(\"PR Curve: precisions\/recalls tradeoff\");\n\nplt.subplot(4, 2, 3)\nfpr, tpr, thresholds = roc_curve(y_test, lr_clf.predict(X_test_std))\nplot_roc_curve(fpr, tpr)","e24d828f":"scores_dict = {\n    'Logistic Regression': {\n        'Train': roc_auc_score(y_train, lr_clf.predict(X_train)),\n        'Test': roc_auc_score(y_test, lr_clf.predict(X_test)),\n    },\n}","f2f87d0d":"from sklearn.ensemble import RandomForestClassifier\n\nrf_clf = RandomForestClassifier(n_estimators=100, bootstrap=False,\n#                                      class_weight={0:stay, 1:leave}\n                                    )\nrf_clf.fit(X_train, y_train)\nevaluate(rf_clf, X_train, X_test, y_train, y_test)","4ca2351c":"param_grid = dict(\n    n_estimators= [100, 500, 900], \n    max_features= ['auto', 'sqrt'],\n    max_depth= [2, 3, 5, 10, 15, None], \n    min_samples_split= [2, 5, 10],\n    min_samples_leaf= [1, 2, 4], \n    bootstrap= [True, False]\n)\n\nrf_clf = RandomForestClassifier(random_state=42)\nsearch = GridSearchCV(rf_clf, param_grid=param_grid, scoring='roc_auc', cv=5, verbose=1, n_jobs=-1)\nsearch.fit(X_train, y_train)\n\nrf_clf = RandomForestClassifier(**search.best_params_, random_state=42)\nrf_clf.fit(X_train, y_train)\nevaluate(rf_clf, X_train, X_test, y_train, y_test)","fc017a49":"precisions, recalls, thresholds = precision_recall_curve(y_test, rf_clf.predict(X_test))\nplt.figure(figsize=(14, 25))\nplt.subplot(4, 2, 1)\nplot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n\nplt.subplot(4, 2, 2)\nplt.plot(precisions, recalls)\nplt.xlabel(\"Precision\")\nplt.ylabel(\"Recall\")\nplt.title(\"PR Curve: precisions\/recalls tradeoff\");\n\nplt.subplot(4, 2, 3)\nfpr, tpr, thresholds = roc_curve(y_test, rf_clf.predict(X_test))\nplot_roc_curve(fpr, tpr)","443c442f":"scores_dict['Random Forest'] = {\n        'Train': roc_auc_score(y_train, rf_clf.predict(X_train)),\n        'Test': roc_auc_score(y_test, rf_clf.predict(X_test)),\n    }","7ee82547":"df = feature_imp(X, rf_clf)[:40]\ndf.set_index('feature', inplace=True)\ndf.plot(kind='barh', figsize=(10, 10))\nplt.title('Feature Importance according to Random Forest')","57b5d491":"from sklearn.svm import SVC\n\nsvm_clf = SVC(kernel='linear')\nsvm_clf.fit(X_train_std, y_train)\n\nevaluate(svm_clf, X_train_std, X_test_std, y_train, y_test)","e1d674d2":"svm_clf = SVC(random_state=42)\n\nparam_grid = [\n    {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n    {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']}\n]\n\nsearch = GridSearchCV(svm_clf, param_grid=param_grid, scoring='roc_auc', cv=3, refit=True, verbose=1)\nsearch.fit(X_train_std, y_train)","2156e8e0":"svm_clf = SVC(**search.best_params_)\nsvm_clf.fit(X_train_std, y_train)\n\nevaluate(svm_clf, X_train_std, X_test_std, y_train, y_test)","0d7d2f07":"precisions, recalls, thresholds = precision_recall_curve(y_test, svm_clf.predict(X_test_std))\nplt.figure(figsize=(14, 25))\nplt.subplot(4, 2, 1)\nplot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n\nplt.subplot(4, 2, 2)\nplt.plot(precisions, recalls)\nplt.xlabel(\"Precision\")\nplt.ylabel(\"Recall\")\nplt.title(\"PR Curve: precisions\/recalls tradeoff\");\n\nplt.subplot(4, 2, 3)\nfpr, tpr, thresholds = roc_curve(y_test, svm_clf.predict(X_test_std))\nplot_roc_curve(fpr, tpr)","96dc4564":"scores_dict['Support Vector Machine'] = {\n        'Train': roc_auc_score(y_train, svm_clf.predict(X_train_std)),\n        'Test': roc_auc_score(y_test, svm_clf.predict(X_test_std)),\n    }","2e072d39":"from xgboost import XGBClassifier\n\nxgb_clf = XGBClassifier()\nxgb_clf.fit(X_train, y_train)\n\nevaluate(xgb_clf, X_train, X_test, y_train, y_test)","952af390":"scores_dict['XGBoost'] = {\n        'Train': roc_auc_score(y_train, xgb_clf.predict(X_train)),\n        'Test': roc_auc_score(y_test, xgb_clf.predict(X_test)),\n    }","0caa27e1":"precisions, recalls, thresholds = precision_recall_curve(y_test, xgb_clf.predict(X_test))\nplt.figure(figsize=(14, 25))\nplt.subplot(4, 2, 1)\nplot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n\nplt.subplot(4, 2, 2)\nplt.plot(precisions, recalls)\nplt.xlabel(\"Precision\")\nplt.ylabel(\"Recall\")\nplt.title(\"PR Curve: precisions\/recalls tradeoff\");\n\nplt.subplot(4, 2, 3)\nfpr, tpr, thresholds = roc_curve(y_test, xgb_clf.predict(X_test))\nplot_roc_curve(fpr, tpr)","004c6014":"df = feature_imp(X, xgb_clf)[:35]\ndf.set_index('feature', inplace=True)\ndf.plot(kind='barh', figsize=(10, 8))\nplt.title('Feature Importance according to XGBoost')","e5d0ca0e":"from lightgbm import LGBMClassifier\n\nlgb_clf = LGBMClassifier()\nlgb_clf.fit(X_train, y_train)\n\nevaluate(lgb_clf, X_train, X_test, y_train, y_test)","2a87b150":"precisions, recalls, thresholds = precision_recall_curve(y_test, lgb_clf.predict(X_test))\nplt.figure(figsize=(14, 25))\nplt.subplot(4, 2, 1)\nplot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n\nplt.subplot(4, 2, 2)\nplt.plot(precisions, recalls)\nplt.xlabel(\"Precision\")\nplt.ylabel(\"Recall\")\nplt.title(\"PR Curve: precisions\/recalls tradeoff\");\n\nplt.subplot(4, 2, 3)\nfpr, tpr, thresholds = roc_curve(y_test, lgb_clf.predict(X_test))\nplot_roc_curve(fpr, tpr)","c790adf7":"scores_dict['LightGBM'] = {\n        'Train': roc_auc_score(y_train, lgb_clf.predict(X_train)),\n        'Test': roc_auc_score(y_test, lgb_clf.predict(X_test)),\n    }","907a2c31":"from catboost import CatBoostClassifier\n\ncb_clf = CatBoostClassifier()\ncb_clf.fit(X_train, y_train, verbose=0)\n\nevaluate(cb_clf, X_train, X_test, y_train, y_test)","857981fd":"precisions, recalls, thresholds = precision_recall_curve(y_test, cb_clf.predict(X_test))\nplt.figure(figsize=(14, 25))\nplt.subplot(4, 2, 1)\nplot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n\nplt.subplot(4, 2, 2)\nplt.plot(precisions, recalls)\nplt.xlabel(\"Precision\")\nplt.ylabel(\"Recall\")\nplt.title(\"PR Curve: precisions\/recalls tradeoff\");\n\nplt.subplot(4, 2, 3)\nfpr, tpr, thresholds = roc_curve(y_test, cb_clf.predict(X_test))\nplot_roc_curve(fpr, tpr)","0f08ad16":"scores_dict['CatBoost'] = {\n        'Train': roc_auc_score(y_train, cb_clf.predict(X_train)),\n        'Test': roc_auc_score(y_test, cb_clf.predict(X_test)),\n    }","2950ddd8":"from sklearn.ensemble import AdaBoostClassifier\n\nab_clf = AdaBoostClassifier()\nab_clf.fit(X_train, y_train)\n\nevaluate(ab_clf, X_train, X_test, y_train, y_test)","35c6ce8e":"precisions, recalls, thresholds = precision_recall_curve(y_test, ab_clf.predict(X_test))\nplt.figure(figsize=(14, 25))\nplt.subplot(4, 2, 1)\nplot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n\nplt.subplot(4, 2, 2)\nplt.plot(precisions, recalls)\nplt.xlabel(\"Precision\")\nplt.ylabel(\"Recall\")\nplt.title(\"PR Curve: precisions\/recalls tradeoff\");\n\nplt.subplot(4, 2, 3)\nfpr, tpr, thresholds = roc_curve(y_test, ab_clf.predict(X_test))\nplot_roc_curve(fpr, tpr)","eaaa95a6":"scores_dict['AdaBoost'] = {\n        'Train': roc_auc_score(y_train, ab_clf.predict(X_train)),\n        'Test': roc_auc_score(y_test, ab_clf.predict(X_test)),\n    }","4480f5f5":"ml_models = {\n    'Random Forest': rf_clf, \n    'XGBoost': xgb_clf, \n    'Logistic Regression': lr_clf,\n    'Support Vector Machine': svm_clf,\n    'LightGBM': lgb_clf,\n    'CatBoost': cb_clf,\n    'AdaBoost': ab_clf\n}\n\nfor model in ml_models:\n    print(f\"{model.upper():{30}} roc_auc_score: {roc_auc_score(y_test, ml_models[model].predict(X_test)):.3f}\")","d0916a53":"scores_df = pd.DataFrame(scores_dict)\n# scores_df.plot(kind='barh', figsize=(15, 8))\nscores_df.hvplot.barh()","a42c97e1":"---\n## \ud83d\udcc9 Categorical Features","67c7b19a":"---\n# \ud83d\udd28 Data Processing","a1ed5886":"---\n# \ud83d\udcca Exploratory Data Analysis\n\n- Find patterns in data through data visualization. Reveal hidden secrets of the data through graphs, analysis and charts.\n    - Univariate analysis\n        > - Continous variables : Histograms, boxplots. This gives us understanding about the central tendency and spread\n        > - Categorical variable : Bar chart showing frequency in each category\n    - Bivariate analysis\n        > - Continous & Continous : Scatter plots to know how continous variables interact with each other\n        > - Categorical & categorical : Stacked column chart to show how the frequencies are spread between two\n        > - categorical variables\n        > - Categorical & Continous : Boxplots, Swamplots or even bar charts\n- Detect outliers\n- Feature engineering","d6c75c0e":"## \u2714\ufe0f Random Forest Classifier","0c09df23":"## \u2714\ufe0f XGBoost Classifier","507ac69a":"# \ud83c\udfaf Comparing Models Prerformance \ud83d\udcca","8423c196":"## \u2714\ufe0f Support Vector Machine","5240204b":"---\n# \ud83d\udcc9 Correlation Matrix","8f45ae88":"## \u2714\ufe0f LightGBM","d5560a28":"## \ud83d\udcdd **Analysis of correlation results (sample analysis):**\n- Monthly income is highly correlated with Job level.\n- Job level is highly correlated with total working hours.\n- Monthly income is highly correlated with total working hours.\n- Age is also positively correlated with the Total working hours.\n- Marital status and stock option level are negatively correlated","d00e381a":"***\n\n## \ud83d\udcdd **Conclusions:**\n\n***\n- The workers with low `JobLevel`, `MonthlyIncome`, `YearAtCompany`, and `TotalWorkingYears` are more likely to quit there jobs.\n- `BusinessTravel` : The workers who travel alot are more likely to quit then other employees.\n\n- `Department` : The worker in `Research & Development` are more likely to stay then the workers on other departement.\n\n- `EducationField` : The workers with `Human Resources` and `Technical Degree` are more likely to quit then employees from other fields of educations.\n\n- `Gender` : The `Male` are more likely to quit.\n\n- `JobRole` : The workers in `Laboratory Technician`, `Sales Representative`, and `Human Resources` are more likely to quit the workers in other positions.\n\n- `MaritalStatus` : The workers who have `Single` marital status are more likely to quit the `Married`, and `Divorced`.\n\n- `OverTime` : The workers who work more hours are likely to quit then others.\n\n*** ","be97084c":"## \u2714\ufe0f CatBoost","5fcde360":"## \ud83d\udccc Note\n\nIt seems that `EnvironmentSatisfaction`, `JobSatisfaction`, `PerformanceRating`, and `RelationshipSatisfaction` features don't have big impact on the detrmination of `Attrition` of employees.","97bacc03":"---\n# \ud83d\udcbc Attrition in an Organization || Why Workers Quit?\n\n---\n\nEmployees are the backbone of the organization. Organization's performance is heavily based on the quality of the employees. Challenges that an organization has to face due employee attrition are:\n\n> 1. Expensive in terms of both money and time to train new employees.\n> 1. Loss of experienced employees\n> 1. Impact in productivity\n> 1. Impact profit\n\nBefore getting our hands dirty with the data, first step is to frame the business question. Having clarity on below questions is very crucial because the solution that is being developed will make sense only if we have well stated problem.\n\n------\n## \ud83d\udccc Business questions to brainstorm:\n---\n> 1. What factors are contributing more to employee attrition?\n> 1. What type of measures should the company take in order to retain their employees?\n> 1. What business value does the model bring?\n> 1. Will the model save lots of money?\n> 1. Which business unit faces the attrition problem?","618b1af9":"## \u2714\ufe0f AdaBoost","1d5ce0d0":"# \ud83e\udd16 Applying machine learning algorithms","d5aecc16":"---\n## \ud83d\udcc9 Data Visualisation","4f0f9851":"## \u2714\ufe0f Logistic Regression","beca535d":"## \ud83c\udfaf What defines success?\n> We have an imbalanced data, so if we predict that all our employees will stay we'll have an accuracy of `83.90%`. \n","2ac8343b":"---\n## \ud83d\udcc9 Numerical Features","a321ea73":"We notice that '`EmployeeCount`', '`Over18`', '`StandardHours`' have only one unique values and '`EmployeeNumber`' has `1470` unique values.\nThis features aren't useful for us, So we are going to drop those columns."}}