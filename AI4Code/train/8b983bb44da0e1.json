{"cell_type":{"ab80931e":"code","57ab8915":"code","9e5d9bd4":"code","f20e897a":"code","7118c6a2":"code","2a3a4859":"code","c0807287":"code","a46bb6ec":"code","15de4b4b":"code","1757d0fc":"code","2df5234d":"code","dc6d3259":"code","53151bb1":"code","81db4f9e":"code","e1e5337f":"code","c0af8ecc":"code","524b839a":"code","34f983fd":"code","86feb914":"code","b0ea3f5e":"code","397a32e5":"code","d80b9abd":"code","ab6ac4d5":"code","080f8108":"code","615c9f47":"code","e48c8b0d":"code","db74e0a9":"code","a0342107":"code","70672280":"code","fa396543":"code","47765cd6":"code","e69b9df0":"code","48deb09b":"code","0f26b46a":"code","cb066421":"code","ba26d81f":"code","3e2ed6ec":"code","7cb6b7eb":"code","ed672362":"code","d8e324e6":"code","1e4a4f8e":"code","351a43d1":"code","3bb616eb":"markdown"},"source":{"ab80931e":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn import model_selection\n\nfrom sklearn.experimental import enable_iterative_imputer  # noqa\nfrom sklearn.impute import IterativeImputer\n\nimport joblib\nimport pandas as pd\nfrom sklearn import metrics\nfrom sklearn import tree\n\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn.metrics import confusion_matrix,f1_score,accuracy_score,classification_report\nfrom sklearn.model_selection import StratifiedKFold\nimport xgboost as xgb\nfrom sklearn import metrics\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler","57ab8915":"train_df = pd.read_csv(\"..\/input\/song-popularity-prediction\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/song-popularity-prediction\/test.csv\")\n","9e5d9bd4":"submission = pd.read_csv(\"..\/input\/song-popularity-prediction\/sample_submission.csv\")","f20e897a":"train_df.head()","7118c6a2":"train_df.describe()","2a3a4859":"train_df.shape\ntest_df.shape","c0807287":"train_df.isnull().sum()\/len(train_df)*100","a46bb6ec":"test_df.isnull().sum()\/len(train_df)*100","15de4b4b":"FEATURES = [i for i in train_df.columns.to_list() if i not in ['n_missing', 'kfold', 'id', 'song_popularity']]","1757d0fc":"train_df[\"isTrain\"] = True\ntest_df[\"isTrain\"] = False\n\ntt = pd.concat([train_df, test_df]).reset_index(drop=True).copy()","2df5234d":"%%time\n# handling missing values \nit_imputer = IterativeImputer(max_iter=10)\ntrain_iterimp = it_imputer.fit_transform(train_df[FEATURES])\ntest_iterimp = it_imputer.transform(test_df[FEATURES])\ntt_iterimp = it_imputer.fit_transform(tt[FEATURES])","dc6d3259":"train_iterimp = pd.DataFrame(train_iterimp, columns=FEATURES)\ntest_iterimp = pd.DataFrame(test_iterimp, columns=FEATURES)\ntt_iterimp = pd.DataFrame(train_iterimp, columns=FEATURES)","53151bb1":" train_df_cl = pd.concat([train_iterimp,train_df['song_popularity']],axis=1)","81db4f9e":"# we create a new column called kfold and fill it with -1\ntrain_df_cl[\"kfold\"] = -1\n# the next step is to randomize the rows of the data\ntrain_df_cl = train_df_cl.sample(frac=1).reset_index(drop=True)\n# fetch targets\ny = train_df_cl.song_popularity.values\n# initiate the kfold class from model_selection module\nkf = model_selection.StratifiedKFold(n_splits=5)\n# fill the new kfold column\nfor f, (t_, v_) in enumerate(kf.split(X=train_df_cl, y=y)):\n    train_df_cl.loc[v_, 'kfold'] = f\n# save the new csv with kfold column\ntrain_df_cl.to_csv(\"train_folds.csv\", index=False)","e1e5337f":"train_df_cl.song_popularity.value_counts()","c0af8ecc":"train_df_cl['kfold']=train_df_cl['kfold'].astype('int')","524b839a":"train_df_cl.head()","34f983fd":"\nfrom sklearn import ensemble\nfrom sklearn import tree","86feb914":"models = {\n\"decision_tree_gini\": tree.DecisionTreeClassifier(\ncriterion=\"gini\"\n),\n\"decision_tree_entropy\": tree.DecisionTreeClassifier(\ncriterion=\"entropy\"\n),\n\"rf\": ensemble.RandomForestClassifier(),\n}","b0ea3f5e":"train_df_cl.head()","397a32e5":"imp_cols=[i for i in train_df_cl.columns.to_list() if i not in ['song_popularity','kfold']]","d80b9abd":"test_iterimp.columns","ab6ac4d5":"\nfinal_test_preds = []\nfor fold in range(5):\n    # read the training data with folds\n    df = pd.read_csv(\".\/train_folds.csv\")\n    # training data is where kfold is not equal to provided fold\n    # also, note that we reset the index\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    # validation data is where kfold is equal to provided fold\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n    # drop the label column from dataframe and convert it to\n    # a numpy array by using .values.\n    # target is label column in the dataframe\n#     df_train = df_train[imp_cols]\n#     df_valid = df_valid[imp_cols]\n    \n    x_train = df_train.drop(columns=[\"song_popularity\",\"kfold\"], axis=1).values\n    y_train = df_train.song_popularity.values\n    # similarly, for validation, we have\n    x_valid = df_valid.drop(columns=[\"song_popularity\",\"kfold\"], axis=1).values\n    y_valid = df_valid.song_popularity.values\n    # initialize simple decision tree classifier from sklearn\n#     clf = tree.DecisionTreeClassifier()\n    \n    # initialize random forest model\n    clf = ensemble.RandomForestClassifier(n_jobs=-1)\n    # fit the model on training data\n    clf.fit(x_train, y_train)\n    # create predictions for validation samples\n    preds = clf.predict(x_valid)\n    \n    # calculate & print accuracy\n    accuracy = metrics.accuracy_score(y_valid, preds)\n    print(f\"Fold={fold}, Accuracy={accuracy}\")\n    # save the model\n    joblib.dump(clf, f\".\/dt_{fold}.bin\")\n    \n    preds_test = clf.predict(test_iterimp)\n    final_test_preds.append(preds_test)\n    ","080f8108":"final_preds = np.mean(np.column_stack(final_test_preds), axis=1)\nsubmit = submission.copy()\nsubmit['song_popularity'] = final_preds\nsubmit","615c9f47":"submit.to_csv('submit_rf.csv', index=False)\nprint(\"--Done--\")","e48c8b0d":"final_test_preds = []\nfor i in range(5):\n    # load the full training data with folds\n    df = pd.read_csv(\".\/train_folds.csv\")\n    # all columns are features except id, target and kfold columns\n    features = [\n    f for f in df.columns if f not in (\"song_popularity\", \"kfold\")\n    ]\n    # fill all NaN values with NONE\n    # note that I am converting all columns to \"strings\"\n    # it doesnt matter because all are categories\n    for col in features:\n        df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")\n        # now it\u2019s time to label encode the features\n    for col in features:\n    # initialize LabelEncoder for each feature column\n        lbl = preprocessing.LabelEncoder()\n        # fit label encoder on all data\n        lbl.fit(df[col])\n        # transform all the data\n        df.loc[:, col] = lbl.transform(df[col])\n    # get training data using folds\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    # get validation data using folds\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n    # get training data\n    x_train = df_train[features].values\n    # get validation data\n    x_valid = df_valid[features].values\n    # initialize xgboost model\n    model = xgb.XGBClassifier(\n    n_jobs=-1,\n    max_depth=7,\n    n_estimators=200\n    )\n    # fit model on training data (ohe)\n    model.fit(x_train, df_train.song_popularity.values)\n    # predict on validation data\n    # we need the probability values as we are calculating AUC\n    # we will use the probability of 1s\n    valid_preds = model.predict_proba(x_valid)[:, 1]\n    # get roc auc score\n    auc = metrics.roc_auc_score(df_valid.song_popularity.values, valid_preds)\n    # print auc\n    print(f\"Fold = {fold}, AUC = {auc}\")\n    \n    preds_test = clf.predict(test_iterimp)\n    final_test_preds.append(preds_test)\n    \n    ","db74e0a9":"final_test_preds","a0342107":"final_preds = np.mean(np.column_stack(final_test_preds), axis=1)\nsubmit = submission.copy()\nsubmit['song_popularity'] = final_preds\nsubmit\n","70672280":"submit.to_csv('submit_xgb.csv', index=False)\nprint(\"--Done--\")","fa396543":"\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, f1_score","47765cd6":"def get_optimal_f1_thresh(target, oofs):\n  thresholds = np.arange(0, 100)\/100\n  thresh_scores = []\n  for thresh in thresholds:\n    oofs_rounded = (lgb_oofs > thresh) * 1\n    thresh_score = f1_score(target, oofs_rounded)\n    thresh_scores.append(thresh_score)\n  \n  all_thresholds_and_scores = pd.Series(index = thresholds, data = thresh_scores)\n  all_thresholds_and_scores.plot(figsize=(10, 6), fontsize=14)\n  \n  plt.xlabel('Threshold', fontsize=14)\n  plt.ylabel('F1 Score', fontsize=14)\n\n  return all_thresholds_and_scores.sort_values(ascending=False).index.values[0]","e69b9df0":"def run_gradient_boosting(clf, fit_params, train, test, features):\n  N_SPLITS = 5\n  oofs = np.zeros(len(train_proc))\n  preds = np.zeros((len(test_proc)))\n\n  folds = StratifiedKFold(n_splits = N_SPLITS)\n\n  for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train[TARGET_COL])):\n    print(f'\\n------------- Fold {fold_ + 1} -------------')\n\n    ### Training Set\n    X_trn, y_trn = train[features].iloc[trn_idx], target.iloc[trn_idx]\n\n    ### Validation Set\n    X_val, y_val = train[features].iloc[val_idx], target.iloc[val_idx]\n\n    ### Test Set\n    X_test = test[features]\n\n    scaler = StandardScaler()\n    _ = scaler.fit(X_trn)\n\n    X_trn = scaler.transform(X_trn)\n    X_val = scaler.transform(X_val)\n    X_test = scaler.transform(X_test)\n    \n    _ = clf.fit(X_trn, y_trn, eval_set = [(X_val, y_val)], **fit_params)\n\n    ### Instead of directly predicting the classes we will obtain the probability of positive class.\n    preds_val = clf.predict_proba(X_val)[:, 1]\n    preds_test = clf.predict_proba(X_test)[:, 1]\n\n    fold_score = f1_score(y_val, preds_val.round())\n    print(f'\\nF1 score for validation set is {fold_score}')\n\n    oofs[val_idx] = preds_val\n    preds += preds_test \/ N_SPLITS\n\n\n  oofs_score = f1_score(target, oofs.round())\n  print(f'\\n\\nF1 score for oofs is {oofs_score}')\n\n  return oofs, preds","48deb09b":"train_proc = pd.read_csv(\".\/train_folds.csv\")\ntrain_proc =train_proc.drop(columns=['kfold'],axis=1)","0f26b46a":"TARGET_COL='song_popularity'","cb066421":"test_proc = test_iterimp","ba26d81f":"features= FEATURES\ntarget = train_proc[TARGET_COL]","3e2ed6ec":"clf = LGBMClassifier(n_estimators = 1000,\n                        learning_rate = 0.01,\n                        colsample_bytree = 0.65,\n                        )\nfit_params = {'verbose': 100, 'early_stopping_rounds': 100}\n\nlgb_oofs, lgb_preds = run_gradient_boosting(clf, fit_params, train_proc, test_proc, features)\n\noptimal_thresh = get_optimal_f1_thresh(target, lgb_oofs)\nprint(f'Optimal threhold is {optimal_thresh}')\noptimized_f1 = f1_score(target, (lgb_oofs > optimal_thresh) * 1)\nprint(f'Optimized F1 is {optimized_f1}')","7cb6b7eb":"lgbpreds= lgb_preds>0.28\n","ed672362":"lgbpreds","d8e324e6":"submit = submission.copy()\nsubmit['song_popularity'] = lgbpreds\n","1e4a4f8e":"submit.to_csv('submit_lgm.csv', index=False)\nprint(\"--Done--\")","351a43d1":"clf = XGBClassifier(n_estimators = 1000,\n                    max_depth = 6,\n                    learning_rate = 0.01,\n                    colsample_bytree = 0.5,\n                    random_state=1452,\n                    )\n\nfit_params = {'verbose': 200, 'early_stopping_rounds': 200}\n\nxgb_oofs, xgb_preds = run_gradient_boosting(clf, fit_params, train_proc, test_proc, features)\n\noptimal_thresh = get_optimal_f1_thresh(target, xgb_oofs)\nprint(f'Optimal threhold is {optimal_thresh}')\noptimized_f1 = f1_score(target, (xgb_oofs > optimal_thresh) * 1)\nprint(f'Optimized F1 is {optimized_f1}')","3bb616eb":"### XGBoost"}}