{"cell_type":{"e1888126":"code","7a442a40":"code","2e5720f2":"code","17c8f097":"code","1b874bcb":"code","672917da":"code","692097fc":"code","f555eacf":"code","c311e266":"code","d1206a31":"code","c19ef58a":"code","e76e3487":"code","083649fa":"code","06212d7a":"code","1d48f47c":"code","4c35f445":"code","a7d79835":"code","2c19722b":"code","78202e2b":"code","5b6ea99f":"code","a630adc1":"code","2fa7e18e":"code","81f6fced":"code","5b93db08":"code","4b67fc0b":"code","3f664983":"markdown","f888f018":"markdown","0d792298":"markdown","ed0cf219":"markdown","3ee21c93":"markdown","4b7ee229":"markdown","a5322e47":"markdown","5d354796":"markdown","e8910087":"markdown","5ea5978a":"markdown","6f43ff12":"markdown","b3363209":"markdown","c09ad629":"markdown","a909cb67":"markdown","fcc7872e":"markdown","441cca7b":"markdown","c416cd00":"markdown"},"source":{"e1888126":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7a442a40":"# Importing the libraries\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport string\nimport math\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom xgboost import XGBClassifier\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import metrics\nimport re\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer","2e5720f2":"train_dataset = pd.read_csv(\"\/kaggle\/input\/covid-19-nlp-text-classification\/Corona_NLP_train.csv\",encoding=\"latin\")\ntest_dataset = pd.read_csv(\"\/kaggle\/input\/covid-19-nlp-text-classification\/Corona_NLP_test.csv\",encoding=\"latin\")","17c8f097":"print(train_dataset.shape)\nprint(test_dataset.shape)","1b874bcb":"print(train_dataset.columns)\nprint(test_dataset.columns)","672917da":"train_dataset[\"Sentiment\"].unique()","692097fc":"def classes_def(x):\n    if x ==  \"Extremely Positive\":\n        return \"2\"\n    elif x == \"Extremely Negative\":\n        return \"0\"\n    elif x == \"Negative\":\n        return \"0\"\n    elif x ==  \"Positive\":\n        return \"2\"\n    else:\n        return \"1\"\n    \n\ntrain_dataset['class']=train_dataset['Sentiment'].apply(lambda x:classes_def(x))","f555eacf":"train_dataset[\"class\"].value_counts(normalize= True)","c311e266":"from bs4 import BeautifulSoup\nSTOPWORDS = set(stopwords.words('english'))\n\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n\nfrom tqdm import tqdm\npreprocessed_tweets = []\n# tqdm is for printing the status bar\nfor sentance in tqdm(train_dataset['OriginalTweet'].values):\n    sentance = re.sub(r'https?:\/\/\\S+|www\\.\\S+', r'', sentance) # remove URLS\n    sentance = re.sub(r'<.*?>', r'', sentance) # remove HTML\n    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n    sentance = decontracted(sentance)\n    sentance = re.sub(r'\\d+', '', sentance).strip() # remove number\n    sentance = re.sub(r\"[^\\w\\s\\d]\",\"\", sentance) # remove pnctuations\n    sentance = re.sub(r'@\\w+','', sentance) # remove mentions\n    sentance = re.sub(r'#\\w+','', sentance) # remove hash\n    sentance = re.sub(r\"\\s+\",\" \", sentance).strip() # remove space\n    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n    \n    sentance = ' '.join([e.lower() for e in sentance.split() if e.lower() not in STOPWORDS])\n    preprocessed_tweets.append(sentance.strip())","d1206a31":"tf_idf_vect = TfidfVectorizer(min_df=10)\ntf_idf_vect.fit(preprocessed_tweets)\nprint(\"some sample features(unique words in the corpus)\",tf_idf_vect.get_feature_names()[0:10])\nprint('='*50)\n\nfinal_tf_idf = tf_idf_vect.transform(preprocessed_tweets)\nprint(\"the type of count vectorizer \",type(final_tf_idf))\nprint(\"the shape of out text TFIDF vectorizer \",final_tf_idf.get_shape())\nprint(\"the number of unique words including both unigrams and bigrams \", final_tf_idf.get_shape()[1])","c19ef58a":"X = final_tf_idf\ny = train_dataset[\"class\"].tolist()\n\nX_train, X_test, y_train, y_test = train_test_split(X.tocsr(), y, test_size= 0.33, stratify=y,  random_state=42)","e76e3487":"grid_params ={'alpha':[10**x for x in range(-4,4)]}\nalpha_log = [math.log(x,10) for x in grid_params[\"alpha\"]]\n\nMultinomialNB_model = GridSearchCV(MultinomialNB(),grid_params,\n                     scoring = 'accuracy', cv=10,n_jobs=-1, return_train_score=True)\nMultinomialNB_model.fit(X_train, y_train)","083649fa":"results = pd.DataFrame.from_dict(MultinomialNB_model.cv_results_)\nresults = results.sort_values(['param_alpha'])\n\nplt.plot(alpha_log, results[\"mean_train_score\"], label='Train Accuracy')\nplt.plot(alpha_log, results[\"mean_test_score\"].values, label='CV Accuracy')\n\nplt.scatter(alpha_log, results[\"mean_train_score\"].values, label='Train Accuracy points')\nplt.scatter(alpha_log, results[\"mean_test_score\"].values, label='CV Accuracy points')\n\nplt.legend()\nplt.xlabel(\"Alpha: hyperparameter\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"ERROR PLOTS\")\nplt.grid()\nplt.show()\nprint(MultinomialNB_model.best_estimator_)","06212d7a":"MultinomialNB_model = MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)\nMultinomialNB_model.fit(X_train,y_train)\n\ny_pred = MultinomialNB_model.predict(X_test)\ncm=confusion_matrix(y_test, y_pred)\ncm_df=pd.DataFrame(cm,index=[0,1,2],columns=[0,1,2])\nprint(\"Accuracy:\",accuracy_score(y_test, y_pred))\n\nsns.set(font_scale=1.4,color_codes=True,palette=\"deep\")\nsns.heatmap(cm_df,annot=True,annot_kws={\"size\":16},fmt=\"d\",cmap=\"YlGnBu\")\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Value\")\nplt.ylabel(\"True Value\")","1d48f47c":"print(metrics.classification_report(y_test, y_pred, \n                                    target_names= train_dataset['class'].unique()))","4c35f445":"max_depth = [1,5,10,50]\nn_estimators = [5,10,100,500]\ngrid_params ={'max_depth':max_depth,'n_estimators':n_estimators}\n\nRandomFoest_model = GridSearchCV(RandomForestClassifier(class_weight = 'balanced'), grid_params,\n                  scoring = 'accuracy', cv=10,n_jobs=-1, return_train_score=True)\nRandomFoest_model.fit(X_train, y_train)\n\nresults = pd.DataFrame.from_dict(RandomFoest_model.cv_results_)\nprint(RandomFoest_model.best_estimator_)","a7d79835":"from mpl_toolkits.mplot3d import Axes3D\nimport matplotlib\n\nmax_depth = [1,1,1,1,5,5,5,5,10,10,10,10,50,50,50,50]\nn_estimators = [5,10,100,500,5,10,100,500,5,10,100,500,5,10,100,500]\nmean_train_score = list(results[\"mean_train_score\"].values)\nmean_test_score = list(results[\"mean_test_score\"].values)\n\nfig = matplotlib.pyplot.figure(figsize=(12,6))\nax = fig.add_subplot(111, projection='3d')\n\nax.scatter(max_depth, n_estimators, mean_train_score, c='r', marker='o')\nax.scatter(max_depth, n_estimators, mean_test_score, c='b', marker='o')\n\nax.set_xlabel('max_depth ')\nax.set_ylabel('n_estimators')\nax.set_zlabel('Accuracy')","2c19722b":"RandomFoest_model = RandomForestClassifier(bootstrap=True, class_weight='balanced',\n            criterion='gini', max_depth=50, max_features='auto',\n            max_leaf_nodes=None, min_impurity_decrease=0.0,\n            min_impurity_split=None, min_samples_leaf=1,\n            min_samples_split=2, min_weight_fraction_leaf=0.0,\n            n_estimators=500, n_jobs=1, oob_score=False, random_state=None,\n            verbose=0, warm_start=False)\nRandomFoest_model.fit(X_train,y_train)\n\ny_pred = RandomFoest_model.predict(X_test)\ncm=confusion_matrix(y_test, y_pred)\ncm_df=pd.DataFrame(cm,index=[0,1,2],columns=[0,1,2])\nprint(\"Accuracy:\",accuracy_score(y_test, y_pred))\n\nsns.set(font_scale=1.4,color_codes=True,palette=\"deep\")\nsns.heatmap(cm_df,annot=True,annot_kws={\"size\":16},fmt=\"d\",cmap=\"YlGnBu\")\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Value\")\nplt.ylabel(\"True Value\")","78202e2b":"print(metrics.classification_report(y_test, y_pred, \n                                    target_names= train_dataset['class'].unique()))","5b6ea99f":"alpha = [10**x for x in range(-4,4)]\npenalty = [\"l1\",\"l2\"]\ngrid_params ={'alpha':alpha,'penalty':penalty}\nalpha_log = [math.log(x,10) for x in grid_params[\"alpha\"]]\n\nSGDClassifier_model = GridSearchCV(SGDClassifier(class_weight= 'balanced'), grid_params,\n                     scoring = 'accuracy', cv=10,n_jobs=-1, return_train_score=True)\nSGDClassifier_model.fit(X_train, y_train)\n\nresults = pd.DataFrame.from_dict(SGDClassifier_model.cv_results_)\nresults = results.sort_values(['param_alpha'])\n\nprint(SGDClassifier_model.best_estimator_)","a630adc1":"SGDClassifier_model = SGDClassifier(class_weight='balanced', penalty='l1')\nSGDClassifier_model.fit(X_train,y_train)\n\ny_pred = SGDClassifier_model.predict(X_test)\ncm=confusion_matrix(y_test, y_pred)\ncm_df=pd.DataFrame(cm,index=[0,1,2],columns=[0,1,2])\nprint(\"Accuracy:\",accuracy_score(y_test, y_pred))\n\nsns.set(font_scale=1.4,color_codes=True,palette=\"deep\")\nsns.heatmap(cm_df,annot=True,annot_kws={\"size\":16},fmt=\"d\",cmap=\"YlGnBu\")\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Value\")\nplt.ylabel(\"True Value\")","2fa7e18e":"print(metrics.classification_report(y_test, y_pred, \n                                    target_names= train_dataset['class'].unique()))","81f6fced":"learning_rate = [0.0001, 0.001, 0.01, 0.1]\nmax_depth = [1,3,5,7]\nn_estimators = [5,10,100,500]\ngrid_params ={'max_depth':max_depth,'n_estimators':n_estimators, 'learning_rate':learning_rate}\n\nXGBoost_model = GridSearchCV(XGBClassifier(), grid_params,\n                      scoring = 'accuracy', cv=10,n_jobs=-1, return_train_score=True)\nXGBoost_model.fit(X_train, y_train)\n\nresults = pd.DataFrame.from_dict(XGBoost_model.cv_results_)\nprint(XGBoost_model.best_estimator_)","5b93db08":"XGBoost_model = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n       max_depth=7, min_child_weight=1, missing=None, n_estimators=500,\n       n_jobs=1, nthread=None, objective='multi:softprob', random_state=0,\n       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n       silent=True, subsample=1)\nXGBoost_model.fit(X_train,y_train)\n\ny_pred = XGBoost_model.predict(X_test)\ncm=confusion_matrix(y_test, y_pred)\ncm_df=pd.DataFrame(cm,index=[0,1,2],columns=[0,1,2])\nprint(\"Accuracy:\",accuracy_score(y_test, y_pred))\n\nsns.set(font_scale=1.4,color_codes=True,palette=\"deep\")\nsns.heatmap(cm_df,annot=True,annot_kws={\"size\":16},fmt=\"d\",cmap=\"YlGnBu\")\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Value\")\nplt.ylabel(\"True Value\")","4b67fc0b":"print(metrics.classification_report(y_test, y_pred, \n                                    target_names= train_dataset['class'].unique()))","3f664983":"# Random Forest","f888f018":"From the results, we can see that recall & Precision is imporved and descent for all classes.","0d792298":"* Precision, Recall and F1 score  for XGBoost model is descent enough than other models. So will go with XGBoost Model.","ed0cf219":"Here, there are five classes:'Neutral, Positive, Extremely Negative, Negative,Extremely Positive'.<br>\nExtremely Negative & Negative is encoded as 0.<br>\nExtremely Positive & Positive is encoded as 2.<br>\nNeutral is encoded as 2.","3ee21c93":"# TF-IDF","4b7ee229":"From the results, we can see that recall is imporved but precision is low for class 2.<br>\nPrecision = True Positive\/(True Positive + False Positive).<br>\nWhich implies, **51% tweets** indicating **Covid-19 Negative** but model predicting as **Positive** or **Netrual**.<br>\nRecall and Precision is descent for class 1 and 2.","a5322e47":"# XGBoost","5d354796":"# MultinomialNB","e8910087":"From the results, we can see that recall is imporved but precision is low for class 2.<br>\nPrecision = True Positive\/(True Positive + False Positive).<br>\nWhich implies, **42% tweets** indicating **Covid-19 Negative** but model predicting as **Positive** or **Netrual**.<br>\nRecall and Precision is good for class 1 and 2.","5ea5978a":"# Coronavirus Tweets NLP - Text Classifiation\nThis notebook aims at building at text classification engine from the content of Coronavirus Tweets NLP - Text Classifiation dataset that contains around 41157 reviews. Basically, the engine works as follows: after user has provided with tweet, the engine cleans the data and tries to classify the tweet as positive, negative or neutral.","6f43ff12":"# Text Cleaning","b3363209":"# Modeling","c09ad629":"# Libraries","a909cb67":"# SGD Classifier","fcc7872e":"The Notebook is organised as follows.\n\n**1.Text Cleaning**\n\n* Removing the URLS \n* Removing HTML tags\n* Removing Numbers\/Digits\n* Removing Punctuations\n* Removing Mentions\n* Removing Hash\n* Removing extra spaces\n\n**2.Converting Text to Numerical Vector**  \n* TF-IDF\n\n**3.Modeling**\n* MultinomialNB\n* Random Forest\n* SGD Classifier\n* XGBoost\n\n**4.Conclusion**","441cca7b":"# Conclusion","c416cd00":"From the results, we can see that the recall is very low for class 2.<br>\nRecall = True Positive\/(True Positive + True Negative).<br>\nWhich implies, **74% tweets** indicating **Covid-19 Positive** but model predicting as **Negative** or **Netrual**."}}