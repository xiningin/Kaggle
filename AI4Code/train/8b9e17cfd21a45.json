{"cell_type":{"5d254815":"code","61c096e1":"code","75b39b53":"code","aa53eeb7":"code","34703489":"code","2cdb4d72":"code","c355ae54":"code","faca9314":"code","4dfda028":"code","a2c2ab5e":"code","0a199d8d":"code","4fa69b98":"code","65657433":"code","231406a9":"code","aa84a697":"code","c174b695":"code","cb802afe":"code","75d26a97":"code","f4205685":"code","0a81ac5e":"code","1caf4a7d":"code","f89f547c":"code","cb93632a":"code","b4e3e0aa":"code","d221505f":"code","5e8862fa":"code","54ce6376":"code","812e9ddb":"code","3e1f818e":"markdown","4bfdd252":"markdown"},"source":{"5d254815":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os, shutil\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import utils\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport matplotlib.image as mpimg\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         #print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","61c096e1":"#Create directory to separate train and validation images\noriginal_dataset_dir = '\/kaggle\/input\/petfinder-pawpularity-score\/'\nbase_dir = '\/kaggle\/petfinder-pawpularity-score-small\/'\nos.mkdir(base_dir)\n","75b39b53":"#Creating train,test and validation directory\ntrain_dir = os.path.join(base_dir, 'train')\nos.mkdir(train_dir)\nvalidation_dir = os.path.join(base_dir, 'validation')\nos.mkdir(validation_dir)\ntest_dir = os.path.join(base_dir, 'test')\nos.mkdir(test_dir)","aa53eeb7":"#Reading score file\ncsv_path = os.path.join(original_dataset_dir , 'train.csv')\ndf_score = pd.read_csv(csv_path)","34703489":"#Reading first ten columns\ndf_score.head(10)","2cdb4d72":"for dirname, _, filenames in os.walk('\/kaggle\/input\/petfinder-pawpularity-score\/'):\n    print(dirname)","c355ae54":"def get_score_by_id(filename):\n    image_id = filename.split('.')[0]\n    if image_id is not None:\n        score = df_score[df_score['Id'] == image_id]['Pawpularity'].values[0]\n        return score","faca9314":"#Creating cat dataset\ntrain_label = np.array([])\nvalidation_label = np.array([])\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/petfinder-pawpularity-score\/train'):\n    for i, fname in enumerate(filenames):\n        if i<9500:\n            src = os.path.join(dirname, fname)\n            dst = os.path.join(train_dir, fname)\n            shutil.copyfile(src, dst)\n            train_label = np.append(train_label, get_score_by_id(fname))\n        elif i>=9500 and i<10000:\n            src = os.path.join(dirname, fname)\n            dst = os.path.join(validation_dir, fname)\n            shutil.copyfile(src, dst)\n            validation_label = np.append(validation_label, get_score_by_id(fname))\n        else:\n            pass","4dfda028":"#Check whether image files aligns to corect label \n#Script also correct any mis-labelled images\ndef create_label_array_from_images(curr_dir):\n    labels = []\n    for dirname,_, filename in os.walk(curr_dir):\n        for i, file in enumerate(filename):\n            labels.append(get_score_by_id(str(file)))\n    print (\"Size of label array: \", len(labels)) \n    return np.array(labels)","a2c2ab5e":"print(\"No of train images in train_dir \", len(os.listdir(train_dir)))\nprint(\"No of train images in validation_dir \", len(os.listdir(validation_dir)))","0a199d8d":"print(\"No of train labels in train_label \",train_label.shape[0])\nprint(\"No of train labels in validation_label \",validation_label.shape[0])\n","4fa69b98":"#test the function\nscore = get_score_by_id('0007de18844b0dbbb5e1f607da0606e0.jpg')\nprint(score)","65657433":"#Instatntied VGG16 model \n\nfrom tensorflow.keras.applications import VGG16\nconv_base = VGG16(weights='imagenet',\ninclude_top=False,\ninput_shape=(150, 150, 3))","231406a9":"import glob\nimport cv2\ntrain_data_np = []\nfiles = glob.glob (train_dir+'\/*')\nfor myFile in files:\n    image = cv2.imread (myFile)\n    resized = cv2.resize(image, (150, 150), interpolation = cv2.INTER_AREA)\n    train_data_np.append(resized\/255)\ntrain_data_np = np.array(train_data_np)\nprint('train_data_np shape:', train_data_np.shape)","aa84a697":"validation_data_np = []\nfiles = glob.glob (validation_dir+'\/*')\nfor myFile in files:\n    image = cv2.imread (myFile)\n    resized = cv2.resize(image, (150, 150), interpolation = cv2.INTER_AREA)\n    validation_data_np.append(resized\/255)\n\nvalidation_data_np = np.array(validation_data_np)\nprint('validation_data_np shape:', validation_data_np.shape)","c174b695":"result_val = conv_base.predict(validation_data_np)","cb802afe":"result_train = conv_base.predict(train_data_np)","75d26a97":"result_val.shape","f4205685":"result_train.shape","0a81ac5e":"import matplotlib.pyplot as plt\n#plt.imshow(result.reshape((4,4), 150))\n#result[0]","1caf4a7d":"train_features = np.reshape(result_train, (9500, 4 * 4 * 512))\nvalidation_features = np.reshape(result_val, (412, 4 * 4 * 512))","f89f547c":"model = keras.Sequential(([\n    layers.Flatten(), \n    layers.Dense(256, activation='relu', input_dim=4 * 4 * 512), \n    layers.Dense(1, activation='linear')]))\n    ","cb93632a":"opt = keras.optimizers.Adam(learning_rate=0.1)\nmodel.compile(optimizer=opt,\n    loss='mean_squared_error',\n    metrics=['mse'])","b4e3e0aa":"history = model.fit(train_features, train_label, epochs=10, batch_size=64, validation_data=(validation_features, validation_label))","d221505f":"import matplotlib.pyplot as plt\nloss = history.history['loss']\nmse = history.history['mse']\nval_loss = history.history['val_loss']\nval_mse = history.history['val_mse']\nepochs = range(1, len(loss) + 1)\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.figure()\nplt.plot(epochs, mse, 'bo', label='Training mse')\nplt.plot(epochs, val_mse, 'b', label='Validation mse')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","5e8862fa":"#Predicting values for the test images\nfiles = glob.glob ('\/home\/kaggle\/input\/petfinder-pawpularity-score\/test\/*')\nscore_df = pd.DataFrame(columns = ['FileName','Score'])\nfor myFile in files:\n    image = cv2.imread (myFile)\n    image_dim_modify = cv2.resize(image, (150, 150), interpolation = cv2.INTER_AREA)\n    resized = image_dim_modify\/255\n    resized_expand_dim = np.expand_dims(resized, axis = 0)\n    interim_arry = conv_base.predict(resized_expand_dim)\n    test_features = np.reshape(interim_arry, (1, 4 * 4 * 512))\n    score = model.predict(test_features)\n    score_row = {'FileName' :myFile.split('\/')[-1], 'Score': score[0][0]}\n    score_df = score_df.append(score_row, ignore_index=True)","54ce6376":"#Save the result\nscore_df.to_csv('result.csv')","812e9ddb":"# #Save hte result\n# a = pd.read_csv('\/kaggle\/input\/submission\/result.csv')\n# a = a.drop(columns = ['Unnamed: 0'])\n# a.to_csv('result.csv')","3e1f818e":"**VGG-16 network for calculating populatiry score**\n\nThe model predicts the popularity score of an image. I have used pre-trained computer vision technique to train my model. I have used Google AI cloud to train my model. \n\nSteps are mentioned below:\n\n1. Created directories and sub-directories for storing training and validation images. \n2. Divided the dataset into training and validation set and stored in respective folder. \n3. Resized the images to speed up the training. \n4. Used convolution based on the VGG-16 and added dense layer and trained with 10 epochs. \n5. Results are calculated.","4bfdd252":"Epoch 1\/10\n149\/149 [==============================] - 2s 12ms\/step - loss: 75421.1406 - mse: 75421.1406 - val_loss: 422.3257 - val_mse: 422.3257 \nEpoch 2\/10\n149\/149 [==============================] - 2s 12ms\/step - loss: 391.7776 - mse: 391.7776 - val_loss: 439.9749 - val_mse: 439.9749\nEpoch 3\/10\n149\/149 [==============================] - 2s 12ms\/step - loss: 377.6066 - mse: 377.6066 - val_loss: 418.5122 - val_mse: 418.5122\nEpoch 4\/10\n149\/149 [==============================] - 2s 13ms\/step - loss: 335.4939 - mse: 335.4939 - val_loss: 435.2131 - val_mse: 435.2131\nEpoch 5\/10\n149\/149 [==============================] - 2s 12ms\/step - loss: 321.1635 - mse: 321.1635 - val_loss: 427.8064 - val_mse: 427.8064\nEpoch 6\/10\n149\/149 [==============================] - 2s 11ms\/step - loss: 320.0218 - mse: 320.0218 - val_loss: 470.0755 - val_mse: 470.0755\nEpoch 7\/10\n149\/149 [==============================] - 2s 11ms\/step - loss: 302.0693 - mse: 302.0693 - val_loss: 434.5990 - val_mse: 434.5990\nEpoch 8\/10\n149\/149 [==============================] - 2s 11ms\/step - loss: 283.7918 - mse: 283.7918 - val_loss: 461.1010 - val_mse: 461.1010\nEpoch 9\/10\n149\/149 [==============================] - 2s 12ms\/step - loss: 272.7795 - mse: 272.7795 - val_loss: 479.4508 - val_mse: 479.4508\nEpoch 10\/10\n149\/149 [==============================] - 2s 11ms\/step - loss: 258.2112 - mse: 258.2112 - val_loss: 525.3227 - val_mse: 525.3227"}}