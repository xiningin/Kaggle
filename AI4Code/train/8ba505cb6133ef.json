{"cell_type":{"d5877278":"code","c63d6d40":"code","8c5a92b5":"code","0189a376":"code","71e9ec7f":"code","619184f1":"code","7400cbf0":"code","a8aeb8b2":"code","eafbd8dc":"code","e2d20d4b":"code","c21d455f":"code","ef158bfc":"code","24aaf75d":"code","e62d6eca":"code","7a7e3aaf":"code","3cb6a5bc":"code","90b06d08":"code","4e5ee343":"code","fa4b84b1":"code","8c009987":"code","e46b2488":"code","efb83bea":"code","d6825765":"code","3e8d5e5d":"code","e9697d46":"code","08a5c228":"code","6acd4e4f":"code","4d36b8ab":"code","faac951d":"code","0c75632c":"code","520b1254":"code","cf800ab9":"code","aaccfcda":"code","4d257782":"code","56b143f2":"code","6b506de4":"code","c4ccec4d":"code","fe6b1894":"code","bd3350ee":"code","bdb6830f":"code","e8f8af37":"markdown","62bb18aa":"markdown","bae37d34":"markdown","0d07d31f":"markdown","2c241a95":"markdown","a1c4a06f":"markdown","25d0ab7a":"markdown","a464dedd":"markdown","0e14c6e6":"markdown","29567033":"markdown","d249363a":"markdown","623a1bb6":"markdown","789a5e95":"markdown","ca401cc0":"markdown","807c4fc5":"markdown"},"source":{"d5877278":"# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pandas_profiling import ProfileReport\nimport warnings\nwarnings.filterwarnings('ignore')","c63d6d40":"train_df = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('..\/input\/titanic\/test.csv')","8c5a92b5":"train_df.head()","0189a376":"test_df.head()","71e9ec7f":"profile_training = ProfileReport(train_df, title=\"Training Data Profiling Report\")\ndisplay(profile_training)","619184f1":"profile_testing = ProfileReport(test_df, title=\"testing Profiling Report\")\ndisplay(profile_testing)","7400cbf0":"profile_training.to_file(output_file='profile_training_report.html')\nprofile_testing.to_file(output_file='profile_testing_report.html')","a8aeb8b2":"train_df.describe(include ='all')","eafbd8dc":"test_df.describe(include = 'all')","e2d20d4b":"train_df.head()","c21d455f":"#How many people survived?\nplt.figure(figsize = (20,2))\nsns.set_theme(style=\"darkgrid\")\nsns.countplot(y=\"Survived\",hue=\"Sex\",data = train_df)\nplt.title(\"Supervived Vs Unsupervived\")\nplt.show()","ef158bfc":"#Droping Feature Name becz it's useless for us\ntrain_df =train_df.drop(['Name'],axis = 1)\ntest_df =test_df.drop(['Name'],axis = 1)","24aaf75d":"#Feature Age have 86 which 20.6% of the data\nplt.figure(figsize= (20,5))\nsns.boxplot(x='Survived',y= 'Age', palette=\"Set3\",data = train_df)","e62d6eca":"#Feature Age have 177 which 19.9% of the data\ntrain_df =train_df.drop(['Age'],axis = 1)\ntest_df =test_df.drop(['Age'],axis = 1)","7a7e3aaf":"train_df =train_df.drop(['Ticket'],axis = 1)\ntest_df =test_df.drop(['Ticket'],axis = 1)","3cb6a5bc":"# Droping Cabin Coulmn becz it's have 327 missing value which 78.2% of the data\ntrain_df =train_df.drop(['Cabin'],axis = 1)\ntest_df =test_df.drop(['Cabin'],axis = 1)","90b06d08":"plt.figure(figsize = (15,2))\nsns.set_theme(style=\"darkgrid\")\nsns.countplot(y='Embarked',data = train_df)\nplt.show()","4e5ee343":"# Since we have just 2 missing values in this feature we can drop it we we can just fill it with  the majority of people embarked  which is S\ntrain_df =train_df.dropna(subset = ['Embarked'])\n#train_df =train_df.fillna( {'Embarked'}: \"S\")","fa4b84b1":"\nplt.figure(figsize = (15,2))\nsns.set_theme(style=\"darkgrid\")\nsns.countplot(y='Sex',data = train_df)\nplt.show()","8c009987":"#Encoding the categorical features\npclass_oh = pd.get_dummies(train_df['Pclass'],prefix = 'pclass')\ntrain_df = pd.get_dummies(train_df)\ntrain_df = pd.concat([train_df,pclass_oh],axis = 1)\ntrain_df =train_df.drop(['Pclass'],axis = 1)\n\n#embarked_oh = pd.get_dummies(train_df['Embarked'],prefix = 'embarked')\n#sex_oh = pd.get_dummies(train_df['Sex'],prefix = 'sex')\n#pclass_oh = pd.get_dummies(train_df['Pclass'],prefix = 'pclass')","e46b2488":"train_df.head()","efb83bea":"pclass_oh = pd.get_dummies(test_df['Pclass'],prefix = 'pclass')\n\ntest_df = pd.get_dummies(test_df)\ntest_df = pd.concat([test_df,pclass_oh],axis = 1)\ntest_df =test_df.drop(['Pclass'],axis = 1)","d6825765":"test_df.head()","3e8d5e5d":"from sklearn.model_selection import train_test_split","e9697d46":"X = train_df.drop(['Survived','PassengerId'],axis = 1) #independed var\ny = train_df['Survived']# Target\nx_train,x_test,y_train,y_test = train_test_split(X,y,test_size = 0.3,random_state = 42)","08a5c228":"print(X.shape)\nprint(y.shape)","6acd4e4f":"train_df.columns","4d36b8ab":"!pip install scikit-learn  -U","faac951d":"from sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n#GradientBoostingClassifier","0c75632c":"#Creating model\nreg1 = LinearRegression()\n#Fitting training data\nreg1 = reg1.fit(x_train,y_train)\n#Y prediction\nreg1.score(x_test, y_test)\n","520b1254":"logReg =  LogisticRegression(solver='liblinear')\n\nlogReg = logReg.fit(x_train,y_train)\n\ny_pred = logReg.predict(x_test)\n\nacc_logReg = round(accuracy_score(y_pred,y_test)*100,2)\nacc_logReg","cf800ab9":"dt = DecisionTreeClassifier()\ndt.fit(x_train,y_train)\ny_pred = dt.predict(x_test)\nacc_dt = round(accuracy_score(y_pred,y_test)*100,2)\nacc_dt","aaccfcda":"#Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier()\nrf.fit(x_train,y_train)\ny_pred = rf.predict(x_test)\nacc_rf = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(\"Random Forest Accuracy\",acc_rf)\n\n\n#KNN | K-Nearest Neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nknn.fit(x_train,y_train)\ny_pred = knn.predict(x_test)\nacc_knn = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(\"K-Nearest Neighbors Accuracy\",acc_knn)\n\n\n# Support Vector Machine\nfrom sklearn.svm import SVC\n\nsvc = SVC()\nsvc.fit(x_train,y_train)\ny_pred = svc.predict(x_test)\nacc_svc = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(\"Support Vector Machine\", acc_svc)\n\n\nclf = ensemble.GradientBoostingClassifier(n_estimators = 2000, max_depth = 5)\nclf.fit(x_train,y_train)\ny_pred = clf.predict(x_test)\nacc_clf = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint('Gradient Boosting Classifier',acc_clf)","4d257782":"test_df.head()","56b143f2":"test_df.isnull().sum()","6b506de4":"test_df = test_df.fillna(0)\ntest_df.isnull().sum()","c4ccec4d":"final = pd.DataFrame()","fe6b1894":"final['PassengerId'] = test_df['PassengerId']\ntest_df = test_df.drop('PassengerId',axis = 1)\ntest_df","bd3350ee":"prediction = clf.predict(test_df)\nfinal['Survived']= prediction\nfinal.to_csv('submission.csv',index=False)","bdb6830f":"final.head()","e8f8af37":"**Look at numeric and categorical values separately:**\n\nNumerical Features: Age, Fare, SibSp, Parch.\n\nCategorical Features: Survived, Sex, Embarked, Pclass.\n\nAlphanumeric Features (but categorical): Ticket, Cabin.\n\nIn our overview report, click on the tab \"Warnings\": \n\n- Tickets and Cabin are features with a high cardinality, and a lot of distinc values. \n- Age and Cabin has a lot of missing values.\n- Name and ID has unique values.\n- SibSp, Parch and Fare has a lot of zeros. ","62bb18aa":"After visualized our entire dataset, **check the describe above or the Sample in our dataframe report**, you see that there are certain data points labeled with a `NaN`. These denote missing values. Different datasets encode missing values in different ways. Sometimes it may be a `9999`, other times a`0` - because real world data can be very messy!\n\n\n**The goal here is to figure out how best to process the data so our machine learning model can learn from it.**","bae37d34":"**Logistic Regression**\n\nLogistic regression measures the relationship between the categorical dependent variable _(feature)_ and one or more independent variables _(features)_ by estimating probabilities using a logistic function, which is the cumulative distribution function of logistic distribution. \n\nReference: [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Logistic_regression)","0d07d31f":"\n ###  Taking care of Missing Values\n","2c241a95":"\n### Taking care of Missing Values\n\n**Important:** Missing data is information that is missing from a database and could be important for the result of an analysis. Working with a dataset with missing values is a problem of great relevance at the time of data analysis and can originate from different sources, such as failures in the collection system, problems with the integration of different sources, etc., the point is: we must be careful to avoid bias in the results we seek.","a1c4a06f":"## Export my Report","25d0ab7a":"### Encoding Categorical Data","a464dedd":"### Building our Machine Learning Models\n\n","0e14c6e6":"#### Import necessary libraries and and make an exploratory data analysis using pandas profilling and seaborn","29567033":"**Variable Notes**\n\n_pclass_: A proxy for socio-economic status (SES)\n- _1st_ = Upper\n- _2nd_ = Middle\n- _3rd_ = Lower\n\n_age_: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\n_sibsp_: The dataset defines family relations in this way...\n- _Sibling_ = brother, sister, stepbrother, stepsister\n- _Spouse_ = husband, wife (mistresses and fianc\u00e9s were ignored)\n\n_parch_: The dataset defines family relations in this way...\n- _Parent_ = mother, father\n- _Child_ = daughter, son, stepdaughter, stepson\nSome children travelled only with a nanny, therefore parch=0 for them.","d249363a":"**train_test_split**: The first argument will be the `feature data`, the second the `target or labels`. The `test_size` keyword argument specifies what proportion of the original data is used for the test set. Lastly, the `random_state` kward sets a seed for the random number generator that splits the data into trains and test.\n\nSplitting the Training Data we will use part of our training data (30% in this case) to test the accuracy of our different models.","623a1bb6":" ### Split the Train & Test datasets\n\n\n","789a5e95":"**Decision Tree**\n\nDecision tree learning is one of the predictive modelling approaches used in statistics, data mining and machine learning. It uses a decision tree (as a predictive model) to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves). Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. Decision trees are among the most popular machine learning algorithms given their intelligibility and simplicity\n\nReference: [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Decision_tree_learning)","ca401cc0":"### Feature Encoding\n\nDocumentation: [pandas.get_dummies](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.get_dummies.html)\n\n","807c4fc5":"### Data Preprocessing"}}