{"cell_type":{"6a3afcea":"code","85d42f30":"code","c9fc3b12":"code","b33abfaf":"code","c7303240":"code","6da7ff5e":"code","8d6fe9f5":"code","d499bbd0":"code","547c48a4":"code","cd5d1f88":"code","f7f3b4d6":"code","fa90b5b8":"code","297b9271":"code","324bab05":"code","d11a358d":"code","e5884bfd":"code","8ef29401":"code","3046dd1b":"code","bfe49892":"code","8a17d6b1":"code","928e40eb":"code","031bdd73":"code","edd84761":"code","3e914c1b":"code","5d9d8c83":"code","83cceff6":"markdown","c8b0f1ba":"markdown","a0eefc7e":"markdown","e3a4939c":"markdown","34e94ea0":"markdown","5289c7a3":"markdown","ac79a12d":"markdown","e44cccc2":"markdown","44f0348b":"markdown","24fe3af0":"markdown","0e52580e":"markdown","37ae6e89":"markdown","68bf05a1":"markdown","348f021d":"markdown"},"source":{"6a3afcea":"# Input data files are available in the \"..\/input\/\" directory.\n# importing libraries and modules\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results to the current directory will be saved as output.\n\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.datasets import fashion_mnist\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.models import load_model","85d42f30":"((X_train, Y_train), (X_test, Y_test)) = fashion_mnist.load_data()","c9fc3b12":"#Create a dictionary of all classes in the target mentioned below.\n\nclass_labels = pd.Series(['T-shirt', 'Trouser', 'Pullover', 'Dress', 'Code', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot'])\nlabels_dict = class_labels.to_dict()\nlabels_dict","b33abfaf":"#Fetching 5 images randomly from the training data, ploting them with their corresponding labels from the training targets.\n\nnp.random.seed(40)\nfor rand_num in np.random.randint(0, len(X_train), 5):\n    plt.figure()\n    plt.imshow(X_train[rand_num]), plt.axis('off')\n    plt.title(labels_dict[Y_train[rand_num]])","c7303240":"X_train_reshaped = X_train.reshape(len(X_train), -1)   \nX_test_reshaped = X_test.reshape(len(X_test), -1)\n\nX_train_norm = X_train_reshaped\/255            \nX_test_norm = X_test_reshaped\/255","6da7ff5e":"n_features = X_train_norm.shape[1]\nn_classes =  len(class_labels)\n\nprint('Number of input features (image pixels) : ', n_features)\nprint('Number of target classes (fashion categories) : ', n_classes)","8d6fe9f5":"Y_train_onehot = to_categorical(Y_train, num_classes = n_classes)\nY_test_onehot = to_categorical(Y_test, num_classes = n_classes)","d499bbd0":"X_train_final, X_valid, Y_train_final, Y_valid = train_test_split(X_train_norm, Y_train_onehot, \n                                                                  test_size=0.16666)\n\nprint('Shape of data used for training, and shape of training targets : \\n ', X_train.shape, ',', Y_train.shape)\nprint('Shape of data used for validation, and shape of validation targets: \\n ', X_valid.shape, ',', Y_valid.shape)","547c48a4":"# Model Set-up\n\nmodel = Sequential()\nmodel.add(Dense(1000, input_dim = n_features, activation='relu', use_bias=False))\nmodel.add(Dense(1000, activation='relu', use_bias=False))\nmodel.add(Dense(500, activation='relu', use_bias=False))\nmodel.add(Dense(200, activation='relu', use_bias=False))\nmodel.add(Dense(n_classes, activation='softmax', use_bias=False))\nmodel.compile(loss='categorical_crossentropy', optimizer=Adam(0.0001), metrics=['accuracy'])\n\nmodel.summary()","cd5d1f88":"# saving the model while training\n\nsave_at = \"\/kaggle\/working\/model.hdf5\"\nsave_best = ModelCheckpoint (save_at, monitor='val_accuracy', verbose=0, \n                             save_best_only=True, save_weights_only=False, mode='max')","f7f3b4d6":"# training\n\nhistory = model.fit( X_train_final, Y_train_final, \n                    epochs = 20, batch_size = 20, \n                    callbacks=[save_best], verbose=1, \n                    validation_data = (X_valid, Y_valid) )\n\nplt.figure(figsize=(6, 5))\nplt.plot(history.history['accuracy'], color='r')\nplt.plot(history.history['val_accuracy'], color='b')\nplt.title('Model Accuracy', weight='bold', fontsize=16)\nplt.ylabel('accuracy', weight='bold', fontsize=14)\nplt.xlabel('epoch', weight='bold', fontsize=14)\nplt.ylim(0.5, 1)\nplt.xticks(weight='bold', fontsize=12)\nplt.yticks(weight='bold', fontsize=12)\nplt.legend(['train', 'val'], loc='upper left', prop={'size': 14})\nplt.grid(color = 'y', linewidth='0.5')\nplt.show()","fa90b5b8":"#testing\n\nscore = model.evaluate(X_test_norm, Y_test_onehot, verbose=0)\nprint('Accuracy over the test set: \\n ', round((score[1]*100), 2), '%')","297b9271":"Y_pred = np.round(model.predict(X_test_norm))\n\nnp.random.seed(50)\nfor rand_num in np.random.randint(0, len(Y_test_onehot), 5):\n    plt.figure()\n    plt.imshow(X_test[rand_num]), plt.axis('off')\n    if np.where(Y_pred[rand_num] == 1)[0].sum() == np.where(Y_test_onehot[rand_num] == 1)[0].sum():\n        plt.title(labels_dict[np.where(Y_pred[rand_num] == 1)[0].sum()], color='g')\n    else :\n        plt.title(labels_dict[np.where(Y_pred[rand_num] == 1)[0].sum()], color='r')","324bab05":"trained_model = load_model(\"\/kaggle\/working\/model.hdf5\")\ntrained_model.layers","d11a358d":"total_no_layers = len(trained_model.layers)\nprint(total_no_layers)","e5884bfd":"#Pruning percentages\nK = [0, 20, 40, 50, 60, 70, 80, 90, 95] ","8ef29401":"all_weights = {}\n\nfor layer_no in range(total_no_layers - 1):         #All except the final layer                                                                          #only the first four dense layers are to be pruned\n    layer_weights = (pd.DataFrame(trained_model.layers[layer_no].get_weights()[0]).stack()).to_dict() \n    layer_weights = { (layer_no, k[0], k[1]): v for k, v in layer_weights.items() }\n    all_weights.update(layer_weights)","3046dd1b":"all_weights_sorted = {k: v for k, v in sorted(all_weights.items(), key=lambda item: abs(item[1]))}","bfe49892":"total_no_weights = len(all_weights_sorted) \ntotal_no_weights ","8a17d6b1":"weight_pruning_scores = []\n\nfor pruning_percent in K:\n\n    new_model = load_model(\"\/kaggle\/working\/model.hdf5\")\n    new_weights = trained_model.get_weights().copy()\n\n    prune_fraction = pruning_percent\/100\n    number_of_weights_to_be_pruned = int(prune_fraction*total_no_weights)\n    weights_to_be_pruned = {k: all_weights_sorted[k] for k in list(all_weights_sorted)[ :  number_of_weights_to_be_pruned]}     \n\n    for k, v in weights_to_be_pruned.items():\n        new_weights[k[0]][k[1], k[2]] = 0\n\n    for layer_no in range(total_no_layers - 1) :\n        new_layer_weights = new_weights[layer_no].reshape(1, new_weights[layer_no].shape[0], new_weights[layer_no].shape[1])\n        new_model.layers[layer_no].set_weights(new_layer_weights)\n    \n    new_score  = new_model.evaluate(X_test_norm, Y_test_onehot, verbose=0)\n    weight_pruning_scores .append(new_score[1])","928e40eb":"all_neurons = {}\n\nfor layer_no in range(total_no_layers - 1):         \n\n    layer_neurons = {}\n    layer_neurons_df = pd.DataFrame(trained_model.layers[layer_no].get_weights()[0])\n\n    for i in range(len(layer_neurons_df.columns)):\n        layer_neurons.update({ i : np.array( layer_neurons_df.iloc[:,i] ) })    \n                                                                 \n    layer_neurons = { (layer_no, k): v for k, v in layer_neurons.items() }\n    all_neurons.update(layer_neurons)","031bdd73":"all_neurons_sorted = {k: v for k, v in sorted(all_neurons.items(), key=lambda item: np.linalg.norm(item[1], ord=2, axis=0))}","edd84761":"total_no_neurons = len(all_neurons_sorted) \ntotal_no_neurons ","3e914c1b":"neuron_pruning_scores = []\n\nfor pruning_percent in K:\n\n    new_model = load_model(\"\/kaggle\/working\/model.hdf5\")\n    new_weights = trained_model.get_weights().copy()\n\n    prune_fraction = pruning_percent\/100\n    number_of_neurons_to_be_pruned = int(prune_fraction*total_no_neurons)\n    neurons_to_be_pruned = {k: all_neurons_sorted[k] for k in list(all_neurons_sorted)[ : number_of_neurons_to_be_pruned]}     \n\n    for k, v in neurons_to_be_pruned.items():\n        new_weights[k[0]][:, k[1]] = 0\n\n    for layer_no in range(total_no_layers - 1) :\n        new_layer_weights = new_weights[layer_no].reshape(1, new_weights[layer_no].shape[0], new_weights[layer_no].shape[1])\n        new_model.layers[layer_no].set_weights(new_layer_weights)\n    \n    new_score  = new_model.evaluate(X_test_norm, Y_test_onehot, verbose=0)\n    neuron_pruning_scores.append(new_score[1])","5d9d8c83":"plt.figure(figsize=(8, 4))\nplt.plot(pd.DataFrame(weight_pruning_scores).set_index(pd.Series(K), drop=True) , color='r')\nplt.plot(pd.DataFrame(neuron_pruning_scores).set_index(pd.Series(K), drop=True) , color='b')\nplt.title('Effect of Pruning on accuracy', weight='bold', fontsize=16)\nplt.ylabel('Score', weight='bold', fontsize=14)\nplt.xlabel('Pruning Percentage (K)', weight='bold', fontsize=14)\nplt.xticks(weight='bold', fontsize=12)\nplt.yticks(weight='bold', fontsize=12)\nplt.legend(['Weight Pruning', 'Neuron Pruning'], loc='best', prop={'size': 14})\nplt.grid(color = 'y', linewidth='0.5')\nplt.show()","83cceff6":"# 1. Importing dependencies","c8b0f1ba":"### 1. Reshaping and normalizing input data\nWe will reshape the image data from 28x28 to 784x1, and normalize all values between 0 and 1.","a0eefc7e":"# Implementation\n###**** I have implementaed the concept on a tensorflow model trained on Fashion-MNIST dataset.","e3a4939c":"### 1. Weight Pruning","34e94ea0":"### 2. Neuron Pruning","5289c7a3":"# 6. Pruning result","ac79a12d":"### 2. One-Hot transformation of Target Data","e44cccc2":"# 2. Fashion MNIST dataset","44f0348b":"# 3. Data Preprocessing","24fe3af0":"### 3. Validation dataset creation","0e52580e":"Now, we will plot 5 images randomly from test set, but with titles as classified by the model, with every correct classification titled in 'green' color, and every incorrect classification titles in 'red' color.","37ae6e89":"# 7. Observations\n1. We are able to prune upto 70% of the weights with an acceptable compromise in accuracy.\n2. We are able to prune upto 20% of the neurons with an acceptable compromise in accuracy.\n3. For weight pruining as high a 80%, the accuracy of the pruned model is still decent.\n\nThese observations suggests that Neural Networks are highly robust. Even if a large protion of the less significant weights are removed, the other significant weights are still able to generate a good enough result.","68bf05a1":"# 5. Model Pruning","348f021d":"# 4. Neural Network"}}