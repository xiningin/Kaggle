{"cell_type":{"3ffafbf1":"code","e56e23d9":"code","c7e3933c":"code","f2291b00":"markdown","1cc40318":"markdown","cdd27aae":"markdown","70b32d84":"markdown"},"source":{"3ffafbf1":"import pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nimport time \nimport numpy as np\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nimport os\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split \nimport datetime\n\nprint('Load starts.', datetime.datetime.now() )\n\ns0 = time.time()\ntrain = pd.read_csv(\"..\/input\/cat-in-the-dat\/train.csv\")#, index_col=\"id\")\ntest = pd.read_csv(\"..\/input\/cat-in-the-dat\/test.csv\")# , index_col=\"id\")\nprint(train.shape, test.shape)\nprint(train.head(2))\nprint(test.head(2))\n\nX = train.drop(['id','target'], axis = 1)\ncategorical_features = [col for c, col in enumerate(X.columns) \\\n                        if not ( np.issubdtype(X.dtypes[c], np.number )  )  ]\ny = train['target']\n\nprint( len(categorical_features), X.shape, y.shape, y.mean()  )\nfor f in categorical_features:\n    X[f] = X[f].astype('category')\n\nX1,X2, y1,y2 = train_test_split(X,y, test_size = 0.2, random_state = 0, stratify = y )\nprint(X1.shape, X2.shape, y1.shape, y2.shape, y1.mean(), y2.mean(), y.mean() )\n\nprint(time.time() - s0)\nprint('Load finished.', datetime.datetime.now() )\n","e56e23d9":"model = lgb.LGBMClassifier(**{\n                'learning_rate': 0.05,\n                'feature_fraction': 0.1,\n                'min_data_in_leaf' : 12,\n                'max_depth': 3,\n                'reg_alpha': 1,\n                'reg_lambda': 1,\n                'objective': 'binary',\n                'metric': 'auc',\n                'n_jobs': -1,\n                'n_estimators' : 5000,\n                'feature_fraction_seed': 42,\n                'bagging_seed': 42,\n                'boosting_type': 'gbdt',\n                'verbose': 1,\n                'is_unbalance': True,\n                'boost_from_average': False})\n\nimport datetime\nprint('Start fit.', datetime.datetime.now() )\n\nmodel = model.fit(X1, y1,\n                  eval_set = [(X1, y1), \n                              (X2, y2)],\n                  verbose = 1,\n                  eval_metric = 'auc',\n                  early_stopping_rounds = 1000)\n\nprint('End fit.', datetime.datetime.now() )","c7e3933c":"X_test = test.drop('id',axis = 1 )\nfor f in categorical_features:\n    X_test[f] = X_test[f].astype('category')\n    \npd.DataFrame({'id': test['id'], 'target': model.predict_proba(X_test)[:,1]}).to_csv('submission.csv', index=False)\n","f2291b00":"# Data Load ","1cc40318":"# LightGBM model","cdd27aae":"# LightGBM\nSimple, short kernel - just pure LightGBM ( no SKF)\n\nPS\n\nParams for lgb taken from kernel: https:\/\/www.kaggle.com\/vincentlugat\/skf-lightgbm-target-encoding \"SKF LightGBM - Target Encoding\"\n\nHere is similar kernel for the second \"cat in dat\" competition:\nhttps:\/\/www.kaggle.com\/alexandervc\/lightgbm\nCurrent outcome is similar - 0.005 roc_auc_ from the current to solution\n","70b32d84":"# PRED CSV"}}