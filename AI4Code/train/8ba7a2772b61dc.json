{"cell_type":{"5431c5bc":"code","7fb84918":"code","3f840d5f":"code","78d1527b":"code","2a35cee5":"code","6726c4f7":"code","2c2d66e3":"code","8c3c7582":"code","48356bc9":"code","7b99a28c":"code","8284a16c":"code","3f75c85b":"code","fc1a4098":"code","32e56d9b":"code","98bade26":"code","578f1f09":"code","fe2f43f0":"code","96cdd508":"code","a84b603f":"code","9c4e656c":"code","91b2bb51":"code","e433f58c":"code","4e7be7cb":"code","3e08d5fd":"code","b1d31635":"code","bee0c04c":"markdown","31999a7c":"markdown","faa9d844":"markdown","31b21f44":"markdown","5e6dfe5f":"markdown","1dc8a07d":"markdown","8c66e07d":"markdown","d6b0b823":"markdown","5ee3d02c":"markdown","44995856":"markdown","59001d96":"markdown","929decf3":"markdown","63e66561":"markdown","5f472e3d":"markdown","e1e4ffdb":"markdown"},"source":{"5431c5bc":"## Check tensorflow version\nimport tensorflow as tf\ntf.__version__","7fb84918":"## Check keras version\nimport keras as k\nk.__version__","3f840d5f":"!pip install --no-deps tensorflow==1.15.3","78d1527b":"!pip install --no-deps keras==2.2.4","2a35cee5":"!git clone https:\/\/github.com\/matterport\/Mask_RCNN.git","6726c4f7":"import os\nos.getcwd()","2c2d66e3":"## set working directory to Mask RCNN\nos.chdir('.\/Mask_RCNN')","8c3c7582":"!wget https:\/\/github.com\/matterport\/Mask_RCNN\/releases\/download\/v2.0\/mask_rcnn_coco.h5","48356bc9":"!wget https:\/\/cdn.mos.cms.futurecdn.net\/uiCrUgVCf64TzEdTM8x9aD-1200-80.jpg","7b99a28c":"!wget https:\/\/ichef.bbci.co.uk\/news\/976\/cpsprodpb\/14C83\/production\/_111832158_gettyimages-1210099902.jpg","8284a16c":"!wget https:\/\/i.pinimg.com\/originals\/e2\/26\/65\/e2266584e63840afb7ceba3c55c76765.jpg","3f75c85b":"!pip install -r requirements.txt","fc1a4098":"!python setup.py install","32e56d9b":"## To confirm if the mask rcnn is installed properly\n!pip show mask-rcnn","98bade26":"# example of inference with a pre-trained coco model\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nfrom mrcnn.config import Config\nfrom mrcnn.model import MaskRCNN\nfrom matplotlib import pyplot\nfrom matplotlib.patches import Rectangle","578f1f09":"# draw an image with detected objects\ndef draw_image_with_boxes(filename, boxes_list):\n     # load the image\n     data = pyplot.imread(filename)\n     # plot the image\n     pyplot.imshow(data)\n     # get the context for drawing boxes\n     ax = pyplot.gca()\n     # plot each box\n     for box in boxes_list:\n          # get coordinates\n          y1, x1, y2, x2 = box\n          # calculate width and height of the box\n          width, height = x2 - x1, y2 - y1\n          # create the shape\n          rect = Rectangle((x1, y1), width, height, fill=False, color='red')\n          # draw the box\n          ax.add_patch(rect)\n     # show the plot\n     pyplot.show()","fe2f43f0":"# define the test configuration\nclass TestConfig(Config):\n     NAME = \"test\"\n     GPU_COUNT = 1\n     IMAGES_PER_GPU = 1\n     NUM_CLASSES = 1 + 80","96cdd508":"# define the model\nrcnn = MaskRCNN(mode='inference', model_dir='.\/', config=TestConfig())\n# load coco model weights\nrcnn.load_weights('mask_rcnn_coco.h5', by_name=True)\n# load photograph\nimg = load_img('uiCrUgVCf64TzEdTM8x9aD-1200-80.jpg')\nimg = img_to_array(img)\n# make prediction\nresults = rcnn.detect([img], verbose=0)\n# visualize the results\ndraw_image_with_boxes('uiCrUgVCf64TzEdTM8x9aD-1200-80.jpg', results[0]['rois'])","a84b603f":"results","9c4e656c":"from mrcnn.visualize import display_instances\nfrom mrcnn.config import Config\nfrom mrcnn.model import MaskRCNN","91b2bb51":"# define 81 classes that the coco model knowns about\nclass_names = ['BG', 'person', 'bicycle', 'car', 'motorcycle', 'airplane',\n               'bus', 'train', 'truck', 'boat', 'traffic light',\n               'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird',\n               'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear',\n               'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie',\n               'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n               'kite', 'baseball bat', 'baseball glove', 'skateboard',\n               'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup',\n               'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n               'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n               'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed',\n               'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n               'keyboard', 'cell phone', 'microwave', 'oven', 'toaster',\n               'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',\n               'teddy bear', 'hair drier', 'toothbrush']","e433f58c":"# define the test configuration\nclass TestConfig(Config):\n     NAME = \"test\"\n     GPU_COUNT = 1\n     IMAGES_PER_GPU = 1\n     NUM_CLASSES = 1 + 80","4e7be7cb":"# define the model\nrcnn = MaskRCNN(mode='inference', model_dir='.\/', config=TestConfig())\n# load coco model weights\nrcnn.load_weights('mask_rcnn_coco.h5', by_name=True)\n# load photograph\nimg = load_img('uiCrUgVCf64TzEdTM8x9aD-1200-80.jpg')\nimg = img_to_array(img)\n# make prediction\nresults = rcnn.detect([img], verbose=0)\n# get dictionary for first prediction\nr = results[0]\n# show photo with bounding boxes, masks, class labels and scores\ndisplay_instances(img, r['rois'], r['masks'], r['class_ids'], class_names, r['scores'])","3e08d5fd":"# define the model\nrcnn = MaskRCNN(mode='inference', model_dir='.\/', config=TestConfig())\n# load coco model weights\nrcnn.load_weights('mask_rcnn_coco.h5', by_name=True)\n# load photograph\nimg = load_img('e2266584e63840afb7ceba3c55c76765.jpg')\nimg = img_to_array(img)\n# make prediction\nresults = rcnn.detect([img], verbose=0)\n# get dictionary for first prediction\nr = results[0]\n# show photo with bounding boxes, masks, class labels and scores\ndisplay_instances(img, r['rois'], r['masks'], r['class_ids'], class_names, r['scores'])","b1d31635":"# define the model\nrcnn = MaskRCNN(mode='inference', model_dir='.\/', config=TestConfig())\n# load coco model weights\nrcnn.load_weights('mask_rcnn_coco.h5', by_name=True)\n# load photograph\nimg = load_img('_111832158_gettyimages-1210099902.jpg')\nimg = img_to_array(img)\n# make prediction\nresults = rcnn.detect([img], verbose=0)\n# get dictionary for first prediction\nr = results[0]\n# show photo with bounding boxes, masks, class labels and scores\ndisplay_instances(img, r['rois'], r['masks'], r['class_ids'], class_names, r['scores'])","bee0c04c":"<h1><span class=\"label label-default\" style=\"background-color:black;border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:#03e8fc; padding:10px\">What is Image Segmentation?<\/span><\/h1>\n\nWe can divide or partition the image into various parts called segments. It\u2019s not a great idea to process the entire image at the same time as there will be regions in the image which do not contain any information. By dividing the image into segments, we can make use of the important segments for processing the image. That, in a nutshell, is how image segmentation works.\n\nAn image is a collection or set of different pixels. We group together the pixels that have similar attributes using image segmentation.\n\nObject detection builds a bounding box corresponding to each class in the image. But it tells us nothing about the shape of the object. We only get the set of bounding box coordinates. We want to get more information \u2013 this is too vague for our purposes.\n\nImage segmentation creates a pixel-wise mask for each object in the image. This technique gives us a far more granular understanding of the object(s) in the image.","31999a7c":"<h1><span class=\"label label-default\" style=\"background-color:black;border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:#03e8fc; padding:10px\">Download Weights in working Director<\/span><\/h1>","faa9d844":"<h1><span class=\"label label-default\" style=\"background-color:black;border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:#03e8fc; padding:10px\">Overview<\/span><\/h1>\n\nThere are multiple ways of dealing with computer vision challenges. The most popular approach I have come across is based on identifying the objects present in an image, aka, object detection. But what if we want to dive deeper? What if just detecting objects isn\u2019t enough \u2013 we want to analyze our image at a much more granular level?\n\nThis notebook will introduce you to the concept of image segmentation. It is a powerful computer vision algorithm that builds upon the idea of object detection and takes us to a whole new level of working with image data. This technique opens up so many possibilities","31b21f44":"<br>\n<h1 style = \"font-size:40px; font-family:Garamond ; font-weight : normal; background-color: black ; color : #CCCC00; text-align: center; border-radius: 100px 100px;padding:10px\">Instance Segmentation using Mask RCNN <\/h1>\n<br>","5e6dfe5f":"<h1><span class=\"label label-default\" style=\"background-color:black;border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:#03e8fc; padding:10px\">Detecting Object with Mask<\/span><\/h1>\n\nThe Mask_RCNN API provides a function called display_instances() that will take the array of pixel values for the loaded image and the aspects of the prediction dictionary, such as the bounding boxes, scores, and class labels, and will plot the photo with all of these annotations.\n\nOne of the arguments is the list of predicted class identifiers available in the \u2018class_ids\u2018 key of the dictionary. The function also needs a mapping of ids to class labels. The pre-trained model was fit with a dataset that had 80 (81 including background) class labels","1dc8a07d":"<h1><span class=\"label label-default\" style=\"background-color:black;border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:#03e8fc; padding:10px\">Instruction<\/span><\/h1>\n\n1) Clone repository:\nhttps:\/\/github.com\/matterport\/Mask_RCNN.git\n\n2) Install below version of tensorflow and keras to avoid errors\n\n* sudo pip install --no-deps tensorflow==1.15.3\n* sudo pip install --no-deps keras==2.2.4\n\n3) Download pretrained maskrcnn weights on MSCOCO dataset from: https:\/\/github.com\/matterport\/Mask_RCNN\/releases\/download\/v2.0\/mask_rcnn_coco.h5\n\n4) Download few sample images and place under working directory \"Mask_RCNN\"","8c66e07d":"### \n<h1><span class=\"label label-default\" style=\"background-color:black;border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:#03e8fc; padding:10px\">Understanding Mask R-CNN<\/span><\/h1>\n\nMask R-CNN is basically an extension of Faster R-CNN. Faster R-CNN is widely used for object detection tasks. For a given image, it returns the class label and bounding box coordinates for each object in the image. So, let\u2019s say you pass the following image:\n\n<img src=\"https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2019\/07\/Screenshot-from-2019-07-18-15-52-17.png\">\n\nFaster RCNN will return the above output.\n\nThe Mask R-CNN framework is built on top of Faster R-CNN. So, for a given image, Mask R-CNN, in addition to the class label and bounding box coordinates for each object, will also return the object mask.\n\nLet\u2019s first quickly understand how Faster R-CNN works. This will help us grasp the intuition behind Mask R-CNN as well.\n\n1) Faster R-CNN first uses a ConvNet to extract feature maps from the images<br>\n2) These feature maps are then passed through a Region Proposal Network (RPN) which returns the candidate bounding boxes<br>\n3) We then apply an RoI pooling layer on these candidate bounding boxes to bring all the candidates to the same size<br>\n4) And finally, the proposals are passed to a fully connected layer to classify and output the bounding boxes for objects<br>\n\nOnce you understand how Faster R-CNN works, understanding Mask R-CNN will be very easy. So, let\u2019s understand it step-by-step starting from the input to predicting the class label, bounding box, and object mask.","d6b0b823":"<h1><span class=\"label label-default\" style=\"background-color:black;border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:#03e8fc; padding:10px\">Making Prediction<\/span><\/h1>","5ee3d02c":"<h1><span class=\"label label-default\" style=\"background-color:black;border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:#03e8fc; padding:10px\">Clone the Mask-RCNN github repository<\/span><\/h1>","44995856":"<h1><span class=\"label label-default\" style=\"background-color:black;border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:#03e8fc; padding:10px\">Inspect the results<\/span><\/h1>","59001d96":"<h1><span class=\"label label-default\" style=\"background-color:black;border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:#03e8fc; padding:10px\">Mask RCNN Architecture<\/span><\/h1>\n\n<img src=\"https:\/\/viso.ai\/wp-content\/uploads\/2021\/03\/mask-r-cnn-framework-for-instance-segmentation-1.jpg\">\n\n<b>Source : https:\/\/viso.ai\/deep-learning\/mask-r-cnn\/ <\/b>\n\n#### Backbone: \n\nSimilar to the ConvNet that we use in Faster R-CNN to extract feature maps from the image, we use the ResNet 101 architecture to extract features from the images in Mask R-CNN. So, the first step is to take an image and extract features using the ResNet 101 architecture. These features act as an input for the next layer.\n\n#### Region Proposal Network (RPN):\n\nNow, we take the feature maps obtained in the previous step and apply a region proposal network (RPM). This basically predicts if an object is present in that region (or not). In this step, we get those regions or feature maps which the model predicts contain some object.\n\n#### Region of Interest (RoI):\n\nThe regions obtained from the RPN might be of different shapes, right? Hence, we apply a pooling layer and convert all the regions to the same shape. Next, these regions are passed through a fully connected network so that the class label and bounding boxes are predicted.\n\nTill this point, the steps are almost similar to how Faster R-CNN works. Now comes the difference between the two frameworks. In addition to this, Mask R-CNN also generates the segmentation mask.\n\nFor that, we first compute the region of interest so that the computation time can be reduced. For all the predicted regions, we compute the Intersection over Union (IoU) with the ground truth boxes. We can computer IoU like this:\n\nIoU = Area of the intersection \/ Area of the union\n\nNow, only if the IoU is greater than or equal to 0.5, we consider that as a region of interest. Otherwise, we neglect that particular region. We do this for all the regions and then select only a set of regions for which the IoU is greater than 0.5.\n\n#### Segmentation Mask:\n\nOnce we have the RoIs based on the IoU values, we can add a mask branch to the existing architecture. This returns the segmentation mask for each region that contains an object. It returns a mask of size 28 X 28 for each region which is then scaled up for inference.","929decf3":"<h1><span class=\"label label-default\" style=\"background-color:black;border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:#03e8fc; padding:10px\">Installing Mask RCNN Library<\/span><\/h1>","63e66561":"<h1><span class=\"label label-default\" style=\"background-color:black;border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:#03e8fc; padding:10px\">Different types of Segmentation<\/span><\/h1>\n\nThere are 2 types of image segmentation:\n\n1) Semantic Segmentation<br>\n2) Instance Segmentation<br>\n\n<img src=\"https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2019\/07\/Screenshot-from-2019-03-28-12-08-09.png\">\n\nAll 5 objects in the left image are people. Hence, semantic segmentation will classify all the people as a single instance. Now, the image on the right also has 5 objects (all of them are people). But here, different objects of the same class have been assigned as different instances. This is an example of instance segmentation.","5f472e3d":"The display_instances() function is flexible, allowing you to only draw the mask or only the bounding boxes.\nLearn more about this in <b>visualize.py source file<b>","e1e4ffdb":"<h1><span class=\"label label-default\" style=\"background-color:black;border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:#03e8fc; padding:10px\">Download sample images in working Directory<\/span><\/h1>\n"}}