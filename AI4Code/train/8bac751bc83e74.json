{"cell_type":{"0c9e0f3a":"code","113e9686":"code","702a7d4b":"code","4768eded":"code","889631ee":"code","89280f32":"code","1f5d8ad5":"code","a13c7ab9":"code","9dd31754":"code","16d3d0fd":"code","ddad5201":"code","6c649881":"code","1234a097":"markdown"},"source":{"0c9e0f3a":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.utils import class_weight\nimport warnings\nfrom keras.layers import Dense, Convolution1D, MaxPool1D, Flatten, Dropout\nfrom keras.layers import Input\nfrom keras.models import Model\nfrom keras.layers.normalization import BatchNormalization\nimport tensorflow.keras as keras\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Conv1D, MaxPooling1D, Softmax, Add, Flatten, Activation, Dropout\nfrom tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint\nfrom tensorflow.keras.optimizers import Adam\nimport keras\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nwarnings.filterwarnings('ignore')","113e9686":"train_df=pd.read_csv('\/kaggle\/input\/heartbeat\/mitbih_train.csv',header=None)\ntest_df=pd.read_csv('\/kaggle\/input\/heartbeat\/mitbih_test.csv',header=None)","702a7d4b":"from sklearn.utils import resample\ndf_1=train_df[train_df[187]==1]\ndf_2=train_df[train_df[187]==2]\ndf_3=train_df[train_df[187]==3]\ndf_4=train_df[train_df[187]==4]\ndf_0=(train_df[train_df[187]==0]).sample(n=20000,random_state=42)\n\ndf_1_upsample=resample(df_1,replace=True,n_samples=20000,random_state=123)\ndf_2_upsample=resample(df_2,replace=True,n_samples=20000,random_state=124)\ndf_3_upsample=resample(df_3,replace=True,n_samples=20000,random_state=125)\ndf_4_upsample=resample(df_4,replace=True,n_samples=20000,random_state=126)\n\ntrain_df=pd.concat([df_0,df_1_upsample,df_2_upsample,df_3_upsample,df_4_upsample])","4768eded":"target_train=train_df[187]\ntarget_test=test_df[187]\ny_train=to_categorical(target_train)\ny_test=to_categorical(target_test)","889631ee":"X_train=train_df.iloc[:,:186].values\nX_test=test_df.iloc[:,:186].values\n#for i in range(len(X_train)):\n#    X_train[i,:186]= add_gaussian_noise(X_train[i,:186])\nX_train = X_train.reshape(len(X_train), X_train.shape[1],1)\nX_test = X_test.reshape(len(X_test), X_test.shape[1],1)","89280f32":"#CNN model\nim_shape=(X_train.shape[1],1)\ninp=Input(shape=(im_shape), name='inputs_cnn')\nC = Conv1D(filters=32, kernel_size=5, strides=1)(inp)\n\nC11 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(C)\nA11 = Activation(\"relu\")(C11)\nC12 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(A11)\nS11 = Add()([C12, C])\nA12 = Activation(\"relu\")(S11)\nM11 = MaxPooling1D(pool_size=5, strides=2)(A12)\n\n\nC21 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(M11)\nA21 = Activation(\"relu\")(C21)\nC22 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(A21)\nS21 = Add()([C22, M11])\nA22 = Activation(\"relu\")(S11)\nM21 = MaxPooling1D(pool_size=5, strides=2)(A22)\n\n\nC31 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(M21)\nA31 = Activation(\"relu\")(C31)\nC32 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(A31)\nS31 = Add()([C32, M21])\nA32 = Activation(\"relu\")(S31)\nM31 = MaxPooling1D(pool_size=5, strides=2)(A32)\n\n\nC41 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(M31)\nA41 = Activation(\"relu\")(C41)\nC42 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(A41)\nS41 = Add()([C42, M31])\nA42 = Activation(\"relu\")(S41)\nM41 = MaxPooling1D(pool_size=5, strides=2)(A42)\n\n\nC51 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(M41)\nA51 = Activation(\"relu\")(C51)\nC52 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(A51)\nS51 = Add()([C52, M41])\nA52 = Activation(\"relu\")(S51)\nM51 = MaxPooling1D(pool_size=5, strides=2)(A52)\n\nF1 = Flatten()(M51)\n\nD1 = Dense(32)(F1)\nA6 = Activation(\"relu\")(D1)\nD2 = Dense(32)(A6)\nD3 = Dense(5)(D2)\nA7 = Softmax()(D3)\n\nmodel = Model(inputs=inp, outputs=A7)\n\nmodel.summary()","1f5d8ad5":"model.compile(optimizer='adam', loss='categorical_crossentropy',metrics = ['accuracy'])\n\ncallbacks = [\n         ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\n\nhistory=model.fit(X_train, y_train,epochs=150,callbacks=callbacks, batch_size=16,validation_data=(X_test,y_test))\nmodel.load_weights('best_model.h5')","a13c7ab9":"def evaluate_model(history,X_test,y_test,model):\n    scores = model.evaluate((X_test),y_test, verbose=0)\n    print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n    \n    print(history)\n    fig1, ax_acc = plt.subplots()\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.title('Model - Accuracy')\n    plt.legend(['Training', 'Validation'], loc='lower right')\n    plt.show()\n    \n    fig2, ax_loss = plt.subplots()\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Model- Loss')\n    plt.legend(['Training', 'Validation'], loc='upper right')\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.show()\n    target_names=['0','1','2','3','4']\n    \n    y_true=[]\n    for element in y_test:\n        y_true.append(np.argmax(element))\n    prediction_proba=model.predict(X_test)\n    prediction=np.argmax(prediction_proba,axis=1)\n    cnf_matrix = confusion_matrix(y_true, prediction)","9dd31754":"evaluate_model(history,X_test,y_test,model)\ny_pred=model.predict(X_test)","16d3d0fd":"import itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\nnp.set_printoptions(precision=2)\n\n# Plot non-normalized confusion matrix\nplt.figure(figsize=(10, 10))\nplot_confusion_matrix(cnf_matrix, classes=['N', 'S', 'V', 'F', 'Q'],normalize=True,\n                      title='Confusion matrix, with normalization')\nplt.show()","ddad5201":"from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nprint(classification_report(y_test.argmax(axis=1), y_pred.argmax(axis=1)))","6c649881":"from keras.models import model_from_json\n\n# serialize model to JSON\nmodel_json = model.to_json()\nwith open(\"model.json\", \"w\") as json_file:\n    json_file.write(model_json)\n# serialize weights to HDF5\nmodel.save_weights(\"model.h5\")\nprint(\"Saved model to disk\")\n \n# later...\n \n# load json and create model\njson_file = open('model.json', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nloaded_model = model_from_json(loaded_model_json)\n# load weights into new model\nloaded_model.load_weights(\"model.h5\")\nprint(\"Loaded model from disk\")","1234a097":"#### Upsampling the data"}}