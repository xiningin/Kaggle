{"cell_type":{"97a8fbc0":"code","ee8436b2":"code","9cebe720":"code","96ae51f0":"code","eb67f696":"code","1a7762b9":"code","06881096":"code","d7e128c6":"code","27969a5e":"code","9c69c013":"code","6c01deb0":"code","452143e2":"code","9b6701a9":"code","2e3cbd8c":"code","d70be779":"code","3ea887ca":"code","83eea289":"code","7c2f2464":"code","e27eb6af":"code","7a68e838":"code","59a27104":"code","05a02d9f":"code","709440dd":"code","4f66cf97":"code","49d537ee":"code","13fe3b99":"code","c831b7bf":"code","9c1bb673":"code","fa1cf933":"code","9bd28129":"code","88c95b0a":"code","98205f37":"code","065903a2":"code","67122759":"code","a23f687b":"code","afb9f7e1":"code","0baa51f8":"code","3f9659a5":"markdown","e101171f":"markdown","9a11fd91":"markdown","e9d5fbc2":"markdown","2214ef7c":"markdown","a4971840":"markdown","a0cbabae":"markdown","2d736089":"markdown","9573444f":"markdown","77a4817e":"markdown","9a4e180e":"markdown","ec2765bb":"markdown","6816644c":"markdown"},"source":{"97a8fbc0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ee8436b2":"path=\"..\/input\/competitive-data-science-predict-future-sales\/\"\nitems=pd.read_csv(path+\"\/items.csv\")\nitem_categories=pd.read_csv(path+\"\/item_categories.csv\")\n                  \nsales=pd.read_csv(path+\"\/sales_train.csv\")\n                  \nshops=pd.read_csv(path+\"\/shops.csv\")\n                  \ntest=pd.read_csv(path+\"test.csv\")\n\nprint(test.head())\n","9cebe720":"from datetime import datetime\nsales['year'] = pd.to_datetime(sales['date']).dt.strftime('%Y')\nsales['month'] = pd.to_datetime(sales['date'], format='%d.%m.%Y').dt.strftime('%m') #another way for same thing\n\nsales.head(2)","96ae51f0":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n#will make your plot outputs appear and be stored within the notebook.\n%matplotlib inline \n\ngrouped=pd.DataFrame(sales.groupby(['year','month']) ['item_cnt_day'].sum().reset_index())\n\n\n\nsns.pointplot(x=\"month\",y=\"item_cnt_day\",hue=\"year\",data=grouped)\n","eb67f696":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n#will make your plot outputs appear and be stored within the notebook.\n%matplotlib inline \n\ngrouped=pd.DataFrame(sales.groupby(['year','month']) ['item_price'].sum().reset_index())\n\n\n\nsns.pointplot(x=\"month\",y=\"item_price\",hue=\"year\",data=grouped)","1a7762b9":"sales=sales.query(\"item_price >0\")","06881096":"print('Data set size before filter valid:', sales.shape)\n\nsales = sales[sales['shop_id'].isin(test['shop_id'].unique())]\nprint(sales.head())\n\nsales =sales[sales['item_id'].isin(test['item_id'].unique())]","d7e128c6":"\n\n# Aggregate to monthly level the sales\n#monthly_sales=sales.groupby([\"date_block_num\",\"shop_id\",\"item_id\"])[\n    #\"date_block_num\",\"date\",\"item_price\",\"item_cnt_day\"].agg({\"date_block_num\":'mean',\"date\":[\"min\",'max'],\"item_price\":\"mean\",\"item_cnt_day\":\"sum\"})\n\n#monthly_sales.head(5)\n\n","27969a5e":"group_dateonly = sales.copy().groupby('date_block_num').sum()\ngroup_dateonly['rolling mean'] = group_dateonly.item_cnt_day.rolling(window=6).mean() #6 month rolling mean\ngroup_dateonly['rolling std'] = group_dateonly.item_cnt_day.rolling(window=6).std() #6 month rolling std\n\nplt.figure(figsize=(15,8))\nplt.plot(group_dateonly['rolling mean'], color = 'blue', label='rolling mean')\nplt.plot(group_dateonly['item_cnt_day'], color = 'green', label='item count per month')\nplt.plot(group_dateonly['rolling std'], color = 'red', label='rolling std')\n\nplt.legend(loc='best')\nplt.grid(True)\nplt.title('general trend for items sold per month')\n","9c69c013":"def subplots_shops(df, rows, columns):\n    f, axes = plt.subplots(rows, columns, figsize=(15, 24), sharex=False)\n    arr = df['shop_id'].unique()\n    i = 0\n    for r in range(rows): \n        for c in range(columns):\n            sns.lineplot(data= df[df['shop_id']==arr[i]], x='date_block_num', y='item_cnt_month', ax=axes[r,c])\n            #axes[r,c].tick_params(axis='y', which='both', labelleft='off')\n            axes[r,c].set_ylabel('')\n            axes[r,c].set_yticklabels([])\n            axes[r,c].set_yticks([])\n            \n            axes[r,c].set_xlabel('Shop ID:'+ str(arr[i]))\n            axes[r,c].set_xticks([])\n            axes[r,c].set_xticklabels([])\n            i = i + 1\n            \n    \n    #plt.title('Revenue over time period')\n    plt.show()\n    f.tight_layout(h_pad=5.0)\n    return \n\nsales_groupbyshops = sales.groupby(['date_block_num', 'shop_id']).sum()\nsales_groupbyshops.rename({'item_cnt_day':'item_cnt_month'},axis=1, inplace=True)\nsales_groupbyshops = sales_groupbyshops.reset_index()\nsubplots_shops(sales_groupbyshops, 7, 6)","6c01deb0":"#groupby shop id and item id (test data format)\nprevious_month = 33 # or oct2015 \nprevious_val_train = sales[sales['date_block_num']==33].groupby(['shop_id', 'item_id'])['item_cnt_day'].sum().copy()\nprevious_val_train = previous_val_train.reset_index().rename({'item_cnt_day':'item_cnt_month'}, axis=1)\nprevious_val_train.shape\nprevious_val_train.head()","452143e2":"#merging these values with test dataframe will add item_cnt_month for submission using previous months items\nprint(test.shape)\nsubmission_1 = pd.merge(previous_val_train, test,how='right', on=['shop_id', 'item_id'])\n\n#for shops or items that werent present in both sets, filled item cnts with NaN so replaced with 0\nsubmission_1.drop(['shop_id','item_id'], axis=1, inplace=True)\nsubmission_1 = submission_1.sort_values(by='ID').fillna(0) \nsubmission_1 = submission_1.reindex(columns=['ID', 'item_cnt_month'])\n\n#graph has outliers due to preprocessing, rarely peoply buy more than 20 items \n#submission_1['item_cnt_month'].value_counts()[20:]\nsubmission_1['item_cnt_month'] = submission_1['item_cnt_month'].clip(0,20)\n\n#submission_1.to_csv('previous_value_bench.csv', index=False)\n#score 1.167 RMSE w\/o using any ML","9b6701a9":"itemcat = pd.merge(items, item_categories, on='item_category_id')\nitemcat = itemcat[['item_id', 'item_category_id']]\ntest = test.merge(itemcat, on='item_id')\nsales = sales.merge(itemcat, on='item_id')","2e3cbd8c":"test['date_block_num'] = 34\ntest['year'] = 2015\ntest['month'] = 11","d70be779":"sales_group = sales.groupby(['date_block_num','shop_id','item_id','item_category_id','year', 'month']).agg({\n    'item_cnt_day':'sum'}).reset_index().rename({'item_cnt_day':'item_cnt_month'}, axis=1)\n#print(sales_group)","3ea887ca":"#merge test and sales data for data preprocessing, feature engineering and modeling \nX_full = pd.concat([sales_group, test], ignore_index=True)\nprint(X_full.columns)\n\n#test --> dateblocknum = 34\n#filling up mean values for item_cnt_month for assistance with feature engineering\n#no regularization (beneficial for lag values)\n#model may overfit \nitem_cnt_mean = sales_group.groupby('item_id')['item_cnt_month'].mean()\nX_full.loc[(X_full['date_block_num']==34), 'item_cnt_month'] = X_full.loc[(X_full['date_block_num']==34), 'item_id'].map(item_cnt_mean)\n\n#fillna with global mean\nglobal_mean = X_full.loc[(X_full['date_block_num']<34), 'item_cnt_month'].mean()\nX_full['item_cnt_month'].fillna(global_mean, inplace=True)\n###item_cnt_month is to be predicted but using previous values, these can be helpful in lag variables ","83eea289":"def lag_features(df, lags, group_cols, shift_col):\n    \"\"\"\n    #Stackoverflow code\n    Arguments:\n        df (pd.DataFrame)\n        lags (list((int)): the number of months to lag by\n        group_cols (list(str)): the list of columns that need to be the merged key\n        shift_col (str): the column name that is to be shifted by\n    \"\"\"\n\n    for lag in lags:\n        new_col = '{0}_lag_{1}'.format(shift_col, lag)\n        df[new_col] = df.groupby(group_cols)[shift_col].shift(lag)\n        df[new_col] = df[new_col].clip(0,20)\n        df[new_col].fillna(0, inplace=True)\n        \n\n    return df","7c2f2464":"lags = [1,2,3,4,6,12]\ngroup_cols = ['shop_id', 'item_id']\n\nX_full = lag_features(X_full,lags,group_cols,'item_cnt_month')","e27eb6af":"#Kfold encoding target [item encode] 0.14 corr\ndef kfold_enc(df):\n    from sklearn.model_selection import KFold\n    kf = KFold(5, shuffle=False, random_state=12)\n    \n    #declare variable\n    df['kfold_targetenc'] = np.nan\n    #prevent data leakage\n    data_1 = df[df['date_block_num']<33]\n    data_2 = df[df['date_block_num']>=33] #test set\n    \n    i = data_1.columns.get_loc('kfold_targetenc')\n    a = data_1.columns.get_loc('item_id')\n    \n    for tr_ind, val_ind in kf.split(data_1):\n        item_target = data_1.iloc[tr_ind].groupby(['item_id'])['item_cnt_month'].mean()\n        data_1.iloc[val_ind,i] = data_1.iloc[val_ind, a].map(item_target)\n        data_2.iloc[:, i] = data_2.iloc[:, i].map(item_target)\n        \n    data_3 = pd.concat([data_1, data_2], ignore_index=True)\n    target_mean = data_1['item_cnt_month'].mean()\n    data_3['kfold_targetenc'].fillna(target_mean, inplace=True)\n    corr = np.corrcoef(data_3['kfold_targetenc'], data_3['item_cnt_month'])[0][1]\n    print('correlation_kfoldenc: ')\n    print(corr)\n    del data_1\n    del data_2\n    return data_3\n    \nX_full = kfold_enc(X_full)","7a68e838":"#Expanding mean scheme [shop\/item pair]\ndef ems_encoding(df):\n    cumsum = df.groupby(['shop_id', 'item_id'])['item_cnt_month'].cumsum()\n    cumcount = df.groupby(['shop_id', 'item_id'])['item_cnt_month'].cumcount()\n    target_mean = df['item_cnt_month'].mean()\n    \n    df['EMS_targetenc'] = (cumsum - df['item_cnt_month'])\/cumcount\n    df['EMS_targetenc'].fillna(target_mean, inplace=True)\n\n    corr = np.corrcoef(df['EMS_targetenc'], df['item_cnt_month'])[0][1]\n    print('expanding mean scheme correlation: ', corr)\n    return df\n\nX_full = ems_encoding(X_full)","59a27104":"def ems_encoding_itemcat(df):\n    cumsum = df.groupby(['item_category_id', 'item_id'])['item_cnt_month'].cumsum()\n    cumcount = df.groupby(['item_category_id', 'item_id'])['item_cnt_month'].cumcount()\n    target_mean = df['item_cnt_month'].mean()\n    \n    df['EMS_item_targetenc'] = (cumsum - df['item_cnt_month'])\/cumcount\n    df['EMS_item_targetenc'].fillna(target_mean, inplace=True)\n\n    corr = np.corrcoef(df['EMS_item_targetenc'], df['item_cnt_month'])[0][1]\n    print('expanding mean scheme correlation: ', corr)\n    return df\n\nX_full = ems_encoding_itemcat(X_full)","05a02d9f":"#Leave_one_out scheme target encoding with shop\/item pair \ndef LOO_shopitem(df):\n    df['LOO_shopitem_target'] = df.groupby(['shop_id', 'item_id'])['item_cnt_month'].transform('sum')\n    n_objects = df.groupby(['shop_id', 'item_id'])['item_cnt_month'].transform('count')\n    df['LOO_shopitem_target'] = (df['LOO_shopitem_target'] - df['item_cnt_month'])\/(n_objects-1)\n    target_mean = df['item_cnt_month'].mean()\n    df['LOO_shopitem_target'].fillna(target_mean, inplace=True)\n    \n    corr = np.corrcoef(df['LOO_shopitem_target'], df['item_cnt_month'])[0][1]\n    print('Leave_one_out scheme correlation: ', corr)\n    return df\n    \nX_full = LOO_shopitem(X_full)","709440dd":"def LOO_item_itemcat(df):\n    df['LOO_itemcat_target'] = df.groupby(['item_category_id', 'item_id'])['item_cnt_month'].transform('sum')\n    n_objects = df.groupby(['item_category_id', 'item_id'])['item_cnt_month'].transform('count')\n    df['LOO_itemcat_target'] = (df['LOO_itemcat_target'] - df['item_cnt_month'])\/(n_objects-1)\n    target_mean = df['item_cnt_month'].mean()\n    df['LOO_itemcat_target'].fillna(target_mean, inplace=True)\n    \n    corr = np.corrcoef(df['LOO_itemcat_target'], df['item_cnt_month'])[0][1]\n    print('Leave_one_out scheme correlation: ', corr)\n    return df\n\nX_full = LOO_item_itemcat(X_full)","4f66cf97":"#Smoothing scheme Target encoding item category \n#formula = (targetmean*n_rows + target_mean*alpha)\/(n_rows + alpha)\n#alpha used for introducing noise\/smoothing\/regularization\n'''\ndef smooth_itemcat(df):\n    alpha = 100 \n    target_mean = df['item_cnt_month'].mean()\n    \n    #mean target\n    itemcat_mean = df.groupby('item_category_id')['item_cnt_month'].transform('mean')\n    #count of appearances\n    n_rows = df.groupby('item_category_id').transform('count')\n    \n    df['smooth_itemcat_target'] = (itemcat_mean*n_rows + target_mean*alpha)\/(n_rows + alpha)\n    df['smooth_itemcat_target'].fillna(target_mean, inplace=True)\n    \n    del itemcat_mean\n    del n_rows\n    \n    corr = np.corrcoef(df['smooth_itemcat_target'], df['item_cnt_month'])[0][1]\n    print('Smoothing scheme correlation: ', corr)\n    return df\n\n_s = smooth_itemcat(X_full)\n_s.isna().sum()\n'''","49d537ee":"plt.figure(figsize=(15,8))\nsns.heatmap(X_full.corr())\n#target encodings + lag features (unsurprisingly) show significant correlation with item_cnt","13fe3b99":"X_full = X_full.astype({'year':'int64', 'month':'int64'})","c831b7bf":"#XGBoost model primary test\n#metric rmse \nimport xgboost as xgb\nfrom xgboost import plot_importance, plot_tree\nfrom sklearn.metrics import mean_squared_error\nx_cols = ['date_block_num', 'shop_id', 'item_id', 'item_category_id', 'year',\n          'month', 'kfold_targetenc', 'EMS_targetenc', 'item_cnt_month_lag_1',\n          'item_cnt_month_lag_2', 'item_cnt_month_lag_3','item_cnt_month_lag_4',\n          'item_cnt_month_lag_6','item_cnt_month_lag_12', 'LOO_shopitem_target', \n          'LOO_itemcat_target', 'EMS_item_targetenc']\n\ny_train = X_full[X_full['date_block_num']<33]['item_cnt_month'].copy().fillna(0).clip(0,20)\ny_valid = X_full[X_full['date_block_num']==33]['item_cnt_month'].copy().fillna(0).clip(0,20)\n\nX_train = X_full[X_full['date_block_num']<33][x_cols].copy()\nX_valid = X_full[X_full['date_block_num']==33][x_cols].copy()\nX_test = X_full[X_full['date_block_num']>33][x_cols].copy()\n\n","9c1bb673":"#basic model fit\n#baseline model rmse=1.87\nmodel = xgb.XGBRegressor(max_depth=8,min_child_weight=2, n_estimators=1000)\nmodel.fit(X_train, y_train,\n          eval_metric=\"rmse\",\n          eval_set=[(X_train, y_train), (X_valid, y_valid)],\n          early_stopping_rounds=20,\n          verbose=True)","fa1cf933":"_ = plot_importance(model, height=0.9)","9bd28129":"#performance evaluation \ny_pred = model.predict(X_valid).clip(0,20)\nrmse_val = mean_squared_error(y_valid,y_pred, squared=False)\n\nprint('rmse_validation: ', rmse_val)","88c95b0a":"#test['item_cnt_month'] = model.predict(X_full[X_full['date_block_num']==34][x_cols]).clip(0,20)\n#baseline_sub = test[['ID', 'item_cnt_month']]\n#baseline_sub.to_csv('baseline_submission.csv', index=False)\n#rsme 1.9","98205f37":"#min(X_full[X_full['date_block_num']==33].index)\n#max(X_full[X_full['date_block_num']==33].index)","065903a2":"#model tuning with xgboost\n#y_train y_valid X_train X_valid\n\ndef parameter_tuning(xtrain, ytrain, xvalid, yvalid):\n    import xgboost as xgb\n    from sklearn.metrics import mean_squared_error\n    \n    model_t =  xgb\n    \n    #Manual trying different combinations\n    param_grid_xgb = {\n        'min_child_weight': 2,\n        'gamma': 0.5,\n        'colsample_bytree': 0.8,\n        'max_depth': 7,\n        'num_round': 30,\n        'eval_metric':'rmse',\n        'n_estimators': 1000\n        }\n    model = xgb.XGBRegressor(param_grid=param_grid_xgb)\n    model.fit(xtrain, ytrain,\n              eval_metric=\"rmse\",\n              eval_set=[(xtrain, ytrain), (xvalid, yvalid)],\n              early_stopping_rounds=20,\n              verbose=True)\n    \nparameter_tuning(X_train, y_train, X_valid, y_valid)\n#rmse of approx 1.599 on validation set","67122759":"#Linear regression + xgb for simple mix\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import r2_score\n\nlr = LinearRegression()\nlr.fit(X_train, y_train)\npred_lr = lr.predict(X_valid)\n\nprint('Test R-squared for linreg is %f' % r2_score(y_valid, pred_lr))\n\n'''\nr_lr = Ridge(alpha=2)\nr_lr.fit(X_train, y_train)\npred_r = r_lr.predict(X_valid)\n\nprint('Test R-squared for Ridgereg is %f' % r2_score(y_valid, pred_r))\n#gives same score as linear regression\n'''\n\n\nimport xgboost as xgb\nparam_grid_xgb = {\n        'min_child_weight': 2,\n        'gamma': 0.5,\n        'colsample_bytree': 0.8,\n        'max_depth': 7,\n        'num_round': 30,\n        'eval_metric':'rmse',\n        'n_estimators': 1000\n        }\n#model = xgb.XGBRegressor(param_grid=param_grid_xgb)\nxgb_model = xgb.XGBRegressor(param_grid=param_grid_xgb)\nxgb_model.fit(X_train, y_train,\n          eval_metric=\"rmse\",\n          eval_set=[(X_train, y_train), (X_valid, y_valid)],\n          early_stopping_rounds=20,\n          verbose=False)\npred_xgb = xgb_model.predict(X_valid)\nprint('Test R-squared for Xgbreg is %f' % r2_score(y_valid, pred_xgb))\n\nX_valid_level2 = np.concatenate([pred_lr, pred_xgb])","a23f687b":"#X_train_level2.shape\nX_float = X_full.select_dtypes('float64').columns\nX_int = X_full.select_dtypes('int64').columns\n\nX_full[X_float] = X_full[X_float].astype('float32')\nX_full[X_int] = X_full[X_int].astype('int32')\nto_drop_cols = ['ID', 'item_cnt_month']\ndates = X_full['date_block_num']\nlast_block = 33 #validation month\ndates_train = dates[dates <  last_block]\ndates_train_level2 = dates_train[dates_train.isin([27, 28, 29, 30, 31, 32])]\n#meta features \n\ny_train_level2 = y_train[dates_train.isin([27, 28, 29, 30, 31, 32])]\nX_train_level2 = np.zeros([y_train_level2.shape[0], 2])","afb9f7e1":"#train and inserts meta features in \n#level 2 X\n\ndates_train_level2 = dates_train_level2.reset_index(drop=True)\n#print(dates_train_level2)\nfor cur_block_num in [27, 28, 29, 30, 31, 32]:\n    '''\n        1. Split `X_train` into parts\n           Remember, that corresponding dates are stored in `dates_train` \n        2. Fit linear regression \n        3. Fit XGB and put predictions          \n        4. Store predictions from 2. and 3. in the right place of `X_train_level2`. \n           You can use `dates_train_level2` for it\n           Make sure the order of the meta-features is the same as in `X_test_level2`\n    ''' \n    train_block = X_full.loc[dates < cur_block_num].drop(to_drop_cols, axis=1)\n    test_block = X_full.loc[dates == cur_block_num].drop(to_drop_cols, axis=1)\n    \n    y_train_block = X_full.loc[dates <  cur_block_num, 'item_cnt_month'].values\n    y_test_block = X_full.loc[dates == cur_block_num, 'item_cnt_month'].values\n    \n    #print(train_block.info())\n    #print(cur_block_num)\n    \n    #metafeature1\n    model_lr = LinearRegression()\n    model_lr.fit(train_block, y_train_block)\n    X_train_level2[dates_train_level2 == cur_block_num, 0] = model_lr.predict(test_block) \n    \n    #metafeature 2\n    model_xgb = xgb.XGBRegressor(param_grid = \n        {\n        'min_child_weight': 2,\n        'gamma': 0.5,\n        'colsample_bytree': 0.8,\n        'max_depth': 7,\n        'num_round': 30,\n        'eval_metric':'rmse',\n        'n_estimators': 1000\n        })\n    \n    model_xgb.fit(train_block, y_train_block,\n                  eval_metric=\"rmse\",\n                  verbose=False)\n    X_train_level2[dates_train_level2 == cur_block_num, 1] = model_xgb.predict(test_block) ","0baa51f8":"plt.scatter(X_train_level2[:, 0], X_train_level2[:, 1])\nplt.title('Xgb vs Linear reg Metafeature')\nplt.xticks([])\nplt.yticks([])\n\n#linear reg w\/ xgboost\n#significant correlation, some outliers","3f9659a5":"* Tune xgboost\n* Ensemble with linear + randomforest + gradient boosting (simple regularization) \n* Stacking ","e101171f":"* -individual shops exhibiting general trend upwards \n* -individual shops exhibiting seasonality in month of december \n* -many shops exhibit the same trend + shape as exhibited in the general trend!","9a11fd91":"## SALES PREDICTION \n### EDA + xgboost model","e9d5fbc2":"- as seen in years 2013 and 2014, upward trend for december (month 12 and 24)\n- General trend Upwards (rolling mean + higher progressive peaks)\n- Item count per month exhibits seasonality as well (peaks at month 12 and 24 + constant rolling std)","2214ef7c":"Trend, seasonality and patterns\n* -General trend\n* -shop wise trend","a4971840":"# Previous value benchmark (score = 1.167)","a0cbabae":"# Hyper parameter turning + Ensembling","2d736089":"Ensembling \n\n* Simple linear mix with xgboost and linear regression \n* Stacked Ensembling with Extratrees, Catboost, Lightgbm, Xgboost, Randomforrest ","9573444f":"Using a simple linear mix to ensemble:\n\nmix = \u03b1\u22c5linreg_prediction+(1\u2212\u03b1)\u22c5xgb_prediction","77a4817e":"# Baseline model with XGBoost ","9a4e180e":"* target (== item_cnt_month) encoding\n* Expanding mean scheme shop\/item pairs\n* KFold and LOO scheme shop\/item pairs\n* item_category_id mean encoding\n* setup train, validation, test set","ec2765bb":"# Basic feature engineering (datetime values + lag values)\n\n* merge test and sales data (test: date_block_num==34, year==2015, month=November(10))\n* merge item categories\n* datetime: year, month, week\n* lag values\n* mean encodings: 'itemcnt mean'(target) for shop id, item_id, item_category etc","6816644c":"Time Series Analysis for Sales prediction\n* Baseline model made with xgboost\n* feature engineering primarily done with mean encoding\n* Ensembling + better feature engineering left "}}