{"cell_type":{"4920ee59":"code","e3c585e8":"code","d6626cc4":"code","3944c1fb":"code","55a06b3a":"code","d5071ec9":"code","9523f401":"code","86c82271":"code","f5729750":"code","ee9955e2":"code","78685a55":"code","d14644dc":"code","f570c611":"code","7df1d363":"code","7936c940":"code","9d4d2998":"code","502ec841":"code","65bb8185":"code","45762a27":"code","50cff1a4":"code","67723abf":"markdown","903ecea5":"markdown","ce929228":"markdown","41fe2d2f":"markdown","fc47f325":"markdown","31ebcc0a":"markdown","ab08aead":"markdown","2232688a":"markdown","5817e472":"markdown","85b1416f":"markdown","8bf29b46":"markdown","2722be2a":"markdown","f7f2d83b":"markdown","0c25008d":"markdown","ccf20ca4":"markdown"},"source":{"4920ee59":"# import libraries \nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.simplefilter(action='ignore')\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC, LinearSVC","e3c585e8":"#import data set\ndata_train  = pd.read_csv('C:\/Users\/sreej\/Documents\/Kaggle\/Titanic Competition\/train.csv')\ndata_test  = pd.read_csv('C:\/Users\/sreej\/Documents\/Kaggle\/Titanic Competition\/test.csv')\n\n","d6626cc4":"#find missing values \ndata_train.isnull().sum(axis = 0)\n","3944c1fb":"#remove missing values\ndata_train.drop(['PassengerId'], axis=1, inplace=True) \ndata_train.drop(['Cabin'], axis=1, inplace=True) ","55a06b3a":"data_train.drop(['Name'], axis=1, inplace=True) ","d5071ec9":"data_train.drop(['Embarked'], axis=1, inplace=True) \ndata_train.drop(['Ticket'], axis=1, inplace=True) \ndata_train.drop(['Fare'], axis=1, inplace=True)","9523f401":"data_train.info()","86c82271":"data_train.Age.fillna(data_train.Age.mean(), inplace=True)","f5729750":"data_train.isnull().sum(axis=0)","ee9955e2":"data_test.drop(['Cabin'], axis=1, inplace=True)\ndata_test.drop(['Name'], axis=1, inplace=True)\ndata_test.drop(['Embarked'], axis=1, inplace=True) \ndata_test.drop(['Fare'], axis=1, inplace=True) \ndata_test.drop(['Ticket'], axis=1, inplace=True) ","78685a55":"data_test.Age.fillna(data_test.Age.mean(), inplace=True)","d14644dc":"data_test.isnull().sum(axis=0)","f570c611":"#create dummy variables for sex \ndata_train =pd.get_dummies(data_train, columns=[\"Sex\"])\ndata_test =pd.get_dummies(data_test, columns=[\"Sex\"])\n","7df1d363":"#create X and Y variables\n\ny_train = data_train['Survived']\nx_train = data_train.drop(['Survived'], axis=1)\nx_test = data_test.drop(['PassengerId'], axis=1).copy()\n","7936c940":"#build the model\n\nlr = LogisticRegression()\n","9d4d2998":"lr.fit(x_train, y_train)","502ec841":"#predict values for new model\ny_predict = lr.predict(x_test)","65bb8185":"lr.score(x_train,y_train)","45762a27":"random_forest = RandomForestClassifier(n_estimators=100)\n\nrandom_forest.fit(x_train, y_train)\n\ny_pred = random_forest.predict(x_test)\n\nrandom_forest.score(x_train, y_train)\n","50cff1a4":"submission = pd.DataFrame({\n        \"PassengerId\": data_test[\"PassengerId\"],\n        \"Survived\": y_pred\n    })\nsubmission.to_csv('titanic.csv', index=False)\nsubmission.head()","67723abf":"# Processing Dataset","903ecea5":"We have the dataset with required values, now let us build our model using the train and test data set's.","ce929228":"The column Sex is having values as male and female, we are now creating dummy variables for the field.","41fe2d2f":"We have to fill the missing values of 'Age' by mean value of all the existing age values from the dataset.","fc47f325":"Now we have to check consistency of the dataset provided before creating the model.","31ebcc0a":"As the missing values of column Embarked & Cabin are not important factor for our model we can remove those from data frame.","ab08aead":"# Titanic : ML from Disaster","2232688a":"The sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\nUse machine learning to create a model that predicts which passengers survived the Titanic shipwreck.","5817e472":"# Model Building - Logistic Regression","85b1416f":"# Submission of results","8bf29b46":"We have created our model using logistic regression algorithm and trained it using our training data.\nNow let's plugin our test data to the model and predict chances of survival among the passengers.","2722be2a":"# Introduction","f7f2d83b":"We have to remove the same fields from test dataset and perform neccessary adjustments to the data.","0c25008d":"# Model building - Random forest classifier","ccf20ca4":"We are training our model using Random forest algorithm and predicting chances of survival among the passengers."}}