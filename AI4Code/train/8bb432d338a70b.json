{"cell_type":{"66c97613":"code","aa7b281a":"code","cd06f7c3":"code","f01b671b":"code","ba868def":"code","e6b5edc2":"code","b32c1809":"code","ef4ddbf3":"code","77e5e889":"code","6570c682":"code","5fca35c6":"code","05354e87":"code","afd41063":"code","7e9bbb9a":"code","dcfa9a28":"code","182333f7":"code","60dd83c2":"code","e0934269":"code","b9ddea77":"code","6b1581a5":"code","78925a39":"code","bda93b0d":"code","f9f6b499":"code","a52723e4":"code","bc36a77d":"code","1b0c43db":"code","32eb1061":"code","675e885e":"code","2bf5c694":"code","73b0a82c":"code","8be8e512":"code","539bd27c":"code","47156a00":"code","81936c4f":"markdown","a692b878":"markdown","1648d895":"markdown","544b1992":"markdown","94f28608":"markdown","ddaf8a8b":"markdown","03f7b2d9":"markdown"},"source":{"66c97613":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","aa7b281a":"data_train = pd.read_csv('..\/input\/train.csv')\ndata_test = pd.read_csv('..\/input\/test.csv')\ndata_gender_submission = pd.read_csv('..\/input\/gender_submission.csv')","cd06f7c3":"categorizador = {'female':0, 'male':1,'C':0, 'Q':1, 'S':2}\ndata_train = data_train.fillna(data_train.mean()).fillna('None').replace(categorizador)\ndata_test = data_test.fillna(data_train.mean()).fillna('None').replace(categorizador)\ndata_gender_submission = data_gender_submission.fillna(data_train.mean()).fillna('None')","f01b671b":"data_train.corr()","ba868def":"values = ['Pclass','Sex','Fare', 'Age']\nclf = LinearDiscriminantAnalysis()\nclf.fit(data_train[values],data_train.Survived)\nprediccion_discriminat_analis=pd.DataFrame(data={'Survived':clf.predict(data_test[values]), 'PassengerId':data_test['PassengerId'].tolist()})","e6b5edc2":"prediccion_discriminat_analis.to_csv('submission_lineal.csv',header=True, index=False)\n(prediccion_discriminat_analis.Survived == data_gender_submission.Survived).sum() - 418\n","b32c1809":"from pandas import Series, DataFrame\nimport pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pylab as plt\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report\nimport sklearn.metrics","ef4ddbf3":"classifier=DecisionTreeClassifier()\nclassifier = classifier.fit(data_train[values],data_train.Survived)","77e5e889":"prediccion_arbol_decision=pd.DataFrame(data={'Survived':classifier.predict(data_test[values]), 'PassengerId':data_test['PassengerId'].tolist()})\nprediccion_arbol_decision.to_csv('submission_arbol.csv',header=True, index=False)\n(prediccion_arbol_decision.Survived == data_gender_submission.Survived).sum() - 418","6570c682":"from pandas import Series, DataFrame\nimport pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pylab as plt\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report\nimport sklearn.metrics\nfrom sklearn import datasets\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import RandomForestClassifier","5fca35c6":"classifier=RandomForestClassifier(n_estimators=100)\nclassifier = classifier.fit(data_train[values],data_train.Survived)\npredictions = classifier.predict(data_test[values])\nsklearn.metrics.confusion_matrix(data_gender_submission.Survived,predictions)\nsklearn.metrics.accuracy_score(data_gender_submission.Survived, predictions)\nprediccion_random_forest=pd.DataFrame(data={'Survived':predictions, 'PassengerId':data_test['PassengerId'].tolist()})\nprediccion_random_forest.to_csv('submission_random_forest.csv',header=True, index=False)","05354e87":"model = ExtraTreesClassifier()\nmodel.fit(data_test[values],data_gender_submission.Survived)\nprint(model.feature_importances_)","afd41063":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","7e9bbb9a":"train_data = data_train\ntest_data = data_test","dcfa9a28":"test_passenger_id=test_data[\"PassengerId\"]","182333f7":"def drop_not_concerned(data, columns):\n    return data.drop(columns, axis=1)\n\nnot_concerned_columns = [\"PassengerId\",\"Name\", \"Fare\", \"Ticket\", \"Cabin\", \"Embarked\"]\ntrain_data = drop_not_concerned(train_data, not_concerned_columns)\ntest_data = drop_not_concerned(test_data, not_concerned_columns)","60dd83c2":"def dummy_data(data, columns):\n    for column in columns:\n        data = pd.concat([data, pd.get_dummies(data[column], prefix=column)], axis=1)\n        data = data.drop(column, axis=1)\n    return data\n\n\ndummy_columns = [\"Pclass\"]\ntrain_data=dummy_data(train_data, dummy_columns)\ntest_data=dummy_data(test_data, dummy_columns)","e0934269":"from sklearn.preprocessing import MinMaxScaler\ndef normalize_column(data, column):\n    scaler = MinMaxScaler()\n    data[column] = scaler.fit_transform(data[column].values.reshape(-1,1))\n    return data","b9ddea77":"train_data = normalize_column(train_data,'Age')\n#train_data = normalize_column(train_data,'Fare')\ntest_data = normalize_column(test_data,'Age')\n#test_data = normalize_column(test_data,'Fare')\ntrain_data.head()","6b1581a5":"from sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split\n\ndef split_valid_test_data(data, fraction=(1 - 0.95)):\n    data_y = data[\"Survived\"]\n    lb = LabelBinarizer()\n    data_y = lb.fit_transform(data_y)\n\n    data_x = data.drop([\"Survived\"], axis=1)\n\n    train_x, valid_x, train_y, valid_y = train_test_split(data_x, data_y, test_size=fraction)\n\n    return train_x.values, train_y, valid_x, valid_y\n\ntrain_x, train_y, valid_x, valid_y = split_valid_test_data(train_data)\nprint(\"train_x:{}\".format(train_x.shape))\nprint(\"train_y:{}\".format(train_y.shape))\nprint(\"train_y content:{}\".format(train_y[:3]))\n\nprint(\"valid_x:{}\".format(valid_x.shape))\nprint(\"valid_y:{}\".format(valid_y.shape))","78925a39":"# Build Neural Network\nfrom collections import namedtuple\n\ndef build_neural_network(hidden_units=10):\n    tf.reset_default_graph()\n    inputs = tf.placeholder(tf.float32, shape=[None, train_x.shape[1]])\n    labels = tf.placeholder(tf.float32, shape=[None, 1])\n    learning_rate = tf.placeholder(tf.float32)\n    is_training=tf.Variable(True,dtype=tf.bool)\n    \n    initializer = tf.contrib.layers.xavier_initializer()\n    fc = tf.layers.dense(inputs, hidden_units, activation=None,kernel_initializer=initializer)\n    fc=tf.layers.batch_normalization(fc, training=is_training)\n    fc=tf.nn.relu(fc)\n    \n    logits = tf.layers.dense(fc, 1, activation=None)\n    cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=logits)\n    cost = tf.reduce_mean(cross_entropy)\n    \n    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n\n    predicted = tf.nn.sigmoid(logits)\n    correct_pred = tf.equal(tf.round(predicted), labels)\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n\n    # Export the nodes \n    export_nodes = ['inputs', 'labels', 'learning_rate','is_training', 'logits',\n                    'cost', 'optimizer', 'predicted', 'accuracy']\n    Graph = namedtuple('Graph', export_nodes)\n    local_dict = locals()\n    graph = Graph(*[local_dict[each] for each in export_nodes])\n\n    return graph\n\nmodel = build_neural_network()","bda93b0d":"def get_batch(data_x,data_y,batch_size=32):\n    batch_n=len(data_x)\/\/batch_size\n    for i in range(batch_n):\n        batch_x=data_x[i*batch_size:(i+1)*batch_size]\n        batch_y=data_y[i*batch_size:(i+1)*batch_size]\n        \n        yield batch_x,batch_y","f9f6b499":"epochs = 2000\ntrain_collect = 50\ntrain_print=train_collect*2\n\nlearning_rate_value = 0.00001\nbatch_size=16\n\nx_collect = []\ntrain_loss_collect = []\ntrain_acc_collect = []\nvalid_loss_collect = []\nvalid_acc_collect = []\n\nsaver = tf.train.Saver()\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    iteration=0\n    for e in range(epochs):\n        for batch_x,batch_y in get_batch(train_x,train_y,batch_size):\n            iteration+=1\n            feed = {model.inputs: train_x,\n                    model.labels: train_y,\n                    model.learning_rate: learning_rate_value,\n                    model.is_training:True\n                   }\n\n            train_loss, _, train_acc = sess.run([model.cost, model.optimizer, model.accuracy], feed_dict=feed)\n            \n            if iteration % train_collect == 0:\n                x_collect.append(e)\n                train_loss_collect.append(train_loss)\n                train_acc_collect.append(train_acc)\n\n                if iteration % train_print==0:\n                     print(\"Epoch: {}\/{}\".format(e + 1, epochs),\n                      \"Train Loss: {:.4f}\".format(train_loss),\n                      \"Train Acc: {:.4f}\".format(train_acc))\n                        \n                feed = {model.inputs: valid_x,\n                        model.labels: valid_y,\n                        model.is_training:False\n                       }\n                val_loss, val_acc = sess.run([model.cost, model.accuracy], feed_dict=feed)\n                valid_loss_collect.append(val_loss)\n                valid_acc_collect.append(val_acc)\n                \n                if iteration % train_print==0:\n                    print(\"Epoch: {}\/{}\".format(e + 1, epochs),\n                      \"Validation Loss: {:.4f}\".format(val_loss),\n                      \"Validation Acc: {:.4f}\".format(val_acc))\n                \n\n    saver.save(sess, \".\/titanic.ckpt\")","a52723e4":"plt.plot(x_collect, train_loss_collect, \"r--\")\nplt.plot(x_collect, valid_loss_collect, \"g^\")\nplt.show()","bc36a77d":"plt.plot(x_collect, train_acc_collect, \"r--\")\nplt.plot(x_collect, valid_acc_collect, \"g^\")\nplt.show()","1b0c43db":"model=build_neural_network()\nrestorer=tf.train.Saver()\nwith tf.Session() as sess:\n    restorer.restore(sess,\".\/titanic.ckpt\")\n    feed={\n        model.inputs:test_data,\n        model.is_training:False\n    }\n    test_predict=sess.run(model.predicted,feed_dict=feed)\n    \ntest_predict[:10]","32eb1061":"from sklearn.preprocessing import Binarizer\nbinarizer=Binarizer(0.5)\ntest_predict_result=binarizer.fit_transform(test_predict)\ntest_predict_result=test_predict_result.astype(np.int32)\ntest_predict_result[:10]","675e885e":"passenger_id=test_passenger_id.copy()\nevaluation=passenger_id.to_frame()\nevaluation[\"Survived\"]=test_predict_result\nevaluation[:10]","2bf5c694":"evaluation.to_csv(\"evaluation_submission.csv\",index=False)","73b0a82c":"data_1 = prediccion_discriminat_analis\ndata_2 = prediccion_arbol_decision\ndata_3 = prediccion_random_forest\ndata_4 = evaluation","8be8e512":"data = (data_1+data_2+data_3+data_4)\/4\ndata.head()","539bd27c":"\ndata['Survived'] = np.where(data['Survived']>=0.5, 1, 0)\ndata['PassengerId'] = data['PassengerId'].astype('int')","47156a00":"data.to_csv('submission_tonto.csv',header=True, index=False)","81936c4f":"Analisis l\u00edneal discriminante","a692b878":"Preparaci\u00f3n de los datos no completos (por la media)","1648d895":"Arbol de decisi\u00f3n","544b1992":"TensorFlow","94f28608":"Carga de ficheros de datos","ddaf8a8b":"Carga de librer\u00edas","03f7b2d9":"Random Forest"}}