{"cell_type":{"6ae49a3c":"code","e5542570":"code","0eafa78a":"code","7e494b34":"code","9fccc50a":"code","845e1420":"code","469cd000":"code","bd62b9f8":"code","72611f34":"code","5554ad6f":"code","c0c63ad1":"code","0444ee11":"code","98de7dbf":"code","8ad5b8e0":"code","d4a30efa":"code","62ff56f5":"code","aefcf434":"code","048b8f2f":"code","20f53fb1":"code","0a00af04":"code","b0069f08":"code","9e8c9d53":"code","02ab9427":"code","3f4f87c3":"code","bd774c86":"code","d24935d5":"code","8a3984db":"code","52a70127":"code","3a0fdea6":"code","5487d8d2":"code","fc97f462":"code","1e30f6ab":"code","0a96042b":"code","7fd52dc1":"code","5d0fb248":"code","33257110":"code","a7b50535":"code","86edc7e5":"code","a93da858":"code","63c790d0":"code","c3efd5dc":"code","d3355130":"markdown","c333713b":"markdown","1eab6751":"markdown","8db8162f":"markdown","07df1bae":"markdown","caae3ef9":"markdown","4331180a":"markdown","f1a48465":"markdown","0bfed73c":"markdown","9d86115f":"markdown","06b0847a":"markdown","6c9794cf":"markdown","e1b11d45":"markdown","e99b4cfa":"markdown","c5cf490f":"markdown","70ea131f":"markdown","60133c81":"markdown","3320f35a":"markdown","2ec0634c":"markdown","996bfbf8":"markdown","faacdfd8":"markdown","1118b5ae":"markdown","1769d3c6":"markdown","60cdfa6a":"markdown","dd1d6e2d":"markdown","86c6d492":"markdown","86293263":"markdown","4af7ed90":"markdown"},"source":{"6ae49a3c":"import nltk\n#nltk.download('stopwords')","e5542570":"import pandas as pd\nimport numpy as np\nimport os\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport seaborn as sns\n\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nstop = set(stopwords.words('english'))\n\nfrom wordcloud import WordCloud\n\nfrom collections import defaultdict\nfrom collections import Counter\nplt.style.use('ggplot')\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport string","0eafa78a":"os.listdir('..\/input\/tweet-sentiment-extraction')","7e494b34":"train = pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv')\ntest = pd.read_csv('..\/input\/tweet-sentiment-extraction\/test.csv')\nsubmission = pd.read_csv(\"..\/input\/tweet-sentiment-extraction\/sample_submission.csv\")","9fccc50a":"print('There are {} rows and {} columns in train'.format(train.shape[0],train.shape[1]))\nprint('There are {} rows and {} columns in test'.format(test.shape[0],test.shape[1]))","845e1420":"train.head(10)","469cd000":"train.info()","bd62b9f8":"train[train.text.isnull()]","72611f34":"train = train[train['text'].notna()]","5554ad6f":"train.sentiment.unique()","c0c63ad1":"positive_len = train[train['sentiment']=='positive'].shape[0]\nneutral_len = train[train['sentiment']=='neutral'].shape[0]\nnegative_len = train[train['sentiment']=='negative'].shape[0]","0444ee11":"plt.rcParams['figure.figsize'] = (7, 5)\nlabels = ['Positive', 'asd0', 'bas']\nplt.bar(10,positive_len,3, label=\"Positive\", color='green')\nplt.bar(15,neutral_len,3, label=\"Neutral\", color='gray')\nplt.bar(20,negative_len,3, label=\"Negative\", color='red')\nplt.legend()\nplt.ylabel('Number of examples')\nplt.title('Sentiment distribution')\nplt.show()","98de7dbf":"def length(text):    \n    '''a function which returns the length of text'''\n    return len(text)","8ad5b8e0":"train['length'] = train['text'].apply(length)","d4a30efa":"plt.rcParams['figure.figsize'] = (18.0, 6.0)\nbins = 150\nplt.hist(train[train['sentiment']=='positive']['length'], alpha=0.3, bins=bins, label='Positive')\nplt.hist(train[train['sentiment']=='neutral']['length'], alpha=0.5, bins=bins, label='Neutral')\nplt.hist(train[train['sentiment']=='negative']['length'], alpha=0.65, bins=bins, label='Negative')\nplt.xlabel('length')\nplt.ylabel('numbers')\nplt.legend(loc='upper right')\nplt.xlim(0,150)\nplt.grid()\nplt.show()","62ff56f5":"fig,(ax1,ax2,ax3)=plt.subplots(1,3,figsize=(15,5))\ntweet_len=train[train['sentiment']=='positive']['text'].str.len()\nax1.hist(tweet_len,color='green')\nax1.set_title('positive tweets')\n\ntweet_len=train[train['sentiment']=='neutral']['text'].str.len()\nax2.hist(tweet_len,color='gray')\nax2.set_title('neutral tweets')\n\ntweet_len=train[train['sentiment']=='negative']['text'].str.len()\nax3.hist(tweet_len,color='red')\nax3.set_title('negative tweets')\n\nfig.suptitle('Characters in tweets')\nplt.show()","aefcf434":"fig,(ax1,ax2,ax3)=plt.subplots(1,3,figsize=(15,5))\ntweet_len=train[train['sentiment']=='positive']['text'].str.split().map(lambda x: len(x))\nax1.hist(tweet_len,color='green')\nax1.set_title('positive tweets')\n\ntweet_len=train[train['sentiment']=='neutral']['text'].str.split().map(lambda x: len(x))\nax2.hist(tweet_len,color='gray')\nax2.set_title('neutral tweets')\n\ntweet_len=train[train['sentiment']=='negative']['text'].str.split().map(lambda x: len(x))\nax3.hist(tweet_len,color='red')\nax3.set_title('negative tweets')\n\nfig.suptitle('Words in a tweet')\nplt.show()\n","048b8f2f":"fig,(ax1,ax2,ax3) = plt.subplots(1,3,figsize=(15,5))\n\nword=train[train['sentiment']=='positive']['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='green')\nax1.set_title('positive tweets')\n\nword=train[train['sentiment']=='neutral']['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='gray')\nax2.set_title('neutral tweets')\n\nword=train[train['sentiment']=='negative']['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax3,color='red')\nax3.set_title('negative tweets')\n\nfig.suptitle('Average word length in each tweet')\nplt.show()","20f53fb1":"def create_corpus(target):\n    corpus = []\n    \n    for x in train[train['sentiment']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","0a00af04":"corpus = create_corpus('positive')\n\ndic = defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n        \ntop = sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10]","b0069f08":"plt.rcParams['figure.figsize'] = (18.0, 6.0)\nx,y = zip(*top)\nplt.bar(x,y,color='green')\nplt.show()","9e8c9d53":"corpus = create_corpus('neutral')\n\ndic = defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n\ntop = sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \n    \n\nplt.rcParams['figure.figsize'] = (18.0, 6.0)\nx,y = zip(*top)\nplt.bar(x,y,color='gray')\nplt.show()","02ab9427":"corpus = create_corpus('negative')\n\ndic = defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n\ntop = sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \n    \n\nplt.rcParams['figure.figsize'] = (18.0, 6.0)\nx,y = zip(*top)\nplt.bar(x,y,color='red')\nplt.show()","3f4f87c3":"plt.figure(figsize=(16,5))\ncorpus = create_corpus('positive')\n\ndic = defaultdict(int)\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\n        \nx,y = zip(*dic.items())\nplt.bar(x,y,color='green')\nplt.show()","bd774c86":"plt.figure(figsize=(16,5))\ncorpus=create_corpus('neutral')\ndic = defaultdict(int)\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\n        \nx,y = zip(*dic.items())\nplt.bar(x,y,color='gray')\nplt.show()","d24935d5":"plt.figure(figsize=(16,5))\ncorpus = create_corpus('negative')\ndic = defaultdict(int)\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\n        \nx,y = zip(*dic.items())\nplt.bar(x,y,color='red')\nplt.show()","8a3984db":"plt.figure(figsize=(16,5))\ncounter = Counter(corpus)\nmost = counter.most_common()\nx = []\ny = []\nfor word,count in most[:40]:\n    if (word not in stop) :\n        x.append(word)\n        y.append(count)","52a70127":"sns.barplot(x=y,y=x)\nplt.show()","3a0fdea6":"def get_top_tweet_ngrams(corpus, ngram=2, n=None):\n    vec = CountVectorizer(ngram_range=(ngram, ngram)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","5487d8d2":"plt.figure(figsize=(16,5))\ntop_tweet_bigrams=get_top_tweet_ngrams(train['text'], 2)[:10]\nx,y=map(list,zip(*top_tweet_bigrams))\nsns.barplot(x=y,y=x)\nplt.show()","fc97f462":"plt.figure(figsize=(16,5))\ntop_tweet_bigrams=get_top_tweet_ngrams(train['text'], 3)[:10]\nx,y=map(list,zip(*top_tweet_bigrams))\nsns.barplot(x=y,y=x)\nplt.show()","1e30f6ab":"def create_corpus_df(tweet, target):\n    corpus = []\n    \n    for x in tweet[tweet['sentiment']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","0a96042b":"corpus_new_positive = create_corpus_df(train,'positive')\nlen(corpus_new_positive)","7fd52dc1":"corpus_new_positive[:10]","5d0fb248":"plt.figure(figsize=(12,8))\nword_cloud = WordCloud(\n    background_color='black',\n    max_font_size=80\n).generate(\" \".join(corpus_new_positive[:50]))\nplt.imshow(word_cloud)\nplt.axis('off')\nplt.show()","33257110":"corpus_new_neutral = create_corpus_df(train,'neutral')\nlen(corpus_new_neutral)","a7b50535":"corpus_new_neutral[:10]","86edc7e5":"# Generating the wordcloud with the values under the category dataframe\nplt.figure(figsize=(12,8))\nword_cloud = WordCloud(\n    background_color='black',\n    max_font_size=80\n).generate(\" \".join(corpus_new_neutral[:50]))\nplt.imshow(word_cloud)\nplt.axis('off')\nplt.show()","a93da858":"corpus_new_negative = create_corpus_df(train,'negative')\nlen(corpus_new_negative)","63c790d0":"corpus_new_negative[:10]","c3efd5dc":"plt.figure(figsize=(12,8))\nword_cloud = WordCloud(\n    background_color='black',\n    max_font_size=80\n).generate(\" \".join(corpus_new_negative[:50]))\nplt.imshow(word_cloud)\nplt.axis('off')\nplt.show()","d3355130":"##### Most common bigrams","c333713b":"### Analyzing punctuations","1eab6751":"##### `Positive` class","8db8162f":"##### `Negative` class","07df1bae":"<a class=\"anchor\" id=\"0.1\"><\/a>\n## Table of Contents\n  \n1. [Import libraries](#1)\n1. [Download data](#2)\n1. [EDA](#3)\n1. [WordCloud](#4)","caae3ef9":"## 2. Download data <a class=\"anchor\" id=\"2\"><\/a>\n\n[Back to Table of Contents](#0.1)","4331180a":"### Number of characters in tweets","f1a48465":"### Common stopwords in tweets","0bfed73c":"### Number of words in a tweet","9d86115f":"##### `Neutral` class","06b0847a":"## 3. EDA <a class=\"anchor\" id=\"3\"><\/a>\n\n[Back to Table of Contents](#0.1)","6c9794cf":"##### Most common trigrams","e1b11d45":"## 4. WordCloud <a class=\"anchor\" id=\"4\"><\/a>\n\n[Back to Table of Contents](#0.1)","e99b4cfa":"Let's remove it.","c5cf490f":"It seems that `text` and `selected_text` got one missing data.","70ea131f":"### Common words","60133c81":"##### `Neutral` class","3320f35a":"##### `Negative` class","2ec0634c":"In all the tweets belonging to the three sentiment classes `to` is the most common stopword, followed by `the` and `a` (except for the *Negative class* that has `my` before `a`).","996bfbf8":"# Tweet Sentiment Extraction - EDA","faacdfd8":"###  Average word length in a tweet","1118b5ae":"##### `Positive` class","1769d3c6":"##### `Neutral` class","60cdfa6a":"##### `Negative` class","dd1d6e2d":"### N-gram analysis","86c6d492":"##### `Positive` class","86293263":"## 1. Import libraries <a class=\"anchor\" id=\"1\"><\/a>\n\n[Back to Table of Contents](#0.1)","4af7ed90":"### Class distribution"}}