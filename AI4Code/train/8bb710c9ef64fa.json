{"cell_type":{"a85c953f":"code","1faebccd":"code","4cb79be7":"code","3a496175":"code","36126ef5":"code","1cf30562":"code","e714039c":"code","52f60369":"code","20f55191":"code","2e8d3431":"code","ab0007bf":"code","df4b064d":"code","21cef53f":"code","2eecf2b4":"code","f02bc63a":"code","c7212506":"code","1c659c7f":"code","06e63ec3":"code","29e40148":"code","0b94b341":"code","61b78a56":"code","ff69becb":"code","9498c230":"code","989a6f6c":"code","3e406241":"code","e76f8c43":"code","b9558157":"code","2c11205b":"code","845cad04":"code","7bfccaf0":"code","b7bd96a0":"code","6f8dc236":"code","d73dbe28":"code","9f900728":"code","0dedcc88":"code","f4e1b241":"code","3bf46145":"code","4f3b8b09":"code","422772f0":"code","ef686cc4":"code","4e03bb8d":"code","2037f2d8":"code","1cc6d396":"code","9fda4413":"code","2f4ff98c":"code","74f9215e":"code","83bfc94c":"code","0e311ae0":"code","7e5cdc61":"code","9691efec":"code","7f50ca9e":"code","4568c9bd":"code","8f079503":"code","932ebd08":"code","b8a8ad34":"code","acb35f18":"code","5997bef8":"code","27b25e3b":"code","84606cc6":"code","721c2912":"code","78b22bd7":"code","06b874f2":"code","0ece5f9f":"code","45f5d029":"code","a90a89b2":"code","cece3902":"code","0533c5f2":"code","7fe6e457":"code","0da584c0":"code","835732bf":"code","4159a922":"code","2fc232c9":"code","b9f44bda":"code","d62a07a8":"code","fa33dff3":"code","42f86c47":"code","365f44ae":"code","fb29c6b7":"code","a6973fa5":"code","51059ce8":"code","5f8fda9d":"code","e1a5ca00":"code","17d3c435":"code","67a9eb65":"code","0cd96e6e":"code","b0b0d7d2":"code","a04cb55d":"code","537ed6e3":"code","50ef1445":"code","af8cf62c":"code","02cc44b2":"code","3676b772":"code","1b9ccf3a":"code","43d8d55c":"code","f09d8fb3":"code","17cde762":"code","cc6c6f0a":"code","f11a6d84":"code","6a2e85d5":"code","0ab9e450":"code","98725132":"code","0177f91c":"code","09baca64":"code","34d95116":"code","10292120":"code","c04d0618":"code","18123872":"code","2c6593b7":"code","a98512ed":"code","85dd1bec":"markdown","76a8239e":"markdown","2f7824fb":"markdown","2a37b80c":"markdown","5fc288d3":"markdown","57a3495d":"markdown","83c28e09":"markdown","0a3ca04a":"markdown","ac72904c":"markdown","b575cabf":"markdown","a40ec02a":"markdown","201cbf33":"markdown","e436da81":"markdown","c8455a15":"markdown","09442a43":"markdown","0f1bfb0e":"markdown","9bd269f8":"markdown","159946fe":"markdown","34792454":"markdown","76bb1465":"markdown","f6721a58":"markdown","a06fba31":"markdown","13144deb":"markdown","992ba0ab":"markdown","46f59be6":"markdown","f4e76444":"markdown","0c11f164":"markdown","037c3578":"markdown","870ecf5b":"markdown","ad6b5a2f":"markdown","20637df0":"markdown","20c48339":"markdown","c0bd4c39":"markdown","5ab0b25e":"markdown","d2512303":"markdown","a28bc087":"markdown","17f01c44":"markdown","3b4f08a0":"markdown","e1537396":"markdown","7836ba3e":"markdown","2235a50c":"markdown","9cc9ed27":"markdown","a5d37744":"markdown","bf3dce2d":"markdown","8a8eafe1":"markdown","1ff37c76":"markdown","acf54f81":"markdown","21a12f1e":"markdown","3134b08e":"markdown","a8e74c63":"markdown","22774314":"markdown","d43b6a32":"markdown","ea876501":"markdown","a5160f73":"markdown","e55f33cb":"markdown","b0c4b4f1":"markdown","598c7bdd":"markdown","d65166bb":"markdown","ce563b38":"markdown","c40bba44":"markdown","f9d79d6b":"markdown","6b96d2cc":"markdown"},"source":{"a85c953f":"#from https:\/\/stackoverflow.com\/a\/31434967\nfrom IPython.display import display\nfrom IPython.display import HTML\nimport IPython.core.display as di # Example: di.display_html('<h3>%s:<\/h3>' % str, raw=True)\n\n# This line will hide code by default when the notebook is exported as HTML\ndi.display_html('<script>jQuery(function() {if (jQuery(\"body.notebook_app\").length == 0) { jQuery(\".input_area\").toggle(); jQuery(\".prompt\").toggle();}});<\/script>', raw=True)\n\n# This line will add a button to toggle visibility of code blocks, for use with the HTML export version\ndi.display_html('''<button onclick=\"jQuery('.input_area').toggle(); jQuery('.prompt').toggle();\">Toggle code<\/button>''', raw=True)","1faebccd":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport plotly.graph_objs as go\nimport squarify\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nfrom plotly import tools\nfrom xgboost import XGBClassifier\nimport warnings\nwarnings.filterwarnings('ignore')\n\nseed=42","4cb79be7":"df_raw = pd.read_csv(r'DATA\\bank.csv')\ndf_full = df_raw.copy()","3a496175":"df_full.head()","36126ef5":"df_full.shape","1cf30562":"df_full.info()","e714039c":"plt.figure(figsize=(16,8))\nlabels =\"Did not deposit\", \"Deposited\"\ncolors = ['c', 'm']\nplt.suptitle('Information on Term Suscriptions', fontsize=20)\nplt.pie(df_full[\"deposit\"].value_counts(),autopct='%1.2f%%', labels=labels, colors=colors)\nplt.ylabel('% Deposited', fontsize=14)","52f60369":"df_full.hist(bins=20, figsize=(16,8))\nplt.show()","20f55191":"df_full = df_full.drop(df_full.loc[df_full.job==\"unknown\"].index)","2e8d3431":"df_full.job = df_full.job.replace(\"admin.\", \"management\")","ab0007bf":"df_full.job.value_counts()","df4b064d":"#need to one-hot encode our target variable\nfrom sklearn.preprocessing import LabelEncoder\n\nfig=plt.figure(figsize=(16,8))\ndf_full.deposit = LabelEncoder().fit_transform(df_full.deposit)\n\n#we can only use numerical features for correlation analysis\nnumeric_df_full = df_full.select_dtypes(exclude=\"object\")\n\ncor_diagram = numeric_df_full.corr()\n\nsns.heatmap(cor_diagram, cbar=True)\nplt.title(\"Correlation matrix\")\nplt.show()","21cef53f":"df_full[\"duration_status\"] = np.nan\n\navg_duration = df_full[\"duration\"].mean()\n\ndf_full[\"duration_status\"] = df_full.apply(lambda row: \"below_average\" if row.duration<avg_duration else \"above_average\",axis=1 )\n\nyes_no_by_durationstatus = pd.crosstab(df_full['duration_status'], df_full['deposit']).apply(lambda row: round(row\/row.sum(),4) * 100, axis=1)\nyes_no_by_durationstatus","2eecf2b4":"sns.set(rc={'figure.figsize':(16,8)})\nax = yes_no_by_durationstatus.plot(kind='bar', stacked=False)\nplt.title(\"The Impact of Duration vs Opening a Deposit\", fontsize=18)\nplt.xlabel(\"Duration Status\", fontsize=18);\nplt.ylabel(\"Percentage (%)\", fontsize=18)\n\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.02, p.get_height() * 1.02))\n    \n\nplt.show()","f02bc63a":"df_full['temp_balance'] = np.ceil(df_full[\"balance\"]\/10000)*10000\nyes_no_by_temp_balance = pd.crosstab(df_full['temp_balance'], df_full['deposit']).apply(lambda row: round(row\/row.sum(),4) * 100, axis=1)\nyes_no_by_temp_balance","c7212506":"sns.set(rc={'figure.figsize':(16,8)})\nax = yes_no_by_temp_balance.plot(kind='bar', stacked=False)\nplt.title(\"The Impact of Balance vs Opening a Deposit\", fontsize=18)\nplt.xlabel(\"Balance\", fontsize=18);\nplt.ylabel(\"Percentage (%)\", fontsize=18)\n\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.02, p.get_height() * 1.02))\n    \n\nplt.show()","1c659c7f":"df_full['temp_age'] = np.ceil(df_full[\"age\"]\/10)*10\nyes_no_by_temp_age = pd.crosstab(df_full['temp_age'], df_full['deposit']).apply(lambda row: round(row\/row.sum(),4) * 100, axis=1)\nyes_no_by_temp_age","06e63ec3":"sns.set(rc={'figure.figsize':(16,8)})\nax = yes_no_by_temp_age.plot(kind='bar', stacked=False)\nplt.title(\"The Impact of Age vs Opening a Deposit\", fontsize=18)\nplt.xlabel(\"Age\", fontsize=18);\nplt.ylabel(\"Percentage (%)\", fontsize=18)\n\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.02, p.get_height() * 1.02))\n    \n\nplt.show()","29e40148":"df_full['temp_pdays_bucket'] = df_full.apply(lambda row: 0 if row['pdays']<=0 else 1 if row['pdays']<=30 else 2 if row['pdays']<=60 else 3 if row['pdays']<=90 else 6 if row['pdays']<=180 else 12 if row['pdays']<=365 else 24 if row['pdays']<=730 else 25,axis=1)","0b94b341":"df_full['temp_pdays_bucket'].value_counts()","61b78a56":"yes_no_by_temp_pdays = pd.crosstab(df_full['temp_pdays_bucket'], df_full['deposit']).apply(lambda row: round(row\/row.sum(),4) * 100, axis=1)\nyes_no_by_temp_pdays","ff69becb":"sns.set(rc={'figure.figsize':(16,8)})\nax = yes_no_by_temp_pdays.plot(kind='bar', stacked=False)#, order=['0m','1m','2m', '3m', '6m','12m', '24m', '>24m'])\nplt.title(\"The Impact of Pdays (months) vs Opening a Deposit\", fontsize=18)\nplt.xlabel(\"Pdays (months)\", fontsize=18);\nplt.ylabel(\"Percentage (%)\", fontsize=18)\n\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.02, p.get_height() * 1.02))\n    \n\nplt.show()","9498c230":"df_full['campaign'].hist()","989a6f6c":"df_full['temp_campaign'] = df_full.apply(lambda row: 11 if row['campaign']>=11 else row['campaign'],axis=1)","3e406241":"yes_no_by_temp_campaign = pd.crosstab(df_full['temp_campaign'], df_full['deposit']).apply(lambda row: round(row\/row.sum(),4) * 100, axis=1)\nyes_no_by_temp_campaign","e76f8c43":"sns.set(rc={'figure.figsize':(16,8)})\nax = yes_no_by_temp_campaign.plot(kind='bar', stacked=False)#, order=['0m','1m','2m', '3m', '6m','12m', '24m', '>24m'])\nplt.title(\"The Impact of Number of times contacted this campaign vs Opening a Deposit\", fontsize=18)\nplt.xlabel(\"Number of times contacted\", fontsize=18);\nplt.ylabel(\"Percentage (%)\", fontsize=18)\n\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.02, p.get_height() * 1.02))\n    \n\nplt.show()","b9558157":"df_full['poutcome'].hist()","2c11205b":"yes_no_by_poutcome = pd.crosstab(df_full['poutcome'], df_full['deposit']).apply(lambda row: round(row\/row.sum(),4) * 100, axis=1)\nyes_no_by_poutcome","845cad04":"sns.set(rc={'figure.figsize':(16,8)})\nax = yes_no_by_poutcome.plot(kind='bar', stacked=False)#, order=['0m','1m','2m', '3m', '6m','12m', '24m', '>24m'])\nplt.title(\"The Impact of previous campaign vs Opening a Deposit\", fontsize=18)\nplt.xlabel(\"Previous campaign outcome\", fontsize=18);\nplt.ylabel(\"Percentage (%)\", fontsize=18)\n\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.02, p.get_height() * 1.02))\n    \n\nplt.show()","7bfccaf0":"df_full['previous'].hist()","b7bd96a0":"df_full['temp_previous'] = df_full.apply(lambda row: 11 if row['previous']>=11 else row['previous'],axis=1)","6f8dc236":"yes_no_by_temp_previous = pd.crosstab(df_full['temp_previous'], df_full['deposit']).apply(lambda row: round(row\/row.sum(),4) * 100, axis=1)\nyes_no_by_temp_previous","d73dbe28":"sns.set(rc={'figure.figsize':(16,8)})\nax = yes_no_by_temp_previous.plot(kind='bar', stacked=False)#, order=['0m','1m','2m', '3m', '6m','12m', '24m', '>24m'])\nplt.title(\"The Impact of Number of times contacted previous campaign vs Opening a Deposit\", fontsize=18)\nplt.xlabel(\"Number of times contacted\", fontsize=18);\nplt.ylabel(\"Percentage (%)\", fontsize=18)\n\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.02, p.get_height() * 1.02))\n    \n\nplt.show()","9f900728":"df_full[\"duration_status\"] = np.nan\n\navg_duration = df_full[\"duration\"].mean()\n\ndf_full[\"duration_status\"] = df_full.apply(lambda row: \"below_average\" if row.duration<avg_duration else \"above_average\",axis=1 )\n\nyes_no_by_durationstatus = pd.crosstab(df_full[df_full['poutcome']==\"success\"]['duration_status'], df_full[df_full['poutcome']==\"success\"]['deposit']).apply(lambda row: round(row\/row.sum(),4) * 100, axis=1)\nyes_no_by_durationstatus","0dedcc88":"sns.set(rc={'figure.figsize':(16,8)})\nax = yes_no_by_durationstatus.plot(kind='bar', stacked=False)\nplt.title(\"The Impact of Duration vs Opening a Deposit is previous campaign was successful\", fontsize=18)\nplt.xlabel(\"Duration Status\", fontsize=18);\nplt.ylabel(\"Percentage (%)\", fontsize=18)\n\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.02, p.get_height() * 1.02))\n    \n\nplt.show()\n","f4e1b241":"\n\ndf_full[\"duration_status\"] = np.nan\n\navg_duration = df_full[\"duration\"].mean()\n\ndf_full[\"duration_status\"] = df_full.apply(lambda row: \"below_average\" if row.duration<avg_duration else \"above_average\",axis=1 )\n\nyes_no_by_durationstatus = pd.crosstab(df_full[df_full['poutcome']==\"failure\"]['duration_status'], df_full[df_full['poutcome']==\"failure\"]['deposit']).apply(lambda row: round(row\/row.sum(),4) * 100, axis=1)\nyes_no_by_durationstatus\n\n","3bf46145":"sns.set(rc={'figure.figsize':(16,8)})\nax = yes_no_by_durationstatus.plot(kind='bar', stacked=False)\nplt.title(\"The Impact of Duration vs Opening a Deposit is previous campaign was a failure\", fontsize=18)\nplt.xlabel(\"Duration Status\", fontsize=18);\nplt.ylabel(\"Percentage (%)\", fontsize=18)\n\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.02, p.get_height() * 1.02))\n    \n\nplt.show()","4f3b8b09":"df_full['temp_previous'] = df_full.apply(lambda row: 11 if row['previous']>=11 else row['previous'],axis=1)","422772f0":"yes_no_by_temp_previous = pd.crosstab(df_full[df_full['poutcome']==\"success\"]['temp_previous'], df_full[df_full['poutcome']==\"success\"]['deposit']).apply(lambda row: round(row\/row.sum(),4) * 100, axis=1)\nyes_no_by_temp_previous","ef686cc4":"sns.set(rc={'figure.figsize':(16,8)})\nax = yes_no_by_temp_previous.plot(kind='bar', stacked=False)#, order=['0m','1m','2m', '3m', '6m','12m', '24m', '>24m'])\nplt.title(\"The Impact of Number of times contacted previous campaign vs Opening a Deposit if previous campaign successful\", fontsize=18)\nplt.xlabel(\"Number of times contacted\", fontsize=18);\nplt.ylabel(\"Percentage (%)\", fontsize=18)\n\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.02, p.get_height() * 1.02))\n    \n\nplt.show()","4e03bb8d":"yes_no_by_temp_previous = pd.crosstab(df_full[df_full['poutcome']==\"failure\"]['temp_previous'], df_full[df_full['poutcome']==\"failure\"]['deposit']).apply(lambda row: round(row\/row.sum(),4) * 100, axis=1)\nyes_no_by_temp_previous","2037f2d8":"sns.set(rc={'figure.figsize':(16,8)})\nax = yes_no_by_temp_previous.plot(kind='bar', stacked=False)#, order=['0m','1m','2m', '3m', '6m','12m', '24m', '>24m'])\nplt.title(\"The Impact of Number of times contacted previous campaign vs Opening a Deposit if previous campaign not successful\", fontsize=18)\nplt.xlabel(\"Number of times contacted\", fontsize=18);\nplt.ylabel(\"Percentage (%)\", fontsize=18)\n\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.02, p.get_height() * 1.02))\n    \n\nplt.show()","1cc6d396":"df_full['temp_pdays_bucket'] = df_full.apply(lambda row: 0 if row['pdays']<=0 else 1 if row['pdays']<=30 else 2 if row['pdays']<=60 else 3 if row['pdays']<=90 else 6 if row['pdays']<=180 else 12 if row['pdays']<=365 else 24 if row['pdays']<=730 else 25,axis=1)","9fda4413":"yes_no_by_temp_pdays = pd.crosstab(df_full[df_full['poutcome']==\"success\"]['temp_pdays_bucket'], df_full[df_full['poutcome']==\"success\"]['deposit']).apply(lambda row: round(row\/row.sum(),4) * 100, axis=1)\nyes_no_by_temp_pdays","2f4ff98c":"sns.set(rc={'figure.figsize':(16,8)})\nax = yes_no_by_temp_pdays.plot(kind='bar', stacked=False)#, order=['0m','1m','2m', '3m', '6m','12m', '24m', '>24m'])\nplt.title(\"The Impact of Pdays (months) vs Opening a Deposit if previous campaign was successful\", fontsize=18)\nplt.xlabel(\"Pdays (months)\", fontsize=18);\nplt.ylabel(\"Percentage (%)\", fontsize=18)\n\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.02, p.get_height() * 1.02))\n    \n\nplt.show()","74f9215e":"yes_no_by_temp_pdays = pd.crosstab(df_full[df_full['poutcome']==\"failure\"]['temp_pdays_bucket'], df_full[df_full['poutcome']==\"failure\"]['deposit']).apply(lambda row: round(row\/row.sum(),4) * 100, axis=1)\nyes_no_by_temp_pdays","83bfc94c":"sns.set(rc={'figure.figsize':(16,8)})\nax = yes_no_by_temp_pdays.plot(kind='bar', stacked=False)#, order=['0m','1m','2m', '3m', '6m','12m', '24m', '>24m'])\nplt.title(\"The Impact of Pdays (months) vs Opening a Deposit if previous campaign was unsuccessful\", fontsize=18)\nplt.xlabel(\"Pdays (months)\", fontsize=18);\nplt.ylabel(\"Percentage (%)\", fontsize=18)\n\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.02, p.get_height() * 1.02))\n    \n\nplt.show()","0e311ae0":"yes_no_by_temp_housing = pd.crosstab(df_full['housing'], df_full['deposit']).apply(lambda row: round(row\/row.sum(),4) * 100, axis=1)\nyes_no_by_temp_housing","7e5cdc61":"sns.set(rc={'figure.figsize':(16,8)})\nax = yes_no_by_temp_housing.plot(kind='bar', stacked=False)#, order=['0m','1m','2m', '3m', '6m','12m', '24m', '>24m'])\nplt.title(\"The Impact of having an existing housing loan vs Opening a Deposit\", fontsize=18)\nplt.xlabel(\"Has housing loan\", fontsize=18);\nplt.ylabel(\"Percentage (%)\", fontsize=18)\n\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.02, p.get_height() * 1.02))\n    \n\nplt.show()","9691efec":"df_full = df_raw.copy()","7f50ca9e":"df_full.head()","4568c9bd":"df_full[df_full.age.isnull()]","8f079503":"from sklearn.model_selection import StratifiedShuffleSplit\n\nstratifier = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n\nfor training_data_indexes, test_data_indexes in stratifier.split(df_full,df_full.loan):\n    stratified_training_data = df_full.loc[training_data_indexes]\n    stratified_test_data = df_full.loc[test_data_indexes]\n    \n    \n#create copies\ntrain = stratified_training_data.copy()\ntest = stratified_test_data.copy()","932ebd08":"df_full","b8a8ad34":"train.shape","acb35f18":"train","5997bef8":"test.shape","27b25e3b":"from sklearn.base import BaseEstimator, TransformerMixin\n\n# A class to select numerical or categorical columns\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X[self.attribute_names]","84606cc6":"train.info()","721c2912":"numeric_cols = train.select_dtypes(include=np.number).columns.tolist()\n#numeric_cols.remove('deposit')","78b22bd7":"numeric_cols","06b874f2":"categoric_cols = train.select_dtypes(exclude=np.number).columns.tolist()\n#remove target variable, to ensure that we end up with feature set, not label\ncategoric_cols.remove('deposit')\ncategoric_cols","0ece5f9f":"from sklearn.preprocessing import OneHotEncoder,StandardScaler","45f5d029":"# Build pipelines to scale numerical features and encode categorical\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n# Making pipelines\nnumerical_pipeline = Pipeline([\n    (\"select_numeric\", DataFrameSelector(numeric_cols)),\n    (\"std_scaler\", StandardScaler()),\n])\n\ncategorical_pipeline = Pipeline([\n    (\"select_cat\", DataFrameSelector(categoric_cols)),\n    (\"cat_encoder\",  OneHotEncoder(handle_unknown='ignore'))\n])\n\nfrom sklearn.pipeline import FeatureUnion\n# https:\/\/stackoverflow.com\/a\/52666039\npreprocess_pipeline = FeatureUnion(transformer_list=[\n        (\"numerical_pipeline\", numerical_pipeline),\n        (\"categorical_pipeline\", categorical_pipeline),\n    ])","a90a89b2":"X_train = preprocess_pipeline.fit_transform(train)\nX_train = X_train.toarray()\nX_train","cece3902":"X_train.shape","0533c5f2":"categorical_pipeline[\"cat_encoder\"].get_feature_names()","7fe6e457":"# do not fit the scaler to test features\nX_test = preprocess_pipeline.transform(test)\nX_test = X_test.toarray()\nX_test","0da584c0":"#instantiatie label from stratified datasets\ny_train = train['deposit']\ny_test = test['deposit']\ny_train.shape","835732bf":"# encode label\nfrom sklearn.preprocessing import LabelEncoder\n\nencode = LabelEncoder()\ny_train = encode.fit_transform(y_train)\ny_test = encode.fit_transform(y_test)","4159a922":"feature_names = numeric_cols.copy()\nfeature_names.extend([col for col in categorical_pipeline[\"cat_encoder\"].get_feature_names()])","2fc232c9":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import cross_val_score, cross_val_predict\n\n\ndict_classifiers = {\n    'logistic_reg': LogisticRegression(),\n    'support_vector_clf': SVC(),\n    'knn_clf': KNeighborsClassifier(),\n    'gradient_boosting_clf': GradientBoostingClassifier(),\n    'random_forest_cld': RandomForestClassifier(),\n    'naive_bayes_clf': GaussianNB(),\n    'xgboost_clf': XGBClassifier()\n}\n\ndict_preds = {}\n\nfor this_classifier in dict_classifiers.keys():\n    this_score = cross_val_score(dict_classifiers[this_classifier], X_train, y_train, cv=5)\n    # https:\/\/stackoverflow.com\/questions\/25006369\/what-is-sklearn-cross-validation-cross-val-score\n    this_mean = this_score.mean()\n    # https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.cross_val_predict.html\n    dict_preds[this_classifier] = cross_val_predict(dict_classifiers[this_classifier], X_train, y_train, cv=5) #get predictions, for ROC curve\n    print(this_classifier, this_mean)","b9f44bda":"from sklearn.metrics import confusion_matrix\n#https:\/\/machinelearningmastery.com\/confusion-matrix-machine-learning\/\n \nfor this_classifier in dict_preds.keys():\n    this_pred = dict_preds[this_classifier]\n    confusion_results = confusion_matrix(y_train, this_pred)\n    print(this_classifier, \"\\n\\n\", confusion_results, \"\\n\\n\")","d62a07a8":"from sklearn.metrics import roc_auc_score\n\nfor this_classifier in dict_preds.keys():\n    print('{}: {}'.format(this_classifier, roc_auc_score(y_train, dict_preds[this_classifier])))","fa33dff3":"import xgboost as xgb","42f86c47":"# https:\/\/stackoverflow.com\/a\/46943417\ndtrain = xgb.DMatrix(X_train, label=y_train,feature_names=feature_names)\ndtest = xgb.DMatrix(X_test, label=y_test,feature_names=feature_names)","365f44ae":"from sklearn.metrics import mean_absolute_error","fb29c6b7":"# https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html\nparams = {\n    # Parameters that we are going to tune.\n    'max_depth':6,\n    'min_child_weight': 1,\n    'eta':.3,\n    'subsample': 1,\n    'colsample_bytree': 1,\n    # Other parameters\n    'objective':'reg:linear',\n}\n\nparams['eval_metric'] = \"auc\"\nparams['verbosity'] = 0\n\nnum_boost_round = 999","a6973fa5":"model = xgb.train(\n    params,\n    dtrain,\n    num_boost_round=num_boost_round,\n    evals=[(dtest, \"Test\")],\n    early_stopping_rounds=10\n)","51059ce8":"cv_results = xgb.cv(\n    params,\n    dtrain,\n    num_boost_round=num_boost_round,\n    seed=42,\n    nfold=5,\n    metrics={'auc'},\n    early_stopping_rounds=10\n)\n\ncv_results","5f8fda9d":"gridsearch_params = [\n    (max_depth, min_child_weight)\n    for max_depth in range(6,9)\n    for min_child_weight in range(1,8)\n]","e1a5ca00":"# Define initial best params and AUC\n# min_mae = float(\"Inf\") # use this if for MAE\nmax_auc = 0\nbest_params = None\nfor max_depth, min_child_weight in gridsearch_params:\n    print(\"CV with max_depth={}, min_child_weight={}\".format(\n                             max_depth,\n                             min_child_weight))\n    # Update our parameters\n    params['max_depth'] = max_depth\n    params['min_child_weight'] = min_child_weight\n    # Run CV\n    cv_results = xgb.cv(\n        params,\n        dtrain,\n        num_boost_round=num_boost_round,\n        seed=42,\n        nfold=5,\n        metrics={'auc'},\n        early_stopping_rounds=10\n    )\n    # Update best AUC\n    mean_auc = cv_results['test-auc-mean'].max() \n    # mean_rmse = cv_results['test-mae-mean'].min()\n    boost_rounds = cv_results['test-auc-mean'].argmax()\n    print(\"\\tAUC {} for {} rounds\".format(mean_auc, boost_rounds))\n    if mean_auc > max_auc:\n        max_auc = mean_auc\n        best_params = (max_depth,min_child_weight)\nprint(\"Best params: {}, {}, AUC: {}\".format(best_params[0], best_params[1], max_auc))","17d3c435":"params['max_depth'] = 6\nparams['min_child_weight'] = 3","67a9eb65":"params","0cd96e6e":"gridsearch_params = [\n    (subsample, colsample)\n    for subsample in [i\/10. for i in range(1,11)]\n    for colsample in [i\/10. for i in range(1,11)]\n]","b0b0d7d2":"max_auc = 0\nbest_params = None\n# We start by the largest values and go down to the smallest\nfor subsample, colsample in reversed(gridsearch_params):\n    print(\"CV with subsample={}, colsample={}\".format(\n                             subsample,\n                             colsample))\n    # We update our parameters\n    params['subsample'] = subsample\n    params['colsample_bytree'] = colsample\n    # Run CV\n    cv_results = xgb.cv(\n        params,\n        dtrain,\n        num_boost_round=num_boost_round,\n        seed=42,\n        nfold=5,\n        metrics={'auc'},\n        early_stopping_rounds=10\n    )\n    # Update best AUC\n    mean_auc = cv_results['test-auc-mean'].max() \n    # mean_rmse = cv_results['test-mae-mean'].min()\n    boost_rounds = cv_results['test-auc-mean'].argmax()\n    print(\"\\tAUC {} for {} rounds\".format(mean_auc, boost_rounds))\n    if mean_auc > max_auc:\n        max_auc = mean_auc\n        best_params = (subsample,colsample)\nprint(\"Best params: {}, {}, AUC: {}\".format(best_params[0], best_params[1], max_auc))","a04cb55d":"params['subsample'] = 0.9\nparams['colsample_bytree'] = 0.8","537ed6e3":"%time\n# This can take some time\u2026\n\nmin_mae = float(\"Inf\")\nbest_params = None\nfor eta in [.3, .2, .1, .05, .01, .005]:\n    print(\"CV with eta={}\".format(eta))\n    # We update our parameters\n    params['eta'] = eta\n    # Run and time CV\n    %time\n    \n    # Run CV\n    cv_results = xgb.cv(\n        params,\n        dtrain,\n        num_boost_round=num_boost_round,\n        seed=42,\n        nfold=5,\n        metrics={'auc'},\n        early_stopping_rounds=10\n    )\n    # Update best AUC\n    mean_auc = cv_results['test-auc-mean'].max() \n    # mean_rmse = cv_results['test-mae-mean'].min()\n    boost_rounds = cv_results['test-auc-mean'].argmax()\n    print(\"\\tAUC {} for {} rounds\".format(mean_auc, boost_rounds))\n    if mean_auc > max_auc:\n        max_auc = mean_auc\n        best_params = eta\nprint(\"Best params: {}, AUC: {}\".format(best_params, max_auc))","50ef1445":"params['eta'] = 0.01\nparams","af8cf62c":"model = xgb.train(\n    params,\n    dtrain,\n    num_boost_round=num_boost_round,\n    evals=[(dtest, \"Test\")],\n    early_stopping_rounds=10\n)","02cc44b2":"num_boost_round = model.best_iteration + 1\nnum_boost_round","3676b772":"our_best_model = xgb.train(\n    params,\n    dtrain,\n    num_boost_round=num_boost_round,\n    evals=[(dtest, \"Test\")]\n)","1b9ccf3a":"params","43d8d55c":"XGBClassifier(num_boost_round=num_boost_round)","f09d8fb3":"dict_classifiers['xgboost_clf'] = XGBClassifier(colsample_bytree=0.8, eta=0.01,\n                      eval_metric='auc', max_depth=6,\n                      min_child_weight=3, objective='reg:linear',\n                      subsample=0.9,seed=42,num_boost_round=804,verbosity=0)\n\ndict_classifiers","17cde762":"# accuracy\nrevised_score = cross_val_score(dict_classifiers['xgboost_clf'], X_train, y_train, cv=5)\n\nrevised_score.mean()","cc6c6f0a":"revised_preds = cross_val_predict(dict_classifiers['xgboost_clf'], X_train, y_train, cv=5)","f11a6d84":"confusion_results = confusion_matrix(y_train, revised_preds)\nconfusion_results","6a2e85d5":"revised_auc = roc_auc_score(y_train, revised_preds)\nrevised_auc","0ab9e450":"our_best_model.save_model(\"my_model.model\")","98725132":"our_best_predictions = our_best_model.predict(dtest)\nfinal_best_predictions = [abs(round(value)) for value in our_best_predictions]","0177f91c":"confusion_results = confusion_matrix(y_test, final_best_predictions)\nconfusion_results","09baca64":"revised_auc = roc_auc_score(y_test, final_best_predictions)\nrevised_auc","34d95116":"from sklearn.metrics import accuracy_score\n\naccuracy_score(y_test, final_best_predictions)","10292120":"from xgboost import plot_importance","c04d0618":"plot_importance(our_best_model)\nplt.show()","18123872":"import statsmodels.api as sm","2c6593b7":"model = sm.GLM.from_formula(\"deposit ~ duration + balance + age + pdays + campaign + poutcome + previous + housing \", family=sm.families.Binomial(), data=train)\nresult = model.fit()\nresult.summary()","a98512ed":"dummy_df = pd.DataFrame(train[['duration', 'previous', 'poutcome']])\ndummy_df['poutcome'] = dummy_df.apply(lambda row: 1 if row['poutcome']=='success' else 0,axis=1)\ndummy_df_cor = dummy_df.corr()\npd.DataFrame(np.linalg.inv(dummy_df.corr().values), index = dummy_df_cor.index, columns=dummy_df_cor.columns)","85dd1bec":"#### Age vs deposit","76a8239e":"Hunch is that jobs impact likelihood of subscribing to term deposit.","2f7824fb":"#### Pdays vs deposit","2a37b80c":"From here, we see campaign last contact duration has the highest correlation with whether or not a person subscribes for a deposit.","5fc288d3":"From the above, we see that generally those who did not subscribe to a term deposit tend to have lower bank balances. \n\nNote: 0 here means they had negative balances ie they were in overdraft.","57a3495d":"We see the following features were more important for the boosted decisions trees to model the data:\n\n- 'duration': duration (in miliseconds) of last contact<br>\n- 'balance': balance of the individual<br>\n- 'age': age of this person<br>\n- 'pdays': days since last contact<br>\n- 'campaign': number of contacts for this person in this campaign<br>\n- 'x6_success': if outcome of the previous marketing campaign was succesful<br>\n- 'previous': number of times someone was contacted in previous campaign<br>\n- 'x4_housing': does not have a housing loan<br>\n\nLet's check these against a basic logistic regression model to see if these features also hold true as significant.","83c28e09":"Not clear but appears to suggest that if someone was recently contacted, and if they were contacted again this time, they were more likely to decline the offer to subscribe. Seems to suggest that the longer someone has not been contacted, the more likely they are to subscribe.\n\nNote: 0 means they were just contacted. 25 means more than 24 months since last contact","0a3ca04a":"Let's remove \"unknown\" job.","ac72904c":"Distribution of numerical data","b575cabf":"#### Days since last campaign vs success of this campaign, given success of previous campaign","a40ec02a":"## 7) Steps to improve analysis\n\nIn future iterations of this analysis, we will likely focus on the following:\n\n- Improve false negative rates. This is potential cash on the table, valuable opportunities we may be missing out on\n- Try a neural network to see if any improvements","201cbf33":"From here, it suggests that if your previous outcome was not a success and if you spent a lot of time in contact with them in that failed campaign, it is unlikely you would see much success in a repeat campaign.\n\n(From earlier sections, we already know that prior campaign success seems to relate with a repeat success!)","e436da81":"## 5) Key features","c8455a15":"### Split train - test","09442a43":"From the above, if you had a housing loan, you were less likely to subscribe to a deposit from this campaign.","0f1bfb0e":"From the above, it looks like we need more rounds to get to the best AUC score on the test set compared to before we did our grid-search but at least we have a higher AUC now.\n\n\n\n\n**Check results on training set**\n\nNow that we know what our best number of boosting trees are needed, let's retrain the model again before saving.","9bd269f8":"Interestingly, the more you contacted someone in previous campaign, the more likely they were to subscribe to a term deposit.\nConversely, if they had never heard from you before (not contacted from previous campaign), they were less likely to subscribe this time.\n\nNote that there may be some correlations between 'duration', 'previous contact times' and 'previous outcome'. As we are likely to use a tree-based model, multicollinearity is not an issue but we will be testing this assumption in order to flush out what exactly are key predictors.\n\nFor now, let's visualise these features.\n\n#### Duration vs deposit, given success of previous campaign","159946fe":"From the above, if the previous campaign failed, it is not exactly clear whether or not an existing 'rapport' (by way of having contacted them a few times before) is actually helpful in convincing them to sign up this time.\n\n(From earlier sections, we already know that prior campaign success seems to relate with a repeat success!)","34792454":"#### Duration vs deposit","76bb1465":"Let's see correlations","f6721a58":"#### Number of days contacted in previous campaign vs success in this campaign, given success of previous campaign","a06fba31":"**Cross-validation**\n\nInitial cv to see baseline AUC.","13144deb":"**Gridsearch - Parameters ETA:**\n\n\"The ETA parameter controls the learning rate. It corresponds to the shrinkage of the weights associated to features after each round, in other words it defines the amount of \"correction\" we make at each step (remember how each boosting round is correcting the errors of the previous).\n\nIn practice, having a lower eta makes our model more robust to overfitting thus, usually, the lower the learning rate, the best. But with a lower eta, we need more boosting rounds, which takes more time to train, sometimes for only marginal improvements.\"\n\nSource: https:\/\/blog.cambridgespark.com\/hyperparameter-tuning-in-xgboost-4ff9100a3b2f","992ba0ab":"#### Number of times contacted in previous campaign vs outcome of this campaign","46f59be6":"#### Previous outcome vs deposit","f4e76444":"And replace \"admin\" with management since fairly similar.","0c11f164":"## 6) Recommended actions for a marketing strategy","037c3578":"From the above, we actually see that age and pdays (days since last contact) may not necessarily have a significant relationship with the odds of someone subscribing to the term deposit (p-values below 0.05). Slightly different conclusion from our xgboost model.\n\nNext, we also look at multicollineariy between previous campaign number of contacts and durations, with previous campaign success. If they were correlated, this may mean our standard errors from the logistic regression model may be wrong, and this would result in incorrect p-values.","870ecf5b":"Plot confusion matrix to see precision and recall. Note the following:\n\n- TN: Those we identified as not taking up a deposit, and in reality did not\n- TP: Those we identfied as taking up a deposit, and in reality did\n- FN: Those we identified as not taking up a deposit, but in reality did\n- FP: Those we identified as taking up a deposit, but in reality did not\n\n\n  \t\t\t\n- (top row) true positive, false positive\n- (bottom row) false negative, true negative","ad6b5a2f":"From the above, if you were very young or a senior citizen, you were more likely to subscribe to a bank deposit.","20637df0":"## 3. Model data","20c48339":"No missing values","c0bd4c39":"AUC for validation set is lower than training set, indicating likely overfitting.","5ab0b25e":"**Set the number of boosting rounds or trees to build, defined by num_boost_round**\n\nIf metric has not improved for n number of rounds (defined by early_stopping_rounds), we print the best number of trees to build.\n\nNote, num_boost_round is highly dependent on other hyperparams. Therefore, we will retune this at the end of our various grid-searches.","d2512303":"On just accuracy evaluation, GB classifier appears to perform the best but only marginally better than an svm.","a28bc087":"#### Campaign vs deposit","17f01c44":"### Evaluate precision-recall using AUC in ROC.\n\nhttps:\/\/towardsdatascience.com\/understanding-auc-roc-curve-68b2303cc9c5","3b4f08a0":"**Gridsearch - Parameters subsample and colsample_bytree:**\n\n\"These parameters control the sampling of the dataset that is done at each boosting round.\nInstead of using the whole training set every time, we can build a tree on slightly different data at each step, which makes it less likely to overfit to a single sample or feature.\n\n- **subsample** corresponds to the fraction of observations (the rows) to subsample at each step. By default it is set to 1 meaning that we use all rows.\n\n- **colsample_bytree** corresponds to the fraction of features (the columns) to use. By default it is set to 1 meaning that we will use all features.\"\n\nSource: https:\/\/blog.cambridgespark.com\/hyperparameter-tuning-in-xgboost-4ff9100a3b2f","e1537396":"1 - **age:** (numeric)<br>\n2 - **job:** type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')<br>\n3 - **marital:** marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)<br>\n4 - **education:** (categorical: primary, secondary, tertiary and unknown)<br>\n5 - **default:** has credit in default? (categorical: 'no','yes','unknown')<br>\n6 - **housing:** has housing loan? (categorical: 'no','yes','unknown')<br>\n7 - **loan:** has personal loan? (categorical: 'no','yes','unknown')<br>\n8 - **balance:** Balance of the individual.<br>\n9 - **month:** last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')<br>\n10 - **day:** last contact day of the week (categorical: 'mon','tue','wed','thu','fri')<br>\n11 - **duration:** last contact duration, in seconds (numeric).<br>\n12 - **campaign:** number of contacts performed during this campaign and for this client (numeric, includes last contact)<br>\n13 - **pdays:** number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)<br>\n14 - **previous:** number of contacts performed before this campaign and for this client (numeric)<br>\n15 - **poutcome:** outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')<br>\n16 - **contact:** contact communication type (categorical: 'cellular','telephone') <br>\n\nOutput variable (desired target):<br>\n17 - **y** - has the client subscribed a term deposit? (binary: 'yes','no')","7836ba3e":"**Gridsearch - Parameters max_depth and min_child_weight:**\n\n\"These parameters add constraints on the architecture of the trees.\n- **max_depth** is the maximum number of nodes allowed from the root to the farthest leaf of a tree. Deeper trees can model more complex relationships by adding more nodes, but as we go deeper, splits become less relevant and are sometimes only due to noise, causing the model to overfit.\n\n- **min_child_weight** is the minimum weight (or number of samples if all samples have a weight of 1) required in order to create a new node in the tree. A smaller min_child_weight allows the algorithm to create children that correspond to fewer samples, thus allowing for more complex trees, but again, more likely to overfit.\"\n\nSource: https:\/\/blog.cambridgespark.com\/hyperparameter-tuning-in-xgboost-4ff9100a3b2f\n\n","2235a50c":"## 8) Resources:\n\n1. https:\/\/www.kaggle.com\/janiobachmann\/bank-marketing-campaign-opening-a-term-deposit: Main reference notebook for steps to proceed\n\n2. https:\/\/blog.cambridgespark.com\/hyperparameter-tuning-in-xgboost-4ff9100a3b2f: Tips to tune xgboost\n\n3. https:\/\/stackoverflow.com\/a\/31434967: Toggle code button\n\n4. https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.cross_val_predict.html: Logic of cross-val predictions\n\n5. https:\/\/machinelearningmastery.com\/confusion-matrix-machine-learning\/: Code confusion matrix\n\n6. https:\/\/towardsdatascience.com\/understanding-auc-roc-curve-68b2303cc9c5: Explanation of AUC for ROC\n\n7. https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html: XGBoost parameters\n\n8. https:\/\/machinelearningmastery.com\/develop-first-xgboost-model-python-scikit-learn\/: Converting XGBoost Classifier output to binary\n\n9. https:\/\/stackoverflow.com\/questions\/25006369\/what-is-sklearn-cross-validation-cross-val-score: Logic of cross-val score\n\n10. https:\/\/towardsdatascience.com\/multicollinearity-in-data-science-c5f6c0fe6edf: Explanation of variance inflation factor\n\n11. https:\/\/stackoverflow.com\/a\/51463149: Code for variance inflation factor","9cc9ed27":"#### Correlations","a5d37744":"#### Housing loan vs deposit","bf3dce2d":"### Build transformation pipeline","8a8eafe1":"The diagonal values indicate the variance inflation factor (VIF) which is a metric for multicollinearity between two predictors. Values below 5 indicate that is collinearity is absent. As such, the signifance of these features are valid.","1ff37c76":"From the above, repeat contacts make it even less likely for someone to subscribe.","acf54f81":"From here, we see that if you had an above average duration in last campaign, and you were contacted again, majority would subscribe. Conversely, if you had a short duration of contact in last campaign, and you were contacted again, majority would not subscribe to a deposit. This is possibly a sign of interest and\/or rapport. Ie customers we spent more time talking to were more open to subscribing.","21a12f1e":"If previous outcome was succesful, you were overwhelmingly more likely to subscribe.","3134b08e":"## 1. Import packages and data","a8e74c63":"From the above, comparing svm, gb and xgboost, unable to see a clear winner. Both have not insignificant false positives and negatives.\n\nTo have a single metric, we can plot a precision-recall curve.","22774314":"Looking at AUC, all three are close but gradient boosting_clf is highest. However, given the recent surge in use of xgboost, let's go with this model for now.","d43b6a32":"Based on the analysis we conducted in the earlier sections of this notebook as well as the insights generated from machine learning, general guidance would be to avoid people with overdrawn bank balances, existing housing loans, and we pestered and failed with in previous attempts.\n\nMore precide tactics would be as follows:\n\n- 'duration' and 'previous': Best to try to avoid individuals we 'pestered' before if our previous campaign failed. However, if we had success with them before, we may see success again regardless of how much we pestered them before.<br>\n- 'balance': Perhaps greater campaign success by not approaching individuals with overdrawn bank balances<br>\n- 'campaign': Repeatedly pestering someone this campaign will make them less likely to sign up. Therefore, need to make sure we build rapport with minimal repeated attempts.<br>\n- 'x6_success': Focus on inidividuals where we had success with in our previous campaign<br>\n- 'x4_housing': Focus on individuals without housing loan debts (perhaps indicative of additional capital to use).<br>\n\nOther tactics that may be helpful:\n- 'age': Approach more people who are either in their 20's or 60's and above<br>\n- 'pdays': Perhaps consider focusing on people we may not have spoken to in a while<br>","ea876501":"### Train using cross-val","a5160f73":"**Cross-validation again, to decide on num_boost_rounds**","e55f33cb":"### Evaluate confusion matrix","b0c4b4f1":"## 2. Understand data\n\nCopied from this kaggle notebook: https:\/\/www.kaggle.com\/janiobachmann\/bank-marketing-campaign-opening-a-term-deposit\/notebook","598c7bdd":"#### Balance vs deposit","d65166bb":"### Tune XG Boost\n\nhttps:\/\/blog.cambridgespark.com\/hyperparameter-tuning-in-xgboost-4ff9100a3b2f","ce563b38":"Best boosting rounds is 32 ie 32 trees to build.","c40bba44":"From here, we see that the test AUC is not too far from training and validation. This means we are not overfitting.\n\nOverall, acceptable results.\n\nNext, we will see what were key features in helping the model make it's decisions.","f9d79d6b":"<h1> Bank Marketing Campaign <\/h1>\n\nWe are provided with data on a recent marketing campaign conducted by a bank to encourage individuals to sign up for a term deposit. \n\nThe challenge here is to build a model to be able to predict whether or not a given individual will sign up for a term subscription if contacted as part of this marketing campaign. \n\nThis notebook will explore the dataset provided and use machine learning and statistical inferencing to build a predictive model. At the end, based on the insights generated by the model, we also provide suggestions on types of individuals to approach to increase the effectiveness of this campaign.\n\nKindly note that the approaches used in this model is based on https:\/\/www.kaggle.com\/janiobachmann\/bank-marketing-campaign-opening-a-term-deposit. Additional resources are included at the end of this notebook.","6b96d2cc":"## 4) Make predictions\n\nhttps:\/\/machinelearningmastery.com\/develop-first-xgboost-model-python-scikit-learn\/\n\n\"To make predictions we use the scikit-learn function model.predict().\n\nBy default, the predictions made by XGBoost are probabilities. Because this is a binary classification problem, each prediction is the probability of the input pattern belonging to the first class. We can easily convert them to binary class values by rounding them to 0 or 1.\""}}