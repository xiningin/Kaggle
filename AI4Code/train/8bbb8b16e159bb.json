{"cell_type":{"e7fa92a3":"code","9be8cb05":"code","0f72adf4":"code","9cc3767f":"code","cad1b21b":"code","4cca4269":"code","dde60c2f":"code","5750beb9":"code","0056f634":"markdown","06843cfd":"markdown","920c7d9e":"markdown","a2c13bf2":"markdown","3d8a7389":"markdown","febdff93":"markdown"},"source":{"e7fa92a3":"%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport os \nimport tensorflow as tf \nfrom tensorflow import keras\nfrom keras.preprocessing.image import ImageDataGenerator,load_img, img_to_array\nfrom keras.layers import Flatten, Dense, Dropout, GlobalAveragePooling2D, BatchNormalization\nfrom keras.models import Sequential, Model\nfrom keras.optimizers import RMSprop\nfrom keras.applications import MobileNet\nfrom keras.callbacks import ModelCheckpoint","9be8cb05":"os.chdir('..\/input\/face-expression-recognition-dataset\/images\/')\ntrain_path = '.\/train\/'\nvalidation_path = '.\/validation\/'\nos.listdir()","0f72adf4":"plt.figure(0, figsize=(48,48))\ncpt = 0\n\nfor expression in os.listdir(train_path):\n    for i in range(1,6):\n        cpt = cpt + 1\n        sp=plt.subplot(7,5,cpt)\n        sp.axis('Off')\n        img_path = train_path + expression + \"\/\" +os.listdir(train_path + expression)[i]\n        img = load_img( img_path, target_size=(48,48))\n        plt.imshow(img, cmap=\"gray\")\n\n\nplt.show()","9cc3767f":"train_datagen = ImageDataGenerator(rescale=1\/255,\n                                   rotation_range=30,\n                                   zoom_range=0.2)\n\nvalidation_datagen = ImageDataGenerator(rescale=1\/255)\n\nbatch_size = 128\n\n\ntrain_generator = train_datagen.flow_from_directory (train_path,   \n                                                     target_size=(48, 48),  \n                                                     batch_size=batch_size,\n                                                     shuffle=True,\n                                                     class_mode='categorical',\n                                                     color_mode=\"rgb\")\n\n\nvalidation_generator = validation_datagen.flow_from_directory(validation_path,  \n                                                              target_size=(48,48), \n                                                              batch_size=batch_size,\n                                                              class_mode='categorical',\n                                                              color_mode=\"rgb\")\n","cad1b21b":"#Loading the Mobilenet model \nfeaturizer = MobileNet(include_top=False, weights='imagenet', input_shape=(48,48,3))\n\n#Since we have 7 types of expressions, we'll set the nulber of classes to 7\nnum_classes = 7\n\n#Adding some layers to the feturizer\nx = Flatten()(featurizer.output)\nx = Dense(1024, activation='relu')(x)\nx = Dropout(0.5)(x)\nx = BatchNormalization()(x)\npredictions = Dense(num_classes, activation = 'softmax')(x)\n\n\n\nmodel = Model(input = featurizer.input, output = predictions)\n\n\nmodel.compile(optimizer=RMSprop(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n\nmodel.summary()","4cca4269":"model_weights_path = r'\/kaggle\/working\/model_weights.h5'\n\ncheckpoint = ModelCheckpoint(model_weights_path, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n\n\nhistory = model.fit(train_generator,\n                        steps_per_epoch=train_generator.n\/\/train_generator.batch_size,\n                        validation_steps=validation_generator.n\/\/validation_generator.batch_size,\n                        epochs=60,\n                        verbose=1,\n                        validation_data = validation_generator,\n                        callbacks=[checkpoint])","dde60c2f":"# serialize model to JSON\nmodel_json = model.to_json()\nwith open(r'\/kaggle\/working\/model.json', \"w\") as json_file:\n    json_file.write(model_json)","5750beb9":"plt.figure(figsize=(20,10))\n\nplt.subplot(1, 2, 1)\nplt.ylabel('Accuracy', fontsize=16)\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.legend(loc='upper right')\n\n\nplt.subplot(1, 2, 2)\nplt.ylabel('Loss', fontsize=16)\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.legend(loc='lower right')\n\n\nplt.show()","0056f634":"# Training \nCallbacks : we'll use **ModelCheckpoint**, which allows us to save our model's weights, and by setting **save_best_only** parameter to true, the latest best model according to the quantity monitored (which is val_accuracy here) won't be overwritten. Our **mode** parameter is set to max because we want the val_accuracy to be the highest. ","06843cfd":"# Saving Our Model","920c7d9e":"# Displaying images from the training directory","a2c13bf2":"# Accuracy & Loss","3d8a7389":"# Building Our Model","febdff93":"# Data Generators"}}