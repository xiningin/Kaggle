{"cell_type":{"af06a5af":"code","837d2826":"code","d1385a5c":"code","aadd0bb4":"code","f68290cc":"code","3f0ac9da":"code","6d428ef2":"code","081f13ff":"markdown","79d73372":"markdown","f5f2c895":"markdown"},"source":{"af06a5af":"from sklearn.metrics import accuracy_score\nfrom collections import Counter\nimport pandas as pd \nimport numpy as np \nimport itertools\nimport random \nimport warnings\nwarnings.filterwarnings(\"ignore\")\nrandom.seed(0)\n\n# Read the data \ntrain_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_df =  pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n\n# Join the train and test dataframes so the data preprocessing \n# will be done simultaneously in both datasets \nfull_df = train_df.append(test_df, ignore_index=True)","837d2826":"def data_preprocessing(df):\n  \n  # Label-encode the sex of a passenger \n  df['Sex'] = df['Sex'].replace(['male'],0)\n  df['Sex'] = df['Sex'].replace(['female'],1)\n\n  # Initialize new columns \n  df['title'] = np.NaN\n  df['alone'] = np.NaN\n  df['cabin_class'] = np.NaN\n\n  # Identify if a passenger is alone in the ship \n  for i,_ in enumerate(df['alone']):\n    if df['SibSp'][i] + df['Parch'][i] == 0:\n      df['alone'][i] = 1\n    else:\n      df['alone'][i] = 0 \n\n  # Handle missing values\n  cols = ['SibSp','Parch','Fare','Age']\n  for col in cols:\n    df[col].fillna(df[col].median(), inplace = True)\n    \n  # Feature-engineer the cabin-class \n  for i,row in enumerate(df['Cabin']):\n    # Get cabin class \n    df['cabin_class'][i] =  str(row)[:1]\n\n  # Count the cabin distribution per class (if available) \n  cabin_distribution = {}\n  count = 0 \n  for row in df['cabin_class']:\n    if row != 'n':\n      count += 1 \n      if row not in cabin_distribution:\n        cabin_distribution[row] = 1 \n      else:\n        cabin_distribution[row] +=1 \n\n  # Calculate the probability of being in a sepcific cabin-class  \n  cabin_pdf = {k:v \/ count for k, v in cabin_distribution.items()}\n\n  # Calculate the cumulative probability of being in a specific cabin-class \n  keys, vals = cabin_pdf.keys(), cabin_pdf.values()\n  cabin_cdf = dict(zip(keys, itertools.accumulate(vals)))\n  cabin_cdf = sorted(cabin_cdf.items(), key=lambda x: x[1])    \n\n  # Randomly assign cabin-classes to passengers that are missing the cabin \n  # field, based on the probabilities calculated above \n  for i,row in enumerate(df['cabin_class']):\n    random_num = random.random()\n    if row == 'n':\n      if random_num < cabin_cdf[0][1]:\n        df['cabin_class'][i] =  cabin_cdf[0][0]\n      elif cabin_cdf[0][1] <= random_num < cabin_cdf[1][1]:\n        df['cabin_class'][i] =  cabin_cdf[1][0]\n\n      elif cabin_cdf[1][1] <= random_num < cabin_cdf[2][1]:\n        df['cabin_class'][i] =  cabin_cdf[2][0]\n      \n      elif cabin_cdf[2][1] <= random_num < cabin_cdf[3][1]:\n        df['cabin_class'][i] =  cabin_cdf[2][0]\n\n      elif cabin_cdf[3][1] <= random_num < cabin_cdf[4][1]:\n        df['cabin_class'][i] =  cabin_cdf[3][0]\n\n      elif cabin_cdf[3][1] <= random_num < cabin_cdf[4][1]:\n        df['cabin_class'][i] =  cabin_cdf[4][0]\n\n      elif cabin_cdf[4][1] <= random_num < cabin_cdf[5][1]:\n        df['cabin_class'][i] =  cabin_cdf[4][0]\n      \n      elif cabin_cdf[5][1] <= random_num < cabin_cdf[6][1]:\n        df['cabin_class'][i] =  cabin_cdf[5][0]\n\n      elif cabin_cdf[6][1] <= random_num < cabin_cdf[7][1]:\n        df['cabin_class'][i] =  cabin_cdf[6][0]\n      else:\n        df['cabin_class'][i] = cabin_cdf[7][0]\n\n  # Perform feature engineering to obtain additional title-info \n  for i,row in enumerate(df['Name']):\n    # Get person's title \n    df['title'][i] = row.split(',')[1].split('.')[0]\n\n  # Embarked one-hot encoding \n  embarked_dummies = pd.get_dummies(df.Embarked, prefix='Embarked')\n  df = pd.concat([df, embarked_dummies], axis=1)\n\n  # Person's title one-hot encoding \n  title_dummies = pd.get_dummies(df.title, prefix='title')\n  df = pd.concat([df, title_dummies], axis=1)\n\n  # Cabin class one-hot encoding \n  cabin_class_dummies = pd.get_dummies(df.cabin_class, prefix = 'cabin_class')\n  df = pd.concat([df, cabin_class_dummies], axis = 1)\n\n  #Remove unecessary columns \n  del df['Name']\n  del df['PassengerId']\n  del df['title']\n  del df['Embarked']\n  del df['Cabin']\n  del df['Ticket']\n  del df['cabin_class']\n\n  return df ","d1385a5c":"# Preprocess the data and create the train \/ test sets \nfull_df = data_preprocessing(full_df)\nX_train = full_df[:891]\ny_train = full_df['Survived'][:891]\nX_test = full_df[891:]\ndel X_train['Survived']\ndel X_test['Survived']\n\n\nprint(f'After preprocessing there are {X_train.shape[0]} rows and {X_train.shape[1]} columns in the training data.\\n')\nprint(f'After preprocessing there are {X_test.shape[0]} rows and {X_test.shape[1]} columns in the test data.')","aadd0bb4":"# Since we are not going to be using any fancy modules for our algorithm \n# we will be converting our data from pandas dataframe to python lists. \nX_train = X_train.values.tolist()\nX_test = X_test.values.tolist()","f68290cc":"class KNearestNeighbors():\n    def __init__(self,k):\n        self.k = k \n        self.X_train = None  \n        self.classes = []\n    \n    @staticmethod \n    def euclidean_distance(row1,row2):\n      sum = 0\n      for i,j in zip(row1,row2):\n        sum += (i-j) ** 2 \n      distance = sum ** 0.5 \n      return distance\n\n\n    def _predict(self,classes,distances,k):\n      # Sort in descending order the distances and retrieve the index \n      # which maps to their corresponding class \n      idxs = sorted(range(len(distances)),key = lambda x:distances[x])[:self.k] \n      # Finding the class for each neighbor  \n      neighbors = [self.classes[idx] for idx in idxs]\n      # Choosing the most ocurring class \n      prediction = Counter(neighbors).most_common(1) # bit of a cheat here :)\n      return prediction[0][0]\n\n    def fit(self,X_train,y_train):\n        self.X_train = X_train\n        predictions = []\n        # For each passenger in the training dataset:\n        for i in range(len(X_train)):\n            distances = []\n            classes = []\n            # Estimate the Euclidean distance for the current \n            # passenger with all the other passengers \n            for x,y in zip(X_train,y_train):\n                distances.append(self.euclidean_distance(X_train[i],x))\n                # Append the class that corresponds to the current passenger\n                self.classes.append(y)\n            # Predict the class for the current passenger \n            prediction = self._predict(self.classes,distances,self.k)\n            predictions.append(prediction)\n        print(f'Utilizing {self.k} groups of nearest-neighbors the training accuracy is at {round(accuracy_score(predictions,y_train)*100,2)}%.')\n        return predictions   \n\n    def predict(self,X_test):\n        # Predictions for the test data \n        predictions = []\n        for i in range(len(X_test)):\n            distances = []\n            for x in self.X_train:\n                distances.append(self.euclidean_distance(X_test[i],x))\n            prediction = self._predict(self.classes,distances,self.k)\n            predictions.append(prediction)\n        return predictions ","3f0ac9da":"# Choosing an arbritrary number for k to be 4 \nKNN = KNearestNeighbors(k=4)\nKNN.fit(X_train,y_train)\npredictions = KNN.predict(X_test)","6d428ef2":"from sklearn.neighbors import KNeighborsClassifier\nneigh = KNeighborsClassifier(n_neighbors=4)\nneigh.fit(X_train, y_train)\ntraining_predictions = neigh.predict(X_train)\nprint(f'Utilizing 4 groups of nearest-neighbors the training accuracy is at {round(accuracy_score(training_predictions,y_train)*100,2)}%')\npredictions = neigh.predict(X_test)","081f13ff":"### **For comparison purposes let's try the same algorithm from scikit learn using the same dataset.**","79d73372":"### **It seems that our implementation of the KNN algorithm performs well and provides good accuracy. I have also made submissions**\n### **to the competition with this implementation, and using the sklearn algorithm, and it seems that our algorithm performs a**\n### **bit better which is another fun bonus \ud83d\ude0a**","f5f2c895":"### **This notebook featueres a very simple implementation of the K-Nearest Neighbors using the titanic dataset.**\n### **Data-preprocessing is taken straight out this  [notebook](https:\/\/www.kaggle.com\/christodoulos\/titanic-top-1-w-simple-ensemble-learning) which it achieves a very high score in the competition.**"}}