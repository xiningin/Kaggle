{"cell_type":{"7236f47c":"code","2ed790a7":"code","6d4dfd36":"code","963060d5":"code","307d717b":"code","509b762d":"code","60c7d476":"code","6da307c1":"code","a06da385":"code","277a8338":"code","0f392a12":"code","c6ca05b7":"code","12aaf017":"code","27b503e4":"code","3c6faf02":"code","02dfa3b6":"code","60e9f060":"code","4b50b0c1":"code","91c91483":"code","c0c01883":"code","22a8aed8":"code","05efa83a":"code","6832dae6":"code","d7bbb8a8":"code","af27fcb6":"code","6bcff054":"code","07cc1d2d":"code","d9d00fb0":"code","69261202":"code","17eeb85c":"code","e70a57f8":"code","7e2247b5":"code","3a887851":"code","f2ab5a94":"code","1d15e052":"code","087c37a1":"code","c25d15f1":"code","5649fcc2":"code","bd2ee39d":"code","2e128c04":"code","a1d4ea5e":"code","6070ca08":"code","4868ebe2":"code","a8610e7d":"code","116ea3f9":"code","ff9cd4e0":"code","149ea498":"code","8d131dc5":"code","659c1e32":"code","3138c123":"markdown","ad1b896f":"markdown","7b7e50d4":"markdown","6fbc9eda":"markdown","c4c4831e":"markdown","c6855bae":"markdown","a2b3e20d":"markdown","7c7ea9dd":"markdown","0a6a04fe":"markdown","cb6cf715":"markdown","cd81f27c":"markdown","b9edff12":"markdown","d680d586":"markdown","586bd1d0":"markdown","9d280f6e":"markdown","c0712e5c":"markdown","f92b4ab3":"markdown","c8a64727":"markdown","342d8300":"markdown","d1962f5d":"markdown","4b61d737":"markdown"},"source":{"7236f47c":"import pandas as pd\nimport numpy as np","2ed790a7":"movie = pd.read_csv(\"..\/input\/netflix-shows\/netflix_titles.csv\")","6d4dfd36":"print(movie.shape)\nmovie.head(10)","963060d5":"movie.isnull().sum()","307d717b":"import seaborn as sns\nimport matplotlib.pyplot as plt","509b762d":"# Palette\ncolour = ['#221f1f', '#b20710', '#e50914','#f5f5f1']\nsns.palplot(colour)\nax = plt.gca()\n\nplt.title(\"Netflix brand palette\")\nfor i, name in enumerate(colour):\n    ax.text(i-0.45,1, name) \nplt.show()","60c7d476":"movie_type = movie.groupby('type').show_id.count()\nplt.bar(x = movie_type.index, height=movie_type.values, color=colour)","6da307c1":"plt.figure(figsize=(12,15))\nsns.countplot(y=\"release_year\", data=movie, palette=colour, order=movie['release_year'].value_counts().index)\nplt.text(-2,-2,\"Which year has the most movies\/TV shows be released?\", family='sans-serif', fontsize=15, weight='bold')","a06da385":"movie['date_added'] = pd.to_datetime(movie['date_added'])\nmovie['date_added_year'] = movie['date_added'].dt.year\nmovie['date_added_month'] = movie['date_added'].dt.month","277a8338":"sns.heatmap(movie.loc[movie['date_added_year'].fillna(0).astype(int).between(2010,2020)].\n            pivot_table(values='show_id',index='date_added_year',columns='date_added_month',aggfunc='count'),\n           cmap='Reds')\nplt.text(-2,-2,\"Which month has the most movies\/TV shows be added?\", family='sans-serif', fontsize=15, weight='bold')","0f392a12":"pip install plotly==4.14.3","c6ca05b7":"import plotly.graph_objects as go","12aaf017":"import plotly.express as px","27b503e4":"country = movie.groupby('country').show_id.count().reset_index()\ncountry = country.rename(columns={'show_id':'no of movies'})\nfig = px.treemap(country, path=['country','no of movies'], values='no of movies',\n                title='Distribution by country')\nfig.show()","3c6faf02":"import itertools as it","02dfa3b6":"pip install wordcloud","60e9f060":"import nltk\nfrom wordcloud import WordCloud","4b50b0c1":"genre = movie['listed_in'].tolist()","91c91483":"cloud = WordCloud(background_color = 'white',colormap='Reds',max_words = 120).generate(''.join(list(it.chain(*genre))))","c0c01883":"plt.figure(figsize=(12,12))\nplt.imshow(cloud)\nplt.axis('off')\nplt.text(-1,-10,\"Most frequent keywords in attribute\", family='sans-serif', fontsize=20, weight='bold')","22a8aed8":"from PIL import Image","05efa83a":"mask = np.array(Image.open('..\/input\/netflix-icon\/453-4532423_icon-netflix-logo-png-transparent-png.png'))\ncloud2 = WordCloud(background_color='white',colormap='Reds',max_words=150, mask=mask).generate(''.join(movie['title'].tolist()))","6832dae6":"plt.figure(figsize=(12,12))\nplt.imshow(cloud2)\nplt.axis('off')\nplt.text(-1,-10,\"Most common words used in movies\/TV shows' titles\", family='sans-serif', fontsize=20, weight='bold')","d7bbb8a8":"import nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer","af27fcb6":"#Replace NaN with an empty string, since the converter cannot deal with integer\/float\nmovie['description'] = movie['description'].fillna('')","6bcff054":"#removing stopwords\ntfidf = TfidfVectorizer(stop_words='english')\n\n#get the tf-idf scores\n#create TF-IDF matrix by fitting and transforming the data\ntfidf_matrix = tfidf.fit_transform(movie['description'])\n\n#shape of tfidf_matrix\ntfidf_matrix.shape","07cc1d2d":"pd.DataFrame(tfidf_matrix[0].T.todense(), index=tfidf.get_feature_names(), columns=[\"tf-idf score\"])","d9d00fb0":"#CountVectorizer: Convert a collection of text documents to a matrix of token counts\n#TfidfTransformer: Transform a count matrix to a normalized tf or tf-idf representation\n\nfrom sklearn.pipeline import Pipeline\n\npipe = Pipeline([('cv', CountVectorizer(stop_words='english')),\n                ('tfid', TfidfTransformer())]).fit(movie['description'])\n\nprint(\"CountVectorizer_matrix: {}\".format(pipe['cv'].transform(movie['description']).toarray()))\nprint(\"CountVectorizer_matrix: {}\".format(pipe['tfid'].idf_))","69261202":"pipe['tfid'].idf_.shape","17eeb85c":"#The lower the IDF value of a word, the less unique it is to any particular document\npd.DataFrame(pipe['tfid'].idf_, index=pipe['cv'].get_feature_names(), columns=['idf_weights'])","e70a57f8":"from sklearn.metrics.pairwise import cosine_similarity\ncosine_sim = cosine_similarity(tfidf_matrix)","7e2247b5":"cosine_sim","3a887851":"from sklearn.metrics.pairwise import linear_kernel\ncosine_sim1 = linear_kernel(tfidf_matrix, tfidf_matrix)","f2ab5a94":"cosine_sim1","1d15e052":"programme_list=movie['title'].to_list()","087c37a1":"def recommend(title, cosine_similarity=cosine_sim):\n    index = programme_list.index(title)\n    sim_score = list(enumerate(cosine_sim[index]))\n    sim_score = sorted(sim_score, key= lambda x: x[1], reverse=True)[1:11] #position 0 is the movie itself, thus exclude\n    recommend_index = [i[0] for i in sim_score]\n    rec_movie = movie['title'].iloc[recommend_index]\n    rec_score = [round(i[1],4) for i in sim_score]\n    rec_table = pd.DataFrame(list(zip(rec_movie,rec_score)), columns=['Recommend programme','Similarity(0-1)'])\n    return rec_table","c25d15f1":"recommend('Avengers: Infinity War')","5649fcc2":"recommend('Why Are We Getting So Fat?')","bd2ee39d":"recommend('About Time')","2e128c04":"recommend('\u5fcd\u8005\u30cf\u30c3\u30c8\u30ea\u304f\u3093')","a1d4ea5e":"recommend(\"Indiana Jones and the Last Crusade\")","6070ca08":"movie.loc[movie['director']=='Steven Spielberg']","4868ebe2":"#select programme type,country,genres,director & description\n#join all the info together\n#convert all words into lowercase so that algorithm will not distinguish as independent words\ndef bag_of_words(df):\n    features = df['type']+' '+df['country']+' '+df['listed_in']+' '+df['director']+' '+df['description']\n    return features.fillna('').str.lower()","a8610e7d":"features = bag_of_words(movie)","116ea3f9":"features.head(10)","ff9cd4e0":"tfidf_matrix2 = tfidf.fit_transform(features)                \ntfidf_matrix2.shape","149ea498":"cosine_sim2 = cosine_similarity(tfidf_matrix2)","8d131dc5":"def recommend2(title, cosine_similarity=cosine_sim2):\n    index = programme_list.index(title)\n    sim_score = list(enumerate(cosine_sim2[index]))\n    sim_score = sorted(sim_score, key= lambda x: x[1], reverse=True)[1:11] #position 0 is the movie itself, thus exclude\n    recommend_index = [i[0] for i in sim_score]\n    rec_movie = movie['title'].iloc[recommend_index]\n    rec_score = [round(i[1],4) for i in sim_score]\n    rec_table = pd.DataFrame(list(zip(rec_movie,rec_score)), columns=['Recommend programme','Similarity(0-1)'])\n    return rec_table","659c1e32":"recommend2(\"Indiana Jones and the Last Crusade\")","3138c123":"The first recommended programme of \"Why Are We Getting So Fat?\" is \"Forks Over Knives\", which is also health-related and emphasing the importance of what we had eaten.","ad1b896f":"# Limitation of TF-IDF\nAs the ordering of the sentence is discarded.\n- compute similarity directly in the word-count space, which may be slow for large vocabularies\n- assume the counts of different words are independent\n- synonyms between words are ignored, eg trip and journey","7b7e50d4":"\"Ninja Hattori\" is actually the english name of \"\u5fcd\u8005\u30cf\u30c3\u30c8\u30ea\u304f\u3093\", so the similarity score is far more than others, and on the top of recommedation list, ie recommend itself again.\n\nThis fault is attributed by the sources of dataset, instead of the algorithm. We need to beware of the dataset whether there are duplicates that showcased in different language in our training set.","6fbc9eda":"# Problem of movie series","c4c4831e":"# Bag-of-Words Model\nWhen analyzing large amounts of natural language data(NLP), text must be converted into numbers, in form of vectors\/matrix, as machine cannot work with raw text directly.\n\nA popular and simple method is called the bag-of-words model.\n\nWhy it is named as \u201cbag\u201d of words is because all words in the document are disordered. The model only counts the occurance, and the positions are not considered.\n\nA problem with bag-of-words model is that words with high frequency will dominate and get larger score), but higher frequency doesn't mean that word is informative, eg is\/are\/an\/the.\n\nTerm Frequency \u2013 Inverse Document Frequency(TF-IDF) is introduced to \"penalize\" those highly frequent words. Term frequency and inverse document frequency refer to the frequency of the word and how rare the word is across documents respectively by giving a score, help highlighting distinct words to distinguish the difference. ","c6855bae":"As above recommendation engine is constructed based on description only, it probably wouldn't suggest the movies in series, but movies with more similar key words.\n\nLet's take \"Indiana Jones\" series as example, its serial movie aren't the top recommendation.","a2b3e20d":"# Tfidftransformer","7c7ea9dd":"# Tfidfvectorizer","0a6a04fe":"Three way to calculate similiarity\n- Euclidean distance\n- Pearson correlation\n- Cosine similiarity\n\nHere cosine similarity is used. It measures the cosine of the angle between two vectors projected in a multi-dimensional space to find out how similar the documents are irrespective of their size. The smaller the angle, higher the cosine similarity.\n\nIt is advantageous because even if the two similar documents are far apart by the Euclidean distance (due to the size of the document).","cb6cf715":"# Recommendation engine by description","cd81f27c":"# Netflix recommendation engine\nData source:\nhttps:\/\/www.kaggle.com\/shivamb\/netflix-shows\n\nSupplementary info, check out in Tableau: https:\/\/public.tableau.com\/views\/Netflix_16175273771650\/Dashboard1?:language=en-GB&:display_count=y&publish=yes&:origin=viz_share_link","b9edff12":"scikit-learn also provides pairwise metrics (aka kernels in machine learning parlance) that work for both dense and sparse representations of vector collections. In calculating similarity, a dot product is also needed (aka linear kernel).\n\nOn L2-normalized data, linear_kernel is equivalent to cosine similarity.","d680d586":"# Recommendation engine by 5 attributes","586bd1d0":"Take Indiana Jones's series as example again, new recommendation engine now suggest its serial movies in the highest ranking. \n\nAlso, Steven Spielberg's movies, such as \"Schindler's List\", \"Lincoln\", \"The Adventures of Tintin Hook\" are also on the list. \"Indiana Jones and the Last Crusade\" is about searching Holy Grail, in which story plot is similar to \"Monty Python and the Holy Grail\", so has a higher ranking.","9d280f6e":"![image.png](attachment:image.png)","c0712e5c":"Sometimes, we watch movies not only based on genres, but the directors' filming style.\n\nSteven Spielberg is a famous director and he has 12 movies in this dataset, including Indiana Jones' series, so movies filmed by him should also on the list.","f92b4ab3":"# Calculate cosine similarity by sklearn.pairwise.package","c8a64727":"# Tfidftransformer vs. Tfidfvectorizer\n\nBoth can convert the text of raw documents into a matrix of TF-IDF.\n\nTfidftransformer: First compute word counts by CountVectorizer, then Tfidftransformer compute IDF values and Tf-idf scores.\nTfidfvectorizer: compute at once\n\nUse Tfidftransformer if word count vectors are needed for different tasks.","342d8300":"# Cosine similarity","d1962f5d":"# Content-based filtering VS collaborative filtering\n- Content-based filtering:\n    - recommend other similar items by item features \n    - does not require other users' data during recommendations to one user.\n- Collaborative filtering:\n    - mimics user-to-user recommendations\n    - does not need the item features","4b61d737":"\"About Time\" is a British romantic comedy-drama film about a man with the ability of time travel.\n\nFrom the recommendation list, some romantic programmes are suggested, while programmes with description containing abverbs of time are also listed, eg \"Tomorrow with You\", \"Running Out Of Time\", \"Second 20s\" that \"Running Out Of Time\" is a Hong Kong action thriller film, rather than romantic firm."}}