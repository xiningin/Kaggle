{"cell_type":{"31269c43":"code","fcf5860d":"code","585822e3":"code","83bcf595":"code","0ae6395e":"code","498f4bc7":"code","6a80e0c1":"code","dfb71444":"code","04cd4426":"code","b960dcd9":"code","3b885154":"code","84925cc4":"code","e7224950":"code","95e34c14":"code","28cc6384":"code","47d1d3a9":"code","920256da":"code","4855bae5":"code","ab38f508":"code","6c8d70de":"code","504bf994":"code","f1b6dbe3":"code","4109ca1e":"code","1e217f91":"code","2655b2a2":"code","4bcbcf0e":"code","a7461785":"code","6d3c9d9b":"code","bd794677":"code","29b2ca76":"code","3bfb4ad8":"code","541ac945":"code","15a964c3":"code","aaf713f1":"code","246ec9dd":"code","e2769dbd":"code","cf3a18e9":"code","a96b4228":"code","af9a9cd0":"code","946fd891":"code","b6dbf1cf":"code","3d0eab9a":"code","66136300":"code","3535e792":"code","e3e77014":"code","312d227c":"code","9c182ae5":"code","c83a849b":"code","5e4d3230":"code","c8cea9a4":"code","37898c86":"code","029d3fbf":"code","13ff5d5f":"code","18a5c7e3":"code","787a2c57":"code","27873956":"code","7c11acfd":"code","5dfca1be":"code","fe539282":"code","492c5c5e":"code","3c68bcb4":"code","7ca8e311":"code","b4f765d9":"code","94e6d8bd":"code","eee06490":"code","332b827e":"code","b99f9ce6":"code","f02aa683":"code","826c5682":"code","d3ac8029":"code","0ce27b21":"code","a0644259":"code","7e4ac9e4":"code","508deb24":"code","7cc97cde":"code","57825f62":"code","eddf63c3":"code","10e1b839":"code","f77ab75a":"code","4ae6c189":"code","e3403510":"code","dcac73cd":"code","3abba87a":"code","1309d00c":"code","f7afacbd":"code","c235bf24":"code","25d7243a":"code","a923024e":"code","a63caf31":"code","563e8e91":"code","5132d63a":"code","69551a11":"code","c53dbe49":"code","3975d35d":"code","24cb1978":"code","61659a16":"code","4c0f9188":"code","0e3af477":"code","ef406d53":"code","98349924":"code","c351c401":"code","3494d039":"markdown","1f430a80":"markdown","00e9367d":"markdown","4fda1c4f":"markdown","a857e841":"markdown","76c19a52":"markdown","e2225886":"markdown","6cbfa33b":"markdown","97516272":"markdown","f655df56":"markdown","0808f77c":"markdown","2c7baeb3":"markdown","6d04abd2":"markdown","f614593a":"markdown","604fbf8a":"markdown","072cfe6f":"markdown","adfeba7c":"markdown","4622fe84":"markdown","6ed08de1":"markdown","5a2088d0":"markdown","727247f2":"markdown","320cdb8b":"markdown","98369de6":"markdown","b96816c8":"markdown","3815a4ee":"markdown","5875dfe1":"markdown","39ca16b8":"markdown","8aa6bb5b":"markdown","8ff2b534":"markdown","57a96bf5":"markdown","558b27e0":"markdown","13d05840":"markdown","75fd1d92":"markdown","40d85147":"markdown","d4752dde":"markdown","032b513f":"markdown","d388318e":"markdown","f11f30a7":"markdown","55a1f64a":"markdown","c4fdca6e":"markdown","25793d70":"markdown","d9884a72":"markdown","9e8ad82c":"markdown","8f222aff":"markdown","3a92b7c9":"markdown","3eff2ce3":"markdown","e0591a13":"markdown","b8985781":"markdown","7cc13fdb":"markdown","97da35d1":"markdown","35c9657b":"markdown","3d021801":"markdown","ed535355":"markdown","18c16891":"markdown","a3a39e68":"markdown","bdb333b5":"markdown","11887d0b":"markdown","db697af9":"markdown","60781cab":"markdown","5780b9b7":"markdown","45ded19d":"markdown","3b41ca7a":"markdown","980f06ef":"markdown","c278c8d9":"markdown","3801854e":"markdown","05cbb7bd":"markdown","418d447d":"markdown","8e76b8ad":"markdown","b6c39429":"markdown","d7943c37":"markdown","3f79918e":"markdown","c218c243":"markdown","c7602ac2":"markdown","f3a7aabc":"markdown","b71b58fd":"markdown"},"source":{"31269c43":"# importing  libraries for general use\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","fcf5860d":"# to help with visualization\n\nfrom matplotlib.pyplot import xticks\n%matplotlib inline","585822e3":"# libraries for machine learning\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import IncrementalPCA\nfrom sklearn.neighbors import NearestNeighbors\nfrom random import sample\nfrom numpy.random import uniform\nfrom math import isnan","83bcf595":"# To perform Hierarchical clustering\n\nfrom sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\nfrom scipy.cluster.hierarchy import linkage, cut_tree, dendrogram\nfrom sklearn.metrics import silhouette_score","0ae6395e":"# Data display coustomization\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_colwidth', -1)","498f4bc7":"# ignore warnings\n\nimport warnings\nwarnings.filterwarnings('ignore')","6a80e0c1":"pip install xlrd ","dfb71444":"# importing the data using .read_excel\n\ndf = pd.read_excel('..\/input\/east-west-airlines\/EastWestAirlines.xls', sheet_name =\"data\")","04cd4426":"# first 5 rows\n\ndf.head(5)","b960dcd9":"# the dimensions of the dataframe\n\ndf.shape","3b885154":"# info on data types & null count\n\n# no null values seem to exist.\n\ndf.info()","84925cc4":"# statistical info for every feature\n\ndf.describe()","e7224950":"# different cc_miles have different max values.\n# so, we want to check what values these columns can take\n\nunique_cc1 = df.cc1_miles.unique()\nunique_cc2 = df.cc2_miles.unique()\nunique_cc3 = df.cc3_miles.unique()\n\nprint(unique_cc1)\nprint(unique_cc2)\nprint(unique_cc3)","95e34c14":"# null count for columns\n\nnull_count_col = df.isnull().sum().value_counts(ascending=False)\n\n# null percentage for columns\n\nnull_percent_col = (df.isnull().sum() * 100 \/ len(df)).value_counts(ascending=False)\n\nprint(\"Null Count for Columns:\\n\\n\", null_count_col, \"\\n\")\nprint(\"Null Percentage for Columns:\\n\\n\", null_percent_col)","28cc6384":"# null count for rows\n\nnull_count_row = df.isnull().sum(axis=1).value_counts(ascending=False)\n\n# null percentage for rows\n\nnull_percent_row = (df.isnull().sum(axis=1) * 100 \/ len(df)).value_counts(ascending=False)\n\nprint(\"Null Count for Rows:\\n\\n\", null_count_row, \"\\n\")\nprint(\"Null Percentage for Rows:\\n\\n\", null_percent_row)","47d1d3a9":"# Balance : Number of miles eligible for award travel\n\nplt.figure(figsize = (5,5))\nBalance = df[['Award','Balance']].sort_values('Balance', ascending = False)\nax = sns.barplot(x='Award', y='Balance', data= Balance)\nax.set(xlabel = 'Award ?', ylabel= 'Balance')\nplt.xticks(rotation=90)\nplt.show()","920256da":"# what is correlated with Balance?\n\ncorr_matrix = df.corr()\ncorr_matrix[\"Balance\"].sort_values(ascending=False)","4855bae5":"# correlation heatmap\n\nf,ax = plt.subplots(figsize=(18,18))\nsns.heatmap(df.corr(), annot=True, linewidths =.5, fmt ='.1f',ax=ax)\nplt.show()","ab38f508":"# Plotting frequent flying bonuses vs. non-flight bonus transactions \nplt.figure(figsize = (10,10))\nsorted_data = df[['cc1_miles','Bonus_trans']].sort_values('Bonus_trans', ascending = False)\nax = sns.barplot(x='cc1_miles', y='Bonus_trans', data= sorted_data)\nax.set(xlabel = 'Miles earned with freq. flyer credit card', ylabel= 'Non-flight bonus transactions')\nplt.xticks(rotation=90)\nplt.show()","6c8d70de":"  # Box Plot for every feature, singled out\n    \nfor n in df.columns:\n    print(n)\n    sns.boxplot(df[n])\n    plt.show()","504bf994":"# Box plot for every feature in the same graph\n\nplt.figure(figsize=(12,8))\nsns.boxplot(data=df)","f1b6dbe3":"# we use sqrt() to see more clearly despite the outliers\n\nplt.figure(figsize=(12,8))\nsns.boxplot(data=np.sqrt(df))","4109ca1e":"df.head(100)","1e217f91":"# removing the outliers from Balance\n\nq1 = df['Balance'].quantile(0.25)\nq3 = df['Balance'].quantile(0.75)\niqr = q3-q1\nul = q3 + (1.5*iqr)\nll = q1 - (1.5*iqr)\ndf1 = df[(df['Balance']>ll)&(df['Balance']<ul)]\n\ndf1.head(100)","2655b2a2":"plt.figure(figsize=(12,8))\nsns.boxplot(data=df1)","4bcbcf0e":"df_shape = df.shape\n\ndf1_shape = df1.shape\n\nprint(\"shape of original dataframe:\", df_shape, \"\\n\")\nprint(\"shape of new dataframe:\", df1_shape)","a7461785":"# removing outliers from Bonus_miles:\n\nq1 = df['Bonus_miles'].quantile(0.25)\nq3 = df['Bonus_miles'].quantile(0.75)\niqr = q3-q1\nul = q3 + (1.5*iqr)\nll = q1 - (1.5*iqr)\ndf2 = df1[(df1['Bonus_miles']>ll)&(df1['Bonus_miles']<ul)]","6d3c9d9b":"plt.figure(figsize=(12,8))\nsns.boxplot(data=df2)","bd794677":"# Removing outliers from Flight_miles_12mo\n\nq1 = df['Flight_miles_12mo'].quantile(0.25)\nq3 = df['Flight_miles_12mo'].quantile(0.75)\niqr = q3-q1\nul = q3 + (1.5*iqr)\nll = q1 - (1.5*iqr)\ndf3 = df2[(df2['Flight_miles_12mo']>ll)&(df2['Flight_miles_12mo']<ul)]","29b2ca76":"plt.figure(figsize=(12,8))\nsns.boxplot(data=df3)","3bfb4ad8":"q1 = df['Qual_miles'].quantile(0.25)\nq3 = df['Qual_miles'].quantile(0.75)\niqr = q3-q1\nul = q3 + (1.5*iqr)\nll = q1 - (1.5*iqr)\ndf4 = df3[(df3['Qual_miles']>ll)&(df3['Qual_miles']<ul)]","541ac945":"plt.figure(figsize=(12,8))\nsns.boxplot(data=df4)","15a964c3":"sns.boxplot(df3['Qual_miles'])","aaf713f1":"df3.head()","246ec9dd":"# we know ID & award will not make much contribution during clutering. we will drop both columns.\n\ndataset1 =  df3.drop(['ID','Award'], axis=1)\ndataset1.head()","e2769dbd":"# our final boxgraphs with ID & Award removed\n\nplt.figure(figsize=(12,8))\nsns.boxplot(data=dataset1)","cf3a18e9":"  # Kernel Density for every feature, singled out\n\nfor n in dataset1.columns:\n    print(n)\n    sns.kdeplot(df[n])\n    plt.show()","a96b4228":"standard_scaler = StandardScaler()\ndf_norm = standard_scaler.fit_transform(dataset1)\ndf_norm.shape","af9a9cd0":"cluster_range = range(1,15)\ncluster_errors = []\nfor num_clusters in cluster_range:\n    clusters = KMeans(num_clusters,n_init=10)\n    clusters.fit(df_norm)\n    labels = clusters.labels_\n    centroids = clusters.cluster_centers_\n    cluster_errors.append(clusters.inertia_)\nclusters_df = pd.DataFrame({\"num_clusters\":cluster_range,\"cluster_errors\":cluster_errors})","946fd891":"clusters_df[0:20]","b6dbf1cf":"# \"Elbow\" plot, clusters vs. errors\nplt.figure(figsize=(10,8))\nplt.plot(clusters_df['num_clusters'],clusters_df['cluster_errors'],marker='o')\nplt.xlabel('Num clusters')\nplt.ylabel('Cluster Errors')","3d0eab9a":"# applying PCA on std_df\n\n# we consider 0.95 variance in n_components to not lose any data.\n\nfrom sklearn.decomposition import PCA\n\npca_std = PCA(random_state=10, n_components=0.95)\npca_std_df = pca_std.fit_transform(df_norm)","66136300":"# eigenvalues\n\nprint(pca_std.singular_values_)","3535e792":"#  variance contained in each formed PCA\n\nprint(pca_std.explained_variance_ratio_*100)","e3e77014":"# Cummulative variance ratio\n\ncum_variance = np.cumsum(pca_std.explained_variance_ratio_*100)\ncum_variance","312d227c":"# Using Minmaxscaler for accuracy result comparison\n\nfrom sklearn.preprocessing import MinMaxScaler\nminmax = MinMaxScaler()\n\nminmax_df = minmax.fit_transform(dataset1)\nminmax_df.shape","9c182ae5":"# applying PCA on minmax_df\n\nfrom sklearn.decomposition import PCA\n\npca_minmax =  PCA(random_state=10, n_components=0.95)\npca_minmax_df = pca_minmax.fit_transform(minmax_df)","c83a849b":"# eigenvalues\n\nprint(pca_minmax.singular_values_)","5e4d3230":"# variance containing in each formed PCA\n\nprint(pca_minmax.explained_variance_ratio_*100)","c8cea9a4":"pip install yellowbrick","37898c86":"# With the elbow method, the ideal number of clusters to use was 6.\n# We will also use the Silhouette score to determine an optimal number.\n\n#Import the KElbowVisualizer method\nfrom sklearn.cluster import KMeans\nfrom yellowbrick.cluster import KElbowVisualizer\nfrom sklearn.metrics import silhouette_score\n\nclust_list = [2,3,4,5,6,7,8,9]\n\n#  Silhouette score for stadardized data with PCA applied.\n\nfor n_clusters in clust_list:\n    clusterer1 = KMeans(n_clusters=n_clusters, random_state=0,n_jobs=-1)\n    cluster_labels1 = clusterer1.fit_predict(pca_std_df)\n    sil_score1= silhouette_score(pca_std_df, cluster_labels1)\n    print(\"For n_clusters =\", n_clusters,\"The average silhouette_score is :\", sil_score1)","029d3fbf":"# Silhouette score for MinMaxScalar transformation data with PCA Applied.\n\nfor n_clusters in clust_list:\n    clusterer2 = KMeans(n_clusters=n_clusters, random_state=0,n_jobs=-1)\n    cluster_labels2 = clusterer1.fit_predict(pca_minmax_df)\n    sil_score2= silhouette_score(pca_std_df, cluster_labels2)\n    print(\"For n_clusters =\", n_clusters,\"The average silhouette_score is :\", sil_score2)","13ff5d5f":"# Hierarchial Clustering with single linkage:\nplt.figure(figsize=(15,10))\nmergings = linkage(df_norm, method='single',metric='euclidean')\ndendrogram(mergings)\nplt.show()","18a5c7e3":"# Hierarchial Clustering with complete linkage as instructed:\n\nplt.figure(figsize=(15,10))\nmergings = linkage(df_norm, method='complete',metric='euclidean')\ndendrogram(mergings)\nplt.show()","787a2c57":"# Hierarchial Clustering with average linkage:\n\nplt.figure(figsize=(15,10))\nmergings = linkage(df_norm, method='average',metric='euclidean')\ndendrogram(mergings)\nplt.show()","27873956":"# Applying Dendrogram on PCA data using different linkage methods. \n# We can see number of clusters using color coding of dendrogram. Each color indicates one cluster.\n\nimport scipy.cluster.hierarchy as shc\nfor methods in ['single','complete','average','weighted','centroid','median','ward']: \n    plt.figure(figsize =(20, 6)) \n    \n    dict = {'fontsize':24,'fontweight' :16, 'color' : 'blue'}\n    \n    plt.title('Visualization with Method: {}'.format(methods),fontdict = dict) \n    Dendrogram1 = shc.dendrogram(shc.linkage(pca_std_df, method = methods,optimal_ordering=False))","7c11acfd":"agg_clustering = AgglomerativeClustering(n_clusters=6, linkage='average')\ny_pred_hie = agg_clustering.fit_predict(pca_std_df)\nprint(y_pred_hie.shape)\ny_pred_hie","5dfca1be":"print(\"Cluster labels for each point:\", agg_clustering.labels_, \"\\n\")\nprint(\"Number of leaves in the hierarchical tree:\", agg_clustering.n_leaves_, \"\\n\")\nprint(\"The estimated number of connected components in the graph:\", agg_clustering.n_connected_components_, \"\\n\")\nprint(\"The children of each non-leaf node:\\n\", agg_clustering.children_, \"\\n\")\nprint(\"Clustering Score:\", (silhouette_score(pca_std_df, agg_clustering.labels_)*100).round(3))","fe539282":"# Creating dataframe of cluster labels.\n\nhie_cluster = pd.DataFrame(agg_clustering.labels_.copy(), columns=['Hie_Clustering'])","492c5c5e":"# Concating model1_Cluster df with main dataset copy\n\nhie_df = pd.concat([dataset1.copy(), hie_cluster], axis=1)\nhie_df.head()","3c68bcb4":"# Plotting barplot using groupby method to get visualize how many row no. in each cluster\n\nfig, ax = plt.subplots(figsize=(10, 6))\nhie_df.groupby(['Hie_Clustering']).count()['Days_since_enroll'].plot(kind='bar')\nplt.ylabel('Balance Labels')\nplt.title('Hierarchical Clustering (pca_std_df)',fontsize='large',fontweight='bold')\nax.set_xlabel('Clusters', fontsize='large', fontweight='bold')\nax.set_ylabel('Passenger count', fontsize='large', fontweight='bold')\nplt.yticks(fontsize=15)\nplt.xticks(fontsize=15)\nplt.show()","7ca8e311":"hie_df.groupby(['Hie_Clustering']).count()","b4f765d9":"dataset1","94e6d8bd":"dataset1.shape","eee06490":"five_percent = 3105\/20\nfive_percent","332b827e":"np.random.seed(425)\n\nremove_n = 155\ndrop_indices = np.random.choice(dataset1.index, remove_n, replace=False)\ndf_95_percent = dataset1.drop(drop_indices)\n\n# our new dataset with 5% removed\n\ndf_95_percent","b99f9ce6":"df_95_percent.head()","f02aa683":"df_norm2 = standard_scaler.fit_transform(df_95_percent)","826c5682":"from sklearn.cluster import KMeans\ncluster_range = range(1,15)\ncluster_errors = []\nfor num_clusters in cluster_range:\n    clusters = KMeans(num_clusters,n_init=10)\n    clusters.fit(df_norm2)\n    labels = clusters.labels_\n    centroids = clusters.cluster_centers_\n    cluster_errors.append(clusters.inertia_)\nclusters_df = pd.DataFrame({\"num_clusters\":cluster_range,\"cluster_errors\":cluster_errors})","d3ac8029":"#Hierarchial Clustering:\nplt.figure(figsize=(15,10))\nmergings = linkage(df_norm2, method='single',metric='euclidean')\ndendrogram(mergings)\nplt.show()","0ce27b21":"plt.figure(figsize=(15,10))\nmergings = linkage(df_norm2, method='complete',metric='euclidean')\ndendrogram(mergings)\nplt.show()","a0644259":"plt.figure(figsize=(15,10))\nmergings = linkage(df_norm2, method='average',metric='euclidean')\ndendrogram(mergings)\nplt.show()","7e4ac9e4":"# Applying Dendrogram on the random \"5%-removed\" data using different linkage methods. \n# We can see number of clusters using color coding of dendrogram. Each color indicates one cluster.\n\nimport scipy.cluster.hierarchy as shc\nfor methods in ['single','complete','average','weighted','centroid','median','ward']: \n    plt.figure(figsize =(20, 6)) \n    \n    dict = {'fontsize':24,'fontweight' :16, 'color' : 'blue'}\n    \n    plt.title('Visualization with Method: {}'.format(methods),fontdict = dict) \n    Dendrogram1 = shc.dendrogram(shc.linkage(df_norm2, method = methods,optimal_ordering=False))","508deb24":"agg_clustering2 = AgglomerativeClustering(n_clusters=6, linkage='average')\ny_pred_hie = agg_clustering2.fit_predict(df_norm2)\nprint(y_pred_hie.shape)\ny_pred_hie","7cc97cde":"print(\"Cluster labels for each point:\", agg_clustering2.labels_, \"\\n\")\nprint(\"Number of leaves in the hierarchical tree:\", agg_clustering2.n_leaves_, \"\\n\")\nprint(\"The estimated number of connected components in the graph:\", agg_clustering2.n_connected_components_, \"\\n\")\nprint(\"The children of each non-leaf node:\\n\", agg_clustering2.children_, \"\\n\")\nprint(\"Clustering Score:\", (silhouette_score(df_norm2, agg_clustering2.labels_)*100).round(3))","57825f62":"model1 = KMeans(n_clusters = 5, max_iter=50)\nmodel1.fit(pca_std_df)","eddf63c3":"# analysis of clusters formed\n\ndataset1.index = pd.RangeIndex(len(dataset1.index))\ndf_km = pd.concat([dataset1,pd.Series(model1.labels_)],axis=1)\ndf_km.columns = [\"Balance\", \"Qual_miles\", \"cc1_miles\", \"cc2_miles\", \"cc3_miles\", \"Bonus_miles\", \"Bonus_trans\", \"Flight_miles_12mo\", \"Flight_trans_12\", \"Days_since_enroll\", \"ClusterID\"]   ","10e1b839":"df_km.isna().sum()","f77ab75a":"df_km","4ae6c189":"dataset1.head()","e3403510":"km_cluster_Balance = pd.DataFrame(df_km.groupby('ClusterID')['Balance'].mean())\nkm_cluster_Qual_miles = pd.DataFrame(df_km.groupby('ClusterID')['Qual_miles'].mean())\nkm_cluster_cc1_miles = pd.DataFrame(df_km.groupby('ClusterID')['cc1_miles'].mean())\nkm_cluster_cc2_miles = pd.DataFrame(df_km.groupby('ClusterID')['cc2_miles'].mean())\nkm_cluster_cc3_miles = pd.DataFrame(df_km.groupby('ClusterID')['cc3_miles'].mean())\nkm_cluster_Bonus_miles = pd.DataFrame(df_km.groupby('ClusterID')['Bonus_miles'].mean())\nkm_cluster_Bonus_trans = pd.DataFrame(df_km.groupby('ClusterID')['Bonus_trans'].mean())\nkm_cluster_Flight_miles_12mo = pd.DataFrame(df_km.groupby('ClusterID')['Flight_miles_12mo'].mean())\nkm_cluster_Flight_trans_12 = pd.DataFrame(df_km.groupby('ClusterID')['Flight_trans_12'].mean())\nkm_cluster_Days_since_enroll = pd.DataFrame(df_km.groupby('ClusterID')['Days_since_enroll'].mean())\n\n\ndf = pd.concat([pd.Series([1,2,3,4,5]),\nkm_cluster_Balance,\nkm_cluster_Qual_miles,\nkm_cluster_cc1_miles,\nkm_cluster_cc2_miles,\nkm_cluster_cc3_miles,\nkm_cluster_Bonus_miles,\nkm_cluster_Bonus_trans,\nkm_cluster_Flight_miles_12mo,\nkm_cluster_Flight_trans_12,\nkm_cluster_Days_since_enroll],axis=1)\ndf.columns = [\"ClusterID\",\"Balance\", \"Qual_miles\", \"cc1_miles\", \"cc2_miles\", \"cc3_miles\", \"Bonus_miles\",  \"Bonus_trans\", \"Flight_miles_12mo\", \"Flight_trans_12\", \"Days_since_enroll\"]\ndf","dcac73cd":"sns.barplot(data=df,x='ClusterID',y='Balance')","3abba87a":"sns.barplot(data=df,x='ClusterID',y='Qual_miles')","1309d00c":"sns.barplot(data=df,x='ClusterID',y='cc1_miles')","f7afacbd":"sns.barplot(data=df,x='ClusterID',y='cc2_miles')","c235bf24":"sns.barplot(data=df,x='ClusterID',y='cc3_miles')","25d7243a":"sns.barplot(data=df,x='ClusterID',y='Bonus_miles')","a923024e":"sns.barplot(data=df,x='ClusterID',y='Bonus_trans')","a63caf31":"sns.barplot(data=df,x='ClusterID',y='Flight_miles_12mo')","563e8e91":"sns.barplot(data=df,x='ClusterID',y='Flight_trans_12')","5132d63a":"sns.barplot(data=df,x='ClusterID',y='Days_since_enroll')","69551a11":"df.columns","c53dbe49":"model1_cluster = pd.DataFrame(model1.labels_.copy(), columns=['Kmeans_Clustering'])","3975d35d":"Kmeans_df = pd.concat([dataset1.copy(), model1_cluster], axis=1)\nKmeans_df.head()","24cb1978":"fig, ax = plt.subplots(figsize=(10, 6))\nKmeans_df.groupby(['Kmeans_Clustering']).count()['Days_since_enroll'].plot(kind='bar')\nplt.ylabel('Days_since_enroll')\nplt.title('Kmeans Clustering (df_km)',fontsize='large',fontweight='bold')\nax.set_xlabel('Clusters', fontsize='large', fontweight='bold')\nax.set_ylabel('Days_since_enroll', fontsize='large', fontweight='bold')\nplt.yticks(fontsize=15)\nplt.xticks(fontsize=15)\nplt.show()","61659a16":"# Comparing this with our Hierarchial Clustering Bar plot:\n\nfig, ax = plt.subplots(figsize=(10, 6))\nhie_df.groupby(['Hie_Clustering']).count()['Days_since_enroll'].plot(kind='bar')\nplt.ylabel('Balance Labels')\nplt.title('Hierarchical Clustering (pca_std_df)',fontsize='large',fontweight='bold')\nax.set_xlabel('Clusters', fontsize='large', fontweight='bold')\nax.set_ylabel('Passenger count', fontsize='large', fontweight='bold')\nplt.yticks(fontsize=15)\nplt.xticks(fontsize=15)\nplt.show()","4c0f9188":"# what are clusters' strong points?","0e3af477":"fig, ax = plt.subplots(figsize=(10, 6))\nKmeans_df.groupby(['Kmeans_Clustering']).count()['Days_since_enroll'].plot(kind='bar')\nplt.ylabel('Days_since_enroll')\nplt.title('Kmeans Clustering (df_km)',fontsize='large',fontweight='bold')\nax.set_xlabel('Clusters', fontsize='large', fontweight='bold')\nax.set_ylabel('Days_since_enroll', fontsize='large', fontweight='bold')\nplt.yticks(fontsize=15)\nplt.xticks(fontsize=15)\nplt.show()","ef406d53":"# Sorting elements based on cluster label assigned and taking average for insights.\n\ncluster1 = pd.DataFrame(Kmeans_df.loc[Kmeans_df.Kmeans_Clustering==0].mean(),columns= ['Cluster_1_avg'])\ncluster2 = pd.DataFrame(Kmeans_df.loc[Kmeans_df.Kmeans_Clustering==1].mean(),columns= ['Cluster_2_avg'])\ncluster3 = pd.DataFrame(Kmeans_df.loc[Kmeans_df.Kmeans_Clustering==2].mean(),columns= ['Cluster_3_avg'])\ncluster4 = pd.DataFrame(Kmeans_df.loc[Kmeans_df.Kmeans_Clustering==3].mean(),columns= ['Cluster_4_avg'])\ncluster5 = pd.DataFrame(Kmeans_df.loc[Kmeans_df.Kmeans_Clustering==4].mean(),columns= ['Cluster_5_avg'])","98349924":"avg_df = pd.concat([cluster1,cluster2,cluster3,cluster4,cluster5],axis=1)\navg_df","c351c401":"# Looking at the bar plots all together once again\n\nfor i , row in avg_df.iterrows():\n    fig = plt.subplots(figsize=(8,6))\n    j = avg_df.xs(i ,axis = 0)\n    plt.title(i, fontsize=16, fontweight=20)\n    j.plot(kind='bar',fontsize=14)\n    plt.show()\n    print()","3494d039":"<a id = \"22\"><\/a>\n\n## 4.C.d) Conclusion: \n\nAccording the the silhouette score of:\n\n- The standardized data, the ideal number of clusters is 6, with a score higher than other options, of 0.37\n\n- The MinMaxScalar transformation data, it is the same for any number of cluster, and is equal to 0.23\n0.37 > 0.23, therefore we proceed with the standardized data with 6 clusters","1f430a80":"<a id = \"37\"><\/a>\n## 9.1) Analyzing Clusters' Strong Points","00e9367d":"**Balance**\n\nCluster 2 has the highest average number of miles eligible for award travel","4fda1c4f":"<a id = \"112\"><\/a>\n\n## TASK II Answer \/ Conclusion","a857e841":"<a id = \"24\"><\/a>\n## 5.A) Hierarchial Clustering","76c19a52":"<a id = \"27\"><\/a>\n# 6 -  TASK II - Compare the cluster centroids\n\n\n- Compare the cluster centroids to characterize the different clusters and try to give each cluster a label.","e2225886":"<a id = \"5\"><\/a> \n##  2.B) Importing & Preprocessing the Data","6cbfa33b":"- __Field Name__=ID#;\t__Data Type__=NUMBER;\t__Description__=Unique ID\n- __Field Name__=Balance;\t__Data Type__=NUMBER;\t__Description__=Number of miles eligible for award travel\n- __Field Name__=Qual_miles;\t__Data Type__=NUMBER;\t__Description__=Number of miles counted as qualifying for Topflight status\n- __Field Name__=cc1_miles;\t__Data Type__=CHAR;\t__Description__=Number of miles earned with freq. flyer credit card in the past 12 months:\n- __Field Name__=cc2_miles;\t__Data Type__=CHAR;\t__Description__=Number of miles earned with Rewards credit card in the past 12 months:\n- __Field Name__=cc3_miles;\t__Data Type__=CHAR;\t__Description__=Number of miles earned with Small Business credit card in the past 12 months:\n- For cc1_miles,cc2_miles and cc3_miles bins mean following ranges:\n        - 1 = under 5,000\n        - 2 = 5,000 - 10,000\n        - 3 = 10,001 - 25,000\n        - 4 = 25,001 - 50,000\n        - 5 = over 50,000\n- __Field Name__=Bonus_miles;\t__Data Type__=NUMBER;\t__Description__=Number of miles earned from non-flight bonus transactions in the past 12 months\n- __Field Name__=Bonus_trans;\t__Data Type__=NUMBER;\t__Description__=Number of non-flight bonus transactions in the past 12 months\n- __Field Name__=Flight_miles_12mo;\t__Data Type__=NUMBER;\t__Description__=Number of flight miles in the past 12 months\n- __Field Name__=Flight_trans_12;\t__Data Type__=NUMBER;\t__Description__=Number of flight transactions in the past 12 months\n- __Field Name__=Days_since_enroll;\t__Data Type__=NUMBER;\t__Description__=Number of days since Enroll_date\n- __Field Name__=Award?;\t__Data Type__=NUMBER;\t__Description__=Dummy variable for Last_award (1=not null, 0=null)","97516272":"Applying PCA on MinMaxscaler transformation data gives 5 PCA components.","f655df56":"<a id = \"114\"><\/a>\n## TASK IV Answer \/ Conclusion","0808f77c":"* We have imported then preprocessed the data.\n* We have performed exploratory data analysis where we've visualized some key aspects of our data using plots.\n* We have then detected and removed the outliers to give us a better base to perform clustering on.\n* We have created a model, and using specific methods, we have detected the ideal numbers of clusters for different methods.\n* We applied Hierarchial Clustering\n* We compared cluster centroids\n* We repeated the hierarchial clustering with a random 95% of the same data, then reported our findings.\n* We applied K Means clustering, then  compared it to hierarchial clustering.\n* We tried answering which clusters to target and how.","2c7baeb3":"Applying PCA on standardized data with 95% variance gives 8 PCA components ","6d04abd2":"<a id = \"30\"><\/a>\n## 7.B) Normalizing","f614593a":"**Bonus Transactions**\n\nCluster 2 has the highest average number of non-flight bonus transactions in the past 12 months","604fbf8a":"We have already established that clusters 0,1 and 3  (1,2,4 from now on) are the most important ones for us to focus on as clusters 2 & 4 (3&5 from now on) had very few passengers.","072cfe6f":"#  Homework 3 - Clustering, Group 12","adfeba7c":"<a id = \"21\"><\/a>\n### 4.C.c) The Sillhouette Method","4622fe84":"<a id = \"10\"><\/a> \n## 3.A.a) Removing the outliers from \"Balance\"","6ed08de1":"<a id = \"6\"><\/a> \n### 2.B.a) Inspecting the Data","5a2088d0":"<a id = \"28\"><\/a>\n# 7 -  TASK III -  Remove a random 5% of the data, and repeat the analysis\n- To check the stability of the clusters, remove a random 5% of the data (by taking a random sample of 95% of the records, namely 200 records), and repeat the analysis. Does the same picture emerge? Use 425 as the seed. (___10 points___)","727247f2":"<a id = \"34\"><\/a>\n## 8.A) Building Unsupervised KMeans Model","320cdb8b":"<a id = \"31\"><\/a>\n## 7.C) Hierarchial Clustering","98369de6":"**Flight Miles & Flight Transactions 12mo**\n\nCluster 4 has by far the highest average number of number of flight miles and the amount of flight transactions in the past 12 months","b96816c8":"<a id = \"29\"><\/a>\n## 7.A) Removing 5% of the Dataset","3815a4ee":"#### Now we apply the same procedure to the 95% of the dataset to see changes, as instructed.","5875dfe1":"<a id = \"1\"><\/a>\n# 1) Case Summary","39ca16b8":"<a id = \"39\"><\/a>\n### Citations:\nThese links have been helpful to me in creating this project in one way or another.\n* [Airlines Clustering by @shauryaa117](https:\/\/www.kaggle.com\/shauryaa117\/airlines-clustering\/data)\n* [Validating Clustering Model thru Silhouette Score by @mhrizvi](https:\/\/www.kaggle.com\/mhrizvi\/validating-clustering-model-thru-silhouette-score)\n* [Kmeans- Detailed Explanation by @vipulgandhi](https:\/\/www.kaggle.com\/vipulgandhi\/kmeans-detailed-explanation)\n* [Unsupervised Learning With Python \u2014 K- Means and Hierarchical Clustering](https:\/\/medium.datadriveninvestor.com\/unsupervised-learning-with-python-k-means-and-hierarchical-clustering-f36ceeec919c)\n* [A Beginner\u2019s Guide to Hierarchical Clustering and how to Perform it in Python](https:\/\/www.analyticsvidhya.com\/blog\/2019\/05\/beginners-guide-hierarchical-clustering\/)\n* [KMeans & Hierarchical Clustering. EastWestAirlines by @shrikantuppin](https:\/\/www.kaggle.com\/shrikantuppin\/kmeans-hierarchical-clustering-eastwestairlines\/data#Step-6:-Hierarchical-Clustering-Algorithm)","8aa6bb5b":"# Introduction\n\n1.  [Case Summary](#1)\n    1. [Data Description](#2)\n1.  [Importing](#3)\n    1. [Importing the Libraries](#4)\n    1. [Importing and Preprocessing the Data](#5)\n        1. [Inspecting the Data](#6)\n        1. [Cleaning the Data](#7)\n1.  [Exploratory Data Analysis](#8)\n    1. [Outlier Analysis](#9)\n        1. [Removing the outliers from \"Balance\"](#10)\n        1. [Removing the outliers from \"Bonus_miles\"](#11)\n        1. [Removing the outliers from \"Flight_miles_12mo\"](#12)\n        1. [Removing the outliers from \"Qual_miles\"](#13)\n    1. [Finalizing the dataframe](#14)\n1.  [Modelling](#15)\n    1. [Normalizing the Data](#16)\n    1. [Elbow Method for Determining Cluster Amount](#17)\n    1. [PCA & the Silhouette Method](#18)\n        1. [Running PCA of standardized data](#19)\n        1. [Running PCA of MinMaxscalar data](#20)\n        1. [The Sillhouette Method](#21)\n        1. [Conclusion](#22)\n1.  [TASK I : How many clusters appear to be appropriate?](#23)\n    1.  [Hierarchial Clustering](#24)\n    1.  [Agglomerative Clustering](#25)\n    1.  [Labeling Clusters](#26)\n1.  [TASK II - Compare the cluster centroids](#27)\n1.  [TASK III -  remove a random 5% of the data, and repeat the analysis](#28)\n    1. [Removing 5% of the Dataset](#29)\n    1. [Normalizing](#30)\n    1. [Hierarchial Clustering](#31)\n    1. [Agglomerative Clustering](#32)\n1. [TASK IV : Use k-means algorithm. Does the same picture emerge?](#33)\n    1. [Building Unsupervised k-means Model](#34)\n    1. [Analyzing Clusters](#35) \n1. [TASK V : Which clusters would you target for offers?](#36)\n    * [Analyzing Clusters' Strong Points](#37)\n1. [Conclusion](#38)\n* [Citations](#39)","8ff2b534":"<a id = \"7\"><\/a> \n### 2.B.b) Cleaning the Data (if needed)","57a96bf5":"K Means seems to be more successful in division of clusters.","558b27e0":"<a id = \"11\"><\/a> \n## 3.A.b) Removing the outliers from \"Bonus_miles\"","13d05840":"As we have determined with the Dendograms, the color seperation in Ward Method indicate that 5 is the optimal number for clusters.\n\nBut, hierarchial clustering overall does not seem to do a really good job for the data we have, since all the passengers have been grouped in cluster 1. And it requires too much cpu. We will proceed with K means clustering.","75fd1d92":"Even in this small sample, we can see that the last few lines of df1.head() are different, so removing the outliers seems to have worked. Now to visualize;","40d85147":"<a id = \"113\"><\/a>\n\n## TASK III Answer \/ Conclusion","d4752dde":"## Shortcuts to Given Tasks & Answers\n\n1.  [TASK I : How many clusters appear to be appropriate?](#23)\n    * [Answer](#111)\n1.  [TASK II - Compare the cluster centroids](#27)\n    * [Answer](#112)\n1.  [TASK III -  remove a random 5% of the data, and repeat the analysis](#28)\n    * [Answer](#113)\n1.  [TASK IV : Use k-means algorithm. Does the same picture emerge?](#33)\n    * [Answer](#114)\n1. [TASK V : Which clusters would you target for offers?](#36)\n    * [Answer](#115)","032b513f":"In clusters numbered 0,1,3 (which are clusters 1,2,4 in the graphs above and below) there are more customers. Airline should focus on clusters 0, 1 & 3. ","d388318e":"<a id = \"13\"><\/a> \n## 3.A.d) Removing the outliers from \"Qual_miles\"","f11f30a7":"<a id = \"35\"><\/a>\n\n## 8.B Analyzing Clusters","55a1f64a":"<a id = \"16\"><\/a>\n## 4.A) Normalizing the Data","c4fdca6e":"**Bonus Miles**\n\nCluster 3 has the highest average number of miles earned from non-flight bonus transactions in the past 12 months, followed by Cluster 2","25793d70":"<a id = \"3\"><\/a>\n# 2) Importing","d9884a72":"<a id = \"9\"><\/a> \n## 3.A) Outlier Analysis","9e8ad82c":"Since our hierarchial model did not seem to work so well, we have done the tasks TASK 2 requires in the following sections ( mainly section 9) \n\n8.B. [Analyzing Clusters](#35) \n\n9. [TASK V : Which clusters would you target for offers?](#36)\n\n    * [Analyzing Clusters' Strong Points](#37)\n    \n10. [Conclusion](#38)","8f222aff":"**Rewards Miles (cc2)**\n\nCluster 5 has the highest average  number of miles earned with Rewards credit card in the past 12 months. \n\nIt is not in our best interest to focus cluster 5, because of the low passenger count.","3a92b7c9":"There are no missing \/ Null values either in columns or rows, so we can move on to the next step, which is Exploratory Data Analysis.","3eff2ce3":"East-West Airlines is trying to learn more about its customers.  Key issues are their flying patterns, earning and use of frequent flyer rewards, and use of the airline credit card. The task is to identify customer segments via clustering. The file EastWestAirlines.xls contains information on 4000 passengers who belong to an airline\u2019s frequent flier program. For each passenger the data include information on their mileage history and on different ways they accrued or spent miles in the last year. __The goal is to try to identify clusters of passengers that have similar charactersitics for the purpose of targeting different segments for different types of mileage offers.__\n\nPlease prepare a notebook including answers to each of the following tasks:\n\n1. Apply hierarchical clustering with Euclidean distance and complete linkage. How many clusters appear to be appropriate? \n\n2. Compare the cluster centroids to characterize the different clusters and try to give each cluster a label.\n\n3. To check the stability of the clusters, remove a random 5% of the data (by taking a random sample of 95% of the records, namely 200 records), and repeat the analysis. Does the same picture emerge? Use 425 as the seed.\n\n4. Use k-means algorithm with the number of clusters you found in part (a). Does the same picture emerge? \n\n5. Which clusters would you target for offers, and what type of offers would you target to customers in that cluster?\n","e0591a13":"<a id = \"20\"><\/a>\n### 4.C.b) Running PCA of MinMaxscalar data.","b8985781":"<a id = \"15\"><\/a>\n# 4 -  Modelling","7cc13fdb":"<a id = \"8\"><\/a> \n# 3) Exploratory Data Analysis","97da35d1":"<a id = \"18\"><\/a>\n## 4.C) PCA & the Silhouette Method ","35c9657b":"The optimal number of clusters is 6, as seen from elbow curve.","3d021801":"<a id = \"19\"><\/a>\n### 4.C.a) Running PCA of normalized data.","ed535355":"<a id = \"38\"><\/a>\n# 10 - Conclusion","18c16891":"<a id = \"23\"><\/a>\n# 5 - TASK I : Apply hierarchical clustering, How many clusters appear to be appropriate?\n\n- Apply hierarchical clustering with Euclidean distance and complete linkage. How many clusters appear to be appropriate?","a3a39e68":"The amount of outliers Qual_miles has seems too many (therefore too important) to be removed, so we continue with df3.","bdb333b5":"Our previous Clustering Score was 64.72\n\nSo, with a new Clustering Score of 61.848, we have a similar but a slightly worse image overall.","11887d0b":"<a id = \"4\"><\/a>\n## 2.A) Importing the Libraries","db697af9":"<a id = \"111\"><\/a>\n\n## TASK I Answer \/ Conclusion","60781cab":"<a id = \"12\"><\/a> \n## 3.A.c) Removing the outliers from \"Flight_miles_12mo\"","5780b9b7":"### The seperation in Ward Method indicate that 6 is the optimal number for clusters.","45ded19d":"**Frequent Flying Miles (cc1)**\n\nCluster 2 has the highest average number of miles earned with freq. flyer credit card in the past 12 month","3b41ca7a":"**Qualifying Miles**\n\nCluster 4 has the highest average of number of miles counted as qualifying for top flight status","980f06ef":"<a id = \"36\"><\/a>\n# 9 -  TASK V : Which clusters would you target for offers?\n- Which clusters would you target for offers, and what type of offers would you target to customers in that cluster?","c278c8d9":"<a id = \"25\"><\/a>\n## 5.B) Agglomerative Clustering:","3801854e":"<a id = \"26\"><\/a>\n## 5.C) Labeling Clusters","05cbb7bd":"<a id = \"33\"><\/a>\n# 8 - TASK IV : Use k-means algorithm, Does the same picture emerge?\n\nUse k-means algorithm with the number of clusters you found in part (a). Does the same picture emerge?","418d447d":"People who fly more frequently use bonus transactions more","8e76b8ad":"<a id = \"14\"><\/a> \n## 3.B) Finalizing the dataframe","b6c39429":"<a id = \"32\"><\/a>\n## 7.D) Agglomerative Clustering:","d7943c37":"<a id = \"2\"><\/a>\n## 1.A) Data Description","3f79918e":"Unique points in this correlation matrix:\n\n- ID# is negatively correlated with days_since_enroll\n- flight_trans_12 is positively correlated with flight_miles_12mo\n- bonus_trans is positively correlated with cc1_miles \n- bonus_trans is positively correlated with bonus_miles\n- bonus_miles is positively correlated with cc1_miles\n\nIn other words, the following are positively correlated\n\n- Unique ID <-> Number of days since Enroll_date\n- Number of flight transactions in the past 12 months <-> Number of flight miles in the past 12 months\n- Miles passengers earn with the freq. flyer credit card <-> Amount of non-flight transactions\n- Number of non-flight bonus transactions <-> Miles passengers earn from non-flight bonus transactions\n- Miles passengers earn from non-flight bonus transactions <-> Miles passengers earn with the freq. flyer credit card","c218c243":"**Days since Enroll**\n\nAverage of \"Days_since_enroll\" shows us that Cluster 2 has the customers that have been with the airline for longer than other clusters.\n\n","c7602ac2":"<a id = \"115\"><\/a>\n\n## TASK V Answer \/ Conclusion\n\nBetween Clusters 1,2 & 4:\n\n1. Cluster 1 passengers are much newer passengers that have lower number of miles eligible for award travel AND the lowest number of miles earned with freq. flyer credit card.\n\n* Which means to attract the passengers in this cluster (which has the highest amount of passengers), we could offer discounts the more they travel.\n\n* Something like: \n\n* **\"20% off on your second travel, 25% if you are on your 3rd or more travel in the past 12 months\"**\n\n2. Cluster 2 passengers are by far the most frequent flyers, that have a lot of miles earned with freq. flyer credit card.\n\n* To encourage spending these saved miles, we could offer Cluster 2 passengers a discount on first class seating. The passengers then would have an incentive to travel, and they would enjoy first class travel, and they would spend their saved miles, which would benefit the airline, creating a win-win situation.\n\n3. Cluster 4 passengers seem to travel the longest distances, and the passenger count in this cluster is less than clusters 2 and 1, which could mean that these are upper class passengers.\n\n* We could offer cluster 4 passengers discounts if they offer our airline to a friend, and maybe follow this process with a confirmation by both passengers. The purpose of this discount offer would be that the potential new passenger would have a higher chance of being an upper class citizen, which would mean that they would be more prone to travel first class, which would benefit the airline.","f3a7aabc":"**Small Business Miles (cc3)**\n\nCluster 3 has the highest average number of miles earned with Small Business credit card in the past 12 months:\n\nIt is not in our best interest to focus cluster 3 either, same reason as cluster 5.","b71b58fd":"<a id = \"17\"><\/a>\n## 4.B) Elbow Method for Determining Cluster Amount"}}