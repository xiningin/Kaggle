{"cell_type":{"c62c8a8d":"code","aa2aa0a5":"code","1ba0be2f":"code","bd14cd9b":"code","d6ecf484":"code","0f5ac492":"code","39333515":"code","b9b62aa3":"code","9c4679f4":"code","ec8f2ba8":"code","1aef3c8d":"code","6c12e448":"code","e5fedeca":"code","589910d9":"code","1cbdd866":"code","2ce571fc":"code","8d3e7444":"code","ce59ece1":"code","38ab97eb":"code","a3b3495c":"code","b03f1d11":"code","1531baa8":"code","c41aa945":"code","ad2fb35c":"code","ac05e9e3":"code","846e4b11":"code","8be01b88":"code","9cbf052a":"code","281b0e43":"code","a2b603fa":"code","0e37e92e":"code","2cb8d9a8":"code","e76a12d6":"code","9ba45621":"code","7bfaa418":"code","ee5ee073":"code","253f139e":"code","385f7824":"code","a5d48612":"code","d0be18bb":"code","8e179f81":"code","17c2fe93":"code","51102d49":"code","b3231862":"code","6a36e51e":"code","1640ac44":"code","5f62bfa7":"code","98fcd133":"code","560de99e":"code","fd188d69":"code","0536687d":"code","59c7eff2":"code","693bc829":"code","8f299cad":"code","79639ca5":"code","654e53e4":"code","f3a99915":"code","ca55091e":"code","133b9b93":"code","c70904cf":"code","e65e5ad1":"code","29681fc9":"code","7c4cf15d":"code","499baeac":"code","4f6ba22a":"code","8a5b1f37":"code","a6a76deb":"code","169bae5f":"code","640abdda":"code","87487f81":"code","a7764c67":"code","172220df":"code","b3003733":"code","ef1884d7":"code","47ffffb3":"code","cc9f746d":"code","63b1a3dc":"code","49ca679a":"code","1335dad9":"code","1476eb15":"code","b0de09cf":"code","51c16f2a":"code","93453d33":"code","6429cdc3":"code","b2da9557":"code","e6619274":"code","86c11a21":"code","ef1fc966":"code","4b42fbbd":"code","717fa2a4":"code","c4e7f6a7":"code","05fd941c":"code","5b4bc6fd":"code","9c552bff":"code","76d01d03":"code","c20d8974":"code","80c8bd41":"code","73e8278e":"code","5d6d6f88":"code","feb383af":"code","61c11df2":"code","eab902a8":"code","2f9b9e26":"code","7e3fa29d":"code","665b0def":"code","8e45caf3":"code","346a2f70":"markdown","dae755a1":"markdown","93337ddf":"markdown","9855de0c":"markdown","56211adf":"markdown","6c95d1c6":"markdown","e0c1d5a0":"markdown","11d003cd":"markdown","b3424d0e":"markdown","a96fa2fe":"markdown","326b0760":"markdown","37794734":"markdown","838c41be":"markdown","b6ae8e79":"markdown","96c667e5":"markdown","384b6c66":"markdown","26ffb915":"markdown","e53943ca":"markdown","580e5cb4":"markdown","d8b1434f":"markdown","10ca5051":"markdown","bc50e423":"markdown","68770fc9":"markdown","b5986301":"markdown","57fd45da":"markdown","7fdfdd3f":"markdown","9c60d3bb":"markdown","3241fb70":"markdown","19d3a550":"markdown","4f3ae306":"markdown","a5192b08":"markdown","5efbea2b":"markdown","81b0bf77":"markdown","b7f53d00":"markdown","904e1356":"markdown","025d5caa":"markdown","b1cbebf3":"markdown","7b52fd2f":"markdown","aeb90d15":"markdown","528908ce":"markdown","2a30b209":"markdown","5c02b2a6":"markdown","44f2d06f":"markdown","a19c2576":"markdown","76bc4a19":"markdown","033f4594":"markdown","d001d97c":"markdown","2708a728":"markdown","2201af18":"markdown","043f3b00":"markdown","8cef8e8e":"markdown","ff6cdc8f":"markdown","d027d6ac":"markdown","8fe21024":"markdown","80689982":"markdown","68da0f07":"markdown","001f3e29":"markdown","441d1b4b":"markdown","1a1fa45f":"markdown","08d38804":"markdown","dee21658":"markdown","e10c3690":"markdown","ea73ae56":"markdown","ba0baa33":"markdown","aa5a0934":"markdown","471136ca":"markdown","f80d329a":"markdown"},"source":{"c62c8a8d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport plotly.offline as py\npy.init_notebook_mode(connected = True)\nimport plotly.graph_objs as go\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nimport plotly.figure_factory as ff\nfrom sklearn.metrics import confusion_matrix,accuracy_score,classification_report\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import precision_score,recall_score\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.metrics import roc_auc_score,roc_curve,scorer\nfrom sklearn.metrics import f1_score\nimport statsmodels.api as sm\nimport plotly.tools as tls","aa2aa0a5":"\ndf =  pd.read_csv('..\/input\/loadpred\/train_AV3.csv')","1ba0be2f":"df.head()","bd14cd9b":"df.columns","d6ecf484":"df.shape","0f5ac492":"df.info()","39333515":"df.describe()","b9b62aa3":"df.isnull().sum()","9c4679f4":"df.Gender.unique()","ec8f2ba8":"df.Gender.fillna(df.Gender.mode().values[0], inplace = True)","1aef3c8d":"df.Gender.isnull().sum()","6c12e448":"df.Married.unique()","e5fedeca":"df.Married.fillna(df.Married.mode().values[0], inplace=True)","589910d9":"df.Married.isnull().sum()","1cbdd866":"df.Dependents.unique()","2ce571fc":"df.Dependents.fillna(df.Dependents.mode().values[0], inplace=True)","8d3e7444":"df.Dependents.isnull().sum()","ce59ece1":"df.Self_Employed.unique()","38ab97eb":"df.Self_Employed.fillna(df.Self_Employed.mode().values[0], inplace=True)","a3b3495c":"df.Self_Employed.isnull().sum()","b03f1d11":"df.LoanAmount.unique()","1531baa8":"df.LoanAmount.fillna(round(df.LoanAmount.mean(),0),inplace=True)","c41aa945":"df.LoanAmount.isnull().sum()","ad2fb35c":"df.Loan_Amount_Term.unique()","ac05e9e3":"df.Loan_Amount_Term.fillna(round(df.Loan_Amount_Term.mean(),0), inplace=True)","846e4b11":"df.Loan_Amount_Term.isnull().sum()","8be01b88":"df.Credit_History.unique()","9cbf052a":"df.Credit_History.fillna(df.Credit_History.mode().values[0], inplace=True)","281b0e43":"df.Credit_History.isnull().sum()","a2b603fa":"df.isnull().sum()","0e37e92e":"def categorical_plots(var, data):\n    \n    \n    #Adjustment of plots, bigger size and space b\/w subplots\n    \n    fig = plt.figure(figsize=(15,5))\n    fig.subplots_adjust(wspace=0.7)\n    \n    #1st Plot:  Bar plot     \n        \n    plt.subplot(1,3,1)\n    sns.countplot(x=var, data= data)\n    plt.xticks(rotation = 45, horizontalalignment='right')\n    plt.xlabel(var.name + ' Distribution')\n\n    #2nd Plot: PIE Chart\n    \n    labels =var.value_counts().index  #Labels that will be written against slices in pie charts\n    \n    #For the slice with highest value to be exploded, explode parameter is passed. Using for loop to make a tuple of \n    # number of slice using len(unique) and exploding the first slice by mentioning 0.1 at first index. Atlast converted list to tuple\n    \n    a=[0.1]\n    for i in range ((len(var.unique()))-1):\n        a.append(0)\n\n    explode1= tuple(a)\n    #if var.name != 'Customer Name':\n    ax1 = plt.subplot(1,3,2)\n    ax1.pie(var.value_counts(), labels=labels,autopct='%1.1f%%', shadow=True,explode= explode1 )\n    ax1.axis('equal')\n    plt.xlabel(var.name + ' Distribution')\n    \n    #3rd Plot: Line Plot\n    \n    plt.subplot(1,3,3)\n    var.value_counts().sort_index().plot.line()\n    plt.xticks(rotation = 45, horizontalalignment='right')\n    plt.xlabel(var.name + ' Distribution')\n    \n    show=plt.show()\n    \n    return(show)\n\n","2cb8d9a8":"#FOR NUMERICL PLOTS WE WILL BE USING THE FOLLOWING FUNCTION\n\ndef numerical_plots(var):\n    \n    #Adjustment of plots, bigger size and space b\/w subplots\n    \n    fig = plt.figure(figsize=(15,4))\n    fig.subplots_adjust(wspace=0.3)\n    \n    #1st Plot:  Histogram with KDE plot          \n \n    plt.subplot(1,3,1)\n    sns.distplot(var, color='b')\n    plt.xlabel(var.name + ' Distribution')\n\n    \n    #2nd Plot:  Box plot\n    \n    plt.subplot(1,3,2)\n    sns.boxplot(y=var)\n    plt.xlabel(var.name + ' Distribution')\n\n\n    #3rd Plot:  Histogram without plot     \n\n    plt.subplot(1,3,3)\n    sns.distplot(var, color='b', kde=False)\n    plt.xlabel(var.name + ' Distribution')\n    \n    #plt.subplot(1,3,3)\n    #sns.kdeplot(var, color='b')\n    #plt.xlabel(var.name + ' Distribution')\n    \n    show=plt.show()\n    \n    return(show)\n","e76a12d6":"categorical_plots(df.Gender, df)","9ba45621":"categorical_plots(df.Married, df)","7bfaa418":"categorical_plots(df.Dependents,df)","ee5ee073":"categorical_plots(df.Education,df)","253f139e":"categorical_plots(df.Self_Employed,df)","385f7824":"numerical_plots(df.ApplicantIncome)","a5d48612":"numerical_plots(df.CoapplicantIncome)","d0be18bb":"numerical_plots(df.LoanAmount)","8e179f81":"categorical_plots(df.Loan_Amount_Term,df)","17c2fe93":"categorical_plots(df.Credit_History,df)","51102d49":"categorical_plots(df.Property_Area,df)","b3231862":"categorical_plots(df.Loan_Status, df)","6a36e51e":"df.Credit_History = np.where(df.Credit_History== 1., 'Yes','No')\ndf.Credit_History.unique()","1640ac44":"for i in df.columns:\n    if df[i].dtype =='O' and i!='Loan_ID':\n        sns.countplot(x=df[i], hue=df.Loan_Status)\n        plt.show()","5f62bfa7":"for i in df.columns:\n    if df[i].dtype !='O':\n        sns.boxplot(y=df[i], x=df.Loan_Status)\n        plt.show()","98fcd133":"df.select_dtypes(include='O').columns","560de99e":"import scipy.stats as s","fd188d69":"def chi2(data,target,alpha):\n    \n    for i in df.columns:\n    \n        if df[i].dtype == 'O' and i != target:\n            col = i\n\n            ov = pd.crosstab(data[col], data[target])\n            #max_least_income = ov.loc[ov[' <=50K'].idxmax()].name\n            #max_highest_income = ov.loc[ov[' >50K'].idxmax()].name\n            plt.style.use('ggplot')\n            ov.plot(kind='bar', figsize=(5,5), stacked=True)\n            plt.xlabel(i.title())\n                 \n            chi = s.chi2_contingency(ov)\n            chi2_s = chi[0]\n            p_value = chi[1]\n            dof = chi[2]\n            critical_value = s.chi2.ppf(q=1-alpha, df=dof)\n            \n            print('\\n\\033[1m\\033[4m', col.upper(),':\\033[0m \\n')\n            print('Significance Level = ', alpha)\n            print('Degree of Freedom = ', dof)\n            print('chi2 = ', chi2_s)\n            print('Critical Value = ',critical_value)\n            print('p-value = ', p_value)\n\n            if chi2_s >=critical_value or p_value <= alpha :\n                print('\\nWe reject the null hypotheses, there is a relationship between the two variables \\n')\n            else:\n                print('\\nThere is no relationship between the two variables and the null hypotheses is retained \\n')\n            \n            plt.show()\n            #print('\\033[1mThe bar chart shows that', max_least_income,i,'has the highest number of people with <=50k income and',max_highest_income,i,'has the highest number of people having income >50K \\n')","0536687d":"chi2(df, 'Loan_Status', 0.05)","59c7eff2":"df.drop(columns=['Loan_ID'], inplace=True)","693bc829":"df.info()","8f299cad":"df.Loan_Status.value_counts()","79639ca5":"df.Loan_Status.dtype","654e53e4":"df.Loan_Status = np.where(df.Loan_Status=='Y', 1, 0)","f3a99915":"df.Loan_Status.value_counts()","ca55091e":"correlation =df.corr()\ncorrelation.Loan_Status","133b9b93":"df.corr()","c70904cf":"df.head()","e65e5ad1":"df.Gender = np.where(df.Gender =='Male', 1,0)\ndf.Married = np.where(df.Married == 'Yes',1,0)\ndf.Education = np.where(df.Education == 'Graduate',1,0)\ndf.Self_Employed = np.where(df.Self_Employed =='No', 1,0)\ndf.Credit_History = np.where(df.Credit_History =='Yes',1,0)","29681fc9":"df.Dummies = pd.get_dummies(df.Property_Area)","7c4cf15d":"df.Dummies.head()","499baeac":"df = pd.concat([df,df.Dummies], axis=1)","4f6ba22a":"df.head()","8a5b1f37":"df.drop(columns=['Property_Area'], inplace=True)","a6a76deb":"df.head()","169bae5f":"from sklearn.preprocessing import StandardScaler\nzscore = StandardScaler()","640abdda":"# We will used copied dataframe here. And 3+ values needs to be changed for normalization, doing so.\ndf['Dependents'] = np.where(df.Dependents == '3+', 4, df.Dependents)","87487f81":"cols = ['ApplicantIncome', 'CoapplicantIncome','LoanAmount','Loan_Amount_Term']\nfor i in cols:\n    df[i] = zscore.fit_transform(df[[i]])","a7764c67":"x = df\ndf.head()","172220df":"df1 =df.copy() # Saving a copy of dataframe to be utilized later on to see the effects of normalization and feature selection on model","b3003733":"y = df['Loan_Status'] #Separating Target Variable\ndf.drop(columns=['Loan_Status'], inplace=True)\nx = df\nx= x.to_dict(orient='records')\n\nfrom sklearn.feature_extraction import DictVectorizer\nvec = DictVectorizer()\nx = vec.fit_transform(x).toarray()\nx","ef1884d7":"y = np.asarray(y)","47ffffb3":"# We will use this split data for all algorithms\nxtrain,xtest,ytrain,ytest =train_test_split(x,y,test_size=0.2, random_state=0)","cc9f746d":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nmodel_LG =LogisticRegression()\nmodel_LG.fit(xtrain,ytrain);","63b1a3dc":"y_pred_LG = model_LG.predict(xtest)\nprobabilities = model_LG.predict_proba(xtest)\nfpr,tpr,thresholds = roc_curve(ytest,probabilities[:,1])","49ca679a":"# Defining a functio to be used for evaluation for all algorithms\ndef evaluation(algorithm):\n    #Classification Report\n    print (\"\\n \\033[1m Classification report : \\033[0m\\n\",classification_report(ytest,algorithm ))\n\n    #Accuracy\n    print (\"\\033[1mAccuracy Score   : \\033[0m\",accuracy_score(ytest, algorithm))\n\n    #conf_matrix\n    conf_matrix = confusion_matrix(ytest,algorithm)\n\n\n    #roc_auc_score\n    model_roc_auc = round(roc_auc_score(ytest, algorithm),3) \n    print (\"\\033[1mArea under curve : \\033[0m\",model_roc_auc)\n    fpr,tpr,thresholds = roc_curve(ytest,probabilities[:,1])\n\n    # roc curve plot\n    trace1 = go.Scatter(x = fpr,y = tpr,\n                        name = \"Roc : \" + str(model_roc_auc),\n                        line = dict(color = ('rgb(22, 96, 167)'),width = 2),\n                       )\n    #confusion matrix plot\n    trace2 = go.Heatmap(z = conf_matrix ,\n                        x = [\"Not Granted\",\"Granted\"],\n                        y = [\"Not Granted\",\"Granted\"],\n                        colorscale = \"Viridis\",name = \"matrix\" )\n    #subplots\n    fig = tls.make_subplots(rows=1, cols=2, horizontal_spacing = 0.40,subplot_titles=('ROC Curve','Confusion Matrix'))\n\n    fig.append_trace(trace1,1,1)\n    fig.append_trace(trace2,1,2)\n\n\n    fig['layout'].update(showlegend=False, title=\"Model performance\" ,\n                         autosize = False,height = 400,width = 800,\n                         plot_bgcolor = 'rgba(240,240,240, 0.95)',\n                         paper_bgcolor = 'rgba(240,240,240, 0.95)',\n                         xaxis = dict(title = \"false positive rate\",\n                                 gridcolor = 'rgb(255, 255, 255)',\n                                 domain=[0, 0.6],\n                                 ticklen=5,gridwidth=2),\n                        yaxis = dict(title = \"true positive rate\",\n                                  gridcolor = 'rgb(255, 255, 255)',\n                                  zerolinewidth=1),\n                        margin = dict(b = 20))\n\n    py.iplot(fig)\n","1335dad9":"print (\"\\n\\033[1m Classification report : \\033[0m\\n\",classification_report(ytest,y_pred_LG))\nprint (\"\\033[1mAccuracy Score   : \\033[0m\",accuracy_score(ytest,y_pred_LG))\nevaluation(y_pred_LG)","1476eb15":"data = pd.DataFrame({'Actual': ytest.flatten(), 'Predicted': y_pred_LG.flatten()})\ndata.head(10)","b0de09cf":"data = data.head(20)\ndata.plot(kind='bar',figsize=(15,5))\nplt.title('Actual vs Predicted')\nplt.grid(which='major', linestyle=':', linewidth='0.99', color='black')\nplt.show()","51c16f2a":"df1.head()","93453d33":"#df1 is a normzalized dataframe saved earlier\ny2 =df1['Loan_Status']\ndf1.drop(columns=['Gender','Dependents','Self_Employed','ApplicantIncome','CoapplicantIncome','LoanAmount','Loan_Amount_Term','Loan_Status'], inplace=True)\n\ndf1.columns","6429cdc3":"x2=df1\nx2.head()","b2da9557":"x2=df1\nx2 = x2.to_dict(orient='records')\nx2 =vec.fit_transform(x2).toarray()\ny2=np.asarray(y2)","e6619274":"xtrain2,xtest2,ytrain2,ytest2 =train_test_split(x2,y2,test_size=0.2, random_state=0)","86c11a21":"#Logistic Regression model training\nmodel_LG.fit(xtrain2,ytrain2); ","ef1fc966":"#Prediction\ny_pred_LG2 = model_LG.predict(xtest2)\nprobabilities = model_LG.predict_proba(xtest2)\nfpr,tpr,thresholds = roc_curve(ytest,probabilities[:,1])","4b42fbbd":"evaluation(y_pred_LG2)","717fa2a4":"from sklearn.neighbors import KNeighborsClassifier\n\n#Model Traning\nmodel_knn = KNeighborsClassifier()\nmodel_knn.fit(xtrain,ytrain);\n","c4e7f6a7":"#Prediction\ny_pred_knn = model_knn.predict(xtest)\nprobabilities = model_knn.predict_proba(xtest)\nfpr,tpr,thresholds = roc_curve(ytest,probabilities[:,1])","05fd941c":"evaluation(y_pred_knn)","5b4bc6fd":"from sklearn.naive_bayes import GaussianNB\n#Model Training\nmodel_nb = GaussianNB()\nmodel_nb.fit(xtrain, ytrain);","9c552bff":"# Model Prediction\ny_pred_nb = model_nb.predict(xtest)\nprobabilities = model_nb.predict_proba(xtest)\nfpr,tpr,thresholds = roc_curve(ytest,probabilities[:,1])","76d01d03":"evaluation(y_pred_nb)","c20d8974":"print(xtrain.shape,xtest.shape,ytrain.shape,ytest.shape)","80c8bd41":"from sklearn import tree\n\n# Model Traning\nmodel_DT = tree.DecisionTreeClassifier()\nmodel_DT.fit(xtrain,ytrain)\ny_pred_DT = model_DT.predict(xtest)\nprobabilities = model_DT.predict_proba(xtest)\nfpr,tpr,thresholds = roc_curve(ytest,probabilities[:,1])","73e8278e":"evaluation(y_pred_DT)","5d6d6f88":"from sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV","feb383af":"\nsvm_classifier = SVC(kernel='rbf', random_state=0, probability=True)\nsvm_classifier.fit(xtrain,ytrain);","61c11df2":"y_pred_svm = svm_classifier.predict(xtest)\nprobabilities = svm_classifier.predict_proba(xtest)\nfpr,tpr,thresholds = roc_curve(ytest,probabilities[:,1])","eab902a8":"evaluation(y_pred_svm)","2f9b9e26":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier()\nmodel = rfc.fit(xtrain, ytrain)\n","7e3fa29d":"y_pred_rfc = rfc.predict(xtest)\nprobabilities = rfc.predict_proba(xtest)\nfpr,tpr,thresholds = roc_curve(ytest,probabilities[:,1])","665b0def":"evaluation(y_pred_rfc)","8e45caf3":"from sklearn.metrics import f1_score\n\ndef model_report(model,training_x,testing_x,training_y,testing_y,name) :\n    model.fit(training_x,training_y)\n    predictions  = model.predict(testing_x)\n    accuracy     = accuracy_score(testing_y,predictions)\n    recallscore  = recall_score(testing_y,predictions)\n    precision    = precision_score(testing_y,predictions)\n    f1score      = f1_score(testing_y,predictions) \n    ROC          = roc_auc_score(testing_y,predictions)\n    \n    df = pd.DataFrame({\"Model\"           : [name],\n                       \"Accuracy_score\"  : [accuracy],\n                       \"Recall_score\"    : [recallscore],\n                       \"Precision\"       : [precision],\n                       \"f1_score\"        : [f1score],\n                       \"Area Under Curve\": [ROC]\n                       })\n    return df\n\nmodel1 = model_report(model_LG,xtrain,xtest,ytrain,ytest,\"Logistic Reg. \")\n\nmodel2 = model_report(model_LG,xtrain2,xtest2,ytrain2,ytest2,\"Log.Reg.Selected Feat.\")\n\nmodel3 = model_report(rfc,xtrain,xtest,ytrain,ytest,\"Random Forest\")\n\nmodel4 = model_report(model_knn,xtrain,xtest,ytrain,ytest,\"KNN Classifier\")\n\nmodel5 = model_report(model_nb,xtrain,xtest,ytrain,ytest,\"Naive Bayes\")\n\nmodel6 = model_report(model_DT,xtrain,xtest,ytrain,ytest,\"Decision Tree\")\n\nmodel7 = model_report(svm_classifier,xtrain,xtest,ytrain,ytest,\"SVM Classifier\")\n\n\nmodel_performances = pd.concat([model1,model2, model3,model4,model5,model6, model7],axis = 0).reset_index()\n\nmodel_performances = model_performances.drop(columns = \"index\",axis =1)\n\ntable  = ff.create_table(np.round(model_performances,4))\n\npy.iplot(table)","346a2f70":"## <center>------End------<\/center>","dae755a1":"For all categorical variabes that have more than two unique we will hot encode them and for the rest we will manually encode them.","93337ddf":"## 7. Pre-processing","9855de0c":"## 8.8 Model Metrics Comparision","56211adf":"### 8.4.2 Naive Bayes Evaluation","6c95d1c6":"#### Dependents","e0c1d5a0":"### 8.7.2 Random Forest Evaluation","11d003cd":"### 5.1. Barplots for Categorical Attributes","b3424d0e":"### 8.2.4 Applying Logistic Regression (Selected Features)","a96fa2fe":"## 8.2 Logistic Regression (Features Selected)","326b0760":"## 8.3 KNN Algorithm","37794734":"## 8.4 Naive Bayes Classifier","838c41be":"### 5.2 Box plots for Numeric Attributes","b6ae8e79":"### 8.7.1 Applying Random Forest Classifier","96c667e5":"# <center><u>8. Machine Learning Models <\/u><\/center>","384b6c66":"### 8.5.1 Decision Tree Evaluation ","26ffb915":"### 8.2.2 Transforming into arrays","e53943ca":"#### Loan Amount Term","580e5cb4":"### 8.1.3 Applying Logistic Regression","d8b1434f":"### 8.5.1 Applying Decision Tree","10ca5051":"#### Credit History","bc50e423":"## 8.1 Logistic Regression","68770fc9":"#### Self Employed","b5986301":"### 8.2.3 Train Test Split","57fd45da":"## 1. Data Loading","7fdfdd3f":"For the purpose of better understanding of Credit History attribute relation with Target variable we will categorize it into Yes and No.","9c60d3bb":"#### 7.1.1 Manual Encoding","3241fb70":"## <center> 4. Uni-Variate Analysis<\/center>","19d3a550":"### 8.1.1 Tranforming features and Target Variables into Arrays","4f3ae306":"<b>Our main target is to find out customers that are elgible for Loan. Therefore we will plot bar charts of every categorical attribute agaist Loan Status and boxplot for every numerical attribute against Loan Status <\/b>","a5192b08":"<b>5.2.1 Results : <\/b> Numerical attributes dont seem to have a noticable relation with the target variable. This further will be conifrmed in the correlation matrix.","5efbea2b":"### 8.3.1 Applying KNN ","81b0bf77":"#### Married","b7f53d00":"#### Gender","904e1356":"### 8.2.1 Dropping Attributes","025d5caa":"## 8.6. SVM Classifer","b1cbebf3":"## 6. Feature Selection","7b52fd2f":"## 8.7 Random Forest","aeb90d15":"### 7.1 Feature Encoding and Descretization","528908ce":"## <center> 5. Bi-Variate Analysis<\/center>","2a30b209":"## 8.5 Decision Tree Classifier","5c02b2a6":"### 8.1.5 Actual vs Prediction","44f2d06f":"### 8.1.4 Logistic Regression Evaluation","a19c2576":"If SVM, Naive Bayes and Decision Tree are compared Naive Bayes has performed better than others in terms of accuracy, f1 score as well as area under the curve. Although the scores are not too high for any of the algorithm but if hyperparameters are tuned, we will be able to see better performances. The performances calculated above are based on when complete data set without any feature selection was passed to the algorithms. With selected features the performance of all algorithms was not good (not listed in table).\n\nIf we are to select the best among all the algorithms above then logistic regression performed well than all others.","76bc4a19":"The purpose of univriate analysis here is to only check and get the idea of distribution of attributes in the data set","033f4594":"#### Loan Amount","d001d97c":"#### 8.1.5.1 Actual vs Predicted Graph","2708a728":"### 8.1.2 Train Test Split","2201af18":"For Numerical Attributed we will check correlation of each with target variable. For this will first manually encode our target variable to 0 and 1.","043f3b00":"### 8.6.2 SVM Evaluation","8cef8e8e":"### 8.2.5 Logistic Regression Evaluation (Selected Features)","ff6cdc8f":"To find if any categorical attribute is independent of our target variable (Loan Status), we will calculate chi square statistics.","d027d6ac":"1.\tData Loading\n\n2.\tData Description\n\n3.\tData Preparation\n    3.1.\tHandling Null and Incorrect Values\n4.\tUnivariate Analysis\n\n5.\tBi Variate Analysis\n    5.1.\tBarplots for Categorical Attributes\n    5.1.1.\tResults\n    5.2.\tBox plots for Numerical Attributes\n    5.2.1.\tResults\n6.\tFeatures Selection\n    6.1.\tChi Square Statistics\n    6.2.\tPearson Correlation of Target Variables and Numerical Attributes\n    6.2.1.\tManual Encoding of Target Variable\n    6.2.2.\tPearson Correlation\n7.\tPreprocessing\n    7.1.\tFeature Encoding and Discretization\n    7.1.1.\tManual Encoding\n    7.1.2.\tOne Hot Encoding\n8.\tMachine Learning Models\n    8.1.\tLogistic Regression\n        8.1.1. Transforming Features and Target Variables into Arrays\n        8.1.2. Train Test Split\n        8.1.3. Applying Logistic Regression\n        8.1.4. Logistic Regression Evaluation\n        8.1.5. Actual vs Prediction\n        8.1.5.1. Actual vs Predicted Graph\n    8.2.\tLogistic Regression (Features Selected)\n        8.2.1. Dropping Attributes\n        8.2.2. Transforming into arrays\n        8.2.3. Train Test Split\n        8.2.4. Applying Logistic Regression (Selected Features)\n        8.2.5. Logistic Regression Evaluation (Selected Features)\n    8.3.\tKNN Algorithm\n        8.3.1. Applying KNN\n        8.3.2. Knn Evaluation\n    \n    8.4.\tNaive Bayes Classifier\n        8.4.1. Applying Naive Bayes Classifier\n        8.4.2. Naive Bayes Evaluation\n\n    8.5.\tDecision Tree Classifier\n        8.5.1. Applying Decision Tree\n        8.5.1. Decision Tree Evaluation\n    \n    8.6. SVM Classifier\n        8.6.1. Applying SVM Classifier\n        8.6.2. SVM Evaluation\n      \n    8.7. Random Forest Classifier\n        8.7.1. Applying Random Forest Classifer\n        8.7.2. Random Forest Evaluation\n    \n    8.8. Model Metrics Comparision","8fe21024":"### 8.6.1. Applying SVM Classifier","80689982":"# <center><u> Credit DataSet <\/u><\/center>","68da0f07":"### 8.3.2 Knn Evaluation","001f3e29":"### 8.4.1 Applying Naive Bayes Classifier","441d1b4b":"## 2. Data Description","1a1fa45f":"<b> 5.1.1 Results: <\/b>\n    \nGender: There are more males than females who have been granted Loan.\n\nMarital Status: From the plots it seems that people who are married have a greater proability for loan.\n    \nDependents: From the plots it seems that people who have no dependents are the one who have mostly applied for the loan. And interestingly are also in majority who have been granted loan.\n\nEducation: Majority of the people who applied for loan are graduates and also have a higher probability of getting loan as compared to non graduates.\n\nSelf Employed: A large portion of the people who applied for loan are of salaried class and majority of them have been granted loan.\n\nCredit History:It is pretty clear from the plot that applicants who have a cerdit history are very likely to be granted loan.\n\nProperty Area: Customers residing in Semiurban area seem to have a higher probability of getting loans.\n\n\n\n","08d38804":"## 6.2 Pearson Correlation of Target Variables and Numerical Attributes\\","dee21658":"## 3. Data Preparation","e10c3690":"### 3.1 Handling Null and Incorrect Values","ea73ae56":"#### 6.2.2 Pearson Correlation","ba0baa33":"#### 6.2.1 Manual Encoding of Target Variable","aa5a0934":"<b> Based on chi square statistics and Pearson correlation matrix we can drop some attributes but first we will train and evaluate model without dropping any and later on evaluate model using selected features. <\/b>","471136ca":"### 6.1. Chi Square Statistics","f80d329a":"#### 7.1.2 One Hot Encoding"}}