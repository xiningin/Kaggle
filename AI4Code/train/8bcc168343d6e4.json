{"cell_type":{"c5abc756":"code","1c448f63":"code","f352c5f4":"code","b0475570":"code","dbfc4cbd":"code","6808318e":"code","ab46b1db":"code","3f7548b6":"code","89c57ec9":"code","fa9acd71":"code","2beba93b":"code","104ec1fc":"code","3bd63d6f":"code","9f614995":"code","eb8e803f":"code","6b2db3a4":"code","2e7f8c53":"code","bfbd5a0f":"code","77a747f5":"code","f0e36ba1":"code","cda11ac3":"code","0baa0d87":"code","a01cbfa7":"code","54ad42ab":"code","a4fe9629":"code","5ca86ed3":"code","ac121e7b":"code","3d9fe3ac":"code","4ad9237c":"code","726d2980":"markdown","bb5b5b07":"markdown","96b7e9cb":"markdown","66f9c829":"markdown","9a85d40c":"markdown","776f747d":"markdown","fd76c60a":"markdown","9edc82ac":"markdown","1e4a27c5":"markdown","ccf16e4c":"markdown","be705b47":"markdown","6284189f":"markdown","72e4bcc5":"markdown","edcac566":"markdown","96333134":"markdown","f1bdcc35":"markdown","e7886750":"markdown","072e5ecc":"markdown","50f7e76a":"markdown","1472cda6":"markdown"},"source":{"c5abc756":"from fastai.vision import *\nfrom sklearn.metrics import roc_auc_score\n\nimport warnings\nwarnings.simplefilter('ignore', category=FutureWarning)\nwarnings.simplefilter('ignore', category=UserWarning)\n\n%matplotlib inline","1c448f63":"PATH = Path('..\/input')\nPATH.ls()","f352c5f4":"train_df = pd.read_csv(PATH\/'train_labels.csv')\nprint(train_df.shape)\ntrain_df.head()","b0475570":"train_df['label'].value_counts(normalize=True)","dbfc4cbd":"src = (ImageItemList.from_csv(PATH, folder='train', csv_name='train_labels.csv', suffix='.tif')\n      .random_split_by_pct(0.1, seed=77)\n      .label_from_df()\n      .add_test_folder())","6808318e":"tfms = get_transforms(do_flip=True, flip_vert=True, max_rotate=0, max_zoom=1., max_lighting=0.05, max_warp=0)\n\ndata = (src.transform(tfms, size=96, resize_method=ResizeMethod.SQUISH)\n       .databunch(bs=64, path='.'))\n\ndata.normalize(imagenet_stats);","ab46b1db":"data.show_batch(rows=5, figsize=(15, 15))","3f7548b6":"class FocalLoss(nn.Module):\n    def __init__(self, alpha=1., gamma=1.):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n\n    def forward(self, inputs, targets, **kwargs):\n        CE_loss = nn.CrossEntropyLoss(reduction='none')(inputs, targets)\n        pt = torch.exp(-CE_loss)\n        F_loss = self.alpha * ((1-pt)**self.gamma) * CE_loss\n        return F_loss.mean()\n\n    \ndef roc_score(inp, target):\n    _, indices = inp.max(1)\n    return torch.Tensor([roc_auc_score(target, indices)])[0]","89c57ec9":"loss_func = FocalLoss(gamma=1.)\nlearn = create_cnn(data, models.densenet121, metrics=[accuracy, roc_score], loss_func=loss_func)","fa9acd71":"learn.lr_find()","2beba93b":"learn.recorder.plot()","104ec1fc":"learn.fit_one_cycle(2, 1e-3)","3bd63d6f":"learn.recorder.plot_lr(show_moms=True)","9f614995":"learn.recorder.plot_losses()","eb8e803f":"learn.save('stage-1')","6b2db3a4":"learn.load('stage-1');","2e7f8c53":"interp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(title='Confusion matrix')","bfbd5a0f":"learn.unfreeze()","77a747f5":"learn.lr_find()","f0e36ba1":"learn.recorder.plot()","cda11ac3":"max_lr = 1e-4\nlearn.fit_one_cycle(4, slice(1e-6, max_lr))","0baa0d87":"learn.save('stage-2')","a01cbfa7":"learn.load('stage-2');","54ad42ab":"interp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(title='Confusion matrix')","a4fe9629":"auc_val = learn.validate()[2].item()","5ca86ed3":"preds, y = learn.TTA(beta=0.4, ds_type=DatasetType.Test)\npreds = torch.softmax(preds, dim=1)[:, 1].numpy()","ac121e7b":"test_ids = [f.stem for f in learn.data.test_ds.items]\nsubm = pd.read_csv(PATH\/'sample_submission.csv')\norig_ids = list(subm['id'])","3d9fe3ac":"def create_submission(orig_ids, test_ids, preds):\n    preds_dict = dict((k, v) for k, v in zip(test_ids, preds))\n    pred_cor = [preds_dict[id] for id in orig_ids]\n    df = pd.DataFrame({'id':orig_ids,'label':pred_cor})\n    df.to_csv(f'submission_{auc_val}.csv', header=True, index=False)\n    \n    return df","4ad9237c":"test_df = create_submission(orig_ids, test_ids, preds)\ntest_df.head()","726d2980":"See how the learning rate and momentum varies with the training","bb5b5b07":"## **Data loading**","96b7e9cb":"Here you will see the loss gets a little bump after the initial drop. This is due to the increase in learning rate in the first half cycle and it will drive the model out of the local minima. In the second cycle the learning rate will decrease gradually which will help obtain the global minima.","66f9c829":"Fit the plot for 2 cycle","9a85d40c":"## **Testing**\nTTA is basically test time augmentation. You can read more about it in the fastai [docs](https:\/\/docs.fast.ai\/basic_train.html#TTA\n). <br>\nHere we need to apply sigmoid since we have to submit the probability. TTA will give logits not the probabilities because we have used FocalLoss here which is not defined in fastai yet but it's not needed for the submission.","776f747d":"Take care of the sequence of the ids to be submitted in **sample_submission.csv**","fd76c60a":"**get_transforms()** defines all the required data augmentation we need such as vertical flip,  horizontal flip, zoom, rotation etc. You can read them [here](https:\/\/docs.fast.ai\/vision.transform.html#get_transforms).","9edc82ac":"## **Model and training**\nWe will define focal loss and roc metric","1e4a27c5":"Find learning rate and plot it","ccf16e4c":"## Let's get started\nLoad the vision package which also loads other required packages <br>\nLoad required metric function <br>","be705b47":"Unfreeze the other two layers and find an optimal learning rate again","6284189f":"To choose the optimal learning rate find the point where the loss is minimum and then either find a point before that where the plot has a steepest slope or just divide that point by 10, both works fine.","72e4bcc5":"## **Intro**\nfastai is a deep-learning framework built on top of Pytorchv1. Recently it got update to [v1](https:\/\/github.com\/fastai\/fastai) which is a huge update from previous version and contains preety amazing and cool apis to load data, create models and train them. You can get the overview on how to use that from their [MOOC](https:\/\/course.fast.ai\/) and can get started with deep learning.\n\nThis kernel uses **fastai.vison** for image classification task. We will use transfer learning using densenet121 model from [torchvision](https:\/\/pytorch.org\/docs\/stable\/torchvision\/index.html) along with [focal loss](https:\/\/arxiv.org\/pdf\/1708.02002.pdf). Following techniques we will be using are already implemented in fastai.\n \n1. Learning rate finder \n2. Cyclic learning rate ([paper](https:\/\/arxiv.org\/pdf\/1506.01186.pdf))\n3. Discriminative learning rates\n4. Data augmentation\n5. Transfer learning with pretrained model\n6. Test time augmentation","edcac566":"Save and load your model","96333134":"Now here we define the learning rates using slice(). This function will create different learning rates for different groups to use discriminative learning. In short the main idea is to fine-tune the layers with different learning rates in which the initial layers will be optimized with lower learning rate and final layers with higher learning rate. <br>\nTrain more...","f1bdcc35":"Let's see the losses.","e7886750":"Define the path using [pathlib](https:\/\/docs.python.org\/3\/library\/pathlib.html).","072e5ecc":"So the ratio is around 3:2","50f7e76a":"## **Useful links**\n* [How fastai data block api works](https:\/\/blog.usejournal.com\/finding-data-block-nirvana-a-journey-through-the-fastai-data-block-api-c38210537fe4)\n* [One cycle scheduler](https:\/\/sgugger.github.io\/the-1cycle-policy.html)\n* [Motivation](https:\/\/www.fast.ai\/2019\/01\/02\/one-year-of-deep-learning)\n* [What can you do with deep learning?](https:\/\/www.fast.ai\/2019\/02\/21\/dl-projects\/)\n* [Humpback Whale Identification Competition Starter Pack](https:\/\/github.com\/radekosmulski\/whale)\n\n","1472cda6":"Let's create a learner which will contain our data, model and metrics. <br>\nAlso we need to define the loss function we want to use. By default fastai uses softmax or cross-entropy loss for classification task but we want to use focal loss."}}