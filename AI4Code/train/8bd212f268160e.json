{"cell_type":{"9da9582d":"code","77e56d31":"code","99efdd05":"code","af3cc511":"code","54f00b78":"code","10dec7f4":"code","2ae94d1b":"code","1460d195":"code","69427ff9":"code","71c96795":"code","085948b2":"code","40436ed5":"code","fb2b82e2":"code","2154c257":"code","a0a91c96":"code","24adc4a1":"code","5b99bf39":"code","0f5a1a26":"code","23dc2e3b":"code","53bd85fa":"code","570d5dcf":"code","a5e2162b":"code","eae6106a":"code","7994099b":"code","612c8cb9":"code","0d450324":"code","57265b38":"code","f0a4bbed":"code","2147e304":"code","e0242bca":"code","2490c1ea":"code","3d1c73fc":"code","d39af188":"code","678a5906":"code","8ba8b494":"code","97ab3dde":"code","68fcd35c":"code","c8786f54":"code","92a4f251":"code","080da0bd":"markdown","380b628b":"markdown","5d4cddaa":"markdown","c3511f49":"markdown","d56f7b18":"markdown","6321b9a9":"markdown","94f37ba1":"markdown","7c550d90":"markdown","d608409d":"markdown","c8a56b6e":"markdown","181d0364":"markdown","c72600e8":"markdown","167d32d3":"markdown","78a57204":"markdown"},"source":{"9da9582d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport imblearn\nimport xgboost \n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","77e56d31":"df_train = pd.read_csv(os.path.join('..\/input\/train', 'train.csv'))\ndf_test = pd.read_csv(os.path.join('..\/input\/test', 'test.csv'))\ndf_breed_labels = pd.read_csv(os.path.join('..\/input', 'breed_labels.csv'))\ndf_color_labels = pd.read_csv(os.path.join('..\/input', 'color_labels.csv'))","99efdd05":"df_train.shape, df_test.shape","af3cc511":"df_breed_labels.head()","54f00b78":"df_train.isnull().sum()","10dec7f4":"df_train.head()","2ae94d1b":"adoption_0 = (df_train['AdoptionSpeed'] == 0).sum()\nadoption_1 = (df_train['AdoptionSpeed'] == 1).sum()\nadoption_2 = (df_train['AdoptionSpeed'] == 2).sum()\nadoption_3 = (df_train['AdoptionSpeed'] == 3).sum()\nadoption_4 = (df_train['AdoptionSpeed'] == 4).sum()\nadoption_0, adoption_1, adoption_2, adoption_3, adoption_4","1460d195":"df_nlp = df_train[['Description', 'AdoptionSpeed']]\ndf_nlp.shape","69427ff9":" df_nlp.head()","71c96795":"index_delete = np.array(df_nlp[df_nlp['Description'].isnull() == True].index)\nindex_delete","085948b2":"df_nlp = df_nlp.drop(index_delete)\ndf_nlp.shape","40436ed5":"import random\nindex_adoption_0 = np.array(df_nlp[df_nlp['AdoptionSpeed'] == 0].index)\nindex_adoption_1 = np.array(df_nlp[df_nlp['AdoptionSpeed'] == 1].index)\nindex_adoption_2= np.array(df_nlp[df_nlp['AdoptionSpeed'] == 2].index)\nindex_adoption_3 = np.array(df_nlp[df_nlp['AdoptionSpeed'] == 3].index)\nindex_adoption_4 = np.array(df_nlp[df_nlp['AdoptionSpeed'] == 4].index)\nindex_adoption_1_reduc = [index_adoption_1[i] for i in range(int(len(index_adoption_0)))]\nindex_adoption_2_reduc = [index_adoption_2[i] for i in range(int(len(index_adoption_0)))]\nindex_adoption_3_reduc = [index_adoption_3[i] for i in range(int(len(index_adoption_0)))]\nindex_adoption_4_reduc = [index_adoption_4[i] for i in range(int(len(index_adoption_0)))]","fb2b82e2":"X = pd.concat([df_nlp['Description'].reindex(index_adoption_0), df_nlp['Description'].reindex(index_adoption_1_reduc),\n               df_nlp['Description'].reindex(index_adoption_2_reduc), df_nlp['Description'].reindex(index_adoption_3_reduc),\n               df_nlp['Description'].reindex(index_adoption_4_reduc)])\n\ny = pd.concat([df_nlp['AdoptionSpeed'].reindex(index_adoption_0), df_nlp['AdoptionSpeed'].reindex(index_adoption_1_reduc),\n               df_nlp['AdoptionSpeed'].reindex(index_adoption_2_reduc), df_nlp['AdoptionSpeed'].reindex(index_adoption_3_reduc),\n               df_nlp['AdoptionSpeed'].reindex(index_adoption_4_reduc)])\nX.shape, y.shape","2154c257":"from sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=.2, random_state=42, stratify=y)\nX_train.shape, y_train.shape, X_valid.shape, y_valid.shape","a0a91c96":"import nltk\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer, PorterStemmer","24adc4a1":"def tokenize(data):\n    tokenized_docs = [word_tokenize(doc) for doc in data]\n    alpha_tokens = [[t.lower() for t in doc if t.isalpha() == True] for doc in tokenized_docs]\n    lemmatizer = WordNetLemmatizer()\n    lem_tokens = [[lemmatizer.lemmatize(alpha) for alpha in doc] for doc in alpha_tokens]\n    X_stem_as_string = [\" \".join(x_t) for x_t in lem_tokens]\n    return X_stem_as_string","5b99bf39":"X_train_tk = tokenize(X_train)\nX_valid_tk = tokenize(X_valid)\n","0f5a1a26":"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.pipeline import Pipeline","23dc2e3b":"vct = CountVectorizer(stop_words='english', lowercase=False)\nsvd = TruncatedSVD(n_components=200, random_state=42)\ntfvec = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), lowercase=False)\n\npreprocessing_pipe = Pipeline([\n    ('vectorizer', tfvec),\n    ('svd', svd)   \n])","53bd85fa":"lsa_train = preprocessing_pipe.fit_transform(X_train_tk)\nlsa_train.shape","570d5dcf":"sns.scatterplot(lsa_train[:, 0], lsa_train[:, 1], hue=y_train);","a5e2162b":"components = pd.DataFrame(data=svd.components_, columns=preprocessing_pipe.named_steps['vectorizer'].get_feature_names())","eae6106a":"fig, axes = plt.subplots(10, 2, figsize=(18, 30))\nfor i, ax in enumerate(axes.flat):\n    components.iloc[i].sort_values(ascending=False)[:10].sort_values().plot.barh(ax=ax)","7994099b":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import classification_report\nfrom sklearn.naive_bayes import MultinomialNB","612c8cb9":"rf = RandomForestClassifier()\nmb = MultinomialNB()\npipe = Pipeline([\n    ('vectorizer', tfvec),\n    ('rf', mb)\n])","0d450324":"pipe.fit(X_train_tk, y_train)\ny_pred = pipe.predict(X_valid_tk)","57265b38":"print(classification_report(y_valid, y_pred))","f0a4bbed":"df_train = df_train.drop(['Name', 'Description', 'RescuerID', 'PetID'], axis=1)\ndf_test = df_test.drop(['Name', 'Description'], axis=1)\nX_test = df_test.drop(['RescuerID', 'PetID'], axis=1)","2147e304":"cor_mat = df_train[:].corr()\nmask = np.array(cor_mat)\nmask[np.tril_indices_from(mask)] = False\nfig = plt.gcf()\nfig.set_size_inches(80,15)\nsns.heatmap(data=cor_mat, mask=mask, square=True, annot=True, cbar=True);","e0242bca":"var = 'Type'\ndata = pd.concat([df_train['AdoptionSpeed'], df_train[var]], axis=1)\nplt.xlabel('Type')\nplt.ylabel('AdoptionSpeed')\nsns.boxplot(x=var, y=\"AdoptionSpeed\", data=data);","2490c1ea":"X = df_train.drop('AdoptionSpeed', axis=1)\ny = df_train['AdoptionSpeed']","3d1c73fc":"from sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import train_test_split","d39af188":"## Data spliting \nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.25, shuffle=y, random_state=42)\n(X_train.shape, y_train.shape), (X_valid.shape, y_valid.shape)","678a5906":"## Data well-balanced\nfrom imblearn.over_sampling import SMOTE\nsmote = SMOTE(ratio='minority')\nX_train_sm, y_train_sm = smote.fit_sample(X_train, y_train)\nX_valid_sm, y_valid_sm = smote.fit_sample(X_valid, y_valid)","8ba8b494":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.metrics import accuracy_score","97ab3dde":"rf = RandomForestClassifier(max_depth=6, n_estimators=24, max_features=19, random_state=1)\nrf.fit(X_train_sm, y_train_sm)\ny_predic = rf.predict(X_train_sm)\nacc_score = accuracy_score(y_train_sm, y_predic)\nacc_score_valid = accuracy_score(y_valid_sm, rf.predict(X_valid_sm))\nacc_score, acc_score_valid","68fcd35c":"xgb = XGBClassifier(max_depth=3, n_estimators=200, learning_rate=0.19, random_state=42)\nxgb.fit(X_train_sm, y_train_sm)\ny_predic = xgb.predict(X_train_sm)\nacc_score = accuracy_score(y_train_sm, y_predic)\nacc_score_valid = accuracy_score(y_valid_sm, xgb.predict(X_valid_sm))\nacc_score, acc_score_valid","c8786f54":"y_pred_true = xgb.predict(X_test.as_matrix())","92a4f251":"model_submission  = pd.DataFrame(y_pred_true).apply(np.round)\nsubmission = pd.DataFrame(data={\"PetID\" : df_test[\"PetID\"], \n                                   \"AdoptionSpeed\" : model_submission[0]})\nsubmission.AdoptionSpeed = submission.AdoptionSpeed.astype(int)\nsubmission.to_csv(\"submission.csv\", index=False)\n","080da0bd":"** Classification Report **","380b628b":"df_train= pd.concat([df_train.iloc[index_adoption_0, :], df_train.iloc[index_adoption_1_reduc, :],\n                     df_train.iloc[index_adoption_2_reduc, :], df_train.iloc[index_adoption_3_reduc, :],\n                     df_train.iloc[index_adoption_4_reduc, :]])","5d4cddaa":"** Tokenizing : creation of a tokenize's function permitting to automatically tokenize our train and valid set **","c3511f49":"## Machine Learning Model ","d56f7b18":"## Quick EDA","6321b9a9":"Warning : proportion for adoption_0 is unbalanced compared to ohter.","94f37ba1":"** Creation of a machine learning model on NLP ** ","7c550d90":"Name and description can be dropped : it doesn't have any influence on adoption speed","d608409d":"## Data normalisation\nrb = RobustScaler()\nX_train_sm = rb.fit_transform(X_train)\nX_valid_sm = rb.fit_transform(X_valid)","c8a56b6e":"## Preprocessing with NLTK","181d0364":"** Preprocessing pipeline **","c72600e8":"## AdoptionSpeed Predictions ","167d32d3":"## NLP on the description colulmn ","78a57204":"** Split datas in train and valid **"}}