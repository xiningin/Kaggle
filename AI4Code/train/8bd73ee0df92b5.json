{"cell_type":{"57d0f39e":"code","f91ee062":"code","2f14d45a":"code","95a03c70":"code","f6435277":"code","c64bf2db":"markdown","fc42fdce":"markdown","bae6b59d":"markdown","303a68e7":"markdown","4259ad1c":"markdown","76414f08":"markdown","eb31a0b3":"markdown","c31f8679":"markdown"},"source":{"57d0f39e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f91ee062":"# naive approach to normalizing the data before splitting the data and evaluating the model\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n# define dataset\nX, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5,\nrandom_state=7)\n# standardize the dataset\nscaler = MinMaxScaler()\nX = scaler.fit_transform(X)\n# split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n# fit the model\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n# evaluate the model\nyhat = model.predict(X_test)\n# evaluate predictions\naccuracy = accuracy_score(y_test, yhat)\nprint('Accuracy: %.3f' % (accuracy*100))\n","2f14d45a":"# correct approach for normalizing the data after the data is split before the model is evaluated\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n# define dataset\nX, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5,\nrandom_state=7)\n# split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n# define the scaler\nscaler = MinMaxScaler()\n# fit on the training dataset\nscaler.fit(X_train)\n# scale the training dataset\nX_train = scaler.transform(X_train)\n# scale the test dataset\nX_test = scaler.transform(X_test)\n# fit the model\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n# evaluate the model\nyhat = model.predict(X_test)\n# evaluate predictions\naccuracy = accuracy_score(y_test, yhat)\nprint('Accuracy: %.3f' % (accuracy*100))","95a03c70":"# naive data preparation for model evaluation with k-fold cross-validation\nfrom numpy import mean\nfrom numpy import std\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LogisticRegression\n# define dataset\nX, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5,\nrandom_state=7)\n# standardize the dataset\nscaler = MinMaxScaler()\nX = scaler.fit_transform(X)\n# define the model\nmodel = LogisticRegression()\n# define the evaluation procedure\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n# evaluate the model using cross-validation\nscores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n# report performance\nprint('Accuracy: %.3f (%.3f)' % (mean(scores)*100, std(scores)*100))\n","f6435277":"# correct data preparation for model evaluation with k-fold cross-validation\nfrom numpy import mean\nfrom numpy import std\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\n# define dataset\nX, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5,\nrandom_state=7)\n# define the pipeline\nsteps = list()\nsteps.append(('scaler', MinMaxScaler()))\nsteps.append(('model', LogisticRegression()))\npipeline = Pipeline(steps=steps)\n# define the evaluation procedure\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n# evaluate the model using cross-validation\nscores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n# report performance\nprint('Accuracy: %.3f (%.3f)' % (mean(scores)*100, std(scores)*100))","c64bf2db":"This avoids data leakage as the calculation of the minimum and maximum value for each\ninput variable is calculated using only the training dataset (X train) instead of the entire\ndataset (X). The model can then be evaluated as before.","fc42fdce":" Example of evaluating a model using a cross-validation without data leakage.\n","bae6b59d":"Data preparation without data leakage when using cross-validation is slightly more challenging.\nIt requires that the data preparation method is prepared on the training set and applied to the\ntrain and test sets within the cross-validation procedure, e.g. the groups of folds of rows.\n\n This can be achieved using the Pipeline class. This class takes a list of steps\nthat define the pipeline. Each step in the list is a tuple with two elements. The first element is\nthe name of the step (a string) and the second is the configured object of the step, such as a\ntransform or a model.","303a68e7":"**Data Preparation With k-fold Cross-Validation**","4259ad1c":"Running the example normalizes the data correctly within the cross-validation folds of the\nevaluation procedure to avoid data leakage.\n","76414f08":" Example of evaluating a model using a train-test split without data leakage.","eb31a0b3":"Example of evaluating a model using a train-test split with data leakage.","c31f8679":" Example of evaluating a model using a cross-validation with data leakage.\n"}}