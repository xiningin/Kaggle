{"cell_type":{"8ad1b93e":"code","f49e1909":"code","2f491b13":"code","cf1b38f4":"code","1e167974":"code","5d34703c":"code","451e803a":"code","422f84d8":"code","0c99a2a9":"code","51dc0bbc":"code","2100b812":"code","7b75f67b":"code","f303b585":"code","be88d3b6":"code","c0e0da61":"code","b5b0cfbe":"code","f52791ac":"code","4b04b627":"code","6aab4eed":"code","2bd664ce":"code","d7fdebf9":"code","ac02ff46":"code","8d2b81ad":"code","500c16db":"code","ae078110":"code","adf98d1a":"code","d48ef7f8":"code","36c59b7e":"code","082e6a14":"code","3134bc03":"code","49339865":"code","c481eae5":"code","9120e92e":"code","754b5375":"code","992c52f1":"code","b9e53fef":"code","6b201d2a":"code","08172635":"code","66b77fb9":"code","035f353b":"code","f5c3b565":"code","f7905ae5":"code","8ec644c3":"code","93ae0d1f":"code","0c3de547":"code","84fd8c29":"code","c07718f0":"code","d6a0ec95":"code","20fdf365":"code","adafa3dc":"code","4c490c33":"code","458f5b84":"code","bf971f10":"code","8a464517":"code","8d1bd24c":"code","218c35e7":"code","06e6b23d":"code","54f6b8ea":"code","6733218d":"code","35db8084":"code","b3e3cdd7":"markdown","66c8c7ed":"markdown","7af01f62":"markdown","813fcc5f":"markdown","b787e76b":"markdown","5ffa94e5":"markdown","6b210b40":"markdown","abb28d23":"markdown","deb2b687":"markdown","43f24fbf":"markdown","3e31df4b":"markdown","de611ed3":"markdown","406f997a":"markdown","f777c80a":"markdown","ca99afa8":"markdown","1a890621":"markdown","8369b845":"markdown","cff8b1b6":"markdown","e8cb7fc2":"markdown","17218758":"markdown","e8dbc790":"markdown","1cc8a876":"markdown","16f1fbb4":"markdown","7a457b96":"markdown","8511e153":"markdown","8da1ce6e":"markdown","bc4141e2":"markdown","43e858b4":"markdown","a9f1bb07":"markdown","07afc5ca":"markdown","578bdcff":"markdown","9a3c2d66":"markdown","c78b1849":"markdown"},"source":{"8ad1b93e":"import warnings                        # To ignore any warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\n%pylab inline\nimport os\nimport pandas as pd\nimport librosa\nimport librosa.display\nimport glob \nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_format = 'retina'","f49e1909":"# gather software versions\nimport tensorflow as tf; print('tensorflow version: ', tf.__version__)\nimport keras; print('keras version: ',keras.__version__)","2f491b13":"# parent folder of sound files\nINPUT_DIR=\"..\/input\"\n# 16 KHz\nSAMPLE_RATE = 16000\n# seconds\nMAX_SOUND_CLIP_DURATION=12   ","cf1b38f4":"# check what is inside each directory and content\n!pwd\n!ls -all ..\/input","1e167974":"set_a=pd.read_csv(INPUT_DIR+\"\/set_a.csv\")\nset_a.head()","5d34703c":"set_a_timing=pd.read_csv(INPUT_DIR+\"\/set_a_timing.csv\")\nset_a_timing.head()","451e803a":"set_b=pd.read_csv(INPUT_DIR+\"\/set_b.csv\")\nset_b.head()","422f84d8":"#merge both set-a and set-b\nframes = [set_a, set_b]\ntrain_ab=pd.concat(frames)\ntrain_ab.describe()","0c99a2a9":"#get all unique labels\nnb_classes=train_ab.label.unique()\n\nprint(\"Number of training examples=\", train_ab.shape[0], \"  Number of classes=\", len(train_ab.label.unique()))\nprint (nb_classes)","51dc0bbc":"# visualize data distribution by category\ncategory_group = train_ab.groupby(['label','dataset']).count()\nplot = category_group.unstack().reindex(category_group.unstack().sum(axis=1).sort_values().index)\\\n          .plot(kind='bar', stacked=True, title=\"Number of Audio Samples per Category\", figsize=(16,5))\nplot.set_xlabel(\"Category\")\nplot.set_ylabel(\"Samples Count\");\n\nprint('Min samples per category = ', min(train_ab.label.value_counts()))\nprint('Max samples per category = ', max(train_ab.label.value_counts()))","2100b812":"print('Minimum samples per category = ', min(train_ab.label.value_counts()))\nprint('Maximum samples per category = ', max(train_ab.label.value_counts()))","7b75f67b":"normal_file=INPUT_DIR+\"\/set_a\/normal__201106111136.wav\"","f303b585":"# heart it\nimport IPython.display as ipd\nipd.Audio(normal_file) ","be88d3b6":"# Load use wave \nimport wave\nwav = wave.open(normal_file)\nprint(\"Sampling (frame) rate = \", wav.getframerate())\nprint(\"Total samples (frames) = \", wav.getnframes())\nprint(\"Duration = \", wav.getnframes()\/wav.getframerate())","c0e0da61":"# Load use scipy\nfrom scipy.io import wavfile\nrate, data = wavfile.read(normal_file)\nprint(\"Sampling (frame) rate = \", rate)\nprint(\"Total samples (frames) = \", data.shape)\nprint(data)","b5b0cfbe":"# plot wave by audio frames\nplt.figure(figsize=(16, 3))\nplt.plot(data, '-', );","f52791ac":"# Load using Librosa\ny, sr = librosa.load(normal_file, duration=5)   #default sampling rate is 22 HZ\ndur=librosa.get_duration(y)\nprint (\"duration:\", dur)\nprint(y.shape, sr)","4b04b627":"# librosa plot\nplt.figure(figsize=(16, 3))\nlibrosa.display.waveplot(y, sr=sr)","6aab4eed":"# murmur case\nmurmur_file=INPUT_DIR+\"\/set_a\/murmur__201108222231.wav\"\ny2, sr2 = librosa.load(murmur_file,duration=5)\ndur=librosa.get_duration(y)\nprint (\"duration:\", dur)\nprint(y2.shape,sr2)","2bd664ce":"# heart it\nimport IPython.display as ipd\nipd.Audio(murmur_file) ","d7fdebf9":"# show it\nplt.figure(figsize=(16, 3))\nlibrosa.display.waveplot(y2, sr=sr2)","ac02ff46":"# Extrasystole case\nextrastole_file=INPUT_DIR+\"\/set_b\/extrastole__127_1306764300147_C2.wav\"\ny3, sr3 = librosa.load(extrastole_file, duration=5)\ndur=librosa.get_duration(y)\nprint (\"duration:\", dur)\nprint(y3.shape,sr3)","8d2b81ad":"# heart it\nimport IPython.display as ipd\nipd.Audio(extrastole_file) ","500c16db":"# show it\nplt.figure(figsize=(16, 3))\nlibrosa.display.waveplot(y3, sr=sr3)","ae078110":"# sample file\nartifact_file=INPUT_DIR+\"\/set_a\/artifact__201012172012.wav\"\ny4, sr4 = librosa.load(artifact_file, duration=5)\ndur=librosa.get_duration(y)\nprint (\"duration:\", dur)\nprint(y4.shape,sr4)","adf98d1a":"# heart it\nimport IPython.display as ipd\nipd.Audio(artifact_file) ","d48ef7f8":"# show it\nplt.figure(figsize=(16, 3))\nlibrosa.display.waveplot(y4, sr=sr4)","36c59b7e":"# sample file\nextrahls_file=INPUT_DIR+\"\/set_a\/extrahls__201101070953.wav\"\ny5, sr5 = librosa.load(extrahls_file, duration=5)\ndur=librosa.get_duration(y)\nprint (\"duration:\", dur)\nprint(y5.shape,sr5)","082e6a14":"# heart it\nimport IPython.display as ipd\nipd.Audio(extrahls_file) ","3134bc03":"# show it\nplt.figure(figsize=(16, 3))\nlibrosa.display.waveplot(y5, sr=sr5)","49339865":"# Here's a sample generate mfccs from a wave file\nnormal_file=INPUT_DIR+\"\/set_a\/normal__201106111136.wav\"\n#y, sr = librosa.load(sample_file, offset=7, duration=7)\ny, sr = librosa.load(normal_file)\nmfccs = librosa.feature.mfcc(y=y, sr=sr)\nprint (mfccs)","c481eae5":"# Use a pre-computed log-power Mel spectrogram\nS = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128,fmax=8000)\nlog_S=librosa.feature.mfcc(S=librosa.power_to_db(S))\nprint (log_S)","9120e92e":"# Get more components\nmfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n#print (mfccs)","754b5375":"# Visualize the MFCC series\n# Mel-frequency cepstral coefficients (MFCCs)\nplt.figure(figsize=(12, 3))\nlibrosa.display.specshow(mfccs, x_axis='time')\nplt.colorbar()\nplt.title('Mel-frequency cepstral coefficients (MFCCs)')\nplt.tight_layout()","992c52f1":"# Compare different DCT bases\nm_slaney = librosa.feature.mfcc(y=y, sr=sr, dct_type=2)\n\n#m_dct1 = librosa.feature.mfcc(y=y, sr=sr, dct_type=1)\nplt.figure(figsize=(12, 6))\n#plt.subplot(3, 1, 1)\n#librosa.display.specshow(m_dct1, x_axis='time')\n#plt.title('Discrete cosine transform (dct_type=1)')\n#plt.colorbar()\nm_htk = librosa.feature.mfcc(y=y, sr=sr, dct_type=3)\nplt.subplot(3, 1, 2)\nlibrosa.display.specshow(m_slaney, x_axis='time')\nplt.title('RASTAMAT \/ Auditory toolbox (dct_type=2)')\nplt.colorbar()\nplt.subplot(3, 1, 3)\nlibrosa.display.specshow(m_htk, x_axis='time')\nplt.title('HTK-style (dct_type=3)')\nplt.colorbar()\nplt.tight_layout()","b9e53fef":"# Get onset times from a signal\nonset_frames = librosa.onset.onset_detect(y=y, sr=sr)\nlibrosa.frames_to_time(onset_frames, sr=sr)","6b201d2a":"# use a pre-computed onset envelope\no_env = librosa.onset.onset_strength(y, sr=sr)\ntimes = librosa.frames_to_time(np.arange(len(o_env)), sr=sr)\nonset_frames = librosa.onset.onset_detect(onset_envelope=o_env, sr=sr)","08172635":"# visualize it\nD = np.abs(librosa.stft(y))\nplt.figure(figsize=(16, 6))\nax1 = plt.subplot(2, 1, 1)\nlibrosa.display.specshow(librosa.amplitude_to_db(D, ref=np.max),x_axis='time', y_axis='log')\nplt.title('Power spectrogram')\nplt.subplot(2, 1, 2, sharex=ax1)\n\nplt.plot(times, o_env, label='Onset strength')\nplt.vlines(times[onset_frames], 0, o_env.max(), color='r', alpha=0.9,linestyle='--', label='Onsets')\nplt.axis('tight')\nplt.legend(frameon=True, framealpha=0.75)\n","66b77fb9":"oenv = librosa.onset.onset_strength(y=y, sr=sr)\n# Detect events without backtracking\nonset_raw = librosa.onset.onset_detect(onset_envelope=oenv, backtrack=False)\n# Backtrack the events using the onset envelope\nonset_bt = librosa.onset.onset_backtrack(onset_raw, oenv)\n# Backtrack the events using the RMS values\nrms = librosa.feature.rms(S=np.abs(librosa.stft(y=y)))\nonset_bt_rms = librosa.onset.onset_backtrack(onset_raw, rms[0])","035f353b":"# Plot the results\nplt.figure(figsize=(16, 6))\nplt.subplot(2,1,1)\nplt.plot(oenv, label='Onset strength')\nplt.vlines(onset_raw, 0, oenv.max(), label='Raw onsets')\nplt.vlines(onset_bt, 0, oenv.max(), label='Backtracked', color='r')\nplt.legend(frameon=True, framealpha=0.75)\nplt.subplot(2,1,2)\nplt.plot(rms[0], label='RMS')\nplt.vlines(onset_bt_rms, 0, rms.max(), label='Backtracked (RMS)', color='r')\nplt.legend(frameon=True, framealpha=0.75)\n","f5c3b565":"D = np.abs(librosa.stft(y))\ntimes = librosa.frames_to_time(np.arange(D.shape[1]))\n\nplt.figure(figsize=(16, 6))\n#ax1 = plt.subplot(2, 1, 1)\n#librosa.display.specshow(librosa.amplitude_to_db(D, ref=np.max),y_axis='log', x_axis='time')\n#plt.title('Power spectrogram')\n\n# Construct a standard onset function\nonset_env = librosa.onset.onset_strength(y=y, sr=sr)\nplt.subplot(2, 1, 1, sharex=ax1)\nplt.plot(times, 2 + onset_env \/ onset_env.max(), alpha=0.8,label='Mean (mel)')\n\n# median\nonset_env = librosa.onset.onset_strength(y=y, sr=sr,aggregate=np.median,fmax=8000, n_mels=256)\nplt.plot(times, 1+ (onset_env\/onset_env.max()), alpha=0.8,label='Median (custom mel)')\n\n# Constant-Q spectrogram instead of Mel\nonset_env = librosa.onset.onset_strength(y=y, sr=sr,feature=librosa.cqt)\nplt.plot(times, onset_env \/ onset_env.max(), alpha=0.8,label='Mean (CQT)')\nplt.legend(frameon=True, framealpha=0.75)\nplt.ylabel('Normalized strength')\nplt.yticks([])\nplt.axis('tight')\nplt.tight_layout()\n\nonset_subbands = librosa.onset.onset_strength_multi(y=y, sr=sr, channels=[0, 32, 64, 96, 128])\n#plt.figure(figsize=(16, 6))\nplt.subplot(2, 1, 2)\nlibrosa.display.specshow(onset_subbands, x_axis='time')\nplt.ylabel('Sub-bands')\nplt.title('Sub-band onset strength')","f7905ae5":"print(\"Number of training examples=\", train_ab.shape[0], \"  Number of classes=\", len(train_ab.label.unique()))","8ec644c3":"def audio_norm(data):\n    max_data = np.max(data)\n    min_data = np.min(data)\n    data = (data-min_data)\/(max_data-min_data+0.0001)\n    return data-0.5\n\n# get audio data without padding highest qualify audio\ndef load_file_data_without_change(folder,file_names, duration=3, sr=16000):\n    input_length=sr*duration\n    # function to load files and extract features\n    # file_names = glob.glob(os.path.join(folder, '*.wav'))\n    data = []\n    for file_name in file_names:\n        try:\n            sound_file=folder+file_name\n            print (\"load file \",sound_file)\n            # use kaiser_fast technique for faster extraction\n            X, sr = librosa.load( sound_file, res_type='kaiser_fast') \n            dur = librosa.get_duration(y=X, sr=sr)\n            # extract normalized mfcc feature from data\n            mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sr, n_mfcc=40).T,axis=0) \n        except Exception as e:\n            print(\"Error encountered while parsing file: \", file)\n        feature = np.array(mfccs).reshape([-1,1])\n        data.append(feature)\n    return data\n\n\n# get audio data with a fix padding may also chop off some file\ndef load_file_data (folder,file_names, duration=12, sr=16000):\n    input_length=sr*duration\n    # function to load files and extract features\n    # file_names = glob.glob(os.path.join(folder, '*.wav'))\n    data = []\n    for file_name in file_names:\n        try:\n            sound_file=folder+file_name\n            print (\"load file \",sound_file)\n            # use kaiser_fast technique for faster extraction\n            X, sr = librosa.load( sound_file, sr=sr, duration=duration,res_type='kaiser_fast') \n            dur = librosa.get_duration(y=X, sr=sr)\n            # pad audio file same duration\n            if (round(dur) < duration):\n                print (\"fixing audio lenght :\", file_name)\n                y = librosa.util.fix_length(X, input_length)                \n            #normalized raw audio \n            # y = audio_norm(y)            \n            # extract normalized mfcc feature from data\n            mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sr, n_mfcc=40).T,axis=0)             \n        except Exception as e:\n            print(\"Error encountered while parsing file: \", file)        \n        feature = np.array(mfccs).reshape([-1,1])\n        data.append(feature)\n    return data","93ae0d1f":"# simple encoding of categories, limited to 3 types\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\n\n# Map label text to integer\nCLASSES = ['artifact','murmur','normal']\n# {'artifact': 0, 'murmur': 1, 'normal': 3}\nNB_CLASSES=len(CLASSES)\n\n# Map integer value to text labels\nlabel_to_int = {k:v for v,k in enumerate(CLASSES)}\nprint (label_to_int)\nprint (\" \")\n# map integer to label text\nint_to_label = {v:k for k,v in label_to_int.items()}\nprint(int_to_label)","0c3de547":"# load dataset-a, keep them separate for testing purpose\nimport os, fnmatch\n\nA_folder=INPUT_DIR+'\/set_a\/'\n# set-a\nA_artifact_files = fnmatch.filter(os.listdir(INPUT_DIR+'\/set_a'), 'artifact*.wav')\nA_artifact_sounds = load_file_data(folder=A_folder,file_names=A_artifact_files, duration=MAX_SOUND_CLIP_DURATION)\nA_artifact_labels = [0 for items in A_artifact_files]\n\nA_normal_files = fnmatch.filter(os.listdir(INPUT_DIR+'\/set_a'), 'normal*.wav')\nA_normal_sounds = load_file_data(folder=A_folder,file_names=A_normal_files, duration=MAX_SOUND_CLIP_DURATION)\nA_normal_labels = [2 for items in A_normal_sounds]\n\nA_extrahls_files = fnmatch.filter(os.listdir(INPUT_DIR+'\/set_a'), 'extrahls*.wav')\nA_extrahls_sounds = load_file_data(folder=A_folder,file_names=A_extrahls_files, duration=MAX_SOUND_CLIP_DURATION)\nA_extrahls_labels = [1 for items in A_extrahls_sounds]\n\nA_murmur_files = fnmatch.filter(os.listdir(INPUT_DIR+'\/set_a'), 'murmur*.wav')\nA_murmur_sounds = load_file_data(folder=A_folder,file_names=A_murmur_files, duration=MAX_SOUND_CLIP_DURATION)\nA_murmur_labels = [1 for items in A_murmur_files]\n\n# test files\nA_unlabelledtest_files = fnmatch.filter(os.listdir(INPUT_DIR+'\/set_a'), 'Aunlabelledtest*.wav')\nA_unlabelledtest_sounds = load_file_data(folder=A_folder,file_names=A_unlabelledtest_files, duration=MAX_SOUND_CLIP_DURATION)\nA_unlabelledtest_labels = [-1 for items in A_unlabelledtest_sounds]\n\nprint (\"loaded dataset-a\")","84fd8c29":"%%time\n# load dataset-b, keep them separate for testing purpose \nB_folder=INPUT_DIR+'\/set_b\/'\n# set-b\nB_normal_files = fnmatch.filter(os.listdir(INPUT_DIR+'\/set_b'), 'normal*.wav')  # include noisy files\nB_normal_sounds = load_file_data(folder=B_folder,file_names=B_normal_files, duration=MAX_SOUND_CLIP_DURATION)\nB_normal_labels = [2 for items in B_normal_sounds]\n\nB_murmur_files = fnmatch.filter(os.listdir(INPUT_DIR+'\/set_b'), 'murmur*.wav')  # include noisy files\nB_murmur_sounds = load_file_data(folder=B_folder,file_names=B_murmur_files, duration=MAX_SOUND_CLIP_DURATION)\nB_murmur_labels = [1 for items in B_murmur_files]\n\nB_extrastole_files = fnmatch.filter(os.listdir(INPUT_DIR+'\/set_b'), 'extrastole*.wav')\nB_extrastole_sounds = load_file_data(folder=B_folder,file_names=B_extrastole_files, duration=MAX_SOUND_CLIP_DURATION)\nB_extrastole_labels = [1 for items in B_extrastole_files]\n\n#test files\nB_unlabelledtest_files = fnmatch.filter(os.listdir(INPUT_DIR+'\/set_b'), 'Bunlabelledtest*.wav')\nB_unlabelledtest_sounds = load_file_data(folder=B_folder,file_names=B_unlabelledtest_files, duration=MAX_SOUND_CLIP_DURATION)\nB_unlabelledtest_labels = [-1 for items in B_unlabelledtest_sounds]\nprint (\"loaded dataset-b\")","c07718f0":"#combine set-a and set-b \nx_data = np.concatenate((A_artifact_sounds, A_normal_sounds,A_extrahls_sounds,A_murmur_sounds, \n                         B_normal_sounds,B_murmur_sounds,B_extrastole_sounds))\n\ny_data = np.concatenate((A_artifact_labels, A_normal_labels,A_extrahls_labels,A_murmur_labels,\n                         B_normal_labels,B_murmur_labels,B_extrastole_labels))\n\ntest_x = np.concatenate((A_unlabelledtest_sounds,B_unlabelledtest_sounds))\ntest_y = np.concatenate((A_unlabelledtest_labels,B_unlabelledtest_labels))\n\nprint (\"combined training data record: \",len(y_data), len(test_y))","d6a0ec95":"# shuffle - whether or not to shuffle the data before splitting. If shuffle=False then stratify must be None.\n# random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.\n\nseed = 1000\n# split data into Train, Validation and Test\nx_train, x_test, y_train, y_test = train_test_split(x_data, y_data, train_size=0.9, random_state=seed, shuffle=True)\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, train_size=0.9, random_state=seed, shuffle=True)\n\n# One-Hot encoding for classes\ny_train = np.array(keras.utils.to_categorical(y_train, len(CLASSES)))\ny_test = np.array(keras.utils.to_categorical(y_test, len(CLASSES)))\ny_val = np.array(keras.utils.to_categorical(y_val, len(CLASSES)))\ntest_y=np.array(keras.utils.to_categorical(test_y, len(CLASSES)))","20fdf365":"print (\"label shape: \", y_data.shape)\nprint (\"data size of the array: : %s\" % y_data.size)\nprint (\"length of one array element in bytes: \", y_data.itemsize)\nprint (\"total bytes consumed by the elements of the array: \", y_data.nbytes)\nprint (y_data[1])\nprint (\"\")\nprint (\"audio data shape: \", x_data.shape)\nprint (\"data size of the array: : %s\" % x_data.size)\nprint (\"length of one array element in bytes: \", x_data.itemsize)\nprint (\"total bytes consumed by the elements of the array: \", x_data.nbytes)\n#print (x_data[1])\nprint (\"\")\nprint (\"training data shape: \", x_train.shape)\nprint (\"training label shape: \", y_train.shape)\nprint (\"\")\nprint (\"validation data shape: \", x_val.shape)\nprint (\"validation label shape: \", y_val.shape)\nprint (\"\")\nprint (\"test data shape: \", x_test.shape)\nprint (\"test label shape: \", y_test.shape)","adafa3dc":"import numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten, LSTM\nfrom keras.layers import Convolution2D, MaxPooling2D\nfrom keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping,ReduceLROnPlateau,ModelCheckpoint,TensorBoard,ProgbarLogger\nfrom keras.utils import np_utils\nfrom sklearn import metrics \nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\nimport itertools","4c490c33":"print('Build LSTM RNN model ...')\nmodel = Sequential()\nmodel.add(LSTM(units=64, dropout=0.05, recurrent_dropout=0.20, return_sequences=True,input_shape = (40,1)))\nmodel.add(LSTM(units=32, dropout=0.05, recurrent_dropout=0.20, return_sequences=False))\nmodel.add(Dense(len(CLASSES), activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='Adamax', metrics=['acc','mse', 'mae', 'mape', 'cosine'])\nmodel.summary()","458f5b84":"%%time\n# saved model checkpoint file\nbest_model_file=\".\/best_model_trained.hdf5\"\n#train_model_file=file_path+\"\/checkpoints\/weights.best_{epoch:02d}-{loss:.2f}.hdf5\"\nMAX_PATIENT=12\nMAX_EPOCHS=100\nMAX_BATCH=32\n\n# callbacks\n# removed EarlyStopping(patience=MAX_PATIENT)\ncallback=[ReduceLROnPlateau(patience=MAX_PATIENT, verbose=1),\n          ModelCheckpoint(filepath=best_model_file, monitor='loss', verbose=1, save_best_only=True)]\n\nprint (\"training started..... please wait.\")\n# training\nhistory=model.fit(x_train, y_train, \n                  batch_size=MAX_BATCH, \n                  epochs=MAX_EPOCHS,\n                  verbose=0,\n                  validation_data=(x_val, y_val),\n                  callbacks=callback) \n\nprint (\"training finised!\")","bf971f10":"# Keras reported accuracy:\nscore = model.evaluate(x_train, y_train, verbose=0) \nprint (\"model train data score       : \",round(score[1]*100) , \"%\")\n\nscore = model.evaluate(x_test, y_test, verbose=0) \nprint (\"model test data score        : \",round(score[1]*100) , \"%\")\n\nscore = model.evaluate(x_val, y_val, verbose=0) \nprint (\"model validation data score  : \", round(score[1]*100), \"%\")\n\nscore = model.evaluate(test_x, test_y, verbose=0) \nprint (\"model unlabeled data score   : \", round(score[1]*100), \"%\")","8a464517":"%%time\n#Plot Keras History\n#Plot loss and accuracy for the training and validation set.\ndef plot_history(history):\n    loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' not in s]\n    val_loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' in s]\n    acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' not in s]\n    val_acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' in s]\n    if len(loss_list) == 0:\n        print('Loss is missing in history')\n        return \n    plt.figure(figsize=(22,10))\n    ## As loss always exists\n    epochs = range(1,len(history.history[loss_list[0]]) + 1)\n    ## Accuracy\n    plt.figure(221, figsize=(20,10))\n    ## Accuracy\n    # plt.figure(2,figsize=(14,5))\n    plt.subplot(221, title='Accuracy')\n    for l in acc_list:\n        plt.plot(epochs, history.history[l], 'b', label='Training accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n    for l in val_acc_list:    \n        plt.plot(epochs, history.history[l], 'g', label='Validation accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n    plt.title('Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    ## Loss\n    plt.subplot(222, title='Loss')\n    for l in loss_list:\n        plt.plot(epochs, history.history[l], 'b', label='Training loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n    for l in val_loss_list:\n        plt.plot(epochs, history.history[l], 'g', label='Validation loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))    \n    plt.title('Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()\n\n# plot history\nplot_history(history)","8d1bd24c":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        title='Normalized confusion matrix'\n    else:\n        title='Confusion matrix'\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()","218c35e7":"# prediction class \ny_pred = model.predict_classes(x_test, batch_size=32)\nprint (\"prediction test return :\",y_pred[1], \"-\", int_to_label[y_pred[1]])","06e6b23d":"plt.figure(1,figsize=(20,10))\n# plot Classification Metrics: Accuracy \nplt.subplot(221, title='Prediction')\nplt.plot(y_pred)\nplt.show()","54f6b8ea":"print (best_model_file)","6733218d":"### Loading a Check-Pointed Neural Network Model\n# How to load and use weights from a checkpoint\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.callbacks import ModelCheckpoint\nimport matplotlib.pyplot as plt\nimport numpy\n# fix random seed for reproducibility\nseed = 7\nnumpy.random.seed(seed)\n# create model\nprint('Build LSTM RNN model ...')\nmodel = Sequential()\nmodel.add(LSTM(units=64, dropout=0.05, recurrent_dropout=0.35, return_sequences=True,input_shape = (40,1)))\nmodel.add(LSTM(units=32, dropout=0.05, recurrent_dropout=0.35, return_sequences=False))\nmodel.add(Dense(len(CLASSES), activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc','mse', 'mae', 'mape', 'cosine'])\nmodel.summary()\n# load weights\nmodel.load_weights(best_model_file)\n# Compile model (required to make predictions)\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(\"Created model and loaded weights from file\")","35db8084":"# make a prediction\ny_pred = model.predict_classes(x_test, batch_size=32)\n#check scores\nscores = model.evaluate(x_test, y_test, verbose=0)\nprint (\"Model evaluation accuracy: \", round(scores[1]*100),\"%\")","b3e3cdd7":"### Prediction Test","66c8c7ed":"## Heart sounds analysis and classification with LSTM","7af01f62":"#TODO -  more optimization, keep improving the model.","813fcc5f":"### Sound Feature: MFCC \n\nMel Frequency Cepstral Coefficient (MFCC) is by far the most successful feature used in the field of Speech Processing. Speech is a non-stationary signal. As such, normal signal processing techniques cannot be directly applied to it. \n\nMel-frequency cepstral coefficients (MFCCs) are coefficients that collectively make up an MFC. They are derived from a type of cepstral representation of the audio clip (a nonlinear \"spectrum-of-a-spectrum\"). The difference between the cepstrum and the mel-frequency cepstrum is that in the MFC, the frequency bands are equally spaced on the mel scale, which approximates the human auditory system's response more closely than the linearly-spaced frequency bands used in the normal cepstrum. This frequency warping can allow for better representation of sound, for example, in audio compression.\n\nMFCCs are commonly derived as follows:\n-Take the Fourier transform of (a windowed excerpt of) a signal.\n-Map the powers of the spectrum obtained above onto the mel scale, using triangular overlapping windows.\n-Take the logs of the powers at each of the mel frequencies.\n-Take the discrete cosine transform of the list of mel log powers, as if it were a signal.\nThe MFCCs are the amplitudes of the resulting spectrum.\n\nIn general, a 39-dimensional feature vector is used which is composed of first 13 MFCCs and their corresponding 13 delta and 13 delta-delta.","b787e76b":"#### 2. Murmur \nHeart murmurs sound as though there is a \u201cwhooshing, roaring, rumbling, or turbulent fluid\u201d noise in one of two temporal locations: (1) between \u201club\u201d and \u201cdub\u201d, or (2) between \u201cdub\u201d and \u201club\u201d. They can be a symptom of many heart disorders, some serious. There will still be a \u201club\u201d and a \u201cdub\u201d. One of the things that confuses non-medically trained people is that murmurs happen between lub and dub or between dub and lub; not on lub and not on dub.(source: Rita Getz)","5ffa94e5":"#### onset detector\n\nBasic onset detector. Locate note onset events by picking peaks in an onset strength envelope.\nThe peak_pick parameters were chosen by large-scale hyper-parameter optimization over the dataset provided ","6b210b40":"#### onset_backtrack\nBacktrack detected onset events to the nearest preceding local minimum of an energy function.\nThis function can be used to roll back the timing of detected onsets from a detected peak amplitude to the preceding minimum. This is most useful when using onsets to determine slice points for segmentation","abb28d23":"### Audio Length\nthe lengths of the audio files in the dataset varies from 1 to 30 seconds long. for training purpose we use first 5 seconds of the audio. padd missing lenght for file smaller than 5 seconds. \n","deb2b687":"#### Reference:\nClassifying Heart Sounds Challenge \nhttp:\/\/www.peterjbentley.com\/heartchallenge\/        ","43f24fbf":"make a prediction\n    x: The input data, as a Numpy array (or list of Numpy arrays if the model has multiple inputs).\n    batch_size: Integer. If unspecified, it will default to 32.\n    steps = Total number of steps (batches of samples) before declaring the prediction round finished. \n    callbacks: List of keras.callbacks.Callback instances. \nreturns\n  Numpy array(s) of predictions.","3e31df4b":"#### onset strength\nCompute a spectral flux onset strength envelope.\nOnset strength at time t is determined by:\nmean_f max(0, S[f, t] - ref_S[f, t - lag])\nwhere ref_S is S after local max filtering along the frequency axis [1].\nBy default, if a time series y is provided, S will be the log-power Mel spectrogram.","de611ed3":"### Loading Data","406f997a":"#### 3. Extrasystole \nExtrasystole sounds may appear occasionally and can be identified because there is a heart sound that is out of rhythm involving extra or skipped heartbeats, e.g. a \u201club-lub dub\u201d or a \u201club dub-dub\u201d. (This is not the same as an extra heart sound as the event is not regularly occuring.) An extrasystole may not be a sign of disease. It can happen normally in an adult and can be very common in children. However, in some situations extrasystoles can be caused by heart diseases. If these diseases are detected earlier, then treatment is likely to be more effective. (source: Rita Getz)","f777c80a":"#### 4. Artifact \nIn the Artifact category there are a wide range of different sounds, including feedback squeals and echoes, speech, music and noise. There are usually no discernable heart sounds, and thus little or no temporal periodicity at frequencies below 195 Hz. This category is the most different from the others. It is important to be able to distinguish this category from the other three categories, so that someone gathering the data can be instructed to try again.(source: Rita Getz)","ca99afa8":"![](http:\/\/)**Here's we go...**","1a890621":"and one more todo. cheers!","8369b845":"#%%bash\n# environement setup checking DELETE IT after first build complete\n!python --version\n!which python\n!pip --version\n!which pip\n!cat \/etc\/os-release\n!uname -a\n#\n# missing libraries install if required\n# install runtime depency libraries if required \n# echo y | apt install ffmpeg \n!pip install librosa\n#\nimport sys; print( 'sys.executable is', sys.executable )\nimport os; print( 'os.getcwd is', os.getcwd() )","cff8b1b6":"### Build Model","e8cb7fc2":"### Explorer data\n\nThe audio files are of varying lengths, between 1 second and 30 seconds (some have been clipped to reduce excessive noise and provide the salient fragment of the sound).\n\nMost information in heart sounds is contained in the low frequency components, with noise in the higher frequencies. It is common to apply a low-pass filter at 195 Hz. Fast Fourier transforms are also likely to provide useful information about volume and frequency over time. More domain-specific knowledge about the difference between the categories of sounds is provided below.\n\nlet's check what is inside each directory and content and input data organization","17218758":"#### 1. Normal case\nIn the Normal category there are normal, healthy heart sounds. These may contain noise in the final second of the recording as the device is removed from the body. They may contain a variety of background noises (from traffic to radios). They may also contain occasional random noise corresponding to breathing, or brushing the microphone against clothing or skin. A normal heart sound has a clear \u201club dub, lub dub\u201d pattern, with the time from \u201club\u201d to \u201cdub\u201d shorter than the time from \u201cdub\u201d to the next \u201club\u201d (when the heart rate is less than 140 beats per minute)(source: Rita Getz)","e8dbc790":"### Data Handling in Audio domain\nAs with all unstructured data formats, audio data has a couple of preprocessing steps which have to be followed before it is presented for analysis. Another way of representing audio data is by converting it into a different domain of data representation, namely the frequency domain.\n\n![frequency domain] https:\/\/s3-ap-south-1.amazonaws.com\/av-blog-media\/wp-content\/uploads\/2017\/08\/23212155\/time_freq.png\n\nThere are a few more ways in which audio data can be represented. example. using MFCs (Mel-Frequency cepstrums)\n\nGeneral Audio Features\n- Time Domain features (eg. RMSE of waveform)\n- Frequency domain features (eg. Amplitude of individual freuencies)\n- Perceptual features (eg. MFCC)\n- Windowing features (eg. Hamming distances of windows)\n\nAfter extracting these features, it is then sent to the machine learning model for further analysis.","1cc8a876":"### let's take a look some sample by category","16f1fbb4":"#### Background\nHeart sounds are the noises generated by the beating heart and the resultant flow of blood through it. In healthy adults, there are two normal heart sounds, often described as a lub and a dub (or dup), that occur in sequence with each heartbeat. These are the first heart sound (S1) and second heart sound (S2), produced by the closing of the atrioventricular valves and semilunar valves, respectively.\n\n#### Problem \nAn estimated 17.1 million people died from CVDs in 2004, representing 29% of all global deaths. Of these deaths, an estimated 7.2 million were due to coronary heart disease. Any method which can help to detect signs of heart disease c\n \nThe problem is of particular interest to machine learning researchers as it involves classification of audio sample data, where distinguishing between classes of interest is non-trivial. Data is gathered in real-world situations and frequently contains background noise of every conceivable type. The differences between heart sounds corresponding to different heart symptoms can also be extremely subtle and challenging to separate. Success in classifying this form of data requires extremely robust classifiers. Despite its medical significance, to date this is a relatively unexplored application for machine learning. Source: Classifying Heart Sounds Challenge [http:\/\/www.peterjbentley.com\/heartchallenge\/]\n\nThis is my first attempt to create a deep learning classification model based on RNN-LSTM as a primary choice. The goal of this notebook is optimized the model for better accuracy overtime. welcome any auggestion or question.","7a457b96":"### Deep learning RNN (Recurrent Neural Networks)-LSTM (Long Short-Term Memory)  \n![image.png](https:\/\/s3-ap-south-1.amazonaws.com\/av-blog-media\/wp-content\/uploads\/2017\/12\/06022525\/bptt-768x313.png)\nLSTM network is comprised of different memory blocks called cells\n(the rectangles that we see in the image).  There are two states that are being transferred to the next cell; the cell state and the hidden state. The memory blocks are responsible for remembering things and manipulations to this memory is done through three major mechanisms, called gates.\n\n-RNN and LSTM are memory-bandwidth limited problems \n-Temporal convolutional network (TCN) \u201coutperform canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory\u201d.\n","8511e153":"Note: nan label indicate unclassified and unlabel test files","8da1ce6e":"Loading od the audio data file will be based on content from directory since each filename is associate with the category type. hence, we can use csv file for cross reference check.  Based on directory content approach will be more flexible.","bc4141e2":"Check input data in csv files","43e858b4":"### Train Model","a9f1bb07":"#### 5. Extra Heart Sound \nIn the Artifact category there are a wide range of different sounds, including feedback squeals and echoes, speech, music and noise. There are usually no discernable heart sounds, and thus little or no temporal periodicity at frequencies below 195 Hz. This category is the most different from the others. It is important to be able to distinguish this category from the other three categories, so that someone gathering the data can be instructed to try again.(source: Rita Getz)","07afc5ca":"### Sound Feature: Onset ","578bdcff":"### Model Evaluation","9a3c2d66":"### Test loaded model","c78b1849":"### Loading a saved training model"}}