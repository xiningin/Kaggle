{"cell_type":{"8550b563":"code","72062f40":"code","4d91fc06":"code","c9d525bb":"code","efebe182":"code","47e47d30":"code","142ec34b":"code","96f308fd":"code","29b09456":"code","e1a6dfa1":"code","53f43b9f":"code","ab91e00e":"code","68eecf03":"code","c0281211":"code","882700cb":"code","456625c6":"code","f9060147":"code","74c932bd":"code","8eec4eb6":"code","7c438497":"code","e3f48a0a":"code","82b1ca3d":"code","7e437871":"code","c705fecc":"code","2a94e0aa":"code","0d9474fb":"code","783e45e3":"code","408cc6d5":"code","052fd15c":"code","02496862":"code","754c1cda":"code","c637659a":"code","251d9b0a":"code","855e8f91":"code","de76f89b":"code","67c513cc":"code","846feaf9":"code","6d0df596":"code","893953da":"code","8034fd4a":"code","9aedd393":"code","1592df39":"code","08b1a890":"code","5b0af577":"code","b789c9ca":"code","a04316e2":"code","1e53ded2":"code","216beabf":"code","dabfffbc":"code","02735f36":"code","062fafa9":"code","614b1663":"code","4eaf12f1":"code","e950c813":"code","37e34b08":"code","f0c173f0":"code","a9b2d834":"code","f365bd16":"code","edab6b51":"code","baf9766b":"code","fec9ea0b":"code","248ebcea":"code","0831866f":"code","0a2f9c31":"code","d170a622":"code","c7bd738a":"code","adc8ffb9":"code","5ffd2c28":"code","33c0b51a":"code","50801217":"code","76f2275c":"code","0b2ebd5e":"code","c4eecc34":"code","08c97679":"code","bd770260":"code","36e8b974":"code","e8a6f415":"code","eaa13a35":"code","a86e8f06":"code","07cbe4b6":"markdown","5bcbf687":"markdown","f2fbe68c":"markdown","f93890d5":"markdown","3f4fb069":"markdown","06fbed3c":"markdown","a55e2c33":"markdown","aaf93229":"markdown","9095fb6d":"markdown","0dd0979b":"markdown","c4195d4b":"markdown","56389fcd":"markdown","dad5c906":"markdown","7fad1db6":"markdown"},"source":{"8550b563":"import pandas as pd\nimport pylab as pl\nimport numpy as np\n%matplotlib inline \nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns","72062f40":"file = '..\/input\/train.csv'\ndf = pd.read_csv(file)","4d91fc06":"df.head()","c9d525bb":"df = df.drop(['PassengerId', 'Name', 'Ticket'], axis=1)","efebe182":"df.shape","47e47d30":"df.head()","142ec34b":"df['Survived'].value_counts()","96f308fd":"#cheking for outliers - none.\n#sns.set(style=\"whitegrid\")\n#g = sns.boxplot(data = df, orient = 'h')\n#g.figure.set_size_inches(8,12)\n#plt.tight_layout()\n#plt.show()","29b09456":"bins = np.linspace(df.Fare.min(), df.Fare.max(), 10)\ng = sns.FacetGrid(df, col=\"Sex\", hue=\"Survived\", palette=\"muted\", col_wrap=2)\ng.map(plt.hist, 'Fare', bins=bins, ec=\"k\")\n\ng.axes[-1].legend()\nplt.show()","e1a6dfa1":"bins = np.linspace(df.Age.min(), df.Age.max(), 10)\ng = sns.FacetGrid(df, col=\"Sex\", hue=\"Survived\", palette=\"muted\", col_wrap=2)\ng.map(plt.hist, 'Age', bins=bins, ec=\"k\")\n\ng.axes[-1].legend()\nplt.show()","53f43b9f":"bins = np.linspace(df.Age.min(), df.Age.max(), 10)\ng = sns.FacetGrid(df, col=\"Pclass\", hue=\"Survived\", palette=\"muted\", col_wrap=3)\ng.map(plt.hist, 'Age', bins=bins, ec=\"k\")\n\ng.axes[-1].legend()\nplt.show()","ab91e00e":"bins = np.linspace(df.Age.min(), df.Age.max(), 10)\ng = sns.FacetGrid(df, col=\"SibSp\", hue=\"Survived\", palette=\"muted\", col_wrap=4)\ng.map(plt.hist, 'Age', bins=bins, ec=\"k\")\n\ng.axes[-1].legend()\nplt.show()","68eecf03":"df['SibSp'].value_counts()","c0281211":"#df['Cabin'].value_counts()","882700cb":"df.isnull().sum(axis = 0)","456625c6":"df_hot = pd.concat([df, pd.get_dummies(df['Sex'])], axis=1)\ndf_hot = df_hot.drop(['Sex'], axis=1)","f9060147":"df_hot = pd.concat([df_hot, pd.get_dummies(df['Pclass'])], axis=1)\ndf_hot = df_hot.drop(['Pclass'], axis=1)","74c932bd":"df_hot = df_hot.rename(columns={df_hot.columns[9]:'Pclass1', df_hot.columns[10]:'Pclass2', df_hot.columns[11]:'Pclass3'})","8eec4eb6":"df_hot['Embarked'].value_counts()","7c438497":"df_hot['Embarked'].fillna(\"S\", inplace = True)","e3f48a0a":"from sklearn.preprocessing import LabelEncoder\npd.options.display.float_format = '{:.2f}'.format # to make legible\nle = LabelEncoder()\nT = df_hot['Embarked']\nencoded = le.fit_transform(np.ravel(T))","82b1ca3d":"df_hot = pd.concat([df_hot, pd.DataFrame(encoded, columns = ['Emb'])], axis=1)\ndf_hot = df_hot.drop(['Embarked'], axis=1)","7e437871":"df_hot = df_hot[['Survived', 'SibSp', 'Parch', 'Fare', 'Emb', 'Cabin', 'Age',\n       'female', 'male', 'Pclass1', 'Pclass2', 'Pclass3']]","c705fecc":"df_hot = df_hot.drop(['Cabin'], axis=1)","2a94e0aa":"df_hot.head(10)","0d9474fb":"# no significant correlations found\n#sns.set(style=\"ticks\", color_codes=True)\n#sns.pairplot(df_hot)","783e45e3":"from fancyimpute import KNN \ntrain_cols = list(df_hot)","408cc6d5":"# Use 5 nearest rows which have a feature to fill in each row's\n# missing features\ndfi = pd.DataFrame(KNN(k=5).fit_transform(df_hot))\ndfi.columns = train_cols","052fd15c":"dfi.head(10)","02496862":"dfi.isnull().sum()","754c1cda":"df_cor = dfi.corr()\n\nplt.figure(figsize=(15,8))\ncolormap = plt.cm.cubehelix_r\nax = sns.heatmap(df_cor, annot = True, cmap=colormap)","c637659a":"df1 = df.drop(['Age'], axis=1)\ndf1 = pd.concat([df1, dfi['Age']], axis=1)","251d9b0a":"df1['Embarked'].fillna(\"S\", inplace = True)","855e8f91":"df1 = df1.drop(['Cabin'], axis=1)","de76f89b":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nT = df1['Sex']\nencoded = le.fit_transform(np.ravel(T))","67c513cc":"df1 = pd.concat([df1, pd.DataFrame(encoded, columns = ['Gender'])], axis=1)\ndf1 = df1.drop(['Sex'], axis=1)","846feaf9":"le = LabelEncoder()\nT = df1['Embarked']\nencoded = le.fit_transform(np.ravel(T))","6d0df596":"df1 = pd.concat([df1, pd.DataFrame(encoded, columns = ['Emb'])], axis=1)\ndf1 = df1.drop(['Embarked'], axis=1)","893953da":"df1.isnull().sum()","8034fd4a":"#df1.to_csv('tit2.csv')\n#dfi.to_csv('tit1.csv')","9aedd393":"from sklearn import preprocessing\nX = np.asarray(dfi.drop(['Survived'], axis = 1))\ny = np.asarray(dfi[['Survived']])","1592df39":"X1 = np.asarray(df1.drop(['Survived'], axis = 1))\ny1 = np.asarray(df1[['Survived']])","08b1a890":"X = preprocessing.StandardScaler().fit(X).transform(X)","5b0af577":"X1 = preprocessing.StandardScaler().fit(X1).transform(X1)","b789c9ca":"print(X.size\/10)\nprint(y.size)\nprint(X1.size\/7)\nprint(y1.size)","a04316e2":"from sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn import metrics","1e53ded2":"from sklearn.naive_bayes import GaussianNB\nGNB = GaussianNB()\nscores_GNB = cross_val_score(GNB, X, y.ravel(), cv=100, scoring = \"accuracy\")","216beabf":"from sklearn.linear_model import LogisticRegression\nLR = LogisticRegression(solver = 'lbfgs', max_iter=1000)\nscores_LR = cross_val_score(LR, X, y.ravel(), cv=100, scoring = \"accuracy\")","dabfffbc":"from sklearn.ensemble import RandomForestClassifier\nRF = RandomForestClassifier(n_estimators=100, random_state=0)\nscores_RF = cross_val_score(RF, X1, y1.ravel(), cv=100, scoring = \"accuracy\")","02735f36":"from sklearn.svm import SVC \nVM = SVC(C = 0.01, kernel = 'rbf', gamma = 'scale', class_weight = 'balanced')\nscores_SVC = cross_val_score(VM, X1, y1.ravel(), cv=100, scoring = \"accuracy\")","062fafa9":"# in this case the optimal N corresponds to the target variable ('Survived') outcomes.\n# No need to run the elbow test.\nfrom sklearn.neighbors import KNeighborsClassifier\nNGH = KNeighborsClassifier(n_neighbors = 2)\nscores_KNN = cross_val_score(NGH, X, y.ravel(), cv=100, scoring = \"accuracy\")","614b1663":"from xgboost import XGBClassifier\nBST = XGBClassifier()\nscores_XGB = cross_val_score(BST, X, y.ravel(), cv=10, scoring = \"accuracy\")","4eaf12f1":"result_df = pd.DataFrame(columns=['Accuracy','Variance'], index=['Naive Bayes', 'Logistic Regression', 'Random Forest', 'SVC', 'KNN', 'XGBoost'])","e950c813":"result_df.iloc[0] = pd.Series({'Accuracy':scores_GNB.mean(),\n                               'Variance':np.std(scores_GNB)})\nresult_df.iloc[1] = pd.Series({'Accuracy':scores_LR.mean(),\n                               'Variance':np.std(scores_LR)})\nresult_df.iloc[2] = pd.Series({'Accuracy':scores_RF.mean(),\n                               'Variance':np.std(scores_RF)})\nresult_df.iloc[3] = pd.Series({'Accuracy':scores_SVC.mean(),\n                               'Variance':np.std(scores_SVC)})\nresult_df.iloc[4] = pd.Series({'Accuracy':scores_KNN.mean(),\n                               'Variance':np.std(scores_KNN)})\nresult_df.iloc[5] = pd.Series({'Accuracy':scores_XGB.mean(),\n                               'Variance':np.std(scores_XGB)})\n","37e34b08":"result_df = result_df.sort_values('Accuracy', ascending = False,  axis=0)\nresult_df.head(6)","f0c173f0":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split( X, y.ravel(), test_size=0.2, random_state=4)\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)","a9b2d834":"# Fitting XGBoost to the Training set\nfrom xgboost import XGBClassifier\ncls = XGBClassifier()\ncls.fit(X_train, y_train)","f365bd16":"file3 = '..\/input\/test.csv'\ndft = pd.read_csv(file3)","edab6b51":"dft = dft.drop(['PassengerId', 'Name', 'Ticket'], axis=1)","baf9766b":"dft.head()","fec9ea0b":"dft_hot = pd.concat([dft, pd.get_dummies(dft['Sex'])], axis=1)\ndft_hot = dft_hot.drop(['Sex'], axis=1)","248ebcea":"dft_hot = pd.concat([dft_hot, pd.get_dummies(dft['Pclass'])], axis=1)\ndft_hot = dft_hot.drop(['Pclass'], axis=1)","0831866f":"dft_hot = dft_hot.rename(columns={dft_hot.columns[8]:'Pclass1', dft_hot.columns[9]:'Pclass2', dft_hot.columns[10]:'Pclass3'})","0a2f9c31":"dft_hot.head()","d170a622":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nT = dft_hot['Embarked']\nencoded = le.fit_transform(np.ravel(T))","c7bd738a":"dft_hot = pd.concat([dft_hot, pd.DataFrame(encoded, columns = ['Emb'])], axis=1)\ndft_hot = dft_hot.drop(['Embarked'], axis=1)","adc8ffb9":"dft_hot = dft_hot[['SibSp', 'Parch', 'Fare', 'Emb', 'Cabin', 'Age',\n       'female', 'male', 'Pclass1', 'Pclass2', 'Pclass3']]","5ffd2c28":"dft_hot = dft_hot.drop(['Cabin'], axis=1)","33c0b51a":"dft_hot['Fare'].fillna((dft_hot['Fare'].mean()), inplace=True)","50801217":"from fancyimpute import KNN \n#We use the train dataframe from Titanic dataset\n#fancy impute removes column names.\n\ntrain_cols = list(dft_hot)\n","76f2275c":"# Use 5 nearest rows which have a feature to fill in each row's\n# missing features\ndft_i = pd.DataFrame(KNN(k=5).fit_transform(dft_hot))\ndft_i.columns = train_cols","0b2ebd5e":"from sklearn import preprocessing\nX_test = np.asarray(dft_i)","c4eecc34":"X_test = preprocessing.StandardScaler().fit(X_test).transform(X_test)","08c97679":"print(X_test.size\/10)","bd770260":"# Predicting the Test set results with XGBoost model\ny_pred = cls.predict(X_test)","36e8b974":"dft2 = pd.read_csv(file3)","e8a6f415":"dft2 = pd.concat([dft2['PassengerId'], pd.DataFrame(y_pred, columns = ['Survived']).astype(int)], axis=1)","eaa13a35":"dft2.head()","a86e8f06":"dft2.to_csv('titanic_results.csv', index=False)","07cbe4b6":"To minimise possible overfitting (without y_test data) I'll use cross-validation with 100 folds <br> (except for XGBoost to save computing resources) <br> I'll test both data sets (`X` and `X1`) but leave rusolts only for the best performing.","5bcbf687":"## Fitting models","f2fbe68c":"For this classification problem I'm going to apply the following algorithms:<br>\n* Naive Bayes\n* Logistic Regression\n* Random Forest\n* SVC\n* K-Nearest Neighbours\n* XGBoost","f93890d5":"## FEATURE ENGINEERING","3f4fb069":"### Predicting outcomes ","06fbed3c":"I'll use `Age` data in the `df1` set as well. In addition to label encoding and other <br> data set transformations identical to `df_hot` set.","a55e2c33":"### Summary of accuracy scores on the train set.","aaf93229":"### Feature engineering","9095fb6d":"#### Imputing NaNs in Age column with KNN","0dd0979b":"I'll make two data sets for further model training: `df_hot` - with One Hot Encoding and <br> `df1` - with  Numeric Encoding only.","c4195d4b":"## Predicting outcomes from the test set","56389fcd":"The best accuracy score (0.840) is provided by Random Trees algorithm with 0.13 variance.<br>\nXGBoost results are slightly lower (0.825) but with variance of only 0.03.","dad5c906":"### Conclusion: Select XGBoost as the winning model","7fad1db6":"## EXPLORATORY DATA ANALYSIS"}}