{"cell_type":{"56a8f951":"code","afff7af1":"code","d36a1349":"code","601cf3a9":"code","85c12fc5":"code","e69ab21b":"code","b8c72dd1":"code","f425e251":"code","8dbbfa46":"code","6e5ca093":"code","d057257c":"code","67ad80ab":"code","911b1ec9":"code","e1b3613d":"code","a6702f62":"markdown","9573c50b":"markdown","0dc72ad1":"markdown","9784736d":"markdown"},"source":{"56a8f951":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\nimport gensim\nimport matplotlib.pyplot as plt\nimport plotly\nimport datetime\nimport plotly.graph_objects as go\nfrom wordcloud import WordCloud, STOPWORDS\nfrom textblob import TextBlob \nimport re\nfrom collections import Counter\n# from allennlp.predictors.predictor import Predictor\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","afff7af1":"tweets = pd.read_csv(\"..\/input\/trump-tweets\/trumptweets.csv\")","d36a1349":"tweets['year'] = tweets.date.apply(lambda x: int(x[0:4]))\ntweets_filter = tweets[tweets['year'] >= 2016]","601cf3a9":"tweets_filter['formatted_date'] = pd.to_datetime(tweets_filter['date'])\ntweets_filter['day_of_year'] = tweets_filter['formatted_date'].apply(lambda x: x.dayofyear)\ntweets_filter['week_of_year'] = tweets_filter['formatted_date'].apply(lambda x: x.weekofyear)","85c12fc5":"start_date = datetime.datetime(2016,1,1).date()\ndates = []\ncounts = []\nreweets = []\ncount = 0\nfor el in tweets_filter.formatted_date.dt.date:\n    if (el-start_date).days <= 7:\n#         print(\"entered here\")\n        count += 1\n    else:\n        counts.append(count)\n        dates.append(start_date.strftime(\"%Y %b-%d\"))\n        start_date = (start_date+datetime.timedelta(days = 7))\n        count = 1","e69ab21b":"fig = go.Figure(data=go.Scatter(x=dates, y=counts,line=dict(color='firebrick', width=4)))\nfig.update_layout(title='No of Tweets by POTUS',\n                   xaxis_title='No of Tweets',\n                   yaxis_title='Week',\n                  xaxis = go.layout.XAxis(\n        tickangle = 270))\nfig.update_xaxes(nticks=10)\nfig.show()\n\n","b8c72dd1":" def clean_tweet(tweet): \n        ''' \n        Utility function to clean tweet text by removing links, special characters \n        using simple regex statements. \n        '''\n        return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\\/\\\/\\S+)\", \" \", tweet).split())\n  \n  ","f425e251":"tweets_filter['content'] = tweets_filter.content.apply(clean_tweet)","8dbbfa46":"def get_tweet_sentiment(tweet): \n        ''' \n        Utility function to classify sentiment of passed tweet \n        using textblob's sentiment method \n        '''\n        # create TextBlob object of passed tweet text \n        analysis = TextBlob(tweet.lower()) \n        # set sentiment \n        if analysis.sentiment.polarity > 0: \n            return 'positive'\n        elif analysis.sentiment.polarity == 0: \n            return 'neutral'\n        else: \n            return 'negative'","6e5ca093":"%%time\ntweets_filter['sentiment'] = tweets_filter.content.apply(get_tweet_sentiment)","d057257c":"%%time\nword_cloud_data = {}\nNER_data = {}\nfor sentiment in tweets_filter.sentiment.unique():\n    data_filter = tweets_filter[(tweets_filter.formatted_date.dt.date >= \\\n                                 datetime.datetime(2017,1,1).date())&\\\n                               (tweets_filter.sentiment == sentiment)]\n    tweetText = data_filter.content.tolist()\n    words = []\n    NER = []\n    for t in tweetText:\n        doc = nlp(t)\n        ner = []\n        for ent in doc.ents:\n            NER.append(ent.label_)\n        for w in t.split():\n            if w.strip().lower() not in STOPWORDS:\n                words.append(w.strip().lower())\n    word_cloud_data[sentiment] = words\n    NER_data[sentiment] = NER","67ad80ab":"fig = plt.figure(figsize=(12,6))\ntweets_filter.sentiment.value_counts().plot(kind = \"bar\", title = \"# Tweets\")\nplt.xlabel('Sentiment')\nplt.ylabel('Tweets')\n# plt.title.set_text(\"No of tweets\")","911b1ec9":"%%time\nfig = plt.figure(figsize=(24,24))\naxes = fig.subplots(nrows=3, ncols=1)\ncounter = 0\nfor row in axes:\n    unique_string = (\" \").join(list(word_cloud_data.values())[counter])\n    wordcloud = WordCloud(width = 1500, height = 750, background_color = \"white\").generate(unique_string)\n    row.title.set_text(list(word_cloud_data.keys())[counter])\n    row.imshow(wordcloud)\n    row.axis(\"off\")\n    counter+=1","e1b3613d":"%%time\nfor sentiment in tweets_filter.sentiment.unique():\n    counter = Counter(NER_data[sentiment])\n    labels = list(counter.keys())\n    values = list(counter.values())\n    fig = go.Figure(data=[go.Pie(labels=labels, values=values)])\n    fig.update_layout(title = sentiment)\n    fig.show()\n","a6702f62":"There was a slight increase in no of tweets per week that peaked in the week of october 4th 2019","9573c50b":"**A word cloud of tweets by the POTUS**","0dc72ad1":"**The POTUS Tweets positively**","9784736d":"**A breakdown of Entities the POTUS talks about in the tweets**"}}