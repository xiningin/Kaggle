{"cell_type":{"f2a9bb70":"code","68c5df43":"code","50a5b9d5":"code","4e7abf2a":"code","48470d21":"code","51a2e85b":"code","e3d41558":"code","7379212e":"code","cc9e8226":"code","e046bfff":"code","4f38346a":"code","a9e579ff":"code","94aa4683":"code","492c2080":"code","1e5de3b1":"code","8e038e9b":"markdown","f5e3fb8b":"markdown","c95cec1d":"markdown","e56d0d42":"markdown","7f06346f":"markdown","9bd670fe":"markdown"},"source":{"f2a9bb70":"from IPython.core.display import display, HTML\n\nimport pandas as pd\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport glob\nimport os\nimport gc\n\nfrom joblib import Parallel, delayed\n\nfrom sklearn import preprocessing, model_selection\nimport lightgbm as lgb\n\nfrom sklearn.metrics import r2_score\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\npath_root = '..\/input\/optiver-realized-volatility-prediction'\npath_data = '..\/input\/optiver-realized-volatility-prediction'\npath_submissions = '\/'\n\ntarget_name = 'target'\nscores_folds = {}","68c5df43":"def log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() \n\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))\n\ndef rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true))))\n\ndef get_stock_stat(stock_id : int, dataType = 'train'):\n    key = ['stock_id', 'time_id', 'seconds_in_bucket']\n    \n    #Book features\n    df_book = pd.read_parquet(os.path.join(path_data, 'book_{}.parquet\/stock_id={}\/'.format(dataType, stock_id)))\n    df_book['stock_id'] = stock_id\n    cols = key + [col for col in df_book.columns if col not in key]\n    df_book = df_book[cols]\n    \n    df_book['wap1'] = (df_book['bid_price1'] * df_book['ask_size1'] +\n                                    df_book['ask_price1'] * df_book['bid_size1']) \/ (df_book['bid_size1'] + df_book['ask_size1'])\n    df_book['wap2'] = (df_book['bid_price2'] * df_book['ask_size2'] +\n                                    df_book['ask_price2'] * df_book['bid_size2']) \/ (df_book['bid_size2'] + df_book['ask_size2'])\n    df_book['log_return1'] = df_book.groupby(by = ['time_id'])['wap1'].apply(log_return).fillna(0)\n    df_book['log_return2'] = df_book.groupby(by = ['time_id'])['wap2'].apply(log_return).fillna(0)\n    \n    features_to_apply_realized_volatility = ['log_return'+str(i+1) for i in range(2)]\n    stock_stat = df_book.groupby(by = ['stock_id', 'time_id'])[features_to_apply_realized_volatility]\\\n                        .agg(realized_volatility).reset_index()\n\n    #Trade features\n    trade_stat =  pd.read_parquet(os.path.join(path_data,'trade_{}.parquet\/stock_id={}'.format(dataType, stock_id)))\n    trade_stat = trade_stat.sort_values(by=['time_id', 'seconds_in_bucket']).reset_index(drop=True)\n    trade_stat['stock_id'] = stock_id\n    cols = key + [col for col in trade_stat.columns if col not in key]\n    trade_stat = trade_stat[cols]\n    trade_stat['trade_log_return1'] = trade_stat.groupby(by = ['time_id'])['price'].apply(log_return).fillna(0)\n    trade_stat = trade_stat.groupby(by = ['stock_id', 'time_id'])[['trade_log_return1']]\\\n                           .agg(realized_volatility).reset_index()\n    #Joining book and trade features\n    stock_stat = stock_stat.merge(trade_stat, on=['stock_id', 'time_id'], how='left').fillna(-999)\n    \n    return stock_stat\n\ndef get_dataSet(stock_ids : list, dataType = 'train'):\n\n    stock_stat = Parallel(n_jobs=-1)(\n        delayed(get_stock_stat)(stock_id, dataType) \n        for stock_id in stock_ids\n    )\n    \n    stock_stat_df = pd.concat(stock_stat, ignore_index = True)\n\n    return stock_stat_df\n\ndef feval_RMSPE(preds, train_data):\n    labels = train_data.get_label()\n    return 'RMSPE', round(rmspe(y_true = labels, y_pred = preds),5), False\n\nparams_lgbm = {\n        'task': 'train',\n        'boosting_type': 'gbdt',\n        'learning_rate': 0.01,\n        'objective': 'regression',\n        'metric': 'None',\n        'max_depth': -1,\n        'n_jobs': -1,\n        'feature_fraction': 0.7,\n        'bagging_fraction': 0.7,\n        'lambda_l2': 1,\n        'verbose': -1\n        #'bagging_freq': 5\n}","50a5b9d5":"train = pd.read_csv(os.path.join(path_data, 'train.csv'))\n%time train_stock_stat_df = get_dataSet(stock_ids = train['stock_id'].unique(), dataType = 'train')\ntrain = pd.merge(train, train_stock_stat_df, on = ['stock_id', 'time_id'], how = 'left')\nprint('Train shape: {}'.format(train.shape))\ndisplay(train.head(2))\n\ntest = pd.read_csv(os.path.join(path_data, 'test.csv'))\ntest_stock_stat_df = get_dataSet(stock_ids = test['stock_id'].unique(), dataType = 'test')\ntest = pd.merge(test, test_stock_stat_df, on = ['stock_id', 'time_id'], how = 'left').fillna(0)\nprint('Test shape: {}'.format(test.shape))\ndisplay(test.head(2))","4e7abf2a":"#list(train.columns)","48470d21":"cats = ['stock_id']\nmodel_name = 'lgb1'\npred_name = 'pred_{}'.format(model_name)\nfeatures_to_consider = ['stock_id', 'log_return1', 'log_return2', 'trade_log_return1']\nprint('We consider {} features'.format(len(features_to_consider)))\n\ntrain[pred_name] = 0\ntest['target'] = 0\n\nn_folds = 4\nn_rounds = 5000\nkf = model_selection.KFold(n_splits=n_folds, shuffle=True, random_state=2016)\nscores_folds[model_name] = []\ncounter = 1\nfor dev_index, val_index in kf.split(range(len(train))):\n    print('CV {}\/{}'.format(counter, n_folds))\n    X_train = train.loc[dev_index, features_to_consider]\n    y_train = train.loc[dev_index, target_name].values\n    X_val = train.loc[val_index, features_to_consider]\n    y_val = train.loc[val_index, target_name].values\n    \n    #############################################################################################\n    #LGB\n    #############################################################################################\n    train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=cats, weight=1\/np.power(y_train,2))\n    val_data = lgb.Dataset(X_val, label=y_val, categorical_feature=cats, weight=1\/np.power(y_val,2))\n    \n    model = lgb.train(params_lgbm, \n                      train_data, \n                      n_rounds, \n                      valid_sets=val_data, \n                      feval=feval_RMSPE,\n                      verbose_eval= 250,\n                      early_stopping_rounds=500\n                     )\n    preds = model.predict(train.loc[val_index, features_to_consider])\n    train.loc[val_index, pred_name] = preds\n    score = round(rmspe(y_true = y_val, y_pred = preds),5)\n    print('Fold {} {}: {}'.format(counter, model_name, score))\n    scores_folds[model_name].append(score)\n    counter += 1\n    test[target_name] += model.predict(test[features_to_consider]).clip(0,1e10)\n\n    \ndel train_data, val_data\ntest[target_name] = test[target_name]\/n_folds\n\nscore = round(rmspe(y_true = train[target_name].values, y_pred = train[pred_name].values),5)\nprint('RMSPE {}: {} - Folds: {}'.format(model_name, score, scores_folds[model_name]))\n\ndisplay(test[['row_id', target_name]].head(2))\ntest[['row_id', target_name]].to_csv('submission.csv',index = False)\n\nimportances = pd.DataFrame({'Feature': model.feature_name(), \n                            'Importance': model.feature_importance(importance_type='gain')})\nimportances.sort_values(by = 'Importance', inplace=True)\nimportances2 = importances.nlargest(50,'Importance', keep='first').sort_values(by='Importance', ascending=True)\nimportances2[['Importance', 'Feature']].plot(kind = 'barh', x = 'Feature', figsize = (8,6), color = 'blue', fontsize=11);plt.ylabel('Feature', fontsize=12)","51a2e85b":"\nfrom eli5.sklearn import PermutationImportance \nimport eli5 \nparams_lgbm = {\n        'task': 'train',\n        'boosting_type': 'gbdt',\n        'learning_rate': 0.01,\n        'objective': 'regression',\n        'metric': 'None',\n        'max_depth': -1,\n        'n_jobs': -1,\n        'feature_fraction': 0.7,\n        'bagging_fraction': 0.7,\n        'lambda_l2': 1,\n        'verbose': -1,\n        'early_stopping_rounds': 500,\n        #'bagging_freq': 5\n}\ny = train[target_name]\nfor fold, (trn_idx, val_idx) in enumerate(kf.split(train[features_to_consider], y)):\n\n    print(\"Fold :\", fold+1)\n    \n    # create dataset\n    X_train = train.loc[dev_index, features_to_consider]\n    y_train = train.loc[dev_index, target_name].values\n    X_valid = train.loc[val_index, features_to_consider]\n    y_valid = train.loc[val_index, target_name].values\n\n    weights_1 = 1\/np.square(y_train)\n    weights_2 = 1\/np.square(y_valid)\n\n    model = lgb.LGBMRegressor(**params_lgbm, \n                              random_state = 1976, \n                              \n                              device_type = 'cpu',\n                              n_estimators= 5000)\n\n\n\n    def rmspe(y_true, y_pred):\n        return  (np.sqrt(np.mean(np.square( ( (y_true - y_pred) \/ y_true ) ) ) ) )\n\n\n    def feval_RMSPE2(y_true, y_pred):\n        #labels = lgbm_train.get_label()\n        return 'RMSPE', round(rmspe(y_true = y_true, y_pred = y_pred), 5), False\n\n\n    model.fit(X_train, y_train, \n              eval_set=[(X_valid, y_valid)], \n              eval_metric = feval_RMSPE2,\n              sample_weight=weights_1,\n              eval_sample_weight=[weights_2],\n              verbose=500,\n              categorical_feature = ['stock_id'])\n\n\n    perm = PermutationImportance(model, random_state=42, n_iter=3)\n\n    perm.fit(X_valid, y_valid, cv=3)\n    \n    eliDf = pd.DataFrame()\n    eliDf['fimp']=perm.feature_importances_\n    eliDf['f']=features_to_consider\n    eliDf.sort_values(by='fimp',ascending=True).to_csv('eliDf_fold_'+str(fold)+'.csv')\n    break\n    ","e3d41558":"# Feature Importance","7379212e":"eli5.show_weights(perm, feature_names = list(X_val.columns), top=500, show_feature_values=True)","cc9e8226":"#X_val.head()","e046bfff":"from eli5 import show_prediction\n#the most volatile sample in the fold\nmy_index = np.argmax(y_valid)\nshow_prediction(model, X_val.iloc[my_index], feature_names=list(X_val.columns), show_feature_values=True)","4f38346a":"#the least volatile sample in the fold\nmy_index = np.argmin(y_valid)\nshow_prediction(model, X_val.iloc[my_index], feature_names=list(X_val.columns), show_feature_values=True)","a9e579ff":"#Good ref on ELI5\n#https:\/\/coderzcolumn.com\/tutorials\/machine-learning\/how-to-use-eli5-to-understand-sklearn-models-their-performance-and-their-predictions","94aa4683":"import shap\nexplainer = shap.Explainer(model)\nshap_values = explainer(X_val)\n\n# visualize the first prediction's explanation\nshap.plots.waterfall(shap_values[0])","492c2080":"shap.plots.beeswarm(shap_values)","1e5de3b1":"shap.plots.scatter(shap_values[:,\"log_return1\"], color=shap_values)","8e038e9b":"## Train and test datasets","f5e3fb8b":"# Out of curiosity something similar with the shap values\n* with hundreds of features it might take ages ","c95cec1d":"# Feature Contribution","e56d0d42":"## Training model and making predictions","7f06346f":"# Feature Importance","9bd670fe":"## LGB starter with ELI5\n\nIn this notebook:\n* I build simple features from book and trade datasets;\n* I train a lightgbm model **with weights** with a custom metric (RMSPE) and obtain a CV score;\n* Adding an example to use ELI5\n\n\nCredits to:\n* https:\/\/www.kaggle.com\/jiashenliu\/introduction-to-financial-concepts-and-data\n* https:\/\/www.kaggle.com\/c\/optiver-realized-volatility-prediction\/discussion\/250324\n* https:\/\/www.kaggle.com\/manels\/lgb-starter\n\n**I hope it will be useful for beginners. By creating new variables you can easily improve this model.**\n\n"}}