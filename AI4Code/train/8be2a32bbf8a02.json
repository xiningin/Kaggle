{"cell_type":{"ce02340d":"code","47ddca76":"code","0b365a98":"code","06019f2f":"code","fc12580e":"code","a478f556":"code","32bffb66":"code","534dfca5":"code","e1bedd4c":"code","70716067":"code","cb7560c0":"code","0698071a":"code","756794c6":"code","b77a443c":"code","926d7dd2":"code","cac73f24":"markdown","bb7d8a83":"markdown","471ecba7":"markdown","f90dbcd0":"markdown","e655b9b7":"markdown","2fd2753d":"markdown","0cca151b":"markdown","c227d139":"markdown","05e6983f":"markdown","488994a6":"markdown","2c00cd5e":"markdown","a213986b":"markdown","fffd5aaf":"markdown"},"source":{"ce02340d":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nprint(os.listdir(\"..\/input\"))","47ddca76":"df = pd.read_csv('..\/input\/uci-turkiye-student-evaluation-data-set\/turkiye-student-evaluation_generic.csv')\ndf.head()","0b365a98":"df.info()","06019f2f":"# finding popularity or mode of the class\nsns.countplot(x='class', data=df)","fc12580e":"sns.countplot(x='class', hue='nb.repeat', data=df)","a478f556":"sns.countplot(x='class', hue='difficulty', data=df)","32bffb66":"sns.countplot(x='difficulty', hue='nb.repeat', data=df)","534dfca5":"plt.figure(figsize=(20, 20))\nsns.boxplot(data=df.iloc[:,5:31 ]);","e1bedd4c":"df_questions = df.iloc[:,5:33]\n#lets do a PCA for feature dimensional reduction\nfrom sklearn.decomposition import PCA\npca = PCA(n_components = 2)\ndf_questions_pca = pca.fit_transform(df_questions)","70716067":"from sklearn.cluster import KMeans\nwcss = []\nfor i in range(1, 7):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n    kmeans.fit(df_questions_pca)\n    wcss.append(kmeans.inertia_)\nplt.plot(range(1, 7), wcss)\nplt.title('The Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.show()","cb7560c0":"kmeans = KMeans(n_clusters = 3, init = 'k-means++')\ny_kmeans = kmeans.fit_predict(df_questions_pca)\n\n# Visualising the clusters\nplt.scatter(df_questions_pca[y_kmeans == 0, 0], df_questions_pca[y_kmeans == 0, 1], s = 100, c = 'yellow', label = 'Cluster 1')\nplt.scatter(df_questions_pca[y_kmeans == 1, 0], df_questions_pca[y_kmeans == 1, 1], s = 100, c = 'green', label = 'Cluster 2')\nplt.scatter(df_questions_pca[y_kmeans == 2, 0], df_questions_pca[y_kmeans == 2, 1], s = 100, c = 'red', label = 'Cluster 3')\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 300, c = 'blue', label = 'Centroids')\nplt.title('Clusters of students')\nplt.xlabel('PCA 1')\nplt.ylabel('PCA 2')\nplt.legend()\nplt.show()","0698071a":"# Let me check the count of students in each cluster\nimport collections\ncollections.Counter(y_kmeans)","756794c6":"# Using the dendrogram to find the optimal number of clusters\nimport scipy.cluster.hierarchy as sch\ndendrogram = sch.dendrogram(sch.linkage(df_questions_pca, method = 'ward'))\nplt.title('Dendrogram')\nplt.xlabel('questions')\nplt.ylabel('Euclidean distances')\nplt.show()","b77a443c":"# Fitting Hierarchical Clustering to the dataset\nfrom sklearn.cluster import AgglomerativeClustering\nhc = AgglomerativeClustering(n_clusters = 2, affinity = 'euclidean', linkage = 'ward')\ny_hc = hc.fit_predict(df_questions_pca)\nX = df_questions_pca\n# Visualising the clusters\nplt.scatter(X[y_hc == 0, 0], X[y_hc == 0, 1], s = 100, c = 'yellow', label = 'Cluster 1')\nplt.scatter(X[y_hc == 1, 0], X[y_hc == 1, 1], s = 100, c = 'red', label = 'Cluster 2')\nplt.title('Clusters of STUDENTS')\nplt.xlabel('PCA 1')\nplt.ylabel('PCA 2')\nplt.legend()\nplt.show()","926d7dd2":"# Let me check the count of students in each cluster\nimport collections\ncollections.Counter(y_hc)","cac73f24":"## After looking at the dataset, let's see how we learn from it and why is it an unsupervised learning approach.","bb7d8a83":"Here, by means of hierarchical clusturing it's found that nearly 2318 students replied negatively and that is near to 2220 found by k-means. ","471ecba7":"## Now, Let's start making our model.","f90dbcd0":"A **dendrogram** is a diagram that shows the hierarchical relationship between objects. It is most commonly created as an output from hierarchical clustering.","e655b9b7":"### Firstly, we will try to reduce dimensionality.\nThere are two principal algorithms for dimensionality reduction: Linear Discriminant Analysis ( **LDA** ) and Principal Component Analysis ( **PCA** ). The basic difference between these two is that LDA uses information of classes to find new features in order to maximize its separability while PCA uses the variance of each feature to do the same. In this context, *LDA can be consider a supervised algorithm* and *PCA an unsupervised algorithm.*\n\nThe idea behind PCA is simply to find a low-dimension set of axes that summarize data. Many of these features will measure related properties and so will be redundant. Therefore, we should remove these redundancy and describe each dataset with less properties. This is exactly what PCA aims to do.","2fd2753d":"The K-means algorithm identifies k(here k=3) number of centroids, and then allocates every data point to the nearest cluster, while keeping the centroids as small as possible.","0cca151b":"**Hierarchical clustering**, also known as hierarchical cluster analysis, is an algorithm that groups similar objects into groups called clusters. The endpoint is a set of clusters, where each cluster is distinct from each other cluster, and the objects within each cluster are broadly similar to each other.","c227d139":"Looking at the above graph, we saw 3 clusters of students who have given like Negative, Neutral and Positive feedback","05e6983f":"Based on the Elbow graph , we can go for 3 clusters.","488994a6":"# Introduction to UNSUPERVISED learning","2c00cd5e":"So we have 2220 students who have given negative ratings overall , 1240 students with positive ratings and 2360 students with nuetral response","a213986b":"## Unsupervised Machine Learning\nUnsupervised learning is where you only have input data (X) and no corresponding output variables.\n<br>The goal for unsupervised learning is to model the underlying structure or distribution in the data in order to learn more about the data.\n\nThese are called unsupervised learning because unlike supervised learning above there is no correct answers and there is no teacher. **Algorithms are left to their own devises to discover and present the interesting structure in the data.**\n\nUnsupervised learning problems can be further grouped into clustering and association problems :\n<br>**Clustering**: A clustering problem is where you want to discover the inherent groupings in the data, such as grouping customers by purchasing behavior.\n<br>**Association**:  An association rule learning problem is where you want to discover rules that describe large portions of your data, such as people that buy X also tend to buy Y.\n\nSome popular examples of unsupervised learning algorithms are:\n<br>**Clusturing : ** \n<br>***k-means***\n<br>***Hierarchial***\n<br>\n<br>**Association :** \n<br>***Apriori***\n<br>***FP-Growth***","fffd5aaf":"### END"}}