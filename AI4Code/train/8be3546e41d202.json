{"cell_type":{"075f4e7b":"code","6e74b053":"code","e95cd2db":"code","9fc796d2":"code","00f325fe":"code","214117f0":"code","266a7a9c":"code","41101d25":"code","b8a3ce54":"code","140062fd":"code","4ad66eea":"code","767eece9":"code","e5123dd6":"code","f06dad53":"code","5da30223":"code","0295ee1b":"code","423b2926":"code","72516a55":"code","58b6082b":"code","90e27557":"code","e13c0a28":"code","6733e023":"code","343efc5d":"code","81891dff":"code","ff55cfc7":"code","5bb160fe":"code","cc0c5953":"code","7c90c155":"markdown","f05cbf71":"markdown","297d9f47":"markdown","ac6da2dc":"markdown","1cd52ebe":"markdown","b3dd26b4":"markdown","db3becc5":"markdown","e5c61257":"markdown","5c0f9823":"markdown","a80cc901":"markdown","497c2bc4":"markdown","97b96b42":"markdown","e368ed47":"markdown","046cdfa9":"markdown"},"source":{"075f4e7b":"import numpy as np \nimport pandas as pd  \nimport matplotlib.pyplot as plt \nimport seaborn as sns  \n\nimport warnings \nwarnings.filterwarnings('ignore') \n\nfrom sklearn.datasets import load_iris","6e74b053":"iris = load_iris() \nX = iris.data  \ny = iris.target \n\ndata = pd.DataFrame(X,columns=iris.feature_names) \ndata[\"Species\"] = y \n\ndata.head()","e95cd2db":"data.shape","9fc796d2":"from sklearn.decomposition import PCA\npca=PCA(n_components= 'mle',svd_solver = 'full')\npca.fit(X)","00f325fe":"X_pca = pca.transform(X)","214117f0":"X.shape","266a7a9c":"X_pca.shape ","41101d25":"data[\"Species\"].unique()","b8a3ce54":"plt.figure(figsize = (10,6))\nsns.countplot(data[\"Species\"])\nplt.show()","140062fd":"data.isnull().sum()","4ad66eea":"plt.figure(figsize = (10,6))\ngrid = sns.FacetGrid(data, col='Species', size=3.8, aspect=1.5) \ngrid.map(sns.countplot, 'sepal length (cm)')\nplt.show()","767eece9":"plt.figure(figsize = (10,6))\ngrid = sns.FacetGrid(data, col='Species', size=3.8, aspect=1.5) \ngrid.map(sns.countplot, 'sepal width (cm)')\nplt.show()","e5123dd6":"plt.figure(figsize = (10,6))\ngrid = sns.FacetGrid(data, col='Species', size=3.8, aspect=1.5) \ngrid.map(sns.countplot, 'petal width (cm)')\nplt.show()","f06dad53":"plt.figure(figsize = (10,6))\ngrid = sns.FacetGrid(data, col='Species', size=3.8, aspect=1.5) \ngrid.map(sns.countplot, 'petal length (cm)')\nplt.show()","5da30223":"from sklearn.model_selection import train_test_split \nX_train, X_test, y_train, y_test = train_test_split(X_pca,y, test_size = 0.1)","0295ee1b":"print(X_train.shape) \nprint(y_train.shape) \nprint(X_test.shape) \nprint(y_test.shape)","423b2926":"from sklearn.model_selection import train_test_split \nX_train1, X_test1, y_train1, y_test1 = train_test_split(X,y, test_size = 0.1)","72516a55":"print(X_train1.shape) \nprint(y_train1.shape) \nprint(X_test1.shape) \nprint(y_test1.shape)","58b6082b":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC \nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.ensemble import GradientBoostingClassifier \n\nmodels = { \n    'Logistic_Regression' : LogisticRegression(random_state=42,solver='liblinear'),\n    'SVC' : SVC(kernel='rbf', gamma=0.5, probability=True),\n    'DecisionTreeClassifier': DecisionTreeClassifier(max_depth=1, random_state=42) ,\n    'GradientBoostingClassifier' : GradientBoostingClassifier(max_depth=1, n_estimators=3, learning_rate=1.0, random_state=42)\n}","90e27557":"from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score \n\ndef loss(y_true, y_pred, retur=False): \n    pre = precision_score(y_true, y_pred, average = 'macro')  \n    rec = recall_score(y_true, y_pred,average = 'macro') \n    accuracy = accuracy_score(y_true, y_pred)\n    f1score = f1_score(y_true, y_pred,average = 'macro')\n    \n    if retur:\n        return pre, rec, accuracy,f1score\n    else: \n        print('Precision: ',pre) \n        print('Recall: ',rec) \n        print('Accuracy: ',accuracy)\n        print('F1_Score: ',f1score)","e13c0a28":"def train_and_eval(models, X,y): \n    for name, model in models.items(): \n        print(name ,':')  \n        model.fit(X_train1,y_train1)\n        loss(y_test1, model.predict(X_test1)) \n        print('-' * 30)","6733e023":"train_and_eval(models, X_train1, y_train1)","343efc5d":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC \nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.ensemble import GradientBoostingClassifier \n\nmodels = { \n    'Logistic_Regression' : LogisticRegression(random_state=42,solver='liblinear'),\n    'SVC' : SVC(kernel='rbf', gamma=0.5, probability=True),\n    'DecisionTreeClassifier': DecisionTreeClassifier(max_depth=1, random_state=42) ,\n    'GradientBoostingClassifier' : GradientBoostingClassifier(max_depth=1, n_estimators=3, learning_rate=1.0, random_state=42)\n}","81891dff":"from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score \n\ndef loss(y_true, y_pred, retur=False): \n    pre = precision_score(y_true, y_pred, average = 'macro')  \n    rec = recall_score(y_true, y_pred,average = 'macro') \n    accuracy = accuracy_score(y_true, y_pred)\n    f1score = f1_score(y_true, y_pred,average = 'macro')\n    \n    if retur:\n        return pre, rec, accuracy,f1score\n    else: \n        print('Precision: ',pre) \n        print('Recall: ',rec) \n        print('Accuracy: ',accuracy)\n        print('F1_Score: ',f1score)","ff55cfc7":"def train_and_eval(models, X_pca,y): \n    for name, model in models.items(): \n        print(name ,':')  \n        model.fit(X_train,y_train)\n        loss(y_test, model.predict(X_test)) \n        print('-' * 30)","5bb160fe":"train_and_eval(models, X_train, y_train)","cc0c5953":"Svc = SVC(kernel='rbf', gamma=0.5, probability=True)\nSvc.fit(X_train, y_train)","7c90c155":"## PCA Implementation","f05cbf71":"### Iris Flower Prediction with Deployment - In this notebook I have implemented Support Vector Classifier to predict or classify the Iris Flowers according to their species. Here I have used Dimentionality Reduction technique - Principle Component Analysis(PCA) to reduce the number of features. Let's check out whether our model performed better after using this technique.\n##### Also do check the Deployment using Flask of this model on my Github- https:\/\/github.com\/AishwaryaKshirsagar\/Iris-Flower-Project.git\n### Feel free to share your thoughts and opinions.","297d9f47":"### We will use X instead of X_pca here to compare the results","ac6da2dc":"### Now let's try to implement with PCA. We will use X_pca instead of X. Let's check if we get any improvements in our results","1cd52ebe":"### We will use the method of Dimentionality Reduction technique(PCA) to reduce the number of features","b3dd26b4":"### Great! We can see that now we have perfect accuracy for Logistic Regression and Support Vector Classifier Models.","db3becc5":"### Hyperparameter tuning with PCA","e5c61257":"### First let's explore our data","5c0f9823":"### We have equal number of Species of all types.","a80cc901":"### Thankyou and Happy Analysing","497c2bc4":"### I will be implementing SVC in this case.","97b96b42":"### Without using PCA we have all 4 columns","e368ed47":"### Hyperparameter tuning without PCA","046cdfa9":"### The number of columns got reduced to 3. We will further take a look how PCA will impact the metrics"}}