{"cell_type":{"ff089361":"code","81902015":"code","cab27aa3":"code","f4e5e822":"code","7a2cfdfe":"code","33b50e5c":"code","f959867d":"code","391b920c":"code","319909c9":"code","f9a09513":"code","f3644d16":"code","ca787fe0":"code","e35b6572":"code","8dbf4981":"code","b37ab7e7":"code","0b312779":"code","6896b967":"code","51c8be33":"code","0d833c83":"code","0b433029":"code","cafb3a1f":"code","afafdda4":"code","ef0d703e":"code","3a32da3f":"code","dbd54f0e":"code","1d35d029":"code","7b5f4e52":"code","17d25b15":"code","6b4fcf31":"code","03573350":"markdown","409189e0":"markdown","5058ae2d":"markdown","8fad798f":"markdown","1baae50d":"markdown","20676f73":"markdown","be045e3b":"markdown","84855abe":"markdown","9699fb3c":"markdown","27e16c35":"markdown","035ad7dd":"markdown","d476420d":"markdown","607877dc":"markdown","a68b83bf":"markdown","44a1829c":"markdown","3cff11ae":"markdown","ea179966":"markdown","2905ddef":"markdown","d57823c6":"markdown"},"source":{"ff089361":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport numpy as np\n\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","81902015":"df_housing = pd.read_csv(\"\/kaggle\/input\/california-housing-prices\/housing.csv\")","cab27aa3":"df_housing.dtypes","f4e5e822":"len (df_housing)","7a2cfdfe":"df_housing['ocean_proximity'].value_counts()","33b50e5c":"df_housing['ocean_proximity'] = df_housing['ocean_proximity'].convert_dtypes()","f959867d":"df_housing.dtypes","391b920c":"for col in df_housing.columns:\n    print(col)\n    print(df_housing[col].isnull().value_counts())\n    print('#####################')","319909c9":"df_housing.dropna(inplace = True)","f9a09513":"for col in df_housing.columns:\n    print(col)\n    print(df_housing[col].isnull().value_counts())\n    print('#####################')","f3644d16":"\ndf_housing = pd.concat([df_housing,pd.get_dummies(df_housing['ocean_proximity'])],axis =1)\ndf_housing.drop('ocean_proximity',axis =1)","ca787fe0":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfigure = plt.figure(figsize=(12,10))\nsns.heatmap(df_housing.corr(),annot =True)","e35b6572":"df_housing.corr().loc[\"median_house_value\"] >=0.1","8dbf4981":"df_housing.corr().loc[\"median_house_value\"] <=-0.1","b37ab7e7":"df_data=df_housing[['housing_median_age','total_rooms','median_income','median_house_value','<1H OCEAN','NEAR BAY','NEAR OCEAN','INLAND','latitude']]","0b312779":"for col in ['housing_median_age','total_rooms','median_income','median_house_value','latitude'] :\n    df_data[col] =  df_data[col]\/ df_data[col].max() \n  ","6896b967":"df_data.head()","51c8be33":"train, validate, test = np.split(df_data.sample(frac=1), [int(.6*len(df_data)), int(.8*len(df_data))])","0d833c83":"X_Train = train[['housing_median_age','total_rooms','median_income','<1H OCEAN','NEAR BAY','NEAR OCEAN','INLAND','latitude']]\nY_Train = train[['median_house_value']]\nX_Val =validate[['housing_median_age','total_rooms','median_income','<1H OCEAN','NEAR BAY','NEAR OCEAN','INLAND','latitude']]\nY_Val = validate[['median_house_value']]\nX_Test =test[['housing_median_age','total_rooms','median_income','<1H OCEAN','NEAR BAY','NEAR OCEAN','INLAND','latitude']]\nY_Test = test[['median_house_value']]","0b433029":"from sklearn.linear_model import LinearRegression \nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.preprocessing import PolynomialFeatures\nimport matplotlib.pyplot as plt \n","cafb3a1f":"reg_model = LinearRegression().fit(X_Train,Y_Train)\nY_hat_Val = reg_model.predict(X_Val)","afafdda4":"print(\"Mean Squared Error on Validation Set is : \",mean_squared_error(Y_Val,Y_hat_Val))\nprint(\"R2 score on Validation Set is : \",r2_score(Y_Val,Y_hat_Val))","ef0d703e":"Y_hat_Test = reg_model.predict(X_Test)\nprint(\"Mean Squared Error on Test Set is : \",mean_squared_error(Y_hat_Test,Y_Test))\nprint(\"R2 score on Validation Set is : \",r2_score(Y_hat_Test,Y_Test))","3a32da3f":"Y_hat_Train = reg_model.predict(X_Train)\nprint(\"Mean Squared Error on Train Set is : \",mean_squared_error(Y_hat_Train,Y_Train))\nprint(\"R2 score on Validation Train is : \",r2_score(Y_hat_Train,Y_Train))","dbd54f0e":"X_Train = train[['housing_median_age','total_rooms','median_income','<1H OCEAN','NEAR BAY','NEAR OCEAN','INLAND','latitude']]\nY_Train = train[['median_house_value']]","1d35d029":"splited_train_X = np.split(X_Train,13)\nsplited_train_Y = np.split(Y_Train,13)\nerror_list_val =[]\nerror_list_train =[]\ntrain_len_list =[]\ntrain_set_len= 0\nfor X,Y in zip(splited_train_X,splited_train_Y) :\n    train_set_len = train_set_len + len(X)\n    reg_model = LinearRegression().fit(X,Y)\n    \n    y_hat_val = reg_model.predict(X_Val)\n    y_hat_train = reg_model.predict(X)\n    \n    error_list_val.append(mean_squared_error(y_hat_val,Y_Val))\n    error_list_train.append(mean_squared_error(y_hat_train,Y))\n    \n    train_len_list.append(train_set_len)\ndf_error_log_CV = pd.DataFrame({'Number of Training Sample': train_len_list,'Cross Validation Error': error_list_val})\ndf_error_log_Train = pd.DataFrame({'Number of Training Sample': train_len_list,'Training Error': error_list_train})","7b5f4e52":"fig = plt.figure(figsize = (8,5))\nax = fig.add_subplot(111)\ndf_error_log_Train.plot(x='Number of Training Sample',y='Training Error',ax=ax)\ndf_error_log_CV.plot(x='Number of Training Sample',y='Cross Validation Error',ax=ax)   ","17d25b15":"reg_model = LinearRegression().fit(X,Y)\ny_hat_test = reg_model.predict(X_Test)","6b4fcf31":"print(\"MSE values on test set :\",mean_squared_error(y_hat_test,Y_Test))\nprint(\"r2 score on test set :\",r2_score(y_hat_test,Y_Test))","03573350":"We cleared the rows that have null values.Any how before we have 20640 in which we have removed 207 records.That is not a such a significant loss of data though.","409189e0":"Coverting the column Ocean_proximity to string types seems to be the best as it contains only string values.","5058ae2d":"Here we are taking features that have posetive coorelation equal to or less then then -0.1","8fad798f":"Now lets covnvert the **ocean_proximity** to categorical data using one hot encoding.","1baae50d":"Thanks for going through my note book. If you like my note book please give a upvote.","20676f73":"Lets find out how well does it performs on testing set.","be045e3b":"**longitude** :\tA measure of how far west a house is; a higher value is farther west\n\t\n**latitude**  :\t A measure of how far north a house is; a higher value is farther north\n\t\t\n**housingMedianAge** :\tMedian age of a house within a block; a lower number is a newer building\n\t\n**totalRooms** :  Total number of rooms within a block\n\t\n**totalBedrooms** :\tTotal number of bedrooms within a block\n\t\n**population** : Total number of people residing within a block\n\t\n**households** : Total number of households, a group of people residing within a home unit, for a block\n\t\n**medianIncome** : Median income for households within a block of houses \n\t\n**medianHouseValue** : Median house value for households within a block (measured in US Dollars)\n\t\n","84855abe":"So these all feature we will be considering for prediction.","9699fb3c":"Now lets preprocess the data","27e16c35":"But the difference between cross validation and training error is not greater then 0.0030.So I don't think that will effect the performance while making prediction on testing data. ","035ad7dd":"**total_bedrooms** columns have a total of 207 null values which we have remove before used for prediction","d476420d":"Here we are taking features that have posetive coorelation equal to or greater then 0.1","607877dc":"Lets now check if any column contains null values","a68b83bf":"**Description of each cloumn**","44a1829c":"Plot of how cross validation error varies as we increase the number of training samples gradually.As per the plot we can say that at after 4000 samples the gap the between cross validation and training error went gradually increasing which indicates the presence of high variance.In an ideal case it should decrease. ","3cff11ae":"The Model is predicting is very well with good scores on test set.","ea179966":"Checking the data types of the columns","2905ddef":"Lets check if all the null values are removed or not.","d57823c6":"**Lets see a plot on how our model learns from data**"}}