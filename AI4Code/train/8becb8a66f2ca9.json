{"cell_type":{"05eb6908":"code","213d7aa6":"code","6b303a72":"code","dbd2da3e":"code","ca6231bc":"code","e66cfe62":"code","3fa2fe7d":"code","9a510805":"code","e3215ea9":"code","f3073398":"code","d9bf60c5":"code","2dd34a50":"code","7ea2d755":"code","49acffab":"code","fb639f39":"code","f08159ce":"code","84ccfb2a":"code","efe0f838":"code","34d2ace7":"code","c19f09b6":"code","5818dc85":"code","f1196b86":"code","4a74bef7":"code","8f8d0af0":"code","a041e786":"code","6a68cb76":"code","d92b9410":"code","01522f29":"code","28a40929":"code","9ff55c66":"code","a932c212":"code","2391122a":"code","101e1107":"code","b41a4505":"code","c698dd33":"code","7240a81b":"code","862cb504":"markdown","a9a0b168":"markdown","1f6ddb1e":"markdown","22eb71d7":"markdown","48a0d4ce":"markdown","03787d23":"markdown","7ec92d36":"markdown","c59b27a0":"markdown","3dac533e":"markdown","2f598f53":"markdown","fe582135":"markdown","64da5aa2":"markdown","8e85cb94":"markdown","6c7f6f51":"markdown","0cd63d42":"markdown"},"source":{"05eb6908":"from pprint import pprint\n\nimport pandas as pd\nimport numpy as np\n\n# Standard plotly imports\nimport plotly.graph_objs as go\nimport plotly.figure_factory as pff\nfrom plotly.subplots import make_subplots\n\nimport cufflinks\ncufflinks.go_offline(connected=True)\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import matthews_corrcoef\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.decomposition import PCA","213d7aa6":"data_dir = '..\/input\/tool-wear-detection-in-cnc-mill\/'\noutcomes = pd.read_csv(data_dir + 'train.csv')","6b303a72":"outcomes.info()","dbd2da3e":"outcomes","ca6231bc":"part_out = outcomes[outcomes['passed_visual_inspection'].notna()]\npprint(pd.crosstab(part_out.tool_condition, part_out.passed_visual_inspection))\n\npassed = part_out.passed_visual_inspection.eq('yes').mul(1)\nwear = part_out.tool_condition.eq('unworn').mul(1)\n\nprint('\\nPearson correlation coefficient: {:.2f}'.format(matthews_corrcoef(passed, wear)))","e66cfe62":"experiment1 = pd.read_csv(data_dir + 'experiment_01.csv')\nexperiment1.reset_index(inplace=True)","3fa2fe7d":"experiment1.info()","9a510805":"experiment1.head()","e3215ea9":"experiment1[['Machining_Process', 'index']].groupby('Machining_Process').count()","f3073398":"# Prep data functions\ndef clean_data(data):\n    \"\"\"Use this function to keep only CNC active cutting actions\"\"\"\n    keep_act = ['Layer 1 Down', 'Layer 1 Up', 'Layer 2 Down', 'Layer 2 Up', 'Layer 3 Down', 'Layer 3 Up']\n    data = data[data['Machining_Process'].isin(keep_act)]\n#     print(data[['Machining_Process', 'index']].groupby('Machining_Process').count())\n    \n    data.drop('Machining_Process', inplace=True, axis=1)\n    return data\n\ndef scale_decompose(data, pca_n=3):\n    # Scale data\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    scale_exper = scaler.fit_transform(data)\n\n    # Apply PCA to data\n    pca = PCA(n_components=pca_n, svd_solver='full')\n    return pca.fit_transform(scale_exper)\n\n# Calculate Mahalanobis dist functions\ndef is_pos_def(A):\n    if np.allclose(A, A.T):\n        try:\n            np.linalg.cholesky(A)\n            return True\n        except np.linalg.LinAlgError:\n            return False\n    else:\n        return False\n    \ndef cov_matrix(data, verbose=False):\n    covariance_matrix = np.cov(data, rowvar=False)\n    if is_pos_def(covariance_matrix):\n        inv_covariance_matrix = np.linalg.inv(covariance_matrix)\n        if is_pos_def(inv_covariance_matrix):\n            return covariance_matrix, inv_covariance_matrix\n        else:\n            print(\"Error: Inverse of Covariance Matrix is not positive definite!\")\n    else:\n        print(\"Error: Covariance Matrix is not positive definite!\")\n        \ndef MahalanobisDist(inv_cov_matrix, mean_distr, data, verbose=False):\n    diff = data - mean_distr\n    md = []\n    for i in range(len(diff)):\n        md.append(np.sqrt(diff[i].dot(inv_cov_matrix).dot(diff[i])))\n    return md\n\ndef MD_detectOutliers(dist, extreme=False, verbose=False):\n    k = 3. if extreme else 2.\n    threshold = np.mean(dist) * k\n    outliers = []\n    for i in range(len(dist)):\n        if dist[i] >= threshold:\n            outliers.append(i)  # index of the outlier\n    return np.array(outliers)\n\ndef MD_threshold(dist, extreme=False, verbose=False):\n    k = 3. if extreme else 2.\n    threshold = np.mean(dist) * k\n    return threshold\n\n","d9bf60c5":"def full_process(experiment_n, components=2, chi2_print=True, exper_num=None):\n    \"\"\"Experiment data should only contain the columns that are desireable\"\"\"\n    exper_pca = scale_decompose(experiment_n, pca_n=components)\n\n    cov, inv_cov = cov_matrix(exper_pca)\n    mean_dist = exper_pca.mean(axis=0)\n\n    m_dist = MahalanobisDist(inv_cov, mean_dist, exper_pca)\n    \n    if chi2_print:\n        fig_x = go.Figure(\n            data=[go.Histogram(x=np.square(m_dist))],\n            layout=go.Layout({'title': 'X^2 Distribution'})\n        )\n        fig_x.show()\n    \n    if exper_num:\n        title = 'Mahalanobis Distribution Experiment {}'.format(exper_num)\n    else:\n        title = 'Mahalanobis Distribution'\n    fig_m = pff.create_distplot([m_dist], group_labels=['m_dist'], bin_size=0.15)\n    fig_m.update_layout(title_text=title)\n    fig_m.show()\n    \n    return exper_pca, m_dist","2dd34a50":"def corr_actualcommand(data, corr_cols):\n    look_data = data[corr_cols]\n    corr_data = look_data.corr()\n\n    fig = go.Figure(data=go.Heatmap(\n        z=corr_data.values,\n        x=list(corr_data.index),\n        y=list(corr_data.columns)\n    ))\n    fig.show()","7ea2d755":"# Unworn data\n# Dataset loaded above\n# Clean up data\nexper1_clean = clean_data(experiment1)\nclean_ex1 = exper1_clean[(exper1_clean['M1_CURRENT_FEEDRATE']!=50) & (exper1_clean['X1_ActualPosition']!=198)]\nprint('')\nprint('Length of data before inaccurate points removed {:d}'.format(len(exper1_clean)))\nprint('Length of data after inaccurate points removed {:d}'.format(len(clean_ex1)))","49acffab":"columns = exper1_clean.columns\ncorr_cols = list(filter(lambda x: ('Actual' in x) | ('Command' in x), columns))\ncorr_actualcommand(exper1_clean, corr_cols)","fb639f39":"fig = go.Figure()\nfig.add_trace(go.Scatter(\n    x=exper1_clean['index'], y=exper1_clean['X1_ActualPosition'],\n    mode='lines',\n    name='x-actual'\n))\nfig.add_trace(go.Scatter(\n    x=exper1_clean['index'], y=exper1_clean['Y1_ActualPosition'],\n    mode='lines',\n    name='y-actual'\n))\n\nfig.show()","f08159ce":"# Worn data\n# Load data\nexperiment6 = pd.read_csv(data_dir + 'experiment_06.csv')\nexperiment6.reset_index(inplace=True)\n\n# Clean up data\nexper6_clean = clean_data(experiment6)\nclean_ex6 = exper6_clean[(exper6_clean['M1_CURRENT_FEEDRATE']!=50) & (exper6_clean['X1_ActualPosition']!=198)]\nprint('')\nprint('Length of data before inaccurate points removed {:d}'.format(len(exper6_clean)))\nprint('Length of data after inaccurate points removed {:d}'.format(len(clean_ex6)))","84ccfb2a":"columns = exper6_clean.columns\ncorr_cols = list(filter(lambda x: ('Actual' in x) | ('Command' in x), columns))\ncorr_actualcommand(exper6_clean, corr_cols)","efe0f838":"fig = make_subplots(rows=3, cols=1)\nfig.add_trace(\n    go.Scatter(\n        x=exper6_clean['index'], y=exper6_clean['X1_ActualPosition'],\n        mode='lines',\n        name='x-actual'\n    ),\n    row=1, col=1\n)\nfig.add_trace(\n    go.Scatter(\n        x=exper6_clean['index'], y=exper6_clean['Y1_ActualPosition'],\n        mode='lines',\n        name='y-actual'\n    ),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(\n        x=exper6_clean['index'], y=exper6_clean['X1_ActualPosition'],\n        mode='lines',\n        name='x-actual'\n    ),\n    row=2, col=1\n)\nfig.add_trace(\n    go.Scatter(\n        x=exper6_clean['index'], y=exper6_clean['Z1_ActualPosition'],\n        mode='lines',\n        name='z-actual'\n    ),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(\n        x=exper6_clean['index'], y=exper6_clean['X1_ActualPosition'],\n        mode='lines',\n        name='x-actual'\n    ),\n    row=3, col=1\n)\nfig.add_trace(\n    go.Scatter(\n        x=exper6_clean['index'], y=exper6_clean['S1_ActualVelocity'],\n        mode='lines',\n        name='s-actual'\n    ),\n    row=3, col=1\n)\n\nfig.show()","34d2ace7":"columns = exper6_clean.columns\nraw_cols = list(filter(lambda x: ('Current' in x) | ('Voltage' in x) | ('Power' in x), columns))\ncorr_actualcommand(exper6_clean, raw_cols+['X1_ActualPosition'])","c19f09b6":"fig = make_subplots(rows=3, cols=1)\nfig.add_trace(\n    go.Scatter(\n        x=exper6_clean['index'], y=exper6_clean['X1_ActualPosition'],\n        mode='lines',\n        name='x-actual'\n    ),\n    row=1, col=1\n)\nfig.add_trace(\n    go.Scatter(\n        x=exper6_clean['index'], y=exper6_clean['S1_OutputVoltage'],\n        mode='lines',\n        name='s-volt'\n    ),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(\n        x=exper6_clean['index'], y=exper6_clean['X1_ActualPosition'],\n        mode='lines',\n        name='x-actual'\n    ),\n    row=2, col=1\n)\nfig.add_trace(\n    go.Scatter(\n        x=exper6_clean['index'], y=exper6_clean['S1_OutputPower']*100,\n        mode='lines',\n        name='s-power'\n    ),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(\n        x=exper6_clean['index'], y=exper6_clean['X1_ActualPosition'],\n        mode='lines',\n        name='x-actual'\n    ),\n    row=3, col=1\n)\nfig.add_trace(\n    go.Scatter(\n        x=exper6_clean['index'], y=exper6_clean['S1_DCBusVoltage']*100,\n        mode='lines',\n        name='s-bus'\n    ),\n    row=3, col=1\n)\n\nfig.show()","5818dc85":"# Collect columns that are desireable in further analysis\nkeeper_cols = list(filter(lambda x: 'Z1' not in x, raw_cols))\n\n# Define number of PCA components\ncomponent = 2 # should use 2 because this fits the basic definition of the Mahalanobis distance","f1196b86":"# Perform outlier analysis\nexper1_pca, exper1_mdist = full_process(exper1_clean[keeper_cols], components=component)\n\nthresh = MD_threshold(exper1_mdist)","4a74bef7":"# Perform outlier analysis\nexper6_pca, exper6_mdist = full_process(exper6_clean[keeper_cols], components=component)","8f8d0af0":"# Load another dataset - worn\nexperiment8 = pd.read_csv(data_dir + 'experiment_08.csv')\nexperiment8.reset_index(inplace=True)\n\n# Clean up data\nexper8_clean = clean_data(experiment8)\nclean_ex8 = exper8_clean[(exper8_clean['M1_CURRENT_FEEDRATE']!=50) & (exper8_clean['X1_ActualPosition']!=198)]\nprint('')\nprint('Length of data before inaccurate points removed {:d}'.format(len(exper8_clean)))\nprint('Length of data after inaccurate points removed {:d}'.format(len(clean_ex8)))","a041e786":"# Perform outlier analysis\nexper8_pca, exper8_mdist = full_process(exper8_clean[keeper_cols], components=component)","6a68cb76":"# Load another dataset - unworn\nexperiment3 = pd.read_csv(data_dir + 'experiment_03.csv')\nexperiment3.reset_index(inplace=True)\n\n# Clean up data\nexper3_clean = clean_data(experiment3)\nclean_ex3 = exper3_clean[(exper3_clean['M1_CURRENT_FEEDRATE']!=50) & (exper3_clean['X1_ActualPosition']!=198)]\nprint('')\nprint('Length of data before inaccurate points removed {:d}'.format(len(exper3_clean)))\nprint('Length of data after inaccurate points removed {:d}'.format(len(clean_ex3)))","d92b9410":"# Perform outlier analysis\nexper3_pca, exper3_mdist = full_process(exper3_clean[keeper_cols], components=component)","01522f29":"fig = go.Figure()\nfig.add_trace(go.Scatter(\n    x=exper1_clean.reset_index().index, y=exper1_mdist,\n    mode='lines',\n    name='unworn_01'\n))\nfig.add_trace(go.Scatter(\n    x=exper6_clean.reset_index().index, y=exper6_mdist,\n    mode='lines',\n    name='worn_06'\n))\nfig.add_trace(go.Scatter(\n    x=exper8_clean.reset_index().index, y=exper8_mdist,\n    mode='lines',\n    name='worn_08'\n))\nfig.add_trace(go.Scatter(\n    x=exper3_clean.reset_index().index, y=exper3_mdist,\n    mode='lines',\n    name='unworn_03'\n))\nfig.add_shape(\n    type='line',\n    y0=thresh,\n    y1=thresh,\n    x0=0,\n    x1=max([len(exper1_mdist), len(exper6_mdist)]),\n    line=dict(color='RoyalBlue', width=2, dash='dot')\n)\nfig.update_shapes(dict(xref='x', yref='y'))\nfig.show()","28a40929":"completed_exper = outcomes[outcomes['machining_finalized']=='yes']\n\nunworn = []\nidx_unworn = []\nworn = []\nidx_worn = []\nfor i, r in completed_exper.iterrows():\n    if r['tool_condition'] == 'unworn':\n        if r['No'] < 10:\n            unw_data = pd.read_csv(data_dir + 'experiment_0{}.csv'.format(r['No']))\n        else:\n            unw_data = pd.read_csv(data_dir + 'experiment_{}.csv'.format(r['No']))\n        unw_data['Experiment'] = r['No']\n        unworn.append(unw_data)\n        idx_unworn.append(r['No'])\n    elif r['tool_condition'] == 'worn':\n        if r['No'] < 10:\n            w_data = pd.read_csv(data_dir + 'experiment_0{}.csv'.format(r['No']))\n        else:\n            w_data = pd.read_csv(data_dir + 'experiment_{}.csv'.format(r['No']))\n        w_data['Experiment'] = r['No']\n        worn.append(w_data)\n        idx_worn.append(r['No'])\n    \nunworn_df = pd.concat(unworn, ignore_index=True)\nworn_df = pd.concat(worn, ignore_index=True)","9ff55c66":"unworn_clean = clean_data(unworn_df)\nworn_clean = clean_data(worn_df)\n\nreduce_unworn = unworn_clean[(unworn_clean['M1_CURRENT_FEEDRATE']!=50) & (unworn_clean['X1_ActualPosition']!=198)]\nreduce_worn = worn_clean[(worn_clean['M1_CURRENT_FEEDRATE']!=50) & (worn_clean['X1_ActualPosition']!=198)]\n\nprint('Unworn data')\nfor ix in idx_unworn:\n    print('% data with noise experiment {}: {:.2f}'.format(\n        ix,\n        (1\n         - len(reduce_unworn[reduce_unworn['Experiment']==ix])\n         \/ len(unworn_clean[unworn_clean['Experiment']==ix]))\n    ))\n\nprint('Worn data')\nfor ix in idx_worn:\n    print('% data with noise experiment {}: {:.2f}'.format(\n        ix,\n        (1\n         - len(reduce_worn[reduce_worn['Experiment']==ix])\n         \/ len(worn_clean[worn_clean['Experiment']==ix]))\n    ))","a932c212":"# Remove bad experiment\nunworn_clean = unworn_clean[unworn_clean['Experiment']!=2]\n\n# Perform outlier analysis\nunworn_pca, unworn_mdist = full_process(unworn_clean[keeper_cols], components=component)\n\nthresh = MD_threshold(unworn_mdist)\nprint('Threshold: {:0.2f}'.format(thresh))","2391122a":"fig = go.Figure()\n\nfig.add_trace(go.Scatter(\n    x=list(range(0, len(unworn_mdist))),\n    y=unworn_mdist,\n    mode='lines',\n    name='unworn'\n))\n\nfig.add_shape(\n    type='line',\n    y0=thresh,\n    y1=thresh,\n    x0=0,\n    x1=len(unworn_mdist),\n    line=dict(color='RoyalBlue', width=2, dash='dot')\n)\nfig.update_shapes(dict(xref='x', yref='y'))\nfig.update_layout(title_text='Mahalanobis Distance Trance All Unworn Data')\nfig.show()","101e1107":"unworn_test = pd.DataFrame(unworn_pca)\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(\n    x=unworn_test[0],\n    y=unworn_test[1],\n    mode='markers',\n    name='unworn'\n))\n\nfig.update_layout(title_text='PCA Plot')\nfig.show()","b41a4505":"# Perform outlier analysis\nworn_pca = dict()\nworn_mdist = dict()\nfor ix in idx_worn:\n    worn_pca_n, worn_mdist_n = full_process(\n        worn_clean[worn_clean['Experiment']==ix][keeper_cols],\n        components=component,\n        chi2_print=False,\n        exper_num=ix\n    )\n    worn_pca[ix] = worn_pca_n\n    worn_mdist[ix] = worn_mdist_n","c698dd33":"fig = go.Figure()\n\nx_size = []\nfor key in worn_mdist:\n    x_size.append(len(worn_mdist[key]))\n    fig.add_trace(go.Scatter(\n        x=list(range(0, len(worn_mdist[key]))),\n        y=worn_mdist[key],\n        mode='lines',\n        name='exper_{}'.format(key)\n    ))\n\nfig.add_shape(\n    type='line',\n    y0=thresh,\n    y1=thresh,\n    x0=0,\n    x1=max(x_size),\n    line=dict(color='RoyalBlue', width=2, dash='dot')\n)\nfig.update_shapes(dict(xref='x', yref='y'))\nfig.update_layout(title_text='Mahalanobis Distance Trance of Each Worn Experiment')\nfig.show()","7240a81b":"fig = go.Figure()\n\nfor key in worn_pca:\n    worn_test = pd.DataFrame(worn_pca[key])\n\n    fig.add_trace(go.Scatter(\n        x=worn_test[0],\n        y=worn_test[1],\n        mode='markers',\n        name='exper_{}'.format(key)\n    ))\n    \nfig.update_layout(title_text='PCA Plot')\nfig.show()","862cb504":"Based on the amount of bad signal from above, going to eliminate experiment 2 as dead on arrival. There is just too much bad signal in this experiment to make it useful.","a9a0b168":"# Conclusion:\nBased on the Mahalanobis distance trace of the unworn data and the PCA plot, it appears that this technique is not appropriate for the dataset at hand. The reason for this is not completely clear. It could be the level of bad\/contaminated signal in the dataset. It could be the bimodal nature of the distribution. It could be the level of bad signal in the data is affecting the PCA decomposition and causing the bimodal distribution.  It could simply be that the worn tools do not generate signal picked up in the positioning sensors of the CNC that identifies them as outliers compared to unworn tools.  \n  \nOne last thing that could be checked is whether the over threshold Mahalanobis distance points correlate to the bad signal data points. However, I am not performing this analysis.\n  \nFor completeness below is the analysis applied to each worn experiment.","1f6ddb1e":"# Feature engineer\nBad practice for data science but I'm going to look at a single unworn tool experiment and a single worn tool experiment to make some generalizations about the data. Mostly looking to see that they follow the same correlations when it comes to COMMAND and ACTUAL positioning data.  \n\nMy hypothesis is that the COMMAND feature and its corresponding ACTUAL feature will both be highly correlated and represent largely the exact same value, and that these columns do not actually provide much information when it comes to detecting tool wear quality.","22eb71d7":"Assumption is that the only data that contains useful information for detecting CNC cutting health is the data that represents active cutting actions by the CNC. Other actions - start-up, repositioning, end - will be consistent across experiments regardless of the health of the cutting tool.","48a0d4ce":"Need to give credit where credit is do. The math for this notebook came from the following Medium.com article: https:\/\/towardsdatascience.com\/machine-learning-for-anomaly-detection-and-condition-monitoring-d4614e7de770","03787d23":"Give the length of data before and after the described (in dataset description) inaccurate points, one can see that about 20% of the data has bad signal.","7ec92d36":"# Objective:\nSince there is contradictory language in the description of the dataset I'm not sure that I can use this dataset as I initially desired. However, I will apply the same techinique that I was going to use to detect anomalies within the same timeseries dataset to validate my assumptions about the data itself.  \n\n_Contradictory language:_  \n\"Outputs per experiment include tool condition (unworn and worn tools) and whether or not the tool passed visual inspection.\"  \nvs  \n\"...could be performed for identification of worn and unworn cutting tools. Eight experiments were run with an unworn tool while ten were run with a worn tool (see tool_condition column...\" & \"...could be used to detect when a workpiece is not being held in the vise with sufficient pressure to pass visual inspection (see passed_visual_inspection column for indication of visual flaws).\"  \n\nThis language leaves me to question whether the worn tool experiments were started with a worn tool or started with an unworn tool and resulted in a worn tool at the end. My assumption is that the full experiment was run with either a worn or unworn tool from the beginning of that experimental run. I can validate this by applying the Mahalanobis distance calculation to an unworn tool run (or to multiple unworn tool runs) to find a threshold. Then use this threshold to detect at which point in the worn tool runs the experiment exceeds the threshold. If the threshold is exceed at or very close to the beginning of the experimental run, then the run was likely started with a worn tool. However, if the threshold is not exceeded until late in the experimental run, it is likely that the tool started out unworn and became worn by the end of the experimental run.","c59b27a0":"# Define functions","3dac533e":"This correlation coefficient value can be interpreted as a loose correlation between a tool being unworn and a part passing visual inspection.  \n\n# Data Exploration\nNow to explore data for clean up. This means dropping out data that does not accurately reflect the CNC operation (see below) and data from the start-up and shutdown of the CNC machine.  \n  \n_From data description:_ \"Note that some variables will not accurately reflect the operation of the CNC machine. This can usually be detected by when M1_CURRENT_FEEDRATE reads 50, when X1_ActualPosition reads 198, or when M1_CURRENT_PROGRAM_NUMBER does not read 0.\"  \n  \nNOTE: Based on exploration of the data, I am ignoring the part of the above comments regarding M1_CURRENT_PROGRAM_NUMBER because I don't feel that I can trust this comment. My reason for distrust is that full experiment files contain M1_CURRENT_PROGRAM_NUMBER not equal to 0, but rather equal to 1. Unless these files are completely inaccurate I'm left to assume that there is a typo in the description.","2f598f53":"__Objective:__ to use the sensor signals from the CNC machine to detect if a cutting tool being used is worn out. Theoretically this allows the CNC operator to detect in real time that the cutting tool is worn out in order to take corrective action prior to the resultant part ending in a quality failure.\n\n__Approach:__ break the problem into parts:\n- the ability to detect that the tool being used is worn out\n- determining if there is a correlation between a worn tool and a failed part\n- the ability to detect if tool wear is leading to a bad part or is within acceptable ranges.\n\n__Hypothesis:__\n- within the sensor readings from the CNC machine there is enough signal to detect that a tool is wearing out\n- if a correlation exists between a worn tool and a failed part, there is signal to detect whether the tool wear is enough to lead to a failed part","fe582135":"Based on visual inspection above and the fact that the correlation coefficients are not much larger than +\/- 0.5, I may be able to keep the signal during the inaccurate X1_ActualPosition readings in the analysis rather than cutting it out when I keep only the raw signals rather than the position\/velocity\/acceleration signals.","64da5aa2":"# Explore outcomes","8e85cb94":"Give the length of data before and after the described (in dataset description) inaccurate points, it is safe to conclude that experiment 1 produced a clean dataset that did not contain false signals.","6c7f6f51":"It appears that the bad signal present in the data is causing correlation between features that should not show correlation - e.g. X1_ActualPosition and Y1_ActualPosition or X1_ActualPosition and Z1_ActualPosition. The line plots allow visual validation that the bad readings represented by X1_ActualPosition = 198 are cross contaminating the signal from the other sensors.   \n  \nNow I am left to question whether these false readings in the command and actual positions are bleeding into the raw current, voltage, and power signals. This is vital because my hypothesis is that these signals provide better signal with which to detect the tool wear.","0cd63d42":"This analysis is not turning out to be effect. \"Worn\" tools fall within the Mahalanobis distance threshold more frequently than the \"unworn\" tools. I can't determine why. Thinking it could be the wrong technique or it could be the noise in the data overwhelming the signal.  \n  \n# Last ditch effort\nCombine all of the unworn datasets into one and then calculate the Mahalanobis distance from the combined dataset."}}