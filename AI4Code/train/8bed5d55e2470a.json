{"cell_type":{"4726d99e":"code","2de56c30":"code","68a7468a":"code","971968f0":"code","60eb3ed7":"code","182c2177":"code","3b767834":"code","9c6aa688":"code","3608ffcd":"code","f22219a9":"code","e7e523e6":"code","1f132df6":"code","fccafad8":"code","ab951944":"code","4168338b":"code","ce5150b6":"code","7823bde9":"code","d69c01b7":"code","db64e387":"code","945ddcad":"code","43a04216":"code","09e89882":"code","5ad6d19e":"code","14e23bc0":"markdown","e006ddb4":"markdown"},"source":{"4726d99e":"import numpy as np \nimport pandas as pd \nimport os\nfrom tqdm import tqdm, trange\nfrom sklearn.metrics import roc_auc_score\nimport pickle\nimport gc","2de56c30":"os.listdir()","68a7468a":"BERT_MODEL = 'bert-base-uncased'\nCASED = 'uncased' in BERT_MODEL\nINPUT = '..\/input\/jigsaw-bert-preprocessed-input\/'\nTEXT_COL = 'comment_text'\nMAXLEN = 250","971968f0":"os.system('pip install --no-index --find-links=\"..\/input\/pytorchpretrainedbert\/\" pytorch_pretrained_bert')","60eb3ed7":"from pytorch_pretrained_bert import BertTokenizer\nfrom pytorch_pretrained_bert.modeling import BertModel\n","182c2177":"BERT_FP = '..\/input\/torch-bert-weights\/bert-base-uncased\/bert-base-uncased\/'","3b767834":"def get_bert_embed_matrix():\n    bert = BertModel.from_pretrained(BERT_FP)\n    bert_embeddings = list(bert.children())[0]\n    bert_word_embeddings = list(bert_embeddings.children())[0]\n    mat = bert_word_embeddings.weight.data.numpy()\n    return mat","9c6aa688":" # tokenizer = BertTokenizer.from_pretrained(BERT_MODEL,do_lower_case = CASED)","3608ffcd":"embedding_matrix = get_bert_embed_matrix()","f22219a9":"train = pd.read_csv(INPUT + 'train_bert-base-uncased_ids.csv').sample(frac = 1.0, random_state = 23)","e7e523e6":"test = pd.read_csv(INPUT + 'test_bert-base-uncased_ids.csv')","1f132df6":"x_train = np.zeros((train.shape[0],MAXLEN),dtype=np.int)\n\nfor i,ids in tqdm(enumerate(list(train[TEXT_COL]))):\n\n    input_ids = [int(i) for i in ids.split()[:MAXLEN]]\n    inp_len = len(input_ids)\n    x_train[i,:inp_len] = np.array(input_ids)\n    \nx_test = np.zeros((test.shape[0],MAXLEN),dtype=np.int)\n\nfor i,ids in tqdm(enumerate(list(test[TEXT_COL]))):\n\n    input_ids = [int(i) for i in ids.split()[:MAXLEN]]\n    inp_len = len(input_ids)\n    x_test[i,:inp_len] = np.array(input_ids)\n    \nwith open('temporary.pickle', mode='wb') as f:\n    pickle.dump(x_test, f) # use temporary file to reduce memory\n\ndel x_test\ngc.collect()","fccafad8":"\nidentity_columns = ['male','female','homosexual_gay_or_lesbian','christian','jewish','muslim','black','white','psychiatric_or_mental_illness']\ny_identities = (train[identity_columns] >= 0.5).astype(int).values\n\n# Overall\nweights = np.ones((len(train),)) \/ 4\n# Subgroup\nweights += (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) \/ 4\n# Background Positive, Subgroup Negative\nweights += (( (train['target'].values>=0.5).astype(bool).astype(np.int) +\n   (train[identity_columns].fillna(0).values<0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) \/ 4\n# Background Negative, Subgroup Positive\nweights += (( (train['target'].values<0.5).astype(bool).astype(np.int) +\n   (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) \/ 4\nloss_weight = 1.0 \/ weights.mean()\n\ny_train = np.vstack([(train['target'].values>=0.5).astype(np.int),weights]).T\ny_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']].values\n\n\n    \n","ab951944":"from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\nfrom keras.layers import Input, Dense, Embedding, SpatialDropout1D, Dropout, add, concatenate\nfrom keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom keras.preprocessing import text, sequence\nfrom keras.losses import binary_crossentropy\nfrom keras import backend as K\nimport keras.layers as L\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints, optimizers, layers\n\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold","4168338b":"def build_model(embedding_matrix, num_aux_targets, loss_weight):\n    '''\n    credits go to: https:\/\/www.kaggle.com\/thousandvoices\/simple-lstm\/\n    '''\n    words = Input(shape=(MAXLEN,))\n    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n    x = SpatialDropout1D(0.3)(x)\n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n\n    hidden = concatenate([GlobalMaxPooling1D()(x),GlobalAveragePooling1D()(x),])\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    result = Dense(1, activation='sigmoid')(hidden)\n    aux_result = Dense(num_aux_targets, activation='sigmoid')(hidden)\n    \n    model = Model(inputs=words, outputs=[result, aux_result])\n    model.compile(loss=[custom_loss,'binary_crossentropy'], loss_weights=[loss_weight, 1.0], optimizer='adam')\n\n    return model","ce5150b6":"def custom_loss(y_true, y_pred):\n    return binary_crossentropy(K.reshape(y_true[:,0],(-1,1)), y_pred) * y_true[:,1]\n","7823bde9":"from sklearn.model_selection import train_test_split\n\ntr_ind, val_ind = train_test_split(list(range(len(x_train))) ,test_size = 0.05, random_state = 23)","d69c01b7":"import gc\nNUM_MODELS = 1\n\nBATCH_SIZE = 512\nEPOCHS = 5\nLSTM_UNITS = 128\nDENSE_HIDDEN_UNITS = 512\ncheckpoint_predictions = []\ncheckpoint_val_preds = []\nweights = []\n\nfor model_idx in range(NUM_MODELS):\n    model = build_model(embedding_matrix, y_aux_train.shape[-1],loss_weight)\n    for global_epoch in range(EPOCHS):\n        model.fit(x_train[tr_ind],[y_train[tr_ind], y_aux_train[tr_ind]],validation_data = (x_train[val_ind],[y_train[val_ind], y_aux_train[val_ind]]),\n            batch_size=BATCH_SIZE,\n            epochs=1,\n            verbose=1,\n            callbacks=[\n                LearningRateScheduler(lambda epoch: 1e-3 * (0.6 ** global_epoch))\n            ]\n        )\n        with open('temporary.pickle', mode='rb') as f:\n            x_test = pickle.load(f) # use temporary file to reduce memory\n        checkpoint_predictions.append(model.predict(x_test, batch_size=2048)[0].flatten())\n        checkpoint_val_preds.append(model.predict(x_train[val_ind], batch_size=2048)[0].flatten())\n        del x_test\n        gc.collect()\n        weights.append(2 ** global_epoch)\n    del model\n    gc.collect()\n\npredictions = np.average(checkpoint_predictions, weights=weights, axis=0)","db64e387":"val_preds = np.average(checkpoint_val_preds, weights=weights, axis=0)","945ddcad":"from sklearn.metrics import roc_auc_score\n\ndef power_mean(x, p=-5):\n    return np.power(np.mean(np.power(x, p)),1\/p)\n\ndef get_s_auc(y_true,y_pred,y_identity):\n    mask = y_identity==1\n    try:\n        s_auc = roc_auc_score(y_true[mask],y_pred[mask])\n    except:\n        s_auc = 1\n    return s_auc\n\ndef get_bpsn_auc(y_true,y_pred,y_identity):\n    mask = (y_identity==1) & (y_true==0) | (y_identity==0) & (y_true==1)\n    try:\n        bpsn_auc = roc_auc_score(y_true[mask],y_pred[mask])\n    except:\n        bpsn_auc = 1\n    return bpsn_auc\n\ndef get_bspn_auc(y_true,y_pred,y_identity):\n    mask = (y_identity==1) & (y_true==1) | (y_identity==0) & (y_true==0)\n    try:\n        bspn_auc = roc_auc_score(y_true[mask],y_pred[mask])\n    except:\n        bspn_auc = 1\n    return bspn_auc\n\ndef get_total_auc(y_true,y_pred,y_identities):\n\n    N = y_identities.shape[1]\n    \n    saucs = np.array([get_s_auc(y_true,y_pred,y_identities[:,i]) for i in range(N)])\n    bpsns = np.array([get_bpsn_auc(y_true,y_pred,y_identities[:,i]) for i in range(N)])\n    bspns = np.array([get_bspn_auc(y_true,y_pred,y_identities[:,i]) for i in range(N)])\n\n    M_s_auc = power_mean(saucs)\n    M_bpsns_auc = power_mean(bpsns)\n    M_bspns_auc = power_mean(bspns)\n    rauc = roc_auc_score(y_true,y_pred)\n\n\n    total_auc = M_s_auc + M_bpsns_auc + M_bspns_auc + rauc\n    total_auc\/= 4\n\n    return total_auc","43a04216":"y_train[val_ind][:,0].shape, val_preds.shape,y_identities[val_ind].shape","09e89882":"get_total_auc(y_train[val_ind][:,0],val_preds,y_identities[val_ind])","5ad6d19e":"df_submit = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/sample_submission.csv')\ndf_submit.prediction = predictions\ndf_submit.to_csv('submission.csv', index=False)","14e23bc0":"2. create BERT model and put on GPU","e006ddb4":"1. pip install pytorch-pretrained-bert without internet"}}