{"cell_type":{"2066dc57":"code","452fafa7":"code","e03651e2":"code","476f5d58":"code","079b8566":"code","170e2ead":"code","954ee1e8":"code","9058a9ca":"code","de90cb8e":"code","c6c85836":"code","796852a6":"code","a57f32c3":"code","24ea813a":"code","67380b4d":"code","b8895bf0":"code","ac181dd0":"code","7f0c464e":"code","940b4a98":"code","388732c1":"code","ed50e70e":"code","bf788f73":"code","3b6c9696":"code","0ee9e498":"code","8bafbf96":"code","ada01a07":"code","13dd58ac":"code","45456b39":"code","f1d8832e":"code","c87567ab":"markdown","09e170ac":"markdown","d51b0e58":"markdown","100fbd0a":"markdown","14071c9f":"markdown","0e472e9c":"markdown","881412ad":"markdown","231e80d5":"markdown","546d1373":"markdown","215790f1":"markdown","9ed2e971":"markdown"},"source":{"2066dc57":"import pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import defaultdict\nfrom sklearn.preprocessing import LabelEncoder\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import FactorAnalysis, PCA\nfrom sklearn.preprocessing import StandardScaler","452fafa7":"data=pd.read_csv('..\/input\/gender-classification-dataset\/gender_classification_v7.csv')","e03651e2":"data.head(10)","476f5d58":"data.info()","079b8566":"data.shape","170e2ead":"data.describe()","954ee1e8":"data.isnull().sum()","9058a9ca":"plt.figure(figsize=(10,10))\nexplode=[0.1,0]\ndata['gender'].value_counts().plot(kind='pie',autopct=\"%2i%%\",explode=explode)\nplt.legend()\nplt.show()","de90cb8e":"datacorr=data.copy()\n#Changing to numeric data\ndatacorr['gender'] = datacorr['gender'].str.replace('Male', '1').str.replace('Female', '0').astype('float')\ncorr = datacorr.corr() #Finding a couple correlation in the data\nmask = np.zeros_like(corr) #Make them all 0\nmask[np.tril_indices_from(mask)] = True #Triangle of values 1\nf, ax = plt.subplots(figsize=(15, 14))\nsns.heatmap(corr, vmax=1, square=True,annot=True,cmap=\"pink\" ,mask=mask.T)\n\nplt.title('Pearson Correlation between different fearures')","c6c85836":"sns.lmplot(x=\"forehead_width_cm\", y=\"forehead_height_cm\", hue=\"gender\", data=data, palette = 'rainbow', height = 7)\nax = plt.gca()\nax.set_title(\"width vs height of forehead\")","796852a6":"sns.lmplot(x=\"lips_thin\", y=\"distance_nose_to_lip_long\", hue=\"gender\", data=data, palette = 'inferno_r', height = 7)\nax = plt.gca()\nax.set_title(\"lips thin vs distance from nose to lip long\")","a57f32c3":"sns.lmplot(x=\"nose_wide\", y=\"nose_long\", hue=\"gender\", data=data, palette = 'inferno_r', height = 7)\nax = plt.gca()\nax.set_title(\"wide vs long of nose\")","24ea813a":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, auc, classification_report,confusion_matrix\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import  roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn import metrics\nfrom sklearn.model_selection import cross_val_score","67380b4d":"#changing the column 'gender' to numeric data\ndata['gender'] = data['gender'].str.replace('Male', '1').str.replace('Female', '0').astype('float')","b8895bf0":"y = data.gender\nX = data.drop(['gender'], axis = 1) #Gender is the purpose of the prediction\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state =np.random.RandomState(0))","ac181dd0":"# try K=1 through K=25 and record testing accuracy\nk_range = list(range(1, 26))\nscores = []\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train, y_train)\n    y_pred = knn.predict(X_test)\n    scores.append(metrics.accuracy_score(y_test, y_pred))\nscores","7f0c464e":"# the connection between K and testing accuracy\nplt.plot(k_range, scores)\nplt.xlabel('Neighbors')\nplt.ylabel('Testing Accuracy')","940b4a98":"#cross validation\nscore = cross_val_score(knn, X, y, cv=10, scoring='accuracy')\nprint(score)","388732c1":"print(score.mean())","ed50e70e":"# instantiate the model with the best known parameters\nknn=KNeighborsClassifier(n_neighbors=12)\n# train the model with X and y (not X_train and y_train)\nknn.fit(X_train, y_train)\n# make a prediction for an out-of-sample observation\npred=knn.predict(X_test)\ncon=confusion_matrix(y_test, pred)\nf, ax = plt.subplots(figsize=(11, 8))\nsns.heatmap(con, annot=True, fmt='g', cmap=\"YlOrRd\")\nplt.title('Confiusion Matrix')","bf788f73":"metrics.accuracy_score(y_test, pred)","3b6c9696":"fpr, tpr, _= roc_curve(y_test, pred)\nauc= roc_auc_score(y_test, pred)\nplt.plot(fpr, tpr, label=\"auc=\"+str(auc))\nplt.box(False)\nplt.title('ROC CURVE KNN')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate (Recall)')\nplt.grid(True)\nplt.show()\n \nprint(f\"The score for the ROC Curve is: {round(auc,3)*100}%\")","0ee9e498":"reglog=LogisticRegression()\nreglog=LogisticRegression(solver='liblinear',C=0.1,penalty='l2')\n# fit the model with data\nreglog.fit(X_train,y_train)\n# predict the response values for the observations in X\nreglog.predict(X_test)","8bafbf96":"# check how many predictions were generated\ny_pred = reglog.predict(X_test)\nlen(y_pred)","ada01a07":"#check accuracy of the model\nmetrics.accuracy_score(y_test, y_pred)","13dd58ac":"mypred=reglog.predict(X_train)\nmetrics.accuracy_score(y_train, mypred)","45456b39":"plt.figure(1, figsize= (10,10))\nplt.title(\"Confusion matrix for \"+'LogisticRegression')\nmat = confusion_matrix(y_test,y_pred)\nsns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False)\nplt.xlabel('true label')\nplt.ylabel('predicted label')\nplt.show()\nplt.close()     ","f1d8832e":"fpr, tpr, _= roc_curve(y_test, y_pred)\nauc= roc_auc_score(y_test, y_pred)\nplt.plot(fpr, tpr, label=\"auc=\"+str(auc))\nplt.box(False)\nplt.title('ROC CURVE LogisticRegression')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate (Recall)')\nplt.grid(True)\nplt.show()\n \nprint(f\"The score for the ROC Curve is: {round(auc,3)*100}%\")","c87567ab":"# Gender Classification","09e170ac":"# Logistic Regression","d51b0e58":"## K Nearest Neighbours ","100fbd0a":"#### We can see that our data is clean from Nulls, Later we will also replace the \"gender\" column to numeric data, Then we can work with numeric data only.","14071c9f":"### From the above graph it can be concluded that the ideal k is K=12.","0e472e9c":"## Model selection\nOur goal is to train the model so that it can identify whether it is a man or a woman.\nFor this we will use two models: KNN and Logistic-Regression.","881412ad":"# Modeling","231e80d5":"### This graph describe the longer and wider the forehead, the more likely it is a man","546d1373":"# An explanation of each feature\n* long_hair - This column contains 0's and 1's where 1 is \"long hair\" and 0 is \"not long hair\".\n* forehead_width_cm - This column is in CM's. This is the width of the forehead.\n* forehead_height_cm - This is the height of the forehead and it's in Cm's.\n* nose_wide - This column contains 0's and 1's where 1 is \"Long nose\" and 0 is \"not long nose\".\n* nose_long - This column contains 0's and 1's where 1 is \"Long nose\" and 0 is \"not long nose\".\n* lips_thin - This column contains 0's and 1's where 1 represents the \"thin lips\" while 0 is \"Not thin lips\".\t\n* distance_nose_to_lip_long\tgender - This column contains 0's and 1's where 1 represents the \"long distance between nose and lips\" while 0 is \"short distance between nose and lips\".","215790f1":"### Here also the division is really equal - men have longer and wider nose then women.","9ed2e971":"### We can conclude from the graph above that the division is really equal - men have thinner lips and a greater distance from the lip to the chin."}}