{"cell_type":{"c25f9ec9":"code","2846c9aa":"code","f830ce8b":"code","b4463859":"code","85ea44f3":"code","46f8300b":"code","e246f602":"code","d4c83588":"code","49e6994e":"code","f89343a2":"code","923b56f0":"code","b6b9e737":"code","65712585":"code","feae204b":"code","65e0674e":"code","b432f13b":"code","0c3495bd":"code","25b57a89":"code","06a9d081":"code","ca29c31c":"code","1ab5666e":"markdown","1e7d35cc":"markdown","d7ec1b2c":"markdown","7491f68f":"markdown","5d0a701b":"markdown","27e8c496":"markdown","6e600626":"markdown","b64c75b4":"markdown","e100b8e6":"markdown","42051344":"markdown","09ea670d":"markdown","bbe231c6":"markdown","83975334":"markdown","3cb066f4":"markdown","f0aea787":"markdown","17d1f63c":"markdown","b3789425":"markdown","0156b000":"markdown","13235aab":"markdown","6397a763":"markdown","110c16ca":"markdown","4726f728":"markdown","9a89805d":"markdown","9927f042":"markdown","a3ac9032":"markdown","f9fdbbcd":"markdown","713bc99a":"markdown","53ce9925":"markdown","3cfbda0a":"markdown"},"source":{"c25f9ec9":"#Importing all packages used in this notebook.\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","2846c9aa":"dataset = pd.read_csv(\"..\/input\/breast-cancer-wisconsin-data\/data.csv\")","f830ce8b":"dataset.columns","b4463859":"dataset.sample(7)","85ea44f3":"num_M_B = pd.Series(dataset.diagnosis.value_counts(),name=\"Malignant or Benign\")\nnum_M_B = pd.DataFrame(num_M_B)\nplot = num_M_B.plot.pie(y=\"Malignant or Benign\", figsize=(5, 5))","46f8300b":"dataset.info()","e246f602":"dataset.drop([\"id\",\"Unnamed: 32\"],axis=1,inplace=True)\nlabel_encoder = LabelEncoder()\ndataset[\"diagnosis_encoded\"] = label_encoder.fit_transform(dataset[\"diagnosis\"]) \ndataset.drop([\"diagnosis\"],axis=1,inplace=True)\ndataset.sample(3)","d4c83588":"corr_matrix = dataset.corr()\ncorr_matrix[\"diagnosis_encoded\"].sort_values(ascending=False)","49e6994e":"dataset.shape","f89343a2":"y = dataset[\"diagnosis_encoded\"]\nX = dataset.loc[:, dataset.columns != \"diagnosis_encoded\"]\nX = StandardScaler().fit_transform(X)\nX_train , X_test , y_train , y_test = train_test_split(X,y,test_size=0.25,random_state=42,stratify=y)","923b56f0":"print(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","b6b9e737":"model_svc = SVC(random_state = 17)\nparameters_for_svc = [{'C': [0.25, 0.5, 0.75, 1],\n                       'kernel': [ \"poly\" , \"rbf\"],\n                       'degree' : [3],\n                       'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9] },]\ngrid_search_svc = GridSearchCV(estimator=model_svc,\n                               param_grid = parameters_for_svc,\n                               scoring = 'accuracy',\n                               cv = 10,\n                               n_jobs = -1)\ngrid_search_svc.fit(X_train,y_train)","65712585":"best_accuracy = grid_search_svc.best_score_\nprint(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\nbest_parameters = grid_search_svc.best_params_\nprint(\"Best Parameters:\", best_parameters)","feae204b":"model_rf = RandomForestClassifier(n_estimators=10,random_state=17)\nparameters_for_rf = [{'n_estimators':range(10,41)}]\ngrid_search_rf = GridSearchCV(estimator=model_rf,\n                               param_grid = parameters_for_rf,\n                               scoring = 'accuracy',\n                               cv = 10,\n                               n_jobs = -1)\ngrid_search_rf.fit(X_train,y_train)","65e0674e":"best_accuracy = grid_search_rf.best_score_\nprint(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\nbest_parameters = grid_search_rf.best_params_\nprint(\"Best Parameters:\", best_parameters)","b432f13b":"log_reg = LogisticRegression(random_state=17)\nparameters_of_lr = [{'C':[0.25,0.5,0.75,1],'penalty':[\"l1\",\"l2\"]}]\ngrid_search_lr = GridSearchCV(estimator=log_reg,\n                               param_grid = parameters_of_lr,\n                               scoring = 'accuracy',\n                               cv = 10,\n                               n_jobs = -1)\ngrid_search_lr.fit(X_train,y_train)","0c3495bd":"best_accuracy_lr = grid_search_lr.best_score_\nprint(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\nbest_parameters_lr = grid_search_lr.best_params_\nprint(\"Best Parameters:\", best_parameters_lr)","25b57a89":"model_1 = SVC(C=0.5,degree=3,gamma=0.2,kernel=\"poly\",random_state=17,probability=True)\nmodel_2 = RandomForestClassifier(n_estimators=24,random_state=17)\nmodel_3 = LogisticRegression(C=1,penalty=\"l2\",random_state=17)","06a9d081":"voting_clf = VotingClassifier(\n    estimators=[('lr', model_3), ('rf', model_2), ('svc', model_1)],\n    voting='soft') \nvoting_clf.fit(X_train,y_train)","ca29c31c":"y_pred = voting_clf.predict(X_test)\nconfusion_matrix(y_test,y_pred)\ncm = confusion_matrix(y_test,y_pred)\nsns.heatmap(cm, annot=True)\nplt.show()\nprint(accuracy_score(y_test,y_pred)*100)","1ab5666e":"What are the results?\n<br>We will use the \"Best Model\" hyperparameters for training our ensemble model.","1e7d35cc":"Lets look at the correlation between features.\n<br>Since scaling features has no impact on correlations between them its a good idea to see what we are dealing with.","d7ec1b2c":"We start by SVC.\n<br>First we perform grid search to find the best best parameters for our model.","7491f68f":"### Model 2 : RandomForestClassifier","5d0a701b":"Based on the pie chart instances with \"Benign\" label are more than the ones with \"Malignant\" label.\n<br> So we need to perform stratified sampling.","27e8c496":"96\/5% Accuracy,could have been better but since we performed cross validation to prevent overfitting and data leakage this results are very good!\n<br>We are confident that deploying this model will result in similar accuracy.","6e600626":"Now we have 31 features and 569 instances.\n<br>We also need to scale our data.(Scaling the dataset is essential since different features have different scales in this dataset.)\n<br>After scaling our dataset its time to split our dataeset.(Stratified Sampling)","b64c75b4":"Now that we have trained all models its time for elections ...","e100b8e6":"## Training Models\n<br> The task is pretty straight forward : Use grid search to make three good enough models,then make an ensemble model based on those three and predict new values!\n(All three will use cross validation)","42051344":"That looks better!","09ea670d":"### Model 3 : LogisticRegression","bbe231c6":"Features have very different scales so that needs to be taken care of.\n<br>And most importantly there's an unnamed feature called \"Unnamed:32\" which all random sample values are \"NaN\" for some reason.","83975334":"# Breast Cancer Softvote Classification (Ensemble Model)","3cb066f4":"After importing the dataset we need to get an insight about the dataset.(Basic information)\n<br>In order to do this we return the column names.","f0aea787":"Wow!\n<br>Most of the features have an impact on diagnosis.\n<br>In this case its better not to drop any feature,instead its better to use some dimensionality reduction algorithm or let the model use feature importance. \n<br>Since the dataset is not very large we wont use dimensionality reduction and let the model handle the features.","17d1f63c":"Surprisingly all values inside \"Unnamed:32\" are null values!So, we drop it all together.\n<br>All features data types is float so we only need to scale the data.(Soon)\n<br>We don't care about the \"id\" column since it is not a time series question.\n<br>And at last we need to convert our labes into zeros and ones.(Since it's binary classification)","b3789425":"## Making an ensemble model based on different classifiers","0156b000":"How many of the instances are Malignant and how many are Benign?\n<br>Knowing this is important, if one of the groups has more instances we can't split the training and test set randomly.(We need to perform stratifies sampling)","13235aab":"First of all we need to load the data as a pandas dataframe.","6397a763":"As someone who has no domain expertise,Its better to do some EDA...\n<br>Before getting into detail it is better to take a simple look at the dataset, therefore we return 7 random samples from the dataset.(7 is my lucky number)","110c16ca":"### Model 1 : SVC","4726f728":"###  What is our goal?\nOur goal is to make a classifier which can classify new instances as \"Malignant\" or \"Benign\" without overfitting on the training set. \n<br>We will achieve this by using cross validation and making an ensemble model.\n<br>The ensemble model is based on three different classifiers which are separately good enough.","9a89805d":"Now we took a quick look at the dataset and need to gather more info in detail.","9927f042":"Now we are gonna train our models again but this time with hyperparameters we got from our grid searches.\n<br>We can use the best models from our grid search but its easier(and manual) if we train them again.","a3ac9032":"Now that we have a good enough svc classifeir we are gonna use a second classifier for our ensemble model.","f9fdbbcd":"Since there is trust(or maybe not) between classifiers we will use soft voting instead of hard voting.","713bc99a":"Now that we have trained our ensemble model its time to use our test set.\n<br>Lets see the results!","53ce9925":"Since we are using cross validation we wont need a separate validation set.\n<br>Double checking the training and test sets.","3cfbda0a":"Not bad at all!\n<br>We have two classifiers which perform good enough.\n<br>Now its time for the third one."}}