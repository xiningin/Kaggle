{"cell_type":{"8a16c077":"code","0f1e0ed8":"code","e0921142":"code","637c8f93":"code","cf9c8223":"code","4f44174a":"code","2177d1cf":"code","d8dd8840":"code","8299e08f":"code","f0d8229b":"code","4b43f5ce":"code","15b80260":"code","a2b51ec0":"code","b21e0b09":"code","201c2613":"code","cb24c6ec":"code","c885d284":"code","fa3e323e":"code","69ecb8b9":"code","997f7a48":"code","45a3d422":"code","d09b358a":"code","c037d7b9":"code","7e3bbdaa":"code","bbfe342c":"code","b692d171":"code","db69bbe2":"code","a5d6385f":"code","e0a7f3a8":"code","b87e8814":"code","e1518620":"code","13bcf937":"code","55449288":"code","764ebb47":"code","fe56e93e":"code","a0d0a845":"code","48d165ae":"code","b21d06cd":"code","6dd1a265":"code","65d5d7d6":"code","3a97b56b":"code","d21b956d":"code","9f9d9a5b":"code","0b9250a2":"code","6846a97f":"code","e2ef562d":"code","2523a36d":"code","9437a016":"code","b32ffb62":"code","d3025933":"code","bda641a6":"code","369e3c85":"code","d1367c85":"code","96d90e8e":"code","0bdf693e":"code","59c06123":"code","f6c73343":"code","79551953":"code","a7e6f9db":"code","a80238c2":"code","b3aec6e6":"code","e004daea":"code","c0893e44":"code","fa258888":"code","37f4c482":"code","92276635":"code","96561822":"code","a3c8e329":"code","ece19f27":"code","c1b6ac5d":"code","b397a700":"code","1451e534":"code","dfb722ce":"code","d58e8d4c":"code","4089df14":"code","8ae15be5":"code","a3e6ded1":"code","fdf8cda4":"code","6783a538":"code","e7efda33":"markdown","47e9d75e":"markdown","df75ada5":"markdown","646e0247":"markdown","57ce4c30":"markdown","b33c08ed":"markdown","a82df9ca":"markdown","6de5b34c":"markdown","604216e5":"markdown","46d55f83":"markdown","436954f5":"markdown","6a6c392c":"markdown","afb8424f":"markdown","26922dbb":"markdown","effca261":"markdown","9d2af164":"markdown","6303c4be":"markdown","fcaf2907":"markdown","00b80b93":"markdown","045d9106":"markdown","bd6deba8":"markdown","7a46d043":"markdown","b3bf4c2c":"markdown","e509611a":"markdown","0709dcfa":"markdown","b99f68fa":"markdown","ec9820e1":"markdown","e839ade2":"markdown","2d8e9429":"markdown","4afd9709":"markdown","9ecc0d8d":"markdown","cf4fd14a":"markdown","c2eceb91":"markdown","d6c735ce":"markdown","e1dd1896":"markdown","014d803d":"markdown","e2efab6c":"markdown","de431a81":"markdown","e0877ac6":"markdown","e02c8d3c":"markdown","cb74bed4":"markdown","1f8d1c7b":"markdown","f8f53b09":"markdown","8609d25f":"markdown","2d02518f":"markdown","6d9f0cdd":"markdown","d7643239":"markdown","64c4f59e":"markdown","dd4efd9e":"markdown","365df8a2":"markdown","89af9284":"markdown","97f36277":"markdown","6062ccc4":"markdown","2a764869":"markdown","f263c475":"markdown","09cf432d":"markdown","afcde5a5":"markdown"},"source":{"8a16c077":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0f1e0ed8":"# Gi\u1ea3i n\u00e9n data\nimport zipfile\nunzip = zipfile.ZipFile('\/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv.zip')\nunzip.extractall()\nunzip = zipfile.ZipFile('\/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/test.csv.zip')\nunzip.extractall()","e0921142":"#Kh\u1edfi ch\u1ea1y th\u1eed data c\u1ea7n train v\u00e0 nh\u1eadn x\u00e9t\n\ntrain_df = pd.read_csv('\/kaggle\/working\/train.csv')# Loading data\ntrain_df.head() #show data","637c8f93":"#Kh\u1edfi ch\u1ea1y th\u1eed data c\u1ea7n test\n\ntest_df = pd.read_csv('\/kaggle\/working\/test.csv')# Loading data\ntest_df.head() #show data","cf9c8223":"import nltk\nfrom nltk.corpus import stopwords  # X\u00f3a k\u00fd t\u1ef1 kh\u00f4ng c\u1ea7n thi\u1ebft\nfrom nltk.stem.lancaster import LancasterStemmer # convert words to base form","4f44174a":"# T\u1ea3i stopword trong th\u01b0 vi\u1ec7n nltk\nnltk.download('stopwords')","2177d1cf":"set(stopwords.words('english')) # Chon English","d8dd8840":"# Hi\u1ec3n th\u1ecb th\u1eed info c\u1ee7a data\ntrain_df.info()","8299e08f":"train_df['comment_text'][0]","f0d8229b":"train_df['comment_text'][1]","4b43f5ce":"#Tr\u1ea3 v\u1ec1 m\u1ed9t Chu\u1ed7i ch\u1ee9a s\u1ed1 l\u01b0\u1ee3ng c\u00e1c gi\u00e1 tr\u1ecb\ntrain_df.toxic.value_counts(normalize=True)\n# normalize=True th\u00ec \u0111\u1ed1i t\u01b0\u1ee3ng \u0111\u01b0\u1ee3c tr\u1ea3 v\u1ec1 s\u1ebd ch\u1ee9a c\u00e1c t\u1ea7n s\u1ed1 t\u01b0\u01a1ng \u0111\u1ed1i c\u1ee7a c\u00e1c gi\u00e1 tr\u1ecb","15b80260":"#Tr\u1ea3 v\u1ec1 m\u1ed9t Chu\u1ed7i ch\u1ee9a s\u1ed1 l\u01b0\u1ee3ng c\u00e1c gi\u00e1 tr\u1ecb \ntrain_df.severe_toxic.value_counts(normalize=True)\n# normalize=True th\u00ec \u0111\u1ed1i t\u01b0\u1ee3ng \u0111\u01b0\u1ee3c tr\u1ea3 v\u1ec1 s\u1ebd ch\u1ee9a c\u00e1c t\u1ea7n s\u1ed1 t\u01b0\u01a1ng \u0111\u1ed1i c\u1ee7a c\u00e1c gi\u00e1 tr\u1ecb","a2b51ec0":"#Tr\u1ea3 v\u1ec1 m\u1ed9t Chu\u1ed7i ch\u1ee9a s\u1ed1 l\u01b0\u1ee3ng c\u00e1c gi\u00e1 tr\u1ecb\ntrain_df.obscene.value_counts(normalize=True)\n# normalize=True th\u00ec \u0111\u1ed1i t\u01b0\u1ee3ng \u0111\u01b0\u1ee3c tr\u1ea3 v\u1ec1 s\u1ebd ch\u1ee9a c\u00e1c t\u1ea7n s\u1ed1 t\u01b0\u01a1ng \u0111\u1ed1i c\u1ee7a c\u00e1c gi\u00e1 tr\u1ecb","b21e0b09":"#Tr\u1ea3 v\u1ec1 m\u1ed9t Chu\u1ed7i ch\u1ee9a s\u1ed1 l\u01b0\u1ee3ng c\u00e1c gi\u00e1 tr\u1ecb\ntrain_df.insult.value_counts(normalize=True)\n# normalize=True th\u00ec \u0111\u1ed1i t\u01b0\u1ee3ng \u0111\u01b0\u1ee3c tr\u1ea3 v\u1ec1 s\u1ebd ch\u1ee9a c\u00e1c t\u1ea7n s\u1ed1 t\u01b0\u01a1ng \u0111\u1ed1i c\u1ee7a c\u00e1c gi\u00e1 tr\u1ecb","201c2613":"#Tr\u1ea3 v\u1ec1 m\u1ed9t Chu\u1ed7i ch\u1ee9a s\u1ed1 l\u01b0\u1ee3ng c\u00e1c gi\u00e1 tr\u1ecb\ntrain_df.threat.value_counts(normalize=True)\n# normalize=True th\u00ec \u0111\u1ed1i t\u01b0\u1ee3ng \u0111\u01b0\u1ee3c tr\u1ea3 v\u1ec1 s\u1ebd ch\u1ee9a c\u00e1c t\u1ea7n s\u1ed1 t\u01b0\u01a1ng \u0111\u1ed1i c\u1ee7a c\u00e1c gi\u00e1 tr\u1ecb","cb24c6ec":"#Tr\u1ea3 v\u1ec1 m\u1ed9t Chu\u1ed7i ch\u1ee9a s\u1ed1 l\u01b0\u1ee3ng c\u00e1c gi\u00e1 tr\u1ecb\ntrain_df.identity_hate.value_counts(normalize=True)\n# normalize=True th\u00ec \u0111\u1ed1i t\u01b0\u1ee3ng \u0111\u01b0\u1ee3c tr\u1ea3 v\u1ec1 s\u1ebd ch\u1ee9a c\u00e1c t\u1ea7n s\u1ed1 t\u01b0\u01a1ng \u0111\u1ed1i c\u1ee7a c\u00e1c gi\u00e1 tr\u1ecb","c885d284":"train_df.head()","fa3e323e":"#t\u00ednh t\u1ed5ng c\u00e1c gi\u00e1 tr\u1ecb l\u1ea5y t\u1eeb c\u1ed9t th\u1ee9 3\ndata_count=train_df.iloc[:,2:].sum() \ndata_count","69ecb8b9":"import matplotlib.pyplot as plt\nimport nltk\nimport re\nimport string\nimport seaborn as sns\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer","997f7a48":"#s\u1eed d\u1ee5ng plot bi\u1ec3u \u0111\u1ed3\nplt.figure(figsize=(8,4))\n\n#S\u1eed d\u1ee5ng ph\u01b0\u01a1ng th\u1ee9c barplot trong Seaborn\n#Hi\u1ec3n th\u1ecb \u01b0\u1edbc t\u00ednh \u0111i\u1ec3m v\u00e0 kho\u1ea3ng tin c\u1eady d\u01b0\u1edbi d\u1ea1ng thanh h\u00ecnh ch\u1eef nh\u1eadt.\nax = sns.barplot(data_count.index, data_count.values, alpha=0.8)\n\nplt.title(\"B\u1ea3ng gi\u00e1 tr\u1ecb\") # \u0111\u1eb7t t\u00ean bi\u1ec3u \u0111\u1ed3\nplt.ylabel(\"\", fontsize=12)\nplt.xlabel(\"Lo\u1ea1i\", fontsize=12) # \u0111\u1eb7t t\u00ean cho tr\u1ee5c ho\u00e0nh , set font l\u00e0 12\n\n#Th\u00eam text cho m\u1ed7i c\u1ed9t\nrects = ax.patches\nlabels = data_count.values\nfor rect, label in zip(rects, labels):\n  height = rect.get_height()\n  ax.text(rect.get_x() + rect.get_width()\/2, height + 5, label, ha='center', va='bottom')\n\nplt.show()","45a3d422":"#l\u1ea5y \u0111\u1ed9 d\u00e0i c\u1ee7a data\nnum_rows=len(train_df)\nprint(num_rows)","d09b358a":"#Create bar graph\nsum_tox = train_df['toxic'].sum() \/ num_rows * 100\nsum_sev = train_df['severe_toxic'].sum() \/ num_rows * 100\nsum_obs = train_df['obscene'].sum() \/ num_rows * 100\nsum_thr = train_df['threat'].sum() \/ num_rows * 100\nsum_ins = train_df['insult'].sum() \/ num_rows * 100\nsum_ide = train_df['identity_hate'].sum() \/ num_rows * 100\n\nind = np.arange(6)\n\nax = plt.barh(ind, [sum_tox, sum_sev, sum_obs, sum_thr, sum_ins, sum_ide])\nplt.xlabel('Percentage (%)', size=20)\nplt.xticks(np.arange(0, 30, 5), size=20)\nplt.yticks(ind, ('Toxic', 'Severe Toxic', 'Obscene', 'Threat', 'Insult', 'Identity Hate'), size=15)\n\nplt.gca().invert_yaxis()\nplt.show()","c037d7b9":"#Create bar graph\nsum_tox = train_df['toxic'].sum() \/ num_rows * 100\nsum_sev = train_df['severe_toxic'].sum() \/ num_rows * 100\nsum_obs = train_df['obscene'].sum() \/ num_rows * 100\nsum_thr = train_df['threat'].sum() \/ num_rows * 100\nsum_ins = train_df['insult'].sum() \/ num_rows * 100\nsum_ide = train_df['identity_hate'].sum() \/ num_rows * 100\n\nind = np.arange(6)\n\nax = plt.barh(ind, [sum_tox, sum_obs, sum_ins,  sum_sev, sum_ide , sum_thr])\nplt.xlabel('Percentage (%)', size=20)\nplt.xticks(np.arange(0, 30, 5), size=20)\nplt.yticks(ind, ('Toxic', 'Obscene', 'Insult', 'Severe Toxic', 'Identity Hate', 'Threat'), size=15)\n\nplt.gca().invert_yaxis()\nplt.show()","7e3bbdaa":"#Ti\u1ec1n x\u1eed l\u00fd\n# x\u00f3a t\u1ea5t c\u1ea3 c\u00e1c s\u1ed1 c\u00f3 ch\u1eef c\u00e1i g\u1eafn li\u1ec1n v\u1edbi ch\u00fang\nalphanumeric = lambda x: re.sub('\\w*\\d\\w*', ' ', x)\n\n# thay th\u1ebf d\u1ea5u c\u00e2u b\u1eb1ng kho\u1ea3ng tr\u1eafng\n#convert t\u1ea5t c\u1ea3 chu\u1ed7i th\u00e0nh ch\u1eef th\u01b0\u1eddng\npunc_lower = lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x.lower())\n\n#x\u00f3a t\u1ea5t c\u1ea3 '\\n'\nremove_n = lambda x:re.sub(\"\\n\", \" \", x)\n\n#x\u00f3a c\u00e1c k\u00fd t\u1ef1 kh\u00f4ng ph\u1ea3i ascii\nremove_non_ascii = lambda x: re.sub(r'[^\\x00-\\x7f]',r' ',x)\n\n#Apply map\ntrain_df['comment_text'] = train_df['comment_text'].map(alphanumeric).map(punc_lower).map(remove_n).map(remove_non_ascii)\n\n#Show comment_text 0\ntrain_df['comment_text'][0]","bbfe342c":"# Chia thanh 6 section\ndata_tox = train_df.loc[:,['id', 'comment_text', 'toxic']]\ndata_sev = train_df.loc[:,['id', 'comment_text', 'severe_toxic']]\ndata_obs = train_df.loc[:,['id', 'comment_text', 'obscene']]\ndata_ins = train_df.loc[:,['id', 'comment_text', 'insult']]\ndata_thr = train_df.loc[:,['id', 'comment_text', 'threat']]\ndata_ide = train_df.loc[:,['id', 'comment_text', 'identity_hate']]","b692d171":"pip install wordcloud","db69bbe2":"import wordcloud\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom nltk.corpus import stopwords","a5d6385f":"def wordcloud(df, label):\n    subset=df[df[label]==1]\n    text=subset.comment_text.values\n    wc=WordCloud(background_color='black',max_words=4000)\n    \n    wc.generate(\" \".join(text))\n    \n    plt.figure(figsize=(20,20))\n    plt.subplot(221)\n    plt.axis('off')\n    plt.title(\"Words frequented in {}\".format(label), fontsize=20)\n    plt.imshow(wc.recolor(colormap='gist_earth', random_state=244), alpha=0.98)","e0a7f3a8":"wordcloud(data_tox,'toxic')","b87e8814":"wordcloud(data_ide,'identity_hate')","e1518620":"wordcloud(data_sev,'severe_toxic')","13bcf937":"wordcloud(data_obs,'obscene')","55449288":"wordcloud(data_ins,'insult')","764ebb47":"wordcloud(data_thr,'threat')","fe56e93e":"data_tox.head()","a0d0a845":"data_tox_1 = data_tox[data_tox['toxic'] == 1].iloc[0:5000,:]\ndata_tox_1.shape","48d165ae":"data_tox_0 = data_tox[data_tox['toxic'] == 0].iloc[0:5000,:]","b21d06cd":"data_tox_done = pd.concat([data_tox_1, data_tox_0], axis = 0)\ndata_tox_done.shape","6dd1a265":"data_sev[data_sev['severe_toxic'] == 1].count()","65d5d7d6":"data_sev_1 = data_sev[data_sev['severe_toxic'] == 1].iloc[0:1595,:]\ndata_sev_0 = data_sev[data_sev['severe_toxic'] == 0].iloc[0:1595,:]\ndata_sev_done = pd.concat([data_sev_1, data_sev_0], axis = 0)\ndata_sev_done.shape","3a97b56b":"data_obs[data_obs['obscene'] == 1].count()","d21b956d":"data_obs_1 = data_obs[data_obs['obscene'] == 1].iloc[0:5000,:]\ndata_obs_0 = data_obs[data_obs['obscene'] == 0].iloc[0:5000,:]\ndata_obs_done = pd.concat([data_obs_1, data_obs_0], axis = 0)\ndata_obs_done.shape","9f9d9a5b":"data_thr[data_thr['threat'] == 1].count()","0b9250a2":"data_thr_1 = data_thr[data_thr['threat'] == 1].iloc[0:478,:] #20%\ndata_thr_0 = data_thr[data_thr['threat'] == 0].iloc[0:1912,:]#80%\ndata_thr_done = pd.concat([data_thr_1, data_thr_0], axis = 0)\ndata_thr_done.shape","6846a97f":"data_ins[data_ins['insult'] == 1].count()","e2ef562d":"data_ins_1 = data_ins[data_ins['insult'] == 1].iloc[0:5000,:]\ndata_ins_0 = data_ins[data_ins['insult'] == 0].iloc[0:5000,:]\ndata_ins_done = pd.concat([data_ins_1, data_ins_0], axis = 0)\ndata_ins_done.shape","2523a36d":"data_ide[data_ide['identity_hate'] == 1].count()","9437a016":"data_ide_1 = data_ide[data_ide['identity_hate'] == 1].iloc[0:1405,:] #20%\ndata_ide_0 = data_ide[data_ide['identity_hate'] == 0].iloc[0:5620,:] #80%\ndata_ide_done = pd.concat([data_ide_1, data_ide_0], axis = 0)\ndata_ide_done.shape","b32ffb62":"#nh\u1eadp c\u00e1c g\u00f3i \u0111\u1ec3 x\u1eed l\u00fd tr\u01b0\u1edbc\nfrom sklearn import preprocessing\nfrom sklearn.feature_selection import SelectFromModel\n\n#nh\u1eadp c\u00e1c c\u00f4ng c\u1ee5 \u0111\u1ec3 chia nh\u1ecf d\u1eef li\u1ec7u v\u00e0 \u0111\u00e1nh gi\u00e1 hi\u1ec7u su\u1ea5t m\u00f4 h\u00ecnh\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.metrics import f1_score, precision_score, recall_score, precision_recall_curve, fbeta_score, confusion_matrix\nfrom sklearn.metrics import roc_auc_score, roc_curve\n\n#import ML algos\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import MultinomialNB, BernoulliNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier","d3025933":"def cv_tf_train_test(df_done,label,vectorizer,ngram):\n    #Chia d\u1eef li\u1ec7u th\u00e0nh c\u00e1c t\u1eadp d\u1eef li\u1ec7u X v\u00e0 y\n    X = df_done.comment_text\n    y = df_done[label]\n    \n    #Chia ng\u00e0y c\u1ee7a ch\u00fang t\u00f4i th\u00e0nh d\u1eef li\u1ec7u \u0111\u00e0o t\u1ea1o v\u00e0 ki\u1ec3m tra\n    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42)\n    \n    #T\u1ea1o m\u1ed9t \u0111\u1ed1i t\u01b0\u1ee3ng Vectorizer v\u00e0 x\u00f3a c\u00e1c stopword d\u1eebng kh\u1ecfi b\u1ea3ng\n    cv1 = vectorizer(ngram_range=(ngram), stop_words='english')\n    \n    X_train_cv1 = cv1.fit_transform(X_train) # H\u1ecdc t\u1eeb \u0111i\u1ec3n t\u1eeb v\u1ef1ng v\u00e0 tr\u1ea3 v\u1ec1 ma tr\u1eadn t\u00e0i li\u1ec7u thu\u1eadt ng\u1eef\n    X_test_sv1 = cv1.transform(X_test) #H\u1ecdc t\u1eeb \u0111i\u1ec3n t\u1eeb v\u1ef1ng c\u1ee7a t\u1ea5t c\u1ea3 c\u00e1c m\u00e3 th\u00f4ng b\u00e1o trong t\u00e0i li\u1ec7u th\u00f4\n    \n    # D\u00f9ng c\u00e1c model \u0111\u1ec3 train\n    lr = LogisticRegression()\n    lr.fit(X_train_cv1, y_train)\n    \n    knn = KNeighborsClassifier(n_neighbors=5)\n    knn.fit(X_train_cv1, y_train)\n    \n    \n    bnb = BernoulliNB()\n    bnb.fit(X_train_cv1, y_train)\n    \n    mnb = MultinomialNB()\n    mnb.fit(X_train_cv1, y_train)\n    \n    svm_model = LinearSVC()\n    svm_model.fit(X_train_cv1, y_train)\n    \n    randomforest = RandomForestClassifier(n_estimators=100, random_state = 42)\n    randomforest.fit(X_train_cv1, y_train)\n    \n    f1_score_data = {'F1 Score':[f1_score(lr.predict(X_test_sv1), y_test), \n                                 f1_score(knn.predict(X_test_sv1), y_test),\n                                 f1_score(bnb.predict(X_test_sv1), y_test),\n                                 f1_score(mnb.predict(X_test_sv1), y_test), \n                                 f1_score(svm_model.predict(X_test_sv1), y_test),\n                                f1_score(randomforest.predict(X_test_sv1), y_test),]}\n    \n    \n    df_f1 = pd.DataFrame(f1_score_data, index=['Log Regression', 'KNN', 'BernoulliNB', 'MultinomialNB', 'SVM', 'Random Forest'])\n    return df_f1","bda641a6":"df_tox_cv = cv_tf_train_test(data_tox_done, 'toxic', TfidfVectorizer, (1,1))\ndf_tox_cv.rename(columns={'F1 Score': 'F1 Score(toxic)'}, inplace=True)\ndf_tox_cv","369e3c85":"df_sev_cv = cv_tf_train_test(data_sev_done, 'severe_toxic', TfidfVectorizer, (1,1))\ndf_sev_cv.rename(columns={'F1 Score': 'F1 Score(Severe toxic)'}, inplace=True)\ndf_sev_cv","d1367c85":"df_obs_cv = cv_tf_train_test(data_obs_done, 'obscene', TfidfVectorizer, (1,1))\ndf_obs_cv.rename(columns={'F1 Score': 'F1 Score(Obscene)'}, inplace=True)\ndf_obs_cv","96d90e8e":"df_ins_cv = cv_tf_train_test(data_ins_done, 'insult', TfidfVectorizer, (1,1))\ndf_ins_cv.rename(columns={'F1 Score': 'F1 Score(Insult)'}, inplace=True)\ndf_ins_cv","0bdf693e":"df_thr_cv = cv_tf_train_test(data_thr_done, 'threat', TfidfVectorizer, (1,1))\ndf_thr_cv.rename(columns={'F1 Score': 'F1 Score(Threat)'}, inplace=True)\ndf_thr_cv","59c06123":"df_ide_cv = cv_tf_train_test(data_ide_done, 'identity_hate', TfidfVectorizer, (1,1))\ndf_ide_cv.rename(columns={'F1 Score': 'F1 Score(Identity Hate)'}, inplace=True)\ndf_ide_cv","f6c73343":"\nf1_all = pd.concat([df_tox_cv, df_sev_cv, df_obs_cv, df_ins_cv, df_thr_cv, df_ide_cv], axis=1)\nf1_all","79551953":"f1_all_trp = f1_all.transpose() #Tr\u1ea3 v\u1ec1 ch\u1ebf \u0111\u1ed9 xem c\u1ee7a m\u1ea3ng c\u00f3 c\u00e1c tr\u1ee5c \u0111\u01b0\u1ee3c ho\u00e1n v\u1ecb.\nf1_all_trp","a7e6f9db":"sns.lineplot(data=f1_all_trp, markers=True)\n#sns.relplot(data=flights, x=\"year\", y=\"passengers\", hue=\"month\", kind=\"line\")\nplt.xticks(rotation='90', fontsize=14)\nplt.yticks(fontsize=14)\nplt.legend(loc='best')\nplt.title('F1 Score', fontsize=20)","a80238c2":"#X\u00e2y d\u1ef1ng l\u1ea1i 1 h\u00e0m \u0111\u1ec3 hu\u1ea5n luy\u1ec7n data\ndef train_test(df_done, label, vectorizer, ngram):\n    X = df_done.comment_text\n    y = df_done[label]\n    \n    #chia th\u00e0nh c\u00e1c ph\u1ea7n t\u1eed con \u0111\u1ec3 th\u1ef1c hi\u1ec7n hu\u1ea5n luy\u1ec7n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n    \n    #T\u1ea1o Vectorizer object v\u00e0 x\u00f3a stopwords kh\u1ecfi b\u1ea3ng\n    cv1 = vectorizer(ngram_range=(ngram), stop_words='english')\n    \n    X_train_cv1 = cv1.fit_transform(X_train) \n    print(X_train_cv1)\n    X_test_cv1 = cv1.transform(X_test)\n    \n    #Ch\u1ecdn m\u00f4 h\u00ecnh Random Forest\n    randomforest = RandomForestClassifier(n_estimators=100, random_state=42)\n    randomforest.fit(X_train_cv1, y_train)\n    return randomforest.predict(X_test_cv1)","b3aec6e6":"df_tox = train_test(data_tox_done,'toxic', TfidfVectorizer, (1,1))\ndf_tox","e004daea":"df_sev = train_test(data_sev_done,'severe_toxic', TfidfVectorizer, (1,1))\ndf_sev","c0893e44":"df_obs = train_test(data_obs_done,'obscene', TfidfVectorizer, (1,1))\ndf_obs","fa258888":"df_ins = train_test(data_ins_done,'insult', TfidfVectorizer, (1,1))\ndf_ins","37f4c482":"df_thr = train_test(data_thr_done,'threat', TfidfVectorizer, (1,1))\ndf_thr","92276635":"df_ide = train_test(data_ide_done,'identity_hate', TfidfVectorizer, (1,1))\ndf_ide","96561822":"    X = data_tox_done.comment_text\n    y = data_tox_done['toxic']\n    \n    #chia th\u00e0nh c\u00e1c ph\u1ea7n t\u1eed con \u0111\u1ec3 th\u1ef1c hi\u1ec7n hu\u1ea5n luy\u1ec7n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n    \n    #T\u1ea1o Vectorizer object v\u00e0 x\u00f3a stopwords kh\u1ecfi b\u1ea3ng\n    tfv = TfidfVectorizer(ngram_range=(1,1), stop_words='english')\n    \n    X_train_fit = tfv.fit_transform(X_train) \n    X_test_fit = tfv.transform(X_test)\n    \n    #Ch\u1ecdn m\u00f4 h\u00ecnh Random Forest\n    randomforest = RandomForestClassifier(n_estimators=100, random_state=42)\n    randomforest.fit(X_train_fit, y_train)\n    randomforest.predict(X_test_fit)","a3c8e329":"#Predict Toxic Function \ndef predictToxic(sample):\n    vect = tfv.transform(sample)\n    return randomforest.predict_proba(vect)[:,1:]","ece19f27":"print('Du doan cua Toxic: ', predictToxic(['Fuck you nigga']))","c1b6ac5d":"print('Du doan cua Toxic: ', predictToxic(['I Love You']))","b397a700":"print('Du doan cua Toxic: ', predictToxic(['How are you today']))","1451e534":"    X = data_sev_done.comment_text\n    y = data_sev_done['severe_toxic']\n    \n    #chia th\u00e0nh c\u00e1c ph\u1ea7n t\u1eed con \u0111\u1ec3 th\u1ef1c hi\u1ec7n hu\u1ea5n luy\u1ec7n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n    \n    #T\u1ea1o Vectorizer object v\u00e0 x\u00f3a stopwords kh\u1ecfi b\u1ea3ng\n    tfv = TfidfVectorizer(ngram_range=(1,1), stop_words='english')\n    \n    X_train_fit = tfv.fit_transform(X_train) \n    X_test_fit = tfv.transform(X_test)\n    \n    #Ch\u1ecdn m\u00f4 h\u00ecnh Random Forest\n    randomforest = RandomForestClassifier(n_estimators=100, random_state=42)\n    randomforest.fit(X_train_fit, y_train)\n    randomforest.predict(X_test_fit)","dfb722ce":"#Predict Severe Toxic Function \ndef predict(sample):\n    vect = tfv.transform(sample)\n    return randomforest.predict_proba(vect)[:,1:]","d58e8d4c":"print('Du doan cua Severe Toxic: ', predict(['Fuck you nigga']))","4089df14":"print('Du doan cua Severe Toxic: ', predict(['I love you']))","8ae15be5":"    X = data_obs_done.comment_text\n    y = data_obs_done['obscene']\n    \n    #chia th\u00e0nh c\u00e1c ph\u1ea7n t\u1eed con \u0111\u1ec3 th\u1ef1c hi\u1ec7n hu\u1ea5n luy\u1ec7n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n    \n    #T\u1ea1o Vectorizer object v\u00e0 x\u00f3a stopwords kh\u1ecfi b\u1ea3ng\n    tfv = TfidfVectorizer(ngram_range=(1,1), stop_words='english')\n    \n    X_train_fit = tfv.fit_transform(X_train) \n    X_test_fit = tfv.transform(X_test)\n    \n    #Ch\u1ecdn m\u00f4 h\u00ecnh Random Forest\n    randomforest = RandomForestClassifier(n_estimators=100, random_state=42)\n    randomforest.fit(X_train_fit, y_train)\n    randomforest.predict(X_test_fit)\n    \n    #Predict Obscene Function \n    def predict(sample):\n        vect = tfv.transform(sample)\n        return randomforest.predict_proba(vect)[:,1:]\n\n    print('Du doan cua Obscene: ', predict(['Fuck you nigga']))\n    print('Du doan cua Obscene: ', predict(['Ilove you']))","a3e6ded1":"    X = data_ins_done.comment_text\n    y = data_ins_done['insult']\n    \n    #chia th\u00e0nh c\u00e1c ph\u1ea7n t\u1eed con \u0111\u1ec3 th\u1ef1c hi\u1ec7n hu\u1ea5n luy\u1ec7n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n    \n    #T\u1ea1o Vectorizer object v\u00e0 x\u00f3a stopwords kh\u1ecfi b\u1ea3ng\n    tfv = TfidfVectorizer(ngram_range=(1,1), stop_words='english')\n    \n    X_train_fit = tfv.fit_transform(X_train) \n    X_test_fit = tfv.transform(X_test)\n    \n    #Ch\u1ecdn m\u00f4 h\u00ecnh Random Forest\n    randomforest = RandomForestClassifier(n_estimators=100, random_state=42)\n    randomforest.fit(X_train_fit, y_train)\n    randomforest.predict(X_test_fit)\n    \n    #Predict Obscene Function \n    def predict(sample):\n        vect = tfv.transform(sample)\n        return randomforest.predict_proba(vect)[:,1:]\n\n    print('Du doan cua Insult: ', predict(['Fuck you nigga']))\n    print('Du doan cua Insult: ', predict(['Ilove you']))","fdf8cda4":"    X = data_thr_done.comment_text\n    y = data_thr_done['threat']\n    \n    #chia th\u00e0nh c\u00e1c ph\u1ea7n t\u1eed con \u0111\u1ec3 th\u1ef1c hi\u1ec7n hu\u1ea5n luy\u1ec7n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n    \n    #T\u1ea1o Vectorizer object v\u00e0 x\u00f3a stopwords kh\u1ecfi b\u1ea3ng\n    tfv = TfidfVectorizer(ngram_range=(1,1), stop_words='english')\n    \n    X_train_fit = tfv.fit_transform(X_train) \n    X_test_fit = tfv.transform(X_test)\n    \n    #Ch\u1ecdn m\u00f4 h\u00ecnh Random Forest\n    randomforest = RandomForestClassifier(n_estimators=100, random_state=42)\n    randomforest.fit(X_train_fit, y_train)\n    randomforest.predict(X_test_fit)\n    \n    #Predict Obscene Function \n    def predict(sample):\n        vect = tfv.transform(sample)\n        return randomforest.predict_proba(vect)[:,1:]\n\n    print('Du doan cua Threat: ', predict(['Fuck you nigga']))\n    print('Du doan cua Threat: ', predict(['Ilove you']))","6783a538":"    X = data_ide_done.comment_text\n    y = data_ide_done['identity_hate']\n    \n    #chia th\u00e0nh c\u00e1c ph\u1ea7n t\u1eed con \u0111\u1ec3 th\u1ef1c hi\u1ec7n hu\u1ea5n luy\u1ec7n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n    \n    #T\u1ea1o Vectorizer object v\u00e0 x\u00f3a stopwords kh\u1ecfi b\u1ea3ng\n    tfv = TfidfVectorizer(ngram_range=(1,1), stop_words='english')\n    \n    X_train_fit = tfv.fit_transform(X_train) \n    X_test_fit = tfv.transform(X_test)\n    \n    #Ch\u1ecdn m\u00f4 h\u00ecnh Random Forest\n    randomforest = RandomForestClassifier(n_estimators=100, random_state=42)\n    randomforest.fit(X_train_fit, y_train)\n    randomforest.predict(X_test_fit)\n    \n    #Predict Obscene Function \n    def predict(sample):\n        vect = tfv.transform(sample)\n        return randomforest.predict_proba(vect)[:,1:]\n\n    print('Du doan cua Identity Hate: ', predict(['Fuck you nigga']))\n    print('Du doan cua Identity Hate: ', predict(['Ilove you']))","e7efda33":"# T\u1ea1o c\u00e1c WordCloud\n**Nh\u1eefng \u0111\u00e1m m\u00e2y ch\u1ee9a r\u1ea5t nhi\u1ec1u t\u1eeb \u1edf c\u00e1c k\u00edch th\u01b0\u1edbc kh\u00e1c nhau, ch\u00fang th\u1ec3 hi\u1ec7n t\u1ea7n su\u1ea5t ho\u1eb7c t\u1ea7m quan tr\u1ecdng c\u1ee7a m\u1ed7i t\u1eeb.**","47e9d75e":"# X\u00e9t th\u1eed ph\u1ea7n tr\u0103m c\u1ee7a c\u00e1c tr\u01b0\u1eddng so v\u1edbi t\u1ed5ng \u0111\u1ed9 d\u00e0i Data","df75ada5":"**Random Forest ho\u1ea1t \u0111\u1ed9ng c\u0169ng nh\u01b0 th\u1ebf, m\u1ed7i c\u00e2y quy\u1ebft \u0111\u1ecbnh \u0111\u01b0\u1ee3c x\u00e2y d\u1ef1ng d\u00f9ng thu\u1eadt to\u00e1n Decision Tree tr\u00ean t\u1eadp d\u1eef li\u1ec7u kh\u00e1c nhau v\u00e0 d\u00f9ng t\u1eadp thu\u1ed9c t\u00ednh kh\u00e1c nhau. Sau \u0111\u00f3 b\u1eb1ng c\u00e1ch \u0111\u00e1nh gi\u00e1 c\u00e1c c\u00e2y quy\u1ebft \u0111\u1ecbnh s\u1eed d\u1ee5ng c\u00e1ch th\u1ee9c voting \u0111\u1ec3 \u0111\u01b0a ra k\u1ebft qu\u1ea3 cu\u1ed1i c\u00f9ng cho b\u00e0i to\u00e1n**","646e0247":"# Nh\u1eadn x\u00e9t t\u1ed5ng quan v\u1ec1 d\u1eef li\u1ec7u: D\u1eef li\u1ec7u s\u1eed d\u1ee5ng \u0111\u1ec3 training ch\u01b0a \u0111\u01b0\u1ee3c t\u1ed1i \u01b0u h\u00f3a trong tr\u01b0\u1eddng comment_text, c\u1ea7n c\u00f3 b\u01b0\u1edbc l\u00e0m s\u1ea1ch d\u1eef li\u1ec7u tr\u01b0\u1edbc khi training","57ce4c30":"# D\u1ef1 \u0111o\u00e1n obscene","b33c08ed":"# D\u1ef1 \u0111o\u00e1n Threat","a82df9ca":"# - So s\u00e1nh v\u1edbi comment_text[0] tr\u01b0\u1edbc \u0111\u00f3:\n- \"Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\"\n\n# - C\u00f3 th\u1ec3 th\u1ea5y, c\u00e1c d\u1ea5u \u0111\u00e3 b\u1ecb b\u1ecf \u0111i, c\u00e1c s\u1ed1 , k\u00fd t\u1ef1 \"\/n\" \u0111\u00e3 b\u1ecb lo\u1ea1i b\u1ecf v\u00e0 chuy\u1ec3n v\u1ec1 d\u1ea1ng vi\u1ebft th\u01b0\u1eddng h\u1ebft","6de5b34c":"# M\u1ed9t function d\u1ef1 \u0111o\u00e1n kh\u1ea3 n\u0103ng toxic","604216e5":"\n# --Topic: Toxic Comment Classifier--\n\n# S\u1eed d\u1ee5ng c\u00e1c m\u00f4 h\u00ecnh h\u1ecdc m\u00e1y \u0111\u1ec3 d\u1ef1 \u0111o\u00e1n kh\u1ea3 n\u0103ng c\u1ee7a \u0111o\u1ea1n v\u0103n (comment_text) theo 6 tr\u01b0\u1eddng d\u1eef li\u1ec7u : \n* Toxic\n* Severe Toxic\n* Obscene\n* Insult \n* Threat\n* Identity Hate","46d55f83":"# B\u00e2y gi\u1edd s\u1ebd d\u1ef1 \u0111o\u00e1n th\u1eed kh\u1ea3 n\u0103ng Severe Toxic c\u1ee7a m\u1ed9t c\u00e2u","436954f5":"\/\/---------------------------------------------\/\/","6a6c392c":"**L\u1ed7i**","afb8424f":"# D\u1ef1 \u0111o\u00e1n t\u1eeb \"I Love You\"","26922dbb":"# L\u1eadp ch\u1ec9 m\u1ee5c ho\u00e0n to\u00e0n d\u1ef1a tr\u00ean v\u1ecb tr\u00ed s\u1ed1 nguy\u00ean \u0111\u1ec3 l\u1ef1a ch\u1ecdn theo v\u1ecb tr\u00ed.","effca261":"# Giai \u0111o\u1ea1n ti\u1ec1n x\u1eed l\u00fd d\u1eef li\u1ec7u","9d2af164":"**N\u1ebfu thu\u1eadt to\u00e1n Random Forest c\u00f3 6 c\u00e2y quy\u1ebft \u0111inh, 5 c\u00e2y d\u1ef1 \u0111o\u00e1n 1, 1 c\u00e2y d\u1ef1 \u0111o\u00e1n 0, do \u0111\u00f3 m\u00ecnh s\u1ebd l\u1ea5y v\u00e0 cho ra d\u1ef1 \u0111o\u00e1n cu\u1ed1i c\u00f9ng l\u00e0 1**","6303c4be":"# Nh\u01b0 v\u1eady l\u00e0 ch\u00fang ta \u0111\u00e3 test th\u1eed v\u1edbi 2 c\u00e2u\n* \"Fuck you nigga\" \n* I Love You \n# C\u00f3 th\u1ec3 th\u1ea5y, k\u1ebft qu\u1ea3 kh\u00e1 l\u00e0 cao nh\u01b0ng v\u1eabn c\u00f2n tr\u01b0\u1eddng threat l\u00e0 h\u01a1i sai 1 ch\u00fat nh\u01b0ng nh\u00ecn chung l\u00e0 m\u00f4 h\u00ecnh c\u1ee7a ch\u00fang ta kh\u00e1 ho\u00e0n ch\u1ec9nh v\u1edbi RandomForest","fcaf2907":"**Severe Toxic kh\u00e1 l\u00e0 ch\u00ednh x\u00e1c**","00b80b93":"# T\u1ea1o m\u1ed9t \u0111\u1ed3 th\u1ecb \u0111\u01b0\u1eddng \u0111\u1ec3 so s\u00e1nh c\u00e1c gi\u00e1 tr\u1ecb m\u1edbi t\u00ecm \u0111\u01b0\u1ee3c:","045d9106":"# D\u1ef1 \u0111o\u00e1n Insult ","bd6deba8":"# Ta \u0111i\u1ec1u ch\u1ec9nh l\u1ea1i 1 ch\u00fat cho d\u1ec5 nh\u00ecn nh\u1eadn h\u01a1n","7a46d043":"# B\u00e2y gi\u1edd ch\u00fang ta c\u00f9ng n\u00f3i v\u1ec1 thu\u1eadt to\u00e1n m\u00ecnh ch\u1ecdn trong b\u00e0i to\u00e1n \n# Random Forest l\u00e0 g\u00ec v\u00e0 t\u1ea1i sao n\u00f3 l\u1ea1i t\u1ed1t\n1. Random = T\u00ednh ng\u1eabu nhi\u00ean\n2. Forest = R\u1eebng = Nhi\u1ec1u c\u00e2y quy\u1ebft \u0111\u1ecbnh","b3bf4c2c":"# Hi\u1ec3n th\u1ecb th\u1eed c\u00e1c tr\u01b0\u1eddng h\u1ee3p th\u00e0nh m\u1ed9t bi\u1ec3u \u0111\u1ed3 c\u1ed9t","e509611a":"# Ch\u00fang ta s\u1eed d\u1ee5ng stopword \u0111\u1ec3 l\u00e0m g\u00ec?\n- V\u00ec trong m\u1ed9t c\u00e2u c\u00f3 nh\u1eefng t\u1eeb c\u00f3 t\u1ea7n s\u1ed1 xu\u1ea5t hi\u1ec7n nhi\u1ec1u nh\u01b0 the, to... c\u00e1c t\u1eeb n\u00e0y th\u01b0\u1eddng mang \u00edt gi\u00e1 tr\u1ecb \u00fd ngh\u0129a v\u00e0 kh\u00f4ng kh\u00e1c nhau nhi\u1ec1u trong c\u00e1c v\u0103n b\u1ea3n kh\u00e1c nhau. \n- V\u00ed d\u1ee5 t\u1eeb \"the\" hay \"to\" th\u00ec \u1edf v\u0103n b\u1ea3n n\u00e0o n\u00f3 c\u0169ng kh\u00f4ng b\u1ecb thay \u0111\u1ed5i v\u1ec1 \u00fd ngh\u0129a.\n- V\u00ec th\u1ebf ch\u00fang ta c\u00f3 th\u1ec3 x\u00f3a b\u1ecf nh\u1eefng t\u1eeb n\u00e0y","0709dcfa":"# Nh\u1eafc nh\u1edf: S\u1ed1 l\u01b0\u1ee3ng nh\u1eadn x\u00e9t thu\u1ed9c c\u00e1c danh m\u1ee5c sau:\n* Toxic ( 14000+)\n* Severe Toxic (1595)\n* Obscene (8449)\n* Threat (478)\n* Insult (7877)\n* Identity Hate (1405) \n","b99f68fa":"# Nh\u01b0 \u0111\u00e3 n\u00f3i \u1edf tr\u00ean, comment_text ch\u01b0a \u0111\u01b0\u1ee3c x\u1eed l\u00fd tr\u01b0\u1edbc khi training ","ec9820e1":"# T\u1ea1o ch\u1ee9c n\u0103ng \u0111\u01a1n gi\u1ea3n c\u00f3 trong t\u1eadp d\u1eef li\u1ec7u v\u00e0 cho ph\u00e9p ng\u01b0\u1eddi d\u00f9ng ch\u1ecdn t\u1eadp d\u1eef li\u1ec7u, nh\u00e3n \u0111\u1ed9c t\u00ednh, vect\u01a1 v\u00e0 s\u1ed1 l\u01b0\u1ee3ng ngam","e839ade2":"# Thay \u0111\u1ed5i g\u00f3c nh\u00ecn 1 ch\u00fat ta \u0111\u01b0\u1ee3c b\u1ea3ng sau:","2d8e9429":"# Part 3 : Ch\u1ea1y M\u00f4 h\u00ecnh ML tr\u00ean d\u1eef li\u1ec7u","4afd9709":"# 1. T\u1ed5ng quan","9ecc0d8d":"> Kh\u00f4ng c\u00f3 v\u1ea5n \u0111\u1ec1 g\u00ec c\u1ea3, d\u01b0\u1edbi 0.5","cf4fd14a":"# D\u1ef1 \u0111o\u00e1n identity Hate","c2eceb91":"**G\u1ea7n nh\u01b0 l\u00e0 ho\u00e0n h\u1ea3o**","d6c735ce":"# \u01afu v\u00e0 nh\u01b0\u1ee3c \u0111i\u1ec3m c\u1ee7a thu\u1eadt to\u00e1n Random Forest ","e1dd1896":"# C\u00f9ng xem x\u00e9t l\u1ea1i th\u00e0nh 1 b\u1ea3ng ho\u00e0n ch\u1ec9nh","014d803d":"- C\u00f3 t\u1eadn 3 tr\u01b0\u1eddng l\u00e0 d\u01b0\u1edbi 5%\n- 2 tr\u01b0\u1eddng Obscene v\u00e0 Insult kho\u1ea3ng 5%\n- V\u00e0 tr\u01b0\u1eddng Toxic v\u01b0\u1ee3t qu\u00e1 10%","e2efab6c":"1. \u01afu \u0111i\u1ec3m \n* Thu\u1eadt to\u00e1n d\u1ec5 s\u1eed d\u1ee5ng v\u00e0 m\u1ea1nh m\u1ebd\n* Gi\u1ea3m ph\u01b0\u01a1ng \u00e1n sai, tr\u00e1nh b\u1ecb Overfitting\n* C\u00f3 th\u1ec3 s\u1eed d\u1ee5ng cho c\u1ea3 2 lo\u1ea1i b\u00e0i to\u00e1n l\u00e0 \n* N\u00f3 c\u00f3 th\u1ec3 l\u00e0m \u0111\u01b0\u1ee3c v\u1edbi nh\u1eefng b\u00e0i to\u00e1n b\u1ecb thi\u1ebfu d\u1eef li","de431a81":"**Ch\u00ednh x\u00e1c tuy\u1ec7t \u0111\u1ed1i**","e0877ac6":"# D\u1ef1 \u0111o\u00e1n kh\u1ea3 n\u0103ng c\u1ee7 t\u1eeb \"Fuck you Nigga\"","e02c8d3c":"# -------------------------------------------","cb74bed4":"# T\u1ea1o m\u1ed9t Frame hi\u1ec7n th\u1ecb t\u1ea5t c\u1ea3 nh\u1eefng m\u00f4 h\u00ecnh m\u00e0 m\u00ecnh v\u1eeba s\u1eed d\u1ee5ng\n# (Nh\u1eadn x\u00e9t sau khi t\u1ea1o bi\u1ec3u \u0111\u1ed3 \u0111\u01b0\u1eddng)","1f8d1c7b":"# D\u1ef1 \u0111o\u00e1n t\u1eeb\"How are you today\"","f8f53b09":"# 2. Th\u1ef1c thi","8609d25f":"# T\u1eeb c\u00e1c b\u1ea3ng v\u00e0 bi\u1ec3u \u0111\u1ed3 tr\u00ean c\u00f3 th\u1ec3 th\u1ea5y \u0111\u01b0\u1ee3c\n* Random Forest v\u00e0 SVM c\u00f3 \u0111\u1ed9 \u1ed5n \u0111\u1ecbnh v\u00e0 \u0111\u00e1nh gi\u00e1 cao h\u01a1n h\u1eb3n\n* KNN v\u00e0 BernoulliNB kh\u00f4ng cao l\u1eafm\n# ==>S\u1ebd d\u00f9ng RandomForest \u0111\u1ec3 d\u1ef1 \u0111o\u00e1n","2d02518f":"2. Nh\u01b0\u1ee3c \u0111i\u1ec3m\n* M\u1ea5t kh\u1ea3 n\u0103ng di\u1ec5n gi\u1ea3i c\u1ee7a m\u00f4 h\u00ecnh\n* Bagging r\u1ea5t m\u1ea1nh, cho ch\u00fang ta \u0111\u1ed9 ch\u00ednh x\u00e1c cao h\u01a1n, nh\u01b0ng l\u1ea1i n\u1eb7ng v\u1ec1 m\u1eb7t t\u00ednh to\u00e1n v\u00e0 c\u00f3 th\u1ec3 c\u00f3 tr\u01b0\u1eddng h\u1ee3p kh\u00f4ng c\u00f3 \u0111\u01b0\u1ee3c k\u1ebft qu\u1ea3 mong mu\u1ed1n gi\u1ed1ng nh\u01b0 tr\u01b0\u1eddng threat c\u1ee7a ch\u00fang ta \u1edf tr\u00ean.","6d9f0cdd":"**- Random forest l\u00e0 m\u1ed9t trong nh\u1eefng thu\u1eadt to\u00e1n h\u1ecdc m\u00e1y m\u1ea1nh m\u1ebd v\u00e0 ph\u1ed5 bi\u1ebfn nh\u1ea5t, n\u00f3 l\u00e0 thu\u1eadt to\u00e1n supervised learning, c\u00f3 th\u1ec3 gi\u1ea3i quy\u1ebft c\u1ea3 b\u00e0i to\u00e1n regression v\u00e0 classification.**\n**Random Forest l\u00e0 s\u1ef1 c\u1ea3i ti\u1ebfn c\u1ee7a bagging. N\u00f3 s\u1eed d\u1ee5ng c\u00e1c c\u00e2y (tree) \u0111\u1ec3 l\u00e0m n\u1ec1n t\u1ea3ng, l\u00e0 m\u1ed9t t\u1eadp h\u1ee3p c\u1ee7a h\u00e0ng tr\u0103m c\u00e2y quy\u1ebft \u0111\u1ecbnh, trong \u0111\u00f3 m\u1ed7i c\u00e2y \u0111\u01b0\u1ee3c t\u1ea1o n\u00ean ng\u1eabu nhi\u00ean t\u1eeb vi\u1ec7c t\u00e1i ch\u1ecdn m\u1eabu (ch\u1ecdn random 1 ph\u1ea7n c\u1ee7a d\u1eef li\u1ec7u \u0111\u1ec3 x\u00e2y d\u1ef1ng) v\u00e0 random c\u00e1c \u0111\u1eb7c tr\u01b0ng (feature) t\u1eeb to\u00e0n b\u1ed9 d\u1eef li\u1ec7u.**\n![](https:\/\/www.freecodecamp.org\/news\/content\/images\/2020\/08\/how-random-forest-classifier-work.PNG)\n\n**- V\u00ed d\u1ee5 cho d\u1ec5 hi\u1ec3u:\nB\u1ea1n mu\u1ed1n \u0111i mua m\u1ed9t th\u1ee9 g\u00ec \u0111\u00f3 nh\u01b0ng b\u1ea1n mu\u1ed1n c\u00f3 \u0111\u01b0\u1ee3c nh\u1eefng th\u1ee9 t\u1ed1t nh\u1ea5t, b\u1ea1n ph\u1ea3i c\u00e2n nh\u1eafc \u0111\u1ecba \u0111i\u1ec3m mua h\u00e0ng cho n\u00ean, b\u1ea1n ph\u1ea3i tham kh\u1ea3o \u00fd ki\u1ebfn c\u1ee7a nhi\u1ec1u n\u01a1i kh\u00e1c nhau. M\u1ed7i m\u1ed9t \u00fd ki\u1ebfn \u1edf \u0111\u00e2y s\u1ebd \u0111\u00f3ng vai tr\u00f2 nh\u01b0 m\u1ed9t C\u00c2Y QUY\u1ebeT \u0110\u1ecaNH tr\u1ea3 l\u1eddi cho nh\u1eefng c\u00e2u h\u1ecfi c\u1ee7a b\u1ea1n. R\u1ed3i sau \u0111\u00f3 b\u1ea1n s\u1ebd c\u00f3 m\u1ed9t lo\u1ea1t c\u00e2u tr\u1ea3 l\u1eddi cho nh\u1eefng c\u00e2u h\u1ecfi c\u1ee7a b\u1ea1n, t\u1eeb \u0111\u00f3 ch\u1ecdn \u0111\u01b0\u1ee3c ph\u01b0\u01a1ng \u00e1n t\u1ed1t nh\u1ea5t.**\n","d7643239":"# M\u00f4 h\u00ecnh d\u00f9ng \u0111\u1ec3 l\u00e0m g\u00ec: \n> M\u00f4 h\u00ecnh \u0111\u01b0\u1ee3c t\u1ea1o ra v\u1edbi m\u1ee5c \u0111\u00edch x\u00e9t kh\u1ea3 n\u0103ng toxic c\u1ee7a d\u1eef li\u1ec7u text m\u00e0 m\u00ecnh mong mu\u1ed1n, to l\u1edbn h\u01a1n l\u00e0 gi\u00fap cho kh\u00f4ng gian m\u1ea1ng s\u1ea1ch s\u1ebd h\u01a1n r\u1ea5t nhi\u1ec1u","64c4f59e":"# Th\u1ee9 t\u1ef1 t\u0103ng d\u1ea7n l\u00e0:\n- threat\n- identity_hate\n- severe_toxic\n- insult\n- obscene\n- toxic\n# Nh\u00ecn v\u00e0o bi\u1ec3u \u0111\u1ed3 c\u00f3 th\u1ec3 th\u1ea5y toxic l\u00e0 tr\u01b0\u1eddng c\u00f3 nhi\u1ec1u nh\u1ea5t","dd4efd9e":"# Chuy\u1ec3n \u0111\u1ed5i m\u1ed9t b\u1ed9 s\u01b0u t\u1eadp c\u00e1c t\u00e0i li\u1ec7u th\u00f4 th\u00e0nh m\u1ed9t ma tr\u1eadn c\u00e1c t\u00ednh n\u0103ng TF-IDF.","365df8a2":"# N\u1ed1i l\u1ea1i b\u1eb1ng concat\n# Axis: Tr\u1ee5c n\u1ed1i d\u1ecdc","89af9284":"# B\u00e2y gi\u1edd s\u1ebd d\u1ef1 \u0111o\u00e1n th\u1eed kh\u1ea3 n\u0103ng Toxic c\u1ee7a m\u1ed9t c\u00e2u","97f36277":"> Kh\u00e1 l\u00e0 ch\u00ednh x\u00e1c\n","6062ccc4":"# \u0110\u1ebfm Severe Toxic ","2a764869":"# M\u1ee5c L\u1ee5c\n# 1. T\u1ed5ng quan\n# 2. Th\u1ef1c hi\u1ec7n\n# 3. \u0110\u00e1nh gi\u00e1 v\u00e0 k\u1ebft lu\u1eadn","f263c475":"\/\/-------------------------------------------------------\/\/","09cf432d":"# Severe Toxic Function","afcde5a5":"# Th\u1ee9 t\u1ef1 t\u0103ng d\u1ea7n l\u00e0:\n- threat\n- identity_hate\n- severe_toxic\n- insult\n- obscene\n- toxic"}}