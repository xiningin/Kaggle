{"cell_type":{"7539aafa":"code","f9d21aba":"code","f5ef5715":"code","e32711d5":"code","90bbe240":"code","e70857d4":"code","982a9164":"code","ad2bb0b9":"code","f7bf78b3":"code","e1194fbf":"code","04d8b7f8":"code","633ce1b9":"code","4477f616":"code","3a6f207a":"code","c727e1ba":"code","cc3d75f7":"code","fcb4a682":"code","54243473":"code","38b4ffbb":"code","498236ae":"code","2cf9c914":"code","e4e83999":"code","16c84fc2":"code","93bb2868":"code","ecd9bdd1":"code","4407d787":"code","a0956198":"code","561c71a6":"code","d081f3fe":"code","8deccbcc":"code","9196c4e3":"code","103f13b0":"code","2dddcb80":"code","bf92eec8":"code","c0f4c5ad":"code","013cc95f":"code","2ba9f7ed":"code","e8acbe3e":"code","6b88a114":"code","814125ff":"code","3e095cf3":"code","729a3cbd":"code","4de81633":"code","87c263d1":"code","7a938919":"code","85d079f4":"code","ff690e03":"code","663472cc":"code","e1a8d60a":"code","211416b4":"code","0aaec95b":"code","68c2ba36":"code","2fe76daa":"code","861d5e44":"code","7f3923c5":"code","72c0e799":"code","a5743354":"code","6b9e2dd1":"code","b9238ca8":"code","86063e82":"code","d6966ab5":"code","c2a6c389":"code","4d1c48c0":"code","6f094903":"code","5a211032":"code","bc109265":"code","21614249":"code","07783941":"code","ab926448":"code","20bb6306":"code","a5e3b6f3":"code","aca949de":"code","e6d1204a":"code","f6ef4559":"code","5ca64de8":"code","03f5dcb6":"code","efa04d3c":"code","db9a21b9":"code","1feafcb9":"code","d635f476":"code","a83526ef":"code","f8af472d":"code","8188bd8c":"code","17ac5ef0":"code","2afa2638":"code","163179ad":"code","556a40be":"code","86ea2baf":"code","af37cd8f":"code","f988c500":"code","b5de259e":"code","2b881fbf":"code","1c86c915":"code","f06affb5":"code","d7cf20f5":"code","51815bee":"code","2a4c9730":"code","cddd57c1":"code","1e8442dd":"code","ec5b4fc9":"code","2df43a19":"code","3ad54fd7":"code","766f025d":"code","d0a1a194":"code","3d1242f2":"code","34b5605c":"code","c2a1e6f9":"code","15c70040":"code","4ed35ad5":"code","768e12a9":"code","cc3b8841":"code","9c38c673":"code","09bb0ec3":"code","c96cb8eb":"code","0be35265":"code","4a4e5301":"code","205641b3":"code","fa108772":"code","73decd5b":"code","c3278e21":"code","4262110d":"code","0c5bf20c":"code","dcd6ee66":"code","e5d81bee":"markdown","ee120ef8":"markdown","f3ea227a":"markdown","39d541ac":"markdown","493573d9":"markdown","7c7fb4aa":"markdown","cd78f8fb":"markdown","39939ed1":"markdown","2c422784":"markdown","558e9d91":"markdown","be6f26fa":"markdown","d4ffeaa6":"markdown","8a0a45a2":"markdown"},"source":{"7539aafa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","f9d21aba":"import numpy as np\nimport os\nimport pandas as pd\nimport cv2\nfrom PIL import Image\nimport scipy\n\nimport tensorflow as tf\nfrom tensorflow.keras.applications import *\nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras.losses import *\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.callbacks import *\nfrom tensorflow.keras.preprocessing.image import *\nfrom tensorflow.keras.utils import *\nfrom sklearn.neural_network import MLPClassifier\n# import pydot\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import *\nfrom sklearn.model_selection import *\nimport tensorflow.keras.backend as K\n\nfrom tqdm import tqdm, tqdm_notebook\nfrom colorama import Fore\nimport json\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom glob import glob\nfrom skimage.io import *\n%config Completer.use_jedi = False\nimport time\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgb\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import AdaBoostClassifier,RandomForestClassifier\n\nfrom sklearn.metrics import confusion_matrix\n\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport os.path\nimport matplotlib.pyplot as plt\nfrom IPython.display import Image, display, Markdown\nimport matplotlib.cm as cm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport tensorflow as tf\nfrom time import perf_counter\nimport seaborn as sns\n\ndef printmd(string):\n    # Print with Markdowns    \n    display(Markdown(string))\n\nprint(\"All modules have been imported\")","f5ef5715":"image_dir = Path('..\/input\/diabetic-retinopathy-224x224-2019-data\/colored_images')\n\n# Get filepaths and labels\nfilepaths = list(image_dir.glob(r'**\/*.png'))\nlabels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], filepaths))","e32711d5":"filepaths = pd.Series(filepaths, name='Filepath').astype(str)\nlabels = pd.Series(labels, name='Label')\n\n# Concatenate filepaths and labels\nimage_df = pd.concat([filepaths, labels], axis=1)\n\n# Shuffle the DataFrame and reset index\nimage_df = image_df.sample(frac=1).reset_index(drop = True)\n\n# Show the result\nimage_df.head()","90bbe240":"level = []\nfor i in image_df['Label']:\n    if i=='No_DR':\n        level.append(0)\n    elif i=='Mild':\n        level.append(1)\n    elif i=='Moderate':\n        level.append(2)\n    elif i=='Severe':\n        level.append(3)\n    else:\n        level.append(4)","e70857d4":"image_df['Level'] = level\nimage_df.head()","982a9164":"X = []\nfor i in image_df['Filepath']:\n    image = cv2.imread(i)\n    X.append(image)\n    \nX = np.asarray(X)\ny = image_df['Level']\nY = np.asarray(y)","ad2bb0b9":"# Y=to_categorical(Y,5)\nx_train, x_test1, y_train, y_test1 = train_test_split(X, Y, test_size=0.3, random_state=42)\nx_val, x_test, y_val, y_test = train_test_split(x_test1, y_test1, test_size=0.3, random_state=42)\nprint(len(x_train),len(x_val),len(x_test))","f7bf78b3":"print(x_train.shape)\nprint(x_val.shape)\nprint(x_test.shape)","e1194fbf":"# Defining our DNN Model\ndnn_model=Sequential()\ndnn_model.add(Dense(32, input_dim=5, kernel_initializer = 'uniform', activation = 'relu'))\n# dnn_model.add(BatchNormalization())\n# dnn_model.add(Dropout(0.2))\ndnn_model.add(Dense(64, kernel_initializer = 'HeUniform', activation = 'relu' ))\n# dnn_model.add(BatchNormalization())\n# dnn_model.add(Dropout(0.2))\ndnn_model.add(Dense(128, kernel_initializer = 'uniform', activation = 'relu' ))\n# dnn_model.add(BatchNormalization())\n# dnn_model.add(Dropout(0.2))\ndnn_model.add(Dense(256, kernel_initializer = 'uniform', activation = 'relu' ))\n# dnn_model.add(BatchNormalization())\n# dnn_model.add(Dropout(0.2))\ndnn_model.add(Dense(128, kernel_initializer = 'uniform', activation = 'relu' ))\n# dnn_model.add(BatchNormalization())\n# dnn_model.add(Dropout(0.2))\n\ndnn_model.add(Dense(5,activation='softmax'))\ndnn_model.summary()","04d8b7f8":"from sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nnames = [\n        \"K Nearest Neighbour Classifier\",\n        'SVM',\n        \"Random Forest Classifier\",\n        \"AdaBoost Classifier\", \n        \"XGB Classifier\"\n         ]\nclassifiers = [\n    KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30),\n    SVC(),\n    RandomForestClassifier(max_depth=9,criterion = 'entropy'),\n    AdaBoostClassifier(),\n    XGBClassifier()\n        ]\nzipped_clf = zip(names,classifiers)","633ce1b9":"def classifier_summary(pipeline, X_train, y_train, X_val, y_val,X_test,y_test):\n    sentiment_fit = pipeline.fit(X_train, y_train)\n    \n    y_pred_train= sentiment_fit.predict(X_train)\n    y_pred_val = sentiment_fit.predict(X_val)\n    y_pred_test = sentiment_fit.predict(X_test)\n    \n    y_pred_train = [1 if x>0.5 else 0 for x in y_pred_train]\n    y_pred_val = [1 if x>0.5 else 0 for x in y_pred_val]\n    y_pred_test = [1 if x>0.5 else 0 for x in y_pred_test]\n    \n    train_accuracy = np.round(accuracy_score(y_train, y_pred_train),4)*100\n    train_precision = np.round(precision_score(y_train, y_pred_train, average='weighted'),4)\n    train_recall = np.round(recall_score(y_train, y_pred_train, average='weighted'),4)\n    train_F1 = np.round(f1_score(y_train, y_pred_train, average='weighted'),4)\n    train_kappa =  np.round(cohen_kappa_score(y_train, y_pred_train),4)\n    \n    \n    val_accuracy = np.round(accuracy_score(y_val, y_pred_val),4)*100\n    val_precision = np.round(precision_score(y_val, y_pred_val, average='weighted'),4)\n    val_recall = np.round(recall_score(y_val, y_pred_val, average='weighted'),4)\n    val_F1 = np.round(f1_score(y_val, y_pred_val, average='weighted'),4)\n    val_kappa =  np.round(cohen_kappa_score(y_val, y_pred_val),4)\n   \n    \n    test_accuracy = np.round(accuracy_score(y_test, y_pred_test),4)*100\n    test_precision = np.round(precision_score(y_test, y_pred_test, average='weighted'),2)\n    test_recall = np.round(recall_score(y_test, y_pred_test, average='weighted'),2)\n    test_F1 = np.round(f1_score(y_test, y_pred_test, average='weighted'),2)\n    test_kappa =  np.round(cohen_kappa_score(y_test, y_pred_test),2) \n  \n    \n    \n    print()\n    print('------------------------ Train Set Metrics------------------------')\n    print()\n    print(\"Accuracy core : {}%\".format(train_accuracy)) \n    \n    print('------------------------ Validation Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(val_accuracy))\n                          \n    print('------------------------ Test Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(test_accuracy))\n    print(\"F1_score : {}\".format(test_F1))\n    print(\"Kappa Score : {} \".format(test_kappa))\n    print(\"Recall score: {}\".format(test_recall))\n    print(\"Precision score : {}\".format(test_precision))\n          \n    print(\"-\"*80)\n    print()","4477f616":"def classifier_comparator(X_train,y_train,X_val,y_val,X_test,y_test,classifier=zipped_clf): \n    result = []\n    for n,c in classifier:\n        checker_pipeline = Pipeline([('Classifier', c)])\n        print(\"------------------------------Fitting {} on input_data-------------------------------- \".format(n))\n        #print(c)\n        classifier_summary(checker_pipeline,X_train, y_train, X_val, y_val,X_test,y_test)","3a6f207a":"base_model= ResNet50(input_shape=(224,224,3), weights='imagenet', include_top=False)\nx = base_model.output\n# x = Dropout(0.5)(x)\nx = Flatten()(x)\n# x = BatchNormalization()(x)\nx = Dense(64,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(128,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(256,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(128,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(64,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\npredictions = Dense(5, activation='softmax')(x)\n\nmodel_feat = Model(inputs=base_model.input,outputs=predictions)\n\ntrain_features = model_feat.predict(x_train)\nval_features=model_feat.predict(x_val)\ntest_features=model_feat.predict(x_test)","c727e1ba":"from sklearn.pipeline import make_pipeline\nfrom sklearn import pipeline\nfrom sklearn.pipeline import Pipeline\nnames = [\n        \"K Nearest Neighbour Classifier\",\n        'SVM',\n        \"Random Forest Classifier\",\n        \"AdaBoost Classifier\", \n        \"XGB Classifier\"\n         ]\nclassifiers = [\n    KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30),\n    SVC(),\n    RandomForestClassifier(max_depth=9,criterion = 'entropy'),\n    AdaBoostClassifier(),\n    XGBClassifier()\n        ]\nzipped_clf = zip(names,classifiers)\ndef classifier_summary(pipeline, X_train, y_train, X_val, y_val,X_test,y_test):\n    sentiment_fit = pipeline.fit(X_train, y_train)\n    \n    y_pred_train= sentiment_fit.predict(X_train)\n    y_pred_val = sentiment_fit.predict(X_val)\n    y_pred_test = sentiment_fit.predict(X_test)\n    \n    train_accuracy = np.round(accuracy_score(y_train, y_pred_train),4)*100\n    train_precision = np.round(precision_score(y_train, y_pred_train, average='weighted'),4)\n    train_recall = np.round(recall_score(y_train, y_pred_train, average='weighted'),4)\n    train_F1 = np.round(f1_score(y_train, y_pred_train, average='weighted'),4)\n    train_kappa =  np.round(cohen_kappa_score(y_train, y_pred_train),4)\n    \n    \n    val_accuracy = np.round(accuracy_score(y_val, y_pred_val),4)*100\n    val_precision = np.round(precision_score(y_val, y_pred_val, average='weighted'),4)\n    val_recall = np.round(recall_score(y_val, y_pred_val, average='weighted'),4)\n    val_F1 = np.round(f1_score(y_val, y_pred_val, average='weighted'),4)\n    val_kappa =  np.round(cohen_kappa_score(y_val, y_pred_val),4)\n    \n    \n    test_accuracy = np.round(accuracy_score(y_test, y_pred_test),4)*100\n    test_precision = np.round(precision_score(y_test, y_pred_test, average='weighted'),2)\n    test_recall = np.round(recall_score(y_test, y_pred_test, average='weighted'),2)\n    test_F1 = np.round(f1_score(y_test, y_pred_test, average='weighted'),2)\n    test_kappa =  np.round(cohen_kappa_score(y_test, y_pred_test),2) \n  \n    \n    \n    print()\n    print('------------------------ Train Set Metrics------------------------')\n    print()\n    print(\"Accuracy core : {}%\".format(train_accuracy))\n    \n    print('------------------------ Validation Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(val_accuracy))\n   \n    print('------------------------ Test Set Metrics------------------------')\n    print(\"Accuracy score : {}%\".format(test_accuracy))\n    \n    print()\n    print(\"Accuracy score : {}%\".format(test_accuracy))\n    print(\"F1_score : {}\".format(test_F1))\n    print(\"Kappa Score : {} \".format(test_kappa))\n    print(\"Recall score: {}\".format(test_recall))\n    print(\"Precision score : {}\".format(test_precision))\n    \n    print(\"-\"*80)\n    print()\n    \ndef classifier_comparator(X_train,y_train,X_val,y_val,X_test,y_test,classifier=zipped_clf): \n    result = []\n    for n,c in classifier:\n        checker_pipeline = Pipeline([('Classifier', c)])\n        print(\"------------------------------Fitting {} on input_data-------------------------------- \".format(n))\n        classifier_summary(checker_pipeline,X_train, y_train, X_val, y_val,X_test,y_test)","cc3d75f7":"classifier_comparator(train_features,y_train,val_features,y_val,test_features,y_test,classifier=zipped_clf)","fcb4a682":"train_y=to_categorical(y_train,5)\nval_y=to_categorical(y_val,5)\ntest_y=to_categorical(y_test,5)\ndnn_model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = dnn_model.fit(train_features, train_y,validation_data=(val_features,val_y), epochs=10)\nloss_value , accuracy = dnn_model.evaluate(train_features, train_y)\nprint('Train_accuracy is:' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(val_features, val_y)\nprint('Validation_accuracy is := ' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(test_features, test_y)\nprint('test_accuracy is : = ' + str(accuracy))","54243473":"print(\"Performance Report:\")\ny_pred10=dnn_model.predict_classes(test_features)\ny_test10=[np.argmax(x) for x in test_y]\ny_pred_prb10=dnn_model.predict_proba(test_features)\ntarget=['0','1','2','3','4']\nfrom sklearn import metrics\nprint('Accuracy score is :', np.round(metrics.accuracy_score(y_test10, y_pred10),4))\nprint('Precision score is :', np.round(metrics.precision_score(y_test10, y_pred10, average='weighted'),4))\nprint('Recall score is :', np.round(metrics.recall_score(y_test10,y_pred10, average='weighted'),4))\nprint('F1 Score is :', np.round(metrics.f1_score(y_test10, y_pred10, average='weighted'),4))\nprint('Cohen Kappa Score:', np.round(metrics.cohen_kappa_score(y_test10, y_pred10),4))\nprint('\\t\\tClassification Report:\\n', metrics.classification_report(y_test10, y_pred10,target_names=target))","38b4ffbb":"knn = KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30)\nknn.fit(train_features, y_train)\nplot_confusion_matrix(knn, test_features, y_test)","498236ae":"svc = SVC()\nsvc.fit(train_features, y_train)\nplot_confusion_matrix(svc, test_features, y_test)","2cf9c914":"rf = RandomForestClassifier()\nrf.fit(train_features, y_train)\nplot_confusion_matrix(rf, test_features, y_test)","e4e83999":"ada = AdaBoostClassifier()\nada.fit(train_features, y_train)\nplot_confusion_matrix(ada, test_features, y_test)","16c84fc2":"xgbc = XGBClassifier()\nxgbc.fit(train_features, y_train)\nplot_confusion_matrix(xgbc, test_features, y_test)","93bb2868":"base_model= VGG16(input_shape=(224,224,3), weights='imagenet', include_top=False)\nx = base_model.output\n# x = Dropout(0.5)(x)\nx = Flatten()(x)\n# x = BatchNormalization()(x)\nx = Dense(64,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(128,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(256,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(128,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(64,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\npredictions = Dense(5, activation='softmax')(x)\n\nmodel_feat = Model(inputs=base_model.input,outputs=predictions)\n\ntrain_features = model_feat.predict(x_train)\nval_features=model_feat.predict(x_val)\ntest_features=model_feat.predict(x_test)","ecd9bdd1":"from sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nnames = [\n        \"K Nearest Neighbour Classifier\",\n        'SVM',\n        \"Random Forest Classifier\",\n        \"AdaBoost Classifier\", \n        \"XGB Classifier\"\n         ]\nclassifiers = [\n    KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30),\n    SVC(),\n    RandomForestClassifier(max_depth=9,criterion = 'entropy'),\n    AdaBoostClassifier(),\n    XGBClassifier()\n        ]\nzipped_clf = zip(names,classifiers)\ndef classifier_summary(pipeline, X_train, y_train, X_val, y_val,X_test,y_test):\n    sentiment_fit = pipeline.fit(X_train, y_train)\n    \n    y_pred_train= sentiment_fit.predict(X_train)\n    y_pred_val = sentiment_fit.predict(X_val)\n    y_pred_test = sentiment_fit.predict(X_test)\n    \n    train_accuracy = np.round(accuracy_score(y_train, y_pred_train),4)*100\n    train_precision = np.round(precision_score(y_train, y_pred_train, average='weighted'),4)\n    train_recall = np.round(recall_score(y_train, y_pred_train, average='weighted'),4)\n    train_F1 = np.round(f1_score(y_train, y_pred_train, average='weighted'),4)\n    train_kappa =  np.round(cohen_kappa_score(y_train, y_pred_train),4)\n    \n    \n    val_accuracy = np.round(accuracy_score(y_val, y_pred_val),4)*100\n    val_precision = np.round(precision_score(y_val, y_pred_val, average='weighted'),4)\n    val_recall = np.round(recall_score(y_val, y_pred_val, average='weighted'),4)\n    val_F1 = np.round(f1_score(y_val, y_pred_val, average='weighted'),4)\n    val_kappa =  np.round(cohen_kappa_score(y_val, y_pred_val),4)\n   \n    \n    test_accuracy = np.round(accuracy_score(y_test, y_pred_test),4)*100\n    test_precision = np.round(precision_score(y_test, y_pred_test, average='weighted'),2)\n    test_recall = np.round(recall_score(y_test, y_pred_test, average='weighted'),2)\n    test_F1 = np.round(f1_score(y_test, y_pred_test, average='weighted'),2)\n    test_kappa =  np.round(cohen_kappa_score(y_test, y_pred_test),2) \n  \n    \n    \n    print()\n    print('------------------------ Train Set Metrics------------------------')\n    print()\n    print(\"Accuracy core : {}%\".format(train_accuracy))\n    \n    print('------------------------ Validation Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(val_accuracy))\n    print('------------------------ Test Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(test_accuracy))\n    print(\"F1_score : {}\".format(test_F1))\n    print(\"Kappa Score : {} \".format(test_kappa))\n    print(\"Recall score: {}\".format(test_recall))\n    print(\"Precision score : {}\".format(test_precision))\n    \n    print(\"-\"*80)\n    print()\n    \ndef classifier_comparator(X_train,y_train,X_val,y_val,X_test,y_test,classifier=zipped_clf): \n    result = []\n    for n,c in classifier:\n        checker_pipeline = Pipeline([('Classifier', c)])\n        print(\"------------------------------Fitting {} on input_data-------------------------------- \".format(n))\n        #print(c)\n        classifier_summary(checker_pipeline,X_train, y_train, X_val, y_val,X_test,y_test)","4407d787":"classifier_comparator(train_features,y_train,val_features,y_val,test_features,y_test,classifier=zipped_clf)","a0956198":"train_y=to_categorical(y_train,5)\nval_y=to_categorical(y_val,5)\ntest_y=to_categorical(y_test,5)\ndnn_model.compile(optimizer='sgd',loss='categorical_crossentropy', metrics=['accuracy'],)\nhistory = dnn_model.fit(train_features, train_y,validation_data=(val_features,val_y), epochs=10)\nloss_value , accuracy = dnn_model.evaluate(train_features, train_y)\nprint('Train_accuracy is:' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(val_features, val_y)\nprint('Validation_accuracy is := ' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(test_features, test_y)\nprint('test_accuracy is : = ' + str(accuracy))","561c71a6":"print(\"Performance Report:\")\ny_pred2=dnn_model.predict_classes(test_features)\ny_test2=[np.argmax(x) for x in test_y]\ny_pred_prb2=dnn_model.predict_proba(test_features)\ntarget=['0','1','2','3','4']\nfrom sklearn import metrics\nprint('Accuracy score is :', np.round(metrics.accuracy_score(y_test2, y_pred2),4))\nprint('Precision score is :', np.round(metrics.precision_score(y_test2, y_pred2, average='weighted'),4))\nprint('Recall score is :', np.round(metrics.recall_score(y_test2,y_pred2, average='weighted'),4))\nprint('F1 Score is :', np.round(metrics.f1_score(y_test2, y_pred2, average='weighted'),4))\nprint('Cohen Kappa Score:', np.round(metrics.cohen_kappa_score(y_test2, y_pred2),4))\nprint('\\t\\tClassification Report:\\n', metrics.classification_report(y_test2, y_pred2,target_names=target))","d081f3fe":"knn = KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30)\nknn.fit(train_features, y_train)\nplot_confusion_matrix(knn, test_features, y_test)","8deccbcc":"svc = SVC()\nsvc.fit(train_features, y_train)\nplot_confusion_matrix(svc, test_features, y_test)","9196c4e3":"rf = RandomForestClassifier()\nrf.fit(train_features, y_train)\nplot_confusion_matrix(rf, test_features, y_test)","103f13b0":"ada = AdaBoostClassifier()\nada.fit(train_features, y_train)\nplot_confusion_matrix(ada, test_features, y_test)","2dddcb80":"xgbc = XGBClassifier()\nxgbc.fit(train_features, y_train)\nplot_confusion_matrix(xgbc, test_features, y_test)","bf92eec8":"base_model= VGG19(input_shape=(224,224,3), weights='imagenet', include_top=False)\nx = base_model.output\n# x = Dropout(0.5)(x)\nx = Flatten()(x)\n# x = BatchNormalization()(x)\nx = Dense(64,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(128,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(256,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(128,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(64,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\npredictions = Dense(5, activation='softmax')(x)\n\nmodel_feat = Model(inputs=base_model.input,outputs=predictions)\n\ntrain_features = model_feat.predict(x_train)\nval_features=model_feat.predict(x_val)\ntest_features=model_feat.predict(x_test)","c0f4c5ad":"from sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nnames = [\n        \"K Nearest Neighbour Classifier\",\n        'SVM',\n        \"Random Forest Classifier\",\n        \"AdaBoost Classifier\", \n        \"XGB Classifier\"\n         ]\nclassifiers = [\n    KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30),\n    SVC(),\n    RandomForestClassifier(max_depth=9,criterion = 'entropy'),\n    AdaBoostClassifier(),\n    XGBClassifier()\n        ]\nzipped_clf = zip(names,classifiers)\ndef classifier_summary(pipeline, X_train, y_train, X_val, y_val,X_test,y_test):\n    sentiment_fit = pipeline.fit(X_train, y_train)\n    \n    y_pred_train= sentiment_fit.predict(X_train)\n    y_pred_val = sentiment_fit.predict(X_val)\n    y_pred_test = sentiment_fit.predict(X_test)\n    \n    train_accuracy = np.round(accuracy_score(y_train, y_pred_train),4)*100\n    train_precision = np.round(precision_score(y_train, y_pred_train, average='weighted'),4)\n    train_recall = np.round(recall_score(y_train, y_pred_train, average='weighted'),4)\n    train_F1 = np.round(f1_score(y_train, y_pred_train, average='weighted'),4)\n    train_kappa =  np.round(cohen_kappa_score(y_train, y_pred_train),4)\n    \n    \n    val_accuracy = np.round(accuracy_score(y_val, y_pred_val),4)*100\n    val_precision = np.round(precision_score(y_val, y_pred_val, average='weighted'),4)\n    val_recall = np.round(recall_score(y_val, y_pred_val, average='weighted'),4)\n    val_F1 = np.round(f1_score(y_val, y_pred_val, average='weighted'),4)\n    val_kappa =  np.round(cohen_kappa_score(y_val, y_pred_val),4)\n   \n    \n    test_accuracy = np.round(accuracy_score(y_test, y_pred_test),4)*100\n    test_precision = np.round(precision_score(y_test, y_pred_test, average='weighted'),2)\n    test_recall = np.round(recall_score(y_test, y_pred_test, average='weighted'),2)\n    test_F1 = np.round(f1_score(y_test, y_pred_test, average='weighted'),2)\n    test_kappa =  np.round(cohen_kappa_score(y_test, y_pred_test),2) \n  \n    \n    \n    print()\n    print('------------------------ Train Set Metrics------------------------')\n    print()\n    print(\"Accuracy core : {}%\".format(train_accuracy))\n    \n    print('------------------------ Validation Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(val_accuracy))\n    print('------------------------ Test Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(test_accuracy))\n    print(\"F1_score : {}\".format(test_F1))\n    print(\"Kappa Score : {} \".format(test_kappa))\n    print(\"Recall score: {}\".format(test_recall))\n    print(\"Precision score : {}\".format(test_precision))\n    \n    print(\"-\"*80)\n    print()\n    \ndef classifier_comparator(X_train,y_train,X_val,y_val,X_test,y_test,classifier=zipped_clf): \n    result = []\n    for n,c in classifier:\n        checker_pipeline = Pipeline([('Classifier', c)])\n        print(\"------------------------------Fitting {} on input_data-------------------------------- \".format(n))\n        #print(c)\n        classifier_summary(checker_pipeline,X_train, y_train, X_val, y_val,X_test,y_test)","013cc95f":"classifier_comparator(train_features,y_train,val_features,y_val,test_features,y_test,classifier=zipped_clf)","2ba9f7ed":"train_y=to_categorical(y_train,5)\nval_y=to_categorical(y_val,5)\ntest_y=to_categorical(y_test,5)\ndnn_model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = dnn_model.fit(train_features, train_y,validation_data=(val_features,val_y), epochs=10)\nloss_value , accuracy = dnn_model.evaluate(train_features, train_y)\nprint('Train_accuracy is:' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(val_features, val_y)\nprint('Validation_accuracy is := ' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(test_features, test_y)\nprint('test_accuracy is : = ' + str(accuracy))","e8acbe3e":"print(\"Performance Report:\")\ny_pred1=dnn_model.predict_classes(test_features)\ny_test1=[np.argmax(x) for x in test_y]\ny_pred_prb1=dnn_model.predict_proba(test_features)\ntarget=['0','1','2','3','4']\nfrom sklearn import metrics\nprint('Accuracy score is :', np.round(metrics.accuracy_score(y_test1, y_pred1),4))\nprint('Precision score is :', np.round(metrics.precision_score(y_test1, y_pred1, average='weighted'),4))\nprint('Recall score is :', np.round(metrics.recall_score(y_test1,y_pred1, average='weighted'),4))\nprint('F1 Score is :', np.round(metrics.f1_score(y_test1, y_pred1, average='weighted'),4))\nprint('Cohen Kappa Score:', np.round(metrics.cohen_kappa_score(y_test1, y_pred1),4))\nprint('\\t\\tClassification Report:\\n', metrics.classification_report(y_test1, y_pred1,target_names=target))","6b88a114":"knn = KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30)\nknn.fit(train_features, y_train)\nplot_confusion_matrix(knn, test_features, y_test)","814125ff":"svc = SVC()\nsvc.fit(train_features, y_train)\nplot_confusion_matrix(svc, test_features, y_test)","3e095cf3":"rf = RandomForestClassifier()\nrf.fit(train_features, y_train)\nplot_confusion_matrix(rf, test_features, y_test)","729a3cbd":"ada = AdaBoostClassifier()\nada.fit(train_features, y_train)\nplot_confusion_matrix(ada, test_features, y_test)","4de81633":"xgbc = XGBClassifier()\nxgbc.fit(train_features, y_train)\nplot_confusion_matrix(xgbc, test_features, y_test)","87c263d1":"base_model= ResNet101(input_shape=(224,224,3), weights='imagenet', include_top=False)\nx = base_model.output\n# x = Dropout(0.5)(x)\nx = Flatten()(x)\n# x = BatchNormalization()(x)\nx = Dense(64,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(128,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(256,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(128,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(64,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\npredictions = Dense(5, activation='softmax')(x)\n\nmodel_feat = Model(inputs=base_model.input,outputs=predictions)\n\ntrain_features = model_feat.predict(x_train)\nval_features=model_feat.predict(x_val)\ntest_features=model_feat.predict(x_test)","7a938919":"from sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nnames = [\n        \"K Nearest Neighbour Classifier\",\n        'SVM',\n        \"Random Forest Classifier\",\n        \"AdaBoost Classifier\", \n        \"XGB Classifier\"\n         ]\nclassifiers = [\n    KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30),\n    SVC(),\n    RandomForestClassifier(max_depth=9,criterion = 'entropy'),\n    AdaBoostClassifier(),\n    XGBClassifier()\n        ]\nzipped_clf = zip(names,classifiers)\ndef classifier_summary(pipeline, X_train, y_train, X_val, y_val,X_test,y_test):\n    sentiment_fit = pipeline.fit(X_train, y_train)\n    \n    y_pred_train= sentiment_fit.predict(X_train)\n    y_pred_val = sentiment_fit.predict(X_val)\n    y_pred_test = sentiment_fit.predict(X_test)\n    \n    train_accuracy = np.round(accuracy_score(y_train, y_pred_train),4)*100\n    train_precision = np.round(precision_score(y_train, y_pred_train, average='weighted'),4)\n    train_recall = np.round(recall_score(y_train, y_pred_train, average='weighted'),4)\n    train_F1 = np.round(f1_score(y_train, y_pred_train, average='weighted'),4)\n    train_kappa =  np.round(cohen_kappa_score(y_train, y_pred_train),4)\n    \n    \n    val_accuracy = np.round(accuracy_score(y_val, y_pred_val),4)*100\n    val_precision = np.round(precision_score(y_val, y_pred_val, average='weighted'),4)\n    val_recall = np.round(recall_score(y_val, y_pred_val, average='weighted'),4)\n    val_F1 = np.round(f1_score(y_val, y_pred_val, average='weighted'),4)\n    val_kappa =  np.round(cohen_kappa_score(y_val, y_pred_val),4)\n   \n    \n    test_accuracy = np.round(accuracy_score(y_test, y_pred_test),4)*100\n    test_precision = np.round(precision_score(y_test, y_pred_test, average='weighted'),2)\n    test_recall = np.round(recall_score(y_test, y_pred_test, average='weighted'),2)\n    test_F1 = np.round(f1_score(y_test, y_pred_test, average='weighted'),2)\n    test_kappa =  np.round(cohen_kappa_score(y_test, y_pred_test),2) \n  \n    \n    \n    print()\n    print('------------------------ Train Set Metrics------------------------')\n    print()\n    print(\"Accuracy core : {}%\".format(train_accuracy))\n    \n    print('------------------------ Validation Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(val_accuracy))\n    print('------------------------ Test Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(test_accuracy))\n    print(\"F1_score : {}\".format(test_F1))\n    print(\"Kappa Score : {} \".format(test_kappa))\n    print(\"Recall score: {}\".format(test_recall))\n    print(\"Precision score : {}\".format(test_precision))\n    \n    print(\"-\"*80)\n    print()\n    \ndef classifier_comparator(X_train,y_train,X_val,y_val,X_test,y_test,classifier=zipped_clf): \n    result = []\n    for n,c in classifier:\n        checker_pipeline = Pipeline([('Classifier', c)])\n        print(\"------------------------------Fitting {} on input_data-------------------------------- \".format(n))\n        #print(c)\n        classifier_summary(checker_pipeline,X_train, y_train, X_val, y_val,X_test,y_test)","85d079f4":"classifier_comparator(train_features,y_train,val_features,y_val,test_features,y_test,classifier=zipped_clf)","ff690e03":"train_y=to_categorical(y_train,5)\nval_y=to_categorical(y_val,5)\ntest_y=to_categorical(y_test,5)\ndnn_model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = dnn_model.fit(train_features, train_y,validation_data=(val_features,val_y), epochs=10)\nloss_value , accuracy = dnn_model.evaluate(train_features, train_y)\nprint('Train_accuracy is:' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(val_features, val_y)\nprint('Validation_accuracy is := ' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(test_features, test_y)\nprint('test_accuracy is : = ' + str(accuracy))","663472cc":"print(\"Performance Report:\")\ny_pred3=dnn_model.predict_classes(test_features)\ny_test3=[np.argmax(x) for x in test_y]\ny_pred_prb3=dnn_model.predict_proba(test_features)\ntarget=['0','1','2','3','4']\nfrom sklearn import metrics\nprint('Accuracy score is :', np.round(metrics.accuracy_score(y_test3, y_pred3),4))\nprint('Precision score is :', np.round(metrics.precision_score(y_test3, y_pred3, average='weighted'),4))\nprint('Recall score is :', np.round(metrics.recall_score(y_test3,y_pred3, average='weighted'),4))\nprint('F1 Score is :', np.round(metrics.f1_score(y_test3, y_pred3, average='weighted'),4))\nprint('Cohen Kappa Score:', np.round(metrics.cohen_kappa_score(y_test3, y_pred3),4))\nprint('\\t\\tClassification Report:\\n', metrics.classification_report(y_test3, y_pred3,target_names=target))","e1a8d60a":"knn = KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30)\nknn.fit(train_features, y_train)\nplot_confusion_matrix(knn, test_features, y_test)","211416b4":"svc = SVC()\nsvc.fit(train_features, y_train)\nplot_confusion_matrix(svc, test_features, y_test)","0aaec95b":"rf = RandomForestClassifier()\nrf.fit(train_features, y_train)\nplot_confusion_matrix(rf, test_features, y_test)","68c2ba36":"ada = AdaBoostClassifier()\nada.fit(train_features, y_train)\nplot_confusion_matrix(ada, test_features, y_test)","2fe76daa":"xgbc = XGBClassifier()\nxgbc.fit(train_features, y_train)\nplot_confusion_matrix(xgbc, test_features, y_test)","861d5e44":"base_model= MobileNetV2(input_shape=(224,224,3), weights='imagenet', include_top=False)\nx = base_model.output\n# x = Dropout(0.5)(x)\nx = Flatten()(x)\n# x = BatchNormalization()(x)\nx = Dense(64,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(128,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(256,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(128,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(64,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\npredictions = Dense(5, activation='softmax')(x)\n\nmodel_feat = Model(inputs=base_model.input,outputs=predictions)\n\ntrain_features = model_feat.predict(x_train)\nval_features=model_feat.predict(x_val)\ntest_features=model_feat.predict(x_test)","7f3923c5":"from sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nnames = [\n        \"K Nearest Neighbour Classifier\",\n        'SVM',\n        \"Random Forest Classifier\",\n        \"AdaBoost Classifier\", \n        \"XGB Classifier\"\n         ]\nclassifiers = [\n    KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30),\n    SVC(),\n    RandomForestClassifier(max_depth=9,criterion = 'entropy'),\n    AdaBoostClassifier(),\n    XGBClassifier()\n        ]\nzipped_clf = zip(names,classifiers)\ndef classifier_summary(pipeline, X_train, y_train, X_val, y_val,X_test,y_test):\n    sentiment_fit = pipeline.fit(X_train, y_train)\n    \n    y_pred_train= sentiment_fit.predict(X_train)\n    y_pred_val = sentiment_fit.predict(X_val)\n    y_pred_test = sentiment_fit.predict(X_test)\n    \n    train_accuracy = np.round(accuracy_score(y_train, y_pred_train),4)*100\n    train_precision = np.round(precision_score(y_train, y_pred_train, average='weighted'),4)\n    train_recall = np.round(recall_score(y_train, y_pred_train, average='weighted'),4)\n    train_F1 = np.round(f1_score(y_train, y_pred_train, average='weighted'),4)\n    train_kappa =  np.round(cohen_kappa_score(y_train, y_pred_train),4)\n    \n    \n    val_accuracy = np.round(accuracy_score(y_val, y_pred_val),4)*100\n    val_precision = np.round(precision_score(y_val, y_pred_val, average='weighted'),4)\n    val_recall = np.round(recall_score(y_val, y_pred_val, average='weighted'),4)\n    val_F1 = np.round(f1_score(y_val, y_pred_val, average='weighted'),4)\n    val_kappa =  np.round(cohen_kappa_score(y_val, y_pred_val),4)\n   \n    \n    test_accuracy = np.round(accuracy_score(y_test, y_pred_test),4)*100\n    test_precision = np.round(precision_score(y_test, y_pred_test, average='weighted'),2)\n    test_recall = np.round(recall_score(y_test, y_pred_test, average='weighted'),2)\n    test_F1 = np.round(f1_score(y_test, y_pred_test, average='weighted'),2)\n    test_kappa =  np.round(cohen_kappa_score(y_test, y_pred_test),2) \n  \n    \n    \n    print()\n    print('------------------------ Train Set Metrics------------------------')\n    print()\n    print(\"Accuracy core : {}%\".format(train_accuracy))\n    \n    print('------------------------ Validation Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(val_accuracy))\n    print('------------------------ Test Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(test_accuracy))\n    print(\"F1_score : {}\".format(test_F1))\n    print(\"Kappa Score : {} \".format(test_kappa))\n    print(\"Recall score: {}\".format(test_recall))\n    print(\"Precision score : {}\".format(test_precision))\n    \n    print(\"-\"*80)\n    print()\n    \ndef classifier_comparator(X_train,y_train,X_val,y_val,X_test,y_test,classifier=zipped_clf): \n    result = []\n    for n,c in classifier:\n        checker_pipeline = Pipeline([('Classifier', c)])\n        print(\"------------------------------Fitting {} on input_data-------------------------------- \".format(n))\n        #print(c)\n        classifier_summary(checker_pipeline,X_train, y_train, X_val, y_val,X_test,y_test)","72c0e799":"classifier_comparator(train_features,y_train,val_features,y_val,test_features,y_test,classifier=zipped_clf)","a5743354":"train_y=to_categorical(y_train,5)\nval_y=to_categorical(y_val,5)\ntest_y=to_categorical(y_test,5)\ndnn_model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = dnn_model.fit(train_features, train_y,validation_data=(val_features,val_y), epochs=10)\nloss_value , accuracy = dnn_model.evaluate(train_features, train_y)\nprint('Train_accuracy is:' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(val_features, val_y)\nprint('Validation_accuracy is := ' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(test_features, test_y)\nprint('test_accuracy is : = ' + str(accuracy))","6b9e2dd1":"print(\"Performance Report:\")\ny_pred4=dnn_model.predict_classes(test_features)\ny_test4=[np.argmax(x) for x in test_y]\ny_pred_prb4=dnn_model.predict_proba(test_features)\ntarget=['0','1','2','3','4']\nfrom sklearn import metrics\nprint('Accuracy score is :', np.round(metrics.accuracy_score(y_test4, y_pred4),4))\nprint('Precision score is :', np.round(metrics.precision_score(y_test4, y_pred4, average='weighted'),4))\nprint('Recall score is :', np.round(metrics.recall_score(y_test4,y_pred4, average='weighted'),4))\nprint('F1 Score is :', np.round(metrics.f1_score(y_test4, y_pred4, average='weighted'),4))\nprint('Cohen Kappa Score:', np.round(metrics.cohen_kappa_score(y_test4, y_pred4),4))\nprint('\\t\\tClassification Report:\\n', metrics.classification_report(y_test4, y_pred4,target_names=target))","b9238ca8":"knn = KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30)\nknn.fit(train_features, y_train)\nplot_confusion_matrix(knn, test_features, y_test)","86063e82":"svc = SVC()\nsvc.fit(train_features, y_train)\nplot_confusion_matrix(svc, test_features, y_test)","d6966ab5":"rf = RandomForestClassifier()\nrf.fit(train_features, y_train)\nplot_confusion_matrix(rf, test_features, y_test)","c2a6c389":"ada = AdaBoostClassifier()\nada.fit(train_features, y_train)\nplot_confusion_matrix(ada, test_features, y_test)","4d1c48c0":"xgbc = XGBClassifier()\nxgbc.fit(train_features, y_train)\nplot_confusion_matrix(xgbc, test_features, y_test)","6f094903":"base_model= MobileNet(input_shape=(224,224,3), weights='imagenet', include_top=False)\nx = base_model.output\n# x = Dropout(0.5)(x)\nx = Flatten()(x)\n# x = BatchNormalization()(x)\nx = Dense(64,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(128,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(256,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(128,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(64,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\npredictions = Dense(5, activation='softmax')(x)\n\nmodel_feat = Model(inputs=base_model.input,outputs=predictions)\n\ntrain_features = model_feat.predict(x_train)\nval_features=model_feat.predict(x_val)\ntest_features=model_feat.predict(x_test)","5a211032":"from sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nnames = [\n        \"K Nearest Neighbour Classifier\",\n        'SVM',\n        \"Random Forest Classifier\",\n        \"AdaBoost Classifier\", \n        \"XGB Classifier\"\n         ]\nclassifiers = [\n    KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30),\n    SVC(),\n    RandomForestClassifier(max_depth=9,criterion = 'entropy'),\n    AdaBoostClassifier(),\n    XGBClassifier()\n        ]\nzipped_clf = zip(names,classifiers)\ndef classifier_summary(pipeline, X_train, y_train, X_val, y_val,X_test,y_test):\n    sentiment_fit = pipeline.fit(X_train, y_train)\n    \n    y_pred_train= sentiment_fit.predict(X_train)\n    y_pred_val = sentiment_fit.predict(X_val)\n    y_pred_test = sentiment_fit.predict(X_test)\n    \n    train_accuracy = np.round(accuracy_score(y_train, y_pred_train),4)*100\n    train_precision = np.round(precision_score(y_train, y_pred_train, average='weighted'),4)\n    train_recall = np.round(recall_score(y_train, y_pred_train, average='weighted'),4)\n    train_F1 = np.round(f1_score(y_train, y_pred_train, average='weighted'),4)\n    train_kappa =  np.round(cohen_kappa_score(y_train, y_pred_train),4)\n    \n    \n    val_accuracy = np.round(accuracy_score(y_val, y_pred_val),4)*100\n    val_precision = np.round(precision_score(y_val, y_pred_val, average='weighted'),4)\n    val_recall = np.round(recall_score(y_val, y_pred_val, average='weighted'),4)\n    val_F1 = np.round(f1_score(y_val, y_pred_val, average='weighted'),4)\n    val_kappa =  np.round(cohen_kappa_score(y_val, y_pred_val),4)\n   \n    \n    test_accuracy = np.round(accuracy_score(y_test, y_pred_test),4)*100\n    test_precision = np.round(precision_score(y_test, y_pred_test, average='weighted'),2)\n    test_recall = np.round(recall_score(y_test, y_pred_test, average='weighted'),2)\n    test_F1 = np.round(f1_score(y_test, y_pred_test, average='weighted'),2)\n    test_kappa =  np.round(cohen_kappa_score(y_test, y_pred_test),2) \n  \n    \n    \n    print()\n    print('------------------------ Train Set Metrics------------------------')\n    print()\n    print(\"Accuracy core : {}%\".format(train_accuracy))\n    \n    print('------------------------ Validation Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(val_accuracy))\n    print('------------------------ Test Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(test_accuracy))\n    print(\"F1_score : {}\".format(test_F1))\n    print(\"Kappa Score : {} \".format(test_kappa))\n    print(\"Recall score: {}\".format(test_recall))\n    print(\"Precision score : {}\".format(test_precision))\n    \n    print(\"-\"*80)\n    print()\n    \ndef classifier_comparator(X_train,y_train,X_val,y_val,X_test,y_test,classifier=zipped_clf): \n    result = []\n    for n,c in classifier:\n        checker_pipeline = Pipeline([('Classifier', c)])\n        print(\"------------------------------Fitting {} on input_data-------------------------------- \".format(n))\n        #print(c)\n        classifier_summary(checker_pipeline,X_train, y_train, X_val, y_val,X_test,y_test)","bc109265":"classifier_comparator(train_features,y_train,val_features,y_val,test_features,y_test,classifier=zipped_clf)","21614249":"train_y=to_categorical(y_train,5)\nval_y=to_categorical(y_val,5)\ntest_y=to_categorical(y_test,5)\ndnn_model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = dnn_model.fit(train_features, train_y,validation_data=(val_features,val_y), epochs=10)\nloss_value , accuracy = dnn_model.evaluate(train_features, train_y)\nprint('Train_accuracy is:' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(val_features, val_y)\nprint('Validation_accuracy is := ' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(test_features, test_y)\nprint('test_accuracy is : = ' + str(accuracy))","07783941":"print(\"Performance Report:\")\ny_pred5=dnn_model.predict_classes(test_features)\ny_test5=[np.argmax(x) for x in test_y]\ny_pred_prb5=dnn_model.predict_proba(test_features)\ntarget=['0','1','2','3','4']\nfrom sklearn import metrics\nprint('Accuracy score is :', np.round(metrics.accuracy_score(y_test5, y_pred5),4))\nprint('Precision score is :', np.round(metrics.precision_score(y_test5, y_pred5, average='weighted'),4))\nprint('Recall score is :', np.round(metrics.recall_score(y_test5,y_pred5, average='weighted'),4))\nprint('F1 Score is :', np.round(metrics.f1_score(y_test5, y_pred5, average='weighted'),4))\nprint('Cohen Kappa Score:', np.round(metrics.cohen_kappa_score(y_test5, y_pred5),4))\nprint('\\t\\tClassification Report:\\n', metrics.classification_report(y_test5, y_pred5,target_names=target))","ab926448":"knn = KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30)\nknn.fit(train_features, y_train)\nplot_confusion_matrix(knn, test_features, y_test)","20bb6306":"svc = SVC()\nsvc.fit(train_features, y_train)\nplot_confusion_matrix(svc, test_features, y_test)","a5e3b6f3":"rf = RandomForestClassifier()\nrf.fit(train_features, y_train)\nplot_confusion_matrix(rf, test_features, y_test)","aca949de":"ada = AdaBoostClassifier()\nada.fit(train_features, y_train)\nplot_confusion_matrix(ada, test_features, y_test)","e6d1204a":"xgbc = XGBClassifier()\nxgbc.fit(train_features, y_train)\nplot_confusion_matrix(xgbc, test_features, y_test)","f6ef4559":"base_model= MobileNet(input_shape=(224,224,3), weights='imagenet', include_top=False)\nx = base_model.output\n# x = Dropout(0.5)(x)\nx = Flatten()(x)\n# x = BatchNormalization()(x)\nx = Dense(64,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(128,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(256,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(128,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(64,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\npredictions = Dense(5, activation='softmax')(x)\n\nmodel_feat = Model(inputs=base_model.input,outputs=predictions)\n\ntrain_features = model_feat.predict(x_train)\nval_features=model_feat.predict(x_val)\ntest_features=model_feat.predict(x_test)","5ca64de8":"from sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nnames = [\n        \"K Nearest Neighbour Classifier\",\n        'SVM',\n        \"Random Forest Classifier\",\n        \"AdaBoost Classifier\", \n        \"XGB Classifier\"\n         ]\nclassifiers = [\n    KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30),\n    SVC(),\n    RandomForestClassifier(max_depth=9,criterion = 'entropy'),\n    AdaBoostClassifier(),\n    XGBClassifier()\n        ]\nzipped_clf = zip(names,classifiers)\ndef classifier_summary(pipeline, X_train, y_train, X_val, y_val,X_test,y_test):\n    sentiment_fit = pipeline.fit(X_train, y_train)\n    \n    y_pred_train= sentiment_fit.predict(X_train)\n    y_pred_val = sentiment_fit.predict(X_val)\n    y_pred_test = sentiment_fit.predict(X_test)\n    \n    train_accuracy = np.round(accuracy_score(y_train, y_pred_train),4)*100\n    train_precision = np.round(precision_score(y_train, y_pred_train, average='weighted'),4)\n    train_recall = np.round(recall_score(y_train, y_pred_train, average='weighted'),4)\n    train_F1 = np.round(f1_score(y_train, y_pred_train, average='weighted'),4)\n    train_kappa =  np.round(cohen_kappa_score(y_train, y_pred_train),4)\n    \n    \n    val_accuracy = np.round(accuracy_score(y_val, y_pred_val),4)*100\n    val_precision = np.round(precision_score(y_val, y_pred_val, average='weighted'),4)\n    val_recall = np.round(recall_score(y_val, y_pred_val, average='weighted'),4)\n    val_F1 = np.round(f1_score(y_val, y_pred_val, average='weighted'),4)\n    val_kappa =  np.round(cohen_kappa_score(y_val, y_pred_val),4)\n   \n    \n    test_accuracy = np.round(accuracy_score(y_test, y_pred_test),4)*100\n    test_precision = np.round(precision_score(y_test, y_pred_test, average='weighted'),2)\n    test_recall = np.round(recall_score(y_test, y_pred_test, average='weighted'),2)\n    test_F1 = np.round(f1_score(y_test, y_pred_test, average='weighted'),2)\n    test_kappa =  np.round(cohen_kappa_score(y_test, y_pred_test),2) \n  \n    \n    \n    print()\n    print('------------------------ Train Set Metrics------------------------')\n    print()\n    print(\"Accuracy core : {}%\".format(train_accuracy))\n    \n    print('------------------------ Validation Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(val_accuracy))\n    print('------------------------ Test Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(test_accuracy))\n    print(\"F1_score : {}\".format(test_F1))\n    print(\"Kappa Score : {} \".format(test_kappa))\n    print(\"Recall score: {}\".format(test_recall))\n    print(\"Precision score : {}\".format(test_precision))\n    \n    print(\"-\"*80)\n    print()\n    \ndef classifier_comparator(X_train,y_train,X_val,y_val,X_test,y_test,classifier=zipped_clf): \n    result = []\n    for n,c in classifier:\n        checker_pipeline = Pipeline([('Classifier', c)])\n        print(\"------------------------------Fitting {} on input_data-------------------------------- \".format(n))\n        #print(c)\n        classifier_summary(checker_pipeline,X_train, y_train, X_val, y_val,X_test,y_test)","03f5dcb6":"classifier_comparator(train_features,y_train,val_features,y_val,test_features,y_test,classifier=zipped_clf)","efa04d3c":"train_y=to_categorical(y_train,5)\nval_y=to_categorical(y_val,5)\ntest_y=to_categorical(y_test,5)\ndnn_model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = dnn_model.fit(train_features, train_y,validation_data=(val_features,val_y), epochs=10)\nloss_value , accuracy = dnn_model.evaluate(train_features, train_y)\nprint('Train_accuracy is:' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(val_features, val_y)\nprint('Validation_accuracy is := ' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(test_features, test_y)\nprint('test_accuracy is : = ' + str(accuracy))","db9a21b9":"print(\"Performance Report:\")\ny_pred6=dnn_model.predict_classes(test_features)\ny_test6=[np.argmax(x) for x in test_y]\ny_pred_prb6=dnn_model.predict_proba(test_features)\ntarget=['0','1','2','3','4']\nfrom sklearn import metrics\nprint('Accuracy score is :', np.round(metrics.accuracy_score(y_test6, y_pred6),4))\nprint('Precision score is :', np.round(metrics.precision_score(y_test6, y_pred6, average='weighted'),4))\nprint('Recall score is :', np.round(metrics.recall_score(y_test6,y_pred6, average='weighted'),4))\nprint('F1 Score is :', np.round(metrics.f1_score(y_test6, y_pred6, average='weighted'),4))\nprint('Cohen Kappa Score:', np.round(metrics.cohen_kappa_score(y_test6, y_pred6),4))\nprint('\\t\\tClassification Report:\\n', metrics.classification_report(y_test6, y_pred6,target_names=target))","1feafcb9":"knn = KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30)\nknn.fit(train_features, y_train)\nplot_confusion_matrix(knn, test_features, y_test)","d635f476":"svc = SVC()\nsvc.fit(train_features, y_train)\nplot_confusion_matrix(svc, test_features, y_test)","a83526ef":"rf = RandomForestClassifier()\nrf.fit(train_features, y_train)\nplot_confusion_matrix(rf, test_features, y_test)","f8af472d":"ada = AdaBoostClassifier()\nada.fit(train_features, y_train)\nplot_confusion_matrix(ada, test_features, y_test)","8188bd8c":"xgbc = XGBClassifier()\nxgbc.fit(train_features, y_train)\nplot_confusion_matrix(xgbc, test_features, y_test)","17ac5ef0":"base_model= InceptionResNetV2(input_shape=(224,224,3), weights='imagenet', include_top=False)\nx = base_model.output\n# x = Dropout(0.5)(x)\nx = Flatten()(x)\n# x = BatchNormalization()(x)\nx = Dense(64,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(128,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(256,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(128,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(64,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\npredictions = Dense(5, activation='softmax')(x)\n\nmodel_feat = Model(inputs=base_model.input,outputs=predictions)\n\ntrain_features = model_feat.predict(x_train)\nval_features=model_feat.predict(x_val)\ntest_features=model_feat.predict(x_test)","2afa2638":"from sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nnames = [\n        \"K Nearest Neighbour Classifier\",\n        'SVM',\n        \"Random Forest Classifier\",\n        \"AdaBoost Classifier\", \n        \"XGB Classifier\"\n         ]\nclassifiers = [\n    KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30),\n    SVC(),\n    RandomForestClassifier(max_depth=9,criterion = 'entropy'),\n    AdaBoostClassifier(),\n    XGBClassifier()\n        ]\nzipped_clf = zip(names,classifiers)\ndef classifier_summary(pipeline, X_train, y_train, X_val, y_val,X_test,y_test):\n    sentiment_fit = pipeline.fit(X_train, y_train)\n    \n    y_pred_train= sentiment_fit.predict(X_train)\n    y_pred_val = sentiment_fit.predict(X_val)\n    y_pred_test = sentiment_fit.predict(X_test)\n    \n    train_accuracy = np.round(accuracy_score(y_train, y_pred_train),4)*100\n    train_precision = np.round(precision_score(y_train, y_pred_train, average='weighted'),4)\n    train_recall = np.round(recall_score(y_train, y_pred_train, average='weighted'),4)\n    train_F1 = np.round(f1_score(y_train, y_pred_train, average='weighted'),4)\n    train_kappa =  np.round(cohen_kappa_score(y_train, y_pred_train),4)\n    \n    \n    val_accuracy = np.round(accuracy_score(y_val, y_pred_val),4)*100\n    val_precision = np.round(precision_score(y_val, y_pred_val, average='weighted'),4)\n    val_recall = np.round(recall_score(y_val, y_pred_val, average='weighted'),4)\n    val_F1 = np.round(f1_score(y_val, y_pred_val, average='weighted'),4)\n    val_kappa =  np.round(cohen_kappa_score(y_val, y_pred_val),4)\n   \n    \n    test_accuracy = np.round(accuracy_score(y_test, y_pred_test),4)*100\n    test_precision = np.round(precision_score(y_test, y_pred_test, average='weighted'),2)\n    test_recall = np.round(recall_score(y_test, y_pred_test, average='weighted'),2)\n    test_F1 = np.round(f1_score(y_test, y_pred_test, average='weighted'),2)\n    test_kappa =  np.round(cohen_kappa_score(y_test, y_pred_test),2) \n  \n    \n    \n    print()\n    print('------------------------ Train Set Metrics------------------------')\n    print()\n    print(\"Accuracy core : {}%\".format(train_accuracy))\n    \n    print('------------------------ Validation Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(val_accuracy))\n    print('------------------------ Test Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(test_accuracy))\n    print(\"F1_score : {}\".format(test_F1))\n    print(\"Kappa Score : {} \".format(test_kappa))\n    print(\"Recall score: {}\".format(test_recall))\n    print(\"Precision score : {}\".format(test_precision))\n    \n    print(\"-\"*80)\n    print()\n    \ndef classifier_comparator(X_train,y_train,X_val,y_val,X_test,y_test,classifier=zipped_clf): \n    result = []\n    for n,c in classifier:\n        checker_pipeline = Pipeline([('Classifier', c)])\n        print(\"------------------------------Fitting {} on input_data-------------------------------- \".format(n))\n        #print(c)\n        classifier_summary(checker_pipeline,X_train, y_train, X_val, y_val,X_test,y_test)","163179ad":"classifier_comparator(train_features,y_train,val_features,y_val,test_features,y_test,classifier=zipped_clf)","556a40be":"train_y=to_categorical(y_train,5)\nval_y=to_categorical(y_val,5)\ntest_y=to_categorical(y_test,5)\ndnn_model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = dnn_model.fit(train_features, train_y,validation_data=(val_features,val_y), epochs=10)\nloss_value , accuracy = dnn_model.evaluate(train_features, train_y)\nprint('Train_accuracy is:' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(val_features, val_y)\nprint('Validation_accuracy is := ' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(test_features, test_y)\nprint('test_accuracy is : = ' + str(accuracy))","86ea2baf":"print(\"Performance Report:\")\ny_pred7=dnn_model.predict_classes(test_features)\ny_test7=[np.argmax(x) for x in test_y]\ny_pred_prb7=dnn_model.predict_proba(test_features)\ntarget=['0','1','2','3','4']\nfrom sklearn import metrics\nprint('Accuracy score is :', np.round(metrics.accuracy_score(y_test7, y_pred7),4))\nprint('Precision score is :', np.round(metrics.precision_score(y_test7, y_pred7, average='weighted'),4))\nprint('Recall score is :', np.round(metrics.recall_score(y_test7,y_pred7, average='weighted'),4))\nprint('F1 Score is :', np.round(metrics.f1_score(y_test7, y_pred7, average='weighted'),4))\nprint('Cohen Kappa Score:', np.round(metrics.cohen_kappa_score(y_test7, y_pred7),4))\nprint('\\t\\tClassification Report:\\n', metrics.classification_report(y_test7, y_pred7,target_names=target))","af37cd8f":"knn = KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30)\nknn.fit(train_features, y_train)\nplot_confusion_matrix(knn, test_features, y_test)","f988c500":"svc = SVC()\nsvc.fit(train_features, y_train)\nplot_confusion_matrix(svc, test_features, y_test)","b5de259e":"rf = RandomForestClassifier()\nrf.fit(train_features, y_train)\nplot_confusion_matrix(rf, test_features, y_test)","2b881fbf":"ada = AdaBoostClassifier()\nada.fit(train_features, y_train)\nplot_confusion_matrix(ada, test_features, y_test)","1c86c915":"xgbc = XGBClassifier()\nxgbc.fit(train_features, y_train)\nplot_confusion_matrix(xgbc, test_features, y_test)","f06affb5":"base_model= InceptionResNetV2(input_shape=(224,224,3), weights='imagenet', include_top=False)\nx = base_model.output\n# x = Dropout(0.5)(x)\nx = Flatten()(x)\n# x = BatchNormalization()(x)\nx = Dense(64,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(128,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(256,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(128,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(64,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\npredictions = Dense(5, activation='softmax')(x)\n\nmodel_feat = Model(inputs=base_model.input,outputs=predictions)\n\ntrain_features = model_feat.predict(x_train)\nval_features=model_feat.predict(x_val)\ntest_features=model_feat.predict(x_test)","d7cf20f5":"from sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nnames = [\n        \"K Nearest Neighbour Classifier\",\n        'SVM',\n        \"Random Forest Classifier\",\n        \"AdaBoost Classifier\", \n        \"XGB Classifier\"\n         ]\nclassifiers = [\n    KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30),\n    SVC(),\n    RandomForestClassifier(max_depth=9,criterion = 'entropy'),\n    AdaBoostClassifier(),\n    XGBClassifier()\n        ]\nzipped_clf = zip(names,classifiers)\ndef classifier_summary(pipeline, X_train, y_train, X_val, y_val,X_test,y_test):\n    sentiment_fit = pipeline.fit(X_train, y_train)\n    \n    y_pred_train= sentiment_fit.predict(X_train)\n    y_pred_val = sentiment_fit.predict(X_val)\n    y_pred_test = sentiment_fit.predict(X_test)\n    \n    train_accuracy = np.round(accuracy_score(y_train, y_pred_train),4)*100\n    train_precision = np.round(precision_score(y_train, y_pred_train, average='weighted'),4)\n    train_recall = np.round(recall_score(y_train, y_pred_train, average='weighted'),4)\n    train_F1 = np.round(f1_score(y_train, y_pred_train, average='weighted'),4)\n    train_kappa =  np.round(cohen_kappa_score(y_train, y_pred_train),4)\n    \n    \n    val_accuracy = np.round(accuracy_score(y_val, y_pred_val),4)*100\n    val_precision = np.round(precision_score(y_val, y_pred_val, average='weighted'),4)\n    val_recall = np.round(recall_score(y_val, y_pred_val, average='weighted'),4)\n    val_F1 = np.round(f1_score(y_val, y_pred_val, average='weighted'),4)\n    val_kappa =  np.round(cohen_kappa_score(y_val, y_pred_val),4)\n   \n    \n    test_accuracy = np.round(accuracy_score(y_test, y_pred_test),4)*100\n    test_precision = np.round(precision_score(y_test, y_pred_test, average='weighted'),2)\n    test_recall = np.round(recall_score(y_test, y_pred_test, average='weighted'),2)\n    test_F1 = np.round(f1_score(y_test, y_pred_test, average='weighted'),2)\n    test_kappa =  np.round(cohen_kappa_score(y_test, y_pred_test),2) \n  \n    \n    \n    print()\n    print('------------------------ Train Set Metrics------------------------')\n    print()\n    print(\"Accuracy core : {}%\".format(train_accuracy))\n    \n    print('------------------------ Validation Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(val_accuracy))\n    print('------------------------ Test Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(test_accuracy))\n    print(\"F1_score : {}\".format(test_F1))\n    print(\"Kappa Score : {} \".format(test_kappa))\n    print(\"Recall score: {}\".format(test_recall))\n    print(\"Precision score : {}\".format(test_precision))\n    \n    print(\"-\"*80)\n    print()\n    \ndef classifier_comparator(X_train,y_train,X_val,y_val,X_test,y_test,classifier=zipped_clf): \n    result = []\n    for n,c in classifier:\n        checker_pipeline = Pipeline([('Classifier', c)])\n        print(\"------------------------------Fitting {} on input_data-------------------------------- \".format(n))\n        #print(c)\n        classifier_summary(checker_pipeline,X_train, y_train, X_val, y_val,X_test,y_test)","51815bee":"classifier_comparator(train_features,y_train,val_features,y_val,test_features,y_test,classifier=zipped_clf)","2a4c9730":"train_y=to_categorical(y_train,5)\nval_y=to_categorical(y_val,5)\ntest_y=to_categorical(y_test,5)\ndnn_model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = dnn_model.fit(train_features, train_y,validation_data=(val_features,val_y), epochs=10)\nloss_value , accuracy = dnn_model.evaluate(train_features, train_y)\nprint('Train_accuracy is:' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(val_features, val_y)\nprint('Validation_accuracy is := ' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(test_features, test_y)\nprint('test_accuracy is : = ' + str(accuracy))","cddd57c1":"print(\"Performance Report:\")\ny_pred8=dnn_model.predict_classes(test_features)\ny_test8=[np.argmax(x) for x in test_y]\ny_pred_prb8=dnn_model.predict_proba(test_features)\ntarget=['0','1','2','3','4']\nfrom sklearn import metrics\nprint('Accuracy score is :', np.round(metrics.accuracy_score(y_test8, y_pred8),4))\nprint('Precision score is :', np.round(metrics.precision_score(y_test8, y_pred8, average='weighted'),4))\nprint('Recall score is :', np.round(metrics.recall_score(y_test8,y_pred8, average='weighted'),4))\nprint('F1 Score is :', np.round(metrics.f1_score(y_test8, y_pred8, average='weighted'),4))\nprint('Cohen Kappa Score:', np.round(metrics.cohen_kappa_score(y_test8, y_pred8),4))\nprint('\\t\\tClassification Report:\\n', metrics.classification_report(y_test8, y_pred8,target_names=target))","1e8442dd":"knn = KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30)\nknn.fit(train_features, y_train)\nplot_confusion_matrix(knn, test_features, y_test)","ec5b4fc9":"svc = SVC()\nsvc.fit(train_features, y_train)\nplot_confusion_matrix(svc, test_features, y_test)","2df43a19":"rf = RandomForestClassifier()\nrf.fit(train_features, y_train)\nplot_confusion_matrix(rf, test_features, y_test)","3ad54fd7":"ada = AdaBoostClassifier()\nada.fit(train_features, y_train)\nplot_confusion_matrix(ada, test_features, y_test)","766f025d":"xgbc = XGBClassifier()\nxgbc.fit(train_features, y_train)\nplot_confusion_matrix(xgbc, test_features, y_test)","d0a1a194":"base_model= InceptionResNetV2(input_shape=(224,224,3), weights='imagenet', include_top=False)\nx = base_model.output\n# x = Dropout(0.5)(x)\nx = Flatten()(x)\n# x = BatchNormalization()(x)\nx = Dense(64,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(128,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(256,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(128,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(64,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\npredictions = Dense(5, activation='softmax')(x)\n\nmodel_feat = Model(inputs=base_model.input,outputs=predictions)\n\ntrain_features = model_feat.predict(x_train)\nval_features=model_feat.predict(x_val)\ntest_features=model_feat.predict(x_test)","3d1242f2":"from sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nnames = [\n        \"K Nearest Neighbour Classifier\",\n        'SVM',\n        \"Random Forest Classifier\",\n        \"AdaBoost Classifier\", \n        \"XGB Classifier\"\n         ]\nclassifiers = [\n    KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30),\n    SVC(),\n    RandomForestClassifier(max_depth=9,criterion = 'entropy'),\n    AdaBoostClassifier(),\n    XGBClassifier()\n        ]\nzipped_clf = zip(names,classifiers)\ndef classifier_summary(pipeline, X_train, y_train, X_val, y_val,X_test,y_test):\n    sentiment_fit = pipeline.fit(X_train, y_train)\n    \n    y_pred_train= sentiment_fit.predict(X_train)\n    y_pred_val = sentiment_fit.predict(X_val)\n    y_pred_test = sentiment_fit.predict(X_test)\n    \n    train_accuracy = np.round(accuracy_score(y_train, y_pred_train),4)*100\n    train_precision = np.round(precision_score(y_train, y_pred_train, average='weighted'),4)\n    train_recall = np.round(recall_score(y_train, y_pred_train, average='weighted'),4)\n    train_F1 = np.round(f1_score(y_train, y_pred_train, average='weighted'),4)\n    train_kappa =  np.round(cohen_kappa_score(y_train, y_pred_train),4)\n    \n    \n    val_accuracy = np.round(accuracy_score(y_val, y_pred_val),4)*100\n    val_precision = np.round(precision_score(y_val, y_pred_val, average='weighted'),4)\n    val_recall = np.round(recall_score(y_val, y_pred_val, average='weighted'),4)\n    val_F1 = np.round(f1_score(y_val, y_pred_val, average='weighted'),4)\n    val_kappa =  np.round(cohen_kappa_score(y_val, y_pred_val),4)\n   \n    \n    test_accuracy = np.round(accuracy_score(y_test, y_pred_test),4)*100\n    test_precision = np.round(precision_score(y_test, y_pred_test, average='weighted'),2)\n    test_recall = np.round(recall_score(y_test, y_pred_test, average='weighted'),2)\n    test_F1 = np.round(f1_score(y_test, y_pred_test, average='weighted'),2)\n    test_kappa =  np.round(cohen_kappa_score(y_test, y_pred_test),2) \n  \n    \n    \n    print()\n    print('------------------------ Train Set Metrics------------------------')\n    print()\n    print(\"Accuracy core : {}%\".format(train_accuracy))\n    \n    print('------------------------ Validation Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(val_accuracy))\n    print('------------------------ Test Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(test_accuracy))\n    print(\"F1_score : {}\".format(test_F1))\n    print(\"Kappa Score : {} \".format(test_kappa))\n    print(\"Recall score: {}\".format(test_recall))\n    print(\"Precision score : {}\".format(test_precision))\n    \n    print(\"-\"*80)\n    print()\n    \ndef classifier_comparator(X_train,y_train,X_val,y_val,X_test,y_test,classifier=zipped_clf): \n    result = []\n    for n,c in classifier:\n        checker_pipeline = Pipeline([('Classifier', c)])\n        print(\"------------------------------Fitting {} on input_data-------------------------------- \".format(n))\n        #print(c)\n        classifier_summary(checker_pipeline,X_train, y_train, X_val, y_val,X_test,y_test)","34b5605c":"classifier_comparator(train_features,y_train,val_features,y_val,test_features,y_test,classifier=zipped_clf)","c2a1e6f9":"train_y=to_categorical(y_train,5)\nval_y=to_categorical(y_val,5)\ntest_y=to_categorical(y_test,5)\ndnn_model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = dnn_model.fit(train_features, train_y,validation_data=(val_features,val_y), epochs=10)\nloss_value , accuracy = dnn_model.evaluate(train_features, train_y)\nprint('Train_accuracy is:' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(val_features, val_y)\nprint('Validation_accuracy is := ' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(test_features, test_y)\nprint('test_accuracy is : = ' + str(accuracy))","15c70040":"print(\"Performance Report:\")\ny_pred9=dnn_model.predict_classes(test_features)\ny_test9=[np.argmax(x) for x in test_y]\ny_pred_prb9=dnn_model.predict_proba(test_features)\ntarget=['0','1','2','3','4']\nfrom sklearn import metrics\nprint('Accuracy score is :', np.round(metrics.accuracy_score(y_test9, y_pred9),4))\nprint('Precision score is :', np.round(metrics.precision_score(y_test9, y_pred9, average='weighted'),4))\nprint('Recall score is :', np.round(metrics.recall_score(y_test9,y_pred9, average='weighted'),4))\nprint('F1 Score is :', np.round(metrics.f1_score(y_test9, y_pred9, average='weighted'),4))\nprint('Cohen Kappa Score:', np.round(metrics.cohen_kappa_score(y_test9, y_pred9),4))\nprint('\\t\\tClassification Report:\\n', metrics.classification_report(y_test9, y_pred9,target_names=target))","4ed35ad5":"knn = KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30)\nknn.fit(train_features, y_train)\nplot_confusion_matrix(knn, test_features, y_test)","768e12a9":"svc = SVC()\nsvc.fit(train_features, y_train)\nplot_confusion_matrix(svc, test_features, y_test)","cc3b8841":"rf = RandomForestClassifier()\nrf.fit(train_features, y_train)\nplot_confusion_matrix(rf, test_features, y_test)","9c38c673":"ada = AdaBoostClassifier()\nada.fit(train_features, y_train)\nplot_confusion_matrix(ada, test_features, y_test)","09bb0ec3":"xgbc = XGBClassifier()\nxgbc.fit(train_features, y_train)\nplot_confusion_matrix(xgbc, test_features, y_test)","c96cb8eb":"base_model= InceptionResNetV2(input_shape=(224,224,3), weights='imagenet', include_top=False)\nx = base_model.output\n# x = Dropout(0.5)(x)\nx = Flatten()(x)\n# x = BatchNormalization()(x)\nx = Dense(64,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(128,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(256,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(128,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\nx = Dense(64,kernel_initializer='he_uniform')(x)\n# x = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = Dropout(0.5)(x)\npredictions = Dense(5, activation='softmax')(x)\n\nmodel_feat = Model(inputs=base_model.input,outputs=predictions)\n\ntrain_features = model_feat.predict(x_train)\nval_features=model_feat.predict(x_val)\ntest_features=model_feat.predict(x_test)","0be35265":"from sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nnames = [\n        \"K Nearest Neighbour Classifier\",\n        'SVM',\n        \"Random Forest Classifier\",\n        \"AdaBoost Classifier\", \n        \"XGB Classifier\"\n         ]\nclassifiers = [\n    KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30),\n    SVC(),\n    RandomForestClassifier(max_depth=9,criterion = 'entropy'),\n    AdaBoostClassifier(),\n    XGBClassifier()\n        ]\nzipped_clf = zip(names,classifiers)\ndef classifier_summary(pipeline, X_train, y_train, X_val, y_val,X_test,y_test):\n    sentiment_fit = pipeline.fit(X_train, y_train)\n    \n    y_pred_train= sentiment_fit.predict(X_train)\n    y_pred_val = sentiment_fit.predict(X_val)\n    y_pred_test = sentiment_fit.predict(X_test)\n    \n    train_accuracy = np.round(accuracy_score(y_train, y_pred_train),4)*100\n    train_precision = np.round(precision_score(y_train, y_pred_train, average='weighted'),4)\n    train_recall = np.round(recall_score(y_train, y_pred_train, average='weighted'),4)\n    train_F1 = np.round(f1_score(y_train, y_pred_train, average='weighted'),4)\n    train_kappa =  np.round(cohen_kappa_score(y_train, y_pred_train),4)\n    \n    \n    val_accuracy = np.round(accuracy_score(y_val, y_pred_val),4)*100\n    val_precision = np.round(precision_score(y_val, y_pred_val, average='weighted'),4)\n    val_recall = np.round(recall_score(y_val, y_pred_val, average='weighted'),4)\n    val_F1 = np.round(f1_score(y_val, y_pred_val, average='weighted'),4)\n    val_kappa =  np.round(cohen_kappa_score(y_val, y_pred_val),4)\n   \n    \n    test_accuracy = np.round(accuracy_score(y_test, y_pred_test),4)*100\n    test_precision = np.round(precision_score(y_test, y_pred_test, average='weighted'),2)\n    test_recall = np.round(recall_score(y_test, y_pred_test, average='weighted'),2)\n    test_F1 = np.round(f1_score(y_test, y_pred_test, average='weighted'),2)\n    test_kappa =  np.round(cohen_kappa_score(y_test, y_pred_test),2) \n  \n    \n    \n    print()\n    print('------------------------ Train Set Metrics------------------------')\n    print()\n    print(\"Accuracy core : {}%\".format(train_accuracy))\n    \n    print('------------------------ Validation Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(val_accuracy))\n    print('------------------------ Test Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(test_accuracy))\n    print(\"F1_score : {}\".format(test_F1))\n    print(\"Kappa Score : {} \".format(test_kappa))\n    print(\"Recall score: {}\".format(test_recall))\n    print(\"Precision score : {}\".format(test_precision))\n    \n    print(\"-\"*80)\n    print()\n    \ndef classifier_comparator(X_train,y_train,X_val,y_val,X_test,y_test,classifier=zipped_clf): \n    result = []\n    for n,c in classifier:\n        checker_pipeline = Pipeline([('Classifier', c)])\n        print(\"------------------------------Fitting {} on input_data-------------------------------- \".format(n))\n        #print(c)\n        classifier_summary(checker_pipeline,X_train, y_train, X_val, y_val,X_test,y_test)","4a4e5301":"classifier_comparator(train_features,y_train,val_features,y_val,test_features,y_test,classifier=zipped_clf)","205641b3":"train_y=to_categorical(y_train,5)\nval_y=to_categorical(y_val,5)\ntest_y=to_categorical(y_test,5)\ndnn_model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = dnn_model.fit(train_features, train_y,validation_data=(val_features,val_y), epochs=10)\nloss_value , accuracy = dnn_model.evaluate(train_features, train_y)\nprint('Train_accuracy is:' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(val_features, val_y)\nprint('Validation_accuracy is := ' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(test_features, test_y)\nprint('test_accuracy is : = ' + str(accuracy))","fa108772":"print(\"Performance Report:\")\ny_pred9=dnn_model.predict_classes(test_features)\ny_test9=[np.argmax(x) for x in test_y]\ny_pred_prb9=dnn_model.predict_proba(test_features)\ntarget=['0','1','2','3','4']\nfrom sklearn import metrics\nprint('Accuracy score is :', np.round(metrics.accuracy_score(y_test9, y_pred9),4))\nprint('Precision score is :', np.round(metrics.precision_score(y_test9, y_pred9, average='weighted'),4))\nprint('Recall score is :', np.round(metrics.recall_score(y_test9,y_pred9, average='weighted'),4))\nprint('F1 Score is :', np.round(metrics.f1_score(y_test9, y_pred9, average='weighted'),4))\nprint('Cohen Kappa Score:', np.round(metrics.cohen_kappa_score(y_test9, y_pred9),4))\nprint('\\t\\tClassification Report:\\n', metrics.classification_report(y_test9, y_pred9,target_names=target))","73decd5b":"knn = KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30)\nknn.fit(train_features, y_train)\nplot_confusion_matrix(knn, test_features, y_test)","c3278e21":"svc = SVC()\nsvc.fit(train_features, y_train)\nplot_confusion_matrix(svc, test_features, y_test)","4262110d":"rf = RandomForestClassifier()\nrf.fit(train_features, y_train)\nplot_confusion_matrix(rf, test_features, y_test)","0c5bf20c":"ada = AdaBoostClassifier()\nada.fit(train_features, y_train)\nplot_confusion_matrix(ada, test_features, y_test)","dcd6ee66":"xgbc = XGBClassifier()\nxgbc.fit(train_features, y_train)\nplot_confusion_matrix(xgbc, test_features, y_test)","e5d81bee":"###### ","ee120ef8":"# VGG-16","f3ea227a":"# VGG-19","39d541ac":"# ResNet101","493573d9":"# InceptionV3","7c7fb4aa":"# XceptionNet","cd78f8fb":"# ResNet50","39939ed1":"# DNN Model","2c422784":"# InceptionResNetV2","558e9d91":"# DenseNet121","be6f26fa":"# DenseNet169","d4ffeaa6":"# MobileNet","8a0a45a2":"# MobileNetV2"}}