{"cell_type":{"2ce228ee":"code","dace1472":"code","e88db088":"code","aa40039d":"code","6bb08078":"code","2edc3d5d":"code","6b226faa":"code","b0c5fc1c":"code","7b7acca0":"code","cf87a3f1":"code","a01436f9":"code","f4561c78":"code","5ae52241":"code","77b7da22":"code","35bce7cb":"code","e38574aa":"code","fa027c66":"code","4bfcc54a":"code","5ea747ae":"code","e34ef8b3":"code","fd197a5e":"code","520171d2":"code","3dc0aa8e":"code","c1878a8a":"code","84937958":"code","a38a3bba":"code","b653e465":"code","ea3e2e8a":"code","d046b1df":"code","992197ac":"code","32145bea":"code","cbdb69fc":"code","52dd94f2":"code","fbf85f8b":"code","b8d8d761":"code","72aa60e1":"code","fcf85e1c":"code","f4681681":"code","5179901b":"code","49596193":"code","678512b0":"code","08c266c7":"code","507e7fdb":"code","a0577ac9":"code","5ef5b8e5":"code","92435095":"code","9a46c2f3":"code","8e786c17":"code","6bf89f03":"code","06ca4ef7":"code","5fe9315f":"code","9876899e":"code","a61442a3":"code","0c748507":"code","6314ea54":"code","d2ce0a1c":"code","8b7c339f":"code","e045070f":"code","f269e2e3":"code","8ae7a161":"code","565ecaf2":"code","0da6618e":"code","dbf7ecd6":"code","0cd83e12":"code","23a290d0":"code","0b7ad244":"code","98ba879c":"code","7b7f8dce":"code","cdb03c0c":"code","91cd9953":"code","215a24f1":"code","f19ee9ca":"code","75ea522f":"code","728e688e":"code","246c8980":"code","11b23610":"code","a30c449a":"code","3ba12bcc":"code","56f3e721":"code","fbd51dc4":"code","26882fa6":"code","5bf8e699":"code","542b881d":"code","1969c872":"code","5979bff7":"code","ddbc396a":"code","25a93ff3":"code","14666886":"code","52b7dd82":"code","d2aa7597":"code","d1d65068":"code","2f0d506e":"code","5ca152f1":"code","06db3356":"code","d4d55a0a":"code","82f7b439":"code","af96eeab":"code","79d7cb35":"code","3ce332f1":"code","793152ba":"code","d7b061cf":"code","15be3f40":"code","ee52ad1b":"code","d8450b6e":"code","ef65d331":"code","598bbbf0":"code","fdffd28c":"markdown","e62d6802":"markdown","3da1504e":"markdown","b51237e1":"markdown","69446d09":"markdown","d1e4f4d3":"markdown","e1db40f6":"markdown","f7cc87b1":"markdown","1f9c13a3":"markdown","d99fdebf":"markdown","8398df8b":"markdown","28e6c738":"markdown","77ffea42":"markdown","8f84d2f8":"markdown","748ede87":"markdown","68672e9d":"markdown","72b5f140":"markdown","2da309fd":"markdown","2aa1df94":"markdown","e1302e0b":"markdown","995e1c58":"markdown","b99da456":"markdown","5a4849c7":"markdown","e313b67f":"markdown","1990e2ec":"markdown","d318c4a1":"markdown"},"source":{"2ce228ee":"!pip install scispacy","dace1472":"!pip install pmdarima","e88db088":"!pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.4\/en_core_sci_md-0.2.4.tar.gz","aa40039d":"!python -m spacy download en","6bb08078":"import glob, json, zipfile, en_core_sci_md\n\"\"\"from bokeh.models import ColumnDataSource, HoverTool, LinearColorMapper, CustomJS\nfrom bokeh.palettes import Category20\nfrom bokeh.transform import linear_cmap\nfrom bokeh.io import output_file, show\nfrom bokeh.transform import transform\nfrom bokeh.io import output_notebook\nfrom bokeh.plotting import figure\nfrom bokeh.layouts import column\nfrom bokeh.models import RadioButtonGroup\nfrom bokeh.models import TextInput\nfrom bokeh.layouts import gridplot\nfrom bokeh.models import Div\nfrom bokeh.models import Paragraph\nfrom bokeh.layouts import column, widgetbox\"\"\"\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport spacy, scispacy, operator\nimport matplotlib.pyplot as plt\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom IPython.core.display import HTML\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom statsmodels.tools.eval_measures import rmse, mse\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.graphics.tsaplots import plot_acf,plot_pacf\nfrom statsmodels.tsa.statespace.tools import diff\nfrom pmdarima import auto_arima\nfrom statsmodels.tsa.stattools import adfuller\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline","2edc3d5d":"\"\"\"from google.colab import drive\ndrive.mount(\"\/content\/gdrive\")\"\"\";","6b226faa":"\"\"\"import zipfile\npath = '\/content\/gdrive\/My Drive\/CORD-19-research-challenge.zip'\nzip_object = zipfile.ZipFile(file = path, mode = 'r')\nzip_object.extractall('.\/')\nzip_object.close()\"\"\";","b0c5fc1c":"corona_features = {'paper_id': [], 'title': [],\n                   'abstract': [], 'text': []}","7b7acca0":"corona_df = pd.DataFrame.from_dict(corona_features)","cf87a3f1":"json_filenames = glob.glob(f'{\".\/\"}\/\/**\/*.json', recursive = True)","a01436f9":"def return_corona_df(json_filenames, df):\n    \"Fun\u00e7\u00e3o para ler os arquivos json\"\n    for file_name in json_filenames:\n        row = {'paper_id': None, 'title': None,\n           'abstract': None,'text': None}\n    \n        with open(file_name) as json_data:\n            try:\n                if file_name == \".\/sample_data\/anscombe.json\":\n                    continue\n      \n                data = json.load(json_data)\n\n                row['paper_id'] = data['paper_id'].strip()\n                row['title'] = data['metadata']['title'].strip()\n\n                abstract_list = [abstract['text'] for abstract in data['abstract']]\n                abstract = '\\n '.join (abstract_list)\n                row['abstract'] = abstract.strip()\n\n                text_list = [text['text'] for text in data['body_text']]\n                text = '\\n '.join(text_list)\n                row['text'] = text.strip()\n\n                df = df.append(row, ignore_index = True)\n            except:\n                pass \n    return df\n","f4561c78":"corona_df = return_corona_df(json_filenames, corona_df)","5ae52241":"corona_df.head()","77b7da22":"corona_df = corona_df[corona_df['title']!= \"\"]\ncorona_df = corona_df[corona_df['abstract']!= \"\"]","35bce7cb":"corona_df.drop_duplicates(['abstract','text','title'], inplace=True)","e38574aa":"corona_df.shape","fa027c66":"corona_df.to_csv(\"pre_processado.csv\")","4bfcc54a":"nlp = en_core_sci_md.load(disable = ['tagger', 'parser', 'ner'])\nnlp.max_length = 2000000","5ea747ae":"new_stop_words = ['et', 'al','doi','cppyright','http', \n                  'https', 'fig','table','result','show']\nfor word in new_stop_words:\n    nlp.vocab[word].is_stop = True","e34ef8b3":"def spacy_tokenizer(sentence):\n    sentence = sentence.lower()\n    lista = []\n    lista = [word.lemma_ for word in nlp(sentence) if not (word.is_stop or\n                                                         word.like_num or\n                                                         word.is_punct or \n                                                         word.is_space or\n                                                         len(word)==1)]\n    lista = ' '.join([str(element) for element in lista])\n    return lista","fd197a5e":"corona_df['text'] = corona_df['text'].apply(spacy_tokenizer)","520171d2":"corona_df.to_csv(\"pos_processamento.csv\", encoding = \"UTF-8\")","3dc0aa8e":"corona_df_completo = pd.read_csv(\"..\/input\/coronavirus\/pos_processamento.csv\", index_col = 0)\n#corona_df_completo = pd.read_csv(\"..\/input\/CORD-19-research-challenge\/metadata.csv\", index_col=0)","c1878a8a":"corona_df_completo.head()","84937958":"print(corona_df_completo.iloc[0][\"text\"])","a38a3bba":"corona_df_completo.dropna(inplace=True)","b653e465":"corona_df_completo.shape","ea3e2e8a":"corona_df_completo.head()","d046b1df":"dataset_texts = corona_df_completo['text'].tolist()","992197ac":"len(dataset_texts)","32145bea":"tfidf = TfidfVectorizer(max_features=2**12) #utiliza\u00e7\u00e3o desse vetor para limita\u00e7\u00e3o do tamanho da matriz esparsa\nvectorized = tfidf.fit_transform(dataset_texts)\nvectorized","cbdb69fc":"\npca = PCA(n_components = 2)\nX_pca = pca.fit_transform(vectorized.toarray())","52dd94f2":"components = pca.explained_variance_ratio_\ncomponents","fbf85f8b":"wcss = []\nfor i in range(1,21):\n    kmeans = MiniBatchKMeans(n_clusters = i, random_state = 0)\n    kmeans.fit(vectorized)\n    wcss.append(kmeans.inertia_)\nplt.plot(range(1,21), wcss)\nplt.xlabel('Number of Clusters')\nplt.ylabel(\"WCSS\")","b8d8d761":"k = 5\nkmeans = MiniBatchKMeans(n_clusters=k, random_state = 16)\ny_pred = kmeans.fit_predict(vectorized)","72aa60e1":"np.unique(y_pred)","fcf85e1c":"# Baseado em: https:\/\/www.kaggle.com\/maksimeren\/covid-19-literature-clustering\n\n\noutput_notebook()\ny_labels = y_pred\n\n# data sources\nsource = ColumnDataSource(data=dict(\n    x= X_pca[:,0], \n    y= X_pca[:,1],\n    x_backup = X_pca[:,0],\n    y_backup = X_pca[:,1],\n    desc= y_labels, \n    titles= corona_df_completo['title'],\n    abstract = corona_df_completo['abstract'],\n    labels = [\"C-\" + str(x) for x in y_labels]\n    ))\n\n# hover over information\nhover = HoverTool(tooltips=[\n    (\"Title\", \"@titles{safe}\"),\n    (\"Abstract\", \"@abstract{safe}\"),\n],\n                 point_policy=\"follow_mouse\")\n\n# map colors\nmapper = linear_cmap(field_name='desc', \n                     palette=Category20[9],\n                     low=min(y_labels) ,high=max(y_labels))\n\n# prepare the figure\np = figure(plot_width=800, plot_height=800, \n           tools=[hover, 'pan', 'wheel_zoom', 'box_zoom', 'reset'], \n           title=\"Covid-19 Papers\", \n           toolbar_location=\"right\")\n\n# plot\np.scatter('x', 'y', size=5, \n          source=source,\n          fill_color=mapper,\n          line_alpha=0.3,\n          line_color=\"black\",\n          legend = 'labels')\n\n# add callback to control \ncallback = CustomJS(args=dict(p=p, source=source), code=\"\"\"\n            \n            var radio_value = cb_obj.active;\n            var data = source.data; \n            \n            x = data['x'];\n            y = data['y'];\n            \n            x_backup = data['x_backup'];\n            y_backup = data['y_backup'];\n            \n            labels = data['desc'];\n            \n            if (radio_value == '5') {\n                for (i = 0; i < x.length; i++) {\n                    x[i] = x_backup[i];\n                    y[i] = y_backup[i];\n                }\n            }\n            else {\n                for (i = 0; i < x.length; i++) {\n                    if(labels[i] == radio_value) {\n                        x[i] = x_backup[i];\n                        y[i] = y_backup[i];\n                    } else {\n                        x[i] = undefined;\n                        y[i] = undefined;\n                    }\n                }\n            }\n\n\n        source.change.emit();\n        \"\"\")\n\n\n# option\noption = RadioButtonGroup(labels=[\"C-0\", \"C-1\", \"C-2\",\n                                  \"C-3\", \"C-5\",\n                                   \"All\"], \n                          active=5, callback=callback)\n\n#header\nheader = Div(text=\"\"\"<h1>Covid-19 Papers<\/h1>\"\"\")\n\n# show\nshow(column(header, widgetbox(option),p))","f4681681":"from IPython.display import Image\nImage(filename='..\/input\/coronavirus\/imagem3.jpg', width=500, height=500)","5179901b":"plt.figure(figsize=(10,8))\nsns.scatterplot(X_pca[:,0], X_pca[:,1], hue=y_pred,palette=\"bright\")\nplt.title(\"Covid-19 Papers\")","49596193":"corona_df = corona_df_completo.copy()","678512b0":"corona_df = corona_df.sample(n = 500, random_state = 0)","08c266c7":"corona_df","507e7fdb":"!pip install https:\/\/github.com\/explosion\/spacy-models\/releases\/download\/en_core_web_sm-2.2.0\/en_core_web_sm-2.2.0.tar.gz","a0577ac9":"text = str(corona_df['text'][25534][:1000])\n","5ef5b8e5":"nlp_ent = spacy.load(\"en_core_web_sm\")\nnlp_ent.max_length = 2000000","92435095":"doc = nlp_ent(text)","9a46c2f3":"from spacy import displacy\ndisplacy.render(doc,style = 'ent', jupyter = True)","8e786c17":"gpe = []\nfor index, row in corona_df.iterrows():\n    text = row['text']\n    doc = nlp_ent(text)\n    for entity in doc.ents:\n        if entity.label_ =='GPE':\n            gpe.append(str(entity.text))","6bf89f03":"values_gpe,counts_gpe = np.unique(np.array(gpe), return_counts = True)","06ca4ef7":"gpe_df = pd.DataFrame({'value': values_gpe, 'counts': counts_gpe})","5fe9315f":"gpe_df = gpe_df.sort_values(by='counts', ascending=False).head(8)","9876899e":"def autolabel(rects):\n    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n    for rect in rects:\n        height = round(float(rect.get_height()),2)\n        ax.annotate('{}'.format(height),\n                    xy=(rect.get_x() + rect.get_width() \/ 2, height),\n                    xytext=(0, 3),  # 3 points vertical offset\n                    textcoords=\"offset points\",\n                    ha='center', va='bottom')","a61442a3":"fig, ax = plt.subplots(figsize=(10,8))\nrect1 = ax.bar(x=gpe_df[\"value\"], height=gpe_df[\"counts\"])\nplt.title(\"Cita\u00e7\u00f5es dos pa\u00edses nos 500 textos\")\nplt.ylabel('Cita\u00e7\u00f5es de cada Pa\u00eds \/ localiza\u00e7\u00e3o')\nplt.xlabel('Pa\u00eds\/Localiza\u00e7\u00e3o')\nautolabel(rect1)","0c748507":"def find_all_texts(input_str, search_str, number_of_words):\n    text_list = []\n    index = 0\n    number_of_words = number_of_words\n    while index < len(input_str):\n        i = input_str.find(search_str, index)\n        if i == -1:\n            return text_list\n    \n        if input_str[i-number_of_words:i] == '':\n            start = 0\n        else:\n            start = i - number_of_words\n    \n        text_list.append(input_str[start:i] + input_str[i:i + number_of_words])\n        index = i + i\n    return text_list","6314ea54":"nlp = en_core_sci_md.load(disable = ['tagger', 'parser', 'ner'])\nnlp.max_length = 2000000","d2ce0a1c":"def spacy_tokenizer(sentence):\n    sentence = sentence.lower()\n    lista = []\n    lista = [word.lemma_ for word in nlp(sentence) if not (word.is_stop or\n                                                         word.like_num or\n                                                         word.is_punct or \n                                                         word.is_space or\n                                                         len(word)==1)]\n    lista = ' '.join([str(element) for element in lista])\n    return lista","8b7c339f":"search_strings = [\"traveler\"]\ntokens_list = [nlp(spacy_tokenizer(item)) for item in search_strings]\ntokens_list","e045070f":"from spacy.matcher import PhraseMatcher\nmatcher = PhraseMatcher(nlp.vocab)\nmatcher.add(\"SEARCH\", None, *tokens_list)\nnumber_of_words = 50","f269e2e3":"corona_df_200 = corona_df.copy()","8ae7a161":"corona_df_200 = corona_df_200[:200]","565ecaf2":"\nfor index, row in corona_df_200.iterrows():\n    marked_text = \"\"\n    doc = nlp(row[\"text\"])\n    paper_id = row[\"paper_id\"]\n    title=row['title']\n    matches = matcher(doc)\n    if matches == []:\n        continue\n        \n    print(f\"\\n \\n \\nWords: {search_strings}\\n\")\n    print(f\"Title: {title}\\n\")\n    print(f\"Paper ID: {paper_id}\\n\")\n    print(f\"Matches: {len(matches)}\\n\")\n    \n    \n    for i in matches:\n        start=i[1] - number_of_words\n        if start<0:\n            start=0\n        for j in range(len(tokens_list)):\n            if doc[i[1]:i[2]].similarity(tokens_list[j]) ==1:\n                search_text = str(tokens_list[j])\n                market_text = str(doc[start:i[2] + number_of_words]).replace(search_text, search_text)\n                print(f\"TEXTO: {market_text}\")\n","0da6618e":"texts = corona_df['text'].tolist()","dbf7ecd6":"tfidf = TfidfVectorizer()\nvectorized = tfidf.fit_transform(texts)","0cd83e12":"search_string = 'Guidance on ways to scale up NPIs in a more coordinated way (e.g., establish funding, infrastructure and authorities to support real time, authoritative (qualified participants) collaboration with all states to gain consensus on consistent guidance and to mobilize resources to geographic areas where critical shortfalls are identified) to give us time to enhance our health care delivery system capacity to respond to an increase in cases. Rapid design and execution of experiments to examine and compare NPIs currently being implemented. DHS Centers for Excellence could potentially be leveraged to conduct these experiments'","23a290d0":"search_string = spacy_tokenizer(search_string)\nprint(search_string)","0b7ad244":"search_string_vectorized = tfidf.transform([search_string])","98ba879c":"similarity = cosine_similarity(search_string_vectorized, vectorized)\nsimilarity","7b7f8dce":"scores_dict = {}\nfor i in range(len(similarity[0])):\n    scores_dict[i] = similarity[0][i]","cdb03c0c":"sorted_scores = sorted(scores_dict.items(), key=operator.itemgetter(1), reverse = True)","91cd9953":"for i in sorted_scores[:5]:\n    df=corona_df.iloc[i[0]]\n    print(f\"Title: {df['title']}\")\n    print(f\"Paper ID: {df['paper_id']}\")\n    print(f\"Score: {i[1]}\")\n    print(f\"Abstract: {str(df['abstract'])[0:500]}\")\n    print(\"--------------------------------------------------------------------------------------------------------------------------------------\")","215a24f1":"df_brasil = pd.read_csv(\"..\/input\/coronavirus\/brazil_covid19_macro.csv\")","f19ee9ca":"brasil = df_brasil[['date', 'deaths']].groupby('date').sum().reset_index()\nbrasil = brasil[brasil['deaths'] >0]\nbrasil['date'] = pd.to_datetime(brasil['date'])\nbrasil.set_index('date', inplace=True)\nbrasil.index.freq = \"D\"","75ea522f":"brasil","728e688e":"plt.figure(figsize=(8,5))\nplt.title(\"Covid-19 no Brasil\")\nplt.plot(brasil.values,label=\"n\u00ba Mortes\")\nplt.xlabel(\"Dias\")\nplt.legend()","246c8980":"plot_acf(brasil['deaths'], lags=40);","11b23610":"plot_pacf(brasil['deaths'], lags=40);","a30c449a":"seasonal = seasonal_decompose(brasil['deaths'], model='aditive');\nseasonal.plot();","3ba12bcc":"plt.figure(figsize=(12,5))\nseasonal.seasonal.plot();\nplt.title(\"Seasonal\")\nplt.xlabel('Date')","56f3e721":"plt.figure(figsize=(12,5))\nseasonal.trend.plot()\nplt.title(\"Tend\u00eancia\")\nplt.xlabel('Date')","fbd51dc4":"plt.figure(figsize=(12,5))\nseasonal.resid.plot()\nplt.title(\"Residual\")\nplt.xlabel('Date')","26882fa6":"def adf_test(series,title=''):\n    \"\"\"\n    Pass in a time series and an optional title, returns an ADF report\n    \"\"\"\n    print(f'Augmented Dickey-Fuller Test: {title}')\n    result = adfuller(series.dropna(),autolag='AIC') # .dropna() handles differenced data\n    \n    labels = ['ADF test statistic','p-value','# lags used','# observations']\n    out = pd.Series(result[0:4],index=labels)\n\n    for key,val in result[4].items():\n        out[f'critical value ({key})']=val\n        \n    print(out.to_string())          # .to_string() removes the line \"dtype: float64\"\n    \n    if result[1] <= 0.05:\n        print(\"Strong evidence against the null hypothesis\")\n        print(\"Reject the null hypothesis\")\n        print(\"Data has no unit root and is stationary\")\n    else:\n        print(\"Weak evidence against the null hypothesis\")\n        print(\"Fail to reject the null hypothesis\")\n        print(\"Data has a unit root and is non-stationary\")","5bf8e699":"adf_test(brasil[\"deaths\"])","542b881d":"df1 = brasil.copy()\nplt.figure(figsize=(12,5))\ndf1['d2'] = diff(brasil['deaths'],k_diff=2)\ndf1['d2'][2:].plot();\nplt.title(\"Stacionary timeseries\")\nplt.ylabel(\"Date\")","1969c872":"adf_test(df1['d2'])","5979bff7":"train_set = brasil[:90]\ntest_set = brasil[90:]","ddbc396a":"sarima = auto_arima(brasil['deaths'],start_p=0, start_q=0,seasonal=True,trace=True, m=7)","25a93ff3":"sarima","14666886":"sarima.summary()","52b7dd82":"start = len(train_set)\nend = len(train_set) + len(test_set) - 1","d2aa7597":"model = SARIMAX(train_set['deaths'], order=(2, 1, 3),seasonal_order=(0, 1, 1, 7)).fit()","d1d65068":"predictions = model.predict(start,end,typ=\"levels\").rename(\"SARIMAX(2, 1, 3)x(0, 1, 1, 7)\")","2f0d506e":"predictions","5ca152f1":"test_set['deaths'].plot(label=\"test set\", legend=True)\ntrain_set['deaths'].plot(legend=True, label=\"train set\")\npredictions.plot(label=\"prediction\", legend=True)","06db3356":"test_set['deaths'].plot(label=\"test set\", legend=True)\npredictions.plot(label=\"prediction\", legend=True)","d4d55a0a":"rmse(test_set['deaths'], predictions)","82f7b439":"start = len(train_set)\nend = len(train_set) + len(test_set) + 6","af96eeab":"predictions = model.predict(start,end,typ=\"levels\").rename(\"SARIMAX(2, 1, 3)x(0, 1, 1, 7)\")","79d7cb35":"predictions","3ce332f1":"brasil_covid = pd.read_csv(\"..\/input\/coronavirus\/brasil_covid.csv\", index_col=1, parse_dates=True,dayfirst=True, sep=\";\")","793152ba":"comparacao = brasil_covid[-21:]","d7b061cf":"plt.figure(figsize=(8,5))\ntrain_set['deaths'].plot(legend=True, label=\"Dataset de Treino\")\ncomparacao[\"obitosAcumulado\"].plot(legend=True, label=\"Dataset de Test\")\npredictions.plot(legend=True, label=\"Predictions\")\nplt.title(\"Predictions\")\nplt.ylabel(\"Mortes\")\nplt.xlabel(\"Data\")","15be3f40":"plt.figure(figsize=(8,5))\ncomparacao[\"obitosAcumulado\"].plot(legend=True, label=\"Dataset de Test\")\npredictions.plot(legend=True, label=\"Predictions\")\nplt.title(\"Predictions\")\nplt.ylabel(\"Mortes\")\nplt.xlabel(\"Data\")","ee52ad1b":"dataframe = pd.DataFrame({\"predictions\": predictions.values,\n                          \"mortes\":comparacao['obitosAcumulado']})","d8450b6e":"dataframe['diferen\u00e7a'] = dataframe['predictions'] - dataframe['mortes']","ef65d331":"dataframe['diferen\u00e7a'] = dataframe['diferen\u00e7a'].astype(int)","598bbbf0":"dataframe","fdffd28c":"At\u00e9 agora foi utilizado o dataset completo, a partir desse ponto, utilizarei apenas uma amostra de 500 artigos para que n\u00e3o demore muito tempo. O prop\u00f3sito \u00e9 demonstrar as ferramentas e como podem ser utilizadas","e62d6802":"**Previs\u00f5es feitas at\u00e9 o dia 05\/07\/2020, agora vamos conferir com os dados oficiais**","3da1504e":"Abaixo podemos ver como o modelo se comporta quando separa as entidades, pessoas, datas, localiza\u00e7\u00f5es etc...","b51237e1":"Abaixo temos o gr\u00e1fico de tend\u00eancia, como ja poder\u00edamos supor, existe uma tend\u00eancia de alta nos n\u00fameros de mortes","69446d09":"Novamente os valores foram muito pr\u00f3ximos aos reais, chegando ao dia 5 com erro de 7 mortes no total acumulado","d1e4f4d3":"Como a fun\u00e7\u00e3o acima tr\u00e1s um gr\u00e1fico panoramico com algumas funcionalidades, na hora de postar no github deu m\u00e1ximo de tamanho permitido, para solucionar esse problema, tive importar o print da imagem de como o gr\u00e1fico funciona. ","e1db40f6":"A seguir ser\u00e1 feita a defini\u00e7\u00e3o dos clusters, apesar do gr\u00e1fico demonstar que seriam interessantes 15 clusters, apenas para fins de demonstra\u00e7\u00e3o, farei apenas com 5 grupos para uma melhor visualiza\u00e7\u00e3o, o interessante que conforme passamos o mouse pelos pontos, podemos ent\u00e3o ler cada um dos t\u00edtulos e o contexto do artigo","f7cc87b1":"At\u00e9 agora foi utilizado a op\u00e7\u00e3o de de busca de apenas 1 palavra, onde aparece o T\u00edtulo, ID e quantas vezes a palavra desejada apareceu no artigo, facilitando assim a busca por conte\u00fado.\n\nA seguir, ser\u00e1 feita a busca por mais de uma palavra, conforme o desejo do especialista de sa\u00fade","1f9c13a3":"A seguir ser\u00e1 realizado alguns testes para saber se \u00e9 poss\u00edvel prever com algum modelo o n\u00famero de mortes pelo covid aqui no Brasil, e sempre lembrando que esse trabalho \u00e9 apenas para fins te\u00f3ricos e de aprendizado. Explicarei tamb\u00e9m um pouco do modelo que ser\u00e1 utilizado para as previs\u00f5es. A inten\u00e7\u00e3o n\u00e3o \u00e9 contar mortes!!\nO primeiro dataset que estarei trabalhando, a data ir\u00e1 do dia 17\/03 (dia do primeiro caso de morte) at\u00e9 o dia 28\/6, e ent\u00e3o farei previs\u00f5es utilizando modelos estat\u00edsticos e de machine learning com a finalidade de tentar prever os pr\u00f3ximos dias","d99fdebf":"Notebook em complemento aos meus outros trabalhos, todos ent\u00e3o dispon\u00edveis em meu github. Esse \u00e9 um dos desafios dispon\u00edveis no Kaggle sobre o covid-19, e tentarei durante o trabalho realizar algumas das taferas.\n\nhttps:\/\/github.com\/LeopoldoZanellato\/python\n\nhttps:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge\/tasks\n\n\nO dataset dispon\u00edvel hoje, tem um total de mais de 13GB e 120 mil artigos, no que eu trabalharei possui cerca de 3GB e 20 mil artigos.\nUm dos motivos pelo qual n\u00e3o utilizarei o de 13GB \u00e9 de que utilizaria muita mem\u00f3ria e tomaria muito tempo, e esse n\u00e3o \u00e9 o prop\u00f3sito. O prop\u00f3sito \u00e9 de apresentar pequenas solu\u00e7\u00f5es e demonstrar como o uso do machine learning poder\u00e1 ajudar os profissionais de sa\u00fades no combate ao covid-19\n\nEsse trabalho ser\u00e1 divido em 2 partes:\n\nPrimeira parte a de explora\u00e7\u00e3o de dados dos artigos cient\u00edficos, onde buscarei algumas respostas pelos desafios e como de uma maneira facil poderemos responder todos eles\n\nSegunda parte \u00e9 a simula\u00e7\u00e3o de predi\u00e7\u00f5es com a base de dados dispon\u00edvel pelo minist\u00e9rio da sa\u00fade. \n\n\nCome\u00e7aremos pelo tratamento dos dados:","8398df8b":"A seguir devemos ent\u00e3o verificar se a s\u00e9rie \u00e9 estacion\u00e1ria ou n\u00e3o, para isso \u00e9 utilizaremos o teste do adfuller\n\nO seguinte teste retornar\u00e1 uma tupla de estat\u00edsticas do teste do ADF, como Estat\u00edstica do teste Valor-P; N\u00famero de defasagens usadas; N\u00famero de observa\u00e7\u00f5es usadas para a regress\u00e3o do ADF e um dicion\u00e1rio de Valores cr\u00edticos.\n\nResumindo, se o valor-p encontrado pelo teste for < 0,05, a s\u00e9rie \u00e9 estacion\u00e1ria, ja se o valor for acima de 0,05 a s\u00e9rie n\u00e3o \u00e9 estacion\u00e1ria e por esse motivo devemos ent\u00e3o normalizala.\n\nUtilizei uma fun\u00e7\u00e3o para simplificar a visualiza\u00e7\u00e3o dos valores, e o valor p encontrado foi superior a 0,05, ou seja, a s\u00e9rie n\u00e3o \u00e9 estacionaria e para realizar as previs\u00f5es devemos transforma-la em estacion\u00e1ria","28e6c738":"Redu\u00e7\u00e3o de dimensionalidade para separa\u00e7\u00e3o dos artigos em classes:","77ffea42":"Ap\u00f3s o tratamento de dados podemos ver como ficou um dos textos","8f84d2f8":"Podemos ent\u00e3o dividir o primeiro gr\u00e1fico entre a tend\u00eancia, a sazonalidade e o residuo\n\nA tend\u00eancia \u00e9 em que forma que o gr\u00e1fico est\u00e1 se aprensentando? Em alta? Em queda?\n\nSazonalidade \u00e9 os per\u00edodos que se repetem, por exemplo, no inverno as pessoas costumam viajar mais do que no ver\u00e3o, e isso se repete todo ano\n\nResiduo \u00e9 tudo aquilo que n\u00e3o pode ser explicado pela tend\u00eancia e pela sazonalidade\n\nA seguir podemos aproximar a sazonalidade e afirmar que existe uma sazonalidade semanal aos finais de semana, isso poderia ser explicado pelo atraso dos dados aos s\u00e1bados e domingos","748ede87":"Abaixo vemos o gr\u00e1fico de res\u00edduos, que \u00e9 tudo aquilo que n\u00e3o pode ser explicado nem pela tend\u00eancia e nem pela sazonalidade","68672e9d":"Agora come\u00e7arei a parte de como explorar os textos!! Faremos a busca por palavras e nos retornar\u00e1 o texto, abstract e o t\u00edtulo do texto","72b5f140":"Aqui est\u00e3o todos os artigos, cada um com o seu respectivo ID, t\u00edtulo , abstract e o texto","2da309fd":"como podemos ver, agora se tornou uma s\u00e9rie estacion\u00e1ria!!","2aa1df94":"**Nos gr\u00e1ficos abaixo podemos ver que as predi\u00e7\u00f5es foram muito pr\u00f3ximas aos valores de teste!! O que faremos depois \u00e9 prever por mais uma semana e depois conferir com os dados**","e1302e0b":"Durante o tratamento dos dados realizaremos a lemmatiza\u00e7\u00e3o, e a fun\u00e7\u00e3o abaixo servir\u00e1 para isso.\n\n\nA lematiza\u00e7\u00e3o \u00e9 uma t\u00e9cnica, geralmente utilizada por buscador de palavras em sites, para abranger a quantidade de op\u00e7\u00f5es de palavras relacionadas a palavra buscada, ignorando o tempo verbal caso seja um verbo, o g\u00eanero da palavra, o plural e etc.\n\nPor exemplo, as palavras pensamento, pensando, e derivadas delas ser\u00e3o trocadas todas por \"pensar\", facilitando assim a \"padroniza\u00e7\u00e3o\" do texto.\n\nSer\u00e3o retirados tamb\u00e9m todos os n\u00fameros (que est\u00e3o sozinhos), stop words (palavras de liga\u00e7\u00e3o que s\u00e3o irrelevantes ao conjunto dos resultados), pontua\u00e7\u00f5es e espa\u00e7os indevidos ","995e1c58":"MODELO SARIMA\n\nDe uma forma resumida, o ARIMA model \u00e9 composto por:\n\nAR(p) = Autoregression model, prevemos utilizando a combina\u00e7\u00e3o de valores passados da pr\u00f3prima vari\u00e1vel. Gera modelos lineares.\n\nRepresentado pela letra P MA(d) = \u00e9 o modelo de m\u00e9dia m\u00f3vel. ARMA(p,q) = A jun\u00e7\u00e3o dos dois acima - representado pela letra Q \n\nARIMA(p,q,d) = O mesmo processo que ocorre para o ARMA + aplica\u00e7\u00e3o da diferencia\u00e7\u00e3o para tornar a s\u00e9rie estacion\u00e1ria.\n\nTemos ent\u00e3o o SARIMAX (termo gen\u00e9rico), al\u00e9m dos par\u00e2metros (p,q,d) aceita tamb\u00e9m o (P,D,Q)m, descrevendo os componentes sazonais. P,D e Q representa a regress\u00e3o sazonal, diferencia\u00e7\u00e3o, e m\u00e9dia movel, m representa o n\u00famero de pontos para cada ciclo.\n\nO X representa a vari\u00e1vel ex\u00f3gena, como n\u00e3o utilizaremos ela, n\u00e3o entrarei em maiores detalhes.\n\nPara a defini\u00e7\u00e3o dos melhores par\u00e2metros existe a forma manual e a forma autom\u00e1tica, onde a fun\u00e7\u00e3o definir\u00e1 os melhores par\u00e2metros para n\u00f3s...\n\nFun\u00e7\u00e3o: Auto_arima, devo informar ainda o meu dataset, definir os pontos de start do \"p\" e do \"q\", definir a sazonalidade e o per\u00edodo de sazonalidade, no caso s\u00e3o de 7 dias.\n\nO objetivo \u00e9 achar os melhores par\u00e2metros em base no valor AIC, que deve ser o menor poss\u00edvel, com a menor complexidade\n\nEssa ferramenta impede que cometemos erros na hora de analisar os gr\u00e1ficos e realizar transforma\u00e7\u00f5es nos dados, assim ele nos informa os melhores par\u00e2metros!!\n\n","b99da456":"Aqui ent\u00e3o temos por ordem de import\u00e2ncia, aqueles artigos onde foi encontrado uma maior similaridade com a frase pesquisada, lembrando que aqui estamos utilizando apenas em 500 artigos, por\u00e9m pode ser utilizado em todos os outros do banco de dados","5a4849c7":"Para transformar em uma s\u00e9rie estacion\u00e1ria, foi necess\u00e1ria a realiza\u00e7\u00e3o de 2 diferencia\u00e7\u00f5es. ","e313b67f":"Utilizarei a biblioteca Spacy para fazer a leitura de linguagem natural. Trouxe uma defini\u00e7\u00e3o de linguagem natural segundo o site do wikipedia:\n\nL\u00edngua natural (l\u00edngua humana, l\u00edngua idiom\u00e1tica, ou somente l\u00edngua ou idioma) \u00e9 qualquer linguagem desenvolvida naturalmente pelo ser humano, de forma n\u00e3o premeditada, como resultado da facilidade inata para a linguagem possu\u00edda pelo intelecto humano. V\u00e1rios exemplos podem ser dados como as l\u00ednguas faladas e as l\u00ednguas de sinais. A linguagem natural \u00e9 normalmente utilizada para a comunica\u00e7\u00e3o. As l\u00ednguas naturais s\u00e3o diferentes das l\u00ednguas constru\u00eddas e das l\u00ednguas formais, tais como a lingu\u00edstica computacional, a l\u00edngua escrita, a linguagem animal e as linguagens usadas no estudo formal da l\u00f3gica, especialmente da l\u00f3gica matem\u00e1tica.\n\n\nUm exemplo disso \u00e9 que o pr\u00f3prio algoritmo ja define o que \u00e9 localiza\u00e7\u00e3o, pessoas, nacionalidade, nomes pr\u00f3prio etc... Assim, podemos verificar os pa\u00edses que s\u00e3o mais citados nos textos\n","1990e2ec":"**CONCLUS\u00c3O:**\n\nPodemos ver de uma forma simplificada, como a utiliza\u00e7\u00e3o do machine learning e t\u00e9cnicas de Intelig\u00eancia Artificial, podem contribuir para a solu\u00e7\u00f5es de alguns problemas e pode tamb\u00e9m automatizar tarefas que poderiam levar muito tempo. Podemos concluir que com a utiliza\u00e7\u00e3o de modelos preditivos, \u00e9 poss\u00edvel prever com uma certa precis\u00e3o os n\u00fameros de mortes para que as governantes possam tomar atitudes acertivas.\n\nAlgumas considera\u00e7\u00f5es: \n\nAs previs\u00f5es foram feitas apenas com rela\u00e7\u00e3o as datas, mas outros fatores podem inteferir nos n\u00fameros.\n\nQualquer tipo de erro que eu cometi, podem entrar em contado, sempre gosto de cr\u00edticas e sugest\u00f5es\n\nPara a explora\u00e7\u00e3o dos textos foi utilizado apenas 500 textos de forma 'aleat\u00f3ria' pois caso utilizasse os 20 mil, tomaria muito tempo de processamento.\n\nAlgumas das fun\u00e7\u00f5es utilizadas tomei como base de outros trabalhos, sendo assim, citei quando necess\u00e1rio.\n\n\n","d318c4a1":"O pr\u00f3ximo desafio \u00e9 verificar a similaridade de alguns textos com os artigos, obtendo aqueles artigos que possuem uma maior similaridade\n\nUm dos textos propostos \u00e9: 'Guidance on ways to scale up NPIs in a more coordinated way (e.g., establish funding, infrastructure and authorities to support real time, authoritative (qualified participants) collaboration with all states to gain consensus on consistent guidance and to mobilize resources to geographic areas where critical shortfalls are identified) to give us time to enhance our health care delivery system capacity to respond to an increase in cases. Rapid design and execution of experiments to examine and compare NPIs currently being implemented. DHS Centers for Excellence could potentially be leveraged to conduct these experiments'"}}