{"cell_type":{"5e4632dd":"code","3d9c3b76":"code","5dee0bb2":"code","90cb150d":"code","9381e82a":"code","08a99946":"code","c8a5bb84":"code","68563175":"code","79f4384e":"code","367c44dc":"code","ca6c8273":"code","bd131fb4":"markdown","7a4dbd51":"markdown","96ebbb1f":"markdown","73ad94f8":"markdown","aadc690c":"markdown","807f4443":"markdown","be34cb3d":"markdown","06c14de6":"markdown","6fac26c6":"markdown","df62d382":"markdown"},"source":{"5e4632dd":"# Install dependencies.\n\n!pip install -q kornia pytorch_lightning","3d9c3b76":"import random\nfrom typing import Callable, Tuple\n\nfrom kornia import augmentation as aug\nfrom kornia import filters\nfrom kornia.geometry import transform as tf\nimport torch\nfrom torch import nn, Tensor\n\n\nclass RandomApply(nn.Module):\n    def __init__(self, fn: Callable, p: float):\n        super().__init__()\n        self.fn = fn\n        self.p = p\n\n    def forward(self, x: Tensor) -> Tensor:\n        return x if random.random() > self.p else self.fn(x)\n\n\ndef default_augmentation(image_size: Tuple[int, int] = (224, 224)) -> nn.Module:\n    return nn.Sequential(\n        tf.Resize(size=image_size),\n        RandomApply(aug.ColorJitter(0.8, 0.8, 0.8, 0.2), p=0.8),\n        aug.RandomGrayscale(p=0.2),\n        aug.RandomHorizontalFlip(),\n        RandomApply(filters.GaussianBlur2d((3, 3), (1.5, 1.5)), p=0.1),\n        aug.RandomResizedCrop(size=image_size),\n        aug.Normalize(\n            mean=torch.tensor([0.485, 0.456, 0.406]),\n            std=torch.tensor([0.229, 0.224, 0.225]),\n        ),\n    )","5dee0bb2":"from typing import Union\n\n\ndef mlp(dim: int, projection_size: int = 256, hidden_size: int = 4096) -> nn.Module:\n    return nn.Sequential(\n        nn.Linear(dim, hidden_size),\n        nn.BatchNorm1d(hidden_size),\n        nn.ReLU(inplace=True),\n        nn.Linear(hidden_size, projection_size),\n    )\n\n\nclass EncoderWrapper(nn.Module):\n    def __init__(\n        self,\n        model: nn.Module,\n        projection_size: int = 256,\n        hidden_size: int = 4096,\n        layer: Union[str, int] = -2,\n    ):\n        super().__init__()\n        self.model = model\n        self.projection_size = projection_size\n        self.hidden_size = hidden_size\n        self.layer = layer\n\n        self._projector = None\n        self._projector_dim = None\n        self._encoded = torch.empty(0)\n        self._register_hook()\n\n    @property\n    def projector(self):\n        if self._projector is None:\n            self._projector = mlp(\n                self._projector_dim, self.projection_size, self.hidden_size\n            )\n        return self._projector\n\n    def _hook(self, _, __, output):\n        output = output.flatten(start_dim=1)\n        if self._projector_dim is None:\n            self._projector_dim = output.shape[-1]\n        self._encoded = self.projector(output)\n\n    def _register_hook(self):\n        if isinstance(self.layer, str):\n            layer = dict([*self.model.named_modules()])[self.layer]\n        else:\n            layer = list(self.model.children())[self.layer]\n\n        layer.register_forward_hook(self._hook)\n\n    def forward(self, x: Tensor) -> Tensor:\n        _ = self.model(x)\n        return self._encoded","90cb150d":"from copy import deepcopy\nfrom itertools import chain\nfrom typing import Dict, List\n\nimport pytorch_lightning as pl\nfrom torch import optim\nimport torch.nn.functional as f\n\n\ndef normalized_mse(x: Tensor, y: Tensor) -> Tensor:\n    x = f.normalize(x, dim=-1)\n    y = f.normalize(y, dim=-1)\n    return 2 - 2 * (x * y).sum(dim=-1)\n\n\nclass BYOL(pl.LightningModule):\n    def __init__(\n        self,\n        model: nn.Module,\n        image_size: Tuple[int, int] = (128, 128),\n        hidden_layer: Union[str, int] = -2,\n        projection_size: int = 256,\n        hidden_size: int = 4096,\n        augment_fn: Callable = None,\n        beta: float = 0.999,\n        **hparams,\n    ):\n        super().__init__()\n        self.augment = default_augmentation(image_size) if augment_fn is None else augment_fn\n        self.beta = beta\n        self.encoder = EncoderWrapper(\n            model, projection_size, hidden_size, layer=hidden_layer\n        )\n        self.predictor = nn.Linear(projection_size, projection_size, hidden_size)\n        self.hparams = hparams\n        self._target = None\n\n        self.encoder(torch.zeros(2, 3, *image_size))\n\n    def forward(self, x: Tensor) -> Tensor:\n        return self.predictor(self.encoder(x))\n\n    @property\n    def target(self):\n        if self._target is None:\n            self._target = deepcopy(self.encoder)\n        return self._target\n\n    def update_target(self):\n        for p, pt in zip(self.encoder.parameters(), self.target.parameters()):\n            pt.data = self.beta * pt.data + (1 - self.beta) * p.data\n\n    # --- Methods required for PyTorch Lightning only! ---\n\n    def configure_optimizers(self):\n        optimizer = getattr(optim, self.hparams.get(\"optimizer\", \"Adam\"))\n        lr = self.hparams.get(\"lr\", 1e-4)\n        weight_decay = self.hparams.get(\"weight_decay\", 1e-6)\n        return optimizer(self.parameters(), lr=lr, weight_decay=weight_decay)\n\n    def training_step(self, batch, *_) -> Dict[str, Union[Tensor, Dict]]:\n        x = batch[0]\n        with torch.no_grad():\n            x1, x2 = self.augment(x), self.augment(x)\n\n        pred1, pred2 = self.forward(x1), self.forward(x2)\n        with torch.no_grad():\n            targ1, targ2 = self.target(x1), self.target(x2)\n        loss = torch.mean(normalized_mse(pred1, targ2) + normalized_mse(pred2, targ1))\n\n        self.log(\"train_loss\", loss.item())\n        self.update_target()\n\n        return {\"loss\": loss}\n\n    @torch.no_grad()\n    def validation_step(self, batch, *_) -> Dict[str, Union[Tensor, Dict]]:\n        x = batch[0]\n        x1, x2 = self.augment(x), self.augment(x)\n        pred1, pred2 = self.forward(x1), self.forward(x2)\n        targ1, targ2 = self.target(x1), self.target(x2)\n        loss = torch.mean(normalized_mse(pred1, targ2) + normalized_mse(pred2, targ1))\n\n        return {\"loss\": loss}\n\n    @torch.no_grad()\n    def validation_epoch_end(self, outputs: List[Dict]) -> Dict:\n        val_loss = sum(x[\"loss\"] for x in outputs) \/ len(outputs)\n        self.log(\"val_loss\", val_loss.item())","9381e82a":"class SupervisedLightningModule(pl.LightningModule):\n    def __init__(self, model: nn.Module, **hparams):\n        super().__init__()\n        self.model = model\n\n    def forward(self, x: Tensor) -> Tensor:\n        return self.model(x)\n\n    def configure_optimizers(self):\n        optimizer = getattr(optim, self.hparams.get(\"optimizer\", \"Adam\"))\n        lr = self.hparams.get(\"lr\", 1e-4)\n        weight_decay = self.hparams.get(\"weight_decay\", 1e-6)\n        return optimizer(self.parameters(), lr=lr, weight_decay=weight_decay)\n\n    def training_step(self, batch, *_) -> Dict[str, Union[Tensor, Dict]]:\n        x, y = batch\n        loss = f.cross_entropy(self.forward(x), y)\n        self.log(\"train_loss\", loss.item())\n        return {\"loss\": loss}\n\n    @torch.no_grad()\n    def validation_step(self, batch, *_) -> Dict[str, Union[Tensor, Dict]]:\n        x, y = batch\n        loss = f.cross_entropy(self.forward(x), y)\n        return {\"loss\": loss}\n\n    @torch.no_grad()\n    def validation_epoch_end(self, outputs: List[Dict]) -> Dict:\n        val_loss = sum(x[\"loss\"] for x in outputs) \/ len(outputs)\n        self.log(\"val_loss\", val_loss.item())","08a99946":"from torchvision.datasets import STL10\nfrom torchvision.transforms import ToTensor\n\n\nTRAIN_DATASET = STL10(root=\"data\", split=\"train\", download=True, transform=ToTensor())\nTRAIN_UNLABELED_DATASET = STL10(\n    root=\"data\", split=\"train+unlabeled\", download=True, transform=ToTensor()\n)\nTEST_DATASET = STL10(root=\"data\", split=\"test\", download=True, transform=ToTensor())","c8a5bb84":"from os import cpu_count\n\nfrom torch.utils.data import DataLoader\nfrom torchvision.models import resnet18\n\n\nmodel = resnet18(pretrained=True)\nsupervised = SupervisedLightningModule(model)\ntrainer = pl.Trainer(max_epochs=25, gpus=-1, weights_summary=None)\ntrain_loader = DataLoader(\n    TRAIN_DATASET,\n    batch_size=128,\n    shuffle=True,\n    drop_last=True,\n)\nval_loader = DataLoader(\n    TEST_DATASET,\n    batch_size=128,\n)\ntrainer.fit(supervised, train_loader, val_loader)","68563175":"def accuracy(pred: Tensor, labels: Tensor) -> float:\n    return (pred.argmax(dim=-1) == labels).float().mean().item()\n\n\nmodel.cuda()\nacc = sum([accuracy(model(x.cuda()), y.cuda()) for x, y in val_loader]) \/ len(val_loader)\nprint(f\"Accuracy: {acc:.3f}\")","79f4384e":"model = resnet18(pretrained=True)\nbyol = BYOL(model, image_size=(96, 96))\ntrainer = pl.Trainer(\n    max_epochs=50, \n    gpus=-1,\n    accumulate_grad_batches=2048 \/\/ 128,\n    weights_summary=None,\n)\ntrain_loader = DataLoader(\n    TRAIN_UNLABELED_DATASET,\n    batch_size=128,\n    shuffle=True,\n    drop_last=True,\n)\ntrainer.fit(byol, train_loader, val_loader)","367c44dc":"# Extract the state dictionary, initialize a new ResNet18 model,\n# and load the state dictionary into the new model.\n#\n# This ensures that we remove all hooks from the previous model,\n# which are automatically implemented by BYOL.\nstate_dict = model.state_dict()\nmodel = resnet18()\nmodel.load_state_dict(state_dict)\n\nsupervised = SupervisedLightningModule(model)\ntrainer = pl.Trainer(\n    max_epochs=25, \n    gpus=-1,\n    weights_summary=None,\n)\ntrain_loader = DataLoader(\n    TRAIN_DATASET,\n    batch_size=128,\n    shuffle=True,\n    drop_last=True,\n)\ntrainer.fit(supervised, train_loader, val_loader)","ca6c8273":"def accuracy(pred: Tensor, labels: Tensor) -> float:\n    return (pred.argmax(dim=-1) == labels).float().mean().item()\n\n\nmodel.cuda()\nacc = sum([accuracy(model(x.cuda()), y.cuda()) for x, y in val_loader]) \/ len(val_loader)\nprint(f\"Accuracy: {acc:.3f}\")","bd131fb4":"### Supervised Training Module\n\nWe also need a Lightning module for supervised training on STL10, after any self-supervised training has completed.  There's not much special here -- just standard supervised training.","7a4dbd51":"### Supervised Training Again\n\nExtract the state dictionary from BYOL, and load it into our ResNet18 model before starting training. Then run supervised training, and watch the accuracy improve from last time.","96ebbb1f":"### BYOL and Training Code\n\nEncapsulate BYOL into a single module. We use PyTorch Lightning here, because it makes training very easy. This code also works for multi-GPU or TPU training, and experiments are logged automatically.","73ad94f8":"### Encoder Wrapper\n\nWe need an Encoder module. The Encoder is responsible for extracting features from the base model, and projecting those features into a lower-dimensional, latent space. We\u2019ll implement it using a wrapper class, which allows us to easily use BYOL with any model \u2014 not just one that we hard-code into our scripts. There are two primary components.\n* Feature Extractor: collects the outputs from one of the last model layers.\n* Projector: a linear layer, which projects outputs down lower dimensions.\nThe feature extraction is implemented using hooks.","aadc690c":"# Self Supervised Learning with BYOL\n\nUsing STL10 dataset.","807f4443":"### Install dependencies\n\n**PyTorch** and **Torchvision** are pre-installed. We only need to install **kornia** and **pytorch_lighting**.\n\n> Kornia for implementing the data augmentation\/transformations.\n\n> PyTorch Lightning for writing the BYOL training code. It\u2019s a fantastic library for deep learning projects\/research written in PyTorch, which includes conveniences like multi-GPU training, experiment logging, model checkpointing, and mixed-precision training.","be34cb3d":"### Self-Supervised Training with BYOL\n\nPerform our self-supervised training.","06c14de6":"### Data Augmentations\/Transformations","6fac26c6":"### Supervised Training without BYOL\n\nRun through supervised training and measure the accuracy.","df62d382":"### STL10 Datasets\n\nWe need 3 separate datasets from STL10 for this experiment:\n1. `\"train\"` -- Contains only labeled training images. Used for supervised training.\n2. `\"train+unlabeled\"` -- Contains training images, plus a large number of unlabelled images.  Used for self-supervised learning with BYOL.\n3. `\"test\"` -- Labeled test images.  We use it both as a validation set, and for computing the final model accuracy."}}