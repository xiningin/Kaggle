{"cell_type":{"f876f3e4":"code","a45cc91e":"code","b029618e":"code","2641fe5b":"code","972defce":"code","ee8dfffe":"code","598c261c":"code","b1364f63":"code","f86e84a8":"code","14243d14":"code","cc1622e2":"code","60d591d5":"code","4241ed1b":"code","259814d4":"code","ebc3bbbc":"code","c333bcf2":"code","1d533f43":"code","256b896a":"code","6d9cb1d0":"code","2ac1e17f":"markdown","6a704d6d":"markdown","fe0e976c":"markdown","ab7449b6":"markdown","74168423":"markdown","9b47df4d":"markdown","d699417e":"markdown","bafbbac4":"markdown","38c0cc7a":"markdown","38f426c4":"markdown"},"source":{"f876f3e4":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport numpy as np\nimport os\nimport re\nimport string\nimport random","a45cc91e":"class Config:\n    vocab_size = 20000\n    batch_size = 128\n    maxlen = 80\n    embed_dim = 256 \n    num_heads = 2\n    feed_forward_dim = 256\n    num_transformer_blocks = 2","b029618e":"shakespear_data = pd.read_csv(\"\/kaggle\/input\/shakespeare-plays\/Shakespeare_data.csv\")\nshakespear_data.head(10)","2641fe5b":"PlayerLines = shakespear_data[\"PlayerLine\"]\nPlayerLines.head(10)","972defce":"PlayerLines= list(PlayerLines)","ee8dfffe":"tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=Config.vocab_size - 1)\ntokenizer.fit_on_texts(PlayerLines)\nsequences = tokenizer.texts_to_sequences(PlayerLines)\nsequences = keras.preprocessing.sequence.pad_sequences(sequences, maxlen=Config.maxlen + 1, padding=\"post\")","598c261c":"def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n    \"\"\"\n    Mask the upper half of the dot product matrix in self attention.\n    This prevents flow of information from future tokens to current token.\n    1's in the lower triangle, counting from the lower right corner.\n    \"\"\"\n    i = tf.range(n_dest)[:, None]\n    j = tf.range(n_src)\n    m = i >= j - n_src + n_dest\n    mask = tf.cast(m, dtype)\n    mask = tf.reshape(mask, [1, n_dest, n_src])\n    mult = tf.concat(\n        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n    )\n    return tf.tile(mask, mult)\n\n\nclass TransformerBlock(layers.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n        super(TransformerBlock, self).__init__()\n        self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n        self.ffn = keras.Sequential(\n            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n\n    def call(self, inputs):\n        input_shape = tf.shape(inputs)\n        batch_size = input_shape[0]\n        seq_len = input_shape[1]\n        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n        attention_output = self.dropout1(attention_output)\n        out1 = self.layernorm1(inputs + attention_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output)\n        return self.layernorm2(out1 + ffn_output)","b1364f63":"class TokenAndPositionEmbedding(layers.Layer):\n    def __init__(self, maxlen, vocab_size, embed_dim):\n        super(TokenAndPositionEmbedding, self).__init__()\n        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n\n    def call(self, x):\n        maxlen = tf.shape(x)[-1]\n        positions = tf.range(start=0, limit=maxlen, delta=1)\n        positions = self.pos_emb(positions)\n        x = self.token_emb(x)\n        return x + positions","f86e84a8":"def get_transformer_model(\n    maxlen, \n    vocab_size, \n    embed_dim, \n    num_heads, \n    feed_forward_dim, \n    num_transformer_blocks=1\n):\n    inputs = layers.Input(shape=(maxlen,), dtype=tf.int32)\n    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n    x = embedding_layer(inputs)\n    for i in range(num_transformer_blocks):\n        transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n        x = transformer_block(x)\n    outputs = layers.Dense(vocab_size)(x)\n    model = keras.Model(inputs=inputs, outputs=[outputs, x])\n    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n    model.compile(\n        \"adam\", \n        loss=[loss_fn, None],\n        metrics=[\"accuracy\", None]\n    )  # No loss and optimization based on word embeddings from transformer block\n    return model","14243d14":"def preprocess(sequence):\n    x = sequence[:, :-1]\n    y = sequence[:, 1:]\n    # Only print once\n    print(sequence, x, y)\n    return x, y","cc1622e2":"data = tf.data.Dataset.from_tensor_slices((sequences)).shuffle(256).batch(Config.batch_size).map(preprocess)\ndata","60d591d5":"for x, y in data.take(1):\n    print(x.shape ,y.shape)","4241ed1b":"class TextGenerator(keras.callbacks.Callback):\n    \"\"\"A callback to generate text from a trained model.\n    1. Feed some starting prompt to the model\n    2. Predict probabilities for the next token\n    3. Sample the next token and add it to the next input\n\n    Arguments:\n        max_tokens: Integer, the number of tokens to be generated after prompt.\n        start_tokens: List of integers, the token indices for the starting prompt.\n        tokenizer: Tokenizer.\n        top_k: Integer, sample from the `top_k` token predictions.\n        print_every: Integer, print after this many epochs.\n    \"\"\"\n\n    def __init__(\n        self, max_tokens, start_sentence, tokenizer, top_k=10, print_every=1\n    ):\n        self.max_tokens = max_tokens\n        self.tokenizer = tokenizer\n        self.print_every = print_every\n        self.k = top_k\n        self.start_tokens = tokenizer.texts_to_sequences([start_sentence.split()])[0]\n\n    def sample_from(self, logits):\n        logits, indices = tf.math.top_k(logits, k=self.k, sorted=True)\n        indices = np.asarray(indices).astype(\"int32\")\n        preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n        preds = np.asarray(preds).astype(\"float32\")\n        return np.random.choice(indices, p=preds)\n    \n    def generate_text_from_texts(self, texts):\n        tokens = tokenizer.texts_to_sequences([texts.split()])[0]\n        return generate_text_from_tokens(tokens)\n    \n    def generate_text_from_tokens(self, start_tokens):\n        num_tokens_generated = 0\n        tokens_generated = []\n        while num_tokens_generated <= self.max_tokens:\n            pad_len = self.max_tokens - len(start_tokens)\n            sample_index = len(start_tokens) - 1\n            if pad_len < 0:\n                x = start_tokens[:self.max_tokens]\n                sample_index = self.max_tokens - 1\n            elif pad_len > 0:\n                x = start_tokens + [0] * pad_len\n            else:\n                x = start_tokens\n            x = np.array([x])\n            y, _ = self.model.predict(x)\n            sample_token = self.sample_from(y[0][sample_index])\n            tokens_generated.append(sample_token)\n            start_tokens.append(sample_token)\n            num_tokens_generated = len(tokens_generated)\n        txt = self.tokenizer.sequences_to_texts([start_tokens + tokens_generated])[0]\n        return txt\n\n    def on_epoch_end(self, epoch, logs=None):\n        if (epoch + 1) % self.print_every != 0:\n            return\n        txt = self.generate_text_from_tokens(self.start_tokens)\n        print(f\"generated text:\\n{txt}\\n\")\ntext_generator = TextGenerator(Config.maxlen, \"I am\", tokenizer)","259814d4":"keras.backend.clear_session()\nmodel = get_transformer_model(\n    Config.maxlen, \n    Config.vocab_size, \n    Config.embed_dim, \n    Config.num_heads, \n    Config.feed_forward_dim, \n    Config.num_transformer_blocks\n)","ebc3bbbc":"keras.utils.plot_model(model)","c333bcf2":"model.summary()","1d533f43":"model.fit(data, verbose=1, epochs=10, callbacks=[text_generator])","256b896a":"for i in np.random.choice(len(sequences), 10):\n    print(text_generator.generate_text_from_tokens(list(sequences[i][0:5])))","6d9cb1d0":"model.save_weights(\"model.h5\")","2ac1e17f":"## Model Development","6a704d6d":"# Shakespeare Play Generation: Transformer\nIn this notebook, I will build a Text Generator using based on Transformer. I will make the Model learn from Shakespare dataset, so that it can generate texts similar to Shakespeare Play.","fe0e976c":"## Create the Model","ab7449b6":"## Configuration","74168423":"## Import datasets","9b47df4d":"## Generating Texts\nI will choose sentences from Shakespeare dataset and choose first few words to let them generate texts.","d699417e":"## Implement an embedding layer\nCreate two seperate embedding layers: one for tokens and one for token index (positions).","bafbbac4":"## Setup","38c0cc7a":"### Implement a Transformer block as a layer","38f426c4":"## Save the Model"}}