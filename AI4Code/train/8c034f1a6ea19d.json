{"cell_type":{"4e6ba37a":"code","1ad10d94":"code","eb523cbe":"code","09b3c9c1":"code","e7208f2c":"code","456f276c":"code","6b34c7f8":"code","4ac11783":"code","a622f19a":"code","8f4b0f72":"code","087f2709":"code","3858e0e7":"code","ec128758":"code","161f2b35":"code","f980b722":"code","400c752b":"code","a2772a04":"code","edb93558":"code","5451cb23":"code","39a31e7f":"code","b0b5af5b":"code","37593abd":"code","62e8e89f":"code","c1547cb8":"markdown","6074ca49":"markdown","1a074048":"markdown","930864e1":"markdown","05e93617":"markdown","29edb722":"markdown","498fea6b":"markdown","b1eb615e":"markdown","22083fa3":"markdown","dd08f556":"markdown","d5532018":"markdown","5cbc6f53":"markdown"},"source":{"4e6ba37a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1ad10d94":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import train_test_split, cross_val_score,GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score","eb523cbe":"df = pd.read_csv('..\/input\/kindle-reviews\/kindle_reviews.csv')\ndf.head()","09b3c9c1":"df['overall'].value_counts()","e7208f2c":"# 22 missing reviews and needs to be dropped\ndf.isnull().sum()","456f276c":"#drop the rows where there are no reviews\ndf.dropna(subset = ['reviewText'], inplace = True)\n\n#changing the reviewTime column to be of datetime type\ndf.reviewTime = pd.to_datetime(df.reviewTime)\n\n#creating a column with just the year\ndf['Year'] = df.reviewTime.dt.year\ndf.head()","6b34c7f8":"df.Year.value_counts().sort_index().plot(kind = 'bar')\nplt.title('Number of Reviews per Year')\nplt.xlabel('Year')\nplt.ylabel('Number of Reviews')\nplt.show()","4ac11783":"df.reviewerID.value_counts().head(10).plot(kind = 'bar')\nplt.xticks(rotation = 80)\nplt.xlabel('UserID')\nplt.ylabel('Number of Reviews')\nplt.show()","a622f19a":"df.overall.value_counts().plot(kind = 'bar')\nplt.title('Number of Good Ratings vs Bad Ratings')\nplt.xlabel('Rating Scales')\nplt.xticks(rotation = 0)\nplt.ylabel('Total ratings')\nplt.show()","8f4b0f72":"import string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer","087f2709":"reviews = df[['reviewText', 'overall']]\nreviews.head()","3858e0e7":"punc = str.maketrans('', '', string.punctuation)\n#apply the empty mapping table to each element of the series where x is the review for one document.\nreviews['reviewText'] = reviews['reviewText'].apply(lambda x : ' '.join(word.translate(punc) for word in x.split()))\n\n#removing words that is non alpha\nreviews['reviewText'] = reviews['reviewText'].apply(lambda x: ' '.join(word for word in x.split() if word.isalpha()))\n\n#making all words to be lowercase\nreviews['reviewText'] = reviews['reviewText'].apply(lambda x: ' '.join(word.lower() for word in x.split()))\n\n#list of stop words\nstop = stopwords.words('english')\n\n#removing the stop words\nreviews['reviewText'] = reviews['reviewText'].apply(lambda x : ' '.join(word for word in x.split() if word not in stop))\n\n#Lemmatize words to reduce them to their root form. Note: added the pos = 'v' to reduce the incoming word to verb root\nlem = WordNetLemmatizer()\nreviews['reviewText'] = reviews['reviewText'].apply(lambda x : ' '.join(lem.lemmatize(word, pos = 'v') for word in x.split()))\n","ec128758":"reviews['overall'] = np.where(reviews['overall'] > 2, 1, 0)\nreviews.head()","161f2b35":"reviews.overall.value_counts()","f980b722":"def make_xy(data, vec, n):\n    \"\"\"Takes in a dataframe with text and labels and returns a vocabulary of some sort\n        depending on the vectorizer used.\n        \n        Arguments:\n        \n        data - the input dataframe containing the text and the labels\n        \n        vec - the chosen vectorizer to use\n        \n        n - the number of samples per class\n        \"\"\"\n    temp = pd.DataFrame()\n    for rating in range(2):\n        temp = pd.concat([temp, data[data.overall == rating].sample(n, random_state = 42)], ignore_index = True)\n        \n    #vectorizing the vocabulary\n    X = vec.fit_transform(temp.reviewText)\n    y = temp.overall\n    return X, y","400c752b":"count = CountVectorizer()\nX, y = make_xy(reviews, count, 20000)","a2772a04":"#using TfidfVectorizer\ntfidf = TfidfVectorizer()\nXt, yt = make_xy(reviews, tfidf, 20000)","edb93558":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .25, random_state = 42, stratify = y)\n\nnb = MultinomialNB()\nnb.fit(X_train, y_train)\nprint('Training Accuracy: {:.2f}'.format(nb.score(X_train, y_train)))\nprint('Testing Accuracy: {:.2f}'.format(nb.score(X_test, y_test)))","5451cb23":"#Tuning the min_df parameter for the vectorizer and the alpha in the multinomial Naive Bayes\nbest_alpha = 0\nbest_min_df = 0\nbest_score = 0\n\n#param_grid\nalphas = [.1, 1, 5, 10, 50]\nmin_dfs = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n\n#iterate throughout the param grid\nfor alpha in alphas:\n    for m_df in min_dfs:\n        tfidf = TfidfVectorizer(min_df = m_df)\n        X, y = make_xy(reviews, tfidf, 20000)\n        nb = MultinomialNB(alpha = alpha)\n        score = np.mean(cross_val_score(nb, X, y, scoring = 'accuracy', cv = 3))\n        if score > best_score:\n            best_score = score\n            best_alpha = alpha\n            best_min_df = m_df\n            \nprint('Best_score: {:.2f}'.format(best_score))\nprint('Best_alpha: {:.2f}'.format(best_alpha))\nprint('Best_min_df: {:.5f}'.format(best_min_df))","39a31e7f":"tfidf = TfidfVectorizer(min_df = .001)\nX, y = make_xy(reviews,tfidf, 20000)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = 42, stratify = y)\n\nnb = MultinomialNB(alpha = 5)\nnb.fit(X_train, y_train)\nnaive_pred = nb.predict(X_test)\nnaive_prob = nb.predict_proba(X_test)[:,1]\n\n\nprint('Training Accuracy with TfidfVectorizer: {:.2f}'.format(nb.score(X_train, y_train)))\nprint('Testing Accuracy with TfidfVectorizer: {:.2f}'.format(nb.score(X_test, y_test)))\nprint('\\n Confusion matrix:')\nprint(confusion_matrix(y_test, naive_pred))\nprint('\\n Classification Report: ')\nprint(classification_report(y_test, naive_pred))","b0b5af5b":"logreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\nlogreg_pred = logreg.predict(X_test)\nlogreg_prob = logreg.predict_proba(X_test)[:,1]\n\nprint('Training Accuracy with TfidfVectorizer: {:.2f}'.format(logreg.score(X_train, y_train)))\nprint('Testing Accuracy with TfidfVectorizer: {:.2f}'.format(logreg.score(X_test, y_test)))\nprint('\\n Confusion matrix:')\nprint(confusion_matrix(y_test, logreg_pred))\nprint('\\n Classification Report: ')\nprint(classification_report(y_test, logreg_pred))","37593abd":"c_space = np.logspace(-5,6, 8)\nbest_score = []\nscore = 0\nbest_c = 0\n\n#exhaustive search for the best parameters\nfor value in c_space:\n    svm = LinearSVC(C = value)\n    score = np.mean(cross_val_score(nb, X_train, y_train, scoring = 'accuracy', cv = 5))\n    if score > best_score:\n        best_score = score\n        best_c = value\n\n#train model using the best parameters determine from kfold\ntry:\n    svm = LinearSVC(best_c)\n    svm.fit(X_train, y_train)\n    svm_pred = svm.predict(X_test)\n    \nexcept:\n    svm = LinearSVC()\n    svm.fit(X_train, y_train)\n    svm_pred = svm.predict(X_test)\n\nprint('Training Accuracy with TfidfVectorizer: {:.2f}'.format(svm.score(X_train, y_train)))\nprint('Testing Accuracy with TfidfVectorizer: {:.2f}'.format(svm.score(X_test, y_test)))\nprint('\\n Confusion Matrix: ')\nprint(confusion_matrix(y_test, svm_pred))\nprint('\\n Classification Report')\nprint(classification_report(y_test, svm_pred))","62e8e89f":"sns.set_style('white')\nfig, ax = plt.subplots()\nax.plot([0,1], [0,1], linestyle = '--', color = 'darkorange')\n\nprobs = [naive_prob, logreg_prob, svm_pred]\nlabels = ['Naive Bayes', 'Logistic Regression', 'SVC']\nfor idx in range(len(probs)):\n    fpr, tpr, thresholds = roc_curve(y_test, probs[idx])\n    ax.plot(fpr, tpr, label = (labels[idx] + ' AUC score = %.2f' % roc_auc_score(y_test, probs[idx])))\n    \nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curves')\nax.legend(loc = 'lower right')\nax.margins(x = 0.01, y =0.02)\nplt.show()","c1547cb8":"### Logistic Regression","6074ca49":"Hyperparameter tuning in Naive Bayes","1a074048":"## Text Preprocessing","930864e1":"### ROC curve","05e93617":"Train Test Split","29edb722":"#### Coverting rating to positive and negative\n* less than 3 = negative\n* greater than 3 = positive","498fea6b":"### Multinomial Naive Bayes","b1eb615e":"Top 10 users based on reviews","22083fa3":"Ratings count","dd08f556":"Balancing data by downsampling to minority class","d5532018":"## Reading data","5cbc6f53":"### SVC"}}