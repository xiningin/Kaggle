{"cell_type":{"8ca5f0f2":"code","97b7400c":"code","84d669ed":"code","f0be0e38":"code","dcd6a7b6":"code","3a673444":"code","98f59ae9":"code","92fc4909":"code","1eb1edcc":"code","84f758e7":"code","d2ab4db3":"code","8ab5df7f":"code","efdb7416":"code","a9447652":"code","9f58cf9d":"code","09803720":"code","f0711321":"code","28694ca1":"code","c5524fc5":"code","2fb29c14":"code","d6097f95":"code","a5d460d2":"code","ed936af1":"code","2330d322":"code","531aa259":"code","f4887785":"code","130b7506":"code","0cc6369f":"code","33e9e761":"code","12831b1a":"code","e825e4f9":"code","acea8558":"code","c3b8848a":"code","ba3ec531":"code","a8e92b7b":"code","90203a06":"code","3429f307":"code","f3cbddbb":"code","a10173e7":"code","d2ec9f2b":"code","2da8ae91":"code","18dd7899":"code","44e6da92":"code","60edbeb0":"code","dee0974a":"code","6f281aa4":"code","3257659f":"code","ba78a3b0":"code","20160405":"code","e7467239":"code","fddee5b3":"code","a818d945":"code","33515c51":"code","19fa9f4e":"code","085588f3":"code","91704003":"code","f9669024":"code","d15042c7":"code","b91a7981":"code","6342c248":"code","56208548":"code","f7fb703b":"code","d9b2f46e":"markdown","63a219c9":"markdown","3dde5303":"markdown","97ef6365":"markdown","8caa494a":"markdown","b2422364":"markdown","1c12ef1d":"markdown","7b33cf3a":"markdown","f10429a0":"markdown","432590a0":"markdown","480dfdee":"markdown","f727a65b":"markdown","44fb0ed1":"markdown","a0ac9677":"markdown","0391c8d1":"markdown","f106b7df":"markdown","55ec91b2":"markdown","e69d9676":"markdown","6a5c8398":"markdown","08d54675":"markdown","46d59396":"markdown","d18e38c1":"markdown","c8b88111":"markdown","42e5cc25":"markdown","8d48b3a6":"markdown","7127b811":"markdown","1ece0d46":"markdown","beb1a003":"markdown","0e195bea":"markdown","055b5798":"markdown","0948aa1b":"markdown","6bfd4d02":"markdown","4ebfc050":"markdown","f263e7b4":"markdown","a70937bc":"markdown","3d92232e":"markdown","bda67d40":"markdown","7f1c6b29":"markdown","a4863135":"markdown","ac60c817":"markdown","c66992b7":"markdown","a36bcc2c":"markdown","c5de10c2":"markdown","4423c1b6":"markdown","8ab7bb5e":"markdown","33cf55af":"markdown","75237b97":"markdown","c667151c":"markdown","7f08f787":"markdown","f6a53fb9":"markdown"},"source":{"8ca5f0f2":"import numpy as np\nimport pandas as pd\nfrom scipy import stats\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.utils import shuffle\n\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom imblearn.under_sampling import NearMiss\nfrom imblearn.over_sampling import SMOTE\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn import svm\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom lightgbm import LGBMClassifier\nfrom lightgbm import record_evaluation\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_auc_score\n\nfrom sklearn.pipeline import Pipeline\nimport joblib\nimport pickle\nimport json\n\nimport random\n\nimport warnings\nwarnings.filterwarnings('ignore')","97b7400c":"ccfr= pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\nccfr.head()","84d669ed":"x= ccfr['Class']\nplt.figure(figsize=(5,7))\nsns.countplot(ccfr['Class'])\nplt.xlabel('Class')\nplt.ylabel('Count')\nplt.show()","f0be0e38":"fraud_df= ccfr[ccfr['Class']==1]\nnonfraud_df= ccfr[ccfr['Class']==0]\nfraudcount= fraud_df.count()[0]\nnonfraudcount= nonfraud_df.count()[0]\nprint (\"Frauds=\",fraudcount)\nprint (\"Non Frauds=\",nonfraudcount)\nprint (ccfr.shape)","dcd6a7b6":"ccfr.describe()","3a673444":"ccfr.isna().sum()","98f59ae9":"plt.figure(figsize=(22,22))\nsns.heatmap(ccfr.corr(),cmap=\"coolwarm\", annot=True)\nplt.show()","92fc4909":"for c in ccfr.columns[0:30]:\n    print (\"******************** COLUMN \",c,\" ***********************\")\n    col= ccfr[c]\n    col=np.array(col)\n    col_mean= np.mean(col)\n    col_median= np.median(col)\n    col_std= np.std(col)\n    col_var= np.var(col)\n    col_range= col.max()-col.min()\n    fig=sns.FacetGrid(ccfr,hue=\"Class\",height=5,aspect=2,palette=[\"blue\", \"green\"])\n    fig.map(sns.distplot,c)\n    fig.add_legend(labels=['Non Fraud','Fraud'])\n    plt.axvline(col_mean,color='red',label='mean')\n    plt.axvline(col_median,color='yellow',label='median')\n    plt.legend()\n    plt.show()","1eb1edcc":"ccfr.columns","84f758e7":"for feature in ccfr.drop('Class', axis= 1).columns:\n    sns.boxplot(x='Class', y= feature, data= ccfr)\n    plt.show()","d2ab4db3":"X = ccfr.drop('Class', axis= 1)\ny = ccfr['Class']","8ab5df7f":"scaler = StandardScaler()\nscaled_features = scaler.fit_transform(X.values)\nX_scaled = pd.DataFrame(scaled_features, columns= X.columns)","efdb7416":"X_scaled.head()","a9447652":"X_scaled.describe()","9f58cf9d":"ccfr_scaled = pd.concat([X_scaled, y], axis= 1)","09803720":"X = ccfr_scaled.drop('Class', axis= 1)\ny = ccfr_scaled['Class']","f0711321":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.13, random_state= 48)","28694ca1":"X_train.shape","c5524fc5":"y_train.value_counts()","2fb29c14":"y_test.value_counts()","d6097f95":"train = pd.concat([X_train, y_train], axis =1)","a5d460d2":"under_sampler = NearMiss(sampling_strategy= {0:100000, 1:410}) \n#under sampling the majority to 80000 records keeping minority as it is\nX_train, y_train = under_sampler.fit_sample(X_train, y_train)","ed936af1":"over_sampler = SMOTE(sampling_strategy= {0:100000, 1:10000}, random_state= 48)\n#over sampling minority class to 20000 records\nX_train, y_train = over_sampler.fit_sample(X_train, y_train)","2330d322":"train_sampled = pd.concat([X_train, y_train], axis= 1)\n#it is good practice to shuffle the training set\ntrain_sampled = train_sampled.sample(frac=1).reset_index(drop= True)\nX_train = train_sampled.drop('Class', axis= 1)\ny_train = train_sampled['Class']","531aa259":"X_train.shape","f4887785":"y_train.value_counts()","130b7506":"X_test.shape","0cc6369f":"y_test.value_counts()","33e9e761":"sns.countplot(y_train)\nplt.show()","12831b1a":"knn_model = KNeighborsClassifier(n_neighbors= 4, n_jobs= -1)\nknn_model.fit(X_train, y_train)\npred = knn_model.predict(X_test)\nprint (\"ROC AUC Score=\",roc_auc_score(y_test, pred))\nprint (\"Classification Report:\")\nprint (classification_report(y_test, pred))","e825e4f9":"cm = pd.DataFrame(confusion_matrix(y_test, pred))\nplt.figure()\nplt.title(\"Confusion Matrix for KNN\")\nsns.heatmap(cm, cmap= \"Blues\", annot= True, fmt= \"d\")\nplt.xlabel(\"Predicted Classes\")\nplt.ylabel(\"True Classes\")\nplt.show()","acea8558":"lr_model= LogisticRegression(solver= 'liblinear')\nlr_model.fit(X_train,y_train)\npred = lr_model.predict(X_test)\nprint (\"ROC AUC Score=\",roc_auc_score(y_test, pred))\nprint (\"Classification Report:\")\nprint (classification_report(y_test, pred))","c3b8848a":"cm = pd.DataFrame(confusion_matrix(y_test, pred))\nplt.figure()\nplt.title(\"Confusion Matrix for LR\")\nsns.heatmap(cm, cmap= \"Blues\", annot= True, fmt= \"d\")\nplt.xlabel(\"Predicted Classes\")\nplt.ylabel(\"True Classes\")\nplt.show()","ba3ec531":"svc_model = svm.SVC(kernel='rbf', gamma= 0.03, C= 1.0)\nsvc_model.fit(X_train, y_train)\npred= svc_model.predict(X_test)","a8e92b7b":"print (\"ROC AUC Score=\",roc_auc_score(y_test, pred))\nprint (\"Classification Report:\")\nprint (classification_report(y_test, pred))","90203a06":"cm = pd.DataFrame(confusion_matrix(y_test, pred))\nplt.figure()\nplt.title(\"Confusion Matrix for SVM\")\nsns.heatmap(cm, cmap= \"Blues\", annot= True, fmt= \"d\")\nplt.xlabel(\"Predicted Classes\")\nplt.ylabel(\"True Classes\")\nplt.show()","3429f307":"voting_clf_hard = VotingClassifier(estimators = [('lr', lr_model), ('knn', knn_model)], voting = 'hard')\nvoting_clf_hard.fit(X_train, y_train)\npred= voting_clf_hard.predict(X_test)\nprint (\"ROC AUC Score=\",roc_auc_score(y_test, pred))\nprint (\"Classification Report:\")\nprint (classification_report(y_test, pred))","f3cbddbb":"cm = pd.DataFrame(confusion_matrix(y_test, pred))\nplt.figure()\nplt.title(\"Confusion Matrix for LR + KNN Voting Classifier (Hard Voting)\")\nsns.heatmap(cm, cmap= \"Blues\", annot= True, fmt= \"d\")\nplt.xlabel(\"Predicted Classes\")\nplt.ylabel(\"True Classes\")\nplt.show()","a10173e7":"voting_clf_soft = VotingClassifier(estimators = [('lr', lr_model), ('knn', knn_model)], voting = 'soft',\n                                  weights= [0.7, 0.3])\nvoting_clf_soft.fit(X_train, y_train)\npred= voting_clf_soft.predict(X_test)\nprint (\"ROC AUC Score=\",roc_auc_score(y_test, pred))\nprint (\"Classification Report:\")\nprint (classification_report(y_test, pred))","d2ec9f2b":"cm = pd.DataFrame(confusion_matrix(y_test, pred))\nplt.figure()\nplt.title(\"Confusion Matrix for LR + KNN Voting Classifier (Soft Voting)\")\nsns.heatmap(cm, cmap= \"Blues\", annot= True, fmt= \"d\")\nplt.xlabel(\"Predicted Classes\")\nplt.ylabel(\"True Classes\")\nplt.show()","2da8ae91":"dctree_model = DecisionTreeClassifier(random_state=0)\ndctree_model.fit(X_train, y_train)\npred= dctree_model.predict(X_test)\nprint (\"ROC AUC Score=\",roc_auc_score(y_test, pred))\nprint (\"Classification Report:\")\nprint (classification_report(y_test, pred))","18dd7899":"cm = pd.DataFrame(confusion_matrix(y_test, pred))\nplt.figure()\nplt.title(\"Confusion Matrix for Decision Tree\")\nsns.heatmap(cm, cmap= \"Blues\", annot= True, fmt= \"d\")\nplt.xlabel(\"Predicted Classes\")\nplt.ylabel(\"True Classes\")\nplt.show()","44e6da92":"sgd_model = SGDClassifier(class_weight = 'balanced', learning_rate = 'adaptive', n_jobs = -1, eta0 = 0.001, \n                          max_iter = 100000)\nsgd_model.fit(X_train, y_train)\npred = sgd_model.predict(X_test)\nprint (\"ROC AUC Score=\",roc_auc_score(y_test, pred))\nprint (\"Classification Report:\")\nprint (classification_report(y_test, pred))","60edbeb0":"cm = pd.DataFrame(confusion_matrix(y_test, pred))\nplt.figure()\nplt.title(\"Confusion Matrix for SGD\")\nsns.heatmap(cm, cmap= \"Blues\", annot= True, fmt= \"d\")\nplt.xlabel(\"Predicted Classes\")\nplt.ylabel(\"True Classes\")\nplt.show()","dee0974a":"bagging_clf = BaggingClassifier(DecisionTreeClassifier(random_state = 0), n_estimators = 1000, bootstrap = True,\n                               max_samples = 0.85, n_jobs = -1, oob_score = True)\n#bootsrap True signifies sampling (max_samples) from the data without replacement for each estimator\n#oob_score True enables training on set of samples chosen and test on the out-of-bag samples(samples not chosen for training)\nbagging_clf.fit(X_train, y_train)\npred = bagging_clf.predict(X_test)\nprint (\"ROC AUC Score=\",roc_auc_score(y_test, pred))\nprint (\"Classification Report:\")\nprint (classification_report(y_test, pred))","6f281aa4":"cm = pd.DataFrame(confusion_matrix(y_test, pred))\nplt.figure()\nplt.title(\"Confusion Matrix for Bagging Classifier\")\nsns.heatmap(cm, cmap= \"Blues\", annot= True, fmt= \"d\")\nplt.xlabel(\"Predicted Classes\")\nplt.ylabel(\"True Classes\")\nplt.show()","3257659f":"lgb_model = LGBMClassifier(boosting_type = 'gbdt', num_leaves = 32, max_depth = 7, learning_rate = 0.007, \n                           n_estimators = 3500, objective = 'binary', min_split_gain = 0.1, min_child_weight = 0.01,\n                           class_weight= {0:0.2, 1:1},\n                           min_child_samples = 20, subsample=0.6, colsample_bytree = 0.8, reg_alpha = 0.3, reg_lambda = 0.7,\n                           n_jobs = -1, verbose = -1)\nhistory = {}\neval_history = record_evaluation(history)\nlgb_model.fit(X_train, y_train,\n             eval_set = [(X_train, y_train), (X_test, y_test)],\n             eval_metric = 'auc', verbose = 500,\n             callbacks = [eval_history])\npred = lgb_model.predict(X_test)\nprint (\"ROC AUC Score=\",roc_auc_score(y_test, pred))\nprint (\"Classification Report:\")\nprint (classification_report(y_test, pred))","ba78a3b0":"train_aucs = history['training']['auc']\ntest_aucs = history['valid_1']['auc']\nplt.figure(figsize = (8,6))\nplt.ylim([0,1.01])\nplt.plot(train_aucs, color= 'r', label= 'training')\nplt.plot(test_aucs, color= 'g', label= 'testing')\nplt.xlabel(\"No. of Estimators\")\nplt.ylabel('AUC Scores')\nplt.legend(loc= 'best')\nplt.title(\"LightGBM Performance with n_estimators chosen\")\nplt.show()","20160405":"train_logloss = history['training']['binary_logloss']\ntest_logloss = history['valid_1']['binary_logloss']\nplt.figure(figsize = (8,6))\nplt.ylim([0,1.01])\nplt.plot(train_logloss, color= 'r', label= 'training')\nplt.plot(test_logloss, color= 'g', label= 'testing')\nplt.xlabel(\"No. of Estimators\")\nplt.ylabel('Binary Log Loss')\nplt.legend(loc= 'best')\nplt.title(\"LightGBM Loss Minimization\")\nplt.show()","e7467239":"cm = pd.DataFrame(confusion_matrix(y_test, pred))\nplt.figure()\nplt.title(\"Confusion Matrix for LightGBM\")\nsns.heatmap(cm, cmap= \"Blues\", annot= True, fmt= \"d\")\nplt.xlabel(\"Predicted Classes\")\nplt.ylabel(\"True Classes\")\nplt.show()","fddee5b3":"lgb_model = LGBMClassifier(boosting_type = 'gbdt', num_leaves = 30, max_depth = 7, learning_rate = 0.007, \n                           n_estimators = 4500, objective = 'binary', min_split_gain = 0.1, min_child_weight = 0.01,\n                           class_weight= {0:0.2, 1:1},\n                           min_child_samples = 20, subsample=0.6, colsample_bytree = 0.8, reg_alpha = 0.3, reg_lambda = 0.7,\n                           n_jobs = -1, verbose = -1)\n\ncv = KFold(n_splits = 10, random_state = 48, shuffle = True)\n\nTP = 0 #TruePositives\nTN = 0 #TrueNegatives\nFP = 0 #FalsePositives\nFN = 0 #FalseNegatives\nroc_auc_scores = []\n\nx1 = X #taking the original scaled data post train test split\ny1 = y\n\nfor train_ind, test_ind in cv.split(x1):\n    xtrain, xtest, ytrain, ytest= x1.loc[list(train_ind)], x1.loc[list(test_ind)], y1.loc[list(train_ind)], y1.loc[list(test_ind)]\n    \n    under_sampler_cv = NearMiss(sampling_strategy= {0:100000, 1:410}) \n    xtrain, ytrain = under_sampler_cv.fit_sample(xtrain, ytrain)\n    \n    over_sampler_cv = SMOTE(sampling_strategy= {0:100000, 1:10000}, random_state= 48)\n    xtrain, ytrain = over_sampler_cv.fit_sample(xtrain, ytrain)\n    \n    train_set = pd.concat([xtrain, ytrain], axis= 1)\n    train_set = train_set.sample(frac=1).reset_index(drop= True)\n    xtrain = train_set.drop('Class', axis= 1)\n    ytrain = train_set['Class']\n    \n    lgb_model.fit(xtrain, ytrain)\n    prd = lgb_model.predict(xtest)\n    true = np.array(ytest)\n    l = len(prd)\n    for i in range (l):\n        if true[i]==1 and prd[i]==1:\n            TP+=1\n        if true[i]==1 and prd[i]==0:\n            FN+=1\n        if true[i]==0 and prd[i]==1:\n            FP+=1\n        if true[i]==0 and prd[i]==0:\n            TN+=1\n    roc_auc_scores.append(roc_auc_score(true, prd))","a818d945":"cm = pd.DataFrame([[TN, FP], [FN, TP]], index= [0,1], columns= [0,1])\nplt.figure()\nplt.title(\"Confusion Matrix for LightGBM with 10 Fold CV\")\nsns.heatmap(cm, cmap= \"Blues\", annot= True, fmt= \"d\")\nplt.xlabel(\"Predicted Classes\")\nplt.ylabel(\"True Classes\")\nplt.show()","33515c51":"print (\"% of Frauds predicted correctly=\",(426\/(66+426))*100)\nprint (\"Average ROC AUC Score=\",np.average(roc_auc_scores))","19fa9f4e":"params = {'num_leaves' : [30, 40],\n          'max_depth' : [7, 9],  \n          'min_child_weight' : [0.01, 1],\n          'subsample' : [0.6, 0.7],\n          'colsample_bytree' : [0.8, 0.9],\n          'reg_alpha' : [0.1, 0.3],\n          'reg_lambda' : [0.1, 0.7]}\nclf = GridSearchCV(lgb_model, params, scoring= 'recall', n_jobs= -1, cv= 2)\nclf.fit(X_train, y_train)","085588f3":"clf.best_params_","91704003":"lgb_model = LGBMClassifier(boosting_type = 'gbdt', num_leaves = 30, max_depth = 9, learning_rate = 0.007, \n                           n_estimators = 3500, objective = 'binary', min_split_gain = 0.1, min_child_weight = 0.01,\n                           class_weight= {0:0.1, 1:1},\n                           min_child_samples = 20, subsample=0.6, colsample_bytree = 0.9, reg_alpha = 0.1, reg_lambda = 0.7,\n                           n_jobs = -1, verbose = -1)\n\ncv = KFold(n_splits = 10, random_state = 48, shuffle = True)\n\nTP = 0 #TruePositives\nTN = 0 #TrueNegatives\nFP = 0 #FalsePositives\nFN = 0 #FalseNegatives\nroc_auc_scores = []\n\nx1 = X #taking the original scaled data post train test split\ny1 = y\n\nfor train_ind, test_ind in cv.split(x1):\n    xtrain, xtest, ytrain, ytest= x1.loc[list(train_ind)], x1.loc[list(test_ind)], y1.loc[list(train_ind)], y1.loc[list(test_ind)]\n    \n    under_sampler_cv = NearMiss(sampling_strategy= {0:100000, 1:410}) \n    xtrain, ytrain = under_sampler_cv.fit_sample(xtrain, ytrain)\n    \n    over_sampler_cv = SMOTE(sampling_strategy= {0:100000, 1:10000}, random_state= 48)\n    xtrain, ytrain = over_sampler_cv.fit_sample(xtrain, ytrain)\n    \n    train_set = pd.concat([xtrain, ytrain], axis= 1)\n    train_set = train_set.sample(frac=1).reset_index(drop= True)\n    xtrain = train_set.drop('Class', axis= 1)\n    ytrain = train_set['Class']\n    \n    lgb_model.fit(xtrain, ytrain)\n    prd = lgb_model.predict(xtest)\n    true = np.array(ytest)\n    l = len(prd)\n    for i in range (l):\n        if true[i]==1 and prd[i]==1:\n            TP+=1\n        if true[i]==1 and prd[i]==0:\n            FN+=1\n        if true[i]==0 and prd[i]==1:\n            FP+=1\n        if true[i]==0 and prd[i]==0:\n            TN+=1\n    roc_auc_scores.append(roc_auc_score(true, prd))","f9669024":"cm = pd.DataFrame([[TN, FP], [FN, TP]], index= [0,1], columns= [0,1])\nplt.figure()\nplt.title(\"Confusion Matrix for LightGBM with 10 Fold CV\")\nsns.heatmap(cm, cmap= \"Blues\", annot= True, fmt= \"d\")\nplt.xlabel(\"Predicted Classes\")\nplt.ylabel(\"True Classes\")\nplt.show()","d15042c7":"print (\"% of Frauds predicted correctly=\",(430\/(62+430))*100)\nprint (\"Average ROC AUC Score=\",np.average(roc_auc_scores))","b91a7981":"import lightgbm\nlightgbm.plot_importance(lgb_model, figsize= (12, 10))","6342c248":"pred = lgb_model.predict(X_test)\nprint (\"ROC AUC Score=\",roc_auc_score(y_test, pred))\nprint (\"Classification Report:\")\nprint (classification_report(y_test, pred))","56208548":"cm = pd.DataFrame(confusion_matrix(y_test, pred))\nplt.figure()\nplt.title(\"Confusion Matrix for LightGBM\")\nsns.heatmap(cm, cmap= \"Blues\", annot= True, fmt= \"d\")\nplt.xlabel(\"Predicted Classes\")\nplt.ylabel(\"True Classes\")\nplt.show()","f7fb703b":"from sklearn.metrics import roc_curve\nfpr , tpr, threshold = roc_curve(y_test, pred)\nroc_auc = roc_auc_score(y_test, pred)\n\nplt.title('Receiver Operating Characteristics')\nplt.plot(fpr, tpr, 'b', label= \"AUC = %0.2f\" % roc_auc)\nplt.legend(loc = 'best')\nplt.plot([0,1], [0,1], 'r--')\nplt.xlim([0,1])\nplt.ylim([0,1])\nplt.xlabel(\"False Postive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.show()","d9b2f46e":"**KNN Classifier**","63a219c9":"With the New params, the average prediction rate is now 87.4% with ROC AUC score of 0.9326","3dde5303":"**SGD Clasifier**","97ef6365":"There are plenty outliers in each feature. However, it is better not removing them as the minority class already has only few samples and there is good chance that these outliers impart some meaning to the problem:","8caa494a":"**LightGBM**<br>\nLightGBM is a gradient boosting framework that is new and has been growing fast in terms of popularity. It promises great results similar to XGBoost and that to at extremely lesser training time. We will train the LightGBM model and track it's evaluation history using the eval_history callback to know the right no. of estimators to stop at. The evaluation metric chosen for the purpose is the roc auc score.","b2422364":"**Post Sampling**<br>\nTrain Set: (110000, 30) -- 100000 Non Frauds, 10000 Frauds <br>\nTest Set: (37025, 30) -- 36943 Non Frauds, 82 Frauds","1c12ef1d":"Thus, with the final trained LGBMClassifier is able to achieve 95% fraud prediction rate at the cost of a meagre 132 non frauds being classified as fraud, with an ROC AUC score of 0.9738","7b33cf3a":"**LightGBM Explanation**","f10429a0":"### Imports","432590a0":"LightGBM gives best overall performance by predicting 94% of the frauds at the cost of just 259 non frauds getting classified as frauds.","480dfdee":"**Logistic Regression + KNN Voting CLassifier**","f727a65b":"### EDA","44fb0ed1":"The Soft Voting Classifier helps us combine the classifiers well. We have retained the better fraud detection % of the Logistic regression model i.e. 94% (77 of 82). We also managed to decrease the number of Non frauds misclassified as Frauds. (1432 of 36943)","a0ac9677":"Using Sci-kit Learn's Standard Scaler","0391c8d1":"**Results**","f106b7df":"Hence, Support Vector Classifier does great and is able to predict 95% (78 out of 82) Frauds correctly at a cost of 2% (893 out of 36943) of the Non Frauds being predicted as Frauds.","55ec91b2":"Hence, with KNN Classifier we are able to predict 93% (76 out of 82) Frauds correctly at a cost of 2% (766 out of 36943) of the Non Frauds being predicted as Frauds.","e69d9676":"Stochastic Gradient Descent Classifier detects 94% of the fraud cases but at a cost of 9% of Non Frauds being predicted as Fraud.","6a5c8398":"**Grid Search CV on LightGBM**","08d54675":"### Sampling to deal with class imbalance","46d59396":"Train test split. Train set can be used for sampling purposes. The test set is kept aside for \"real world\" testing.","d18e38c1":"Hence, with Logistic Regression we are able to predict 94% (77 out of 82) Frauds correctly at a cost of 5% (1749 out of 36943) of the Non Frauds being predicted as Frauds.","c8b88111":"**Logistic Regression**","42e5cc25":"Plotting the correlation heatmap shows that the independent features donot possess significant inter feature correlation. Hence, all of them can be a feature in our model as they are independent of each other. The pearson correlation indices tell us that the independent variables donot show significant linear relationship with the target variable. Note that the correlation coefficient is a way to represent linear relationships. However, the relationship may be of a higher degree (e.g. tertiary, quadratic and so on) which is not depicted by the correlation coefficient. To visualize such relations, more univariate and multivariate plots need to be checked. (I prefer doing this using Tableau)","8d48b3a6":"**Thanks for the Read!**","7127b811":"So, the average fraud prediction on entire data is 86.58% on 10 splits of the data which is a good overall result.","1ece0d46":"### Feature Scaling","beb1a003":"Decision Tree classifier detects 90% of the frauds. We can do better by using the ensemble tree based approaches.","0e195bea":"There are no missing values in our data:","055b5798":"Let's see if KFold CV can help us run more estimators without overfitting on the train set and evaluate the predictions on the test set! <br>\n**Note:** KFold CV is more of an evaluation method to check if our classifier doesnot overfit on the training set and has good overall prediction on different testing samples. A KFold CV operation should be done on the parent data and not on the sampled set. The sampling should be done within the CV after attaining the train test split for each fold. Using sampled data for KFold splits in the Cross validation will not suffice the \"real world\" testing. <br>\n**Note 2:** For sampling the data splits during each Fold of the process, use the same sampling method used before training phase of the model. In this case it is Near Miss + Smote","0948aa1b":"# Credit Card Fraud Detection System","6bfd4d02":"The features are now standardised and fit for modelling as all of them have mean almost equal to zero and standard deviation nearly equal to 1:","4ebfc050":"### Loading Data","f263e7b4":"Voting Classifier combines predictions on the data set from two different classifiers and votes on the best prediction. Voting Classifier is a good means to combine predictions from two similar kind of classifiers and bring out the best parts of both the classifiers and combine them. We can see that KNN Classifier predicted 93% frauds compared to the 94% predicted by Logistic Regression. However, KNN predicted lesser no. of Non- Frauds as Frauds as compared to Logistic. (KNN predicted 766 non frauds as frauds compared to 1749 in case of LR.) So we will try and combine LR and KNN to see if we can get a better combination of results.","a70937bc":"**Bagging Classifier using Decision Tree**","3d92232e":"Now lets Redo 10 Fold CV again with the new params, and see if the prediction rate improves..","bda67d40":"By using the Decision tree in a bagging classifier, the performance has improved significantly. The fraud prediction rate is now 94% compared to 90% achieved with decision trees classifier.","7f1c6b29":"### Data Segmentation","a4863135":"**SVM Classifier**","ac60c817":"**Evaluating LightGBM with K Fold CV**","c66992b7":"SGD Classifier comes with set of hyperparameters that can be set accordingly to train a model similar to the working linear models like Logistic regression or SVMs while minimizing the loss using Stochastic Gradient Descent. SGD Classifier models do good on large sized data.","a36bcc2c":"A Bagging Classifier can be used to build an ensemble of a number of similar model. Here we will build an ensemble by bagging 1000 Decision Tree Classifiers that we built earlier. RandomForest is an example of pre- available Bagging Tree classifiers present in Scikit Learn.","c5de10c2":"**Decision Tree Classifier**","4423c1b6":"Credit Card Fraud is a common form of theft in the finance sector. Unauthorized and suspicious transaction on bank statements have been an rising issue and these frauds constitute a significant part of the fiscal loss in the banking sector. Detecting a credit card fraud in real time is an uphill task primarily because of the fact that fraud transactions are very few in amount in comparison to the normal transactions. This Notebook follows all basic steps of a Machine Learning Classification lifecycle, and can be used as reference for the few steps involved in construction of a ML pipeline: <br>\n1. Loading Data\n2. Exploratory Data Analysis: Correlations, Distributions, Outlier detection\n3. Feature Scaling: Standard Scaler\n4. Data Segmentation\n5. Safe Sampling to mitigate class imbalance: Near Miss and SMOTE\n6. Model Training: using KNN, Logistic Regression, SVM, Logistic-KNN Ensemble with Hard Voting and Soft Voting, Tree Based methods, Bagging Classifier, SGD Classifier and LightGBM\n7. Predictions and Evaluation\n8. Cross Validation: Grid Search CV and KFold CV\n9. Model Explanation\n\n**Objective:** Detecting as many frauds possible and at same time minimizing the number of Non- Frauds categorized as Frauds in the process. <br>\n**Challenges:** Class Imbalance, No description available about the features <br>\n\nFor subsequent steps after model training that will help us in deployment, access notebook at: https:\/\/github.com\/nayaksubhankar\/CreditCardFraudDetection <br>\n--> Forming a sklearn pipeline <br>\n--> Serializing the pipeline and dumping it using Joblib <br>\n--> Inference Testing","8ab7bb5e":"1. Undersampling the majority class using Near Miss strategy to get the samples from majority class that are nearest to the minority class records in terms of distance. The overall idea is to reduce information loss with undersampling. If our classifier trains on the samples closest to the decision boundary then it will be able to classify points that are farther apart.\n2. Oversampling the minority class using SMOTE strategy that is common over sampling used in many class imbalance problems.\n3. Aim is to sample to an extent without introducing many systhetic samples of the minority class nor losing out on significant majority class samples. The remaining imbalance can be handled by using class weights in our model parameters when and where required.","33cf55af":"Let's use seaborn distplot to see the distribution of each feature with respect to the target variable 'Class'. The distplots give us a visual presentation of how a particular feature's values can tell apart among Fraud and Non-fraud cases. The plots showing a clear divide can be considered as significant features that may have good impact on our target variable, which we will cross validate with other measures as well.","75237b97":"Describing the dataframe, we can see that all features are numeric but are not standardised. Hence we will standardise them using a Scaler in further steps.","c667151c":"We can perform a Grid Search to get the best parameter list. We can use 'recall' metric for scoring as our model should be more recall oriented.","7f08f787":"### Model Training and Evaluation \nThe motive of our classifier will be to predict maximum of the fraud transactions as fraud at the cost of some non fraud transactions being predicted as fraud. But we must not lose out on the fraud transactions! Hence we will aim for High recall on the frauds (that is the percentage of frauds correctly classified as frauds) but a low precision on the frauds is manageable. Thus, a good overall ROC Score and good recall score for the minority class will be apt.","f6a53fb9":"The Hard voting classifier using LR and KNN gets 91% of Fraud predictions right at a reduced cost of just 2% (660 of 36943) of Non frauds getting classified as fraud. It reduces the no. of Non Frauds misclassified, but also hinders the fraud detection %. So, we will change the voting mechanism to Soft voting and provide weights to give more weights to probability calculated by the LR model:"}}