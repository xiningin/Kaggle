{"cell_type":{"bc4fc468":"code","1dab8b9c":"code","160a46f0":"code","02373ec0":"code","dd3105c8":"code","2c857257":"code","9d227c3a":"code","f95ce87a":"code","5d6976d1":"code","eefd8b9b":"code","29352906":"code","526e159c":"code","892b8770":"code","047ac662":"code","3b5ed75b":"code","2ad11f2d":"code","e86f5c4e":"code","e01f344f":"code","c05577a8":"code","9b0539c8":"code","19a26f2e":"code","af070606":"code","c5d8b548":"code","bd05a3f3":"code","65d3fa53":"code","25fcee72":"code","99d84cba":"code","8c0daa5d":"code","52574979":"markdown","4b29d8cb":"markdown","f99e2fbf":"markdown","d3ae8f60":"markdown","70bd82aa":"markdown"},"source":{"bc4fc468":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1dab8b9c":"import torch\nimport numpy as np\nimport torchvision\nimport matplotlib.pyplot as plt\nimport time\nimport copy\nimport cv2\n","160a46f0":"import zipfile\nwith zipfile.ZipFile('..\/input\/platesv2\/plates.zip', 'r') as zip_obj:\n   zip_obj.extractall('\/kaggle\/working\/')\n    \nprint('After zip extraction:')\nprint(os.listdir(\"\/kaggle\/working\/\"))\ndata_root = '\/kaggle\/working\/plates\/'\nprint(os.listdir(data_root))","02373ec0":"path = data_root+'train\/dirty\/'+'0018.jpg'\n\nimage = cv2.imread(path)\noutput = image.copy()\ngray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\ngray = cv2.medianBlur(gray, 5)\n\n# \u0438\u0449\u0435\u043c \u043e\u043a\u0440\u0443\u0436\u043d\u043e\u0441\u0442\u044c!\ncircles = cv2.HoughCircles(gray, cv2.HOUGH_GRADIENT, 1, minDist=300,\n                               param1=100, param2=30,\n                               minRadius=1, maxRadius=500)\n\nif circles is not None:\n    circles = np.round(circles[0, :]).astype(\"int\")\n    for (x, y, r) in circles:\n        #\u0440\u0438\u0441\u0443\u0435\u043c \u043e\u043a\u0440\u0443\u0436\u043d\u043e\u0441\u0442\u044c       \n        cv2.circle(output, (x, y), r, (1, 255, 1), 4)\n        #\u0430 \u0442\u0435\u043f\u0435\u0440\u044c \u043e\u043f\u0438\u0441\u044b\u0432\u0430\u0435\u043c \u0432\u043e\u043a\u0440\u0443\u0433 \u043d\u0435\u0451 \u043a\u0432\u0430\u0434\u0440\u0430\u0442\n        cv2.rectangle(output,(x - r, y - r), (x + r, y + r), (0,255,0),3)   \n    crop_image = image[(y-r):(y+r), (x-r):(x+r)]\n    plt.imshow(np.hstack([image, output]))\n","dd3105c8":"plt.imshow(crop_image)","2c857257":"class ExtractPlate:\n    \n    def __init__(self,img):\n        self.img = img\n\n    def FindAndCrop(self):\n        image = cv2.convertScaleAbs(self.img, alpha=1.2, beta=0.0)\n        #output = self.image.copy()\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        gray = cv2.medianBlur(gray, 5)\n\n\n        circles = cv2.HoughCircles(gray, cv2.HOUGH_GRADIENT, 1, minDist=300,\n                               param1=100, param2=30,\n                               minRadius=1, maxRadius=500)\n        if circles is not None:\n\n            circles = np.round(circles[0, :]).astype(\"int\")\n            for (x, y, r) in circles:\n                crop_image = image[(y-r):(y + r), (x-r):(x + r)]\n                #\u0437\u0434\u0435\u0441\u044c \u0438 \u0434\u0430\u043b\u0435\u0435 \u0438\u0434\u0443\u0442 \u0443\u0441\u043b\u043e\u0432\u0438\u044f \u0434\u043b\u044f \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0438 \u043d\u0430\u043b\u0438\u0447\u0438\u044f \u043f\u0440\u044f\u043c\u043e\u0443\u0433\u043e\u043b\u044c\u043d\u0438\u043a\u0430\n                if crop_image.size == 0: \n                    cv2.imwrite(image_folder[:-4] + '.jpg',image)\n                    print('\u0418 \u0442\u0430\u043a \u0441\u043e\u0439\u0434\u0451\u0442!')\n                else:\n                    #\u0433\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u044f \u043f\u0440\u044f\u043c\u043e\u0443\u0433\u043e\u043b\u044c\u043d\u0438\u043a\u0430, \u0432 \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0432\u043f\u0438\u0441\u0430\u043d\u0430 \u043e\u043a\u0440\u0443\u0436\u043d\u043e\u0441\u0442\u044c \u0442\u0430\u0440\u0435\u043b\u043a\u0438\n                    crop_image = image[(y-r):(y + r), (x-r):(x + r)] \n                    cv2.imwrite(image_folder[:-4] + 'crop0'+'.jpg',crop_image)\n                    \n                #\u0434\u0430\u043b\u0435\u0435 \u043f\u0440\u043e\u0438\u0441\u0445\u043e\u0434\u0438\u0442 \u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043f\u0440\u044f\u043c\u043e\u0443\u0433\u043e\u043b\u044c\u043d\u0438\u043a\u0430 \u043d\u0430 \u0431\u043e\u043b\u0435\u0435 \u043c\u0435\u043b\u043a\u0438\u0435 \u0447\u0430\u0441\u0442\u0438\n                crop_image = image[y:(y + r), x:(x + r)]\n                if crop_image.size == 0: \n                    print('\u0418 \u0442\u0430\u043a \u0441\u043e\u0439\u0434\u0451\u0442!')\n                else:\n                    cv2.imwrite(image_folder[:-4] + 'crop1'+'.jpg',crop_image)\n                crop_image = image[(y-r):(y), (x-r):(x)]\n                if crop_image.size == 0: \n                    print('\u0418 \u0442\u0430\u043a \u0441\u043e\u0439\u0434\u0451\u0442!')\n                else:\n                    cv2.imwrite(image_folder[:-4] + 'crop2'+'.jpg',crop_image)\n                crop_image = image[y:(y + r), (x - r):(x)]\n                if crop_image.size == 0: \n                    print('\u0418 \u0442\u0430\u043a \u0441\u043e\u0439\u0434\u0451\u0442!')\n                else:\n                    cv2.imwrite(image_folder[:-4] + 'crop3'+'.jpg',crop_image)\n                crop_image = image[(y - r):(y), (x):(x + r)]\n                if crop_image.size == 0: \n                    print('\u0418 \u0442\u0430\u043a \u0441\u043e\u0439\u0434\u0451\u0442!')\n                else:\n                    cv2.imwrite(image_folder[:-4] + 'crop4'+'.jpg',crop_image)\n                \n                \n        \nfor image_index in range (20):\n    print (\"Complete dirty: \",\"{0:04}\".format(image_index),\"\/0021\", end=\"\\r\")\n    image_folder = '\/kaggle\/working\/plates\/train\/dirty\/{0:04}.jpg'.format(image_index) \n    img = cv2.imread(image_folder)\n    out_img  = ExtractPlate(img)\n    out_img.FindAndCrop()\n    \nprint (\"\\n\\r\", end=\"\")    \n    \nfor image_index in range (20):\n    print (\"Complete cleaned: \",\"{0:04}\".format(image_index),\"\/0021\", end=\"\\r\")\n    image_folder = '\/kaggle\/working\/plates\/train\/cleaned\/{0:04}.jpg'.format(image_index) \n    img = cv2.imread(image_folder)\n    out_img  = ExtractPlate(img)\n    out_img.FindAndCrop()\n\nprint (\"\\n\\r\", end=\"\")\n\nfor image_index in range (744):\n    print (\"Complete test: \",\"{0:04}\".format(image_index),\"\/745\", end=\"\\r\")\n    image_folder = '\/kaggle\/working\/plates\/test\/{0:04}.jpg'.format(image_index) \n    img = cv2.imread(image_folder)\n    out_img  = ExtractPlate(img)\n    out_img.FindAndCrop()\n\nprint (\"\\n\\r\", end=\"\") ","9d227c3a":"import shutil \nfrom tqdm import tqdm\n\n#\u0432\u044b\u0431\u0438\u0440\u0430\u0435\u043c \u0438\u0437 \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u0445 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435\ntrain_dir = 'train' \nval_dir = 'val' \n \nclass_names = ['cleaned', 'dirty']\n \nfor dir_name in [train_dir, val_dir]:\n    for class_name in class_names:\n        os.makedirs(os.path.join(dir_name, class_name), exist_ok=True)\n \nfor class_name in class_names:\n    source_dir = os.path.join(data_root, 'train', class_name)\n    for i, file_name in enumerate(tqdm(os.listdir(source_dir))):\n        if i % 6 != 0:\n            dest_dir = os.path.join(train_dir, class_name) \n        else:\n            dest_dir = os.path.join(val_dir, class_name)\n        shutil.copy(os.path.join(source_dir, file_name), os.path.join(dest_dir, file_name))","f95ce87a":"from torchvision import transforms, models\ntrain_transforms = transforms.Compose([   \n    transforms.RandomPerspective(distortion_scale=0.1, p=0.8, interpolation=3, fill=255),\n    transforms.ColorJitter(hue=(-0.5,0.5), saturation=(0.6,1), brightness=(0.5,1), contrast=(0.7, 1)),\n    #transforms.CenterCrop(100),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomVerticalFlip(),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\nval_transforms = transforms.Compose([\n    #transforms.CenterCrop(200),\n    transforms.RandomPerspective(distortion_scale=0.1, p=0.8, interpolation=3, fill=255),\n    transforms.Resize((224, 224)),    \n    transforms.ColorJitter(hue=(-0.5,0.5), saturation=(0.6,1), brightness=(0.5,1), contrast=(0.7, 1)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomVerticalFlip(),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\ntest_transforms = transforms.Compose([\n    #transforms.CenterCrop(200),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\ntrain_dataset = torchvision.datasets.ImageFolder(train_dir, train_transforms)\nval_dataset = torchvision.datasets.ImageFolder(val_dir, val_transforms)\n\nbatch_size = 9 \ntrain_dataloader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=batch_size, shuffle=True, num_workers=batch_size)\nval_dataloader = torch.utils.data.DataLoader(\n    val_dataset, batch_size=batch_size, shuffle=False, num_workers=batch_size)","5d6976d1":"len(train_dataloader), len(train_dataset)","eefd8b9b":"def show_input(input_tensor, title=''):\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    image = input_tensor.permute(1, 2, 0).numpy()\n    image = std * image + mean\n    plt.imshow(image.clip(0, 1))\n    plt.title(title)\n    plt.show()\n    plt.pause(0.001)\n\nX_batch, y_batch = next(iter(train_dataloader))\n\nfor x_item, y_item in zip(X_batch, y_batch):\n    show_input(x_item, title=class_names[y_item])","29352906":"import random\nrandom.seed(2)\nnp.random.seed(2)\ntorch.manual_seed(2)\ntorch.cuda.manual_seed(2)\ntorch.backends.cudnn.deterministic = True\n\n#\u0432\u043d\u043e\u0441\u0438\u043c \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u0443\u044e \u043d\u0435\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0451\u043d\u043d\u043e\u0441\u0442\u044c \u0432 \u0440\u0430\u0431\u043e\u0442\u0443","526e159c":"def train_model(model, loss, optimizer, scheduler, num_epochs):\n    loss_history = {'train':[], 'val':[]}\n    acc_history = {'train':[], 'val':[]}\n    \n    for epoch in range(num_epochs):\n        \n        print('Epoch {}\/{}:'.format(epoch, num_epochs - 1), flush=True)\n\n\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                dataloader = train_dataloader\n                scheduler.step()\n                model.train() \n            else:\n                dataloader = val_dataloader\n                model.eval() \n\n            running_loss = 0.\n            running_acc = 0.\n\n\n            for inputs, labels in tqdm(dataloader):\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                optimizer.zero_grad()\n\n    \n                with torch.set_grad_enabled(phase == 'train'):\n                    preds = model(inputs)\n                    loss_value = loss(preds, labels)\n                    preds_class = preds.argmax(dim=1)\n\n                    if phase == 'train':\n                        loss_value.backward()\n                        optimizer.step()\n\n\n\n                running_loss += loss_value.item()\n                running_acc += (preds_class == labels.data).float().mean().data.cpu().numpy()\n            epoch_loss = running_loss \/ len(dataloader)\n            epoch_acc = running_acc \/ len(dataloader)\n            loss_history[phase].append(epoch_loss)\n            acc_history[phase].append(epoch_acc)\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc), flush=True)\n\n    return model, loss_history, acc_history","892b8770":"model = models.resnet152(pretrained=True)\n\n#\u043d\u0435 \u0437\u0430\u0431\u044b\u0432\u0430\u0435\u043c \u0437\u0430\u043c\u043e\u0440\u043e\u0437\u0438\u0442\u044c \u043d\u0438\u0436\u043d\u0438\u0435 \u0441\u043b\u043e\u0438 \u043d\u0435\u0439\u0440\u043e\u0441\u0435\u0442\u0438\nfor param in model.parameters():\n    param.requires_grad = False\n\nmodel.fc = torch.nn.Linear(model.fc.in_features, 2)\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\nloss = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n\n#\u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0434\u043b\u044f lr \u0438 gamma \u0434\u043e\u043b\u0436\u043d\u044b \u0431\u044b\u0442\u044c \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u043c\u0430\u043b\u0435\u043d\u044c\u043a\u0438\u043c\u0438, \u043a\u0430\u043a \u043f\u043e\u043a\u0430\u0437\u0430\u043b\u0430 \u043f\u0440\u0430\u043a\u0442\u0438\u043a\u0430\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.01)","047ac662":"model, loss_history, acc_history=train_model(model, loss, optimizer, scheduler, num_epochs=30);","3b5ed75b":"plt.rcParams['figure.figsize'] = (14, 7)\nfor experiment_id in acc_history.keys():\n    plt.plot(acc_history[experiment_id], label=experiment_id)\nplt.legend(loc='upper left')\nplt.title('Model Accuracy')\nplt.xlabel('Epoch num', fontsize=15)\nplt.ylabel('Accuracy value', fontsize=15);\nplt.grid(linestyle='--', linewidth=0.5, color='.7')","2ad11f2d":"plt.rcParams['figure.figsize'] = (14, 7)\nfor experiment_id in loss_history.keys():\n    plt.plot(loss_history[experiment_id], label=experiment_id)\nplt.legend(loc='upper left')\nplt.title('Model Loss')\nplt.xlabel('Epoch num', fontsize=15)\nplt.ylabel('Loss value', fontsize=15);\nplt.grid(linestyle='--', linewidth=0.5, color='.7')","e86f5c4e":"test_dir = 'test'\nshutil.copytree(os.path.join(data_root, 'test'), os.path.join(test_dir, 'unknown'))","e01f344f":"class ImageFolderWithPaths(torchvision.datasets.ImageFolder):\n    def __getitem__(self, index):\n        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n        path = self.imgs[index][0]\n        tuple_with_path = (original_tuple + (path,))\n        return tuple_with_path\n    \ntest_dataset = ImageFolderWithPaths('\/kaggle\/working\/test\/', test_transforms)\n\ntest_dataloader = torch.utils.data.DataLoader(\n    test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)","c05577a8":"test_dataset","9b0539c8":"model.eval()\n\ntest_predictions = []\ntest_img_paths = []\nfor inputs, labels, paths in tqdm(test_dataloader):\n    inputs = inputs.to(device)\n    labels = labels.to(device)\n    with torch.set_grad_enabled(False):\n        preds = model(inputs)\n    test_predictions.append(\n        torch.nn.functional.softmax(preds, dim=1)[:,1].data.cpu().numpy())\n    test_img_paths.extend(paths)\n    \ntest_predictions = np.concatenate(test_predictions)","19a26f2e":"inputs, labels, paths = next(iter(test_dataloader))\n\nfor img, pred in zip(inputs, test_predictions):\n    show_input(img, title=pred)","af070606":"res = {'file_name':test_img_paths,\n      'label':test_predictions}\nresult_df = pd.DataFrame(data=res)\nresult_df['label']=result_df['label'].astype(float)\n\ncrop0_df = result_df.loc[result_df['file_name'].str.contains('crop0', regex=False)]\ncrop1_df = result_df.loc[result_df['file_name'].str.contains('crop1', regex=False)]\ncrop2_df = result_df.loc[result_df['file_name'].str.contains('crop2', regex=False)]\ncrop3_df = result_df.loc[result_df['file_name'].str.contains('crop3', regex=False)]\ncrop4_df = result_df.loc[result_df['file_name'].str.contains('crop4', regex=False)]\n\nresult_df = result_df.loc[~result_df['file_name'].str.contains('crop', regex=False)]\n\nnames =result_df['file_name'].str.extract('(\\d+)')\nresult_df['names']=names\n\nresult_df['label_crop0']=np.nan\nfor name in result_df['names']:\n        crop_label=crop0_df.loc[crop0_df['file_name'].str.contains(name, regex=False), 'label']\n        if crop_label.size!=0:\n            result_df.loc[result_df['names']==name, 'label_crop0'] = crop_label.values[0]\nresult_df['label_crop1']=np.nan\nfor name in result_df['names']:\n        crop_label=crop1_df.loc[crop1_df['file_name'].str.contains(name, regex=False), 'label']\n        if crop_label.size!=0:\n            result_df.loc[result_df['names']==name, 'label_crop1'] = crop_label.values[0]\nresult_df['label_crop2']=np.nan\nfor name in result_df['names']:\n        crop_label=crop2_df.loc[crop2_df['file_name'].str.contains(name, regex=False), 'label']\n        if crop_label.size!=0:\n            result_df.loc[result_df['names']==name, 'label_crop2'] = crop_label.values[0]\nresult_df['label_crop3']=np.nan\nfor name in result_df['names']:\n        crop_label=crop3_df.loc[crop3_df['file_name'].str.contains(name, regex=False), 'label']\n        if crop_label.size!=0:\n            result_df.loc[result_df['names']==name, 'label_crop3'] = crop_label.values[0]\nresult_df['label_crop4']=np.nan\nfor name in result_df['names']:\n        crop_label=crop4_df.loc[crop4_df['file_name'].str.contains(name, regex=False), 'label']\n        if crop_label.size!=0:\n            result_df.loc[result_df['names']==name, 'label_crop4'] = crop_label.values[0]\n            \nresult_df['label_mean']=result_df[['label_crop4', 'label_crop2', 'label_crop3', 'label_crop1', 'label_crop0']].mean(axis=1)","c5d8b548":"result_df","bd05a3f3":"result=result_df.drop(['file_name', 'label', 'label_crop4', 'label_crop2', 'label_crop3', 'label_crop1', 'label_crop0'], axis=1)\nresult=result.rename(columns={\"names\": \"id\", \"label_mean\": \"label\"})\nresult['label'] = result['label'].map(lambda pred: 'dirty' if pred >= 0.50 else 'cleaned')","65d3fa53":"result['label'].hist()","25fcee72":"result=result.reset_index()\nsub_df = pd.read_csv('..\/input\/platesv2\/sample_submission.csv')\nsub_df['label'] = result['label']\n\nsub_df.head(n=10)","99d84cba":"sub_df.info()","8c0daa5d":"sub_df=sub_df.set_index('id')\nsub_df.to_csv('platewithcrop_pytorchresnet152.csv')","52574979":"\u044d\u0442\u043e\u0442 \u043d\u043e\u0443\u0442\u0431\u0443\u043a \u044f \u0434\u043e\u0432\u0435\u043b\u0430 \u0434\u043e \u0431\u043e\u043b\u0435\u0435-\u043c\u0435\u043d\u0435\u0435 \u043f\u0440\u0438\u043b\u0438\u0447\u043d\u043e\u0433\u043e \u0441\u043a\u043e\u0440\u0430, \u0431\u043b\u0430\u0433\u043e\u0434\u0430\u0440\u044f \u043a\u0430\u043a\u043e\u0439-\u0442\u043e \u043c\u0438\u0441\u0442\u0438\u043a\u0435 \u0438 \u0441\u043e\u0431\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0439 \u043f\u043e\u0434\u043b\u043e\u0441\u0442\u0438, \u043f\u043e\u0442\u043e\u043c\u0443 \u0447\u0442\u043e \u0432\u0441\u0435 \u0430\u043d\u0430\u043b\u043e\u0433\u0438\u0447\u043d\u044b\u0435 \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u0438 \u0432 keras, \u0432\u043a\u043b\u044e\u0447\u0430\u044f \u0442\u0440\u044e\u043a \u0432 opencv, \u043a\u0430\u043a\u0438\u043c\u0438 \u0431\u044b \u0445\u043e\u0440\u043e\u0448\u0438\u043c\u0438 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438 \u043d\u0435 \u0431\u044b\u043b\u0438 (\u0430 \u043e\u043d\u0438 \u0431\u044b\u043b\u0438 \u043e\u0447\u0435\u043d\u044c \u0445\u043e\u0440\u043e\u0448\u0438\u043c\u0438, \u043d\u043e \u043f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u0442\u044c \u044f \u0438\u0445 \u043d\u0435 \u0431\u0443\u0434\u0443), \u0441\u043a\u043e\u0440 \u0431\u044b\u043b \u043d\u0438\u0436\u0435, \u0447\u0435\u043c \u0435\u0441\u043b\u0438 \u0431\u044b \u044f \u043f\u0440\u043e\u0441\u0442\u043e \u0441\u043a\u043e\u0440\u043c\u0438\u043b\u0430 \u043a\u044d\u0433\u0433\u043b\u0443 \u043f\u0440\u0438\u043c\u0435\u0440 \u0441\u0430\u0431\u043c\u0438\u0442\u0430, \u0433\u0434\u0435 \u0432\u0441\u044f \u043f\u043e\u0441\u0443\u0434\u0430 \u0431\u044b\u043b\u0430 \u0433\u0440\u044f\u0437\u043d\u043e\u0439, \u043a\u0430\u043a \u0432 \u043c\u043e\u0435\u0439 \u043e\u0431\u0449\u0430\u0433\u0435\n\n\u044f \u0438\u0441\u043a\u0440\u0435\u043d\u043d\u0435 \u043d\u0430\u0434\u0435\u044e\u0441\u044c, \u0447\u0442\u043e \u043a\u0442\u043e-\u0442\u043e \u0441\u043e \u0441\u0442\u0435\u043f\u0438\u043a\u0430 \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u043e \u0437\u0430\u0439\u0434\u0451\u0442 \u0441\u044e\u0434\u0430 \u0438 \u043f\u0440\u043e\u0440\u043e\u043d\u0435\u0442 \u043f\u0430\u0440\u0443 \u0441\u043b\u0451\u0437 \u043f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u044f\n","4b29d8cb":"\u043c\u043d\u0435 \u043e\u0447\u0435\u043d\u044c \u0441\u0438\u043b\u044c\u043d\u043e \u043f\u043e\u043d\u0440\u0430\u0432\u0438\u043b\u0430\u0441\u044c \u0438\u0434\u0435\u044f \u0441 \u043e\u0447\u0438\u0449\u0435\u043d\u0438\u0435\u043c \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a \u043e\u0442 \u0437\u0430\u0434\u043d\u0435\u0433\u043e \u0444\u043e\u043d\u0430 \u0438 \u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u0445 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439 \u043d\u0430 4 \u0447\u0430\u0441\u0442\u0438, \u043d\u043e \u044f \u0447\u0435\u0441\u0442\u043d\u043e \u0440\u0435\u0448\u0438\u043b\u0430 \u0435\u0433\u043e \u043d\u0435 \u0431\u0440\u0430\u0442\u044c, \u043f\u043e\u0442\u043e\u043c\u0443 \u0447\u0442\u043e \u044d\u0442\u043e \u043d\u0435 \u0431\u0443\u0434\u0435\u0442 \u043c\u043e\u0438\u043c \u0440\u0435\u0448\u0435\u043d\u0438\u0435\u043c \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u044b, \u043e\u0434\u043d\u0430\u043a\u043e \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438 \u043a\u0430\u043a-\u0442\u043e \u043d\u0443\u0436\u043d\u043e \u0438\u0437\u0431\u0430\u0432\u043b\u044f\u0442\u044c \u043e\u0442 \u043b\u0438\u0448\u043d\u0435\u0433\u043e. ","f99e2fbf":"# \u041d\u0410 \u0421\u0410\u041c\u041e\u041c \u0414\u0415\u041b\u0415\n\n\u044d\u0442\u043e\u0442 \u043d\u043e\u0443\u0442\u0431\u0443\u043a \u043e\u043f\u0443\u0431\u043b\u0438\u043a\u043e\u0432\u0430\u043d \u0434\u043b\u044f \u0442\u043e\u0433\u043e, \u0447\u0442\u043e\u0431\u044b \u0432\u044b \u0443\u0437\u043d\u0430\u043b\u0438 \u0441\u043a\u0430\u0437\u043a\u0443 \u043f\u0440\u043e \u041b\u0435\u043d\u0438\u043d\u0430 \u043f\u043e\u0434 \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u0435\u043c \"\u041e\u0431\u0449\u0435\u0441\u0442\u0432\u043e \u0447\u0438\u0441\u0442\u044b\u0445 \u0442\u0430\u0440\u0435\u043b\u043e\u043a\". \u0441\u0443\u0442\u044c \u0438\u0441\u0442\u043e\u0440\u0438\u0438 \u043f\u0440\u043e\u0441\u0442\u0430: \u041b\u0435\u043d\u0438\u043d \u043f\u043e\u0447\u0435\u043c\u0443-\u0442\u043e \u0437\u0430\u0432\u0442\u0440\u0430\u043a\u0430\u043b \u0441 \u0434\u0435\u0442\u044c\u043c\u0438, \u0443\u0436\u0435 \u0431\u0443\u0434\u0443\u0447\u0438 \u043e\u0442\u0446\u043e\u043c \u0440\u0435\u0432\u043e\u043b\u044e\u0446\u0438\u0438, \u0438 \u043e\u0434\u0438\u043d \u0438\u0437 \u0434\u0435\u0442\u0435\u0439 \u043d\u0435 \u0445\u043e\u0442\u0435\u043b \u0434\u043e\u0435\u0434\u0430\u0442\u044c \u043a\u0430\u0448\u0443, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u041b\u0435\u043d\u0438\u043d \u0431\u044b\u0441\u0442\u0440\u043e \u043e\u0440\u0433\u0430\u043d\u0438\u0437\u043e\u0432\u0430\u043b \u041e\u0431\u0449\u0435\u0441\u0442\u0432\u043e \u0447\u0438\u0441\u0442\u044b\u0445 \u0442\u0430\u0440\u0435\u043b\u043e\u043a, \u043a\u0443\u0434\u0430 \u0432\u0441\u0442\u0443\u043f\u0438\u043b\u0438 \u0432\u0441\u0435 \u0434\u0435\u0442\u0438, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u0442\u043e\u0442 \u0440\u0435\u0431\u0451\u043d\u043e\u043a \u0431\u044b\u043b \u0432\u044b\u043d\u0443\u0436\u0434\u0435\u043d \u043e\u0442\u0441\u0442\u0443\u043f\u0438\u0442\u044c \u043f\u0435\u0440\u0435\u0434 \u043e\u0431\u0449\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u043c \u0434\u0430\u0432\u043b\u0435\u043d\u0438\u0435\u043c \u0438 \u0432\u0441\u0451-\u0442\u0430\u043a\u0438 \u0434\u043e\u0435\u0441\u0442\u044c \u043a\u0430\u0448\u0443, \u0447\u0442\u043e\u0431\u044b \u043f\u043e\u043f\u0430\u0441\u0442\u044c \u0432 \u044d\u0442\u043e \u043e\u0431\u0449\u0435\u0441\u0442\u0432\u043e\n\n\u043a\u0430\u043a \u0432\u0438\u0434\u0438\u0442\u0435, \u043f\u043e\u0441\u043b\u0435 \u0447\u0442\u0435\u043d\u0438\u044f \u0443\u0436\u0430\u0441\u0430\u044e\u0449\u0438\u0445 \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438\u0435\u0432 \u043e \u043a\u0430\u043a\u0438\u0445-\u0442\u043e \u0430\u043d\u0441\u0430\u043c\u0431\u043b\u044f\u0445 \u043f\u0435\u0441\u0435\u043d \u0438 \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u044b\u0445 \u043f\u043b\u044f\u0441\u043e\u043a, \u044f \u0442\u043e\u0436\u0435 \u0431\u044b\u043b\u0430 \u0432\u044b\u043d\u0443\u0436\u0434\u0435\u043d\u0430 \u043f\u0440\u043e\u0433\u043d\u0443\u0442\u044c\u0441\u044f \u0438 \u0434\u043e\u0432\u0435\u0441\u0442\u0438 \u0440\u0430\u0431\u043e\u0442\u0443 \u0434\u043e \u043a\u0430\u043a\u043e\u0433\u043e-\u0442\u043e \u043b\u043e\u0433\u0438\u0447\u0435\u0441\u043a\u043e\u0433\u043e \u0437\u0430\u0432\u0435\u0440\u0448\u0435\u043d\u0438\u044f, \u043d\u043e \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0442\u043e\u0440\u044b \u043f\u0440\u0430\u0432\u044b: \u0437\u0434\u0435\u0441\u044c \u0440\u0435\u0448\u0430\u0435\u0442 \u043c\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0440\u0430\u043d\u0434\u043e\u043c\n","d3ae8f60":"\u0437\u0434\u0435\u0441\u044c \u0438 \u0434\u0430\u043b\u0435\u0435 \u0441\u043b\u0435\u0434\u0443\u0435\u0442 \u0431\u043e\u043b\u044c\u0448\u043e\u0439 \u043a\u043e\u0441\u0442\u044b\u043b\u044c \u0441\u043e\u0431\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0433\u043e \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0441\u0442\u0432\u0430. \u043f\u0440\u0438\u0447\u0438\u043d\u0430 \u043f\u043e\u044f\u0432\u043b\u0435\u043d\u0438\u044f \u043a\u043e\u0441\u0442\u044b\u043b\u044f - \u043c\u043e\u044f \u043f\u0430\u0440\u0430\u043d\u043e\u0439\u044f \u0438 \u043d\u0435\u0434\u043e\u0432\u0435\u0440\u0438\u0435 \u043a \u0441\u043e\u0431\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u043c\u0443 \u043a\u043e\u0434\u0443, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u043a\u043e\u0441\u0442\u044b\u043b\u044c \u0443\u0447\u0438\u0442\u044b\u0432\u0430\u0435\u0442 \u0432\u0441\u0435 \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u044b\u0435 \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u044b\n\u044d\u0442\u043e \u0441\u0432\u044f\u0437\u0430\u043d\u043e \u0441 \u0442\u0435\u043c, \u0447\u0442\u043e \u043d\u0435 \u0432\u0441\u0435 \u043e\u0440\u0438\u0433\u0438\u043d\u0430\u043b\u044c\u043d\u044b\u0435 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0438\u043c\u0435\u044e\u0442 \u043e\u0431\u0440\u0435\u0437\u0430\u043d\u043d\u044b\u0435 \u0432\u0435\u0440\u0441\u0438\u0438, \u0430 \u0435\u0441\u043b\u0438 \u0434\u0430\u0436\u0435 \u0438 \u0438\u043c\u0435\u044e\u0442, \u0442\u043e \u0438\u0445 \u043d\u0435 \u0432\u0441\u0435\u0433\u0434\u0430 5 \u0448\u0442\u0443\u043a, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u044f \u0440\u0435\u0448\u0438\u043b\u0430 \u0431\u0440\u0430\u0442\u044c \u043d\u0435 \u0445\u0438\u0442\u0440\u043e\u0441\u0442\u044c\u044e, \u0430 \u043f\u0440\u043e\u0441\u0442\u043e\u0442\u043e\u0439","70bd82aa":"# \u041a\u0410\u0417\u0410\u041b\u041e\u0421\u042c \u0411\u042b\n\u0432\u0441\u0451 \u0445\u043e\u0440\u043e\u0448\u043e, \u0432\u044b\u0440\u0435\u0437\u0430\u0435\u043c \u0442\u0430\u0440\u0435\u043b\u043a\u0438, \u043d\u043e \u0434\u0435\u043b\u043e \u0432 \u0442\u043e\u043c, \u0447\u0442\u043e \u0432 \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0445 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0430\u0445 \u0435\u0441\u0442\u044c, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u043a\u0430\u043a\u0438\u0435-\u0442\u043e \u043a\u0432\u0430\u0434\u0440\u0430\u0442\u043d\u044b\u0435 \u0438 \u043f\u0440\u044f\u043c\u043e\u0443\u0433\u043e\u043b\u044c\u043d\u044b\u0435 \u043a\u043e\u043d\u0442\u0435\u0439\u043d\u0435\u0440\u044b \u0432 \u0438\u043d\u043e\u043f\u043b\u0430\u043d\u0435\u0442\u043d\u043e\u0439 \u0436\u0438\u0436\u0435, \u0430 \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0442\u0430\u0440\u0435\u043b\u043a\u0438 \u0432 \u0438 \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u0441\u0442\u0440\u0435\u043c\u044f\u0442\u0441\u044f \u043a \u0444\u043e\u0440\u043c\u0435 \u043a\u0432\u0430\u0434\u0440\u0430\u0442\u0430. \n\n\u043d\u043e \u0438 \u0442\u0443\u0442 \u044f \u043d\u0430\u0440\u0432\u0430\u043b\u0430\u0441\u044c \u043d\u0430 \u043a\u043e\u0435-\u0447\u0442\u043e \u043d\u0435\u043f\u0440\u0438\u044f\u0442\u043d\u043e\u0435, \u043f\u043e\u0441\u043a\u043e\u043b\u044c\u043a\u0443 \u0438\u0437\u043d\u0430\u0447\u0430\u043b\u044c\u043d\u043e \u0437\u0430\u0446\u0435\u043f\u0438\u043b\u0430\u0441\u044c \u0437\u0430 \u0438\u0434\u0435\u044e \u0441 \u043e\u0431\u043d\u0430\u0440\u0443\u0436\u0435\u043d\u0438\u0435\u043c \u043a\u0440\u0443\u0433\u043e\u0432: \u043a\u043e\u0435-\u0433\u0434\u0435 \u0442\u0430\u0440\u0435\u043b\u043a\u0438 \u0440\u0430\u0441\u043f\u043e\u043b\u043e\u0436\u0435\u043d\u044b \u0442\u0430\u043a\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c, \u0447\u0442\u043e \u0432\u0438\u0434\u043d\u0430 \u0442\u043e\u043b\u044c\u043a\u043e \u043e\u0441\u043d\u043e\u0432\u043d\u0430\u044f \u0447\u0430\u0441\u0442\u044c.\n\n\u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u044f \u0440\u0435\u0448\u0438\u043b\u0430 \u0437\u0430\u043a\u0430\u0442\u0430\u0442\u044c \u044f\u0447\u0435\u0439\u043a\u0443 \u0432\u044b\u0448\u0435 \u0432 \u043a\u043b\u0430\u0441\u0441, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043f\u043e\u0437\u0432\u043e\u043b\u0438\u0442 \u0431\u0435\u0437 \u043b\u0438\u0448\u043d\u0438\u0445 \u0443\u0447\u0438\u043b\u0438\u0439 \u043d\u0430\u0445\u043e\u0434\u0438\u0442\u044c \u0442\u0430\u0440\u0435\u043b\u043a\u0443 \u0442\u0430\u043c, \u0433\u0434\u0435 \u044d\u0442\u043e \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e, \u0432\u044b\u0440\u0435\u0437\u0430\u0442\u044c \u0435\u0451 \u0438 \u0434\u0435\u043b\u0438\u0442\u044c \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u043e\u0435 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435 \u043d\u0430 4 \u0447\u0430\u0441\u0442\u0438 \u0442\u0430\u043c, \u0433\u0434\u0435 \u044d\u0442\u043e \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e.\n\n\u0432 \u043a\u043b\u0430\u0441\u0441\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u0446\u0438\u043a\u043b \u0432 \u0446\u0438\u043a\u043b\u0435 \u0443\u0441\u043b\u043e\u0432\u0438\u0435 \u0430 \u0432 \u0443\u0441\u043b\u043e\u0432\u0438\u0438 \u0438\u0433\u043b\u0430 \u0441 \u043c\u043e\u0435\u0439 \u0441\u043c\u0435\u0440\u0442\u044c\u044e"}}