{"cell_type":{"f3a963d8":"code","ffe1b205":"code","d0080b73":"code","0575aa97":"code","ffd9818b":"code","756a0a0a":"code","196fb50a":"code","977c7f97":"code","c439c39c":"code","eb3cf913":"code","200bc0a8":"code","04aec2a4":"markdown","0f923622":"markdown","477df378":"markdown","cc365395":"markdown","fdefdbcf":"markdown","dcc4b21d":"markdown","8e4a9816":"markdown","d1840a03":"markdown","135823ba":"markdown","83a54c3b":"markdown","820661c7":"markdown","3e590482":"markdown","e575516c":"markdown"},"source":{"f3a963d8":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \n%matplotlib inline\nimport seaborn as sns\nfrom tqdm import tqdm_notebook as tqdm\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nfrom scipy import stats\nimport lightgbm as lgb\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport gc\nimport optuna\npd.set_option('display.max_columns', 1000)","ffe1b205":"train = pd.read_csv('..\/input\/tabular-playground-series-jan-2021\/train.csv')\ntest  = pd.read_csv('..\/input\/tabular-playground-series-jan-2021\/test.csv')\nsample_submission  = pd.read_csv('..\/input\/tabular-playground-series-jan-2021\/sample_submission.csv')","d0080b73":"corr=train.corr()[\"target\"]\ncorr[np.argsort(corr, axis=0)[:-1]]","0575aa97":"#plotting correlations\nnum_feat=train.columns[train.dtypes!=object]\nnum_feat=num_feat [:-1]\nlabels = []\nvalues = []\nfor col in num_feat:\n    labels.append(col)\n    values.append(np.corrcoef(train[col].values, train.target.values)[0,1])\n    \nind = np.arange(len(labels))\nwidth = 0.9\nfig, ax = plt.subplots(figsize=(8,15))\nrects = ax.barh(ind, np.array(values), color='red')\nax.set_yticks(ind+((width)\/2.))\nax.set_yticklabels(labels, rotation='horizontal')\nax.set_xlabel(\"Correlation coefficient\")\nax.set_title(\"Correlation Coefficients each feature with target\");","ffd9818b":"features=['cont1','cont2','cont3','cont4','cont5','cont6','cont7','cont8','cont9','cont10','cont11','cont12','cont13','cont14']\ntarget = 'target'\nxTrain, xTest = train[features],test[features]\nyTrain= train[target]\n\nprint('******** Finished Reading & Preparing the Data**********')\n","756a0a0a":"from optuna import Trial\n\ndef objective(trial:Trial,fastCheck=True,targetMeter=0,returnInfo=False):\n    folds = 10\n    seed  = 0\n    shuffle = False\n    kf = KFold(n_splits=folds,shuffle=False,random_state=seed)\n    yValidPredTotal = np.zeros(xTrain.shape[0])\n    gc.collect()\n    models=[]\n    validScore=0\n    for trainIdx,validIdx in kf.split(xTrain,yTrain):\n        trainData=xTrain.iloc[trainIdx,:],yTrain[trainIdx]\n        validData=xTrain.iloc[validIdx,:],yTrain[validIdx]\n        model,yPredValid,log = fitLGBM(trial,trainData,validData,numRounds=5000)\n        yValidPredTotal[validIdx]=yPredValid\n        models.append(model)\n        gc.collect()\n        validScore+=log[\"validRMSE\"]\n    validScore\/=len(models)\n    return validScore","196fb50a":"def fitLGBM(trial,train,val,numRounds=5000): \n    xTrainLGBM,yTrainLGBM = train\n    xValidLGBM,yValidLGBM = val\n    boosting_list = ['gbdt','goss']\n    objective_list_reg = ['huber', 'gamma', 'fair', 'tweedie']\n    objective_list_class = ['regression','binary', 'cross_entropy']\n    params={\n      'boosting':trial.suggest_categorical('boosting',boosting_list),\n      'num_leaves':trial.suggest_int('num_leaves', 2, 2**11),\n      'max_depth':trial.suggest_int('max_depth', 2, 25),\n      'max_bin': trial.suggest_int('max_bin', 32, 450,550),      \n      'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 150,256),\n      'min_data_in_bin': trial.suggest_int('min_data_in_bin', 1, 150,256),\n      'min_gain_to_split' : trial.suggest_discrete_uniform('min_gain_to_split', 0.1, 5, 0.01),      \n      'lambda_l1':trial.suggest_loguniform('lamda_l1',1e-8,10),\n      'lambda_l2':trial.suggest_loguniform('lamda_l2',1e-8,10),\n      'learning_rate': trial.suggest_loguniform('learning_rate', 0.001, 0.1),\n      'metric':trial.suggest_categorical('metric', ['RMSE']),\n      'objective':trial.suggest_categorical('objective',objective_list_reg),\n      'bagging_fraction':trial.suggest_discrete_uniform('bagging_fraction',0.4, 1, 0.01),\n      'feature_fraction':trial.suggest_discrete_uniform('feature_fraction',0.4, 1, 0.01),\n    }\n    earlyStop=1000\n    verboseEval=100\n    dTrain = lgb.Dataset(xTrainLGBM,label=yTrainLGBM)\n    dValid = lgb.Dataset(xValidLGBM,label=yValidLGBM)\n    watchlist = [dTrain,dValid]\n\n    # Callback for pruning.\n    lgbmPruningCallback = optuna.integration.LightGBMPruningCallback(trial, 'rmse', valid_name='valid_1')\n\n    model = lgb.train(params,train_set=dTrain,num_boost_round=numRounds,valid_sets=watchlist,verbose_eval=verboseEval,early_stopping_rounds=earlyStop,callbacks=[lgbmPruningCallback])\n\n    #predictions\n    pred_val=model.predict(xValidLGBM,num_iteration=model.best_iteration)\n    oofPred = pred_val.astype(int)        \n    log={'trainRMSE':model.best_score['training']['rmse'],\n       'validRMSE':model.best_score['valid_1']['rmse']}\n    return model,pred_val,log","977c7f97":"study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\nstudy.optimize(objective,n_trials=200)#For the sake of simplicity, I have kept n_trials as less, but this can be altered for better results","c439c39c":"print('Best trial: score {}, params {}'.format(study.best_trial.value, study.best_trial.params))","eb3cf913":"optuna.visualization.plot_optimization_history(study)","200bc0a8":"optuna.visualization.plot_slice(study)","04aec2a4":"## optuna.visualization.plot_optimization_history(study) : This function plots optimization history of all trials in a study","0f923622":"# Correlation in Data\n\nIn this work, I try to measure correlation in data using Correlation coefficients.\n\nCorrelation coefficientsare used to measure how strong a relationship is between two variables.Correlation coefficient formulas are used to find how strong a relationship is between data. The formulas return a value between -1 and 1, where:\n\n1 indicates a strong positive relationship.\n-1 indicates a strong negative relationship.\n\nA result of zero indicates no relationship at all.\n\n<img src=\"https:\/\/www.statisticshowto.com\/wp-content\/uploads\/2012\/10\/pearson-2-small.png\" width=\"500\">\n\n\n","477df378":" # Importing Libraries","cc365395":"## Optimization & STUDY Object","fdefdbcf":"## Reference : \n\n* https:\/\/www.kaggle.com\/kst6690\/dsb2019-tuning-lightgbm-parameter-using-optuna\n* https:\/\/www.statisticshowto.com\/probability-and-statistics\/correlation-coefficient-formula\/#Pearson","dcc4b21d":"## OPTUNA Study History : Analysis & Visualization","8e4a9816":"# Plotting correlations","d1840a03":"## optuna.visualization.plot_slice(study, params=None) : This function plots the parameter relationship as slice plot in a study","135823ba":"# Reading & Preparing the Data","83a54c3b":"## Defining Parameter Space for OPTUNA","820661c7":"## Objective Function","3e590482":"# OPTUNA\n\nOptuna uses Bayesian methods to figure out an optimal set of hyperparameters. For more information on Bayesian methods for searching optimal parameters, check out this wonderful article : https:\/\/towardsdatascience.com\/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f .","e575516c":"# Loading the Data"}}