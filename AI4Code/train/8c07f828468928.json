{"cell_type":{"7c8fccc5":"code","c75f185e":"code","a59bbd9c":"code","a01df9f0":"code","121b5cda":"code","b8e5c1fc":"code","7e7d8ff0":"code","bb04f4b6":"code","810b3c05":"code","ca8d5028":"code","ce1cb24f":"code","bbdb3fd2":"code","57ec3d90":"code","fb19c8b1":"code","ace5f489":"code","81e7f374":"code","599eb33b":"code","e10b23e2":"code","92bd5633":"code","e72a31b2":"code","6998d1c5":"code","ed9e799c":"code","eac72e4e":"code","8c86de2e":"code","1db4e516":"code","fd55e264":"code","8cc745e2":"code","260b0ea3":"code","fac45553":"code","a71577f8":"code","726697e4":"markdown","5ac7a97a":"markdown","034a694b":"markdown","a1769ebf":"markdown","067e786e":"markdown","6ef71a24":"markdown","4b2a831c":"markdown","67730cf4":"markdown","c57f4a91":"markdown","4161cf36":"markdown","91e3af91":"markdown","eb54bd4f":"markdown","9de3a3d7":"markdown","49bb7d65":"markdown","4f72d9c7":"markdown","f5625363":"markdown","9b78abef":"markdown","348550b8":"markdown","d46e1bd9":"markdown","073e2f3e":"markdown","83444bf9":"markdown","55d8744e":"markdown","ee66d70d":"markdown","fc23eb6f":"markdown","16b25cc6":"markdown","0b2dc633":"markdown","c552dd8f":"markdown","19a741e0":"markdown"},"source":{"7c8fccc5":"\nimport math\nimport os\nimport random\nimport concurrent.futures\nimport re\nfrom pathlib import Path\n\nimport numpy\nimport pandas as pd\nimport seaborn as sns\nimport sentencepiece as spm\nimport torch\nimport torch.optim as optim\nfrom torch import nn\nfrom torch.nn import functional as f\nfrom torch.utils.data import Dataset, DataLoader\nimport spacy\n\n!python -m spacy download en_core_web_sm\n\n!pip install neptune-client\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nNEPTUNE_API_TOKEN = user_secrets.get_secret(\"NEPTUNE_API_TOKEN\")\nimport neptune\n\nCLS = 'CLS'\nMASK = 'MASK'\nSEP = 'SEP'\nPAD = 'PAD'\nUNK = 'UNK'\n\n# enable cuda if it exists\nif torch.cuda.is_available():\n    TORCH_DEVICE = \"cuda\"\nelse:\n    TORCH_DEVICE = \"cpu\"\n\ncurrent_device = torch.device(TORCH_DEVICE)","c75f185e":"\n\nclass Bert(nn.Module):\n    # pylint: disable=too-many-arguments\n    def __init__(self, stack_size, voc_size,\n                 dim_model, mh_size, padding_idx=0):\n        super().__init__()\n        self.dim_model = dim_model\n        self.emb = nn.Embedding(\n            embedding_dim=dim_model,\n            num_embeddings=voc_size,\n            padding_idx=padding_idx\n        )\n        self.encoder_layer = nn.ModuleList()\n        for _ in range(stack_size):\n            self.encoder_layer.append(Encoder(dim_model, mh_size))\n\n    def forward(self, tokens):\n        mask = (tokens > 0).unsqueeze(1).repeat(1, tokens.size(1), 1).unsqueeze(1)\n        embeddings = self.emb(tokens)\n        pos_embedding = positional_enc(embeddings.shape[1], embeddings.shape[2],\n                                       self.emb.weight.device.type)\n        z_n = pos_embedding + embeddings * math.sqrt(self.dim_model)\n        for encoder in self.encoder_layer:\n            z_n = encoder(z_n, mask)\n        return z_n","a59bbd9c":"\nclass FeedForwardNetwork(nn.Module):\n    def __init__(self, dim_model):\n        super().__init__()\n        self.linear_1 = nn.Linear(dim_model, dim_model * 4)\n        self.linear_2 = nn.Linear(dim_model * 4, dim_model)\n\n    def forward(self, x_n):\n        out_l1 = f.relu(self.linear_1(x_n))\n        return self.linear_2(out_l1)\n\n\nclass Encoder(nn.Module):\n    def __init__(self, dim_model, mh_size):\n        super().__init__()\n        self.mh_att = MultiHeadAttention(mh_size, dim_model)\n        self.add_norm_l1 = AddNormalizeLayer(dim_model)\n        self.feed_forward_network = FeedForwardNetwork(dim_model)\n        self.add_norm_l2 = AddNormalizeLayer(dim_model)\n\n    def forward(self, x_n, mask):\n        z_n = self.mh_att(x_n, mask)\n        l1_out = self.add_norm_l1(x_n, z_n)\n        ffn_out = self.feed_forward_network(l1_out)\n        return self.add_norm_l2(l1_out, ffn_out)","a01df9f0":"class MultiHeadAttention(nn.Module):\n    def __init__(self, multi_head_size, dim_model):\n        super().__init__()\n        self.dim_model = dim_model\n        self.multi_head_size = multi_head_size\n        self.linear_o = nn.Linear(self.dim_model, self.dim_model)\n        self.query = nn.Linear(self.dim_model, self.dim_model)\n        self.key = nn.Linear(self.dim_model, self.dim_model)\n        self.value = nn.Linear(self.dim_model, self.dim_model)\n\n    def forward(self, tokens, mask):\n        batch_size = tokens.shape[0]\n        z_n = self.compute_attention(tokens, batch_size, mask)\n        return self.linear_o(z_n.transpose(1, 2).contiguous().view(batch_size, -1, self.dim_model))\n\n    def compute_attention(self, tokens, batch_size, mask):\n        d_k = self.dim_model \/\/ self.multi_head_size\n        query_mat = self.query(tokens).view(batch_size, -1, self.multi_head_size, d_k) \\\n            .transpose(2, 1)\n        key_mat = self.key(tokens).view(batch_size, -1, self.multi_head_size, d_k) \\\n            .transpose(2, 1)\n        value_mat = self.value(tokens).view(batch_size, -1, self.multi_head_size, d_k) \\\n            .transpose(2, 1)\n        scores = (query_mat.matmul(key_mat.transpose(-2, -1)) \/ math.sqrt(self.dim_model)) \\\n            .masked_fill(mask == 0, 1e-11)\n\n        return f.softmax(scores, dim=-1).matmul(value_mat)","121b5cda":"\nclass AddNormalizeLayer(nn.Module):\n    def __init__(self, normalized_shape):\n        super().__init__()\n        self.layer_norm = nn.LayerNorm(normalized_shape)\n\n    def forward(self, residual_in, prev_res):\n        return residual_in + self.layer_norm(prev_res)","b8e5c1fc":"\n\ndef positional_enc(seq_len, model_dim, device=\"cpu\"):\n    pos_emb_vector = torch.empty(seq_len, model_dim).to(device)\n    for pos in range(seq_len):\n        for i_col in range(model_dim):\n            power_ind = 10000 ** ((2 * i_col) \/ model_dim)\n            if i_col % 2 == 0:\n                pos_emb_vector[pos, i_col] = math.sin(pos \/ power_ind)\n            else:\n                pos_emb_vector[pos, i_col] = math.cos(pos \/ power_ind)\n    return pos_emb_vector","7e7d8ff0":"TRAIN_PATH = '..\/input\/tweet-sentiment-extraction\/train.csv'\nTEST_PATH = '..\/input\/tweet-sentiment-extraction\/test.csv'\nPR_TRAIN_PATH = '.\/processed_train.csv'\nPR_TEST_PATH = '.\/processed_test.csv'\nif not Path(PR_TRAIN_PATH).is_file():\n    train_csv = pd.read_csv(TRAIN_PATH, dtype={'text': 'string'})\n    test_dt = pd.read_csv(TEST_PATH, dtype={'text': 'string'})\nelse:\n    train_csv = pd.read_csv(PR_TRAIN_PATH, dtype={'text': 'string'})\n    test_dt = pd.read_csv(PR_TEST_PATH, dtype={'text': 'string'})","bb04f4b6":"if not Path(PR_TRAIN_PATH).is_file():\n    train_csv = train_csv.dropna()\n    train_csv = train_csv.reset_index(drop=True)\n    test_dt = test_dt.dropna()\n    test_dt = test_dt.reset_index(drop=True)\n    train_csv.head()","810b3c05":"nlp = spacy.load(\"en_core_web_sm\", disable=['ner', 'parser'])\n\n\ndef processing_text(entry, dataframe, df_idx):\n    text = entry['text'].lower().replace(\"`\", \"'\").strip()\n    text = ' '.join([token.text\n                     if token.lemma_ == \"-PRON-\" or '*' in token.text else token.lemma_\n    if not token.is_punct else '' for token in nlp(text)]).strip()\n    text = re.sub(r'http[s]?:\/\/\\S+', '[URL]', text)\n    dataframe.at[df_idx, 'text'] = re.sub(r'\\s\\s+', ' ', text)\n\n\ndef processing_df(dataframe, path):\n    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n        future_to_url = {executor.submit(processing_text, df_entry, dataframe, df_idx):\n                         df_entry for df_idx, df_entry in enumerate(dataframe.iloc)}\n    for _ in concurrent.futures.as_completed(future_to_url):\n        pass\n    dataframe.to_csv(path)\n\n\nif not Path(PR_TRAIN_PATH).is_file():\n    processing_df(test_dt, PR_TEST_PATH)\n    processing_df(train_csv, PR_TRAIN_PATH)\ntrain_csv.head()","ca8d5028":"PATH = '.\/tweet-sentiment-extraction'\nwith open(PATH + '.txt', 'w') as voc_txt:\n    for t in train_csv['text']:\n        voc_txt.write(t + '\\n')\nSPM_ARGS = \"\" \\\n           \"--input={0}.txt \" \\\n           \"--model_prefix={0} \" \\\n           \"--pad_id=0 \" \\\n           \"--unk_id=1 \" \\\n           \"--bos_id=2 \" \\\n           \"--eos_id=3 \" \\\n           \"--pad_piece={1} \" \\\n           \"--unk_piece={2} \" \\\n           \"--bos_piece={3} \" \\\n           \"--eos_piece={4}\" \\\n    .format(PATH, PAD, UNK, CLS, SEP)\nspm.SentencePieceTrainer.Train(SPM_ARGS)\nsp = spm.SentencePieceProcessor()\nsp.Load(PATH + '.model')\nprint(sp.EncodeAsPieces('this is a test'))\nprint(sp.EncodeAsIds('this is a test'))","ce1cb24f":"if \"CORPUS_SIZE\" in os.environ:\n    corpus_size = int(os.environ.get(\"CORPUS_SIZE\"))\n    train_csv = train_csv[:corpus_size]\n    test_dt = test_dt[:corpus_size]\nelse:\n    train_csv = train_csv[:100]\n    test_dt = test_dt[:10]\n","bbdb3fd2":"train_csv['sequence length'] = ''\nURL_COUNT = 0\nfor idx, d in enumerate(train_csv.iloc):\n    train_csv.at[idx, 'sequence length'] = len(sp.EncodeAsIds(d['text']))\nfor idx, d in enumerate(test_dt.iloc):\n    test_dt.at[idx, 'sequence length'] = len(sp.EncodeAsIds(d['text']))\nsns.set(font_scale=2)\nsns.displot(x='sequence length', data=train_csv, aspect=2, height=20)\nprint('number of entries containing a url : ' + str(URL_COUNT))\nprint('number of entries in train.csv : ' + str(len(train_csv)))","57ec3d90":"del train_csv['selected_text']\ntrain_csv = train_csv.drop(train_csv[train_csv['sequence length'].ge(35)].index)\ntrain_csv = train_csv.drop(train_csv[train_csv['sequence length'].le(5)].index)\ntrain_csv = train_csv.reset_index(drop=True)\nprint('number of entries in train.csv after filtering : ' + str(len(train_csv)))\nsns.displot(x='sequence length', data=train_csv, aspect=2, height=20)","fb19c8b1":"len_train_csv = len(train_csv)\nlen_test_df = len(test_dt)\ntotal_size = len_train_csv + len_test_df\n\ntrain_dt = train_csv.iloc[:int(len_train_csv * 70 \/ 100)]\neval_dt = train_csv.iloc[int(len_train_csv * 70 \/ 100):]\n\nprint(\n    \"\"\"size of train.csv file : {0}\nsize of test.csv file : {1}\ntotal size : {2}\n\nsize of train dataset : {3}\nsize of eval dataset : {4}\nsize of test dataset : {5}\n\"\"\".format(\n        len_train_csv,\n        len_test_df,\n        total_size,\n        len(train_dt),\n        len(eval_dt),\n        len(test_dt)))","ace5f489":"class TwitterDataset(Dataset):\n    def __init__(self, train_dataset, eval_dataset, test_dataset, sentence_piece):\n        self.train_dataset = train_dataset\n        self.eval_dataset = eval_dataset\n        self.current_dataset = self.train_dataset\n        self.test_dataset = test_dataset\n        self.sentence_piece = sentence_piece\n        self.st_voc = []\n        self.max_seq_len = int(pd.concat(\n            [train_dataset, eval_dataset, test_dataset])['sequence length'].max()) + 2\n        self.__init_sentiment_vocab()\n\n    def __init_sentiment_vocab(self):\n        self.st_voc = [UNK, *self.train_dataset['sentiment'].unique()]\n\n    def get_vocab_size(self):\n        return self.sentence_piece.vocab_size() + 1\n\n    def __getitem__(self, index):\n        return {\n            'vectorized_tokens': self.vectorize(self.current_dataset.iloc[index][\"text\"]),\n            \"sentiment_i\": self.get_sentiment_i(self.current_dataset.iloc[index][\"sentiment\"])\n        }\n\n    def __len__(self):\n        return len(self.current_dataset)\n\n    def switch_to_dataset(self, flag):\n        if flag == 'train':\n            self.current_dataset = self.train_dataset\n        elif flag == 'eval':\n            self.current_dataset = self.eval_dataset\n        elif flag == 'test':\n            self.current_dataset = self.test_dataset\n        else:\n            raise ValueError(\"this dataset doesn't exist !\")\n\n    # noinspection PyArgumentList\n    def vectorize(self, tokens):\n        vector = self.sentence_piece.EncodeAsIds(tokens)\n        return torch.LongTensor(\n            [sp.bos_id()] + vector + [sp.eos_id()] +\n            [self.get_pad()] * (self.max_seq_len - len(vector) - 2)\n        )\n\n    def get_mask(self):\n        return self.sentence_piece.vocab_size()\n\n    def get_pad(self):\n        return self.sentence_piece.pad_id()\n\n    def get_cls(self):\n        return self.sentence_piece.bos_id()\n\n    def get_sep(self):\n        return self.sentence_piece.eos_id()\n\n    def get_tokens(self, ids):\n        return ' '.join([self.sentence_piece.Decode(i) if i != self.get_mask()\n                         else MASK for i in ids.tolist()]).strip()\n\n    def get_sentiment_i(self, st_token):\n        return self.st_voc.index(st_token) if st_token in self.st_voc else self.st_voc.index(UNK)","81e7f374":"twitter_dataset = TwitterDataset(train_dt, eval_dt, test_dt, sp)","599eb33b":"parameters = {\n    \"stack_size\": 6,\n    \"vocabulary_size\": twitter_dataset.get_vocab_size(),\n    \"bert_dim_model\": 256,\n    \"multi_heads\": 8,\n    \"pre_train_learning_rate\": 1e-4,\n    \"st_learning_rate\": 2e-5,\n    \"batch_size\": 1,\n    \"epochs\": 100,\n    \"device\": current_device,\n    \"corpus test size\": len(test_dt),\n    \"corpus train size\": len(train_csv),\n}","e10b23e2":"bert = Bert(\n    stack_size=parameters[\"stack_size\"],\n    voc_size=parameters[\"vocabulary_size\"],\n    dim_model=parameters[\"bert_dim_model\"],\n    mh_size=parameters[\"multi_heads\"],\n    padding_idx=twitter_dataset.get_pad()\n).to(current_device)\n\nce_loss = nn.CrossEntropyLoss(ignore_index=twitter_dataset.get_pad()) \\\n    .to(current_device)\n\n\ndef generate_batches(dataset, batch_size, shuffle=True, drop_last=True, device=\"cpu\"):\n    \"\"\"\n    A generator function which wraps the PyTorch DataLoader. It will\n    ensure each tensor is on the write device location.\n    \"\"\"\n    data_loader = DataLoader(dataset=dataset, batch_size=batch_size,\n                             shuffle=shuffle, drop_last=drop_last)\n\n    for data_dict in data_loader:\n        data = {}\n        for name, _ in data_dict.items():\n            data[name] = data_dict[name].to(device)\n        yield data","92bd5633":"# noinspection PyArgumentList\n\n\ndef generate_masked_lm(vector, dataset, mask_prob=.15, rnd_t_prob=.1, unchanged_prob=.1):\n    return torch.LongTensor([\n        replace_token(idx_token, dataset, rnd_t_prob, unchanged_prob)\n        if numpy.random.uniform() < mask_prob and is_not_markers(idx_token, dataset)\n        else idx_token\n        for idx_token in vector\n    ])","e72a31b2":"def is_not_markers(token, dataset):\n    return token not in [dataset.get_cls(), dataset.get_sep(),\n                         dataset.get_pad(), dataset.get_mask()]","6998d1c5":"def replace_token(token, dataset, rnd_t_prob, unchanged_prob):\n    prob = numpy.random.uniform()\n    if prob < rnd_t_prob:\n        return replace_by_another_id(token, dataset)\n    if rnd_t_prob < prob < unchanged_prob + rnd_t_prob:\n        return token\n    return dataset.get_mask()","ed9e799c":"def replace_by_another_id(index_token, dataset):\n    replaced_index_t = index_token\n    not_include_t = [\n        dataset.get_cls(),\n        dataset.get_sep(),\n        dataset.get_mask(),\n        dataset.get_pad(),\n        index_token\n    ]\n    while replaced_index_t in not_include_t:\n        replaced_index_t = random.choice(range(twitter_dataset.get_vocab_size()))\n    return replaced_index_t","eac72e4e":"def generate_batched_masked_lm(batched_vectors, dataset,\n                               mask_prob=.15, rnd_t_prob=.1, unchanged_prob=.1):\n    batched_masked_lm = [\n        generate_masked_lm(vector, dataset, mask_prob, rnd_t_prob, unchanged_prob)\n        for vector in batched_vectors\n    ]\n    return torch.stack(batched_masked_lm)","8c86de2e":"class PreTrainingClassifier(nn.Module):\n    def __init__(self, zn_size, voc_size):\n        super().__init__()\n        self.l_1 = nn.Linear(zn_size, voc_size)\n\n    def forward(self, z_n):\n        return self.l_1(z_n)","1db4e516":"optimizer = optim.Adam(bert.parameters(), lr=parameters['pre_train_learning_rate'])","fd55e264":"pre_train_classifier = PreTrainingClassifier(parameters['bert_dim_model'],\n                                             parameters['vocabulary_size']).to(current_device)","8cc745e2":"if \"TEST_ENV\" not in os.environ.keys():\n    neptune.init('smeoni\/bert-impl', api_token=NEPTUNE_API_TOKEN)\n    neptune.create_experiment(name='bert-impl-experiment', params=parameters)\n    for epoch in range(parameters['epochs']):\n        # train loop\n        twitter_dataset.switch_to_dataset(\"train\")\n        for batch in generate_batches(twitter_dataset,\n                                      parameters['batch_size'],\n                                      device=parameters['device']):\n            x_obs = generate_batched_masked_lm(batch['vectorized_tokens'],\n                                               twitter_dataset).to(current_device)\n            y_target = batch['vectorized_tokens'].to(current_device)\n            # Step 1: Clear the gradients\n            bert.zero_grad()\n            # Step 2: Compute the forward pass of the model\n            bert_zn = bert(x_obs)\n            y_pred = pre_train_classifier(bert_zn)\n            # Step 3: Compute the loss value that we wish to optimize\n            loss = ce_loss(y_pred.reshape(-1, y_pred.shape[2]), y_target.reshape(-1))\n            # Step 4: Propagate the loss signal backward\n            loss.backward()\n            # Step 5: Trigger the optimizer to perform one update\n            optimizer.step()\n            neptune.log_metric('pre-train loss', loss.item())\n            observed_ids = torch.argmax(y_pred, dim=2)[-1]\n            RAW_TEXT_OBSERVED = sp.Decode([id_obv for id_obv in observed_ids.tolist()\n                                           if id_obv != twitter_dataset.get_mask()])\n            neptune.send_text('raw pre-train text observed', RAW_TEXT_OBSERVED)\n            RAW_TEXT_EXPECTED = sp.Decode(y_target[-1].tolist())\n            neptune.send_text('raw pre-train text expected', RAW_TEXT_EXPECTED)","260b0ea3":"class FineTuningClassifier(nn.Module):\n    def __init__(self, zn_size, st_voc_size, voc_size):\n        super().__init__()\n        self.l_1 = nn.Linear(zn_size, voc_size)\n        self.l_2 = nn.Linear(voc_size, st_voc_size)\n\n    def forward(self, z_n):\n        l1_out = f.relu((self.l_1(z_n)))\n        out = self.l_2(l1_out)\n        return out","fac45553":"fine_tuning_classifier = FineTuningClassifier(parameters['bert_dim_model'],\n                                              len(twitter_dataset.st_voc),\n                                              parameters['vocabulary_size']).to(current_device)\n\noptimizer = optim.Adam(bert.parameters(), lr=parameters['st_learning_rate'])\n\n\ndef no_learn_loop(corpus, model, no_learn_loss, dataset, no_learn_device):\n    dataset.switch_to_dataset(corpus)\n    # evaluation loop\n    for no_learn_batch in generate_batches(dataset, parameters['batch_size'],\n                                           device=no_learn_device):\n        no_learn_x_obs = generate_batched_masked_lm(no_learn_batch['vectorized_tokens'], dataset) \\\n            .to(no_learn_device)\n        no_learn_y_target = no_learn_batch['sentiment_i'].to(no_learn_device)\n        # Step 1: Compute the forward pass of the model\n        no_learn_zn = model(no_learn_x_obs)\n        no_learn_y_pred = fine_tuning_classifier(no_learn_zn[:, -1, :])\n        # Step 2: Compute the loss value that we wish to optimize\n        no_ll_res = no_learn_loss(no_learn_y_pred, no_learn_y_target.reshape(-1))\n\n        neptune.log_metric('sentiment ' + corpus + ' loss', no_ll_res.item())\n        neptune.send_text('sentiment ' + corpus + ' text',\n                          sp.Decode(x_obs[-1].tolist()))\n        neptune.send_text('sentiment' + corpus + ' observed',\n                          twitter_dataset.st_voc[torch.argmax(y_pred, dim=-1)[-1]])\n        neptune.send_text('sentiment ' + corpus + ' expected', twitter_dataset.st_voc[y_target[-1]])\n\n\nif \"TEST_ENV\" not in os.environ.keys():\n    for epoch in range(parameters['epochs']):\n        # train loop\n        twitter_dataset.switch_to_dataset(\"train\")\n        for batch in generate_batches(twitter_dataset,\n                                      parameters['batch_size'],\n                                      device=parameters['device']):\n            x_obs = batch['vectorized_tokens'].to(current_device)\n            y_target = batch['sentiment_i'].to(current_device)\n            # Step 1: Clear the gradients\n            bert.zero_grad()\n            # Step 2: Compute the forward pass of the model\n            bert_zn = bert(x_obs)\n            y_pred = fine_tuning_classifier(bert_zn[:, -1, :])\n            # Step 3: Compute the loss value that we wish to optimize\n            loss = ce_loss(y_pred, y_target.reshape(-1))\n            # Step 4: Propagate the loss signal backward\n            loss.backward()\n            # Step 5: Trigger the optimizer to perform one update\n            optimizer.step()\n            neptune.log_metric('sentiment train loss', loss.item())\n            neptune.send_text('sentiment train text', sp.Decode(x_obs[-1].tolist()))\n            neptune.send_text('sentiment train observed',\n                              twitter_dataset.st_voc[torch.argmax(y_pred, dim=-1)[-1]])\n            neptune.send_text('sentiment train expected',\n                              twitter_dataset.st_voc[y_target[-1]])\n\n        no_learn_loop('eval', bert, ce_loss, twitter_dataset, parameters['device'])","a71577f8":"if \"TEST_ENV\" not in os.environ.keys():\n    no_learn_loop('test', bert, ce_loss, twitter_dataset, parameters['device'])\n    neptune.stop()","726697e4":"### Bert Encoder Stacks\n* Bert takes as input a sequence of plain text tokens\n* the output is a representation vector of the size of the hidden layers\n* Bert is a stack of multi-layer bidirectional Transformer encoder","5ac7a97a":"# BERT: Pre-Training of Bidirectional Tranformers for Language Understanding\n**see the full paper [here](https:\/\/arxiv.org\/pdf\/1810.04805.pdf)**","034a694b":"## Parameters","a1769ebf":"### split & create training, evaluation & test datasets","067e786e":"### Dataset Instantiation\n","6ef71a24":"### Cleaning and Normalization Step before Sentence Piece Training","4b2a831c":"### Add & Normalize Layer","67730cf4":"### Fine-Tuning Test Loop","c57f4a91":"## Train & Initialize Sentence Piece","4161cf36":"## Dataset : Analyze & Vectorization","91e3af91":"## Architecture\nFor the architecture, the BERT paper referenced to the original\nimplementation of the multi-layer bidirectional\nTransformer encoder described in\n[Vaswani et al. (2017)](https:\/\/arxiv.org\/pdf\/1706.03762.pdf).\nThe Bert model has only one encoders stack.\nSo for this part, I am using the architecture described by the paper above.\n\n![architecture](https:\/\/tinyurl.com\/y5ck5j7c)","eb54bd4f":"## Import CSV files","9de3a3d7":"### Requirements\n**Note**: Don't forget to set the environment variable `CORPUS_SIZE`\nto set the size of corpus if it needed","49bb7d65":"### Encoder\n\n* The encoder is composed of two modules. The first is the attention module\n and the second is the feed-forward network\nmodule.\n\n* this model is execute sequentially but the computation of each token\nis independent and could be compute concurrently","4f72d9c7":"## get a word tokenisation and lemmatization for each entry","f5625363":"## Model Instantiation and DataLoader\n","9b78abef":"## Pre-Training & Fine-Tuning\nFor the Pre-Training, we use instead the RoBERTa learning method.\nWe use only one Pre-Training Task and we mask tokens dynamically.\nFor more details to the dynamic masking\nsee the original paper : https:\/\/arxiv.org\/pdf\/1907.11692.pdf","348550b8":"## Fine-Tuning Step\n### Fine-Tuning Classifier","d46e1bd9":"### resize the corpus if it needed","073e2f3e":"### Positional Encoding","83444bf9":"### Self Attention\n![attention](https:\/\/tinyurl.com\/y47nyfeg)","55d8744e":"### Filter the entries containing url and the less frequent length sequences","ee66d70d":"### Masked LM method","fc23eb6f":"### Fine-Tuning Training Loop","16b25cc6":"### Vectorizer","0b2dc633":"## Pre-Training Step\n### Training and Evaluation Loop","c552dd8f":"### analysis","19a741e0":"### Pre-Training Classifier\na pre-training l_1 is needed to predict the masked token\nBert model give only a bi contextual representation of the sentence"}}