{"cell_type":{"2061fd48":"code","340e1cd5":"code","c50e5002":"code","a512880f":"code","b52e8aff":"code","be245d92":"code","f6b3e91c":"code","2c88f312":"code","3cb57e04":"code","c71f9c44":"code","9c2b8bf4":"code","8b550330":"code","314d48c2":"code","165878e2":"code","0c09929c":"code","30485f1c":"code","d1e735e6":"code","39207c7b":"code","20c9101a":"markdown","42f8ccb1":"markdown","e74af17c":"markdown","3d5523c0":"markdown","00269503":"markdown","796903e0":"markdown","e4aa791d":"markdown","e04c4278":"markdown","e114ceb0":"markdown","8255da5a":"markdown","1a1c5360":"markdown","926c25a9":"markdown","b37eae9c":"markdown","aed62377":"markdown","e1e93001":"markdown","73aabd2e":"markdown","cbb95498":"markdown","d411fea1":"markdown","76b688a2":"markdown","2f934bf9":"markdown","490374a9":"markdown","23c875cc":"markdown","34fdbe8f":"markdown","28cef217":"markdown","fb222151":"markdown","d21c943e":"markdown","6d78e966":"markdown","c9ab51a5":"markdown","95989372":"markdown"},"source":{"2061fd48":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# %matplotlib notebook","340e1cd5":"file = '..\/input\/covid19\/dataset.xlsx'\n\ndata = pd.read_excel(file)\n\ndata.tail()","c50e5002":"# define the label to be predicted\nlabel = 'SARS-Cov-2 exam result'","a512880f":"# replace positive\/negative by 1\/0\ndata[label] = [1 if result=='positive' else 0 for result in data[label]]\n\ndata.tail()","b52e8aff":"ref = data[label].mean()\n\nprint(f'Percentage of all COVID-19 positives in data: {round(ref*100)}%')","be245d92":"other_labels = [\n    'Patient addmited to regular ward (1=yes, 0=no)'\n    'Patient addmited to semi-intensive unit (1=yes, 0=no)',\n    'Patient addmited to intensive care unit (1=yes, 0=no)',\n]","f6b3e91c":"features = list(set(data.columns) - set(['Patient ID', label] + other_labels))\n\nfeatures_numerical = []\nfeatures_categorical = []\nfeatures_empties = []\n\nfor feature in features:\n    hues = set(data[feature][~data[feature].isna()])\n    if len(hues)==0:\n        features_empties.append(feature)\n    else:\n        is_string = isinstance(list(hues)[0], str)\n        if is_string:\n            features_categorical.append(feature)\n        else:\n            features_numerical.append(feature)\n        \n        \nprint(f'{len(features_numerical)} numerical features: ', ', '.join(features_numerical))\nprint()\nprint(f'{len(features_categorical)} categorical features: ', ', '.join(features_categorical))\nprint()\nprint(f'{len(features_empties)} discarded empty features: ', ', '.join(features_empties))","2c88f312":"def plot_age(data, label, save=False):\n    \n    feature = 'Patient age quantile'\n    \n    plt.figure()\n    \n    # plot dashed reference line\n    dat = data.groupby(feature).count().reset_index()\n    x = list(dat[feature])\n    y = [ref*100] * len(x)\n    ax = sns.lineplot(x, y, c='darkred', label='Population Reference')\n    ax.lines[0].set_linestyle(\"--\")\n\n    # Plot the percentages\n    sns.set_color_codes('pastel')\n    dat = data.groupby(feature).mean().reset_index()\n    dat[label] = dat[label] * 100\n    sns.barplot(x=feature, y=label, label='Positive Rate', data=dat, color='r')\n    \n    plt.legend()\n    plt.ylabel('Percentage (%)')\n    samples = data.groupby(feature)[label].count().sum()\n    plt.title(f'{label} (out of {samples}) per {feature}')\n\n    if save: plt.savefig(f'.\/plots\/{label}\/{label}_per_{feature.replace(\"\/\", \"_\")}.png')\n        \n        \nplot_age(data, label)","3cb57e04":"def plot_categorical(data, feature, label, save=False):\n    plt.figure()\n\n    dat = data.groupby(feature).count().reset_index()\n\n    # Plot the total crashes\n    sns.set_color_codes('pastel')\n    sns.barplot(x=feature, y=label, label='Negative', data=dat, color='b')\n\n    # Plot the total crashes\n    sns.set_color_codes('muted')\n    dat = data.groupby(feature).sum().reset_index()\n    sns.barplot(x=feature, y=label, label='Positive', data=dat, color='r')\n    \n    plt.legend()\n    plt.ylabel('Suspected')\n    samples = data.groupby(feature)[label].count().sum()\n    plt.title(f'{label} (out of {samples}) per {feature}')\n\n    if save: plt.savefig(f'.\/plots\/{label}\/{label}_per_{feature.replace(\"\/\", \"_\")}.png')\n        \n        \nfor feature in features_categorical:\n    plot_categorical(data, feature, label)","c71f9c44":"def plot_numerical(data, feature, label, save=False):\n    plt.figure()\n    hues = list(set(data[label]))\n    for hue in hues:\n        sns.distplot(data[feature][data[label]==hue].values, norm_hist=False, kde=False)\n\n        \n    hues = ['Negative' if hue==0 else 'Positive' if hue==1 else hue for hue in hues]\n    plt.legend(hues)\n    plt.xlabel(feature)\n    plt.ylabel('Suspected')\n    samples = data.groupby(feature)[label].count().sum()\n    plt.title(f'{label} (out of {samples}) per {feature}')\n    if save: plt.savefig(f'.\/plots\/{label}\/{label}_per_{feature.replace(\"\/\",\"_\")}.png')\n    \n\nfor feature in features_numerical:\n    plot_numerical(data, feature, label)","9c2b8bf4":"features_covid = [\n    'Leukocytes',\n    'Monocytes',\n    'Platelets',\n    'Patient age quantile',\n]\n\ndat = data[features_covid + [label,]].dropna()\n\nprint(dat.shape)\ndat.head()","8b550330":"plt.figure()\nsns.scatterplot(x=features_covid[0], y=features_covid[1], hue=label, data=dat, \n                  linewidth=0, s=16, alpha = 0.8)\nplt.title(f'Clusters {features_covid[0]}-{features_covid[1]}')\n\nplt.figure()\nsns.scatterplot(x=features_covid[0], y=features_covid[2], hue=label, data=dat, \n                  linewidth=0, s=16, alpha = 0.8)\nplt.title(f'Clusters {features_covid[0]}-{features_covid[2]}')","314d48c2":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\n\nX = dat[features_covid].values\ny = dat[label].values\n\nX = scaler.fit_transform(X)\n\nprint(X.min(), X.max())\nprint(X)","165878e2":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\n\ntest_ratio = .2\n\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n        test_size=test_ratio, shuffle=True)\n\nprint(X_train.shape)\nprint(X_test.shape)","0c09929c":"from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, accuracy_score, fbeta_score\n\navg = y_train.mean()\navg_int = round(avg)\n\ny_naive = [avg] * len(y_test)\ny_naive_int = [avg_int] * len(y_test)\n\nscore_naive = mean_absolute_error(y_test, y_naive)\nscore_naive_int = mean_absolute_error(y_test, y_naive_int)\n\nprint(f'Mean of test dataset: {avg_int} = round({avg})')\nprint(f'Score for naive predictions: {score_naive_int} ({score_naive})')","30485f1c":"from sklearn.linear_model import SGDClassifier, RidgeClassifier, RidgeClassifierCV, Perceptron, PassiveAggressiveClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\n\nmodels = [\n    SGDClassifier(),\n    RidgeClassifier(),\n    RidgeClassifierCV(),\n    Perceptron(),\n    PassiveAggressiveClassifier(),\n    SVC(kernel='rbf'),\n    SVC(kernel=\"linear\", C=0.025),\n    SVC(gamma=2, C=1),\n    KNeighborsClassifier(2),\n    GaussianProcessClassifier(1.0 * RBF(1.0)),\n    DecisionTreeClassifier(max_depth=5),\n    DecisionTreeClassifier(max_depth=6),\n    DecisionTreeClassifier(max_depth=7),\n    RandomForestClassifier(),\n    RandomForestClassifier(max_depth=7, n_estimators=100),\n    MLPClassifier(alpha=1, max_iter=1000),\n    AdaBoostClassifier(),\n    GaussianNB(),\n    QuadraticDiscriminantAnalysis(),\n]\n\n\nbest_score = 999\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n        test_size=test_ratio, shuffle=True)\n\nfor model in models:\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    score = mean_absolute_error(y_test, y_pred)\n#     score = accuracy_score(y_test, y_pred)\n#     score = fbeta_score(y_test, y_pred, average='weighted', beta=1)\n    if score < best_score:\n        best_model = model\n        best_score = score\n        \n    model_class = str(model.__class__).split('.')[-1][:-2]\n    print(f'{score}\\t{model_class}')","d1e735e6":"from sklearn.metrics import confusion_matrix\n\nmodel = best_model\n\ny_pred = model.predict(X_test)\nall_labels = list(set(y_test))\nCM = confusion_matrix(y_test, y_pred, labels=all_labels)\nCM = CM \/ CM.sum(axis=1, keepdims=True)\n\nCM = pd.DataFrame(CM, index=all_labels, columns=all_labels)\n\nFN = CM.values[0,1] \/ CM.values[0,:].sum()\nFP = CM.values[1,0] \/ CM.values[1,:].sum()\nN  = sum(y_pred==0)\nn_test = len(y_test)\n\nprint(f'False Positives: {round(FP*100,1)}%')\nprint(f'False Negatives: {round(FN*100,1)}%')\nprint(f'Negative Results: {round(N\/n_test*100,1)}%')\n        \n\nplt.figure()\nsns.heatmap(CM, annot=True, cmap=\"Blues\")\nmodel_class = str(model.__class__).split('.')[-1][:-2]\nplt.title(f'Normalized Confusion Matrix for {model_class}')\nplt.xlabel('True')\nplt.ylabel('Predicted')","39207c7b":"import numpy as np\n\nn_cross_valid = 599\n    \nn_test = len(y_test)\n\nFP = []\nFN = []\nNN = []\nCM = np.zeros((2,2))\n\nfor i in range(n_cross_valid):\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y,\n        test_size=0.5, shuffle=True)\n\n#     model = RandomForestClassifier(max_depth=2, n_estimators=100)\n    model = RidgeClassifier()\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    CM = confusion_matrix(y_test, y_pred, labels=[0, 1])\n    if (CM[:,1].sum() > 0):# and (np.isnan(CM).sum().sum() == 0):\n        FN.append(CM[0,1] \/ CM[0,:].sum())\n        FP.append(CM[1,0] \/ CM[1,:].sum())\n        NN.append(CM[0,:].sum() \/ CM.sum())\n    \n\nprint(f'Valid cross validations: {len(NN)} out of {n_cross_valid}')\nprint(f'False Positives: {round(np.mean(FP)*100,1)}% +- {round(np.std(FP, ddof=1)*100,1)}%')\nprint(f'False Negatives: {round(np.mean(FN)*100,1)}% +- {round(np.std(FN, ddof=1)*100,1)}%')\nprint(f'Negative Results: {round(np.mean(NN)*100,1)}% +- {round(np.std(NN, ddof=1)\/n_test*100,1)}%')","20c9101a":"# Data Visualization","42f8ccb1":"The values above confirm that the Na\u00efve Classifier should predict always 0 (negative). However, such Na\u00efve Classifier would have False Negative 10%, which is not acceptable.\n\nThe Na\u00efve Classifier's Mean Absolute Error (MAE) above is our reference to evalute our new classifier.\n\nThe chosen score metrics is MAE because it gives the best trade-off between low False Positives and high amount of Negative predictions, after some trial-and-error in the followin piece of code.","e74af17c":"Inspecting the table above leads to the following observations:\n- The label to be predicted is: 'SARS-Cov-2 exam result'\n- There are categorical and numerical data\n- There are considerable amount of empty values (NaN), probably because not all patients performed every possible clinical exam.","3d5523c0":"# COVID-19 exclusive diagnosis from hemogram\n\n## Abstract\n\nThe present work proposes a method to predict the result of the SARS-CoV-2 RT-PCR result from a common hemogram test.\n\nThe presented model is the capable to issue 'Negative' result for 86% of the suspects from this dataset with less than 1% of False Negative. As the result, 86% of the SARS-CoV-2 RT-PCR test kits ould have been spared.\n\nThe expected impact of such model is to save costs and lives during the COVID-19 pandemic peak of infections. **It can be deployed as a triage process to decide which suspects need to proceed with the SARS-CoV-2 RT-PCR test kits**.\n\n\n## Background\nThe World Health Organization (WHO) characterized the COVID-19, caused by the SARS-CoV-2, as a pandemic on March 11, while the exponential increase in the number of cases was risking to overwhelm health systems around the world with a demand for ICU beds far above the existing capacity, with regions of Italy being prominent examples.\n\nBrazil recorded the first case of SARS-CoV-2 on February 26, and the virus transmission evolved from imported cases only, to local and finally community transmission very rapidly, with the federal government declaring nationwide community transmission on March 20.\n\nUntil March 27, the state of S\u00e3o Paulo had recorded 1,223 confirmed cases of COVID-19, with 68 related deaths, while the county of S\u00e3o Paulo, with a population of approximately 12 million people and where Hospital Israelita Albert Einstein is located, had 477 confirmed cases and 30 associated death, as of March 23. Both the state and the county of S\u00e3o Paulo decided to establish quarantine and social distancing measures, that will be enforced at least until early April, in an effort to slow the virus spread.\n\nOne of the motivations for this challenge is the fact that in the context of an overwhelmed health system with the possible limitation to perform tests for the detection of SARS-CoV-2, testing every case would be impractical and tests results could be delayed even if only a target subpopulation would be tested.\n\n## Kaggle competition\nThe present notebook addresses only the TASK 1 (Predict confirmed COVID-19 cases among suspected cases) from [Kaggle competition](https:\/\/www.kaggle.com\/einsteindata4u\/covid19).\n\n## Dataset\nThis dataset contains anonymized data from patients seen at the Hospital Israelita Albert Einstein, at S\u00e3o Paulo, Brazil, and who had samples collected to perform the SARS-CoV-2 RT-PCR and additional laboratory tests during a visit to the hospital.\n\nAll data were anonymized following the best international practices and recommendations. All clinical data were standardized to have a mean of zero and a unit standard deviation.\n\nThis dataset was originally uploaded at the [Kaggle website](https:\/\/www.kaggle.com\/einsteindata4u\/covid19#dataset.xlsx).\n\nThis source-code is copied on [this repository](https:\/\/github.com\/diogodutra\/COVID-19-Albert_Eintein\/blob\/master\/COVID-19_Albert_Einstein.ipynb).\n\n## Disclaimer\nThe [author of this notebook](https:\/\/diogodutra.github.io\/) is a Machine Learning engineer and Lead Data Scientist. Michel Zreik and Bruno Mourao Siqueira are co-authors. The authors have no medical studies. Therefore, the authors can not be liable for the use of the present work. Use at your own discretion.\n\nFeel free to contact at diogodutra@gmail.com to discuss these results or have another customized model for your data.","00269503":"# Data Preparation\nLet's prepare the data for further plots.","796903e0":"Ideally, our best classifier should have MAE=0. However, we know from a comment above that this would not be possible since there is a considerable overlap between the clusters (0\/negative, 1\/positive).\n\nLet's see the normalized Confusion Metrics to evaluate accuracy, False Positives and False Negatives of the best classifier.","e4aa791d":"Which features are in the data?","e04c4278":"# Data Exploration\nHow many of the population is positive for the SARS-Cov-2 test?","e114ceb0":"The split above was random. Nonetheless, I have some concerns about the fact that only 10% are positive and the dataset contains only around 600 cases.\n\nTherefore, let's create several times the same type of classifier training over a different split of data in order to evaluate its performance with statistical meaning (average and standard deviation). This cross-validation approach help us to avoid overtraining.","8255da5a":"It seems from the cluster plots above that there is no limitation neither special recomendation for any particular type of classifier. Therefore, let's create many types of classifiers and check which one have the best performance.\n\nAll the neural network types are not considered because the dataset is rather too small for a proper training.","1a1c5360":"Let's check what is the Na\u00efve Classifier performance as benchmark.","926c25a9":"# Create Model","b37eae9c":"The plot below shows that the babies (age quantile <= 2) are considerably less likely to have positive SARS-Cov-2 result. Therefore, age is useful and it is going to be included in the feature list.","aed62377":"Let's use these chosen features to create some cluster plots and double-check if they are useful.","e1e93001":"The first plot I want to draw the attention is to the age, as shown below.","73aabd2e":"Let's move to the numerical features.","cbb95498":"Split the data into train and test.","d411fea1":"This number is important as a reference. It means that the \"Na\u00efve Classifier\" blindly guesses that all patients are negative and its False Negative rate is 10%.\n\nTherefore, our new classifier have to provide better performance than this reference in order to be useful.","76b688a2":"There are 100 features available in the data. They are divided in 2 types: numerical and categorical.\n\n6 features are empty so they are dropped from our analyses.\n\nWhich features from these 100 are really useful to tell between negative and positive for the SARS-Cov-2 test?","2f934bf9":"# Data Cleaning\n\nSome minor data cleaning was performed directly on the `dataset.xlsx` file before running the code below for convenience as follows:\n- cells values replaced from 'N\u00e3o Realizado' to 'not_done'\n- cells values with ',' replaced by '.' to represent decimals numbers","490374a9":"Besides the \"Patient age quantile\", all the other categorical features are not useful to predict the result of the SARS-Cov-2 for the following reasons:\n- some have too few samples, preventing any meaninful conclusions (ie: Unire - Aspect)\n- some have no meaninful separation between positive and negative (high entropy)","23c875cc":"# Conclusion\n\nOur new classifier is promising.\n\n\nIt is valuable to tell which patients do not need to perform the SARS-CoV-2 RT-PCR test. Instead, the new suspects get through a simple hemogram which is cheaper and faster. The main benefits of this new procedure are:\n\n    - 86% of the suspects from this sample were predicted as 'Negative', which could have spared 86% of the SARS-CoV-2 RT-PCR test kits if this test were applied as triage before using the kits; and\n    - Faster results using the present new classifier, increasing throughput of suspects admitted to the ICU.\n   \n   \nHowever, it is not a flawless test since:\n    - 0.4% of the above ('Negative' for the blood test) would in fact be 'Positive' for the SARS-CoV-2 RT-PCR test. It can be tuned, though;\n    - The other 14% of the suspects will have result as 'Inconclusive' and shall move forward with the SARS-CoV-2 RT-PCR test; and\n    - This dataset is restrict to a small parcel so there is no guarantee when applied to different population or time frame.\n\n\nThe value of this new classifier is the capability of handling the following scenarios:\n- Shortage of SARS-CoV-2 RT-PCR test kits; and\n- Crowded ICU that can not wait for the long delays associated to the SARS-CoV-2 RT-PCR results for all the suspects.","34fdbe8f":"Scale features between 0 and 1 to ensure data normalization, improving the classifier performance.","28cef217":"On the one hand, the Confusion Matrix above shows low values for False Negative. This result is interesting for our goal.\n\nOn the other hand, the False Positive is extremelly high. Therefore, the model can not reliably predict Positive. Instead, 1 will mean \"Inconclusive\".","fb222151":"Most of the numerical features have the same problem explained above for the categorical features.\n\nHowever, there are some that indeed show some difference in the distribution between positive and negative:\n- Leukocytes\n- Monocytes\n- Platelets","d21c943e":"All the rest of the categorical features are plotted below.","6d78e966":"# Data Load\nLet's take a look at the data.","c9ab51a5":"The results above are interesting: high amount of 'Negative Results' along with a low 'False Negatives'. These results have some standard deviation generated from the many cross validations, which allow us to improve the confidence of our claims by applying the statistical p-value tests.","95989372":"The plots above do show some distintict regions for positive and negative clusters.\n\nHowever, there is a considerable overlap of negatives over some positives. For this reason, we shall not expect to have a flawless classifier trained over these data.\n\nFor this reason, our strategy for now on is find a test to reliably tell if a patient is negative (ie: 0 or blue) with low False Negative.\n\nIn other words, our new classifier will predict as either:\n    - 0 = Negative\n    - 1 = Inconclusive"}}