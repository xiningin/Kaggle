{"cell_type":{"7cc1eb60":"code","de6f9534":"code","c322764f":"code","754e342b":"code","3a93c981":"code","6b1b8001":"code","ed084248":"code","a5b644e7":"code","b3ab6020":"code","d91fd074":"code","f5ffe50b":"code","4cfbca09":"code","6573ee7a":"code","4d7a5460":"code","06cab1e6":"code","8583314e":"code","e9f9dd9c":"code","1e755998":"code","f457e885":"code","6eb620b7":"code","9daab312":"code","5300cea8":"code","38b86d35":"code","1e18c7d9":"code","32bf9744":"code","3c4b84b4":"code","4835c34d":"code","7499b071":"code","456b2d8e":"code","907a8c08":"code","84eab814":"markdown","b15e43d1":"markdown","982546e0":"markdown","6e258644":"markdown","5a5bb1ed":"markdown","ee754a1b":"markdown","c865cd1d":"markdown","0d96f0e2":"markdown","05ff27ae":"markdown","06a40e10":"markdown","974d09b5":"markdown","d0f6d5da":"markdown"},"source":{"7cc1eb60":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n\n# built-in imports\nimport time\nimport sys\nimport os\n\n# data handling imports\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n","de6f9534":"# Display values of outputs in the middle of the code without the need for a print statement\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","c322764f":"# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","754e342b":"files = ['feb_mo19.csv', 'mar_mo19.csv', 'abr_mo19.csv', 'may_mo19.csv']","3a93c981":"# 1. Cargar datos\ndf = pd.DataFrame()\nfor f_name in files:\n    df_aux = pd.read_csv(os.path.join(dirname, f_name), sep=';', header=0)\n    df_aux.head()\n    df = df.append(df_aux, ignore_index = True, sort = False)","6b1b8001":"df.info()","ed084248":"# 2. Quedarse con datos estaci\u00f3n que queremos\ndf = df[df['ESTACION'] == 39]","a5b644e7":"# 3. Filtrar por datos NO2\ndf = df[(df['MAGNITUD'] == 8)]\ndf.head()\ndf.tail()","b3ab6020":"# 4. A\u00f1adir nueva columna fecha\ndf['fecha'] = pd.to_datetime(df[['ANO','MES','DIA']].rename(columns = {'ANO': 'YEAR', 'MES': 'MONTH', 'DIA': 'DAY'}))","d91fd074":"df.info()","f5ffe50b":"# 4. Extraer datos de las medidas tomadas y creamos un nuevo dataframe 'data' con el formato que nosotros queremos\ndata_columns = [ 'H01', 'H02', 'H03', 'H04', 'H05', 'H06', 'H07', 'H08', 'H09', 'H10', 'H11', 'H12', 'H13', 'H14', 'H15', 'H16', 'H17', 'H18', 'H19', 'H20', 'H21', 'H22', 'H23', 'H24'] # feature\nveri_columns = [ 'V01', 'V02', 'V03', 'V04', 'V05', 'V06', 'V07', 'V08', 'V09', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24']\n\nhora_column = np.arange(1,25) # feature label\n\ndata = pd.DataFrame(columns=['hora', 'dato', 'verificado']) # DataFrame con datos finales\naux = pd.DataFrame(columns=['hora', 'dato', 'verificado']) # aux de pr\u00f3xima fila a ser a\u00f1adida\n\n# para cada hora de cada fila, crea una nueva fila con su dato y su valor de verificado\nfor  (h,d,v) in zip(hora_column, data_columns, veri_columns): \n     aux['dato'] = df [ data_columns[h-1] ] \n     aux['verificado'] = df [ veri_columns[h-1] ] \n     aux['hora'] = h\n     aux['fecha'] = df ['fecha']\n     data = data.append(aux, sort = False, ignore_index = True) \n\ndata = data.astype({'hora': 'int32'}, copy = False)\n        \ndata.head(10)","4cfbca09":"# 5. Ordenamos datos por orden cronol\u00f3gico\ndata = data.sort_values(['fecha','hora'])\nn_periods = len(data)\ndata.index = pd.timedelta_range(start='0 days', periods=n_periods, freq='H')\ndata.head()\ndata.tail()","6573ee7a":"# 6. Marcar datos no verificados como NaN\ndata.loc[data['verificado'] != 'V', ['dato']] = np.nan\ndata.loc[data['verificado'] != 'V']","4d7a5460":"# borramos columna verificado\ndel data['verificado']\ndata.info()","06cab1e6":"# 7. A\u00f1adimos columna para el d\u00eda de la semana\ndates = data['fecha'].dt.to_pydatetime()\ndays_of_week = ['Lunes', 'Martes', 'Miercoles', 'Jueves', 'Viernes', 'Sabado', 'Domingo']\ndata['dia_semana'] = np.array([days_of_week[d.weekday()] for d in dates])","8583314e":"# 8. algunos detalles m\u00e1s","e9f9dd9c":"data = data.rename(columns={'dato': 'NO2'}, copy = False)","1e755998":"data.info()\ndata","f457e885":"# Serie con solamente los datos de contaminaci\u00f3n\ndatos = data['NO2']","6eb620b7":"# N\u00famero de datos no v\u00e1lidos\nlen(datos[datos.isnull()])","9daab312":"# Una alternativa es simplemente borrar los datos no v\u00e1lidos (son pocos) -> mala\n#data_valids = datos.dropna()\n#len(data_valids)","5300cea8":"# Otra alternativa mejor es llenarlos con \u00faltimo valor v\u00e1lido anterior:\ndatos.fillna(method='ffill', inplace = True )","38b86d35":"# N\u00famero de datos no v\u00e1lidos\nlen(datos[datos.isnull()])","1e18c7d9":"datos.to_csv('pre_feb-may_2019.csv') # Salvado serie solo datos hist\u00f3ricos\ndata.to_csv('pre_feb-may_2019+date.csv') # Salvado con los datos de las fechas","32bf9744":"from itertools import accumulate\n\n# Visualizamos datos de salida\nrange_data = range(len(data['NO2']))\nplt.title('Datos de NO\u00b2 por horas para el periodo seleccionado')\nplt.xlabel('Horas (en secuencia)')\nplt.ylabel('NO\u00b2 (en \u00b5g\/m\u00b3)')\n\ndays_month = [28, 31, 30, 31]\nmonth_marks = list(map(lambda x: x * 24, accumulate(days_month, lambda x, y: (x+y))))\nplt.plot(range_data, data['NO2'])\nfor mark in month_marks:\n    plt.axvline(x=mark, color='r', linestyle='--')","3c4b84b4":"# Algunas estad\u00edsticas b\u00e1sicas:\ndatos.describe()","4835c34d":"# N\u00famero de datos no v\u00e1lidos\nlen(datos[datos.isnull()])","7499b071":"# buscamos si hay alg\u00fan caso con alg\u00fan campo con valor 0 o negativo donde no deber\u00eda\n(datos <= 0).any()","456b2d8e":"# boxplot de la distribuc\u00ed\u00f3n de las medidas\ngreen_diamond = dict(markerfacecolor='g', marker='D')\ndatos.plot.box(title='Distribuci\u00f3n de los valores de las medidas a usar', flierprops=green_diamond)","907a8c08":"# Imprimir tabla con los descriptores en latex\ndes = datos.describe().round(2)\nprint(des.to_latex())","84eab814":"Observamos que se trata de datos con una gran variabilidad y que pueden ir desde 1 hasta m\u00e1s de 200. Haciendo un diagrama de cajas vemos, de hecho, que el conjunto de datos tiene muchos datos extremos por arriba, muy alejados de la media aritm\u00e9tica, y que la diferencia entre la media y la mediana es m\u00e1s pronunciada en los valores por encima de la media. No pasa as\u00ed en cuanto a valores peque\u00f1os, es decir, alejados de la media por abajo.\n\nEn la gr\u00e1fica de visualizaci\u00f3n de los datos podemos constatar m\u00e1s claramente esta gran variabilidad. Adem\u00e1s, en la gr\u00e1fica observamos que se producen muchos vaivenes de subida y bajada en poco tiempo, y observamos grandes picos de subida.","b15e43d1":"# Carga de datos","982546e0":"# Tratamiento de valores perdidos","6e258644":"# Preparaci\u00f3n datos de salida","5a5bb1ed":"## Estad\u00edsticos b\u00e1sicos","ee754a1b":"# Salvado de los datos pre-procesados","c865cd1d":"# Tabla estad\u00edsticos en latex","0d96f0e2":"# An\u00e1lisis estad\u00edstico de los datos","05ff27ae":"# Visualizaci\u00f3n de los datos","06a40e10":"# Selecci\u00f3n de datos","974d09b5":"# Conclusiones an\u00e1lisis","d0f6d5da":"Con este boxplot vemos que hay varios valores pico muy alejados de la media hacia arriba. No pasa as\u00ed en cuanto a valores peque\u00f1os (alejados de la media por abajo)."}}