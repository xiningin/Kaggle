{"cell_type":{"d7e7f063":"code","35511593":"code","67cbd500":"code","e88aa828":"code","875de091":"code","c6737efe":"code","25ab0ecc":"code","ed5c1ca2":"code","44d674ac":"code","0b635e73":"code","1337db82":"code","5b0d26d9":"code","cef877dc":"code","49411205":"code","30c652c9":"code","2409c984":"code","ce57ef26":"code","97803f09":"code","3be1b7c1":"code","f7b32925":"code","fd1f5c2a":"code","e383ebd0":"code","80063728":"code","10b94b28":"code","6ea6e7e6":"code","ca1536d2":"code","c4952c37":"code","02eadd2a":"code","e37efe5e":"code","c760e645":"code","fb3662b5":"code","0cc8bc93":"code","cc3f808c":"code","66b12a3d":"code","bf63330e":"code","2a0ca5e3":"markdown","fb7e7083":"markdown","bd690189":"markdown","f963f983":"markdown","6371a8e3":"markdown","9b7eb550":"markdown","91458af6":"markdown","583d1999":"markdown","d2f5fcb6":"markdown","905c1922":"markdown","c4e594cc":"markdown","958df9a8":"markdown","291e3325":"markdown","cff260c0":"markdown","71363e59":"markdown","ad029707":"markdown","718ada17":"markdown","1533ea6d":"markdown","50f4a6b4":"markdown","b17be1d4":"markdown","4a8b5c7b":"markdown","618ff6c3":"markdown","c9aad927":"markdown","4e826a35":"markdown"},"source":{"d7e7f063":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as lm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor","35511593":"df = pd.read_csv('..\/input\/the-boston-houseprice-data\/boston.csv')\ndf.shape","67cbd500":"df.head()","e88aa828":"df.isnull().sum()","875de091":"df.describe()","c6737efe":"sns.set_style('whitegrid')\nsns.displot(df['MEDV'], kde=True)\nplt.title('Distribution of median value of owner-occupied homes')","25ab0ecc":"sns.displot(df['RM'], kde=True)\nplt.title('Distribution of average number of rooms per dwelling')","ed5c1ca2":"sns.lmplot(x='CRIM', y='MEDV', data=df, aspect=2)\nplt.xlabel('capita crime rate by town')\nplt.ylabel(\"Median value of owner-occupied homes in 1000's\")\nplt.title('houses pricing vs crime rate')","44d674ac":"sns.lmplot(x='PTRATIO', y='MEDV', data=df, aspect=2)\nplt.xlabel('pupil-teacher ratio by town')\nplt.ylabel(\"Median value of owner-occupied homes in 1000's\")\nplt.title('houses pricing vs pupil-teacher ratio by town')","0b635e73":"sns.lmplot(x='RM', y='MEDV', data=df, aspect=2)\nplt.xlabel('average number of rooms per dwelling')\nplt.ylabel(\"Median value of owner-occupied homes in 1000's\")\nplt.title('houses pricing vs the average number of rooms per dwelling')","1337db82":"X = df.iloc[:,:-1].values\ny = df.iloc[:,-1].values","5b0d26d9":"X_withC = lm.add_constant(X)","cef877dc":"X_opt = X_withC[:,[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]]\nregressor_OLS = lm.OLS(endog=y, exog=X_opt).fit()\nregressor_OLS.summary()","49411205":"X_opt = X_withC[:,[0, 1, 2, 4, 5, 6, 8, 9, 10, 11, 12, 13]]\nregressor_OLS = lm.OLS(endog=y, exog=X_opt).fit()\nregressor_OLS.summary()","30c652c9":"X = X_opt[:, 1:]","2409c984":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=150\/506, random_state=0)","ce57ef26":"regressor = LinearRegression()\nregressor.fit(X_train, y_train)","97803f09":"mse_train, mse_test = mean_squared_error(y_train, regressor.predict(X_train)), mean_squared_error(y_test, regressor.predict(X_test))\nrmse_train, rmse_test = np.sqrt(mse_train), np.sqrt(mse_test)\nprint(f'On train set:\\nMSE: {mse_train};    RMSE: {rmse_train}\\n------------------------------------------------\\nOn test set:\\nMSE: {mse_test};    RMSE: {rmse_test}')","3be1b7c1":"# r_squared\nr_squared = regressor.score(X_train, y_train)\nr_squared","f7b32925":"y_pred = regressor.predict(X_test)\ny_df = pd.DataFrame(list(zip(y_test, y_pred)), columns=['y_test', 'y_pred'])\nsns.lmplot(x='y_test', y='y_pred', data=y_df)\nplt.xlabel('y_test')\nplt.ylabel('y_pred')\nplt.title('y_test vs y_pred')\nplt.show()","fd1f5c2a":"results_df = pd.DataFrame([['Linear Regression', mse_test, rmse_test, r_squared]], columns=['Model', 'MSE', 'RMSE', 'R^2'])\nresults_df","e383ebd0":"X = df.iloc[:,:-1].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=150\/506, random_state=0)\nresults = []\nfor i in range(2,5):\n    poly_feat = PolynomialFeatures(degree=i)\n    X_poly_train = poly_feat.fit_transform(X_train)\n    X_poly_test = poly_feat.transform(X_test)\n    regressor = LinearRegression()\n    regressor.fit(X_poly_train, y_train)\n    mse_train, mse_test = mean_squared_error(y_train, regressor.predict(X_poly_train)), mean_squared_error(y_test, regressor.predict(X_poly_test))\n    rmse_train, rmse_test = np.sqrt(mse_train), np.sqrt(mse_test)\n    r2_train, r2_test = regressor.score(X_poly_train, y_train), regressor.score(X_poly_test, y_test)\n    results.append([i, mse_train, rmse_train, r2_train, mse_test, rmse_test, r2_test])\npoly_results_df = pd.DataFrame(results, columns=['degree', 'MSE_train', 'RMSE_train', 'r^2_train', 'MSE_test', 'RMSE_test', 'r^2_test'])\npoly_results_df","80063728":"results_df_2 = pd.DataFrame([['Polynomial Regression', *results[0][4:]]], columns=['Model', 'MSE', 'RMSE', 'R^2'])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","10b94b28":"epsilons = [0.001, 0.003, 0.1, 0.3, 1, 3]\nCs = [1000, 3000, 10000, 30000, 90000]\nresults = []\nfor eps in epsilons:\n    for C in Cs:\n        regressor = SVR(kernel='rbf', epsilon=eps, C=C)\n        regressor.fit(X_train, y_train)\n        mse_train, mse_test = mean_squared_error(y_train, regressor.predict(X_train)), mean_squared_error(y_test, regressor.predict(X_test))\n        rmse_train, rmse_test = np.sqrt(mse_train), np.sqrt(mse_test)\n        r2_train, r2_test = regressor.score(X_train, y_train), regressor.score(X_test, y_test)\n        results.append([eps, C, mse_train, rmse_train, r2_train, mse_test, rmse_test, r2_test])\nsvr_results_df = pd.DataFrame(results, columns=['epsilon', 'C', 'MSE_train', 'RMSE_train', 'r^2_train', 'MSE_test', 'RMSE_test', 'r^2_test'])\nsvr_results_df","6ea6e7e6":"max_r2 = max(svr_results_df['r^2_test'])\nfilt = svr_results_df['r^2_test']==max_r2\nepsilon, C = svr_results_df[filt][['epsilon', 'C']].values[0]","ca1536d2":"svr_results_df[filt][['MSE_test', 'RMSE_test', 'r^2_test']].values[0]","c4952c37":"regressor = SVR(kernel='rbf', epsilon=epsilon, C=C)\nregressor.fit(X_train, y_train)","02eadd2a":"mse_train, mse_test = mean_squared_error(y_train, regressor.predict(X_train)), mean_squared_error(y_test, regressor.predict(X_test))\nrmse_train, rmse_test = np.sqrt(mse_train), np.sqrt(mse_test)\nprint(f'On train set:\\nMSE: {mse_train};    RMSE: {rmse_train}\\n------------------------------------------------\\nOn test set:\\nMSE: {mse_test};    RMSE: {rmse_test}')","e37efe5e":"results_df_3 = pd.DataFrame([['SVR', mse_test, rmse_test, regressor.score(X_test, y_test)]], columns=['Model', 'MSE', 'RMSE', 'R^2'])\nresults_df = results_df.append(results_df_3, ignore_index=True)\nresults_df","c760e645":"nodes = [5, 10, 15, 20, 30, 50]\nresults = []\nfor nbr_nodes in nodes:\n    regressor = DecisionTreeRegressor(criterion='mse', max_leaf_nodes=nbr_nodes, random_state=0)\n    regressor.fit(X_train, y_train)\n    mse_train, mse_test = mean_squared_error(y_train, regressor.predict(X_train)), mean_squared_error(y_test, regressor.predict(X_test))\n    rmse_train, rmse_test = np.sqrt(mse_train), np.sqrt(mse_test)\n    r2_train, r2_test = regressor.score(X_train, y_train), regressor.score(X_test, y_test)\n    results.append([nbr_nodes, mse_train, rmse_train, r2_train, mse_test, rmse_test, r2_test])\ndt_results_df = pd.DataFrame(results, columns=['nbr_nodes', 'MSE_train', 'RMSE_train', 'r^2_train', 'MSE_test', 'RMSE_test', 'r^2_test'])\ndt_results_df","fb3662b5":"results_df_4 = pd.DataFrame([['Decision Tree', *list(dt_results_df.iloc[1, 4:].values)]], columns=['Model', 'MSE', 'RMSE', 'R^2'])\nresults_df = results_df.append(results_df_4, ignore_index=True)\nresults_df","0cc8bc93":"nbr_estimators = [100, 300, 1000, 3000]\nnodes = [10, 20, 50]\nresults = []\nfor nbr_nodes in nodes:\n    for n_estimators in nbr_estimators:\n        regressor = RandomForestRegressor(n_estimators=n_estimators, max_leaf_nodes=nbr_nodes, random_state=0)\n        regressor.fit(X_train, y_train)\n        mse_train, mse_test = mean_squared_error(y_train, regressor.predict(X_train)), mean_squared_error(y_test, regressor.predict(X_test))\n        rmse_train, rmse_test = np.sqrt(mse_train), np.sqrt(mse_test)\n        r2_train, r2_test = regressor.score(X_train, y_train), regressor.score(X_test, y_test)\n        results.append([nbr_nodes, n_estimators ,mse_train, rmse_train, r2_train, mse_test, rmse_test, r2_test])\nrf_results_df = pd.DataFrame(results, columns=['nbr_nodes', 'nbr_estimators', 'MSE_train', 'RMSE_train', 'r^2_train', 'MSE_test', 'RMSE_test', 'r^2_test'])\nrf_results_df","cc3f808c":"max_r2 = max(rf_results_df['r^2_test'])\nfilt = rf_results_df['r^2_test']==max_r2\nnbr_nodes, n_estimators = rf_results_df[filt][['nbr_nodes', 'nbr_estimators']].values[0]\nprint(f\"nbr_nodes = {nbr_nodes}\\nn_estimators = {n_estimators}\\nr_squared = {max_r2}\")","66b12a3d":"results_df_5 = pd.DataFrame([['Random Forest', *list(rf_results_df[filt].iloc[0,5:].values)]], columns=['Model', 'MSE', 'RMSE', 'R^2'])\nresults_df = results_df.append(results_df_5, ignore_index=True)\nresults_df","bf63330e":"sns.barplot(x='R^2', y='Model', data=results_df, palette=sns.color_palette(\"rocket\"))\nplt.xlabel('r_squared')\nplt.ylabel('Models')\nplt.title('comparing r_squared for different models')\nplt.show()","2a0ca5e3":"We choose nbr_nodes and nbr_estimators where we got the maximum r_squared on the test set.","fb7e7083":"***\n# Random Forest:","bd690189":"# Boston Houseprice Data (EDA\/building models):\nIn this notebook I'm going to explore Boston Houseprice dataset and build different machine learning models to predict the price of the house.<br>\nPS: This work was inspired by: [Weining mai](https:\/\/www.kaggle.com\/weibbi)\n### Import Libraries:","f963f983":"As we see from the plot the multiple linear regression model is making good predictions overall \\(the mean of the predictions almost equals the mean of the real values\\).<br>","6371a8e3":"As we see on the plot the median value of owner-occupied homes has a tendency to decrease as crime rate increases. This shows that people care about the neighbourhood safety.\n#### PTRATIO (pupil-teacher ratio by town):","9b7eb550":"***\n# Support Vector Regression:\nWe try to find a good value for epsilon and C:","91458af6":"### Import Dataset:","583d1999":"### Data\n* CRIM: capita crime rate by town\n* ZN: proportion of residential land zoned for lots over 25,000 sq.ft.\n* INDUS: proportion of non-retail business acres per town\n* CHAS: Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n* NOX: nitric oxides concentration (parts per 10 million) [parts\/10M]\n* RM: average number of rooms per dwelling\n* AGE: proportion of owner-occupied units built prior to 1940\n* DIS: weighted distances to five Boston employment centres\n* RAD: index of accessibility to radial highways\n* TAX: full-value property-tax rate per 10,000.\n* PTRATIO: pupil-teacher ratio by town\n* B: The result of the equation B=1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n* LSTAT: % lower status of the population\n* MEDV: Median value of owner-occupied homes in 1000's","d2f5fcb6":"We see that Random Forest Model outperforms all other models based on r_squared value.","905c1922":"#### Train Test Split:\nSplitting the dateset into 356 train observations and 150 test observations.","c4e594cc":"# Multiple Linear Regression:\ndefining the set of independent and dependent variables","958df9a8":"***\n# Polynomial Regression:\nFitting multiple polynomial regression models with different variables degrees (2 to 4).","291e3325":"We eventually removed two independent variales which are:\n> 1) INDUS: proportion of non-retail business acres per town<br>2) AGE: proportion of owner-occupied units built prior to 1940<br>\n\nAs they don't contribute much in predicting MEDV (Median value of owner-occupied homes)","cff260c0":"Notice that the values on RMSE on both the training set and the test set are:\n> close (model is not overfitted).<br>\n> small (model is not underfitted).\n\nWe see that the square root of the mean squared error is 5.207 which means that the model's predictions have an average error of almost 5207$.","71363e59":"Now we are going to see the effect of different independent variables on the houses pricing:\n#### CRIM (capita crime rate by town):","ad029707":"#### Backward Elimination:\nin this part, we are going to eliminate the independent variables that has little to no impact on the prices of homes.<br>\nWe choose our SL (significance level) to be 0.1 so if the p-value of the variable is greater than SL it is removed.","718ada17":"We choose epsilon and C where we got the highest r_squared score on the test set:","1533ea6d":"We can see that the RMSE obtained by SVR on the test set is the best we could find yet compared to the other models.","50f4a6b4":"The plot shows that the median value of owner-occupied homes is linearly propotional  with the average number of rooms per dwelling (Median value increases as number of rooms increases).\n***","b17be1d4":"As we see on the plot the median value of owner-occupied homes is also likely to decrease as pupil\/teacher ratio by town increases. Which means that the better education the area has, the more expensive the houses are.\n#### RM (average number of rooms per dwelling):","4a8b5c7b":"# Data Analysis:","618ff6c3":"***\n# Decision Tree:","c9aad927":"The table shows the different evaluation metrics of the polynomial regression models.<br>\nwhile MSE_train is getting smaller as we add mode features, MSE_test was increasing exponentially. This means that we overfitted the training set. The model tries to fit the training set with all its new polynomial features that it fails to generalise on unseen data (in this case the test set).","4e826a35":"From the table we conclude that increasing the number of bins will lead to overfitting the training set.<br>\nWe choose nbr_nodes = 10"}}