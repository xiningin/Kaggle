{"cell_type":{"d4d6d55d":"code","a51003cc":"code","7bbe4f21":"code","b80e6613":"code","0ff0cdda":"code","dfa6fcd1":"code","d1f75401":"code","60e3ff73":"code","8eb8453a":"code","6fda424c":"code","493545ea":"code","8e1016c0":"code","dac688a5":"code","37c0bc93":"code","e9861870":"code","7e109d3d":"code","14ab3618":"code","824448e5":"code","83dabafa":"code","75589c34":"code","7f9d47ff":"code","b06bc24e":"code","489cafa2":"code","2df904d0":"code","6ff5934e":"code","34a27726":"code","3410c1c4":"code","a378e74d":"code","bf7bda53":"markdown","8f5a93b4":"markdown","df5c47f2":"markdown","c03176da":"markdown","becdacdf":"markdown","3fed18c4":"markdown","c9ec246d":"markdown","75b2a306":"markdown","f71d4941":"markdown","aa54359e":"markdown","d686a41e":"markdown","48acf6cd":"markdown"},"source":{"d4d6d55d":"import tensorflow as tf\nprint(tf.__version__)","a51003cc":"!ls \/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/","7bbe4f21":"# Load datasets\nimport pandas as pd\nimport os\n\nDATA_PATH = \"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/\"\n\nTEST_PATH = os.path.join(DATA_PATH, \"test.csv\")\nVAL_PATH = os.path.join(DATA_PATH, \"validation.csv\")\nTRAIN_PATH = os.path.join(DATA_PATH, \"jigsaw-toxic-comment-train.csv\")\n\nval_data = pd.read_csv(VAL_PATH)\ntest_data = pd.read_csv(TEST_PATH)\ntrain_data = pd.read_csv(TRAIN_PATH)","b80e6613":"# Preview train set\ntrain_data.sample(5)","0ff0cdda":"val_data.sample(5)","dfa6fcd1":"test_data.sample(5)","d1f75401":"# Remove usernames and links\nimport re\n\nval = val_data\ntrain = train_data\n\ndef clean(text):\n    # fill the missing entries and convert them to lower case\n    text = text.fillna(\"fillna\").str.lower()\n    # replace the newline characters with space \n    text = text.map(lambda x: re.sub('\\\\n',' ',str(x)))\n    text = text.map(lambda x: re.sub(\"\\[\\[User.*\",'',str(x)))\n    # remove usernames and links\n    text = text.map(lambda x: re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",'',str(x)))\n    text = text.map(lambda x: re.sub(\"\\(http:\/\/.*?\\s\\(http:\/\/.*\\)\",'',str(x)))\n    return text\n\nval[\"comment_text\"] = clean(val[\"comment_text\"])\ntest_data[\"content\"] = clean(test_data[\"content\"])\ntrain[\"comment_text\"] = clean(train[\"comment_text\"])","60e3ff73":"# Load DistilBERT tokenizer\nimport transformers\n\ntokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')","8eb8453a":"import numpy as np\nimport tqdm\n\ndef create_bert_input_features(tokenizer, docs, max_seq_length):\n    \n    all_ids, all_masks = [], []\n    for doc in tqdm.tqdm(docs, desc=\"Converting docs to features\"):\n        tokens = tokenizer.tokenize(doc)\n        if len(tokens) > max_seq_length-2:\n            tokens = tokens[0 : (max_seq_length-2)]\n        tokens = ['[CLS]'] + tokens + ['[SEP]']\n        ids = tokenizer.convert_tokens_to_ids(tokens)\n        masks = [1] * len(ids)\n        # Zero-pad up to the sequence length.\n        while len(ids) < max_seq_length:\n            ids.append(0)\n            masks.append(0)\n        all_ids.append(ids)\n        all_masks.append(masks)\n    encoded = np.array([all_ids, all_masks])\n    return encoded","6fda424c":"# Segregate the comments and their labels (not applicable for test set)\ntrain_comments = train.comment_text.astype(str).values\nval_comments = val_data.comment_text.astype(str).values\ntest_comments = test_data.content.astype(str).values\n\ny_valid = val.toxic.values\ny_train = train.toxic.values","493545ea":"import gc\ngc.collect()","8e1016c0":"# Encode the comments\nMAX_SEQ_LENGTH = 500\n\ntrain_features_ids, train_features_masks = create_bert_input_features(tokenizer, train_comments, \n                                                                      max_seq_length=MAX_SEQ_LENGTH)\nval_features_ids, val_features_masks = create_bert_input_features(tokenizer, val_comments, \n                                                                  max_seq_length=MAX_SEQ_LENGTH)\n# test_features = create_bert_input_features(tokenizer, test_comments, \n#                                            max_seq_length=MAX_SEQ_LENGTH)","dac688a5":"# Verify the shapes\nprint(train_features_ids.shape, train_features_masks.shape, y_train.shape)\nprint(val_features_ids.shape, val_features_masks.shape, y_valid.shape)","37c0bc93":"# Configure TPU\nfrom kaggle_datasets import KaggleDatasets\n\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\nstrategy = tf.distribute.experimental.TPUStrategy(tpu)\n\nGCS_DS_PATH = KaggleDatasets().get_gcs_path('jigsaw-multilingual-toxic-comment-classification')\n\nEPOCHS = 2\nBATCH_SIZE = 32 * strategy.num_replicas_in_sync","e9861870":"# Create TensorFlow datasets for better performance\ntrain_ds = (\n    tf.data.Dataset\n    .from_tensor_slices(((train_features_ids, train_features_masks), y_train))\n    .repeat()\n    .shuffle(2048)\n    .batch(BATCH_SIZE)\n    .prefetch(tf.data.experimental.AUTOTUNE)\n)\n    \nvalid_ds = (\n    tf.data.Dataset\n    .from_tensor_slices(((val_features_ids, val_features_masks), y_valid))\n    .repeat()\n    .batch(BATCH_SIZE)\n    .prefetch(tf.data.experimental.AUTOTUNE)\n)","7e109d3d":"# Create utility function to get a training ready model on demand\ndef get_training_model():\n    inp_id = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), dtype=tf.int64, name=\"bert_input_ids\")\n    inp_mask = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), dtype=tf.int64, name=\"bert_input_masks\")\n    inputs = [inp_id, inp_mask]\n\n    hidden_state = transformers.TFDistilBertModel.from_pretrained('distilbert-base-multilingual-cased')(inputs)[0]\n    pooled_output = hidden_state[:, 0]    \n    dense1 = tf.keras.layers.Dense(128, activation='relu')(pooled_output)\n    output = tf.keras.layers.Dense(1, activation='sigmoid')(dense1)\n\n    model = tf.keras.Model(inputs=inputs, outputs=output)\n    model.compile(optimizer=tf.optimizers.Adam(learning_rate=2e-5, \n                                            epsilon=1e-08), \n                loss='binary_crossentropy', metrics=['accuracy'])\n\n    return model","14ab3618":"# Authorize wandb\nimport wandb\n\nwandb.login()\nfrom wandb.keras import WandbCallback","824448e5":"# Initialize wandb\nwandb.init(project=\"jigsaw-toxic\", id=\"distilbert-tpu-kaggle-weighted\")","83dabafa":"# Create 32 random indices from the English only test comments\nRANDOM_INDICES = np.random.choice(test_comments.shape[0], 32)\nRANDOM_INDICES","75589c34":"!pip install -q googletrans","7f9d47ff":"# Demo examples of translations\nfrom googletrans import Translator\n\nsample_comment = test_comments[48649]\nprint(\"Original comment:\", sample_comment)\ntranslated_comment = Translator().translate(sample_comment)\nprint(\"\\n\")\nprint(\"Translated comment:\", translated_comment.text)","b06bc24e":"# Create a sample prediction logger\n# A custom callback to view predictions on the above samples in real-time\nclass TextLogger(tf.keras.callbacks.Callback):\n    def __init__(self):\n        super(TextLogger, self).__init__()\n\n    def on_epoch_end(self, logs, epoch):\n        samples = []\n        for index in RANDOM_INDICES:\n            # Grab the comment and translate it\n            comment = test_comments[index]\n            translated_comment = Translator().translate(comment).text\n            # Create BERT features\n            comment_feature_ids, comment_features_masks = create_bert_input_features(tokenizer,  \n                                    comment, max_seq_length=MAX_SEQ_LENGTH)\n            # Employ the model to get the prediction and parse it\n            predicted_label = self.model.predict([comment_feature_ids, comment_features_masks])\n            predicted_label = np.argmax(predicted_label[0])\n            if predicted_label==0: predicted_label=\"Non-Toxic\"\n            else: predicted_label=\"Toxic\"\n            \n            sample = [comment, translated_comment, predicted_label]\n            \n            samples.append(sample)\n        wandb.log({\"text\": wandb.Table(data=samples, \n                                       columns=[\"Comment\", \"Translated Comment\", \"Predicted Label\"])})","489cafa2":"# Garbage collection\ngc.collect()","2df904d0":"# Account for the class imbalance\nfrom sklearn.utils import class_weight\n\nclass_weights = class_weight.compute_class_weight('balanced',\n                                                 np.unique(y_train),\n                                                 y_train)\nclass_weights","6ff5934e":"# Train the model\nimport time\n\nstart = time.time()\n\n# Compile the model with TPU Strategy\nwith strategy.scope():\n    model = get_training_model()\n    \nmodel.fit(train_ds, \n          steps_per_epoch=train_data.shape[0] \/\/ BATCH_SIZE,\n          validation_data=valid_ds,\n          validation_steps=val_data.shape[0] \/\/ BATCH_SIZE,\n          epochs=EPOCHS,\n          class_weight=class_weights,\n          callbacks=[WandbCallback(), TextLogger()],\n          verbose=1)\nend = time.time() - start\nprint(\"Time taken \",end)\nwandb.log({\"training_time\":end})","34a27726":"# Create utility function to get a training ready model on demand\ndef get_training_model_cnn():\n    inp_id = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), dtype=tf.int64, name=\"bert_input_ids\")\n    inp_mask = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), dtype=tf.int64, name=\"bert_input_masks\")\n    inputs = [inp_id, inp_mask]\n\n    hidden_state = transformers.TFDistilBertModel.from_pretrained('distilbert-base-multilingual-cased')(inputs)[0]\n    pooled_output = hidden_state[:, 0]    \n    reshaped_pooled = tf.keras.layers.Reshape((768,1), input_shape=(768,))(pooled_output)\n    conv_1 = tf.keras.layers.Conv1D(64, 2, activation='relu')(reshaped_pooled)\n    pooled_2 = tf.keras.layers.GlobalAveragePooling1D()(conv_1)\n    dense_1 = tf.keras.layers.Dense(128, activation='relu')(pooled_2)\n    output = tf.keras.layers.Dense(1, activation='sigmoid')(dense_1)\n\n    model = tf.keras.Model(inputs=inputs, outputs=output)\n    model.compile(optimizer=tf.optimizers.Adam(learning_rate=2e-5, \n                                            epsilon=1e-08), \n                loss='binary_crossentropy', metrics=['accuracy'])\n\n    return model","3410c1c4":"# Garbage collection\ngc.collect()\n\n# Reinitialize wandb\nwandb.init(project=\"jigsaw-toxic\", id=\"distilbert-tpu-kaggle-weighted-cnn\")","a378e74d":"# Train the CNN-based model\nstart = time.time()\n\n# Compile the model with TPU Strategy\nwith strategy.scope():\n    model = get_training_model_cnn()\n    \nmodel.fit(train_ds, \n          steps_per_epoch=train_data.shape[0] \/\/ BATCH_SIZE,\n          validation_data=valid_ds,\n          validation_steps=val_data.shape[0] \/\/ BATCH_SIZE,\n          epochs=EPOCHS,\n          class_weight=class_weights,\n          callbacks=[WandbCallback(), TextLogger()],\n          verbose=1)\nend = time.time() - start\nprint(\"Time taken \",end)\nwandb.log({\"training_time\":end})","bf7bda53":"## Model building and training","8f5a93b4":"Columns (comes from [here](https:\/\/www.kaggle.com\/c\/jigsaw-multilingual-toxic-comment-classification\/data)): \n- id - identifier within each file.\n- comment_text - the text of the comment to be classified.\n- toxic:identity_hate - whether or not the comment is classified as toxic. ","df5c47f2":"Data description is available [here](https:\/\/www.kaggle.com\/c\/jigsaw-multilingual-toxic-comment-classification\/data). ","c03176da":"An amazing EDA on the dataset in available here: https:\/\/www.kaggle.com\/tarunpaparaju\/jigsaw-multilingual-toxicity-eda-models. ","becdacdf":"We will be logging some sample predictions on the test dataset to see how our model is doing as it is getting trained. Now, as this is a mulitlingual dataset, we may need to convert a given comment to a language of our choice to make sense of the model's prediction. We will be using the `googletrans` library. ","3fed18c4":"The model generalizes better. ","c9ec246d":"The following function comes from [here](https:\/\/github.com\/dipanjanS\/deep_transfer_learning_nlp_dhs2019\/blob\/master\/notebooks\/6%20-%20Transformers%20-%20DistilBERT.ipynb).","75b2a306":"**As I am logging some demo predictions in between this training time should not be used for any benchmarks. **\n\nLet's try a CNN (with 1D convolutions) now. ","f71d4941":"It's a multilingual dataset as you can see. \n\nI am going to borrow the helper functions as shown here: https:\/\/www.kaggle.com\/tarunpaparaju\/jigsaw-multilingual-toxicity-eda-models. ","aa54359e":"In this notebook, I am going to build a baseline model based on [DistilBERT](https:\/\/medium.com\/huggingface\/distilbert-8cf3380435b5) for the Jigsaw Multilingual Toxic Comment Classification (Kaggle challenge [link](https:\/\/www.kaggle.com\/c\/jigsaw-multilingual-toxic-comment-classification)). \n\n**What am I predicting?** (comes from the challenge homepage)\n\nYou are predicting the probability that a comment is toxic. A toxic comment would receive a 1.0. A benign, non-toxic comment would receive a 0.0. In the test set, all comments are classified as either a 1.0 or a 0.0.","d686a41e":"## Load and prepare data","48acf6cd":"## Acknowledgements:\n- https:\/\/www.kaggle.com\/tarunpaparaju\/jigsaw-multilingual-toxicity-eda-models#Introduction\n- https:\/\/github.com\/dipanjanS\/deep_transfer_learning_nlp_dhs2019\n\nRun it on [Kaggle Kernels](https:\/\/www.kaggle.com\/spsayakpaul\/jigsaw-multilingual-toxic-comment-classification). "}}