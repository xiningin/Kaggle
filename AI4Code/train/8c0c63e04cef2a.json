{"cell_type":{"61c6dd5a":"code","d8922093":"code","ea81b660":"code","c5136509":"code","316f0ccc":"code","4df73583":"code","89e69d88":"code","05258d66":"code","c5a93b0a":"code","c237a286":"code","5aef623a":"code","d083b27a":"code","4561b92b":"code","a2fd2456":"code","3e0c7ac9":"code","93b34c6c":"markdown","40b078a6":"markdown","66c6cb59":"markdown","a360443c":"markdown","5b4dac76":"markdown","6aa7999d":"markdown","547407c3":"markdown","41710093":"markdown","c89160e1":"markdown","420f0a31":"markdown"},"source":{"61c6dd5a":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","d8922093":"from IPython.display import clear_output\n!pip3 install -U lazypredict\n\nclear_output()","ea81b660":"!pip3 install -U pandas==1.2.3 #Upgrading pandas\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport lazypredict\nfrom lazypredict import Supervised\nfrom lazypredict.Supervised import LazyRegressor\nclear_output()","c5136509":"# import required modules\nfrom sklearn.model_selection import train_test_split\nimport datetime\n\ntrain = pd.read_csv(\"..\/input\/ventilator-pressure-prediction\/train.csv\")\ntest = pd.read_csv(\"..\/input\/ventilator-pressure-prediction\/test.csv\")","316f0ccc":"train.columns","4df73583":"X = train.drop(['id','breath_id','pressure'], axis=1)\ny = train['pressure']","89e69d88":"# #Spliting into training and validation set\nX_train, X_valid, y_train, y_valid = train_test_split(X, y,test_size=.2,random_state =1)","05258d66":"total_num_models = 42\nreg_idx = [i for i in range(total_num_models)]\nregs_name =[]\nregs = []\nfor i in range(42):\n    regs_name.append(lazypredict.Supervised.REGRESSORS[i][0])\n    regs.append(lazypredict.Supervised.REGRESSORS[i][1])\n\n\nprint(\"ALL 42 AVAILABLE REGRESSION MODELS:\")\nfor i in range(total_num_models):\n    print(i+1 , regs_name[i])","c5a93b0a":"num_models = 25\nreg_idx = [i for i in range(num_models)]\nskip_list = [8,9,10,11,12,15, 16,17,18,26,27,28,29,30,31,33, 37]\n # Removing 8 models from 42 models. Removed models require lot of ram and getting error too \nregs_name =[]\nregs = []\nfor i in range(42):\n    if i in skip_list:\n        print('Skipping', i, \" ->\", lazypredict.Supervised.REGRESSORS[i][0])\n    else:    \n        regs_name.append(lazypredict.Supervised.REGRESSORS[i][0])\n        regs.append(lazypredict.Supervised.REGRESSORS[i][1])\n\n","c237a286":"from sklearn.metrics import r2_score,mean_poisson_deviance\n\noffset = int(X.shape[0] * 0.9)\n#Let\u2019s split the dataset into the training and testing part:\n\n### splitting dataset into training and testing part.\nX_train, y_train = X[:offset], y[:offset]\nX_test, y_test = X[offset:], y[offset:]\n#Let\u2019s create an object of LazyRegressor class:\n\n### fitting data in LazyRegressor because here we are solving Regression use case. \nnum_models = 42 - len(skip_list)\nmod_idx = [m for m in range(num_models)]\nresults = pd.DataFrame()\nfor i in range(0,num_models):\n    print(i,regs_name[i])\n    reg = LazyRegressor(verbose=0,\n                        ignore_warnings=True,\n                    predictions=True,\n                    custom_metric=  r2_score,\n                    regressors = [regs[i]])\n    models, predictions = reg.fit(X_train, X_valid, y_train, y_valid)\n    models.index = [regs_name[i]]\n    results = results.append(models)\n\nclear_output()\nprint(results)\n\n### fitting data in LazyClassifier\nmodels, predictions = reg.fit(X_train, X_test, y_train, y_test)\n","5aef623a":"results = results.sort_values(by = \"r2_score\")\nprint(results)","d083b27a":"results.head()","4561b92b":"reg_idx1=[i for i in range(17)]\nreg_idx1","a2fd2456":"plt.plot(reg_idx1 , results[\"r2_score\"],label = \"r2_score\" ,marker='o')\nplt.xlabel(\"Model ID\")\nplt.ylabel(\"r2_score\")\nplt.title(\"r2_score Comparison of 25 Different Models\")\nplt.legend()\nplt.show()","3e0c7ac9":"plt.plot(reg_idx1 , results[\"Time Taken\"],label = \"Time Taken\" ,marker='*' , color = 'r')\nplt.xlabel(\"Model ID\")\nplt.ylabel(\"Time Taken\")\nplt.title(\"TIME TAKEN of 25 Different Models\")\nplt.legend()\nplt.show()","93b34c6c":"# <h1 style=' border:0; color:#FFA500 '><left>\ud83e\udd47 Top 5 Performing Models<\/left><\/h1> ","40b078a6":"# <h1 style=' border:0; color:#FFA500 '><left>Model Selection \ud83d\udcda<\/left><\/h1> ","66c6cb59":"#### Inspired from https:\/\/www.kaggle.com\/odins0n\/30dml-comparison-of-36-different-models","a360443c":"# <h1 style=' border:0; color:#FFA500 '><left>Loading Data \ud83d\ude9a<\/left><\/h1> ","5b4dac76":"# <h1 style=' border:0; color:#FFA500 '><left>Eliminating Time consuming models<\/left><\/h1> ","6aa7999d":"# <h1 style=' border:0; color:#FFA500 '><left>\u2714\ufe0f Results<\/left><\/h1> ","547407c3":"# <h1 style=' border:0; color:#FFA500 '><center> If you find this notebook usefull support me with giving \u2b06\ufe0f!<\/left><\/h1> ","41710093":"# <h1 style=' border:0; color:#FFA500 '><left>Training on 25 different models \ud83c\udfcb\ufe0f<\/left><\/h1> ","c89160e1":"# <h1 style=' border:0; color:#FFA500 '><left>Import Libraries \ud83d\udcda<\/left><\/h1> ","420f0a31":"# <h1 style=' border:0; color:#FFA500 '><left> Comparison Plots \ud83d\udcc8 <\/left><\/h1> "}}