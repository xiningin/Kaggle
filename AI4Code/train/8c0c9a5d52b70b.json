{"cell_type":{"55fc5d44":"code","1f1f9f3d":"code","91a92d47":"code","af9ba334":"code","7a7b709c":"code","b8c95395":"code","dc26b2c8":"code","39bcd3ca":"code","c5317851":"code","d4245418":"markdown"},"source":{"55fc5d44":"from keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras.layers import Embedding\nfrom keras.layers import Conv1D, GlobalMaxPooling1D\nfrom keras.datasets import imdb\n\n%matplotlib inline\nimport matplotlib.pyplot as plt","1f1f9f3d":"# set parameters:\nmax_features = 5000\nmaxlen = 400\nbatch_size = 32\nembedding_dims = 50\nfilters = 500\nkernel_size = 3\nhidden_dims = 500\nepochs = 4","91a92d47":"print('Loading data...')\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\nprint(len(x_train), 'train sequences')\nprint(len(x_test), 'test sequences')","af9ba334":"print('Pad sequences (samples x time)')\nx_train = sequence.pad_sequences(x_train, maxlen=maxlen)\nx_test = sequence.pad_sequences(x_test, maxlen=maxlen)\nprint('x_train shape:', x_train.shape)\nprint('x_test shape:', x_test.shape)","7a7b709c":"print('Build model...')\nmodel = Sequential()\n\n# we start off with an efficient embedding layer which maps\n# our vocab indices into embedding_dims dimensions\nmodel.add(Embedding(max_features,\n                    embedding_dims,\n                    input_length=maxlen))\nmodel.add(Dropout(0.2))\n\n# we add a Convolution1D, which will learn filters\n# word group filters of size filter_length:\nmodel.add(Conv1D(filters,\n                 kernel_size,\n                 padding='valid',\n                 activation='relu',\n                 strides=1))\n# we use max pooling:\nmodel.add(GlobalMaxPooling1D())\n\n# We add a vanilla hidden layer:\nmodel.add(Dense(hidden_dims))\nmodel.add(Dropout(0.5))\nmodel.add(Activation('relu'))\n\n# We project onto a single unit output layer, and squash it with a sigmoid:\nmodel.add(Dense(1))\nmodel.add(Activation('sigmoid'))","b8c95395":"model.summary()","dc26b2c8":"model.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])","39bcd3ca":"history = model.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          validation_data=(x_test, y_test))","c5317851":"acc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n \n_epochs = range(len(acc))\n \nplt.plot(_epochs, acc, 'b', label='Training acc')\nplt.plot(_epochs, val_acc, 'r', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n \nplt.figure()\n \nplt.plot(_epochs, loss, 'b', label='Training loss')\nplt.plot(_epochs, val_loss, 'r', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n \nplt.show()","d4245418":"# Keras CNN IMDB classifier"}}