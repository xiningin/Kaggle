{"cell_type":{"59e0e37e":"code","036eba40":"code","171aafef":"code","968dec9e":"code","143e540a":"code","f62cc3e3":"code","0dda5c05":"code","c09ccbe5":"code","d3281883":"markdown","a9823bf0":"markdown","7a46fe27":"markdown","2cc481d4":"markdown","b77871b6":"markdown","861c57be":"markdown","95b1d00e":"markdown","7ee4fd67":"markdown","8ba15bea":"markdown"},"source":{"59e0e37e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import LabelEncoder,StandardScaler #(encoding and standardising data)\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","036eba40":"train_id = pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/train_identity.csv\")\ntrain_tran = pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/train_transaction.csv\")\ntest_id = pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/test_identity.csv\")\ntest_tran = pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/test_transaction.csv\")\nsample_submission = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/sample_submission.csv', index_col='TransactionID')","171aafef":"train = pd.merge(train_tran, train_id, on=\"TransactionID\", how=\"left\")\ntest = pd.merge(test_tran, test_id, on=\"TransactionID\", how=\"left\")\ntrain = pd.DataFrame(train)\ntest = pd.DataFrame(test)\ndel train_id, train_tran, test_id, test_tran","968dec9e":"train_y = train.isFraud\ntrain_x=train.drop([\"isFraud\"], axis=1)\ndel train","143e540a":"categorical = ['ProductCD','card1' ,'card2' ,'card3' ,'card4' ,'card5' ,'card6', 'addr1', 'addr2', 'P_emaildomain', 'R_emaildomain', 'M1'\n               ,'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'DeviceType', 'DeviceInfo', 'id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', \n               'id_18', 'id_19', 'id_20', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29', 'id_30', \n               'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38']\n\n\n\nlb_make = LabelEncoder()\n\nlength_cat = len(categorical)\nfor i in range(length_cat):\n    train_x[categorical[i]] = lb_make.fit_transform(train_x[categorical[i]].astype(str))\n    test[categorical[i]] = lb_make.fit_transform(test[categorical[i]].astype(str))\n   ","f62cc3e3":"train_x = StandardScaler().fit_transform(train_x)\ntest = StandardScaler().fit_transform(test)","0dda5c05":"model = XGBClassifier(boosting=\"gbdt\", max_depth=10, min_child_weight=3, subsample=0.9, colsample_bytree=0.9, gamma=0.4)\nmodel.fit(train_x, train_y)\npred = model.predict_proba(train_x)[:, 1]\nprint(\"Tuned\")\nprint(\"AUC\", roc_auc_score(train_y, pred))","c09ccbe5":"sample_submission['isFraud'] = model.predict_proba(test)[:, 1]\nsample_submission.to_csv('tuned_xgb3.csv')","d3281883":"This is my first notebook and submission to a kaggle competition. I took on this competition for a university project and have tested many different models including, logistic regression, random forest, svm and neural networks. I discovered out of these model tested my best result came from a random forest classifier. I tuned this model and improved it too a auc_roc_score of 0.92. Upon searching for more ways to improve this model I found gradient boosting models and tested both LGB and XGB with XGB providing larger auc scores. From here I began to tune the model by testing different stratified samples and parameters to come up with the result below. ","a9823bf0":"I am completing a more in-depth report for my university project and this is just the model I have found to be the strongest so far and I will continue to explore different models. Any tips for improving my model or problems please comment as I wish to gain as much knowledge from this experience as possible.","7a46fe27":"To apply the data to xgboost I label encode the data using sklearns labelencoder() method","2cc481d4":"I have tuned a model by selecting optimal, max_depth, min_child_weight, sub_sample and colsample_bytree I selected random forest as my boosting alogrithm as apart of my report process I explored the use of random forest and wanted to apply the xgboosting alogrithm to strengthen its learning ability. This model gave my best result so far. ","b77871b6":"Next I standardise the data to equalise the range and variability of the data ","861c57be":"Splitting the target variables for the trainning set & deleting unused dataset ","95b1d00e":"First Loading in the datasets","7ee4fd67":"* Merging the transaction and identity dataset for test and train.\n* Converting to a dataframe\n* Deleting unused datasets to save memory","8ba15bea":"Creating a submission for the competition"}}