{"cell_type":{"393cf50a":"code","52858d76":"code","2d78f312":"code","8ca95b3a":"code","287c9626":"code","bcef1e32":"code","4fedadd4":"code","a6c7777e":"code","09f4f818":"code","6e0c98fc":"code","df8fa08f":"code","23990dc9":"code","6602fe42":"code","e420d21c":"code","8b45c13f":"code","409fd3de":"code","0edaa6c8":"code","5af0bb4a":"code","dde3f2ad":"code","d1224223":"code","48e935bb":"code","065b74db":"code","11c60b30":"code","eef57900":"code","6eda497c":"code","14b07fc6":"code","be98e4e5":"code","d2b3d8ed":"code","2a1838d4":"code","ef1e7208":"code","bb45ba76":"code","0523ea46":"code","5d8d2729":"code","ff562f19":"code","aba84fd5":"code","f9195153":"code","c033b2dd":"code","b5d6e8d4":"code","a8e95197":"code","02041ebd":"code","fc5ff6af":"code","5bca7116":"code","a072ce54":"code","75dbe495":"code","b1645a23":"code","aa8194c5":"code","7c48e3ce":"code","fce8ed3e":"code","dee0c78c":"code","079c6a6b":"code","75ebe758":"code","ad2c1976":"code","58fa11a7":"code","5d263ad8":"code","6903944a":"code","100e8710":"code","45dabf9d":"code","561d9682":"code","e8274df0":"code","729ea2c4":"code","f7dfc74a":"code","7173f3eb":"code","6d666515":"code","96700494":"code","4f4c1f80":"code","b63bfadf":"code","18d7b98c":"code","cb9b51e3":"code","98368cbe":"code","288a05a8":"code","1ea05aca":"code","f9c5bf6f":"code","3dffe19b":"code","a4ba4010":"code","d41d912b":"code","8e7c25c2":"code","62c6eaa7":"code","0089e7fe":"code","8231c586":"code","88714c2f":"code","6539060f":"code","2fec932a":"code","aab8e573":"code","da24bc4a":"code","39eb0198":"code","1291ecdf":"code","fffc5ed4":"code","5d3a7a2b":"code","ceea2584":"code","92ad7404":"code","109fe2eb":"code","c91644a8":"code","0fb9a847":"code","3bc8fb3d":"code","4c428f70":"markdown","d42b20f1":"markdown","ac01e9da":"markdown","d91f22b5":"markdown","3e6667c6":"markdown","a32a1c80":"markdown","1078662e":"markdown","50c7f20e":"markdown","250dae39":"markdown","397389a6":"markdown","0406bc86":"markdown","2badf392":"markdown","477e5c23":"markdown","7b28ee1d":"markdown","64a4e6d2":"markdown","51ec51d7":"markdown","0cd2ef74":"markdown","cdbfe5d0":"markdown","3fa7e689":"markdown","462fef13":"markdown","810787ee":"markdown","6cbf6ad1":"markdown","42e9c740":"markdown","1f79847c":"markdown","4d02fa2c":"markdown","8967bb41":"markdown","2592df3c":"markdown","eedf696e":"markdown","d2970966":"markdown","941ada03":"markdown","663cc9e5":"markdown","7994cb8d":"markdown","d7fd8b39":"markdown","9002b037":"markdown","8208caca":"markdown","de7c475d":"markdown","c848e50e":"markdown","69aaf204":"markdown","115bd63c":"markdown","64194b52":"markdown","0b67cc13":"markdown","34995393":"markdown","339d5cda":"markdown"},"source":{"393cf50a":"import warnings\nwarnings.filterwarnings('ignore')","52858d76":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","2d78f312":"wine = pd.read_csv(\"..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv\")","8ca95b3a":"wine.head()","287c9626":"wine.info()","bcef1e32":"wine.describe()","4fedadd4":"wine.shape","a6c7777e":"round(100*(wine.isnull().sum()\/len(wine)),2).sort_values(ascending=False)","09f4f818":"round(100*(wine.isnull().sum(axis=1)\/len(wine)),2).sort_values(ascending=False)","6e0c98fc":"dub_wine=wine.copy()\ndub_wine.drop_duplicates(subset=None,inplace=True)","df8fa08f":"dub_wine.shape","23990dc9":"wine.shape","6602fe42":"wine=dub_wine","e420d21c":"for col in wine:\n    print(wine[col].value_counts(ascending=False), '\\n\\n\\n')","8b45c13f":"wine.shape","409fd3de":"wine.info()","0edaa6c8":"from sklearn.model_selection import train_test_split\nnp.random.seed(0)\ndf_train,df_test=train_test_split(wine,train_size=0.7,test_size=0.3,random_state=100)","5af0bb4a":"df_train.info()","dde3f2ad":"df_train.shape","d1224223":"df_test.info()","48e935bb":"df_train.shape","065b74db":"df_train.info()","11c60b30":"df_train.columns","eef57900":"sns.pairplot(df_train) \nplt.show()","6eda497c":"plt.figure(figsize=(20,25))\nsns.heatmap(wine.corr(), annot=True,cmap='RdBu')\nplt.show()","14b07fc6":"from sklearn.preprocessing import MinMaxScaler","be98e4e5":"scaler=MinMaxScaler()","d2b3d8ed":"df_train.head()","2a1838d4":"df_train.columns","ef1e7208":"df_train[:]=scaler.fit_transform(df_train[:])","bb45ba76":"df_train.head()","0523ea46":"y_train=df_train.pop('quality')\nX_train=df_train","5d8d2729":"from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression","ff562f19":"lm = LinearRegression()\nlm.fit(X_train, y_train)\nrfe = RFE(lm,9)             \nrfe = rfe.fit(X_train, y_train)","aba84fd5":"list(zip(X_train.columns,rfe.support_,rfe.ranking_))","f9195153":"col = X_train.columns[rfe.support_]\ncol","c033b2dd":"X_train.columns[~rfe.support_]","b5d6e8d4":"X_train_rfe = X_train[col]","a8e95197":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = pd.DataFrame()\nvif['Features'] = X_train_rfe.columns\nvif['VIF'] = [variance_inflation_factor(X_train_rfe.values, i) for i in range(X_train_rfe.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","02041ebd":"import statsmodels.api as sm\nX_train_lm1 = sm.add_constant(X_train_rfe)\nlr1 = sm.OLS(y_train, X_train_lm1).fit()","fc5ff6af":"lr1.params","5bca7116":"print(lr1.summary())","a072ce54":"X_train_new = X_train_rfe.drop([\"residual sugar\"], axis = 1)","75dbe495":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = pd.DataFrame()\nvif['Features'] = X_train_new.columns\nvif['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","b1645a23":"X_train_lm2 = sm.add_constant(X_train_new)\nlr2 = sm.OLS(y_train, X_train_lm2).fit()","aa8194c5":"lr2.params","7c48e3ce":"print(lr2.summary())","fce8ed3e":"X_train_new = X_train_new.drop([\"density\"], axis = 1)","dee0c78c":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = pd.DataFrame()\nvif['Features'] = X_train_new.columns\nvif['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","079c6a6b":"X_train_lm3 = sm.add_constant(X_train_new)\nlr3 = sm.OLS(y_train, X_train_lm3).fit()","75ebe758":"lr3.params","ad2c1976":"print(lr3.summary())","58fa11a7":"X_train_new = X_train_new.drop([\"free sulfur dioxide\"], axis = 1)","5d263ad8":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = pd.DataFrame()\nvif['Features'] = X_train_new.columns\nvif['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","6903944a":"X_train_lm4 = sm.add_constant(X_train_new)\nlr4 = sm.OLS(y_train, X_train_lm4).fit()","100e8710":"lr4.params","45dabf9d":"print(lr4.summary())","561d9682":"X_train_new = X_train_new.drop([\"pH\"], axis = 1)","e8274df0":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = pd.DataFrame()\nvif['Features'] = X_train_new.columns\nvif['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","729ea2c4":"X_train_lm5 = sm.add_constant(X_train_new)\nlr5 = sm.OLS(y_train, X_train_lm5).fit()","f7dfc74a":"lr5.params","7173f3eb":"print(lr5.summary())","6d666515":"X_train_new = X_train_new.drop([\"sulphates\"], axis = 1)","96700494":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = pd.DataFrame()\nvif['Features'] = X_train_new.columns\nvif['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","4f4c1f80":"X_train_lm6 = sm.add_constant(X_train_new)\nlr6 = sm.OLS(y_train, X_train_lm6).fit()","b63bfadf":"lr6.params","18d7b98c":"print(lr6.summary())","cb9b51e3":"X_train_new = X_train_new.drop([\"chlorides\"], axis = 1)","98368cbe":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = pd.DataFrame()\nvif['Features'] = X_train_new.columns\nvif['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","288a05a8":"X_train_lm7 = sm.add_constant(X_train_new)\nlr7 = sm.OLS(y_train, X_train_lm7).fit()","1ea05aca":"lr7.params","f9c5bf6f":"print(lr7.summary())","3dffe19b":"X_train_new = X_train_new.drop([\"total sulfur dioxide\"], axis = 1)","a4ba4010":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = pd.DataFrame()\nvif['Features'] = X_train_new.columns\nvif['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif\n\n","d41d912b":"X_train_lm8 = sm.add_constant(X_train_new)\nlr8 = sm.OLS(y_train, X_train_lm8).fit()\n\n","8e7c25c2":"lr8.params\n\n","62c6eaa7":"print(lr8.summary())","0089e7fe":"y_train_pred = lr8.predict(X_train_lm8)","8231c586":"res = y_train-y_train_pred\n# Plot the histogram of the error terms\nfig = plt.figure()\nsns.distplot((res), bins = 20)\nfig.suptitle('Error Terms', fontsize = 20)                  # Plot heading \nplt.xlabel('Errors', fontsize = 18)","88714c2f":"wine_num=wine[[ 'volatile acidity', 'alcohol', 'quality']]\n\nsns.pairplot(wine_num)\nplt.show()","6539060f":"vif = pd.DataFrame()\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nvif['Features'] = X_train_new.columns\nvif['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","2fec932a":"df_test[:]=scaler.fit_transform(df_test[:])","aab8e573":"df_test.head()","da24bc4a":"df_test.describe()","39eb0198":"y_test = df_test.pop('quality')\nX_test = df_test\nX_test.info()","1291ecdf":"#Selecting the variables that were part of final model.\ncol1=X_train_new.columns\nX_test=X_test[col1]\n# Adding constant variable to test dataframe\nX_test_lm8 = sm.add_constant(X_test)\nX_test_lm8.info()","fffc5ed4":"y_pred = lr8.predict(X_test_lm8)","5d3a7a2b":"fig = plt.figure()\nplt.scatter(y_test, y_pred, alpha=.5)\nfig.suptitle('y_test vs y_pred', fontsize = 20)              # Plot heading \nplt.xlabel('y_test', fontsize = 18)                          # X-label\nplt.ylabel('y_pred', fontsize = 16) \nplt.show()","ceea2584":"df= pd.DataFrame({'Actual':y_test,'Predictions':y_pred})\ndf['Predictions']= round(df['Predictions'],2)\ndf.head()","92ad7404":"sns.regplot('Actual','Predictions',data=df)","109fe2eb":"from sklearn.metrics import r2_score\nr2_score(y_test, y_pred)","c91644a8":"r2=0.32264089150785114\nX_test.shape","0fb9a847":"n = X_test.shape[0]\n\n\n# Number of features (predictors, p) is the shape along axis 1\np = X_test.shape[1]\n\n# We find the Adjusted R-squared using the formula\n\nadjusted_r2 = 1-(1-r2)*(n-1)\/(n-p-1)\nadjusted_r2","3bc8fb3d":"from sklearn import metrics\n\nprint('MAE:', metrics.mean_absolute_error(y_test, y_pred))\nprint('MSE:', metrics.mean_squared_error(y_test, y_pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))","4c428f70":"## Model 8 ","d42b20f1":"### Evaluating Model Performance:\nMean Absolute Error (MAE) is the mean of the absolute value of the errors.\n\nMean Squared Error (MSE) is the mean of the squared errors.\n\nRoot Mean Squared Error (RMSE) is the square root of the mean of the squared errors.","ac01e9da":"# *Data Split*","d91f22b5":"## **The shape after running the drop duplicate command is not same as the original dataframe.Hence we can conclude that there were duplicate values in the dataset.**","3e6667c6":"## Rescaling","a32a1c80":"Train R^2 :0.325\n\nTrain Adjusted R^2 :0.323\n\nTest R^2 :0.322\n\nTest Adjusted R^2 :0.319","1078662e":"We want the value of RMSE to be as low as possible, as lower the RMSE value is, the better the model is with its predictions","50c7f20e":"### Model 7","250dae39":"## Model 1","397389a6":"## Model 6","0406bc86":"### VIF","2badf392":"## **No missing \/ Null value in either rows or columns**","477e5c23":"### Final Result Comparison","7b28ee1d":"## Correlation Matrix","64a4e6d2":"### VIF","51ec51d7":"### Interpretation of Coefficients:\nvolatile acidity: A coefficient value of \u20180.3838\u2019 indicated that a unit decrease in volatite acidity variable, increases the quality numbers by 0.3828 units.\n\nalcohol: A coefficient value of \u20180.4148\u2019 indicated that a unit increase in alcohol variable, increases the quality numbers by 0.4148 units.","0cd2ef74":"### R^2 Value for TEST","cdbfe5d0":"# **EDA**","3fa7e689":"### Assign non duplicate data to original data ","462fef13":"## Model 3","810787ee":"## Hypothesis Testing:\n\nHypothesis testing states that:\n\nH0:B1=B2=...=Bn=0\nH1: at least one Bi!=0\n\nlr8 model coefficient values\n- const                0.4990\n- volatile acidity    -0.3828     \n- alcohol              0.4148 ","6cbf6ad1":"### VIF","42e9c740":"### Dividing into X_test and y_test","1f79847c":"### The equation of best fitted surface based on model lr8:\nquality = 0.4990 - (volatile acidity \u00d7 0.3828) + (alcohol \u00d7 0.4148 ) ","4d02fa2c":"# Building Linear Model","8967bb41":"## Model 5","2592df3c":"### VIF Check","eedf696e":"## There is a linear relationship between X and Y","d2970966":"## Model 2","941ada03":"### VIF","663cc9e5":"## Applying the scaling on the test sets","7994cb8d":"# MAKING PREDICTION USING FINAL MODEL","d7fd8b39":"# ASSUMPTIONS\n\n## Error terms are normally distributed with mean zero (not X, Y)\n\n### Residual Analysis Of Training Data","9002b037":"### Adjusted R^2 Value for TEST","8208caca":"### VIF","de7c475d":"### VIF","c848e50e":"## Quality Check ","69aaf204":"# Final Model Interpretation","115bd63c":"## **There seems to be no Junk\/Unknown values in the entire dataset**","64194b52":"### VIF","0b67cc13":"### F Statistics\nF-Statistics is used for testing the overall significance of the Model: Higher the F-Statistics, more significant the Model is.\n\nF-statistic:                     227.8\nProb (F-statistic):           1.68e-81\nThe F-Statistics value of 227.8 (which is greater than 1) and the p-value of '~0.0000' states that the overall model is significant","34995393":"### There is No Multicollinearity between the predictor variables","339d5cda":"## Model 4"}}