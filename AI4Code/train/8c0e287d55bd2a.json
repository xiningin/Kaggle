{"cell_type":{"a0ba656d":"code","17eaa84d":"code","be40692a":"code","8b156275":"code","9d3893c7":"code","19299392":"code","5d6695da":"code","cd5ca46b":"code","a6866a3f":"code","d6be9404":"code","2de9a105":"code","f7e64121":"code","84ccce68":"code","86da998f":"code","76f7d90b":"code","5fe7c13d":"code","07fa8cb8":"code","2cc9f03d":"code","b2ff65f8":"code","54eddeee":"code","3c173eb4":"code","9e4fb552":"code","32faadc9":"code","8e1f9d36":"code","763648a1":"code","08667ba3":"code","c09fa92d":"code","730bf562":"code","2576125a":"code","a6c9f18e":"code","5b56a21d":"code","9933600f":"markdown","5030b190":"markdown","c6b3220e":"markdown","252bcdc5":"markdown","5269abbc":"markdown","d1d7ae28":"markdown","4fdc3947":"markdown","3b052548":"markdown","7b5a5fb2":"markdown","2b3772f3":"markdown","19c3026f":"markdown","a9246333":"markdown","7eb58f0b":"markdown","11bb86dc":"markdown","5c60b3de":"markdown","5f716d8e":"markdown","4f3fa767":"markdown","93f2c05b":"markdown","5b761b80":"markdown","12f46f5a":"markdown","41e5155a":"markdown","f5726745":"markdown","90b928e2":"markdown","0253e8c2":"markdown","cc29e5ff":"markdown","0a5e8969":"markdown","9129b126":"markdown","566cb9d0":"markdown","c45546b0":"markdown","91275508":"markdown","450112f1":"markdown","7f6b7789":"markdown","b116f73d":"markdown","187a4fd0":"markdown","e2ce0e31":"markdown","50a3bcd5":"markdown","d5a24ccb":"markdown","f3b945af":"markdown","c119e88f":"markdown","37966b3a":"markdown","fca78a95":"markdown","3a18c9bd":"markdown"},"source":{"a0ba656d":"import numpy as np\nfrom scipy import stats\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nimport pandas as pd\nfrom IPython import display\n\npd.options.display.max_rows = 10\nplt.style.use(\"ggplot\")\nwarnings.filterwarnings(\"ignore\")","17eaa84d":"age = np.array([1,2,3,5,6,7,7,10,12,13])\n\n# Mean\nmean_age = np.mean(age)\nprint(mean_age)\n\n# Median\nmedian_age = np.median(age)\nprint(median_age)\n\n# Mode\nmode_age = stats.mode(age)\nprint(mode_age)","be40692a":"np.var(age) # Variance of the age array","8b156275":"np.std(age)  # Standart devitation of the age","9d3893c7":"y = np.random.uniform(5,8,100)\nx1 = np.random.uniform(10,20,100)\nx2 = np.random.uniform(0,30,100)\nplt.scatter(x1,y,color=\"black\")\nplt.scatter(x2,y,color=\"orange\")\nplt.xlim([-1,31])\nplt.ylim([2,11])\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nprint(\"X1 mean: {} and meadian: {}\".format(np.mean(x1),np.median(x1)))\nprint(\"X2 mean: {} and meadian: {}\".format(np.mean(x2),np.median(x2)))","19299392":"data = pd.read_csv(\"..\/input\/data.csv\")\ndata = data.drop(['Unnamed: 32','id'],axis = 1)\ndata.head()","5d6695da":"benign = data[data[\"diagnosis\"] == \"B\"]\nmalignant = data[data[\"diagnosis\"] == \"M\"]\ndesc = benign.radius_mean.describe()\ndesc","cd5ca46b":"Q1 = desc[4] # %25 value\nQ3 = desc[6] # %75 value\nIQR = Q3-Q1\nlower_bound = Q1 - 1.5*IQR\nupper_bound = Q3 + 1.5*IQR\nprint(\"Anything outside this range is an outlier: [{:5.3f}, {:5.3f}]\".format(lower_bound, upper_bound))","a6866a3f":"benign[benign.radius_mean < lower_bound].radius_mean","d6be9404":"print(\"Outliers: \", benign[(benign.radius_mean < lower_bound) | (benign.radius_mean > upper_bound)].radius_mean.values)","2de9a105":"melted_data = pd.melt(data, id_vars = \"diagnosis\", value_vars = ['radius_mean'])\n#   | diagnosis | variable  | value \n# 0 |         M| radius_mean| 17.99\n\nsns.boxplot(x='variable', y='value', data=melted_data, hue='diagnosis')\nplt.show()","f7e64121":"f, ax = plt.subplots(figsize=(18,18))\nsns.heatmap(data.corr(), annot=True, linewidths=0.5, fmt='.1f', ax=ax)\nplt.xticks(rotation=90)\nplt.yticks(rotation=0)\nplt.title('Correlation Map')\nplt.show()","84ccce68":"sns.jointplot(x='radius_mean', y='area_mean', data=data, kind='reg')\nsns.jointplot(x='radius_mean', y='fractal_dimension_mean', data=data, kind='reg')\nplt.show()","86da998f":"# Let's look at the relation with more than one data\ndf = data.loc[:,[\"radius_mean\",\"area_mean\",\"fractal_dimension_se\"]]","76f7d90b":"g = sns.PairGrid(df, diag_sharey=False)\ng.map_lower(sns.kdeplot, cmap='Blues_d') # kernel density plot\ng.map_upper(sns.scatterplot)\ng.map_diag(sns.kdeplot, lw=3)  # kernel density plot\nplt.show()","5fe7c13d":"# [cov(a,a)  cov(a,b)\n# cov(a,b)  cov(b,b)]\n# Numpy returns the result as 2x2 array. So select the diagonal ones.\nnp.cov(data.radius_mean, data.area_mean)[0][1]","07fa8cb8":"print(\"Covariance between radius mean and area mean: \",data.radius_mean.cov(data.area_mean))\nprint(\"Covariance between radius mean and fractal dimension se: \",data.radius_mean.cov(data.fractal_dimension_se))","2cc9f03d":"fig, (ax1, ax2) = plt.subplots(1, 2)\nsns.scatterplot(data.radius_mean, data.area_mean, ax=ax1)\nsns.scatterplot(data.fractal_dimension_se, data.radius_mean, ax=ax2)\nplt.tight_layout()\nplt.show()","b2ff65f8":"p1 = data.loc[:,[\"area_mean\",\"radius_mean\"]].corr(method= \"pearson\")\np2 = np.cov( data.radius_mean, data.area_mean)\/(data.radius_mean.std()*data.area_mean.std())","54eddeee":"p1","3c173eb4":"p2","9e4fb552":"sns.jointplot(data.radius_mean, data.area_mean, kind=\"reg\")\nplt.show()","32faadc9":"spear = data.loc[:,[\"area_mean\",\"radius_mean\"]].corr(method= \"spearman\")\nspear","8e1f9d36":"kendall = data.loc[:,[\"area_mean\",\"radius_mean\"]].corr(method= \"kendall\")\nkendall","763648a1":"def cohend(d1, d2):\n    # calculate the size of samples\n    n1, n2 = len(d1), len(d2)\n    # calculate the variance of the samples\n    s1, s2 = np.var(d1, ddof=1), np.var(d2, ddof=1)\n    # calculate the pooled standard deviation\n    s = np.sqrt(((n1 - 1) * s1 + (n2 - 1) * s2) \/ (n1 + n2 - 2))\n    # calculate the means of the samples\n    u1, u2 = np.mean(d1), np.mean(d2)\n    # calculate the effect size\n    return (u1 - u2) \/ s","08667ba3":"cohend(malignant.radius_mean, benign.radius_mean)","c09fa92d":"# dice example\na = np.random.randint(1,7,60000)\nprint(\"sample space: \",np.unique(a))\nplt.hist(a, bins=12) # bins =12 for pretty plot. Normally it is 6\nplt.ylabel(\"Number of outcomes\")\nplt.xlabel(\"Possible outcomes\")\nplt.show()","730bf562":"# dice rolling\nn = 2 # number of trials\np = 0.5 # probability of each trial\ns = np.random.binomial(n, p, size=10000) # 10000 = number of test\nweights = np.ones_like(s)\/float(len(s))\nplt.hist(s, weights=weights)\nplt.xlabel(\"number of success\")\nplt.ylabel(\"probability\")\nplt.show()","2576125a":"n = 10\nr = 4 # success\np = 1\/6 # success rate\n\nstats.binom.pmf(r,n,p) # probability mass function","a6c9f18e":"lamda = 3\ns1 = np.random.poisson(lamda, size=100000)\nweights1 = np.ones_like(s1)\/float(len(s1))\nplt.hist(s1, weights=weights1, bins = 100)\nplt.xlabel(\"number of occurances\") \nplt.ylabel(\"probability\")","5b56a21d":"# parameters of normal distribution\nmu, sigma = 110, 20  # mean and standard deviation\ns = np.random.normal(mu, sigma, size=100000)\nprint(\"mean: \", np.mean(s))\nprint(\"standart deviation: \", np.std(s))\n\n# visualize with histogram\nplt.figure(figsize = (10,7))\nplt.hist(s, 100, normed=False)\nplt.ylabel(\"frequency\")\nplt.show()","9933600f":"### Pearson Correlation Coeefficient (Pearson R)","5030b190":"#### Gaussian(Normal) Distributions","c6b3220e":"**Variance** \n  \n$\\sigma^{2}=\\sum\\frac{({X-\\mu})^{2}}{N}$  \n\n**Standart Deviation**  \n  \n$\\sigma=\\sqrt{\\sum\\frac{({X-\\mu})^{2}}{N}}$\n","252bcdc5":"## Probability Distributions","5269abbc":"### Continuous Probability Distributions","d1d7ae28":"In probability theory and statistics, the Poisson distribution is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time or space if these events occur with a known constant rate and independently of the time since the last event. The Poisson distribution can also be used for the number of events in other specified intervals such as distance, area or volume.\n\n* Binomial: number of successes out of n trials\n* Poisson: number of successes per unit of time\n* lambda = number of occurences \/ interval\n\n$P(X)=\\frac{\\lambda^{x} e^{-\\lambda}}{X!}$","4fdc3947":"In the analysis of bivariate data, one typically either compares summary statistics of each of the variables or uses regression analysis to find the strength and direction of a specific relationship between the variables. If each variable can only take one of a small number of values, such as only \"male\" or \"female\", or only \"left-handed\" or \"right-handed\", then the joint frequency distribution can be displayed in a contingency table, which can be analyzed for the strength of the relationship between the two variables.","3b052548":"#### Uniform Distributions","7b5a5fb2":"#### Binomial Distributions","2b3772f3":"Covariance is measure of the tendency of two variables to vary together.  \n* If two vectors are identical, then the covariance would be maximum.  \n* If two vectors are not identical, then the covariance would be zero.  \n* If two vectors are in different positions, then the covariance would be negative.  \n\n$Cov(x, y) = \\frac{\\sum (x_{i}-\\overline{x})*(y_{i}-\\overline{y})}{n}$","19c3026f":"### Variance, Standart Deviation","a9246333":"## Import Libraries","7eb58f0b":"### Kendall Rank Coefficient","11bb86dc":"## Dispersion","5c60b3de":"### Spearman Rank Coefficient","5f716d8e":"### Coveriance  ","4f3fa767":"## Quartiles","93f2c05b":"In statistics, dispersion (also called variability, scatter, or spread) is the extent to which a distribution is stretched or squeezed. Common examples of measures of statistical dispersion are the variance, standard deviation, and interquartile range.\nDispersion is contrasted with location or central tendency, and together they are the most used properties of distributions.","5b761b80":"In probability theory and statistics, the binomial distribution with parameters n and p is the discrete probability distribution of the number of successes in a sequence of n independent experiments, each asking a yes\u2013no question, and each with its own boolean-valued outcome: a random variable containing a single bit of information: success\/yes\/true\/one (with probability p) or failure\/no\/false\/zero (with probability q = 1 \u2212 p). A single success\/failure experiment is also called a `Bernoulli trial` or `Bernoulli` experiment and a sequence of outcomes is called a `Bernoulli process`; for a single trial, i.e., n = 1, the binomial distribution is a `Bernoulli distribution`. The binomial distribution is the basis for the popular binomial test of statistical significance.\n.\n    \n$\\left(\\begin{array}{c}n\\\\ r\\end{array}\\right)p^{r}(1-p)^{n-r}$\n\n\n* Bernoulli Trial:\n    * Two Outcomes: Success or failure \n    * n = number of trial\n    * p = probability of success\n    * r = number of success\n    * Trials are independent","12f46f5a":"## Finding Outliers with IQR","41e5155a":"### Mean, Median, Mode","f5726745":"Pearson correlation works well if the relationship between variables are linear and variables are roughly normal. But it is not robust, if there are outliers. The assumptions of the Spearman correlation are that data must be at least ordinal and the scores on one variable must be monotonically related to the other variable.","90b928e2":"A variable has one of four different levels of measurement: Nominal, Ordinal, Interval, or Ratio.  (Interval and Ratio levels of measurement are sometimes called Continuous or Scale).\n\nIn descending order of precision, the four different levels of measurement are:\n\n**Nominal**: Latin for name only (Republican, Democrat, Green, Libertarian)  \n**Ordinal**: Think ordered levels or ranks (small\u20138oz, medium\u201312oz, large\u201332oz)  \n**Interval**:Equal intervals among levels (1 dollar to 2 dollars is the same interval as 88 dollars to 89 dollars)  \n**Ratio**:Let the \u201co\u201d in ratio remind you of a zero in the scale (Day 0, day 1, day 2, day 3, \u2026)  \n \n\nThe first level of measurement is nominal level of measurement.  In this level of measurement, the numbers in the variable are used only to classify the data.  In this level of measurement, words, letters, and alpha-numeric symbols can be used. \n\nThe second level of measurement is the ordinal level of measurement.  This level of measurement depicts some ordered relationship among the variable\u2019s observations. The ordinal level of measurement indicates an ordering of the measurements.\n\nThe third level of measurement is the interval level of measurement.  The interval level of measurement not only classifies and orders the measurements, but it also specifies that the distances between each interval on the scale are equivalent along the scale from low interval to high interval. \n\nThe fourth level of measurement is the ratio level of measurement.  In this level of measurement, the observations, in addition to having equal intervals, can have a value of zero as well.  The zero in the scale makes this type of measurement unlike the other types of measurement, although the properties are similar to that of the interval level of measurement.  In the ratio level of measurement, the divisions between the points on the scale have an equivalent distance between them.\n","0253e8c2":"We can see from the graph that they are not exactly the same data. So these variables alone doesn't tell us if they are same or not.","cc29e5ff":"## Conclusions\n\nLet me know about your thoughts on my kernel :). I will be sharing part 2 where it will continue from here with more details soon.","0a5e8969":"## Bivariate and Coveriance","9129b126":"* One of the summary statistics.\n* It describes size of an effect. It is simple way of quantifying the difference between two groups.\n* In an other saying, effect size emphasises the size of the difference\n* Use cohen effect size\n* Cohen suggest that if d(effect size)= 0.2, it is small effect size, d = 0.5 medium effect size, d = 0.8 large effect size.\n\nd = $\\frac{\\overline{\\mu}_{1}-\\overline{\\mu}_{2}}{s}$  \n  \ns = $\\sqrt{\\frac{s_1^2*(n_{1}-1)+s_2^2*(n_{2}-1)}{n_{1}+n_{2}}}$","566cb9d0":"A uniform distribution, sometimes also known as a rectangular distribution, is a distribution that has constant probability. The probability density function and cumulative distribution function for a continuous uniform distribution on the interval are.","c45546b0":"#### Poisson Distributions","91275508":"Bivariate data is data on each of two variables, where each value of one of the variables is paired with a value of the other variable. Typically it would be of interest to investigate the possible association between the two variables. The association can be studied via a tabular or graphical display, or via sample statistics which might be used for inference. The method used to investigate the association would depend on the level of measurement of the variable.  \n\nFor two quantitative variables (ordinal, interval, or ratio in level of measurement) a scatterplot can be used and a correlation coefficient or regression model can be used to quantify the association. For two qualitative variables (nominal or ordinal in level of measurement) a contingency table can be used to view the data, and a measure of association or a test of independence could be used.  \n\nIf the variables are quantitative, the pairs of values of these two variables are often represented as individual points in a plane using a scatter plot. This is done so that the relationship (if any) between the variables is easily seen.  \n\n**Dependent and independent variables**  \n\nIn some instances of bivariate data, it is determined that one variable influences or determines the second variable, and the terms dependent and independent variables are used to distinguish between the two types of variables.  \n\nCorrelations between the two variables are determined as strong or weak correlations and are rated on a scale of \u20131 to 1, where 1 is a perfect direct correlation, \u20131 is a perfect inverse correlation, and 0 is no correlation. ","450112f1":"In Statistics, it is much more common to use pearson r rather than coveriance value. In fact, pearson r values is derived from coveriance by dividing it with standart deviation of variables. For the Pearson r correlation, both variables should be normally distributed (normally distributed variables have a bell-shaped curve).  Other assumptions include linearity and homoscedasticity.  Linearity assumes a straight line relationship between each of the two variables and homoscedasticity assumes that data is equally distributed about the regression line. \n\nCorrelation coefficient formulas are used to find how strong a relationship is between data. The formulas return a value between -1 and 1, where:  \n* 1 indicates a strong positive relationship.  \n* -1 indicates a strong negative relationship.  \n* A result of zero indicates no relationship at all.  ","7f6b7789":"* Spearman's correlation is little higher than pearson correlation\n    * If relationship between distributions are non linear, spearman's correlation tends to better estimate the strength of relationship\n    * Pearson correlation can be affected by outliers. Spearman's correlation is more robust. ","b116f73d":"In probability theory, the normal (or Gaussian or Gauss or Laplace\u2013Gauss) distribution is a very common continuous probability distribution. Normal distributions are important in statistics and are often used in the natural and social sciences to represent real-valued random variables whose distributions are not known. A random variable with a Gaussian distribution is said to be normally distributed and is called a normal deviate.\n\nThe normal distribution is useful because of the central limit theorem. In its most general form, under some conditions (which include finite variance), it states that averages of samples of observations of random variables independently drawn from independent distributions converge in distribution to the normal, that is, they become normally distributed when the number of observations is sufficiently large. Physical quantities that are expected to be the sum of many independent processes (such as measurement errors) often have distributions that are nearly normal. Moreover, many results and methods (such as propagation of uncertainty and least squares parameter fitting) can be derived analytically in explicit form when the relevant variables are normally distributed.\n\nThe normal distribution is sometimes informally called the bell curve. However, many other distributions are bell-shaped (such as the Cauchy, Student's t, and logistic distributions).\n\n$f(x|\\mu,\\sigma^{2}) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}e^{-\\frac{(x-\\mu^{2})}{2\\sigma^{2}}}$","187a4fd0":"## Level of Measurements","e2ce0e31":"## Central Tendency","50a3bcd5":" Is data same when both variance and standart deviations are same?","d5a24ccb":"## Effect Size (Cohen's d)","f3b945af":"### Discrete Probability Distributions","c119e88f":"The median is the number that is in middle of the sequence.  \nThe lower quartile(first quartile (Q1)(25%)) is the median in between the smallest number and the median  \nThe upper quartile(third quartile (Q3)(75%)), you find the median between the median and the largest number   \nIQR(inter quartile range) = Q3-Q1  \nOutliers: (Q1 - 1.5*IQR and Q3 + 1.5*IQR)  everything that is outside this one is considered as outlier.","37966b3a":"### Analysis of bivariate data ","fca78a95":"Kendall\u2019s Tau is a non-parametric measure of relationships between columns of ranked data. The Tau correlation coefficient returns a value of 0 to 1, where:  \n* 0 is no relationship,\n* 1 is a perfect relationship.","3a18c9bd":"## Correlation Types"}}