{"cell_type":{"d4b5418b":"code","ba49a0a9":"code","9b069289":"code","97f085db":"code","92c7043e":"code","b0b63112":"code","1c1574a8":"code","cd067b6d":"code","fcfd998b":"code","22504561":"code","1ce90327":"code","3cdfc389":"code","5b0829b0":"code","66205fd4":"code","08b97376":"code","95d2f57b":"code","f0564d9c":"code","b210390c":"code","3cf990da":"code","3414c156":"code","301f986e":"code","7f99c89b":"code","53eece10":"code","5941aa5a":"code","2751fffe":"code","f6287d3b":"code","be2f2bca":"code","9d0b4ed4":"code","7ff1d8e8":"code","3f32f43a":"code","d8e3141b":"code","79daed4f":"code","bf6f1f42":"code","60182ade":"code","24ca4340":"code","6613e59b":"code","7ed69bd7":"code","cc12e983":"code","3b980875":"code","0685acd7":"code","2a602cac":"code","0d1b7807":"code","fd98d09e":"code","275b7ecd":"code","291ca43a":"code","cdc0a680":"code","f60ed4d1":"code","ad7ae91d":"code","bd2139fc":"code","8f51ebef":"code","d14e1357":"code","7b632ad6":"code","b7029aa3":"code","26993707":"code","7a7823e7":"code","253c14f2":"code","9d9fc017":"code","ed3aafcb":"code","603341d7":"code","ff5c2be4":"code","1919ac8e":"code","95027996":"markdown","f0b7c32b":"markdown","6748e7ae":"markdown","5cd222fc":"markdown","a09364de":"markdown","5a2b667e":"markdown","aeeba0a4":"markdown","8217baad":"markdown","e57d7aab":"markdown","bbe0698c":"markdown","d7a454c8":"markdown","1680d812":"markdown","6228c3de":"markdown","c893c4af":"markdown","22b1d516":"markdown","49d37764":"markdown","01ed98ed":"markdown","4bc655ca":"markdown","0f95387e":"markdown","6922bdc9":"markdown","60d92049":"markdown","41a786c7":"markdown","e59867ec":"markdown","7ec933ff":"markdown","0146c18a":"markdown","baff8a1f":"markdown","2c77aee6":"markdown","b425e25a":"markdown","b16f343f":"markdown","63440d1e":"markdown","eb32a184":"markdown","a67987f1":"markdown","37f504f9":"markdown","bb28e75f":"markdown","96de6d73":"markdown","d2f57108":"markdown","2e6bab74":"markdown","e0838428":"markdown","94af9591":"markdown","21500ae2":"markdown","da168dc6":"markdown","bf999fc9":"markdown","502464d4":"markdown","c10d3295":"markdown","37ea9519":"markdown","4ee72bc6":"markdown","3d4ab77b":"markdown","3dda35f1":"markdown","d8950429":"markdown","a20004a7":"markdown","13a02a3b":"markdown","cf09abb8":"markdown","737eb49f":"markdown","bfe474c6":"markdown"},"source":{"d4b5418b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ba49a0a9":"!pip install captum\n!pip install pycaret-nightly","9b069289":"import numpy as np\nimport torch\n\nfrom captum.attr import IntegratedGradients\nfrom captum.attr import LayerConductance\nfrom captum.attr import NeuronConductance\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nfrom scipy import stats\nimport pandas as pd\nimport re\n\nfrom matplotlib import rcParams\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\nfrom sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn import preprocessing\nimport umap\nimport umap.plot\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.impute import SimpleImputer","97f085db":"train_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nsubmission_data = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')\ntitanic_df = pd.concat([train_data, test_data], ignore_index = True, sort = False)\ntr_idx = titanic_df['Survived'].notnull()\ntitanic_df_copy = titanic_df.copy()","92c7043e":"titanic_df['Has_Cabin'] = titanic_df['Cabin'].isnull().astype(int)\ntitanic_df['FamilySize'] = titanic_df['SibSp'] + titanic_df['Parch'] + 1\n\ntitanic_df['IsAlone'] = 0\ntitanic_df.loc[titanic_df['FamilySize'] == 1, 'IsAlone'] = 1\n\ntitanic_df['Cabin'] = titanic_df['Cabin'].fillna('N')\ntitanic_df['Cabin_label'] = titanic_df['Cabin'].str.get(0)\n\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n\ntitanic_df['Title'] = titanic_df['Name'].apply(get_title)\n\ntitanic_df['Title'] = titanic_df['Title'].replace(\n       ['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], \n       'Rare')\n\ntitanic_df['Title'] = titanic_df['Title'].replace('Mlle', 'Miss')\ntitanic_df['Title'] = titanic_df['Title'].replace('Ms', 'Miss')\ntitanic_df['Title'] = titanic_df['Title'].replace('Mme', 'Mrs')\n\ntitanic_df['Has_Age'] = titanic_df['Age'].isnull().astype(int)\n\nimputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\ntitanic_df[['Age_knn']] = imputer.fit_transform(titanic_df[['Age']])\n\nrobuster = RobustScaler()\ntitanic_df['Age_knn'] = robuster.fit_transform(titanic_df[['Age_knn']])\n\ntitanic_df.drop(['Age'],axis=1,inplace=True)\n\ntransformer = QuantileTransformer(n_quantiles=100, random_state=0, output_distribution='normal')\ntitanic_df['Fare'] = transformer.fit_transform(titanic_df[['Fare']])\n\nimp = SimpleImputer(missing_values=np.nan, strategy='mean')\ntitanic_df[['Fare']] = imp.fit_transform(titanic_df[['Fare']])\n\ntitanic_df['Fare_class'] = pd.qcut(titanic_df['Fare'], 5, labels=['F1', 'F2', 'F3','F4','F5' ])\ntitanic_df['Fare_class'] = titanic_df['Fare_class'].replace({'F1':1,'F2':2,'F3':3,'F4':4,'F5':5})\n\nimp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\ntitanic_df[['Embarked']] = imp.fit_transform(titanic_df[['Embarked']])","b0b63112":"from sklearn.decomposition import PCA","1c1574a8":"def apply_pca(X, standardize=True):\n    # Standardize\n    if standardize:\n        X = (X - X.mean(axis=0)) \/ X.std(axis=0)\n    # Create principal components\n    pca = PCA()\n    X_pca = pca.fit_transform(X)\n    # Convert to dataframe\n    component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n    X_pca = pd.DataFrame(X_pca, columns=component_names)\n    # Create loadings\n    loadings = pd.DataFrame(\n        pca.components_.T,  # transpose the matrix of loadings\n        columns=component_names,  # so the columns are the principal components\n        index=X.columns,  # and the rows are the original features\n    )\n    return pca, X_pca, loadings","cd067b6d":"def outlier_iqr(data):\n    q1,q3 = np.percentile(data,[25,75])\n    iqr = q3-q1\n    lower = q1-(iqr*1.5)\n    upper = q3+(iqr*1.5)\n    return np.where((data>upper)|(data<lower))","fcfd998b":"def encode_features(dataDF,feat_list):\n    for feature in feat_list:\n        le = preprocessing.LabelEncoder()\n        le = le.fit(dataDF[feature])\n        dataDF[feature] = le.transform(dataDF[feature])\n        \n    return dataDF","22504561":"features = [\"Sex\",\"Age_knn\",\"FamilySize\",\"IsAlone\",'Embarked','Cabin_label']\ntitanic_copy = titanic_df[tr_idx].copy()\ny_copy = titanic_copy.pop(\"Survived\")\nX_copy = titanic_copy.loc[:, features]\nencode_features(X_copy,['Sex', 'Embarked','Cabin_label'])\npca, X_pca, loadings = apply_pca(X_copy)\nprint(loadings)","1ce90327":"import plotly.express as px\nfig = px.histogram(X_pca.melt(), color=\"variable\", \n                   marginal=\"box\",\n                   barmode =\"overlay\",\n                   histnorm ='density'\n                  )  \nfig.update_layout(\n    title_font_color=\"black\",\n    legend_title_font_color=\"green\",\n    title={\n        'text': \"PCA Histogram\",\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'},\n)","3cdfc389":"pc1_outlier_idx = list(outlier_iqr(X_pca['PC1'])[0])","5b0829b0":"component = \"PC1\"\n\ndef highlight_min(s, props=''):\n    return np.where(s == np.nanmin(s.values), props, '')\n\ntrain_data.iloc[pc1_outlier_idx,:].style.set_properties(**{'background-color': 'Grey',\n                            'color': 'white',\n                            'border-color': 'darkblack'})","66205fd4":"titanic_df = pd.get_dummies(titanic_df, columns = ['Title','Sex', 'Embarked','Cabin_label'],drop_first=True)","08b97376":"def drop_features(df):\n    df.drop(['Name','Ticket','SibSp','Parch','Fare_class',\n             'Cabin','Cabin_label_G','Cabin_label_T',\n             'Cabin_label_F','FamilySize','Embarked_Q','Title_Rare','PassengerId'],\n            axis=1,\n            inplace=True)\n    return df\n\ntitanic_df = drop_features(titanic_df)","95d2f57b":"corr=titanic_df.corr().round(1)\nsns.set(font_scale=1.15)\nplt.figure(figsize=(14, 10))\nsns.set_style(\"white\")\nsns.set_palette(\"bright\")\nabs(corr['Survived']).sort_values()[:-1].plot.barh()\nplt.gca().set_facecolor('#FFFFFF')","f0564d9c":"corr=titanic_df.corr().round(1)\n\nsns.set(font_scale=1.15)\nplt.figure(figsize=(14, 10))\nsns.set_style(\"white\")\nsns.set_palette(\"bright\")\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corr,annot=True,cmap='Blues',mask=mask,cbar=True)\nplt.title('Correlation Plot')","b210390c":"sns.set(font_scale=2)\nplt.figure(figsize=(14, 10))\nsns.set_style(\"white\")\nsns.set_palette(\"bright\")\nsns.pairplot(titanic_df,kind = 'reg',corner = True,palette ='Blues',hue='Survived' )","3cf990da":"tr_idx = titanic_df['Survived'].notnull()\ny_titanic_df = titanic_df[tr_idx]['Survived']\nX_titanic_df= titanic_df[tr_idx].drop('Survived',axis=1)\nX_test_df = titanic_df[~tr_idx].drop('Survived',axis=1)","3414c156":"train_final = titanic_df[tr_idx]\nall_cols = [cname for cname in X_titanic_df.columns]","301f986e":"train_final.shape","7f99c89b":"mapper = umap.UMAP().fit(X_titanic_df) \numap.plot.points(mapper, labels=y_titanic_df, theme='fire')","53eece10":"umap.plot.connectivity(mapper, show_points=True)","5941aa5a":"from umap import UMAP\nimport plotly.express as px\n\numap_3d = UMAP(n_components=3, init='random', random_state=0)\nx_umap = umap_3d.fit_transform(X_titanic_df)\numap_df = pd.DataFrame(x_umap)\ntrain_y_sr = pd.Series(y_titanic_df,name='label').astype(str)\nprint(type(x_umap))\nnew_df = pd.concat([umap_df,train_y_sr],axis=1)\nfig = px.scatter_3d(\n    new_df, x=0, y=1, z=2,\n    color='label', labels={'color': 'label'},\n    opacity=0.7\n)\nfig.update_traces(marker_size=1.5)\nfig.show()","2751fffe":"from pycaret.classification import *\nclf1 = setup(data = train_final, \n             target = 'Survived',\n             preprocess = False,\n             numeric_features = all_cols,\n             silent=True)","f6287d3b":"catboost = create_model('catboost')\nrf = create_model('rf')\nlightgbm = create_model('lightgbm')\ngbc = create_model('gbc')\nlda = create_model('lda')\nlr = create_model('lr')","be2f2bca":"tuned_rf = tune_model(rf, optimize = 'Accuracy',early_stopping = True)\ntuned_lightgbm = tune_model(lightgbm, optimize = 'Accuracy',early_stopping = True)\ntuned_catboost = tune_model(catboost, optimize = 'Accuracy',early_stopping = True)\ntuned_gbc = tune_model(gbc, optimize = 'Accuracy',early_stopping = True)\ntuned_lda = tune_model(lda, optimize = 'Accuracy',early_stopping = True)\ntuned_lr = tune_model(lr, optimize = 'Accuracy',early_stopping = True)","9d0b4ed4":"interpret_model(lightgbm)","7ff1d8e8":"interpret_model(catboost)","3f32f43a":"interpret_model(rf)","d8e3141b":"blend_soft = blend_models(estimator_list = [lr,rf,lightgbm,catboost,gbc,lda], optimize = 'Accuracy',method = 'soft')","79daed4f":"cali_model = calibrate_model(blend_soft)","bf6f1f42":"final_model = finalize_model(cali_model)","60182ade":"plt.figure(figsize=(8, 8))\nplot_model(final_model, plot='boundary')","24ca4340":"plt.figure(figsize=(7, 7))\nplot_model(final_model, plot='confusion_matrix')","6613e59b":"plt.figure(figsize=(8, 8))\nplot_model(final_model, plot = 'auc')","7ed69bd7":"last_prediction = final_model.predict(X_test_df)\nsubmission_data['Survived'] = last_prediction.astype(int)\nsubmission_data.to_csv('submission.csv', index = False)","cc12e983":"def drop_features(df):\n    df.drop(['Ticket','PassengerId','Cabin'],\n            axis=1,\n            inplace=True,\n            errors='ignore')\n    return df\n\ntitanic_dl_df = drop_features(titanic_df_copy)\ntitanic_dl_df['Pclass'] = titanic_dl_df['Pclass'].astype(object)","3b980875":"def replace_name(name):\n    if \"Mr.\" in name: return \"Mr\"\n    elif \"Mrs.\" in name: return \"Mrs\"\n    elif \"Miss.\" in name: return \"Miss\"\n    elif \"Master.\" in name: return \"Master\"\n    elif \"Ms.\": return \"Ms\"\n    else: return \"No\"\ntitanic_dl_df['Name'] = titanic_dl_df['Name'].apply(replace_name)","0685acd7":"imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\ntitanic_dl_df[['Age']] = imputer.fit_transform(titanic_dl_df[['Age']])\n\ntransformer = QuantileTransformer(n_quantiles=100, random_state=0, output_distribution='normal')\ntitanic_dl_df['Fare'] = transformer.fit_transform(titanic_dl_df[['Fare']])\n\nmean_imp = SimpleImputer(missing_values=np.nan, strategy='mean')\ntitanic_dl_df[['Fare']] = mean_imp.fit_transform(titanic_dl_df[['Fare']])\n\nfreq_imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\ntitanic_dl_df[['Embarked']] = freq_imp.fit_transform(titanic_dl_df[['Embarked']])","2a602cac":"titanic_dl_df = pd.get_dummies(titanic_dl_df,drop_first=True)","0d1b7807":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ntitanic_dl_df.loc[:,'Age':] = scaler.fit_transform(titanic_dl_df.loc[:,'Age':])","fd98d09e":"train_data = titanic_dl_df[tr_idx]\ntest_data = titanic_dl_df[~tr_idx]","275b7ecd":"test_data.drop('Survived',axis = 1,inplace=True)\ntest_data = test_data.to_numpy()","291ca43a":"# Set random seed for reproducibility.\nnp.random.seed(42)\n\n# Convert features and labels to numpy arrays.\nlabels = train_data[\"Survived\"].to_numpy().astype(bool)\ntrain_data = train_data.drop(['Survived'], axis=1)\nfeature_names = list(train_data.columns)\ndata = train_data.to_numpy()\n\n# Separate training and test sets using \ntrain_indices = np.random.choice(len(labels), int(0.8*len(labels)), replace=False)\ntest_indices = list(set(range(len(labels))) - set(train_indices))\ntrain_features = data[train_indices]\ntrain_labels = labels[train_indices]\nval_features = data[test_indices]\nval_labels = labels[test_indices]","cdc0a680":"import torch\nimport torch.nn as nn\ntorch.manual_seed(1)  # Set seed for reproducibility.\nclass TitanicNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = nn.Linear(13, 13)\n        self.BatchNorm1d1 = nn.BatchNorm1d(13, affine=False)\n        self.drop1 = nn.Dropout(0.3)\n        self.act1 = nn.ReLU()\n        self.linear2 = nn.Linear(13, 13)\n        self.BatchNorm1d2 = nn.BatchNorm1d(13, affine=False)\n        self.drop2 = nn.Dropout(0.3)\n        self.act2 = nn.ReLU()\n        self.linear3 = nn.Linear(13, 13)\n        self.BatchNorm1d3 = nn.BatchNorm1d(13, affine=False)\n        self.drop3 = nn.Dropout(0.3)\n        self.act3 = nn.ReLU()\n        self.linear4 = nn.Linear(13, 8)\n        self.BatchNorm1d4 = nn.BatchNorm1d(8, affine=False)\n        self.drop4 = nn.Dropout(0.3)\n        self.act4 = nn.ReLU()\n        self.linear5 = nn.Linear(8, 8)\n        self.BatchNorm1d5 = nn.BatchNorm1d(8, affine=False)\n        self.drop5 = nn.Dropout(0.3)\n        self.act5 = nn.ReLU()\n        self.linear6 = nn.Linear(8, 2)\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        lin1_out = self.linear1(x)\n        lin1_out = self.BatchNorm1d1(lin1_out)\n        lin1_out = self.drop1(lin1_out)\n        act1_out = self.act1(lin1_out)\n        lin2_out = self.linear2(act1_out)\n        lin2_out = self.BatchNorm1d2(lin2_out)\n        lin2_out = self.drop2(lin2_out)\n        act2_out = self.act2(lin2_out)\n        lin3_out = self.linear3(act2_out)\n        lin3_out = self.BatchNorm1d3(lin3_out)\n        lin3_out = self.drop3(lin3_out)\n        act3_out3 = self.act3(lin3_out)\n        lin4_out = self.linear4(act3_out3)\n        lin4_out = self.BatchNorm1d4(lin4_out)\n        lin4_out = self.drop4(lin4_out)\n        act4_out4 = self.act4(lin4_out)\n        lin5_out = self.linear5(act4_out4)\n        lin5_out = self.BatchNorm1d5(lin5_out)\n        lin5_out = self.drop4(lin5_out)\n        act_out5 = self.act5(lin5_out)\n        return self.softmax(self.linear6(act_out5))","f60ed4d1":"net = TitanicNet()\nprint(net)","ad7ae91d":"criterion = nn.CrossEntropyLoss()\nnum_epochs = 2000\n\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\ninput_tensor = torch.from_numpy(train_features).type(torch.FloatTensor)\nlabel_tensor = torch.from_numpy(train_labels)\nfor epoch in range(num_epochs):    \n    output = net(input_tensor)\n    loss = criterion(output, label_tensor.long())\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    if epoch % 20 == 0:\n        print ('Epoch {}\/{} => Loss: {:.2f}'.format(epoch+1, num_epochs, loss.item()))","bd2139fc":"out_probs = net(input_tensor).detach().numpy()\nout_classes = np.argmax(out_probs, axis=1)\nprint(\"Train Accuracy:\", sum(out_classes == train_labels) \/ len(train_labels))","8f51ebef":"val_input_tensor = torch.from_numpy(val_features).type(torch.FloatTensor)\nout_probs = net(val_input_tensor).detach().numpy()\nout_classes = np.argmax(out_probs, axis=1)\nprint(\"Test Accuracy:\", sum(out_classes == val_labels) \/ len(val_labels))","d14e1357":"ig = IntegratedGradients(net)\nval_input_tensor.requires_grad_()\nattr, delta = ig.attribute(val_input_tensor,target=1, return_convergence_delta=True)\nattr = attr.detach().numpy()","7b632ad6":"def visualize_importances(feature_names, importances, title=\"Average Feature Importances\", axis_title=\"Features\"):\n    x_pos = (np.arange(len(feature_names)))\n    plt.figure(figsize=(12,6))\n    sns.barplot(x_pos, importances)\n    plt.xticks(x_pos, feature_names, rotation=90)\n    plt.xlabel(axis_title)\n    plt.title(title)\n    sns.despine()\n    \nvisualize_importances(feature_names, np.mean(attr, axis=0))","b7029aa3":"sns.histplot(attr[:,3]);\nplt.title(\"Distribution of Fare Attribution Values\");\nsns.despine()","26993707":"sns.histplot(attr[:,10]);\nplt.title(\"Distribution of Sex_male Attribution Values\");\nsns.despine()","7a7823e7":"cond = LayerConductance(net, net.act1)\ncond_vals = cond.attribute(val_input_tensor,target=1)\ncond_vals = cond_vals.detach().numpy()","253c14f2":"visualize_importances(range(13),np.mean(cond_vals, axis=0),\n                      title=\"Average Neuron Importances\",\n                      axis_title=\"Neurons\")","9d9fc017":"sns.set(font_scale = 1.5)\nsns.set_style(\"white\")\nsns.set_palette(\"bright\")\nplt.figure(figsize=(20, 30))\nplt.subplots_adjust(hspace=1)\nfor neuron in range(13):\n    plt.subplot(7,2,neuron+1)\n    mean = cond_vals[:,neuron].mean()\n    std = cond_vals[:,neuron].std()\n    sns.histplot(cond_vals[:,neuron],color='blue');\n    plt.title(f\"Neuron {neuron} : Mean = {mean:.2f}, Std = {std:.2f}\")\n    sns.despine()","ed3aafcb":"neuron_cond = NeuronConductance(net, net.act1)","603341d7":"sns.set(font_scale = 1.5)\nsns.set_style(\"white\")\nsns.set_palette(\"bright\")\nplt.figure(figsize=(20, 40))\nplt.subplots_adjust(hspace=2)\nfor neuron in range(13):\n    neuron_cond_vals = neuron_cond.attribute(val_input_tensor, neuron_selector=neuron, target=1)\n    plt.subplot(7,2,neuron+1)\n    x_pos = (np.arange(len(feature_names)))\n    ax = sns.barplot(x_pos, neuron_cond_vals.mean(dim=0).detach().numpy())\n    ax.set_xticks(x_pos, feature_names, rotation=90)\n    ax.set_title(f\"Average Feature Importances for Neuron {neuron}\")\n    sns.despine()","ff5c2be4":"test_input_tensor = torch.from_numpy(test_data).type(torch.FloatTensor)\nout_probs = net(test_input_tensor).detach().numpy()\nout_classes = np.argmax(out_probs, axis=1)\nsubmission_data['Survived'] = out_classes\nsubmission_data.to_csv('dl_submission.csv', index = False)","1919ac8e":"submission_data","95027996":"Here we cover the following:\n* **Are features in neural networks important?**\n* **Which neuron plays an important role in a specific layer?**\n* **What are the distributional differences between important and non-significant neurons?**\n* **What are the important features for each neuron?**","f0b7c32b":"**In this case, we choose net.act1, the output of the first hidden layer.**","6748e7ae":"----------------------------------------------------------------\n## Training\n\n![](https:\/\/machinelearningknowledge.ai\/wp-content\/uploads\/2019\/10\/Backpropagation.gif)\n\nPicture Credit: https:\/\/machinelearningknowledge.ai\n\nIn the case of DL, the weight of each neuron (node) is adjusted through error backpropagation. This process is the most important part of the process of learning. Through this gradient descent process, we can understand which neurons play an important role and which features are important.","5cd222fc":"------------------------------------------------------------------\n## Understanding the importance of all the neurons in the output of a particular layer\n\n> An artificial neuron is a mathematical function conceived as a model of biological neurons, a neural network. Artificial neurons are elementary units in an artificial neural network.The artificial neuron receives one or more inputs (representing excitatory postsynaptic potentials and inhibitory postsynaptic potentials at neural dendrites) and sums them to produce an output (or activation, representing a neuron's action potential which is transmitted along its axon). Usually each input is separately weighted, and the sum is passed through a non-linear function known as an activation function or transfer function. The transfer functions usually have a sigmoid shape, but they may also take the form of other non-linear functions, piecewise linear functions, or step functions.\n\nRef: https:\/\/en.wikipedia.org\/wiki\/Artificial_neuron","a09364de":"<span style=\"color:Blue\"> **Observation**:\n    \n* The importance of each neuron's survival rate varies with different magnitudes.","5a2b667e":"<span style=\"color:Blue\"> Observation:\n* The Sage family started from S port, there was no Cabin, and the ages were not recorded, and they appear to be a poor and pitiful family with a pclass 3 rating.\n* All three females in this family using the Miss title have died.\n\n**The last sad news is that the training dataset is small, so it seems difficult to remove even if the above data are outliers.    \nNevertheless, fortunately, this problem is not a regression problem, but a classification problem. If it is a regression problem, outliers should be removed.**","aeeba0a4":"## Finalizing Model","8217baad":"## Calibrating Models","e57d7aab":"The first layer of our neural network has 13 neurons. \n\n**Let's visualize which neuron played an important role among the 13 neurons.**","bbe0698c":"## Sex_male Attribution Distribution\n\nLet's draw a distribution of Sex_male attribution that has a negative relationship with the survival rate.","d7a454c8":"----------------------------------------------------------------------------------------------------------------------\n# Classic Machine Learning","1680d812":"In this notebook, we will try to find out the differences between the classic machine learning method and the deep learning method through the titanic problem. In particular, I would like to focus on the deep learning method.","6228c3de":"-------------------------------------------------------------------------------------\n## Preprocessing","c893c4af":"**Let's check out outliers in PC 1**","22b1d516":"Most of the classic ML processes can be thought of as the process of determining the boundary as shown in the figure above. Keep this picture in mind for later comparison with the learning process of DL.","49d37764":"If you enlarge or rotate the picture above, you can see how the preprocessed dataset is mapped in 3D. The preprocessed dataset is 17-dimensional, so we cannot visualize the preprocessed dataset.\n\nHowever, it can be expected that clustering will be possible to some degree even when viewed in the above three-dimensional view.","01ed98ed":"--------------------------------------------\n## Encoding and Scaling","4bc655ca":"### Dropout\n\n![](https:\/\/miro.medium.com\/max\/1100\/1*WPr205gm0CsQXGa0oltOew.gif)\n\nPicture Credit: https:\/\/miro.medium.com\n\n**Dropout**\n\n> **Dilution (also called Dropout) is a regularization technique for reducing overfitting in artificial neural networks by preventing complex co-adaptations on training data.** It is an efficient way of performing model averaging with neural networks. The term dilution refers to the thinning of the weights.The term dropout refers to randomly \"dropping out\", or omitting, units (both hidden and visible) during the training process of a neural network. Both the thinning of weights and dropping out units trigger the same type of regularization, and often the term dropout is used when referring to the dilution of weights.\n\nRef: https:\/\/en.wikipedia.org\/\n\nDropout ensures that only certain neurons are not trained, so that the diversity of the neural network is not reduced. In addition, it maintains generality by preventing the neural network from overfitting to a specific dataset.\nIt seems safe to say that it creates an ensemble effect of a neural network.","0f95387e":"<hr style=\"border: solid 3px blue;\">\n\n# Deep Learing\n\n\n![](https:\/\/cdnb.artstation.com\/p\/assets\/images\/images\/010\/538\/265\/original\/dane-vranes-datascience-optimized.gif?1524951477)\n\nPicture Credit: https:\/\/cdnb.artstation.com\n\nOne of the advantages of DL is that the preprocessing process is simpler than that of classic ML.\nAnd, one of the disadvantages is that it is difficult to explain the model.\nIn particular, this notebook summarizes the methods for explaining DL.\n\nCompared to classic ML in DL, modeling and hyperparameter selection are more important than preprocessing.","6922bdc9":"-----------------------------------------------------------------------------------------------------------------\n# Interpreting Neural Networks\n\n![](https:\/\/d1aueex22ha5si.cloudfront.net\/Conference\/1263\/BackGround\/brain-pic5_animated_synapses-1-1629371355924.gif)\nPicture Credit: https:\/\/d1aueex22ha5si.cloudfront.net\n\n\n**neural network\/neurons**\n\n> A neural network is a network or circuit of neurons, or in a modern sense, an artificial neural network, composed of artificial neurons or nodes. Thus a neural network is either a biological neural network, made up of biological neurons, or an artificial neural network, for solving artificial intelligence (AI) problems. The connections of the biological neuron are modeled in artificial neural networks as weights between nodes. A positive weight reflects an excitatory connection, while negative values mean inhibitory connections. All inputs are modified by a weight and summed. This activity is referred to as a linear combination. Finally, an activation function controls the amplitude of the output. For example, an acceptable range of output is usually between 0 and 1, or it could be \u22121 and 1\n\nRef: https:\/\/en.wikipedia.org\/wiki\/Neural_network\n","60d92049":"The operation of ensemble in ML can be thought of as using the diversity of other models as above. Looking at the above figures, different models have different feature importance. Let's also remember this picture for comparison with DL.","41a786c7":"---------------------------------------------------------------------------------------\n# Testing using Test Dataset","e59867ec":"### Batch Normalization Layers\n\n![](https:\/\/miro.medium.com\/max\/1548\/1*1HNT2c2bAu37RgxNCCxFZw.gif)\n\nPicture Credit: https:\/\/miro.medium.com\n\n> Batch normalization (also known as batch norm) is a method used to make artificial neural networks faster and more stable through normalization of the layers' inputs by re-centering and re-scaling.\n\nRef: https:\/\/en.wikipedia.org\/","7ec933ff":"## Interpreting Models","0146c18a":"--------------------------------------------------------------------------------------------------\n# Detecting Outliers by PCA\n\n![](https:\/\/miro.medium.com\/max\/920\/1*v9VXKpHGnrtt96voOTt7nQ.gif)\n\nPicture Credit: https:\/\/miro.medium.com\n\nIn the case of Classic ML, outliers must be identified and removed to improve performance. Therefore, outlier removal is an important process for classic ML.\n\nThe more features, the higher the dimension. When projecting to a lower dimension through PCA, new insights can be gained. PCA can effectively detect outliers.\n\nPC 1 has the largest variance in the dataset distribution. That is, the outlier in PC 1 is very likely to be real outlier","baff8a1f":"<span style=\"color:Blue\"> **Observation**:\n    \n* The importance of each neuron's survival rate varies with different magnitudes.","2c77aee6":"In this notebook, we want to do classic ML using ensemble. Through ensemble, weak models learn differently to form collective intelligence. In this case, modeling is done using soft voting method using soft blending.\n\nThe ensemble learning method is observed in the learning method of DL in some way. DL will be explained later.","b425e25a":"----------------------------------------------------------------------------------------------\n# Modeling\n\n![](https:\/\/machinelearningknowledge.ai\/wp-content\/uploads\/2019\/12\/Bagging-Bootstrap-Aggregation.gif)\n\nPicture Credit: https:\/\/machinelearningknowledge.ai\n","b16f343f":"-----------------------------------------------------------------------------------------------------------------------\n# Setting Up","63440d1e":"From the feature attribution information, we obtain some interesting insights regarding the importance of various features.\n\n<span style=\"color:Blue\"> **Observation**:\n    \n* The Name_Mr and Sex_male features have a strong negative relationship and affect the survival rate.\n* The Fare feature has a strong positive relationship and affects the survival rate.\n* The effect on Age is not as big as I thought.\n* SibSp feature had little effect on survival rate.  ","eb32a184":"<span style=\"color:Blue\"> **Observation**:    \n* If you look at the distribution, you can see a long tail on the left.\n* This distribution is consistent with the fact that many of the Males died.","a67987f1":"-----------------------------------------------------------------------------\n# Feature Engineering\n\n> Feature engineering is the process of using domain knowledge to extract features (characteristics, properties, attributes) from raw data.\n> \n> **The feature engineering process is:**\n> * Brainstorming or testing features;\n> * Deciding what features to create;\n> * Creating features;\n> * Testing the impact of the identified features on the task;\n> * Improving your features if needed;\n> * Repeat.\n\nRef: https:\/\/en.wikipedia.org\/wiki\/Feature_engineering\n\nThe below feature engineering process actually requires a lot of thought and time. The purpose of this notebook is to compare ML and DL, so we will omit the EDA process during the preprocessing process. If you are curious about the additional EDA process, please refer to the notebook below.\n\n[titanic-missing-and-small-data-are-disaster](https:\/\/www.kaggle.com\/ohseokkim\/titanic-missing-and-small-data-are-disaster)","37f504f9":"### Fare Attribution Distribution\n\nLet's draw a distribution of Fare attribution that has a positive relationship with the survival rate.","bb28e75f":"-----------------------------------------------------------------\n## Visualizing Training Dataset after Dimension Reduction\n\nBefore training, let's draw a scaled down titanic dataset in 2D and 3D.","96de6d73":"# Conclusion\n\nWhat choices did you make?\n\nI think I will use the classic ML method for the titanic dataset. The reason for this judgment is as follows.\n* Titanic dataset is small in size. DL seems to learn generalized knowledge more effectively when the dataset is large.\n* There are many missing values in the Titanic dataset, so a feature engineering process to fill in the missing values through EDA seems to be necessary. DL requires little or no preprocessing, which is a common orthodoxy.\n\n<hr style=\"border: solid 3px blue;\">","d2f57108":"![](https:\/\/miro.medium.com\/max\/1400\/1*Da7wVx5j1KcSJ-I4DVFZyQ.png)\n\nPicture Credit: https:\/\/miro.medium.com\n\nClassic Machine Learning requires feature engineering. Also, with good feature engineering, the model can learn better. Therefore, when using classic ML, it is necessary to observe the dataset in detail and perform good preprocessing based on it.","2e6bab74":"---------------------------------------------------------------------------------------\n## Which of the features were actually important to the model to reach this decision?\n\nIn this case, let's check what features are important for making a decision that the target value (Survived) is true in our designed neural network.","e0838428":"<span style=\"color:Blue\"> **Classical ML** \n\n> **Advantages:**\n> * More suitable for small data\n> * Easier to interpret outcomes\n> * Cheaper to perform\n> * Can run on low-end machines\n> * Does not require large computational power\n\n> **Disadvantages:**\n> * Difficult to learn large datasets\n> * Require feature engineering\n> * Difficult to learn complex functions\n\n<span style=\"color:Blue\"> **Deep learning**\n\n> **Advantages:**\n> * Suitable for high complexity problems\n> * Better accuracy, compared to classical ML\n> * Better support for big data\n> * Complex features can be learned\n\n> **Disadvantages:**\n> * Difficult to explain trained data\n> * Require significant computational power","94af9591":"Looking at the above figures, the importance of features that each neutron judges to be important are different. It looks similar to how weak models of classic ML's ensambles have different feature importance.\n\nThis diversity of neurons seems to be one of the factors that make DL strong. If each neuron does not have diversity, the performance of the neural network will be reduced. In this case, it seems that modeling needs to be done again.","21500ae2":"## Visualizing Feature Importances","da168dc6":"<hr style=\"border: solid 3px blue;\">\n\n# Introduction\n\n![](https:\/\/thumbs.gfycat.com\/InexperiencedLongAmphibian-max-1mb.gif)\n\nPicture Credit: https:\/\/thumbs.gfycat.com","bf999fc9":"--------------------------------\n## Encoding","502464d4":"## Tuning Hyperparmaters","c10d3295":"-----------------------------------------------------------\n## Preprecessing for DL\n\nCompared to Classic ML, the process of Feature Engineering is simple.\n\n**In most cases, the following steps are required.**\n* Handling missing values\n* Encoding for categorical features (one-hot encoding is mainly used).\n* Standard or Min-Max Scaling","37ea9519":"<span style=\"color:Blue\"> **Observation**:    \n* If you look at the distribution, you can see a long tail on the right.\n* This distribution is consistent with the fact that people who pay high rates are more likely to survive.","4ee72bc6":"----------------------------------------------------------------------------------\n## Understanding what parts of the input contribute to activating a particular input neuron","3d4ab77b":"For the Titanic problem, since the dataset size is small, classical ML seems to be more suitable.\nHowever, we will try to solve the titanic problem in two ways without any bias.\n\n**However, please keep that in mind. The final choice is up to you.\nWe look forward to making a wise choice for you.**","3dda35f1":"------------------------------------------------\n## Selecting Features\n\nIn the case of classic ML, the process of selecting features is required.\nHowever, this process is not necessary for deep learning because important features are naturally selected through the erre backpropagation process.\n\n","d8950429":"## Creating Models","a20004a7":"------------------------------------------------------------------------\n## Modeling\n\nIn neural networks, the model process is very important. An appropriate model should be designed according to the problem to be solved. In some cases, a well-trained model and related parameters are used. For the Titanic problem, we decided to use a simple model consisting of fully connected layers.\n\nDropout and Batch Normalization Layers are used here.","13a02a3b":"----------------------------------------------------------------------\n## Separating Traing and Test Datasets","cf09abb8":"## Blending Models","737eb49f":"**Let's plot the distributions of values \u200b\u200bfor all neurons in the first layer.**","bfe474c6":"## Setting Up"}}