{"cell_type":{"08928d9b":"code","81fd33ed":"code","e5da0712":"code","aceb0fe2":"code","54059712":"code","a06444d0":"code","b628ab2e":"code","db8d7bee":"code","aa5d7e9f":"code","02d60b57":"code","d7227986":"code","dc57ddc7":"code","adca4b18":"code","cc7dbf2f":"code","1a7cab21":"code","907edfcf":"code","8f48e5a2":"code","ff306216":"code","ff2f3cb9":"code","50f17791":"code","ec14ef73":"code","e094c559":"code","fece4c51":"code","d4bf47be":"code","82c021c9":"code","3cc2b862":"code","4eaead3a":"markdown","c625b28a":"markdown","9ea8b032":"markdown","08a3951d":"markdown","70565c3b":"markdown"},"source":{"08928d9b":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport matplotlib.style as style\nfrom scipy import stats\n\nstyle.use('ggplot')\nsns.set_style('whitegrid')\n\nimport warnings\nwarnings.filterwarnings('ignore')","81fd33ed":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\nprint('test shape: ',test.shape)\nprint('train shape:',train.shape)\n","e5da0712":"train.drop(\"Id\", axis=1,inplace=True)\ntest.drop(\"Id\", axis=1,inplace=True)\nprint(train.corr()['SalePrice'].sort_values(ascending=False))","aceb0fe2":"style.use('fivethirtyeight')\nplt.scatter(train['GrLivArea'], train['SalePrice'])\nplt.ylabel('SalePrice')\nplt.xlabel('GrLivArea')","54059712":"train = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<200000)].index)","a06444d0":"style.use('ggplot')\nfig,ax = plt.subplots()\nstats.probplot(train['SalePrice'],plot=ax)","b628ab2e":"train.SalePrice = np.log1p(train.SalePrice)\nsns.distplot(train['SalePrice'], norm_hist=True)","db8d7bee":"\n# fig,ax = plt.subplots(nrows=int(len(numerical)\/3)+1,ncols=3,figsize=(20,60))\n# i=0\n# j=0\n# for col in numerical:\n#     ax[i,j].scatter(train[col],train['SalePrice'])\n#     ax[i,j].set_xlabel(col,fontsize=15)\n#     ax[i,j].set_ylabel('SalePrice',fontsize=15)\n#     j+=1\n#     if j>2:\n#         j=0\n#         i+=1\n\n\n# plt.subplots(figsize=(30,20))\n# sns.heatmap(train.corr(), cmap=sns.diverging_palette(20, 220, n=200), annot=True,center=0)","aa5d7e9f":"print(train.isnull().sum().sort_values(ascending = False)[train.isnull().sum().sort_values(ascending = False) != 0])\nprint(test.isnull().sum().sort_values(ascending = False)[test.isnull().sum().sort_values(ascending = False) != 0])","02d60b57":"y = train['SalePrice']\ntrain.drop('SalePrice', axis=1,inplace=True)","d7227986":"all_data = pd.concat((train,test)).reset_index(drop = True)\nall_data.isnull().sum().sort_values(ascending = False)[all_data.isnull().sum().sort_values(ascending = False) != 0]","dc57ddc7":"# missing values with type none\nmiss_val_1 = [\"Alley\",\n              \"PoolQC\", \n              \"MiscFeature\",\n              \"Fence\",\n              \"FireplaceQu\",\n              \"GarageType\",\n              \"GarageFinish\",\n              \"GarageQual\",\n              \"GarageCond\",\n              'BsmtQual',\n              'BsmtCond',\n              'BsmtExposure',\n              'BsmtFinType1',\n              'BsmtFinType2',\n              'MasVnrType']\n\n\n#missing values with type 0\nmiss_val_2=['BsmtFinSF1',\n            'BsmtFinSF2',\n            'BsmtUnfSF',\n            'TotalBsmtSF',\n            'BsmtFullBath', \n            'BsmtHalfBath', \n            'GarageYrBlt',\n            'GarageArea',\n            'GarageCars',\n            'MasVnrArea']\n\nfor i  in miss_val_1:\n    all_data[i]=all_data[i].fillna('None')\n    \nfor i in miss_val_2:\n    all_data[i]=all_data[i].fillna(0)","adca4b18":"all_data.isnull().sum().sort_values(ascending = False)[all_data.isnull().sum().sort_values(ascending = False) != 0]","cc7dbf2f":"all_data['LotFrontage'] = all_data.groupby('Neighborhood')['LotFrontage'].transform( lambda x: x.fillna(x.mean()))\nall_data['MSSubClass'] = all_data['MSSubClass'].astype(str)\nall_data['MSZoning'] = all_data.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)\nall_data['Functional'] = all_data['Functional'].fillna('Typ') \nall_data['Utilities'] = all_data['Utilities'].fillna('AllPub') \nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0]) \nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\nall_data['KitchenQual'] = all_data['KitchenQual'].fillna(\"TA\") \nall_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\nall_data['Electrical'] = all_data['Electrical'].fillna(\"SBrkr\")","1a7cab21":"all_data.isnull().sum().sort_values(ascending = False)[all_data.isnull().sum().sort_values(ascending = False) != 0]","907edfcf":"numerical = all_data.dtypes[all_data.dtypes != \"object\"].index","8f48e5a2":"from scipy.stats import skew\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\nskew = all_data[numerical].apply(lambda x: skew(x)).sort_values(ascending=False)\n\nskew_index = skew[abs(skew)>0.5].index\n\nfor feat in skew_index:\n        all_data[feat] = boxcox1p(all_data[feat], boxcox_normmax(all_data[feat] + 1))","ff306216":"from sklearn.preprocessing import LabelEncoder\ncat = all_data.dtypes[all_data.dtypes == \"object\"].index\nfor i in cat:\n    all_data[i].dtype\n    le = LabelEncoder()\n    all_data[i] = le.fit_transform(all_data[i])\n    all_data[i].dtype","ff2f3cb9":"X = all_data.iloc[:len(y), :]\n\nX_sub = all_data.iloc[len(y):, :]","50f17791":"def cv_rmse(model, X=X):\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=kfolds))\n    return (rmse)","ec14ef73":"from xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import cross_val_score, KFold\n\nkfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n\nalphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\ne_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\n\nridge = make_pipeline(RobustScaler(), RidgeCV(alphas=alphas_alt, cv=kfolds))\nlasso = make_pipeline(RobustScaler(), LassoCV(max_iter=1e7, alphas=alphas2, random_state=42, cv=kfolds))\nelasticnet = make_pipeline(RobustScaler(), ElasticNetCV(max_iter=1e7, alphas=e_alphas, cv=kfolds, l1_ratio=e_l1ratio))                                \nsvr = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003,))\n\n\nxgboost = XGBRegressor(learning_rate=0.01,n_estimators=3460,\n                                     max_depth=3, min_child_weight=0,\n                                     gamma=0, subsample=0.7,\n                                     colsample_bytree=0.7,\n                                     nthread=-1,\n                                     scale_pos_weight=1, seed=27,\n                                     reg_alpha=0.00006)\n\nlightgbm = LGBMRegressor(objective='regression', \n                                       num_leaves=4,\n                                       learning_rate=0.01, \n                                       n_estimators=5000,\n                                       max_bin=200, \n                                       bagging_fraction=0.75,\n                                       bagging_freq=5, \n                                       bagging_seed=7,\n                                       feature_fraction=0.2,\n                                       feature_fraction_seed=7,\n                                       verbose=-1,\n                                       )\n\n","e094c559":"score = cv_rmse(ridge)\nprint(\"Ridge: {:.4f} \\n\".format(score.mean()))\n\nscore = cv_rmse(lasso)\nprint(\"LASSO: {:.4f} \\n\".format(score.mean()))\n\nscore = cv_rmse(elasticnet)\nprint(\"elastic net: {:.4f} \\n\".format(score.mean()))\n\nscore = cv_rmse(svr)\nprint(\"SVR: {:.4f} \\n\".format(score.mean()))\n\nscore = cv_rmse(lightgbm)\nprint(\"lightgbm: {:.4f} \\n\".format(score.mean()))\n\nscore = cv_rmse(xgboost)\nprint(\"xgboost: {:.4f} \\n\".format(score.mean()))","fece4c51":"elastic = elasticnet.fit(X, y)\n\nlasso = lasso.fit(X, y)\n\nridge = ridge.fit(X, y)\n\nsvr = svr.fit(X, y)\n\nxgb = xgboost.fit(X, y)\n\nlgb = lightgbm.fit(X, y)","d4bf47be":"def models_predict(X):\n    return ((0.15 * elastic.predict(X)) + \\\n            (0.15 * lasso.predict(X)) + \\\n            (0.15 * ridge.predict(X)) + \\\n            (0.15 * svr.predict(X)) + \\\n            (0.20 * xgb.predict(X)) + \\\n            (0.20 * lgb.predict(X)))","82c021c9":"submission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\nsubmission.iloc[:,1] = np.expm1(models_predict(X_sub))\nsubmission.to_csv(\"submission111.csv\", index=False)","3cc2b862":"# sub_1 = pd.read_csv('..\/input\/lasso-model-for-regression-problem\/lasso_sol22_Median.csv')\n# sub_2 = pd.read_csv('..\/input\/hybrid-svm-benchmark-approach-0-11180-lb-top-2\/hybrid_solution.csv')\n\n# submission.iloc[:,1] = np.floor((0.50 * models_predict(X_sub)) + \n#                                 (0.25 * sub_1.iloc[:,1]) +\n#                                 (0.25 * sub_2.iloc[:,1]))\n# submission.to_csv(\"submission222.csv\", index=False)","4eaead3a":"overallqual is a categorical feature so we are using grlivarea","c625b28a":"positive skewed. so normalizing saleprice","9ea8b032":"we have to check  **1** linear relationship between target and feature\n                            we will do this by using scatter plot to plot target against feature it is highly correlated with\n                            by this we can manage outliers in dataset","08a3951d":"here we can see 2 outliers easily , dropping those outliers","70565c3b":"**2** now checking for normal distribution of error terms"}}