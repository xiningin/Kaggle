{"cell_type":{"1ea916c1":"code","b5f3959a":"code","d0bebcef":"code","9a9254c0":"code","f044fb9e":"code","4bb9ac3e":"code","49abc6c7":"code","e6f14be3":"code","6c1f8861":"code","1bc453c3":"code","602e0adc":"code","2d9480dd":"code","1125b7bb":"code","484bd0b5":"code","324282bc":"code","70306610":"code","9282384c":"code","cff11513":"code","ed5beb86":"code","0ef53fb4":"code","58a72e51":"code","df42da48":"code","bc522785":"code","faf90c4c":"code","698343c1":"code","964b3b70":"code","77ae333a":"markdown","34ab099c":"markdown","4d7f0db7":"markdown","319f7a5a":"markdown","af58c02e":"markdown","27d5bf19":"markdown","c46a2cce":"markdown","dc188791":"markdown","c0ddf287":"markdown","4a8dc938":"markdown","5585c837":"markdown","c3272542":"markdown","639647aa":"markdown","415b8871":"markdown","2d161944":"markdown","574769a6":"markdown","d1916cc3":"markdown","e20dbe2c":"markdown"},"source":{"1ea916c1":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport category_encoders as ce\nfrom itertools import combinations\n\nfrom sklearn import metrics\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import BayesianRidge\nimport sklearn.preprocessing as prep\nimport sklearn.impute as skimp\n\nimport catboost as cb\n\nprint(\"Pandas\",pd.__version__)\nprint(\"NumPy\",np.__version__)\nprint(\"Seaborn\",sns.__version__)\nprint(\"Category Encoders\", ce.__version__)\n\nprint('CatBoost',cb.__version__)","b5f3959a":"!python -V","d0bebcef":"sns.set_style('darkgrid')\nSEED = 123\nnp.random.seed(SEED)\n\npd.set_option('display.max_columns', None)\npd.set_option('display.float_format', lambda x: '%.3f' % x)\nnp.set_printoptions(suppress=True)","9a9254c0":"train = pd.read_csv('..\/input\/pro-data-2020\/train.csv')\ntest = pd.read_csv('..\/input\/pro-data-2020\/test.csv')\n\ndf_train = train.copy(deep=True)\ndf_train.head()\n\nprint(\"Number of fully duplicated rows:\", df_train.duplicated().sum())","f044fb9e":"def drop_dups(X):\n    df = X.copy(deep=True)\n    df = df.drop_duplicates()\n    return df\n\nduplicates_transformer = prep.FunctionTransformer(func=drop_dups)\n\ndef remove_price_outliers(X, max_price=142000):\n    df = X.copy(deep=True)\n    df = df[df['\u0426\u0435\u043d\u0430 \u0432 \u0434\u043e\u043b\u043b\u0430\u0440\u0430\u0445'] <= max_price]\n    return df\n\nprice_outlier_transformer = prep.FunctionTransformer(\n    func=remove_price_outliers)\n\ndef engine_cap(X, max_engine_size=10):\n    df = X.copy(deep=True)\n    engine_size_outlier_models = df.loc[\n        df['\u041e\u0431\u044a\u0435\u043c \u0434\u0432\u0438\u0433\u0430\u0442\u0435\u043b\u044f'] > max_engine_size, \"\u041c\u043e\u0434\u0435\u043b\u044c\"]\n    g = df.groupby(\"\u041c\u043e\u0434\u0435\u043b\u044c\", observed=True)[\"\u041e\u0431\u044a\u0435\u043c \u0434\u0432\u0438\u0433\u0430\u0442\u0435\u043b\u044f\"].value_counts()\n    \n    for idx, model in engine_size_outlier_models.iteritems():\n        if g[model].max() > 1:\n            df.loc[idx, \"\u041e\u0431\u044a\u0435\u043c \u0434\u0432\u0438\u0433\u0430\u0442\u0435\u043b\u044f\"] = g[model].idxmax()\n        else:\n            df.loc[idx, \"\u041e\u0431\u044a\u0435\u043c \u0434\u0432\u0438\u0433\u0430\u0442\u0435\u043b\u044f\"] = df['\u041e\u0431\u044a\u0435\u043c \u0434\u0432\u0438\u0433\u0430\u0442\u0435\u043b\u044f'].median()\n            \n    return df\n\nengine_cap_transformer = prep.FunctionTransformer(func=engine_cap)\n\ndef brand_model_clean(X):\n    \n    df = X.copy(deep=True)\n    # Brand assignment based on model majority.\n    # The following brand\/model discrepancies were adjusted in favor of the\n    # brand with the majority of given models.\n    df.loc[(df['\u0411\u0440\u0435\u043d\u0434'] == \"AAD\") & (df['\u041c\u043e\u0434\u0435\u043b\u044c'] == \"WMN\"), '\u0411\u0440\u0435\u043d\u0434'] = 'ABJ'\n    df.loc[(df['\u0411\u0440\u0435\u043d\u0434'] == \"ACK\") & (df['\u041c\u043e\u0434\u0435\u043b\u044c'] == \"XDQ\"), '\u0411\u0440\u0435\u043d\u0434'] = 'ABO'\n    df.loc[(df['\u0411\u0440\u0435\u043d\u0434'] == \"ABN\") & (df['\u041c\u043e\u0434\u0435\u043b\u044c'] == \"XIA\"), '\u0411\u0440\u0435\u043d\u0434'] = 'AAZ'\n    \n    # Brand assignment based on model and body type majority.\n    # The following brand\/model discrepancies were adjusted in favor of the\n    # brand with the majority of the given body type (e.g. if brand AAN has\n    # more cars listed with the same body type as the car at question,\n    # then the model is assigned to brand AAN)\n    df.loc[(df['\u0411\u0440\u0435\u043d\u0434'] == \"ACO\") & (df['\u041c\u043e\u0434\u0435\u043b\u044c'] == \"WQB\"), '\u0411\u0440\u0435\u043d\u0434'] = 'AAN'\n    df.loc[(df['\u0411\u0440\u0435\u043d\u0434'] == \"AAH\") & (df['\u041c\u043e\u0434\u0435\u043b\u044c'] == \"MSD\"), '\u0411\u0440\u0435\u043d\u0434'] = 'ACO'\n    df.loc[(df['\u0411\u0440\u0435\u043d\u0434'] == \"AAE\") & (df['\u041c\u043e\u0434\u0435\u043b\u044c'] == \"BRU\"), '\u0411\u0440\u0435\u043d\u0434'] = 'ABQ'\n\n    return df\n\nbrandmodel_cleaner = prep.FunctionTransformer(func=brand_model_clean)\n\ndef color_body_clean(X):\n    df = X.copy(deep=True)\n    \n    # Create a new feature from color\n    x = df[\"\u0426\u0432\u0435\u0442\"].str.contains('\u043c\u0435\u0442\u0430\u043b\u043b\u0438\u043a')\n    df['Metallic'] = x.map({False:0,True:1})\n    \n    # Adjust color for redundancy\n    df['\u0426\u0432\u0435\u0442'] = df['\u0426\u0432\u0435\u0442'].str.replace(' \u043c\u0435\u0442\u0430\u043b\u043b\u0438\u043a', '')\n    df.loc[df['\u0426\u0432\u0435\u0442'] == \"\u0437\u043e\u043b\u043e\u0442\u0438\u0441\u0442\u044b\u0439\", \"\u0426\u0432\u0435\u0442\"] = \"\u0437\u043e\u043b\u043e\u0442\u043e\u0439\"\n    df.loc[df['\u0426\u0432\u0435\u0442'] == \"\u0432\u0438\u0448\u043d\u044f\", \"\u0426\u0432\u0435\u0442\"] = \"\u0432\u0438\u0448\u043d\u0435\u0432\u044b\u0439\"\n    \n    # Latin c got mixed up with Cyrillic c\n    df.loc[df['\u041a\u0443\u0437\u043e\u0432'] == 'c\u0435\u0434\u0430\u043d', '\u041a\u0443\u0437\u043e\u0432'] = '\u0441\u0435\u0434\u0430\u043d'\n    \n    # Merge redundant categories\n    df.loc[df['\u041a\u0443\u0437\u043e\u0432'].isin([\"\u043c\u0438\u043a\u0440\u043e\u0430\u0432\u0442\u043e\u0431\u0443\u0441\", \"\u043c\u0438\u043a\u0440\u043e\u0432\u044d\u043d\"]),\n           '\u041a\u0443\u0437\u043e\u0432'] = \"\u043c\u0438\u043d\u0438\u0432\u044d\u043d \/ \u0431\u0443\u0441\"\n    df.loc[df['\u041a\u0443\u0437\u043e\u0432'] == '\u043b\u0438\u0444\u0442\u0431\u044d\u043a', '\u041a\u0443\u0437\u043e\u0432'] = '\u0445\u044d\u0442\u0447\u0431\u0435\u043a\/\u043b\u0438\u0444\u0442\u0431\u0435\u043a'\n    df.loc[df['\u041a\u0443\u0437\u043e\u0432'] == '\u0432\u043d\u0435\u0434\u043e\u0440\u043e\u0436\u043d\u0438\u043a', '\u041a\u0443\u0437\u043e\u0432'] = '\u0432\u043d\u0435\u0434\u043e\u0440\u043e\u0436\u043d\u0438\u043a \/ \u043f\u0438\u043a\u0430\u043f'\n    \n    # Merge the underrepresented classes into one class - Other\n    df.loc[df['\u041a\u0443\u0437\u043e\u0432'].\n           isin([\"\u0444\u0443\u0440\u0433\u043e\u043d\", \"\u043a\u0430\u0431\u0440\u0438\u043e\u043b\u0435\u0442\", \"\u043b\u0438\u043c\u0443\u0437\u0438\u043d\", \"\u0440\u043e\u0434\u0441\u0442\u0435\u0440\", \"\u0444\u0430\u0441\u0442\u0431\u044d\u043a\"]),\n           '\u041a\u0443\u0437\u043e\u0432'] = \"\u0434\u0440\u0443\u0433\u043e\u0435\"\n    return df\n\ncolor_body_cleaner = prep.FunctionTransformer(func=color_body_clean)\n\ndef clean_yr(X):\n    X.loc[X['\u0413\u043e\u0434'] > 2020, '\u0413\u043e\u0434'] = X['\u0413\u043e\u0434'].median()\n    return X\n\nyear_cleaner = prep.FunctionTransformer(func=clean_yr)\n\ndrop = Pipeline(steps=[\n    ('dups', duplicates_transformer),\n    ('clip',price_outlier_transformer),\n])\n\ndf_level_pipe = Pipeline(steps=[\n    ('enginecap', engine_cap_transformer),\n    ('brandmodelclean', brandmodel_cleaner),\n    ('color_body_cleaner', color_body_cleaner)\n])\n\nonehot_features = df_train.iloc[:,12:].columns.tolist()\nall_cols = df_train.columns.tolist()\n\nyear_transformers = Pipeline(steps=[\n    ('year_cleaners', year_cleaner),\n    ('year_imp', skimp.IterativeImputer(\n        estimator=BayesianRidge(),initial_strategy='median',\n        random_state=SEED)),\n])\n\ncolumn_level_transformer = ColumnTransformer(transformers=[\n    ('price_bc', prep.PowerTransformer(method='box-cox', standardize=False), \n     ['\u0426\u0435\u043d\u0430 \u0432 \u0434\u043e\u043b\u043b\u0430\u0440\u0430\u0445']),\n    ('ohe_trans', skimp.SimpleImputer(\n        strategy='constant', fill_value=0), onehot_features),\n    ('year_trans', year_transformers, ['\u0413\u043e\u0434'])\n], remainder='passthrough', n_jobs=-1)\n\ndef toDF(X):\n    remainder_columns = all_cols[1:12]\n    remainder_columns.remove('\u0413\u043e\u0434')\n    pipe1_feature_names = ['Price_bc'] + onehot_features + ['Year'] + remainder_columns + ['Metallic']\n    df = pd.DataFrame.from_records(X, columns=pipe1_feature_names,\n                                   coerce_float=True)\n    return df\n\nconvert_todf = prep.FunctionTransformer(func=toDF)\n\ndef concatenate(df):\n    X = df.copy(deep=True)\n    X['Brand_Model'] = X['\u0411\u0440\u0435\u043d\u0434'] + \"_\" + X[\"\u041c\u043e\u0434\u0435\u043b\u044c\"]\n    return X\n\ncon = prep.FunctionTransformer(func= concatenate, check_inverse=False)\n\npipe1 = Pipeline([\n    ('drop_rows', drop),\n    ('clean', df_level_pipe),\n    ('price_year_ohe', column_level_transformer),\n    ('toDF', convert_todf),\n    ('con_brand_model', con)\n])\n\npipe1_df = pipe1.fit_transform(df_train)\n\npipe2 = Pipeline(steps=[\n    ('glmm_enc', ce.GLMMEncoder(random_state=SEED, randomized=True, \n               handle_missing='return_nan',  handle_unknown='return_nan')),\n    ('imp', skimp.IterativeImputer(\n    estimator=BayesianRidge(verbose=0, ),\n    max_iter=10, n_nearest_features=15, verbose=1, \n    initial_strategy='median', random_state=SEED))\n])\n\npipe2_df = pipe2.fit_transform(\n    pipe1_df, pipe1_df.Price_bc)\n\npipe2_df = pd.DataFrame(pipe2_df, columns=pipe1_df.columns)\n\ny = pipe2_df.Price_bc\nX = pipe2_df.drop(columns='Price_bc')\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.1765, \n                                                    random_state=SEED)\n\ncb_reg = cb.CatBoostRegressor(random_seed=SEED,\n                              custom_metric='R2', #task type GPU won't work\n                              verbose=500,\n                              loss_function='RMSE',\n                              eval_metric='R2',\n                              iterations=25000, # converges around 10496\n                              use_best_model=True,\n                              learning_rate=0.03,\n                                     )\ndata_train = cb.Pool(X_train, y_train)\ndata_test = cb.Pool(X_test, y_test)\ncb_reg.fit(X=data_train, eval_set=data_test, plot=True, verbose=500, \n           save_snapshot=False)\n\nprint(cb_reg.score(X_test, y_test))\nprint('X_test r2 score:',metrics.r2_score(y_test, cb_reg.predict(X_test)))\ny_pred_25k = cb_reg.predict(X_test)\ny_pred_25k_inv = pipe1.named_steps.price_year_ohe.named_transformers_.price_bc.inverse_transform(y_pred_25k.reshape(-1,1))\ny_test_inv = pipe1.named_steps.price_year_ohe.named_transformers_.price_bc.inverse_transform(y_test.ravel().reshape(-1,1))\nprint('X_test r2 score after inversion:',metrics.r2_score(y_test_inv, y_pred_25k_inv))\n\ncb_reg.save_model('cb_reg_25k.dump')\n\ndef test_pipe(X):\n    '''Function to preprocess the test dataset for the regressor.'''\n    df = X.copy(deep=True)\n    df = pipe1.named_steps.clean.transform(df)\n    df = pipe1.named_steps.price_year_ohe.transform(df)\n    df = pipe1.named_steps.toDF.transform(df)\n    df = pipe1.named_steps.con_brand_model.transform(df)\n    df_complete = pipe2.transform(df)\n    df_complete = pd.DataFrame(df_complete, columns=df.columns)\n    \n    return df_complete    \n\ndf_test = test_pipe(test)\n\nX_val = df_test.drop(columns='Price_bc')\ny_val = df_test['Price_bc']\n\nprint('R2 on X_val:', cb_reg.score(X_val,y_val))\ny_pred = cb_reg.predict(X_val)\ny_pred_inv = pipe1.named_steps.price_year_ohe.named_transformers_.price_bc.inverse_transform(y_pred.reshape(-1,1))\ny_val_inv = pipe1.named_steps.price_year_ohe.named_transformers_.price_bc.inverse_transform(y_val.ravel().reshape(-1,1))\nassert np.allclose(y_val_inv.ravel(), test['\u0426\u0435\u043d\u0430 \u0432 \u0434\u043e\u043b\u043b\u0430\u0440\u0430\u0445'].ravel())\nprint('Inverted R2 on X_val:', metrics.r2_score(y_val_inv, y_pred_inv))\n\nsubmission = pd.DataFrame({'Id':X_val.index,'Predicted':y_pred_inv.ravel()})\nsubmission.to_csv('my_submission4.csv',index=False)","4bb9ac3e":"def drop_dups(X):\n    df = X.copy(deep=True)\n    df = df.drop_duplicates()\n    return df\n\nduplicates_transformer = prep.FunctionTransformer(func=drop_dups)","49abc6c7":"def remove_price_outliers(X, max_price=142000):\n    df = X.copy(deep=True)\n    df = df[df['\u0426\u0435\u043d\u0430 \u0432 \u0434\u043e\u043b\u043b\u0430\u0440\u0430\u0445'] <= max_price]\n    return df\n\nprice_outlier_transformer = prep.FunctionTransformer(func=remove_price_outliers)","e6f14be3":"def engine_cap(X, max_engine_size=10):\n    df = X.copy(deep=True)\n    engine_size_outlier_models = df.loc[\n        df['\u041e\u0431\u044a\u0435\u043c \u0434\u0432\u0438\u0433\u0430\u0442\u0435\u043b\u044f'] > max_engine_size, \"\u041c\u043e\u0434\u0435\u043b\u044c\"]\n    g = df.groupby(\"\u041c\u043e\u0434\u0435\u043b\u044c\", observed=True)[\"\u041e\u0431\u044a\u0435\u043c \u0434\u0432\u0438\u0433\u0430\u0442\u0435\u043b\u044f\"].value_counts()\n\n    for idx, model in engine_size_outlier_models.iteritems():\n        if g[model].max() > 1:\n            df.loc[idx, \"\u041e\u0431\u044a\u0435\u043c \u0434\u0432\u0438\u0433\u0430\u0442\u0435\u043b\u044f\"] = g[model].idxmax()\n        else:\n            df.loc[idx, \"\u041e\u0431\u044a\u0435\u043c \u0434\u0432\u0438\u0433\u0430\u0442\u0435\u043b\u044f\"] = df['\u041e\u0431\u044a\u0435\u043c \u0434\u0432\u0438\u0433\u0430\u0442\u0435\u043b\u044f'].median()\n\n    return df\n\nengine_cap_transformer = prep.FunctionTransformer(func=engine_cap)","6c1f8861":"def brand_model_clean(X):\n    df = X.copy(deep=True)\n\n    # Brand assignment based on model majority.\n    # The following brand\/model discrepancies were adjusted in favor of the\n    # brand with the majority of given models.\n    df.loc[(df['\u0411\u0440\u0435\u043d\u0434'] == \"AAD\") & (df['\u041c\u043e\u0434\u0435\u043b\u044c'] == \"WMN\"), '\u0411\u0440\u0435\u043d\u0434'] = 'ABJ'\n    df.loc[(df['\u0411\u0440\u0435\u043d\u0434'] == \"ACK\") & (df['\u041c\u043e\u0434\u0435\u043b\u044c'] == \"XDQ\"), '\u0411\u0440\u0435\u043d\u0434'] = 'ABO'\n    df.loc[(df['\u0411\u0440\u0435\u043d\u0434'] == \"ABN\") & (df['\u041c\u043e\u0434\u0435\u043b\u044c'] == \"XIA\"), '\u0411\u0440\u0435\u043d\u0434'] = 'AAZ'\n\n    # Brand assignment based on model and body type majority.\n    # The following brand\/model discrepancies were adjusted in favor of the\n    # brand with the majority of the given body type (e.g. if brand AAN has\n    # more cars listed with the same body type as the car at question,\n    # then the model is assigned to brand AAN)\n    df.loc[(df['\u0411\u0440\u0435\u043d\u0434'] == \"ACO\") & (df['\u041c\u043e\u0434\u0435\u043b\u044c'] == \"WQB\"), '\u0411\u0440\u0435\u043d\u0434'] = 'AAN'\n    df.loc[(df['\u0411\u0440\u0435\u043d\u0434'] == \"AAH\") & (df['\u041c\u043e\u0434\u0435\u043b\u044c'] == \"MSD\"), '\u0411\u0440\u0435\u043d\u0434'] = 'ACO'\n    df.loc[(df['\u0411\u0440\u0435\u043d\u0434'] == \"AAE\") & (df['\u041c\u043e\u0434\u0435\u043b\u044c'] == \"BRU\"), '\u0411\u0440\u0435\u043d\u0434'] = 'ABQ'\n\n    return df\n\nbrandmodel_cleaner = prep.FunctionTransformer(func=brand_model_clean)","1bc453c3":"def clean_yr(X):\n    X.loc[X['\u0413\u043e\u0434'] > 2020, '\u0413\u043e\u0434'] = X['\u0413\u043e\u0434'].median()\n    return X\n\nyear_cleaner = prep.FunctionTransformer(func=clean_yr)","602e0adc":"def color_body_clean(X):\n    df = X.copy(deep=True)\n    \n    # Create a new feature from color\n    x = df[\"\u0426\u0432\u0435\u0442\"].str.contains('\u043c\u0435\u0442\u0430\u043b\u043b\u0438\u043a')\n    df['Metallic'] = x.map({False:0,True:1})\n    \n    # Adjust color for redundancy\n    df['\u0426\u0432\u0435\u0442'] = df['\u0426\u0432\u0435\u0442'].str.replace(' \u043c\u0435\u0442\u0430\u043b\u043b\u0438\u043a', '')\n    df.loc[df['\u0426\u0432\u0435\u0442'] == \"\u0437\u043e\u043b\u043e\u0442\u0438\u0441\u0442\u044b\u0439\", \"\u0426\u0432\u0435\u0442\"] = \"\u0437\u043e\u043b\u043e\u0442\u043e\u0439\"\n    df.loc[df['\u0426\u0432\u0435\u0442'] == \"\u0432\u0438\u0448\u043d\u044f\", \"\u0426\u0432\u0435\u0442\"] = \"\u0432\u0438\u0448\u043d\u0435\u0432\u044b\u0439\"\n    \n    # Latin c got mixed up with Cyrillic c\n    df.loc[df['\u041a\u0443\u0437\u043e\u0432'] == 'c\u0435\u0434\u0430\u043d', '\u041a\u0443\u0437\u043e\u0432'] = '\u0441\u0435\u0434\u0430\u043d'\n    \n    # Merge redundant categories\n    df.loc[df['\u041a\u0443\u0437\u043e\u0432'].isin([\"\u043c\u0438\u043a\u0440\u043e\u0430\u0432\u0442\u043e\u0431\u0443\u0441\", \"\u043c\u0438\u043a\u0440\u043e\u0432\u044d\u043d\"]),\n           '\u041a\u0443\u0437\u043e\u0432'] = \"\u043c\u0438\u043d\u0438\u0432\u044d\u043d \/ \u0431\u0443\u0441\"\n    df.loc[df['\u041a\u0443\u0437\u043e\u0432'] == '\u043b\u0438\u0444\u0442\u0431\u044d\u043a', '\u041a\u0443\u0437\u043e\u0432'] = '\u0445\u044d\u0442\u0447\u0431\u0435\u043a\/\u043b\u0438\u0444\u0442\u0431\u0435\u043a'\n    df.loc[df['\u041a\u0443\u0437\u043e\u0432'] == '\u0432\u043d\u0435\u0434\u043e\u0440\u043e\u0436\u043d\u0438\u043a', '\u041a\u0443\u0437\u043e\u0432'] = '\u0432\u043d\u0435\u0434\u043e\u0440\u043e\u0436\u043d\u0438\u043a \/ \u043f\u0438\u043a\u0430\u043f'\n    \n    # Merge the underrepresented classes into one class - Other\n    df.loc[df['\u041a\u0443\u0437\u043e\u0432'].\n           isin([\"\u0444\u0443\u0440\u0433\u043e\u043d\", \"\u043a\u0430\u0431\u0440\u0438\u043e\u043b\u0435\u0442\", \"\u043b\u0438\u043c\u0443\u0437\u0438\u043d\", \"\u0440\u043e\u0434\u0441\u0442\u0435\u0440\", \"\u0444\u0430\u0441\u0442\u0431\u044d\u043a\"]),\n           '\u041a\u0443\u0437\u043e\u0432'] = \"\u0434\u0440\u0443\u0433\u043e\u0435\"\n    return df\n\ncolor_body_cleaner = prep.FunctionTransformer(func=color_body_clean)","2d9480dd":"# This transformer can't applied to test data\ndrop = Pipeline(steps=[\n    ('dups', duplicates_transformer),\n    ('clip',price_outlier_transformer),\n])\ndf_level_pipe = Pipeline(steps=[\n    ('enginecap', engine_cap_transformer),\n    ('brandmodelclean', brandmodel_cleaner),\n    ('color_body_cleaner', color_body_cleaner)\n])","1125b7bb":"# Collecting column names to trace their importance later\nonehot_features = df_train.iloc[:,12:].columns.tolist()\nall_cols = df_train.columns.tolist()","484bd0b5":"year_transformers = Pipeline(steps=[\n    ('year_cleaners', year_cleaner),\n    ('year_imp', skimp.IterativeImputer(estimator=BayesianRidge(),initial_strategy='median',random_state=SEED)),\n])","324282bc":"column_level_transformer = ColumnTransformer(transformers=[\n    ('price_bc', prep.PowerTransformer(method='box-cox', standardize=False), ['\u0426\u0435\u043d\u0430 \u0432 \u0434\u043e\u043b\u043b\u0430\u0440\u0430\u0445']),\n    ('ohe_trans', skimp.SimpleImputer(\n        strategy='constant', fill_value=0), onehot_features),\n    ('year_trans', year_transformers, ['\u0413\u043e\u0434'])\n], remainder='passthrough', n_jobs=-1)","70306610":"def toDF(X):\n    remainder_columns = all_cols[1:12]\n    remainder_columns.remove('\u0413\u043e\u0434')\n    pipe1_feature_names = ['Price_bc'] + onehot_features + ['Year'] + remainder_columns + ['Metallic']\n    df = pd.DataFrame.from_records(X, columns=pipe1_feature_names,\n                                   coerce_float=True)\n    return df\n\nconvert_todf = prep.FunctionTransformer(func=toDF)","9282384c":"def concatenate(df):\n    X = df.copy(deep=True)\n    X['Brand_Model'] = X['\u0411\u0440\u0435\u043d\u0434'] + \"_\" + X[\"\u041c\u043e\u0434\u0435\u043b\u044c\"]\n    return X\n\ncon = prep.FunctionTransformer(func= concatenate, check_inverse=False)","cff11513":"pipe1 = Pipeline([\n    ('drop_rows', drop),\n    ('clean', df_level_pipe),\n    ('price_year_ohe', column_level_transformer),\n    ('toDF', convert_todf),\n    ('con_brand_model', con)\n])\n\nfrom sklearn import set_config\nset_config(display='diagram')\npipe1","ed5beb86":"# Generate a dataframe to pass to the second pipe\npipe1_df = pipe1.fit_transform(df_train)","0ef53fb4":"pipe2 = Pipeline(steps=[\n    ('glmm_enc', ce.GLMMEncoder(random_state=SEED, randomized=True, \n               handle_missing='return_nan',  handle_unknown='return_nan')),\n    ('imp', skimp.IterativeImputer(\n    estimator=BayesianRidge(verbose=0, ),\n    max_iter=10, n_nearest_features=15, verbose=1, \n    initial_strategy='median', random_state=SEED))\n])\n\npipe2_df = pipe2.fit_transform(pipe1_df, pipe1_df.Price_bc)\n\npipe2_df = pd.DataFrame(pipe2_df, columns=pipe1_df.columns)","58a72e51":"y = pipe2_df.Price_bc\nX = pipe2_df.drop(columns='Price_bc')\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.1765, random_state=SEED)\n\ncb_reg = cb.CatBoostRegressor(random_seed=SEED,\n                                     custom_metric='R2', #task type GPU won't work\n                                     verbose=500,\n                                     loss_function='RMSE',\n                                     eval_metric='R2',\n                                     iterations=25000, # converges around 10496, but I left it as is for consistency\n                                     use_best_model=True,\n                                     learning_rate=0.03,\n                                     )\ndata_train = cb.Pool(X_train, y_train)\ndata_test = cb.Pool(X_test, y_test)\ncb_reg.fit(X=data_train, eval_set=data_test, plot=True, verbose=500, save_snapshot=False)","df42da48":"cb_reg.save_model('cb_reg_25k.dump')","bc522785":"fi = list(zip(cb_reg.feature_names_, cb_reg.feature_importances_))\nfi = sorted(fi, key=lambda x: x[1],reverse=True)\n\nplt.figure(figsize=(15,7))\nsns.barplot(y=[k for k,v in fi][:15], x=[v for k,v in fi][:15],\n           orient='h')\nplt.title('Feature Importance')\nplt.show()","faf90c4c":"print(cb_reg.score(X_test, y_test))\nprint('X_test r2 score:',metrics.r2_score(y_test, cb_reg.predict(X_test)))\ny_pred_25k = cb_reg.predict(X_test)\ny_pred_25k_inv = pipe1.named_steps.price_year_ohe.named_transformers_.price_bc.inverse_transform(y_pred_25k.reshape(-1,1))\ny_test_inv = pipe1.named_steps.price_year_ohe.named_transformers_.price_bc.inverse_transform(y_test.ravel().reshape(-1,1))\nprint('X_test r2 score after inversion:',metrics.r2_score(y_test_inv, y_pred_25k_inv))","698343c1":"def test_pipe(X):\n    '''Function to preprocess the test dataset for the regressor.'''\n    df = X.copy(deep=True)\n    df = pipe1.named_steps.clean.transform(df)\n    df = pipe1.named_steps.price_year_ohe.transform(df)\n    df = pipe1.named_steps.toDF.transform(df)\n    df = pipe1.named_steps.con_brand_model.transform(df)\n    df_complete = pipe2.transform(df)\n    df_complete = pd.DataFrame(df_complete, columns=df.columns)\n    \n    return df_complete    \n\ndf_test = test_pipe(test)\n\nX_val = df_test.drop(columns='Price_bc')\ny_val = df_test['Price_bc']\n\nprint('R2 on X_val:', cb_reg.score(X_val,y_val))\ny_pred = cb_reg.predict(X_val)\ny_pred_inv = pipe1.named_steps.price_year_ohe.named_transformers_.price_bc.inverse_transform(y_pred.reshape(-1,1))\ny_val_inv = pipe1.named_steps.price_year_ohe.named_transformers_.price_bc.inverse_transform(y_val.ravel().reshape(-1,1))\nassert np.allclose(y_val_inv.ravel(), test['\u0426\u0435\u043d\u0430 \u0432 \u0434\u043e\u043b\u043b\u0430\u0440\u0430\u0445'].ravel())\nprint('Inverted R2 on X_val:', metrics.r2_score(y_val_inv, y_pred_inv))","964b3b70":"submission = pd.DataFrame({'Id':X_val.index,'Predicted':y_pred_inv.ravel()})\n\nsubmission.to_csv('my_submission4.csv',index=False)","77ae333a":"## General Data Cleaning\nData cleaning is implemented in the form of scikit-learn transformers combined in pipeline objects. Such method provides consistency, traceability of transformations and flexibility in terms of serialization and repeated usage.","34ab099c":"### Transformer for adjusting the year values\nThis transformer serves a preventative purpose of ensuring there are no discrepancies in entry of year values. Specifically, it caps them on the current year (2020) and replaces the values above it with the overall median value.","4d7f0db7":"### The second pipeline\nThis pipeline encodes the categorical data using the General Linear Mixed Model Encoder, which is intended for use with binary targets, but it weirdly enough it worked best for me. \ud83d\udc40\n\nThe MICE imputation with BayesianRidge estimator completes the dataset.","319f7a5a":"### Transformer for cleaning color and body type variables\nVariables color and body type are cleaned in the same manner, which is why they are combined into one transformer. The transformer removes the redundancy in class names. The underrepresented classes in variable body type are combined into one class ('\u0434\u0440\u0443\u0433\u043e\u0435'), while variable color is mapped to another one-hot variable that represents if the color is metallic or not.","af58c02e":"### Transformer for converting the array into a DataFrame object","27d5bf19":"### Transformer for adjusting the engine size discrepancies in the training dataset\nThe engine size variable suffers from erroneous data entries. To adjust for this issue, the maximum engine size of 10 liters is introduced (note that the largest car engine size as of today is 8.4 liters). For our purposes, any values larger than this threshold are considered data entry errors and are replaced by the median values for the same model or the overall median engine size.","c46a2cce":"### Transformer for resolving brand\/model discrepancies in the training dataset\nThere are models that belong to multiple brands, which may signalize data entry errors. Theoretically, it is possible that model names overlap across brands (e.g. LS models from Lexus and Lincoln). But without any information about decoding the brand and model names, it is impossible to say for certain which way of handling this irregularity is appropriate. For our purposes, we adjusted the brands manually as described in the function below.","dc188791":"### Combining the cleaning transformers into pipelines*\n*The year_cleaner transformer is added to the pipeline later in the list of column-level transformers.\n\nThe transformers that involve dropping rows from the dataset are separated to ensure they are not applied to the testing dataset.","c0ddf287":"### Transformer for concatenating brand and model names\nThis transformer generates a new feature by concatenating the strings of brand and model names. The purpose of the generated variable is to serve as the interaction feature between model and brand names. This helps measuring the combined impact of the brand and model.","4a8dc938":"## Model fitting\nThis code obviously can be refactored into a single transformer or estimator object. I didn't have enough time to do that. And I also didn't have time to cross-validate and tune the model \ud83d\ude34, but I did use the learning rate from my previous experiments with Hyperopt package.","5585c837":"# PRODATA 2020","c3272542":"### Transformer for dropping full duplicate entries in the training dataset","639647aa":"### Combining the transformers into a pipeline before estimator based transformers","415b8871":"## Imputations, transformations and generation of new features\n\nMICE imputations in scikit-learn's implementation is applied to year variable using BayesianRidge algorithm.","2d161944":"## TL;DR\nThe cell below is just an aggregation of all of the code. Not a function, not a class object \ud83d\ude10. Run it to waste no time. It takes a while to train, so just **ask me for the dump file of the regressor**. For the detailed explanations, keep scrolling \ud83d\udd3d\ud83d\udd3d\ud83d\udd3d.","574769a6":"Box-Cox transformation is applied to the price variable to enforce a more normal distribution.\nOne-hot features are imputed with zeros.","d1916cc3":"## Running the model on the test dataset","e20dbe2c":"### Transformer for removing price outliers in the training dataset\nThe maximum price of 142,000 is hard coded in for transformation consistency. It was established after an exploratory analysis of the training dataset. $142,000 is the price within the 99.7th percentile (3 standard deviations from the mean). All prices above this threshold were dropped from the training dataset to remove bias towards higher prices during model fitting and ensure fair pricing in predictions."}}