{"cell_type":{"2f274bea":"code","f3f1d265":"code","925fda57":"code","699a3339":"code","4b0f6d47":"code","b273ceec":"code","c454d16a":"code","19c825ec":"code","9c7f03b3":"code","6ca4a9e0":"code","faedb611":"code","d819edfa":"code","90ef1afa":"code","daa10aee":"code","41e38424":"code","830a8f9b":"code","1da81420":"code","aadee902":"code","a89dc5fc":"code","60084a4a":"code","a32ffa07":"code","e576e11e":"code","a01e4a33":"code","2d707c87":"markdown","cd310a21":"markdown","e0f471d1":"markdown","b715e460":"markdown","030beb86":"markdown","1f1aa62c":"markdown","f6ef6bb3":"markdown","b456d45e":"markdown","c137059a":"markdown","bebc8356":"markdown"},"source":{"2f274bea":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost.sklearn import XGBClassifier\nfrom pandas import Series, DataFrame\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\nnp.random.seed(500)\nimport warnings\nfrom warnings import simplefilter\nsimplefilter(action='ignore', category=FutureWarning)","f3f1d265":"Red = pd.read_csv('..\/input\/wine-data-redwhite\/winequality-red.csv', sep=';')\nWhite = pd.read_csv('..\/input\/wine-data-redwhite\/winequality-white.csv', sep=';')","925fda57":"Red['type'] = 'red'\nWhite['type'] = 'white'\nwine = pd.concat([Red, White])\nwine.head(3)","699a3339":"wine = wine.sample(frac=1).reset_index(drop=True)\nwine.head(3)","4b0f6d47":"wine.info()","b273ceec":"fig = plt.figure(figsize=(22,8)) \nplt.subplot2grid((2,3),(0,0))\nsns.countplot(wine['quality'])\nplt.title(\"Analysing target variable\"); plt.ylabel(\"Count\"); plt.xlabel(\"Quality\")\n\nplt.subplot2grid((2,3),(0,1))\nwine['quality'].hist(bins=np.arange(11)-0.5, rwidth=0.5, density=True)\nplt.title(\"Analysing target variable\"); plt.ylabel(\"Density\"); plt.xlabel(\"Quality\")","c454d16a":"fig = plt.figure(figsize=(15,6)) \nplt.subplot2grid((2,3),(0,0))\nsns.countplot(wine['pH'])\nplt.title(\"Analysing wine pH\"); plt.ylabel(\"Count\"); plt.xlabel(\"pH\")\n\nplt.subplot2grid((2,3),(0,1))\nsns.distplot(wine['alcohol'])\nplt.title(\"Analysing wine alcohol\"); plt.ylabel(\"Density\"); plt.xlabel(\"alcohol\")\n\nplt.subplot2grid((2,3),(0,2))\nsns.kdeplot(wine.query('quality > 2').quality)\nplt.title(\"Analysing target variable\")","19c825ec":"fig = plt.figure(figsize=(18,6)) \nplt.subplot2grid((2,3),(0,0))\nsns.violinplot(x='quality', y='pH', data=wine)\nplt.title(\"Wine quality VS pH\");\n\nplt.subplot2grid((2,3),(0,1))\nsns.violinplot(x='quality', y='alcohol', data=wine)\nplt.title(\"Wine quality VS alcohol\");","9c7f03b3":"fig = plt.figure(figsize=(22,8)) \nplt.subplot2grid((2,3),(0,0))\nsns.countplot(wine['type'])\nplt.title(\"Proportion of wine according to its type\"); plt.ylabel(\"Count\"); plt.xlabel(\"Type of wine\")\n\nplt.subplot2grid((2,3),(0,1))\nwine['type'].hist(bins=np.arange(3)-0.5, rwidth=0.5, density=True)\nplt.title(\"Proportion of wine according to its type\"); plt.ylabel(\"Density\"); plt.xlabel(\"Type of wine\")\n\nwine['type'].value_counts()","6ca4a9e0":"fig, ax = plt.subplots(figsize=(14, 6))\nViz = pd.crosstab(wine['type'], wine['quality'])\nViz.div(Viz.sum(axis=1), axis=0).plot.bar(ax=ax)\nplt.title(\"Wine quality according to its type\"); plt.ylabel(\"Density\"); plt.xlabel(\"Type of wine\")","faedb611":"wine.columns\nsns.pairplot(wine)\nplt.suptitle('Relationship between continuous variables')","d819edfa":"fig, axs = plt.subplots(3, 4, figsize=(25, 15))\ncontinuous_features = list(wine.columns)\ncontinuous_features.remove('quality')\ncontinuous_features.remove('type')\naxs = axs.ravel()\n\nfor ax, colname in zip(axs, continuous_features):\n    sns.distplot(wine[colname], ax=ax)\n    \nplt.suptitle('Univariate analysis on continuous variables')","90ef1afa":"wine.hist(figsize=(10,10),bins=50)\nplt.show()","daa10aee":"wine.corrwith(wine.quality).plot.bar(figsize = (15, 6), title = \"Correlation with Quality with respect to attributes\", fontsize = 15, rot = 45, grid = True)","41e38424":"sc = StandardScaler()\npca = PCA(n_components = 2)\n\n# Standardize & project data on a 2-dimensional plane\ntypes = (wine['type']=='red').astype(int).values\nscaled_wines = sc.fit_transform(wine[continuous_features])\ntwodim_wines = pca.fit_transform(scaled_wines)\n\n# Display data\nfig, ax = plt.subplots(figsize=(8, 8))\nn = int(0.2 * wine.shape[0])\nplt.scatter(twodim_wines[:n,0], twodim_wines[:n,1], c=types[:n], alpha=.8, linewidth=.8)\nplt.title(\"Wine data separated in two clusters\")","830a8f9b":"wine = wine.drop('type',axis=1)\nwine.head(3)","1da81420":"#define wine class [1 = 'Good Quality', 0 = 'Bad Quality']\nwine['def_quality'] = [0 if x < 7 else 1 for x in wine['quality']]# Separate feature variables and target variable\nX = wine.drop(['quality','def_quality'], axis = 1)\nY = wine['def_quality']","aadee902":"wine['def_quality'].value_counts()\n\nfig = plt.figure(figsize=(22,8)) \nplt.subplot2grid((2,3),(0,0))\nsns.countplot(wine['def_quality'])\nplt.title(\"Proportion of wine according to its quality\"); plt.ylabel(\"Count\"); plt.xlabel(\"Type of wine\")","a89dc5fc":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.25,random_state=7)","60084a4a":"#Hyperparameter tuning of classifier\nmodel = LogisticRegression()\nsolvers = ['newton-cg', 'lbfgs', 'liblinear']\npenalty = ['l1','l2']\nc_values = [50, 10, 1.0, 0.1, 0.01]\n\n# define grid search\ngrid = dict(solver=solvers,penalty=penalty,C=c_values)\n\nKcv= KFold(n_splits=5, random_state=100)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv = Kcv,scoring='accuracy',error_score=0)\ngrid_result = grid_search.fit(X_train, y_train)\n# summarize results\nprint(\"Best LR: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","a32ffa07":"classifier = LogisticRegression(C=1.0, penalty='l2', solver='newton-cg')\nclassifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = classifier.predict(X_test)\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nresults = pd.DataFrame([['Logistic Regression', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\nprint(results)\nconfusion_mat = confusion_matrix(y_test,y_pred)\nprint(\"\\n Confusion Matrix \\n\", confusion_mat)","e576e11e":"#Hyperparameter tuning of XGBclassifier\nparam_tuning = {\n        'learning_rate': [0.01, 0.1],\n        'max_depth': [3, 5, 7, 10],\n        'min_child_weight': [1, 3, 5],\n        'subsample': [0.5, 0.7],\n        'colsample_bytree': [0.5, 0.7],\n        'n_estimators' : [100, 200, 500],\n        'objective': ['reg:squarederror']\n    }\nKcv= KFold(n_splits=10, random_state=100)\nxgb_model = XGBClassifier()\ngrids = GridSearchCV(estimator = xgb_model, param_grid = param_tuning, cv = Kcv, n_jobs = -1, scoring='accuracy',error_score=0)\n\n#gsearch.fit(X_train,y_train)\ngrid_result = grids.fit(X_train, y_train)\nprint(\"Best LR: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","a01e4a33":"import xgboost as xgb\ncm = xgb.XGBClassifier(n_estimators = 1000)\ncm.fit(X_train, y_train)\ny_pred = cm.predict(X_test)\n\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nresults = pd.DataFrame([['XGBoost Classifier', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\nprint(results)\nconfusion_mat = confusion_matrix(y_test,y_pred)\nprint(\"\\n Confusion Matrix \\n\", confusion_mat)","2d707c87":"# 1. Import Libraries","cd310a21":"# 6. Create Classification version of target variable","e0f471d1":"# 5. Clustering Wine data with Principal component analysis (PCA)","b715e460":"# 4. Exploratory data analysis of Wine dataset","030beb86":"# Shuffling data","1f1aa62c":"**XGBoost Outperformed! Better F1 Score as well.**\nHope You Liked the EDA. :)","f6ef6bb3":"The original dataset is in the **UCI Machine Learning Repository**. The original datasets include two datasets about red and white wine. This dataset is combined into one dataset by adding a column that represents the color, that is red or white. This data contains 6,497 instances (wines) including 4,898 white wines and 1,599 red wines.\nThe original dataset has no missing value but the values in this data are randomly eliminated with a probability of 0.01. \n****Now, Let's explore the dataset and do some prediction!!****\n\n![Wine](https:\/\/welpmagazine.com\/wp-content\/uploads\/2020\/11\/image.jpg)","b456d45e":"# 7. Prediction of Wine quality","c137059a":"# 3. Concatenate the Red and White wine datasets","bebc8356":"# 2. Import data"}}