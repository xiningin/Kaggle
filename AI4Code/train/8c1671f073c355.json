{"cell_type":{"da612953":"code","a5634278":"code","da47e9e9":"code","a48d0baf":"code","8721909d":"code","38c812d9":"code","6704db6c":"code","65b168c5":"markdown"},"source":{"da612953":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport pickle\n\n# Pipeline\nfrom sklearn.pipeline import Pipeline, FeatureUnion\n# Feature extraction\nfrom sklearn.preprocessing import FunctionTransformer, OneHotEncoder, StandardScaler\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.base import BaseEstimator, TransformerMixin\n# Model\nfrom sklearn.linear_model import LogisticRegression\n# Cross-val\nfrom sklearn.model_selection import GridSearchCV, TimeSeriesSplit\nfrom itertools import combinations, chain\n\n# ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Your Alice data path\nDATA_PATH = '..\/input'\n\n# load train & kaggle test\ntrain_data = pd.read_csv(DATA_PATH + '\/train_sessions.csv')\nsubmit_data = pd.read_csv(DATA_PATH + '\/test_sessions.csv')\n\n# frequently used column names\nsite_cols = ['site%d' % i for i in range(1, 11)]\ntime_cols = ['time%d' % i for i in range(1, 11)]\n\n# light preprocessing\ndef convert_types(df):\n    df[site_cols] = df[site_cols].fillna(0).astype('int')\n    df[time_cols] = df[time_cols].apply(pd.to_datetime)\n    return df\n\ntrain_data = convert_types(train_data)\nsubmit_data = convert_types(submit_data)\n\n# sort by session start date, for time-based cross-validation\ntrain_data = train_data.sort_values(by=time_cols[0])\n\n# get the target column\ny = train_data['target']\n\n# Load websites dictionary\nwith open(DATA_PATH + '\/site_dic.pkl', 'rb') as input_file:\n    site_dict = pickle.load(input_file)\n\nsite_ids = list(site_dict.values())\n# Create dataframe for the dictionary\nsites_dict = pd.DataFrame(list(site_dict.keys()), index=list(site_dict.values()), columns=['site'])\n","a5634278":"#####################################\n## Helper functions that extract different data\n#####################################\n\n# Return sites columns as a single string\n# This string can be supplied into CountVectorizer or TfidfVectorizer\ndef extract_sites_as_string(X):\n    return X[site_cols].astype('str').apply(' '.join, axis=1)\n\n# Year-month feature from A4\ndef feature_year_month(X):\n    return pd.DataFrame(X['time1'].dt.year * 100 + X['time1'].dt.month)\n\n# Hour feature from A4\ndef feature_hour(X):\n    return pd.DataFrame(X['time1'].dt.hour)\n\n# Month\ndef feature_month(X):\n    return pd.DataFrame(X['time1'].dt.month)\n\n# Weekday\ndef feature_weekday(X):\n    return pd.DataFrame(X['time1'].dt.weekday)\n\n# Is morning feature from A4\ndef feature_is_morning(X):\n    return pd.DataFrame(X['time1'].dt.hour <= 11)\n\n# Session length feature from A4\ndef feature_session_len(X):\n    X['session_end_time'] = X[time_cols].max(axis=1)\n    X['session_duration'] = (X['session_end_time'] - X['time1']).astype('timedelta64[s]')\n    return X[['session_duration']]\n\n\n# Add more functions here :)\n# ...\n","da47e9e9":"# Special transformer to save output shape\nclass ShapeSaver(BaseEstimator, TransformerMixin):\n    def transform(self, X):\n        self.shape = X.shape\n        return X\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n\n###################################\n## Defining the processing pipeline\n## NOTE: ShapeSaver() is required as the last step for each feature\n##################################\ntransform_pipeline = Pipeline([\n    ('features', FeatureUnion([\n        # List of features goes here:\n        ('year_month_val', Pipeline([\n            ('extract', FunctionTransformer(feature_year_month, validate=False)),\n            ('scale', StandardScaler()),\n            ('shape', ShapeSaver())\n        ])),\n        ('session_len', Pipeline([\n            ('extract', FunctionTransformer(feature_session_len, validate=False)),\n            ('scale', StandardScaler()),\n            ('shape', ShapeSaver())\n        ])),\n        ('weekday_cat', Pipeline([\n            ('extract', FunctionTransformer(feature_weekday, validate=False)),\n            ('ohe', OneHotEncoder()),\n            ('shape', ShapeSaver())\n        ])),\n        ('hour_val', Pipeline([\n            ('extract', FunctionTransformer(feature_hour, validate=False)),\n            ('scale', StandardScaler()),\n            ('shape', ShapeSaver())\n         ])),\n        ('hour_cat', Pipeline([\n            ('extract', FunctionTransformer(feature_hour, validate=False)),\n            ('ohe', OneHotEncoder()),\n            ('shape', ShapeSaver())\n         ])),\n        ('month_cat', Pipeline([\n            ('extract', FunctionTransformer(feature_month, validate=False)),\n            ('ohe', OneHotEncoder()),\n            ('shape', ShapeSaver())\n         ])),\n        ('is_morning', Pipeline([\n            ('extract', FunctionTransformer(feature_is_morning, validate=False)),\n            ('shape', ShapeSaver())\n         ])),\n        ('sites_tfidf', Pipeline([\n            ('extract', FunctionTransformer(extract_sites_as_string, validate=False)),\n            ('count', TfidfVectorizer(token_pattern=r'(?u)\\b\\w+\\b')),\n            ('shape', ShapeSaver())\n        ])),\n        # Add more features here :)\n        # ...\n    ]))\n])\n\n# Join train & submit data\nfull_df = pd.concat([train_data, submit_data])\n\n# Remember train dataset size to split it later\ntrain_size = train_data.shape[0]\n\n# Run preprocessing on full data\nfull_transformed_df = transform_pipeline.fit_transform(full_df)","a48d0baf":"##########################################\n## Define feature selection & model pipeline\n##########################################\n\n# Column selection transformer\nclass ColumnSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, ranges=[]):\n        self.ranges = ranges\n    \n    def transform(self, X):\n        # flatten ranges to plain list of column numbers\n        cols = [i for r in self.ranges for i in r[1]]\n        return X[:, cols]\n    \n    def fit(self, X, y=None, **fit_params):\n        return self\n\n# Pipeline to select features and fit model\npipeline = Pipeline([\n    ('select_features', ColumnSelector()),\n    # Our basic model\n    ('model', LogisticRegression())\n])","8721909d":"# Let's get list of all features and their columns from preprocessing Pipeline\nfeatures = [i[0] for i in transform_pipeline.get_params()['features'].get_params()['transformer_list']]\nsizes = [transform_pipeline.get_params()['features'].get_params()[i + '__shape'].shape[1] for i in features]\n\n# Create list of column ranges for each feature\nfeature_col_ranges = []\nidx = 0\nfor size in sizes:\n    feature_col_ranges.append(range(idx, size + idx))\n    idx = size + idx\n\n# Dict feature name => column range\nfeature_ranges = dict(zip(features, feature_col_ranges))\n\n#####################################\n## Features and hyper-params search!\n####################################\n\n# Combinations of features you want to check\n# Names should match to step names defined in the processing pipeline\n# Here's example of how more features does not result in better model; of these two the smaller set of features will yield better cross-val score\nsel_features = [\n    ['year_month_val', 'hour_cat', 'hour_val', 'session_len', 'weekday_cat', 'month_cat', 'is_morning', 'sites_tfidf'],\n    ['hour_cat', 'session_len','month_cat', 'is_morning', 'weekday_cat', 'sites_tfidf'],\n]\n\n# Generate all possible combinations of features automatically, if you have fast machine to test them all\n# Range defines min\/max features number to try\n# sel_features = list(chain.from_iterable(combinations(features, r) for r in range(4, 7)))\n\n# Get ranges of columns for selected features\nsel_col_ranges = list(map(lambda feats: [(f, feature_ranges[f]) for f in feats], sel_features))\nparam_grid = {\n    # Feature selection params\n    'select_features__ranges': sel_col_ranges,\n    # Model hyper parameters, can try tuning these too\n    'model__C': [1], #np.linspace(0.01, 5, 15)\n    'model__random_state': [17]\n}\n\n# Double check param grid\nprint(param_grid)","38c812d9":"# Grid search\n# Unfortunately doesn't work with n_jobs=-1, let me know if you know how to fix this in kaggle kernel :)\ngrid = GridSearchCV(pipeline, param_grid=param_grid, scoring='roc_auc', cv=TimeSeriesSplit(n_splits=5), n_jobs=1, verbose=1)\n# Take only train part of the full dataset\ngrid.fit(full_transformed_df[:train_size, :], y)\n\nprint('Best cross-val score: ', grid.best_score_)\nprint('Best params: ', grid.best_params_)","6704db6c":"# Make predictions for submission data (take only submission data from full dataset)\nsubmission = grid.predict_proba(full_transformed_df[train_size:, :])[:,1]\n\n# Save to CSV\ndf = pd.DataFrame(submission, index=np.arange(1, submit_data.shape[0] + 1), columns=['target'])\ndf.to_csv('submission.csv', index_label='session_id')","65b168c5":"# Pipeline-based framework for features evaluation\n\nThis approach allows easily adding new features and trying different combinations of features and model parameters. It's possible to run grid search over combinations of features.\n\nThe general idea:\n  * First pipeline processes the data and creates dataset with extracted features\n  * Second pipeline allows to keep only selected features and fit the model\n  * Grid search is done with the 2nd pipeline\n  * Get predictions from 2nd pipeline with the best parameters\n\nDownsides & possible improvements:\n  * Grid search only applies to features list and model parameters but not to pre-processing pipeline (as an ugly workaround, it's possible to create multiple features with different params, e.g. multiple TfidfVectorizer with different ngram param, and test them out)\n  * Grid search can't be scaled to multiple workers, this fails with pickling error (probably ColumnSelector should be imported from separate file for that)\n \n \n Please leave any feedback or improvement ideas!"}}