{"cell_type":{"8ebef36a":"code","b543f0b9":"code","5c24b37b":"code","f8c9b2fe":"code","f80a5872":"code","7a5d450e":"code","460e1d04":"code","0cf1238e":"code","84279e9a":"code","a20662f0":"code","95de70b5":"code","eee55e41":"code","e0e75361":"code","a2e8f8e0":"code","dfb43da9":"code","edaa007e":"code","b5087b69":"code","f03ae8dc":"code","1a2998d4":"code","020673a2":"code","07b84b1d":"code","8c2f06c1":"code","cfaa7c7b":"code","d6fc577e":"code","29f970ec":"code","3c02562b":"code","0f8d1969":"code","db6df1fc":"code","80e55384":"code","ac756fab":"code","d7f3e581":"code","eb288621":"code","f2a9c0ce":"code","ef40c5b4":"code","40ca2f65":"code","b1edc257":"code","59d47e35":"code","8f52e8bc":"code","1615eb14":"code","9a36253a":"code","a9652528":"code","7139efed":"code","2f77bb2d":"code","010a1e28":"code","fc4f812d":"code","028a465b":"code","e35ff34d":"code","a56c724f":"code","bb01dec9":"code","d292746b":"code","67fdc767":"code","74c8910c":"code","b7d843d5":"code","371bdb7e":"code","c979c30c":"code","99400a54":"code","34757b11":"code","21849bff":"code","a4664159":"code","fb604130":"code","585e84b2":"code","0f328a01":"markdown","23d8b57c":"markdown","68074eda":"markdown","15ea8a80":"markdown","1e6d0d96":"markdown","9bacd45f":"markdown","21ba4e7e":"markdown","523bae52":"markdown","ee2a6187":"markdown","8277624e":"markdown","a641744e":"markdown"},"source":{"8ebef36a":"# run only if you want to create class activation mappings for visualization\n# installing old version of scipy\n# warning will restart runtime on google colab\n!pip install -I scipy==1.2.*","b543f0b9":"!pip install -q kaggle","5c24b37b":"# make sure the kaggle.json file is in the working directory\n# The Kaggle API client expects this file to be in ~\/.kaggle,\n# so move it there.\n!mkdir -p ~\/.kaggle\n!cp kaggle.json ~\/.kaggle\/\n\n# This permissions change avoids a warning on Kaggle tool startup.\n!chmod 600 ~\/.kaggle\/kaggle.json","f8c9b2fe":"# List available datasets.\n!kaggle datasets list -s Xrays","f80a5872":"# downloading dataset\n!kaggle datasets download jbeltranleon\/xrays-chest-299-small","7a5d450e":"# downloading trained models\n!kaggle datasets download SmolBoi96\/CheXNet-Ensemble-Model","460e1d04":"# Unzip the data\n!unzip -qq -n xrays-chest-299-small.zip\n!unzip -qq -n CheXNet-Ensemble-Model.zip\n# Switch directory and show its content\n!cd 299_small && ls","0cf1238e":"import shutil\n\nshutil.rmtree('input_299_small')\nshutil.rmtree('sample_data')\n\n#shutil.rmtree('keras-vis')","84279e9a":"import os\n\nbase_dir = '299_small'\n\n# Directory to our training data\ntrain_folder = os.path.join(base_dir, 'train')\n\n# Directory to our validation data\nval_folder = os.path.join(base_dir, 'val')\n\n# Directory to our validation data\ntest_folder = os.path.join(base_dir, 'test')\n\n# List folders and number of files\nprint(\"Directory, Number of files\")\nfor root, subdirs, files in os.walk(base_dir):\n    print(root, len(files))","a20662f0":"%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\n# Parameters for our graph; we'll output images in a 4x4 configuration\nnrows = 4\nncols = 4\n\n# Index for iterating over images\npic_index = 0\n\n","95de70b5":"atelectasis_dir = '299_small\/train\/atelectasis'\natelectasis_fnames = os.listdir(atelectasis_dir)\ncardiomegaly_dir = '299_small\/train\/cardiomegaly'\ncardiomegaly_fnames = os.listdir(cardiomegaly_dir)\nconsolidation_dir = '299_small\/train\/consolidation'\nconsolidation_fnames = os.listdir(consolidation_dir)\neffusion_dir = '299_small\/train\/effusion'\neffusion_fnames = os.listdir(effusion_dir)\ninfiltration_dir = '299_small\/train\/infiltration'\ninfiltration_fnames = os.listdir(infiltration_dir)\nmass_dir = '299_small\/train\/mass'\nmass_fnames = os.listdir(mass_dir)\nnodule_dir = '299_small\/train\/nodule'\nnodule_fnames = os.listdir(nodule_dir)\nnofinding_dir = '299_small\/train\/no_finding'\nnofinding_fnames = os.listdir(nofinding_dir)\npneumothorax_dir = '299_small\/train\/pneumothorax'\npneumothorax_fnames = os.listdir(pneumothorax_dir)\n\n# Set up matplotlib fig, and size it to fit 4x4 pics\nfig = plt.gcf()\nfig.set_size_inches(15, 15)\n# showing Pneumothorax and No Finding pix\npic_index += 8\nnext_nofinding_pix = [os.path.join(nofinding_dir, fname) \n                for fname in nofinding_fnames[pic_index-8:pic_index]]\nnext_pneumothorax_pix = [os.path.join(pneumothorax_dir, fname) \n                for fname in pneumothorax_fnames[pic_index-8:pic_index]]\n\nfor i, img_path in enumerate(next_nofinding_pix+next_pneumothorax_pix):\n    # Set up subplot; subplot indices start at 1\n    sp = plt.subplot(nrows, ncols, i + 1)\n    sp.axis('Off') # Don't show axes (or gridlines)\n    \n    img = mpimg.imread(img_path)\n    #print(img.shape)\n    plt.imshow(img)\n    \nprint(255*img)\nprint(img_path)\nprint(img_path.endswith(\".jpg\"))","eee55e41":"# Plot class distribution chart\n\nimport pandas as pd\n\ncount_classes = pd.Series([len(atelectasis_fnames),len(cardiomegaly_fnames),len(consolidation_fnames),len(effusion_fnames),len(infiltration_fnames),len(mass_fnames),len(nofinding_fnames),len(nodule_fnames),len(pneumothorax_fnames)])\ncount_classes.plot(kind = 'bar')\nplt.title(\"Dataset Class Distribution\")\nplt.xlabel(\"Class\")\nplt.ylabel(\"Frequency\")","e0e75361":"import os\nfrom imblearn.over_sampling import ADASYN\nfrom sklearn.datasets import make_classification\nfrom PIL import Image\nimport numpy as np\nfrom numpy import asarray\n\npath, dirs, files = next(os.walk(\"299_small\/test\/no_finding\"))\nX = np.zeros((139, 299,299))\nfor i in range(139):\n  image = Image.open(\"299_small\/test\/no_finding\/\"+files[i])\n  y = asarray(image)\n  #print(i)\n  dim = y.shape\n  if len(y.shape) == 2:\n    X[i]=y\n  if len(y.shape) == 3:\n    X[i]=y[:,:,0]\n  #print(i)\n  #print(X[i].shape)\n\nprint(X.shape)\n\ny = np.asarray([0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1])\n#z = np.ones(30)\n#y = np.concatenate(y,z)\nprint(y.shape)","a2e8f8e0":"#from imblearn.over_sampling import SMOTE\nada = ADASYN(random_state=42)\nX_res, y_res = ada.fit_sample(X.reshape(X.shape[0], -1), y)\nprint(X_res.shape)\nX_res = X_res.reshape(X_res.shape[0], 299,299)\nprint(X_res.shape)\n","dfb43da9":"c = 32\n\nfor i in range (50,66):\n  #print(y_res[i])\n  i = i + c\n  img = Image.fromarray(X_res[i])\n  sp = plt.subplot(4, 4, i - 49 - c)\n  sp.axis('Off')\n  plt.imshow(img)","edaa007e":"img = Image.fromarray(X_res[49 + 36 + 9 + 1])\nplt.imshow(img)","b5087b69":"img = Image.fromarray(X_res[49 + 36 + 9])\nplt.imshow(img)","f03ae8dc":"import os\nfrom imblearn.over_sampling import RandomOverSampler\nfrom PIL import Image\nimport numpy as np\n\npathall, dirsall, filesall = next(os.walk(\"299_small\/train\"))\nk = 0\nX = np.ones((10000, 299,299))\ny = np.zeros(10000)\nfor i in range(9):\n  path, dirs, files = next(os.walk(\"299_small\/train\/\" + dirsall[i]))\n  for j in range(len(files)):\n    image = Image.open(\"299_small\/train\/\" + dirsall[i]+\"\/\"+files[j])\n    y[k] = i\n    image = np.asarray(image)\n  #print(i)\n  #print(y.shape)\n    if hasattr(image, 'shape'):\n      #print('hi1')\n      if len(image.shape) == 2:\n        #print('hi2')\n        X[k]=image\n        k = k +1\n      if len(image.shape) == 3:\n        #print('hi3')\n        X[k]=image[:,:,0]\n        k = k +1\n    if k >= 10000:\n      break\n  if k >= 10000:\n    break\n  #print(i)\n  #print(X[i].shape)\n    \n\nprint(X.shape)\nprint(y.shape)","1a2998d4":"LABELS = [\"atelectasis\",\"cardiomegaly\",\"consolidation\",\"effusion\",\"infiltration\",\"mass\",\"no_finding\",\"nodule\",\"pneumothorax\"]\nsize = [2697,699,838,2531,6109,1368,6400,1731,1404]","020673a2":"import numpy as np\n\nfor j in range(9):\n  path, dirs, files = next(os.walk(\"299_small\/train\/\" + LABELS[j]))\n  for i in range(size[6]-size[j]):\n    k = np.random.randint(0, len(files)-1)\n    shutil.copy(\"299_small\/train\/\" + LABELS[j]+\"\/\"+files[k], \"299_small\/train\/\" + LABELS[j]+\"\/\"+str(i)+\".png\")","07b84b1d":"#shutil.rmtree(\"299_small\/train\/atelectasisnew\")\nfor i in range(9):\n  path, dirs, files = next(os.walk(\"299_small\/train\/\"+LABELS[i]))\n  print(len(files))","8c2f06c1":"from keras.preprocessing.image import ImageDataGenerator\n\n# Batch size\nbs = 16\n\n# All images will be resized to this value\nimage_size = (299, 299)\n\n# All images will be rescaled by 1.\/255. We apply data augmentation here.\ntrain_datagen = ImageDataGenerator(rescale=1.\/255,\n                                   brightness_range= [0.5,1.5],\n                                   zoom_range=0.2)\n\nval_datagen = ImageDataGenerator(rescale=1.\/255)\n\n# Flow training images in batches of 16 using train_datagen generator\nprint(\"Preparing generator for train dataset\")\ntrain_generator = train_datagen.flow_from_directory(\n    directory= train_folder, # This is the source directory for training images \n    target_size=image_size, # All images will be resized to value set in image_size\n    batch_size=bs,\n    class_mode='categorical')\n\n# Flow validation images in batches of 16 using val_datagen generator\nprint(\"Preparing generator for validation dataset\")\nval_generator = val_datagen.flow_from_directory(\n    directory= val_folder, \n    target_size=image_size,\n    batch_size=bs,\n    class_mode='categorical')","cfaa7c7b":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, GlobalAveragePooling2D, Dense, Dropout, MaxPooling2D\nfrom tensorflow.keras.applications.densenet import DenseNet121\n\n# Here we specify the input shape of our data \n# This should match the size of images ('image_size') along with the number of channels (3)\ninput_shape = (299, 299, 3)\n\n# Define the number of classes\nnum_classes = 14\n\ninput_img = Input(shape=input_shape)\n\nmodel = DenseNet121(include_top=False, weights=None, input_tensor=input_img, input_shape=input_shape, pooling='avg', classes = num_classes)","d6fc577e":"temp = model.layers[-1].output\n\npredictions = Dense(14, activation='softmax',name = 'last')(temp)\n\nmodel = Model(inputs=input_img, outputs=predictions)","29f970ec":"# loading weights from model trained on ChestXray14 for transfer learning \nmodel_path = 'brucechou1983_CheXNet_Keras_0.3.0_weights.h5'\nmodel.load_weights( model_path )","3c02562b":"from tensorflow.keras import optimizers\n#from keras import regularizers\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=optimizers.Adam(),\n              metrics=['accuracy'])","0f8d1969":"temp = model.layers[-2].output\n\nnum_classes = 9\npredictions = Dense(num_classes, activation='softmax',name = 'last')(temp)\n\nmodel = Model(inputs=input_img, outputs=predictions)\n# train only last layer first\nfor layer in model.layers:\n    if layer.name != 'last':\n        layer.trainable = False","db6df1fc":"model.summary()","80e55384":"from tensorflow.keras import optimizers\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=optimizers.Adam(),\n              metrics=['accuracy'])","ac756fab":"from tensorflow.keras.callbacks import ModelCheckpoint\n\nCheckpointer = ModelCheckpoint('balanced.hdf5', monitor='val_accuracy', save_best_only=True, verbose=1)","d7f3e581":"history = model.fit(\n        train_generator, # train generator has 23777 train images\n        steps_per_epoch=train_generator.samples \/\/ bs + 1,\n        epochs=20,\n        validation_data=val_generator, # validation generator has 5948 validation images\n        validation_steps=val_generator.samples \/\/ bs + 1,\n        callbacks=[Checkpointer]\n)","eb288621":"from tensorflow.keras.models import load_model\n\nmodel_path = 'balanced.hdf5'\nmodel = load_model( model_path )","f2a9c0ce":"# unfreeze all weights and train whole model\n\nfor layer in model.layers:\n  layer.trainable = True\nmodel.summary()\n\nfrom tensorflow.keras import optimizers\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=optimizers.Adam(),\n              metrics=['accuracy'])\n\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nCheckpointer = ModelCheckpoint('balanced2.hdf5', monitor='val_accuracy', save_best_only=True, verbose=1)","ef40c5b4":"history = model.fit(\n        train_generator, # train generator has 23777 train images\n        steps_per_epoch=train_generator.samples \/\/ bs + 1,\n        epochs=20,\n        validation_data=val_generator, # validation generator has 5948 validation images\n        validation_steps=val_generator.samples \/\/ bs + 1,\n        callbacks=[Checkpointer]\n)","40ca2f65":"from tensorflow.keras.callbacks import ModelCheckpoint\n\nCheckpointer = ModelCheckpoint('CheXNet.hdf5', monitor='val_accuracy', save_best_only=True, verbose=1)","b1edc257":"history = model.fit_generator(\n        train_generator, # train generator has 23777 train images\n        steps_per_epoch=train_generator.samples \/\/ bs + 1,\n        epochs=20,\n        validation_data=val_generator, # validation generator has 5948 validation images\n        validation_steps=val_generator.samples \/\/ bs + 1,\n        callbacks=[Checkpointer]\n)","59d47e35":"from tensorflow.keras.models import load_model\n\nmodel_path = 'CheXNet.hdf5'\nmodel = load_model( model_path )","8f52e8bc":"# unfreeze all weights and train whole model\n\nfor layer in model.layers:\n  layer.trainable = True\nmodel.summary()\n\nfrom tensorflow.keras import optimizers\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=optimizers.Adam(),\n              metrics=['accuracy'])\n\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nCheckpointer = ModelCheckpoint('CheXNet2.hdf5', monitor='val_accuracy', save_best_only=True, verbose=1)","1615eb14":"history = model.fit_generator(\n        train_generator, # train generator has 23777 train images\n        steps_per_epoch=train_generator.samples \/\/ bs + 1,\n        epochs=20,\n        validation_data=val_generator, # validation generator has 5948 validation images\n        validation_steps=val_generator.samples \/\/ bs + 1,\n        callbacks=[Checkpointer]\n)","9a36253a":"from tensorflow.keras.models import load_model\n\nmodel_path = 'CheXNet2.hdf5'\nmodel = load_model( model_path )","a9652528":"scores = model.evaluate_generator(train_generator, steps=train_generator.samples \/\/ train_generator.batch_size + 1, verbose=1)\nprint('Train loss:', scores[0])\nprint('Train accuracy:', scores[1])","7139efed":"scores = model.evaluate_generator(val_generator, steps=val_generator.samples \/\/ val_generator.batch_size + 1, verbose=1)\nprint('Validation loss:', scores[0])\nprint('Validation accuracy:', scores[1])","2f77bb2d":"test_datagen = ImageDataGenerator(rescale=1.\/255)\n\nprint(\"Preparing generator for test dataset\")\ntest_generator = test_datagen.flow_from_directory(\n    directory= test_folder, \n    target_size=image_size,\n    batch_size=16,\n    shuffle=False,\n    class_mode='categorical')\n\nscores = model.evaluate_generator(test_generator, steps=test_generator.samples \/\/ test_generator.batch_size + 1, verbose=1)\nprint('Test loss:', scores[0])\nprint('Test accuracy:', scores[1])","010a1e28":"from tensorflow.keras.models import load_model\nfrom tensorflow.keras.models import Model\n\nmodel_path = 'CheXNet2.hdf5'\nmodel = load_model( model_path )","fc4f812d":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n\ntest_datagen = ImageDataGenerator(rescale=1.\/255)\n\nprint(\"Preparing generator for test dataset\")\ntest_generator = test_datagen.flow_from_directory(\n    directory= test_folder, \n    target_size=(299,299),\n    batch_size=16,\n    shuffle=False,\n    class_mode='categorical')\n\n#scores = model.evaluate(test_generator, steps=test_generator.samples \/\/ test_generator.batch_size + 1, verbose=1)\n#print('Test loss:', scores[0])\n#print('Test accuracy:', scores[1])","028a465b":"import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\ninput_shape = (299, 299, 3)\nnum_classes = 9\nnb_samples = 7433\nbs = 16\n\ny_test = pd.get_dummies(pd.Series(test_generator.classes))\ny_pred =  model.predict(test_generator,steps = nb_samples\/\/bs + 1, verbose=1)\ny_test = y_test.to_numpy()\ny_preds = [np.argmax(i) for i in y_pred]\ny_preds = np.asarray(y_preds)\ny_tests = [np.argmax(i) for i in y_test]\ny_tests = np.asarray(y_tests)\n\ncnf_matrix1 = confusion_matrix(test_generator.classes, y_preds)\n\nLABELS = [\"atelectasis\",\"cardiomegaly\",\"consolidation\",\"effusion\",\"infiltration\",\"mass\",\"no_finding\",\"nodule\",\"pneumothorax\"]\n\nplt.figure(figsize=(12, 12))\nsns.heatmap(cnf_matrix1, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\");\nplt.title(\"Confusion matrix (Unnormalized)\")\nplt.ylabel('True class')\nplt.xlabel('Predicted class')\nplt.show()","e35ff34d":"cnf_matrix2 = confusion_matrix(y_tests, y_preds, normalize = 'true')\n\nplt.figure(figsize=(12, 12))\nsns.heatmap(cnf_matrix2, xticklabels=LABELS, yticklabels=LABELS, annot=True,fmt=\"0.3f\",annot_kws={\"size\": 15});\nplt.title(\"Confusion matrix for 'Augmented' Model (Row Normalized)\")\nplt.ylabel('True class')\nplt.xlabel('Predicted class')\nplt.show()","a56c724f":"cnf_matrix3 = confusion_matrix(y_tests, y_preds, normalize = 'pred')\n\nplt.figure(figsize=(12, 12))\nsns.heatmap(cnf_matrix3, xticklabels=LABELS, yticklabels=LABELS, annot=True);\nplt.title(\"Confusion matrix (Column Normalized)\")\nplt.ylabel('True class')\nplt.xlabel('Predicted class')\nplt.show()","bb01dec9":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\n\n# Compute ROC curve and ROC area for each class\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(num_classes):\n    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\nfor i in range(num_classes):\n    plt.plot(fpr[i], tpr[i],\n             label='ROC curve of class {0} (area = {1:0.2f})'\n             ''.format(i, roc_auc[i]))\nplt.title(\"ROC for 'Augmented' Model\")\nplt.legend(loc='best')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()\n\nAUCsum = 0\nfor i in range(num_classes):\n    print(\"AUC of class {0} = {1:0.4f}\".format(LABELS[i], roc_auc[i]))\n    AUCsum +=roc_auc[i]\nprint(\"Average ROC AUC = {0:0.4f}\".format(AUCsum\/9))","d292746b":"from sklearn.metrics import precision_recall_curve\n\n# Compute PR curve and area for each class\nprecision = dict()\nrecall = dict()\npr_auc = dict()\n\nf_scores = np.linspace(0.2, 0.8, num=4)\nlines = []\nlabels = []\nfor f_score in f_scores:\n    x = np.linspace(0.01, 1)\n    y = f_score * x \/ (2 * x - f_score)\n    l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n    plt.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n\nlines.append(l)\nlabels.append('iso-f1 curves')\n\nfor i in range(num_classes):\n    precision[i], recall[i], _ = precision_recall_curve(y_test[:, i], y_pred[:, i])\n    pr_auc[i] = auc(recall[i], precision[i])\nfor i in range(num_classes):\n    plt.plot(recall[i], precision[i],\n             label='PR curve of class {0} (area = {1:0.4f})'\n             ''.format(i, pr_auc[i]))\nplt.title(\"Precision Recall Curve for 'Augmented' Model\")\nplt.legend(loc=(0.05, -1.05), prop=dict(size=14))\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.xlim([0.0, 1.05])\nplt.ylim([0.0, 1.05])\nplt.show()\nAUCsum = 0\nfor i in range(num_classes):\n    print(\"AUC of class {0} = {1:0.4f}\".format(LABELS[i], pr_auc[i]))\n    AUCsum +=pr_auc[i]\nprint(\"Average PR AUC = {0:0.4f}\".format(AUCsum\/9))","67fdc767":"# getting the indices of the images for each class that is classified correctly and classified wrongly\n\nprint(y_tests[0])\nprint(y_preds[0])\nprint(y_tests[0 + 1])\nprint(y_preds[0 + 1])\nactelectasis_idx = 1\nprint(\"Top\")\nprint(y_tests[843])\nprint(y_preds[843])\nprint(y_tests[843 + 2])\nprint(y_preds[843 + 2])\ncardiomegaly_idx = 2\nprint(\"Top\")\nprint(y_tests[1062])\nprint(y_preds[1062])\nprint(y_tests[1062 + 145])\nprint(y_preds[1062 + 145])\nconsolidation_idx = 145\nprint(\"Bottom\")\nprint(y_tests[1324])\nprint(y_preds[1324])\nprint(y_tests[1324 + 1])\nprint(y_preds[1324 + 1])\neffusion_idx = 1\nprint(\"Bottom\")\nprint(y_tests[2115])\nprint(y_preds[2115])\nprint(y_tests[2115 + 1])\nprint(y_preds[2115 + 1])\ninfiltration_idx = 1\nprint(\"Bottom\")\nprint(y_tests[4025])\nprint(y_preds[4025])\nprint(y_tests[4025 + 1])\nprint(y_preds[4025 + 1])\nmass_idx = 1\nprint(\"Top\")\nprint(y_tests[4453])\nprint(y_preds[4453])\nprint(y_tests[4453 + 1])\nprint(y_preds[4453 + 1])\nnofinding_idx = 1\nprint(\"Bottom\")\nprint(y_tests[6453])\nprint(y_preds[6453])\nprint(y_tests[6453 + 19])\nprint(y_preds[6453 + 19])\nnodule_idx = 19\nprint(\"Bottom\")\nprint(y_tests[6994])\nprint(y_preds[6994])\nprint(y_tests[6994 + 438])\nprint(y_preds[6994 + 438])\npneumothorax_idx = 438\nprint(\"Bottom\")\n#print(np.where(y_preds == 7))\n#print(np.where(y_preds == 2))","74c8910c":"from PIL import Image\nfrom vis.visualization import visualize_cam, overlay\n\ndef visualize_top_convolution(model,image_batch):\n    layer_idx=-1\n\n    # credit: https:\/\/github.com\/raghakot\/keras-vis\/blob\/master\/applications\/self_driving\/visualize_attention.ipynb\n    heatmap = visualize_cam(model, layer_idx=-1, filter_indices=None, seed_input=image_batch, grad_modifier=None)\n    img = image_batch.squeeze()\n\n    # credit: (gaussian filter for a better UI) http:\/\/bradsliz.com\/2017-12-21-saliency-maps\/\n    import scipy.ndimage as ndimage\n    smooth_heatmap = ndimage.gaussian_filter(heatmap[:,:,2], sigma=5)\n\n    nn = 5\n    fig = plt.figure(figsize=(20,20))\n    a = fig.add_subplot(1, nn, 1)\n    plt.imshow(img)\n    a.set_title(\"original\",fontsize=10)\n    plt.axis('off')\n    a = fig.add_subplot(1, nn, 2)\n    plt.imshow(overlay(img, heatmap, alpha=0.7))\n    a.set_title(\"heatmap\",fontsize=10)\n    plt.axis('off')\n    a = fig.add_subplot(1, nn, 3)\n    plt.imshow(img)\n    plt.imshow(smooth_heatmap, alpha=0.7)\n    a.set_title(\"heatmap\/gaussian\",fontsize=10)\n    plt.axis('off')\n    plt.show()","b7d843d5":"filenames = os.listdir(atelectasis_dir)\nfilenames.sort()\nfilename1 = filenames[0]\nfilename2 = filenames[actelectasis_idx]\n\npath1 = os.path.join(atelectasis_dir, filename1)\nimage1 = mpimg.imread(path1)\nimage1 = np.stack((image1,image1, image1), axis=2)\n\npath2 = os.path.join(atelectasis_dir, filename2)\nimage2 = mpimg.imread(path2)\nimage2 = np.stack((image2,image2, image2), axis=2)\n\nprint(\"**Atelectasis Correct Prediction**\")\nvisualize_top_convolution(model,image1)\nprint(\"**Atelectasis mistaken for Cardiomegaly**\")\nvisualize_top_convolution(model,image2)","371bdb7e":"filenames = os.listdir(cardiomegaly_dir)\nfilenames.sort()\nfilename1 = filenames[0]\nfilename2 = filenames[cardiomegaly_idx]\n\npath1 = os.path.join(cardiomegaly_dir, filename1)\nimage1 = mpimg.imread(path1)\nimage1 = np.stack((image1,image1, image1), axis=2)\n\npath2 = os.path.join(cardiomegaly_dir, filename2)\nimage2 = mpimg.imread(path2)\nimage2 = np.stack((image2,image2, image2), axis=2)\n\nprint(\"**Cardiomegaly Correct Prediction**\")\nvisualize_top_convolution(model,image1)\nprint(\"**Cardiomegaly mistaken for No Finding**\")\nvisualize_top_convolution(model,image2)","c979c30c":"filenames = os.listdir(consolidation_dir)\nfilenames.sort()\nfilename1 = filenames[0]\nfilename2 = filenames[consolidation_idx]\n\npath1 = os.path.join(consolidation_dir, filename1)\nimage1 = mpimg.imread(path1)\nimage1 = np.stack((image1,image1, image1), axis=2)\n\npath2 = os.path.join(consolidation_dir, filename2)\nimage2 = mpimg.imread(path2)\nimage2 = np.stack((image2,image2, image2), axis=2)\n\nprint(\"**Consolidation Correct Prediction**\")\nvisualize_top_convolution(model,image2)\nprint(\"**Consolidation mistaken for Infiltration**\")\nvisualize_top_convolution(model,image1)","99400a54":"filenames = os.listdir(effusion_dir)\nfilenames.sort()\nfilename1 = filenames[0]\nfilename2 = filenames[effusion_idx]\n\npath1 = os.path.join(effusion_dir, filename1)\nimage1 = mpimg.imread(path1)\nimage1 = np.stack((image1,image1, image1), axis=2)\n\npath2 = os.path.join(effusion_dir, filename2)\nimage2 = mpimg.imread(path2)\nimage2 = np.stack((image2,image2, image2), axis=2)\n\nprint(\"**Effusion Correct Prediction**\")\nvisualize_top_convolution(model,image2)\nprint(\"**Effusion mistaken for No Finding**\")\nvisualize_top_convolution(model,image1)","34757b11":"filenames = os.listdir(infiltration_dir)\nfilenames.sort()\nfilename1 = filenames[0]\nfilename2 = filenames[infiltration_idx]\n\npath1 = os.path.join(infiltration_dir, filename1)\nimage1 = mpimg.imread(path1)\nimage1 = np.stack((image1,image1, image1), axis=2)\n\npath2 = os.path.join(infiltration_dir, filename2)\nimage2 = mpimg.imread(path2)\nimage2 = np.stack((image2,image2, image2), axis=2)\n\nprint(\"**Infiltration Correct Prediction**\")\nvisualize_top_convolution(model,image2)\nprint(\"**Infiltration mistaken for No Finding**\")\nvisualize_top_convolution(model,image1)","21849bff":"filenames = os.listdir(mass_dir)\nfilenames.sort()\nfilename1 = filenames[0]\nfilename2 = filenames[mass_idx]\n\npath1 = os.path.join(mass_dir, filename1)\nimage1 = mpimg.imread(path1)\nimage1 = np.stack((image1,image1, image1), axis=2)\n\npath2 = os.path.join(mass_dir, filename2)\nimage2 = mpimg.imread(path2)\nimage2 = np.stack((image2,image2, image2), axis=2)\n\nprint(\"**Mass Correct Prediction**\")\nvisualize_top_convolution(model,image1)\nprint(\"**Mass mistaken for Effusion**\")\nvisualize_top_convolution(model,image2)","a4664159":"filenames = os.listdir(nofinding_dir)\nfilenames.sort()\nfilename1 = filenames[0]\nfilename2 = filenames[nofinding_idx]\n\npath1 = os.path.join(nofinding_dir, filename1)\nimage1 = mpimg.imread(path1)\nimage1 = np.stack((image1,image1, image1), axis=2)\n\npath2 = os.path.join(nofinding_dir, filename2)\nimage2 = mpimg.imread(path2)\nimage2 = np.stack((image2,image2, image2), axis=2)\n\nprint(\"**No Finding Correct Prediction**\")\nvisualize_top_convolution(model,image2)\nprint(\"**No Finding mistaken for Cardiomegaly**\")\nvisualize_top_convolution(model,image1)","fb604130":"filenames = os.listdir(nodule_dir)\nfilenames.sort()\nfilename1 = filenames[0]\nfilename2 = filenames[nodule_idx]\n\npath1 = os.path.join(nodule_dir, filename1)\nimage1 = mpimg.imread(path1)\nimage1 = np.stack((image1,image1, image1), axis=2)\n\npath2 = os.path.join(nodule_dir, filename2)\nimage2 = mpimg.imread(path2)\nimage2 = np.stack((image2,image2, image2), axis=2)\n\nprint(\"**Nodule Correct Prediction**\")\nvisualize_top_convolution(model,image2)\nprint(\"**Nodule mistaken for No Finding**\")\nvisualize_top_convolution(model,image1)","585e84b2":"filenames = os.listdir(pneumothorax_dir)\nfilenames.sort()\nfilename1 = filenames[0]\nfilename2 = filenames[pneumothorax_idx]\n\npath1 = os.path.join(pneumothorax_dir, filename1)\nimage1 = mpimg.imread(path1)\nimage1 = np.stack((image1,image1, image1), axis=2)\n\npath2 = os.path.join(pneumothorax_dir, filename2)\nimage2 = mpimg.imread(path2)\nimage2 = np.stack((image2,image2, image2), axis=2)\n\nprint(\"**Pneumothorax Correct Prediction**\")\nvisualize_top_convolution(model,image2)\nprint(\"**Pneumothorax mistaken for Effusion**\")\nvisualize_top_convolution(model,image1)","0f328a01":"Training (skip to evaluation if you do not want to train the models from scratch)","23d8b57c":"RANDOM UPSAMPLING to balance the dataset for the 'balanced' model\n#Do not run this section if you are trying to create the CheXNet model ('augmented' model)","68074eda":"Run this section to create the 'balanced' model","15ea8a80":"Continue running the code from here if you are trying to train the CheXNet model ('augmented' model)","1e6d0d96":"Class Activation Map for CheXNet","9bacd45f":"SMOTE\/ADASYN just superimposes imagese to create new samples which can create multiple sets of lungs","21ba4e7e":"Plot some X-rays for inspection","523bae52":"Here is what happens if we naively turn the image into a 1-D array and use SMOTE\/ADASYN to upsample. (To deal with imbalanced dataset)","ee2a6187":"This notebook trains 2 models, the first is a recreation of CheXNet (paper below). This is dubbed the 'augmented' model in my paper. The second model is the 'balanced' model which is similar to CheXNet except that I trained it on a randomly upsampled balanced datatset. As it turns out the 'balanced' model turned out worse and only the 'augmented' model is evaluated.\n\n(Rajpurkar, P., Irvin, J., Zhu, K., Yang, B., Mehta, H., Duan, T., Ding, D., Bagul, A., Langlotz, C., Shpanskaya, K. and Lungren, M.P., 2017. Chexnet: Radiologist-level pneumonia detection on chest x-rays with deep learning. arXiv preprint arXiv:1711.05225.)","8277624e":"Run this section to recreate CheXNet ('Augmented' Model)","a641744e":"Evaluation of CheXNet recreation ('augmented' model)"}}