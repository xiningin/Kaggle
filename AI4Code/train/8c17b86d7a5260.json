{"cell_type":{"0ffeae9a":"code","62af29cf":"code","dcbc8000":"code","99f5aa2e":"code","3a78acac":"code","c54550aa":"code","3212be60":"code","77f74f65":"code","cbfe9004":"code","21d6a940":"code","ecd355e3":"code","9d4081e1":"code","ba7e0faa":"code","6cc9b2db":"markdown","5e034953":"markdown","4d55eecc":"markdown","4506cfb7":"markdown","363d5d45":"markdown","2b745622":"markdown","13faaf59":"markdown","7b28417a":"markdown","c0c06183":"markdown","c07954ce":"markdown","0bd0801e":"markdown"},"source":{"0ffeae9a":"import numpy as np\nimport tensorflow as tf\nimport pandas as pd\nimport cv2\nimport glob\nimport matplotlib.pyplot as plt\nimport random\nfrom random import randint\nfrom pathlib import Path\nimport os\nprint(os.listdir(\"..\/input\"))","62af29cf":"train_dir = '..\/input\/chest_xray\/chest_xray\/train'\ntest_dir =  '..\/input\/chest_xray\/chest_xray\/test'\nX = []\nY = []\n\n#Loop through the training and test folders, as well as the 'NORMAL' and 'PNEUMONIA' subfolders and append all images into array X.  Append the classification (0 or 1) into array Y.\n\nfor fileName in os.listdir(train_dir + \"\/NORMAL\"): \n        img = cv2.imread(train_dir + \"\/NORMAL\/\" + fileName)\n        if img is not None:\n            Y.append(0)\n            img = cv2.resize(img,(64,64))\n            X.append(img)\n    \nfor fileName in os.listdir(train_dir + \"\/PNEUMONIA\"): \n        img = cv2.imread(train_dir + \"\/PNEUMONIA\/\" + fileName)\n        if img is not None:\n            Y.append(1)\n            img = cv2.resize(img,(64,64))\n            X.append(img)\n            \nfor fileName in os.listdir(test_dir + \"\/NORMAL\"): \n        img = cv2.imread(test_dir + \"\/NORMAL\/\" + fileName)\n        if img is not None:\n            Y.append(0)\n            img = cv2.resize(img,(64,64))\n            X.append(img)\n    \nfor fileName in os.listdir(test_dir + \"\/PNEUMONIA\"): \n        img = cv2.imread(test_dir + \"\/PNEUMONIA\/\" + fileName)\n        if img is not None:\n            Y.append(1)\n            img = cv2.resize(img,(64,64))\n            X.append(img)\n","dcbc8000":"print(\"This is an example of a patient X-ray who does not have pneumonia:\")\nnormal = cv2.imread(test_dir + \"\/NORMAL\/IM-0003-0001.jpeg\")\nplt.axis('off')\nplt.imshow(normal)\n","99f5aa2e":"print(\"This is an example of an X-ray of a patient diagnosed with pneumonia:\")\npnumonia = cv2.imread(test_dir + \"\/PNEUMONIA\/person15_virus_46.jpeg\")\nplt.axis('off')\nplt.imshow(pnumonia)","3a78acac":"#Data visualization\n\npos = 0\nneg = 0\n\nfor i in range(1,len(Y)):\n    if Y[i] == 1:\n        pos = pos + 1\n    else:\n        neg = neg + 1\n\nobjects = ('Positive', 'Negative')\ny_pos = np.arange(len(objects))\nperformance = [pos,neg]\n \nplt.bar(y_pos, performance, align='center', alpha=0.5)\nplt.xticks(y_pos, objects)\nplt.ylabel('Count')\nplt.title('X-Ray Diagnosis')\n \nplt.show()\n\nprint(\"There are \" +str(pos) +\" pneumonia positive X-ray's in our data\")\nprint(\"There are \" +str(neg) +\" pneumonia negative X-ray's in our data\")","c54550aa":"#Normalize our images to ensure gradients do not diverge\nX = np.array(X)\/255\n\n#Normalize our data by setting the mean to 0 and variance to 1.\nX = (X - np.average(X,0))\/np.std(X,0)\n\n#Convert our Y vector into a categorical (e.g. 0 -> (0,1), 1 -> (1,0))           \nfrom keras.utils.np_utils import to_categorical\nY = to_categorical(Y, num_classes = 2) #Randomly create our training and test sets using 2\/3 of the data for training, and the remaining 1\/3 for testing\n","3212be60":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.33, random_state=42)\n\npos_train =0\nneg_train = 0\npos_test = 0\nneg_test = 0\n\n\nfor i in range(1,len(Y_train)):\n    if Y_train[i][0] == 1:\n        pos_train = pos_train + 1\n    else:\n        neg_train = neg_train + 1\n        \nfor i in range(1,len(Y_test)):\n    if Y_test[i][0] == 1:\n        pos_test = pos_test + 1\n    else:\n        neg_test = neg_test + 1\n        \nprint(\"Positive in training: \" + str(pos_train))\nprint(\"Negative in training: \" + str(neg_train))\nprint(\"Train pos\/neg ratio: \" + str (pos_train\/neg_train))\n      \nprint(\"Positive in test: \" + str(pos_test))\nprint(\"Negative in test: \" + str(neg_test))\nprint(\"Test pos\/neg ratio: \" + str (pos_test\/neg_test))","77f74f65":"def create_placeholders(n_H0, n_W0, n_C0, n_y):\n\n    X = tf.placeholder(tf.float32,[None,n_H0,n_W0,n_C0])\n    Y = tf.placeholder(tf.float32,[None,n_y])\n    \n    return X, Y","cbfe9004":"\ndef initialize_parameters():\n    W1 = tf.get_variable(\"W1\",[3,3,3,10],initializer = tf.contrib.layers.xavier_initializer()) #Define our weight for the first convolutional layer\n    W2 = tf.get_variable(\"W2\",[5,5,10,8],initializer = tf.contrib.layers.xavier_initializer()) #Define our weight for the second convolutional layer\n    \n    parameters ={'W1':W1,\n                'W2':W2,\n                }\n    \n    return parameters\n","21d6a940":"#Define our forward propogation algorithm\ndef forward_propagation(X,parameters):\n    W1 = parameters['W1']\n    W2 = parameters['W2']\n    \n    Z1 = tf.nn.conv2d(X,W1,strides=[1,1,1,1],padding='SAME') #First convolution\n    A1 = tf.nn.relu(Z1) #ReLu activation on first convolution\n    Z2 = tf.nn.conv2d(A1,W2,strides=[1,1,1,1],padding='SAME') #Second convolution\n    A2 = tf.nn.relu(Z2) #ReLu activation on second convolution\n    P = tf.contrib.layers.flatten(A2) #Flatten A2 into a vector\n    Z3 = tf.contrib.layers.fully_connected(P,2,activation_fn=None) #Apply fully connected layer with two outputs\n    A3 = tf.nn.sigmoid(Z3) #Sigmoid activation on fully connected layer\n    \n    return A3","ecd355e3":"#Define our cost function\ndef compute_cost(A3,Y):\n    \n    cost = -tf.reduce_sum(Y*tf.log(A3) + (1-Y)*tf.log(1-A3)) \n    \n    return cost","9d4081e1":"def model(X_train,Y_train,X_test,Y_test,learning_rate,mini_batch_size,epochs):\n    costs = [] #Set up an array to store our costs at each iteration\n    tf.reset_default_graph() #Reset our TensorFlow graph\n    X,Y = create_placeholders(64,64,3,2) #Create our placeholder variables\n    parameters = initialize_parameters() #Initialize our weight parameters\n    A3 = forward_propagation(X,parameters) #Assign our forward propogation output to a variable\n    cost = compute_cost(A3,Y) #Assign our cost to a varaible\n    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost) #Define our optimzer (Adam Optimization)\n    init = tf.global_variables_initializer() #Initialize the variables in our TensorFlow graph\n    temp_cost = 0\n    with tf.Session() as sess:\n        sess.run(init)\n        \n        for i in range(1,epochs + 1): #Iterate for the number of user defined epochs\n            z = randint(0,X_train.shape[0] - mini_batch_size) #Chose a random integer between 0 and our sample size - user defined mini-batch size\n            _ , temp_cost = sess.run([optimizer,cost],feed_dict={X: X_train[z:z+mini_batch_size], Y: Y_train[z:z+mini_batch_size]}) #Run our optimization on a randomly selected mini-batch of size 'mini_batch_size'\n            #print(\"iteration \" + str(i) +\" \" + str(temp_cost))\n            costs.append(temp_cost)\n            \n        parameters = sess.run(parameters)\n        print (\"Parameters have been optimized.\")\n        \n        #Plot our logged costs by iteration\n        plt.plot(np.squeeze(costs))\n        plt.ylabel('cost')\n        plt.xlabel('iterations')\n        plt.title(\"Learning rate =\" + str(learning_rate))\n        plt.show()\n\n        # Calculate the correct predictions\n        predictions_train = np.argmax(np.array(sess.run(A3,feed_dict={X:X_train,Y:Y_train})),1)\n    \n        accuracy_bool_train = predictions_train == np.argmax(Y_train,1)\n        accuracy_perc_train = np.average(accuracy_bool_train.astype(int))\n        \n        predictions_test = np.argmax(np.array(sess.run(A3,feed_dict={X:X_test,Y:Y_test})),1)\n    \n        accuracy_bool_test = predictions_test == np.argmax(Y_test,1)\n        accuracy_perc_test = np.average(accuracy_bool_test.astype(int))\n        \n        print('Training set accuracy is: ' + str(100*accuracy_perc_train) + \"%\")\n        print('Test set accuracy is: ' + str(100*accuracy_perc_test) + \"%\")\n        \n        count_pos = 0\n        count_true_pos = 0\n        count_neg = 0\n        count_true_neg = 0\n        \n        #The following loop will determine the accuracy of predicting pneumonia positive and negative x-ray\n        for i in range(1,predictions_test.shape[0]):\n            if predictions_test[i] == 1 and np.argmax(Y_test,1)[i]==1:\n                count_pos = count_pos+1\n            if np.argmax(Y_test,1)[i] == 1:\n                count_true_pos = count_true_pos + 1\n            if predictions_test[i] == 0 and np.argmax(Y_test,1)[i] == 0:\n                count_neg = count_neg + 1\n            if np.argmax(Y_test,1)[i] == 0:\n                count_true_neg = count_true_neg + 1\n                \n        print(\"Positive pneumonia prediction accuracy: \" + str((count_pos\/count_true_pos)*100) + \"%\")\n        print(\"Negative pneumonia prediction accuracy: \" + str((count_neg\/count_true_neg)*100) + \"%\")\n    \n        return accuracy_perc_train,accuracy_perc_test\n            \n        ","ba7e0faa":"tf.reset_default_graph()\nmodel(X_train[1:3390],Y_train[1:3390],X_test,Y_test,0.000025,32,1500)","6cc9b2db":"# Step 1.1 - Data visualization\n\nLet's visualize our data before we split it into our training and test sets in the following steps:\n\n* Observe an example of a positive and negative pneumonia diagnosed X-ray to see what our algorithm is trying to detect\n\n* Count the number of pneumonia positive and negative results in our data set (i.e. our data's distribution)\n\n* Create a bar graph of the positive and negative count as a simple visualization of our distribution\n\nLet's start with our visualization:","5e034953":"# Step 4 - Analyzing our results\n\nOur results are looking good.  In addition to our overall excellent training and test prediction accuracy, both our positive and negative predictions independently have beyond acceptable accuracy.\n\nLooking at our cost vs. iterations graph, we see a general downward trend which seems to flatten out.  This suggests we have chosen an appropriate learning rate and number of iterations to balance weight optimization time and accuracy. \n\nWe have created a convolutional neural network that can be used to predict pneumonia from X-ray images!  \n\nThis concludes the kernel.\n\n# Thank you for viewing!","4d55eecc":"# Step 1.2 - Normalizing and splitting our data\n\nWe can see our data is biased to pneumonia positive results.\n\nIt will be important to check the distribution of our training and test sets after we randomly shuffle and create them.  \n\nIn addition, to ensure our gradients do not diverge, we will normalize our image data.","4506cfb7":"# Step 1 - Visualize our image data and split it into training and test sets\n\n\nThe dataset has been split into 'train' and 'test'' folders.  Additionally, the images within these folders have been split into 'NORMAL' and 'PNEUMONIA' subfolders. \n\nFirst, we will go through the folders of the dataset and append each image into an array. Both the training and test set images will be placed into the same matrices for now (later to be randomly split into our training and test sets).  Note that each image will be resized to 64x64 to reduce computation time and to maintain consistency throughout our dataset.\n\nBecause this is a binary classification problem, each image will have an associated classification vector of the form (1,0), meaning the patient has pneumonia, or (0,1), meaning the patient does not have pneumonia. \n\nAfter our images have been pulled and classified, they will be normalized.  This is done to ensure that our gradients do not diverge during back propagation. \n\nFinally, our data will be split into training and test sets.  The training set will use 2\/3 of the available X-ray images.","363d5d45":"# Step 3 - Define our model\n\nNow that our variables, forward propagation and cost function have been defined, we can bring this all together to create our convolutional neural network model.\n\nWe will start by setting up the structure of our network, and then coding a mini-batch optimization loop.  We will log the costs as this loop progresses to ensure the algorithm is behaving as intended (these costs will be plotted against iterations)\n\nWe will start by resetting out TensorFlow graph for each model call\n\nThe following steps will be taken to set up the structure of our network:\n\n* Call and assign out placeholder values with the appropriate parameters\n\n* Initialize our weight parameters\n\n* Assign our forward propogation to a variable\n\n* Assign our cost function to a variable\n\n* Define our optimization algorithm (we will be using an Adam Optimizer for this network)\n\n* Initialize the variables in the TensorFlow Graph\n\n","2b745622":"Looks like our data has a similar distribution for both our training and test sets.","13faaf59":"Now we will define our forward propagation algorithm and cost function.","7b28417a":"# Convolutional Neural Network - Pneumonia Prediction Using X-Ray Images\n\nWelcome to my third machine learning project! \n\nWe will be using TensorFlow to create a convolutional neural network with two convolutional layers and one fully connected layer to predict pneumonia, given a chest X-ray image as the input. \n\nLet's start by importing the packages we will be using throughout the kernel:","c0c06183":"# Step 2 - Set up TensorFlow variables\n\nWe will now set up our TensorFlow variables to be used in our forward propogation and cost function.\n\nFirst, we set up placeholder values for our image and classification matrices.  \n\n* X will represent our image matrix, which will have dimensions #samples x height x width x # filters\n\n* Y will represent our classification matrix, which will have dimensions # samples x # classifications.  In our case, the # classifications is 2 (0 or 1)","c07954ce":"Next, we will set up our randomly initialized weights for each of our layers.  Recall we are creating a convolutional neural network with two convolutional layers and one fully connected layer.  \n\nOur weights must be initialized appropriately to correspond with our desired network.  The dimension of our weights, as initialized below, is (height, width, layers, # filters).  The following will be the structure of our convolutional neural network, and our weights will be initialized accordingly:\n\n* The first convolutional layer will consisnt of 10 3-layered 3x3 filters.  A ReLu activation function will be applied after convolution.  Our weights for this layer should be of dimension (3,3,3,10)\n\n* The second convolutional layer will consist of 8 10-layered 5x5 filters. A ReLu activation function will be applied after convolution. Our weights for this layer should be of dimension (5,5,10,8)\n\n* Our final layer, the fully connected one, will flatten out the values from the second convolutional layer and apply a sigmoid activation\n\nFor simplicity, we will not use any pooling layers, nor will we use padding or strides > 1.\n\nSee below for a visual representation of our convolutional neural network for a single image:\n\n<img src=\"https:\/\/imgur.com\/ZDKAac5.png\" width=\"1000px\"\/>","0bd0801e":"We have now seen an example of both positive and negative pneumonia X-rays.  Note the 'fog' like discoloration in the pneumonia positive X-ray.  This is essentially what we would like our algorithm to detect.\n\nLet's see how our classifications are distributed throughout out dataset:"}}