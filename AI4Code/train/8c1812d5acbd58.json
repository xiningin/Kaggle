{"cell_type":{"0b81f258":"code","e4df1444":"code","1d18d057":"code","5af54b09":"code","115b78f4":"code","41455940":"code","478d681d":"code","921b7ce3":"code","d41d1fec":"code","238d183c":"code","65452b10":"code","353428d4":"code","986b1a51":"code","7bc3cd79":"code","0fe56b50":"code","8ca089d7":"code","99c4db72":"markdown","b75df287":"markdown","24c9a877":"markdown","9a0964b6":"markdown","b4b05a20":"markdown","e555ad46":"markdown","5aad9f5d":"markdown","1db3b49e":"markdown","0c16dbfa":"markdown","3c951540":"markdown","eac9cd15":"markdown","3aa49348":"markdown","d851c9fb":"markdown","45cdd815":"markdown","e6d327e4":"markdown","732d419e":"markdown","2f2a375f":"markdown","b590a206":"markdown","343c9196":"markdown","ed65305a":"markdown","5b516bea":"markdown"},"source":{"0b81f258":"#Core packages\nimport pandas as pd\nimport numpy as np\n\n#Data Profiling\nimport pandas_profiling as pp\n\n#Visualisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#sklearn\nimport category_encoders as ce\nfrom sklearn.preprocessing import PowerTransformer, RobustScaler\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LinearRegression, ElasticNet,ElasticNetCV\n\n#XGBoost\nfrom xgboost import XGBRegressor","e4df1444":"train = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\n\nprint(test.shape)\nprint(train.shape)","1d18d057":"featuresDF = train.drop(['Id','SalePrice'],axis=1)   \nfeaturesDF.profile_report(style={'full_width':True})","5af54b09":"# Filling 'None' for missing values\nfor col in ('Alley','Utilities','MasVnrType','BsmtQual','BsmtCond','BsmtExposure',\n            'BsmtFinType1','BsmtFinType2','Electrical','FireplaceQu','GarageType',\n            'GarageFinish','GarageQual','GarageCond','PoolQC','Fence','MiscFeature'):\n    train.loc[:,col] = train.loc[:,col].fillna('None')\n    test.loc[:,col] = test.loc[:,col].fillna('None')\n\n# Filling the mode value for the features actual missing values\nfor col in ('MSZoning','Exterior1st','Exterior2nd','KitchenQual','SaleType','Functional'):\n    train.loc[:,col] = train.loc[:,col].fillna(train.loc[:,col].mode()[0])\n    test.loc[:,col] = test.loc[:,col].fillna(train.loc[:,col].mode()[0])","115b78f4":"# Filling '0' for missing values\nfor col in ('2ndFlrSF','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF',\n            'BsmtHalfBath','GarageCars','TotalBsmtSF', 'BsmtFullBath'):\n    train.loc[:,col] = train.loc[:,col].fillna(0)\n    test.loc[:,col] = test.loc[:,col].fillna(0)\n\n# Filling the mean value for the missing values\ntrain.loc[:,'LotFrontage'] = train.loc[:,'LotFrontage'].fillna(train.loc[:,'LotFrontage'].mean())\ntest.loc[:,'LotFrontage'] = test.loc[:,'LotFrontage'].fillna(train.loc[:,'LotFrontage'].mean())","41455940":"train = train.loc[train.loc[:,'GrLivArea']<4000,:]\nidx_train = train.shape[0] #keep track of the training inde\nidx_train","478d681d":"corrMatrix = train.drop('Id',axis=1).corr()\nplt.figure(figsize=[30,15])\nheatmap = sns.heatmap(corrMatrix, annot=True, cmap='Blues')","921b7ce3":"df = pd.concat([train,test], sort=False)\ndf.drop(['TotRmsAbvGrd','GarageArea', 'GarageYrBlt', '1stFlrSF'], axis=1, inplace=True)\ndf.shape","d41d1fec":"# transforming MSSubClass to category type\ndf.loc[:,'MSSubClass'] = df.loc[:,'MSSubClass'].astype('category')\nprint(df.loc[:,'MSSubClass'].dtype)\n\n#splitting the dataset in test id, the training set and the testing set\ntest_id = df.iloc[idx_train:,0]\ndf.drop(['Id'], axis=1, inplace=True)\n\ntrain = df.iloc[:idx_train,:]\ntest = df.iloc[idx_train:,:]\n\nprint(test.shape)\nprint(train.shape)","238d183c":"#saving predictor columns and label column\npredictors = list(df.columns.drop(['SalePrice']))\nlabel = 'SalePrice'\n\n#splitting the dataset\nX_train = df.iloc[:idx_train, :].loc[:, predictors]\ny_train = df.iloc[:idx_train, :].loc[:, label]\nX_test = df.iloc[idx_train:, :].loc[:, predictors]\n\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)","65452b10":"#obtaining numerical features with absolute skewness > .5\nskewed_cols_bool = np.absolute(df.select_dtypes(include=['int','float']).skew(axis = 0)) > .5\nskewed_cols = skewed_cols_bool.loc[skewed_cols_bool].index.drop('SalePrice').tolist()\n\n#applying a power transformer to skweded cols\npt = PowerTransformer()\nX_train.loc[:,skewed_cols] = pt.fit_transform(X_train.loc[:,skewed_cols])\nX_test.loc[:,skewed_cols] = pt.transform(X_test.loc[:,skewed_cols])\n\n# print(X_test.shape)\n# print(X_train.shape)\n# print(X_train.head())\n# print(skewed_cols)","353428d4":"#one-hot encoding\ndf = pd.concat([X_train,X_test], sort=False)\ndf = pd.get_dummies(df)\n\nX_train = df.iloc[:idx_train, :]\nX_test = df.iloc[idx_train:, :]\n\nprint(df.shape)\nprint(X_train.shape)\nprint(X_test.shape)","986b1a51":"scaler=RobustScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\nprint(X_train.shape)\nprint(X_test.shape)","7bc3cd79":"#log transforming the label\ntemp = np.log1p(y_train) # creating a temp series to avoid chained indexing\ny_train = temp","0fe56b50":"elasticNet = ElasticNetCV(l1_ratio = [.1, .3, .5, .7, .8, .85, .9, .95, .99, 1], cv=10, n_jobs=-1)\nelasticNet.fit(X_train, y_train)\npredictions = elasticNet.predict(X_test)\npredictionsExp = np.exp(predictions)\n\npredictionsExp.shape","8ca089d7":"submission = pd.DataFrame({\n    \"Id\": test_id,\n    \"SalePrice\": predictionsExp\n})\nsubmission.to_csv('submission_ElasticNetCVFinal.csv', index=False)\n\n# Prepare CSV\nprint(submission.head())","99c4db72":"I choose to use an `ElasticNetCV` model for this experiment. I set the `l1_ratio` to be `[.1, .3, .5, .7, .8, .85, .9, .95, .99, 1]`, with most values closer to 1.0 so that the model is closer to a Lasso model. I also do a 10-fold CV to prevent overfitting. Finally, I transform the predictions exponentially to return its final results.","b75df287":"# Data\n","24c9a877":"### Categorical\n\nAs mentioned above, some categorical features have missing values that mean `None`. I will fill these NAs with such value. Other missing values will be filled with the mode of their respective features. I fill the missing values of the test set with the mode of the training set to prevent information leakage.","9a0964b6":"## Removing Outliers\n\nThere are outliers as identified by the author of the dataset. These outliers are values of the feature `GrLivArea` that are larger than 4000 in the training set. I will remove those outliers.","b4b05a20":"## Correlation Heatmap\n\nAfter filling the missing values, I will graph a heatmap to observe the correlation between every pair of features. After obtaining the pairs of features with high correlation, I would remove the feature with lower correlation to our training output in those pairs. Excluding highly correlated features from our model will prevent overfitting.","e555ad46":"Normalizing all features using `RobustScaler` will hypothetically handle any remaining outliers that are not captured in the process","5aad9f5d":"As mentioned above, `MSSubClass` feature needs to be transformed into categorical type","1db3b49e":"Some numerical features are highly skewed. I will use a Jeo-Johnson `PowerTransformer` to transform these features into a more normal distribution.","0c16dbfa":"# Model","3c951540":"Through this overview, I notice that many features have a large amount of missing values. On careful examination, it comes into light that some of the missing values actually mean `None` and `0` for their respective categorial and numerical features. I would fill these NAs with such values accordingly.\n\nSome of the features are highly skewed, which will require power transforming.\n\nThe `MSSubClass` is numerical, even though it is supposed to be categorical, and will be transformed accordingly.","eac9cd15":"# Abstract\n\nI conduct a machine learning experiment on the Ames Housing dataset. Through an iteration of experiments with different preprocessing methods and models, I achieve the final mean r2 score of 0.8745 and the final RMSLE score of 0.11812, ranking at top 20%.","3aa49348":"# [](http:\/\/)Imports ","d851c9fb":"First, I use the `pandas_profiling` package to have an overview of the data, checking for their types","45cdd815":"From the heatmap, I identify pairs of features with high correlation (>.80): \n`[(GarageArea, GarageCars) (1stFlrSF,TotalBstmSF), (TotRmsAbvGrd, GrLivArea), (GarageYrBuilt, YearBuilt)]`\n\nFrom these pairs, I remove the features with lower correlation to `SalePrice`","e6d327e4":"From the heatmap, I identify pairs of features with high correlation (>.65): \n`[(GarageArea, GarageCars) (1stFlrSF,TotalBstmSF), (TotRmsAbvGrd, GrLivArea), (TotRmsAbvGr, BedroomAbvGrd),\n(GarageYrBuilt, YearBuilt), (2ndFlrSF,GrLivArea), (BsmtFullBath, BsmtFinSF1)]`\n\nFrom these pairs, I remove the features with lower correlation to `SalePrice`","732d419e":"# Exploratory Data Analysis & Preprocessing","2f2a375f":"Log transforming the label is empirically proven to provide a better relationship between the output and the predictors in this dataset","b590a206":"### Numerical\n\nSimilarly, some numerical features have missing values that mean `0`. I will perform the same process as with the cateogrical features.","343c9196":"Transforming all categorical features into one-hot encoding","ed65305a":"## Filling Missing Values","5b516bea":"## Transformation\n"}}