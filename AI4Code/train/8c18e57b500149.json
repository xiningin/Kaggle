{"cell_type":{"5e391937":"code","87191fb2":"code","8f9cc17b":"code","5ebbd1d2":"code","7d623e2a":"code","d3fc8a98":"code","e854135e":"code","15c273c4":"code","f82fb621":"code","4394cde9":"code","a3bce1d4":"code","cad77381":"code","e035c12b":"code","9b60ce27":"code","7da7d574":"code","d80c86aa":"code","eeba0f6f":"code","594934e2":"code","a48c4e82":"code","a2eb18d0":"code","895caced":"code","a359f422":"code","3f42405f":"code","02da6470":"code","14232772":"code","5518baa0":"code","aa2dc320":"code","e4af3ced":"code","5a74e161":"code","18d24ad5":"code","b5564b3c":"code","d0fb5d1d":"code","af8e612a":"code","0bf055d4":"code","2e639ac7":"code","cf07abd7":"code","97eeefcc":"markdown","e03fe208":"markdown","476452e0":"markdown","dd12d2e9":"markdown","e934d6a3":"markdown","43104953":"markdown","826f99bf":"markdown","df2ad0bc":"markdown","9e664f05":"markdown","5774ee38":"markdown","e88c81b2":"markdown","57fb6a11":"markdown","52c06c92":"markdown","19788e6f":"markdown","faccad94":"markdown","ce8c4331":"markdown","46eab558":"markdown","b7fcf611":"markdown","0af76e51":"markdown","c3964b2c":"markdown","e7262244":"markdown","bbceccd1":"markdown","dc50417f":"markdown","0e70041f":"markdown","98169274":"markdown","1222b2e7":"markdown","1bc36c35":"markdown","d5c06e47":"markdown","475a760a":"markdown","8e7e6d63":"markdown","05829ef5":"markdown","77c269d8":"markdown","a667432b":"markdown","2e37335a":"markdown","2619e203":"markdown","6f26b096":"markdown","19d011da":"markdown","69cd51ba":"markdown","6c55cb86":"markdown"},"source":{"5e391937":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sb\nfrom matplotlib import rcParams\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\nfrom sklearn import metrics\nfrom mlxtend.plotting import plot_decision_regions\n\nplt.style.use('default')\nrcParams[\"figure.figsize\"] = [10, 8]","87191fb2":"%matplotlib inline","8f9cc17b":"wine_df = pd.read_csv('..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')\nwine_df.head(10)","5ebbd1d2":"wine_df.describe()","7d623e2a":"print(wine_df.isna().sum())","d3fc8a98":"plt.figure(figsize=[15,15])\nsb.heatmap(wine_df.corr(), annot=True)\nplt.show()","e854135e":"X = wine_df.drop('quality', axis=1)\nX = StandardScaler().fit_transform(X)\ny = np.ravel(wine_df[['quality']])","15c273c4":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=892)","f82fb621":"svm = SVC(random_state=892)\nsvm = svm.fit(X_train, y_train)","4394cde9":"print(\"5-fold cross-validation error rate: {}%\".format(round(100-100*np.mean(cross_val_score(svm, X_train, y_train, cv=5)),2)))\nprint(\"Out-of-sample error rate: {}%\".format(round(100-100*metrics.accuracy_score(y_test, svm.predict(X_test)),2)))","a3bce1d4":"metrics.plot_confusion_matrix(svm, X, y, cmap=plt.cm.Blues)\nplt.show()","cad77381":"lda = LinearDiscriminantAnalysis()\nlda = lda.fit(X_train, y_train)","e035c12b":"print(\"5-fold cross-validation error rate: {}%\".format(round(100-100*np.mean(cross_val_score(lda, X_train, y_train, cv=5)),2)))\nprint(\"Out-of-sample error rate: {}%\".format(round(100-100*metrics.accuracy_score(y_test, lda.predict(X_test)),2)))","9b60ce27":"metrics.plot_confusion_matrix(lda, X_test, y_test, cmap=plt.cm.Blues)\nplt.show()","7da7d574":"logreg = LogisticRegression(solver='newton-cg', random_state=892)\nlogreg = logreg.fit(X_train, y_train)","d80c86aa":"print(\"5-fold cross-validation error rate: {}%\".format(round(100-100*np.mean(cross_val_score(logreg, X_train, y_train, cv=5)),2)))\nprint(\"Out-of-sample error rate: {}%\".format(round(100-100*metrics.accuracy_score(y_test, logreg.predict(X_test)),2)))","eeba0f6f":"metrics.plot_confusion_matrix(logreg, X_test, y_test, cmap=plt.cm.Blues)\nplt.show()","594934e2":"for i in range(0,11):\n    print(\"Number of entries with quality of score {}: {}\".format(i, sum(wine_df['quality'] == i)))","a48c4e82":"wine_df.drop(wine_df.query('quality == 5').sample(frac=0.7, random_state=892).index, inplace=True)\nwine_df.drop(wine_df.query('quality == 6').sample(frac=0.7, random_state=892).index, inplace=True)\nwine_df.reset_index(drop=True, inplace=True)","a2eb18d0":"print('Post-correction:')\nfor i in range(0,11):\n    print(\"Number of entries with quality of score {}: {}\".format(i, sum(wine_df['quality'] == i)))","895caced":"for i in range(len(wine_df)):\n    if wine_df.loc[i, 'quality'] <= 4:\n        wine_df.loc[i, 'quality'] = 0\n    elif 5 <= wine_df.loc[i, 'quality'] <= 6:\n        wine_df.loc[i, 'quality'] = 1\n    elif wine_df.loc[i, 'quality'] >= 7:\n        wine_df.loc[i, 'quality'] = 2","a359f422":"wine_df","3f42405f":"X = wine_df.drop('quality', axis=1)\nX = StandardScaler().fit_transform(X)\ny = np.ravel(wine_df[['quality']])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=892)","02da6470":"svm = SVC(random_state=892)\nsvm = svm.fit(X_train, y_train)","14232772":"print(\"5-fold cross-validation error rate: {}%\".format(round(100-100*np.mean(cross_val_score(svm, X_train, y_train, cv=5)),2)))\nprint(\"Out-of-sample error rate: {}%\".format(round(100-100*metrics.accuracy_score(y_test, svm.predict(X_test)),2)))","5518baa0":"metrics.plot_confusion_matrix(svm, X_test, y_test, cmap=plt.cm.Blues)\nplt.show()","aa2dc320":"lda = LinearDiscriminantAnalysis()\nlda = lda.fit(X_train, y_train)","e4af3ced":"print(\"20-fold cross-validation error rate: {}%\".format(round(100-100*np.mean(cross_val_score(lda, X_train, y_train, cv=5)),2)))\nprint(\"Out-of-sample error rate: {}%\".format(round(100-100*metrics.accuracy_score(y_test, lda.predict(X_test)),2)))","5a74e161":"metrics.plot_confusion_matrix(lda, X_test, y_test, cmap=plt.cm.Blues)\nplt.show()","18d24ad5":"logreg = LogisticRegression(solver='newton-cg', random_state=892)\nlogreg = logreg.fit(X_train, y_train)","b5564b3c":"print(\"20-fold cross-validation error rate: {}%\".format(round(100-100*np.mean(cross_val_score(logreg, X_train, y_train, cv=5)),2)))\nprint(\"Out-of-sample error rate: {}%\".format(round(100-100*metrics.accuracy_score(y_test, logreg.predict(X_test)),2)))","d0fb5d1d":"metrics.plot_confusion_matrix(logreg, X_test, y_test, cmap=plt.cm.Blues)\nplt.show()","af8e612a":"parameters = {'C':[0.001, 0.01, 0.1, 1, 10, 100],\n              'solver':('newton-cg', 'lbfgs', 'liblinear', 'saga', 'sag')}\nlogreg = GridSearchCV(LogisticRegression(max_iter=500, random_state=892), parameters, cv=5)\nlogreg = logreg.fit(X_train, y_train)\nresult = pd.DataFrame(logreg.cv_results_).sort_values('rank_test_score')[['params', 'mean_test_score', 'std_test_score', 'rank_test_score']]\nresult.head(10)","0bf055d4":"logreg_tuned = LogisticRegression(C=1, solver='liblinear', random_state=892)\nlogreg_tuned = logreg_tuned.fit(X_train, y_train)","2e639ac7":"print(\"5-fold cross-validation error rate: {}%\".format(round(100-100*np.mean(cross_val_score(logreg_tuned, X_train, y_train, cv=5)),2)))\nprint(\"Out-of-sample error rate: {}%\".format(round(100-100*metrics.accuracy_score(y_test, logreg_tuned.predict(X_test)),2)))","cf07abd7":"plt.figure(figsize=[10,15])\nfor i in range(0,10):\n    plt.subplot(5,2,i+1)\n    logreg = LogisticRegression(C=1, solver='liblinear', random_state=892)\n    logreg = logreg.fit(X_train[:,[i+1,0]], y_train)\n    plot_decision_regions(X_test[:,[i+1,0]], y_test, clf=logreg, legend=2)\n    plt.xlabel(wine_df.columns[i+1])\n    plt.ylabel(wine_df.columns[0])\n    plt.tight_layout()","97eeefcc":"## Evaluating the tuned Logistic Regression model","e03fe208":"# Comparison of Results","476452e0":"## Logistic Regression","dd12d2e9":"## Logistic Regression","e934d6a3":"### Check for missing values","43104953":"Since there are no missing values, we can proceed with the data as is.","826f99bf":"To visualize the decision regions, the mlxtend library was used (https:\/\/rasbt.github.io\/mlxtend\/). As this dataset contains more than one predictor variable, it is not possible to visualize them in a 2D plot. Instead, we look at only two predictor variables at a time and look at the decision region boundaries.\n\nThe model has a hard time trying to classify poor quality wine (labeled 0), and some pairwise plots do not even have a decision region for that class, because the number of data points for that class is still quite low compared to the other classes. Despite the undersampling and reclassification, the decision regions are still quite biased towards the average quality wines (labeled 1), as the majority of the plot areas are being classified as such.","df2ad0bc":"After making the corrections to the dataset to remove bias, we can see that there is an improvement in performance as the error rates have decreased across the board for all methods. The Logistic Regression method gives the lowest out-of-sample error rate and therefore we will select this model and tune its parameters to improve its performance.","9e664f05":"From the results of each model, it can be observed that all of them perform quite poorly as they have very high error rates.","5774ee38":"# Objective\n\nThe objective of this project is to predict the quality of wine using the concepts learned in DSA5842 Learning from Data: Support Vector Machines. The Wine Quality dataset consists of red wine samples. The inputs include objective tests (e.g. pH values) and the output is based on sensory data (median of at least 3 evaluations made by wine experts). Each expert graded the wine quality between 0 (very bad) and 10 (very excellent).\n\nThe dataset input variables (based on physicochemical tests) are:\n\n1. fixed acidity (tartaric acid - g \/ dm^3)\n2. volatile acidity (acetic acid - g \/ dm^3)\n3. citric acid (g \/ dm^3)\n4. residual sugar (g \/ dm^3)\n5. chlorides (sodium chloride - g \/ dm^3\n6. free sulfur dioxide (mg \/ dm^3)\n7. total sulfur dioxide (mg \/ dm^3)\n8. density (g \/ cm^3)\n9. pH\n10. sulphates (potassium sulphate - g \/ dm^3)\n11. alcohol (% by volume)\n\n\nThe output variable (based on sensory data) is:\n\n12. quality (score between 0 and 10)","e88c81b2":"# Conclusion","57fb6a11":"| Method                       | K-fold CV Error Rate | Out-of-sample Error Rate |\n|------------------------------|----------------------|--------------------------|\n| SVM                          | 39.59%               | 37.71%                   |\n| Linear Discriminant Analysis | 43.16%               | 39.17%                   |\n| Logistic Regression          | 42.00%               | 39.79%                   |","52c06c92":"# Transformation of Data","19788e6f":"# Exploratory Data Analysis","faccad94":"| Method (pre-correction)      | K-fold CV Error Rate | Out-of-sample Error Rate |\n|------------------------------|----------------------|--------------------------|\n| SVM                          | 39.59%               | 37.71%                   |\n| Linear Discriminant Analysis | 43.16%               | 39.17%                   |\n| Logistic Regression          | 42.00%               | 39.79%                   |\n\n| Method (post-correction)     | K-fold CV Error Rate | Out-of-sample Error Rate |\n|------------------------------|----------------------|--------------------------|\n| SVM                          | 23.95%               | 30.54%                   |\n| Linear Discriminant Analysis | 27.98%               | 31.53%                   |\n| Logistic Regression          | 29.46%               | 29.06%                   |","ce8c4331":"# Train\/Test Split","46eab558":"# Approach","b7fcf611":"### Correlation Heatmap","0af76e51":"Before the data can be reclassified, the number of entries with a quality score of 5 and 6 need to be reduced. Those particular entries are being over-represented and the results are biased towards them. As such, undersampling is performed on these entries so that only 30% of the original entries remain, bringing down the number of entries for each score closer to the next highest number of entries in any one category, which is 199.","c3964b2c":"### Summary Statistics","e7262244":"We can see that the distribution of the data is not ideal as there are significantly less entries with a quality score of 3, 4, 7, and 8 as compared to 5 and 6. Additionally, there are no entries in the dataset with a quality score of 0, 1, 2, 9, or 10. This would lead to high misclassification rates as the model does not have enough data points to learn from and make more accurate predictions. Therefore, the data needs to be reclassified.","bbceccd1":"# Re-fitting the Model","dc50417f":"## Linear Discriminant Analysis","0e70041f":"From the results, the parameters that give the best performance i.e. the highest score are $C=1$ and the *liblinear* solver.","98169274":"# Fitting the Model","1222b2e7":"Here we can see that there is a decrease in the K-fold cross-validation error rate from 29.46% to 27.97% and a decrease in the out-of-sample error rate from 29.06% to 28.57%, thereby verifying that tuning the model parameters has resulted in a better performance.","1bc36c35":"## SVM","d5c06e47":"Despite the efforts made to improve the quality of both the dataset and the model, a satsifactory performance could not be achieved. Looking at the pairwise plots of all of the variables, the different classes are clustered and overlap with one other and it is difficult to separate them, linearly or otherwise.\n\nIt is possible that there are other methods more suited to tackling this problem or perhaps it would be more appropriate to frame it as a regression problem instead of a classification problem, but it is outside the scope of this module and will be left for future consideration.","475a760a":"# Loading Wine Quality Dataset","8e7e6d63":"## SVM","05829ef5":"# Plotting the Decision Regions","77c269d8":"The prediction of wine qualities will be treated as a multiclass classification problem as the wine quality can take on any value from 0 to 10, hence there are 11 possible classes. An SVM classifier will be used to solve this problem as well as linear discriminant analysis and logistic regression.","a667432b":"# Tuning the Parameters","2e37335a":"After resolving the over-representation of the two majority categories, the wine quality scores will be reclassified into 3 categories:\n\n- If the wine quality is 4 and below, it is assigned as *Poor* (denoted as 0).\n- If the wine quality is 5 or 6, it is assigned as *Average* (denoted as 1).\n- If the wine quality is 7 and above, it is assigned as *Good* (denoted as 2).","2619e203":"# Preliminary Analysis of Results","6f26b096":"A heatmap of the pairwise correlation coefficients are plotted to observe the collinearity of the variables. As none of the correlation coefficients are close to +1 or -1, we do not need to remove any of the variables and can proceed with the data.","19d011da":"The input variables are normalized first before performing the train\/test split, and a train\/test split of 70\/30 is used.","69cd51ba":"## Logistic Regression\n\nFor logistic regression, the parameters that can be tuned are:\n- C\n- solver\n\nThe solver refers to the method used to solve the optimization problem, and each solver uses a different algorithm. Namely, they are Newton's Method (*newton-cg*), Limited-memory Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno Algorithm (*lbfgs*), and Library for Large Linear Classification (*liblinear*).\n\nA grid search is performed and it iterates through the possible combinations of the parameters together with K-fold cross validation to determine the best combination of parameters.","6c55cb86":"## Linear Discriminant Analysis"}}