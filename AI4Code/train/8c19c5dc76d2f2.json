{"cell_type":{"8ee69a18":"code","0e3d94d2":"code","8c31b300":"code","37c69abe":"code","18b27e0a":"code","db647b5c":"code","18a71437":"code","9e4b0c04":"code","446434ff":"code","572822ee":"code","a365defc":"code","5a7fd6f0":"code","0c8af26b":"code","92831aa0":"code","030211e6":"code","af45bafb":"code","eb74b387":"code","5794b1f7":"code","677c7787":"code","c09a9aef":"code","3d9f4c9d":"code","8fc74fb9":"code","3b8666db":"code","726d6794":"code","818cd491":"markdown","8b52902d":"markdown","7b0dd158":"markdown","035974c6":"markdown","12f9115c":"markdown","0827f99c":"markdown","acf43e51":"markdown","8abe6e96":"markdown","836fadbf":"markdown","0351e017":"markdown","56376021":"markdown","0cea1d54":"markdown","cb5c6aad":"markdown","a75d4105":"markdown","5a1c63bd":"markdown","60ffcf07":"markdown","d84b68aa":"markdown","96a70dcf":"markdown","8b5d95ba":"markdown","4b01ed5a":"markdown","4ebe9504":"markdown","851471bf":"markdown","c6272521":"markdown"},"source":{"8ee69a18":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport math","0e3d94d2":"dataset = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')\ndataset = dataset.fillna(0)","8c31b300":"dataset.head()","37c69abe":"dataset.info()","18b27e0a":"X = dataset.iloc[:,2:-1].values\ny = dataset.iloc[:,1:2].values\n","db647b5c":"from sklearn.preprocessing import LabelEncoder;\nle =LabelEncoder();\ny = le.fit_transform(y);\ny=y.reshape(569,1)","18a71437":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","9e4b0c04":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","446434ff":"def initialize_params(layer_dims):\n    \n    \"\"\"\n    Arguments:\n    layer_dims -- python array (list) containing the size of each layer.\n    \n    Returns:\n    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])\n                    b1 -- bias vector of shape (layers_dims[1], 1)\n                    ...\n                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])\n                    bL -- bias vector of shape (layers_dims[L], 1)\n    \"\"\"\n    params = {}\n    np.random.seed(42)\n    L = len(layer_dims)-1\n    for l in range (L):\n        params[\"W\"+str(l+1)] = np.random.randn(layer_dims[l+1],layer_dims[l])*(2\/layer_dims[l])**0.5\n        params[\"b\"+str(l+1)] = np.zeros((layer_dims[l+1],1))\n    \n    return params","572822ee":"def Linear_forward(A,W,b):\n    Z = np.dot(W,A)+b\n    cache = (A,W,b)\n    return Z,cache","a365defc":"def Activation_forward(A,W,b,Activation):\n    if Activation == \"relu\":\n        Z,Linear_cache = Linear_forward(A,W,b)\n        A  = np.maximum(0,Z)\n        A, activation_cache = A,Z\n    elif Activation == 'sigmoid':\n        Z,Linear_cache = Linear_forward(A,W,b)\n        A,activation_cache = (1\/(1+np.exp(-Z)),Z)\n    cache= (Linear_cache,activation_cache)\n    return A,cache","5a7fd6f0":"def forward_prop(X,params):\n    A=X\n    caches = []\n    L = len(params)\/\/2\n    for l in range (L-1):\n        A_prev = A\n        A, cache = Activation_forward(A_prev,params[\"W\"+str(l+1)],params[\"b\"+str(l+1)],\"relu\")\n        caches.append(cache)\n    AL,cache = Activation_forward(A,params[\"W\"+str(L)],params[\"b\"+str(L)],\"sigmoid\")\n    caches.append(cache)\n    return AL,caches","0c8af26b":"def cost(AL,Y) :\n    m = Y.shape[1]\n    cost = -np.sum(Y*np.log(AL)+(1-Y)*np.log(1-AL))\/m\n    return np.squeeze(cost)","92831aa0":"def linear_backward(dZ,cache):\n    A_prev,W,b = cache\n    m =A_prev.shape[1]\n    dW = (np.dot(dZ,A_prev.T)\/m) \n    db = np.sum(dZ,axis=1,keepdims=True)\/m\n    dA_prev = np.dot(W.T,dZ)\n    return dA_prev , dW , db","030211e6":"def activation_backward(dA,cache,activation):\n    linear_cache,activation_cache = cache\n    Z=activation_cache\n    if activation == \"relu\":\n        dZ = (Z>0).astype(int)\n        dZ = dA*dZ\n        dA_prev, dW, db = linear_backward(dZ,linear_cache)\n    elif activation == \"sigmoid\":\n        dZ = np.multiply(dA,(1\/(1+np.exp(-Z)))*(1-(1\/(1+np.exp(-Z)))))\n        dA_prev, dW, db = linear_backward(dZ,linear_cache)\n    \n    return dA_prev, dW, db","af45bafb":"def backward_prop(AL,Y,caches):\n    \n    L = len(caches)\n    m = AL.shape[1]\n    Y = Y.reshape(AL.shape)\n    grads = {}\n    \n    grads[\"dA\"+str(L)] = -(np.divide(Y,AL)-np.divide(1-Y,1-AL))\n    \n    current_cache = caches[L-1]\n    \n    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] =  activation_backward(grads[\"dA\"+str(L)], current_cache, 'sigmoid')\n    for l in reversed (range(L-1)):\n        current_cache = caches[l]\n        dA_prev_temp, dW_temp, db_temp =  activation_backward(grads['dA'+str(l+1)], current_cache, 'relu')\n        grads[\"dA\" + str(l)] = dA_prev_temp\n        grads[\"dW\" + str(l + 1)] = dW_temp\n        grads[\"db\" + str(l + 1)] = db_temp\n        \n    return grads\n        ","eb74b387":"def random_mini_batches(X, Y, mini_batch_size ):\n    \"\"\"\n    Creates a list of random minibatches from (X, Y)\n    \n    Arguments:\n    X -- input data, of shape (input size, number of examples)\n    Y -- true \"label\" vector (1 for blue dot \/ 0 for red dot), of shape (1, number of examples)\n    mini_batch_size -- size of the mini-batches, integer\n    \n    Returns:\n    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n    \"\"\"\n    \n\n    m = X.shape[1]                  # number of training examples\n    mini_batches = []\n    np.random.seed(42)\n    # Step 1: Shuffle (X, Y)\n    permutation = list(np.random.permutation(m))\n    shuffled_X = X[:, permutation]\n    shuffled_Y = Y[:, permutation].reshape((1,m))\n\n    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n    num_complete_minibatches = math.floor(m\/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n    for k in range(0, num_complete_minibatches):\n        \n        mini_batch_X = shuffled_X[:,k*mini_batch_size:(k+1)*mini_batch_size]\n        mini_batch_Y = shuffled_Y[:,k*mini_batch_size:(k+1)*mini_batch_size]\n     \n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    # Handling the end case (last mini-batch < mini_batch_size)\n    if m % mini_batch_size != 0:\n        \n        mini_batch_X = shuffled_X[:,num_complete_minibatches*mini_batch_size:m]\n        mini_batch_Y = shuffled_Y[:,num_complete_minibatches*mini_batch_size:m]\n        \n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    return mini_batches","5794b1f7":"def initialize_adam(parameters) :\n    L = len(parameters) \/\/ 2 # number of layers in the neural networks\n    v = {}\n    s = {}\n    \n    # Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".\n    for l in range(L):\n    \n        v[\"dW\" + str(l+1)] = np.zeros(parameters[\"W\"+str(l+1)].shape)\n        v[\"db\" + str(l+1)] = np.zeros(parameters[\"b\"+str(l+1)].shape)\n        s[\"dW\" + str(l+1)] = np.zeros(parameters[\"W\"+str(l+1)].shape)\n        s[\"db\" + str(l+1)] = np.zeros(parameters[\"b\"+str(l+1)].shape)\n        \n    \n    return v, s","677c7787":"def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01, beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n    L = len(parameters) \/\/ 2                 # number of layers in the neural networks\n    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n    s_corrected = {}                         # Initializing second moment estimate, python dictionary\n    \n    # Perform Adam update on all parameters\n    for l in range(L):\n        # Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\n       \n        v[\"dW\" + str(l+1)] = beta1*v[\"dW\"+str(l+1)]+(1-beta1)*grads[\"dW\"+str(l+1)]\n        v[\"db\" + str(l+1)] = beta1*v[\"db\"+str(l+1)]+(1-beta1)*grads[\"db\"+str(l+1)]\n       \n\n        # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\".\n        \n        v_corrected[\"dW\" + str(l+1)] = v[\"dW\"+str(l+1)]\/(1-beta1**t)\n        v_corrected[\"db\" + str(l+1)] = v[\"db\"+str(l+1)]\/(1-beta1**t)\n        \n        # Moving average of the squared gradients. Inputs: \"s, grads, beta2\". Output: \"s\".\n        s[\"dW\" + str(l+1)] = beta2*s[\"dW\"+str(l+1)]+(1-beta2)*grads[\"dW\"+str(l+1)]**2\n        s[\"db\" + str(l+1)] = beta2*s[\"db\"+str(l+1)]+(1-beta2)*grads[\"db\"+str(l+1)]**2\n\n\n        # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\".\n     \n        s_corrected[\"dW\" + str(l+1)] = s[\"dW\"+str(l+1)]\/(1-beta2**t)\n        s_corrected[\"db\" + str(l+1)] = s[\"db\"+str(l+1)]\/(1-beta2**t)\n        \n\n        # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n\n        parameters[\"W\" + str(l+1)] = parameters[\"W\"+str(l+1)] - learning_rate*(v_corrected[\"dW\"+str(l+1)]\/((s_corrected[\"dW\"+str(l+1)]**0.5)+epsilon))\n        parameters[\"b\" + str(l+1)] = parameters[\"b\"+str(l+1)] - learning_rate*(v_corrected[\"db\"+str(l+1)]\/((s_corrected[\"db\"+str(l+1)]**0.5)+epsilon))\n   \n\n    return parameters, v, s","c09a9aef":"def model(X, Y, layers_dims, optimizer, learning_rate = 0.0007, mini_batch_size = 16, beta = 0.9,beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, num_epochs = 50, print_cost = True):\n    L = len(layers_dims)             # number of layers in the neural networks\n    costs = []                       # to keep track of the cost\n    t = 0  \n    m =X.shape[1]\n    \n    params = initialize_params(layers_dims)\n    \n    v, s = initialize_adam(params)\n    \n    for i in range(num_epochs):\n        minibatches = random_mini_batches(X, Y, mini_batch_size)\n        cost_total = 0\n        for minibatch in minibatches:\n            \n            (minibatch_X,minibatch_Y) = minibatch\n            AL, caches = forward_prop(minibatch_X, params)\n            \n            cost_total += cost(AL, minibatch_Y)\n            \n            grads = backward_prop(AL, minibatch_Y, caches)\n            t=t+1\n            params, v, s = update_parameters_with_adam(params, grads, v, s, t, learning_rate, beta1, beta2,  epsilon)\n            \n        cost_avg = cost_total \/ m\n            \n        if print_cost and i %10 == 0:\n            print (\"Cost after epoch %i: %f\" %(i, cost_avg))\n        costs.append(cost_avg)\n                \n    plt.plot(costs)\n    plt.ylabel('cost')\n    plt.xlabel('epochs')\n    plt.title(\"Learning rate = \" + str(learning_rate))\n    plt.show()\n\n    return params","3d9f4c9d":"def predict(X,y,parameters):\n    \n    pre, cache = forward_prop(X,parameters)\n    predictions = (pre>0.5).astype(int)\n    from sklearn.metrics import accuracy_score\n    \n    print(accuracy_score(predictions[0],y[0]))\n    return predictions\n\n    ","8fc74fb9":"layers_dims = [X.shape[1],6,2,1]\nlrates = [1e-4,2e-4,3e-4,5e-4,6e-4,7e-4,8e-4,9e-4,1e-3]\nfor lr in lrates:\n    params = model(X_train.T, y_train.T ,layers_dims, optimizer = \"adam\",learning_rate=lr)\n    print(\"Train Accuracy:\")\n    predictions_train = predict(X_train.T, y_train.T, params)\n    print(\"Test Accuracy:\")  \n    predictions_test = predict(X_test.T, y_test.T, params)","3b8666db":"parameters = model(X_train.T, y_train.T ,layers_dims=[X.shape[1],6,2,1], optimizer = \"adam\",learning_rate=0.00095)\nprint(\"Accuracy\")\npredictions = predict(X_test.T, y_test.T, parameters)","726d6794":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test.reshape(1,y_test.shape[0])[0],predictions[0])","818cd491":"# 1. Intializing Parameters","8b52902d":"## Importing Libs","7b0dd158":"### no null value found \ud83d\udc40","035974c6":"# 5. Gradient descent with ADAM optimization","12f9115c":"# Gentle Reminder for an Upvote \ud83d\ude0f!!","0827f99c":"## Learning rates 0.0009 - 0.001 works fine!!","acf43e51":"# Don't forget to upvote!! \ud83d\ude09","8abe6e96":"### A Sample notebook which gives a method to construct a neural network from scratch, that means we are not using any of the predefined libraries like tensorflow, keras etc.\n\nFor this purpose I'm using real life Dataset.\n\nHope you like the work! \ud83d\ude0a","836fadbf":"# 9. Making Predictions","0351e017":"We are using a model with last layer having activation sigmoid and all other layers with ReLu as activation function","56376021":"# 2. Forward Propagation","0cea1d54":"# 7. Predictions","cb5c6aad":"# 3. Backward Propagation","a75d4105":"# 6. Model code (Combining Every thing) ","5a1c63bd":"# 8. Training our model","60ffcf07":"## Train test split","d84b68aa":"## Feature Scaling","96a70dcf":"# Let's get started with our network","8b5d95ba":"# Breast Cancer predictions using custom ANN ","4b01ed5a":"### Scaled data will help in training of neural network, i generally speeds up the gradient descent!!","4ebe9504":"# 4. Making Mini Batches","851471bf":"## It's just a demonstration for building a neural network, so we'll not look for EDA in depth.\n","c6272521":"## He Initialization\n"}}