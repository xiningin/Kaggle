{"cell_type":{"88b86a62":"code","62d628ae":"code","b8a88ecf":"code","cc27a2ef":"code","f64bd4ff":"code","25fac94e":"code","e558e811":"code","4fb1542f":"code","37ca9e31":"code","7c9e2594":"code","9c15c803":"code","05599634":"code","7bcb621c":"code","7ea6459e":"code","08eeacdf":"code","ba480504":"code","8e29a850":"code","c7b9604a":"code","e3e0bbf3":"code","71e249aa":"code","2da6b136":"code","3738fa7a":"code","1d690972":"code","2fff1f44":"code","03b58b0d":"code","489a9965":"code","79ee1d61":"code","437def26":"code","b40dd7e4":"code","c8615641":"code","243900ad":"code","ecea0557":"code","53b67dcf":"code","d534af93":"code","d65303c4":"code","a461da5f":"code","a3a9a443":"markdown","4a093eaf":"markdown","37211673":"markdown","90b0d169":"markdown"},"source":{"88b86a62":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","62d628ae":"data = pd.read_csv('\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv')","b8a88ecf":"data.head()","cc27a2ef":"import matplotlib.pyplot as plt\n%matplotlib inline","f64bd4ff":"#Checking whether there is any null value\ndata.isnull().values.any()","25fac94e":"#displaying the heatmap\nimport seaborn as sns\ncorrmat = data.corr()\ntop_corr_factors = corrmat.index\nplt.figure(figsize = (10,8))\nsns.heatmap(corrmat, annot = True, cmap = \"YlGnBu\")\n","e558e811":"#Checking whether the data is balanced or not\n\nis_diabetes = len(data.loc[data['Outcome']==1])\nno_diabetes = len(data.loc[data['Outcome']==0])\nprint('total people with diabetes are : {}'.format(is_diabetes))\nprint(\"total people who don't have diabetes are : {}\".format(no_diabetes))","4fb1542f":"index = (corrmat.index)\nprint(index)","37ca9e31":"#Printing the total number of 0 entries in each column\nfor value in index:\n    print(f\"Total no of missing data in column {value} : {len(data.loc[data[value]==0])}\")","7c9e2594":"#Splitting X and y\nX = data.iloc[:,:8]\ny = data['Outcome']\nX.head()","9c15c803":"#Splitting training and testing data\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)","05599634":"#Replacing all the 0 entries by the mean of the respectiv column\nfrom sklearn.impute import SimpleImputer\n\nimp = SimpleImputer(missing_values = 0, strategy = \"mean\")\nX_train[:] = imp.fit_transform(X_train)\nX_test[:] = imp.fit_transform(X_test)","7bcb621c":"'''from sklearn.impute import SimpleImputer\n\nimp = SimpleImputer(missing_values = 0, strategy = 'mean')\nX_train[1:,:] = imp.fit_transform(X_train)\nX_test.iloc[1:,:] = imp.fit_transform(X_test)\nX_newtest = pd.DataFrame(X_test)\nX_newtrain = pd.DataFrame(X_train)\nX_newtest.head()'''","7ea6459e":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(random_state = 42)","08eeacdf":"params_rf = {\n    \"n_estimators\" : [50,100,150,200]\n}","ba480504":"#Trying the RandomizedSearchCV approach for getting the best best of hyperparameters \nfrom sklearn.model_selection import RandomizedSearchCV\n\nrandom_search_rf = RandomizedSearchCV(rf, param_distributions = params_rf)\nrandom_search_rf.fit(X_train,y_train)\nrandom_search_rf.best_estimator_","8e29a850":"rf = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n                       criterion='gini', max_depth=None, max_features='auto',\n                       max_leaf_nodes=None, max_samples=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, n_estimators=200,\n                       n_jobs=None, oob_score=False, random_state=42, verbose=0,\n                       warm_start=False)\nrf.fit(X_train,y_train)","c7b9604a":"y_pred = rf.predict(X_test)","e3e0bbf3":"from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\nacc = accuracy_score(y_pred, y_test)\ncm = confusion_matrix(y_pred, y_test)\nf1 = f1_score(y_pred, y_test)","71e249aa":"print(f\"accuracy is : {acc}\")\nprint(f\"confusion matrix is : {cm}\")\nprint(f\"f1 score is : {f1}\")","2da6b136":"import xgboost\nclassifier = xgboost.XGBClassifier()","3738fa7a":"params = {\n\"learning_rate\" : [0.05,0.10,0.15,0.20,0.25,0.30],\n\"max_depth\" : [3,4,5,6,8,10,12,15],\n\"min_child_weight\" : [1,3,5,7],\n\"gamma\" : [0.0,0.1,0.2,0.3,0.4],\n\"colsample_bytree\" : [0.3,0.4,0.5,0.7]\n}","1d690972":"#Trying the RandomizedSearchCV approach for getting the best best of hyperparameters \nfrom sklearn.model_selection import RandomizedSearchCV\n\nrandom_search_xg = RandomizedSearchCV(classifier, param_distributions = params)\nrandom_search_xg.fit(X_train,y_train)\nrandom_search_xg.best_estimator_","2fff1f44":"classifier = xgboost.XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=0.7, gamma=0.3, gpu_id=-1,\n              importance_type='gain', interaction_constraints=None,\n              learning_rate=0.05, max_delta_step=0, max_depth=5,\n              min_child_weight=7, monotone_constraints=None,\n              n_estimators=100, n_jobs=0, num_parallel_tree=1,\n              objective='binary:logistic', random_state=0, reg_alpha=0,\n              reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n              validate_parameters=False, verbosity=None)","03b58b0d":"classifier.fit(X_train,y_train)","489a9965":"y_pred_xg = classifier.predict(X_test)","79ee1d61":"acc_xg = accuracy_score(y_pred_xg,y_test)\ncm_xg = confusion_matrix(y_pred_xg,y_test)\nf1_xg = f1_score(y_pred_xg,y_test)\nprint(f\"accuracy by xgboost is : {acc_xg}\")\nprint(f\"f1 score by xgboost is : {f1_xg}\")","437def26":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors = 15,n_jobs = -1)","b40dd7e4":"knn_params = {\n    \"n_neighbors\" : [5,7,10,12,15,17,20],\n    \"leaf_size\" : [10,20,30,40,50,60,70]\n}","c8615641":"knn_search = RandomizedSearchCV(knn, knn_params)\nknn_search.fit(X_train,y_train)\nknn_search.best_estimator_","243900ad":"knn = KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n                     metric_params=None, n_jobs=-1, n_neighbors=15, p=2,\n                     weights='uniform')\nknn.fit(X_train,y_train)\nknn_pred = knn.predict(X_test)\nacc_knn = accuracy_score(knn_pred, y_test)\nf1_knn = f1_score(knn_pred, y_test)\nprint(\"The model accuracy is : {}\".format(acc_knn))\nprint(\"The f1 score of the model is : {}\".format(f1_knn))","ecea0557":"import warnings\nwarnings.filterwarnings('ignore')\n#Importing PermutationImportance\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n","53b67dcf":"head = [*data]\nX_test = pd.DataFrame(X_test)\nX_test.colums = head\nX_train = pd.DataFrame(X_train)\nX_train.colums = head\nX_train.head()","d534af93":"#Permutation importance for the RF model\nperm_rf = PermutationImportance(rf, random_state = 42).fit(X_train,y_train)\neli5.show_weights(perm_rf, feature_names = X_test.columns.tolist())","d65303c4":"#Permutation importance for the XGBoost model\nperm_xg = PermutationImportance(classifier, random_state = 42).fit(X_train,y_train)\neli5.show_weights(perm_xg, feature_names = X_test.columns.tolist())","a461da5f":"#Permutation importance for the KNN model\nperm_knn = PermutationImportance(knn, random_state = 42).fit(X_train,y_train)\neli5.show_weights(perm_knn, feature_names = X_test.columns.tolist())","a3a9a443":"Using XGboost for the classification","4a093eaf":"All the three classifiers report Glucose to be the most important feature, which actually is quite intuitive.","37211673":"Permutation Importance","90b0d169":"Using Random Forest for the classification"}}