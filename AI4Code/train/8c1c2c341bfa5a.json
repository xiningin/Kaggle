{"cell_type":{"ac49087f":"code","e5fd049a":"code","2e690e68":"code","233411bf":"code","81ce699e":"code","9558f181":"code","ead7722e":"code","6c406872":"code","63a07f0c":"code","f938c223":"code","4dc6e4e5":"code","8fbadff0":"code","1521c0c3":"code","d17ee633":"code","672106fe":"code","2faf30e4":"code","d18816f0":"code","32e4d9a3":"code","7836f51f":"code","cbb9498a":"code","1fae3e14":"code","af87458a":"code","d967f88d":"code","544de984":"code","772fc86e":"code","536e1428":"code","2f4b1cda":"code","fd75f210":"code","53150f51":"code","eb019708":"code","bf845831":"code","5202e939":"code","07cd0444":"code","f67af2b2":"code","4228b115":"code","2bf17ede":"code","1d68f3a3":"code","13a7ca3e":"code","e5f332d7":"code","b0a4541f":"code","3f7ada16":"code","060f817d":"code","b4c50b1b":"code","37f039b5":"code","f7ce0cfd":"code","43bdf2d0":"code","c9701bec":"code","0405f33e":"code","83893459":"code","c966de36":"code","d0fbec18":"code","f309795f":"code","ef241e1a":"code","e2881ca6":"code","56caf95e":"code","6d736779":"code","f0d1353a":"code","edc8edda":"code","1be614d6":"code","dd8874f4":"code","92fb82e3":"code","a1c8b597":"code","fda6318d":"code","fb7e596d":"code","7520bdc9":"code","ae12c21e":"code","eee99d0f":"code","8df1568b":"code","322c1ff7":"code","ac3d3785":"code","5c662876":"code","52e68e4f":"code","bea51f1e":"code","4588e94a":"code","f8f7f4b3":"code","f5fc343c":"code","5742ed76":"markdown","c2ed67b0":"markdown","28c70cf3":"markdown","9dbf4290":"markdown","18842c7e":"markdown","e0074f9b":"markdown","0f4a3f48":"markdown","6fc657a6":"markdown","fbf44b9f":"markdown","d2d12dbd":"markdown","5705f9f5":"markdown","1abca993":"markdown","9367dc18":"markdown","dc9f9c77":"markdown","a149a76c":"markdown","49f9bcc1":"markdown","38002ec3":"markdown","ba97c92f":"markdown","7d742a5c":"markdown","f0c84ac4":"markdown","52d88717":"markdown","4ae53e0a":"markdown","dc6a9f6f":"markdown","873a38f8":"markdown","97201ac4":"markdown","cffef722":"markdown","0be44d8f":"markdown","4fc532c8":"markdown","fab97289":"markdown","ee237cb1":"markdown","894b7b65":"markdown","6f9c7de0":"markdown","24e04d75":"markdown","65482fcf":"markdown","a0c62927":"markdown","22495c3b":"markdown","5fde35cc":"markdown","78b4d779":"markdown","1a96ca71":"markdown","81dbab4c":"markdown","2ec584e6":"markdown","632c01f4":"markdown","f4793461":"markdown","1cc7cf9b":"markdown","24bb8aeb":"markdown","30603178":"markdown","2835d0be":"markdown","54d96145":"markdown","9b21fa84":"markdown","49e19ce2":"markdown","90261e0e":"markdown","784697fa":"markdown","2a169554":"markdown","7ce3acf8":"markdown","b52b7315":"markdown","6bd9d7db":"markdown","0359ec7d":"markdown","6f8d078e":"markdown","0e62b781":"markdown","27af7d95":"markdown","c66e573c":"markdown","500e15c9":"markdown","c11b8f65":"markdown","f0ed4f1c":"markdown","ad072a13":"markdown","7d31db0b":"markdown","31ca3e53":"markdown","dd582dd0":"markdown","6505a2eb":"markdown","0f330d3e":"markdown","0b86e1e6":"markdown","4e54cc80":"markdown","fb0f5cda":"markdown","bc6dda38":"markdown","eefe4ca5":"markdown","e4c83c3b":"markdown","84dec22c":"markdown","5f04f229":"markdown","cc8ac1c6":"markdown","49af537c":"markdown","b6abf2b3":"markdown"},"source":{"ac49087f":"%matplotlib inline\n\nimport sys\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score, StratifiedKFold, learning_curve, ShuffleSplit\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.gaussian_process import GaussianProcessClassifier\n\nfrom sklearn.pipeline import Pipeline\n\nfrom xgboost import XGBClassifier\n\nsns.set_style('whitegrid')","e5fd049a":"# load data\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\n\n# preview\ntrain.head()","2e690e68":"# join train and test datasets in order to obtain the same number of features\ntrain_len = len(train)\nfull_dataset =  pd.concat(objs=[train, test], axis=0).reset_index(drop=True)","233411bf":"# fill empty and Nan's values with NaN\ndataset = full_dataset.fillna(np.nan)\n\n# null values\ndataset.isnull().sum().sort_values(ascending=False)","81ce699e":"train.isnull().sum().sort_values(ascending=False)","9558f181":"train.info()\ntrain.drop(['PassengerId'], axis=1).describe()","ead7722e":"dataset['Embarked'].isnull().sum()","6c406872":"dataset['Embarked'] = dataset['Embarked'].fillna(dataset['Embarked'].mode())","63a07f0c":"dataset['Fare'].isnull().sum()","f938c223":"dataset['Fare'] = dataset['Fare'].fillna(dataset['Fare'].median())","4dc6e4e5":"#  Age vs Sex, Parch, Pclass and SibSP (whole dataset)\np = sns.catplot(y='Age', x='Sex', data=dataset, kind='box')\np = sns.catplot(y='Age', x='Sex', hue='Pclass', data=dataset, kind='box')\np = sns.catplot(y='Age', x='Parch', data=dataset, kind='box')\np = sns.catplot(y='Age', x='SibSp', data=dataset, kind='box')","8fbadff0":"# fill Age with the median age of similar rows according to Pclass, Parch and SibSp\ndataset['Age'] = dataset['Age'].fillna(dataset.groupby(['SibSp', 'Parch', 'Pclass'])['Age'].transform('median'))\ndataset['Age'] = dataset['Age'].fillna(dataset['Age'].median())\n\ntrain['Age'] = dataset.loc[:train.shape[0]-1, 'Age']","1521c0c3":"# correlation matrix\np = sns.heatmap(train[['Survived','SibSp','Parch','Age','Fare']].corr(), annot=True, fmt = '.2f', cmap = 'coolwarm')","d17ee633":"# SibSp vs Survived\np = sns.catplot(x='SibSp', y='Survived', data=train, kind='bar', height = 6, palette='Spectral_r')\np.despine(left=True)\np = p.set_ylabels('Survival probability')","672106fe":"# Parch vs Survived\np = sns.catplot(x='Parch', y='Survived', data=train, kind='bar', height = 6, palette='Spectral_r')\np.despine(left=True)\np = p.set_ylabels('Survival probability')","2faf30e4":"# Age distribution\np = sns.distplot(dataset['Age'], color='LightSeaGreen', label='Skewness : %.2f'%(dataset['Age'].skew()))\np = p.legend(loc='best')\n\n# Age vs Survived\np = sns.FacetGrid(train, col='Survived', height = 6)\np = p.map(sns.distplot, 'Age')","d18816f0":"# comparison on single plot\np = sns.kdeplot(train['Age'][(train['Survived'] == 0) & (train['Age'].notnull())], color='lightcoral', shade=True)\np = sns.kdeplot(train['Age'][(train['Survived'] == 1) & (train['Age'].notnull())], ax=p, color='darkturquoise', shade=True)\np.set_xlabel('Age')\np.set_ylabel('Frequency')\np = p.legend(['Not survived','Survived'])","32e4d9a3":"plt.figure(figsize=(25,6))\n\navg_survival_by_age = train[['Age', 'Survived']].apply(np.ceil).groupby(['Age'], as_index=False).mean()\navg_survival_by_age[['Age']] = avg_survival_by_age[['Age']].astype(int)\np = sns.barplot(x='Age', y='Survived', data=avg_survival_by_age, color='LightSeaGreen')\np = p.set_ylabel('Survival probability')","7836f51f":"plt.figure(figsize=(25,6))\n\navg_survival_by_age = train[['Age', 'Survived', 'Sex']].apply(lambda x: np.ceil(x) if x.name == 'Age' else x).groupby(['Age', 'Sex'], as_index=False).mean()\navg_survival_by_age[['Age']] = avg_survival_by_age[['Age']].astype(int)\np = sns.barplot(x='Age', y='Survived', hue='Sex', data=avg_survival_by_age, palette='Spectral_r')\np = p.set_ylabel('Survival probability')","cbb9498a":"# Fare distribution over whole data\np = sns.distplot(dataset['Fare'], color='LightSeaGreen', label='Skewness : %.2f'%(dataset['Fare'].skew()))\np = p.legend(loc='best')","1fae3e14":"# log function on Fare to reduce skewness\ndataset['Fare'] = dataset['Fare'].map(lambda i: np.log(i) if i > 0 else 0)\ntrain['Fare'] = train['Fare'].map(lambda i: np.log(i) if i > 0 else 0)","af87458a":"p = sns.distplot(dataset['Fare'], color='LightSeaGreen', label='Skewness : %.2f'%(dataset['Fare'].skew()))\np = p.legend(loc='best')","d967f88d":"# comparison on single plot\np = sns.kdeplot(train['Fare'][(train['Survived'] == 0) & (train['Age'].notnull())], color='lightcoral', shade=True)\np = sns.kdeplot(train['Fare'][(train['Survived'] == 1) & (train['Age'].notnull())], ax=p, color='darkturquoise', shade=True)\np.set_xlabel('Fare')\np.set_ylabel('Frequency')\np = p.legend(['Not Survived','Survived'])\n\n# Age vs Survived\np = sns.FacetGrid(train, col='Survived', height = 6)\np = p.map(sns.distplot, 'Fare')","544de984":"p = sns.barplot(x='Sex' ,y='Survived', data=train)\np = p.set_ylabel('Survival probability')","772fc86e":"train[['Sex','Survived']].groupby('Sex').mean()","536e1428":"# Pclass vs Survived\np = sns.catplot(x='Pclass', y='Survived', data=train, kind='bar', height=6 , palette='Spectral_r')\np.despine(left=True)\np = p.set_ylabels('Survival probability')","2f4b1cda":"# Pclass vs Survived vs Sex\np = sns.catplot(x='Pclass', y='Survived', hue='Sex', data=train, height=6, kind='bar', palette='Spectral_r')\np.despine(left=True)\np = p.set_ylabels('Survival probability')","fd75f210":"# Embarked vs Survived\np = sns.catplot(x='Embarked', y='Survived', data=train, kind='bar', height=6 , palette='Spectral_r')\np.despine(left=True)\np = p.set_ylabels('Survival probability')","53150f51":"# Embarked vs Survived vs Sex\np = sns.catplot(x='Embarked', y='Survived', hue='Sex', data=train, height=6, kind='bar', palette='Spectral_r')\np.despine(left=True)\np = p.set_ylabels('Survival probability')","eb019708":"# Pclass vs Embarked \np = sns.catplot('Pclass', col='Embarked', data=dataset, height=6, kind='count', palette='Spectral_r')\np.despine(left=True)\np = p.set_ylabels('Count')","bf845831":"# family size descriptor from SibSp and Parch\ndataset['FamS'] = dataset['SibSp'] + dataset['Parch'] + 1\ntrain['FamS'] = train['SibSp'] + train['Parch'] + 1\n\np = sns.catplot(x='FamS', y='Survived', data = train, kind = 'point')\np = p.set_ylabels('Survival probability')","5202e939":"# Create new feature of family size\ndef family_size(peers):\n    size = 'Large'\n    if peers == 1:\n        size = 'Single'\n    elif peers <= 3:\n        size = 'Small'\n    elif peers == 4:\n        size = 'Medium'\n    return size\n\ndataset['FamSCat'] = dataset['FamS'].map(family_size)\ntrain['FamSCat'] = train['FamS'].map(family_size)","07cd0444":"# FamSCat vs Survived\np = sns.catplot(x='FamSCat', y='Survived', data=train, height=6, kind='bar', palette='Spectral_r', order=['Single', 'Small', 'Medium', 'Large'])\np.despine(left=True)\np = p.set_ylabels('Survival probability')\n\n# FamSCat vs Survived vs Sex\np = sns.catplot(x='FamSCat', y='Survived', hue='Sex', data=train, height=6, kind='bar', palette='Spectral_r', order=['Single', 'Small', 'Medium', 'Large'])\np.despine(left=True)\np = p.set_ylabels('Survival probability')","f67af2b2":"dataset['Name'].head()","4228b115":"# Title from Name\ndataset['Title'] = [i.split(',')[1].split('.')[0].strip() for i in dataset['Name']]\ndataset['Title'].head()","2bf17ede":"p = sns.countplot(x='Title', data=dataset, palette='Spectral_r')\np = plt.setp(p.get_xticklabels(), rotation=60) ","1d68f3a3":"# convert to categorical values \ndataset['Title'] = dataset['Title'].replace(['Lady', 'the Countess', 'Countess', 'Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ndataset['Title'] = dataset['Title'].replace(['Miss', 'Ms', 'Mme', 'Mlle', 'Mrs'], 'Ladys_title')\ndataset['Title'].head()","13a7ca3e":"p = sns.countplot(dataset['Title'])","e5f332d7":"p = sns.catplot(x='Title', y='Survived', data=dataset, kind='bar')\np = p.set_ylabels('Survival probability')","b0a4541f":"dataset.columns","3f7ada16":"dataset.loc[:,'Pclass'] = dataset.loc[:,'Pclass'].astype('category')\ndataset_final = pd.get_dummies(dataset.drop(labels=['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1, inplace=False), prefix=['Pclass', 'Sex','Embarked', 'FamSCat', 'Title'])\ndataset_final.head()","060f817d":"train_final = dataset_final.iloc[:train.shape[0]-1, ]\ntest_final = dataset_final.iloc[train.shape[0]:, 1:]","b4c50b1b":"#ignore warning\ntrain_final.loc[:,'Survived'] = train_final.loc[:,'Survived'].astype(int)\n\nY_train = train_final.iloc[:,0]\nX_train = train_final.iloc[:,1:]","37f039b5":"# CV\nkfold = StratifiedKFold(n_splits=24)","f7ce0cfd":"# aggregated testing\nrandom_state = 2137\n\nclassifiers = []\nclassifiers.append(RandomForestClassifier(random_state=random_state))\nclassifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state), random_state=random_state, learning_rate=0.1))\nclassifiers.append(GradientBoostingClassifier(random_state=random_state))\nclassifiers.append(ExtraTreesClassifier(random_state=random_state))\nclassifiers.append(LinearDiscriminantAnalysis())\nclassifiers.append(QuadraticDiscriminantAnalysis())\nclassifiers.append(LogisticRegression(random_state = random_state))\nclassifiers.append(KNeighborsClassifier())\nclassifiers.append(DecisionTreeClassifier(random_state=random_state))\nclassifiers.append(MLPClassifier(random_state=random_state))\nclassifiers.append(SVC(random_state=random_state))\nclassifiers.append(XGBClassifier(random_state=random_state))\nclassifiers.append(GaussianNB())\nclassifiers.append(GaussianProcessClassifier(random_state=random_state))","43bdf2d0":"cv_results = []\n\nfor classifier in classifiers:\n    cv_results.append(cross_val_score(Pipeline([('scaler', StandardScaler()),\n                                                ('classifier', classifier)]), X_train, y=Y_train, scoring='accuracy', cv=kfold, n_jobs=-1))      \n\ncv_means = []\ncv_std = []\n\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n\ncv_res = pd.DataFrame({'Algorithm':['RandomForest', \n                                    'AdaBoost',\n                                    'GradientBoosting',\n                                    'ExtraTrees',\n                                    'LDA',\n                                    'QDA',\n                                    'LogisticRegression',\n                                    'kNN',\n                                    'DecisionTree',\n                                    'MultipleLayerPerceptron',\n                                    'SVC',\n                                    'XGBoost',\n                                    'GaussianNB',\n                                    'GaussianProcess'\n                                   ], \n                       'CrossValMeans':cv_means, 'CrossValErrors':cv_std})","c9701bec":"cv_res.sort_values(by=['CrossValMeans'], ascending=False, inplace=True)\ncv_res.reset_index(drop=True, inplace=True)\ncv_res","0405f33e":"p = sns.barplot('CrossValMeans', 'Algorithm', data=cv_res, palette='Spectral_r',orient='h', **{'xerr':cv_std})\np.set_xlabel('Mean accuracy')\np = p.set_title('CV scores')","83893459":"# RF tunning \n\n# search grid for optimal parameters\nrf_param_grid = {'classifier__n_estimators': [150],             # np.arange(50, 255, 50)\n                 'classifier__max_depth': [10],                 # None, 5, 10, 15, 20   \n                 'classifier__min_samples_split': [28],         # 2, 4, 8, 12, 16, 20, 24, 28\n                 'classifier__min_samples_leaf': [5],           # 1, 5, 10, 15, 20\n                 'classifier__min_weight_fraction_leaf': [0.0], # 0.0, 0.1\n                 'classifier__max_features': [10],              # 'auto', 'log2', 1, 3, 5, 10, 15   \n                 'classifier__bootstrap': [True]                # True, False\n                }\n\n\ngs_rf = GridSearchCV(Pipeline([('scaler', StandardScaler()),\n                               ('classifier', RandomForestClassifier())]),\n                     param_grid=rf_param_grid, cv=kfold, scoring='accuracy', n_jobs=-1, verbose=1)\n\ngs_rf.fit(X_train, Y_train)\n\nrf_best = gs_rf.best_estimator_\n\n# best score\ngs_rf.best_score_","c966de36":"# AdaBoost tunning \n\n# search grid for optimal parameters\nada_param_grid = {\"classifier__n_estimators\": [200],  # np.arange(100, 255, 25)\n                  \"classifier__learning_rate\": [0.05] # 0.025, 0.05, 0.1, 0.25, 0.5, 1, 1.5\n                 }\n\ngs_ada = GridSearchCV(Pipeline([('scaler', StandardScaler()),\n                                ('classifier', AdaBoostClassifier())]),\n                     param_grid=ada_param_grid, cv=kfold, scoring='accuracy', n_jobs=-1, verbose=1)\n\ngs_ada.fit(X_train, Y_train)\n\nada_best = gs_ada.best_estimator_\n\n# best score\ngs_ada.best_score_","d0fbec18":"# gradient boosting tunning\n\ngb_param_grid = {'classifier__learning_rate': [0.1],            # 0.1, 0.25, 0.5\n                 'classifier__n_estimators': [50],              # np.arange(50, 255, 50)\n                 'classifier__min_samples_split': [2],          # 2, 4, 8, 16, 20\n                 'classifier__min_samples_leaf': [1],           # 1, 5, 10, 15\n                 'classifier__min_weight_fraction_leaf': [0.0], # 0.0, 0.1\n                 'classifier__max_depth': [5],                  # 3, 5, 10\n                 'classifier__max_features': ['auto']           # 'auto', 'log2', 1, 3, 5\n                }\n\ngs_gb = GridSearchCV(Pipeline([('scaler', StandardScaler()),\n                               ('classifier', GradientBoostingClassifier())]),\n                     param_grid=gb_param_grid, cv=kfold, scoring='accuracy', n_jobs=-1, verbose=1)\n\ngs_gb.fit(X_train, Y_train)\n\ngb_best = gs_gb.best_estimator_\n\n# best score\ngs_gb.best_score_","f309795f":"# Extra-trees tunning\n\net_param_grid = {'classifier__n_estimators': [50],              # 50, 100, 150\n                 'classifier__max_depth': [5],                  # None, 5, 10       \n                 'classifier__min_samples_split': [4],          # 2, 4, 8, 16, 20 \n                 'classifier__min_samples_leaf': [5],           # 1, 5, 10, 15 \n                 'classifier__min_weight_fraction_leaf': [0.0], # 0.0, 0.1\n                 'classifier__max_features': ['auto'],          # 'auto', 'log2', 1, 3, 5  \n                 'classifier__bootstrap': [True]                # True, False             \n                }\n\ngs_et = GridSearchCV(Pipeline([('scaler', StandardScaler()),\n                               ('classifier', ExtraTreesClassifier())]),\n                     param_grid=et_param_grid, cv=kfold, scoring='accuracy', n_jobs=-1, verbose=1)\n\ngs_et.fit(X_train, Y_train)\n\net_best = gs_et.best_estimator_\n\n# best score\ngs_et.best_score_","ef241e1a":"# LDA tunning\n\nlda_param_grid = {'classifier__shrinkage': [0.5]} # np.linspace(0, 1, 11)\n\ngs_lda = GridSearchCV(Pipeline([('scaler', StandardScaler()),\n                               ('classifier', LinearDiscriminantAnalysis(solver='lsqr'))]),\n                      param_grid=lda_param_grid, cv=kfold, scoring='accuracy', n_jobs=-1, verbose=1)\n\ngs_lda.fit(X_train, Y_train)\n\nlda_best = gs_lda.best_estimator_\n\n# best score\ngs_lda.best_score_","e2881ca6":"# QDA tunning\n\nqda_param_grid = {'classifier__reg_param': [0.9]} # np.linspace(0, 1, 11)\n\ngs_qda = GridSearchCV(Pipeline([('scaler', StandardScaler()),\n                                ('classifier', QuadraticDiscriminantAnalysis(tol=1.0e-15))]),\n                      param_grid=qda_param_grid, cv=kfold, scoring='accuracy', n_jobs=-1, verbose=1)\n\ngs_qda.fit(X_train, Y_train)\n\nqda_best = gs_qda.best_estimator_\n\n# best score\ngs_qda.best_score_","56caf95e":"# Logistic regression tunning\n\nlr_param_grid = {'classifier__C': [0.1],       # np.linspace(0, 1, 11)      \n                 'classifier__l1_ratio': [0.5] # np.linspace(0, 1, 11)\n                }\n\ngs_lr = GridSearchCV(Pipeline([('scaler',  StandardScaler()),\n                               ('classifier', LogisticRegression(penalty='elasticnet',\n                                                                 solver='saga'))]), \n                     param_grid=lr_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs=-1, verbose=1)\n\ngs_lr.fit(X_train, Y_train)\n\nlr_best = gs_lr.best_estimator_\n\n# best score\ngs_lr.best_score_","6d736779":"# kNN classifier tunning\n\nknn_param_grid = {'classifier__n_neighbors': np.arange(1, 26),   \n                  'classifier__p': [1]                         # np.arange(1, 3)              \n                 }\n\ngs_knn = GridSearchCV(Pipeline([('scaler',  StandardScaler()),\n                                ('classifier', KNeighborsClassifier())]),\n                      param_grid=knn_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs=-1, verbose=1)\n\ngs_knn.fit(X_train, Y_train)\n\nknn_best = gs_knn.best_estimator_\n\n# best score\ngs_knn.best_score_","f0d1353a":"knn_cv_results = pd.DataFrame(gs_knn.cv_results_)\nknn_cv_results['Mean error'] = 1 - knn_cv_results['mean_test_score']\n\n\np = sns.catplot(x='param_classifier__n_neighbors', y='Mean error', data=knn_cv_results, kind='point', aspect=3)\np = p.set_xlabels('k')","edc8edda":"knn_best[1].n_neighbors = 10","1be614d6":"# decision tree tunning\n\ndt_param_grid = {'classifier__max_depth': [None],               # None, 5, 10, 15, 20\n                 'classifier__min_samples_split': [20],         # 2, 4, 8, 12, 16, 20\n                 'classifier__min_samples_leaf': [15],          # 1, 5, 10, 15, 20\n                 'classifier__min_weight_fraction_leaf': [0.0], # 0.0, 0.1\n                 'classifier__max_features': [15],              # 'auto', 'log2', 1, 3, 5, 10, 15, 20\n                }            \n\ngs_dt = GridSearchCV(Pipeline([('scaler',  StandardScaler()),\n                               ('classifier', DecisionTreeClassifier())]),\n                      param_grid=dt_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs=-1, verbose=1)\n\ngs_dt.fit(X_train, Y_train)\n\ndt_best = gs_dt.best_estimator_\n\n# best score\ngs_dt.best_score_","dd8874f4":"# multilayer perceptron tunning\n\nnn_param_grid = {'classifier__hidden_layer_sizes': [(25, )], # (100, ), (90, ), (80, ), (70, ), (60, ), (50, ), (40, ), (35, ), (30, ), (25, )\n                 'classifier__activation': ['relu']          # 'logistic', 'tanh', 'relu'\n                }\n\ngs_nn = GridSearchCV(Pipeline([('scaler',  StandardScaler()),\n                               ('classifier', MLPClassifier(solver='adam',\n                                                            max_iter=1000))]), \n                     param_grid=nn_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs=-1, verbose=1)\n\ngs_nn.fit(X_train, Y_train)\n\nnn_best = gs_nn.best_estimator_\n\n# best score\ngs_nn.best_score_","92fb82e3":"# SVM classifier tunning\n\nsvm_param_grid = {'classifier__C': [31],        # np.arange(1, 50, 3)             \n                  'classifier__gamma': [0.0025] # np.logspace(1, 10, 20, base=0.5)\n                 }\n\ngs_svm = GridSearchCV(Pipeline([('scaler',  StandardScaler()),\n                                ('classifier', SVC(probability=True))]),\n                      param_grid=svm_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs=-1, verbose=1)\n\ngs_svm.fit(X_train,Y_train)\n\nsvm_best = gs_svm.best_estimator_\n\n# best score\ngs_svm.best_score_","a1c8b597":"# XGBoost classifier tunning\n\nxgb_param_grid = {'classifier__max_depth': [15],         # 3, 4, 5, 6, 8, 10, 12, 15\n                  'classifier__learning_rate': [0.15],   # 0.05, 0.10, 0.15, 0.20, 0.25, 0.30\n                  'classifier__gamma': [1.65],           # np.arange(0, 2, 0.15)\n                  'classifier__min_child_weight': [3],   # 1, 3, 5, 7\n                  'classifier__colsample_bytree': [0.4], # 0.4, 0.5, 0.7, 0.8\n                  'classifier__reg_alpha': [0.0],        # np.linspace(0, 1, 4)\n                  'classifier__reg_lambda': [1.0]        # np.linspace(0, 1, 4)\n                 }\n\ngs_xgb = GridSearchCV(Pipeline([('scaler',  StandardScaler()),\n                                ('classifier', XGBClassifier())]),\n                      param_grid=xgb_param_grid, cv=kfold, scoring='accuracy', n_jobs=-1, verbose=1)\n\ngs_xgb.fit(X_train, Y_train)\n\nxgb_best = gs_xgb.best_estimator_\n\n# best score\ngs_xgb.best_score_","fda6318d":"gnb = Pipeline([('scaler',  StandardScaler()),\n                ('classifier', GaussianNB())])\n\ncv_gnb = cross_val_score(gnb, X_train, y=Y_train, scoring='accuracy', cv=kfold, n_jobs=-1)\n      \ngnb.fit(X_train, Y_train)\n\nnp.mean(cv_gnb)","fb7e596d":"gpc = Pipeline([('scaler',  StandardScaler()),\n                ('classifier', GaussianProcessClassifier())])\n\ncv_gpc = cross_val_score(gpc, X_train, y=Y_train, scoring='accuracy', cv=kfold, n_jobs=-1)\n      \ngpc.fit(X_train, Y_train)\n\nnp.mean(cv_gpc)","7520bdc9":"def plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"\n    Generate 3 plots: the test and training learning curve, the training\n    samples vs fit times curve, the fit times vs score curve.\n\n    Parameters\n    ----------\n    estimator : object type that implements the \"fit\" and \"predict\" methods\n        An object of that type which is cloned for each validation.\n\n    title : string\n        Title for the chart.\n\n    X : array-like, shape (n_samples, n_features)\n        Training vector, where n_samples is the number of samples and\n        n_features is the number of features.\n\n    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n        Target relative to X for classification or regression;\n        None for unsupervised learning.\n\n    axes : array of 3 axes, optional (default=None)\n        Axes to use for plotting the curves.\n\n    ylim : tuple, shape (ymin, ymax), optional\n        Defines minimum and maximum yvalues plotted.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n          - None, to use the default 5-fold cross-validation,\n          - integer, to specify the number of folds.\n          - :term:`CV splitter`,\n          - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer\/None inputs, if ``y`` is binary or multiclass,\n        :class:`StratifiedKFold` used. If the estimator is not a classifier\n        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validators that can be used here.\n\n    n_jobs : int or None, optional (default=None)\n        Number of jobs to run in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    train_sizes : array-like, shape (n_ticks,), dtype float or int\n        Relative or absolute numbers of training examples that will be used to\n        generate the learning curve. If the dtype is float, it is regarded as a\n        fraction of the maximum size of the training set (that is determined\n        by the selected validation method), i.e. it has to be within (0, 1].\n        Otherwise it is interpreted as absolute sizes of the training sets.\n        Note that for classification the number of samples usually have to\n        be big enough to contain at least one sample from each class.\n        (default: np.linspace(0.1, 1.0, 5))\n    \"\"\"\n    if axes is None:\n        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n\n    axes[0].set_title(title)\n    if ylim is not None:\n        axes[0].set_ylim(*ylim)\n    axes[0].set_xlabel(\"Training examples\")\n    axes[0].set_ylabel(\"Score\")\n\n    train_sizes, train_scores, test_scores, fit_times, _ = \\\n        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n                       train_sizes=train_sizes,\n                       return_times=True)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    fit_times_mean = np.mean(fit_times, axis=1)\n    fit_times_std = np.std(fit_times, axis=1)\n\n    # Plot learning curve\n    axes[0].grid()\n    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n                         train_scores_mean + train_scores_std, alpha=0.1,\n                         color=\"r\")\n    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1,\n                         color=\"g\")\n    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n                 label=\"Training score\")\n    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n                 label=\"Cross-validation score\")\n    axes[0].legend(loc=\"best\")\n\n    # Plot n_samples vs fit_times\n    axes[1].grid()\n    axes[1].plot(train_sizes, fit_times_mean, 'o-')\n    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n                         fit_times_mean + fit_times_std, alpha=0.1)\n    axes[1].set_xlabel(\"Training examples\")\n    axes[1].set_ylabel(\"fit_times\")\n    axes[1].set_title(\"Scalability of the model\")\n\n    # Plot fit_time vs score\n    axes[2].grid()\n    axes[2].plot(fit_times_mean, test_scores_mean, 'o-')\n    axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1)\n    axes[2].set_xlabel(\"fit_times\")\n    axes[2].set_ylabel(\"Score\")\n    axes[2].set_title(\"Performance of the model\")\n\n    return plt","ae12c21e":"fig, axes = plt.subplots(3, 3, figsize=(25, 20))\n\ntitle = \"Learning Curves (Random forest)\"\nestimator = rf_best\nplot_learning_curve(estimator, title, X_train, Y_train, axes=axes[:,0], ylim=(0.7, 1.01), cv=kfold, n_jobs=-1)\n\n\ntitle = \"Learning Curves (AdaBoost)\"\nestimator = ada_best\nplot_learning_curve(estimator, title, X_train, Y_train, axes=axes[:,1], ylim=(0.7, 1.01), cv=kfold, n_jobs=-1)\n\n\ntitle = \"Learning Curves (Gradient boosting)\"\nestimator = gb_best\nplot_learning_curve(estimator, title, X_train, Y_train, axes=axes[:,2], ylim=(0.7, 1.01), cv=kfold, n_jobs=-1)\n\nplt.show()","eee99d0f":"fig, axes = plt.subplots(3, 3, figsize=(25, 20))\n\ntitle = \"Learning Curves (Extra-trees)\"\nestimator = et_best\nplot_learning_curve(estimator, title, X_train, Y_train, axes=axes[:,0], ylim=(0.7, 1.01), cv=kfold, n_jobs=-1)\n\n\ntitle = \"Learning Curves (LDA)\"\nestimator = lda_best\nplot_learning_curve(estimator, title, X_train, Y_train, axes=axes[:,1], ylim=(0.7, 1.01), cv=kfold, n_jobs=-1)\n\n\ntitle = \"Learning Curves (QDA)\"\nestimator = qda_best\nplot_learning_curve(estimator, title, X_train, Y_train, axes=axes[:,2], ylim=(0.7, 1.01), cv=kfold, n_jobs=-1)\n\nplt.show()","8df1568b":"fig, axes = plt.subplots(3, 3, figsize=(25, 20))\n\ntitle = \"Learning Curves (Logistic regression)\"\nestimator = lr_best\nplot_learning_curve(estimator, title, X_train, Y_train, axes=axes[:,0], ylim=(0.7, 1.01), cv=kfold, n_jobs=-1)\n\n\ntitle = \"Learning Curves (kNN)\"\nestimator = knn_best\nplot_learning_curve(estimator, title, X_train, Y_train, axes=axes[:,1], ylim=(0.7, 1.01), cv=kfold, n_jobs=-1)\n\n\ntitle = \"Learning Curves (Decision tree)\"\nestimator = dt_best\nplot_learning_curve(estimator, title, X_train, Y_train, axes=axes[:,2], ylim=(0.7, 1.01), cv=kfold, n_jobs=-1)\n\nplt.show()","322c1ff7":"fig, axes = plt.subplots(3, 3, figsize=(25, 20))\n\ntitle = \"Learning Curves (Multilayer perceprton)\"\nestimator = nn_best\nplot_learning_curve(estimator, title, X_train, Y_train, axes=axes[:,0], ylim=(0.7, 1.01), cv=kfold, n_jobs=-1)\n\n\ntitle = \"Learning Curves (SVM)\"\nestimator = svm_best\nplot_learning_curve(estimator, title, X_train, Y_train, axes=axes[:,1], ylim=(0.7, 1.01), cv=kfold, n_jobs=-1)\n\n\ntitle = \"Learning Curves (XGBoost)\"\nestimator = xgb_best\nplot_learning_curve(estimator, title, X_train, Y_train, axes=axes[:,2], ylim=(0.7, 1.01), cv=kfold, n_jobs=-1)\n\nplt.show()","ac3d3785":"fig, axes = plt.subplots(3, 2, figsize=(25, 20))\n\ntitle = \"Learning Curves (Gaussian naive bayes)\"\nestimator = gnb\nplot_learning_curve(estimator, title, X_train, Y_train, axes=axes[:,0], ylim=(0.7, 1.01), cv=kfold, n_jobs=-1)\n\n\ntitle = \"Learning Curves (Gaussian process)\"\nestimator = gpc\nplot_learning_curve(estimator, title, X_train, Y_train, axes=axes[:,1], ylim=(0.7, 1.01), cv=kfold, n_jobs=-1)\n\nplt.show()","5c662876":"nrows = 2\nncols = 3\nfig, axes = plt.subplots(nrows = nrows, ncols = ncols, sharex=\"all\", figsize=(25,20))\n\nnames_classifiers = [(\"Random forest\", rf_best[1]),\n                     (\"AdaBoosting\", ada_best[1]),\n                     (\"Gradient boosting\", gb_best[1]),\n                     (\"Extra-Trees\", et_best[1]),\n                     (\"Decision Tree\", dt_best[1]),\n                     (\"XGBoost\", xgb_best[1])\n                    ]\n\nnclassifier = 0\nfor row in range(nrows):\n    for col in range(ncols):\n        name = names_classifiers[nclassifier][0]\n        classifier = names_classifiers[nclassifier][1]\n        indices = np.argsort(classifier.feature_importances_)[::-1][:40]\n        g = sns.barplot(y=X_train.columns[indices][:40],x = classifier.feature_importances_[indices][:40] , orient='h',ax=axes[row][col])\n        g.set_xlabel(\"Relative importance\",fontsize=12)\n        g.set_ylabel(\"Features\",fontsize=12)\n        g.tick_params(labelsize=9)\n        g.set_title(\"Feature importance (\" + name + \")\")\n        nclassifier += 1","52e68e4f":"voting = VotingClassifier(estimators=[('RF', rf_best),\n                                      ('ADA', ada_best),\n                                      #('GB', gb_best),\n                                      ('ET', et_best), \n                                      ('LDA', lda_best),\n                                      ('QDA', qda_best),\n                                      ('LR', lr_best),\n                                      ('kNN', knn_best),\n                                      ('DT', dt_best),\n                                      ('NN', nn_best),\n                                      ('SVM', svm_best),\n                                      ('XGB', xgb_best),\n                                      #('GNB', gnb),\n                                      #('GPC', gpc)\n                                      ], voting='soft', n_jobs=-1)\n\nvoting = voting.fit(X_train, Y_train)","bea51f1e":"cv_voting = cross_val_score(voting, X_train, y=Y_train, scoring='accuracy', cv=kfold, n_jobs=-1)\n      \nvoting.fit(X_train, Y_train)\n\nnp.mean(cv_voting)","4588e94a":"fig, axes = plt.subplots(3, 1, figsize=(25, 20))\n\ntitle = \"Learning Curves (Voting classifier)\"\nestimator = voting\nplot_learning_curve(estimator, title, X_train, Y_train, axes=axes, ylim=(0.7, 1.01), cv=kfold, n_jobs=-1)\n\nplt.show()","f8f7f4b3":"test_survived_rf = pd.Series(rf_best.predict(test_final), name=\"RF\")\ntest_survived_ada = pd.Series(ada_best.predict(test_final), name=\"ADA\")\ntest_survived_gb = pd.Series(gb_best.predict(test_final), name=\"GB\")\ntest_survived_et = pd.Series(et_best.predict(test_final), name=\"ET\")\ntest_survived_lda = pd.Series(lda_best.predict(test_final), name=\"LDA\")\ntest_survived_qda = pd.Series(qda_best.predict(test_final), name=\"QDA\")\ntest_survived_lr = pd.Series(lr_best.predict(test_final), name=\"LR\")\ntest_survived_knn = pd.Series(knn_best.predict(test_final), name=\"kNN\")\ntest_survived_dt = pd.Series(dt_best.predict(test_final), name=\"DT\")\ntest_survived_nn = pd.Series(nn_best.predict(test_final), name=\"NN\")\ntest_survived_svm = pd.Series(svm_best.predict(test_final), name=\"SVM\")\ntest_survived_xgb = pd.Series(xgb_best.predict(test_final), name=\"XGB\")\ntest_survived_gnb = pd.Series(gnb.predict(test_final), name=\"GNB\")\ntest_survived_gpc = pd.Series(gpc.predict(test_final), name=\"GPC\")\n\ntest_survived_vot = pd.Series(voting.predict(test_final), name=\"VOT\")\n\n\n# concatenate all classifier results\nensemble_results = pd.concat([test_survived_rf,\n                              test_survived_ada,\n                              test_survived_gb,\n                              test_survived_et,\n                              test_survived_lda,\n                              test_survived_qda,\n                              test_survived_lr,\n                              test_survived_knn,\n                              test_survived_dt,\n                              test_survived_nn,\n                              test_survived_svm,\n                              test_survived_xgb,\n                              test_survived_gnb,\n                              test_survived_gpc,\n                              test_survived_vot\n                             ], axis=1)\n\n\nfig, ax = plt.subplots(figsize=(20,10))\np = sns.heatmap(ensemble_results.corr(), annot=True, ax=ax)","f5fc343c":"results = pd.concat([test[\"PassengerId\"], test_survived_vot], axis=1)\nresults.columns = ['PassengerId', 'Survived']\nresults.to_csv('submission.csv', index=False)","5742ed76":"### Embarked","c2ed67b0":"A port of embarkation coded as:\n * C = Cherbourg;\n * Q = Queenstown;\n * S = Southampton.","28c70cf3":"## Prediction diffrences among models","9dbf4290":"# Feature engineering","18842c7e":"Seems that that higher number of siblings\/spouses decrease the chance of survival. As the passengers with **SibSP** equal to one, two or zero have the highest probability of survival we could suspect that being a part of large family decrease your chance of survival the disaster.","e0074f9b":"We handled missing values in several diffrent ways and presented it below.","0f4a3f48":"*Passenger fare*","6fc657a6":"## Numerical variables","fbf44b9f":"We will present different approaches to model fitting. At first we fit simple models without any additional steps, later we add scaling and in the last approach we also add *PCA*\n\nThe general pipeline for modeling looks as follows:\n\n*scaler* \u2192 *PCA* \u2192 *ML algorithm*.","d2d12dbd":"## Data import","5705f9f5":"### Decision tree","1abca993":"# Modelling ","9367dc18":"### Age","dc9f9c77":"## Learning curves","a149a76c":"#### Pclass","49f9bcc1":" There we two observations with `Null` value in the **Embarked** column.","38002ec3":"### Sex","ba97c92f":"## Title","7d742a5c":"### QDA","f0c84ac4":"## Age","52d88717":"Fare distribution is very skewed, we tried to *unskew* it using logarithm.","4ae53e0a":"There is similiar conclusion as in case of **SibSp** variable - little families (**Parch** below 3) seems to have higher probability of survival. Passengers with larger number of relatives of different generations has higher variance of **Survived** variable. \n\nWe use this observation in a *Feature engineering* part. ","dc6a9f6f":"After tranformation it looks much better, no other modifications are curently needed. Let plot **Fare** variable and its distribution with relation to **Survival probability**.","873a38f8":"Before start of modelling we create dummy variables for all categorical variables we had in the dataset.","97201ac4":"### MLP","cffef722":"<span style=\"font-size:5em;\">Titanic Dataset<\/span>\n\n**Micha\u0142 M.**","0be44d8f":"### Gaussian methods","4fc532c8":"These two plot confirms what we know about Titanic disaster from historical sources - *Childern and ladies first!*. There is visible diffrrence among sexes in the survival probability, moreover younger passengers have a higher chance of survival in comparison to older ones.","fab97289":"Recall, there was one observation with `Null` value in the **Fare** column.","ee237cb1":"We can conclude that the third class is the most frequent for passenger coming from Southampton and Queenstown, whereas Cherbourg passengers are mostly in first class which have the highest survival rate.\n\nWe can't explain why first class has an higher survival rate. Probably first class passengers were prioritised during the evacuation due to their influence.","894b7b65":"##  Feature importance","6f9c7de0":"*Ticket class*","24e04d75":"Now we proceed to categorical variables and their distribution with relation to variable *Survived*","65482fcf":"### SibSp","a0c62927":"# Variables exploration","22495c3b":"## Categorical Variables","5fde35cc":"## Data integrity","78b4d779":"## Embarked and Fare","1a96ca71":"## Joining train and train sets","81dbab4c":"There is slight, visible diffrence beetween distributions.","2ec584e6":"## Ensamble modeling","632c01f4":"Variable **Age** contains 256 missing values which should be somehow filled in order to preserve usability of **Age** variable. \n\nLet's look at the variables most correlated with the **Age** and its distribution among them.  ","f4793461":"There is no obvious relationship between variable **Survived** and other dependent, numerical variables.","1cc7cf9b":"### Random forest","24bb8aeb":"*Age in years*\n\nAge is fractional if less than 1. If the age is estimated, is it in the form of xx.5","30603178":"### LDA","2835d0be":"Data are provided in typical for Kaggle way - two datasets, *train* and *test*.","54d96145":"## Final prediction","9b21fa84":"### SVM  ","49e19ce2":"###  Logistic regression","90261e0e":"We filled it with most common value, i.e. 'S'.","784697fa":"### Fare","2a169554":"There were a lot missing values in the **cabin** column (almost 80%), what probably would make it hard to use. Missing values in **Survived** column come from the *test* dataset. There were some missing values for **Age** and we would try to fill them with more advanced method, **Embraked** and **Fare** variables could be filled using some standard approach like *mean* or *median*. ","7ce3acf8":"# Introduction\nThis is my first Kaggle kernel. I choosed the well-known Titanic dataset, I just wanted to imporve my python skills. I am advenced R user, but have minor experience with *the snake*.\n\nTo all of you who does not know the *Titanic* dataset, it is one of the most popular Kaggle competition. The data contais information about Titanic passengers, the main objective of this task is to predict which people survived the disater.\n\nThis notebook follows four main parts:\n\n* Data import\n* Missing values imputation\n* Feature analysis\n* Feature engineering\n* Modeling\n\nWe began with importing necessary libraries.","b52b7315":"It seems that passenger coming from Cherbourg have more chance to survive.\n\nLet's check proportion of classes among all three ports.","6bd9d7db":"*Port of Embarkation*","0359ec7d":"## (Hyper)parameter tunning","6f8d078e":"# Filling missing values","0e62b781":"## Family size","27af7d95":"# Data load and initial checks","c66e573c":"According to the Kaggle data dictionary, both **SibSp** and **Parch** relate to traveling with family. We can combine both to introduce variable **Family size**, deonted **FamS**. We suppose that large families might be hard to be rescued during disaster. ","500e15c9":"We add additional categorical variable to aggregate family size.","c11b8f65":"We would perform grid search to find best hyperparameters for all algorithms.","f0ed4f1c":"### Extra-trees","ad072a13":"### kNN","7d31db0b":"We choose the LDA, Logistic regression, MLP, and the Gradient boosting classifiers for the further tunning and ensamble modeling modeling. \nWe also choose SVM just to check whether we are be able to tunning it well.","31ca3e53":"I decided to exclude classifiers which seem to overfit from ensamble modeling.","dd582dd0":"A proxy for socio-economic status (SES):\n * 1st = Upper\n * 2nd = Middle\n * 3rd = Lower","6505a2eb":"### XGBoost","0f330d3e":"According to the provided data dictionary, variables could be descibed as follows.\n\n| Variable |                 Definition                 |                       Key                      |\n|----------|--------------------------------------------|------------------------------------------------|\n| survival | Survival                                   | 0 = No, 1 = Yes                                |\n| pclass   | Ticket class                               | 1 = 1st, 2 = 2nd, 3 = 3rd                      |\n| sex      | Sex                                        |                                                |\n| Age      | Age in years                               |                                                |\n| sibsp    | # of siblings \/ spouses aboard the Titanic |                                                |\n| parch    | # of parents \/ children aboard the Titanic |                                                |\n| ticket   | Ticket number                              |                                                |\n| fare     | Passenger fare                             |                                                |\n| cabin    | Cabin number                               |                                                |\n| embarked | Port of Embarkation                        | C = Cherbourg, Q = Queenstown, S = Southampton |\n\nThere are 11 independent variables.","0b86e1e6":"*number of parents\/children aboard the Titanic*\n\nThe dataset defines family relations in this way:\n * Parent = mother, father;\n * Child = daughter, son, stepdaughter, stepson.\n \nSome children travelled only with a nanny, therefore parch=0 for them.","4e54cc80":"We try to extract title from the **Name** colum.","fb0f5cda":"*number of siblings\/spouses aboard the Titanic*\n\nThe dataset defines family relations in this way:\n * Sibling = brother, sister, stepbrother, stepsister;\n * Spouse = husband, wife (mistresses and fianc\u00e9s were ignored).","bc6dda38":"### AdaBoost","eefe4ca5":"## Dummy variables\/train set preparation","e4c83c3b":"It is visible, that **sex** play import role in survival of the underlying passenger. ","84dec22c":"## Fast modelling","5f04f229":"Obtained variable migh be usefull during forecasting as there is visible diffrence among the groups.","cc8ac1c6":"### Parch","49af537c":"### Gradient boosting","b6abf2b3":"We fill it with median value."}}