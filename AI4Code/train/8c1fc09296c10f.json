{"cell_type":{"28a5cadf":"code","257b23ab":"code","50bad0d1":"code","d9c041c6":"code","299295cb":"code","d60ed36d":"code","b85e02fd":"code","f3b63b63":"code","c2f0b8df":"code","5c0ad297":"code","e546849d":"code","2027b3bf":"code","2bbc75d9":"code","4a9f0def":"code","7b0ae898":"code","9d6f00f2":"code","b423c67b":"code","05f7fd1e":"code","68829ef2":"code","f8583643":"code","c270ca7a":"code","faa3ab62":"code","13d2ca08":"code","69fc76f7":"code","b7296fc5":"code","97fa0bee":"code","72755924":"code","65cf1227":"code","ca014719":"code","5feed8b8":"code","c74788cc":"code","5170d3db":"code","e944086f":"code","bab6f184":"code","e7f27828":"code","b79313c9":"code","c752ec44":"code","6b7b604c":"code","47d87fa0":"code","efee767c":"code","8d266111":"code","52474fc4":"code","091e267d":"code","84c9a7ed":"code","7d52f4df":"code","2f3ff01f":"code","4034b861":"code","259ba2f2":"code","d5d65735":"code","cb4de423":"code","24d821c1":"code","fedbbacc":"code","affb7bcf":"code","64131d0e":"code","07d05b92":"code","18c440a0":"code","03f78b1c":"code","18a5ffd6":"markdown","151db3bd":"markdown","f0c50495":"markdown","8a52cbf8":"markdown","5569085b":"markdown","843d51b7":"markdown","dc86450d":"markdown","cce50653":"markdown","6091df04":"markdown","e638b7fb":"markdown","4910a3eb":"markdown","7c66471b":"markdown","abe54831":"markdown","7719bbaf":"markdown","d14e7553":"markdown","2a9207bd":"markdown","e3955a2c":"markdown","c223371c":"markdown"},"source":{"28a5cadf":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport random\nimport statistics \nfrom pandas import get_dummies\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import roc_auc_score\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import normalize\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error\nfrom sklearn.metrics import roc_auc_score,roc_curve\nfrom imblearn.pipeline import Pipeline as imbPipe\nfrom xgboost import  XGBClassifier\nimport warnings\nwarnings.filterwarnings(\"ignore\")","257b23ab":"df_train = pd.read_csv(\"..\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv\")\ndf_test = pd.read_csv(\"..\/input\/hr-analytics-job-change-of-data-scientists\/aug_test.csv\")","50bad0d1":"df_train","d9c041c6":"df_train.info()","299295cb":"df_train.isnull().sum()","d60ed36d":"df_test.isnull().sum()","b85e02fd":"#Countplots showing the frequency of each category with respect to education level \nplt.figure(figsize=[15,17])\nplot=[\"relevent_experience\", \"education_level\",\"major_discipline\", \"experience\",\"company_size\",\"company_type\", \"training_hours\",\"target\"]\nn=1\nfor f in plot:\n    plt.subplot(4,2,n)\n    sns.countplot(x=f, hue='education_level', edgecolor=\"black\", alpha=0.7, data=df_train)\n    sns.despine()\n    plt.title(\"Countplot of {}  by education_level\".format(f))\n    n=n+1\nplt.tight_layout()\nplt.show()","f3b63b63":"import plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\n\nel = df_train['education_level'].value_counts().reset_index()\nel.columns = [\n    'education_level', \n    'percent'\n]\nel['percent'] \/= len(df_train)\n\nfig = px.pie(\n    el, \n    names='education_level', \n    values='percent', \n    title='Education_level', \n    width=800,\n    height=500 \n)\n\nfig.show()","c2f0b8df":"sns.pairplot(df_train)","5c0ad297":"# Enrolee total experience in years\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\n\nep = df_train['experience'].value_counts().reset_index()\nep.columns = [\n    'experience', \n    'percent'\n]\nep['percent'] \/= len(df_train)\n\nfig = px.pie(\n    ep, \n    names='experience', \n    values='percent', \n    title='Experience', \n    width=800,\n    height=500 \n)\n\nfig.show()","e546849d":"def clean_experience(df):\n    for i in df[\"experience\"]:\n        if(i==\">20\"):\n            df[\"experience\"][df[\"experience\"]==i]=27\n        if(i == \"<1\"):\n            df[\"experience\"][df[\"experience\"]==i]=0\nclean_experience(df_train)\nclean_experience(df_test)\n\ndf_train[\"experience\"] = df_train[\"experience\"].fillna(0)\ndf_train[\"experience\"] = df_train['experience'].astype('int')\ndf_test[\"experience\"] = df_test[\"experience\"].fillna(0)\ndf_test[\"experience\"] = df_test['experience'].astype('int')","2027b3bf":"f, axes = plt.subplots(1,1, figsize = (16, 5))\ng1 = sns.distplot(df_train[\"training_hours\"], color=\"blue\",ax = axes)\nplt.title(\"Distributional of training_hours\")","2bbc75d9":"# education_level:training_hours\net = df_train.sort_values(by='training_hours', ascending=True)[:7000]\nfigure = plt.figure(figsize=(10,6))\nsns.barplot(y=et.education_level, x=et.training_hours)\nplt.xticks()\nplt.xlabel('training_hours')\nplt.ylabel('education_level')\nplt.title('education_level:training_hours ')\nplt.show()","4a9f0def":"import plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\n\ncd = df_train['city_development_index'].value_counts().reset_index()\ncd.columns = [\n    'city_development_index', \n    'count'\n]\ncd['city_development_index'] = cd['city_development_index'].astype(str) + '-'\ncd = cd.sort_values(['count']).tail(50)\n\nfig = px.bar(\n    cd, \n    x='count', \n    y='city_development_index', \n    orientation='h', \n    title='Count: City development index', \n    width=1000,\n    height=900 \n)\n\nfig.show()","7b0ae898":"cdi = df_train.sort_values(by='city_development_index', ascending=True)[:2000]\nfigure = plt.figure(figsize=(10,6))\nsns.barplot(y=cdi.city, x=cdi.city_development_index)\nplt.xticks()\nplt.xlabel('city_development_index')\nplt.ylabel('city')\nplt.title('City by city development index')\nplt.show()","9d6f00f2":"f, axes = plt.subplots(1,1, figsize = (16, 5))\ng1 = sns.distplot(df_train[\"city_development_index\"], color=\"red\",ax = axes)\nplt.title(\"Distributional of city_development_index\")","b423c67b":"def clean_NAN(df):\n    df[\"gender\"] = df[\"gender\"].fillna(\"Unknown\")\n    df[\"education_level\"]=df[\"education_level\"].fillna(\"Unknown\")\n    df[\"major_discipline\"].fillna(value=\"Unknown\", inplace=True)\n    df[\"experience\"] = df[\"experience\"].fillna(df[\"experience\"].mean())\n    df[\"company_type\"] = df[\"company_type\"].fillna(\"Unknown\")\nclean_NAN(df_train)\nclean_NAN(df_test)","05f7fd1e":"def clean_company_size_1(df):\n    converted_list_1 = []\n    converted_list_2 = []\n    converted_list_3 = []\n    for i in df[\"company_size\"]:\n        if i == \"10\/49\":\n            i = \"10-49\"\n            converted_list_1.append(i)\n        converted_list_1.append(i)\n        if i == \"<10\":\n            i = '1-9'\n            converted_list_2.append(i)\n        converted_list_2.append(i)\n        if i == \"10000+\":\n            i = '10000-20000'\n            converted_list_3.append(i)\n        converted_list_3.append(i)\n    df[\"company_size\"]=pd.Series(converted_list_1)\n    df[\"company_size\"]=pd.Series(converted_list_2)\n    df[\"company_size\"]=pd.Series(converted_list_3)\n    new = df['company_size'].str.split(\"-\", n = 1, expand = True) \n    df['company_size_min']= new[0]\n    df['company_size_max']= new[1] \n    df[\"company_size_max\"] = df['company_size_max'].astype('int')\n    df[\"company_size_min\"] = df['company_size_min'].astype('int')\ndf_train[\"company_size\"]=df_train[\"company_size\"].fillna(\"0-0\")\ndf_test[\"company_size\"]=df_test[\"company_size\"].fillna(\"0-0\")\nclean_company_size_1(df_train)\nclean_company_size_1(df_test)","68829ef2":"def clean_last_new_job(df):\n    converted_list_1 = []\n    converted_list_2 = []\n    for i in df[\"last_new_job\"]:\n        if i == \"never\" or i == np.NaN:\n            i = 0\n            converted_list_1.append(i)\n        converted_list_1.append(i)\n        if i == \">4\":\n            i = 6\n            converted_list_2.append(i)\n        converted_list_2.append(i)\n    df[\"last_new_job\"]=pd.Series(converted_list_1)\n    df[\"last_new_job\"]=pd.Series(converted_list_2)\nclean_last_new_job(df_train)\nclean_last_new_job(df_test)","f8583643":"def clean_city(df):\n    converted_list_1 = []\n    for i in range(len(df[\"city\"])):\n        j = df[\"city\"][i].replace(\"city_\",\"\")\n        converted_list_1.append(j)\n    df[\"city\"]=pd.Series(converted_list_1)\nclean_city(df_train)\nclean_city(df_test)","c270ca7a":"def clean_relevent_experience(df):\n    converted_list_1 = []\n    converted_list_2 = []\n    for i in df[\"relevent_experience\"]:\n        if i == \"Has relevent experience\":\n            i = 1\n            converted_list_1.append(i)\n        converted_list_1.append(i)\n        if i == \"No relevent experience\":\n            i = 0\n            converted_list_2.append(i)\n        converted_list_2.append(i)\n    df[\"relevent_experience\"]=pd.Series(converted_list_1)\n    df[\"relevent_experience\"]=pd.Series(converted_list_2)\nclean_relevent_experience(df_train)\nclean_relevent_experience(df_test)","faa3ab62":"def one_hot_encoding(df):\n    enrolled_dummies = pd.get_dummies(df[\"enrolled_university\"], dummy_na=True)\n    gender_dummies = pd.get_dummies(df[\"gender\"], dummy_na=True)\n    education_dummies = pd.get_dummies(df[\"education_level\"],dummy_na=True)\n    stream_dummies = pd.get_dummies(df[\"major_discipline\"],dummy_na=True)\n    company_dummies = pd.get_dummies(df[\"company_type\"],dummy_na=True)\n    df[\"Type_no_enrollment\"] = enrolled_dummies[\"no_enrollment\"]\n    df[\"Type_Full_time_course\"] = enrolled_dummies[\"Full time course\"]\n    df[\"Type_Part_time_course\"]=enrolled_dummies[\"Part time course\"]\n    df[\"Gender_Male\"] = gender_dummies[\"Male\"]\n    df[\"Gender_Female\"] =gender_dummies[\"Female\"]\n    df[\"Gender_Unknown\"]=gender_dummies[\"Unknown\"]\n    df[\"Gender_Other\"]=gender_dummies[\"Other\"]\n    df[\"Education_Graduate\"] = education_dummies[\"Graduate\"]\n    df[\"Education_Masters\"] = education_dummies[\"Masters\"]\n    df[\"Education_High_School\"] = education_dummies[\"High School\"]\n    df[\"Education_Primary_School\"] = education_dummies[\"Primary School\"]\n    df[\"Education_Phd\"] = education_dummies[\"Phd\"]\n    df[\"Education_Unknown\"] = education_dummies[\"Unknown\"]\n    df[\"Stream_STEM\"] = stream_dummies[\"STEM\"]\n    df[\"Stream_Humanities\"] = stream_dummies[\"Humanities\"]\n    df[\"Stream_Other\"] = stream_dummies[\"Other\"]\n    df[\"Stream_Business_Degree\"] = stream_dummies[\"Business Degree\"]\n    df[\"Stream_Arts\"] = stream_dummies[\"Arts\"]\n    df[\"Stream_No_Major\"] = stream_dummies[\"No Major\"]\n    df[\"Stream_Unknown\"] = stream_dummies[\"Unknown\"]\n    df[\"Company_Pvt_Ltd\"] = company_dummies[\"Pvt Ltd\"]\n    df[\"Company_Funded_Startup\"] = company_dummies[\"Funded Startup\"]\n    df[\"Company_Public_Sector\"]=company_dummies[\"Public Sector\"]\n    df[\"Company_Early_Stage_Startup\"] = company_dummies[\"Early Stage Startup\"]\n    df[\"Company_NGO\"] = company_dummies[\"NGO\"]\n    df[\"Company_Other\"] = company_dummies[\"Other\"]\n    df[\"Company_Unknown\"] = company_dummies[\"Unknown\"]\none_hot_encoding(df_train)\none_hot_encoding(df_test)\n","13d2ca08":"df_train = df_train.dropna(subset=['enrolled_university',\"last_new_job\"])\ndf_test = df_test.dropna(subset=['enrolled_university',\"last_new_job\"])","69fc76f7":"def clean_company_size_2(df):\n    converted_list_1 = []\n    converted_list_2 = []\n    for i in df[\"company_size_min\"]:\n        if i == 0:\n            i = int(df[\"company_size_min\"].mean())\n            converted_list_1.append(i)\n        converted_list_1.append(i)\n    for i in df[\"company_size_max\"]:\n        if i == 0:\n            i = int(df[\"company_size_max\"].mean())\n            converted_list_2.append(i)\n        converted_list_2.append(i)\n    df[\"company_size_min\"]=pd.Series(converted_list_1)\n    df[\"company_size_max\"]=pd.Series(converted_list_2)\ndf_train[\"company_size_min\"] = df_train[\"company_size_min\"].fillna(int(df_train[\"company_size_min\"].mean()))\ndf_train[\"company_size_max\"] = df_train[\"company_size_max\"].fillna(int(df_train[\"company_size_max\"].mean()))\n\ndf_test[\"company_size_min\"] = df_test[\"company_size_min\"].fillna(int(df_test[\"company_size_min\"].mean()))\ndf_test[\"company_size_max\"] = df_test[\"company_size_max\"].fillna(int(df_test[\"company_size_max\"].mean()))\n\nclean_company_size_2(df_test)\nclean_company_size_2(df_train)","b7296fc5":"df_test.isnull().sum()","97fa0bee":"# Target\n# 0 \u2013 Not looking for job change,\n# 1 \u2013 Looking for a job change\n# As you can see, here we have imbalanced data, the number of 1 ( Looking for a job change) < 0 (Not looking for job change)\nmnj = df_train['target'].value_counts()  \nplt.figure(figsize=(6,4))\nsns.barplot(mnj.index, mnj.values, alpha=0.8)\nplt.ylabel('Number of Data', fontsize=12)\nplt.xlabel('target', fontsize=9)\nplt.xticks(rotation=90)\nplt.show();","72755924":"df_test.index = np.arange(0,len(df_test))","65cf1227":"df_test_copy = df_test.copy()\ndf_test","ca014719":"df_train = df_train.drop(['enrollee_id','gender','enrolled_university','education_level','major_discipline','company_type','company_size'],axis=1)\ndf_test = df_test.drop(['enrollee_id','gender','enrolled_university','education_level','major_discipline','company_type',\"company_size\"],axis=1)","5feed8b8":"X = df_train.drop(\"target\",axis=1)\nY = pd.DataFrame(df_train[\"target\"])","c74788cc":"smote = SMOTE()\nX, Y = smote.fit_resample(X, Y)","5170d3db":"X","e944086f":"Y[\"target\"].value_counts()","bab6f184":"df_train_final = X.copy()\ndf_train_final['target'] = Y\ndf_test_final = df_test.copy()","e7f27828":"cols_to_be_normalized = [\"city\",\"city_development_index\",\"experience\",\"last_new_job\",\"training_hours\",\"company_size_min\",\"company_size_max\"]\ncols_not_to_be_normalized = [\"relevent_experience\",\"Type_no_enrollment\",\"Type_Full_time_course\",\"Type_Part_time_course\",\"Gender_Male\",\"Gender_Female\",\"Gender_Unknown\",\n                            \"Gender_Other\",\"Education_Graduate\",\"Education_Masters\",\"Education_High_School\",\"Education_Primary_School\",\"Education_Phd\",\n                            \"Education_Unknown\",\"Stream_STEM\",\"Stream_Humanities\",\"Stream_Other\",\"Stream_Business_Degree\",\"Stream_Arts\",\"Stream_No_Major\",\n                            \"Stream_Unknown\",\"Company_Pvt_Ltd\",\"Company_Funded_Startup\",\"Company_Public_Sector\", \"Company_Early_Stage_Startup\", \"Company_NGO\",\n                            \"Company_Other\", \"Company_Unknown\", \"target\"]","b79313c9":"train_normalized = normalize(df_train_final[cols_to_be_normalized])\ntrain_boolean = df_train_final[cols_not_to_be_normalized]\ndf_train_normalized = pd.DataFrame(train_normalized,columns = cols_to_be_normalized)\ndf_train_boolean = pd.DataFrame(train_boolean,columns=cols_not_to_be_normalized)","c752ec44":"cols_to_be_normalized = [\"city\",\"city_development_index\",\"experience\",\"last_new_job\",\"training_hours\",\"company_size_min\",\"company_size_max\"]\ncols_not_to_be_normalized = [\"relevent_experience\",\"Type_no_enrollment\",\"Type_Full_time_course\",\"Type_Part_time_course\",\"Gender_Male\",\"Gender_Female\",\"Gender_Unknown\",\n                            \"Gender_Other\",\"Education_Graduate\",\"Education_Masters\",\"Education_High_School\",\"Education_Primary_School\",\"Education_Phd\",\n                            \"Education_Unknown\",\"Stream_STEM\",\"Stream_Humanities\",\"Stream_Other\",\"Stream_Business_Degree\",\"Stream_Arts\",\"Stream_No_Major\",\n                            \"Stream_Unknown\",\"Company_Pvt_Ltd\",\"Company_Funded_Startup\",\"Company_Public_Sector\", \"Company_Early_Stage_Startup\", \"Company_NGO\",\n                            \"Company_Other\", \"Company_Unknown\"]","6b7b604c":"test_normalized = normalize(df_test_final[cols_to_be_normalized])\ntest_boolean = df_test_final[cols_not_to_be_normalized]\ndf_test_normalized = pd.DataFrame(test_normalized,columns = cols_to_be_normalized)\ndf_test_boolean = pd.DataFrame(test_boolean,columns=cols_not_to_be_normalized)","47d87fa0":"df_train_final = df_train_normalized.merge(df_train_boolean,left_index=True, right_index=True)\ndf_test_final = df_test_normalized.merge(df_test_boolean,left_index=True, right_index=True)\ndf_test_final.index = np.arange(0,len(df_test_final))\ndf_test_final","efee767c":"df_test_final","8d266111":"X = df_train_final.drop(\"target\",axis = 1)\nY = df_train_final[\"target\"]","52474fc4":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=42,shuffle=True, stratify = Y)","091e267d":"logitsic_model = LogisticRegression()","84c9a7ed":"logitsic_model.fit(X_train,Y_train)","7d52f4df":"Y_pred = logitsic_model.predict(X_test)","2f3ff01f":"print(classification_report(Y_test,Y_pred))","4034b861":"confusion_matrix(Y_test,Y_pred)","259ba2f2":"print(mean_squared_error(Y_test,Y_pred))","d5d65735":"print (\"Accuracy : \", accuracy_score(Y_test, Y_pred)) ","cb4de423":"from sklearn.ensemble import GradientBoostingClassifier\n# Now we can try setting different learning rates, so that we can compare the performance of the classifier's \n#performance at different learning rates. Let's see what the performance was for different learning rates:\n\nlr_list = [0.005, 0.0075, 0.01, 0.025, 0.05,0.1,0.25,0.5,1,0.88,0.9,1]\n\nfor learning_rate in lr_list:\n    gb_clf = GradientBoostingClassifier(n_estimators=50, learning_rate=learning_rate, max_features=2, max_depth=2, random_state=0)\n    gb_clf.fit(X_train, Y_train)\n\n    print(\"Learning rate: \", learning_rate)\n    print(\"Accuracy score (training): {0:.3f}\".format(gb_clf.score(X_train, Y_train)))\n    print(\"Accuracy score (validation): {0:.3f}\".format(gb_clf.score(X_test, Y_test)))","24d821c1":"import warnings\nwarnings.filterwarnings(\"ignore\")\nXGBoost_pipe = imbPipe([\n    (\"XGBoost\", XGBClassifier(random_state=42,n_jobs=-1,tree_method=\"hist\"))\n])\n\nparams={\n    \"XGBoost__max_depth\": [20,21],\n    \"XGBoost__min_child_weight\":[22,23],\n    \"XGBoost__n_estimators\":[25,27],\n    \"XGBoost__subsample\":[0.4,0.5,0.6],\n    \"XGBoost__colsample_bytree\":[0.4,0.5,0.6],\n    \"XGBoost__gamma\":[1,2,3],\n    \n}\n\nXGBoost_grid = GridSearchCV(XGBoost_pipe, params, n_jobs=-1,cv=3,scoring=\"roc_auc\")\nXGBoost_grid.fit(X_train, Y_train)\nprint(\"Best Parameters for Model:  \",XGBoost_grid.best_params_)\nY_pred=XGBoost_grid.predict(X_train)\nprint(\"\\n\")\nprint(classification_report(Y_train, Y_pred))","fedbbacc":"Y_pred=XGBoost_grid.predict(X_test)  \nprint(classification_report(Y_test, Y_pred))","affb7bcf":"accuracy_score(Y_test,Y_pred)","64131d0e":"X_test = df_test_final.copy()","07d05b92":"# We predict Target values\nY_pred = XGBoost_grid.predict(X_test)  \nY_pred","18c440a0":"#Create Dataframe\nsubmission = pd.DataFrame(df_test_copy['enrollee_id'])\nsubmission[\"target\"] = Y_pred\n\nsubmission.head()","03f78b1c":"filename = 'submission.csv'\nsubmission.to_csv(filename,index=False)\n\n\nprint('Saved file: ' + filename)","18a5ffd6":"## XGBoost","151db3bd":"#### Convert DataFrame to a csv file that can be uploaded\n#### This is saved in the same directory as your notebook","f0c50495":"## Gradient Boosting Classifier","8a52cbf8":"# Visualization","5569085b":"# Models","843d51b7":"## Count :City Development Index","dc86450d":"## Data Upscalling","cce50653":"## Experience","6091df04":"## Logistic Regression","e638b7fb":"## Preprocessing","4910a3eb":"## Education Level","7c66471b":"## Bar plots\n### showing the frequency of each category separated by label","abe54831":"## Splitting Independent and Dependent variables","7719bbaf":"## Training Hours","d14e7553":"## City to City Development Index","2a9207bd":"## Predictions","e3955a2c":"## Education level to training hours","c223371c":"## Normalisation"}}