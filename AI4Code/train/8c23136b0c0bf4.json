{"cell_type":{"28920af7":"code","b9d5122e":"code","766bbc8d":"code","4cb05d60":"code","c8a8677e":"code","e0a9cb9b":"code","c6f97891":"code","a9a000da":"code","8a29f92f":"code","870dd22b":"code","115d45e3":"code","a4a180d1":"code","7ea1c926":"code","ce0be05f":"code","6764aa66":"code","7f22c5a7":"code","a0369f3d":"code","db1ebf66":"code","e566e797":"code","81f1d890":"code","d1d1f372":"code","6f406256":"code","c4f79cbe":"code","abffdae6":"code","9b53a66e":"code","cf2c66dd":"code","9f9727f6":"code","862c8b14":"code","ed541c73":"code","bd759bcd":"code","032f9ee8":"code","37673454":"code","080efa28":"code","60d04311":"code","18032c24":"code","3c0cb816":"code","b9d88a2f":"code","6cdd3b41":"code","e6fd682a":"code","927b3728":"code","6221515d":"code","015749fc":"code","116c24f5":"code","64147c4f":"code","96861d64":"code","22e7a560":"code","ce9097ce":"code","971f383b":"code","a83db4ef":"code","30bb50f4":"code","2bead418":"code","58c59cc8":"code","21215477":"code","d3fb0d63":"markdown","af30045b":"markdown","f5417557":"markdown","711fcca0":"markdown","d1ad2a01":"markdown","7a5e752c":"markdown","8313d0a3":"markdown","e7e9a143":"markdown","6d7baf0b":"markdown","53e46654":"markdown","0a425a2b":"markdown","7f1657e6":"markdown","0e8a9700":"markdown","4b070259":"markdown","b4abe436":"markdown","dd1eaf90":"markdown","57cea149":"markdown","f1c6fc82":"markdown","88f3e757":"markdown","2cfa9d40":"markdown","e3f19c7e":"markdown","7eef6d84":"markdown","db743f6f":"markdown","3409b99f":"markdown","a89f9045":"markdown","df65a31d":"markdown","c417f76b":"markdown","49cbed30":"markdown","7dc96058":"markdown","ed1b9a88":"markdown","f999711e":"markdown","b1523c4a":"markdown","b835cfec":"markdown","500f3fe2":"markdown","469f5d55":"markdown","041413fb":"markdown","de2cd11a":"markdown","a5d87f96":"markdown","28b6e47b":"markdown","4a96828c":"markdown","2881e2d4":"markdown","d8ccc451":"markdown","bbad3880":"markdown","134f40c1":"markdown","6f406f1f":"markdown","abf4d94d":"markdown","d123923d":"markdown","ee66e919":"markdown","20b502b0":"markdown","9f275efe":"markdown","2eff6fd2":"markdown","d44aa77d":"markdown","a4de8ad3":"markdown","bd98d714":"markdown","d4621556":"markdown","9981eee2":"markdown","178a89e4":"markdown","851e523c":"markdown","706852bc":"markdown","6f62cbb8":"markdown","ecd568b7":"markdown","5410ed85":"markdown","dd1bb637":"markdown","e2e01818":"markdown","1167d386":"markdown","28a6af56":"markdown","945ddef8":"markdown","e7ce0e98":"markdown","e84fe613":"markdown","66631638":"markdown","1b2c4bac":"markdown","a5c6d4b9":"markdown","99347a0d":"markdown","10d59163":"markdown","ac1b5a08":"markdown","97e8efc6":"markdown","ed516b06":"markdown","ba192406":"markdown","182196d1":"markdown","2c4729df":"markdown","342ff031":"markdown","8cac5113":"markdown","2eb7a46c":"markdown","ba129aab":"markdown","f60c5db9":"markdown","80064669":"markdown","5307ed72":"markdown","06f4e2f9":"markdown","b3ca54e4":"markdown","e0561f98":"markdown","72e40a0f":"markdown","2987e9c4":"markdown","2344fb60":"markdown","db94d4c1":"markdown"},"source":{"28920af7":"#for data and data visualization\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nfrom plotly import tools\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\n%matplotlib inline\n\n#Classification models\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifierCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier, VotingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n#Model validation and preprocessing\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split, cross_validate, ShuffleSplit, GridSearchCV, StratifiedShuffleSplit, PredefinedSplit\nfrom sklearn.metrics import classification_report, make_scorer, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\nfrom sklearn_pandas import DataFrameMapper, gen_features\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.decomposition import PCA\n\n#Deep learning\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ReduceLROnPlateau\n\n#Helpers\nimport re\nimport time\nfrom datetime import datetime\nfrom scipy.stats import boxcox\nfrom collections import Counter\nfrom sklearn_pandas import DataFrameMapper, gen_features\nfrom imblearn.over_sampling import SMOTE\nfrom IPython.display import clear_output\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = 'all'\n\n%config InlineBackend.figure_format = 'retina'","b9d5122e":"full = pd.read_csv('..\/input\/WA_Fn-UseC_-Telco-Customer-Churn.csv')\nfull.sample(5)","766bbc8d":"print('Number of customers: {}'.format(full.shape[0]))\nprint('Number of features: {}'.format(full.shape[1]))\ndata = full[full.notnull().all(axis=1)].iloc[0].T.to_frame().reset_index()\ndata.columns = ['Features','Example']\ndata['Missing values'] = full.isnull().sum().values\ndata['Number of unique values'] = full.nunique().values\ndata['Data Type']= full.dtypes.values\ndata","4cb05d60":"##Helper functions\ndef plot_pie(column):\n    churned = full.loc[full['Churn']=='Yes',column].copy()\n    notchurned = full.loc[full['Churn']=='No',column].copy()\n    pie1 = go.Pie(labels=churned.value_counts().index.tolist(),\n                  values=churned.value_counts().values.tolist(),\n                  hole=0.5,\n                  domain=dict(x=[0,0.48]),\n                  hoverinfo='label+value')\n    \n    pie2 = go.Pie(labels=notchurned.value_counts().index.tolist(),\n                  values=notchurned.value_counts().values.tolist(),\n                  hole=0.5,\n                  domain=dict(x=[0.52,1]), \n                  hoverinfo='label+value')\n    \n    layout = go.Layout(title='Distribution of {}'.format(column),\n                       annotations=[dict(text='Churn',\n                                                font=dict(size = 13),\n                                                showarrow=False,\n                                                x=0.21, y=0.5),\n                                           dict(text='Did not churn',\n                                                font=dict(size=13),\n                                                showarrow=False,\n                                                x=0.84,y =0.5\n                                               )\n                                          ])\n    data = [pie1,pie2]\n    fig = go.Figure(data=data,layout=layout)\n    py.iplot(fig)\n    \ndef plot_dist(column):\n    churned = full.loc[full['Churn']=='Yes',column].values.tolist()\n    notchurned = full.loc[full['Churn']=='No',column].values.tolist()\n    \n    histdata = [churned,notchurned]\n    group_labels = ['Churn','Did not churn']\n    fig = ff.create_distplot(histdata,\n                             group_labels,\n                             show_hist=False,\n                             show_rug=False)\n    fig['layout'].update(title='Distribution of {}'.format(column),\n                         yaxis=dict(title='Distribution probability'),\n                         xaxis=dict(showgrid=False,title=column))\n    py.iplot(fig)","c8a8677e":"labels = full['Churn'].value_counts().index.tolist()\nvalues = full['Churn'].value_counts().values.tolist()\npie = go.Pie(labels=labels,values=values,opacity=0.9)\nlayout = go.Layout(title='Survival rate',\n                   autosize=False)\nfig = go.Figure(data=[pie],layout=layout)\npy.iplot(fig)","e0a9cb9b":"plot_pie('gender')","c6f97891":"plot_pie('SeniorCitizen')","a9a000da":"plot_pie('Partner')","8a29f92f":"plot_pie('Dependents')","870dd22b":"plot_dist('tenure')","115d45e3":"plot_pie('PhoneService')","a4a180d1":"plot_pie('MultipleLines')","7ea1c926":"plot_pie('InternetService')","ce0be05f":"plot_pie('OnlineSecurity')","6764aa66":"plot_pie('OnlineBackup')","7f22c5a7":"plot_pie('DeviceProtection')","a0369f3d":"plot_pie('TechSupport')","db1ebf66":"plot_pie('StreamingTV')","e566e797":"plot_pie('StreamingMovies')","81f1d890":"plot_pie('Contract')","d1d1f372":"plot_pie('PaperlessBilling')","6f406256":"plot_pie('PaymentMethod')","c4f79cbe":"plot_dist('MonthlyCharges')","abffdae6":"full[pd.to_numeric(full['TotalCharges'], errors='coerce').isnull()]","9b53a66e":"full = full[pd.to_numeric(full['TotalCharges'], errors='coerce').notnull()]\nprint('Number of customers: {}'.format(full.shape[0]))\nprint('Number of features (including target variable): {}'.format(full.shape[1]))\nfull.sample(5)","cf2c66dd":"full['ratio'] = (full['TotalCharges'].astype(float).div(full['tenure'])).div(full['MonthlyCharges'])\nplot_dist('ratio')","9f9727f6":"full['TotalServices'] = (full[['OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies']]=='Yes').sum(axis=1)\npd.crosstab(index=full['Churn'],columns=full['TotalServices'],normalize='columns')","862c8b14":"full.sample(5)","ed541c73":"#Seperating the independent and dependent variable\nx = full.drop('Churn',axis=1)\ny = full['Churn']\n\n#Splitting to train test set\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,stratify=y,random_state=0)\n\n#Check the split\ntrace1 = go.Table(header=dict(values=[['<b>TRAIN SET<\/b>'],['<b>COUNTS<\/b>'],['<b>PERCENTAGE<\/b>']],\n                             fill = dict(color='#C2D4FF'),\n                             align = ['left','center']),\n                 cells=dict(values=[y_train.value_counts().index.tolist(),\n                                    y_train.value_counts().values.tolist(),\n                                    np.round(y_train.value_counts(normalize=True).values * 100,2)],\n                            align = ['left', 'center'],\n                            fill = dict(color='#EDFAFF')),\n                domain=dict(x=[0,0.48]))\n\ntrace2 = go.Table(header=dict(values=[['<b>TEST SET<\/b>'],['<b>COUNTS<\/b>'],['<b>PERCENTAGE<\/b>']],\n                             fill = dict(color='#C2D4FF'),\n                             align = ['left','center']),\n                 cells=dict(values=[y_test.value_counts().index.tolist(),\n                                    y_test.value_counts().values.tolist(),\n                                    np.round(y_test.value_counts(normalize=True).values * 100,2)],\n                            align = ['left', 'center'],\n                            fill = dict(color='#EDFAFF')),\n                 domain=dict(x=[0.52,1]))\nlayout = dict(width=800, height=300)\nfig = dict(data=[trace1,trace2],layout=layout)\npy.iplot(fig)","bd759bcd":"from sklearn.exceptions import DataConversionWarning\nimport warnings\nwarnings.filterwarnings(action='ignore', category=DataConversionWarning)\n\n#Map churn to integers\ny_train = y_train.map({'Yes':1,'No':0})\ny_test = y_test.map({'Yes':1,'No':0})\n\n#Dropping customerID as it is just a unique identifier for the customers in the dataset\nx_train.drop('customerID',axis=1,inplace=True)\nx_test.drop('customerID',axis=1,inplace=True)\n\n#Dropping PhoneService as MultipleLines already provides that information\nx_train.drop('PhoneService',axis=1,inplace=True)\nx_test.drop('PhoneService',axis=1,inplace=True)\n\n#Replacing 'No internet service' to 'no'\n#extra_col = ['OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies']\n#x_train[extra_col] = x_train[extra_col].replace('No internet service','No')\n#x_test[extra_col] = x_test[extra_col].replace('No internet service','No')\n\n#Convert binary features to 1 and 0 (avoids unnecessary increase in dimensionality) \nbi_col = ['gender','Partner','Dependents','PaperlessBilling']\nfeature_LE = gen_features([[i,] for i in bi_col],[LabelEncoder])\nmapper_LE = DataFrameMapper(feature_LE,df_out=True,default=None)\nx_train = mapper_LE.fit_transform(x_train)\nx_test = mapper_LE.transform(x_test)\n\n#One hot encode categorical columns with more than 2 categories\nmulti_col = ['MultipleLines','InternetService','Contract','PaymentMethod','OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies']\nx_train = pd.get_dummies(x_train,columns=multi_col)\nx_test = pd.get_dummies(x_test,columns=multi_col)\n\n#Drop 'No internet service' as the information is duplicated many times\nextra_col = ['OnlineSecurity_No internet service','OnlineBackup_No internet service','DeviceProtection_No internet service','TechSupport_No internet service','StreamingTV_No internet service','StreamingMovies_No internet service']\nx_train.drop(extra_col,axis=1,inplace=True)\nx_test.drop(extra_col,axis=1,inplace=True)\n\n#StandardScalar numerical columns \nnum_col = ['tenure','MonthlyCharges','TotalCharges','TotalServices']\nfeature_SS = gen_features([[i] for i in num_col],[StandardScaler])\nmapper_SS = DataFrameMapper(feature_SS,df_out=True,default=None)\nx_train = mapper_SS.fit_transform(x_train)\nx_test = mapper_SS.transform(x_test)\n\n#Convert dataframe to numeric instead of objects to save memory space\nx_train = x_train.apply(pd.to_numeric)\nx_test = x_test.apply(pd.to_numeric)\n\n#View preprocessed data\ntemp = x_train.head()\ntemp['Churn'] = y_train.head()\nprint('Number of features (including target variable): {}'.format(temp.shape[1]))\ntemp","032f9ee8":"model = RandomForestClassifier(n_estimators=1000,random_state=0)\nmodel = model.fit(x_train, y_train)\nfeatures = pd.DataFrame(index=x_train.columns)\nfeatures['Importance'] = model.feature_importances_\nfeatures = features.sort_values('Importance',ascending=False)\n#Visualing feature importance\ndata= [go.Bar(\n    x=features.values.flatten()[::-1],\n    y=features.index[::-1],\n    orientation='h',\n    opacity=0.8)]\n\nlayout = go.Layout(title='Feature Importance',\n                   autosize=True,\n                   xaxis=dict(title='Features',tickangle=0,fixedrange=True),\n                   yaxis=dict(fixedrange=True,tickangle=0,tickfont=dict(size=6)),\n                   margin=dict(l=200,t=0))\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig)","37673454":"#Select top k features\ntop_k = 20\nselected_features = features.index.tolist()[:top_k]\nx_train_reduced = x_train[selected_features]\nx_test_reduced = x_test[selected_features]\nx_train_reduced.head()","080efa28":"pca = PCA(n_components = None)\npca.fit(x_train_reduced)\nexplained_variance = pca.explained_variance_ratio_\n\n#Bar plot\ntrace1 = go.Bar(x=[i for i in range(1,21)],\n                y=explained_variance)\n\n#Graph\ntrace2 = go.Scatter(x = [i for i in range(1,21)],\n                    y = np.cumsum(explained_variance),\n                    mode = 'lines+markers'\n)\n\nfig = tools.make_subplots(rows=1, cols=2, subplot_titles=('Variation','Cumulated Variation'))\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 1, 2)\nfig['layout']['xaxis1'].update(dtick=1)\nfig['layout']['xaxis2'].update(dtick=1,showgrid=False)\nfig['layout']['yaxis2'].update(dtick=0.05)\nfig['layout'].update(showlegend=False)\nclear_output(wait=True)\npy.iplot(fig)","60d04311":"n_components=10\npca = PCA(n_components=n_components)\ncolnames = ['PC{}'.format(i+1) for i in range(n_components)]\nx_train_pca = pd.DataFrame(pca.fit_transform(x_train_reduced, y_train),index=x_train.index,columns=colnames)\nx_test_pca = pd.DataFrame(pca.transform(x_test_reduced),index=x_test.index,columns=colnames)\nx_train_pca.head()","18032c24":"sm = SMOTE(random_state=0,ratio=0.5)\nx_train_sm, y_train_sm = sm.fit_sample(x_train_pca, y_train)\nx_train_sm = pd.DataFrame(x_train_sm,columns=x_train_pca.columns)\ny_train_sm = pd.DataFrame(y_train_sm)","3c0cb816":"counter = Counter(y_train_sm.values.flatten())\nprint('Number of non churners: {}\\nNumber of churners: {}'.format(counter[0],counter[1]))","b9d88a2f":"classifiers = {'LogReg': LogisticRegression(),\n               'RidgeClassifier': RidgeClassifierCV(),\n               'KNN': KNeighborsClassifier(),\n               'SVC': SVC(gamma='auto'),\n               'GaussianNB': GaussianNB(priors=[0.27, 0.73]),\n               'DecisionTree': DecisionTreeClassifier(),\n               'RandomForest': RandomForestClassifier(n_estimators=100),\n               'AdaBoost': AdaBoostClassifier(n_estimators=100),\n               'GradientBoosting': GradientBoostingClassifier(n_estimators=100),\n               'ExtraTrees': ExtraTreesClassifier(n_estimators=100),\n               'BaggingClassifier': BaggingClassifier(n_estimators=100),\n               'XGB': XGBClassifier(),\n               'LDA': LinearDiscriminantAnalysis(),\n               'LGB': LGBMClassifier()}","6cdd3b41":"scoring = {'accuracy' : make_scorer(accuracy_score), \n           'f1_score' : make_scorer(f1_score),\n           'recall' : make_scorer(recall_score),\n           'precision' : make_scorer(precision_score),\n           'AUC' : make_scorer(roc_auc_score)}\n\ncv_split = StratifiedShuffleSplit(n_splits=10, test_size=0.2, train_size=0.8, random_state=0)\nselection_cols = ['Classifier','Mean Train Accuracy','Mean Test Accuracy','Mean F1 train','Mean F1 Test','Mean Train Recall','Mean Test Recall','Mean Train Precision','Mean Test Precision','Mean Train AUC','Mean Test AUC']\n\nclassifiers_summary = pd.DataFrame(columns=selection_cols)\nstart_time = time.time()\n\nfor name,classifier in classifiers.items():\n    print('Validating ',name)\n    clear_output(wait=True)\n    cv = cross_validate(classifier,x_train_sm,y_train_sm,return_train_score=True,cv=cv_split,scoring=scoring)\n    cv_calc = [name,\n               cv['train_accuracy'].mean(),\n               cv['test_accuracy'].mean(),\n               cv['train_f1_score'].mean(),\n               cv['test_f1_score'].mean(),\n               cv['train_recall'].mean(),\n               cv['test_recall'].mean(),\n               cv['train_precision'].mean(),\n               cv['test_precision'].mean(),\n               cv['train_AUC'].mean(),\n               cv['test_AUC'].mean(),\n              ]\n    cv_calc_s = pd.Series(cv_calc,index=selection_cols)\n    classifiers_summary = classifiers_summary.append(cv_calc_s,ignore_index=True)\n  \nclear_output(wait=True)\nrun_min,run_sec = divmod(time.time()-start_time,60)\nprint('Total validating time: {:.0f}min {:.0f}sec'.format(run_min,run_sec))","e6fd682a":"classifiers_summary = classifiers_summary.sort_values('Mean F1 Test',ascending=False)\nclassifiers_summary_styled = classifiers_summary[selection_cols].style.highlight_max(axis=0).set_properties(**{'width': '150px'})\nclassifiers_summary_styled","927b3728":"classifiers = {'LogReg': LogisticRegression(),\n               'RidgeClassifier': RidgeClassifierCV(),\n               'KNN': KNeighborsClassifier(),\n               'SVC': SVC(gamma='auto'),\n               'GaussianNB': GaussianNB(priors=[0.27,0.73]),\n               'DecisionTree': DecisionTreeClassifier(),\n               'RandomForest': RandomForestClassifier(n_estimators=100),\n               'AdaBoost': AdaBoostClassifier(n_estimators=100),\n               'GradientBoosting': GradientBoostingClassifier(n_estimators=100),\n               'ExtraTrees': ExtraTreesClassifier(n_estimators=100),\n               'BaggingClassifier': BaggingClassifier(n_estimators=100),\n               'XGB': XGBClassifier(),\n               'LDA': LinearDiscriminantAnalysis(),\n               'LGB': LGBMClassifier()}\n\nselection_cols = ['Classifier',\n                  'Accuracy',\n                  'F1',\n                  'Precision',\n                  'Recall',\n                  'AUC']\n\nclassifiers_summary = pd.DataFrame(columns=selection_cols)\npredictions = pd.DataFrame({'Actual':y_test})\nstart_time = time.time()\n\nfor name,classifier in classifiers.items():\n    print('Validating ',name)\n    clear_output(wait=True)\n    classifier.fit(x_train_sm, y_train_sm)\n    pred = classifier.predict(x_test_pca)\n    cv_calc = [name,\n               accuracy_score(y_test,pred),\n               f1_score(y_test,pred),\n               precision_score(y_test,pred),\n               recall_score(y_test,pred),\n               roc_auc_score(y_test,pred)\n              ]\n    predictions[name] = pred\n    cv_calc_s = pd.Series(cv_calc,index=selection_cols)\n    classifiers_summary = classifiers_summary.append(cv_calc_s,ignore_index=True)\n \nclear_output(wait=True)\nrun_min,run_sec = divmod(time.time()-start_time,60)\nprint('Total validating time: {:.0f}min {:.0f}sec'.format(run_min,run_sec))","6221515d":"classifiers_summary = classifiers_summary.sort_values('F1',ascending=False)\n\nclassifiers_summary_styled = classifiers_summary[selection_cols].style.highlight_max(axis=0).set_properties(**{'width': '150px'})\nclassifiers_summary_styled\n\n#Comparison visualization\ntrace1 = go.Bar(x=classifiers_summary['Accuracy'].values.tolist()[::-1],\n                y=classifiers_summary['Classifier'].values.tolist()[::-1],\n                name='Accuracy',\n                marker=dict(color='red'),\n                orientation='h',\n                opacity=0.7)\n    \ntrace2 = go.Bar(x=classifiers_summary['F1'].values.tolist()[::-1],\n                y=classifiers_summary['Classifier'].values.tolist()[::-1],\n                name='F1',\n                marker=dict(color='blue'),\n                orientation='h',\n                opacity=0.6)\n\ntrace3 = go.Bar(x=classifiers_summary['Precision'].values.tolist()[::-1],\n                y=classifiers_summary['Classifier'].values.tolist()[::-1],\n                name='Precision',\n                marker=dict(color='green'),\n                orientation='h',\n                opacity=0.6)\n\ntrace4 = go.Bar(x=classifiers_summary['Recall'].values.tolist()[::-1],\n                y=classifiers_summary['Classifier'].values.tolist()[::-1],\n                name='Recall',\n                marker=dict(color='blueviolet'),\n                orientation='h',\n                opacity=0.6)\n\ntrace5 = go.Bar(x=classifiers_summary['AUC'].values.tolist()[::-1],\n                y=classifiers_summary['Classifier'].values.tolist()[::-1],\n                name='AUC',\n                marker=dict(color='gray'),\n                orientation='h',\n                opacity=0.6)\n\ndata = [trace1,trace2,trace3,trace4,trace5]\nlayout = go.Layout(title='Scoring of classifiers',\n                   autosize=True,\n                   xaxis=dict(title='Scores',tickangle=0,fixedrange=True,range=[0,0.9],dtick=0.05),\n                   yaxis=dict(fixedrange=True,tickangle=-30))\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig)","015749fc":"fig = plt.figure(figsize=(30,17.5))\nsns.set(font_scale=1.5)\nfor i,c in enumerate(classifiers_summary['Classifier'][::-1]):\n    plt.subplot(3,5,i+1)\n    sns.heatmap(pd.crosstab(y_test,predictions[c]).values,\n                annot=True,\n                fmt='d',\n                cbar=False,\n                cmap='jet',\n                xticklabels=['No churn','Churn'],\n                yticklabels=['No churn','Churn'],\n                linewidth=2\n               )\n    plt.title(c,fontdict=dict(fontweight='bold'))\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    clear_output(wait=True)\nplt.tight_layout()","116c24f5":"#Without feature engineering\ntemp = XGBClassifier()\ntemp.fit(x_train.drop(['TotalServices','ratio'],axis=1),y_train)\ncm_no_fe = pd.crosstab(y_test,temp.predict(x_test.drop(['TotalServices','ratio'],axis=1)))\n\n#No dimension reduction\ntemp = XGBClassifier()\ntemp.fit(x_train,y_train)\ncm_no_dr = pd.crosstab(y_test,temp.predict(x_test))\n\n#Just top 20 features\ntemp = XGBClassifier()\ntemp.fit(x_train_reduced,y_train)\ncm_no_pca = pd.crosstab(y_test,temp.predict(x_test_reduced))\n\n#Without upsampling\ntemp = XGBClassifier()\ntemp.fit(x_train_pca,y_train)\ncm_no_us = pd.crosstab(y_test,temp.predict(x_test_pca))\n\nclear_output()","64147c4f":"from matplotlib.gridspec import GridSpec\nfig=plt.figure(figsize=(10,10))\ngs=GridSpec(4,4) \n\n#Base model\nax1=fig.add_subplot(gs[0:4,0:3]) \nsns.set(font_scale=2)\nsns.heatmap(pd.crosstab(predictions['Actual'],predictions[c]).values,\n                annot=True,\n                fmt='d',\n                cbar=False,\n                cmap='jet',\n                xticklabels=['No churn','Churn'],\n                yticklabels=['No churn','Churn'],\n                linewidth=2\n               )\nax1.title.set_text('Base model')\nax1.title.set_fontweight('bold')\n\n#No feature engineering\nsns.set(font_scale=1)\nax2=fig.add_subplot(gs[0,3]) \nsns.heatmap(cm_no_fe.values,\n                annot=True,\n                fmt='d',\n                cbar=False,\n                cmap='jet',\n                xticklabels=['No churn','Churn'],\n                yticklabels=['No churn','Churn'],\n                linewidth=2\n               )\nax2.title.set_text('No feature engineering')\nax2.title.set_fontweight('bold')\n\n#No dimension reduction\nax3=fig.add_subplot(gs[1,3])\nsns.heatmap(cm_no_dr.values,\n                annot=True,\n                fmt='d',\n                cbar=False,\n                cmap='jet',\n                xticklabels=['No churn','Churn'],\n                yticklabels=['No churn','Churn'],\n                linewidth=2\n               )\nax3.title.set_text('No dimension reduction')\nax3.title.set_fontweight('bold')\n\n#No PCA\nax4=fig.add_subplot(gs[2,3]) \nsns.heatmap(cm_no_pca.values,\n                annot=True,\n                fmt='d',\n                cbar=False,\n                cmap='jet',\n                xticklabels=['No churn','Churn'],\n                yticklabels=['No churn','Churn'],\n                linewidth=2\n               )\nax4.title.set_text('No PCA')\nax4.title.set_fontweight('bold')\n\n#No upsampling\nax5=fig.add_subplot(gs[3,3]) \nsns.heatmap(cm_no_us.values,\n                annot=True,\n                fmt='d',\n                cbar=False,\n                cmap='jet',\n                xticklabels=['No churn','Churn'],\n                yticklabels=['No churn','Churn'],\n                linewidth=2\n               )\nax5.title.set_text('No Upsampling')\nax5.title.set_fontweight('bold')\nclear_output(wait=True)\nfig.tight_layout()","96861d64":"#Upsampling without dimension reduction\nsm = SMOTE(random_state=0,ratio=0.5)\nx_train_sm2, y_train_sm2 = sm.fit_sample(x_train, y_train)\nx_train_sm2 = pd.DataFrame(x_train_sm2,columns=x_train.columns)\ny_train_sm2 = pd.DataFrame(y_train_sm2)\n\n#Fit XGB and predict\ntemp = XGBClassifier()\ntemp.fit(x_train_sm2,y_train_sm2)\nsns.heatmap(pd.crosstab(y_test,temp.predict(x_test)).values,\n                annot=True,\n                fmt='d',\n                cbar=False,\n                cmap='jet',\n                xticklabels=['No churn','Churn'],\n                yticklabels=['No churn','Churn'],\n                linewidth=2\n               )\nclear_output(wait=True)","22e7a560":"temp = XGBClassifier()\ntemp.fit(x_train_pca,y_train)\nconfidence_level = 0.432\npreds = np.where(temp.predict_proba(x_test_pca)[:,1]>confidence_level,1,0)\nsns.heatmap(pd.crosstab(y_test,preds).values,\n            annot=True,\n            fmt='d',\n            cbar=False,\n            cmap='jet',\n            xticklabels=['No churn','Churn'],\n            yticklabels=['No churn','Churn'],\n            linewidth=2\n           )\nplt.title('Confidence level: {}'.format(confidence_level),fontdict=dict(fontweight='bold'))\nclear_output(wait=True)","ce9097ce":"%%capture\nfrom catboost import CatBoostClassifier\n#Reset data\nfull = pd.read_csv('..\/input\/WA_Fn-UseC_-Telco-Customer-Churn.csv')\n\n#Remove blank totalcharges\nfull = full[full['TotalCharges']!=' ']\n#full['TotalCharges'] = full['TotalCharges'].astype(float)\nfull = full.apply(pd.to_numeric, errors='ignore')\n\n#Feature engineering\nfull['ratio'] = (full['TotalCharges'].astype(float).div(full['tenure'])).div(full['MonthlyCharges'])\nfull['TotalServices'] = (full[['OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies']]=='Yes').sum(axis=1)\n\n#Split to train test set\nx = full.drop(['customerID','Churn'],axis=1)\ny = full['Churn'].map({'Yes':1,'No':0})\nx_train_cb,x_test_cb,y_train_cb,y_test_cb = train_test_split(x,y,test_size=0.2,stratify=y,random_state=0)\n\n#Train catboost classifier\ncat_features =  [i for i,e in enumerate(x_train_cb.columns) if not np.issubdtype(x_train_cb[e].dtype , np.number)]\nmodel=CatBoostClassifier(iterations=200, depth=2, learning_rate=0.5,eval_metric='F1',random_seed=0)\nmodel.fit(x_train_cb,\n          y_train_cb,\n          cat_features=cat_features,\n          eval_set=(x_test_cb, y_test_cb),\n          early_stopping_rounds=100,\n          #plot=True,\n          verbose = 50,\n          use_best_model=True)","971f383b":"fig=plt.figure(figsize=(10,6))\ngs=GridSpec(1,2) \n\n#0.5\nax1=fig.add_subplot(gs[0,0]) \ncat_pred = model.predict(x_test_cb)\nsns.heatmap(pd.crosstab(y_test_cb, cat_pred).values,\n            annot=True,\n            fmt='d',\n            cbar=False,\n            cmap='jet',\n            xticklabels=['No churn','Churn'],\n            yticklabels=['No churn','Churn'],\n            linewidth=2\n           )\nax1.title.set_text('0.5')\nax1.title.set_fontweight('bold')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\n\n#Changing probability\nax2=fig.add_subplot(gs[0,1])\nconfidence_level = 0.433\ncat_pred2 = np.where(model.predict_proba(x_test_cb)[:,1]>confidence_level,1,0)\nsns.heatmap(pd.crosstab(y_test_cb, cat_pred2).values,\n            annot=True,\n            fmt='d',\n            cbar=False,\n            cmap='jet',\n            xticklabels=['No churn','Churn'],\n            yticklabels=['No churn','Churn'],\n            linewidth=2\n           )\nax2.title.set_text(confidence_level)\nax2.title.set_fontweight('bold')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nclear_output(wait=True)\nplt.tight_layout()","a83db4ef":"from keras import backend as K\n\ndef f1(y_true, y_pred):\n    def recall(y_true, y_pred):\n        \"\"\"Recall metric.\n\n        Only computes a batch-wise average of recall.\n\n        Computes the recall, a metric for multi-label classification of\n        how many relevant items are selected.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives \/ (possible_positives + K.epsilon())\n        return recall\n\n    def precision(y_true, y_pred):\n        \"\"\"Precision metric.\n\n        Only computes a batch-wise average of precision.\n\n        Computes the precision, a metric for multi-label classification of\n        how many selected items are relevant.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives \/ (predicted_positives + K.epsilon())\n        return precision\n    precision = precision(y_true, y_pred)\n    recall = recall(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))\n\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_f1', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)","30bb50f4":"ann = Sequential()\nann.add(Dense(units=15, kernel_initializer='RandomUniform', activation='relu', input_dim=10))\nann.add(Dense(units=10, kernel_initializer='RandomUniform', activation='relu'))\nann.add(Dense(units=1, kernel_initializer='RandomUniform', activation='sigmoid'))\nann.compile(optimizer=Adam(lr=.01), loss='binary_crossentropy', metrics=[f1])\nann.summary()","2bead418":"epochs = 30\ncallback = ann.fit(x_train_sm,y_train_sm,validation_data=(x_test_pca,y_test),epochs=epochs,batch_size=48,verbose=2,callbacks=[learning_rate_reduction])","58c59cc8":"#Loss\ntrace1 = go.Scatter(x=[i for i in range(1,epochs+1)],\n                    y=callback.history['val_loss'],\n                    marker = dict(color='red'),\n                    mode = 'lines+markers',\n                    name = 'Validation'\n                   )\n\ntrace2 = go.Scatter(x=[i for i in range(1,epochs+1)],\n                    y=callback.history['loss'],\n                    marker = dict(color='blue'),\n                    mode = 'lines+markers',\n                    name = 'Train'\n                   )\n#F1\ntrace3 = go.Scatter(x = [i for i in range(1,epochs+1)],\n                    y = callback.history['val_f1'],\n                    marker = dict(color='red'),\n                    mode = 'lines+markers',\n                    name = 'Validation'\n                   )\n\ntrace4 = go.Scatter(x = [i for i in range(1,epochs+1)],\n                    y = callback.history['f1'],\n                    marker = dict(color='blue'),\n                    mode = 'lines+markers',\n                    name = 'Train'\n                   )\n\nfig = tools.make_subplots(rows=1, cols=2, subplot_titles=('Loss','F1 Score'))\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 1, 1)\nfig.append_trace(trace3, 1, 2)\nfig.append_trace(trace4, 1, 2)\nfig['layout']['xaxis1'].update(dtick=1,showgrid=False,tickangle=0)\nfig['layout']['xaxis2'].update(dtick=1,showgrid=False,tickangle=0)\nfig['layout']['yaxis2'].update(dtick=0.05)\nfig['layout'].update(showlegend=True)\nclear_output(wait=True)\npy.iplot(fig)","21215477":"ann_pred = ann.predict(x_test_pca).flatten()\nann_pred = (ann_pred>0.5).astype(int)\nsns.heatmap(pd.crosstab(y_test, ann_pred).values,\n            annot=True,\n            fmt='d',\n            cbar=False,\n            cmap='jet',\n            xticklabels=['No churn','Churn'],\n            yticklabels=['No churn','Churn'],\n            linewidth=2\n           )\nplt.title('ANN',fontdict=dict(fontweight='bold'))\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nclear_output(wait=True)","d3fb0d63":"# <a id='7'>7. Principal Component analysis (Extraction)<\/a>","af30045b":"Customers without partners look more likely to churn than those with a partner.","f5417557":"# <a id='4'>4. Split into train and test set<\/a>","711fcca0":"While the result without upsampling was slightly better without upsampling, it was the other way around with upsampling.","d1ad2a01":"It can be observed that the more services the customers subscribed to, the less likely they will churn. The customers with 0 services does not seem to follow the trend but that could be due to the segment of customers including those without internet service.","7a5e752c":"The distribution looks very similar and might not be too useful. However, we will let the model decide whether it is useful.","8313d0a3":"# <a id='10'>10. Classifiers results<\/a>","e7e9a143":"The logic behind this is that a possible reason for customer churning is due to the increase in charges. ","6d7baf0b":"To validate the models, we will split the dataset provided to train and test set. As the distribution of the target variable is quite skewed, the spliting done would try to ensure similar proportion in both sets. The split for this would be 80% train set and 20% test set.","53e46654":"## <a id='2.1'>2.1 Target variable - Churn<\/a>","0a425a2b":"Those customers without dependents appears to be more like to churn those with dependents.","7f1657e6":"Instead of upsampling, we can reduce, or increase depending on which values you are looking at, the probability to determine whether the customer churns or not. By default, for binary classification like this dataset, the magic number is 0.5, we can reduce the number to get a better result.","0e8a9700":"Customers with electronic check as payment method looks more likely to churn.","4b070259":"Nothing out of the ordinary here except for the data type for TotalCharges, which should be float64. We will take a look into it later.","b4abe436":"Having 10 components explain 90% of the total variance, which would be a good choice.","dd1eaf90":"It does seem that the data manipulation does provide better information for the model, however the model performs better without dimension reduction. In that case lets try upsampling without dimension reduction.","57cea149":"Companies providing services such as telecommunication, internet, TV  or insurances often have to deal with customer attrition, also known as customer churn. Customer churning could provide a significant loss in income for the companies as firstly, the company would no longer be able to generate income from the customer and the cost of attracting the customer in the first place would be wasted. Secondly, a loss in market share would not reflect well for the companies. ","f1c6fc82":"# <a id='13'>13. Appendix 2 (An alternative)<\/a>","88f3e757":"## Import modules","2cfa9d40":"Looking at the table (sorted descending by F1 test score), randomforest looks to be the best classifiers. However, looking at the train scores, it seems that there could be a possibility of overfitting. Using that train of thought, LDA, RidgeClassifier, GradientBoosting, XGB and LogReg looks to be the top 5 models. To make sure, we can take a look at the scores on the test set. ","e3f19c7e":"With all the data manipulation done, we can proceed to the fun part, fitting the data to some classifier models. The models choosen are some of the more common ones like LogisticRegression, SVC,  XGB, LGB.","7eef6d84":"## <a id='2.16'>2.16 Contract<\/a>","db743f6f":"Lets take a look at how the data manipulation improved the scores at each step. We would just use the default XGBoost classifier to test out the changes.","3409b99f":"## <a id='6.1'>6.1 Reducing model<\/a>","a89f9045":"From the above pie graphs, it appears that senior citizens are more likely to churn.","df65a31d":"For both churning and non churning customers, the distribution of gender seems fairly similar. This suggest gender would be a good predictive indicator for churning.","c417f76b":"The plots suggests that those customers without streamingTV are slightly more likely to churn.","49cbed30":"## <a id='2.8'>2.8 MultipleLines<\/a>","7dc96058":"# <a id='14'>14. Appendix 3 (CatBoostClassifier)<\/a>","ed1b9a88":"# Introduction","f999711e":"As most models available on sklearn requires the features to be numbers, we would have to do some preprocessing in order for them to work. In order to make sure that there is no data leakage, we will fit the transformation on the training set first before transforming the test set.","b1523c4a":"It appears that the CatBoost classifier performs almost as well as our choosen classifier, XGB, with only an additional 5 false positives and the same number of true positives. (Catboost classifier was manually optimized as much as possible and changing the confidence required to predict was a way around upsampling. Upsampling using SMOTE was troublesome for CatBoost as SMOTE required numerical data, which defeats the purpose of showing CatBoost.) Looking at the length of the codes, maybe trading 5 extra errors would be worth it to some.","b835cfec":"The ANN performs decently but still loses out to the other top performers. ","500f3fe2":"## <a id='2.18'>2.18 PaymentMethod<\/a>","469f5d55":"From the plots, customers without device protection looks very likely to churn.","041413fb":"To determine which is the best model, we would do a cross validation and look at the F1 scores. The splitting done would be 80-20 using stratifiedshufflesplit by sklearn.","de2cd11a":"From the plots, customers without technical support looks very likely to churn.","a5d87f96":"# <a id='12'>12. Appendix 1 (Step by step analysis)<\/a>","28b6e47b":"## <a id='2.3'>2.3 Senior citizen<\/a>","4a96828c":"# <a id='15'>15. Appendix 4 (ANN)<\/a>","2881e2d4":"## <a id='2.10'>2.10 OnlineSecurity<\/a>","d8ccc451":"Based on the exploratory analysis, we could tell that those customers with additional services are less likely to churn, so we shall just sum the number of extra services the customer signed up for and make it a new feature.","bbad3880":"As expected, random forest has overfitted on the training set. XGB looks to be the best based on F1 score, however there are models such as LDA that predicts more churners accurately. In the end, you have to choose between and balance of recall and precision to make your choice.","134f40c1":"Customers with paperlessbilling looks slightly more likely to churn than those without.","6f406f1f":"## <a id='2.6'>2.6 Tenure<\/a>","abf4d94d":"## <a id='2.7'>2.7 PhoneService<\/a>","d123923d":"## Training the ANN","ee66e919":"# <a id='16'>16. Conclusion<\/a>","20b502b0":"It seems that for those new customers (tenure being 0), the data assigns an empty string for total charges. Logically, we can assigned the total charges to be 0 for this group of customers. However, taking a look at the target variable, none of these customers churned, which makes sense as new customers would technically be unable to churn. Hence, it is better to drop these customers from the dataset for training purposes, as it is trivial to predict churning for new customers. ","9f275efe":"- <a href='#1'>1. Import and view dataset<\/a>\n    - <a href='#1.1'>1.1. Data overview<\/a>\n- <a href='#2'>2. Exploratory analysis<\/a>\n    - <a href='#2.1'>2.1 Target variable - Churn<\/a>\n    - <a href='#2.2'>2.2 Gender<\/a>\n    - <a href='#2.3'>2.3 Senior citizen<\/a>\n    - <a href='#2.4'>2.4 Partner<\/a>\n    - <a href='#2.5'>2.5 Dependents<\/a>\n    - <a href='#3.6'>2.6 Tenure<\/a>\n    - <a href='#2.7'>2.7 PhoneService<\/a>\n    - <a href='#2.8'>2.8 MultipleLines<\/a>\n    - <a href='#2.9'>2.9 InternetService<\/a>\n    - <a href='#2.10'>2.10 OnlineSecurity<\/a>\n    - <a href='#2.11'>2.11 OnlineBackup<\/a>\n    - <a href='#2.12'>2.12 DeviceProtection<\/a>\n    - <a href='#2.13'>2.13 TechSupport<\/a>\n    - <a href='#2.14'>2.14 StreamingTV<\/a>\n    - <a href='#2.15'>2.15 StreamingMovies<\/a>\n    - <a href='#2.16'>2.16 Contract<\/a>\n    - <a href='#2.17'>2.17 PaperlessBilling<\/a>\n    - <a href='#2.18'>2.18 PaymentMethod<\/a>\n    - <a href='#2.19'>2.19 MonthlyCharges<\/a>\n    - <a href='#2.20'>2.20 TotalCharges<\/a>\n- <a href='#3'>3. Feature engineering<\/a>\n    - <a href='#3.1'>3.1 Ratio of past monthly charges to current month charge<\/a>\n    - <a href='#3.2'>3.2 Count of extra services<\/a>\n- <a href='#4'>4. Split into train and test set<\/a>\n- <a href='#5'>5. Data Preprocessing<\/a>\n- <a href='#6'>6. Feature importance<\/a>\n    - <a href='#6.1'>6.1 Reducing model<\/a>\n- <a href='#7'>7. Principal Component analysis<\/a>\n- <a href='#8'>8. Upsampling<\/a>\n- <a href='#9'>9. Building classifiers<\/a>\n- <a href='#10'>10. Classifiers Result<\/a>\n- <a href='#11'>11. Hypertuning Parameters<\/a>\n- <a href='#12'>12. Appendix 1 (Step by step analysis)<\/a>\n- <a href='#13'>13. Appendex 2 (An alternative)<\/a>\n- <a href='#14'>14. Appendex 3 (CatBoostClassifier)<\/a>\n- <a href='#15'>15. Appendex 4 (ANN)<\/a>\n- <a href='#16'>16. Conclusion<\/a>","2eff6fd2":"## <a id='2.2'>2.2 Gender<\/a>","d44aa77d":"Deep learning is a very popular machine tool in modern data science. We shall attempt using a simple model to see how well it does. However, it is important to note that the dataset is rather small and deep learning might not be as effective.","a4de8ad3":"# <a id='3'>3. Feature engineering<\/a>","bd98d714":"## <a id='1.1'>1.1 Data overview<\/a>","d4621556":"From the plots, customers without online security looks very likely to churn.","9981eee2":"## <a id='2.5'>2.5 Dependents<\/a>","178a89e4":"Another method of dimensionality reduction is using feature extraction. The difference between feature extraction and selection is that while selection chooses a subset of all the features, feature extraction creates new features. One popular method for feature extraction is principal component analysis. PCA works by extracting linear composites of the features, to maximize total variance to bring out distinguishable pattern in the dataset.\n\nBefore transforming the dataset using PCA, we have to decide how many components we want. This can be done by looking at the total variance ratio.","851e523c":"The distribution curves suggest that customers with higher monthly charges are more likely to churn.","706852bc":"Feature engineering is a process whereby we create new features based on the available ones to aid the machine learning algorithm. This is usually a way to use one's domain expertise or helping out the machine gather information that is not so obvious from the raw dataset. Afterall, data manipulation in the right way would benefit the machine algorithm greatly later on.","6f62cbb8":"Feature importance is a way to tell us which feature offer the most infomation on the target variable. It is also a good way to reduce noise in our model by selecting only the top few features, hence it can also be called feature selection.","ecd568b7":"# <a id='2'>2. Exploratory analysis<\/a>","5410ed85":"## <a id='2.13'>2.13 TechSupport<\/a>","dd1bb637":"## <a id='2.9'>2.9 InternetService<\/a>","e2e01818":"# <a id='5'>5. Data Preprocessing<\/a>","1167d386":"From the pie plots, subscribing to phone service does not appear to offer much information for predicting customer churn.","28a6af56":"# <a id='8'>8. Upsampling<\/a>","945ddef8":"## <a id='2.4'>2.4 Partner<\/a>","e7ce0e98":"# <a id='1'>1. Import and view dataset<\/a>","e84fe613":"Customers with month-to-month contract seems very likely to churn as compared to those with other forms of other contract.","66631638":"## <a id='2.12'>2.12 DeviceProtection<\/a>","1b2c4bac":"## <a id='2.11'>2.11 OnlineBackup<\/a>","a5c6d4b9":"## <a id='2.20'>2.20 TotalCharges<\/a>","99347a0d":"The distribution for StreamingTV and StreamingMovies looks very similar. Like streamingTV, those without subscription for streamingMovie are slightly more likely to churn.","10d59163":"## <a id='3.1'>3.1 Ratio of past monthly charges to current month charge<\/a>","ac1b5a08":"Now that we know which features offers the most explanation, we shall select the top 20 features. This could potentially improve the models as there would be less noise.","97e8efc6":"As mentioned previously, the data type of 'totalcharges' does not seem right, lets take a look why.","ed516b06":"While most models provided by sklearn require the features to be numerical, catboost does not and in a sense, it's sort of a short cut as we do not really have to preprocess the data for the classifier to work. We try shall try it out as an example. The only data manipulation we would be doing is to remove those with no 'TotalCharges' and the 2 feature engineering to keep things consistent. (Please note that the parameters are manually optimize as much as possible to save time.) ","ba192406":"## ANN structure","182196d1":"## <a id='2.19'>2.19 MonthlyCharges<\/a>","2c4729df":"Upsampling would allow a better performance for the classifiers later as the distribution of the target variable would be more balanced. We would be using SMOTE provided by sklearn to upsample. SMOTE creates synthetic observations of the minority class (churners) by:\n1. Finding the k-nearest-neighbors for minority class observations (finding similar observations)\n2. Randomly choosing one of the k-nearest-neighbors and using it to create a similar, but randomly tweaked, new observation.\n\nWe would upsample the minority class till 0.5 of the majority class.","342ff031":"The similar distribution of both pie graphs suggests having multiple lines or not would not be too important a factor for churning.","8cac5113":"This notebook has shown some ways to manipulate data, some methods to select models, and how to hypertune parameters. Overall, the results are generally acceptable (though not fantastic) with most models having around 80% accuracy and slightly above 60% F1 score. ","2eb7a46c":"From the plots, customers without online backup looks very likely to churn.","ba129aab":"The churn rate is rather imbalanced, with only around 26.5% of the customers churning. This suggest we might have to take some measures before training a model in order to achieve better results. Also, the F1 score evaluation metric would be a more appropriate option than accuracy.","f60c5db9":"The graphs suggest that the artificial neural network did not overfit but there was no significant improvements to the predictions after the 10th epoch.","80064669":"Customers with shorter tenures looks more like to churn.","5307ed72":"# <a id='9'>9. Building classifiers<\/a>","06f4e2f9":"## <a id='3.2'>3.2 Count of extra services<\/a>","b3ca54e4":"## <a id='2.14'>2.14 StreamingTV<\/a>","e0561f98":"# <a id='6'>6. Feature importance (Selection)<\/a>","72e40a0f":"## <a id='2.15'>2.15 StreamingMovies<\/a>","2987e9c4":"The plots suggests that fiber optic users looks very likely to churn","2344fb60":"## <a id='2.17'>2.17 PaperlessBilling<\/a>","db94d4c1":"# Table of content"}}