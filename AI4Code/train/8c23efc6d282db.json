{"cell_type":{"74fcca82":"code","8b38c0f8":"code","c984697e":"code","b0332127":"code","f04fe40a":"code","fe81e3d9":"code","044bb62f":"code","01086ec3":"code","7c137024":"code","b8ee7b66":"code","d346022a":"code","2e0d92bd":"code","15a52ea2":"code","bdf1f451":"code","6ffa8944":"code","878d9f6d":"code","80d976c0":"code","3c5d4d86":"code","7ed979f2":"code","d014c2bc":"code","a3e995ce":"code","ab97aee0":"code","e94a4200":"code","776836ef":"code","bcf87b27":"code","9ea6ae76":"markdown","1965524d":"markdown","52ed2c13":"markdown","765760a3":"markdown","85b843e0":"markdown","454d01a6":"markdown","439904a5":"markdown","bee882ec":"markdown","fb9d3231":"markdown","b5876677":"markdown"},"source":{"74fcca82":"import gc\n\nimport pandas as pd\nimport numpy as np\n\nfrom scipy.sparse import hstack\n\nfrom sklearn.preprocessing import OrdinalEncoder,OneHotEncoder,StandardScaler, MinMaxScaler, LabelEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score,roc_auc_score, log_loss\nfrom sklearn.model_selection import StratifiedKFold, KFold, cross_val_score,cross_validate\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectKBest, f_classif, chi2, SelectPercentile\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.calibration import CalibratedClassifierCV, calibration_curve\n\nimport optuna\nimport lightgbm as lgbm\nimport xgboost as xgb\nimport pickle\nimport category_encoders as ce\nfrom catboost import CatBoostClassifier, Pool\n\nimport string\nimport re\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport tensorflow.keras.backend as K\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import activations,callbacks\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import initializers\n\nfrom keras.models import Model","8b38c0f8":"train = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/test.csv')\nsample_sub = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/sample_submission.csv')","c984697e":"train.head()","b0332127":"test.head()","f04fe40a":"X = train.drop(['id', 'target'], axis=1, inplace=False).copy()\ny = train['target'].values\n\nX_test = test.drop(['id'], axis=1, inplace=False).copy()\nrandom_seed = 0","fe81e3d9":"encoder = OrdinalEncoder()\nall_encoded = encoder.fit_transform(X.append(X_test))\n\nX = all_encoded[0:len(X)]\nX_test = all_encoded[len(X):]","044bb62f":"params = {\n          'bootstrap':True,\n          'max_depth':30,\n          'max_features':'auto'  ,\n          'min_samples_leaf' :10,\n          'min_samples_split':5,\n          'n_estimators':500\n          }","01086ec3":"name = 'Random_forest'\nk = 5\nseed_list = [0, 1, 2]\nkf = StratifiedKFold(n_splits = k, shuffle=True, random_state=random_seed)\noof = np.zeros((len(train), 9))\ntest_preds_list = []\nscore_list = []\nfold = 1\n\nsplits = list(kf.split(X, y))\nfor train_idx, val_idx in splits:\n    X_train, X_val = X[train_idx], X[val_idx]\n    y_train, y_val = y[train_idx], y[val_idx]\n    \n    val_preds_list = []\n    \n    for seed in seed_list:\n        base_model = RandomForestClassifier(**params, random_state = seed)\n        model = CalibratedClassifierCV(base_model, method='sigmoid', cv=k)\n        \n        model.fit(X_train, y_train)\n        \n        val_preds_list.append(model.predict_proba(X_val))\n        test_preds_list.append(model.predict_proba(X_test))\n    \n    oof[val_idx] = np.mean(val_preds_list, axis=0)\n    score = log_loss(y_val, oof[val_idx])\n    print(f'fold: {fold}, log_loss: {score}')\n    score_list.append(score)\n    fold += 1\n    \ncv_logloss = np.mean(score_list)\nprint(f'{name}, log_loss: {cv_logloss}')\n\npreds = np.mean(test_preds_list, axis = 0)\n\nsample_sub[['Class_1','Class_2','Class_3','Class_4','Class_5','Class_6','Class_7','Class_8','Class_9']] = preds\nsample_sub.to_csv('submission_2.csv',index=None)","7c137024":"X = train.drop(['id', 'target'], axis=1, inplace=False).copy()\ny = train['target'].values\n\nX_test = test.drop(['id'], axis=1, inplace=False).copy()\nrandom_seed = 0","b8ee7b66":"encoder = OrdinalEncoder()\nall_encoded = encoder.fit_transform(X.append(X_test))\nX = all_encoded[0:len(X)]\nX_test = all_encoded[len(X):]","d346022a":"params = {\n  'learning_rate': .02,\n   'max_depth': 3,\n    'num_leaves': 6,\n    'min_split_gain': 0.17865452483871047,\n    'reg_alpha': 9.540720621520459,\n    'reg_lambda': 4.5781292529661375,\n    'colsample_bytree': 0.0644950794287173,\n    'subsample': 0.9314592865852914,\n    'subsample_freq': 7,\n    'min_child_samples': 57\n}","2e0d92bd":"params_lgbm = params\nparams_lgbm['boosting_type'] = 'gbdt'\nparams_lgbm['device'] = 'gpu'\nparams_lgbm ['objective'] = 'multiclasss'\nparams_lgbm ['num_classes'] = 9,\n\nparams_lgbm ['metric'] = 'multi_logloss'\nparams_lgbm ['verbosity'] = -1\nparams_lgbm ['n_estimators']= 500\n#params_lgbm[\"cat_feature\"] = cat_features\n\nname = 'lighgbm_3seeds_5fold'\nk = 5\nseed_list = [0, 1, 2]\nkf = StratifiedKFold(n_splits = k, shuffle=True, random_state=random_seed)\noff = np.zeros((len(train), 9))\ntest_preds_list = []\nscore_list = []\nfold = 1\n\nsplits = list(kf.split(X, y))\nfor train_idx, val_idx in splits:\n    X_train, X_val = X[train_idx], X[val_idx]\n    y_train, y_val = y[train_idx], y[val_idx]\n    \n    val_preds_list = []\n    \n    for seed in seed_list:\n        params_lgbm['random_state'] = seed\n        model = lgbm.LGBMClassifier(**params_lgbm)\n    \n        model.fit(X_train, y_train, eval_set = [(X_train,y_train),(X_val,y_val)],\n                 early_stopping_rounds=100,\n                 eval_names=['train','val'],verbose=200)\n        \n        val_preds_list.append(model.predict_proba(X_val))\n        test_preds_list.append(model.predict_proba(X_test))\n    \n    oof[val_idx] = np.mean(val_preds_list, axis=0)\n    score = log_loss(y_val, oof[val_idx])\n    print(f'fold: {fold}, log_loss: {score}')\n    score_list.append(score)\n    fold += 1\n    \ncv_logloss = np.mean(score_list)\nprint(f'{name}, log_loss: {cv_logloss}')\n\npreds = np.mean(test_preds_list, axis = 0)\n\nsample_sub[['Class_1','Class_2','Class_3','Class_4','Class_5','Class_6','Class_7','Class_8','Class_9']] = preds\nsample_sub.to_csv('submission_3.csv',index=None)","15a52ea2":"X = train.drop(labels=['id','target'],axis=1,inplace=False).copy()\ny = train['target'].map({\"Class_1\":0,\"Class_2\":1,\"Class_3\":2,\"Class_4\":3,\"Class_5\":4,\"Class_6\":5, \"Class_7\":6, \"Class_8\":7, \"Class_9\":8}).values\nX_test = test.drop(labels=['id'],axis=1,inplace=False).copy()\nrandom_seed = 0\n\nencoder = OrdinalEncoder()\nall_encoded = encoder.fit_transform(X.append(X_test))\nX = all_encoded[0:len(X)]\nX_test = all_encoded[len(X):]","bdf1f451":"params =  {'lambda': 1.3718620937297796, \n           'alpha': 6.395781966352342, \n           'colsample_bytree': 0.2390564723786096, \n           'colsample_bynode': 0.7459555518737353, \n           'colsample_bylevel': 0.36002014547566097, \n           'subsample': 0.6302863949739616,\n           'eta': 0.01, \n           'grow_policy': 'lossguide', \n           'max_depth': 19, \n           'min_child_weight': 28, \n           'max_bin': 258, \n           'deterministic_histogram': False}","6ffa8944":"params_xgb = params\nparams_xgb[\"tree_method\"] = \"gpu_hist\"\nparams_xgb[\"predictor\"] = 'gpu_predictor'\nparams_xgb[\"objective\"] = 'multi:softprob'\nparams_xgb[\"num_class\"] = 9\n#params_xgb[\"eval_metric\"] ='logloss'\n\nname = 'xgboost_3seeds_5fold'\nk=5\nseed_list=[0,1,2]\nkf = StratifiedKFold(n_splits=k,shuffle=True,random_state=random_seed)\noof = np.zeros((len(train),9))\ntest_preds_list = []\nscore_list = []\nfold=1\n  \nsplits = list(kf.split(X,y))\nfold = 1\nfor train_idx, val_idx in splits:\n    X_train, X_val = X[train_idx], X[val_idx]\n    y_train, y_val = y[train_idx], y[val_idx]\n\n    val_preds_list = []\n\n    for seed in seed_list:\n    \n        # fit and run model\n        params_xgb['seed'] = seed\n    \n        dtrain = xgb.DMatrix(data=X_train, label=y_train)\n        dval = xgb.DMatrix(data=X_val, label=y_val)\n        dtest = xgb.DMatrix(data=X_test)\n    \n        model = xgb.train(params_xgb, dtrain,\\\n                       evals=[(dtrain,'train'),(dval,'val')],\\\n                       verbose_eval=100,\n                       early_stopping_rounds=100,\n                       num_boost_round=100000)\n    \n    \n\n    \n        val_preds_list.append(model.predict(dval))\n        test_preds_list.append(model.predict(dtest))\n    \n    oof[val_idx] = np.mean(val_preds_list,axis=0)\n    score = log_loss(y_val, oof[val_idx])\n    print(f\"fold: {fold},log_loss: {score}\")\n    score_list.append(score)\n    # print(f\"fold: {fold}, class0 tr %: {y_train.value_counts()[0]\/len(y_train)}, class0 val %: {y_val.value_counts()[0]\/len(y_val)} \")\n    fold +=1\n  \ncv_logloss = np.mean(score_list)\nprint(f\"{name} ,log_loss: {cv_logloss}\")\n\npreds= np.mean(test_preds_list,axis=0)\n\nsample_sub[['Class_1','Class_2','Class_3','Class_4','Class_5','Class_6','Class_7','Class_8','Class_9']] = preds\nsample_sub.to_csv('submission_4.csv',index=None)","878d9f6d":"X = train.drop(labels=['id','target'],axis=1,inplace=False).copy()\ny = train['target'].values\nX_test = test.drop(labels=['id'],axis=1,inplace=False).copy()\nrandom_seed = 0\n\nencoder = OrdinalEncoder()\nall_encoded = encoder.fit_transform(X.append(X_test))\nX = all_encoded[0:len(X)]\nX_test = all_encoded[len(X):]\n\nX = X.astype(int)\nX_test = X_test.astype(int)\ncat_features = np.arange(0,X.shape[1]).tolist()","80d976c0":"params =   {'learning_rate': 0.03470328317940195, \n           'depth': 2, \n           'l2_leaf_reg': 820.7804346737378, \n           'random_strength': 0.336019499813798, \n           'border_count': 128,\n           'grow_policy': 'Lossguide',\n           'min_data_in_leaf': 267}","3c5d4d86":"params_cb = params\n\n#params_cb[\"cat_features\"] = cat_features\n#params_cb [\"learning_rate\"] = 0.01\n#params_cb [\"depth\"] = 4\nparams_cb [\"loss_function\"] = 'MultiClass'\nparams_cb [\"od_wait\"] = 1000\nparams_cb [\"od_type\"] = 'Iter'\n#params_cb [\"min_data_in_leaf\"] = 1\n#params_cb [\"max_ctr_complexity\"] = 15\nparams_cb [\"task_type\"] = \"GPU\"\nparams_cb[\"cat_features\"] = cat_features\n            \n\nname = 'catboost_3seeds_5fold'\nk=5\nseed_list=[0,1,2]\nkf = StratifiedKFold(n_splits=k,shuffle=True,random_state=random_seed)\noof = np.zeros((len(train),9))\ntest_preds_list = []\nscore_list = []\nfold=1\n  \nsplits = list(kf.split(X,y))\nfold = 1\nfor train_idx, val_idx in splits:\n    X_train, X_val = X[train_idx], X[val_idx]\n    y_train, y_val = y[train_idx], y[val_idx]\n\n    val_preds_list = []\n\n    for seed in seed_list:\n    \n    # fit and run model\n        params_cb['random_state'] = seed\n        \n        model = CatBoostClassifier(**params_cb,\n            iterations=50000,\n            use_best_model=True,\n            )\n\n        model.fit(X_train,y=y_train,\n              use_best_model=True,\n              eval_set=[(X_val,y_val)],\n              verbose=100)\n    \n\n    \n        val_preds_list.append(model.predict_proba(X_val))\n        test_preds_list.append(model.predict_proba(X_test))\n    \n    oof[val_idx] = np.mean(val_preds_list,axis=0)\n    score = log_loss(y_val, oof[val_idx])\n    print(f\"fold: {fold},log_loss: {score}\")\n    score_list.append(score)\n  # print(f\"fold: {fold}, class0 tr %: {y_train.value_counts()[0]\/len(y_train)}, class0 val %: {y_val.value_counts()[0]\/len(y_val)} \")\n    fold +=1\n  \ncv_logloss = np.mean(score_list)\nprint(f\"{name} ,log_loss: {cv_logloss}\")\n\npreds= np.mean(test_preds_list,axis=0)\n\nsample_sub[['Class_1','Class_2','Class_3','Class_4','Class_5','Class_6','Class_7','Class_8','Class_9']] = preds\nsample_sub.to_csv('submission_5.csv',index=None)","7ed979f2":"targets = pd.get_dummies(train['target'])","d014c2bc":"def custom_metric(y_true, y_pred):\n    y_pred = K.clip(y_pred, 1e-15, 1-1e-15)\n    loss = K.mean(cce(y_true, y_pred))\n    return loss\n\ncce = tf.keras.losses.CategoricalCrossentropy()\n\nes = tf.keras.callbacks.EarlyStopping(\n    monitor='val_custom_metric', min_delta=1e-05, patience=5, verbose=0,\n    mode='min', baseline=None, restore_best_weights=True)\n\nplateau = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_custom_metric', factor=0.7, patience=2, verbose=0,\n    mode='min')","a3e995ce":"def conv_model():\n\n    conv_inputs = layers.Input(shape = (75))\n    embed = layers.Embedding (input_dim = 354, \n                              output_dim = 7,\n                              embeddings_regularizer='l2')(conv_inputs)\n    embed = layers.Conv1D(12,1,activation = 'relu')(embed)        \n    embed = layers.Flatten()(embed)\n    hidden = layers.Dropout(0.3)(embed)\n    \n    hidden = tfa.layers.WeightNormalization(\n                layers.Dense(\n                units=32,\n                activation ='selu',\n                kernel_initializer = \"lecun_normal\"))(hidden)\n    \n    output = layers.Dropout(0.3)(layers.Concatenate()([embed, hidden]))\n    output = tfa.layers.WeightNormalization(\n    layers.Dense(\n                units = 32,\n                activation='relu',\n                kernel_initializer = \"lecun_normal\"))(output) \n    output = layers.Dropout(0.4)(layers.Concatenate()([embed, hidden, output]))\n    output = tfa.layers.WeightNormalization(\n    layers.Dense(\n                units = 32, \n                activation = 'relu',\n                kernel_initializer = \"lecun_normal\"))(output)\n    \n    conv_outputs = layers.Dense(\n                units = 9, \n                activation ='softmax',\n                kernel_initializer =\"lecun_normal\")(output)\n    \n    model = Model(conv_inputs,conv_outputs)\n    \n    return model","ab97aee0":"oof_NN_a = np.zeros((train.shape[0],9))\npred_NN_a = np.zeros((test.shape[0],9))\n\nN_FOLDS = 5\nSEED = 2021\nEPOCH = 60\n\n\nskf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n\nfor fold, (tr_idx, ts_idx) in enumerate(skf.split(train,train.iloc[:,-1])):\n    print(f\"\\n ====== TRAINING FOLD {fold} =======\\n\")\n\n    X_train = train.iloc[:,1:-1].iloc[tr_idx]\n    y_train = targets.iloc[tr_idx]\n    X_test = train.iloc[:,1:-1].iloc[ts_idx]\n    y_test = targets.iloc[ts_idx]\n\n    K.clear_session()\n    \n    print(\"\\n-----Convolution model Training----\\n\")\n\n    model_conv = conv_model()\n\n    model_conv.compile(loss='categorical_crossentropy', \n                            optimizer = keras.optimizers.Adam(learning_rate=2e-4), \n                            metrics=custom_metric)\n    model_conv.fit(X_train, y_train,\n              batch_size = 256, epochs = EPOCH,\n              validation_data=(X_test, y_test),\n              callbacks=[es, plateau],\n              verbose = 0)\n   \n    pred_a = model_conv.predict(X_test) \n    oof_NN_a[ts_idx] += pred_a \n    score_NN_a = log_loss(y_test, pred_a)\n    print(f\"\\nFOLD {fold} Score convolution model: {score_NN_a}\\n\")\n    pred_NN_a += model_conv.predict(test.iloc[:,1:]) \/ N_FOLDS \n \nscore_a = log_loss(targets, oof_NN_a)\nprint(f\"\\n=== FINAL SCORE CONVOLUTION MODEL : {score_a}===\\n\")","e94a4200":"pred_embedding = pred_NN_a","776836ef":"sample_sub['Class_1']=pred_embedding[:,0]\nsample_sub['Class_2']=pred_embedding[:,1]\nsample_sub['Class_3']=pred_embedding[:,2]\nsample_sub['Class_4']=pred_embedding[:,3]\nsample_sub['Class_5']=pred_embedding[:,4]\nsample_sub['Class_6']=pred_embedding[:,5]\nsample_sub['Class_7']=pred_embedding[:,6]\nsample_sub['Class_8']=pred_embedding[:,7]\nsample_sub['Class_9']=pred_embedding[:,8]","bcf87b27":"sample_sub.to_csv(\"submission_6.csv\", index=False)","9ea6ae76":"name = 'Logistic_regression'\nk = 5\nseed_list = [0, 1, 2]\nkf = StratifiedKFold(n_splits = k, shuffle=True, random_state=random_seed)\noof = np.zeros((len(train), 9))\ntest_preds_list = []\nscore_list = []\nfold = 1\n\nsplits = list(kf.split(X, y))\nfor train_idx, val_idx in splits:\n    X_train, X_val = X[train_idx], X[val_idx]\n    y_train, y_val = y[train_idx], y[val_idx]\n    \n    val_preds_list = []\n    \n    for seed in seed_list:\n        base_model = LogisticRegression(**params, random_state=seed)\n        model = CalibratedClassifierCV(base_model, method='sigmoid', cv=k)\n        \n        model.fit(X_train, y_train)\n        \n        val_preds_list.append(model.predict_proba(X_val))\n        test_preds_list.append(model.predict_proba(X_test))\n        \n    oof[val_idx] = np.mean(val_preds_list, axis=0)\n    score = log_loss(y_val, oof[val_idx])\n    print(f'fold: {fold}, log_loss: {score}')\n    score_list.append(score)\n    \n    fold += 1\n    \ncv_logloss = np.mean(score_list)\nprint(f'{name}, log_loss: {cv_logloss}')\n\npreds = np.mean(test_preds_list, axis = 0)\n\nsample_sub[['Class_1','Class_2','Class_3','Class_4','Class_5','Class_6','Class_7','Class_8','Class_9']] = preds\nsample_sub.to_csv('submission_1.csv',index=None)","1965524d":"## CNN","52ed2c13":"## XGBoost","765760a3":"params = {\n    'penalty': 'l2',\n    'multi_class':'ovr',\n    'solver':'lbfgs',\n    'C':0.01,\n    'max_iter':10000,\n    'class_weight':None\n}","85b843e0":"## LightGBM","454d01a6":"## Logistic Regression","439904a5":"encoder = OneHotEncoder()\nall_encoded = encoder.fit_transform(X.append(X_test))\n\nX = all_encoded.tocsr()[0:len(X)]\nX_test = all_encoded[len(train):]","bee882ec":"## CatBoost","fb9d3231":"X = train.drop(['id', 'target'], axis=1, inplace=False).copy()\ny = train['target'].values\n\nX_test = test.drop(['id'], axis=1, inplace=False).copy()\nrandom_seed = 0","b5876677":"## Random Forest"}}