{"cell_type":{"e1476502":"code","23411f44":"code","df84ca08":"code","d9ff1c2f":"code","e123772e":"code","84991d02":"code","be4bd2bd":"code","32f42917":"code","763630ae":"code","cdcb9bff":"code","3093d2f6":"code","013727d8":"code","5837346e":"code","e2c8c0e6":"code","c70aeb5e":"code","748c3856":"code","aee50852":"code","290c7393":"code","ddcee503":"code","48a5597d":"code","3f6d76ed":"code","4ebf6b96":"code","d19019f4":"code","eb1cd085":"code","f9bd3d98":"code","a333c5de":"code","083e13d3":"code","7395c0b5":"code","f4616096":"code","28117d07":"code","f99d8913":"code","3a549916":"code","a110d63e":"code","584f4b7a":"code","6d5296e2":"code","5e785951":"code","cfd4597b":"code","faffb8f1":"code","90db83c1":"code","d09ae530":"code","1012a41f":"code","a16c4cac":"code","027c58b4":"code","36c5f6e9":"code","bdc3f04d":"code","27bef1ef":"code","6d97315b":"code","db80b180":"code","0b363b20":"code","368e63de":"code","0e7d2dd3":"code","a0d59a1f":"code","3d60ddfa":"code","e8dfe635":"markdown"},"source":{"e1476502":"conda install pydotplus","23411f44":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","df84ca08":"data = pd.read_csv(\"\/kaggle\/input\/bank-marketing-dataset\/bank.csv\")","d9ff1c2f":"data.head()","e123772e":"from sklearn import preprocessing\nimport matplotlib.pyplot as plt \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns","84991d02":"data.isnull().sum()","be4bd2bd":"data.columns","32f42917":"data.shape","763630ae":"data.dtypes","cdcb9bff":"data_c = data.select_dtypes(include=[object])","3093d2f6":"for c in data_c.columns:\n    print(\"----------------------\")\n    print(data_c[c].value_counts())","013727d8":"data = data.drop(['poutcome', 'contact'], axis = 1)","5837346e":"cat_vars = ['job','marital','education','default','housing','loan','month']\n\nfor var in cat_vars:\n    cat_list='var'+'_'+var\n    cat_list = pd.get_dummies(data[var], prefix=var)\n    data1=data.join(cat_list)\n    data=data1\n\ncat_vars=['job','marital','education','default','housing','loan','month']\n\ndata_vars=data.columns.values.tolist()\nto_keep=[i for i in data_vars if i not in cat_vars]","e2c8c0e6":"def binary_map(x):\n        return x.map({'yes': 1, \"no\": 0})\n    \ncols = ['deposit']\ndata[cols] = data[cols].apply(binary_map)","c70aeb5e":"data_final = data[to_keep]\ndata_final.columns.values","748c3856":"list(data_final.select_dtypes(['object']).columns)","aee50852":"data_final_v = data_final.columns.values.tolist()","290c7393":"X = data_final.loc[:, data_final.columns != 'deposit']\ny = data_final.loc[:, data_final.columns == 'deposit']","ddcee503":"from sklearn.feature_selection import RFE\nimport warnings\nwarnings.filterwarnings(\"ignore\")","48a5597d":"logreg = LogisticRegression()\nmodel = logreg.fit(X, y)\nrfe = RFE(logreg, 15)\nrfe = rfe.fit(X, y.values.ravel())\n\nprint(rfe.support_)\nprint(rfe.ranking_)","3f6d76ed":"import statsmodels.api as sm\nlogit_model=sm.Logit(y, X)\nresult=logit_model.fit()\nprint(result.summary2())","4ebf6b96":"def column_index(data, query_cols):\n    cols = data.columns.values\n    sidx = np.argsort(cols)\n    return sidx[np.searchsorted(cols, query_cols, sorter = sidx)]\n\nfeature_index = []\nfeatures = []\ncolumn_index(X, X.columns.values)\n\nfor num, i in enumerate(rfe.get_support(), start=0):\n    if i == True:\n        feature_index.append(str(num))\n\nfor num, i in enumerate(X.columns.values, start=0):\n    if str(num) in feature_index:\n        features.append(X.columns.values[num])\n\nprint(\"Top Features : {}\\n\".format(len(feature_index)))\nprint(\"Indexes: \\n{}\\n\".format(feature_index))\nprint(\"Feature Names: \\n{}\".format(features))","d19019f4":"X = data[['job_retired', 'job_student', 'education_primary', 'housing_yes', 'loan_no', \n          'month_aug', 'month_dec', 'month_jan', 'month_jul', 'month_jun', 'month_mar', \n          'month_may', 'month_nov', 'month_oct', 'month_sep']] \n\ny = data['deposit']","eb1cd085":"logit_model=sm.Logit(y, X)\nresult = logit_model.fit()\nprint(result.summary2())","f9bd3d98":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","a333c5de":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)","083e13d3":"from sklearn.model_selection import GridSearchCV\nclf = LogisticRegression()\ngrid_values = {'penalty': ['l1', 'l2'],'C':[0.001,.009,0.01,.09,1,5,10,25]}\ngrid_clf_acc = GridSearchCV(clf, param_grid = grid_values,scoring = 'recall')\ngrid_clf_acc.fit(X_train, y_train)\n\nprint(grid_clf_acc.best_estimator_)\n\ny_pred_acc = grid_clf_acc.predict(X_test)","7395c0b5":"from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score\nprint('Accuracy Score : ' + str(accuracy_score(y_test,y_pred_acc)))\nprint('Precision Score : ' + str(precision_score(y_test,y_pred_acc)))\nprint('Recall Score : ' + str(recall_score(y_test,y_pred_acc)))\nprint('F1 Score : ' + str(f1_score(y_test,y_pred_acc)))\n\n#Dummy Classifier Confusion matrix\nfrom sklearn.metrics import confusion_matrix\nprint('Confusion Matrix : \\n' + str(confusion_matrix(y_test,y_pred_acc)))","f4616096":"from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix","28117d07":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42) \n\nlr_list = [0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1]\n\nfor learning_rate in lr_list:\n    gb_clf = GradientBoostingClassifier(n_estimators=20, learning_rate=learning_rate, \n                                        max_features=2, max_depth=2, random_state=0)\n    gb_clf.fit(X_train, y_train)\n\n    print(\"Learning rate: \", learning_rate)\n    print(\"Accuracy score (training): {0:.3f}\".format(gb_clf.score(X_train, y_train)))\n    print(\"Accuracy score (validation): {0:.3f}\".format(gb_clf.score(X_test, y_test)))","f99d8913":"gb_clf2 = GradientBoostingClassifier(n_estimators=20, learning_rate=0.5, max_features=2, \n                                     max_depth=2, random_state=0)\ngb_clf2.fit(X_train, y_train)\npredictions = gb_clf2.predict(X_test)\n\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, predictions))\n\nprint(\"Classification Report\")\nprint(classification_report(y_test, predictions))","3a549916":"from sklearn.externals.six import StringIO  \nfrom IPython.display import Image  \nfrom sklearn.tree import export_graphviz\nimport pydotplus","a110d63e":"from sklearn.ensemble import GradientBoostingClassifier\ngb = GradientBoostingClassifier(max_depth=3)\ngb.fit(X, y)\n\ndot_data = StringIO()\nexport_graphviz(gb.estimators_[0][0], out_file=dot_data, filled=True, rounded=True, \n                special_characters=True, feature_names = X.columns)\n\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \nImage(graph.create_png())","584f4b7a":"from xgboost import XGBClassifier\n\nxgb_clf = XGBClassifier()\nxgb_clf.fit(X_train, y_train)\n\nscore = xgb_clf.score(X_test, y_test)\nprint(score)","6d5296e2":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(oob_score=True, n_estimators=100, random_state=42, n_jobs=-1, max_depth=2)\nrf.fit(X, y)\nrf.feature_importances_","5e785951":"export_graphviz(rf.estimators_[1], out_file = 'tree.dot', feature_names = X.columns, rounded = True, precision = 1)\n\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \nImage(graph.create_png())","cfd4597b":"from sklearn import tree\ntreemodel = tree.DecisionTreeClassifier(max_depth = 3, random_state=42)\ntreemodel.fit(X, y)","faffb8f1":"treemodel.feature_importances_","90db83c1":"y_pred = treemodel.predict(X)","d09ae530":"dot_data = StringIO()\nexport_graphviz(treemodel, out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True, feature_names = X.columns)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \nImage(graph.create_png())","1012a41f":"y_pred = gb.predict(X)\nconfusion_matrix(y, y_pred)","a16c4cac":"from sklearn.model_selection import cross_val_score, cross_val_predict\nfrom sklearn import metrics","027c58b4":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=8)\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\nmetrics.accuracy_score(y_test, y_pred)","36c5f6e9":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42) \n  \nneighbors = np.arange(1, 10) \ntrain_accuracy = np.empty(len(neighbors)) \ntest_accuracy = np.empty(len(neighbors)) \n  \n# Loop over K values \nfor i, k in enumerate(neighbors): \n    knn = KNeighborsClassifier(n_neighbors=k) \n    knn.fit(X_train, y_train) \n      \n    # Compute traning and test data accuracy \n    train_accuracy[i] = knn.score(X_train, y_train) \n    test_accuracy[i] = knn.score(X_test, y_test) \n  \n# Generate plot \nplt.plot(neighbors, test_accuracy, label = 'Testing Accuracy') \nplt.plot(neighbors, train_accuracy, label = 'Training Accuracy') \n  \nplt.legend() \nplt.xlabel('n_neighbors') \nplt.ylabel('Accuracy') \nplt.show()","bdc3f04d":"scores = cross_val_score(logreg, X, y, cv=10)\nprint(scores)","27bef1ef":"from sklearn.metrics import mean_squared_error\nfrom math import sqrt\nfrom sklearn import model_selection\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import LeaveOneOut\nfrom sklearn.model_selection import LeavePOut\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import StratifiedKFold","6d97315b":"X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.30, random_state=100)\nmodel = LogisticRegression()\n\nmodel.fit(X_train, y_train)\nresult = model.score(X_test, y_test)\n\nprint(\"Accuracy: %.2f%%\" % (result*100.0))","db80b180":"kfold = model_selection.KFold(n_splits=10, random_state=100)\nmodel_kfold = LogisticRegression()\n\nresults_kfold = model_selection.cross_val_score(model_kfold, X, y, cv=kfold)\n\nprint(\"Accuracy: %.2f%%\" % (results_kfold.mean()*100.0))","0b363b20":"skfold = StratifiedKFold(n_splits=6, random_state=100)\nmodel_skfold = LogisticRegression()\nresults_skfold = model_selection.cross_val_score(model_skfold, X, y, cv=skfold)\nprint(\"Accuracy: %.2f%%\" % (results_skfold.mean()*100.0))","368e63de":"kfold2 = model_selection.ShuffleSplit(n_splits=10, test_size=0.30, random_state=100)\nmodel_shufflecv = LogisticRegression()\nresults_4 = model_selection.cross_val_score(model_shufflecv, X, y, cv=kfold2)\nprint(\"Accuracy: %.2f%% (%.2f%%)\" % (results_4.mean()*100.0, results_4.std()*100.0))","0e7d2dd3":"logreg = LogisticRegression()\nlogreg.fit(X_train, y_train)","a0d59a1f":"y_pred = logreg.predict(X_test)\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))","3d60ddfa":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nlogit_roc_auc = roc_auc_score(y_test, logreg.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()","e8dfe635":"## Do upvote if you found this to be of any help. "}}