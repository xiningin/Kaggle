{"cell_type":{"8dc8f859":"code","bad83727":"code","5ad4b3d5":"code","547073d9":"code","f6c03561":"code","42413b42":"code","5dddbe53":"code","18307462":"code","51313084":"code","18417e15":"code","1fd7d625":"code","c0fc83fa":"code","1075c76b":"code","dcd323ab":"code","119eb7c8":"code","834bc992":"code","8c967845":"code","79229295":"code","b6b79e55":"code","7491c907":"code","43e59a5f":"code","1fe6dbfe":"code","e8f3d43f":"code","1425e789":"code","b4963a61":"code","ef484a51":"code","f3d0afcb":"code","6a66078f":"code","e26a2bb3":"code","5f09ab03":"code","ace0e39b":"code","3ab6e8d0":"code","808852a5":"code","a845a46d":"code","8a802c3e":"code","9dbcf8e8":"code","1ecd0429":"code","1fef7945":"code","41be3ac1":"code","819c51c6":"code","6f12bd01":"code","db3322db":"code","9f502691":"code","1ee4af1b":"code","724b70ec":"code","82637b6e":"code","e525b8f3":"markdown","66a3308a":"markdown","7108d516":"markdown","4907b1de":"markdown","8a2d44ea":"markdown","b8e41913":"markdown","9b90d54d":"markdown","dba29abe":"markdown","aa8c8caa":"markdown","611f8255":"markdown","8d14b146":"markdown","3684e920":"markdown","5aacd910":"markdown","8f3688f1":"markdown","de941a20":"markdown","49c5fabd":"markdown","104e6782":"markdown","51c6f749":"markdown","e65120dd":"markdown","fe43d49e":"markdown","66446373":"markdown","389fb2d2":"markdown","a5a1f163":"markdown","ee767703":"markdown","0be4c230":"markdown","f6d933b6":"markdown","6adb066b":"markdown","c8eeace6":"markdown","c410a050":"markdown","77791347":"markdown","2ac1aa55":"markdown","1d7606a5":"markdown","022d53de":"markdown","690a3840":"markdown","10d38952":"markdown","8973d974":"markdown","f6954a2a":"markdown","540597dd":"markdown","6fd87968":"markdown","54c5b817":"markdown","2315163f":"markdown","0a81f76c":"markdown","1a04a861":"markdown","186cff11":"markdown","65340fd0":"markdown","f0719044":"markdown","4745624d":"markdown","5b7e0a5e":"markdown","2b7f48d3":"markdown","2b7b793a":"markdown","91d82418":"markdown","3c831fcd":"markdown","7b9d9493":"markdown","8a6b648a":"markdown","85646d30":"markdown","c65e9998":"markdown"},"source":{"8dc8f859":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n\nfrom sklearn.model_selection import cross_val_score,cross_val_predict, train_test_split\nfrom sklearn.preprocessing import OneHotEncoder,StandardScaler,PowerTransformer,LabelEncoder\n\nfrom sklearn.metrics import accuracy_score,classification_report, recall_score,confusion_matrix, roc_auc_score, precision_score, f1_score, roc_curve, auc, plot_confusion_matrix,plot_roc_curve\n\n\nimport optuna\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\nimport optuna\nimport lightgbm as lgb\nfrom xgboost import XGBClassifier\n#from lightgbm import LGBMClassifier, plot_importance\nfrom catboost import CatBoostClassifier\n\n\n#importing plotly and cufflinks in offline mode\nimport cufflinks as cf\nimport plotly.offline\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\n\n\nimport plotly \nimport plotly.express as px\nimport plotly.graph_objs as go\nimport plotly.offline as py\nfrom plotly.offline import iplot\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\n\nimport shap \n\nimport missingno as msno\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","bad83727":"pd.set_option('max_columns',100)\npd.set_option('max_rows',900)\n\npd.set_option('max_colwidth',200)\ndata = pd.read_csv(\"..\/input\/customer-churn-prediction-2020\/train.csv\")\ndata.head()","5ad4b3d5":"df1 = data.copy()","547073d9":"df1.head()","f6c03561":"print(f\"We have {df1.shape[0]} rows and {df1.shape[1]} columns in our dataset.\")","42413b42":"df1.duplicated().sum()","5dddbe53":"def missing (df1):\n    missing_number = df1.isnull().sum().sort_values(ascending=False)\n    missing_percent = (df1.isnull().sum()\/df1.isnull().count()).sort_values(ascending=False)\n    missing_values = pd.concat([missing_number, missing_percent], axis=1, keys=['Missing_Number', 'Missing_Percent'])\n    return missing_values\n\nmissing(df1)","18307462":"df1.info()","51313084":"df1.churn.value_counts()","18417e15":"df1.churn.value_counts(normalize=True)","1fd7d625":"y = df1['churn']\nprint(f'Percentage of Churn:  {round(y.value_counts(normalize=True)[1]*100,2)} %  --> ({y.value_counts()[1]} customers)')\nprint(f'Percentage Non_Churn: {round(y.value_counts(normalize=True)[0]*100,2)}  %  --> ({y.value_counts()[0]} customers)')","c0fc83fa":"y.iplot(kind=\"hist\", title=\"Churns vs. NonChurns\");","1075c76b":"# We converted type of \"churn\" from object to int\nle = LabelEncoder()\ndf1.churn = le.fit_transform(df1.churn)","dcd323ab":"df1.info()","119eb7c8":"numerical= df1.select_dtypes(include = 'number').columns\n\ncategorical = df1.select_dtypes(include = 'object').columns\n\nprint(f'Numerical Columns:  {df1[numerical].columns}')\nprint('\\n')\nprint(f'Categorical Columns: {df1[categorical].columns}')","834bc992":"col_int = []\n\nfor col in numerical:\n    if df1[col].dtype == \"int64\":\n        col_int.append(col)\n\ncol_int.remove(\"churn\")\ncol_int","8c967845":"for i in col_int:\n    df1[i] = df1[i].astype(float)","79229295":"df1.info()","b6b79e55":"df1[numerical].describe()\n# df1.describe()","7491c907":"plt.figure(figsize=(16, 8))\nsns.heatmap (df1[numerical].corr(), annot=True, fmt= '.2f', vmin=-1, vmax=1, center=0, cmap='coolwarm');","43e59a5f":"drop_col = ['total_day_charge', 'total_eve_charge', 'total_night_charge', 'total_intl_charge']\ndf1 = df1.drop(drop_col, axis=1)\ndf1.shape","1fe6dbfe":"numerical= df1.select_dtypes(include = 'number').columns\nnumerical","e8f3d43f":"plt.figure(figsize=(16, 8))\nsns.heatmap (df1[numerical].corr(), annot=True, fmt= '.2f', vmin=-1, vmax=1, center=0, cmap='coolwarm');","1425e789":"df1[categorical].nunique()","b4963a61":"for column in df1[categorical]:\n    print(f\"{column}: {df1[column].unique()}\")","ef484a51":"for i in df1[\"state\"].unique():\n    print(f'A customer from state of {i} has a probability of {round(df1[df1[\"state\"]==i][\"churn\"].mean()*100,2)} % churn.')","f3d0afcb":"fig = px.histogram(data_frame=df1, x=\"state\", color=\"churn\", width=1200, height=400)\nfig.show()","6a66078f":"area_code: ['area_code_415' 'area_code_408' 'area_code_510']\n    \nprint(f'A customer with area_code_415 has a probability of {round(df1[df1[\"area_code\"]==\"area_code_415\"][\"churn\"].mean()*100,2)} % churn.')\nprint()\nprint(f'A customer with area_code_408 has a probability of {round(df1[df1[\"area_code\"]==\"area_code_408\"][\"churn\"].mean()*100,2)} % churn.')\nprint()\nprint(f'A customer with area_code_510 has a probability of {round(df1[df1[\"area_code\"]==\"area_code_510\"][\"churn\"].mean()*100,2)} % churn.')","e26a2bb3":"fig = px.histogram(data_frame=df1, x=\"area_code\", color=\"churn\", width=420, height=420)\nfig.show()","5f09ab03":"print(f'A customer with an international plan has a probability of {round(df1[df1[\"international_plan\"]==\"yes\"][\"churn\"].mean()*100,2)} % churn.')\nprint()\nprint(f'A customer wwithout an international plan has a probability of {round(df1[df1[\"international_plan\"]==\"no\"][\"churn\"].mean()*100,2)} % churn.')","ace0e39b":"fig = px.histogram(data_frame=df1, x=\"international_plan\", color=\"churn\", width=420, height=420)\nfig.show()","3ab6e8d0":"print(f'A customer with a voice mail plan has a probability of {round(df1[df1[\"voice_mail_plan\"]==\"yes\"][\"churn\"].mean()*100,2)} % churn.')\nprint()\nprint(f'A customer wwithout a vocie mail plan has a probability of {round(df1[df1[\"voice_mail_plan\"]==\"no\"][\"churn\"].mean()*100,2)} % churn.')","808852a5":"fig = px.histogram(data_frame=df1, x=\"voice_mail_plan\", color=\"churn\", width=420, height=420)\nfig.show()","a845a46d":"numerical_1 = ['account_length', 'number_vmail_messages', 'total_day_minutes',\n       'total_day_calls', 'total_eve_minutes', 'total_eve_calls',\n       'total_night_minutes', 'total_night_calls', 'total_intl_minutes',\n       'total_intl_calls', 'number_customer_service_calls']\nnumerical_1","8a802c3e":"accuracy= []\nrecall =[]\nroc_auc= []\nprecision = []\n\n\ndf = pd.read_csv('..\/input\/customer-churn-prediction-2020\/train.csv')\ndf1 = df.copy()\nle = LabelEncoder()\ndf1['churn']=le.fit_transform(df1['churn'])\n\n\n#for i in numerical_1:\n    #df1[i] = df1[i].astype(float)\n\n    \nX= df1.drop('churn', axis=1)\ny= df1['churn']\n\ncategorical_features_indices = np.where(X.dtypes != np.float)[0]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# With scale_pos_weight=5, minority class gets 5 times more impact and 5 times more correction than errors made on the majority class.\ncatboost_5 = CatBoostClassifier(verbose=False,random_state=0,scale_pos_weight=5)\n\ncatboost_5.fit(X_train, y_train,cat_features=categorical_features_indices,eval_set=(X_test, y_test))\ny_pred = catboost_5.predict(X_test)\n\naccuracy.append(round(accuracy_score(y_test, y_pred),4))\nrecall.append(round(recall_score(y_test, y_pred),4))\nroc_auc.append(round(roc_auc_score(y_test, y_pred),4))\nprecision.append(round(precision_score(y_test, y_pred),4))\n\nmodel_names = ['Catboost_adjusted_weight_5']\nresult_df1 = pd.DataFrame({'Accuracy':accuracy,'Recall':recall, 'Roc_Auc':roc_auc, 'Precision':precision}, index=model_names)\nresult_df1","9dbcf8e8":"fig, ax = plt.subplots(figsize=(10, 6))\nplot_confusion_matrix(catboost_5, X_test, y_test, cmap=plt.cm.plasma, ax=ax);","1ecd0429":"def objective(trial):\n    df = pd.read_csv('..\/input\/customer-churn-prediction-2020\/train.csv')\n    df1 = df.copy()\n    \n    le = LabelEncoder()\n    df1['churn']=le.fit_transform(df1['churn'])\n    \n    X= df1.drop('churn', axis=1)\n    y= df1['churn']\n    \n    categorical_features_indices = np.where(X.dtypes != np.float)[0]\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n    param = {\n        \"objective\": \"Logloss\",\n        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1),\n        \"depth\": trial.suggest_int(\"depth\", 1, 12),\n        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n        \"bootstrap_type\": trial.suggest_categorical(\n            \"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]\n        ),\n        \"used_ram_limit\": \"3gb\",\n    }\n\n    if param[\"bootstrap_type\"] == \"Bayesian\":\n        param[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n    elif param[\"bootstrap_type\"] == \"Bernoulli\":\n        param[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n\n    cat_cls = CatBoostClassifier(verbose=False,random_state=0,scale_pos_weight=1.2, **param)\n\n    cat_cls.fit(X_train, y_train, eval_set=[(X_test, y_test)], cat_features=categorical_features_indices,verbose=0, early_stopping_rounds=100)\n\n    preds = cat_cls.predict(X_test)\n    pred_labels = np.rint(preds)\n    accuracy = accuracy_score(y_test, pred_labels)\n    return accuracy\n\n\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=100, timeout=600)\n\n    print(\"Number of finished trials: {}\".format(len(study.trials)))\n\n    print(\"Best trial:\")\n    trial = study.best_trial\n\n    print(\"  Value: {}\".format(trial.value))\n\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))","1fef7945":"accuracy= []\nrecall =[]\nroc_auc= []\nprecision = []\n\n\ndf = pd.read_csv('..\/input\/customer-churn-prediction-2020\/train.csv')\ndf1 = df.copy()\n\n#for target feature\nle = LabelEncoder()\ndf1['churn']=le.fit_transform(df1['churn'])\n\n\nX=df1.drop('churn', axis=1)\ny=df1['churn']\n\n#indeces of categorical observations\ncategorical_features_indices = np.where(X.dtypes != np.float)[0]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n#since our dataset is not imbalanced, we do not have to use scale_pos_weight parameter to counter balance our results\n#catboost_5 = CatBoostClassifier(verbose=False,random_state=0,scale_pos_weight=5)\ncatboost_5 = CatBoostClassifier(verbose=False,random_state=0,\n                                 colsample_bylevel=0.091134936724785,\n                                 depth=9,\n                                 boosting_type=\"Ordered\",\n                                 bootstrap_type=\"MVS\")\n\ncatboost_5.fit(X_train, y_train,cat_features=categorical_features_indices,eval_set=(X_test, y_test), early_stopping_rounds=100)\ny_pred = catboost_5.predict(X_test)\n\naccuracy.append(round(accuracy_score(y_test, y_pred),4))\nrecall.append(round(recall_score(y_test, y_pred),4))\nroc_auc.append(round(roc_auc_score(y_test, y_pred),4))\nprecision.append(round(precision_score(y_test, y_pred),4))\n\nmodel_names = ['Catboost_adjusted_weight_5_optuna']\nresult_df2 = pd.DataFrame({'Accuracy':accuracy,'Recall':recall, 'Roc_Auc':roc_auc, 'Precision':precision}, index=model_names)\nresult_df2","41be3ac1":"accuracy= []\nrecall =[]\nroc_auc= []\nprecision = []\n\n\ndf = pd.read_csv('..\/input\/customer-churn-prediction-2020\/train.csv')\ndf1 = df.copy()\nle = LabelEncoder()\ndf1['churn']=le.fit_transform(df1['churn'])\n\n                 \ndf1= pd.get_dummies(df1)\nX= df1.drop('churn', axis=1)\ny= df1['churn']\n\nfor col in X.columns:\n    col_type = X[col].dtype\n    if col_type == 'object' or col_type.name == 'category':\n        X[col] = X[col].astype('category')\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nlgbmc_5=LGBMClassifier(random_state=0,scale_pos_weight=5)\n\nlgbmc_5.fit(X_train, y_train,categorical_feature = 'auto',eval_set=(X_test, y_test),feature_name='auto', verbose=0)\n\ny_pred = lgbmc_5.predict(X_test)\n\naccuracy.append(round(accuracy_score(y_test, y_pred),4))\nrecall.append(round(recall_score(y_test, y_pred),4))\nroc_auc.append(round(roc_auc_score(y_test, y_pred),4))\nprecision.append(round(precision_score(y_test, y_pred),4))\n\nmodel_names = ['LightGBM_adjusted_weight_5']\nresult_df3 = pd.DataFrame({'Accuracy':accuracy,'Recall':recall, 'Roc_Auc':roc_auc, 'Precision':precision}, index=model_names)\nresult_df3","819c51c6":"def objective(trial):\n    df = pd.read_csv('..\/input\/customer-churn-prediction-2020\/train.csv')\n    df1 = df.copy()\n    le = LabelEncoder()\n    df1['churn']=le.fit_transform(df1['churn'])\n   \n    \n    X= df1.drop('churn', axis=1)\n    y= df1['churn']\n    \n    for col in X.columns:\n        col_type = X[col].dtype\n        if col_type == 'object' or col_type.name == 'category':\n            X[col] = X[col].astype('category')    \n    \n    param = {\n        \"objective\": \"binary\",\n        \"metric\": \"binary_logloss\",\n        \"verbosity\": -1,\n        \"boosting_type\": \"dart\",\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2,2000),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n    }\n    \n    lgbmc_adj=lgb.LGBMClassifier(random_state=0,scale_pos_weight=5,**param)\n    lgbmc_adj.fit(X_train, y_train,categorical_feature = 'auto',eval_set=(X_test, y_test),feature_name='auto', verbose=0, early_stopping_rounds=100)\n\n    preds = lgbmc_adj.predict(X_test)\n    pred_labels = np.rint(preds)\n    accuracy = accuracy_score(y_test, pred_labels)\n    return accuracy\n\n\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=100)\n\n    print(\"Number of finished trials: {}\".format(len(study.trials)))\n\n    print(\"Best trial:\")\n    trial = study.best_trial\n\n    print(\"  Value: {}\".format(trial.value))\n\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))","6f12bd01":"accuracy= []\nrecall =[]\nroc_auc= []\nprecision = []\n\n\ndf = pd.read_csv('..\/input\/customer-churn-prediction-2020\/train.csv')\ndf1 = df.copy()\nle = LabelEncoder()\ndf1['churn']=le.fit_transform(df1['churn'])\n\n\nX= df1.drop('churn', axis=1)\ny= df1['churn']\n\n#if you want a variable to be perecived as categorical then you need to covert it to object type\nfor col in X.columns:\n    col_type = X[col].dtype\n    if col_type == 'object' or col_type.name == 'category':\n        X[col] = X[col].astype('category')\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nlgbmc_5=lgb.LGBMClassifier(random_state=0,scale_pos_weight=5,\n                           num_leaves=724,\n                           max_depth=9,\n                           lambda_l1=8.142384644362947e-06,\n                           lambda_l2=3.432798202818561e-08,\n                           feature_fraction=0.5164384666114301,\n                           bagging_fraction=0.7323707200247135,\n                           bagging_freq=5,\n                           min_child_samples=5)\n\n#y_train,categorical_feature = 'auto' takes all categoricals automatically\nlgbmc_5.fit(X_train, y_train,categorical_feature = 'auto',eval_set=(X_test, y_test),feature_name='auto', verbose=0, early_stopping_rounds=100)\n\ny_pred = lgbmc_5.predict(X_test)\n\naccuracy.append(round(accuracy_score(y_test, y_pred),4))\nrecall.append(round(recall_score(y_test, y_pred),4))\nroc_auc.append(round(roc_auc_score(y_test, y_pred),4))\nprecision.append(round(precision_score(y_test, y_pred),4))\n\nmodel_names = ['LightGBM_adjusted_weight_5_optuna']\nresult_df4 = pd.DataFrame({'Accuracy':accuracy,'Recall':recall, 'Roc_Auc':roc_auc, 'Precision':precision}, index=model_names)\nresult_df4","db3322db":"accuracy= []\nrecall =[]\nroc_auc= []\nprecision = []\n\n\ndf = pd.read_csv(\"..\/input\/customer-churn-prediction-2020\/train.csv\")\ndf1 = df.copy()\nle = LabelEncoder()\ndf1['churn']=le.fit_transform(df1['churn'])\n\n#Since XGBoost does not handle categorical values itself, we use get_dummies to convert categorical variables into numeric variables.\ndf1= pd.get_dummies(df1)\nX= df1.drop('churn', axis=1)\ny= df1['churn']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nxgbc_5 = XGBClassifier(random_state=0)\n\nxgbc_5.fit(X_train, y_train)\ny_pred = xgbc_5.predict(X_test)\n\naccuracy.append(round(accuracy_score(y_test, y_pred),4))\nrecall.append(round(recall_score(y_test, y_pred),4))\nroc_auc.append(round(roc_auc_score(y_test, y_pred),4))\nprecision.append(round(precision_score(y_test, y_pred),4))\n\nmodel_names = ['XGBoost_adjusted_weight_5']\nresult_df5 = pd.DataFrame({'Accuracy':accuracy,'Recall':recall, 'Roc_Auc':roc_auc, 'Precision':precision}, index=model_names)\nresult_df5","9f502691":"import numpy as np\nimport optuna\n\nimport sklearn.datasets\nimport sklearn.metrics\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\n\ndef objective(trial):\n    \n    df = pd.read_csv(\"..\/input\/customer-churn-prediction-2020\/train.csv\")\n    df1 = df.copy()\n    le = LabelEncoder()\n    df1['churn']=le.fit_transform(df1['churn'])\n\n    df1= pd.get_dummies(df1)\n    X= df1.drop('churn', axis=1)\n    y= df1['churn']\n    \n    #(data, target) = sklearn.datasets.load_breast_cancer(return_X_y=True)\n    train_x, valid_x, train_y, valid_y = train_test_split(X, y, test_size=0.25)\n    dtrain = xgb.DMatrix(train_x, label=train_y)\n    dvalid = xgb.DMatrix(valid_x, label=valid_y)\n\n    param = {\n        \"verbosity\": 0,\n        \"objective\": \"binary:logistic\",\n        # use exact for small dataset.\n        \"tree_method\": \"exact\",\n        # defines booster, gblinear for linear functions.\n        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n        # L2 regularization weight.\n        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n        # L1 regularization weight.\n        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n        # sampling ratio for training data.\n        \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n        # sampling according to each tree.\n        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n    }\n\n    if param[\"booster\"] in [\"gbtree\", \"dart\"]:\n        # maximum depth of the tree, signifies complexity of the tree.\n        param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 3, 9, step=2)\n        # minimum child weight, larger the term more conservative the tree.\n        param[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 2, 10)\n        param[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n        # defines how selective algorithm is.\n        param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n        param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n\n    if param[\"booster\"] == \"dart\":\n        param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n        param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n        param[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n        param[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n\n    bst = xgb.train(param, dtrain)\n    preds = bst.predict(dvalid)\n    pred_labels = np.rint(preds)\n    accuracy = sklearn.metrics.accuracy_score(valid_y, pred_labels)\n    return accuracy\n\n\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=100, timeout=600)\n\n    print(\"Number of finished trials: \", len(study.trials))\n    print(\"Best trial:\")\n    trial = study.best_trial\n\n    print(\"  Value: {}\".format(trial.value))\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))","1ee4af1b":"from  xgboost import XGBClassifier\naccuracy= []\nrecall =[]\nroc_auc= []\nprecision = []\n\n\ndf = pd.read_csv(\"..\/input\/customer-churn-prediction-2020\/train.csv\")\ndf1 = df.copy()\nle = LabelEncoder()\ndf1['churn']=le.fit_transform(df1['churn'])\n\ndf1= pd.get_dummies(df1)\nX= df1.drop('churn', axis=1)\ny= df1['churn']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nxgbc_5 = XGBClassifier(random_state=0,\n     booster=\"gbtree\",\n     lambda_=1.0747585763388536e-08,\n     alpha=4.888494937862174e-05,\n     subsample=0.9424632124541714,\n     colsample_bytree=0.9950004607929119,\n     max_depth=9,\n     min_child_weight=3,\n     eta=0.053153334432134325,\n     gamma=0.0017328227799719943,\n     grow_policy=\"lossguide\")\n\nxgbc_5.fit(X_train, y_train)\ny_pred = xgbc_5.predict(X_test)\n\naccuracy.append(round(accuracy_score(y_test, y_pred),4))\nrecall.append(round(recall_score(y_test, y_pred),4))\nroc_auc.append(round(roc_auc_score(y_test, y_pred),4))\nprecision.append(round(precision_score(y_test, y_pred),4))\n\nmodel_names = ['XGBoost_adjusted_weight_5_optuna']\nresult_df6 = pd.DataFrame({'Accuracy':accuracy,'Recall':recall, 'Roc_Auc':roc_auc, 'Precision':precision}, index=model_names)\nresult_df6","724b70ec":"result_final= pd.concat([result_df1,result_df2,result_df3,result_df4,result_df5,result_df6],axis=0)\nresult_final","82637b6e":"result_final.sort_values(by=['Accuracy'], ascending=True,inplace=True)\nfig = px.bar(result_final, x='Accuracy', y=result_final.index,title='Model Comparison',height=600,labels={'index':'MODELS'})\nfig.show()","e525b8f3":"- With **OPTUNA** hyperparameters, we managed to increase our **Accuracy** score by 2%.\n\n\n![image.png](attachment:b3f5c277-ef75-4395-88af-c6653493bb3f.png)","66a3308a":"# Conclusion","7108d516":"## Target Variable","4907b1de":"# Table of Contents\n- Data\n\n- Problem at Hand and Metric to Use?\n\n- Exploratory Data Analysis\n\n    - Target Variable\n\n    - Numerical Features\n\n    - Categorical Features\n\n- Famous Trio and Imbalanced Data\n\n    - CATBOOST \/ OPTUNA\n\n    - XGBOOST \/ OPTUNA\n\n    - LIGHTGBM \/OPTUNA\n\n- Model Comparision\n\n- Conclusion","8a2d44ea":"# Problem at Hand and Metric to Use?\n- After analyzing data and data dictionary we see that we have a **classification** problem.\n- We wil make classification on the target variable **Churn**.\n- For this purpose we will look at the balance of the target variable.\n- Since our target variable has imblanced data we are not going to use **Accuracy** score.\n- Based on the problem on the hand, we will use **Recall** score.","b8e41913":"- We converted **churn** column into numeric type.","9b90d54d":"- We got rid of multicollinear columns.\n- **total_day_minutes** has the highest correleation with **churn**.\n- Overall, there is low correleations among features.\n","dba29abe":"# Data\nThis dataset is about predicting whether a customer will change telecommunications provider, something known as \"churning\".\n\nThe training dataset contains 4250 samples. Each sample contains 19 features and 1 boolean variable \"churn\" which indicates the class of the sample. The 19 input features and 1 target variable are:\n\n![image.png](attachment:11dae1ed-7c24-4d7f-8948-aac71aad2c37.png)","aa8c8caa":"### voice_mail_plan vs. churn","611f8255":"![image.png](attachment:775354cf-ce1c-47d3-9e91-5a24c8013cba.png)","8d14b146":"After applying **OPTUNA** parameters to our **XGBoost** model, we get silightly higher score than the one with default parameters.","3684e920":"**OPTUNA** parameters give us a higher Accuracy score (0.96)","5aacd910":"- CatBoost, XGBoost, and LightGBM use **scale_pos_weight** hyperparameter to tune the training algorithm for the imbalanced data.\n\n- By defualt, **scale_pos_weight** is 1.\n\n- Both major class and minority class get the same weight in balanced data. However, when dealing with imbalanced data, story changes a bit.\n\n- Formula for calculating value of **scale_pos_weight**: \n    - Number of Non-churned (**majority**) customer: 5174\n    - Number of Churned customer(**minority**): 1869\n    - **scale_pos_weight** = 5174 \/ 1869 or almost 3\n- By adjusting the weight, minority class gets 3 times more impact and 3 times more correction than errors made on the majority class.\n\n**Note1**: If we use extreme values for the **scale_pos_weight**, we can overfit the minority class and model could make worse predictions.\n\n**Note2**: While **CatBoost** and **LightGBM** can handle categorical features, **XGBoost** cannot. You have to convert categorical features before creating your model.","8f3688f1":"It was developed by Microsoft ","de941a20":"**Based on our preliminary analysis, we conclude that**:\n- We won't drop any column.\n- For **CatBoost** model, types of columns with int64 will be converted into float type.\n- We will look at the cardinality of the categorical variables.\n- And finally, we will convert **churn** column to numeric type by using **label encoding**.","49c5fabd":"We will also deal with imbalanced data by using the famous trio modles and hyperparameter tunning with **OPTUNA**.","104e6782":"### state vs. churn","51c6f749":"### OPTUNA - Hyperparameter Tunning","e65120dd":"## XGBoost","fe43d49e":"With parameters provided by **OPTUNA**, our accuracy score is almost 0.96.","66446373":"With **OPTUNA** parameters in **LightGBM**, our accuracy score did not change much. ","389fb2d2":"- It seems that there is not much difference among **area_codes** on churn rate.\n- We may drop it later.","a5a1f163":"- Now, let's look at the **CatBoost**, **XGBoost**, and **LightGBM** and see how they handle imbalanced data internally.\n- By giving an opportunity to focus more on the minority class and accordingly tunning the training, they do good job even on imbalanced data.","ee767703":"## CATBOOST","0be4c230":"# Exploratory Data Analysis","f6d933b6":"### XGBoost - scale_pos_weight = 5","6adb066b":"- Ok let's use our **CatBoost** model with new parameters.","c8eeace6":"### LightGBM - scale_pos_weight = 5","c410a050":"## LightGBM","77791347":"- Customers without a voice mail plan is almost 2.5 times more likely to churn than those with voice mail plan.","2ac1aa55":"- While **CA** (California) has the highest rate of churn, **VA** (Virginia) has the lowest churn rate.\n- Overall churn rates among states range from 5 percent to 25 percent.","1d7606a5":"![image.png](attachment:54868641-bc9b-4d60-9e7f-a283c4f130f0.png)","022d53de":"- We have developed model to classifiy churn cases.\n\n- First, we made the detailed exploratory analysis.\n\n- We have decided which metric to use (**Accuracy** - since the author of the dataset required so).\n\n- We looked in detail **Catboost**, **LightGBM**, and **XGBoost** models.\n\n- We made hyperparameter tuning of for each model with **OPTUNA** to see the improvement.","690a3840":"## Numerical Features","10d38952":"### OPTUNA - Hyperparameter Tunning","8973d974":"- It is an Boosting algorithm that was created by Yandex.\n- It can handle both missing values and categorical values internally.","f6954a2a":"- Great news! We do not have a high cardinality or zero variance issues.","540597dd":"### CatBoost - scale_pos_weight = 5","6fd87968":"# Famous Trio and Imbalanced Data","54c5b817":"With defualt parameters, we got 0.95 as accuracy score.","2315163f":"Today, we will create models with famous trio (**XGBoost** & **LightGBM** & **Catboost**) that predict behavior to retain customers. We will analyze all relevant customer data and develop focused customer retention programs.","0a81f76c":"- We have just converted all types of all columns (except for **churn**) with int64 into float type.","1a04a861":"- It is obvious that we have imbalanced data.\n- Almost 14% of the customers (598 customers) didn't continue with the company and churned.\n- Almost 86% of the customers (3562 customers) continue with the company and didn't churn.","186cff11":"We have neither duplicated nor missing value (some deeper checks might always be needed to be 100% sure).","65340fd0":"- For ease of usage, we got the list of the **numerical** and **categorical** features.","f0719044":"### area_code vs. churn","4745624d":"- Customers with an international plan is almost 4 times more likely to churn than those without international plan.","5b7e0a5e":"- We can see in heatmap that we have some multicollinerity. \n- We need to drop one of each highly correleated column pairs.","2b7f48d3":"# Model Comparion","2b7b793a":"### international_plan vs. churn ","91d82418":"![image.png](attachment:c30320f4-9693-4166-b972-26b5c751919a.png)","3c831fcd":"### OPTUNA - Hyperparameter Tunning","7b9d9493":"With our defult parameters, we got almost 0.96 as accuracy score.","8a6b648a":"![image.png](attachment:f9f51bdc-d385-4a11-9bb4-d226f9760a62.png)","85646d30":"# Hi all. \ud83d\ude4b","c65e9998":"## Categorical Features"}}