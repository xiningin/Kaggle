{"cell_type":{"4413951c":"code","42944051":"code","448bd479":"code","0c6934d1":"code","e2bef926":"code","bdc02391":"code","a67a4e76":"code","6a87c270":"code","cd9a750f":"code","47595885":"code","14f4b5bb":"code","e0f6b8a0":"code","ba171053":"code","ff885ee0":"code","3e6953ff":"code","0bec81d5":"code","2a9f7840":"code","08ea4d3c":"code","dfe2117a":"code","ef79bd8c":"code","569aa248":"code","5911f8b7":"code","1663cf88":"code","829916ae":"code","017d0520":"code","9a72e7c8":"code","63b92a00":"code","e0e2a22b":"markdown","7b4bc20f":"markdown","beae4562":"markdown","e8e4176c":"markdown","f82c11f8":"markdown","f3311a7c":"markdown","819840c3":"markdown","2afbf051":"markdown","fd345736":"markdown","b78ce19d":"markdown","24034091":"markdown","640f88da":"markdown","1a6397af":"markdown","fa90932b":"markdown","ec428aff":"markdown","26200aad":"markdown","19ca48a5":"markdown","c640ee24":"markdown","1278a287":"markdown"},"source":{"4413951c":"!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py","42944051":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfrom wordcloud import WordCloud\nfrom nltk.corpus import stopwords\nfrom tqdm.notebook import tqdm\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\nimport tokenization\nfrom sklearn.model_selection import train_test_split","448bd479":"pd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nplt.style.use('fivethirtyeight')","0c6934d1":"train_data = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")","e2bef926":"train_data.head()","bdc02391":"train_data.describe()","a67a4e76":"train_data.info()","6a87c270":"print(\"Shape of the training dataset: {}.\".format(train_data.shape))\nprint(\"Shape of the testing dataset: {}\".format(test_data.shape))\nfor col in train_data.columns:\n    nan_vals = train_data[col].isna().sum()\n    pcent = (train_data[col].isna().sum() \/ train_data[col].count()) * 100\n    print(\"Total NaN values in column '{}' are: {}, which is {:.2f}% of the data in that column\".format(col, nan_vals, pcent))","cd9a750f":"# Let's plot NaN value distribution\nfig = sns.barplot(\n    x=train_data[['keyword', 'location']].isna().sum().index,\n    y=train_data[['keyword', 'location']].isna().sum().values,\n)","47595885":"vals = [len(train_data[train_data['target']==1]['target']), len(train_data[train_data['target']==0]['target'])]\n\nplt.pie(vals, labels=[\"Non-Disaster\", \"Disaster\"])\nplt.axis('equal')\nplt.title(\"Target Value Distribution\")\nplt.show()","14f4b5bb":"fig = plt.figure(figsize=(10, 70), dpi=100)\nsns.countplot(y=train_data['keyword'].sort_values(), hue=train_data['target'])\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=12)\nplt.legend(loc=1)\nplt.title('Target Distribution in Keywords')\nfig.show()","e0f6b8a0":"dis_twt = train_data[train_data['target']==1]['text'].str.len()\nnon_dis_twt = train_data[train_data['target']==0]['text'].str.len()\n\nsns.distplot([dis_twt, non_dis_twts_twt])","ba171053":"dis_cnt = train_data[train_data['target']==1]['text'].str.split().map(lambda x: len(x))\nndis_cnt = train_data[train_data['target']==0]['text'].str.split().map(lambda x: len(x))\n\nfig = make_subplots(rows=1, cols=2)\n\nfig.add_trace(\n    go.Histogram(x=list(dis_cnt), name='Disaster Tweets'),\n    row=1, \n    col=1\n)\n\nfig.add_trace(\n    go.Histogram(x=list(ndis_cnt), name='Non Disaster Tweets'),\n    row=1, \n    col=2,\n)\n\nfig.update_layout(height=500, width=950, title_text=\"Words Count\")\nfig.show()","ff885ee0":"dis_avg = train_data[train_data['target']==1]['text'].str.split().map(lambda x: [len(j) for j in x]).map(lambda x: np.mean(x)).to_list()\nndis_avg = train_data[train_data['target']==0]['text'].str.split().map(lambda x: [len(j) for j in x]).map(lambda x: np.mean(x)).to_list()\n\nfig = ff.create_distplot([dis_avg, ndis_avg], ['Disaster', 'Non Disaster'])\nfig.update_layout(height=500, width=950, title_text=\"Average Word Length Distribution\")\nfig.show()","3e6953ff":"dis_uvc = train_data[train_data['target']==1]['text'].apply(lambda x: len(set(str(x).split()))).to_list()\nndis_uvc = train_data[train_data['target']==0]['text'].apply(lambda x: len(set(str(x).split()))).to_list()\n\nfig = ff.create_distplot([dis_uvc, ndis_uvc], ['Disaster', 'Non Disaster'])\nfig.update_layout(height=500, width=950, title_text=\"Unique Word Count Distribution\")\nfig.show()","0bec81d5":"dis_uc = train_data[train_data['target']==1]['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w])).to_list()\nndis_uc = train_data[train_data['target']==0]['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w])).to_list()\n\nfig = make_subplots(rows=1, cols=2)\n\nfig.add_trace(\n    go.Histogram(x=dis_uc, name='Disaster Tweets'),\n    row=1, \n    col=1\n)\n\nfig.add_trace(\n    go.Histogram(x=ndis_uc, name='Non Disaster Tweets'),\n    row=1, \n    col=2,\n)\n\nfig.update_layout(height=500, width=950, title_text=\"URL Count\")\nfig.show()","2a9f7840":"dis_snt = train_data[train_data['target']==1]['text'].to_list()\ndis_snt = \" \".join(dis_snt)\n\ndis_wc = WordCloud(width=256, height=256, collocations=False).generate(dis_snt)\nplt.figure(figsize = (7,7))\nplt.imshow(dis_wc)\nplt.axis('off')\nplt.tight_layout(pad = 0)\nplt.show()","08ea4d3c":"ndis_snt = train_data[train_data['target']==0]['text'].to_list() \nndis_snt = \" \".join(ndis_snt)\n\nndis_wc = WordCloud(width=256, height=256, collocations=False).generate(ndis_snt)\nplt.figure(figsize = (7,7))\nplt.imshow(ndis_wc)\nplt.axis('off')\nplt.tight_layout(pad = 0)\nplt.show()","dfe2117a":"def bert_encode(texts, tokenizer, max_len=512):\n    all_tokens, all_masks, all_segments = [], [], []\n    \n    for text in tqdm(texts):\n        # Tokenize the current text\n        text = tokenizer.tokenize(text)\n        # Select text only till \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","ef79bd8c":"%%time\nurl = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/1\"\nbert_layer = hub.KerasLayer(url, trainable=True)","569aa248":"# Get the vocab file (for tokenizing) and tokenizer itself\nvocab_fl = bert_layer.resolved_object.vocab_file.asset_path.numpy()\nlower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_fl, lower_case)","5911f8b7":"%%time\ntrain_input = bert_encode(train_data['text'].values, tokenizer, max_len=160)\ntest_input = bert_encode(test_data['text'].values, tokenizer, max_len=160)\ntrain_labels = train_data['target'].values","1663cf88":"def build_model(transformer, max_len=512):\n    # Naming your keras ops is very important \ud83d\ude09\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name='input_word_ids')\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name='input_mask')\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name='segment_ids')\n    # Get the sequence output\n    _, seq_op = transformer([input_word_ids, input_mask, segment_ids])\n    # Get the respective class token from that sequence output\n    class_tkn = seq_op[:, 0, :]\n    # Final Neuron (for Classification)\n    op = Dense(1, activation='sigmoid')(class_tkn)\n    # Bind the inputs and outputs together into a Model\n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=op)\n    \n    model.compile(optimizer=Adam(1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","829916ae":"# Build the model\nmodel = build_model(bert_layer, max_len=160)\nmodel.summary()","017d0520":"checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\n\ntrain_history = model.fit(\n    train_input, train_labels,\n    validation_split=0.1,\n    epochs=3,\n    callbacks=[checkpoint],\n    batch_size=16\n)","9a72e7c8":"preds = model.predict(test_input)","63b92a00":"# Load Submission CSV file\nsub_fl = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\nsub_fl['target'] = preds.round().astype(int)\nsub_fl.to_csv(\"submission.csv\", index=False)","e0e2a22b":"### 2.7 Average Word length","7b4bc20f":"# 1. Data Loading","beae4562":"### 2.2 NaN (missing) values visualization ","e8e4176c":"### 2.5 Character Count","f82c11f8":"# 2. Data Visualization","f3311a7c":"### 2.4 Keyword Frequency Count","819840c3":"### 2.8 Unique Word Count Distribution","2afbf051":"### 3.3 Encode the data\nNow, let's encode the training and testing data into","fd345736":"### 2.6 Word Count Distribution","b78ce19d":"### 3.1 Tokenizer function\nBelow is a function defined to tokenize the data and also adds the `CLS` and `SEP` tokens at the start & end as required by BERT ","24034091":"### 3.4 Building the Model\nWe'll make a function for fine-tuning BERT","640f88da":"# 3. Models\nFor training the model, we'll fine-tune BERT","1a6397af":"### 2.10 WordCloud for Disaster and Non-Disaster Tweets","fa90932b":"### 3.6 Testing the model\nLet's test our model and submit predictions!","ec428aff":"### 2.1 Missing Values\nIt so happens that only NaN values in the dataset are present in `keyword` and `location` columns. The former one having **0.81%** of NaN values and the later one having shy of **50%** NaN values.","26200aad":"### 3.2 Get the Model from TFHub\nGet the model from TFHub and the vocab file with it","19ca48a5":"### 3.5 Training the model\nFinally! Let's train our model on GPU","c640ee24":"### 2.9 URL Count","1278a287":"### 2.3 Visualizing Target Values"}}