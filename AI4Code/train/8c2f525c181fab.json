{"cell_type":{"bae93119":"code","57a3657c":"code","8f15b603":"code","9e0009ea":"code","eaaa1139":"code","b19a4a93":"code","9228f76a":"code","0a8e2f1c":"code","e1e75983":"code","4e365b96":"code","f8675852":"code","2300ac1a":"code","d6ef7fc4":"code","45c4702f":"code","b0c341e3":"code","6775b261":"code","d048b716":"code","f863779b":"code","c11d9d37":"code","83455e72":"code","95b63dda":"code","98f3ef84":"code","f9e30c6e":"code","6b0346ec":"code","c83494e5":"code","4f87752a":"code","bf35b63b":"code","03011577":"code","cb90c747":"code","c85b4b72":"code","0c4445d3":"code","f1905aef":"code","cbdc4f70":"code","bc32c6ad":"code","68a5cb89":"code","6233d867":"code","6b2f714b":"markdown","22d0b483":"markdown","53fc305f":"markdown","e8e8d581":"markdown","228bd55a":"markdown","cca717d4":"markdown","1e9aa30d":"markdown","c97f7e02":"markdown","b81b2f0d":"markdown","c63d2b93":"markdown"},"source":{"bae93119":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression,Ridge, Lasso\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom scipy.stats import skew\nimport matplotlib\n%pylab inline\npd.options.display.max_columns = 300","57a3657c":"train = pd.read_csv(\"..\/input\/train.csv\")\ntarget = train[\"SalePrice\"]\ntrain = train.drop(\"SalePrice\",1) # take out the target variable\ntest = pd.read_csv(\"..\/input\/test.csv\")\ncombi = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'],test.loc[:,'MSSubClass':'SaleCondition'])) # this is the combined data frame without the target variable","8f15b603":"print(shape(train))\nprint(shape(test))\nprint(shape(combi))\ncombi.head()","9e0009ea":"figure(figsize(8,4))\nsubplot(1,2,1)\nhist(target*1e-6);\nxlabel(\"Sale Price in Mio Dollar\")\nsubplot(1,2,2)\nhist(log1p(target));\nxlabel(\"log1p(Sale Price in Dollar)\")","eaaa1139":"target1 = log1p(target)\n#log transform skewed numeric features:\nnumeric_feats = combi.dtypes[combi.dtypes != \"object\"].index\n\nskewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\nskewed_feats = skewed_feats[skewed_feats > 0.75]\nskewed_feats = skewed_feats.index\n\ncombi[skewed_feats] = np.log1p(combi[skewed_feats])","b19a4a93":"#Next, let's look at some rows of the data. There are a lot of categorical data and NaN values, it's a bit of a mess:\ncombi.head(10)","9228f76a":"# create new features from categorical data:\ncombi = pd.get_dummies(combi)\n# and fill missing entries with the column mean:\ncombi = combi.fillna(combi.mean())\n\n# create the new train and test arrays:\ntrain = combi[:train.shape[0]]\ntest = combi[train.shape[0]:]\ntrain.isnull().sum().max()\ncombi.shape","0a8e2f1c":"model = LinearRegression()\nscore = mean(sqrt(-cross_val_score(model, train, target,scoring=\"neg_mean_squared_error\", cv = 5)))\nscore1 = mean(sqrt(-cross_val_score(model, train, target1,scoring=\"neg_mean_squared_error\", cv = 5)))\nprint(\"linear regression score: \", score)\nprint(\"linear regression score1: \", score1)","e1e75983":"from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV, Lasso\nfrom sklearn.model_selection import cross_val_score\n\ndef rmse_cv(model):\n    rmse= np.sqrt(-cross_val_score(model, train, target1, scoring=\"neg_mean_squared_error\", cv = 5))\n    return(rmse)","4e365b96":"model_ridge = Ridge()","f8675852":"alphas = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75]\ncv_ridge = [rmse_cv(Ridge(alpha = alpha)).mean() \n            for alpha in alphas]","2300ac1a":"cv_ridge = pd.Series(cv_ridge, index = alphas)\ncv_ridge.plot(title = \"Validation - Just Do It\")\nplt.xlabel(\"alpha\")\nplt.ylabel(\"rmse\")","d6ef7fc4":"cv_ridge.min()\n#cv_ridge","45c4702f":"model_linearRegression = LinearRegression()\ncv_linearRegression = rmse_cv(model_linearRegression).mean()","b0c341e3":"cv_linearRegression = pd.Series(cv_linearRegression)\ncv_linearRegression.plot(title = \"Validation - Just Do It\")\nplt.xlabel(\"alpha\")\nplt.ylabel(\"rmse\")\ncv_linearRegression","6775b261":"model_lasso = LassoCV(alphas = [1, 0.1, 0.001, 0.0005]).fit(train, target1)\ncv_lasso = [rmse_cv(model_lasso).mean()]","d048b716":"print(cv_ridge.min())\nprint(cv_linearRegression.min())\nprint(cv_lasso)","f863779b":"coef = pd.Series(model_lasso.coef_, index = train.columns)","c11d9d37":"coef = pd.Series(model_lasso.coef_, index = train.columns)","83455e72":"print(\"lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")","95b63dda":"imp_coef = pd.concat([coef.sort_values().head(10),\n                     coef.sort_values().tail(10)])","98f3ef84":"matplotlib.rcParams['figure.figsize'] = (8.0, 10.0)\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Coefficients in the ridge Model\")","f9e30c6e":"#let's look at the residuals as well:\nmatplotlib.rcParams['figure.figsize'] = (6.0, 6.0)\n\npreds = pd.DataFrame({\"preds\":model_lasso.predict(train), \"true\":target1})\npreds[\"residuals\"] = preds[\"true\"] - preds[\"preds\"]\npreds.plot(x = \"preds\", y = \"residuals\",kind = \"scatter\")","6b0346ec":"#Feedforward Neural Nets doesn't seem to work well at all...I wonder why.\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.regularizers import l1\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split","c83494e5":"X_train = StandardScaler().fit_transform(train)","4f87752a":"X_tr, X_val, y_tr, y_val = train_test_split(train, target1,test_size=0.2, random_state = 3)","bf35b63b":"X_tr.shape","03011577":"X_val.shape","cb90c747":"model = Sequential()\n#model.add(Dense(256, activation=\"relu\", input_dim = X_train.shape[1]))\nmodel.add(Dense(1, input_dim = X_train.shape[1], W_regularizer=l1(0.001)))\n\nmodel.compile(loss = \"mse\", optimizer = \"adam\")","c85b4b72":"model.summary()","0c4445d3":"hist = model.fit(X_tr, y_tr, validation_data = (X_val, y_val))","f1905aef":"pd.Series(model.predict(X_val)[:,0]).hist()","cbdc4f70":"model_tree = DecisionTreeRegressor()\nmodel_tree.fit(X_tr, y_tr)\nscore = (y_val - model_tree.predict(X_val)).mean()\nprint(score)\nprint(rmse_cv(model_tree).mean())","bc32c6ad":"model_random = RandomForestRegressor()\nmodel_random.fit(X_tr, y_tr)\nprint(rmse_cv(model_random).mean())\n","68a5cb89":"from sklearn.linear_model import LogisticRegression\nmodel_logReg = LogisticRegression()\nmodel_logReg.fit(train,target1)\n","6233d867":"from sklearn.svm import SVR\nmodel_svm = SVR()\nmodel_svm.fit(X_tr,y_tr)\n#print(model_svm.score(model_svm.predict(X_val),y_val))\nprint(rmse_cv(model_svm).mean())","6b2f714b":"## 5-1 K-Nearest Neighbours\nIn **Machine Learning**, the **k-nearest neighbors algorithm** (k-NN) is a non-parametric method used for classification and regression. In both cases, the input consists of the k closest training examples in the feature space. The output depends on whether k-NN is used for classification or regression:\n\nIn k-NN classification, the output is a class membership. An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.\nIn k-NN regression, the output is the property value for the object. This value is the average of the values of its k nearest neighbors.\nk-NN is a type of instance-based learning, or lazy learning, where the function is only approximated locally and all computation is deferred until classification. The k-NN algorithm is among the simplest of all machine learning algorithms.\n###### [Go to top](#top)","22d0b483":"##  5-2 Radius Neighbors Classifier\nClassifier implementing a **vote** among neighbors within a given **radius**\n\nIn scikit-learn **RadiusNeighborsClassifier** is very similar to **KNeighborsClassifier** with the exception of two parameters. First, in RadiusNeighborsClassifier we need to specify the radius of the fixed area used to determine if an observation is a neighbor using radius. Unless there is some substantive reason for setting radius to some value, it is best to treat it like any other hyperparameter and tune it during model selection. The second useful parameter is outlier_label, which indicates what label to give an observation that has no observations within the radius - which itself can often be a useful tool for identifying outliers.","53fc305f":"The main tuning parameter for the Ridge model is alpha - a regularization parameter that measures how flexible our model is. The higher the regularization the less prone our model will be to overfit. However it will also lose flexibility and might not capture all of the signal in the data.","e8e8d581":"## 5-3 Logistic Regression\nLogistic regression is the appropriate regression analysis to conduct when the dependent variable is **dichotomous** (binary). Like all regression analyses, the logistic regression is a **predictive analysis**.\n\nIn statistics, the logistic model (or logit model) is a widely used statistical model that, in its basic form, uses a logistic function to model a binary dependent variable; many more complex extensions exist. In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model; it is a form of binomial regression. Mathematically, a binary logistic model has a dependent variable with two possible values, such as pass\/fail, win\/lose, alive\/dead or healthy\/sick; these are represented by an indicator variable, where the two values are labeled \"0\" and \"1\"","228bd55a":"Note the U-ish shaped curve above. When alpha is too large the regularization is too strong and the model cannot capture all the complexities in the data. If however we let the model be too flexible (alpha small) the model begins to overfit. A value of alpha = 10 is about right based on the plot above.","cca717d4":"\n\nYou are passing floats to a classifier which expects categorical values as the target vector. If you convert it to int it will be accepted as input (although it will be questionable if that's the right way to do it). \n**use lableencoder**","1e9aa30d":" Decision Tree\nDecision Trees (DTs) are a non-parametric supervised learning method used for **classification** and **regression**. The goal is to create a model that predicts the value of a target variable by learning simple **decision rules** inferred from the data features.\n\n##  ExtraTreeClassifier\nAn extremely randomized tree classifier.\n\nExtra-trees differ from classic decision trees in the way they are built. When looking for the best split to separate the samples of a node into two groups, random splits are drawn for each of the **max_features** randomly selected features and the best split among those is chosen. When max_features is set 1, this amounts to building a totally random decision tree.\n\n**Warning**: Extra-trees should only be used within ensemble methods.","c97f7e02":"## RandomForest\nA random forest is a meta estimator that **fits a number of decision tree classifiers** on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. \n\nThe sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).","b81b2f0d":"We see that the target variable has a more symmetric distribution in log-space, and therefore we perform the following simple tranformation:","c63d2b93":" 5-7 SVM\n\nThe advantages of support vector machines are:\n* Effective in high dimensional spaces.\n* Still effective in cases where number of dimensions is greater than the number of samples. \n* Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n* Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n\nThe disadvantages of support vector machines include:\n\n* If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.\n* SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation"}}