{"cell_type":{"3f586dd4":"code","cdeccbdb":"code","e6a9703e":"code","d63c308b":"code","1e8e2c1e":"code","56fee79d":"code","58529bab":"code","2de6b853":"code","a5e52bd2":"code","5575f186":"code","b3e0b23c":"code","0d65582d":"code","17cef1a1":"code","863c30aa":"code","197cda0e":"code","8d0e5692":"code","20b5bbd6":"code","5defd325":"code","53c0ebbc":"code","c7679894":"code","0d7e08f8":"code","2731a3f3":"code","12ec3ee7":"markdown","14e2b513":"markdown","4d2f34f8":"markdown","947e10f0":"markdown","b7b722d8":"markdown","ed8d9188":"markdown","a59a309b":"markdown","f06f8e8d":"markdown","0643a25b":"markdown","167f9606":"markdown","39531c4b":"markdown","3e25aa1c":"markdown","1795b8d6":"markdown"},"source":{"3f586dd4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cdeccbdb":"#Import libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sb\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","e6a9703e":"#Load spreadsheet\nfile = pd.read_csv('..\/input\/heartdisease\/framingham_heart_disease.csv')\n\n#Exploratory data analysis\nprint (file.shape)\nprint (file.info())","d63c308b":"data = pd.DataFrame (file)\n\nfill_feat = [\"glucose\", \"education\", \"BPMeds\", \"totChol\", \"cigsPerDay\", \"BMI\", \"heartRate\"]\nfor i in fill_feat: \n    data[i].fillna(np.mean(data[i]),inplace=True)","1e8e2c1e":"#Normalization using Sklearn\nData = np.asarray(data[['male', 'age', 'education', 'currentSmoker', 'cigsPerDay', 'BPMeds',\n       'prevalentStroke', 'prevalentHyp', 'diabetes', 'totChol', 'sysBP',\n       'diaBP', 'BMI', 'heartRate', 'glucose']])\nTarget = data['TenYearCHD']\n\n#Scaling the data before training\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\n\n\nData = preprocessing.StandardScaler().fit(Data).transform(Data)\nData = pd.DataFrame (Data)\nData.columns = ['male', 'age', 'education', 'currentSmoker', 'cigsPerDay', 'BPMeds',\n       'prevalentStroke', 'prevalentHyp', 'diabetes', 'totChol', 'sysBP',\n       'diaBP', 'BMI', 'heartRate', 'glucose']\n#print (Data.head())\n\nTarget = pd.DataFrame (Target)\nTarget.columns = [\"label\"]\n#print(Target.head())\n\ndroplist = ['BPMeds', 'prevalentHyp','diabetes']\nData = Data.drop(droplist, axis = 1 )\n\nX = Data\ny = Target","56fee79d":"target_count = Target.label.value_counts()\nprint('Class 0:', target_count[0])\nprint('Class 1:', target_count[1])\nprint('Proportion:', round(target_count[0] \/ target_count[1], 2), ': 1')\n\n\ncount_class_N, count_class_P =Target.label.value_counts()\n\n# Divide by class\nClass_N = Target == 0\nClass_P = Target == 1\n\n#Plot the target\nsb.countplot (x = Target.label, data = Data, palette = \"bwr\")\nplt.show()","58529bab":"#Random under-sampling\nfrom imblearn.under_sampling import RandomUnderSampler\n\nrus = RandomUnderSampler()\nX_rus, y_rus = rus.fit_resample(X, y)\nX_res = pd.DataFrame(X_rus)\ny_res = pd.DataFrame(y_rus)\n\nNew_data = pd.concat([X_res, y_res], axis=0)\n\ntarget_count = New_data.label.value_counts()\nprint('Class 0:', target_count[0])\nprint('Class 1:', target_count[1])\nprint('Proportion:', round(target_count[0] \/ target_count[1], 2), ': 1')\n\ncount_class_N, count_class_P = New_data.label.value_counts()\n\n# Divide by class\nClass_N = New_data.label == 0\nClass_P = New_data.label == 1\n\n#Plot the target\nsb.countplot (x = New_data.label, data = New_data, palette = \"bwr\")\nplt.show()","2de6b853":"#Under-sampling based on the Condensed Nearest Neighbor\nfrom imblearn.under_sampling import CondensedNearestNeighbour\n\ncon = CondensedNearestNeighbour()\nX_con, y_con = con.fit_resample(X, y)\nX_res = pd.DataFrame(X_con)\ny_res = pd.DataFrame(y_con)\n\nNew_data = pd.concat([X_res, y_res], axis=0)\n\ntarget_count = New_data.label.value_counts()\nprint('Class 0:', target_count[0])\nprint('Class 1:', target_count[1])\nprint('Proportion:', round(target_count[0] \/ target_count[1], 2), ': 1')\n\ncount_class_N, count_class_P = New_data.label.value_counts()\n\n# Divide by class\nClass_N = New_data.label == 0\nClass_P = New_data.label == 1\n\n#Plot the target\nsb.countplot (x = New_data.label, data = New_data, palette = \"bwr\")\nplt.show()","a5e52bd2":"#Under-sampling based on the Edited Nearest Neighbor\nfrom imblearn.under_sampling import EditedNearestNeighbours\n\nenn = EditedNearestNeighbours()\nX_enn, y_enn = enn.fit_resample(X, y)\nX_res = pd.DataFrame(X_enn)\ny_res = pd.DataFrame(y_enn)\n\nNew_data = pd.concat([X_res, y_res], axis=0)\n\ntarget_count = New_data.label.value_counts()\nprint('Class 0:', target_count[0])\nprint('Class 1:', target_count[1])\nprint('Proportion:', round(target_count[0] \/ target_count[1], 2), ': 1')\n\ncount_class_N, count_class_P = New_data.label.value_counts()\n\n# Divide by class\nClass_N = New_data.label == 0\nClass_P = New_data.label == 1\n\n#Plot the target\nsb.countplot (x = New_data.label, data = New_data, palette = \"bwr\")\nplt.show()","5575f186":"#Under-sampling based on the Repeated Edited Nearest Neighbor\nfrom imblearn.under_sampling import RepeatedEditedNearestNeighbours\n\nrenn = RepeatedEditedNearestNeighbours()\nX_renn, y_renn = renn.fit_resample(X, y)\nX_res = pd.DataFrame(X_renn)\ny_res = pd.DataFrame(y_renn)\n\nNew_data = pd.concat([X_res, y_res], axis=0)\n\ntarget_count = New_data.label.value_counts()\nprint('Class 0:', target_count[0])\nprint('Class 1:', target_count[1])\nprint('Proportion:', round(target_count[0] \/ target_count[1], 2), ': 1')\n\ncount_class_N, count_class_P = New_data.label.value_counts()\n\n# Divide by class\nClass_N = New_data.label == 0\nClass_P = New_data.label == 1\n\n#Plot the target\nsb.countplot (x = New_data.label, data = New_data, palette = \"bwr\")\nplt.show()","b3e0b23c":"#Under-sampling using the cluster centroids\nfrom imblearn.under_sampling import ClusterCentroids\n\ncc = ClusterCentroids(sampling_strategy = 'majority')\nX_cc, y_cc = cc.fit_sample(X, y)\nX_res = pd.DataFrame(X_cc)\ny_res = pd.DataFrame(y_cc)\n\nNew_data = pd.concat([X_res, y_res], axis=0)\n\ntarget_count = New_data.label.value_counts()\nprint('Class 0:', target_count[0])\nprint('Class 1:', target_count[1])\nprint('Proportion:', round(target_count[0] \/ target_count[1], 2), ': 1')\n\ncount_class_N, count_class_P = New_data.label.value_counts()\n\n# Divide by class\nClass_N = New_data.label == 0\nClass_P = New_data.label == 1\n\n#Plot the target\nsb.countplot (x = New_data.label, data = New_data, palette = \"bwr\")\nplt.show()","0d65582d":"#Under-sampling based on one-sided selection\nfrom imblearn.under_sampling import OneSidedSelection\n\nos = OneSidedSelection()\nX_os, y_os = os.fit_sample(X, y)\nX_res = pd.DataFrame(X_os)\ny_res = pd.DataFrame(y_os)\n\nNew_data = pd.concat([X_res, y_res], axis=0)\n\ntarget_count = New_data.label.value_counts()\nprint('Class 0:', target_count[0])\nprint('Class 1:', target_count[1])\nprint('Proportion:', round(target_count[0] \/ target_count[1], 2), ': 1')\n\ncount_class_N, count_class_P = New_data.label.value_counts()\n\n# Divide by class\nClass_N = New_data.label == 0\nClass_P = New_data.label == 1\n\n#Plot the target\nsb.countplot (x = New_data.label, data = New_data, palette = \"bwr\")\nplt.show()","17cef1a1":"#Under-sampling based on removing the Tomek links\nfrom imblearn.under_sampling import TomekLinks\n\ntomek = TomekLinks()\nX_tl, y_tl = os.fit_sample(X, y)\nX_res = pd.DataFrame(X_tl)\ny_res = pd.DataFrame(y_tl)\n\nNew_data = pd.concat([X_res, y_res], axis=0)\n\ntarget_count = New_data.label.value_counts()\nprint('Class 0:', target_count[0])\nprint('Class 1:', target_count[1])\nprint('Proportion:', round(target_count[0] \/ target_count[1], 2), ': 1')\n\ncount_class_N, count_class_P = New_data.label.value_counts()\n\n# Divide by class\nClass_N = New_data.label == 0\nClass_P = New_data.label == 1\n\n#Plot the target\nsb.countplot (x = New_data.label, data = New_data, palette = \"bwr\")\nplt.show()","863c30aa":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\nimport sklearn.metrics as metrics\nfrom sklearn.metrics import classification_report\n\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier as KNN\n\n\n#Create a list of sampling methods\nmodels = []\nmodels.append((\"Random Under-sampling\", rus))\nmodels.append((\"Condensed Nearest Neighbor\", con))\nmodels.append((\"Edited Nearest Neighbor\", enn))\nmodels.append((\"Repeated Edited Nearest Neighbor\", renn))\nmodels.append((\"Cluster Centroids\", cc))\nmodels.append((\"One-sided Selection\", os))\nmodels.append((\"Tomek Links\", tomek))\n\n#Evaluate each model in turn\n\ndef Evaluate(MODEL):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20) \n    knn = KNN()\n    knn.fit(X_train, y_train.values.ravel()) \n    #Making predictions\n    y_pred = knn.predict(X_test) \n    #print(classification_report(y_test, y_pred))\n    Accuracy = metrics.accuracy_score(y_test, y_pred)\n    print(\"Accuracy:\", Accuracy)\n\nfor i in models:\n    print (\"Model:\")  \n    print (i)\n    Evaluate(i)\n    print (\"******************************************\")  ","197cda0e":"#Random over-samplong \nfrom imblearn.over_sampling import RandomOverSampler\n\nros = RandomOverSampler()\nX_ros, y_ros = ros.fit_sample(X, y)\nX_res = pd.DataFrame(X_ros)\ny_res = pd.DataFrame(y_ros)\n\nNew_data = pd.concat([X_res, y_res], axis=0)\n\ntarget_count = New_data.label.value_counts()\nprint('Class 0:', target_count[0])\nprint('Class 1:', target_count[1])\nprint('Proportion:', round(target_count[0] \/ target_count[1], 2), ': 1')\n\ncount_class_N, count_class_P = New_data.label.value_counts()\n\n# Divide by class\nClass_N = New_data.label == 0\nClass_P = New_data.label == 1\n\n#Plot the target\nsb.countplot (x = New_data.label, data = New_data, palette = \"bwr\")\nplt.show()","8d0e5692":"#Over-samplong using Adaptive Synthetic (ADASYN) sampling approach\nfrom imblearn.over_sampling import ADASYN\n\nad = ADASYN()\nX_ad, y_ad = ad.fit_sample(X, y)\nX_res = pd.DataFrame(X_ad)\ny_res = pd.DataFrame(y_ad)\n\nNew_data = pd.concat([X_res, y_res], axis=0)\n\ntarget_count = New_data.label.value_counts()\nprint('Class 0:', target_count[0])\nprint('Class 1:', target_count[1])\nprint('Proportion:', round(target_count[0] \/ target_count[1], 2), ': 1')\n\ncount_class_N, count_class_P = New_data.label.value_counts()\n\n# Divide by class\nClass_N = New_data.label == 0\nClass_P = New_data.label == 1\n\n#Plot the target\nsb.countplot (x = New_data.label, data = New_data, palette = \"bwr\")\nplt.show()","20b5bbd6":"#Over-samplong using Synthetic Minority Over-sampling Technique (SMOTE)\nfrom imblearn.over_sampling import SMOTE\n\nsm = SMOTE()\nX_sm, y_sm = smote.fit_sample(X, y)\nX_res = pd.DataFrame(X_sm)\ny_res = pd.DataFrame(y_sm)\n\nNew_data = pd.concat([X_res, y_res], axis=0)\n\ntarget_count = New_data.label.value_counts()\nprint('Class 0:', target_count[0])\nprint('Class 1:', target_count[1])\nprint('Proportion:', round(target_count[0] \/ target_count[1], 2), ': 1')\n\ncount_class_N, count_class_P = New_data.label.value_counts()\n\n# Divide by class\nClass_N = New_data.label == 0\nClass_P = New_data.label == 1\n\n#Plot the target\nsb.countplot (x = New_data.label, data = New_data, palette = \"bwr\")\nplt.show()","5defd325":"#Over-samplong using Synthetic Minority Over-sampling Technique (SMOTE) with SVM algorithm\nfrom imblearn.over_sampling import SVMSMOTE\n\nsvmsm = SVMSMOTE()\nX_sm, y_sm = svmsm.fit_sample(X, y)\nX_res = pd.DataFrame(X_sm)\ny_res = pd.DataFrame(y_sm)\n\nNew_data = pd.concat([X_res, y_res], axis=0)\n\ntarget_count = New_data.label.value_counts()\nprint('Class 0:', target_count[0])\nprint('Class 1:', target_count[1])\nprint('Proportion:', round(target_count[0] \/ target_count[1], 2), ': 1')\n\ncount_class_N, count_class_P = New_data.label.value_counts()\n\n# Divide by class\nClass_N = New_data.label == 0\nClass_P = New_data.label == 1\n\n#Plot the target\nsb.countplot (x = New_data.label, data = New_data, palette = \"bwr\")\nplt.show()","53c0ebbc":"#Create a list of sampling methods\nmodels = []\nmodels.append((\"Random Over-sampling\", ros))\nmodels.append((\"Adaptive Synthetic (ADASYN)\", ad))\nmodels.append((\"Synthetic Minority Technique (SMOTE)\", sm))\nmodels.append((\"SMOTE with SVM\", svmsm))\n\n#Evaluate each model in turn\n\ndef Evaluate(MODEL):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20) \n    knn = KNN()\n    knn.fit(X_train, y_train.values.ravel()) \n    #Making predictions\n    y_pred = knn.predict(X_test) \n    #print(classification_report(y_test, y_pred))\n    Accuracy = metrics.accuracy_score(y_test, y_pred)\n    print(\"Accuracy:\", Accuracy)\n\nfor i in models:\n    print (\"Model:\")  \n    print (i)\n    Evaluate(i)\n    print (\"******************************************\")  ","c7679894":"#Combination of ver-sampling and under-sampling using SMOTE and cleaning using Edited Nearest Neighbor.\nfrom imblearn.combine import SMOTEENN\n\nsmenn = SMOTEENN()\nX_smenn, y_smenn = smenn.fit_sample(X, y)\nX_res = pd.DataFrame(X_sm)\ny_res = pd.DataFrame(y_sm)\n\nNew_data = pd.concat([X_res, y_res], axis=0)\n\ntarget_count = New_data.label.value_counts()\nprint('Class 0:', target_count[0])\nprint('Class 1:', target_count[1])\nprint('Proportion:', round(target_count[0] \/ target_count[1], 2), ': 1')\n\ncount_class_N, count_class_P = New_data.label.value_counts()\n\n# Divide by class\nClass_N = New_data.label == 0\nClass_P = New_data.label == 1\n\n#Plot the target\nsb.countplot (x = New_data.label, data = New_data, palette = \"bwr\")\nplt.show()","0d7e08f8":"#Combination of ver-sampling and under-sampling using SMOTE and cleaning using Tomek links.\nfrom imblearn.combine import SMOTETomek\n\nsmtomek = SMOTETomek()\nX_smtomek, y_smtomek = smtomek.fit_sample(X, y)\nX_res = pd.DataFrame(X_sm)\ny_res = pd.DataFrame(y_sm)\n\nNew_data = pd.concat([X_res, y_res], axis=0)\n\ntarget_count = New_data.label.value_counts()\nprint('Class 0:', target_count[0])\nprint('Class 1:', target_count[1])\nprint('Proportion:', round(target_count[0] \/ target_count[1], 2), ': 1')\n\ncount_class_N, count_class_P = New_data.label.value_counts()\n\n# Divide by class\nClass_N = New_data.label == 0\nClass_P = New_data.label == 1\n\n#Plot the target\nsb.countplot (x = New_data.label, data = New_data, palette = \"bwr\")\nplt.show()","2731a3f3":"#Create a list of sampling methods\nmodels = []\nmodels.append((\"SMOTE and cleaning using Edited Nearest Neighbor\", smenn))\nmodels.append((\"SMOTE and cleaning using Tomek links\", smtomek))\n\n#Evaluate each model in turn\n\ndef Evaluate(MODEL):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20) \n    knn = KNN()\n    knn.fit(X_train, y_train.values.ravel()) \n    #Making predictions\n    y_pred = knn.predict(X_test) \n    #print(classification_report(y_test, y_pred))\n    Accuracy = metrics.accuracy_score(y_test, y_pred)\n    print(\"Accuracy:\", Accuracy)\n\nfor i in models:\n    print (\"Model:\")  \n    print (i)\n    Evaluate(i)\n    print (\"******************************************\") ","12ec3ee7":"**Comparison of different under-sampling methods**  \nIn this example we have used KNN as claasifier.","14e2b513":"**Under-sampling methods**","4d2f34f8":"# Resampling methods for imbalance data","947e10f0":"**Recommended reading**\n\nThe imbalanced-learn documentation:\nhttp:\/\/contrib.scikit-learn.org\/imbalanced-learn\/stable\/index.html\n\nThe imbalanced-learn GitHub:\nhttps:\/\/github.com\/scikit-learn-contrib\/imbalanced-learn\n\nComparison of the combination of over- and under-sampling algorithms:\nhttp:\/\/contrib.scikit-learn.org\/imbalanced-learn\/stable\/auto_examples\/combine\/plot_comparison_combine.html\n\nChawla, Nitesh V., et al. \"SMOTE: synthetic minority over-sampling technique.\" Journal of artificial intelligence research 16 (2002):\nhttps:\/\/www.jair.org\/media\/953\/live-953-2037-jair.pdf","b7b722d8":"**Combination of over-sampling and under-sampling**","ed8d9188":"**An Overview of resampling methods**","a59a309b":"![Capture.JPG](attachment:Capture.JPG)","f06f8e8d":"**Resampling imbalanced data**","0643a25b":"**Over-sampling methods**","167f9606":"**Comparison of different over-sampling methods**  \nIn this example we have used KNN as claasifier.","39531c4b":"**Comparison of combination methods**","3e25aa1c":"Resampling methods includes under-sampling the majority class, over-sampling the minority class or a combination of both.","1795b8d6":"In binary classification problemas, data imbalance occurs when the number of instances in one class known as the majority class is significantly larger than the other class, known as the minority class. Resampling is a data processing method used to balance the dataset. The \"imbalanced-learn\" toolbox provides methods for resampling the data."}}