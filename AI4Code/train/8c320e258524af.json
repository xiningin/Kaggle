{"cell_type":{"2888654c":"code","941aff29":"code","e3d96d94":"code","209909a1":"code","097ee508":"code","7fb764c9":"code","adb065f6":"code","3f78df0b":"code","3ebe9303":"markdown","28b1d327":"markdown","96d5946f":"markdown","dece3fea":"markdown","a442489f":"markdown","8a1ecb81":"markdown","15d2fd30":"markdown"},"source":{"2888654c":"# General library imports\nimport numpy as np\nimport pandas as pd","941aff29":"# Input data files are available in the \"..\/input\/\" directory.\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n# Read the data\nX = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv', index_col='Id')\nX_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv', index_col='Id')\n\n# Remove rows with missing target, separate target from predictors\nX.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = X.SalePrice\nX.drop(['SalePrice'], axis=1, inplace=True)","e3d96d94":"# Indicate missing values\n# Idea from https:\/\/www.kaggle.com\/mrshih\/here-s-a-house-salad-it-s-on-the-house\ncols_with_missing = [col for col in X.columns if X[col].isnull().any()]\nfor col in cols_with_missing:\n    X[col+'_was_missing']= X[col].isnull()\n    X[col+'_was_missing']= X[col].isnull()\n\n# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\ncategorical_cols = [cname for cname in X.columns if\n                    X[cname].nunique() < 10 and \n                    X[cname].dtype == \"object\"]\n\n# Select numerical columns\nnumerical_cols = [cname for cname in X.columns if \n                X[cname].dtype in ['int64', 'float64']]\n\n# Keep selected columns only\nmy_cols = categorical_cols + numerical_cols\nX = X[my_cols].copy()\nX_test = X_test[my_cols].copy()","209909a1":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, RobustScaler\nfrom sklearn.metrics import mean_absolute_error\nfrom xgboost import XGBRegressor\n\nnumerical_transformer = Pipeline(steps=[\n    ('num_imputer', SimpleImputer(strategy='median')),\n    ('num_scaler', RobustScaler())\n])\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))\n])\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n\nmodel = XGBRegressor(random_state=0, \n                      learning_rate=0.005, n_estimators=1000,\n                      max_depth=4,colsample_bytree=0.5, subsample=0.5,\n                      min_child_weight = 1, gamma = 0, scale_pos_weight = 1)\n\npipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                           ('model', model)\n                          ])","097ee508":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = {'model__n_estimators': np.arange(4400, 5500, 100),\n              #'model__max_depth': np.arange(1, 11, 1),\n              #'model__learning_rate': [0.001, 0.005, 0.01, 0.05, 0.1],\n              #'model__booster':['gbtree'],\n              #'model__min_child_weight':[0, 1, 5, 10, 50],\n              #'model__objective':['reg:squarederror'],\n            #'model__gamma':[0.001, 0.01, 0.1, 1, 10, 100],\n            #'model__max_delta_step':[0, 0.01, 0.1, 1, 10, 100]\n            #'model__subsample':[0.4, 0.45, 0.5, 0.55, 0.6],\n            #'model__colsample_bytree':[0.4, 0.45, 0.5, 0.55],\n            #'model__colsample_bylevel':[0.25,0.5, 0.75, 1],\n            #'model__colsample_bynode':[0.25,0.5, 0.75, 1]\n            #'model__reg_lambda':[0, 0.01, 0.1, 1, 10, 100]\n            #'model__reg_alpha':[0.001, 0.01, 0.1, 1, 10]\n             }\n\n#search = GridSearchCV(pipeline, param_grid, cv=3, n_jobs=8)\n#search.fit(X, y)\n#print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n#print(search.best_params_)\n#print(search.grid_scores_)","7fb764c9":"# score = 14907 @ n_estimators = 3460 (15172 @ 1000)\nmodel = XGBRegressor(random_state=0, \n                      learning_rate=0.01, n_estimators=3460,\n                      max_depth=4,colsample_bytree=0.5, subsample=0.5)\n\n\npipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                           ('model', model)\n                          ])\n\nfrom sklearn.model_selection import cross_val_score\n\n# Multiply by -1 since sklearn calculates *negative* MAE\nscores = -1 * cross_val_score(pipeline, X, y,\n                              cv=5, n_jobs=-1,\n                              scoring='neg_mean_absolute_error')\n\nprint(\"Average MAE score:\", scores.mean())","adb065f6":"pipeline.fit(X, y)\npreds_test = pipeline.predict(X_test)\n","3f78df0b":"output = pd.DataFrame({'Id': X_test.index,\n                       'SalePrice': preds_test})\noutput.to_csv('submission.csv', index=False)","3ebe9303":"## Save test predictions to file","28b1d327":"## Predict test data","96d5946f":"## Validate model\n\nUse GridSearchCV to find optimal model parameters.\n\nMost relevant parameters:\n- n_estimators\n- max_depth\n- learning_rate\n- subsample\n- colsample_bytree\n\nInformation on XGBoost parameters and optimization: https:\/\/www.analyticsvidhya.com\/blog\/2016\/03\/complete-guide-parameter-tuning-xgboost-with-codes-python\/\n","dece3fea":"## Ideas for improvement:\n- stacked regressions\n- increase cardinality limit of categorical variables\n- perform XGBoost optimization using the CV function. This should make the search for optimal n_estimators more efficient.\n\n## Refs:\n- RobustScaler from https:\/\/www.kaggle.com\/jjbuchanan\/submit-instructions-housing-prices-competition","a442489f":"## Data import","8a1ecb81":"In this notebook I have tried to improve my score after finishing the Intermediate Machine Learning Course by optimizing the XGBoost regressor parameters.","15d2fd30":"## Define pipeline"}}