{"cell_type":{"8520dfcd":"code","66bd0327":"code","2c58831f":"code","c69896ff":"code","888bc532":"code","5c62340d":"code","38d7db68":"code","5e3ffe5b":"code","697f8139":"code","fe34128f":"code","b6242719":"code","c428bf56":"code","0e26cc47":"markdown","53994760":"markdown","e1db85f1":"markdown","1c1eeafd":"markdown","8cecd688":"markdown","e721ffe4":"markdown","f1923617":"markdown","d7c5ed62":"markdown"},"source":{"8520dfcd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","66bd0327":"import keras\nfrom keras.datasets import mnist # 28x28 image data written digits 0-9\nfrom keras.utils import normalize\n\n#print(keras.__version__)\n\n#split train and test dataset \n(x_train, y_train), (x_test,y_test) = mnist.load_data()\n\n#normalize data \nx_train = normalize(x_train, axis=1)\nx_test = normalize(x_test, axis=1)","2c58831f":"import matplotlib.pyplot as plt\n\nplt.imshow(x_train[0], cmap=plt.cm.binary)\nplt.show()\n#print(x_train[0])","c69896ff":"from keras.models import Sequential\nfrom keras.layers import Flatten\nfrom keras.layers import Dense \n\n# created model \nmodel = Sequential()\n\n# flatten layer so it is operable by this layer  \nmodel.add(Flatten())\n\n# regular densely-connected NN layer.\n#layer 1, 128 node \nmodel.add(Dense(128, activation='relu'))\n\n#layer 2, 128 node \nmodel.add(Dense(128, activation='relu'))\n\n#output layer, since it is probability distribution we will use 'softmax'\nmodel.add(Dense(10, activation='softmax'))","888bc532":"from keras.callbacks import EarlyStopping\n\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics = ['accuracy'])\n\n#stop when see model not improving \nearly_stopping_monitor = EarlyStopping(monitor='val_loss', patience=2)\n","5c62340d":"model.fit(x_train, y_train, epochs=10, callbacks=[early_stopping_monitor], validation_data=(x_test, y_test))","38d7db68":"val_loss, val_acc = model.evaluate(x_test,y_test)\nprint(val_loss, val_acc)","5e3ffe5b":"model.save('mnist_digit.h5')\nmodel.summary()","697f8139":"from keras.models import load_model\n\nnew_model = load_model('mnist_digit.h5')","fe34128f":"predict = new_model.predict([x_test])\n\n#return the probability \nprint(predict)","b6242719":"print(predict[1].argmax(axis=-1))","c428bf56":"plt.imshow(x_test[1])\nplt.show()","0e26cc47":"### Compile\n--- \nwe have compiled the model with earlystopping callback. when we see there are no improvement on accuracy we will stop compiling. ","53994760":"### Evaluate\n---\nEvaluate the accuracy of the model. ","e1db85f1":"### Load\n----\nLoad the model. ","1c1eeafd":"### Import Data Set & Normalize\n--- \nwe have imported the famoous mnist dataset, it is a 28x28 gray-scale hand written digits dataset. we have loaded the dataset, split the dataset. we also need to normalize the dataset. The original dataset has pixel value between 0 to 255. we have normalized it to 0 to 1. ","8cecd688":"### Save\n--- \nSave the model and show summary. ","e721ffe4":"### Fit\n---\nFit the model with train data, with epochs 10. \n","f1923617":"## Specify Architecture: \n--- \nwe have specified our model architecture. added commonly used densely-connected neural network. For the output node we specified our activation function **softmax** it is a probability distribution function. \n","d7c5ed62":"### Predict \n----\nHere our model predicted the probability distribution, we have to covnert it to classifcation\/label."}}