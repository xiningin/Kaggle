{"cell_type":{"a618f70b":"code","ecfa4a47":"code","ae96c76a":"code","7cc21771":"code","ac3aa449":"code","3675d014":"code","894847a9":"code","8e4d7ae1":"code","503c8116":"code","df86168e":"code","93c632bb":"code","b3efd13a":"code","7b4c8107":"code","7aa207d3":"code","eaf8e60e":"code","aa678c5f":"code","4588dc62":"code","6edc987c":"code","19e39ed9":"code","4eacc512":"code","55a2366f":"code","32e1f675":"code","fb982eb2":"code","37e5ad3f":"code","5a03b406":"code","a0444b1a":"code","263a3a4b":"code","663ec0f2":"code","f693e1a2":"code","112fafaa":"code","1106cc7b":"code","1c2fc292":"code","0a2fe97b":"code","f2871464":"code","7faedcdc":"code","64bef17b":"code","02f68aa2":"code","c8624cb7":"code","c17b7ffd":"code","ef40c23e":"code","78ad1255":"code","4197e045":"code","60946f85":"code","38f4b3cf":"code","63f3aa8b":"code","1895b607":"code","9197f1db":"code","15323f59":"code","1972341b":"code","3e3616d1":"code","a1166fd0":"code","6f980cbf":"code","0557d5fb":"markdown","2ee28d9a":"markdown","d56da0b2":"markdown","bdd7e54c":"markdown","cbd301e5":"markdown","5c577fd8":"markdown","eb5533d0":"markdown","b1f29e86":"markdown","781c6019":"markdown","4312ec86":"markdown","667b8a92":"markdown","8dfecd79":"markdown","09c3bd12":"markdown","ee35a5fb":"markdown","894a7318":"markdown","8b80f624":"markdown","4a7d55ca":"markdown","e691d527":"markdown","262a7166":"markdown","aff368cb":"markdown","ddf56ce3":"markdown","0fe5e9eb":"markdown","9e8a7fa2":"markdown","c40d0534":"markdown","957f6e00":"markdown","f7b4dc1a":"markdown","277166fc":"markdown","16d96e36":"markdown","a507be52":"markdown","59cead86":"markdown","c1b674c8":"markdown","2da63adf":"markdown","6a8bc97d":"markdown","f195675e":"markdown","cfb82a59":"markdown","8c9b4c89":"markdown","9d1ab54e":"markdown","2312a48f":"markdown","01d8278d":"markdown","b877709f":"markdown","9480702a":"markdown"},"source":{"a618f70b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","ecfa4a47":"import json\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\n\n","ae96c76a":"from functools import reduce \nimport numpy as np\n\n# definicion de corpus\ntexts = [['i', 'have', 'a', 'cat'], \n        ['he', 'have', 'a', 'dog'], \n        ['he', 'and', 'i', 'have', 'a', 'cat', 'and', 'a', 'dog']]\n\ndictionary = list(enumerate(set(list(reduce(lambda x, y: x + y, texts)))))\nprint(dictionary)\ndef vectorize(text): \n    vector = np.zeros(len(dictionary)) \n    for i, word in dictionary: \n        num = 0 \n        for w in text: \n            if w == word: \n                num += 1 \n        if num: \n            vector[i] = num \n    return vector\n\nfor t in texts: \n    print(vectorize(t))","7cc21771":"from sklearn.feature_extraction.text import CountVectorizer\n\nvect = CountVectorizer(ngram_range=(1,1))\nvect.fit_transform(['i have no cows','no, i have cows']).toarray()","ac3aa449":"vect.vocabulary_ ","3675d014":"vect = CountVectorizer(ngram_range=(1,2))\nvect.fit_transform(['i have no cows','no, i have cows']).toarray()","894847a9":"vect.vocabulary_","8e4d7ae1":"from scipy.spatial.distance import euclidean\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nvect = CountVectorizer(ngram_range=(3,3), analyzer='char_wb')\n\nn1, n2, n3, n4 = vect.fit_transform(['andersen', 'petersen', 'petrov', 'smith']).toarray()\n\n\neuclidean(n1, n2), euclidean(n2, n3), euclidean(n3, n4)","503c8116":"## Algunos ejemplos utilizar\u00e1n el dataset de la compa\u00f1\u00eda Renthop, que se usa en la competencia \n## Two Sigma Connect: Consultas de listado de alquileres de Kaggle. \n## En esta tarea, debe predecir la popularidad de un nuevo listado de alquiler, es decir, \n### clasificar el listado en tres clases: `['low', 'medium' , 'high']`. Para evaluar las soluciones, \n### \nimport json\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Let's load the dataset from Renthop right away\nwith open('..\/input\/twosigmaconnect\/renthop_train.json', 'r') as raw_data:\n    data = json.load(raw_data)\n    df = pd.DataFrame(data)","df86168e":"df.tail()","93c632bb":"!pip install reverse_geocoder\nimport reverse_geocoder as revgc\n\n","b3efd13a":"revgc.search([df.latitude[1], df.longitude[2]])","7b4c8107":"df['dow'] = df['created'].apply(lambda x: pd.to_datetime(x).weekday())\ndf['is_weekend'] = df['created'].apply(lambda x: 1 if pd.to_datetime(x).weekday() in (5, 6) else 0)","7aa207d3":"#df['is_weekend']","eaf8e60e":"def make_harmonic_features(value, period=24):\n    value *= 2 * np.pi \/ period \n    return np.cos(value), np.sin(value)","aa678c5f":"#import numpy as np\nimport matplotlib.pyplot as plt\nxx=np.arange(0,24,1)\nX=[]\nY=[]\nfor i in xx:\n    x,y=make_harmonic_features(i)\n    X.append(x)\n    Y.append(y)\n    \nplt.plot(X, Y)\nplt.show()","4588dc62":"from scipy.spatial import distance\neuclidean(make_harmonic_features(23), make_harmonic_features(1)) ","6edc987c":"euclidean(make_harmonic_features(9), make_harmonic_features(11)) ","19e39ed9":"euclidean(make_harmonic_features(9), make_harmonic_features(21))","4eacc512":"!pip install -q pyyaml ua-parser user-agents\nimport user_agents\n\nua = 'Mozilla\/5.0 (Macintosh; Intel Mac OS X 10_15_0) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/81.0.4044.113 Safari\/537.36'\nua = user_agents.parse(ua)\n\nprint('Is a bot? ', ua.is_bot)\nprint('Is mobile? ', ua.is_mobile)\nprint('Is PC? ',ua.is_pc)\nprint('OS Family: ',ua.os.family)\nprint('OS Version: ',ua.os.version)\nprint('Browser Family: ',ua.browser.family)\nprint('Browser Version: ',ua.browser.version)","55a2366f":"import numpy as np\nplt.style.use('seaborn-whitegrid')\nsize = 100\nx = np.linspace(0, 10, size) \ny = x**2 + 10 - (20 * np.random.random(size))","32e1f675":"plt.figure(figsize=(10,5))\nplt.plot(x,y,'o')","fb982eb2":"import lightgbm as lgb\noverfit_model = lgb.LGBMRegressor(silent=False, min_child_samples=5)\noverfit_model.fit(x.reshape(-1,1), y)\n \n#predicted output from the model from the same input\nprediction = overfit_model.predict(x.reshape(-1,1))\n","37e5ad3f":"plt.figure(figsize=(10,5))\nplt.plot(x,y,'o')\nplt.plot(x,prediction,color='r')","5a03b406":"monotone_model = lgb.LGBMRegressor(min_child_samples=5, \n                                   monotone_constraints=\"1\")\nmonotone_model.fit(x.reshape(-1,1), y)","a0444b1a":"#predicted output from the model from the same input\nprediction = monotone_model.predict(x.reshape(-1,1))","263a3a4b":"plt.figure(figsize=(10,5))\nplt.plot(x,y,'o')\nplt.plot(x,prediction,color='r')","663ec0f2":"from sklearn.metrics import mean_squared_error as mse\n \nsize = 1000000\nx = np.linspace(0, 10, size) \ny = x**2  -10 + (20 * np.random.random(size))\n \nprint (\"MSE del modelo por defecto\", mse(y, overfit_model.predict(x.reshape(-1,1))))\nprint (\"MSE del modelo Monotono\", mse(y, monotone_model.predict(x.reshape(-1,1))))","f693e1a2":"# EJEMPLO: usando Shapiro-Wilk - Test\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.stats import beta\nfrom scipy.stats import shapiro\nimport numpy as np\n\n#Generamos 1000 n\u00fameros aleatorios [1,10]\ndata = beta(1, 10).rvs(1000).reshape(-1, 1)\n\n# Realizamos la prueba de Shapiro-Wilk para la normalidad.\nshapiro_stat,shapiro_p_value=shapiro(data)\n\n#conclusi\u00f3n\nif shapiro_p_value > 0.05:\n    print('con 95% de confianza los datos son similares a una distribuci\u00f3n normal')\nelse:\n    print('con 95% de confianza los datos NO son similares a una distribuci\u00f3n normal')","112fafaa":"#\nshapiro_stat,shapiro_p_value=shapiro(StandardScaler().fit_transform(data))\n\n# Con el valor p tendr\u00edamos que rechazar la hip\u00f3tesis nula de normalidad de los datos.\n#conclusi\u00f3n\nif shapiro_p_value > 0.05:\n    print('con 95% de confianza los datos son similares a una distribuci\u00f3n normal')\nelse:\n    print('con 95% de confianza los datos NO son similares a una distribuci\u00f3n normal')","1106cc7b":"from sklearn.preprocessing import MinMaxScaler\nshapiro_stat,shapiro_p_value=shapiro(MinMaxScaler().fit_transform(data))\n\n# Con el valor p tendr\u00edamos que rechazar la hip\u00f3tesis nula de normalidad de los datos.\n#conclusi\u00f3n\nif shapiro_p_value > 0.05:\n    print('con 95% de confianza los datos son similares a una distribuci\u00f3n normal')\nelse:\n    print('con 95% de confianza los datos NO son similares a una distribuci\u00f3n normal')\n","1c2fc292":"(data - data.min()) \/ (data.max() - data.min()) ","0a2fe97b":"from scipy.stats import lognorm\n\ndata = lognorm(s=1).rvs(1000)\nshapiro_stat,shapiro_p_value=shapiro(data)\n#conclusi\u00f3n\nif shapiro_p_value > 0.05:\n    print('con 95% de confianza los datos son similares a una distribuci\u00f3n normal')\nelse:\n    print('con 95% de confianza los datos NO son similares a una distribuci\u00f3n normal')","f2871464":"# graficando\nplt.figure(figsize=(10,5))\nplt.hist(data, bins=50)\n","7faedcdc":"shapiro_stat,shapiro_p_value=shapiro(np.log(data))\n#conclusi\u00f3n\nif shapiro_p_value > 0.05:\n    print('con 95% de confianza los datos son similares a una distribuci\u00f3n normal')\nelse:\n    print('con 95% de confianza los datos NO son similares a una distribuci\u00f3n normal')","64bef17b":"# graficando\nplt.figure(figsize=(10,5))\nplt.hist(np.log(data), bins=50)","02f68aa2":"# \u00a1Dibujemos !\nimport statsmodels.api as sm\n\n\n# Tomemos la caracter\u00edstica de precio del dataset de Renthop y filtremos los valores m\u00e1s extremos para mayor claridad.\nprice = df.price[(df.price <= 20000) & (df.price > 500)]\nprice_log = np.log(price)\n\n# usamos transformaciones\nprice_mm = MinMaxScaler().fit_transform(price.values.reshape(-1, 1).astype(np.float64)).flatten()\nprice_z = StandardScaler().fit_transform(price.values.reshape(-1, 1).astype(np.float64)).flatten()","c8624cb7":"sm.qqplot(price, loc=price.mean(), scale=price.std())","c17b7ffd":"sm.qqplot(price_z, loc=price_z.mean(), scale=price_z.std())","ef40c23e":"sm.qqplot(price_mm, loc=price_mm.mean(), scale=price_mm.std())","78ad1255":"sm.qqplot(price_log, loc=price_log.mean(), scale=price_log.std())","4197e045":"rooms = df[\"bedrooms\"].apply(lambda x: max(x, .5))\n# Evitar la divisi\u00f3n por cero; .5 se elige m\u00e1s o menos arbitrariamente\ndf[\"price_per_bedroom\"] = df[\"price\"] \/ rooms","60946f85":"df[\"price_per_bedroom\"]","38f4b3cf":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom sklearn import feature_extraction, model_selection, naive_bayes, metrics, svm\nfrom IPython.display import Image\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline  ","63f3aa8b":"data = pd.read_csv('..\/input\/sms-spam-collection-dataset\/spam.csv', encoding='latin-1')\ndata.head(n=10)","1895b607":"count1 = Counter(\" \".join(data[data['v1']=='ham'][\"v2\"]).split()).most_common(10)\ndf1 = pd.DataFrame.from_dict(count1)\nprint(df1.head())\ndf1 = df1.rename(columns={0: \"palabras non-spam\", 1 : \"count\"})\ncount2 = Counter(\" \".join(data[data['v1']=='spam'][\"v2\"]).split()).most_common(10)\ndf2 = pd.DataFrame.from_dict(count2)\ndf2 = df2.rename(columns={0: \"palabras spam\", 1 : \"count_\"})","9197f1db":"df1.plot.bar(legend = False)\ny_pos = np.arange(len(df1[\"palabras non-spam\"]))\nplt.xticks(y_pos, df1[\"palabras non-spam\"])\nplt.title('Palabras frecuentes en mensajes no-spam')\nplt.xlabel('Palabras')\nplt.ylabel('Numero')\nplt.show()","15323f59":"df2.plot.bar(legend = False, color = 'orange')\ny_pos = np.arange(len(df2[\"palabras spam\"]))\nplt.xticks(y_pos, df2[\"palabras spam\"])\nplt.title('Palabras frecuentes en mensajes spam ')\nplt.xlabel('Palabras')\nplt.ylabel('numero')\nplt.show()\n","1972341b":"#usando CountVectorizer\n#f = feature_extraction.text.CountVectorizer(stop_words = 'english')\n#feat = feature_extraction.text.CountVectorizer(stop_words = 'english', ngram_range=(1,2)) ## usando n_gram?\nfeat = feature_extraction.text.CountVectorizer(stop_words = 'english')\n\nX = feat.fit_transform(data[\"v2\"])\n\nnp.shape(X)","3e3616d1":"data[\"v1\"]=data[\"v1\"].map({'spam':1,'ham':0})\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, data['v1'], test_size=0.2, random_state=42)\nprint([np.shape(X_train), np.shape(X_test)])","a1166fd0":"# usamos Support Vector Machine de :https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html\nsvc = svm.SVC()\nsvc.fit(X_train, y_train)\nscore_train = svc.score(X_train, y_train)\nscore_test = svc.score(X_test, y_test)","6f980cbf":"# para validar debe usar una matriz de confusi\u00f3n usando el siguiente c\u00f3digo:\nmatr_confusion_test = metrics.confusion_matrix(y_test, svc.predict(X_test))\npd.DataFrame(data = matr_confusion_test, columns = ['Prediccion spam', 'Prediccion no-spam'],\n            index = ['Real spam', 'Real no-spam'])","0557d5fb":"### Interactions\n\nSi las transformaciones anteriores parec\u00edan orientadas a las matem\u00e1ticas, esta parte trata m\u00e1s sobre la naturaleza de los datos; se puede atribuir tanto a las transformaciones de caracter\u00edsticas como a la creaci\u00f3n de caracter\u00edsticas.\n\nIf previous transformations seemed rather math-driven, this part is more about the nature of the data; it can be attributed to both feature transformations and feature creation.\n\nVolvamos nuevamente al problema del dataset de Renthop: Consultas de listado de alquileres. Entre las caracter\u00edsticas de este problema est\u00e1n la `cantidad de habitaciones` y el `precio`. La l\u00f3gica sugiere que el costo por habitaci\u00f3n individual es m\u00e1s sugerente que el costo total, por lo que podemos generar dicha caracter\u00edstica.\n","2ee28d9a":"### Rellenando valores faltantes\n\nNo muchos algoritmos pueden funcionar con valores faltantes, y data real a menudo se proporciona datos incompletos. Afortunadamente, esta es una de las tareas para las que no se necesita creatividad. Ambas bibliotecas de python para el an\u00e1lisis de datos proporcionan soluciones f\u00e1ciles de usar: [pandas.DataFrame.fillna](http:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.fillna.html) y [sklearn.preprocessing.Imputer](http:\/\/scikit-learn.org\/stable\/modules\/preprocessing.html#imputation).\n\nEstas soluciones no tienen ninguna magia detr\u00e1s de escena. Los enfoques para manejar los valores faltantes son bastante sencillos:\n\n* codificar valores faltantes con un valor en blanco separado como `\"n\/a\"` (para variables categ\u00f3ricas);\n* usa el valor m\u00e1s probable de la caracter\u00edstica (media o mediana para las variables num\u00e9ricas, el valor m\u00e1s com\u00fan para las variables categ\u00f3ricas);\n* o, por el contrario, codificar con alg\u00fan valor extremo (bueno para los modelos decision-tree, ya que permite que el modelo haga una partici\u00f3n entre los valores faltantes y los no faltantes);\n* para los datos ordenados (por ejemplo, series de tiempo), tome el valor adyacente: siguiente o anterior.\n\n\n![image](https:\/\/cdn-images-1.medium.com\/max\/800\/0*Ps-v8F0fBgmnG36S.)\n\nLas soluciones de biblioteca f\u00e1ciles de usar a veces sugieren apegarse a algo como `df = df.fillna (0)` y no preocuparse por las brechas. Pero esta no es la mejor soluci\u00f3n: la preparaci\u00f3n de datos lleva m\u00e1s tiempo que la construcci\u00f3n de modelos, por lo que el relleno irreflexivo puede ocultar un error en el procesamiento y da\u00f1ar el modelo.\n\n","d56da0b2":"<center>\n<img src=\"https:\/\/drive.google.com\/uc?export=download&id=1nv5uGKO9BLD9Y19LnZZH35nnQghZsPdD\" \/>\n\n# Feature Engineering and Feature Selection\n\nPara empezar, vamos a revisar tres tareas similares pero diferentes: \n\n* **feature extraction** and **feature engineering**: Transformaci\u00f3n de data(raw) en caracter\u00edsticas adecuadas para modelado;\n* **feature transformation**: Transformaci\u00f3n de data para mejorar la precisi\u00f3n de los algoritmos;\n* **feature selection**: Removiendo caracter\u00edsticas innecesarias.","bdd7e54c":"### **Y si verificamos el rendimiento del modelo, podemos ver que la restricci\u00f3n de monotonicidad no s\u00f3lo proporciona un ajuste m\u00e1s natural, sino que el modelo tambi\u00e9n generaliza mejor (como se esperaba). Al medir el error cuadr\u00e1tico medio MSE en los nuevos datos de prueba, vemos que el error es menor para el modelo mon\u00f3tono.**","cbd301e5":"### **feature extraction and feature engineering **\n\n1. Preprocesamiento de texto \n2. Creaci\u00f3n de tokens y el filtrado de palabras clave \n(puede usar un componente de alto nivel como: CountVectorizer que puede crear un diccionario de caracter\u00edsticas y transformar documentos en vectores de caracter\u00edsticas.)","5c577fd8":"## Temas\n\n1. [Feature Extraction](#1.-Feature-Extraction)\n - [Texts](#Texts)\n - [Geospatial data](#Geospatial-data)\n - [Date and time](#Date-and-time)\n - [Time series, web, etc.](#Time-series,-web,-etc.)\n2. [Feature transformations](#Feature-transformations)\n - [Normalization and changing distribution](#Normalization-and-changing-distribution)","eb5533d0":"`StandardScaling` y `MinMaxScaling` tienen aplicaciones similares y, a menudo, son m\u00e1s o menos intercambiables. Sin embargo, si el algoritmo implica el c\u00e1lculo de distancias entre puntos o vectores, la opci\u00f3n predeterminada es `StandardScaling`. Pero `MinMaxScaling` es \u00fatil para la visualizaci\u00f3n al incorporar caracter\u00edsticas dentro del intervalo (0, 255).\n\nSi suponemos que algunos datos no se distribuyen normalmente, sino que se describen mediante [la distribuci\u00f3n log-normal ](https:\/\/en.wikipedia.org\/wiki\/Log-normal_distribution), se pueden transformar f\u00e1cilmente en una distribuci\u00f3n normal\n\n","b1f29e86":"## 1. Feature Extraction\n\n\nEn la pr\u00e1ctica, los datos rara vez se presentan en forma de matrices listas para usar. Es por eso que cada tarea comienza con la extracci\u00f3n de caracter\u00edsticas. A veces, puede ser suficiente leer el archivo .csv y convertirlo en `numpy.array`, pero esta es una rara excepci\u00f3n. Veamos algunos de los tipos populares de datos de los que se pueden extraer caracter\u00edsticas.\n","781c6019":"### Debes limitarte en este proceso. \n\nSi hay un n\u00famero limitado de caracter\u00edsticas, es posible generar todas las interacciones posibles y luego eliminar las innecesarias utilizando las t\u00e9cnicas descritas en la siguiente secci\u00f3n. Adem\u00e1s, no todas las interacciones entre caracter\u00edsticas deben tener un significado f\u00edsico; por ejemplo, las caracter\u00edsticas polin\u00f3micas (ver [sklearn.preprocessing.PolynomialFeatures](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.PolynomialFeatures.html)) a menudo se usan en modelos lineales y son casi imposibles de interpretar.\n","4312ec86":"Tambi\u00e9n hay razones puramente de ingenier\u00eda: `np.log` es una forma de tratar con grandes n\u00fameros que no caben en `np.float64`. Esta es una excepci\u00f3n m\u00e1s que una regla; a menudo es impulsado por el deseo de adaptar el dataset a los requisitos del algoritmo. Los m\u00e9todos param\u00e9tricos generalmente requieren un m\u00ednimo de distribuci\u00f3n sim\u00e9trica y unimodal de datos, que no siempre se da en datos reales.\n\nSin embargo, los requisitos de datos se imponen no s\u00f3lo por m\u00e9todos param\u00e9tricos; [K nearest neighbors](https:\/\/medium.com\/open-machine-learning-course\/open-machine-learning-course-topic-3-classification-decision-trees-and-k-nearest-neighbors-8613c6b6d2cd) predecir\u00e1 salidas sin sentido completo si las caracter\u00edsticas no est\u00e1n normalizadas, por ejemplo cuando una distribuci\u00f3n se encuentra cerca de cero y no va m\u00e1s all\u00e1 (-1, 1).\n\n### Un ejemplo simple: suponga que la tarea es predecir el costo de un apartamento a partir de dos variables: la `distancia desde el centro de la ciudad` y la `cantidad de habitaciones`. El n\u00famero de habitaciones rara vez excede de 5, mientras que la distancia desde el centro de la ciudad puede ser f\u00e1cilmente de miles de metros.\n\n\nLa transformaci\u00f3n m\u00e1s simple es Standard Scaling (o la normalizaci\u00f3n Z-score):\n\n$$ \\large z= \\frac{x-\\mu}{\\sigma} $$\n\nTenga en cuenta que Standard Scaling no hace que la distribuci\u00f3n sea normal en sentido estricto.","667b8a92":"En el siguiente ejemplo, entrenaremos un modelos que usa `LightGBM` en un conjunto de datos *toy dataset* donde sabemos que la relaci\u00f3n entre X e Y es mon\u00f3tona (pero con ruido) y comparamos el modelo predeterminado y el monotono.","8dfecd79":"Agregando a la idea de Bag of Words: las palabras que rara vez se encuentran en el corpus (en todos los documentos del dataset) pero que est\u00e1n presentes en un documento en particular podr\u00edan ser m\u00e1s importantes. Entonces tiene sentido aumentar el peso de m\u00e1s palabras espec\u00edficas del dominio para separarlas de las palabras comunes. Este enfoque se llama TF-IDF (term frequency-inverse document frequency), que no se puede escribir en unas pocas l\u00edneas, por lo que debe consultar los detalles en referencias como [wiki](https:\/\/en.wikipedia.org\/wiki\/Tf%E2%80%93idf). La opci\u00f3n predeterminada es la siguiente:\n\n<img src=\"https:\/\/drive.google.com\/uc?export=download&id=1zRnAL7xslzRl3odfsLa3yzbSR9SMw0CM\" \/>","09c3bd12":"## Q-Q plot despu\u00e9s de MinMaxScaler... la forma tampoco cambia","ee35a5fb":"### Date and time\n\nSe podr\u00eda pensar que la fecha y la hora est\u00e1n estandarizadas debido a su prevalencia, pero, sin embargo, persisten algunas dificultades.\n\n\nComencemos con el d\u00eda de la semana, que son f\u00e1ciles de convertir en 7 variables ficticias utilizando one-hot encoding. Adem\u00e1s, tambi\u00e9n crearemos una funci\u00f3n binaria separada para el fin de semana llamada `is_weekend`.\n\n","894a7318":"## Q-Q plot despu\u00e9s de StandardScaler......la forma no cambia","8b80f624":"### **Librerias**","4a7d55ca":"### **An\u00e1lisis Predictivo**\n\n1. El objetivo es predecir si un nuevo sms es spam o no-spam. usando SVM\n2. validar: Matriz de confusi\u00f3n ","e691d527":"## Feature transformations\n\n### Normalization and changing distribution\n\nLa transformaci\u00f3n monot\u00f3nica de caracter\u00edsticas es cr\u00edtica para algunos algoritmos y no tiene efecto en otros. Esta es una de las razones de la mayor popularidad de los \u00e1rboles de decisi\u00f3n `decision tree`  y todos sus algoritmos derivados (random forest, gradient boosting). No todos pueden o quieren jugar con las transformaciones, y estos algoritmos son robustos para distribuciones inusuales.\n\n\n\n","262a7166":"Al trabajar con geocodificaci\u00f3n, no debemos olvidar que las direcciones pueden contener errores tipogr\u00e1ficos, lo que hace que la limpieza de datos sea necesario. La posici\u00f3n de las coordenadas pueden ser incorrectas debido al ruido del GPS o la mala precisi\u00f3n en lugares como t\u00faneles, \u00e1reas del centro, etc. Si la fuente de datos es un dispositivo m\u00f3vil, la ubicaci\u00f3n geogr\u00e1fica no puede determinarse por GPS sino por redes WiFi en el \u00e1rea, que conduce a agujeros en el espacio y la teletransportaci\u00f3n. Mientras viaja por Arequipa, de repente puede haber una ubicaci\u00f3n WiFi desde Lima.\n\n\nUn punto generalmente se ubica entre una infraestructura. Aqu\u00ed, puede liberar su imaginaci\u00f3n e inventar caracter\u00edsticas basadas en su experiencia de vida y conocimiento de dominio: la proximidad de un punto a una estaci\u00f3n de servicio, la distancia a la tienda m\u00e1s cercana, la cantidad de cajeros autom\u00e1ticos alrededor, etc. Para cualquier tarea, puede crear f\u00e1cilmente docenas de funciones y extraerlas de varias fuentes externas. Para problemas fuera de un entorno urbano, puede considerar caracter\u00edsticas de fuentes m\u00e1s espec\u00edficas, por ejemplo la altura sobre el nivel del mar.\n\nSi dos o m\u00e1s puntos est\u00e1n interconectados, puede valer la pena extraer caracter\u00edsticas de la ruta entre ellos. En ese caso, ser\u00e1n \u00fatiles las distancias ( distancia de carretera calculada por el gr\u00e1fico de ruta), n\u00famero de giros con la proporci\u00f3n de giros de izquierda a derecha, n\u00famero de sem\u00e1foros, cruces y puentes. ","aff368cb":"### **An\u00e1lisis de texto**\n1.  Graficar y encontrar la frecuencia de las palabras en los mensajes spam y no-spam(ham)\n2.  Describir cada gr\u00e1fico\n","ddf56ce3":"### **Explorando el Dataset**","0fe5e9eb":"## Pero: ","9e8a7fa2":"### Texts\n\nEl texto es un tipo de datos que puede venir en diferentes formatos, revisaremos los m\u00e1s populares.\n\nAntes de trabajar con texto, hay que tokenizarlo. La tokenizaci\u00f3n implica dividir el texto en unidades (tokens). Los tokens son s\u00f3lo las palabras. Pero el dividir por palabras nos puede llevar a perder parte del significado-- \"Santa B\u00e1rbara\" es un token, no dos, pero \"rock'n'roll\" no debe dividirse en dos token. Hay tokenizadores listos para usar que tienen en cuenta las peculiaridades del lenguaje, pero tambi\u00e9n cometen errores, especialmente cuando trabajas con fuentes de texto espec\u00edficas (peri\u00f3dicos, jerga, errores ortogr\u00e1ficos, errores tipogr\u00e1ficos).\n\nDespu\u00e9s de la tokenizaci\u00f3n, normalizamos los datos. Para el texto, se trata de la derivaci\u00f3n y\/o lematizaci\u00f3n; Estos son procesos similares utilizados para procesar diferentes formas de una palabra. Se puede leer sobre la diferencia entre ellos [aqui](http:\/\/nlp.stanford.edu\/IR-book\/html\/htmledition\/stemming-and-lemmatization-1.html).\nEntonces, ahora que hemos convertido el documento en una secuencia de palabras, podemos representarlo con vectores. El enfoque m\u00e1s f\u00e1cil se llama Bag of Words: creamos un vector con la longitud del diccionario, calculamos el n\u00famero de ocurrencias de cada palabra en el texto y colocamos ese n\u00famero de ocurrencias en la posici\u00f3n apropiada en el vector. El proceso descrito parece m\u00e1s simple en el c\u00f3digo:\n\n","c40d0534":"Usando estos algoritmos, es posible obtener una soluci\u00f3n para un problema simple, que puede servir como l\u00ednea base. Sin embargo, para aquellos a quienes no les gustan los cl\u00e1sicos, hay nuevos enfoques. Un m\u00e9todo popular es Word2Vec, pero tambi\u00e9n hay algunas alternativas (GloVe, Fasttext, etc.).\n\nWord2Vec es un caso especial de los algoritmos word embedding. Usando Word2Vec y modelos similares, no s\u00f3lo podemos vectorizar palabras en un espacio de alta dimensi\u00f3n (t\u00edpicamente unos pocos cientos de dimensiones) sino tambi\u00e9n comparar su similitud sem\u00e1ntica. Este es un ejemplo cl\u00e1sico de operaciones que se pueden realizar en conceptos vectorizados: **king - man + woman = queen.**\n\n![image](https:\/\/cdn-images-1.medium.com\/max\/800\/1*K5X4N-MJKt8FGFtrTHwidg.gif)","957f6e00":"Esta transformaci\u00f3n preserva la distancia entre puntos, lo cual es importante para los algoritmos que estiman la distancia (kNN, SVM, k-means ...)","f7b4dc1a":"La distribuci\u00f3n lognormal es adecuada para describir: salarios, precios de seguro, la poblaci\u00f3n urbana, la cantidad de comentarios sobre art\u00edculos en Internet, etc. Sin embargo, para aplicar este procedimiento, la distribuci\u00f3n subyacente no necesariamente tiene que ser `lognormal`; puede intentar aplicar esta transformaci\u00f3n a cualquier distribuci\u00f3n con una cola derecha sesgada. Adem\u00e1s, se puede tratar de usar otras transformaciones similares, formulando sus propias hip\u00f3tesis sobre c\u00f3mo aproximar la distribuci\u00f3n disponible a una normal. Ejemplos de tales transformaciones son [transformaci\u00f3n Box-Cox](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.stats.boxcox.html) (el logaritmo es un caso especial de la transformaci\u00f3n Box-Cox ) o [transformaci\u00f3n de Yeo-Johnson](https:\/\/gist.github.com\/mesgarpour\/f24769cd186e2db853957b10ff6b7a95) (extiende el rango de aplicabilidad a n\u00fameros negativos). Adem\u00e1s, tambi\u00e9n puede intentar agregar una constante a la funci\u00f3n: `np.log (x + const)`.\n\nEn los ejemplos anteriores, hemos trabajado con datos sint\u00e9ticos y probamos estrictamente la normalidad utilizando la prueba de Shapiro-Wilk. Intentemos ver algunos datos reales y comprobar la normalidad utilizando un m\u00e9todo menos formal: [Q-Q plot](https:\/\/en.wikipedia.org\/wiki\/Q%E2%80%93Q_plot). Para una distribuci\u00f3n normal, se ver\u00e1 como una l\u00ednea diagonal suave, y las anomal\u00edas visuales deben ser intuitivamente comprensibles.\n\n\n\n![image](https:\/\/habrastorage.org\/webt\/on\/bk\/qg\/onbkqg1j1tdcj9kc4txfjv6soco.png)\nQ-Q plot para distribuci\u00f3n lognormal\n\n![image](https:\/\/habrastorage.org\/webt\/cs\/vq\/xw\/csvqxwpf023p16m4pu6zrndynvm.png)\nQ-Q plot para la misma distribuci\u00f3n despu\u00e9s de tomar el logaritmo","277166fc":"### Time series, web, etc.\n\nCon respecto a las series de tiempo--time series--: no entraremos en demasiados detalles aqu\u00ed, pero podemos recomendar [biblioteca \u00fatil que genera autom\u00e1ticamente caracter\u00edsticas para series de tiempo](https:\/\/github.com\/blue-yonder\/tsfresh).\n\nSi est\u00e1 trabajando con datos web, generalmente tiene informaci\u00f3n sobre el 'User-Agent'. Es una gran cantidad de informaci\u00f3n. Primero, uno necesita extraer el sistema operativo de este. En segundo lugar, crear una funci\u00f3n `is_mobile`. Tercero, mirar el navegador.\n","16d96e36":"La siguiente informaci\u00f3n \u00fatil es la direcci\u00f3n IP, desde la cual puede extraer el pa\u00eds y posiblemente la ciudad, el proveedor y el tipo de conexi\u00f3n (m\u00f3vil \/ estacionario). Se debe comprender que existe una variedad de bases de datos proxy y obsoletas, por lo que esta caracter\u00edstica puede contener ruido. Los gur\u00fas de la administraci\u00f3n de redes pueden intentar extraer caracter\u00edsticas a\u00fan m\u00e1s sofisticadas como  [VPN](https:\/\/habrahabr.ru\/post\/216295\/).Por cierto, los datos de la direcci\u00f3n IP se combinan bien con`http_accept_language`: si el usuario est\u00e1 sentado en los servidores proxy de Chile y la configuraci\u00f3n regional del navegador es `ru_RU`, algo no est\u00e1 bien y vale la pena echarle un vistazo a la columna correspondiente de la tabla (`is_traveler_or_proxy_user`).","a507be52":"### **Ejercicio: Clasificaci\u00f3n de Spam usando Support Vector Machines.(SOLUCI\u00f3N)**\n#### CONTEXTO:    SMS Spam Collection es un conjunto de mensajes etiquetados SMS que se han recopilado para la investigaci\u00f3n de SMS Spam. Contiene un conjunto de mensajes SMS en ingl\u00e9s 5,574 mensajes etiquetados de acuerdo a su contenido 'ham' (leg\u00edtimo) o 'spam'.\n\nAGRADECIMIENTO:  El dataset original se puede encontrar [aqu\u00ed](https:\/\/archive.ics.uci.edu\/ml\/datasets\/SMS+Spam+Collection). Los creadores desean tener en cuenta que en caso de que encuentre \u00fatil el conjunto de datos, haga referencia al documento anterior y la p\u00e1gina web: http:\/\/www.dt.fee.unicamp.br\/~tiago\/smsspamcollection\/\n\nCONTENIDO: Los archivos contienen un mensaje por l\u00ednea. Cada l\u00ednea est\u00e1 compuesta por dos columnas: v1 contiene la etiqueta (ham o spam) y v2 contiene el texto sin formato. Este corpus se ha recopilado de forma gratuita.","59cead86":"Vale la pena se\u00f1alar que este modelo no comprende el significado de las palabras, simplemente trata de posicionar los vectores de manera que las palabras utilizadas en el contexto com\u00fan est\u00e9n cerca unas de otras.\n\nDichos modelos necesitan ser entrenados en data sets muy grandes para que las coordenadas del vector capturen la sem\u00e1ntica. Se puede descargar un modelo previamente entrenado para sus propias tareas\n[aqu\u00ed](https:\/\/github.com\/3Top\/word2vec-api#where-to-get-a-pretrained-models).","c1b674c8":"El modelo se sobreajustar\u00e1 ligeramente (debido valor peque\u00f1o de 'min_child_samples'), lo que podemos ver al trazar los valores de X contra los valores predichos de Y: la l\u00ednea roja no es mon\u00f3tona como nos gustar\u00eda que fuera.","2da63adf":"Tambi\u00e9n tenga en cuenta que uno no tiene que usar s\u00f3lo palabras. En algunos casos, es posible generar N-gram de caracteres. Este enfoque podr\u00eda dar cuenta de la similitud de palabras relacionadas o manejar errores tipogr\u00e1ficos.","6a8bc97d":"## Q-Q plot despu\u00e9s de tomar el logaritmo. \u00a1Las cosas est\u00e1n mejorando!","f195675e":"### Vamos a ajustar un modelo gradient boosted en estos datos, estableciendo min_child_samples = 5.","cfb82a59":"Esta es una ilustraci\u00f3n del proceso:\n<img src=\"https:\/\/drive.google.com\/uc?export=download&id=18dEqfTKT10i5_EJz4MLy_hywpQ_IusUJ\" \/>\n\nEsta es una implementaci\u00f3n extremadamente ingenua. En la pr\u00e1ctica, debe considerar palabras de parada, la longitud m\u00e1xima del diccionario, estructuras de datos m\u00e1s eficientes (generalmente los datos de texto se convierten en un matrices esparsa), etc.\n\nCuando utilizamos algoritmos como Bag of Words, perdemos el orden de las palabras en el texto, lo que significa que los textos \"i have no cows\" y \"no, i have cows\" aparecer\u00e1n id\u00e9nticos despu\u00e9s de la vectorizaci\u00f3n cuando, de hecho, tienen el significado opuesto. Para evitar este problema, podemos volver a visitar nuestro paso de tokenizaci\u00f3n y usar N-grams (la *secuencia* de N tokens consecutivos) en su lugar.\n\n","8c9b4c89":"Algunas tareas pueden requerir funciones de calendario adicionales. Por ejemplo, los retiros de efectivo se pueden vincular a un d\u00eda de pago. En general, cuando se trabaja con datos de series de tiempo, es una buena idea tener un calendario con d\u00edas festivos, condiciones clim\u00e1ticas anormales y otros eventos importantes.\n\n\n> Q: \u00bfQu\u00e9 tienen en com\u00fan el A\u00f1o Nuevo chino, la marat\u00f3n de Nueva York ?\n\n> A: Todos deben colocarse en el calendario de posibles anomal\u00edas..\n\nTratar con la hora (minuto, d\u00eda del mes ...) no es tan simple como parece. Si utiliza la hora como una variable real, contradecimos ligeramente la naturaleza de los datos: `0 <23` mientras que `0:00:00 02.01 > 01.01 23:00:00`. Para algunos problemas, esto puede ser cr\u00edtico. Al mismo tiempo, si las codifica como variables categ\u00f3ricas, generar\u00e1 una gran cantidad de caracter\u00edsticas y perder\u00e1 informaci\u00f3n sobre la proximidad\n\nTambi\u00e9n existen algunos enfoques m\u00e1s esot\u00e9ricos para tales datos, como proyectar el tiempo en un c\u00edrculo y usar las dos coordenadas.","9d1ab54e":"### Geospatial data\n\nLos datos geogr\u00e1ficos no se encuentran tan a menudo en los problemas, pero sigue siendo \u00fatil dominar las t\u00e9cnicas b\u00e1sicas para trabajar con ellos, especialmente porque hay bastantes soluciones listas para usar en este campo.\n\nLos datos geoespaciales a menudo se presentan en forma de direcciones o coordenadas de (Latitud, Longitud). Dependiendo de la tarea, es posible que necesite dos operaciones mutuamente inversas: geocodificaci\u00f3n (recuperar un punto de una direcci\u00f3n) y geocodificaci\u00f3n inversa (recuperar una direcci\u00f3n de un punto). Ambas operaciones son accesibles en la pr\u00e1ctica a trav\u00e9s de API externas de Google Maps u OpenStreetMap. Los diferentes geocodificadores tienen sus propias caracter\u00edsticas, y la calidad var\u00eda de una regi\u00f3n a otra. Afortunadamente, hay bibliotecas universales como [geopy] (https:\/\/github.com\/geopy\/geopy) que act\u00faan como encapsuladores para estos servicios externos.\n\n* Si tiene **MUCHOS DATOS**, alcanzar\u00e1 r\u00e1pidamente los l\u00edmites de la API externa. Adem\u00e1s, no siempre es el m\u00e1s r\u00e1pido para recibir informaci\u00f3n a trav\u00e9s de HTTP. Por lo tanto, es necesario considerar usar una versi\u00f3n local de OpenStreetMap.\n\n* Si tiene una **PEQUE\u00f1A CANTIDAD DE DATOS**, tiempo suficiente y no desea extraer caracter\u00edsticas sofisticadas, puede usar reverse_geocoder en lugar de OpenStreetMap:","2312a48f":"El par\u00e1metro `monotone_constraints = \"1\u2033` establece que la salida deber\u00eda aumentar monot\u00f3nicamente con respecto a las primeras caracter\u00edsticas (que en nuestro caso es la \u00fanica caracter\u00edstica). Despu\u00e9s de entrenar el modelo mon\u00f3tono, podemos ver que la relaci\u00f3n ahora es estrictamente mon\u00f3tona.","01d8278d":"**Otra opci\u00f3n** bastante popular es MinMaxScaling, que re\u00fane todos los puntos dentro de un intervalo predeterminado (t\u00edpicamente (0, 1)).\n\n$$ \\large X_{norm}=\\frac{X-X_{min}}{X_{max}-X_{min}} $$","b877709f":"Como sabemos que la relaci\u00f3n entre X e Y debe ser mon\u00f3tona, podemos establecer esta restricci\u00f3n al especificar el modelo.","9480702a":"## Q-Q plot inicial"}}