{"cell_type":{"784dcd3d":"code","43deba2a":"code","f1babc37":"code","fe00d879":"code","7e94da14":"code","d0480d4d":"code","59ec5e7e":"code","f33e476e":"code","ab20f987":"code","68e6ed07":"code","09787201":"code","5097a936":"code","3018b658":"code","8b66b39b":"code","335d8c5b":"code","a301d653":"code","029a44b0":"code","12ae4360":"code","6ec410b1":"code","558dc49e":"code","f7578f9c":"code","e677bff7":"code","c7b06a2a":"code","21d1d938":"code","74c1540d":"code","94f0972a":"markdown","597f0b76":"markdown","5a0261ef":"markdown","befc5cf4":"markdown","6be6c541":"markdown","64e96824":"markdown","3b2483cd":"markdown","b27baf6d":"markdown","f2b7bae8":"markdown","6ed54700":"markdown","8d69e563":"markdown","833c56e6":"markdown","8f6828a6":"markdown","cbd6ea7a":"markdown","45858aa7":"markdown","edeb310f":"markdown"},"source":{"784dcd3d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","43deba2a":"sample = pd.read_csv(r\"\/kaggle\/input\/digit-recognizer\/sample_submission.csv\",dtype = np.int32)\ntrain = pd.read_csv(r\"\/kaggle\/input\/digit-recognizer\/train.csv\",dtype = np.float32)\ntest = pd.read_csv(r\"\/kaggle\/input\/digit-recognizer\/test.csv\",dtype = np.float32)","f1babc37":"print(train.shape)\ntrain.head()","fe00d879":"print(test.shape)\ntest.head()","7e94da14":"print(sample.shape)\nsample.head()","d0480d4d":"submissions=pd.DataFrame({\"ImageId\": list(range(1,len(test)+1)), \"Label\": 0})\nsubmissions.to_csv(\"my_submission.csv\", index=False, header=True)","59ec5e7e":"import matplotlib.pyplot as plt\n%matplotlib inline","f33e476e":"X_train = (train.iloc[:,1:].values).astype('float32') # 2\u5217\u76ee\u306f\u753b\u7d20\u5024\ny_train = train.iloc[:,0].values.astype('int32') # 1\u5217\u76ee\u306f\u30e9\u30d9\u30eb\u30c7\u30fc\u30bf\nX_test = test.values.astype('float32')\n\nprint(X_train.shape)\nprint(y_train.shape)","ab20f987":"# 28 \u00d7 28 \u306e\u884c\u5217\u306b\u753b\u50cf\u3092\u5909\u63db\nX_train = X_train.reshape(X_train.shape[0], 28, 28)\n\nplt.figure(figsize=(10,10))\nfor i in range(0, 9):\n    plt.subplot(330 + (i+1))\n    plt.imshow(X_train[i], cmap=plt.get_cmap('gray'))\n    plt.title(y_train[i], size=21, color=\"white\");","68e6ed07":"classes = np.unique(y_train)\nprint(\"Digit class = \",classes)\n\nfig = plt.figure()\nax = fig.add_subplot(1,1,1)\nfor item in ax.get_xticklabels():\n    item.set_color(\"white\")\n \nfor item in ax.get_yticklabels():\n    item.set_color(\"white\")\n\nax.hist(y_train, bins=10)\nfig.show()","09787201":"# Import Libraries\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split","5097a936":"# train \u3068 validation \u306b\u5206\u5272\ntrain_image, validation_image, train_label, validation_label = train_test_split(X_train,\n                                                                             y_train,\n                                                                             test_size = 0.2,\n                                                                             random_state = 42) ","3018b658":"print(train_image.shape)\nprint(train_label.shape)","8b66b39b":"print(type(train_image))","335d8c5b":"train_image = torch.from_numpy(train_image)\nprint(type(train_image))","a301d653":"validation_image = torch.from_numpy(validation_image)\ntrain_label = torch.from_numpy(train_label).type(torch.LongTensor)\nvalidation_label = torch.from_numpy(validation_label).type(torch.LongTensor)","029a44b0":"# \u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u3068\u30a8\u30dd\u30c3\u30af\u6570\u306e\u5b9a\u7fa9\nbatch_size = 100\nnum_epochs = 30\n\n# \u30c7\u30fc\u30bf\u30ed\u30fc\u30c0\u306b\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u307e\u305b\u308b\ntrain = torch.utils.data.TensorDataset(train_image,train_label)\nvalidation = torch.utils.data.TensorDataset(validation_image,validation_label)\ntrain_loader = DataLoader(train, batch_size = batch_size, shuffle = False)\nvalidation_loader = DataLoader(validation, batch_size = batch_size, shuffle = False)","12ae4360":"# \u30e2\u30c7\u30eb\u306e\u4f5c\u6210\n# \u30d9\u30af\u30c8\u30eb(input_dim) -> \u30d9\u30af\u30c8\u30eb(100) -> ReLU -> \u30d9\u30af\u30c8\u30eb(output_dim) -> softmax\nclass LogisticRegressionModel(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(LogisticRegressionModel, self).__init__()\n\n        self.linear1 = nn.Linear(input_dim, 100)\n        self.activation1 = nn.ReLU(inplace=True)\n        self.linear2 = nn.Linear(100, output_dim)\n        self.activation2 = nn.Softmax(dim=1)\n    \n    def forward(self, x):\n        x = self.linear1(x)\n        x = self.activation1(x)\n        x = self.linear2(x)\n        out = self.activation2(x)\n        return out","6ec410b1":"# \u30e2\u30c7\u30eb\u306e\u8aad\u307f\u8fbc\u307f\ninput_dim = 28*28 # \u5165\u529b\u30b5\u30a4\u30ba\u306e\u6307\u5b9a: \u9577\u3055784(28\u00d728)\u306e\u30d9\u30af\u30c8\u30eb\noutput_dim = 10  # \u51fa\u529b\u30b5\u30a4\u30ba\u306e\u6307\u5b9a: \u9577\u305510(\u30e9\u30d9\u30eb\u6570)\u306e\u30d9\u30af\u30c8\u30eb\nmodel = LogisticRegressionModel(input_dim, output_dim)","558dc49e":"# \u640d\u5931\u95a2\u6570\u306e\u5b9a\u7fa9(cross entropy)\ncriterion = nn.CrossEntropyLoss()\n\n# \u30aa\u30d7\u30c6\u30a3\u30de\u30a4\u30b6\u306e\u5b9a\u7fa9(\u4eca\u56de\u306fSGD)\nlearning_rate = 0.001\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)","f7578f9c":"import time\n\n# \u5b66\u7fd2\u904e\u7a0b\u3067\u30ed\u30b9\u3068\u7cbe\u5ea6\u3092\u4fdd\u6301\u3059\u308b\u30ea\u30b9\u30c8\ntrain_losses, val_losses = [], []\ntrain_accu, val_accu = [], []\n\nfor epoch in range(num_epochs):\n    epoch_start_time = time.time()\n\n    # \u5b66\u7fd2\u7528\u30c7\u30fc\u30bf\u3067\u5b66\u7fd2\n    train_loss = 0\n    correct=0\n    model.train()\n    for images, labels in train_loader:\n        # \u52fe\u914d\u306e\u521d\u671f\u5316\n        optimizer.zero_grad()\n        \n        # \u9806\u4f1d\u64ad\n        outputs = model(images.view(-1, 28*28))\n\n        # \u30ed\u30b9\u306e\u8a08\u7b97\u3068\u9006\u4f1d\u64ad\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        # \u6b63\u7b54\u6570\u3092\u8a08\u7b97\n        predicted = torch.max(outputs.data, 1)[1]\n        correct += (predicted == labels).sum()\n        \n        train_loss += loss.item()\n\n    train_losses.append(train_loss\/len(train_loader))\n    train_accu.append(correct\/len(train_loader))\n\n    # \u691c\u8a3c\u7528\u30c7\u30fc\u30bf\u3067loss\u3068\u7cbe\u5ea6\u306e\u8a08\u7b97\n    val_loss = 0\n    correct = 0\n    model.eval()\n    with torch.no_grad():\n        for images, labels in validation_loader: \n            # \u9806\u4f1d\u64ad\n            outputs = model(images.view(-1, 28*28))\n            \n            # \u30ed\u30b9\u306e\u8a08\u7b97\n            val_loss += criterion(outputs, labels)\n\n            # \u6b63\u7b54\u6570\u3092\u8a08\u7b97\n            predicted = torch.max(outputs.data, 1)[1]\n            correct += (predicted == labels).sum()\n\n    val_losses.append(val_loss \/ len(validation_loader))\n    val_accu.append(correct \/ len(validation_loader))\n\n    print(f\"Epoch: {epoch+1}\/{num_epochs}.. \",\n          f\"Time: {time.time()-epoch_start_time:.2f}s..\",\n          f\"Training Loss: {train_losses[-1]:.3f}.. \",\n          f\"Training Accu: {train_accu[-1]:.3f}.. \",\n          f\"Val Loss: {val_losses[-1]:.3f}.. \",\n          f\"Val Accu: {val_accu[-1]:.3f}\")\n","e677bff7":"# loss \u3068 accuracy \u306e\u53ef\u8996\u5316\nplt.figure(figsize=(12,12))\nplt.subplot(2,1,1)\nax = plt.gca()\nax.set_xlim([0, epoch + 2])\nplt.ylabel('Loss')\nplt.plot(range(1, epoch + 2), train_losses[:epoch+1], 'r', label='Training Loss')\nplt.plot(range(1, epoch + 2), val_losses[:epoch+1], 'b', label='Validation Loss')\nax.grid(linestyle='-.')\nplt.legend()\nplt.subplot(2,1,2)\nax = plt.gca()\nax.set_xlim([0, epoch+2])\nplt.ylabel('Accuracy')\nplt.plot(range(1, epoch + 2), train_accu[:epoch+1], 'r', label='Training Accuracy')\nplt.plot(range(1, epoch + 2), val_accu[:epoch+1], 'b', label='Validation Accuracy')\nax.grid(linestyle='-.')\nplt.legend()\nplt.show()","c7b06a2a":"# \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3092\u4f7f\u3063\u3066\u56de\u7b54\u3092\u4f5c\u308b\nX_test = torch.from_numpy(X_test)\nmodel.eval()\noutput = model(X_test)\nprediction = torch.argmax(output, 1)","21d1d938":"# \u56de\u7b54\u30c7\u30fc\u30bf\u306e\u51fa\u529b\nsubmissions = pd.DataFrame({\"ImageId\": list(range(1,len(test)+1)), \"Label\": prediction.cpu().tolist()})\nsubmissions.to_csv(\"my_submission.csv\", index=False, header=True)","74c1540d":"# device = torch.device(\"cuda\")\n\n# # to \u3067GPU\u3092\u5229\u7528\u3059\u308b\u3088\u3046\u306b\u6307\u5b9a\n# model_gpu = LogisticRegressionModel(input_dim, output_dim)\n# model_gpu.to(device)\n\n# optimizer = torch.optim.SGD(model_gpu.parameters(), lr=learning_rate)\n\n# import time\n# # \u5b66\u7fd2\u904e\u7a0b\u3067\u30ed\u30b9\u3068\u7cbe\u5ea6\u3092\u4fdd\u6301\u3059\u308b\u30ea\u30b9\u30c8\n# train_losses, val_losses = [], []\n# train_accu, val_accu = [], []\n\n# for epoch in range(num_epochs):\n#     epoch_start_time = time.time()\n\n#     # \u5b66\u7fd2\u7528\u30c7\u30fc\u30bf\u3067\u5b66\u7fd2\n#     train_loss = 0\n#     correct=0\n#     model_gpu.train()\n#     for images, labels in train_loader:\n#         # to \u3067GPU\u3092\u5229\u7528\u3059\u308b\u3088\u3046\u306b\u6307\u5b9a\n#         images = images.to(device)\n#         labels = labels.to(device)\n\n#         # \u52fe\u914d\u306e\u521d\u671f\u5316\n#         optimizer.zero_grad()\n        \n#         # \u9806\u4f1d\u64ad\n#         outputs = model_gpu(images.view(-1, 28*28))\n\n#         # \u30ed\u30b9\u306e\u8a08\u7b97\u3068\u9006\u4f1d\u64ad\n#         loss = criterion(outputs, labels)\n#         loss.backward()\n#         optimizer.step()\n        \n#         # \u6b63\u7b54\u6570\u3092\u8a08\u7b97\n#         predicted = torch.max(outputs.data, 1)[1]\n#         correct += (predicted == labels).sum()\n        \n#         train_loss += loss.item()\n\n#     train_losses.append(train_loss\/len(train_loader))\n#     train_accu.append(correct\/len(train_loader))\n\n#     # \u691c\u8a3c\u7528\u30c7\u30fc\u30bf\u3067loss\u3068\u7cbe\u5ea6\u306e\u8a08\u7b97\n#     val_loss = 0\n#     correct = 0\n#     model_gpu.eval()\n#     with torch.no_grad():\n#         for images, labels in validation_loader: \n#             # to \u3067GPU\u3092\u5229\u7528\u3059\u308b\u3088\u3046\u306b\u6307\u5b9a\n#             images = images.to(device)\n#             labels = labels.to(device)\n\n#             # \u9806\u4f1d\u64ad\n#             outputs = model_gpu(images.view(-1, 28*28))\n            \n#             # \u30ed\u30b9\u306e\u8a08\u7b97\n#             val_loss += criterion(outputs, labels)\n\n#             # \u6b63\u7b54\u6570\u3092\u8a08\u7b97\n#             predicted = torch.max(outputs.data, 1)[1]\n#             correct += (predicted == labels).sum()\n\n#     val_losses.append(val_loss \/ len(validation_loader))\n#     val_accu.append(correct \/ len(validation_loader))\n\n#     print(f\"Epoch: {epoch+1}\/{num_epochs}.. \",\n#           f\"Time: {time.time()-epoch_start_time:.2f}s..\",\n#           f\"Training Loss: {train_losses[-1]:.3f}.. \",\n#           f\"Training Accu: {train_accu[-1]:.3f}.. \",\n#           f\"Val Loss: {val_losses[-1]:.3f}.. \",\n#           f\"Val Accu: {val_accu[-1]:.3f}\")\n","94f0972a":"# \u5b66\u7fd2\u306e\u6e96\u5099","597f0b76":"### Pytorch \u3067 GPU \u3092\u4f7f\u3063\u3066\u307f\u308b\n\n- GPU\u306e\u5229\u7528\u3092\u660e\u793a\u7684\u306b\u6307\u5b9a -> \u30c7\u30fc\u30bf\u3084\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092GPU\u3067\u8aad\u307f\u8fbc\u3080(\u8a08\u7b97\u3059\u308b)\u3088\u3046\u306b\u6307\u5b9a \u3067OK","5a0261ef":"## \u30c7\u30fc\u30bf\u306e\u78ba\u8a8d","befc5cf4":"# \u30c7\u30fc\u30bf\u306e\u78ba\u8a8d","6be6c541":"# \u6642\u9593\u304c\u4f59\u3063\u305f\u3089(Tips)","64e96824":"# \u3053\u3053\u304b\u3089\u5148\u306f...\n\n- \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u69cb\u9020\u3092\u5909\u3048\u308b\u3082\u826f\u3057\n- \u30aa\u30d7\u30c6\u30a3\u30de\u30a4\u30b6\u3092\u5909\u3048\u3066\u307f\u308b\u3082\u826f\u3057\n- Augmentation (\u30c7\u30fc\u30bf\u306e\u6c34\u5897\u3057) \u3057\u3066\u307f\u308b\u3082\u826f\u3057\n- \u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u3057\u3066\u307f\u308b\u3082\u826f\u3057\n- \u601d\u3044\u3064\u3044\u305f\u7279\u5fb4\u69cb\u7bc9\u3057\u3066\u307f\u308b\u3082\u826f\u3057\n\n# \u304a\u3059\u3059\u3081\u306e\u9032\u3081\u65b9\n\n- \u3042\u308b\u7a0b\u5ea6\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u69cb\u9020\u3092\u6e80\u8db3\u884c\u304f\u3068\u3053\u308d\u307e\u3067\u5909\u3048\u3066\u307f\u308b\u306e\u304c\u826f\u3044\u3068\u601d\u3044\u307e\u3059\n  - \u3072\u3068\u307e\u305a\u5168\u7d50\u5408\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u304b\u3089\u7573\u307f\u8fbc\u307f\u30cd\u30c3\u30c8\u30ef\u30fc\u30af(CNN)\u306b\u3059\u308b\u3068\u826f\u3044\u3068\u601d\u3044\u307e\u3059\n  - LeNet -> AlexNet -> VGG (-> GoogleNet) -> ResNet \u3068\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u9032\u5316\u3092\u8ffd\u3044\u306a\u304c\u3089\u5b66\u3076\u306e\u3082\u3044\u3044\u304b\u306a\u3068\u601d\u3044\u307e\u3059\n  - weight decay, Batch Normalization, Dropout \u306e\u3088\u3046\u306a\u30c6\u30af\u30cb\u30c3\u30af\u3092\u4f75\u305b\u3066\u5b66\u3076\u3068\u826f\u3044\u3068\u601d\u3044\u307e\u3059\n \n \n- \u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u69cb\u9020\u306b\u6e80\u8db3\u304c\u884c\u3063\u305f\u3089Augmentaion\u3092\u8272\u3005\u8abf\u6574\u3057\u3066\u307f\u308b\u306e\u304c\u826f\u3044\u3068\u601d\u3044\u307e\u3059\n  - \u56de\u8ee2, \u4f38\u7e2e, \u30ce\u30a4\u30ba\u4ed8\u52a0, ...\n  \n  \n- \u305d\u306e\u5f8c\u8fbc\u307f\u5165\u3063\u305f\u7279\u5fb4\u69cb\u7bc9\u3084\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u3092\u3057\u3066\u307f\u308b\u3068\u826f\u3044\u3068\u601d\u3044\u307e\u3059\n\n\n- \u3068\u308a\u3042\u3048\u305aKaggle\u306e\u4e0a\u4f4d\u30ab\u30fc\u30cd\u30eb\u306e\u89e3\u8aac\u3068NoteBook\u3092\u898b\u308b\u306e\u3082\u3044\u3044\u3068\u601d\u3044\u307e\u3059","3b2483cd":"## \u30e9\u30d9\u30eb\u3068\u30c7\u30fc\u30bf\u3092\u5206\u5272\u3059\u308b","b27baf6d":"# \u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u5229\u7528\u3057\u3066\u63a8\u5b9a\u3092\u3059\u308b\n\n\u4eca\u56de\u306f Pytorch \u3092\u5229\u7528\u3057\u307e\u3059","f2b7bae8":"## \u30e9\u30d9\u30eb\u306e\u78ba\u8a8d","6ed54700":"# \u30e2\u30c7\u30eb\u306e\u6e96\u5099","8d69e563":"# \u30c7\u30fc\u30bf\u3092\u8a73\u3057\u304f\u78ba\u8a8d\u3057\u3066\u3044\u304f","833c56e6":"# ndarray\u5f62\u5f0f\u304b\u3089Tensor\u5f62\u5f0f\u3078\u5909\u63db","8f6828a6":"# \u63d0\u51fa!","cbd6ea7a":"# GPU\u306e\u5229\u7528\n\n## Kaggle Notebook \u3067\u306f\u5236\u9650\u4ed8\u304d\u306eGPU\u304c\u4f7f\u3048\u307e\u3059\uff01\n\n### GPU \u304c\u4f7f\u3048\u308b\u3068\u5b66\u7fd2\u304c\u65e9\u304f\u306a\u308a\u307e\u3059\uff01\u3042\u3068\u4eba\u306b\u3088\u3063\u3066\u306f\u30e1\u30e2\u30ea\u304c\u4f7f\u3048\u3066\u5b09\u3057\u3044\uff01\u3063\u3066\u306a\u308a\u307e\u3059\uff01\n\n- \u4eca\u56de\u306fMNIST\u3067\u5c0f\u3055\u306a\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u4f7f\u3063\u305f\u306e\u3067\u5b66\u7fd2\u304c\u65e9\u304f\u7d42\u308f\u308a\u307e\u3057\u305f\u304c\u3001\u30b3\u30f3\u30da\u306b\u6311\u6226\u3059\u308b\u3068CPU\u3058\u3083\u7121\u7406\uff01\u3063\u3066\u306a\u308a\u307e\u3059","45858aa7":"# \u203b\u3053\u308c\u4ee5\u5916\u306e\u753b\u50cf\u30b3\u30f3\u30da\u306b\u3064\u3044\u3066\n\n- \u753b\u50cf\u306f\u3069\u3046\u3044\u3046\u5f62\u5f0f\u306e\u30d5\u30a1\u30a4\u30eb\u3067\u4e0e\u3048\u3089\u308c\u308b\u304b\u306f\u30b3\u30f3\u30da\u306b\u3088\u3063\u3066\u7570\u306a\u308a\u307e\u3059\n- \u4eca\u56de\u306f csv \u5f62\u5f0f\u3067\u3057\u305f\u304c\u3001 jpeg \u3084 png \u306a\u3069\u3067\u4e0e\u3048\u3089\u308c\u308b\u5834\u5408\u3082\u3042\u308a\u307e\u3059\n- \u533b\u7642\u7528\u753b\u50cf\u3068\u304b\u306f\u7279\u6b8a\u306a\u5f62\u5f0f\u3060\u3063\u305f\u308a\u3082\u3059\u308b\u306e\u3067\u3001\u8aad\u307f\u8fbc\u307f\u65b9\u3084\u30c7\u30fc\u30bf\u306e\u6271\u3044\u65b9\u306f\u90fd\u5ea6\u8abf\u3079\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\n- \u4ed6\u306e\u4eba\u306e\u516c\u958b\u3055\u308c\u3066\u3044\u308bNotebook\u3092\u898b\u308b\u3068\u89e3\u6c7a\u3057\u305f\u308a\u3082\u3057\u307e\u3059","edeb310f":"# \u3068\u308a\u3042\u3048\u305a\u5168\u90e8 0 \u3068\u3057\u3066\u63d0\u51fa\u3057\u3066\u307f\u308b"}}