{"cell_type":{"2ea1549d":"code","cc477531":"code","5639dab4":"code","dde75185":"code","54647e61":"code","b89cdcd9":"code","589f38b5":"code","e06052d8":"code","8d80552e":"code","6749c01e":"code","cf26e939":"code","de03823f":"code","2422c533":"code","9119f972":"code","46349c09":"code","48bc6346":"code","cc64314b":"code","35bf7be3":"code","cad06f83":"code","52e00cc8":"code","00d3fb37":"code","1aa92b29":"code","0a20137e":"code","1aa231ac":"code","1c52a820":"code","446fc2c2":"code","3826b3c3":"code","bc633705":"code","12eca29a":"code","f9731882":"code","5123065e":"code","7e74898c":"code","b9420683":"code","e327cbfc":"code","919e2748":"code","701e3cbc":"code","32e382ac":"code","aa6de490":"code","4bbf8227":"code","eb7c311a":"code","6ec3ed68":"code","c6f0ad79":"code","14c59949":"code","75a52cd3":"code","2774ebee":"code","c587d7df":"code","a9249c6b":"code","ef1100de":"code","402574eb":"code","3927293f":"code","28cf8e5f":"code","98a27cdc":"code","9effb254":"code","2afac92b":"code","97f60c0b":"code","7b455491":"code","468effb7":"code","45730a8d":"code","f97e6684":"code","8e873159":"code","7f0a6fb1":"code","72f8c106":"markdown","09038406":"markdown","b82839e8":"markdown","8083072a":"markdown","7fe3e730":"markdown","f7a3d307":"markdown","4dcc984e":"markdown","96a43631":"markdown","5c1b7f59":"markdown","51a2d61d":"markdown","5fb8e98b":"markdown","39ed735b":"markdown","4aaaddba":"markdown","d8e8644f":"markdown","1131250b":"markdown"},"source":{"2ea1549d":"# necessary imports\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.style.use('ggplot')\n%matplotlib inline","cc477531":"df = pd.read_csv('..\/input\/concrete-compressive-strength\/Concrete Compressive Strength.csv')","5639dab4":"df.head()","dde75185":"df.describe()","54647e61":"df.info()","b89cdcd9":"df.isna().sum()","589f38b5":"# visualizing missing values\n\nmsno.bar(df)\nplt.show()","e06052d8":"plt.figure(figsize = (25, 20))\nplotnumber = 1\n\nfor col in df.columns:\n    if plotnumber <= 9: \n        ax = plt.subplot(3, 3, plotnumber)\n        sns.distplot(df[col])\n        plt.xlabel(col, fontsize = 15)\n        \n    plotnumber += 1\n\nplt.tight_layout()\nplt.show()","8d80552e":"# creating feature and label\n\nX = df.iloc[:, :-1]\ny = df.iloc[:, -1]","6749c01e":"X.var()","cf26e939":"# normalizing features\n# let's add 1 to each value in everycolumn so that we don't get exception while calculating the log value of 0\n\nfor column in X.columns:\n    X[column] += 1\n    X[column] = np.log(X[column])","de03823f":"X.var()","2422c533":"plt.figure(figsize = (25, 20))\nplotnumber = 1\n\nfor col in X.columns:\n    if plotnumber <= 8:\n        ax = plt.subplot(3, 3, plotnumber)\n        sns.distplot(X[col])\n        plt.xlabel(col, fontsize = 15)\n        \n    plotnumber += 1\n    \nplt.tight_layout()\nplt.show()","9119f972":"plt.figure(figsize = (20, 15))\nplotnumber = 1\n\nfor col in X.columns:\n    if plotnumber <= 8:\n        ax = plt.subplot(3, 3, plotnumber)\n        sns.boxplot(X[col])\n        plt.xlabel(col, fontsize = 15)\n    \n    plotnumber += 1\nplt.tight_layout()\nplt.show()","46349c09":"# let's check how our features are related to the target column\n\nplt.figure(figsize = (20, 20))\nplotnumber = 1\n\nfor col in X.columns:\n    if plotnumber <= 8:\n        ax = plt.subplot(3, 3, plotnumber)\n        sns.scatterplot(df[col], y)\n        plt.xlabel(col, fontsize = 15)\n        \n    plotnumber += 1\n    \nplt.tight_layout()\nplt.show()","48bc6346":"# checking for correlation using heatmap\n\nplt.figure(figsize = (18, 10))\n\ncorr = X.corr()\nmask = np.triu(np.ones_like(corr, dtype = bool))\n\nsns.heatmap(data = corr, mask = mask, annot = True, fmt = '.2g', linewidths = 1, cbar = False)\nplt.show()","cc64314b":"# splitting data into training and test set\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)","35bf7be3":"# scaling data\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test) ","cad06f83":"from sklearn.linear_model import LinearRegression\n\nlr = LinearRegression()\nlr.fit(X_train, y_train)","52e00cc8":"lr.score(X_train, y_train)","00d3fb37":"lr.score(X_test, y_test)","1aa92b29":"from sklearn.linear_model import Lasso, LassoCV","0a20137e":"lassocv = LassoCV(alphas = None, cv = 10, max_iter = 10000, normalize = True)\nlassocv.fit(X_train, y_train)","1aa231ac":"lasso = Lasso(alpha = lassocv.alpha_)\nlasso.fit(X_train, y_train)","1c52a820":"lasso.score(X_train, y_train)","446fc2c2":"lasso.score(X_test, y_test)","3826b3c3":"from sklearn.tree import DecisionTreeRegressor\n\ndtr = DecisionTreeRegressor()\ndtr.fit(X_train, y_train)","bc633705":"dtr.score(X_train, y_train)","12eca29a":"dtr.score(X_test, y_test)","f9731882":"# Hyper Parameter Tuning Decision Tree Regressor\n\nfrom sklearn.model_selection import GridSearchCV\n\ngrid_params = {\n    'criterion' : ['mse', 'friedman_mse', 'mae'],\n    'splitter' : ['best', 'random'],\n    'max_depth' : [3, 5, 7, 9, 10],\n    'min_samples_split' : [1, 2, 3, 4, 5],\n    'min_samples_leaf' : [1, 2, 3, 4, 5]\n}\n\ngrid_search = GridSearchCV(dtr, grid_params, cv = 5, n_jobs = -1, verbose = 1)\ngrid_search.fit(X_train, y_train)","5123065e":"# best parameters and best score\n\nprint(grid_search.best_params_)\nprint(grid_search.best_score_)","7e74898c":"dtr = DecisionTreeRegressor(criterion = 'friedman_mse', max_depth = 10, min_samples_leaf = 1, min_samples_split = 2, splitter = 'random')\ndtr.fit(X_train, y_train)","b9420683":"dtr.score(X_train, y_train)","e327cbfc":"dtr.score(X_test, y_test)","919e2748":"from sklearn.ensemble import RandomForestRegressor\n\nrfr = RandomForestRegressor()\nrfr.fit(X_train, y_train)","701e3cbc":"rfr.score(X_train, y_train)","32e382ac":"rfr.score(X_test, y_test)","aa6de490":"from sklearn.ensemble import AdaBoostRegressor\n\nada = AdaBoostRegressor(base_estimator = dtr)\nada.fit(X_train, y_train)","4bbf8227":"ada.score(X_train, y_train)","eb7c311a":"ada.score(X_test, y_test)","6ec3ed68":"# hyper parameter tuning \n\ngrid_params = {\n    'n_estimators' : [40, 50, 80, 100],\n    'learning_rate' : [0.01, 0.1, 0.05, 0.5, 1, 10],\n    'loss' : ['linear','square', 'exponential']\n}\n\ngrid_search = GridSearchCV(ada, grid_params, cv = 5, n_jobs = -1, verbose = 1)\ngrid_search.fit(X_train, y_train)","c6f0ad79":"# best parameters and best score\n\nprint(grid_search.best_params_)\nprint(grid_search.best_score_)","14c59949":"ada = AdaBoostRegressor(base_estimator = dtr, learning_rate = 1, loss = 'exponential', n_estimators = 100)\nada.fit(X_train, y_train)","75a52cd3":"ada.score(X_train, y_train)","2774ebee":"ada.score(X_test, y_test)","c587d7df":"from sklearn.ensemble import GradientBoostingRegressor\n\ngbr = GradientBoostingRegressor()\ngbr.fit(X_train, y_train)","a9249c6b":"gbr.score(X_train, y_train)","ef1100de":"gbr.score(X_test, y_test)","402574eb":"# hyper parameter tuning of gradient boost regressor\n\ngrid_params = {\n    'n_estimators': [90, 100, 120, 180, 200],\n    'learning_rate' : [0.01, 0.1, 0.05, 0.5, 1],\n    'loss' : ['ls', 'lad', 'huber', 'quantile']\n}\n\ngrid_search = GridSearchCV(gbr, grid_params, cv = 5, n_jobs = -1, verbose = 1)\ngrid_search.fit(X_train, y_train)","3927293f":"# best parameters and best score\n\nprint(grid_search.best_params_)\nprint(grid_search.best_score_)","28cf8e5f":"gbr = GradientBoostingRegressor(learning_rate = 0.5, loss = 'ls', n_estimators = 200)\ngbr.fit(X_train, y_train)","98a27cdc":"gbr.score(X_train, y_train)","9effb254":"gbr.score(X_test, y_test)","2afac92b":"from xgboost import XGBRegressor\n\nxgb = XGBRegressor(booster = 'gbtree', learning_rate = 0.1, max_depth = 7, n_estimators = 200)\nxgb.fit(X_train, y_train)","97f60c0b":"xgb.score(X_train, y_train)","7b455491":"xgb.score(X_test, y_test)","468effb7":"from sklearn.ensemble import VotingRegressor\n\nregressors = [('Linear Regression', lr), ('Lasso Regression', lasso), ('Decision Tree', dtr), ('Random Forest', rfr), ('Ada Boost', ada), ('Gradient Boost', gbr),\n              ('XgBoost', xgb)]\n\nvr = VotingRegressor(estimators = regressors, n_jobs = -1, verbose = 1, weights = (0.1, 0.1, 0.1, 0.2, 0.2, 0.7, 0.8))\nvr.fit(X_train, y_train)","45730a8d":"vr.score(X_train, y_train)","f97e6684":"vr.score(X_test, y_test)","8e873159":"models = pd.DataFrame({\n    'Model' : ['Linear Regression', 'Lasso Regression', 'Decision Tree', 'Random Forest', 'Ada Boost', 'Gradient Boost', 'XgBoost', \"Voting Regressor\"],\n    'Score' : [lr.score(X_test, y_test), lasso.score(X_test, y_test), dtr.score(X_test, y_test), rfr.score(X_test, y_test), ada.score(X_test, y_test),\n               gbr.score(X_test, y_test), xgb.score(X_test, y_test), vr.score(X_test, y_test)]\n})\n\n\nmodels.sort_values(by = 'Score', ascending = False)","7f0a6fb1":"plt.figure(figsize = (20, 8))\n\nsns.barplot(x = 'Model', y = 'Score', data = models)\nplt.ylim(0.70, 1)\nplt.show()\n","72f8c106":"## Cement Strength Prediction","09038406":"### XgBoost Regressor","b82839e8":"### Decision Tree Regressor","8083072a":"### Data Description\n\nGiven is the variable name, variable type, the measurement unit and a brief description. \nThe concrete compressive strength is the regression problem. The order of this listing \ncorresponds to the order of numerals along the rows of the database. \n\n- Name -- Data Type -- Measurement -- Description\n\n- Cement (component 1) -- quantitative -- kg in a m3 mixture -- Input Variable\n- Blast Furnace Slag (component 2) -- quantitative -- kg in a m3 mixture -- Input Variable\n- Fly Ash (component 3) -- quantitative -- kg in a m3 mixture -- Input Variable\n- Water (component 4) -- quantitative -- kg in a m3 mixture -- Input Variable\n- Superplasticizer (component 5) -- quantitative -- kg in a m3 mixture -- Input Variable\n- Coarse Aggregate (component 6) -- quantitative -- kg in a m3 mixture -- Input Variable\n- Fine Aggregate (component 7) -- quantitative -- kg in a m3 mixture -- Input Variable\n- Age -- quantitative -- Day (1~365) -- Input Variable\n- Concrete compressive strength -- quantitative -- MPa -- Output Variable ","7fe3e730":"### Voting Regressor","f7a3d307":"#### If you like this kernel, Please do upvote","4dcc984e":"### Ada Boost ","96a43631":"Now data is normalized and looks good, let's check for outliers.","5c1b7f59":"Great! None of column seem to be correlated.","51a2d61d":"### Great we get accuracy above 90% which is quite good.","5fb8e98b":"### Random Forest Regression","39ed735b":"### Gradient Boost Regressor","4aaaddba":"### Lasso Regression","d8e8644f":"**Looks like there are no missing values in the data, let's check missing values**","1131250b":"### Linear Regression"}}