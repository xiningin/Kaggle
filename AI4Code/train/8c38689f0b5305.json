{"cell_type":{"66122102":"code","642d9454":"code","d3d53be7":"code","4f0c1166":"code","0004dc27":"code","5b89e64a":"code","428bed0d":"code","aa44c710":"code","2955086e":"code","444cbecc":"code","efc3a723":"code","686a7d54":"code","afa22942":"code","4060eb87":"code","931413a3":"code","28d32ea4":"code","fe52f0b2":"code","472a8491":"code","cb7c151f":"code","3b453e6f":"code","1faedea7":"code","37ec3c9e":"code","d1fbe0c7":"code","2787f568":"code","8c2fc130":"code","1511f2bb":"code","2aa59bac":"code","41783830":"code","e903c0d7":"code","e26bd732":"code","4b13c3ca":"code","67b88f31":"code","cc1159e5":"code","581a9fc3":"code","916695eb":"code","793f81cd":"code","f290e6a2":"code","7beb2e44":"markdown","e9bd61bb":"markdown","211b5ce0":"markdown","244429bf":"markdown","1eea0086":"markdown","e684308a":"markdown","69c94eef":"markdown","342887eb":"markdown","22d0e603":"markdown","61da6528":"markdown","5dab98f0":"markdown","2f3424c4":"markdown","2eed1f58":"markdown","4a47bf86":"markdown","de6ec09e":"markdown","0b8dbcc2":"markdown","3ddd200d":"markdown","370da38c":"markdown","75933884":"markdown","90ee09dc":"markdown","b0dcd052":"markdown","1c7c7967":"markdown","5080db9d":"markdown","b0d0cd95":"markdown","3151dce5":"markdown","9a336b94":"markdown","8d5f5e81":"markdown","66b0858f":"markdown","17f9ce2f":"markdown"},"source":{"66122102":"from IPython.display import display, Math, Latex\n\ndisplay(Math(r'y_p = \\theta\\cdot X'))","642d9454":"import numpy as np\nimport matplotlib.pyplot as plt","d3d53be7":"np.random.seed(42)\nm = 1000\nn = 2\nx = np.linspace(0, 1, m)\nones = np.ones(m)\nX = np.column_stack((ones, x))\nY = 5 * x + 4\ny = 5 * x + 4 + np.random.randn(len(x))\/5\ny = y.reshape(m, 1)","4f0c1166":"labels = ['Instances', 'Initial model']\n\nplt.plot(x, y, 'go')\nplt.plot(x, Y, 'r')\nplt.legend(labels)\nplt.show()","0004dc27":"np.random.seed(42)\ntheta0 = np.array([np.random.randn(), np.random.randn()])\ntheta0 = theta0.reshape(n, 1)\n\nm0 = np.array([0, 0])\nm0 = m0.reshape(n, 1)\n\ns0 = np.array([0, 0])\ns0 = s0.reshape(n, 1)","5b89e64a":"display(Math(r'MSE(\\theta) = \\frac{1}{m} \\cdot \\sum_{i=1}^{m}(X^{(i)} \\theta - y^{(i)})^2'))","428bed0d":"display(Math(r'\\nabla_\\theta MSE(\\theta) = \\frac{2}{m} X^T (X\\theta - y)'))","aa44c710":"mse = lambda x: (1\/m) * np.sum((X @ x - y) ** 2)\n\ngrad_mse = lambda x: (2\/m) * (X.T @ (X @ x - y))","2955086e":"learning_rate = 0.005\nmax_epochs = 100000","444cbecc":"thetas = []\n_losses = []\nn_epochs = []","efc3a723":"display(Math(r'\\theta = (X^T X)^{-1}X^T y'))","686a7d54":"mathematical_solution = np.linalg.inv(X.T @ X) @ X.T @ y\nmathematical_solution","afa22942":"display(Math(r'\\theta_{next step} = \\theta - \\alpha \\nabla_\\theta MSE(\\theta)'))","4060eb87":"def gradient_descent(epochs, learning_rate, loss_variation=10**-5):\n    \n    theta = theta0\n    losses = []\n    last_loss = 9999\n    \n    for epoch in range(epochs):\n        loss = mse(theta)\n        gradients = grad_mse(theta)\n        theta = theta - (learning_rate * gradients)\n        \n        if epoch % 20 == 0:\n            losses.append((loss, epoch))\n            if(abs(last_loss - loss) < loss_variation):\n                losses = np.array(losses)\n                return theta, losses, epoch\n            else:\n                last_loss = loss\n    \n    losses = np.array(losses)\n    print('Max. number of iterations without converging')   \n    return theta, losses, epochs","931413a3":"p1, losses, epoch = gradient_descent(max_epochs, learning_rate)\nthetas.append(p1)\n_losses.append(losses)\nn_epochs.append(epoch)","28d32ea4":"plt.plot(_losses[0][0:, 1], _losses[0][0:, 0])\nplt.show()","fe52f0b2":"plt.plot(_losses[0][50:, 1], _losses[0][50:, 0])\nplt.show()","472a8491":"display(Math(r'1.\\quad m \\leftarrow \\beta m - \\alpha \\nabla_\\theta MSE(\\theta)'))\ndisplay(Math(r'2.\\quad \\theta = \\theta + m'))","cb7c151f":"def momentum_optimization(epochs, learning_rate, beta, loss_variation=10**-5):\n    \n    theta = theta0\n    losses = []\n    m = m0\n    last_loss = 9999\n    \n    for epoch in range(epochs):\n        loss = mse(theta)\n        gradients = grad_mse(theta)\n        m = beta * m + learning_rate * gradients\n        theta = theta - m\n        \n        if epoch % 20 == 0:\n            losses.append((loss, epoch))\n            if(abs(last_loss - loss) < loss_variation):\n                losses = np.array(losses)\n                return theta, losses, epoch\n            else:\n                last_loss = loss\n            \n    losses = np.array(losses)\n    print('Max. number of iterations without converging')  \n    return theta, losses, epochs","3b453e6f":"p1, losses, epoch = momentum_optimization(max_epochs, learning_rate, 0.9)\nthetas.append(p1)\n_losses.append(losses)\nn_epochs.append(epoch)\n\nplt.plot(_losses[1][:, 1], _losses[1][:, 0])\nplt.show()","1faedea7":"plt.plot(_losses[1][3:, 1], _losses[1][3:, 0])\nplt.show()","37ec3c9e":"display(Math(r'1.\\quad m \\leftarrow \\beta m - \\alpha \\nabla_\\theta MSE(\\theta + \\beta m)'))\ndisplay(Math(r'2.\\quad \\theta = \\theta + m'))","d1fbe0c7":"def nesterov_accelerated_gradient(epochs, learning_rate, beta, loss_variation=10**-5):\n    \n    theta = theta0\n    losses = []\n    m = m0\n    last_loss = 9999\n    \n    for epoch in range(epochs):\n        loss = mse(theta)\n        gradients = grad_mse(theta + beta * m)\n        m = beta * m + learning_rate * gradients\n        theta = theta - m\n        \n        if epoch % 20 == 0:\n            losses.append((loss, epoch))\n            if(abs(last_loss - loss) < loss_variation):\n                losses = np.array(losses)        \n                return theta, losses, epoch\n            else:\n                    last_loss = loss\n            \n    losses = np.array(losses)\n    print('Max. number of iterations without converging')\n    return theta, losses, epochs","2787f568":"p1, losses, epoch = nesterov_accelerated_gradient(max_epochs, learning_rate, 0.9)\n\nthetas.append(p1)\n_losses.append(losses)\nn_epochs.append(epoch)\n\nplt.plot(_losses[1][:, 1], _losses[1][:, 0])\nplt.show()","8c2fc130":"plt.plot(_losses[1][3:, 1], _losses[1][3:, 0])\nplt.show()","1511f2bb":"display(Math(r'1.\\quad s \\leftarrow s + \\nabla_\\theta MSE(\\theta)\\otimes \\nabla_\\theta MSE(\\theta)'))\ndisplay(Math(r'2. \\quad \\theta \\leftarrow \\theta - \\alpha \\nabla_\\theta MSE(\\theta) \\oslash \\sqrt{s + \\epsilon}'))","2aa59bac":"def adaGrad(epochs, learning_rate, epsilon=10**-10, loss_variation=10**-5):\n    \n    theta = theta0\n    losses = []\n    s = s0\n    last_loss = 9999\n    \n    for epoch in range(epochs):\n        loss = mse(theta)\n        gradients = grad_mse(theta)\n        s = s + gradients * gradients\n        theta = theta - (learning_rate * gradients) \/ (np.sqrt(s+ epsilon))\n        \n        if epoch % 20 == 0:\n            losses.append((loss, epoch))\n            if (abs(last_loss - loss) < loss_variation):\n                losses = np.array(losses)\n                return theta, losses, epoch\n            else:\n                last_loss = loss\n            \n    losses = np.array(losses)\n    print('Max. number of iterations without converging')\n    return theta, losses, epochs","41783830":"p1, losses, epoch = adaGrad(max_epochs, learning_rate)\n\nthetas.append(p1)\n_losses.append(losses)\nn_epochs.append(epoch)\n\nplt.plot(_losses[3][:, 1], _losses[3][:, 0])\nplt.show()","e903c0d7":"display(Math(r'1.\\quad s \\leftarrow \\beta s + (1 - \\beta) \\nabla_\\theta MSE(\\theta)\\otimes \\nabla_\\theta MSE(\\theta)'))\ndisplay(Math(r'2. \\quad \\theta \\leftarrow \\theta - \\alpha \\nabla_\\theta MSE(\\theta) \\oslash \\sqrt{s + \\epsilon}'))","e26bd732":"def RMSProp(epochs, learning_rate, beta, epsilon=10**-10, loss_variation=10**-5):\n    \n    theta = theta0\n    losses = []\n    s = s0\n    last_loss = 9999\n    \n    for epoch in range(epochs):\n        loss = mse(theta)\n        gradients = grad_mse(theta)\n        s = beta * s + (1 - beta) * gradients * gradients\n        theta = theta - (learning_rate * gradients) \/ (np.sqrt(s + epsilon))\n        if epoch % 20 == 0:\n            losses.append((loss, epoch))\n            if(abs(last_loss - loss) < loss_variation):\n                losses = np.array(losses)\n                return theta, losses, epoch\n            else:\n                last_loss = loss\n            \n    losses = np.array(losses)\n    print('Max. number of iterations without converging')    \n    return theta, losses, epochs","4b13c3ca":"p1, losses, epoch = RMSProp(max_epochs, learning_rate, 0.9)\n\nthetas.append(p1)\n_losses.append(losses)\nn_epochs.append(epoch)\n\nplt.plot(_losses[4][:, 1], _losses[4][:, 0])\nplt.show()","67b88f31":"display(Math(r'1. \\quad m \\leftarrow \\beta_1 m - (1 - \\beta_1)\\nabla_\\theta MSE(\\theta)'))\ndisplay(Math(r'2. \\quad s \\leftarrow \\beta_2 s + (1 - \\beta_2)\\nabla_\\theta MSE(\\theta) \\otimes\\nabla_\\theta MSE(\\theta)'))\ndisplay(Math(r'3. \\quad \\hat{m} \\leftarrow \\frac{m}{1 - \\beta_1^T}'))\ndisplay(Math(r'4. \\quad \\hat{s} \\leftarrow \\frac{s}{1 - \\beta_2^T}'))\ndisplay(Math(r'5. \\quad \\theta \\leftarrow \\theta + \\alpha \\hat{m} \\oslash \\sqrt{\\hat{s} + \\epsilon}'))\nprint('Where T represents the iteration number')","cc1159e5":"def adam_opt(epochs, learning_rate, beta1, beta2, epsilon=10**-10, loss_variation=10**-5):\n    \n    theta = theta0\n    losses = []\n    s = s0\n    m = m0\n    last_loss = 9999\n    \n    for epoch in range(epochs):\n        e = epoch\n        loss = mse(theta)\n        gradients = grad_mse(theta)\n\n        m = beta1 * m + (1 - beta1) * gradients\n        s = beta2 * s + (1 - beta2) * gradients * gradients\n\n        m2 = m \/ (1 - beta1**(epoch+1))\n        s2 = s \/ (1 - beta2**(epoch+1))\n\n        theta = theta - (learning_rate * m2 )\/ (np.sqrt(s2 + epsilon))\n\n        if epoch % 20 == 0:\n            losses.append((loss, epoch))\n            if(abs(last_loss - loss) < loss_variation):\n                losses = np.array(losses)\n                return theta, losses, epoch\n            else:\n                last_loss = loss\n        \n    losses = np.array(losses)\n    print('Max. number of iterations without converging') \n    return theta, losses, epochs","581a9fc3":"p1, losses, epoch = adam_opt(max_epochs, learning_rate, 0.9, 0.9)\n\nthetas.append(p1)\n_losses.append(losses)\nn_epochs.append(epoch)\n\nplt.plot(_losses[5][:, 1], _losses[5][:, 0])\nplt.show()","916695eb":"labels = ['Gradient Descent', 'Momentum', 'Nesterov', 'AdaGrad', 'RMSProp', 'Adam']\ny_pos = np.arange(len(n_epochs))\n\nfig = plt.figure(figsize=(15,6))\nplt.bar(y_pos, n_epochs)\nplt.xticks(y_pos, labels)\nplt.show()","793f81cd":"labels.remove('AdaGrad')\nn_epochs = np.delete(n_epochs, 3)\ny_pos = np.arange(len(n_epochs))\n\nfig = plt.figure(figsize=(15,6))\nplt.bar(y_pos, n_epochs)\nplt.xticks(y_pos, labels)\nplt.show()","f290e6a2":"fig = plt.figure(figsize=(15,6))\n# Instances\nplt.plot(x, y, 'go')\n# First model\nplt.plot(x, Y, 'r')\n# Mathematical solution\nms = mathematical_solution[0] + mathematical_solution[1] * x\nplt.plot(x, ms, 'y')\n# Gradient descent\ngd = thetas[0][0] + thetas[0][1] * x\nplt.plot(x, gd, 'b')\n# Momentum\nmo = thetas[1][0] + thetas[1][1] * x\nplt.plot(x, mo, 'cyan')\n# Nesterov\nno = thetas[2][0] + thetas[2][1] * x\nplt.plot(x, no, 'salmon')\n# AdaGrad\nag = thetas[3][0] + thetas[3][1] * x\nplt.plot(x, ag, 'black')\n# RMSProp\nrms = thetas[4][0] + thetas[4][1] * x\nplt.plot(x, rms, 'orange')\n# Adam\nadam = thetas[4][0] + thetas[4][1] * x\nplt.plot(x, adam, 'purple')\n\nlabels = ['Instances', 'Initial model', 'Mathematical solution', 'Gradient descent',\n         'Momentum optimizer', 'Nesterov optimizer', 'AdaGrad optmizer', 'RMSProp optimizer', 'Adam optimizer']\n\nplt.legend(labels)\n\nplt.show()","7beb2e44":"## Gradient descent optimization\n\nThe first algorithm we will code is the gradient descent, that works as follows:","e9bd61bb":"# Comparison of 6 optimization algorithms used in deep learning\n\nThe aim of this notebook is to compare the performance of six different optimization algorithms used in deep learning. This algorithms are the following ones:\n* Gradient descent optimization\n* Momentum optimization\n* Nesterov accelerated gradient optimization\n* AdaGrad optimizer\n* RMSProp optimizer\n* Adam optimization","211b5ce0":"Zooming it:","244429bf":"Let's zoom as we did in gradient descent:","1eea0086":"As you might see, Gradient Descent is much slower than the rest of algorithms. Momentum and Nesterov are slightly faster due to the fact that they don't have adaptative learning rate, like RMSProp and Adam have. However, this result does not mean that Momentum and Nesterov are always faster than RMSProp and Adam, it depends both in the shape of the cost function and the initialization of theta.","e684308a":"As we are using linear regression, there is a mathematical solution that solves the problem:","69c94eef":"## Momentum optimization\n\nGradient descent simply updates the weights theta substracting the gradient of the cost function multiplied by the learning rate. It doesn't care about what the earlier gradients were. Thus, if the local gradient is small, it goes very slowly. Momentum optimization cares about the previous gradient, however. At each iteration, it updates the momentum vector with the gradients, what it means that gradients are used as acceleration, not as speed. The algorithm is shown below:","342887eb":"So as to measure the optimization algorithms performance, the same learning rate and max. num of epochs will be used in all of them:","22d0e603":"To generate the data set, a linear model of parameters 5 (slope) and 4 (bias) will be simulated, and then it will be aded some noise with a normal distribution of 0 mean and 0.2 deviation","61da6528":"Where:\n* alpha = learning rate\n\nEvery 20 epochs the loss will be evaluated, and if it's difference with the loss 20 epochs before is lower than 10exp(-5), actual values of theta will be taken as good, so we will stop iterating. If not, we will continue with iterations","5dab98f0":"To compute the cost of the model, it has been used the mean square error function shown below:","2f3424c4":"Due to the fact that theta is randomly initialized, the mse in the first epochs is very high, so the learning curve is not very clear. Thus, we will zoom it starting in the 1000th epoch:","2eed1f58":"This functions are created in the cell below:","4a47bf86":"## Nesterov accelerated gradient optimization\n\nIt is slightly different to momentum optimization: it measures the gradient not in the local position, but in slightly ahead in the direction of the momentum:","de6ec09e":"Where:\n* theta is the model's parameter vector, containing the bias term. In this case, theta is a column vector containing two values, the bias term and the slope term\n* X is the feature vector, containing a x0 feature equal to 1 (multiplied to the bias term) and a x1 term\n* yp is the prediction of the model","0b8dbcc2":"### Needed iterations to reach the optimum solution","3ddd200d":"As you may notice, this algorithm needs more iterations to converge, due to the fact that theta0 is randomly initialized, so it is very far away the optimum solution. As the learning rate decays, the algorithm does not have time to reach the solution","370da38c":"### Obtanied model","75933884":"As shown above, all models has reached a the optimal solution except the AdaGrad optimizer, which would need more iterations or a higher learning rate to reach the solution","90ee09dc":"## Results","b0dcd052":"## Adam optimization\n\nThis algorithm combines the ideas of the momentum optimization and RMSProp:\n* keeps track of an exponentially decaying average of past gradients\n* keeps track of an exponentially decaying average of past squared gradients","1c7c7967":"Some initializations that will be used later:","5080db9d":"## AdaGrad optimization\n\nGradient descent starts by quickly going down the steepest slope, and then slowly goes down to the bottom of the valley. The AdaGrad algorithm detects this and corrects it's direction to point more toward the global optimum, scalling down the gradient vector along the steepest dimentions. This leads to a decay of the learning rate, but it dos as fast in steep dimentions as in gentler slopes.","b0d0cd95":"Let's see the learning curve:","3151dce5":"Owing to the fact that AdaGrad has not converged, we'll remove it from the visualization, so as to have a clearer one","9a336b94":"Their performance will be tested in a simple linear regression, so as to visualizing the obtained results\n\nAs you may know, the linear regression model is the shown below:","8d5f5e81":"## RMSProp optimizer\n\nOwing to the fact that AdaGrad slows down too fast, RMSProp algoithm only accumulates the gradients from the most recent iterations, by using exponential decay:","66b0858f":"Which has the following gradient:","17f9ce2f":"## Normal equation"}}