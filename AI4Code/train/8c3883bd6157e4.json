{"cell_type":{"e686cfb2":"code","0a393fe1":"code","34516e46":"code","17af0aad":"code","a9e71fee":"code","a777ceff":"code","5c53c9d0":"code","f6218a3e":"code","1e7269d7":"code","4ac77a7f":"code","f0bc88eb":"code","c2298f17":"code","91c2854f":"code","fba2b207":"code","82130215":"code","10b6a6de":"code","c5de69a4":"code","10197f63":"code","9b404047":"code","1079bc1c":"code","857c2089":"code","148d262a":"code","f141212b":"code","ff75c837":"code","3ea9089e":"code","216c08db":"code","3c68a558":"code","6114ab29":"code","5ce91c0d":"code","91cbbfff":"code","cdc0325e":"code","c7f67779":"code","cf9c7c68":"code","317dadda":"code","171b3521":"code","e3812cd2":"code","16190be7":"code","5b7ec82e":"code","a8d6ae59":"code","87a2a005":"code","31cabf73":"code","c95fb11e":"code","a41c57f1":"code","69165787":"code","4207a05c":"code","31dca885":"code","1aa8eb4a":"code","24448d80":"code","174480a0":"code","633a500a":"code","8bdbcb93":"code","cf4e46c3":"code","b8b91801":"code","e0c66271":"code","d23735d5":"code","daad749e":"code","5b05df83":"code","8fa03e55":"code","5ccd9aa2":"code","b45ccdde":"code","ad1948fa":"code","54d802d5":"code","bd8b5078":"code","fb038858":"code","3cf25d24":"code","598b16c0":"code","6cfbd0d9":"code","ddedc0d2":"code","87e40e08":"code","1fc8a582":"code","ccde8901":"code","88fe935c":"code","c6007823":"code","6ebc7ed4":"code","c8bebeb4":"code","406bdb17":"code","4afab060":"code","5f67de9b":"code","e6e96b7b":"code","ec4c89a9":"code","95791e68":"code","e4cd6c84":"code","b6d01671":"code","58cc94f5":"code","603c2213":"code","a491fee3":"code","7eff3284":"code","3daae131":"code","d5ddb9b2":"code","20ecad57":"code","fa7a106b":"code","fccc2bf2":"code","178442a5":"code","68ed01ec":"code","4ed1e73c":"code","0a1354fa":"code","1f608cf3":"code","fce0a24e":"code","d63a3ea3":"code","7ebc6977":"code","81d71eb0":"code","1c8e45f3":"code","b4641b86":"code","195ac492":"code","00c39131":"code","9044334b":"code","8cd8feb3":"code","37cad09d":"code","b83994b5":"code","50b1f64f":"code","2b235b35":"code","bcc32911":"code","a553c0e1":"code","1d29822c":"code","81794410":"code","c13c55c1":"code","706adf1e":"code","0c8b448f":"code","e66d2ed2":"code","d25cc18f":"code","81cee682":"code","b17af515":"code","55e64de6":"code","343dc2ef":"code","c1ab173a":"code","c0735109":"code","644eeb05":"code","921540ab":"code","6a84cf13":"code","c7dc3de6":"code","df394db9":"code","52183080":"code","ab1a13ba":"code","d5e0b4f6":"code","af97317e":"code","49fee002":"code","ece1a0ed":"code","492224e1":"code","a2622588":"code","adb5b576":"code","3ca551da":"code","92f3d22a":"code","af487dbb":"code","ce70a4e8":"code","22b477f4":"code","fe403dc6":"code","390acc24":"code","ef972251":"code","c10201c3":"code","c7c792c2":"code","dc97f2af":"code","d7d3a362":"code","8336a89e":"code","a9d914e0":"code","41669082":"code","79e98912":"code","f5e644e5":"code","a10b2d65":"code","cfd49d98":"code","6e690e24":"code","f1aade46":"code","30bf5acb":"code","7c6d04b2":"code","dc6199a5":"code","ed79fed0":"code","b919a6e5":"code","de7dd135":"code","ee875430":"code","101124dc":"code","07639c42":"code","4e08a704":"code","d8ea8c54":"code","c3c0b024":"code","26ca0293":"code","a90ff6b8":"code","9463799e":"code","137ccfa1":"code","5c400ae2":"code","2abf97bb":"code","a599efcd":"code","e1b33149":"code","ad397c71":"code","edd821c9":"code","118d7f3c":"code","304ead25":"code","90aa08ff":"code","d59f8d15":"code","d9904cd0":"code","3d58e5e2":"code","eb6a9cc9":"code","ba2524e4":"code","19f3cdf2":"markdown","a14bd92d":"markdown","206eda61":"markdown","0c5b7ab9":"markdown","5118f096":"markdown","a3926447":"markdown","d457e623":"markdown","54faf13c":"markdown","ead1fd4a":"markdown","335293ec":"markdown","a9589bb9":"markdown","8768becd":"markdown","fd3260a3":"markdown","e50a5504":"markdown","17126d0e":"markdown","fd2b31bd":"markdown","ffbe60cc":"markdown","be163a55":"markdown","2896b87b":"markdown","66b24fb1":"markdown","56bf7f6a":"markdown","2f724af6":"markdown","f8e77fd6":"markdown","06dbdf28":"markdown","81fe0188":"markdown","86c51494":"markdown","97529d71":"markdown","675fd045":"markdown","c7b0c7b7":"markdown","2ec81ff3":"markdown","ec178199":"markdown","870d427b":"markdown","e1626895":"markdown","336eaedf":"markdown","8d74854d":"markdown","32d1bfd0":"markdown","22755015":"markdown","a7af2055":"markdown","2af17ede":"markdown","cf395a0b":"markdown","f7054387":"markdown","6cb622dc":"markdown","6fb6fb72":"markdown","4b99ee14":"markdown","5de75278":"markdown","40ad49fa":"markdown","8ddc43d9":"markdown","5c457b1b":"markdown","6d321aec":"markdown","73c9072e":"markdown","c386ecf8":"markdown","adba1977":"markdown","5a8d9c7c":"markdown","21a30a95":"markdown","83388e73":"markdown","574515db":"markdown","bb5d5d0a":"markdown","fc8e32ec":"markdown","dc03f45f":"markdown","86df4880":"markdown","785dc224":"markdown","225066de":"markdown","698ad49a":"markdown","93b7c834":"markdown","ae826796":"markdown","53998c57":"markdown","329f69b8":"markdown","4f7eb897":"markdown","29b9ac4f":"markdown","55a0cde9":"markdown","e601867e":"markdown","2d19dbd4":"markdown","f04a63bf":"markdown","6f52d241":"markdown","82fdaf4c":"markdown","72e4b8f1":"markdown","8e3da22b":"markdown","c5e2e6ab":"markdown","321dcced":"markdown","bcb52e4b":"markdown","54f92ea8":"markdown","bc6b8b5d":"markdown","fadd8fa4":"markdown","0ae472dc":"markdown","f68c7a62":"markdown","fa408936":"markdown","385027eb":"markdown","7502da6b":"markdown","ce5ee10f":"markdown","6e677c78":"markdown","b06b40be":"markdown","89ac8c9d":"markdown","aac451ae":"markdown","026b5a6d":"markdown","c20f41dd":"markdown","2982e77c":"markdown","95b93d2d":"markdown","62d4505e":"markdown","97c11191":"markdown","57c3e711":"markdown","13854db9":"markdown","7bde76e2":"markdown","bffac8b5":"markdown","5fe8d041":"markdown","18aee1e6":"markdown","704b4487":"markdown","8f2a0110":"markdown","b0c4fa4a":"markdown","a39ea95c":"markdown","d1c1755c":"markdown","e5de0d73":"markdown","a14a8542":"markdown","0f8be93a":"markdown","06a18ef9":"markdown","397a13c0":"markdown","59e2b4c1":"markdown","699d13d9":"markdown","6bfe1758":"markdown","32509102":"markdown","dbb83f3c":"markdown","9c065c4f":"markdown","fe76f794":"markdown","f848fa76":"markdown","41cb4930":"markdown","b7c04d64":"markdown","7bcc297e":"markdown","a396211c":"markdown","9e696ea6":"markdown","55a1ed6e":"markdown","22693225":"markdown","3198af73":"markdown","7d33067d":"markdown","be8fb642":"markdown","bb5209ed":"markdown","08d7d6b6":"markdown","ec4617bc":"markdown","767f0243":"markdown","cf64144c":"markdown","75abc4d4":"markdown","e429a72c":"markdown","93f6e6fa":"markdown","9862e6e5":"markdown","19effca0":"markdown","b9d535f2":"markdown","93d14bac":"markdown","49d49b41":"markdown","dd617d84":"markdown","b965247e":"markdown","21402267":"markdown","1e46a475":"markdown","e2dc3e43":"markdown","f2ddeaf4":"markdown","1958b13f":"markdown","1f04fbf6":"markdown","57309059":"markdown","a1b6098c":"markdown","9a4bd360":"markdown","cf410637":"markdown","71b54d8d":"markdown","aa3531ae":"markdown"},"source":{"e686cfb2":"#import some necessary libraries\nimport datetime\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n%matplotlib inline\nimport matplotlib.pyplot as plt  # Matlab-style plotting\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\nimport time\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew, kurtosis, boxcox #for some statistics\nfrom scipy.special import boxcox1p, inv_boxcox, inv_boxcox1p\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\n\npd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points\n\n\nfrom subprocess import check_output\n#print(check_output([\"ls\", \"-rlt\", \"..\/StackedRegression\"]).decode(\"utf8\")) #check the files available in the directory\n#print(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\")) #check the files available in the directory\n\nStartTime = datetime.datetime.now()","0a393fe1":"class MyTimer():\n    # usage:\n    #with MyTimer():                            \n    #    rf.fit(X_train, y_train)\n    \n    def __init__(self):\n        self.start = time.time()\n    def __enter__(self):\n        return self\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        end = time.time()\n        runtime = end - self.start\n        msg = 'The function took {time} seconds to complete'\n        print(msg.format(time=runtime))","34516e46":"#train = pd.read_csv('input\/train.csv')\n#test = pd.read_csv('input\/test.csv')\n\n# default competition\ncompetition = 'SR' # StackedRegression\n\ntry:\n    a = check_output([\"ls\", \"..\/input\"]).decode(\"utf8\") # new Kaggle competition\nexcept:\n    a=''\nfinally:\n    print('')\ntry:\n    b = check_output([\"ls\", \"-rlt\", \"..\/StackedRegression\"]).decode(\"utf8\")\nexcept:\n    b=''\nfinally:\n    print('')    \n#if (competition == 'SRP_2'): # Stacked Regressions Part 2\nif (len(a) > 0): # new competition\n    competition = 'SR'\n    train = pd.read_csv('..\/input\/home-data-for-ml-course\/train.csv')#,index_col='Id')\n    test = pd.read_csv('..\/input\/home-data-for-ml-course\/test.csv')#,index_col='Id')\nelif (len(b)): # run locally\n    competition = 'SR'\n    train = pd.read_csv('input\/train.csv')\n    test = pd.read_csv('input\/test.csv')\nelse: # old competition\n    competition = 'SRP_2'\n    train = pd.read_csv('..\/input\/train.csv')\n    test = pd.read_csv('..\/input\/test.csv')","17af0aad":"##display the first five rows of the train dataset.\ntrain.head(5)","a9e71fee":"##display the first five rows of the test dataset.\ntest.head(5)\n","a777ceff":"fig, ax = plt.subplots()\n#ax.scatter(x = train['GrLivArea'], y = train['SalePrice']\nax.scatter(x = train['GrLivArea'], y = np.log(train['SalePrice']))\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\n\n#m, b = np.polyfit(train['GrLivArea'], train['SalePrice'], 1)\nm, b = np.polyfit(train['GrLivArea'], np.log(train['SalePrice']), 1)\n#m = slope, b=intercept\nplt.plot(train['GrLivArea'], m*train['GrLivArea'] + b)\n\nplt.show()\n","5c53c9d0":"train.shape[1]\n#a = int(np.sqrt(train.shape[1]))\na = 4\nb = int(train.shape[1]\/4)\nr = int(train.shape[1]\/a)\nc = int(train.shape[1]\/b)\ni = 0\nfig, ax = plt.subplots(nrows=r, ncols=c, figsize=(15, 60))\nfor row in ax:\n    for col in row:\n        #print(train.columns[i])\n        #print(train[train.columns[i]].dtype)\n        #col.plot()\n        #ax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\n        try:\n            #col.scatter(x = train[train.columns[i]], y = train['SalePrice'])\n            col.scatter(x = train[train.columns[i]], y = np.log(train['SalePrice']))\n            col.title.set_text(train.columns[i])\n            #col.title(train.columns[i])\n        except:\n            temp=1\n        #except Exception as e:\n        #    print(e.message, e.args)\n        finally:\n            temp=1\n        i = i + 1\n        \nplt.show()","f6218a3e":"#Deleting outliers\n\n#train = train.drop(train[(train['LotFrontage']>300) & (train['SalePrice']<400000)].index)\n#train = train.drop(train[(train['LotArea']>100000) & (train['SalePrice']<400000)].index)\n#train = train.drop(train[(train['BsmtFinSF1']>4000) & (train['SalePrice']<250000)].index)\n#train = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<250000)].index)\n\ntrain = train.drop(train[(train['OverallQual']>9) & (train['SalePrice']<220000)].index)\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n","1e7269d7":"test","4ac77a7f":"#check the numbers of samples and features\nprint(\"The train data size before dropping Id feature is : {} \".format(train.shape))\nprint(\"The test data size before dropping Id feature is : {} \".format(test.shape))\n\n#Save the 'Id' column\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\n\n#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)\n\n#check again the data size after dropping the 'Id' variable\nprint(\"\\nThe train data size after dropping Id feature is : {} \".format(train.shape)) \nprint(\"The test data size after dropping Id feature is : {} \".format(test.shape))","f0bc88eb":"#Check the graphic again\nfig, ax = plt.subplots()\n#ax.scatter(train['GrLivArea'], train['SalePrice'])\nax.scatter(train['GrLivArea'], np.log(train['SalePrice']))\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\n#m, b = np.polyfit(train['GrLivArea'], train['SalePrice'], 1)\nm, b = np.polyfit(train['GrLivArea'], np.log(train['SalePrice']), 1)\n#m = slope, b=intercept\nplt.plot(train['GrLivArea'], m*train['GrLivArea'] + b)\nplt.show()","c2298f17":"# linear \nx_data = train['GrLivArea']\ny_data = train['SalePrice']\nlog_y_data = np.log(train['SalePrice'])\n\ncurve_fit = np.polyfit(x_data, log_y_data, 1)\nprint(curve_fit)\ny = np.exp(1.11618915e+01) * np.exp(5.70762509e-04*x_data)\nplt.plot(x_data, y_data, \"o\")\nplt.scatter(x_data, y,c='red')","91c2854f":"# linear with log y\ny = np.exp(1.11618915e+01) * np.exp(5.70762509e-04*x_data)\nplt.plot(x_data, np.log(y_data), \"o\")\nplt.scatter(x_data, np.log(y),c='red')","fba2b207":"# polynomial\nx_data = train['GrLivArea']\ny_data = train['SalePrice']\nlog_y_data = np.log(train['SalePrice'])\ncurve_fit_gla = np.polyfit(x_data, y_data, 2)\ny = curve_fit_gla[2] + curve_fit_gla[1]*x_data + curve_fit_gla[0]*x_data**2","82130215":"plt.plot(x_data, y_data, \"o\")\nplt.scatter(x_data, y,c='red')","10b6a6de":"# polynomial with log y\nplt.plot(x_data, np.log(y_data), \"o\")\nplt.scatter(x_data, np.log(y),c='red')","c5de69a4":"y_data #HERE","10197f63":"# polynomial with log y\nplt.plot(x_data, np.log(y_data), \"o\")\nplt.scatter(x_data, np.log(y),c='red')","9b404047":"sns.distplot(train['SalePrice'] , fit=norm)\n\n_skew = skew(train['SalePrice'])\n_kurtosis = kurtosis(train['SalePrice'])\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}, skew = {:.2f} kurtosis = {:.2f}\\n'.format(mu, sigma, _skew, _kurtosis))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()\n","1079bc1c":"#We try the numpy function log1p which  applies log(1+x) to all elements of the column\n\n# option 1 - original\n#train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\n# Option 2: use box-cox transform - this performs worse than the log(1+x), reverting back...now it seems to be scoring better. Maybe my earlier evaluation was incorrect\n# try different alpha values  between 0 and 1\nlam_l = 0.35\ntrain[\"SalePrice\"] = boxcox1p(train[\"SalePrice\"], lam_l) \n\n# Option 3: boxcox letting the algorithm select lmbda based on least-likelihood calculation\n#train[\"SalePrice\"], lam_l = boxcox1p(x=train[\"SalePrice\"], lmbda=None)\n\n# option 4 - compare to log1p => score is same\n#train[\"SalePrice\"] = np.log(train[\"SalePrice\"])","857c2089":"x = np.linspace(0, 20)\ny1 = np.log(x)\ny2 = np.log1p(x)\ny3 = boxcox1p(x, 0.35)\ny4 = boxcox1p(x, 0.10)\ny5 = boxcox1p(x, 0.50)\nplt.plot(x,y1,label='log') \nplt.plot(x,y2,label='log1p') \nplt.plot(x,y3,label='boxcox .35') \nplt.plot(x,y4,label='boxcox .10') \nplt.plot(x,y5,label='boxcox .50') \nplt.legend()\nplt.show()\n\nx = np.linspace(0, 100000)\ny1 = np.log(x)\ny2 = np.log1p(x)\ny3 = boxcox1p(x, 0.35)\ny4 = boxcox1p(x, 0.10)\ny5 = boxcox1p(x, 0.50)\nplt.plot(x,y1,label='log') \nplt.plot(x,y2,label='log1p') \nplt.plot(x,y3,label='boxcox .35') \nplt.plot(x,y4,label='boxcox .10') \nplt.plot(x,y5,label='boxcox .50') \nplt.legend()\nplt.show()","148d262a":"# to revert back use y = inv_boxcox1p(x, lam_l) => train[\"SalePrice\"] = inv_boxcox1p(train[\"SalePrice\"], lam_l)\n# think the underlying formula is this: # pred_y = np.power((y_box * lambda_) + 1, 1 \/ lambda_) - 1\n\n\n#Check the new distribution \nsns.distplot(train['SalePrice'] , fit=norm);\n\n_skew = skew(train['SalePrice'])\n_kurtosis = kurtosis(train['SalePrice'])\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}, skew = {:.2f} kurtosis = {:.2f}\\n'.format(mu, sigma, _skew, _kurtosis))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()\n","f141212b":"ntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.SalePrice.values\nall_data = pd.concat((train, test)).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","ff75c837":"def add_gla(row, p):\n    return (p[2] + p[1]*row.GrLivArea + p[0]*(row.GrLivArea**2))\n\n#all_data['GrLivAreaPoly'] = all_data.apply(lambda row: add_gla(row,curve_fit_gla), axis=1)\n#all_data[['GrLivAreaPoly','GrLivArea']].head()","3ea9089e":"all_data.GrLivArea[:ntrain]","216c08db":"correlation_matrix = np.corrcoef(all_data.GrLivArea[:ntrain], y_train)\ncorrelation_xy = correlation_matrix[0,1]\nr_squared = correlation_xy**2\n\nprint(r_squared)","3c68a558":"a='''\ncorrelation_matrix = np.corrcoef(all_data.GrLivAreaPoly[:ntrain], y_train)\ncorrelation_xy = correlation_matrix[0,1]\nr_squared = correlation_xy**2\n\nprint(r_squared)'''","6114ab29":"for i in range(1,11,1):\n    j = i\/10\n    correlation_matrix = np.corrcoef(all_data.GrLivArea[:ntrain]**j, y_train)\n    correlation_xy = correlation_matrix[0,1]\n    r_squared = correlation_xy**2\n\n    print(j, r_squared)","5ce91c0d":"def add_gla2(row, p):\n    return (row.GrLivArea**p)\n\n#all_data['GrLivAreaRoot'] = all_data.apply(lambda row: add_gla2(row,0.3), axis=1)","91cbbfff":"all_data.head()","cdc0325e":"all_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head(30)","c7f67779":"all_numerical = all_data.select_dtypes(include=np.number).columns.tolist()\nmissing_data.index.values.tolist()\nmissing_df = all_data[missing_data.index.values.tolist()]\nmissing_numerical = missing_df.select_dtypes(include=np.number).columns.tolist()","cf9c7c68":"f, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=all_data_na.index, y=all_data_na)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","317dadda":"#Correlation map to see how features are correlated with each other and with SalePrice\ncorrmat = train.corr(method='kendall')\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=0.9, square=True)","171b3521":"print (corrmat['SalePrice'].sort_values(ascending=False)[:5], '\\n')\nprint (corrmat['SalePrice'].sort_values(ascending=False)[-5:])","e3812cd2":"all_data.shape","16190be7":"ImputeToNone = [\"Alley\", \"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinType2\", \"FireplaceQu\", \"GarageType\", \"GarageFinish\", \"GarageQual\", \"GarageCond\", \"PoolQC\", \"Fence\", \"MiscFeature\"]\nfor col in ImputeToNone:  \n    all_data[col].fillna(\"None\", inplace=True)","5b7ec82e":"#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\n#all_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n#    lambda x: x.fillna(x.median()))","a8d6ae59":"# selecting another column\nall_data[missing_numerical]","87a2a005":"corrmat_all = all_data[all_numerical].corr(method='kendall')\ncorrmat_all","31cabf73":"corrmat2 = all_data[missing_numerical].corr(method='kendall')\ncorrmat2\n#print (corrmat2['LotFrontage'].sort_values(ascending=False), '\\n')","c95fb11e":"all_data","a41c57f1":"def ImputeData(all_data, numerical_input, col_to_impute):\n    from sklearn.impute import KNNImputer\n    \n    Missing = all_data[numerical_input]\n    imputer = KNNImputer(n_neighbors=5, weights='uniform', metric='nan_euclidean')\n    imputer.fit(Missing)\n    Xtrans = imputer.transform(Missing)\n    df_miss = pd.DataFrame(Xtrans,columns = Missing.columns)\n    all_data[col_to_impute] = df_miss[col_to_impute]\n    return (all_data)\n    ","69165787":"all_data = ImputeData(all_data, all_numerical, 'LotFrontage')","4207a05c":"all_data","31dca885":"for col in ('GarageYrBlt', 'GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)","1aa8eb4a":"all_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)","24448d80":"all_data['MSZoning'].value_counts()","174480a0":"# this one may be a bit dangerous, maybe try to get zone from neighborhood most common value, similar to LotFrontage previously\nall_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\n\n# NEW, slightly worse score\n#all_data[\"MSZoning\"] = all_data.groupby(\"Neighborhood\")[\"MSZoning\"].transform(\n#    lambda x: x.fillna(x.mode()))","633a500a":"all_data['Utilities'].value_counts()","8bdbcb93":"all_data = all_data.drop(['Utilities'], axis=1)","cf4e46c3":"all_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")","b8b91801":"all_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])","e0c66271":"all_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])","d23735d5":"all_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])","daad749e":"all_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])","5b05df83":"\nall_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")\n\n","8fa03e55":"#Check remaining missing values if any \nall_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head()","5ccd9aa2":"all_data['MSSubClass'].value_counts()\n","b45ccdde":"all_data['OverallCond'].value_counts()","ad1948fa":"all_data.shape","54d802d5":"import datetime\nYr = all_data['YrSold'].min()\nMo = all_data['MoSold'].min()\nt = datetime.datetime(Yr, Mo, 1, 0, 0)\n\ndef calculateYrMo (row):   \n    return int((datetime.datetime(row.YrSold,row.MoSold,1) - t).total_seconds())","bd8b5078":"# either way will work\n#all_data['YrMoSold'] = all_data.apply(lambda row: int((datetime.datetime(row.YrSold,row.MoSold,1) - t).total_seconds()), axis=1)\n\nall_data['YrMoSold'] = all_data.apply(lambda row: calculateYrMo(row), axis=1)","fb038858":"#MSSubClass=The building class\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n\n\n#Changing OverallCond into a categorical variable\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\n\n\n#Year and month sold are transformed into categorical features.\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)\n","3cf25d24":"from sklearn.preprocessing import LabelEncoder\n#cols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n#        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n#        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n#        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n#        'YrSold', 'MoSold', 'YrMoSold')\n\n# Edit: Dropping PoolQC for missing values => makes the model worse, reverting\n#all_data = all_data.drop(['PoolQC'], axis=1)\n\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold', 'YrMoSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))\n\n# shape\nprint('Shape all_data: {}'.format(all_data.shape))\n","598b16c0":"#all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']","6cfbd0d9":"# feature engineering add new features \nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n\nall_data['YrBltAndRemod'] = all_data['YearBuilt'] + all_data['YearRemodAdd'] # A-\nall_data['Total_sqr_footage'] = (all_data['BsmtFinSF1'] + all_data['BsmtFinSF2'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']) # B-\n#all_data['Total_Bathrooms'] = (all_data['FullBath'] + (0.5 * all_data['HalfBath']) + all_data['BsmtFullBath'] + (0.5 * all_data['BsmtHalfBath'])) # C-\n#all_data['Total_porch_sf'] = (all_data['OpenPorchSF'] + all_data['3SsnPorch'] + all_data['EnclosedPorch'] + all_data['ScreenPorch'] + all_data['WoodDeckSF']) # D-\n#all_data['haspool'] = all_data['PoolArea'].apply(lambda x: 1 if x > 0 else 0) # E-\n#all_data['has2ndfloor'] = all_data['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0) # F-\n#all_data['hasgarage'] = all_data['GarageArea'].apply(lambda x: 1 if x > 0 else 0) # G-\n#all_data['hasbsmt'] = all_data['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0) # H-\n#all_data['hasfireplace'] = all_data['Fireplaces'].apply(lambda x: 1 if x > 0 else 0) # I-\n","ddedc0d2":"numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)\n","87e40e08":"skewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\n\nlam_f = 0.15\nfor feat in skewed_features:\n    #all_data[feat] += 1\n    all_data[feat] = boxcox1p(all_data[feat], lam_f)\n    \n    #all_data[skewed_features] = np.log1p(all_data[skewed_features])","1fc8a582":"all_data = pd.get_dummies(all_data)\nprint(all_data.shape)","ccde8901":"correlations = corrmat['SalePrice'].sort_values(ascending=False)\ndf_corr = correlations.to_frame()\nprint(df_corr.query(\"abs(SalePrice) < 0.011\"))\nlow_corr = df_corr.query(\"abs(SalePrice) < 0.011\").index.values.tolist()\n#print('dropping these columns for low correlation', low_corr)\n#for i in low_corr: \n#    all_data = all_data.drop([i], axis=1)","88fe935c":"# to choose number of components, look at this chart. Reference: https:\/\/jakevdp.github.io\/PythonDataScienceHandbook\/05.09-principal-component-analysis.html\nfrom sklearn.decomposition import PCA\n\npca = PCA().fit(all_data)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')","c6007823":"#do PCA\/StandardScaler+clip here or before the skew boxcox1p transform\n\nn_components = 50\npca = PCA(n_components=n_components)\nall_data_pca = pca.fit_transform(all_data)","6ebc7ed4":"print(all_data.shape)\nprint(all_data_pca.shape)","c8bebeb4":"weights = np.round(pca.components_, 3) \nev = np.round(pca.explained_variance_ratio_, 3)\nprint('explained variance ratio',ev)\npca_wt = pd.DataFrame(weights)#, columns=all_data.columns)\npca_wt.head()","406bdb17":"pca_wt.shape ","4afab060":"#Correlation map to see how features are correlated with each other and with SalePrice\ncorrmat = pd.DataFrame(all_data_pca).corr(method='kendall')\nplt.subplots(figsize=(12,9))\nplt.title(\"Kendall's Correlation Matrix PCA applied\", fontsize=16)\nsns.heatmap(corrmat, vmax=0.9, square=True)\n\n\n#Correlation map to see how features are correlated with each other and with SalePrice\ncorrmat = train.corr(method='kendall')\nplt.subplots(figsize=(12,9))\nplt.title(\"Kendall's Correlation Matrix Initial Train Set\", fontsize=16)\nsns.heatmap(corrmat, vmax=0.9, square=True);\n","5f67de9b":"train_orig = train.copy()\ntrain_orig['SalePrice'] = y_train\ncorrmat = train_orig.corr(method='kendall')\nprint (corrmat['SalePrice'].sort_values(ascending=False)[:5], '\\n')\nprint (corrmat['SalePrice'].sort_values(ascending=False)[-5:])","e6e96b7b":"a=''' not working now\ncorrelations = corrmat[\"SalePrice\"].sort_values(ascending=False)\nfeatures = correlations.index[0:10]\n\nsns.pairplot(train[features], size = 2.5)\nplt.show();'''","ec4c89a9":"print(type(all_data))\nprint(type(pd.DataFrame(all_data_pca)))","95791e68":"from sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nrc = RobustScaler()\n\nuse_pca = 0 # using PCA currently hurts the score\nuse_normalization = 0 # using StandardScaler doesn't work, try RobustScaler now\n\nif (use_pca == 1):\n    all_data_pca = pd.DataFrame(all_data_pca)\n    train = all_data_pca[:ntrain]\n    test = all_data_pca[ntrain:]\n    all_data_pca.head()\nelif (use_normalization == 1):\n    train = all_data[:ntrain]\n    test = all_data[ntrain:]\n    scaled_features = sc.fit_transform(train.values)\n    train = pd.DataFrame(scaled_features, index=train.index, columns=train.columns)\n    scaled_features = sc.transform(test.values)\n    test = pd.DataFrame(scaled_features, index=test.index, columns=test.columns)   \nelif (use_normalization == 2):\n    train = all_data[:ntrain]\n    test = all_data[ntrain:]\n    scaled_features = rc.fit_transform(train.values)\n    train = pd.DataFrame(scaled_features, index=train.index, columns=train.columns)\n    scaled_features = rc.transform(test.values)\n    test = pd.DataFrame(scaled_features, index=test.index, columns=test.columns)  \nelse:\n    # back to original splits (from train.csv and test.csv)\n    train = all_data[:ntrain]\n    test = all_data[ntrain:]\n","e4cd6c84":"save_data = 0\nif (save_data == 1):\n    df1 = train.copy()\n    df1['SalePrice'] = inv_boxcox1p(y_train, lam_l)\n    df1.insert(0, 'Id', list(train_ID), allow_duplicates=False)\n    df1.to_csv('HousePricesTrain.csv', index=False)  \n    df2 = test.copy()\n    df2.insert(0, 'Id', list(test_ID), allow_duplicates=False)\n    df2.to_csv('HousePricesTest.csv', index=False) ","b6d01671":"#Correlation map to see how features are correlated with each other and with SalePrice\ncorrmat = train.corr(method='kendall')\nplt.subplots(figsize=(24,18))\nsns.heatmap(corrmat, vmax=0.9, square=True)","58cc94f5":"#Correlation map to see how features are correlated with each other and with SalePrice\ncorrmat = test.corr(method='kendall')\nplt.subplots(figsize=(24,18))\nsns.heatmap(corrmat, vmax=0.9, square=True)","603c2213":"train.hist(bins=20, figsize=(30,20))\nplt.show()","a491fee3":"test.hist(bins=20, figsize=(30,20))\nplt.show()","7eff3284":"train.describe()","3daae131":"test.describe()","d5ddb9b2":"a='''\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nX = train\ny = y_train\ntrain, val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=201)#, stratify=y)'''","20ecad57":"print(\"Modelling: \", datetime.datetime.now())","fa7a106b":"from sklearn.linear_model import ElasticNet, Lasso, BayesianRidge, LassoLarsIC, LinearRegression, Ridge\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, AdaBoostRegressor, BaggingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split, cross_validate\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\n\n#from sklearn.metrics import mean_squared_log_error\n# to run locally: conda install -c anaconda py-xgboost\nimport xgboost as xgb\nimport lightgbm as lgb\n","fccc2bf2":"#Validation function\n# train.values and y_train are both log scaled so just need to take the square of the delta between them to calculate the error, then take the sqrt to get rmsle\n# but for now y_train is boxcox1p(), not log(). Use this to convert back: inv_boxcox1p(y_train, lam_l)\nn_folds=5 # was 5 => better score but twice as slow now\n\ndef rmsle_cv(model):\n    print(\"running rmsle_cv code\")\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values) # was 42\n    # other scores: https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#scoring-parameter\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf)) # also r2\n    print(\"raw rmse scores for each fold:\", rmse)\n    return(rmse)\n\ndef r2_cv(model):\n    print(\"running r2_cv code\")\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values) # was 42\n    # other scores: https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#scoring-parameter\n    r2= cross_val_score(model, train.values, y_train, scoring=\"r2\", cv = kf) # also r2\n    print(\"raw r2 scores for each fold:\", r2)\n    return(r2)\n\n# used for another competition\ndef mae_cv(model):\n    print(\"running mae_cv code\")\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values) # was 42\n    # other scores: https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#scoring-parameter\n    mae = -cross_val_score(model, train.values, y_train, scoring=\"neg_mean_absolute_error\", cv = kf) # also r2\n    print(\"raw mae scores for each fold:\", mae)\n    return(mae)\n\ndef all_cv(model, n_folds, cv):\n    print(\"running cross_validate\")\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values) # was 42\n    # other scores: https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#scoring-parameter\n    scorers = {\n        'r2': 'r2',\n        'nmsle': 'neg_mean_squared_log_error', \n        'nmse': 'neg_mean_squared_error',\n        'mae': 'neg_mean_absolute_error'\n    }\n    scores = cross_validate(model, train.values, y_train, scoring=scorers,\n                           cv=kf, return_train_score=True)\n    return(scores)","178442a5":"print(y_train.mean())\nprint(inv_boxcox1p(y_train, lam_l).mean())","68ed01ec":"def runGSCV(num_trials, features, y_values):\n    non_nested_scores = np.zeros(num_trials) # INCREASES BIAS\n    nested_scores = np.zeros(num_trials)\n    # Loop for each trial\n    for i in range(num_trials):\n        print(\"Running GridSearchCV:\")\n        with MyTimer():    \n            #grid_result = gsc.fit(train, y_train)  \n            grid_result = gsc.fit(features, y_values)  \n        non_nested_scores[i] = grid_result.best_score_\n        if (competition == 'SR'):\n            print(\"Best mae %f using %s\" % ( -grid_result.best_score_, grid_result.best_params_))\n        else:\n            print(\"Best rmse %f using %s\" % ( np.sqrt(-grid_result.best_score_), grid_result.best_params_))\n        \n        # nested\/non-nested cross validation: https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_nested_cross_validation_iris.html\n        with MyTimer():    \n            #nested_score = cross_val_score(gsc, X=train, y=y_train, cv=outer_cv, verbose=0).mean() \n            nested_score = cross_val_score(gsc, X=features, y=y_values, cv=outer_cv, verbose=0).mean() \n            # source code for cross_val_score is here: https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/master\/sklearn\/model_selection\/_validation.py#L137\n        if (competition == 'SR'):\n            print(\"nested mae score from KFold %0.3f\" % -nested_score)\n        else:\n            print(\"nested rmse score from KFold %0.3f\" % np.sqrt(-nested_score))\n        \n        nested_scores[i] = nested_score\n        print('grid_result',grid_result)\n        #if (competition == 'SR'):\n        print(\"mean scores: r2(%0.3f) mae(%0.3f) nmse(%0.3f) nmsle(%0.3f)\" % (grid_result.cv_results_['mean_test_r2'].mean(), -grid_result.cv_results_['mean_test_mae'].mean(),  np.sqrt(-grid_result.cv_results_['mean_test_nmse'].mean()), grid_result.cv_results_['mean_test_nmsle'].mean() ))\n        #print(\"mean scores: r2(%0.3f) nmse(%0.3f) mae(%0.3f)\" % (grid_result.cv_results_['mean_test_r2'].mean(), np.sqrt(-grid_result.cv_results_['mean_test_nmse'].mean()), grid_result.cv_results_['mean_test_mae'].mean()))\n    return grid_result\n","4ed1e73c":"def calc_scores(model):\n    score_mae = mae_cv(model)\n    print(\"\\n mae_cv score: {:.4f} ({:.4f})\\n\".format(score_mae.mean(), score_mae.std()))\n    score_rmsle = rmsle_cv(model)\n    print(\"\\n rmsle_cv score: {:.4f} ({:.4f})\\n\".format(score_rmsle.mean(), score_rmsle.std()))\n    score_r2 = r2_cv(model)\n    print(\"\\n r2_cv score: {:.4f} ({:.4f})\\n\".format(score_r2.mean(), score_r2.std()))\n    return (score_mae, score_rmsle, score_r2)\n\n# calculate all 3 at once, takes 1\/3 the time as calc_scores\ndef calc_all_scores(model, n_folds=5, cv=5):\n    scores = all_cv(model, n_folds, cv)\n    #scores['train_<scorer1_name>'']\n    #scores['test_<scorer1_name>'']\n    print(\"\\n mae_cv score: {:.4f} ({:.4f})\\n\".format( (-scores['test_mae']).mean(), scores['test_mae'].std() ))\n    print(\"\\n rmsle_cv score: {:.4f} ({:.4f})\\n\".format( (np.sqrt(-scores['test_nmse'])).mean(), scores['test_nmse'].std() ))\n    print(\"\\n r2_cv score: {:.4f} ({:.4f})\\n\".format( scores['test_r2'].mean(), scores['test_r2'].std() ))\n    return (scores)\n\n# useful when you can't decide on parameter setting from best_params_\n# result_details(grid_result,'mean_test_nmse',100)\ndef result_details(grid_result,sorting='mean_test_nmse',cols=100):\n    param_df = pd.DataFrame.from_records(grid_result.cv_results_['params'])\n    param_df['mean_test_nmse'] = np.sqrt(-grid_result.cv_results_['mean_test_nmse'])\n    param_df['std_test_nmse'] = np.sqrt(grid_result.cv_results_['std_test_nmse'])\n    param_df['mean_test_mae'] = -grid_result.cv_results_['mean_test_mae']\n    param_df['std_test_mae'] = -grid_result.cv_results_['std_test_mae']\n    param_df['mean_test_r2'] = -grid_result.cv_results_['mean_test_r2']\n    param_df['std_test_r2'] = -grid_result.cv_results_['std_test_r2']\n    return param_df.sort_values(by=[sorting]).tail(cols)\n\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef mae(y, y_pred):\n    return mean_absolute_error(y,y_pred)","0a1354fa":"# initialize the algorithm for the GridSearchCV function\nlasso = Lasso()\ntuningLasso = 1 # takes 2 minutes to complete\n\nif (tuningLasso == 1):\n    # use this when tuning\n    param_grid={\n        'alpha':[0.01,], # done, lower keeps getting better, but don't want to go too low and begin overfitting (alpha is related to L1 reg)\n        'fit_intercept':[True], # done, big difference\n        'normalize':[False], # done, big difference\n        'precompute':[False], # done, no difference\n        'copy_X':[True], # done, no difference\n        'max_iter':[200], # done\n        'tol':[0.005], # done, not much difference\n        'warm_start':[False], # done, no difference\n        'positive':[False], # done, big difference\n        'random_state':[1],\n        'selection':['cyclic'] # done both are same, cyclic is default\n    }\n\nelse:\n    # use this when not tuning\n    param_grid={\n        'alpha':[0.2],\n        'fit_intercept':[True],\n        'normalize':[False],\n        'precompute':[False],\n        'copy_X':[True],\n        'max_iter':[200],\n        'tol':[0.0001],\n        'warm_start':[False],\n        'positive':[False],\n        'random_state':[None],\n        'selection':['cyclic']\n    }\n\nscorers = {\n    'r2': 'r2',\n    'nmsle': 'neg_mean_squared_log_error', \n    'nmse': 'neg_mean_squared_error',\n    'mae': 'neg_mean_absolute_error'\n}\n# To be used within GridSearch \ninner_cv = KFold(n_splits=4, shuffle=True, random_state=None)\n\n# To be used in outer CV \nouter_cv = KFold(n_splits=4, shuffle=True, random_state=None)    \n\n#inner loop KFold example:\ngsc = GridSearchCV(\n    estimator=lasso,\n    param_grid=param_grid,\n    #scoring='neg_mean_squared_error', # 'roc_auc', # or 'r2', etc\n    scoring=scorers, # 'roc_auc', # or 'r2', etc -> can output several scores, but only refit to one. Others are just calculated\n    #scoring='neg_mean_squared_error', # or look here for other choices \n    # https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#scoring-parameter\n    #cv=5,\n    cv=inner_cv, # this will use KFold splitting to change train\/test\/validation datasets randomly\n    verbose=0,\n    return_train_score=True, # keep the other scores\n    refit='nmse' # use this one for optimizing\n)\n\ngrid_result = runGSCV(2, train, y_train)\n\nrd = result_details(grid_result,'random_state',100)\nrd[['random_state','alpha','mean_test_nmse','std_test_nmse','mean_test_mae','std_test_mae','mean_test_r2','std_test_r2']].sort_values(by=['random_state','alpha'])\n","1f608cf3":"tuning_lasso = 1\nlasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1)) # was 1\nlasso_new = make_pipeline(RobustScaler(), Lasso(**grid_result.best_params_))\n#l = \"{'alpha': 0.2, 'copy_X': True, 'fit_intercept': True, 'max_iter': 200, 'normalize': False, 'positive': False, 'precompute': False, 'random_state': 1, 'selection': 'cyclic', 'tol': 0.005, 'warm_start': False}\"\n#Lasso_new = make_pipeline(RobustScaler(), Lasso(**l))\n#lasso_ss = make_pipeline(StandardScaler(), Lasso(alpha =0.0005, random_state=1)) # was 1 => worse score","fce0a24e":"if (tuning_lasso == 1):\n    #TEMP\n    model_results = [] # model flow, mae, rmsle\n    models = [lasso, lasso_new]\n\n    for model in models:\n        #print(model)\n        with MyTimer(): \n            scores = calc_all_scores(model,5,5)\n        #print(\"------------------------------------------\")\n        model_results.append([model, (-scores['test_mae']).mean(), (np.sqrt(-scores['test_nmse'])).mean(), scores['test_r2'].mean()])\n\n    df_mr = pd.DataFrame(model_results, columns = ['model','mae','rmsle','r2'])\n    df_mr.sort_values(by=['rmsle'])","d63a3ea3":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1)) # was 1\n\nif (tuning_lasso == 1):\n    for i in [2,5,20,42,99]:\n        from sklearn.linear_model import Lasso\n        print('random_state =',i)\n\n        l = {'alpha': 0.01, 'copy_X': True, 'fit_intercept': True, 'max_iter': 200, 'normalize': False, 'positive': False, 'precompute': False, 'selection': 'cyclic', 'tol': 0.005, 'warm_start': False}\n        lasso_new = make_pipeline(RobustScaler(), Lasso(**l, random_state=i))\n        #lasso_new = Lasso(**l, random_state=i)\n\n        model_results = [] # model flow, mae, rmsle\n        models = [lasso, lasso_new]\n\n        for model in models:\n            #print(model)\n            with MyTimer(): \n                scores = calc_all_scores(model,5,5)\n            #print(\"------------------------------------------\")\n            model_results.append([model, (-scores['test_mae']).mean(), (np.sqrt(-scores['test_nmse'])).mean(), scores['test_r2'].mean()])\n\n        df_mr = pd.DataFrame(model_results, columns = ['model','mae','rmsle','r2'])\n        print(df_mr.sort_values(by=['rmsle']))\nelse:\n    lasso_new = make_pipeline(RobustScaler(), Lasso(**grid_result.best_params_, random_state=i))","7ebc6977":"model_results","81d71eb0":"# initialize the algorithm for the GridSearchCV function\nENet = ElasticNet()\ntuningENet = 0 # takes 2 minutes to complete\n\nif (tuningENet == 1):\n    # use this when tuning\n    param_grid={\n        'alpha':[0.005,0.01,0.05,0.1],\n        'l1_ratio':[.6,.65,.7,.75,.8,.85,.9],\n        'fit_intercept':[True], # ,False\n        'normalize':[False], # True,\n        'max_iter':range(50,500,50),\n        'selection':['random'], # 'cyclic',\n        'random_state':[None]\n    }\n\nelse:\n    # use this when not tuning\n    param_grid={\n        'alpha':[0.05],\n        'l1_ratio':[.85],\n        'fit_intercept':[True],\n        'normalize':[False],\n        'max_iter':[100], # default 1000\n        'selection':['random']\n    }\n\nscorers = {\n    'r2': 'r2',\n    'nmsle': 'neg_mean_squared_log_error',\n    'nmse': 'neg_mean_squared_error',\n    'mae': 'neg_mean_absolute_error'\n}\n# To be used within GridSearch \ninner_cv = KFold(n_splits=4, shuffle=True, random_state=None)\n\n# To be used in outer CV \nouter_cv = KFold(n_splits=4, shuffle=True, random_state=None)    \n\n#inner loop KFold example:\ngsc = GridSearchCV(\n    estimator=ENet,\n    param_grid=param_grid,\n    #scoring='neg_mean_squared_error', # 'roc_auc', # or 'r2', etc\n    scoring=scorers, # 'roc_auc', # or 'r2', etc -> can output several scores, but only refit to one. Others are just calculated\n    #scoring='neg_mean_squared_error', # or look here for other choices \n    # https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#scoring-parameter\n    #cv=5,\n    cv=inner_cv, # this will use KFold splitting to change train\/test\/validation datasets randomly\n    verbose=0,\n    return_train_score=True, # keep the other scores\n    refit='nmse' # use this one for optimizing\n)\n\ngrid_result = runGSCV(2, train, y_train)\n\nrd = result_details(grid_result,'mean_test_nmse',100)\nrd[['alpha','l1_ratio','mean_test_nmse','std_test_nmse','mean_test_mae','std_test_mae','mean_test_r2','std_test_r2']]#.sort_values(by=['n_estimators','mean_test_nmse'])\n","1c8e45f3":"#ENet_orig = make_pipeline(StandardScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\nENet = make_pipeline(StandardScaler(), ElasticNet(**grid_result.best_params_, random_state=3))\nENet_new = make_pipeline(RobustScaler(), ElasticNet(**grid_result.best_params_, random_state=3))\n","b4641b86":"# initialize the algorithm for the GridSearchCV function\nKRR = KernelRidge()\ntuningKRR = 0 # this took 40 mins, 20 per iteration\n\nif (tuningKRR == 1):\n    # use this when tuning\n    param_grid={\n        'alpha':[2.0,2.2,2.4,2.6], \n        'kernel':['polynomial'], #for entire list see: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.pairwise.kernel_metrics.html#sklearn.metrics.pairwise.kernel_metrics\n        'gamma':[0.0001,0.001,0.01,0.1],\n        'degree':[1,2,3,4,5,6], \n        'coef0':[0.1,0.3,0.5,1.0,2.0]\n    }\n\nelse:\n    # use this when not tuning\n    # nmse: Best mae 583416973.611280 using {'alpha': 2.2, 'coef0': 0.5, 'degree': 5, 'gamma': 0.001, 'kernel': 'polynomial'}\n    # mae: Best mae 15805.764347 using {'alpha': 2.0, 'coef0': 0.1, 'degree': 5, 'gamma': 0.001, 'kernel': 'polynomial'}\n    param_grid={\n        'alpha':[2.2], \n        'kernel':['polynomial'], # 'linear', 'rbf'\n        'gamma':[0.001],\n        'degree':[5], \n        'coef0':[0.5]\n    }\nscorers = {\n    'r2': 'r2',\n    'nmsle': 'neg_mean_squared_log_error',\n    'nmse': 'neg_mean_squared_error',\n    'mae': 'neg_mean_absolute_error'\n}\n# To be used within GridSearch \ninner_cv = KFold(n_splits=4, shuffle=True, random_state=None)\n# To be used in outer CV \nouter_cv = KFold(n_splits=4, shuffle=True, random_state=None)    \n\n#inner loop KFold example:\ngsc = GridSearchCV(\n    estimator=KRR,\n    param_grid=param_grid,\n    #scoring='neg_mean_squared_error', # 'roc_auc', # or 'r2', etc\n    scoring=scorers, # 'roc_auc', # or 'r2', etc -> can output several scores, but only refit to one. Others are just calculated\n    #scoring='neg_mean_squared_error', # or look here for other choices \n    # https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#scoring-parameter\n    #cv=5,\n    cv=inner_cv, # this will use KFold splitting to change train\/test\/validation datasets randomly\n    verbose=0,\n    return_train_score=True, # keep the other scores\n    refit='nmse' # use this one for optimizing\n)\n\ngrid_result = runGSCV(2, train, y_train)\n\nrd = result_details(grid_result,'mean_test_nmse',100)\nrd[['alpha','degree','mean_test_nmse','std_test_nmse','mean_test_mae','std_test_mae','mean_test_r2','std_test_r2']]#.sort_values(by=['n_estimators','mean_test_nmse'])\n","195ac492":"#KRR_orig = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\nKRR = KernelRidge(**grid_result.best_params_)","00c39131":"# initialize the algorithm for the GridSearchCV function\nrf = RandomForestRegressor()\ntuningRF = 0 # this took 2 hours last time, 1 hour per iteration\n\nif (tuningRF == 1):\n    # use this when tuning\n    param_grid={\n        'max_depth':[3,4,5],\n        'max_features':[None,'sqrt','log2'], \n        # 'max_features': range(50,401,50),\n        # 'max_features': [50,100], # can be list or range or other\n        'n_estimators':range(25,100,25), \n        #'class_weight':[None,'balanced'],  \n        'min_samples_leaf':range(5,15,5), \n        'min_samples_split':range(10,30,10), \n        'criterion':['mse', 'mae'] \n    }\n\nelse:\n    # use this when not tuning\n    param_grid={\n        'max_depth':[5],\n        'max_features':[None], # max_features is None is default and works here, removing 'sqrt','log2'\n        # 'max_features': range(50,401,50),\n        # 'max_features': [50,100], # can be list or range or other\n        'n_estimators': [50], # number of trees selecting 100, removing range(50,126,25)\n        #'class_weight':[None], # None was selected, removing 'balanced'\n        'min_samples_leaf': [5], #selecting 10, removing range 10,40,10)\n        'min_samples_split': [10], # selecting 20, removing range(20,80,10),\n        'criterion':['mse'] # remove gini as it is never selected\n    }\n\nscorers = {\n    'r2': 'r2',\n    'nmsle': 'neg_mean_squared_log_error',\n    'nmse': 'neg_mean_squared_error',\n    'mae': 'neg_mean_absolute_error'\n}\n# To be used within GridSearch \ninner_cv = KFold(n_splits=4, shuffle=True, random_state=None)\n\n# To be used in outer CV \nouter_cv = KFold(n_splits=4, shuffle=True, random_state=None)    \n\n#inner loop KFold example:\ngsc = GridSearchCV(\n    estimator=rf,\n    param_grid=param_grid,\n    #scoring='neg_mean_squared_error', # 'roc_auc', # or 'r2', etc\n    scoring=scorers, # 'roc_auc', # or 'r2', etc -> can output several scores, but only refit to one. Others are just calculated\n    #scoring='neg_mean_squared_error', # or look here for other choices \n    # https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#scoring-parameter\n    #cv=5,\n    cv=inner_cv, # this will use KFold splitting to change train\/test\/validation datasets randomly\n    verbose=0,\n    return_train_score=True, # keep the other scores\n    refit='nmse' # use this one for optimizing\n)\n\ngrid_result = runGSCV(2, train, y_train)\n\nrd = result_details(grid_result,'mean_test_nmse',100)\nrd[['n_estimators','mean_test_nmse','std_test_nmse','mean_test_mae','std_test_mae','mean_test_r2','std_test_r2']]#.sort_values(by=['n_estimators','mean_test_nmse'])\n","9044334b":"#RF_orig = make_pipeline(StandardScaler(), RandomForestRegressor(max_depth=3,n_estimators=500))\nRF = make_pipeline(StandardScaler(), RandomForestRegressor(**grid_result.best_params_)) # better than default, but still not good\nRF_new = make_pipeline(RobustScaler(), RandomForestRegressor(**grid_result.best_params_)) # better than default, but still not good","8cd8feb3":"print(\"Optimize GBoost: \", datetime.datetime.now())","37cad09d":"GBoost_orig = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state=5) # was 5\n\ntuning_gb = 0\nif (tuning_gb == 1):\n    # initialize the algorithm for the GridSearchCV function\n    GBoost = GradientBoostingRegressor()\n    tuningGB = 0\n\n    if (tuningGB == 1):\n        # use this when tuning\n        param_grid={\n            #'loss':['ls','lad','huber','quantile'],\n            'loss':['huber'], # done\n            'learning_rate':[0.05],\n            'n_estimators':[2000], # done\n            'subsample':[1.0],\n            #'criterion':['friedman_mse','mse','mae'],\n            'criterion':['friedman_mse'], # done\n            'min_samples_split':[10],\n            'min_samples_leaf':[15],\n            'min_weight_fraction_leaf':[0.0],\n            'max_depth':[2], # done\n            'min_impurity_decrease':[0.0],\n            'min_impurity_split':[None],\n            'init':[None],\n            'random_state':[None],\n            #'max_features':[None,'auto','sqrt','log2'],\n            'max_features':['sqrt'], # done\n            'alpha':[0.60], # done\n            'verbose':[0],\n            'max_leaf_nodes':[None],\n            'warm_start':[False],\n            'presort':['deprecated'],\n            'validation_fraction':[0.1],\n            'n_iter_no_change':[None],\n            'tol':[0.0001],\n            'ccp_alpha':[0.0],\n            'random_state':[5]\n        }\n\n    else:\n        # use this when not tuning\n        param_grid={\n            'loss':['huber'], \n            'learning_rate':[0.05],\n            'n_estimators':[2000], \n            'subsample':[1.0],\n            'criterion':['friedman_mse'], \n            'min_samples_split':[10],\n            'min_samples_leaf':[15],\n            'min_weight_fraction_leaf':[0.0],\n            'max_depth':[2], \n            'min_impurity_decrease':[0.0],\n            'min_impurity_split':[None],\n            'init':[None],\n            'random_state':[None],\n            'max_features':['sqrt'], \n            'alpha':[0.60], \n            'verbose':[0],\n            'max_leaf_nodes':[None],\n            'warm_start':[False],\n            'presort':['deprecated'],\n            'validation_fraction':[0.1],\n            'n_iter_no_change':[None],\n            'tol':[0.0001],\n            'ccp_alpha':[0.0],\n            'random_state':[5]\n        }\n    scorers = {\n        'r2': 'r2',\n        'nmsle': 'neg_mean_squared_log_error',\n        'nmse': 'neg_mean_squared_error',\n        'mae': 'neg_mean_absolute_error'\n    }\n    # To be used within GridSearch \n    inner_cv = KFold(n_splits=4, shuffle=True, random_state=None)\n    # To be used in outer CV \n    outer_cv = KFold(n_splits=4, shuffle=True, random_state=None)    \n\n    #inner loop KFold example:\n    gsc = GridSearchCV(\n        estimator=GBoost,\n        param_grid=param_grid,\n        scoring=scorers, # 'roc_auc', # or 'r2', etc -> can output several scores, but only refit to one. Others are just calculated\n        cv=inner_cv, # this will use KFold splitting to change train\/test\/validation datasets randomly\n        verbose=0,\n        return_train_score=True, # keep the other scores\n        refit='nmse' # use this one for optimizing\n    )\n\n    grid_result = runGSCV(2, train, y_train)\n\nrd = result_details(grid_result,'mean_test_nmse',100)\nrd[['n_estimators','mean_test_nmse','std_test_nmse','mean_test_mae','std_test_mae','mean_test_r2','std_test_r2']]#.sort_values(by=['n_estimators','mean_test_nmse'])\n","b83994b5":"\nif (tuning_gb == 1):\n    GBoost = GradientBoostingRegressor(**grid_result.best_params_)\nelse:\n    gbr = gbr  = {'alpha': 0.6, 'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.05, 'loss': 'huber', 'max_depth': 2, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 15, 'min_samples_split': 10, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 2000, 'n_iter_no_change': None, 'presort': 'deprecated', 'random_state': None, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n    GBoost = GradientBoostingRegressor(**gbr)","50b1f64f":"if (tuning_gb == 1):\n    #TEMP\n    model_results = [] # model flow, mae, rmsle\n    models = [GBoost_orig, GBoost]\n\n    for model in models:\n        #print(model)\n        with MyTimer(): \n            scores = calc_all_scores(model,5,5)\n        #print(\"------------------------------------------\")\n        model_results.append([model, (-scores['test_mae']).mean(), (np.sqrt(-scores['test_nmse'])).mean(), scores['test_r2'].mean()])\n\n    df_mr = pd.DataFrame(model_results, columns = ['model','mae','rmsle','r2'])\n    df_mr.sort_values(by=['rmsle'])","2b235b35":"if (tuning_gb == 1):\n    #TEMP\n\n    # random_state=None\n    #new  = {'alpha': 0.6, 'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.05, 'loss': 'huber', 'max_depth': 2, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 15, 'min_samples_split': 10, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 2000, 'n_iter_no_change': None, 'presort': 'deprecated', 'random_state': 5, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n    #GBoost_new = GradientBoostingRegressor(**new)\n\n    model_results = [] # model flow, mae, rmsle\n    models = [GBoost_orig, GBoost]\n\n    for model in models:\n        #print(model)\n        with MyTimer(): \n            scores = calc_all_scores(model,10,10)\n        #print(\"------------------------------------------\")\n        model_results.append([model, (-scores['test_mae']).mean(), (np.sqrt(-scores['test_nmse'])).mean(), scores['test_r2'].mean()])\n\n    df_mr = pd.DataFrame(model_results, columns = ['model','mae','rmsle','r2'])\n    df_mr.sort_values(by=['rmsle'])","bcc32911":"print(\"Optimize XGB: \", datetime.datetime.now())","a553c0e1":"tuning_xgb = 0\nif (tuning_xgb == 1):\n    # initialize the algorithm for the GridSearchCV function# initialize the algorithm for the GridSearchCV function\n    xgb1 = xgb.XGBRegressor()\n    tuningXGB = 1 # this took 2 hours last time, 1 hour per iteration\n\n    if (tuningXGB == 1):\n        # use this when tuning\n        param_grid={\n            'colsample_bytree':[0.4603],\n            'gamma':[0.0468], # done - all values almost identical results\n            'colsample_bylevel':[0.3], # done - all give same result\n            'objective':['reg:squarederror'], # done - Default:'reg:squarederror', None, reg:pseudohubererror, reg:squaredlogerror, reg:gamma\n            'booster':['gbtree'], # done - Default: 'gbtree', 'gblinear' or 'dart'\n            'learning_rate':[0.04], # done\n            'max_depth':[3], # - done\n            'importance_type':['gain'], # done - all give same value, Default:'gain', 'weight', 'cover', 'total_gain' or 'total_cover'\n            'min_child_weight':[1.7817], # done - no difference with several values\n            'n_estimators':[1000], # done\n            'reg_alpha':[0.4], # done\n            'reg_lambda':[0.8571], # done\n            'subsample':[0.5], # done\n            'silent':[1],\n            'random_state':[7],\n            'scale_pos_weight':[1],\n            'eval_metric':['rmse'], # done - all options have same results  Default:rmse for regression rmse, mae, rmsle, logloss, cox-nloglik\n            #'nthread ':[-1],\n            'verbosity':[0]\n        }\n\n    else:\n        # use this when not tuning\n        param_grid={\n            'colsample_bytree':[0.4603],\n            'gamma':[0.0468],\n            'colsample_bylevel':[0.3],\n            'objective':['reg:squarederror'], # 'binary:logistic', 'reg:squarederror', 'rank:pairwise', None\n            'booster':['gbtree'], # 'gbtree', 'gblinear' or 'dart'\n            'learning_rate':[0.04],\n            'max_depth':[3],\n            'importance_type':['gain'], # 'gain', 'weight', 'cover', 'total_gain' or 'total_cover'\n            'min_child_weight':[1.7817],\n            'n_estimators':[1000],\n            'reg_alpha':[0.4],\n            'reg_lambda':[0.8571],\n            'subsample':[0.5],\n            'silent':[1],\n            'random_state':[7],\n            'scale_pos_weight':[1],\n            'eval_metric':['rmse'],\n            #'nthread ':[-1],\n            'nthread ':[-1],\n            'verbosity':[0]\n        }\n\n    scorers = {\n        'r2': 'r2',\n        'nmsle': 'neg_mean_squared_log_error',\n        'nmse': 'neg_mean_squared_error',\n        'mae': 'neg_mean_absolute_error'\n    }\n    # To be used within GridSearch \n    inner_cv = KFold(n_splits=4, shuffle=True, random_state=None)\n\n    # To be used in outer CV \n    outer_cv = KFold(n_splits=4, shuffle=True, random_state=None)    \n\n    #inner loop KFold example:\n    gsc = GridSearchCV(\n        estimator=xgb1,\n        param_grid=param_grid,\n        #scoring='neg_mean_squared_error', # 'roc_auc', # or 'r2', etc\n        scoring=scorers, # 'roc_auc', # or 'r2', etc -> can output several scores, but only refit to one. Others are just calculated\n        #scoring='neg_mean_squared_error', # or look here for other choices \n        # https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#scoring-parameter\n        #cv=5,\n        cv=inner_cv, # this will use KFold splitting to change train\/test\/validation datasets randomly\n        verbose=0,\n        return_train_score=True, # keep the other scores\n        refit='nmse' # use this one for optimizing\n    )\n\n    grid_result = runGSCV(2, train, y_train)\n","1d29822c":"if (tuning_xgb == 1):\n    rd = result_details(grid_result,'mean_test_nmse',100)\n\n    rd[['random_state','eval_metric','mean_test_nmse','std_test_nmse','mean_test_mae','std_test_mae','mean_test_r2','std_test_r2']].sort_values(by=['random_state','mean_test_nmse'])\n","81794410":"model_xgb = xgb.XGBRegressor(booster='gbtree',colsample_bytree=0.4603, gamma=0.0468, # colsample_bylevel, objective, booster (gbtree, gblinear or dart.) # Default: 'gbtree'\n                             learning_rate=0.05, max_depth=3, # importance_type (\u201cgain\u201d, \u201cweight\u201d, \u201ccover\u201d, \u201ctotal_gain\u201d or \u201ctotal_cover\u201d.)\n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state=7) # was random_state=7, cannot set to None \n# maybe an issue with silent=1...\n\nif (tuning_xgb == 1):\n    import xgboost as xgb\n    for i in [2,5,20,42,99]:\n        print('random_state =',i)\n\n        model_xgb_new = xgb.XGBRegressor(booster='gbtree',colsample_bytree=0.4603, gamma=0.0468, # colsample_bylevel, objective, booster (gbtree, gblinear or dart.) # Default: 'gbtree'\n                                 learning_rate=0.04, max_depth=3, # importance_type (\u201cgain\u201d, \u201cweight\u201d, \u201ccover\u201d, \u201ctotal_gain\u201d or \u201ctotal_cover\u201d.)\n                                 min_child_weight=1.7817, n_estimators=1000,\n                                 reg_alpha=0.4, reg_lambda=0.8571,\n                                 subsample=0.45, silent=1,\n                                 random_state=i) # was random_state=7, cannot set to None \n\n        model_xgb_new2 = xgb.XGBRegressor(booster='gbtree',colsample_bytree=0.4603, gamma=0.0468, # colsample_bylevel, objective, booster (gbtree, gblinear or dart.) # Default: 'gbtree'\n                                 learning_rate=0.04, max_depth=3, # importance_type (\u201cgain\u201d, \u201cweight\u201d, \u201ccover\u201d, \u201ctotal_gain\u201d or \u201ctotal_cover\u201d.)\n                                 min_child_weight=1.7817, n_estimators=1000,\n                                 reg_alpha=0.4, reg_lambda=0.8571,\n                                 subsample=0.5, silent=1,\n                                 random_state=i) # was random_state=7, cannot set to None \n\n        model_xgb_new3 = xgb.XGBRegressor(booster='gbtree',colsample_bytree=0.4603, gamma=0.0468, # colsample_bylevel, objective, booster (gbtree, gblinear or dart.) # Default: 'gbtree'\n                                 learning_rate=0.04, max_depth=3, # importance_type (\u201cgain\u201d, \u201cweight\u201d, \u201ccover\u201d, \u201ctotal_gain\u201d or \u201ctotal_cover\u201d.)\n                                 min_child_weight=1.7817, n_estimators=1000,\n                                 reg_alpha=0.4, reg_lambda=0.8571,\n                                 subsample=0.5213, silent=1,\n                                 random_state=i) # was random_state=7, cannot set to None\n\n        model_results = [] # model flow, mae, rmsle\n        models = [model_xgb, model_xgb_new, model_xgb_new2, model_xgb_new3]\n\n        for model in models:\n            #print(model)\n            with MyTimer(): \n                scores = calc_all_scores(model,5,5)\n            #print(\"------------------------------------------\")\n            model_results.append([model, (-scores['test_mae']).mean(), (np.sqrt(-scores['test_nmse'])).mean(), scores['test_r2'].mean()])\n\n        df_mr = pd.DataFrame(model_results, columns = ['model','mae','rmsle','r2'])\n        print(df_mr.sort_values(by=['rmsle']))\nelse:\n    model_xgb_new = xgb.XGBRegressor(**grid_result.best_params_)","c13c55c1":"import graphviz\nmodel_xgb.fit(train, y_train,  verbose=False) #  eval_set=[(X_test, y_test)]\nxgb.plot_importance(model_xgb)\nxgb.to_graphviz(model_xgb, num_trees=20)","706adf1e":"print(\"Optimize LightGBM: \", datetime.datetime.now())","0c8b448f":"tuning_lgb = 0\nif (tuning_lgb == 1):\n    # initialize the algorithm for the GridSearchCV function\n    #model_lgb = lgb.LGBMRegressor(boosting_type='gbdt', max_depth=- 1, \n    #                            subsample_for_bin=200000, \n    #                            objective='regression',num_leaves=5,\n    #                            learning_rate=0.05, n_estimators=720,\n    #                            class_weight=None, min_split_gain=0.0, \n    #                            min_child_weight=0.001, min_child_samples=20, subsample=1.0, \n    #                            subsample_freq=0, colsample_bytree=1.0, reg_alpha=0.0, reg_lambda=0.0, \n    #                            random_state=None, n_jobs=- 1, silent=True, importance_type='split')\n\n    lgb1 = lgb.LGBMRegressor()\n    tuningLGB = 0\n\n    if (tuningLGB == 1):\n        # use this when tuning\n        param_grid={\n            'objective':['regression'], # - only one option for regression\n            'boosting_type':['gbdt'], # - done gbdt dart goss rf\n            'num_leaves':[5,6], # - done\n            'learning_rate':[0.05], # - done\n            'n_estimators':[650,750], # - done\n            'max_bin':[45,55], # - done\n            'bagging_fraction':[0.85], # - done\n            'bagging_freq':[5], # - done\n            'feature_fraction':[0.2319], # - done\n            'feature_fraction_seed':[9], \n            'bagging_seed':[9],\n            'min_data_in_leaf':[9], # - done\n            'min_sum_hessian_in_leaf':[11], # - done\n            'max_depth':[-1], # - -1 means no limit\n            'subsample_for_bin':[500,1000], # - done\n            'class_weight':[None],\n            'min_split_gain':[0.0],\n            'min_child_weight':[0.001],\n            'min_child_samples':[5], # - done\n            'subsample':[1.0],\n            'subsample_freq':[0],\n            'colsample_bytree':[1.0],\n            'reg_alpha':[0.0], # - l1 regularization done\n            'reg_lambda':[0.0], # - L2 regularization done\n            'random_state':[1],\n            'importance_type':['split'] # - done\n        }\n    else:\n        # use this when not tuning\n        param_grid={\n            'objective':['regression'], # - only one option for regression\n            'boosting_type':['gbdt'], # - done gbdt dart goss rf\n            'num_leaves':[5], # - done, maybe 5 is okay too\n            'learning_rate':[0.05], # - done\n            'n_estimators':[650], # - done\n            'max_bin':[55], # - done\n            'bagging_fraction':[0.85], # - done\n            'bagging_freq':[5], # - done\n            'feature_fraction':[0.2319], # - done\n            'feature_fraction_seed':[9], \n            'bagging_seed':[9],\n            'min_data_in_leaf':[9], # - done\n            'min_sum_hessian_in_leaf':[11], # - done\n            'max_depth':[-1], # - -1 means no limit\n            'subsample_for_bin':[1000], # - done\n            'class_weight':[None],\n            'min_split_gain':[0.0],\n            'min_child_weight':[0.001],\n            'min_child_samples':[5], # - done\n            'subsample':[1.0],\n            'subsample_freq':[0],\n            'colsample_bytree':[1.0],\n            'reg_alpha':[0.0], # - l1 regularization done\n            'reg_lambda':[0.0], # - L2 regularization done\n            'random_state':[1],\n            'importance_type':['split'] # - done\n        }\n\n    scorers = {\n        'r2': 'r2',\n        'nmsle': 'neg_mean_squared_log_error',\n        'nmse': 'neg_mean_squared_error',\n        'mae': 'neg_mean_absolute_error'\n    }\n    # To be used within GridSearch \n    inner_cv = KFold(n_splits=4, shuffle=True, random_state=None)\n\n    # To be used in outer CV \n    outer_cv = KFold(n_splits=4, shuffle=True, random_state=None)    \n\n    #inner loop KFold example:\n    gsc = GridSearchCV(\n        estimator=lgb1,\n        param_grid=param_grid,\n        #scoring='neg_mean_squared_error', # 'roc_auc', # or 'r2', etc\n        scoring=scorers, # 'roc_auc', # or 'r2', etc -> can output several scores, but only refit to one. Others are just calculated\n        #scoring='neg_mean_squared_error', # or look here for other choices \n        # https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#scoring-parameter\n        #cv=5,\n        cv=inner_cv, # this will use KFold splitting to change train\/test\/validation datasets randomly\n        verbose=0,\n        return_train_score=True, # keep the other scores\n        refit='nmse' # use this one for optimizing\n    )\n\n    grid_result = runGSCV(2, train, y_train)\n","e66d2ed2":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\nif (tuning_lgb == 1):\n    model_lgb_op = lgb.LGBMRegressor(**grid_result.best_params_)\nelse:\n    lgbm = {'bagging_fraction': 0.85, 'bagging_freq': 5, 'bagging_seed': 9, 'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'feature_fraction': 0.2319, 'feature_fraction_seed': 9, 'importance_type': 'split', 'learning_rate': 0.05, 'max_bin': 55, 'max_depth': -1, 'min_child_samples': 5, 'min_child_weight': 0.001, 'min_data_in_leaf': 9, 'min_split_gain': 0.0, 'min_sum_hessian_in_leaf': 11, 'n_estimators': 650, 'num_leaves': 5, 'objective': 'regression', 'random_state': None, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 1000, 'subsample_freq': 0}\n    model_lgb_op = lgb.LGBMRegressor(**lgbm)","d25cc18f":"# initialize the algorithm for the GridSearchCV function\nbr = BayesianRidge()\ntuningBR = 1 # this took 2 hours last time, 1 hour per iteration\n\nif (tuningBR == 1):\n    # use this when tuning\n    param_grid={\n        'n_iter':[50],\n        'tol':[0.001],\n        'alpha_1':[1e-06],\n        'alpha_2':[1e-05],\n        'lambda_1':[1e-05],\n        'lambda_2':[1e-06],\n        'alpha_init':[None],\n        'lambda_init':[None],\n        'compute_score':[True,False],\n        'fit_intercept':[True,False],\n        'normalize':[False,True],\n        'copy_X':[True],\n        'verbose':[False]\n    }\n\nelse:\n    # use this when not tuning\n    param_grid={\n        'n_iter':[50],\n        'tol':[0.001],\n        'alpha_1':[1e-06],\n        'alpha_2':[1e-05],\n        'lambda_1':[1e-05],\n        'lambda_2':[1e-06],\n        'alpha_init':[None],\n        'lambda_init':[None],\n        'compute_score':[True,False],\n        'fit_intercept':[True,False],\n        'normalize':[False,True],\n        'copy_X':[True],\n        'verbose':[False]\n    }\n\nscorers = {\n    'r2': 'r2',\n    'nmsle': 'neg_mean_squared_log_error',\n    'nmse': 'neg_mean_squared_error',\n    'mae': 'neg_mean_absolute_error'\n}\n# To be used within GridSearch \ninner_cv = KFold(n_splits=4, shuffle=True, random_state=None)\n\n# To be used in outer CV \nouter_cv = KFold(n_splits=4, shuffle=True, random_state=None)    \n\n#inner loop KFold example:\ngsc = GridSearchCV(\n    estimator=br,\n    param_grid=param_grid,\n    #scoring='neg_mean_squared_error', # 'roc_auc', # or 'r2', etc\n    scoring=scorers, # 'roc_auc', # or 'r2', etc -> can output several scores, but only refit to one. Others are just calculated\n    #scoring='neg_mean_squared_error', # or look here for other choices \n    # https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#scoring-parameter\n    #cv=5,\n    cv=inner_cv, # this will use KFold splitting to change train\/test\/validation datasets randomly\n    verbose=0,\n    return_train_score=True, # keep the other scores\n    refit='nmse' # use this one for optimizing\n)\n\ngrid_result = runGSCV(2, train, y_train)","81cee682":"tuning_br = 0\nBR = BayesianRidge()\nif (tuning_br == 1):\n    BR_new = BayesianRidge(**grid_result.best_params_)","b17af515":"rd = result_details(grid_result,'alpha_1',100)\nrd[['alpha_1','alpha_2','mean_test_nmse','std_test_nmse','mean_test_mae','std_test_mae','mean_test_r2','std_test_r2']]","55e64de6":"if (tuning_br == 1):\n    model_results = [] # model flow, mae, rmsle\n    models = [BR, BR_new]\n\n    for model in models:\n        #print(model)\n        with MyTimer(): \n            scores = calc_all_scores(model,10,10)\n        #print(\"------------------------------------------\")\n        model_results.append([model, (-scores['test_mae']).mean(), (np.sqrt(-scores['test_nmse'])).mean(), scores['test_r2'].mean()])\n\n    df_mr = pd.DataFrame(model_results, columns = ['model','mae','rmsle','r2'])\n    df_mr.sort_values(by=['rmsle'])","343dc2ef":"# initialize the algorithm for the GridSearchCV function\n# defaults are best\nET = ExtraTreesRegressor()\ntuningET = 0 # this took 2 hours last time, 1 hour per iteration\n\nif (tuningET == 1):\n    # use this when tuning\n    param_grid={\n        'n_estimators':[100], # revisit at possibly 2000, but this algorithm becomes really slow for large values\n        'criterion':['mse'], # done Default: mse, mae\n        'max_depth':[None], # done - above 30 result converges to None\n        'min_samples_split':[2], # done - inconsistently better\n        'min_samples_leaf':[1],\n        'min_weight_fraction_leaf':[0.0],\n        'max_features':['auto'], # done - Default:\u201cauto\u201d, \u201csqrt\u201d, \u201clog2\u201d, None\n        'max_leaf_nodes':[None],\n        'min_impurity_decrease':[0.0],\n        'min_impurity_split':[None],\n        'bootstrap':[False],\n        'oob_score':[False], # done - True doesn't work, results in a nan value\n        'n_jobs':[None],\n        'random_state':[1,5,42,55,98],\n        'verbose':[0],\n        'warm_start':[False],\n        'ccp_alpha':[0.0],\n        'max_samples':[None] # done - no difference\n    }\n\nelse:\n    # use this when not tuning\n    param_grid={\n        'n_estimators':[100],\n        'criterion':['mse'],\n        'max_depth':[None],\n        'min_samples_split':[2],\n        'min_samples_leaf':[1],\n        'min_weight_fraction_leaf':[0.0],\n        'max_features':['auto'],\n        'max_leaf_nodes':[None],\n        'min_impurity_decrease':[0.0],\n        'min_impurity_split':[None],\n        'bootstrap':[False],\n        'oob_score':[False],\n        'n_jobs':[None],\n        'random_state':[None],\n        'verbose':[0],\n        'warm_start':[False],\n        'ccp_alpha':[0.0],\n        'max_samples':[None]\n    }\n\nscorers = {\n    'r2': 'r2',\n    'nmsle': 'neg_mean_squared_log_error',\n    'nmse': 'neg_mean_squared_error',\n    'mae': 'neg_mean_absolute_error'\n}\n# To be used within GridSearch \ninner_cv = KFold(n_splits=4, shuffle=True, random_state=None)\n\n# To be used in outer CV \nouter_cv = KFold(n_splits=4, shuffle=True, random_state=None)    \n\n#inner loop KFold example:\ngsc = GridSearchCV(\n    estimator=ET,\n    param_grid=param_grid,\n    #scoring='neg_mean_squared_error', # 'roc_auc', # or 'r2', etc\n    scoring=scorers, # 'roc_auc', # or 'r2', etc -> can output several scores, but only refit to one. Others are just calculated\n    #scoring='neg_mean_squared_error', # or look here for other choices \n    # https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#scoring-parameter\n    #cv=5,\n    cv=inner_cv, # this will use KFold splitting to change train\/test\/validation datasets randomly\n    verbose=0,\n    return_train_score=True, # keep the other scores\n    refit='nmse' # use this one for optimizing\n)\n\ngrid_result = runGSCV(2, train, y_train)","c1ab173a":"rd = result_details(grid_result,'mean_test_nmse',100)\nrd[['random_state','n_estimators','mean_test_nmse','std_test_nmse','mean_test_mae','std_test_mae','mean_test_r2','std_test_r2']]","c0735109":"#ET = make_pipeline(RobustScaler(), ExtraTreesRegressor()) # was 1 Tree algorithms don't need scaling\ntuning_et = 0\nif (tuning_et == 1):\n    for i in [2,5,20,42,99]:\n        print('random_state =',i)\n        ET = ExtraTreesRegressor(random_state=i)\n        #ET2 = make_pipeline(StandardScaler(), ExtraTreesRegressor(random_state=i))\n\n        e = {'bootstrap': False, 'ccp_alpha': 0.0, 'criterion': 'mse', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 6, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 50, 'n_jobs': None, 'oob_score': False, 'verbose': 0, 'warm_start': False}\n        ET_new = ExtraTreesRegressor(**e, random_state=i)\n        #ET_new2 = make_pipeline(StandardScaler(), ExtraTreesRegressor(**e, random_state=i))\n\n        model_results = [] # model flow, mae, rmsle\n        models = [ET, ET_new]\n\n        for model in models:\n            #print(model)\n            with MyTimer(): \n                scores = calc_all_scores(model,5,5)\n            #print(\"------------------------------------------\")\n            model_results.append([model, (-scores['test_mae']).mean(), (np.sqrt(-scores['test_nmse'])).mean(), scores['test_r2'].mean()])\n\n        df_mr = pd.DataFrame(model_results, columns = ['model','mae','rmsle','r2'])\n        print(df_mr.sort_values(by=['rmsle']))\nelse:\n    ET_new = make_pipeline(RobustScaler(), ExtraTreesRegressor(**grid_result.best_params_))","644eeb05":"# initialize the algorithm for the GridSearchCV function\nR = Ridge(alpha=1.0)\ntuningR = 1 # this took 2 hours last time, 1 hour per iteration\n\nif (tuningR == 1):\n    # use this when tuning\n    param_grid={\n        'alpha':[8], # done\n        'fit_intercept':[True], # done\n        'normalize':[False], # done\n        'copy_X':[True],\n        'max_iter':[None], # done - no difference\n        'tol':[0.001],\n        'solver':['auto'], # done - Default:\u2018auto\u2019, \u2018svd\u2019, \u2018cholesky\u2019, \u2018lsqr\u2019, \u2018sparse_cg\u2019, \u2018sag\u2019, \u2018saga\u2019\n        'random_state':[1,10,42,99,127]\n    }\n\nelse:\n    # use this when not tuning\n    param_grid={\n        'alpha':[1.0],\n        'fit_intercept':[True],\n        'normalize':[False],\n        'copy_X':[True],\n        'max_iter':[None],\n        'tol':[0.001],\n        'solver':['auto'],\n        'random_state':[None]\n    }\n\nscorers = {\n    'r2': 'r2',\n    'nmsle': 'neg_mean_squared_log_error',\n    'nmse': 'neg_mean_squared_error',\n    'mae': 'neg_mean_absolute_error'\n}\n# To be used within GridSearch \ninner_cv = KFold(n_splits=4, shuffle=True, random_state=None)\n\n# To be used in outer CV \nouter_cv = KFold(n_splits=4, shuffle=True, random_state=None)    \n\n#inner loop KFold example:\ngsc = GridSearchCV(\n    estimator=R,\n    param_grid=param_grid,\n    #scoring='neg_mean_squared_error', # 'roc_auc', # or 'r2', etc\n    scoring=scorers, # 'roc_auc', # or 'r2', etc -> can output several scores, but only refit to one. Others are just calculated\n    #scoring='neg_mean_squared_error', # or look here for other choices \n    # https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#scoring-parameter\n    #cv=5,\n    cv=inner_cv, # this will use KFold splitting to change train\/test\/validation datasets randomly\n    verbose=0,\n    return_train_score=True, # keep the other scores\n    refit='nmse' # use this one for optimizing\n)\n\ngrid_result = runGSCV(5, train, y_train)","921540ab":"rd = result_details(grid_result,'mean_test_nmse',100)\nsummary = rd[['alpha','random_state','mean_test_nmse','std_test_nmse','mean_test_mae','std_test_mae','mean_test_r2','std_test_r2']].sort_values(by=['random_state','mean_test_nmse'])\n#summary.groupby(['fit_intercept'], as_index=False).agg({'mean_test_nmse': 'mean', 'mean_test_nmse': 'std', 'mean_test_mae': 'mean', 'mean_test_r2': 'mean'}).sort_values(by=['mean_test_mae'])\nsummary.groupby(['alpha'], as_index=False).agg({'mean_test_nmse': 'mean', 'mean_test_mae': 'mean', 'mean_test_r2': 'mean'}).sort_values(by=['mean_test_mae'])\n","6a84cf13":"#R = make_pipeline(RobustScaler(), Ridge(alpha =0.0005, random_state=1)) # was 1\ntuning_r = 1\nif (tuning_r == 1):\n    for i in [2,5,20,42,99]:\n        print('random_state =',i)\n\n        r= {'alpha': 8, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'normalize': False, 'solver': 'auto', 'tol': 0.001}\n        #R_new = make_pipeline(RobustScaler(), Ridge(**r, random_state=i))\n        R_new = Ridge(**r, random_state=i)\n\n        model_results = [] # model flow, mae, rmsle\n        models = [R, R_new]\n\n        for model in models:\n            #print(model)\n            with MyTimer(): \n                scores = calc_all_scores(model,5,5)\n            #print(\"------------------------------------------\")\n            model_results.append([model, (-scores['test_mae']).mean(), (np.sqrt(-scores['test_nmse'])).mean(), scores['test_r2'].mean()])\n\n        df_mr = pd.DataFrame(model_results, columns = ['model','mae','rmsle','r2'])\n        print(df_mr.sort_values(by=['rmsle']))\nelse:\n    #R_new = make_pipeline(RobustScaler(), Ridge(**grid_result.best_params_))\n    R_new = Ridge(**grid_result.best_params_)","c7dc3de6":"AB = AdaBoostRegressor()\nSVR = SVR()\nDT = DecisionTreeRegressor()\nKN = KNeighborsRegressor()\nB = BaggingRegressor()","df394db9":"if (tuning_gb == 1):\n    model_results = [] # model flow, mae, rmsle\n    models = [GBoost, GBoost_orig]#, GBoost] # model_lgb_op, lasso_ns, \n\n    for model in models:\n        #print(model)\n        with MyTimer(): \n            scores = calc_all_scores(model)\n        #print(\"------------------------------------------\")\n        model_results.append([model, (-scores['test_mae']).mean(), (np.sqrt(-scores['test_nmse'])).mean(), scores['test_r2'].mean()])\n\n    df_mr = pd.DataFrame(model_results, columns = ['model','mae','rmsle','r2'])\n    df_mr.sort_values(by=['rmsle'])","52183080":"print(\"Calculate Metrics: \", datetime.datetime.now())","ab1a13ba":"compare_models = 0\nif (compare_models == 1):\n    model_results = [] # model flow, mae, rmsle\n    # GBoost is better than GBoost_orig, but has lower final score, think this may be  overfitting\n    # BR_new has same results, here, but better final score\n    models = [lasso, lasso_new, ENet, ENet_new, KRR, GBoost_orig, GBoost, model_xgb, model_lgb, BR, ET, ET_new, RF, RF_new, AB, SVR, DT, KN, B, R, R_new] # worse or same: BR_new, model_lgb_op, lasso_ns, model_xgb_new,\n\n    for model in models:\n        #print(model)\n        with MyTimer(): \n            scores = calc_all_scores(model)\n        #print(\"------------------------------------------\")\n        model_results.append([model, (-scores['test_mae']).mean(), (np.sqrt(-scores['test_nmse'])).mean(), scores['test_r2'].mean()])\n\n    df_mr = pd.DataFrame(model_results, columns = ['model','mae','rmsle','r2'])\n    df_mr.sort_values(by=['rmsle'])","d5e0b4f6":"print(\"Stacking and Ensembling: \", datetime.datetime.now())","af97317e":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        # IDEA: return weighted means\n        return np.mean(predictions, axis=1)\n","49fee002":"#AveragingModels will fit and predict each model and predict using the mean of the individual predictions\nwith MyTimer():\n    averaged_models = AveragingModels(models = (model_lgb, model_xgb, GBoost_orig, KRR, BR)) # Adding ENet and RF is worse, model_xgb_new is worse\n    #averaged_models = AveragingModels(models = (GBoost))\n\nif (competition == 'SR'):\n    score = mae_cv(averaged_models)\n    print(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\nelse:\n    score = rmsle_cv(averaged_models)\n    print(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","ece1a0ed":"averaged_models.fit(train.values, y_train)\naveraged_train_pred = averaged_models.predict(train.values)\naveraged_pred = inv_boxcox1p(averaged_models.predict(test.values), lam_l)\n\nif (competition == 'SR'):\n    print(mae(y_train, averaged_train_pred))\nelse:\n    print(rmsle(y_train, averaged_train_pred))","492224e1":"averaged_pred","a2622588":"use_average = 0 # AveragingModels Averages\n#use_average = 1 # VotingRegressor Averages\nif (use_average == 1):\n    from sklearn.ensemble import VotingRegressor\n\n    e = {'alpha': 0.05, 'fit_intercept': True, 'l1_ratio': 0.85, 'max_iter': 100, 'normalize': False, 'selection': 'random'}\n    r = {'alpha': 8, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'normalize': False, 'solver': 'auto', 'tol': 0.001, 'cv': None, 'gcv_mode': 'auto', 'random_state': 99} #  \n    k = {'alpha': 2.2, 'coef0': 0.5, 'degree': 5, 'gamma': 0.001, 'kernel': 'polynomial'}\n\n    # ('enet', make_pipeline(StandardScaler(), ElasticNet(**e, random_state=3)))\n    estimator_list = [('xgb', xgb.XGBRegressor(booster='gbtree',colsample_bytree=0.4603, gamma=0.0468, learning_rate=0.05, max_depth=3, min_child_weight=1.7817, n_estimators=2200, reg_alpha=0.4640, reg_lambda=0.8571, subsample=0.5213, silent=1, random_state=7)),\n                  ('lgb', lgb.LGBMRegressor(objective='regression',num_leaves=5, learning_rate=0.05, n_estimators=720, max_bin = 55, bagging_fraction = 0.8, bagging_freq = 5, feature_fraction = 0.2319, feature_fraction_seed=9, bagging_seed=9, min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)),\n                  ('gboost', GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt', min_samples_leaf=15, min_samples_split=10,  loss='huber', random_state=5)),\n                  ('krr', KernelRidge(**k)),\n                  ('br', BayesianRidge())]\n\n    ereg = VotingRegressor(estimators=estimator_list, weights=[1,1,1,1,1]) \n    ereg = ereg.fit(train, y_train)\n    averaged_pred = inv_boxcox1p(ereg.predict(test), lam_l)\n    averaged_pred","adb5b576":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=10): # increasing this value should give a more accurate prediction, averaged over n_fold iterations\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156) # was 156\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models): # for each model passed in\n            for train_index, holdout_index in kfold.split(X, y): # create train,test for the number of folds\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index]) # fit the model for this fold\n                y_pred = instance.predict(X[holdout_index]) # predict values for this fold\n                out_of_fold_predictions[holdout_index, i] = y_pred # think we either use all of these values as features later, or the mean value?\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new and only feature\n        print(\"out_of_fold_predictions\", out_of_fold_predictions)\n        self.meta_model_.fit(out_of_fold_predictions, y) # need to see out_of_fold_predictions feature set\n        return self\n   \n    # Calculate the predictions of all base models on the test data and use the averaged predictions as \n    # meta-features for the final prediction which is calculated by the meta-model\n    def predict(self, X):\n        # column_stack() function is used to stack 1-D arrays as columns into a 2-D array.\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","3ca551da":"# ~ 10 minutes to run\n\n#stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR), # adding RF here did not help\n#                                                 meta_model = lasso)\n# verify: this class uses out of fold predictions in the stacking method, so rows in dataset are split up betwen models and each row in dataset is only used once\nstacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost_orig, KRR, BR), # adding RF here did not help\n                                                 meta_model = R_new)\n\nif (compare_models == 1):\n    with MyTimer():\n        if (competition == 'SR'):\n            score = mae_cv(stacked_averaged_models)\n            print(\"Stacking Averaged models score mean and std: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n        else:\n            score = rmsle_cv(stacked_averaged_models)\n            print(\"Stacking Averaged models score mean and std: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","92f3d22a":"stacked_averaged_models","af487dbb":"use_Regressor = 0 # default - best score\n#use_Regressor = 1 # StackingRegressor - worst score\n#use_Regressor = 2 # StackingCVRegressor - middle score\n\nif (use_Regressor == 1):\n    from sklearn.ensemble import StackingRegressor\n    from sklearn.linear_model import RidgeCV, LassoCV\n    from sklearn.linear_model import ElasticNetCV\n\n    use_cv = 1\n    k = {'alpha': 2.2, 'coef0': 0.5, 'degree': 5, 'gamma': 0.001, 'kernel': 'polynomial'}\n    if (use_cv == 1):\n        e = {'fit_intercept': True, 'l1_ratio': 0.85, 'max_iter': 100, 'normalize': False, 'selection': 'random', 'cv': 10} # 'alpha': 0.05,\n        r = {'fit_intercept': True, 'normalize': False, 'cv': None, 'gcv_mode': 'auto'} # cv value has no effect\n        estimators = [('enet', make_pipeline(StandardScaler(), ElasticNetCV(**e, random_state=3))),\n                      ('gboost', GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt', min_samples_leaf=15, min_samples_split=10,  loss='huber', random_state=5)),\n                      ('krr', KernelRidge(**k)),\n                      ('br', BayesianRidge())]\n        reg = StackingRegressor(\n            estimators=estimators,\n            final_estimator=RidgeCV(**r))\n    else:\n        e = {'alpha': 0.05, 'fit_intercept': True, 'l1_ratio': 0.85, 'max_iter': 100, 'normalize': False, 'selection': 'random'}\n        r = {'alpha': 8, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'normalize': False, 'solver': 'auto', 'tol': 0.001, 'cv': None, 'gcv_mode': 'auto', 'random_state': 99} #  \n        estimators = [('enet', make_pipeline(StandardScaler(), ElasticNet(**e, random_state=3))),\n                      ('gboost', GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt', min_samples_leaf=15, min_samples_split=10,  loss='huber', random_state=5)),\n                      ('krr', KernelRidge(**k)),\n                      ('br', BayesianRidge())]\n        reg = StackingRegressor(\n            estimators=estimators,\n            final_estimator=Ridge(**r))\n\n    reg.fit(train, y_train)\n    stacked_pred = inv_boxcox1p(reg.predict(test.values), lam_l)\n    #reg.transform(inv_boxcox1p(stacked_averaged_models2, lam_l))\n    print(stacked_pred)\n    print(reg)","ce70a4e8":"a='''\n# Note Multiple stacking layers can be achieved by assigning final_estimator to a StackingClassifier or StackingRegressor:\n\nfinal_layer = StackingRegressor(\n    estimators=[('rf', RandomForestRegressor(random_state=42)),\n                ('gbrt', GradientBoostingRegressor(random_state=42))],\n    final_estimator=RidgeCV()\n    )\nmulti_layer_regressor = StackingRegressor(\n    estimators=[('ridge', RidgeCV()),\n                ('lasso', LassoCV(random_state=42)),\n                ('svr', SVR(C=1, gamma=1e-6, kernel='rbf'))],\n    final_estimator=final_layer\n)\nmulti_layer_regressor.fit(X_train, y_train)'''","22b477f4":"if (use_Regressor == 2):\n    from mlxtend.regressor import StackingCVRegressor\n    from sklearn.linear_model import RidgeCV, LassoCV\n    from sklearn.linear_model import ElasticNetCV\n\n    use_cv = 0\n    k = {'alpha': 2.2, 'coef0': 0.5, 'degree': 5, 'gamma': 0.001, 'kernel': 'polynomial'}\n    if (use_cv == 1):\n        e = {'fit_intercept': True, 'l1_ratio': 0.85, 'max_iter': 100, 'normalize': False, 'selection': 'random', 'cv': 10} # 'alpha': 0.05,\n        r = {'fit_intercept': True, 'normalize': False, 'cv': None, 'gcv_mode': 'auto'} # cv value has no effect\n        enet = make_pipeline(StandardScaler(), ElasticNetCV(**e, random_state=3))\n        gboost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt', min_samples_leaf=15, min_samples_split=10,  loss='huber', random_state=5)\n        krr = KernelRidge(**k)\n        br = BayesianRidge()\n        r = RidgeCV(**r)\n        reg = StackingCVRegressor(\n            regressors=(enet, gboost, krr, br),\n            meta_regressor=r)\n    else:\n        e = {'alpha': 0.05, 'fit_intercept': True, 'l1_ratio': 0.85, 'max_iter': 100, 'normalize': False, 'selection': 'random'}\n        r = {'alpha': 8, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'normalize': False, 'solver': 'auto', 'tol': 0.001, 'random_state': 99} #  \n        enet = make_pipeline(StandardScaler(), ElasticNet(**e, random_state=3))\n        gboost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt', min_samples_leaf=15, min_samples_split=10,  loss='huber', random_state=5)\n        krr = KernelRidge(**k)\n        br = BayesianRidge()\n        r = Ridge(**r)\n        reg = StackingCVRegressor(\n            regressors=(enet, gboost, krr, br),\n            meta_regressor=r)\n\n    reg.fit(train, y_train)\n    stacked_pred = inv_boxcox1p(reg.predict(test.values), lam_l)\n    print(stacked_pred)\n    print(reg)\n    \n    print('5-fold cross validation scores:\\n')\n    for clf, label in zip([enet, gboost, krr, br], ['enet', 'gboost', \n                                                'krr', 'br',\n                                                'StackingCVRegressor']):\n        scores = cross_val_score(clf, train, y_train, cv=5, scoring='neg_mean_squared_error')\n        print(\"Neg. MSE Score: %0.2f (+\/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n        \n        #scores = cross_val_score(clf, train, y_train, cv=5)\n        #print(\"R^2 Score: %0.2f (+\/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))","fe403dc6":"averaged_models.fit(train.values, y_train)\naveraged_train_pred = averaged_models.predict(train.values)\nif (use_average == 0):\n    averaged_pred = inv_boxcox1p(averaged_models.predict(test.values), lam_l)\n\nif (competition == 'SR'):\n    print(mae(y_train, averaged_train_pred))\nelse:\n    print(rmsle(y_train, averaged_train_pred))","390acc24":"#pre adjustment\naveraged_pred","ef972251":"plt.scatter(averaged_train_pred, y_train, alpha=.7,\n            color='b') #alpha helps to show overlapping data\nplt.xlabel('Predicted Price')\nplt.ylabel('Actual Price')\nplt.title('Averaged Model')\nplt.show()","c10201c3":"averaged_models.predict(test.values)","c7c792c2":"averaged_train_pred","dc97f2af":"#post adjustment\naveraged_pred","d7d3a362":"stacked_averaged_models.fit(train.values, y_train)\nstacked_train_pred = stacked_averaged_models.predict(train.values)\nif (use_Regressor == 0):\n    stacked_pred = inv_boxcox1p(stacked_averaged_models.predict(test.values), lam_l)\n    \nwith MyTimer():\n    if (competition == 'SR'):\n        print(mae(y_train, stacked_train_pred))\n    else:\n        print(rmsle(y_train, stacked_train_pred))","8336a89e":"stacked_pred","a9d914e0":"print(y_train)\nprint(stacked_train_pred)\nprint(stacked_pred)","41669082":"def fit_pred(train, ytrain, test, model):\n        model.fit(train, y_train)\n        model_train_pred = model.predict(train)\n        model_pred = inv_boxcox1p(model.predict(test), lam_l)\n        return(model_train_pred, model_pred)\n    \nmodels = [lasso, lasso_new, ENet, KRR, GBoost_orig, GBoost, model_xgb, model_xgb_new, BR, ET, ET_new, RF, RF_new, AB, SVR, DT, KN, B] # model_lgb,\nmodel_names = ['lasso', 'lasso_new', 'ENet', 'KRR', 'GBoost_orig', 'GBoost', 'model_xgb', 'model_xgb_new', 'BR', 'ET', 'ET_new', 'RF', 'RF_new', 'AB', 'SVR', 'DT', 'KN', 'B']\n\nwith MyTimer():\n    for i in range(0,len(models)):\n        mn = model_names[i]+\"_pred\"\n        train_pred, test_pred = fit_pred(train, y_train, test, models[i])\n        print(mn, test_pred)    \n","79e98912":"# values are not normalized\ntrain[train.columns].mean().head()","f5e644e5":"# are train and test normalized? between -1 and 1\nreplace_xgb = 0 # new optimized model is worse, was overfit\nif (replace_xgb == 1):\n    model_xgb_new.fit(train, y_train)\n    xgb_train_pred = model_xgb_new.predict(train)\n    xgb_pred = inv_boxcox1p(model_xgb_new.predict(test), lam_l)\nelse:\n    model_xgb.fit(train, y_train)\n    xgb_train_pred = model_xgb.predict(train)\n    xgb_pred = inv_boxcox1p(model_xgb.predict(test), lam_l)\n\nif (competition == 'SR'):\n    print(mae(y_train, xgb_train_pred))\nelse:\n    print(rmsle(y_train, xgb_train_pred))","a10b2d65":"model_lgb.fit(train, y_train)\nlgb_train_pred = model_lgb.predict(train)\nlgb_pred = inv_boxcox1p(model_lgb.predict(test.values), lam_l)\n\nif (competition == 'SR'):\n    print(mae(y_train, lgb_train_pred))\nelse:\n    print(rmsle(y_train, lgb_train_pred))","cfd49d98":"if (tuning_lgb == 1):\n    model_lgb_op.fit(train, y_train)\n    lgb_train_pred_op = model_lgb_op.predict(train)\n    lgb_pred_op = inv_boxcox1p(model_lgb_op.predict(test.values), lam_l)\n\n    if (competition == 'SR'):\n        print(mae(y_train, lgb_train_pred_op))\n    else:\n        print(rmsle(y_train, lgb_train_pred_op))","6e690e24":"# compare values with optimization\nprint(lgb_train_pred)\nif (tuning_lgb == 1):\n    print(lgb_train_pred_op)\nprint(lgb_pred)\nif (tuning_lgb == 1):\n    print(lgb_pred_op)","f1aade46":"'''RMSE on the entire Train data when averaging'''\n\nprint('RMSLE score on train data:')\n\n\nstk = 0.70\nxgb = 0.15\nlgb = 0.15\n\n\nif (competition == 'SR'):\n    print(mae(y_train,stacked_train_pred*stk + xgb_train_pred*xgb + lgb_train_pred*lgb ))\nelse:\n    print(rmsle(y_train,stacked_train_pred*stk + xgb_train_pred*xgb + lgb_train_pred*lgb ))","30bf5acb":"#method = 'ensemble'\nmethod = 'stacked'\nif (method == 'stacked'):\n    #stacked_pred => ENet, GBoost_orig, KRR\n    ensemble = stacked_pred*stk + xgb_pred*xgb + lgb_pred*lgb  # if using averaged_pred, need to add averaged_pred here\nelse:\n    ensemble = averaged_pred\n","7c6d04b2":"print(lgb_pred)\nprint(xgb_pred)\nprint(stacked_pred)\nprint(averaged_pred)\nprint(ensemble)","dc6199a5":"print(y_train,stacked_train_pred * stk + xgb_train_pred * xgb + lgb_train_pred * lgb) # if using averaged_pred, need to add averaged_pred here\nprint(y_train,stacked_train_pred)","ed79fed0":"sub_train = pd.DataFrame()\nsub_train['Id'] = train_ID\nsub_train['SalePrice'] = inv_boxcox1p(stacked_train_pred, lam_l)","b919a6e5":"Predicted = sub_train['SalePrice']\nActual = inv_boxcox1p(y_train, lam_l)\nplt.scatter(sub_train['SalePrice'], Actual, alpha=.7,\n            color='b') #alpha helps to show overlapping data\nplt.xlabel('Predicted Price')\nplt.ylabel('Actual Price')\nplt.title('Averaged Model')\n\nm, b = np.polyfit(Predicted, Actual, 1)\n#m = slope, b=intercept\nplt.plot(Predicted, m*Predicted+b,c='red')\nplt.show()","de7dd135":"# Pre-adjustment score\nprint(\"mae for boxcox(SalePrice)\",mae(y_train, sub_train['SalePrice']))\nprint(\"mse for boxcox(SalePrice)\",rmsle(y_train, sub_train['SalePrice']))\nprint(\"mae for SalePrice\",mae(Actual, Predicted))\nprint(\"mse for SalePrice\",rmsle(Actual, Predicted))","ee875430":"AdjustedScores = []\n\nfor i in np.arange(.994, .996, 0.001):\n    for j in np.arange(1.06, 1.08, .01):\n\n        q1 = sub_train['SalePrice'].quantile(0.0025)\n        q2 = sub_train['SalePrice'].quantile(0.0045)\n        q3 = sub_train['SalePrice'].quantile(i)\n\n        #Verify the cutoffs for the adjustment\n        print(q1,q2,q3)\n        # adjust at low end\n        #sub_train['SalePrice2'] = sub_train['SalePrice'].apply(lambda x: x if x > q1 else x*0.79)\n        #sub_train['SalePrice2'] = sub_train['SalePrice'].apply(lambda x: x if x > q2 else x*0.89)\n\n        # adjust at high end\n        sub_train['SalePrice2'] = sub_train['SalePrice'].apply(lambda x: x if x < q3 else x*j)\n\n        Predicted = sub_train['SalePrice2']\n        Actual = inv_boxcox1p(y_train, lam_l)\n\n        # Pre-adjustment score\n        print(\"mae for boxcox(SalePrice)\",mae(y_train, sub_train['SalePrice2']))\n        print(\"mse for boxcox(SalePrice)\",rmsle(y_train, sub_train['SalePrice2']))\n        print(\"mae for SalePrice\",mae(Actual, Predicted))\n        print(\"mse for SalePrice\",rmsle(Actual, Predicted))\n\n        AdjustedScores.append([i, j, mae(y_train, boxcox1p(sub_train['SalePrice2'], lam_l)), rmsle(y_train, boxcox1p(sub_train['SalePrice2'], lam_l)), mae(Actual, Predicted), rmsle(Actual, Predicted)])\n\ndf_adj = pd.DataFrame(AdjustedScores, columns=[\"QUANT\",\"COEF\",\"MAE_BC\",\"RMSE_BC\",\"MAE_SP\",\"RMSE_SP\"])","101124dc":"q1 = sub_train['SalePrice'].quantile(0.0015)\nq2 = sub_train['SalePrice'].quantile(0.01)\nq3 = sub_train['SalePrice'].quantile(0.995)\n\n#Verify the cutoffs for the adjustment\nprint(q1,q2,q3)\n# adjust at low end\n#sub_train['SalePrice2'] = sub_train['SalePrice'].apply(lambda x: x if x > q1 else x*0.79)\n#sub_train['SalePrice2'] = sub_train['SalePrice'].apply(lambda x: x if x > q2 else x*0.89)\n\n# adjust at high end\nsub_train['SalePrice2'] = sub_train['SalePrice'].apply(lambda x: x if x < q3 else x*1.07)","07639c42":"plt.scatter(sub_train['SalePrice'], sub_train['SalePrice2'], alpha=.7,\n            color='b') #alpha helps to show overlapping data\nplt.xlabel('Predicted Price')\nplt.ylabel('Adjusted Predicted Price')\nplt.title('Averaged Model')\nplt.show()","4e08a704":"sub_train.query(\"SalePrice != SalePrice2\")","d8ea8c54":"Predicted = sub_train['SalePrice2']\nActual = inv_boxcox1p(y_train, lam_l)\nplt.figure(figsize=(15,15))\nplt.scatter(sub_train['SalePrice2'], Actual, alpha=.7,\n            color='b') #alpha helps to show overlapping data\nplt.xlabel('Adj Predicted Price')\nplt.ylabel('Actual Price')\nplt.title('Averaged Model')\n\nm, b = np.polyfit(Predicted, Actual, 1)\n#m = slope, b=intercept\nplt.plot(Predicted, m*Predicted+b,c='red')\nplt.show()","c3c0b024":"# Post adjustment for high score\nprint(\"mae for boxcox(SalePrice2)\",mae(y_train, sub_train['SalePrice2']))\nprint(\"mse for boxcox(SalePrice2)\",rmsle(y_train, sub_train['SalePrice2']))\nprint(\"mae for SalePrice2\",mae(Actual, Predicted))\nprint(\"mse for SalePrice2\",rmsle(Actual, Predicted))","26ca0293":"#sub_train = pd.DataFrame()\n#sub_train['Id'] = train_ID\n#sub_train['SalePrice'] = inv_boxcox1p(stacked_train_pred, lam_l)\nq1 = sub_train['SalePrice'].quantile(0.013)\nq2 = sub_train['SalePrice'].quantile(0.10)\nq3 = sub_train['SalePrice'].quantile(0.995)\nprint(q1,q2,q3)","a90ff6b8":"sub_train.min()","9463799e":"AdjustedScores = []\n\nfor i in np.arange(.005, .015, 0.001):\n    for j in np.arange(.90, 1.00, 0.01):\n\n        q1 = sub_train['SalePrice'].quantile(i)\n        q2 = sub_train['SalePrice'].quantile(0.1)\n        q3 = sub_train['SalePrice'].quantile(.995)\n\n        #Verify the cutoffs for the adjustment\n        print(q1,q2,q3)\n        # adjust at low end\n        sub_train['SalePrice2'] = sub_train['SalePrice'].apply(lambda x: x if x > q1 else x*j)\n        #sub_train['SalePrice2'] = sub_train['SalePrice'].apply(lambda x: x if x > q2 else x*0.89)\n\n        # adjust at high end\n        #sub_train['SalePrice2'] = sub_train['SalePrice'].apply(lambda x: x if x < q3 else x*j)\n\n        Predicted = sub_train['SalePrice2']\n        Actual = inv_boxcox1p(y_train, lam_l)\n\n        # Pre-adjustment score\n        print(\"mae for boxcox(SalePrice)\",mae(y_train, sub_train['SalePrice2']))\n        print(\"mse for boxcox(SalePrice)\",rmsle(y_train, sub_train['SalePrice2']))\n        print(\"mae for SalePrice\",mae(Actual, Predicted))\n        print(\"mse for SalePrice\",rmsle(Actual, Predicted))\n\n        AdjustedScores.append([i, j, mae(y_train, boxcox1p(sub_train['SalePrice2'], lam_l)), rmsle(y_train, boxcox1p(sub_train['SalePrice2'], lam_l)), mae(Actual, Predicted), rmsle(Actual, Predicted)])\n\ndf_adj = pd.DataFrame(AdjustedScores, columns=[\"QUANT\",\"COEF\",\"MAE_BC\",\"RMSE_BC\",\"MAE_SP\",\"RMSE_SP\"])","137ccfa1":"df_adj.sort_values(by=['RMSE_BC'])","5c400ae2":"q1 = sub_train['SalePrice'].quantile(0.01)\nq2 = sub_train['SalePrice'].quantile(0.1)\nq3 = sub_train['SalePrice'].quantile(0.995)\n\n#Verify the cutoffs for the adjustment\nprint(q1,q2,q3)\n# adjust at low end\nsub_train['SalePrice2'] = sub_train['SalePrice'].apply(lambda x: x if x > q1 else x*0.91) # also try .94\n#sub_train['SalePrice2'] = sub_train['SalePrice'].apply(lambda x: x if x > q2 else x*0.89)\n\n# adjust at high end\nsub_train['SalePrice2'] = sub_train['SalePrice'].apply(lambda x: x if x < q3 else x*1.07)","2abf97bb":"plt.figure(figsize=(15,15))\nplt.scatter(sub_train['SalePrice'], sub_train['SalePrice2'], alpha=.7,\n            color='b') #alpha helps to show overlapping data\nplt.xlabel('Predicted Price')\nplt.ylabel('Adjusted Predicted Price')\nplt.title('Averaged Model')\nplt.show()","a599efcd":"Predicted = sub_train['SalePrice2']\nActual = inv_boxcox1p(y_train, lam_l)\nplt.figure(figsize=(15,15))\nplt.scatter(sub_train['SalePrice2'], Actual, alpha=.7,\n            color='b') #alpha helps to show overlapping data\nplt.xlabel('Adj Predicted Price')\nplt.ylabel('Actual Price')\nplt.title('Averaged Model')\n\nm, b = np.polyfit(Predicted, Actual, 1)\n#m = slope, b=intercept\nplt.plot(Predicted, m*Predicted+b,c='red')\n\nplt.show()","e1b33149":"sub_train.query(\"SalePrice != SalePrice2\")","ad397c71":"# Post adjustment for low and high score\nprint(\"mae for boxcox(SalePrice2)\",mae(y_train, sub_train['SalePrice2']))\nprint(\"mse for boxcox(SalePrice2)\",rmsle(y_train, sub_train['SalePrice2']))\nprint(\"mae for SalePrice2\",mae(Actual, Predicted))\nprint(\"mse for SalePrice2\",rmsle(Actual, Predicted))","edd821c9":"sub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = ensemble\n#sub.to_csv('submission.csv',index=False)","118d7f3c":"q1 = sub['SalePrice'].quantile(0.01)\nq2 = sub['SalePrice'].quantile(0.1)\nq3 = sub['SalePrice'].quantile(0.995)\n\n# adjust at low end\n#sub['SalePrice'] = sub['SalePrice'].apply(lambda x: x if x > q1 else x*0.99) # didn't help, with several values\n#sub['SalePrice2'] = sub['SalePrice'].apply(lambda x: x if x > q2 else x*0.89)\n\n# adjust at high end\nsub['SalePrice'] = sub['SalePrice'].apply(lambda x: x if x < q3 else x*1.07)","304ead25":"sub.to_csv('submission.csv',index=False)","90aa08ff":"sub.head()","d59f8d15":"!pwd","d9904cd0":"ls -arlt","3d58e5e2":"!head -10 submission.csv","eb6a9cc9":"print(\"Start: \", StartTime)\nprint(\"End: \", datetime.datetime.now())","ba2524e4":"from mlens.ensemble import SuperLearner\nensemble = SuperLearner(random_state=1, verbose=2)","19f3cdf2":"-  **LASSO  Regression**  : \n\nThis model may be very sensitive to outliers. So we need to made it more robust on them. For that we use the sklearn's  **Robustscaler()**  method on pipeline, also want to compare to StandardScaler() => RobustScaler() is slightly better","a14bd92d":"<pre>\nTypical flow of model building: Use GridSearch for determining Best parameters => **best_estimator_ \n\ngood basic resource for models: \nhttps:\/\/scikit-learn.org\/stable\/modules\/cross_validation.html\nhttps:\/\/scikit-learn.org\/stable\/modules\/grid_search.html\n\nA search consists of:\nan estimator (regressor or classifier such as sklearn.svm.SVC());  \na parameter space;  \na method for searching or sampling candidates;  \na cross-validation scheme; and  \na score function.  ****\n","206eda61":"- **Elastic Net Regression** :\n\nagain made robust to outliers\n\ncombines Lasso L1 Linear regularization and Ridge L2 Quadratic\/Squared regularization penalties together into one algorithm","0c5b7ab9":"![image.png](attachment:image.png)","5118f096":"As you can see, the PCA analysis did its job, the features show little correlation now","a3926447":"Another look at the feature to output correlations","d457e623":"#Modelling","54faf13c":"maybe do a groupby to make this table more manageable and easier to read","ead1fd4a":"- **GarageYrBlt, GarageArea and GarageCars** : Replacing missing data with 0 (Since No garage = no cars in such garage.)\n- **BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath** : missing values are likely zero for having no basement\n","335293ec":"- **LightGBM** :","a9589bb9":"We get again a better score by adding a meta learner","8768becd":"- **MSZoning (The general zoning classification)** :  'RL' is by far  the most common value.  So we can fill in missing values with 'RL'\n","fd3260a3":"Seems that RMSLE does not correlate to a good score\n\n![image.png](attachment:image.png)","e50a5504":"Edit: Look for fliers in other columns","17126d0e":"My equation has a worse correlation... will need to investigate this later. For now, i will simplify an just use a direct function...","fd2b31bd":"use the 0.3 value as the improvement below this value is minimal","ffbe60cc":"Do some PCA for the dataset, to remove some of the collinearity. Not sure if this will have any effect as collinearity is usually not detrimental to many or most algorithms","be163a55":"Objective: Predict prices for test (test.csv) dataset based on model build from train (train.csv) dataset  \n\nEvaluation metric: \"The RMSE between log of Saleprice and log of prediction\". Need to convert salesprice to log value first. However seems that BoxCox does a better job here. For my testing E will remove BoxCox, but may want to put it back for the final submissions. Maybe one with boxcox1p() and one with log().\n\nOriginal competition (explains the evaluation metric): https:\/\/www.kaggle.com\/c\/home-data-for-ml-course\/overview\/evaluation. My work is paying off, my submission on that site is #3 out of 38000, top .0001%\n\nOriginal notebook source: https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard  \n\nReference for good ideas: https:\/\/towardsdatascience.com\/tricks-i-used-to-succeed-on-a-famous-kaggle-competition-adaa16c09f22\n\nOriginal default score was .11543, new best score is 0.11353\n\nStacked and Ensembled Regressions to predict House Prices  \nHow to Kaggle: https:\/\/www.youtube.com\/watch?v=GJBOMWpLpTQ\n\nReferences for stacking and ensembling:\nhttps:\/\/www.kaggle.com\/getting-started\/18153\nhttps:\/\/developer.ibm.com\/technologies\/artificial-intelligence\/articles\/stack-machine-learning-models-get-better-results\n\nDonald S  \nJuly 2020  \n\nNeed to submit every 2 months as the leaderboard will rollover after this 2 month period","2896b87b":"**Import libraries**","66b24fb1":"**Adding one more important feature**","56bf7f6a":"I am getting conflicting results for  best params, sometimes huber or lad and sometimes friedman_mse or mae, so will look at more detailed output. This style output is much more useful for deciding between parameter values, adding the different random states shows the consistency, or lack of, for each setting","2f724af6":"Making the same adjustment on the test data for our submission","f8e77fd6":"Then, add this column from previous investigation to the dataset y = curve_fit_gla[2] + curve_fit_gla[1]x_data + curve_fit_gla[0]x_data**2","06dbdf28":"our column count went from 216 to n_component value","81fe0188":"I recommend plotting the cumulative sum of eigenvalues (assuming they are in descending order). If you divide each value by the total sum of eigenvalues prior to plotting, then your plot will show the fraction of total variance retained vs. number of eigenvalues. The plot will then provide a good indication of when you hit the point of diminishing returns (i.e., little variance is gained by retaining additional eigenvalues).","86c51494":"Safe to drop the Id column now as we are finished deleting points (otherwise our Train_ID list will be smaller than y_train","97529d71":"![Faron](http:\/\/i.imgur.com\/QBuDOjs.jpg)\n\n(Image source [Faron](https:\/\/www.kaggle.com\/getting-started\/18153#post103381))\n\nThe only difference between M6 and M1-M5 is, that M6 is trained on the entire original training data, whereas M1-M5 are trained only on 4 out of 5 folds.\n\nWith M1-M5 you can build valid out-of-fold predictions for the training set (the orange ones) to form a \"new feature\" for the 2nd layer (not possible with M6). You can also predict the test set with M1-M5 to get 5 sets of test set predictions .. but you only need one set of test set predictions for the corresponding feature to the orange out-of-fold train set predictions.\n\nHence, you reduce those 5 sets to 1 by averaging. That's the first variant. Alternatively, you train M6 and use its test set predictions as feature for the 2nd layer (instead of the average of the test set predictions from M1-M5).","675fd045":"- **Additional testing**\n\nCompare any random state","c7b0c7b7":"On this gif, the base models are algorithms 0, 1, 2 and the meta-model is algorithm 3. The entire training dataset is \nA+B (target variable y known) that we can split into train part (A) and holdout part (B). And the test dataset is C. \n\nB1 (which is the prediction from the holdout part)  is the new feature used to train the meta-model 3 and C1 (which\nis the prediction  from the test dataset) is the meta-feature on which the final prediction is done. ","2ec81ff3":"correlation looks much better at the high end","ec178199":"Kfold is useful for thorough testing of a model, will give a more accurate score based on remove some data test on the remaining and change the data removed each time. See image below for details:","870d427b":"plot the poly fit data","e1626895":"This fit looks like it may be better, will add a column to the dataset using this equation after it is merged (all_data)\n\ny = curve_fit_gla[2] + curve_fit_gla[1]*x_data + curve_fit_gla[0]*x_data**2\n\nx_data = train['GrLivArea']\n","336eaedf":"- **SaleType** : Fill in again with most frequent which is \"WD\"","8d74854d":"Alternate Method to calculate missing Zoning values, neighborhood should be zoned the same most of the time","32d1bfd0":"**Getting dummy categorical features**","22755015":"replace cross_val_score() with cross_validate()\n# reference: https:\/\/scikit-learn.org\/stable\/modules\/cross_validation.html\n\n    from sklearn.metrics import make_scorer\n    scoring = {'prec_macro': 'precision_macro',\n               'rec_macro': make_scorer(recall_score, average='macro')}\n    scores = cross_validate(clf, X, y, scoring=scoring,\n                            cv=5, return_train_score=True)\n    sorted(scores.keys())\n    ['fit_time', 'score_time', 'test_prec_macro', 'test_rec_macro',\n     'train_prec_macro', 'train_rec_macro']\n    scores['train_rec_macro']\n    array([0.97..., 0.97..., 0.99..., 0.98..., 0.98...])","a7af2055":"Look for any good correlations to use for imputation","2af17ede":"**Stacking averaged Models Class**","cf395a0b":"###Final Training and Prediction","f7054387":"Diagram of out of fold cross validated stacked models\n\n\n![image.png](attachment:image.png)\n\nReference: http:\/\/rasbt.github.io\/mlxtend\/user_guide\/regressor\/StackingCVRegressor\/","6cb622dc":"Here is the difference between BoxCox and Log values, the difference is substantial at the value of a house","6fb6fb72":"**Averaged base models score**","4b99ee14":"multilayer stacking","5de75278":"This chart does not look linear, or at least the line is not matching the data across the entire x axis. Looks like a drop off for High GrLivArea, seems home buyers are not willing to pay a corresponding amount extra for the large living area, looking for a \"volume discount\" maybe...\nLets look at this closer, first fit a line, next try a polynomial fit to compare","40ad49fa":"Now we look at a polynomial fit","8ddc43d9":"The skew seems now corrected and the data appears more normally distributed. \n\nEdit: Both distributions have a positive kurtosis, which means there is a steep dropoff in the curve from the center, or the tails have few points. Skewness is close to 0 now, so that metric is closer to a normal distribution. BoxCox1p() has worse numbers than log() but still performs better in the predictions, will keep BoxCox1p","5c457b1b":"* check and compare the new columns","6d321aec":"##Features engineering","73c9072e":"#Data Processing","c386ecf8":"###Base models scores","adba1977":"**Stacking Averaged models Score**","5a8d9c7c":"let's first  concatenate the train and test data in the same dataframe","21a30a95":"We add **XGBoost and LightGBM** to the **StackedRegressor** defined previously. ","83388e73":"try votingRegressor, it allows different weighting for each model","574515db":"No missing values\n","bb5d5d0a":"More filtering of data to try","fc8e32ec":"We just average four models here **ENet, GBoost,  KRR and lasso**.  Of course we could easily add more models in the mix. ","dc03f45f":"Now try to improve upon this","86df4880":"We impute them  by proceeding sequentially  through features with missing values \n\n- **PoolQC** : data description says NA means \"No  Pool\". That make sense, given the huge ratio of missing value (+99%) and majority of houses have no Pool at all in general. \n- **MiscFeature** : data description says NA means \"no misc feature\"\n- **Alley** : data description says NA means \"no alley access\"\n- **Fence** : data description says NA means \"no fence\"\n- **FireplaceQu** : data description says NA means \"no fireplace\"\n- **GarageType, GarageFinish, GarageQual and GarageCond** : Replacing missing data with None\n- **BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2** : For all these categorical basement-related features, NaN means that there is no  basement.","785dc224":"Want to add ordinal or int column for Year and Month, this is the function to perform that task","225066de":"add the previous averaged models here","698ad49a":"Show new predictions","93b7c834":"separate into lists for each data type","ae826796":"Show adjustments","53998c57":"- **Electrical** : It has one NA value. Since this feature has mostly 'SBrkr', we can set that for the missing value.\n","329f69b8":"In this approach, we add a meta-model on averaged base models and use the out-of-folds predictions of these base models to train our meta-model. \n\nThe procedure, for the training part, may be described as follows:\n\n\n1. Split the total training set into two disjoint sets (here **train** and .**holdout** )\n\n2. Train several base models on the first part (**train**)\n\n3. Test these base models on the second part (**holdout**)\n\n4. Use the predictions from 3)  (called  out-of-folds predictions) as the inputs, and the correct responses (target variable) as the outputs  to train a higher level learner called **meta-model**.\n\nThe first three steps are done iteratively . If we take for example a 5-fold stacking , we first split the training data into 5 folds. Then we will do 5 iterations. In each iteration,  we train every base model on 4 folds and predict on the remaining fold (holdout fold). \n\nSo, we will be sure, after 5 iterations , that the entire data is used to get out-of-folds predictions that we will then use as \nnew feature to train our meta-model in the step 4.\n\nFor the prediction part , We average the predictions of  all base models on the test data  and used them as **meta-features**  on which, the final prediction is done with the meta-model.\n","4f7eb897":"Use another Stacking function, from mlxtend","29b9ac4f":"try different pipelines, scaler, etc\nreference: https:\/\/scikit-learn.org\/stable\/auto_examples\/inspection\/plot_permutation_importance.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-py\n\n![image.png](attachment:image.png)\n\nTry polynomial features:\nreference: https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_underfitting_overfitting.html#sphx-glr-auto-examples-model-selection-plot-underfitting-overfitting-py\nfrom sklearn.preprocessing import PolynomialFeatures\n\npolynomial_features = PolynomialFeatures(degree=degrees[i],\n                                         include_bias=False)\nlinear_regression = LinearRegression()\npipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n                     (\"linear_regression\", linear_regression)])\n\nscores = cross_val_score(pipeline, X[:, np.newaxis], y,\n                         scoring=\"neg_mean_squared_error\", cv=10)       \n                             \n                             \nestimators = []\nestimators.append(('standardize', StandardScaler()))\nestimators.append(('lda', LinearDiscriminantAnalysis()))\nmodel = Pipeline(estimators)    \n\n\nfeatures = []\nfeatures.append(('pca', PCA(n_components=3)))\nfeatures.append(('select_best', SelectKBest(k=6)))\nfeature_union = FeatureUnion(features)\nestimators = []\nestimators.append(('feature_union', feature_union))\nestimators.append(('logistic', LogisticRegression()))\nmodel = Pipeline(estimators)","55a0cde9":"Optimize","e601867e":"**Averaged base models class**\n\nIn averaging methods, the driving principle is to build several estimators independently and then to average their predictions. On average, the combined estimator is usually better than any of the single base estimator because its variance is reduced.","2d19dbd4":"![image.png](attachment:image.png)","f04a63bf":"**Box Cox Transformation of (highly) skewed features**","6f52d241":"We begin with this simple approach of averaging base models.  We build a new **class**  to extend scikit-learn BaseEstimator, RegressorMixin, TransformerMixin classes with our model and also to leverage encapsulation and code reuse ([inheritance][1]) \n\n\n  [1]: https:\/\/en.wikipedia.org\/wiki\/Inheritance_(object-oriented_programming)","82fdaf4c":"\nList of possible scoring values:  \nRegression  \n\n\u2018explained_variance\u2019 metrics.explained_variance_score  \n\u2018max_error\u2019 metrics.max_error  \n\u2018neg_mean_absolute_error\u2019 metrics.mean_absolute_error  \n\u2018neg_mean_squared_error\u2019 metrics.mean_squared_error  \n\u2018neg_root_mean_squared_error\u2019 metrics.mean_squared_error  \n\u2018neg_mean_squared_log_error\u2019 metrics.mean_squared_log_error  \n\u2018neg_median_absolute_error\u2019 metrics.median_absolute_error  \n\u2018r2\u2019 metrics.r2_score  \n\u2018neg_mean_poisson_deviance\u2019 metrics.mean_poisson_deviance  \n\u2018neg_mean_gamma_deviance\u2019 metrics.mean_gamma_deviance  ","72e4b8f1":"The target variable is right skewed.  As (linear) models love normally distributed data , we need to transform this variable and make it more normally distributed.\n","8e3da22b":"**Data Correlation**\n","c5e2e6ab":"Testing the new PCA dataset for analysis - RMSE looks works after PCA is applied, need to look at the Kaggle score later and see if it correlates, could be a mistake to use PCA on categorical dummy data. However, XGB is better with PCA n=50 option. Maybe use a heavier weight for that portion, or use all_data_pca only on that model...","321dcced":"###Imputing missing values ","bcb52e4b":"- **Kernel Ridge Regression** :","54f92ea8":"**Skewed features**","bc6b8b5d":"###Missing Data","fadd8fa4":"Another way to combine multiple models:\n\nThe function cross_val_predict is appropriate for:\nVisualization of predictions obtained from different models.\n\nModel blending: When predictions of one supervised estimator are used to train another estimator in ensemble methods.","0ae472dc":"Get Pre-adjustment score for comparison","f68c7a62":"This model needs improvement, will run cross validate on it","fa408936":"**SalePrice** is the variable we need to predict. So let's do some analysis on this variable first.","385027eb":"Cross Correlation chart to view collinearity within the features. Kendall's seems appropriate as the Output Variable is numerical and much of the input is categorical. Here is a chart to guide you to which method to use based on your dataset.\n\n![image.png](attachment:image.png)","7502da6b":"###Note : \n Outliers removal is note always safe.  We decided to delete these two as they are very huge and  really  bad ( extremely large areas for very low  prices). \n\nThere are probably others outliers in the training data.   However, removing all them  may affect badly our models if ever there were also  outliers  in the test data. That's why , instead of removing them all, we will just manage to make some of our  models robust on them. You can refer to  the modelling part of this notebook for that. \n\nEdit: todo - look for other outliers in all columns. Maybe better to use StandardScaler() and np.clip","ce5ee10f":"We use the scipy  function boxcox1p which computes the Box-Cox transformation of **\\\\(1 + x\\\\)**. \n\nNote that setting \\\\( \\lambda = 0 \\\\) is equivalent to log1p used above for the target variable.  \n\nSee [this page][1] for more details on Box Cox Transformation as well as [the scipy function's page][2]\n[1]: http:\/\/onlinestatbook.com\/2\/transformations\/box-cox.html\n[2]: https:\/\/docs.scipy.org\/doc\/scipy-0.19.0\/reference\/generated\/scipy.special.boxcox1p.html","6e677c78":"Find best cutoff and adjustment at low end","b06b40be":"The shape values are the number of columns in the PCA x the number of original columns","89ac8c9d":"Calculate Metrics for model list","aac451ae":"Some imputation methods result in biased parameter estimates, such as means, correlations, and regression coefficients, unless the data are Missing Completely at Random (MCAR). The bias is often worse than with listwise deletion, the default in most software.  \n\nThe extent of the bias depends on many factors, including the imputation method, the missing data mechanism, the proportion of the data that is missing, and the information available in the data set.  \n\nMoreover, all single imputation methods underestimate standard errors.\n\nReference: https:\/\/www.theanalysisfactor.com\/mar-and-mcar-missing-data\/\n\nMultiple imputation can overcome most of these shortcomings, but at the expense of time. They take more time to implement and run\n\nReference: https:\/\/www.theanalysisfactor.com\/missing-data-two-recommended-solutions\/\n\nMethods: (can use in pipeline - estimator = make_pipeline(imputer, regressor) ) reference: https:\/\/scikit-learn.org\/stable\/auto_examples\/impute\/plot_missing_values.html#sphx-glr-auto-examples-impute-plot-missing-values-py  \n\nSimple Regression:\n\nfrom sklearn.impute import SimpleImputer\nimp = SimpleImputer(missing_values=np.nan, strategy='mean') for numerical\nimp = SimpleImputer(strategy=\"most_frequent\") for categorical\n\nMultivariate Regression:\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nimp = IterativeImputer(max_iter=10, random_state=0)\n\nkNN:\nfrom sklearn.impute import KNNImputer\nimputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n\nOther:\nStochastic regression, Bayesian Linear Regression, Bayesian Binary Logistic Regression\nhttps:\/\/pypi.org\/project\/autoimpute\/","026b5a6d":"- **Gradient Boosting Regression** :\n\nWith **huber**  loss that makes it robust to outliers\n    ","c20f41dd":"**Label Encoding some categorical variables that may contain information in their ordering set** ","2982e77c":"Is there any remaining missing value ? ","95b93d2d":"At the high end, SP > 520000, the model predicts too low for 7\/8 points","62d4505e":"- **Random Forest Regressor** :","97c11191":"Edit: pca.components_ is the matrix you can use to calculate the inverse of the PCA analysis, i.e. go back to the original dataset\nreference: https:\/\/stats.stackexchange.com\/a\/143949","57c3e711":"Try dropping the lowest correlating columns","13854db9":"Visualize the results","7bde76e2":"**Create File for Submission to Kaggle**","bffac8b5":"![image.png](attachment:image.png)","5fe8d041":"**StackedRegressor:**","18aee1e6":"Plot prediction vs actual for train for one last verification of the model","704b4487":"Compare the r-squared values","8f2a0110":"- **Utilities** : For this categorical feature all records are \"AllPub\", except for one \"NoSeWa\"  and 2 NA . Since the house with 'NoSewa' is in the training set, **this feature won't help in predictive modelling**. We can then safely  remove it.\n","b0c4fa4a":"###Simplest Stacking approach : Averaging base models","a39ea95c":"- **BayesianRidge** :","d1c1755c":"Let's see how these base models perform on the data by evaluating the  cross-validation rmsle error","e5de0d73":"We can see at the bottom right two with extremely large GrLivArea that are of a low price. These values are huge oultliers.\nTherefore, we can safely delete them.\n\nEdit: Think this deletion is safe to do as there are only 2 points and they seem to be abnormal, possibly data error","a14a8542":"Find best cutoff and adjustment at high end","0f8be93a":"Show both adjustments","06a18ef9":"**XGBoost:**","397a13c0":"Check predictions, are they one same scale as SalePrice in Train dataset?","59e2b4c1":"##Stacking  models","699d13d9":"##Base models","6bfe1758":"Dropping even a single column gives a worse score\nConclusion: there is information in every column and the models are able to extract that information effectively!","32509102":"- **XGBoost** :","dbb83f3c":"Compare the r-squared values for different functions","9c065c4f":"- **Exterior1st and Exterior2nd** : Again Both Exterior 1 & 2 have only one missing value. We will just substitute in the most common string\n","fe76f794":"**Ensemble prediction:**\n\nwhen deciding which models to include in an ensemble:  \n    fewer are better  \n    more diverse are better  ","f848fa76":" We choose number of eigenvalues to calculate based on previous chart, 20 looks like a good number, the chart starts to roll off around 15 and almost hits max a 20. We can also try 30,40 and 50 to squeeze a little more variance out...","41cb4930":"Save Cleansed Data to disk","b7c04d64":"**Define a cross validation strategy**","7bcc297e":"**Tansformation of the target variable**\n \nDefault is to use log1p as this is included in the evaluation metric, rmsle \nEdit: also trying box-cox transform.\nThe Box-Cox transform is given by:\n<pre>\ny = (x**lmbda - 1) \/ lmbda,  for lmbda > 0  \n    log(x),                  for lmbda = 0\n    \ny = ((1+x)**lmbda - 1) \/ lmbda  if lmbda != 0\n    log(1+x)                    if lmbda == 0","a396211c":"xgb reference: https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html","9e696ea6":"Since area related features are very important to determine house prices, we add one more feature which is the total area of basement, first and second floor areas of each house","55a1ed6e":"- **MSSubClass** : Na most likely means No building class. We can replace missing values with None\n","22693225":"**Transforming some numerical variables that are really categorical**\n\nEdit: I will add a new column year and date as ordinal, since there is some order to the values it may be useful","3198af73":"Now for r squared calculations we need to limit the comparison to only the train data since the test data does not have a Sales Price to compare to","7d33067d":"##Outliers","be8fb642":"- **KitchenQual**: Only one NA value, and same as Electrical, we set 'TA' (which is the most frequent)  for the missing value in KitchenQual.\n","bb5209ed":"**LightGBM:**","08d7d6b6":"## Ensembling StackedRegressor, XGBoost and LightGBM","ec4617bc":"Look for other correlations, maybe all the basement olumns will correlate like BsmtFullBath and BsmtFinSF1 and Fin vs Unf have negative correlation. Both make sense...","767f0243":"Getting the new train and test sets. ","cf64144c":"Show new predictions","75abc4d4":"- **More Models** :","e429a72c":"<pre>\n1)   12038.23038     Original\n2)   12093.91656     Rerun\n3)   12053.81581     Rerun\n4)   12082.28964     put random_state=5 back for consistency in scoring\n5)   12081.92530     Rerun\n6)   12082.04429     Rerun\n7)   12115.07300     change gla2 from 0.3 to 0.5, add new code back but only up to #Modelling for now => worse\n8)   12116.07891     change gla2 back to 0.3 => worse\n9)   12081.92530     remove GrLivAreaRoot, => back to good * score\n10)  11986.26534     add more of the changes back, including the model optimizations, up to AveragingModels => score is better, at #23 out of 38000 (13 are false scores, finding the answer online and posting)\n11)  11986.26534     add remaining changes back => everything works, can proceed with new ideas now\n13)  11986.38476     fixing the inv_boxcox to inv_boxcox1p as it should be => score is almost the same since boxcoc1p only adds 1 to the final value (see equation)\n16)  12089.21619     drop columns for low corr (Kendall<0.05): ['MoSold', 'MSSubClass', 'BsmtHalfBath', 'YrSold', 'BsmtFinSF2'] => worse\n17)  12015.48290     drop columns for low corr (Kendall<0.011) ['MSSubClass', 'BsmtHalfBath'] => even dropping 2 columns is worse, every column helps improve the score\n18)  11986.38476     revert to #13\n19)  12206.26936     remove all_data['TotalSF'] => worse, put back as this new column is helpful\n20)  12008.25906     add 10 more features\n21)  11986.38476     revert to #18\n22)  11971.43569     add A- => better, so keep\n23)  11896.96234     add B- => much better, so keep\n24)  12017.87183     add C- => worse, remove\n25)  11950.69668     add D- => worse, remove\n26)  11965.40092     add E- => worse, remove\n27)  11999.80145     add F- => worse, remove\n28)  11958.25419     add G- => worse, remove\n29)  11951.18043     add H- => worse, remove\n30)  11939.66776     add I- => worse, remove\n31)  11896.96234     use simpler logic for original imputing\n37)  11965.58796     replace LotFrontage with KNN Imputed LotFrontageImputed using all numerical columns as input => worse\n39)  11898.33406     revert back to LotFrontage, use KNN imputing on BsmtFinSF1 => slightly worse\n40)  11900.33167     use all numerical columns to impute, not just the ones with missing data => even worse\n42)  11893.52015     set BsmtFinSF1 back to original imputing, retry LotFrontage with all columns => better score\n43)  11894.97258     add neighborhood to MSZoning => worse\n44)  11893.52015     revert MSZoning\n46)  11807.38620     make adjustments to high end, increasing the predicted price as we seem to be lowballing there => much better!! In 4th place now!\n47)  12481.29466     split train data (random_state=201) into 70\/30 train\/validate and use train data only for a few rounds to see if there is enough data to split by and not lose information. This is for comparing my changes each run, now I just have a train dataset to use for optimizations\n48)  12584.92761     random_state=10\n49)  12081.15247     try 80\/20\n50)  12259.77739     80\/20 random_state=201\n51)  11807.38620     revert to #46, using a validate dataset takes too much information away\n52)  11807.38620     simplify code => code is still working correctly\n53)  11866.16188     replace Optimized ENet with Default BayesianRidge in stacked_averaged_models, add 8 new regression models for testing: lr, br, et, ab, svr, dt, kn, b => worse, remove\n54)  11807.18265     put ENet back, replace lasso with BayesianRidge => slightly better\n55)  11880.69886     removing the parameters causing errors from lgbm => score is worse\n56)  11807.18265     add parameters back\n57)  11792.82553     after optimizing for mse, set high end coef to 1.07 => slightly better\n59)  11819.13831     set low end coef to .90 => worse\n60)  11802.44687     coef to .95 => still worse\n63)  11814.58529     adjust arange for high coef, coef to .91\n64)  11796.11511     set to .98\n65)  11794.47032     set to .99 => worse, looks like moving the bottom points over doesn't help\n66)  11792.82553     revert to #57\n67)  11831.78934     use lgb_train_pred_op instead of lgb_train_pred => worse, not optimized correctly\n68)  12042.87197     revert to #66 using lgb_train_pred, optimize GBoost for nmse (took 3 days as this algorithm is slow and has many hyperparameters), replace calc_scores with calc_all_scores\n69)  11792.82553     revert to GBoost_orig\n70)  12042.87197     one more try for new GBoost => still worse, must be overfitting\n71)  11792.82553     revert and optimize BR to BR_new => no difference, seems that changing Bayesian Ridge hyperparameters have no effect on its performance.\n73)  11792.82553     output train and test data to files, fix train_ID and test_ID by using list()\n74)  12235.29723     try the average calculation, averaged_models (ENet, GBoost, KRR, lasso)\n75)  12099.87755     try different averaging - top 4 (model_xgb, GBoost, KRR, model_lgb) => better, but maybe need different ratios\n76)  12387.37189     remove GBoost tuning, optimizations, use top 3 models only (model_xgb, GBoost, model_lgb)\n77)  11895.13427     add KRR back and add BR (model_xgb, GBoost, KRR, model_lgb, BR) => better score\n78)  11982.80779     add ENet back (model_xgb, GBoost, KRR, model_lgb, BR, ENet) turn off lgb tuning => worse\n79)  11931.10693     remove ENet and optimize xgb (model_xgb_new (model_xgb, GBoost, KRR, model_lgb, BR), add Ridge to testing list => worse\n80)  11812.88873**   revert to model_xgb and random_state=7, nfor the _cv functions, for speed\n81)  11783.39376     revert to #73, using stacked model \n82)  11819.03134     try model_xgb_new on stacked model (replace_xgb = 1) => score is worse\n83)  11783.39376     revert to #81\n84)  11817.60539     add lasso to base model in StackingAveragedModels => worse\n85)  11828.73685     try lasso_ns instead => worse\n87)  11823.85406     try lasso_ss and remove BR_new since it has same results as BR => lasso_ss better than lasso_ns, but still not as good as lasso with RobustScaler\n88)  11861.71319     try lasso_new with robustScaler and optimized hyperparameters => worse\n89)  11789.55306     revert to StackingAveragedModels with lasso_new as metamodel = not as good as BR\n90)  11783.39376     revert to BR as metamodel\n91)  11828.13571     replace ENet with ENet_new using RobustScaler in StackingAveragedModels, added RF and ENet with RobustScaler _new, optimize ExtraTrees and Ridge, for future testing\n92)  11783.39376     revert to original ENet, with StandardScaler\n93)  11783.61009     try lasso with RobustScaler as metamodel => slightly worse\n94)  11783.57603     try Ridge as metamodel => seems all ridge or lasso models perform similarly as the metamodel\n95)  11783.57286     use R_new as metamodel => slightly better\n96)  11781.87529*    Add BR to base models => slightly better\n97)  11839.87466     add ET to base models => worse\n98)  11826.88618     use new function StackingRegressor instead of class StackingAveragedModels,  remove ET from base model\n99)  11928.97584     using RidgeCV and ElasticCV with StackingRegressor\n100) 11918.89722     use cv=10 for ElasticNetCV (was 5) => slightly better, but still not as good as not using the ...CV version\n101) 11781.87529*    revert to #96, StackingAveragedModels is better than StackingRegressor as it uses cross validation within the function\/class and out_of_fold methods, StackingRegressor does not\n102) 11813.10758     use StackingCVRegressor => better than StackingRegressor, but worse than StackedAveragedModels class\n104) 12286.31847     try VotingRegressor, the averaged model alternate with all weights set to 1, averaged_pred, use_averaged=1 => worse\n106) 11903.16032     using top 5 models (xgb,lgb,gboost,krr,br) from previous testing, similar to #80\n107) 12052.49018     change weights of 5 models from [1,1,1,1,1] to [4,5,3,2,1] => worse\n108) 11979.29438     try [2,2,2,1,1] => better, but not as good as even weights\n109) 11958.15860     revert to AveragingModels \n110) 11933.93411     try again, should get same score as #80\n111) 11903.16032     replace GBoost with GBoost_orig, random_state=5\n112) 11903.16032     rerun => consistent, but still not the correct score, different score from #80 before but the code is the same\n113) 11781.87529     revert to StackingAveragedModels #101\n\ncompare nmse for train for new vs old models, test is worse for new, is train also worse (underfit), or better (overfit)?\n\n*** best score overall, stacking\n** best score for averaging only\n\n\n\n\nfind and try other ridge regression and lasso regression algorithms and techniques\n\ndone:   \n    try pca on features => no improvement in score. will drop this for now\n    change ratios of models to find which is best predictor (gives best score), then work on improving that model first\n    change scale factor of 3 models => score does not correlate to rmsle, try to find a better metric\n    try StandardScaler and np.clip on features, currently see RobustScaler() being used but i already screened fliers so StandardScaler should be adequate\n    more filtering of fliers\n\nto do:\n    need to investigate why using BoxCox1p is (slightly) better than the evaluation metric further...  \n    use GridsearchCV to find optimal parameters, however, need to find the best way to evaluate the scores. RMSLE doesn't seem to match well...\n    try different pipelines, scaler, etc\n    reference: https:\/\/scikit-learn.org\/stable\/auto_examples\/inspection\/plot_permutation_importance.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-py\n    \n    ideas from other kernels:\n\n    add new features for non-linear features (OverallQual, etc) add a new column with log or square of feature\n       \n    tune min_samples_leaf and min_samples_split for GradientBoostingRegressor!\n    \n    add linear regression from here: https:\/\/github.com\/chouhbik\/Kaggle-House-Prices\/blob\/master\/Kaggle-house-prices-KamalChouhbi.ipynb\n    \nhttps:\/\/machinelearningmastery.com\/stacking-ensemble-machine-learning-with-python\/    \n    \nLook at this later for deep learning stacking: https:\/\/machinelearningmastery.com\/stacking-ensemble-for-deep-learning-neural-networks\/\n","93f6e6fa":"use different function to calculate ensembles","9862e6e5":"Need to look at the y_log relationship since that is what we will be predicting in the model (convert back later)","19effca0":"try another method, imputation","b9d535f2":"- **MasVnrArea and MasVnrType** : NA most likely means no masonry veneer for these houses. We can fill 0 for the area and None for the type. \n","93d14bac":"Look at some correlation values in a list format","49d49b41":"##Target Variable","dd617d84":"- **LotFrontage** : Since the area of each street connected to the house property most likely have a similar area to other houses in its neighborhood , we can **fill in missing values by the median LotFrontage of the neighborhood**.","b965247e":"We will be using Ensembling methods in this notebook. I will only create one dataset for all models to use, but you can create multiple datasets, each to be used by a different set of models, depending on your use case.\n\n![image.png](attachment:image.png)","21402267":"split train into train and validate 1458 records -> 70\/30 => 1000 Train\/400 Val","1e46a475":"* save this inv function for later, may need it:\n<pre>\nimport cmath\nd = (b**2) - (4*a*c)\nprint(-b-cmath.sqrt(d))\/(2*a)\nprint(-b+cmath.sqrt(d))\/(2*a)\ndef add_gla3(row, p):\n    return (p[2] + p[1]*row.GrLivArea + p[0]*(row.GrLivArea**2)) <-- change this function\nall_data['GrLivAreaPoly2'] = all_data.apply(lambda row: add_gla3(row,curve_fit_gla), axis=1)","e2dc3e43":"Compare Train to Test data, verify the distributions look similar, maybe add probablity plots per feature with train and test on same chart","f2ddeaf4":"We use the **cross_val_score** function of Sklearn. However this function has no shuffle attribute, so we add one line of code,  in order to shuffle the dataset  prior to cross-validation","1958b13f":"[Documentation][1] for the Ames Housing Data indicates that there are outliers present in the training data\n[1]: http:\/\/ww2.amstat.org\/publications\/jse\/v19n3\/Decock\/DataDocumentation.txt","1f04fbf6":"###Less simple Stacking : Adding a Meta-model","57309059":"Let's explore these outliers\n","a1b6098c":"- **Functional** : data description says NA means typical","9a4bd360":"We have bumped up the predictions and they look correct so far, now to verify on the previous chart","cf410637":"Create predictions for all models for testing","71b54d8d":"Now lets use a log y scale","aa3531ae":"looks like we have 2, MasVnrArea and BsmtFinSF1 so use those for this test"}}