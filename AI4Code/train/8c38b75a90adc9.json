{"cell_type":{"0e588d9a":"code","386daf6e":"code","d7f9b26e":"code","a50fdf8c":"code","7bde9252":"code","ecee6421":"code","8435d3db":"code","9fc7063f":"code","ffc629cf":"code","5a445eb3":"code","9f01f2cc":"code","d2ef582c":"code","330dacdb":"code","30a95a0d":"code","c28effe0":"code","8566c9ad":"code","b737d244":"code","6e05064a":"code","ef8aeda0":"code","5104f5d8":"code","83c3d2a0":"code","4a30f927":"code","96ab9fc1":"code","732603e4":"code","d1cbe6c4":"code","3addd9b1":"code","c7c8abc1":"code","0af05596":"code","7c0f820c":"code","87567b5e":"code","9583ba88":"code","550b9a23":"code","260b48db":"code","a1eb0d3c":"code","c2cc595d":"code","cddbd100":"code","4d460eff":"code","20e145bf":"code","223dab05":"code","11208e29":"code","5f8d2acb":"code","dcc32ef1":"code","55c601b6":"code","f2ff611f":"code","de5d781e":"code","7bbe8238":"code","d94a4e4b":"code","95020dc5":"code","95807cc4":"code","3b22521c":"code","21474e35":"code","d6c1d2fd":"code","e272cdce":"markdown","1950a742":"markdown","95a8b24c":"markdown","68359a0f":"markdown","64b7e776":"markdown","3126e78a":"markdown","33a357c7":"markdown","b0185656":"markdown","631bcd95":"markdown","44e22d3a":"markdown","9b591c8c":"markdown","21e039b8":"markdown","1c20f7f2":"markdown","4465f5c9":"markdown","24e5cb4a":"markdown","26f27441":"markdown"},"source":{"0e588d9a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","386daf6e":"import pandas as pd\nimport pandas_profiling as pp\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","d7f9b26e":"# load the data\ndat_train = pd.read_csv('..\/input\/titanic\/train.csv')\ndat_test = pd.read_csv('..\/input\/titanic\/test.csv')","a50fdf8c":"dat_train.info()\ndat_train.describe(include='all')","7bde9252":"dat_test.info()\ndat_test.describe(include='all')","ecee6421":"dat_train.sample(5)","8435d3db":"dat_test.sample(5)","9fc7063f":"dat_all = dat_train.copy()\ndat_all.drop('Survived',axis=1,inplace=True)\ndat_all['Source']='Train'\ndat_all.info()\n\ndat_all = pd.concat([dat_all, dat_test], ignore_index=True)\ndat_all['Source'].fillna('Test',inplace=True)\ndat_all.info()","ffc629cf":"dat_all.describe(include='all')","5a445eb3":"# split name into title, first and last names\ndat_all[['LastName','FirstName']]= dat_all['Name'].str.split(',',expand=True)","9f01f2cc":"dat_all[['Salutation','FirstName']]= dat_all['FirstName'].str.split('.', 1, expand=True)","d2ef582c":"dat_all.sample(5)","330dacdb":"lsal = dat_all.Salutation.unique()\nprint(lsal)","30a95a0d":"Saldict = {}\nfor l in lsal:\n    if l.strip() in ['Lady','Sir','the Countess']:\n        Saldict[l] = \"Royalty\"\n    if l.strip() in ['Capt','Col','Major','Dr','Rev']:\n        Saldict[l] = \"Services\"\n    if l.strip() in ['Miss','Master','Don','Dona','Jonkheer','Mlle','Mme','Mr','Ms','Mrs']:\n        Saldict[l] = \"Others\"\n\nprint(Saldict)","c28effe0":"dat_all['SubCategory'] = dat_all['Salutation'].apply(lambda x: Saldict[x])","8566c9ad":"dat_all.sample(5)","b737d244":"dat_all[dat_all['Fare'].isnull()]","6e05064a":"dat_all.groupby(['Pclass']).Fare.agg(np.median)","ef8aeda0":"dat_all['Fare'].fillna(dat_all.groupby(['Pclass']).Fare.agg(np.median)[3], inplace=True)","5104f5d8":"dat_all.info()","83c3d2a0":"print(dat_all[dat_all.Age.isnull()].Salutation.unique())\ndat_all.Age = dat_all.groupby(['Salutation']).Age.apply(lambda x: x.fillna(x.median()))\ndat_all.info()","4a30f927":"dat_all[dat_all['Embarked'].isnull()]","96ab9fc1":"dat_all.Embarked.unique()","732603e4":"dat_all.Embarked.fillna('S',inplace=True)\ndat_all.info()","d1cbe6c4":"temp = dat_all[dat_all['Cabin'].isnull()]\ntemp.sample(10)","3addd9b1":"dat_all[dat_all.LastName.str.contains('Nakid')]","c7c8abc1":"dat_all.Cabin.fillna('?000',inplace=True)\ndat_all.info()","0af05596":"# pp.ProfileReport(dat_train)","7c0f820c":"dat_all['CabinL'] = dat_all.Cabin.map(lambda x: x[0])\ndat_all['CabinCode'] = dat_all.Cabin.map(lambda x: x[1:])\ndat_all.CabinL.unique()","87567b5e":"dat_all.sample(5)","9583ba88":"dat_all.info()","550b9a23":"dat_all['FamilySize'] = dat_all['SibSp']+dat_all['Parch']+1","260b48db":"dat_all.Pclass = dat_all.Pclass.astype('category')\nfor col in dat_all.columns:\n    if dat_all[col].dtype == 'object':\n        dat_all[col] = dat_all[col].astype('category')   \n    \ndat_all.info()","a1eb0d3c":"df_train = dat_all[dat_all.Source == \"Train\"].copy()\ndf_train.info()","c2cc595d":"df_test = dat_all[dat_all.Source == \"Test\"].copy()\ndf_test.info()","cddbd100":"# drop Ticket (high cardinality) and Cabin (high % missing) and Name (irrelevant)\nfeatures = ['Pclass','Sex','Age','FamilySize','Fare','Embarked','SubCategory','CabinL']\ncol2drop = list(set(df_train.columns)-set(features))\nprint(col2drop)\ntarget = 'Survived'","4d460eff":"for col in features:\n    if df_train[col].dtype not in [np.int, np.float]:\n        print(col,':',len(df_train[col].unique()))","20e145bf":"# trim training data to include relevant columns\ny_train = dat_train[target]\nX_train = df_train[features].copy()\nprint(X_train.info())\n\nX_test = df_test[features].copy()\nprint(X_test.info())","223dab05":"print(features)","11208e29":"from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split\n\ncat_feat = ['Pclass','Sex','Embarked','SubCategory','CabinL']\nflt_feat = ['Fare','Age']\nnum_feat = list(set(features)-set(cat_feat)-set(flt_feat))\n\nnp.random.seed(0)\nX_tr, X_valid, y_tr, y_valid = train_test_split(X_train,y_train,test_size=0.2,random_state=0)\n\nprect = ColumnTransformer([('OH',OneHotEncoder(),cat_feat),('stdsc',StandardScaler(),flt_feat)], remainder='passthrough')\nX_train_ct = prect.fit_transform(X_tr)\nX_valid_ct = prect.transform(X_valid)\nX_test_ct = prect.transform(X_test)\n\nprint(X_train_ct.shape)\nprint(X_valid_ct.shape)\nprint(X_test_ct.shape)","5f8d2acb":"fig, axs = plt.subplots(6,4, figsize=(20,20))\nfor i in range(X_train_ct.shape[1]):\n    r,c = divmod(i,4)\n    sns.histplot(X_train_ct[:,i],bins=20,ax=axs[r,c],color='red',alpha=0.5,linewidth=0)\n    sns.histplot(X_valid_ct[:,i],bins=20,ax=axs[r,c],color='green',alpha=0.4,linewidth=0)\n    sns.histplot(X_test_ct[:,i],bins=20,ax=axs[r,c],color='blue',alpha=0.3,linewidth=0)\n","dcc32ef1":"from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score, precision_score, recall_score, classification_report\nfrom sklearn.metrics import plot_roc_curve, plot_confusion_matrix, plot_precision_recall_curve\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB","55c601b6":"def model_fit_summarize(model):\n    model.fit(X_train_ct, y_tr)\n    y = model.predict(X_valid_ct)\n\n    confmat = confusion_matrix(y_valid,y)\n    acc = accuracy_score(y_valid, y)*100\n    pre = precision_score(y_valid, y)*100\n    rec = recall_score(y_valid, y)*100\n    roc_auc = roc_auc_score(y_valid, y)*100\n\n#     print('Confusion Matrix :\\n',confmat)\n    print('Accuracy score : {:.2f} %'.format(acc))\n    print('Precision score : {:.2f} %'.format(pre))\n    print('Recall score : {:.2f} %'.format(rec))\n    print('ROC_AUC score : {:.2f} %'.format(roc_auc))\n    \n    print('\\nClassification report :\\n', classification_report(y_valid, y))\n\n    plot_roc_curve(model, X_valid_ct, y_valid)\n    plot_precision_recall_curve(model, X_valid_ct, y_valid)\n    plot_confusion_matrix(model, X_valid_ct, y_valid)\n    \n    return acc, pre, rec, roc_auc","f2ff611f":"lr = LogisticRegression(solver='liblinear',random_state=0)\na,b,c,d = model_fit_summarize(lr)\nsummary = pd.DataFrame([a,b,c,d],index=['Accuracy','Precision','Recall','ROC_AUC'],columns=['LogisticRegression'])","de5d781e":"rf = RandomForestClassifier(random_state=0)\na,b,c,d = model_fit_summarize(rf)\nsummary['RandomForest']=[a,b,c,d]","7bbe8238":"xg = XGBClassifier(random_state=0, n_estimators=25, learning_rate=0.01,use_label_encoder=False,eval_metric='logloss')\na,b,c,d = model_fit_summarize(xg)\nsummary['XGBoost']=[a,b,c,d]","d94a4e4b":"nb = GaussianNB()\na,b,c,d = model_fit_summarize(nb)\nsummary['NaiveBayes']=[a,b,c,d]","95020dc5":"pd.set_option('precision',1)\nprint(summary)","95807cc4":"# n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# max_features = ['auto', 'sqrt']\n# max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n# max_depth.append(None)\n# min_samples_split = [2, 5, 10]\n# min_samples_leaf = [1, 2, 4]\n# bootstrap = [True, False]\n# random_grid = {'n_estimators': n_estimators,\n#                'max_features': max_features,\n#                'max_depth': max_depth,\n#                'min_samples_split': min_samples_split,\n#                'min_samples_leaf': min_samples_leaf,\n#                'bootstrap': bootstrap}\n\n# from sklearn.model_selection import RandomizedSearchCV\n# rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, \n#                                n_iter = 100, cv = 5, verbose=2, random_state=0, n_jobs = -1)\n# rf_random.fit(X_train_ct, y_tr)\n\n# print(\"Best Params:\",rf_random.best_params_)\n# print(\"Best Estimator\", rf_random.best_estimator_)","3b22521c":"rf_tuned = RandomForestClassifier(bootstrap=False,max_depth=100, min_samples_leaf=4, min_samples_split=2,\n                                  n_estimators=1400, random_state=0, max_features='sqrt')\na,b,c,d = model_fit_summarize(rf_tuned)\nsummary['TunedRF']=[a,b,c,d]","21474e35":"predtest = rf_tuned.predict(X_test_ct)","d6c1d2fd":"# Save test predictions to file\noutput = pd.DataFrame({'PassengerId': dat_test.PassengerId,\n                       'Survived': predtest})\noutput.to_csv('submission.csv', index=False)","e272cdce":"# Preprocessing the data","1950a742":"**Identify rows with null Age**","95a8b24c":"Notes on the Titanic mentions Icard to have been the personal maid to Mrs. Stone - as is validated by the first class cabin being shared between the two. They embarked at Southhampton.","68359a0f":"From grid search cv ...\n\nFitting 5 folds for each of 100 candidates, totalling 500 fits\n\nBest Params: {'n_estimators': 1000, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'auto', 'max_depth': 70, 'bootstrap': True}\n\nBest Estimator RandomForestClassifier(max_depth=70, min_samples_leaf=2, n_estimators=1000,\n                       random_state=0)","64b7e776":"# Model","3126e78a":"**Identify the rows with null Fare**","33a357c7":"**Identify rows with null Embarked**","b0185656":"# Build functions","631bcd95":"**Load the data and get some basic stats on it**","44e22d3a":"# Titanic - ML from disaster Competition\n![1200px-RMS_Titanic_3.jpg](attachment:8e173b1e-17ac-4ae8-b9c6-e08cd8424480.jpg)\n\nPicture from Wikipedia","9b591c8c":"**Combine the train and test data sets to help fill in null values**","21e039b8":"# Checking distributions across train and test data","1c20f7f2":"# Know your data\n\n","4465f5c9":"**Identify rows with null Cabin**","24e5cb4a":"**Split Name to help extract useful features**","26f27441":"**Hyperparameter tuning on Random Forest**"}}