{"cell_type":{"f428083e":"code","45449334":"code","2ca1127d":"code","2eca9406":"code","4b16aa69":"code","772854b6":"code","8f2fb0b8":"code","36c584ae":"code","8677a431":"code","9ef2721c":"code","056befd6":"code","d238044e":"code","7931a04c":"code","03238864":"code","33c155bf":"code","e8a49e1f":"code","9c84dbe2":"code","2d8d3966":"code","41f1d12d":"code","64f445f3":"code","47e7b3b3":"code","97428614":"code","dce333f4":"code","08970f06":"code","ecece9ac":"code","bce027b4":"code","99edc514":"code","ace9d7c1":"code","6b6c89dd":"code","cf1fbe87":"code","781f6477":"code","b236a74e":"code","24374aa1":"code","949d74e4":"code","7318d573":"code","9515c7fa":"code","f235719f":"markdown","70522b06":"markdown","a6d99606":"markdown","5917a7b7":"markdown","8edf1f14":"markdown","0542ca76":"markdown","99103bb9":"markdown","b74c94a1":"markdown","0251d939":"markdown","5fa8a5ef":"markdown","265dff0a":"markdown","a1dbabd9":"markdown","fec2cc96":"markdown"},"source":{"f428083e":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n%matplotlib inline","45449334":"#importing data\ndf=pd.read_csv(\"..\/input\/video-games\/convertcsv-2.csv\")","2ca1127d":"#creating train and test dataset\nfrom sklearn.model_selection import train_test_split\ntrain,test=train_test_split(df,test_size=0.18,random_state=0)","2eca9406":"#First look\ntrain.head()","4b16aa69":"train.describe()","772854b6":"train.info()\n","8f2fb0b8":"#missing values\ntrain.isnull().sum().sort_values()\n#comment :there are a lot of missing values in metascore,let's clean them","36c584ae":"#Keeping Original data\ndata=train.copy()","8677a431":"#Deleting rows where the target is missing\ntrain.dropna(axis=0,inplace=True)","9ef2721c":"#Removing unnecessary features and adding other ones\nl=list(train['release_date'])\nlf=[2019 -int(m[len(m)-4:]) for m in l] #2019 is the year when the data was collected\ntrain['game_age']=lf\ntrain=train.drop(['game','release_date'],axis=1)","056befd6":"train.head()","d238044e":"train.describe(include='object')","7931a04c":"train.corr()","03238864":"#Converting categorical variables to dummies\none_hot_encode_cols = train.dtypes[train.dtypes == np.object]  # filtering by string categoricals\none_hot_encode_cols = one_hot_encode_cols.index.tolist()\none_hot_encode_cols","33c155bf":"#one_hot_encode_cols.remove('owners')\n\ntrain= pd.get_dummies(train, columns=one_hot_encode_cols, drop_first=True)\ntrain.head()\n","e8a49e1f":"#Creating features: min_owners and max_owners\nls=list(train['owners'])\nls_min=[m[:i]  for m in ls for i in range(len(m)) if ('\\xa0..\\xa0' in m[i:i+4])]\nls_max=[m[i:]  for m in ls for i in range(len(m)) if ('\\xa0..\\xa0' in m[i-4:i])]\nls_m=[]\nls_mx=[]\nfor i in ls_max:\n  if(i==''):\n    continue\n  else:\n    i=i.strip(',')\n    i=i.replace(',','')    \n    i=float(i)\n    ls_mx.append(i)\nfor i in ls_min:\n  if(i==''):\n    continue\n  else:\n    i=i.strip(',')\n    i=i.replace(',','')    \n    i=float(i)\n    ls_m.append(i)\n    \n","9c84dbe2":"train['min_owners']=ls_m\ntrain['max_owners']=ls_mx\ntrain.drop(['owners'],inplace=True,axis=1)","2d8d3966":"train.head()","41f1d12d":"train.hist()","64f445f3":"def skew_df(data: pd.DataFrame, skew_limit: float) -> pd.DataFrame:\n    # Define a limit above which we will log transform\n    skew_vals = data.skew()\n\n    # Showing the skewed columns\n    skew_cols = (skew_vals\n                 .sort_values(ascending=False)\n                 .to_frame('Skew')\n                 .query('abs(Skew) > {}'.format(skew_limit))\n    )\n    return skew_cols","47e7b3b3":"train=train.astype({'price':'float64','number':'int'})\nskew_col=skew_df(train,0.75)\nskew_col\n","97428614":"for col in skew_col.index.values:\n    train['log_' + col] = train[col].apply(np.log1p)","dce333f4":"log_df = train.filter(regex='^log_', axis=1)\ntrain.head()","08970f06":"small_df=train[['price','log_price','log_max_owners','log_min_owners','max_owners','log_median_playtime','log_average_playtime','game_age','metascore','log_number','number']]\nsns.pairplot(data=small_df,plot_kws=dict(alpha=.2, edgecolor='none'))\n","ecece9ac":"from sklearn.preprocessing import PolynomialFeatures\npf = PolynomialFeatures(degree=2)","bce027b4":"small_df","99edc514":"features = ['log_average_playtime', 'log_max_owners','game_age','max_owners']\npf.fit(small_df[features])","ace9d7c1":"feat_array = pf.transform(small_df[features])\nf_df=pd.DataFrame(feat_array,index=small_df.index, columns = pf.get_feature_names(input_features=features))\nf_df.drop('1', axis=1, inplace=True)\nf_df.head()\n","6b6c89dd":"# Plot a heatmap of correlations\nwith sns.axes_style('dark'):\n    fig, axes = plt.subplots(figsize=(12,12))\n    mask = np.triu(np.ones_like(f_df.corr(), dtype=bool))\n    sns.heatmap(f_df.corr(),\n                mask=mask,\n                cmap='binary',\n                cbar=False,\n                annot=True,\n                annot_kws={'size':10},\n                fmt='.2f')\n    plt.title('Polynomial Features and Their Correlations', fontsize=16)\n    plt.tick_params(axis='x', labelsize=12)\n    plt.tick_params(axis='y', labelsize=12)","cf1fbe87":"for col in f_df.columns:\n    if col in small_df.columns:\n        f_df.drop(col, axis=1, inplace=True)\nsmall_df = small_df.join(f_df)","781f6477":"small_df.head()","b236a74e":"# Ploting a heatmap of correlations\nwith sns.axes_style('dark'):\n    fig, axes = plt.subplots(figsize=(12,12))\n    mask = np.triu(np.ones_like(small_df.corr(), dtype=bool))\n    sns.heatmap(small_df.corr(),\n                mask=mask,\n                cmap='binary',\n                cbar=False,\n                annot=True,\n                annot_kws={'size':10},\n                fmt='.2f')\n    plt.title('Correlations', fontsize=16)\n    plt.tick_params(axis='x', labelsize=12)\n    plt.tick_params(axis='y', labelsize=12)","24374aa1":"small_df.head()","949d74e4":"small_df.shape","7318d573":"from scipy.stats.mstats import normaltest # D'Agostino K^2 Test","9515c7fa":"normaltest(small_df.metascore.values)","f235719f":"<h2>Hypothesis Testing","70522b06":"<h2>Overview<\/h2>","a6d99606":"<h3>Observations :<\/h3>\n\n\n*  The target (metascore) has a normal distribution.\n*   No strong linear relationship between the features and the target. Linear regression might not be well-suited to this problem.\n\n\n\n\n*  We can try adding polynomial and interaction terms and examine their correlation with the target.","5917a7b7":"<h2>Data Cleaning ","8edf1f14":"<h4>Log transformation for skewed variables","0542ca76":"<h4>Create a function to check skewness","99103bb9":"<h4>Ploting log columns that have nearly normal distribution","b74c94a1":"<h4>Joining these new columns to our dataset","0251d939":"<h5>p-value<0.05 ===> we reject the null hypothesis","5fa8a5ef":"<h2>FINAL DATASET","265dff0a":"<h5>H0 : We have a normally distributed trget<\/h5>\n<h5>H1: We haven't","a1dbabd9":"<h2>Project-1 :Explorary Data Analysis For Machine Learning<\/h2>\n<h4>BY IHEB CHAABANE<\/h4>\nThis notebook is part of my first project required by IBM Machine Learning Program.\n<h5>DATA Source:https:\/\/github.com\/rfordatascience\/tidytuesday\/tree\/master\/data\/2019\/2019-07-30\nIn this Notebook i will clean the data,explore relationship between features and target(Metascore) and if necessary do some feature engineering and finally do hypothesis testing<\/h5>","fec2cc96":"<h4>Adding polynomial Features"}}