{"cell_type":{"74e13250":"code","948543cb":"code","c31030e4":"code","26aa13d7":"code","6c73e87f":"code","94c453a6":"code","c41c1b7b":"code","7a9d8632":"code","b6f94089":"code","01d48329":"code","0c43a5bf":"code","8cb1c56f":"code","849c89a8":"code","ef547898":"code","e96a0e44":"code","0e55cd25":"code","739832aa":"code","79798f0e":"code","60c3a906":"code","a54c4c8b":"code","9b6b243d":"code","22aa0a10":"code","d84d0700":"code","6fa45da4":"code","1d631393":"code","b9022985":"code","6c7e47f7":"code","2e6e850a":"code","2c8e4d17":"code","0324cd45":"code","dcbd266c":"code","9deb99db":"code","f8791e32":"code","f60c081a":"markdown"},"source":{"74e13250":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","948543cb":"df = pd.read_csv('\/kaggle\/input\/car-sales\/Car_sales.csv')\ndf.head()","c31030e4":"df.isnull().sum()","26aa13d7":"df.dropna(inplace = True)","6c73e87f":"df.head()","94c453a6":"sns.heatmap(df.isnull())","c41c1b7b":"df.describe()","7a9d8632":"df = df[['Engine_size','Horsepower','Fuel_efficiency','Price_in_thousands']]\nprint(df.head())","b6f94089":"plt.scatter(df['Engine_size'],df['Price_in_thousands'])","01d48329":"plt.scatter(df['Horsepower'],df['Price_in_thousands'])","0c43a5bf":"plt.scatter(df['Fuel_efficiency'],df['Price_in_thousands'])","8cb1c56f":"features = df[['Engine_size','Horsepower','Fuel_efficiency']].to_numpy()\nprint(features)","849c89a8":"for i in range(3):\n    min = np.min(features[:,i])\n    max = np.max(features[:,i])\n    features[:,i] = np.min(features[:,i] - min)\/(max-min)","ef547898":"target = df['Price_in_thousands'].to_numpy()\nprint(target)","e96a0e44":"weights = np.random.rand(3) #randomly generated weights\nprint(weights)","0e55cd25":"b = np.random.rand(1) #randomly generated bias\nbias = np.array([b[0] for i in range(len(features))])\nprint(bias)","739832aa":"def linear(features,weights,bias):\n    y_hat = weights.dot(features.transpose()) + np.array([b[0] for i in range(len(features))])\n    return y_hat","79798f0e":"y_hat = linear(features,weights,b)\nprint(y_hat)","60c3a906":"def meansquare(y,y_hat):\n    MSE = np.sum((y-y_hat) **2) \/ len(y)\n    return MSE","a54c4c8b":"error = meansquare(target,y_hat)\nprint(error)","9b6b243d":"def gradient(target,features,weights,bias):\n    m =len(features)\n    target_pred = linear(features,weights,bias)\n    loss = target - target_pred\n    grad_bias = np.array([-2\/m * np.sum(loss)])\n    grad_weights = np.ones(3)\n    feature_0 = np.array([feature[0] for feature in features])\n    grad_weights[0] = -2\/m * np.sum(loss * feature_0)\n    feature_1 = np.array([feature[1] for feature in features])\n    grad_weights[1] = -2\/m * np.sum(loss * feature_1)\n    feature_2 = np.array([feature[1] for feature in features])\n    grad_weights[2] = -2\/m * np.sum(loss * feature_2)\n    return grad_bias,grad_weights","22aa0a10":"def grad_desc(learning_rate,epochs,target,features,weights,bias):\n    MSE_list = []\n    for i in range(epochs):\n        grad_bias,grad_weights = gradient(target,features,weights,bias)\n        weights -= grad_weights * learning_rate\n        bias -= grad_bias * learning_rate\n        new_pred = linear(features,weights,bias)\n        mse_new = meansquare(target,new_pred)\n        MSE_list.append(mse_new)\n    return_dict = {'weights': weights, 'bias': bias[0], 'MSE': mse_new, 'MSE_list': MSE_list}\n    return return_dict","d84d0700":"model = grad_desc(0.0001,2000,target,features,weights,bias)","6fa45da4":"print(\"Weights- {}\\nBias- {}\\nMSE- {}\".format(model['weights'], model['bias'], model['MSE']))","1d631393":"def linearmodel(model,feature_list):\n    price = np.sum(model['weights'] * feature_list) + model['bias']\n    return price","b9022985":"target_price = 196\nfeature_list = [2.0,4,8.5]\npredicted_price = linearmodel(model, feature_list)\nprint(\"Price in thousands:\",predicted_price)","6c7e47f7":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler","2e6e850a":"X = df[['Engine_size','Horsepower','Fuel_efficiency']]\ny = df['Price_in_thousands']","2c8e4d17":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state =101)","0324cd45":"scaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","dcbd266c":"linear  = LinearRegression()\nlinear.fit(X_train,y_train)\npred = linear.predict(X_test)","9deb99db":"from sklearn.metrics import r2_score, mean_squared_error\nprint('r2 score: '+str(r2_score(y_test, pred)))\nprint('RMSE : '+str(np.sqrt(mean_squared_error(y_test, pred))))","f8791e32":"user_input = [[2.0,4,8.5]]       #['Watch time(Minutes)','Stream time(minutes)','Peak viewers','Average viewers','Followers','Views gained']\nuser_pred = linear.predict(user_input,)\nprint(\"Price in thousands:\",user_pred)","f60c081a":"**Note to Self and the Community** - This is a request to anyone reading this kernel. Please do not spam the links of your notebooks in the comments or on any other notebook as it reduces the chances of other Kagglers to get recognised. I myself was guilty of this practise and i making ammendments now that i understand. So please provide your genuine feedbacks on the kernels and if you are commenting down i will definitely check out your notebooks. So keep Kaggling and help the community be more friendly and grow. Alright lets start with the notebook."}}