{"cell_type":{"563a9281":"code","49a91037":"code","e67c27c9":"code","177d726e":"code","5d4c6991":"code","50175a75":"code","8a84bb4f":"code","3c84557e":"code","b9616fe8":"code","312c63cf":"code","74612022":"markdown","b2df259a":"markdown","c06f40d4":"markdown","09d6b131":"markdown","28680678":"markdown","1aab8b08":"markdown","59378415":"markdown"},"source":{"563a9281":"import numpy as np\nimport pandas as pd\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance\nfrom xgboost import cv\nimport matplotlib.pyplot as plt\nplt.rcParams.update({'font.size': 12})","49a91037":"# read in the data\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest  = pd.read_csv(\"..\/input\/titanic\/test.csv\")\n\n# select only the numerical features\nX_test  = test.select_dtypes(include=['number']).copy()\nX_train = train.select_dtypes(include=['number']).copy()\n\n# drop the target column from the training data\nX_train = X_train.drop(['Survived'], axis=1)\n\n# add the train\/test labels\nX_train[\"AV_label\"] = 0\nX_test[\"AV_label\"]  = 1\n\n# make one big dataset\nall_data = pd.concat([X_train, X_test], axis=0, ignore_index=True)\n\n# shuffle\nall_data_shuffled = all_data.sample(frac=1)\n\n# create our DMatrix (the XGBoost data structure)\nX = all_data_shuffled.drop(['AV_label'], axis=1)\ny = all_data_shuffled['AV_label']\nXGBdata = xgb.DMatrix(data=X,label=y)\n\n# our XGBoost parameters\nparams = {\"objective\":\"binary:logistic\",\n          \"eval_metric\":\"logloss\",\n          'learning_rate': 0.05,\n          'max_depth': 5, }\n\n# perform cross validation with XGBoost\ncross_val_results = cv(dtrain=XGBdata, params=params, \n                       nfold=5, metrics=\"auc\", \n                       num_boost_round=200,early_stopping_rounds=20,\n                       as_pandas=True)\n\n# print out the final result\nprint((cross_val_results[\"test-auc-mean\"]).tail(1))","e67c27c9":"classifier = XGBClassifier(eval_metric='logloss',use_label_encoder=False)\nclassifier.fit(X, y)\nfig, ax = plt.subplots(figsize=(12,4))\nplot_importance(classifier, ax=ax)\nplt.show();","177d726e":"X = X.drop(['PassengerId'], axis=1)","5d4c6991":"XGBdata = xgb.DMatrix(data=X,label=y)\ncross_val_results = cv(dtrain=XGBdata, params=params, \n                       nfold=5, metrics=\"auc\", \n                       num_boost_round=200,early_stopping_rounds=20,\n                       as_pandas=True)\n\nprint((cross_val_results[\"test-auc-mean\"]).tail(1))","50175a75":"train = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest  = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\nX_test  = test.select_dtypes(include=['number']).copy()\nX_train = train.select_dtypes(include=['number']).copy()\n# drop the target column from the training data\nX_train = X_train.drop(['SalePrice'], axis=1)\n# add the train\/test labels\nX_train[\"AV_label\"] = 0\nX_test[\"AV_label\"]  = 1\nall_data = pd.concat([X_train, X_test], axis=0, ignore_index=True)\nall_data_shuffled = all_data.sample(frac=1)\nX = all_data_shuffled.drop(['AV_label'], axis=1)\ny = all_data_shuffled['AV_label']\nXGBdata = xgb.DMatrix(data=X,label=y)\ncross_val_results = cv(dtrain=XGBdata, params=params, \n                       nfold=5, metrics=\"auc\", \n                       num_boost_round=200,early_stopping_rounds=20,\n                       as_pandas=True)\n\nprint((cross_val_results[\"test-auc-mean\"]).tail(1))","8a84bb4f":"classifier = XGBClassifier(eval_metric='logloss',use_label_encoder=False)\nclassifier.fit(X, y)\nfig, ax = plt.subplots(figsize=(12,4))\nplot_importance(classifier, ax=ax)\nplt.show();","3c84557e":"X = X.drop(['Id'], axis=1)","b9616fe8":"XGBdata = xgb.DMatrix(data=X,label=y)\ncross_val_results = cv(dtrain=XGBdata, params=params, \n                       nfold=5, metrics=\"auc\", \n                       num_boost_round=200,early_stopping_rounds=20,\n                       as_pandas=True)\n\nprint((cross_val_results[\"test-auc-mean\"]).tail(1))","312c63cf":"from scipy import stats\nfeatures_list = X_test.columns.values.tolist()\nfor feature in features_list:\n    statistic, pvalue = stats.kstest(X_train[feature], X_test[feature])\n    print(\"p-value %.2f\" %pvalue, \"for the feature\",feature)","74612022":"again we have a much better result, indicating that both the `train` and the `test` features have the same distributions.\n\n## A little more detailed examination\nWe shall now compare each feature individually. For continuous distributions one uses the [Kolmogorov-Smirnov test](https:\/\/en.wikipedia.org\/wiki\/Kolmogorov%E2%80%93Smirnov_test) of goodness of fit, here using the SciPy [`kstest`](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.stats.kstest.html) (For categorical data one should use [Pearson's chi-squared test](https:\/\/en.wikipedia.org\/wiki\/Pearson%27s_chi-squared_test)). We shall be calculating the $p$-values of the hypothesis that the two distributions are indeed the same.","b2df259a":"# What is Adversarial Validation?\nThe objective of any predictive modelling project is to create a model using the training data, and afterwards apply this model to the test data. However, for the best results it is essential that the training data is a representative sample of the data we intend to use it on (*i.e.* the test data), otherwise our model will, at best, under-perform, or at worst, be completely useless.   \n\n***Adversarial Validation*** is a very clever and very simple way to let us know if our test data and our training data are similar; we combine our `train` and `test` data, labeling them with say a `0` for the training data and a `1` for the test data, mix them up, then see if we are able to correctly re-identify them using a binary classifier.\n\nIf we cannot correctly classify them, *i.e.* we obtain an area under the [receiver operating characteristic curve](https:\/\/en.wikipedia.org\/wiki\/Receiver_operating_characteristic) (ROC) of 0.5 then they are indistinguishable and we are good to go.\n\nHowever, if we can classify them (ROC > 0.5) then we have a problem, either with the whole dataset or more likely with some features in particular, which are probably from  different distributions in the test and train datasets.\nIf we have a problem, we can look at the feature that was most out of place. The problem may be that there were values that were only seen in, say, training data, but not in the test data. If the contribution to the ROC is very high from one feature, it may well be a good idea to remove that feature from the model.\n\n\n## Adversarial Validation to reduce overfitting\nThe key to avoid overfitting is to create a situation where the local cross-vlidation (CV) score is representative of the competition score. When we have a ROC of 0.5 then your local data is representative of the test data, thus your local CV score should now be representative of the Public LB score.\n\nProcedure:\n\n* drop the training data target column \n* label the `test` and `train` data with `0` and `1` (it doesn't really matter which is which)\n* combine the training and test data into one big dataset\n* perform the binary classification, for example using XGboost\n* look at our AUC ROC score\n\nWe shall look at two examples of adversarial validation. Note: For the purposes of these demonstrations we shall only be using the numeric features.\n# Titanic\nFor our first example we shall look at the [Titanic - Machine Learning from Disaster](https:\/\/www.kaggle.com\/c\/titanic) dataset","c06f40d4":"We can see an AUC of 1 which indicates that our classifier is able to perfectly distinguish between the original training and test data. Let us look at the most important features:","09d6b131":"we can see very small $p$-values for the features `Id` and `AV_label`. This is wonderful news as these are precisely the features that have completely different distributions between the training and the test datasets.\n# Related reading\nAs far as I can tell Adversarial Validation was first described in the following two blog posts:\n* [\"Adversarial validation, part one\"](http:\/\/fastml.com\/adversarial-validation-part-one\/) by  Zygmunt Zaj\u0105c (2016-05-23)\n* [\"Adversarial validation, part two\"](http:\/\/fastml.com\/adversarial-validation-part-two\/) by Zygmunt Zaj\u0105c (2016-06-08)","28680678":"we now have a much more reasonable value, much closer to our ideal value of 0.5\n# House Prices\nFor our second example we shall use the [House Prices - Advanced Regression Techniques](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques) dataset","1aab8b08":"This was actually to be expected since we did not drop the `PassengerId` column. Our classifier has learned that the distribution of values of this feature are very different for the `train` and `test` rows. Let us drop the `PassengerId` column and re-calculate the ROC.","59378415":"again we have a value of 1, this time because we did no drop the `Id` column"}}