{"cell_type":{"f5fb9a04":"code","539c1af3":"code","8f92808c":"code","9fe264da":"code","8e13ac3b":"code","99a2ca16":"code","98166fdd":"code","cc92dd16":"code","7cb305cb":"code","8ce7923b":"code","56265550":"code","597dab88":"code","02801447":"code","c076d546":"code","4df2b347":"code","c5950349":"code","d8d92d50":"code","7aa97de0":"code","76000004":"code","dce6b96a":"code","cb5b521e":"code","71c87d7c":"code","923a82b5":"code","0a6d7b6f":"code","05e7043f":"code","6e4450cd":"code","b716a7d6":"markdown","36864279":"markdown","2d0ba925":"markdown","0eabade9":"markdown","44a00c4a":"markdown"},"source":{"f5fb9a04":"from __future__ import absolute_import, division, print_function\nimport codecs \nimport glob \nimport multiprocessing\nimport os \nimport pprint\nimport re \nimport nltk \nimport gensim.models.word2vec as w2v\nimport sklearn.manifold\nimport numpy as np\nimport matplotlib as plt\nimport pandas as pd \nimport seaborn as sns","539c1af3":"nltk.download('punkt')#This is a pretrained tokenizer \nnltk.download('stopwords')","8f92808c":"#To get the book\nbook_filenames = sorted(glob.glob('..\/input\/game-of-thrones-books-for-nlp\/*.txt'))\nbook_filenames","9fe264da":"corpus_raw = u\"\"\nfor book_filename in book_filenames:\n    print(\"Reading '{0}'......\".format(book_filename))\n    with codecs.open(book_filename,\"r\",\"utf-8\") as book_file:\n        corpus_raw += book_file.read()\n    print(\"corpus is now {0} characters long\".format(len(corpus_raw)))\n#     print()","8e13ac3b":"tokenizer = nltk.data.load('tokenizers\/punkt\/english.pickle')\nraw_sentences = tokenizer.tokenize(corpus_raw)","99a2ca16":"len(raw_sentences)","98166fdd":"#to convert into a list of words :\n#Plus removal of unnecessary characters like \"[,\/,\\,#,\" etc. \ndef sentence_to_wordlist(raw):\n    clean = re.sub(\"[^a-zA-Z]\", \" \",raw)\n    words = clean.split()\n    return words","cc92dd16":"sentences = []\nfor raw_sentence in raw_sentences:\n    if len(raw_sentence)>0:\n        sentences.append(sentence_to_wordlist(raw_sentence))","7cb305cb":"token_count = sum([len(sentance) for sentance in sentences])\nprint(\"The book contains {0:,} tokens\".format(token_count))","8ce7923b":"#DISTANCE, SIMILARITY, RANKING is what we get amongst words when we get the vectors out of text\ndimensionality = 300\nmin_word_count = 3 #min num of word count threshold - \"what is the smallest set of words that we want to choose to train\"\ncpu_cores = multiprocessing.cpu_count()\ncontext_size = 7\ndownsampling = 1e-3 #threshold for downweighing or reducing the importance of more frequent words \nseed = 1 #random number generator ","56265550":"thrones2vec = w2v.Word2Vec(\n    sg =1,\n    seed = seed,\n    workers = cpu_cores,\n    size = dimensionality,\n    min_count = min_word_count,\n    window = context_size,\n    sample = downsampling\n    \n)","597dab88":"thrones2vec.build_vocab(sentences)","02801447":"# thrones2vec.train(sentences,total_words=thrones2vec.corpus_count,epochs=thrones2vec.epochs)\nthrones2vec.train(sentences=sentences,total_examples=thrones2vec.corpus_count, epochs=10)","c076d546":"print(\"The shape of the matrix we are plugging into TSNE is : \",thrones2vec.wv.syn0.shape)","4df2b347":"tsne = sklearn.manifold.TSNE(n_components=2,random_state=0)\nword_vec_matrix = thrones2vec.wv.syn0\nword_vec_matrix_in_2d = tsne.fit_transform(word_vec_matrix)","c5950349":"word_vec_matrix_in_2d[thrones2vec.wv.vocab['stark'].index]","d8d92d50":"points = pd.DataFrame(\n    [\n        (word,coordinates[0],coordinates[1])\n        for word,coordinates in [\n            (word,word_vec_matrix_in_2d[thrones2vec.wv.vocab[word].index]) \n            for word in thrones2vec.wv.vocab \n        ]\n    ],columns = [\"word\", \"x\", \"y\"])","7aa97de0":"points.head(10)","76000004":"sns.set_context(\"poster\")\npoints.plot.scatter(\"x\", \"y\", s=10, figsize=(20, 12))","dce6b96a":"\ndef plot_region(x_bounds, y_bounds):\n    slice = points[\n        (x_bounds[0] <= points.x) &\n        (points.x <= x_bounds[1]) & \n        (y_bounds[0] <= points.y) &\n        (points.y <= y_bounds[1])\n    ]\n    \n    ax = slice.plot.scatter(\"x\", \"y\", s=35, figsize=(10, 8))\n    for i, point in slice.iterrows():\n        ax.text(point.x + 0.005, point.y + 0.005, point.word, fontsize=11)","cb5b521e":"plot_region(x_bounds=(0,3),y_bounds=(-3,3))","71c87d7c":"def linear_relationship_between_words(start1,end1,end2):\n    similarities = thrones2vec.most_similar_cosmul(positive=[end2,start1],negative=[end1])\n    start2 = similarities[0][0]\n    print(\"{start1} is related to {end1}, as {start2} is related to {end2}\".format(**locals()))\n    return start2","923a82b5":"linear_relationship_between_words(\"Cersei\", \"Lannister\", \"Jon\")","0a6d7b6f":"linear_relationship_between_words(\"Arya\",\"Jon\", \"Cersei\")","05e7043f":"thrones2vec.most_similar('Lannister')","6e4450cd":"thrones2vec.most_similar('Stark', topn=20)","b716a7d6":"# EXPLORATION :","36864279":"## Exploring semantic similarities between book characters","2d0ba925":"## Training Word2Vec","0eabade9":"- How the data looks after reducing dimensionality using TSNE :","44a00c4a":"* Using TSNE to reduce dimensionality and visualize our model's embeddings "}}