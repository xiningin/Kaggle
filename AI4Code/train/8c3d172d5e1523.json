{"cell_type":{"8e21a945":"code","61769227":"code","9bb5103d":"code","6fa132cc":"code","e857fc45":"code","67b87631":"code","a78c328a":"code","ae39342f":"code","8e3f3ed7":"code","37941d06":"code","c38d0f79":"code","73e7987c":"code","540219a0":"code","e11421d6":"code","a53bbf7c":"code","d0d833b0":"code","9d388244":"code","07165f07":"markdown","b85e6efc":"markdown","6b8e0ef1":"markdown","5176399d":"markdown","2061b339":"markdown","6ff831c0":"markdown","18043d5e":"markdown","b60803ca":"markdown","07467438":"markdown","ac5e8d52":"markdown","ac3335fe":"markdown","2bcda5fd":"markdown","1779e105":"markdown","feb9f93d":"markdown","20279496":"markdown"},"source":{"8e21a945":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.impute import KNNImputer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import svm\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import balanced_accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.base import clone\nimport seaborn as sns\n!pip install shap\nimport shap\nimport matplotlib.pyplot as plt","61769227":"df = pd.read_excel(\"\/kaggle\/input\/covid19\/dataset.xlsx\")\nprint(df.shape)","9bb5103d":"df_countNull = df.isnull().sum()\ndf_countNullunder5000 = df_countNull.loc[df_countNull<5000]\nprint(df_countNullunder5000, len(df_countNullunder5000))\nNullunder5000NamesArr = df_countNullunder5000.index.values","6fa132cc":"df_filtered = df[Nullunder5000NamesArr]\ndf_filtered = df_filtered.dropna()\nprint(df_filtered.shape)\ndf_filtered.head()","e857fc45":"df_filtered.groupby('SARS-Cov-2 exam result').count()","67b87631":"data_filtered = df_filtered.drop(['Patient ID','Patient addmited to regular ward (1=yes, 0=no)',\n                                  'Patient addmited to semi-intensive unit (1=yes, 0=no)',\n                                  'Patient addmited to intensive care unit (1=yes, 0=no)'], axis=1)\n\n# Factorize categorical data and keep mapping codes for later\ncol_info = [(col_n, str(col_t)) for col_n, col_t in zip(list(data_filtered), data_filtered.dtypes)]\n\nfactorized_codes = {}\nfor (col_n, col_type) in col_info:\n    if col_type == 'object':\n        factor = pd.factorize(data_filtered.loc[:, col_n])\n        data_filtered.loc[:, col_n] = factor[0]\n        factorized_codes[col_n] = factor[1]\n\ndata_filtered.head()","a78c328a":"feature_cols = data_filtered.drop([\"SARS-Cov-2 exam result\"], axis=1).columns.to_list()\nX = data_filtered[feature_cols]\ny = data_filtered[\"SARS-Cov-2 exam result\"]","ae39342f":"parameters = {}\nparameters['model__n_estimators'] = [10,50,70,100,150,200,300]\nparameters['model__criterion'] = ['gini', 'entropy']\nparameters['model__max_depth'] = [None, 3, 4, 5]\nparameters['model__max_features'] = ['auto', 'sqrt', 'log2', None]\nparameters['model__min_impurity_decrease'] = [0.0, 1e-3]\nparameters['model__min_samples_split'] = [2, 10, 40]\nparameters['model__min_samples_leaf'] = [1, 5]\n\n\nmodel1 = RandomForestClassifier(\n    max_depth=5,\n    criterion = 'gini',\n    min_samples_split=2,\n    min_samples_leaf=1,\n    min_weight_fraction_leaf=0.0,\n    max_leaf_nodes=None,\n    min_impurity_decrease=0.0,\n    min_impurity_split=None,\n    bootstrap=True,\n    oob_score=False,\n    n_jobs=1,\n    random_state=0,\n    verbose=0,\n    warm_start=False,\n    class_weight='balanced'\n)","8e3f3ed7":"my_pipeline1 = Pipeline(steps=[('standard', StandardScaler()),\n                              ('model', model1)\n                             ])\n\nskf = StratifiedKFold(n_splits=5)\n\n\ngrid = GridSearchCV(estimator=my_pipeline1,\n                            param_grid=parameters,\n                            scoring='recall',\n                            cv=skf,\n                            n_jobs=-1)\n\n\nfitted_model = grid.fit(X,y)\n\nresults = grid.cv_results_\nprint(grid.best_estimator_)\n\n# Use best model to get predictions\nmy_pipeline1 = Pipeline(steps=[('standard', StandardScaler()),\n                              ('model', grid.best_estimator_['model'])\n                             ])\n\ny_pred1 = cross_val_predict(my_pipeline1, X, y, cv=skf)\nconf_mat1 = confusion_matrix(y, y_pred1)","37941d06":"def plot_confusion_matrix(cm):\n    ax= plt.subplot()\n    sns.heatmap(cm, annot=True, ax = ax,fmt='g',cmap=sns.color_palette(\"GnBu_d\"), cbar=False, linewidths=1, linecolor='black');\n    ax.set_xlabel('Predict');ax.set_ylabel('True'); \n    ax.set_title('Confusion matrix'); \n    ax.xaxis.set_ticklabels(['Negative', 'Positive']); ax.yaxis.set_ticklabels(['Negative','Positive']);","c38d0f79":"print(\"Balanced Accuracy: \"+str(balanced_accuracy_score(y,y_pred1)*100)+\"%\\n\")\nprint(classification_report(y,y_pred1))\n\nX_trans = StandardScaler().fit_transform(X)\n\nplt.figure(1)\nplot_confusion_matrix(conf_mat1)\nplt.figure(2)\nshap_values1 = shap.TreeExplainer(\n        grid.best_estimator_['model'].fit(X_trans,y), feature_perturbation='tree_path_dependent'\n).shap_values(X)\nshap.summary_plot(shap_values1, X_trans, plot_type=\"bar\",feature_names = X.columns.tolist())","73e7987c":"test_data = df[Nullunder5000NamesArr]\n\n# Select rows with missing values as the test data\nna_rows = test_data.isnull().any(axis=1)\ntest_data = test_data.loc[na_rows, :]\n\ntest_data = test_data.drop(['Patient ID','Patient addmited to regular ward (1=yes, 0=no)',\n                            'Patient addmited to semi-intensive unit (1=yes, 0=no)',\n                            'Patient addmited to intensive care unit (1=yes, 0=no)'], axis=1)\n\ntest_data.head()","540219a0":"for col_n in factorized_codes:\n    replacement = {category: code for code, category in enumerate(factorized_codes[col_n].values)}\n    \n    # Feature has just one category\n    if len(replacement) == 1:\n        # Check whether there are more categories in the testing set\n        mask = test_data.loc[:, col_n].isnull()\n        unqs = np.unique(test_data.loc[~mask, col_n]).tolist()\n        if len(unqs) > len(replacement):\n            \n            # Add new categories\n            for new_code, category in enumerate(unqs):\n                if category not in replacement:\n                    replacement[category] = new_code + 1  # new code\n        \n        \n    test_data.loc[:, col_n] = test_data.loc[:, col_n].replace(replacement)\ntest_data.head()","e11421d6":"y_test = test_data['SARS-Cov-2 exam result'].copy()\nX_test = test_data.loc[:, test_data.columns != 'SARS-Cov-2 exam result'].copy()\n\nX_test.shape","a53bbf7c":"X_train = X.copy()\n\nnormalizer = {}\n\ncolnames = set(list(X_train))\n\n# Normalize training and testing data before creating Inputer\nfor (col_n, col_t) in col_info:\n    if col_n not in colnames:  # Target\n        continue\n    \n    if col_t != 'object':  # Min-max normalization just for the numeric values\n        cmin = np.min(X_train.loc[:, col_n])\n        cmax = np.max(X_train.loc[:, col_n])\n        \n        mask = X_test.loc[:, col_n].isnull()\n        X_train.loc[:, col_n] = (X_train.loc[:, col_n] - cmin) \/ (cmax - cmin)\n        X_test.loc[~mask, col_n] = (X_test.loc[~mask, col_n] - cmin) \/ (cmax - cmin)\n        \n        normalizer[col_n] = {'cmin': cmin, 'cmax': cmax}\n\n# Data inputation\ninputer = KNNImputer(weights='distance', n_neighbors=5)\ninputer.fit(X_train)\n\nX_test = pd.DataFrame(inputer.transform(X_test), columns=list(X_test))\nX_test.head()","d0d833b0":"for (col_n, col_t) in col_info:\n    if col_n not in colnames:  # Target\n        continue\n    \n    if col_t != 'object':  # Denormalize\n        X_test[col_n] = (normalizer[col_n]['cmax'] - normalizer[col_n]['cmin']) * \\\n                X_test[col_n] + normalizer[col_n]['cmin']\n    else:\n        X_test.loc[:, col_n][X_test.loc[:, col_n] <= 0.5] = 0\n        X_test.loc[:, col_n][X_test.loc[:, col_n] > 0.5] = 1\n\nX_test.head()","9d388244":"clf = clone(grid.best_estimator_['model'])\nscaler = StandardScaler().fit(X)\nX_train_transf = scaler.fit_transform(X)\nX_test_transf = scaler.transform(X_test)\n\n\nclf.fit(X_train_transf, y)\n\ny_pred = clf.predict(X_test_transf)\nconf_mat = confusion_matrix(y_test, y_pred)\nprint(conf_mat)\nprint(\"Balanced Accuracy: \" + str(balanced_accuracy_score(y_test,y_pred)*100)+\"%\\n\")\nprint(classification_report(y_test, y_pred))","07165f07":"\n# Conclusions\n\nWe observed the following main points:\n\n    - We got around 84% of balanced accuracy for this problem of unbalanced data, and tried to minimize false negative and false positive cases as much as possible. Our approach can contribute to aid in the decision-making of screening and optimization of bed occupation.\n\n    - Identification of 2 variables that had a great influence on the generation of the models: \"Patient age quantile\" and \"Rhinovirus \/ Enterovirus\". All other variables that influenced the decision are also described in the document. This makes the analysis transparent.\n\n    - These models are extensible from the point of view that we can just add other data sources of the same nature. The processing is automatic.\n\n    - We applied a data inputation technique to predict the missing values and used this data portion as an additional testing set. Our models obtained 57% of balanced accuracy on this analysis. The performance decrease was expected, given that we had to predict the majority of the values.\n    \n\nOur approach utilizes a reduced set of input feature and reaches increased balanced accuracy (84%). For that reason, we believe this reduced set of descriptors should be prioritized during patient screening. Hence, ML models could be efficiently created to help health professionals. With more available data, our models would also become more accurate.\n\n","b85e6efc":"## Input missing data\n\nWe apply a Min-Max standardization strategy to make all features have the same range before performing the k-NN-based data inputation (we apply this process only to the numerical features, the categorical ones are already binary). We are going to denormalize the features after the inputation.\n\nWe also define the inputation model using the training data (samples without missing values) and apply this model to the new testing data.","6b8e0ef1":"## Check how unbalanced is the classification task at hand","5176399d":"## Let's apply the same factors from training to the test data\n\nWe apply the previously stored factors to this new data. Moreover, we observe that at least one of the features in this new testing portion has an additional categories that did not appear on the non-missing data portion. To circunvent this problem we create additional category codes in these cases.","2061b339":"The task is pretty unbalanced. We are going to rely on class weighting to tackle this problem.\n\n\n## Drop patient ID and other non-relevant input features and factorize categories\n\nWe factorize the categorical features appearing on this \"non-missing\" data portion and store the transformation codes to apply to the remaining cases, at part 2 of this analysis.","6ff831c0":"## Denormalize numeric variables and binarize the remaining ones","18043d5e":"# Part 1: prediction model validation\n\n\n## Filter input features: keep only those with fewer than 5000 missing cases\n\nWe limit our analysis to the tests and measurements that were performed frequently. The following features have fewer than 5K missing items.","b60803ca":"## Setup Random Forest (RF) model that uses class weighting to compensate unbalancing\n\nWe are going to use a simple grid search strategy to adjust the parameters of our model.","07467438":"## Separate features and the target","ac5e8d52":"# Data science and Machine Learning applied to COVID 19 diagnosis by using the clinical spectrum dataset assembled by Hospital Israelita Albert Einstein.\n\nBased on real patient data tested for COVID 19 made available by Hospital Israelita Albert Einstein, data science and machine learning techniques, we predict positive cases among suspects. The input data consists of laboratory tests commonly collected for a suspect of COVID-19, and as a result, we obtain a pre-diagnosis and the variables that most influenced the decision of the model. One of the significant challenges here is related to pre-processing the data and the generation of models that minimize False Negative cases. In other words, our main concern is to avoid learning models which classify a case with a positive diagnosis as negative. For this purpose, class balancing techniques are being used, among other analyzes.\n\n\nThis analysis is comprised of two parts:\n\n1. We limit our analisys to samples that have no missing data.\n    - We tune and create an ensemble-based predictor for this data, and validate it using cross-validation\n    - We also analyze which of the features had more impact on the learner\n2. We select the samples with missing data, perform data inputation on them and use the previously defined model to predict their classes\n\nLet's start!\n","ac3335fe":"## Apply model to test data\n\nWe use the same configuration found on the grid search to create a new learner using all the \"non-missing\" data as training set. This learner then predicts the testing set whose missing values were previously filled up. ","2bcda5fd":"## Setup a stratified 5-fold CV evaluation with a Standard Scaler (z-score) and RF","1779e105":"We got around 84% of balanced accuracy.\n\n# Part 2: applying model to the remaining cases\n\n\n## Now we are going to take the rows with missing data to use as testing set\n\nSince we are going to input the missing values, we expect to observe a decrease on the predictive performance. Nonetheless, were the missing tests performed, we believe the obtained accuracy would increase and reach similar vaues as those obtained in Part 1.\n\n\nWe perform the same filtering steps previously used.","feb9f93d":"## Set the target variable and the input features","20279496":"## Performance evaluation:\n\n1 - Get the confusion matrix\n\n2 - Obtain the performance metrics\n\n3 - Analyze input features' importance using the SHAP strategy"}}