{"cell_type":{"b1519526":"code","ac5e2e2f":"code","83d59ec0":"code","a18ac931":"code","dd30f9cf":"code","93315efb":"code","00277469":"code","48cd4a66":"code","a877914e":"code","abe06a71":"code","d63fccc3":"code","565c132d":"code","2f291df5":"code","b15ec62b":"code","e648f3a3":"code","6cee5f42":"code","7aafbd8b":"code","a53e5548":"code","b76199cc":"code","d062ca97":"code","e7861e3f":"code","686a4a79":"code","9b4017b2":"code","98c71538":"code","f383be3f":"code","44779a9a":"code","68cb96ef":"code","0f7afd0a":"code","77237c5d":"code","5a015aa6":"code","a35a27b7":"code","aeebe663":"code","e147ec6a":"code","2657d79a":"code","472c0c1b":"code","59636383":"code","95837c43":"code","6c798849":"code","b6e6e6d9":"code","ea3d5f52":"markdown","d3af1703":"markdown","71e8980a":"markdown","d92b6931":"markdown","ddbcd3d9":"markdown","b227cf85":"markdown"},"source":{"b1519526":"from fastai.vision import *\nfrom fastai.metrics import accuracy\nfrom fastai.basic_data import *\nimport pandas as pd\n\nfrom fastai import *\nfrom fastai.vision import *","ac5e2e2f":"!ls ..\/input","83d59ec0":"path = Path('..\/input\/')\npath_test = Path('..\/input\/test')\npath_train = Path('..\/input\/train')","a18ac931":"df = pd.read_csv(path\/'train.csv')#.sample(frac=0.05)\ndf.head()","dd30f9cf":"df.Id.value_counts().head()","93315efb":"(df.Id == 'new_whale').mean()","00277469":"(df.Id.value_counts() == 1).mean()","48cd4a66":"df.Id.nunique()","a877914e":"df.shape","abe06a71":"fn2label = {row[1].Image: row[1].Id for row in df.iterrows()}","d63fccc3":"SZ = 224\nBS = 64\nNUM_WORKERS = 0\nSEED=0","565c132d":"#tfms = get_transforms(do_flip=False)","2f291df5":"#data = ImageDataBunch.from_df(path_train, df, ds_tfms=tfms, size=150,num_workers=0)","b15ec62b":"data = (\n    ImageItemList\n        .from_folder('..\/input\/train')\n        .random_split_by_pct(seed=SEED)\n        .label_from_func(lambda path: fn2label[path.name])\n        .add_test(ImageItemList.from_folder('..\/input\/test'))\n        .transform(get_transforms(do_flip=False, max_zoom=1, max_warp=0, max_rotate=2), size=SZ, resize_method=ResizeMethod.SQUISH)\n        .databunch(bs=BS, num_workers=NUM_WORKERS, path='..\/input\/train')\n)","e648f3a3":"data.show_batch(rows=3)","6cee5f42":"name = f'res50-{SZ}'","7aafbd8b":"!git clone https:\/\/github.com\/radekosmulski\/whale","a53e5548":"import sys\n # Add directory holding utility functions to path to allow importing utility funcitons\n#sys.path.insert(0, '\/kaggle\/working\/protein-atlas-fastai')\nsys.path.append('\/kaggle\/working\/whale')","b76199cc":"from whale.utils import map5","d062ca97":"MODEL_PATH = \"\/tmp\/model\/\"","e7861e3f":"learn = create_cnn(data, models.resnet50, metrics=[accuracy, map5], model_dir=MODEL_PATH)","686a4a79":"learn.fit_one_cycle(2)","9b4017b2":"learn.recorder.plot_losses()","98c71538":"learn.save(f'{name}-stage-1')","f383be3f":"learn.unfreeze()","44779a9a":"learn.lr_find()","68cb96ef":"learn.recorder.plot()","0f7afd0a":"max_lr = 1e-4\nlrs = [max_lr\/100, max_lr\/10, max_lr]","77237c5d":"learn.fit_one_cycle(5, lrs)","5a015aa6":"learn.save(f'{name}-stage-2')","a35a27b7":"learn.recorder.plot_losses()","aeebe663":"preds, _ = learn.get_preds(DatasetType.Test)","e147ec6a":"from whale.utils import *","2657d79a":"def create_submission(preds, data, name, classes=None):\n    if not classes: classes = data.classes\n    sub = pd.DataFrame({'Image': [path.name for path in data.test_ds.x.items]})\n    sub['Id'] = top_5_pred_labels(preds, classes)\n    sub.to_csv(f'{name}.csv', index=False) # compression='gzip'","472c0c1b":"create_submission(preds, learn.data, name)","59636383":"pd.read_csv(f'{name}.csv').head()","95837c43":"!rm -rf \/kaggle\/working\/whale","6c798849":"#!kaggle competitions submit -c humpback-whale-identification -f {name}.csv.gz -m \"{name}\"","b6e6e6d9":"#!kaggle competitions submit -c humpback-whale-identification -f {name}.csv -m \"{name}\"","ea3d5f52":"https:\/\/github.com\/radekosmulski\/whale","d3af1703":"## Train","71e8980a":"## Predict","d92b6931":"This is not a loss plot you would normally expect to see. Why does it look like this? Let's consider what images appear in the validation set:\n * images of whales that do not appear in the train set (whales where all their images were randomly assigned to the validation set) - there is nothing our model can learn about these!\n * images of whales with multiple images in the dataset where some subset of those got assigned to the validation set\n * `new_whale` images\n \nIntuitively, a model such as the above does not seem to frame the problem in a way that would be easy for a neural network to solve. Nonetheless, it is interesting to think how we could improve on the construction of the validation set? What tweaks could be made to the model to improve its performance?","ddbcd3d9":"## A look at the data","b227cf85":"41% of all whales have only a single image associated with them.\n\n38% of all images contain a new whale - a whale that has not been identified as one of the known whales.\n\nThere is a superb writeup on what a solution to this problem might look like [here](https:\/\/www.kaggle.com\/martinpiotte\/whale-recognition-model-with-score-0-78563\/notebook). In general, the conversation in the Kaggle [forum](https:\/\/www.kaggle.com\/c\/humpback-whale-identification\/discussion) also seems to have some very informative threads.\n\nEither way, starting with a simple model that can be hacked together in a couple of lines of code is a recommended approach. It is good to have a baseline to build on - going for a complex model from start is a way for dying a thousand deaths by subtle bugs."}}