{"cell_type":{"0437c5e2":"code","99e64a10":"code","d7f054d8":"code","61bac22e":"code","02f049f5":"code","f1a42a0d":"code","9b8e2149":"code","cdbcdc7a":"code","43f5c5db":"code","bfc8e725":"code","78eb3fc9":"code","de18441a":"code","41f691bc":"code","079799bb":"markdown","6a5af620":"markdown","0f2f9996":"markdown","80f3d14b":"markdown","5d84feb6":"markdown"},"source":{"0437c5e2":"import sqlite3\nimport re\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom nltk import pos_tag\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer \nfrom matplotlib import pyplot as plt\nfrom collections import defaultdict\nplt.style.use('fivethirtyeight') \n%matplotlib inline\n\nimport plotly.offline as py\nimport plotly.graph_objs as go\npy.init_notebook_mode(connected=True)\nconn = sqlite3.connect('..\/input\/database.sqlite')","99e64a10":"def remove_special_char(text):\n    #replace special characters with ''\n    text = re.sub('[^\\w\\s]', '', text)\n    #remove chinese character\n    text = re.sub(r'[^\\x00-\\x7f]',r'', text)\n    #remove numbers\n    text = re.sub('\\d+', '', text)\n    text = re.sub('_', '', text)\n    text = re.sub('\\s+', ' ', text)\n    text = text.strip()\n    text = text.lower()\n    return text\n\ndef remove_stopwords(text):\n    stop_words = stopwords.words('english')\n    text = [word for word in text.split() if word not in stop_words]\n    return \" \".join(text)\n\ndef stem_text(text):\n    stemmer = PorterStemmer()\n    text = [stemmer.stem(word) for word in text.split()]\n    return \" \".join(text)\n\ndef compute_sentiment(comment):\n    sid = SentimentIntensityAnalyzer()\n    ss = sid.polarity_scores(comment)  \n    series = pd.Series({'positive': ss['pos'], 'negative': ss['neg'], 'neutral': ss['neu'], 'compound': ss['compound']})\n    return series\n\ndef drop_non_negative(series):\n    if series.loc['negative'] == 0:\n        series.loc['body'] = np.nan\n    return series\n\ndef trim_df(df, subfield, mini):\n    trim_df = pd.DataFrame()\n    for subfield, data in df.groupby(df[subfield]):\n        data = data.iloc[:mini]\n        trim_df = pd.concat([trim_df, data])\n    return trim_df\n\ndef batch_query(subreddits):\n    complete_df = pd.DataFrame()\n    comment_size = []\n    for subreddit in subreddits:\n        query = \"SELECT subreddit, author, score, controversiality, body FROM May2015 \\\n        WHERE subreddit = '{}' LIMIT 10000\".format(subreddit)\n        df = pd.read_sql(query, conn)\n        df['body'] = df['body'].apply(lambda x: remove_special_char(x))\n        df['body'] = df['body'].apply(lambda x: remove_stopwords(x))\n        df['body'].replace('', np.nan, inplace=True)\n        df.dropna(subset=['body'], inplace=True)\n        df = df.apply(lambda x: pd.concat([x, compute_sentiment(x['body'])]), axis=1)\n        df = df.apply(drop_non_negative, axis=1) \n        df.dropna(subset=['body'], inplace=True)\n        comment_size.append(df['body'].size)\n        complete_df = pd.concat([complete_df, df])\n   \n    complete_df = trim_df(complete_df, 'subreddit', min(comment_size))\n    return complete_df","d7f054d8":"# Query and clean our data in one call\nsports_subreddits = ['nba', 'nfl', 'soccer', 'hockey', 'baseball']\ndf = batch_query(sports_subreddits) ","61bac22e":"display(df.groupby(['subreddit']).mean())","02f049f5":"grouped = df.groupby(['subreddit'])\nfor name, group in grouped:\n    group.hist(column=['negative'], figsize=(5,5), bins=500)\n    plt.suptitle(f'{name}', fontsize=20)    ","f1a42a0d":"for name, group in grouped:\n    pplt = sns.pairplot(group[['negative', 'score']])\n    pplt.fig.suptitle(f'{name}', y=1.08 )","9b8e2149":"# Batches all comments together along with a dict that maps\n# subreddit name to comment arr inx \ndef batch_text(g_df, subfield):\n    doc_to_idx = defaultdict(int)\n    documents = []\n    for idx, (subfield, data) in enumerate(g_df.groupby(g_df[subfield])):\n        documents.append(data['body'].values.tolist())\n        doc_to_idx[subfield] = idx\n    return documents, doc_to_idx\n\ndef get_word_frequencies(document, words, ngram):\n    cvec = CountVectorizer(ngram_range=ngram, max_df=0.75, min_df=1, stop_words='english') \n    cvec_term_doc = cvec.fit_transform(document)\n    occ = np.asarray(cvec_term_doc.sum(axis=0)).ravel().tolist()\n    counts_df = pd.DataFrame({'term': cvec.get_feature_names(), 'occurrences': occ})\n    word_freq = defaultdict(int)\n    for word in words:\n        amount = counts_df.loc[counts_df['term'] == word].occurrences.item()\n        word_freq[word] = amount\n    return word_freq\n    \ndef get_tfidf_keywords(documents, document, topn=10, ngram=(1, 2)):\n    flat_documents = [word for document in documents for word in document]\n    cvec = CountVectorizer(ngram_range=ngram, max_df=0.75, min_df=1, stop_words='english') \n    cvec_term_doc = cvec.fit_transform(flat_documents)\n    tvec = TfidfTransformer(smooth_idf=True, norm = None, use_idf=True)\n    feature_names = cvec.get_feature_names()\n    tvec.fit(cvec_term_doc)\n    tfidf = tvec.transform(cvec.transform(documents[document]), copy=False)\n    sorted_items = sort_coo(tfidf.tocoo())\n    keywords = extract_topn_from_vector(feature_names, sorted_items, topn)\n    return keywords\n\n# Sort sparse matrix \ndef sort_coo(coo_matrix):\n    tuples = zip(coo_matrix.col, coo_matrix.data)\n    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n\ndef extract_topn_from_vector(feature_names, sorted_items, topn):\n    sorted_items = sorted_items[:topn]\n    score_vals = []\n    feature_vals = []\n    for idx, score in sorted_items:\n        score_vals.append(round(score, 3))\n        feature_vals.append(feature_names[idx])\n    results= {}\n    for idx in range(len(feature_vals)):\n        results[feature_vals[idx]]=score_vals[idx]\n    return results    \n\ndef append_keyword_data(keywords, frequency):\n    for key in keywords.keys():\n        keywords[key] = [keywords[key], frequency[key]]\n    return keywords","cdbcdc7a":"# Calls our other tfidf methods and returns our top n words\n# with both the occurences and tf-idf scores\ndef gen_tfidf_results(df, sport, nwords, ngram):\n    documents, document_map = batch_text(df, 'subreddit')\n    keywords = get_tfidf_keywords(documents, document_map[sport], nwords, ngram)\n    words = keywords.keys()\n    word_freq = get_word_frequencies(documents[document_map[sport]], words, ngram)\n    tfidf_freq = append_keyword_data(keywords, word_freq)\n    return tfidf_freq\n\n# Returns the results df and the subjects, words with 'n' POS tag\ndef organize_results(tfidf):\n    terms = tfidf.keys()\n    tf_score = [tfidf[key][0] for key in terms]\n    frequency = [tfidf[key][1] for key in terms]\n    dict = {\n        'term': list(terms),\n        'tfidf score': tf_score,\n        'freq': frequency\n    }\n    results = pd.DataFrame(dict)\n    # Join all ngrams and then split into individual strings\n    term_list =  ' '.join(list(terms)).split(' ')\n    tagged = pos_tag(term_list)\n    # Extract words tagged as N for noun\n    subjects = set([w[0] for w in tagged if w[1][0] == 'N'])\n    \n    return results, subjects","43f5c5db":"sport = 'nba'\nnwords = 100\nnba_tfidf = gen_tfidf_results(df, sport, nwords, (1, 3))\nnba_df, subjects = organize_results(nba_tfidf)\nsub_df = nba_df.loc[nba_df['term'].isin(list(subjects))].reset_index()\n\ntrace = go.Scatter(\n    x = nba_df['tfidf score'],\n    y = nba_df['freq'],\n    mode = 'markers',\n    text = nba_df['term']\n)\n\nlayout= go.Layout(\n    title= f'{sport} Subreddit TF-IDF Scores and Term Frequency',\n    hovermode= 'closest',\n    xaxis= dict(\n        title= 'tf-idf score',\n        ticklen= 5,\n        gridwidth= 2,\n    ),\n    yaxis=dict(\n        title= 'Frequency',\n        ticklen= 5,\n        gridwidth= 2,\n    ),\n    showlegend= False\n)\n\ndata = [trace]\nfig = dict(data=data, layout=layout)\npy.iplot(fig)\n\npd.set_option('display.max_columns', 150)\nprint(f'TF-IDF Scores for Top {nwords} N-Grams')\ndisplay(nba_df.transpose())\nprint('\\n\\nTF-IDF Scores for Subject Words')\ndisplay(sub_df.transpose())","bfc8e725":"sport = 'nfl'\nnwords = 100\nnfl_tfidf = gen_tfidf_results(df, sport, nwords, (1, 3))\nnfl_df, subjects = organize_results(nfl_tfidf)\nsub_df = nfl_df.loc[nfl_df['term'].isin(list(subjects))].reset_index()\n\ntrace = go.Scatter(\n    x = nfl_df['tfidf score'],\n    y = nfl_df['freq'],\n    mode = 'markers',\n    text = nfl_df['term']\n)\n\nlayout= go.Layout(\n    title= f'{sport} Subreddit TF-IDF Scores and Term Frequency',\n    hovermode= 'closest',\n    xaxis= dict(\n        title= 'tf-idf score',\n        ticklen= 5,\n        gridwidth= 2,\n    ),\n    yaxis=dict(\n        title= 'Frequency',\n        ticklen= 5,\n        gridwidth= 2,\n    ),\n    showlegend= False\n)\n\ndata = [trace]\nfig = dict(data=data, layout=layout)\npy.iplot(fig)\n\npd.set_option('display.max_columns', 150)\nprint(f'TF-IDF Scores for Top {nwords} N-Grams')\ndisplay(nfl_df.transpose())\nprint('\\n\\nTF-IDF Scores for Subject Words')\ndisplay(sub_df.transpose())","78eb3fc9":"sport = 'baseball'\nnwords = 100\nbaseball_tfidf = gen_tfidf_results(df, sport, nwords, (1, 3))\nbaseball_df, subjects = organize_results(baseball_tfidf)\nsub_df = baseball_df.loc[baseball_df['term'].isin(list(subjects))].reset_index()\n\ntrace = go.Scatter(\n    x = baseball_df['tfidf score'],\n    y = baseball_df['freq'],\n    mode = 'markers',\n    text = baseball_df['term']\n)\n\nlayout= go.Layout(\n    title= f'{sport} Subreddit TF-IDF Scores and Term Frequency',\n    hovermode= 'closest',\n    xaxis= dict(\n        title= 'tf-idf score',\n        ticklen= 5,\n        gridwidth= 2,\n    ),\n    yaxis=dict(\n        title= 'Frequency',\n        ticklen= 5,\n        gridwidth= 2,\n    ),\n    showlegend= False\n)\n\ndata = [trace]\nfig = dict(data=data, layout=layout)\npy.iplot(fig)\n\npd.set_option('display.max_columns', 150)\nprint(f'TF-IDF Scores for Top {nwords} N-Grams')\ndisplay(baseball_df.transpose())\nprint('\\n\\nTF-IDF Scores for Subject Words')\ndisplay(sub_df.transpose())","de18441a":"sport = 'hockey'\nnwords = 100\nhockey_tfidf = gen_tfidf_results(df, sport, nwords, (1, 3))\nhockey_df, subjects = organize_results(hockey_tfidf)\nsub_df = hockey_df.loc[hockey_df['term'].isin(list(subjects))].reset_index()\n\ntrace = go.Scatter(\n    x = hockey_df['tfidf score'],\n    y = hockey_df['freq'],\n    mode = 'markers',\n    text = hockey_df['term']\n)\n\nlayout= go.Layout(\n    title= f'{sport} Subreddit TF-IDF Scores and Term Frequency',\n    hovermode= 'closest',\n    xaxis= dict(\n        title= 'tf-idf score',\n        ticklen= 5,\n        gridwidth= 2,\n    ),\n    yaxis=dict(\n        title= 'Frequency',\n        ticklen= 5,\n        gridwidth= 2,\n    ),\n    showlegend= False\n)\n\ndata = [trace]\nfig = dict(data=data, layout=layout)\npy.iplot(fig)\n\npd.set_option('display.max_columns', 150)\nprint(f'TF-IDF Scores for Top {nwords} N-Grams')\ndisplay(hockey_df.transpose())\nprint('\\n\\nTF-IDF Scores for Subject Words')\ndisplay(sub_df.transpose())","41f691bc":"sport = 'soccer'\nnwords = 100\nsoccer_tfidf = gen_tfidf_results(df, sport, nwords, (1, 3))\nsoccer_df, subjects = organize_results(soccer_tfidf)\nsub_df = soccer_df.loc[soccer_df['term'].isin(list(subjects))].reset_index()\n\ntrace = go.Scatter(\n    x = soccer_df['tfidf score'],\n    y = soccer_df['freq'],\n    mode = 'markers',\n    text = soccer_df['term']\n)\n\nlayout= go.Layout(\n    title= f'{sport} Subreddit TF-IDF Scores and Term Frequency',\n    hovermode= 'closest',\n    xaxis= dict(\n        title= 'tf-idf score',\n        ticklen= 5,\n        gridwidth= 2,\n    ),\n    yaxis=dict(\n        title= 'Frequency',\n        ticklen= 5,\n        gridwidth= 2,\n    ),\n    showlegend= False\n)\n\ndata = [trace]\nfig = dict(data=data, layout=layout)\npy.iplot(fig)\n\npd.set_option('display.max_columns', 150)\nprint(f'TF-IDF Scores for Top {nwords} N-Grams')\ndisplay(soccer_df.transpose())\nprint('\\n\\nTF-IDF Scores for Subject Words')\ndisplay(sub_df.transpose())","079799bb":"<a id='analysis'><\/a>\n# NLP Analysis\n---\nWe will be using a variety of techniques to try to describe and understand what people fight about in reddit sports subreddits. First, we will perform tf-idf with several ngram levels. Tf-idf will return a score for each ngram that measures their relative use and importance in the subreddit. From these lists we can extract the top 'n' phrases and the number of occurences for each of those phrases. We want not only to understand the kind are keywords that exist among our negative sentiment comments, but what kind of adjectives and phrases apper around them. Next, we will use POS or part of speech tagging to identify subject nouns, which we will look at seperately. Last, we will visualize these keywords and phrases with Plotly \ufffcin an interactive chart that lets us look for interesting data points. ","6a5af620":"<a id='conclusion'><\/a>\n# Conclusion\n---\nThe results were really fun to look at, and I think this is just the beginning of a much larger look at how conflict happens in subreddits and how we could use those patterns in the creation of bots or moderation. With a little bit of domain knowldge we can look at the dataframes of subjects and tell what were the topics most fought over during May 2015. For the 'nfl' subreddit they were Dan Snyder, Lions, Bengals, Cooper, Gruden, Brees, Eagles, Steelers, and Collins. In the 'nba' users were fighting about mj, rodman, chandler, conley, parsons, bulls, dame, and noel. The use of plot.ly graphs make it really easy to peruse the results. If you liked this analysis add in the comments what you would do next! I think there's a lot of ways you could build on this analysis. ","0f2f9996":" ---\n# Conflict in Sports Subreddits\n**[Nicholas Holloway](https:\/\/github.com\/nholloway)**\n\n---\n### Mission \nWe are going to investigate what people fight about in sports subreddits. Our goal is to show how a variety of NLP techniques can be used to parse and analyze our text data. We will be using **tf-idf**, **pos tagging**, **sentiment analysis**, several preprocessing techniques, and plotly graphs to explore our results. \n\n### Table of Contents\n1. [Text Preprocessing](#preprocess)\n1. [Data Exploration](#explore)\n1. [NLP Analysis](#analysis)\n3. [Conclusion](#conclusion)","80f3d14b":"<a id='explore'><\/a>\n# Data Exploration\n---\nAt the subreddit level we are trying to get an idea about how negativity and score are distributed among negative comments. There is quite a bit of data exploration we could do- especially with some feature engineering. We could try clustering together like comments, use a measure like WMD (word movers distance) to see how similar some comments are, or use part of speech tools to take apart comments that fall into outlier categories. I messed around with a few approaches, looking at likelihood for words to occur in negative comments and some of this will be used in our final analysis but for the data exploration shown I just want to present a high level description of each subreddit's comments. ","5d84feb6":"<a id='preprocess'><\/a>\n# Text Preprocessing\n---\n\nThere is a substantial amount of text preprocessing for this kernel. We will:\n1. Remove special characters\n2. Remove stopwords\n3. Compute the sentiment score for each comment\n4. Drop all columns that have no negative sentiment\n5. Drop all rows with NaN\n6. Trim each query batch so that we have equal sizes for each sport. \n    \nThere are many ways we can text preprocess, NLTK has its own tools- some of which we will use. Spacy is great, especially when we are using other Spacy functions. Spacy wraps text in its own object for processing, which makes it great for building pipelines but adds complexity for small batch analysis. For this kernel we will write our own- which is a good opportunity to understand our text and extra processing steps we might need- like removing chinese characters, for example. "}}