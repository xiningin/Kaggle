{"cell_type":{"78616d71":"code","45ad2f21":"code","d8ecb9b7":"code","09aa1b9c":"code","9b6ddeb3":"code","d29a4aed":"code","30bff1a7":"code","c1b6b6c2":"code","d6a762b4":"code","15e4772e":"code","e4cd09f6":"code","661e831a":"code","606f121e":"code","db83648a":"code","0f4a2174":"code","0bc1bd5f":"code","ebd63b1b":"code","03b4116d":"code","62489ceb":"code","e2172050":"code","6bf677f5":"code","b5eb410b":"code","34ffea07":"markdown"},"source":{"78616d71":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","45ad2f21":"data = pd.read_csv('\/kaggle\/input\/fer2013\/fer2013.csv')\ndata.head(3)","d8ecb9b7":"data.Usage.unique()","09aa1b9c":"data = data.drop(['Usage'], axis=1)\ndata.shape","9b6ddeb3":"from sklearn.model_selection import train_test_split","d29a4aed":"train = data['pixels']\ntest = data['emotion']","30bff1a7":"X_train, X_test, y_train, y_test = train_test_split(train, test, test_size=0.1, random_state=10)","c1b6b6c2":"print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","d6a762b4":"width, height = 48, 48\n\nX_train1 = []\nfor i in X_train:\n    X_train1.append([int(p) for p in i.split()])\nX_train1 = np.array(X_train1)\/255.\nX_train1.shape","15e4772e":"X_test1 = []\nfor i in X_test:\n    X_test1.append([int(p) for p in i.split()])\nX_test1 = np.array(X_test1)\/255.\nX_test1.shape","e4cd09f6":"X_test1[:2]","661e831a":"X_train1 = X_train1.reshape(X_train1.shape[0], 48, 48, 1)\n\nX_train1.shape","606f121e":"X_test1 = X_test1.reshape(X_test1.shape[0], 48, 48, 1)\n\nX_test1.shape","db83648a":"X_train32 = X_train1.astype('float32')\nX_test32 = X_test1.astype('float32')\n\nX_train32.dtype, X_test32.dtype","0f4a2174":"import tensorflow as tf\nfrom tensorflow import keras\nfrom keras.layers import Conv2D, Flatten, Dense, Activation, Dropout, MaxPooling2D, BatchNormalization","0bc1bd5f":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D, Activation, BatchNormalization","ebd63b1b":"model = Sequential()\nmodel.add(Conv2D(filters = 64, kernel_size = (5,5), input_shape = (48, 48, 1), padding='same'))\nmodel.add(Conv2D(64, kernel_size=(5,5), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Activation('relu'))\n\nmodel.add(Conv2D(filters=128, kernel_size=(5,5), padding='same'))\nmodel.add(Conv2D(128, kernel_size=(5,5), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Activation('relu'))\n\nmodel.add(Conv2D(filters=256, kernel_size=(3,3), padding='same'))\nmodel.add(Conv2D(256, kernel_size=(3,3), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Activation('relu'))\n\nmodel.add(Flatten())\nmodel.add(Dense(128))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(rate=0.25))\nmodel.add(Dense(7, activation='softmax'))\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","03b4116d":"model.summary()","62489ceb":"from tensorflow.keras.utils import to_categorical\nn_epochs = 20\nbatch_size = 64\nlr = 0.0001","e2172050":"history = model.fit(X_train32, to_categorical(y_train), \n                    batch_size = batch_size, epochs = n_epochs, validation_data= (X_test32, to_categorical(y_test)))","6bf677f5":"from keras.models import model_from_json\nimport numpy\nimport os\n\n# serialize model to JSON\nmodel_json = model.to_json()\nwith open(\"model.json\", \"w\") as json_file:\n    json_file.write(model_json)\n# serialize weights to HDF5\nmodel.save_weights(\"model.h5\")\nprint(\"Saved model to disk\")","b5eb410b":"pd.DataFrame(history.history).tail()","34ffea07":"# Start to build model and train"}}