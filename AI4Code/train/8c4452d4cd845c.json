{"cell_type":{"c973809e":"code","d586319b":"code","6a5d74f3":"code","249e542f":"code","79bbffbb":"code","4e016a43":"code","6ba22f4a":"code","d323cce2":"code","f20ac5a1":"code","b3fbc8c5":"code","4a89a77d":"code","8b83bc84":"code","fd02265f":"code","65aab1d5":"code","d4594347":"code","fa072c77":"code","59d3161c":"code","ed155892":"code","d4be92a9":"code","3faf9be8":"code","132af22e":"code","861b6d05":"code","452dc313":"code","b7fe4433":"code","771f36bc":"code","ba1e1f7d":"code","0764a254":"code","96baef5a":"code","7f3411ae":"code","a3acdd05":"code","5decd217":"code","08e338a0":"code","e8ac3538":"code","ef2cad8b":"code","32c5d906":"code","1c548827":"code","05dc0d6a":"code","d6c30b36":"code","29751302":"code","9963c594":"code","1c2ee9bf":"code","891ef8ab":"code","5ccf23d7":"code","31756b89":"code","f3f0dcc4":"code","c0cae7ce":"code","fb875340":"code","c76fc3d8":"code","2415fead":"code","219ac72a":"code","019d9b52":"code","c838ff40":"code","a8c1626b":"code","cfc0a713":"code","4a8900cf":"code","f288040d":"code","e4ef3674":"code","cfaa9fb2":"code","cab90ad1":"code","ad7af202":"code","e3e2a90c":"code","a763b22b":"code","5c092e8c":"code","81a02ed6":"code","37baf34d":"code","3103ac60":"code","649a02e9":"code","a8ff82fd":"code","4b99d44a":"code","ef5ac037":"code","c2ca791d":"code","65a6ea9e":"code","31d8a734":"code","d197690e":"code","133a78fc":"code","4f290f87":"code","e023e16b":"code","af682c2c":"code","68d1fdad":"code","b5829f73":"code","a1214cd1":"code","46acbb37":"code","64336bac":"code","4000808e":"code","ce3a1464":"code","30dcc135":"code","0038d1ce":"code","d5f414f6":"code","e0e55c5d":"code","7fdc66bb":"code","63311080":"code","9a5d9ad2":"code","85c4bdbf":"code","036b2400":"code","917c4585":"code","307aa651":"code","37f9b386":"code","ca167759":"code","6cf29aa4":"code","cfb91025":"code","e52fe71d":"code","0575fe92":"code","375521d2":"code","7b055bec":"code","f8fa74d9":"code","5d99ba19":"code","d621526c":"code","9c307821":"code","bd5496e2":"code","28ab7449":"code","b380af74":"code","1a3c4f2f":"code","feec6b68":"code","e1add497":"code","85045985":"code","769bca67":"code","d64a9d2f":"code","a7d3c38b":"code","6244ef28":"markdown","00a3fdb9":"markdown","f7221836":"markdown","4ccf717f":"markdown","655c374d":"markdown","ccd82d34":"markdown","944d3e75":"markdown","7d72d423":"markdown","33cc626e":"markdown","5de03b77":"markdown","0b37a428":"markdown","2b7a9190":"markdown","bd68346c":"markdown","c54b781e":"markdown"},"source":{"c973809e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d586319b":"# Import related libraries\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport gc\nimport psutil\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import LabelEncoder,StandardScaler\nfrom sklearn.decomposition import PCA, KernelPCA\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import (confusion_matrix,accuracy_score,precision_score,recall_score,f1_score,\n                             make_scorer,classification_report,roc_auc_score,roc_curve,\n                             average_precision_score,precision_recall_curve)\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,VotingClassifier\n\nfrom imblearn.pipeline import make_pipeline\nfrom imblearn.under_sampling import OneSidedSelection\nfrom imblearn.under_sampling import NearMiss\nfrom imblearn.under_sampling import EditedNearestNeighbours\nfrom imblearn.under_sampling import TomekLinks\nfrom imblearn.under_sampling import RandomUnderSampler\n\npd.set_option('display.max_columns', None)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nRANDOM_SEED = 101\n\nimport collections\nfrom mpl_toolkits import mplot3d","6a5d74f3":"sub_file = pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")\nsub_file.head()","249e542f":"train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain.head()","79bbffbb":"val = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\nval.head()","4e016a43":"train.columns","6ba22f4a":"val.columns","d323cce2":"train['Survived'].value_counts().plot.bar()","f20ac5a1":"train.info()","b3fbc8c5":"train.isnull().mean()","4a89a77d":"train.shape","8b83bc84":"train.describe()","fd02265f":"target = 'Survived'","65aab1d5":"\"Braund, Mr. Owen Harris\".split(',')[1].split()[0][:-1]","d4594347":"train[\"Name\"].map(lambda x: x.split(',')[1].split()[0][:-1]).value_counts().plot.bar()","fa072c77":"val[\"Name\"].map(lambda x: x.split(',')[1].split()[0][:-1]).value_counts().plot.bar()","59d3161c":"def get_salutation_map(df,var,rare):\n    sal_dict = {}\n    for sal, count in df[var].value_counts().to_dict().items():\n        count = int(count)\n        if count < 10:\n            sal_dict[sal] = rare\n        else:\n            sal_dict[sal] = sal\n    return sal_dict","ed155892":"train[\"Salutation\"] = train[\"Name\"].map(lambda x: x.split(',')[1].split()[0][:-1])","d4be92a9":"train.head()","3faf9be8":"get_salutation_map(train,\"Salutation\",\"Rare\")","132af22e":"train[\"Salutation\"] = train[\"Name\"].map(lambda x: x.split(',')[1].split()[0][:-1])\ntrain[\"Salutation\"] = train[\"Salutation\"].map(get_salutation_map(train,'Salutation','Rare'))\ntrain.head(2)","861b6d05":"train[\"Salutation\"].value_counts().plot.bar()","452dc313":"sns.countplot(x=\"Salutation\",data=train)","b7fe4433":"train.head()","771f36bc":"sns.boxplot(y = 'Age',\n            x = 'Salutation', \n            data = train)\nplt.xlabel('Saluation')\nplt.ylabel('Age')\nplt.title('Distribution of Age with respect to Saluations', fontsize = 10)","ba1e1f7d":"sns.boxplot(y = 'Fare',\n            x = 'Salutation', \n            data = train)\nplt.xlabel('Saluation')\nplt.ylabel('Fare')\nplt.title('Distribution of Fare with respect to Saluations', fontsize = 10)","0764a254":"train['SibSp'].unique()","96baef5a":"train['SibSp'].nunique()","7f3411ae":"train['SibSp'].value_counts().plot.bar()","a3acdd05":"train['Parch'].unique()","5decd217":"train['Parch'].value_counts().plot.bar()","08e338a0":"train[\"Family_Size\"] = train[\"SibSp\"] + train[\"Parch\"]\ntrain[\"Family_Size\"].unique()","e8ac3538":"(train[\"Family_Size\"].value_counts(normalize=True)*100).plot.bar()","ef2cad8b":"train[\"Family_Size\"].value_counts(normalize=True)*100","32c5d906":"def get_family_size_map(df,var):\n    fam_dict = {}\n    for size, pct in (df[var].value_counts(normalize=True)*100).to_dict().items():\n        if size == 0:\n            fam_dict[size] = \"Alone\"\n        elif (size != 0) & (pct > 10.0):\n            fam_dict[size] = \"Small\"\n        else:\n            fam_dict[size] = \"Large\"\n    return fam_dict","1c548827":"train[\"Family_Size\"] = train[\"Family_Size\"].map(get_family_size_map(train,'Family_Size'))\ntrain.head(2)","05dc0d6a":"sns.boxplot(y = 'Age',\n            x = 'Family_Size', \n            data = train)\nplt.xlabel('Family_Size')\nplt.ylabel('Age')\nplt.title('Distribution of Age with respect to Family_Size', fontsize = 10)","d6c30b36":"sns.boxplot(y = 'Age',\n            x = 'Family_Size', \n            hue = 'Pclass',\n            data = train)\nplt.xlabel('Family_Size')\nplt.ylabel('Age')\nplt.title('Distribution of Age with respect to Family_Size', fontsize = 10)","29751302":"sns.boxplot(y = 'Fare',\n            x = 'Family_Size', \n            data = train)\nplt.xlabel('Family_Size')\nplt.ylabel('Fare')\nplt.title('Distribution of Fare with respect to Family_Size', fontsize = 10)","9963c594":"train.isnull().mean()","1c2ee9bf":"train['had_Cabin'] = np.where(train['Cabin'].isna(),0,1)","891ef8ab":"train.head()","5ccf23d7":"train['Cabin'].dropna().map(lambda x:x[0]).value_counts()","31756b89":"train['Cabin'] = train['Cabin'].fillna(\"M\")\ntrain['Cabin'] = train['Cabin'].map(lambda x: x[0])","f3f0dcc4":"train.head()","c0cae7ce":"sns.boxplot(y = 'Age',\n            x = 'Cabin',\n            data = train)\nplt.xlabel('Cabin')\nplt.ylabel('Age')\nplt.title('Distribution of Age with respect to Cabin', fontsize = 10)","fb875340":"sns.boxplot(y = 'Cabin',\n            x = 'Fare',\n            data = train)\nplt.xlabel('had_Cabin')\nplt.ylabel('Fare')\nplt.title('Distribution of Fare with respect to had_Cabin', fontsize = 10)","c76fc3d8":"train.head()","2415fead":"train.groupby(['Salutation','had_Cabin'])","219ac72a":"mean_dict = {}\nfor k, df in train.groupby(['Salutation','Family_Size','had_Cabin']):\n    if df['Age'].isnull().sum() != 0:\n        mean_dict[k] = df[\"Age\"].mean()\nmean_dict","019d9b52":"for k,v in mean_dict.items():\n    train.loc[(train[\"Salutation\"] == k[0]) & (train[\"Family_Size\"] == k[1]) & (train[\"had_Cabin\"] == k[2]) & (train[\"Age\"].isna()), \"Age\"] = v","c838ff40":"train['Embarked'].value_counts()","a8c1626b":"train['Embarked'] = train['Embarked'].fillna(train['Embarked'].mode().values[0])","cfc0a713":"train.isnull().sum()","4a8900cf":"train.head()","f288040d":"num_cols = ['Age','Fare']\ncat_cols = ['Pclass','Sex','Embarked','Cabin','had_Cabin','Salutation','Family_Size']","e4ef3674":"for col in num_cols:\n    fig = plt.figure(figsize = (10,5))\n    ax = fig.add_subplot(111)\n    ax = sns.distplot(train[col], color=\"m\", label=\"Skewness : %.2f\"%(train[col].skew()))\n    ax.set_xlabel(col)\n    ax.set_ylabel(\"Frequency\")\n    ax.legend(loc='best')\n    ax.set_title('Frequency Distribution of {}'.format(col), fontsize = 15)","cfaa9fb2":"fig = plt.figure(figsize = (50,15))\nj = 1\nfor cat_col in cat_cols:\n    ax = fig.add_subplot(1,len(cat_cols),j)\n    sns.countplot(x = cat_col,\n                  data = train,\n                  ax = ax)\n    ax.set_xlabel(cat_col)\n    ax.set_ylabel(\"Frequency\")\n    ax.set_title('Frequency Distribution for individual classes in {}'.format(cat_col), fontsize = 10)\n    j = j + 1","cab90ad1":"sns.pairplot(train[num_cols])","ad7af202":"for col in num_cols:\n    fig = plt.figure(figsize = (15,4))\n    ax = fig.add_subplot(111)\n    j = 0\n    for key, df in train.groupby([target]):\n        ax = sns.kdeplot(df[col], shade = True, label=key)\n        ax.set_xlabel(col)\n        ax.set_ylabel(\"Frequency\")\n        ax.legend(loc=\"best\")\n        fig.suptitle('Frequency Distribution of {}'.format(col), fontsize = 10)\n        j = j + 1","e3e2a90c":"for col in num_cols:\n    fig = plt.figure(figsize = (15,4))\n    j = 1\n    for key, df in train.groupby([target]):\n        ax = fig.add_subplot(1,train[target].nunique(),j)\n        ax = sns.distplot(df[col], label=\"Skewness : %.2f\"%(df[col].skew()))\n        ax.set_xlabel(key)\n        ax.set_ylabel(\"Frequency\")\n        ax.legend(loc=\"best\")\n        fig.suptitle('Frequency Distribution of {}'.format(col), fontsize = 10)\n        j = j + 1","a763b22b":"for num_col in num_cols:\n    fig = plt.figure(figsize = (30,10))\n    j = 1\n    for cat_col in cat_cols:\n        ax = fig.add_subplot(1,len(cat_cols),j)\n        sns.boxplot(y = train[num_col],\n                    x = train[cat_col], \n                    data = train, \n                    ax = ax)\n        ax.set_xlabel(cat_col)\n        ax.set_ylabel(num_col)\n        ax.set_title('Distribution of {} with respect to {}'.format(num_col,cat_col), fontsize = 10)\n        j = j + 1","5c092e8c":"for num_col in num_cols:\n    fig = plt.figure(figsize = (30,10))\n    j = 1\n    for cat_col in cat_cols:\n        ax = fig.add_subplot(1,len(cat_cols),j)\n        sns.boxplot(y = train[num_col],\n                    x = train[cat_col],\n                    hue = target,\n                    data = train, \n                    ax = ax)\n        ax.set_xlabel(cat_col)\n        ax.set_ylabel(num_col)\n        ax.set_title('Distribution of {} with respect to {}'.format(num_col,cat_col), fontsize = 10)\n        j = j + 1","81a02ed6":"train_data = pd.get_dummies(train,columns=cat_cols,drop_first=True)\ntrain_data.head(2)","37baf34d":"explore_data, validation_data = train_test_split(train_data, test_size = 0.2, random_state=RANDOM_SEED, stratify=train[target])","3103ac60":"train_data, test_data = train_test_split(explore_data, test_size = 0.2, random_state=RANDOM_SEED)","649a02e9":"def handle_outliers_per_target_class(df,var,target,tol):\n    gdf = df[df[target] == 1]\n    var_data = gdf[var].values\n    q25, q75 = np.percentile(var_data, 25), np.percentile(var_data, 75)\n    \n    print('Outliers handling for {}'.format(var))\n    print('Quartile 25: {} | Quartile 75: {}'.format(q25, q75))\n    \n    iqr = q75 - q25\n    print('IQR {}'.format(iqr))\n    \n    cut_off = iqr * tol\n    lower, upper = q25 - cut_off, q75 + cut_off\n    \n    print('Cut Off: {}'.format(cut_off))\n    print('{} Lower: {}'.format(var,lower))\n    print('{} Upper: {}'.format(var,upper))\n    \n    outliers = [x for x in var_data if x < lower or x > upper]\n\n    print('Number of Outliers in feature {} in {}: {}'.format(var,key,len(outliers)))\n\n    print('{} outliers:{}'.format(var,outliers))\n\n    print('----' * 25)\n    print('\\n')\n    print('\\n')\n        \n    return list(df[(df[var] > upper) | (df[var] < lower)].index)","a8ff82fd":"outliers_wrt_target = []\nfor num_col in num_cols:\n    outliers_wrt_target.extend(handle_outliers_per_target_class(train_data,num_col,target,1.5))\noutliers_wrt_target = list(set(outliers_wrt_target))\n\ntrain_data = train_data.drop(outliers_wrt_target)","4b99d44a":"train_data[\"Fare\"] = np.where(train_data[\"Fare\"] != 0,np.log(train_data[\"Fare\"]),np.log(0.00001))\ntest_data[\"Fare\"] = np.where(test_data[\"Fare\"] != 0,np.log(test_data[\"Fare\"]),np.log(0.00001))\nvalidation_data[\"Fare\"] = np.where(validation_data[\"Fare\"] != 0,np.log(validation_data[\"Fare\"]),np.log(0.00001))","ef5ac037":"X_train = train_data.drop(['PassengerId', 'Survived', 'Name', 'SibSp', 'Parch', 'Ticket'],axis=1)\ny_train = train_data[target]","c2ca791d":"X_test = test_data.drop(['PassengerId', 'Survived', 'Name', 'SibSp', 'Parch', 'Ticket'],axis=1)\ny_test = test_data[target]","65a6ea9e":"X_val = validation_data.drop(['PassengerId', 'Survived', 'Name', 'SibSp', 'Parch', 'Ticket'],axis=1)\ny_val = validation_data[target]","31d8a734":"y_enc = LabelEncoder()\ny_train = y_enc.fit_transform(y_train)\ny_test = y_enc.transform(y_test)\ny_val = y_enc.transform(y_val)","d197690e":"X_train.head()","133a78fc":"sc = StandardScaler()\nX_train[num_cols] = sc.fit_transform(X_train[num_cols])\nX_test[num_cols] = sc.transform(X_test[num_cols])\nX_val[num_cols] = sc.transform(X_val[num_cols])","4f290f87":"sc.mean_","e023e16b":"sc.var_","af682c2c":"X_train.shape","68d1fdad":"clf = LogisticRegression()","b5829f73":"clf.fit(X_train,y_train)","a1214cd1":"clf.intercept_","46acbb37":"y_pred = clf.predict(X_test)","64336bac":"y_pred","4000808e":"confusion_matrix(y_test,y_pred)","ce3a1464":"accuracy_score(y_test,y_pred)","30dcc135":"classification_models = ['LogisticRegression',\n                         'SVC',\n                         'DecisionTreeClassifier',\n                         'RandomForestClassifier',\n                         'AdaBoostClassifier']","0038d1ce":"cm = []\nacc = []\nprec = []\nrec = []\nf1 = []\nmodels = []\nestimators = []","d5f414f6":"for classfication_model in classification_models:\n    \n    model = eval(classfication_model)()\n    \n    model.fit(X_train,y_train)\n    y_pred = model.predict(X_test)\n    \n    models.append(type(model).__name__)\n    estimators.append((type(model).__name__,model))\n    cm.append(confusion_matrix(y_test,y_pred))\n    acc.append(accuracy_score(y_test,y_pred))\n    prec.append(precision_score(y_test,y_pred))\n    rec.append(recall_score(y_test,y_pred))\n    f1.append(f1_score(y_test,y_pred))","e0e55c5d":"vc = VotingClassifier(estimators)\nvc.fit(X_train,y_train)","7fdc66bb":"y_pred = vc.predict(X_test)\n    \nmodels.append(type(vc).__name__)\n\ncm.append(confusion_matrix(y_test,y_pred))\nacc.append(accuracy_score(y_test,y_pred))\nprec.append(precision_score(y_test,y_pred))\nrec.append(recall_score(y_test,y_pred))\nf1.append(f1_score(y_test,y_pred))","63311080":"model_dict = {\"Models\":models,\n             \"CM\":cm,\n             \"Accuracy\":acc,\n             \"Precision\":prec,\n             \"Recall\":rec,\n             \"f1_score\":f1}","9a5d9ad2":"model_df = pd.DataFrame(model_dict)\nmodel_df","85c4bdbf":"model_df.sort_values(by=['Accuracy','f1_score','Recall','Precision'],ascending=False,inplace=True)\nmodel_df","036b2400":"model_param_grid = {}","917c4585":"model_param_grid['LogisticRegression'] = {'penalty' : ['l1', 'l2'],\n                                          'C' : np.logspace(0, 4, 10)}","307aa651":"model_param_grid['SVC'] = [{'kernel': ['rbf'], \n                            'gamma': [1e-2, 1e-3, 1e-4, 1e-5],\n                            'C': [0.001, 0.10, 0.1, 10, 25, 50, 100, 1000]},\n                           {'kernel': ['sigmoid'],\n                            'gamma': [1e-2, 1e-3, 1e-4, 1e-5],\n                            'C': [0.001, 0.10, 0.1, 10, 25, 50, 100, 1000]},\n                           {'kernel': ['linear'], \n                            'C': [0.001, 0.10, 0.1, 10, 25, 50, 100, 1000]},\n                           {'kernel': ['poly'], \n                            'degree' : [0, 1, 2, 3, 4, 5, 6]}\n                          ]","37f9b386":"model_param_grid['DecisionTreeClassifier'] = {'criterion' : [\"gini\",\"entropy\"],\n                                              'max_features': ['auto', 'sqrt', 'log2'],\n                                              'min_samples_split': [10,11,12,13,14,15],\n                                              'min_samples_leaf':[1,2,3,4,5,6,7]}","ca167759":"model_param_grid['RandomForestClassifier'] = {'n_estimators' : [50,100,150,200],\n                                              'criterion' : [\"gini\",\"entropy\"],\n                                              'max_features': ['auto', 'sqrt', 'log2'],\n                                              'class_weight' : [\"balanced\", \"balanced_subsample\"]}","6cf29aa4":"model_param_grid['AdaBoostClassifier'] = {'n_estimators' : [25,50,75,100],\n                                          'learning_rate' : [0.001,0.01,0.05,0.1,1,10],\n                                          'algorithm' : ['SAMME', 'SAMME.R']}","cfb91025":"from sklearn.model_selection import GridSearchCV\ndef tune_parameters(model_name,model,params,cv,scorer,X,y):\n    best_model = GridSearchCV(estimator = model,\n                              param_grid = params,\n                              scoring = scorer,\n                              cv = cv,\n                              n_jobs = -1).fit(X, y)\n    print(\"Tuning Results for \", model_name)\n    print(\"Best Score Achieved: \",best_model.best_score_)\n    print(\"Best Parameters Used: \",best_model.best_params_)\n    return best_model.best_estimator_","e52fe71d":"from sklearn.metrics import make_scorer\n\n# Define scorer\ndef roc_metric(y_test, y_pred):\n    score = roc_auc_score(y_test, y_pred)\n    return score","0575fe92":"# Scorer function would try to maximize calculated metric\nroc_scorer = make_scorer(roc_metric,greater_is_better=True)","375521d2":"best_estimators = []","7b055bec":"for m_name, m_obj in estimators:\n    best_estimators.append((m_name,tune_parameters(m_name,\n                                                   m_obj,\n                                                   model_param_grid[m_name],\n                                                   10,\n                                                   roc_scorer,\n                                                   X_train,\n                                                   y_train)))","f8fa74d9":"best_estimators","5d99ba19":"tuned_vc = VotingClassifier(best_estimators)\ntuned_vc.fit(X_train,y_train)","d621526c":"y_pred = tuned_vc.predict(X_test)","9c307821":"confusion_matrix(y_test,y_pred)","bd5496e2":"accuracy_score(y_test,y_pred)","28ab7449":"precision_score(y_test,y_pred)","b380af74":"recall_score(y_test,y_pred)","1a3c4f2f":"f1_score(y_test,y_pred)","feec6b68":"# Importing the Keras libraries and packages\nimport keras\nfrom keras.utils import plot_model\nfrom keras.models import Model,Sequential,load_model\nfrom keras.layers import Input, Flatten, Dense, Dropout\nfrom keras.layers.merge import concatenate\nfrom keras import backend as K\nfrom keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau","e1add497":"def nn_model(X,y,optimizer,kernels):\n    input_shape = X.shape[1]\n       \n    if(len(np.unique(y)) == 2):\n        op_neurons = 1\n        op_activation = 'sigmoid'\n        loss = 'binary_crossentropy'\n    else:\n        op_neurons = len(np.unique(y))\n        op_activation = 'softmax'\n        loss = 'categorical_crossentropy'\n    \n    classifier = Sequential()\n    classifier.add(Dense(units = input_shape,\n                         kernel_initializer = kernels,\n                         activation = 'relu',\n                         input_dim = input_shape))\n    classifier.add(Dense(units = 8,\n                         kernel_initializer = kernels,\n                         activation = 'relu'))\n    classifier.add(Dense(units = 4,\n                         kernel_initializer = kernels,\n                         activation = 'relu'))\n    classifier.add(Dropout(rate = 0.25))\n    classifier.add(Dense(units = op_neurons,\n                         kernel_initializer = kernels,\n                         activation = op_activation))\n    \n    classifier.compile(optimizer = optimizer,\n                       loss = loss,\n                       metrics = ['accuracy'])\n    \n    classifier.summary()\n    return classifier","85045985":"model = nn_model(X_train,y_train,'adam','he_uniform')\nhistory = model.fit(X_train,\n                    y_train,\n                    batch_size = 64,\n                    epochs = 1000,\n                    validation_data=(X_test, y_test))","769bca67":"his_df = pd.DataFrame(history.history)\nhis_df.shape","d64a9d2f":"plt.plot(his_df['loss'])\nplt.plot(his_df['val_loss'])\nplt.title(\"Loss Plot\")\nplt.legend([\"train\",\"test\"])","a7d3c38b":"plt.plot(his_df['accuracy'])\nplt.plot(his_df['val_accuracy'])\nplt.title(\"Accuracy Plot\")\nplt.legend([\"train\",\"test\"])","6244ef28":"## Exploratory Data Analysis","00a3fdb9":"### Feature Engineering","f7221836":"## Modelling","4ccf717f":"### Baseline models","655c374d":"### Hyper parameter Tuning","ccd82d34":"#### Bivariate Analysis","944d3e75":"### Implementing Neural Network","7d72d423":"### Stacking Ensemble","33cc626e":"#### Data Transformations","5de03b77":"#### Function to perform Grid Search with Cross Validation","0b37a428":"#### Univariate Analysis","2b7a9190":"#### Run iterations for all the trained baseline models","bd68346c":"## Introduction","c54b781e":"#### Define custom Scorer function"}}