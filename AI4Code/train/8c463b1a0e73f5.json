{"cell_type":{"c62678c9":"code","ddcf4ed1":"code","a7a18add":"code","4f726b6b":"code","b10265e2":"code","1c868558":"code","759db464":"code","05119891":"code","a80a56b5":"code","52482ba7":"code","63c282d2":"code","1af775e4":"code","f4883b2d":"code","85357287":"code","9cdd2fff":"code","45ff3a81":"code","fa5ce986":"code","1c0dca54":"code","c0444827":"code","940e3288":"code","9c2d2b0c":"code","345a9de9":"code","eafb2d2a":"code","e81c08d6":"code","9e8da5b6":"code","ba5c61ce":"markdown","a451d245":"markdown","7f9b2090":"markdown","adc3eefe":"markdown","3aa17e99":"markdown","79ca1a22":"markdown","47760597":"markdown","487bc000":"markdown","0c5bbb99":"markdown","2a7f334e":"markdown","a02b06ae":"markdown","6714c3c8":"markdown","a80efdc7":"markdown","d906c450":"markdown","2ff7ab65":"markdown","21df2bef":"markdown","7cafc8c4":"markdown","54fb7b4d":"markdown","edc0b5af":"markdown","9fadaba6":"markdown","293e842c":"markdown","b4386695":"markdown","a59e8de6":"markdown","a235ea90":"markdown","76e5c622":"markdown","8c624709":"markdown","77bcf86a":"markdown","8ea298b5":"markdown","b5f926f3":"markdown","ce7b4442":"markdown","ae04edd7":"markdown","83ca2c07":"markdown","64186fa5":"markdown","9d9830ef":"markdown","b5eaa675":"markdown","78b946d9":"markdown","873a710d":"markdown","9d9ea7a0":"markdown","4f3a991b":"markdown","254aef24":"markdown"},"source":{"c62678c9":"import numpy as np\nimport json\nimport pandas as pd\nfrom PIL import Image, ImageDraw\nimport matplotlib.pyplot as plt\nfrom pandas.io.json import json_normalize\nimport random\nimport tensorflow as tf\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import KFold,train_test_split\nimport matplotlib.pyplot as plt\nimport glob\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Dense,Dropout, Conv2D,Conv2DTranspose, BatchNormalization, Activation,AveragePooling2D,GlobalAveragePooling2D, Input, Concatenate, MaxPool2D, Add, UpSampling2D, LeakyReLU,ZeroPadding2D\nfrom keras.models import Model\nfrom keras.objectives import mean_squared_error\nfrom keras import backend as K\nfrom keras.losses import binary_crossentropy\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau,LearningRateScheduler\nimport os  \n\nfrom keras.optimizers import Adam, RMSprop, SGD\nfrom tensorflow.compat.v1 import ConfigProto\nfrom tensorflow.compat.v1 import InteractiveSession\n","ddcf4ed1":"path_1=\"..\/input\/train.csv\"\npath_2=\"..\/input\/train_images\/\"\npath_3=\"..\/input\/test_images\/\"\npath_4=\"..\/input\/sample_submission.csv\"\ndf_train=pd.read_csv(path_1)\n#print(df_train.head())\n#print(df_train.shape)\ndf_train=df_train.dropna(axis=0, how='any')#you can use nan data(page with no letter)\ndf_train=df_train.reset_index(drop=True)\n#print(df_train.shape)\n\nannotation_list_train=[]\ncategory_names=set()\n\nfor i in range(len(df_train)):\n  ann=np.array(df_train.loc[i,\"labels\"].split(\" \")).reshape(-1,5)#cat,x,y,width,height for each picture\n  category_names=category_names.union({i for i in ann[:,0]})\n\ncategory_names=sorted(category_names)\ndict_cat={list(category_names)[j]:str(j) for j in range(len(category_names))}\ninv_dict_cat={str(j):list(category_names)[j] for j in range(len(category_names))}\n#print(dict_cat)\n  \nfor i in range(len(df_train)):\n  ann=np.array(df_train.loc[i,\"labels\"].split(\" \")).reshape(-1,5)#cat,left,top,width,height for each picture\n  for j,category_name in enumerate(ann[:,0]):\n    ann[j,0]=int(dict_cat[category_name])  \n  ann=ann.astype('int32')\n  ann[:,1]+=ann[:,3]\/\/2#center_x\n  ann[:,2]+=ann[:,4]\/\/2#center_y\n  annotation_list_train.append([\"{}{}.jpg\".format(path_2,df_train.loc[i,\"image_id\"]),ann])\n\nprint(\"sample image\")\ninput_width,input_height=512, 512\nimg = np.asarray(Image.open(annotation_list_train[0][0]).resize((input_width,input_height)).convert('RGB'))\nplt.imshow(img)\nplt.show()\n","a7a18add":"# get directory of test images\ndf_submission=pd.read_csv(path_4)\nid_test=path_3+df_submission[\"image_id\"]+\".jpg\"","4f726b6b":"aspect_ratio_pic_all=[]\naspect_ratio_pic_all_test=[]\naverage_letter_size_all=[]\ntrain_input_for_size_estimate=[]\nresize_dir=\"resized\/\"\nif os.path.exists(resize_dir) == False:os.mkdir(resize_dir)\nfor i in range(len(annotation_list_train)):\n    with Image.open(annotation_list_train[i][0]) as f:\n        width,height=f.size\n        area=width*height\n        aspect_ratio_pic=height\/width\n        aspect_ratio_pic_all.append(aspect_ratio_pic)\n        letter_size=annotation_list_train[i][1][:,3]*annotation_list_train[i][1][:,4]\n        letter_size_ratio=letter_size\/area\n    \n        average_letter_size=np.mean(letter_size_ratio)\n        average_letter_size_all.append(average_letter_size)\n        train_input_for_size_estimate.append([annotation_list_train[i][0],np.log(average_letter_size)])#log\u306b\u3057\u3068\u304f\n    \n\nfor i in range(len(id_test)):\n    with Image.open(id_test[i]) as f:\n        width,height=f.size\n        aspect_ratio_pic=height\/width\n        aspect_ratio_pic_all_test.append(aspect_ratio_pic)\n\n\nplt.hist(np.log(average_letter_size_all),bins=100)\nplt.title('log(ratio of letter_size to picture_size))',loc='center',fontsize=12)\nplt.show()","b10265e2":"\ncategory_n=1\nimport cv2\ninput_width,input_height=512, 512\n\ndef Datagen_sizecheck_model(filenames, batch_size, size_detection_mode=True, is_train=True,random_crop=True):\n  x=[]\n  y=[]\n  \n  count=0\n\n  while True:\n    for i in range(len(filenames)):\n      if random_crop:\n        crop_ratio=np.random.uniform(0.7,1)\n      else:\n        crop_ratio=1\n      with Image.open(filenames[i][0]) as f:\n        #random crop\n        if random_crop and is_train:\n          pic_width,pic_height=f.size\n          f=np.asarray(f.convert('RGB'),dtype=np.uint8)\n          top_offset=np.random.randint(0,pic_height-int(crop_ratio*pic_height))\n          left_offset=np.random.randint(0,pic_width-int(crop_ratio*pic_width))\n          bottom_offset=top_offset+int(crop_ratio*pic_height)\n          right_offset=left_offset+int(crop_ratio*pic_width)\n          f=cv2.resize(f[top_offset:bottom_offset,left_offset:right_offset,:],(input_height,input_width))\n        else:\n          f=f.resize((input_width, input_height))\n          f=np.asarray(f.convert('RGB'),dtype=np.uint8)          \n        x.append(f)\n      \n      \n      if random_crop and is_train:\n        y.append(filenames[i][1]-np.log(crop_ratio))\n      else:\n        y.append(filenames[i][1])\n      \n      count+=1\n      if count==batch_size:\n        x=np.array(x, dtype=np.float32)\n        y=np.array(y, dtype=np.float32)\n\n        inputs=x\/255\n        targets=y       \n        x=[]\n        y=[]\n        count=0\n        yield inputs, targets\n\n\n\ndef aggregation_block(x_shallow, x_deep, deep_ch, out_ch):\n  x_deep= Conv2DTranspose(deep_ch, kernel_size=2, strides=2, padding='same', use_bias=False)(x_deep)\n  x_deep = BatchNormalization()(x_deep)   \n  x_deep = LeakyReLU(alpha=0.1)(x_deep)\n  x = Concatenate()([x_shallow, x_deep])\n  x=Conv2D(out_ch, kernel_size=1, strides=1, padding=\"same\")(x)\n  x = BatchNormalization()(x)   \n  x = LeakyReLU(alpha=0.1)(x)\n  return x\n  \n\n\ndef cbr(x, out_layer, kernel, stride):\n  x=Conv2D(out_layer, kernel_size=kernel, strides=stride, padding=\"same\")(x)\n  x = BatchNormalization()(x)\n  x = LeakyReLU(alpha=0.1)(x)\n  return x\n\ndef resblock(x_in,layer_n):\n  x=cbr(x_in,layer_n,3,1)\n  x=cbr(x,layer_n,3,1)\n  x=Add()([x,x_in])\n  return x  \n\n\n#I use the same network at CenterNet\ndef create_model(input_shape, size_detection_mode=True, aggregation=True):\n    input_layer = Input(input_shape)\n    \n    #resized input\n    input_layer_1=AveragePooling2D(2)(input_layer)\n    input_layer_2=AveragePooling2D(2)(input_layer_1)\n\n    #### ENCODER ####\n\n    x_0= cbr(input_layer, 16, 3, 2)#512->256\n    concat_1 = Concatenate()([x_0, input_layer_1])\n\n    x_1= cbr(concat_1, 32, 3, 2)#256->128\n    concat_2 = Concatenate()([x_1, input_layer_2])\n\n    x_2= cbr(concat_2, 64, 3, 2)#128->64\n    \n    x=cbr(x_2,64,3,1)\n    x=resblock(x,64)\n    x=resblock(x,64)\n    \n    x_3= cbr(x, 128, 3, 2)#64->32\n    x= cbr(x_3, 128, 3, 1)\n    x=resblock(x,128)\n    x=resblock(x,128)\n    x=resblock(x,128)\n    \n    x_4= cbr(x, 256, 3, 2)#32->16\n    x= cbr(x_4, 256, 3, 1)\n    x=resblock(x,256)\n    x=resblock(x,256)\n    x=resblock(x,256)\n    x=resblock(x,256)\n    x=resblock(x,256)\n \n    x_5= cbr(x, 512, 3, 2)#16->8\n    x= cbr(x_5, 512, 3, 1)\n    \n    x=resblock(x,512)\n    x=resblock(x,512)\n    x=resblock(x,512)\n    \n    if size_detection_mode:\n      x=GlobalAveragePooling2D()(x)\n      x=Dropout(0.2)(x)\n      out=Dense(1,activation=\"linear\")(x)\n    \n    else:#centernet mode\n    #### DECODER ####\n      x_1= cbr(x_1, output_layer_n, 1, 1)\n      x_1 = aggregation_block(x_1, x_2, output_layer_n, output_layer_n)\n      x_2= cbr(x_2, output_layer_n, 1, 1)\n      x_2 = aggregation_block(x_2, x_3, output_layer_n, output_layer_n)\n      x_1 = aggregation_block(x_1, x_2, output_layer_n, output_layer_n)\n      x_3= cbr(x_3, output_layer_n, 1, 1)\n      x_3 = aggregation_block(x_3, x_4, output_layer_n, output_layer_n) \n      x_2 = aggregation_block(x_2, x_3, output_layer_n, output_layer_n)\n      x_1 = aggregation_block(x_1, x_2, output_layer_n, output_layer_n)\n      \n      x_4= cbr(x_4, output_layer_n, 1, 1)\n\n      x=cbr(x, output_layer_n, 1, 1)\n      x= UpSampling2D(size=(2, 2))(x)#8->16 tconv\u306e\u304c\u3044\u3044\u304b\n\n      x = Concatenate()([x, x_4])\n      x=cbr(x, output_layer_n, 3, 1)\n      x= UpSampling2D(size=(2, 2))(x)#16->32\n    \n      x = Concatenate()([x, x_3])\n      x=cbr(x, output_layer_n, 3, 1)\n      x= UpSampling2D(size=(2, 2))(x)#32->64   128\u306e\u304c\u3044\u3044\u304b\u3082\uff1f \n    \n      x = Concatenate()([x, x_2])\n      x=cbr(x, output_layer_n, 3, 1)\n      x= UpSampling2D(size=(2, 2))(x)#64->128 \n      \n      x = Concatenate()([x, x_1])\n      x=Conv2D(output_layer_n, kernel_size=3, strides=1, padding=\"same\")(x)\n      out = Activation(\"sigmoid\")(x)\n    \n    model=Model(input_layer, out)\n    \n    return model\n  \n    \n\n\ndef model_fit_sizecheck_model(model,train_list,cv_list,n_epoch,batch_size=32):\n    hist = model.fit_generator(\n        Datagen_sizecheck_model(train_list,batch_size, is_train=True,random_crop=True),\n        steps_per_epoch = len(train_list) \/\/ batch_size,\n        epochs = n_epoch,\n        validation_data=Datagen_sizecheck_model(cv_list,batch_size, is_train=False,random_crop=False),\n        validation_steps = len(cv_list) \/\/ batch_size,\n        callbacks = [lr_schedule, model_checkpoint],#[early_stopping, reduce_lr, model_checkpoint],\n        shuffle = True,\n        verbose = 1\n    )\n    return hist\n\n  \n","1c868558":"K.clear_session()\nmodel=create_model(input_shape=(input_height,input_width,3),size_detection_mode=True)\n\"\"\"\n# EarlyStopping\nearly_stopping = EarlyStopping(monitor = 'val_loss', min_delta=0, patience = 10, verbose = 1)\n# ModelCheckpoint\nweights_dir = '\/model_1\/'\nif os.path.exists(weights_dir) == False:os.mkdir(weights_dir)\nmodel_checkpoint = ModelCheckpoint(weights_dir + \"val_loss{val_loss:.3f}.hdf5\", monitor = 'val_loss', verbose = 1,\n                                      save_best_only = True, save_weights_only = True, period = 1)\n# reduce learning rate\nreduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.5, patience = 10, verbose = 1)\n\"\"\"\ndef lrs(epoch):\n    lr = 0.0005\n    if epoch>10:\n        lr = 0.0001\n    return lr\n\nlr_schedule = LearningRateScheduler(lrs)\nmodel_checkpoint = ModelCheckpoint(\"final_weights_step1.hdf5\", monitor = 'val_loss', verbose = 1,\n                                      save_best_only = True, save_weights_only = True, period = 1)\nprint(model.summary())","759db464":"train_list, cv_list = train_test_split(train_input_for_size_estimate, random_state = 111,test_size = 0.2)\n\n\nlearning_rate=0.0005\nn_epoch=12\nbatch_size=32\n\nmodel.compile(loss=mean_squared_error, optimizer=Adam(lr=learning_rate))\nhist = model_fit_sizecheck_model(model,train_list,cv_list,n_epoch,batch_size)\n\n#model.save_weights('final_weights_step1.h5')\nmodel.load_weights('final_weights_step1.hdf5')","05119891":"predict = model.predict_generator(Datagen_sizecheck_model(cv_list,batch_size, is_train=False,random_crop=False),\n                                  steps=len(cv_list) \/\/ batch_size)\ntarget=[cv[1] for cv in cv_list]\nplt.scatter(predict,target[:len(predict)])\nplt.title('---letter_size\/picture_size--- estimated vs target ',loc='center',fontsize=10)\nplt.show()","a80a56b5":"batch_size=1\npredict_train = model.predict_generator(Datagen_sizecheck_model(train_input_for_size_estimate,batch_size, is_train=False,random_crop=False, ),\n                                  steps=len(train_input_for_size_estimate)\/\/batch_size)","52482ba7":"base_detect_num_h,base_detect_num_w=25,25\nannotation_list_train_w_split=[]\nfor i, predicted_size in enumerate(predict_train):\n  detect_num_h=aspect_ratio_pic_all[i]*np.exp(-predicted_size\/2)\n  detect_num_w=detect_num_h\/aspect_ratio_pic_all[i]\n  h_split_recommend=np.maximum(1,detect_num_h\/base_detect_num_h)\n  w_split_recommend=np.maximum(1,detect_num_w\/base_detect_num_w)\n  annotation_list_train_w_split.append([annotation_list_train[i][0],annotation_list_train[i][1],h_split_recommend,w_split_recommend])\nfor i in np.arange(0,1):\n  print(\"recommended height split:{}, recommended width_split:{}\".format(annotation_list_train_w_split[i][2],annotation_list_train_w_split[i][3]))\n  img = np.asarray(Image.open(annotation_list_train_w_split[i][0]).convert('RGB'))\n  plt.imshow(img)\n  plt.show()","63c282d2":"category_n=1\noutput_layer_n=category_n+4\noutput_height,output_width=128,128\n\ni=0\n\nh_split=annotation_list_train_w_split[i][2]\nw_split=annotation_list_train_w_split[i][3]\nmax_crop_ratio_h=1\/h_split\nmax_crop_ratio_w=1\/w_split\ncrop_ratio=np.random.uniform(0.5,1)\ncrop_ratio_h=max_crop_ratio_h*crop_ratio\ncrop_ratio_w=max_crop_ratio_w*crop_ratio\n\nwith Image.open(annotation_list_train_w_split[i][0]) as f:\n        \n        #random crop\n        pic_width,pic_height=f.size\n        f=np.asarray(f.convert('RGB'),dtype=np.uint8)\n        top_offset=np.random.randint(0,pic_height-int(crop_ratio_h*pic_height))\n        left_offset=np.random.randint(0,pic_width-int(crop_ratio_w*pic_width))\n        bottom_offset=top_offset+int(crop_ratio_h*pic_height)\n        right_offset=left_offset+int(crop_ratio_w*pic_width)\n        img=cv2.resize(f[top_offset:bottom_offset,left_offset:right_offset,:],(input_height,input_width))\n\n      \n      \noutput_layer=np.zeros((output_height,output_width,(output_layer_n+category_n)))\nfor annotation in annotation_list_train_w_split[i][1]:\n\n          x_c=(annotation[1]-left_offset)*(output_width\/int(crop_ratio_w*pic_width))\n          y_c=(annotation[2]-top_offset)*(output_height\/int(crop_ratio_h*pic_height))\n          width=annotation[3]*(output_width\/int(crop_ratio_w*pic_width))\n          height=annotation[4]*(output_height\/int(crop_ratio_h*pic_height))\n          \n          top=np.maximum(0,y_c-height\/2)\n          left=np.maximum(0,x_c-width\/2)\n          bottom=np.minimum(output_height,y_c+height\/2)\n          right=np.minimum(output_width,x_c+width\/2)\n          \n          if top>=output_height or left>=output_width or bottom<=0 or right<=0:#random crop(\u30a8\u30ea\u30a2\u5916\u306e\u9664\u53bb)\n            continue\n          width=right-left\n          height=bottom-top\n          x_c=(right+left)\/2\n          y_c=(top+bottom)\/2\n          \n        \n        \n          category=0#not classify\n          heatmap=((np.exp(-(((np.arange(output_width)-x_c)\/(width\/10))**2)\/2)).reshape(1,-1)\n                            *(np.exp(-(((np.arange(output_height)-y_c)\/(height\/10))**2)\/2)).reshape(-1,1))\n          output_layer[:,:,category]=np.maximum(output_layer[:,:,category],heatmap[:,:])\n          output_layer[int(y_c\/\/1),int(x_c\/\/1),category_n+category]=1\n          output_layer[int(y_c\/\/1),int(x_c\/\/1),2*category_n]=y_c%1#height offset\n          output_layer[int(y_c\/\/1),int(x_c\/\/1),2*category_n+1]=x_c%1\n          output_layer[int(y_c\/\/1),int(x_c\/\/1),2*category_n+2]=height\/output_height\n          output_layer[int(y_c\/\/1),int(x_c\/\/1),2*category_n+3]=width\/output_width\n\nfig, axes = plt.subplots(1, 3,figsize=(15,15))\naxes[0].set_axis_off()\naxes[0].imshow(img)\naxes[1].set_axis_off()\naxes[1].imshow(output_layer[:,:,1])\naxes[2].set_axis_off()\naxes[2].imshow(output_layer[:,:,0])\nplt.show()","1af775e4":"category_n=1\noutput_layer_n=category_n+4\noutput_height,output_width=128,128\n\ndef Datagen_centernet(filenames, batch_size):\n  x=[]\n  y=[]\n  \n  count=0\n\n  while True:\n    for i in range(len(filenames)):\n      h_split=filenames[i][2]\n      w_split=filenames[i][3]\n      max_crop_ratio_h=1\/h_split\n      max_crop_ratio_w=1\/w_split\n      crop_ratio=np.random.uniform(0.5,1)\n      crop_ratio_h=max_crop_ratio_h*crop_ratio\n      crop_ratio_w=max_crop_ratio_w*crop_ratio\n      \n      with Image.open(filenames[i][0]) as f:\n        \n        #random crop\n        \n        pic_width,pic_height=f.size\n        f=np.asarray(f.convert('RGB'),dtype=np.uint8)\n        top_offset=np.random.randint(0,pic_height-int(crop_ratio_h*pic_height))\n        left_offset=np.random.randint(0,pic_width-int(crop_ratio_w*pic_width))\n        bottom_offset=top_offset+int(crop_ratio_h*pic_height)\n        right_offset=left_offset+int(crop_ratio_w*pic_width)\n        f=cv2.resize(f[top_offset:bottom_offset,left_offset:right_offset,:],(input_height,input_width))\n        x.append(f)      \n\n      output_layer=np.zeros((output_height,output_width,(output_layer_n+category_n)))\n      for annotation in filenames[i][1]:\n        x_c=(annotation[1]-left_offset)*(output_width\/int(crop_ratio_w*pic_width))\n        y_c=(annotation[2]-top_offset)*(output_height\/int(crop_ratio_h*pic_height))\n        width=annotation[3]*(output_width\/int(crop_ratio_w*pic_width))\n        height=annotation[4]*(output_height\/int(crop_ratio_h*pic_height))\n        top=np.maximum(0,y_c-height\/2)\n        left=np.maximum(0,x_c-width\/2)\n        bottom=np.minimum(output_height,y_c+height\/2)\n        right=np.minimum(output_width,x_c+width\/2)\n          \n        if top>=(output_height-0.1) or left>=(output_width-0.1) or bottom<=0.1 or right<=0.1:#random crop(out of picture)\n          continue\n        width=right-left\n        height=bottom-top\n        x_c=(right+left)\/2\n        y_c=(top+bottom)\/2\n\n        \n        category=0#not classify, just detect\n        heatmap=((np.exp(-(((np.arange(output_width)-x_c)\/(width\/10))**2)\/2)).reshape(1,-1)\n                            *(np.exp(-(((np.arange(output_height)-y_c)\/(height\/10))**2)\/2)).reshape(-1,1))\n        output_layer[:,:,category]=np.maximum(output_layer[:,:,category],heatmap[:,:])\n        output_layer[int(y_c\/\/1),int(x_c\/\/1),category_n+category]=1\n        output_layer[int(y_c\/\/1),int(x_c\/\/1),2*category_n]=y_c%1#height offset\n        output_layer[int(y_c\/\/1),int(x_c\/\/1),2*category_n+1]=x_c%1\n        output_layer[int(y_c\/\/1),int(x_c\/\/1),2*category_n+2]=height\/output_height\n        output_layer[int(y_c\/\/1),int(x_c\/\/1),2*category_n+3]=width\/output_width\n      y.append(output_layer)  \n    \n      count+=1\n      if count==batch_size:\n        x=np.array(x, dtype=np.float32)\n        y=np.array(y, dtype=np.float32)\n\n        inputs=x\/255\n        targets=y       \n        x=[]\n        y=[]\n        count=0\n        yield inputs, targets\n\ndef all_loss(y_true, y_pred):\n    mask=K.sign(y_true[...,2*category_n+2])\n    N=K.sum(mask)\n    alpha=2.\n    beta=4.\n\n    heatmap_true_rate = K.flatten(y_true[...,:category_n])\n    heatmap_true = K.flatten(y_true[...,category_n:(2*category_n)])\n    heatmap_pred = K.flatten(y_pred[...,:category_n])\n    heatloss=-K.sum(heatmap_true*((1-heatmap_pred)**alpha)*K.log(heatmap_pred+1e-6)+(1-heatmap_true)*((1-heatmap_true_rate)**beta)*(heatmap_pred**alpha)*K.log(1-heatmap_pred+1e-6))\n    offsetloss=K.sum(K.abs(y_true[...,2*category_n]-y_pred[...,category_n]*mask)+K.abs(y_true[...,2*category_n+1]-y_pred[...,category_n+1]*mask))\n    sizeloss=K.sum(K.abs(y_true[...,2*category_n+2]-y_pred[...,category_n+2]*mask)+K.abs(y_true[...,2*category_n+3]-y_pred[...,category_n+3]*mask))\n    \n    all_loss=(heatloss+1.0*offsetloss+5.0*sizeloss)\/N\n    return all_loss\n\ndef size_loss(y_true, y_pred):\n    mask=K.sign(y_true[...,2*category_n+2])\n    N=K.sum(mask)\n    sizeloss=K.sum(K.abs(y_true[...,2*category_n+2]-y_pred[...,category_n+2]*mask)+K.abs(y_true[...,2*category_n+3]-y_pred[...,category_n+3]*mask))\n    return (5*sizeloss)\/N\n\ndef offset_loss(y_true, y_pred):\n    mask=K.sign(y_true[...,2*category_n+2])\n    N=K.sum(mask)\n    offsetloss=K.sum(K.abs(y_true[...,2*category_n]-y_pred[...,category_n]*mask)+K.abs(y_true[...,2*category_n+1]-y_pred[...,category_n+1]*mask))\n    return (offsetloss)\/N\n  \ndef heatmap_loss(y_true, y_pred):\n    mask=K.sign(y_true[...,2*category_n+2])\n    N=K.sum(mask)\n    alpha=2.\n    beta=4.\n\n    heatmap_true_rate = K.flatten(y_true[...,:category_n])\n    heatmap_true = K.flatten(y_true[...,category_n:(2*category_n)])\n    heatmap_pred = K.flatten(y_pred[...,:category_n])\n    heatloss=-K.sum(heatmap_true*((1-heatmap_pred)**alpha)*K.log(heatmap_pred+1e-6)+(1-heatmap_true)*((1-heatmap_true_rate)**beta)*(heatmap_pred**alpha)*K.log(1-heatmap_pred+1e-6))\n    return heatloss\/N\n\n  \ndef model_fit_centernet(model,train_list,cv_list,n_epoch,batch_size=32):\n    hist = model.fit_generator(\n        Datagen_centernet(train_list,batch_size),\n        steps_per_epoch = len(train_list) \/\/ batch_size,\n        epochs = n_epoch,\n        validation_data=Datagen_centernet(cv_list,batch_size),\n        validation_steps = len(cv_list) \/\/ batch_size,\n        callbacks = [lr_schedule],#early_stopping, reduce_lr, model_checkpoint],\n        shuffle = True,\n        verbose = 1\n    )\n    return hist","f4883b2d":"K.clear_session()\nmodel=create_model(input_shape=(input_height,input_width,3),size_detection_mode=False)\n\ndef lrs(epoch):\n    lr = 0.001\n    if epoch >= 20: lr = 0.0002\n    return lr\n\nlr_schedule = LearningRateScheduler(lrs)\n\n\"\"\"\n\n# EarlyStopping\nearly_stopping = EarlyStopping(monitor = 'val_loss', min_delta=0, patience = 60, verbose = 1)\n# ModelCheckpoint\nweights_dir = '\/model_2\/'\n\nif os.path.exists(weights_dir) == False:os.mkdir(weights_dir)\nmodel_checkpoint = ModelCheckpoint(weights_dir + \"val_loss{val_loss:.3f}.hdf5\", monitor = 'val_loss', verbose = 1,\n                                      save_best_only = True, save_weights_only = True, period = 3)\n# reduce learning rate\nreduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.5, patience = 10, verbose = 1)\n\"\"\"\nmodel.load_weights('final_weights_step1.h5',by_name=True, skip_mismatch=True)\n\nprint(model.summary())","85357287":"train_list, cv_list = train_test_split(annotation_list_train_w_split, random_state = 111,test_size = 0.2)#stratified split is better\n\nlearning_rate=0.001\nn_epoch=30\nbatch_size=32\nmodel.compile(loss=all_loss, optimizer=Adam(lr=learning_rate), metrics=[heatmap_loss,size_loss,offset_loss])\nhist = model_fit_centernet(model,train_list,cv_list,n_epoch,batch_size)\n\nmodel.save_weights('final_weights_step2.h5')","9cdd2fff":"pred_in_h=512\npred_in_w=512\npred_out_h=int(pred_in_h\/4)\npred_out_w=int(pred_in_w\/4)\n\nfor i in np.arange(0,1):\n  img = np.asarray(Image.open(cv_list[i][0]).resize((pred_in_w,pred_in_h)).convert('RGB'))\n  predict=model.predict((img.reshape(1,pred_in_h,pred_in_w,3))\/255).reshape(pred_out_h,pred_out_w,(category_n+4))\n  heatmap=predict[:,:,0]\n\n  fig, axes = plt.subplots(1, 2,figsize=(15,15))\n  axes[0].set_axis_off()\n  axes[0].imshow(img)\n  axes[1].set_axis_off()\n  axes[1].imshow(heatmap)\n  plt.show()","45ff3a81":"from PIL import Image, ImageDraw\n\ndef NMS_all(predicts,category_n,score_thresh,iou_thresh):\n  y_c=predicts[...,category_n]+np.arange(pred_out_h).reshape(-1,1)\n  x_c=predicts[...,category_n+1]+np.arange(pred_out_w).reshape(1,-1)\n  height=predicts[...,category_n+2]*pred_out_h\n  width=predicts[...,category_n+3]*pred_out_w\n\n  count=0\n  for category in range(category_n):\n    predict=predicts[...,category]\n    mask=(predict>score_thresh)\n    #print(\"box_num\",np.sum(mask))\n    if mask.all==False:\n      continue\n    box_and_score=NMS(predict[mask],y_c[mask],x_c[mask],height[mask],width[mask],iou_thresh)\n    box_and_score=np.insert(box_and_score,0,category,axis=1)#category,score,top,left,bottom,right\n    if count==0:\n      box_and_score_all=box_and_score\n    else:\n      box_and_score_all=np.concatenate((box_and_score_all,box_and_score),axis=0)\n    count+=1\n  score_sort=np.argsort(box_and_score_all[:,1])[::-1]\n  box_and_score_all=box_and_score_all[score_sort]\n  #print(box_and_score_all)\n\n \n  _,unique_idx=np.unique(box_and_score_all[:,2],return_index=True)\n  #print(unique_idx)\n  return box_and_score_all[sorted(unique_idx)]\n  \ndef NMS(score,y_c,x_c,height,width,iou_thresh,merge_mode=False):\n  if merge_mode:\n    score=score\n    top=y_c\n    left=x_c\n    bottom=height\n    right=width\n  else:\n    #flatten\n    score=score.reshape(-1)\n    y_c=y_c.reshape(-1)\n    x_c=x_c.reshape(-1)\n    height=height.reshape(-1)\n    width=width.reshape(-1)\n    size=height*width\n    \n    \n    top=y_c-height\/2\n    left=x_c-width\/2\n    bottom=y_c+height\/2\n    right=x_c+width\/2\n    \n    inside_pic=(top>0)*(left>0)*(bottom<pred_out_h)*(right<pred_out_w)\n    outside_pic=len(inside_pic)-np.sum(inside_pic)\n    #if outside_pic>0:\n    #  print(\"{} boxes are out of picture\".format(outside_pic))\n    normal_size=(size<(np.mean(size)*10))*(size>(np.mean(size)\/10))\n    score=score[inside_pic*normal_size]\n    top=top[inside_pic*normal_size]\n    left=left[inside_pic*normal_size]\n    bottom=bottom[inside_pic*normal_size]\n    right=right[inside_pic*normal_size]\n  \n\n    \n\n  #sort  \n  score_sort=np.argsort(score)[::-1]\n  score=score[score_sort]  \n  top=top[score_sort]\n  left=left[score_sort]\n  bottom=bottom[score_sort]\n  right=right[score_sort]\n  \n  area=((bottom-top)*(right-left))\n  \n  boxes=np.concatenate((score.reshape(-1,1),top.reshape(-1,1),left.reshape(-1,1),bottom.reshape(-1,1),right.reshape(-1,1)),axis=1)\n  \n  box_idx=np.arange(len(top))\n  alive_box=[]\n  while len(box_idx)>0:\n  \n    alive_box.append(box_idx[0])\n    \n    y1=np.maximum(top[0],top)\n    x1=np.maximum(left[0],left)\n    y2=np.minimum(bottom[0],bottom)\n    x2=np.minimum(right[0],right)\n    \n    cross_h=np.maximum(0,y2-y1)\n    cross_w=np.maximum(0,x2-x1)\n    still_alive=(((cross_h*cross_w)\/area[0])<iou_thresh)\n    if np.sum(still_alive)==len(box_idx):\n      print(\"error\")\n      print(np.max((cross_h*cross_w)),area[0])\n    top=top[still_alive]\n    left=left[still_alive]\n    bottom=bottom[still_alive]\n    right=right[still_alive]\n    area=area[still_alive]\n    box_idx=box_idx[still_alive]\n  return boxes[alive_box]#score,top,left,bottom,right\n\n\n\ndef draw_rectangle(box_and_score,img,color):\n  number_of_rect=np.minimum(500,len(box_and_score))\n  \n  for i in reversed(list(range(number_of_rect))):\n    top, left, bottom, right = box_and_score[i,:]\n\n    \n    top = np.floor(top + 0.5).astype('int32')\n    left = np.floor(left + 0.5).astype('int32')\n    bottom = np.floor(bottom + 0.5).astype('int32')\n    right = np.floor(right + 0.5).astype('int32')\n    #label = '{} {:.2f}'.format(predicted_class, score)\n    #print(label)\n    #rectangle=np.array([[left,top],[left,bottom],[right,bottom],[right,top]])\n\n    draw = ImageDraw.Draw(img)\n    #label_size = draw.textsize(label)\n    #print(label_size)\n    \n    #if top - label_size[1] >= 0:\n    #  text_origin = np.array([left, top - label_size[1]])\n    #else:\n    #  text_origin = np.array([left, top + 1])\n    \n    thickness=4\n    if color==\"red\":\n      rect_color=(255, 0, 0)\n    elif color==\"blue\":\n      rect_color=(0, 0, 255)\n    else:\n      rect_color=(0, 0, 0)\n      \n    \n    if i==0:\n      thickness=4\n    for j in range(2*thickness):#\u8584\u3044\u304b\u3089\u4f55\u91cd\u306b\u304b\u63cf\u304f\n      draw.rectangle([left + j, top + j, right - j, bottom - j],\n                    outline=rect_color)\n    #draw.rectangle(\n    #            [tuple(text_origin), tuple(text_origin + label_size)],\n    #            fill=(0, 0, 255))\n    #draw.text(text_origin, label, fill=(0, 0, 0))\n    \n  del draw\n  return img\n            \n  \ndef check_iou_score(true_boxes,detected_boxes,iou_thresh):\n  iou_all=[]\n  for detected_box in detected_boxes:\n    y1=np.maximum(detected_box[0],true_boxes[:,0])\n    x1=np.maximum(detected_box[1],true_boxes[:,1])\n    y2=np.minimum(detected_box[2],true_boxes[:,2])\n    x2=np.minimum(detected_box[3],true_boxes[:,3])\n    \n    cross_section=np.maximum(0,y2-y1)*np.maximum(0,x2-x1)\n    all_area=(detected_box[2]-detected_box[0])*(detected_box[3]-detected_box[1])+(true_boxes[:,2]-true_boxes[:,0])*(true_boxes[:,3]-true_boxes[:,1])\n    iou=np.max(cross_section\/(all_area-cross_section))\n    #argmax=np.argmax(cross_section\/(all_area-cross_section))\n    iou_all.append(iou)\n  score=2*np.sum(iou_all)\/(len(detected_boxes)+len(true_boxes))\n  print(\"score:{}\".format(np.round(score,3)))\n  return score\n\n                \n\n\n\nfor i in np.arange(0,5):\n  #print(cv_list[i][2:])\n  img=Image.open(cv_list[i][0]).convert(\"RGB\")\n  width,height=img.size\n  predict=model.predict((np.asarray(img.resize((pred_in_w,pred_in_h))).reshape(1,pred_in_h,pred_in_w,3))\/255).reshape(pred_out_h,pred_out_w,(category_n+4))\n  \n  box_and_score=NMS_all(predict,category_n,score_thresh=0.3,iou_thresh=0.4)\n\n  #print(\"after NMS\",len(box_and_score))\n  if len(box_and_score)==0:\n    continue\n\n  true_boxes=cv_list[i][1][:,1:]#c_x,c_y,width_height\n  top=true_boxes[:,1:2]-true_boxes[:,3:4]\/2\n  left=true_boxes[:,0:1]-true_boxes[:,2:3]\/2\n  bottom=top+true_boxes[:,3:4]\n  right=left+true_boxes[:,2:3]\n  true_boxes=np.concatenate((top,left,bottom,right),axis=1)\n    \n  heatmap=predict[:,:,0]\n \n  print_w, print_h = img.size\n  #resize predocted box to original size\n  box_and_score=box_and_score*[1,1,print_h\/pred_out_h,print_w\/pred_out_w,print_h\/pred_out_h,print_w\/pred_out_w]\n  check_iou_score(true_boxes,box_and_score[:,2:],iou_thresh=0.5)\n  img=draw_rectangle(box_and_score[:,2:],img,\"red\")\n  img=draw_rectangle(true_boxes,img,\"blue\")\n  \n  fig, axes = plt.subplots(1, 2,figsize=(15,15))\n  #axes[0].set_axis_off()\n  axes[0].imshow(img)\n  #axes[1].set_axis_off()\n  axes[1].imshow(heatmap)#, cmap='gray')\n  #axes[2].set_axis_off()\n  #axes[2].imshow(heatmap_1)#, cmap='gray')\n  plt.show()\n","fa5ce986":"def split_and_detect(model,img,height_split_recommended,width_split_recommended,score_thresh=0.3,iou_thresh=0.4):\n  width,height=img.size\n  pred_in_w,pred_in_h=512,512\n  pred_out_w,pred_out_h=128,128\n  category_n=1\n  maxlap=0.5\n  height_split=int(-(-height_split_recommended\/\/1)+1)\n  width_split=int(-(-width_split_recommended\/\/1)+1)\n  height_lap=(height_split-height_split_recommended)\/(height_split-1)\n  height_lap=np.minimum(maxlap,height_lap)\n  width_lap=(width_split-width_split_recommended)\/(width_split-1)\n  width_lap=np.minimum(maxlap,width_lap)\n\n  if height>width:\n    crop_size=int((height)\/(height_split-(height_split-1)*height_lap))#crop_height and width\n    if crop_size>=width:\n      crop_size=width\n      stride=int((crop_size*height_split-height)\/(height_split-1))\n      top_list=[i*stride for i in range(height_split-1)]+[height-crop_size]\n      left_list=[0]\n    else:\n      stride=int((crop_size*height_split-height)\/(height_split-1))\n      top_list=[i*stride for i in range(height_split-1)]+[height-crop_size]\n      width_split=-(-width\/\/crop_size)\n      stride=int((crop_size*width_split-width)\/(width_split-1))\n      left_list=[i*stride for i in range(width_split-1)]+[width-crop_size]\n\n  else:\n    crop_size=int((width)\/(width_split-(width_split-1)*width_lap))#crop_height and width\n    if crop_size>=height:\n      crop_size=height\n      stride=int((crop_size*width_split-width)\/(width_split-1))\n      left_list=[i*stride for i in range(width_split-1)]+[width-crop_size]\n      top_list=[0]\n    else:\n      stride=int((crop_size*width_split-width)\/(width_split-1))\n      left_list=[i*stride for i in range(width_split-1)]+[width-crop_size]\n      height_split=-(-height\/\/crop_size)\n      stride=int((crop_size*height_split-height)\/(height_split-1))\n      top_list=[i*stride for i in range(height_split-1)]+[height-crop_size]\n  \n  count=0\n\n  for top_offset in top_list:\n    for left_offset in left_list:\n      img_crop = img.crop((left_offset, top_offset, left_offset+crop_size, top_offset+crop_size))\n      predict=model.predict((np.asarray(img_crop.resize((pred_in_w,pred_in_h))).reshape(1,pred_in_h,pred_in_w,3))\/255).reshape(pred_out_h,pred_out_w,(category_n+4))\n  \n      box_and_score=NMS_all(predict,category_n,score_thresh,iou_thresh)#category,score,top,left,bottom,right\n      \n      #print(\"after NMS\",len(box_and_score))\n      if len(box_and_score)==0:\n        continue\n      #reshape and offset\n      box_and_score=box_and_score*[1,1,crop_size\/pred_out_h,crop_size\/pred_out_w,crop_size\/pred_out_h,crop_size\/pred_out_w]+np.array([0,0,top_offset,left_offset,top_offset,left_offset])\n      \n      if count==0:\n        box_and_score_all=box_and_score\n      else:\n        box_and_score_all=np.concatenate((box_and_score_all,box_and_score),axis=0)\n      count+=1\n  #print(\"all_box_num:\",len(box_and_score_all))\n  #print(box_and_score_all[:10,:],np.min(box_and_score_all[:,2:]))\n  if count==0:\n    box_and_score_all=[]\n  else:\n    score=box_and_score_all[:,1]\n    y_c=(box_and_score_all[:,2]+box_and_score_all[:,4])\/2\n    x_c=(box_and_score_all[:,3]+box_and_score_all[:,5])\/2\n    height=-box_and_score_all[:,2]+box_and_score_all[:,4]\n    width=-box_and_score_all[:,3]+box_and_score_all[:,5]\n    #print(np.min(height),np.min(width))\n    box_and_score_all=NMS(box_and_score_all[:,1],box_and_score_all[:,2],box_and_score_all[:,3],box_and_score_all[:,4],box_and_score_all[:,5],iou_thresh=0.5,merge_mode=True)\n  return box_and_score_all\n\n\nprint(\"test run. 5 image\")\nall_iou_score=[]\nfor i in np.arange(0,5):\n  img=Image.open(cv_list[i][0]).convert(\"RGB\")\n  box_and_score_all=split_and_detect(model,img,cv_list[i][2],cv_list[i][3],score_thresh=0.3,iou_thresh=0.4)\n  if len(box_and_score_all)==0:\n    print(\"no box found\")\n    continue\n  true_boxes=cv_list[i][1][:,1:]#c_x,c_y,width_height\n  top=true_boxes[:,1:2]-true_boxes[:,3:4]\/2\n  left=true_boxes[:,0:1]-true_boxes[:,2:3]\/2\n  bottom=top+true_boxes[:,3:4]\n  right=left+true_boxes[:,2:3]\n  true_boxes=np.concatenate((top,left,bottom,right),axis=1)\n\n  \n\n \n  print_w, print_h = img.size\n  iou_score=check_iou_score(true_boxes,box_and_score_all[:,1:],iou_thresh=0.5)\n  all_iou_score.append(iou_score)\n  \"\"\"\n  img=draw_rectangle(box_and_score_all[:,1:],img,\"red\")\n  img=draw_rectangle(true_boxes,img,\"blue\")\n  \n  fig, axes = plt.subplots(1, 2,figsize=(15,15))\n  #axes[0].set_axis_off()\n  axes[0].imshow(img)\n  #axes[1].set_axis_off()\n  axes[1].imshow(heatmap)#, cmap='gray')\n\n  plt.show()\n  \"\"\"\nprint(\"average_score:\",np.mean(all_iou_score))","1c0dca54":"from tqdm import tqdm\ncount=0\ncrop_dir=\"\/crop_letter\/\"\nif os.path.exists(crop_dir) == False:os.mkdir(crop_dir)\n\ntrain_input=[]\npic_count=0\nfor ann_pic in tqdm(annotation_list_train):\n  pic_count+=1\n  with Image.open(ann_pic[0]) as img:\n    for ann in ann_pic[1]:#cat,center_x,center_y,width,height for each picture\n      cat=ann[0]\n      c_x=ann[1]\n      c_y=ann[2]\n      width=ann[3]\n      height=ann[4]\n      save_dir=crop_dir+str(count)+\".jpg\"\n      img.crop((int(c_x-width\/2),int(c_y-height\/2),int(c_x+width\/2),int(c_y+height\/2))).save(save_dir)\n      train_input.append([save_dir,cat])\n      count+=1\n                 ","c0444827":"df_unicode_translation=pd.read_csv(\"..\/input\/unicode_translation.csv\")\nunicode=df_unicode_translation[\"Unicode\"].values\nchar=df_unicode_translation[\"char\"].values\ndict_translation={unicode[i]:char[i] for i in range(len(unicode))}\n\ni=0\nimg = np.asarray(Image.open(train_input[i][0]).resize((32,32)).convert('RGB'))\nname = dict_translation[inv_dict_cat[str(train_input[i][1])]]\nprint(name)\nplt.imshow(img)\nplt.show()\n  ","940e3288":"input_height,input_width=32,32\n\ndef Datagen_for_classification(filenames, batch_size, is_train=True,random_crop=True):\n  x=[]\n  y=[]\n  \n  count=0\n\n  while True:\n    for i in range(len(filenames)):\n      if random_crop:\n        crop_ratio=np.random.uniform(0.8,1)\n      else:\n        crop_ratio=1\n      with Image.open(filenames[i][0]) as f:\n        \n        #random crop\n        if random_crop and is_train:\n          pic_width,pic_height=f.size\n          f=np.asarray(f.convert('RGB'),dtype=np.uint8)\n          top_offset=np.random.randint(0,pic_height-int(crop_ratio*pic_height))\n          left_offset=np.random.randint(0,pic_width-int(crop_ratio*pic_width))\n          bottom_offset=top_offset+int(crop_ratio*pic_height)\n          right_offset=left_offset+int(crop_ratio*pic_width)\n          f=cv2.resize(f[top_offset:bottom_offset,left_offset:right_offset,:],(input_height,input_width))\n        else:\n          f=f.resize((input_width, input_height))\n          f=np.asarray(f.convert('RGB'),dtype=np.uint8)          \n        x.append(f)\n      \n        y.append(int(filenames[i][1]))\n      count+=1\n      if count==batch_size:\n        x=np.array(x, dtype=np.float32)\n        y=np.identity(len(dict_cat))[y].astype(np.float32)\n\n        inputs=x\/255\n        targets=y       \n        x=[]\n        y=[]\n        count=0\n        yield inputs, targets\n        \ndef create_classification_model(input_shape, n_category):\n    input_layer = Input(input_shape)#32\n    x=cbr(input_layer,64,3,1)\n    x=resblock(x,64)\n    x=resblock(x,64)\n    x=cbr(input_layer,128,3,2)#16\n    x=resblock(x,128)\n    x=resblock(x,128)\n    x=cbr(input_layer,256,3,2)#8\n    x=resblock(x,256)\n    x=resblock(x,256)\n    x=GlobalAveragePooling2D()(x)\n    x=Dropout(0.2)(x)\n    out=Dense(n_category,activation=\"softmax\")(x)#sigmoid???catcross\u3066\u3044\u304e\n    \n    classification_model=Model(input_layer, out)\n    \n    return classification_model\n      \ndef model_fit_classification(model,train_list,cv_list,n_epoch,batch_size=32):\n    hist = model.fit_generator(\n        Datagen_for_classification(train_list,batch_size, is_train=True,random_crop=True),\n        steps_per_epoch = len(train_list) \/\/ batch_size,\n        epochs = n_epoch,\n        validation_data=Datagen_for_classification(cv_list,batch_size, is_train=False,random_crop=False),\n        validation_steps = len(cv_list) \/\/ batch_size,\n        #callbacks = [early_stopping, reduce_lr, model_checkpoint],\n        shuffle = True,\n        verbose = 1\n    )\n    return hist","9c2d2b0c":"K.clear_session()\ninput_height,input_width=32,32\nmodel=create_classification_model(input_shape=(input_height,input_width,3),n_category=len(dict_cat))\n\"\"\"\n\n# EarlyStopping\nearly_stopping = EarlyStopping(monitor = 'val_loss', min_delta=0, patience = 60, verbose = 1)\n# ModelCheckpoint\nweights_dir = '.\/model_3\/'\n\nif os.path.exists(weights_dir) == False:os.mkdir(weights_dir)\nmodel_checkpoint = ModelCheckpoint(weights_dir + \"val_loss{val_loss:.3f}.hdf5\", monitor = 'val_loss', verbose = 1,\n                                      save_best_only = True, save_weights_only = True, period = 1)\n# reduce learning rate\nreduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.5, patience = 10, verbose = 1)\n\"\"\"\n\nprint(model.summary())","345a9de9":"train_list, cv_list = train_test_split(train_input, random_state = 111,test_size = 0.2)\nlearning_rate=0.005\nn_epoch=10\nbatch_size=64\n\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=learning_rate),metrics=[\"accuracy\"])\nhist = model_fit_classification(model,train_list,cv_list,n_epoch,batch_size)\n\nmodel.save_weights('final_weights_step3.h5')","eafb2d2a":"for i in range(3):\n  img = np.asarray(Image.open(train_input[i][0]).resize((32,32)).convert('RGB'))\n  predict=np.argmax(model.predict(img.reshape(1,32,32,3)\/255),axis=1)[0]\n  name = dict_translation[inv_dict_cat[str(predict)]]\n  print(name)\n  plt.imshow(img)\n  plt.show()","e81c08d6":"from tqdm import tqdm\n\nK.clear_session()\nprint(\"loading models...\")\nmodel_1=create_model(input_shape=(512,512,3),size_detection_mode=True)\n#model_1.load_weights('final_weights_step1.h5')\nmodel_1.load_weights('final_weights_step1.hdf5')\n\nmodel_2=create_model(input_shape=(512,512,3),size_detection_mode=False)\nmodel_2.load_weights('final_weights_step2.h5')\n\nmodel_3=create_classification_model(input_shape=(32,32,3),n_category=len(dict_cat))\nmodel_3.load_weights('final_weights_step3.h5')\n\n\ndef pipeline(i,print_img=False):\n  # model1: determine how to split image\n  if print_img: print(\"model 1\")\n  img = np.asarray(Image.open(id_test[i]).resize((512,512)).convert('RGB'))\n  predicted_size=model_1.predict(img.reshape(1,512,512,3)\/255)\n  detect_num_h=aspect_ratio_pic_all_test[i]*np.exp(-predicted_size\/2)\n  detect_num_w=detect_num_h\/aspect_ratio_pic_all_test[i]\n  h_split_recommend=np.maximum(1,detect_num_h\/base_detect_num_h)\n  w_split_recommend=np.maximum(1,detect_num_w\/base_detect_num_w)\n  if print_img: print(\"recommended split_h:{}, split_w:{}\".format(h_split_recommend,w_split_recommend))\n\n  # model2: detection\n  if print_img: print(\"model 2\")\n  img=Image.open(id_test[i]).convert(\"RGB\")\n  box_and_score_all=split_and_detect(model_2,img,h_split_recommend,w_split_recommend,score_thresh=0.3,iou_thresh=0.4)#output:score,top,left,bottom,right\n  if print_img: print(\"find {} boxes\".format(len(box_and_score_all)))\n  print_w, print_h = img.size\n  if (len(box_and_score_all)>0) and print_img: \n    img=draw_rectangle(box_and_score_all[:,1:],img,\"red\")\n    plt.imshow(img)\n    plt.show()\n\n  # model3: classification\n  count=0\n  if (len(box_and_score_all)>0):\n    for box in box_and_score_all[:,1:]:\n      top,left,bottom,right=box\n      img_letter=img.crop((int(left),int(top),int(right),int(bottom))).resize((32,32))#\u5927\u304d\u76ee\u306e\u30d4\u30af\u30bb\u30eb\u306e\u304c\u3044\u3044\u304b\uff1f\n      predict=(model_3.predict(np.asarray(img_letter).reshape(1,32,32,3)\/255))\n      predict=np.argmax(predict,axis=1)[0]\n      code=inv_dict_cat[str(predict)]\n      c_x=int((left+right)\/2)\n      c_y=int((top+bottom)\/2)\n      if count==0:\n        ans=code+\" \"+str(c_x)+\" \"+str(c_y)\n      else:\n        ans=ans+\" \"+code+\" \"+str(c_x)+\" \"+str(c_y)\n      count+=1\n  else:\n    ans=\"\"\n  return ans\n\n_=pipeline(0,print_img=True)\n\n#I'm sorry. Not nice coding. Time consuming.\nfor i in tqdm(range(len(id_test))):\n  ans=pipeline(i,print_img=False)\n  df_submission.set_value(i, 'labels', ans)\n      \ndf_submission.to_csv(\"submission.csv\",index=False)\n","9e8da5b6":"df_submission.head()","ba5c61ce":"**Goal of stage2 is to detect the letters by CenterNet.**","a451d245":"### Create Model","7f9b2090":"### Result","adc3eefe":"## Introduction","3aa17e99":"First of all, let's convert the input data into the labels for CenterNet.\n\nCenterNet\u7528\u306b\u5165\u529b\u30c7\u30fc\u30bf\u3092\u5f62\u6210\u3057\u3066\u304a\u304d\u307e\u3059\u3002","79ca1a22":"You can see the average size of the objects varies among pictures. So it would be better to split the picture into several parts. So cropping pictures with appropriate ratio would be important when detecting letters.  One way to find the best cropping size is creating the model to predict the average letter size.\n\n\u56f3\u306f\u3001\u6587\u5b57\u30b5\u30a4\u30ba\u00f7\u30d4\u30af\u30c1\u30e3\u30b5\u30a4\u30ba\u306e\u6bd4\u7387\u306e\u5206\u5e03\u3092\u793a\u3057\u305f\u3082\u306e\u3067\u3059\u304c\u3001\u30d4\u30af\u30c1\u30e3\u6bce\u306b\u7d50\u69cb\u3070\u3089\u3064\u3044\u3066\u3044\u307e\u3059\u3002\u76f8\u5bfe\u7684\u306a\u6587\u5b57\u306e\u5927\u304d\u3055\u304c\u5927\u304d\u304f\u306a\u3063\u305f\u308a\u5c0f\u3055\u304f\u306a\u3063\u305f\u308a\u3059\u308b\u3088\u308a\u3082\u7d71\u4e00\u3055\u308c\u3066\u3044\u305f\u65b9\u304c\u691c\u51fa\u3057\u3084\u3059\u3044\u306e\u3067\u3001\u7d71\u4e00\u65b9\u6cd5\u3092\u8003\u3048\u307e\u3059\u3002\u3053\u3053\u3067\u306f\u3001\u3084\u3084\u5f37\u5f15\u306b\u3001CNN\u306b\u6587\u5b57\u306e\u5e73\u5747\u30b5\u30a4\u30ba\u3092\u7b97\u51fa\u3057\u3066\u3082\u3089\u3048\u308b\u304b\u8a66\u3057\u3066\u307f\u308b\u3053\u3068\u306b\u3057\u307e\u3059\u3002","47760597":"Many studies on **Keypoint based detector** are conducted these days. One of the well-known keypoint-based detectors is **CornerNet**. The concept of ConerNet is predicting the coner of the bounding boxes using heatmaps like semantic segmentation. This detector is relatively easy to implement since it **does not need multiple ankers** unlike other well-known one-stage detectors such as RCNN, YOLO and SSD. \n**CenterNet (Object as Points)** I show here is one of keypoint based detectors derieved from CornerNet. CenterNet directly predicts the center pixel of the object with heatmap. It's quite similar to YOLO with single anker, but the way of using heatmap is interesting.\nFor further information, please refer to the original papers.\n\n\n*   CornerNet:https:\/\/arxiv.org\/abs\/1808.01244\n*   CenterNet (Object as Points):https:\/\/arxiv.org\/abs\/1904.07850\n\nI'm newbie on image detection\/classification task.\nPlease let me know if you find any mistake. Thanks!","487bc000":"## Test & Submit","0c5bbb99":"Then, let's convert the outputs into bounding boxes. I use NMS (Non Maximum Suppression) to find the best boxes. \n(The original paper says that the max pooling also works well instead of NMS.)\n\n\u51fa\u529b\u3092\u30d0\u30a6\u30f3\u30c7\u30a3\u30f3\u30b0\u30dc\u30c3\u30af\u30b9\u306b\u3057\u307e\u3059\u3002128x128\u500b\u306e\u30d0\u30a6\u30f3\u30c7\u30a3\u30f3\u30b0\u30dc\u30c3\u30af\u30b9\u304c\u3042\u308b\u3053\u3068\u306b\u306a\u308b\u306e\u3067\u3001NMS\u3067\u51e6\u7406\u3057\u307e\u3059\u3002\n(\u5143\u306e\u8ad6\u6587\u3067\u306f\u5468\u56f28\u30de\u30b9\u3092max pooling\u3067\u51e6\u7406\u3059\u308b\u65b9\u6cd5\u3082\u63d0\u6848\u3055\u308c\u3066\u3044\u307e\u3059)","2a7f334e":"Let's check a result of heatmap with validation data. You can see the centers of letters are detected.\n\n\u4e2d\u5fc3\u70b9\u306e\u30d2\u30fc\u30c8\u30de\u30c3\u30d7\u306e\u51fa\u529b\u4f8b\u3067\u3059\u3002\u306a\u3093\u3068\u306a\u304f\u6587\u5b57\u306e\u4e2d\u5fc3\u304c\u8272\u6fc3\u304f\u306a\u3063\u3066\u3044\u307e\u3059\u3002","a02b06ae":"Start training. This kernel runs 30 epochs. Longer training can improve the accuracy.\n\n\u3068\u308a\u3042\u3048\u305a30epoch\u307b\u3069\u8a08\u7b97\u3057\u307e\u3059\u304c\u3001\u591a\u5206\u3082\u3063\u3068\u9577\u3044\u65b9\u304c\u3088\u3044\u3067\u3059\u3002","6714c3c8":"### Training","a80efdc7":"## STEP 3: Classification","d906c450":"I'd like to change the generator and model. The differences from step_1 are as follows:\n\n\n*   Add random cropping to generator\n*  Make target data for CenterNet\n*   Add decoder to the network (similar to U-Net)\n\n**Input of CenterNet:** \n\n>Cropped image resized into 512x512x3\n\n\n**Output of CenterNet:** \n\n>Heatmap of center point 128x128x1\n\n>x-offset and y-offset of the center point inside the detected cell 128x128x2\n\n>width and height of the the detected object 128x128x2\n\nIt should be noted that this network can cope with multiple categories by increasing the number of output layers of center point. \n\n","2ff7ab65":"## STEP 1: Preprocessing (Check Object Size)","21df2bef":"### Training","7cafc8c4":"Based on the detector's size, split numbers are determined for each picture. In the next section I will make the CenterNet whose output shape is 128x128. Assuming the letters are detected every 5 pixcels, this detector can find the letters 25x25 at most.\n\n\u5f8c\u8ff0\u3059\u308bCenterNet\u3067\u306f\u51fa\u529b\u3092128x128\u306e\u30d2\u30fc\u30c8\u30de\u30c3\u30d7\u306b\u3057\u307e\u3059\u3002\u691c\u51fa\u3067\u304d\u308b\u306e\u306f\u305b\u3044\u305c\u304425x25\u304f\u3089\u3044\u3060\u3068\u8003\u3048\u3066\u3001\u5143\u306e\u753b\u50cf\u30c7\u30fc\u30bf\u306e\u5206\u5272\u6570\u3092\u9069\u5f53\u306b\u6c7a\u3081\u307e\u3059\u3002","54fb7b4d":"- Left image is the original picture(cropped).\n\n- Middle one is the target image in which center points are 1, other pixcels are 0.\n\n- Right one shows center points with gaussian distributions.\n\nRight image is necessary to reduce the training loss when the model detect the points near the exact center. \n\n\u771f\u3093\u4e2d\u304c1-0\u306e\u8a13\u7df4\u7528\u30bf\u30fc\u30b2\u30c3\u30c8\u30c7\u30fc\u30bf\u3067\u3059\u3002\u53f3\u5074\u304c\u305d\u308c\u306b\u30ac\u30a6\u30b9\u5206\u5e03\u3092\u3042\u3066\u305f\u3082\u306e\u3067\u3001\u691c\u51fa\u5668\u304c\u4e2d\u5fc3\u306e\u8fd1\u508d\u70b9\u3092\u691c\u51fa\u3057\u305f\u5834\u5408\u306e\u30ed\u30b9\u3092\u4f4e\u6e1b\u3059\u308b\u305f\u3081\u306b\u4f7f\u7528\u3057\u307e\u3059\u3002","edc0b5af":"Step1 is not main topic of this kernel. Run only 15 epochs.\n\n\u672c\u30ab\u30fc\u30cd\u30eb\u306e\u4e3b\u76ee\u7684\u306fstep2(CenterNet)\u306a\u306e\u3067\u3001\u5c11\u3057\u77ed\u3081\u306e\u30a8\u30dd\u30c3\u30af\u3067\u5207\u308a\u4e0a\u3052\u307e\u3059\u3002","9fadaba6":"Only 10 epoch. Execution time is limited...\n\n10epoch\u307b\u3069\u306b\u3057\u307e\u3059\u304c\u3001\u5b66\u7fd2\u7387\u4e0b\u3052\u308b\u306a\u3069\u3057\u3066\u3082\u3046\u5c11\u3057\u9811\u5f35\u308b\u307b\u3046\u304c\u3088\u3044\u3068\u601d\u308f\u308c\u307e\u3059\u3002","293e842c":"### Training","b4386695":"## STEP 2: Detection by CenterNet","a59e8de6":"crop all letters in advance.\n\n\u6587\u5b57\u90e8\u5206\u3092\u5168\u90e8\u30af\u30ed\u30c3\u30d7\u3057\u3066\u3057\u307e\u3044\u307e\u3059\u3002","a235ea90":"*-----This kernel is written in both English and Japanese.------*\n\n\u521d\u5fc3\u8005\u30ab\u30fc\u30cd\u30eb\u3067\u3059\u304c\u3001\u65e5\u672c\u8a9e\u3067\u3082\u4e26\u8a18\u3057\u307e\u3059\u3002\u7686\u69d8\u306e\u3054\u53c2\u8003\u306b\u306a\u308c\u3070\u5e78\u3044\u3067\u3059\u3002\n\n\u672c\u30ab\u30fc\u30cd\u30eb\u3067\u306f\u3001\u6700\u8fd1\u8a71\u984c\u306b\u306a\u3063\u3066\u3044\u308b\u30ad\u30fc\u30dd\u30a4\u30f3\u30c8\u30d9\u30fc\u30b9\u306e\u691c\u51fa\u5668\u3092\u8a66\u3057\u3066\u307f\u307e\u3057\u305f\u3002CornerNet\u6d3e\u751f\u306e\u300eCenterNet\u300f\u3068\u547c\u3070\u308c\u308b\u3082\u306e\u3067\u3001YOLO\u306a\u3069\u306e\u3088\u3046\u306b\u30a2\u30f3\u30ab\u30fc\u3092\u4f7f\u7528\u305b\u305a\u3001\u30bb\u30b0\u30e1\u30f3\u30c6\u30fc\u30b7\u30e7\u30f3(U-Net)\u306e\u3088\u3046\u306a\u30d2\u30fc\u30c8\u30de\u30c3\u30d7\u3067\u5bfe\u8c61\u7269\u306e\u4e2d\u5fc3\u70b9\u3092\u691c\u51fa\u3059\u308b\u624b\u6cd5\u3067\u3059\u3002(\u30b7\u30f3\u30b0\u30eb\u30a2\u30f3\u30ab\u30fc\u306e\u3088\u3046\u306a\u96f0\u56f2\u6c17\u3067\u3059\u304c\u3001\u30d2\u30fc\u30c8\u30de\u30c3\u30d7\u3060\u3051\u3067\u3044\u3044\u306e\u3067\u5b9f\u88c5\u3057\u3084\u3059\u3044\u5370\u8c61\u3067\u3059)\n\nDeNA\u3055\u3093\u306f\u3058\u3081\u3068\u3057\u305f\u3001\u69d8\u3005\u306a\u65e5\u672c\u8a9e\u306e\u8a18\u4e8b\u3067\u3082\u52c9\u5f37\u3055\u305b\u3066\u3044\u305f\u3060\u3044\u3066\u304a\u308a\u307e\u3059\u306e\u3067\u3001\u3053\u306e\u5834\u3092\u501f\u308a\u3066\u304a\u793c\u7533\u3057\u4e0a\u3052\u307e\u3059\u3002","76e5c622":"Umm... not so good. Training is not enough. But I might as well use it.\n\n\u30a4\u30de\u30a4\u30c1\u3067\u3059\u304c\u3001\u3068\u308a\u3042\u3048\u305a\u4f7f\u3063\u3066\u304a\u304d\u307e\u3059\u3002\u306a\u304a\u3001\u5b66\u7fd2\u3092\u3082\u3046\u5c11\u3057\u7d9a\u3051\u308c\u3070\u3082\u3046\u5c11\u3057\u30de\u30c8\u30e2\u306b\u306f\u306a\u308a\u307e\u3059\u3002","8c624709":"### Result","77bcf86a":"\u3053\u3053\u304b\u3089CenterNet\u306b\u306a\u308a\u307e\u3059\u3002step1\u3068\u306e\u30e2\u30c7\u30eb\u306e\u9055\u3044\u306f\n* \u30e9\u30f3\u30c0\u30e0\u30af\u30ed\u30c3\u30d4\u30f3\u30b0\u306e\u8ffd\u52a0\n* \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u306e\u30bf\u30fc\u30b2\u30c3\u30c8\u30c7\u30fc\u30bf\u3092CenterNet\u7528\u306b\u52a0\u5de5\n* CNN\u306b\u30c7\u30b3\u30fc\u30c0\u90e8\u5206\u3092\u8ffd\u52a0(U-Net\u307f\u305f\u3044\u306a\u611f\u3058)\n\n\u5165\u529b\uff1a512x512x3\u306e\u30a4\u30e1\u30fc\u30b8\u3001\n\n\u51fa\u529b\uff1a128x128x1\u306e\u4e2d\u5fc3\u4f4d\u7f6e\u30d2\u30fc\u30c8\u30de\u30c3\u30d7\u3001x,y\u4e2d\u5fc3\u30aa\u30d5\u30bb\u30c3\u30c8128x128x2\u3001\u7269\u4f53\u5e45\u3068\u9ad8\u3055128x128x2\u306e\u5408\u8a085\u5c64\u306b\u306a\u308a\u307e\u3059\n","8ea298b5":"Before creating the model, I'd like to show the example image of target heatmap showing center points of objects.","b5f926f3":"Not so bad? The letters in the pictures may be too small considering the output size(heatmap) of 128x128. \n\nSo let's split pictures into several parts using the results in step1, then run CenterNet for each splitted picture. After then, integrate them and run NMS.\n\n\u306a\u3093\u3068\u306a\u304f\u691c\u51fa\u3067\u304d\u3066\u3044\u307e\u3059\u304c\u3001\u3053\u306e\u6642\u70b9\u3067\u306f\u5143\u753b\u50cf\u3092\u5206\u5272\u3057\u3066\u3044\u306a\u3044\u306e\u3067\u3001\u6587\u5b57\u30b5\u30a4\u30ba\u304c\u51fa\u529b\u30b5\u30a4\u30ba(128x128)\u306b\u5bfe\u3057\u3066\u5c0f\u3055\u3059\u304e\u307e\u3059\u3002\nStep1\u3067\u5f97\u305f\u5206\u5272\u6570\u3092\u9069\u5f53\u306b\u4f7f\u3063\u3066\u3001\u5206\u5272\u3057\u305f\u753b\u50cf\u306b\u5bfe\u3057\u3066\u305d\u308c\u305e\u308cCenterNet\u306b\u304b\u3051\u3066\u3001\u5f97\u3089\u308c\u305f\u30c7\u30fc\u30bf\u3092\u3059\u3079\u3066\u3072\u3063\u304f\u308b\u3081\u3066NMS\u306b\u6295\u3052\u308b\u3053\u3068\u306b\u3057\u307e\u3059\u3002","ce7b4442":"### Create Model & Training Set","ae04edd7":"**Goal of Step1 is to check the object size and determine the size of input image for detector.**","83ca2c07":"show example of cropped picture.","64186fa5":"OK. Score has improved.","9d9830ef":"**Goal of stage3 is to classify the letters.**\n\nSince this is not main topic of this kernel, I apply simple classification with CNN and skip any pre\/postprocessing. I don't care inbalanced data as well.\n\n\u6700\u5f8c\u306f\u6587\u5b57\u306e\u5206\u985e\u3067\u3059\u3002CenterNet\u306e\u691c\u51fa\u304c\u672c\u30ab\u30fc\u30cd\u30eb\u4e3b\u984c\u306a\u306e\u3067\u3001\u304b\u306a\u308a\u3044\u3044\u52a0\u6e1b\u306a\u611f\u3058\u3067\u3059\u3002\u3054\u3081\u3093\u306a\u3055\u3044\u3002","b5eaa675":"### Result","78b946d9":"Let's check some results.","873a710d":"### Create Model & Training Set","9d9ea7a0":"Thank you for reading!","4f3a991b":"Let's create CNN model(almost ResNet).\n\n*   Input: Image (resized into 512x512x3)\n*   Output: Ratio of letter_size to picture_size\n\nResNet\u3063\u307d\u3044\u30e2\u30c7\u30eb\u3067\u8a66\u3057\u3066\u307f\u307e\u3059\u3002\u5165\u529b\u306f1\u30da\u30fc\u30b8\u3054\u3068\u306e\u753b\u50cf\u3002\u51fa\u529b\u306f\u6587\u5b57\u3068\u30d4\u30af\u30c1\u30e3\u306e\u30b5\u30a4\u30ba\u6bd4\u3068\u3057\u307e\u3059\u3002","254aef24":"I'd like to check the size of objects before creating the model. If the object size is too small, detector cannot find the target. \n\n\u307e\u305a\u3001\u691c\u51fa\u30e2\u30c7\u30eb\u3092\u4f5c\u308b\u524d\u306b\u3001\u6587\u5b57\u30b5\u30a4\u30ba\u3092\u30c1\u30a7\u30c3\u30af\u3057\u3066\u304a\u304d\u307e\u3059\u3002CenterNet\u306e\u51fa\u529b\u65b9\u5f0f\u306b\u5bfe\u3057\u3066\u904e\u5c11\u306b\u5c0f\u3055\u3044\u6587\u5b57\u306f\u3001\u691c\u51fa\u3067\u304d\u307e\u305b\u3093\u306e\u3067\u3002"}}