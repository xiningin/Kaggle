{"cell_type":{"f8cc4bcc":"code","457f076c":"code","29322f18":"code","a3738582":"code","fd252506":"code","d60ebd1b":"code","701ebc22":"code","87b2d35c":"code","343285e0":"code","23b56157":"code","2b5a7fbc":"code","41d39e47":"code","b3bb7428":"code","fdbff5b1":"code","2ec97aaa":"code","f31b30ab":"code","41c5de2a":"code","0e5a875b":"code","3a994e05":"code","6cac0b6a":"code","89ad5127":"code","71d6c4c8":"code","d75a47fa":"code","5b211895":"code","f050dcac":"code","09a7c3fb":"code","fca67ea7":"code","a908e15d":"code","f3cf0776":"code","c73fd87b":"code","acb695c7":"code","a8bbe90c":"code","ddc92340":"code","db4865a7":"code","14599945":"code","11ea8056":"code","f77df092":"code","e09749ba":"code","6d9aed71":"code","2dd9ade1":"code","7d1d3701":"code","6bdfd0b8":"code","3b22075c":"code","a95be489":"code","247d5ed6":"code","b3745bfa":"code","ce1281a9":"code","eab0c69b":"code","7a5d1959":"code","9ea08c56":"code","07ebd01e":"code","93804965":"code","3ba446ce":"code","323527af":"code","212d61dc":"code","17686ff8":"code","4cf0ad51":"code","f8489257":"code","cfa8a081":"code","19e551fb":"code","c99609d2":"code","88232332":"code","8e6c9518":"code","fecc698d":"code","f18b5a77":"code","393c5edf":"code","39b83058":"code","620a114d":"code","4b91fc30":"code","0b5ec671":"code","e4dcce1d":"code","b902a63f":"code","f8b608a2":"code","28bce4f4":"code","943e1df8":"code","fc107d5b":"code","9e746d7c":"code","b13ee446":"code","14056485":"code","eb11788c":"code","eb2feb2f":"code","0c882b7d":"code","7c15ef59":"code","dd6ba9dd":"code","71566325":"code","2134d5b5":"code","c77bd0f5":"code","5841dce7":"code","87d10b62":"code","165d770c":"code","21539489":"code","12c43f03":"code","0bc122bd":"code","d2897338":"code","86a1eed0":"code","6a3e8c42":"code","53d4a088":"code","db058080":"code","da275078":"markdown","c5927746":"markdown","db72dc55":"markdown","4f69523f":"markdown","7cf7fe84":"markdown","861683f8":"markdown","b525c3ec":"markdown","1f193636":"markdown","239b18fb":"markdown","341d59ff":"markdown","d11f8ee5":"markdown","cdd7fe9a":"markdown","996e29b2":"markdown","a0ac3b03":"markdown","5acb62b7":"markdown","cce51cbe":"markdown","38c43a6d":"markdown","23c19c61":"markdown","1acf900f":"markdown","8d1983c5":"markdown","3cc4a3c6":"markdown","6fa5df2c":"markdown","c275a000":"markdown","ca377b69":"markdown","1e9082ac":"markdown","1b0a6892":"markdown","1f5260b0":"markdown","d4a52357":"markdown","9df1cf34":"markdown","79b3ac43":"markdown","63c1d859":"markdown","0d0d6abe":"markdown","53a2cc18":"markdown","d2e1ced5":"markdown","94c341be":"markdown","78a781a2":"markdown","9040debe":"markdown","3a16fb8b":"markdown","762254ce":"markdown","d0996880":"markdown","5c430d86":"markdown","5b16ea9f":"markdown","7cbf2bfc":"markdown","6a4f8875":"markdown","d1a0fc03":"markdown","18363cac":"markdown","066a1464":"markdown","b5487f05":"markdown","b4497ba3":"markdown","13106845":"markdown","5f12d9c8":"markdown","4329e57a":"markdown","7c7efa9d":"markdown","ececfa02":"markdown","86b1f416":"markdown","76ef8ed6":"markdown","dadcd409":"markdown","1802eece":"markdown","2b967464":"markdown","3a893f68":"markdown","d515cccb":"markdown"},"source":{"f8cc4bcc":"import pandas as pd\nimport numpy as np\nimport sys\nimport sklearn\nprint(pd.__version__)\nprint(np.__version__)\nprint(sys.version)\nprint(sklearn.__version__)","457f076c":"# attach the column names to the dataset\ncol_names = [\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\n    \"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\n    \"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\n    \"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n    \"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n    \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n    \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n    \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n    \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n    \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"label\"]\n\n# KDDTrain+_2.csv & KDDTest+_2.csv are the datafiles without the last column about the difficulty score\n# these have already been removed.\ndf = pd.read_csv(\"..\/input\/kddddd\/KDDTrain_2.csv\", header=None, names = col_names)\ndf_test = pd.read_csv(\"..\/input\/kddtttttt\/KDDTest_2.csv\", header=None, names = col_names)\n\n# shape, this gives the dimensions of the dataset\nprint('Dimensions of the Training set:',df.shape)\nprint('Dimensions of the Test set:',df_test.shape)","29322f18":"# first five rows\ndf.head(5)","a3738582":"df.describe()","fd252506":"print('Label distribution Training set:')\nprint(df['label'].value_counts())\nprint()\nprint('Label distribution Test set:')\nprint(df_test['label'].value_counts())","d60ebd1b":"# colums that are categorical and not binary yet: protocol_type (column 2), service (column 3), flag (column 4).\n# explore categorical features\nprint('Training set:')\nfor col_name in df.columns:\n    if df[col_name].dtypes == 'object' :\n        unique_cat = len(df[col_name].unique())\n        print(\"Feature '{col_name}' has {unique_cat} categories\".format(col_name=col_name, unique_cat=unique_cat))\n\n#see how distributed the feature service is, it is evenly distributed and therefore we need to make dummies for all.\nprint()\nprint('Distribution of categories in service:')\nprint(df['service'].value_counts().sort_values(ascending=False).head())","701ebc22":"# Test set\nprint('Test set:')\nfor col_name in df_test.columns:\n    if df_test[col_name].dtypes == 'object' :\n        unique_cat = len(df_test[col_name].unique())\n        print(\"Feature '{col_name}' has {unique_cat} categories\".format(col_name=col_name, unique_cat=unique_cat))","87b2d35c":"from sklearn.preprocessing import LabelEncoder,OneHotEncoder\ncategorical_columns=['protocol_type', 'service', 'flag']\n# insert code to get a list of categorical columns into a variable, categorical_columns\ncategorical_columns=['protocol_type', 'service', 'flag'] \n # Get the categorical values into a 2D numpy array\ndf_categorical_values = df[categorical_columns]\ntestdf_categorical_values = df_test[categorical_columns]\ndf_categorical_values.head()","343285e0":"# protocol type\nunique_protocol=sorted(df.protocol_type.unique())\nstring1 = 'Protocol_type_'\nunique_protocol2=[string1 + x for x in unique_protocol]\n# service\nunique_service=sorted(df.service.unique())\nstring2 = 'service_'\nunique_service2=[string2 + x for x in unique_service]\n# flag\nunique_flag=sorted(df.flag.unique())\nstring3 = 'flag_'\nunique_flag2=[string3 + x for x in unique_flag]\n# put together\ndumcols=unique_protocol2 + unique_service2 + unique_flag2\nprint(dumcols)\n\n#do same for test set\nunique_service_test=sorted(df_test.service.unique())\nunique_service2_test=[string2 + x for x in unique_service_test]\ntestdumcols=unique_protocol2 + unique_service2_test + unique_flag2","23b56157":"df_categorical_values_enc=df_categorical_values.apply(LabelEncoder().fit_transform)\nprint(df_categorical_values_enc.head())\n# test set\ntestdf_categorical_values_enc=testdf_categorical_values.apply(LabelEncoder().fit_transform)","2b5a7fbc":"enc = OneHotEncoder()\ndf_categorical_values_encenc = enc.fit_transform(df_categorical_values_enc)\ndf_cat_data = pd.DataFrame(df_categorical_values_encenc.toarray(),columns=dumcols)\n# test set\ntestdf_categorical_values_encenc = enc.fit_transform(testdf_categorical_values_enc)\ntestdf_cat_data = pd.DataFrame(testdf_categorical_values_encenc.toarray(),columns=testdumcols)\n\ndf_cat_data.head()","41d39e47":"trainservice=df['service'].tolist()\ntestservice= df_test['service'].tolist()\ndifference=list(set(trainservice) - set(testservice))\nstring = 'service_'\ndifference=[string + x for x in difference]\ndifference","b3bb7428":"for col in difference:\n    testdf_cat_data[col] = 0\n\ntestdf_cat_data.shape","fdbff5b1":"newdf=df.join(df_cat_data)\nnewdf.drop('flag', axis=1, inplace=True)\nnewdf.drop('protocol_type', axis=1, inplace=True)\nnewdf.drop('service', axis=1, inplace=True)\n# test data\nnewdf_test=df_test.join(testdf_cat_data)\nnewdf_test.drop('flag', axis=1, inplace=True)\nnewdf_test.drop('protocol_type', axis=1, inplace=True)\nnewdf_test.drop('service', axis=1, inplace=True)\nprint(newdf.shape)\nprint(newdf_test.shape)","2ec97aaa":"# take label column\nlabeldf=newdf['label']\nlabeldf_test=newdf_test['label']\n# change the label column\nnewlabeldf=labeldf.replace({ 'normal' : 0, 'neptune' : 1 ,'back': 1, 'land': 1, 'pod': 1, 'smurf': 1, 'teardrop': 1,'mailbomb': 1, 'apache2': 1, 'processtable': 1, 'udpstorm': 1, 'worm': 1,\n                           'ipsweep' : 2,'nmap' : 2,'portsweep' : 2,'satan' : 2,'mscan' : 2,'saint' : 2\n                           ,'ftp_write': 3,'guess_passwd': 3,'imap': 3,'multihop': 3,'phf': 3,'spy': 3,'warezclient': 3,'warezmaster': 3,'sendmail': 3,'named': 3,'snmpgetattack': 3,'snmpguess': 3,'xlock': 3,'xsnoop': 3,'httptunnel': 3,\n                           'buffer_overflow': 4,'loadmodule': 4,'perl': 4,'rootkit': 4,'ps': 4,'sqlattack': 4,'xterm': 4})\nnewlabeldf_test=labeldf_test.replace({ 'normal' : 0, 'neptune' : 1 ,'back': 1, 'land': 1, 'pod': 1, 'smurf': 1, 'teardrop': 1,'mailbomb': 1, 'apache2': 1, 'processtable': 1, 'udpstorm': 1, 'worm': 1,\n                           'ipsweep' : 2,'nmap' : 2,'portsweep' : 2,'satan' : 2,'mscan' : 2,'saint' : 2\n                           ,'ftp_write': 3,'guess_passwd': 3,'imap': 3,'multihop': 3,'phf': 3,'spy': 3,'warezclient': 3,'warezmaster': 3,'sendmail': 3,'named': 3,'snmpgetattack': 3,'snmpguess': 3,'xlock': 3,'xsnoop': 3,'httptunnel': 3,\n                           'buffer_overflow': 4,'loadmodule': 4,'perl': 4,'rootkit': 4,'ps': 4,'sqlattack': 4,'xterm': 4})\n# put the new label column back\nnewdf['label'] = newlabeldf\nnewdf_test['label'] = newlabeldf_test\nprint(newdf['label'].head())","f31b30ab":"to_drop_DoS = [2,3,4]\nto_drop_Probe = [1,3,4]\nto_drop_R2L = [1,2,4]\nto_drop_U2R = [1,2,3]\nDoS_df=newdf[~newdf['label'].isin(to_drop_DoS)];\nProbe_df=newdf[~newdf['label'].isin(to_drop_Probe)];\nR2L_df=newdf[~newdf['label'].isin(to_drop_R2L)];\nU2R_df=newdf[~newdf['label'].isin(to_drop_U2R)];\n\n#test\nDoS_df_test=newdf_test[~newdf_test['label'].isin(to_drop_DoS)];\nProbe_df_test=newdf_test[~newdf_test['label'].isin(to_drop_Probe)];\nR2L_df_test=newdf_test[~newdf_test['label'].isin(to_drop_R2L)];\nU2R_df_test=newdf_test[~newdf_test['label'].isin(to_drop_U2R)];\nprint('Train:')\nprint('Dimensions of DoS:' ,DoS_df.shape)\nprint('Dimensions of Probe:' ,Probe_df.shape)\nprint('Dimensions of R2L:' ,R2L_df.shape)\nprint('Dimensions of U2R:' ,U2R_df.shape)\nprint('Test:')\nprint('Dimensions of DoS:' ,DoS_df_test.shape)\nprint('Dimensions of Probe:' ,Probe_df_test.shape)\nprint('Dimensions of R2L:' ,R2L_df_test.shape)\nprint('Dimensions of U2R:' ,U2R_df_test.shape)","41c5de2a":"# Split dataframes into X & Y\n# assign X as a dataframe of feautures and Y as a series of outcome variables\nX_DoS = DoS_df.drop('label',1)\nY_DoS = DoS_df.label\nX_Probe = Probe_df.drop('label',1)\nY_Probe = Probe_df.label\nX_R2L = R2L_df.drop('label',1)\nY_R2L = R2L_df.label\nX_U2R = U2R_df.drop('label',1)\nY_U2R = U2R_df.label\n# test set\nX_DoS_test = DoS_df_test.drop('label',1)\nY_DoS_test = DoS_df_test.label\nX_Probe_test = Probe_df_test.drop('label',1)\nY_Probe_test = Probe_df_test.label\nX_R2L_test = R2L_df_test.drop('label',1)\nY_R2L_test = R2L_df_test.label\nX_U2R_test = U2R_df_test.drop('label',1)\nY_U2R_test = U2R_df_test.label","0e5a875b":"colNames=list(X_DoS)\ncolNames_test=list(X_DoS_test)","3a994e05":"from sklearn import preprocessing\nscaler1 = preprocessing.StandardScaler().fit(X_DoS)\nX_DoS=scaler1.transform(X_DoS) \nscaler2 = preprocessing.StandardScaler().fit(X_Probe)\nX_Probe=scaler2.transform(X_Probe) \nscaler3 = preprocessing.StandardScaler().fit(X_R2L)\nX_R2L=scaler3.transform(X_R2L) \nscaler4 = preprocessing.StandardScaler().fit(X_U2R)\nX_U2R=scaler4.transform(X_U2R) \n# test data\nscaler5 = preprocessing.StandardScaler().fit(X_DoS_test)\nX_DoS_test=scaler5.transform(X_DoS_test) \nscaler6 = preprocessing.StandardScaler().fit(X_Probe_test)\nX_Probe_test=scaler6.transform(X_Probe_test) \nscaler7 = preprocessing.StandardScaler().fit(X_R2L_test)\nX_R2L_test=scaler7.transform(X_R2L_test) \nscaler8 = preprocessing.StandardScaler().fit(X_U2R_test)\nX_U2R_test=scaler8.transform(X_U2R_test) ","6cac0b6a":"print(X_DoS.std(axis=0))","89ad5127":"X_Probe.std(axis=0);\nX_R2L.std(axis=0);\nX_U2R.std(axis=0);","71d6c4c8":"#univariate feature selection with ANOVA F-test. using secondPercentile method, then RFE\n#Scikit-learn exposes feature selection routines as objects that implement the transform method\n#SelectPercentile: removes all but a user-specified highest scoring percentage of features\n#f_classif: ANOVA F-value between label\/feature for classification tasks.\n# from sklearn.feature_selection import SelectPercentile, f_classif\n# np.seterr(divide='ignore', invalid='ignore');\n# selector=SelectPercentile(f_classif, percentile=10)\n# X_newDoS = selector.fit_transform(X_DoS,Y_DoS)\n# X_newDoS.shape\n# Applying PCA function on training \n# and testing set of X component \nfrom sklearn.decomposition import PCA \n\npca = PCA(n_components = 8) \n\nX_newDoS = pca.fit_transform(X_DoS) \n# X_test = pca.transform(X_test) \nX_newDoS.shape\n\nexplained_variance = pca.explained_variance_ratio_ \nX_newDoS.shape","d75a47fa":"# true=selector.get_support()\nnewcolindex_DoS=[i for i, x in enumerate(explained_variance) if x]\nnewcolname_DoS=list( colNames[i] for i in newcolindex_DoS )\nnewcolname_DoS","5b211895":"# X_newProbe = selector.fit_transform(X_Probe,Y_Probe)\nX_newProbe = pca.fit_transform(X_Probe)\nX_newProbe.shape","f050dcac":"# true=selector.get_support()\nnewcolindex_Probe=[i for i, x in enumerate(explained_variance) if x]\nnewcolname_Probe=list( colNames[i] for i in newcolindex_Probe )\nnewcolname_Probe","09a7c3fb":"# X_newR2L = selector.fit_transform(X_R2L,Y_R2L)\nX_newR2L = pca.fit_transform(X_R2L)\nX_newR2L.shape","fca67ea7":"# true=selector.get_support()\nnewcolindex_R2L=[i for i, x in enumerate(explained_variance) if x]\nnewcolname_R2L=list( colNames[i] for i in newcolindex_R2L)\nnewcolname_R2L","a908e15d":"# X_newU2R = selector.fit_transform(X_U2R,Y_U2R)\nX_newU2R = pca.fit_transform(X_U2R)\nX_newU2R.shape","f3cf0776":"# true=selector.get_support()\nnewcolindex_U2R=[i for i, x in enumerate(explained_variance) if x]\nnewcolname_U2R=list( colNames[i] for i in newcolindex_U2R)\nnewcolname_U2R","c73fd87b":"print('Features selected for DoS:',newcolname_DoS)\nprint()\nprint('Features selected for Probe:',newcolname_Probe)\nprint()\nprint('Features selected for R2L:',newcolname_R2L)\nprint()\nprint('Features selected for U2R:',newcolname_U2R)","acb695c7":"'''from sklearn import preprocessing\nfrom sklearn import utils\n\nlab_enc = preprocessing.LabelEncoder()\nencoded = lab_enc.fit_transform(Y_DoS) '''\n","a8bbe90c":"#print(utils.multiclass.type_of_target(Y_DoS))\n","ddc92340":"#print(utils.multiclass.type_of_target(Y_DoS.astype('int')))","db4865a7":"#print(utils.multiclass.type_of_target(encoded))","14599945":"Y_DoS=Y_DoS.astype('int')\nY_Probe=Y_Probe.astype('int')\nY_R2L=Y_R2L.astype('int')\nY_U2R=Y_U2R.astype('int')","11ea8056":"#X_newDoS","f77df092":"from sklearn.feature_selection import RFE\nfrom sklearn.tree import DecisionTreeClassifier\n# Create a decision tree classifier. By convention, clf means 'classifier'\nclf = DecisionTreeClassifier(random_state=0)\n\n#rank all features, i.e continue the elimination until the last one\nrfe = RFE(clf, n_features_to_select=1)\nrfe.fit(X_newDoS, Y_DoS)\nprint (\"DoS Features sorted by their rank:\")\nprint (sorted(zip(map(lambda x: round(x, 4), rfe.ranking_), newcolname_DoS)))","e09749ba":"rfe.fit(X_newProbe, Y_Probe)\nprint (\"Probe Features sorted by their rank:\")\nprint (sorted(zip(map(lambda x: round(x, 4), rfe.ranking_), newcolname_Probe)))","6d9aed71":"rfe.fit(X_newR2L, Y_R2L)\n \nprint (\"R2L Features sorted by their rank:\")\nprint (sorted(zip(map(lambda x: round(x, 4), rfe.ranking_), newcolname_R2L)))","2dd9ade1":"rfe.fit(X_newU2R, Y_U2R)\n \nprint (\"U2R Features sorted by their rank:\")\nprint (sorted(zip(map(lambda x: round(x, 4), rfe.ranking_), newcolname_U2R)))","7d1d3701":"from sklearn.feature_selection import RFE\nclf = DecisionTreeClassifier(random_state=0)\nrfe = RFE(estimator=clf, n_features_to_select=8, step=1)\nrfe.fit(X_DoS, Y_DoS)\nX_rfeDoS=rfe.transform(X_DoS)\ntrue=rfe.support_\nrfecolindex_DoS=[i for i, x in enumerate(true) if x]\nrfecolname_DoS=list(colNames[i] for i in rfecolindex_DoS)","6bdfd0b8":"rfe.fit(X_Probe, Y_Probe)\nX_rfeProbe=rfe.transform(X_Probe)\ntrue=rfe.support_\nrfecolindex_Probe=[i for i, x in enumerate(true) if x]\nrfecolname_Probe=list(colNames[i] for i in rfecolindex_Probe)","3b22075c":"rfe.fit(X_R2L, Y_R2L)\nX_rfeR2L=rfe.transform(X_R2L)\ntrue=rfe.support_\nrfecolindex_R2L=[i for i, x in enumerate(true) if x]\nrfecolname_R2L=list(colNames[i] for i in rfecolindex_R2L)","a95be489":"rfe.fit(X_U2R, Y_U2R)\nX_rfeU2R=rfe.transform(X_U2R)\ntrue=rfe.support_\nrfecolindex_U2R=[i for i, x in enumerate(true) if x]\nrfecolname_U2R=list(colNames[i] for i in rfecolindex_U2R)","247d5ed6":"print('Features selected for DoS:',rfecolname_DoS)\nprint()\nprint('Features selected for Probe:',rfecolname_Probe)\nprint()\nprint('Features selected for R2L:',rfecolname_R2L)\nprint()\nprint('Features selected for U2R:',rfecolname_U2R)","b3745bfa":"print(X_rfeDoS.shape)\nprint(X_rfeProbe.shape)\nprint(X_rfeR2L.shape)\nprint(X_rfeU2R.shape)","ce1281a9":"# all features\nclf_DoS=DecisionTreeClassifier(random_state=0)\nclf_Probe=DecisionTreeClassifier(random_state=0)\nclf_R2L=DecisionTreeClassifier(random_state=0)\nclf_U2R=DecisionTreeClassifier(random_state=0)\nclf_DoS.fit(X_DoS, Y_DoS)\nclf_Probe.fit(X_Probe, Y_Probe)\nclf_R2L.fit(X_R2L, Y_R2L)\nclf_U2R.fit(X_U2R, Y_U2R)","eab0c69b":"# selected features\nclf_rfeDoS=DecisionTreeClassifier(random_state=0)\nclf_rfeProbe=DecisionTreeClassifier(random_state=0)\nclf_rfeR2L=DecisionTreeClassifier(random_state=0)\nclf_rfeU2R=DecisionTreeClassifier(random_state=0)\nclf_rfeDoS.fit(X_rfeDoS, Y_DoS)\nclf_rfeProbe.fit(X_rfeProbe, Y_Probe)\nclf_rfeR2L.fit(X_rfeR2L, Y_R2L)\nclf_rfeU2R.fit(X_rfeU2R, Y_U2R)","7a5d1959":"# Apply the classifier we trained to the test data (which it has never seen before)\nclf_DoS.predict(X_DoS_test)","9ea08c56":"# View the predicted probabilities of the first 10 observations\nclf_DoS.predict_proba(X_DoS_test)[0:10]","07ebd01e":"Y_DoS_pred=clf_DoS.predict(X_DoS_test)\n# Create confusion matrix\npd.crosstab(Y_DoS_test, Y_DoS_pred, rownames=['Actual attacks'], colnames=['Predicted attacks'])","93804965":"Y_Probe_pred=clf_Probe.predict(X_Probe_test)\n# Create confusion matrix\npd.crosstab(Y_Probe_test, Y_Probe_pred, rownames=['Actual attacks'], colnames=['Predicted attacks'])","3ba446ce":"Y_R2L_pred=clf_R2L.predict(X_R2L_test)\n# Create confusion matrix\npd.crosstab(Y_R2L_test, Y_R2L_pred, rownames=['Actual attacks'], colnames=['Predicted attacks'])","323527af":"Y_U2R_pred=clf_U2R.predict(X_U2R_test)\n# Create confusion matrix\npd.crosstab(Y_U2R_test, Y_U2R_pred, rownames=['Actual attacks'], colnames=['Predicted attacks'])","212d61dc":"from sklearn.model_selection import cross_val_score\nfrom sklearn import metrics\naccuracy = cross_val_score(clf_DoS, X_DoS_test, Y_DoS_test, cv=10, scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\nprecision = cross_val_score(clf_DoS, X_DoS_test, Y_DoS_test, cv=10, scoring='precision')\nprint(\"Precision: %0.5f (+\/- %0.5f)\" % (precision.mean(), precision.std() * 2))\nrecall = cross_val_score(clf_DoS, X_DoS_test, Y_DoS_test, cv=10, scoring='recall')\nprint(\"Recall: %0.5f (+\/- %0.5f)\" % (recall.mean(), recall.std() * 2))\nf = cross_val_score(clf_DoS, X_DoS_test, Y_DoS_test, cv=10, scoring='f1')\nprint(\"F-measure: %0.5f (+\/- %0.5f)\" % (f.mean(), f.std() * 2))","17686ff8":"accuracy = cross_val_score(clf_Probe, X_Probe_test, Y_Probe_test, cv=10, scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\nprecision = cross_val_score(clf_Probe, X_Probe_test, Y_Probe_test, cv=10, scoring='precision_macro')\nprint(\"Precision: %0.5f (+\/- %0.5f)\" % (precision.mean(), precision.std() * 2))\nrecall = cross_val_score(clf_Probe, X_Probe_test, Y_Probe_test, cv=10, scoring='recall_macro')\nprint(\"Recall: %0.5f (+\/- %0.5f)\" % (recall.mean(), recall.std() * 2))\nf = cross_val_score(clf_Probe, X_Probe_test, Y_Probe_test, cv=10, scoring='f1_macro')\nprint(\"F-measure: %0.5f (+\/- %0.5f)\" % (f.mean(), f.std() * 2))","4cf0ad51":"accuracy = cross_val_score(clf_R2L, X_R2L_test, Y_R2L_test, cv=10, scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\nprecision = cross_val_score(clf_R2L, X_R2L_test, Y_R2L_test, cv=10, scoring='precision_macro')\nprint(\"Precision: %0.5f (+\/- %0.5f)\" % (precision.mean(), precision.std() * 2))\nrecall = cross_val_score(clf_R2L, X_R2L_test, Y_R2L_test, cv=10, scoring='recall_macro')\nprint(\"Recall: %0.5f (+\/- %0.5f)\" % (recall.mean(), recall.std() * 2))\nf = cross_val_score(clf_R2L, X_R2L_test, Y_R2L_test, cv=10, scoring='f1_macro')\nprint(\"F-measure: %0.5f (+\/- %0.5f)\" % (f.mean(), f.std() * 2))","f8489257":"accuracy = cross_val_score(clf_U2R, X_U2R_test, Y_U2R_test, cv=10, scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\nprecision = cross_val_score(clf_U2R, X_U2R_test, Y_U2R_test, cv=10, scoring='precision_macro')\nprint(\"Precision: %0.5f (+\/- %0.5f)\" % (precision.mean(), precision.std() * 2))\nrecall = cross_val_score(clf_U2R, X_U2R_test, Y_U2R_test, cv=10, scoring='recall_macro')\nprint(\"Recall: %0.5f (+\/- %0.5f)\" % (recall.mean(), recall.std() * 2))\nf = cross_val_score(clf_U2R, X_U2R_test, Y_U2R_test, cv=10, scoring='f1_macro')\nprint(\"F-measure: %0.5f (+\/- %0.5f)\" % (f.mean(), f.std() * 2))","cfa8a081":"%matplotlib inline","19e551fb":"print(__doc__)\n\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.model_selection import StratifiedKFold\n\n# Create the RFE object and compute a cross-validated score.\n# The \"accuracy\" scoring is proportional to the number of correct\n# classifications\nrfecv_DoS = RFECV(estimator=clf_DoS, step=1, cv=10, scoring='accuracy')\nrfecv_DoS.fit(X_DoS_test, Y_DoS_test)\n# Plot number of features VS. cross-validation scores\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score (nb of correct classifications)\")\nplt.title('RFECV DoS')\nplt.plot(range(1, len(rfecv_DoS.grid_scores_) + 1), rfecv_DoS.grid_scores_)\nplt.show()","c99609d2":"rfecv_Probe = RFECV(estimator=clf_Probe, step=1, cv=10, scoring='accuracy')\nrfecv_Probe.fit(X_Probe_test, Y_Probe_test)\n# Plot number of features VS. cross-validation scores\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score (nb of correct classifications)\")\nplt.title('RFECV Probe')\nplt.plot(range(1, len(rfecv_Probe.grid_scores_) + 1), rfecv_Probe.grid_scores_)\nplt.show()","88232332":"rfecv_R2L = RFECV(estimator=clf_R2L, step=1, cv=10, scoring='accuracy')\nrfecv_R2L.fit(X_R2L_test, Y_R2L_test)\n# Plot number of features VS. cross-validation scores\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score (nb of correct classifications)\")\nplt.title('RFECV R2L')\nplt.plot(range(1, len(rfecv_R2L.grid_scores_) + 1), rfecv_R2L.grid_scores_)\nplt.show()","8e6c9518":"rfecv_U2R = RFECV(estimator=clf_U2R, step=1, cv=10, scoring='accuracy')\nrfecv_U2R.fit(X_U2R_test, Y_U2R_test)\n# Plot number of features VS. cross-validation scores\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score (nb of correct classifications)\")\nplt.title('RFECV U2R')\nplt.plot(range(1, len(rfecv_U2R.grid_scores_) + 1), rfecv_U2R.grid_scores_)\nplt.show()","fecc698d":"# reduce test dataset to 13 features, use only features described in rfecolname_DoS etc.\nX_DoS_test2=X_DoS_test[:,rfecolindex_DoS]\nX_Probe_test2=X_Probe_test[:,rfecolindex_Probe]\nX_R2L_test2=X_R2L_test[:,rfecolindex_R2L]\nX_U2R_test2=X_U2R_test[:,rfecolindex_U2R]\nX_U2R_test2.shape","f18b5a77":"Y_DoS_pred2=clf_rfeDoS.predict(X_DoS_test2)\n# Create confusion matrix\npd.crosstab(Y_DoS_test, Y_DoS_pred2, rownames=['Actual attacks'], colnames=['Predicted attacks'])","393c5edf":"Y_Probe_pred2=clf_rfeProbe.predict(X_Probe_test2)\n# Create confusion matrix\npd.crosstab(Y_Probe_test, Y_Probe_pred2, rownames=['Actual attacks'], colnames=['Predicted attacks'])","39b83058":"Y_R2L_pred2=clf_rfeR2L.predict(X_R2L_test2)\n# Create confusion matrix\npd.crosstab(Y_R2L_test, Y_R2L_pred2, rownames=['Actual attacks'], colnames=['Predicted attacks'])","620a114d":"Y_U2R_pred2=clf_rfeU2R.predict(X_U2R_test2)\n# Create confusion matrix\npd.crosstab(Y_U2R_test, Y_U2R_pred2, rownames=['Actual attacks'], colnames=['Predicted attacks'])","4b91fc30":"accuracy = cross_val_score(clf_rfeDoS, X_DoS_test2, Y_DoS_test, cv=10, scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\nprecision = cross_val_score(clf_rfeDoS, X_DoS_test2, Y_DoS_test, cv=10, scoring='precision')\nprint(\"Precision: %0.5f (+\/- %0.5f)\" % (precision.mean(), precision.std() * 2))\nrecall = cross_val_score(clf_rfeDoS, X_DoS_test2, Y_DoS_test, cv=10, scoring='recall')\nprint(\"Recall: %0.5f (+\/- %0.5f)\" % (recall.mean(), recall.std() * 2))\nf = cross_val_score(clf_rfeDoS, X_DoS_test2, Y_DoS_test, cv=10, scoring='f1')\nprint(\"F-measure: %0.5f (+\/- %0.5f)\" % (f.mean(), f.std() * 2))","0b5ec671":"accuracy = cross_val_score(clf_rfeProbe, X_Probe_test2, Y_Probe_test, cv=10, scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\nprecision = cross_val_score(clf_rfeProbe, X_Probe_test2, Y_Probe_test, cv=10, scoring='precision_macro')\nprint(\"Precision: %0.5f (+\/- %0.5f)\" % (precision.mean(), precision.std() * 2))\nrecall = cross_val_score(clf_rfeProbe, X_Probe_test2, Y_Probe_test, cv=10, scoring='recall_macro')\nprint(\"Recall: %0.5f (+\/- %0.5f)\" % (recall.mean(), recall.std() * 2))\nf = cross_val_score(clf_rfeProbe, X_Probe_test2, Y_Probe_test, cv=10, scoring='f1_macro')\nprint(\"F-measure: %0.5f (+\/- %0.5f)\" % (f.mean(), f.std() * 2))","e4dcce1d":"accuracy = cross_val_score(clf_rfeR2L, X_R2L_test2, Y_R2L_test, cv=10, scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\nprecision = cross_val_score(clf_rfeR2L, X_R2L_test2, Y_R2L_test, cv=10, scoring='precision_macro')\nprint(\"Precision: %0.5f (+\/- %0.5f)\" % (precision.mean(), precision.std() * 2))\nrecall = cross_val_score(clf_rfeR2L, X_R2L_test2, Y_R2L_test, cv=10, scoring='recall_macro')\nprint(\"Recall: %0.5f (+\/- %0.5f)\" % (recall.mean(), recall.std() * 2))\nf = cross_val_score(clf_rfeR2L, X_R2L_test2, Y_R2L_test, cv=10, scoring='f1_macro')\nprint(\"F-measure: %0.5f (+\/- %0.5f)\" % (f.mean(), f.std() * 2))","b902a63f":"accuracy = cross_val_score(clf_rfeU2R, X_U2R_test2, Y_U2R_test, cv=10, scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\nprecision = cross_val_score(clf_rfeU2R, X_U2R_test2, Y_U2R_test, cv=10, scoring='precision_macro')\nprint(\"Precision: %0.5f (+\/- %0.5f)\" % (precision.mean(), precision.std() * 2))\nrecall = cross_val_score(clf_rfeU2R, X_U2R_test2, Y_U2R_test, cv=10, scoring='recall_macro')\nprint(\"Recall: %0.5f (+\/- %0.5f)\" % (recall.mean(), recall.std() * 2))\nf = cross_val_score(clf_rfeU2R, X_U2R_test2, Y_U2R_test, cv=10, scoring='f1_macro')\nprint(\"F-measure: %0.5f (+\/- %0.5f)\" % (f.mean(), f.std() * 2))","f8b608a2":"from sklearn.model_selection import StratifiedKFold\naccuracy = cross_val_score(clf_rfeDoS, X_DoS_test2, Y_DoS_test, cv=StratifiedKFold(10), scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))","28bce4f4":"accuracy = cross_val_score(clf_rfeProbe, X_Probe_test2, Y_Probe_test, cv=StratifiedKFold(10), scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))","943e1df8":"accuracy = cross_val_score(clf_rfeR2L, X_R2L_test2, Y_R2L_test, cv=StratifiedKFold(10), scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))","fc107d5b":"accuracy = cross_val_score(clf_rfeU2R, X_U2R_test2, Y_U2R_test, cv=StratifiedKFold(10), scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))","9e746d7c":"accuracy = cross_val_score(clf_rfeDoS, X_DoS_test2, Y_DoS_test, cv=2, scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))","b13ee446":"accuracy = cross_val_score(clf_rfeDoS, X_DoS_test2, Y_DoS_test, cv=5, scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))","14056485":"accuracy = cross_val_score(clf_rfeDoS, X_DoS_test2, Y_DoS_test, cv=10, scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))","eb11788c":"accuracy = cross_val_score(clf_rfeDoS, X_DoS_test2, Y_DoS_test, cv=30, scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))","eb2feb2f":"accuracy = cross_val_score(clf_rfeDoS, X_DoS_test2, Y_DoS_test, cv=50, scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))","0c882b7d":"accuracy = cross_val_score(clf_rfeProbe, X_Probe_test2, Y_Probe_test, cv=2, scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))","7c15ef59":"accuracy = cross_val_score(clf_rfeProbe, X_Probe_test2, Y_Probe_test, cv=5, scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))","dd6ba9dd":"accuracy = cross_val_score(clf_rfeProbe, X_Probe_test2, Y_Probe_test, cv=10, scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))","71566325":"accuracy = cross_val_score(clf_rfeProbe, X_Probe_test2, Y_Probe_test, cv=30, scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))","2134d5b5":"accuracy = cross_val_score(clf_rfeProbe, X_Probe_test2, Y_Probe_test, cv=50, scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))","c77bd0f5":"accuracy = cross_val_score(clf_rfeR2L, X_R2L_test2, Y_R2L_test, cv=2, scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))","5841dce7":"accuracy = cross_val_score(clf_rfeR2L, X_R2L_test2, Y_R2L_test, cv=5, scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))","87d10b62":"accuracy = cross_val_score(clf_rfeR2L, X_R2L_test2, Y_R2L_test, cv=10, scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))","165d770c":"accuracy = cross_val_score(clf_rfeR2L, X_R2L_test2, Y_R2L_test, cv=30, scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))","21539489":"accuracy = cross_val_score(clf_rfeR2L, X_R2L_test2, Y_R2L_test, cv=50, scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))","12c43f03":"accuracy = cross_val_score(clf_rfeU2R, X_U2R_test2, Y_U2R_test, cv=2, scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))","0bc122bd":"accuracy = cross_val_score(clf_rfeU2R, X_U2R_test2, Y_U2R_test, cv=5, scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))","d2897338":"accuracy = cross_val_score(clf_rfeU2R, X_U2R_test2, Y_U2R_test, cv=10, scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))","86a1eed0":"accuracy = cross_val_score(clf_rfeU2R, X_U2R_test2, Y_U2R_test, cv=30, scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))","6a3e8c42":"accuracy = cross_val_score(clf_rfeU2R, X_U2R_test2, Y_U2R_test, cv=50, scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))","53d4a088":"import pandas as pd\nKDDTest_2 = pd.read_csv(\"..\/input\/kddtttttt\/KDDTest_2.csv\")","db058080":"import pandas as pd\nKDDTrain_2 = pd.read_csv(\"..\/input\/kddddd\/KDDTrain_2.csv\")","da275078":"## Probe","c5927746":"# LabelEncoder","db72dc55":"### Get the features that were selected: U2R","4f69523f":"# Using 13 Features for each category","7cf7fe84":"# RFECV for illustration","861683f8":"## DoS","b525c3ec":"## Use StandardScaler() to scale the dataframes","1f193636":"# Step 2: Feature Scaling:","239b18fb":"### Check that the Standard Deviation is 1","341d59ff":"## U2R","d11f8ee5":"### Get the features that were selected: R2L","cdd7fe9a":"## U2R","996e29b2":"## Probe","a0ac3b03":"# Step 1: Data preprocessing:\nOne-Hot-Encoding (one-of-K) is used to to transform all categorical features into binary features. \nRequirement for One-Hot-encoding:\n\"The input to this transformer should be a matrix of integers, denoting the values taken on by categorical (discrete) features. The output will be a sparse matrix where each column corresponds to one possible value of one feature. It is assumed that input features take on values in the range [0, n_values).\"\n\nTherefore the features first need to be transformed with LabelEncoder, to transform every category to a number.","5acb62b7":"## Transform categorical features into numbers using LabelEncoder()","cce51cbe":"# Step 3: Feature Selection:","38c43a6d":"### Add 6 missing categories from train set to test set","23c19c61":"# Modelling Intrusion Detection: Analysis of a Feature Selection Mechanism\n\n## Method Description\n\n### Step 1: Data preprocessing:\nAll features are made numerical using one-Hot-encoding. The features are scaled to avoid features with large values that may weigh too much in the results.\n\n### Step 2: Feature Selection:\nEliminate redundant and irrelevant data by selecting a subset of relevant features that fully represents the given problem.\nUnivariate feature selection with ANOVA F-test. This analyzes each feature individually to detemine the strength of the relationship between the feature and labels. Using SecondPercentile method (sklearn.feature_selection) to select features based on percentile of the highest scores. \nWhen this subset is found: Recursive Feature Elimination (RFE) is applied.\n\n### Step 4: Build the model:\nDecision tree model is built.\n\n### Step 5: Prediction & Evaluation (validation):\nUsing the test data to make predictions of the model.\nMultiple scores are considered such as:accuracy score, recall, f-measure, confusion matrix.\nperform a 10-fold cross-validation.","1acf900f":"# Cross Validation: Accuracy, Precision, Recall, F-measure","8d1983c5":"## DoS","3cc4a3c6":"## Probe","6fa5df2c":"## R2L","c275a000":"## R2L","ca377b69":"# Step 5: Prediction & Evaluation (validation):","1e9082ac":"### Get the features that were selected: DoS","1b0a6892":"## Statistical Summary","1f5260b0":"# Cross Validation: Accuracy, Precision, Recall, F-measure","d4a52357":"## DoS","9df1cf34":"## Version Check","79b3ac43":"## U2R","63c1d859":"## U2R","0d0d6abe":"# 1. Univariate Feature Selection using ANOVA F-test","53a2cc18":"# Confusion Matrices\n## DoS","d2e1ced5":"### Conclusion: Need to make dummies for all categories as the distribution is fairly even. In total: 3+70+11=84 dummies.\n### Comparing the results shows that the Test set has fewer categories (6), these need to be added as empty columns.","94c341be":"# Confusion Matrices\n## DoS","78a781a2":"## Label Distribution of Training and Test set","9040debe":"### Make column names for dummies","3a16fb8b":"# Summary of features selected by Univariate Feature Selection","762254ce":"# Summary of features selected by RFE","d0996880":"### Get the features that were selected: Probe","5c430d86":"# Using all Features for each category","5b16ea9f":"# 2. Recursive Feature Elimination, select 13 features each of 122 (Option 2: get 13 best features from 122 from RFE)","7cbf2bfc":"## Identify categorical features","6a4f8875":"# CV 2, 5, 10, 30, 50 fold","d1a0fc03":"## R2L","18363cac":"## R2L","066a1464":"### Insert categorical features into a 2D numpy array","b5487f05":"# Step 4: Build the model:\n### Classifier is trained for all features and for reduced features, for later comparison.\n#### The classifier model itself is stored in the clf variable.","b4497ba3":"## The authors state that \"After obtaining the adequate number of features during the univariate selection process, a recursive feature elimination (RFE) was operated with the number of features passed as parameter to identify the features selected\". This either implies that RFE is only used for obtaining the features previously selected but also obtaining the rank. This use of RFE is however very redundant as the features selected can be obtained in another way (Done in this project). One can also not say that the features were selected by RFE, as it was not used for this. The quote could however also imply that only the number 13 from univariate feature selection was used. RFE is then used for feature selection trying to find the best 13 features. With this use of RFE one can actually say that it was used for feature selection. However the authors obtained different numbers of features for every attack category, 12 for DoS, 15 for Probe, 13 for R2L and 11 for U2R. This concludes that it is not clear what mechanism is used for feature selection. \n\n## To procede with the data mining, the second option is considered as this uses RFE. From now on the number of features for every attack category is 13.","13106845":"## Probe","5f12d9c8":"# 2. Recursive Feature Elimination for feature ranking (Option 1: get importance from previous selected)","4329e57a":"# Stratified CV => Stays the same","7c7efa9d":"## R2L","ececfa02":"## Sample view of the training dataset","86b1f416":"### Save a list of feature names for later use (it is the same for every attack category). Column names are dropped at this stage.","76ef8ed6":"# Split Dataset into 4 datasets for every attack category\n## Rename every attack label: 0=normal, 1=DoS, 2=Probe, 3=R2L and 4=U2R.\n## Replace labels column with new labels column\n## Make new datasets\n","dadcd409":"## U2R","1802eece":"# One-Hot-Encoding","2b967464":"## Probe","3a893f68":"## Load the Dataset","d515cccb":"## Join encoded categorical dataframe with the non-categorical dataframe"}}