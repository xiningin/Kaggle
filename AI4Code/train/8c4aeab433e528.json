{"cell_type":{"79dcd056":"code","507ca512":"code","22065a15":"code","79caae3c":"code","4fb4448d":"code","1c9c2696":"code","4c87db58":"code","4c7b7252":"code","47f1a026":"code","ed65f3f8":"code","7ccfdc3d":"code","0ec286b8":"code","e2fc8dc9":"code","2fb0dc1f":"code","80364a9c":"code","a60aca54":"code","372f939d":"code","d6cccb0a":"code","0effc607":"code","a9028963":"code","278c0ec7":"code","7bbe8d8b":"code","3825dc72":"code","d727a2a9":"code","8676c040":"code","0defac33":"code","d7877586":"code","ed3ff26a":"code","182bddb7":"code","2fca2c49":"code","5a3509a8":"code","ba13b912":"code","74982182":"code","90e13448":"code","f295191a":"code","e7a3c86e":"code","aed77d67":"code","928bcf22":"code","6a95cb9e":"code","0449b7ae":"code","2f66a803":"code","19aaf877":"code","c7d11d64":"code","d06b1fbd":"code","b6d0052c":"code","5efdee6f":"code","412772e1":"code","684dfc45":"code","5b4ad764":"code","6958eaee":"code","953fff98":"code","22aefc59":"code","091ba521":"code","bd52aaf7":"code","919f3ad4":"code","7336e70e":"code","7671f17b":"code","ec2cc703":"code","a4218d92":"code","9cf12543":"code","011343a2":"code","e92b1c11":"markdown","1d12e89d":"markdown","f127f931":"markdown","89f0213e":"markdown","fed4c7e6":"markdown","2b81bc9a":"markdown","1b6d3a1d":"markdown","bb0cc921":"markdown","2ccdb720":"markdown","57c44e31":"markdown","8b3f70e0":"markdown","5ac8967f":"markdown","24a6ee7a":"markdown","88eb7769":"markdown","555dfa41":"markdown","eaf23fa5":"markdown","06df220e":"markdown","ecd1b236":"markdown","e78f10d7":"markdown","5ade61f5":"markdown","f5eacfea":"markdown","f90036bd":"markdown","7aa5169c":"markdown","183313d0":"markdown","c1c7abd9":"markdown","04ab2aef":"markdown","3bb7c713":"markdown","ce952e31":"markdown","d3da8f15":"markdown","0f73b3b4":"markdown","7abb336f":"markdown","dd520526":"markdown","53a94e3f":"markdown","2efd4450":"markdown","f66cb0fc":"markdown","477fe965":"markdown","8ada8f20":"markdown"},"source":{"79dcd056":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nimport itertools\nimport gc\nimport tensorflow as tf\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\nimport optuna.integration.lightgbm as lgb\nimport lightgbm as lgbm\nimport warnings\nimport shap\n\npd.set_option('display.max_columns',None)\nwarnings.filterwarnings('ignore')","507ca512":"ic = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\nitem = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/items.csv')\nshop = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/shops.csv')\ntrain = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\ntest = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/test.csv').set_index('ID')","22065a15":"shop.shape[0],item.shape[0],ic.shape[0]","79caae3c":"plt.figure(figsize=(10,6))\nplt.subplot(121)\nplt.boxplot(train['item_price'])\nplt.title('Item Price')\nplt.subplot(122)\nplt.boxplot(train['item_cnt_day'])\nplt.title('Item count Day')\nplt.tight_layout(pad=3)\nplt.show()","4fb4448d":"train = train[(train['item_price']<10000) & (train['item_cnt_day']<1001)]","1c9c2696":"negp = train[train['item_price']<=0]\nprint(negp)\nmedian = train[(train.shop_id==32)&(train.item_id==2973)&\n               (train.date_block_num==4)&(train.item_price>0)]['item_price'].median()\ntrain.loc[train.item_price<0,'item_price'] = median","4c87db58":"train.loc[train.shop_id == 0,'shop_id']=57\ntest.loc[test.shop_id == 0,'shop_id']=57\ntrain.loc[train.shop_id == 1,'shop_id']=58\ntest.loc[test.shop_id == 1,'shop_id']=58\ntrain.loc[train.shop_id == 10,'shop_id']=11\ntest.loc[test.shop_id == 10,'shop_id']=11","4c7b7252":"ic['split'] = ic['item_category_name'].apply(lambda x: x.split('-'))\nic['type'] = ic['split'].apply(lambda x:x[0].strip())\nic['type_code'] = LabelEncoder().fit_transform(ic['type'])\nic['subtype'] = ic['split'].map(lambda x: x[1].strip() if len(x)>1 else x[0].strip())\nic['subtype_code'] = LabelEncoder().fit_transform(ic['subtype'])\nic = ic[['item_category_id','type_code','subtype_code']]\nic.head()","47f1a026":"shop['split'] = shop['shop_name'].apply(lambda x: x.split(' '))\nshop['city'] = shop['split'].apply(lambda x: x[0].strip())\nshop['city_code'] = LabelEncoder().fit_transform(shop['city'])\nshop = shop[['shop_id','city_code']]\nshop.head()","ed65f3f8":"item.drop(['item_name'],axis=1,inplace=True)\nitem.head()","7ccfdc3d":"len(set(test['item_id'])) - len(set(train['item_id']).intersection(set(test['item_id'])))","0ec286b8":"len(set(train['shop_id'])) - len(set(train['shop_id']).intersection(set(test['shop_id'])))","e2fc8dc9":"train = train[train['shop_id'].isin(test.shop_id.unique())]","2fb0dc1f":"train_match = []\nfor i in range(34):\n    sales = train[train.date_block_num==i]\n    train_match.append(np.array(list(itertools.product([i],sales.shop_id.unique(),\n                                        sales.item_id.unique())),dtype=np.int16))\n\ntrain_match = pd.DataFrame(np.vstack(train_match),columns =['date_block_num','shop_id','item_id'])\ntrain_match.sort_values(by = ['date_block_num','shop_id','item_id'],inplace=True)\ntrain_match.head()","80364a9c":"grouped = train[['date_block_num','shop_id','item_id',\n                 'item_cnt_day']].groupby(['date_block_num','shop_id','item_id']).agg('sum')\ngrouped.columns = ['item_cnt_month']\ngrouped.reset_index(inplace = True)\ntrain_match = pd.merge(train_match,grouped,on=['date_block_num','shop_id','item_id'],how='left')\n\ntrain_match['item_cnt_month'] = (train_match['item_cnt_month']\n                                 .fillna(0).clip(0,20).astype(np.float16))\ntrain_match.head()","a60aca54":"test['date_block_num'] = 34\ntrain_match = pd.concat([train_match,test],ignore_index=True, axis=0,\n                        sort = False,keys = ['date_block_num','shop_id','item_id'])\ntrain_match.fillna(0,inplace=True)\ntrain_match.tail()","372f939d":"train_match = pd.merge(train_match, item, on=['item_id'], how = 'left')\ntrain_match = pd.merge(train_match, ic, on=['item_category_id'], how = 'left')\ntrain_match = pd.merge(train_match, shop, on=['shop_id'], how='left')\ntrain_match.head()","d6cccb0a":"def create_lag_feature(df,lags,col):\n    tmp = df[['date_block_num','shop_id','item_id',col]]\n    for lag in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num','shop_id','item_id',col+'_lag_'+str(lag)]\n        ##col value is the timestamp 'lag' (months\/date_block_num) before \n        shifted['date_block_num'] += lag\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'],how = 'left')\n    \n    return df","0effc607":"train_match = create_lag_feature(train_match,[1,2,3,6,12],'item_cnt_month')","a9028963":"grouped2 = train_match.groupby(['date_block_num']).agg({'item_cnt_month':['mean']})\ngrouped2.columns = ['avg_item_cnt_month']\ngrouped2.reset_index(inplace = True)","278c0ec7":"train_match = pd.merge(train_match, grouped2, on=['date_block_num'], how='left')\ntrain_match['avg_item_cnt_month'] = train_match['avg_item_cnt_month'].astype('float16')\ntrain_match = create_lag_feature(train_match, [1],'avg_item_cnt_month')\ntrain_match.drop(['avg_item_cnt_month'],axis =1, inplace = True)","7bbe8d8b":"grouped3 = train_match.groupby(['date_block_num','item_id']).agg({'item_cnt_month':['mean']})\ngrouped3.columns = ['item_avg_item_cnt_month']\ngrouped3.reset_index(inplace = True)\n\ntrain_match = pd.merge(train_match, grouped3, on=['date_block_num','item_id'], how ='left')\ntrain_match['item_avg_item_cnt_month'] = train_match['item_avg_item_cnt_month'].astype('float16')\ntrain_match = create_lag_feature(train_match, [1,2,3,6,12], 'item_avg_item_cnt_month')\ntrain_match.drop(['item_avg_item_cnt_month'], axis =1, inplace =True)","3825dc72":"grouped4 = train_match.groupby(['date_block_num','shop_id']).agg({'item_cnt_month':['mean']})\ngrouped4.columns = ['shop_avg_item_cnt_month']\ngrouped4.reset_index(inplace = True)\n\ntrain_match = pd.merge(train_match, grouped4, on=['date_block_num','shop_id'], how ='left')\ntrain_match['shop_avg_item_cnt_month'] = train_match['shop_avg_item_cnt_month'].astype('float16')\ntrain_match = create_lag_feature(train_match, [1,2,3,6,12], 'shop_avg_item_cnt_month')\ntrain_match.drop(['shop_avg_item_cnt_month'], axis =1, inplace =True)","d727a2a9":"grouped5 = train_match.groupby(['date_block_num','item_category_id']).agg({'item_cnt_month':['mean']})\ngrouped5.columns = ['ic_avg_item_cnt_month']\ngrouped5.reset_index(inplace = True)\n\ntrain_match = pd.merge(train_match, grouped5, on=['date_block_num','item_category_id'], how ='left')\ntrain_match['ic_avg_item_cnt_month'] = train_match['ic_avg_item_cnt_month'].astype('float16')\ntrain_match = create_lag_feature(train_match, [1], 'ic_avg_item_cnt_month')\ntrain_match.drop(['ic_avg_item_cnt_month'], axis =1, inplace =True)","8676c040":"grouped6 = train_match.groupby(['date_block_num','type_code']).agg({'item_cnt_month':['mean']})\ngrouped6.columns = ['type_avg_item_cnt_month']\ngrouped6.reset_index(inplace = True)\n\ntrain_match = pd.merge(train_match, grouped6, on=['date_block_num','type_code'], how ='left')\ntrain_match['type_avg_item_cnt_month'] = train_match['type_avg_item_cnt_month'].astype('float16')\ntrain_match = create_lag_feature(train_match, [1], 'type_avg_item_cnt_month')\ntrain_match.drop(['type_avg_item_cnt_month'], axis =1, inplace =True)","0defac33":"grouped7 = train_match.groupby(['date_block_num','subtype_code']).agg({'item_cnt_month':['mean']})\ngrouped7.columns = ['subtype_avg_item_cnt_month']\ngrouped7.reset_index(inplace = True)\n\ntrain_match = pd.merge(train_match, grouped7, on=['date_block_num','subtype_code'], how ='left')\ntrain_match['subtype_avg_item_cnt_month'] = train_match['subtype_avg_item_cnt_month'].astype('float16')\ntrain_match = create_lag_feature(train_match, [1], 'subtype_avg_item_cnt_month')\ntrain_match.drop(['subtype_avg_item_cnt_month'], axis =1, inplace =True)","d7877586":"grouped8 = train_match.groupby(['date_block_num','city_code']).agg({'item_cnt_month':['mean']})\ngrouped8.columns = ['city_avg_item_cnt_month']\ngrouped8.reset_index(inplace = True)\n\ntrain_match = pd.merge(train_match, grouped8, on=['date_block_num','city_code'], how ='left')\ntrain_match['city_avg_item_cnt_month'] = train_match['city_avg_item_cnt_month'].astype('float16')\ntrain_match = create_lag_feature(train_match, [1], 'city_avg_item_cnt_month')\ntrain_match.drop(['city_avg_item_cnt_month'], axis =1, inplace =True)","ed3ff26a":"grouped9 = train_match.groupby(['date_block_num','shop_id','item_category_id']).agg({'item_cnt_month':['mean']})\ngrouped9.columns = ['shop_ic_avg_item_cnt_month']\ngrouped9.reset_index(inplace = True)\n\ntrain_match = pd.merge(train_match, grouped9, on=['date_block_num','shop_id','item_category_id'], how ='left')\ntrain_match['shop_ic_avg_item_cnt_month'] = train_match['shop_ic_avg_item_cnt_month'].astype('float16')\ntrain_match = create_lag_feature(train_match, [1], 'shop_ic_avg_item_cnt_month')\ntrain_match.drop(['shop_ic_avg_item_cnt_month'], axis =1, inplace =True)","182bddb7":"grouped10 = train_match.groupby(['date_block_num','shop_id','type_code']).agg({'item_cnt_month':['mean']})\ngrouped10.columns = ['shop_type_avg_item_cnt_month']\ngrouped10.reset_index(inplace = True)\n\ntrain_match = pd.merge(train_match, grouped10, on=['date_block_num','shop_id','type_code'], how ='left')\ntrain_match['shop_type_avg_item_cnt_month'] = train_match['shop_type_avg_item_cnt_month'].astype('float16')\ntrain_match = create_lag_feature(train_match, [1], 'shop_type_avg_item_cnt_month')\ntrain_match.drop(['shop_type_avg_item_cnt_month'], axis =1, inplace =True)","2fca2c49":"grouped11 = train_match.groupby(['date_block_num','shop_id','subtype_code']).agg({'item_cnt_month':['mean']})\ngrouped11.columns = ['shop_subtype_avg_item_cnt_month']\ngrouped11.reset_index(inplace = True)\n\ntrain_match = pd.merge(train_match, grouped11, on=['date_block_num','shop_id','subtype_code'], how ='left')\ntrain_match['shop_subtype_avg_item_cnt_month'] = train_match['shop_subtype_avg_item_cnt_month'].astype('float16')\ntrain_match = create_lag_feature(train_match, [1], 'shop_subtype_avg_item_cnt_month')\ntrain_match.drop(['shop_subtype_avg_item_cnt_month'], axis =1, inplace =True)","5a3509a8":"grouped12 = train_match.groupby(['date_block_num','item_id','city_code']).agg({'item_cnt_month':['mean']})\ngrouped12.columns = ['item_city_avg_item_cnt_month']\ngrouped12.reset_index(inplace = True)\n\ntrain_match = pd.merge(train_match, grouped12, on=['date_block_num','item_id','city_code'], how ='left')\ntrain_match['item_city_avg_item_cnt_month'] = train_match['item_city_avg_item_cnt_month'].astype('float16')\ntrain_match = create_lag_feature(train_match, [1], 'item_city_avg_item_cnt_month')\ntrain_match.drop(['item_city_avg_item_cnt_month'], axis =1, inplace =True)","ba13b912":"train_match.columns","74982182":"train_match.sample(5)","90e13448":"grouped13 = train.groupby(['item_id']).agg({'item_price':['mean']})\ngrouped13.columns = ['item_avg_item_price']\ngrouped13.reset_index(inplace = True)\ntrain_match = pd.merge(train_match, grouped13, on=['item_id'], how='left')\ntrain_match['item_avg_item_price'] = train_match['item_avg_item_price'].astype('float16')\n\ngrouped14 = train.groupby(['date_block_num','item_id']).agg({'item_price':['mean']})\ngrouped14.columns = ['item_avg_item_price_month']\ngrouped14.reset_index(inplace = True)\ntrain_match = pd.merge(train_match, grouped14, on=['date_block_num','item_id'], how='left')\ntrain_match['item_avg_item_price_month'] = train_match['item_avg_item_price_month'].astype('float16')\n\nall_lags = [1,2,3,4,5,6]\ntrain_match = create_lag_feature(train_match,all_lags,'item_avg_item_price_month')","f295191a":"for l in all_lags:\n    train_match['delta_price_lag_'+str(l)]= (train_match['item_avg_item_price_month_lag_'+str(l)] - train_match['item_avg_item_price'])\/train_match['item_avg_item_price']\n\ndef valid_trend(row):\n    for l in all_lags:\n        if row['delta_price_lag_'+str(l)]:\n            return row['delta_price_lag_'+str(l)]\n    return 0\n\ntrain_match['delta_price_lag'] = train_match.apply(valid_trend, axis=1)\ntrain_match['delta_price_lag'] = train_match['delta_price_lag'].astype('float16')\ntrain_match['delta_price_lag'].fillna(0, inplace =True)\n\nto_drop = ['item_avg_item_price','item_avg_item_price_month']\nfor l in all_lags:\n    to_drop += ['item_avg_item_price_month_lag_'+str(l)]\n    to_drop += ['delta_price_lag_'+str(l)]\n    \ntrain_match.drop(to_drop, axis=1, inplace=True)","e7a3c86e":"train_match.columns","aed77d67":"train_match['month'] = ((train_match['date_block_num']%12)+1).astype('int8')\ndof = {1:31, 2:28, 3:31, 4:30, 5:31, 6:30,\n       7:31, 8:31, 9:30, 10:31, 11:30, 12:31}\ntrain_match['day'] = train_match['month'].map(dof).astype('int8')","928bcf22":"sale_record = {}\ntrain_match['last_item_sale'] = -1\ntrain_match['last_item_sale'] = train_match['last_item_sale'].astype('int8')\nfor idx,row in train_match.iterrows():\n    key = row.item_id\n    if key not in sale_record.keys():\n        if row.item_cnt_month != 0:\n            sale_record[key] = row.date_block_num\n            \n    else:\n        last_date_block_num = sale_record[key]\n        if row.date_block_num > last_date_block_num:\n            train_match.at[idx,'last_item_sale'] = row.date_block_num - last_date_block_num\n            sale_record[key] = row.date_block_num            ","6a95cb9e":"sale_record = {}\ntrain_match['last_shop_item_sale'] = -1\ntrain_match['last_shop_item_sale'] = train_match['last_shop_item_sale'].astype('int8')\nfor idx,row in train_match.iterrows():\n    key = (row.item_id,row.shop_id)\n    if key not in sale_record.keys():\n        if row.item_cnt_month != 0:\n            sale_record[key] = row.date_block_num\n            \n    else:\n        last_date_block_num = sale_record[key]\n        if row.date_block_num > last_date_block_num:\n            train_match.at[idx,'last_shop_item_sale'] = row.date_block_num - last_date_block_num\n            sale_record[key] = row.date_block_num ","0449b7ae":"train_match['first_item_shop_sale'] = train_match['date_block_num'] - train_match.groupby(['item_id','shop_id'])['date_block_num'].transform('min')\ntrain_match['first_item_sale'] = train_match['date_block_num'] -train_match.groupby('item_id')['date_block_num'].transform('min')","2f66a803":"train_match = train_match[train_match['date_block_num']>11]","19aaf877":"def fillna_lag(df):\n    for col in df.columns:\n        if '_lag_' in col and df[col].isnull().any():\n            if 'item_cnt' in col:\n                df[col].fillna(0, inplace = True)\n                \n    return df\n\ntrain_match = fillna_lag(train_match)","c7d11d64":"train_match.to_pickle('training.pkl')","d06b1fbd":"del train_match\ndel sale_record  \ndel item\ndel shop\ndel ic\ndel train\ndel grouped\ndel grouped2\ndel grouped3\ndel grouped4\ndel grouped5\ndel grouped6\ndel grouped7\ndel grouped8\ndel grouped9\ndel grouped10\ndel grouped11\ndel grouped12\ndel grouped13\ndel grouped14\n\ngc.collect()","b6d0052c":"training = pd.read_pickle('..\/input\/training-data-for-predicting-fututre-sales\/training.pkl')","5efdee6f":"training.info()","412772e1":"train_X = training[training.date_block_num<33].drop(['item_cnt_month'],axis=1)\ntrain_y = training[training.date_block_num<33]['item_cnt_month']\nvalid_X = training[training.date_block_num==33].drop(['item_cnt_month'],axis=1)\nvalid_y = training[training.date_block_num==33]['item_cnt_month']\ntest_X = training[training.date_block_num==34].drop(['item_cnt_month'],axis=1)","684dfc45":"max_depth_range = {'max_depth':range(6,10,1)}\ncv1 = GridSearchCV(estimator = XGBRegressor(n_estimators = 1000,  \n                                            eta = 0.3,\n                                            subsample=0.8,\n                                            colsample_bytree=0.8,\n                                            seed = 0),\n                  param_grid = max_depth_range,\n                  scoring = 'neg_root_mean_squared_error')\n\ncv1.fit(train_x,train_y)\ncv1.cv_results_, cv1.best_score_, cv1.best_params_","5b4ad764":"xgb_params = {'n_estimators':1500, \n             'max_depth':8, \n             'eta':0.1,\n             'subsample':0.8,\n             'colsample_bytree':0.6,\n             'min_child_weight':3,\n             'seed':98}","6958eaee":"xgb_model = XGBRegressor(**xgb_params)\nxgb_model.fit(train_X,train_y,\n             eval_metric = 'rmse',\n             eval_set = [(train_X, train_y), (valid_X, valid_y)],\n             early_stopping_rounds = 10)","953fff98":"result = xgb_model.predict(test_X).clip(0,20)\ntest = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/test.csv').set_index('ID')\nsubmission = pd.DataFrame({'ID':test.index, 'item_cnt_month':result})\nsubmission.to_csv('xgb_submission.csv',index =False)","22aefc59":"cat_features = ['shop_id', 'item_id', \n                'item_category_id', 'type_code', 'subtype_code', \n                'city_code','month']","091ba521":"dtrain = lgb.Dataset(train_X, label=train_y, categorical_feature=cat_features, free_raw_data=False)\ndval = lgb.Dataset(valid_X, label=valid_y, categorical_feature=cat_features, free_raw_data=False)\nparams = {\"objective\": \"regression\",\"metric\": \"rmse\",\"verbosity\": -1,\"boosting_type\": \"gbdt\"}\nmodel = lgb.train(params, dtrain, valid_sets=[dval], verbose_eval=100, early_stopping_rounds=10)","bd52aaf7":"best_lgbm_params = model.params\nbest_score = model.best_score\nbest_lgbm_params,best_score","919f3ad4":"best_lgbm_params = {'objective': 'regression',\n                  'metric': 'rmse',\n                  'verbosity': 1,\n                  'boosting_type': 'gbdt',\n                  'lambda_l1': 4.7870372679959425,\n                  'lambda_l2': 2.1719574172416273e-06,\n                  'num_leaves': 235,\n                  'feature_fraction': 0.5479999999999999,\n                  'bagging_fraction': 1.0,\n                  'bagging_freq': 0,\n                  'min_child_samples': 20}\n\n#X = training[training.date_block_num<34].drop(['item_cnt_month'],axis=1)\n#y = training[training.date_block_num<34]['item_cnt_month']\n\nlgbm_model = lgbm.LGBMRegressor(**best_lgbm_params)\nlgbm_model.fit(train_X,train_y,\n             eval_metric = 'rmse',\n             eval_set = [(train_X, train_y), (valid_X, valid_y)],\n             early_stopping_rounds = 10, categorical_feature=cat_features)","7336e70e":"result = lgbm_model.predict(test_X,categorical_feature=cat_features).clip(0,20)\ntest = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/test.csv').set_index('ID')\nsubmission = pd.DataFrame({'ID':test.index, 'item_cnt_month':result})\nsubmission.to_csv('lgbm_submission.csv',index =False)","7671f17b":"fig,ax = plt.subplots(figsize=(10,8))\nlgbm.plot_importance(lgbm_model, height=0.4, max_num_features=20,\n                importance_type='gain', ax=ax)\nplt.yticks(fontsize=13)\nax.set_title('Feature Importance',fontsize=15)","ec2cc703":"explainer = shap.TreeExplainer(lgbm_model)\nshap_values = explainer.shap_values(valid_X)\nshap.summary_plot(shap_values, valid_X, max_display=20)","a4218d92":"shap.dependence_plot('item_cnt_month_lag_1', shap_values, \n                     valid_X, interaction_index=None, show=False)","9cf12543":"shap.dependence_plot('shop_id', shap_values, \n                     valid_X, interaction_index=None, show=False)","011343a2":"shap.dependence_plot('first_item_sale', shap_values, \n                     valid_X, interaction_index=None, show=False)","e92b1c11":"#  Data Cleaning","1d12e89d":"## Item Categories","f127f931":"## Merge training data and test data","89f0213e":"Fill in the created (shop,item) pairs with zero and clip the item_cnt_month into (0,20)","fed4c7e6":"From the box plot, the outlier in item price and item sales number is obvious.","2b81bc9a":"15 shops are not in the test data, but are found in training data","1b6d3a1d":"GridSearchCV from Scikit-learn is a good tool to do cv and find the good parameters. However, it is too slow because of huge training data.","bb0cc921":"no leap years between 2013-2015","2ccdb720":"1. shops in training data, but not in test data\n2. items in test data, but not in training data","57c44e31":"# Fine Tuning and Model Fitting","8b3f70e0":"387 new items found in the test data","5ac8967f":"Split train, test, valid data","24a6ee7a":"In conclusion, most recent sales record of an item will strongly improve the model predicting.","88eb7769":"Feature importance plot only shows how important the feature is during spliting. However, shap values plot can show negative or positive effects of the features on the model.","555dfa41":"## LightGBM","eaf23fa5":"## Time features creation","06df220e":"Shop name of shop id 0 and 57, 1 and 58, 10 and 11 are the duplicated pairs. So that we should modify it in both training and test dataset","ecd1b236":"Use integrated fine tuning tools for LightGBM model in optuna","e78f10d7":"# Simple Visualization","5ade61f5":"## Price Trend feature engineering","f5eacfea":"## Item sales feature engineering","f90036bd":"1. For most lag features, the larger the feature value, the more positive impacts on model predicting.\n2. For categorical features, both large and small feature value can have both positive and negative features impacts on model predicting.\n3. For the features which represent months diff towards first item sales records, the smaller the feature value, the more positive impacts on model predicting.","7aa5169c":"60 unique shops , 22170 unique items and 84 unique category of items","183313d0":"Drop training data with shops not in test data","c1c7abd9":"## Xgboost","04ab2aef":"Use itertools.product to create all possible (shop,item) pairs in all months","3bb7c713":"# Fit result plot and Explaination","ce952e31":"## Items","d3da8f15":"transform function works like agg('sum') and merge on (item_id,shop_id)","0f73b3b4":"## Shops","7abb336f":"# Feature Engineering","dd520526":"After adding the value of lag to date_block_num in shifted dataframe and concatenating it with original dataframe, the lag feature is added.","53a94e3f":"Add price fluctuation rate within 6 months. If no fluctuation occurs, fill with zero. If fluctuation occurs, fill with the most recent price fluctuation.","2efd4450":"Concanate training and test data","f66cb0fc":"Use median price to modify the data with negative prices","477fe965":"1. Simple Visualization \n\n\n2. Data Cleaning\n\n\n3. Feature Engineering\n\n   3.1 Item Categories\n   \n   3.2 Shops\n   \n   3.3 Items\n   \n   3.4 Merge training and test data\n   \n   3.5 Sales Lag creation\n   \n   3.6 Price Trend Lag creation\n   \n   3.7 Time features creation\n   \n   \n4. Fine tuning and modeling \n   \n   4.1 XGBoost\n   \n   4.2 LightGBM\n   \n   \n5. Fit result plot and Analysis\n   ","8ada8f20":"LightGBM has better performance so I use the fit result of lightgbm to plot the feature importance"}}