{"cell_type":{"466d9949":"code","984e857d":"code","d8efbeea":"code","79bb8796":"code","f1b180e5":"code","e8cfdf10":"code","c8862de1":"code","b9483f4d":"code","2c4922f4":"code","dfd5f97d":"code","72b0159f":"code","d1c72b1a":"code","02b18c86":"code","df890b01":"code","952d3b65":"code","80a047ae":"code","e7768709":"code","be5de02a":"code","bad0daa3":"code","86ef3363":"code","d837e99a":"code","da929355":"code","b14e6d6b":"code","33764bef":"code","67767020":"code","d203c2e3":"code","034c8554":"code","fd9b0f79":"code","98d5b6f0":"code","b8b6aee5":"code","5deaa37a":"code","3855a2b3":"code","649e1461":"code","24cfb7c2":"code","ee8cc330":"code","465a8a0a":"code","cdcbc45d":"code","b7c19bb8":"code","6df915c4":"code","97472147":"markdown","536eff8c":"markdown","582936bf":"markdown","c5ed5943":"markdown","a275843c":"markdown","a9b24f84":"markdown","35f1f8ba":"markdown","3a75c240":"markdown","9f194ebd":"markdown","e08e9895":"markdown","12d943a0":"markdown","fc0cfe40":"markdown","c8724fa5":"markdown","1115352a":"markdown","24de6fd5":"markdown","e9102355":"markdown","0b64e554":"markdown"},"source":{"466d9949":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd","984e857d":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nfrom wordcloud import WordCloud , STOPWORDS \nimport plotly.graph_objects as go\nfrom sklearn.metrics import plot_confusion_matrix\n\n%matplotlib inline","d8efbeea":"\ndataset = pd.read_csv('..\/input\/reviews\/Restaurant_Reviews.tsv',delimiter='\\t', quoting = 3 ) #since the file is in tabluar seperated values format\n#since the dataset contains many double quotes and so to overcome the preprocessing errors we add quoting to 3 , 3 for double quotes","79bb8796":"dataset.head()","f1b180e5":"\nplt.subplots(figsize=(16,13))\nwordcloud = WordCloud(\n                            background_color='black',max_words=5000,\n                            width=1200,stopwords=STOPWORDS,\n                            height=1000\n                            ).generate(\" \".join(dataset['Review']))\nplt.title(\"MOST USED WORDS\",fontsize=20)\nplt.imshow(wordcloud.recolor( colormap= 'viridis'))\nplt.axis('off')\nplt.show()\n","e8cfdf10":"#Wordcloud for positive reviews\n\npositive_reviews=dataset.loc[dataset['Liked']==1]['Review']\nplt.subplots(figsize=(16,13))\nwordcloud = WordCloud(\n                            background_color='black',max_words=5000,\n                            width=1200,stopwords=STOPWORDS,\n                            height=1000\n                            ).generate(\" \".join(positive_reviews)) #join used for making them to a single string\nplt.title(\"MOST USED WORDS\",fontsize=20)\nplt.imshow(wordcloud.recolor( colormap= 'viridis'))\nplt.axis('off')\nplt.show()","c8862de1":"#Wordcloud for negative reviews\n\nnegative_reviews=dataset.loc[dataset['Liked']==0]['Review']\nplt.subplots(figsize=(16,13))\nwordcloud = WordCloud(\n                            background_color='black',max_words=5000,\n                            width=1200,stopwords=STOPWORDS,\n                            height=1000\n                            ).generate(\" \".join(negative_reviews)) #join used for making them to a single string\nplt.title(\"MOST USED WORDS\",fontsize=20)\nplt.imshow(wordcloud.recolor( colormap= 'Pastel2'))\nplt.axis('off')\nplt.show()","b9483f4d":"\nimport re\nimport nltk #for removing the stopforwards\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer #this is for stemming , stemming : removing prefix,suffix,plurals,tenses such that only root word remains to get the real sense\n#since we are about to use bag of words model , which creates sparse matrix for words , if it is not stemmed the child and children words have two columns making it redundant\n\ncorpus = []\n\nfor i in range(0,1000):\n    review = re.sub('[^a-zA-Z]' , ' ' , dataset['Review'][i])   #re.sub is used to replace any word by any word . we will use this to replace any punctuation mark by space , since if we not replace it by space then two words join\n#hat operator is for not\n    review=review.lower()\n    review = review.split() #it splits in different words\n    ps = PorterStemmer()\n    all_stopwords = stopwords.words('english')\n    all_stopwords.remove('not')\n    review = [ps.stem(word) for word in review if not word in set(all_stopwords)] #applying stemming to all words in 1 review other than stopwords\n    review = ' '.join(review) #joining all back to a sentence with 1 space gap \n    corpus.append(review)","2c4922f4":"# creating a matrix for model and the process is called as tokenization\n\nfrom sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer()\nX = cv.fit_transform(corpus).toarray() #making it to 2d array since we use naive bayes model\nY = dataset.iloc[:,-1].values\n","dfd5f97d":"len(X[0]) #no of elemnts in 1 row","72b0159f":"len(X)","d1c72b1a":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(max_features=1450) #using this parameter to reduce useless words or the words that appear only few times\nX = cv.fit_transform(corpus).toarray()\nY = dataset.iloc[:,-1].values\nlen(X[0])","02b18c86":"# train test split\n\nfrom sklearn.model_selection import train_test_split\nX_train , X_test , Y_train , Y_test = train_test_split(X , Y ,train_size=0.8,test_size=0.2 ,random_state=0)","df890b01":"# training the model naive bayes\n\nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train , Y_train)\n","952d3b65":"\ny_pred = classifier.predict(X_test)\n#print(np.concatenate((y_pred.reshape(len(y_pred),1),Y_test.reshape(len(Y_test),1)),1))\n","80a047ae":"from sklearn.metrics import confusion_matrix , accuracy_score\n\ncm = confusion_matrix(Y_test , y_pred)\nprint(cm)\nprint(accuracy_score(Y_test,y_pred))","e7768709":"#positive\n\nnew_review = 'I love this restaurant so much'\nnew_review = re.sub('[^a-zA-Z]', ' ', new_review)\nnew_review = new_review.lower()\nnew_review = new_review.split()\nps = PorterStemmer()\nall_stopwords = stopwords.words('english')\nall_stopwords.remove('not')\nnew_review = [ps.stem(word) for word in new_review if not word in set(all_stopwords)]\nnew_review = ' '.join(new_review)\nnew_corpus = [new_review]\nnew_X_test = cv.transform(new_corpus).toarray()\nnew_y_pred = classifier.predict(new_X_test)\nprint(new_y_pred)","be5de02a":"#negative\nnew_review = 'I hate this restaurant so much'\nnew_review = re.sub('[^a-zA-Z]', ' ', new_review)\nnew_review = new_review.lower()\nnew_review = new_review.split()\nps = PorterStemmer()\nall_stopwords = stopwords.words('english')\nall_stopwords.remove('not')\nnew_review = [ps.stem(word) for word in new_review if not word in set(all_stopwords)]\nnew_review = ' '.join(new_review)\nnew_corpus = [new_review]\nnew_X_test = cv.transform(new_corpus).toarray()\nnew_y_pred = classifier.predict(new_X_test)\nprint(new_y_pred)","bad0daa3":"#ambiguity case\n\nnew_review = 'I love this restaurant so much and at the same time i hate this'\nnew_review = re.sub('[^a-zA-Z]', ' ', new_review)\nnew_review = new_review.lower()\nnew_review = new_review.split()\nps = PorterStemmer()\nall_stopwords = stopwords.words('english')\nall_stopwords.remove('not')\nnew_review = [ps.stem(word) for word in new_review if not word in set(all_stopwords)]\nnew_review = ' '.join(new_review)\nnew_corpus = [new_review]\nnew_X_test = cv.transform(new_corpus).toarray()\nnew_y_pred = classifier.predict(new_X_test)\nprint(new_y_pred)","86ef3363":"    \nfrom sklearn.svm import SVC\nclassifier = SVC(kernel = 'rbf', random_state = 0)\nclassifier.fit(X_train, Y_train)\n","d837e99a":"y_pred = classifier.predict(X_test)\n#print(np.concatenate((y_pred.reshape(len(y_pred),1),Y_test.reshape(len(Y_test),1)),1))\n","da929355":"from sklearn.metrics import confusion_matrix , accuracy_score\n\ncm = confusion_matrix(Y_test , y_pred)\nprint(cm)\nprint(accuracy_score(Y_test,y_pred))\n","b14e6d6b":"#ambiguity case\n\nnew_review = 'I love this restaurant so much and at the same time i hate this'\nnew_review = re.sub('[^a-zA-Z]', ' ', new_review)\nnew_review = new_review.lower()\nnew_review = new_review.split()\nps = PorterStemmer()\nall_stopwords = stopwords.words('english')\nall_stopwords.remove('not')\nnew_review = [ps.stem(word) for word in new_review if not word in set(all_stopwords)]\nnew_review = ' '.join(new_review)\nnew_corpus = [new_review]\nnew_X_test = cv.transform(new_corpus).toarray()\nnew_y_pred = classifier.predict(new_X_test)\nprint(new_y_pred)","33764bef":"#Cleaning the texts\n\nimport re\nimport nltk #for removing the stopforwards\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer #this is for stemming , stemming : removing prefix,suffix,plurals,tenses such that only root word remains to get the real sense\n#since we are about to use bag of words model , which creates sparse matrix for words , if it is not stemmed the child and children words have two columns making it redundant\nfrom nltk.stem import WordNetLemmatizer \n\ncorpus2 = []\n\nfor i in range(0,1000):\n    review = re.sub('[^a-zA-Z]' , ' ' , dataset['Review'][i])   #re.sub is used to replace any word by any word . we will use this to replace any punctuation mark by space , since if we not replace it by space then two words join\n#hat operator is for not\n    review=review.lower()\n    review = review.split() #it splits in different words\n    #ps = PorterStemmer()\n    ps = WordNetLemmatizer()\n    all_stopwords = stopwords.words('english')\n    all_stopwords.remove('not')\n    review = [ps.lemmatize(word) for word in review if not word in set(all_stopwords)] #applying stemming to all words in 1 review other than stopwords\n    review = ' '.join(review) #joining all back to a sentence with 1 space gap \n    corpus2.append(review)","67767020":"from sklearn.feature_extraction.text import CountVectorizer\n\ncv = CountVectorizer()\nbag_of_words = cv.fit_transform(corpus).toarray()\nbag_of_words","d203c2e3":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier","034c8554":"y = dataset.iloc[:,1].values\n\nX_train, X_test, y_train, y_test = train_test_split(bag_of_words, y, test_size = 0.20, random_state = 0)","fd9b0f79":"def clf_model(model):\n    clf = model\n    clf.fit(X_train, y_train)\n    accuracy = accuracy_score(y_test, clf.predict(X_test).round())\n    recall = recall_score(y_test, clf.predict(X_test).round())\n    precision = precision_score(y_test, clf.predict(X_test).round())\n    return clf, accuracy, recall, precision\n","98d5b6f0":"model_performance = pd.DataFrame(columns = [\"Model\", \"Accuracy\", \"Recall\", \"Precision\"])\n\nmodels_to_evaluate = [DecisionTreeClassifier(), LogisticRegression(), RandomForestClassifier(n_estimators=1000),\n                      KNeighborsClassifier(n_neighbors = 7, metric = \"minkowski\", p = 2),\n                      SVC(kernel = 'rbf'), GaussianNB(), XGBClassifier(n_estimators=300, learning_rate=0.01)]\n\nfor model in models_to_evaluate:\n    clf, accuracy, recall, precision = clf_model(model)\n    model_performance = model_performance.append({\"Model\": model, \"Accuracy\": accuracy,\n                                                  \"Recall\": recall, \"Precision\": precision}, ignore_index=True)\n\nmodel_performance","b8b6aee5":"from sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.neural_network import MLPClassifier\n\nfrom sklearn.model_selection import RandomizedSearchCV\nparameter_space = {\n    'hidden_layer_sizes': [(1024), (50,), (50,100, 50), (48,), (48, 48, 48), (96,), (144,), (192,), (96, 144, 192), (240,), (144, 192, 240)],\n    'activation': ['tanh', 'logistic', 'relu'],\n    'solver': ['adam'],\n    'alpha': [0.0001, 0.001, 0.05, 0.1, 1],\n    'beta_1': [0.9, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99],\n    'beta_2': [0.990, 0.991, 0.992, 0.993, 0.994, 0.995, 0.996, 0.997, 0.998, 0.999],\n    'learning_rate': ['constant','adaptive'],\n                }","5deaa37a":"mlp = MLPClassifier(random_state=0)\n","3855a2b3":"score = ['accuracy', 'precision']\nclf = RandomizedSearchCV(mlp, parameter_space, n_jobs = -1, n_iter = 15,  cv=3, refit='precision', scoring=score, random_state=0)","649e1461":"clf.fit(X_train, y_train)\nscore = clf.score(X_test, y_test)\nprint(\"Accuracy\",score*100,\"%\")","24cfb7c2":"plot_confusion_matrix(clf, X_test, y_test)","ee8cc330":"!pip install NRCLex","465a8a0a":"from nrclex import NRCLex\ntext_object = NRCLex('The food is worse')","cdcbc45d":"emotions2 = NRCLex(\" \".join(dataset['Review']))\nemotions2.affect_frequencies","b7c19bb8":"emotions_data = pd.DataFrame(emotions2.affect_frequencies.keys(),emotions2.affect_frequencies.values())\n\nemotions_data.columns=['Emotions']\nemotions_data.head()","6df915c4":"sns.barplot(x=emotions_data.index*100,y=emotions_data.Emotions.values)\nplt.title('Emotions over Reviews(Emotion Analysis)')\nplt.show()","97472147":"**Using Lemmatization instead of Stemming**","536eff8c":"**Importing Dataset**","582936bf":"**trying out kernal-svm for better accuracy**","c5ed5943":"**Accuracy and confusion matrix**","a275843c":"**MOST USED WORDS**","a9b24f84":"**predicting a single review**","35f1f8ba":"**predicting**","3a75c240":"Tokenization","9f194ebd":"**Creating the model**\n\n","e08e9895":"**Cleaning the texts**","12d943a0":"**Maximum Accuracy case**","fc0cfe40":"**Understanding underlying emotions**","c8724fa5":"**Importing libraries**","1115352a":"**Creating the bag of words model**\n\n","24de6fd5":"**Using Multilayer Perceptron**","e9102355":"**Models performence**","0b64e554":"**Testing out multiple models**"}}