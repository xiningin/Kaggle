{"cell_type":{"8a443446":"code","4142d9d7":"code","126831d0":"code","19eba4cf":"code","5dbc7edf":"code","baba667d":"code","0f17c838":"code","4f26dd29":"code","c3abf8ce":"code","a93266bb":"code","6511e70b":"code","9ca18cb9":"code","15ce7318":"code","20f6541e":"code","26067116":"code","d388aa57":"code","1293fb13":"code","2163bcdf":"code","6dc883a9":"code","796e044d":"code","b643d695":"code","06723e51":"code","ed142ee9":"code","22803833":"code","e6006e36":"code","8a40d857":"code","77f0dd88":"code","8559f05d":"code","f928a184":"markdown","c91ad5d4":"markdown","042a93ab":"markdown","6cc2b8d2":"markdown","63cd83c3":"markdown","61523195":"markdown","5dc05fe2":"markdown","c7f1ba8e":"markdown","a79f1159":"markdown","7f255252":"markdown","8df79f31":"markdown"},"source":{"8a443446":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.colors\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, mean_squared_error\nfrom tqdm import tqdm_notebook\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.datasets import make_blobs","4142d9d7":"class SigmoidNeuron:\n    \n    def __init__(self):\n        self.w = None\n        self.b = None\n        \n    def perceptron(self, x):\n        return np.dot(x, self.w.T) + self.b\n    \n    def sigmoid(self, x):\n        return 1.0\/(1.0 + np.exp(-x))\n    \n    def grad_w_mse(self, x, y):\n        y_pred = self.sigmoid(self.perceptron(x))\n        return (y_pred - y) * y_pred * (1 - y_pred) * x\n    \n    def grad_b_mse(self, x, y):\n        y_pred = self.sigmoid(self.perceptron(x))\n        return (y_pred - y) * y_pred * (1 - y_pred)\n    \n    def grad_w_ce(self, x, y):\n        y_pred = self.sigmoid(self.perceptron(x))\n        if y == 0:\n            return y_pred * x\n        elif y == 1:\n            return -1 * (1 - y_pred) * x\n        else:\n            raise ValueError(\"y should be 0 or 1\")\n            \n    def grad_b_ce(self, x, y):\n        y_pred = self.sigmoid(self.perceptron(x))\n        if y == 0:\n            return y_pred\n        elif y == 1:\n            return -1 * (1 - y_pred)\n        else:\n            raise ValueError(\"y should be 0 or 1\")\n            \n    def fit(self, X, Y, epochs=1, learning_rate=1, initialise=True, loss_fn=\"mse\", display_loss=False):\n        \n        # initailse w, b\n        if initialise:\n            self.w = np.random.randn(1, X.shape[1])\n            self.b = 0\n        \n        if display_loss:\n            loss={}\n        \n        for i in tqdm_notebook(range(epochs), total=epochs, unit=\"epochs\"):\n            dw = 0\n            db = 0\n            for x, y in zip(X, Y):\n                if loss_fn == \"mse\":\n                    dw += self.grad_w_mse(x, y)\n                    db += self.grad_b_mse(x, y)\n                elif loss_fn == \"ce\":\n                    dw += self.grad_w_ce(x, y)\n                    db += self.grad_b_ce(x, y)\n                    \n            m = X.shape[1]\n            self.w -= learning_rate * dw\/m\n            self.b -= learning_rate * db\/m\n            \n            if display_loss:\n                Y_pred = self.sigmoid(self.perceptron(X))\n                if loss_fn == \"mse\":\n                    loss[i] = mean_squared_error(Y, Y_pred)\n                elif loss_fn == \"ce\":\n                    loss[i] = log_loss(Y, Y_pred)\n        if display_loss:\n            plt.plot(np.fromiter(loss.values(), dtype = float))\n            plt.xlabel('Epochs')\n            if loss_fn == \"mse\":\n                plt.ylabel(\"Mean Squared Error\")\n            elif loss_fn == \"ce\":\n                plt.ylabel(\"Log Loss\")\n            plt.show()\n            \n    def predict(self, X):\n        Y_pred = []\n        for x in X:\n            y_pred = self.sigmoid(self.perceptron(x))\n            Y_pred.append(y_pred)\n        return np.array(Y_pred)","126831d0":"my_cmap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\",[\"red\",\"yellow\",\"green\"])","19eba4cf":"np.random.seed(0)","5dbc7edf":"data, labels = make_blobs(n_samples=1000, centers=4, n_features=2, random_state=0)\nprint(data.shape, labels.shape)","baba667d":"plt.scatter(data[:,0], data[:,1], c=labels, cmap=my_cmap)\nplt.show()","0f17c838":"labels_orig = labels\nlabels = np.mod(labels_orig, 2)","4f26dd29":"plt.scatter(data[:,0], data[:,1], c=labels, cmap=my_cmap)\nplt.show()","c3abf8ce":"X_train, X_val, Y_train, Y_val = train_test_split(data, labels, stratify=labels, random_state=0)\nprint(X_train.shape, X_val.shape)","a93266bb":"sn = SigmoidNeuron()\nsn.fit(X_train, Y_train, epochs=1000, learning_rate=0.5, display_loss=True)","6511e70b":"Y_pred_train = sn.predict(X_train)\nY_pred_binarised_train = (Y_pred_train >= 0.5).astype(\"int\").ravel()","9ca18cb9":"Y_pred_val = sn.predict(X_val)\nY_pred_binarised_val = (Y_pred_val >= 0.5).astype(\"int\").ravel()","15ce7318":"accuracy_train = accuracy_score(Y_pred_binarised_train, Y_train)\naccuracy_val = accuracy_score(Y_pred_binarised_val, Y_val)\n\nprint(\"Training Accuracy\", round(accuracy_train, 2))\nprint(\"Validation Accuracy\", round(accuracy_val, 2))","20f6541e":"plt.scatter(X_train[:,0], X_train[:,1], c=Y_pred_binarised_train, cmap=my_cmap, s = 15*(np.abs(Y_pred_binarised_train-Y_train)+.2))\nplt.show()","26067116":"class FirstFFNetwork:\n    \n    def __init__(self):\n        self.w1 = np.random.randn()\n        self.w2 = np.random.randn()\n        self.w3 = np.random.randn()\n        self.w4 = np.random.randn()\n        self.w5 = np.random.randn()\n        self.w6 = np.random.randn()\n        self.b1 = 0\n        self.b2 = 0\n        self.b3 = 0\n    \n    def sigmoid(self, x):\n        return 1.0\/(1.0 + np.exp(-x))\n    \n    def forward_pass(self, x):\n        self.x1, self.x2 = x\n        self.a1 = self.w1*self.x1 + self.w2*self.x2 + self.b1\n        self.h1 = self.sigmoid(self.a1)\n        self.a2 = self.w3*self.x1 + self.w4*self.x2 + self.b2\n        self.h2 = self.sigmoid(self.a2)\n        self.a3 = self.w5*self.h1 + self.w6*self.h2 + self.b3\n        self.h3 = self.sigmoid(self.a3)\n        return self.h3\n    \n    def grad(self, x, y):\n        self.forward_pass(x)\n\n        self.dw5 = (self.h3-y) * self.h3*(1-self.h3) * self.h1\n        self.dw6 = (self.h3-y) * self.h3*(1-self.h3) * self.h2\n        self.db3 = (self.h3-y) * self.h3*(1-self.h3)\n\n        self.dw1 = (self.h3-y) * self.h3*(1-self.h3) * self.w5 * self.h1*(1-self.h1) * self.x1\n        self.dw2 = (self.h3-y) * self.h3*(1-self.h3) * self.w5 * self.h1*(1-self.h1) * self.x2\n        self.db1 = (self.h3-y) * self.h3*(1-self.h3) * self.w5 * self.h1*(1-self.h1)\n\n        self.dw3 = (self.h3-y) * self.h3*(1-self.h3) * self.w6 * self.h2*(1-self.h2) * self.x1\n        self.dw4 = (self.h3-y) * self.h3*(1-self.h3) * self.w6 * self.h2*(1-self.h2) * self.x2\n        self.db2 = (self.h3-y) * self.h3*(1-self.h3) * self.w6 * self.h2*(1-self.h2)\n       \n    def fit(self, X, Y, epochs=1, learning_rate=1, initialise=True, display_loss=False):\n        \n        # initialise w, b\n        if initialise:\n            self.w1 = np.random.randn()\n            self.w2 = np.random.randn()\n            self.w3 = np.random.randn()\n            self.w4 = np.random.randn()\n            self.w5 = np.random.randn()\n            self.w6 = np.random.randn()\n            self.b1 = 0\n            self.b2 = 0\n            self.b3 = 0\n        \n        if display_loss:\n            loss = {}\n            \n        for i in tqdm_notebook(range(epochs), total=epochs, unit=\"epoch\"):\n            dw1, dw2, dw3, dw4, dw5, dw6, db1, db2, db3 = [0]*9\n            for x, y in zip(X, Y):\n                self.grad(x, y)\n                dw1 += self.dw1\n                dw2 += self.dw2\n                dw3 += self.dw3\n                dw4 += self.dw4\n                dw5 += self.dw5\n                dw6 += self.dw6\n                db1 += self.db1\n                db2 += self.db2\n                db3 += self.db3\n            \n            m = X.shape[1]\n            self.w1 -= learning_rate * dw1 \/m\n            self.w2 -= learning_rate * dw2 \/m\n            self.w3 -= learning_rate * dw3 \/m\n            self.w4 -= learning_rate * dw4 \/m\n            self.w5 -= learning_rate * dw5 \/m\n            self.w6 -= learning_rate * dw6 \/m\n            self.b1 -= learning_rate * db1 \/m\n            self.b2 -= learning_rate * db2 \/m\n            self.b3 -= learning_rate * db3 \/m\n    \n            if display_loss:\n                Y_pred = self.predict(X)\n                loss[i] = mean_squared_error(Y_pred, Y)\n            \n        if display_loss:\n            plt.plot(np.fromiter(loss.values(), dtype = float))\n            plt.xlabel('Epochs')\n            plt.ylabel('Mean Squared Error')\n            plt.show()\n            \n    \n    def predict(self, X):\n        Y_pred = []\n        for x in X:\n            y_pred = self.forward_pass(x)\n            Y_pred.append(y_pred)\n        return np.array(Y_pred)","d388aa57":"ffn = FirstFFNetwork()\nffn.fit(X_train, Y_train, epochs=2000, learning_rate=.01, display_loss=True)","1293fb13":"Y_pred_train = ffn.predict(X_train)\nY_pred_binarised_train = (Y_pred_train >= 0.5).astype(\"int\").ravel()\nY_pred_val = ffn.predict(X_val)\nY_pred_binarised_val = (Y_pred_val >= 0.5).astype(\"int\").ravel()\naccuracy_train = accuracy_score(Y_pred_binarised_train, Y_train)\naccuracy_val = accuracy_score(Y_pred_binarised_val, Y_val)\n\nprint(\"Training Accuracy\", round(accuracy_train, 2))\nprint(\"Validation Accuracy\", round(accuracy_val, 2))","2163bcdf":"plt.scatter(X_train[:,0], X_train[:,1], c=Y_pred_binarised_train, cmap=my_cmap, s=15*(np.abs(Y_pred_binarised_train-Y_train)+.2))\nplt.show()","6dc883a9":"class FFSNNetwork:\n    \n    def __init__(self, n_inputs, hidden_sizes=[2]):\n        self.nx = n_inputs\n        self.ny = 1\n        self.nh = len(hidden_sizes)\n        self.sizes = [self.nx] + hidden_sizes + [self.ny]\n        \n        self.W = {}\n        self.B = {}\n        for i in range(self.nh+1):\n            self.W[i+1] = np.random.randn(self.sizes[i], self.sizes[i+1])\n            self.B[i+1] = np.zeros((1, self.sizes[i+1]))\n        \n    def sigmoid(self, x):\n        return 1.0\/(1.0 + np.exp(-x))\n    \n    def forward_pass(self, x):\n        self.A = {}\n        self.H = {}\n        self.H[0] = x.reshape(1, -1)\n        for i in range(self.nh+1):\n            self.A[i+1] = np.matmul(self.H[i], self.W[i+1]) + self.B[i+1]\n            self.H[i+1] = self.sigmoid(self.A[i+1])\n        return self.H[self.nh+1]\n                      \n    def grad_sigmoid(self, x):\n        return x * (1-x)\n    \n    def grad(self, x, y):\n        self.forward_pass(x)\n        self.dW = {}\n        self.dB = {}\n        self.dH = {}\n        self.dA = {}\n        L = self.nh + 1\n        self.dA[L] = (self.H[L] - y)\n        for k in range(L, 0, -1):\n            self.dW[k] = np.matmul(self.H[k-1].T, self.dA[k])\n            self.dB[k] = self.dA[k]\n            self.dH[k-1] = np.matmul(self.dA[k], self.W[k].T)\n            self.dA[k-1] = np.multiply(self.dH[k-1], self.grad_sigmoid(self.H[k-1]))\n           \n       \n    def fit(self, X, Y, epochs=1, learning_rate=1, initialise=True, display_loss=False):\n        \n        # initialise w, b\n        if initialise:\n            for i in range(self.nh+1):\n                self.W[i+1] = np.random.randn(self.sizes[i], self.sizes[i+1])\n                self.B[i+1] = np.zeros((1, self.sizes[i+1]))\n                \n        if display_loss:\n            loss={}\n            \n        for e in tqdm_notebook(range(epochs), total=epochs, unit=\"epoch\"):\n            dW = {}\n            dB = {}\n            for i in range(self.nh+1):\n                dW[i+1] = np.zeros((self.sizes[i], self.sizes[i+1]))\n                dB[i+1] = np.zeros((1, self.sizes[i+1]))\n            \n            for x, y in zip(X, Y):\n                self.grad(x, y)\n                for i in range(self.nh+1):\n                    dW[i+1] += self.dW[i+1]\n                    dB[i+1] += self.dB[i+1]\n            \n            m = X.shape[1]\n            for i in range(self.nh+1):\n                self.W[i+1] -= learning_rate * dW[i+1] \/ m\n                self.B[i+1] -= learning_rate * dB[i+1] \/ m\n                \n            if display_loss:\n                Y_pred = self.predict(X)\n                loss[e] = mean_squared_error(Y_pred, Y)\n            \n        if display_loss:\n            plt.plot(np.fromiter(loss.values(), dtype = float))\n            plt.xlabel('Epochs')\n            plt.ylabel('Mean Squared Error')\n            plt.show()\n    \n    def predict(self, X):\n        Y_pred = []\n        for x in X:\n            y_pred = self.forward_pass(x)\n            Y_pred.append(y_pred)\n        return np.array(Y_pred).squeeze()\n                ","796e044d":"ffsnn = FFSNNetwork(2, [2,3])\nffsnn.fit(X_train, Y_train, epochs=2000, learning_rate=.001, display_loss=True)","b643d695":"Y_pred_train = ffsnn.predict(X_train)\nY_pred_binarised_train = (Y_pred_train >= 0.5).astype(\"int\").ravel()\nY_pred_val = ffsnn.predict(X_val)\nY_pred_binarised_val = (Y_pred_val >= 0.5).astype(\"int\").ravel()\naccuracy_train = accuracy_score(Y_pred_binarised_train, Y_train)\naccuracy_val = accuracy_score(Y_pred_binarised_val, Y_val)\n\nprint(\"Training accuracy\", round(accuracy_train, 2))\nprint(\"Validation accuracy\", round(accuracy_val, 2))","06723e51":"plt.scatter(X_train[:,0], X_train[:,1], c=Y_pred_binarised_train, cmap = my_cmap, s=15*(np.abs(Y_pred_binarised_train-Y_train)+.2))\nplt.show()","ed142ee9":"class FFSN_MultiClass:\n  \n  def __init__(self, n_inputs, n_outputs, hidden_sizes=[3]):\n    self.nx = n_inputs\n    self.ny = n_outputs\n    self.nh = len(hidden_sizes)\n    self.sizes = [self.nx] + hidden_sizes + [self.ny] \n\n    self.W = {}\n    self.B = {}\n    for i in range(self.nh+1):\n      self.W[i+1] = np.random.randn(self.sizes[i], self.sizes[i+1])\n      self.B[i+1] = np.zeros((1, self.sizes[i+1]))\n      \n  def sigmoid(self, x):\n    return 1.0\/(1.0 + np.exp(-x))\n  \n  def softmax(self, x):\n    exps = np.exp(x)\n    return exps \/ np.sum(exps)\n\n  def forward_pass(self, x):\n    self.A = {}\n    self.H = {}\n    self.H[0] = x.reshape(1, -1)\n    for i in range(self.nh):\n      self.A[i+1] = np.matmul(self.H[i], self.W[i+1]) + self.B[i+1]\n      self.H[i+1] = self.sigmoid(self.A[i+1])\n    self.A[self.nh+1] = np.matmul(self.H[self.nh], self.W[self.nh+1]) + self.B[self.nh+1]\n    self.H[self.nh+1] = self.softmax(self.A[self.nh+1])\n    return self.H[self.nh+1]\n  \n  def predict(self, X):\n    Y_pred = []\n    for x in X:\n      y_pred = self.forward_pass(x)\n      Y_pred.append(y_pred)\n    return np.array(Y_pred).squeeze()\n \n  def grad_sigmoid(self, x):\n    return x*(1-x) \n  \n  def cross_entropy(self,label,pred):\n    yl=np.multiply(pred,label)\n    yl=yl[yl!=0]\n    yl=-np.log(yl)\n    yl=np.mean(yl)\n    return yl\n \n  def grad(self, x, y):\n    self.forward_pass(x)\n    self.dW = {}\n    self.dB = {}\n    self.dH = {}\n    self.dA = {}\n    L = self.nh + 1\n    self.dA[L] = (self.H[L] - y)\n    for k in range(L, 0, -1):\n      self.dW[k] = np.matmul(self.H[k-1].T, self.dA[k])\n      self.dB[k] = self.dA[k]\n      self.dH[k-1] = np.matmul(self.dA[k], self.W[k].T)\n      self.dA[k-1] = np.multiply(self.dH[k-1], self.grad_sigmoid(self.H[k-1])) \n    \n  def fit(self, X, Y, epochs=100, initialize='True', learning_rate=0.01, display_loss=False):\n      \n    if display_loss:\n      loss = {}\n      \n    if initialize:\n      for i in range(self.nh+1):\n        self.W[i+1] = np.random.randn(self.sizes[i], self.sizes[i+1])\n        self.B[i+1] = np.zeros((1, self.sizes[i+1]))\n        \n    for epoch in tqdm_notebook(range(epochs), total=epochs, unit=\"epoch\"):\n      dW = {}\n      dB = {}\n      for i in range(self.nh+1):\n        dW[i+1] = np.zeros((self.sizes[i], self.sizes[i+1]))\n        dB[i+1] = np.zeros((1, self.sizes[i+1]))\n      for x, y in zip(X, Y):\n        self.grad(x, y)\n        for i in range(self.nh+1):\n          dW[i+1] += self.dW[i+1]\n          dB[i+1] += self.dB[i+1]\n                  \n      m = X.shape[1]\n      for i in range(self.nh+1):\n        self.W[i+1] -= learning_rate * (dW[i+1]\/m)\n        self.B[i+1] -= learning_rate * (dB[i+1]\/m)\n        \n      if display_loss:\n        Y_pred = self.predict(X) \n        loss[epoch] = self.cross_entropy(Y, Y_pred)\n    \n    if display_loss:\n      plt.plot(np.fromiter(loss.values(), dtype = float))\n      plt.xlabel('Epochs')\n      plt.ylabel('CE')\n      plt.show()","22803833":"X_train, X_val, Y_train, Y_val = train_test_split(data, labels_orig, stratify=labels_orig, random_state=0)\nprint(X_train.shape, X_val.shape, labels_orig.shape)","e6006e36":"enc = OneHotEncoder()\n# 0 -> (1, 0, 0, 0), 1 -> (0, 1, 0, 0), 2 -> (0, 0, 1, 0), 3 -> (0, 0, 0, 1)\ny_OH_train = enc.fit_transform(np.expand_dims(Y_train,1)).toarray()\ny_OH_val = enc.fit_transform(np.expand_dims(Y_val,1)).toarray()\nprint(y_OH_train.shape, y_OH_val.shape)","8a40d857":"ffsn_multi = FFSN_MultiClass(2,4,[2,3])\nffsn_multi.fit(X_train,y_OH_train, epochs=2000, learning_rate=.005, display_loss=True)","77f0dd88":"Y_pred_train = ffsn_multi.predict(X_train)\nY_pred_train = np.argmax(Y_pred_train,1)\n\nY_pred_val = ffsn_multi.predict(X_val)\nY_pred_val = np.argmax(Y_pred_val,1)\n\naccuracy_train = accuracy_score(Y_pred_train, Y_train)\naccuracy_val = accuracy_score(Y_pred_val, Y_val)\n\nprint(\"Training accuracy\", round(accuracy_train, 2))\nprint(\"Validation accuracy\", round(accuracy_val, 2))","8559f05d":"plt.scatter(X_train[:,0], X_train[:,1], c=Y_pred_train, cmap=my_cmap, s=15*(np.abs(np.sign(Y_pred_train-Y_train))+.1))\nplt.show()","f928a184":"<img src='https:\/\/github.com\/taruntiwarihp\/raw_images\/blob\/master\/SimpleNetwork-1553058835338.png?raw=true'>","c91ad5d4":"<img src='https:\/\/github.com\/taruntiwarihp\/raw_images\/blob\/master\/FFNetworkMultiClass-1553058835337.png?raw=true'>","042a93ab":"#### Outline\n\n1. Generate data that is not linearly separable\n2. Train with SN and see performance\n3. Write from scratch our first feed forward network\n4. Train the FF network on the data and compare with SN\n5. Write a generic class for a FF network\n6. Train generic class on binary classification\n7. Generate data for multi-class classification\n8. Train a FF network for 7\n9. Use softmax as the output layer and cross-entropy loss function\n10. Train with 8 for multi-class classification\n11. Exercises on other datasets","6cc2b8d2":"### Setup important libraries","63cd83c3":"### Feed Forward Network - Generic Class","61523195":"# Feedforward Neural Networks\n\nFeedforward neural networks are also known as Multi-layered Network of Neurons (MLN). These network of models are called feedforward because the information only travels forward in the neural network, through the input nodes then through the hidden layers (single or many layers) and finally through the output nodes.\n\n<img src='https:\/\/github.com\/taruntiwarihp\/raw_images\/blob\/master\/1_LsmIIg6u4BhGHIykJNa2hA.png?raw=true'>\n\n                        Generic Network with Connections\nTraditional models such as McCulloch Pitts, Perceptron and Sigmoid neuron models capacity is limited to linear functions. To handle the complex non-linear decision boundary between input and the output we are using the Multi-layered Network of Neurons.","5dc05fe2":"### Our First FF Network","c7f1ba8e":"### SN Classification","a79f1159":"### Generate Data","7f255252":"### Multi Class Classification","8df79f31":"<img src='https:\/\/github.com\/taruntiwarihp\/raw_images\/blob\/master\/FFNetworkSingle-1552984072224.png?raw=true'>"}}