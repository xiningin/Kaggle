{"cell_type":{"a72fe2dc":"code","875dfa71":"code","48dcd172":"code","e95109db":"code","e349799c":"code","d3670bee":"code","359a57e3":"code","15cbf17c":"code","7eff521d":"code","6917939e":"code","01720266":"code","521bf93a":"code","b4a7f33a":"code","9a90cff3":"code","125e2e0b":"code","ed7f090c":"code","820c2b12":"code","06d4db83":"code","a56b9a8a":"code","0dec1776":"code","f586f925":"code","851eb2b3":"code","b7118505":"code","b5e306c5":"code","fa37ebd7":"code","de7d806a":"code","aa166785":"code","5a52659c":"code","6d2c0536":"code","23fd940a":"code","62b0cf85":"code","15136144":"code","98434505":"code","62a4243d":"code","c9e8eb79":"code","8793d9be":"code","98897d95":"code","83b66768":"code","0159e23e":"code","7352c74a":"code","d73777d7":"code","6a992d10":"code","fda9b42c":"code","25b4d37c":"code","a3b2aaaf":"code","1072a3a5":"code","48ae42bd":"markdown","28b6ef91":"markdown","4c296302":"markdown","bb37ddd5":"markdown","16e3c875":"markdown","76503124":"markdown","2acba369":"markdown","cbb52a98":"markdown","9326585d":"markdown","ec8b9f70":"markdown","f0ba2631":"markdown","4856a983":"markdown","1b750386":"markdown","344bca1e":"markdown","bc02ebd8":"markdown","7d34b858":"markdown","541fe18d":"markdown","57905e3c":"markdown","d1bb7fc0":"markdown","a9b4abb6":"markdown","c9360ffd":"markdown","80bd0929":"markdown","9b23a8e6":"markdown","382852a4":"markdown","5211c85d":"markdown","70bb4c9b":"markdown"},"source":{"a72fe2dc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","875dfa71":"train_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\nprint(train_df.info())\nprint('-*'*20)\nprint(test_df.info())","48dcd172":"sample = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\nsample.head()","e95109db":"train_df.columns","e349799c":"train_df.head()","d3670bee":"train_df.describe()","359a57e3":"train_df['SalePrice'].describe()","15cbf17c":"import seaborn as sns\nfrom matplotlib import pyplot as plt\n#histogram\nsns.distplot(train_df['SalePrice']);","7eff521d":"#skewness and kurtosis\nprint(\"Skewness: %f\" % train_df['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train_df['SalePrice'].kurt())\n\n# If the kurtosis is greater than 3, then the dataset has heavier tails than a normal distribution (more in the tails).","6917939e":"#scatter plot grlivarea\/saleprice\nvar = 'GrLivArea'\ndata = pd.concat([train_df['SalePrice'], train_df[var]], axis=1)\nsns.regplot(x=var, y='SalePrice', data = data)","01720266":"#scatter plot totalbsmtsf\/saleprice\nvar = 'TotalBsmtSF'\ndata = pd.concat([train_df['SalePrice'], train_df[var]], axis=1)\nsns.regplot(x=var, y='SalePrice', data=data);","521bf93a":"#box plot overallqual\/saleprice\nvar = 'OverallQual'\ndata = pd.concat([train_df['SalePrice'], train_df[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);","b4a7f33a":"var = 'YearBuilt'\ndata = pd.concat([train_df['SalePrice'], train_df[var]], axis=1)\nf, ax = plt.subplots(figsize=(16, 8))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\nplt.xticks(rotation=90);","9a90cff3":"#correlation matrix\ncorrmat = train_df.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);","125e2e0b":"#saleprice correlation matrix\nk = 10 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train_df[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","ed7f090c":"#scatterplot\nsns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(train_df[cols], size = 2.5)\nplt.show();","820c2b12":"#missing data\ntotal = train_df.isnull().sum().sort_values(ascending=False)\npercent = (train_df.isnull().sum()\/train_df.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","06d4db83":"#dealing with missing data\ntrain_df = train_df.drop((missing_data[missing_data['Total'] > 1]).index,axis =1) #all the variables except Electrical are dropped\ntrain_df = train_df.drop(train_df.loc[train_df['Electrical'].isnull()].index) #droppinf the row containing 1 null value in Electrical variable\ntrain_df.isnull().sum().max() ","a56b9a8a":"#standardizing data to have mean 0 and standard deviation 1\nfrom sklearn.preprocessing import StandardScaler\nsaleprice_scaled = StandardScaler().fit_transform(train_df['SalePrice'][:,np.newaxis]);\nlow_range = saleprice_scaled[saleprice_scaled[:,0].argsort()][:10]\nhigh_range= saleprice_scaled[saleprice_scaled[:,0].argsort()][-10:]\nprint('outer range (low) of the distribution:')\nprint(low_range)\nprint('\\nouter range (high) of the distribution:')\nprint(high_range)","0dec1776":"from scipy.stats import norm\nfrom scipy import stats\ndef diagnostic_plots(train_df, variable):## defining a function to plot histogram and Q-Q plot\n    plt.figure(figsize = (15,6))\n    plt.subplot(1,2,1)\n    sns.distplot(train_df[variable], fit=norm);\n    plt.subplot(1,2,2)\n    stats.probplot(train_df[variable], dist = 'norm', plot = plt)\n    plt.show()","f586f925":"#histogram and normal probability plot of  SalePrice\ndiagnostic_plots(train_df, 'SalePrice')","851eb2b3":"#applying log transformation\ntrain_df['SalePrice'] = np.log(train_df['SalePrice'] + 1)# +1 is added in case there is any 0 input to it which would create issue in taking log\ndiagnostic_plots(train_df, 'SalePrice')","b7118505":"#histogram and normal probability plot of  GrLivArea\ndiagnostic_plots(train_df, 'GrLivArea')","b5e306c5":"#applying log transformation\ntrain_df['GrLivArea'] = np.log(train_df['GrLivArea'] + 1)# +1 is added in case there is any 0 input to it which would create issue in taking log\ndiagnostic_plots(train_df, 'GrLivArea')","fa37ebd7":"#histogram and normal probability plot of  TotalBsmtSF\ndiagnostic_plots(train_df, 'TotalBsmtSF')","de7d806a":"#applying reciprocal transformation\ntrain_df['GrLivArea'] = 1\/ (train_df['GrLivArea']+1)\ndiagnostic_plots(train_df, 'GrLivArea')","aa166785":"train_df.head()","5a52659c":"train_df.columns","6d2c0536":"train_df.info()","23fd940a":"train_df.describe()","62b0cf85":"#correlation matrix\ncorrmat = train_df.corr()\nf, ax = plt.subplots(figsize=(50, 20))\nsns.heatmap(corrmat, vmax=.8, square=True, annot = True);","15136144":"train_df.drop(['MSSubClass', 'OverallCond', 'BsmtFinSF2', 'LowQualFinSF', \\\n               'BsmtFullBath', 'KitchenAbvGr', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold', 'Id'], axis = 1, inplace = True)\ntrain_df.info()","98434505":"from sklearn.preprocessing import LabelEncoder\ncols = ( 'ExterQual', 'ExterCond','HeatingQC','KitchenQual', \n       'Functional',   'LandSlope',\n        'LotShape', 'PavedDrive', 'Street','CentralAir')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(train_df[c].values)) \n    train_df[c] = lbl.transform(list(train_df[c].values))\n\n# shape        \nprint('Shape all_data: {}'.format(train_df.shape))","62a4243d":"# Adding total sqfootage feature \ntrain_df['TotalSF'] = train_df['TotalBsmtSF'] + train_df['1stFlrSF'] + train_df['2ndFlrSF']","c9e8eb79":"train_df = pd.get_dummies(train_df)\ntrain_df.head()","8793d9be":"y= train_df['SalePrice']\nX = train_df.drop('SalePrice',axis =1)","98897d95":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","83b66768":"def rmse_cv(model):\n    rmse= np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv = 5))\n    return(rmse)","0159e23e":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\nlasso.fit(X,y)\npreds = np.expm1(lasso.predict(X))\nlasso_score = rmse_cv(lasso)\nprint(\"\\nLasso score: {:.4f} \\n\".format(lasso_score.mean()))","7352c74a":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\nENet.fit(X,y)\npreds = np.expm1(ENet.predict(X))\nENet_score = rmse_cv(ENet)\nprint(\"\\nENet score: {:.4f} \\n\".format(ENet_score.mean()))\n","d73777d7":"KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\nKRR.fit(X,y)\npreds = np.expm1(KRR.predict(X))\nKRR_score = rmse_cv(KRR)\nprint(\"\\nKRR score: {:.4f} \\n\".format(KRR_score.mean()))\n","6a992d10":"GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\nGBoost.fit(X,y)\npreds = np.expm1(GBoost.predict(X))\nGBoost_score = rmse_cv(GBoost)\nprint(\"\\nGBoost score: {:.4f} \\n\".format(GBoost_score.mean()))","fda9b42c":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\nmodel_xgb.fit(X,y)\npreds = np.expm1(model_xgb.predict(X))\nXGBoost_score = rmse_cv(model_xgb)\nprint(\"\\XGBoost score: {:.4f} \\n\".format(XGBoost_score.mean()))","25b4d37c":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\nmodel_lgb.fit(X,y)\npreds = np.expm1(model_lgb.predict(X))\nlgb_score = rmse_cv(model_lgb)\nprint(\"\\LightGBM score: {:.4f} \\n\".format(lgb_score.mean()))","a3b2aaaf":"solution = pd.DataFrame({\"id\":test_df.Id, \"SalePrice\":preds})\nsolution.to_csv(\"Submission.csv\", index = False)\n\nsolution.head(20)","1072a3a5":"sample.head()","48ae42bd":"# Data Visualisation","28b6ef91":"The numerical variables having correlation with SalePrice < | 0.2 | are dropped","4c296302":"Elastic Net Regression","bb37ddd5":"LASSO Regression","16e3c875":"LightGBM","76503124":"**Now we shall do the following:**\n\n* Correlation matrix (heatmap style).\n* 'SalePrice' correlation matrix (zoomed heatmap style).\n* Scatter plots between the most correlated variables (move like Jagger style).","2acba369":"The graph shown above is not a normally distributed one. Apply Logarithmic Transformation to convert it into a normal distribution","cbb52a98":"# Import Datasets","9326585d":"XGBoost","ec8b9f70":"Gradient Boosting Regression","f0ba2631":"We shall check the same for GrLivArea and TotalBsmtSF and apply the same procedure if these are not normally distributed.","4856a983":"It shows that a linear relationship exists between TotalBsmtSF and SalePrice","1b750386":"The Total Missing Data and their Percentage is shown below","344bca1e":"* 'OverallQual', 'GrLivArea' and 'TotalBsmtSF' are strongly correlated with 'SalePrice'.\n* 'GarageCars' and 'GarageArea' are also some of the most strongly correlated variables.in brothers. Therefore, we just need one of these variables in our analysis (we can keep 'GarageCars' since its correlation with 'SalePrice' is higher).\n* 'TotalBsmtSF' and '1stFloor' also seem to be twin brothers. We can keep 'TotalBsmtSF' just to say that our first guess was right \n* 'FullBath' can be ignored.\n* 'TotRmsAbvGrd' too has very less correlation and can be ignored.\n* 'YearBuilt' is slightly correlated with 'SalePrice'.","bc02ebd8":"It shows that a linear relationship exists between GrLivArea and SalePrice","7d34b858":"# MODELLING","541fe18d":"**Normality**","57905e3c":"# Convert Categorical Variables into ordinal numerics","d1bb7fc0":"It shows that with increasing OverallQual, SalePrice also increases.","a9b4abb6":"**Note:**\n* 'GrLivArea' and 'TotalBsmtSF' seem to be linearly related with 'SalePrice'. Both relationships are positive, which means that as one variable increases, the other also increases. In the case of 'TotalBsmtSF', we can see that the slope of the linear relationship is particularly high.\n* 'OverallQual' and 'YearBuilt' also seem to be related with 'SalePrice'. The relationship seems to be stronger in the case of 'OverallQual', where the box plot shows how sales prices increase with the overall quality.","c9360ffd":"Kernel Ridge Regression","80bd0929":"It can be seen that SalePrice has increased with year.","9b23a8e6":"Since GrLivArea is now normally distributed, we shall look into TotalBsmtSF","382852a4":"The missing data shall be removed now.","5211c85d":"Not normally distributed, so we shall apply logarithmic transformation.","70bb4c9b":"**Univariate analysis**"}}