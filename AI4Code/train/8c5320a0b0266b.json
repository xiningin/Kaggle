{"cell_type":{"0177d449":"code","094a96e4":"code","55f6e042":"code","04eb914f":"markdown","d2f823bc":"markdown","e88076ef":"markdown","d4df529c":"markdown","0a8aab2e":"markdown"},"source":{"0177d449":"import collections\nimport numpy as np\narray_norm = np.random.normal(10, 15, 1000) # mean =0, std = 0.1\n# def calculate_entropy(array: []):\ndist = collections.Counter(array_norm)\n","094a96e4":"# markov Decision process implemented from scratch","55f6e042":"class state(object):\n    ","04eb914f":"### A[01] Visualization","d2f823bc":"### A[03] Normalization\n#### A[03.01] [Batch Normalization](https:\/\/standardfrancis.wordpress.com\/2015\/04\/16\/batch-normalization\/)\n* [Paper](https:\/\/arxiv.org\/pdf\/1502.03167.pdf)\n  * Due to the parameter updates slows learning after using the activation function. So, We use the normalization to change the distribution of activations. **They call this change the internal covariate shift**\n  * Reduce the internal covariate shift to fixing the distribution of the layer input x as the training progresses. That expect to improve the training speed.\n  * Batch Normalizing Transform, applied to activation x over mini batch\n  * Pros And Cons:\n    * Pros\n      * \n    * Cons\n      * Can not used when the batch size is 1, but event slightly large batch sizes can cause problems.\n      * For the RNN. Batch normalization difficult to apply to recurrent connections due to each each time-step will have different statistics. That means The model to fit a separate batch normalization layer for each time-stap. This make model more complicated and more importanctly. take time to training.\n      ","e88076ef":"#### Gradient Boosting \n * input: training set with ${(x_i,y_u)}^n_i=1$ a differentiable loss function L(y, F(x)). number of interations M.\n * Algorithm: \n   1. Initialize model with a constant value: \n   $$F_0(x) = \\underset{\\gamma}{argmin}\\sum_{i=1}^n L(y_i,\\gamma)$$\n   2. for m in M: \n    1. compute so-called pseudo-residuals: \n    $$r_{im} = -\\Big[\\frac{\\delta L(y_i,F{x_i})}{\\delta F(x_i)}\\Big]_{F(x)=F_{m-1}x}$$\n    2. fit a base learner $h_m(x)$ to pseudo-residuals. train it using the training set ${(x_i, r_{im})}_{i=1}^n$\n    3. Compute multiplier $\\gamma_m = \\underset{\\gamma}{argmin}\\sum_{i=1}^n L(y_i,F_{m-1}(x_i)+\\gamma h_m(x_i))$\n    4. update the model: \n    $$F_m(x) = F_{m-1}(x) + \\gamma_mh_m(x)$$\n   3. ouput: $F_M(x)$\n * source:wikipedia ","d4df529c":"#### Decision Tree: \n* Information gain is the main key that is used by Decision Tree.\n* DT will always tries to maximize Information gain \n* The Information Gain that compute buy the entropy of parent and entropy of children\n1. What is Entropy: \n * for the Wiki media: \"Entropy of random variable is the average level of 'information', 'surprise' or 'uncertainty' inherent in the variable's possible outcomes.\n * So Entropy is the method that measure not only disorder.and that compute the uncertainty and the gold of the machine learning models and Ds in genral is to reduce uncertainty.\n * The Mathematical formula is as follows:  $E(S) = \\sum_{i=1}^{c} -p_ilog_2p_i$\n2. Why the Entropy measure the disorder and uncertainty of the data ?\n3. In this example. Etropy on the array that is created follow as probability density of the Gaussian distribution is: $p(x) = \\frac{1}{\\sqrt(2\\pi std^2)}e^{-\\frac{x-mean}{2*std^2}}$\n4. Information Gan: use to find the base features which serves a root node in terms of information gain.","0a8aab2e":"#### A[01.01] t - Distributed Stochastic Neighbor Embedding (t_SNE)\n* Is a technique for dimensionality reduction that is particularly well sited for the visualization of high-dimensional datasets.\n* The Idea as Van der Maaten and Hinton explained: \"The similarity of data point $x_j$ to data point $x_i$ is the condition probability, $p_{j|i}$, that $x_i$ would pick $x_j$ as its neighbor if neighbors were picked in proportion to their probability density under a Gaussian centered at $x_i$\"\n* [wiki media](https:\/\/en.wikipedia.org\/wiki\/T-distributed_stochastic_neighbor_embedding)"}}