{"cell_type":{"38f34ee8":"code","72af36a8":"code","13db0ca1":"code","ce4d0868":"code","35f0f9bd":"code","d2a2db34":"code","b4d466f7":"code","ece0c71f":"code","b67e404c":"code","eae4cd49":"code","d0254b62":"code","0419aaf5":"code","7dc4d52f":"code","f55b252c":"code","6ba729c9":"code","b8018655":"code","ac82a21a":"code","f38b6ca2":"code","a11b50ac":"code","098bd338":"code","0e21e034":"code","a066ef50":"code","e937dccf":"code","37c93bc1":"code","b407d7b4":"markdown","29f8b76e":"markdown","779e27c4":"markdown","edbf4625":"markdown","5f5b2a3c":"markdown","0c5300eb":"markdown","324dc505":"markdown","c30306f1":"markdown","12af3a18":"markdown","d70cd094":"markdown","ef6eb377":"markdown","f0fa537f":"markdown","1384509f":"markdown","58b18a4f":"markdown","a9ff7838":"markdown","69037ec4":"markdown","47b1e6e6":"markdown","f472c221":"markdown"},"source":{"38f34ee8":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\npd.set_option('display.max_colwidth',None)\npd.set_option('display.max_columns',None)\npd.set_option('display.max_rows',None)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","72af36a8":"data = pd.read_csv('\/kaggle\/input\/company-bankruptcy-prediction\/data.csv')\ndata.head()","13db0ca1":"Y = data['Bankrupt?']\ndata = data.drop(['Bankrupt?'], axis=1)","ce4d0868":"data.isnull().sum().values","35f0f9bd":"data_mean = np.mean(data, axis=0)\n\nvariance = np.var(data\/data_mean, axis=0)\nvar_df = pd.DataFrame(variance,columns=['variance'])\nvar_df.sort_values(by='variance').style.background_gradient(sns.light_palette('green', as_cmap=True))","d2a2db34":"from sklearn.feature_selection import VarianceThreshold\n\nvt = VarianceThreshold(5.0).fit(data\/data_mean)\n\nvt_transform = vt.transform(data\/data_mean)","b4d466f7":"high_var_cols = data.columns[vt.get_support()]\nlow_var_cols = data.columns[~vt.get_support()]\n\nprint(\"Removed cols :\",len(low_var_cols))\nprint(\"Remaining cols :\",len(high_var_cols))","ece0c71f":"corr_mat = np.corrcoef(vt_transform, rowvar=False)\ncorr_mat = pd.DataFrame(corr_mat)\n\ncorr_mat.style.background_gradient(sns.light_palette('blue', as_cmap=True))","b67e404c":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import RFE\n\nrfe = RFE(estimator = RandomForestClassifier(n_estimators=300, class_weight={0:1,1:2}), n_features_to_select=18, verbose=1).fit(vt_transform, Y.values)","eae4cd49":"vt_transform_rfe = rfe.transform(vt_transform)\nvt_transform_rfe.shape","d0254b62":"Y.value_counts()","0419aaf5":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\nimport xgboost  \n\nX_train, X_test,y_train, y_test = train_test_split(vt_transform_rfe, Y.values, test_size=0.25, stratify=Y.values)\nscaler = StandardScaler().fit(X_train)\n\nmodels = dict()\n\nmodels['Random Forest'] = RandomForestClassifier(n_estimators=300, class_weight={0:1,1:3})\nmodels['Logreg'] = LogisticRegression(penalty='elasticnet',  class_weight={0:1,1:3}, solver='saga', l1_ratio=0.7)\nmodels['GradientBoost'] = GradientBoostingClassifier(n_estimators=300)\nmodels['AdaBoost'] = AdaBoostClassifier(n_estimators=300)\nmodels['XGBoost'] = xgboost.XGBClassifier()\n\nfor model in models:\n    if model == 'Logreg':\n        train = scaler.transform(train)\n    else:\n        train = X_train\n    models[model].fit(train, y_train)\n    print(model + ' : fit')\n","7dc4d52f":"\nfor x in models:\n\n    if x == 'Logreg':\n        train = scaler.transform(X_train)\n    else:\n        train = X_train\n        \n    print('------------------------'+x+'------------------------')\n    model = models[x]\n    y_train_pred = model.predict(train)\n    arg_train = {'y_true':y_train, 'y_pred':y_train_pred}\n    print(confusion_matrix(**arg_train))\n    print(classification_report(**arg_train))\n    ","f55b252c":"for x in models:\n    \n    if x == 'Logreg':\n        test = scaler.transform(X_test)\n    else:\n        test=X_test\n    print('------------------------'+x+'------------------------')\n    model = models[x]\n    y_test_pred = model.predict(test)\n    arg_test = {'y_true':y_test, 'y_pred':y_test_pred}\n    print(confusion_matrix(**arg_test))\n    print(classification_report(**arg_test))","6ba729c9":"'''\nPrecision = \u0e40\u0e23\u0e32\u0e2d\u0e22\u0e32\u0e01\u0e17\u0e32\u0e22 1 \u0e43\u0e2b\u0e49\u0e16\u0e39\u0e01\/ \u0e43\u0e2b\u0e49\u0e42\u0e21\u0e40\u0e14\u0e25\u0e17\u0e32\u0e22 1 \u0e41\u0e21\u0e48\u0e19\u0e46 <br>\nRecall = \u0e40\u0e23\u0e32\u0e44\u0e21\u0e48\u0e2d\u0e22\u0e32\u0e01\u0e17\u0e32\u0e22 1 \u0e1c\u0e34\u0e14\n\n\u0e1a\u0e23\u0e34\u0e29\u0e31\u0e17\u0e19\u0e35\u0e49\u0e08\u0e30\u0e25\u0e49\u0e21\u0e25\u0e30\u0e25\u0e32\u0e22(1) \u0e41\u0e15\u0e48\u0e40\u0e23\u0e32\u0e17\u0e32\u0e22\u0e1c\u0e34\u0e14\u0e27\u0e48\u0e32\u0e21\u0e31\u0e19\u0e44\u0e21\u0e48\u0e25\u0e49\u0e21(0) = \u0e17\u0e32\u0e22 1 \u0e1c\u0e34\u0e14 = \u0e44\u0e21\u0e48\u0e2d\u0e22\u0e32\u0e01\u0e43\u0e2b\u0e49\u0e40\u0e01\u0e34\u0e14\u0e2a\u0e34\u0e48\u0e07\u0e19\u0e35\u0e49\u0e02\u0e36\u0e49\u0e19 = \u0e15\u0e49\u0e2d\u0e07\u0e01\u0e32\u0e23 recall \u0e2a\u0e39\u0e07\u0e46 \n\n\u0e42\u0e21\u0e40\u0e14\u0e25\u0e19\u0e35\u0e49\u0e21\u0e35 recall \u0e15\u0e48\u0e33\u0e21\u0e32\u0e01 = \u0e40\u0e1b\u0e47\u0e19\u0e42\u0e21\u0e40\u0e14\u0e25\u0e17\u0e35\u0e48\u0e44\u0e21\u0e48\u0e14\u0e35\n'''","b8018655":"# Test set\nmodel = models['XGBoost']\ny_test_pred_prob = model.predict_proba(X_test)\n\ny_test_pred_prob_lowerThres = y_test_pred_prob[:,1] > 0.1\n\narg_test = {'y_true':y_test, 'y_pred':y_test_pred_prob_lowerThres}\nprint(\"TEST\\n\")\nprint(confusion_matrix(**arg_test))\nprint(classification_report(**arg_test))\n\n\n# Train set\ny_train_pred_prob = model.predict_proba(X_train)\n\ny_train_pred_prob_lowerThres = y_train_pred_prob[:,1] > 0.1\n\narg_train = {'y_true':y_train, 'y_pred':y_train_pred_prob_lowerThres}\nprint(\"TRAIN\\n\")\nprint(confusion_matrix(**arg_train))\nprint(classification_report(**arg_train))","ac82a21a":"rfe.ranking_ \n#array([1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1])\n#--> 1 \u0e04\u0e37\u0e14\u0e44\u0e21\u0e48\u0e42\u0e14\u0e19\u0e15\u0e31\u0e14\u0e2d\u0e2d\u0e01\u0e44\u0e1b, 4 \u0e04\u0e37\u0e2d\u0e42\u0e14\u0e19\u0e15\u0e31\u0e14\u0e2d\u0e2d\u0e01\u0e44\u0e1b\u0e04\u0e19\u0e41\u0e23\u0e01, 3 \u0e04\u0e37\u0e2d\u0e42\u0e14\u0e19\u0e15\u0e31\u0e14\u0e04\u0e19\u0e16\u0e31\u0e14\u0e44\u0e1b\n\n#array([False, False, False, False, False, False,  True, False, False, False, False, False, False, False, False, False,  True, False, False, False, False])\n#array([10, 17, 14, 15, 11,  9,  1, 16, 19,  2, 13, 20, 18,  4,  7, 12,  1,  3,  5,  6,  8])","f38b6ca2":"from sklearn.metrics import make_scorer, recall_score\n\nrecall_scorer = make_scorer(recall_score)\ncv_score = cross_val_score(models['XGBoost'], X_train, y_train, cv=5, scoring=recall_scorer)","a11b50ac":"print('cv_score :', cv_score)\nprint('mean :',cv_score.mean())","098bd338":"from sklearn.metrics import roc_curve\n\nfig, ax = plt.subplots()\nfig.set_size_inches(13,6)\n\nfor m in models:\n    y_pred = models[m].predict_proba(X_test)\n    fpr, tpr, _ = roc_curve(y_test, y_pred[:,1].ravel())\n    plt.plot(fpr,tpr, label=m)\nplt.xlabel('False-Positive rate')\nplt.ylabel('True-Positive rate')\nplt.legend()\nplt.show()","0e21e034":"from sklearn.model_selection import RandomizedSearchCV\n\nparams = {'eta':[0.2,0.3,0.4],\n         'max_depth':[5,6,7],\n         'sampling_method':['uniform','gradient_based'],\n         'lambda':[1,1.5],\n         'alpha':[0,0.5],\n         }\n\nsearch = RandomizedSearchCV(estimator = models['XGBoost'], n_iter=50, scoring = recall_scorer, cv=5, verbose=1, param_distributions=params)","a066ef50":"search.fit(X_train, y_train)","e937dccf":"print(\"Best cv score :\",search.best_score_)\nprint(\"Best params :\",search.best_params_)","37c93bc1":"models['XGB_searched'] = search.best_estimator_\n\nmodel = models['XGB_searched']\ntest=X_test\nprint('TEST')\nprint('------------------------'+x+'------------------------')\ny_test_pred = model.predict_proba(test)[:,1] > 0.1\narg_test = {'y_true':y_test, 'y_pred':y_test_pred}\nprint(confusion_matrix(**arg_test))\nprint(classification_report(**arg_test))\n\n\ntrain = X_train\nprint(\"TRAIN\")\nprint('------------------------'+x+'------------------------')\ny_train_pred = model.predict_proba(train)[:,1] > 0.1\narg_train = {'y_true':y_train, 'y_pred':y_train_pred}\nprint(confusion_matrix(**arg_train))\nprint(classification_report(**arg_train))","b407d7b4":"# 2.1) Build the models","29f8b76e":"# 1.3) Recursive Feature Elimination <br>\nUsing Random Forest","779e27c4":"# 2.2) Performance in train set","edbf4625":"This data is **Highly imbalanced class** because not so many companies are bankrupted. <br>\nThus, we need to concern very much about **Recall** score because we don't want to misclassified the bankrupted.<br>\n\n\"This company is bankrupted, but we predicted that this company is safe\" -> We don't want this to happen.","5f5b2a3c":"### Lower the probability threshold to improve Recall score","0c5300eb":"Recall is also improved.","324dc505":"# 1) Feature Selection","c30306f1":"# 2.5) ROC curve","12af3a18":"# 1.1) Remove too low variance","d70cd094":"### Cross validation score on Recall score","ef6eb377":"# 2) Predictive models","f0fa537f":"We see that cv_score is improved.","1384509f":"# 2.6) Randomized search ","58b18a4f":"XGBoost have the greatest Recall !!","a9ff7838":"# 2.4) Take care of XGB","69037ec4":"We see that XGBoost has done a very good job!!","47b1e6e6":"# 2.3) Performance in test set","f472c221":"# 1.2) Inspect Correlation"}}