{"cell_type":{"94000950":"code","e88a5696":"code","6e1c2bee":"code","23d22f1e":"code","981a998f":"code","28384fc0":"code","d7f7a525":"code","1792440b":"code","b132d459":"code","df05b26b":"code","dfb2b3a9":"code","241b7425":"code","a7284bb9":"code","889c4c8f":"code","57cf39bc":"code","fcfc9825":"code","0f112182":"code","b2486501":"code","d1d30660":"code","48076b90":"code","fab45545":"code","7c456a73":"code","102c23dc":"code","fe13eb07":"markdown","bf1ef484":"markdown","d831fe69":"markdown","742b9545":"markdown","be0f7ea1":"markdown","a67ea9e0":"markdown","9a13bdcf":"markdown","d6631a6c":"markdown"},"source":{"94000950":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","e88a5696":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold, KFold, GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, confusion_matrix, f1_score","6e1c2bee":"# Read the data \nimport pandas as pd\n\ndf = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ndf_test  = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\n\n# Show some training data\ndf.head()","23d22f1e":"df_test.head()","981a998f":"print(\"Training :\")\nprint(\"Length of the data :\", len(df))\n# Missing value in the training set\nprint(df.isnull().sum())","28384fc0":"print(\"Test :\")\nprint(\"Length of the data :\", len(df_test))\n# Missing value in the test set\nprint(df_test.isnull().sum())","d7f7a525":"# Distributy if the target \ntarget_values = df['target'].value_counts()\nsns.barplot(target_values.index, target_values)\nplt.gca().set_ylabel('samples')","1792440b":"# df['target_mean'] = df.groupby('keyword')['target'].transform('mean')\n\n# fig = plt.figure(figsize=(8, 72), dpi=100)\n\n# sns.countplot(y=df.sort_values(by='target_mean', ascending=False)['keyword'],\n#               hue=df.sort_values(by='target_mean', ascending=False)['target'])\n\n# plt.tick_params(axis='x', labelsize=15)\n# plt.tick_params(axis='y', labelsize=12)\n# plt.legend(loc=1)\n# plt.title('Target Distribution in Keywords')\n\n# plt.show()\n\n# df.drop(columns=['target_mean'], inplace=True)","b132d459":"from nltk.tokenize import word_tokenize\n\n# Extract all the words\ntokens = word_tokenize(df[\"text\"][0])\n\n# Lowercase the words\ntokens = [word.lower() for word in tokens]\n\nprint(df[\"text\"][0])\nprint(tokens)","df05b26b":"# Remove all tokens that are not alphabetic\nwords = [word for word in tokens if word.isalpha()]\nprint(words)","dfb2b3a9":"# Filters - Remove stop words\nfrom nltk.corpus import stopwords\n\n# Get all stop words\nstop_words = set(stopwords.words(\"english\"))\n\nwords = [word for word in words if not word in stop_words]\nprint(words)","241b7425":"# Stem Words (Racinisation)\n# Process of reducins inflected words to their word stem, base or root form.\nfrom nltk.stem.porter import PorterStemmer\n\nporter = PorterStemmer()\nstemmed = [porter.stem(word) for word in words]\n\nprint(stemmed)","a7284bb9":"import re\nimport string\n\n#Function for removing URL\ndef remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\n#Function for removing HTML codes\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\n#Function for removing Emojis\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n\n#Function for removing punctuations\ndef remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\ndf['text']=df['text'].apply(remove_URL)\ndf['text']=df['text'].apply(remove_html)\ndf['text']=df['text'].apply(remove_emoji)\ndf['text']=df['text'].apply(remove_punct)\n\n\ndf_test['text'] = df_test['text'].apply(remove_URL)\ndf_test['text'] = df_test['text'].apply(remove_html)\ndf_test['text'] = df_test['text'].apply(remove_emoji)\ndf_test['text'] = df_test['text'].apply(remove_punct)\n","889c4c8f":"from nltk.stem import WordNetLemmatizer\n\n# Get all stop words\nstop_words = set(stopwords.words(\"english\"))\nporter = PorterStemmer()\nlemmatizer = WordNetLemmatizer()\n\ndef preprocess_text(text):\n    \n    # Extract all the words\n    tokens = word_tokenize(text)\n    \n    # Lowercase the words\n    tokens = [word.lower() for word in tokens]\n    \n    # Remove all tokens that are not alphabetic\n    words = [word for word in tokens if word.isalpha()]\n    \n    # Remove word in the stop word\n    words = [word for word in words if not word in stop_words]\n\n    # Get the root of the word \n    stemmed = [porter.stem(word) for word in words]\n    \n    # Lematize the word\n    lematized = [lemmatizer.lemmatize(word) for word in stemmed]\n\n    return lematized\n\ndf[\"preprocess_text\"] = df.text.apply(preprocess_text)\ndf_test[\"preprocess_text\"] = df_test.text.apply(preprocess_text)\ndf.head()","57cf39bc":"def join_list(tab):\n    return \" \".join(tab)\ndf[\"text_preprocessed\"] = df[\"preprocess_text\"].apply(join_list)\ndf_test[\"text_preprocessed\"] = df_test[\"preprocess_text\"].apply(join_list)\n\ndef transform_keyword(word) :\n    # Split when %20\n    return word.split('%20')\n\n# Transform NaN value to empty string\ndf[\"keyword\"] = df.keyword.fillna(\" \")\ndf_test[\"keyword\"] = df_test.keyword.fillna(\" \")\n\ndf[\"keyword\"] = df[\"keyword\"].apply(transform_keyword).apply(join_list)\ndf_test[\"keyword\"] = df_test[\"keyword\"].apply(transform_keyword).apply(join_list)\n\n# Concant keyword to the phrases\ndf[\"text_preprocessed\"] = df[\"keyword\"] + \" \" + df[\"text_preprocessed\"] \ndf_test[\"text_preprocessed\"] = df_test[\"keyword\"] + \" \" + df_test[\"text_preprocessed\"] ","fcfc9825":"from sklearn.model_selection import train_test_split\n\nX_all = pd.concat([df[\"text_preprocessed\"], df_test[\"text_preprocessed\"]])\n\ntfidf = TfidfVectorizer(stop_words = 'english')\ntfidf.fit(X_all)\n\nX = tfidf.transform(df[\"text_preprocessed\"])\nX_test = tfidf.transform(df_test[\"text_preprocessed\"])\ndel X_all\n\ntrain, test = train_test_split(df, test_size=0.2)\n\ntrain_x = train[\"text_preprocessed\"]\ntrain_y = train[\"target\"]\n\ntest_x = test[\"text_preprocessed\"]\ntest_y = test[\"target\"]\n\n\nX_train, X_val, y_train, y_val = train_test_split(X, df[\"target\"], test_size=0.1, random_state=42)\n","0f112182":"parameters = { \n    'gamma': [0.001, 0.01, 0.1, 0.4, 0.5, 0.6, 0.7, 1], \n    'kernel': ['rbf'], \n    'C': [0.001, 0.01, 0.1, 1, 1.5, 2, 3, 10],\n}\n\n# {'C': 2, 'gamma': 0.9, 'kernel': 'rbf'}","b2486501":"# parameters = { \n#     'gamma':  [0.5],\n#     'kernel': ['rbf'], \n#     'C':[2]\n# }\nmodel = GridSearchCV(SVC(), parameters, cv=10, n_jobs=-1).fit(X_train, y_train)","d1d30660":"model.cv_results_['params'][model.best_index_]","48076b90":"y_val_pred = model.predict(X_val)\naccuracy_score(y_val, y_val_pred), f1_score(y_val, y_val_pred)","fab45545":"confusion_matrix(y_val, y_val_pred)","7c456a73":"y_test_pred = model.predict(X_test)","102c23dc":"sub_df = pd.read_csv(os.path.join('..\/input\/nlp-getting-started\/', 'sample_submission.csv'))\nsub_df[\"target\"] = y_test_pred\nsub_df.to_csv(\"submission.csv\",index=False)","fe13eb07":"# Text preprocessing - Normalization\n\nIn this part, we are going to normalize our data. Firt of all, we are going to **tokenize** our data. We are going to split our phrase into array of word. We need to separate the word and also the ponctuation.","bf1ef484":"# SVM","d831fe69":"# Explore our data","742b9545":"# TF-IDF ","be0f7ea1":"# Real or Not? NLP with Disaster Tweets\n\nMachine learning can be very usefull, but take a long time to train and test. Others approches exist and can also have pretty good result. In this kernel, we are going to see how to implement the TF-IDF method and use a SVM network to do the classification.\n\n## Competition \n\nIn this competition, we want to predict which Tweets are about real disasters and which ones are not. Then for each tweet (input), we want to know if it refers to a disaster event (output). The output of our system will be a boolean (true or false).\n\n- Explore our data.\n- Preprocess our data\n- TF-IDF implementation.\n- SVM.\n- Result and tuning.","a67ea9e0":"As you can see, in our training data, we have 3 features :\n- Keyword : a particular keyword from the tweet (may be blank)\n- Location : the location the tweet was sent from (may be blank)\n- Text : the text of the tweet\n\nIn our approach, we will use only the **keyword** and the **text** features. ","9a13bdcf":"Then, we are going to remove all the ponctuation.","d6631a6c":"In order to describe our tweet with word, we need to remove the \"stopwords\". That means all the basic word like 'the', 'you'... These words are present in all documents, so they can't be use to differentiate tweet. "}}