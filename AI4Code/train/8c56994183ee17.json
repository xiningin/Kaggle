{"cell_type":{"364f49d0":"code","26280cc5":"code","b1bc4d10":"code","3039f162":"code","f55fc016":"code","fa5bfa30":"code","4f1c83ab":"code","19b53bbe":"code","d83e4139":"code","b29e053c":"code","f5e6921a":"code","a0cf23ac":"code","14f42201":"code","265eb72d":"code","4596dca9":"code","878ccfbe":"code","3c3b0270":"code","1c9c23b9":"code","859d94d9":"code","448a91d8":"code","c5de68be":"code","b191e2cb":"code","979a7dd5":"code","7707bd22":"code","62e95708":"code","1eb34620":"code","d81fbf00":"code","55637b52":"code","977ab933":"code","484e889c":"code","0d86f16a":"code","a6d4fd79":"markdown","dae03b61":"markdown","1ede4a6a":"markdown","926f044f":"markdown","b269254e":"markdown","9ea26a52":"markdown","002f868e":"markdown","d98feec9":"markdown","1a85ed76":"markdown","d9e80aaa":"markdown","b5cf1f7b":"markdown","bf88839f":"markdown","fcb29659":"markdown","880c10cb":"markdown","f570ef1b":"markdown"},"source":{"364f49d0":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","26280cc5":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom keras import Sequential\nfrom keras.backend import clear_session\nfrom keras.layers import Dense, Dropout, BatchNormalization, Input\nfrom keras.optimizers import Adam\n\nfrom tensorflow_addons.layers import WeightNormalization\nfrom tensorflow.keras import callbacks\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import log_loss\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.multioutput import ClassifierChain, MultiOutputClassifier\n\nfrom skopt import gp_minimize\nfrom skopt.space import Real, Categorical, Integer\nfrom skopt.utils import use_named_args","b1bc4d10":"#load data\ntrain_features = pd.read_csv('\/kaggle\/input\/lish-moa\/train_features.csv')\ntrain_targets = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv')\n\ntrain_features = train_features.set_index('sig_id')\ntrain_targets = train_targets.set_index('sig_id')\n\ntrain_targets.head()","3039f162":"print('Features shape:', train_features.shape)\nprint('Scored targets shape:', train_targets.shape)","f55fc016":"#features are in categories\ng_features = [x for x in train_features.columns if x.startswith('g-')]\nc_features = [x for x in train_features.columns if x.startswith('c-')]\nother_features = [x for x in train_features.columns if x not in g_features+c_features]","fa5bfa30":"#encode binary features\ntrain_features['cp_type'] = train_features['cp_type'].map({\n    'trt_cp' : 0,\n    'ctl_vehicle' : 1})\ntrain_features['cp_dose'] = train_features['cp_dose'].map({\n    'D1' : 0,\n    'D2' : 1})","4f1c83ab":"#top features for modelling\ntop_features = [  1,   2,   3,   4,   5,   6,   7,   9,  11,  14,  15,  16,  17,\n        18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  29,  30,  31,\n        32,  33,  35,  36,  37,  38,  39,  40,  41,  42,  43,  44,  46,\n        47,  48,  49,  50,  51,  52,  53,  54,  55,  56,  58,  59,  60,\n        61,  62,  63,  64,  65,  66,  67,  68,  69,  70,  71,  72,  73,\n        74,  75,  76,  78,  79,  80,  81,  82,  83,  84,  86,  87,  88,\n        89,  90,  91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101,\n       102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114,\n       115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128,\n       129, 130, 131, 132, 133, 136, 137, 138, 139, 140, 141, 142, 143,\n       144, 145, 146, 147, 149, 150, 151, 152, 153, 154, 155, 156, 157,\n       158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170,\n       171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n       184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197,\n       198, 199, 200, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212,\n       213, 214, 215, 216, 217, 218, 219, 220, 221, 223, 224, 225, 226,\n       227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239,\n       240, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n       254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266,\n       267, 268, 269, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280,\n       281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 294,\n       295, 296, 298, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309,\n       310, 311, 312, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323,\n       324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336,\n       337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n       350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362,\n       363, 364, 365, 366, 367, 368, 369, 370, 371, 374, 375, 376, 377,\n       378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 390, 391,\n       392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404,\n       405, 406, 407, 408, 409, 411, 412, 413, 414, 415, 416, 417, 418,\n       419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431,\n       432, 434, 435, 436, 437, 438, 439, 440, 442, 443, 444, 445, 446,\n       447, 448, 449, 450, 453, 454, 456, 457, 458, 459, 460, 461, 462,\n       463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n       476, 477, 478, 479, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n       490, 491, 492, 493, 494, 495, 496, 498, 500, 501, 502, 503, 505,\n       506, 507, 509, 510, 511, 512, 513, 514, 515, 518, 519, 520, 521,\n       522, 523, 524, 525, 526, 527, 528, 530, 531, 532, 534, 535, 536,\n       538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 549, 550, 551,\n       552, 554, 557, 559, 560, 561, 562, 565, 566, 567, 568, 569, 570,\n       571, 572, 573, 574, 575, 577, 578, 580, 581, 582, 583, 584, 585,\n       586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 599,\n       600, 601, 602, 606, 607, 608, 609, 611, 612, 613, 615, 616, 617,\n       618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630,\n       631, 632, 633, 634, 635, 636, 637, 638, 639, 641, 642, 643, 644,\n       645, 646, 647, 648, 649, 650, 651, 652, 654, 655, 656, 658, 659,\n       660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672,\n       673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685,\n       686, 687, 688, 689, 691, 692, 693, 694, 695, 696, 697, 699, 700,\n       701, 702, 704, 705, 707, 708, 709, 710, 711, 713, 714, 716, 717,\n       718, 720, 721, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732,\n       733, 734, 735, 737, 738, 739, 740, 742, 743, 744, 745, 746, 747,\n       748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 759, 760, 761,\n       762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774,\n       775, 776, 777, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788,\n       789, 790, 792, 793, 794, 795, 796, 797, 798, 800, 801, 802, 803,\n       804, 805, 806, 808, 809, 811, 813, 814, 815, 816, 817, 818, 819,\n       821, 822, 823, 825, 826, 827, 828, 829, 830, 831, 832, 834, 835,\n       837, 838, 839, 840, 841, 842, 845, 846, 847, 848, 850, 851, 852,\n       854, 855, 856, 858, 859, 860, 861, 862, 864, 866, 867, 868, 869,\n       870, 871, 872, 873, 874]","19b53bbe":"#prepare datasets for testing\nn_targets = train_targets.shape[1]\n\nX = train_features.iloc[:, top_features]\ny = train_targets.iloc[:,:n_targets]\n\nX = pd.get_dummies(X, columns = ['cp_time'])\n\nX.columns","d83e4139":"scale_cols = list(set(X.columns).intersection(set(g_features + c_features)))\nscaler = StandardScaler()\nX[scale_cols] = scaler.fit_transform(X[scale_cols])","b29e053c":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 5)\n\nprint(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)","f5e6921a":"#baseline model has two layers of 1000 neurons\nmodel = Sequential()\n\nmodel.add(Input(X_train.shape[1]))\n\nmodel.add(BatchNormalization())\nmodel.add(WeightNormalization(Dense(1000, activation = 'selu', kernel_initializer = 'lecun_normal')))\nmodel.add(Dropout(0.2))\n\nmodel.add(BatchNormalization())\nmodel.add(WeightNormalization(Dense(500, activation = 'selu', kernel_initializer = 'lecun_normal')))\nmodel.add(Dropout(0.4))\n\nmodel.add(BatchNormalization())\nmodel.add(WeightNormalization(Dense(250, activation = 'selu', kernel_initializer = 'lecun_normal')))\nmodel.add(Dropout(0.4))\n\nmodel.add(BatchNormalization())\nmodel.add(WeightNormalization(Dense(y_train.shape[1], activation = 'sigmoid')))\n\nmodel.compile(optimizer = 'adam', loss = 'binary_crossentropy')\n\nmodel.summary()","a0cf23ac":"early_stopping = callbacks.EarlyStopping(patience = 10, restore_best_weights = True)\nreduce_lr = callbacks.ReduceLROnPlateau(patience = 3, mode = 'min', monitor = 'val_loss')","14f42201":"model.fit(X_train, y_train,\n         batch_size = 64,\n         epochs = 20,\n         validation_data = (X_test, y_test),\n         callbacks = [early_stopping, reduce_lr])","265eb72d":"#metric for this comp is log loss\ndef score(y_test, y_pred):\n    metric = []\n    y_test, y_pred = np.array(y_test), np.array(y_pred)\n    for col in range(y_test.shape[1]):\n        metric.append(log_loss(y_test[:, col], y_pred[:, col].astype('float'), labels = [0, 1]))\n    return np.mean(metric)","4596dca9":"y_pred = model.predict(X_test)\ntest_loss = score(y_test, y_pred)\n\ntrain_loss = score(y_train, model.predict(X_train))\n\nprint('Train loss: {}'.format(train_loss))\nprint('Test loss: {}'.format(test_loss))","878ccfbe":"#function to create the model given settings\ndef build_model(hidden_layers, neurons, dropout_rate):\n    model = Sequential()\n\n    model.add(Input(X_train.shape[1]))\n    \n    for i in range(hidden_layers):\n        model.add(BatchNormalization())\n        model.add(WeightNormalization(Dense(neurons \/\/ 2**i, activation = 'selu', kernel_initializer = 'lecun_normal')))\n        model.add(Dropout(dropout_rate))\n\n    model.add(BatchNormalization())\n    model.add(Dense(y_train.shape[1], activation = 'sigmoid'))\n\n    model.compile(optimizer = Adam(), loss = 'binary_crossentropy')\n    \n    return model","3c3b0270":"#dimensions for each variable to optimise\ndim_hidden_layers = Integer(low = 2, high = 4, name = 'hidden_layers')\ndim_neurons = Integer(low = 1000, high = 2000, name = 'neurons')\ndim_dropout_rate = Real(low = 0.2, high = 0.5, name = 'dropout_rate')\n#dim_learning_rate = Real(low = 1e-4, high = 1e-2, prior = 'log-uniform', name = 'learning_rate')\n#dim_batch_size = Integer(low = 4, high = 16, name = 'batch_size')\n#dim_epochs = Integer(low = 10, high = 20, name = 'epochs')\n\ndimensions = [\n    dim_hidden_layers,\n    dim_neurons,\n    dim_dropout_rate]\n\ndefault = [3, 1024, 0.3]","1c9c23b9":"#the objective function that skopt will optimise\n@use_named_args(dimensions = dimensions)\ndef obj_fun(hidden_layers, neurons, dropout_rate):\n    model = build_model(\n        hidden_layers = hidden_layers,\n        neurons = neurons,\n        dropout_rate = dropout_rate)\n    \n    history = model.fit(X_train, y_train,\n                        batch_size = 64,\n                        epochs = 20,\n                        validation_data = (X_test, y_test),\n                        callbacks = [early_stopping, reduce_lr])\n    \n    logloss = score(y_test, model.predict(X_test))\n    \n    clear_session()\n    \n    return logloss","859d94d9":"clear_session() #good to clear session each time","448a91d8":"#perform the optimisation\nopt_result = gp_minimize(func = obj_fun,\n                         dimensions = dimensions,\n                         n_calls = 50,\n                         n_jobs = -1,\n                         x0 = default)","c5de68be":"#put the results of the optimisation into a dataframe\nresults = pd.DataFrame(opt_result.x_iters,\n                      columns = [\n                          'hidden_layers',\n                          'neurons',\n                          'dropout_rate'])\nresults['obj_fun'] = opt_result.func_vals","b191e2cb":"#top 10 results\nresults.sort_values('obj_fun', ascending = True).head(10)","979a7dd5":"kf = KFold(n_splits = 7, shuffle = True, random_state = 69)","7707bd22":"#prepare the submission input data\ntest = pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv')\ntest = test.set_index('sig_id')\n\ntest['cp_type'] = test['cp_type'].map({\n    'trt_cp' : 0,\n    'ctl_vehicle' : 1})\ntest['cp_dose'] = test['cp_dose'].map({\n    'D1' : 0,\n    'D2' : 1})\n\nX_test = test.iloc[:, top_features]\nX_test = pd.get_dummies(X_test, columns = ['cp_time'])\nX_test[scale_cols] = scaler.transform(X_test[scale_cols])","62e95708":"# fit the model to each fold and store val loss and test predictions\nval_losses = []\ny_preds = []\nfor i, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n    #split data\n    X_train, X_val = X.to_numpy()[train_idx], X.to_numpy()[val_idx]\n    y_train, y_val = y.to_numpy()[train_idx], y.to_numpy()[val_idx]\n    \n    clear_session()\n    #fit model with opt params\n    model = build_model(hidden_layers = opt_result.x[0],\n                        neurons = opt_result.x[1],\n                        dropout_rate = opt_result.x[2])\n\n    history = model.fit(X_train, y_train,\n             batch_size = 64,\n             epochs = 25,\n             validation_data = (X_val, y_val),\n             callbacks = [early_stopping, reduce_lr])\n    \n    val_losses.append(history.history['val_loss'][-1])\n    \n    #test predictions\n    y_pred = model.predict(X_test)\n    y_preds.append(y_pred)","1eb34620":"print('Mean val loss: {}'.format(np.mean(val_losses)))","d81fbf00":"predictions = np.mean(np.array(y_preds), axis = 0) #average the predictions from each fold","55637b52":"submission = pd.DataFrame(predictions, columns = train_targets.columns)\nsubmission['sig_id'] = test.index\nsubmission = submission[['sig_id']+list(train_targets.columns)]","977ab933":"#set ctl vehicle predictions to 0\nsubmission.loc[list(test.cp_type == 1), train_targets.columns] = 0","484e889c":"print(submission.shape)\nsubmission.head()","0d86f16a":"submission.to_csv('submission.csv', index = False)","a6d4fd79":"# Submission model","dae03b61":"The model performs fairly well however we need to optimise the hyperparameters to improve the test loss.","1ede4a6a":"# Data preparation","926f044f":"## Version History\n\n* v6 first submission\n* v9 added line to set ctl_vehicle predictions to 0\n* v11 add weight normalization, switched optimisation to fix the number of layers and optimise number of neurons\n* v12 change architecture and convert to ensemble for prediction","b269254e":"Prepare the file for submission.","9ea26a52":"The results show we have found a better solution than the baseline model. Despite this the results are slightly better than the baseline model. In future versions we may need to expand our search space or run for more optimisations.","002f868e":"# Baseline model","d98feec9":"Idea: use Bayesian Optimisation to optimize a neural network. Then fit a network to multiple CV folds and average predictions.","1a85ed76":"# Bayesian Optimisation","d9e80aaa":"We will split the data into 7 folds and fit the model to each. The fitted models will predict the test set and the predictions will be averaged for submission.","b5cf1f7b":"# Mechanism of Action Classification","bf88839f":"We will fit a nerual network with three hidden layers to give us an idea of the strength of the model.","fcb29659":"We will attempt to improve the model using Bayesian Optimisation. We will try to tune the number of hidden layers, number of neurons in the layers (constrained to fall by a factor of two each layer) and the dropout rate.","880c10cb":"# Submission","f570ef1b":"To prepare the data we will choose features recommended by DmitryS https:\/\/www.kaggle.com\/simakov\/keras-multilabel-neural-network-v1-2. We will split the data into training and test sets and perform rescaling."}}