{"cell_type":{"7f622f3b":"code","79f86693":"code","1b748696":"code","dcf52f07":"code","fdc15a56":"code","92f9128a":"code","13f7c02b":"code","4d8e5322":"code","db85b252":"code","04309ce8":"code","64e264cd":"code","e2e27474":"code","357a178a":"code","b40483d4":"code","85842de6":"code","1d17a500":"code","5a240c38":"code","a5eddb6b":"code","2c3e97f6":"code","eb1671fa":"code","e27f1ff8":"code","6d6c3711":"code","6462fcce":"code","4ec206bb":"code","928096b2":"code","7baac31a":"code","cf79b4e8":"markdown","b0253dde":"markdown","59dd514b":"markdown","7e3f4d95":"markdown","4cec2652":"markdown","8ba97145":"markdown","01766543":"markdown","3f2e76e2":"markdown"},"source":{"7f622f3b":"# Generic\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\nimport os, random,time\nfrom IPython.display import display, HTML\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# SK-Learn & Other Models\nfrom sklearn.naive_bayes import GaussianNB,MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nfrom sklearn.experimental import enable_hist_gradient_boosting   #enabling experimental feature\nfrom sklearn.ensemble import HistGradientBoostingClassifier\n\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nimport lightgbm as lgbm\nfrom xgboost import XGBClassifier\n\n# SK-Learn Metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\n\n# SK-Learn Preprocessing & Model Selection\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n# Garbage Collection\nimport gc\n\n# Plotting Fine-Tuning Plots\nimport matplotlib.pyplot as plt\nimport plotly.express as px","79f86693":"# Path\npath = '..\/input\/titanic\/'\n\n# Train Data\ntrain_data = pd.read_csv(os.path.join(path,'train.csv'), header='infer')\n\n# Test Data\ntest_data = pd.read_csv(os.path.join(path,'test.csv'), header='infer')\n\n# Submission\nsub_data = pd.read_csv(os.path.join(path,'gender_submission.csv'), header='infer')\n","1b748696":"# Extract Title from Name & Drop Name Column  [for both Train & Test]\ntrain_data['Title'] = train_data.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\ntrain_data.drop(['Name'], axis=1, inplace=True)\n\ntest_data['Title'] = test_data.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\ntest_data.drop(['Name'], axis=1, inplace=True)\n\n\n# Fill Missing Age with Median Age for Title [for both Train & Test]\nage_miss_tr = list(train_data[train_data.Age.isnull()].Title.unique())\nage_miss_ts = list(test_data[test_data.Age.isnull()].Title.unique())\n\nfor i in age_miss_tr:\n    median_age = train_data.groupby('Title')['Age'].median()[i]\n    train_data.loc[train_data['Age'].isnull() & (train_data['Title'] == i), 'Age'] = median_age\n\nfor i in age_miss_ts:\n    median_age = test_data.groupby('Title')['Age'].median()[i]\n    test_data.loc[test_data['Age'].isnull() & (test_data['Title'] == i), 'Age'] = median_age\n    \n\n# Test has only one person that has title = Ms\n# In Train Title = Ms has Age = 28\ntest_data.Age.fillna(28, inplace=True)  ","dcf52f07":"# Merge Titles\nmapping = {'Mlle': 'Miss', 'Major': 'Rare', 'Col': 'Rare', 'Sir': 'Rare', 'Don': 'Rare', 'Mme': 'Mrs',\n          'Jonkheer': 'Rare', 'Lady': 'Rare', 'Capt': 'Rare', 'Countess': 'Rare', 'Ms': 'Miss', 'Dona': 'Rare',\n           'Dr': 'Rare', 'Rev': 'Rare'}\n\ntrain_data.replace({'Title': mapping}, inplace=True)\ntest_data.replace({'Title': mapping}, inplace=True)\n\n# Age Groups\nbins = [ 0, 12, 30, 60, np.inf]\nlabels = ['Children', 'Teenager', 'Adult', 'Senior']\n\ntrain_data['AgeGroup'] = pd.cut(train_data[\"Age\"], bins, labels = labels)\ntest_data['AgeGroup'] = pd.cut(test_data[\"Age\"], bins, labels = labels)\n","fdc15a56":"# Creating a new feature from Sibling & Spouses\ntrain_data['Family'] = train_data['SibSp'] + train_data['Parch'] + 1\ntrain_data['TravelAlone']=np.where(train_data['Family']>1, 0, 1)\n\ntest_data['Family'] = test_data['SibSp'] + test_data['Parch'] + 1\ntest_data['TravelAlone']=np.where(test_data['Family']>1, 0, 1)\n\n# Creating a Group for Fare\nmissing_value = test_data[(test_data.Pclass == 3) & (test_data.Embarked == \"S\") & (test_data.Sex == \"male\")].Fare.mean()\ntest_data.Fare.fillna(missing_value, inplace=True)\n\ntrain_data['Fare_Bin'] = pd.qcut(train_data['Fare'], 5)\ntest_data['Fare_Bin'] = pd.qcut(test_data['Fare'], 5)","92f9128a":"# There are still 2 records in Train with missing records for Embarked column \n# Randomly fill these records\n\nembrk =  ['S','C','Q']\ntrain_data.Embarked.fillna(random.choice(embrk), inplace=True)","13f7c02b":"# Dropping unwanted columns from Train & Test\ndrop_list_tr = ['PassengerId','Age','SibSp','Parch','Ticket', 'Cabin', 'Fare']\ndrop_list_ts = ['Age','SibSp','Parch','Ticket', 'Cabin', 'Fare']\n\ntrain_data = train_data.drop(drop_list_tr, axis=1)\ntest_data = test_data.drop(drop_list_ts, axis=1)","4d8e5322":"# Encoding Sex & Embarked Column Data from Train & Test \nencoder= LabelEncoder()\ncols = ['Sex', 'Embarked', 'AgeGroup', 'Fare_Bin', 'Title']\n\nfor col in cols:\n    # Train\n    train_data[col] = encoder.fit_transform(train_data[col])\n    # Test\n    test_data[col] = encoder.fit_transform(test_data[col])\n","db85b252":"# Display Train & Test Data\n\ndef multi_table(table_list):\n    return HTML(\n        '<table><tr style=\"background-color:white;\">' +\n        ''.join(['<td>' + table._repr_html_() + '<\/td>' for table in table_list]) +\n        '<\/tr><\/table>'\n    )\n\nmulti_table([train_data.head(), test_data.head()])","04309ce8":"# Feature Selection & Data Split\n\nsize = 0.1  #90% train & 10% validation\n\nfeatures = ['Pclass', 'Sex', 'Embarked', 'Title', 'AgeGroup', 'Family', 'TravelAlone', 'Fare_Bin'] \ntarget = [\"Survived\"]\n\nX = train_data[features]   #feature\nY = train_data[target]     #target\n\nx_train, x_val, y_train, y_val = train_test_split(X,Y, test_size = size, random_state = 0)\n\n# Test Data\nX_test = test_data\nX_test = X_test.drop('PassengerId', axis=1)\n\n\n# Feature Scaling\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_val = sc.fit_transform(x_val)","64e264cd":"# Garbage Collection\ngc.collect()","e2e27474":"# -- Building Model List --\nmodels = []\n\nmodels.append(('GaussianNB', GaussianNB()))\nmodels.append(('RandomForest', RandomForestClassifier(verbose=0, random_state=0)))\nmodels.append(('LinearSVC', LinearSVC(verbose=0, random_state=0)))\nmodels.append(('SVM', SVC(verbose=0, random_state=0)))\nmodels.append(('DecisionTree', DecisionTreeClassifier(random_state=0)))\nmodels.append(('GradientBoost', GradientBoostingClassifier(verbose=0, random_state=0)))\nmodels.append(('StochasticGradient', SGDClassifier(loss=\"log\", penalty=\"l2\", max_iter=5, verbose=0, random_state=0)))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('HistGradientBoost', HistGradientBoostingClassifier(verbose=0, random_state=0)))\nmodels.append(('LiteGradientBoost', lgbm.LGBMClassifier(silent=0, random_state=0)))\nmodels.append(('XGradientBoost', XGBClassifier()))","357a178a":"# -- Model Evaluation --\nmodel_results = []\n\nfor name, model in models:\n    \n    kfold = KFold(n_splits=10, random_state=None, shuffle=False)\n    cross_val = cross_val_score(model, x_train, y_train, cv=kfold, scoring='accuracy')\n    model_results.append([name,'{:.2%}'.format(cross_val.mean())])\n    \n# -- Model Evaluation Table --\nmodel_df = pd.DataFrame(model_results, columns=['Model','Mean_Accuracy'])\nmodel_df.sort_values(by='Mean_Accuracy', ascending=False)","b40483d4":"# Garbage Collection\ngc.collect()","85842de6":"# Import Optuna\nimport optuna\n\noptuna.logging.set_verbosity(optuna.logging.WARNING)\n\nclass model_objectif(object):\n    def __init__(self, models, x, y):\n        self.models = models\n        self.x = x\n        self.y = y\n\n    def __call__(self, trial):\n        models, x, y = self.models, self.x, self.y\n\n        classifier_name = models\n\n        if classifier_name == \"SVM\": \n            \n            model = SVC( C = trial.suggest_loguniform('C', 0.1, 10),\n                         kernel = trial.suggest_categorical('kernel', ['linear', 'rbf', 'sigmoid']),\n                         gamma  = trial.suggest_categorical('gamma', [\"scale\", \"auto\"]),\n                         verbose=0)\n            \n            model.fit(x_train, y_train)\n\n        elif classifier_name == \"GradientBoost\":\n            \n            model = GradientBoostingClassifier( n_estimators = trial.suggest_int('n_estimators', 10, 100),\n                                    min_samples_leaf =  trial.suggest_int('min_samples_leaf', 1, 10),\n                                    max_depth = trial.suggest_int('max_depth', 1, 6),\n                                    min_samples_split = trial.suggest_int('min_samples_split', 2, 16),\n                                    max_features = trial.suggest_categorical('max_features', ['auto', 'sqrt', 'log2', None]),\n                                    verbose=0)\n\n            model.fit(x_train, y_train)\n\n  \n        y_pred   = model.predict(x_val)\n        acc_score = round(accuracy_score(y_pred, y_val) * 100, 2)\n        return acc_score\n\n    \ndef parameters(study_model):\n\n    print(\"Number of finished trials: {}\".format(len(study_model.trials)))\n    print(\"Best trial:\")\n    trial = study_model.best_trial\n    best_params = study_model.best_params\n    print(\"  Score_value: {}\".format(trial.value))\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))\n\n    return best_params, trial.value\n\n\ndef visual_study(study_model):\n\n    fig = optuna.visualization.plot_optimization_history(study_model)\n    fig.show()\n","1d17a500":"# Fine Tuning Support Vector Machine (SVM)\nmodel = 'SVM'\nobjective = model_objectif(model, X, Y)\nstudy_SVM = optuna.create_study(direction='maximize')\nstudy_SVM.optimize(objective, n_trials=150)","5a240c38":"# Fetching the best parameters\nbest_params_SVM, best_score_SVM = parameters(study_SVM)","a5eddb6b":"# Plot\nvisual_study(study_SVM)","2c3e97f6":"# Fine Tuning Gradient Boost\nmodel = 'GradientBoost'\nobjective = model_objectif(model, X, Y)\nstudy_GB = optuna.create_study(direction='maximize')\nstudy_GB.optimize(objective, n_trials=150)","eb1671fa":"# Fetching the best parameters\nbest_params_GB, best_score_GB = parameters(study_GB)","e27f1ff8":"# Plot\nvisual_study(study_GB)","6d6c3711":"# create dataframe score tuning\nmodel_df_tuned = pd.DataFrame({\n    'Model': ['Gradient Boost','SVM'],\n    'FinedTuned_Acc': [best_score_GB, best_score_SVM] })\n\nmodel_df_tuned","6462fcce":"# Garbage Collection\ngc.collect()","4ec206bb":"# Making Prediction with GradientBoost & Optuna HyperParameters\nmodel = GradientBoostingClassifier(**best_params_GB, verbose=0)\nmodel.fit(x_train,y_train)\ny_pred = model.predict(X_test)","928096b2":"submission = pd.DataFrame({\n    \"PassengerId\": test_data['PassengerId'], \n    \"Survived\": y_pred\n})","7baac31a":"submission.to_csv('submission.csv', index=False)","cf79b4e8":"# Model Comparison & Fine-tuning\n\nIn this notebook\/submission we'll be comparing a range of supervised learning models from SK-Learn library. These models will be trained on the [Titanic: Machine Learning from Disaster](https:\/\/www.kaggle.com\/c\/titanic) dataset.\n\n**Note:** The Titanic dataset is a typical classifier problem & all the models listed below are classifiers.\n\n\n\n**List of Models compared:**\n* Gaussian Naive Bayes\n* Random Forest\n* Linear SVC (Support Vector Classifier)\n* Decision Tree\n* Gradient Boost\n* Stochastic Gradient Descent\n* KNN\n* LDA (Linear Discriminant Analysis)\n* Histogram Based Gradient Boost\n* Light Gradient Boost\n* Extreme Gradient Boost\n\n\n### Note: This is an exclusively model-comparison & fine-tuning notebook with NO EDA performed.","b0253dde":"We shall use Gradient Boost as our model of choice for Prediction & Submission.","59dd514b":"# Fine-Tuning\n\nFor fine-tuning we'll be using Optuna, an automatic hyperparameter optimization framework. ","7e3f4d95":"# Data (Load, Prep & Split)","4cec2652":"### As we can observe, SVM & Gradient Boost have the highest accuracy. We will now continue fine-tuning these two models.","8ba97145":"# Cross Validation of Models","01766543":"# Libraries","3f2e76e2":"# Prediction & Submission"}}