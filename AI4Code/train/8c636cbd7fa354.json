{"cell_type":{"9d7f4319":"code","b6212e29":"code","2d8b8ab8":"code","a054bd45":"code","5cbb3636":"code","5886f069":"code","21f51a51":"code","356ca9a3":"code","d98cda2f":"code","40e530c8":"code","b3c43354":"code","6147775e":"code","bc76f1cd":"code","1edbe776":"code","be05fced":"code","c0835985":"code","5bc55329":"code","d341ccc4":"code","56767d42":"code","0312614f":"code","5cfedacd":"code","6c460d49":"code","fe4f419b":"markdown","42c948fa":"markdown","95aa7b70":"markdown","a0b0f81c":"markdown"},"source":{"9d7f4319":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b6212e29":"import h2o\nfrom h2o.automl import H2OAutoML\nfrom sklearn.metrics import mean_squared_error","2d8b8ab8":"DATASET_PATH = '\/kaggle\/input\/real-time-advertisers-auction\/Dataset.csv'\ndataset = pd.read_csv(DATASET_PATH)","a054bd45":"dataset.head()","5cbb3636":"dataset.info()","5886f069":"dataset['date'] = dataset['date'].apply(lambda x: pd.to_datetime(x))","21f51a51":"dataset['cpm'] = (dataset['total_revenue'] * 100 \/ dataset['measurable_impressions']) * 1000\ndataset['cpm'] = dataset['cpm'].replace(np.inf, 0)\ndataset['cpm'] = dataset['cpm'].fillna(0)","356ca9a3":"dataset = dataset[dataset['cpm'] >= 0]\ndataset = dataset[dataset['cpm'] < dataset['cpm'].quantile(0.95)]","d98cda2f":"dataset['cpm'].describe()","40e530c8":"dataset = dataset.drop(columns=['total_revenue'])","b3c43354":"DATE_X = pd.to_datetime('21.06.2019')","6147775e":"train = dataset[dataset['date'] <= DATE_X]\ntest = dataset[dataset['date'] > DATE_X]","bc76f1cd":"print(f\"Train size = {train.shape[0]}, test size = {test.shape[0]}\") ","1edbe776":"train.nunique()","be05fced":"train = train.drop(columns=['revenue_share_percent', 'integration_type_id'])\ntest = test.drop(columns=['revenue_share_percent', 'integration_type_id'])","c0835985":"cat_features = [\n                'site_id', 'ad_type_id','geo_id', 'device_category_id',\n               'advertiser_id', 'order_id', 'line_item_type_id', 'os_id',\n                'monetization_channel_id', 'monetization_channel_id'\n               ]","5bc55329":"h2o.init()","d341ccc4":"train = h2o.H2OFrame(train)\ntest = h2o.H2OFrame(test)\nx = train.columns\ny = \"cpm\"\nx.remove(y)\ntrain[cat_features] = train[cat_features].asfactor()\ntest[cat_features] = test[cat_features].asfactor()","56767d42":"aml = H2OAutoML(max_runtime_secs= 5 * 60)\naml.train(x=x, y=y, training_frame=train, leaderboard_frame=test)","0312614f":"preds = aml.predict(test).as_data_frame()['predict'].values","5cfedacd":"print(f\"Baseline in this dataset is {mean_squared_error(test[y].as_data_frame().values, preds)}\")","6c460d49":"aml.leaderboard","fe4f419b":"## Import some usefull libraries","42c948fa":" #### In this notebook i will use [h2o](https:\/\/h2o.ai) as simple baseline\n#### h2o is very usefull tool, that can show you what model performance you can get in \"fit-predict-data-scientist\" mode\n#### Moreover, h2o is very strong baseline, that is not always so easy to beat. So, using h2o as a baseline is help you to understand the real plank, you should get to approve that you can do smth cool","95aa7b70":"## Modeling","a0b0f81c":"## Slightly preprocess dataset"}}