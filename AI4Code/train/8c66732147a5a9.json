{"cell_type":{"d20bf5da":"code","1a13fd91":"code","e451c2be":"code","582b442d":"code","870924c0":"code","1027d3ab":"code","d3caee58":"code","3cb253b0":"code","b6971e53":"code","8fb5a54b":"code","cb5ea39f":"code","49743137":"code","0ea3f7f1":"code","9baa93a2":"code","69668825":"code","c10b27a6":"code","cbf0d99b":"code","c65d3b9d":"code","8328792e":"code","8b6defb5":"code","384efafd":"code","bd2babe7":"code","473b85db":"code","bddc1d77":"code","36178e02":"code","0b9f0101":"code","8d1c55d4":"code","84854d90":"code","8343ed00":"code","7ffcbb88":"code","8615672d":"code","951b54f2":"code","ce4a92ad":"code","652c4665":"code","316510a7":"code","ae1498e8":"code","f3fed845":"code","b3766c43":"code","25937205":"code","7dc45ab0":"code","e83807c9":"code","616b1c74":"code","8d48e203":"code","9a0e279d":"code","bead06e8":"code","6a1da03c":"code","7605a31e":"markdown","66d3eec7":"markdown","ea3ea111":"markdown","fe0971c8":"markdown","3f5f0b97":"markdown","9290cfa3":"markdown","3e2241dc":"markdown","f4ec7efb":"markdown","6fc90c21":"markdown","3944354a":"markdown","8f278823":"markdown","42d89187":"markdown","15cf6972":"markdown","74d228a5":"markdown","b0ddd83e":"markdown","c378ab87":"markdown","bc9b6370":"markdown","289cbc93":"markdown","920a3c47":"markdown","ea1d1a1a":"markdown","8cfc0e1c":"markdown","58be87c4":"markdown","8fbd3247":"markdown","c25dcbe8":"markdown","b6e84e9a":"markdown","7d409f79":"markdown","28df758c":"markdown","1a277f9d":"markdown","e9ad6f54":"markdown","edf77f88":"markdown"},"source":{"d20bf5da":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport warnings\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n#ignore warnings for convenience\nwarnings.filterwarnings(\"ignore\")\n\nimport os\n\nprint(os.listdir(\"..\/input\/vectordigits\"))\n\n# Any results you write to the current directory are saved as output.","1a13fd91":"#extracting training data and obtaining the basic info\n\ntrain_data = pd.read_csv(\"..\/input\/vectordigits\/training.csv\")\ntrain_data.info()","e451c2be":"y = train_data['label']\ntrain_data = train_data.drop(['label'], axis = 1)","582b442d":"train_data.describe()","870924c0":"from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nscaled_data = pd.DataFrame(ss.fit_transform(train_data.values), columns=train_data.columns, index=train_data.index)\nscaled_data.head()","1027d3ab":"from sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\npca = PCA(n_components = len(train_data.columns), whiten = True)\npca.fit(train_data)\nplt.bar(range(len(train_data.columns)), pca.explained_variance_ratio_)","d3caee58":"import seaborn as sns\n\ng = sns.heatmap(train_data.corr(), cmap = \"coolwarm\")","3cb253b0":"from sklearn import tree\nmodel_1 = tree.DecisionTreeClassifier(random_state = 1) #decision tree\nmodel_1.fit(train_data, y)\nfrom sklearn.model_selection import cross_val_score\ncross_val_score(model_1, train_data, y, scoring = 'accuracy', cv = 5).mean()","b6971e53":"from sklearn.model_selection import GridSearchCV\npars = {\n    'criterion': ['gini', 'entropy'],\n    'max_features': ['auto', 'log2', None],\n    'min_samples_split': [2, 4, 5, 10],\n    'max_depth': [None, 5, 10, 15]\n}\n\ngs = GridSearchCV(estimator = tree.DecisionTreeClassifier(random_state = 1), param_grid = pars, \n                  scoring = 'accuracy', cv = 5)\n\ngs.fit(train_data, y)\n\ngs.best_estimator_","8fb5a54b":"gs.best_score_","cb5ea39f":"model_1 = gs.best_estimator_\nmodel_1.fit(train_data, y)\ncross_val_score(model_1, train_data, y, scoring = 'accuracy', cv = 5).mean()","49743137":"model_1_s = gs.best_estimator_\nmodel_1_s.fit(scaled_data, y)\ncross_val_score(model_1_s, scaled_data, y, scoring = 'accuracy', cv = 5).mean()\n\nscores = dict({}) #holds scores of different algorithms\nscores['Decision tree'] = cross_val_score(model_1_s, scaled_data, y, scoring = 'accuracy', cv = 5).mean()","0ea3f7f1":"pars = {\n    'criterion': ['gini', 'entropy'],\n    'max_features': ['auto', 'log2', None],\n    'min_samples_split': [2, 4, 5, 10],\n    'max_depth': [None, 5, 10, 15]\n}\n\ngs = GridSearchCV(estimator = tree.DecisionTreeClassifier(random_state = 1), param_grid = pars, \n                  scoring = 'accuracy', cv = 5)\n\ngs.fit(scaled_data, y)\n\ngs.best_estimator_","9baa93a2":"gs.best_score_","69668825":"from sklearn.ensemble import RandomForestClassifier\nmodel_2 = RandomForestClassifier(random_state = 1)\nmodel_2.fit(scaled_data, y)\ncross_val_score(model_2, scaled_data, y, scoring = 'accuracy', cv = 5).mean()","c10b27a6":"from matplotlib.pyplot import plot\naccuracy = []\nfor i in range(30):\n    a = cross_val_score(RandomForestClassifier(random_state = 1, n_estimators = i+1), \n                           scaled_data, y, scoring = 'accuracy', cv = 5).mean()\n    accuracy.append(a)\nplot(accuracy)","cbf0d99b":"pars = {\n    'n_estimators': [10, 50, 100, 250],\n    'criterion': ['gini', 'entropy'],\n    'min_samples_split': [2, 4, 5, 10],\n    'max_depth': [None, 5, 10, 15]\n}\n\ngs = GridSearchCV(estimator = RandomForestClassifier(random_state = 1), param_grid = pars, \n                  scoring = 'accuracy', cv = 5)\n\ngs.fit(scaled_data, y)\n\ngs.best_estimator_","c65d3b9d":"gs.best_score_","8328792e":"model_2 = gs.best_estimator_\nscores['Random forest'] = cross_val_score(model_2, scaled_data, y, scoring = 'accuracy', cv = 5).mean()\nprint(scores)","8b6defb5":"from sklearn.linear_model import LogisticRegression\nmodel_3 = LogisticRegression(random_state = 1)\nmodel_3.fit(scaled_data, y)\ncross_val_score(model_3, scaled_data, y, scoring = 'accuracy', cv = 5).mean()","384efafd":"pars = {\n    'l1_ratio': [0, 0.25, 0.5, 0.75, 1],\n    'max_iter': [50, 100, 250],\n}\n\ngs = GridSearchCV(estimator = LogisticRegression(random_state = 1, penalty = 'elasticnet', solver = 'saga'), param_grid = pars, \n                  scoring = 'accuracy', cv = 5)\n\ngs.fit(scaled_data, y)\n\ngs.best_estimator_","bd2babe7":"gs.best_score_","473b85db":"model_3 = gs.best_estimator_\nscores['Logistic regression'] = cross_val_score(model_3, scaled_data, y, scoring = 'accuracy', cv = 5).mean()\nprint(scores)","bddc1d77":"from sklearn.ensemble import AdaBoostClassifier\nmodel_4 = AdaBoostClassifier(random_state = 1)\nmodel_4.fit(scaled_data, y)\ncross_val_score(model_4, scaled_data, y, scoring = 'accuracy', cv = 5).mean()","36178e02":"pars = {\n    'n_estimators': [100, 250, 500],\n    'learning_rate': [0.25, 0.5, 0.75, 1.0, 1.5],\n}\n\ngs = GridSearchCV(estimator = AdaBoostClassifier(random_state = 1), param_grid = pars, \n                  scoring = 'accuracy', cv = 5)\n\ngs.fit(scaled_data, y)\n\ngs.best_estimator_","0b9f0101":"gs.best_score_","8d1c55d4":"model_4 = gs.best_estimator_\nscores['AdaBoost'] = cross_val_score(model_4, scaled_data, y, scoring = 'accuracy', cv = 5).mean()\nprint(scores)","84854d90":"accuracy = []\nfor i in range(50):\n    a = cross_val_score(AdaBoostClassifier(random_state = 1, n_estimators = i+1, learning_rate = 0.5), \n                           scaled_data, y, scoring = 'accuracy', cv = 5).mean()\n    accuracy.append(a)\nplot(accuracy)","8343ed00":"from sklearn.ensemble import GradientBoostingClassifier\nmodel_5 = GradientBoostingClassifier(random_state = 1)\nmodel_5.fit(scaled_data, y)\ncross_val_score(model_5, scaled_data, y, scoring = 'accuracy', cv = 5).mean()","7ffcbb88":"pars = {\n    'n_estimators': [15, 25, 35, 45],\n    'learning_rate': [0.05, 0.1, 0.15, 0.2],\n    'max_depth':[1,2,3]\n}\n\ngs = GridSearchCV(estimator = GradientBoostingClassifier(random_state = 1, warm_start = True), param_grid = pars, \n                  scoring = 'accuracy', cv = 5)\n\ngs.fit(scaled_data, y)\n\ngs.best_estimator_","8615672d":"gs.best_score_","951b54f2":"model_5 = gs.best_estimator_\nscores['GradientBoost'] = cross_val_score(model_5, scaled_data, y, scoring = 'accuracy', cv = 5).mean()\nprint(scores)","ce4a92ad":"accuracy = []\nfor i in range(50):\n    a = cross_val_score(GradientBoostingClassifier(random_state = 1, n_estimators = i+1, learning_rate = 0.15, max_depth = 1), \n                           scaled_data, y, scoring = 'accuracy', cv = 5).mean()\n    accuracy.append(a)\nplot(accuracy)","652c4665":"from xgboost import XGBClassifier\nmodel_6 = XGBClassifier(random_state = 1)\nmodel_6.fit(scaled_data, y)\ncross_val_score(model_6, scaled_data, y, scoring = 'accuracy', cv = 5).mean()","316510a7":"pars = {\n    #'lambda': [0, 0.5, 1],\n    'learning_rate':[0.05, 0.1, 0.15, 0.2, 0.3],\n    'max_depth':[6, 8, 10],\n    'min_child_weight':[0.25, 0.5, 0.75, 1],\n    'n_estimators':[100, 250, 500]\n}\n\n#best results - learning_rate = 0.05, max_depth = 6, min_child_weight = 0.75, n_estimators = 250\n\n#gs = GridSearchCV(estimator = XGBClassifier(random_state = 1), param_grid = pars, \n#                  scoring = 'accuracy', cv = 5)\n\n#gs.fit(scaled_data, y)\n\n#gs.best_estimator_","ae1498e8":"model_6 = XGBClassifier(learning_rate = 0.05, max_depth = 6, min_child_weight = 0.75, n_estimators = 250)\nmodel_6.fit(scaled_data, y)\nscores['XGBoost'] = cross_val_score(model_6, scaled_data, y, scoring = 'accuracy', cv = 5).mean()\nprint(scores)","f3fed845":"accuracy = []\nfor i in range(50):\n    a = cross_val_score(XGBClassifier(random_state = 1, n_estimators = i+1, learning_rate = 0.05, max_depth = 6, min_child_weight = 0.75), \n                           scaled_data, y, scoring = 'accuracy', cv = 5).mean()\n    accuracy.append(a)\nplot(accuracy)","b3766c43":"from sklearn.svm import SVC\nmodel_7 = SVC() \nmodel_7.fit(scaled_data, y)\ncross_val_score(model_7, scaled_data, y, scoring = 'accuracy', cv = 5).mean()","25937205":"pars = {\n    'kernel':['linear', 'rbf', 'poly', 'sigmoid'],\n    'C':[0.1, 0.2, 0.4, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0]\n}\n\ngs = GridSearchCV(estimator = SVC(), param_grid = pars, \n                  scoring = 'accuracy', cv = 5)\n\ngs.fit(scaled_data, y)\n\ngs.best_estimator_","7dc45ab0":"gs.best_score_","e83807c9":"model_7 = gs.best_estimator_\nmodel_7.fit(scaled_data, y)\nscores['SVC'] = cross_val_score(model_7, scaled_data, y, scoring = 'accuracy', cv = 5).mean()\nprint(scores)","616b1c74":"from sklearn.naive_bayes import BernoulliNB\nmodel_8 = BernoulliNB(binarize=0.0)\nmodel_8.fit(scaled_data, y)\ncross_val_score(model_8, scaled_data, y, scoring = 'accuracy', cv = 5).mean()","8d48e203":"pars = {\n    'alpha':[0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2]\n}\n\ngs = GridSearchCV(estimator = BernoulliNB(), param_grid = pars, \n                  scoring = 'accuracy', cv = 5)\n\ngs.fit(scaled_data, y)\n\ngs.best_estimator_","9a0e279d":"gs.best_score_","bead06e8":"model_8 = gs.best_estimator_\nmodel_8.fit(scaled_data, y)\nscores['BernoulliNB'] = cross_val_score(model_8, scaled_data, y, scoring = 'accuracy', cv = 5).mean()\nprint(scores)","6a1da03c":"from sklearn.naive_bayes import GaussianNB\nmodel_9 = GaussianNB()\nmodel_9.fit(scaled_data, y)\nscores['GaussianNB'] = cross_val_score(model_9, scaled_data, y, scoring = 'accuracy', cv = 5).mean()\nprint(scores)","7605a31e":"Now we can tune hyperparameters, like we did with the single decision tree:","66d3eec7":"This is slightly worse than AdaBoost. Convergence of accuracy can be seen in the figure below:","ea3ea111":"We clearly see that the ranges of values of each feature are too large to encode it in binary. There are no missing values, so no imputing is needed. We might want to create a normalized (scaled) dataset:  ","fe0971c8":"As we can see, most features contain negligible amount of information. For now, we do not drop any of them, since their number is not too large. The correlation map looks like:","3f5f0b97":"Next, we try to implement boosted algorithms. The first one we look at is adaptive boosting:","9290cfa3":"We can clearly see that random forest helps achieve a higher accuracy. Now let us look at how the accuracy is affected by the number of estimators (i.e. trees in the random forest):","3e2241dc":"We can see that random forest gives a much higher accuracy than a single decision tree. Now let us look at the performance of logistic regression:  ","f4ec7efb":"It is clear that the features are not correlated too much with each other. Now we test different algorithms on the dataset","6fc90c21":"As we see, there is no difference between accuracy for scaled and original data. From now on, however, we will use scaled data.","3944354a":"First, we get the basic info about the dataset.","8f278823":"This is already quite a high accuracy. With the scaled data we get:","42d89187":"The accuracy achieved is really high even without parameter tuning. After parameter tuning we get:","15cf6972":"Now we test different algorithms on the dataset. Separate the target variable and the arguments","74d228a5":"The score is rather good as well. The most important parameter is smoothing, which we can tune:","b0ddd83e":"The cross-validation score of 1.0 is achieved, meaning that the data is well-separated enough for SVM to classify every hand-written digit correctly. Moving on to Bernoulli Naive Bayes:","c378ab87":"Although accuracy is higher than for both of the previous boosting algorithms, convergence is noticeably slower. The two algorithms we have not tried yet are SVM and Naive Bayes. First, we look at SVM:","bc9b6370":"It might be worth looking at how accuracy changes with the number of estimators: ","289cbc93":"Performing parameter tuning:","920a3c47":"Accuracy achieved is already pretty high. Let us try parameter tuning using GridSearchCV:","ea1d1a1a":"The convergence of the algorithm for this dataset is somewhat slower than the one of random forest. Next, we can take a look at how other boosted algorithms perform. The first one would be gradient boosting classifier:","8cfc0e1c":"The result is better than for other boosting algorithms, but still worse than for logistic regression and random forest. The convergence of accuracy against number of estimators looks like:","58be87c4":"The convergence is faster than for AdaBoost, and somewhat smoother as well. Note that for both algorithms, the other parameters were same as for the best estimator found by grid search. Finally, let us have a look at extreme gradient boosting:","8fbd3247":"Clearly, logistic regression performs really well. After doing some parameter tuning, we get:","c25dcbe8":"Performing parameter tuning for scaled data:","b6e84e9a":"This is not a satisfactory result - some parameter tuning is needed. ","7d409f79":"The score is very high. Trying Gaussian Naive Bayes:","28df758c":"The algorithm performs better than AdaBoost from the start. Tuning parameters gives:","1a277f9d":"Next, we try random forest (from now on, we use scaled data only):","e9ad6f54":"Logistic regression performs slightly worse than random forest, but better than a single decision tree.","edf77f88":"The next step would be to find out which features contain most information."}}