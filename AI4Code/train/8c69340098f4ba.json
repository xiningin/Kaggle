{"cell_type":{"85a6eaf1":"code","0af46372":"code","e6ba2e23":"code","edb626a4":"code","a1f65a30":"code","dc913dc4":"code","445fe3ed":"code","ed1b82cb":"code","32fe57d4":"code","13ee4b80":"code","737be5d1":"code","9322c1f3":"code","e994f32d":"code","da8f4a74":"code","74d10efd":"code","f8f85a91":"code","8e46a9d7":"code","fc1d8a89":"code","378ecc23":"code","aeb9e2cc":"code","f4d5d935":"code","0b8fd8f8":"code","8ab255cf":"markdown","c085001d":"markdown","8a58ed82":"markdown","201f7572":"markdown","2f3d162d":"markdown","0b5631ca":"markdown","897aa627":"markdown","f4f1714e":"markdown","f16455b6":"markdown","e9937424":"markdown","cda702c8":"markdown","e7a6e16d":"markdown","6fa0dcbf":"markdown","515b3c38":"markdown","4380ac52":"markdown"},"source":{"85a6eaf1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Matplotlib and Seaborn\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import GridSearchCV\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Any results you write to the current directory are saved as output.","0af46372":"DF_data = pd.read_csv('..\/input\/insurance.csv')\nprint(DF_data.shape)\nprint(DF_data.keys())\nprint(DF_data.dtypes)","e6ba2e23":"DF_data.head() # preview top 5 rows","edb626a4":"DF_data.describe() # display some very brief stats on numeric data","a1f65a30":"print('Missing Training Data:')\nDF_data.isnull().sum() # count number of missing frames for each column","dc913dc4":"plt.figure(figsize=(10,6))\nax = sns.distplot(DF_data['charges'])\nax.set_title('Distribution of Medical Charges');","445fe3ed":"plt.figure(figsize=(15,5))\nplt.subplot(1,3,1)\nsns.countplot(DF_data.sex);\nplt.subplot(1,3,2)\nsns.countplot(DF_data.smoker);\nplt.subplot(1,3,3)\nsns.countplot(DF_data.region);","ed1b82cb":"plt.figure(figsize=(15,5))\nplt.subplot(1,3,1)\nsns.distplot(DF_data.age);\nplt.subplot(1,3,2)\nsns.distplot(DF_data.bmi);\nplt.subplot(1,3,3)\nsns.distplot(DF_data.children);","32fe57d4":"# First See if charges differ within the categorical variables...\nplt.figure(figsize=(20,5))\nplt.subplot(1,3,1)\nsns.boxplot(x='sex',y='charges',data=DF_data)\nplt.subplot(1,3,2)\nsns.boxplot(x='smoker',y='charges',data=DF_data)\nplt.subplot(1,3,3)\nsns.boxplot(x='region',y='charges',data=DF_data);","13ee4b80":"# Next let's try the non-categorical data:\n# sns.jointplot(\"age\", \"charges\", data=DF_data, kind=\"reg\");\nplt.figure(figsize=(20,5))\nplt.subplot(1,3,1)\nsns.regplot(x='age',y='charges',data=DF_data)\nplt.subplot(1,3,2)\nsns.regplot(x='bmi',y='charges',data=DF_data)\nplt.subplot(1,3,3)\nsns.regplot(x='children',y='charges',data=DF_data);","737be5d1":"g = sns.pairplot(DF_data, hue='smoker', height=4)","9322c1f3":"# For sex... Lets Change 'Female' to 0 and 'Male' to 1\nDF_data.loc[DF_data['sex'] == 'male', 'sex'] = 0\nDF_data.loc[DF_data['sex'] == 'female', 'sex'] = 1\n\n# For smoker... Lets Change 'no' to 0 and 'yes' to 1\nDF_data.loc[DF_data['smoker'] == 'no', 'smoker'] = 0\nDF_data.loc[DF_data['smoker'] == 'yes', 'smoker'] = 1\n\n# For region... Lets Change to 1:4\nDF_data.loc[DF_data['region'] == 'southwest', 'region'] = 0\nDF_data.loc[DF_data['region'] == 'southeast', 'region'] = 1\nDF_data.loc[DF_data['region'] == 'northwest', 'region'] = 2\nDF_data.loc[DF_data['region'] == 'northeast', 'region'] = 3\n# DF_data.head()","e994f32d":"DF_data.head()","da8f4a74":"# Add weight classifications based on BMI\n# underweight <18 , normal = 18-25, overweight = 25-30, obese= >30\n# seems to mildly help RF model; no effect on XGB model\n\nDF_data.loc[DF_data['bmi'] < 18, 'weightclass'] = 0 #Underweight\nDF_data.loc[(DF_data['bmi'] >= 18) & (DF_data['bmi'] < 25), 'weightclass'] = 1 # Normal Weight\nDF_data.loc[(DF_data['bmi'] >= 25) & (DF_data['bmi'] < 30), 'weightclass'] = 2 #overweight\nDF_data.loc[DF_data['bmi'] >= 30, 'weightclass'] = 4 # Obese\n\nDF_data.head()","74d10efd":"plt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nsns.boxplot(x='weightclass',y='charges',data=DF_data);\nplt.subplot(1,2,2)\nsns.boxplot(x='weightclass',y='age',data=DF_data);","f8f85a91":"# create a feature called youngadult:\n# seems to mildly help RF model. no effect on XGB model\n\nDF_data.loc[DF_data['age'] < 30, 'youngadult'] = 1 \nDF_data.loc[DF_data['age'] >= 30, 'youngadult'] = 0\nDF_data.head()","8e46a9d7":"DF_data.corr()['charges'].sort_values()","fc1d8a89":"fig, (ax) = plt.subplots(1, 1, figsize=(10,6))\n\nhm = sns.heatmap(DF_data.corr(), \n                 ax=ax, # Axes in which to draw the plot\n                 cmap=\"coolwarm\", # color-scheme\n                 annot=True, \n                 fmt='.2f',       # formatting  to use when adding annotations.\n                 linewidths=.05)\n\nfig.suptitle('Health Costs Correlation Heatmap', \n              fontsize=14, \n              fontweight='bold');","378ecc23":"# Create X, y\nX = DF_data.copy().drop(['charges'], axis=1)\n\ny = DF_data.copy().charges\n\n# Split into validation and training data\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)","aeb9e2cc":"RF_model = RandomForestRegressor(random_state=1)\nRF_model.fit(train_X, train_y)\n\n# make predictions\nRF_predictions = RF_model.predict(val_X)\n# Print MAE for initial XGB model\nRF_mae = mean_absolute_error(RF_predictions, val_y)\nprint(\"Validation MAE for Random Forest Model : \" + str(RF_mae))","f4d5d935":"# XGBoost model:\nXGB_model = XGBRegressor(random_state=1)\nXGB_model.fit(train_X, train_y, verbose=False)\n\n# make predictions\nXGB_predictions = XGB_model.predict(val_X)\n# Print MAE for initial XGB model\nXGB_mae = mean_absolute_error(XGB_predictions, val_y)\nprint(\"Validation MAE for XGBoost Model : \" + str(XGB_mae))","0b8fd8f8":"%%time\n# Slightly Tuned XGB Model:\nXGB_model = XGBRegressor(random_state=1)\n\nparameters = {'learning_rate': [0.02, 0.025, 0.05, 0.075, 0.1], #so called `eta` value\n              'max_depth': [2, 3, 4, 5],\n             'n_estimators': [100, 150, 200, 250, 300, 500]}\n\nXGB_grid = GridSearchCV(XGB_model,\n                        parameters,\n                        cv = 2,\n                        n_jobs = 5,\n                        verbose=True)\n\nXGB_grid.fit(train_X, train_y)\n\nprint(XGB_grid.best_score_)\nprint(XGB_grid.best_params_)\n\n# make predictions\nXGB_grid_predictions = XGB_grid.predict(val_X)\n# Print MAE for initial XGB model\nXGB_grid_mae = mean_absolute_error(XGB_grid_predictions, val_y)\nprint(\"Validation MAE for grid search XGBoost Model : \" + str(XGB_grid_mae))\n","8ab255cf":"# Let's Quickly Explore the Charges (dependant variable)\nAs we can see from this distribution plot, medical charges seem to be pretty heavily skewwed...","c085001d":"# Lets Create Some Simple Machine Learning Models:\n\nFirst, Let's create X and y, then split into training and test datasets...","8a58ed82":"# Preview the Data and Check for Missing Values:","201f7572":"Next, Let's plot some distributions of the numeric dependant variables:\n\nPretty even distribution of ages (aside from the youngest group being higher.) BMI is pretty normally distibuted, but the population is Obese (BMI >30), on average...","2f3d162d":"# Next, Let's Quickly Explore the Independant Variables:\n\nFirst let's just plot counts for the categorical variables (we will have to encode those later for modeling)...","0b5631ca":"# Imports:","897aa627":"**Modeling Recap:**\n\nOk. After creating 2 very simple models, we can see that we are able to predict medical charges with a mean average error of  ~2700-2800 for RF model and ~ 2400 for the XGB model. Considering that the average Medical charge was  13,000, this isn't too amazing.  Some additional feature engineering (e.g. brekaing up BMI into weightclasses, adding a youngadult class) helped the weaker RF model, but had no effect on the XGB model. \n\nTuning of the XGB model using gridsearchCV only improved the mean estimated error by ~ 30 dollars... (2352 vs 2385). However, since the data set was so small, this only took ~ 7 seconds, so it is still worth it.","f4f1714e":"# Load data to Pandas DataFrame","f16455b6":"# Exploring and Predicting Medical Costs\nOriginally created by DHaight on 10\/7\/2018\n\n![](https:\/\/78.media.tumblr.com\/876d34152b88f5becd6e8fade5a01e86\/tumblr_nci6j5CZbN1rmght3o1_500.gif)\n\nIt turns out, smoking is pretty dang expensive...\n\n**Outline**:\n1. Imports\n2. Load Data to Pandas DataFrame\n3. Preview the Data and Check for Missing Values\n4. Explore the Dependant Variable (Charges)\n5. Explore the Independant Variables\n6. How independant variables influence medical costs\n7. Variable Encoding (categorical --> numerical) \/ Feature Engineering\n8. Simple Random Forest Model\n9. Simple XGBoost Model\n10. Light XGBoost Model Tuning with GridSearchCV\n11. Recap\n\n","e9937424":"From this quick exploration, it looks like Smoking has a pretty huge impact on medical charges. Age and BMI can also have an effect... Next, let's see how these variables relate to eachother, as well as to charges.","cda702c8":"# Random Forest Model:","e7a6e16d":"# Let's Explore How the Independant Variables Influence Medical Cost","6fa0dcbf":"Now that we have created a new variable 'weightclass' based on bmi cut-offs, lets see if there are any great differences in medical costs between groups.  It does seem that there are deifnitely more high-cost outliers in the obese group, but surprisingly the median healthcare costs arent too much greater than normal, and over-weight groups. Also surprisingly, underweight folks tend to have lower median healthcare costs... \n\nIn the seconds plot we see if the the underweight weightclass group may also just be younger on average... confirmed!","515b3c38":"**Now that all of the variables are numerical, let's examine the correlations of all variables...**","4380ac52":"# Variable Encoding \/ Feature Engineering:\nWhen we start machine leanring, it may be helpful to further cagtegorize the data"}}