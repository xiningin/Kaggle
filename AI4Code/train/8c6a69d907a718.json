{"cell_type":{"a16adeae":"code","c10be860":"code","b1272d7c":"code","0a840187":"code","33317d89":"code","a8e094cc":"code","7a55ecd6":"code","bebb869f":"code","5321785a":"code","919fbee7":"code","d15c6963":"code","52099a63":"code","ec6c1450":"code","5265080d":"code","fab347f3":"code","41438532":"code","e19d7ad7":"code","ac162d75":"code","f64041db":"code","9259ed22":"code","00e37ee5":"code","6810eac6":"code","a086315d":"code","827b80ba":"code","052dca87":"code","5f046e6a":"markdown","920c9b06":"markdown","fa54d4a8":"markdown","0a40478d":"markdown","3411d358":"markdown","f2cf5eb5":"markdown","660eb4d7":"markdown","6ddd4b2b":"markdown","6cd62419":"markdown","8ddce821":"markdown","bc6acff8":"markdown","6decccf2":"markdown","22809cd1":"markdown","44ad0a2e":"markdown","d3684734":"markdown","1a7ae8ac":"markdown","2213a6c2":"markdown","760ef912":"markdown","5cd9162d":"markdown","9d9f572b":"markdown"},"source":{"a16adeae":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c10be860":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport cufflinks as cf\ncf.go_offline()\n","b1272d7c":"from sklearn.datasets import load_breast_cancer\ncancer=load_breast_cancer()\ntype(cancer)","0a840187":"cancer.keys()","33317d89":"print(cancer[\"DESCR\"])\n#Here we get overall description of the data\n","a8e094cc":"print(cancer[\"feature_names\"]) #The names of the attributes are listed here","7a55ecd6":"print(cancer[\"data\"])#The data of the features are listed here\n#The next step is to combine data with the feature names and make a pandas dataframe","bebb869f":"features=pd.DataFrame(cancer[\"data\"],columns=cancer[\"feature_names\"])\nfeatures.head()","5321785a":"features.info() #Here we get an overall picture of our data","919fbee7":"cancer[\"target\"] #This is our target data that is listed 1 or 0 which represent malignant and benign tumors","d15c6963":"features.describe()","52099a63":"plt.figure(figsize=(15,10))\nsns.distplot(cancer[\"target\"])","ec6c1450":"X=features\ny=cancer[\"target\"]\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=101)\n","5265080d":"from sklearn.svm import SVC\nsvm_model=SVC()\nsvm_model.fit(X_train,y_train)","fab347f3":"predictions=svm_model.predict(X_test)\npredictions","41438532":"from sklearn.metrics import classification_report,confusion_matrix\nprint(classification_report(y_test,predictions))\n#here we get the classification report to learn how accurate our model is","e19d7ad7":"print(confusion_matrix(y_test,predictions))","ac162d75":"from sklearn.model_selection import GridSearchCV","f64041db":"param_grid={\"C\":[1,10,100,100],\"gamma\":[1,0.1,0.01,0.001,0.0001]} \n#here we select values for grid search to try\ngrid=GridSearchCV(SVC(),param_grid,verbose=3)","9259ed22":"grid.fit(X_train,y_train) # we apply it to our training data to see the best C and gamma values\n#grid.fit() will find the best combination of C and gamma values for our model","00e37ee5":"grid.best_params_","6810eac6":"grid.best_estimator_","a086315d":"grid_predictions=grid.predict(X_test) \n#Now we predict with this readjustment","827b80ba":"print(classification_report(y_test,grid_predictions))","052dca87":"print(confusion_matrix(y_test,grid_predictions))","5f046e6a":"*According to confusion matrix;\n\n        -True Negatives: 56\n        -False Positive:10\n        -False Negative:3\n        -True Positive:102\n        \n*These results are also very good","920c9b06":"* This return {'C': 1, 'gamma': 0.0001} as the best combination for our data for training the algorithm","fa54d4a8":"*There are 569 instances and 30 numeric attributes\n\n*The prediction we want to make is whether or not a agiven tumor is malignant or benign","0a40478d":"*Here we call our SVM algorithm and apply it for the training dataset","3411d358":"*They are supervised machine learning models with associated algorithms that analyze data and recognize patterns used for classification and regression analysis\n\n*This notebook will focus for classification\n\n*SVM training algorithm builds a model that assigns new examples or at least test data points into one category or the other making it a non-probabilistic binary linear classifier\n\n*The separate categories are divided by a clear gap as wide as possible and new examples are predicted to belong to a category based on which side of the gap they fall on\n\n*The algorith draw a separating hyperplane between the two classes. But this separating line should be the best option. The algorithm chooses a hyperplane that maximizes the margin between the classes which touch the end points of the both classes, which are also known as the support vectors.\n\n*This separation can also be extended to nonlinear data","f2cf5eb5":"*Now we will evaluate again how the algorithm performs with the new parameters of the grid search","660eb4d7":"*Now we will grab the data from the datasource and make our pandas data frame for our further analysis","6ddd4b2b":"## 4.Evaluation of the Performance of Our Model","6cd62419":"GridSearchCV(estimator, param_grid, *, scoring=None, n_jobs=None, iid='deprecated', refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs', error_score=nan, return_train_score=False)\n |  \n |  Exhaustive search over specified parameter values for an estimator.","8ddce821":"*After training our model,we will predict the test dataset","bc6acff8":"*Our model has pretty good prediction with precission,accuracy,recall and F1 scores over %90 for both 0 and 1's","6decccf2":"*We can adjust C and gamma parameters with a grid research instead of trying one by one","22809cd1":"## 3. Splitting and Training the Algorithm","44ad0a2e":"SVC(*, C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None)","d3684734":"SVC(*, C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None)\n\n*we can play with some of these parameters of the algorithm to predict better if the algorithm performs bad\n\n*C as a parameter of SVC model controls the cost of misclassification; a large C value gives us low bias and high variance.Bias is the simplifying assumptions made by the model to make the target function easier to approximate. Variance is the amount that the estimate of the target function will change given different training data. Trade-off is tension between the error introduced by the bias and the variance\n\n*gamma as a parameter; large gamma value leads to high bias and low variance in the model or vice versa.\n","1a7ae8ac":"## 2.Importing Libraries and Data","2213a6c2":"*According to confusion matrix;\n\n    -True Negatives: 59\n    -False Positive:7\n    -False Negative:4\n    -True Positive:101\n \n *The new predictions make fewer mistakes that the previous one as we can see from the results of the confusion matrix","760ef912":"*The precision and accuracy a little bit better that the previous one","5cd9162d":"*We split our data before applying the algorithm ","9d9f572b":"# 1. Overall Explanation of Support Vector Machines:"}}