{"cell_type":{"ec03168e":"code","73498c2c":"code","5719ee7c":"code","2900ef38":"code","4aecdf95":"code","2354b331":"code","a7d38f21":"code","21814a3e":"code","1e383cec":"code","5509f304":"code","9d2e5d98":"code","54686fc6":"markdown","ffb3a60c":"markdown","85d874a9":"markdown","febbbe02":"markdown","695e0d56":"markdown","ae23c8fe":"markdown","571e2e38":"markdown","ea273ae8":"markdown","2c2af5c0":"markdown","9c890c12":"markdown"},"source":{"ec03168e":"import xgboost\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom xgboost import XGBClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingClassifier","73498c2c":"df = pd.read_csv(\n    '\/kaggle\/input\/forest-cover-type-dataset\/covtype.csv')\ndf.head()\ndf.describe()\nX = (df.drop('Cover_Type', axis=1)).values\ny = (df['Cover_Type']).values\nfor i in range(len(np.unique(y)-1)):\n    y[:][y[:] == i+1] = i\nx_train, x_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=1)","5719ee7c":"print(df['Cover_Type'].describe())","2900ef38":"sns.distplot(df['Cover_Type'], color='r', hist_kws={'alpha': 0.4});","4aecdf95":"df.hist(figsize=(16, 20), bins=50, xlabelsize=8, ylabelsize=8)","2354b331":"corr = df.corr()['Cover_Type']\nfeatures_list = corr[abs(corr) > 0.5].sort_values(ascending=False)\nprint(\"There is {} strongly correlated values with SalePrice:\\n{}\".format(len(features_list), features_list))\n","a7d38f21":"# for i in range(0, len(df.columns), 5):\n#     sns.pairplot(data=df,\n#                 x_vars=df.columns[i:i+5],\n#                 y_vars=['Cover_Type'])","21814a3e":"def _dataframe(X, y):\n    X = pd.DataFrame(X)\n    y = pd.DataFrame(y)\n    X = X.astype(\"int64\")\n    y = y.astype(\"int64\")\n\n    feature = []\n    for i in range(len(X.columns)):\n        feature.append(str(i))\n\n    col_rename = {i: j for i, j in zip(X.columns, feature)}\n    X = X.rename(columns=col_rename, inplace=False)\n\n    return X, y, feature","1e383cec":"def _make_input_fn(X, y, n_epochs=None, shuffle=True):\n    def input_fn():\n        NUM_EXAMPLES = len(y)\n        dataset = tf.data.Dataset.from_tensor_slices((dict(X), y))\n        if shuffle:\n            dataset = dataset.shuffle(NUM_EXAMPLES)\n        # For training, cycle thru dataset as many times as need (n_epochs=None).\n        dataset = dataset.repeat(n_epochs)\n        # In memory training doesn't use batching.\n        dataset = dataset.batch(NUM_EXAMPLES)\n        return dataset\n    return input_fn\n\ndef _accuracy(evaluate):\n    item = list(evaluate.items())\n    array = np.array(item)\n    return (array[0, 1]).astype(np.float64)\n\ndef BostedTree(X, y, step):\n    X, y, feature = _dataframe(X, y)\n\n# Training and evaluation input functions.\n    train_input_fn = _make_input_fn(X, y)\n    eval_input_fn = _make_input_fn(X, y,\n                                   shuffle=False,\n                                   n_epochs=1)\n\n# feature selection\n    num_columns = feature\n    feature_columns = []\n    n_classes = len(np.unique(y))\n\n    for feature_name in num_columns:\n        feature_columns.append(tf.feature_column.numeric_column(feature_name,\n                                                                dtype=tf.float32))\n    est = tf.estimator.BoostedTreesClassifier(feature_columns,\n                                              n_batches_per_layer=1,\n                                              n_classes=n_classes,\n                                              n_trees=100,\n                                              max_depth=5,\n                                              learning_rate=0.1)\n    fit = est.train(train_input_fn, max_steps=step)\n    score = _accuracy(est.evaluate\n                      (eval_input_fn, steps=1))\n\n    return fit, score","5509f304":"skl = GradientBoostingClassifier(max_depth=2,\n                                 subsample=0.75,\n                                 max_features=None,\n                                 learning_rate=0.25,\n                                 random_state=2,\n                                 criterion=\"mse\",\n                                 n_estimators=5)\n\npipe_skl = Pipeline([(\"scaler\", StandardScaler()), (\"clf\", skl)])\npipe_skl.fit(x_train, y_train)\nerr_skl = pipe_skl.score(x_test, y_test)\n\nfit, score = BostedTree(x_train, y_train, 5)\nerr_tfbt = score\n\nxgb = xgboost.XGBClassifier(learning_rate=0,\n                            max_depth=2,\n                            n_estimators=5,\n                            subsample=1,\n                            min_child_weight=1)\npipe_xgb = Pipeline([(\"scaler\", StandardScaler()), (\"clf\", xgb)])\npipe_xgb.fit(x_train, y_train)\nerr_xgb = pipe_xgb.score(x_test, y_test)","9d2e5d98":"print('xgboost accuracy:', err_xgb, '\\n',\n      'TFBT accuracy:', err_tfbt, '\\n', 'MART accuracy:', err_skl)","54686fc6":"# 2. Data preparation\n## 2.1 Load data","ffb3a60c":"# 1. Introduction\nThis is comparison between three well-known gradients boosting classifier from two major libraries (Namely Sklearn, TFBT, and xgboost)\n\n**For computational reasons, I set the number of steps (epochs) and the number of trees to 2, and 50. If you want to achieve better accuracy, set them to 10, and 100, and respectively**","85d874a9":"## 2.2 Describe the target","febbbe02":"## 2.3 Data distribution\n### Check the distribution of the cover type","695e0d56":"## 2.5 Correlation\n### Examine the features that correlate with Cover type. ","ae23c8fe":"# 3. Model preparation","571e2e38":"# 3. Model Training","ea273ae8":"# Evaluation","2c2af5c0":"# A comparison between three well-known gradients boosting classifier\n### **Seyedsaman Emami**\n#### 23\/April\/2021\n\n* **1. Introduction**\n* **2. Data preparation**\n    * 2.1 Load data\n    * 2.2 Describe the target\n    * 2.3 data distribution\n    * 2.4 features overview\n    * 2.5 Correlation\n* **3. Model Training**\n* **4. Evaluation**\n    * 4.1 training time\n    * 4.2 Accuracy\n* **5. Conclusion**","9c890c12":"## 2.4 features Overview\n### Overview of all the features"}}