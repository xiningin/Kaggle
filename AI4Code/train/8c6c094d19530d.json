{"cell_type":{"883cd1d4":"code","477b9f32":"code","90fb0388":"code","2a8ac4f1":"code","28bb1b0a":"code","90053703":"code","a7de53cc":"code","8eb780e3":"code","cb92b20c":"code","f8b9acfc":"code","4b1b748c":"code","182b6958":"code","1190d2a3":"code","3b3cff07":"code","6ee6eb26":"code","86a7fb87":"code","e94c1a9c":"code","0f0796aa":"code","ba66ab81":"code","ad3683df":"code","817f4122":"code","09d69433":"code","bd142346":"code","1c08b6dc":"code","d741de42":"code","9fa19d29":"code","de57fed2":"code","55d9c187":"code","26803054":"code","8cd2b2a5":"code","f68ab573":"code","5e9a39f5":"code","e8683409":"code","6b3b1fb8":"code","f62b5f29":"code","fce83e15":"code","d64ebaf1":"code","b460064a":"code","9b394f73":"code","e686239b":"code","f86c5108":"markdown","d45c99bc":"markdown","429de680":"markdown","ee5dfe50":"markdown","919a315f":"markdown","d8103596":"markdown","8dc4ea07":"markdown","e460c574":"markdown","f8dc6899":"markdown","56106b60":"markdown","eef7858a":"markdown","807c3164":"markdown","a15e7e96":"markdown","d9240eb8":"markdown","984d44a6":"markdown","c4ba0741":"markdown","dff7b589":"markdown","b948db82":"markdown","22b0fca9":"markdown","438fc457":"markdown","eecb9b90":"markdown","a264f968":"markdown","eefb177d":"markdown","d13c75a6":"markdown","783ecbf9":"markdown","04196045":"markdown","a60b29a0":"markdown","ca43f419":"markdown","f4505d60":"markdown"},"source":{"883cd1d4":"# importing the basic libraries\nimport pandas as pd\nimport numpy as np\nimport datetime\n\n# importing visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#for balancing dataset\nfrom imblearn.over_sampling import SMOTE\n\n# importing sklearn libraries\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\nfrom sklearn.pipeline import Pipeline\nimport xgboost as xgb\nfrom sklearn.metrics import f1_score,confusion_matrix, classification_report,accuracy_score,recall_score\n\n# display setting\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)","477b9f32":"# reading the train data\n# from problem statement, wkt 'INCIDENT_ID' is the index col and have a 'DATE' column\ntrain=pd.read_csv('..\/input\/hacked\/train.csv',index_col=['INCIDENT_ID'],parse_dates=['DATE'])\ntest=pd.read_csv('..\/input\/hacked\/test.csv',index_col=['INCIDENT_ID'],parse_dates=['DATE'])\n\ndisplay(train.head(2))","90fb0388":"#shape of train and test data\nprint(\"Shape of Train Data: {} \\nShape of Test Data: {}\".format(train.shape,test.shape))","2a8ac4f1":"# Creating new column\ntrain['DAYOFWEEK']=train['DATE'].dt.dayofweek\ntrain['WEEK']=train['DATE'].dt.week\n\ntest['DAYOFWEEK']=test['DATE'].dt.dayofweek\ntest['WEEK']=test['DATE'].dt.week\n\n#creating new dataset by merging train and test\nmerge=train.append(test)","28bb1b0a":"#checking datatypes of the column\ntrain.info()","90053703":"#Target Variable counts\ntrain.MULTIPLE_OFFENSE.value_counts(normalize=True)","a7de53cc":"#5 number summary statistics\nmerge.describe().T","8eb780e3":"# Exploring the Unique values\nfor x in ['X_1','X_2','X_3','X_4','X_5','X_6','X_7','X_8','X_9','X_10','X_11','X_12','X_13','X_14','X_15','DAYOFWEEK','WEEK']:\n    print(\"Number of Unique values in \",x,\" is \",merge[x].nunique())\n    print(\"The list of unique values is \\n\",np.sort(merge[x].unique()),\"\\n\")","cb92b20c":"# Plotting frequency of Unique Values in Each Column\ncolumns_label=['X_1', 'X_2', 'X_3', 'X_4', 'X_5', 'X_6', 'X_7', 'X_8', 'X_9','X_10', 'X_11', 'X_12', 'X_13', 'X_14', 'X_15','DAYOFWEEK','WEEK']\nfor col in columns_label:\n    plt.figure(figsize=(10,7))\n    sns.countplot(merge[col])","f8b9acfc":"# checking the NAN values in 'X_12' belongs to which target class\ntrain[np.isnan(train['X_12'])]['MULTIPLE_OFFENSE'].value_counts()","4b1b748c":"# Number of Duplicate values after removing Date Related columns.\ntrain_dup_size=train.drop(['DATE','DAYOFWEEK','WEEK'],axis=1).duplicated().sum()\ntest_dup_size=test.drop(['DATE','DAYOFWEEK','WEEK'],axis=1).duplicated().sum()\n\ntrain_dup_size,test_dup_size","182b6958":"# removing duplicates\nmod_train=train.drop(['DATE','DAYOFWEEK','WEEK'],axis=1).drop_duplicates()\nmod_test=test.drop(['DATE','DAYOFWEEK','WEEK'],axis=1)\nmod_merge=train.append(test)\nprint(\"The shape of the Train and test dataframe after is\",mod_train.shape,mod_test.shape)\n\n#checking value distribution of target column\nprint(mod_train.MULTIPLE_OFFENSE.value_counts())\nmod_train.MULTIPLE_OFFENSE.value_counts(normalize=True).round(4)","1190d2a3":"df=mod_train[np.isnan(mod_train['X_12'])]\ndf['MULTIPLE_OFFENSE'].value_counts()","3b3cff07":"# Assigning X and y\ny=mod_train['MULTIPLE_OFFENSE']\nX=mod_train.drop(['MULTIPLE_OFFENSE','X_12'],axis=1)\n\n# Splitting Dataframe into Train and Test Dataset\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=.20,random_state=5)\n\n# Oversampling\nsampler = SMOTE(sampling_strategy='minority')\nX_train_smote, y_train_smote = sampler.fit_sample(X_train,y_train)\n\n# parameter grid\nxg_cl_param_grid={'n_estimators':[400,500,600,700],'max_depth':[2,3,4,5]}\n\n# Instantiate XGBoost Classifier\nxg_cl=xgb.XGBClassifier(objective='reg:logistic',seed=123)\n\n# Grid Search CV\ngrid_mse=GridSearchCV(xg_cl,param_grid=xg_cl_param_grid,cv=3,scoring='accuracy',verbose=1,n_jobs=-1)\n\n# Training the Model\ngrid_mse.fit(X_train_smote, y_train_smote)\n\n# Predicting\ny_predict=grid_mse.predict(X_test)\ny_predict_1a=grid_mse.predict(mod_test.drop(['X_12'],axis=1))\n\n# Different Metrics\nf1s=f1_score(y_test,y_predict)\ncm=confusion_matrix(y_test,y_predict)\ncr=classification_report(y_test,y_predict)\nres=recall_score(y_test,y_predict)\n\nprint(\"Best Parameter\",grid_mse.best_params_)\nprint(grid_mse.best_score_)\nprint('\\n')\nprint(\"Recall Score\",res)\nprint(\"F1 Score\",f1s)\nprint(\"Confusion Matrix\\n\",cm)\nprint(\"Classification Score\\n\",cr)","6ee6eb26":"# submission File\ntest['MULTIPLE_OFFENSE']=y_predict_1a\nsub=test['MULTIPLE_OFFENSE']\nsub.to_csv('sub_1a.csv')\ntest=test.drop(['MULTIPLE_OFFENSE'],axis=1)","86a7fb87":"# Parameter Dictionary for GridSearch CV\nxg_cl_param_grid={'n_estimators':[200,300,400,500,600],'max_depth':[2,3,4,5]}\n\n# Instantiate XGBoost Classifier\nxg_cl=xgb.XGBClassifier(objective='reg:logistic',seed=123)\n\n# Grid Search CV\ngrid_mse=GridSearchCV(xg_cl,param_grid=xg_cl_param_grid,cv=3,scoring='accuracy',verbose=1,n_jobs=-1)\n\n# Training the Model\ngrid_mse.fit(X_train, y_train)\n\n# Predicting\ny_predict=grid_mse.predict(X_test)\ny_predict_1b=grid_mse.predict(mod_test.drop(['X_12'],axis=1))\n\n# Different Metrics\nf1s=f1_score(y_test,y_predict)\ncm=confusion_matrix(y_test,y_predict)\ncr=classification_report(y_test,y_predict)\nres=recall_score(y_test,y_predict)\n\nprint(\"Recall score\",res)\nprint(\"Best Parameter\",grid_mse.best_params_)\nprint(grid_mse.best_score_)\nprint('\\n')\nprint(\"Recall score\",res)\nprint(\"F1 Score\",f1s)\nprint(\"Confusion Matrix\\n\",cm)\nprint(\"Classification Score\\n\",cr)","e94c1a9c":"# Model created with best hyperparameter\nxg_cl1=xgb.XGBClassifier(objective='reg:logistic',seed=123,n_estimators=500,max_depth=2)\nxg_cl1.fit(X_train, y_train)\ny_predict=xg_cl1.predict(X_test)\nf1s=f1_score(y_test,y_predict)\ncm=confusion_matrix(y_test,y_predict)\ncr=classification_report(y_test,y_predict)\nres=recall_score(y_test,y_predict)\n\nprint(\"Recall score\",res)\nprint(\"F1 Score\",f1s)\nprint(\"Confusion Matrix\\n\",cm)\nprint(\"Classification Score\\n\",cr)","0f0796aa":"# Ploting Important Feature\nxgb.plot_importance(xg_cl1)","ba66ab81":"# submission File\ntest['MULTIPLE_OFFENSE']=y_predict_1b\nsub=test['MULTIPLE_OFFENSE']\nsub.to_csv('sub_1b.csv')\ntest=test.drop(['MULTIPLE_OFFENSE'],axis=1)","ad3683df":"# Assigning X and y\ny=train['MULTIPLE_OFFENSE']\nX=train.drop(['MULTIPLE_OFFENSE','X_12','DATE'],axis=1)\n\n# Train-test split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=.20,random_state=5)\n\n# Oversampling\nsampler = SMOTE(sampling_strategy='minority')\nX_train_smote, y_train_smote = sampler.fit_sample(X_train,y_train)\n\n# Parameter dictionary\nxg_cl_param_grid={'n_estimators':[50,100,200,300],'max_depth':[2,3,4,5]}\n\n# Instantiate XGBoost\nxg_cl=xgb.XGBClassifier(objective='reg:logistic',seed=123)\n\n# Grid Search CV\ngrid_mse=GridSearchCV(xg_cl,param_grid=xg_cl_param_grid,cv=3,scoring='recall',verbose=1)\n\n# Fit the model\ngrid_mse.fit(X_train_smote, y_train_smote)\n\n# Predict\ny_predict=grid_mse.predict(X_test)\ny_predict_2a=grid_mse.predict(test.drop(['DATE','X_12'],axis=1))\n\n# Metrics\nf1s=f1_score(y_test,y_predict)\ncm=confusion_matrix(y_test,y_predict)\ncr=classification_report(y_test,y_predict)\nres=recall_score(y_test,y_predict)\n\nprint(\"Recall score\",res)\nprint(grid_mse.best_params_)\nprint(grid_mse.best_score_)\nprint('\\n')\nprint(f1s)\nprint(cm)\nprint(cr)","817f4122":"# Submission file\ntest['MULTIPLE_OFFENSE']=y_predict_2a\nsub=test['MULTIPLE_OFFENSE']\nsub.to_csv('sub_2a.csv')\ntest=test.drop(['MULTIPLE_OFFENSE'],axis=1)","09d69433":"# Assigning X and y\ny=train['MULTIPLE_OFFENSE']\nX=train.drop(['MULTIPLE_OFFENSE','X_12','DATE'],axis=1)\n\n# Train-test split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=.20,random_state=5)\n\n# Parameter dictionary\nxg_cl_param_grid={'n_estimators':[50,100,200,300],'max_depth':[2,3,4,5]}\n\n# Instantiate XGBoost\nxg_cl=xgb.XGBClassifier(objective='reg:logistic',seed=123)\n\n# Grid Search CV\ngrid_mse=GridSearchCV(xg_cl,param_grid=xg_cl_param_grid,cv=3,scoring='recall',verbose=1)\n\n# Fit the model\ngrid_mse.fit(X_train, y_train)\n\n# Predict\ny_predict=grid_mse.predict(X_test)\ny_predict_2b=grid_mse.predict(test.drop(['DATE','X_12'],axis=1))\n\n# Metrics\nf1s=f1_score(y_test,y_predict)\ncm=confusion_matrix(y_test,y_predict)\ncr=classification_report(y_test,y_predict)\nres=recall_score(y_test,y_predict)\n\nprint(\"Recall score\",res)\nprint(grid_mse.best_params_)\nprint(grid_mse.best_score_)\nprint('\\n')\nprint(f1s)\nprint(cm)\nprint(cr)","bd142346":"# Model created with best hyperparameter\nxg_cl1=xgb.XGBClassifier(objective='reg:logistic',seed=123,n_estimators=200,max_depth=2)\n\nxg_cl1.fit(X_train, y_train)\ny_predict=xg_cl1.predict(X_test)\nf1s=f1_score(y_test,y_predict)\ncm=confusion_matrix(y_test,y_predict)\ncr=classification_report(y_test,y_predict)\nres=recall_score(y_test,y_predict)\n\nprint(\"Recall score\",res)\nprint(\"F1 Score\",f1s)\nprint(\"Confusion Matrix\\n\",cm)\nprint(\"Classification Score\\n\",cr)","1c08b6dc":"# Ploting Important Feature\nxgb.plot_importance(xg_cl1)","d741de42":"# submission File\ntest['MULTIPLE_OFFENSE']=y_predict_2b\nsub=test['MULTIPLE_OFFENSE']\nsub.to_csv('sub_2b.csv')\ntest=test.drop(['MULTIPLE_OFFENSE'],axis=1)","9fa19d29":"# Assigning X and y\ny=train['MULTIPLE_OFFENSE']\nX=train.drop(['MULTIPLE_OFFENSE','DATE'],axis=1)\nX=X.fillna(0)\n \n# Train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=.20,random_state=5)\n\n# Oversampling\nsampler = SMOTE(sampling_strategy='minority')\nX_train_smote, y_train_smote = sampler.fit_sample(X_train,y_train)\n\n# Parameter dictionary\nxg_cl_param_grid={'n_estimators':[50,100,200,300],'max_depth':[2,3,4,5]}\n\n# Instantiate XGBClassifier\nxg_cl=xgb.XGBClassifier(objective='reg:logistic',seed=123)\n\n# Gridsearch cv\ngrid_mse=GridSearchCV(xg_cl,param_grid=xg_cl_param_grid,cv=3,scoring='recall',verbose=1)\n\n# Train model\ngrid_mse.fit(X_train_smote, y_train_smote)\n\n# Predict\ny_predict=grid_mse.predict(X_test)\ny_predict_3a=grid_mse.predict(test.drop(['DATE'],axis=1))\n\n# Metrics\nf1s=f1_score(y_test,y_predict)\ncm=confusion_matrix(y_test,y_predict)\ncr=classification_report(y_test,y_predict)\nres=recall_score(y_test,y_predict)\n\nprint(\"Recall score\",res)\nprint(grid_mse.best_params_)\nprint(grid_mse.best_score_)\nprint('\\n')\nprint(f1s)\nprint(cm)\nprint(cr)","de57fed2":"# Submission file\ntest['MULTIPLE_OFFENSE']=y_predict_3a\nsub=test['MULTIPLE_OFFENSE']\nsub.to_csv('sub_3a.csv')\ntest=test.drop(['MULTIPLE_OFFENSE'],axis=1)","55d9c187":"# Assigning X and y \ny=train['MULTIPLE_OFFENSE']\nX=train.drop(['MULTIPLE_OFFENSE','DATE'],axis=1)\n\n# Train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=.20,random_state=5)\n\n# Parameter dictionary\nxg_cl_param_grid={'n_estimators':[50,100,200,300],'max_depth':[2,3,4,5]}\n\n# Instantiate XGBClassifier\nxg_cl=xgb.XGBClassifier(objective='reg:logistic',seed=123)\n\n# Grid Search CV\ngrid_mse=GridSearchCV(xg_cl,param_grid=xg_cl_param_grid,cv=3,scoring='recall',verbose=1)\n\n# Train the model\ngrid_mse.fit(X_train,y_train)\n\n# Predict the model\ny_predict=grid_mse.predict(X_test)\ny_predict_3b=grid_mse.predict(test.drop(['DATE'],axis=1))\n\n# Metric\nf1s=f1_score(y_test,y_predict)\ncm=confusion_matrix(y_test,y_predict)\ncr=classification_report(y_test,y_predict)\nres=recall_score(y_test,y_predict)\n\nprint(\"Recall score\",res)\nprint(grid_mse.best_params_)\nprint(grid_mse.best_score_)\nprint('\\n')\nprint(f1s)\nprint(cm)\nprint(cr)","26803054":"# Model created with best hyperparameter\nxg_cl1=xgb.XGBClassifier(objective='reg:logistic',seed=123,n_estimators=200,max_depth=2)\n\nxg_cl1.fit(X_train, y_train)\ny_predict=xg_cl1.predict(X_test)\nf1s=f1_score(y_test,y_predict)\ncm=confusion_matrix(y_test,y_predict)\ncr=classification_report(y_test,y_predict)\nres=recall_score(y_test,y_predict)\n\nprint(\"Recall score\",res)\nprint(\"F1 Score\",f1s)\nprint(\"Confusion Matrix\\n\",cm)\nprint(\"Classification Score\\n\",cr)","8cd2b2a5":"# Ploting Important Feature\nxgb.plot_importance(xg_cl1)","f68ab573":"# Submission file\ntest['MULTIPLE_OFFENSE']=y_predict_3b\nsub=test['MULTIPLE_OFFENSE']\nsub.to_csv('sub_3b.csv')\ntest=test.drop(['MULTIPLE_OFFENSE'],axis=1)","5e9a39f5":"# Creating New columns\nmerge['DAYOFWEEK']=merge['DATE'].dt.dayofweek\nmerge['WEEK']=merge['DATE'].dt.week\n\n# Creating New Train and Test dataset. To predict NAN values by utilising other columns\nnan_test=merge[merge['X_12'].isnull()==True]\nnan_train=merge[merge['X_12'].isnull()==False]\n\n# Train_test_split foor NAN values\nb=nan_train['X_12']\nA=nan_train.drop(['DATE','X_12','MULTIPLE_OFFENSE'],axis=1)\nA_train,A_test,b_train,b_test=train_test_split(A,b,test_size=0.3)\n\n# XGBClassifier\nxb=xgb.XGBClassifier(n_estimator=1000,objective='reg:logistic',seed=123)\n\n# Train the Model\nxb.fit(A_train,b_train)\n\n# Predict \nb_predict=xb.predict(A_test)\nb_test_predict=xb.predict(nan_test.drop(['DATE','X_12','MULTIPLE_OFFENSE'],axis=1))\nprint(classification_report(b_test,b_predict))","e8683409":"# Predicted NAN Values\nb_test_predict","6b3b1fb8":"# Creating New columns\ntrain['DAYOFWEEK']=train['DATE'].dt.dayofweek\ntrain['WEEK']=train['DATE'].dt.week\ntest['DAYOFWEEK']=test['DATE'].dt.dayofweek\ntest['WEEK']=test['DATE'].dt.week\n\n# Filling NAN values\ntrain['X_12']=train['X_12'].fillna(0)\ntest['X_12']=test['X_12'].fillna(0)\n\n# Assigning X and y\ny=train['MULTIPLE_OFFENSE']\nX=train.drop(['MULTIPLE_OFFENSE','DATE'],axis=1)\n\n# Train test split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=.20,random_state=5)\n\n# Oversampling\nsampler = SMOTE(sampling_strategy='minority')\nX_train_smote, y_train_smote = sampler.fit_sample(X_train,y_train)\n\n# Parameter Dictionary\nxg_cl_param_grid={'n_estimators':[50,100,200,300,400],'max_depth':[2,3,4,5]}\n\n# XGBCLassifier\nxg_cl=xgb.XGBClassifier(objective='reg:logistic',seed=123)\n\n# GridSearchCV\ngrid_mse=GridSearchCV(xg_cl,param_grid=xg_cl_param_grid,cv=3,scoring='recall',verbose=1,n_jobs=-1)\ngrid_mse.fit(X_train_smote, y_train_smote)\ny_predict=grid_mse.predict(X_test)\ny_predict_4a=grid_mse.predict(test.drop(['DATE'],axis=1))\n# metrics\nf1s=f1_score(y_test,y_predict)\ncm=confusion_matrix(y_test,y_predict)\ncr=classification_report(y_test,y_predict)\nres=recall_score(y_test,y_predict)\n\nprint(\"Recall score\",res)\nprint(grid_mse.best_params_)\nprint(grid_mse.best_score_)\nprint('\\n')\nprint(f1s)\nprint(cm)\nprint(cr)","f62b5f29":"# Submission file\ntest['MULTIPLE_OFFENSE']=y_predict_4a\nsub=test['MULTIPLE_OFFENSE']\nsub.to_csv('sub_4a.csv')\ntest=test.drop(['MULTIPLE_OFFENSE'],axis=1)","fce83e15":"# Creating New columns\ntrain['DAYOFWEEK']=train['DATE'].dt.dayofweek\ntrain['WEEK']=train['DATE'].dt.week\ntest['DAYOFWEEK']=test['DATE'].dt.dayofweek\ntest['WEEK']=test['DATE'].dt.week\n\n# Filling NAN values\ntrain['X_12']=train['X_12'].fillna(0)\ntest['X_12']=test['X_12'].fillna(0)\n\n# Assign X and y\ny=train['MULTIPLE_OFFENSE']\nX=train.drop(['MULTIPLE_OFFENSE','DATE'],axis=1)\n\n# Train test Split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=.20,random_state=5)\n\n# Parameter grid\nxg_cl_param_grid={'n_estimators':[50,100,200,300,400],'max_depth':[2,3,4,5]}\n\n# XGBClassifier\nxg_cl=xgb.XGBClassifier(objective='reg:logistic',seed=123)\n\n# GridSearch CV\ngrid_mse=GridSearchCV(xg_cl,param_grid=xg_cl_param_grid,cv=3,scoring='recall',verbose=1,n_jobs=-1)\n\n# Train the model\ngrid_mse.fit(X_train,y_train)\n\n# Predict\ny_predict=grid_mse.predict(X_test)\ny_predict_4b=grid_mse.predict(test.drop(['DATE'],axis=1))\n\n# Metrics\nf1s=f1_score(y_test,y_predict)\ncm=confusion_matrix(y_test,y_predict)\ncr=classification_report(y_test,y_predict)\nres=recall_score(y_test,y_predict)\n\nprint(\"Recall score\",res)\nprint(grid_mse.best_params_)\nprint(grid_mse.best_score_)\nprint('\\n')\nprint(f1s)\nprint(cm)\nprint(cr)","d64ebaf1":"# Model created with best hyperparameter\nxg_cl1=xgb.XGBClassifier(objective='reg:logistic',seed=123,n_estimators=200,max_depth=2)\nxg_cl1.fit(X_train, y_train)\ny_predict=xg_cl1.predict(X_test)\nf1s=f1_score(y_test,y_predict)\ncm=confusion_matrix(y_test,y_predict)\ncr=classification_report(y_test,y_predict)\nres=recall_score(y_test,y_predict)\n\nprint(\"Recall score\",res)\nprint(\"F1 Score\",f1s)\nprint(\"Confusion Matrix\\n\",cm)\nprint(\"Classification Score\\n\",cr)","b460064a":"# Ploting Important Feature\nxgb.plot_importance(xg_cl1)","9b394f73":"# 200th tree\nxgb.plot_tree(xg_cl1,num_trees=199)","e686239b":"# Submission file\ntest['MULTIPLE_OFFENSE']=y_predict_4b\nsub=test['MULTIPLE_OFFENSE']\nsub.to_csv('sub_4b.csv')\ntest=test.drop(['MULTIPLE_OFFENSE'],axis=1)","f86c5108":"Our Model predicted all NAN values belongs to '0' in X_12 Columns.","d45c99bc":"# Table of Content\n\n    1. Problem Attributes\n    2. Problem Statement\n    3. Evaluation Criteria\n    4. Loading Data\n    5. Exploratory Data Analysis\n    6. EDA Conclusion\n    7. Model 1.A\n    8. Model 1.B\n    9. Model 2.A\n    10. Model 2.B\n    11. Model 3.A\n    12. Model 3.B\n    13. Model 4.A\n    14. Model 4.B\n","429de680":"Model 1.B. Summary\n\n* Removed 'DATE' and new columns 'DAYOFWEEK','WEEK'.\n* Removed 5042 duplicates in train dataset , 2820 duplicates in test dataset.\n* After removing  dupliicates 94.8% data belongs to Unhacked and 5.2% Hacked Target variable.\n* We removed 'X_12' column because it has NAN values.(Later in the process implemented XGBoost to predict the NAN values)\n* Not Balancing the Dataset.\n* Predicted Target VAriable for test Dataset.\n\n\n* **'X_11','X_15','X_10','X_2','X_14' are five important features.**","ee5dfe50":"* All missing values in column 'X_12' belongs to Target class 1(Hacked).","919a315f":"# Model 1.B.\n\nWhat we gonna do in this model?\n\n- Removing the Date column and Date Related Columns.\n- Check for duplicates.\n- Removing X_12 column because of NAN values.\n- Not balancing the dataset.","d8103596":"# Model 3.A.\n\nWhat we gonna do in this model?\n\n* Creating new features from Date column - week of the year, day of the year.\n* Not removing X_12 column.(because XGBoost can handle NAN Values).\n* Balancing Target Variable.","8dc4ea07":"Model 1.A. summary:\n\n* Removed 'DATE' and new columns 'DAYOFWEEK','WEEK'.\n* Removed 5042 duplicates in train dataset , 2820 duplicates in test dataset.\n* After removing  dupliicates 94.8% data belongs to Unhacked and 5.2% Hacked Target variable.\n* We removed 'X_12' column because it has NAN values.(Later in the process implemented XGBoost to predict the NAN values)\n* Balanced the Dataset.\n* Predicted the Target Variables\n","e460c574":"Model 2.B. Summary\n\n* Model with week of the year, day of the year.\n* without 'X_12' feature.\n* balancing the dataset.\n\n* 'X_11','X_15','X_10','X_4','WEEK','DAYOFWEEK' are six important features.\n\n\n* The New features really added value to the model.","f8dc6899":"# Evaluation Criteria\n\nRecall Score","56106b60":"* 94.8% target values belong to 'Not Hacked' Target class\n* 5.2% target values belong to 'Hacked' Target class","eef7858a":"Model 4.B. Summary\n\n* NAN value filled with XGBoost Model\n* X_11,X_15,X_10,X_12,WEEK are 5 most important features.","807c3164":"# EDA Summary\n\n* Train Dataset has 23856 rows and 17 columns.\n* Test Dataset has 15903 rows and 16 columns.\n* Target Variable has 95.5% of data belong to 'NotHacked Incients' and 4.4% of data belong to 'Hacked Incidents' target class. Dataset is unbalanced.We need to balance the Dataset to avoid domination of majority class over the minority class.\n* From 5 number Summary statistics,\n    * 'X_12' column only has missing values.\n    * All Masked Columns have Discrete Values.\n* From analysing Number of Unique Values per column and the Unique values, There may be high chance, all the Masked column features are Categorical column. Categories of the each column are label encoded.\n* All missing values in column 'X_12' belongs to Target class 1(Hacked).\n\n\n# EDA Conclusion\n\n**Choosed to Create XGBoost Model.**\n1. Because XGBoost can handle categorical and continuous features. \n2. We don't know either the features are categorical\/continuous.\n3. Handle NAN Values.\n4. Highly Suitable for Low latency applications.\n5. Model is Interpretable.\n","a15e7e96":"# Model 4.A.\n\nWhat we gonna do in this model?\n\n* Filling NaN with Xgboost model.\n* Balanced Dataset.","d9240eb8":"# Model 4.B.\n\nWhat we gonna do in this model?\n\n* Filling NaN with Xgboost model.\n* with Balanced Dataset","984d44a6":"* 95.5% target values belong to 'Not Hacked' Target class\n* 4.4% target values belong to 'Hacked' Target class\n* Dataset is unbalanced. We need to balance the Dataset to avoid domination of majority class over the minority class.","c4ba0741":"### Model 3.B.\n\nWhat we gonna do in this model?\n\n* Creating new features from date column - week of the year, day of the year\n* Not removing X_12 column.(because XGBoost can handle NAN Values).\n* Not Balancing Target Variable.","dff7b589":"# Problem Statement\n\nNeed to Predict if the server will be hacked","b948db82":"# Model 2.A.\n\nWhat we gonna do in this model?\n\n* Modelling with new features from date column - week of the year, day of the year.\n* Removing the X_12 column.\n* Balancing target class.","22b0fca9":"1. Dataset Shape\n2. Creating New Feature from Date Coumn.\n2. Column Datatype\n3. 5 number summary statistics\n4. Missing Values\n5. Duplicated Values\n6. Target Value count","438fc457":"# Model 2.B.\n\nWhat we gonna do in this model?\n\n* Modelling with new features from date column - week of the year, day of the year.\n* Removing the X_12 column.\n* Not Balancing target Class.","eecb9b90":"The rows with NaN values belongs to the \"Hacked Incidents\" target class.\n\nTo Handle Missing values,\nWe are going to remove the X_12 columns","a264f968":"# Problem Attributes\n\n* INCIDENT_ID - Unique identifier for an Incident Log\n* DATE - Date of Incident Occured\n* X_1 - X_15 - Anonymized Logging Parameter\n* MULTIPLE_OFFENSE - Indicates incident is a hack or not","eefb177d":"* \"X_12\" is the only column has missing values.","d13c75a6":"* All feature column has discrete values.\n* Looks like categorical columns that are label encoded.","783ecbf9":"Model 3.B. Summary\n\n* Model with week of the year, day of the year\n* 'X_12' feature with NAN value\n* Unbalanced dataset\n\n\n* 'X_11','X_15','X_10','X_12','X_2','DAYOFWEEK' are six important features.","04196045":"# Exploratory Data Analysis","a60b29a0":"Model 2.A. Summary\n\n* Model with week of the year, day of the year\n* without 'X_12' feature\n* balancing the dataset","ca43f419":"# Model 1.A.\n\nWhat we gonna do in this model?\n- Removing the Date column and Date Related Columns.\n- Checked for duplicates after removing. \n- Removing X_12 column because of NAN value.\n- Balancing the dataset.","f4505d60":"# Loading Data"}}