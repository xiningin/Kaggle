{"cell_type":{"07066f51":"code","43611728":"code","f6a825c5":"code","63be089d":"code","04f0357f":"code","5dea9c90":"code","10e5805c":"code","77408081":"code","c965e7d9":"code","cd748cb0":"code","54a2e70d":"code","8a09b39d":"code","9ee055f2":"code","9dacf4d5":"code","5576b8e0":"code","65ca5ebf":"code","8cd53d14":"code","52ab0c81":"code","235f69c7":"code","7d853d5d":"code","1160328d":"code","790d8597":"code","7807dfab":"code","eb510c86":"code","31d3680f":"code","c543d40b":"code","38b41462":"code","30fbc89f":"code","d489f2b4":"code","76251938":"code","4b0e08b9":"code","ac69485b":"code","dfec0fad":"code","7e8f79d7":"code","ed7a919d":"code","7f33898d":"code","7ff02e68":"code","1d23f818":"code","e232166e":"code","f8f0ab56":"code","47b63efb":"code","098a54d5":"code","52098abf":"code","dd38611e":"code","4eb5bc6e":"code","f28ed918":"code","1239b30c":"code","dd8838af":"code","ea2866a6":"code","41f0f9a9":"code","9f4ada02":"code","7dc296a9":"code","a886a406":"code","6373cab6":"code","ffdd2b60":"code","f281b0a0":"code","d4299427":"code","78b00359":"code","9cc5976c":"code","69d057e0":"code","ac4d6c2a":"code","7117f669":"code","7b4af2ce":"code","209d93a2":"code","6f4ce02e":"code","4aa2b915":"code","e2750bbd":"code","74130866":"code","deb36cf2":"code","443f64d5":"code","55a6bfb0":"code","5777e8a0":"code","7786002c":"code","1837d908":"code","b2e979e4":"code","34009414":"markdown","6c6df09b":"markdown","fea40782":"markdown","cdf05d4f":"markdown"},"source":{"07066f51":"# We use 'Numpy'for mathematical operations on large, multi-dimensional arrays and matrices\n# 'Pandas' is used for data manipulation and analysis\nimport numpy as np\nimport pandas as pd \n\n# To check the data type we import 'is_string_dtype' and 'is_numeric_dtype'\nfrom pandas.api.types import is_string_dtype\nfrom pandas.api.types import is_numeric_dtype\n\n# 'Matplotlib' is a data visualization library for 2D and 3D plots\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# seaborn is used for plotting statistical graphics\nimport seaborn as sns\n\n#  To build and analyze various statistical models we use 'Statsmodels'\nimport statsmodels\nimport statsmodels.api as sm\nfrom statsmodels.compat import lzip\nimport statsmodels.stats.api as sms\nimport statsmodels.formula.api as smf\nfrom statsmodels.formula.api import ols\nfrom statsmodels.tools.eval_measures import rmse\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# 'Scikit-learn' (sklearn) emphasizes various regression, classification and clustering algorithms\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\nfrom sklearn import preprocessing\n\n# To perform scientific computations\nfrom scipy.stats import shapiro\nfrom scipy import stats\n\n# 'Os' module provides functions for interacting with the operating system \nimport os\n","43611728":"# suppress the warnings \nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\n# set the plot size using 'rcParams'\n# once the plot size is set using 'rcParams', it sets the size of all the forthcoming plots in the file\n# 15 and 8 are width and height in inches respectively\nplt.rcParams['figure.figsize'] = [15,8]","f6a825c5":"# display all columns of the dataframe\npd.options.display.max_columns = None\n\n# display all rows of the dataframe\npd.options.display.max_rows = None\n\n# use below code to convert the 'exponential' values to float\nnp.set_printoptions(suppress=True)\n\npd.set_option('display.float_format', lambda x: '%.3f' % x)","63be089d":"# read csv file using pandas\ndf_property = pd.read_csv('..\/input\/ames-housing-dataset\/AmesHousing.csv')\n#..\/input\/ames-housing-dataset\/AmesHousing.csv\n# display the top 5 rows of the dataframe\ndf_property.head()","04f0357f":"# 'shape' function returns a tuple that gives the total number of rows and columns in the data\ndf_property.shape","5dea9c90":"df_property.dtypes","10e5805c":"df_property['Fireplaces'] = df_property['Fireplaces'].astype('O')\ndf_property['Kitchen AbvGr'] = df_property['Kitchen AbvGr'].astype('O')\ndf_property['Bedroom AbvGr'] = df_property['Bedroom AbvGr'].astype('O')\ndf_property['Half Bath'] = df_property['Half Bath'].astype('O')\ndf_property['Bsmt Half Bath'] = df_property['Bsmt Half Bath'].astype('O')\ndf_property['Bsmt Full Bath'] = df_property['Bsmt Full Bath'].astype('O')\ndf_property['Overall Cond'] = df_property['Overall Cond'].astype('O')\ndf_property['Overall Qual'] = df_property['Overall Qual'].astype('O')","77408081":"df_property.dtypes","c965e7d9":"# 'dtypes' provides the data type for each column\ndf_property[['Garage Cars', 'Fireplaces', 'Kitchen AbvGr', 'Bedroom AbvGr', 'Half Bath', 'Full Bath', 'Bsmt Half Bath','Bsmt Full Bath', 'Overall Cond', 'Overall Qual']].dtypes","cd748cb0":"df_property.drop(['Order'], axis=1, inplace=True)","54a2e70d":"df_property.columns","8a09b39d":"# For Statistical summary of numerical variable\n\ndf_property.describe()","9ee055f2":"# summary of categorical variables\ndf_property.describe(include = object)","9dacf4d5":"# use drop() to drop the redundant variables\n# 'axis = 1' drops the corresponding columns\ndf_property = df_property.drop(['Pool QC', 'Pool Area', 'Garage Yr Blt'], axis= 1)\n\n# re-check the shape of the dataframe\ndf_property.shape","5576b8e0":"# filter the numerical features in the dataset using select_dtypes()\n# include=np.number is used to select the numeric features\ndf_numeric_features = df_property.select_dtypes(include=np.number)\n\n# display the numeric features\ndf_numeric_features.columns","65ca5ebf":"# plot the histogram of numeric variables\n# hist() by default considers the numeric variables only, \n# rotate the x-axis labels by 20 degree using the parameter, 'xrot'\ndf_property.hist(xrot = 20, color = \"maroon\")\n\n# adjust the subplots\nplt.tight_layout()\n\n# display the plot\nplt.show()  ","8cd53d14":"# Sale Price Frequency Distribution\n# set the xlabel and the fontsize\nplt.xlabel(\"SalePrice\", fontsize=15)\n\n# set the ylabel and the fontsize\nplt.ylabel(\"Frequency\", fontsize=15)\n\n# set the title of the plot\nplt.title(\"Frequency Distribution\", fontsize=15)\n\n# plot the histogram for the target variable\nplt.hist(df_property[\"SalePrice\"], color = 'maroon')\nplt.show()","52ab0c81":"# create an empty list to store all the categorical variables\ncategorical=[]\n\n# check the data type of each variable\nfor column in df_property:\n\n    # check if the variable has the categorical type \n    if is_string_dtype(df_property[column]):\n        \n        # append the categorical variables to the list 'categorical'\n        categorical.append(column)\n\n# plot the count plot for each categorical variable \n# 'figsize' sets the figure size\nfig, ax = plt.subplots(nrows=7, ncols=6, figsize = (50, 35))\n\n# plot the count plot using countplot() for each categorical variable\nfor variable, subplot in zip(categorical, ax.flatten()):\n    sns.countplot(df_property[variable], ax = subplot)\n    \n# display the plot\nplt.show()","235f69c7":"# draw the boxplot for OverallQuality and the Property_Sale_Price\nsns.boxplot(y=\"SalePrice\", x=\"Overall Cond\", data= df_property)\n\n# set the title of the plot and the fontsize\nplt.title(\"Overall Cond vs SalePrice\", fontsize=15)\n\n# set the xlabel and the fontsize\nplt.xlabel(\"Overall Cond\", fontsize=15)\n\n# set the ylabel and the fontsize\nplt.ylabel(\"Sale Price\", fontsize=15)\n\n# display the plot\nplt.show()","7d853d5d":"# draw the boxplot for OverallQuality and the Property_Sale_Price\nsns.boxplot(y=\"SalePrice\", x=\"Overall Qual\", data= df_property)\n\n# set the title of the plot and the fontsize\nplt.title(\"Overall Quality vs Sale Price\", fontsize=15)\n\n# set the xlabel and the fontsize\nplt.xlabel(\"Overall Quality\", fontsize=15)\n\n# set the ylabel and the fontsize\nplt.ylabel(\"Sale Price\", fontsize=15)\n\n# display the plot\nplt.show()","1160328d":"# Pairplot of numeric variables\n\n# select the columns for the pairplot\ncolumns= [\"SalePrice\", \"Gr Liv Area\" ,\"Garage Cars\", \"Total Bsmt SF\", \"Year Built\", \"Year Remod\/Add\",\"Full Bath\"]\n         \n# draw the pairplot such that the diagonal should be density plot and the other graphs should be scatter plot\nsns.pairplot(df_property[columns], size=2, kind= \"scatter\", diag_kind=\"kde\")\n\n# display the plot\nplt.show()","790d8597":"# plot a boxplot of target variable to detect the outliers\nsns.boxplot(df_property['SalePrice'], color='maroon')\n\n# set plot label\n# set text size using 'fontsize'\nplt.title('Distribution of Target Variable (Property_Sale_Price)', fontsize = 15)\n\n# display the plot\nplt.show()","7807dfab":"# remove the observations with the house price greater than or equal to 500000\ndf_property = df_property[df_property['SalePrice'] < 500000]\n\n# check the dimension of the data\ndf_property.shape","eb510c86":"# 'isnull().sum()' returns the number of missing values in each variable\n# 'ascending = False': sorts values in the descending order\ntotal_nulls = df_property.isnull().sum().sort_values(ascending = False)          \n\n# calculate the percentage of missing values\n# 'ascending = False' sorts values in the descending order\npercent_null = (df_property.isnull().sum()*100\/df_property.isnull().count())  \npercent_null = percent_null.sort_values(ascending = False) \n\n# concat the 'total_nulls' and 'percent_null' columns\n# 'axis = 1' stands for columns\nmissing_values = pd.concat([total_nulls, percent_null], axis = 1, keys = ['Total Nulls', 'Percentage of Missing Values'])    \n\n# add the column containing data type of each variable\nmissing_values['Data Type'] = df_property[missing_values.index].dtypes\nmissing_values","31d3680f":"# filter out the categorical variables and consider only the numeric variables with missing values\nnum_missing_values = missing_values[(missing_values['Total Nulls'] > 0) & (missing_values['Data Type'] != 'object')]\nnum_missing_values","c543d40b":"# use the function fillna() to fill the missing values\ndf_property['Lot Frontage'] = df_property['Lot Frontage'].fillna(df_property['Lot Frontage'].median())","38b41462":"# use the function fillna() to fill the missing values\ndf_property['Mas Vnr Area'] = df_property['Mas Vnr Area'].fillna(df_property['Mas Vnr Area'].median())","30fbc89f":"# filter out the numerical variables and consider only the categorical variables with missing values\ncat_missing_values = missing_values[(missing_values['Total Nulls'] > 0) & (missing_values['Data Type'] == 'object')]\ncat_missing_values","d489f2b4":"# according to the data definition, 'NA' denotes the absence of miscellaneous feature\n# replace NA values in 'MiscFeature' with a valid value, 'None'\ndf_property['Misc Feature'] = df_property['Misc Feature'].fillna('None')\n\n# replace NA values in 'Alley' with a valid value, 'No alley access' \ndf_property['Alley'] = df_property['Alley'].fillna('No alley access')\n\n# replace NA values in 'Fence' with a valid value, 'No Fence'\ndf_property['Fence'] = df_property['Fence'].fillna('No Fence')\n\n# replace null values in 'FireplaceQu' with a valid value, 'No Fireplace' \ndf_property['Fireplace Qu'] = df_property['Fireplace Qu'].fillna('No Fireplace')","76251938":"# replace the missing values in the categoric variables representing the garage by `No Garage`\nfor col in ['Garage Type', 'Garage Finish', 'Garage Cond', 'Garage Qual']:\n    df_property[col].fillna('No Garage', inplace = True)","4b0e08b9":"# according to the data definition, 'NA' denotes the absence of basement in the variabels 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1','BsmtFinType2'\n# replace the missing values in the categoric variables representing the basement by `No Basement`\nfor col in ['BsmtFin Type 2', 'Bsmt Exposure', 'Bsmt Qual','Bsmt Cond','BsmtFin Type 1']:\n    df_property[col].fillna('No Basement', inplace = True)","ac69485b":"# according to the data definition, 'NA' denotes the absence of masonry veneer\n# replace the missing values in the categorical variable 'MasVnrType' with a value, 'None'\ndf_property['Mas Vnr Type'] = df_property['Mas Vnr Type'].fillna('None')","dfec0fad":"# replace the missing values in the categorical variable 'Electrical' with its mode\nmode_electrical = df_property['Electrical'].mode()\ndf_property['Electrical'].fillna(mode_electrical[0] , inplace = True)","7e8f79d7":"# filter out the categorical variables and consider only the numeric variables with missing values\nnum_missing_values = missing_values[(missing_values['Total Nulls'] > 0) & (missing_values['Data Type'] != 'object')]\nnum_missing_values","ed7a919d":"\n# use the function fillna() to fill the missing values\ndf_property['Lot Frontage'] = df_property['Lot Frontage'].fillna(0)","7f33898d":"# filter out the categorical variables and consider only the numeric variables with missing values\nnum_missing_values = missing_values[(missing_values['Total Nulls'] > 0) & (missing_values['Data Type'] != 'object')]\nnum_missing_values","7ff02e68":"df_property.isnull().sum()","1d23f818":"df_property.drop(df_property.columns[df_property.isnull().sum()>0], axis = 1, inplace = True)\n\ndf_property.head()","e232166e":"df_property.isnull().sum()","f8f0ab56":"df_property.shape","47b63efb":"# use the corr() function to generate the correlation matrix of the numeric variables\ncorrmat = df_property.corr()\n\n# print the correlation matrix\ncorrmat","098a54d5":"# set the plot size\nplt.figure(figsize = (35,25))\n\n# plot the heat map\n# corr: give the correlation matrix\n# cmap: color code used for plotting\n# annot_kws: sets the font size of the annotation\n# annot: prints the correlation values in the chart\n# vmax: gives a maximum range of values for the chart\n# vmin: gives a minimum range of values for the chart\nsns.heatmap(corrmat, annot = True, vmax = 1.0, vmin = -1.0, cmap = 'bwr', annot_kws = {\"size\": 11.5})\n\n# set the size of x and y axes labels using 'fontsize'\nplt.xticks(fontsize = 15)\nplt.yticks(fontsize = 15)\n\n# display the plot\nplt.show()","52098abf":"# create an empty list to store all the categorical variables\ncategorical=[]\n\n# check the data type of each variable\nfor column in df_property:\n\n    # check if the variable has the categorical type \n    if is_string_dtype(df_property[column]):\n        \n        # append the categorical variables to the list 'categorical'\n        categorical.append(column)\n\n# plot the count plot for each categorical variable \n# 'figsize' sets the figure size\nfig, ax = plt.subplots(nrows = 14, ncols = 3, figsize = (40, 100))\n\n# plot the boxplot for each categoric and target variable\nfor variable, subplot in zip(categorical, ax.flatten()):\n    sns.boxplot(x = variable, y = 'SalePrice', data = df_property, ax = subplot)\n    \n# display the plot\nplt.show()","dd38611e":"# check the distribution of target variable\ndf_property.SalePrice.hist(color = 'maroon')\n\n# add plot and axes labels\n# set text size using 'fontsize'\nplt.title('Distribution of Target Variable (SalePrice)', fontsize = 15)\nplt.xlabel('Property Sale Price', fontsize = 15)\nplt.ylabel('Frequency', fontsize = 15)\n\n# display the plot\nplt.show()","4eb5bc6e":"# shapiro() returns the the test statistics along with the p-value of the test\nstat, p = shapiro(df_property.SalePrice)\n\n# print the numeric outputs of the Shapiro-Wilk test upto 3 decimal places\nprint('Statistics=%.3f, p-value=%.3f' % (stat, p))\n\n# set the level of significance (alpha) to 0.05\nalpha = 0.05\n\n# if the p-value is less than alpha print we reject alpha\n# if the p-value is greater than alpha print we accept alpha \nif p > alpha:\n    print('The data is normally distributed (fail to reject H0)')\nelse:\n    print('The data is not normally distributed (reject H0)')","f28ed918":"# log transformation using np.log()\ndf_property['log_SalePrice'] = np.log(df_property['SalePrice'])\n\n# display the top 5 rows of the data\ndf_property.head()","1239b30c":"# recheck for normality \n# plot the histogram using hist\ndf_property.log_SalePrice.hist(color = 'maroon')\n\n# add plot and axes labels\n# set text size using 'fontsize'\nplt.title('Distribution of Log-transformed Target Variable (log_SalePrice)', fontsize = 15)\nplt.xlabel('Sale Price (log-transformed)', fontsize = 15)\nplt.ylabel('Frequency', fontsize = 15)\n\n# display the plot\nplt.show()","dd8838af":"# shapiro() returns the the test statistics along with the p-value of the test\nstat, p = shapiro(df_property['log_SalePrice'])\n\n# print the numeric outputs of the Shapiro-Wilk test upto 3 decimal places\nprint('Statistics=%.3f, p-value=%.3f' % (stat, p))\n\n# set the level of significance (alpha) to 0.05\nalpha = 0.05\n\n# if the p-value is less than alpha print we reject alpha\n# if the p-value is greater than alpha print we accept alpha \nif p > alpha:\n    print('The data is normally distributed (fail to reject H0)')\nelse:\n    print('The data is not normally distributed (reject H0)')","ea2866a6":"# find the skewness of the variable log_SalePrice\ndf_property['log_SalePrice'].skew()","41f0f9a9":"# filter out the categorical variables and consider only the numeric variables using (include=np.number)\ndf_numeric_features = df_property.select_dtypes(include=np.number)\n\n# display the numeric features\ndf_numeric_features.columns","9f4ada02":"# filter out the numerical variables and consider only the categorical variables using (include=object)\ndf_categoric_features = df_property.select_dtypes(include = object)\n\n# display categorical features\ndf_categoric_features.columns","7dc296a9":"# to create the dummy variables  we use 'get_dummies()' from pandas \n# to create (n-1) dummy variables we use 'drop_first = True' \ndummy_encoded_variables = pd.get_dummies(df_categoric_features, drop_first = True)","a886a406":"# concatenate the numerical and dummy encoded categorical variables column-wise\ndf_property_dummy = pd.concat([df_numeric_features, dummy_encoded_variables], axis=1)\n\n# display data with dummy variables\ndf_property_dummy.head()","6373cab6":"# check the dimensions of the dataframe\ndf_property_dummy.shape","ffdd2b60":"# add the intercept column using 'add_constant()'\ndf_property_dummy = sm.add_constant(df_property_dummy)\n\n# separate the independent and dependent variables\n# drop(): drops the specified columns\nX = df_property_dummy.drop(['SalePrice','log_SalePrice'], axis = 1)\n\n# extract the target variable from the data set\ny = df_property_dummy[['SalePrice','log_SalePrice']]\n\n# split data into train data and test data \n# what proportion of data should be included in test data is passed using 'test_size'\n# set 'random_state' to get the same data each time the code is executed \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1)\n\n# check the dimensions of the train & test subset for \n# print dimension of predictors train set\nprint(\"The shape of X_train is:\",X_train.shape)\n\n# print dimension of predictors test set\nprint(\"The shape of X_test is:\",X_test.shape)\n\n# print dimension of target train set\nprint(\"The shape of y_train is:\",y_train.shape)\n\n# print dimension of target test set\nprint(\"The shape of y_test is:\",y_test.shape)","f281b0a0":"# build a full model using OLS()\n# consider the log of sales price as the target variable\n# use fit() to fit the model on train data\nlinreg_logmodel_full = sm.OLS(y_train['log_SalePrice'], X_train).fit()\n\n# print the summary output\nprint(linreg_logmodel_full.summary())","d4299427":"# predict the 'log_Property_Sale_Price' using predict()\nlinreg_logmodel_full_predictions = linreg_logmodel_full.predict(X_test)","78b00359":"# take the exponential of predictions using np.exp()\npredicted_Property_Sale_Price = np.exp(linreg_logmodel_full_predictions)\n\n# extract the 'Property_Sale_Price' values from the test data\nactual_Property_Sale_Price = y_test['SalePrice']","9cc5976c":"# calculate rmse using rmse()\nlinreg_logmodel_full_rmse = rmse(actual_Property_Sale_Price, predicted_Property_Sale_Price)\n\n# calculate R-squared using rsquared\nlinreg_logmodel_full_rsquared = linreg_logmodel_full.rsquared\n\n# calculate Adjusted R-Squared using rsquared_adj\nlinreg_logmodel_full_rsquared_adj = linreg_logmodel_full.rsquared_adj ","69d057e0":"# create the result table for all accuracy scores\n# accuracy measures considered for model comparision are RMSE, R-squared value and Adjusted R-squared value\n# create a list of column names\ncols = ['Model', 'RMSE', 'R-Squared', 'Adj. R-Squared']\n\n# create a empty dataframe of the colums\n# columns: specifies the columns to be selected\nresult_tabulation = pd.DataFrame(columns = cols)\n\n# compile the required information\nlinreg_logmodel_full_metrics = pd.Series({'Model': \"Linreg full model with log of target variable \",\n                     'RMSE':linreg_logmodel_full_rmse,\n                     'R-Squared': linreg_logmodel_full_rsquared,\n                     'Adj. R-Squared': linreg_logmodel_full_rsquared_adj     \n                   })\n\n# append our result table using append()\n# ignore_index=True: does not use the index labels\n# python can only append a Series if ignore_index=True or if the Series has a name\nresult_tabulation = result_tabulation.append(linreg_logmodel_full_metrics, ignore_index = True)\n\n# print the result table\nresult_tabulation","ac4d6c2a":"# build a OLS model using function OLS()\n# Property_Sale_Price is our target variable\n# use fit() to fit the model on train data\nlinreg_nolog_model = sm.OLS(y_train['SalePrice'], X_train).fit()\n\n# print the summary output\nprint(linreg_nolog_model.summary())","7117f669":"# predict the 'Property_Sale_Price' using predict()\nlinreg_nolog_model_predictions = linreg_nolog_model.predict(X_test)","7b4af2ce":"# calculate rmse using rmse()\nlinreg_nolog_model_rmse = rmse(actual_Property_Sale_Price, linreg_nolog_model_predictions)\n\n# calculate R-squared using rsquared\nlinreg_nolog_model_rsquared = linreg_nolog_model.rsquared\n\n# calculate Adjusted R-Squared using rsquared_adj\nlinreg_nolog_model_rsquared_adj = linreg_nolog_model.rsquared_adj ","209d93a2":"# append the result table \n# compile the required information\nlinreg_nolog_model_metrics = pd.Series({'Model': \"Linreg full model without log of target variable \",\n                                                 'RMSE':linreg_nolog_model_rmse,\n                                                 'R-Squared': linreg_nolog_model_rsquared,\n                                                 'Adj. R-Squared': linreg_nolog_model_rsquared_adj})\n\n# append our result table using append()\n# ignore_index=True: does not use the index labels\n# python can only append a Series if ignore_index=True or if the Series has a name\nresult_tabulation = result_tabulation.append(linreg_nolog_model_metrics, ignore_index = True)\n\n# print the result table\nresult_tabulation","6f4ce02e":"# consider the independent variables in the dataframe 'df_property' \n# remove the target variables 'Property_Sale_Price' and 'log_Property_Sale_Price' using drop() function\ndf_property_features = df_property.drop(['SalePrice', 'log_SalePrice'], axis = 1)\n\n# filter out the categorical variables and consider only the numerical variables using (include=np.number)\ndf_numeric_features_vif = df_property_features.select_dtypes(include=[np.number])\n\n# display the first five observations\ndf_numeric_features_vif.head()","4aa2b915":"# create an empty dataframe to store the VIF for each variable\nvif = pd.DataFrame()\n\n# calculate VIF using list comprehension \n# use for loop to access each variable \n# calculate VIF for each variable and create a column 'VIF_Factor' to store the values \nvif[\"VIF_Factor\"] = [variance_inflation_factor(df_numeric_features_vif.values, i) for i in range(df_numeric_features_vif.shape[1])]\n\n# create a column of variable names\nvif[\"Features\"] = df_numeric_features_vif.columns\n\n# sort the dataframe based on the values of VIF_Factor in descending order\n# 'reset_index' resets the index of the dataframe\n# 'ascending = False' sorts the data in descending order\n# 'drop = True' drops the index that was previously created\nvif.sort_values('VIF_Factor', ascending = False).reset_index(drop = True)","e2750bbd":"# we will calculate the VIF for each numerical variable\nfor ind in range(len(df_numeric_features_vif.columns)):\n    \n    # create an empty dataframe\n    vif = pd.DataFrame()\n\n    # calculate VIF for each variable and create a column 'VIF_Factor' to store the values \n    vif[\"VIF_Factor\"] = [variance_inflation_factor(df_numeric_features_vif.values, i) for i in range(df_numeric_features_vif.shape[1])]\n\n    # create a column of feature names\n    vif[\"Features\"] = df_numeric_features_vif.columns\n\n    # filter the variables with VIF greater than 10 and store it in a dataframe 'vif_more_than_10' \n    # one can choose the threshold other than 10 (it depends on the business requirements)\n    vif_more_than_10 = vif[vif['VIF_Factor'] > 10]\n    \n    # if dataframe 'vif_more_than_10' is not empty, then sort the dataframe by VIF values\n    # if dataframe 'vif_more_than_10' is empty (i.e. all VIF <= 10), then print the dataframe 'vif' and break the for loop using 'break' \n    # 'by' sorts the data using given variable(s)\n    # 'ascending = False' sorts the data in descending order\n    if(vif_more_than_10.empty == False):\n        df_sorted = vif_more_than_10.sort_values(by = 'VIF_Factor', ascending = False)\n    else:\n        print(vif)\n        break\n    \n    # if  dataframe 'df_sorted' is not empty, then drop the first entry in the column 'Features' from the numeric variables\n    # select the variable using 'iloc[]'\n    # 'axis=1' drops the corresponding column\n    #  else print the final dataframe 'vif' with all values after removal of variables with VIF less than 10  \n    if (df_sorted.empty == False):\n        df_numeric_features_vif = df_numeric_features_vif.drop(df_sorted.Features.iloc[0], axis=1)\n    else:\n        print(vif)","74130866":"# lets consider the variables obtained from VIF\n# use the dummy variables created previously\n# concatenate the numerical and dummy encoded categorical variables\ndf_dummy = pd.concat([df_numeric_features_vif, dummy_encoded_variables], axis=1)\n\n# display data with dummy variables\ndf_dummy.head()","deb36cf2":"# add the intercept column using 'add_constant()'\ndf_dummy = sm.add_constant(df_dummy)\n\n# consider independent variables\n# create a copy of 'df_dummy' and store it as X\nX = df_dummy.copy()\n\n# extract the target variable from the data set\ny = df_property[['SalePrice','log_SalePrice']]\n\n# split data into train data and test data \n# what proportion of data should be included in test data is specified using 'test_size'\n# set 'random_state' to get the same data each time the code is executed  \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1)\n\n# check the dimensions of the train & test subset for \n# print dimension of predictors train set\nprint(\"The shape of X_train is:\",X_train.shape)\n\n# print dimension of predictors test set\nprint(\"The shape of X_test is:\",X_test.shape)\n\n# print dimension of target train set\nprint(\"The shape of y_train is:\",y_train.shape)\n\n# print dimension of target test set\nprint(\"The shape of y_test is:\",y_test.shape)","443f64d5":"# build a full model using OLS()\n# consider the target variable log_Property_Sale_Price \n# use fit() to fit the model on train data\nlinreg_vif_model = sm.OLS(y_train['log_SalePrice'], X_train).fit()\n\n# print the summary output\nprint(linreg_vif_model.summary())","55a6bfb0":"# predict the 'log_Property_Sale_Price' using predict()\nlinreg_vif_model_predictions = linreg_vif_model.predict(X_test)","5777e8a0":"# take the exponential of predictions using np.exp()\npredicted_Property_Sale_Price = np.exp(linreg_vif_model_predictions)\n\n# extract the 'Property_Sale_Price' values from the test data\nactual_Property_Sale_Price = y_test['SalePrice']","7786002c":"# calculate rmse using rmse()\nlinreg_vif_model_rmse = rmse(actual_Property_Sale_Price, predicted_Property_Sale_Price)\n\n# calculate R-squared using rsquared\nlinreg_vif_model_rsquared = linreg_vif_model.rsquared\n\n# calculate Adjusted R-Squared using rsquared_adj\nlinreg_vif_model_rsquared_adj = linreg_vif_model.rsquared_adj ","1837d908":"# append the accuracy scores to the table\n# compile the required information\nlinreg_vif_model_metrics = pd.Series({'Model': \"Linreg with VIF\",\n                                                'RMSE': linreg_vif_model_rmse,\n                                                'R-Squared': linreg_vif_model_rsquared,\n                                                'Adj. R-Squared': linreg_vif_model_rsquared_adj})\n\n# append our result table using append()\n# ignore_index=True: does not use the index labels\n# python can only append a Series if ignore_index=True or if the Series has a name\nresult_tabulation = result_tabulation.append(linreg_vif_model_metrics, ignore_index = True)\n\n# print the result table\nresult_tabulation","b2e979e4":"fig, ax = plt.subplots(1, 1)\nsm.ProbPlot(linreg_vif_model.resid).qqplot(line='s', color='#1f77b4', ax=ax)\nax.title.set_text('QQ Plot')","34009414":"No log method","6c6df09b":"It can be visually seen that the data has near-normal distribution, but Shapiro-Wilk test does not support the claim.\nNote that in reality it might be very tough for your data to adhere to all assumptions your algorithm needs.","fea40782":"Linear regression Using Ols Method\n","cdf05d4f":"Linear Regression Using VIF Variation Inflation Factor"}}