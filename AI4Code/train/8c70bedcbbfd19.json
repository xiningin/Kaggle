{"cell_type":{"facc109e":"code","35215bc2":"code","aeac17b8":"code","41042dfa":"code","eaf697ca":"code","418b3fc3":"code","84ffdd0b":"code","393ac6b9":"code","25329e91":"code","cb75a7e9":"code","0fedcc5c":"code","eb3daa50":"code","7aeb7d40":"markdown"},"source":{"facc109e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","35215bc2":"df = pd.read_csv('..\/input\/bnp-paribas-cardif-claims-management\/train.csv.zip')\ndf.shape","aeac17b8":"df.head()","41042dfa":"df.describe()","eaf697ca":"df['target'].unique()","418b3fc3":"df.dropna(inplace=True)","84ffdd0b":"df.isna().sum()","393ac6b9":"df","25329e91":"import sklearn\nprint(sklearn.__version__)","cb75a7e9":"import xgboost\nprint(xgboost.__version__)","0fedcc5c":"# xgboost for feature importance on a classification problem\nfrom sklearn.datasets import make_classification\nfrom xgboost import XGBClassifier\nfrom matplotlib import pyplot\n# define dataset\nX, y = make_classification(n_samples=1000, n_features=133, n_informative=100, n_redundant=5, random_state=1)\n# define the model\nmodel = XGBClassifier()\n# fit the model\nmodel.fit(X, y)\n# get importance\nimportance = model.feature_importances_\n# summarize feature importance\nfor i,v in enumerate(importance>0.01):\n\tprint('Feature: %0d, Score: %.5f' % (i,v))\n# plot feature importance\npyplot.bar([x for x in range(len(importance))], importance)\npyplot.show()","eb3daa50":"df[]","7aeb7d40":"XGBoost Classification Feature Importance\nThe complete example of fitting an XGBClassifier and summarizing the calculated feature importance scores is listed below.\n"}}