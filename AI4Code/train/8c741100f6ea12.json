{"cell_type":{"ec7f6330":"code","a7ea289d":"code","c06e618c":"code","b88981e2":"code","fbc06e36":"code","46b8e45d":"code","306650c5":"code","65c09c36":"code","7f90c9cc":"code","879a4247":"code","747d51b0":"code","0c228794":"code","5916d7f7":"code","1fbf441b":"code","92f158dc":"code","1fcf4124":"code","d8ba4975":"code","4fb011d9":"code","11576e49":"code","18c38f39":"code","9764e7a0":"code","d67cf8de":"code","269cc175":"code","59740c46":"code","34cbb784":"code","8b653d9c":"code","b84430a8":"code","508d2438":"code","0b5571fc":"code","11c3810b":"code","74f826ce":"code","cc8d37b1":"code","d3db46e5":"code","787f9d9a":"code","06780585":"markdown","a8bf097a":"markdown","18dbdf92":"markdown","632a8d5a":"markdown","3a3a1ca7":"markdown","c264927c":"markdown","eb382e0d":"markdown","aeed716e":"markdown","a2767ed7":"markdown","975ed6df":"markdown","cd182236":"markdown","3e988e69":"markdown","cbcc305c":"markdown","d2067549":"markdown","7c5a2477":"markdown","3bec2ea0":"markdown","c5ad4d33":"markdown","ce3103c1":"markdown","1abcdfc3":"markdown","0d25df1c":"markdown","baf44b6b":"markdown","4a85d236":"markdown","5ed2a891":"markdown"},"source":{"ec7f6330":"import pandas as pd\nimport requests\nfrom pandas.io.json import json_normalize\nr = requests.get(\"https:\/\/im6qye3mc3.execute-api.eu-central-1.amazonaws.com\/prod\", headers={'Accept': 'application\/json'})\njson_body = r.json()[\"body\"]","a7ea289d":"import math\nimport dateutil.parser\nfrom collections import defaultdict\n\nsocial_distancing_dict = defaultdict(list)\nsources = [\"hystreet_score\", \"zug_score\", \"nationalExpress_score\", \"regional_score\", \"suburban_score\", \"national_score\", \"bus_score\", \"tomtom_score\", \"webcam_score\", \"bike_score\", \"gmap_score\", \"lemgoDigital\", \"date\", \"airquality_score\"]\nfor date, districtJson in json_body.items():\n    for ags, district in districtJson.items():\n        social_distancing_dict[\"key\"].append(ags+\"_\"+date)\n        social_distancing_dict[\"date\"].append(dateutil.parser.parse(date))\n        social_distancing_dict[\"AGS\"].append(ags)\n        copySources = sources.copy()\n        for key, value in district.items():\n            try:\n                copySources.remove(key)\n                if(not (key == \"date\")):\n                    social_distancing_dict[key].append(value)\n            except:\n                print(\"Problem with: \"+key)\n        for valuesLeft in copySources:\n            social_distancing_dict[valuesLeft].append(math.nan)\n\n# for key, list in social_distancing_dict.items():\n#    print(key+\" \"+str(len(list)))\n\nsocial_distancing = pd.DataFrame.from_dict(social_distancing_dict)\n\n# copy Berlin values to all suburbs\nfor idx, row in social_distancing[social_distancing['AGS'] == \"11000\"].iterrows():\n    for berlinAgs in [\"11002\", \"11001\", \"11008\", \"11010\", \"11004\", \"11011\", \"11007\", \"11012\", \"11005\", \"11006\", \"11003\", \"11009\"]:\n        x1=social_distancing.loc[[idx],:]\n        x1.key = berlinAgs+\"_\"+row.date.date().isoformat()\n        x1.AGS=berlinAgs\n#        print(x1)\n        social_distancing = social_distancing.append(x1, ignore_index=True)\n\n# social_distancing[social_distancing['AGS'] == '11002']\n# social_distancing.key = social_distancing[\"AGS\"]+\"_\"+social_distancing[\"date\"].astype(str)","c06e618c":"social_distancing.head()","b88981e2":"social_distancing.describe()","fbc06e36":"hamburg_sd_rows = social_distancing[\"AGS\"] == \"02000\"\nhamburg_sd = social_distancing[hamburg_sd_rows]\nmuenchen_sd_rows = social_distancing[\"AGS\"] == \"09162\"\nmuenchen_sd = social_distancing[muenchen_sd_rows]","46b8e45d":"hamburg_sd.head()","306650c5":"import seaborn as sn\nimport matplotlib.pyplot as plt","65c09c36":"df = hamburg_sd.pivot(index='date', columns='AGS', values='hystreet_score')\ndf.plot()\nplt.show()","7f90c9cc":"landkreise = pd.read_csv(\"..\/input\/landkreise\/Landkreise.csv\",dtype={\"AGS\":\"str\", \"RS\": \"str\"})","879a4247":"import folium\nfrom folium import Choropleth, Circle, Marker\nfoliumMap = folium.Map(location=[51.0,9.0], tiles='openstreetmap', zoom_start=5)\n\n# Add points to the map\nfor idx, row in landkreise.iterrows():\n    Marker([row['Y'], row['X']], popup=row['GEN']).add_to(foliumMap)\nfoliumMap","747d51b0":"import pylab as pl\nhist = landkreise.hist(column=\"EWZ\", bins=100)\npl.suptitle(\"Count of districts with amount of inhabitans\")","0c228794":"r = requests.get(\"https:\/\/services7.arcgis.com\/mOBPykOjAyBO2ZKk\/arcgis\/rest\/services\/Covid19_RKI_Sums\/FeatureServer\/0\/query?where=%28Meldedatum%3Etimestamp+%272020-01-01+22%3A59%3A59%27+AND+%28Meldedatum%3Ctimestamp+%272020-12-31+22%3A00%3A00%27+OR+Meldedatum+%3E+timestamp+%272020-04-05+21%3A59%3A59%27%29%29&objectIds=&time=&resultType=none&outFields=ObjectId%2CSummeFall%2CSummeTodesfall%2CMeldedatum%2CIdLandkreis%2CAnzahlFall%2CAnzahlTodesfall&returnIdsOnly=false&returnUniqueIdsOnly=false&returnCountOnly=true&returnDistinctValues=false&cacheHint=true&orderByFields=Meldedatum+asc&groupByFieldsForStatistics=&outStatistics=&having=&sqlFormat=none&f=json&token=\", headers={'Accept': 'application\/json'})\ncount = r.json()[\"count\"]\njson_features = []\nfor i in range(0, count, 2000):\n    r = requests.get(\"https:\/\/services7.arcgis.com\/mOBPykOjAyBO2ZKk\/arcgis\/rest\/services\/Covid19_RKI_Sums\/FeatureServer\/0\/query?where=%28Meldedatum%3Etimestamp+%272020-01-01+22%3A59%3A59%27+AND+%28Meldedatum%3Ctimestamp+%272020-12-31+22%3A00%3A00%27+OR+Meldedatum+%3E+timestamp+%272020-04-05+21%3A59%3A59%27%29%29&objectIds=&time=&resultType=none&outFields=ObjectId%2CSummeFall%2CSummeTodesfall%2CMeldedatum%2CIdLandkreis%2CAnzahlFall%2CAnzahlTodesfall&returnIdsOnly=false&returnUniqueIdsOnly=false&returnCountOnly=false&returnDistinctValues=false&cacheHint=true&orderByFields=IdLandkreis,Meldedatum+asc&groupByFieldsForStatistics=&outStatistics=&having=&resultOffset=\"+str(i)+\"&resultRecordCount=2000&sqlFormat=none&f=json&token=\", headers={'Accept': 'application\/json'})\n    r.encoding = \"utf-8\"\n    json_features.extend(r.json()[\"features\"])","5916d7f7":"from datetime import datetime, timedelta\nCovid19_RKI_Sums_Dict = defaultdict(list)\nSummeFall = 0\nAnzahlFall = 0\nSummeTodesfall = 0\nrelativeGrowthCases = 0\nrelativeGrowthDeath = 0\nags2firstCaseDate = {}\nags2casesDate = {}\nagsDayAfterRow = {}\njson_features.sort(key=lambda o: o.attributes.Meldedatum if 'attribute' in o.keys() else 0 )\nfor feature in json_features:\n    keyDate = datetime.fromtimestamp(feature[\"attributes\"][\"Meldedatum\"]\/1000-60*60*24*7)\n    date = datetime.fromtimestamp(feature[\"attributes\"][\"Meldedatum\"]\/1000)\n    ags = feature[\"attributes\"][\"IdLandkreis\"]\n    Covid19_RKI_Sums_Dict[\"key\"].append(ags+\"_\"+keyDate.date().isoformat())\n    Covid19_RKI_Sums_Dict[\"Weekday\"].append(keyDate.date().weekday())\n    # next key\n    Covid19_RKI_Sums_Dict[\"next_key\"].append(ags+\"_\"+(keyDate.date()+timedelta(days=1)).isoformat())\n    for key, value in feature[\"attributes\"].items():\n        Covid19_RKI_Sums_Dict[key].append(date if key == \"Meldedatum\" else value)\n        if(key == \"AnzahlFall\"):\n            AnzahlFall = value\n        if(key == \"SummeFall\"):\n            relativeGrowthCases = value\/SummeFall if SummeFall != 0 else 0\n            SummeFall = value\n            # If there is a case and we don't have a first date case yet or the date is smaller than the date we already have\n            if(SummeFall != 0 and ((not ags in ags2firstCaseDate.keys()) or date < ags2firstCaseDate[ags])):\n                ags2firstCaseDate[ags] = date\n        if(key == \"SummeTodesfall\"):\n            relativeGrowthDeath = value\/SummeTodesfall if SummeTodesfall != 0 else 0\n            SummeTodesfall = value\n    Covid19_RKI_Sums_Dict[\"relativeGrowthCases\"].append(math.nan if relativeGrowthCases == 0 else relativeGrowthCases-1)\n    Covid19_RKI_Sums_Dict[\"relativeGrowthDeath\"].append(math.nan if relativeGrowthDeath == 0 else relativeGrowthDeath-1)\n    Covid19_RKI_Sums_Dict[\"daysSinceFirstCase\"].append(math.nan if not (ags in ags2firstCaseDate) else (date-ags2firstCaseDate[ags]).days)\n    \n    for d in range(0,7):\n        Covid19_RKI_Sums_Dict[\"cases\"+str(d+1)+\"DaysBefore\"].append(0 if (not ags in ags2casesDate.keys() or len(ags2casesDate[ags]) < d) else ags2casesDate[ags][len(ags2casesDate[ags])-d-1])\n    \n    # create features with cases the last 7 days before\n    if(not ags in ags2casesDate):\n        ags2casesDate[ags] = []\n    ags2casesDate[ags].append(AnzahlFall)\n        \n\nCovid19_RKI_Sums = pd.DataFrame.from_dict(Covid19_RKI_Sums_Dict)\n\n# Covid19_RKI_Sums[Covid19_RKI_Sums['IdLandkreis'] == '11001']","1fbf441b":"df = Covid19_RKI_Sums.pivot(index='Meldedatum', columns='IdLandkreis', values='SummeFall')\ndf.plot(figsize=(20,10))\nplt.show()","92f158dc":"biggestCitiesOver250000 = {\n    \"02000\": \"Hamburg\",\n    \"09162\": \"M\u00fcnchen\",\n    \"05315\": \"K\u00f6ln\",\n    \"06412\": \"Frankfurt am Main\",\n    \"05111\": \"D\u00fcsseldorf\",\n    \"14713\": \"Leipzig\",\n    \"05913\": \"Dortmund\",\n    \"05113\": \"Essen\",\n    \"04011\": \"Bremen\",\n    \"14612\": \"Dresden\",\n    \"09564\": \"N\u00fcrnberg\",\n    \"05112\": \"Duisburg\",\n    \"05911\": \"Bochum\",\n    \"05124\": \"Wuppertal\",\n    \"05711\": \"Bielefeld\",\n    \"05314\": \"Bonn\",\n    \"05515\": \"M\u00fcnster\",\n    \"09761\": \"Augsburg\",\n    \"06414\": \"Wiesbaden\",\n    \"05116\": \"M\u00f6nchengladbach\",\n    \"05513\": \"Gelsenkirchen\"\n}\n\ncityRows = Covid19_RKI_Sums[\"IdLandkreis\"].isin(biggestCitiesOver250000.keys())\n\ndf = Covid19_RKI_Sums[cityRows].pivot(index='Meldedatum', columns='IdLandkreis', values='SummeFall')\ndf.plot(figsize=(20,10))\nplt.show()","1fcf4124":"hamburg_rows = Covid19_RKI_Sums[\"IdLandkreis\"] == \"02000\"\nhamburg = Covid19_RKI_Sums[hamburg_rows]","d8ba4975":"hamburg","4fb011d9":"df = hamburg.pivot(index='Meldedatum', columns='IdLandkreis', values='SummeFall')\ndf.plot(figsize=(20,10))\nplt.show()","11576e49":"Covid19_RKI_Sums[\"AGS\"] = Covid19_RKI_Sums[\"IdLandkreis\"]\nsocial_distancing = social_distancing.drop(columns=['AGS'])\nCovid19 = Covid19_RKI_Sums.merge(social_distancing, on=\"key\", how=\"left\").merge(landkreise, on=\"AGS\")\n# social_distancing.AGS.unique()\n# Covid19_RKI_Sums.AGS.unique()\n# landkreise.AGS.unique()","18c38f39":"from datetime import date, datetime, timedelta\nimport numpy as np\nfrom scipy import stats\n\nCityVsCountry = Covid19.copy()\nCityVsCountry[\"CasesPer100000\"] = Covid19.AnzahlFall\/Covid19.EWZ\/100000\n\nyesterday = np.datetime64(date.today() - timedelta(days=1))\ncities = CityVsCountry[(CityVsCountry['BEZ'] == \"Kreisfreie Stadt\") & (CityVsCountry['Meldedatum'] == yesterday)]\ncountryside = CityVsCountry[(CityVsCountry['BEZ'] == \"Landkreis\") & (CityVsCountry['Meldedatum'] == yesterday)]\ncities.hist(column=\"CasesPer100000\", bins=100)\ncountryside.hist(column=\"CasesPer100000\", bins=100)\nstats.ttest_ind(cities['CasesPer100000'],countryside['CasesPer100000'], equal_var = False)","9764e7a0":"Covid19\n# Covid19_RKI_Sums[Covid19_RKI_Sums['IdLandkreis'] == '11001']\n# social_distancing[social_distancing[\"key\"] == ]\n# Covid19[Covid19['AGS'] == '11001']","d67cf8de":"for ags, name in biggestCitiesOver250000.items():\n    city_covid19_rows = Covid19[\"AGS\"] == ags\n    cityFrame = Covid19[city_covid19_rows]\n    sn.lmplot(x='hystreet_score',y='relativeGrowthCases',data=cityFrame) \n    ax = plt.gca()\n    ax.set_title(name); #+\" (\"+np.corrcoef(np.array(cityFrame[\"hystreet_score\"]), np.array(cityFrame[\"relativeGrowthCases\"]))+\")\")\n    plt.show()\n\n","269cc175":"Covid19.to_csv('COVID19.csv',index=False)","59740c46":"import numpy as np\nimport lightgbm as lgb\nimport hyperopt\nfrom hyperopt import hp, tpe, Trials\nfrom hyperopt.fmin import fmin\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\n# Define the official root mean square logaritmic error function, that is officially used in the forecast compeition 0.2 - excellent, 1 - no so good, >1 terrible \ndef RMSLE(predict, target):\n    total = 0 \n    for k in range(len(predict)):\n        LPred= np.log1p(predict[k]+1)\n        LTarg = np.log1p(target[k] + 1)\n        if not (math.isnan(LPred)) and  not (math.isnan(LTarg)): \n            total = total + ((LPred-LTarg) **2)\n        \n    total = total \/ len(predict)        \n    return np.sqrt(total)\n\n# Copy the dataframe because we are going to do some heavy modifications\nLightGBMCovid19 = Covid19.copy()\n# Sort the data frame by data, this is necessary to have a split between old data and newer data\nLightGBMCovid19 = LightGBMCovid19.sort_values(by=['date'])\n# Save the next key, this is later needed to build the data for prediction\nLightGBMCovid19NextKeys = LightGBMCovid19['next_key']\n# Drop all columns that we are not going to need\nLightGBMCovid19 = LightGBMCovid19.drop(columns=['key', 'next_key', 'Meldedatum', 'IdLandkreis', 'date', 'GEN', 'BEM', 'NBD', 'FK_S3', 'NUTS', 'WSK', 'DEBKG_ID', 'relativeGrowthCases',\n                                                'X', 'Y', 'SummeTodesfall', 'SummeFall', 'AnzahlTodesfall', 'SN_K', 'OBJECTID',\n                                                'KFL', 'ObjectId', 'RS', 'SDV_RS', 'RS_0', 'AGS_0', 'SN_L', 'SN_R',\n                                                'relativeGrowthDeath', 'IBZ', 'ADE', 'GF', 'BSG', 'SN_V1', 'SN_V2', 'SN_G', 'Shape_Length'])\n\n#LightGBMCovid19 = LightGBMCovid19.drop(columns=['cases1DaysBefore', 'cases2DaysBefore', 'cases3DaysBefore', 'cases4DaysBefore', 'cases5DaysBefore',\n#                                                'cases6DaysBefore', 'cases7DaysBefore', \"zug_score\", \"nationalExpress_score\",\n#                                                \"regional_score\", \"suburban_score\", \"national_score\", \"bus_score\", \"tomtom_score\",\n#                                                \"webcam_score\", \"bike_score\", \"gmap_score\", \"lemgoDigitalLightGBMCovid19\", \"EWZ\", \"Shape_Area\", \"daysSinceFirstCase\", 'hystreet_score'])\n\n# Make a category feature out of the AGS\nLightGBMCovid19.AGS = LightGBMCovid19.AGS.astype('category')\nLightGBMCovid19.BEZ = LightGBMCovid19.BEZ.astype('category')\nLightGBMCovid19.Weekday = LightGBMCovid19.Weekday.astype('category')\nLightGBMCovid19['PopulationDensity'] = LightGBMCovid19.EWZ\/LightGBMCovid19.Shape_Area\n\n#y = np.log1p(LightGBMCovid19.AnzahlFall)\n# We are going to predict the cases on a daily basis\ny = LightGBMCovid19.AnzahlFall\nAnzahlFall = LightGBMCovid19.AnzahlFall\n# Drop the column\nLightGBMCovid19 = LightGBMCovid19.drop(columns=['AnzahlFall'])\n\n# We are splitting the set for 90% training set and 10% test set, we are using date for finding the spliting point\nX_train, X_test, y_train, y_test = train_test_split(LightGBMCovid19, y, test_size=0.10, shuffle=False)","34cbb784":"for n in range(7, 10):\n    # check the RMSE for taking a constant as the new cases\n    rmse = mean_squared_error(y_test, np.repeat(n, len(y_test))) ** 0.5\n    print('The rmse of constant '+str(n)+' benchmark is:', rmse)\n\nrmse = mean_squared_error(y_test, X_test[\"cases1DaysBefore\"]) ** 0.5\nprint('The rmse of using the value from the day before benchmark is:', rmse)","8b653d9c":"# Create LightGBM datasets \nlgb_train = lgb.Dataset(X_train, y_train)\nlgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n\n\n# specify your configurations as a dict\n#params = {\n#    'boosting_type': 'gbdt',\n#    'objective': 'regression',\n#    'metric': {'l2', 'l1'},\n#    'num_leaves': 30,\n#    'learning_rate': 0.05,\n#    'feature_fraction': 0.9,\n#    'bagging_fraction': 0.8,\n#    'bagging_freq': 5,\n#    'verbose': 0\n#}\n# optimized with hyperopt\nparams = {\n    'bagging_fraction': 0.9009356793140582,\n    'boosting_type': 'dart',\n    'metric': {'l2', 'l1'},\n    'colsample_bytree': None,\n    'feature_fraction': 0.9406373736120387,\n    'lambda_l1': 4.318973164557415,\n    'lambda_l2': 0.3976487343083751,\n    'learning_rate': 0.030871120513205383,\n    'min_child_samples': None,\n    'min_child_weight': 0.000420873079335671,\n    'min_data_in_leaf': 7,\n    'min_sum_hessian_in_leaf': None,\n    'num_leaves': 149,\n    'objective': 'regression',\n    'reg_alpha': None,\n    'reg_lambda': None,\n    'subsample': None,\n    'subsample_for_bin': 40000,\n    'verbosity': 0\n}\nevals_result = {}\ngbm = lgb.train(params,\n            lgb_train,\n            #num_boost_round=35,\n            valid_sets=lgb_eval,\n            evals_result=evals_result,\n            #early_stopping_rounds=5\n)\n\nprint('Starting predicting...')\n# predict\ny_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n# eval\nrmse = mean_squared_error(y_test, y_pred) ** 0.5\nprint('The rmse of prediction is:', rmse)\nprint('The RMSLE of prediction is:', RMSLE(y_pred.tolist(), y_test.tolist()))\n\n\nprint('Saving model...')\n# save model to file\ngbm.save_model('model.txt')\n\n\n\nprint('Plotting metrics recorded during training...')\nax = lgb.plot_metric(evals_result, metric='l1')\nplt.show()\n\nprint('Plotting feature importances...')\nax = lgb.plot_importance(gbm, max_num_features=10)\nplt.show()\n\nprint('Plotting split value histogram...')\nax = lgb.plot_split_value_histogram(gbm, feature='cases1DaysBefore', bins='auto')\nplt.show()\n","b84430a8":"all_params = []\n\nspace = {\n    #this is just piling on most of the possible parameter values for LGBM\n    #some of them apparently don't make sense together, but works for now.. :)\n    'objective':'regression',\n    'boosting_type': hp.choice('boosting_type',\n                               [{'boosting_type': 'gbdt',\n#                                     'subsample': hp.uniform('dart_subsample', 0.5, 1)\n                                 },\n                                {'boosting_type': 'dart',\n#                                     'subsample': hp.uniform('dart_subsample', 0.5, 1)\n                                 },\n                                {'boosting_type': 'goss'}]),\n    'num_leaves': hp.quniform('num_leaves', 30, 150, 1),\n    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.2)),\n    'subsample_for_bin': hp.quniform('subsample_for_bin', 20000, 300000, 20000),\n    'feature_fraction': hp.uniform('feature_fraction', 0.5, 1),\n    'bagging_fraction': hp.uniform('bagging_fraction', 0.5, 1), #alias \"subsample\"\n    'min_data_in_leaf': hp.qloguniform('min_data_in_leaf', 0, 6, 1),\n    'lambda_l1': hp.choice('lambda_l1', [0, hp.loguniform('lambda_l1_positive', -16, 2)]),\n    'lambda_l2': hp.choice('lambda_l2', [0, hp.loguniform('lambda_l2_positive', -16, 2)]),\n    'verbosity': 0,\n    #the LGBM parameters docs list various aliases, and the LGBM implementation seems to complain about\n    #the following not being used due to other params, so trying to silence the complaints by setting to None\n    'subsample': None, #overridden by bagging_fraction\n    'reg_alpha': None, #overridden by lambda_l1\n    'reg_lambda': None, #overridden by lambda_l2\n    'min_sum_hessian_in_leaf': None, #overrides min_child_weight\n    'min_child_samples': None, #overridden by min_data_in_leaf\n    'colsample_bytree': None, #overridden by feature_fraction\n#        'min_child_samples': hp.quniform('min_child_samples', 20, 500, 5),\n    'min_child_weight': hp.loguniform('min_child_weight', -16, 5), #also aliases to min_sum_hessian\n#        'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),\n#        'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),\n#        'colsample_bytree': hp.uniform('colsample_by_tree', 0.6, 1.0),\n}\n#check if given parameter can be interpreted as a numerical value\ndef is_number(s):\n    if s is None:\n        return False\n    try:\n        float(s)\n        return True\n    except ValueError:\n        return False\n\n#convert given set of paramaters to integer values\n#this at least cuts the excess float decimals if they are there\ndef convert_int_params(names, params):\n    for int_type in names:\n        #sometimes the parameters can be choices between options or numerical values. like \"log2\" vs \"1-10\"\n        raw_val = params[int_type]\n        if is_number(raw_val):\n            params[int_type] = int(raw_val)\n    return params\n\nprint('Starting training...')\n# i call it objective_sklearn because the lgbm functions called use sklearn API\ndef objective_sklearn(params):\n    global X_train, X_test, y_train, y_test, lgb_eval\n    evals_result= {}\n    int_types = [\"num_leaves\", \"min_child_samples\", \"subsample_for_bin\", \"min_data_in_leaf\"]\n    params = convert_int_params(int_types, params)\n    all_params.append(params)\n\n    # Extract the boosting type\n    params['boosting_type'] = params['boosting_type']['boosting_type']\n    #    print(\"running with params:\"+str(params))\n\n    gbm = lgb.train(params,\n                lgb_train,\n                #num_boost_round=35,\n                valid_sets=lgb_eval,\n                evals_result=evals_result\n                #early_stopping_rounds=5\n    )\n    \n    print('Starting predicting...')\n    # predict\n    y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n    # eval\n    rmse = mean_squared_error(y_test, y_pred) ** 0.5\n    print('The rmse of prediction is:', rmse)\n    print('The RMSLE of prediction is:', RMSLE(y_pred.tolist(), y_test.tolist()))\n    result = {\"loss\": rmse, \"params\": params, 'status': hyperopt.STATUS_OK}\n    return result\n\nn_trials=100\ntrials = Trials()\n# Train parameters with hyperopt\n#best = fmin(fn=objective_sklearn,\n#            space=space,\n#            algo=tpe.suggest,\n#            max_evals=n_trials,\n#            trials=trials)\n\n# find the trial with lowest loss value. this is what we consider the best one\n#idx = np.argmin(trials.losses())\n\n#print(all_params[idx])","508d2438":"tomorrow_test = defaultdict(list)\n\nfor ags in LightGBMCovid19.AGS.unique():\n    agsRows = LightGBMCovid19[LightGBMCovid19.AGS == ags]\n    maxDaysRowIndex = agsRows.daysSinceFirstCase.idxmax()\n    nextKey = LightGBMCovid19NextKeys[maxDaysRowIndex]\n\n    maxDaysRow = LightGBMCovid19.loc[maxDaysRowIndex, : ]\n    tomorrow_test[\"AGS\"].append(maxDaysRow.AGS)\n    tomorrow_test[\"BEZ\"].append(maxDaysRow.BEZ)\n    tomorrow_test[\"PopulationDensity\"].append(maxDaysRow.PopulationDensity)\n    tomorrow_test[\"daysSinceFirstCase\"].append(maxDaysRow.daysSinceFirstCase+1)\n    tomorrow_test[\"Weekday\"].append(7 % (maxDaysRow.Weekday+1))\n    tomorrow_test[\"cases1DaysBefore\"].append(AnzahlFall[maxDaysRowIndex])\n    tomorrow_test[\"cases2DaysBefore\"].append(maxDaysRow.cases1DaysBefore)\n    tomorrow_test[\"cases3DaysBefore\"].append(maxDaysRow.cases2DaysBefore)\n    tomorrow_test[\"cases4DaysBefore\"].append(maxDaysRow.cases3DaysBefore)\n    tomorrow_test[\"cases5DaysBefore\"].append(maxDaysRow.cases4DaysBefore)\n    tomorrow_test[\"cases6DaysBefore\"].append(maxDaysRow.cases5DaysBefore)\n    tomorrow_test[\"cases7DaysBefore\"].append(maxDaysRow.cases6DaysBefore)\n    tomorrow_test['EWZ'].append(maxDaysRow.EWZ)\n    tomorrow_test['Shape_Area'].append(maxDaysRow.Shape_Area)\n\n    aWeekAgowRow = social_distancing[social_distancing.key == nextKey]\n    if len(aWeekAgowRow) > 0:\n        tomorrow_test['hystreet_score'].append(aWeekAgowRow.hystreet_score.iloc[0])\n        tomorrow_test['zug_score'].append(aWeekAgowRow.zug_score.iloc[0])\n        tomorrow_test['nationalExpress_score'].append(aWeekAgowRow.nationalExpress_score.iloc[0])\n        tomorrow_test['regional_score'].append(aWeekAgowRow.regional_score.iloc[0])\n        tomorrow_test['suburban_score'].append(aWeekAgowRow.suburban_score.iloc[0])\n        tomorrow_test['national_score'].append(aWeekAgowRow.national_score.iloc[0])\n        tomorrow_test['bus_score'].append(aWeekAgowRow.bus_score.iloc[0])\n        tomorrow_test['tomtom_score'].append(aWeekAgowRow.tomtom_score.iloc[0])\n        tomorrow_test['webcam_score'].append(aWeekAgowRow.webcam_score.iloc[0])\n        tomorrow_test['bike_score'].append(aWeekAgowRow.bike_score.iloc[0])\n        tomorrow_test['gmap_score'].append(aWeekAgowRow.gmap_score.iloc[0])\n        tomorrow_test['lemgoDigital'].append(aWeekAgowRow.lemgoDigital.iloc[0])\n        tomorrow_test['airquality_score'].append(aWeekAgowRow.airquality_score.iloc[0])\n    else:\n        tomorrow_test['hystreet_score'].append(math.nan)\n        tomorrow_test['zug_score'].append(math.nan)\n        tomorrow_test['nationalExpress_score'].append(math.nan)\n        tomorrow_test['regional_score'].append(math.nan)\n        tomorrow_test['suburban_score'].append(math.nan)\n        tomorrow_test['national_score'].append(math.nan)\n        tomorrow_test['bus_score'].append(math.nan)\n        tomorrow_test['tomtom_score'].append(math.nan)\n        tomorrow_test['webcam_score'].append(math.nan)\n        tomorrow_test['bike_score'].append(math.nan)\n        tomorrow_test['gmap_score'].append(math.nan)\n        tomorrow_test['lemgoDigital'].append(math.nan)\n        tomorrow_test['airquality_score'].append(math.nan)\n\n        \n#for key, value in tomorrow_test.items():\n#    print(key+\" \"+str(len(value)))\n\ntomorrow_test_df = pd.DataFrame.from_dict(tomorrow_test)\ntomorrow_test_df.AGS = tomorrow_test_df.AGS.astype('category')\ntomorrow_test_df.BEZ = tomorrow_test_df.BEZ.astype('category')\ntomorrow_test_df.Weekday = tomorrow_test_df.Weekday.astype('category')\n\ntomorrow_pred = gbm.predict(tomorrow_test_df, num_iteration=gbm.best_iteration)\n#tomorrow_test_df['SummeFall'] = pd.Series(np.expm1(tomorrow_pred))\ntomorrow_test_df['AnzahlFall'] = pd.Series(tomorrow_pred)\ntomorrow_test_df","0b5571fc":"tomorrow_test_df = tomorrow_test_df.merge(landkreise, on=\"AGS\")","11c3810b":"from folium.plugins import HeatMap\n\nfoliumMap = folium.Map(location=[51.0,9.0], tiles='openstreetmap', zoom_start=5)\nHeatMap(data=tomorrow_test_df[['Y', 'X', 'AnzahlFall']].groupby(['Y', 'X']).sum().reset_index().values.tolist(), radius=25, max_zoom=13).add_to(foliumMap)\n\nfoliumMap","74f826ce":"foliumMap = folium.Map(location=[51.0,9.0], tiles='openstreetmap', zoom_start=5)\n\n# Add points to the map\nfor idx, row in tomorrow_test_df.iterrows():\n    Marker([row['Y'], row['X']], popup=row['GEN']+'\\n <strong>Predicted Cases: '+str(round(row['AnzahlFall']))+'<\/strong> Cases in the last seven days: <ol><li>'+str(row.cases1DaysBefore)+'<\/li><li>'+str(row.cases2DaysBefore)+'<\/li><li>'+str(row.cases3DaysBefore)+'<\/li><li>'+str(row.cases4DaysBefore)+'<\/li><li>'+str(row.cases5DaysBefore)+'<\/li><li>'+str(row.cases6DaysBefore)+'<\/li><li>'+str(row.cases7DaysBefore)+'<\/li><\/ol>').add_to(foliumMap)\nfoliumMap","cc8d37b1":"from IPython.core.display import HTML\n# does not work\nHTML(\"<style type='text\/css'>@import 'https:\/\/cdn.jsdelivr.net\/npm\/leaflet-timedimension@1.1.1\/dist\/leaflet.timedimension.control.min.css'; <\/style>\")","d3db46e5":"\nfrom folium.plugins import HeatMapWithTime\n\nheatMap = folium.Map(location=[51.0,9.0], tiles='openstreetmap', zoom_start=5)\n\nlistOfHeatmaps = []\nfor attribute in [\"cases1DaysBefore\", \"cases2DaysBefore\", \"cases3DaysBefore\", \"cases4DaysBefore\", \"cases5DaysBefore\", \"cases6DaysBefore\", \"cases7DaysBefore\", \"AnzahlFall\"]:\n    listOfHeatmaps.append(tomorrow_test_df[['Y', 'X', attribute]].groupby(['Y', 'X']).sum().reset_index().values.tolist())\n\n    \nflatten = lambda l: [item for sublist in l for item in sublist]\n\nmaxCases = max(map(lambda item : item[2], flatten(listOfHeatmaps)))\n\nlistOfHeapMapsNormalized = []\nfor listOfHeatmap in listOfHeatmaps:\n    listOfHeapMapsNormalized.append(list(map(lambda item : [item[0], item[1], item[2]\/maxCases], listOfHeatmap)))\n\nHeatMapWithTime(data=listOfHeapMapsNormalized, radius=25).add_to(heatMap)\n\nheatMap","787f9d9a":"foliumMap = folium.Map(location=[51.0,9.0], tiles='openstreetmap', zoom_start=5)\ntomorrow_test_df[\"Cases-7-days-by-EWZ\"] = (tomorrow_test_df[\"cases1DaysBefore\"]+tomorrow_test_df[\"cases2DaysBefore\"]+tomorrow_test_df[\"cases3DaysBefore\"]+tomorrow_test_df[\"cases4DaysBefore\"]+tomorrow_test_df[\"cases5DaysBefore\"]+tomorrow_test_df[\"cases6DaysBefore\"]+tomorrow_test_df[\"AnzahlFall\"])\/(tomorrow_test_df[\"EWZ_y\"]\/100000)\n\nmorethan45 = tomorrow_test_df[tomorrow_test_df[\"Cases-7-days-by-EWZ\"] >= 40]\n\n# Add points to the map\nfor idx, row in morethan45.iterrows():\n    Marker([row['Y'], row['X']], popup=row['GEN']+'\\n <strong>Total Cases with prediction for 7 days by 100.000 capita: '+str(round(row['Cases-7-days-by-EWZ']))+'<\/strong>\\n Predicted cases '+str(round(row['AnzahlFall']))+' Cases in the last seven days: <ol><li>'+str(row.cases1DaysBefore)+'<\/li><li>'+str(row.cases2DaysBefore)+'<\/li><li>'+str(row.cases3DaysBefore)+'<\/li><li>'+str(row.cases4DaysBefore)+'<\/li><li>'+str(row.cases5DaysBefore)+'<\/li><li>'+str(row.cases6DaysBefore)+'<\/li><li>'+str(row.cases7DaysBefore)+'<\/li><\/ol>').add_to(foliumMap)\nfoliumMap","06780585":"# Compare cases by big cities","a8bf097a":"# Show some data from Hamburg as an example\n\nHamburg has the [Amtlicher Gemeindeschl\u00fcssel](https:\/\/de.wikipedia.org\/wiki\/Amtlicher_Gemeindeschl%C3%BCssel) \"02000\". Let's extract this data and do some data analysis.","18dbdf92":"# Show the markers with information to see","632a8d5a":"# Show the heatmap as a time series","3a3a1ca7":"# German description of the everyone counts data fields\n\n* \"gmap_score\":\"Menschen an Haltestellen des \u00d6PNV\",\n* \"gmap_supermarket_score\":\"Besucher in Superm\u00e4rkten\",\n* \"hystreet_score\":\"Fu\u00dfg\u00e4nger in Innenst\u00e4dten\",\n* \"zug_score\":\"DB Z\u00fcge\",\n* \"bike_score\":\"Fahrradfahrer\",\n* \"bus_score\":\"\u00d6PV Busse\",\n* \"national_score\":\"\u00d6PV IC-Z\u00fcge\",\n* \"suburban_score\":\"\u00d6PV Nahverkehr\",\n* \"regional_score\":\"\u00d6PV Regionalz\u00fcge\",\n* \"nationalExpress_score\":\"\u00d6PV ICE-Z\u00fcge\",\n* \"webcam_score\":\"Fu\u00dfg\u00e4nger auf \u00f6ffentlichen Webcams\",\n* \"tomtom_score\":\"Autoverkehr\"\n* \"airquality_score\":\"Luftqualit\u00e4t\"\n\nDetails: https:\/\/www.everyonecounts.de\/","c264927c":"Load data from this excellent website: https:\/\/www.everyonecounts.de\/","eb382e0d":"# Load districts (Landkreise)","aeed716e":"# Show a Heatmap for the new hotspots","a2767ed7":"It is possible to see that after a slow ramp up phase nearly in all districts the cases are growing exponentially. It depends more or less on luck if when it starts in a certain district. Especially super spreader (https:\/\/www.spiegel.de\/wissenschaft\/medizin\/coronavirus-die-gefahr-der-superspreader-a-ed6e694e-8691-4d14-a299-6062b94dd2f4) are giving districts a kick start in cases.","975ed6df":"# Closer look into hamburg","cd182236":"# Where are tomorrow more than 40 cases per 100.000 EWZ","3e988e69":"# People in the city of Hamburg\n\nThe following graph shows how the amount of people decreased starting since 2020-03-12. Big efforts for social distancing have been published on 2020-03-16. https:\/\/www.bundeskanzlerin.de\/bkin-de\/aktuelles\/vereinbarung-zwischen-der-bundesregierung-und-den-regierungschefinnen-und-regierungschefs-der-bundeslaender-angesichts-der-corona-epidemie-in-deutschland-1730934 ","cbcc305c":"# RKI Data\nLets get the data from RKI COVID-19 dashboard (http:\/\/corona.rki.de\/) for all districts (Landkreise) and times.","d2067549":"# Merge districts and everyonecounts data with COVID-19 data","7c5a2477":"# Hyperopt for parameter optimization","3bec2ea0":"# Comapare cases between cities and countryside","c5ad4d33":"# Create the necessary data for a prediction","ce3103c1":"# Histogramm of inhabitans in german districts","1abcdfc3":"# Predicting cases for german districts (Landkreise)\nIn the following section we are creating some benchmarks and in the end we are training a LighGBM model for predicting the COVID-19 cases for the next day, this should be tomorrow.","0d25df1c":"# Correlate the people in the city with the corona cases 7 days afterwards for the biggest cities in germany","baf44b6b":"# Germany COVID-19 Case prediction on district (Landkreis) basis\n\nThis notebook just tries to find more insights in germany between social distancing and cases. It is not meant for submitting a result for the global challenge because it is using very detailed german data.\n\nIt loads data from the website https:\/\/www.everyonecounts.de\/ and tries to correlate this data with the COVID-19 cases 7 days later.","4a85d236":"# Show a plot how COVID-19 infections developed in the different districts","5ed2a891":"# Preprocess data from everyonecounts\n\nFormat data so we can use it in a panda data frame. The everycounts data does not always have the same values for a certain reason. The code below makes sure that every region has the all the attributes. If they are not available an NA value is used. "}}