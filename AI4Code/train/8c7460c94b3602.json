{"cell_type":{"03f5c0be":"code","99e39379":"code","157b433a":"code","97f5734e":"code","15d94ffe":"code","9add896d":"code","01d5e934":"code","072edf64":"code","7c05a6ee":"code","4582d891":"code","e3057692":"code","106754e0":"code","fbbee75c":"code","753253c5":"code","d7c75b45":"code","51db56a6":"code","63327700":"code","4865d9ad":"code","46eea5d3":"code","380cfb68":"code","9dee9e89":"code","b80f3395":"code","25617af2":"code","7b01982e":"code","4740dc86":"code","5876e99b":"code","22ab55af":"code","db86dd0d":"code","3c5988b0":"code","24307b2c":"code","6966cdc2":"code","7291c5be":"code","3f1f0587":"code","a86b05b2":"code","165e6f33":"code","60f36603":"code","bee24965":"code","e7e8047d":"code","96f39a80":"code","e941a330":"markdown","51969e04":"markdown","84be23b7":"markdown","3fec0062":"markdown","13f6a0e6":"markdown","eef3113d":"markdown","d96b0b9f":"markdown","157dcfee":"markdown","780189e9":"markdown","a5b0ee40":"markdown","15976be7":"markdown","7024b064":"markdown","1ba03c2c":"markdown","60cbd07c":"markdown","6c0562b7":"markdown","4039213e":"markdown","f8832865":"markdown","352a3af7":"markdown","4b31eeea":"markdown","fe850b95":"markdown","977beb18":"markdown","e9681eef":"markdown","914643c8":"markdown","95bfb676":"markdown","1741f3c7":"markdown","7b7550ce":"markdown","5fc9e81c":"markdown","8b7a7956":"markdown","b044b5fd":"markdown","13dc7517":"markdown","a58933f6":"markdown"},"source":{"03f5c0be":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sns\n%matplotlib inline\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","99e39379":"# ref: https:\/\/www.kaggle.com\/mesadowski\/moneyball-golf-scikit-learn-and-tf-estimator-api\n\n# read data\ndf = pd.read_csv('..\/input\/pga-tour-20102018-data\/PGA_Data_Historical.csv')\n\n# unstack\ndf = df.set_index(['Player Name', 'Variable', 'Season'])['Value'].unstack('Variable').reset_index()\n\n# drop non-numeric features\nkeep_columns = [\n    'Player Name',\n    'Season',\n    'Total Money (Official and Unofficial) - (MONEY)', # \u5e74\u9593\u7372\u5f97\u8cde\u91d1 (\u30c9\u30eb)\n    'Driving Distance - (AVG.)', # \u30c9\u30e9\u30a4\u30d0\u30fc\u306e\u5e73\u5747\u98db\u8ddd\u96e2 (\u30e4\u30fc\u30c9)\n    'Driving Accuracy Percentage - (%)', # \u30c9\u30e9\u30a4\u30d0\u30fc\u306e\u30d5\u30a7\u30a2\u30a6\u30a7\u30a4\u30ad\u30fc\u30d7\u7387\n    'Total Distance Efficiency - (AVERAGE DISTANCE (YARDS))', # \u30c9\u30e9\u30a4\u30d0\u30fc\u306e\u3001\u30dc\u30fc\u30eb \u30b9\u30d4\u30fc\u30c9\u306b\u5bfe\u3059\u308b\u98db\u8ddd\u96e2 (\u9ad8\u3044\u307b\u3046\u304c\u52b9\u7387\u3088\u304f\u98db\u3093\u3067\u3044\u308b)\n    'Average Distance to Hole After Tee Shot - (AVG)', # \u5e73\u5747\u306e\u5e73\u5730\u30b7\u30e7\u30c3\u30c8\u98db\u8ddd\u96e2\n    'Ball Speed - (AVG.)', # \u5e73\u5747\u30dc\u30fc\u30eb\u901f\u5ea6\n    'Scrambling from the Sand - (%)', # \u30d0\u30f3\u30ab\u30fc\u304b\u3089\u306e\u30b9\u30af\u30e9\u30f3\u30d6\u30eb\u7387 (\u30d1\u30fc\u30aa\u30f3\u51fa\u6765\u306a\u304b\u3063\u305f\u30db\u30fc\u30eb\u3067\u3001\u30d1\u30fc\u4ee5\u4e0a\u3067\u3042\u304c\u308b\u3053\u3068)\n    'Scrambling from the Fringe - (%)', # \u30b0\u30ea\u30fc\u30f3\u306e\u30d5\u30ea\u30f3\u30b8\u304b\u3089\u306e\u30b9\u30af\u30e9\u30f3\u30d6\u30eb\u7387\n    'Scrambling from the Rough - (%)', # \u30e9\u30d5\u304b\u3089\u306e\u30b9\u30af\u30e9\u30f3\u30d6\u30eb\u7387\n    '3-Putt Avoidance - (%)', # 3 \u30d1\u30c3\u30c8\u3057\u305f\u30db\u30fc\u30eb\u306e\u5272\u5408\n    'Birdie or Better Conversion Percentage - (%)' # \u30d0\u30fc\u30c7\u30a3\u30fc\u3088\u308a\u826f\u3044\u6210\u7e3e\u3067\u6319\u304c\u308b\u30db\u30fc\u30eb\u306e\u5272\u5408\n]\ndf = df[keep_columns].dropna()\n\n# rename the columns to something shorter\ndf.rename(columns = {'Total Money (Official and Unofficial) - (MONEY)':'Money'}, inplace = True)\ndf.rename(columns = {'3-Putt Avoidance - (%)':'ThreePuttRate'}, inplace = True)\ndf.rename(columns = {'Average Distance to Hole After Tee Shot - (AVG)':'NonDrivingDistance'}, inplace = True)\ndf.rename(columns = {'Total Distance Efficiency - (AVERAGE DISTANCE (YARDS))':'DistanceEfficiency'}, inplace=True)\ndf.rename(columns = {'Ball Speed - (AVG.)':'BallSpeed'}, inplace=True)\ndf.rename(columns = {'Driving Distance - (AVG.)':'DrivingDistance'}, inplace = True)\ndf.rename(columns = {'Driving Accuracy Percentage - (%)':'DrivingAccuracy'}, inplace=True)\ndf.rename(columns = {'Scrambling from the Sand - (%)':'ScramblingSand'}, inplace = True)\ndf.rename(columns = {'Scrambling from the Fringe - (%)':'ScramblingFringe'}, inplace=True)\ndf.rename(columns = {'Scrambling from the Rough - (%)':'ScramblingRough'}, inplace=True)\ndf.rename(columns = {'Birdie or Better Conversion Percentage - (%)':'BirdieConversion'}, inplace=True)\n\n# remove $ and commas from Money\ndf['Money']= df['Money'].str.replace('$','')\ndf['Money']= df['Money'].str.replace(',','')\n\n# make all variables into number\nfor col in  df.columns[2:]:\n   df[col] = df[col].astype(float)","157b433a":"np.random.seed(0)\nindex = np.random.randint(df.shape[0], size=10)\ndf.iloc[index,:]","97f5734e":"df.mean()","15d94ffe":"df.groupby(\"Season\").mean()","9add896d":"df.loc[df.groupby(\"Season\")[\"Money\"].idxmax()]","01d5e934":"def plot_corr(corr, figsize=(11,15)):\n    mask = np.zeros_like(corr, dtype=np.bool)\n    mask[np.triu_indices_from(mask)]= True\n\n    f, ax = plt.subplots(figsize=figsize)\n    heatmap = sns.heatmap(corr, \n                          square = True,\n                          mask = mask,\n                          linewidths = .5,\n                          cmap = 'coolwarm',\n                          cbar_kws = {'shrink': .4, \n                                    'ticks' : [-1, -.5, 0, 0.5, 1]},\n                          vmin = -1, \n                          vmax = 1,\n                          annot = True,\n                          annot_kws = {\"size\": 12})\n\n    ax.set_yticklabels(corr.columns, rotation = 0)\n    ax.set_xticklabels(corr.columns)\n    sns.set_style({'xtick.bottom': True}, {'ytick.left': True})\n    \ncorr = df[df.columns[2:]].corr()\nplot_corr(corr)","072edf64":"def cond(corr):\n    eig_values = np.linalg.eigvals(corr)\n    return eig_values.max() \/ (eig_values.min() + 1e-10)\n\ndef validate_cond(cond):\n    # https:\/\/www3.nd.edu\/~rwilliam\/stats2\/l11.pdf\n    if cond > 30:\n        return 'danger'\n    elif cond > 15:\n        return 'warning'\n    else:\n        return 'good'\n    \nprint('\u76f8\u95a2\u884c\u5217\u306e\u6761\u4ef6\u6570:', cond(corr))\nprint('\u4fe1\u53f7:', validate_cond(cond(corr)))","7c05a6ee":"df[\"Money\"].describe()","4582d891":"fig, axes = plt.subplots(figsize=(10,4), ncols=2)\ndf[\"Money\"].plot.hist(bins=100, ax=axes[0])\ndf[\"Money\"].apply(np.log1p).plot.hist(bins=100, ax=axes[1])\naxes[0].set_title('Money')\naxes[1].set_title('log(Money)')\nplt.show()","e3057692":"from sklearn.model_selection import train_test_split\n\nX = df[df.columns[3:]]\ny = df[\"Money\"]\n\n# \u7279\u5fb4\u91cf\u9078\u629e\u7528\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\nrate_for_feature_selection = 0.1\nX_fs, X, y_fs, y = train_test_split(X, y, train_size=rate_for_feature_selection, random_state=2018)","106754e0":"X_fs.shape, y_fs.shape","fbbee75c":"from lightgbm import LGBMRegressor\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.metrics import mean_squared_error\n\nrfecv = RFECV(estimator=LGBMRegressor(),\n              cv=3,\n              scoring='neg_mean_squared_error')\nrfecv.fit(X_fs, np.log1p(y_fs))\n\nprint(\"Optimal number of features : %d\" % rfecv.n_features_)\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score (nb of correct classifications)\")\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.show()","753253c5":"from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n\nsfs = SFS(LGBMRegressor(),\n          k_features=4,\n          forward=True,\n          floating=False,\n          scoring='neg_mean_squared_error',\n          cv=3)\n\nsfs.fit(X_fs, y_fs)\nselected_features = X_fs.columns[list(sfs.k_feature_idx_)]\n\nprint(selected_features)","d7c75b45":"corr = df[selected_features].corr()\nplot_corr(corr, figsize=(6,4))\nprint('\u76f8\u95a2\u884c\u5217\u306e\u6761\u4ef6\u6570:', cond(corr))\nprint('\u4fe1\u53f7:', validate_cond(cond(corr)))","51db56a6":"# > rate_for_feature_selection = 0.1\n# > X_fs, X, y_fs, y = train_test_split(X, y, train_size=rate_for_feature_selection, random_state=2018)\n\nX = X[selected_features]\ny = y","63327700":"X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=2018)\n\nprint('X_train', X_train.shape)\nprint('X_test', X_test.shape)","4865d9ad":"!pip install pygam","46eea5d3":"from pygam import s, LinearGAM\n\nlineargam = LinearGAM(s(0) + s(1) + s(2) + s(3)).fit(X_train, y_train)\nlineargam.summary()","380cfb68":"def plot_splines(gam):\n    fig, axes = plt.subplots(ncols=4, figsize=(14, 5))\n    axes = np.array(axes).flatten()\n    for i, (ax, title, p_value) in enumerate(zip(axes, X_train.columns, gam.statistics_['p_values'])):\n        XX = gam.generate_X_grid(term=i)\n        ax.plot(XX[:, i], gam.partial_dependence(term=i, X=XX))\n        ax.plot(XX[:, i], gam.partial_dependence(term=i, X=XX, width=.95)[1], c='r', ls='--')\n        ax.axhline(0, c='#cccccc')\n        ax.set_title(\"{0:} (p={1:.2})\".format(title, p_value))\n        ax.set_yticks([])\n        \nplot_splines(lineargam)","9dee9e89":"y.plot.hist(bins=100)","b80f3395":"from pygam import GammaGAM\n\ngammagam = GammaGAM(s(0) + s(1) + s(2) + s(3)).fit(X_train, y_train)\ngammagam.summary()","25617af2":"print('GAM (gaussian)', lineargam.statistics_['AIC'])\nprint('GAM (gamma)   ', gammagam.statistics_['AIC'])","7b01982e":"print('GAM (gaussian)', lineargam.statistics_['GCV'])\nprint('GAM (gamma)   ', gammagam.statistics_['GCV'])","4740dc86":"def msle(y_pred, y_test):\n    return np.sum((np.log1p(y_pred) - np.log1p(y_test))**2) \/ len(y_test)\n\ndef gam_msle(gamobj):\n    y_pred = gamobj.predict(X_test)\n    return msle(y_pred, y_test)\n\nprint('LinearGAM', gam_msle(lineargam))\nprint('GammaGAM',  gam_msle(gammagam))","5876e99b":"from pygam import l, GammaGAM\n\ngammaglm = GammaGAM(l(0) + l(1) + l(2) + l(3)).fit(X_train, y_train)\ngammaglm.summary()","22ab55af":"plot_splines(gammaglm)","db86dd0d":"print('GAM (dist=gaussian, link=identity)')\nplot_splines(lineargam)\nplt.show()\n\nprint('GAM (dist=gamma, link=log)')\nplot_splines(gammagam)\nplt.show()\n\nprint('GLM (dist=gamma, link=log)')\nplot_splines(gammaglm)\nplt.show()","3c5988b0":"print('GAM (gaussian)', lineargam.statistics_['AIC'])\nprint('GAM (gamma)   ', gammagam.statistics_['AIC'])\nprint('GLM (gamma)   ', gammaglm.statistics_['AIC'])","24307b2c":"print('GAM (gaussian)', lineargam.statistics_['GCV'])\nprint('GAM (gamma)   ', gammagam.statistics_['GCV'])\nprint('GLM (gamma)   ', gammaglm.statistics_['GCV'])","6966cdc2":"print('GAM (gaussian)', gam_msle(lineargam))\nprint('GAM (gamma)   ', gam_msle(gammagam))\nprint('GLM (gamma)   ', gam_msle(gammaglm))","7291c5be":"!pip install interpret","3f1f0587":"from interpret import show\nfrom interpret.data import Marginal\n\n# \u30ed\u30b0\u5909\u63db\u3092\u4f7f\u3046\ny_train_log, y_test_log = np.log1p(y_train), np.log1p(y_test)\n\nmarginal = Marginal().explain_data(X_test, y_test_log, name='test data')\nshow(marginal)","a86b05b2":"from interpret.glassbox import ExplainableBoostingRegressor, LinearRegression, RegressionTree\n\nebm = ExplainableBoostingRegressor(random_state=0, scoring=\"mean_squared_error\")\nebm.fit(X_train, y_train_log)","165e6f33":"ebm_global = ebm.explain_global(name='EBM')\nshow(ebm_global)","60f36603":"ebm_local = ebm.explain_local(X_test[:5], y_test_log[:5], name='EBM')\nshow(ebm_local)","bee24965":"from interpret import show\nfrom interpret.perf import RegressionPerf\n\nebm_perf = RegressionPerf(ebm.predict).explain_perf(X_test, y_test_log, name='EBM')\nshow(ebm_perf)","e7e8047d":"from sklearn.metrics import mean_squared_error\nfrom interpret.glassbox import LinearRegression, RegressionTree\nfrom lightgbm import LGBMRegressor\n\n# Train some models\n# -----------------------\nlr = LinearRegression(random_state=0)\nlr.fit(X_train, y_train_log)\n\nrt = RegressionTree(random_state=0)\nrt.fit(X_train, y_train_log)\n\nlgb = LGBMRegressor()\nlgb.fit(X_train, y_train_log)\n\n\n# Evaluate the models\n# -----------------------\n\n# model, name, is_log_pred_model\nmodels = [\n    (lineargam, \"GAM (Normal)\",             False),\n    (gammagam,  \"GAM (Gamma)\",              False),\n    (gammaglm,  \"GLM (Gamma)\",              False),\n    (ebm,       \"InterpretML (EBM = GA2M)\", True),\n    (lr,        \"InterpretML (LR)\",         True),\n    (rt,        \"InterpretML (RT)\",         True),\n    (lgb,       \"LightGBM\",                 True)\n]\n\nresult = pd.DataFrame()\nfor (model, name, is_log_pred_model) in models:\n    if is_log_pred_model:\n        y_pred_log = model.predict(X_test)\n        error = mean_squared_error(y_test_log, y_pred_log)\n    else:\n        y_pred = model.predict(X_test)\n        error = msle(y_test, y_pred)\n        \n    series = pd.Series()\n    series[\"model\"] = name\n    series[\"MSLE\"] = error\n    result = result.append(series, ignore_index=True)","96f39a80":"result.sort_values('MSLE')","e941a330":"#### MSLE","51969e04":"\u5b66\u7fd2\u3057\u305f\u30e2\u30c7\u30eb\u3092\u89e3\u91c8\u3059\u308b\u306b\u306f\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306b `explain_global` \u3092\u7528\u3044\u308b\u3002\n\u30d1\u30cd\u30eb\u3092\u64cd\u4f5c\u3059\u308c\u3070\u3001\u5404\u7279\u5fb4\u91cf\u306b\u5bfe\u3057\u3066\u3001EBM \u304c\u30b9\u30b3\u30a2\u3078\u306e\u5bc4\u4e0e\u3092\u30ce\u30f3\u30d1\u30e9\u30e1\u30c8\u30ea\u30c3\u30af \u30e2\u30c7\u30eb\u3067\u63a8\u5b9a\u3057\u3066\u3044\u308b\u3053\u3068\u304c\u78ba\u8a8d\u3067\u304d\u308b\u3002","84be23b7":"## \u7d71\u8a08\u91cf\u306e\u51fa\u529b","3fec0062":"\u3055\u3089\u306b\u30c7\u30fc\u30bf\u70b9\u3078\u306e\u4e88\u6e2c\u306e\u8aac\u660e\u3082\u884c\u3046\u3053\u3068\u304c\u51fa\u6765\u308b\u3002\n\u6700\u521d\u306e 5 \u3064\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306b\u5bfe\u3057\u3066\u4e88\u6e2c\u3092\u884c\u3044\u3001\u305d\u306e\u8aac\u660e\u3092\u3059\u308b\u306b\u306f\u6b21\u306e\u30b3\u30fc\u30c9\u3092\u5b9f\u884c\u3059\u308b\u3002","13f6a0e6":"\u3068\u3053\u308d\u3067\u3001\u4ee5\u4e0b\u306e\u30d7\u30ed\u30c3\u30c8\u304b\u3089\u3082\u308f\u304b\u308b\u3088\u3046\u306b\u3001\u76ee\u7684\u5909\u6570\u306f\u6b63\u898f\u5206\u5e03\u3057\u3066\u3044\u306a\u3044\u3002","eef3113d":"SVR \u3067\u306e\u518d\u5e30\u7684\u7279\u5fb4\u91cf\u524a\u6e1b (recursive feature elimination; RFE) \u306e\u7d50\u679c","d96b0b9f":"## \u7279\u5fb4\u91cf\u3092\u9078\u629e","157dcfee":"## pyGAM \u306b\u3088\u308b GAM \u30e2\u30c7\u30eb","780189e9":"## \u5404\u5e74\u5ea6\u306e\u6700\u5927\u8cde\u91d1\u30d7\u30ec\u30a4\u30e4\u30fc","a5b0ee40":"\u6027\u80fd\u306e\u6bd4\u8f03\n\n- AIC: \u8d64\u6c60\u60c5\u5831\u91cf\u57fa\u6e96\n- GCV: \u4e00\u822c\u5316\u30af\u30ed\u30b9\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3 (jstage.jst.go.jp\/article\/ciqs2001\/tokusi\/0\/tokusi_0_JP06\/_pdf \u306b\u3088\u308b\u3068\u3001GAM \u306e\u6bd4\u8f03\u306e\u89b3\u70b9\u3067\u306f\u3001AIC \u3088\u308a\u5b89\u5b9a\u3057\u305f\u6307\u6a19\u3068\u306a\u308b)\n- MSLE: \u5e73\u5747\u5bfe\u6570\u4e8c\u4e57\u8aa4\u5dee","15976be7":"## \u7cbe\u5ea6\u306e\u6bd4\u8f03\n\n\u4ee5\u4e0b\u306e\u30e2\u30c7\u30eb\u306b\u5bfe\u3057\u3001\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3078\u306e MSLE (mean squared logarithmic error) \u3092\u5831\u544a\u3059\u308b\u3002\n(\u751f\u306e\u76ee\u7684\u5909\u6570\u3067\u5b66\u7fd2\u3057\u305f\u30e2\u30c7\u30eb\u3068\u3001\u5bfe\u6570\u5909\u63db\u5f8c\u306e\u76ee\u7684\u5909\u6570\u3067\u5b66\u7fd2\u3057\u305f\u30e2\u30c7\u30eb\u306e 2 \u7a2e\u985e\u304c\u5b58\u5728\u3057\u3066\u3044\u308b\u70b9\u306b\u6ce8\u610f)\n\n![image.png](attachment:image.png)\n\n- `pyGAM` \u306e GammaGAM, LinearGAM\n- `pyGAM` \u306e GammaGLM\n- `interpret` \u306e EBM (GA2M)\n- `interpret` \u306e Linear Regression\n- `interpret` \u306e Regression Tree\n- LightGBM","7024b064":"# GLM\/GAM \u306b\u3088\u308b\u5206\u6790","1ba03c2c":"\u9010\u6b21\u7279\u5fb4\u91cf\u9078\u629e (forward mode) \u3067 4 \u3064\u306e\u7279\u5fb4\u91cf\u3092\u9078\u629e\u3059\u308b","60cbd07c":"## pyGAM \u306b\u3088\u308b GLM \u30e2\u30c7\u30eb","6c0562b7":"\u6700\u5f8c\u306b MSE \u306e\u5206\u5e03\u3092\u8868\u793a\u3059\u308b\u3002\n\u6b8b\u5dee\u5206\u5e03\u304c\u6b63\u898f\u5206\u5e03\u306b\u8fd1\u3044\u3053\u3068\u304c\u78ba\u8a8d\u3067\u304d\u308b\u3002","4039213e":"#### AIC","f8832865":"\u8cde\u91d1\u3068\u6b63\u306e\u76f8\u95a2\u304c\u5f37\u3044\u3082\u306e\n\n- \u30c9\u30e9\u30a4\u30d0\u30fc\u306e\u8ddd\u96e2\n- \u98db\u8ddd\u96e2\u306e\u52b9\u7387\u6027 (\u30b9\u30d4\u30f3\u91cf)\n- \u30dc\u30fc\u30eb\u30b9\u30d4\u30fc\u30c9\n- \u30e9\u30d5\u304b\u3089\u306e\u30ea\u30ab\u30d0\u30ea\u306e\u4e0a\u624b\u3055\n- **\u30d0\u30fc\u30c7\u30a3\u3088\u308a\u3088\u3044\u6210\u7e3e\u3067\u30db\u30fc\u30eb\u3092\u7d42\u3048\u308b\u7387**\n\n\u8cde\u91d1\u3068\u8ca0\u306e\u76f8\u95a2\u304c\u8a8d\u3081\u3089\u308c\u308b\u3082\u306e\n\n- \u30c9\u30e9\u30a4\u30d0\u30fc\u4ee5\u5916\u306e\u30b7\u30e7\u30c3\u30c8\u98db\u8ddd\u96e2\n- 3 \u30d1\u30c3\u30c8\u3059\u308b\u30db\u30fc\u30eb\u306e\u5272\u5408","352a3af7":"#### GVC","4b31eeea":"# \u30c7\u30fc\u30bf\u306e\u524d\u51e6\u7406","fe850b95":"![](https:\/\/pga-tour-res.cloudinary.com\/image\/upload\/c_fill,f_auto,g_center,h_478,q_auto,w_850\/v1\/pgatour\/editorial\/2019\/08\/18\/JustinThomasTrophyClean-847-KK.jpg)\n\nJustin Thomas \u306f\u3053\u3093\u306a\u4eba\u3089\u3057\u3044\u3002\n\nhttps:\/\/www.pgatour.com\/news\/2019\/08\/18\/justin-thomas-shows-he-still-knows-how-to-win-bmw-championship-medinah-country-club-fedexcup.html","977beb18":"\u307e\u305a\u306f\u3001`interpret.data.Marginal` \u3092\u4f7f\u3063\u3066\u3001\u30c7\u30fc\u30bf\u306e\u69d8\u5b50 (i.e. \u5404\u7279\u5fb4\u91cf\u306e\u5206\u5e03) \u3092\u898b\u308b","e9681eef":"# InterpretML\n\nhttps:\/\/github.com\/microsoft\/interpret\n\n## \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb","914643c8":"\u826f\u597d\u306a\u306e\u3067\u3001\u3053\u3044\u3064\u306b\u6c7a\u5b9a","95bfb676":"## \u5404\u7279\u5fb4\u91cf\u540c\u58eb\u306e\u76f8\u95a2","1741f3c7":"\u8b66\u544a\uff1a\n\n```\nWARNING: Fitting splines and a linear function to a feature introduces a model identifiability problem which can cause p-values to appear significant when they are not.\n\nWARNING: p-values calculated in this manner behave correctly for un-penalized models or models with  known smoothing parameters, but when smoothing parameters have been estimated, the p-values are typically lower than they should be, meaning that the tests reject the null too readily.\n```\n\n- \u3042\u308b\u4e00\u3064\u306e\u30b9\u30d7\u30e9\u30a4\u30f3\/\u7dda\u5f62\u95a2\u6570\u3092\u3069\u308c\u304b\u4e00\u3064\u306e\u7279\u5fb4\u91cf\u306b\u5f53\u3066\u306f\u3081\u308b\u3068\u3001\u30e2\u30c7\u30eb\u8b58\u5225\u53ef\u80fd\u6027\u306e\u554f\u984c\u304c\u767a\u751f\u3057\u3066\u3057\u307e\u3044\u3001p \u5024\u304c\u6709\u610f\u3067\u306a\u3044\u5834\u5408\u306b\u3082\u6709\u610f\u3067\u3042\u308b\u3068\u3055\u308c\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002\n- \u7f70\u5247\u306a\u3057\u30e2\u30c7\u30eb\u3001\u307e\u305f\u306f\u3001\u5e73\u6ed1\u5316\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u304c\u65e2\u77e5\u3067\u3042\u308b\u30e2\u30c7\u30eb\u306b\u5bfe\u3057\u3066\u306f\u3001\u3053\u306e\u65b9\u6cd5 (\u7f70\u5247\u4ed8\u304d\u53cd\u5fa9\u518d\u91cd\u307f\u4ed8\u3051\u6700\u5c0f\u4e8c\u4e57\u6cd5, penalized iteratively reweighted least squares; PIRLS) \u3067\u8a08\u7b97\u3055\u308c\u305f p \u5024\u306f\u6b63\u3057\u304f\u52d5\u4f5c\u3057\u307e\u3059\u3002\u3057\u304b\u3057\u3001\u5e73\u6ed1\u5316\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u540c\u6642\u306b\u63a8\u5b9a\u3057\u3088\u3046\u3068\u3057\u305f\u5834\u5408\u3001p \u5024\u306f\u4f4e\u304f\u898b\u7a4d\u3082\u3089\u308c\u308b\u50be\u5411\u304c\u3042\u308a\u307e\u3059\u3002\u3064\u307e\u308a\u3001\u975e\u5e38\u306b\u305f\u3084\u3059\u304f\u5e30\u7121\u4eee\u8aac (\u3053\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u306f\u52b9\u679c\u304c\u306a\u3044\u8aac) \u3092\u68c4\u5374\u3057\u3084\u3059\u304f\u306a\u308a\u307e\u3059\u3002","7b7550ce":"\u306a\u306e\u3067\u3001\u30ac\u30f3\u30de\u306e\u5206\u5e03\u3082\u8003\u3048\u308b","5fc9e81c":"## GA2M\n\n\u3044\u3088\u3044\u3088 GA2M \u3092\u5229\u7528\u3057\u3066\u3001\u8aac\u660e\u53ef\u80fd\u306a\u30e2\u30c7\u30eb\u3092\u5b66\u7fd2\u3059\u308b\u3002","8b7a7956":"\u5404\u7279\u5fb4\u91cf\u306e\u4e88\u6e2c\u5024\u3078\u306e\u5bc4\u4e0e\u3092\u898b\u308b\u305f\u3081\u306b\u3001\u30b9\u30d7\u30e9\u30a4\u30f3\u95a2\u6570\u3092\u30d7\u30ed\u30c3\u30c8\u3059\u308b\u3002","b044b5fd":"### \u6700\u5f8c\u306b GAM \u3068 GLM \u306e\u6bd4\u8f03\u307e\u3068\u3081","13dc7517":"### \u9078\u629e\u3055\u308c\u305f\u7279\u5fb4\u91cf\u3067\u6761\u4ef6\u6570\u306e\u518d\u691c\u8a0e","a58933f6":"## pyGAM \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\n\nhttps:\/\/pygam.readthedocs.io\/en\/latest\/"}}