{"cell_type":{"6cf158a9":"code","be55128c":"code","fdd654ea":"code","da469533":"code","41915c53":"code","e8d148bc":"code","97c90cb0":"code","3be79b6b":"code","1adfefee":"code","308aaef7":"code","dcb9df54":"code","ca5a417d":"code","d5315a68":"code","9ed91ec4":"code","3d9f5c2f":"code","7fb01699":"code","86b24acf":"code","7ac5102a":"code","b4b57205":"code","780d5bf4":"code","c4e99b25":"code","b3ccedc5":"code","f3eddca0":"code","8f76dfa4":"code","0098df1b":"code","266ae544":"code","ba670369":"code","ec43e45c":"code","2c743f1b":"code","a9eebaad":"code","406d2249":"code","4094196f":"code","32f8f947":"code","824832e0":"code","678fd4ea":"code","5a272cb7":"code","76329ba5":"code","59c9c1b9":"markdown","061dd6b7":"markdown","905f2e75":"markdown","8bf5ad09":"markdown","6add7549":"markdown","f6256d1e":"markdown","06c9ece7":"markdown","dff6fa19":"markdown","948659b9":"markdown","9a55c1c8":"markdown"},"source":{"6cf158a9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","be55128c":"#importing data\ntrain=pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest=pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntrain.head()","fdd654ea":"#checking the data types of each columns\ntrain.dtypes","da469533":"#Describing the data to get more understanding on its stats.\ntrain.describe()","41915c53":"#to check both data tyoe and null values at once among other features\ntrain.info()","e8d148bc":"#imputing with 'most_frequent' strategy to deal with missing object as well as int values\nfrom sklearn.impute import SimpleImputer\n\nimpute=SimpleImputer(strategy='most_frequent')","97c90cb0":"imp_train=pd.DataFrame(impute.fit_transform(train))\nimp_train.columns=train.columns\nimp_train.head()\nimp_train.info()","3be79b6b":"imp_test=pd.DataFrame(impute.fit_transform(test))\nimp_test.columns=test.columns\nimp_test.info()\n\n#After imputing we can see that the data types of all columns have been converted to objects, we need to convert some of them back to\n#integer","1adfefee":"#First we will drop two columns: Name and Ticket as both of these are useless for making predictions and we dont want any noise.\ntrain_df=imp_train.drop(['Name','Ticket','PassengerId'], axis=1)\ntrain_df.head()","308aaef7":"test_df=imp_test.drop(['Name','Ticket','PassengerId'], axis=1)\ntest_df.head()\ntest_df.info()","dcb9df54":"#Filter out the columns with object data types\nobject_cols=[cols for cols in train_df.columns if train_df[cols].dtypes=='object']\nobject_cols","ca5a417d":"#onverting those columns back to integer type\ntrain_df['Age']=train_df['Age'].astype(int)\ntrain_df['Survived']=train_df['Survived'].astype(int)\ntrain_df['Pclass']=train_df['Pclass'].astype(int)\ntrain_df['SibSp']=train_df['SibSp'].astype(int)\ntrain_df['Parch']=train_df['Parch'].astype(int)\ntrain_df['Fare']=train_df['Fare'].astype(int)\n\ntest_df['Age']=test_df['Age'].astype(int)\ntest_df['Pclass']=test_df['Pclass'].astype(int)\ntest_df['SibSp']=test_df['SibSp'].astype(int)\ntest_df['Parch']=test_df['Parch'].astype(int)\ntest_df['Fare']=test_df['Fare'].astype(int)\n\nprint(train_df.info())\ntest_df.info()","d5315a68":"#Combining SibSp and Parch to form a single column Family and then drop the two columns\ntrain_df['Famliy']=train_df['SibSp'] + train_df['Parch']\ntest_df['Family']=test_df['SibSp'] + test_df['Parch']\n\nvalid_df=train_df['Survived']\ntrain_df=train_df.drop(['SibSp','Parch','Survived','Cabin'], axis=1)\ntest_df=test_df.drop(['SibSp','Parch','Cabin'], axis=1)\n\nprint(train_df.head())\nprint(valid_df.head())\nprint(test_df.head())\n\ntr_df=train_df.copy()\nte_df=test_df.copy()","9ed91ec4":"#For Dealing with categorical values\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n\nLabelEnc=LabelEncoder()\nOhe=OneHotEncoder(handle_unknown='ignore', sparse=False)\n\n\ntrain_df.head()","3d9f5c2f":"object_cols=[cols for cols in train_df.columns if train_df[cols].dtypes=='object']\nobject_cols","7fb01699":"#LabelEncoding the dataset\n\nlabel_train_df=train_df\nlabel_test_df=test_df\nfor cols in set(object_cols):\n    label_train_df[cols]=LabelEnc.fit_transform(train_df[cols])\n    label_test_df[cols]=LabelEnc.transform(test_df[cols])\n    \nprint(train_df.info())\nlabel_train_df.head()\n\nage_tr_df=label_train_df.copy()\nage_te_df=label_test_df.copy()","86b24acf":"#Importing various classification models\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nX_train,X_valid,y_train,y_valid=train_test_split(label_train_df,valid_df, test_size=0.2, random_state=1)\n\n#Logistic Regression\n\nlogreg=LogisticRegression()\n\nlogreg.fit(X_train,y_train)\n\ny_pred=logreg.predict(X_valid)\n\nprint(accuracy_score(y_valid,y_pred))","7ac5102a":"#RandomForestClassifier\nrfc=RandomForestClassifier(n_estimators=60, max_depth=5)\n\nrfc.fit(X_train,y_train)\n\nrfc_y_pred=rfc.predict(X_valid)\n\nprint(accuracy_score(y_valid,rfc_y_pred))","b4b57205":"#Support Vector Classifier\n\nsvc=SVC()\n\nsvc.fit(X_train,y_train)\n\nsvc_y_pred=svc.predict(X_valid)\n\nprint(accuracy_score(y_valid,svc_y_pred))","780d5bf4":"#Select the model with the highest accuracy i.e. Logistic Regression\npred=logreg.predict(label_test_df)","c4e99b25":"#Submit\n'''\nSubmission=pd.DataFrame({\"PassengerId\":test.PassengerId,\"Survived\":pred})\nSubmission.to_csv(\"Submission.csv\", index=False) '''","b3ccedc5":"#Now try out the same models with OneHotEncoding\n\no_tr_df=pd.DataFrame(Ohe.fit_transform(tr_df[object_cols]))\no_te_df=pd.DataFrame(Ohe.transform(te_df[object_cols]))\n\no_tr_df.index=tr_df.index\no_te_df.index=te_df.index\n\nnum_tr_df=tr_df.drop(object_cols, axis=1)\nnum_te_df=te_df.drop(object_cols,axis=1)\n\nfinal_tr=pd.concat([o_tr_df,num_tr_df], axis=1)\nfinal_te=pd.concat([o_te_df,num_te_df], axis=1)\n\n\nfinal_tr.head()","f3eddca0":"#Logistic\nX_train,X_valid,y_train,y_valid=train_test_split(final_tr,valid_df, test_size=0.2, random_state=1)\n\nlogreg.fit(X_train,y_train)\nlog_pred=logreg.predict(X_valid)\nprint(accuracy_score(y_valid,log_pred))","8f76dfa4":"#RandomFOrest\nrfc=RandomForestClassifier(n_estimators=100, max_depth=9)\nrfc.fit(X_train,y_train)\n\nrfc_pred=rfc.predict(X_valid)\n\nprint(accuracy_score(y_valid,rfc_pred))","0098df1b":"#SupportVectorMAchines\nsvc=SVC()\n\nsvc.fit(X_train,y_train)\n\nsvc_pred=svc.predict(X_valid)\n\nprint(accuracy_score(y_valid,svc_pred))","266ae544":"#Highest accuracy: RandomFOrest \n\nfinal_pred=rfc.predict(final_te)\nSubmission=pd.DataFrame({\"PassengerId\":test.PassengerId,\"Survived\":final_pred})\nSubmission.to_csv(\"Submission2.csv\", index=False)","ba670369":"age_tr_df.insert(6,'Survived',train['Survived'])\nage_tr_df['AgeBand']=pd.cut(age_tr_df['Age'],5)\nage_tr_df[['AgeBand','Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand',ascending=True)","ec43e45c":"\nfor dataset in [age_tr_df,age_te_df]:\n    dataset.loc[dataset['Age']<=16,'Age']=0\n    dataset.loc[(dataset['Age']>16) & (dataset['Age']<=32),'Age']=1\n    dataset.loc[(dataset['Age']>32) & (dataset['Age']<=48),'Age']=2\n    dataset.loc[(dataset['Age']>48) & (dataset['Age']<=64),'Age']=3\n    dataset.loc[(dataset['Age']>64) & (dataset['Age']<=80),'Age']=4\n    \nage_tr_df.head()","2c743f1b":"#age_tr_df=age_tr_df.drop(['AgeBand'], axis=1)\nage_tr_df.insert(5,'Family', age_tr_df['Famliy'])\nage_tr_df.drop(['Famliy'], axis=1,inplace=True)\nage_tr_df.head()","a9eebaad":"for dataset in [age_tr_df,age_te_df]:\n    dataset['FamilySize']=dataset['Family']+1\nage_tr_df[['FamilySize','Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='FamilySize', ascending=True)","406d2249":"age_tr_df.insert(1,'Name',train['Name'])\nage_te_df.insert(1,'Name',test['Name'])\nprint(age_tr_df.head())\nfor dataset in [age_tr_df,age_te_df]:\n    dataset['Title']=dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\npd.crosstab(age_tr_df['Title'], age_tr_df['Sex'])","4094196f":"for dataset in [age_tr_df, age_te_df]:\n    dataset['Title']=dataset['Title'].replace(['Capt','Col','Countess','Don','Dr','Jonkheer','Lady','Major','Rev','Sir'],'Rare')\n    dataset['Title']=dataset['Title'].replace(['Mlle','Ms'],'Miss')\n    dataset['Title']=dataset['Title'].replace('Mme','Mrs')\n    \nage_tr_df[['Title','Survived']].groupby(['Title'], as_index=False).mean()","32f8f947":"title_map={'Master':0, 'Miss':1, 'Mr':2, 'Mrs':3, 'Rare':4}\n\nfor dataset in [age_tr_df, age_te_df]:\n    dataset['Title']=dataset['Title'].map(title_map)\n    dataset['Title']=dataset['Title'].fillna(0)\n    \nage_tr_df.head()","824832e0":"\nage_te_df=age_te_df.drop(['Name'], axis=1)\nage_tr_df.head()\nage_te_df.head()","678fd4ea":"'''\nrand=RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\ny=age_tr_df['Survived']\nX=age_tr_df.drop(['Survived'], axis=1)\n\nX_train,X_valid,y_train,y_valid=train_test_split(X,y,test_size=0.2, random_state=1)\n\nrand.fit(X_train,y_train)\npredictions=rand.predict(X_valid)\n\nprint(accuracy_score(y_valid,predictions))\n\npredictions=rand.predict(age_te_df)\n\n\noutput = pd.DataFrame({'PassengerId': test.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission4.csv', index=False)\nprint(\"Your submission was successfully saved!\")    '''","5a272cb7":"from xgboost import XGBClassifier\n\nxg_model=XGBClassifier(random_state=0, n_estimators=100,learning_rate=0.9)\n\nxg_model.fit(X_train,y_train, early_stopping_rounds=10, eval_set=[(X_valid,y_valid)])\n\npred=xg_model.predict(X_valid)\n\nprint(accuracy_score(y_valid,pred))\n\n","76329ba5":"'''\npredictions=xg_model.predict(age_te_df)\n\noutput = pd.DataFrame({'PassengerId': test.PassengerId, 'Survived': predictions})\noutput.to_csv('xg_submission1.csv', index=False)\nprint(\"Your submission was successfully saved!\") \n'''","59c9c1b9":"# XGBClassifier on the resultant DataFrame. I achieved my Highest Accuracy from this model.","061dd6b7":"# **Not Satisfied with the result, Trying to analyse which age groups are most likely to survive**","905f2e75":"# Inserting Name column back in the dataframe to extract titles.\n\n** I worked on accuracy after trying out all the methods above and then i moved to this title extraction and grouping method. I deleted all those code just to make it short as the notebook was getting too complicated to understand at one go.**","8bf5ad09":"# Submit Predictions","6add7549":"# RandomForestClassifier on the resultant dataframe","f6256d1e":"# Grouping the titles","06c9ece7":"# Checking How the Family size affects the survival rate","dff6fa19":"# Dropping the name column after completing the process of title extraction","948659b9":"# Alloting a number to each title ( Manual Imputing).","9a55c1c8":"# Creation of Age groups and every group alloted a number. Significantly increases accuracy."}}