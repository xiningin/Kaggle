{"cell_type":{"876fa403":"code","55165183":"code","adcdc17b":"code","e952e844":"code","fd1775b3":"code","ceb0bafe":"code","d936a77b":"code","874ffff4":"code","84738fb3":"code","488f0cb5":"code","e25e61d5":"code","c14a3cc7":"code","f3600384":"code","018ff993":"code","5febd263":"code","1b4e5ec6":"code","e26e6f10":"code","60672b06":"code","f37df912":"code","3744fac3":"code","90a4c5ae":"code","f0215e43":"code","5c3166d5":"code","abe351dd":"code","a98edf28":"markdown","b6771a2d":"markdown","177a3e3f":"markdown","fbd1f9a4":"markdown","4a983b28":"markdown","f20a3f54":"markdown","75448970":"markdown","26751ab7":"markdown","adcfdc67":"markdown","d48fa538":"markdown","280358a4":"markdown","ea321cf8":"markdown","60a0b8c2":"markdown","2b05158d":"markdown"},"source":{"876fa403":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, KFold, cross_validate, GridSearchCV, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import metrics\nfrom sklearn.ensemble import RandomForestRegressor,RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import model_selection\nfrom sklearn import neighbors\nfrom sklearn import svm\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.ensemble import AdaBoostClassifier \nfrom sklearn.ensemble import BaggingClassifier \nfrom sklearn.ensemble import VotingClassifier, StackingClassifier\nfrom sklearn.metrics import classification_report\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ndf= pd.read_csv('..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')\n","55165183":"df.head()","adcdc17b":"df.info()","e952e844":"df.describe()","fd1775b3":"corrMatrix = df.corr()\nplt.figure(figsize=(15,10))\nsns.heatmap(corrMatrix, annot=True, cmap=\"coolwarm\")\nplt.show()","ceb0bafe":"# Target distribution\n\nsns.displot(df['quality'],kde = True);","d936a77b":"# Pairplot to show correlation \nplt.figure()\nsns.pairplot(df,hue = 'quality', palette = \"Set1\")\nplt.show()","874ffff4":"# Define features and target\nX = df.drop('quality', axis = 1)\ny = df['quality']\n\n#Train and Test splitting of data \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n\n#Applying Standard scaling to get optimized result\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\n","84738fb3":"lr = LogisticRegression(solver='liblinear')\nlr.fit(X_train, y_train)\ny_pred_lr = lr.predict(X_test)\n\n# Score\nlr_score = lr.score(X_test,y_test)\nprint(\"Logistic Regression Score = \", lr_score, \"\\n\")\n\n### Confusion Matrix\ncm = pd.crosstab(y_test, y_pred_lr, rownames=['Actual'], colnames=['Predicted'])\ncm","488f0cb5":"# Model training\nsvm = svm.SVC()\nsvm.fit(X_train, y_train)\ny_pred_svm = svm.predict(X_test)\n\n# SVM Score\nsvm_score = svm.score(X_test,y_test)\nprint(\"SVM Score = \", svm_score, \"\\n\")\n\n# Confusion Matrix\ncm = pd.crosstab(y_test, y_pred_svm, rownames=['Actual'], colnames=['Predicted'])\ncm","e25e61d5":"# Let's find best parameters\n\nparam  = {'C':[0.1,1,10],'kernel': ['rbf', 'linear','poly'],'gamma':[0.001, 0.1, 0.5]} # parameters to be tested\ngrid_clf = model_selection.GridSearchCV(estimator=svm, param_grid=param)\ngrille = grid_clf.fit(X_train, y_train)\nprint(pd.DataFrame.from_dict(grille.cv_results_).loc[:,['params', 'mean_test_score']]) ","c14a3cc7":"print(\"Best parameters are \" ,grid_clf.best_params_) # Best parameters","f3600384":"# New predictions with best parameters\ny_pred_svm_bis = grid_clf.predict(X_test)\n\n# New SVM Score\nsvm_bis_score = grid_clf.score(X_test,y_test)\nprint(\"New SVM Score = \", svm_bis_score, \"\\n\")\n\n# Confusion Matrix\ncm = pd.crosstab(y_test, y_pred_svm_bis, rownames=['Actual'], colnames=['Predicted'])\ncm","018ff993":"# Model training\nknn = neighbors.KNeighborsClassifier()\nknn.fit(X_train , y_train)\ny_pred_knn = knn.predict(X_test)\n\n# Score\nknn_score = knn.score(X_test,y_test)\nprint(\"KNN Score = \", knn_score, \"\\n\")\n\n# Confusion Matrix\ncm = pd.crosstab(y_test, y_pred_knn, rownames=['Actual'], colnames=['Predicted'])\ncm","5febd263":"# Let's find best parameters\nscore_minko = []\nscore_man = []\nscore_cheb = []\n\nfor k in range (1,41):\n    knn = neighbors.KNeighborsClassifier(n_neighbors=k,metric = 'minkowski')\n    knn.fit(X_train, y_train)\n    score_minko.append(knn.score(X_test, y_test))\n    \nfor k in range(1, 41):\n    knn = neighbors.KNeighborsClassifier(n_neighbors=k, metric='manhattan')\n    knn.fit(X_train, y_train)\n    score_man.append(knn.score(X_test, y_test))\n    \nfor k in range(1, 41):\n    knn = neighbors.KNeighborsClassifier(n_neighbors=k, metric='chebyshev')\n    knn.fit(X_train, y_train)\n    score_cheb.append(knn.score(X_test, y_test))\n    \n# Plotting results\nplt.plot(range(1, 41),score_minko, label='Minkowski')\nplt.plot(range(1, 41),score_man, label='Manhattan')\nplt.plot(range(1, 41),score_cheb, label='Chebyshev')\nplt.title('Score - valeur de K')  \nplt.xlabel('Valeur de K')  \nplt.ylabel('Accuracy') \nplt.legend();\n    ","1b4e5ec6":"# Model training with better parameters\nknn_bis = neighbors.KNeighborsClassifier(n_neighbors=5,metric = 'minkowski')\nknn_bis.fit(X_train , y_train)\ny_pred_knn_bis = knn_bis.predict(X_test)\n\n# Score\nknn_bis_score = knn_bis.score(X_test,y_test)\nprint(\"KNN Score = \", knn_bis_score, \"\\n\")\n\n# Confusion Matrix\ncm = pd.crosstab(y_test, y_pred_knn_bis, rownames=['Actual'], colnames=['Predicted'])\ncm","e26e6f10":"# Model training\nrfc = RandomForestClassifier(n_estimators=200)\nrfc.fit(X_train, y_train)\ny_pred_rfc = rfc.predict(X_test)\n\n# Score\nrfc_score = rfc.score(X_test,y_test)\nprint(\"Score Random Forest Classifier = \", rfc_score, \"\\n\")\n\n# Confusion Matrix\ncm = pd.crosstab(y_test, y_pred_rfc, rownames=['Actual'], colnames=['Predicted'])\ncm","60672b06":"# Classification report\nprint(classification_report(y_test, y_pred_rfc))\nrfc_score = rfc.score(X_test,y_test)","f37df912":"rf=RandomForestRegressor()\nrf.fit(X_train, y_train)\ny_pred_rf = rf.predict(X_test)\n\n# Score\nrf_score = rf.score(X_test,y_test)\nprint(\"Score Random Forest Regressor = \", rf_score, \"\\n\")\n\n# Confusion Matrix\ncm = pd.crosstab(y_test, y_pred_rf, rownames=['Actual'], colnames=['Predicted'])\ncm","3744fac3":"dt_clf = DecisionTreeClassifier()\ndt_clf.fit(X_train, y_train)\ny_pred_dt_clf = dt_clf.predict(X_test)\n\n# Score\ndt_clf_score = dt_clf.score(X_test,y_test)\nprint(\"Score Random Forest Regressor = \", dt_clf_score, \"\\n\")\n\n# Confusion Matrix\ncm = pd.crosstab(y_test, y_pred_dt_clf, rownames=['Actual'], colnames=['Predicted'])\ncm","90a4c5ae":"ac = AdaBoostClassifier (base_estimator=dt_clf,n_estimators=140)\nac.fit(X_train, y_train)\ny_pred_ac = ac.predict(X_test)\n\n# Score\nac_score = ac.score(X_test,y_test)\nprint(\"AdaBoostClassifier Score = \", ac_score, \"\\n\")\n\n# Confusion Matrix\ncm = pd.crosstab(y_test, y_pred_ac, rownames=['Actual'], colnames=['Predicted'])\ncm","f0215e43":"bc = BaggingClassifier(n_estimators=1000,oob_score=True)\nbc.fit(X_train,y_train)\nbc.oob_score_","5c3166d5":"#Importing libraries \n\nfrom bokeh.io import output_file,show,output_notebook,push_notebook\nfrom bokeh.plotting import figure\nfrom bokeh.models import ColumnDataSource,HoverTool,CategoricalColorMapper\nfrom bokeh.layouts import row,column,gridplot\nfrom bokeh.models.widgets import Tabs,Panel\noutput_notebook()\nfrom bokeh.models import Range1d\n\nscore = ['Logistic Regression','Random Forest Classifier',\n        'SVM','SVM with best parameters','KNN','KNN with best parameters',\n        'Decision Tree','Bagging']\n\nplot = figure(plot_width=800, plot_height=300,title = \"Score Synthesis\",y_range=score)\n\nplot.hbar(y = score, right = [lr_score,rfc_score,svm_score,svm_bis_score,knn_score,knn_bis_score,dt_clf_score,bc.oob_score_],fill_color=\"#b3de69\",height = 0.8,line_color = 'black')\nplot.x_range=Range1d(0, 1)\nplot.xaxis.axis_label = \"Score\"\n\nshow(plot)","abe351dd":"print(\"Logistic Regression Score = \", round(lr_score,2), \"\\n\")\nprint(\"Random Forest Classifier Score = \", round(rfc_score,2), \"\\n\")\nprint(\"SVM Score = \", round(svm_score,2), \"\\n\")\nprint(\"SVM Score with best parameters = \", round(svm_bis_score,2), \"\\n\")\nprint(\"KNN Score = \", round(knn_score,2), \"\\n\")\nprint(\"KNN Score with best parameters = \", round(knn_bis_score,2), \"\\n\")\nprint(\"Decision Tree Score = \", round(dt_clf_score,2), \"\\n\")\nprint(\"Bagging Score = \", round(bc.oob_score_,2), \"\\n\")","a98edf28":"### 4.1.6) Bagging","b6771a2d":"## 4.1) Classification","177a3e3f":"# 1) Initialization","fbd1f9a4":"### 4.1.4) Random Forest","4a983b28":"# 2) Data vizualisation","f20a3f54":"# 5 ) Score Synthesis","75448970":"# 4) Modeling","26751ab7":"### 4.1.1) Logistic Regression","adcfdc67":"### 4.1.3) KNN","d48fa538":"# 3) Preprocessing","280358a4":"# 1) DataFrame Analyze","ea321cf8":"### 4.1.2) SVM","60a0b8c2":"### 4.1.5) Decision Tree","2b05158d":"### 4.1.5) Adaboost"}}