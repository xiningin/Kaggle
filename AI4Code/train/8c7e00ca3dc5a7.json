{"cell_type":{"41beb46e":"code","6c7b0cea":"code","4838b3a6":"code","bbae061b":"code","3e736e02":"code","13f2f65f":"code","1e979b73":"code","bfca3b17":"code","aa60545f":"code","c255f3c7":"code","b2e57997":"code","482392b5":"code","67e0af24":"code","c031447b":"code","385e9a11":"code","81c93018":"code","d71b8b5a":"code","b1f6a535":"code","98c1a709":"code","c9b1488a":"code","fcfa5a1c":"code","0f6a7040":"code","c9720d0d":"code","3629fcf3":"code","66715b6e":"code","9753359b":"code","d0833676":"code","db614605":"code","f9a074a5":"code","a019cc59":"code","50d2a0d6":"code","e0e14c09":"markdown","59710385":"markdown","0154fedf":"markdown","9d08b832":"markdown","c6323a7e":"markdown","15f58ac7":"markdown","70d99340":"markdown","d2c93e9e":"markdown","19f5061e":"markdown","491f0dce":"markdown","5c1f6a9a":"markdown","c77d0a56":"markdown","a27f2252":"markdown","7dd879c3":"markdown","f77ba53a":"markdown","fd14b85e":"markdown","0e235fc7":"markdown"},"source":{"41beb46e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","6c7b0cea":"df_train = pd.read_csv(r\"..\/input\/train.csv\")\ndf_test = pd.read_csv(r\"..\/input\/test.csv\")\npd.set_option('display.width', 700)\ndf_train.head(5)","4838b3a6":"df_test.head(5)","bbae061b":"# Size of data before any operations:\n# Here one less col in df_test becoz SalePrice is missing.\nprint(\"The train data shape : {} \".format(df_train.shape))\nprint(\"The test data shape : {} \".format(df_test.shape))","3e736e02":"print(df_train.dtypes)\ndf_train.describe()","13f2f65f":"# Remove Id column. It doesn't contribute to the price prediction\ndf_train.drop(\"Id\", inplace=True, axis=1)\ndf_test.drop(\"Id\", inplace=True, axis=1)\n\nprint(\"The train data shape after removing Id col: {} \".format(df_train.shape))\nprint(\"The test data shape after removing Id col: {} \".format(df_test.shape))","1e979b73":"# Remove all the rows and columns from the data which has all values Nans\/Empty cells\ndf_train.dropna(axis=1, how=\"all\", inplace=True)\ndf_train.dropna(axis=0, how=\"all\", inplace=True)\n\ndf_test.dropna(axis=1, how=\"all\", inplace=True)\ndf_test.dropna(axis=0, how=\"all\", inplace=True)\n\nprint(\"The train data shape after removing all col and rows with Nans : {} \".format(df_train.shape))\nprint(\"The test data shape after removing all col and rows with Nans: {} \".format(df_test.shape))","bfca3b17":"# Find out the frequency of nulls in the columns\n\n# For training Data\ncount_nans = len(df_train) - df_train.count()\ndf_count_nans = count_nans.to_frame()\ndf_count_nans.columns=[\"train_nan_count\"]\ndf_count_nans[\"%_train_nans\"]=(df_count_nans[\"train_nan_count\"]\/df_train.shape[0]) * 100\n\n# For test data\ndf_count_nans[\"test_nan_count\"] = len(df_test) - df_test.count()\ndf_count_nans[\"%_test_nans\"]=(df_count_nans[\"test_nan_count\"]\/df_test.shape[0]) * 100\n\ndf_count_nans.sort_values(\"train_nan_count\", ascending=False, inplace=True)\ndf_count_nans.query('train_nan_count > 0 or test_nan_count > 0')","aa60545f":"# take out the SalePrice from the train data before further processing\ny_train = df_train.SalePrice.values\nprint(y_train)\ndf_train.drop(\"SalePrice\", inplace=True, axis=1)","c255f3c7":"# Combining all data for the further processing\ndf_all_data = pd.concat([df_train, df_test])\ndf_all_data.reset_index(inplace=True, drop=True)\nprint(df_all_data.shape)\ndf_all_data.columns\ndf_all_data.head()","b2e57997":"# By looking at the analysis of Nans it is clear that the PoolQC, Alley, MiscFeature, Fence has many null values.\n# So those aren't contributing to the price a lot. So these columns will be dropped.\ndf_all_data.drop([\"PoolQC\", \"Alley\", \"MiscFeature\", \"Fence\"], axis=1, inplace=True)","482392b5":"# Fill the values for the rest of the NAs\n\n# No changes in FirplaceQu. Because the NAs indicates NA=No Fireplace. Fill with None\ndf_all_data[\"FireplaceQu\"].fillna(\"None\", inplace=True)\n\n# GarageCond, GarageType, GarageFinish, GarageQual. Fill with None\ndf_all_data[[\"GarageCond\", \"GarageType\", \"GarageFinish\", \"GarageQual\"]] = df_all_data[[\"GarageCond\", \"GarageType\", \"GarageFinish\", \"GarageQual\"]].fillna(\"None\")\n\n# For GarageYrBlt filling with the 0. Assuming garage isn't available.\ndf_all_data[\"GarageYrBlt\"].fillna(0,  inplace=True)\n\n# Fill with None\/0 for basement. It is likely that basment isn't available in the houses\ndf_all_data[[\"BsmtExposure\",\"BsmtFinType1\", \"BsmtFinType2\", \"BsmtCond\", \"BsmtQual\"]] = df_all_data[[\"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinType2\", \"BsmtCond\", \"BsmtQual\"]].fillna(\"None\")\ndf_all_data[[\"BsmtFullBath\", \"BsmtFinSF1\", \"BsmtFinSF2\", \"BsmtUnfSF\", \"TotalBsmtSF\", \"BsmtHalfBath\"]] = df_all_data[[\"BsmtFullBath\", \"BsmtFinSF1\", \"BsmtFinSF2\", \"BsmtUnfSF\", \"TotalBsmtSF\", \"BsmtHalfBath\"]].fillna(0)\n\n# MasVnrArea. Fill with 0. Prob it is not available in these houses\ndf_all_data[\"MasVnrArea\"].fillna(0, inplace=True)\ndf_all_data[\"MasVnrType\"].fillna(\"None\", inplace=True)\n\n# Electrical. Fill with the most common one.\nmost_common = df_all_data[\"Electrical\"].value_counts().index[0]\ndf_all_data[\"Electrical\"].fillna(most_common, inplace=True)\n\n# Functional. As directed in data definition, use \"Typ\" as the default val\ndf_all_data[\"Functional\"].fillna(\"Typ\", inplace=True)\n\n# KitchenQual. Considering \"TA\" as default val, which is short of Typical\/Average\ndf_all_data[\"KitchenQual\"].fillna(\"TA\", inplace=True)\n   \n# Fill with the most common val\nmost_common =  df_all_data[\"SaleType\"].value_counts().index[0]\ndf_all_data[\"SaleType\"].fillna(most_common, inplace=True)\n    \n# No assumption can be made for the following columns val\ndf_all_data[\"Utilities\"].fillna(\"None\", inplace=True)\ndf_all_data[\"Exterior1st\"].fillna(\"None\", inplace=True)\ndf_all_data[\"Exterior2nd\"].fillna(\"None\", inplace=True)\n\n# Fill with the most common val\nmost_common =  df_all_data[\"MSZoning\"].value_counts().index[0]\ndf_all_data[\"MSZoning\"].fillna(most_common, inplace=True)","67e0af24":"# Filling GarageCars per neighborhood. It is most likely that per neighborhood the car space is similar.\ngrp=df_all_data.groupby(\"Neighborhood\")[\"GarageCars\"].mean()\nnan_idx = df_all_data[df_all_data[\"GarageCars\"].isnull()==True].index.tolist()\nfor idx in nan_idx:\n    df_all_data.loc[idx, \"GarageCars\"] = int(round(grp.loc[df_all_data.iloc[idx][\"Neighborhood\"]]))\n\n# Filling GarageArea per neighborhood. It is most likely that per neighborhood the car space is similar.\ngrp=df_all_data.groupby(\"Neighborhood\")[\"GarageArea\"].mean()\nnan_idx = df_all_data[df_all_data[\"GarageArea\"].isnull()==True].index.tolist()\nfor idx in nan_idx:\n    df_all_data.loc[idx, \"GarageArea\"] = int(round(grp.loc[df_all_data.iloc[idx][\"Neighborhood\"]]))","c031447b":"# LotFrontage: The linear feet of the street connected to the property can be based on the building types. So per building type it will be filed in with the avg.\ndf_all_data[\"LotFrontage\"] = df_all_data.groupby(\"BldgType\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.mean()))","385e9a11":"# Find highly correlated features\ncorr_matrix = df_all_data.corr().abs()\nplt.subplots(figsize=(15,10))\nsns.heatmap(corr_matrix, cmap=\"jet\")","81c93018":"# Only for Trainingdata. The same relationship can be seen as above\ncorr_matrix = df_train.corr().abs()\nplt.subplots(figsize=(15,10))\nsns.heatmap(corr_matrix, cmap=\"jet\")","d71b8b5a":"# Only for Test data. The same relationship can be seen as above\ncorr_matrix = df_test.corr().abs()\nplt.subplots(figsize=(15,10))\nsns.heatmap(corr_matrix, cmap=\"jet\")","b1f6a535":"# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Find index of feature columns with correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(upper[column] >= 0.75)]\nprint(to_drop)","98c1a709":"df_all_data.drop(to_drop, axis=1, inplace=True)","c9b1488a":"sns.regplot(x=df_train[\"GrLivArea\"], y=y_train)","fcfa5a1c":"drop_points = df_train.sort_values(by = 'GrLivArea', ascending = False)[:2][\"GrLivArea\"]\ndrop_points_list = drop_points.index.tolist()\ndf_all_data.drop(drop_points_list, inplace=True)\n\n# Updating the indexes for training data.\ny_train = np.delete(y_train, drop_points_list)\ndf_train_last_index=df_train.shape[0]-len(drop_points_list)","0f6a7040":"#MSSubClass=The building class\ndf_all_data['MSSubClass'] = df_all_data['MSSubClass'].apply(str)\ndf_all_data[\"MSSubClass\"] = LabelEncoder().fit_transform(df_all_data[\"MSSubClass\"])","c9720d0d":"# detect skewed columns\nskew_thresh = 0.5\nskewed = df_all_data.skew().sort_values(ascending=False)\na=skewed[abs(skewed)>skew_thresh]","3629fcf3":"skewed_cols = skewed[abs(skewed)>skew_thresh].index.tolist()\nprint(len(skewed_cols))\nprint(skewed_cols)\ndf_all_data[skewed_cols] = df_all_data[skewed_cols].apply(np.log1p)\ndf_all_data[skewed_cols].head()","66715b6e":"# Use the one hot encoder to change the categorical data in the numeric data\ncategorical_data_cols = df_all_data.select_dtypes(include=['object'])\nprint(categorical_data_cols.columns.tolist())\ndf_all_data = pd.get_dummies(df_all_data)","9753359b":"# Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.\ny_train = np.log1p(y_train)","d0833676":"df_all_data = (df_all_data - df_all_data.mean()) \/ (df_all_data.max() - df_all_data.min())","db614605":"# Create training and test dataset after data munging\ndf_tr = df_all_data.iloc[:df_train_last_index]\ndf_te = df_all_data.iloc[df_train_last_index:]\nprint(df_tr.shape)\nprint(df_te.shape)","f9a074a5":"from sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\n\ndef _ApplyLinearAlgo(model_obj, df_tr, df_te, y_train):\n    model_obj.fit(df_tr, y_train)\n    y_predict = model_obj.predict(df_tr)\n    print(\"r2 score train \" + str(r2_score(y_train, y_predict)))\n    print(\"rmse score train \" + str(mean_squared_error(y_train, y_predict)))\n\n    print(df_tr.shape)\n    print(df_te.shape)\n    y_te_pred = np.expm1(model_obj.predict(df_te))\n    \n    return y_te_pred","a019cc59":"print(\"\\n\")\nprint(\"ElasticNetCV\")\nfrom sklearn.linear_model import ElasticNetCV\nlr = ElasticNetCV(alphas=[0.0001, 0.0005, 0.001, 0.01, 0.1, 1, 10], l1_ratio=[.01, .1, .5, .9, .99], max_iter=5000)\ny_pred_Elastic = _ApplyLinearAlgo(lr, df_tr, df_te, y_train)\n\nprint(\"\\n\")\nprint (\"\\nRidgeCV\")\nfrom sklearn.linear_model import RidgeCV\nlr=RidgeCV(alphas=[0.0001, 0.0005, 0.001, 0.01, 0.1, 1, 10])\ny_te_Ridge = _ApplyLinearAlgo(lr, df_tr, df_te, y_train)\n\nprint(\"\\n\")\nprint(\"RandomForestRegressor\")\nfrom sklearn.ensemble import RandomForestRegressor\nlr = RandomForestRegressor()\ny_te_RF = _ApplyLinearAlgo(lr, df_tr, df_te, y_train)","50d2a0d6":"idx = pd.read_csv(\"..\/input\/test.csv\").Id\nmy_submission = pd.DataFrame({'Id': idx, 'SalePrice': y_pred_Elastic})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission.csv', index=False)\nmy_submission.head()","e0e14c09":"## Data Preview","59710385":"## Log Transformation of the SalePrice","0154fedf":"## Categorical Data Transformation","9d08b832":"Based on above data there're some skewed data present in the data. So it will be log transformed","c6323a7e":"## Outliers\n\nBased on the orignal [Ames data defintion](http:\/\/ww2.amstat.org\/publications\/jse\/v19n3\/Decock\/DataDocumentation.txt) there are outliers present into the data. Especially in the GrLivArea column. So need to find out those observations and remove it if possible.","15f58ac7":"The following kernel does some of the simple but essential steps to perform the Regression Analysis. By doing the it, it gives nice performence which puts the kernel in top 20% with the score of 0.12023 at the time of submission.","70d99340":"## Check for Nans\/NA\/Missing Data","d2c93e9e":"## Primary Exploration of the Data","19f5061e":"With the few submission trials , the ElasticNet gives the best score. So submitting with that. Other algorithms are also giving similar performance.","491f0dce":"# Normalize the data. \n","5c1f6a9a":"**References:**\n\nTook some ideas from the https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard","c77d0a56":"## Handle the Skewness in the Data","a27f2252":"\n## Applying Regression Algorithms\nNow is the time to apply the regression algorithms! Following algorithms are tried the ElasticNetCV gives the best result.\n\n* ElasticNet\n* Ridge\n* Random Forest","7dd879c3":"In the above plot we can see the 2 dots in the bottom-right. They are above 4000 sq ft. but has been sold very less amount. Whereas other sellings like top right corner are sold at much more higher prices. So those 2 are outliers and should be deleted. Otherwise the models will try to capture those points and resulting overfit. The data definition also tells there're more outliers but we can keep it. By looking at the plot, it seems like all other points are following trend.","f77ba53a":"The description below inffered based on the data defintions and the correlation matrix.\n* GarageYrBlt and YearBlt has the high correlation. Because garages are mostly built when the houses are built.\n* GarageArea and GarageCars has very high correlation. Because if more cars can be parked, then there will be more garages space and vice versa.\n* The '1stFlrSF', 'TotalBsmtSF'  are correlated. It makes sense because usually basements are usually right below the first floor and mostly similar in size.  \n* TotRmsAbvGrd and the GrLivArea are correlated. It also makes sense because in both of the columns the basement isn't considered.\n\nSo the above features will be dropped for future calculation. ","fd14b85e":"## Correlation Study","0e235fc7":"After applying OneHotEncoder, the data is at different scales now. It needs to be normalized."}}