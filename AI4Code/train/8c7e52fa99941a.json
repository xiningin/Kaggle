{"cell_type":{"eda1d94f":"code","5b9c5149":"code","7c13abd5":"code","7b0c606f":"code","44875d0b":"code","0271386d":"code","e0a5e8d7":"code","54846638":"code","33434eab":"code","0f1d76d6":"code","b0dcc3b8":"code","6d4608f9":"code","83f9ccd3":"code","64ebc217":"code","0c1fea10":"code","9a4b156b":"code","51f1d876":"code","e0043974":"code","70a871cd":"code","6318b6c7":"code","6d6ae0e9":"code","18f33e11":"code","2c4c8737":"code","aea6bce9":"code","9b961f9b":"code","514208e0":"code","4606ff68":"code","69483df0":"code","7e87cf8f":"code","b780500b":"code","77238850":"code","29698c0b":"code","493b8352":"code","e4ce5126":"code","a914fa99":"code","b6bcdfcc":"code","2b1e47f5":"code","b2209e68":"code","223f2a98":"code","7cfe29fc":"code","307bcff6":"code","4b6b1ab3":"code","a53c4aa4":"code","b50f6961":"code","4d8603f7":"code","473f7b44":"code","352ed883":"code","a906cb3d":"code","f70d0d8f":"code","9ce7fd84":"code","d9fb86f7":"code","56a76cc4":"code","cdda925d":"code","43ca421f":"code","eeea3faa":"code","d03bfd3d":"code","7a3d9f0e":"markdown","770eb25c":"markdown","e4dbce1b":"markdown","a8565606":"markdown","7df06646":"markdown","b64e98c9":"markdown","3342c946":"markdown","09610cee":"markdown","d341a6e3":"markdown","71572f37":"markdown","afcbb287":"markdown","a91d63ed":"markdown","3b89c920":"markdown","fb0d2fa3":"markdown","7221b2b7":"markdown","b3f774e6":"markdown","51f738ba":"markdown","a2114970":"markdown","1ea22bc1":"markdown","4f12e5e1":"markdown","a8eadaed":"markdown","1cc7a6de":"markdown","1966567a":"markdown","43f0035d":"markdown","7c703043":"markdown","98fe86f0":"markdown","749a4e4c":"markdown","67ab676a":"markdown","d3c073de":"markdown","affbc53c":"markdown"},"source":{"eda1d94f":"import gc\nimport os\nimport time\nimport sys\nimport logging\nimport datetime\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom scipy import stats\nfrom scipy.signal import hann\nfrom tqdm import tqdm_notebook\nimport matplotlib.pyplot as plt\nfrom scipy.signal import hilbert\nfrom scipy.signal import convolve\nfrom sklearn.svm import NuSVR, SVR\nfrom catboost import CatBoostRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.model_selection import KFold,StratifiedKFold, RepeatedKFold\nfrom sklearn.model_selection import cross_val_score, train_test_split, cross_val_predict\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n\nwarnings.filterwarnings(\"ignore\")","5b9c5149":"PATH=\"..\/input\/\"\nos.listdir(PATH)","7c13abd5":"print(\"test\u6709{}\u500b\u6a94\u6848\".format(len(os.listdir(os.path.join(PATH, 'test' )))))","7b0c606f":"%%time\ntrain_df = pd.read_csv(os.path.join(PATH,'train.csv'), \n                       dtype={'acoustic_data': np.int16, 'time_to_failure': np.float32})","44875d0b":"pd.options.display.precision = 15\ntrain_df.head()","0271386d":"print(train_df['acoustic_data'].max())\nprint(train_df['acoustic_data'].min())\nprint(train_df['time_to_failure'].max())\nprint(train_df['time_to_failure'].min())","e0a5e8d7":"# \u5148\u5099\u4efd\u539f\u59cb\u8cc7\u6599\ntrain_df_save = train_df.copy","54846638":"# np.log1p(x) : \u8a08\u7b97 log(1 + x)\n# \u5c0dacoustic_data\u53d6\u7d55\u5c0d\u503c\u4e26\u505a\u5e73\u6ed1\u8655\u7406\ntrain_df.acoustic_data = np.log1p(abs(train_df.acoustic_data))","33434eab":"train_df.shape","0f1d76d6":"pd.options.display.precision = 15\ntrain_df.head(5)","b0dcc3b8":"'''# \u6bcf50\u7b46\u63a1\u6a231\u7b46\ntrain_ad_sample_df = train_df['acoustic_data'].values[::50]\ntrain_ttf_sample_df = train_df['time_to_failure'].values[::50]\n\ndef plot_acc_ttf_data(train_ad_sample_df, train_ttf_sample_df, title=\"Acoustic data and time to failure: 1% sampled data\"):\n    fig, ax1 = plt.subplots(figsize=(12, 8))\n    plt.title(title)\n    plt.plot(train_ad_sample_df, color='r')\n    ax1.set_ylabel('acoustic data', color='r')\n    plt.legend(['acoustic data'], loc=(0.01, 0.95))\n    ax2 = ax1.twinx()\n    plt.plot(train_ttf_sample_df, color='b')\n    ax2.set_ylabel('time to failure', color='b')\n    plt.legend(['time to failure'], loc=(0.01, 0.9))\n    plt.grid(True)\n\nplot_acc_ttf_data(train_ad_sample_df, train_ttf_sample_df)\ndel train_ad_sample_df\ndel train_ttf_sample_df'''","6d4608f9":"gc.collect()","83f9ccd3":"'''train_ad_sample_df = train_df['acoustic_data'].values[:50580000]\ntrain_ttf_sample_df = (train_df['time_to_failure'].values[:50580000])\nplot_acc_ttf_data(train_ad_sample_df, train_ttf_sample_df, title=\"Acoustic data and time to failure: 1st 2 quakes\")\ndel train_ad_sample_df\ndel train_ttf_sample_df'''","64ebc217":"#gc.collect()","0c1fea10":"# \u67e5\u770btest\u8cc7\u6599\ntest = pd.read_csv('..\/input\/test\/seg_00030f.csv')\ntest.shape","9a4b156b":"test.head()","51f1d876":"rows = 150000\nsegments = int(np.floor(train_df.shape[0] * 8 \/ rows))\nprint(\"Number of segments: \", segments)","e0043974":"def add_trend_feature(arr, abs_values=False):\n    idx = np.array(range(len(arr)))\n    if abs_values:\n        arr = np.abs(arr)\n    lr = LinearRegression()\n    lr.fit(idx.reshape(-1, 1), arr)\n    return lr.coef_[0]\n\ndef classic_sta_lta(x, length_sta, length_lta):\n    sta = np.cumsum(x ** 2)\n    # Convert to float\n    sta = np.require(sta, dtype=np.float)\n    # Copy for LTA\n    lta = sta.copy()\n    # Compute the STA and the LTA\n    sta[length_sta:] = sta[length_sta:] - sta[:-length_sta]\n    sta \/= length_sta\n    lta[length_lta:] = lta[length_lta:] - lta[:-length_lta]\n    lta \/= length_lta\n    # Pad zeros\n    sta[:length_lta - 1] = 0\n    # Avoid division by zero by setting zero values to tiny float\n    dtiny = np.finfo(0.0).tiny\n    idx = lta < dtiny\n    lta[idx] = dtiny\n    return sta \/ lta","70a871cd":"train_X = pd.DataFrame(index=range(segments), dtype=np.float32)\ntrain_y = pd.DataFrame(index=range(segments), dtype=np.float32, columns=['time_to_failure'])","6318b6c7":"# These may be needed later\n'''need_aggregated_features = True\nif need_aggregated_features:\n    total_mean = train_df['acoustic_data'].mean()\n    total_std = train_df['acoustic_data'].std()\n    total_max = train_df['acoustic_data'].max()\n    total_min = train_df['acoustic_data'].min()\n    total_sum = train_df['acoustic_data'].sum()\n    total_abs_sum = np.abs(train_df['acoustic_data']).sum()'''","6d6ae0e9":"train_X.shape, train_y.shape","18f33e11":"def create_features(seg_id, seg, X):\n    xc = pd.Series(seg['acoustic_data'].values)\n    zc = np.fft.fft(xc)    #\u505a\u5085\u7acb\u8449\u8f49\u63db\n    \n    # \u6574\u500bsegment\u7684\u5e73\u5747\u503c\u3001\u6a19\u6e96\u5dee\u3001\u6700\u5927\u503c\n    X.loc[seg_id, 'mean'] = xc.mean()\n    X.loc[seg_id, 'std'] = xc.std()\n    X.loc[seg_id, 'max'] = xc.max()\n    \n    # \u5085\u7acb\u8449\u8f49\u63db\u5f8c\uff0c\u6709\u5206\u70ba\u5be6\u90e8\u548c\u865b\u90e8\n    realFFT = np.real(zc)\n    imagFFT = np.imag(zc)\n    \n    # \u5be6\u90e8\u5e73\u65b9\u52a0\u865b\u90e8\u5e73\u65b9\n    X.loc[seg_id, 'RIsquare'] = np.mean(np.square(realFFT) + np.square(imagFFT))\n    X.loc[seg_id, 'RImsquare'] = np.square(realFFT.mean()) + np.square(imagFFT.mean())\n    \n    # \u5be6\u90e8\u7684\u5e73\u5747\u3001\u6a19\u6e96\u5dee\u3001\u6700\u5927\u6700\u5c0f\u503c\n    X.loc[seg_id, 'Rmean'] = realFFT.mean()\n    X.loc[seg_id, 'Rstd'] = realFFT.std()\n    X.loc[seg_id, 'Rmax'] = realFFT.max()\n    X.loc[seg_id, 'Rmin'] = realFFT.min()\n    \n    # \u865b\u90e8\u7684\u5e73\u5747\u3001\u6a19\u6e96\u5dee\u3001\u6700\u5927\u6700\u5c0f\u503c\n    X.loc[seg_id, 'Imean'] = imagFFT.mean()\n    X.loc[seg_id, 'Istd'] = imagFFT.std()\n    X.loc[seg_id, 'Imax'] = imagFFT.max()\n    X.loc[seg_id, 'Imin'] = imagFFT.min()\n    \n    # \u5be6\u90e8\u7684\u5012\u65785000\u500b\u9ede\u7684\u5e73\u5747\u3001\u6a19\u6e96\u5dee\u3001\u6700\u5927\u6700\u5c0f\u503c\n    X.loc[seg_id, 'Rmean_last_5000'] = realFFT[-5000:].mean()\n    X.loc[seg_id, 'Rstd__last_5000'] = realFFT[-5000:].std()\n    X.loc[seg_id, 'Rmax_last_5000'] = realFFT[-5000:].max()\n    X.loc[seg_id, 'Rmin_last_5000'] = realFFT[-5000:].min()\n    \n    # \u5be6\u90e8\u7684\u5012\u657815000\u500b\u9ede\u7684\u5e73\u5747\u3001\u6a19\u6e96\u5dee\u3001\u6700\u5927\u6700\u5c0f\u503c\n    X.loc[seg_id, 'Rmean_last_15000'] = realFFT[-15000:].mean()\n    X.loc[seg_id, 'Rstd_last_15000'] = realFFT[-15000:].std()\n    X.loc[seg_id, 'Rmax_last_15000'] = realFFT[-15000:].max()\n    X.loc[seg_id, 'Rmin_last_15000'] = realFFT[-15000:].min()\n    \n    # \u6574\u500bsegment\u7684\u5e73\u5747\u8b8a\u5316(\u5dee\u5206)\u3001\u8b8a\u5316\u7387\n    X.loc[seg_id, 'mean_change_abs'] = np.mean(np.diff(xc))\n    X.loc[seg_id, 'mean_change_rate'] = np.mean(np.nonzero((np.diff(xc) \/ xc[:-1]))[0])\n    \n    # \u6574\u500bsegment\u53d6\u7d55\u5c0d\u503c\u5f8c\u7684\u6700\u5927\u6700\u5c0f\u503c\n    X.loc[seg_id, 'abs_max'] = np.abs(xc).max()\n    X.loc[seg_id, 'abs_min'] = np.abs(xc).min()\n    \n    # \u539f\u59cbsegmant\u7684\u524d50000\u3001\u5f8c50000\u3001\u524d10000\u3001\u5f8c10000\u7684\u6a19\u6e96\u5dee\n    X.loc[seg_id, 'std_first_50000'] = xc[:50000].std()\n    X.loc[seg_id, 'std_last_50000'] = xc[-50000:].std()\n    X.loc[seg_id, 'std_first_10000'] = xc[:10000].std()\n    X.loc[seg_id, 'std_last_10000'] = xc[-10000:].std()\n    # \u539f\u59cbsegmant\u7684\u524d50000\u3001\u5f8c50000\u3001\u524d10000\u3001\u5f8c10000\u7684\u5e73\u5747\u503c\n    X.loc[seg_id, 'avg_first_50000'] = xc[:50000].mean()\n    X.loc[seg_id, 'avg_last_50000'] = xc[-50000:].mean()\n    X.loc[seg_id, 'avg_first_10000'] = xc[:10000].mean()\n    X.loc[seg_id, 'avg_last_10000'] = xc[-10000:].mean()\n    # \u539f\u59cbsegmant\u7684\u524d50000\u3001\u5f8c50000\u3001\u524d10000\u3001\u5f8c10000\u7684\u6700\u5c0f\u503c\n    X.loc[seg_id, 'min_first_50000'] = xc[:50000].min()\n    X.loc[seg_id, 'min_last_50000'] = xc[-50000:].min()\n    X.loc[seg_id, 'min_first_10000'] = xc[:10000].min()\n    X.loc[seg_id, 'min_last_10000'] = xc[-10000:].min()\n    # \u539f\u59cbsegmant\u7684\u524d50000\u3001\u5f8c50000\u3001\u524d10000\u3001\u5f8c10000\u7684\u6700\u5927\u503c\n    X.loc[seg_id, 'max_first_50000'] = xc[:50000].max()\n    X.loc[seg_id, 'max_last_50000'] = xc[-50000:].max()\n    X.loc[seg_id, 'max_first_10000'] = xc[:10000].max()\n    X.loc[seg_id, 'max_last_10000'] = xc[-10000:].max()\n\n    #X.loc[seg_id, 'max_to_min'] = xc.max() \/ np.abs(xc.min())\n    \n    # \u539f\u59cbsegment\u53d6\u7d55\u5c0d\u503c\u5f8c\u7684\u5168\u8ddd\n    X.loc[seg_id, 'max_to_min_diff'] = xc.max() - np.abs(xc.min())\n    \n    # \u8a08\u7b97\u7d55\u5c0d\u503c\u5927\u65bc500\u7684\u500b\u6578\n    X.loc[seg_id, 'count_big'] = len(xc[np.abs(xc) > 500])\n    \n    # \u539f\u59cbsegment\u7684\u52a0\u7e3d\n    X.loc[seg_id, 'sum'] = xc.sum()\n    \n    # \u524d50000\u3001\u5f8c50000\u3001\u524d10000\u3001\u5f8c100000\u7b46\u7684\u5e73\u5747\u8b8a\u5316\u7387\n    X.loc[seg_id, 'mean_change_rate_first_50000'] = np.mean(np.nonzero((np.diff(xc[:50000]) \/ xc[:50000][:-1]))[0])\n    X.loc[seg_id, 'mean_change_rate_last_50000'] = np.mean(np.nonzero((np.diff(xc[-50000:]) \/ xc[-50000:][:-1]))[0])\n    X.loc[seg_id, 'mean_change_rate_first_10000'] = np.mean(np.nonzero((np.diff(xc[:10000]) \/ xc[:10000][:-1]))[0])\n    X.loc[seg_id, 'mean_change_rate_last_10000'] = np.mean(np.nonzero((np.diff(xc[-10000:]) \/ xc[-10000:][:-1]))[0])\n    \n    # \u5404\u500b\u5206\u4f4d\u6578\n    X.loc[seg_id, 'q95'] = np.quantile(xc, 0.95)\n    X.loc[seg_id, 'q99'] = np.quantile(xc, 0.99)\n    X.loc[seg_id, 'q05'] = np.quantile(xc, 0.05)\n    X.loc[seg_id, 'q01'] = np.quantile(xc, 0.01)\n    X.loc[seg_id, 'q999'] = np.quantile(xc,0.999)\n    X.loc[seg_id, 'q001'] = np.quantile(xc,0.001)\n    \n    # \u7d55\u5c0d\u503c\u7684\u5206\u4f4d\u6578\n    X.loc[seg_id, 'abs_q95'] = np.quantile(np.abs(xc), 0.95)\n    X.loc[seg_id, 'abs_q99'] = np.quantile(np.abs(xc), 0.99)\n    X.loc[seg_id, 'abs_q05'] = np.quantile(np.abs(xc), 0.05)\n    X.loc[seg_id, 'abs_q01'] = np.quantile(np.abs(xc), 0.01)\n    \n    \n    X.loc[seg_id, 'trend'] = add_trend_feature(xc)\n    X.loc[seg_id, 'abs_trend'] = add_trend_feature(xc, abs_values=True)\n    \n    # \u7d55\u5c0d\u503c\u7684\u5e73\u5747\u503c\u3001\u6a19\u6e96\u5dee\n    X.loc[seg_id, 'abs_mean'] = np.abs(xc).mean()\n    X.loc[seg_id, 'abs_std'] = np.abs(xc).std()\n    \n    # \u539f\u59cbsegmant\u7684 Mean Absolute Deviation (\u5e73\u5747\u7d55\u5c0d\u504f\u5dee)\n    X.loc[seg_id, 'mad'] = xc.mad()    \n    \n    # \u8a08\u7b97\u4e0d\u540cwindow\u5927\u5c0f\u7684\u79fb\u52d5\u5e73\u5747\u503c\n    X.loc[seg_id, 'Moving_average_700_mean'] = xc.rolling(window=700).mean().mean(skipna=True)\n    X.loc[seg_id, 'Moving_average_1500_mean'] = xc.rolling(window=1500).mean().mean(skipna=True)\n    X.loc[seg_id, 'Moving_average_3000_mean'] = xc.rolling(window=3000).mean().mean(skipna=True)\n    X.loc[seg_id, 'Moving_average_6000_mean'] = xc.rolling(window=6000).mean().mean(skipna=True)\n    \n    # EWM(exponential weighted functions)\n    ewma = pd.Series.ewm\n    X.loc[seg_id, 'exp_Moving_average_300_mean'] = (ewma(xc, span=300).mean()).mean(skipna=True)\n    X.loc[seg_id, 'exp_Moving_average_3000_mean'] = ewma(xc, span=3000).mean().mean(skipna=True)\n    X.loc[seg_id, 'exp_Moving_average_30000_mean'] = ewma(xc, span=6000).mean().mean(skipna=True)\n    \n    no_of_std = 2\n    X.loc[seg_id, 'MA_700MA_std_mean'] = xc.rolling(window=700).std().mean()\n    X.loc[seg_id,'MA_700MA_BB_high_mean'] = (X.loc[seg_id, 'Moving_average_700_mean'] + no_of_std * X.loc[seg_id, 'MA_700MA_std_mean']).mean()\n    X.loc[seg_id,'MA_700MA_BB_low_mean'] = (X.loc[seg_id, 'Moving_average_700_mean'] - no_of_std * X.loc[seg_id, 'MA_700MA_std_mean']).mean()\n    X.loc[seg_id, 'MA_400MA_std_mean'] = xc.rolling(window=400).std().mean()\n    X.loc[seg_id,'MA_400MA_BB_high_mean'] = (X.loc[seg_id, 'Moving_average_700_mean'] + no_of_std * X.loc[seg_id, 'MA_400MA_std_mean']).mean()\n    X.loc[seg_id,'MA_400MA_BB_low_mean'] = (X.loc[seg_id, 'Moving_average_700_mean'] - no_of_std * X.loc[seg_id, 'MA_400MA_std_mean']).mean()\n    X.loc[seg_id, 'MA_1000MA_std_mean'] = xc.rolling(window=1000).std().mean()\n    \n    # \u56db\u5206\u4f4d\u8ddd IQR(interquartile range) \n    X.loc[seg_id, 'iqr'] = np.subtract(*np.percentile(xc, [75, 25]))\n    \n    # \u6e1b\u53bb\u5206\u5e03\u5169\u7aef10%\u7684\u503c\u5f8c\uff0c\u8a08\u7b97\u5e73\u5747\u6578\n    X.loc[seg_id, 'ave10'] = stats.trim_mean(xc, 0.1)\n\n    for windows in [5, 10, 50, 100, 500, 1000, 5000, 10000]:\n        x_roll_std = xc.rolling(windows).std().dropna().values\n        x_roll_mean = xc.rolling(windows).mean().dropna().values\n        X.loc[seg_id, 'ave_roll_std_' + str(windows)] = x_roll_std.mean()\n        X.loc[seg_id, 'std_roll_std_' + str(windows)] = x_roll_std.std()\n        X.loc[seg_id, 'max_roll_std_' + str(windows)] = x_roll_std.max()\n        X.loc[seg_id, 'min_roll_std_' + str(windows)] = x_roll_std.min()\n        X.loc[seg_id, 'q01_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.01)\n        X.loc[seg_id, 'q05_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.05)\n        X.loc[seg_id, 'q95_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.95)\n        X.loc[seg_id, 'q99_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.99)\n        X.loc[seg_id, 'av_change_abs_roll_std_' + str(windows)] = np.mean(np.diff(x_roll_std))\n        X.loc[seg_id, 'av_change_rate_roll_std_' + str(windows)] = np.mean(np.nonzero((np.diff(x_roll_std) \/ x_roll_std[:-1]))[0])\n        X.loc[seg_id, 'abs_max_roll_std_' + str(windows)] = np.abs(x_roll_std).max()\n        \n        X.loc[seg_id, 'ave_roll_mean_' + str(windows)] = x_roll_mean.mean()\n        X.loc[seg_id, 'std_roll_mean_' + str(windows)] = x_roll_mean.std()\n        X.loc[seg_id, 'max_roll_mean_' + str(windows)] = x_roll_mean.max()\n        X.loc[seg_id, 'min_roll_mean_' + str(windows)] = x_roll_mean.min()\n        X.loc[seg_id, 'q01_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.01)\n        X.loc[seg_id, 'q05_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.05)\n        X.loc[seg_id, 'q95_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.95)\n        X.loc[seg_id, 'q99_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.99)\n        X.loc[seg_id, 'av_change_abs_roll_mean_' + str(windows)] = np.mean(np.diff(x_roll_mean))\n        X.loc[seg_id, 'av_change_rate_roll_mean_' + str(windows)] = np.mean(np.nonzero((np.diff(x_roll_mean) \/ x_roll_mean[:-1]))[0])\n        X.loc[seg_id, 'abs_max_roll_mean_' + str(windows)] = np.abs(x_roll_mean).max()  ","2c4c8737":"# iterate over all segments\nfor seg_id in tqdm_notebook(range(segments)):\n    seg = train_df.iloc[seg_id*int(rows\/8):seg_id*int(rows\/8)+rows]\n    create_features(seg_id, seg, train_X)\n    # the y value is the last entry in the time to failure in the segment\n    train_y.loc[seg_id, 'time_to_failure'] = seg['time_to_failure'].values[-1]","aea6bce9":"train_X_save = train_X.copy\ntrain_y_save = train_y.copy\ntrain_y.head()\n#experiment with variations of time\ntrain_y = train_y**1.0\ntrain_X.head()","9b961f9b":"# We will not train on the segments with a quake, because there are likely outliers\n# np.diff\u7684\u529f\u80fd\u662f\u5f97\u5230\u6bcf\u4e00\u9805\u6e1b\u6389\u524d\u4e00\u9805\u7684\u503c\uff0c'> 0'\u4f7f\u5f97\u767c\u751fquake\u7684\u5730\u65b9\u662fTrue\uff0c\u5176\u9918\u662fFalse\n# np.nonzero\u53d6\u5f97\u6bcf\u9805\u975e\u96f6\u7684\u503c\u7684index\uff0c\u5982\u4e0b(array([  36,  331,  694,  921, 1245, 1451, 1631, 2044, 2246, 2492, 2784, 3066, 3292, 3511, 3888, 4130]),)\ntrain_y_quake = np.nonzero(np.diff(train_y.time_to_failure) > 0)[0] + 1\nprint(len(train_y_quake))\nprint (len(train_y))\n\nfor idx in train_y_quake: \n    train_y.drop([idx],inplace=True)\n    train_X.drop([idx],inplace=True)","514208e0":"print (len(train_y))\ntrain_y.head(10)","4606ff68":"train_X.shape, train_y.shape","69483df0":"train_X.head(), train_y.head()","7e87cf8f":"train_X.to_csv('train_X.csv',index=False)\ntrain_y.to_csv('train_y.csv',index=False)","b780500b":"scaler = StandardScaler()\nscaler.fit(train_X)\nscaled_train_X = pd.DataFrame(scaler.transform(train_X), columns=train_X.columns)\n#scaled_train_X = train_X","77238850":"scaled_train_X.head(10)","29698c0b":"temp = train_X.copy","493b8352":"submission = pd.read_csv('..\/input\/sample_submission.csv', index_col='seg_id')\ntest_X = pd.DataFrame(columns=train_X.columns, dtype=np.float64, index=submission.index)","e4ce5126":"submission.shape, test_X.shape","a914fa99":"for seg_id in tqdm_notebook(test_X.index):\n    seg = pd.read_csv('..\/input\/test\/' + seg_id + '.csv')\n    # convert to the log of the absolute values\n    seg.acoustic_data =np.log1p(abs(seg.acoustic_data))\n    create_features(seg_id, seg, test_X)","b6bcdfcc":"test_X.to_csv('test_X.csv',index=False)","2b1e47f5":"scaled_test_X = pd.DataFrame(scaler.transform(test_X), columns=test_X.columns)\n#scaled_test_X = test_X\nscaled_test_X.values[1117]","b2209e68":"scaled_test_X.shape","223f2a98":"scaled_test_X.head()","7cfe29fc":"n_fold = 5\ndef mae_cv (model):\n    folds = KFold(n_splits=n_fold, shuffle=True, random_state=42).get_n_splits(scaled_train_X.values)\n    mae = -cross_val_score (model, scaled_train_X.values, train_y, scoring=\"neg_mean_absolute_error\",\n                           verbose=0,\n                           cv=folds)\n    return mae","307bcff6":"'''%%time\n\nlgb_params = {'num_leaves': 51,\n         'min_data_in_leaf': 10, \n         'objective':'regression',\n         'max_depth': -1,\n         'learning_rate': 0.001,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.91,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.91,\n         \"bagging_seed\": 42,\n         \"metric\": 'mae',\n         \"lambda_l1\": 0.1,\n         \"verbosity\": -1,\n         \"nthread\": -1,\n         \"random_state\": 42}\n\n\nlgb_model = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.01, n_estimators=720,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11, n_jobs = -1)\n\n#score = mae_cv(lgb_model)\n#print(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))\nlgb_model'''","4b6b1ab3":"'''%%time \nxgb_params = {'eta': 0.03,\n              'max_depth': 9,\n              'subsample': 0.85,\n              'objective': 'reg:linear',\n              'eval_metric': 'mae',\n              'silent': True,\n              'nthread': 4}\n    \nxgb_model = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1, eval_metric = 'mae',)\n\n#score = mae_cv(xgb_model)\n#print(\"XGB score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))\nxgb_model\n\n#    xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, \n#                          verbose_eval=500, params=xgb_params)'''","a53c4aa4":"%%time\nrf_model = RandomForestRegressor(n_estimators=120, n_jobs=-1, min_samples_leaf=1, \n                           max_features = \"auto\",max_depth=15, )\n#score = mae_cv(rf_model)\n#print(\"Random Forest score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))\nrf_model","b50f6961":"%%time\nparams = {'loss_function':'MAE',}\ncat_model = CatBoostRegressor(iterations=1000,  eval_metric='MAE', verbose=False, **params)\n\n#score = mae_cv(cat_model)\n#print(\"Cat Boost score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))\ncat_model","4d8603f7":"'''%%time\nKRR_model = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n#score = mae_cv(KRR_model)\n#print(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))\n#print (score)\nKRR_model'''","473f7b44":"'''%%time\n#ENet_model = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=0.9, random_state=3,max_iter=5000))\nENet_model = ElasticNet(alpha=0.0005, l1_ratio=0.9, random_state=3,max_iter=5000)\n#score = mae_cv(ENet_model)\n#print(\"Elastic Net score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))\nENet_model'''","352ed883":"'''%%time\nlasso_model = Lasso(alpha =0.0005, random_state=1)\n#score = mae_cv(lasso_model)\n#print(\"Lasso score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))\nlasso_model'''","a906cb3d":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        \n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   ","f70d0d8f":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        print (type(X))\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        print (KFold)\n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X.iloc[train_index], y.iloc[train_index])\n                y_pred = instance.predict(X.iloc[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","9ce7fd84":"class StackingCVRegressorRetrained(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, regressors, meta_regressor, n_folds=5, use_features_in_secondary=False):\n        self.regressors = regressors\n        self.meta_regressor = meta_regressor\n        self.n_folds = n_folds\n        self.use_features_in_secondary = use_features_in_secondary\n\n    def fit(self, X, y):\n        self.regr_ = [clone(x) for x in self.regressors]\n        self.meta_regr_ = clone(self.meta_regressor)\n\n        kfold = KFold(n_splits=self.n_folds, shuffle=True)\n\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.regressors)))\n\n        # Create out-of-fold predictions for training meta-model\n        for i, regr in enumerate(self.regr_):\n            for train_idx, holdout_idx in kfold.split(X, y):\n                instance = clone(regr)\n                instance.fit(X[train_idx], y[train_idx])\n                out_of_fold_predictions[holdout_idx, i] = instance.predict(X[holdout_idx])\n\n        # Train meta-model\n        if self.use_features_in_secondary:\n            self.meta_regr_.fit(np.hstack((X, out_of_fold_predictions)), y)\n        else:\n            self.meta_regr_.fit(out_of_fold_predictions, y)\n        \n        # Retrain base models on all data\n        for regr in self.regr_:\n            regr.fit(X, y)\n\n        return self\n\n    def predict(self, X):\n        meta_features = np.column_stack([\n            regr.predict(X) for regr in self.regr_\n        ])\n\n        if self.use_features_in_secondary:\n            return self.meta_regr_.predict(np.hstack((X, meta_features)))\n        else:\n            return self.meta_regr_.predict(meta_features)","d9fb86f7":"%%time\n#averaged_models = AveragingModels(models = (rf_model, xgb_model, KRR_model, lgb_model, ENet_model, cat_model, lasso_model))\n#averaged_models = AveragingModels(models = (rf_model, lgb_model,  cat_model, lasso_model))\naveraged_models = AveragingModels(models = (rf_model,cat_model))\n\nscore = mae_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n#averaged_models.fit (scaled_train_X.values, train_y)","56a76cc4":"averaged_models.fit (scaled_train_X.values, train_y)\naveraged_train_predict = averaged_models.predict(scaled_train_X.values)\nprint(mean_absolute_error(train_y, averaged_train_predict))","cdda925d":"averaged_prediction = np.zeros(len(scaled_test_X))\naveraged_prediction += averaged_models.predict(scaled_test_X.values)\naveraged_prediction","43ca421f":"%%time\n#stacked_predict = StackingAveragedModels(base_models =(rf_model, xgb_model, lgb_model, cat_model,ENet_model), \n#                                          meta_model =lasso_model) \n#stacked_predict.fit(scaled_train_X, train_y)","eeea3faa":"#stacked_train_pred = stacked_predict.predict(scaled_train_X)\n\n#print(mean_absolute_error(train_y, stacked_train_pred))\n\n#stacked_prediction = np.zeros(len(scaled_test_X))\n#stacked_prediction += stacked_predict.predict(scaled_test_X)**1.0\n#stacked_prediction[0:4]","d03bfd3d":"submission.time_to_failure = averaged_prediction\nsubmission.to_csv('submission_averaged_cat_rf_8.csv',index=True)\n#submission.time_to_failure = stacked_prediction\n#submission.to_csv('submissionV28_stacked.csv',index=True)","7a3d9f0e":"## Cat Boost","770eb25c":"# Submission","e4dbce1b":"# \u958b\u59cb\u5b9a\u7fa9\u5404\u7a2e\u4e0d\u540c\u6a21\u578b\uff0c\u7528\u4ee5Ensemble","a8565606":"## \u8655\u7406\u6e2c\u8a66\u8cc7\u6599\n\u5c0d\u65bc\u6e2c\u8a66\u8cc7\u6599\u505a\u76f8\u540c\u7684\u8655\u7406","7df06646":"\u4e0a\u65b9\u7d50\u679c\u986f\u793a\u4e00\u4efd\u6e2c\u8a66\u8cc7\u6599\u662f15\u842c\u7dad\u7684\uff0c\u4e8b\u5be6\u4e0a\uff0c\u6240\u6709\u6e2c\u8a66\u8cc7\u6599\u90fd\u662f\u4e00\u6a23\u7dad\u5ea6\u7684\u3002\n\n\u6211\u5011\u5c07\u8a13\u7df4\u8cc7\u6599\u5207\u5272\u6210\u8207\u6e2c\u8a66\u8cc7\u6599\u76f8\u540c\u7dad\u5ea6\u3002","b64e98c9":"\u8a13\u7df4\u8cc7\u6599\u7684\u5927\u5c0f: 629145480\u7b46\uff0c\u6bcf\u7b46\u5169\u500b\u5c6c\u6027","3342c946":"\u5c07\u6240\u6709\u8cc7\u6599\u8f49\u70ba\u6b63\u6578\uff0c\u53ea\u4fdd\u7559\u9707\u5e45\u5927\u5c0f\uff0c\u4e26\u4e14\u53d6log","09610cee":"## Elastic Net","d341a6e3":"## Kernel Ridge","71572f37":"\u5c0d\u8a13\u7df4\u8cc7\u6599\u9032\u884c\u5c3a\u5ea6\u7e2e\u653e\u7684\u8655\u7406","afcbb287":"\u770b\u4e00\u4e0b\u8cc7\u6599\u7684\u6700\u5927\u8207\u6700\u5c0f\u503c","a91d63ed":"# \u5c0e\u5165\u51fd\u5f0f\u5eab","3b89c920":"\u5c0d\u6e2c\u8a66\u8cc7\u6599\u4e5f\u9032\u884c\u4f7f\u5ea6\u7e2e\u653e\u8655\u7406","fb0d2fa3":"# \u6a21\u578b\u90e8\u4efd\n\u5148\u5b9a\u7fa9\u4e00\u500b\u9a57\u8b49\u7684function\uff0c\u7528\u4ee5\u8a55\u4f30\u6a21\u578b\u6548\u80fd","7221b2b7":"## LGB Model","b3f774e6":"input\u76ee\u9304\u4e2d\u6709train.csv\u3001sample_submission.csv\uff0c\u4ee5\u53ca\u53e6\u5916\u4e00\u500btest\u8cc7\u6599\u593e\uff0c\u518d\u4f86\u770b\u4e00\u4e0btest\u8cc7\u6599\u593e\u5167:","51f738ba":"## Random Forest","a2114970":"\u5728\u9019\u500b\u653e\u5927\u7684\u5716\u4e2d\uff0c\u53ef\u4ee5\u767c\u73fe\u5927\u9707\u76ea\u4e0d\u53ea\u6703\u51fa\u73fe\u5728failure\u9644\u8fd1\uff0c\u5728\u5169\u6b21failure\u4e2d\u9593\u4e5f\u6709\u51fa\u73fe\u8f03\u5927\u7684\u9707\u76ea\u3002","1ea22bc1":"\u8cc7\u6599\u578b\u614b\u8f49\u63db\u7528\n```python=\nd = {'a': [1, 2], 'b': [3, 4]}\ndf = pd.DataFrame(data=d, columns=['a', 'b'], dtype=np.int16)\nprint(sys.getsizeof(df))\ndf = df.astype({\"a\":np.int32})\nprint(sys.getsizeof(df))\nprint(df.a.dtype)\nprint(df.b.dtype)\ndf.head()\n```","4f12e5e1":"## \u8655\u7406\u8a13\u7df4\u8cc7\u6599","a8eadaed":"\u4e0a\u5716show\u51fa\u4e862%\u7684\u8cc7\u6599\uff0c\u53ef\u767c\u73fe\uff0c\u5728failure\u767c\u751f\u8655\uff0caccoustic data\u901a\u5e38\u6709\u660e\u986f\u7684\u5c16\u5cf0\uff0c\u4e14\u5373\u4f7f\u5728\u5169\u6b21failure\u4e4b\u9593\uff0c\u4e5f\u6703\u6709\u4e00\u4e9b\u9ad8\u5cf0\u5b58\u5728\u3002\n\n\u63a5\u8457\u6211\u5011plot\u51fa\u982d\u5169\u6b21\u7684\u5730\u9707 (time_to_failure\u70ba0\u7684\u5730\u65b9)","1cc7a6de":"\u89c0\u5bdf\u4e00\u4e0b\u524d\u5e7e\u7b46\u9577\u76f8:","1966567a":"## XGB Model","43f0035d":"\u5c0d\u6e2c\u8a66\u8cc7\u6599\u53d6\u540c\u6a23\u7684feature","7c703043":"# \u8b80\u53d6\u8cc7\u6599\n\u6211\u5011\u5148\u770b\u4e00\u4e0binput\u76ee\u9304\u4e2d\u7684\u6587\u4ef6","98fe86f0":"# EDA (Exploratory Data Analysis)\n\u7531\u65bc\u8cc7\u6599\u91cf\u592a\u5927\uff0c\u7e3d\u5171\u67096\u5104\u591a\u7b46\uff0c\u6545\u6211\u5011\u5148\u6bcf50\u7b46\u63a1\u6a23\u51fa1\u7b46\uff0cplot\u51fa\u8cc7\u6599\u4f86\u770b\u770b","749a4e4c":"## Lasso","67ab676a":"# \u7279\u5fb5\u5de5\u7a0b","d3c073de":"## \u4ea4\u53c9\u9a57\u8b49 Function","affbc53c":"* accoustic_data: \u5be6\u9a57\u4e2d\u6e2c\u91cf\u7684\u8072\u5b78\u4fe1\u865f\n* time_to_failure: \u96e2\u767c\u751ffailure\u9084\u6709\u5e7e\u79d2"}}