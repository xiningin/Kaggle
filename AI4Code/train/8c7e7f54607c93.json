{"cell_type":{"0070adf5":"code","b5c3f040":"code","0743816d":"code","5489dbd7":"code","87cf251f":"code","26d0e90b":"code","f073265c":"code","76f2c1d4":"code","4aad7d03":"code","ad7b39be":"code","ccd23166":"code","467603e1":"code","1660d28b":"code","847712ba":"code","52c94f2b":"code","57f8ee4d":"code","af858f64":"code","69bc58ae":"code","bcd89ee5":"code","20cf49dd":"code","a6c459f6":"code","0aa260d5":"code","4ce19c05":"code","76a0f181":"code","d9a7493e":"code","964b9f7b":"code","83becce2":"code","be118515":"code","2118b69c":"code","1fe01399":"code","6d880d7d":"code","89a23d2b":"code","ce22c622":"code","434eece6":"code","89712bbc":"code","bbd2fc4a":"markdown","2cbd0d43":"markdown","54a2de7e":"markdown","2f483237":"markdown","f33ab2a8":"markdown","953e22bc":"markdown","e0c7fdc9":"markdown","c251b19c":"markdown","78fe0434":"markdown","bb18a363":"markdown","31e417fb":"markdown","0971d0f6":"markdown","5cc9cd90":"markdown","ca0b82b5":"markdown","d1877162":"markdown","44bf118c":"markdown","c450b592":"markdown","b2a33829":"markdown","1cfc0bfc":"markdown","70967cad":"markdown","9631fce5":"markdown","099f23f5":"markdown","3e174fb4":"markdown","b7ecb889":"markdown","f3cddf7d":"markdown","46126b20":"markdown","3f48f8ff":"markdown","97af4a6b":"markdown","5a1560fb":"markdown","1639cd35":"markdown","1cbb256f":"markdown","6771202f":"markdown","30e070ff":"markdown","f7849481":"markdown","db17d3a3":"markdown","e44f6474":"markdown","4e233e5e":"markdown","cd258e7e":"markdown","7fa31eb7":"markdown","719ddcf5":"markdown"},"source":{"0070adf5":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport sklearn\nfrom scipy import stats\n\nimport webbrowser\n\n#import sklearn packages\nfrom sklearn.model_selection import KFold, cross_val_score, RepeatedStratifiedKFold, StratifiedKFold\nfrom sklearn.model_selection import cross_val_predict, train_test_split, GridSearchCV\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, PowerTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.dummy import DummyClassifier\n\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.compose import make_column_transformer\n\nimport optuna\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\n#plotly packages\nimport plotly\nimport cufflinks as cf\nfrom plotly.offline import init_notebook_mode, iplot\n\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport plotly.offline as py\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\n\ninit_notebook_mode(connected=True)\ncf.go_offline()\n","b5c3f040":"#pd.set_option('display.expand_frame_repr',False)\n#there are 918 rows and max_rows is set to 1000(>918)to display whole dataset as output, \n#if max_rows is set to less than 918 then whole dataset won't be displayed.\npd.set_option('display.max_rows', 900)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \ndf = pd.read_csv(\"..\/input\/heart-failure-prediction\/heart.csv\")\ndf\n","0743816d":"#returns number of unique values in each column\ndf.iloc[:,:11].nunique()","5489dbd7":"df.info()","87cf251f":"numerical = df.drop(\"HeartDisease\", axis=1).select_dtypes(include=\"number\").columns\ncategorical = df.drop(\"HeartDisease\", axis=1).select_dtypes(include=\"object\").columns\n\nprint(\"Numerical columns : \", numerical)\nprint(\"Categorical features : \", categorical)","26d0e90b":"#target variable\ny = df[\"HeartDisease\"]\ny.value_counts()","f073265c":"df[\"HeartDisease\"].iplot(kind=\"hist\")","76f2c1d4":"pd.set_option('display.max_columns', 50)\ndf.groupby(\"HeartDisease\")[numerical].describe()","4aad7d03":"df[numerical].iplot(kind=\"hist\", subplots=True)","ad7b39be":"for i in numerical:\n    df[i].iplot(kind=\"box\", title=i, boxpoints=\"all\", color=\"lightseagreen\")","ccd23166":"index = 0\nplt.figure(figsize=(20,12))\nfor feature in numerical:\n    index += 1\n    plt.subplot(2, 3, index)\n    sns.boxplot(data=df, x=\"HeartDisease\", y=feature)","467603e1":"sns.pairplot(df, hue=\"HeartDisease\", corner=True)","1660d28b":"df[numerical].skew()","847712ba":"df[numerical].kurtosis()","52c94f2b":"#merging HeartDisease column with dataframe consisting of numerical columns\ndf_num = pd.concat([df[numerical], df[\"HeartDisease\"]], axis=1)\n#np.triu returns upper triangle\nmatrix = np.triu(df_num.corr())\n\nplt.figure(figsize=(18,12))\n# mask - If passed, data will not be shown in cells where mask is True. Cells with missing values are automatically masked\nsns.heatmap(df_num.corr(), mask = matrix, annot = True, cmap = \"RdYlBu\", fmt = '.2f')\n","57f8ee4d":"df[categorical]","af858f64":"df[categorical].iplot(kind=\"hist\", subplots=True)","69bc58ae":"index = 0\nplt.figure(figsize=(30, 300))\nfor cat_feat in categorical:\n    for num_feat in numerical:\n        index += 1\n        plt.subplot(numerical.shape[0] * categorical.shape[0], 1, index)\n        sns.swarmplot(data=df, x=cat_feat, y=num_feat, hue=\"HeartDisease\", palette=\"husl\")\n","bcd89ee5":"df.groupby(\"Sex\")[\"HeartDisease\"].describe()","20cf49dd":"fig = px.histogram(df, color=\"HeartDisease\", x=\"Sex\")\nfig.show()","a6c459f6":"df.groupby(\"ChestPainType\")[\"HeartDisease\"].describe()","0aa260d5":"fig = px.histogram(df, x=\"ChestPainType\", color=\"HeartDisease\")\nfig.show()","4ce19c05":"df.groupby(\"RestingECG\")[\"HeartDisease\"].describe()","76a0f181":"df.groupby(\"ExerciseAngina\")[\"HeartDisease\"].describe()","d9a7493e":"df.groupby(\"ST_Slope\")[\"HeartDisease\"].describe()","964b9f7b":"X = df.drop(\"HeartDisease\", axis=1)\ny = df[\"HeartDisease\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nohe = OneHotEncoder()\nct = make_column_transformer((ohe, categorical), remainder=\"passthrough\")\nct","83becce2":"accuracy = []\nmodel_names = []\n\nmodel = DummyClassifier(strategy=\"constant\", constant=1)\npipe = make_pipeline(ct, model) \n#make_pipeline takes transformers and an estimator\/predictor. It applies fit_transform to all transformers and fit to estimator.\n#It provides you with an easy way for chaining multiple steps of ML.\npipe.fit(X_train, y_train)\n\ny_pred = pipe.predict(X_test)\n\naccuracy.append(round(accuracy_score(y_test, y_pred), 4))\nmodel_names.append(\"DummyClassifier\")\n\npd.DataFrame({\"Accuracy\" : accuracy}, index=model_names)\n","be118515":"accuracy = []\nmodel_names = [\"Logistic\", \"LDA\", \"SVM\", \"KNN\"]\n\nlr = LogisticRegression(solver=\"liblinear\")\nlda = LinearDiscriminantAnalysis()\nsvm = SVC()\nknn = KNeighborsClassifier()\n\nmodels = [lr, lda, svm, knn]\n\nfor model in models:\n    pipe = make_pipeline(ct, model)\n    pipe.fit(X_train, y_train)\n    y_pred = pipe.predict(X_test)\n    accuracy.append(round(accuracy_score(y_test, y_pred), 4))\n    \npd.DataFrame({\"Accuracy\" : accuracy}, index=model_names)\n","2118b69c":"ohe = OneHotEncoder()\n#Standard scaler standardise data such as mean = 0 and covariance = 1. For all x in X, x* = (x - mean)\/covariance. \nsc = StandardScaler()\nct1 = make_column_transformer((ohe, categorical), (sc, numerical))\n\naccuracy = []\nmodel_names = [\"Logistic_sc\", \"LDA_sc\", \"SVM_sc\", \"KNN_sc\"]\n\nlr_sc = LogisticRegression(solver=\"liblinear\")\nlda_sc = LinearDiscriminantAnalysis()\nsvm_sc = SVC()\nknn_sc = KNeighborsClassifier()\n\nmodels = [lr_sc, lda_sc, svm_sc, knn_sc]\n\nfor model in models:\n    pipe = make_pipeline(ct1, model)\n    pipe.fit(X_train, y_train)\n    y_pred = pipe.predict(X_test)\n    accuracy.append(round(accuracy_score(y_test, y_pred), 4))\n\npd.DataFrame({\"Accuracy\": accuracy}, index=model_names)\n","1fe01399":"accuracy_ct = []\naccuracy_ct1 = []\nmodel_names = [\"AdaBoost\", \"GradientBoost\", \"RandomForest\", \"ExtraTrees\"]\n\nada = AdaBoostClassifier(random_state=42)\ngb = GradientBoostingClassifier(random_state=42)\nrf = RandomForestClassifier(random_state=42)\net = ExtraTreesClassifier(random_state=42)\n\nmodels = [ada, gb, rf, et]\n\nfor model in models:\n    pipe = make_pipeline(ct, model)\n    pipe1 = make_pipeline(ct1, model)\n    \n    pipe.fit(X_train, y_train)\n    pipe1.fit(X_train, y_train)\n\n    y_pred_ct = pipe.predict(X_test)\n    y_pred_ct1 = pipe1.predict(X_test)\n    \n    accuracy_ct.append(round(accuracy_score(y_test, y_pred_ct), 4))\n    accuracy_ct1.append(round(accuracy_score(y_test, y_pred_ct1), 4))\n    \npd.DataFrame({\"Accuracy_ct\" : accuracy_ct, \"Accuracy_ct1\" : accuracy_ct1}, index=model_names)","6d880d7d":"accuracy_ct = []\naccuracy_ct1 = []\nmodel_names = [\"XGBoost\", \"LightGBM\"]\n\nxgb = XGBClassifier(random_state=42, use_label_encoder=False)\nlgbm = LGBMClassifier(random_state=42)\n\nmodels = [xgb, lgbm]\n\nfor model in models:\n    pipe = make_pipeline(ct, model)\n    pipe1 = make_pipeline(ct1, model)\n\n    pipe.fit(X_train, y_train)\n    pipe1.fit(X_train, y_train)\n\n    y_pred_ct = pipe.predict(X_test)\n    y_pred_ct1 = pipe1.predict(X_test)\n\n    accuracy_ct.append(round(accuracy_score(y_test, y_pred_ct), 4))\n    accuracy_ct1.append(round(accuracy_score(y_test, y_pred_ct1), 4))\n\npd.DataFrame({\"Accuracy_ct\": accuracy_ct, \"Accuracy_ct1\": accuracy_ct1}, index=model_names)","89a23d2b":"accuracy = []\nmodel_names = [\"CatBoost_default\"]\n\ncategorical_feat_indices = np.where(X.dtypes == np.object)[0]\n\ncb = CatBoostClassifier(random_state=42)\ncb.fit(X_train, y_train, cat_features=categorical_feat_indices, eval_set=(X_test, y_test))\n\ny_pred = cb.predict(X_test)\naccuracy.append(round(accuracy_score(y_test, y_pred), 4))\n\npd.DataFrame({\"Accuracy\" : accuracy}, index=model_names)","ce22c622":"pd.set_option('display.max_rows', 100)\n\n#we create 'objective' function with 'trial' as parameter which is passed to object created using optuna.create_study()\n#'objective' returns a loss function or accuracy or cost function which is to be maximized or minimized.\ndef objective(trial):\n    #param is to be passed as paramters to CatBoostClassifier\n    param = {\n        #trial.suggest_{categorical,float,int} randomly selects a {feature, a float between low and high, a int between low and high}\n        \"loss_function\": trial.suggest_categorical(\"loss_function\", [\"Logloss\", \"CrossEntropy\"]),\n        \"rsm\": trial.suggest_float(\"rsm\", 0.01, 0.5),\n        \"depth\": trial.suggest_int(\"depth\", 1, 12),\n        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n        \"bootstrap_type\": trial.suggest_categorical(\"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]),\n    }\n    #additional paramters relating to bootstrap_type\n    if param[\"bootstrap_type\"] == \"Bayesian\":\n        param[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n    elif param[\"bootstrap_type\"] == \"Bernoulli\":\n        param[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n\n    #creating a CatBoostClassifier using param as parameters\n    cat_cls = CatBoostClassifier(**param)\n\n    cat_cls.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False, cat_features=categorical_feat_indices, early_stopping_rounds=100)\n\n    preds = cat_cls.predict(X_test)\n    pred_labels = np.rint(preds)\n    accuracy = accuracy_score(y_test, pred_labels)\n    return accuracy\n\n#create a Study object\nstudy = optuna.create_study(direction=\"maximize\")\n#optimize the 'objective' function and specify the number of trials using n_trials\nstudy.optimize(objective, n_trials=50)\n\n#printing values of param corresponding to best trial\nprint(\"Number of finished trials: {}\".format(len(study.trials)))\n\nprint(\"Best trial:\")\ntrial = study.best_trial\n\nprint(\"  Value: {}\".format(trial.value))\n\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))\n","434eece6":"accuracy = []\nmodel_names = ['Catboost_tuned']\n\nmodel = CatBoostClassifier(random_state=0, verbose=False, **trial.params)\n\nmodel.fit(X_train, y_train, cat_features=categorical_feat_indices, eval_set=(X_test, y_test))\ny_pred = model.predict(X_test)\naccuracy.append(round(accuracy_score(y_test, y_pred), 4))\n\nprint(classification_report(y_test, y_pred))\n\npd.DataFrame({\"Accuracy\": accuracy}, index=model_names)\n","89712bbc":"feature_importance = np.array(model.get_feature_importance())\nfeature = np.array(X.columns)\n\nfig = px.bar(x=feature_importance, y=feature, title=\"Catboost_Feature_Importance\")\nfig.update_layout(xaxis_title=\"feature_importance\", yaxis_title=\"features\", yaxis={\"categoryorder\":\"total ascending\"})\nfig.show()","bbd2fc4a":"inference - people having heartdisease have higher mean FastingBS and mean Oldpeak than people not having heart disease. Also, shockingly mean Cholestrol level of people having heart disease is lower than people not having heart disease.","2cbd0d43":"### **Plotting Feature Importance using Tuned Catboost Model**","54a2de7e":"inference - FastingBS has only 2 distinct values and thus, it is skewed. So, we can safely ignore it. On the other hand, Oldpeak is positively skewed. The rest of data is pretty much symmetrical.","2f483237":"### **Plotting pair plot for df using HeartDisease as hue**","f33ab2a8":"### **Using Ensemble Models to train data**\n**(AdaBoost, GradientBoost, RandomForest and ExtraTrees)**","953e22bc":"### **Plotting heatmap of correlation matrix of numerical features of df (including HeartDisease)**","e0c7fdc9":"### **Using XGBoosting and LightGBM to train models**","c251b19c":"inference - except for people with Up ST_Slope, others are at a much higher risk for having a heat disease","78fe0434":"### **ExtraTrees** (Extremely Randomized Forest)\n\nOVERVIEW - This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\\\nEach Decision Tree in the Extra Trees Forest is constructed from the original training sample. Then, at each test node, Each tree is provided with a random sample of k features from the feature-set from which each decision tree must select the best feature to split the data based on some mathematical criteria (typically the Gini Index). This random sample of features leads to the creation of multiple de-correlated decision trees.\n\n`class sklearn.ensemble.ExtraTreesClassifier(n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=False, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)`\n<br\/>\n","bb18a363":"**Correlation** is a statistical measure that indicates the extent to which two or more variables fluctuate together. In simple terms, it tells us how much does one variable changes for a slight change in another variable. It may take positive, negative and zero values depending on the direction of the change. A high correlation value between a dependent variable and an independent variable indicates that the independent variable is of very high significance in determining the output.","31e417fb":"## **Attribute information**\n>1. Age: age of the patient [years]\n>2. Sex: sex of the patient [M: Male, F: Female]\n>3. ChestPainType: chest pain type [TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic]\n>4. RestingBP: resting blood pressure [mm Hg]\n>5. Cholesterol: serum cholesterol [mm\/dl]\n>6. FastingBS: fasting blood sugar [1: if FastingBS > 120 mg\/dl, 0: otherwise]\n>7. RestingECG: resting electrocardiogram results [Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria]\n>8. MaxHR: maximum heart rate achieved [Numeric value between 60 and 202]\n>9. ExerciseAngina: exercise-induced angina [Y: Yes, N: No]\n>10. Oldpeak: oldpeak = ST [Numeric value measured in depression]\n>11. ST_Slope: the slope of the peak exercise ST segment [Up: upsloping, Flat: flat, Down: downsloping]\n>12. HeartDisease: output class [1: heart disease, 0: Normal]","0971d0f6":"### **RandomForest**\nOVERVIEW - A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled with the max_samples parameter if bootstrap=True (default), otherwise the whole dataset is used to build each tree.\n\n`class sklearn.ensemble.RandomForestClassifier(n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)`\n<br\/>\n> * **criterion** : {\u201cgini\u201d, \u201centropy\u201d}, default=\u201dgini\u201d\\\nThe function to measure the quality of a split. Supported criteria are \u201cgini\u201d for the Gini impurity and \u201centropy\u201d for the information gain. Note: this parameter is tree-specific.\n> * **bootstrap** : bool, default=True\\\nWhether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree.\n> * **max_samples** : int or float, default=None\\\nIf bootstrap is True, the number of samples to draw from X to train each base estimator.\\\n1.If None (default), then draw X.shape[0] samples.\\\n2.If int, then draw max_samples samples.\\\n3.If float, then draw max_samples * X.shape[0] samples. Thus, max_samples should be in the interval (0.0, 1.0].","5cc9cd90":"inference - There aren't any missing or null values in dataset.","ca0b82b5":"inference - men has 63.17% and women has 25.91% chances of having a heart disease.","d1877162":"### **AdaBoost**\n`class sklearn.ensemble.AdaBoostClassifier(base_estimator=None, $*$, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=None)` \n<br\/>\n> * **base_estimator** : object, default=None\\\nThe base estimator from which the boosted ensemble is built. Support for sample weighting is required, as well as proper classes_ and n_classes_ attributes. If None, then the base estimator is DecisionTreeClassifier initialized with max_depth=1.\n> * **algorithm** : {\u2018SAMME\u2019, \u2018SAMME.R\u2019}, default=\u2019SAMME.R\u2019\\\nIf \u2018SAMME.R\u2019 then use the SAMME.R real boosting algorithm. base_estimator must support calculation of class probabilities. If \u2018SAMME\u2019 then use the SAMME discrete boosting algorithm. The SAMME.R algorithm typically converges faster than SAMME, achieving a lower test error with fewer boosting iterations.","44bf118c":"### **Accuracy Report for fine tuned Catboost using Optuna**","c450b592":"### **Grouping dataset by categorical features to obtain insights on HeartDisease.**","b2a33829":"### **Plotting histograms for each numerical column**","1cfc0bfc":"# **Model Selection and Training**","70967cad":"### **Loading Datatset**","9631fce5":"### **Using default CatBoostClassifier for training model**","099f23f5":"### **Importing Relevant Libraries**","3e174fb4":"### **Kurtosis** are of three types:\n\n> 1. **Mesokurtic:** When the tails of the distibution is similar to the normal distribution then it is mesokurtic. The kutosis for normal distibution is 3.\n\n> 2. **Leptokurtic:** If the kurtosis is greater than 3 then it is leptokurtic. In this case, the tails will be heaviour than the normal distribution which means lots of outliers are present in the data. It can be recognized as thin bell shaped distribution with peak higher than normal distribution.\n\n> 3. **Platykurtic:** Kurtosis will be less than 3 which implies thinner tail or lack of outliers than normal distribution.In case of platykurtic, bell shaped distribution will be broader and peak will be lower than the mesokurtic.\n\nHair et al. (2010) and Bryne (2010) argued that data is considered to be normal if Skewness is between \u20102 to +2 and Kurtosis is between \u20107 to +7.\n\nMulti-normality data tests are performed using leveling asymmetry tests (skewness < 3), (Kurtosis between -2 and 2) and Mardia criterion (< 3). Source Chemingui, H., & Ben lallouna, H. (2013).\n\nSkewness and kurtosis index were used to identify the normality of the data. The result suggested the deviation of data from normality was not severe as the value of skewness and kurtosis index were below 3 and 10 respectively (Kline, 2011). Source Yadav, R., & Pathak, G. S. (2016).","b7ecb889":"### **Optimizing Catboost model using Optuna**","f3cddf7d":"inference - people with ExerciseAngina are very much likely to have heart disease -- 85.17%","46126b20":"make_column_transformer -- 'remainder'\n\n>**remainder{\u2018drop\u2019, \u2018passthrough\u2019} or estimator, default=\u2019drop\u2019 :**\\\n\\\nBy default, only the specified columns in transformers are transformed and combined in the output, and the non-specified columns are dropped. (default of 'drop'). By specifying remainder='passthrough', all remaining columns that were not specified in transformers will be automatically passed through. This subset of columns is concatenated with the output of the transformers. By setting remainder to be an estimator, the remaining non-specified columns will use the remainder estimator. The estimator must support fit and transform.","3f48f8ff":"**pd.set_option()** is used to set of number of max rows and columns to display in output editor.","97af4a6b":"### **Splitting columns into numerical and categorical features**","5a1560fb":"inference - data is almost balanced.","1639cd35":"### **CatBoost** (Category Boosting)\n\nOVERVIEW -\\\nCatBoost is an algorithm for gradient boosting on decision trees. It is a readymade classifier in scikit-learn\u2019s conventions terms that would deal with categorical features automatically.\\\nWe can use CatBoost without any explicit pre-processing to convert categories into numbers. CatBoost converts categorical values into numbers using various statistics on combinations of categorical features and combinations of categorical and numerical features.\\\nIt reduces the need for extensive hyper-parameter tuning and lower the chances of overfitting also which leads to more generalized models.","1cbb256f":"### **Plotting swarmplots for each x=categorical_feature v\/s y=numerical_feature with HeartDisease as hue.**","6771202f":"### **GradientBoost** *(it is used to minimize bias error of the model.)*\n\nOVERVIEW - GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.\n\n`class sklearn.ensemble.GradientBoostingClassifier($*$, loss='deviance', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)`\n<br\/>\n> * **loss** : {\u2018deviance\u2019, \u2018exponential\u2019}, default=\u2019deviance\u2019\\\nThe loss function to be optimized. \u2018deviance\u2019 refers to deviance (= logistic regression) for classification with probabilistic outputs. For loss \u2018exponential\u2019 gradient boosting recovers the AdaBoost algorithm.\n> * **subsample** : float, default=1.0\\\nThe fraction of samples to be used for fitting the individual base learners. If smaller than 1.0 this results in Stochastic Gradient Boosting. subsample interacts with the parameter n_estimators. <u>Choosing subsample < 1.0 leads to a reduction of variance and an increase in bias.<\/u>\n> * **criterion** : {\u2018friedman_mse\u2019, \u2018squared_error\u2019}, default=\u2019friedman_mse\u2019\\\nThe function to measure the quality of a split. Supported criteria are \u2018friedman_mse\u2019 for the mean squared error with improvement score by Friedman, \u2018squared_error\u2019 for mean squared error. The default value of \u2018friedman_mse\u2019 is generally the best as it can provide a better approximation in some cases. \n> * **min_impurity_decrease** : float, default=0.0\\\nA node will be split if this split induces a decrease of the impurity greater than or equal to this value.\\\nThe weighted impurity decrease equation is the following:\\\n&emsp;&emsp; *N_t* \/ *N* * *(impurity* - *N_t_R* \/ *N_t* * *right_impurity*\n                    - *N_t_L* \/ *N_t* * *left_impurity)*\\\nwhere *N* is the total number of samples, *N_t* is the number of samples at the current node, *N_t_L* is the number of samples in the left child, and *N_t_R* is the number of samples in the right child.\\\n*N*, *N_t*, *N_t_R* and *N_t_L* all refer to the weighted sum, if *sample_weight* is passed.\n> * **init** : estimator or \u2018zero\u2019, default=None\\\nAn estimator object that is used to compute the initial predictions. init has to provide fit and predict_proba. If \u2018zero\u2019, the initial raw predictions are set to zero. By default, a DummyEstimator predicting the classes priors is used.\n> * **max_features** : {\u2018auto\u2019, \u2018sqrt\u2019, \u2018log2\u2019}, int or float, default=None\\\nThe number of features to consider when looking for the best split:\\\n1.If int, then consider max_features features at each split.\\\n2.If float, then max_features is a fraction and int(max_features * n_features) features are considered at each split.\\\n3.If \u2018auto\u2019, then max_features=sqrt(n_features).\\\n4.If \u2018sqrt\u2019, then max_features=sqrt(n_features).\\\n5.If \u2018log2\u2019, then max_features=log2(n_features).\\\n6.If None, then max_features=n_features.\\\n<br\/>\n<u>Choosing max_features < n_features leads to a reduction of variance and an increase in bias.<\/u>\nNote: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than max_features features.","30e070ff":"### **Plotting boxplot for each numerical feature**","f7849481":"inference - there isn't much difference between different RestingECG. Still, people with ST are more likely to hae heartdisease.","db17d3a3":"### **Using Classical ML models to train unnormalized data**","e44f6474":"### **Using DummyClassifier to obtain baseline accuracy**","4e233e5e":"inference - People with ASY has highest chances of having a heart disease whereas people with ATA has east chances.","cd258e7e":"### **Plotting histogram for categorical features**","7fa31eb7":"`sklearn.dummy.DummyClassifier($*$, strategy='prior', random_state=None, constant=None)`\n\n> DummyClassifier is a classifier that makes predictions using simple rules.\\\n> This classifier is useful as a simple baseline to compare with other (real) classifiers.\n\nStrategies :\\\n&emsp;&emsp;&emsp; 1. \"most_frequent\" : always predicts the most frequent label in the training set.\\\n&emsp;&emsp;&emsp; 2. \"uniform\" : generates predictions uniformly at random.\\\n&emsp;&emsp;&emsp; 3. \"constant\" : always predicts a constant label that is provided by the user. This is useful for metrics that evaluate a non-majority class.\\\n\nconstant : The explicit constant as predicted by the \u201cconstant\u201d strategy. This parameter is useful only for the \u201cconstant\u201d strategy.","719ddcf5":"### **Using Classical ML models to train scaled data**"}}