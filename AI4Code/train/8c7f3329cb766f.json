{"cell_type":{"fc84449c":"code","742a8b4b":"code","989ae28b":"code","9826b701":"code","2a17a5ab":"code","b12aa21e":"code","18480d76":"code","d96ef8e3":"code","7593bbf6":"code","da153139":"code","e78553d5":"code","22f7a82d":"code","90991602":"code","fdb98f08":"code","26e80917":"code","14b0662c":"code","924a4d21":"code","4fe406c6":"code","bd6518b3":"code","20ac77bb":"code","ad1c6ad1":"code","00b1a8cf":"code","4d625ec8":"markdown","0e448770":"markdown","fda71909":"markdown","67dcc288":"markdown","fe8bcf46":"markdown","315c6922":"markdown","96690a54":"markdown","53e4a183":"markdown","09c31c11":"markdown","97b79cde":"markdown","059b795c":"markdown"},"source":{"fc84449c":"#Imputs\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.pipeline import Pipeline","742a8b4b":"#Read the data\nX = pd.read_csv('..\/input\/iowa-house-prices\/train.csv', index_col = 'Id')\nX_test = pd.read_csv('..\/input\/iowa-house-prices\/test.csv', index_col ='Id')\n\n#Filter from target column null values\nX.dropna(axis = 0, subset = ['SalePrice'], inplace = True)\ny = X['SalePrice']\n\n#Filter out the target column from X dataset\nX.drop(axis = 1, labels = ['SalePrice'], inplace = True)","989ae28b":"#Verify if there are the same number of columns in both test and train data\nprint(X.shape)\nprint((X.columns == X_test.columns).sum())","9826b701":"X.columns","2a17a5ab":"# MoSold, YrSOld, SaleType, SaleCondition won't be available for a prediction for the new house\n#so these columns will be dropped\nleakage_columns = ['MoSold', 'YrSold', 'SaleType', 'SaleCondition']\nX.drop(labels = leakage_columns, axis = 1, inplace = True)\nX_test.drop(labels = leakage_columns, axis = 1, inplace= True)","b12aa21e":"numerical_cols = [col for col in X.columns if X[col].dtype in ['int64', 'float64']]\n\ncols_with_nulls = X[numerical_cols].isnull().sum()\nprint(cols_with_nulls[cols_with_nulls > 0])","18480d76":"sns.set_style('whitegrid')\ndf = X[['LotFrontage', 'MasVnrArea', 'GarageYrBlt']].dropna(axis = 0)","d96ef8e3":"#we'll impute the mode, as the distributions is asymetric and the mean is influenced by outliers\nplt.figure(figsize = (12,4))\nsns.distplot(a = df['LotFrontage'], bins = 30, norm_hist=False, kde=True, color = 'blue')","7593bbf6":"#we'll use most frequent, as the distribution is strongly asymetric to the right\nplt.figure(figsize = (12,4))\nsns.distplot(a = df['MasVnrArea'], bins = 30, norm_hist=False, kde=True, color = 'purple')","da153139":"#for this distribution mean should work just fine :)\nplt.figure(figsize = (12,4))\nsns.distplot(a = df['GarageYrBlt'], bins = 30, norm_hist=False, kde=True, color = 'orange')","e78553d5":"#There are only 3 columns with missing values, and they are small in number, so we'll apply Simple Imputation\nfrom sklearn.impute import SimpleImputer\n\nnumerical_cols_median = ['LotFrontage']\nnumerical_transformer_median = SimpleImputer(strategy = 'median')\n\nnumerical_cols_mod = ['MasVnrArea']\nnumerical_transformer_mod = SimpleImputer(strategy = 'most_frequent')\n\nnumerical_cols_mean = ['GarageYrBlt']\nnumerical_transformer_mean = SimpleImputer(strategy = 'mean')\n\nnumerical_cols_remain = set(numerical_cols) - set(numerical_cols_mean) - set(numerical_cols_median) - set(numerical_cols_mod)\nnumerical_cols_remain = list(numerical_cols_remain)","22f7a82d":"categorical_cols = [col for col in X.columns if X[col].dtype == 'object']\ncols_with_nulls_categs = X[categorical_cols].isnull().sum()\ncols_with_nulls_categs[cols_with_nulls_categs > 0]","90991602":"#Because there are a few columns with too many missing values, we'll filter them out from the data\nX.drop(labels = ['Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature'], axis = 1, inplace = True)\nX_test.drop(labels = ['Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature'], axis = 1, inplace = True)","fdb98f08":"#Redo categorical_cols\ncategorical_cols = [col for col in X.columns if X[col].dtype == 'object']\nX.shape","26e80917":"from sklearn.preprocessing import OneHotEncoder\n\ncateg_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy = 'most_frequent')),\n                                         ('onehot', OneHotEncoder(handle_unknown = 'ignore'))])","14b0662c":"from sklearn.compose import ColumnTransformer\npreprocessor = ColumnTransformer(transformers=[('num_median', numerical_transformer_median, numerical_cols_median),\n                                               ('num_mod', numerical_transformer_mod, numerical_cols_mod),\n                                               ('num_mean', numerical_transformer_mean, numerical_cols_mean),\n                                               ('num_rest', numerical_transformer_mean, numerical_cols_remain),\n                                              ('cat', categ_transformer, categorical_cols)])","924a4d21":"#split the training data into train & valid\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.3, random_state = 0)","4fe406c6":"from xgboost import XGBRegressor\nxgboost_1 = XGBRegressor(learning_rate = 0.05, n_estimators=1000, random_state=0)\npipeline_1 = Pipeline(steps=[('preprocessor', preprocessor),\n                            ('model', xgboost_1)])\npipeline_1.fit(X_train, y_train)\npreds_1 = pipeline_1.predict(X_valid)\nmae = mean_absolute_error(y_valid, preds_1)\nprint('MAE 1:', mae)","bd6518b3":"X_train_prep = preprocessor.fit_transform(X_train)","20ac77bb":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = {'learning_rate' : [0.1, 0.08, 0.05, 0.03, 0.01], 'n_estimators' : [50, 100, 200, 500, 700, 1000]}\ngrid = GridSearchCV(XGBRegressor(), param_grid, cv = 5, verbose=5)\n\ngrid.fit(X_train_prep, y_train)","ad1c6ad1":"print('Best params:', grid.best_params_)\nprint('Best estim:', grid.best_estimator_)\nprint('Best score:', grid.best_score_)","00b1a8cf":"xgboost_2 = XGBRegressor(learning_rate=0.01, n_estimators= 1000)\n\npipeline_2 = Pipeline(steps=[('preprocessor', preprocessor),\n                            ('model', xgboost_2)])\n\npipeline_2.fit(X_train, y_train)\n\npreds_2 = pipeline_2.predict(X_valid)\nmae = mean_absolute_error(y_valid, preds_2)\nprint('MAE 2:', mae)","4d625ec8":"Well, this is it.\nLoved this project!","0e448770":"Well, this is my personal view for predicting the Sale Price of a houses in Iowa.\nAs this being my very first machine learning problem (and also Competition), I'm really excited of how this turned out.\n\nAs I learn along, maybe in the future I'll be able to bring this kernel to absolute perrrfection. ^^","fda71909":"**Data Setup**","67dcc288":"Model 1: XGBoost","fe8bcf46":"XGBoost (with GridSearch), finding the best params[](http:\/\/)","315c6922":"**Data Preparation: Numerical Data**","96690a54":"Bundle Preprocessing","53e4a183":"Data Preparation: Categorical Data","09c31c11":"Model 2: XGBoost with best Params","97b79cde":"Data Validation","059b795c":"**Check for Leakage**"}}