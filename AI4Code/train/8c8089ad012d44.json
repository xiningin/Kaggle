{"cell_type":{"a3bfea60":"code","b45ec1d0":"code","d168e160":"code","4db06b1d":"code","0264f945":"code","4fd9f5bd":"code","589be76c":"code","4a4a3430":"code","1fe39a99":"code","73cff64e":"code","1c9133df":"code","da6ab38e":"markdown","10621b28":"markdown"},"source":{"a3bfea60":"import csv\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\nfrom tensorflow import keras\nfrom keras import layers\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nstopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]","b45ec1d0":"# read the csv file\n\ndf = pd.read_csv(\"..\/input\/bbcnewsarchive\/bbc-news-data.csv\", sep=\"\\t\")\n\nprint(df)\n\ndf[\"category\"].hist()","d168e160":"# shuffle the dataframe to evenly distribute the labels\n\ndf = df.sample(frac=1).reset_index(drop=True)\ndf","4db06b1d":"content = []\nlabels = []\n\nfor label in df.category:\n    labels.append(label)\n    \nfor con in df.content:\n    for word in stopwords:\n        token = \" \" + word + \" \"\n        con = con.replace(token, \" \")\n        con = con.replace(\" \", \" \")\n    content.append(con)\n\nprint(len(content))\nprint(len(labels))\nprint(\"\\nContent:\", content[0])\nprint(\"\\nLabel:\", labels[0])","0264f945":"# split the dataset into training set and test set\n\ntrain_content, test_content = content[:1900], content[1900:]\ntrain_labels, test_labels = labels[:1900], labels[1900:]\n\ntrain_content = np.array(train_content)\ntest_content = np.array(test_content)\n\ntrain_labels = np.array(train_labels)\ntest_labels = np.array(test_labels)\n\nprint(len(train_content))\nprint(len(train_labels))\nprint(len(test_content))\nprint(len(test_labels))","4fd9f5bd":"# check the distribution of labels in the training set and test set\n\nunique_train_content, number_train_content = np.unique(train_labels, return_counts=True)\n\nprint(\"Training set labels:\")\nprint(unique_train_content)\nprint(number_train_content)\n\nunique_test_content, number_test_content = np.unique(test_labels, return_counts=True)\n\nprint(\"\\nTest set labels:\")\nprint(unique_test_content)\nprint(number_test_content)","589be76c":"# tokenize the content\n\nvocab_size = 10000\nembedding_dim = 32\nmax_len = 200\ntrunc_type = \"post\"\noov_tok = \"<OOV>\"\n\ntokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(train_content)\n\nword_index = tokenizer.word_index\n\nsequences = tokenizer.texts_to_sequences(train_content)\npadded = pad_sequences(sequences, maxlen=max_len, truncating=trunc_type)\n\ntest_sequences = tokenizer.texts_to_sequences(test_content)\ntest_padded = pad_sequences(test_sequences, maxlen=max_len, truncating=trunc_type)\n\nprint(test_padded.shape)","4a4a3430":"# tokenize the labels\n\nlabel_tokenizer = Tokenizer()\nlabel_tokenizer.fit_on_texts(labels)\n\nlabel_index = label_tokenizer.word_index\n\nlabel_sequences = np.array(label_tokenizer.texts_to_sequences(train_labels))\n\ntest_label_sequences = np.array(label_tokenizer.texts_to_sequences(test_labels))\n\nprint(label_sequences.shape)\nprint(test_label_sequences.shape)","1fe39a99":"# define an NN model\n\nmodel = keras.Sequential([layers.Embedding(vocab_size, embedding_dim, input_length=max_len),\n                        \n                         layers.GlobalAveragePooling1D(), #simpler and faster than Flatten()\n                         layers.Dense(128, activation=\"relu\"),\n                         layers.Dense(6, activation=\"softmax\")])\n\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n\nmodel.summary()\nkeras.utils.plot_model(model)","73cff64e":"# train the model\n\nnum_epochs = 10\n\nhistory = model.fit(padded,\n                   label_sequences,\n                   epochs=num_epochs,\n                   validation_data = (test_padded, test_label_sequences))","1c9133df":"# plot accuracy and loss\n\nacc = history.history[\"accuracy\"]\nval_acc = history.history[\"val_accuracy\"]\nloss = history.history[\"loss\"]\nval_loss = history.history[\"val_loss\"]\n\nepochs = range(1, len(acc) + 1)\n\n# accuracy\n\nplt.plot(epochs, acc, \"b\", label=\"Training accuracy\")\nplt.plot(epochs, val_acc, \"b--\", label=\"Validation accuracy\")\nplt.title(\"Training and validation accuracy\")\nplt.legend()\nplt.show()\n\n# loss\n\nplt.plot(epochs, loss, \"r\", label=\"Training loss\")\nplt.plot(epochs, val_loss, \"r--\", label=\"Validation loss\")\nplt.title(\"Training and validation loss\")\nplt.legend()\nplt.show()","da6ab38e":"### The Result \n\n> **After training for 10 epochs, the model was able to reach 100% accuracy on training set and near 100% on the validation set.**","10621b28":"## BBC News Categorization using Keras\n\nIn this notebook, I am going to train a model to categorize BBC news using embedding technique.\n\nThe dataset contains 5 news categories namely: business, entertainment, politics, sport, and tech. \n\nThere are 2225 rows and 4 columns. However, only 2 columns (category and content) are used.\n \n![](https:\/\/images.spot.im\/v1\/production\/jyzxethffjsr6xwwz0ky)\n\n*Image Source: Daily Express*"}}