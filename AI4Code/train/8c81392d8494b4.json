{"cell_type":{"39095cef":"code","1fc85425":"code","53ade6a5":"code","01aedb50":"code","c0d4cb8e":"code","16eed2c8":"code","2ad1a699":"code","07fded8b":"code","d03cf9e8":"code","6a38b37e":"code","2c1bc0d4":"code","9d974670":"code","5214ca56":"code","af12e97d":"code","fbb21a5b":"markdown","6a64c92c":"markdown","361ce443":"markdown","700888cb":"markdown","45baa0b2":"markdown","27fdba6e":"markdown","95d7bcc2":"markdown","b0cb40bd":"markdown","934b48f3":"markdown","debcf362":"markdown","f8024e99":"markdown","352f64ea":"markdown","edbc46db":"markdown","467a8e5c":"markdown"},"source":{"39095cef":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","1fc85425":"# !pip install chart_studio\n\nimport re\n# Tutorial about Python regular expressions: https:\/\/pymotw.com\/2\/re\/ import string\nfrom nltk.corpus import stopwords\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport os\nimport nltk\nimport spacy\nimport random\nfrom spacy.util import compounding\nfrom spacy.util import minibatch\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","53ade6a5":"BASE_PATH = '..\/input\/tweet-sentiment-extraction\/'\n\ntrain_df = pd.read_csv(BASE_PATH + 'train.csv')\ntest_df = pd.read_csv( BASE_PATH + 'test.csv')\nsubmission_df = pd.read_csv( BASE_PATH + 'sample_submission.csv')","01aedb50":"train_df = train_df.dropna()","c0d4cb8e":"def save_model(output_dir, nlp, new_model_name):\n    output_dir = f'..\/working\/{output_dir}'\n    if output_dir is not None:        \n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n        nlp.meta[\"name\"] = new_model_name\n        nlp.to_disk(output_dir)\n        print(\"Saved model to\", output_dir)","16eed2c8":"# pass model = nlp if you want to train on top of existing model \n\ndef train(train_data, output_dir, n_iter=20, model=None):\n    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n    \"\"\n    if model is not None:\n        nlp = spacy.load(output_dir)  # load existing spaCy model\n        print(\"Loaded model '%s'\" % model)\n    else:\n        nlp = spacy.blank(\"en\")  # create blank Language class\n        print(\"Created blank 'en' model\")\n    \n    # create the built-in pipeline components and add them to the pipeline\n    # nlp.create_pipe works for built-ins that are registered with spaCy\n    if \"ner\" not in nlp.pipe_names:\n        ner = nlp.create_pipe(\"ner\")\n        nlp.add_pipe(ner, last=True)\n    # otherwise, get it so we can add labels\n    else:\n        ner = nlp.get_pipe(\"ner\")\n    \n    # add labels\n    for _, annotations in train_data:\n        for ent in annotations.get(\"entities\"):\n            ner.add_label(ent[2])\n\n    # get names of other pipes to disable them during training\n    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n    with nlp.disable_pipes(*other_pipes):  # only train NER\n        # sizes = compounding(1.0, 4.0, 1.001)\n        # batch up the examples using spaCy's minibatch\n        if model is None:\n            nlp.begin_training()\n        else:\n            nlp.resume_training()\n\n        for itn in tqdm(range(n_iter)):\n            random.shuffle(train_data)\n            batches = minibatch(train_data, size=compounding(4.0, 500.0, 1.001))    \n            losses = {}\n            for batch in batches:\n                texts, annotations = zip(*batch)\n                nlp.update(\n                    texts,  # batch of texts\n                    annotations,  # batch of annotations\n                    drop=0.5,   # dropout - make it harder to memorise data\n                    losses=losses, \n                )\n            \n            print(\"Losses\", losses)\n    save_model(output_dir, nlp, 'st_ner')","2ad1a699":"def get_model_out_path(sentiment):\n    model_out_path = None\n    if sentiment == 'positive':\n        model_out_path = 'models\/model_pos'\n    elif sentiment == 'negative':\n        model_out_path = 'models\/model_neg'\n    else:\n        model_out_path = 'models\/model_neu'\n    return model_out_path\n    ","07fded8b":"## creating data in spacy data input format\n\ndef get_training_data(sentiment):\n    train_data = []\n    for index, row in train_df.iterrows():\n        if row.sentiment == sentiment:\n            selected_text = row.selected_text\n            text = row.text\n            start = text.find(selected_text)\n            end = start + len(selected_text)\n            train_data.append((text, {\"entities\": [[start, end, 'selected_text']]}))\n    return train_data","d03cf9e8":"sentiment = 'positive'\n\ntrain_data = get_training_data(sentiment)\nmodel_path = get_model_out_path(sentiment)\n\n# for demo purpose i am just training the model for 2 iterations, feel free to experiment.\ntrain(train_data, model_path, n_iter=2, model=None)","6a38b37e":"sentiment = 'negative'\n\ntrain_data = get_training_data(sentiment)\nmodel_path = get_model_out_path(sentiment)\n\n# for demo purpose i am just training the model for 2 iterations, feel free to experiment.\ntrain(train_data, model_path, n_iter=2, model=None)","2c1bc0d4":"sentiment = 'neutral'\n\ntrain_data = get_training_data(sentiment)\nmodel_path = get_model_out_path(sentiment)\n\n# for demo purpose i am just training the model for 2 iterations, feel free to experiment.\ntrain(train_data, model_path, n_iter=2, model=None)","9d974670":"TRAINED_MODELS_BASE_PATH = '..\/input\/tse-spacy-model\/models\/'","5214ca56":"def predict_entities(text, model):\n    doc = model(text)\n    ent_array = []\n    for ent in doc.ents:\n        start = text.find(ent.text)\n        end = start + len(ent.text)\n        new_int = [start, end, ent.label_]\n        if new_int not in ent_array:\n            ent_array.append([start, end, ent.label_])\n    selected_text = text[ent_array[0][0]: ent_array[0][1]] if len(ent_array) > 0 else text\n    return selected_text","af12e97d":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))\n\n\nif TRAINED_MODELS_BASE_PATH is not None:\n    print(\"Loading Models  from \", TRAINED_MODELS_BASE_PATH)\n    model_pos = spacy.load(TRAINED_MODELS_BASE_PATH + 'model_pos')\n    model_neg = spacy.load(TRAINED_MODELS_BASE_PATH + 'model_neg')\n    model_neu = spacy.load(TRAINED_MODELS_BASE_PATH + 'model_neu')\n        \n    jaccard_score = 0\n    for index, row in tqdm(train_df.iterrows(), total=train_df.shape[0]):\n        text = row.text\n        if row.sentiment == 'neutral':\n            jaccard_score += jaccard(predict_entities(text, model_neu), row.selected_text)\n        elif row.sentiment == 'positive':\n            jaccard_score += jaccard(predict_entities(text, model_pos), row.selected_text)\n        else:\n            jaccard_score += jaccard(predict_entities(text, model_neg), row.selected_text) \n        \n    print(f'Average Jaccard Score is {jaccard_score \/ train_df.shape[0]}') ","fbb21a5b":"### 3.3 Traing for `neutral` sentiment","6a64c92c":"### 2.2 Dropping columns with  values","361ce443":"### 3.1 Traing for `positive` sentiment","700888cb":"### 2. Reading Data","45baa0b2":"Note: If you like my work, please, upvote \u263a","27fdba6e":"### 1.3 Posing this problem as NER problem\n\n**What is NER?**\n\n`Named-entity recognition (NER)` (also known as entity identification, entity chunking and entity extraction) is a subtask of information extraction that seeks to locate and classify named entity mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc.\n\n\n**Few import links to visit**\n\n* I have used `spacy` for this purpose\n* Please visit [this](https:\/\/towardsdatascience.com\/custom-named-entity-recognition-using-spacy-7140ebbb3718) to know more about `spacy`\n* Please visit [this](https:\/\/spacy.io\/usage\/training) to read about training custom models using `spacy` ","95d7bcc2":"### 5. Whats Next?\n\n* For submission related part please visit [here](https:\/\/www.kaggle.com\/rohitsingh9990\/spacy-inference)\n* I am going to create a discussion thread for those who are trying to approach this problem as NER. Will share link soon.\n* Will try to update this kernel with more approaches to increase LB score.\n* Suggestions are most welcome.","b0cb40bd":"### 3.2 Traing for `negative` sentiment","934b48f3":"# Tweet Sentiment Extraction\n\n\n## 1. Introduction\n\n* With all of the tweets circulating every second it is hard to tell whether the sentiment behind a specific tweet will impact a company, or a person's, brand for being viral (positive), or devastate profit because it strikes a negative tone. Capturing sentiment in language is important in these times where decisions and reactions are created and updated in seconds. But, which words actually lead to the sentiment description? In this competition you will need to pick out the part of the tweet (word or phrase) that reflects the sentiment.\n\n* The goal of this competition is to extract those word or phrases which determines the sentiment of whole tweet.\n\n> Disclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive.\n\n","debcf362":"### 4. jaccard score on train data","f8024e99":"### 1.2 Competition metric:\n\nThe metric in this competition is the word-level Jaccard score.\nJaccard similarity or intersection over union is defined as size of intersection divided by size of union of two sets. Let\u2019s take example of two sentences:\n\n`Sentence 1: AI is our friend and it has been friendly`\n\n`Sentence 2: AI and humans have always been friendly`\n\nIn order to calculate similarity using Jaccard similarity, we will first perform lemmatization to reduce words to the same root word. In our case, \u201cfriend\u201d and \u201cfriendly\u201d will both become \u201cfriend\u201d, \u201chas\u201d and \u201chave\u201d will both become \u201chas\u201d. Drawing a Venn diagram of the two sentences we get:\n\n![](https:\/\/miro.medium.com\/max\/926\/1*NSK8ERXexyIZ_SRaxioFEg.png)\n\nPlease read [this](https:\/\/medium.com\/@adriensieg\/text-similarities-da019229c894) article for better understanding","352f64ea":"### 1.4. 3 Models Vs 1 Model\n\n* Earlier i have tried using single model for prediction but unfortunately results are not convincing enough, this may be due to lots of wrong labels in the dataset or may be the model was not good enough to generalize.\n\n* Here in this kernel i have used 3 models one for each sentiment category.\n\n* The trained models are available at this [link](https:\/\/www.kaggle.com\/rohitsingh9990\/tse-spacy-model)\n\n* The models are still baseline and can be improved further.\n\n* Using current models you will be able to get `0.628 LB score` which is quite decent, as competition is still in its early stage.","edbc46db":"## 3. Training Model\n\n* To train a model from scratch pass `model` parameter as `None`\n* To resume training of a saved model pass `model` parameter as `some_value`","467a8e5c":"### 1.1 About Tweet Sentiment Extraction Dataset\n\nThe data folder contains following files, all in csv format\n\n**Files**\n* `train.csv` - the training set\n* `test.csv` - the test set\n* `sample_submission.csv` - a sample submission file in the correct format\n\n**Columns**\n* `textID` - unique ID for each piece of text\n* `text` - the text of the tweet\n* `sentiment` - the general sentiment of the tweet\n* `selected_text` - [train only] the text that supports the tweet's sentiment"}}