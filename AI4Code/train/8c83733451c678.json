{"cell_type":{"db2830f7":"code","a9fe12bb":"code","f8ccd928":"code","4f99f085":"code","8c7e1fe7":"code","80d7c394":"code","27f320af":"code","96d78b58":"code","8e4dc855":"code","907b2bfe":"code","d6826623":"code","218322f5":"code","73da8b6f":"code","b2161f3a":"code","2853a315":"code","ad544b77":"code","e0c9976f":"code","451589dc":"code","1080cd10":"code","4af87dfd":"code","e040d38e":"code","f7c95acd":"code","8908d5e9":"code","0dc10ba5":"code","331fcae0":"code","a442fe5b":"code","217dd2d8":"code","68b846b6":"code","9afefdec":"code","b1701a9f":"code","80c5a484":"code","4d1b8719":"code","c6d86299":"code","e669a8f2":"code","4a194d17":"code","3df0954c":"code","f51a6e28":"code","7e22f3eb":"code","df6b89ad":"code","64e63dfe":"code","643ad4d3":"code","d69ca577":"code","ae11b118":"code","5682697d":"code","651ab1c4":"code","5f3faca2":"code","77ff54c8":"code","5f78f7e2":"code","90cabc07":"code","67ea64ef":"code","99b00d93":"code","beb9d1d6":"code","78f6b346":"code","1c3017df":"code","e2f46a8f":"code","a7f538b4":"code","2941d378":"code","890701dd":"code","357d5e46":"code","fc084ea4":"code","6463b964":"code","111579f4":"code","b4b5f466":"code","828098f0":"markdown","44329ceb":"markdown","2f6687be":"markdown","e4126305":"markdown","68370898":"markdown","b9fe535b":"markdown","5f4778d9":"markdown","7b3c38dc":"markdown","bf93e999":"markdown","9fc2b6a8":"markdown","43101858":"markdown","c0a2bbfc":"markdown","2e1a95f1":"markdown","fcb97cde":"markdown","5b5541d5":"markdown","7b0bac78":"markdown","2332fa7c":"markdown","108ad0dc":"markdown","b5f25b06":"markdown","46ef5050":"markdown","21c545d2":"markdown","9048c28f":"markdown","19274429":"markdown","b95f249c":"markdown","c242cb16":"markdown","7a1a21c5":"markdown","34c31f2f":"markdown","b395651e":"markdown","e6bc6bd4":"markdown","88246009":"markdown","627aeb19":"markdown","10c22f0f":"markdown","e76f3881":"markdown","c119f966":"markdown","70926d24":"markdown","e01701ed":"markdown","d6488d75":"markdown","7e0ec996":"markdown","28c5aa7a":"markdown","6697ebff":"markdown","caf65dbe":"markdown","7bedecb4":"markdown","93130396":"markdown","ee2d7ea9":"markdown","bffc709f":"markdown","aad3ee41":"markdown","42fb9866":"markdown","441cb861":"markdown","6cba7c38":"markdown","a58fafdd":"markdown"},"source":{"db2830f7":"# data analysis libraries:\nimport numpy as np\nimport pandas as pd\nimport re\n\n# data visualization libraries:\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# to ignore warnings:\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# to display all columns:\npd.set_option('display.max_columns', None)\n\n#timer\nimport time\nfrom contextlib import contextmanager\n\n# Importing modelling libraries\nfrom sklearn.model_selection import train_test_split,GridSearchCV,cross_val_score,KFold\nfrom sklearn.preprocessing import StandardScaler  \nfrom sklearn.metrics import accuracy_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,VotingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\n\n@contextmanager\ndef timer(title):\n    t0 = time.time()\n    yield\n    print(\"{} done in {:.0f}s\".format(title, time.time() - t0))","a9fe12bb":"# Read train and test data with pd.read_csv():\ntrain = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","f8ccd928":"train.info()","4f99f085":"test.info()","8c7e1fe7":"train.head()","80d7c394":"test.head()","27f320af":"train.iloc[:,1:len(train)].describe([0.01,0.1,0.25,0.5,0.75,0.99]).T","96d78b58":"test.iloc[:,1:len(test)].describe([0.01,0.1,0.25,0.5,0.75,0.99]).T","8e4dc855":"print('There seem to be obvious outlier observations for Fare variable.')","907b2bfe":"    for var in train:\n        if var != 'Survived':\n            if len(list(train[var].unique())) <= 10:\n                    print(pd.DataFrame({'Mean_Survived': train.groupby(var)['Survived'].mean()}), end = \"\\n\\n\\n\")","d6826623":"print('Number of missing values and their percentage for Train and Test sampleS respectively', end = \"\\n\\n\")\nfor df in [train,test]:\n    total = df.isnull().sum().sort_values(ascending=False)\n    percent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\n    print(pd.concat([total,percent], axis=1, keys=['Total','Percent']), end = \"\\n\\n\")","218322f5":"print('There are missing observations for Age, Fare,Embarked and Cabin variables.')","73da8b6f":"train.drop(['Ticket','Embarked'], axis = 1,inplace=True)   \ntest.drop(['Ticket','Embarked'], axis = 1,inplace=True)","b2161f3a":"for d in [train,test]:\n    d['Nicknamed']=d['Name'].apply(lambda x: 1 if '''\"''' in x else 0)\n    d[\"Title\"] = d[\"Name\"].str.extract(' ([A-Za-z]+)\\.', expand=False)","2853a315":"train[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","ad544b77":"train[['Title', 'Age']].groupby(['Title'], as_index=False).mean() ","e0c9976f":"test[['Title', 'Age']].groupby(['Title'], as_index=False).mean()","451589dc":"train.groupby(\"Sex\")['Title'].value_counts()","1080cd10":"for d in [train,test]:\n    d['Title'] = d['Title'].replace(['Lady','Mme','Mlle','Don','Col', 'Major', 'Dona','Countess', 'Sir'], 'Noble')\n    d['Title'] = d['Title'].replace('Ms', 'Mrs')\n    d['Title'] = d['Title'].replace(['Capt','Jonkheer','Rev'], 'Mr')\n    d.loc[d['Sex']=='female', 'Title']=d.loc[d['Sex']=='female', 'Title'].replace('Dr', 'Mrs')\n    d.loc[d['Sex']=='male', 'Title']=d.loc[d['Sex']=='male', 'Title'].replace('Dr', 'Mr')\n    d.drop(['Name'], axis = 1,inplace=True)","4af87dfd":"train[[\"Title\",\"PassengerId\"]].groupby(\"Title\").count()","e040d38e":"test[[\"Title\",\"PassengerId\"]].groupby(\"Title\").count()","f7c95acd":"train[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","8908d5e9":"train.head()","0dc10ba5":"print('New variables are created using Name variable: Title and Nicknamed.')\nprint('Name variable is dropped')","331fcae0":"for d in [train,test]:\n    d[\"Cabin_dummy\"] = d[\"Cabin\"].notnull().astype('int')\n    d.drop(['Cabin'], axis = 1, inplace=True)","a442fe5b":"print('Cabin_dummy variable is created using Cabin variable')\nprint('Cabin variable is dropped')","217dd2d8":"##Create all sample including test and train data\nfull_data=pd.concat([train, test], ignore_index=True)","68b846b6":"sns.boxplot(x = full_data['Fare']);","9afefdec":"#Defining the upper limit as 99% of all data for winsoring its above\nupper_limit = full_data['Fare'].quantile(0.99)\nprint('Outlier treatment starts...')\nprint('Repress the Fare variable at maximum to %99 value:','%.2f'% upper_limit )","b1701a9f":"for d in [train,test,full_data]:\n    d.loc[d['Fare'] > upper_limit,'Fare'] = upper_limit","80c5a484":"sns.boxplot(x = full_data['Fare']);","4d1b8719":"print('Missing value treatment starts...')","c6d86299":"train[\"Age\"].fillna(full_data.groupby(\"Title\")[\"Age\"].transform(\"median\"), inplace=True)\ntest[\"Age\"].fillna(full_data.groupby(\"Title\")[\"Age\"].transform(\"median\"), inplace=True)\nprint('Set the median age of each title for the missing Age values')","e669a8f2":"test[test[\"Fare\"].isnull()]","4a194d17":"test[\"Fare\"].fillna(full_data.groupby(\"Pclass\")[\"Fare\"].transform(\"median\"), inplace=True)\nprint('Set the median Fare of each passenger class for the missing Fare values.')","3df0954c":"test[\"Fare\"].isnull().sum()","f51a6e28":"for d in [train,test]:\n    d[\"Sex\"]=d[\"Sex\"].map(lambda x: 0 if x=='female' else 1)","7e22f3eb":"train, test= [ pd.get_dummies(data, columns = ['Title','Pclass']) for data in [train, test]]","df6b89ad":"train.head()","64e63dfe":"test.head()","643ad4d3":"# Let's visualize the correlations between numerical features of the train set.\nfig, ax = plt.subplots(figsize=(12,6)) \nsns.heatmap(train.iloc[:,1:len(train)].corr(), annot = True, fmt = \".2f\", linewidths=0.5, ax=ax) \nplt.show()","d69ca577":"g= sns.factorplot(x = \"SibSp\", y = \"Survived\", data = train, kind = \"bar\", size = 6)\ng.set_ylabels(\"Survival Probability\")\nplt.show()","ae11b118":"g= sns.factorplot(x = \"Parch\", y = \"Survived\", data = train, kind = \"bar\", size = 6)\ng.set_ylabels(\"Survived Probability\")\nplt.show()","5682697d":"train.groupby(\"Survived\")['Parch'].value_counts()","651ab1c4":"g= sns.FacetGrid(train, col = \"Survived\")\ng.map(sns.distplot, \"Age\", bins = 25)\nplt.show()","5f3faca2":"# FamilySize: Siblings+Spouse+Parent+Children+1(passenger)\nfor d in [train,test]:\n    d[\"Familysize\"] = d[\"SibSp\"] + d[\"Parch\"] + 1","77ff54c8":"# The relationship between survival rate and FamilySize is. \ng = sns.factorplot(x = \"Familysize\", y = \"Survived\", data = train, kind = \"bar\");\ng.set_ylabels(\"Survival probability\");","5f78f7e2":"for d in [train,test]:\n    d[\"small_family\"] = [1 if 1<i < 5 else 0 for i in d[\"Familysize\"] ]\n    d.drop([\"Familysize\"], axis = 1, inplace=True)\n    d.drop([\"Sex\"], axis = 1, inplace=True)","90cabc07":"train.groupby(\"small_family\")['Survived'].value_counts()","67ea64ef":"#  The relationship between survival rate and Familysize is. \ng = sns.factorplot(x = \"small_family\", y = \"Survived\", data = train, kind = \"bar\");\ng.set_ylabels(\"Survival probability\");","99b00d93":"fig, ax = plt.subplots(figsize=(12,6)) \nsns.heatmap(train.iloc[:,1:len(train)].corr(), annot = True, fmt = \".2f\", linewidths=0.5, ax=ax) \nplt.show()","beb9d1d6":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\npredictors = train.drop(['Survived', 'PassengerId'], axis=1)\ntarget = train[\"Survived\"]\nx_train, x_val, y_train, y_val = train_test_split(predictors, target, test_size = 0.20, random_state = 0)","78f6b346":"x_train.shape","1c3017df":"x_val.shape","e2f46a8f":"r=1309\nmodels = [LogisticRegression(random_state=r),GaussianNB(), KNeighborsClassifier(),\n          SVC(random_state=r,probability=True),DecisionTreeClassifier(random_state=r),\n          RandomForestClassifier(random_state=r), GradientBoostingClassifier(random_state=r),\n          XGBClassifier(random_state=r), MLPClassifier(random_state=r),\n          CatBoostClassifier(random_state=r,verbose = False)]\nnames = [\"LogisticRegression\",\"GaussianNB\",\"KNN\",\"SVC\",\n             \"DecisionTree\",\"Random_Forest\",\"GBM\",\"XGBoost\",\"Art.Neural_Network\",\"CatBoost\"]","a7f538b4":"\nprint('Default model validation accuracies for the train data:', end = \"\\n\\n\")\nfor name, model in zip(names, models):\n    model.fit(x_train, y_train)\n    y_pred = model.predict(x_val) \n    print(name,':',\"%.3f\" % accuracy_score(y_pred, y_val))","2941d378":"results = []\nprint('10 fold Cross validation accuracy and std of the default models for the train data:', end = \"\\n\\n\")\nfor name, model in zip(names, models):\n    kfold = KFold(n_splits=10, random_state=1001)\n    cv_results = cross_val_score(model, predictors, target, cv = kfold, scoring = \"accuracy\")\n    results.append(cv_results)\n    print(\"{}: {} ({})\".format(name, \"%.3f\" % cv_results.mean() ,\"%.3f\" %  cv_results.std()))","890701dd":"# Possible hyper parameters\nnames = [\"LogisticRegression\",\"GaussianNB\",\"KNN\",\"SVC\",\n             \"DecisionTree\",\"Random_Forest\",\"GBM\",\"XGBoost\",\"Art.Neural_Network\",\"CatBoost\"]\nlogreg_params= {\"C\":np.logspace(-1, 1, 10),\n                    \"penalty\": [\"l1\",\"l2\"], \"solver\":['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], \"max_iter\":[1000]}\n\nNB_params = {'var_smoothing': np.logspace(0,-9, num=100)}\nknn_params= {\"n_neighbors\": np.linspace(1,19,10, dtype = int).tolist(),\n                 \"weights\": [\"uniform\",\"distance\"],\n                 \"metric\":[\"euclidean\",\"manhattan\"]}\nsvc_params= {\"kernel\" : [\"rbf\"],\n                 \"gamma\": [0.001, 0.01, 0.1, 1, 5, 10 ,50 ,100],\n                 \"C\": [1,10,50,100,200,300,1000]}\ndtree_params = {\"min_samples_split\" : range(10,500,20),\n                \"max_depth\": range(1,20,2)}\nrf_params = {\"max_features\": [\"log2\",\"Auto\",\"None\"],\n                \"min_samples_split\":[2,3,5],\n                \"min_samples_leaf\":[1,3,5],\n                \"bootstrap\":[True,False],\n                \"n_estimators\":[50,100,150],\n                \"criterion\":[\"gini\",\"entropy\"]}\ngbm_params = {\"learning_rate\" : [0.001, 0.01, 0.1, 0.05],\n             \"n_estimators\": [100,500,100],\n             \"max_depth\": [3,5,10],\n             \"min_samples_split\": [2,5,10]}\ngbm_params = {\"learning_rate\" : [0.001, 0.01, 0.1, 0.05],\n             \"n_estimators\": [100,500,100],\n             \"max_depth\": [3,5,10],\n             \"min_samples_split\": [2,5,10]}\n\nxgb_params ={\n        'n_estimators': [50, 100, 200],\n        'subsample': [ 0.6, 0.8, 1.0],\n        'max_depth': [1,2,3,4],\n        'learning_rate': [0.1,0.2, 0.3, 0.4, 0.5],\n        \"min_samples_split\": [1,2,4,6]}\n\nmlpc_params = {\"alpha\": [0.1, 0.01, 0.02, 0.005, 0.0001,0.00001],\n              \"hidden_layer_sizes\": [(10,10,10),\n                                     (100,100,100),\n                                     (100,100),\n                                     (3,5), \n                                     (5, 3)],\n              \"solver\" : [\"lbfgs\",\"adam\",\"sgd\"],\n              \"activation\": [\"relu\",\"logistic\"]}\ncatb_params =  {'depth':[2, 3, 4],\n              'loss_function': ['Logloss', 'CrossEntropy'],\n              'l2_leaf_reg':np.arange(2,31)}\nclassifier_params = [logreg_params,NB_params,knn_params,svc_params,dtree_params,rf_params,\n                     gbm_params, xgb_params,mlpc_params,catb_params]               \n                  ","357d5e46":"# Tuning by Cross Validation  \ncv_result = {}\nbest_estimators = {}\nfor name, model,classifier_param in zip(names, models,classifier_params):\n    with timer(\">Model tuning\"):\n        clf = GridSearchCV(model, param_grid=classifier_param, cv =10, scoring = \"accuracy\", n_jobs = -1,verbose = False)\n        clf.fit(x_train,y_train)\n        cv_result[name]=clf.best_score_\n        best_estimators[name]=clf.best_estimator_\n        print(name,'cross validation accuracy : %.3f'%cv_result[name])","fc084ea4":"accuracies={}\nprint('Validation accuracies of the tuned models for the train data:', end = \"\\n\\n\")\nfor name, model_tuned in zip(best_estimators.keys(),best_estimators.values()):\n    y_pred =  model_tuned.fit(x_train,y_train).predict(x_val)\n    accuracy=accuracy_score(y_pred, y_val)\n    print(name,':', \"%.3f\" %accuracy)\n    accuracies[name]=accuracy","6463b964":"n=6\naccu=sorted(accuracies, reverse=True, key= lambda k:accuracies[k])[:n]\nfirstn=[[k,v] for k,v in best_estimators.items() if k in accu]","111579f4":"# Ensembling First n Score\n\nvotingC = VotingClassifier(estimators = firstn, voting = \"soft\", n_jobs = -1)\nvotingC = votingC.fit(x_train, y_train)\nprint(accuracy_score(votingC.predict(x_val),y_val))","b4b5f466":"ids = test['PassengerId']\nx_test=test.drop('PassengerId', axis=1)\npredictions = votingC.predict(x_test)\n\n#set the output as a dataframe and convert to csv file named submission.csv\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions })\noutput.to_csv('submission_first{}.csv'.format(n), index=False)","828098f0":"## 3.4 Outlier Treatment <a id = '3.4'><\/a><br>","44329ceb":"## 3.5 Missing Value Treatment <a id = '3.5'><\/a><br>","2f6687be":"In the names, there are title and nickname information which may be useful in our analysis. Title gives more information about socioeconomics status, and the people with nicknames may be more lively.","e4126305":"Survival probability of small families is almost more than two times higher than big families.","68370898":"### 3.6.2 One hot encoding of Title and Pclass <a id = '3.6.2'><\/a><br>","b9fe535b":"## 6.2 Validation Set Accuracy for the default models <a id = '6.2'><\/a><br>","5f4778d9":"## 4.4 Age versus Survived <a id = '4.4'><\/a><br>   ","7b3c38dc":"The given train set is splitted again into inner train and validation sets to test the accuracy of training with the untrained 20% of the sample.","bf93e999":"Having one sibling or spouse has the highest correlation with survival, incrasing the number more than 2 survival probability decreases dractically.  ","9fc2b6a8":"### 3.5.2 For Fare <a id = '3.5.2'><\/a><br>","43101858":"## 3.2 Extraction of Title and Nicknamed variables from Name variable <a id = '3.2'><\/a><br>","c0a2bbfc":"## 5.2 Generating small_family, dropping family size and sex <a id = '5.2'><\/a><br>","2e1a95f1":"# 3. Data Preparation <a id = '3'><\/a><br> ","fcb97cde":"####  Extracting first n (e.g. 6) models","5b5541d5":"## 5.3 Final correlation matrix as heatmap <a id = '5.3'><\/a><br>","7b0bac78":"This variable seems not to have a clear\n  ","2332fa7c":"##### Descriptive statistics excluding PassengerId which does not carry any meaningful information for Survival.","108ad0dc":"## 6.4 Model tuning using crossvalidation <a id = '6.4'><\/a><br>","b5f25b06":"# 1. Introduction ","46ef5050":"### 3.6.1 Label encoding of sex variable to a dummy variable (0-1) <a id = '3.6.1'><\/a><br>","21c545d2":"# 5. More Feature Engineering and Final Correlation Matrix <a id = '5'><\/a><br>  ","9048c28f":"## 6.1 Splitting the train data <a id = '6.1'><\/a><br>","19274429":"# 7. Submission  <a id = '7'><\/a><br>","b95f249c":"<a id = '6'><\/a><br> \n# 6. Modeling, Evaluation and Model Tuning  ","c242cb16":"## 3.3 Create Cabin_dummy variable <a id = '3.3'><\/a><br>","7a1a21c5":"## 3.1 Dropping Ticket number and Embarked Variables <a id = '3.1'><\/a><br>","34c31f2f":"## 4.2 SibSp and Survived <a id = '4.2'><\/a><br>       ","b395651e":"## 2.3 Basic summary statistics about the data <a id = '2.3'><\/a><br>","e6bc6bd4":"### 2.1 Importing Libraries <a id = '2.1'><\/a><br>","88246009":"## 4.3 Parch and Survived <a id = '4.3'><\/a><br>       ","627aeb19":"\n## 4.1 Correlation matrix <a id = '4.1'><\/a><br>","10c22f0f":"## 6.3 Cross validation accuracy and std of the default models for all the train data <a id = '6.3'><\/a><br>","e76f3881":"# 2. Exploratory Data Analysis <a id = '2'><\/a><br> ","c119f966":"## 2.2 Loading Data <a id = '2.2'><\/a><br>","70926d24":"It is very clear that, the survival is higher if the family size between 1 and 5. Hence, I can crate a new feature of small_family which is 1 if family size between 1 and 5 and 0 else (for single and big families As I mentioned in the previous section, sex variabile has a very high correlation with 'Mr' title and likewise Familysize with its source variables SibSp and Parch. So we drop Familysize and Sex variables.","e01701ed":"## 3.6 Categorical Variables' Encoding <a id = '3.6'><\/a><br>","d6488d75":"Cabin_dummy variablecan give imformation about whether someone has a Cabin data or not:","7e0ec996":"In this section we are going to illustrate the relationship between variables by using visualization tools.","28c5aa7a":"Amount of survived small families are considerbaly higher with respect to large families and singles. Amount of deaths are more than two times higher in large families and singles with respect to small families.","6697ebff":"We can drop Ticket feature since it is unlikely to have useful information. In addition, although the embark places shows diffent correlations of the passengers is unlikely to be related to the future survival of the passengers.","caf65dbe":"We can see from the heatmaps that \"Survived\" variable has the highest positive correlations with \"Mrs\" and \"Miss\" titles,\"Cabin_dummy\",\"PClass\" and \"Fare\" variables. The passengers with nicknames have 17% positive correlation survival which descriptively supports our hypothesis of them for being a useful predictor. Highest negative correlation occurs with 'Mr' title.\n**We can say,**\n* The passengers are more likely to survive if they paid more, if their cabin information is known, and\/or they are in first class.\n* Being woman has a very dominant effect on survival.\n* As there is 90% correlation between Sex and 'Mr', it is better to drop Sex variable which is already represented by titles.\n* The correlation is positive for Age and Parch, however it is negative for SibSp. On the other hand, it is only -0.04 for  SibSp and 0.08 for the former ones. It could be better to visualize these variables in detail.","7bedecb4":"### 3.5.1 For Age <a id = '3.5.1'><\/a><br>","93130396":"<font color = 'blue'>\n CONTENTS:  \n    \n   1. [Introduction](#1)\n       * 1.1 [Summary Information about the variables and their types in the data](#1.1)\n   2. [Exploratory Data Analysis](#2)\n       * 2.1 [Importing Libraries](#2.1)\n       * 2.2 [Loading Data](#2.2)\n       * 2.3 [Basic summary statistics about the data](#2.3)       \n   3. [Data Preparation](#3)\n       * 3.1 [Dropping Ticket number and Embarked Variables](#3.1)  \n       * 3.2 [Extraction of Title and Nicknamed variables from Name variable](#3.2)\n           * 3.2.1 [Clustering Title variable](#3.2.1) \n       * 3.3 [Create Cabin_dummy variable](#3.3)\n       * 3.4 [Outlier Treatment](#3.4)\n       * 3.5 [Missing Value Treatment](#3.5)\n           * 3.5.1 [For Age](#3.5.1)     \n           * 3.5.2 [For Fare](#3.5.2)   \n       * 3.6 [Categorical Variables' Encoding](#3.6)\n           * 3.6.1 [Label encoding of sex variable to a dummy variable (0-1)](#3.6.1)\n           * 3.6.2 [One hot encoding of Title and Pclass](#3.6.2)         \n   4. [Visualizations](#4)\n       * 4.1 [Correlation matrix as heatmap](#4.1)\n       * 4.2 [SibSp versus Survived](#4.2)\n       * 4.3 [Parch versus Survived](#4.3)\n       * 4.4 [Age versus Survived](#4.4)\n   5. [More Feature Engineering and Final Correlation Matrix](#5)\n       * 5.1 [Correlation matrix](#5.1)\n       * 5.2 [Generating small_family, dropping family size and sex](#5.2)\n       * 5.3 [Final correlation matrix as heatmap](#5.3)\n   6. [Modeling, Model Evaluation and Model Tuning](#6)\n       * 6.1 [Splitting the train data](#6.1) \n       * 6.2 [Validation Set Test Accuracy for the default models](#6.2) \n       * 6.3 [Cross validation accuracy and std of the default models for all the train data](#6.3)    \n       * 6.4 [Model tuning using crossvalidation](#6.4)   \n       * 6.5 [Ensembling](#6.5) \n   7. [Submission](#7)\n ","ee2d7ea9":"### This study is going to predict which passengers would survive from the Titanic disaster by means of machine learning modelling techniques. It is going present a full machine learning work flow, use 10 Machine Learning algorithms, tune their parameters and ensemble the best n (e.g. 6) of them using their accuracy scores for the validation set. ","bffc709f":"The graph on the left hand side shows the distribution of the died passengers while the graph on the right hand side demonstrates the distribution of the survived passengers. \n* For very old and very young passengers, there are high survival rates with respect to death rates which shows they had been saved priviligously. \n* Most of the passengers in the Titanic were between the ages of 15-35.\n* Most of the **died passengers** in the Titanic were between the ages of **15-35.**\n* Most of the **survived passengers** in the Titanic were between the ages of **20-35.**","aad3ee41":"## 1.1 Summary Information about the variables and their types in the data <a id = '1.1'><\/a><br>\n\n\nSurvival: Survival -> 0 = No, 1 = Yes\n\nPclass: Passennger ticket class -> 1 = 1st (Upper), 2 = 2nd (Middle), 3 = 3rd (Lower)\n\nName: Name of the passenger including title and (if written in quotes) nickname\n\nSex: Male or Female\n\nAge: Age in years\n\nSibSp: # of Siblings (brother,sister,stepbrother,stepsister) and Spouses (husband or wife) aboard the ship\n\nParch: # of Parents and Children  aboard the ship\n\nTicket: Ticket code\n\nFare: Passenger fare paid\n\nCabin: Cabin code\n\nEmbarked: Port of Embark for the passenger -> C = Cherbourg, Q = Queenstown, S = Southampton","42fb9866":"Variables \"SibSp\" and \"Parch\" give the information about passengers' family, hence we can add them up to reach the family size of the passenger including the passenger himself\/herself.","441cb861":"### 3.2.1 Clustering Title variable <a id = '3.2.1'><\/a><br>\n\nLady,Madame,Mademoiselle,Don,Dona,Countess and Sir are used to show nobility hence they are groupped in the noble category. Jonkheer and Reverand are at one of the lowest nobility categories, thus they are classified in ordinary 'Mr' Category. \n(Sources:https:\/\/en.wikipedia.org\/wiki\/Imperial,_royal_and_noble_ranks, https:\/\/en.wikipedia.org\/wiki\/Forms_of_address_in_the_United_Kingdom ,https:\/\/en.wikipedia.org\/wiki\/Don_(honorific))\n\n\nColonels and Majors are among the highest rank army officials so they are also classified in the noble category. As being in a lower rank 'Captain' is classified in ordinary 'Mr' Category. (Source: https:\/\/www.va.gov\/vetsinworkplace\/docs\/em_rank.html). \n\n'Dr' titles are classified according to their gender (Mr or Mrs). ","6cba7c38":"# 4. Visualizations <a id = '3'><\/a><br> ","a58fafdd":"## 5.1 Family size <a id = '5.1'><\/a><br>"}}