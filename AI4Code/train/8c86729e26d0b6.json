{"cell_type":{"eed5c0d2":"code","f0097dd0":"code","1fa6218f":"code","539cf531":"code","9aab0e41":"code","9669ebc6":"code","cf0abd81":"code","d892e698":"code","f04de376":"code","b37fed78":"code","27a11ac6":"code","76c2dbb4":"code","8c9c7324":"code","85031b49":"code","36660612":"code","d5660d06":"code","0476f5ae":"code","1682fd00":"code","ac4c0aff":"code","0e9f1f9c":"code","5b46118c":"markdown","56955cb8":"markdown","cd37c22b":"markdown","3e3b7aac":"markdown","bcd8b636":"markdown","a1bba5e1":"markdown","b0a394b0":"markdown","233d34f4":"markdown","f6989b87":"markdown","d135e4fd":"markdown","081de09a":"markdown","8861f06c":"markdown","38c7e0f3":"markdown","92bc3c08":"markdown","0e713f8c":"markdown","fdde8d03":"markdown","d97126d4":"markdown","de83cdda":"markdown","f84b91bf":"markdown"},"source":{"eed5c0d2":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom bs4 import BeautifulSoup\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer \nfrom wordcloud import WordCloud, STOPWORDS\nfrom nltk.tokenize import word_tokenize\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.feature_extraction.text import TfidfVectorizer","f0097dd0":"#import dataset\nfake = pd.read_csv(\"..\/input\/fake-and-real-news-dataset\/Fake.csv\")\ntrue = pd.read_csv(\"..\/input\/fake-and-real-news-dataset\/True.csv\")","1fa6218f":"#data exploration\nfake.head()","539cf531":"true.head()","9aab0e41":"#adding label columns to both fake news and true news dataset\nfake[\"label\"] = 1\ntrue[\"label\"] = 0","9669ebc6":"#combining both the datasets into one\ndf = pd.concat([fake, true], ignore_index = True)\ndf","cf0abd81":"#EDA\n#checking for missing values in the combined dataset\ndf.isnull().sum()","d892e698":"#checking for imbalance in the dataset\ncount = df['label'].value_counts().values\nsns.barplot(x = [0,1], y = count)\nplt.title('Target variable count')","f04de376":"#distribution of fake and real news among subjects\nplt.figure(figsize=(12,8))\nsns.countplot(x = \"subject\", data=df, hue = \"label\")\nplt.show()","b37fed78":"#data cleaning\n#combining the title and text columns\ndf['text'] = df['title'] + \" \" + df['text']\n#deleting few columns from the data \ndel df['title']\ndel df['subject']\ndel df['date']","27a11ac6":"#choosing the language as english\nstop = set(stopwords.words('english'))\n#removing punctuation marks\npunctuation = list(string.punctuation)\n#adding punctuations to the list of stop words \nstop.update(punctuation)\n\n#Removing the square brackets\ndef remove_brackets(text):\n    return re.sub('\\[[^]]*\\]', '', text)\n\n# Removing URL's\ndef remove_urls(text):\n    return re.sub(r'http\\S+', '', text)\n\n#Removing the stopwords from text\ndef remove_stopwords(text):\n    final_text = []\n    text = text.lower()\n    for i in text.split():\n        if i.strip() not in stop:\n            final_text.append(i.strip())\n    return \" \".join(final_text)\n\n#Removing the noisy text\ndef clean_text(text):\n    text = remove_brackets(text)\n    text = remove_urls(text)\n    text = remove_stopwords(text)\n    return text\n\n#Apply function on text column\ndf['text']=df['text'].apply(clean_text)\ndf['text']","76c2dbb4":"#lemmatization\n# Init the Wordnet Lemmatizer\nlemmatizer = WordNetLemmatizer()\n\n#A function which takes a sentence\/corpus and gets its lemmatized version.\ndef lemmatize_text(text):\n    token_words=word_tokenize(text) \n#we need to tokenize the sentence or else lemmatizing will return the entire sentence as is.\n    lemma_sentence=[]\n    for word in token_words:\n        lemma_sentence.append(lemmatizer.lemmatize(word))\n        lemma_sentence.append(\" \")\n    return \"\".join(lemma_sentence)\n\n#Apply function on text column\ndf['text']=df['text'].apply(lemmatize_text)\ndf","8c9c7324":"#word cloud for fake news\ncloud = WordCloud(max_words = 500, stopwords = STOPWORDS, background_color = \"white\").generate(\" \".join(df[df.label == 1].text))\nplt.figure(figsize=(40, 30))\nplt.imshow(cloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show()","85031b49":"#word cloud for real news\ncloud = WordCloud(max_words = 500, stopwords = STOPWORDS, background_color = \"white\").generate(\" \".join(df[df.label == 0].text))\nplt.figure(figsize=(40, 30))\nplt.imshow(cloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show()","36660612":"#finding n-grams\ntexts = ''.join(str(df['text'].tolist()))\n\n# first get individual words\ntokenized = texts.split()","d5660d06":"#unigram\nunigram = (pd.Series(nltk.ngrams(tokenized, 1)).value_counts())[:20]\nunigram.sort_values().plot.barh(width=.9, figsize=(12, 8))\nplt.title('20 Most Frequently Occuring Unigrams')\nplt.ylabel('Unigram')\nplt.xlabel('# of Occurances')","0476f5ae":"#bigrams\nbigram = (pd.Series(nltk.ngrams(tokenized, 2)).value_counts())[:20]\nbigram.sort_values().plot.barh(width=.9, figsize=(12, 8))\nplt.title('20 Most Frequently Occuring Bigrams')\nplt.ylabel('Bigram')\nplt.xlabel('# of Occurances')","1682fd00":"#trigrams\ntrigram = (pd.Series(nltk.ngrams(tokenized, 3)).value_counts())[:20]\ntrigram.sort_values().plot.barh(width=.9, figsize=(12, 8))\nplt.title('20 Most Frequently Occuring Trigrams')\nplt.ylabel('Trigram')\nplt.xlabel('# of Occurances')","ac4c0aff":"#modeling\ndef get_prediction(vectorizer, classifier, X_train, X_test, y_train, y_test):\n    pipe = Pipeline([('vector', vectorizer),\n                    ('model', classifier)])\n    model = pipe.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    print(\"Accuarcy: {}\".format(round(accuracy_score(y_test, y_pred)*100,2)))\n    cm = confusion_matrix(y_test, y_pred)\n    print(\"Confusion Matrix: \\n\", cm)\n","0e9f1f9c":"#pipeline implementation\nX_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size = 0.3, random_state= 0)\nclassifiers = [LogisticRegression(),KNeighborsClassifier(n_neighbors=5), DecisionTreeClassifier(), GradientBoostingClassifier(), \n               RandomForestClassifier()]\nfor classifier in classifiers:\n    print(\"\\n\\n\", classifier)\n    get_prediction(TfidfVectorizer(), classifier, X_train, X_test, y_train, y_test)","5b46118c":"<a id = \"3\" ><\/a>\n# Exploratory Data Analysis","56955cb8":"We will combine the seperate datasets into one for our further analysis","cd37c22b":"The columns in the datasets are:\n* **title** - The title of the article\n* **text** - The text of the article\n* **subject** - The subject of the article\n* **date** - The date at which the article was posted\n\nThe dataset contains no target variable. We need to create manually and add it to the datasets. We will create a binary variable called label. The label variable will have '0' for real news and '1' for fake news. ","3e3b7aac":"<a id = \"13\" ><\/a>\n# Conclusion\nDecision Tree, Gradient Boosting and Random Forest Algorithms are giving an accuracy above 99% which is a really good score. There might be chances of overfitting which can be explored using validation curve. I will explore overfitting furthur. \n\n**Upvote if you like this notebook. Happy Learning!**","bcd8b636":"<a id = \"11\" ><\/a>\n## Trigram Analysis","a1bba5e1":"<a id = \"5\" ><\/a>\n## Removing stopwords\nOne of the major forms of pre-processing is to filter out useless data. In NLP, useless words, are referred to as stop words. We will use the `nltk` library for this purpose. This is how we are making our processed content more efficient by removing words that do not contribute to any future operations.","b0a394b0":"There are no null\/missing values in the dataset.","233d34f4":"<a id = \"12\" ><\/a>\n# Modeling\nIn this step, I am making use of various Classification models for prediction. The models use cleaned text data for analysis.\n\n#### Using TF-IDF Vectorizer\nThis is an acronym than stands for \"Term Frequency \u2013 Inverse Document Frequency\" which are the components of the resulting scores assigned to each word.The TfidfVectorizer will tokenize documents, learn the vocabulary and inverse document frequency weightings, and allow you to encode new documents.","f6989b87":"<a id = \"7\" ><\/a>\n## Word Cloud\n### Fake News Word Cloud\n","d135e4fd":"From the plot above, you can see there is no class imbalance in the target variable. We have almost equal instances for negative class (\"0\" - Real) and the class of interest (\"1\" - Fake).","081de09a":"<a id = \"6\" ><\/a>\n## Lemmatization\nThe next step is to perform Lemmatization. It is the process of converting a word to its base form. For example: 'Caring' -> 'Care'; 'hanging' -> 'hang'","8861f06c":"<a id = \"1\" ><\/a>\n# Load Required Libraries","38c7e0f3":"<a id = \"2\" ><\/a>\n# Import the Dataset\n\nThere are two datasets seperate for fake and real news. We will import them into the environment","92bc3c08":"<a id = \"9\" ><\/a>\n## Unigram Analysis","0e713f8c":"### Real News Word Cloud","fdde8d03":"<a id = \"4\" ><\/a>\n# Data Cleaning\nWe will begin with the preprocessing steps before the text is fed into the model for prediction. ","d97126d4":"<a id = \"8\" ><\/a>\n# N-gram Analysis","de83cdda":"<h1 style=\"text-align:center\">   \n      <font color = Black >\n                Fake and Real News \n        <\/font>    \n<\/h1>   \n<hr style=\"width:100%;height:5px;border-width:0;color:gray;background-color:gray\">\n<center><img style = \"height:450px;\" src=\"https:\/\/www.txstate.edu\/cache78a0c25d34508c9d84822109499dee61\/imagehandler\/scaler\/gato-docs.its.txstate.edu\/jcr:21b3e33f-31c9-4273-aeb0-5b5886f8bcc4\/fake-fact.jpg?mode=fit&width=1600\"><\/center>\n\n# Introduction\n\nThis table of contents gives an overview about different sections in the notebook.\n\n1. [Load Required Libraries](#1)\n2. [Import the Dataset](#2)\n3. [Exploratory Data Analysis](#3)\n4. [Data Cleaning](#4)\n    * [Removing Stopwords](#5)\n    * [Lemmatization](#6)\n    * [Word Cloud](#7)\n5. [N-gram Analysis](#8)  \n    * [Unigram Analysis](#9)\n    * [Bigram Analysis](#10)\n    * [Trigram Analysis](#11)\n6. [Modeling](#12)\n7. [Conclusion](#13)","f84b91bf":"<a id = \"10\" ><\/a>\n## Bigram Analysis"}}