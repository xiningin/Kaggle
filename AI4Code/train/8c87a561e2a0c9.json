{"cell_type":{"7676dc8a":"code","804ea5e4":"code","77fa02d5":"code","8d35b839":"code","f2d26eeb":"code","bb1b090c":"code","a0deeaf0":"code","c88ed2cf":"code","c0e97ddc":"code","36d25eac":"code","422bd53a":"code","8fd7f059":"code","35b80954":"code","d06f979c":"code","f3723924":"code","3d970b9e":"code","25e32b40":"code","8862c66f":"code","b674cb76":"code","255945dc":"code","583ede23":"code","5d4a067d":"code","ffe92189":"code","44057a83":"code","e6c0ba95":"code","31050f89":"code","402b0fb2":"code","445b834b":"code","251477fd":"markdown","33189a0d":"markdown","543a70e0":"markdown","bc2732da":"markdown","ff7a1295":"markdown","8697321b":"markdown","45cf2193":"markdown","ccfeaf67":"markdown","0a3c3f69":"markdown","311084db":"markdown","6083a7b2":"markdown","88ae0ee0":"markdown","1fe85d89":"markdown"},"source":{"7676dc8a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","804ea5e4":"import tensorflow as tf\nfrom tensorflow.keras.layers import Input, Reshape, Dropout, Dense \nfrom tensorflow.keras.layers import Flatten, BatchNormalization\nfrom tensorflow.keras.layers import Activation, ZeroPadding2D\nfrom tensorflow.keras.layers import LeakyReLU\nfrom tensorflow.keras.layers import UpSampling2D, Conv2D\nfrom tensorflow.keras.models import Sequential, Model, load_model\nfrom tensorflow.keras.optimizers import Adam\nimport numpy as np\nfrom PIL import Image\nfrom tqdm import tqdm\nimport os \nimport time\nimport matplotlib.pyplot as plt\nimport pathlib","77fa02d5":"# Generation resolution - Must be square \n# Training data is also scaled to this.\n\nGENERATE_RES = 3 # Generation resolution factor \n# (1=32, 2=64, 3=96, 4=128, etc.)\nGENERATE_SQUARE = 32 * GENERATE_RES # rows\/cols (should be square)\nIMAGE_CHANNELS = 3  #In a color image, we normally have 3 channels: red, green and blue\n\n# Preview image \nPREVIEW_ROWS = 4\nPREVIEW_COLS = 7\nPREVIEW_MARGIN = 16\n\n# Size vector to generate images from\nSEED_SIZE = 100\n\n# Configuration\nDATA_PATH = '..\/input\/faces-data-new\/images'\nEPOCHS = 50\nBATCH_SIZE = 256\nBUFFER_SIZE = 60000   # No matter what buffer size you will choose, all samples will be used, \n                      #  it only affects the randomness of the shuffle. If buffer size is 100,\n                      # it means that Tensorflow will keep a buffer of the next 100 samples, and will randomly select one those 100 samples.\n\nprint(f\"Will generate {GENERATE_SQUARE}px square images.\")","8d35b839":"#For time. We'll use on the below.\ndef hms_string(sec_elapsed):\n    h = int(sec_elapsed \/ (60 * 60))\n    m = int((sec_elapsed % (60 * 60)) \/ 60)\n    s = sec_elapsed % 60\n    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)","f2d26eeb":"training_binary_path = os.path.join(DATA_PATH,\n        f'training_data_{GENERATE_SQUARE}_{GENERATE_SQUARE}.npy')\n\nprint(f\"Looking for file: {training_binary_path}\")\n\nif not os.path.isfile(training_binary_path):\n  start = time.time()\n  print(\"Loading training images...\")\n\n  training_data = []\n  faces_path = os.path.join(DATA_PATH,'images')\n  for filename in tqdm(os.listdir(faces_path)):\n      path = os.path.join(faces_path,filename)\n      image = Image.open(path).resize((GENERATE_SQUARE,\n            GENERATE_SQUARE),Image.ANTIALIAS)\n      training_data.append(np.asarray(image))\n  training_data = np.reshape(training_data,(-1,GENERATE_SQUARE,\n            GENERATE_SQUARE,IMAGE_CHANNELS))\n  training_data = training_data.astype(np.float32)\n  training_data = training_data \/ 127.5 - 1.\n\n\n  #print(\"Saving training image binary...\")\n  #np.save(training_binary_path,training_data)\n  elapsed = time.time()-start\n  print (f'Image preprocess time: {hms_string(elapsed)}')\nelse:\n  #print(\"Loading previous training pickle...\")\n  #training_data = np.load(training_binary_path)\n  print(\"Error is taken.\")","bb1b090c":"#training_data = training_data.reshape(training_data.shape[0], 28, 28, 1).astype('float32')\n#training_data = (training_data - 127.5) \/ 127.5 # Normalize the images to [-1, 1]\n\n#BUFFER_SIZE = 60000\n#BATCH_SIZE = 256\n\n# train_dataset = tf.data.Dataset.from_tensor_slices((training_data)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)","a0deeaf0":"train_dataset = tf.data.Dataset.from_tensor_slices((training_data)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)","c88ed2cf":"\n\ndef generator_model(seed_size, channels):\n    model = Sequential()\n\n    model.add(Dense(4*4*256,activation=\"relu\",input_dim=seed_size))\n    model.add(Reshape((4,4,256)))\n\n    model.add(UpSampling2D())\n    model.add(Conv2D(256,kernel_size=3,padding=\"same\"))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(Activation(\"relu\"))\n\n    model.add(UpSampling2D())\n    model.add(Conv2D(256,kernel_size=3,padding=\"same\"))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(Activation(\"relu\"))\n   \n    # Output resolution, additional upsampling\n    model.add(UpSampling2D())\n    model.add(Conv2D(128,kernel_size=3,padding=\"same\"))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(Activation(\"relu\"))\n\n    if GENERATE_RES>1:\n      model.add(UpSampling2D(size=(GENERATE_RES,GENERATE_RES)))\n      model.add(Conv2D(128,kernel_size=3,padding=\"same\"))\n      model.add(BatchNormalization(momentum=0.8))\n      model.add(Activation(\"relu\"))\n\n    # Final CNN layer\n    model.add(Conv2D(channels,kernel_size=3,padding=\"same\"))\n    model.add(Activation(\"tanh\"))\n\n    return model","c0e97ddc":"def discriminator_model(image_shape):\n    model = Sequential()\n\n    model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=image_shape, \n                     padding=\"same\"))\n    model.add(LeakyReLU(alpha=0.2))\n\n    model.add(Dropout(0.25))\n    model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n    model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(LeakyReLU(alpha=0.2))\n\n    model.add(Dropout(0.25))\n    model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(LeakyReLU(alpha=0.2))\n\n    model.add(Dropout(0.25))\n    model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(LeakyReLU(alpha=0.2))\n\n    model.add(Dropout(0.25))\n    model.add(Conv2D(512, kernel_size=3, strides=1, padding=\"same\"))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(LeakyReLU(alpha=0.2))\n\n    model.add(Dropout(0.25))\n    model.add(Flatten())\n    model.add(Dense(1, activation='sigmoid'))\n\n    return model","36d25eac":"def save_images(cnt,noise):\n  image_array = np.full(( \n      PREVIEW_MARGIN + (PREVIEW_ROWS * (GENERATE_SQUARE+PREVIEW_MARGIN)), \n      PREVIEW_MARGIN + (PREVIEW_COLS * (GENERATE_SQUARE+PREVIEW_MARGIN)), IMAGE_CHANNELS), \n      255, dtype=np.uint8)\n  \n  generated_images = generator.predict(noise)\n\n  generated_images = 0.5 * generated_images + 0.5\n\n  image_count = 0\n  for row in range(PREVIEW_ROWS):\n      for col in range(PREVIEW_COLS):\n        r = row * (GENERATE_SQUARE+16) + PREVIEW_MARGIN\n        c = col * (GENERATE_SQUARE+16) + PREVIEW_MARGIN\n        image_array[r:r+GENERATE_SQUARE,c:c+GENERATE_SQUARE] \\\n            = generated_images[image_count] * 255\n        image_count += 1\n\n          \n  output_path = os.path.join(DATA_PATH,'output')\n  if not os.path.exists(output_path):\n    os.makedirs(output_path)\n  \n  filename = os.path.join(output_path,f\"train-{cnt}.png\")\n  im = Image.fromarray(image_array)\n  im.save(filename)","422bd53a":"#generator = generator_model()\n#discriminator = discriminator_model()","8fd7f059":"generator = generator_model(SEED_SIZE, IMAGE_CHANNELS)\n\nnoise = tf.random.normal([1, SEED_SIZE])\ngenerated_image = generator(noise, training=False)\n\nplt.imshow(generated_image[0, :, :, 0])","35b80954":"image_shape = (GENERATE_SQUARE,GENERATE_SQUARE,IMAGE_CHANNELS)\n\ndiscriminator = discriminator_model(image_shape)\ndecision = discriminator(generated_image)\nprint (decision)","d06f979c":"cross_entropy = tf.keras.losses.BinaryCrossentropy()\n\ndef discr_loss(real_output, fake_output):\n    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n    total_loss = real_loss + fake_loss\n    return total_loss\n\ndef gener_loss(fake_output):\n    return cross_entropy(tf.ones_like(fake_output), fake_output)","f3723924":"generator_optimizer = tf.keras.optimizers.Adam(1e-4)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(1e-4)","3d970b9e":"checkpoint_dir = '.\/training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\ncheckpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n                                 discriminator_optimizer=discriminator_optimizer,\n                                 generator=generator,\n                                 discriminator=discriminator)\n","25e32b40":"# EPOCHS = 50\nnoise_dim = 100\nnum_examples_to_generate = 16\n\n# You will reuse this seed overtime (so it's easier)\n# to visualize progress in the animated GIF)\nseed = tf.random.normal([num_examples_to_generate, noise_dim])\n","8862c66f":"# Notice the use of `tf.function`\n# This annotation causes the function to be \"compiled\".\n@tf.function\ndef train_step(images):\n    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n      generated_images = generator(noise, training=True)\n\n      real_output = discriminator(images, training=True)\n      fake_output = discriminator(generated_images, training=True)\n\n      gen_loss = gener_loss(fake_output)\n      disc_loss = discr_loss(real_output, fake_output)\n\n    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n\n    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n\n    return gen_loss,disc_loss","b674cb76":"def train(dataset, epochs):\n  for epoch in range(epochs):\n    start = time.time()\n\n    for image_batch in dataset:\n      train_step(image_batch)\n\n    # Produce images for the GIF as you go\n    #display.clear_output(wait=True)\n    generate_and_save_images(generator,\n                             epoch + 1,\n                             seed)\n\n    # Save the model every 15 epochs\n    if (epoch + 1) % 15 == 0:\n      checkpoint.save(file_prefix = checkpoint_prefix)\n\n    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n\n  # Generate after the final epoch\n  #display.clear_output(wait=True)\n  generate_and_save_images(generator,\n                           epochs,\n                           seed)\n","255945dc":"def generate_and_save_images(model, epoch, test_input):\n  # Notice `training` is set to False.\n  # This is so all layers run in inference mode (batchnorm).\n  predictions = model(test_input, training=False)\n\n  fig = plt.figure(figsize=(4, 4))\n\n  for i in range(predictions.shape[0]):\n      plt.subplot(4, 4, i+1)\n      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n      plt.axis('off')\n\n  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n  plt.show()\n","583ede23":"train(train_dataset, EPOCHS)\n","5d4a067d":"checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n","ffe92189":"# !pip uninstall PIL\n# !pip install pillow\n# !pip uninstall image","44057a83":"# from PIL import Image\n# # from pymaging import Image\n\n# # Display a single image using the epoch number\n# def display_image(epoch_no):\n#     img = Image.open_from_path('image_at_epoch_{:04d}.png'.format(epoch_no))  \n#     return im.show()\n\n# display_image(EPOCHS)\n\n# I work on Ubuntu, this code didnt work properly on it. But for Win, there will be no problem.","e6c0ba95":"# To generate GIFs\n!pip install imageio\n!pip install git+https:\/\/github.com\/tensorflow\/docs\n","31050f89":"import imageio\nimport glob","402b0fb2":"anim_file = 'dcgan.gif'\n\nwith imageio.get_writer(anim_file, mode='I') as writer:\n  filenames = glob.glob('image*.png')\n  filenames = sorted(filenames)\n  for filename in filenames:\n    image = imageio.imread(filename)\n    writer.append_data(image)\n  image = imageio.imread(filename)\n  writer.append_data(image)\n","445b834b":"import tensorflow_docs.vis.embed as embed\nembed.embed_file(anim_file)\n","251477fd":"<a id =\"ct\"><\/a> \n# **Creating the training loop**","33189a0d":"<a id =\"tm\"><\/a> \n# TRAIN THE MODEL","543a70e0":"I compiled my code with Jeff Heaton and original Tensorflow doc.\n\nSome links that Jeff shared: \n\n[Deep Convolutional Generative Adversarial Network (TensorFlow 2.0 example code)\n](https:\/\/www.tensorflow.org\/tutorials\/generative\/dcgan)   \n\n[Keep Calm and train a GAN. Pitfalls and Tips on training Generative Adversarial Networks](https:\/\/medium.com\/@utk.is.here\/keep-calm-and-train-a-gan-pitfalls-and-tips-on-training-generative-adversarial-networks-edd529764aa9)\n\n [Collection of Keras implementations of Generative Adversarial Networks GANs](https:\/\/github.com\/eriklindernoren\/Keras-GAN)\n \n [dcgan-facegenerator](https:\/\/github.com\/platonovsimeon\/dcgan-facegenerator) \n  \n [Semi-Paywalled Article by GitHub Author](https:\/\/medium.com\/datadriveninvestor\/generating-human-faces-with-keras-3ccd54c17f16)\n","bc2732da":"<a id =\"bg\"><\/a> \n# **Building Generator and Discriminator Model**","ff7a1295":"<a id =\"il\"><\/a> \n# Import Libraries","8697321b":"![image.png](attachment:4894ce93-6acd-4326-9a51-7ff874bf7566.png)","45cf2193":"<a id =\"sc\"><\/a> \n# SAVE CHECKPOINTS\n","ccfeaf67":"<a id =\"ip\"><\/a> \n# Indicating the PATH","0a3c3f69":"<a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#il\" role=\"tab\" aria-controls=\"profile\"><span class=\"badge badge-primary badge-pill\">1<\/span> <button type=\"button\" class=\"btn btn-lg btn-danger\" data-toggle=\"popover\" title=\"Popover title\" >IMPORT LIBRARIES<\/button>\n <\/a>\n <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#ip\" role=\"tab\" aria-controls=\"profile\"><span class=\"badge badge-primary badge-pill\">2<\/span> <button type=\"button\" class=\"btn btn-lg btn-danger\" data-toggle=\"popover\" title=\"Popover title\" >INDICATING PATH<\/button>\n <\/a>\n <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#bg\" role=\"tab\" aria-controls=\"profile\"><span class=\"badge badge-primary badge-pill\">3<\/span> <button type=\"button\" class=\"btn btn-lg btn-danger\" data-toggle=\"popover\" title=\"Popover title\" >Building Generator and Discriminator Model<\/button>\n <\/a>\n <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#lf\" role=\"tab\" aria-controls=\"profile\"><span class=\"badge badge-primary badge-pill\">4<\/span> <button type=\"button\" class=\"btn btn-lg btn-danger\" data-toggle=\"popover\" title=\"Popover title\" >LOSS FUNCTION<\/button>\n <\/a>\n <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#sc\" role=\"tab\" aria-controls=\"profile\"><span class=\"badge badge-primary badge-pill\">5<\/span> <button type=\"button\" class=\"btn btn-lg btn-danger\" data-toggle=\"popover\" title=\"Popover title\" >SAVE CHECKPOINTS<\/button>\n <\/a>\n <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#ct\" role=\"tab\" aria-controls=\"profile\"><span class=\"badge badge-primary badge-pill\">6<\/span> <button type=\"button\" class=\"btn btn-lg btn-danger\" data-toggle=\"popover\" title=\"Popover title\" >CREATING THE TRAINING LOOP<\/button>\n <\/a>\n <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#tm\" role=\"tab\" aria-controls=\"profile\"><span class=\"badge badge-primary badge-pill\">7<\/span> <button type=\"button\" class=\"btn btn-lg btn-danger\" data-toggle=\"popover\" title=\"Popover title\" >TRAIN THE MODEL<\/button>\n <\/a>\n <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#ig\" role=\"tab\" aria-controls=\"profile\"><span class=\"badge badge-primary badge-pill\">8<\/span> <button type=\"button\" class=\"btn btn-lg btn-danger\" data-toggle=\"popover\" title=\"Popover title\" >IMAGE\/ GIF OUTPUT<\/button>\n <\/a>\n \n \n\n\n","311084db":"> The loss is calculated for each of these models, and the gradients are used to update the generator and discriminator.\n\nSource: https:\/\/www.tensorflow.org\/tutorials\/generative\/dcgan\n","6083a7b2":"> **Rendered faces will show how nicely the generator has become**","88ae0ee0":"# <p style=\"color:purple;font-size:20px \" >Please bring your comment and votes, if you find it useful.<\/p> \n# <p style=\"color:purple;font-size:20px \" >Have a nice coding!<\/p> \n\n","1fe85d89":"<a id =\"lf\"><\/a> \n\n# Loss Function\n\nGenerator and discriminator model are being trained independently, so that requires two discrete loss functions. \n\nAnd, in the discriminator training set, the x contains the input images and the y contains a value of 1 for real images and 0 for generated ones.\n\n![image.png](attachment:c3f15afc-8904-4762-9686-c697deafdfe9.png)\n"}}