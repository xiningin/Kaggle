{"cell_type":{"2856cc65":"code","2d36f5bf":"code","2881e4ec":"code","0713764f":"code","d30b5caf":"code","5cbe4a4a":"code","05606b97":"code","1a7a3ede":"code","c3da9af9":"code","608558f8":"code","9a3b2f9e":"code","2328c52c":"code","a1044707":"code","90b2b4ad":"code","b1ec8071":"code","3082c49e":"code","bd616e53":"code","0c504455":"code","09815c22":"code","ef0d27b6":"code","cb771a9c":"markdown","62aa87b0":"markdown","909a838f":"markdown","2ff75033":"markdown","ff2a5303":"markdown","639edd5f":"markdown","1f1f00ac":"markdown","e1d90dc2":"markdown","bbe1ee8d":"markdown","628bb4fd":"markdown","6fc7081e":"markdown","22d46446":"markdown"},"source":{"2856cc65":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import preprocessing\n\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split, validation_curve\n\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score,auc, accuracy_score\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import ensemble\nimport xgboost as XGB","2d36f5bf":"df = pd.read_csv(\"..\/input\/pima-indians-diabetes-database\/diabetes.csv\")\ndf.head().T","2881e4ec":"def replace_0(df,col) :\n    df1 = df.copy()\n    n = df.shape[0]\n    m = df[col].mean()\n    s = df[col].std()\n    for i in range(len(df)) :\n        if df.loc[i,col]==0 :\n            df1.loc[i,col] = np.random.normal(m,s)\n    return df1\n\ndf = replace_0(df,'Glucose')\ndf = replace_0(df,'BloodPressure')\ndf = replace_0(df,'SkinThickness')\ndf = replace_0(df,'Insulin')\ndf = replace_0(df,'BMI')\n\ndf.head().T","0713764f":"fig, axs = plt.subplots(1,5, figsize=(15,4))\naxs[0].hist(df.Glucose, bins=50)\naxs[1].hist(df.BloodPressure, bins=50)\naxs[2].hist(df.SkinThickness, bins=50)\naxs[3].hist(df.Insulin, bins=50)\naxs[4].hist(df.BMI, bins=50)","d30b5caf":"minmax = preprocessing.MinMaxScaler(feature_range=(0, 1))\ndf[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']] = minmax.fit_transform(df[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']])\n\nscaler = preprocessing.StandardScaler()\ndf[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']] = scaler.fit_transform(df[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']])\n\nsns.kdeplot(df.Pregnancies)\nsns.kdeplot(df.Glucose)\nsns.kdeplot(df.BloodPressure)\nsns.kdeplot(df.SkinThickness)\nsns.kdeplot(df.Insulin)\nsns.kdeplot(df.BMI)\nsns.kdeplot(df.DiabetesPedigreeFunction)\nsns.kdeplot(df.Age)","5cbe4a4a":"X = df.drop(['Outcome'], axis=1)\ny = df['Outcome']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)","05606b97":"lr = LogisticRegression()\nlr.fit(X_train,y_train)\ny_lr = lr.predict(X_test)","1a7a3ede":"print('Confusion Matrix:')\nprint(confusion_matrix(y_test,y_lr))\n\nprint('')\nprint('Accuracy Score:')\nprint(accuracy_score(y_test,y_lr))\n\nprint('')\nprint('Classification Report:')\nprint(classification_report(y_test,y_lr))","c3da9af9":"probas = lr.predict_proba(X_test)\n\ndfprobas = pd.DataFrame(probas,columns=['proba_0','proba_1'])\ndfprobas['y'] = np.array(y_test)\n\ndfprobas","608558f8":"plt.figure(figsize=(10,10))\nsns.distplot(1-dfprobas.proba_0[dfprobas.y==0], bins=50)\nsns.distplot(dfprobas.proba_1[dfprobas.y==1], bins=50)","9a3b2f9e":"false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,probas[:, 1])\nroc_auc = auc(false_positive_rate, true_positive_rate)\nprint (roc_auc)","2328c52c":"plt.figure(figsize=(12,12))\nplt.title('Receiver Operating Characteristic')\nplt.plot(false_positive_rate, true_positive_rate, 'b', label='AUC = %0.2f'% roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')        # plus mauvaise courbe\nplt.plot([0,0,1],[0,1,1],'g:')     # meilleure courbe\nplt.xlim([-0.1,1.2])\nplt.ylim([-0.1,1.2])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')","a1044707":"rf = ensemble.RandomForestClassifier()\nrf.fit(X_train,y_train)\ny_rf = rf.predict(X_test)\n\nprint('Confusion Matrix:')\nprint(confusion_matrix(y_test,y_rf))\n\nprint('')\nprint('Accuracy Score:')\nprint(accuracy_score(y_test,y_rf))\n\nprint('')\nprint('Classification Report:')\nprint(classification_report(y_test,y_rf))","90b2b4ad":"rf1 = ensemble.RandomForestClassifier(n_estimators=10, min_samples_leaf=10, max_features=3)\nrf1.fit(X_train,y_train)\ny_rf1 = rf.predict(X_test)\n\nprint('Confusion Matrix:')\nprint(confusion_matrix(y_test,y_rf1))\n\nprint('')\nprint('Accuracy Score:')\nprint(accuracy_score(y_test,y_rf1))\n\nprint('')\nprint('Classification Report:')\nprint(classification_report(y_test,y_rf1))","b1ec8071":"params = np.arange(1, 300,step=30)\ntrain_score, val_score = validation_curve(rf, X, y, 'n_estimators', params, cv=7)\nplt.figure(figsize=(12,12))\nplt.plot(params, np.median(train_score, 1), color='blue', label='training score')\nplt.plot(params, np.median(val_score, 1), color='red', label='validation score')\nplt.legend(loc='best')\nplt.ylim(0, 1)\nplt.xlabel('n_estimators')\nplt.ylabel('score');","3082c49e":"param_grid = {\n              'n_estimators': [10, 100, 500],\n              'min_samples_leaf': [1, 20, 50]\n             }\nestimator = ensemble.RandomForestClassifier()\n\nrf_gs = model_selection.GridSearchCV(estimator, param_grid)\nrf_gs.fit(X_train, y_train)\nprint(rf_gs.best_params_)","bd616e53":"rf2 = rf_gs.best_estimator_\ny_rf2 = rf2.predict(X_test)\n\nprint('Confusion Matrix:')\nprint(confusion_matrix(y_test,y_rf2))\n\nprint('')\nprint('Accuracy Score:')\nprint(accuracy_score(y_test,y_rf2))\n\nprint('')\nprint('Classification Report:')\nprint(classification_report(y_test,y_rf2))","0c504455":"importances = rf2.feature_importances_\nindices = np.argsort(importances)","09815c22":"plt.figure(figsize=(8,5))\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nplt.yticks(range(len(indices)), X_test.columns[indices])\nplt.title('Importance des caracteristiques')","ef0d27b6":"xgb  = XGB.XGBClassifier()\nxgb.fit(X_train,y_train)\ny_xgb = xgb.predict(X_test)\n\nprint('Confusion Matrix:')\nprint(confusion_matrix(y_test,y_xgb))\n\nprint('')\nprint('Accuracy Score:')\nprint(accuracy_score(y_test,y_xgb))\n\nprint('')\nprint('Classification Report:')\nprint(classification_report(y_test,y_xgb))","cb771a9c":"## XGBoost","62aa87b0":"## R\u00e9gression logistique","909a838f":"# Analyse du dataset : \"Pima Indians Diabetes\"\n### Linear regression, Random forest et XGBoost\n\n##### \n##### Pr\u00e9diction du diab\u00e8tes sur le dataset :\n##### https:\/\/www.kaggle.com\/uciml\/pima-indians-diabetes-database\n\n###### \n###### En utilisant des m\u00e9thodes sur le notebook :\n###### https:\/\/www.kaggle.com\/pyim59\/2-titanic-v3-6-test","2ff75033":"## Librairies et fonctions utiles","ff2a5303":"Cette forme de remplacement des informations manquantes fonctionne tr\u00e8s bien, car bien que simple, elle ne modifie pas les distributions de chaque colonne. Cependant, la m\u00e9thode ne tient pas compte de la corr\u00e9lation entre les donn\u00e9es, et il est possible d'am\u00e9liorer la substitution en utilisant l'estimation, en choisissant de nouvelles valeurs par le biais de r\u00e9gressions, par exemple.","639edd5f":"Ce jeu de donn\u00e9es provient du National Institute of Diabetes and Digestive and Kidney Diseases. L'objectif de ce jeu de donn\u00e9es est de pr\u00e9dire de mani\u00e8re diagnostique si un patient est diab\u00e9tique ou non, sur la base de certaines mesures diagnostiques incluses dans le jeu de donn\u00e9es. Plusieurs contraintes ont \u00e9t\u00e9 impos\u00e9es \u00e0 la s\u00e9lection de ces instances \u00e0 partir d'une base de donn\u00e9es plus importante. En particulier, tous les patients sont des femmes \u00e2g\u00e9es d'au moins 21 ans et d'origine indienne Pima.\n\nL'ensemble de donn\u00e9es se compose de plusieurs variables pr\u00e9dictives m\u00e9dicales et d'une variable cible, le r\u00e9sultat. Les variables pr\u00e9dictives comprennent le nombre de grossesses que la patiente a eues, son IMC, son niveau d'insuline, son \u00e2ge, etc.","1f1f00ac":"# Cr\u00e9ation des jeux d'apprentissage et de test","e1d90dc2":"### Donn\u00e9es manquantes\n\nCertaines donn\u00e9es, contenues dans les colonnes \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\" et \"BMI\", n'ont pas \u00e9t\u00e9 obtenues. Au lieu de cela, ils ont la valeur 0, ce qui est illogique. Pour g\u00e9rer cela, un remplacement des cellules probl\u00e9matiques est effectu\u00e9 \u00e0 l'aide de la fonction donn\u00e9e. La fonction g\u00e9n\u00e8re de nouvelles valeurs en prenant la moyenne et l'\u00e9cart-type des donn\u00e9es.","bbe1ee8d":"## Normalisation du dataset","628bb4fd":"### Importance des caract\u00e9ristiques","6fc7081e":"## Ajustement des hyperparam\u00e8tres (Random Forests)","22d46446":"### Mesures de performance"}}