{"cell_type":{"15fdde82":"code","c6363381":"code","92a5eb47":"code","dae24cf0":"code","1e85e291":"code","517f33bf":"code","bd446a32":"code","1810df48":"code","bd6e580f":"code","70d3ed04":"code","626d3d34":"code","3c47215e":"code","0331ed33":"code","8b4219a0":"code","800ffbed":"code","92b599e8":"code","b6c71462":"code","2f23a15f":"code","7dee3165":"code","9e4586a3":"code","f4804615":"code","e3b110a6":"code","ed7c1a19":"code","028af935":"code","da1915c9":"code","58d02a38":"code","dc1a7d20":"code","f19f3494":"code","e6d08d0b":"code","0dd35884":"code","eef5a4d5":"code","8935d946":"code","0d4de076":"code","6d5cf1d0":"code","58d413f7":"markdown","eb0a0960":"markdown","a27c99f1":"markdown","b6d90d27":"markdown","1f674cbd":"markdown","3e051489":"markdown","8fc793af":"markdown","834e6448":"markdown"},"source":{"15fdde82":"from concurrent.futures import ProcessPoolExecutor","c6363381":"from concurrent.futures import ProcessPoolExecutor\nimport itertools\nimport functools\nimport time\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nimport seaborn as sns\n\nimport sklearn\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, LassoCV, ElasticNet, ElasticNetCV\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold, cross_val_score, GridSearchCV\nfrom sklearn.metrics import roc_curve, make_scorer, mean_squared_error\nfrom sklearn.preprocessing import RobustScaler\n\nfrom mlxtend.regressor import StackingCVRegressor\n\nimport lightgbm as lgb\nimport xgboost as xgb","92a5eb47":"%%script false --no-raise-error\ntrain = pd.read_csv('.\/train.csv')\ntest = pd.read_csv('.\/test.csv')","dae24cf0":"# %%script false --no-raise-error\ntrain = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","1e85e291":"\ntrain_ids = train.Id\ntest_ids = test.Id\ntrain_y = train.pop('SalePrice')","517f33bf":"\ndef plotting_3_chart(df, feature=None):\n    ## Importing seaborn, matplotlab and scipy modules. \n    import seaborn as sns\n    import matplotlib.pyplot as plt\n    import matplotlib.gridspec as gridspec\n    from scipy import stats\n    import matplotlib.style as style\n    style.use('fivethirtyeight')\n    \n    if type(df) == pd.Series:\n        print(\"it's series\")\n        to_plot = df\n    else:\n        print(\"it's dataframe\")\n        to_plot = df.loc[:,feature]\n\n    ## Creating a customized chart. and giving in figsize and everything. \n    fig = plt.figure(constrained_layout=True, figsize=(10,7))\n    ## creating a grid of 3 cols and 3 rows. \n    grid = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)\n    #gs = fig3.add_gridspec(3, 3)\n\n    ## Customizing the histogram grid. \n    ax1 = fig.add_subplot(grid[0, :2])\n    ## Set the title. \n    ax1.set_title('Histogram')\n    ## plot the histogram. \n    sns.distplot(to_plot, norm_hist=True, ax = ax1)\n\n    # customizing the QQ_plot. \n    ax2 = fig.add_subplot(grid[1, :2])\n    ## Set the title. \n    ax2.set_title('QQ_plot')\n    ## Plotting the QQ_Plot. \n    stats.probplot(to_plot, plot = ax2)\n\n    ## Customizing the Box Plot. \n    ax3 = fig.add_subplot(grid[:, 2])\n    ## Set title. \n    ax3.set_title('Box Plot')\n    ## Plotting the box plot. \n    sns.boxplot(to_plot, orient='v', ax = ax3 );","bd446a32":"\ndef allyficy (train, test):\n    return pd.concat([train, test]).reset_index(drop=True)","1810df48":"\nclass Get_rid_of_Nans(BaseEstimator, TransformerMixin):\n    def __init__ (self):\n        pass\n    \n    def fit (self, X, y=None):\n        return self\n    \n    def Stringificy (self, X):\n        # make int features to string ones should be\n        X['MSZoning'] = X.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n        X['YrSold'] = X.YrSold.astype(str)\n        X['YearRemodAdd'] = X.YearRemodAdd.astype(str)\n        X['MoSold']= X['MoSold'].astype(str)\n        X['YearBuilt']= X['YearBuilt'].astype(str)\n        \n    def Nans_from_str (self, X):\n        # fill nas in string where should be zero\n        missing_val_col = [\"Alley\", \"PoolQC\", \"MiscFeature\", \"Fence\", \"FireplaceQu\", \"GarageType\", \"GarageFinish\", \"GarageQual\", \n                           \"GarageCond\", 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'MasVnrType']\n        for i in missing_val_col:\n            X[i] = X[i].fillna('None')\n        # fill nas in strings\n        X['Electrical'] = X['Electrical'].fillna(\"SBrkr\")\n        X['Exterior1st'] = X['Exterior1st'].fillna(X['Exterior1st'].mode()[0])\n        X['Exterior2nd'] = X['Exterior2nd'].fillna(X['Exterior2nd'].mode()[0])\n        X['Functional'] = X['Functional'].fillna(X['Functional'].mode()[0])\n        X['SaleType'] = X['SaleType'].fillna(X['SaleType'].mode()[0])\n        X['Utilities'] = X['Utilities'].fillna(X['Utilities'].mode()[0])\n        X['KitchenQual'] = X['KitchenQual'].fillna(X['KitchenQual'].mode()[0])\n            \n    def Nans_from_int (self, X):\n        # fill nas where should be zero\n        ## These features are continous variable, we used \"0\" to replace the null values. \n        missing_val_col2 = ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'GarageYrBlt',\n                    'GarageArea', 'GarageCars', 'MasVnrArea']\n        for i in missing_val_col2:\n            X[i] = X[i].fillna(0)\n            \n    def Nans_from_float (self, X):\n        # fill nas in floats\n        X['LotFrontage'] = X.groupby('Neighborhood')['LotFrontage'].transform( lambda x: x.fillna(x.mean()))\n\n    \n    def transform (self, X, y=None):\n        self.X = X.copy()\n        self.Stringificy(self.X)\n        self.Nans_from_int(self.X)\n        self.Nans_from_str(self.X)\n        self.Nans_from_float(self.X)\n        return self.X\n    \n    def fit_transform (self, X, y=None):\n        return self.fit(X).self.transform(X)","bd6e580f":"\nclass Create_dummies (TransformerMixin):\n    def __init__ (self):\n        pass\n    \n    def fit (self):\n        return self\n    \n    def transform (self, df):\n        return pd.get_dummies(df).reset_index(drop=True)","70d3ed04":"\nclass Overfit_reducer (TransformerMixin):\n    def __init__ (self):\n        pass\n    \n    def overfit_reducer(self, df):\n        \"\"\"\n        This function takes in a dataframe and returns a list of features that are overfitted.\n        \"\"\"\n        overfit = []\n        for i in df.columns:\n            counts = df[i].value_counts()\n            zeros = counts.iloc[0]\n            if zeros \/ len(df) * 100 > 99.94:\n                overfit.append(i)\n        overfit = list(overfit)\n        print('list of overfitted features: ',overfit)\n        return overfit\n \n    \n    def fit (self):\n        return self\n    \n    def transform (self, df):\n        return df.drop(self.overfit_reducer(df), axis=1)","626d3d34":"\nclass Add_new_features(TransformerMixin):\n    def __init__ (self):\n        pass\n    \n    def add_features (self, df):\n        # feture engineering a new feature \"TotalFS\"\n        df['TotalSF'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\n        df['YrBltAndRemod'] = df['YearBuilt']+df['YearRemodAdd']\n\n        df['Total_sqr_footage'] = (df['BsmtFinSF1'] + df['BsmtFinSF2'] +\n                                         df['1stFlrSF'] + df['2ndFlrSF'])\n\n        df['Total_Bathrooms'] = (df['FullBath'] + (0.5 * df['HalfBath']) +\n                                       df['BsmtFullBath'] + (0.5 * df['BsmtHalfBath']))\n\n        df['Total_porch_sf'] = (df['OpenPorchSF'] + df['3SsnPorch'] +\n                                      df['EnclosedPorch'] + df['ScreenPorch'] +\n                                      df['WoodDeckSF'])\n\n        df['haspool'] = df['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n        df['has2ndfloor'] = df['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n        df['hasgarage'] = df['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n        df['hasbsmt'] = df['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n        df['hasfireplace'] = df['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n        \n        return df\n    \n    def fit (self):\n        return self\n    \n    def transform (self, df):\n        return self.add_features(df)","3c47215e":"\nclass Separate (TransformerMixin):\n    def __init__ (self, y):\n        self.y =y\n    \n    def fit (self):\n        return self\n    \n    def transform (self, X):\n        return {'train': X.iloc[:len(self.y),:], 'test': X.iloc[len(train_y):,:]}\n    \n    \nclass Numerificy (TransformerMixin):\n    def __init__ (self):\n        pass\n    \n    def fit (self):\n        return self\n    \n    def transform (self, X):\n        train_num = X['train'].select_dtypes(exclude=\"object\")\n        test_num = X['test'].select_dtypes(exclude='object')\n        return {'train': train_num, 'test': test_num}\n\n","0331ed33":"\n# preprocessing data\nmodel_1st_without_nans_and_strings = Pipeline([('without_nans', Get_rid_of_Nans()),  \n                                               ('dummyficeted', Create_dummies()),\n                                ('overfit_reduced', Overfit_reducer()),\n                                ('separated', Separate(train_y)),\n                                ('numyficyed', Numerificy())])\n\nall_data = allyficy(train, test)\ntrain_X, test_X = model_1st_without_nans_and_strings.transform(all_data).values()","8b4219a0":"\n# preprocessing data + RobustScaler\nr = RobustScaler().fit(train_X)\ntrain_X_robust, test_X_robust = r.transform(train_X), r.transform(test_X)","800ffbed":"#my inplementation of rmsle metrc\ndef rmsle(clf, X, y):\n    y_ = clf.predict(X)\n    return -np.sqrt(mean_squared_error(np.log1p(y),np.log1p(y_)))","92b599e8":"def plot_functions_from_params(all_params, scores):\n    fig = plt.figure(figsize=(12,8))\n    ax = fig.add_subplot(111, projection='3d')\n    ax.scatter([i['alpha'] for i in all_params if i['max_iter']==10], [i['l1_ratio'] for i in all_params[::2]], scores[::2], c=scores[::2])","b6c71462":"predict_LM = Pipeline([('model', LinearRegression())])\npredict_model_Elastic = Pipeline([('model', ElasticNet())])\n\n","2f23a15f":"\nclass Tune_XGB:\n    def __init__(self, train_X, train_y, test_X, base_params=None):\n        self.train_X, self.train_y, self.test_X = train_X.copy(), train_y.copy(), test_X.copy()\n        FOLDS = 5\n        EARLY_STOP = 50\n        MAX_ROUNDS = 5\n        if base_params == None:\n            self.base_params =  {\n                'n_estimators': 100,\n                \n                'max_depth': -1,\n                'min_child_weight': 0,\n                \n                'gamma': 10,\n                \n                'subsample': 0.8,\n                'colsample_bytree': 0.5,\n                \n                'reg_alpha': 0.0,\n                'reg_lambda': 1,\n                \n                'learning_rate': 0.01,\n\n            \n            'objective': 'reg:linear',\n            'silent': 1,\n            'seed': 42,      \n            'verbosity':0}\n            \n        else:\n            self.base_params = base_params\n            \n\n    def update(self, base_dict, update_copy):\n        for key in update_copy.keys():\n            base_dict[key] = update_copy[key]\n            \n    def Step_0_find_n_estimators(self, n_estimators):\n        print(n_estimators)\n        test0 = {\n            'n_estimators': n_estimators,\n        }\n        return self.grid_search(self.base_params, test0)\n    \n    def Step_05_find_n_estimators(self, test05):\n        curent = self.base_params['n_estimators']\n        \n        test05 = {\n            'n_estimators': [int(curent\/1.3), int(curent\/1.1), curent, int(curent\/0.9), int(curent\/0.7)]\n        }\n        return self.grid_search(self.base_params, test05)\n    \n\n\n    def Step_1_find_depth_and_child(self, test1):\n\n        return self.grid_search(self.base_params, test1)\n\n    def Step_2_narrow_depth(self, test2):\n        max_depth = self.base_params['max_depth']\n        test2 = {\n            'max_depth': [max_depth-1,max_depth,max_depth+1]\n        }\n        return self.grid_search(self.base_params, test2)\n\n    def Step_3_gamma(self, test3):\n        return self.grid_search(self.base_params, test3)\n\n    def Step_4_sample(self, test4):\n        return self.grid_search(self.base_params, test4)\n\n    def Step_5_reg1(self, test5):\n        return self.grid_search(self.base_params, test5)\n    \n    def Step_6_eta_nround(self, test6):\n        return self.grid_search(self.base_params, test6)\n    \n    \n\n    def train_test(self, X, y):\n        train_X_v, test_X_v, train_y_v, test_y_v = train_test_split(X, y, test_size=0.2)\n        return train_X_v, test_X_v, train_y_v, test_y_v\n    \n    def validate(self, model):\n        X,x, Y,y = self.train_test(self.train_X, self.train_y)\n        model.fit(X, Y, \n                eval_set=[(X, Y), (x, y)],\n                eval_metric='rmse',\n                verbose=False)\n        train_pred = model.predict(X)\n        test_pred = model.predict(x)\n        \n        train_score = mean_squared_error(train_pred, Y)\n        test_score = mean_squared_error(test_pred, y)\n        \n        error = model.evals_result()\n        eval_score = error['validation_0']['rmse'][-1]\n        \n#         self.draw(error)\n        return test_score, train_score, eval_score\n            \n \n    def grid_search(self, base_params, grid):\n        print(\"Starn Tuning: \", grid)\n        if self.mode==1:\n            return self.grid_search_parallel(base_params, grid)\n        else:\n            keys = set(grid.keys())\n            l = [grid[x] for x in keys]\n            perm = list(itertools.product(*l))\n            jobs = []\n            for i in perm:\n                jobs.append({k:v for k,v in zip(keys,i)})\n\n            test_score = []\n            train_score = []\n            eval_score = []\n\n            for i, job in enumerate(jobs):\n                base_params = self.base_params.copy()\n                print(job)\n                self.update(base_params, job)\n                model = xgb.XGBRegressor(**base_params)\n                score = self.validate(model)\n                test_score.append((job, score[0]))\n                train_score.append((job, score[1]))\n                eval_score.append((job, score[2]))\n            return {'train_scores':train_score, 'test_scores':test_score, 'eval_scores':eval_score}\n        \n    def grid_search_parallel(self, base_params, grid):\n        keys = set(grid.keys())\n        l = [grid[x] for x in keys]\n        perm = list(itertools.product(*l))\n        jobs = []\n        for i in perm:\n            jobs.append({k:v for k,v in zip(keys,i)})\n\n        test_score = []\n        train_score = []\n        eval_score = []\n\n        e = ProcessPoolExecutor()\n\n\n        score = list(e.map(self.build_val ,jobs))\n        for i, scor in enumerate(score):\n            test_score.append((jobs[i], scor[0]))\n            train_score.append((jobs[i], scor[1]))\n            eval_score.append((jobs[i], scor[2]))\n\n        return {'train_scores':train_score, 'test_scores':test_score, 'eval_scores':eval_score}\n\n    def build_val(self, job):\n        base_params = self.base_params.copy()\n        self.update(base_params, job)\n        model = xgb.XGBRegressor(**base_params)\n        score = self.validate(model)\n        return(score)\n    \n    def draw(self, errors=None):\n        fig, ax = plt.subplots(figsize=(6,4))\n        ax.plot(range(len(errors['validation_0']['rmse'])), errors['validation_0']['rmse'])\n        plt.show()\n            \n    def draw_all(self, errors):\n        fig, ax = plt.subplots(figsize=(6,4))\n        ax.plot(range(len(errors['train_scores'])),[i[1] for i in errors['train_scores']], label='train')\n        ax.plot(range(len(errors['test_scores'])),[i[1] for i in errors['test_scores']], label='test')\n        ax.plot(range(len(errors['eval_scores'])),[i[1] for i in errors['eval_scores']], label='eval_scores')\n        ax.legend()\n        plt.show()\n        \n    def start_tuning(self, num, mode=0):\n        self.mode=mode\n        print(\"loading\")\n        start_time = time.time()\n        base_params = dict(self.base_params)\n        \n        steps = {self.Step_0_find_n_estimators:[100, 200, 400, 800,1600,3200],\n            self.Step_05_find_n_estimators:None,\n            self.Step_1_find_depth_and_child:{'max_depth': [10, 14, 18, 22, -1], \n                                           'min_child_weight': list(range(1,10,2))},\n            self.Step_2_narrow_depth: None,\n            self.Step_3_gamma:{'gamma': list([i\/10.0 for i in range(0,5)])},\n            self.Step_4_sample: {'subsample': list([i\/10.0 for i in range(6,10)]),\n                              'colsample_bytree': list([i\/10.0 for i in range(6,10)])},\n            self.Step_5_reg1:{'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100],\n                             'reg_lambda': [1e-5, 1e-2, 0.1, 1, 100]},\n            self.Step_6_eta_nround:{'learning_rate':[0.001, 0.005, 0.01, 0.05, 0.1, 0.3]},\n                }\n        \n        for i,key in enumerate(steps.keys()):\n            if i in range(num):\n                e = ProcessPoolExecutor\n                result = key(steps[key])\n                self.draw_all(result)\n                best_param = min(result['test_scores'], key=lambda x:x[1])\n            \n                print('best selected param:', best_param)\n                self.update(self.base_params, best_param[0])\n                print(f'params after step {i}:', self.base_params)\n\n\n        print(\"FINAL RESUTL: {}\".format(self.base_params))\n        elapsed_time = time.time() - start_time\n        \n       \n        \nprint('done')\ntune = Tune_XGB(train_X_robust, train_y, test_X_robust)","7dee3165":"class Tune_ligthGBM:\n    def __init__(self, train_X, train_y, test_X, base_params=None):\n        self.train_X, self.train_y, self.test_X = train_X.copy(), train_y.copy(), test_X.copy()\n        if base_params == None:\n            self.base_params =  {'n_estimators':100,\n                \n                                'max_depth':7,                                 \n                                 'num_leaves': 31,\n                                 'min_data_in_leaf':20,\n                                 \n                                'bagging_fraction':0.75,\n                                'bagging_freq':5, \n                                'bagging_seed':7,                                 \n                                'feature_fraction':0.8,\n                                'feature_fraction_seed':7,\n                                 \n                                'reg_alpha': 0.0,\n                                'reg_lambda': 1,\n                                 \n                                 'learning_rate':0.01,\n                            \n                                 'objective':'regression', \n                            'num_leaves':4,\n                            'max_bin':200, \n                            \n                            \n                            'verbose':-1,}\n        else:\n            self.base_params = base_params\n            \n\n    def update(self, base_dict, update_copy):\n        for key in update_copy.keys():\n            base_dict[key] = update_copy[key]\n            \n    def Step_0_find_n_estimators(self, n_estimators):\n        print(n_estimators)\n        test0 = {\n            'n_estimators': n_estimators,\n        }\n        return self.grid_search(self.base_params, test0)\n    \n    def Step_05_find_n_estimators(self, test05):\n        curent = self.base_params['n_estimators']\n        \n        test05 = {\n            'n_estimators': [int(curent\/1.3), int(curent\/1.1), curent, int(curent\/0.9), int(curent\/0.7)]\n        }\n        return self.grid_search(self.base_params, test05)\n    \n    def Step_06_min_child_weight(self, teset06):\n        print(teset06)\n        return self.grid_search(self.base_params, teset06)\n    \n    def Step_07_min_child_weight_closer(self, teset07):\n        curent = self.base_params['min_child_weight']\n        if curent>=4:\n            teset07={\n                'min_child_weight':[int(curent\/1.3),curent, int(curent\/0.7)]\n            }\n        else:\n            teset07={'min_child_weight': [curent]}\n\n        return self.grid_search(self.base_params, teset07)\n\n    def Step_1_find_depth_and_child(self, test1):\n\n        return self.grid_search(self.base_params, test1)\n\n    def Step_2_narrow_depth(self, test2):\n        max_depth = self.base_params['max_depth']\n        test2 = {\n            'max_depth': [max_depth-1,max_depth,max_depth+1]\n        }\n        return self.grid_search(self.base_params, test2)\n\n\n    def Step_4_sample(self, test4):\n        return self.grid_search(self.base_params, test4)\n\n    def Step_5_reg1(self, test5):\n        return self.grid_search(self.base_params, test5)\n    \n    def Step_6_eta_nround(self, test6):\n        return self.grid_search(self.base_params, test6)\n\n    def train_test(self, X, y):\n        train_X_v, test_X_v, train_y_v, test_y_v = train_test_split(X, y, test_size=0.2)\n        return train_X_v, test_X_v, train_y_v, test_y_v\n    \n    def validate(self, model):\n        X,x, Y,y = self.train_test(self.train_X, self.train_y)\n        model.fit(X, Y, \n                eval_set=[(X, Y), (x, y)],\n                eval_metric='rmse',\n                verbose=False)\n        train_pred = model.predict(X)\n        test_pred = model.predict(x)\n        \n        train_score = mean_squared_error(train_pred, Y)\n        test_score = mean_squared_error(test_pred, y)\n        \n        \n#         self.draw(error)\n        return test_score, train_score\n            \n \n    def grid_search(self, base_params, grid):\n        print(\"Starn Tuning: \", grid)\n        if self.mode==1:\n            return self.grid_search_parallel(base_params, grid)\n        else:\n            keys = set(grid.keys())\n            l = [grid[x] for x in keys]\n            perm = list(itertools.product(*l))\n            jobs = []\n            for i in perm:\n                jobs.append({k:v for k,v in zip(keys,i)})\n\n            test_score = []\n            train_score = []\n\n            for i, job in enumerate(jobs):\n                base_params = self.base_params.copy()\n                print(job)\n                self.update(base_params, job)\n                model = lgb.LGBMRegressor(**base_params)\n                score = self.validate(model)\n                test_score.append((job, score[0]))\n                train_score.append((job, score[1]))\n                eval_score.append((job, score[2]))\n            return {'train_scores':train_score, 'test_scores':test_score}\n        \n    def grid_search_parallel(self, base_params, grid):\n        keys = set(grid.keys())\n        l = [grid[x] for x in keys]\n        perm = list(itertools.product(*l))\n        jobs = []\n        for i in perm:\n            jobs.append({k:v for k,v in zip(keys,i)})\n\n        test_score = []\n        train_score = []\n\n        e = ProcessPoolExecutor()\n        \n        score = list(map(self.build_val ,jobs))\n        for i, scor in enumerate(score):\n            test_score.append((jobs[i], scor[0]))\n            train_score.append((jobs[i], scor[1]))\n\n        return {'train_scores':train_score, 'test_scores':test_score}\n\n    def build_val(self, job):\n        base_params = self.base_params.copy()\n        self.update(base_params, job)\n        model = lgb.LGBMRegressor(**base_params)\n        score = self.validate(model)\n        return(score)\n    \n    def draw(self, errors=None):\n        fig, ax = plt.subplots(figsize=(6,4))\n        ax.plot(range(len(errors['validation_0']['rmse'])), errors['validation_0']['rmse'])\n        plt.show()\n            \n    def draw_all(self, errors):\n        fig, ax = plt.subplots(figsize=(6,4))\n        ax.plot(range(len(errors['train_scores'])),[i[1] for i in errors['train_scores']], label='train')\n        ax.plot(range(len(errors['test_scores'])),[i[1] for i in errors['test_scores']], label='test')\n        ax.legend()\n        plt.show()\n        \n    def start_tuning(self, num, mode=0):\n        self.mode=mode\n        print(\"loading\")\n        start_time = time.time()\n        base_params = dict(self.base_params)\n        \n        steps = {self.Step_0_find_n_estimators:[100, 200, 400, 800, 1600, 3200],\n            self.Step_05_find_n_estimators:None,\n            \n            self.Step_1_find_depth_and_child:{'max_depth': list(range(10,20,2)) ,                                 \n                                             'num_leaves': [20, 31, 40],\n                                             'min_data_in_leaf':[10, 15, 20, 25, 30]},\n            self.Step_2_narrow_depth: None,\n                 \n            self.Step_4_sample: {'bagging_fraction':[0.2, 0.4, 0.6, 0.8],\n                                'feature_fraction': [0.2, 0.4, 0.6, 0.8]},\n            self.Step_5_reg1:{'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100],\n                             'reg_lambda':[1e-5, 1e-2, 0.1, 1, 100]},\n            self.Step_6_eta_nround:{'learning_rate':[0.001, 0.005, 0.01, 0.05, 0.1, 0.3]}\n                }\n        for i,key in enumerate(steps.keys()):\n            if i in range(num):\n                e = ProcessPoolExecutor\n                result = key(steps[key])\n                self.draw_all(result)\n                best_param = min(result['test_scores'], key=lambda x:x[1])\n            \n                print('best selected param:', best_param)\n                self.update(self.base_params, best_param[0])\n                print(f'params after step {i}:', self.base_params)\n\n\n        print(\"FINAL RESUTL: {}\".format(self.base_params))\n        elapsed_time = time.time() - start_time\n        \n       \n        \nprint('done')\ntune_l = Tune_ligthGBM(train_X_robust, train_y, test_X_robust)","9e4586a3":"%%script false --no-raise-error\ntune_l.start_tuning(10, mode=1)","f4804615":"%%script false --no-raise-error\ntune.start_tuning(10, mode=1)\n","e3b110a6":"# XGB params \n\nmy_local1 = {'objective': 'reg:linear', \n          'learning_rate': 0.01, \n          'seed': 42, \n          'n_estimators': 3200, \n          'max_depth': 5, \n          'min_child_weight': 9, \n          'gamma': 0.0, \n          'subsample': 0.6, \n          'colsample_bytree': 0.6, \n          'reg_alpha': 0.01} # 0.13015\n\nmy_local_eval={'n_estimators': 4571, 'num_round': 50, 'max_depth': 14, 'min_child_weight': 1, 'gamma': 0.0, 'subsample': 0.9, 'colsample_bytree': 0.6, 'reg_alpha': 0.01, \n               'reg_lambda': 1e-05, 'learning_rate': 0.0005, 'objective': 'reg:linear',  'seed': 42, 'verbosity': 0} #  0.13015\n\nmy_local_test_1_log =  {'n_estimators': 4571, 'num_round': 50, 'max_depth': 6, 'min_child_weight': 11, 'gamma': 0.1, 'subsample': 0.8, 'colsample_bytree': 0.6, 'reg_alpha': 0.01, \n                        'reg_lambda': 0.1, 'learning_rate': 0.01, 'objective': 'reg:linear', 'seed': 42, 'verbosity': 0} #0.12847\n\n\n\n\n","ed7c1a19":"# lgbm models\n\nmy_local_l2 = {'n_estimators': 6400, \n               'max_depth': -1, \n               'num_leaves': 40, \n               'min_data_in_leaf': 15, \n               'bagging_fraction': 0.6,\n               'bagging_freq': 5, \n               'bagging_seed': 7, \n               'feature_fraction': 0.8, \n               'feature_fraction_seed': 7, \n               'reg_alpha': 1e-05, \n               'reg_lambda': 1, \n               'learning_rate': 0.01,\n               'objective': 'regression',\n               'max_bin': 200,\n               'verbose': -1} # 0.13121\n\n\nmy_local_l4 = {'n_estimators': 2909, 'max_depth': 8, 'num_leaves': 20, 'min_data_in_leaf': 20, \n               'bagging_fraction': 0.8, 'bagging_freq': 5, 'bagging_seed': 7, \n               'feature_fraction': 0.6, 'feature_fraction_seed': 7, 'reg_alpha': 0.1,\n               'reg_lambda': 1, 'learning_rate': 0.01, \n               'objective': 'regression', 'max_bin': 200, 'verbose': -1} #  0.12883\n\nmy_local_l5 = {'n_estimators': 1777, 'max_depth': 10, 'num_leaves': 40, 'min_data_in_leaf': 10, 'bagging_fraction': 0.6, 'bagging_freq': 5, 'bagging_seed': 7, \n          'feature_fraction': 0.8, 'feature_fraction_seed': 7, 'reg_alpha': 1e-05, 'reg_lambda': 1e-05, 'learning_rate': 0.005, 'objective': 'regression',\n               'max_bin': 200, 'verbose': -1} #0.12808\n\ntest_l = {'n_estimators': 6400, 'max_depth': 17, 'num_leaves': 40, 'min_data_in_leaf': 15, 'bagging_fraction': 0.8, 'bagging_freq': 5, 'bagging_seed': 7,\n          'feature_fraction': 0.4, 'feature_fraction_seed': 7, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'learning_rate': 0.05, 'objective': 'regression', \n          'max_bin': 200, 'verbose': -1} #0.12947","028af935":"# elastic params\nbase = 0.39823\n\nmy_first = {'l1_ratio': 0.17, 'alpha': 0.01, 'max_iter': 12} # 0.14057\nmy_second = {'max_iter': 500.0, 'l1_ratio': 0.16, 'alpha': 0.01}#  0.13960\nmy_therd = {'max_iter': 50.0, 'l1_ratio': 0.16, 'alpha': 0.01} #0.13951\ntest_el = {'max_iter': 50.0, 'l1_ratio': 0.16, 'alpha': 0.01} #0.13266\n\n","da1915c9":"%%script false --no-raise-error\n# GreedSearch with elasticNet\nparametersGrid = {\"max_iter\": [5, 10],\n                      \"alpha\": np.arange(0.14, 0.18, 0.005),\n                      \"l1_ratio\": np.arange(0.5, 0.62, 0.005)}\nmodel = ElasticNet()\ngrid_search = GridSearchCV(model, parametersGrid, verbose=0)\ngrid_search.fit(train_X, train_y)\n\n\nbest_params = grid_search.best_params_\nall_params = grid_search.cv_results_['params']\nscores = grid_search.cv_results_['mean_test_score']\n\n\npredict = grid_search.predict(test_X)\nplot_functions_from_params(all_params, scores)\nbest_params","58d02a38":"%%script false --no-raise-error\n# GreedSearch with elasticNet + RobustScaler\nparametersGrid = {\"max_iter\": [5, 10],\n                      \"alpha\": np.arange(0.1, 0.2, 0.005),\n                      \"l1_ratio\": np.arange(0.3, 0.45, 0.005)}\nmodel = ElasticNet()\ngrid_search_robust = GridSearchCV(model, parametersGrid, verbose=0)\ngrid_search_robust.fit(train_X_robust, train_y)\n\n\nbest_params_rubost = grid_search_robust.best_params_\nall_params_rubost = grid_search_robust.cv_results_['params']\nscores_rubost = grid_search_robust.cv_results_['mean_test_score']\n\n\npredict = grid_search_robust.predict(test_X_robust)\n\nplot_functions_from_params(all_params_rubost, scores_rubost)\nbest_params_rubost","dc1a7d20":"%%script false --no-raise-error\nxg_model = xgb.XGBRegressor(**my_local_test_1_log, n_jobs = 4)\n# xg_model.fit(train_X, np.log1p(train_y))\n# predict_x = xg_model.predict(test_X)\n# predict_x = np.expm1(predict_x)","f19f3494":"# %%script false --no-raise-error\nlightgbm = lgb.LGBMRegressor(**test_l, n_jobs = 4)\nlightgbm.fit(train_X, np.log1p(train_y))\npredict_l = lightgbm.predict(test_X)\npredict_l = np.expm1(predict_l)","e6d08d0b":"%%script false --no-raise-error\nelastic = ElasticNet(**test_el)\nelastic_pipe = make_pipeline(RobustScaler(), elastic)\n# elastic_pipe.fit(train_X, np.log1p(train_y))\n# predict_e = elastic_pipe.predict(test_X)\n# predict_e = np.expm1(predict_e)","0dd35884":"%%script false --no-raise-error\nkfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n\ndef cv_rmse(model, X=train_X, y=np.log1p(train_y)):\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=kfolds))\n    return (rmse)\n\nalphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\n\nlasso_pipe = make_pipeline(RobustScaler(), LassoCV(max_iter=1e7, alphas=alphas2, random_state=42, cv=kfolds,n_jobs=4))\n\n# lasso_pipe.fit(train_X, np.log1p(train_y))\n# predict_las = lasso_pipe.predict(test_X)\n# predict_las = np.expm1(predict_las)","eef5a4d5":"%%script false --no-raise-error\nmeta_xgboost = xgb.XGBRegressor(n_jobs=4)\nstack = StackingCVRegressor(regressors=[elastic_pipe, lightgbm, xg_model],meta_regressor=meta_xgboost)\nstack.fit(np.array(train_X), np.log1p(train_y))\npredict_s = stack.predict(np.array(test_X))\npredict_s = np.expm1(predict_s)","8935d946":"\npredict = predict_l","0d4de076":"sub = pd.concat([test_ids, pd.Series(predict)], axis=1)\nsub = sub.rename(columns={0:'SalePrice'})\nsub.to_csv('sumbission.csv', index=False, header=True)","6d5cf1d0":"print(sub)\nprint('FINISHED')","58d413f7":"\n### --------------------------------------------------------------------\n# Metrics\n### ====================================================================","eb0a0960":"# My Hypotesys\n===========================================================================","a27c99f1":"### --------------------------------------------------------\n# Preprocess data\n### =======================================================","b6d90d27":"### --------------------------------------------------------------------\n# My EDA Tryes\n### ====================================================================","1f674cbd":"### ====================================================\n# Form sumbmission\n### ------------------------------\n\n","3e051489":"### ====================================================\n# Tryes\n### ------------------------------","8fc793af":"### -----------------------------------------------------------------------\n###  \n# Tuning models\n### ========================================================================","834e6448":"### ====================================================\n# Predict\n### ------------------------------"}}