{"cell_type":{"fff6c1f8":"code","e683b860":"code","c8a5dd4d":"code","cf8bb3e1":"code","038cfa98":"code","39dba6f1":"code","34c9cc2e":"code","c09e3383":"code","c481b2a5":"code","023f9a8d":"code","a260338b":"code","d9567798":"code","6bd8bb25":"code","70ff0b91":"code","9988cf84":"code","a7c6c1de":"code","2384c5af":"markdown"},"source":{"fff6c1f8":"# Data processing\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n\n# Machine Learning\nimport optuna\nimport xgboost as xgb\nfrom optuna.samplers import TPESampler\nfrom sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, KFold\nfrom sklearn.metrics import mean_squared_error","e683b860":"input_dir = Path('..\/input\/tabular-playground-series-aug-2021\/')\ntrain_df = pd.read_csv(input_dir \/ 'train.csv')\ntest_df = pd.read_csv(input_dir \/ 'test.csv')\nsample_submission = pd.read_csv(input_dir \/ 'sample_submission.csv')","c8a5dd4d":"train_df.head()","cf8bb3e1":"test_df.head()","038cfa98":"sample_submission.head()","39dba6f1":"X = train_df.drop(['id', 'loss'], axis=1).values\ny = train_df['loss'].values\nX_test = test_df.drop(['id'], axis=1).values","34c9cc2e":"# I've found many using MinMaxScaling but I've personally had better results with StandardScaling\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\nX_test = scaler.transform(X_test)","c09e3383":"y_min = y.min()\ny_max = y.max()\n\nprint(y_min, y_max)\n\n# While it's probably rare that values will fall outside the y-min-max range, we should probably do it anyway.\ndef my_rmse(y_true, y_hat):\n    y_true[y_true < y_min] = y_min\n    y_true[y_true > y_max] = y_max\n    \n    y_hat[y_hat < y_min] = y_min\n    y_hat[y_hat > y_max] = y_max\n    \n    y_true_nan = np.isnan(y_true)\n    y_hat_nan = np.isnan(y_hat)\n    \n    if y_true_nan.sum() > 0:\n        print(y_true_nan.sum())\n        np.where(y_true_nan, np.ma.array(y_true, mask=np.isnan(y_true)).mean(axis=0), y_true)\n    if y_hat_nan.sum() > 0:\n        print(y_hat_nan.sum())\n        np.where(y_hat_nan, np.ma.array(y_hat, mask=np.isnan(y_hat)).mean(axis=0), y_hat)\n    \n    return mean_squared_error(y_true, y_hat, squared=False)","c481b2a5":"def objective(trial):\n    # Split the train data for each trial.\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, stratify=y, test_size=0.4)\n\n    param_grid = {\n        'tweedie_variance_power': trial.suggest_discrete_uniform('tweedie_variance_power', 1.0, 2.0, 0.1),\n        'max_depth': trial.suggest_int('max_depth', 6, 10), # Extremely prone to overfitting!\n        'n_estimators': trial.suggest_int('n_estimators', 400, 4000, 400), # Extremely prone to overfitting!\n        'eta': trial.suggest_float('eta', 0.007, 0.013), # Most important parameter.\n        'subsample': trial.suggest_discrete_uniform('subsample', 0.2, 0.9, 0.1),\n        'colsample_bytree': trial.suggest_discrete_uniform('colsample_bytree', 0.2, 0.9, 0.1),\n        'colsample_bylevel': trial.suggest_discrete_uniform('colsample_bylevel', 0.2, 0.9, 0.1),\n        'min_child_weight': trial.suggest_loguniform('min_child_weight', 1e-4, 1e4), # I've had trouble with LB score until tuning this.\n        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-4, 1e4), # L2 regularization\n        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-4, 1e4), # L1 regularization\n        'gamma': trial.suggest_loguniform('gamma', 1e-4, 1e4),\n    } \n    \n    reg = xgb.XGBModel(\n        # These parameters should help with trial speed.\n        objective='reg:tweedie',\n        tree_method='gpu_hist',\n        predictor='gpu_predictor',\n        n_jobs=4,\n        **param_grid\n    )\n    \n    reg.fit(X_train, y_train,\n            eval_set=[(X_valid, y_valid)], eval_metric='rmse',\n            verbose=False)\n\n    # Returns the best RMSE for the trial.\n    # Readers may want to try returning a cross validation score here.\n    return my_rmse(y_valid, reg.predict(X_valid))","023f9a8d":"train_time = 1 * 60 * 60\nstudy = optuna.create_study(direction='minimize', sampler=TPESampler(), study_name='XGBRegressor')\nstudy.optimize(objective, timeout=train_time)\n\nprint('Number of finished trials: ', len(study.trials))\nprint('Best trial:')\ntrial = study.best_trial\n\nprint('\\tValue: {}'.format(trial.value))\nprint('\\tParams: ')\nfor key, value in trial.params.items():\n    print('\\t\\t{}: {}'.format(key, value))","a260338b":"optuna.visualization.plot_optimization_history(study)","d9567798":"optuna.visualization.plot_parallel_coordinate(study)","6bd8bb25":"optuna.visualization.plot_slice(study)","70ff0b91":"optuna.visualization.plot_param_importances(study)","9988cf84":"# Fetch the best trial parameters and set some settings for the KFold predictions.\nxgb_params = trial.params\nxgb_params['objective'] = 'reg:tweedie'\nxgb_params['tree_method'] = 'gpu_hist'\nxgb_params['predictor'] = 'gpu_predictor'\nxgb_params['n_jobs'] = 4\n        \nn_splits = 10\ntest_preds = None\nkf_rmse = []\n\nfor fold, (train_idx, valid_idx) in enumerate(KFold(n_splits=n_splits, shuffle=True).split(X, y)):\n    # Fetch the train-validation indices.\n    X_train, y_train = X[train_idx], y[train_idx]\n    X_valid, y_valid = X[valid_idx], y[valid_idx]\n    \n    # Create and fit a new model using the best parameters.\n    model = xgb.XGBModel(**xgb_params)\n    model.fit(X_train, y_train,\n            eval_set=[(X_valid, y_valid)],\n            eval_metric='rmse', verbose=False)\n    \n    # Validation predictions.\n    valid_pred = model.predict(X_valid)\n    rmse = my_rmse(y_valid, valid_pred)\n    print(f'Fold {fold+1}\/{n_splits} RMSE: {rmse:.4f}')\n    kf_rmse.append(rmse)\n    \n    # Use the model trained for 1\/n_splits of the output predictions.\n    if test_preds is None:\n        test_preds = model.predict(X_test)\n    else:\n        # This is kind of naughty for numerical accuracy (may overflow on other problems) but slightly quicker.\n        test_preds += model.predict(X_test)\n\ntest_preds \/= n_splits\nprint(f'Average KFold RMSE: {np.mean(np.array(kf_rmse)):.5f}')","a7c6c1de":"test_preds[test_preds < y_min] = y_min\ntest_preds[test_preds > y_max] = y_max\nsample_submission['loss'] = test_preds\nsample_submission.to_csv('submission.csv', index=False)\nsample_submission","2384c5af":"# TPS August 2021\n\nFor this iteration of TPS I wanted to focus more on validation to match LB and CV scores. In this notebook I choose to do an Optuna study for XGBoost that splits the train data 60-40. The high test split is to prevent the study from overfitting. Early iterations of this notebook scored the model through cross validation in the study. However, I've found that method to be painfully slow, and not much better compared to the KFold validation used later. I chose to use KFold instead of StratifiedKFold since I was getting instances where there was not enough classes to fit each split. \n\nI've you've found this notebook useful, please don't forget to upvote! \ud83d\ude42"}}