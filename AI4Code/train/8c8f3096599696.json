{"cell_type":{"8a44fb00":"code","38ba4ee4":"code","ec0e1945":"code","f50157f1":"code","8837ebcf":"code","b25b9759":"code","70f20446":"code","dab16938":"code","4dda7de3":"code","94384191":"code","ae0a742d":"markdown","70e55b4d":"markdown","6001f410":"markdown","89ec97f7":"markdown","ee8d8d3f":"markdown","3873f971":"markdown","89b3485f":"markdown","9b01c442":"markdown","cfb2973d":"markdown"},"source":{"8a44fb00":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')","38ba4ee4":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\ntrain.head()","ec0e1945":"# concate dataset(train, test)\ndf = pd.concat(objs = [train, test], axis = 0).reset_index(drop = True)\n\n# fill null-values in Age\nage_by_pclass_sex = df.groupby(['Sex', 'Pclass'])['Age'].median()\ndf['Age'] = df.groupby(['Sex', 'Pclass'])['Age'].apply(lambda x: x.fillna(x.median()))\n\n# create Cabin_Initial feature\ndf['Cabin_Initial'] = df['Cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')\ndf['Cabin_Initial'] = df['Cabin_Initial'].replace(['A', 'B', 'C', 'T'], 'ABCT')\ndf['Cabin_Initial'] = df['Cabin_Initial'].replace(['D', 'E'], 'DE')\ndf['Cabin_Initial'] = df['Cabin_Initial'].replace(['F', 'G'], 'FG')\ndf['Cabin_Initial'].value_counts()\n\n# fill null-values in Embarked\ndf['Embarked'] = df['Embarked'].fillna('S')\n\n# fill null-values in Fare\ndf['Fare'] = df['Fare'].fillna(df.groupby(['Pclass', 'Embarked'])['Fare'].median()[3]['S'])\n\n\n# binding function for Age and Fare\ndef binding_band(column, binnum):\n    df[column + '_band'] = pd.qcut(df[column].map(int), binnum)\n\n    for i in range(len(df[column + '_band'].value_counts().index)):\n        print('{}_band {} :'.format(column, i), df[column + '_band'].value_counts().index.sort_values(ascending = True)[i])\n        df[column + '_band'] = df[column + '_band'].replace(df[column + '_band'].value_counts().index.sort_values(ascending = True)[i], int(i))\n        \n    df[column + '_band'] = df[column + '_band'].astype(int)    \n    \n    return df.head()\n\nbinding_band('Age',8)\nbinding_band('Fare', 6)\n\n\n# create Initial feature\ndf['Initial'] = 0\nfor i in range(len(df['Name'])):\n    df['Initial'].iloc[i] = df['Name'][i].split(',')[1].split('.')[0].strip()\n\nMrs_Miss_Master = []\nOthers = []\n\nfor i in range(len(df.groupby('Initial')['Survived'].mean().index)):\n    if df.groupby('Initial')['Survived'].mean()[i] > 0.5:\n        Mrs_Miss_Master.append(df.groupby('Initial')['Survived'].mean().index[i])\n    elif df.groupby('Initial')['Survived'].mean().index[i] != 'Mr':\n        Others.append(df.groupby('Initial')['Survived'].mean().index[i])\n    \ndf['Initial'] = df['Initial'].replace(Mrs_Miss_Master, 'Mrs\/Miss\/Master')\ndf['Initial'] = df['Initial'].replace(Others, 'Others')    \n    \n# create Alone feature\ndf['Alone'] = 0\ndf['Alone'].loc[(df['SibSp'] + df['Parch']) == 0] = 1\n\n# create Companinon's survival rate feature\ndf['Ticket_Number'] = df['Ticket'].replace(df['Ticket'].value_counts().index, df['Ticket'].value_counts())\ndf['Family_Size'] = df['Parch'] + df['SibSp'] + 1\ndf['Companion_Survival_Rate'] = 0\nfor i, j in df.groupby(['Family_Size', 'Ticket_Number'])['Survived'].mean().index:\n    df['Companion_Survival_Rate'].loc[(df['Family_Size'] == i) & (df['Ticket_Number'] == j)] = df.groupby(['Family_Size', 'Ticket_Number'])[\"Survived\"].mean()[i, j]\n    \ncomb_sum = df.loc[df['Family_Size'] == 5]['Survived'].sum() + df.loc[df['Ticket_Number'] == 3]['Survived'].sum()\ncomb_counts = df.loc[df['Family_Size'] == 5]['Survived'].count() + df.loc[df['Ticket_Number'] == 3]['Survived'].count()\nmean = comb_sum \/ comb_counts\n\ndf['Companion_Survival_Rate'] = df['Companion_Survival_Rate'].fillna(mean)    \n\n# select categorical features\ncate_col = []\nfor i in [4, 11, 12, 15]:\n    cate_col.append(df.columns[i])\n\ncate_df = pd.get_dummies(df.loc[:,(cate_col)], drop_first = True)\ndf = pd.concat(objs = [df, cate_df], axis = 1).reset_index(drop = True)\n\ndf = df.drop(['Name', 'Sex', 'Age', 'Ticket', 'Fare', 'Embarked', 'Cabin_Initial', 'SibSp', 'Parch',\n              'Cabin', 'Initial', 'Ticket_Number', 'Family_Size'], axis = 1)\n\n# split data\ndf = df.astype(float)\ntrain = df[:891]\ntest = df[891:]\n\ntrain_X = train.drop(['Survived', 'PassengerId'], axis = 1)\ntrain_y = train.iloc[:, 1]\ntest_X = test.drop(['Survived', 'PassengerId'], axis = 1)","f50157f1":"class Perceptron(object):\n    \n    def __init__(self, eta = 0.01, n_iter = 50, random_state = 1):\n        self.eta = eta\n        self.n_iter = n_iter\n        self.random_state = random_state\n        \n    \n    def fit(self, X, y):\n        rgen = np.random.RandomState(self.random_state)\n        self.w_ = rgen.normal(loc = 0, scale = 0.01,\n                             size = 1 + X.shape[1])\n        \n        self.errors_ = []\n        for _ in range(self.n_iter):\n            errors = 0\n            for i in range(len(X)):\n                xi = X.iloc[i].values\n                target = y[i]\n                update = self.eta * (target - self.predict(xi))\n                self.w_[1:] += update * xi\n                self.w_[0] += update\n                errors += int(update != 0.0)\n            self.errors_.append(errors)\n        return self\n    \n    def predict(self, X):\n        return np.where(self.net_input(X) >= 0.0, 1, 0)\n    \n    def net_input(self, X):\n        return np.dot(X, self.w_[1:]) + self.w_[0]","8837ebcf":"Pc = Perceptron(n_iter = 100)\nPc.fit(train_X, train_y)","b25b9759":"fig = plt.subplots(figsize = (15, 8))\nplt.plot(range(1, len(Pc.errors_) + 1), Pc.errors_)\n\nplt.legend(['Perceptron errors'], loc = 'upper right')\nplt.xlabel('Epoch', fontsize = 10)\nplt.ylabel('Number of errors', fontsize = 10)\n\nplt.show()","70f20446":"class Adjusted_Perceptron(object):\n    \n    def __init__(self, eta = 0.01, n_iter = 50, random_state = 1):\n        self.eta = eta\n        self.n_iter = n_iter\n        self.random_state = random_state\n        \n    \n    def fit(self, X, y):\n        \n        cor_X = X.copy()\n        cor_y = y.copy()\n        cor_X['Target'] = cor_y\n        self.w_ = [cor_X.corr()['Target'].values[i] \n                   for i in range(len(cor_X.corr()['Target'].values)) \n                   if cor_X.corr()['Target'].index[i] != 'Target']\n        self.w_.insert(0, 0)\n        \n        self.errors_ = []\n        for _ in range(self.n_iter):\n            errors = 0\n            for i in range(len(X)):\n                xi = X.iloc[i].values\n                target = y[i]\n                update = self.eta * (target - self.predict(xi))\n                self.w_[1:] += update * xi\n                self.w_[0] += update\n                errors += int(update != 0.0)\n            self.errors_.append(errors)\n        return self\n    \n    def predict(self, X):\n        return np.where(self.net_input(X) >= 0.0, 1, 0)\n    \n    def net_input(self, X):\n        return np.dot(X, self.w_[1:]) + self.w_[0]","dab16938":"APc = Adjusted_Perceptron(n_iter = 100)\nAPc.fit(train_X, train_y)","4dda7de3":"fig = plt.subplots(figsize = (15, 8))\nplt.plot(range(1, len(APc.errors_) + 1), APc.errors_)\nplt.plot(range(1, len(Pc.errors_) + 1), Pc.errors_)\n\nplt.legend(['Adjusted_Perceptron', 'Perceptron'], loc = 'upper right')\nplt.title('Adjusted Perceptron vs Perceptron', fontsize = 20)\nplt.xlabel('Epoch', fontsize = 10)\nplt.ylabel('Number of errors', fontsize = 10)\n\nplt.show()","94384191":"Pc = Perceptron(n_iter = 10)\nPc.fit(train_X, train_y)\npredict = [Pc.predict(test_X)]\nAPc = Adjusted_Perceptron(n_iter = 2)\nAPc.fit(train_X, train_y)\npredict.append(APc.predict(test_X))\n\nfor i in range(len(predict)):\n    submission = pd.DataFrame(columns = ['PassengerId', 'Survived'])\n    submission['PassengerId'] = df['PassengerId'][891:].map(int)\n    submission['Survived'] = predict[i]\n    submission.to_csv('Perceptron_submission_{}.csv'.format(i + 1), header = True, index = False)","ae0a742d":"# Titanic survival rate prediction by Perceptron","70e55b4d":"<b>Perceptron : 0.75358\n\nAdjusted Perceptron : 0.75837\n\nEnsemble : 0.76794\n<\/b>\n\n\nEnsemble is the result of previously submitted combinations of classification algorithms[Titanic Survival Prediction(EDA, Ensemble)](https:\/\/www.kaggle.com\/choihanbin\/titanic-survival-prediction-eda-ensemble). However, the accuracy of Perceptron and Adjusted Perceptron is not that bad. Of course, if you have to classify complex data, you may have a big difference in accuracy, but I think it's a pretty good model for Titanic.\n\n\n","6001f410":"### Feature engineering\nWhen I joined titanic survived prediction competition, I wrote the notebook, [Titanic Survival Prediction(EDA, Ensemble)](https:\/\/www.kaggle.com\/choihanbin\/titanic-survival-prediction-eda-ensemble). So I will pick the features to use based on it.","89ec97f7":"This notebook goal:\n- Understanding perceptron's principle \n- Trying to predict titanic survived.\n- Checking accuracy of models created without being imported through the library.\n\n### You can learn about Perceptron principle in [my blog](https:\/\/konghana01.tistory.com\/13). ","ee8d8d3f":"### Explaination of Perceptron class\nI was able to create algorithms more simply than I thought. I'll briefly describe the parameters and properties used in that class.\n\n- eta: Learning rate (float)\n\n- n_iter : Number of repetitions of training data (int)\n\n- random_state : random number (int)\n\n- w_ : learned weight (1d-array)\n\n- errors_ : Classification errors accumulated per fork (list)\n\n- X: Training data\n\n- y: Target value\n\n\n\nProceed in the manner described above. Let's look at the sequence of the code.\n\n1. Set the initial w_ (weight value). The number of input variable is as large as + 1, which randomly consists of a normal distribution value based on zero.\n\n2. Jump Xi and w_[1:] to np.dot and store the value added to w_[1] as the final input value. The concept of jumbo can be found in the posting of basic components of linear regression. The method of multiplication calculation between matrices is briefly described in the corresponding posting.\n\n3. If the final input value is greater than 0, print out 0 if it is less than or equal to or less than 0. Unlike the above explanation, the target variable is made up of values of 0 and 1.\n\n4. If the final output of the predicted value (yhat) is the same as the actual value, the update will be zero and the first prediction will be finished without modifying the weights. If the predicted and actual values differ, the update stores the eta (learning rate), which is added to w_[1], and the values multiplied by eta and xi are added to w_[1:]. Then save it in error.\n\n5. Repeat 2-4 and add errors to errors_.\n\n6. Repeat 1-5 for the given n_iter.\n\n7. Print the self.\n\n\n\nIt is an algorithm that can be implemented less complex than expected. We learned this in the train set and figured out the accuracy according to the number of iterations.","3873f971":"### Mathematical definitions of artificial neurons\n\nPrior to implementing Perceptron, the mathematical definition of Perceptron must precede.\n\nThe Perceptron we want to implement is simply a binary classification task with two classes. Defines the determination function by linear combination of the input value x and corresponding weight vector w. Linear combinations have organized concepts in different postings. Click to move on to the post.\n\n\n\nAnyway, the final z is determined by the multiplication between the transpose vector of the weight vector W and the vector of X. If the z that is last entered is greater than a certain threshold, assign it as 1, if it is less than -1. We'll update the weight by comparing it to the actual value. You will continue to modify the weights as you repeat this process, while predicting classes.\n\n\n\nTo sum up this process briefly,\n\n1. Initialize weights to zero or randomly small values.\n\n2. Proceed to the next task of each training sample drawer.\n\na. Calculates the output value yhat.\n\nb. Update weights.\n\ncan be expressed as\n\n\n\nNow let's start implementing the perceptron.","89b3485f":"The results were more interesting than I thought. Adjusted Perceptron did not differ much from what was learned more than once or twice. This is the result of setting the initial setting as correlation and proceeding with the prediction. This is the biggest difference from Perceptron. And if there's a lot of learning data, it means that in terms of learning time, the Adjusted Perceptron can be a much better algorithm. When I saw that my own algorithms were better than time costs, I was able to achieve a greater sense of accomplishment than I thought. \n\n \n\nWe also selected the appropriate number of iterations for each algorithm and submitted the predicted value to the kaggle to check the public acuity.","9b01c442":"### load the dataset","cfb2973d":"After 7 to 8 studies, the number of errors dropped to about 200, and then the number of errors converging to 190 to 210 afterwards.\n\n\n\n3. Implementing a coordinated perceptron\n\n Suddenly, I was wondering if there was a way to supplement this method, and I wondered if I could set the initial w_value differently. So we decided to add the initial w_value to the correlation value between each variable and target variable. \n \n First of all, the correlation is directly related to target variability, so I thought that if you adjust this value, it can be more accurately predictable and faster.\n \n Also, the correlation with 'Survived' (target variable) has values from 0 to 1 so I thought it would be possible to reflect the relationship with target variable well without scale problems. Here's the Adjusted Perceptron algorithm that we created."}}