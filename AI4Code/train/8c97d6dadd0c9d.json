{"cell_type":{"fa8d465b":"code","f5e0870b":"code","f9b94759":"code","e321bc5d":"code","9fcc98ff":"code","5c2f5baa":"code","28461e99":"code","e02e24cc":"code","ed3b4e17":"code","8e48836f":"code","8d470161":"code","7f48aed1":"code","cc3e6c01":"code","ea14a9d8":"code","7e0e4be3":"code","b1e4a533":"code","2ca1fd29":"code","6f791794":"code","786211ec":"code","47833ba7":"code","fccdf61b":"code","705e29e3":"code","9f8f647d":"code","18db1bb2":"code","7b12c309":"markdown","f9bbfaed":"markdown","bb3bc7bc":"markdown","e64b124f":"markdown","7ff2de79":"markdown","ada9a8a3":"markdown","80a6588b":"markdown","2e0313cd":"markdown"},"source":{"fa8d465b":"from IPython.display import display, Image\ndisplay(Image(filename='..\/input\/digitslrimages\/karpathy_optimal.png'))","f5e0870b":"display(Image(filename='..\/input\/digitslrimages\/leet.png'))","f9b94759":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\n\nimport tensorflow as tf\nimport keras\nfrom keras.layers import Activation, Dropout, Flatten, Dense, SpatialDropout2D, Conv2D, MaxPooling2D, AveragePooling1D, Reshape\nfrom keras.optimizers import Adam, SGD\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.callbacks import EarlyStopping, LearningRateScheduler\n\nimport matplotlib.pyplot as plt\nprint(os.listdir('..\/input\/'))","e321bc5d":"x_train = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\nx_train.head()","9fcc98ff":"# set up training data and labels\ndim_x = 28\ndim_y = 28\nbatch_size=32\n\n# read in data\/labels\nx_train.shape\ny_train = np.array(x_train['label'])\nx_train.drop('label', axis = 1, inplace = True)\nx_train = np.array(x_train.values)\n\nprint(\"data shapes\", x_train.shape, y_train.shape, \"classes: \",len(np.unique(y_train)))\n\nclasses = len(np.unique(y_train))\nx_train = x_train.reshape((-1, dim_x,dim_y,1))\n# convert labels to one-hot\nprint(np.unique(y_train))\ny = np.zeros((np.shape(y_train)[0],len(np.unique(y_train))))\n\n# convert index labels to one-hot\nfor ii in range(len(y_train)):\n    #print(y_train[ii])\n    y[ii,y_train[ii]] = 1\ny_train = y","5c2f5baa":"# split into training\/validation\nno_validation = int(0.1 * int(x_train.shape[0]))\n\nx_val = x_train[0:no_validation,...]\ny_val = y_train[0:no_validation,...]\n\nx_train = x_train[no_validation:,...]\ny_train = y_train[no_validation:,...]\n\nprint(x_train.shape, y_train.shape, x_val.shape, y_val.shape)\n\n# define image generators with mild augmentation\ntrain_datagen = ImageDataGenerator(rescale = 1.\/255,\\\n                                   rotation_range=15,\\\n                                   width_shift_range=0.125,\\\n                                   height_shift_range=0.125,\\\n                                   shear_range=0.25,\\\n                                   zoom_range=0.075)\n\ntrain_generator = train_datagen.flow(x=x_train,\\\n                                     y=y_train,\\\n                                     batch_size=batch_size,\\\n                                     shuffle=True)\n\ntest_datagen = ImageDataGenerator(rescale=1.\/255)\n\nval_generator = test_datagen.flow(x=x_val,\\\n                                    y=y_val,\\\n                                    batch_size=batch_size,\\\n                                    shuffle=True)","28461e99":"# define model (topless conv-net)\nmodel = Sequential()\nmodel.add(Conv2D(filters=256, kernel_size=(3,3), strides=1,input_shape=(dim_x,dim_y,1), activation=tf.nn.relu))\nmodel.add(MaxPooling2D(pool_size=2, strides=2))\nmodel.add(Conv2D(filters=512, kernel_size=(3,3), strides=1,input_shape=(dim_x,dim_y,1), activation=tf.nn.relu))\nmodel.add(MaxPooling2D(pool_size=2, strides=2))\nmodel.add(Conv2D(filters=1024, kernel_size=(3,3), strides=1,input_shape=(dim_x,dim_y,1), activation=tf.nn.relu))\nmodel.add(SpatialDropout2D(rate=0.5))\nmodel.add(Conv2D(filters=512, kernel_size=(2,2), strides=1,input_shape=(dim_x,dim_y,1), activation=tf.nn.relu))\nmodel.add(Conv2D(filters=250, kernel_size=(2,2), strides=1,input_shape=(dim_x,dim_y,1), activation=tf.nn.relu))\nmodel.add(Reshape((250,1)))\nmodel.add(AveragePooling1D(pool_size=25,strides=25))\nmodel.add(Reshape(([10])))\nmodel.add(Activation(tf.nn.softmax))\n\nmodel.summary()\n","e02e24cc":"steps_per_epoch = int(len(y_train)\/batch_size)\nmax_epochs = 10\nlo_lr = 1e-8\n\nlo_lr_model = keras.models.clone_model(model) #, input_tensors)model\nlo_lr_model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=lo_lr), metrics=['accuracy'])\n\nlo_lr_history = lo_lr_model.fit_generator(generator=train_generator,\\\n                                steps_per_epoch=steps_per_epoch\/10,\\\n                                validation_data=val_generator,\\\n                                validation_steps=50,\\\n                                epochs=max_epochs*10,\\\n                                verbose=0)\n\nhi_lr = 9e-1\n\nhi_lr_model = keras.models.clone_model(model) #, input_tensors)model\nhi_lr_model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=hi_lr), metrics=['accuracy'])\n\nhi_lr_history = hi_lr_model.fit_generator(generator=train_generator,\\\n                                steps_per_epoch=steps_per_epoch\/10,\\\n                                validation_data=val_generator,\\\n                                validation_steps=50,\\\n                                epochs=max_epochs*10,\\\n                                verbose=0)\n\n","ed3b4e17":"plt.figure(figsize=(15,12))\nplt.subplot(211)\nplt.plot(lo_lr_history.history['acc'],':',lw=3)\nplt.plot(lo_lr_history.history['val_acc'],':',lw=3)\n\nplt.plot(hi_lr_history.history['acc'],'-.',lw=3)\nplt.plot(hi_lr_history.history['val_acc'],'-.',lw=3)\nplt.title(\"Accuracy and Loss, bad learning rates\",fontsize=28)\nplt.ylabel('accuracy',fontsize=24)\nplt.legend(['Lo-Train','Lo-Val', 'Hi-Train', 'Hi-Val'],fontsize=18)\n\nplt.subplot(212)\nplt.plot(lo_lr_history.history['loss'],':',lw=3)\nplt.plot(lo_lr_history.history['val_loss'],':',lw=3)\n\nplt.plot(hi_lr_history.history['loss'],'-.',lw=3)\nplt.plot(hi_lr_history.history['val_loss'],'-.',lw=3)\nplt.xlabel('epoch',fontsize=24)\nplt.ylabel('loss',fontsize=24)\nplt.legend(['Lo-Train','Lo-Val', 'Hi-Train', 'Hi-Val'],fontsize=18)\nplt.show()","8e48836f":"steps_per_epoch = int(len(y_train)\/batch_size)\nmax_epochs = 10\nnaive_lr = 3e-4\n\nnaive_lr_model = keras.models.clone_model(model) #, input_tensors)model\nnaive_lr_model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=naive_lr), metrics=['accuracy'])\n\nnaive_lr_history = naive_lr_model.fit_generator(generator=train_generator,\\\n                                steps_per_epoch=steps_per_epoch\/10,\\\n                                validation_data=val_generator,\\\n                                validation_steps=50,\\\n                                epochs=max_epochs*10,\\\n                                verbose=0)","8d470161":"plt.figure(figsize=(15,12))\nplt.subplot(211)\nplt.plot(naive_lr_history.history['acc'],'--',lw=3)\nplt.plot(naive_lr_history.history['val_acc'],'--',lw=3)\n\nplt.title(\"Accuracy and Loss, 3e-4 Adam\",fontsize=28)\nplt.ylabel('accuracy',fontsize=24)\nplt.legend(['Lo-Train','Lo-Val', 'Hi-Train', 'Hi-Val'],fontsize=18)\n\nplt.subplot(212)\nplt.plot(naive_lr_history.history['loss'],'--',lw=3)\nplt.plot(naive_lr_history.history['val_loss'],'--',lw=3)\n\nplt.xlabel('epoch',fontsize=24)\nplt.ylabel('loss',fontsize=24)\nplt.legend(['Lo-Train','Lo-Val', 'Hi-Train', 'Hi-Val'],fontsize=18)\nplt.show()","7f48aed1":"class lr_finder():\n    \n    def __init__(self,model,begin_lr=1e-8, end_lr=1e-1, num_epochs=10, period=5):\n        # lr_finder generates learning schedules for finding upper and lower bounds on the best learning rate, as well as \n        # a cyclical learning rate schedule based on those bounds\n        self.period = period\n        # make a copy of the model to train through a sweep of learning rates\n        self.model = keras.models.clone_model(model)\n        \n        # define bounds to sweep through\n        self.begin_lr = np.log(begin_lr)\/np.log(10)\n        self.end_lr = np.log(end_lr)\/np.log(10)\n        self.num_epochs = num_epochs\n        self.lower_bound = begin_lr\n        self.upper_bound = 1e-2 #end_lr\n        # define learning rates to use in schedules\n        self.lr = np.logspace(self.begin_lr,self.end_lr,self.num_epochs)\n        self.clr = np.logspace(np.log(self.lower_bound)\/np.log(10), np.log(self.upper_bound)\/np.log(10), self.period)\n        \n        \n    def reset_model(self, model):\n        # reset the model to find new lr bounds \n        self.begin_lr = -10 \n        self.end_lr = 0 \n        self.lr = np.logspace(self.begin_lr,self.end_lr,self.num_epochs)\n        self.model = keras.models.clone_model(model)\n        \n    def lr_schedule(self,epoch):\n        # return lr according to a sweeping schedule\n        if epoch < self.num_epochs:\n            return self.lr[epoch]\n        else:\n            return self.lr[0]\n        \n    def clr_schedule(self,epoch,period=5):\n        # return lr according to cyclical learning rate schedule\n        my_epoch = int(epoch % self.period)\n        return self.clr[my_epoch]\n    \n    def lr_vector(self,epochs):\n        # return the vector of learning rates used in a schedule\n        lrv = []\n        for ck in range(epochs):\n            lrv.append(self.lr_schedule(ck))\n        return lrv\n    \n    def lr_plot(self,history_loss,please_plot=True):\n        # plot the lr sweep results and set upper and lower bounds on learning rate\n        x_axis = self.lr_vector(self.num_epochs)\n        y_axis = history_loss\n                   \n        d_loss = []\n        for cc in range(1,len(y_axis)):\n            if cc == 1:\n                d_loss.append(y_axis[cc] - y_axis[cc-1])\n            else:\n                d_loss.append(0.8*(y_axis[cc] - y_axis[cc-1])+0.2*(y_axis[cc-1] - y_axis[cc-2]))\n        d_loss = np.array(d_loss)\n        \n        self.lower_bound = x_axis[d_loss.argmin()]\n        self.upper_bound = x_axis[np.array(y_axis).argmin()]\n        self.clr = np.logspace(np.log(self.lower_bound)\/np.log(10), np.log(self.upper_bound)\/np.log(10), self.period)\n        \n        print(\"recommended learning rate: more than %.2e, less than %.2e \"%(self.lower_bound, self.upper_bound))\n        if(please_plot):\n            plt.figure(figsize=(10,5))\n            plt.loglog(x_axis,y_axis)\n            plt.xlabel('learning rate')\n            plt.ylabel('loss')\n            plt.title('Loss \/ learning rate progression')\n            plt.show()\n            \n    def get_lr(self,epoch):\n        # return the geometric mean of the upper and lower bound learning rates\n        return (self.lower_bound *self.upper_bound)**(1\/2)\n    \n    ","cc3e6c01":"# initialize learning rate finder callback\n\nlrf = lr_finder(model,begin_lr=1e-8, end_lr=1e0, num_epochs=20)\nlr_rate = LearningRateScheduler(lrf.lr_schedule)","ea14a9d8":"steps_per_epoch = int(len(y_train)\/batch_size)\nmax_epochs = 20\n\nlrf.model.compile(loss='categorical_crossentropy',optimizer=Adam(), metrics=['accuracy'])\n\n\nlr_history = lrf.model.fit_generator(generator=train_generator,\\\n                                steps_per_epoch=steps_per_epoch\/20,\\\n                                validation_data=val_generator,\\\n                                validation_steps=50,\\\n                                epochs=max_epochs,\\\n                                callbacks=[lr_rate],\\\n                                verbose=0)","7e0e4be3":"lrf.lr_plot(lr_history.history['loss'])","b1e4a533":"steps_per_epoch = int(len(y_train)\/batch_size)\nmax_epochs = 10\n\nbest_lr_model = keras.models.clone_model(model) #, input_tensors)model\n# take the geometric mean of learning rates\nlearning_rate = (lrf.lower_bound*lrf.upper_bound)**(1\/2)\nbest_lr_model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=learning_rate), metrics=['accuracy'])\n\nprint(\"Learning with rate %.2e:\"%learning_rate)\nbest_lr_history = best_lr_model.fit_generator(generator=train_generator,\\\n                                steps_per_epoch=steps_per_epoch\/10,\\\n                                validation_data=val_generator,\\\n                                validation_steps=50,\\\n                                epochs=max_epochs*10,\\\n                                verbose=0)","2ca1fd29":"plt.figure(figsize=(15,12))\nplt.subplot(211)\nplt.plot(naive_lr_history.history['acc'],'--',lw=3)\nplt.plot(naive_lr_history.history['val_acc'],'--',lw=3)\n\nplt.plot(best_lr_history.history['acc'],marker='o',linestyle='--',lw=3)\nplt.plot(best_lr_history.history['val_acc'],marker='o',linestyle='--',lw=3)\n\n\nplt.title(\"Optimal learning rate vs thumbwise 3e-4 \",fontsize=28)\nplt.ylabel('accuracy',fontsize=24)\nplt.legend(['3e-4 Train','3e-4 Val', 'Optimal Train', 'Optimal Val'],fontsize=18)\n\nplt.subplot(212)\nplt.plot(naive_lr_history.history['loss'],'--',lw=3)\nplt.plot(naive_lr_history.history['val_loss'],'--',lw=3)\n\n\nplt.plot(best_lr_history.history['loss'],marker='o',linestyle='--',lw=3)\nplt.plot(best_lr_history.history['val_loss'],marker='o',linestyle='--',lw=3)\n\nplt.xlabel('epoch',fontsize=24)\nplt.ylabel('loss',fontsize=24)\nplt.legend(['3e-4 Train','3e-4 Val', 'Optimal Train', 'Optimal Val'],fontsize=18)\nplt.show()","6f791794":"clr_rate = LearningRateScheduler(lrf.clr_schedule)\n\nmax_epochs = 10\n\nbest_clr_model = keras.models.clone_model(model) #, input_tensors)model\nbest_clr_model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n\nbest_clr_history = best_clr_model.fit_generator(generator=train_generator,\\\n                                    steps_per_epoch=steps_per_epoch\/10,\\\n                                    validation_data=val_generator,\\\n                                    validation_steps=50,\\\n                                    callbacks = [clr_rate],\\\n                                    epochs=max_epochs*10,\\\n                                    verbose=0)\n#lrf.reset_model(best_clr_model)","786211ec":"plt.figure(figsize=(15,12))\nplt.subplot(211)\n\nplt.plot(naive_lr_history.history['acc'],'--',lw=3)\nplt.plot(naive_lr_history.history['val_acc'],'--',lw=3)\n\nplt.plot(best_lr_history.history['acc'],marker='o',linestyle='--',lw=3)\nplt.plot(best_lr_history.history['val_acc'],marker='o',linestyle='--',lw=3)\n\nplt.plot(best_clr_history.history['acc'],marker='+',linestyle='--',lw=3)\nplt.plot(best_clr_history.history['val_acc'],marker='+',linestyle='--',lw=3)\n\nplt.title(\"Optimal learning rate vs thumbwise 3e-4 \",fontsize=28)\nplt.ylabel('accuracy',fontsize=24)\nplt.legend(['3e-4 Train','3e-4 Val', 'Optimal Train', 'Optimal Val',\\\n            'Cyclical Train', 'Cyclical Val'],fontsize=18)\nplt.axis([5,100,0.85,1.01])\n\nplt.subplot(212)\nplt.plot(naive_lr_history.history['loss'],'--',lw=3)\nplt.plot(naive_lr_history.history['val_loss'],'--',lw=3)\n\nplt.plot(best_lr_history.history['loss'],marker='o',linestyle='--',lw=3)\nplt.plot(best_lr_history.history['val_loss'],marker='o',linestyle='--',lw=3)\n\nplt.plot(best_clr_history.history['loss'],marker='+',linestyle='--',lw=3)\nplt.plot(best_clr_history.history['val_loss'],marker='+',linestyle='--',lw=3)\n\nplt.xlabel('epoch',fontsize=24)\nplt.ylabel('loss',fontsize=24)\nplt.legend(['3e-4 Train','3e-4 Val', 'Optimal Train', 'Optimal Val',\\\n            'Cyclical Train', 'Cyclical Val'],fontsize=18)\nplt.axis([5,100,0,0.4])\nplt.show()","47833ba7":"plt.figure(figsize=(15,12))\nplt.subplot(211)\n\nplt.plot(naive_lr_history.history['acc'],'--',lw=3)\nplt.plot(naive_lr_history.history['val_acc'],'--',lw=3)\n\nplt.plot(best_lr_history.history['acc'],marker='o',linestyle='--',lw=3)\nplt.plot(best_lr_history.history['val_acc'],marker='o',linestyle='--',lw=3)\n\n\nplt.plot(best_clr_history.history['acc'],marker='+',linestyle='--',lw=3)\nplt.plot(best_clr_history.history['val_acc'],marker='+',linestyle='--',lw=3)\n\nplt.plot(lo_lr_history.history['acc'],':',lw=3)\nplt.plot(lo_lr_history.history['val_acc'],':',lw=3)\n\nplt.plot(hi_lr_history.history['acc'],'-.',lw=3)\nplt.plot(hi_lr_history.history['val_acc'],'-.',lw=3)\n\nplt.title(\"Heuristic vs. optimal vs. cyclical learning rate\",fontsize=28)\nplt.ylabel('accuracy',fontsize=24)\nplt.legend(['3e-4 Train','3e-4 Val', 'Optimal Train', 'Optimal Val',\\\n            'Cyclical Train', 'Cyclical Val', 'Lo Train', 'Lo Val','Hi Train', 'Hi Val'],fontsize=18)\n\nplt.subplot(212)\nplt.plot(naive_lr_history.history['loss'],'--',lw=3)\nplt.plot(naive_lr_history.history['val_loss'],'--',lw=3)\n\nplt.plot(best_lr_history.history['loss'],marker='o',linestyle='--',lw=3)\nplt.plot(best_lr_history.history['val_loss'],marker='o',linestyle='--',lw=3)\n\n\nplt.plot(best_clr_history.history['loss'],marker='+',linestyle='--',lw=3)\nplt.plot(best_clr_history.history['val_loss'],marker='+',linestyle='--',lw=3)\n\n\n\nplt.plot(lo_lr_history.history['loss'],':',lw=3)\nplt.plot(lo_lr_history.history['val_loss'],':',lw=3)\n\n# loss blows up, doesn't fit nicely with other runs\n#plt.plot(hi_lr_history.history['loss'],'-.',lw=3)\n#plt.plot(hi_lr_history.history['val_loss'],'-.',lw=3)\n\nplt.xlabel('epoch',fontsize=24)\nplt.ylabel('loss',fontsize=24)\nplt.legend(['3e-4 Train','3e-4 Val', 'Optimal Train', 'Optimal Val',\\\n            'Cyclical Train', 'Cyclical Val', 'Lo Train', 'Lo Val'],fontsize=18)\nplt.show()","fccdf61b":"x_test = pd.read_csv('..\/input\/digit-recognizer\/test.csv')\nx_test.head()\n\nx_test = np.array(x_test.values)\nx_test = x_test \/ 255.\n\nprint(\"data shape\", x_test.shape)\n\nx_test = x_test.reshape((-1, dim_x,dim_y,1))\n","705e29e3":"# predict!\ny_pred = best_lr_model.predict(x_test)","9f8f647d":"# visualize success (?!?) :\/\n\ndef imshow_w_labels(img,  pred,count):\n    plt.subplot(1,4,count+1)\n    plt.imshow(img, cmap=\"gray\")\n    plt.title(\"Prediction: %i, \"%(pred), fontsize=24)\n    \n    \ncount = 0\nmask = [1,3,3,7]\nplt.figure(figsize=(24,6))\nfor kk in range(50,600):\n    \n    if y_pred[kk,:].argmax() == mask[count]:\n        imshow_w_labels(x_test[kk,:,:,0],y_pred[kk,...].argmax(), count)\n        count += 1\n    if count >= 4: break\nplt.show()","18db1bb2":"# convert one-hot predictions to indices\nresults = np.argmax(y_pred,axis = 1)\n\nresults = pd.Series(results,name=\"Label\")\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\nsubmission.to_csv(\"cnn_mnist_datagen.csv\",index=False)","7b12c309":"# Learning rate finder\n\nWe'll use the following ```lr_finder``` class to try to find a good initial learning rate. This class generates a learning rate schedule sweeping logarithmically from low to high. The lower and upper bound is set as the steepest descent and saddle point where the model begins to \"blow up,\" that is the point at which increasing the learning rate further leeds to higher losses. You can use the plot to manually choose a good learning, or, as below, take the geometric mean of the upper and lower bound to get something reasonable. Note that in it's current state there are no sanity checks on bounds recommended by ```lr_finder```, so if the recommendation looks weird it probably is.","f9bbfaed":"# Set up the training data\n\nFirst we just need to parse in our training data to get something we can feed into our model. ","bb3bc7bc":"# Learning by rule-of-thumb\n\nDespite the trap of setting silly learning rates, it's possible to get good results using an empirical heuristic, _i.e._ something that seems to work pretty well most of the time. This works especially well with an adaptive gradient descent method like Adam. Andrej Karpathy is well known as an evangelist for using an [Adam optimizer with a learning rate of 3e-4](https:\/\/karpathy.github.io\/2019\/04\/25\/recipe\/), so much so that some people call this the ['Karpathy constant'](https:\/\/twitter.com\/karpathy\/status\/801621764144971776). In any case it works pretty well for simple problems.","e64b124f":"# Building the model \n\nPut together a model or load a pre-trained model to train. This one is a relatively small conv-net that eschews dense layers on the top for average pooling the output neurons with ```AveragePooling1D```.","7ff2de79":"# Predict classes for test data\n\nNow that we've trained our model a few different ways, we can choose the one that seems to be the best and make predictions on the test set.  Hopefully all our effort to find an optimal learning rate and design a good learning rate schedule have paid off, and we'll use the ```best_clr_model``` variant to make test predictions. However, in this case with training limited to 10 epochs, the optimal learning rate model seems to fare best on the validation data, so we'll use that. Extending the training or training on more complicated datasets may have different results. The model using 3e-4 or the optimal learning rate without a schedule also probably did pretty well and should make reasonable predictions, and are worth trying.","ada9a8a3":"# Finding the best learning rate\n\nDeep neural network model are capable of approximating arbitrary functions [0](https:\/\/en.wikipedia.org\/wiki\/Universal_approximation_theorem), and their ability to transform input data in a wide variety of ways is an important part of what makes them so powerful. Being able to fit some arbitrary data relationship is not the same thing as being able to learn it, however, and one of the most visible hyperparameters available for tuning is the learning rate. Optimal learning rates and learning rate schedules (even [silly ones](https:\/\/blog.evjang.com\/2018\/04\/aesthetic-lr.html)) are worth thinking about. \n\nSetting your learning rate too low is generally considered to be harmless so long as you are comfortable waiting forever, but setting your learning rate too high can push a model into corners of parameter space far removed from an optimal solution, eventually dropping into ```nan``` territory. \n\nSetting a learning rate to Karpathy's constant is one way to choose, but another more effective yet still simple way to find a good starting learning rate is described in a [2015 article by Leslie Smith](https:\/\/arxiv.org\/abs\/1506.01186). Starting with a learning rate much smaller than what you expect to be effective and incremented it gradually over several learning batches until learning degrades, many learning situations will generate a trough shape that can be used to set the initial learning rate. By choosing a learning rate where the decrease in loss was greatest (not, as many may assume, as large as possible before learning blows up), your model may move to a local optima much more quickly than with a naive learning rate choice. \n\n","80a6588b":"# Training with a cyclical learning rate\n\nSweeping through learning rates is one small part of the 2015 paper [Cyclical Learning Rates for Training Neural Networks\n](https:\/\/arxiv.org\/abs\/1506.01186) by Leslie Smith. In fact, the paper suggests we can improve our results by using a cyclical learning rate schedule, sweeping between a high and low bound. We can generate such a schedule with the function ```clr_rate``` in our ```lr_finder``` class.","2e0313cd":"# People of straw: learning rates that are too low and too high\n\nYou've probably heard the conventional wisdom: learning rates that are too low are relatively safe but may take forever to learn. Learning rates that are too high are unstable and will push your model to a part of parameter space that is far, far removed from the optimal parameters, eventually setting everything to ```nans```. Below we'll set learning rates that fit these criteria before making some more sensible heuristic assumptions later on. Note that in all these example, I decimate ```steps_per_epoch``` so we can get more informative plots of the learning process. "}}