{"cell_type":{"ea2b23d2":"code","2b9c00c8":"code","de9ec368":"code","a8ef4c7f":"code","005abe48":"code","bdf27809":"code","d35b9fcb":"code","6db68e6a":"code","1805eb0b":"code","ef226989":"code","6053ef05":"code","0cbc4ade":"code","bc3db75f":"code","86a11055":"code","86e8f22f":"code","a83456d5":"code","4512f82c":"code","dda7ff2b":"markdown","da9dc82d":"markdown","388b3f01":"markdown","242a9871":"markdown","afc09fe4":"markdown","2dbfa4cb":"markdown","fe9f5e51":"markdown","026ac9a8":"markdown","588acd3c":"markdown"},"source":{"ea2b23d2":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nfrom matplotlib.ticker import MaxNLocator\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelBinarizer","2b9c00c8":"#Opening the data\ndata = pd.read_csv('..\/input\/credit-card-customers\/BankChurners.csv')","de9ec368":"#Lets see the shapes of the data so we know what we are dealing with\ndata.shape","a8ef4c7f":"#lets observe some of his elements\ndata.head(10)","005abe48":"#Lets delete the last two columns as they are irrelevant\ndata.drop(columns=[\"Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1\",\n                  \"Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2\"],\n         inplace=True)\n","bdf27809":"# Dividing the label and features columns in X, y and then eliminating ids as they are irrelevants for the analysis and modeling\nX = data.copy()\nX.drop(columns=['CLIENTNUM', 'Attrition_Flag'], inplace=True)\ny = data['Attrition_Flag']","d35b9fcb":"# Using a label binarizer to convert y label into 1's and 0's\nlabelBinarizer = LabelBinarizer()\ny = labelBinarizer.fit_transform(y)\ny = np.reshape(y, -1)\ny = pd.Series(y)","6db68e6a":"#For this purpose, i'll concatenate y and X\nanalysisData = X.copy()\nanalysisData['Attrition_Flag'] = y\ncorrelation = analysisData.corr()\n\nf, ax = plt.subplots(figsize=(14,12))\nplt.title('Correlation of numerical attributes', size=16)\nsns.heatmap(correlation)\nplt.show()","1805eb0b":"# To be clearer, let's take a look at a simple plot Total_Trans_Amt in function of Total_Trans_Ct\n\namount = analysisData['Total_Trans_Amt']\ncount = analysisData['Total_Trans_Ct']\n\nfig, ax = plt.subplots()\nax.plot(count, amount)\n\nax.set(xlabel='Number of transactions', ylabel='Total amount of transactions',\n       title='Transactions total amount in function of the number of transactions')\nax.grid()\n\nplt.show()","ef226989":"#Lets see the variability and some other statistics of categorical columns\ncat_columns = X.select_dtypes(include=['object']).columns\nfor col in cat_columns:\n    print(X[col].value_counts(ascending=True, normalize=True))\n    print(X[col].describe())\n    print(\"---------------------------------------------------\")","6053ef05":"X.isnull().sum()","0cbc4ade":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=10)","bc3db75f":"from xgboost import XGBClassifier\nxgb = XGBClassifier(colsample_bytree= 0.7, learning_rate= 0.07, max_depth=7, min_child_weight=4,\n                  n_estimators = 500, nthread=4, objective= 'reg:linear', subsample= 0.7, tree_method='gpu_hist')","86a11055":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import QuantileTransformer","86e8f22f":"catTransformer = ColumnTransformer([('encoder', OneHotEncoder(), cat_columns)], remainder='passthrough')","a83456d5":"from sklearn.pipeline import Pipeline\n\nmodel_pipeline = Pipeline(steps=[\n                                ('One Hot Encoding', catTransformer),\n                                ('Quantile_Proccesing', QuantileTransformer(n_quantiles=10, random_state=0)),\n                                ('XGBoost', xgb)\n                                ])\nmodel_pipeline.fit(X_train, y_train)","4512f82c":"# Accuracy Metrics\nfrom sklearn.metrics import accuracy_score\n\ny_pred = model_pipeline.predict(X_test)\npredictions = [round(value) for value in y_pred]\naccuracy = accuracy_score(y_test, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","dda7ff2b":"# Preprocessing + Pipeline\n<li>First, lets split the data into train and test dataframes<\/li>\n<p>Pipeline Steps:<\/p>\n<ol>\n    <li>One Hot Encoding<\/li>\n    <li>Quantile Proccesing<\/li>\n    <li>Fit the model<\/li>\n<\/ol>","da9dc82d":"# End\n<p>Thanks for going all the way down through my notebook! I hope you were able to get something usefull from this. Feel free to ask your questions and use my code<\/p>","388b3f01":"# Introduction\n<p>Welcome! In this notebook i'm going to analyze credit card's customers data and implement a Machine Learning Classfier to predict the attrition probabilty of customers<\/p>\n<h3>My main objectives on this project are:<\/h3>   \n<ul>\n    <li>Applying exploratory data analysis and trying to get some insights about our dataset<\/li>\n    <li>Getting data in better shape by transforming and feature engineering to help us in building better models<\/li>\n    <li>Building and tuning different ML algorithms to get some results on predicting Attrition<\/li>\n<\/ul>","242a9871":"# Missing Data\n<ul>\n    <li>Lets see if there any missing values and visualize them<\/li>\n<\/ul>","afc09fe4":"# EDA\n<p>Exploratory Data Analysis<\/p>\n\n<p>Lets create a heatmap graphic here. With this graphic we can see the correlation between different features<\/p>","2dbfa4cb":"# Meeting the data\n<p>Lets open the data and see what we have<\/p>","fe9f5e51":"<h2>Importing Libraries<\/h2>\n<p>Lets start by importing some packages we are going to need<\/p>","026ac9a8":"<h4>Observations<\/h4>\n<li>Let's focus on the lighter parts of the graph<\/li>\n<ol>\n    <li>Customer age and Months on book have a high correlation because these customers just got the possibility of getting a credit card<\/li>\n    <li>Avg_Open_To_Buy and Credit_Limit have a high correlation because they are telling the \"same thing\"<\/li>\n    <li>Total Transaction Amount is high correlated with Total Transacion Count because usually the amount tends to get higher as the count of transactions grow<\/li>\n<\/ol>","588acd3c":"<li>Luckily we don't have any missing values, so we can proceed with modeling<\/li>"}}