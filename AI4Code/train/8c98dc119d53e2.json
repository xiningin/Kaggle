{"cell_type":{"38974309":"code","116d6714":"code","be2b93ab":"code","7b10bce6":"code","7365cf22":"code","99569571":"code","665eb32a":"code","ce1b47d1":"code","9274f12b":"code","b55b4021":"code","5c1bd88e":"code","a59deff4":"code","caba0566":"code","791d83e6":"code","39bfc856":"code","5adef257":"code","050acea6":"code","ae860ab9":"code","89fb77dd":"code","652fe854":"code","9742b54e":"code","320b2770":"code","84172642":"code","bb436d4d":"code","1bc1a0e0":"code","f408ebca":"code","b402ae2e":"code","02abab2f":"code","79804ce7":"code","d58f9d54":"code","92dd4bee":"code","307f43e2":"code","4d7678a4":"code","1439e307":"code","5659c9ad":"code","3c88aa32":"code","718a2ed4":"code","9ea41282":"code","a18699d0":"code","7b83b552":"code","a6b66e90":"code","2aa7c67e":"code","3fea3503":"code","a6ecf7ea":"code","7eb92a34":"code","d846bc8c":"code","6d0fc25d":"code","2b1efd83":"code","46208f72":"code","66e970f4":"code","cf4d1ff9":"code","edbcdd88":"code","575bb6a8":"code","2716e753":"code","7ea5704f":"code","f96618a5":"code","61a146f0":"code","9a0c058f":"code","acdb5021":"code","58ba4081":"code","0d581661":"code","79d32063":"code","c5bf338a":"code","7a882005":"code","daa591b9":"code","f7ceacf2":"markdown","ffdcfa71":"markdown","5e3c2925":"markdown","d551fd8d":"markdown","dae01f9e":"markdown","bb9fb572":"markdown","cfeb1fbd":"markdown","93cdb20d":"markdown","2ed4ab0a":"markdown","ac1f24e9":"markdown","4829899f":"markdown","e6abf1e3":"markdown"},"source":{"38974309":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","116d6714":"trained = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')               # importing datasets\ntrained.head()                                               ","be2b93ab":"test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntest.head()","7b10bce6":"test.shape","7365cf22":"trained.drop(['PassengerId','Name','Ticket','Cabin'],axis=1,inplace=True)","99569571":"test.drop(['PassengerId','Name','Ticket','Cabin'],axis=1,inplace=True)","665eb32a":"trained.head(10)","ce1b47d1":"trained.info()                 #information about the dataset","9274f12b":"trained.describe()           #describing the dataset","b55b4021":"\ntrained.isnull().sum()       #checking the null values","5c1bd88e":"# filling the null values with its mean\ntrained['Age'].fillna(trained['Age'].mean(),inplace=True)\ntrained['Age']","a59deff4":"trained.columns","caba0566":"test['Age'].fillna(test['Age'].mean(),inplace=True)\ntest['Age']","791d83e6":"trained.isnull().sum()","39bfc856":"trained['Fare'] = trained['Fare'].replace(0, trained['Fare'].mean())","5adef257":"test['Fare'].replace('nan',np.nan,inplace=True)\ntest['Fare'].fillna(test['Fare'].mean(),inplace=True)","050acea6":"\ntest['Fare'] = test['Fare'].replace(0, test['Fare'].mean())","ae860ab9":"trained['Embarked'].replace('nan',np.nan,inplace=True)\ntrained['Embarked'].fillna(trained['Embarked'].mode()[0],inplace=True)","89fb77dd":"trained.isnull().sum()","652fe854":"test['Embarked'].replace('nan',np.nan,inplace=True)\ntest['Embarked'].fillna(test['Embarked'].mode()[0],inplace=True)","9742b54e":"trained.head()","320b2770":"trained['Sex']=trained['Sex'].map({'male':0,'female':1})\n\ntrained['Sex']","84172642":"test['Sex']=test['Sex'].map({'male':0,'female':1})\n\ntest['Sex']","bb436d4d":"trained['Embarked']=trained['Embarked'].map({'S':0,'C':1,'Q':2})","1bc1a0e0":"test['Embarked']=test['Embarked'].map({'S':0,'C':1,'Q':2})","f408ebca":"trained['Fare'].replace(0,trained['Fare'].mean(),inplace=True)\n","b402ae2e":"trained.head()","02abab2f":"trained['Embarked'].unique()\n\n","79804ce7":"\ntrained['Age']=np.log(trained['Age'])\n\ntrained['Fare']=np.log(trained['Fare'])\n\ntest.var()","d58f9d54":"test['Fare']=np.log(test['Fare'])\ntest['Age']=np.log(test['Age'])\n\ntest.var()","92dd4bee":"test.isnull().sum()","307f43e2":"x=trained.drop(['Survived'],axis=1)\n\ny=trained['Survived']","4d7678a4":"#train_test_splitting of the dataset\nfrom sklearn.model_selection import train_test_split \nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25,random_state=0)","1439e307":"from sklearn.linear_model import LogisticRegression\nlog_reg = LogisticRegression()\nlog_reg.fit(x_train,y_train)                         \n\n","5659c9ad":"#hyperparameter tuning of logistic regression\nfrom sklearn.model_selection import GridSearchCV\nparam = {\n         'penalty':['l1','l2'],\n         'C':[0.001, 0.01, 0.1, 1, 10, 20,100, 1000]\n}\nlr= LogisticRegression(penalty='l1')\ncv=GridSearchCV(log_reg,param,cv=5,n_jobs=-1)\ncv.fit(x_train,y_train)\ncv.predict(x_test)","3c88aa32":" #best_parameters\nprint(\"Best CV params\", cv.best_params_)          ","718a2ed4":" #best_score\nprint(\"Best CV score\", cv.best_score_)","9ea41282":" #best_estimators\nbest_lr = cv.best_estimator_\nbest_lr","a18699d0":"prob = log_reg.predict_proba(x_train)\nprint(\"Maximum predicted probability\",np.max(prob))","7b83b552":"from sklearn.svm import SVC\nsvm= SVC()\nsvm.fit(x_train,y_train)\ny_preds=svm.predict(x_test)\nsvm.score(x_test,y_test)","a6b66e90":"from sklearn.linear_model import SGDClassifier\nlinear_classifier = SGDClassifier(random_state=0)\n\n\nparameters = {'alpha':[0.00001, 0.0001, 0.001, 0.01, 0.1, 1], \n             'loss':['hinge','log'], 'penalty':['l1','l2']}\nsearcher = GridSearchCV(linear_classifier, parameters, cv=10)\nsearcher.fit(x_train, y_train)\n\n\nprint(\"Test accuracy of best grid search hypers:\", searcher.score(x_test, y_test))","2aa7c67e":"from sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier()\n\nknn.fit(x_train, y_train)\n\npred = knn.predict(x_test)\n","3fea3503":"#accuracy,confusion matrix and classification report\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nprint(\"accuracy score for trained data\",accuracy_score(y_train,knn.predict(x_train)))\nprint(\"accuracy score is\",accuracy_score(y_test,pred))\n\nprint(\"Confusion matrix\",confusion_matrix(y_test,pred))\n\nprint(\"Report\",classification_report(y_test,pred))","a6ecf7ea":"from sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier(max_depth=6, random_state=123)\n\ndt.fit(x_train,y_train)\n\ny_preds = dt.predict(x_test)","7eb92a34":"print(\"accuracy score for trained data\",accuracy_score(y_train,dt.predict(x_train)))\nprint(\"accuracy score is\",accuracy_score(y_test,y_preds))\n\nprint(\"Confusion matrix\",confusion_matrix(y_test,y_preds))\n\nprint(\"Report\",classification_report(y_test,y_preds))","d846bc8c":"#hyper parameter tuning\ngrid_param = {\n    'criterion' : ['gini', 'entropy'],\n    'max_depth' : [3, 5, 7, 10,13],\n    'splitter' : ['best', 'random'],\n    'min_samples_leaf' : [1, 2, 3, 5, 7],\n    'min_samples_split' : [1, 2, 3, 5, 7],\n    'max_features' : ['auto', 'sqrt', 'log2']\n}\n\ndecision = GridSearchCV(dt, grid_param, cv = 5, n_jobs = -1, verbose = 1)\ndecision.fit(x_train, y_train)\ndecision.best_score_","6d0fc25d":"from sklearn.ensemble import RandomForestClassifier\nrfc=RandomForestClassifier(max_depth=10,min_samples_split=9)\nrfc.fit(x_train,y_train)\n\npredict=rfc.predict(x_test)\n\nprint(accuracy_score(y_test,predict))\n\n","2b1efd83":"#accuracy score,classification report,confusion matrix\nprint(\"accuracy score for trained data\",accuracy_score(y_train,rfc.predict(x_train)))\nprint(\"accuracy score is\",accuracy_score(y_test,predict))\n\nprint(\"Confusion matrix\",confusion_matrix(y_test,predict))\n\nprint(\"Report\",classification_report(y_test,predict))","46208f72":"from sklearn.ensemble import AdaBoostClassifier\nadb = AdaBoostClassifier(base_estimator = dt)\n\nadb.fit(x_train,y_train)\n\npredicts=adb.predict(x_test)","66e970f4":"print(\"accuracy score for trained data\",accuracy_score(y_train,adb.predict(x_train)))\nprint(\"accuracy score is\",accuracy_score(y_test,predicts))\n\nprint(\"Confusion matrix\",confusion_matrix(y_test,predicts))\n\nprint(\"Report\",classification_report(y_test,predicts))","cf4d1ff9":"#hyper parameter tuning.\ngrid_param = {\n    'n_estimators' : [100, 120, 150, 180, 200],\n    'learning_rate' : [0.01, 0.1, 1, 10],\n    'algorithm' : ['SAMME', 'SAMME.R']\n}\n\nsearch_ada = GridSearchCV(adb, grid_param, cv = 10, n_jobs = -1, verbose = 1)\nsearch_ada.fit(x_train, y_train)","edbcdd88":"search_ada.best_score_","575bb6a8":"search_ada.best_estimator_","2716e753":"adc = AdaBoostClassifier(algorithm='SAMME',\n                   base_estimator=DecisionTreeClassifier(max_depth=6,\n                                                         random_state=123),\n                   learning_rate=0.01, n_estimators=100)\nadc.fit(x_train, y_train)","7ea5704f":"accuracy_score(y_test,adc.predict(x_test))","f96618a5":"from sklearn.ensemble import GradientBoostingClassifier\n\ngb = GradientBoostingClassifier(max_depth=3, validation_fraction=1)\ngb.fit(x_train, y_train)","61a146f0":"accuracy_score(y_test,gb.predict(x_test))","9a0c058f":"sgb = GradientBoostingClassifier(subsample = 0.90, max_features = 0.70)\nsgb.fit(x_train, y_train)","acdb5021":"# accuracy score, confusion matrix and classification report of stochastic gradient boosting classifier\n\nsgb_acc = accuracy_score(y_test, sgb.predict(x_test))\n\nprint(f\"Training Accuracy of Decision Tree Classifier is {accuracy_score(y_train, sgb.predict(x_train))}\")\nprint(f\"Test Accuracy of Decision Tree Classifier is {sgb_acc} \\n\")\n\nprint(f\"{confusion_matrix(y_test, sgb.predict(x_test))}\\n\")\nprint(classification_report(y_test, sgb.predict(x_test)))","58ba4081":"from xgboost import XGBClassifier\n\nxgb = XGBClassifier(booster = 'gbtree', gamma=5,learning_rate = 0.1, max_depth = 5, n_estimators = 100,colsample_bytree=1)\nxgb.fit(x_train, y_train)","0d581661":"accuracy_score(y_test,xgb.predict(x_test))","79d32063":"print(\"accuracy score for trained data\",accuracy_score(y_train,xgb.predict(x_train)))\nprint(\"accuracy score is\",accuracy_score(y_test,xgb.predict(x_test)))\n\nprint(\"Confusion matrix\",confusion_matrix(y_test,xgb.predict(x_test)))\n\nprint(\"Report\",classification_report(y_test,xgb.predict(x_test)))","c5bf338a":"from sklearn.ensemble import VotingClassifier \nclassifiers = [('Random forest',rfc),('ADA bossting',adc),('Gradient Boosting',gb),('Xgboost',xgb)]\n# Instantiate a VotingClassifier vc\nvc = VotingClassifier(estimators=classifiers )     \n\n# Fit vc to the training set\nvc.fit(x_train,y_train)   \n\n# Evaluate the test set predictions\ny_pred = vc.predict(x_test)\n\n# Calculate accuracy score\naccuracy = accuracy_score(y_test,y_pred)\nprint('Voting Classifier: {:.3f}'.format(accuracy))\n","7a882005":"finals_predictions = sgb.predict(test)\n\nfinals_predictions","daa591b9":"predictionsss = pd.DataFrame(finals_predictions)\nsubs_dfgs = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\nsubs_dfgs['Survived'] = predictionsss\nsubs_dfgs.to_csv('Submissionsd2s.csv', index = False)","f7ceacf2":"**AdaBoostClassifier**","ffdcfa71":"**GradientBoostingClassifier**","5e3c2925":"**Support Vector Classifier**","d551fd8d":"**RandomForestClassifier**","dae01f9e":"**DecisionTreeClassifier**","bb9fb572":"Dropping some columns which doesnot play a significant role in the dataset.","cfeb1fbd":"**VotingClassifier**","93cdb20d":"**Logistic Regression**","2ed4ab0a":"Converting categorical data to numerical.","ac1f24e9":"**SGDClassifier**","4829899f":"**KNN**","e6abf1e3":"**XGBClassifier**"}}