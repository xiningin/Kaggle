{"cell_type":{"0a206ee3":"code","b53ecdd4":"code","619696fe":"code","c6b4bc28":"code","da43ac51":"code","8c672155":"code","57d97359":"code","ac2732db":"code","0e1c6c56":"code","75da19a1":"code","2c5f8cb2":"code","932f8eb6":"code","0c422ec4":"code","1d45adf8":"code","1ed0cf83":"code","c90d8398":"code","096985fb":"code","25f2830d":"code","3067ddbd":"code","a9e2f4c3":"code","5b0f0cca":"code","dbde8e9e":"code","35c70996":"code","05c15535":"code","fa0310e8":"code","d231f33e":"code","17eac14f":"code","9d722fde":"code","e9bd393a":"code","0c402f9a":"code","6d47fecd":"code","552240de":"code","b590104f":"code","d00a58b5":"code","c5fb0d5c":"code","c745eb3e":"code","4726681e":"code","300e1937":"code","6e943548":"code","3921d85c":"code","572c7f22":"code","627480ff":"code","687eb4cb":"code","8bb82c62":"code","9fc51674":"code","f3d06c4a":"code","ad3fdd6d":"code","fe252eec":"code","c8c77cd5":"code","2e84d397":"code","8dce4f7a":"code","6f8d7396":"code","ab63bc93":"code","8d23bc91":"code","e3685fff":"code","11d79409":"code","d1742f71":"code","d1d5fd1a":"code","a7434f6f":"code","d45513ff":"code","78bcb95a":"code","bf0ceed5":"code","f6186e3a":"code","cb2d36b6":"code","13d5286f":"code","333be4b4":"code","1399ca43":"code","c3c5b69b":"code","effe970d":"code","3c3e1bab":"code","73f16698":"code","e3bfe64e":"code","6deef54a":"code","4addf338":"code","0836c3ee":"code","fe928937":"code","fd41a935":"code","778eb889":"code","84229c49":"code","059f9065":"code","16153075":"code","ef53171e":"code","86c64f16":"code","b2b7118e":"code","35f9df32":"code","fc585a44":"code","f3157369":"code","5cbd6dc4":"code","2f6936be":"code","135b980f":"code","0e292337":"code","104e64cc":"code","ad670c5b":"code","ec2fb701":"code","6cd9dc42":"code","97880d52":"code","cd403a00":"code","ba530ccc":"code","a4102ef6":"code","f484f945":"code","186c277f":"code","0ae5e97b":"code","148ab19e":"code","d5eae2ba":"code","359629c7":"code","e7e698c4":"code","e1bb9231":"code","9a7bde61":"code","3e3e7efd":"code","d69c76f1":"code","12f35804":"code","ef39385b":"code","974c8c6f":"code","7d9dc9ef":"code","c4bb689f":"code","d5bf4d61":"code","51696ba6":"code","c79b79b5":"code","142d43e0":"code","e9b8bf4e":"code","f6b5b15e":"code","b6e3899e":"code","698874f5":"code","f8533f5b":"code","8f9eafd0":"code","d9c9e74d":"code","f957dfe3":"code","bd1b33df":"code","cae2abac":"code","f1a41eb4":"code","1ee7174c":"code","9eaa1b2b":"code","b28bea88":"code","3799f55b":"code","fb207d70":"code","da45fc9a":"code","663e9780":"code","409b113d":"code","61debc06":"code","5ea3c2c5":"code","179abbb7":"code","31640978":"code","3cb509b8":"code","abe6b7e6":"code","97a89f0b":"code","f4e7532a":"code","b5242609":"code","24bf169c":"code","ebf41292":"code","9c425a68":"code","ce0979e3":"code","00dbc0b9":"code","d299ea2d":"code","b364138f":"code","7e9e34a5":"code","f2d0079c":"code","0481e683":"code","5f1ea2c1":"code","5c48e5de":"code","a66b8545":"code","41929b9f":"code","8844364b":"code","012951fb":"code","32c370c8":"code","1c1eef9d":"code","4e1ef7f9":"code","f4c38082":"code","023a955e":"code","9f2d13fa":"code","63aef41f":"code","6f0df76b":"code","bbce0662":"code","ddf8f2d9":"code","75d791ec":"code","427505a4":"code","3af2ca67":"code","5727320b":"code","7c49083e":"code","5e7af6db":"code","c1a982cd":"code","c27fdd8a":"code","816bfbe8":"code","24d946a1":"code","d4890f9a":"code","222277a1":"code","91f0d709":"code","7a8fccdf":"code","2984c2a0":"code","a4c82983":"markdown","605f5354":"markdown","8b67fb1a":"markdown","e7e8d188":"markdown","f345bda3":"markdown","3a663259":"markdown","bddff509":"markdown","a7497aff":"markdown","d20582b8":"markdown","6d26d716":"markdown","db79a1c3":"markdown","deb778b2":"markdown","09218d86":"markdown","f0205a26":"markdown","43345f0a":"markdown","2b3a11c4":"markdown","4cb753c5":"markdown","44bcf1cc":"markdown","fa619347":"markdown","c32bdb68":"markdown","ecc93279":"markdown","41053d73":"markdown","60a8aef9":"markdown","fe0a2b82":"markdown","40dca853":"markdown","33bc06f2":"markdown","17832260":"markdown","75107eaf":"markdown","f8bcac87":"markdown","85b39ceb":"markdown","17349697":"markdown","3442c06e":"markdown","a684374f":"markdown","38cef9e2":"markdown","c1a98909":"markdown","f2b87f2e":"markdown","8aa6a094":"markdown","54bdb498":"markdown","ab57f2ae":"markdown","e3969a87":"markdown","daf414c0":"markdown","891c4e27":"markdown","3a2d9d6d":"markdown","c6440318":"markdown","faca2dd3":"markdown","201620be":"markdown","ccc234c7":"markdown","8067f1a1":"markdown","92b62629":"markdown","6f479cd9":"markdown","74f3ffb1":"markdown","991cbc0c":"markdown","f48b8f3d":"markdown"},"source":{"0a206ee3":"from shutil import copy\n\ncopy('..\/input\/1828166csv\/1828166.csv','.\/1828166.csv')","b53ecdd4":"import gc\nimport math\nimport PIL \nimport pandas as pd\nimport numpy as np\nimport seaborn \nimport datetime\nimport random\nimport warnings\nimport xgboost as xgb\nfrom scipy import stats\nfrom sklearn.linear_model import LinearRegression,Ridge,Lasso\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nimport matplotlib.pyplot as plt\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split , GridSearchCV,cross_val_score,cross_val_predict,cross_validate,RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error,explained_variance_score,max_error,r2_score,median_absolute_error,mean_squared_log_error\nfrom sklearn.feature_selection import VarianceThreshold,SelectKBest,f_regression\nfrom sklearn.preprocessing import MinMaxScaler,normalize,StandardScaler,RobustScaler\nfrom sklearn.preprocessing import OneHotEncoder,LabelEncoder\nfrom sklearn.decomposition import PCA\nimport featuretools as ft\nfrom sklearn.svm import SVR\nfrom mlxtend.feature_selection import SequentialFeatureSelector,ExhaustiveFeatureSelector","619696fe":"# Reading the csv file\ndata = pd.read_csv(\"1828166.csv\")","c6b4bc28":"data","da43ac51":"data.shape","8c672155":"# Data Description\ndata.describe()","57d97359":"# Dataset olumns\ndata.columns","ac2732db":"coun_t  = data.nunique().to_frame(name = 'Counts')\ncoun_t","0e1c6c56":"plt.style.use('dark_background')\n\nx_val = coun_t.index\ny_val = coun_t.values.reshape(coun_t.shape[0])\nplt.figure(figsize = (35,10))\nplt.bar(x_val,y_val,color='maroon')\nplt.show()","75da19a1":"data[['business_code','buisness_year','invoice_currency','document type','cust_payment_terms','isOpen']].agg(pd.Series.mode)        ","2c5f8cb2":"data[['invoice_id','buisness_year','invoice_currency','isOpen']].agg(np.median).to_frame()         ","932f8eb6":"data['total_open_amount'].agg(['mean','std']).to_frame()","0c422ec4":"null_vals = data.isnull().sum().to_frame(name = 'NULL_COUNTS')\nnull_vals = null_vals[null_vals['NULL_COUNTS']>0]\nnull_features = null_vals.index\nnull_vals","1d45adf8":"plt.title(\"NULL VALUES IN DATA\")\nseaborn.barplot(x=null_features,y=null_vals['NULL_COUNTS'])","1ed0cf83":"# Analysis: \"area_business\" has all NaN values --> hence we drop that first\ndata=data.drop(columns=['area_business'])\ndata.shape","c90d8398":"test_dataset = data[data['clear_date'].isnull() == True]\ndata = data.dropna()","096985fb":"test_dataset.shape","25f2830d":"data.isnull().sum()  # All the rows with NULL values are dropped","3067ddbd":"print(\"Percentage of data lost due to dropping the NULL values in the dataframe : \",((50000-data.shape[0])\/50000)*100 ,\"%\")  ","a9e2f4c3":"#Dealing with the duplicates in the dataframe\ndata.duplicated().sum() # No Duplicate Values","5b0f0cca":"test_dataset.duplicated().sum()","dbde8e9e":"#looking for duplicated columns\ndata_trans = data.T\ndata_trans.duplicated()","35c70996":"#looking for duplicated columns\ntest_dataset_trans = test_dataset.T\ntest_dataset_trans.duplicated()","05c15535":"(data['doc_id'] != data['invoice_id']).sum() # Hence column 'doc_id' and 'invoice_id' are identical","fa0310e8":"# We are hence dropping the 'doc_id' column and some other unnecessary columns \ndata = data.drop(columns=['doc_id'])  ","d231f33e":"test_dataset = test_dataset.drop(columns=['doc_id'])","17eac14f":"gc.collect()\ndata.shape","9d722fde":"test_dataset.shape","e9bd393a":"data = data.drop(columns = 'invoice_id')\ntest_dataset = test_dataset.drop(columns = 'invoice_id')","0c402f9a":"data1 = data.copy()\ntest_dataset1 = test_dataset.copy()","6d47fecd":"data.columns","552240de":"data = data.drop(columns = ['name_customer', 'posting_date','document_create_date'])\n","b590104f":"test_dataset = test_dataset.drop(columns =  ['name_customer', 'posting_date','document_create_date'])","d00a58b5":"list(data.columns)","c5fb0d5c":"const_feature = []\nuniq_val_count = []\nunique_cols = dict()\nfor col in list(data.columns):\n    uniq_val_count.append(data[col].nunique())\n    if(data[col].nunique()==1):\n        const_feature.append(col)\n    \nprint('\\n\\n\\nConstant Features are   :',const_feature)\nprint('\\n\\nALL FEATURES WITH UNIQUE VALUES : \\n')\npd.DataFrame({'COLUMN NAMES':list(data.columns) ,'UNIQUE VALUES COUNT':uniq_val_count})\n\n\n# Removing the constant feature\ndata=data.drop(columns=const_feature)\n\n# Removing the constant feature\ntest_dataset=test_dataset.drop(columns=const_feature)","c745eb3e":"list(data.columns)","4726681e":"data['business_code'].unique()","300e1937":"data['cust_number'].unique()","6e943548":"data['cust_payment_terms'].unique()","3921d85c":"data['buisness_year'].unique()","572c7f22":"### Assign 0 for unknown classes\n\nclass LabelEncoderExt(object):\n    def __init__(self):\n        self.label_encoder = LabelEncoder()\n\n    def fit(self, data_list):\n        self.label_encoder = self.label_encoder.fit(list(data_list) + ['Unknown'])\n        self.classes_ = self.label_encoder.classes_\n        return self\n\n    def transform(self, data_list):\n        new_data_list = list(data_list)\n        for unique_item in np.unique(data_list):\n            if unique_item not in self.label_encoder.classes_:\n                new_data_list = ['Unknown' if x==unique_item else x for x in new_data_list]\n        return self.label_encoder.transform(new_data_list)\n    \n    ","627480ff":"list_cust_details = ['buisness_year','cust_number','business_code','cust_payment_terms']\n\nlabel_enc_list = dict()\nfor col in range(len(list_cust_details)):\n    label_encoder = LabelEncoderExt()\n    label_encoder.fit(data[list_cust_details[col]])\n    data[list_cust_details[col]] = label_encoder.transform(data[list_cust_details[col]])\n    label_enc_list[list_cust_details[col]]=label_encoder\n","687eb4cb":"test_dataset.columns","8bb82c62":"for col in range(len(list_cust_details)):\n    test_dataset[list_cust_details[col]] = label_enc_list[list_cust_details[col]].transform(test_dataset[list_cust_details[col]])","9fc51674":"for col in list_cust_details:\n    print(col,\"   :     \\n\",data[col].unique(),\"   \\n     \\n\")","f3d06c4a":"data","ad3fdd6d":"def str_to_date(time_s,st):\n    st=''\n    for i in range(len(time_s)+3):\n        if(i<4):\n            st+=time_s[i]\n        if(i==4):\n            st+='-'\n        if(i==5 or i==6):\n            st+= time_s[i-1]\n        if(i==7):\n            st+='-'\n        if(i==8 or i==9):\n            st+=time_s[i-2]\n    return st  #### REMOVING 'NULL' VALUES FROM THE DATAFRAME","fe252eec":"dt_lis = ['baseline_create_date','clear_date','due_in_date','document_create_date.1']\n\nst = ''\ndata['baseline_create_date'] = pd.Series(data['baseline_create_date']).map(lambda x: str_to_date(str(int(x)),st))\ndata['due_in_date'] = pd.Series(data['due_in_date']).map(lambda x: str_to_date(str(int(x)),st))\ndata['document_create_date.1'] = pd.Series(data['document_create_date.1']).map(lambda x: str_to_date(str(int(x)),st))","c8c77cd5":"st = ''\ntest_dataset['baseline_create_date'] = pd.Series(test_dataset['baseline_create_date']).map(lambda x: str_to_date(str(int(x)),st))\ntest_dataset['due_in_date'] = pd.Series(test_dataset['due_in_date']).map(lambda x: str_to_date(str(int(x)),st))\ntest_dataset['document_create_date.1'] = pd.Series(test_dataset['document_create_date.1']).map(lambda x: str_to_date(str(int(x)),st))","2e84d397":"dt_lis = ['baseline_create_date','clear_date','due_in_date','document_create_date.1']\n\nfor col in dt_lis:\n    data[col] = pd.to_datetime(data[col],format='%Y-%m-%d')\ndata.shape","8dce4f7a":"for col in dt_lis:\n    test_dataset[col] = pd.to_datetime(test_dataset[col],format='%Y-%m-%d')\ntest_dataset.shape","6f8d7396":"data.columns","ab63bc93":"selection = VarianceThreshold(threshold=0.01) # of more that 99% values are same -- we remove the column\n\ncols = list(set(data.columns) - (set(dt_lis) | set(['invoice_currency'])))\n\nselection.fit(data[cols])\n\nprint(\"No. of Features that are Quasi-Constant : \",(len(cols) - sum(selection.get_support())))\n\nquasi_ = list(selection.get_support())\n\nfor i in range(len(quasi_)):\n    if quasi_[i] == False:\n        print(\"The Quasi-Constant Feature in train data is  :\",cols[i])\n        data=data.drop(columns=[cols[i]])\n\nfor i in range(len(quasi_)):\n    if quasi_[i] == False:\n        print(\"The Quasi-Constant Feature in test data is  :\",cols[i])\n        test_dataset=test_dataset.drop(columns=[cols[i]])\n","8d23bc91":"temp = pd.Series(data['document_create_date.1'] - data['baseline_create_date']).dt.days","e3685fff":"rows_to_drop = temp.value_counts().to_frame(name = 'count')","11d79409":"# No. of rows to drop satisfying the condition\nrows_to_drop[(rows_to_drop.index <0)].sum()","d1742f71":"data = data[((temp==0) | (temp>0))].reset_index(drop=True)","d1d5fd1a":"data.shape","a7434f6f":"data = data.drop(columns = ['document_create_date.1'])","d45513ff":"test_dataset = test_dataset.drop(columns = ['document_create_date.1'])","78bcb95a":"dates_list = ['due_in_date','baseline_create_date'] #payment date is excluded  ","bf0ceed5":"data.columns","f6186e3a":"data['payment_term']=pd.Series(data['clear_date'] - data['baseline_create_date']).dt.days\ndata['due_term']=pd.Series(data['due_in_date'] - data['baseline_create_date']).dt.days\ndata['delay'] = data['payment_term'] - data['due_term']\ntest_dataset['due_term']=pd.Series(test_dataset['due_in_date'] - test_dataset['baseline_create_date']).dt.days","cb2d36b6":"def bucketization(x):\n    if x<0:\n        return('(< 0) Days')\n    if x in range(0,16):\n        return('(0-15) Days')\n    elif x in range(16,31):\n        return('(16-30) Days')\n    elif x in range(31,45):\n        return('(31-45) Days')\n    elif x in range(45,60):\n        return('(45-60) Days')\n    else:\n        return('(> 60) Days')","13d5286f":"gc.collect()\n\ndata = data.reset_index(drop=True)\n\ndata['DELAY BUCKET(DAYS)']= pd.Series([bucketization(x=data['delay'][i])  for i in range(len(data))])\ndata['DUE TERM BUCKET(DAYS)']= pd.Series([bucketization(x=data['due_term'][i])  for i in range(len(data))])\n\n\nbucket_mapper = {'(< 0) Days':0,'(0-15) Days':1,'(16-30) Days':2,'(31-45) Days':4,'(45-60) Days':5,'(> 60) Days':6}\n\ndata['delay_bucket_id'] = data['DELAY BUCKET(DAYS)'].map(bucket_mapper)\ndata['due_term_bucket_id'] = data['DUE TERM BUCKET(DAYS)'].map(bucket_mapper)\n\ndata.columns","333be4b4":"test_dataset = test_dataset.reset_index(drop=True)\ntest_dataset['DUE TERM BUCKET(DAYS)']= pd.Series([bucketization(x=test_dataset['due_term'][i])  for i in range(len(test_dataset))])\ntest_dataset['due_term_bucket_id'] = test_dataset['DUE TERM BUCKET(DAYS)'].map(bucket_mapper)","1399ca43":"test_dataset.columns","c3c5b69b":"test_dataset=test_dataset.drop(columns=['clear_date'])","effe970d":"for col1 in ['baseline_create_date','due_in_date']:\n    data['{}.day'.format(col1)]=data[col1].dt.day\n    data['{}.year'.format(col1)]=data[col1].dt.year\n    data['{}.month'.format(col1)]=data[col1].dt.month\n    test_dataset['{}.day'.format(col1)]=test_dataset[col1].dt.day\n    test_dataset['{}.year'.format(col1)]=test_dataset[col1].dt.year\n    test_dataset['{}.month'.format(col1)]=test_dataset[col1].dt.month\n    test_dataset['{}.day_of_week'.format(col1)]=test_dataset[col1].dt.dayofweek","3c3e1bab":"df1 = data.groupby('cust_number').sum()\ndf = df1.rename(columns = {'total_open_amount':'Sum_base_amount'})['Sum_base_amount']\ndf2 = df1.rename(columns = {'due_term':'Sum_due_term'})['Sum_due_term']\n\ndata = pd.merge(data,df,on = 'cust_number' )\ndata = pd.merge(data,df2,on = 'cust_number' )\n\ndf = data.groupby('cust_number').mean()\ndf = df.rename(columns = {'total_open_amount':'mean_base_amount','due_term':'mean_due_term'})\ndata = pd.merge(data,df['mean_base_amount'],on = 'cust_number' )\ndata = pd.merge(data,df['mean_due_term'],on = 'cust_number' )\n\ndata['amount\/mean_amount'] = data['total_open_amount']\/data['mean_base_amount']\ndata['amount-\/mean_amount'] = (data['total_open_amount']-data['mean_base_amount'])\/data['mean_base_amount']\ndata['due_term\/amount'] = data['due_term']\/data['total_open_amount']\ndata['mean_due_term\/amount'] = data['mean_due_term']\/data['total_open_amount']\ndata['mean_due_term\/Sum_base_amount'] = data['mean_due_term']\/data['Sum_base_amount']","73f16698":"df1 = test_dataset.groupby('cust_number').sum()\ndf = df1.rename(columns = {'total_open_amount':'Sum_base_amount'})['Sum_base_amount']\ndf2 = df1.rename(columns = {'due_term':'Sum_due_term'})['Sum_due_term']\ntest_dataset = pd.merge(test_dataset,df,on = 'cust_number' )\ntest_dataset = pd.merge(test_dataset,df2,on = 'cust_number' )\n\ndf = test_dataset.groupby('cust_number').mean()\ndf = df.rename(columns = {'total_open_amount':'mean_base_amount','due_term':'mean_due_term'})\ntest_dataset = pd.merge(test_dataset,df['mean_base_amount'],on = 'cust_number' )\ntest_dataset = pd.merge(test_dataset,df['mean_due_term'],on = 'cust_number' )\n\ntest_dataset['amount\/mean_amount'] = test_dataset['total_open_amount']\/test_dataset['mean_base_amount']\ntest_dataset['amount-\/mean_amount'] = (test_dataset['total_open_amount']- test_dataset['mean_base_amount'])\/test_dataset['mean_base_amount']\ntest_dataset['due_term\/amount'] = test_dataset['due_term']\/test_dataset['total_open_amount']\ntest_dataset['mean_due_term\/amount'] = test_dataset['mean_due_term']\/test_dataset['total_open_amount']\ntest_dataset['mean_due_term\/Sum_base_amount'] = test_dataset['mean_due_term']\/test_dataset['Sum_base_amount']","e3bfe64e":"df = dict(zip(list(data['cust_number'].value_counts().index),list(data['cust_number'].value_counts())))\n\ndata['cust_count'] = data['cust_number'].map(df)\ndata['cust_count\/mean_amount'] = data['cust_count']\/data['mean_base_amount']\n\ntest_dataset['cust_count'] = test_dataset['cust_number'].map(df)\ntest_dataset['cust_count\/mean_amount'] = test_dataset['cust_number']\/test_dataset['mean_base_amount']","6deef54a":"data = data.sort_values(['baseline_create_date']).reset_index(drop=True)\ndata['cust_count*due_term\/amount'] =(data['cust_count']*data['due_term'])\/data['total_open_amount']","4addf338":"test_dataset = test_dataset.sort_values(['baseline_create_date']).reset_index(drop=True)\ntest_dataset['cust_count*due_term\/amount'] =(test_dataset['cust_count']*test_dataset['due_term'])\/test_dataset['total_open_amount']","0836c3ee":"data.isnull().sum()","fe928937":"x = data[data['invoice_currency']=='CAD']['total_open_amount'].map(lambda x: x*0.79)    \n\nfor i in x.index:\n    data['total_open_amount'][i] = 0.79*x[i]\n\n\ndata = data.drop(columns=['invoice_currency'])\n\nx = test_dataset[test_dataset['invoice_currency']=='CAD']['total_open_amount'].map(lambda x: x*0.79)    \n\nfor i in x.index:\n    test_dataset['total_open_amount'][i] = 0.79*x[i]\ntest_dataset = test_dataset.drop(columns=['invoice_currency'])  ","fd41a935":"test_dataset = test_dataset.sort_values(['baseline_create_date']).reset_index(drop=True)\ntest_dataset","778eb889":"cols_drop1 = set(data.columns) - set(test_dataset.columns)\ncols_drop1","84229c49":"data.corr()['payment_term']","059f9065":"# Dropping the date columns\n\n#data = data.drop(columns = ['clear_date', 'due_in_date','baseline_create_date'])\n#test_dataset = test_dataset.drop(columns = ['due_in_date','baseline_create_date'])","16153075":"data['total_open_amount'] = np.log(data['total_open_amount'])","ef53171e":"test_dataset['total_open_amount'] = np.log(test_dataset['total_open_amount'])","86c64f16":"# Looking For the values of categorical features which are diffrent from train data\nlist_cust_details = ['cust_number','business_code','cust_payment_terms']\n\nfor col in list_cust_details:\n    coun = 0\n    for i in range(len(test_dataset)):\n        if test_dataset[col][i] in set(test_dataset[col])-set(data[col]):\n            coun+=1\n    print(\"\\n\\n\\nColumn Name:    \",col,\"\\nTotal Unique Values in train-set:     \",data[col].nunique(),\"\\nNew Valuess introduced in Test:     \",len(set(test_dataset[col].unique()) - set(data[col].unique())),\"\\nValues are:\",set(test_dataset[col])-set(data[col]),\"\\nCount :\",coun)","b2b7118e":"data.columns","35f9df32":"gc.collect()","fc585a44":"d_corr=data.corr()\nd_corr","f3157369":"data.cov()","5cbd6dc4":"for col in ['business_code','cust_payment_terms', 'delay_bucket_id','due_term_bucket_id']:\n    plt.style.use('classic')\n    fig,ax = plt.subplots(figsize=(10,10))\n    plt.pie(data[col].value_counts(),labels=data[col].unique(),shadow=True,autopct='%1.1f%%')\n    plt.title('\\nCount of {}'.format(col))\n    plt.show()","2f6936be":"features = ['business_code','cust_payment_terms', 'delay_bucket_id','due_term_bucket_id']\n\nfor col in features:\n    plt.style.use('dark_background')\n    fig,ax = plt.subplots(figsize=(10,8))\n    plt.bar(list(data[col].value_counts().index),list(data[col].value_counts()),color = random.sample(['maroon','yellow'],1))            \n    plt.title('\\nCount of {}'.format(col))\n    plt.figure(figsize=(20,20))\n    plt.show()","135b980f":"features = ['business_code','cust_payment_terms', 'delay_bucket_id','due_term_bucket_id']\n\nfor col in features:\n    plt.style.use('dark_background')\n    seaborn.boxplot(data[col],color='maroon',notch=True)\n    plt.title('Box Plot of {}'.format(col))\n    plt.figure(figsize=(20,20))\n    plt.show()","0e292337":"data.columns","104e64cc":"x_val = ['cust_number','business_code', 'total_open_amount', 'cust_payment_terms']\ny_val = 'payment_term'\nfor col in x_val:\n    plt.scatter(data[col],data['delay'],color=random.sample(['yellow','maroon'],1),linewidth = .5)\n    plt.title('{} V\/S {}'.format(col,y_val))\n    plt.xlabel(col)\n    plt.ylabel(y_val)\n    plt.show()","ad670c5b":"data.nunique()","ec2fb701":"for col in list(set(data.columns) - set(['DELAY BUCKET(DAYS)' ,'DUE TERM BUCKET(DAYS)'])):\n    seaborn.distplot(data[col],color='maroon')\n    plt.show()","6cd9dc42":"data_copy = data.copy()\ntest_dataset_copy = test_dataset.copy()","97880d52":"data.describe()","cd403a00":"pay_max = 80\ndata[data['payment_term']>=pay_max]['payment_term'].value_counts().sum()","ba530ccc":"data = data[(data['total_open_amount'] > 1) & (data['payment_term']<=pay_max )].reset_index(drop=True)","a4102ef6":"data.shape","f484f945":"test_dataset.shape","186c277f":"for col in list(set(data.columns) - set(['DELAY BUCKET(DAYS)' ,'DUE TERM BUCKET(DAYS)'])):\n    seaborn.distplot(data[col],color='maroon')\n    plt.show()","0ae5e97b":"print(list(data.columns))\nprint(\"\\n\\n\",len(list(data.columns)))","148ab19e":"categorical_features = ['business_code', 'cust_payment_terms','due_term_bucket_id']","d5eae2ba":"numerical_features = list(set(set(data.columns) - set(['DELAY BUCKET(DAYS)', 'DUE TERM BUCKET(DAYS)', 'delay_bucket_id', 'due_term_bucket_id','clear_date', 'due_in_date', 'baseline_create_date'])) - set(categorical_features))","359629c7":"len(numerical_features)","e7e698c4":"len(categorical_features)                                               ","e1bb9231":"infos_df = data[['clear_date', 'due_in_date', 'baseline_create_date','total_open_amount']]","9a7bde61":"train_num = data[list(set(numerical_features)   -  set(['payment_term','delay']))]\ntest_num = test_dataset[list(set(numerical_features)   -  set(['payment_term','delay']))]\ntrain_cat = data[categorical_features]\ntest_cat = test_dataset[list(set(categorical_features))]","3e3e7efd":"scaler = MinMaxScaler()\ny_scaler = MinMaxScaler()\nfinal_train_n = pd.DataFrame(scaler.fit_transform(train_num[list(set(numerical_features)-set(['payment_term','delay']))]),columns=list(set(numerical_features)-set(['payment_term','delay'])))\nfinal_test_n = pd.DataFrame(scaler.fit_transform(test_num),columns=list(set(numerical_features)-set(['payment_term','delay'])))\ndata['delay'] = y_scaler.fit_transform(np.array(data['delay']).reshape(data['delay'].shape[0],1))","d69c76f1":"num_cols = final_train_n.columns","12f35804":"final_train_n=pd.merge(final_train_n,data['cust_number'],left_index=True,right_index=True)\nfinal_test_n=pd.merge(final_test_n,test_dataset['cust_number'],left_index=True,right_index=True)","ef39385b":"final_test_n=pd.merge(final_test_n,infos_df.drop(columns = ['clear_date']),left_index=True,right_index=True)","974c8c6f":"final_train_n=pd.merge(final_train_n,infos_df,left_index=True,right_index=True)","7d9dc9ef":"final_test_n.shape","c4bb689f":"final_train_n.shape","d5bf4d61":"data_cat=pd.concat((train_cat,test_cat),sort=False).reset_index(drop=True)\ndata_cat","51696ba6":"data_cat1 = pd.get_dummies(data_cat,prefix='enc_c_',columns = ['business_code'],drop_first=False)\ndata_cat2 = pd.get_dummies(data_cat,prefix='enc_c___',columns = ['cust_payment_terms'],drop_first=False)\ndata_cat1 = pd.merge(data_cat1,data_cat2,left_index=True,right_index=True)\ndata_cat2 = pd.get_dummies(data_cat,prefix='enc_c',columns = ['due_term_bucket_id'],drop_first=False)\ndata_cat1 = pd.merge(data_cat1,data_cat2,left_index=True,right_index=True)\n\ndata_cat1 = data_cat1.drop(columns =['cust_payment_terms_x', 'due_term_bucket_id_x','due_term_bucket_id_y','business_code_y','cust_payment_terms_y','business_code_x'])    ","c79b79b5":"data[['business_code','cust_payment_terms','due_term_bucket_id']].nunique().sum()","142d43e0":"data_cat1.columns","e9b8bf4e":"final_train_c = data_cat1[:data.shape[0]].reset_index(drop=True)\nfinal_test_c = data_cat1[data.shape[0]:].reset_index(drop=True)","f6b5b15e":"#final_train_c = train_cat\n#final_test_c = test_cat","b6e3899e":"final_train_c.shape","698874f5":"final_test_c.shape","f8533f5b":"test_data = final_test_n.merge(final_test_c.reset_index(drop=True),left_index=True,right_index=True)\n#test_data = test_data.merge(final_test_c.reset_index(drop=True),left_index=True,right_index=True)\ntest_data","8f9eafd0":"train_data = final_train_n.merge(final_train_c.reset_index(drop=True),left_index=True,right_index=True)\ntrain_data = train_data.merge(data['delay'].reset_index(drop=True),left_index=True,right_index=True)\ntrain_data","d9c9e74d":"set(train_data.columns) - set(test_data.columns)","f957dfe3":"train_data = train_data.copy()\ndata_copy = data.copy()","bd1b33df":"x_data = train_data[set(train_data.columns) - set(['delay'])]\nx_data.shape","cae2abac":"x_test = test_data[set(test_data.columns) - set(['delay'])]\nx_test.shape","f1a41eb4":"y_data = train_data['delay']\ny_data.shape","1ee7174c":"eval_range = (math.ceil(data.shape[0]*(0.25)))+1\nx_train,x_eval,y_train,y_eval = x_data[eval_range:].reset_index(drop=True),x_data[0:eval_range].reset_index(drop=True),y_data[eval_range:].reset_index(drop=True),y_data[0:eval_range].reset_index(drop=True)       ","9eaa1b2b":"eval_range = (math.ceil(x_eval.shape[0]*(0.4)))+1\nx_eval1,x_eval2,y_eval1,y_eval2 = x_eval[eval_range:].reset_index(drop=True),x_eval[0:eval_range].reset_index(drop=True),y_eval[eval_range:].reset_index(drop=True),y_eval[0:eval_range].reset_index(drop=True)","b28bea88":"x_eval1.columns","3799f55b":"x_train_info = x_train[['clear_date','total_open_amount_y','cust_number_y','baseline_create_date','due_in_date']]\nx_eval1_info = x_eval1[['clear_date','total_open_amount_y','cust_number_y','baseline_create_date','due_in_date']]\nx_eval2_info = x_eval2[['clear_date','total_open_amount_y','cust_number_y','baseline_create_date','due_in_date']]\nx_test_info = x_test[['total_open_amount_y','cust_number_y','baseline_create_date','due_in_date']]\n\nx_train = x_train.drop(columns=['clear_date','baseline_create_date','due_in_date'])\nx_eval1 = x_eval1.drop(columns=['clear_date','baseline_create_date','due_in_date'])\nx_eval2 = x_eval2.drop(columns=['clear_date','baseline_create_date','due_in_date'])\nx_test = x_test.drop(columns=['baseline_create_date','due_in_date'])","fb207d70":"x_train.shape,y_train.shape","da45fc9a":"x_eval1.shape,y_eval1.shape","663e9780":"x_eval2.shape,y_eval2.shape","409b113d":"x_test.shape","61debc06":"#y_scaler.inverse_transform(np.array(y_eval).reshape(y_eval.shape[0],1))","5ea3c2c5":"#To write the final created dataset to the working directory\nx_train.to_csv('x_train.csv')\nx_test.to_csv('x_test.csv')\ny_train.to_csv('y_train.csv')\n\nx_eval1.to_csv('x_eval1.csv')\nx_eval2.to_csv('x_eval2.csv')\ny_eval1.to_csv('y_eval1.csv')\ny_eval2.to_csv('y_eval2.csv')","179abbb7":"gc.collect()","31640978":"def select_features(x_train,y_train,x_test,x_eval1,x_eval2):\n    fs = SelectKBest(score_func = f_regression,k=10)\n    fs.fit(x_train,y_train)\n    col_indices = fs.get_support(indices=True)\n    x_features = x_train.columns[col_indices]\n    x_train_fs = pd.DataFrame(fs.transform(x_train),columns=x_features)\n    x_test_fs = pd.DataFrame(fs.transform(x_test),columns=x_features)\n    x_eval1_fs = pd.DataFrame(fs.transform(x_eval1),columns=x_features)\n    x_eval2_fs = pd.DataFrame(fs.transform(x_eval2),columns=x_features)\n    return x_train_fs,x_test_fs,x_eval1_fs,x_eval2_fs,fs,x_features,col_indices","3cb509b8":"#x_train_fs,x_test_fs,x_eval1_fs,x_eval2_fs,fs,x_features,col_indices = select_features(x_train,y_train,x_test,x_eval1,x_eval2)","abe6b7e6":"#for i in range(len(fs.scores_[col_indices])):\n#    print(\"\\n\",x_features[i],\"    \",fs.scores_[col_indices][i])","97a89f0b":"#selected_features = list(set(train_data.corr()['delay'][train_data.corr()['delay']>0.5].index) - set(['delay']) )\n#selected_features","f4e7532a":"x_train.columns","b5242609":"def evaluate_metrics(x,y_true,y_pred,mod):\n    mean_abs_error = mean_absolute_error(y_true,y_pred)\n    mean_sq_error = mean_squared_error(y_true,y_pred)\n    root_mean_sq_error = mean_squared_error(y_true,y_pred)**0.5\n    r2_scr = r2_score(y_true,y_pred)\n    median_abs_score = median_absolute_error(y_true,y_pred)\n    explained_variance = explained_variance_score(y_true,y_pred)\n    return mean_abs_error,mean_sq_error,root_mean_sq_error,r2_scr,median_abs_score,explained_variance  ","24bf169c":"data.corr()['payment_term']","ebf41292":"regression_algo = ['1. LINEAR REGRESSION','2. RIDGE REGRESSION','3. LASSO REGRESSION','4. RANDOM FOREST REGRESSION','5. SUPPORT VECTOR REGRESSION(RBF)','6. DECISION TREE REGRESSION','7. XBF REGRESSION'] \n\nmetric_names = [\"MEAN ABSOLUTE ERROR\",\"SCORE\",\"MEAN SQUARED ERROR\",\"ROOT MEAN SQUARED ERROR\",\"R2 SCORE\",\"MEDIAN ABSOLUTE ERROR\",\"EXPLAINED VARIANCE SCORE\"]\n\nreg_metrics = []","9c425a68":"gc.collect()","ce0979e3":"model1 = LinearRegression()\nparameters = {'fit_intercept':[True,False],'n_jobs':[1,2,3],'normalize':[True,False]}\ngrid = GridSearchCV(estimator = model1,param_grid= parameters,scoring = 'neg_mean_squared_error',verbose = 1)     \ngrid_results = grid.fit(x_train,y_train)\nprint('BEST SCORE:  ',grid_results.best_score_)\nprint('BEST PARAMS:  ',grid_results.best_params_)","00dbc0b9":"model1 = LinearRegression(fit_intercept = grid_results.best_params_['fit_intercept'],n_jobs = grid_results.best_params_['n_jobs'])\nmodel1.fit(x_train,y_train)\ny_pred1 = model1.predict(x_eval1)\ny_pred1 = y_pred1.reshape(y_pred1.shape[0])\ny_eval1 = np.array(y_eval1)\ny_predicted = y_scaler.inverse_transform(y_pred1.reshape(y_pred1.shape[0],1)).reshape(y_pred1.shape[0])\ny_true = y_scaler.inverse_transform(y_eval1.reshape(y_eval1.shape[0],1)).reshape(y_eval1.shape[0])\nmean_abs_error,mean_sq_error,root_mean_sq_error,r2_scr,median_abs_score,explained_variance = evaluate_metrics(x_eval1,y_eval1,y_pred1,model1)\nscr = model1.score(x_eval1,y_eval1)\nreg_metrics.append([mean_abs_error,scr,mean_sq_error,root_mean_sq_error,r2_scr,median_abs_score,explained_variance])\n\nprint(\"METRICS  :\\n\\nMean Absolute Error :  \",mean_abs_error,\"\\nScore :   \",scr,\"\\nMean Squared Error :  \",mean_sq_error,\"\\nRoot Mean Squared Error :  \",root_mean_sq_error,\"\\nR2 Square :  \",r2_scr,\"\\nMedian Absolute Score :  \",median_abs_score,\"\\nExplained Variance Score :  \",explained_variance)    ","d299ea2d":"reg_metrics","b364138f":"def plot_regression_line(x_test,y_test,y_pred):\n    for col in x_test.columns:\n        #plt.scatter(x_test[col],y_test,color='red')\n        plt.plot(x_test[col],y_pred1,color='yellow',linewidth = 0.5)\n        plt.xlim(-0.00000000000001,0.000000000000001)\n        plt.xlabel(col)\n        plt.ylabel('delay')\n        plt.title('{} V\/S {} Regression Line\\n'.format(col,'delay'))\n        plt.show()","7e9e34a5":"#plot_regression_line(x_eval1,y_eval1,y_pred1)","f2d0079c":"ridge = Ridge()\nparameters = {'alpha':[x for x in [0.0005,0.0001,0.00021,0.0006,0.1,0.001,0.005,0.008,0.1,0.5,1,0.1,0.09,0.08,0.06,0.05,0.03,0.01,0.02,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]]}  \n\nridge_reg = GridSearchCV(ridge,param_grid = parameters,verbose=1)\nridge_reg = ridge_reg.fit(x_train,y_train)\n\nalpha = ridge_reg.best_params_['alpha']\n\nprint(\"\\n\\nBest Alpha:  \",ridge_reg.best_params_,\"\\nScore:  \",ridge_reg.best_score_)\n\nridge_mod = Ridge(alpha=alpha)\nridge_mod=ridge_mod.fit(x_train,y_train)\ny_pred1 = ridge_mod.predict(x_eval1)\ny_pred2 = ridge_mod.predict(x_eval2)\n\n\nmean_abs_error,mean_sq_error,root_mean_sq_error,r2_scr,median_abs_score,explained_variance = evaluate_metrics(x_eval1,y_eval1,y_pred1,ridge_mod)\nscr = ridge_mod.score(x_eval1,y_eval1)\nprint(\"\\n\\n\\nMETRICS  :\\n\\nMean Absolute Error :  \",mean_abs_error,\"\\nScore :   \",scr,\"\\nMean Squared Error :  \",mean_sq_error,\"\\nRoot Mean Squared Error :  \",root_mean_sq_error,\"\\nR2 Square :  \",r2_scr,\"\\nMedian Absolute Score :  \",median_abs_score,\"\\nExplained Variance Score :  \",explained_variance)\n\n","0481e683":"reg_metrics.append([mean_abs_error,scr,mean_sq_error,root_mean_sq_error,r2_scr,median_abs_score,explained_variance])\nreg_metrics","5f1ea2c1":"# hyper parameter tuninhg using the evaluation set\n\nridge_reg = GridSearchCV(ridge,param_grid = parameters,verbose=1)\nridge_reg = ridge_reg.fit(x_eval1,y_eval1)\nalpha = ridge_reg.best_params_['alpha']\n\nprint(\"\\n\\nBest Alpha:  \",ridge_reg.best_params_,\"\\nScore:  \",ridge_reg.best_score_)\n","5c48e5de":"# For 2nd Evaluation set\n\nmean_abs_error,mean_sq_error,root_mean_sq_error,r2_scr,median_abs_score,explained_variance = evaluate_metrics(x_eval2,y_eval2,y_pred2,ridge_mod)\nscr = ridge_mod.score(x_eval2,y_eval2)\nprint(\"\\n\\n\\nMETRICS  :\\n\\nMean Absolute Error :  \",mean_abs_error,\"\\nScore :   \",scr,\"\\nMean Squared Error :  \",mean_sq_error,\"\\nRoot Mean Squared Error :  \",root_mean_sq_error,\"\\nR2 Square :  \",r2_scr,\"\\nMedian Absolute Score :  \",median_abs_score,\"\\nExplained Variance Score :  \",explained_variance)    ","a66b8545":"# Hyper Parameter tuning using the train data\n\nlasso_reg = Lasso()\nparameters = {'alpha':[x for x in [0.0005,0.0001,0.00021,0.0006,0.1,0.001,0.005,0.008,0.1,0.5,1]]}  \nlasso_reg = GridSearchCV(lasso_reg,param_grid=parameters,verbose=1)\nlasso_reg.fit(x_train,y_train)\n\nprint(\"\\n\\n\\nBest Alpha : \",lasso_reg.best_params_,\"\\nBest Score : \",lasso_reg.best_score_)\n\nlasso_reg = Lasso(alpha=0.0001)\nlasso_reg = lasso_reg.fit(x_train,y_train)\ny_pred1 = np.array(lasso_reg.predict(x_eval1))\n\nmean_abs_error,mean_sq_error,root_mean_sq_error,r2_scr,median_abs_score,explained_variance = evaluate_metrics(x_eval1,y_eval1.reshape(y_eval1.shape[0],1),y_pred1.reshape(y_pred1.shape[0],1),ridge_mod)\nscr = lasso_reg.score(x_eval1,y_eval1)\nreg_metrics.append([mean_abs_error,scr,mean_sq_error,root_mean_sq_error,r2_scr,median_abs_score,explained_variance])\n\nprint(\"METRICS  :\\n\\nMean Absolute Error :  \",mean_abs_error,\"\\nScore :   \",scr,\"\\nMean Squared Error :  \",mean_sq_error,\"\\nRoot Mean Squared Error :  \",root_mean_sq_error,\"\\nR2 Square :  \",r2_scr,\"\\nMedian Absolute Score :  \",median_abs_score,\"\\nExplained Variance Score :  \",explained_variance)    ","41929b9f":"# Hyper Parameter tuning using the 1st evaluation Data\n\nparameters = {'alpha':[x for x in [0.0005,0.0002,0.0003,0.0004,0.009,0.0001,0.00021,0.0006,0.1,0.001,0.005,0.008,0.1,0.5,1]]}  \nlasso_reg = GridSearchCV(lasso_reg,param_grid=parameters)\nlasso_reg.fit(x_eval1,y_eval1)\n\nprint(\"\\n\\n\\nBest Alpha : \",lasso_reg.best_params_,\"\\nBest Score : \",lasso_reg.best_score_)\n\nlasso_reg = Lasso(alpha=0.0001)\nlasso_reg = lasso_reg.fit(x_train,y_train)    ","8844364b":"## We still get the best value of alpha as : 0.0001.. So ther e is not much difference\n\n\n# Evaluating on the 2nd Evaluation Data\n\ny_pred2 = np.array(lasso_reg.predict(x_eval2))\n\nmean_abs_error,mean_sq_error,root_mean_sq_error,r2_scr,median_abs_score,explained_variance = evaluate_metrics(x_eval2,np.array(y_eval2).reshape(np.array(y_eval2).shape[0],1),y_pred2.reshape(y_pred2.shape[0],1),ridge_mod)\nscr = lasso_reg.score(x_eval2,y_eval2)\n#reg_metrics.append([mean_abs_error,scr,mean_sq_error,root_mean_sq_error,r2_scr,median_abs_score,explained_variance])\n\nprint(\"\\n\\n1st Evaluation Data\\n\\nMETRICS  :\\n\\nMean Absolute Error :  \",mean_abs_error,\"\\nScore :   \",scr,\"\\nMean Squared Error :  \",mean_sq_error,\"\\nRoot Mean Squared Error :  \",root_mean_sq_error,\"\\nR2 Square :  \",r2_scr,\"\\nMedian Absolute Score :  \",median_abs_score,\"\\nExplained Variance Score :  \",explained_variance)    ","012951fb":"gc.collect()","32c370c8":"%%time\n\n# Performing Grid-Search\n#gsc = GridSearchCV(estimator = RandomForestRegressor(),param_grid={'max_depth':[9,10],'n_estimators':(500,1000)},cv=5,scoring = 'neg_mean_squared_error',verbose=1,n_jobs=-1)\n#grid_result = gsc.fit(x_train,y_train)\n#best_params = grid_result.best_params_\n#print(\"\\n\\nBest Params : \\n\",best_params)\n\nrfr = RandomForestRegressor(max_depth=10,n_estimators=500,random_state=False,verbose=True) \n\nrfr.fit(x_train,y_train)\n\ny_pred1 = rfr.predict(x_eval1)\n\nscr = rfr.score(x_eval1,y_eval1)\n\nmean_abs_error,mean_sq_error,root_mean_sq_error,r2_scr,median_abs_score,explained_variance = evaluate_metrics(x_eval1,y_eval1,y_pred1,rfr)\nreg_metrics.append([mean_abs_error,scr,mean_sq_error,root_mean_sq_error,r2_scr,median_abs_score,explained_variance])\n\nprint(\"METRICS  :\\n\\nMean Absolute Error :  \",mean_abs_error,\"\\nScore :   \",scr,\"\\nMean Squared Error :  \",mean_sq_error,\"\\nRoot Mean Squared Error :  \",root_mean_sq_error,\"\\nR2 Square :  \",r2_scr,\"\\nMedian Absolute Score :  \",median_abs_score,\"\\nExplained Variance Score :  \",explained_variance)    ","1c1eef9d":"# Evaluating in with the 2nd evaluation data\n\ny_pred2 = rfr.predict(x_eval2)\n\nscr = rfr.score(x_eval2,y_eval2)\n\nmean_abs_error,mean_sq_error,root_mean_sq_error,r2_scr,median_abs_score,explained_variance = evaluate_metrics(x_eval2,np.array(y_eval2).reshape(np.array(y_eval2).shape[0],1),y_pred2,rfr)\nprint(\"METRICS  :\\n\\nMean Absolute Error :  \",mean_abs_error,\"\\nScore :   \",scr,\"\\nMean Squared Error :  \",mean_sq_error,\"\\nRoot Mean Squared Error :  \",root_mean_sq_error,\"\\nR2 Square :  \",r2_scr,\"\\nMedian Absolute Score :  \",median_abs_score,\"\\nExplained Variance Score :  \",explained_variance)    ","4e1ef7f9":"reg_metrics","f4c38082":"# C: regulation parameter --> inversely proportional to regularization\n# 'epsilon' --> It specifies the epsilon-tube within whic LPVh no penalty is associated in the training loss function with points predicted within a distance epsilon from the actual value.","023a955e":"%%time\n\n#gsc1 = GridSearchCV(estimator=SVR(kernel='linear'),param_grid={'C':[0.1,1,10,100,1000],'epsilon':[10,5,1,0.1,0.5,0.01,0.001,0.0001,0.00001]},cv=5,scoring='neg_mean_squared_error',verbose=0,n_jobs=-1)      \n#grid_result = gsc1.fit(x_train,y_train)\n#best_params = grid_result.best_params_\n#svr1 = SVR(kernel='linear',C=10,epsilon=0.01,cache_size=200,verbose=True,max_iter = -1)        \n#svr1 = svr1.fit(x_train,y_train)\n#y_pred1 = svr1.fit(x_eval1,y_eval1)\n#mean_abs_error,mean_sq_error,root_mean_sq_error,r2_scr,median_abs_score,explained_variance = evaluate_metrics(x_eval1,y_eval1,y_pred1,svr1)\n#scr = svr1.score(x_eval1,y_eval1)\n#reg_metrics.append([mean_abs_error,scr,mean_sq_error,root_mean_sq_error,r2_scr,median_abs_score,explained_variance])\n#print(\"METRICS  :\\n\\nMean Absolute Error :  \",mean_abs_error,\"\\nScore :   \",scr,\"\\nMean Squared Error :  \",mean_sq_error,\"\\nRoot Mean Squared Error :  \",root_mean_sq_error,\"\\nR2 Square :  \",r2_scr,\"\\nMedian Absolute Score :  \",median_abs_score,\"\\nExplained Variance Score :  \",explained_variance)   ","9f2d13fa":"%%time\n\n#gsc1 = GridSearchCV(estimator=SVR(kernel='poly'),param_grid={'C':[0.1,1,10,100,1000],'epsilon':[10,5,1,0.1,0.5,0.01,0.01,0.005,0.001,0.0001,0.00001],'degree':[2,3,4,5],'coef0':[0.1,0.01,0.001,0.0001]},cv=5,scoring='neg_mean_squared_error',verbose=0,n_jobs=-1)      \n#grid_result = gsc1.fit(x_train,y_train)\n#best_params = grid_result.best_params_\n#svr1 = SVR(kernel='poly',C=1,epsilon=0.01,coef0=0.001,degree = 2,shrinking = True, tol=0.001 ,cache_size=200,verbose=False,max_iter = -1)        \n#y_pred1 = svr1.predict(x_eval1,y_eval1)\n#mean_abs_error,mean_sq_error,root_mean_sq_error,r2_scr,median_abs_score,explained_variance = evaluate_metrics(x_eval1,y_eval1,y_pred1,svr1)\n#scr = svr1.score(x_eval1,y_eval1)\n#reg_metrics.append([mean_abs_error,mean_sq_error,root_mean_sq_error,r2_scr,median_abs_score,explained_variance])\n\n#print(\"METRICS  :\\n\\nMean Absolute Error :  \",mean_abs_error,\"\\nScore :   \",scr,\"\\nMean Squared Error :  \",mean_sq_error,\"\\nRoot Mean Squared Error :  \",root_mean_sq_error,\"\\nR2 Square :  \",r2_scr,\"\\nMedian Absolute Score :  \",median_abs_score,\"\\nExplained Variance Score :  \",explained_variance)   ","63aef41f":"%%time\n\n#gsc1 = GridSearchCV(estimator=SVR(kernel='rbf'),param_grid={'C':[0.1,1,10,100,1000],'epsilon':[10,5,1,0.1,0.5,0.01,0.01,0.005,0.001,0.0001,0.0005],'gamma':[5,3,1,0.1,0.01,0.05,0.001,0.005,0.0001]},cv=5,scoring='neg_mean_squared_error',verbose=0,n_jobs=-1)      \n#grid_result = gsc1.fit(x_train,y_train)\n#best_params = grid_result.best_params_\n#svr1 = SVR(kernel='rbf',C=10,epsilon=0.01,gamma=0.01,coef0=0.1,shrinking = True, tol=0.001 ,cache_size=200,verbose=True,max_iter = -1)        \n#svr1 = svr1.fit(x_train,y_train)\n#y_pred1 = svr1.predict(x_eval1)\n#mean_abs_error,mean_sq_error,root_mean_sq_error,r2_scr,median_abs_score,explained_variance = evaluate_metrics(x_eval1,y_eval1,y_pred1,svr1)\n#scr = svr1.score(x_eval1,y_eval1)\n#reg_metrics.append([mean_abs_error,mean_sq_error,root_mean_sq_error,r2_scr,median_abs_score,explained_variance])\n#print(\"METRICS  :\\n\\nMean Absolute Error :  \",mean_abs_error,\"\\nScore :   \",scr,\"\\nMean Squared Error :  \",mean_sq_error,\"\\nRoot Mean Squared Error :  \",root_mean_sq_error,\"\\nR2 Square :  \",r2_scr,\"\\nMedian Absolute Score :  \",median_abs_score,\"\\nExplained Variance Score :  \",explained_variance)   ","6f0df76b":"dtr = DecisionTreeRegressor()\ndtr.fit(x_train,y_train)\ny_pred1 = dtr.predict(x_eval1)\nmean_abs_error,mean_sq_error,root_mean_sq_error,r2_scr,median_abs_score,explained_variance = evaluate_metrics(x_eval1,y_eval1,y_pred1,dtr)\nscr = dtr.score(x_eval1,y_eval1)\nreg_metrics.append([mean_abs_error,scr,mean_sq_error,root_mean_sq_error,r2_scr,median_abs_score,explained_variance])\nprint(\"METRICS  :\\n\\nMean Absolute Error :  \",mean_abs_error,\"\\nScore :   \",scr,\"\\nMean Squared Error :  \",mean_sq_error,\"\\nRoot Mean Squared Error :  \",root_mean_sq_error,\"\\nR2 Square :  \",r2_scr,\"\\nMedian Absolute Score :  \",median_abs_score,\"\\nExplained Variance Score :  \",explained_variance)   ","bbce0662":"cols = list(set(x_train.columns) - set(['cust_payment_terms', 'business_code']) )","ddf8f2d9":"xgb_model = xgb.XGBRegressor()\nxgb_model.fit(x_train,y_train)\ny_pred1 = xgb_model.predict(x_eval1)\nmean_abs_error,mean_sq_error,root_mean_sq_error,r2_scr,median_abs_score,explained_variance = evaluate_metrics(x_eval1,y_eval1,y_pred1,dtr)\nscr = dtr.score(x_eval1,y_eval1)\nreg_metrics.append([mean_abs_error,scr,mean_sq_error,root_mean_sq_error,r2_scr,median_abs_score,explained_variance])\nprint(\"METRICS  :\\n\\nMean Absolute Error :  \",mean_abs_error,\"\\nScore :   \",scr,\"\\nMean Squared Error :  \",mean_sq_error,\"\\nRoot Mean Squared Error :  \",root_mean_sq_error,\"\\nR2 Square :  \",r2_scr,\"\\nMedian Absolute Score :  \",median_abs_score,\"\\nExplained Variance Score :  \",explained_variance)   ","75d791ec":"regression_algo = ['1. LINEAR REGRESSION','2. RIDGE REGRESSION','3. LASSO REGRESSION','4. RANDOM FOREST REGRESSION','5. DECISION TREE REGRESSION','6. XBF REGRESSION'] \n\nmetric_names = [\"MEAN ABSOLUTE ERROR\",\"SCORE\",\"MEAN SQUARED ERROR\",\"ROOT MEAN SQUARED ERROR\",\"R2 SCORE\",\"MEDIAN ABSOLUTE ERROR\",\"EXPLAINED VARIANCE SCORE\"]\n","427505a4":"comp_metrics = pd.DataFrame(reg_metrics,columns= metric_names, index = regression_algo)","3af2ca67":"comp_metrics","5727320b":"x_test_info","7c49083e":"output = pd.DataFrame(columns=['INVOICE CREATE DATE','AMOUNT(IN USD)','INVOICE DUE DATE','INVOICE PAYMENT TERM','INVOICE PAYMENT DATE','DELAY','BUCKET ID'])","5e7af6db":"output['INVOICE CREATE DATE'] = x_test_info['baseline_create_date']\noutput['INVOICE DUE DATE'] = x_test_info['due_in_date']\noutput['AMOUNT(IN USD)'] = x_test_info['total_open_amount_y'].apply(np.exp)","c1a982cd":"x_test = x_test.dropna()\ny_pred = model1.predict(x_test)\ny_pred = y_scaler.inverse_transform(np.array(y_pred).reshape(np.array(y_pred).shape[0],1)).reshape(np.array(y_pred).shape[0])\ny_pred = np.ceil(np.array(y_pred))","c27fdd8a":"output['INVOICE PAYMENT TERM'] = y_pred\noutput['INVOICE PAYMENT DATE'] = output['INVOICE CREATE DATE'] + pd.to_timedelta(np.ceil(output['INVOICE PAYMENT TERM']),'D')","816bfbe8":"output['INVOICE PAYMENT TERM'] ","24d946a1":"output['DELAY'] = pd.Series(output['INVOICE PAYMENT TERM'] - output['INVOICE DUE DATE']).dt.days","d4890f9a":"num_cols","222277a1":"x_test_num = x_test[(set(num_cols) - set(['cust_number']))|set(['cust_number_x'])]","91f0d709":"x_test_num.shape","7a8fccdf":"x_test_num = pd.Series(scaler.inverse_transform(np.array(x_test_num)))","2984c2a0":"output['INVOICE CREATE DATE'] = pd.Datetime(x_test['metric_names'])","a4c82983":"### Removing the column unique to each row : 'invoice_id'","605f5354":"### 7. XGB REGRESSOR","8b67fb1a":"### DROPPING THE COLUMNS WHERE THE DOCUMENT FOR INVOICES ARE CREATED BEFORE THE BASELINE CREATE DATE","e7e8d188":"#### Frequency Table","f345bda3":"###  Scaling the Numerical features","3a663259":"### Linear Kernel","bddff509":"#### Distribution after removing outliers","a7497aff":"### 3. REGULARIZATION AND EVALUATION : LASSO REGRESSION","d20582b8":" ### COMPUTATION OF OUR ESTIMATED PAYMENT DATE ( We are using the '' model --- Better Accuracy as compared to other)","6d26d716":"#### LOG TRANSFORMATION OF 'total_open_amount'","db79a1c3":"#### Bar Plot Representing the Frequency Counts","deb778b2":"### Removing the insignificant Columns","09218d86":"### FEATURE SELECTION","f0205a26":"#### BOX PLOT ","43345f0a":"### 5.2. Using 'polynomial' kernel[](http:\/\/)","2b3a11c4":"### Label Encoding across categorical columns : [ 'business_code' ,'cust_number'  , 'cust_payment_terms']","4cb753c5":"### Sorting the dataframe according to the'baseline_create_date'","44bcf1cc":"### FILTERING OUT THE QUASI-CONSTANT FEATURES","fa619347":"#### REMOVING DUPLICATE VALUES FROM DATAFRAME ","c32bdb68":"#### Mean and Standard Deviation - interval level variables (total_open_amount)","ecc93279":"#### REMOVING 'NULL' VALUES FROM THE DATAFRAME","41053d73":"clear_date-due_in_date(Payment term) > 100\n\n\n15 > total_open_amount > 1 \n\n\n-45 < delay <50\n\n\ncustumer_num  >50","60a8aef9":"### FEATURE ENGINEERING \/ FEATURE GENERATION","fe0a2b82":"### DATA PRE-PROCESSING","40dca853":"#### Scatter Plot","33bc06f2":"###  Diffrent Features","17832260":"### COVARIANCE MATRIX","75107eaf":"### PIE PLOT","f8bcac87":"### 5.3. Using 'RBF' kernel","85b39ceb":"### EXPLORATORY  DATA   ANALYSIS","17349697":"#### Mode - Most Frequently occuring Element","3442c06e":"### GENERATING THE TEST-TRAIN-EVALUATION DATA","a684374f":"#### One-Hot Encoding the Categorical Data","38cef9e2":"### B2B- INVOICE PAYMENT DATE PREDICTION MODEL - BY MISHA DEY ","c1a98909":"#### Median - Ordinal Variables ( 'invoice_id' ,buisness_year, isOpen )","f2b87f2e":" #### BAR PLOT","8aa6a094":" ### 4. RANDOM FOREST REGRESSOR","54bdb498":"#### DISTRIBUTION PLOTS","ab57f2ae":"### 1. LINEAR REGRESSION","e3969a87":"#### Scatter Plot","daf414c0":"### Grouping on the basis of features: 'cust_number'and generating new features","891c4e27":"### 6. DECISION TREE REGRESSOR","3a2d9d6d":"### 2. REGULARIZATION AND EVALUATION : RIDGE REGRESSION","c6440318":"#### DATE-TIME CONNVERSION","faca2dd3":"### Comparision between METRICS","201620be":"### BUCKET COMPUTATION - > BASED ON DUE TERM, PAYMENT TERM , AND DELAY","ccc234c7":"## MODEL TRAINING , PREDICTION AND EVALUATION","8067f1a1":"### CORRELATION MATRIX","92b62629":"### Merging the Numerical, Categorical and Numerical-Categorical Data","6f479cd9":" ### OUTLIER DETECTION AND REMOVAL","74f3ffb1":"### FILTERING OUT THE CONSTANT FEATURES","991cbc0c":"### 5. SUPPORT VECTOR MACHINES: REGRESSION","f48b8f3d":" ### MANIPULATING THE 'AMOUNT COLUMN - CONVERTING TO USD\n##### CAD -> CANADIAN DOLLAR \n##### USD -> US DOLLAR \n\n##### Convertion:\n\n##### 1 CAD = 0.79 USD"}}