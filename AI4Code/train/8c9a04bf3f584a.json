{"cell_type":{"5b501dab":"code","c086f990":"code","ecfbaf3b":"code","da2b41e8":"code","f20ff108":"code","63db9f6d":"code","c4374e46":"code","e4fdaae4":"markdown","e099014a":"markdown","6ddcd16f":"markdown","1c6dbf9b":"markdown","a81f3b13":"markdown","056f0048":"markdown","bbc98b71":"markdown","19113483":"markdown"},"source":{"5b501dab":"import os\n\n# File locations\nJSON_DATA_DIR = '..\/input\/CORD-19-research-challenge\/'\nHOME = '\/kaggle\/working\/'\n\n#   JSON data\nJSON_DATA_DIRS = ['biorxiv_medrxiv\/'*2 + 'pdf_json\/', 'comm_use_subset\/'*2 + 'pdf_json\/']\nDICTS = HOME + 'dictionaries\/'\ntry:\n    os.mkdir(DICTS)\nexcept FileExistsError:\n    pass\n\n#   Dict data\nWORD_DATA_DIR = HOME + 'dictionaries\/'\nCORPUS_F = WORD_DATA_DIR + 'corpus.data'\n\n#   Graph data\nGRAPH_FILE = HOME + 'graph.npz'\nNODE_EMBEDDINGS = HOME + 'node_embeddings.model'\n\n# File naming functions\nF_TO_JSON = lambda x : (x.split('\/')[-1]).split('.')[0] + '.json'\nF_TO_DICT = lambda x : (x.split('\/')[-1]).split('.')[0] + '.data'\nF_TO_HASH = lambda x : F_TO_JSON(x).split('.')[0]\n\n# Lists of all data files\n#   JSON data\nJSON_FILES = []\nfor d in JSON_DATA_DIRS:\n    JSON_FILES += [JSON_DATA_DIR + d + f for f in os.listdir(JSON_DATA_DIR + d)]\n#   Dict data\nWORD_DATA_FILES = [WORD_DATA_DIR + F_TO_DICT(f) for f in JSON_FILES]\n\nNUM_DOCS = len(JSON_FILES)\n\nHASH_IDX = {F_TO_HASH(JSON_FILES[i]): i for i in range(len(JSON_FILES))}\n\n\n# Added some stopwords specific to journal papers\n# or that slipped through NLTK's default list    \nCUSTOM_STOPWORDS = [\n    \"n't\", \n    \"'m\", \n    \"'re\", \n    \"'s\", \n    \"nt\", \n    \"may\",\n    \"also\",\n    \"fig\",\n    \"http\"\n]","c086f990":"import os\nimport nltk\nimport json\nimport pickle\nimport random\n\nfrom tqdm.notebook import tqdm\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\nLEMMATIZER = WordNetLemmatizer()\nSTOPWORDS = set(stopwords.words('english') + CUSTOM_STOPWORDS)\n\ndef pipeline(text):\n    '''\n    Processes raw text to remove all stopwords and lemmatize\n    '''\n    t = nltk.word_tokenize(text)\n    t = [LEMMATIZER.lemmatize(w) for w in t]\n    t = [w for w in t if w not in STOPWORDS]\n    t = [w for w in t if len(w) > 1]\n    return t\n\n\ndef run(documents, save=True):\n    ''' \n    Given several documents, generates 1 dict of all words\n    used in the whole corpus, and a dict for each document.\n    The dicts map the word to its frequency\n    '''\n    progress = tqdm(total=NUM_DOCS, desc='JSON Files parsed:')\n    corpus = {}\n\n    for document in documents:\n        with open(document, 'r') as f:\n            schema = json.loads(f.read().lower())\n            paper_id = schema['paper_id']\n\n            text = schema['metadata']['title'] + ' '\n\n            '''\n            # Naive (?) approach: use paper text to build graphs\n            paragraphs = schema['body_text']\n            for p in paragraphs:\n                text += p['text']\n            '''\n            \n            '''\n            # Use only titles to build graph\n            for ref in schema['bib_entries'].values():\n                text += ref['title'] + ' '\n            '''\n              \n            # Use only abstract and title to build graph\n            for par in schema['abstract']:\n                text += par['text'] + ' '\n            \n\n        text = pipeline(text)\n        \n        doc_dict = {}\n        for word in text:\n            # Assume it's already accounted for in corpus\n            if word in doc_dict:\n                doc_dict[word] += 1\n                corpus[word]['count'] += 1\n\n            else:\n                doc_dict[word] = 1\n                \n                # Make sure to add this paper to the corpus to make building\n                # the graph eaiser later on\n                if word in corpus:\n                    corpus[word]['count'] += 1\n                    corpus[word]['papers'].add(paper_id)\n                else:\n                    corpus[word] = {'count': 1, 'papers': {paper_id}}\n\n        if save:\n            pickle.dump(\n                doc_dict, \n                open(DICTS+F_TO_DICT(document), 'wb+'), \n                protocol=pickle.HIGHEST_PROTOCOL\n            )\n        \n        progress.update()\n\n    if save:\n        pickle.dump(\n            corpus, \n            open(CORPUS_F, 'wb+'),\n            protocol=pickle.HIGHEST_PROTOCOL\n        )\n    \n    return corpus\n\ndef runall():\n    # I'm sure there's a smarter way to do this but who cares\n    run(JSON_FILES)\n\ndef test(num_docs=10):\n    test_docs = []\n    for i in range(num_docs):\n        test_docs.append(random.choice(JSON_FILES))\n\n    return run(test_docs, save=False)\n    \nrunall()","ecfbaf3b":"import os\nimport json \nimport pickle\nimport random\nimport numpy as np\n\nfrom scipy.sparse import csr_matrix, save_npz \nfrom tqdm.notebook import tqdm\nfrom math import log\n\nTF_IDF_THRESHOLD = 10\n\ndef tf_idf(tf, doc_count):\n    idf = log(NUM_DOCS\/doc_count)\n    return tf*idf\n\ndef build_graph(documents):\n    # Undirected, regular old graph\n    g = np.zeros((NUM_DOCS, NUM_DOCS))\n    corpus = pickle.load(open(CORPUS_F, 'rb'))\n    \n    # Represent graph as a sparse CSR matrix as row slices are important\n    # but most papers have very few neighbors\n    row = [0]\n    cols = []\n    data = []\n    \n    last_idx = 0\n    progress = tqdm(total=NUM_DOCS, desc='Number of nodes added:')\n    for node_id in range(len(documents)):\n        doc_dict = pickle.load(open(WORD_DATA_FILES[node_id], 'rb'))\n        col = []\n        cdata = []\n        \n        # Link with all papers that share significant words\n        for word, count in doc_dict.items():\n            thresh = tf_idf(count, len(corpus[word]['papers']))\n            \n            if thresh > TF_IDF_THRESHOLD:\n                for paper in corpus[word]['papers']:\n                    neigh_id = HASH_IDX[paper]\n                    \n                    # Prevent self-loops\n                    if neigh_id == node_id:\n                        continue\n                    \n                    # Edge weights are the sum of each tf-idf score of shared words\n                    # This is functionally equivilant to using a multi-graph\n                    # as later on, we do random walks based on these weights\n                    # so P(B|A) is the same in both cases\n                    if neigh_id in col:\n                        cdata[col.index(neigh_id)] += thresh\n                    else:\n                        col.append(neigh_id)\n                        cdata.append(thresh)\n        \n        # Update CSR Matrix stuff           \n        last_idx += len(col)     \n        row.append(last_idx)\n        cols += col\n        data += cdata\n        \n        progress.update()\n    \n    print(\"Building matrix from parsed data\")\n    g = csr_matrix((data, cols, row), shape=(len(documents), len(documents)))\n    save_npz(GRAPH_FILE, g)\n    return g\n    \ndef test(num_nodes=10):\n    docs = []\n    for i in range(num_nodes):\n        docs.append(random.choice(WORD_DATA_FILES))\n    \n    g = build_graph(docs)\n    return g\n    \ndef run():\n    return build_graph(WORD_DATA_FILES)\n\nif __name__ == '__main__':\n    run()","da2b41e8":"import random\nimport numpy as np\n\nfrom joblib import Parallel, delayed\nfrom tqdm.notebook import tqdm \nfrom gensim.models import Word2Vec\nfrom scipy.sparse import load_npz\n\n# Model parameters\nNUM_WALKS = 200\nWALK_LEN = 3\n\n# W2V params\nNUM_WORKERS = 4\nW2V_PARAMS = {\n    'size': 256,\n    'workers': NUM_WORKERS,\n    'sg': 1,\n}\n\ndef generate_walks(num_walks, walk_len, g, starter):\n    '''\n    Generate random walks on graph for use in skipgram\n    '''\n    \n    # Allow random walks to be generated in parallel given list of nodes\n    # for each worker thread to explore\n    walks = []\n    \n    # Can't do much about nodes that have no neighbors\n    if g[starter].data.shape[0] == 0:\n        return [[str(starter)]]\n    \n    for _ in range(num_walks):\n        walk = [str(starter)]\n        n = starter\n        \n        # Random walk with weights based on tf-idf score\n        for __ in range(walk_len):\n            # Pick a node weighted randomly from neighbors\n            # Stop walk if hit a dead end\n            if g[n].data.shape[0] == 0:\n                break\n            \n            next_node = random.choices(\n                g[n].indices,\n                weights=g[n].data\n            )[0]  \n            \n            walk.append(str(next_node))\n            n = next_node \n                \n        walks.append(walk)\n    \n    return walks\n\ndef generate_walks_parallel(g, walk_len, num_walks, workers=1):\n    '''\n    Distributes nodes needing embeddings across all CPUs \n    Because this is just many threads reading one datastructure this\n    is an embarrasingly parallel task\n    '''\n    flatten = lambda l : [item for sublist in l for item in sublist]     \n        \n    print('Executing tasks')\n    # Tell each worker to generate walks on a subset of\n    # nodes in the graph\n    walk_results = Parallel(n_jobs=workers, prefer='processes')(\n        delayed(generate_walks)(\n            num_walks, \n            walk_len,\n            g,\n            node\n        ) \n        for node in tqdm(range(NUM_DOCS), desc='Walks generated:')\n    )\n    \n    return flatten(walk_results)\n\n\ndef embed_walks(walks, params, fname):\n    '''\n    Sends walks to Word2Vec for embeddings\n    '''\n    print('Embedding walks...')\n    model = Word2Vec(walks, **params)\n    model.save(fname)\n    return model.wv.vectors\n\ndef load_embeddings(fname=NODE_EMBEDDINGS):\n    return Word2Vec.load(fname).wv.vectors\n\n\nfname = NODE_EMBEDDINGS\n\nprint('Loading graph')\ng = load_npz(GRAPH_FILE)\n\nprint('Generating walks')\nwalks = generate_walks_parallel(g, WALK_LEN, NUM_WALKS, workers=NUM_WORKERS)\n    \nprint('Embedding nodes')\nembed_walks(walks, W2V_PARAMS, fname)","f20ff108":"import pandas as pd\nMETA = JSON_DATA_DIR + 'metadata.csv'\ndf = pd.read_csv(META)","63db9f6d":"from gensim.models import Word2Vec\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.manifold import TSNE\n\nN = 25\n\ndef get_titles(idxs):\n    tl = []\n    al = []\n    \n    for i in idxs:\n        sha = F_TO_HASH(JSON_FILES[int(i)])\n        df_idx = df.index[df['sha'] == sha]\n        \n        # Sometimes not in the data for whatever reason\n        if df_idx.empty:\n            tl.append(\"UNK\")\n            al.append('')\n        else:\n            tl.append(df['title'][df_idx[0]])\n            abstract = df['abstract'][df_idx[0]]\n            \n            # Check for NaNs\n            if abstract == abstract:\n                al.append(abstract[:256])\n            else:\n                al.append('')\n    \n    return tl, al\n        \n\ndef get_labels(fname):\n    #print('Loading Embeddings')\n    model = Word2Vec.load(fname)\n    v = model.wv.vectors\n\n    titles, abstracts = get_titles(model.wv.index2word)\n    y = AgglomerativeClustering(n_clusters=N).fit(v).labels_\n    \n    simplest = TSNE(n_components=2, perplexity=40, n_iter=1500, learning_rate=300)\n    v = simplest.fit_transform(v)\n    \n    return v,y,titles,abstracts\n\nX,y,titles,abstracts = get_labels(NODE_EMBEDDINGS)\nprint('Embeddings ready')","c4374e46":"from bokeh.models import ColumnDataSource, HoverTool, LinearColorMapper\nfrom bokeh.palettes import Category20\nfrom bokeh.transform import linear_cmap\nfrom bokeh.io import show\nfrom bokeh.transform import transform\nfrom bokeh.io import output_notebook\nfrom bokeh.plotting import figure\n\noutput_notebook()\n\n# data sources\nsource = ColumnDataSource(data=dict(\n    x= X[:,0], \n    y= X[:,1],\n    desc= y, \n    titles= titles,\n    abstracts = abstracts\n    ))\n\nhover = HoverTool(tooltips=[\n    (\"Title\", \"@titles{safe}\"),\n    (\"Abstract\", \"@abstracts{safe}\"),], \n    point_policy=\"follow_mouse\")\n\nmapper = linear_cmap(field_name='desc', \n                     palette=Category20[20],\n                     low=min(y) ,high=max(y))\n\np = figure(plot_width=800, plot_height=800, \n           tools=[hover, 'pan', 'wheel_zoom', 'box_zoom', 'reset'], \n           title=\"t-SNE Covid-19 Articles, Clustered(node2vec), Tf-idf with Plain Text\", \n           toolbar_location=\"right\")\n\np.scatter('x', 'y', size=5, \n          source=source,\n          fill_color=mapper,\n          line_alpha=0.3,\n          line_color=\"black\"\n)\n\nshow(p)","e4fdaae4":"# Generating vectors\n### Building a Graph\nNice. It looks like the frequency dictionaries are done. Now we can use the frequency data we've collected to build a graph. Nothing fancy, just an undirected, homogeneous graph with weighted edges. Nodes are papers, and edges between them are shared words weighted by TF-IDF score. For simplicity, we merge edges together by summing their weights. To filter out any noise we missed when removing stop words, we don't consider edges with lower TF-IDF than some preset threshold. We have selected 10 for this threshold.","e099014a":"# Clustering \nFinally, we cluster the papers. We have selected 25 classes, but this is easilly changable. Visualizing the data is a little trickier, but we use t-SNE to attempt to make 2D visualization possible. Without classes, it is difficult to say for sure if this technique worked, but from the titles of the papers near each other, it appears the clustering works rather nicely","6ddcd16f":"### Generate Node Embeddings\nLast, we need to have some vectors to work with. In order to do this, we use the node2vec technique with the skip-gram model. However, we weight the random walks based on edge weights, so the walk is more likely to go to similar nodes. We use 200 walks of length 3 (the source node plus 3 more) to generate the \"sentences\" that we feed into Word2Vec. By using a random-walk approach, this is like an approximation of KNN clustering. Rather than checking the distance from all other vectors, we simply classify nodes by what nodes they are strongly connected to. ","1c6dbf9b":"# Pre-processing\n### Word Frequency Dictionaries\nNow we can get down to business. First we generate the dictionaries of words contained in the data. The pipeline for this process is pretty standard: tokenize, lemmatize, & remove stopwords. These dictionaries on their own may be enough to make some vectors an do KNN analysis clustering, but we're using a graph-based approach, so when we're done here we have one more step. ","a81f3b13":"# Housekeeping\nFirst set some filenames. It just makes the code look cleaner. Forgive me for being used to multi-file environments where you wouldn't have to see this mess.","056f0048":"First, a bit of housekeeping. We need to load the metadata so we can actually map the file hashes back to the titles of the papers","bbc98b71":"Next, we cluster using the AgglomerativeClustering method, out of the box. This appears to work well enough. We use this to predict 25 distinct clusters, then run the embeddings through t-SNE to better visualize the 256-dimensional vectors","19113483":"Finally, we're done. We build a graph using the method used by [COVID-19 Literature Clustering](https:\/\/www.kaggle.com\/maksimeren\/covid-19-literature-clustering). Though the techniques to generate these graphs are very different, the clusters look largely the same"}}