{"cell_type":{"25b071f6":"code","398cdf8d":"code","c714d503":"code","04a53407":"code","e700ca82":"code","6efb01be":"code","33453952":"code","c8b6abeb":"code","678c0b28":"code","e3108fdb":"code","719b6ba1":"code","f77a8973":"code","3acd38c8":"code","7451d07d":"code","b5004d5a":"code","d263fdd4":"code","26790e20":"code","4149c465":"code","bb8d292c":"code","f879f14a":"code","81fd774f":"code","f9f73cf5":"code","679b2cfb":"code","a2bb8a80":"code","9a44f1bc":"code","d6b37ffe":"code","6cf0099d":"code","497a7163":"code","3705288f":"code","3ef0868b":"code","7d6781d2":"code","66887f39":"code","a3ccfff5":"code","5b883ade":"code","80cf196e":"code","c08816af":"code","913a858d":"code","44d8d794":"code","431dfd96":"code","8149827e":"code","c44f3d54":"code","bc639684":"code","a719384f":"code","ebf82fde":"code","f276a5a3":"code","649e5473":"code","3950170b":"code","3e6915dc":"code","7b90860b":"code","a335ff8d":"code","a0c47b8e":"code","7b093933":"markdown","de226938":"markdown","a9dbebfa":"markdown","44ccb60e":"markdown","bbb4e409":"markdown","8a7de485":"markdown","7c06fb73":"markdown","c57f9268":"markdown"},"source":{"25b071f6":"# Importing Libaries\nimport requests\nfrom torch.backends import cudnn\nimport re\nimport spacy\nimport torch\nimport nltk\nfrom matplotlib import pyplot\nimport pandas as pd,numpy as np\nfrom torch.autograd import Variable","398cdf8d":"# Setting up device\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","c714d503":"device","04a53407":"# torch internal optimization\ncudnn.benchmark = True","e700ca82":"response = requests.get('http:\/\/www.gutenberg.org\/files\/11\/11-0.txt')\ntext = response.text","6efb01be":"text[:100]","33453952":"len(text)","c8b6abeb":"# Sentence Tokenizer\nsentModel = spacy.load(\"en_core_web_sm\",disable=['parser','tagger', 'ner'])\nsentModel.add_pipe(sentModel.create_pipe('sentencizer'))","678c0b28":"# Global cleaning of the text\ndef cleanText(text):\n    text = text.strip()\n    text = re.sub(r'\\n+',' ',text)\n    doc = sentModel(text)\n    #text = ' '.join([str(token.lemma_.strip().lower()) for token in sentModel(text) if token.lemma_.strip()])\n    #text = text.replace('-pron-','')\n    textList = [senttoken.text.lower() for senttoken in doc.sents]\n    return ' '.join(textList)","e3108fdb":"# All Sentensed Tokenized\ntextRefined,tokenList = [],[] \ntextRefinedTemp=cleanText(text)\ntextRefined = textRefinedTemp","719b6ba1":"textRefined[:100]","f77a8973":"# Importing Libraries\nimport torch\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch import nn","3acd38c8":"# Dataset Class Builder\nclass datasetClass(Dataset):\n    \n    def __init__(self,text):\n        super(datasetClass,self).__init__()\n        self.textList = text.split()\n        self.text,self.char2idx,self.idx2char,self.textSplit,self.textOriginal,self.trainX,self.trainY= '','','','','','',''\n        self.preprocessing()\n        self.encoding()\n        self.x,self.y= self.segmentation(self.text)\n        self.x,self.y = self.x.to(device),self.y.to(device)\n    \n    # Clean function\n    def clean(self,text):\n        #text = unidecode(text)\n        REPLACE_BY_SPACE_RE = re.compile('[\/(){}\\[\\]\\|@,;]')\n        BAD_SYMBOLS_RE = re.compile('[^0-9a-z +_]')\n        text = re.sub(' +',' ',text)\n        text = re.sub('\\n+',' ',text)\n        text = REPLACE_BY_SPACE_RE.sub(' ', text)\n        text = BAD_SYMBOLS_RE.sub('', text)\n        text = text.lower()\n        text = re.sub(' +',' ',text)\n        text = re.sub('\\n+',' ',text)\n        text.strip()\n        return text\n    \n    # Character Encoding \n    def encoding(self,):\n        self.idx2char = {idx:token for idx,token in enumerate(set(self.text.split()))}\n        self.char2idx = {token:idx for idx,token in enumerate(set(self.text.split()))}\n    \n\n    # TrainX , TrainY dataset preperation\n    def segmentation(self,text,ngram=10):\n        '''\n         Segemention for making windowed input of size (10) and output (1) , Total = 11\n         Please check the variables (self.trainX,self.trainY) for further analysis.\n        '''\n        self.textSplit = [self.char2idx[token] for token in text.split()]\n        self.textOriginal = [token for token in text.split()]\n        self.trainX,self.trainY = zip(*[((self.textOriginal[idx:idx+(ngram)]),self.textOriginal[idx+ngram]) for idx,token in enumerate(self.textSplit[:-ngram])])\n        x,y = zip(*[((self.textSplit[idx:idx+(ngram)]),self.textSplit[idx+ngram]) for idx,token in enumerate(self.textSplit[:-ngram])])\n        x,y = torch.tensor(x,dtype=torch.long),torch.tensor(y,dtype=torch.long)\n        return x,y\n    \n    # Cleaning for sentences\n    def preprocessing(self,):\n        self.text = ' '.join([self.clean(itemStr) for itemStr in self.textList])\n     \n    def  __getitem__(self,index):\n        return self.x[index],self.y[index]\n    \n    def __len__(self,):\n        return len(self.y)","7451d07d":"# Model Building \nclass Model(nn.Module):\n    def __init__(self,vocabSize,embedDim,targetDim,sentenceLength):\n        super(Model,self).__init__()\n        self.sentenceLength = sentenceLength\n        self.embeddingLayer1 = nn.Embedding(vocabSize,embedDim)\n        self.lstmLayer1 = nn.LSTM(embedDim,100,num_layers=1)\n        self.linearLayer1 = nn.Linear(100*self.sentenceLength,targetDim)\n        self.logsoftmaxLayer1 = nn.LogSoftmax(dim=1)\n        self.hidden = Variable(torch.zeros(1,self.sentenceLength,100,requires_grad=True)).to(device)\n        self.cell = Variable(torch.zeros(1,self.sentenceLength,100,requires_grad=True)).to(device)\n        \n        \n    def forward(self,x,hidden,cell):\n        x = self.embeddingLayer1(x)\n        x, (self.hidden, self.cell) = self.lstmLayer1(x,(self.hidden,self.cell))\n        x = x.view(-1,self.sentenceLength*100)\n        x = self.linearLayer1(x)\n        x = self.logsoftmaxLayer1(x)\n        return x,(self.hidden,self.cell)","b5004d5a":"def reinitialize(hidden,state,sentence_length):\n    '''\n    Reinitializing the layers hidden,state\n    This will be used to carry on the hidden\/state till certain epochs.\n    '''\n    #print(hidden.shape,state.shape)\n    hidden = Variable(torch.zeros(1,sentence_length,100,requires_grad=True)).to(device)\n    state = Variable(torch.zeros(1,sentence_length,100,requires_grad=True)).to(device)\n    return hidden,state","d263fdd4":"traintext = textRefined[:int(0.8*len(textRefined))]","26790e20":"testtext = textRefined[int(0.8*len(textRefined)):]","4149c465":"datasetClassObj = datasetClass(textRefined)","bb8d292c":"datasetClassTestObj = datasetClass(testtext)","f879f14a":"print('X1 : ',datasetClassObj.trainX[0],'Y1 : ',datasetClassObj.trainY[0])\nprint('X2 : ',datasetClassObj.trainX[1],'Y2 : ',datasetClassObj.trainY[1])\nprint('X3 : ',datasetClassObj.trainX[2],'Y3 : ',datasetClassObj.trainY[2])","81fd774f":"print('Length of the sentence : ',len(datasetClassObj.trainX[0]))","f9f73cf5":"# Model Configs\nvocabSize = len(datasetClassObj.idx2char)\ntargetDim = len(datasetClassObj.char2idx)\nsentenceLength = 10\nembedDim = 10\nepochs = 100\nbs = 64","679b2cfb":"dataloaderClassObj = DataLoader(datasetClassObj,batch_size=bs,shuffle=False)","a2bb8a80":"dataloaderClassTestObj = DataLoader(datasetClassTestObj,batch_size=bs,shuffle=False)","9a44f1bc":"# Length of the vocab\nprint(len(datasetClassObj.char2idx))\nprint(len(datasetClassTestObj.char2idx))","d6b37ffe":"datasetClassObj.x,datasetClassObj.y","6cf0099d":"modelObj = Model(vocabSize,embedDim,targetDim,sentenceLength)","497a7163":"modelObj.to(device)","3705288f":"x,y = next(iter(dataloaderClassObj))","3ef0868b":"# Forward Pass\nhidden,cell = reinitialize(hidden=0,state=0,sentence_length=10)\n_,(_,_) = modelObj(x,hidden,cell)","7d6781d2":"# Loss Function and Optimizer\nloss_function = nn.NLLLoss()\noptimizer = torch.optim.Adam(modelObj.parameters(), lr=0.001)","66887f39":"# To detect anomaly for gradient error detection \ntorch.autograd.set_detect_anomaly(True)","a3ccfff5":"# Statefull LSTM\nmodelObj.hidden,modelObj.cell = reinitialize(hidden=0,state=0,sentence_length=10)\nlosses = []\nfor epoch in range(epochs):\n    modelObj.train()\n    for x,y in dataloaderClassObj:\n        modelObj.hidden.detach_()\n        modelObj.cell.detach_()\n        ypred,(modelObj.hidden,modelObj.cell) = modelObj(x,modelObj.hidden,modelObj.cell)\n        loss = loss_function(ypred,y)\n        loss.backward()\n        with torch.no_grad():\n            optimizer.step()\n            modelObj.zero_grad()\n    print('epoch',epoch,loss)\n    losses.append(loss)","5b883ade":"# Training Loss\nlossesCpu = [float(item.to('cpu')) for item in losses]","80cf196e":"# Training Loss Graph\npyplot.plot(lossesCpu)","c08816af":"# Fitment Test\nhidden,cell = reinitialize(hidden=0,state=0,sentence_length=10)\npercentage = []\nfor x,y in dataloaderClassObj:\n    modelObj.eval()\n    ypred,(_,_) = modelObj(x,hidden,cell)\n    predictedWord = [datasetClassObj.idx2char[int(item)] for item in torch.argmax(ypred,dim=1)]\n    actualWord = [datasetClassObj.idx2char[int(item)] for item in y]\n    percentage.append(sum([actualWord[index]== predictedWord[index] for index,word in enumerate(actualWord)])\/len(actualWord)*100)","913a858d":"pyplot.plot(percentage)","44d8d794":"# Unit Test with sample of length 10(mandatory)\n# For best prediction the input should be in a batch of specific batchsize \n\nmodelObj.eval()\ninpTest =  ['the','gutenberg', 'ebook', 'of', 'alices', 'adventures', 'in', 'wonderland', 'by', 'lewis']","431dfd96":"# To get the words in the vocab \n#datasetClassObj.char2idx","8149827e":"hidden,cell = reinitialize(hidden=0,state=0,sentence_length=10)\nmodelObj.eval()\nxValid = torch.tensor([datasetClassObj.char2idx[token] for token in inpTest],dtype=torch.long).to(device)\n\nprint('Convert Text to Index : ', xValid)\n\nypredValid,(_,_) = modelObj(xValid.unsqueeze(0),hidden,cell)\nypredValid = torch.argmax(ypredValid,dim=1)\n\nprint('Prediction : ', ypredValid,'\\n\\n')\n\nprint('Input in Text : ',inpTest)\nprint('Prediction in Text : ', datasetClassObj.idx2char[int(ypredValid.to('cpu'))])","c44f3d54":"PATH = r'\/kaggle\/working\/SingleWordPredictor'\n\ntorch.save({\n            'epochs': epochs,\n            'model_state_dict': modelObj.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'loss': loss,\n            }, PATH)","bc639684":"len(list(datasetClassObj.char2idx.keys()))","a719384f":"wordEmbedding = {}","ebf82fde":"# Embeddings\nmodelObj.eval()\ninpTest = list(datasetClassObj.char2idx.keys())\nxTest = torch.tensor([datasetClassObj.char2idx[token] for token in inpTest],dtype=torch.long).to(device)","f276a5a3":"embeddingLookupFrame = pd.DataFrame(modelObj.embeddingLayer1(xTest).to('cpu').detach().numpy(),index=inpTest)","649e5473":"embeddingLookupFrame.head(10)","3950170b":"embeddingLookupFrame.loc['impossible'].values","3e6915dc":"searchWord = embeddingLookupFrame.loc['son'].values","7b90860b":"def distance(row,searchWord):\n    return  np.linalg.norm(row.values-searchWord,ord=2)","a335ff8d":"searchSeries = embeddingLookupFrame.apply(distance,args=(searchWord,),axis=1)","a0c47b8e":"searchSeries.sort_values()[:10]","7b093933":"## Word Embeddings","de226938":"# Next Steps :\n\n1. Training and Testing with Actual Bigger corpus.\n2. Using the embeddings, as these embeddings are context based.\n","a9dbebfa":"## About This Notebook\n\n* This is an experiment to explore the **B**ack **P**ropagation **T**hrough **T**ime Concept using Stateful LSTM.\n* The hidden and cell state are carried from One batch to the another.\n* This provides a useful connection between batches.\n* The trained word embedding can be used for future work.\n\n![image.png](attachment:image.png)","44ccb60e":"* ## Case 1 : Lets build single word predictor","bbb4e409":"# # Use Case\n* Get the next word in a sequence ('*the project gutenberg ebook of alices adventures in wonderland by lewis*') : \n    \n       Input :  the project gutenberg ebook of alices adventures in wonderland by\n       Output : lewis","8a7de485":"## Credits :\nGutenberg Corpus : 'http:\/\/www.gutenberg.org\/files\/11\/11-0.txt","7c06fb73":"## Getting the Text","c57f9268":"## Cleaning the Text"}}