{"cell_type":{"562f97f7":"code","a197ebf2":"code","08f652e2":"code","865b174a":"code","e63c719c":"code","c18c6396":"code","4986d76a":"code","27c44d71":"code","e5367e31":"code","af3f13f1":"code","bea54442":"code","192bf17d":"code","9486ed4c":"code","2dff845d":"code","2b923b40":"code","a60fa5d8":"markdown","8c43d402":"markdown","55230438":"markdown","f68d7d11":"markdown","66768f83":"markdown","24bc2b8c":"markdown","a257e6a5":"markdown","02d2900e":"markdown","0698b47f":"markdown","2f39aaa9":"markdown","75582a4b":"markdown","66edc0db":"markdown","2e8e4952":"markdown"},"source":{"562f97f7":"import collections\nimport helper\nimport numpy as np\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Model\nfrom keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional\nfrom keras.layers.embeddings import Embedding\nfrom keras.optimizers import Adam\nfrom keras.losses import sparse_categorical_crossentropy","a197ebf2":"import os\n\ndef load_data(path):\n    \"\"\"\n    Load dataset\n    \"\"\"\n    input_file = os.path.join(path)\n    with open(input_file, \"r\") as f:\n        data = f.read()\n\n    return data.split('\\n')\n\nimport numpy as np\nfrom keras.losses import sparse_categorical_crossentropy\nfrom keras.models import Sequential\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import to_categorical\n\n\ndef _test_model(model, input_shape, output_sequence_length, french_vocab_size):\n    if isinstance(model, Sequential):\n        model = model.model\n\n    assert model.input_shape == (None, *input_shape[1:]),'Wrong input shape. Found input shape {} using parameter input_shape={}'.format(model.input_shape, input_shape)\n\n    assert model.output_shape == (None, output_sequence_length, french_vocab_size),'Wrong output shape. Found output shape {} using parameters output_sequence_length={} and french_vocab_size={}'.format(model.output_shape, output_sequence_length, french_vocab_size)\n\n    assert len(model.loss_functions) > 0,'No loss function set.  Apply the `compile` function to the model.'\n\n    assert sparse_categorical_crossentropy in model.loss_functions,'Not using `sparse_categorical_crossentropy` function for loss.'\n\n\ndef test_tokenize(tokenize):\n    sentences = [\n        'The quick brown fox jumps over the lazy dog .',\n        'By Jove , my quick study of lexicography won a prize .',\n        'This is a short sentence .']\n    tokenized_sentences, tokenizer = tokenize(sentences)\n    assert tokenized_sentences == tokenizer.texts_to_sequences(sentences),\\\n        'Tokenizer returned and doesn\\'t generate the same sentences as the tokenized sentences returned. '\n\n\ndef test_pad(pad):\n    tokens = [\n        [i for i in range(4)],\n        [i for i in range(6)],\n        [i for i in range(3)]]\n    padded_tokens = pad(tokens)\n    padding_id = padded_tokens[0][-1]\n    true_padded_tokens = np.array([\n        [i for i in range(4)] + [padding_id]*2,\n        [i for i in range(6)],\n        [i for i in range(3)] + [padding_id]*3])\n    assert isinstance(padded_tokens, np.ndarray),\\\n        'Pad returned the wrong type.  Found {} type, expected numpy array type.'\n    assert np.all(padded_tokens == true_padded_tokens), 'Pad returned the wrong results.'\n\n    padded_tokens_using_length = pad(tokens, 9)\n    assert np.all(padded_tokens_using_length == np.concatenate((true_padded_tokens, np.full((3, 3), padding_id)), axis=1)),\\\n        'Using length argument return incorrect results'\n\n\ndef test_simple_model(simple_model):\n    input_shape = (137861, 21, 1)\n    output_sequence_length = 21\n    english_vocab_size = 199\n    french_vocab_size = 344\n\n    model = simple_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size)\n    _test_model(model, input_shape, output_sequence_length, french_vocab_size)\n\n\ndef test_embed_model(embed_model):\n    input_shape = (137861, 21)\n    output_sequence_length = 21\n    english_vocab_size = 199\n    french_vocab_size = 344\n\n    model = embed_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size)\n    _test_model(model, input_shape, output_sequence_length, french_vocab_size)\n\n\ndef test_encdec_model(encdec_model):\n    input_shape = (137861, 15, 1)\n    output_sequence_length = 21\n    english_vocab_size = 199\n    french_vocab_size = 344\n\n    model = encdec_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size)\n    _test_model(model, input_shape, output_sequence_length, french_vocab_size)\n\n\ndef test_bd_model(bd_model):\n    input_shape = (137861, 21, 1)\n    output_sequence_length = 21\n    english_vocab_size = 199\n    french_vocab_size = 344\n\n    model = bd_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size)\n    _test_model(model, input_shape, output_sequence_length, french_vocab_size)\n\n\ndef test_model_final(model_final):\n    input_shape = (137861, 15)\n    output_sequence_length = 21\n    english_vocab_size = 199\n    french_vocab_size = 344\n\n    model = model_final(input_shape, output_sequence_length, english_vocab_size, french_vocab_size)\n    _test_model(model, input_shape, output_sequence_length, french_vocab_size)","08f652e2":"english_sentences = load_data('..\/input\/ai-project\/english_sentences.txt')\nfrench_sentences = load_data('..\/input\/ai-project\/french_sentences.txt')\nprint('Dataset Loaded')","865b174a":"english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()])\nfrench_words_counter = collections.Counter([word for sentence in french_sentences for word in sentence.split()])\nprint('{} English words.'.format(len([word for sentence in english_sentences for word in sentence.split()])))\nprint('{} unique English words.'.format(len(english_words_counter)))\nprint('10 Most common words in the English dataset:')\nprint('\"' + '\" \"'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '\"')\nprint()\nprint('{} French words.'.format(len([word for sentence in french_sentences for word in sentence.split()])))\nprint('{} unique French words.'.format(len(french_words_counter)))\nprint('10 Most common words in the French dataset:')\nprint('\"' + '\" \"'.join(list(zip(*french_words_counter.most_common(10)))[0]) + '\"')","e63c719c":"def tokenize(x):\n    x_tk = Tokenizer(char_level = False)\n    x_tk.fit_on_texts(x)\n    return x_tk.texts_to_sequences(x), x_tk\ntext_sentences = [\n    'The quick brown fox jumps over the lazy dog .',\n    'By Jove , my quick study of lexicography won a prize .',\n    'This is a short sentence .']\ntext_tokenized, text_tokenizer = tokenize(text_sentences)\nprint(text_tokenizer.word_index)\nprint()\nfor sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n    print('Sequence {} in x'.format(sample_i + 1))\n    print('  Input:  {}'.format(sent))\n    print('  Output: {}'.format(token_sent))","c18c6396":"# import project_tests as tests\ndef pad(x, length=None):\n    if length is None:\n        length = max([len(sentence) for sentence in x])\n    return pad_sequences(x, maxlen = length, padding = 'post')\ntest_pad(pad)\n# Pad Tokenized output\ntest_pad = pad(text_tokenized)\nfor sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n    print('Sequence {} in x'.format(sample_i + 1))\n    print('  Input:  {}'.format(np.array(token_sent)))\n    print('  Output: {}'.format(pad_sent))","4986d76a":"def preprocess(x, y):\n    preprocess_x, x_tk = tokenize(x)\n    preprocess_y, y_tk = tokenize(y)\n    preprocess_x = pad(preprocess_x)\n    preprocess_y = pad(preprocess_y)\n    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n    return preprocess_x, preprocess_y, x_tk, y_tk\n\npreproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer = preprocess(english_sentences, french_sentences)    \nmax_english_sequence_length = preproc_english_sentences.shape[1]\nmax_french_sequence_length = preproc_french_sentences.shape[1]\nenglish_vocab_size = len(english_tokenizer.word_index)\nfrench_vocab_size = len(french_tokenizer.word_index)\nprint('Data Preprocessed')\nprint(\"Max English sentence length:\", max_english_sequence_length)\nprint(\"Max French sentence length:\", max_french_sequence_length)\nprint(\"English vocabulary size:\", english_vocab_size)\nprint(\"French vocabulary size:\", french_vocab_size)","27c44d71":"def logits_to_text(logits, tokenizer):\n    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n    index_to_words[0] = '<PAD>'\n    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\nprint('`logits_to_text` function loaded.')","e5367e31":"def simple_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n    learning_rate = 1e-3\n    input_seq = Input(input_shape[1:])\n    rnn = GRU(64, return_sequences = True)(input_seq)\n    logits = TimeDistributed(Dense(french_vocab_size))(rnn)\n    model = Model(input_seq, Activation('softmax')(logits))\n    model.compile(loss = sparse_categorical_crossentropy, \n                 optimizer = Adam(learning_rate), \n                 metrics = ['accuracy'])\n    \n    return model\n# test_simple_model(simple_model)\ntmp_x = pad(preproc_english_sentences, max_french_sequence_length)\ntmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n# Train the neural network\nsimple_rnn_model = simple_model(\n    tmp_x.shape,\n    max_french_sequence_length,\n    english_vocab_size+1,\n    french_vocab_size+1)\nsimple_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)\n# Print prediction(s)\nprint(logits_to_text(simple_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))","af3f13f1":"from keras.models import Sequential\ndef embed_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n    learning_rate = 1e-3\n    rnn = GRU(64, return_sequences=True, activation=\"tanh\")\n    \n    embedding = Embedding(french_vocab_size, 64, input_length=input_shape[1]) \n    logits = TimeDistributed(Dense(french_vocab_size, activation=\"softmax\"))\n    \n    model = Sequential()\n    #em can only be used in first layer --> Keras Documentation\n    model.add(embedding)\n    model.add(rnn)\n    model.add(logits)\n    model.compile(loss=sparse_categorical_crossentropy,\n                  optimizer=Adam(learning_rate),\n                  metrics=['accuracy'])\n    \n    return model\n# tests.test_embed_model(embed_model)\ntmp_x = pad(preproc_english_sentences, max_french_sequence_length)\ntmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2]))\nembeded_model = embed_model(\n    tmp_x.shape,\n    max_french_sequence_length,\n    english_vocab_size+1,\n    french_vocab_size+1)\nembeded_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)\nprint(logits_to_text(embeded_model.predict(tmp_x[:1])[0], french_tokenizer))","bea54442":"def bd_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n   \n    learning_rate = 1e-3\n    model = Sequential()\n    model.add(Bidirectional(GRU(128, return_sequences = True, dropout = 0.1), \n                           input_shape = input_shape[1:]))\n    model.add(TimeDistributed(Dense(french_vocab_size, activation = 'softmax')))\n    model.compile(loss = sparse_categorical_crossentropy, \n                 optimizer = Adam(learning_rate), \n                 metrics = ['accuracy'])\n    return model\n# tests.test_bd_model(bd_model)\ntmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\ntmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\nbidi_model = bd_model(\n    tmp_x.shape,\n    preproc_french_sentences.shape[1],\n    len(english_tokenizer.word_index)+1,\n    len(french_tokenizer.word_index)+1)\nbidi_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=20, validation_split=0.2)\n# Print prediction(s)\nprint(logits_to_text(bidi_model.predict(tmp_x[:1])[0], french_tokenizer))","192bf17d":"def encdec_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n  \n    learning_rate = 1e-3\n    model = Sequential()\n    model.add(GRU(128, input_shape = input_shape[1:], return_sequences = False))\n    model.add(RepeatVector(output_sequence_length))\n    model.add(GRU(128, return_sequences = True))\n    model.add(TimeDistributed(Dense(french_vocab_size, activation = 'softmax')))\n    \n    model.compile(loss = sparse_categorical_crossentropy, \n                 optimizer = Adam(learning_rate), \n                 metrics = ['accuracy'])\n    return model\n# tests.test_encdec_model(encdec_model)\ntmp_x = pad(preproc_english_sentences)\ntmp_x = tmp_x.reshape((-1, preproc_english_sentences.shape[1], 1))\nencodeco_model = encdec_model(\n    tmp_x.shape,\n    preproc_french_sentences.shape[1],\n    len(english_tokenizer.word_index)+1,\n    len(french_tokenizer.word_index)+1)\nencodeco_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=20, validation_split=0.2)\nprint(logits_to_text(encodeco_model.predict(tmp_x[:1])[0], french_tokenizer))\n","9486ed4c":"def model_final(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n  \n    model = Sequential()\n    model.add(Embedding(input_dim=english_vocab_size,output_dim=128,input_length=input_shape[1]))\n    model.add(Bidirectional(GRU(256,return_sequences=False)))\n    model.add(RepeatVector(output_sequence_length))\n    model.add(Bidirectional(GRU(256,return_sequences=True)))\n    model.add(TimeDistributed(Dense(french_vocab_size,activation='softmax')))\n    learning_rate = 0.005\n    \n    model.compile(loss = sparse_categorical_crossentropy, \n                 optimizer = Adam(learning_rate), \n                 metrics = ['accuracy'])\n    \n    return model\n# tests.test_model_final(model_final)\nprint('Final Model Loaded')","2dff845d":"tmp_X = pad(preproc_english_sentences)\nmodel = model_final(tmp_X.shape,preproc_french_sentences.shape[1],len(english_tokenizer.word_index)+1,len(french_tokenizer.word_index)+1)\nmodel.fit(tmp_X, preproc_french_sentences, batch_size = 1024, epochs = 20, validation_split = 0.2)\nmodel.save('Finalmodel.model')","2b923b40":"def final_predictions(x, y, x_tk, y_tk):\n    y_id_to_word = {value: key for key, value in y_tk.word_index.items()}\n    y_id_to_word[0] = '<PAD>'\n    sentence = 'he saw a old yellow truck'\n    sentence = [x_tk.word_index[word] for word in sentence.split()]\n    sentence = pad_sequences([sentence], maxlen=x.shape[-1], padding='post')\n    sentences = np.array([sentence[0], x[0]])\n    predictions = model.predict(sentences, len(sentences))\n    print('Sample 1:')\n    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[0]]))\n    print('Il a vu un vieux camion jaune')\n    print('Sample 2:')\n    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[1]]))\n    print(' '.join([y_id_to_word[np.max(x)] for x in y[0]]))\nfinal_predictions(preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer)","a60fa5d8":"## **Training Models** ","8c43d402":"## **Analysing and Preprocessing the Vocab of the dataset**. ","55230438":"#### Custom Model (Embedded + Bidirectional)","f68d7d11":"#### **Tokenizing**","66768f83":"#### **Bidirectional RNN**","24bc2b8c":"#### **Padding**","a257e6a5":"## **Loading the Dataset**","02d2900e":"#### **Simple Model**","0698b47f":"#### RNN with Embedding","2f39aaa9":"#### **Converting the Ids back to text**","75582a4b":"#### Encoder-Decoder RNN","66edc0db":"## **Helper function to load dataset**","2e8e4952":"#### **Preprocess Pipe-line**"}}