{"cell_type":{"707def84":"code","0024dbf3":"code","6324c8f2":"code","44ebbbe5":"code","8a0f023c":"code","e6a7e281":"code","3ad00cf3":"code","a398c4f5":"code","78c90e24":"code","f40cf3d4":"code","250ae853":"code","47eaa926":"code","896f4037":"code","b80cab8a":"code","8f762dcd":"code","9501fb61":"code","d6e4c82a":"code","53e25be5":"code","1fb0a61a":"code","2c7f9425":"code","59dd8f2e":"code","cf441d53":"code","1acbea55":"code","1fde6857":"code","85418da1":"code","d1c76ec4":"code","c9d93445":"code","49eb1d53":"code","11be00e8":"code","7b65dec7":"code","90ab72c8":"code","60e4e135":"code","16365acb":"code","1f214d8d":"code","aae61a01":"code","f8833fff":"code","69a1db27":"code","c41e0ed3":"code","d8952d68":"code","5422ce74":"code","0732c73c":"code","2db7fa44":"code","fc93ba89":"code","aaae0412":"code","310937f3":"code","05367f77":"code","8049e222":"code","d3d34f20":"code","d3c75f8e":"code","cbbcfba8":"code","707083ba":"code","636ec726":"code","459f573f":"code","2aae8d56":"code","816c999a":"code","555de996":"code","4c227d48":"code","c9b368f9":"code","25588bb0":"code","52086555":"code","e4e37d5e":"code","e4d57f88":"code","4c83cd09":"code","1e3943bf":"code","ad0b7471":"code","4ea9ad54":"code","49d3cbc9":"code","544871da":"code","6efc342a":"code","d56d471b":"code","7ea3776a":"code","77145a82":"code","3928a43c":"code","1ce5e3cd":"code","cedca478":"code","ae23e246":"code","5a2becf8":"code","6883663b":"code","049cb420":"code","1b42f4d9":"code","b44ca7a0":"code","4fa49a83":"code","51c74859":"markdown","2aa6a745":"markdown","e898e6de":"markdown","563538fc":"markdown","e2ec65cb":"markdown","bdf7cb46":"markdown","55a05bbc":"markdown","91f6a5f4":"markdown","9efd805e":"markdown","20e57bc4":"markdown","06843607":"markdown","3f0e4104":"markdown","36f02c89":"markdown","ac133ab6":"markdown","0d50f1cc":"markdown","05ccd69e":"markdown","6cf9e5d1":"markdown","a95b6426":"markdown","32d08bf8":"markdown","0f10381a":"markdown","4f9fea24":"markdown","f2981adf":"markdown","968a191d":"markdown","e0a50f21":"markdown","42ea1dea":"markdown","03a45ec2":"markdown","53a09667":"markdown","fd814294":"markdown","33d5b492":"markdown","377f2c65":"markdown","dfe780bf":"markdown","b25f783a":"markdown","9126af46":"markdown","44799fde":"markdown","3bcf80ed":"markdown","140c0f2b":"markdown","5b0339c4":"markdown","108c309f":"markdown","7ca89d9e":"markdown","5f050acf":"markdown","5835e234":"markdown","a8b36a60":"markdown","e5cab7fc":"markdown","9dd34b1a":"markdown","4588aecb":"markdown","1a7bde9b":"markdown","68d7de90":"markdown","b06e7ea1":"markdown","bf70ac35":"markdown","0c277f32":"markdown"},"source":{"707def84":"import numpy as np\r\nimport matplotlib.pyplot as plt\r\n%matplotlib inline\r\n\r\nnp.random.seed(0)\r\n#y = 4X + 6\uc744 \uadfc\uc0ac(w1 = 4, w0 = 6). \uc784\uc758\uc758 \uac12\uc740 \ub178\uc774\uc988\ub97c \uc704\ud574 \ub9cc\ub4e6\r\nX =  2*np.random.rand(100, 1)\r\ny = 6 + 4*X + np.random.randn(100, 1)\r\n\r\n# X, y \ub370\uc774\ud130 \uc138\ud2b8 \uc0b0\uc810\ub3c4\ub85c \uc2dc\uac01\ud654\r\nplt.scatter(X, y)","0024dbf3":"#\ube44\uc6a9 \ud568\uc218\ub97c \uc815\uc758 \r\ndef get_cost(y, y_pred):\r\n    N = len(y)\r\n    cost = np.sum(np.square(y - y_pred))\/N\r\n    return cost","6324c8f2":"#w1\uacfc w0\ub97c \uc5c5\ub370\uc774\ud2b8\ud560 w1_update, w0_update\ub97c \ubc18\ud658\r\ndef get_weight_updates(w1, w0, X, y, learning_rate = 0.01):\r\n    N = len(y)\r\n    #\uba3c\uc800 w1_update, w0_update\ub97c \uac01\uac01 w1, w0\uc758 shape\uc640 \ub3d9\uc77c\ud55c \ud06c\uae30\ub97c \uac00\uc9c4 0 \uac12\uc73c\ub85c \ucd08\uae30\ud654\r\n    w1_update = np.zeros_like(w1)\r\n    w0_update = np.zeros_like(w0)\r\n    #\uc608\uce21 \ubc30\uc5f4 \uacc4\uc0b0\ud558\uace0 \uc608\uce21\uacfc \uc2e4\uc81c \uac12\uc758 \ucc28\uc774 \uacc4\uc0b0\r\n    y_pred = np.dot(X, w1.T) + w0\r\n    diff = y - y_pred\r\n\r\n    # w0_update\ub97c dot \ud589\ub82c \uc5f0\uc0b0\uc73c\ub85c \uad6c\ud558\uae30 \uc704\ud574 \ubaa8\ub450 1\uac12\uc744 \uac00\uc9c4 \ud589\ub82c \uc0dd\uc131\r\n    w0_factors = np.ones((N, 1))\r\n\r\n    # w1\uacfc w0\uc744 \uc5c5\ub370\uc774\ud2b8\ud558 w1_update\uc640 w0_update \uacc4\uc0b0\r\n    w1_update = -(2\/N)*learning_rate*(np.dot(X.T, diff))\r\n    w0_update = -(2\/N)*learning_rate*(np.dot(w0_factors.T, diff))\r\n\r\n    return w1_update, w0_update","44ebbbe5":"#gradient_descent_steps() : get_weight_updates()\ub97c \uacbd\uc0ac \ud558\uac15 \ubc29\uc2dd\uc73c\ub85c \ubc18\ubcf5\uc801\uc73c\ub85c \uc218\ud589\ud558\uc5ec w1\uacfc w0\uc744 \uc5c5\ub370\uc774\ud2b8\ud558\ub294 \ud568\uc218 \r\ndef gradient_descent_steps(X, y, iters = 10000):\r\n    #w0\uc640 w1\uc744 \ubaa8\ub450 0\uc73c\ub85c \ucd08\uae30\ud654\r\n    w0 = np.zeros((1, 1))\r\n    w1 = np.zeros((1, 1))\r\n\r\n    #\uc778\uc790\ub85c \uc8fc\uc5b4\uc9c4 iters \ub9cc\ud07c \ubc18\ubcf5\uc801\uc73c\ub85c get_weight_updates() \ud638\ucd9c\ud574 w1, w0 \uc5c5\ub370\uc774\ud130 \uc218\ud589\r\n    for ind in range(iters):\r\n        w1_update, w0_update = get_weight_updates(w1, w0, X, y, learning_rate= 0.01)\r\n        w1 = w1 - w1_update\r\n        w0 = w0 - w0_update\r\n\r\n    return w1, w0","8a0f023c":"def get_cost(y, y_pred):\r\n    N = len(y)\r\n    cost = np.sum(np.square(y - y_pred))\/N\r\n    return cost\r\n\r\nw1, w0 = gradient_descent_steps(X, y, iters= 1000)\r\nprint('w1:{0:.3f} w0:{1:.3f}'.format(w1[0, 0], w0[0, 0]))\r\ny_pred = w1[0, 0]*X + w0\r\nprint('Gradient Descent Total Cost: {0:.4f}'.format(get_cost(y, y_pred)))","e6a7e281":"plt.scatter(X, y)\r\nplt.plot(X, y_pred)","3ad00cf3":"#\ud655\ub960\uc801 \uacbd\uc0ac \ud558\uac15\ubc95 : \uc77c\ubd80 \ub370\uc774\ud130\ub9cc\uc744 \uc774\uc6a9\ud574 w\uac00 \uc5c5\ub370\uc774\ud2b8\ub418\ub294 \uac12\uc744 \uacc4\uc0b0 -> \uacbd\uc0ac \ud558\uac15\ubc95\ubcf4\ub2e4 \ube60\ub978 \uc18d\ub3c4\ub97c \ubcf4\uc7a5\r\ndef stochastic_gradient_descent_steps(X, y, batch_size = 10, iters = 1000):\r\n    w0 = np.zeros((1, 1))\r\n    w1 = np.zeros((1, 1))\r\n    prev_cost = 100000\r\n    iter_index = 0\r\n\r\n    for ind in range(iters):\r\n        np.random.seed(ind)\r\n        #\uc804\uccb4 X, y \ub370\uc774\ud130\uc5d0\uc11c \ub79c\ub364\ud558\uac8c batch_size\ub9cc\ud07c \ub370\uc774\ud130\ub97c \ucd94\ucd9c\ud574 sample_X, sample_y\ub85c \uc800\uc7a5\r\n        stochastic_random_index = np.random.permutation(X.shape[0])\r\n        sample_X = X[stochastic_random_index[0:batch_size]]\r\n        sample_y = y[stochastic_random_index[0:batch_size]]\r\n        #\ub79c\ub364\ud558\uac8c batch_size\ub9cc\ud07c \ucd94\ucd9c\ub41c \ub370\uc774\ud130 \uae30\ubc18\uc73c\ub85c w1_update, w0_update \uacc4\uc0b0 \ud6c4 \uc5c5\ub370\uc774\ud2b8\r\n        w1_update, w0_update = get_weight_updates(w1, w0, sample_X, sample_y, learning_rate= 0.01)\r\n        w1 = w1 - w1_update\r\n        w0 = w0 - w0_update\r\n\r\n    return w1, w0\r\n","a398c4f5":"w1, w0 = stochastic_gradient_descent_steps(X, y, iters = 1000)\r\nprint('w1:', round(w1[0, 0], 3), 'w0', round(w0[0, 0], 3))\r\ny_pred = w1[0, 0]*X + w0\r\nprint('Stochastic Gradient Descent Total Cost: {0:.4f}'.format(get_cost(y, y_pred)))","78c90e24":"import sklearn","f40cf3d4":"#LinearRegression\uc744 \uc774\uc6a9\ud574 \ubcf4\uc2a4\ud134 \uc8fc\ud0dd \uac00\uaca9 \ud68c\uadc0 \uad6c\ud604\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport pandas as pd\r\nimport seaborn as sns\r\nfrom scipy import stats\r\nfrom sklearn.datasets import load_boston\r\n%matplotlib inline\r\n\r\n#boston \ub370\uc774\ud130 \uc138\ud2b8 \ub85c\ub4dc\r\nboston = load_boston()\r\n\r\n#boston \ub370\uc774\ud130 \uc138\ud2b8 DataFrame \ubcc0\ud658\r\nbostonDF = pd.DataFrame(boston.data, columns=boston.feature_names)\r\n\r\n#boston \ub370\uc774\ud130 \uc138\ud2b8\uc758 target \ubc30\uc5f4\uc740 \uc8fc\ud0dd\uac00\uaca9, \uc774\ub97c PRICE \uce7c\ub7fc\uc73c\ub85c DataFrame\uc5d0 \ucd94\uac00\r\nbostonDF['PRICE'] = boston.target\r\nprint('Boston \ub370\uc774\ud130 \uc138\ud2b8 \ud06c\uae30 :', bostonDF.shape)\r\nbostonDF.head()","250ae853":"bostonDF.info()\r\n#Null\uac12\uc774 \uc5c6\uace0 \ubaa8\ub450 float\ud615","47eaa926":"#\uac01 \uceec\ub7fc\uc774 \ud68c\uadc0 \uacb0\uacfc\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc774 \uc5b4\ub290 \uc815\ub3c4\uc778\uc9c0 \uc2dc\uac01\ud654\r\n#seaborn\uc758 regplot() : X, Y\ucd95 \uac12\uc758 \uc0b0\uc810\ub3c4\uc640 \ud568\uacc4 \uc120\ud615 \ud68c\uadc0 \uc9c1\uc120\uc744 \uadf8\ub824\uc90c\r\n\r\n#2\uac1c\uc758 \ud589\uacfc 4\uac1c\uc758 \uc5f4\uc744 \uac00\uc9c4 subplots\ub97c \uc774\uc6a9. axs\ub294 4*2\uac1c\uc758 ax\ub97c \uac00\uc9d0\r\nfig, axs = plt.subplots(figsize = (16, 8), ncols= 4, nrows= 2)\r\nlm_features = ['RM', 'ZN', 'INDUS', 'NOX', 'AGE', 'PTRATIO', 'LSTAT', 'RAD']\r\nfor i, feature in enumerate(lm_features):\r\n    row = int(i\/4)\r\n    col = i%4\r\n    #\uc2dc\ubcf8\uc758 regplot\uc744 \uc774\uc6a9\ud574 \uc0b0\uc810\ub3c4\uc640 \uc120\ud615 \ud68c\uadc0 \uc9c1\uc120\uc744 \ud568\uaed8 \ud45c\ud604\r\n    sns.regplot(x = feature, y = 'PRICE', data = bostonDF, ax = axs[row][col])\r\n\r\n\r\n#RM\uc740 \uc591 \ubc29\ud5a5\uc758 \uc120\ud615\uc131 -> \ubc29\uc758 \ud06c\uae30\uac00 \ud074\uc218\ub85d \uac00\uaca9\uc774 \uc99d\uac00\r\n#LSTAT\uc740 \uc74c \ubc29\ud5a5\uc758 \uc120\ud615\uc131 -> LSTAT\uc774 \uc801\uc744\uc218\ub85d PRICE\uac00 \uc99d\uac00","896f4037":"#LinearRegression\ud074\ub798\uc2a4\ub97c \uc774\uc6a9\ud574 \ubcf4\uc2a4\ud134 \uc8fc\ud0dd \uac00\uaca9\uc758 \ud68c\uadc0 \ubaa8\ub378\uc744 \ub9cc\ub4e4\uae30\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.linear_model import LinearRegression\r\nfrom sklearn.metrics import mean_squared_error, r2_score\r\n\r\ny_target = bostonDF['PRICE']\r\nX_data = bostonDF.drop(['PRICE'], axis = 1, inplace=False)\r\n\r\nX_train, X_test, y_train, y_test = train_test_split(X_data, y_target, test_size= 0.3, random_state= 156)\r\n\r\n#\uc120\ud615 \ud68c\uadc0 OLS\ub85c \ud559\uc2b5\/\uc608\uce21\/\ud3c9\uac00 \uc218\ud589\r\nlr = LinearRegression()\r\nlr.fit(X_train, y_train)\r\ny_preds = lr.predict(X_test)\r\nmse = mean_squared_error(y_test, y_preds)\r\nrmse = np.sqrt(mse)\r\n\r\nprint('MSE : {0:.3f}, RMSE : {1:.3f}'.format(mse, rmse))\r\nprint('Variance score: {0:.3f}'.format(r2_score(y_test, y_preds)))","b80cab8a":"print('\uc808\ud3b8 \uac12:', lr.intercept_)\r\nprint('\ud68c\uadc0 \uacc4\uc218\uac12:', np.round(lr.coef_, 1))","8f762dcd":"#\ud68c\uadc0 \uacc4\uc218\ub97c \ud070 \uac12 \uc21c\uc73c\ub85c \uc815\ub82c\ud558\uae30 \uc704\ud574 Series\ub85c \uc0dd\uc131. \uc778\ub371\uc2a4 \uce7c\ub7fc\uba85\uc5d0 \uc720\uc758\r\ncoeff = pd.Series(data = np.round(lr.coef_, 1), index = X_data.columns)\r\ncoeff.sort_values(ascending=False)","9501fb61":"from sklearn.model_selection  import cross_val_score\r\n\r\ny_target = bostonDF['PRICE']\r\nX_data = bostonDF.drop(['PRICE'], axis= 1, inplace=False)\r\nlr = LinearRegression()\r\n\r\n#cross_val_score()\ub85c 5 \ud3f4\ub4dc \uc138\ud2b8\ub85c MSE\ub97c \uad6c\ud55c \ub4a4 \uc774\ub97c \uae30\ubc18\uc73c\ub85c \ub2e4\uc2dc RMSE \uad6c\ud568\r\nneg_mse_scores = cross_val_score(lr, X_data, y_target, scoring='neg_mean_squared_error', cv = 5)\r\nrmse_scores = np.sqrt(-1*neg_mse_scores)\r\navg_rmse = np.mean(rmse_scores)\r\n\r\n#cross_val_score(scoring = 'neg_mean_squared_error')\ub85c \ubc18\ud658\ub41c \uac12\uc740 \ubaa8\ub450 \uc74c\uc218\r\nprint('5 folds\uc758 \uac1c\ubcc4 Negative MSE scores:', np.round(neg_mse_scores, 2))\r\nprint('5 folda\uc758 \uac1c\ubcc4 RMSE scores: ', np.round(rmse_scores, 2))\r\nprint('5 folds\uc758 \ud3c9\uade0 RMSE : {0:.3f}'.format(avg_rmse))","d6e4c82a":"#\ub2e4\ud56d \ud68c\uadc0 -> \ub2e4\ud56d \ud68c\uadc0 \uc5ed\uc2dc \uc120\ud615 \ud68c\uadc0\uc774\uae30 \ub54c\ubb38\uc5d0 \ube44\uc120\ud615 \ud568\uc218\ub97c \uc120\ud615 \ubaa8\ub378\uc5d0 \uc801\uc6a9\uc2dc\ud0a4\ub294 \ubc29\ubc95\uc744 \uc0ac\uc6a9\ud574 \uad6c\ud604(\uc0ac\uc774\ud0b7\ub7f0\uc5d0\uc11c\ub294 \ub2e4\ud56d \ud68c\uadc0\ub97c \uc704\ud55c \ud074\ub798\uc2a4\ub97c \uc81c\uacf5X)\r\n#\uc0ac\uc774\ud0b7\ub7f0\uc758 PolynomialFeatures \ud074\ub798\uc2a4\ub97c \ud1b5\ud574 \ud53c\ucc98\ub97c Polynomial\ud53c\ucc98\ub85c \ubcc0\ud658 -> \ub2e8\ud56d\uc2dd \ud53c\ucc98\ub97c degree\uc5d0 \ud574\ub2f9\ud558\ub294 \ub2e4\ud56d\uc2dd \ud53c\ucc98\ub85c \ubcc0\ud658\r\n\r\nfrom sklearn.preprocessing import PolynomialFeatures\r\nimport numpy as np\r\n\r\n#\ub2e4\ud56d\uc2dd\uc73c\ub85c \ubcc0\ud658\ud55c \ub2e8\ud56d\uc2dd \uc0dd\uc131, [[0, 1], [2, 3]]\uc758  2*2 \ud589\ub82c \uc0dd\uc131\r\nX = np.arange(4).reshape(2, 2)\r\nprint('\uc77c\ucc28 \ub2e8\ud56d\uc2dd \uacc4\uc218 \ud53c\ucc98:\\n', X)\r\n\r\n#degree = 2\uc778 2\ucc28 \ub2e4\ud56d\uc2dd\uc73c\ub85c \ubcc0\ud658\ud558\uae30 \uc704\ud574 PolynomialFeatures\ub97c \uc774\uc6a9\ud574 \ubcc0\ud658\r\npoly = PolynomialFeatures(degree= 2)\r\npoly.fit(X)\r\npoly_ftr = poly.transform(X)\r\nprint('\ubcc0\ud658\ub41c 2\ucc28 \ub2e4\ud56d\uc2dd \uacc4\uc218 \ud53c\ucc98:\\n', poly_ftr)","53e25be5":"def polynomial_func(X):\r\n    y = 1 + 2*X[:, 0] + 3*X[:, 0]**2 + 4*X[:, 1]**3\r\n    return y\r\n\r\nX = np.arange(4).reshape(2, 2)\r\nprint('\uc77c\ucc28 \ub2e8\ud56d\uc2dd \uacc4\uc218 feature: \\n', X)\r\ny = polynomial_func(X)\r\nprint('\uc0bc\ucc28 \ub2e4\ud56d\uc2dd \uacb0\uc815\uac12: \\n', y)","1fb0a61a":"#3\ucc28 \ub2e4\ud56d\uc2dd \ubcc0\ud658\r\npoly_ftr = PolynomialFeatures(degree= 3).fit_transform(X)\r\nprint('3\ucc28 \ub2e4\ud56d\uc2dd \uacc4\uc218 feature: \\n', poly_ftr)\r\n\r\n#Linear Regression\uc5d0 3\ucc28 \ub2e4\ud56d\uc2dd \uacc4\uc218 feature\uc640 3\ucc28 \ub2e4\ud56d\uc2dd \uacb0\uc815\uac12\uc73c\ub85c \ud559\uc2b5 \ud6c4 \ud68c\uadc0 \uacc4\uc218 \ud655\uc778\r\nmodel = LinearRegression()\r\nmodel.fit(poly_ftr, y)\r\nprint('Polynomial \ud68c\uadc0 \uacc4\uc218\\n', np.round(model.coef_, 2))\r\nprint('Polynomial \ud68c\uadc0 Shape :', model.coef_.shape)","2c7f9425":"#\ud53c\ucc98 \ubcc0\ud658\uacfc \uc120\ud615 \ud68c\uadc0 \uc801\uc6a9\uc744 \uac01\uac01 \ubcc4\ub3c4\ub85c \ud558\ub294 \uac83\ubcf4\ub2e4\ub294 \uc0ac\uc774\ud0b7\ub7f0\uc758 Pipeline\uac1d\uccb4\ub97c \uc774\uc6a9\ud574 \ud55c \ubc88\uc5d0 \ub2e4\ud56d \ud68c\uadc0\ub97c \uad6c\ud604\r\nfrom sklearn.preprocessing import PolynomialFeatures\r\nfrom sklearn.linear_model import LinearRegression\r\nfrom sklearn.pipeline import Pipeline\r\nimport numpy as np\r\n\r\ndef polynomial_func(X):\r\n    y = 1 + 2*X[:, 0] + 3*X[:, 0]**2 + 4*X[:, 1]**3\r\n    return y\r\n\r\n#Pipeline \uac1d\uccb4\ub85c Streamline\ud558\uac8c Polynomial Feature \ubcc0\ud658\uacfc Linear Regression\uc744 \uc5f0\uacb0\r\nmodel = Pipeline([('poly', PolynomialFeatures(degree= 3)), ('linear', LinearRegression())])\r\nX = np.arange(4).reshape(2, 2)\r\ny = polynomial_func(X)\r\n\r\nmodel = model.fit(X, y)\r\n\r\nprint('Polynomial \ud68c\uadc0 \uacc4\uc218\\n', np.round(model.named_steps['linear'].coef_, 2))","59dd8f2e":"#\ub2e4\ud56d \ud68c\uadc0\ub97c \uc774\uc6a9\ud55c \uacfc\uc18c\uc801\ud569 \ubc0f \uacfc\uc801\ud569 \uc774\ud574\r\n#\ub2e4\ud56d \ud68c\uadc0\uc5d0\uc11c \ucc28\uc218\uac00 \ub192\uc544\uc9c8\uc218\ub85d \uacfc\uc801\ud569\uc758 \ubb38\uc81c\uac00 \ud06c\uac8c \ubc1c\uc0dd\r\n\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nfrom sklearn.pipeline import Pipeline\r\nfrom sklearn.preprocessing import PolynomialFeatures\r\nfrom sklearn.linear_model import LinearRegression\r\nfrom sklearn.model_selection import cross_val_score\r\n%matplotlib inline\r\n\r\n#\uc784\uc758\uc758 \uac12\uc73c\ub85c \uad6c\uc131\ub41c X\uac12\uc5d0 \ub300\ud574 \ucf54\uc0ac\uc778 \ubcc0\ud658 \uac12\uc744 \ubc18\ud658.\r\ndef true_fun(X):\r\n    return np.cos(1.5 * np.pi * X)\r\n\r\n#X\ub294 0\ubd80\ud130 1\uae4c\uc9c0 30\uac1c\uc758 \uc784\uc758\uc758 \uac12\uc744 \uc21c\uc11c\ub300\ub85c \uc0d8\ud50c\ub9c1\ud55c \ub370\uc774\ud130\r\nnp.random.seed(0)\r\nn_samples = 30\r\nX = np.sort(np.random.rand(n_samples))\r\n\r\n#y \uac12\uc740 \ucf54\uc0ac\uc778 \uae30\ubc18\uc758 true_fun()\uc5d0\uc11c \uc57d\uac04\uc758 \ub178\uc774\uc988 \ubcc0\ub3d9 \uac12\uc744 \ub354\ud55c \uac12\r\ny = true_fun(X) + np.random.randn(n_samples)*0.1","cf441d53":"plt.figure(figsize=(14, 5))\r\ndegrees = [1, 4, 15]\r\n\r\n#\ub2e4\ud56d \ud68c\uadc0\uc758 \ucc28\uc218(degree)\ub97c 1, 4, 15\ub85c \uac01\uac01 \ubcc0\ud654\uc2dc\ud0a4\uba74\uc11c \ube44\uad50\r\nfor i in range(len(degrees)):\r\n    ax = plt.subplot(1, len(degrees), i + 1)\r\n    plt.setp(ax, xticks = (), yticks = ())\r\n\r\n    #\uac1c\ubcc4 degree\ubcc4\ub85c Polynomial \ubcc0\ud658\r\n    polynomial_features = PolynomialFeatures(degree= degrees[i], include_bias=False)\r\n    linear_regression = LinearRegression()\r\n    pipeline = Pipeline([('polynomial_features', polynomial_features), ('linear_regression', linear_regression)])\r\n    pipeline.fit(X.reshape(-1, 1), y)\r\n\r\n    #\uad50\ucc28 \uac80\uc99d\uc73c\ub85c \ub2e4\ud56d \ud68c\uadc0\ub97c \ud3c9\uac00\r\n    scores = cross_val_score(pipeline, X.reshape(-1, 1), y, scoring = 'neg_mean_squared_error', cv = 10)\r\n    #Pipeline\uc744 \uad6c\uc131\ud558\ub294 \uc138\ubd80 \uac1d\uccb4\ub97c \uc811\uadfc\ud558\ub294 named_steps['\uac1d\uccb4\uba85']\uc744 \uc774\uc6a9\ud574 \ud68c\uadc0\uacc4\uc218 \ucd94\ucd9c\r\n    coefficients = pipeline.named_steps['linear_regression'].coef_\r\n    print('\\nDegree {0} \ud68c\uadc0 \uacc4\uc218\ub294 {1}.'.format(degrees[i], np.round(coefficients, 2)))\r\n    print('Degree {0} MSE \ub294 {1}.'.format(degrees[i], -1*np.mean(scores)))\r\n\r\n    #0\ubd80\ud130 1\uae4c\uc9c0 \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130 \uc138\ud2b8\ub97c 100\uac1c\ub85c \ub098\ub220 \uc608\uce21\uc744 \uc218\ud589\r\n    #\ud14c\uc2a4\ud2b8 \ub370\uc774\ud130 \uc138\ud2b8\uc5d0 \ud68c\uadc0 \uc608\uce21\uc744 \uc218\ud589\ud558\uace0 \uc608\uce21 \uace1\uc120\uacfc \uc2e4\uc81c \uace1\uc120\uc744 \uadf8\ub824\uc11c \ube44\uad50\r\n    X_test = np.linspace(0, 1, 100)\r\n    #\uc608\uce21\uac12 \uace1\uc120\r\n    plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label = 'Model')\r\n    #\uc2e4\uc81c\uac12 \uace1\uc120\r\n    plt.plot(X_test, true_fun(X_test), '--', label = 'True function')\r\n    plt.scatter(X, y, edgecolors='b', s = 20, label = 'Samples')\r\n\r\n    plt.xlabel('x');plt.ylabel('y');plt.xlim((0, 1));plt.ylim((-2, 2));plt.legend(loc = 'best')\r\n    plt.title('Degree {}\\nMSE = {:.2e}(+\/-{:.2e})'.format(degrees[i], -scores.mean(), scores.std()))\r\n\r\nplt.show()","1acbea55":"#\uaddc\uc81c \uc120\ud615 \ubaa8\ub378 - \ub9bf\uc9c0, \ub77c\uc3d8, \uc5d8\ub77c\uc2a4\ud2f1\ub137\r\n#\uaddc\uc81c : \ube44\uc6a9\ud568\uc218\uc5d0 alpha\uac12\uc73c\ub85c \ud328\ub110\ud2f0\ub97c \ubd80\uc5ec\ud574 \ud68c\uadc0 \uacc4\uc218 \uac12\uc758 \ud06c\uae30\ub97c \uac10\uc18c\uc2dc\ucf1c \uacfc\uc801\ud569\uc744 \uac1c\uc120\ud558\ub294 \ubc29\uc2dd\r\n#\ub9bf\uc9c0(Ridge) \ud68c\uadc0 : W\uc758 \uc81c\uacf1\uc5d0 \ub300\ud574 \ud398\ub110\ud2f0\ub97c \ubd80\uc5ec\ud558\ub294 \ubc29\uc2dd(L2 \uaddc\uc81c)\r\n#\ub77c\uc3d8(Lasso) \ud68c\uadc0 : W\uc758 \uc808\ub313\uac12\uc5d0 \ub300\ud574 \ud398\ub110\ud2f0\ub97c \ubd80\uc5ec\ud558\ub294 \ubc29\uc2dd(L1 \uaddc\uc81c)\r\n\r\n## \ub9bf\uc9c0 \ud68c\uadc0 ##\r\nfrom sklearn.linear_model import Ridge\r\nfrom sklearn.model_selection import cross_val_score\r\n\r\n#alpha = 10\uc73c\ub85c \uc124\uc815\ud574 \ub9bf\uc9c0 \ud68c\uadc0 \uc218\ud589\r\nridge = Ridge(alpha = 10)\r\nneg_mse_scores = cross_val_score(ridge, X_data, y_target, scoring = 'neg_mean_squared_error', cv = 5)\r\nrmse_scores = np.sqrt(-1*neg_mse_scores)\r\navg_rmse = np.mean(rmse_scores)\r\n\r\nprint(' 5 folds \uc758 \uac1c\ubcc4 Negative MSE scores: ', np.round(neg_mse_scores, 3))\r\nprint(' 5 folds \uc758 \uac1c\ubcc4 RMSE scores : ', np.round(rmse_scores, 3))\r\nprint(' 5 folds \uc758 \ud3c9\uade0 RMSE : {0:.3f} '.format(avg_rmse))","1fde6857":"#alpha \uac12\uc744 0, 0.1, 1, 10, 100\uc73c\ub85c \ubcc0\ud654\uc2dc\ud0a4\uba74\uc11c RMSE\uc640 \ud68c\uadc0 \uacc4\uc218 \uac12\uc758 \ubcc0\ud654 \ud655\uc778\r\n\r\n#\ub9bf\uc9c0\uc5d0 \uc0ac\uc6a9\ub420 alpha \ud30c\ub77c\ubbf8\ud130\uc758 \uac12\uc744 \uc815\uc758\r\nalphas = [0, 0.1, 1, 10, 100]\r\n\r\n#alphas list \uac12\uc744 \ubc18\ubcf5\ud558\uba74\uc11c alpha\uc5d0 \ub530\ub978 \ud3c9\uade0 rmse\ub97c \uad6c\ud568\r\nfor alpha in alphas:\r\n    ridge = Ridge(alpha= alpha)\r\n\r\n    #cross_val_score\ub97c \uc774\uc6a9\ud574 5 \ud3f4\ub4dc\uc758 \ud3c9\uade0 RMSE\ub97c \uacc4\uc0b0\r\n    neg_mse_scores = cross_val_score(ridge, X_data, y_target, scoring='neg_mean_squared_error', cv = 5)\r\n    avg_rmse = np.mean(np.sqrt(-1*neg_mse_scores))\r\n    print('alpha {0} \uc77c \ub54c 5 folds\uc758 \ud3c9\uade0 RMSE : {1:.3f}'.format(alpha, avg_rmse)) ","85418da1":"#\uac01 alpha\uc5d0 \ub530\ub978 \ud68c\uadc0 \uacc4\uc218 \uac12\uc744 \uc2dc\uac01\ud654\ud558\uae30 \uc704\ud574 5\uac1c\uc758 \uc5f4\ub85c \ub41c \ub9f7\ud50c\ub86f\ub9bd \ucd95 \uc0dd\uc131\r\nfig, axs = plt.subplots(figsize = (18, 6), nrows= 1 , ncols= 5)\r\n#\uac01 alpha\uc5d0 \ub530\ub978 \ud68c\uadc0 \uacc4\uc218 \uac12\uc744 \ub370\uc774\ud130\ub85c \uc800\uc7a5\ud558\uae30 \uc704\ud55c DataFrame \uc0dd\uc131\r\ncoeff_df = pd.DataFrame()\r\n\r\n#alphas \ub9ac\uc2a4\ud2b8 \uac12\uc744 \ucc28\ub840\ub85c \uc785\ub825\ud574 \ud68c\uadc0 \uacc4\uc218 \uac12 \uc2dc\uac01\ud654 \ubc0f \ub370\uc774\ud130 \uc800\uc7a5. pos\ub294 axis\uc758 \uc704\uce58 \uc9c0\uc815\r\nfor pos, alpha in enumerate(alphas):\r\n    ridge = Ridge(alpha= alpha)\r\n    ridge.fit(X_data, y_target)\r\n    #alpha\uc5d0 \ub530\ub978 \ud53c\ucc98\ubcc4\ub85c \ud68c\uadc0 \uacc4\uc218\ub97c Series\ub85c \ubcc0\ud658\ud558\uace0 \uc774\ub97c DataFrame\uc758 \uce7c\ub7fc\uc73c\ub85c \ucd94\uac00\r\n    coeff = pd.Series(data = ridge.coef_, index = X_data.columns)\r\n    colname = 'alpha:' + str(alpha)\r\n    coeff_df[colname] = coeff\r\n    #\ub9c9\ub300 \uadf8\ub798\ud504\ub85c \uac01 alpha\uac12\uc5d0\uc11c\uc758 \ud68c\uadc0 \uacc4\uc218\ub97c \uc2dc\uac01\ud654. \ud68c\uadc0 \uacc4\uc218\uac12\uc774 \ub192\uc740 \uc21c\uc73c\ub85c \ud45c\ud604\r\n    coeff = coeff.sort_values(ascending= False)\r\n    axs[pos].set_title(colname)\r\n    axs[pos].set_xlim(-3, 6)\r\n    sns.barplot(x = coeff.values, y = coeff.index, ax = axs[pos])\r\n\r\n#for \ubb38 \ubc14\uae65\uc5d0\uc11c \ub9f7\ud50c\ub86f\ub9bd\uc758 show \ud638\ucd9c \ubc0f alpha\uc5d0 \ub530\ub978 \ud53c\ucc98\ubcc4 \ud68c\uadc0 \uacc4\uc218\ub97c DataFrame\uc73c\ub85c \ud45c\uc2dc\r\nplt.show()","d1c76ec4":"ridge_alphas = [0, 0.1, 1, 10, 100]\r\nsort_column = 'alpha:' + str(ridge_alphas[0])\r\ncoeff_df.sort_values(by = sort_column, ascending = False)\r\n#alpha\uac12\uc774 \uc99d\uac00\ud558\uba74\uc11c \ud68c\uadc0 \uacc4\uc218\uac00 \uc9c0\uc18d\uc801\uc73c\ub85c \uc791\uc544\uc9d0","c9d93445":"## \ub77c\uc3d8 \ud68c\uadc0 ##\r\n#L1 \uaddc\uc81c\ub294 \ubd88\ud544\uc694\ud55c \ud68c\uadc0 \uacc4\uc218\ub97c \uae09\uaca9\ud558\uac8c \uac10\uc18c\uc2dc\ucf1c 0\uc73c\ub85c \ub9cc\ub4e4\uace0 \uc81c\uac70 -> \uc801\uc808\ud55c \ud53c\ucc98\ub9cc \ud68c\uadc0\uc5d0 \ud3ec\ud568\uc2dc\ud0a4\ub294 \ud53c\ucc98 \uc120\ud0dd\uc758 \ud2b9\uc131\uc744 \uac00\uc9d0\r\nfrom sklearn.linear_model import Lasso, ElasticNet\r\n\r\n#alpha\uac12\uc5d0 \ub530\ub978 \ud68c\uadc0 \ubaa8\ub378\uc758 \ud3f4\ub4dc \ud3c9\uade0 RMSE\ub97c \ucd9c\ub825\ud558\uace0 \ud68c\uadc0 \uacc4\uc218\uac12\ub4e4\uc744 DataFrame\uc73c\ub85c \ubcc0\ud658\r\ndef get_linear_reg_eval(model_name, params = None, X_data_n = None, y_target_n = None, verbose = True, return_coeff = True):\r\n    coeff_df = pd.DataFrame()\r\n    if verbose : print('######', model_name, '######')\r\n    for param in params:\r\n        if model_name == 'Ridge' : model = Ridge(alpha=param)\r\n        elif model_name == 'Lasso' : model = Lasso(alpha=param)\r\n        elif model_name == 'ElasticNet' : model = ElasticNet(alpha = param, l1_ratio=0.7)\r\n        neg_mse_scores = cross_val_score(model, X_data_n, y_target_n, scoring = 'neg_mean_squared_error', cv= 5)\r\n        avg_rmse = np.mean(np.sqrt(-1*neg_mse_scores))\r\n        print('alpha {0}\uc77c \ub54c 5 \ud3f4\ub4dc \uc138\ud2b8\uc758 \ud3c9\uade0 RMSE: {1:.3f} '.format(param, avg_rmse))\r\n        #cross_val_score\ub294 evaluation metric\ub9cc \ubc18\ud658\ud558\ubbc0\ub85c \ubaa8\ub378\uc744 \ub2e4\uc2dc \ud559\uc2b5\ud558\uc5ec \ud68c\uadc0 \uacc4\uc218 \ucd94\ucd9c\r\n\r\n        model.fit(X_data_n, y_target_n)\r\n        if return_coeff:\r\n            #alpha\uc5d0 \ub530\ub978 \ud53c\ucc98\ubcc4 \ud68c\uadc0 \uacc4\uc218\ub97c Series\ub85c \ubcc0\ud658\ud558\uace0 \uc774\ub97c DataFrame\uc758 \uce7c\ub7fc\uc73c\ub85c \ucd94\uac00\r\n            coeff = pd.Series(data = model.coef_, index = X_data_n.columns)\r\n            colname = 'alpha : ' + str(param)\r\n            coeff_df[colname] = coeff\r\n\r\n    return coeff_df","49eb1d53":"#\ub77c\uc3d8\uc5d0 \uc0ac\uc6a9\ub420 alpha \ud30c\ub77c\ubbf8\ud130\uc758 \uac12\uc744 \uc815\uc758\ud558\uace0 get_linear_reg_eval() \ud568\uc218 \ud638\ucd9c\r\nlasso_alphas = [0.07, 0.1, 0.5, 1, 3]\r\ncoeff_lasso_df = get_linear_reg_eval('Lasso', params=lasso_alphas, X_data_n=X_data, y_target_n=y_target)","11be00e8":"#\ubc18\ud690\ub41c coeff_lasso_df\ub97c \uccab \ubc88\uc9f8 \uce7c\ub7fc\uc21c\uc73c\ub85c \ub0b4\ub9bc\ucc28\uc21c \uc815\ub82c\ud574 \ud68c\uadc0\uacc4\uc218 DataFrame \ucd9c\ub825\r\nsort_column = 'alpha : ' + str(lasso_alphas[0])\r\ncoeff_lasso_df.sort_values(by = sort_column, ascending= False)\r\n\r\n#alpha\uc758 \ud06c\uae30\uac00 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \uc77c\ubd80 \ud53c\ucc98\uc758 \ud68c\uadc0 \uacc4\uc218\ub294 \uc544\uc608 0\uc73c\ub85c \ubc14\ub01c -> \ud68c\uadc0 \uacc4\uc218\uac00 0\uc778 \ud53c\ucc98\ub294 \ud68c\uadc0 \uc2dd\uc5d0\uc11c \uc81c\uc678\ub418\uba74\uc11c \ud53c\ucc98 \uc120\ud0dd\uc758 \ud6a8\uacfc\ub97c \uc5bb\uc744 \uc218 \uc788\uc74c","7b65dec7":"### \uc5d8\ub77c\uc2a4\ud2f1\ub137 \ud68c\uadc0 ###\r\n#L1 \uaddc\uc81c\uc640 L2 \uaddc\uc81c\ub97c \uacb0\ud569\ud55c \ud68c\uadc0 \r\n#\uc5d8\ub77c\uc2a4\ud2f1\ub137\uc758 \uaddc\uc81c\ub294 a*L1 + b*L2 (a : L1 \uaddc\uc81c\uc758 alpha\uac12, b : L2 \uaddc\uc81c\uc758 alpha\uac12)\r\n#alpha\ud30c\ub77c\ubbf8\ud130 \uac12 = a + b\r\n#l1_ratio \ud30c\ub77c\ubbf8\ud130 \uac12 = a\/(a + b)\r\n\r\n#\uc5d8\ub77c\uc2a4\ud2f1\ub137\uc5d0 \uc0ac\uc6a9\ub420 alpha \ud30c\ub77c\ubbf8\ud130\uc758 \uac12\ub4e4\uc744 \uc815\uc758\ud558\uace0 get_linear_reg_eval() \ud568\uc218 \ud638\ucd9c\r\n#l1_ratio\ub294 0.7\ub85c \uace0\uc815\r\nelastic_alphas = [0.07, 0.1, 0.5, 1, 3]\r\ncoeff_elastic_df = get_linear_reg_eval('ElasticNet', params=elastic_alphas, X_data_n=X_data, y_target_n=y_target)","90ab72c8":"#\ubc18\ud658\ub41c coeff_elastic_df\ub97c \uccab \ubc88\uc9f8 \uce7c\ub7fc\uc21c\uc73c\ub85c \ub0b4\ub9bc\ucc28\uc21c \uc815\ub82c\ud574 \ud68c\uadc0\uacc4\uc218 DataFrame \ucd9c\ub825\r\nsort_column = 'alpha : ' + str(elastic_alphas[0])\r\ncoeff_elastic_df.sort_values(by = sort_column, ascending=False)","60e4e135":"## \uc120\ud615 \ud68c\uadc0 \ubaa8\ub378\uc744 \uc704\ud55c \ub370\uc774\ud130 \ubcc0\ud658 ##\r\n\r\n\r\n\r\n#method\ub294 \ud45c\uc900 \uc815\uaddc \ubd84\ud3ec \ubcc0\ud658(Standard), \ucd5c\ub313\uac12\/\ucd5c\uc19f\uac12 \uc815\uaddc\ud654(MinMax), \ub85c\uadf8\ubcc0\ud658(Log) \uacb0\uc815\r\n#p_degree\ub294 \ub2e4\ud56d\uc2dd \ud2b9\uc131\uc744 \ucd94\uac00\ud560 \ub54c \uc801\uc6a9. p_degree\ub294 2 \uc774\uc0c1 \ubd80\uc5ec\ud558\uc9c0 \uc54a\uc74c\r\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\r\n\r\ndef get_scaled_data(method = 'None', p_degree = None, input_data = None):\r\n    if method == 'Standard':\r\n        scaled_data = StandardScaler().fit_transform(input_data)\r\n    elif method == 'MinMax':\r\n        scaled_data = MinMaxScaler().fit_transform(input_data)\r\n    elif method == 'Log':\r\n        scaled_data = np.log1p(input_data)\r\n    else:\r\n        scaled_data = input_data\r\n\r\n    if p_degree != None:\r\n        scaled_data = PolynomialFeatures(degree=p_degree, include_bias=False).fit_transform(scaled_data)\r\n\r\n    return scaled_data","16365acb":"#Ridge\uc758 alpha\uac12\uc744 \ub2e4\ub974\uac8c \uc801\uc6a9\ud558\uace0 \ub2e4\uc591\ud55c \ub370\uc774\ud130 \ubcc0\ud658 \ubc29\ubc95\uc5d0 \ub530\ub978 RMSE \ucd94\ucd9c\r\nalphas = [0.1, 1, 10, 100]\r\n\r\n#5\uac1c \ubc29\uc2dd\uc73c\ub85c \ubcc0\ud658. \uba3c\uc800 \uc6d0\ubcf8 \uadf8\ub300\ub85c, \ud45c\uc900\uc815\uaddc\ubd84\ud3ec, \ud45c\uc900\uc815\uaddc \ubd84\ud3ec+\ub2e4\ud56d\uc2dd \ud2b9\uc131\r\n#\ucd5c\ub300\/\ucd5c\uc18c \uc815\uaddc\ud654, \ucd5c\ub300\/\ucd5c\uc18c \uc815\uaddc\ud654 + \ub2e4\ud56d\uc2dd \ud2b9\uc131, \ub85c\uadf8\ubcc0\ud658\r\nscale_methods = [(None, None), ('Standard', None), ('Standard', 2), ('MinMax', None), ('MinMax', 2), ('Log', None)]\r\nfor scale_method in scale_methods:\r\n    X_data_scaled = get_scaled_data(method=scale_method[0], p_degree=scale_method[1], input_data=X_data)\r\n    print('\\n## \ubcc0\ud658 \uc720\ud615:{0}, Polynomial Degree:{1}'.format(scale_method[0], scale_method[1]))\r\n    get_linear_reg_eval('Ridge', params=alphas, X_data_n=X_data_scaled, y_target_n=y_target, verbose=False, return_coeff=False)","1f214d8d":"## \ub85c\uc9c0\uc2a4\ud2f1 \ud68c\uadc0 ##\r\nimport pandas as pd\r\nimport matplotlib.pyplot as plt\r\n%matplotlib inline\r\n\r\nfrom sklearn.datasets import load_breast_cancer\r\nfrom sklearn.linear_model import LogisticRegression\r\n\r\ncancer = load_breast_cancer()","aae61a01":"from sklearn.preprocessing import StandardScaler\r\nfrom sklearn.model_selection import train_test_split\r\n\r\n#StandardScaler()\ub85c \ud3c9\uade0\uc774 0, \ubd84\uc0b0 1\ub85c \ub370\uc774\ud130 \ubd84\ud3ec\ub3c4 \ubcc0\ud658\r\nscaler = StandardScaler()\r\ndata_scaled = scaler.fit_transform(cancer.data)\r\n\r\nX_train, X_test, y_train, y_test = train_test_split(data_scaled, cancer.target, test_size = 0.3, random_state= 0)","f8833fff":"from sklearn.metrics import accuracy_score, roc_auc_score\r\n\r\n#\ub85c\uc9c0\uc2a4\ud2f1 \ud68c\uadc0\ub97c \uc774\uc6a9\ud574 \ud559\uc2b5 \ubc0f \uc608\uce21 \uc218\ud589\r\nlr_clf = LogisticRegression()\r\nlr_clf.fit(X_train, y_train)\r\nlr_preds = lr_clf.predict(X_test)\r\n\r\n#\uc815\ud655\ub3c4\uc640 roc_auc \uce21\uc815\r\nprint('accuracy: {:.3f}'.format(accuracy_score(y_test, lr_preds)))\r\nprint('roc_auc: {:.3f}'.format(roc_auc_score(y_test, lr_preds)))","69a1db27":"from sklearn.model_selection import GridSearchCV\r\n\r\nparams = {'penalty' : ['l2', 'l1'], 'C' : [0.01, 0.1, 1, 5, 10]}\r\n\r\ngrid_clf = GridSearchCV(lr_clf, param_grid=params, scoring='accuracy', cv = 3)\r\ngrid_clf.fit(data_scaled, cancer.target)\r\nprint('\ucd5c\uc801 \ud558\uc774\ud37c \ud30c\ub77c\ubbf8\ud130:{0}, \ucd5c\uc801 \ud3c9\uade0 \uc815\ud655\ub3c4:{1:.3f}'.format(grid_clf.best_params_, grid_clf.best_score_))","c41e0ed3":"#\ub79c\ub364 \ud3ec\ub808\uc2a4\ud2b8 \ud68c\uadc0\ub97c \uc774\uc6a9\ud574 \uc55e\uc758 \uc120\ud615 \ud68c\uadc0\uc5d0\uc11c \ub2e4\ub978 \ubcf4\uc2a4\ud134 \uc8fc\ud0dd \uac00\uaca9 \uc608\uce21\r\nfrom sklearn.datasets import load_boston\r\nfrom sklearn.model_selection import cross_val_score\r\nfrom sklearn.ensemble import RandomForestRegressor\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\n#\ubcf4\uc2a4\ud134 \ub370\uc774\ud130 \uc138\ud2b8 \ub85c\ub4dc\r\nboston = load_boston()\r\nbostonDF = pd.DataFrame(boston.data, columns= boston.feature_names)\r\n\r\nbostonDF['PRICE'] = boston.target\r\ny_target = bostonDF['PRICE']\r\nX_data = bostonDF.drop(['PRICE'], axis = 1, inplace= False)\r\n\r\nrf = RandomForestRegressor(random_state= 0, n_estimators= 100)\r\nneg_mse_scores = cross_val_score(rf, X_data, y_target, scoring='neg_mean_squared_error', cv = 5)\r\nrmse_scores = np.sqrt(-1*neg_mse_scores)\r\navg_rmse = np.mean(rmse_scores)\r\n\r\nprint(' 5 \uad50\ucc28 \uac80\uc99d\uc758 \uac1c\ubcc4 Negative MSE scores: ', np.round(neg_mse_scores, 2))\r\nprint(' 5 \uad50\ucc28 \uac80\uc99d\uc758 \uac1c\ubcc4 RMSE scores : ', np.round(rmse_scores, 2))\r\nprint(' 5 \uad50\ucc28 \uac80\uc99d\uc758 \ud3c9\uade0 RMSE : {0:.3f}'.format(avg_rmse))","d8952d68":"#\uacb0\uc815 \ud2b8\ub9ac, GBM, XGBoost, LightGBM\uc758 Regressor\ub97c \ubaa8\ub450 \uc774\uc6a9\ud574 \ubcf4\uc2a4\ud134 \uc8fc\ud0dd \uac00\uaca9 \uc608\uce21\uc744 \uc218\ud589\r\n\r\n#\uc785\ub825 \ubaa8\ub378\uacfc \ub370\uc774\ud130 \uc138\ud2b8\ub97c \uc785\ub825 \ubc1b\uc544 \uad50\ucc28 \uac80\uc99d\uc73c\ub85c \ud3c9\uade0 RMSE\ub97c \uacc4\uc0b0\ud574\uc8fc\ub294 \ud568\uc218\r\ndef get_model_cv_prediction(model, X_data, y_target):\r\n    neg_mse_scores = cross_val_score(model, X_data, y_target, scoring = 'neg_mean_squared_error', cv = 5)\r\n    rmse_scores = np.sqrt(-1*neg_mse_scores)\r\n    avg_rmse = np.mean(rmse_scores)\r\n    print('#### ', model.__class__.__name__, ' ####')\r\n    print(' 5 \uad50\ucc28 \uac80\uc99d\uc758 \ud3c9\uade0 RMSE : {0:3f}'.format(avg_rmse))","5422ce74":"from sklearn.tree import DecisionTreeRegressor\r\nfrom sklearn.ensemble import GradientBoostingRegressor\r\nfrom xgboost import XGBRegressor\r\nfrom lightgbm import LGBMRegressor\r\n\r\ndt_reg = DecisionTreeRegressor(random_state= 0, max_depth = 4)\r\nrf_reg = RandomForestRegressor(random_state= 0, n_estimators= 1000)\r\ngb_reg = GradientBoostingRegressor(random_state = 0, n_estimators = 1000)\r\nxgb_reg = XGBRegressor(n_estimators = 1000)\r\nlgb_reg = LGBMRegressor(n_estimators = 1000)\r\n\r\n#\ud2b8\ub9ac \uae30\ubc18\uc758 \ud68c\uadc0 \ubaa8\ub378\uc744 \ubc18\ubcf5\ud558\uba74\uc11c \ud3c9\uac00 \uc218\ud589 \r\nmodels = [dt_reg, rf_reg, gb_reg, lgb_reg]\r\nfor model in models:\r\n    get_model_cv_prediction(model, X_data, y_target)","0732c73c":"import seaborn as sns\r\n\r\nrf_reg = RandomForestRegressor(n_estimators= 1000)\r\n\r\n#\uc55e \uc608\uc800\uc5d0\uc11c \ub9cc\ub4e4\uc5b4\uc9c4 X_data, y_target \ub370\uc774\ud130 \uc138\ud2b8\ub97c \uc801\uc6a9\ud574 \ud559\uc2b5\r\nrf_reg.fit(X_data, y_target)\r\n\r\nfeature_series = pd.Series(data = rf_reg.feature_importances_, index = X_data.columns)\r\nfeature_series = feature_series.sort_values(ascending=False)\r\nsns.barplot(x = feature_series, y = feature_series.index)","2db7fa44":"import matplotlib.pyplot as plt\r\nbostonDF_sample = bostonDF[['RM', 'PRICE']]\r\nbostonDF_sample = bostonDF_sample.sample(n = 100, random_state= 0)\r\nprint(bostonDF_sample.shape)\r\nplt.figure()\r\nplt.scatter(bostonDF_sample.RM, bostonDF_sample.PRICE, c= 'darkorange')","fc93ba89":"from sklearn.linear_model import LinearRegression\r\n\r\n#\uc120\ud615 \ud68c\uadc0\uc640 \uacb0\uc815 \ud2b8\ub9ac \uae30\ubc18\uc758 Regressor \uc0dd\uc131. DecisionTreeRegressor\uc758 max_depth\ub294 \uac01\uac01 2, 7\r\nlr_reg = LinearRegression()\r\nrf_reg2 = DecisionTreeRegressor(max_depth=2)\r\nrf_reg7 = DecisionTreeRegressor(max_depth= 7)\r\n\r\n#\uc2e4\uc81c \uc608\uce21\uc744 \uc801\uc6a9\ud560 \ud14c\uc2a4\ud2b8\uc6a9 \ub370\uc774\ud130 \uc138\ud2b8\ub97c 4.5~8.5\uae4c\uc9c0\uc758 100\uac1c \ub370\uc774\ud130 \uc138\ud2b8\ub85c \uc0dd\uc131\r\nX_test = np.arange(4.5, 8.5, 0.04).reshape(-1, 1)\r\n\r\n#\ubcf4\uc2a4\ud134 \uc8fc\ud0dd \uac00\uaca9 \ub370\uc774\ud130\uc5d0\uc11c \uc2dc\uac01\ud654\ub97c \uc704\ud574 \ud53c\ucc98\ub294 RM\ub9cc, \uadf8\ub9ac\uace0 \uacb0\uc815 \ub370\uc774\ud130\uc778 PRICE \ucd94\ucd9c\r\nX_feature = bostonDF_sample['RM'].values.reshape(-1, 1)\r\ny_target = bostonDF_sample['PRICE'].values.reshape(-1, 1)\r\n\r\n#\ud559\uc2b5\uacfc \uc608\uce21 \uc218\ud589\r\nlr_reg.fit(X_feature, y_target)\r\nrf_reg2.fit(X_feature, y_target)\r\nrf_reg7.fit(X_feature, y_target)\r\n\r\npred_lr = lr_reg.predict(X_test)\r\npred_rf2 = rf_reg2.predict(X_test)\r\npred_rf7 = rf_reg7.predict(X_test)","aaae0412":"fig, (ax1, ax2, ax3) = plt.subplots(figsize = (14, 4), ncols= 3)\r\n\r\n#X\ucd95 \uac12\uc744 4.5~8.5\ub85c \ubcc0\ud658\ud558\uba70 \uc785\ub825\ud588\uc744 \ub54c \uc120\ud615 \ud68c\uadc0\uc640 \uacb0\uc815 \ud2b8\ub9ac \ud68c\uadc0 \uc608\uce21\uc120 \uc2dc\uac01\ud654\r\n#\uc120\ud615 \ud68c\uadc0\ub85c \ud559\uc2b5\ub41c \ubaa8\ub378 \ud68c\uadc0 \uc608\uce21\uc120\r\nax1.set_title('Linear Regression')\r\nax1.scatter(bostonDF_sample.RM, bostonDF_sample.PRICE, c = 'darkorange')\r\nax1.plot(X_test, pred_lr, label = 'linear', linewidth = 2)\r\n\r\n#DecisionTreeRegressor\uc758 max_depth\ub97c 2\ub85c \ud588\uc744 \ub54c \ud68c\uadc0 \uc608\uce21\uc120\r\nax2.set_title('Decision Tree Regression: \\n max_depth = 2')\r\nax2.scatter(bostonDF_sample.RM, bostonDF_sample.PRICE, c = 'darkorange')\r\nax2.plot(X_test, pred_rf2, label = 'max_depth:3', linewidth = 2)\r\n\r\n#DecisionTreeRegressor\uc758 max_depth\ub97c 7\ub85c \ud588\uc744 \ub54c \ud68c\uadc0 \uc608\uce21\uc120\r\nax3.set_title('Decision Tree Regression: \\n max_depth = 7')\r\nax3.scatter(bostonDF_sample.RM, bostonDF_sample.PRICE, c= 'darkorange')\r\nax3.plot(X_test, pred_rf7, label = 'max_depth:7', linewidth = 2)","310937f3":"import numpy as np\r\nimport pandas as pd\r\nimport seaborn as sns\r\nimport matplotlib.pyplot as plt\r\nimport warnings\r\nwarnings.filterwarnings('ignore', category = RuntimeWarning)\r\n\r\nbike_df = pd.red..\/input\/bike-sharing-demand\/train.csvain.csv')\r\nprint(bike_df.shape)\r\nbike_df.head()","05367f77":"bike_df.info()","8049e222":"#\ubb38\uc790\uc5f4\uc744 datetime \ud0c0\uc785\uc73c\ub85c \ubcc0\uacbd\r\nbike_df['datetime'] = bike_df.datetime.apply(pd.to_datetime)\r\n\r\n#datetime \ud0c0\uc785\uc5d0\uc11c \ub144, \uc6d4, \uc77c, \uc2dc\uac04 \ucd94\ucd9c\r\nbike_df['year'] = bike_df.datetime.apply(lambda x : x.year)\r\nbike_df['month'] = bike_df.datetime.apply(lambda x: x.month)\r\nbike_df['day'] = bike_df.datetime.apply(lambda x : x.day)\r\nbike_df['hour'] = bike_df.datetime.apply(lambda x : x.hour)\r\nbike_df.head(3)","d3d34f20":"drop_columns = ['datetime', 'casual', 'registered']\r\nbike_df.drop(drop_columns, axis = 1, inplace= True)","d3c75f8e":"from sklearn.metrics import mean_squared_error, mean_absolute_error\r\n\r\n#log \uac12 \ubcc0\ud658 \uc2dc NaN \ub4f1\uc758 \uc774\uc288\ub85c log()\uac00 \uc544\ub2cc log1p()\ub97c \uc774\uc6a9\ud574 RMSLE \uacc4\uc0b0\r\ndef rmsle(y, pred):\r\n    log_y = np.log1p(y)\r\n    log_pred = np.log1p(pred)\r\n    squared_error = (log_y - log_pred)**2\r\n    rmsle = np.sqrt(np.mean(squared_error))\r\n    return rmsle\r\n\r\n#\uc0ac\uc774\ud0b7\ub7f0 mean_square_error()\ub97c \uc774\uc6a9\ud574 RMSE \uacc4\uc0b0\r\ndef rmse(y, pred):\r\n    return np.sqrt(mean_squared_error(y, pred))\r\n\r\n#MSE, RMSE, RMSLE\ub97c \ubaa8\ub450 \uacc4\uc0b0\r\ndef evaluate_regr(y, pred):\r\n    rmsle_val = rmsle(y, pred)\r\n    rmse_val = rmse(y, pred)\r\n    #MAE\ub294 \uc0ac\uc774\ud0b7\ub7f0\uc758 mean_absolute_error()\ub85c \uacc4\uc0b0\r\n    mae_val = mean_absolute_error(y, pred)\r\n    print('RMSLE: {0:3f}, RMSE: {1:.3F}, MAE: {2:.3F}'.format(rmsle_val, rmse_val, rmse_val, mae_val))","cbbcfba8":"from sklearn.model_selection import train_test_split, GridSearchCV\r\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\r\n\r\ny_target = bike_df['count']\r\nX_features = bike_df.drop(['count'], axis = 1, inplace = False)\r\n\r\nX_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size = 0.3, random_state= 0)\r\n\r\nlr_reg = LinearRegression()\r\nlr_reg.fit(X_train, y_train)\r\npred = lr_reg.predict(X_test)\r\n\r\nevaluate_regr(y_test, pred)","707083ba":"def get_top_error_data(y_test, pred, n_tops = 5):\r\n    #DataFrame\uc758 \uce7c\ub7fc\uc73c\ub85c \uc2e4\uc81c \ub300\uc5ec \ud69f\uc218(count)\uc640 \uc608\uce21\uac12\uc744 \uc11c\ub85c \ube44\uad50\ud560 \uc218 \uc788\ub3c4\ub85d \uc0dd\uc131.\r\n    result_df = pd.DataFrame(y_test.values, columns=['real_count'])\r\n    result_df['predicted_count'] = np.round(pred)\r\n    result_df['diff'] = np.abs(result_df['real_count'] - result_df['predicted_count'])\r\n    \r\n    #\uc608\uce21\uac12\uacfc \uc2e4\uc81c\uac12\uc758 \ucc28\uc774\uac00 \uac00\uc7a5 \ud070 \ub370\uc774\ud130 \uc21c\uc73c\ub85c \ucd9c\ub825\r\n    print(result_df.sort_values('diff', ascending=False)[:n_tops])\r\n\r\nget_top_error_data(y_test, pred, n_tops= 5)","636ec726":"y_target.hist()","459f573f":"y_log_transform = np.log1p(y_target)\r\ny_log_transform.hist()","2aae8d56":"#\ud0c0\uac9f \uce7c\ub7fc\uc778 count \uac12\uc744 log1p\ub85c \ub85c\uadf8 \ubcc0\ud658\r\ny_target_log = np.log1p(y_target)\r\n\r\n#\ub85c\uadf8 \ubcc0\ud658\ub41c y_target_log\ub97c \ubc18\uc601\ud574 \ud559\uc2b5\/\ud14c\uc2a4\ud2b8 \ub370\uc774\ud130 \uc138\ud2b8 \ubd84\ud560\r\nX_train, X_test, y_train, y_test = train_test_split(X_features, y_target_log, test_size = 0.3, random_state= 0)\r\n\r\nlr_reg = LinearRegression()\r\nlr_reg.fit(X_train, y_train)\r\npred = lr_reg.predict(X_test)\r\n\r\n#\ud14c\uc2a4\ud2b8 \ub370\uc774\ud130 \uc138\ud2b8\uc758 Target \uac12\uc740 \ub85c\uadf8 \ubcc0\ud658\ub410\uc73c\ubbc0\ub85c \ub2e4\uc2dc expm1\uc744 \uc774\uc6a9\ud574 \uc6d0\ub798 \uc2a4\ucf00\uc77c\ub85c \ubcc0\ud658\r\ny_test_exp = np.expm1(y_test)\r\n\r\n#\uc608\uce21\uac12 \uc5ed\uc2dc \ub85c\uadf8 \ubcc0\ud658\ub41c \ud0c0\uae43 \uae30\ubc18\uc73c\ub85c \ud559\uc2b5\ub3fc \uc608\uce21\ub410\uc73c\ubbc0\ub85c \ub2e4\uc2dc expm1\ub85c \uc2a4\ucf00\uc77c \ubcc0\ud658\r\npred_exp = np.expm1(pred)\r\n\r\nevaluate_regr(y_test_exp, pred_exp)","816c999a":"coef = pd.Series(lr_reg.coef_, index = X_features.columns)\r\ncoef_sort = coef.sort_values(ascending=False)\r\nsns.barplot(x = coef_sort.values, y = coef_sort.index)","555de996":"#year, month, day, hour \ub4f1\uc758 \ud53c\ucc98\ub97c One-Hot Encoding\r\nX_features_ohe = pd.get_dummies(X_features, columns = ['year', 'month', 'day', 'hour', 'holiday','workingday', 'season', 'weather'])","4c227d48":"#\uc6d0-\ud56b \uc778\ucf54\ub529\uc774 \uc801\uc6a9\ub41c \ud53c\ucc98 \ub370\uc774\ud130 \uc138\ud2b8\uae30\ubc18\uc73c\ub85c \ud559\uc2b5\/\uc608\uce21 \ub370\uc774\ud130 \ubd84\ud560\r\nX_train, X_test, y_train, y_test = train_test_split(X_features_ohe, y_target_log, test_size=0.3, random_state=0)\r\n\r\n#\ubaa8\ub378\uacfc \ud559\uc2b5\/\ud14c\uc2a4\ud2b8 \ub370\uc774\ud130 \uc138\ud2b8\ub97c \uc785\ub825\ud558\uba74 \uc131\ub2a5 \ud3c9\uac00 \uc218\uce58\ub97c \ubc18\ud658\r\ndef get_model_predict(model, X_train, X_test, y_train, y_test, is_expm1 = False):\r\n    model.fit(X_train, y_train)\r\n    pred = model.predict(X_test)\r\n    if is_expm1 :\r\n        y_test = np.expm1(y_test)\r\n        pred = np.expm1(pred)\r\n    print('###', model.__class__.__name__, '###')\r\n    evaluate_regr(y_test, pred)\r\n#end of function get_model_predict\r\n\r\n#\ubaa8\ub378\ubcc4\ub85c \ud3c9\uac00 \uc218\ud589\r\nlr_reg = LinearRegression()\r\nridge_reg = Ridge(alpha = 10)\r\nlasso_reg = Lasso(alpha=0.01)\r\n\r\nfor model in [lr_reg, ridge_reg, lasso_reg]:\r\n    get_model_predict(model, X_train, X_test, y_train, y_test, is_expm1=True)","c9b368f9":"#\uc6d0-\ud56b \uc778\ucf54\ub529\ub41c \ub370\uc774\ud130 \uc138\ud2b8\uc5d0\uc11c \ud68c\uadc0 \uacc4\uc218\uac00 \ub192\uc740 \ud53c\ucc98\ub97c \ub2e4\uc2dc \uc2dc\uac01\ud654\r\ncoef = pd.Series(lr_reg.coef_, index = X_features_ohe.columns)\r\ncoef_sort = coef.sort_values(ascending=False)[:20]\r\nsns.barplot(x = coef_sort.values, y = coef_sort.index)","25588bb0":"from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\r\nfrom xgboost import XGBRegressor\r\nfrom lightgbm import LGBMRegressor\r\n\r\nrf_reg = RandomForestRegressor(n_estimators= 500)\r\ngbm_reg = GradientBoostingRegressor(n_estimators= 500)\r\nxgb_reg = XGBRegressor(n_estimators = 500)\r\nlgbm_reg = LGBMRegressor(n_estimators = 500)\r\n\r\nfor model in [rf_reg, gbm_reg, xgb_reg, lgbm_reg]:\r\n    #XGBoost\uc758 \uacbd\uc6b0 DataFrame\uc774 \uc785\ub825\ub41c \uacbd\uc6b0 \ubc84\uc804\uc5d0 \ub530\ub77c \uc624\ub958 \ubc1c\uc0dd \uac00\ub2a5. ndarray\ub85c \ubcc0\ud658\r\n    get_model_predict(model, X_train.values, X_test.values, y_train.values, y_test.values, is_expm1=True)","52086555":"import warnings\r\nwarnings.filterwarnings('ignore')\r\nimport pandas as pd\r\nimport numpy as np\r\nimport seaborn as sns\r\nimport matplotlib.pyplot as plt\r\n%matplotlib inline\r\n\r\nhouse_df_org = pd.ra..\/input\/house-prices-advanced-regression-techniques\/train.csvrain.csv')\r\nhouse_df = house_df_org.copy()\r\nhouse_df.head(3)","e4e37d5e":"print('\ub370\uc774\ud130 \uc138\ud2b8\uc758 Shape:', house_df.shape)\r\nprint('\\n\uc804\uccb4 \ud53c\ucc98\uc758 type \\n', house_df.dtypes.value_counts())\r\nisnull_series = house_df.isnull().sum()\r\nprint('\\nNull \uce7c\ub7fc\uacfc \uadf8 \uac74\uc218:\\n', isnull_series[isnull_series > 0].sort_values(ascending = False))","e4d57f88":"plt.title('Original Sale Price Histogram')\r\nsns.distplot(house_df['SalePrice'])","4c83cd09":"plt.title('Log Transfromed Sale Price Histogram')\r\nlog_SalePrice = np.log1p(house_df['SalePrice'])\r\nsns.distplot(log_SalePrice)","1e3943bf":"#SalePrice \ub85c\uadf8 \ubcc0\ud658\r\noriginal_SalePrice = house_df['SalePrice']\r\nhouse_df['SalePrice'] = np.log1p(house_df['SalePrice'])\r\n\r\n#Null\uc774 \ub108\ubb34 \ub9ce\uc740 \uce7c\ub7fc\uacfc \ubd88\ud544\uc694\ud55c \uce7c\ub7fc \uc0ad\uc81c\r\nhouse_df.drop(['Id', 'PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu'], axis = 1, inplace = True)\r\n\r\n#\ub4dc\ub86d\ud558\uc9c0 \uc54a\uc740 \uc22b\uc790\ud615 Null \uce7c\ub7fc\uc740 \ud3c9\uade0\uac12\uc73c\ub85c \ub300\uccb4\r\nhouse_df.fillna(house_df.mean(), inplace = True)\r\n\r\n#Null \uac12\uc774 \uc788\ub294 \ud53c\ucc98\uba85\uacfc \ud0c0\uc785\uc744 \ucd94\ucd9c\r\nnull_column_count = house_df.isnull().sum()[house_df.isnull().sum() > 0]\r\nprint('## Null \ud53c\ucc98\uc758 type:\\n', house_df.dtypes[null_column_count.index])","ad0b7471":"print('get_dummies() \uc218\ud589 \uc804 \ub370\uc774\ud130 Shape:', house_df.shape)\r\nhouse_df_ohe = pd.get_dummies(house_df)\r\nprint('get_dummies() \uc218\ud589 \ud6c4 \ub370\uc774\ud130 Shape:', house_df_ohe.shape)\r\n\r\nnull_column_count = house_df_ohe.isnull().sum()[house_df_ohe.isnull().sum() > 0]\r\nprint('## Null \ud53c\ucc98\uc758 Type: \\n', house_df_ohe.dtypes[null_column_count.index])","4ea9ad54":"def get_rmse(model):\r\n    pred = model.predict(X_test)\r\n    mse = mean_squared_error(y_test, pred)\r\n    rmse = np.sqrt(mse)\r\n    print(model.__class__.__name__, '\ub85c\uadf8 \ubcc0\ud658\ub41c RMSE:', np.round(rmse, 3))\r\n    return rmse\r\n\r\ndef get_rmses(models):\r\n    rmses = []\r\n    for model in models:\r\n        rmse = get_rmse(model)\r\n        rmses.append(rmse)\r\n    return rmses","49d3cbc9":"from sklearn.linear_model import LinearRegression, Ridge, Lasso\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.metrics import mean_squared_error\r\n\r\ny_target = house_df_ohe['SalePrice']\r\nX_features = house_df_ohe.drop('SalePrice', axis = 1, inplace=False)\r\nX_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size = 0.2, random_state=156)\r\n\r\n#LinearRegression, Ridge, Lasso \ud559\uc2b5, \uc608\uce21, \ud3c9\uac00\r\nlr_reg = LinearRegression()\r\nlr_reg.fit(X_train, y_train)\r\nridge_reg = Ridge()\r\nridge_reg.fit(X_train, y_train)\r\nlasso_reg = Lasso()\r\nlasso_reg.fit(X_train, y_train)\r\n\r\nmodels = [lr_reg, ridge_reg, lasso_reg]\r\nget_rmses(models)","544871da":"def get_top_bottom_coef(model, n = 10):\r\n    #coef_ \uc18d\uc131\uc744 \uae30\ubc18\uc73c\ub85c Series \uac1d\uccb4\ub97c \uc0dd\uc131. index\ub294 \uce7c\ub7fc\uba85\r\n    coef = pd.Series(model.coef_, index = X_features.columns)\r\n\r\n    # + \uc0c1\uc704 10\uac1c, - \ud558\uc704 10\uac1c\uc758 \ud68c\uadc0 \uacc4\uc218\ub97c \ucd94\ucd9c\ud574 \ubc18\ud658.\r\n    coef_high = coef.sort_values(ascending=False).head(n)\r\n    coef_low = coef.sort_values(ascending=False).tail(n)\r\n    return coef_high, coef_low","6efc342a":"def visualize_coefficient(models):\r\n    #3\uac1c \ud68c\uadc0 \ubaa8\ub378\uc758 \uc2dc\uac01\ud654\ub97c \uc704\ud574 3\uac1c\uc758 \uce7c\ub7fc\uc744 \uac00\uc9c0\ub294 subplot \uc0dd\uc131\r\n    fig, axs = plt.subplots(figsize = (24, 10), nrows = 1, ncols = 3)\r\n    fig.tight_layout()\r\n    #\uc785\ub825 \uc778\uc790\ub85c \ubc1b\uc740 list \uac1d\uccb4\uc778 models\uc5d0\uc11c \ucc28\ub840\ub85c model\uc744 \ucd94\ucd9c\ud574 \ud68c\uadc0 \uacc4\uc218 \uc2dc\uac01\ud654\r\n    for i_num, model in enumerate(models):\r\n        #\uc0c1\uc704 10\uac1c, \ud558\uc704 10\uac1c \ud68c\uadc0 \uacc4\uc218\ub97c \uad6c\ud558\uace0, \uc774\ub97c \ud310\ub2e4\uc2a4 concat\uc73c\ub85c \uacb0\ud569\r\n        coef_high, coef_low = get_top_bottom_coef(model)\r\n        coef_concat = pd.concat([coef_high, coef_low])\r\n        #ax subplot\uc5d0 barchar\ub85c \ud45c\ud604. \ud55c \ud654\uba74\uc5d0 \ud45c\ud604\ud558\uae30 \uc704\ud574 tick label \uc704\uce58\uc640 font \ud06c\uae30 \uc870\uc815\r\n        axs[i_num].set_title(model.__class__.__name__+' Coeffiecents', size = 25)\r\n        axs[i_num].tick_params(axis = 'y', direction = 'in', pad = -120)\r\n        for label in (axs[i_num].get_xticklabels() + axs[i_num].get_yticklabels()):\r\n            label.set_fontsize(22)\r\n        sns.barplot(x = coef_concat.values, y = coef_concat.index, ax = axs[i_num])\r\n\r\n#\uc55e \uc608\uc81c\uc5d0\uc11c \ud559\uc2b5\ud55c lr_reg, ridge_reg, lasso_reg \ubaa8\ub378\uc758 \ud68c\uadc0 \uacc4\uc218 \uc2dc\uac01\ud654\r\nmodels = [lr_reg, ridge_reg, lasso_reg]\r\nvisualize_coefficient(models)","d56d471b":"from sklearn.model_selection import cross_val_score\r\n\r\ndef get_avg_rmse_cv(models):\r\n\r\n    for model in models: \r\n        #\ubd84\ud560\ud558\uc9c0 \uc54a\uace0 \uc804\uccb4 \ub370\uc774\ud130\ub85c cross_val_score() \uc218\ud589. \ubaa8\ub378\ubcc4 CV RMSE\uac12\uacfc \ud3c9\uade0 RMSE \ucd9c\ub825\r\n        rmse_list = np.sqrt(-cross_val_score(model, X_features, y_target, scoring = 'neg_mean_squared_error', cv = 5))\r\n        rmse_avg = np.mean(rmse_list)\r\n        print('\\n{0} CV RMSE \uac12 \ub9ac\uc2a4\ud2b8: {1}'.format(model.__class__.__name__, np.round(rmse_list, 3)))\r\n        print('{0} CV \ud3c9\uade0 RMSE \uac12 : {1}'.format(model.__class__.__name__, np.round(rmse_avg, 3)))\r\n\r\n#\uc55e \uc608\uc81c\uc5d0\uc11c \ud559\uc2b5\ud55c lr_reg, ridge_reg, lasso_reg \ubaa8\ub378\uc758 CV RMSE \uac12 \ucd9c\ub825\r\nmodels = [lr_reg, ridge_reg, lasso_reg]\r\nget_avg_rmse_cv(models)","7ea3776a":"from sklearn.model_selection import GridSearchCV\r\n\r\ndef print_best_params(model, params):\r\n    grid_model = GridSearchCV(model, param_grid=params, scoring = 'neg_mean_squared_error', cv = 5)\r\n    grid_model.fit(X_features, y_target)\r\n    rmse = np.sqrt(-1*grid_model.best_score_)\r\n    print('{0} 5 CV \uc2dc \ucd5c\uc801 \ud3c9\uade0 RMSE \uac12:{1}, \ucd5c\uc801 alpha:{2}'.format(model.__class__.__name__, np.round(rmse, 4), grid_model.best_params_))\r\n\r\nridge_params = {'alpha': [0.05, 0.1, 1, 5, 8, 10, 12, 15, 20]}\r\nlasso_params = {'alpha': [0.001, 0.005, 0.008, 0.05, 0.03, 0.1, 0.5, 1, 5, 10]}\r\nprint_best_params(ridge_reg, ridge_params)\r\nprint_best_params(lasso_reg, lasso_params)","77145a82":"#\uc55e\uc758 \ucd5c\uc801\ud654 alpha \uac12\uc73c\ub85c \ud559\uc2b5 \ub370\uc774\ud130\ub85c \ud559\uc2b5, \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130\ub85c \uc608\uce21 \ubc12 \ud3c9\uac00 \uc218\ud589\r\nlr_reg = LinearRegression()\r\nlr_reg.fit(X_train, y_train)\r\nridge_reg = Ridge(alpha= 12)\r\nridge_reg.fit(X_train, y_train)\r\nlasso_reg = Lasso(alpha = 0.001)\r\nlasso_reg.fit(X_train, y_train)\r\n\r\n#\ubaa8\ub4e0 \ubaa8\ub378\uc758 RMSE \ucd9c\ub825\r\nmodels = [lr_reg, ridge_reg, lasso_reg]\r\nget_rmses(models)\r\n\r\n#\ubaa8\ub4e0 \ubaa8\ub378\uc758 \ud68c\uadc0 \uacc4\uc218 \uc2dc\uac01\ud654\r\nmodels = [lr_reg, ridge_reg, lasso_reg]\r\nvisualize_coefficient(models)","3928a43c":"from scipy.stats import skew\r\n\r\n#object\uac00 \uc544\ub2cc \uc22b\uc790\ud615 \ud53c\ucc98\uc758 \uce7c\ub7fc index \uac1d\uccb4 \ucd94\ucd9c\r\nfeatures_index = house_df.dtypes[house_df.dtypes != 'object'].index\r\n#house_df\uc5d0 \uce7c\ub7fc index\ub97c []\ub85c \uc785\ub825\ud558\uba74 \ud574\ub2f9\ud558\ub294 \uce7c\ub7fc \ub370\uc774\ud130 \uc138\ud2b8 \ubc18\ud658. apply lambda\ub85c skew() \ud638\ucd9c\r\nskew_features = house_df[features_index].apply(lambda x : skew(x))\r\n#skew(\uc65c\uace1) \uc815\ub3c4\uac00 1\uc774\uc0c1\uc778 \uce7c\ub7fc\ub9cc \ucd94\ucd9c\r\nskew_features_top = skew_features[skew_features > 1]\r\nprint(skew_features_top.sort_values(ascending = False))","1ce5e3cd":"#\ub85c\uadf8 \ubcc0\ud658\r\nhouse_df[skew_features_top.index] = np.log1p(house_df[skew_features_top.index])","cedca478":"#\uc65c\uace1 \uc815\ub3c4\uac00 \ub192\uc740 \ud53c\ucc98\ub97c \ub85c\uadf8 \ubcc0\ud658\ud588\uc73c\ubbc0\ub85c \ub2e4\uc2dc \uc6d0-\ud56b \uc778\ucf54\ub529\uc744 \uc801\uc6a9\ud558\uace0 \ud53c\ucc98\/\ud0c0\uae43 \ub370\uc774\ud130 \uc138\ud2b8 \uc0dd\uc131\r\nhouse_df_ohe = pd.get_dummies(house_df)\r\ny_target = house_df_ohe['SalePrice']\r\nX_features = house_df_ohe.drop('SalePrice', axis = 1, inplace = False)\r\nX_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size = 0.2, random_state= 156)\r\n\r\n#\ud53c\ucc98\ub97c \ub85c\uadf8 \ubcc0\ud658\ud55c \ud6c4 \ub2e4\uc2dc \ucd5c\uc801 \ud558\uc774\ud37c \ud30c\ub77c\ubbf8\ud130\uc640 RMSE \ucd9c\ub825\r\nridge_params = {'alpha' : [0.05, 0.1, 1, 5, 8, 10, 12, 15, 20]}\r\nlasso_params = {'alpha' : [0.001, 0.005, 0.008, 0.05, 0.03, 0.1, 0.5, 1, 5, 10]}\r\nprint_best_params(ridge_reg, ridge_params)\r\nprint_best_params(lasso_reg, lasso_params)","ae23e246":"#GrLivArea\uc640 \ud0c0\uae43 \uac12\uc778 SalePrice\uc758 \uad00\uacc4\ub97c \uc2dc\uac01\ud654\r\nplt.scatter(x = house_df_org['GrLivArea'], y = house_df_org['SalePrice'])\r\nplt.ylabel('SalePrice', fontsize = 15)\r\nplt.xlabel('GrLivArea', fontsize = 15)\r\nplt.show()","5a2becf8":"#GrLivArea\uc640 SalePrice \ubaa8\ub450 \ub85c\uadf8 \ubcc0\ud658\ub410\uc73c\ubbc0\ub85c \uc774\ub97c \ubc18\uc601\ud55c \uc870\uac74 \uc0dd\uc131\r\ncond1 = house_df_ohe['GrLivArea'] > np.log1p(4000)\r\ncond2 = house_df_ohe['SalePrice'] < np.log1p(500000)\r\noutlier_index = house_df_ohe[cond1 & cond2].index\r\n\r\nprint('\uc774\uc0c1\uce58 \ub808\ucf54\ub4dc index :', outlier_index.values)\r\nprint('\uc774\uc0c1\uce58 \uc0ad\uc81c \uc804 house_df_ohe shape:', house_df_ohe.shape)\r\n\r\n#DataFrame\uc758 \uc778\ub371\uc2a4\ub97c \uc774\uc6a9\ud574 \uc774\uc0c1\uce58 \ub808\ucf54\ub4dc \uc0ad\uc81c\r\nhouse_df_ohe.drop(outlier_index, axis = 0, inplace=True)\r\nprint('\uc774\uc0c1\uce58 \uc0ad\uc81c \ud6c4 house_df_ohe shape:', house_df_ohe.shape)","6883663b":"y_target = house_df_ohe['SalePrice']\r\nX_features = house_df_ohe.drop('SalePrice', axis = 1, inplace= False)\r\nX_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.2, random_state=156)\r\n\r\nridge_params = {'alpha' : [0.05, 0.1, 1, 5, 8, 10, 12, 15, 20]}\r\nlasso_params = {'alpha' : [0.001, 0.005, 0.008, 0.05, 0.03, 0.1, 0.5, 1, 5, 10]}\r\nprint_best_params(ridge_reg, ridge_params)\r\nprint_best_params(lasso_reg, lasso_params)","049cb420":"#XGBoost, LightGBM \uc0ac\uc6a9 -> \uc218\ud589 \uc2dc\uac04\uc774 \uc624\ub798 \uac78\ub9ac\ubbc0\ub85c \ud558\uc774\ud37c \ud30c\ub77c\ubbf8\ud130 \uc124\uc815\uc744 \ubbf8\ub9ac \uc801\uc6a9 & 5 \ud3f4\ub4dc \uc138\ud2b8\uc5d0 \ub300\ud55c \ud3c9\uade0 RMSE \uac12\uc744 \uad6c\ud568\r\nfrom xgboost import XGBRegressor\r\n\r\nxgb_params = {'n_estimators' : [1000]}\r\nxgb_reg = XGBRegressor(n_estimators = 1000, learning_rate = 0.05, colsample_bytree = 0.5, subsample = 0.8)\r\nprint_best_params(xgb_reg, xgb_params)","1b42f4d9":"from lightgbm import LGBMRegressor\r\n\r\nlgbm_params = {'n_estimators' : [1000]}\r\nlgbm_reg = LGBMRegressor(n_estimators = 1000, learning_rate = 0.05, num_leaves = 4, subsample = 0.6, colsample_bytree = 0.4, reg_lambda = 10, n_jobs = -1)\r\nprint_best_params(lgbm_reg, lgbm_params)","b44ca7a0":"def get_rmse_pred(preds):\r\n    for key in preds.keys():\r\n        pred_value = preds[key]\r\n        mse = mean_squared_error(y_test, pred_value)\r\n        rmse = np.sqrt(mse)\r\n        print('{0} \ubaa8\ub378\uc758 RMSE : {1}'.format(key, rmse))\r\n\r\n#\uac1c\ubcc4 \ubaa8\ub378\uc758 \ud559\uc2b5\r\nridge_reg = Ridge(alpha = 8)\r\nridge_reg.fit(X_train, y_train)\r\nlasso_reg = Lasso(alpha = 0.001)\r\nlasso_reg.fit(X_train, y_train)\r\n#\uac1c\ubcc4 \ubaa8\ub378 \uc608\uce21\r\nridge_pred = ridge_reg.predict(X_test)\r\nlasso_pred = lasso_reg.predict(X_test)\r\n\r\n#\uac1c\ubcc4 \ubaa8\ub378 \uc608\uce21\uac12 \ud63c\ud569\uc73c\ub85c \ucd5c\uc885 \uc608\uce21\uac12 \ub3c4\ucd9c\r\npred = 0.4*ridge_pred + 0.6*lasso_pred\r\npreds = {'\ucd5c\uc885 \ud63c\ud569' : pred, 'Ridge' : ridge_pred, 'Lasso' : lasso_pred}\r\n\r\n#\ucd5c\uc885 \ud63c\ud569 \ubaa8\ub378, \uac1c\ubcc4 \ubaa8\ub378\uc758 RMSE \uac12 \ucd9c\ub825\r\nget_rmse_pred(preds)","4fa49a83":"xgb_reg = XGBRegressor(n_estimators = 1000, learning_rate = 0.05, colsample_bytree = 0.5, subsample = 0.8)\r\nlgbm_reg = LGBMRegressor(n_estimators= 1000, learning_rate= 0.05, num_leaves = 4, subsample= 0.6, colsample_bytree=0.4, reg_lambda=10, n_jobs= -1)\r\nxgb_reg.fit(X_train, y_train)\r\nlgbm_reg.fit(X_train, y_train)\r\nxgb_pred = xgb_reg.predict(X_test)\r\nlgbm_pred = lgbm_reg.predict(X_test)\r\n\r\npred = 0.5*xgb_pred + 0.5*lgbm_pred\r\npreds = {'\ucd5c\uc885 \ud63c\ud569' : pred, 'XGBM' : xgb_pred, 'LGBM' : lgbm_pred}\r\nget_rmse_pred(preds)","51c74859":"* \uc8fc\uac70 \uacf5\uac04\uc774 \ud070 \uc9d1\uc77c\uc218\ub85d \uac00\uaca9\uc774 \ube44\uc2f8\uae30 \ub54c\ubb38\uc5d0 GrLivArea\ud53c\ucc98\ub294 SalePrice\uc640 \uc591\uc758 \uc0c1\uad00\uad00\ub3c4\uac00 \ub9e4\uc6b0 \ub192\uc74c\r\n* \uc624\ub978\ucabd \ubc11 \ub450\uac1c\uc758 \ub370\uc774\ud130\ub294 \uc774\uc0c1\uce58\ub85c \uac04\uc8fc(GrLivArea\uac00 400\ud3c9\ubc29\ud53c\ud2b8 \uc774\uc0c1\uc784\uc5d0\ub3c4 \uac00\uaca9\uc774 500,000\ub2ec\ub7ec \uc774\ud558\uc778 \ub370\uc774\ud130\ub294 \ubaa8\ub450 \uc774\uc0c1\uce58\ub85c \uac04\uc8fc)","2aa6a745":"\ud68c\uadc0 \ud2b8\ub9ac\ub97c \uc774\uc6a9\ud574 \ud68c\uadc0 \uc608\uce21\uc744 \uc218\ud589\r\n* \ub79c\ub364 \ud3ec\ub808\uc2a4\ud2b8, GBM, XGBoost, LightGBM\uc744 \uc21c\ucc28\uc801\uc73c\ub85c \uc131\ub2a5 \ud3c9\uac00","e898e6de":"\ub2e8 \ub450 \uac1c\uc758 \uc774\uc0c1\uce58 \ub370\uc774\ud130\ub9cc \uc81c\uac70\ud588\ub294\ub370\ub3c4 \uc608\uce21 \uc218\uce58\uac00 \ub9e4\uc6b0 \ud06c\uac8c \ud5a5\uc0c1 -> \uc6ec\ub9cc\ud07c \ud558\uc774\ud37c \ud30c\ub77c\ubbf8\ud130 \ud29c\ub2dd\uc744 \ud574\ub3c4 \uc774 \uc815\ub3c4\uc758 \uc218\uce58 \uac1c\uc120\uc740 \uc5b4\ub824\uc6c0\r\n* GrLivArea \uc18d\uc131\uc774 \ud68c\uadc0 \ubaa8\ub378\uc5d0\uc11c \ucc28\uc9c0\ud558\ub294 \uc601\ud5a5\ub3c4\uac00 \ud06c\uae30\uc5d0 \uc774 \uc774\uc0c1\uce58\ub97c \uac1c\uc120\ud558\ub294 \uac83\uc774 \uc131\ub2a5 \uac1c\uc120\uc5d0 \ud070 \uc758\ubbf8\ub97c \uac00\uc9d0\r\n","563538fc":"RMSLE \uc624\ub958\ub294 \uc904\uc5b4\ub4e4\uc5c8\uc9c0\ub9cc, RMSE\ub294 \uc624\ud788\ub824 \ub298\uc5b4\ub0a8. \uc774\uc720\ub294?","e2ec65cb":"## \uc120\ud615 \ud68c\uadc0 \ubaa8\ub378 \ud559\uc2b5\/\uc608\uce21\/\ud3c9\uac00","bdf7cb46":"\uc5ec\uc804\ud788 Lasso\uc758 \uacbd\uc6b0\uac00 OLS \ubaa8\ub378\uc774\ub098 \ub9bf\uc9c0 \ubaa8\ub378\ubcf4\ub2e4 \uc131\ub2a5\uc774 \ub5a8\uc5b4\uc9d0, \ub9bf\uc9c0\uc640 \ub77c\uc3d8 \ubaa8\ub378\uc5d0 \ub300\ud574\uc11c alpha \ud558\uc774\ud37c \ud30c\ub77c\ubbf8\ud130\ub97c \ubcc0\ud654\uc2dc\ud0a4\uba74\uc11c \ucd5c\uc801 \uac12\uc744 \ub3c4\ucd9c","55a05bbc":"2011\ub144 1\uc6d4\ubd80\ud130 2012\ub144 12\uc6d4\uae4c\uc9c0 \ub0a0\uc9dc\/\uc2dc\uac04, \uae30\uc628, \uc2b5\ub3c4, \ud48d\uc18d \ub4f1\uc758 \uc815\ubcf4\ub97c \uae30\ubc18\uc73c\ub85c 1\uc2dc\uac04 \uac04\uaca9 \ub3d9\uc548\uc758 \uc790\uc804\uac70 \ub300\uc5ec \ud69f\uc218\uac00 \uae30\uc7ac\r\n* \uacb0\uc815 \uac12\uc740 \ub9e8 \ub9c8\uc9c0\ub9c9 \uce7c\ub7fc\uc778 count('\ub300\uc5ec \ud69f\uc218')\r\n* datetime : hourly date + timestamp\r\n* season : 1 = \ubd04, 2 = \uc5ec\ub984, 3 = \uac00\uc744, 4 = \uaca8\uc6b8\r\n* holiday : 1 = \ud1a0, \uc77c\uc694\uc77c\uc758 \uc8fc\ub9d0\uc744 \uc81c\uc678\ud55c \uad6d\uacbd\uc77c \ub4f1\uc758 \ud734\uc77c, 0 = \ud734\uc77c\uc774 \uc544\ub2cc \ub0a0\r\n* workingday : 1 = \ud1a0, \uc77c\uc694\uc77c\uc758 \uc8fc\ub9d0 \ubc0f \ud734\uc77c\uc774 \uc544\ub2cc \uc8fc\uc911, 0 = \uc8fc\ub9d0 \ubc0f \ud734\uc77c\r\n* weather : 1 = \ub9d1\uc74c, \uc57d\uac04 \uad6c\ub984 \ub080 \ud750\ub9bc\/2 = \uc548\uac1c, \uc548\uac1c + \ud750\ub9bc\/3 = \uac00\ubcbc\uc6b4 \ub208, \uac00\ubcbc\uc6b4 \ube44 + \ucc9c\ub465\/4 = \uc2ec\ud55c \ub208 or \ube44, \ucc9c\ub465 or \ubc88\uac1c\r\n* temp : \uc628\ub3c4(\uc12d\uc528)\r\n* atemp : \uccb4\uac10\uc628\ub3c4(\uc12d\uc528)\r\n* humidity : \uc0c1\ub300\uc2b5\ub3c4\r\n* windspeed : \ud48d\uc18d\r\n* casual : \uc0ac\uc804\uc5d0 \ub4f1\ub85d\ub418\uc9c0 \uc54a\uc740 \uc0ac\uc6a9\uc790\uac00 \ub300\uc5ec\ud55c \ud69f\uc218\r\n* registered : \uc0ac\uc804\uc5d0 \ub4f1\ub85d\ub41c \uc0ac\uc6a9\uc790\uac00 \ub300\uc5ec\ud55c \ud69f\uc218\r\n* count : \ub300\uc5ec \ud69f\uc218 ","91f6a5f4":"* OLS \uae30\ubc18\uc758 LinearRegression\uacfc Ridge\uc758 \uacbd\uc6b0\ub294 \ud68c\uadc0 \uacc4\uc218\uac00 \uc720\uc0ac\ud55c \ud615\ud0dc\ub85c \ubd84\ud3ec\r\n* Lasso\ub294 \uc804\uccb4\uc801\uc73c\ub85c \ud68c\uadc0 \uacc4\uc218\uac12\uc774 \ub9e4\uc6b0 \uc791\uace0, \uadf8 \uc911\uc560 YearBuilt\uac00 \uac00\uc7a5 \ud06c\uace0 \ub2e4\ub978 \ud53c\ucc98\uc758 \ud68c\uadc0 \uacc4\uc218\uac00 \ub108\ubb34 \uc791\uc74c","9efd805e":"## \ud68c\uadc0 \ud2b8\ub9ac \ubaa8\ub378 \ud559\uc2b5\/\uc608\uce21\/\ud3c9\uac00","20e57bc4":"\ud68c\uadc0 \ud2b8\ub9ac Regressor\uac00 \uc5b4\ub5bb\uac8c \uc608\uce21\uac12\uc744 \ud310\ub2e8\ud558\ub294\uc9c0 \uc120\ud615 \ud68c\uadc0\uc640 \ube44\uad50&\uc2dc\uac01\ud654\r\n* max_depth\uc758 \ud06c\uae30\ub97c \ubcc0\ud654\uc2dc\ud0a4\uba74\uc11c \uc5b4\ub5bb\uac8c \ud68c\uadc0 \ud2b8\ub9ac \uc608\uce21\uc120\uc774 \ubcc0\ud654\ud558\ub294\uc9c0 \ud655\uc778\r\n* Price\uc640 \uac00\uc7a5 \ubc00\uc811\ud55c \uc591\uc758 \uc0c1\uad00\uad00\uacc4\ub97c \uac00\uc9c0\ub294 RM \uce7c\ub7fc\ub9cc \uc774\uc6a9\ud574\uc11c \uc120\ud615 \ud68c\uadc0\uc640 \uacb0\uc815 \ud2b8\ub9ac \ud68c\uadc0\ub85c PRICE \uc608\uce21 \ud68c\uadc0\uc120\uc744 \ud45c\ud604","06843607":"\ud53c\ucc98 \ub370\uc774\ud130 \uc138\ud2b8\uc5d0\uc11c \uc65c\uace1\ub41c \ud53c\ucc98\uac00 \uc874\uc7ac\ud560 \uacbd\uc6b0 \ud68c\uadc0 \uc608\uce21 \uc131\ub2a5\uc744 \uc800\ud558\r\n* scipy stats \ubaa8\ub4c8\uc758 skew() \ud568\uc218\ub97c \uc774\uc6a9\ud574 \uce7c\ub7fc\uc758 \ub370\uc774\ud130 \uc138\ud2b8\uc758 \uc65c\uace1\ub41c \uc815\ub3c4\ub97c \uc27d\uac8c \ucd94\ucd9c\uac00\ub2a5\r\n* skew() \ud568\uc218\uc758 \ubc18\ud658 \uac12\uc774 1\uc774\uc0c1\uc778 \uacbd\uc6b0 \uc65c\uace1 \uc815\ub3c4\uac00 \ub192\ub2e4\uace0 \ud310\ub2e8\r\n* 1 \uc774\uc0c1\uc758 \uac12\uc744 \ubc18\ud658\ud558\ub294 \ud53c\ucc98\ub9cc \ucd94\ucd9c\ud574 \ub85c\uadf8 \ubcc0\ud658 \uc801\uc6a9\r\n* \uc8fc\uc758\ud560 \uc810 : \uc6d0-\ud56b \uc778\ucf54\ub529\ub41c \uce74\ud14c\uace0\ub9ac \uc22b\uc790\ud615 \ud53c\ucc98\ub294 \uc81c\uc678","3f0e4104":"\ub9bf\uc9c0\uc640 \ub77c\uc3d8 \ubaa8\ub378\uc5d0\uc11c \ube44\uc2b7\ud55c \ud53c\ucc98\uc758 \ud68c\uadc0 \uacc4\uc218\uac00 \ub192\uc74c. \ub2e4\ub9cc, \ub77c\uc3d8 \ubaa8\ub378\uc758 \uacbd\uc6b0\ub294 \ub9bf\uc9c0\uc5d0 \ube44\ud574 \ub3d9\uc77c\ud55c \ud53c\ucc98\ub77c\ub3c4 \ud68c\uadc0 \uacc4\uc218\uc758 \uac12\uc774 \uc0c1\ub2f9\ud788 \uc791\uc74c","36f02c89":"Year \ud53c\ucc98\uc758 \ud68c\uadc0 \uacc4\uc218 \uac12\uc774 \ub3c5\ubcf4\uc801\uc73c\ub85c \ud070\uac12\r\n* year\ub294 2011, 2012\ub144 \ub450\uac1c\uc758 \uac12\uc73c\ub85c \ub3fc\uc788\uc74c -> year\uc5d0 \ub530\ub77c\uc11c \uc790\uc804\uac70 \ub300\uc5ec \ud69f\uc218\uac00 \ud06c\uac8c \uc601\ud5a5\uc744 \ubc1b\ub294\ub2e4\ub294 \uac83\uc740 \ub0a9\ub4dd\ud558\uae30 \uc5b4\ub824\uc6c0\r\n* year \ud53c\ucc98\ub294 \uc5f0\ub3c4\ub97c \ub73b\ud558\ubbc0\ub85c \uce74\ud14c\uace0\ub9ac\ud615 \ud53c\ucc98\uc9c0\ub9cc, \uc22b\uc790\ud615 \uac12\uc73c\ub85c \ub3fc\uc788\uc74c & \uc544\uc8fc \ud070 \uac12\uc774 2011, 2012\ub85c \ub3fc \uc788\uc74c\r\n* \uc774\ucc98\ub7fc \uc22b\uc790\ud615 \uce74\ud14c\uace0\ub9ac \uac12\uc744 \uc120\ud615 \ud68c\uadc0\uc5d0 \uc0ac\uc6a9\ud560 \uacbd\uc6b0 \ud68c\uadc0 \uacc4\uc218\ub97c \uc5f0\uc0b0\ud560 \ub54c \uc774 \uc22b\uc790\ud615 \uac12\uc5d0 \ud06c\uac8c \uc601\ud5a5\uc744 \ubc1b\ub294 \uacbd\uc6b0\uac00 \ubc1c\uc0dd\r\n* \uc120\ud615 \ud68c\uadc0\uc5d0\uc11c\ub294 \uc774\ub7ec\ud55c \ud53c\ucc98 \uc778\ucf54\ub529\uc5d0 \uc6d0-\ud56b \uc778\ucf54\ub529\uc744 \uc801\uc6a9\ud574 \ubcc0\ud658\ud574\uc57c\ud568","ac133ab6":"Null \uac12\uc740 \uc5c6\uc73c\uba70 \ub300\ubd80\ubd84\uc758 \uce7c\ub7fc\uc774 int \ub610\ub294 float \uc22b\uc790\ud615\uc778\ub370, datetime \uce7c\ub7fc\ub9cc object \ud615\r\n* Datetime \uce7c\ub7fc\uc758 \uacbd\uc6b0 \ub144-\uc6d4-\uc77c \uc2dc:\ubd84:\ucd08 \ubb38\uc790 \ud615\uc2dd -> \uac00\uacf5 \ud544\uc694\r\n* datetime\uc744 \ub144, \uc6d4, \uc77c, \uadf8\ub9ac\uace0 \uc2dc\uac04\uacfc \uac19\uc774 4\uac1c\uc758 \uc18d\uc131\uc73c\ub85c \ubd84\ub9ac\r\n* \ud310\ub2e4\uc2a4\uc5d0\uc11c\ub294 datetime\uacfc \uac19\uc740 \ud615\ud0dc\uc758 \ubb38\uc790\uc5f4\uc744 \ub144\ub3c4, \uc6d4, \uc77c, \uc2dc\uac04, \ubd84, \ucd08\ub85c \ud3b8\ub9ac\ud558\uac8c \ubcc0\ud658\ud558\ub824\uba74 \uba3c\uc800 \ubb38\uc790\uc5f4\uc744 'datetime'\ud0c0\uc785\uc73c\ub85c \ubcc0\uacbd\r\n* \ud310\ub2e4\uc2a4\ub294 \ubb38\uc790\uc5f4\uc744 datetime \ud0c0\uc785\uc73c\ub85c \ubcc0\ud658\ud558\ub294 apply(pd.to_datetime)\uba54\uc11c\ub4dc\ub97c \uc81c\uacf5","0d50f1cc":"0~200 \uc0ac\uc774\uc5d0 \uc65c\uace1\ub3fc \uc788\uc74c\r\n* \uc774\ub807\uac8c \uc65c\uace1\ub41c \uac12\uc744 \uc815\uaddc \ubd84\ud3ec \ud615\ud0dc\ub85c \ubc14\uafb8\ub294 \uac00\uc7a5 \uc77c\ubc18\uc801\uc778 \ubc29\ubc95\uc740 \ub85c\uadf8\ub97c \uc801\uc6a9\ud574 \ubcc0\ud658","05ccd69e":"## \ud68c\uadc0 \ubaa8\ub378\uc758 \uc608\uce21 \uacb0\uacfc \ud63c\ud569\uc744 \ud1b5\ud55c \ucd5c\uc885 \uc608\uce21","6cf9e5d1":"\uc774\uc0c1\uce58 \ucc98\ub9ac","a95b6426":"\uc2e4\uc81c \uac12\uacfc \uc608\uce21\uac12\uc774 \uc5b4\ub290 \uc815\ub3c4 \ucc28\uc774\uac00 \ub098\ub294\uc9c0 DataFrame\uc758 \uce7c\ub7fc\uc73c\ub85c \ub9cc\ub4e4\uc5b4\uc11c \uc624\ub958 \uac12\uc774 \uac00\uc7a5 \ud070 \uc21c\uc73c\ub85c 5\uac1c\ub9cc \ud655\uc778\r\n","32d08bf8":"\ud68c\uadc0 \ubaa8\ub378\uc744 \uc801\uc6a9\ud558\uae30 \uc804\uc5d0 \ud0c0\uae43 \uac12\uc758 \ubd84\ud3ec\ub3c4\uac00 \uc815\uaddc \ubd84\ud3ec\uc778\uc9c0 \ud655\uc778","0f10381a":"## \ud68c\uadc0 \uc2e4\uc2b5 - \uc790\uc804\uac70 \ub300\uc5ec \uc218\uc694 \uc608\uce21","4f9fea24":"* \ub370\uc774\ud130 \uc138\ud2b8\ub294 1460\uac1c\uc758 \ub808\ucf54\ub4dc\uc640 81\uac1c\uc758 \ud53c\ucc98\ub85c \uad6c\uc131\r\n* Target\uc744 \uc81c\uc678\ud55c 80\uac1c\uc758 \ud53c\ucc98 \uc911 43\uac1c\uac00 \ubb38\uc790\ud615, \ub098\uba38\uc9c0\uac00 \uc22b\uc790\ud615\r\n* Null\uac12\uc774 \ub108\ubb34 \ub9ce\uc740 \ud53c\ucc98\ub294 drop","f2981adf":"## \ud68c\uadc0 \ud2b8\ub9ac\r\n\ud68c\uadc0 \ud568\uc218\ub97c \uae30\ubc18\uc73c\ub85c \ud558\uc9c0 \uc54a\uace0 \uacb0\uc815 \ud2b8\ub9ac\uc640 \uac19\uc774 \ud2b8\ub9ac\ub97c \uae30\ubc18\uc73c\ub85c \ud558\ub294 \ud68c\uadc0 \ubc29\uc2dd\r\n* \ud2b8\ub9ac \uae30\ubc18\uc758 \ud68c\uadc0 -> \ud68c\uadc0 \ud2b8\ub9ac\ub97c \uc774\uc6a9. \uc989, \ud68c\uadc0\ub97c \uc704\ud55c \ud2b8\ub9ac\ub97c \uc0dd\uc131\ud558\uace0 \uc774\ub97c \uae30\ubc18\uc73c\ub85c \ud68c\uadc0 \uc608\uce21\r\n* \ud68c\uadc0 \ud2b8\ub9ac\ub294 \ub9ac\ud504 \ub178\ub4dc\uc5d0 \uc18d\ud55c \ub370\uc774\ud130 \uac12\uc758 \ud3c9\uade0\uac12\uc744 \uad6c\ud574 \ud68c\uadc0 \uc608\uce21\uac12\uc744 \uacc4\uc0b0\r\n* \uacb0\uc815 \ud2b8\ub9ac, \ub79c\ub364 \ud3ec\ub808\uc2a4\ud2b8, GBM, XGBoost, LightGBM \ub4f1 \ubaa8\ub4e0 \ud2b8\ub9ac \uae30\ubc18\uc758 \uc54c\uace0\ub9ac\uc998\uc740 \ubd84\ub958\ubfd0\ub9cc \uc544\ub2c8\ub77c \ud68c\uadc0\ub3c4 \uac00\ub2a5 -> CART \uc54c\uace0\ub9ac\uc998\uc5d0 \uae30\ubc18\ud558\uace0 \uc788\uae30 \ub54c\ubb38\r\n* CART : \ubd84\ub958\ubfd0\ub9cc \uc544\ub2c8\ub77c \ud68c\uadc0\ub3c4 \uac00\ub2a5\ud558\uac8c \ud574\uc8fc\ub294 \ud2b8\ub9ac \uc0dd\uc131 \uc54c\uace0\ub9ac\uc998\r\n\r\n\r\n","968a191d":"\uc6d0-\ud56b \uc778\ucf54\ub529 \uc801\uc6a9 \ud6c4 \uc131\ub2a5\uc774 \ub300\ud3ed \ud5a5\uc0c1","e0a50f21":"## \ud68c\uadc0 \uc2e4\uc2b5 - \uce90\uae00 \uc8fc\ud0dd \uac00\uaca9 : \uace0\uae09 \ud68c\uadc0 \uae30\ubc95","42ea1dea":"LinearRegression\uacfc DecisionTreeRegressor\ub97c max_depth\ub97c \uac01\uac01 2, 7\ub85c \ud559\uc2b5\r\n* \uc774\ub807\uac8c \ud559\uc2b5\ub41c Regressor\uc5d0 RM\uac12\uc744 4.5~8.5\uae4c\uc9c0\uc758 100\uac1c\uc758 \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130 \uc138\ud2b8\ub85c \uc81c\uacf5\ud588\uc744 \ub54c \uc608\uce21\uac12\uc744 \uad6c\ud558\uae30","03a45ec2":"\uc815\uaddc \ubd84\ud3ec\uac00 \uc544\ub2cc \uacb0\uad0e\uac12\uc744 \uc815\uaddc \ubd84\ud3ec \ud615\ud0dc\ub85c \ubcc0\ud658\ud558\uae30 \uc704\ud574 \ub85c\uadf8 \ubcc0\ud658\uc744 \uc801\uc6a9","53a09667":"Lasso \ubaa8\ub378\uc758 \uacbd\uc6b0, alpha\uac12 \ucd5c\uc801\ud654 \uc774\ud6c4 \uc608\uce21 \uc131\ub2a5\uc774 \ub9ce\uc774 \ud5a5\uc0c1 -> \uc120\ud615 \ubaa8\ub378\uc5d0 \ucd5c\uc801 alpha \uac12\uc744 \uc124\uc815\ud55c \ub4a4, train_test_split()\uc73c\ub85c \ubd84\ud560\ub41c \ud559\uc2b5 \ub370\uc774\ud130\uc640 \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130\ub97c \uc774\uc6a9\ud574 \ubaa8\ub378\uc758 \ud559\uc2b5\/\uc608\uce21\/\ud3c9\uac00\ub97c \uc218\ud589","fd814294":"\uac00\uc7a5 \ud070 \uc0c1\uc704 5\uc704 \uc624\ub958\uac12\uc740 546~568\ub85c \uc2e4\uc81c \uac12\uc744 \uac10\uc548\ud558\uba74 \uc608\uce21 \uc624\ub958\uac00 \ud07c\r\n* \ud68c\uadc0\uc5d0\uc11c \uc774\ub807\uac8c \ud070 \uc608\uce21 \uc624\ub958\uac00 \ubc1c\uc0dd\ud560 \uacbd\uc6b0 \uac00\uc7a5 \uba3c\uc800 \uc0b4\ud3b4\ubcfc \uac83\uc740 Target \uac12\uc758 \ubd84\ud3ec\uac00 \uc65c\uace1\ub41c \ud615\ud0dc\ub97c \uc774\ub8e8\uace0 \uc788\ub294\uc9c0 \ud655\uc778\r\n* Target \uac12\uc758 \ubd84\ud3ec\ub294 \uc815\uaddc \ubd84\ud3ec \ud615\ud0dc\uac00 \uac00\uc7a5 \uc88b\uc74c","33d5b492":"* datetime \uce7c\ub7fc \uc0ad\uc81c\r\n* casual \uce7c\ub7fc\uc740 \uc0ac\uc804\uc5d0 \ub4f1\ub85d\ud558\uc9c0 \uc54a\uc740 \uc0ac\uc6a9\uc790\uc758 \uc790\uc804\uac70 \ub300\uc5ec \ud69f\uc218\r\n* registered : \uc0ac\uc804\uc5d0 \ub4f1\ub85d\ud55c \uc0ac\uc6a9\uc790\uc758 \ub300\uc5ec \ud69f\uc218\r\n* casual + registered = count -> casual\uacfc registered\uac00 \ub530\ub85c \ud544\uc694\ud558\uc9c0\ub294 \uc54a\uc74c (\uc624\ud788\ub824 \uc0c1\uad00\ub3c4\uac00 \ub192\uc544 \uc608\uce21\uc744 \uc800\ud574\ud560 \uc6b0\ub824\uac00 \uc788\uc74c)","377f2c65":"## \ub370\uc774\ud130 \uc0ac\uc804 \ucc98\ub9ac(Preprocessing)","dfe780bf":"## \ub370\uc774\ud130 \ud074\ub80c\uc9d5 \ubc0f \uac00\uacf5","b25f783a":"\uc608\uce21 \ud3c9\uac00\ub294 RMSLE But, \uc774\ubbf8 SalePrice\uac00 \ub85c\uadf8 \ubcc0\ud658 -> \uc608\uce21 \uacb0\uacfc \uc624\ub958\uc5d0 RMSE\ub9cc \uc801\uc6a9\ud558\uba74 RMSLE\uac00 \uc790\ub3d9\uc73c\ub85c \uce21\uc815","9126af46":"## \ub85c\uadf8 \ubcc0\ud658, \ud53c\ucc98 \uc778\ucf54\ub529\uacfc \ubaa8\ub378 \ud559\uc2b5\/\uc608\uce21\/\ud3c9\uac00\r\n* \ud68c\uadc0 \ubaa8\ub378\uc744 \uc801\uc6a9\ud558\uae30 \uc804\uc5d0 \ub370\uc774\ud130 \uc138\ud2b8\uc5d0 \ub300\ud574\uc11c \uba3c\uc800 \ucc98\ub9ac \ud574\uc57c\ud560 \uc0ac\ud56d\r\n1) \uacb0\uad0f\uac12\uc774 \uc815\uaddc \ubd84\ud3ec\ub85c \ub3fc \uc788\ub294\uc9c0 \ud655\uc778\r\n2) \uce74\ud14c\uace0\ub9ac\ud615 \ud68c\uadc0 \ubaa8\ub378\uc758 \uacbd\uc6b0 \uc6d0-\ud56b \uc778\ucf54\ub529\uc73c\ub85c \ud53c\ucc98\ub97c \uc778\ucf54\ub529","44799fde":"* Target\uac12\uc740 Saleprice","3bcf80ed":"\ub77c\uc3d8 \ud68c\uadc0\uc758 \uacbd\uc6b0 \ud68c\uadc0 \uc131\ub2a5\uc774 \ud0c0 \ud68c\uadc0 \ubc29\uc2dd\ubcf4\ub2e4 \ub9ce\uc774 \ub5a8\uc5b4\uc9c0\ub294 \uacb0\uacfc\uac00 \ub098\uc634 -> \ub77c\uc3d8\uc758 \uacbd\uc6b0 \ucd5c\uc801 \ud558\uc774\ud37c \ud30c\ub77c\ubbf8\ud130 \ud29c\ub2dd\uc774 \ud544\uc694\ud574\ubcf4\uc784","140c0f2b":"* A\ubaa8\ub378 40%, B\ubaa8\ub378 60% & A \ubaa8\ub378\uc758 \uc608\uce21\uac12 = [100, 80, 60] , B \ubaa8\ub378\uc758 \uc608\uce21\uac12 = [120, 80, 50]\r\n* \ucd5c\uc885 \uc608\uce21\uac12: [100*0.4 + 120*0.6, 80*0.4 + 80*0.6, 60*0.4 + 50*0.6]","5b0339c4":"\ud68c\uadc0\uc5d0 \uc911\uc694\ud558 \uc601\ud5a5\uc744 \ubbf8\uce58\ub294 \ud53c\ucc98\ub97c \uc704\uc8fc\ub85c \uc774\uc0c1\uce58 \ub370\uc774\ud130\ub97c \ucc3e\uc73c\ub824\ub294 \ub178\ub825\uc774 \uc911\uc694, \ubcf4\ud1b5 \uc54c\uace0\ub9ac\uc998 \uc804\uc5d0 \ub370\uc774\ud130\ub97c \uac00\uacf5\ud558\uc9c0\ub9cc \uc774\ub294 \uc54c\uace0\ub9ac\uc998\uc744 \uc801\uc6a9\ud558\uae30 \uc804\uc5d0 \uaf2d \uc644\ubcbd\ud55c \uac00\uacf5\uc744 \ud558\ub77c\ub294 \uc758\ubbf8\ub294 \uc544\ub2d8","108c309f":"Reference - \ud30c\uc774\uc36c \uba38\uc2e0\ub7ec\ub2dd \uc644\ubcbd\uac00\uc774\ub4dc, \uad8c\ucca0\ubbfc (\uc704\ud0a4\ubd81\uc2a4)","7ca89d9e":"* Null \uac12\uc774 \ub9ce\uc740 \ud53c\ucc98\ub4e4 drop(PoolQC, MiscFeature, Alley, Fence, FireplaceQu)\r\n* ID\ub294 \ub2e8\uc21c \uc2dd\ubcc4\uc790\uc774\ubbc0\ub85c drop\r\n* LotFrontage\ub294 Null\uc774 259\uac1c\ub85c \ube44\uad50\uc801 \ub9ce\uc73c\ub098 \ud3c9\uade0\uac12\uc73c\ub85c \ub300\uccb4\r\n* \ub098\uba38\uc9c0 Null \ud53c\ucc98\ub294 \ud3c9\uade0\uac12\uc73c\ub85c \ub300\uccb4","5f050acf":"\ub9bf\uc9c0 \ubaa8\ub378 \uc608\uce21\uac12\uc5d0 0.4, \ub77c\uc3d8 \ubaa8\ub378 \uc608\uce21\uac12\uc5d0 0.6\uc744 \uacf1\ud55c \ub4a4 \ub354\ud568 -> 0.4\ub098 0.6\uc744 \uc815\ud558\ub294 \ud2b9\ubcc4\ud55c \uae30\uc900\uc740 \uc5c6\uc74c(\ub450 \uac1c \uc911 \uc131\ub2a5\uc774 \uc870\uae08 \uc88b\uc740 \ucabd\uc5d0 \uac00\uc911\uce58\ub97c \ub354 \ub480\uc74c)","5835e234":"\uce90\uae00\uc5d0\uc11c \uc694\uad6c\ud55c \uc131\ub2a5 \ud3c9\uac00 \ubc29\ubc95\uc740 RMSLE -> \uc624\ub958 \uac12\uc758 \ub85c\uadf8\uc5d0 \ub300\ud55c RMSE\r\n* \uc0ac\uc774\ud0b7\ub7f0\uc740 RMSLE\ub97c \uc81c\uacf5\ud558\uc9c0 \uc54a\uc74c -> \uc9c1\uc811 \ub9cc\ub4e4\uc5b4\uc57c\ud568","a8b36a60":"XGBoost\uc640 LightGBM\uc744 \ud63c\ud569\ud558\uae30","e5cab7fc":"\ud559\uc2b5 \ub370\uc774\ud130\uc758 \ub370\uc774\ud130 \ubd84\ud560\uc5d0 \ubb38\uc81c\uac00 \uc788\uc5b4\uc11c \uadf8\ub7f0 \uac83\uc778\uc9c0 \uc774\ubc88\uc5d0\ub294 train_test_split()\uc73c\ub85c \ubd84\ud560\ud558\uc9c0 \uc54a\uace0 \uc804\uccb4 \ub370\uc774\ud130 \uc138\ud2b8\uc778 X_features\uc640 y_target\uc744 5\uac1c\uc758 \uad50\ucc28 \uac80\uc99d \ud3f4\ub4dc \uc138\ud2b8\ub85c \ubd84\ud560\ud574 \ud3c9\uade0 RMSE\ub97c \uce21\uc815 -> cross_val_score()\uc774\uc6a9","9dd34b1a":"* \ubb38\uc790\ud615 \ud53c\ucc98\ub294 \ubaa8\ub450 \uc6d0-\ud56b \uc778\ucf54\ub529\uc73c\ub85c \ubcc0\ud658 -> get_dummies() -> \uc790\ub3d9\uc73c\ub85c Null\uac12\uc740 'None'\uce7c\ub7fc\uc73c\ub85c \ub300\uccb4","4588aecb":"\ub370\uc774\ud130 \uc138\ud2b8\ub97c \ucd94\uac00\uc801\uc73c\ub85c \uac00\uacf5 \ud6c4 \ubaa8\ub378 \ud29c\ub2dd \uc9c4\ud589\r\n* \ud53c\ucc98 \ub370\uc774\ud130 \uc138\ud2b8\uc758 \ub370\uc774\ud130 \ubd84\ud3ec\ub3c4 \ud655\uc778\r\n* \uc774\uc0c1\uce58 \ub370\uc774\ud130 \ucc98\ub9ac","1a7bde9b":"* \uc120\ud615 \ud68c\uadc0\ub294 \uc9c1\uc120\uc73c\ub85c \uc608\uce21 \ud68c\uadc0\uc120\uc744 \ud45c\ud604\r\n* \ud68c\uadc0 \ud2b8\ub9ac\uc758 \uacbd\uc6b0, \ubd84\ud560\ub418\ub294 \ub370\uc774\ud130 \uc9c0\uc810\uc5d0 \ub530\ub77c \ube0c\ub79c\uce58\ub97c \ub9cc\ub4e4\uba74\uc11c \uacc4\ub2e8 \ud615\ud0dc\ub85c \ud68c\uadc0\uc120\uc744 \ub9cc\ub4e6\r\n* DecisionTreeRegressor\uc758 max_depth = 7\uc778 \uacbd\uc6b0\uc5d0\ub294 \ud559\uc2b5 \ub370\uc774\ud130 \uc138\ud2b8\uc758 \uc774\uc0c1\uce58 \ub370\uc774\ud130\ub3c4 \ud559\uc2b5\ud558\uba74\uc11c \ubcf5\uc7a1\ud55c \uacc4\ub2e8 \ud615\ud0dc\uc758 \ud68c\uadc0\uc120\uc744 \ub9cc\ub4e4\uc5b4 \uacfc\uc801\ud569\uc774 \ub418\uae30 \uc26c\uc6b4 \ubaa8\ub378","68d7de90":"\ud68c\uadc0 \ud2b8\ub9ac Regressor \ud074\ub798\uc2a4\ub294 \uc120\ud615 \ud68c\uadc0\uc640 \ub2e4\ub978 \ucc98\ub9ac \ubc29\uc2dd\uc774\ubbc0\ub85c \ud68c\uadc0 \uacc4\uc218\ub97c \uc81c\uacf5\ud558\ub294 coef_ \uc18d\uc131\uc774 \uc5c6\uc74c. \ub300\uc2e0 feature_importances_\ub97c \uc774\uc6a9\ud574 \ud53c\ucc98\ubcc4 \uc911\uc694\ub3c4\ub97c \uc54c \uc218 \uc788\uc74c","b06e7ea1":"* Decision Tree -> \ud68c\uadc0 \ud074\ub798\uc2a4 : DecisionTreeRegressor, \ubd84\ub958 \ud074\ub798\uc2a4 : DecisionTreeClassifier\r\n* Gradient Boosting -> \ud68c\uadc0 \ud074\ub798\uc2a4 : GradientBoostingRegressor, \ubd84\ub958 \ud074\ub798\uc2a4 : GradientBoostingClassifier\r\n* XGBoost -> \ud68c\uadc0 \ud074\ub798\uc2a4 : XGBRegressor, \ubd84\ub958 \ud074\ub798\uc2a4 : XGBClassifier\r\n* LightGBM -> \ud68c\uadc0 \ud074\ub798\uc2a4 : LGBMRegressor, \ubd84\ub958 \ud074\ub798\uc2a4 : LGBMClassifier","bf70ac35":"* \ud68c\uadc0 \uacc4\uc218\uac00 \ub192\uc740 \ud53c\ucc98, \uc989 \uc608\uce21\uc5d0 \ub9ce\uc740 \uc601\ud5a5\uc744 \ubbf8\uce58\ub294 \uc911\uc694 \ud53c\ucc98\uc758 \uc774\uc0c1\uce58 \ub370\uc774\ud130\uc758 \ucc98\ub9ac\uac00 \uc911\uc694\r\n* GrLivArea\ud53c\ucc98\uc758 \ud68c\uadc0 \uacc4\uc218\uac00 \uac00\uc7a5 \ub192\uc73c\ubbc0\ub85c \uc774 \ud53c\ucc98\uc758 \ub370\uc774\ud130 \ubd84\ud3ec\ub97c \uc0b4\ud3b4\ubd04","0c277f32":"\uc2e4\uc81c target \ub370\uc774\ud130 \uac12\uc778 count\ub97c \uac10\uc548\ud558\uba74 \uc608\uce21 \uc624\ub958\ub85c\uc11c\ub294 \ube44\uad50\uc801 \ud070 \uac12"}}