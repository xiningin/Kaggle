{"cell_type":{"762dca9c":"code","253fa6b3":"code","c668f385":"code","611306a6":"code","e9fd40a8":"code","4d6885e4":"code","5d0c6dc3":"code","4541c086":"code","c1fe76aa":"code","a31055f9":"code","1220567c":"code","920f2abc":"code","7b856571":"code","1f795d31":"code","f9d7c597":"code","cc7da057":"code","fbe0432f":"markdown","00b5f439":"markdown","c6a2f2ce":"markdown","005f2403":"markdown","f22f1ec2":"markdown","aaf03d3f":"markdown","c784515e":"markdown","5095f053":"markdown","b4cea0ab":"markdown","f667ad00":"markdown","24e7a28c":"markdown","2a068971":"markdown","d3a8a446":"markdown","01357562":"markdown","6b969a26":"markdown","b1b3c8ee":"markdown"},"source":{"762dca9c":"import os\nfrom collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\n\nfrom wordcloud import WordCloud, STOPWORDS\n\n%matplotlib inline\n\nos.listdir('\/kaggle\/input\/commonlitreadabilityprize')","253fa6b3":"train = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntrain","c668f385":"train.info()","611306a6":"for col in train.columns:\n    print(f\"{col}: {len(train[col].unique())}\")","e9fd40a8":"fig = ff.create_distplot([train['target']], ['target'])\nfig.show()","4d6885e4":"# Top 5 excerpts with lowest scores\n\nmin_5_targets = sorted(train['target'])[:5]\nfor min_target in min_5_targets:\n    print(\"Target:\", train[train['target'] == min_target].iloc[0,4])\n    print(train[train['target'] == min_target].iloc[0,3])\n    print(\"*\" * 50)","5d0c6dc3":"# Top 5 excerpts with highest scores\n\nmax_5_targets = sorted(train['target'])[-5:]\nfor max_target in max_5_targets:\n    print(\"Target:\", train[train['target'] == max_target].iloc[0,4])\n    print(train[train['target'] == max_target].iloc[0,3])\n    print(\"*\" * 50)","4541c086":"# Defining our word cloud drawing function\ndef wordcloud_draw(data, color = 'white'):\n    wordcloud = WordCloud(stopwords = STOPWORDS,\n                          background_color = color,\n                          width = 3000,\n                          height = 2000\n                         ).generate(' '.join(data))\n    plt.figure(1, figsize = (12, 8))\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    plt.show()","c1fe76aa":"words_in_lower_scoring_excerpts = []\n\nfor _, row in train.sort_values('target').head(100).iterrows():\n    words_in_lower_scoring_excerpts.extend(row['excerpt'].split())\n\nprint(\"Wordcloud for excerpts with lowest targets:\")\nwordcloud_draw(words_in_lower_scoring_excerpts, color='black')","a31055f9":"words_in_higher_scoring_excerpts = []\n\nfor _, row in train.sort_values('target').tail(100).iterrows():\n    words_in_higher_scoring_excerpts.extend(row['excerpt'].split())\n\nprint(\"Wordcloud for excerpts with highest targets:\")\nwordcloud_draw(words_in_higher_scoring_excerpts)","1220567c":"test = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\ntest","920f2abc":"submission_df = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv', index_col=0)\nsubmission_df","7b856571":"range_len = 100\nrange_counter = 0\nlower_bound = 0\nupper_bound = 0\ntarget_ranges = []\nfor _, row in train.sort_values('target').iterrows():\n    if range_counter >= range_len - 1:\n        range_counter = 0\n        upper_bound = row['target']\n        target_ranges.append((lower_bound, upper_bound))\n    elif range_counter == 0:\n        lower_bound = row['target']\n        range_counter += 1\n    else:\n        range_counter += 1\n\nif range_counter > 0:\n    target_ranges.append((lower_bound, train.sort_values('target').iloc[-1,4]))\n        \ntarget_ranges","1f795d31":"target_ranges_prediction = {}\nmargin = 0.1\nfor target_range in target_ranges:\n    prediction = sum(target_range)\/2\n    if prediction < -2: prediction += margin\n    if prediction > 0: prediction -= margin\n    target_ranges_prediction[target_range] = prediction\n\ntarget_ranges_prediction","f9d7c597":"words_in_target_ranges = defaultdict(set)\n\nfor target_range in target_ranges:\n    lower_bound, upper_bound = target_range\n    for _, row in train[(train['target'] > lower_bound) & (train['target'] < upper_bound)].iterrows():\n        words_in_target_ranges[target_range] |= set(row['excerpt'].lower().split())","cc7da057":"predictions = []\nfor index in submission_df.index:\n    excerpt_words = test[test['id'] == index].iloc[0,3].lower().split()\n    max_intersection = sum([1 if word in words_in_target_ranges[target_ranges[0]] else 0 for word in excerpt_words])\n    max_target_range = target_ranges[0]\n    for target_range in target_ranges[1:]:\n        intersection = sum([1 if word in words_in_target_ranges[target_range] else 0 for word in excerpt_words])\n        if intersection > max_intersection:\n            max_intersection = intersection\n            max_target_range = target_range\n    predictions.append(target_ranges_prediction[max_target_range])\n\nsubmission_df['target'] = predictions\n\nsubmission_df.to_csv('submission.csv')\n\nsubmission_df","fbe0432f":"# 3. A Na\u00efve String Matching Submission <a class=\"anchor\" id=\"head-3\"><\/a>","00b5f439":"It looks like higher scoring excerpts tend to have a lower reading complexity than excerpts with lower scores.\n\nSentences and simpler, and context is easily caught in higher scoring excerpts, whereas the opposite can be observed in lower scoring excerpts.\n\nFor a deeper grasp of the differences in text excerpts, let's take a look at the world cloud of the top 100 excerpts with the highest and lowest target scores.","c6a2f2ce":"# Overview\n\nCiting the competition's hosts:\n> In this competition, you\u2019ll build algorithms to rate the complexity of reading passages for grade 3-12 classroom use.\n\nThus, given a set of text excerpts, we'll have to predict their relative *textual complexity*. Such work would prove to be extremely beneficial in the context of knowledge sharing and availability. One would be able to quickly search text excerpts of interest while consequently getting a match that perfectly fits the person's reading capabilities. Consequently, knowledge sharing could be automated and education speed greatly accelerated.\n\nIn this notebook, we'll take a look at an Exploratory Data Analysis of the training data provided for this competition, as well as building and running a na\u00efve solution that basically performs string matching frequencies to predict whether a given text excerpt has a higher or lower textual complexity.\n\n\n### Outline:\n\n1. [Setup and Basic EDA](#head-1)\n2. [Understanding Excerpts and their Associated Targets](#head-2) \n3. [A Na\u00efve String Matching Submission](#head-3)","005f2403":"Let's first take a look at the distribution of the targets in the training set.","f22f1ec2":"We are provided with 3 main pieces of data:\n\n* `train.csv`: The CSV file containing all the training reading passages as well as their corresponding metadata, such as their ID and their target complexities (ground truths).\n* `test.csv`: The CSV file containing (a small subset of) the actual reading passages that will be used for testing purposes (thus, with no ground truth column available).\n* `sample_submission.csv`: The CSV file containing all the publications IDs in the test set, for which we'll have to populate the prediction column.","aaf03d3f":"<center><h1> CommonLit Readability Prize <\/h1>\n    <h2>\ud83d\udcd6 EDA + Na\u00efve Submission \ud83d\udcd6<\/h2>\n\n<img src=\"https:\/\/www.commonsense.org\/education\/sites\/default\/files\/tlr-blog\/commonlit-logo-1.png\" width=\"500\"\/>\n    <p style=\"text-align:center;\">Image <a href=\"https:\/\/www.commonsense.org\/education\/website\/commonlit\">source<\/a>.<\/p>\n<\/center>","c784515e":"It looks like **all targets and standard errors** are unique in the dataset. This comes with no surprise as the problem at hand is a regression problem and not a classification problem.\n\nAlso, all 830 excerpts having a license share a pool of only 16 unique licenses.","5095f053":"Obviously, the end goal of such a competition is not simply do string matching of known text excerpts in order to predict textual complecity, however, it is to build a strong enough NLP model that can infer from context whether or not a piece of text contains cohesion and semantics of high or low textual complexity.\n\nThat being said, below we will implement a very simple string matching technique as a POC and template for building a submission.","b4cea0ab":"The training data contains 2,834 rows, with 6 columns describing each row.","f667ad00":"That's great! There are no missing values (except for legal and license information), and the dataset looks complete.","24e7a28c":"Targets follow a normal distribution centered at **-1**. It is apparent that negative targets are more common than positive ones, with the training range going **from -3.67 up to 1.71**.\n\nBut what do those numbers actualy mean? Which direction is the \"easier\" complexity of text excerpts? To answer this, let's take a look at the 5 excerpts with the highest and lowest target scores.","2a068971":"Words present in higher scoring excerpts seems to be more relaxed and geared towards *story-telling*. Words such as **said**, **went**, **little** and **mother** stand out the most.","d3a8a446":"# 2. Understanding Excerpts and their Associated Targets <a class=\"anchor\" id=\"head-2\"><\/a>","01357562":"Words present in lower scoring excerpts seems to be more precise or *scientific*. Words such as **system**, **light**, **matter** and **surface** stand out the most.","6b969a26":"# 1. Setup and Basic EDA <a class=\"anchor\" id=\"head-1\"><\/a>","b1b3c8ee":"# This notebook is under development \ud83d\udea7\n\n---\n\n## Please upvote if you found it useful \ud83d\ude0a"}}