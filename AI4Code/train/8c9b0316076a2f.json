{"cell_type":{"eff8d71a":"code","851f60b7":"code","61007904":"code","d9261df8":"code","ef3dd5bd":"code","05c62cc3":"code","98043ba4":"code","793feada":"code","416ac576":"code","a4515dd2":"code","e88a1402":"code","b58d48dc":"code","812d9298":"code","ed4bef34":"code","14ace9b0":"code","59889e0b":"code","b4061fab":"code","7ca48627":"code","8594b4bd":"code","8b8b1c83":"code","060ffcad":"code","f7922e4d":"code","a1700fd5":"code","7f92dd29":"code","04a8c36a":"code","9d87dd78":"code","0e8b9c2a":"code","ddad3ec3":"code","f7be39a7":"code","402b7dc6":"code","93b2c6f1":"code","7635fe74":"markdown","8a46c019":"markdown","84a8a768":"markdown","3d0ed3ac":"markdown"},"source":{"eff8d71a":"import pandas as pd\nimport torch","851f60b7":"dataset = pd.read_csv('..\/input\/prima-diabetes-dataset\/diabetes.csv')\nprint(dataset.shape)","61007904":"dataset.head()","d9261df8":"dataset.isnull().sum()","ef3dd5bd":"X = dataset.drop('Outcome', axis = 1).values ## independent features\ny = dataset['Outcome'].values ## dependent features","05c62cc3":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","98043ba4":"### Libraries from Pytorch\nimport torch\nimport torch.nn as nn   ## Helps you to create and train the neural networks\nimport torch.nn.functional as F  ## this functional F contains functions like sigmoid, relu etc...","793feada":"### Creating Tensors\n\n## Remember that your independent features need to be converted into Float Tensors\nX_train = torch.FloatTensor(X_train)\nX_test = torch.FloatTensor(X_test)\n\n## No need to convert to float tensors in case of dependent features\ny_train = torch.LongTensor(y_train)  ## LongTensor won't convert values into float tensors\ny_test = torch.LongTensor(y_test)","416ac576":"### Creating model with pytorch\n\nclass ANN_Model(nn.Module):\n    def __init__(self, input_features = 8, hidden1 = 20, hidden2 = 20, out_features = 2):\n        super().__init__()\n        self.f_connected1 = nn.Linear(input_features, hidden1)\n        self.f_connected2 = nn.Linear(hidden1, hidden2)\n        self.out = nn.Linear(hidden2, out_features)\n    def forward(self, x):\n        x = F.relu(self.f_connected1(x))\n        x = F.relu(self.f_connected2(x))\n        x = self.out(x)\n        return x","a4515dd2":"## Instantiate ANN_MODEL\n\ntorch.manual_seed(20) ##it will set the seed of the random number generator to a fixed value, so that when you call for example torch.rand(2), the results will be reproducible\nmodel = ANN_Model()","e88a1402":"model.parameters","b58d48dc":"model.parameters() ## it's an generator, so we can iterate and retirve all the parameter one by one","812d9298":"## Backward Propagation | Define the Loss Function | Define the Optimizer\n\nloss_function = nn.CrossEntropyLoss() ## for multiclassification problem use CrossEntropyLoss function\noptimizer = torch.optim.Adam(model.parameters(), lr = 0.01)","ed4bef34":"epochs = 500\nfinal_loss = []\nfor i in range(epochs):\n    i = i + 1\n    y_pred = model.forward(X_train)\n    loss = loss_function(y_pred, y_train)\n    final_loss.append(loss)\n    \n    ## after eveery 10 epochs print this\n    if i % 10 == 1:\n        print('Epoch number: {} and the loss: {}'.format(i, loss.item()))\n        \n    optimizer.zero_grad() ## Clears the gradients of all optimized class\n    loss.backward() ## for backward propagation and to find the derivative\n    optimizer.step() ## performs a single optimization step.","14ace9b0":"## Plot the loss function\nimport matplotlib.pyplot as plt\n%matplotlib inline","59889e0b":"## To see the if loss is decreasing or not\nplt.plot(range(epochs), final_loss)\nplt.ylabel('Loss')\nplt.xlabel('Epoch')","b4061fab":"## Let's find out the prediction\n\n## Prediction in X_test data\nprediction = []  \n\n## To remove this gradient --> (grad_fn=<AddBackward0>). it is required when we are taining the model. here it has no use.\nwith torch.no_grad():\n    for i, data in enumerate(X_test):\n#         print(model(data)) ## Here model is out ANN_Model\n        y_pred = model(data)\n        prediction.append(y_pred.argmax().item())\n#         print(y_pred.argmax().item())  ## 1 = Diabetic Person     0 = No Diabetic Person","7ca48627":"## list of prediction. ## 1 = Diabetic Person     0 = No Diabetic Person\nprediction","8594b4bd":"## Check the accuracy by using confusion matrix\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, prediction)\ncm\n\n# result: 92+34 = 126 are true predicted values and 15+13 = 28 are wrong predicted values","8b8b1c83":"import seaborn as sns\n\nplt.figure(figsize=(8, 5))\nsns.heatmap(cm, annot = True)\nplt.xlabel('Actual Values')\nplt.ylabel('Predicted Values')","060ffcad":"from sklearn.metrics import accuracy_score\nacc_score = accuracy_score(y_test, prediction)","f7922e4d":"acc_score ## Accuracy is 81%","a1700fd5":"## Save the model\ntorch.save(model, 'diabetes.pt') ## pt is extension for pytorch files","7f92dd29":"## To load the model\nmodel = torch.load('diabetes.pt')","04a8c36a":"model.eval()","9d87dd78":"## how to do prediction of the new data points\n\n## Let's take the only first column of dataset and change it's values for prediction\ndataset.iloc[0, :-1].head()","0e8b9c2a":"list(dataset.iloc[0, :-1])","ddad3ec3":"## New data\nlst = [6.0, 130.0, 72.0, 40.0, 0.0, 33.6, 0.627, 45.0]","f7be39a7":"new_data = torch.tensor(lst) ## Converting into torch tensors.\nnew_data","402b7dc6":"new_data","93b2c6f1":"## Predict new_data using PyTorch\n\n## since this is just a single list, we don't have to use enumeraet\nwith torch.no_grad():\n    print(model(new_data))\n    print(model(new_data).argmax())\n    print(model(new_data).argmax().item())  \n    \n## result = 1. which means for this data point(single list that we provided) person is diabetic.","7635fe74":"## How to use existing model for the prediction of new dataset","8a46c019":"## Pre-Processing of dataset","84a8a768":"## Accuracy","3d0ed3ac":"## Creating an ANN using Pytorch"}}