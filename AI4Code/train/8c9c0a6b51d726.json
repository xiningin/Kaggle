{"cell_type":{"f3f69e1c":"code","a4e46bbc":"code","00033cd4":"code","fbb18b48":"code","f2f37c6b":"code","dbb98111":"code","d44e7080":"code","428ec290":"code","bf8221e4":"code","9f576e15":"code","e58ca9c7":"code","3f69ca4b":"code","db329bb4":"code","914579ca":"code","93b612c8":"code","507f9a83":"code","05ef8823":"code","77a43b31":"code","bb7f2831":"code","37ea7104":"code","42a7f3ac":"code","7018bac0":"code","6c417e09":"code","685337de":"code","1ce1119c":"code","e92be9ca":"code","9ff362f1":"code","066e16e4":"code","14f62452":"code","1b05ed3d":"code","8f6ea550":"code","1bb02ed2":"code","50849e76":"code","3c7c3486":"code","3828bc83":"code","23ee228a":"code","82f49d61":"code","aaac04a3":"code","bb3b64b6":"code","9781be4b":"code","b4c535bd":"code","9789af37":"code","8941e3f2":"code","693f7400":"code","c48443c4":"markdown","2d9c709b":"markdown","5d85e641":"markdown","fdbfd4c7":"markdown","bbd62b5d":"markdown","6bacc102":"markdown","628b394c":"markdown","85f2343f":"markdown","45a6f772":"markdown","fa3c0f86":"markdown","da565c22":"markdown","bf4baad2":"markdown","2089ce19":"markdown","176fb9cf":"markdown","e8dfb3ed":"markdown","3089c983":"markdown","b0c6ea8d":"markdown","832d90f0":"markdown","1b10bc13":"markdown","726e0cfc":"markdown","cb099c0e":"markdown","f174a770":"markdown","0dab6c40":"markdown","f216084b":"markdown","a7b55f91":"markdown","c26af3a8":"markdown","5019c93f":"markdown","7d896145":"markdown","73ce2b6b":"markdown","8aa12558":"markdown","41106dbb":"markdown"},"source":{"f3f69e1c":"# importing the required modules\n\nimport pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import balanced_accuracy_score, roc_auc_score, make_scorer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix, plot_confusion_matrix","a4e46bbc":"# importing the data\n\ndf = pd.read_csv('..\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv')","00033cd4":"df.head()","fbb18b48":"df['customerID'].count()","f2f37c6b":"len(df['customerID'].unique())","dbb98111":"df.drop('customerID', axis= 1, inplace = True)","d44e7080":"df.columns","428ec290":"df['MultipleLines'].unique()","bf8221e4":"miss = []\nfor var in df.columns:\n    if df[var].isnull().values.any() == True:\n        miss.append(var)","9f576e15":"print(var)","e58ca9c7":"df.describe()","3f69ca4b":"df.info","db329bb4":"df.head()","914579ca":"df.columns = df.columns.str.replace(' ', '_')\ndf.head()","93b612c8":"df.dtypes","507f9a83":"print(df['TotalCharges'].unique())\nprint(len(df['TotalCharges'].unique()))","05ef8823":"#df['TotalCharges']= pd.to_numeric(df['TotalCharges'])\n# throws error ","77a43b31":"len(df.loc[df['TotalCharges'] == ' '])","bb7f2831":"df.loc[df['TotalCharges'] == ' ']","37ea7104":"df.loc[(df['TotalCharges'] == ' '), 'TotalCharges'] = 0","42a7f3ac":"df.loc[df['tenure'] == 0]","7018bac0":"df['TotalCharges'] = pd.to_numeric(df['TotalCharges'])","6c417e09":"df.dtypes","685337de":"df.replace(' ', '_', regex= True, inplace= True)\ndf.head()","1ce1119c":"df.columns","e92be9ca":"X = df.drop('Churn', axis = 1).copy() \n# ALTERNATE:\n# X = df_no_missing.iloc[:,:-1]\n\nX.head()","9ff362f1":"y = df['Churn'].copy()\ny.head()","066e16e4":"X.dtypes","14f62452":"print(df.columns)\nfor x in df.columns:\n    print(df[x].unique())","1b05ed3d":"pd.get_dummies(X, columns= ['Contract']).head()","8f6ea550":"X_encoded = pd.get_dummies(X, columns= ['gender', \n                                        'Partner', \n                                        'Dependents', \n                                        'PhoneService', \n                                        'MultipleLines', \n                                        'InternetService',\n                                        'OnlineSecurity',\n                                        'OnlineBackup',\n                                        'DeviceProtection',\n                                        'TechSupport',\n                                        'StreamingTV',\n                                        'StreamingMovies',\n                                        'Contract',\n                                        'PaperlessBilling',\n                                        'PaymentMethod'])\n\nX_encoded.head()","1bb02ed2":"y.unique()","50849e76":"# REPLACING YESs WITH 1s and NOs with 0s\ny = y.str.replace('Yes', '1')\ny = y.str.replace('No', '0')\ny.unique()","3c7c3486":"y = pd.to_numeric(y)","3828bc83":"sum(y)\/len(y)","23ee228a":"X_train, X_test, y_train, y_test= train_test_split(X_encoded, y, random_state = 42, stratify = y)","82f49d61":"print(sum(y_train)\/len(y_train))\nprint(sum(y_test)\/len(y_test))","aaac04a3":"xgb_clf = xgb.XGBClassifier(objective='binary:logistic', missing= None, seed= 42)\nxgb_clf.fit(X_train,\n            y_train,\n            verbose= True,\n            early_stopping_rounds= 10,\n            eval_metric= 'aucpr',\n            eval_set= [(X_test, y_test)])","bb3b64b6":"plot_confusion_matrix(xgb_clf, \n                      X_test,\n                      y_test,\n                      values_format= 'd',\n                      display_labels=[\"Did not leave\", \"Left\"])","9781be4b":"# NOTE: When data is imbalanced, the XGBoost manual says\n# If you care only about the overall performance matric (AUC) of your predictions\n# -> Balance the positive and negative weights via scale_pos_weight\n# -> Use AUC for evaluation.\n# Running GridSearchCV() sequentially on subsets of parameter options, rather than all at once in order\n# to optimize parameters in a short period of time.\n\n## ROUND 1\n\nparam_grid= {\n    'max_depth': [3, 4, 5],\n    'learning_rate': [0.1, 0.01, 0.05],\n    'gamma': [0, 0.25, 1.0],\n    'reg_lambda': [0, 1.0, 10.0],\n    'scale_pos_weight': [1, 3, 5]\n}\n\n## ROUND 2\n\nparam_grid= {\n    'max_depth': [4],\n    'learning_rate': [0.1, 0.5, 1],\n    'gamma': [0.25],\n    'reg_lambda': [10.0, 20, 100],\n    'scale_pos_weight': [3]\n}\n\n\n# In order to speed up the Cross Validation, for each tree we are using a random subset of the actual data ie. we are not using \n# all the data. We are only using 90 % and that is randomly selected per tree. We are also only selecting per tree 50 % of the \n# columns in that dataset so for every tree we create, we select a different 50 % of the column and that helps us with overfitting\n# issues as well as speeding things up considerably. Other than that we are just using AUC score and we are not doing a lot of \n# Cross Validation (not 10 fold only 3 fold).\n\noptimal_params= GridSearchCV(\n    estimator= xgb.XGBClassifier(objective= 'binary:logistic',\n                                 seed= 42,\n                                 subsample= 0.9,\n                                 colsample_bytree= 0.5),\n    param_grid= param_grid,\n    scoring= 'roc_auc',\n    verbose= 0,\n    n_jobs= 10,\n    cv= 3\n)\n\noptimal_params.fit(X_train,\n                   y_train,\n                   early_stopping_rounds= 10,\n                   eval_metric= 'auc',\n                   eval_set= [(X_test, y_test)],\n                   verbose= False)\n\nprint(optimal_params.best_params_)\n","b4c535bd":"xgb_clf= xgb.XGBClassifier(seed= 42,\n                           objective= 'binary:logistic',\n                           gamma= 0.25,\n                           learn_rate= 0.1,\n                           max_depth= 4,\n                           reg_lambda= 10,\n                           scale_pos_weight= 3,\n                           subsample= 0.9,\n                           colsample_bytree= 0.5)\n\nxgb_clf.fit(X_train,\n            y_train,\n            verbose= True,\n            early_stopping_rounds= 10,\n            eval_metric= 'aucpr',\n            eval_set= [(X_test, y_test)])","9789af37":"plot_confusion_matrix(xgb_clf,\n                      X_test,\n                      y_test,\n                      values_format= 'd',\n                      display_labels= [\"Did not leave\", \"Left\"])","8941e3f2":"xgb_clf= xgb.XGBClassifier(seed= 42,\n                           objective= 'binary:logistic',\n                           learn_rate= 0.1,\n                           max_depth= 4,\n                           reg_lambda= 10,\n                           scale_pos_weight= 3,\n                           subsample= 0.9,\n                           colsample_bytree= 0.5,\n                           n_estimators= 1)\n# n_estimators set to 1 so that we can get gain, cover etc.\nxgb_clf.fit(X_train, y_train)","693f7400":"bst= xgb_clf.get_booster()\n\nfor importance_type in ('weight', 'gain', 'cover', 'total_gain', 'total_cover'):\n    print('%s: ' % importance_type, bst.get_score(importance_type= importance_type))\n    \n    \nnode_params= {'shape': 'box',  # makes the node fancy\n              'style': 'filled, rounded',\n              'fillcolor': '#78cbe'\n             }\n\nleaf_params= {'shape': 'box',\n              'style': 'filled',\n              'fillcolor': '#e48038'}\n\n# NOTE: num_trees is NOT the number of trees to plot, but the specific tree that we are going to plot\n# The default value is 0, but let's set it just to show it since it is counter-intuitive.\n# xgb.to_graphviz(xgb_clf, num_trees= 0, size= \"10, 10\")\n\nxgb.to_graphviz(xgb_clf, num_trees= 0, size= \"10, 10\",\n                condition_node_params= node_params,\n                leaf_node_params= leaf_params)\n\n# TO SAVE THE FIGURE (in jupyter notebook):\n# graph_data= xgb.to_graphviz(xgb_clf, num_trees= 0, size= \"10, 10\",\n#                 condition_node_params= node_params,\n#                 leaf_node_params= leaf_params)\n# graph_data.view(filename= 'insert arbitrary file name as required')","c48443c4":"# DATA FORMATTING : PART 1\n\n- Using an XGBoost model to format the data.\n\n**STEPS**\n\n    1. Split the data into 2 parts:\n        a. the columns of data that we will use to make classifications\n        b. the column of data that we want to predict\n        ","2d9c709b":"# DATA FORMATTING : PART 2\n\nONE-HOT ENCODING\n\nAfter splitting the data into 2 parts, we observe the variables in X.","5d85e641":"# **Drawing and Interpreting the XGBoost Tree**\n\nIf we need to gain information, such as gain and cover etc, at each node in the first tree, we just build the first tree, otherwise we'll get the average over all of the trees.","fdbfd4c7":"Lets verify that we modified  TotalCharges column correctly by observing everyone who had tenure = 0.","bbd62b5d":"Now we have 45 columns instead of the initially present 21 columns. Next we have to verify that y only contains 1s and 0s with unique().","6bacc102":"Thus it is obvious that the stratification worked from the above cell output as we have the same percentage of people that left the company in  both y_train and y_test. Let's build the preliminary model.\n\nNOTE:\n\nInstead of determining the optimal number of trees with cross validation, we will be using **early stopping** to stop building trees when they no longer improve the situation.","628b394c":"Xgboost is an useful ML method when you don't want to sacrifice the ability to correctly classify observations but you still want a model that is fairly easy to understand and interpret.","85f2343f":"Objective was taken as 'binary:logistic' since XGBoost uses a sort of logistic regression based approach to evaluate how good it is at classifying the observation. The default value of missing argument is None and hence it is unnecessary to set missing= None there in the XGBClassifier function. The missing argument represents what character or value we are using to depict missing values. The default value when we have missing= None is zero or numpy.Nan (numpy:not a number) but it uses zeros in that sparse matrix so it doesn't have to allocate any memory for the same. \n\nThe training is done on the training data set but the evaluating how many trees to build (for early stopping mechanism) is done on the testing data set. \n\nNow that we have built the **XGBoost model** for classification, let's see how it performs on the testing dataset by running the testing dataset down the model and drawing a confusion matrix.","45a6f772":"So we see only around 26.5 % of the people in the dataset left the company. Due to this, when we split the data using train_test_split, we split using stratification in order to maintain the same percentage of people who left the company in both the training as well as the testing set.","fa3c0f86":"Now that we have dealt with the missing data, we will replace all the white spaces (' ') with underscores (_).","da565c22":"XGBoost chooses the split for data that gives the best value for **Gain**.\n\nQUESTION: Doesn't XGBoost consume a lot of memory for keeping track of all 1s and 0s?\n\nANSWER :  No, this is so because XGBoost uses sparse matrices and hence only keeps track of all 1s and it doesn't allocate any memory to the 0s.","bf4baad2":"So, SeniorCitizen, tenure, MonthlyCharges and TotalCharges are all int64 or float64 which is as per requirements for implementing a XGBoost model.\nHowever we need to take care of the categorical data because XGBoost being good at handling continuous data, does not natively support categorical data, like Contract which contains 3 different categories. Thus in order to use categorical data along with XGBoost, we have to use One-Hot Encoding to convert a column of categorical data into multiple columns of binary values.\n\n**Question: Why not treat categorical data as continuous data by taking the 3 categories from Contract column as 3 integer values (say 1, 2, 3) ?**\nAnswer: \nThe XGBoost may cluster 2 of the 3 categories now represented as numbers to other categories with number close to the former category. Example: 1, 2 or 2, 3 get clustered together which results in 3 and 1 getting isolated in the XGBoost tree.\n\n# **ONE-HOT ENCODING**\n\nThere are 2 popular ways to implement OHE :\n1. ColumnTransformer() from sklearn\n2. get_dummies() from pandas\n\nBoth methods have their own pros and cons.\n\nColumnTransformer() has a very cool feature where it creates a persistent function that can validate data that we (may) get in the future. Example: if my XGBoost model using a categorical variable favoriteColor that has red, blue and green as options, then ColumnTransformer() can remember those options and later on when my XGBoost model is being used in a production system, if someone says their favorite color is orange, then ColumnTransformer() can throw an error or handle the situation in another way. The downside of ColumnTransformer() is that it turns our data into an array and loses all the column names, making it difficult to verify that the usage of our columns works as we intended to.\n\nIn contrast, get_dummies() leaves our data in the form of a dataframe and retains the column names as well. However, it does not have the persistent behaviour as that of ColumnTransformer().\nSo I will be using get_dummies() method in this case.","2089ce19":"# **BUILD A PRELIMINARY XGBoost MODEL**\n\nNow we need to split the data first into the training and test sets. Let's ensure that the data is imbalanced by dividing the number of people who left the company in both the training and the testing set.","176fb9cf":"Now let's check whether the optimized **XGBoost** model does better by plotting another confusion matrix.","e8dfb3ed":"OHE is different from the way we would encode it for the same data ie. OHE gives us a result that is different for linear and logistic regressions. OHE is not suitable for linear and logistic regressions but it works great for trees.\n\nNow lets implement OHE via get_dummies() method on all of the categorical columns and save the result.","3089c983":"# **Building, Evaluating, Drawing and Interpreting the Optimized XGBoost Model**","b0c6ea8d":"Basically exit interview data from people that left telco.","832d90f0":"NOTE : The TotalCharges column still holds object data type values and this is NOT useful since XGBoost only allows int, float or boolean data types.\n\nTo fix the above issue we can do the conversion to float using to_numeric() function from the Pandas library.","1b10bc13":"In the confusion matrix, we observe that 1294 people that did not leave the company, 1155 (89.26 %) were correctly classified and of the remaining 467 people that left the company, 225 (48.2 %) were correctly satisfied. The accuracy of the model is NOT really anything impressive. The cause for the issue is that our data is imbalanced. Since people leaving costs the company a lot of financial losses due to default mainly, we must strive to predict more of these leaving people with our model. Let's try to improve the predictions using **Cross Validation** to optimize the parameters. \n\nThe positive point to take in here is that we have a way to do so since **XGBoost** has a parameter called *scale_pos_weight* that helps in dealing with imbalanced data. It adds a penalty for incorrectly classifying the minority class (in this case the people that left the company). So we need to increase that penalty so that the trees will correctly classify more of them to reduce the penalty.\n\n\n# **Optimize Parameters using Cross Validation and GridSearch()**\n\n**XGBoost** has a lot of *hyperparameters* that we need to manually configure and are NOT determined by XGBoost itself, including *max_depth*, the maximum tree depth, *learning rate*, the learning rate, or ***eta***, *gamma*, the parameter that encourages pruning, and *reg_lambda*, the regularization parameter lambda. Let's try to find the optimal values for these parameters to imporove the accuracy with the **Testing Dataset**.\n\n**NOTE**: Since we have many hyperparameters to optimize, we will use *GridSearchCV()*. We specify a bunch of potential values for the hyperparameters and *GridSearchCV* tests all possible combinations of the parameters for us.","726e0cfc":"Now let's print the weight, gain, cover etc for the tree.\n\nweight= number of times a feature is used in a branch or root across all trees\ngain= the average gain across all splits a feature is used in \ncover= the average coverage across all splits a feature is used in \ntotal_gain= the total gain across all splits a feature is used in \ntotal_cover= the total coverage across all splits a feature is used in \n\n**NOTE**: Since only 1 tree is being built :\n    gain= total_gain ; cover= total_cover","cb099c0e":"Let's discuss how to interpret the XGBoost tree. In each node, we have:\n\n    -  The variable (column name) and the threshold for splitting the observations. For example: in the tree's root, we use Contract_month_to_month to split the observations. All the observations with Contract-month-to-month < 1 go to the LEFT and all the observations with the value =< 1 go to the RIGHT.\n    \n    - Each branch either says YES or NO and some branches also say MISSING:\n        -> **yes** and **no** refer to whether the threshold in the node above it is **true** or **not**. If so, then **yes**\n            otherwise **no**.\n        -> **missing** is the default option if the data is missing in any instance.\n      \n    - **leaf** tells us the output value for each leaf.\n    \n# **SUMMARY**:\n\n1. Loaded the Data from a File\n2. Identified and Dealt with the Missing Data\n3. Formatted the Data for **XGBoost** using OHE (One-Hot Encoding)\n4. Built an **XGBoost** Model for classification\n5. Optimize the **XGBoost Parameters** with Cross Validation and GridSearchCV()\n6. Built, Drew, Interpreted and Evaluated  the Optimized XGBoost Model","f174a770":"So we can draw the conclusion that the optimized **XGBoost** model is doing a comparatively much better job at classifying the people that left the company. Out of the 467 poeple that left 380 (81.37 %) were correctly identified. Before optimization, the percentage of correct identification was 48.2 % which looking at the present result is way worse than what we could be hoping for.\n\nNOTE:-\n\nHowever this improvement was at the cost of not being able to correctly classify as many people that did not leave the company. With the optimized model, out of 1294 people that didn't leave the company only 932 (72 %) were correctly classified. That said this trade off may be better for the company since the people that leave the company take their money with them resulting in the increases in financial losses suffered by the company. So from the company's perspective it would be better to identify such people before they leave and take necessary steps to prevent as many people as possible from leaving consequently reducing the (expected) losses.","0dab6c40":"Since only 11 rows are missing we can take observations manually.","f216084b":"Observations:\n\n    1) We see that all 11 people with TotalCharges == ' ' have just signed up, because tenure is 0 for all of them. \n    \n    2) All of these people also have churn == 'No'.\n    \nSo we have few choices here: \n    - We can set TotalCharges = 0 for these 11 people or we can remove them from the dataset. Let's try by setting TotalCharges = 0.","a7b55f91":"So there are 7043 unique customer IDs.","c26af3a8":"There is a blank space present in the Total charges column. So we have to deal with that.\n\n# MISSING DATA PART 2: DEALING WITH MISSING DATA, XGBoost Style\n\nOne thing that is relatively unique about **XGBoost** is that it determines default behaviour for missing data. So all we have to do is identify missing values and make sure they are set to 0.\n\nHowever, before we do that, let's see how many rows are missing data. If it's a lot, then we might have a problem than what XGBoost can deal with on its own. If it's not that many, we can just set them to be 0.","5019c93f":"# STEP 1: **IMPORTING THE DATA**\n            \n# STEP 2: **MISSING DATA**\n            - identifying the missing data\n            - dealing with the missing data\n            \n# STEP 3: **FORMATTING THE DATA FOR XGBOOST**\n            - splitting the data into dependent and independent variables\n            - One hot encoding\n            - converting all columns to int, float or bool\n\n# STEP 4: **BUILDING A PRELIMINARY XGBOOST MODEL**\n\n# STEP 5: **OPTIMIZING PARAMETERS WITH CROSS VALIDATION AND gridsearch()**\n            - optimizing the learning rate, tree depth, number of trees, gamma (for pruning) and lambda (for regularization)","7d896145":"There are a total of 7043 data rows ","73ce2b6b":"Let's verify whether stratification worked or not.","8aa12558":"Observation from above 2 commands:\n The churn column has some missing values.","41106dbb":"# MISSING DATA PART 1: IDENTIFYING MISSING DATA\n\nThe biggest part of a data science project is ensuring that the data are correctly formatted and fixing it when it is not. The first part of this process is identifying **missing data**.\n\nMissing data is simply a blank space, or a surrogate value like NA, that indicates that we failed to collect data for one of the features. For example: if we forgot to ask someone's age, or forgot to write it down, then we would have a blank space in the dataset for that person's age.\n\nOne thing that is realtively unique about Xgboost is that it has default behaviour for missing data. So all we have to do is idenify missing values and make sure they are set to 0.\n\nIn this section, we will focus on identifying missing values in the dataset.\nFirst, let's see what sort of data is in each column."}}