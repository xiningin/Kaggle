{"cell_type":{"44b1682d":"code","17e71807":"code","bc79f84f":"code","62eddb52":"code","fca46777":"code","40c926df":"code","d219b564":"code","e38d30b3":"code","f4ea2cd4":"code","2319d26b":"code","9c27ed76":"code","d2968fd1":"code","c9a4a0c1":"code","ffe48d4b":"code","bfbab3ce":"code","f79c6c06":"markdown","3a844457":"markdown","591869df":"markdown","242e4f78":"markdown","518f2696":"markdown","4eb255c0":"markdown","26aa95e9":"markdown","1c8194f0":"markdown","a38784f3":"markdown","74d64b23":"markdown","1dbfdbc2":"markdown"},"source":{"44b1682d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","17e71807":"import tensorflow as tf\nimport keras\nfrom keras import backend as K\nfrom keras.layers import Layer\nfrom keras.layers import Concatenate\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, TimeDistributed, LSTM, Embedding, Input\nfrom keras import Model\nimport re\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.translate.bleu_score import sentence_bleu\nfrom nltk.translate.bleu_score import SmoothingFunction\nfrom sklearn.model_selection import train_test_split","bc79f84f":"data = pd.read_csv('\/kaggle\/input\/news-summary\/news_summary.csv', encoding='latin-1')\ndata_more = pd.read_csv('\/kaggle\/input\/news-summary\/news_summary_more.csv', encoding='latin-1')\ndata = pd.concat([data, data_more], axis=0).reset_index(drop=True)\ndata","62eddb52":"contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n\n                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n\n                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n\n                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n\n                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n\n                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n\n                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n\n                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n\n                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n\n                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n\n                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n\n                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n\n                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n\n                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n\n                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n\n                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n\n                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n\n                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n\n                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n\n                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n\n                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n\n                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n\n                           \"you're\": \"you are\", \"you've\": \"you have\"}","fca46777":"StopWords = set(stopwords.words('english'))\ndef preprocess(text):\n    new_text = text.lower() #Lowercasing text.\n    new_text = re.sub(r'\\([^)]*\\)', '', new_text) #Removing punctuations and special characters.\n    new_text = re.sub('\"','', new_text) #Removing double quotes.\n    new_text = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in new_text.split(\" \")]) #Replacing contractions.   \n    new_text = re.sub(r\"'s\\b\",\"\",new_text) #Eliminating apostrophe.\n    new_text = re.sub(\"[^a-zA-Z]\", \" \", new_text) #Removing non-alphabetical characters\n    new_text = ' '.join([word for word in new_text.split() if word not in StopWords]) #Removing stopwords.\n    new_text = ' '.join([word for word in new_text.split() if len(word) >= 3]) #Removing very short words\n    return new_text\n\n#Apply above preprocessing to both text and summary separately.\ntext_cleaned = []\nsumm_cleaned = []\nfor text in data['text']:\n    text_cleaned.append(preprocess(text))\nfor summary in data['headlines']:\n    summ_cleaned.append(preprocess(summary))\nclean_df = pd.DataFrame()\nclean_df['text'] = text_cleaned\nclean_df['headline'] = summ_cleaned\n\n#Replacing empty string summaries with nan values and then dropping those datapoints.\nclean_df['headline'].replace('', np.nan, inplace=True)\nclean_df.dropna(axis=0, inplace=True)\n\n#Adding START and END tokens to summaries for later use.\nclean_df['headline'] = clean_df['headline'].apply(lambda x: '<START>' + ' '+ x + ' '+ '<END>')\nfor i in range(10):\n    print('News: ', clean_df['text'][i])\n    print('Headline:', clean_df['headline'][i])\n    print('\\n')","40c926df":"#Get max length of texts and summaries.\nmax_len_news = max([len(text.split()) for text in clean_df['text']])\nmax_len_headline = max([len(text.split()) for text in clean_df['headline']])\nprint(max_len_news, max_len_headline)","d219b564":"X_train, X_test, y_train, y_test = train_test_split(clean_df['text'], clean_df['headline'], test_size=0.2, random_state=0)\n\n#Keras tokenizer for news text.\nnews_tokenizer = Tokenizer()\nnews_tokenizer.fit_on_texts(list(X_train))\nx_train_seq = news_tokenizer.texts_to_sequences(X_train)\nx_test_seq = news_tokenizer.texts_to_sequences(X_test)\nx_train_pad = pad_sequences(x_train_seq, maxlen=max_len_news, padding='post') #Post padding short texts with 0s.\nx_test_pad = pad_sequences(x_test_seq, maxlen=max_len_news, padding='post')\n#Vocab size of texts.\nnews_vocab = len(news_tokenizer.word_index) + 1\n\n#Keras Tokenizer for summaries.\nheadline_tokenizer = Tokenizer()\nheadline_tokenizer.fit_on_texts(list(y_train))\ny_train_seq = headline_tokenizer.texts_to_sequences(y_train)\ny_test_seq = headline_tokenizer.texts_to_sequences(y_test)\ny_train_pad = pad_sequences(y_train_seq, maxlen=max_len_headline, padding='post')\ny_test_pad = pad_sequences(y_test_seq, maxlen=max_len_headline, padding='post')\n#Vocab size of summaries.\nheadline_vocab = len(headline_tokenizer.word_index) + 1","e38d30b3":"class AttentionLayer(Layer):\n    \"\"\"\n    This class implements Bahdanau attention (https:\/\/arxiv.org\/pdf\/1409.0473.pdf).\n    There are three sets of weights introduced W_a, U_a, and V_a\n     \"\"\"\n\n    def __init__(self, **kwargs):\n        super(AttentionLayer, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert isinstance(input_shape, list)\n        # Create a trainable weight variable for this layer.\n\n        self.W_a = self.add_weight(name='W_a',\n                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n                                   initializer='uniform',\n                                   trainable=True)\n        self.U_a = self.add_weight(name='U_a',\n                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n                                   initializer='uniform',\n                                   trainable=True)\n        self.V_a = self.add_weight(name='V_a',\n                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n                                   initializer='uniform',\n                                   trainable=True)\n\n        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n\n    def call(self, inputs, verbose=False):\n        \"\"\"\n        inputs: [encoder_output_sequence, decoder_output_sequence]\n        \"\"\"\n        assert type(inputs) == list\n        encoder_out_seq, decoder_out_seq = inputs\n        if verbose:\n            print('encoder_out_seq>', encoder_out_seq.shape)\n            print('decoder_out_seq>', decoder_out_seq.shape)\n\n        def energy_step(inputs, states):\n            \"\"\" Step function for computing energy for a single decoder state \"\"\"\n\n            assert_msg = \"States must be a list. However states {} is of type {}\".format(states, type(states))\n            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n\n            \"\"\" Some parameters required for shaping tensors\"\"\"\n            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n            de_hidden = inputs.shape[-1]\n\n            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n            # <= batch_size*en_seq_len, latent_dim\n            reshaped_enc_outputs = K.reshape(encoder_out_seq, (-1, en_hidden))\n            # <= batch_size*en_seq_len, latent_dim\n            W_a_dot_s = K.reshape(K.dot(reshaped_enc_outputs, self.W_a), (-1, en_seq_len, en_hidden))\n            if verbose:\n                print('wa.s>',W_a_dot_s.shape)\n\n            \"\"\" Computing hj.Ua \"\"\"\n            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n            if verbose:\n                print('Ua.h>',U_a_dot_h.shape)\n\n            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n            # <= batch_size*en_seq_len, latent_dim\n            reshaped_Ws_plus_Uh = K.tanh(K.reshape(W_a_dot_s + U_a_dot_h, (-1, en_hidden)))\n            if verbose:\n                print('Ws+Uh>', reshaped_Ws_plus_Uh.shape)\n\n            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n            # <= batch_size, en_seq_len\n            e_i = K.reshape(K.dot(reshaped_Ws_plus_Uh, self.V_a), (-1, en_seq_len))\n            # <= batch_size, en_seq_len\n            e_i = K.softmax(e_i)\n\n            if verbose:\n                print('ei>', e_i.shape)\n\n            return e_i, [e_i]\n\n        def context_step(inputs, states):\n            \"\"\" Step function for computing ci using ei \"\"\"\n            # <= batch_size, hidden_size\n            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n            if verbose:\n                print('ci>', c_i.shape)\n            return c_i, [c_i]\n\n        def create_inital_state(inputs, hidden_size):\n            # We are not using initial states, but need to pass something to K.rnn funciton\n            fake_state = K.zeros_like(inputs)  # <= (batch_size, enc_seq_len, latent_dim\n            fake_state = K.sum(fake_state, axis=[1, 2])  # <= (batch_size)\n            fake_state = K.expand_dims(fake_state)  # <= (batch_size, 1)\n            fake_state = K.tile(fake_state, [1, hidden_size])  # <= (batch_size, latent_dim\n            return fake_state\n\n        fake_state_c = create_inital_state(encoder_out_seq, encoder_out_seq.shape[-1])\n        fake_state_e = create_inital_state(encoder_out_seq, encoder_out_seq.shape[1])  # <= (batch_size, enc_seq_len, latent_dim\n\n        \"\"\" Computing energy outputs \"\"\"\n        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n        last_out, e_outputs, _ = K.rnn(\n            energy_step, decoder_out_seq, [fake_state_e],\n        )\n\n        \"\"\" Computing context vectors \"\"\"\n        last_out, c_outputs, _ = K.rnn(\n            context_step, e_outputs, [fake_state_c],\n        )\n\n        return c_outputs, e_outputs\n\n    def compute_output_shape(self, input_shape):\n        \"\"\" Outputs produced by the layer \"\"\"\n        return [\n            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n        ]","f4ea2cd4":"K.clear_session()\n\nembedding_dim = 300 #Size of word embeddings.\nlatent_dim = 500 #No. of neurons in LSTM layer.\n\nencoder_input = Input(shape=(max_len_news, ))\nencoder_emb = Embedding(news_vocab, embedding_dim, trainable=True)(encoder_input) #Embedding Layer\n\n#Three-stacked LSTM layers for encoder. Return_state returns the activation state vectors, a(t) and c(t), return_sequences return the output of the neurons y(t).\n#With layers stacked one above the other, y(t) of previous layer becomes x(t) of next layer.\nencoder_lstm1 = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.3, recurrent_dropout=0.2)\ny_1, a_1, c_1 = encoder_lstm1(encoder_emb)\n\nencoder_lstm2 = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.3, recurrent_dropout=0.2)\ny_2, a_2, c_2 = encoder_lstm2(y_1)\n\nencoder_lstm3 = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.3, recurrent_dropout=0.2)\nencoder_output, a_enc, c_enc = encoder_lstm3(y_2)\n\n#Single LSTM layer for decoder followed by Dense softmax layer to predict the next word in summary.\ndecoder_input = Input(shape=(None,))\ndecoder_emb = Embedding(headline_vocab, embedding_dim, trainable=True)(decoder_input)\n\ndecoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.3, recurrent_dropout=0.2)\ndecoder_output, decoder_fwd, decoder_back = decoder_lstm(decoder_emb, initial_state=[a_enc, c_enc]) #Final output states of encoder last layer are fed into decoder.\n\n#Attention Layer\nattn_layer = AttentionLayer(name='attention_layer') \nattn_out, attn_states = attn_layer([encoder_output, decoder_output]) \n\ndecoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_output, attn_out])\n\ndecoder_dense = TimeDistributed(Dense(headline_vocab, activation='softmax'))\ndecoder_output = decoder_dense(decoder_concat_input)\n\nmodel = Model([encoder_input, decoder_input], decoder_output)\nmodel.summary()\n","2319d26b":"#Training the model with Early Stopping callback on val_loss.\nmodel.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\ncallback = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\nhistory=model.fit([x_train_pad,y_train_pad[:,:-1]], y_train_pad.reshape(y_train_pad.shape[0],y_train_pad.shape[1], 1)[:,1:] ,epochs=50,callbacks=[callback],batch_size=512, validation_data=([x_test_pad,y_test_pad[:,:-1]], y_test_pad.reshape(y_test_pad.shape[0],y_test_pad.shape[1], 1)[:,1:]))","9c27ed76":"#Encoder inference model with trained inputs and outputs.\nencoder_model = Model(inputs=encoder_input, outputs=[encoder_output, a_enc, c_enc])\n\n#Initialising state vectors for decoder.\ndecoder_initial_state_a = Input(shape=(latent_dim,))\ndecoder_initial_state_c = Input(shape=(latent_dim,))\ndecoder_hidden_state = Input(shape=(max_len_news, latent_dim))\n\n#Decoder inference model\ndecoder_out, decoder_a, decoder_c = decoder_lstm(decoder_emb, initial_state=[decoder_initial_state_a, decoder_initial_state_c])\nattn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state, decoder_out])\ndecoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_out, attn_out_inf])\n\ndecoder_final = decoder_dense(decoder_inf_concat)\ndecoder_model = Model([decoder_input]+[decoder_hidden_state, decoder_initial_state_a, decoder_initial_state_c], [decoder_final]+[decoder_a, decoder_c])","d2968fd1":"#Function to generate output summaries.\ndef decoded_sequence(input_seq):\n    encoder_out, encoder_a, encoder_c = encoder_model.predict(input_seq) #Collecting output from encoder inference model.\n    #Initialise input to decoder neuron with START token. Thereafter output token predicted by each neuron will be used as input for the subsequent.\n    #Single elt matrix used for maintaining dimensions.\n    next_input = np.zeros((1,1))\n    next_input[0,0] = headline_tokenizer.word_index['start']\n    output_seq = ''\n    #Stopping condition to terminate loop when one summary is generated.\n    stop = False\n    while not stop:\n        #Output from decoder inference model, with output states of encoder used for initialisation.\n        decoded_out, trans_state_a, trans_state_c = decoder_model.predict([next_input] + [encoder_out, encoder_a, encoder_c])\n        #Get index of output token from y(t) of decoder.\n        output_idx = np.argmax(decoded_out[0, -1, :])\n        #If output index corresponds to END token, summary is terminated without of course adding the END token itself.\n        if output_idx == headline_tokenizer.word_index['end']: \n            stop = True\n        elif output_idx>0 and output_idx != headline_tokenizer.word_index['start'] :\n            output_token = headline_tokenizer.index_word[output_idx] #Generate the token from index.\n            output_seq = output_seq + ' ' + output_token #Append to summary\n        \n        #Pass the current output index as input to next neuron.\n        next_input[0,0] = output_idx\n        #Continously update the transient state vectors in decoder.\n        encoder_a, encoder_c = trans_state_a, trans_state_c\n        \n    return output_seq        ","c9a4a0c1":"#Print predicted summmaries and actual summaries for 10 texts. \n#We see that some of the summaries go a bit off topic, but the domain concerned, eg. politics, cricket, etc. remains largely same. \n#Further improvements can be done with Attention mechanisms, where each subsequent word generated in the summary receives different weightage\n#from earlier generated words.\npredicted = []\nfor i in range(15):\n    print('News:', X_test.iloc[i])\n    print('Actual Headline:', y_test.iloc[i])\n    print('Predicted Headline:', decoded_sequence(x_test_pad[i].reshape(1, max_len_news)))\n    predicted.append(decoded_sequence(x_test_pad[i].reshape(1, max_len_news)).split())","ffe48d4b":"summaries = list(y_test)\nreferences = []\nfor summ in summaries:\n    ref = summ.split()\n    ref.remove('<START>')\n    ref.remove('<END>')\n    references.append(ref)","bfbab3ce":"refs = []\nsmooth = SmoothingFunction().method4\nfor i in range(15):\n    refs.append(references[i])\n    print(refs, predicted[i])\n    print(sentence_bleu(refs, predicted[i], weights=(0.25,0.25,0.25,0.25)))\n    refs.remove(references[i])","f79c6c06":"* BLEU score, evaluated between 0 to 1, checks how much the predicted sequence is similar to 1 or more reference sequences, in this case our y_test data.\n* Naively, it is the ratio of: No. of words in machine output matching with ref \/ Total words in machine output. \n* A modified version involves clipping the count of matches if a word is present is more often in the machine output than in the reference. \n   Ex: \n       The cat is on the mat (Reference)\n       the the the the  (Machine output)\n       As per the naive evaluation, the BLEU score would be 1, since all the's appear in the reference. But we clip that count to just 2 (see why?), and get 0.5.\n* Further, if the machine output is shorter than the reference, a Brevity Penalty(BP) is applied to the BLEU. \n           BP = exp(1 - r\/c), where r is len(reference) and c is len(candidate output).\n  In case of longer candidates, the BLEU score already penalises it adequately in the modified case.\n* Here, we use the BLEU metric available in the NLTK library.","3a844457":"# VI. Evaluation- BLEU score\n    (BLEU - Bilingual Evaluation Understudy: commonly used for such seq2seq tasks like Machine Translation)","591869df":"**The following dataset will be used for abstractive text summarization:**\n[Kaggle Abstractive Text Summarization](https:\/\/www.kaggle.com\/sunnysai12345\/news-summary)","242e4f78":"# III. Tokenization and Data Split","518f2696":"**Text Summarization - as the name suggests - involves generating short summaries of text data, in a few words or sentences. A good example of this in day-to-day life is the Inshorts news summary app, which generates summaries upto max of ~ 60 words.**\n* Extractive Summarization: This involves using the exact same sentences from the text in the summary, hence extractive.\n* Abstractive Summarization: Here, we capture the semantics of the sentence, and generate summaries using words, which could come from anywhere in our data      vocabulary.","4eb255c0":"# V. Inference Stage: Making Predictions!","26aa95e9":"# II. Data Preprocessing","1c8194f0":"# What is Abstractive Text Summarization?","a38784f3":"# **[Attention Layer Reference](https:\/\/github.com\/jananiarunachalam\/Research-Paper-Summarization\/blob\/master\/Abstractive_Summarization.ipynb)**","74d64b23":"# I. Importing Libraries and Data","1dbfdbc2":" # IV. Training Model (Encoder-Decoder Architecture)"}}