{"cell_type":{"be3dfe17":"code","51d8dbc2":"code","c78b7818":"code","e80120b0":"code","1a3fcec9":"code","154116e1":"code","be7b5498":"code","d2c69f09":"code","eb1c2ba0":"code","20510a52":"code","e422e8ca":"code","6a273c53":"code","93df3798":"code","87545eaf":"code","b126779f":"code","e6275c64":"code","2f5cd9d2":"code","a7fca069":"code","70e40b87":"code","c71aacdf":"code","6d4851a4":"code","3652db9c":"code","6867d5f6":"code","496f308f":"code","726600df":"code","da47222b":"code","3641b5cf":"code","d4dd6834":"code","3e9a9180":"code","33bf47d5":"markdown","7171800d":"markdown","31fc6862":"markdown","27805d7f":"markdown","921768ea":"markdown"},"source":{"be3dfe17":"import pathlib\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nimport tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nimport tensorflow_docs as tfdocs\nimport tensorflow_docs.plots\nimport tensorflow_docs.modeling\n\nprint(tf.__version__)","51d8dbc2":"raw_dataset = pd.read_csv('.\/bike-rental-prediction\/train.csv')","c78b7818":"dataset = raw_dataset.copy()\ndataset.tail()","e80120b0":"# transforming dteday to day month and year\ndataset['day'] = pd.DatetimeIndex(dataset['dteday']).day\ndataset['month'] = pd.DatetimeIndex(dataset['dteday']).month\ndataset['year'] = pd.DatetimeIndex(dataset['dteday']).year\n\n# removing not important columns\ndataset.drop(columns = ['instant','dteday', 'mnth', 'casual', 'registered'],inplace=True)\ndataset.tail()","1a3fcec9":"# splitting to testing and training datasets\ntrain_dataset = dataset.sample(frac=0.8,random_state=0)\ntest_dataset = dataset.drop(train_dataset.index)","154116e1":"train_stats = train_dataset.describe()\ntrain_stats.pop(\"cnt\")\ntrain_stats = train_stats.transpose()\ntrain_stats","be7b5498":"train_labels = train_dataset.pop('cnt')\ntest_labels = test_dataset.pop('cnt')","d2c69f09":"# normalization of dataset\ndef norm(x):\n    return (x - train_stats['mean']) \/ train_stats['std']\n\nnormed_train_data = norm(train_dataset)\nnormed_test_data = norm(test_dataset)","eb1c2ba0":"normed_train_data.head()","20510a52":"from tensorflow.keras import backend as K\n\ndef root_mean_squared_error(y_true, y_pred):\n        return K.sqrt(K.mean(K.square(y_pred - y_true)))\n    \ndef rmsle(y, y0):\n        return K.sqrt(K.mean(K.square(tf.math.log1p(y) - tf.math.log1p(y0))))","e422e8ca":"def build_model():\n    model = keras.Sequential([\n        layers.Dense(64, activation='relu', input_shape=[len(train_dataset.keys())]),\n        layers.Dense(64, activation='relu'),\n        layers.Dense(1, kernel_constraint=tf.keras.constraints.NonNeg())\n    ])\n\n    optimizer = tf.keras.optimizers.RMSprop(0.001)\n\n    model.compile(loss=root_mean_squared_error,\n                optimizer=optimizer,\n                metrics=[root_mean_squared_error, rmsle,'mae', 'mse'])\n\n    return model","6a273c53":"model = build_model()","93df3798":"model.summary()","87545eaf":"EPOCHS = 1000\n\nhistory = model.fit(\n  normed_train_data, train_labels,\n  epochs=EPOCHS, validation_split = 0.2, verbose=0,\n  callbacks=[tfdocs.modeling.EpochDots()])","b126779f":"# history of loss and metrics\nhist = pd.DataFrame(history.history)\nhist['epoch'] = history.epoch\nhist.tail()","e6275c64":"plotter = tfdocs.plots.HistoryPlotter(smoothing_std=2)","2f5cd9d2":"plotter.plot({'Basic': history}, metric = \"loss\")","a7fca069":"loss, root_mean_squared_error, rmsle, mae, mse = model.evaluate(normed_test_data, test_labels, verbose=2)","70e40b87":"test_predictions = model.predict(normed_test_data).flatten()\n\na = plt.axes(aspect='equal')\nplt.scatter(test_labels, test_predictions)\nplt.xlabel('CNT')\nplt.ylabel('CNT Predictions')\nplt.plot()","c71aacdf":"test_labels[0:10]","6d4851a4":"test_predictions[0:10]","3652db9c":"raw_test_dataset = pd.read_csv('.\/bike-rental-prediction\/test.csv')\noriginal_test_dataset = raw_test_dataset.copy()","6867d5f6":"# removing not important columns\noriginal_test_dataset['day'] = pd.DatetimeIndex(original_test_dataset['dteday']).day\noriginal_test_dataset['month'] = pd.DatetimeIndex(original_test_dataset['dteday']).month\noriginal_test_dataset['year'] = pd.DatetimeIndex(original_test_dataset['dteday']).year\n\noriginal_test_dataset_instant  = original_test_dataset.pop('instant')\noriginal_test_dataset.drop(columns = ['dteday', 'mnth'],inplace=True)\n\noriginal_test_dataset.tail()","496f308f":"normed_original_test_data = norm(original_test_dataset)","726600df":"normed_original_test_data.head()","da47222b":"original_test_predictions = model.predict(normed_original_test_data).flatten()","3641b5cf":"original_test_predictions","d4dd6834":"len(original_test_predictions)","3e9a9180":"result_file = open('.\/results\/r2.txt', 'w')\nresult_file.write(\"instant,cnt\\n\")\n\nfor i in range(0, len(original_test_dataset_instant) - 1):\n    result_file.write(str(original_test_dataset_instant[i]) + \",\" + str(original_test_predictions[i]) + \"\\n\")\n    \nresult_file.close()","33bf47d5":"## Train","7171800d":"## Split","31fc6862":"## Normalization of the dataset","27805d7f":"## Generating of csv result","921768ea":"## Model"}}