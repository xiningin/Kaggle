{"cell_type":{"d1027f46":"code","4b8b5b5b":"code","e5cde8dd":"code","6d77e288":"code","0d302fb8":"code","c9ed97da":"code","b4787763":"code","55d5795f":"code","9a93d5b2":"code","c213bb23":"code","76138fde":"code","acbdb360":"code","6b78c614":"code","fc01b341":"code","64bbd082":"code","8ca7fdde":"code","922facf4":"code","999a1d62":"code","dd33a9ab":"code","a846a9c8":"code","07c10d84":"code","6aae8a8c":"code","646997be":"code","dd8ddf46":"code","73d2fa7f":"code","ede247c8":"code","24e18a5b":"code","36882e44":"code","49f3b642":"code","697035ec":"code","def15f6a":"code","3e91e52d":"code","f447ff6c":"code","fbd0fee6":"code","89726a22":"code","51b37ccc":"markdown","39e90847":"markdown","47f7e0ca":"markdown","88e6f7e4":"markdown","766325eb":"markdown","19c9335c":"markdown","bb32e5cc":"markdown","3e8ff5d1":"markdown","8c609c3a":"markdown"},"source":{"d1027f46":"DATA_ROOT = '..\/input\/imet-2020-fgvc7\/'","4b8b5b5b":"from collections import defaultdict, Counter\nimport random\n\nimport pandas as pd\nimport tqdm","e5cde8dd":"def make_folds(n_folds: int) -> pd.DataFrame:\n    df = pd.read_csv(DATA_ROOT + 'train.csv')\n    cls_counts = Counter(cls for classes in df['attribute_ids'].str.split()\n                         for cls in classes)\n    fold_cls_counts = defaultdict(int)\n    folds = [-1] * len(df)\n    for item in tqdm.tqdm(df.sample(frac=1, random_state=42).itertuples(),\n                          total=len(df)):\n        cls = min(item.attribute_ids.split(), key=lambda cls: cls_counts[cls])\n        fold_counts = [(f, fold_cls_counts[f, cls]) for f in range(n_folds)]\n        min_count = min([count for _, count in fold_counts])\n        random.seed(item.Index)\n        fold = random.choice([f for f, count in fold_counts\n                              if count == min_count])\n        folds[item.Index] = fold\n        for cls in item.attribute_ids.split():\n            fold_cls_counts[fold, cls] += 1\n    df['fold'] = folds\n    return df","6d77e288":"df = make_folds(n_folds=5)\ndf.to_csv('folds.csv', index=None)","0d302fb8":"folds = pd.read_csv('..\/input\/imet2002folds\/folds.csv')","c9ed97da":"folds.head(5)","b4787763":"from pathlib import Path\nfrom typing import Callable, List\n\nimport cv2\nimport pandas as pd\nfrom PIL import Image\nimport torch\nfrom torch.utils.data import Dataset","55d5795f":"N_CLASSES = 3474","9a93d5b2":"class TrainDataset(Dataset):\n    def __init__(self, root: Path, df: pd.DataFrame,\n                 image_transform: Callable, debug: bool = True):\n        super().__init__()\n        self._root = root\n        self._df = df\n        self._image_transform = image_transform\n        self._debug = debug\n\n    def __len__(self):\n        return len(self._df)\n\n    def __getitem__(self, idx: int):\n        item = self._df.iloc[idx]\n\n        image = load_transform_image(\n            item, self._root, self._image_transform, debug=self._debug)\n        target = torch.zeros(N_CLASSES)\n        for cls in item.attribute_ids.split():\n            target[int(cls)] = 1\n        return image, target\n\n\nclass TTADataset:\n    def __init__(self, root: Path, df: pd.DataFrame,\n                 image_transform: Callable, tta: int):\n        self._root = root\n        self._df = df\n        self._image_transform = image_transform\n        self._tta = tta\n\n    def __len__(self):\n        return len(self._df) * self._tta\n\n    def __getitem__(self, idx):\n        item = self._df.iloc[idx % len(self._df)]\n        #print(item)\n        image = load_transform_image(item, self._root, self._image_transform)\n        return image, item.id","c213bb23":"import random\nimport math\n\nfrom PIL import Image\nfrom torchvision.transforms import (\n    ToTensor, Normalize, Compose, Resize, CenterCrop, RandomCrop,\n    RandomHorizontalFlip)","76138fde":"train_transform = Compose([\n    RandomCrop(288),\n    RandomHorizontalFlip(),\n])\n\ntest_transform = Compose([\n    #RandomCrop(288),\n    RandomCrop(256),\n    RandomHorizontalFlip(),\n])\n\ntensor_transform = Compose([\n    ToTensor(),\n    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])","acbdb360":"def load_transform_image(\n        item, root: Path, image_transform: Callable, debug: bool = False):\n    image = load_image(item, root)\n    image = image_transform(image)\n    if debug:\n        image.save('_debug.png')\n    return tensor_transform(image)\n\n\ndef load_image(item, root: Path) -> Image.Image:\n    image = cv2.imread(str(root + '\/' + f'{item.id}.png'))\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    return Image.fromarray(image)\n\n\ndef get_ids(root: Path) -> List[str]:\n    return sorted({p.name.split('_')[0] for p in root.glob('*.png')})","6b78c614":"folds = pd.read_csv('..\/input\/imet2002folds\/folds.csv')","fc01b341":"fold = 0","64bbd082":"train_fold = folds[folds['fold'] != 0]\nvalid_fold = folds[folds['fold'] == 0]","8ca7fdde":"from itertools import islice\nimport json\nfrom pathlib import Path\nimport shutil\nimport warnings\nfrom typing import Dict\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import fbeta_score\nfrom sklearn.exceptions import UndefinedMetricWarning\nimport torch\nfrom torch import nn, cuda\nfrom torch.optim import Adam\nimport tqdm","922facf4":"from torch.utils.data import DataLoader","999a1d62":"train_root = DATA_ROOT + 'train'","dd33a9ab":"num_workers = 4\nbatch_size = 64","a846a9c8":"def make_loader(df: pd.DataFrame, image_transform) -> DataLoader:\n        return DataLoader(\n            TrainDataset(train_root, df, image_transform, debug=0),\n            shuffle=True,\n            batch_size=batch_size,\n            num_workers=num_workers,\n        )","07c10d84":"train_loader = make_loader(train_fold, train_transform)\nvalid_loader = make_loader(valid_fold, test_transform)\nprint(f'{len(train_loader.dataset):,} items in train, '\n      f'{len(valid_loader.dataset):,} in valid')","6aae8a8c":"from torch.nn import functional as F\nimport torchvision.models as M\nfrom functools import partial","646997be":"class AvgPool(nn.Module):\n    def forward(self, x):\n        return F.avg_pool2d(x, x.shape[2:])\n\n\ndef create_net(net_cls, pretrained: bool):\n    if pretrained:\n        net = net_cls()\n        model_name = net_cls.__name__\n        weights_path = f'..\/input\/{model_name}\/{model_name}.pth'\n        net.load_state_dict(torch.load(weights_path))\n    else:\n        net = net_cls(pretrained=pretrained)\n    return net\n\n\nclass ResNet(nn.Module):\n    def __init__(self, num_classes,\n                 pretrained=False, net_cls=M.resnet50, dropout=False):\n        super().__init__()\n        self.net = create_net(net_cls, pretrained=pretrained)\n        self.net.avgpool = AvgPool()\n        if dropout:\n            self.net.fc = nn.Sequential(\n                nn.Dropout(),\n                nn.Linear(self.net.fc.in_features, num_classes),\n            )\n        else:\n            self.net.fc = nn.Linear(self.net.fc.in_features, num_classes)\n\n    def fresh_params(self):\n        return self.net.fc.parameters()\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass DenseNet(nn.Module):\n    def __init__(self, num_classes,\n                 pretrained=False, net_cls=M.densenet121):\n        super().__init__()\n        self.net = create_net(net_cls, pretrained=pretrained)\n        self.avg_pool = AvgPool()\n        self.net.classifier = nn.Linear(\n            self.net.classifier.in_features, num_classes)\n\n    def fresh_params(self):\n        return self.net.classifier.parameters()\n\n    def forward(self, x):\n        out = self.net.features(x)\n        out = F.relu(out, inplace=True)\n        out = self.avg_pool(out).view(out.size(0), -1)\n        out = self.net.classifier(out)\n        return out\n\n\nresnet18 = partial(ResNet, net_cls=M.resnet18)\nresnet34 = partial(ResNet, net_cls=M.resnet34)\nresnet50 = partial(ResNet, net_cls=M.resnet50)\nresnet101 = partial(ResNet, net_cls=M.resnet101)\nresnet152 = partial(ResNet, net_cls=M.resnet152)\n\ndensenet121 = partial(DenseNet, net_cls=M.densenet121)\ndensenet169 = partial(DenseNet, net_cls=M.densenet169)\ndensenet201 = partial(DenseNet, net_cls=M.densenet201)\ndensenet161 = partial(DenseNet, net_cls=M.densenet161)","dd8ddf46":"criterion = nn.BCEWithLogitsLoss(reduction='none')","73d2fa7f":"use_cuda = cuda.is_available()","ede247c8":"model = resnet50(num_classes=N_CLASSES, pretrained=True)","24e18a5b":"model","36882e44":"fresh_params = list(model.fresh_params())\nall_params = list(model.parameters())\nif use_cuda:\n    model = model.cuda()","49f3b642":" train_kwargs = dict(\n            model=model,\n            criterion=criterion,\n            train_loader=train_loader,\n            valid_loader=valid_loader,\n            patience=4,\n            init_optimizer=lambda params, lr: Adam(params, lr),\n            use_cuda=use_cuda,\n        )","697035ec":"def load_model(model: nn.Module, path: Path) -> Dict:\n    state = torch.load(str(path))\n    model.load_state_dict(state['model'])\n    print('Loaded model from epoch {epoch}, step {step:,}'.format(**state))\n    return state","def15f6a":"def _reduce_loss(loss):\n    return loss.sum() \/ loss.shape[0]","3e91e52d":"def binarize_prediction(probabilities, threshold: float, argsorted=None,\n                        min_labels=1, max_labels=10):\n    \"\"\" Return matrix of 0\/1 predictions, same shape as probabilities.\n    \"\"\"\n    assert probabilities.shape[1] == N_CLASSES\n    if argsorted is None:\n        argsorted = probabilities.argsort(axis=1)\n    max_mask = _make_mask(argsorted, max_labels)\n    min_mask = _make_mask(argsorted, min_labels)\n    prob_mask = probabilities > threshold\n    return (max_mask & prob_mask) | min_mask\n\n\ndef _make_mask(argsorted, top_n: int):\n    mask = np.zeros_like(argsorted, dtype=np.uint8)\n    col_indices = argsorted[:, -top_n:].reshape(-1)\n    row_indices = [i \/\/ top_n for i in range(len(col_indices))]\n    mask[row_indices, col_indices] = 1\n    return mask\n","f447ff6c":"def validation(\n        model: nn.Module, criterion, valid_loader, use_cuda,\n        ) -> Dict[str, float]:\n    model.eval()\n    all_losses, all_predictions, all_targets = [], [], []\n    with torch.no_grad():\n        for inputs, targets in valid_loader:\n            all_targets.append(targets.numpy().copy())\n            if use_cuda:\n                inputs, targets = inputs.cuda(), targets.cuda()\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            all_losses.append(_reduce_loss(loss).item())\n            predictions = torch.sigmoid(outputs)\n            all_predictions.append(predictions.cpu().numpy())\n    all_predictions = np.concatenate(all_predictions)\n    all_targets = np.concatenate(all_targets)\n\n    def get_score(y_pred):\n        with warnings.catch_warnings():\n            warnings.simplefilter('ignore', category=UndefinedMetricWarning)\n            return fbeta_score(\n                all_targets, y_pred, beta=2, average='samples')\n\n    metrics = {}\n    argsorted = all_predictions.argsort(axis=1)\n    for threshold in [0.10, 0.20]:\n        metrics[f'valid_f2_th_{threshold:.2f}'] = get_score(\n            binarize_prediction(all_predictions, threshold, argsorted))\n    metrics['valid_loss'] = np.mean(all_losses)\n    print(' | '.join(f'{k} {v:.3f}' for k, v in sorted(\n        metrics.items(), key=lambda kv: -kv[1])))\n\n    return metrics","fbd0fee6":"def train( model: nn.Module, criterion, *, params,\n          train_loader, valid_loader, init_optimizer, use_cuda,\n          n_epochs=None, patience=2, max_lr_changes=2) -> bool:\n    \n    lr = 1e-4\n    batch_size = 64\n    n_epochs = 1\n    params = list(params)\n    optimizer = init_optimizer(params, lr)\n\n    model_path = 'model.pt'\n    best_model_path = 'best-model.pt'\n    uptrain = False\n    if uptrain:\n        state = load_model(model, model_path)\n        epoch = state['epoch']\n        step = state['step']\n        best_valid_loss = state['best_valid_loss']\n    else:\n        epoch = 1\n        step = 0\n        best_valid_loss = float('inf')\n    lr_changes = 0\n\n    save = lambda ep: torch.save({\n        'model': model.state_dict(),\n        'epoch': ep,\n        'step': step,\n        'best_valid_loss': best_valid_loss\n    }, str(model_path))\n\n    report_each = 100\n    valid_losses = []\n    lr_reset_epoch = epoch\n    for epoch in range(epoch, n_epochs + 1):\n        model.train()\n        tq = tqdm.tqdm(total=(len(train_loader) * batch_size))\n        tq.set_description(f'Epoch {epoch}, lr {lr}')\n        losses = []\n        tl = train_loader\n        try:\n            mean_loss = 0\n            for i, (inputs, targets) in enumerate(tl):\n                if use_cuda:\n                    inputs, targets = inputs.cuda(), targets.cuda()\n                outputs = model(inputs)\n                loss = _reduce_loss(criterion(outputs, targets))\n                batch_size = inputs.size(0)\n                (batch_size * loss).backward()\n                if (i + 1) % 1 == 0:\n                    optimizer.step()\n                    optimizer.zero_grad()\n                    step += 1\n                tq.update(batch_size)\n                losses.append(loss.item())\n                mean_loss = np.mean(losses[-report_each:])\n                tq.set_postfix(loss=f'{mean_loss:.3f}')\n            tq.close()\n            save(epoch + 1)\n            valid_metrics = validation(model, criterion, valid_loader, use_cuda)\n            \n            valid_loss = valid_metrics['valid_loss']\n            valid_losses.append(valid_loss)\n            if valid_loss < best_valid_loss:\n                best_valid_loss = valid_loss\n                shutil.copy(str(model_path), str(best_model_path))\n            elif (patience and epoch - lr_reset_epoch > patience and\n                  min(valid_losses[-patience:]) > best_valid_loss):\n                lr_changes +=1\n                if lr_changes > max_lr_changes:\n                    break\n                lr \/= 5\n                print(f'lr updated to {lr}')\n                lr_reset_epoch = epoch\n                optimizer = init_optimizer(params, lr)\n        except KeyboardInterrupt:\n            tq.close()\n            print('Ctrl+C, saving snapshot')\n            save(epoch)\n            print('done.')\n            return False\n    return True","89726a22":"train(params=all_params, **train_kwargs)","51b37ccc":"## Transforms","39e90847":"## Load data ","47f7e0ca":"## Train model\n\nFor 0.699 score you nedd train 20+ epochs","88e6f7e4":"You can also use ready-made already folded folds.\n","766325eb":"## Models","19c9335c":"> ## Make Folds\n\nThe first thing we will do is split the train into folds for cross-validation.\n\nFor example, divide by 5, for validation we will use zero fold.","bb32e5cc":"# Yandex Praktikum PyTorch train baseline - LB 0.699","3e8ff5d1":"A small guide prepared for students of [Yandex Praktikum](https:\/\/praktikum.yandex.ru\/) Data Science Track","8c609c3a":"## Implement your dataset to load data\n\nWe implement our own Dataset class for loading data. Its purpose is to load data from the disk and issue a tensor on it with the network input, label and picture identifier.\n\nHere is a link where it\u2019s well explained how to do this with an example: https:\/\/pytorch.org\/tutorials\/beginner\/data_loading_tutorial.html"}}