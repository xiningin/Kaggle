{"cell_type":{"8633f423":"code","c6b6cb66":"code","7a210d74":"code","fa97d34d":"code","d1bc9550":"code","4cb70971":"code","a0f98d91":"code","72dac815":"code","af3fe0b5":"code","3d364312":"code","5dd8b934":"code","b20642da":"code","08928142":"code","396a4687":"code","3b51cdaa":"code","a44248db":"code","12ef96f2":"code","585e71e7":"code","46eca1a3":"code","6274ff5f":"code","b08bd4df":"code","8529b3af":"code","cb10d62d":"code","b9bfe8f5":"code","b838a0da":"code","d18cc39f":"code","0d1f8188":"code","e68be53d":"code","ea2d9e2a":"code","9e24856f":"code","33195db5":"code","29855d5d":"code","da12a888":"code","f0d2eb7d":"code","c6a281ae":"code","fcba2635":"code","ac5dad5d":"code","e28c51da":"code","0226f4c8":"code","ff5c356d":"code","be06d8cc":"code","8007c260":"code","ca6f7f8e":"code","b5006efa":"code","0d8a5101":"code","c6b63a70":"code","085cdb77":"code","7909dfdb":"code","9b8bf3cd":"code","315a5a77":"code","d11ac1bb":"code","69922d43":"code","5af9e42f":"code","d5c20136":"code","5c2eb9ca":"code","892c9c49":"code","a5d89f69":"code","91929dcb":"code","18f513de":"code","21a2b79e":"code","22cc5a16":"code","a417f3e6":"markdown","19980468":"markdown","8bfc1f51":"markdown","5eb74d69":"markdown","4c8b65a8":"markdown","dc6f087d":"markdown","cc6c3d68":"markdown","a5a9c19f":"markdown","a94dd3c0":"markdown","eed4f5df":"markdown","04ad4ee0":"markdown","0a772036":"markdown","03fce5e5":"markdown","821b5a4a":"markdown","77fc94d8":"markdown","9a8c1da8":"markdown","1d2c4eee":"markdown","4605d15f":"markdown","2e1c6490":"markdown","2dfc82b4":"markdown","358f660b":"markdown","cca13b6d":"markdown","7bea6772":"markdown","acd8d301":"markdown","148125a5":"markdown","5d573325":"markdown","d28fc788":"markdown","6f314b30":"markdown","b531d981":"markdown","11880965":"markdown","0aaabe55":"markdown","add85abe":"markdown","f7fffb9f":"markdown","8b728996":"markdown","dad5b616":"markdown","79ba5e96":"markdown"},"source":{"8633f423":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.pyplot import xticks\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_selection import RFE\npd.pandas.set_option('display.max_columns', None)\n\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn import metrics","c6b6cb66":"# load the dataset\nhouse = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\nhouse.head()","7a210d74":"house.shape","fa97d34d":"# check the dataset\nhouse.info()","d1bc9550":"# lets check the target variable \"SalePrice\"\nhouse[\"SalePrice\"].describe()","4cb70971":"# lets check the distribution of saleprice\nsns.distplot(house.SalePrice)","a0f98d91":"# lets drop Id because its of no use to us\nhouse.drop(\"Id\",1,inplace = True)","72dac815":"# Let's display the variables with more than 0 null values\nnull_cols = []\nfor col in house.columns:\n    if house[col].isnull().sum() > 0 :\n        print(\"Column\",col, \"has\", house[col].isnull().sum(),\"null values\")    \n        null_cols.append(col)","af3fe0b5":"# lets visualize the null vaues\nplt.figure(figsize=(12,10))\nsns.barplot(x=house[null_cols].isnull().sum().index, y=house[null_cols].isnull().sum().values)\nxticks(rotation=45)\nplt.show()","3d364312":"# lets check if these null values actually have any relation with the target variable\n\nhouse_eda = house.copy()\n\nfor col in null_cols:\n    house_eda[col] = np.where(house_eda[col].isnull(), 1, 0)  \n\n# lets see if these null values have to do anything with the sales price\nplt.figure(figsize = (16,48))\nfor idx,col in enumerate(null_cols):\n    plt.subplot(10,2,idx+1)\n    sns.barplot(x = house_eda.groupby(col)[\"SalePrice\"].median(),y =house_eda[\"SalePrice\"])\nplt.show()","5dd8b934":"# all missing values for the categorical columns will be replaced by \"None\"\n# all missing values for the numeric columns will be replaced by median of that field\n\nfor col in house.columns:\n    if house[col].dtypes == 'O':\n        house[col] = house[col].replace(np.nan,\"None\")\n    else:\n        house[col] = house[col].replace(np.nan,house[col].median())","b20642da":"# making list of date variables\nyr_vars = []\nfor col in house.columns:\n    if \"Yr\" in col or \"Year\" in col:\n        yr_vars.append(col)\n\nyr_vars = set(yr_vars)\nyr_vars","08928142":"plt.figure(figsize = (15,12))\nfor idx,col in enumerate(yr_vars):\n    plt.subplot(2,2,idx+1)\n    plt.plot(house.groupby(col)[\"SalePrice\"].median())\n    plt.xlabel(col)\n    plt.ylabel(\"SalePrice\")","396a4687":"# creating age variables\nhouse['HouseAge'] =  house['YrSold'] - house['YearBuilt']\n# age of master after remodelling\nhouse['RemodAddAge'] = house['YrSold'] - house['YearRemodAdd']\n# creating age of the garage from year built of the garage to the sale of the master\nhouse['GarageAge'] = house['YrSold'] - house['GarageYrBlt'] \n\n# lets drop original variables\nhouse.drop([\"YearBuilt\",\"YearRemodAdd\",\"GarageYrBlt\"],1,inplace = True)","3b51cdaa":"# lets firs create seperate lists of categorical and numeric columns\ncat_vars = []\nnum_vars = []\nfor col in house.columns.drop(\"SalePrice\"):\n    if house[col].dtypes == 'O':\n        cat_vars.append(col)\n    else:\n        num_vars.append(col)\n\n#lets check the lists created.\nprint(\"List of Numeric Columns:\",num_vars)\nprint(\"\\n\")\nprint(\"List of Categorical Columns:\",cat_vars)","a44248db":"# Let's further seperate the numeric features into continous and discrete numeric features\nnum_cont = []\nnum_disc = []\nfor col in num_vars:\n    if house[col].nunique() > 25: # if variable has more than 25 different values, we consider it as continous variable\n        num_cont.append(col)\n    else:\n        num_disc.append(col)","12ef96f2":"# lets check for the variance in the different continous numeric columns present in the dataset\nhouse.hist(num_cont,bins=50, figsize=(20,15))\nplt.tight_layout(pad=0.4)\nplt.show()","585e71e7":"# lets check the variance in numbers\nfor col in num_cont:\n    print(house[col].value_counts())\n    print(\"\\n\")","46eca1a3":"# lets check for the variance in the different discrete numeric columns present in the dataset\nplt.figure(figsize = (16,96))\nfor idx,col in enumerate(num_disc):\n    plt.subplot(9,2,idx+1)\n    ax=sns.countplot(house[col])\n    #for p in ax.patches:\n    #    ax.annotate(p.get_height(), (p.get_x()+0.1, p.get_height()+10))","6274ff5f":"# lets check for the variance in the categorical columns present in the dataset\nplt.figure(figsize = (20,200))\nfor idx,col in enumerate(cat_vars):\n    plt.subplot(22,2,idx+1)\n    ax=sns.countplot(house[col])\n    xticks(rotation=45)\n    #for p in ax.patches:\n    #    ax.annotate(p.get_height(), (p.get_x()+0.25, p.get_height()+5))","b08bd4df":"# lets check the variance in numbers\nfor col in cat_vars:\n    print(house[col].value_counts())\n    print(\"\\n\")","8529b3af":"# lets drop the variables identified above as they have low variance\nlow_var_num_cont = ['MasVnrArea','BsmtFinSF2','2ndFlrSF','EnclosedPorch','ScreenPorch']\n\nlow_var_num_disc = ['LowQualFinSF','BsmtHalfBath','KitchenAbvGr','3SsnPorch','PoolArea','MiscVal']\n\nlow_var_cat_vars = ['MSZoning','Alley','LandContour','Utilities','LotConfig','Condition1','LandSlope','Condition2','BldgType','RoofStyle','RoofMatl','ExterCond','BsmtCond','BsmtFinType2','Heating','CentralAir','Electrical','Functional','GarageQual','GarageCond','PavedDrive','PoolQC','SaleType','SaleCondition','Street','Fence','MiscFeature']\n\nhouse.drop(low_var_num_cont,1,inplace= True)\nhouse.drop(low_var_num_disc,1,inplace= True)\nhouse.drop(low_var_cat_vars,1,inplace= True)\n\nnum_cont = list(set(num_cont)-set(low_var_num_cont))\nnum_disc = list(set(num_disc)-set(low_var_num_disc))\ncat_vars = list(set(cat_vars)-set(low_var_cat_vars))\n       \nnum_vars = num_cont + num_disc","cb10d62d":"# lets handle skewness in saleprice, lets take log to get normal distribution\nhouse.SalePrice = np.log(house.SalePrice)\n \n# lets check the distribution of saleprice again\nsns.distplot(house.SalePrice)","b9bfe8f5":"# taking the log of numeric variables to hanlde skewness\nnum_features=['LotFrontage', 'LotArea', '1stFlrSF', 'GrLivArea']\nfor col in num_features:\n    house[col] = np.log(house[col])","b838a0da":"# now lets plot the graphs for continous variables\nplt.figure(figsize=(16,48))\nfor idx,col in enumerate(num_cont):\n    plt.subplot(7,2,idx+1)\n    plt.scatter(x = house[col],y=house[\"SalePrice\"])\n    plt.ylabel(\"SalePrice\")\n    plt.xlabel(col)","d18cc39f":"# now lets plot the graphs for discrete variables\nplt.figure(figsize=(16,48))\nfor idx,col in enumerate(num_disc):\n    plt.subplot(10,2,idx+1)\n    sns.boxplot(x = house[col],y=house[\"SalePrice\"])\n    plt.ylabel(\"SalePrice\")\n    plt.xlabel(col)","0d1f8188":"# dropping the variables\nhouse.drop(['MSSubClass','YrSold','MoSold'],1,inplace= True)\n\nnum_disc = list(set(num_disc)-set(['MSSubClass','YrSold','MoSold']))\nnum_vars = list(set(num_vars)-set(['MSSubClass','YrSold','MoSold']))","e68be53d":"# lets check relation of sale price with categorical variables\nplt.figure(figsize=(16,48))\nfor idx,col in enumerate(cat_vars):\n    plt.subplot(10,2,idx+1)\n    sns.boxplot(x = house[col],y=house[\"SalePrice\"])\n    xticks(rotation=45)\n    plt.ylabel(\"SalePrice\")\n    plt.xlabel(col)","ea2d9e2a":"# lets create boxplots to detect outliars detection \nplt.figure(figsize=(16,48))\nfor idx,col in enumerate(num_vars):\n    plt.subplot(11,2,idx+1)\n    plt.boxplot(house[col])\n    plt.xlabel(col)","9e24856f":"#train_set[\"SalePrice\"] = train_sp\n# lets check the variables\n#num_vars = []\n#for col in train_set.columns:\n#    if train_set[col].dtypes != 'O':\n#        num_vars.append(col)\n\nfor col in num_vars:\n    print(house[col].describe(percentiles = [0.05,0.10,0.25,0.50,0.75,0.90,0.95,0.99]))\n    print(\"\\n\")\n    \n# lets handle the outliers\nq3 = house['OpenPorchSF'].quantile(0.99)\nhouse = house[house.OpenPorchSF <= q3]\n    \nq3 = house['GarageArea'].quantile(0.99)\nhouse = house[house.GarageArea <= q3]\n\nq3 = house['TotalBsmtSF'].quantile(0.99)\nhouse = house[house.TotalBsmtSF <= q3]\n\nq3 = house['BsmtUnfSF'].quantile(0.99)\nhouse = house[house.BsmtUnfSF <= q3]\n\nq3 = house['WoodDeckSF'].quantile(0.99)\nhouse = house[house.WoodDeckSF <= q3]\n\nq3 = house['BsmtFinSF1'].quantile(0.99)\nhouse = house[house.BsmtFinSF1 <= q3]","33195db5":"house.shape","29855d5d":"# lets read the test dataset, we will apply all the feature engineering operations on test set as well\ntest_set = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")\n\n# save \"Id\" in a variable and drop the column (as we have already dropped from train dataset)\ntest_set_id = test_set.Id\ntest_set.drop(\"Id\",1,inplace = True)\n\n# save SalePrice to a variable and drop it from training dataset as test dataset does not have this column\ntrain_sp = house.SalePrice\nhouse.drop(\"SalePrice\",1,inplace=True)\n\n# all missing values for the categorical columns will be replaced by \"None\"\n# all missing values for the numeric columns will be replaced by median of that field\nfor col in test_set.columns:\n    if test_set[col].dtypes == 'O':\n        test_set[col] = test_set[col].replace(np.nan,\"None\")\n    else:\n        test_set[col] = test_set[col].replace(np.nan,test_set[col].median())\n\n\n# creating age of the master from year built to the sale of the master\ntest_set['HouseAge'] =  test_set['YrSold'] - test_set['YearBuilt']\n# age of master after remodelling\ntest_set['RemodAddAge'] = test_set['YrSold'] - test_set['YearRemodAdd']\n# creating age of the garage from year built of the garage to the sale of the master\ntest_set['GarageAge'] = test_set['YrSold'] - test_set['GarageYrBlt'] \n\n# lets drop original variables\ntest_set.drop([\"YearBuilt\",\"YearRemodAdd\",\"GarageYrBlt\"],1,inplace = True)\n        \n        \n# skewness in test set\n# taking the log of numeric variables to hanlde skewness\nnum_features=['LotFrontage', 'LotArea', '1stFlrSF', 'GrLivArea']\nfor col in num_features:\n    test_set[col] = np.log(test_set[col])\n\n            \ntest_set.drop(low_var_num_cont,1,inplace= True)\ntest_set.drop(low_var_num_disc,1,inplace= True)\ntest_set.drop(low_var_cat_vars,1,inplace= True)\n\ntest_set.drop(['MSSubClass','YrSold','MoSold'],1,inplace= True)        \n        \n\n# merge the two datasets\nmaster=pd.concat((house,test_set)).reset_index(drop=True)","da12a888":"master.shape","f0d2eb7d":"# In order to perform linear regression, we need to convert categorical variables to numeric variables.\n\n# We have ordinal variables present in the dataest, lets treat them first:\nmaster['ExterQual'] = master['ExterQual'].map({'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1,'None':0})\nmaster['BsmtQual'] = master['BsmtQual'].map({'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1,'None':0})\nmaster['BsmtExposure'] = master['BsmtExposure'].map({'Gd':4,'Av':3,'Mn':2,'No':1,'None':0})\nmaster['BsmtFinType1'] = master['BsmtFinType1'].map({'GLQ':6,'ALQ':5,'BLQ':4,'Rec':3,'LwQ':2,'Unf':1,'None':0})\nmaster['HeatingQC'] = master['HeatingQC'].map({'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1,'None':0})\nmaster['KitchenQual'] = master['KitchenQual'].map({'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1,'None':0})\nmaster['GarageFinish'] = master['GarageFinish'].map({'Fin':3,'RFn':2,'Unf':1,'None':0})\nmaster['FireplaceQu'] = master['FireplaceQu'].map({'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1,'None':0})","c6a281ae":"# now lets create dummy variables for the remaining cateogorical variables\ncat_vars = []\nfor col in master.columns:\n    if master[col].dtypes == 'O':\n        cat_vars.append(col)\n\n# convert into dummies\nmaster_dummies = pd.get_dummies(master[cat_vars], drop_first=True)\n\n# drop categorical variables \nmaster.drop(cat_vars,1,inplace = True)\n\n# concat dummy variables with X\nmaster = pd.concat([master, master_dummies], axis=1)\n\n# lets check the shape of the final dataset\nmaster.shape","fcba2635":"# we have perfomed all the necessary operations on the train and test datasets, time to sperate the two sets again\ntrain_set = master[:1372]\n\ntest_set = master[1372:]","ac5dad5d":"from sklearn.preprocessing import MinMaxScaler\nscaler=MinMaxScaler()\n\ny = train_sp.reset_index(drop=True)\n\nscaler.fit(train_set)\nX = scaler.transform(train_set)\n\n# transform the train and test set, and add on the Id and SalePrice variables\nX = pd.DataFrame(X,columns = train_set.columns).reset_index(drop=True)\nX.head()\n\nscaler.fit(test_set)\ntest_set = scaler.transform(test_set)\ntest_set = pd.DataFrame(test_set,columns = train_set.columns).reset_index(drop=True)","e28c51da":"#Improting the PCA module\nfrom sklearn.decomposition import PCA\npca = PCA(svd_solver='randomized', random_state=42)","0226f4c8":"X.isnull().sum().sort_values(ascending = False)","ff5c356d":"#let's apply PCA\npca.fit(X)","be06d8cc":"X.isnull().sum()","8007c260":"#List of PCA components.It would be the same as the number of variables\npca.components_","ca6f7f8e":"#Let's check the variance ratios\npca.explained_variance_ratio_","b5006efa":"#Plotting the scree plot\n#Making the screeplot - plotting the cumulative variance against the number of components\n%matplotlib inline\nfig = plt.figure(figsize = (12,8))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\nplt.show()","0d8a5101":"#Using incremental PCA for efficiency - saves a lot of time on larger datasets\nfrom sklearn.decomposition import IncrementalPCA\npca_final = IncrementalPCA(n_components=60)","c6b63a70":"df_pca = pd.DataFrame(pca_final.fit_transform(X))\ndf_pca.shape","085cdb77":"df_pca.head()","7909dfdb":"import statsmodels.api as sm","9b8bf3cd":"# Add a constant to get an intercept\nX_train_sm = sm.add_constant(df_pca)\n\n# train the model\nlr = sm.OLS(y, X_train_sm).fit()","315a5a77":"# Performing a summary operation lists out all the different parameters of the regression line fitted\nprint(lr.summary())","d11ac1bb":"# prediction on training dataset\ny_train_pred = lr.predict(X_train_sm)","69922d43":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score","5af9e42f":"r_squared = r2_score(y_train_pred, y)\nr_squared","d5c20136":"from sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\nrms = sqrt(mean_squared_error(y, y_train_pred))\nrms","5c2eb9ca":"# lets make predictions on the test dataset\n\ntest_pca = pd.DataFrame(pca_final.fit_transform(test_set))\n\ntest_pca_sm = sm.add_constant(test_pca)\ny_test_pred = lr.predict(test_pca_sm)","892c9c49":"from sklearn.ensemble import RandomForestRegressor\n\n# training the model\nregr = RandomForestRegressor(n_estimators=50,random_state=0,n_jobs=1)\nregr.fit(df_pca,y)","a5d89f69":"# lets make prediction on training dataset\ny_train_pred = regr.predict(df_pca)","91929dcb":"r_squared = r2_score(y_train_pred, y)\nr_squared","18f513de":"from sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\nrms = sqrt(mean_squared_error(y, y_train_pred))\nrms","21a2b79e":"# lets make prediction on test dataset\ny_test_pred = regr.predict(test_pca)","22cc5a16":"# lets prepare for the prediction submission\nsub = pd.DataFrame()\nsub['Id'] = test_set_id\nsub['SalePrice'] = np.exp(y_test_pred)\nsub.to_csv('submission.csv',index=False)","a417f3e6":"**following variables seems to have low variance:**\n* **LowQualFinSF**\n* **BsmtHalfBath**\n* **KitchenAbvGr**\n* **3SsnPorch**\n* **PoolArea**\n* **MiscVal**\n","19980468":"There are outliers in the dataset, these will be treated in the data engineering section","8bfc1f51":"**Most of the features above seems to have a good relation with SalePrice**\n\n**There are some outliars present which need to be treated**","5eb74d69":"We can drop MSSubClass, YrSold & MoSold as they have no impact on SalePrice","4c8b65a8":"**Make a note of the trend of sale price with the field \"YrSold\", it shows a decreasing trend which seems unreal in real state scenario, price is expected to increase as the time passes by, but here it shows opposite. Let's handle this by creating \"Age\" variables from these variables**","dc6f087d":"**Following variables seems to have low variance:**\n* MasVnArea\n* BsmtFinSF2\n* 2ndFlrSF\n* EnclosedPorch\n* ScreenPorch","cc6c3d68":"**Clearly, performance is better with Random Forest, lets make final submission with RF model**","a5a9c19f":"SalePrice looks good now, lets handle other numeric variables","a94dd3c0":"Linear Regression produced good resuls, but, lets try Random Forest as well","eed4f5df":"# PCA  (Dimensionality reduction)","04ad4ee0":"# Feature Engineering on Test Data Set","0a772036":"**lets check RMSE as this is used by the kaggle competition for to evaluate model's predictive power**","03fce5e5":"######  Wohooo!!! 81 features!! This dataset would surely require lots of data analysis!!!","821b5a4a":"# Null Values Treatment","77fc94d8":"**From Scree plot we can conclude that we 60 PCs can explain around 90% variation of the dataset**","9a8c1da8":"### Outliars Detection","1d2c4eee":"# Bi-Variate analysis with \"SalePrice\"","4605d15f":"### \"SalePrice\" is the target variable, lets see how it looks like!!","2e1c6490":"# Check variation in the feature values","2dfc82b4":"# Data Analysis & EDA","358f660b":"**Great!! There are no negative values in the dataset for sale price which is good**","cca13b6d":"**We have got 88% accuracy on the training dataset**","7bea6772":"# Lets handle Skewness before moving to Bi-Variate Analysis","acd8d301":"**From the above graphs, we can clearly see that the null values have strong relation with the SalePrice, hence we can niether drop the columns with null values, nor we can drop the rows with null values.**","148125a5":"**Now we will see how SalePrice varies with respect to \"Discrete numeric variables\" in the dataset**","5d573325":"**Now we will see how SalePrice varies with respect to \"Continous numeric variables\" in the dataset**","d28fc788":"**As we can see there are lots of null values present in the dataset, we will check them in detail while performing Data Analysis and EDA.**","6f314b30":"# Data Scaling","b531d981":"**There are some Date Variables in the dataset when we performed df.head(),Let's check again**","11880965":"**So many Null Values!!!**\n\n**Let's take a look at the data dictionary, these are not actually the null values, rather these are the features which are not present in the house.**\n\n**For example, let check the field Alley, Value \"NA: here means house has \"No Alley Access\"**","0aaabe55":"**Let's check relation of these fields with the target variable**","add85abe":"# Random Forest Model","f7fffb9f":"# Linear Regression Model","8b728996":"**Saleprice seems to be skewed, This need to be handled else this will adversly impact our model**","dad5b616":"# House Price Prediction","79ba5e96":"**Following variables seems to have low variance:**\n\n* MSZoning\n* Street,\n* Alley\n* LandContour,\n* Utilities,\n* LotConfig\n* Condition1\n* LandSlope\n* Condition2,\n* BldgType\n* RoofStyle\n* RoofMatl\n* ExterCond\n* BsmtCond\n* BsmtFinType2\n* Heating\n* CentralAir\n* Electrical\n* Functional\n* GarageQual\n* GarageCond\n* PavedDrive\n* PoolQC\n* Fence\n* MiscFeature\n* SaleType\n* SaleCondition"}}