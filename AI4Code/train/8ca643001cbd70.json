{"cell_type":{"5e71c381":"code","2045acfa":"code","f839fe6d":"code","9b5f156b":"code","e73477da":"code","57843eaa":"code","d4b5dbf7":"code","951c4660":"code","f5332ab7":"code","afd5f8ff":"code","7b1c4dec":"code","7eb729a7":"code","ada2d73c":"code","d66c0394":"code","08822d92":"code","d46acf24":"code","4b4899ca":"code","85fe6663":"code","d32df3c6":"code","50294933":"code","30a485a3":"code","10152344":"code","df297cc3":"code","2533546f":"code","970a0026":"code","df5a4bb6":"code","43660f43":"code","9c65759c":"code","bea5cb7e":"code","db07b883":"code","ae93dfc2":"code","13f79add":"code","00867b77":"code","535e979e":"code","69d99ee2":"code","b5fd1cdb":"code","7441c01e":"code","c87ea3f3":"code","706453a7":"code","88b78d21":"code","f7d7dd79":"code","781bd658":"code","9ac03266":"code","d5655997":"code","1b9d857b":"code","6cbc6f02":"code","afef57b9":"code","97e21c0d":"code","ff290d92":"code","c038f8c7":"code","add82c11":"code","4f0a7d26":"code","13a902be":"code","317ec24b":"code","e19c7654":"code","2be9a0ff":"code","4dc0938e":"code","31a63919":"code","c2c9c866":"markdown","62066273":"markdown","9362a51c":"markdown","1d28fb38":"markdown","a7651838":"markdown","bff20ee4":"markdown","e54d3a17":"markdown","a7b4c7ff":"markdown","f5a2a7bf":"markdown","5b04078e":"markdown","028255a5":"markdown","fcc7ddbe":"markdown","7fb880c4":"markdown","1fec8f6c":"markdown","e8a4a6b2":"markdown","42cb7b83":"markdown","dfc53509":"markdown","6b05e979":"markdown","f862be12":"markdown","d79d9e2f":"markdown","656de22a":"markdown","9d1c9fc0":"markdown","60ba1099":"markdown","bd45923a":"markdown","e58b03ec":"markdown","96a20e36":"markdown","7c10f041":"markdown","b79bb571":"markdown","21a2b9ec":"markdown","f5421260":"markdown","dc6c9299":"markdown","2c374721":"markdown","73847de2":"markdown"},"source":{"5e71c381":"import math\nimport numpy as np\nimport pandas as pd\nimport os\nimport random\n        \nimport collections\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","2045acfa":"TRAIN_PATH = '..\/input\/cat-in-the-dat\/train.csv'\nTEST_PATH = '..\/input\/cat-in-the-dat\/test.csv'","f839fe6d":"df_train = pd.read_csv(TRAIN_PATH)\ndf_test = pd.read_csv(TEST_PATH)","9b5f156b":"df_train, df_valid = train_test_split(df_train, train_size=0.8, random_state=0)","e73477da":"sns.countplot(x='target', data=df_train)\nplt.title('target')\nplt.xticks([0,1],[0,1])\nplt.show()","57843eaa":"n = df_train.shape[0]\nnc1 = df_train.loc[:, 'target'].sum()\nnc0 = n - nc1\n\npc1_mle = nc1 \/ n\npc0_mle = nc0 \/ n\npc1_map = nc1 * (1+nc1) \/ (n * (1+n))\npc0_map = nc0 * (1+nc0) \/ (n * (1+n))","d4b5dbf7":"pc1_mle","951c4660":"pc0_mle","f5332ab7":"pc1_map","afd5f8ff":"pc0_map","7b1c4dec":"df_train","7eb729a7":"df_train.loc[:, 'bin_3'] = df_train.loc[:, 'bin_3'].replace({'F':0, 'T':1})\ndf_train.loc[:, 'bin_4'] = df_train.loc[:, 'bin_4'].replace({'N':0, 'Y':1})\ndf_train.loc[:, 'bin_0':'bin_4'] = df_train.loc[:, 'bin_0':'bin_4'].astype(np.uint8)","ada2d73c":"replace_map = {}\n\nfor column in df_train.loc[:, 'nom_0':'nom_9'].columns:\n    replace_map[column] = {}\n    for i, key in enumerate(collections.Counter(df_train.loc[:, column]).keys()):\n        replace_map[column][key] = i","d66c0394":"for column in df_train.loc[:, 'nom_0':'nom_9'].columns:\n    df_train.loc[:, column] = df_train.loc[:, column].replace(replace_map[column])\ndf_train.loc[:, 'nom_0':'nom_9'] = df_train.loc[:, 'nom_0':'nom_9'].astype(np.uint8)","08822d92":"df_train.loc[:, 'nom_0':'nom_9']","d46acf24":"sns.heatmap(pd.concat([df_train.loc[:, 'bin_0':'nom_9'], df_train.loc[:, 'target']], axis=1).corr(), fmt='.2f', annot=True)","4b4899ca":"m_mle = [[], []]\nm_map = [[], []]","85fe6663":"for column in df_train.loc[:, 'bin_0':'bin_4'].columns:\n    class1 = df_train.loc[df_train.loc[:, 'target'] == 1, 'bin_0':'nom_9']\n    class0 = df_train.loc[df_train.loc[:, 'target'] == 0, 'bin_0':'nom_9']\n    mle1j = []\n    mle0j = []\n    map1j = []\n    map0j = []\n    for j in range(2):\n        n1j = np.sum(class1.loc[:, column] == j)\n        n0j = np.sum(class0.loc[:, column] == j)\n        mle1j.append(n1j \/ class1.shape[0])\n        mle0j.append(n0j \/ class0.shape[0])\n        map1j.append((n1j * (1+n1j)) \/ (class1.shape[0] * (1+class1.shape[0])))\n        map0j.append((n0j * (1+n0j)) \/ (class0.shape[0] * (1+class0.shape[0])))\n    m_mle[1].append(mle1j)\n    m_mle[0].append(mle0j)\n    m_map[1].append(map1j)\n    m_map[0].append(map0j)\n\n\nfor column in df_train.loc[:, 'nom_0':'nom_9'].columns:\n    class1 = df_train.loc[df_train.loc[:, 'target'] == 1, 'bin_0':'nom_9']\n    class0 = df_train.loc[df_train.loc[:, 'target'] == 0, 'bin_0':'nom_9']\n    mle1j = []\n    mle0j = []\n    map1j = []\n    map0j = []\n    for j in range(len(replace_map[column])):\n        n1j = np.sum(class1.loc[:, column] == j)\n        n0j = np.sum(class0.loc[:, column] == j)\n        mle1j.append(n1j \/ class1.shape[0])\n        mle0j.append(n0j \/ class0.shape[0])\n        map1j.append((n1j * (1+n1j)) \/ (class1.shape[0] * (1+class1.shape[0])))\n        map0j.append((n0j * (1+n0j)) \/ (class0.shape[0] * (1+class0.shape[0])))\n    m_mle[1].append(mle1j)\n    m_mle[0].append(mle0j)\n    m_map[1].append(map1j)\n    m_map[0].append(map0j)","d32df3c6":"df_train.loc[:, 'y_mle_pred'] = np.log(pc1_mle) - np.log(pc0_mle)\ndf_train.loc[:, 'y_map_pred'] = np.log(pc1_map) - np.log(pc0_map)","50294933":"def predict(df):\n    df.loc[:, 'y_mle_pred'] = np.log(pc1_mle) - np.log(pc0_mle)\n    df.loc[:, 'y_map_pred'] = np.log(pc1_map) - np.log(pc0_map)\n    eps = 1e-8\n    \n    @np.vectorize\n    def sigmoid(x):\n        sigmoid_range = 34.538776394910684\n\n        if x <= -sigmoid_range:\n            return 1e-15\n        if x >= sigmoid_range:\n            return 1.0 - 1e-15\n\n        return 1.0 \/ (1.0 + np.exp(-x))\n\n\n    for i, column in enumerate(df.loc[:, 'bin_0':'nom_9'].columns):\n        try:\n            df.loc[:, 'y_mle_pred'] += \\\n                df.loc[:, column].apply(\\\n                    lambda j: np.log(max(eps, m_mle[1][i][j])) - np.log(max(eps, m_mle[0][i][j]))\n                )\n        except TypeError:\n            j = random.randrange(0, len(replace_map[column]))\n            df.loc[:, 'y_mle_pred'] += \\\n                np.log(max(eps, m_mle[1][i][j])) - np.log(max(eps, m_mle[0][i][j]))\n        \n    for i, column in enumerate(df.loc[:, 'bin_0':'nom_9'].columns):\n        try:\n            df.loc[:, 'y_map_pred'] += \\\n                df.loc[:, column].apply(\\\n                    lambda j: np.log(max(eps, m_map[1][i][j])) - np.log(max(eps, m_map[0][i][j]))\n                )\n        except TypeError:\n            j = random.randrange(0, len(replace_map[column]))\n            df.loc[:, 'y_map_pred'] += \\\n                np.log(max(eps, m_map[1][i][j])) - np.log(max(eps, m_map[0][i][j]))\n            \n    df.loc[:, 'y_mle_pred'] = sigmoid(df.loc[:, 'y_mle_pred'])\n    df.loc[:, 'y_map_pred'] = sigmoid(df.loc[:, 'y_map_pred'])","30a485a3":"predict(df_train)","10152344":"df_train.loc[:, 'y_mle_pred':'y_map_pred']","df297cc3":"roc_auc_score(df_train.loc[:, 'target'], df_train.loc[:, 'y_mle_pred'])","2533546f":"roc_auc_score(df_train.loc[:, 'target'], df_train.loc[:, 'y_map_pred'])","970a0026":"accuracy_score(df_train.loc[:, 'target'], (df_train.loc[:, 'y_mle_pred'] > 0.5) * 1)","df5a4bb6":"accuracy_score(df_train.loc[:, 'target'], (df_train.loc[:, 'y_map_pred'] > 0.5) * 1)","43660f43":"for column in df_train.loc[:, 'ord_0':'ord_5'].columns:\n    replace_map[column] = {}\n    for i, key in enumerate(collections.Counter(df_train.loc[:, column]).keys()):\n        replace_map[column][key] = i","9c65759c":"for column in df_train.loc[:, 'ord_0':'ord_5'].columns:\n    df_train.loc[:, column] = df_train.loc[:, column].replace(replace_map[column])\ndf_train.loc[:, 'ord_0':'ord_5'] = df_train.loc[:, 'ord_0':'ord_5'].astype(np.uint8)","bea5cb7e":"for column in df_train.loc[:, 'day':'month'].columns:\n    replace_map[column] = {}\n    for i, key in enumerate(collections.Counter(df_train.loc[:, column]).keys()):\n        replace_map[column][key] = i","db07b883":"df_train.loc[:, 'day':'month'] = df_train.loc[:, 'day':'month'] - 1","ae93dfc2":"for column in df_train.loc[:, 'ord_0':'month'].columns:\n    class1 = df_train.loc[df_train.loc[:, 'target'] == 1, 'ord_0':'month']\n    class0 = df_train.loc[df_train.loc[:, 'target'] == 0, 'ord_0':'month']\n    mle1j = []\n    mle0j = []\n    map1j = []\n    map0j = []\n    for j in range(len(replace_map[column])):\n        n1j = np.sum(class1.loc[:, column] == j)\n        n0j = np.sum(class0.loc[:, column] == j)\n        mle1j.append(n1j \/ class1.shape[0])\n        mle0j.append(n0j \/ class0.shape[0])\n        map1j.append((n1j * (1+n1j)) \/ (class1.shape[0] * (1+class1.shape[0])))\n        map0j.append((n0j * (1+n0j)) \/ (class0.shape[0] * (1+class0.shape[0])))\n    m_mle[1].append(mle1j)\n    m_mle[0].append(mle0j)\n    m_map[1].append(map1j)\n    m_map[0].append(map0j)","13f79add":"def predict_all(df):\n    df.loc[:, 'y_mle_pred'] = np.log(pc1_mle) - np.log(pc0_mle)\n    df.loc[:, 'y_map_pred'] = np.log(pc1_map) - np.log(pc0_map)\n    eps = 1e-8\n    \n    @np.vectorize\n    def sigmoid(x):\n        sigmoid_range = 34.538776394910684\n\n        if x <= -sigmoid_range:\n            return 1e-15\n        if x >= sigmoid_range:\n            return 1.0 - 1e-15\n\n        return 1.0 \/ (1.0 + np.exp(-x))\n\n\n    for i, column in enumerate(df.loc[:, 'bin_0':'month'].columns):\n        try:\n            df.loc[:, 'y_mle_pred'] += \\\n                df.loc[:, column].apply(\\\n                    lambda j: np.log(max(eps, m_mle[1][i][j])) - np.log(max(eps, m_mle[0][i][j]))\n                )\n        except TypeError:\n            j = random.randrange(0, len(replace_map[column]))\n            df.loc[:, 'y_mle_pred'] += \\\n                np.log(max(eps, m_mle[1][i][j])) - np.log(max(eps, m_mle[0][i][j]))\n        \n    for i, column in enumerate(df.loc[:, 'bin_0':'month'].columns):\n        try:\n            df.loc[:, 'y_map_pred'] += \\\n                df.loc[:, column].apply(\\\n                    lambda j: np.log(max(eps, m_map[1][i][j])) - np.log(max(eps, m_map[0][i][j]))\n                )\n        except TypeError:\n            j = random.randrange(0, len(replace_map[column]))\n            df.loc[:, 'y_map_pred'] += \\\n                np.log(max(eps, m_map[1][i][j])) - np.log(max(eps, m_map[0][i][j]))\n            \n    df.loc[:, 'y_mle_pred'] = sigmoid(df.loc[:, 'y_mle_pred'])\n    df.loc[:, 'y_map_pred'] = sigmoid(df.loc[:, 'y_map_pred'])","00867b77":"predict_all(df_train)","535e979e":"df_train.loc[:, 'y_mle_pred':'y_map_pred']","69d99ee2":"roc_auc_score(df_train.loc[:, 'target'], df_train.loc[:, 'y_mle_pred'])","b5fd1cdb":"roc_auc_score(df_train.loc[:, 'target'], df_train.loc[:, 'y_map_pred'])","7441c01e":"accuracy_score(df_train.loc[:, 'target'], (df_train.loc[:, 'y_mle_pred'] > 0.5) * 1)","c87ea3f3":"accuracy_score(df_train.loc[:, 'target'], (df_train.loc[:, 'y_map_pred'] > 0.5) * 1)","706453a7":"def preprocess(df):\n    df.loc[:, 'bin_3'] = df.loc[:, 'bin_3'].replace({'F':0, 'T':1})\n    df.loc[:, 'bin_4'] = df.loc[:, 'bin_4'].replace({'N':0, 'Y':1})\n    df.loc[:, 'bin_0':'bin_4'] = df.loc[:, 'bin_0':'bin_4'].astype(np.uint8)\n    \n    for column in df.loc[:, 'nom_0':'nom_9'].columns:\n        df.loc[:, column] = df.loc[:, column].replace(replace_map[column])\n        \n    for column in df.loc[:, 'ord_0':'ord_5'].columns:\n        df.loc[:, column] = df.loc[:, column].replace(replace_map[column])\n        \n    df.loc[:, 'day':'month'] = df.loc[:, 'day':'month'] - 1","88b78d21":"preprocess(df_valid)","f7d7dd79":"predict(df_valid)","781bd658":"df_valid.loc[:, 'y_mle_pred':'y_map_pred']","9ac03266":"roc_auc_score(df_valid.loc[:, 'target'], df_valid.loc[:, 'y_mle_pred'])","d5655997":"roc_auc_score(df_valid.loc[:, 'target'], df_valid.loc[:, 'y_map_pred'])","1b9d857b":"accuracy_score(df_valid.loc[:, 'target'], (df_valid.loc[:, 'y_mle_pred'] > 0.5) * 1)","6cbc6f02":"accuracy_score(df_valid.loc[:, 'target'], (df_valid.loc[:, 'y_map_pred'] > 0.5) * 1)","afef57b9":"predict_all(df_valid)","97e21c0d":"roc_auc_score(df_valid.loc[:, 'target'], df_valid.loc[:, 'y_mle_pred'])","ff290d92":"roc_auc_score(df_valid.loc[:, 'target'], df_valid.loc[:, 'y_map_pred'])","c038f8c7":"accuracy_score(df_valid.loc[:, 'target'], (df_valid.loc[:, 'y_mle_pred'] > 0.5) * 1)","add82c11":"accuracy_score(df_valid.loc[:, 'target'], (df_valid.loc[:, 'y_map_pred'] > 0.5) * 1)","4f0a7d26":"preprocess(df_test)","13a902be":"predict_all(df_test)","317ec24b":"df_test.loc[:, 'y_mle_pred':'y_map_pred']","e19c7654":"df_test.loc[:, 'target'] = df_test.loc[:, 'y_map_pred']","2be9a0ff":"submission = df_test.loc[:, ['id', 'target']]","4dc0938e":"submission","31a63919":"submission.to_csv('submission.csv', index=False)","c2c9c866":"### validation 2: Apply one-hot encoding also to sequential features (ord_0 to ord_5)","62066273":"With maximum likelihood estimate, we obtain prior class probability:\n$$\np_{\\mathrm{MLE}}(C_{1})=\\frac{N(C_{1})}{N}, \\:\\:\\:\np_{\\mathrm{MLE}}(C_{0})=\\frac{N(C_{0})}{N}\n$$\n\nWith MAP estimate:\n$$\np_{\\mathrm{MAP}}(C_{1})=\\frac{(N(C_{1}))(1+N(C_{1}))}{N(1+N)}, \\:\\:\\:\np_{\\mathrm{MAP}}(C_{0})=\\frac{(N(C_{0}))(1+N(C_{0}))}{N(1+N)}\n$$","9362a51c":"# Submission","1d28fb38":"### 1-3. naive bayes assumption","a7651838":"In probabilistic generative model, we obtain posterior probability for class by Bayes theorem:\n\n$$\n\\begin{align}\np(C_{k}|\\mathbb{x})\n&=\n\\frac{p(\\mathbb{x}|C_{k})p(C_{k})}{p(\\mathbb{x})}\\\\\n&=\n\\frac{p(\\mathbb{x}|C_{k})p(C_{k})}{\\sum_{c}p(\\mathbb{x}|c)p(c)}\n\\end{align}\n$$","bff20ee4":"Under Naive bayes assumption -- if we treat each feature as independent -- , we have this class-conditional distribution form:\n$$ p(\\mathbb{x}|C_{k})=\\prod_{i=1}^{D}\\prod_{j=0}^{l_{i}-1}\\mu_{kij}^{\\delta_{x_{i},j}}$$\n\nwhere\n\n$$\n\\delta_{x_{i},j} = \n\\begin{cases}\n1 \\;\\;\\mathrm{when}\\;\\; x_{i}=j \\\\\n0 \\;\\;\\mathrm{when}\\;\\; x_{i}\\ne j \\\\\n\\end{cases}\n\\;\\;,\\;\\;\n\\sum_{j=0}^{l_{i}-1} \\mu_{kij}=1\n$$","e54d3a17":"## 0. Outline","a7b4c7ff":"As naive bayes assumption is valid, we obtain class-conditional distribution as this:\n$$ \np(\\mathbb{x}|C_{k})=\\prod_{i=1}^{D}\\prod_{j=0}^{l_{i}-1}\\mu_{kij}^{\\delta_{x_{i},j}}$$\n\nwhere\n\n$$\n\\delta_{x_{i},j} = \n\\begin{cases}\n1 \\;\\;\\mathrm{when}\\;\\; x_{i}=j \\\\\n0 \\;\\;\\mathrm{when}\\;\\; x_{i}\\ne j \\\\\n\\end{cases}\n\\;\\;,\\;\\;\n\\sum_{j=0}^{l_{i}-1} \\mu_{kij}=1\n$$\n\nParameters can be determined by maximum likelihood estimation:\n$$\n\\mu_{\\mathrm{MLE},kij} = \\frac{N(\\mathbb{x}\\in C_{k}, x_{i}=j)}{N(\\mathbb{x}\\in C_{k})}\n$$\n\nSimilarly, determined parameters with MAP estimation are:\n$$\n\\mu_{\\mathrm{MAP},kij} =\n\\frac{N(\\mathbb{x}\\in C_{k}, x_{i}=j)\\bigl\\{1+N(\\mathbb{x}\\in C_{k}, x_{i}=j)\\bigr\\}}\n{N(\\mathbb{x}\\in C_{k})\\bigl\\{1+N(\\mathbb{x}\\in C_{k})\\bigr\\}}\n$$","f5a2a7bf":"Little correletion is seen among features bin_0 to nom_9:","5b04078e":"Then we can predict class as this:\n$$\ny_{n} = \\biggl\\{\n\\begin{array}{l}\n1 \\;\\;\\mathrm{when}\\; \\sigma(a)>0.5\\\\\n0 \\;\\;\\mathrm{when}\\; \\sigma(a)<0.5\\\\\n\\end{array}\n$$","028255a5":"For more details, refer to PRML book chapter 4.2.3:\n[https:\/\/www.microsoft.com\/en-us\/research\/people\/cmbishop\/prml-book\/](https:\/\/www.microsoft.com\/en-us\/research\/people\/cmbishop\/prml-book\/)","fcc7ddbe":"# Probabilistic generative model to discrete features (feat. PRML 4.2.3)","7fb880c4":"Let us make a prediction!\n\nActivation value of logistic sigmoid is obtained as this:\n\n$$\n\\begin{align}\na &= \\ln\\frac{p(\\mathbb{x}|C_{1})p(C_{1})}{p(\\mathbb{x}|C_{0})p(C_{0})} \\\\\n&=\n\\ln\\prod_{i=1}^{D}\\prod_{j=0}^{l_{i}-1}\\mu_{1ij}^{\\delta_{x_{i},j}}\n- \\ln\\prod_{i=1}^{D}\\prod_{j=0}^{l_{i}-1}\\mu_{0ij}^{\\delta_{x_{i},j}}\n+ \\ln p(C_{1})\n- \\ln p(C_{0})\n\\end{align}\n$$","1fec8f6c":"Since this competition is a 2-class classification task, the posterior probability for class is given as this:\n$$\np(C_{1}|\\mathbb{x}) = \\sigma(a) = \\frac{1}{1+e^{-a}},\n$$\n\nwhere\n$$\na = \\ln\\frac{p(\\mathbb{x}|C_{1})p(C_{1})}{p(\\mathbb{x}|C_{0})p(C_{0})}\n$$","e8a4a6b2":"### 1-1. prior class probability","42cb7b83":"## 2. Same procedure to sequential features, day and month","dfc53509":"Thus, all we need to do is to calculate class-conditional distribution and prior class probability:\n$$\np(\\mathbb{x}|C_{1}), \\;\\;p(\\mathbb{x}|C_{0}), \\;\\;p(C_{1}), \\;\\;p(C_{0})\n$$","6b05e979":"Finally, we obtain posterior probability for class is given as this:\n$$\np(C_{1}|\\mathbb{x}) = \\sigma(a) = \\frac{1}{1+e^{-a}},\n$$\n\nwhere\n\n$$\n\\begin{align}\na &= \\ln\\frac{p(\\mathbb{x}|C_{1})p(C_{1})}{p(\\mathbb{x}|C_{0})p(C_{0})} \\\\\n&=\n\\ln\\prod_{i=1}^{D}\\prod_{j=0}^{l_{i}-1}\\mu_{\\mathrm{MLE},1ij}^{\\delta_{x_{i},j}}\n- \\ln\\prod_{i=1}^{D}\\prod_{j=0}^{l_{i}-1}\\mu_{\\mathrm{MLE},0ij}^{\\delta_{x_{i},j}}\n+ \\ln p_{\\mathrm{MLE}}(C_{1})\n- \\ln p_{\\mathrm{MLE}}(C_{0}) \\\\\n&=\n\\sum_{i=1}^{D}\n\\sum_{j=0}^{l_{i}-1}\n\\delta_{x_{i},j} \\ln \\frac{N(x\\in C_{1}, x_{i}=j)}{N(x\\in C_{0}, x_{i}=j)}\n+ \\ln\\frac{N(x\\in C_{1})}{N(x\\in C_{0})}\n\\end{align}\n$$","f862be12":"Let input is a D-dimention vector of discrete components:\n$$ \\mathbb{x}=(x_{1}, ..., x_{D}),\\;\\;x_{i}\\in\\{0,1,...,l_{i}-1\\} $$","d79d9e2f":"## 1. Coding","656de22a":"#### (convert feature to numbers for nom_0 to nom_9)","9d1c9fc0":"The same procedure is available to test data.","60ba1099":"### 1-4. class-conditional distribution","bd45923a":"### 1-5. prediction","e58b03ec":"### validation 1: Probabilistic generative model to discrete features (bin_0 to nom_9)","96a20e36":"Here, parameters to be determined are:\n\n$$\n\\mu_{kij}, \\;\\;p(C_{1}), \\;\\;p(C_{0})\n$$\n\nWith maximum likelihood estimate, they are determined as this:\n\n$$\n\\mu_{\\mathrm{MLE},kij} = \\frac{N(\\mathbb{x}\\in C_{k}, x_{i}=j)}{N(\\mathbb{x}\\in C_{k})} \\\\\np_{\\mathrm{MLE}}(C_{k})=\\frac{N(\\mathbb{x}\\in C_{k})}{N(\\mathbb{x})}\n$$\n\nTo supplement, with MAP estimate, they are determined as this:\n\n$$\n\\mu_{\\mathrm{MAP},kij} =\n\\frac{N(\\mathbb{x}\\in C_{k}, x_{i}=j)\\bigl\\{1+N(\\mathbb{x}\\in C_{k}, x_{i}=j)\\bigr\\}}\n{N(\\mathbb{x}\\in C_{k})\\bigl\\{1+N(\\mathbb{x}\\in C_{k})\\bigr\\}}\n\\\\\np_{\\mathrm{MAP}}(C_{k})\n=\n\\frac{N(\\mathbb{x}\\in C_{k})\\bigl\\{1+N(\\mathbb{x}\\in C_{k})\\bigr\\}}\n{N(\\mathbb{x})\\bigl\\{1+N(\\mathbb{x})\\bigr\\}}\n$$","7c10f041":"### 1-2. feature encoding","b79bb571":"For simplicity, we simply apply the same technique to sequential features (ord_0 to ord_5) and cyclic features (day, month).\n\nThat is, here we treat them like categorical features and ignore their sequence.","21a2b9ec":"#### (convert feature to numbers for bin_3, bin_4)","f5421260":"We need to convert non-numerical features into numerical features.","dc6c9299":"## Validation","2c374721":"By this probability, we can predict the class for new inputs with a threshold of 0.5:\n$$\n\\begin{cases}\n\\mathbb{x} \\in C_{1} \\;\\; \\mathrm{when} \\; p(C_{1}|\\mathbb{x}) > 0.5\\\\\n\\mathbb{x} \\in C_{0} \\;\\; \\mathrm{when} \\; p(C_{1}|\\mathbb{x}) < 0.5\n\\end{cases}\n$$","73847de2":"Therefore, naive bayes assumption is valid -- we may treat features bin_0 to nom_9 as independent."}}