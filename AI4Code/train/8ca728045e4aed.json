{"cell_type":{"b476bc9e":"code","fa188cb7":"code","36622bf3":"code","8da96d41":"code","8f1b47b1":"code","4b029203":"code","a8aadeb8":"code","55afaef4":"code","9a593cf8":"markdown","c6cf03ae":"markdown","75edaa43":"markdown","111dafb6":"markdown","36be473e":"markdown","a34a49d9":"markdown","a19d6b60":"markdown","c1939bf6":"markdown","9274c0e7":"markdown"},"source":{"b476bc9e":"import random\nimport pprint\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, accuracy_score, recall_score, precision_score\nimport warnings\nwarnings.simplefilter(action='ignore', category=UserWarning)\nwarnings.simplefilter(action='ignore', category=DeprecationWarning)\ndf = pd.read_csv(\"..\/input\/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\ndf.head(3)","fa188cb7":"def preprocessing(df):\n    \"\"\"Preprocess df and return X (train features) and Y (target feature).\"\"\"\n    # Impute missing values on TotalCharges\n    df['TotalCharges'] = df['TotalCharges'].replace(\" \", 0).astype('float32')\n    # Drop customer id\n    df.drop(['customerID'], axis=1, inplace=True)\n    # Encode categorical features\n    cat_cols = [c for c in df.columns if df[c].dtype == 'object' or c == 'SeniorCitizen']\n    for col in cat_cols:\n        if df[col].nunique() == 2:\n            df[col], _ = pd.factorize(df[col])\n        else:\n            df = pd.get_dummies(df, columns=[col])\n    # Drop target column and some correlated features\n    drop_features = ['OnlineSecurity_No internet service', 'OnlineBackup_No internet service',\n                     'DeviceProtection_No internet service', 'TechSupport_No internet service',\n                     'StreamingTV_No internet service', 'StreamingMovies_No internet service',\n                     'PhoneService', 'Churn']\n    feats = [c for c in df.columns if c not in drop_features]\n    return df[feats], df['Churn']\nx, y = preprocessing(df)\nx.head(3)","36622bf3":"# Split dataset in train and test\ntrain_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.2, random_state=50)\n# Create a training and testing dataset (lightgbm api object)\ntrain_set = lgb.Dataset(data= train_x, label= train_y, silent=-1)\ntest_set = lgb.Dataset(data= test_x, label= test_y, silent=-1)","8da96d41":"def hyperparameter_random_search(train_set, params_grid, fixed_params, max_evals,\n                                 num_folds, print_params=False):\n    \"\"\"Random search for hyperparameter optimization\"\"\"\n    # Dataframe for results\n    results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n                                  index = list(range(max_evals)))\n    \n    # Keep searching until reach max evaluations\n    for i in range(max_evals):\n        # Choose random hyperparameters\n        hyperparameters = {k: random.sample(v, 1)[0] for k, v in params_grid.items()}\n        hyperparameters.update(fixed_params)\n        if print_params:\n            print(hyperparameters)\n        # Evaluate randomly selected hyperparameters\n        eval_results = objective(train_set, hyperparameters, i, num_folds)\n        # Add results to our dataframe\n        results.loc[i, :] = eval_results\n    # Sort with best score on top\n    results.sort_values('score', ascending=False, inplace=True)\n    results.reset_index(inplace=True)\n    return results\n\ndef objective(train_set, hyperparameters, iteration, num_folds):\n    \"\"\"Objective function for grid and random search. Returns\n       the cross-validation score from a set of hyperparameters.\"\"\"\n\n     # Perform n_folds cross validation\n    hyperparameters['verbose'] = -1\n    cv_results = lgb.cv(hyperparameters, train_set, num_boost_round=10000, nfold=num_folds,\n                        early_stopping_rounds=50, metrics ='auc', seed=50)\n    score = cv_results['auc-mean'][-1]\n    estimators = len(cv_results['auc-mean'])\n    hyperparameters['n_estimators'] = estimators \n    return score, hyperparameters, iteration","8f1b47b1":"params_grid = {\n    'num_leaves': list(range(8, 255)),  # Control tree size\n    # Percentage (sample) of columns and rows\n    'colsample_bytree': list(np.linspace(0.4, 0.99)),\n    'subsample': list(np.linspace(0.4, 0.99)),\n    # Min data points to create a leaf\n    'min_child_samples': list(range(1, 101, 5)),\n    # Regularization\n    'reg_alpha': list(np.linspace(0, 1)),\n    'reg_lambda': list(np.linspace(0, 1)),\n}\n\nfixed_params = {'boosting_type': 'rf', 'objective': 'binary',\n                'subsample_freq': 1, 'n_jobs':4}\nres = hyperparameter_random_search(train_set, params_grid, fixed_params, 1000, 5)\nres.head()","4b029203":"# Create, train, test model\nmodel = lgb.LGBMClassifier(**res.loc[0, 'params'], random_state=50)\nmodel.fit(train_x.values, train_y.values)\npredict_proba = model.predict_proba(test_x.values)[:, 1]\npredict_labels = model.predict(test_x.values)\n\n# Print final results\nprint(\"Scores on test set: {:.4f} ROC AUC,  {:.4f} accuracy, {:.4f} recall, {:.4f} precision\"\n      .format(roc_auc_score(test_y, predict_proba),\n              accuracy_score(test_y, predict_labels),\n              recall_score(test_y, predict_labels), \n              precision_score(test_y, predict_labels)))","a8aadeb8":"# Possible hyperparameters (grid)\nparam_grid = {\n    'num_leaves': list(range(7, 95)),\n    'learning_rate': list(np.logspace(np.log(0.005), np.log(0.2))),\n    'subsample_for_bin': list(range(20000, 300000, 20000)),\n    'min_child_samples': list(range(20, 500, 5)),\n    'reg_alpha': list(np.linspace(0, 1)),\n    'reg_lambda': list(np.linspace(0, 1)),\n    'colsample_bytree': list(np.linspace(0.4, 1, 10)),\n    'subsample': list(np.linspace(0.5, 1, 100)),\n}\nfixed_params = {'boosting': 'gbdt', 'objective': 'binary', 'n_jobs': 4}\nres = hyperparameter_random_search(train_set, param_grid, fixed_params, 1000, 5)\nres.head()","55afaef4":"# Create, train, test model\nmodel = lgb.LGBMClassifier(**res.loc[0, 'params'], random_state=50)\nmodel.fit(train_x.values, train_y.values)\npredict_proba = model.predict_proba(test_x.values)[:, 1]\npredict_labels = model.predict(test_x.values)\n\n# Print final results\nprint(\"Scores on test set: {:.4f} ROC AUC,  {:.4f} accuracy, {:.4f} recall, {:.4f} precision\"\n      .format(roc_auc_score(test_y, predict_proba),\n              accuracy_score(test_y, predict_labels),\n              recall_score(test_y, predict_labels), \n              precision_score(test_y, predict_labels)))","9a593cf8":"<h2>3. Random Forests<\/h2>\n\nThe ideia behind Random Forests is to build multiple decision trees and merges them together to get a more accurate and stable prediction. In each estimator, only a random subset of the features is taken into consideration for splitting a node, therefore adding additional randomness to the final model. Random Forests can be considered an averaging ensemble method.","c6cf03ae":"<h3>Testing the model<\/h3>","75edaa43":"<h2>Introduction<\/h2>\n\nIn this notebook we will be using ensemble methods to predict if a customer left the company in the last month. The goal of ensemble methods is to combine the predictions of several base estimators built with a given learning algorithm in order to improve robustness. There are two general categories of ensemble methods: averaging and boosting methods. Decision trees will be used as our base estimator for both.","111dafb6":"<h2>2. Hyperparameter tunning<\/h2>\n\nWe have to define many hyperparameters since we have the ensemble estimator and the base estimator. Instead of doing this by hand, the next function will randomly pick a set of parameters and test it with a KFold cross-validation scheme. The LightGBM library has a built-in function to easily implement this validation method.\n\nThe procedure can be described with the following steps:\n* Randomly pick a subset of hyperparameters\n* Train and validate our model with KFold cross-validation (second function)\n* Repeat this procedure MAX_EVALS times\n* Get the best cross-validation score and hyperparameters\n\nEarly stopping will be used to stop the algorithm when the validation score doesn't improve for 50 boosting rounds.","36be473e":"<h2>4. Gradient Boosting<\/h2>\n\nIn gradient boosting, the predictors are not made independently, but sequentially. In each iteration, a decision tree is fit on the error from the previous round. It's usually better to use shallow trees (weak learners) and a low learning rate, so each iteration does small improvements to the overall problem. ","a34a49d9":"<h2>1. Preprocessing<\/h2>\n\nSince all ensemble methods in this notebook are based on Decision Trees, we don't need feature scalling. However, we need to encode categorical features and drop a few columns. ","a19d6b60":"<h2>5. Conclusion<\/h2>\n\nIn this short notebook, we compared the performance of two ensemble methods in a binary classification problem. The first algorithm (Random Forests) is an averaging ensemble technique that train multiple decision trees in parallel and combine their predictions. Our second classifier was a boosting ensemble method, also using decision trees as base estimator. \n\nHyperparameters were found using random search with a KFold validation scheme over more than one thousand tries for each estimator.","c1939bf6":"<h3>Split data<\/h3>\n\nDivide our data in 80% for training and 20% for our final test.","9274c0e7":"<h3>Testing the model<\/h3>"}}