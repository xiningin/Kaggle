{"cell_type":{"0e2facfa":"code","5da7bd33":"code","02bbfb52":"code","d5b4dd05":"code","6be0a949":"code","d110b864":"code","a31fe06f":"code","4e80b1b5":"code","4ee55b05":"code","0496988d":"code","a945d740":"code","137b48b9":"code","fec95ed3":"code","0b2eba86":"code","f569f1de":"code","c7f87474":"code","074c0905":"code","9509e3b4":"code","ddbf0b43":"code","d88d8a56":"markdown","23113f9b":"markdown","dbac4dab":"markdown","736613c9":"markdown","8d13f1fe":"markdown","a91a73eb":"markdown","8fa35308":"markdown","4edbe567":"markdown","fd7afc85":"markdown"},"source":{"0e2facfa":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport copy\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.metrics import confusion_matrix,accuracy_score\nfrom sklearn.utils import resample\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import MinMaxScaler,OneHotEncoder\nfrom sklearn.model_selection import train_test_split,StratifiedKFold\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","5da7bd33":"data = '\/kaggle\/input\/weather-dataset-rattle-package\/weatherAUS.csv'\ndf = pd.read_csv(data)","02bbfb52":"display(df.shape)\ndisplay(df.head())\ndf.info()\ndf.describe()","d5b4dd05":"df_column_names=list(df.columns)\nprint(df_column_names)\nprint()\nprint('{} columns'.format(len(df_column_names)))","6be0a949":"print('Data types of this dataset :')\nprint(list(df.dtypes.unique()))","d110b864":"categorical_type_columns=[]\nnumerical_type_columns=[]\nfor one_column_name in df:\n    if 'object' in str(df[one_column_name].dtype):\n        categorical_type_columns.append(one_column_name)\n    elif 'float' in str(df[one_column_name].dtype):\n        numerical_type_columns.append(one_column_name)\n\nprint(categorical_type_columns)\nprint()\nprint(numerical_type_columns)\nprint()\nprint('Categorical type columns : {} \/ {}'.format(len(categorical_type_columns),len(df.columns)))\nprint('Numerical type columns : {} \/ {}'.format(len(numerical_type_columns),len(df.columns)))","a31fe06f":"# Check mean and variance pattern of numerical type data \n# on artificial normal distribution generated by mean and variance\n\nfig=plt.figure(figsize=(10,10))\ngs=fig.add_gridspec(4,4)\nax=[None for _ in range(16)]\n\nint_temp=0\nfor i in range(4):\n    for j in range(4):\n        ax[int_temp]=fig.add_subplot(gs[i,j]) \n        mean,std=df.describe().iloc[1,int_temp],df.describe().iloc[2,int_temp]\n        s=np.random.normal(mean,std,1000)\n        count,bins=np.histogram(s,30,density=True)\n        ax[int_temp].plot(bins, 1\/(std * np.sqrt(2 * np.pi)) * np.exp( - (bins - mean)**2 \/ (2 * std**2) ),linewidth=2, color='r')\n        ax[int_temp].set_title('{}'.format(df_column_names[int_temp]))\n        int_temp+=1\n\nplt.tight_layout()\nplt.show()","4e80b1b5":"number_of_rows=df.shape[0]\nnumber_of_nan_in_column=df.isnull().sum(axis=0)\nprint(pd.concat([number_of_nan_in_column,(number_of_nan_in_column\/number_of_rows*100).round(1)],axis=1).rename(columns={0:'Number of NaN',1:'Number of NaN in %'}))","4ee55b05":"df=df.dropna(subset=['RainTomorrow'])","0496988d":"df['RainTomorrow'].value_counts().plot(kind='bar')","a945d740":"print(\"Categorical column cardinality :\")\nfor var in categorical_type_columns:\n    print('{} : {} labels'.format(var,len(df[var].unique())))","137b48b9":"df['Date']=df['Date'].apply(pd.to_datetime)","fec95ed3":"target_attributes=[\"year\",\"month\",\"day\"]\nfor one_target_attribute in target_attributes:\n    new_column_name=one_target_attribute[0].upper()+one_target_attribute[1:]\n    df[new_column_name]=getattr(df['Date'].dt,one_target_attribute)","0b2eba86":"df=df.drop(columns=['Date'])","f569f1de":"plt.subplots(figsize=(20, 15))\nax = sns.heatmap(df.corr(), square=True, annot=True, fmt='.2f', linecolor='white')\nax.set_xticklabels(ax.get_xticklabels(), rotation=90)\nax.set_yticklabels(ax.get_yticklabels(), rotation=30)           \nplt.show()","c7f87474":"df.corr()>0.7\ncorrelation_df_columns=df.corr().columns\nfor one_column_name in correlation_df_columns:\n    one_column_data=df.corr()[one_column_name]\n    high_positively_correlated=one_column_data[one_column_data>0.7]\n    del high_positively_correlated[one_column_name]\n    if high_positively_correlated.shape[0]==0:\n        continue\n    print(one_column_name)\n    print(high_positively_correlated)\n    print()\n","074c0905":"X=df.drop(columns=['RainTomorrow'])\ny=df['RainTomorrow']","9509e3b4":"numerical_type_columns_temp=copy.deepcopy(numerical_type_columns);numerical_type_columns_temp.append(\"Year\");numerical_type_columns_temp.append(\"Month\");numerical_type_columns_temp.append(\"Day\")\ncategorical_type_columns_temp=copy.deepcopy(categorical_type_columns);categorical_type_columns_temp.remove(\"Date\");categorical_type_columns_temp.remove(\"Location\");categorical_type_columns_temp.remove(\"RainTomorrow\")\n\nstratified_k_fold_object=StratifiedKFold(n_splits=5) \nfor fold_index,(train_indices,validation_indices) in enumerate(stratified_k_fold_object.split(X,y)):\n    print(\"=========={} fold==========\".format(fold_index+1))\n\n#     ================================================================================\n    X_train=X.iloc[train_indices]\n    y_train=y.iloc[train_indices]\n    X_validation=X.iloc[validation_indices]\n    y_validation=y.iloc[validation_indices]\n    \n    X_columns=X_train.columns\n    \n#     ================================================================================\n#     8.1. Resample (oversampling) on train dataset\n\n    merged_train=pd.concat([X_train,y_train],axis=1)\n\n    no_df = merged_train[merged_train.RainTomorrow == \"No\"]\n    yes_df = merged_train[merged_train.RainTomorrow == \"Yes\"]\n\n    # Resample (oversampling)\n    yes_oversampled_df = resample(yes_df, replace=True, n_samples=len(no_df), random_state=123)\n\n    # Concat oversampled one and existing one\n    oversampled_train_df = pd.concat([no_df, yes_oversampled_df])\n    \n    y_train=oversampled_train_df['RainTomorrow']\n    X_train=oversampled_train_df.drop(columns=['RainTomorrow'])\n    \n#     ================================================================================    \n#     8.2. Manage null values\n\n    median_data=[]\n    for one_numerical_column_name in numerical_type_columns_temp:\n        median_value=X_train[one_numerical_column_name].median()\n        \n#         Fill median value of train data on train data\n        X_train[one_numerical_column_name].fillna(median_value,inplace=True)\n\n#         Fill median value of train data on validation data\n        X_validation[one_numerical_column_name].fillna(median_value,inplace=True)\n\n#     ================================================================================\n#     8.3. Impute missing categorical variables with most frequent value\n    \n    for one_column in categorical_type_columns_temp:\n        X_train[one_column].fillna(X_train[one_column].mode()[0], inplace=True)\n        X_validation[one_column].fillna(X_validation[one_column].mode()[0], inplace=True)\n\n#     ================================================================================\n#     8.4. One hot encoding\n\n#     Create model\n    one_hot_encoder_object=OneHotEncoder()\n\n#     Fit\n    one_hot_encoder_object.fit(X_train[categorical_type_columns_temp])\n\n#     Transform\n    X_train_one_hot_part=one_hot_encoder_object.transform(X_train[categorical_type_columns_temp]).toarray()\n    X_validation_one_hot_part=one_hot_encoder_object.transform(X_validation[categorical_type_columns_temp]).toarray()\n\n#     ================================================================================\n#     8.5. Scaling (min max scaling)\n\n#     Create model\n    minmax_scaler=MinMaxScaler()\n    \n    # Fit\n    minmax_scaler.fit(X_train[numerical_type_columns])\n\n    # Transform\n    X_train_numerical_scaled=minmax_scaler.transform(X_train[numerical_type_columns])\n    X_validation_numerical_scaled=minmax_scaler.transform(X_validation[numerical_type_columns])\n    \n#     Concatenate\n    X_train_np=np.concatenate((X_train_one_hot_part, X_train_numerical_scaled), axis=1)\n    X_validation_np=np.concatenate((X_validation_one_hot_part, X_validation_numerical_scaled), axis=1)\n    \n#     ================================================================================\n#     8.6. Train\n\n#     Create model\n    logreg = LogisticRegression(solver='liblinear', random_state=0)\n\n    # Fit\n    logreg.fit(X_train_np, y_train)\n\n#     ================================================================================\n#     8.7. Measure performance of prediction on validation dataset\n\n    y_pred_validation = logreg.predict(X_validation_np)\n    print('Model accuracy score: {0:0.4f} with validation dataset'. format(accuracy_score(y_validation, y_pred_validation)))\n    \n    y_pred_train=logreg.predict(X_train_np)\n    print('Model accuracy score: {0:0.4f} with train dataset'. format(accuracy_score(y_train, y_pred_train)))\n\n    print('tn fp\\nfn tp\\n',confusion_matrix(y_validation,y_pred_validation))\n    tn=confusion_matrix(y_validation,y_pred_validation)[0,0]\n    fp=confusion_matrix(y_validation,y_pred_validation)[0,1]\n    fn=confusion_matrix(y_validation,y_pred_validation)[1,0]\n    tp=confusion_matrix(y_validation,y_pred_validation)[1,1]\n    print('accuracy : ',(tp+tn)\/(tp+fp+tn+fn))\n    print('recall (sensitivity,true positive rate) : ',tp\/(tp+fn))\n    print('specificity : ',tn\/(tn+fp))\n    print('precision : ',tp\/(tp+fp))\n    print('f1-score : ',((tp\/(tp+fp))*(tp\/(tp+fn)))\/((tp\/(tp+fp))+(tp\/(tp+fn))))\n    \n    print()","ddbf0b43":"# ================================================================================\n# Resample train dataset (oversampling)\n\nmerged_train=pd.concat([X,y],axis=1)\n\nno_df = merged_train[merged_train.RainTomorrow == \"No\"]\nyes_df = merged_train[merged_train.RainTomorrow == \"Yes\"]\n\n# Resample (oversampling)\nyes_oversampled_df = resample(yes_df, replace=True, n_samples=len(no_df), random_state=123)\n\n# Concat oversampled one and existing one\noversampled_train_df = pd.concat([no_df, yes_oversampled_df])\n\ny_train=oversampled_train_df['RainTomorrow']\nX_train=oversampled_train_df.drop(columns=['RainTomorrow'])\n    \n# ================================================================================    \n# Manage null values\n\nmedian_data=[]\nfor one_numerical_column_name in numerical_type_columns_temp:\n    median_value=X_train[one_numerical_column_name].median()\n\n#     Fill median value of train data on train data\n    X_train[one_numerical_column_name].fillna(median_value,inplace=True)\n\n# ================================================================================\n# Impute missing categorical variables with most frequent value\n    \nfor one_column in categorical_type_columns_temp:\n    X_train[one_column].fillna(X_train[one_column].mode()[0], inplace=True)\n\n# ================================================================================\n# Create model\none_hot_encoder_object=OneHotEncoder()\n\n# Fit\none_hot_encoder_object.fit(X_train[categorical_type_columns_temp])\n\n# Transform\nX_train_one_hot_part=one_hot_encoder_object.transform(X_train[categorical_type_columns_temp]).toarray()\n\n# ================================================================================\n# Create model\nminmax_scaler=MinMaxScaler()\n    \n# Fit\nminmax_scaler.fit(X_train[numerical_type_columns])\n\n# Transform\nX_train_numerical_scaled=minmax_scaler.transform(X_train[numerical_type_columns])\n    \n# Concatenate\nX_train_np=np.concatenate((X_train_one_hot_part, X_train_numerical_scaled), axis=1)\n    \n# ================================================================================\n# Create model\nlogreg = LogisticRegression(solver='liblinear', random_state=0)\n\n# Fit\nlogreg.fit(X_train_np, y_train)\n\n# ================================================================================\n# Measure performance of prediction on validation dataset\n\ny_pred_train=logreg.predict(X_train_np)\nprint('Model accuracy score: {0:0.4f} with train dataset'. format(accuracy_score(y_train, y_pred_train)))\n\nprint('tn fp\\nfn tp\\n',confusion_matrix(y_train, y_pred_train))\ntn=confusion_matrix(y_train, y_pred_train)[0,0]\nfp=confusion_matrix(y_train, y_pred_train)[0,1]\nfn=confusion_matrix(y_train, y_pred_train)[1,0]\ntp=confusion_matrix(y_train, y_pred_train)[1,1]\nprint('accuracy : ',(tp+tn)\/(tp+fp+tn+fn))\nprint('recall (sensitivity,true positive rate) : ',tp\/(tp+fn))\nprint('specificity : ',tn\/(tn+fp))\nprint('precision : ',tp\/(tp+fp))\nprint('f1-score : ',((tp\/(tp+fp))*(tp\/(tp+fn)))\/((tp\/(tp+fp))+(tp\/(tp+fn))))","d88d8a56":"## 3. Check the cardinality of categorical columns <a class=\"anchor\" id=\"3\"><\/a>","23113f9b":"## 7. Compose dataset for machine learning model <a class=\"anchor\" id=\"7\"><\/a>","dbac4dab":"## 9. Train model with full train dataset <a class=\"anchor\" id=\"9\"><\/a>","736613c9":"## 4. Process date data <a class=\"anchor\" id=\"4\"><\/a>","8d13f1fe":"## Table of Contents\n\n1. [Overview of loaded data](#1)  \n    1.1. shape, head, info, describe  \n    1.2. column composition  \n    1.3. Data types of dataset  \n    1.4. Categorical column, numerical column  \n    1.5. Mean and variance pattern of numerical type data on normal distribution  \n    1.6. Check NaNs\n1. [Check the class imbalance](#2)\n1. [Check the cardinality of categorical columns](#3)\n1. [Process date data](#4)\n1. [Check and manage outliers from numerical data](#5)\n1. [Check correlation of features](#6)\n1. [Compose dataset for machine learning model](#7)\n1. [Make 5 folds stratified cross validation loop](#8)  \n    8.1. Resample (oversampling) on train dataset  \n    8.2. Manage null values  \n    8.3. Impute missing categorical variables with most frequent value  \n    8.4. One hot encoding  \n    8.5. Scaling (min max scaling)  \n    8.6. Train logistic classification model  \n    8.7. Measure performance of prediction on validation dataset  \n1. [Train validated model with full train dataset](#9)","a91a73eb":"## 1. Overview of loaded data <a class=\"anchor\" id=\"1\"><\/a>","8fa35308":"## 6. Check correlation of features <a class=\"anchor\" id=\"6\"><\/a>","4edbe567":"## 2. Check the class imbalance <a class=\"anchor\" id=\"2\"><\/a>","fd7afc85":"## 8. Make 5 folds stratified cross validation loop <a class=\"anchor\" id=\"8\"><\/a>"}}