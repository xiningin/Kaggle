{"cell_type":{"4e35200a":"code","9e88e71b":"code","b0336b5e":"code","af8d0b12":"code","66a2d4dc":"code","85a0b856":"code","d4c50993":"code","f7308a96":"code","70624999":"code","1c5e026b":"code","623f88cb":"code","b2badc61":"code","d62106cd":"code","80ff2e82":"code","73ca5efc":"code","901616c1":"code","b5b902ec":"code","9da34c62":"code","e12bf792":"code","80293e26":"code","1f05e1e9":"code","befb7953":"code","8269358a":"code","0193ae03":"code","bd2a5727":"code","fe1e3b7c":"code","22c69f18":"code","133a707b":"code","e5f466c0":"code","4380d05e":"code","6e632072":"code","f557a8f2":"code","ebbf0cdd":"code","f50315df":"code","32210e80":"code","ef7a4acd":"code","7bc4eb61":"code","8e754310":"code","d5113658":"code","263fe940":"code","141b08c7":"code","2c846a27":"code","0ae4bbcd":"code","41099f52":"code","daae0dce":"code","e36c7077":"code","76e7e834":"code","99ff6dba":"code","8926ece5":"code","f22e2bba":"code","a35b2e4b":"code","b4e93efa":"code","34de4b0b":"code","90845f9f":"code","a55237bc":"code","c71c0d9f":"markdown","c7f7f209":"markdown","088b101c":"markdown","460fab7e":"markdown","7692a6af":"markdown","f5c64d4c":"markdown","d4931b91":"markdown","6a8cd98c":"markdown","6cde2497":"markdown","00e21e6a":"markdown","94a38238":"markdown","e4945e26":"markdown","e222122c":"markdown","8fdc6c3e":"markdown","bd033281":"markdown","c2921ab5":"markdown","9e0e7fba":"markdown","b0e31fee":"markdown","ee95c403":"markdown","66b2fce6":"markdown","c6f6a898":"markdown","7f680a96":"markdown","f7adb73f":"markdown","9fe355c8":"markdown","9689bc3d":"markdown","2d452994":"markdown","e0766d61":"markdown","63fdd816":"markdown","e4d1b9ec":"markdown","16acd2a1":"markdown","0d792a7c":"markdown","f66c0d90":"markdown","69bf8a2e":"markdown","c5eb5392":"markdown","51ac12fb":"markdown","b3e27c48":"markdown","b86ab348":"markdown","84215ffb":"markdown","5cfac92c":"markdown","f0d2c74c":"markdown","67111716":"markdown","684fbd1f":"markdown","7305ef17":"markdown","2b6ad99f":"markdown"},"source":{"4e35200a":"pip install transformers","9e88e71b":"import re\nimport numpy as np\nimport pandas as pd\nimport os, json\nimport torch\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nimport seaborn as sns\nimport matplotlib.pyplot as plt","b0336b5e":"import torch\n\nif torch.cuda.is_available():    \n\n    device = torch.device(\"cuda\")\n\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","af8d0b12":"df= pd.read_csv('\/kaggle\/input\/complete-tweet-sentiment-extraction-data\/tweet_dataset.csv')\ndf.head()","66a2d4dc":"train= pd.DataFrame()\ntrain['text']= df['text']\ntrain['Sentiment']=df['new_sentiment']","85a0b856":"train=train.dropna()","d4c50993":"train.isnull().sum()","f7308a96":"train['Sentiment'] = train['Sentiment'].replace(['negative','neutral','positive'],[0,1,2])","70624999":"from transformers import BertTokenizer\n\nprint('Loading BERT tokenizer...')\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)","1c5e026b":"print(tokenizer.tokenize(\"Hello. This is a sample statement for visualisation purposes\"))","623f88cb":"print(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"Hello. This is a sample statement for visualisation purposes\")))","b2badc61":"max_len = 0\n\n\nfor tweet in train['text']:\n    \n    # The following code is used to clean the tweets by removing URLs, accounts, quotation marks,etc\n    url_pattern = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    tweet = url_pattern.sub(r'', tweet)\n    tweet = re.sub('\\S*@\\S*\\s?', '', tweet)\n    tweet = re.sub('\\s+', ' ', tweet)\n    tweet = re.sub(\"\\'\", \"\", tweet)\n\n    input_ids = tokenizer.encode(tweet, add_special_tokens=True)\n\n    max_len = max(max_len, len(input_ids))        #maximum length of input ids\n\nprint('Max sequence length: ', max_len)","d62106cd":"\ninput_ids = []\nattention_masks = []\n\nfor sent in train['text']:\n    \n    encoded_dict = tokenizer.encode_plus(\n                        sent,                      \n                        add_special_tokens = True, \n                        max_length = 110,           \n                        pad_to_max_length = True,\n                        return_attention_mask = True,   \n                        return_tensors = 'pt',     \n                   )\n     \n    input_ids.append(encoded_dict['input_ids'])\n    attention_masks.append(encoded_dict['attention_mask'])\n\n\ninput_ids = torch.cat(input_ids, dim=0)\nattention_masks = torch.cat(attention_masks, dim=0)\nlabels = torch.tensor(train['Sentiment'].tolist())","80ff2e82":"from torch.utils.data import TensorDataset, random_split\n\ndataset = TensorDataset(input_ids, attention_masks, labels)  #storing the input ids,masks and labels in dataset\n\ntrain_size = int(0.9 * len(dataset))\nval_size = len(dataset) - train_size\n\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])  #90-10 train-val split","73ca5efc":"from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n\n\nbatch_size = 32\n\ntrain_dataloader = DataLoader(\n            train_dataset,  \n            sampler = RandomSampler(train_dataset),     #random sampling in training\n            batch_size = batch_size \n        )\n\nvalidation_dataloader = DataLoader(\n            val_dataset, \n            sampler = SequentialSampler(val_dataset),    #sequential sampling in validation\n            batch_size = batch_size \n        )","901616c1":"from transformers import BertForSequenceClassification, AdamW, BertConfig\n\n\nmodel = BertForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\", \n    num_labels = 3,   \n    output_attentions = False,\n    output_hidden_states = False,\n)","b5b902ec":"optimizer = AdamW(model.parameters(),\n                  lr = 2e-5, \n                  eps = 1e-8 \n                )","9da34c62":"from transformers import get_linear_schedule_with_warmup\n\nepochs = 4\n\n\ntotal_steps = len(train_dataloader) * epochs\n\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps = 0, \n                                            num_training_steps = total_steps)","e12bf792":"import numpy as np\n\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) \/ len(labels_flat)","80293e26":"import time\nimport datetime\n\ndef format_time(elapsed):\n    '''\n    Takes a time in seconds and returns a string hh:mm:ss\n    '''\n    elapsed_rounded = int(round((elapsed)))\n    \n    return str(datetime.timedelta(seconds=elapsed_rounded))","1f05e1e9":"import random\nimport numpy as np\n\n\nseed_val = 42\nepochs=1\n\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\n\nmodel.to(device)\n\n\ntraining_stats = []\n\ntotal_t0 = time.time()\n\nfor epoch_i in range(0, epochs):\n    \n    print(\"\")\n    print('======== Epoch {:} \/ {:} ========'.format(epoch_i + 1, epochs))\n    print('Training...')\n\n    t0 = time.time()\n\n    total_train_loss = 0\n\n    model.train()\n\n    for step, batch in enumerate(train_dataloader):\n        \n        # Printing the progress after every 40 epochs\n        if step % 40 == 0 and not step == 0:\n            \n            elapsed = format_time(time.time() - t0)\n            \n            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n\n        model.zero_grad()        \n\n        result = model(b_input_ids, \n                       token_type_ids=None, \n                       attention_mask=b_input_mask, \n                       labels=b_labels,\n                       return_dict=True)\n\n        loss = result.loss\n        logits = result.logits\n\n        total_train_loss += loss.item()\n\n        loss.backward()\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        optimizer.step()\n        scheduler.step()\n\n    avg_train_loss = total_train_loss \/ len(train_dataloader)            \n\n    training_time = format_time(time.time() - t0)\n\n    print(\"\")\n    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n    print(\"  Training epcoh took: {:}\".format(training_time))\n        \n    # ========================================\n    #               Validation\n    # ========================================\n\n    print(\"\")\n    print(\"Running Validation...\")\n\n    t0 = time.time()\n\n    model.eval()\n\n    total_eval_accuracy = 0\n    total_eval_loss = 0\n    nb_eval_steps = 0\n\n    for batch in validation_dataloader:\n\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n\n        with torch.no_grad():        \n\n            result = model(b_input_ids, \n                           token_type_ids=None, \n                           attention_mask=b_input_mask,\n                           labels=b_labels,\n                           return_dict=True)\n\n        loss = result.loss\n        logits = result.logits\n            \n        total_eval_loss += loss.item()\n\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n\n        total_eval_accuracy += flat_accuracy(logits, label_ids)\n        \n    avg_val_accuracy = total_eval_accuracy \/ len(validation_dataloader)\n    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n\n    avg_val_loss = total_eval_loss \/ len(validation_dataloader)\n\n    validation_time = format_time(time.time() - t0)\n    \n    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n    print(\"  Validation took: {:}\".format(validation_time))\n\n    training_stats.append(\n        {\n            'epoch': epoch_i + 1,\n            'Training Loss': avg_train_loss,\n            'Valid. Loss': avg_val_loss,\n            'Valid. Accur.': avg_val_accuracy,\n            'Training Time': training_time,\n            'Validation Time': validation_time\n        }\n    )\n\nprint(\"\")\nprint(\"Training complete!\")\n\nprint(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))","befb7953":"df_vax = pd.read_csv('..\/input\/all-covid19-vaccines-tweets\/vaccination_all_tweets.csv')\ndf_vax","8269358a":"vax_tweets= df_vax['text']","0193ae03":"\ninput_ids = []\nattention_masks = []\n\nfor sent in vax_tweets:\n    \n    url_pattern = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    tweet = url_pattern.sub(r'', tweet)\n    tweet = re.sub('\\S*@\\S*\\s?', '', tweet)\n    tweet = re.sub('\\s+', ' ', tweet)\n    tweet = re.sub(\"\\'\", \"\", tweet)\n    \n    encoded_dict = tokenizer.encode_plus(\n                        sent,                      \n                        add_special_tokens = True, \n                        max_length = 110,           \n                        pad_to_max_length = True,\n                        return_attention_mask = True,   \n                        return_tensors = 'pt',     \n                   )\n     \n    input_ids.append(encoded_dict['input_ids'])\n    attention_masks.append(encoded_dict['attention_mask'])\n\n\ninput_ids = torch.cat(input_ids, dim=0)\nattention_masks = torch.cat(attention_masks, dim=0)","bd2a5727":"vax_dataset= TensorDataset(input_ids, attention_masks)\n\nbatch_size = 32\n\nvax_dataloader = DataLoader(\n            vax_dataset,  \n            sampler = SequentialSampler(vax_tweets),     #Sequential Sampling\n            batch_size = batch_size \n        )","fe1e3b7c":"print('Predicting labels for {:,} vaccine tweets...'.format(len(input_ids)))\n\nmodel.eval()\n\npredictions = []\n\nfor batch in vax_dataloader:\n \n  batch = tuple(t.to(device) for t in batch)\n  \n  b_input_ids, b_input_mask = batch\n\n  with torch.no_grad():\n\n      result = model(b_input_ids, \n                     token_type_ids=None, \n                     attention_mask=b_input_mask,\n                     return_dict=True)\n\n  logits = result.logits\n\n  logits = logits.detach().cpu().numpy()\n  \n  logits=np.argmax(logits,axis=1)\n  for i in range(len(logits)):\n    predictions.append(logits[i])\n    \n\nprint('    DONE.')","22c69f18":"df_vax['Sentiment'] = predictions\ndf_vax=df_vax.dropna()      #drops the null values","133a707b":"df_vax.head()","e5f466c0":"vax= df_vax.drop(['user_name','user_description','user_created','user_followers','user_friends','user_favourites','source','is_retweet'],axis=1)","4380d05e":"vax.head()","6e632072":"vax['Sentiment']=vax['Sentiment'].map({0:'negative',1:'neutral',2:'positive'})","f557a8f2":"vax['Sentiment'].value_counts(normalize=True).plot.bar()","ebbf0cdd":"vax['date'] = pd.to_datetime(vax['date'], errors='coerce').dt.date\nvax['Sentiment'] = vax['Sentiment'].map({'negative':-1,'neutral':0,'positive':1})","f50315df":"vax.head()","32210e80":"all_vax = ['covaxin', 'sinopharm', 'sinovac', 'moderna', 'pfizer', 'biontech', 'oxford', 'astrazeneca', 'sputnik']\n\nvax_sentiment = pd.DataFrame()\nvax_sentiment['Vaccine']=all_vax\nsentiments=list()","ef7a4acd":"def time_variance(vacc) :\n    if vacc=='all':\n        vacc=vax\n    else :   \n        vacc=vax[vax['text'].str.lower().str.contains(vacc)]\n        sentiments.append(vacc.Sentiment.mean())\n\n    temp=pd.DataFrame()\n    temp['date'] = sorted(vacc['date'].unique())\n    senti=list()\n\n    for date in temp['date']:\n        senti.append(vacc[vacc['date']==date].Sentiment.mean())\n\n    temp['Sentiment']=senti\n    \n    fg,axs=plt.subplots(figsize=(15,7))\n    sns.lineplot(ax=axs,x='date',y='Sentiment', data= temp)\n    axs.set_xlabel(\"Time\",size=16)\n    axs.set_ylabel(\"Sentiment\",size=16)\n    axs.set_title(\"Variance of Sentiment wrt Time\",size=24)\n    axs.grid()","7bc4eb61":"time_variance('all')","8e754310":"time_variance('covaxin')","d5113658":"time_variance('sinopharm')","263fe940":"time_variance('sinovac')","141b08c7":"time_variance('moderna')","2c846a27":"time_variance('pfizer')","0ae4bbcd":"time_variance('biontech')","41099f52":"time_variance('oxford')","daae0dce":"time_variance('astrazeneca')","e36c7077":"time_variance('sputnik')","76e7e834":"vax_sentiment['Sentiment']=sentiments\n\nfg,axs=plt.subplots(figsize=(15,7))\nsns.barplot(ax=axs,x='Vaccine',y='Sentiment',data=vax_sentiment)\naxs.set_xlabel(\"Vaccines\",size=16)\naxs.set_ylabel(\"Mean Sentiment\",size=16)\naxs.set_title(\"Mean Sentiment of Vaccines\",size=24)","99ff6dba":"country_sentiment=pd.DataFrame()\ncountries=['india','usa','canada','germany','spain','pakistan','uk','brazil','russia','italy','australia','france','argentina','uae','israel','mexico','japan']\ncountry_sentiment['countries']=countries\nsenti=list()\n\nfor country in countries :\n    senti.append(vax[vax['user_location'].str.lower().str.contains(country)].Sentiment.mean())\n    \ncountry_sentiment['Sentiment']=senti","8926ece5":"fg,axs=plt.subplots(figsize=(15,10))\nsns.barplot(ax=axs,x='countries',y='Sentiment',data=country_sentiment)\naxs.set_xlabel(\"Countries\",size=16)\naxs.set_ylabel(\"Mean Sentiment\",size=16)\naxs.set_title(\"Mean Sentiment of Countries\",size=24)\naxs.grid()","f22e2bba":"def time_variance_country(country) :\n    count=vax[vax['user_location'].str.lower().str.contains(country)]\n\n    temp=pd.DataFrame()\n    temp['date'] = sorted(count['date'].unique())\n    senti=list()\n\n    for date in temp['date']:\n        senti.append(count[count['date']==date].Sentiment.mean())\n\n    temp['Sentiment']=senti\n    \n    fg,axs=plt.subplots(figsize=(15,7))\n    sns.lineplot(ax=axs,x='date',y='Sentiment', data= temp)\n    axs.set_xlabel(\"Time\",size=16)\n    axs.set_ylabel(\"Sentiment\",size=16)\n    axs.set_title(\"Variance of Sentiment wrt Time\",size=24)\n    axs.grid()","a35b2e4b":"time_variance_country('india')","b4e93efa":"time_variance_country('canada')","34de4b0b":"time_variance_country('uae')","90845f9f":"time_variance_country('usa')","a55237bc":"time_variance_country('spain')","c71c0d9f":"The Covid19 pandemic has forced the entire world to be practically shut down for the larger part of the last year. The new year 2021 had brought with itself a new hope with the discoveries of various vaccines for the virus. A global vaccination drive is underway and the moment and we hope that the world will soon recover from this pandemic and normal lifestyles will be restored.\n\nThis notebook focuses on the tweets related to the vaccination drive. We use the BERT model and fine tune it on a twitter dataset containing 40K tweets. This is used to predict the sentiment of vaccination related tweets. We analyze the variations in the sentiment with respect to time, vaccine, and country.","c7f7f209":"**India**","088b101c":"**Astrazenca**","460fab7e":"# Comparison among countries","7692a6af":"We take only the coloumns which are useful for us","f5c64d4c":"**USA**","d4931b91":"We select the useful coloumns from the dataset and make a new dataframe for training purpose.","6a8cd98c":"**Canada**(lowest mean sentiment)","6cde2497":"**Optimizer and Scheduler**","00e21e6a":"**Moderna**","94a38238":"**Datasets and Dataloaders**","e4945e26":"As a common trend in all vaccines, we can see that in the initial stages the sentiment seems to be on an extreme end. The positive extreme can be attributed to the celebration regarding the discovery of a vaccine whereas the negative extreme seems to be a result of popular rumours regarding the credibility of the vaccines. All the vaccines which had started with a negative sentiment have also achieved the positive sentiment extreme when the rumours regarding them were cancelled with concrete proof from the authorities. Gradually with the progress of the vaccination drive the sentiment tends to be more neutral","e222122c":"**Pre Processing the tweets**","8fdc6c3e":"<center><div><img src='attachment:3b1d55b8-095d-4d8c-89ab-cb5172052a1e.jpg' width=500><\/div><\/center>","bd033281":"**UAE**(highest mean sentiment)","c2921ab5":"**Sinopharm**","9e0e7fba":"**Importing the BERT Tokenizer**","b0e31fee":"# Training the model","ee95c403":"Most of the countries seem to have a positive sentiment towards the vaccination drive. However, Canada has a significantly negative sentiment and might be a cause for concern. Lets examine the variance of sentiment with respect to time in some of these countries.","66b2fce6":"**Pfizer**","c6f6a898":"**Pre Processing**","7f680a96":"# Sentiment Prediction","f7adb73f":"# Comparison amongst Vaccines","9fe355c8":"# Conclusion","9689bc3d":"**Prediction**","2d452994":"**Spain**","e0766d61":"# Varinace with Time in Different Countries","63fdd816":"Biontech vaccine has recieved the best response among all these vaccines whereas astrazeneca has the lowest mean sentiment. It is a good thing to note that the mean sentiments for all the vaccines is positive.","e4d1b9ec":"The variance of sentiment in the countries seems to be much more erratic than the one we observed for the case of different vaccines. However, the variation should be easily explained by the internal matters and\/or politics of the respective countries. For example the sentiment in India seems to have been negative in the initial phases when the vaccines were available only for the frontline workers. However, the sentiment has improved after the vaccines were made publically available with the government ministers taking active participation as well for encouragement.\n\nIt falls on the governments all over the world to ensure that the general public has access to the vaccines. Also politicians and other influencers should stand united against this pandemic and refrain from dragging politics and rumors into this very sensitive and important aspect of human life.","16acd2a1":"**Sputnik**","0d792a7c":"# Variance with Time","f66c0d90":"**Training Loop**","69bf8a2e":"**Biontech**","c5eb5392":"**Sinovac**","51ac12fb":"**Oxford**","b3e27c48":"P.S: The dataset has tweets upto early May 2021 and this notebook does not represent any change in sentiment post this period.","b86ab348":"Mean overall sentiment","84215ffb":"**Covaxin**","5cfac92c":"# Sentiment Analysis of COVID 19 vaccine tweets","f0d2c74c":"We use a general tweet sentiment analysis dataset in order to fine tune the BERT model. This dataset can be downloaded from this [link](https:\/\/www.kaggle.com\/maxjon\/complete-tweet-sentiment-extraction-data)","67111716":"The covid 19 vaccination tweets dataset imported below can be downloaded from this [link](https:\/\/www.kaggle.com\/gpreda\/all-covid19-vaccines-tweets)","684fbd1f":"This notebook analyzes the sentiment of tweets related to the covid19 vaccination drive. We see the affect of widespread rumours that led to a highly negative sentiment in the beginning of the vaccination drive. However, with concrete proof showing the credibility of the vaccines, the vaccination drive is running in full swing all over the world.We also observe that the internal matters of a country also significantly affect the sentiment of the tweets. We hope that this vaccination drive ends in a success and we can see a sentiment spike in the coming months.","7305ef17":"# THANK YOU\n\nPlease comment your views and feedback.","2b6ad99f":"**SETUP**"}}