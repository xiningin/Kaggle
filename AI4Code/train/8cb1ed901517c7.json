{"cell_type":{"8635b097":"code","cbee2716":"code","a28dcdb4":"code","5cc8ec06":"code","0637cbc2":"code","277e0559":"code","67cde117":"code","11e809ca":"code","77e3b463":"code","38238015":"code","20c2230a":"code","e99b6381":"code","a2d5aa5f":"code","a17cb865":"code","660dcb27":"code","57df6b0c":"markdown","3a0574cb":"markdown","ad2d2b89":"markdown","685fdab7":"markdown","70e00fd5":"markdown","bfa50f44":"markdown","63e5b903":"markdown","1a094835":"markdown","8d3619e3":"markdown","2f100f97":"markdown","7f95811f":"markdown","6150b659":"markdown","bb3f996e":"markdown","9d0cd4ed":"markdown"},"source":{"8635b097":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.layers import Dense, Activation,Dropout,Conv2D, MaxPooling2D,BatchNormalization, Flatten\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\nfrom tensorflow.keras.applications import imagenet_utils\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.models import Model, load_model, Sequential\nfrom tensorflow.keras.utils import to_categorical\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\nimport os\nfrom IPython.core.display import display, HTML\ndisplay(HTML(\"<style>.container { width:100% !important; }<\/style>\"))\n","cbee2716":"# set ansi color values\nCblu ='\\33[34m'\nCend='\\33[0m'   # sets color back to default \nCred='\\033[91m'\nCblk='\\33[39m'\nCgreen='\\33[32m'\nCyellow='\\33[33m'","a28dcdb4":"def get_paths(source_dir,output_dir,mode,subject):\n    # NOTE if running on kaggle these directories will need to be amended\n    # if all files are in a single kaggle input directory change the string 'consolidated'\n    # to match the directory name used in the database.\n    #if files are seperated in training, test and validation directories change the strings\n    # 'train', 'test' and 'valid' to match the directory names used in the database\n    if mode =='ALL':\n        # all data is in a single directory must be split into train, test, valid data sets\n        train_path=os.path.join(source_dir, 'train')    \n        classes=os.listdir(train_path) \n        class_num=len(classes)\n        test_path=None        \n        valid_path=None\n       \n    else:\n        # data is seperated in 3 directories train, test, valid\n        test_path=os.path.join(source_dir,'test')\n        classes=os.listdir(test_path)\n        class_num=len(classes)  #determine number of class directories in order to set leave value intqdm    \n        train_path=os.path.join(source_dir, 'train')\n        valid_path=os.path.join(source_dir,'validation')\n                  \n    # save the class dictionary as a text file so it can be used by predictor.py in the future\n    #saves file as subject.txt  structure is similar to a python dictionary\n    msg=''\n    for i in range(0, class_num):\n        msg=msg + str(i) + ':' + classes[i] +','\n    id=subject  + '.txt'   \n    dict_path=os.path.join (output_dir, id)\n    f=open(dict_path, 'w')\n    f.write(msg)\n    f.close()    \n    return [train_path, test_path, valid_path,classes]\n      \n    \n   ","5cc8ec06":"def make_model(classes,lr_rate, height,width,model_size, rand_seed):\n    size=len(classes)\n    if model_size=='V1':\n        weights='imagenet'\n        if height==224:\n            Top=True            \n            cut=-2\n        else:\n            Top=False\n            cut=-1            \n        mobile = tf.keras.applications.mobilenet.MobileNet( include_top=Top,\n                                                           input_shape=(height,width,3),\n                                                           pooling='avg', weights=weights,\n                                                           alpha=1, depth_multiplier=1)\n        x=mobile.layers[cut].output\n        x=Dense(128, kernel_regularizer = regularizers.l2(l = 0.015), activation='relu')(x)\n        x=Dropout(rate=.5,name= 'drop2', seed=rand_seed)(x)\n        predictions=Dense (size, activation='softmax')(x)\n        model = Model(inputs=mobile.input, outputs=predictions)       \n        for layer in model.layers:\n            layer.trainable=True\n        model.compile(Adam(lr=lr_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n        \n    elif model_size=='V2':    \n        weights='imagenet'\n        if height==224:\n            Top=True            \n            cut=-2\n        else:\n            Top=False\n            cut=-1 \n             \n        mobile =keras.applications.mobilenet_v2.MobileNetV2(input_shape=(height, width,3),  include_top=Top, weights='imagenet'  )\n        x=mobile.layers[cut].output\n        x=Dense(128, kernel_regularizer = regularizers.l2(l = 0.015), activation='relu')(x)\n        x=Dropout(rate=.5,name= 'drop2', seed=rand_seed)(x)\n        predictions=Dense (size, activation='softmax')(x)\n        model = Model(inputs=mobile.input, outputs=predictions)\n        model.summary()\n        for layer in model.layers:\n            layer.trainable=True\n        #model.compile(Adam(lr=lr_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n        model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=lr_rate),loss='binary_crossentropy', metrics=['accuracy'])\n        \n    return model","0637cbc2":"def make_generators( paths, mode, batch_size, v_split, classes, height, width, rand_seed, model_size):\n    #paths[0]=train path,paths[1]=test path paths[2]= valid path paths[3]=classes\n    v_split=v_split\/100.0\n    file_names=[]\n    labels=[] \n    # determine batch_size for test images determine numberof test files\n    test_batch_size, test_steps=get_batch_size(paths[1], model_size)\n    valid_batch_size, valid_steps=get_batch_size(paths[2], model_size)\n    train_batch_size, train_steps=get_batch_size(paths[0], model_size)\n    trbatch= 80 if model_size=='V1' else 40\n    \n    if mode == 'SEP':\n        train_gen=ImageDataGenerator(preprocessing_function=keras.applications.mobilenet.preprocess_input,\n                horizontal_flip=True,\n                samplewise_center=True,\n                width_shift_range=.2,\n                height_shift_range=.2,\n                validation_split=v_split,\n                samplewise_std_normalization=True).flow_from_directory(paths[0], target_size=(height, width),\n                batch_size=trbatch, seed=rand_seed, class_mode=\"categorical\")\n        \n        valid_gen=ImageDataGenerator(preprocessing_function=keras.applications.mobilenet.preprocess_input,\n                samplewise_center=True,                \n                samplewise_std_normalization=True).flow_from_directory(paths[2], \n                target_size=(height, width), batch_size=valid_batch_size,\n                seed=rand_seed, shuffle=False, class_mode=\"categorical\")\n        \n        test_gen=ImageDataGenerator(preprocessing_function=keras.applications.mobilenet.preprocess_input,\n                samplewise_center=True,                \n                samplewise_std_normalization=True).flow_from_directory(paths[1],\n                target_size=(height, width), batch_size=test_batch_size,\n                seed=rand_seed, shuffle=False )\n        for file in test_gen.filenames:\n            file_names.append(file)            \n        for label in test_gen.labels:\n            labels.append(label)\n        \n        return [train_gen, test_gen, valid_gen, file_names, labels, [train_steps, test_steps, valid_steps]]\n                  \n    else:\n        # all data is in a single directory there are no test images use validation images as test images\n        train_gen=ImageDataGenerator(preprocessing_function=keras.applications.mobilenet.preprocess_input,\n                             horizontal_flip=True,\n                             samplewise_center=True,\n                             validation_split=v_split,\n                             samplewise_std_normalization=True).flow_from_directory(paths[0],\n                                                                                    target_size=(height, width),\n                                                                                    batch_size=batch_size,\n                                                                                    subset='training',seed=rand_seed)\n        valid_gen=ImageDataGenerator(preprocessing_function=keras.applications.mobilenet.preprocess_input,\n                             horizontal_flip=False,\n                             samplewise_center=True,\n                             validation_split=v_split,\n                             samplewise_std_normalization=True).flow_from_directory(paths[0],\n                                                                                    target_size=(height, width),\n                                                                                    batch_size=batch_size,\n                                                                                    subset='validation',\n                                                                                    seed=rand_seed, shuffle=False)\n        \n        \n        for file in val_gen.filenames:\n            file_names.append(file)\n        for label in val_gen.labels:\n            labels.append(label)\n    return [train_gen, test_gen,valid_gen, file_names, labels ]","277e0559":"def train(model, callbacks, train_gen, val_gen,steps_list, epochs,start_epoch):\n    # steps_list[0]=training steps, steps_list[2]=validations steps\n    start=time.time()\n    data = model.fit_generator(generator = train_gen,\n                               validation_data= val_gen, epochs=epochs, initial_epoch=start_epoch,\n                               validation_steps=steps_list[2],callbacks = callbacks, verbose=1)\n    stop=time.time()\n    duration = stop-start\n    hrs=int(duration\/3600)\n    mins=int((duration-hrs*3600)\/60)\n    secs= duration-hrs*3600-mins*60\n    msg='{0}Training took\\n {1} hours {2} minutes and {3:6.2f} seconds {4}'\n    print(msg.format(Cblu,hrs, mins,secs,Cend))\n    return data\n    ","67cde117":"def tr_plot(tacc,vacc,tloss,vloss):\n    #Plot the training and validation data\n    Epoch_count=len(tloss)\n    Epochs=[]\n    for i in range (0,Epoch_count):\n        Epochs.append(i+1)\n    index_loss=np.argmin(vloss)#  this is the epoch with the lowest validation loss\n    val_lowest=vloss[index_loss]\n    index_acc=np.argmax(vacc)\n    val_highest=vacc[index_acc]\n    plt.style.use('fivethirtyeight')\n    sc_label='best epoch= '+ str(index_loss+1)\n    vc_label='best epoch= '+ str(index_acc + 1)\n    fig,axes=plt.subplots(nrows=1, ncols=2, figsize=(15,5))\n    axes[0].plot(Epochs,tloss, 'r', label='Training loss')\n    axes[0].plot(Epochs,vloss,'g',label='Validation loss' )\n    axes[0].scatter(index_loss+1,val_lowest, s=150, c= 'blue', label=sc_label)\n    axes[0].set_title('Training and Validation Loss')\n    axes[0].set_xlabel('Epochs')\n    axes[0].set_ylabel('Loss')\n    axes[0].legend()\n    axes[1].plot (Epochs,tacc,'r',label= 'Training Accuracy')\n    axes[1].plot (Epochs,vacc,'g',label= 'Validation Accuracy')\n    axes[1].scatter(index_acc+1,val_highest, s=150, c= 'blue', label=vc_label)\n    axes[1].set_title('Training and Validation Accuracy')\n    axes[1].set_xlabel('Epochs')\n    axes[1].set_ylabel('Accuracy')\n    axes[1].legend()\n    plt.tight_layout\n    #plt.style.use('fivethirtyeight')\n    plt.show()\n    \n ","11e809ca":"def display_pred(output_dir, pred, file_names, labels, subject, model_size,classes, kaggle):    \n    trials=len(labels)\n    errors=0\n    e_list=[]\n    prob_list=[]\n    true_class=[]\n    pred_class=[]\n    x_list=[]\n    index_list=[]\n    pr_list=[]\n    error_msg=''    \n    for i in range (0,trials):\n        p_class=pred[i].argmax()\n        if p_class !=labels[i]: #if the predicted class is not the same as the test label it is an error\n            errors=errors + 1\n            fname=os.path.basename(file_names[i])\n            e_list.append(fname)  # list of file names that are in error\n            true_class.append(classes[labels[i]]) # list classes that have an eror\n            pred_class.append(classes[p_class]) #class the prediction selected\n            prob_list.append(100 *pred[i][p_class])# probability of the predicted class\n            add_msg='{0:^24s}{1:5s}{2:^20s}\\n'.format(classes[labels[i]], ' ', file_names[i])\n            error_msg=error_msg + add_msg\n            \n    accuracy=100*(trials-errors)\/trials\n    print('{0}\\n There were {1} errors in {2} trials for an accuracy of {3:7.3f}{4}'.format(Cblu,errors, trials,accuracy,Cend))\n    if kaggle==True and errors<26:\n        ans='Y'\n    else:\n        ans='N'\n    if kaggle==False:\n        ans=input('To see a listing of prediction errors enter Y to skip press Enter\\n ')\n    if ans== 'Y' or ans  =='y':\n        msg='{0}{1}{2:^20s}{1:3s}{3:^20s}{1:3s}{4:^20s}{1:5s}{5}{6}'\n        print(msg.format(Cblu, ' ', 'File Name', 'True Class', 'Predicted Class', 'Probability', Cend))\n        for i in range(0,errors):\n            msg='{0}{1:^20s}{0:3s}{2:^20s}{0:3s}{3:^20s}{0:5s}{4:^6.2f}'\n            print (msg.format(' ',e_list[i], true_class[i], pred_class[i], prob_list[i]))\n    if kaggle==True:\n        ans='Y'\n    else:\n        ans=input('\\nDo you want to save the list of error files?. Enter Y to save or press Enter to not save  ')\n    if ans=='Y' or ans=='y':\n        acc='{0:6.2f}'.format(accuracy)\n        if model_size=='L':\n            ms='Large'\n        elif model_size=='M':\n            ms= 'Medium'\n        else:\n            ms= 'Small'\n        header='Classification subject: {0} There were {1} errors in {2} tests for an accuracy of {3} using a {4} model\\n'.format(subject,errors,trials,acc,ms)\n        header= header +'{0:^24s}{1:5s}{2:^20s}\\n'.format('CLASS',' ', 'FILENAME') \n        error_msg=header + error_msg\n        file_id='error list-' + model_size + acc +'.txt'\n        file_path=os.path.join(output_dir,file_id)\n        f=open(file_path, 'w')\n        f.write(error_msg)\n        f.close()\n    for c in classes:\n        count=true_class.count(c)\n        x_list.append(count)\n        pr_list.append(c)\n    for i in range(0, len(x_list)):  # only plot classes that have errors\n        if x_list[i]==0:\n            index_list.append(i)\n    for i in sorted(index_list, reverse=True):  # delete classes with no errors\n        del x_list[i]\n        del pr_list[i]      # use pr_list - can't change class_list must keep it fixed\n    fig=plt.figure()\n    fig.set_figheight(len(pr_list)\/4)\n    fig.set_figwidth(6)\n    plt.style.use('fivethirtyeight')\n    for i in range(0, len(pr_list)):\n        c=pr_list[i]\n        x=x_list[i]\n        plt.barh(c, x, )\n        plt.title( subject +' Classification Errors on Test Set')\n    if errors>0:\n        plt.show()\n    if kaggle==False:\n        ans=input('Press Enter to continue')\n    return accuracy        ","77e3b463":"def save_model(output_dir,subject, accuracy, height, width, model, weights):\n    # save the model with the  subect-accuracy.h5\n    acc=str(accuracy)[0:5]\n    tempstr=subject + '-' +str(height) + '-' + str(width) + '-' + acc + '.h5'\n    model.set_weights(weights)\n    model_save_path=os.path.join(output_dir,tempstr)\n    model.save(model_save_path)    ","38238015":"def make_predictions( model, weights, test_gen, lr):\n    config = model.get_config()\n    pmodel = Model.from_config(config)  # copy of the model\n    pmodel.set_weights(weights) #load saved weights with lowest validation loss\n    pmodel.compile(Adam(lr=lr), loss='categorical_crossentropy', metrics=['accuracy'])    \n    print('Training has completed. Now loading test set to see how accurate the model is')\n    results=pmodel.evaluate(test_gen, verbose=0)\n    print('Model accuracy on Test Set is {0:7.2f} %'.format(results[1]* 100))\n    predictions=pmodel.predict_generator(test_gen, verbose=0)     \n    return predictions","20c2230a":"def wrapup (output_dir,subject, accuracy, height,width, model, weights,run_num, kaggle):\n    if accuracy >= 95:\n        msg='{0} With an accuracy of {1:5.2f} % the results appear satisfactory{2}'\n        print(msg.format(Cgreen, accuracy, Cend))\n        if kaggle:\n            save_model(output_dir, subject, accuracy, height, width , model, weights)\n            print ('*********************Process is completed******************************')            \n            return [False, None]        \n    elif accuracy >=85 and accuracy < 95:\n        if kaggle:\n            if run_num==2:\n                save_model(output_dir, subject, accuracy, height, width , model, weights)\n                print ('*********************Process is completed******************************')\n                return [False, None]\n            else:\n                print('running for 6 more epochs to see if accuracy improves')\n                return[True,6] # run for 8 more epochs\n        else:\n            msg='{0}With an accuracy of {1:5.2f} % the results are mediocure. Try running more epochs{2}'\n            print (msg.format(Cblu, accuracy,Cend))\n    else:\n        if kaggle:\n            if run_num==2:\n                save_model(output_dir, subject, accuracy, height,width , model, weights)\n                print ('*********************Process is completed******************************')\n                return [False, None]\n            else:\n                print('Running for 8 more epochs to see if accuracy improves')\n                return[True,8] # run for 8 more epochs\n        else:\n            msg='{0} With an accuracy  of {1:5.2f} % the results would appear to be unsatisfactory{2}'\n            print (msg.format(Cblu, accuracy, Cend))\n            msg='{0}You might try to run for more epochs or get more training data '\n            msg=msg + 'or perhaps crop your images so the desired subject takes up most of the image{1}'\n            print (msg.format(Cblu, Cend))\n    \n    tryagain=True\n    if kaggle==False:\n        while tryagain==True:\n            ans=input('To continue training from where it left off enter the number of additional epochs or enter H to halt  ')\n            if ans =='H' or ans == 'h':\n                run=False\n                tryagain=False\n                save_model(output_dir, subject, accuracy, height,width , model, weights)                                      \n                print ('*********************Process is completed******************************')\n                return [run,None]\n            else:\n                try:\n                    epochs=int(ans)\n                    run=True\n                    tryagain=False\n                    return [run,epochs]\n                except ValueError:\n                    print('{0}\\nyour entry {1} was neither H nor an integer- re-enter your response{2}'.format(Cred,ans,Cend))\n","e99b6381":"def set_dim(w,h):\n    wh_list=[224,160, 128, 96 ]\n    if w  in wh_list and h in wh_list and w==h:\n        return(w,w)\n    else:\n        x=h if h>w else w\n        # find closest value to what is in the list\n        delta_min=np.inf\n        for s in wh_list:\n            delta =abs( x-s)\n            if delta< delta_min:\n                delta_min=delta\n                h=s    \n    return (h,h)","a2d5aa5f":"def get_batch_size(dir, model_size): \n    max_batch_size = 80 if model_size=='V1' else 40\n    count=0    \n    dir_list=os.listdir(dir) # lists content of  directory\n    for d in dir_list:       # d is one of the class sub directories\n        d_path=os.path.join(dir,d)\n        if os.path.isdir(d_path):                        \n            file_list=os.listdir(d_path)\n            for f in file_list:      # f is a file in a class directory\n                f_path=os.path.join(d_path, f)\n                if os.path.isfile(f_path):\n                    count=count + 1\n    factors=[]\n    # find out if number of good files is divisable\n    for i in range (1, int(count\/2) +2):        \n        if count % i ==0:\n            factors.append(i) \n    # find the largest factor that is less than or equal to 100    \n    end=len(factors)-1 \n    for i in range(end, -1, -1):\n        if factors[i]<=max_batch_size:\n            batch_size=int(factors[i])\n            steps=int(count\/batch_size)\n            break    \n    return (batch_size, steps)","a17cb865":"def TF2_classify(source_dir, output_dir, mode, subject, v_split=5, epochs=20, batch_size=80,\n                 lr_rate=.002, height=224, width=224, rand_seed=128, model_size='V1', kaggle=False):\n    model_size=model_size.upper()\n    width, height =set_dim(width, height)\n    mode=mode.upper() \n    height, width = set_dim(height, width)\n    paths=get_paths(source_dir,output_dir,mode,subject)\n    #paths[0]=train path,paths[1]=test path paths[2]= valid path paths[3]=classes \n    gens=make_generators( paths, mode, batch_size, v_split, paths[3], height, width, rand_seed, model_size)\n    #gens[0]=train generator gens[1]= test generator  gens[2]= validation generator\n    #gens[3]=test file_names  gens[4]=test labels gens[5]=[train_steps, test_steps, valid_steps]\n    model=make_model(paths[3],lr_rate, height, width, model_size, rand_seed) \n    class val(tf.keras.callbacks.Callback):\n        # functions in this class adjust the learning rate \n        lowest_loss=np.inf\n        best_weights=model.get_weights()\n        lr=float(tf.keras.backend.get_value(model.optimizer.lr))\n        epoch=0\n        highest_acc=0\n        \n        def __init__(self):\n            super(val, self).__init__()\n            self.lowest_loss=np.inf\n            self.best_weights=model.get_weights()\n            self.lr=float(tf.keras.backend.get_value(model.optimizer.lr))\n            self.epoch=0\n            self.highest_acc=0\n            \n        def on_epoch_end(self, epoch, logs=None): \n            val.lr=float(tf.keras.backend.get_value(self.model.optimizer.lr))\n            val.epoch=val.epoch +1            \n            v_loss=logs.get('val_loss')\n            v_acc=logs.get('accuracy')\n            if val.highest_acc<v_acc:\n                val.highest_acc=v_acc\n                val.best_weights=model.get_weights()\n            if v_acc<=.95 and v_acc<val.highest_acc:\n                lr=float(tf.keras.backend.get_value(self.model.optimizer.lr))\n                ratio=v_acc\/val.highest_acc  # add a factor to lr reduction\n                new_lr=lr * .7 * ratio\n                tf.keras.backend.set_value(model.optimizer.lr, new_lr)\n                msg='{0}\\n current accuracy {1:7.4f} % is below 95 % and below the highest accuracy of {2:7.4f}, reducing lr to {3:11.9f}{4}'\n                print(msg.format(Cyellow, v_acc* 100, val.highest_acc, new_lr,Cend))   \n            if val.lowest_loss > v_loss:\n                msg='{0}\\n validation loss improved,saving weights with validation loss= {1:7.4f}\\n{2}'\n                print(msg.format(Cgreen, v_loss, Cend))\n                val.lowest_loss=v_loss\n                val.best_weights=model.get_weights()\n                \n            else:\n                 if v_acc>.95 and val.lowest_loss<v_loss:\n                        # reduce learning rate based on validation loss> val.best_loss\n                        lr=float(tf.keras.backend.get_value(self.model.optimizer.lr))\n                        ratio=val.lowest_loss\/v_loss  # add a factor to lr reduction\n                        new_lr=lr * .7 * ratio\n                        tf.keras.backend.set_value(model.optimizer.lr, new_lr)\n                        msg='{0}\\n current loss {1:7.4f} exceeds lowest loss of {2:7.4f}, reducing lr to {3:11.9f}{4}'\n                        print(msg.format(Cyellow, v_loss, val.lowest_loss, new_lr,Cend))\n           \n    callbacks=[val()]\n    run_num=0\n    run=True\n    tacc=[]\n    tloss=[]\n    vacc=[]\n    vloss=[]\n    start_epoch=0\n    while run:\n        run_num=run_num +1\n        if run_num==1:\n            print(' Starting Training Cycle')\n        else:\n            print('Resuming training from epoch {0}'.format(start_epoch))\n        results=train(model,callbacks, gens[0], gens[2],gens[5], epochs,start_epoch)\n        # returns data from training the model - append the results for plotting\n        tacc_new=results.history['accuracy']\n        tloss_new=results.history['loss']\n        vacc_new =results.history['val_accuracy']\n        vloss_new=results.history['val_loss']\n        for d in tacc_new:  # need to append new data from training to plot all epochs\n            tacc.append(d)\n        for d in tloss_new:\n            tloss.append(d)\n        for d in vacc_new:\n            vacc.append(d)\n        for d in vloss_new:\n            vloss.append(d)       \n        last_epoch=results.epoch[len(results.epoch)-1] # this is the last epoch run\n        tr_plot(tacc,vacc,tloss,vloss) # plot the data on loss and accuracy\n        bestw=val.best_weights  # these are the saved weights with the lowest validation loss\n        lr_rate=val.lr \n        predictions=make_predictions(model, bestw, gens[1], lr_rate)\n        accuracy=display_pred(output_dir, predictions, gens[3], gens[4], subject, model_size, paths[3], kaggle)        \n        model_path=os.path.join(source_dir, 'autism-224-224-95.71.h5')        \n        decide=wrapup(output_dir,subject, accuracy, height, width, model, bestw,run_num, kaggle)\n        run=decide[0]        \n        if run==True:\n            epochs=last_epoch + decide[1]+1\n            start_epoch=last_epoch +1\n        \n           ","660dcb27":"d=r'\/kaggle\/input'\nd_list=os.listdir(d)\np1=os.path.join(d, d_list[0])\nsource_dir=os.path.join(p1, 'DisasterModel')\noutput_dir=r'\/kaggle\/working'\nsubject='disasters'\nv_split=8\nepochs=15\nbatch_size=80\nlr_rate=.002\nheight=224\nwidth=224\nrand_seed=100\nmodel_size='V1'\nmode='SEP'\nkaggle=True  # added to deal with fact that kaggle 'commit' does not allow user entry\n              # set to True if you are doing a kaggle commit\nif kaggle:\n    output_dir=r'\/kaggle\/working'\n    \n\nTF2_classify(source_dir, output_dir, mode,subject, v_split= v_split, epochs=epochs,batch_size= batch_size,\n         lr_rate= lr_rate,height=height, width=width,rand_seed=rand_seed, model_size=model_size, kaggle=kaggle)\n\n\n    \n\n","57df6b0c":"The function below defines the paths for the data. When mode=ALL it takes all the data from a single\ndirectory. When the mode is not ALL it assumes the data is seperated into training set, validation set and a test set. This function may need to be modified dependent on the names of the sub directories. It also generates an output text file listing the class names and labels. This is useful for use with a companion prediction program that uses the trained model to make predictions. For this data set there is no test set. I used the validation data as the test set and created a seperate validation set in the make_generators function","3a0574cb":"This function is called when the specified number of epochs have completed training. It operates in two modes based on the setting of the parameter kaggle. If kaggle is false this function print out the test set accuracy then asks the user if they wish to enter a value for how many more epochs to run, or the user can enter H to end the program.\nIf kaggle-true this function evaluates the accuracy.If the accuracy is >=95% the program halts. If the accuracy is between 85% to 95% it will automatically run the training for 6 more epochs. If the accuracy is less than 85% it will automatically run the training for 8 more epochs.\nThis dependency on the kaggle parameter was necessitated because when committing a kernel Kaggle does\nnot allow user input.","ad2d2b89":"The code below determines where the source of the data resides, and sets parameters for the TF_classify function. The source_dir is the main directory where the data is stored. mode if set to all indictes that all the data is in the source directory. If mode is not ALL the program assumes that the data is partitioned into train, test and validation subdirectories of the source dir. Subject is a string used to define the subject of the classification and is used to labeloutput file names. epochs is the number of initial epochs to train. output_dir is the directory where output files will be stored. batch size is the training batch size. For the large model it is best set to 80 otherwise a resource exhaust error may get thrown. model_size is a single string character either 'L', 'M' or 'S' for selection of the large, medium or small model.v-split is used when there is either no validation set or if you regard it as to small to be representative of the data set probability distribution. If you elect to create your own\nvalidation set v_split is the percentage of training samples that will be used for validation. Set the markup for make_generators to see how to handle that case. Otherwise v_split is not used. The kagle parameter is a boolean. It was added because Kaggle commits do not allow user inputs. When set to true\nit preempts any user input code and automatically makes a selection based on model accuracy on the test set.","685fdab7":"This function produces two plots. The first is a plot of training loss and validation loss versus epochs. The second plot is training accuracy and validation accuracy vs epochs","70e00fd5":"this data set does not have test images only training and validation images. For convinience I will split the training set into a training set and a validation set. Parameter v_split is used to split the training set. Note the validation generator does not used the validation path path[2] but uses the training path path[0]. Also note the use of subset='training for the train generator, and subset='validation in the validation generator. The validation data from the data set is used as the test set in the test generator path[1] is set to be the path to the validation data.","bfa50f44":"This function initiates training","63e5b903":"This function calculates the proper batch size  and steps so that the  data generator goes through the list of files exactly one time. It counts all the files within the directory\nincluding files in all sub directories(classes)","1a094835":"This function sets the width and height of the MobileNet model. Mobile Net models only\nhave pre-trained weights for the dimensions in the list. This function takes the original\nwidth and height and determine the closest standard width and height.","8d3619e3":"the function below makes the model based on the selection of the model size. model_size='L' selects\nthe large model which is based on transfer learning for MobileNet. MobileNet works best when the\nimage size is selected as 224 X 224 if your image files are that size or above.\nMobilenet only has pretrained weights for image sizes 224 X 224, 160 X 160, 128 X 128 and 96 X 96.\nFunction get_dim takes the input height and width and selects a new height and width from the\nlist above that is the closest to what was specified. Otherwise the network would be randomly\ninitialized and the training time would be very large.\n\nIf model_size='M' a medium sized model is used. It is faster for computation but less accurate then the\nlarge model. If model_size='S' a small model is used. Again it is less accurate than the medium and large\nmodels but has significantly less computations.","2f100f97":"this function is called at the end of the program. It saves the trained model to the output directory\nso it can be used with a companion prediction program. The model is saved with the title\nsubject-height-width-model.txt where subject is the subject the user entered, height and width are those set by the user, model is the model_size, L, M or S.","7f95811f":"This is the main function. It calls the path function to get the test set paths. Then calls the make_generators function to create the generators. Then the make_model function is called to create the model. This function includes two classes tr and val used to subclass the keras callbacks class.\nThe tr class has a function on_batch_end called end the end of every batch while training. The function monitors the training accuracy. If the training accuracy does not improves for 10 consecutive batches it\nlowers the learning rate by a factor of .95. It continues to monitor the training accuracy until the\naccuracy reaches above 90%. At that point this function stops operating and the on_epoch_end function\ndefined in the val class takes over. IT monitors the validation loss at the end of each epoch. If the\nvalidation loss is the lowest to date it saves that loss value and also saves the model weights. If the loss at the end of the current epoch is above the stored lowest loss the function reduces the learning rate by a factor of .5.\nThis main function inititates training by calling the train function. When training is complete the history object results contains a list of the training loss, validation loss, training accuracy and validation accuracy. These values are saved and appended to a list of these values stored at the end of each training cycle. The function then calls the make_predictions function to make predictions on the test set using the model weights saved for the lowest validation loss. It then calls the display_predictions function to display the results of the predictions. Finally it calls the wrapup function which processes the prediction results and determines if more epochs will be run.","6150b659":"This function receives the model weights that were saved during training for the lowest validation loss.\nIt then makes predictions on the test set. Note when the data set has no test set it makes predictions on the validation set.","bb3f996e":"This is a general purpose image classifier that can be used for most image classification problems. No knowledge of neural networks or Tensorflow is required to use it. An example of use on the Autism data set is shown below\nsource_dir='c:\/\/Temp\/\/autism'\nsubject='autism'\nt_split=5\nv_split=5\nepochs=30\nbatch_size=80\nlr_rate=.0025\nimage_size=128\nrand_seed=256\nmodel_size='L'\nmode='sep'\n\nTF2_classify(source_dir,mode,subject, t_split=t_split, v_split=v_split, epochs=epochs,batch_size=batch_size,\n         lr_rate=lr_rate,image_size=image_size,rand_seed=rand_seed, model_size=model_size)\n         \nthe program operates in one of two modes, If mode-'all' the training, test and validation files are taken from the source_dir and split into train, test and validation files as defined by t_split and v_split integer percentages.\nIf model='sep' images are read in from the train, test and valid directories within the source directory. T-split and\nv_split values are not used.\nepochs is the number of training epochs\nlr_rate is the learning rate \nbatch size is the number of images processed as a group during training. \nThe program has 3 models. model_size='L' is a large model based on the MobileNet architecture. It is accurate but propcessing time and memory requirements can be large. For this model set batch_size-80. If you get a resource exhaust error reduce its value. If model_size=\"M' the program uses a medium sixed model. Execution is fairly fast but it is less accurate. A batch size of 150 works well. If model_size=\"S\" a small model is used. Execution is fast but accuracy is reduced.\nrand_seed sets the seed for the random generators. It's value is arbitrary  but when changed will give a different mix of training, test and validation files when mode='all'.\nimage_size is the size that images are converted to for processing. If mode=\"L\" image size is set internally at 224.\nsubject is a string you can use to denote the name of files that are stored in your source directory at the conclusion of\nthe program.\nAt the conclusion of training test results are displayed and you can save the error list to a file in the source directory.\nYou are also given the option to run for additional epochs starting from where you left off. At the conclusion of the \nprogram two files are stored in the source_dir. One of them is the resulting trained model file. It is labelled as\nsubject-image_size-accuracy.h5 , For example autism-224-95.35.h5 means the subject was autism, the image size was 224 X 224 and the accuracy on the test set was 95.35%. This model file can then be used with a prediction program.\nThe other file saved is a text file that can be easily converted into a python dictionary. The key is the class number and the value is the associated class name. It is labelled as subject.txt. This file will also be needed by a prediction program to generate a list of classes. \nIf you elected to save the error list it is stored in the source directory as error-list-M-accuracy.txt where M is the model type.\nTo use the program you need a python 3 environment in which you have loaded the modules\ntensorflow 2.0, numpy, matplot, sklearn, tqdm cv2,random and PIL","9d0cd4ed":"The function below processes the results of making predictions on the test set using the model weights that were saved \nfor the lowest validation loss. It determines the number of prediction errors and the accuracy. IT has two modes of \noperation dependent on the setting of the input parameter kaggle. On Kaggle when you commit your kernel it does not allow\nuser input. So I created the kaggle parameter to deal with that problem. With kaggle=True this function will print out a list of errors that includes the filename, true class, predicted class and probability of the prediction only if there are less than 35 errors. When kaggle=False, the user is given the option to print out the error. In either case an output file is created in the output directory that contains this error list data. The function also creates a horizontal bar chart of the number of errors by  class."}}