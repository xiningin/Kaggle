{"cell_type":{"4465f97c":"code","e67a3644":"code","3d2dc029":"code","106521ce":"code","c4a5cb27":"code","ad3d0030":"code","e54b4c9f":"code","9645a1b2":"code","94ac075c":"code","77de8820":"code","f464485d":"code","fcf12c94":"code","2f1f0f49":"code","833b508e":"code","28c04593":"code","4e1e040d":"code","fe9f178c":"code","9bfca24f":"code","3503885f":"code","bb501491":"code","52e8455a":"code","ef634710":"code","bb25c995":"code","e973fcc9":"code","b47f1ffd":"code","dcbf3ee3":"code","aac8e3d9":"code","94652316":"code","c7c77936":"code","e84a3fcc":"code","4948a5e9":"code","ed2777da":"code","00f22e31":"code","6a598c74":"code","13b390a2":"code","0093849a":"code","afeb036a":"code","48d06611":"code","55a595d8":"code","ecaa71e3":"code","52b162ae":"code","21db5e96":"code","224053b0":"code","19c23290":"code","12b12014":"code","05abd1b9":"code","bc6373d9":"code","fa404e5a":"code","e5c73a92":"code","0c3867e8":"code","2841c4c6":"code","2068fd2f":"code","ee02a41d":"code","79492a91":"code","cfd659c9":"code","e5c294fd":"code","383f1d42":"code","a519d3d8":"code","05c2152b":"code","f4e50d3f":"code","ff5a9e85":"code","b6d083ce":"code","6240bf4d":"code","8a17f978":"code","f5173e3f":"code","578df68c":"code","0e1baee9":"code","18000b25":"code","3e5377c1":"code","808ab593":"code","18b56f24":"code","20b52290":"code","96a317e3":"code","53bb2224":"code","cc07fda3":"code","ba989637":"code","861ae6a3":"code","bd0e65a7":"code","79266289":"code","9b7195c1":"code","9ed3e8f2":"code","cf446f74":"code","664017f2":"code","34cdc828":"code","5bbd49c8":"code","298bf10c":"code","dcbe5d45":"code","1a23b25e":"code","1a6b2e43":"code","96885907":"code","39d046ee":"code","c48e52dd":"code","6cf8bee1":"code","b123498e":"code","e7db44d6":"code","a1849e5e":"code","6b273ba2":"markdown","dacf120e":"markdown","ca89458e":"markdown","8585d155":"markdown","5c78a1da":"markdown","3862428d":"markdown","857d2f78":"markdown","f01f05ec":"markdown","fece86af":"markdown","5038019f":"markdown","79fe7099":"markdown","9c8e9c8d":"markdown","98d6b405":"markdown","ab424a30":"markdown","174e3594":"markdown","e568f998":"markdown","a1c31806":"markdown","b0651c1d":"markdown","204bdf17":"markdown","058ea1c1":"markdown","f52bcdea":"markdown","35b9e6ab":"markdown","c0ecc1f4":"markdown","33631829":"markdown","a8121b00":"markdown","44858ed6":"markdown","fca7a44f":"markdown","4c477387":"markdown","4fa2abc0":"markdown","bf56a67f":"markdown","2503425d":"markdown","a8f55b20":"markdown","fdcbe7b1":"markdown","1bbec39d":"markdown","86252660":"markdown","cf402cbe":"markdown","44a89671":"markdown","44921703":"markdown","ce643ada":"markdown","cfe05eb7":"markdown","f39cd49d":"markdown","37d2c220":"markdown","676eb036":"markdown","b4ff6259":"markdown","d07dabe7":"markdown","4893e37f":"markdown"},"source":{"4465f97c":"###############################################################################\n#                         Import libraries                                    #\n###############################################################################\n\n#Load the librarys\nimport pandas as pd #To work with dataset\nimport numpy as np #Math library\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns #Graph library that use matplot in background\nimport matplotlib.pyplot as plt #to plot some parameters in seaborn\nimport warnings\n# Preparation  \nfrom sklearn.preprocessing import LabelEncoder, OrdinalEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import PowerTransformer, StandardScaler,Normalizer,RobustScaler,MaxAbsScaler,MinMaxScaler,QuantileTransformer\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import KBinsDiscretizer\n# Import StandardScaler from scikit-learn\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.impute import KNNImputer,IterativeImputer\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import make_pipeline, Pipeline,FeatureUnion\nfrom sklearn.manifold import TSNE\n# Import train_test_split()\n# Metrics\nfrom sklearn.metrics import roc_auc_score, average_precision_score\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import roc_curve,confusion_matrix\nfrom datetime import datetime, date\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.linear_model import LinearRegression, RidgeCV\nfrom sklearn.linear_model import LogisticRegression\n\nimport tensorflow as tf \nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import LearningRateScheduler\n#import smogn\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import HistGradientBoostingClassifier\n# For training random forest model\nimport lightgbm as lgb\nfrom scipy import sparse\nfrom sklearn.neighbors import KNeighborsRegressor \nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans \n# Model selection\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression,f_classif,chi2\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.feature_selection import SelectPercentile\nfrom sklearn.feature_selection import mutual_info_classif,VarianceThreshold\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\n\nfrom scipy import stats, optimize, interpolate\n\nfrom lightgbm import LGBMClassifier\nimport lightgbm as lgbm\nfrom catboost import CatBoostRegressor, CatBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import svm\nfrom xgboost import XGBClassifier\nfrom sklearn import set_config\nfrom itertools import combinations\n# Cluster :\nfrom sklearn.cluster import MiniBatchKMeans\nfrom yellowbrick.cluster import KElbowVisualizer\n#import smong \nimport category_encoders as ce\nimport warnings\nimport optuna \nfrom joblib import Parallel, delayed\nimport joblib \nfrom sklearn import set_config\n# it's a library that we work with plotly\nimport plotly.offline as py \npy.init_notebook_mode(connected=True) # this code, allow us to work with offline plotly version\nimport plotly.graph_objs as go # it's like \"plt\" of matplot\nimport plotly.tools as tls # It's useful to we get some tools of plotly\nset_config(display='diagram')\nwarnings.filterwarnings('ignore')","e67a3644":"!pip  install klib","3d2dc029":"import klib","106521ce":"%%time \n###############################################################################\n#                         Load data                                           #\n###############################################################################\n\ntrain = pd.read_csv('..\/input\/GiveMeSomeCredit\/cs-training.csv')\ntest = pd.read_csv('..\/input\/GiveMeSomeCredit\/cs-test.csv')\ntrain.head(3)","c4a5cb27":"!pip install xlrd","ad3d0030":"###############################################################################\n#                         General View                                        #\n###############################################################################\n# Understand the variables\nvariables = pd.DataFrame(columns=['Variable','Number of unique values','Number of nulls', 'Values'])\n\nfor i, var in enumerate(train.columns):\n    variables.loc[i] = [var, train[var].nunique(), train[var].isnull().sum(), train[var].unique().tolist()]\n    \n# Join with the variables dataframe\nvar_dict = pd.read_excel('..\/input\/GiveMeSomeCredit\/Data Dictionary.xls', index_col=0)\nvariables.set_index('Variable').join(var_dict)","e54b4c9f":"# Understand the variables\nvariables = pd.DataFrame(columns=['Variable','Number of unique values','Number of nulls', 'Values'])\n\nfor i, var in enumerate(test.columns):\n    variables.loc[i] = [var, test[var].nunique(), test[var].isnull().sum(), test[var].unique().tolist()]\n    \n# Join with the variables dataframe\nvar_dict = pd.read_excel('..\/input\/GiveMeSomeCredit\/Data Dictionary.xls', index_col=0)\nvariables.set_index('Variable').join(var_dict)","9645a1b2":"###############################################################################\n#                         Stat                                                #\n###############################################################################\n\ntrain[[\"RevolvingUtilizationOfUnsecuredLines\", \"DebtRatio\"]].describe()","94ac075c":"train[[\"MonthlyIncome\"]].describe()","77de8820":"train[ [\"NumberOfDependents\",\n                       \"NumberOfTime60-89DaysPastDueNotWorse\",\n                       \"NumberRealEstateLoansOrLines\",\n                       \"NumberOfTimes90DaysLate\",\n                       \"NumberOfOpenCreditLinesAndLoans\",\n                       \"NumberOfTime30-59DaysPastDueNotWorse\",\n                       \"age\"]].describe()","f464485d":"###############################################################################\n#                         Caridnality                                         #\n###############################################################################\n\n# Cardinality : \n# - RevolvingUtilizationOfUnsecuredLines :125728, high Outlier\n# - DebtRatio :114194 , high Outlier \n# deal with outlier + bin \nPERCENTAGE = [\"RevolvingUtilizationOfUnsecuredLines\", \"DebtRatio\"]\n# MonthlyIncome:13594 , high outlier +bin \nREAL= [\"MonthlyIncome\"]\n# Can be considred as cat \nNUMERIC_DISCRET_low = [\"NumberOfDependents\",\n                       \"NumberOfTime60-89DaysPastDueNotWorse\",\n                       \"NumberRealEstateLoansOrLines\",\n                       \"NumberOfTimes90DaysLate\",\n                       \"NumberOfOpenCreditLinesAndLoans\",\n                       \"NumberOfTime30-59DaysPastDueNotWorse\",\n                       \"age\"]\n\nTARGET = [\"SeriousDlqin2yrs\"]\n\n#also change the type for TARGET to categorical\n#df[TARGET] = df[TARGET].astype('category')","fcf12c94":"###############################################################################\n#                         Reduce Memory                                       #\n###############################################################################\n\n# Author : https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        name =df[col].dtype.name \n        \n        if col_type != object and col_type.name != 'category':\n        #if name != \"category\":    \n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df\ntrain= reduce_mem_usage(train)","2f1f0f49":"train.info()","833b508e":"###############################################################################\n#                         Missing Value                                       #\n###############################################################################\n\n# summarize the number of rows with missing values for each column\nfor i in range(train.shape[1]):\n    # count number of rows with missing values\n    n_miss = train.iloc[:,i].isnull().sum()\n    perc = n_miss \/ train.shape[0] * 100\n    print('> %d, Missing: %d (%.1f%%)' % (i, n_miss, perc))","28c04593":"klib.missingval_plot(train)","4e1e040d":"train.duplicated().sum()","fe9f178c":"len(train)-len(train.drop_duplicates())","9bfca24f":"#df[NUMERIC].agg(['min','max']).round(2)\ntrain.agg(['min','max']).round(2)","3503885f":"###############################################################################\n#                         Outlier Value                                       #\n###############################################################################\n\nplt.figure(figsize=(10,2))\nsns.boxplot(data=train, x='RevolvingUtilizationOfUnsecuredLines')\nplt.title(\"Distribution of RevolvingUtilizationOfUnsecuredLines\");","bb501491":"plt.figure(figsize=(10,2))\nsns.boxplot(data=train, x='DebtRatio')\nplt.title(\"Distribution of DebtRatio\");","52e8455a":"plt.figure(figsize=(10,2))\nsns.boxplot(data=train, x='MonthlyIncome')\nplt.title(\"Distribution of MonthlyIncome\");","ef634710":"skew =train.skew().sort_values(ascending =False )\nskew_df= pd.DataFrame({'skew':skew})\nskew_df.head(10)","bb25c995":"skew_df[(skew_df['skew']>=1) |(skew_df['skew']<=-1) ].index","e973fcc9":"ax = sns.distplot(train['MonthlyIncome'])","b47f1ffd":"sns.boxplot(data=train['MonthlyIncome'], saturation=.3)","dcbf3ee3":"amount_corrected= np.log(train['MonthlyIncome' ]+1)\n\nprint(train['MonthlyIncome'].skew())\n\nprint(amount_corrected.skew())","aac8e3d9":"ax = sns.distplot(amount_corrected)","94652316":"kurtosis= pd.DataFrame(train.kurtosis(),columns=['Kurtosis'])\nkurtosis.head(10)","c7c77936":"kurtosis[(kurtosis['Kurtosis']>=3) |(kurtosis['Kurtosis']<=-3) ].index","e84a3fcc":"amount_corrected.kurtosis()","4948a5e9":"var= train.var().sort_values(ascending =True )\nvar_df= pd.DataFrame({'var':var})\nvar_df.head(10)","ed2777da":"Q1 = train.quantile(0.25)\nQ3 = train.quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)","00f22e31":"###############################################################################\n#                         Quantile  Value                                     #\n###############################################################################\n\ntrain_num=train.select_dtypes(exclude=['category']) \ndf_out = train_num[~((train_num < (Q1 - 1.5 * IQR))|(train_num > (Q3 + 1.5 * IQR))).any(axis=1)]\nprint(df_out.shape)\ndel train_num \ndel df_out ","6a598c74":"# shape of our tain data \ntrain.shape","13b390a2":"# stat Analysis\ntrain.describe().T","0093849a":"sns.pairplot(train.iloc[0:2000,:], hue= 'SeriousDlqin2yrs')","afeb036a":"###############################################################################\n#                         Pearson Corrolation                                 #\n###############################################################################\n\nplot = klib.corr_plot(train,  figsize=(12,10))","48d06611":"klib.corr_plot(train, split='pos') # displaying only positive correlations, other settings include threshold, cmap...","55a595d8":"klib.corr_plot(train, split='neg') # displaying only negative correlations","ecaa71e3":"corrmat = train.corr()\ncorr_with_target = corrmat['SeriousDlqin2yrs'].apply(abs).sort_values(ascending=False)\ncorr_with_target.drop(['SeriousDlqin2yrs'], inplace=True)\ndf = pd.DataFrame(data={'features': corr_with_target.index, 'target': corr_with_target.values})\nplt.figure(figsize=(20, 20))\nsns.barplot(x=\"target\", y=\"features\", data=df)\nplt.title('Correlation with target')\nplt.tight_layout()\nplt.show()","52b162ae":"klib.dist_plot(train)","21db5e96":"#g= sns.countplot(x='SeriousDlqin2yrs', data=train)\n#plt.xticks(rotation=45)","224053b0":"# Convert Dtypes :\ntrain[train.select_dtypes(['int64','int16','float16','float32','float64','int8']).columns] = train[train.select_dtypes(['int64','int16','float16','float32','float64','int8']).columns].apply(pd.to_numeric)\ntrain[train.select_dtypes(['object','category']).columns] = train.select_dtypes(['object','category']).apply(lambda x: x.astype('category'))","19c23290":"# Pour le train test\ntarget= \"SeriousDlqin2yrs\"\nX = train.drop(['SeriousDlqin2yrs','Unnamed: 0'], axis='columns')# axis=1\ny = train[target]","12b12014":"cat_columns = X.select_dtypes(exclude=['int64','int16','float16','float32','float64','int8']).columns\nnum_columns = X.select_dtypes(include=['int64','int16','float16','float32','float64','int8']).columns","05abd1b9":"f, axes = plt.subplots(nrows=3, ncols=1, figsize=(12, 12))\nf.suptitle('MonthlyIncome', fontsize=16)\ng = sns.kdeplot(train['MonthlyIncome'], shade=True, label=\"%.2f\"%(train['MonthlyIncome'].skew()), ax=axes[0])\ng = g.legend(loc=\"best\")\nstats.probplot(train['MonthlyIncome'], plot=axes[1])\nsns.boxplot(x='MonthlyIncome', data=train, orient='h', ax=axes[2]);\nplt.tight_layout()\nplt.show()","bc6373d9":"# Numerical features distribution \ni = 1\nplt.figure()\nfig, ax = plt.subplots(6, 2,figsize=(20, 24))\nfor feature in num_columns:\n    plt.subplot(6, 2,i)\n    sns.histplot(train[feature],color=\"red\", kde=True,bins=100, label='train')\n    sns.histplot(test[feature],color=\"olive\", kde=True,bins=100, label='test')\n    plt.xlabel(feature, fontsize=9); plt.legend()\n    i += 1\nplt.show()","fa404e5a":"train.corr()['SeriousDlqin2yrs'][:-1].plot.barh(figsize=(8,6),alpha=.6,color='darkblue')\nplt.xlim(-.075,.075);\nplt.xticks([-0.065, -0.05 , -0.025,  0.   ,  0.025,  0.05 ,  0.065],\n           [str(100*i)+'%' for i in [-0.065, -0.05 , -0.025,  0.   ,  0.025,  0.05 ,  0.065]],fontsize=12)\nplt.title('Correlation between target and numerical variables',fontsize=14);","e5c73a92":"v0 = sns.color_palette(palette='viridis').as_hex()[0]\nfig = plt.figure(figsize=(18,6))\nsns.boxplot(data=train[num_columns], color=v0,saturation=.5);\nplt.xticks(fontsize= 14)\nplt.title('Box plot of train numerical columns', fontsize=16);","0c3867e8":"# Categorical features distribution \n#i = 1\n#plt.figure()\n#fig, ax = plt.subplots(1, 1,figsize=(10,10))\n#for feature in ['SeriousDlqin2yrs']:\n #   plt.subplot(1, 1,i)\n #   sns.histplot(train[feature],color=\"blue\", label='train')\n #   #sns.histplot(test[feature],color=\"olive\", label='test')\n #   plt.xlabel(feature, fontsize=9); plt.legend()\n #   i += 1\n#plt.show()","2841c4c6":"train.SeriousDlqin2yrs.value_counts()","2068fd2f":"labels = train['SeriousDlqin2yrs'].astype('category').cat.categories.tolist()\ncounts = train['SeriousDlqin2yrs'].value_counts()\nsizes = [counts[var_cat] for var_cat in labels]\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=True) #autopct is show the % on plot\nax1.axis('equal')\nplt.show()","ee02a41d":"# Categorical features distribution \nplt.figure()\nsns.countplot(train['SeriousDlqin2yrs'], label='IsFraud')\nplt.xlabel(feature, fontsize=9); plt.legend()\nplt.xticks(rotation=45)\nplt.show()","79492a91":"train.SeriousDlqin2yrs.value_counts()\/len(train)","cfd659c9":"fig = plt.figure(figsize=(30,40))\ngrid =  gridspec.GridSpec(10,1,figure=fig,hspace=.5,wspace=.5)\nn =0\nfor i in range(10):\n    for j in range(1):\n        ax = fig.add_subplot(grid[i, j])\n        sns.violinplot(data =  train, y = num_columns[n] , x ='SeriousDlqin2yrs' ,ax=ax, alpha =.7, fill=True,palette='viridis')\n        ax.set_title(num_columns[n],fontsize=14)\n        ax.set_xlabel('')\n        ax.set_ylabel('')\n        n += 1\nfig.suptitle('Violin plot of target with numerical features', fontsize=16,y=.93);","e5c294fd":"fig = plt.figure(figsize=(50,50))\ngrid =  gridspec.GridSpec(10,1,figure=fig,hspace=.8,wspace=.8)\nn =0\nfor i in range(10):\n    for j in range(1):\n        ax = fig.add_subplot(grid[i, j])\n        sns.kdeplot(data = train, y = num_columns[n],  hue = 'SeriousDlqin2yrs',ax=ax, alpha =.7, fill=False)\n        ax.set_title(num_columns[n],fontsize=14)\n        ax.set_xlabel('')\n        ax.set_ylabel('')\n        n += 1\n        \nfig.suptitle('KDE plot of train target with  features', fontsize=16,y=.93);","383f1d42":"# Not fully paid \ntrain[train['SeriousDlqin2yrs']==1].describe().style.bar(subset=['mean'], color='#205ff2')\\\n                            .background_gradient(subset=['min'], cmap='Reds')\\\n                            .background_gradient(subset=['max'], cmap='coolwarm')","a519d3d8":"# fully paid \ntrain[train['SeriousDlqin2yrs']==0].describe().style.bar(subset=['mean'], color='#205ff2')\\\n                            .background_gradient(subset=['min'], cmap='Reds')\\\n                            .background_gradient(subset=['max'], cmap='coolwarm')","05c2152b":"df_good = train.loc[train[\"SeriousDlqin2yrs\"] == 0]['DebtRatio'].values.tolist()\ndf_bad = train.loc[train[\"SeriousDlqin2yrs\"] == 1]['DebtRatio'].values.tolist()\ndf_age = train['DebtRatio'].values.tolist()\n\n#First plot\ntrace0 = go.Histogram(\n    x=df_good,\n    histnorm='probability',\n    name=\"Good Credit\"\n)\n#Second plot\ntrace1 = go.Histogram(\n    x=df_bad,\n    histnorm='probability',\n    name=\"Bad Credit\"\n)\n#Third plot\ntrace2 = go.Histogram(\n    x=df_age,\n    histnorm='probability',\n    name=\"Overall DebtRatio\"\n)\n\n#Creating the grid\nfig = tls.make_subplots(rows=2, cols=2, specs=[[{}, {}], [{'colspan': 2}, None]],\n                          subplot_titles=('Good','Bad', 'General Distribuition'))\n\n#setting the figs\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig.append_trace(trace2, 2, 1)\n\nfig['layout'].update(showlegend=True, title='DebtRatio Distribuition', bargap=0.05)\npy.iplot(fig, filename='custom-sized-subplot-with-subplot-titles')","f4e50d3f":"df_good = train.loc[train[\"SeriousDlqin2yrs\"] == 0]['NumberOfTime30-59DaysPastDueNotWorse'].values.tolist()\ndf_bad = train.loc[train[\"SeriousDlqin2yrs\"] == 1]['NumberOfTime30-59DaysPastDueNotWorse'].values.tolist()\ndf_age = train['NumberOfTime30-59DaysPastDueNotWorse'].values.tolist()\n\n#First plot\ntrace0 = go.Histogram(\n    x=df_good,\n    histnorm='probability',\n    name=\"Good Credit\"\n)\n#Second plot\ntrace1 = go.Histogram(\n    x=df_bad,\n    histnorm='probability',\n    name=\"Bad Credit\"\n)\n#Third plot\ntrace2 = go.Histogram(\n    x=df_age,\n    histnorm='probability',\n    name=\"Overall NumberOfTime30-59DaysPastDueNotWorse\"\n)\n\n#Creating the grid\nfig = tls.make_subplots(rows=2, cols=2, specs=[[{}, {}], [{'colspan': 2}, None]],\n                          subplot_titles=('Good','Bad', 'General Distribuition'))\n\n#setting the figs\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig.append_trace(trace2, 2, 1)\n\nfig['layout'].update(showlegend=True, title='NumberOfTime30-59DaysPastDueNotWorse', bargap=0.05)\npy.iplot(fig, filename='custom-sized-subplot-with-subplot-titles')","ff5a9e85":"df_good = train.loc[train[\"SeriousDlqin2yrs\"] == 0]['NumberOfTimes90DaysLate'].values.tolist()\ndf_bad = train.loc[train[\"SeriousDlqin2yrs\"] == 1]['NumberOfTimes90DaysLate'].values.tolist()\ndf_age = train['NumberOfTimes90DaysLate'].values.tolist()\n\n#First plot\ntrace0 = go.Histogram(\n    x=df_good,\n    histnorm='probability',\n    name=\"Good Credit\"\n)\n#Second plot\ntrace1 = go.Histogram(\n    x=df_bad,\n    histnorm='probability',\n    name=\"Bad Credit\"\n)\n#Third plot\ntrace2 = go.Histogram(\n    x=df_age,\n    histnorm='probability',\n    name=\"Overall NumberOfTimes90DaysLate\"\n)\n\n#Creating the grid\nfig = tls.make_subplots(rows=2, cols=2, specs=[[{}, {}], [{'colspan': 2}, None]],\n                          subplot_titles=('Good','Bad', 'General Distribuition'))\n\n#setting the figs\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig.append_trace(trace2, 2, 1)\n\nfig['layout'].update(showlegend=True, title='NumberOfTimes90DaysLate Distribuition', bargap=0.05)\npy.iplot(fig, filename='custom-sized-subplot-with-subplot-titles')","b6d083ce":"df_good = train.loc[train[\"SeriousDlqin2yrs\"] == 0]['NumberOfTimes90DaysLate'].values.tolist()\ndf_bad = train.loc[train[\"SeriousDlqin2yrs\"] == 1]['NumberOfTimes90DaysLate'].values.tolist()\n\nfig, ax = plt.subplots(nrows=2, figsize=(12,8))\nplt.subplots_adjust(hspace = 0.4, top = 0.8)\n\ng1 = sns.distplot(df_good, ax=ax[0], \n             color=\"g\")\ng1 = sns.distplot(df_bad, ax=ax[0], \n             color='r')\ng1.set_title(\"NumberOfTimes90DaysLate Distribuition\", fontsize=15)\ng1.set_xlabel(\"Age\")\ng1.set_xlabel(\"Frequency\")\n\ng2 = sns.countplot(x=\"NumberOfTimes90DaysLate\",data=train, \n              palette=\"hls\", ax=ax[1], \n              hue = \"SeriousDlqin2yrs\")\ng2.set_title(\"NumberOfTimes90DaysLate Counting by Risk\", fontsize=15)\ng2.set_xlabel(\"NumberOfTimes90DaysLate\")\ng2.set_xlabel(\"Count\")\nplt.show()","6240bf4d":"df_good = train.loc[train[\"SeriousDlqin2yrs\"] == 0]['age'].values.tolist()\ndf_bad = train.loc[train[\"SeriousDlqin2yrs\"] == 1]['age'].values.tolist()\ndf_age = train['age'].values.tolist()\n\n#First plot\ntrace0 = go.Histogram(\n    x=df_good,\n    histnorm='probability',\n    name=\"Good Credit\"\n)\n#Second plot\ntrace1 = go.Histogram(\n    x=df_bad,\n    histnorm='probability',\n    name=\"Bad Credit\"\n)\n#Third plot\ntrace2 = go.Histogram(\n    x=df_age,\n    histnorm='probability',\n    name=\"Overall age\"\n)\n\n#Creating the grid\nfig = tls.make_subplots(rows=2, cols=2, specs=[[{}, {}], [{'colspan': 2}, None]],\n                          subplot_titles=('Good','Bad', 'General Distribuition'))\n\n#setting the figs\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig.append_trace(trace2, 2, 1)\n\nfig['layout'].update(showlegend=True, title='age', bargap=0.05)\npy.iplot(fig, filename='custom-sized-subplot-with-subplot-titles')","8a17f978":"fig, ax = plt.subplots(nrows=2, figsize=(12,8))\nplt.subplots_adjust(hspace = 0.4, top = 0.8)\n\ng1 = sns.distplot(df_good, ax=ax[0], \n             color=\"g\")\ng1 = sns.distplot(df_bad, ax=ax[0], \n             color='r')\ng1.set_title(\"Age Distribuition\", fontsize=15)\ng1.set_xlabel(\"Age\")\ng1.set_xlabel(\"Frequency\")\n\ng2 = sns.countplot(x=\"age\",data=train, \n              palette=\"hls\", ax=ax[1], \n              hue = \"SeriousDlqin2yrs\")\ng2.set_title(\"Age Counting by Risk\", fontsize=15)\ng2.set_xlabel(\"Age\") \ng2.set_xlabel(\"Count\")\nplt.show()","f5173e3f":"#Let's look the Credit Amount column\ninterval = (0,18, 25, 35, 60, 110)\ncats = ['Child','Student', 'Young', 'Adult', 'Old']\ntrain[\"age_cat\"] = pd.cut(train.age, interval, labels=cats)\ndf_good = train[train[\"SeriousDlqin2yrs\"] == 0]\ndf_bad = train[train[\"SeriousDlqin2yrs\"] == 1]\ntrace0 = go.Box(\n    y=df_good[\"NumberOfTimes90DaysLate\"],\n    x=df_good[\"age_cat\"],\n    name='Good credit',\n    marker=dict(\n        color='#3D9970'\n    )\n)\n\ntrace1 = go.Box(\n    y=df_bad['NumberOfTimes90DaysLate'],\n    x=df_bad['age_cat'],\n    name='Bad credit',\n    marker=dict(\n        color='#FF4136'\n    )\n)\n    \ndata = [trace0, trace1]\n\nlayout = go.Layout(\n    yaxis=dict(\n        title='NumberOfTimes90DaysLate with age ',\n        zeroline=False\n    ),\n    xaxis=dict(\n        title='Age Categorical'\n    ),\n    boxmode='group'\n)\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig, filename='box-age-cat')","578df68c":"%%time \nm = TSNE()\ndf_numeric =X.iloc[0:25000,:]._get_numeric_data()\ndf_numeric=df_numeric.dropna()\nX_train =RobustScaler().fit_transform(df_numeric)\ndel df_numeric \n# Fit and transform the t-SNE model on the numeric dataset\ntsne_features = m.fit_transform(X_train)\nprint(tsne_features.shape)","0e1baee9":"trainessai=train.iloc[0:25000,:]\ntrainessai=trainessai.dropna()\ntrainessai['x']=tsne_features[:, 0]\ntrainessai['y']=tsne_features[:, 1]\n# Color the points according to Army Component\n# Plot tsne scatter plot\ndef tsne_scatterplot(data, hue):\n    # Color the points \n    sns.scatterplot(x=\"t_SNE_PC_1\", y=\"t_SNE_PC_2\", hue=hue, data=data, alpha=0.3)\n    plt.title(hue)\nsns.scatterplot(x='x', y='y', hue='SeriousDlqin2yrs', data=trainessai)\n# Show the plot\nplt.show","18000b25":"# Define K-means model \nX_imputed=SimpleImputer( strategy='median').fit_transform(X)\nX_train =PowerTransformer().fit_transform(X_imputed)\nkmeans = MiniBatchKMeans( random_state=42)\nvisualizer = KElbowVisualizer(kmeans, k=(1,40))\nvisualizer.fit(X_train)        # Fit the data to the visualizer\nvisualizer.show()        # Finalize and render the figure","3e5377c1":"\"\"\"\n    For each of the given values of k, perform the following steps:\n    Create a KMeans instance called model with k clusters.\n    Fit the model to the grain data samples.\n    Append the value of the inertia_ attribute of model to the list inertias.\n    The code to plot ks vs inertias has been written for you, so hit 'Submit Answer' to see the plot!\n\"\"\"\nks = range(1, 12)\ninertias = []\n\nfor k in ks:\n    # Create a KMeans instance with k clusters: model\n    model= KMeans(n_clusters=k)\n    \n    # Fit model to samples\n    model.fit(X_train)\n    \n    # Append the inertia to the list of inertias\n    inertias.append(model.inertia_)\n    \n# Plot ks vs inertias\nplt.plot(ks, inertias, '-o')\nplt.xlabel('number of clusters, k')\nplt.ylabel('inertia')\nplt.xticks(ks)\nplt.show()","808ab593":"kmeans = MiniBatchKMeans(n_clusters=12 ,random_state=42)\nkmean_label= kmeans.fit_predict(X_train)\nprint(kmean_label)","18b56f24":"train['cluster'] = kmean_label\ntrain['cluster'] = train['cluster'].astype('object')","20b52290":"#Getting the Centroids\ncentroids = kmeans.cluster_centers_\n#Getting unique labels\nu_labels = np.unique(kmean_label)\n#plotting the results:\nfor i in u_labels:\n    plt.scatter(train.iloc[kmean_label == i ,1] , train.iloc[kmean_label == i , 2] , label = i)\n\nplt.scatter(centroids[:,1] , centroids[:,2] , s = 80,marker='D', color = 'k')\nplt.legend()\nplt.show()","96a317e3":"red = sns.light_palette(\"red\", as_cmap=True)\n\ncross_tab=pd.crosstab(train['cluster'], train['SeriousDlqin2yrs'], margins = True)\n\nH=cross_tab\/cross_tab.loc[\"All\"] # Divide by column totals\n\nH.style.background_gradient(cmap=red)","53bb2224":"plt.figure()\n\nsns.countplot(x='cluster', hue='SeriousDlqin2yrs', data=train, palette='RdBu')\n\n#plt.xticks([0,1], ['No', 'Yes'])\nplt.title(\"Number of loans by cluster \");\n\nplt.show()","cc07fda3":"# plot the loan purposes for each cluster\npd.crosstab(train.cluster, train.SeriousDlqin2yrs).plot.bar(figsize=(9,5), rot=0, sort_columns=False, )\nplt.grid(axis='y')\nplt.xlabel(\"\")\nplt.title(\"Number of loans by cluster and purpose\");","ba989637":"ax = sns.boxplot(x=\"SeriousDlqin2yrs\", y=\"cluster\", data=train)","861ae6a3":"ax = sns.boxplot(x=\"cluster\", y=\"MonthlyIncome\", hue=\"SeriousDlqin2yrs\",\n\n                 data=train, palette=\"Set3\")","bd0e65a7":"ax = sns.boxplot(x=\"cluster\", y=\"DebtRatio\", hue=\"SeriousDlqin2yrs\",\n\n                 data=train, palette=\"Set3\")","79266289":"ax = sns.boxplot(x=\"cluster\", y='NumberOfTime60-89DaysPastDueNotWorse', hue=\"SeriousDlqin2yrs\",\n\n                 data=train, palette=\"Set3\")","9b7195c1":"# Create a KMeans model with 3 clusters: model\nmodel = KMeans(n_clusters=2)\n\n# Use fit_predict to fit model and obtain cluster labels: labels\nlabels = model.fit_predict(X_train)\n\n# Create a DataFrame with labels and varieties as columns: df\ndf = pd.DataFrame({'labels': labels, 'real_value': y})\n\n# Create crosstab: ct\nct =pd.crosstab(df['labels'],df['real_value'])\n\n# Display ct\nprint(ct)","9ed3e8f2":"#train=train.drop(['Unnamed: 0'],axis=1)\ntrain.columns","cf446f74":"X.columns","664017f2":"X.shape","34cdc828":"# Set attributes for PCA analysis\nn=10\ncolumns=['PCA_1', 'PCA_2', 'PCA_3', 'PCA_4', 'PCA_5', 'PCA_6', 'PCA_7', 'PCA_8', 'PCA_9', 'PCA_10']","5bbd49c8":"# Create the PCA instance and fit and transform the data with pca\npca = PCA(n_components=n)\npc = pca.fit_transform(X_train)\ndf_pc = pd.DataFrame(pc, columns=columns, index=X.index)\ndf_pc.head()","298bf10c":"pca.explained_variance_ratio_","dcbe5d45":"# PCA df which store PCA componenets and corresponding y\ndf_PCA = pd.concat([df_pc, y], axis=1)\ndf_PCA.head()","1a23b25e":"targets = [0, 1]\ncolors = ['orange', 'blue']\n\nfig, ax = plt.subplots(figsize=(8,8))\n\n\n# For loop to create plot\nfor target, color in zip(targets,colors):\n    indicesToKeep = df_PCA['SeriousDlqin2yrs'] == target\n    ax.scatter(df_PCA.loc[indicesToKeep, 'PCA_1']\n               , df_PCA.loc[indicesToKeep, 'PCA_2']\n               , c = color\n               , s = 50\n              , alpha =0.2)\n\n# Legend    \nax.legend(targets)\nax.grid()\nplt.show()","1a6b2e43":"# Instantiate\npca = PCA(n_components=n)\n\n# Fit and transform\nprincipalComponents = pca.fit_transform(X_train)\n\n# List principal components names\nprincipal_components =columns\n\n# Create a DataFrame\npca_df = pd.DataFrame({'Variance Explained': pca.explained_variance_ratio_,\n             'PC':principal_components})\n\nplt.figure(figsize=(8, 8))\nplt.title('PCA - explained variance ratio')\n# Plot DataFrame\nsns.barplot(x='PC',y='Variance Explained', \n           data=pca_df, color=\"c\")\nplt.xticks(rotation=45)\nplt.show()","96885907":"plt.figure(figsize=(8, 8))\n# Instantiate, fit and transform\n#pca2 = PCA()\n#principalComponents2 = pca2.fit_transform(X_train_valid_e_n_up)\n\n# Assign variance explained\nvar = pca.explained_variance_ratio_\n\n# Plot cumulative variance\ncumulative_var = np.cumsum(var)*100\nplt.plot(cumulative_var,'k-o',markerfacecolor='None',markeredgecolor='k')\nplt.title('Principal Component Analysis \\n Cumulative Proportion of Variance Explained',fontsize=12)\nplt.xlabel(\"Principal Component\",fontsize=12)\nplt.ylabel(\"Cumulative Proportion of Variance \",fontsize=12)\nplt.show()","39d046ee":"exp_var_cumsum=pd.Series(np.round(pca.explained_variance_ratio_.cumsum(),4)*100)  \nfor index,var in enumerate(exp_var_cumsum):  \n    print('if n_components= %d,   variance=%f' %(index,np.round(var,3)))","c48e52dd":"train.columns","6cf8bee1":"# Split the dataset and labels into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1,random_state=0,stratify=y )\nprint(\"{} rows in test set vs. {} in training set. {} Features.\".format(X_test.shape[0], X_train.shape[0], X_test.shape[1]))","b123498e":"# select non-numeric columns\ncat_columns = X.select_dtypes(exclude=['int64','int16','float32','float64','int8']).columns","e7db44d6":"# select the float columns\nnum_columns = X.select_dtypes(include=['int64','int16','float32','float64','int8']).columns","a1849e5e":"all_columns = (num_columns.append(cat_columns))\nprint(cat_columns)\nprint(num_columns)\nprint(all_columns)","6b273ba2":"# PCA Analysis ","dacf120e":"Numerical Data seems to be with  outliers appearing in the box plot.","ca89458e":"As we see we can't detect wich features have diffrent distubition with the target as we have a such imb data .","8585d155":"As we see non linear transformation give us some cluster of fraud well separated from non fraud data \nWe can get possible cluster for this data ","5c78a1da":"# What should we do for each colmun\n\n**Separate features by dtype**\n\nNext we\u2019ll separate the features in the dataframe by their datatype. There are a few different ways to achieve this. I\u2019ve used the select_dtypes() function to obtain specific data types by passing in np.number to obtain the numeric data and exclude=['np.number'] to return the categorical data. Appending .columns to the end returns an Index list containing the column names. For the categorical features, we don\u2019t want to include the target income column, so I\u2019ve dropped that.\n\n**Cat Features**\n\n\n\n","3862428d":"# Null Data\n\nHow sparse is my data? Most data sets contain missing values, often represented as NaN (Not a Number). If you are working with Pandas you can easily check how many missing values exist in each column.","857d2f78":"# Kmeans :\nLet's see if there is hidden cluster\n","f01f05ec":"\n# Box plot of numerical columns\n\n","fece86af":"# Num\/Cat Features\nwe should extract them and see what we should do for each one ","5038019f":"# Target Class distribution","79fe7099":"\n# Create test and train groups\n\nNow we\u2019ve got our dataframe ready we can split it up into the train and test datasets for our model to use. We\u2019ll use the Scikit-Learn train_test_split() function for this. By passing in the X dataframe of raw features, the y series containing the target, and the size of the test group (i.e. 0.1 for 10%), we get back the X_train, X_test, y_train and y_test data to use in the model.\n","9c8e9c8d":"All data is num and we have some nulls","98d6b405":"**Num Features**\n\n","ab424a30":"# Convert Dtypes :","174e3594":"\n## Step 2: Load the data\nComplete guid to read data : \nhttps:\/\/github.com\/DeepSparkChaker\/CRISPDM_ULTIME\/blob\/main\/CRISPDM_0_StreamlinedDataIngestionWithPandas.ipynb\n\n\nNext, we'll load the training and test data.\n","e568f998":"# Introduction: \n    \nBanks play a crucial role in market economies. They decide who can get finance and on what terms and can make or break investment decisions. For markets and society to function, individuals and companies need access to credit. \n\nCredit scoring algorithms, which make a guess at the probability of default, are the method banks use to determine whether or not a loan should be granted.\n\n<a id=0><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\">\n\n<center>CRISP-DM Methodology<\/center><\/h3>\n\n* [Buissness Understanding](#1)\n* [Data Understanding](#2)\n* [Data Preparation](#3)\n* [Data Modeling](#4)   \n* [Data Evaluation](#5)\n    \n\nIn this section we overview our selected method for engineering our solution. CRISP-DM stands for Cross-Industry Standard Process for Data Mining. It is an open standard guide that describes common approaches that are used by data mining experts. CRISP-DM includes descriptions of the typical phases of a project, including tasks details and provides an overview of the data mining lifecycle. The lifecycle model consists of six phases with arrows indicating the most important and frequent dependencies between phases. The sequence of the phases is not strict. In fact, most projects move back and forth between phases as necessary. It starts with business understanding, and then moves to data understanding, data preparation, modelling, evaluation, and deployment. The CRISP-DM model is flexible and can be customized easily.\n## Buissness Understanding\n\n    Tasks:\n\n    1.Determine business objectives\n\n    2.Assess situation\n\n    3.Determine data mining goals\n\n    4.Produce project plan\n\n## Data Understanding\n     Tasks:\n\n    1.Collect data\n\n    2.Describe data\n\n    3.Explore data    \n\n## Data Preparation\n    \n    Tasks:\n    \n    1.Data selection\n\n    2.Data preprocessing\n\n    3.Feature engineering\n\n    4.Dimensionality reduction\n\n            Steps:\n\n            Data cleaning\n\n            Data integration\n\n            Data sampling\n\n            Data dimensionality reduction\n\n            Data formatting\n\n            Data transformation\n\n            Scaling\n\n            Aggregation\n\n            Decomposition\n\n## Data Modeling :\n\nModeling is the part of the Cross-Industry Standard Process for Data Mining (CRISP-DM) process model that i like best. Our data is already in good shape, and now we can search for useful patterns in our data.\n\n   Tasks:\n    \n    1. Select modeling technique Select technique\n\n    2. Generate test design\n\n    3. Build model\n\n    4. Assess model\n\n## Data Evaluation :\n    \n    Tasks:\n\n    1.Evaluate Result\n\n    2.Review Process\n\n    3.Determine next steps\n\n<a id=1><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\">\n<center>Buissness Understanding<\/center><\/h3>\n\n\nThere may be two types of questions:\n\n**A.Technical Questions:**\n  \nCan ML be a solution to the problem?\n\n    \n                Do we have THE data?\n                Do we have all necessary related data?\n                Is there enough amount of data to develop algorithm?\n                Is data collected in the right way?\n                Is data saved in the right format?\n                Is the access to information guaranteed?\n\nCan we satisfy all the Business Questions by means of ML?\n\n**B.Business Questions:**\n    \nWhat are the organization's business goals?\n    \n                To reduce cost and increase revenue? \n                To increase efficiencies?\n                To avoid risks? To improve quality?\n    \nIs it worth to develop ML?\n    \n                In short term? In long term?\n                What are the success metrics?\n                Can we handle the risk if the project is unsuccessful?\n    \nDo we have the resources?\n    \n                Do we have enough time to develop ML?\n                Do we have a right talented team?\n\nThe goal of this project  is to build a model that borrowers can use to help make the best financial decisions.\n\nHistorical data are provided on 250,000 borrowers.https:\/\/www.kaggle.com\/c\/GiveMeSomeCredit\n\n**Data Dictionary**\n\n**SeriousDlqin2yrs:** Person experienced 90 days past due delinquency or worse.\n\n**RevolvingUtilizationOfUnsecuredLines:** Total balance on credit cards and personal lines of credit except real estate and no installment debt like car loans divided by the sum of credit limits.\n\n**age:** Age of borrower in years.\n\n**NumberOfTime30-59DaysPastDueNotWorse:** Number of times borrower has been 30-59 days past due but no worse in the last 2 years.\n\n**DebtRatio:** Monthly debt payments, alimony,living costs divided by monthy gross income.\n\n**MonthlyIncome:** Monthly income.\n\n**NumberOfOpenCreditLinesAndLoans:** Number of Open loans (installment like car loan or mortgage) and Lines of credit (e.g. credit cards).\n\n**NumberOfTimes90DaysLate:** Number of times borrower has been 90 days or more past due.\n\n**NumberRealEstateLoansOrLines:** Number of mortgage and real estate loans including home equity lines of credit.\n\n**NumberOfTime60-89DaysPastDueNotWorse:** Number of times borrower has been 60-89 days past due but no worse in the last 2 years.\n\n**NumberOfDependents:** Number of dependents in family excluding themselves (spouse, children etc.).\n    \n    \n We are not looking for a winning solution but more for:\n\n- how you approach the problem\n\n- how do you look at the data\n\n- what do you look at\n\n- how do you structure your projects\/prototypes.\n\n- If you chose to build a simple classifier which one did you chose, why etc.\n\n    \n**summary:**\nwe are expecting the following:\n    \n- 1.Methodology: method for engineering our solution \n\n\n        a.CRISP_DM\n        \n    \n- 2. Model\n\n\n        a.We are expecting a machine learning model that can correctly classify financial decisions.\n\n\n**What is the objective of the machine learning model?**\n\nWe aim to predict Credit scoring algorithms, which make a guess at the probability of default, are the method banks use to determine whether or not a loan should be granted. We will evaluate model performance with the:\n\n   - F beta score\n    \n   - ROC AUC score\n    \n   - PR AUC score | Average precision\n    \n    \n## Step 1: Import helpful libraries","a1c31806":"From the plots above, we see that approximately 12 features are needed to explain 90% of the variance in the dataset.\nThis gives us a good intuition of the number of features required in our model.\n\nNext, We will next use RFE to which are the important features for modelling.","b0651c1d":"# Duplicates Data ","204bdf17":"<a id=6><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\">\n<center>Deploy<\/center><\/h3>\n\nThe deployment of machine learning models is the process for making models available in production environments, where they can provide predictions to other software systems.\n\n\u25cfOne of the last stages in the Machine Learning Lifecycle.\n\n\u25cfPotentially the most challenging stage.\n\n\u25cfChallenges of traditional software\n\noReliability\noReusability\noMaintainability\noFlexibility\n\n\u25cfAdditional challenges specific to Machine Learning\n\noReproducibility\n\nNeeds coordination of data scientists, IT teams, software developers and business professionals:\n\noEnsure model works reliably\noEnsure model delivers the intended result.\n\n\u25cfPotential discrepancy between programming language in which the model is developed and the production system language.\n\noRe-coding the model extends the project timeline and risks lack of reproducibility\n\nWhy is Model Deployment important?\n\n\u25cfTo start using a Machine Learning Model, it needs to be effectively deployed into production, so that they can provide predictions to other software systems.\n\n\u25cfTo maximize the value of the Machine Learning Model, we need to be able to reliably extract the predictions and share them with other systems.\n\n\n**Research Environment**\n\n\u25cfThe Research Environment is a setting with tools, programs and software suitable for data analysis and the development of machine learning models.\n\n\u25cfHere, we develop the Machine Learning Models and identify their value.\nIts done by a data scientist : i prefer work on jupyter for this phase .\n\n**Production Environment**\n\n\u25cfThe Production Environment is a real-time setting with running programs and hardware setups that allow the organization\u2019s daily operations.\n\n\u25cfIt\u2019s the place where the machine learning models is actually available for business use.\n\n\u25cfIt allows organisations to show clients a \u201clive\u201d service.\nThis job is done by solid sofware+ml engineer+ devops team\n\n\n\nwe have 4 ways to deploy models .\nML System Architectures:\n1. Model embedded in application\n\n2. Served via a dedicated service\n\n3. Model published as data(streaming)\n\n4. Batch prediction (offline process)\n\n<a id=7><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\">\n<center>Summary<\/center><\/h3> \n\nWe had developed end-to-end machine learning using the CRISP_DM methodology. Work still in progress. Always keep in mind that the data science \/ ML project must be done as a team and iteratively in order to properly exploit our data and add value to our business. Also keep in mind that AI helps you make the decision by using the added value extracted from the data but not the accountability. So we have to keep in mind to always use a composite AI in order to make the final decision.\n\nReferences :\n\nhttps:\/\/developer.nvidia.com\/blog\/leveraging-machine-learning-to-detect-fraud-tips-to-developing-a-winning-kaggle-solution\/\n\npython guidline : \n\nhttps:\/\/gist.github.com\/sloria\/7001839\n\nfeatures  selections :\n\nhttps:\/\/www.kaggle.com\/sz8416\/6-ways-for-feature-selection\n\nhttps:\/\/pub.towardsai.net\/feature-selection-and-removing-in-machine-learning-dd3726f5865c\n\nhttps:\/\/www.kaggle.com\/bannourchaker\/1-featuresengineer-selectionpart1?scriptVersionId=72906910\n\nCripspdm :\nhttps:\/\/www.kaggle.com\/bannourchaker\/4-featureengineer-featuresselectionpart4?scriptVersionId=73374083\n\nQuanrile transformer : \n\nhttps:\/\/machinelearningmastery.com\/quantile-transforms-for-machine-learning\/\n\nBest link for all : \n\nhttps:\/\/neptune.ai\/blog\/tabular-data-binary-classification-tips-and-tricks-from-5-kaggle-competitions\n\ncomplete guide Stacking :\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2021\/08\/ensemble-stacking-for-machine-learning-and-deep-learning\/\n\nhttps:\/\/neptune.ai\/blog\/ensemble-learning-guide\n\nhttps:\/\/www.kaggle.com\/prashant111\/adaboost-classifier-tutorial\n\n\nMissing : \n\nhttps:\/\/www.kaggle.com\/dansbecker\/handling-missing-values\n\nBinning : \n\nhttps:\/\/heartbeat.fritz.ai\/hands-on-with-feature-engineering-techniques-variable-discretization-7deb6a5c6e27\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2020\/10\/getting-started-with-feature-engineering\/\n\nCat :\n\nhttps:\/\/innovation.alteryx.com\/encode-smarter\/\n\nhttps:\/\/github.com\/alteryx\/categorical_encoding\/blob\/main\/guides\/notebooks\/categorical-encoding-guide.ipynb\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2020\/08\/types-of-categorical-data-encoding\/\n\nhttps:\/\/maxhalford.github.io\/blog\/target-encoding\/\n\n\nChoice of kmeans : \n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2021\/05\/k-mean-getting-the-optimal-number-of-clusters\/\n\nImputation : \n\nhttps:\/\/machinelearningmastery.com\/knn-imputation-for-missing-values-in-machine-learning\/\n\nhttps:\/\/machinelearningmastery.com\/iterative-imputation-for-missing-values-in-machine-learning\/\n\nChoice of  roc vs precssion_recall : \n\nhttps:\/\/machinelearningmastery.com\/roc-curves-and-precision-recall-curves-for-classification-in-python\/\n\n\nhttps:\/\/machinelearningmastery.com\/roc-curves-and-precision-recall-curves-for-classification-in-python\/\n\n\nHow to tune for he futur work : \n\nhttps:\/\/www.kaggle.com\/hamidrezabakhtaki\/xgboost-catboost-lighgbm-optuna-final-submission\n\nhttps:\/\/www.kaggle.com\/bextuychiev\/lgbm-optuna-hyperparameter-tuning-w-understanding\n\n\n\nDeploy:\n\nhttps:\/\/towardsdatascience.com\/from-jupyter-notebook-to-deployment-a-straightforward-example-1838c203a437\n\n https:\/\/github.com\/DeepSparkChaker\/Titanic_Deep_Spark\/blob\/main\/app.py\nhttps:\/\/github.com\/Kunal-Varma\/Deployment-of-ML-model-using-FASTAPI\/tree\/2cc0319abbec469010a5139f460004f2a75a7482\nhttps:\/\/realpython.com\/fastapi-python-web-apis\/\n https:\/\/github.com\/tiangolo\/fastapi\/issues\/3373\n https:\/\/www.freecodecamp.org\/news\/data-science-and-machine-learning-project-house-prices\/\nhttps:\/\/github.com\/tiangolo\/fastapi\/issues\/1616\nhttps:\/\/stackoverflow.com\/questions\/68244582\/display-dataframe-as-fastapi-output\nhttps:\/\/www.kaggle.com\/sakshigoyal7\/credit-card-customers\nhttps:\/\/github.com\/renanmouraf\/data-science-house-prices    \nhttps:\/\/towardsdatascience.com\/data-science-quick-tips-012-creating-a-machine-learning-inference-api-with-fastapi-bb6bcd0e6b01\nhttps:\/\/towardsdatascience.com\/how-to-build-and-deploy-a-machine-learning-model-with-fastapi-64c505213857\nhttps:\/\/analyticsindiamag.com\/complete-hands-on-guide-to-fastapi-with-machine-learning-deployment\/\n\nhttps:\/\/github.com\/shaz13\/katana\/blob\/develop\/Dockerfile\n\n\nhttps:\/\/github.com\/TripathiAshutosh\/FastAPI\/blob\/main\/main.py\n\nBest practices : \n    \nhttps:\/\/theaisummer.com\/best-practices-deep-learning-code\/    \nhttps:\/\/github.com\/The-AI-Summer\/Deep-Learning-In-Production\/tree\/master\/2.%20Writing%20Deep%20Learning%20code:%20Best%20Practises\n\n Docker :\n \n https:\/\/towardsdatascience.com\/docker-in-pieces-353525ec39b0?fbclid=IwAR102sks2L0vRTde2qz1g4I4NhqXxnoqfV4IFzmZke4DvGcuiuYhj25eVSY\n \nhttps:\/\/github.com\/dkhundley\/ds-quick-tips\/blob\/master\/012_dockerizing_fastapi\/Dockerfile\n\n\n Deploy + scaling :\nhttps:\/\/towardsdatascience.com\/deploying-ml-models-in-production-with-fastapi-and-celery-7063e539a5db\nhttps:\/\/github.com\/jonathanreadshaw\/ServingMLFastCelery\n\nhttps:\/\/github.com\/trainindata\/deploying-machine-learning-models\/blob\/aaeb3e65d0a58ad583289aaa39b089f11d06a4eb\/section-04-research-and-development\/07-feature-engineering-pipeline.ipynb\n\nMl OPS : \nhttps:\/\/www.linkedin.com\/posts\/vipulppatel_getting-started-with-mlops-21-page-tutorial-activity-6863895411837415424-dWMh\/?fbclid=IwAR3Y4clbzujS_s2FFWg3tTYMKaGhh3vo25NUyoVdKHAJ7zynmCTNtzlHQ4M\n\nhttps:\/\/towardsai.net\/p\/machine-learning\/mlops-demystified?utm_source=twitter&utm_medium=social&utm_campaign=rop-content-recycle&fbclid=IwAR3MimsSXCFq3GqiLKoaQqXbeb3bkSwKhSkfQSKT_c1gsHDMGSBAv63s7Po\nhttps:\/\/www.youtube.com\/watch?v=9I8X-3HIErc\n\nhttps:\/\/pub.towardsai.net\/deployment-ml-ops-guide-series-2-69d4a13b0dcf\n\nPublish to medium : \n\nhttps:\/\/towardsai.net\/p\/data-science\/how-to-publish-a-jupyter-notebook-as-a-medium-blogpost?utm_source=twitter&utm_medium=social&utm_campaign=rop-content-recycle&fbclid=IwAR2-an7kknO3bsI5xjRdjL3jiwuPy7MBN5lVBc6fzx15mGY2iLS5KndCYWc\n\ncredit risk:\n\nhttps:\/\/app.datacamp.com\/workspace\/w\/cf77a77b-3dc4-498c-a9b7-0bdcd6090dbe#42-feature-importance---recursive-feature-elimination\n\nhttps:\/\/app.datacamp.com\/workspace\/w\/929aaba3-7b63-41d8-ac1c-6940ace94fd9\n\nhttps:\/\/machinelearningmastery.com\/imbalanced-classification-of-good-and-bad-credit\/\n\n","058ea1c1":"We have a so imb Data Set , we should keep this in  mind when we begin modeling \n\nFrom the above, we see huge class imbalance in the target variable. i.e., Only approximately 15% of the loans are not fully paid.\nThus, we must take care of this in the modelling (e.g., apply re-sampling technique) and ensure that huge class imbalance do not negatively impact our modelling performance.\n\nAt this juncture, we argue that recall is the most important performance metric for our modelling purpose.\nThis is because if a loan is not fully paid, the lender will incur high default risk. Thus, it is more important to have a good recall (low FN) instead of a high precision (low FP). In cases of FN, the lender will lose huge amount of monies due to default and this is highly undesirable. On the other hand, in cases of FP, the lender loses only potential revenue (at the expense of lower default risk) which we believe is comparatively acceptable.\n# Violin plot of target with features","f52bcdea":"## Define the model features and target\n\n### Extract X and y ","35b9e6ab":"<a id=3><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\">\n<center>Data Preparation<\/center><\/h3>\n\n## Data preprocessing\n\nData preprocessing comes after you've cleaned up your data and after you've done some exploratory analysis to understand your dataset. Once you understand your dataset, you'll probably have some idea about how you want to model your data. Machine learning models in Python require numerical input, so if your dataset has categorical variables, you'll need to transform them. Think of data preprocessing as a prerequisite for modeling.\n\n","c0ecc1f4":"The ranges of each feature seem to be within the expected ranges, except for revol_bal: this range is from 0 to 3008750.0  milions! Let's examine this feature in more detail by visually showing its distribution with a boxplot:","33631829":"We go from highly skewed to moderated data ","a8121b00":"# KDE plot of target with features","44858ed6":"# Stat Analysis: shape , mean,median...","fca7a44f":"**Zooming on the correlation between numerical variables and target.**","4c477387":"\n<a id=2><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\">\n<center>Data Understanding<\/center><\/h3>\n\n\n### Explore the data\/Analysis \n\nWe will analyse the following:\n\n    The target variable\n    \n    Variable types (categorical and numerical)\n    \n    Numerical variables\n        Discrete\n        Continuous\n        Distributions\n        Transformations\n\n    Categorical variables\n        Cardinality\n        Rare Labels\n        Special mappings\n\n    Null Data\n\n    Text data \n    \n    wich columns will we use\n    \n    IS there outliers that can destory our algo\n    \n    IS there diffrent range of data\n    \n    Curse of dimm...","4fa2abc0":"# General View ","bf56a67f":"Identifying outliers in data is an important part of statistical analyses.  One\nsimple rule of thumb (due to John Tukey) for finding outliers is based on the\nquartiles of the data: the first quartile $Q_1$ is the value $\\geq 1\/4$ of the\ndata, the second quartile $Q_2$ or the median is the value $\\geq 1\/2$ of the\ndata, and the third quartile $Q_3$ is the value $\\geq 3\/4$ of the data.  The\ninterquartile range, $IQR$, is $Q_3 - Q_1$.  \n\nTukey's rule says that the outliers are values more than $1.5$ times the interquartile range from the quartiles --- either below $Q_1 - 1.5 IQR$, or above $Q_3 + 1.5 IQR$.\n\n**(76854, 12)**  this is the shape of data keeped after eliminating outliers ","2503425d":"As we see data become more normal \n\n### Kurtosis \n\n**Describe:**\n\n\u00b7 Kurtosis is one of the two measures that quantify shape of a distribution. **kutosis determine the volume of the outlier**\n\nKurtosis is a measure of whether the data are heavy-tailed or light-tailed relative to a normal distribution. That is, data sets with high kurtosis tend to have heavy tails, or outliers. \n\n\u00b7 Kurtosis describes the peakedness of the distribution.\n\n\u00b7 If the distribution is tall and thin it is called a leptokurtic distribution(Kurtosis > 3). Values in a leptokurtic distribution are near the mean or at the extremes.\n\n\u00b7 A flat distribution where the values are moderately spread out (i.e., unlike leptokurtic) is called platykurtic(Kurtosis <3) distribution.\n\n\u00b7 A distribution whose shape is in between a leptokurtic distribution and a platykurtic distribution is called a mesokurtic(Kurtosis=3) distribution. A mesokurtic distribution looks more close to a normal distribution.\n\n\u00b7 Kurtosis is sometimes reported as \u201cexcess kurtosis.\u201d Excess kurtosis is determined by subtracting 3 from the kurtosis. This makes the normal distribution kurtosis equal 0.\n\n**Important Notes:**\n\n\u00b7 Along with skewness, kurtosis is an important descriptive statistic of data distribution. However, the two concepts must not be confused with each other. Skewness essentially measures the symmetry of the distribution, while kurtosis determines the heaviness of the distribution tails.\n\n\u00b7 It is the sharpness of the peak of a frequency-distribution curve .It is actually the measure of outliers present in the distribution.\n\n\u00b7 **High kurtosis** in a data set is an indicator that **data has heavy outliers**.\n\n\u00b7 **Low kurtosis** in a data set is an indicator that **data has lack of outliers.**\n\n    The kurtosis of a normal distribution is 3.\n    If a given distribution has a kurtosis less than 3, it is said to be playkurtic, which means it tends to produce fewer and less extreme outliers than the normal distribution.\n    If a given distribution has a kurtosis greater than 3, it is said to be leptokurtic, which means it tends to produce more outliers than the normal distribution.","a8f55b20":"Our Data is Highly skewed , we should correct it before modeling ","fdcbe7b1":"As we  see again data preprocessing is really so important to correct our data , i will do this step  on depth  in the next part . \n\n### Variance : \nFeatures with low variance should be eliminated","1bbec39d":"OUTLIERS\nOutliers might skew aggregations and create bias in the training model. The dataset does not have many features (columns) so we can check the min & max of each feature and locate outliers. For example, for the binary features we expect values of 0 minimum and 1 maximum.","86252660":"MISSING VALUES & IMPUTATION\nMissing values might create errors in the analysis. From the table above, we can see that there are 2 missing values.They should be imputed .","cf402cbe":"\n\nIt's clear tat there is a clear relation betweenamount and step  variables and target.\n\nNow Exploring correlation between all numerical variables. First we get a correlation grid of all numercial variables and target\n","44a89671":"### Quantile data :\n\n\n\n","44921703":"<a id=5><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\">\n<center>Evaluation<\/center><\/h3>\n\n# Model accuracy scoring\n\nThe easiest way to analyze performance is with accuracy. \nIt measures how many observations, both positive and negative, were correctly classified.\n\n\nYou shouldn\u2019t use accuracy on imbalanced problems. Then, it is easy to get a high accuracy score by simply classifying all observations as the majority class. For example in our case, by classifying all transactions as non-fraudulent we can get an accuracy of over 0.9.\n\n**When to use it:**\n\n    When your problem is balanced using accuracy is usually a good start. An additional benefit is that it is really easy to explain it to non-technical stakeholders in your project,\n    When every class is equally important to you.\n\n# Confusion Matrix\n\n**How to compute:**\n\nIt is a common way of presenting true positive (tp), true negative (tn), false positive (fp) and false negative (fn) predictions. Those values are presented in the form of a matrix where the Y-axis shows the true classes while the X-axis shows the predicted classes.\n\nIt is calculated on class predictions, which means the outputs from your model need to be thresholded first.\n\n**When to use it:**\n\n    Pretty much always. I like to see the nominal values rather than normalized to get a feeling on how the model is doing on different, often imbalanced, classes.\n\n\n\n# ROC Curve\n\n\nIt is a chart that visualizes the tradeoff between true positive rate (TPR) and false positive rate (FPR). Basically, for every threshold, we calculate TPR and FPR and plot it on one chart.\n\nOf course, the higher TPR and the lower FPR is for each threshold the better and so classifiers that have curves that are more top-left side are better.\n\nSince we have an imbalanced data set, Receiver Operating Characteristic Curves are not that useful although it's an expected output of most binary classifiers.\nBecause you can generate a pretty good-looking curve by just simply guessing each one is the non-fraud case.\n\n**When to use it:**\n\n    You should use it when you ultimately care about ranking predictions and not necessarily about outputting well-calibrated probabilities (read this article by Jason Brownlee if you want to learn about probability calibration).\n    You should not use it when your data is heavily imbalanced. It was discussed extensively in this article by Takaya Saito and Marc Rehmsmeier. The intuition is the following: false positive rate for highly imbalanced datasets is pulled down due to a large number of true negatives.\n    You should use it when you care equally about positive and negative classes.. If we care about true negatives as much as we care about true positives then it totally makes sense to use ROC AUC.\n    \n# ROC AUC score   \nAUC - ROC curve is a performance measurement for the classification problems at various threshold settings. ROC is a probability curve and AUC represents the degree or measure of separability. It tells how much the model is capable of distinguishing between classes. Higher the AUC, the better the model is at predicting 0 classes as 0 and 1 classes as 1. By analogy, the Higher the AUC, the better the model is at distinguishing between patients with the disease and no disease. The ROC curve is plotted with TPR against the FPR where TPR is on the y-axis and FPR is on the x-axis.\n\n**When to use it:**\n\n    You should use it when you ultimately care about ranking predictions and not necessarily about outputting well-calibrated probabilities (read this article by Jason Brownlee if you want to learn about probability calibration).\n    You should not use it when your data is heavily imbalanced. It was discussed extensively in this article by Takaya Saito and Marc Rehmsmeier. The intuition is the following: false positive rate for highly imbalanced datasets is pulled down due to a large number of true negatives.\n    You should use it when you care equally about positive and negative classes. It naturally extends the imbalanced data discussion from the last section. If we care about true negatives as much as we care about true positives then it totally makes sense to use ROC AUC.\n\n# Recall    \nIt measures how many observations out of all positive observations have we classified as positive. It tells us how many fraudulent transactions we recalled from all fraudulent transactions.\ntrue positive rate\n\nWhen you are optimizing recall you want to put all guilty in prison.\n**When to use it:**\n\n    Usually, you will not use it alone but rather coupled with other metrics like precision.\n    That being said, recall is a go-to metric, when you really care about catching all fraudulent transactions even at a cost of false alerts. Potentially it is cheap for you to process those alerts and very expensive when the transaction goes unseen.\n    \n# Precision\n\nIt measures how many observations predicted as positive are in fact positive. Taking our fraud detection example, it tells us what is the ratio of transactions correctly classified as fraudulent.\npositive predictive value\n\nWhen you are optimizing precision you want to make sure that people that you put in prison are guilty. \n\n**When to use it:**\n\n    Again, it usually doesn\u2019t make sense to use it alone but rather coupled with other metrics like recall.\n    When raising false alerts is costly, when you want all the positive predictions to be worth looking at you should optimize for precision.\n    \n\n\n**Precision vs. Recall for Imbalanced Classification:**\n\nYou may decide to use precision or recall on your imbalanced classification problem.\n\nMaximizing precision will minimize the number false positives, whereas maximizing the recall will minimize the number of false negatives.\n\n    Precision: Appropriate when minimizing false positives is the focus.\n    Recall: Appropriate when minimizing false negatives is the focus.\n\nSometimes, we want excellent predictions of the positive class. We want high precision and high recall.\n\nThis can be challenging, as often increases in recall often come at the expense of decreases in precision.\n\n    In imbalanced datasets, the goal is to improve recall without hurting precision. These goals, however, are often conflicting, since in order to increase the TP for the minority class, the number of FP is also often increased, resulting in reduced precision.\n    \n    \n# PR AUC score | Average precision\n\nSimilarly to ROC AUC score you can calculate the Area Under the Precision-Recall Curve to get one number that describes model performance.\n\nYou can also think about PR AUC as the average of precision scores calculated for each recall threshold [0.0, 1.0]. You can also adjust this definition to suit your business needs by choosing\/clipping recall thresholds if needed.\n\n**When to use it:**\n\n    when you want to communicate precision\/recall decision to other stakeholders\n    when you want to choose the threshold that fits the business problem.\n    when your data is heavily imbalanced. As mentioned before, it was discussed extensively in this article by Takaya Saito and Marc Rehmsmeier. The intuition is the following: since PR AUC focuses mainly on the positive class (PPV and TPR) it cares less about the frequent negative class.\n    when you care more about positive than negative class. If you care more about the positive class and hence PPV and TPR you should go with Precision-Recall curve and PR AUC (average precision).\n    \n# F beta score\n\nSimply put, it combines precision and recall into one metric. The higher the score the better our model is. You can calculate it in the following way:\n\n\n\n\n\nWhen choosing beta in your F-beta score the more you care about recall over precision the higher beta you should choose. For example, with F1 score we care equally about recall and precision with F2 score, recall is twice as important to us.\nF beta by beta\n\nWith 0<beta<1 we care more about precision and so the higher the threshold the higher the F beta score. When beta>1 our optimal threshold moves toward lower thresholds and with beta=1 it is somewhere in the middle.  \n\n**When to use it:**\n\n    Pretty much in every binary classification problem. It is my go-to metric when working on those problems. It can be easily explained to business stakeholders.\n    \n for more details see this article:[https:\/\/neptune.ai\/blog\/evaluation-metrics-binary-classification](http:\/\/)    \n \n==>Complete evaluation will be done when we train the model on all data that we have and with the best tuned model.","ce643ada":"## Outlier Identification\n### Skewness : \n\nSkewness is computed for each row or each column of the data present in the DataFrame object.\n\n\nSkewness is a measure of symmetry, or more precisely, the lack of symmetry. A distribution, or data set, is symmetric if it looks the same to the left and right of the center point. \n\n**Important Notes:**\n\n\u00b7 If the skewness is between **-0.5 and 0.5**, the data are **fairly symmetrical**\n\n\u00b7 If the skewness is between **-1 and \u2014 0.5** or between **0.5 and 1**, the **data are moderately skewed**\n\n\u00b7 If the skewness is **less than -1 or greater than 1**, the data are **highly skewed**","cfe05eb7":"# Visual Exploratory","f39cd49d":"# Reduce Memory: \nBe aware for high resolution features don't use it ","37d2c220":"FEATURES\nFrom the introduction above we know what features are available and their types. For convenience we can organize the features of the dataset in useful groups:\n\nNUMERIC features containing numeric data\nBINARY features containing binary data (0,1)\nCATEGORICAL features with categorical values\nLOAN features related to the loan itself\nPERSON features related to the person getting the loan\nTARGET the target feature for training the model","676eb036":"<a id=4><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\">\n<center>Modeling<\/center><\/h3>\n\n\n\nModeling is the part of the Cross-Industry Standard Process for Data Mining (CRISP-DM) process model that i like best. Our data is already in good shape, and now we can search for useful patterns in our data.\n\nTasks\n\n    Select modeling technique Select technique\n\n    Generate test design\n\n    Build model\n\n    Assess model\n","b4ff6259":"From the PCA plot, we can see distinct clusters between loans fully paid (orange) vs loans which are not (blue).\nWe will study next the number of features required to explain the variance in the dataset to gain an intuition of the number of features required for our modelling purpose.","d07dabe7":"\n# t-SNE visualization of high-dimensional data\n\nt-SNE intuition t-SNE is super powerful, but do you know exactly when to use it? When you want to visually explore the patterns in a high dimensional dataset.\n","4893e37f":"\n# Numerical features distribution\n## Histograms of numerical features"}}