{"cell_type":{"7ffa05e3":"code","dcb58568":"code","da5b5083":"code","8fb35476":"code","327fdedc":"code","598072ec":"code","47f61a01":"code","aca6f569":"code","f2d8b32e":"code","e85c61d2":"code","a273edf1":"code","e6490309":"code","ac42b0a8":"code","ad09fdf9":"code","4f42b36d":"code","e86da141":"code","b7bb5933":"code","100ec62f":"code","6eb5d033":"code","8fa66290":"code","b175d7f4":"code","be1f9543":"code","797d8fc9":"code","44eabcff":"code","42374776":"code","fedf384c":"code","d8422b05":"code","db781ee5":"code","1d6fc3d2":"code","de33a997":"code","8276c8ff":"code","91af2e4d":"code","a197f3ad":"code","e761a687":"code","b47389c2":"code","6121b2a9":"code","71a599c2":"code","594a52e5":"code","a384faf2":"code","ff28956e":"code","0140e3ab":"code","a41aaaac":"code","f4f1e6e1":"code","3d3d6211":"code","87918288":"code","36b19ad0":"code","ba91b1fe":"code","c1e25977":"code","1f23bbcc":"code","d29f3211":"code","96808b63":"code","e7b2f9ee":"code","11b50efd":"code","5e5a20c5":"code","885d37a5":"code","f5e2f5a2":"code","908c3f17":"code","f3e9e9f0":"code","c409ed26":"code","93d7c2c1":"code","562fb59e":"code","af395b1b":"code","12f31643":"code","75dd9055":"code","188b2ef0":"code","2910c989":"code","8e32a2e6":"markdown","cf43f976":"markdown","83069ea5":"markdown","778ce589":"markdown","256d32e8":"markdown","36af7d56":"markdown","dff17566":"markdown","c05a055f":"markdown","7eef9bea":"markdown","ee550cae":"markdown","b5a799c6":"markdown","6040d255":"markdown","c8bcd018":"markdown","85b451cf":"markdown","2b3a7f18":"markdown","1b297688":"markdown","166a20d9":"markdown","e4e3c351":"markdown","9c163695":"markdown","938b7cbe":"markdown","79e00aa6":"markdown","14d11be3":"markdown","6f77e387":"markdown","3ba01a7f":"markdown","22bd7862":"markdown","482ba91f":"markdown","914a8054":"markdown","94f619ce":"markdown","456effa5":"markdown","fb2be6a9":"markdown","c26f1dff":"markdown","7081ca76":"markdown","a71aef35":"markdown","3aa9aa57":"markdown","67761c77":"markdown","66abbc75":"markdown","d9d4dd93":"markdown","f35623ec":"markdown","c8ac6ca3":"markdown","73d6fad9":"markdown","f336a4f4":"markdown"},"source":{"7ffa05e3":"import re\nimport os\nimport nltk\nimport string\nimport warnings\n\nimport numpy as np\nimport pandas as pd\n\nfrom plotly import tools\nimport plotly.offline as pyo\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\n\nimport matplotlib.pyplot as plt\n\nfrom collections import Counter","dcb58568":"warnings.filterwarnings('ignore')\n\npyo.offline.init_notebook_mode(connected= True)","da5b5083":"df = pd.read_json('..\/input\/Sarcasm_Headlines_Dataset.json', lines= True)","8fb35476":"df.head()","327fdedc":"df.describe()","598072ec":"df.info()","47f61a01":"for col in df.columns:\n    print(sum(df.isnull()[col]))","aca6f569":"x = 'https:\/\/www.github.com'","f2d8b32e":"re.findall('\\w+', x)","e85c61d2":"df['source'] = df['article_link'].apply(lambda x: re.findall('\\w+', x)[2])","a273edf1":"df.head()","e6490309":"labels = ['Sarcasm', 'Not Sarcasm']\nvalues = [df['is_sarcastic'].sum(), len(df) - df['is_sarcastic'].sum()]\n\ntrace0 = go.Bar(x = [labels[1]], y = [values[1]], name = 'No Sarcasm')\ntrace1 = go.Bar(x = [labels[0]], y = [values[0]], marker= {'color': '#00cc66'} , name = 'Sarcasm')\n\n\ndata = [trace0, trace1]\n\nlayout = go.Layout(title = 'Number of Sarcastic Articles',\n                   width = 800,\n                   height = 500,\n                  yaxis= dict(title = 'Number of articles'),)\n\nfig = go.Figure(data, layout)\n\npyo.offline.iplot(fig)","ac42b0a8":"df.head(10)","ad09fdf9":"list(df['source'].value_counts())","4f42b36d":"trace0 = go.Bar(x = df['source'].unique(), y = list(df['source'].value_counts()) , marker = {'color': '#9900e6'}, name = 'Sarcasm')\n\ndata = [trace0]\n\nlayout = go.Layout(title = 'Sources of Articles',\n                   width = 800,\n                   height = 500,\n                  yaxis= dict(title = 'Number of articles'),)\n\nfig = go.Figure(data, layout)\n\npyo.offline.iplot(fig)","e86da141":"x = 'F*&!@#$%&*(U)+>\"{}C<>!@#$K%&*()+>\"{}<> T%&!@H#$%&*()+E>\"{}<S>!@#$%&*()E+>\"{}<> *&P(($UN#$$@!CT%$&UA&&*T()I$%@(O*&N!S'\nprint(x)\nprint('\\n'+re.sub('[^a-zA-z0-9\\s]','',x))","b7bb5933":"df['headlineNoPunc'] = df['headline'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]','',x))","100ec62f":"print(df['headlineNoPunc'][456])\ndf['headline'][456]","6eb5d033":"allWords_df = df['headlineNoPunc'].str.split(expand = True).unstack().value_counts()\nallWords_df = pd.DataFrame(allWords_df).reset_index()","8fa66290":"trace0 = go.Bar(x = allWords_df['index'][0:50] , y = allWords_df[0][0:50], marker = dict(color = allWords_df[0]\n                                                                                   , colorscale = 'Portland')\n                                                                                , name = 'Top 50 Frequent Headline Words')\n\ndata = [trace0]\n\nlayout = go.Layout(title = 'Most Frequent Words',\n                   width = 950,\n                   height = 600,\n                   xaxis = dict(title = 'Words', nticks = 50),\n                  yaxis= dict(title = 'Count'),)\n\nfig = go.Figure(data, layout)\n\npyo.offline.iplot(fig)","b175d7f4":"nltk.download('stopwords')","be1f9543":"from nltk.corpus import stopwords","797d8fc9":"word_counts = df['headlineNoPunc'].str.split(expand = True).unstack().value_counts()\nword_counts = pd.DataFrame(word_counts).reset_index()","44eabcff":"stopwords = stopwords.words('english') # let's save us some hassle","42374776":"noSarcasmWords_all = df[df['is_sarcastic'] == 0]['headlineNoPunc'].str.split(expand = True).unstack()\nsarcasmWords_all = df[df['is_sarcastic'] == 1]['headlineNoPunc'].str.split(expand = True).unstack()","fedf384c":"noSarcasmWords = [wo for wo in noSarcasmWords_all if wo not in stopwords]\nsarcasmWords = [wo for wo in sarcasmWords_all if wo not in stopwords]","d8422b05":"from collections import Counter","db781ee5":"noSarcasmWordCount = pd.DataFrame(list(Counter(noSarcasmWords).items()), columns= ['word','count'])\nsarcasmWordCount = pd.DataFrame(list(Counter(sarcasmWords).items()), columns= ['word','count'])","1d6fc3d2":"temp1 = noSarcasmWordCount.sort_values(by = ['count'], ascending= False)[1:31]\ntemp2 = sarcasmWordCount.sort_values(by = ['count'], ascending= False)[1:31]","de33a997":"trace0 = go.Bar(y = temp1['word'] , x = temp1['count'], marker = dict(color = '#ff3333'), opacity= 0.6, orientation = 'h' ,name = 'Frequent words in Non Sarcastic articles')\ntrace1 = go.Bar(y = temp2['word'] , x = temp2['count'], marker = dict(color = '#33cc33'), opacity= 0.6, orientation = 'h' ,name = 'Frequent words in Sarcastic articles')\n\nfig = tools.make_subplots(rows= 1, cols= 2)\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update(height=600, width=1200, title = 'Frequent words', yaxis = dict(nticks = 30))\n                     \npyo.offline.iplot(fig)","8276c8ff":"nltk.download('punkt')\nnltk.download('wordnet')","91af2e4d":"from nltk.stem import WordNetLemmatizer","a197f3ad":"wordnet_lemmatizer = WordNetLemmatizer()\n\nsentence = \"i am trying, he is frying, she learned shit, i mowed the lawn\"\n\nsentence_words = sentence.split()\n\nsentence_words = [re.sub('[^a-zA-z0-9\\s]','',x) for x in sentence_words]\nprint(sentence_words)\nsentence_words\nprint(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\nfor word in sentence_words:\n    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word, pos = 'v')))","e761a687":"noSarcasmWords_lemm = []\nfor word in noSarcasmWords:\n    if word != None:\n        noSarcasmWords_lemm.append(wordnet_lemmatizer.lemmatize(word, pos = 'v'))\n\nsarcasmWords_lemm = []\nfor word in sarcasmWords:\n    if word != None:\n        sarcasmWords_lemm.append(wordnet_lemmatizer.lemmatize(word, pos = 'v'))","b47389c2":"print('finding' in noSarcasmWords)\n\nprint('finding' in noSarcasmWords_lemm)\nprint('find' in noSarcasmWords_lemm)","6121b2a9":"from wordcloud import WordCloud","71a599c2":"sar_cloud1 = WordCloud(background_color= 'white', colormap= 'viridis').generate(\" \".join(noSarcasmWords_lemm))\nsar_cloud2 = WordCloud(background_color= 'white', colormap= 'viridis').generate(\" \".join(sarcasmWords_lemm))\n\n\nfig = plt.figure(figsize=(20,20))\n\nax1 = fig.add_subplot(1, 2, 1)\nax1.tick_params(length=0, width=0)\nax1.set_xticklabels([])\nax1.set_yticklabels([])\nax1.set_title(\"In Non Sarcastic Articles\")\nax1.imshow(sar_cloud1)\n\nax2 = fig.add_subplot(1, 2, 2)\nax2.tick_params(length=0, width=0)\nax2.set_xticklabels([])\nax2.set_yticklabels([])\nax2.set_title(\"In Sarcastic Articles\")\nax2.imshow(sar_cloud2)\n\nplt.show()","594a52e5":"bigrams = list(nltk.bigrams(sarcasmWords_lemm))\nbigram_counts = Counter(bigrams)\n\nbigram_name = []\nbigram_freq = []\nfor key in bigram_counts.keys():\n    if bigram_counts[key] > 1:\n        bigram_name.append(str(key))\n        bigram_freq.append(bigram_counts[key])\n        \nbigram_df = pd.DataFrame({'bigram_names': bigram_name, 'bigram_freq': bigram_freq})\n\n\n# our values in the first columns look like '('doctor', 'man')'\n# Let's clean it up to 'doctor man'\n\nbigram_df['bigram_names'] = [re.sub('[^a-zA-z0-9\\s]','',x) for x in bigram_df['bigram_names']]\n\nbigram_df.head()","a384faf2":"temp = bigram_df.sort_values(by = 'bigram_freq')[-20:]\ntrace0 = go.Bar(y = temp['bigram_names'] , x = temp['bigram_freq'], orientation= 'h', marker = dict(color = '#00997a')\n               , opacity = 0.6, showlegend = False)\n\nfig = tools.make_subplots(1,2, subplot_titles = ['in Sarcasm Articles','in Non Sarcasm Articles'] ,horizontal_spacing = 0.05, shared_xaxes = False)\nfig.append_trace(trace0, 1, 1)","ff28956e":"bigrams = list(nltk.bigrams(noSarcasmWords_lemm))\nbigram_counts = Counter(bigrams)\n\nbigram_name = []\nbigram_freq = []\nfor key in bigram_counts.keys():\n    if bigram_counts[key] > 1:\n        bigram_name.append(str(key))\n        bigram_freq.append(bigram_counts[key])\n        \nbigram_df = pd.DataFrame({'bigram_names': bigram_name, 'bigram_freq': bigram_freq})\n\nbigram_df['bigram_names'] = [re.sub('[^a-zA-z0-9\\s]','',x) for x in bigram_df['bigram_names']]\n\nbigram_df.head()","0140e3ab":"temp = bigram_df.sort_values(by = 'bigram_freq')[-20:]\ntrace1 = go.Bar(y = temp['bigram_names'] , x = temp['bigram_freq'], orientation= 'h', marker = dict(color = '#00e6b8')\n                , showlegend = False, opacity = 0.6)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update( height=600, width=1100, title = 'Bigrams', yaxis = dict(nticks = 20))\npyo.offline.iplot(fig)","a41aaaac":"# Finding the number of words in the headline\ndf['num_words'] = df['headlineNoPunc'].apply(lambda x: len(x.split()))\n\n# the unique number of words in the headline\ndf['unique_num_words'] = df['headlineNoPunc'].apply(lambda x: len(set(x.split())))\n\n# the number of characters in the headline\ndf['num_chars'] = df['headline'].apply(lambda x: len(x))\n\n# number of genuine words with no stopwords  \ndf['num_words_nostop'] = df['headlineNoPunc'].apply(lambda xx: len([x for x in xx.split() if x not in stopwords]))\n\n# number of stopwords\ndf['num_stop'] = df['num_words'] - df['num_words_nostop']\n\n# number of punctuations\ndf['num_punc'] = df['headline'].apply(lambda xx: len([x for x in xx if x in string.punctuation]))","f4f1e6e1":"trace0 = go.Box(y = df[df['is_sarcastic'] == 0]['num_words'], name = 'Non Sarcasm')\ntrace1 = go.Box(y = df[df['is_sarcastic'] == 1]['num_words'], name = 'Sarcasm')\ndata = [trace0, trace1]\nlayout = go.Layout(title = 'Number of Words',\n                   width = 900,\n                   height = 600)\nfig = go.Figure(data, layout)\npyo.offline.iplot(fig)","3d3d6211":"trace0 = go.Box(y = df[df['is_sarcastic'] == 0]['unique_num_words'], name = 'Non Sarcasm')\ntrace1 = go.Box(y = df[df['is_sarcastic'] == 1]['unique_num_words'], name = 'Sarcasm')\ndata = [trace0, trace1]\nlayout = go.Layout(title = 'Number of Unique Words',\n                   width = 900,\n                   height = 600)\nfig = go.Figure(data, layout)\npyo.offline.iplot(fig)","87918288":"trace0 = go.Box(y = df[df['is_sarcastic'] == 0]['num_chars'], name = 'Non Sarcasm')\ntrace1 = go.Box(y = df[df['is_sarcastic'] == 1]['num_chars'], name = 'Sarcasm')\ndata = [trace0, trace1]\nlayout = go.Layout(title = 'Number of Characters',\n                   width = 900,\n                   height = 600)\nfig = go.Figure(data, layout)\npyo.offline.iplot(fig)","36b19ad0":"trace0 = go.Box(y = df[df['is_sarcastic'] == 0]['num_punc'], name = 'Non Sarcasm')\ntrace1 = go.Box(y = df[df['is_sarcastic'] == 1]['num_punc'], name = 'Sarcasm')\ndata = [trace0, trace1]\nlayout = go.Layout(title = 'Number of Punctuations',\n                   width = 900,\n                   height = 600)\nfig = go.Figure(data, layout)\npyo.offline.iplot(fig)","ba91b1fe":"from sklearn.preprocessing import LabelEncoder","c1e25977":"X = df['headlineNoPunc']\ny = df['is_sarcastic']","1f23bbcc":"enc = LabelEncoder()\ny = enc.fit_transform(y)\ny = y.reshape(-1,1)","d29f3211":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 101)","96808b63":"import keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence","e7b2f9ee":"max_words = 1000\nmax_len = 150\ntok = Tokenizer(num_words= max_words)\ntok.fit_on_texts(X_train)\nsequences = tok.texts_to_sequences(X_train)\nsequences_matrix = sequence.pad_sequences(sequences, max_len)","11b50efd":"import pickle\npickle.dump(tok,open('tokenizer.pkl','wb'))  ","5e5a20c5":"from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\nfrom keras.optimizers import RMSprop\nfrom keras.models import Model","885d37a5":"def RNN():\n    inputs = Input(name='inputs',shape=[max_len])\n    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n    layer = LSTM(64)(layer)\n    layer = Dense(256,name='FC1')(layer)\n    layer = Activation('relu')(layer)\n    layer = Dropout(0.2)(layer)\n    layer = Dense(1,name='out_layer')(layer)\n    layer = Activation('sigmoid')(layer)\n    model = Model(inputs=inputs,outputs=layer)\n    return model","f5e2f5a2":"model = RNN()\nmodel.summary()\nmodel.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])","908c3f17":"checkpoint_path = \"model.ckpt\"\ncheckpoint_dir = os.path.dirname(checkpoint_path)\n\n# Create checkpoint callback\ncp_callback = keras.callbacks.ModelCheckpoint(checkpoint_path,\n                                                 save_weights_only=True,\n                                                 verbose=1)","f3e9e9f0":"from keras.callbacks import EarlyStopping\nmodel.fit(sequences_matrix,y_train,batch_size=100,epochs=3,\n          validation_split=0.1,callbacks=[cp_callback])","c409ed26":"pickle.dump(model,open('model.pkl','wb'))","93d7c2c1":"# model_2 = RNN()\n# model_2.summary()\n# model_2.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])","562fb59e":"# model_2.load_weights('model.ckpt')","af395b1b":"test_sequences = tok.texts_to_sequences(X_test)\ntest_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)","12f31643":"accr = model.evaluate(test_sequences_matrix,y_test)\nprint('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))","75dd9055":"model_loaded = pickle.load(open('model.pkl', 'rb'))\ntoken = pickle.load(open('tokenizer.pkl', 'rb'))","188b2ef0":"def detect_sarcasm(sent):\n    sent = re.sub('[^a-zA-z0-9\\s]','',sent)\n    inp_sent = pd.Series([sent])\n    seq = token.texts_to_sequences(inp_sent)\n    sequences_matrix = sequence.pad_sequences(seq, max_len)\n    res = model_loaded.predict(sequences_matrix)\n    if res[0][0] <= 0.5:\n        print(\"\\nNah! seems legit!\")\n    if res[0][0] > 0.5:\n        print(\"\\nThis shit is sarcastic bro!\")","2910c989":"detect_sarcasm(input())","8e32a2e6":"### Bi- grams","cf43f976":"### Testing User Input Headlines","83069ea5":"So it is pretty obvious that the most frequent words are the *stopwords*. So we better remove these from the data to get to the real stuff.","778ce589":"First of all let's explore how much share are there of sarcasm in the dataset and what kind of are they.","256d32e8":"# Sarcastic Headline Detection","36af7d56":"We will save the model checkpoint after every epoch so we don't lose progress. We can use it or if we want to use pickle if the model is finsihed one that 'll work fine too.","dff17566":"### Importing the Data","c05a055f":"### de nada!","7eef9bea":"#### Preprocessing the data","ee550cae":"### Predictive Modeling","b5a799c6":"Now we can use the above approach to lemmaize our both of the lists","6040d255":"___","c8bcd018":"See! We're good!","85b451cf":"If you use checkpoint rather than pickle then use following","2b3a7f18":"Saving the model as pickle( Not for Kaggle)","1b297688":"So there's peculiar observation that all the articles in the data from ***theonion*** are sarcasm while all others are from ***huffington*** and not sarcastic.  \n\nSeeing this observation we won't be using the sources column at all, it will be a biased model as in a way it is the target variable itself.","166a20d9":"separating out the word counts based on their target class","e4e3c351":"Now we do the same for non sarcastic articles and append its plot to the grid","9c163695":"___","938b7cbe":"#### Testing on the test set","79e00aa6":"loading the pickles","14d11be3":"\n[Prashant Brahmbhatt](https:\/\/github.com\/hashbanger)","6f77e387":"Seems like it worked","3ba01a7f":"### Wordclouds","22bd7862":"We can use the regular expression for finding the source of the article from the website","482ba91f":"#### Adding the source of the article to the data","914a8054":"#### Removing the punctuations from the headlines","94f619ce":"#### Creating the RNN","456effa5":"Now we get the counts of the words","fb2be6a9":"This looks fine now!","c26f1dff":"### Lemmatization","7081ca76":"Here we will add some meta features of the headlines","a71aef35":"___","3aa9aa57":"We save the tokenizer so we could use it when testing single user inputs and don't need to re fit ( Not for Kaggle)","67761c77":"So we have **no missing values**, that's good!","66abbc75":"### Adding new Features","d9d4dd93":"___","f35623ec":"Here we can see the Lemmatization of a sample sentence","c8ac6ca3":"#### Tokenizing the Sentences","73d6fad9":"___","f336a4f4":"___"}}