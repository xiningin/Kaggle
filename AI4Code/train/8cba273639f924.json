{"cell_type":{"9885cef5":"code","edca660a":"code","32c2b2b3":"code","3487ef76":"code","43c48f66":"code","b5d36ed1":"code","5599ef3b":"code","e45384cd":"code","2952cffd":"code","dbdba272":"code","ed72a321":"code","c8805d7e":"code","a78d2d32":"code","83498bc8":"code","8a06f717":"code","9ddac1b0":"markdown","735bdf77":"markdown"},"source":{"9885cef5":"import os\nimport pickle\n\nimport librosa\nimport numpy as np\nimport pandas as pd\nimport yaml\nfrom tqdm import tqdm\n\n\nimport math\nimport random\n\nimport numpy as np\nimport torch\nimport torch.utils.data\nimport torch.nn.functional as F\n\nfrom librosa.filters import mel as librosa_mel_fn\nfrom librosa.util import normalize\nfrom scipy.io.wavfile import read\n\nALLOWED_EXTS = [x.upper() for x in ['.wav', '.m4a', '.flac']]\ntop_DB_dict = {'VCTK': 15, 'VC1': 23, 'VC1T': 23, 'VC2': 23, 'Libri': 23}\nMAX_WAV_VALUE = 32768.0\n","edca660a":"hp = {\n    \"audio\": {\n        \"raw_vox\": \"..\/input\/voxceleb1-audio-wav-files-for-india-celebrity\/vox1_indian\/content\/vox_indian\",\n        \"meta_file\": \"..\/input\/voxceleb1-audio-wav-files-for-india-celebrity\/vox1_meta.csv\"\n    },\n    \n    \n    \"Sound\": {\n        \"N_FFT\": 1024,\n        \"Mel_Dim\": 80,\n        \"Frame_Length\": 1024,\n        \"Frame_Shift\": 256,\n        \"Sample_Rate\": 22050,\n        \"Mel_F_Min\": 0,\n        \"Mel_F_Max\": 8000,\n    },\n\n    \"GE2E\":{\n        \"Embedding_Size\": 256,\n        \"Positional_Encoding\": {\n            \"Max_Position\": 1024,\n            \"Dropout_Rate\": 0.1,\n            },\n        \n        \"Transformer\":{\n            \"Num_Layers\": 3,\n            \"Head\": 4,\n            \"Dropout_Rate\": 0.1,\n            }\n    },\n    \n    \"Training\": {\n        \"Use_Pattern_Cache\": False,\n        \"Train_Pattern\": {\n            \"Path\": \".\/train\/\",\n            \"Metadata_File\": \"train_metadata.pickle\",\n        },\n        \"Eval_Pattern\": {\n            \"Path\": '.\/eval\/',\n            \"Metadata_File\": 'eval_metadata.pickle',\n        },\n\n        # Each epoch is based on the number of speakers, so using a lot of workers slows down the pattern generating speed.\n        \"Num_Workers\": 2,\n        \"Batch\": {\n            \"Train\": {\n                \"Speaker\": 64,\n                \"audio_per_speaker\": 15,\n            },\n\n            \"Eval\": {\n                \"Speaker\": 64,\n                \"audio_per_speaker\": 40,\n            },\n\n        },\n\n        \"Frame_Length\": {\n            \"Min\": 140,\n            \"Max\": 180,\n        },\n\n        \"Inference\": {\n            \"Samples\": 5,\n            \"Frame_Length\": 64,\n            \"Overlap_Length\": 32,\n        },\n\n        \"Learning_Rate\": {\n            \"Initial\": 1.0e-3,\n            \"Epsilon\": 1.0e-7,\n            \"Decay_Step\": 10000,\n            \"Decay_Rate\": 0.5,\n        },\n\n        \"ADAM\": {\n            \"Beta1\": 0.9,\n            \"Beta2\": 0.999,\n            \"Epsilon\": 1.0e-8,\n        },\n\n        \"Gradient_Norm\": 1.0,\n        \"Max_Step\": 100000,\n        \"Checkpoint_Save_Interval\": 1000,\n        \"Logging_Interval\": 100,\n        \"Evaluation_Interval\": 1000,\n        \"Initial_Inference\": True,\n    },\n    \"Checkpoint_Path\": '',\n    \"Log_Path\": \"\",\n    \"Device\": '0',\n}","32c2b2b3":"class DictWithDotNotation(dict):\n    \"\"\"\n    a dictionary that supports dot notation\n    as well as dictionary access notation\n    usage: d = DotDict() or d = DotDict({'val1':'first'})\n    set attributes: d.val2 = 'second' or d['val2'] = 'second'\n    get attributes: d.val2 or d['val2']\n    \"\"\"\n    __getattr__ = dict.__getitem__\n    __setattr__ = dict.__setitem__\n    __delattr__ = dict.__delitem__\n\n    def __init__(self, dct=None):\n        dct = dict() if not dct else dct\n        for key, value in dct.items():\n            if hasattr(value, 'keys'):\n                value = DictWithDotNotation(value)\n            self[key] = value\n\n\nclass HyperParameters(DictWithDotNotation):\n\n    def __init__(self, hp_dict=None):\n        super(DictWithDotNotation, self).__init__()\n\n        hp_dotdict = DictWithDotNotation(hp_dict)\n        for k, v in hp_dotdict.items():\n            setattr(self, k, v)\n\n    __getattr__ = DictWithDotNotation.__getitem__\n\n# coverting hp dictionary to dot notation\nhp = HyperParameters(hp)\nhp.audio","3487ef76":"# #############################################################################\n# MIT License\n\n# Copyright (c) 2020 Jungil Kong\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and\/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n# #############################################################################\n\ndef load_wav(full_path):\n    \"\"\"\n        loads a audio file and returns audio and sampling rate\n    \"\"\"\n    sampling_rate, data = read(full_path)\n    return data, sampling_rate\n\n\ndef dynamic_range_compression(x, C=1, clip_val=1e-5):\n    \"\"\"\n        clips a data and returns a log of it.\n    \"\"\"\n    return np.log(np.clip(x, a_min=clip_val, a_max=None) * C)\n\n\ndef dynamic_range_decompression(x, C=1):\n    return np.exp(x) \/ C\n\n\ndef dynamic_range_compression_torch(x, C=1, clip_val=1e-5):\n    return torch.log(torch.clamp(x, min=clip_val) * C)\n\n\ndef dynamic_range_decompression_torch(x, C=1):\n    return torch.exp(x) \/ C\n\n\ndef spectral_normalize_torch(magnitudes):\n    \"\"\"\n        normalize the inputs\n    \"\"\"\n    output = dynamic_range_compression_torch(magnitudes)\n    return output\n\n\ndef spectral_de_normalize_torch(magnitudes):\n    \"\"\"\n        denormalize the input\n    \"\"\"\n    output = dynamic_range_decompression_torch(magnitudes)\n    return output\n\n\nmel_basis = {}\nhann_window = {}\n\n\ndef mel_spectrogram(y, n_fft, num_mels, sampling_rate, hop_size, win_size, fmin, fmax, center=False):\n    \"\"\"\n        takes y as input (np.array) and converts it into mel spectrogram\n        two great resources to learn about spectrograms\n        \n        https:\/\/towardsdatascience.com\/getting-to-know-the-mel-spectrogram-31bca3e2d9d0\n        \n        https:\/\/medium.com\/analytics-vidhya\/understanding-the-mel-spectrogram-fca2afa2ce53\n    \"\"\"\n    if torch.min(y) < -1.:\n        print('min value is ', torch.min(y))\n    if torch.max(y) > 1.:\n        print('max value is ', torch.max(y))\n\n    global mel_basis, hann_window\n    if fmax not in mel_basis:\n        mel = librosa_mel_fn(sampling_rate, n_fft, num_mels, fmin, fmax)\n        mel_basis[str(fmax) + '_' + str(y.device)] = torch.from_numpy(mel).float().to(y.device)\n        hann_window[str(y.device)] = torch.hann_window(win_size).to(y.device)\n\n    y = F.pad(y.unsqueeze(1), (int((n_fft - hop_size) \/ 2), int((n_fft - hop_size) \/ 2)), mode='reflect')\n    y = y.squeeze(1)\n\n    spec = torch.stft(y, n_fft, hop_length=hop_size, win_length=win_size, window=hann_window[str(y.device)],\n                      center=center, pad_mode='reflect', normalized=False, onesided=True)\n\n    spec = torch.sqrt(spec.pow(2).sum(-1) + (1e-9))\n\n    spec = torch.matmul(mel_basis[str(fmax) + '_' + str(y.device)], spec)\n    spec = spectral_normalize_torch(spec)\n\n    return spec\n\n\ndef spectrogram(y, n_fft, hop_size, win_size, center=False):\n    if torch.min(y) < -1.:\n        print('min value is ', torch.min(y))\n    if torch.max(y) > 1.:\n        print('max value is ', torch.max(y))\n\n    global hann_window\n    hann_window[str(y.device)] = torch.hann_window(win_size).to(y.device)\n\n    y = torch.nn.functional.pad(y.unsqueeze(1), (int((n_fft - hop_size) \/ 2), int((n_fft - hop_size) \/ 2)),\n                                mode='reflect')\n    y = y.squeeze(1)\n\n    spec = torch.stft(y, n_fft, hop_length=hop_size, win_length=win_size, window=hann_window[str(y.device)],\n                      center=center, pad_mode='reflect', normalized=False, onesided=True)\n\n    spec = torch.sqrt(spec.pow(2).sum(-1) + (1e-9))\n    spec = spectral_normalize_torch(spec)\n\n    return spec\n\n\ndef spec_energy(y, n_fft, hop_size, win_size, center=False):\n    if torch.min(y) < -1.:\n        print('min value is ', torch.min(y))\n    if torch.max(y) > 1.:\n        print('max value is ', torch.max(y))\n\n    global hann_window\n    hann_window[str(y.device)] = torch.hann_window(win_size).to(y.device)\n\n    y = torch.nn.functional.pad(y.unsqueeze(1), (int((n_fft - hop_size) \/ 2), int((n_fft - hop_size) \/ 2)),\n                                mode='reflect')\n    y = y.squeeze(1)\n\n    spec = torch.stft(y, n_fft, hop_length=hop_size, win_length=win_size, window=hann_window[str(y.device)],\n                      center=center, pad_mode='reflect', normalized=False, onesided=True)\n    spec = torch.sqrt(spec.pow(2).sum(-1) + (1e-9))\n    energy = torch.norm(spec, dim=1)\n\n    return energy\n\n\ndef get_dataset_filelist(a):\n    with open(a.input_training_file, 'r', encoding='utf-8') as fi:\n        training_files = [os.path.join(a.input_wavs_dir, x.split('|')[0] + '.wav')\n                          for x in fi.read().split('\\n') if len(x) > 0]\n\n    with open(a.input_validation_file, 'r', encoding='utf-8') as fi:\n        validation_files = [os.path.join(a.input_wavs_dir, x.split('|')[0] + '.wav')\n                            for x in fi.read().split('\\n') if len(x) > 0]\n    return training_files, validation_files","43c48f66":"def pattern_generate(path, n_fft: int, num_mels: int, sample_rate: int, hop_size: int, win_size: int,\n                     fmin: int,\n                     fmax: int,\n                     center: bool = False,\n                     top_db=60\n                     ):\n    \"\"\"\n        this method read a file from the path and converts it into a mel-spectrogram and returns the audio and mel\n    \"\"\"\n    try:\n        audio, _ = librosa.load(path, sr=sample_rate)\n    except Exception as e:\n        return None, None\n\n    audio = librosa.effects.trim(audio, top_db=top_db, frame_length=512, hop_length=256)[0]\n    audio = librosa.util.normalize(audio) * 0.95\n    mel = mel_spectrogram(\n        y=torch.from_numpy(audio).float().unsqueeze(0),\n        n_fft=n_fft,\n        num_mels=num_mels,\n        sampling_rate=sample_rate,\n        hop_size=hop_size,\n        win_size=win_size,\n        fmin=fmin,\n        fmax=fmax,\n        center=center\n    ).squeeze(0).T.numpy()\n\n    return audio, mel\n\n","b5d36ed1":"def create_and_save_mel_from_audio(path, speaker_id, speaker, dataset, tag='', eval=False):\n    pattern_path = hp.Training.Eval_Pattern.Path if eval else hp.Training.Train_Pattern.Path\n\n    file = '{}.{}{}.PICKLE'.format(\n        speaker if dataset in speaker else '{}.{}'.format(dataset, speaker),\n        '{}.'.format(tag) if tag != '' else '',\n        os.path.splitext(os.path.basename(path))[0]\n    ).upper()\n    file = os.path.join(pattern_path, dataset, speaker, file).replace(\"\\\\\", \"\/\")\n\n    if os.path.exists(file):\n        return\n\n    _, mel = pattern_generate(\n        path=path,\n        n_fft=hp.Sound.N_FFT,\n        num_mels=hp.Sound.Mel_Dim,\n        sample_rate=hp.Sound.Sample_Rate,\n        hop_size=hp.Sound.Frame_Shift,\n        win_size=hp.Sound.Frame_Length,\n        fmin=hp.Sound.Mel_F_Min,\n        fmax=hp.Sound.Mel_F_Max,\n        top_db=top_DB_dict[dataset] if dataset in top_DB_dict.keys() else 60\n    )\n    if mel is None:\n        print('Failed: {}'.format(path))\n        return\n\n    new_pattern_dict = {\n        'Mel': mel.astype(np.float32),\n        'Speaker_ID': speaker_id,\n        'Speaker': speaker,\n        'Dataset': dataset,\n    }\n\n    os.makedirs(os.path.join(pattern_path, dataset, speaker).replace('\\\\', '\/'), exist_ok=True)\n    \n    # the new_pattern_dict as a pickel file, this file be used read later during the training and eval of a model\n    with open(file, 'wb') as f:\n        pickle.dump(new_pattern_dict, f, protocol=4)\n","5599ef3b":"def get_file_infos(walks, max_num_audio_per_spkr):\n    paths = []\n    speaker_dict = {}\n    tag_dict = {}\n    count_by_speaker = {}\n\n    for root, _, files in walks:\n        for file in files:\n            file = os.path.join(root, file).replace('\\\\', '\/')\n            if os.path.splitext(file)[1].upper() not in ALLOWED_EXTS:\n                continue\n\n            speaker_id = file.split('\/')[-3]\n            speaker_name = meta_data[meta_data[\"VoxCeleb1 ID\"] == speaker_id][\"VGGFace1 ID\"].values[0]\n            speaker = 'vc1.{}_{}'.format(speaker_name.lower(), speaker_id.lower())\n\n            if max_num_audio_per_spkr is not None:\n                if speaker in count_by_speaker.keys() and count_by_speaker[speaker] >= max_num_audio_per_spkr:\n                    continue\n\n            paths.append(file)\n            speaker_dict[file] = speaker\n            tag_dict[file] = file.split('\/')[-2]\n\n            if speaker not in count_by_speaker.keys():\n                count_by_speaker[speaker] = 0\n\n            count_by_speaker[speaker] += 1\n\n    print('VC1 info generated: {}'.format(len(paths)))\n    return paths, speaker_dict, tag_dict\n\n\ndef VC1_raw_data_info_load(path, max_num_audio_per_spkr=None):\n    lst_vox_celeb_folders = os.listdir(path)\n    random.shuffle(lst_vox_celeb_folders)\n\n    tot_len = len(lst_vox_celeb_folders)\n    train_items = tot_len * 80 \/\/ 100\n    eval_items = tot_len - train_items\n\n    lst_vox_celeb_folders_train = lst_vox_celeb_folders[:train_items]\n    lst_vox_celeb_folders_eval = lst_vox_celeb_folders[train_items:]\n\n    train_walks = []\n    for celeb in lst_vox_celeb_folders_train:\n        walks = [x for x in os.walk(os.path.join(path, celeb))]\n        train_walks.extend(walks)\n\n    eval_walks = []\n    for celeb in lst_vox_celeb_folders_eval:\n        walks = [x for x in os.walk(os.path.join(path, celeb))]\n        eval_walks.extend(walks)\n\n    random.shuffle(train_walks)\n    random.shuffle(eval_walks)\n\n    # this function returns a tuple\n    # paths, speaker_dict, tag_dict = get_file_infos(walks_eval, max_num_audio_per_spkr)\n    train_info = get_file_infos(train_walks, max_num_audio_per_spkr)\n    eval_info = get_file_infos(eval_walks, max_num_audio_per_spkr)\n\n    return train_info, eval_info\n\n\ndef speaker_dict_to_idx_generate(speaker_dict):\n    \"\"\"\n        this function gives each speaker an index\n        e.g. speaker1 = 0, speaker2 = 1 and so on\n    \"\"\"\n    idx_speaker_dict = {speaker: index for index, speaker in enumerate(sorted(set(speaker_dict.values())))}\n    return idx_speaker_dict\n\n\n\n","e45384cd":"def metadata_generate(speaker_index_dict, eval=False):\n    pattern_path = hp.Training.Eval_Pattern.Path if eval else hp.Training.Train_Pattern.Path\n    metadata_file = hp.Training.Eval_Pattern.Metadata_File if eval else hp.Training.Train_Pattern.Metadata_File\n\n    pattern_path = pattern_path.replace(\"\\\\\", \"\/\")\n    metadata_file = metadata_file.replace(\"\\\\\", \"\/\")\n\n    new_Metadata_Dict = {\n        'N_FFT': hp.Sound.N_FFT,\n        'Mel_Dim': hp.Sound.Mel_Dim,\n        'Frame_Shift': hp.Sound.Frame_Shift,\n        'Frame_Length': hp.Sound.Frame_Length,\n        'Sample_Rate': hp.Sound.Sample_Rate,\n        'File_List': [],\n        'Mel_Length_Dict': {},\n        'Speaker_ID_Dict': {},\n        'Speaker_Dict': {},\n        'Dataset_Dict': {},\n        'File_List_by_Speaker_Dict': {},\n        'Text_Length_Dict': {},\n        'ID_Reference': {'Speaker': speaker_index_dict}\n    }\n\n    files_tqdm = tqdm(\n        total=sum([len(files) for root, _, files in os.walk(pattern_path)]),\n        desc='Eval_Pattern' if eval else 'Train_Pattern'\n    )\n\n    for root, _, files in os.walk(pattern_path):\n        for file in files:\n            with open(os.path.join(root, file).replace(\"\\\\\", \"\/\"), \"rb\") as f:\n                pattern_dict = pickle.load(f)\n\n            file = os.path.join(root, file).replace(\"\\\\\", \"\/\").replace(pattern_path, '').lstrip('\/')\n            try:\n                if not all([\n                    key in pattern_dict.keys()\n                    for key in ('Mel', 'Speaker_ID', 'Speaker', 'Dataset')\n                ]):\n                    continue\n                new_Metadata_Dict['Mel_Length_Dict'][file] = pattern_dict['Mel'].shape[0]\n                new_Metadata_Dict['Speaker_ID_Dict'][file] = pattern_dict['Speaker_ID']\n                new_Metadata_Dict['Speaker_Dict'][file] = pattern_dict['Speaker']\n                new_Metadata_Dict['Dataset_Dict'][file] = pattern_dict['Dataset']\n                new_Metadata_Dict['File_List'].append(file)\n\n                if not pattern_dict['Speaker'] in new_Metadata_Dict['File_List_by_Speaker_Dict'].keys():\n                    new_Metadata_Dict['File_List_by_Speaker_Dict'][pattern_dict['Speaker']] = []\n\n                new_Metadata_Dict['File_List_by_Speaker_Dict'][pattern_dict['Speaker']].append(file)\n            except:\n                print('File \\'{}\\' is not correct pattern file. This file is ignored.'.format(file))\n            files_tqdm.update(1)\n\n    with open(os.path.join(pattern_path, metadata_file.upper()).replace(\"\\\\\", \"\/\"), 'wb') as f:\n        pickle.dump(new_Metadata_Dict, f, protocol=4)\n\n    print('Metadata generate done.')","2952cffd":"meta_data = pd.read_csv(hp.audio.meta_file, delimiter=\"\\t\")\nmeta_data.head()","dbdba272":"\nspeaker_dict = {}\ndataset_dict = {}\ntag_dict = {}\n\n# train_info, eval_info = VC1_raw_data_info_load(path=hp.audio.raw_vox, max_num_audio_per_spkr=2)\ntrain_info, eval_info = VC1_raw_data_info_load(path=hp.audio.raw_vox, max_num_audio_per_spkr=None)\n","ed72a321":"# saving the mel-spectrograms for training\n# paths, speaker_dict, tag_dict\n# paths_train is from where all the files needs to be read to create mel-spectrogram\npaths_train = train_info[0]\nvc1_speaker_dict = train_info[1]\nvc1_tag_dict = train_info[2]\n\nspeaker_dict.update(vc1_speaker_dict)\ndataset_dict.update({path: 'VC1' for path in paths_train})\ntag_dict.update(vc1_tag_dict)\n\nspeaker_index_dict = speaker_dict_to_idx_generate(speaker_dict)\n\n# using tqdm to show the progress in the console\nfor path in tqdm(paths_train):\n    create_and_save_mel_from_audio(path, speaker_index_dict[speaker_dict[path]],\n                                   speaker_dict[path],\n                                   dataset_dict[path],\n                                   tag_dict[path],\n                                   False)\n\n# this will create the metadata for that we will use later to read the mel-spectrograms of the celeb\nmetadata_generate(speaker_index_dict)\n\n","c8805d7e":"#####################################################\n# saving the mel-spectrograms for eval              #\n#####################################################\n\nspeaker_dict = {}\ndataset_dict = {}\ntag_dict = {}\n\npaths_eval = eval_info[0]\nvc1_speaker_dict = eval_info[1]\nvc1_tag_dict = eval_info[2]\n\nspeaker_dict.update(vc1_speaker_dict)\ndataset_dict.update({path: 'VC1' for path in paths_eval})\ntag_dict.update(vc1_tag_dict)\n\nspeaker_index_dict = speaker_dict_to_idx_generate(speaker_dict)\n\nfor path in tqdm(paths_eval):\n    create_and_save_mel_from_audio(\n        path,\n        speaker_index_dict[speaker_dict[path]],\n        speaker_dict[path],\n        dataset_dict[path],\n        tag_dict[path],\n        True\n    )\n\n# this will create the metadata for that we will use later to read the mel-spectrograms of the celeb\nmetadata_generate(speaker_index_dict, eval=True)","a78d2d32":"hp.Training","83498bc8":"# view files\nos.listdir(\".\/train\/VC1\/\")","8a06f717":"# view files\nos.listdir(\".\/eval\/VC1\/\")","9ddac1b0":"# How to get voxceleb1 data\n> This notebook is a boiler plate example of how one can use this dataset. I am using this dataset to create melspectrograms out of it. These melspectrogram will be used for speaker identification.\n\n**This notebook does the following**\n* Creates a hyperparameter dictionary with dot functionality. Meaning, the python dictionary can be used as a.element","735bdf77":"# Utilities to process audio file.\n\n"}}