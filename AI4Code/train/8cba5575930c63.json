{"cell_type":{"49a518ec":"code","6b53fcd2":"code","dadd43b1":"code","c0b3634e":"code","558473af":"code","252c5469":"code","2faae994":"code","b0a3f56f":"code","22c02509":"code","16fe9dec":"code","bb3fe91e":"code","d46e446b":"code","371f2717":"code","65a79bdc":"code","396c28fa":"code","9ec3a459":"code","7306da57":"code","0a47e0d6":"code","57802f5d":"code","8a43ebe4":"code","5b894857":"code","35d754c9":"code","6c7a78b6":"code","7d920f4e":"code","11b3f284":"code","214474df":"code","5823dff6":"code","85d29808":"code","2a2f931e":"code","11f1c4b8":"code","8861e678":"code","fe29e3ac":"code","ebd425d5":"markdown","5ac15768":"markdown","cd8c6025":"markdown","c82073a7":"markdown","0c6f8961":"markdown","6ef637a6":"markdown","c7af4188":"markdown","7f805770":"markdown","1441f8c7":"markdown","73b13dea":"markdown","09a5f217":"markdown","d4ebd088":"markdown","ba80a878":"markdown","c2d8f231":"markdown","f2569b72":"markdown","36824383":"markdown","8ccda510":"markdown","775e27a5":"markdown","ce276806":"markdown"},"source":{"49a518ec":"from IPython.display import YouTubeVideo\n\nYouTubeVideo('HctunZLmc10', width=800\/1.2, height=450\/1.25)","6b53fcd2":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport re\nimport os\nimport string\nimport pickle\n\n\nimport nltk\nimport gensim\n\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom gensim.parsing.preprocessing import remove_stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import LancasterStemmer\n\nfrom gensim.test.utils import common_corpus, common_dictionary\nfrom gensim.similarities import MatrixSimilarity\n\nfrom gensim.test.utils import datapath, get_tmpfile\nfrom gensim.similarities import Similarity\n\nfrom IPython.display import display, Markdown, Math, Latex, HTML\n\n\nimport pandas as pd\nimport seaborn as sns\n\n!pip install webdriverdownloader\nfrom webdriverdownloader import GeckoDriverDownloader\n\n!pip install selenium\nfrom selenium.webdriver.common.by  import By as selenium_By\nfrom selenium.webdriver.support.ui import Select as selenium_Select\nfrom selenium.webdriver.support.ui import WebDriverWait as selenium_WebDriverWait\nfrom selenium.webdriver.support    import expected_conditions as selenium_ec\nfrom IPython.display import Image\n\n\n\nfrom selenium import webdriver as selenium_webdriver\nfrom selenium.webdriver.firefox.options import Options as selenium_options\nfrom selenium.webdriver.common.desired_capabilities import DesiredCapabilities as selenium_DesiredCapabilities\n\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\n\n# YOU MUST ADD YOUR USERNAME AND PASSWORD OF RESEARCH GATE TO THE SECRET CREDENTIALS TO BE ABLE TO GET THE SCRAPED DATA\n# email = user_secrets.get_secret(\"email\")\n# password = user_secrets.get_secret(\"pass\")\nemail = user_secrets.get_secret(\"email\")\npassword = user_secrets.get_secret(\"pass\")\nprox_s = user_secrets.get_secret(\"proxy_server\")","dadd43b1":"meta = pd.read_csv(\"\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv\")\nprint(\"Cols names: {}\".format(meta.columns))\nmeta.head(7)","c0b3634e":"plt.figure(figsize=(20,10))\nmeta.isna().sum().plot(kind='bar', stacked=True)","558473af":"meta.columns","252c5469":"meta_dropped = meta.drop(['who_covidence_id'], axis = 1)","2faae994":"plt.figure(figsize=(20,10))\n\nmeta_dropped.isna().sum().plot(kind='bar', stacked=True)","b0a3f56f":"miss = meta['abstract'].isna().sum()\nprint(\"The number of papers without abstracts is {:0.0f} which represents {:.2f}% of the total number of papers\".format(miss, 100* (miss\/meta.shape[0])))","22c02509":"abstracts_papers = meta[meta['abstract'].notna()]\nprint(\"The total number of papers is {:0.0f}\".format(abstracts_papers.shape[0]))\nmissing_doi = abstracts_papers['doi'].isna().sum()\nprint(\"The number of papers without doi is {:0.0f}\".format(missing_doi))\nmissing_url = abstracts_papers['url'].isna().sum()\nprint(\"The number of papers without url is {:0.0f}\".format(missing_url))","16fe9dec":"abstracts_papers = abstracts_papers[abstracts_papers['publish_time'].notna()]\nabstracts_papers['year'] = pd.DatetimeIndex(abstracts_papers['publish_time']).year","bb3fe91e":"missing_url_data = abstracts_papers[abstracts_papers[\"url\"].notna()]\nprint(\"The total number of papers with abstracts, urls, but missing doi = {:.0f}\".format( missing_url_data.doi.isna().sum()))","d46e446b":"abstracts_papers = abstracts_papers[abstracts_papers[\"url\"].notna()]","371f2717":"!cat \/etc\/os-release\n\n!mkdir \"\/kaggle\/working\/firefox\"\n!ls -l \"\/kaggle\/working\"\n!cp -a \"\/kaggle\/input\/firefox-63.0.3.tar.bz2\/firefox-63.0.3\/firefox\/.\" \"\/kaggle\/working\/firefox\"\n!ls -l \"\/kaggle\/working\/firefox\"\n\n\n!chmod -R 777 \"\/kaggle\/working\/firefox\"\n!ls -l \"\/kaggle\/working\/firefox\"\n\ngdd = GeckoDriverDownloader()\ngdd.download_and_install(\"v0.23.0\")\n!apt-get install -y libgtk-3-0 libdbus-glib-1-2 xvfb\n!export DISPLAY=:99","65a79bdc":"from selenium.webdriver.common.proxy import Proxy, ProxyType","396c28fa":"browser_options = selenium_options()\nbrowser_options.add_argument(\"--headless\")\nbrowser_options.add_argument(\"--window-size=1920,1080\")\n\nproxy = prox_s\n\ncapabilities_argument = selenium_DesiredCapabilities().FIREFOX\ncapabilities_argument[\"marionette\"] = True\ncapabilities_argument['proxy'] = {\n    \"proxyType\": \"MANUAL\",\n    \"httpProxy\": proxy,\n    \"ftpProxy\": proxy,\n    \"sslProxy\": proxy\n}\n\nbrowser = selenium_webdriver.Firefox(\n    options=browser_options,\n    firefox_binary=\"\/kaggle\/working\/firefox\/firefox\",\n    capabilities=capabilities_argument\n)","9ec3a459":"browser.get(\"https:\/\/www.researchgate.net\/login\")\nbrowser.find_element_by_id('input-login').send_keys(str(email))\nbrowser.find_element_by_id('input-password').send_keys(str(password))\nbrowser.find_element_by_class_name('action-submit').click()","7306da57":"# ids = browser.find_elements_by_xpath('\/\/*[@class]')","0a47e0d6":"# for ii in ids:\n# #     print(ii.tag_name)\n#     print (ii.get_attribute('id'))    # id name as string","57802f5d":"# browser.find_element_by_class_name('challenge-form').click()","8a43ebe4":"# print(browser.current_url)","5b894857":"# We will be usingn researchgate to scrape more data as follows\n# browser.get(\"https:\/\/www.researchgate.net\/login\")\nbrowser.save_screenshot(\"screenshot.png\")\nImage(\"screenshot.png\", width=800, height=500)","35d754c9":"def get_citations(top10):\n    \n    citations = []\n    for paper in range(10):\n        if(not (type(top10.doi[paper]) == float)):\n            browser.get(\"https:\/\/www.researchgate.net\/search.Search.html?type=researcher&query=\" + str(top10.doi[paper]))\n\n            element = browser.find_elements_by_class_name('nova-c-nav__item-label')\n            if(len(element) >= 3):\n                numbers = re.findall(\"\\d+\" , (element[3].text).lower())\n                if len(numbers) >= 1:\n                    citations.append(max(int(numbers[0]), 0.1))\n                else:\n                    citations.append(0.1)\n            else:\n                citations.append(0.1)   \n        else:\n            citations.append(0.1)\n            \n    return citations","6c7a78b6":"porter = PorterStemmer()\nlancaster=LancasterStemmer()\n\n# abstracts_only = abstracts_papers['abstract']\n# tokenized_abs = []\n\n# for abst in abstracts_only:\n#     tokens_without_stop_words = remove_stopwords(abst)\n#     tokens_cleaned = sent_tokenize(tokens_without_stop_words)\n#     words = [porter.stem(w.lower()) for text in tokens_cleaned for w in word_tokenize(text) if (w.translate(str.maketrans('', '', string.punctuation))).isalnum()]\n#     tokenized_abs.append(words)\n    \n    \n# dictionary = gensim.corpora.Dictionary(tokenized_abs)\n# corpus = [dictionary.doc2bow(abstract) for abstract in tokenized_abs]\n# tf_idf = gensim.models.TfidfModel(corpus)\n\n# tf_idf.save(\"tfidf\")\n# dictionary.save(\"dict\")\n\n# with open(\"corpus.txt\", \"wb\") as fp:\n#     pickle.dump(corpus, fp)\n","7d920f4e":"with open(\"\/kaggle\/input\/tfidfcovid19\/corpus.txt\", \"rb\") as fp:\n    corpus = pickle.load(fp)\n    \ndictionary = gensim.corpora.Dictionary.load(\"\/kaggle\/input\/tfidfcovid19\/dict\")\ntf_idf = gensim.models.TfidfModel.load(\"\/kaggle\/input\/tfidfcovid19\/tfidf\")","11b3f284":"def query_tfidf(query):\n    \n    query_without_stop_words = remove_stopwords(query)\n    tokens = sent_tokenize(query_without_stop_words)\n\n    query_doc = [porter.stem(w.lower()) for text in tokens for w in word_tokenize(text) if (w.translate(str.maketrans('', '', string.punctuation))).isalnum()]\n\n    # mapping from words into the integer ids\n    query_doc_bow = dictionary.doc2bow(query_doc)\n    query_doc_tf_idf = tf_idf[query_doc_bow]\n    \n    return query_doc_tf_idf","214474df":"def rankings(query):\n\n    query_doc_tf_idf = query_tfidf(query)\n    index_temp = get_tmpfile(\"index\")\n    index = Similarity(index_temp, tf_idf[corpus], num_features=len(dictionary))\n    similarities = index[query_doc_tf_idf]\n\n    # Storing similarity in the dataframe and sort from high to low simmilatiry\n    print(similarities)\n    abstracts_papers[\"similarity\"] = similarities\n    abstracts_papers_sorted = abstracts_papers.sort_values(by ='similarity' , ascending=False)\n    abstracts_papers_sorted.reset_index(inplace = True)\n    \n    top20 = abstracts_papers_sorted.head(10)\n    top20[\"doi\"].astype(str)\n    citations = get_citations(top20)\n    top20[\"citations\"] = citations\n    top20[\"similarity\"] = top20[\"similarity\"] * (top20.citations \/ top20.citations.max()) * 0.5\n    norm_range = top20['year'].max() - top20['year'].min()\n    top20[\"similarity\"] -= (abs(top20['year'] - top20['year'].max()) \/ norm_range)*0.1\n    top20 = top20.sort_values(by ='similarity' , ascending=False)\n    top20.reset_index(inplace = True)\n    \n    return top20","5823dff6":"top = rankings(\"Non-pharmaceutical interventions\")","85d29808":"pd.set_option('display.max_colwidth', -1)\ntop[['index','abstract']].style.set_properties(**{'text-align': \"justify\"})","2a2f931e":"pd.set_option('display.max_colwidth', -1)\ntop[['index','url']].style.set_properties(**{'text-align': \"left\"})","11f1c4b8":"top = rankings(\"MERS respiratory sendrom\")","8861e678":"pd.set_option('display.max_colwidth', -1)\ntop[['index','abstract',]].style.set_properties(**{'text-align': \"justify\"})","fe29e3ac":"pd.set_option('display.max_colwidth', -1)\ntop[['index','url']].style.set_properties(**{'text-align': \"left\"})","ebd425d5":"### Exploratory Data Analysis\nThere are a lot of files in the directory, json files for each paper, a description file and a metadata file for each paper. We will explore the metadata file to see whether we can find some useful features that serve our purpose or not.","5ac15768":"Let's see an example on how the web page would look like. We are only interested on the integer number written on the citation tab.","cd8c6025":"We will also vectorize the corpus using TF-IDF, the TF-IDF technique to be computed to be used to compute the similarity between the query and the abstracts for prioritization in later steps.\n\n### *NOTE: IT TAKES A LOT OF TIME TO CALCULATE THE TFIDF SO WE CALCULATED IT AND STORED IT AS A KAGGLE DATASET FOR FASTER REFERENCE, TO RE-CALCULATE IT YOU CAN UNCOMMENT THE FOLLOWING CELL*\n","c82073a7":"Now we will use Selenium library to get the number of citations for each research paper based on the unique doi number","0c6f8961":"### Model Performance\nWe will measure the model performance using the precision@k method. Since the data doesn't contain a groundtruth of relevance, We needed to evaluate each research paper manually to calculate the precision@K.\n#### Precision:\nIt measures the amount of instances that were identified correctly as positive and can be measured as follows:\n\n$$Precision = {TP \\over TP + FP}.$$\n\nWhere:<br>\nTP stands for True Positives: The number of data points that were marked correctly as relevant.<br>\nFP stands for False Positives: The number of data points that were wrongly marked as relevant while it truly belongs to the irrelevant class.\n\n#### Precision@K:\nIt measures the precision at each k level starting from `k = 1` till `k = n`, where n is the total number of queries that you are using to measure the precision. For our case, we will only recommend the top 10 research articles so we will have 10 levels from `k = 1` to `k = 10`.\n\nFor the first query \"Social distancing measures query result\" ::\n\n$$P@K = [{{1 \\over 1}, {2 \\over 2}, {2 \\over 3}, {3 \\over 4}, {4 \\over 5}, {5 \\over 6}, {6 \\over 7}, {7 \\over 8}, {8 \\over 9}, {9 \\over 10}}].$$\n\n\nFor the second query \"MERS respiratory sendrom\" ::\n\n$$P@K = [{{1 \\over 1}, {2 \\over 2}, {3 \\over 3}, {4 \\over 4}, { 5 \\over 5 }, {6 \\over 6}, {7 \\over 7}, {8 \\over 8}, {9 \\over 9}, {10 \\over 10}}].$$\n\nIt seems for the second query, all the papers are relevant.\n\nNow we will use the Mean Average Precision (MAP)  to get an estimate of the precision of our system on multiple queries:\n\n#### Mean Average Precision (MAP):\n$$ MAP(Q) = {{\\sum_{j=1}^{\\lvert Q \\lvert} { {1 \\over m_j} \\sum_{k=1}^{m_j} {precision(R_{j, k})} } } \\over \\lvert Q \\lvert }$$\n\nWhere:<br>\n`Q`: The number of Queries performed.<br>\n`m_j`: the total number of true positives for query `j`.<br>\n`R_(j, k)`: precision of the relevant result `k` in query `j`\n\n\n$$ MAP = {{(1+1+ 3\/4 + 4\/5 + 5\/6 + 6\/7 + 7\/8 + 8\/9 + 9\/10)\/ 8} + {(1 * 10)\/ 10} \\over 2} = { 0.99} $$\n\nFrom the performance that we calculated, it seems that our direction is promising but can be further improved by retrieving more data about the authors and the impact factors of the journals.","6ef637a6":"Now lets see if the papers with urls have doi or not","c7af4188":"### Web Scraping\nThe number of citations can be a good factor that helps us determine the importance of the research work. This dataset doesn't contain the number of citations but we are going to use selenium to get this number from researchgate based on the doi identification number of each research paper.\nNow I will do some web scraping to get the number of citations for each paper in the top 10 results and to do that I am going to use the code from the great [Notebook](https:\/\/www.kaggle.com\/dierickx3\/kaggle-web-scraping-via-headless-firefox-selenium) by Tom Dierickx","7f805770":"Now, we will need to apply the same cleaning steps, that we applied before to the corpus, to the query itself so that we can get consistent results. We will do the following: Removing stop words, removing punctuation and stemming. then we will map the words to their integer ids using the dictionary of words computed before.","1441f8c7":"# Prioritizing research papers based on abstracts\n#### The aim of this project is to Implement an efficient system that prioritizes and recommends papers based on research topic or query. This notebook contains the baseline implementation of the core feature in the Medical Cloud Platform (MCP) which aims to speed up the pace of medical research. This can be achieved increasing the collaboration between researchers around the world which, as stated by the WHO, can speed up the pace of [solidarity clinical Trials](https:\/\/www.who.int\/emergencies\/diseases\/novel-coronavirus-2019\/global-research-on-novel-coronavirus-2019-ncov\/solidarity-clinical-trial-for-covid-19-treatments) by 80%. It also aims to save the researchers' time and make them able to direct all their efforts towards innovating new ways to fight COVID-19.\n\n#### In this Notebooks we will focus on the basic features of the search engine only which includes the following steps:\n1. Exploratory Data Analysis, EDA\n2. Data Cleaning\n3. Web Scraping\n4. Implementation\n5. Performance\n\n##### We will start first by data exploration and cleaning then make a hypothesis based on our data exploration and domain knowledge, after that we will be implementing our solution using NLP and ML libraries like Gensim, Scikit-Learn and Selenium. For more details about the MCP project watch the video below.","73b13dea":"### Remove stopwords: \nnow we are going to remove the stop words which are common words as (the, a, an, etc.) we will also remove punctuation, we will then revert the words to its basis using the famous stemming algorithm (Porter algorithm) before using the TF-IDF techniques. We will also take care while removing punctuation as we don't want to lose the meaning of words like (non-medical) while using the dash but we will convert it to (nonmedical) instead to keep its meaning with respect to the search algorithm","09a5f217":"Now we will use the cosine similarity algorithm from gensim, we tried to use text vectorizer instead of the TF-IDF but the TF-IDF gave better results. We will store the similarity in the same dataframe with the abstracts and sort from highest similarity to the lowest one.","d4ebd088":"To save time, we are gonna load the TFIDF, Corpus and Dictionary saved in the previous step instead of running it every time we modify the code for faster inference","ba80a878":"From the above histogram we can see there is a small number of papers with missing urls and a considerable number with missing doi and abstracts. We are interested only in papers that have abstracts and either doi or url to be able to recommend them to the researcher. So let's explore some statistics about the papers with missing abstracts and remove them if possible.","c2d8f231":"It seems that the two features 'Microsoft Academic Paper ID', 'WHO #Covidence' have a very huge number of missing values as obvious from the histogram so we will remove them to regulate the scale of the histogram's frequencies","f2569b72":"Now we will print the top 10 abstracts from the similarity algorithm","36824383":"We will also reorder the top 20 papers by penalizing the older papers using the equation: similarity - 0.1*abs(x - newest_year_puplication) \/ (newest_year_puplication - oldest_year_puplication) and by giving the papers with more citations a higher priority","8ccda510":"As we see from these previous results the percentage of missing abstracts is considerable but we can ignore it as it is one of the most important features in our approach. Now lets see the number of missing doi and the number of missing urls from the papers without missing abstracts.","775e27a5":"Based on the above data exploration, we will need to remove the data that doesn't have both url so that research scientists can find the paper recommended to them. We will only choose papers with abstracts and either doi or url as follows:","ce276806":"We will need to extract the year of publication to give high priority to the papers with earlier publication data as they are more likely to be cited in newer papers and also because the might not be sufficient for today's usage (ex: papers from 1955)"}}