{"cell_type":{"922096bc":"code","4768f1b6":"code","a7a55485":"code","dc3090cd":"code","22c0997b":"code","6687f39e":"code","7b54569e":"code","9f1619cf":"code","002ae990":"code","811b45a1":"code","9946deeb":"code","7ed05ae0":"code","5230b2c2":"code","df55a18f":"code","54e0e51c":"code","7a901050":"code","92312bb1":"code","8f0af632":"code","ea539152":"code","dcb7a9b7":"code","b8d325e0":"code","4d017022":"code","8ae9b5fd":"code","673bb74a":"code","25b18b18":"code","12280398":"code","660f10fb":"code","c09c49ca":"code","c2d8d103":"code","e58def5d":"code","78791ff3":"code","511c648a":"code","eeaf9178":"code","68fef614":"code","5bf5cff0":"code","99a57fa7":"code","9817156e":"code","e7e7d2fa":"code","0c2c9cae":"code","8b4fb4fe":"code","617eb350":"code","468602ec":"markdown","703757c0":"markdown","28c4f5e8":"markdown","91b33769":"markdown","b6e3ac68":"markdown","ab26bab2":"markdown"},"source":{"922096bc":"import pandas as pd\nimport numpy as np\nimport math\nimport datetime\nfrom fastai.tabular.all import *\n# from fastai.tabular import *\nfrom fastai.imports import *\nfrom fastai.metrics import error_rate\n#from fastai.callbacks import *\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import Normalizer\nimport scipy.stats as spstats","4768f1b6":"path = '\/kaggle\/input\/predict-volcanic-eruptions-ingv-oe\/'\noutput_path = '\/kaggle\/output\/kaggle\/working\/modles\/'","a7a55485":"train = pd.read_csv(path+\"train.csv\")\ntrain","dc3090cd":"sample_submission = pd.read_csv(\"..\/input\/predict-volcanic-eruptions-ingv-oe\/sample_submission.csv\")\nsample_submission","22c0997b":"train['time_to_eruption'].describe()","6687f39e":"train_readings = glob.glob(path+\"train\/*\")\nlen(train_readings)","7b54569e":"test_readings = glob.glob(path+\"test\/*\")\nlen(test_readings)","9f1619cf":"train_readings[0]","002ae990":"sensor_file = pd.read_csv(train_readings[0])\nsensor_file","811b45a1":"def create_features(df,signal,seg_id,sensor_id):\n    f = np.fft.fft(signal)\n    f_real = np.real(f)\n    df.loc[seg_id, f'{sensor_id}_sum']       = signal.sum()\n    df.loc[seg_id, f'{sensor_id}_mean']      = signal.mean()\n    df.loc[seg_id, f'{sensor_id}_std']       = signal.std()\n    df.loc[seg_id, f'{sensor_id}_var']       = signal.var() \n    df.loc[seg_id, f'{sensor_id}_max']       = signal.max()\n    df.loc[seg_id, f'{sensor_id}_min']       = signal.min()\n    df.loc[seg_id, f'{sensor_id}_skew']      = signal.skew()\n    df.loc[seg_id, f'{sensor_id}_mad']       = signal.mad()\n    df.loc[seg_id, f'{sensor_id}_kurtosis']  = signal.kurtosis()\n    df.loc[seg_id, f'{sensor_id}_quantile99']= np.quantile(signal, 0.99)\n    df.loc[seg_id, f'{sensor_id}_quantile95']= np.quantile(signal, 0.95)\n    df.loc[seg_id, f'{sensor_id}_quantile85']= np.quantile(signal, 0.85)\n    df.loc[seg_id, f'{sensor_id}_quantile75']= np.quantile(signal, 0.75)\n    df.loc[seg_id, f'{sensor_id}_quantile55']= np.quantile(signal, 0.55)\n    df.loc[seg_id, f'{sensor_id}_quantile45']= np.quantile(signal, 0.45) \n    df.loc[seg_id, f'{sensor_id}_quantile25']= np.quantile(signal, 0.25) \n    df.loc[seg_id, f'{sensor_id}_quantile15']= np.quantile(signal, 0.15) \n    df.loc[seg_id, f'{sensor_id}_quantile05']= np.quantile(signal, 0.05)\n    df.loc[seg_id, f'{sensor_id}_quantile01']= np.quantile(signal, 0.01)\n    df.loc[seg_id, f'{sensor_id}_fft_real_mean']= f_real.mean()\n    df.loc[seg_id, f'{sensor_id}_fft_real_std'] = f_real.std()\n    df.loc[seg_id, f'{sensor_id}_fft_real_max'] = f_real.max()\n    df.loc[seg_id, f'{sensor_id}_fft_real_min'] = f_real.min()\n    df.loc[seg_id, f'{sensor_id}_fft_real_median'] = np.median(f_real)\n    df.loc[seg_id, f'{sensor_id}_fft_real_skew'] = spstats.skew(f_real)\n    df.loc[seg_id, f'{sensor_id}_fft_real_kurtosis'] = spstats.kurtosis(f_real)\n    \n    return df","9946deeb":"train = pd.read_csv(path+'train.csv')\ntrain_df = pd.DataFrame()\ntrain_df['segment_id'] = train.segment_id\ntrain_df = train_df.set_index('segment_id')\n\nj=0\nfor seg in train.segment_id:\n    signals = pd.read_csv(path+f'train\/{seg}.csv')\n    if j%500 == 0:\n        print(j)\n    for i in range(1, 11):\n        sensor_id = f'sensor_{i}'\n        train_df = create_features(train_df, signals[sensor_id].fillna(0), seg, sensor_id,)\n    j+=1    ","7ed05ae0":"train_df = pd.merge(train_df.reset_index(), train, on=['segment_id'], how='left').set_index('segment_id')\ntrain_df","5230b2c2":"train_df = train_df.reset_index()","df55a18f":"y = train_df['time_to_eruption']\ntrain_df = train_df.drop(['segment_id'], axis = 1)\ntrain_df","54e0e51c":"test = pd.read_csv(path+'sample_submission.csv')\ntest_df = pd.DataFrame()\ntest_df['segment_id'] = test.segment_id\ntest_df = test_df.set_index('segment_id')\n\nj=0\nfor seg in test.segment_id:\n    signals = pd.read_csv(path+f'test\/{seg}.csv')\n    if j%500 == 0:\n        print(j)\n    for i in range(1, 11):\n        sensor_id = f'sensor_{i}'\n        test_df = create_features(test_df, signals[sensor_id].fillna(0), seg, sensor_id,)\n    j+=1 ","7a901050":"test_df","92312bb1":"test_df = test_df.reset_index()\ntest_set = test_df\ntest_df = test_df.drop(['segment_id'], axis = 1)\ntest_df","8f0af632":"test_df","ea539152":"train_df.columns","dcb7a9b7":"for i in list(train_df.columns):\n    print(i)","b8d325e0":"cont_names = list(train_df.columns)\n#removing time to eruption column\ncont_names.pop()\ncont_names","4d017022":"cat_names = []\nprocs = [Categorify, FillMissing, Normalize]","8ae9b5fd":"splits = RandomSplitter(valid_pct=0.2)(range_of(train_df))","673bb74a":"to = TabularPandas(train_df, procs=[Categorify, FillMissing,Normalize],\n                   cat_names = cat_names,\n                   cont_names = cont_names,\n                   y_names='time_to_eruption',\n                   splits=splits)","25b18b18":"dls = to.dataloaders(bs=64)","12280398":"dls.show_batch()","660f10fb":"learn = tabular_learner(dls, layers=[200,100], metrics=mae, ps=[0.001,0.01], emb_drop=0.01)","c09c49ca":"learn.model","c2d8d103":"learn.lr_find(suggestions=True)","e58def5d":"learn.model_dir='\/kaggle\/working\/' ","78791ff3":"learn.fit_one_cycle(50,0.33113112449646,cbs=SaveModelCallback(monitor='mae', comp=np.less, fname=\"stage-1\"))","511c648a":"learn.load('stage-1')","eeaf9178":"test_df","68fef614":"dl = learn.dls.test_dl(test_df)","5bf5cff0":"preds = learn.get_preds(dl=dl)","99a57fa7":"preds[0]","9817156e":"test_preds = []\nfor i in np.array(preds[0]):\n#     print(i[0])\n    test_preds.append(i[0])","e7e7d2fa":"test","0c2c9cae":"test['time_to_eruption'] = test_preds","8b4fb4fe":"test","617eb350":"test.to_csv('submission.csv', index=False)","468602ec":"####  Create features for Training Data","703757c0":"### Feature Creation","28c4f5e8":"For each segment ID, we have got 10 sensors and 60001 readings from each one of them.","91b33769":"## Work in Progress","b6e3ac68":"**The below dataframe consist of the segment ID and the target value i.e. the time left for the volcano to erupt.\nFor each of the segment ID's we have been provided with a csv file with 10 mins of logs of readings belonging to 10 different sensors.**","ab26bab2":"#### Create features for Test Data"}}