{"cell_type":{"33d6d432":"code","b0674b58":"code","ea7b0e43":"code","a8af22e2":"code","8b37c040":"code","baee7ba5":"code","710dd1d3":"code","b610f62a":"code","f5fd70b1":"code","c6326961":"code","82443e37":"code","70cd9791":"code","f68de9d0":"code","2c671688":"code","c93afbe4":"code","cf788df0":"code","adf6777d":"code","63ff4df5":"code","b82dc522":"code","bdc58e1b":"code","242991d1":"code","6377ad88":"code","926d9006":"code","efbb3726":"code","2ad5f5b7":"code","96c72e04":"code","bfb02f53":"code","67c8803e":"code","5be52f47":"code","4bb9b21b":"code","8f8edc48":"code","1c1b3cad":"code","adc801d5":"code","ef1e6635":"code","8464d670":"code","618144f6":"code","5ccc8eaa":"markdown","618ba276":"markdown","a0db9adf":"markdown","999a42e8":"markdown","7942f0de":"markdown","500a3bba":"markdown","20a042e9":"markdown","9bafa514":"markdown","94aa0a5b":"markdown","6bd6ab47":"markdown","9160d89f":"markdown","25e54209":"markdown","d2cd947a":"markdown","fa803547":"markdown","698399c2":"markdown","f171eedd":"markdown","aa0c1326":"markdown"},"source":{"33d6d432":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b0674b58":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd\nimport numpy as np\nimport re\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import Phrases, phrases, ldamodel, CoherenceModel\nimport nltk\nfrom nltk.corpus import stopwords\nimport spacy\nimport gensim.corpora as corpora\nfrom pprint import pprint\nimport pyLDAvis.gensim \nimport matplotlib.pyplot as plt\nimport seaborn as sns","ea7b0e43":"authors_data_df = pd.read_csv('\/kaggle\/input\/spooky-author-identification\/train.zip')\nauthors_data_df.head()","a8af22e2":"# the length of the dataset\nauthors_data_df.shape","8b37c040":"# any null value\nauthors_data_df.isnull().sum()","baee7ba5":"#for the topic modelling we will focus only on the text data\nauthors_data_df = authors_data_df.drop(columns = ['id'], axis=1)\nauthors_data_df.head()","710dd1d3":"authors_data_df['author'].value_counts()","b610f62a":"authors_data_df['text_processed'] = authors_data_df['text'].map(lambda x: re.sub('[,\\.!?]','',x))","f5fd70b1":"authors_data_df['text_processed'] = authors_data_df['text_processed'].map(lambda x:x.lower())\nprint(authors_data_df['text_processed'].head())","c6326961":"# remove all characters, number or characters\ndef cleanText(input_string):\n    modified_string = re.sub('[^A-Za-z0-9]+', ' ', input_string)\n    return(modified_string)\nauthors_data_df['text_processed'] = authors_data_df.text_processed.apply(cleanText)\nauthors_data_df['text_processed'][150]","82443e37":"#NLTK stop words\nnltk.download('stopwords')","70cd9791":"stopWords = stopwords.words('english')\nstopWords.extend([\"make\",\"mr\",\"de\",\"without\",\"let\",\"rather\",\"upon\",\"within\",\"made\",\"must\",\"much\",\"yet\",\"thought\",\"see\",\n                  \"said\",\"us\",\"say\",\"whose\",\"though\",\"every\",\"know\",\n                  \"many\",\"will\",\"never\",\"even\",\"found\",\"might\",\"almost\",'although','indeed','thus','still',\n                  'this','me','of','may', 'would', 'ever','could','shall','come','go','soon','however','become',\n                  'give','take','well'])\ndef removeStopWords(stopWords, rvw_txt):\n    newtxt = ' '.join([word for word in rvw_txt.split() if word not in stopWords])\n    return newtxt\nauthors_data_df['text_processed'] = [removeStopWords(stopWords,x) for x in authors_data_df['text_processed']]","f68de9d0":"# join the different text together\nlongText = ','.join(list(authors_data_df['text_processed'].values))\n# generate the word cloud\nwordcloud = WordCloud(background_color=\"white\",\n                      max_words= 500,\n                      contour_width = 8,\n                      contour_color = \"steelblue\",\n                     collocations=False).generate(longText)\n# visualize the word cloud\nfig = plt.figure(1, figsize = (10, 10))\nplt.axis('off')\nplt.imshow(wordcloud)\nplt.show()\n","2c671688":"from collections import Counter\nfig = plt.figure(1, figsize = (20,10))\n# split() returns list of all the words in the string\nsplit_it = longText.split()\n# Pass the split_it list to instance of Counter class.\nCounter = Counter(split_it)\n#print(Counter)\n# most_common() produces k frequently encountered\n# input values and their respective counts.\nmost_occur = Counter.most_common(30)\nx_df = pd.DataFrame(most_occur, columns=(\"words\",\"count\"))\nsns.barplot(x = 'words', y = 'count', data = x_df)","c93afbe4":"hplDatadf = authors_data_df[authors_data_df.author==\"HPL\"]\nhplDatadf.head()","cf788df0":"mwsDatadf = authors_data_df[authors_data_df.author==\"MWS\"]\nmwsDatadf.head()","adf6777d":"eapDatadf = authors_data_df[authors_data_df.author==\"EAP\"]\neapDatadf.head()","63ff4df5":"# join the different text together\nlongText = ','.join(list(hplDatadf['text_processed'].values))\n# generate the word cloud\nwordcloud = WordCloud(background_color=\"white\",\n                      max_words= 500,\n                      contour_width = 8,\n                      contour_color = \"steelblue\",\n                     collocations=False).generate(longText)\n\n# visualize the word cloud\nfig = plt.figure(1, figsize = (10, 10))\nplt.axis('off')\nplt.imshow(wordcloud)\nplt.show()","b82dc522":"from collections import Counter\nfig = plt.figure(1, figsize = (20,10))\n# split() returns list of all the words in the string\nsplit_it = longText.split()\n# Pass the split_it list to instance of Counter class.\nCounter = Counter(split_it)\n#print(Counter)\n# most_common() produces k frequently encountered\n# input values and their respective counts.\nmost_occur = Counter.most_common(30)\nx_df = pd.DataFrame(most_occur, columns=(\"words\",\"count\"))\nsns.barplot(x = 'words', y = 'count', data = x_df)","bdc58e1b":"# join the different text together\nlongText = ','.join(list(mwsDatadf['text_processed'].values))\n# generate the word cloud\nwordcloud = WordCloud(background_color=\"white\",\n                      max_words= 500,\n                      contour_width = 8,\n                      contour_color = \"steelblue\",\n                     collocations=False).generate(longText)\n# visualize the word cloud\nfig = plt.figure(1, figsize = (10, 10))\nplt.axis('off')\nplt.imshow(wordcloud)\nplt.show()","242991d1":"from collections import Counter\nfig = plt.figure(1, figsize = (20,10))\n# split() returns list of all the words in the string\nsplit_it = longText.split()\n# Pass the split_it list to instance of Counter class.\nCounter = Counter(split_it)\n#print(Counter)\n# most_common() produces k frequently encountered\n# input values and their respective counts.\nmost_occur = Counter.most_common(30)\nx_df = pd.DataFrame(most_occur, columns=(\"words\",\"count\"))\nsns.barplot(x = 'words', y = 'count', data = x_df)","6377ad88":"# join the different text together\nlongText = ','.join(list(eapDatadf['text_processed'].values))\n# generate the word cloud\nwordcloud = WordCloud(background_color=\"white\",\n                      max_words= 500,\n                      contour_width = 8,\n                      contour_color = \"steelblue\",\n                     collocations=False).generate(longText)\n# visualize the word cloud\nfig = plt.figure(1, figsize = (10, 10))\nplt.axis('off')\nplt.imshow(wordcloud)\nplt.show()","926d9006":"from collections import Counter\nfig = plt.figure(1, figsize = (20,10))\n# split() returns list of all the words in the string\nsplit_it = longText.split()\n# Pass the split_it list to instance of Counter class.\nCounter = Counter(split_it)\n#print(Counter)\n# most_common() produces k frequently encountered\n# input values and their respective counts.\nmost_occur = Counter.most_common(30)\nx_df = pd.DataFrame(most_occur, columns=(\"words\",\"count\"))\nsns.barplot(x = 'words', y = 'count', data = x_df)","efbb3726":"def text_to_tokens (textSentences):\n    for sent in textSentences:\n        yield(simple_preprocess(str(sent),deacc=True))\n\nwordsData=authors_data_df.text_processed.values.tolist()\nwordsDataList = list(text_to_tokens(wordsData))\nprint(wordsDataList[:1])","2ad5f5b7":"# Building the tokens\ntokens = Phrases(wordsDataList,min_count=5,threshold=100)\ntokensModel = phrases.Phraser(tokens)","96c72e04":"def make_tokens_model(textSentences):\n   return[tokensModel[doc] for doc in textSentences]","bfb02f53":"def lemmatizedText(textSentences, allowed_postags=['NOUN','ADJ','VERB','ADV']):\n    textSent_Output = []\n    for sent in textSentences:\n        doc = nlp(\" \".join(sent))\n        textSent_Output.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n    return textSent_Output","67c8803e":"# form n grams\ndataWordsngrams = make_tokens_model(wordsDataList)\ndataWordsngrams[:2]","5be52f47":"# initialize spacy \"en\" model keeping only the tagger component \nnlp = spacy.load(\"en_core_web_sm\",disable=['parser','ner'])\n# lemmatize keeping only noun, adj, adv and verbs\nlemmatizedTextData = lemmatizedText(dataWordsngrams, allowed_postags=[\"NOUN\",\"ADJ\",\"VERB\",\"ADV\"])\nprint(lemmatizedTextData[:1])","4bb9b21b":"# create dictionary\ndictObject = corpora.Dictionary(lemmatizedTextData)\n# create corpus\ntextData = lemmatizedTextData\ndictObject[0]","8f8edc48":"# term document frequency\ncorpusData = [dictObject.doc2bow(text) for text in textData]\nprint(corpusData[:1])","1c1b3cad":"# Human readable format of the term frequency corpus \n[[(dictObject[idx], count) for idx, count in x] for x in corpusData[:1]]","adc801d5":"n_topics = 3\nldaModel = ldamodel.LdaModel(corpus=corpusData,\n                            id2word = dictObject,\n                            num_topics=n_topics,\n                            random_state=123,\n                            chunksize=100,\n                            passes=10,\n                            alpha=0.01,\n                            eta='auto',\n                            iterations=400,\n                            per_word_topics = True                            \n                            )","ef1e6635":"# print the keyword in the topics\npprint(ldaModel.print_topics())","8464d670":"coherenceModelLda = CoherenceModel(model=ldaModel, texts = lemmatizedTextData,dictionary=dictObject,coherence='c_v')\ncoherenceScore = coherenceModelLda.get_coherence()\nprint(coherenceScore)","618144f6":"pyLDAvis.enable_notebook()\nvis=pyLDAvis.gensim.prepare(ldaModel,corpusData,dictObject)\nvis","5ccc8eaa":"Step 3 Lemmatize Is the process of converting the words to the root words","618ba276":"#### Step 2 Building N grams","a0db9adf":"### Exploratory data analysis with Wordcloud","999a42e8":"## Data cleaning and preparation","7942f0de":"In this notebook we will apply topic modelling and create three topics as given dataset consists of three authors and the excerpts from their horror stories. These authors are Edgar Allan Poe, Mary Shelley, and HP Lovecraft. Dataset contains text from works of fiction written by spooky authors of the public domain: Edgar Allan Poe, HP Lovecraft and Mary Shelley.EAP work is around tales of mystery and the macabre. Mary Shelley work is around science fiction and  HP Lovecraft, best known as a writer of weird fiction.","500a3bba":"HP lovecraft wordcloud displays words like \"night\", \"death\",\"dream\", \"dead\",\"fear\",\"horror\", \"strange\", \"window\", \"ancient\" which seem to resonate with themes that the author was famous fo","20a042e9":"Edgar Allan Poe wordcloud displays words as life,end,friend, night, far, open,eye, great, one, little, time, good, manner, moment etc","9bafa514":"When we compare the top 30 words from the model per topic and then look into the top 30 words from the work of each of the author then may be Topic1 maps to EAP, Topic2 maps tp MWS and Topic3 maps to HPL based on the subset of words that overlap the most.","94aa0a5b":"#### The distribution of the excerpts script by author and we can view the number of excerpts are more for EAP then MWS and then HPL","6bd6ab47":"Build the model","9160d89f":"## Inspecting data","25e54209":"#### Step 1 Processed text to words or tokens","d2cd947a":"Do not find any null column","fa803547":"Mary Shelley wordcloud displays words as fear, heart, raymond, mind, soul, power, hope, feeling, death,spirit,friend, death. Positive as well as negative words","698399c2":"### Preparing data for Topic Modelling","f171eedd":"Step 4 Building the corpora","aa0c1326":"The human interpretability of these topics returned by the statistical analysis is not easy. We may involve domain expertise to check whether the topics makes sense."}}