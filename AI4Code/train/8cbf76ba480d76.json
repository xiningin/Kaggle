{"cell_type":{"f583afe3":"code","327961b9":"code","22cc54bd":"code","e51d9348":"code","66b4aa23":"code","e0b7861c":"code","21c531c1":"code","bb155a2c":"code","143e7477":"code","21df5233":"code","5bcb4770":"code","46cf2ed3":"code","c2ab2b70":"code","71bb7c9c":"code","5d432bd2":"code","4efa73bf":"code","a2dfd475":"code","b28a707e":"code","e7d670a8":"code","daa4050c":"code","843d654e":"code","13b049c3":"code","136914dc":"code","b84f70ea":"code","356e3cc2":"code","67f316ef":"code","1dbf96c9":"code","815f4040":"code","de344bdd":"code","ce808dd7":"code","6ed494df":"code","481dc06c":"code","83fe9e83":"code","b808f11f":"code","5eb293b8":"code","eb12a108":"code","c39190dd":"code","a936900e":"code","fc684304":"code","8257c493":"code","014201af":"code","0fb853e0":"code","a7f53c86":"code","245b97c6":"code","768eb1f3":"code","cf64173a":"code","6d903cac":"code","2e1fba38":"code","0a148e19":"code","d23f157b":"code","b050b89c":"code","31d1b2e7":"code","0736b3f0":"code","51efc85b":"code","b0e9113c":"code","65e5c0ea":"code","be3c3b89":"code","7679fcf8":"code","6532824c":"code","00a87537":"code","12f54f00":"code","ab4ee219":"code","663f7bcd":"code","aabbe6ce":"code","8c1e0acb":"code","8d9b55b3":"code","71c9cb91":"code","b9650653":"code","bdf1b576":"code","65b38eb0":"code","93d90a7f":"code","849abb7f":"code","e55f6927":"code","54f96869":"code","b5b34462":"code","cd7f489a":"code","a4287840":"code","73ebe992":"code","415f6ebe":"code","153c120d":"code","0de43ebe":"code","81515c8e":"markdown","5c536d98":"markdown","bc0be326":"markdown","42fb866a":"markdown","0072c0d4":"markdown","b0e36701":"markdown","14bce9ed":"markdown","85a729ab":"markdown","bd9de295":"markdown","5f10c7ef":"markdown","d5362465":"markdown","a630a81c":"markdown","006d7318":"markdown","d997ba09":"markdown"},"source":{"f583afe3":"import pandas as pd\nimport numpy as np","327961b9":"df_train = pd.read_csv('\/kaggle\/input\/liverpool-ion-switching\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/liverpool-ion-switching\/test.csv')\ndf_train.shape, df_test.shape","22cc54bd":"import matplotlib.pyplot as plt","e51d9348":"plt.figure(figsize=(16,4))\nplt.title('Train signal and trend')\nplt.plot(df_train['signal'], label='signal')\nplt.plot(df_train['open_channels'], label='target', alpha=0.5)\nplt.legend()","66b4aa23":"plt.figure(figsize=(16,4))\nplt.title('Test signal')\nplt.plot(df_test['signal'])","e0b7861c":"def prepare(df, limits):\n    df['batch_idx'] = np.zeros_like(df['signal'], dtype=np.int8)\n    df['local_time'] = np.zeros_like(df['signal'], dtype=np.int8)\n    for idx, start, finish in zip(range(len(limits)-1), limits[:-1], limits[1:]):\n        mask = np.arange(start, finish)\n        df.loc[mask, 'batch_idx'] = idx\n        df.loc[mask, 'local_time'] = df.loc[mask, 'time'] - df.loc[mask, 'time'].min()","21c531c1":"TRAIN_LIMITS = [\n    0,\n    500_000,\n    600_000,\n    1_000_000,\n    1_500_000,\n    2_000_000,\n    2_500_000,\n    3_000_000,\n    3_500_000,\n    # 3_642_000,\n    # 3_823_000,\n    4_000_000,\n    4_500_000,\n    5_000_000,\n]\nprepare(df_train, TRAIN_LIMITS)\nplt.plot(df_train['batch_idx'])\ndf_train['batch_idx'].unique().shape","bb155a2c":"TEST_LIMITS = [\n    0,\n    100_000,\n    200_000,\n    300_000,\n    400_000,\n    500_000,\n    600_000,\n    700_000,\n    800_000,\n    900_000,\n    1_000_000,\n    1_500_000,\n    2_000_000,\n]\nprepare(df_test, TEST_LIMITS)\nplt.plot(df_test['batch_idx'])\ndf_test['batch_idx'].unique().shape","143e7477":"df_train.head()","21df5233":"TRAIN_DEGREES = [0, 1, 0, 0, 0, 0, 0, 2, 2, 2, 2]\nTEST_DEGREES = [1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 2, 0]","5bcb4770":"from sklearn.linear_model import LinearRegression","46cf2ed3":"def calculate_trend(df, degrees, roll=None):\n    if roll is not None:\n        df['trend'] = np.zeros(df.shape[0])\n    df['trend_degree'] = df['batch_idx'].apply(lambda x: degrees[x]).astype(np.int)\n    for cn in ['trend_lr', 'w0', 'w1', 'w2']:\n        df[cn] = np.zeros(df.shape[0])\n    for idx in df['batch_idx'].unique():\n        print('series #', idx)\n        mask = df['batch_idx'] == idx\n        time = df.loc[mask, 'local_time'].values.reshape(-1, 1)\n        time_2 = np.hstack([time, time ** 2])\n        signal = df.loc[mask, 'signal']\n        if roll is not None:\n            print('rolling...')\n            df.loc[mask, 'trend'] = signal.rolling(roll, center=True, win_type='parzen').mean()\n        print('regression...')\n        reg = LinearRegression()\n        if degrees[idx] == 2:\n            reg.fit(time_2, signal)\n            df.loc[mask, 'trend_lr'] = reg.predict(time_2)\n            df.loc[mask, 'w2'] = reg.coef_[1]\n        else:\n            reg.fit(time, signal)\n            df.loc[mask, 'trend_lr'] = reg.predict(time)\n        df.loc[mask, 'w1'] = reg.coef_[0]\n        df.loc[mask, 'w0'] = reg.intercept_\n        print('coefs: ', reg.coef_, reg.intercept_)\n    if roll is not None:\n        df['signal-trend'] = df['signal'] - df['trend']\n    df['signal-trend_lr'] = df['signal'] - df['trend_lr']","c2ab2b70":"calculate_trend(df_train, TRAIN_DEGREES, roll=None)","71bb7c9c":"df_train.head()","5d432bd2":"def plot_graph(df, test=False):\n    plt.figure(figsize=(16,4))\n    plt.plot(df['signal'], label='signal', alpha=0.5)\n    plt.plot(df['batch_idx'], label='batch_idx', alpha=0.8)\n    if 'trend' in df.columns:\n        plt.plot(df['trend'], label='trend', alpha=0.8)\n        plt.plot(df['signal-trend'], label='signal-trend', alpha=0.5)\n    if 'trend_lr' in df.columns:\n        plt.plot(df['trend_lr'], label='trend_lr', alpha=0.8)\n        plt.plot(df['signal-trend_lr'], label='signal-trend_lr', alpha=0.5)\n    if not test:\n        plt.plot(df['open_channels'], label='open_channels', alpha=0.5)\n    plt.legend(loc='upper left')","4efa73bf":"plot_graph(df_train)","a2dfd475":"plot_graph(df_train[996_000:998_000])","b28a707e":"plot_graph(df_train[15_250:15_800])","e7d670a8":"plot_graph(df_train[503_000:505_000])","daa4050c":"calculate_trend(df_test, TEST_DEGREES, roll=None)","843d654e":"plot_graph(df_test, test=True)","13b049c3":"from scipy.fft import fft, fftfreq, ifft","136914dc":"def total_fft(df, sig='signal-trend_lr', limits=(-100, 100), logy=False, d=1e-4):\n    sig_fft = fft(df[sig].values)\n    power = np.abs(sig_fft)\n    freq = fftfreq(df.shape[0], d=d)\n    plt.figure(figsize=(12,4))\n    l, r = limits\n    mask = np.where((l <= freq) & (freq <= r))\n    plt.plot(freq[mask], power[mask])\n    plt.grid()\n    if logy:\n        plt.yscale('log')","b84f70ea":"total_fft(df_train)","356e3cc2":"total_fft(df_test)","67f316ef":"total_fft(df_train, 'open_channels', (45, 55), logy=False)","1dbf96c9":"def batch_ffts(df, sig='signal-trend_lr', limits=(-5_000, 5_000), logy=False, d=1e-4):\n    n_rows, n_cols = 4, 3\n    fig, axs = plt.subplots(n_rows, n_cols, figsize=(20,20))\n    for bi, dfg in df.groupby('batch_idx'):\n        idx_row, idx_col = bi \/\/ n_cols, bi % n_cols\n        ax = axs[idx_row][idx_col]\n        ax.set_title(f'batch {bi}')\n        sig_fft = fft((dfg[sig] - dfg[sig].mean()).values)\n        power = np.abs(sig_fft)\n        freq = fftfreq(dfg.shape[0], d=d)\n        l, r = limits\n        mask = np.where((l <= freq) & (freq <= r))\n        ax.plot(freq[mask], power[mask])\n        ax.grid()\n        if logy:\n            ax.set_yscale('log')","815f4040":"batch_ffts(df_train, limits=(45, 55))","de344bdd":"batch_ffts(df_train, limits=(0, 0.2))","ce808dd7":"batch_ffts(df_train, 'open_channels', (45, 55), logy=True)","6ed494df":"batch_ffts(df_train, 'open_channels', (0, 0.2), logy=True)","481dc06c":"batch_ffts(df_test, limits=(45, 55))","83fe9e83":"batch_ffts(df_test, limits=(0, 0.2))","b808f11f":"np.random.seed(42)\n\ndef filter_freq(df, sig='signal-trend_lr', means=[50], widths=[0.4],\n                batch_idxs=None, limits=(-100, 100), d=1e-4):\n    total_fft(df, sig, limits)\n    for bi, dfg in df.groupby('batch_idx'):\n        sig_fft = fft(dfg[sig].values)\n        freq = fftfreq(dfg.shape[0], d=d)\n        for mean, width in zip(means, widths):\n            if batch_idxs is not None and bi not in batch_idxs:\n                continue\n            l, r = mean - width\/2, mean + width\/2\n            print('batch', bi, 'limits', l, r)\n            mask = (np.abs(freq) > l) & (np.abs(freq) <= r)\n            sig_fft[mask] = 0\n        power = np.abs(sig_fft)\n        mask = df['batch_idx'] == bi\n        df.loc[mask, sig+'-f'] = ifft(sig_fft)\n    df[sig+'-f'] = df[sig+'-f'].apply(lambda x: x.real)\n    total_fft(df, sig+'-f', limits)","5eb293b8":"filter_freq(df_train, means=[50, 0.04], \n            widths=[0.4, 0.01], limits=(45, 55))","eb12a108":"batch_ffts(df_train, 'signal-trend_lr-f', (45, 55))","c39190dd":"filter_freq(df_test, means=[50, 0.04], \n            widths=[0.4, 0.01], limits=(0, 0.2))","a936900e":"batch_ffts(df_test, 'signal-trend_lr-f', (0, 0.2))","fc684304":"plt.title('Signal delta')\nplt.plot(df_train.loc[np.arange(2_000_000, 2_005_000), 'signal-trend_lr'] - df_train.loc[np.arange(2_000_000, 2_005_000), 'signal-trend_lr-f'])","8257c493":"plt.title('Signal delta')\nplt.plot(df_train.loc[np.arange(1_998_000, 2_002_000), 'signal-trend_lr'] - df_train.loc[np.arange(1_998_000, 2_002_000), 'signal-trend_lr-f'])","014201af":"def plot_batch_filtered(df, batch_idx=0):\n    dfb = df[df['batch_idx'] == batch_idx]\n    plt.figure(figsize=(16,4))\n    plt.title(f'Batch {batch_idx}')\n    plt.plot(dfb['signal-trend_lr'], label='signal', alpha=0.5)\n    plt.plot(dfb['signal-trend_lr-f'], label='filtered', alpha=0.5)\n    plt.legend()","0fb853e0":"plot_batch_filtered(df_train, 7)\nplot_batch_filtered(df_train, 10)","a7f53c86":"plot_batch_filtered(df_train, 0)\nplot_batch_filtered(df_train, 2)","245b97c6":"def batch_signals(df, logy=False):\n    n_rows, n_cols = 4, 3\n    fig, axs = plt.subplots(n_rows, n_cols, figsize=(20,20))\n    for bi in df['batch_idx'].unique():\n        print(f'----- batch {bi} -----')\n        mask = df['batch_idx'] == bi\n        idx_row, idx_col = bi \/\/ n_cols, bi % n_cols\n        ax = axs[idx_row][idx_col]\n        ax.set_title(f'batch {bi}')\n        if 'open_channels' in df.columns:\n            means = []\n            for label in np.sort(df.loc[mask, 'open_channels'].unique()):\n                mask_label = df['open_channels'] == label\n                means += [df.loc[mask & mask_label, 'signal-trend_lr-f'].mean()]\n                ax.hist(df.loc[mask & mask_label, 'signal-trend_lr-f'], bins=100, alpha=0.6, label=label)\n            ax.legend()\n            print('means:', means)\n        else:\n            ax.hist(df.loc[mask, 'signal-trend_lr-f'], bins=200)\n        ax.grid()\n        if logy:\n            ax.set_yscale('log')","768eb1f3":"batch_signals(df_train)","cf64173a":"batch_signals(df_train, logy=True)","6d903cac":"batch_signals(df_test)","2e1fba38":"batch_signals(df_test, logy=True)","0a148e19":"TRAIN_COMPS = [20, 20, 20, 25, 40, 90, 60, 25, 40, 60, 90]\nTEST_COMPS = [20, 40, 60, 20, 25, 90, 60, 90, 20, 40, 20, 20]","d23f157b":"train_comp_dict = {k: v for k, v in zip(range(11), TRAIN_COMPS)}\ntest_comp_dict = {k: v for k, v in zip(range(12), TEST_COMPS)}","b050b89c":"from collections import defaultdict\ntrain_comp_dict_inv = defaultdict(list)\ntest_comp_dict_inv = defaultdict(list)","31d1b2e7":"for k, v in train_comp_dict.items():\n    train_comp_dict_inv[v].append(k)\nfor k, v in test_comp_dict.items():\n    test_comp_dict_inv[v].append(k)","0736b3f0":"train_comp_dict_inv, test_comp_dict_inv","51efc85b":"from imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.mixture import GaussianMixture","b0e9113c":"plt.figure(figsize=(12,4))\nplt.title('Means')\n\ndf_train['signal_ground'] = np.zeros(df_train.shape[0], dtype=np.float32)\ndf_test['signal_ground'] = np.zeros(df_test.shape[0], dtype=np.float32)\ndf_train['n_components'] = np.zeros(df_train.shape[0], dtype=np.byte)\ndf_test['n_components'] = np.zeros(df_test.shape[0], dtype=np.byte)\n\ndeltas = []\nmeans_dict = {}\nfor group in sorted(train_comp_dict_inv.keys()):\n    n_components = group \/\/ 10\n    train_batch_idxs = train_comp_dict_inv[group]\n    mask = df_train['batch_idx'].isin(train_batch_idxs)\n    if n_components == 9:\n        mask = mask & df_train['open_channels'].isin(range(2, 11))\n    X_train = df_train.loc[mask, 'signal-trend_lr-f'].values.reshape(-1, 1)\n    y_train = df_train.loc[mask, 'open_channels'].values.reshape(-1, 1)\n    samp = RandomUnderSampler(random_state=42)\n    X_train_samp, _ = samp.fit_sample(X_train, y_train)\n    print(f'----- Resampled to {X_train_samp.shape[0]} samples -----')\n    print(f'Fitting mixture of {n_components} ...')\n    gm = GaussianMixture(n_components=n_components, covariance_type='tied', random_state=42, verbose=1)\n    gm.fit(X_train_samp)\n    means = gm.means_[:, 0]\n    means_dict[group] = means\n    print('Means(sorted):', sorted(means))\n    plt.bar(means - np.min(means), [n_components] * means.shape[0], \n            alpha=0.5, width=0.1, label=group)\n    deltas += [b - a for a, b in zip(sorted(means)[:-1], sorted(means)[1:])]\n    ground = np.min(means)\n    mask = df_train['batch_idx'].isin(train_batch_idxs)\n    df_train.loc[mask, ['signal_ground', 'n_components']] = ground, n_components\n    test_batch_idxs = test_comp_dict_inv[group]\n    mask = df_test['batch_idx'].isin(test_batch_idxs)\n    df_test.loc[mask, ['signal_ground', 'n_components']] = ground, n_components\n\nplt.legend()","65e5c0ea":"plt.hist(deltas, bins=5)","be3c3b89":"delta = np.median(deltas)\ndelta","7679fcf8":"df_train['gm_label'] = np.zeros(df_train.shape[0], dtype=np.int8)\ndf_test['gm_label'] = np.zeros(df_test.shape[0], dtype=np.int8)\n\nfor group in sorted(train_comp_dict_inv.keys()):\n    n_components = group \/\/ 10\n    if n_components != 9:\n        continue\n    train_batch_idxs = train_comp_dict_inv[group]\n    mask = df_train['batch_idx'].isin(train_batch_idxs)\n    df_train.loc[mask, 'signal_ground'] -= 2*delta\n    test_batch_idxs = test_comp_dict_inv[group]\n    mask = df_test['batch_idx'].isin(test_batch_idxs)\n    df_test.loc[mask, 'signal_ground'] -= 2*delta","6532824c":"df_train['signal_ground'].unique()","00a87537":"df_train['signal-trend_lr-ground'] = df_train['signal-trend_lr-f'] - df_train['signal_ground']\ndf_test['signal-trend_lr-ground'] = df_test['signal-trend_lr-f'] - df_test['signal_ground']","12f54f00":"X_train = df_train['signal-trend_lr-ground'].values.reshape(-1, 1)\ny_train = df_train['open_channels'].values.reshape(-1, 1)\nsamp = RandomUnderSampler(random_state=42)\nX_train_samp, _ = samp.fit_sample(X_train, y_train)\nplt.figure()\nplt.title('Resampled train')\nplt.hist(X_train_samp, bins=100)\ngm = GaussianMixture(n_components=11, means_init=np.arange(11).reshape(-1, 1)*delta,\n                     random_state=42, verbose=1)\ngm.fit(X_train_samp)\ndf_train['gm_label'] = gm.predict(X_train)\nmeans = gm.means_[:, 0]\nprint('Means:', means)\nprint('Mean weights:', [f'{k} : {v}' for k, v in zip(means, gm.weights_)])\nplt.figure()\nplt.title('Means GMM')\nplt.bar(means - np.min(means), [n_components] * means.shape[0], \n        width=0.1, label=group)\ndeltas = [b - a for a, b in zip(sorted(means)[:-1], sorted(means)[1:])]\nprint('Deltas', deltas)\nprint('Delta', np.median(deltas))\nX_test = df_test['signal-trend_lr-ground'].values.reshape(-1, 1)\ndf_test['gm_label'] = gm.predict(X_test)","ab4ee219":"df_train['gm_label'].unique()","663f7bcd":"df_test['gm_label'].unique()","aabbe6ce":"set(df_train['gm_label'].unique()).difference(df_test['gm_label'].unique())","8c1e0acb":"set(df_test['gm_label'].unique()).difference(df_train['gm_label'].unique())","8d9b55b3":"plt.figure(figsize=(6,6))\nplt.imshow(df_train.corr())\nplt.xticks(range(df_train.shape[1]), df_train.columns, rotation=75)\nplt.yticks(range(df_train.shape[1]), df_train.columns)\nplt.colorbar()","71c9cb91":"df_train.corr()['open_channels']","b9650653":"error = df_train['open_channels'] - df_train['gm_label']","bdf1b576":"plt.figure(figsize=(16,4))\nplt.plot(error.loc[np.arange(2_000_600,2_000_700)])","65b38eb0":"np.mean(df_train['open_channels'] == df_train['gm_label'])","93d90a7f":"from sklearn.metrics import f1_score","849abb7f":"f1_score(df_train['open_channels'], df_train['gm_label'], average='macro')","e55f6927":"from sklearn.metrics import classification_report","54f96869":"print(classification_report(df_train['open_channels'], df_train['gm_label'], digits=3))","b5b34462":"def plot_graph(df, mask, levels=None, show_batch=False):\n    plt.figure(figsize=(16, 8))\n    plt.plot(df.loc[mask, 'signal-trend_lr-ground'], label='signal', alpha=0.9)\n    if levels is not None:\n        for lid, (level, level_next) in enumerate(zip(levels[:-1], levels[1:])):\n            color = 'red' if lid % 2 == 0 else 'blue'\n            plt.axhline((level + level_next) \/ 2, color=color, alpha=0.5, linestyle='--')\n    if 'open_channels' in df.columns:\n        plt.plot(df.loc[mask, 'open_channels'], label='open_channels', alpha=0.9)\n    if 'gm_label' in df.columns:\n        plt.plot(df.loc[mask, 'gm_label'], label='gm_label', alpha=0.5)\n    if show_batch:\n        plt.plot(df.loc[mask, 'batch_idx'], label='batch_idx', alpha=0.8)\n    plt.grid()\n    plt.legend(loc='upper left')","cd7f489a":"plot_graph(df_train, np.arange(2_000_600, 2_000_700), means)","a4287840":"plot_graph(df_train, np.arange(2_001_600, 2_001_700), means)","73ebe992":"y_pred = df_test['gm_label']","415f6ebe":"subm = pd.read_csv('\/kaggle\/input\/liverpool-ion-switching\/sample_submission.csv')","153c120d":"subm['open_channels'] = y_pred","0de43ebe":"SUBM = 'subm_111'\nFN = f'{SUBM}.csv'\nFN_ZIP = f'{FN}.zip'\ncompression_opts = dict(method='zip', archive_name=FN) \nsubm.to_csv(FN_ZIP, index=False,\n            float_format='%.4f',\n            compression=compression_opts)","81515c8e":"- frequencies to filter:\n  - 50Hz - all batches\n  - 0.04Hz - batches 7-10 (quadratic)","5c536d98":"## 4. Unsupervised cluster learning\n\nCore ideas used in this section:\n- undersampling to achieve the balance of the classes\n- gaussian distributions of the samples in each class:\n  - usage of the gaussian mixture (GM) models to find the cluster means on train and to separate the samples on test\n  - different models for batches with different number of the classes\n    - for batches with number of the classes > `6` (batch_idx == 5 or 10) let's use model with n_components=9 (ignoring classes `0` and `1` because there is very few samples of classes `0` and `1`) and then perform calculation of the means of the classes `0` and `1` using the mean of the class deltas\n- calculation of the \"signal ground\" (the mean of the class `0`) and then substracting it from the signal\n- final single GM model for all the batches\n  - usage of the predefined means for 'good' class labels (target-like)","bc0be326":"## 2. Trend removal\n\nThe idea is very simple - let's just fit different linear regression models with degrees 0 (constant), 1 (linear) and 2 (quadratic) for different batches and then substract this trend from the signal","42fb866a":"## 5. Target prediction\n\nHere is a GM label used as a target prediction","0072c0d4":"###### Fitting different GM models","b0e36701":"## 1. EDA and manual batch detection\n\nThere are batches with different durations (not only 500_000 samples) - let's set them manually.","14bce9ed":"# Ion switching: signal transformation to hit 0.92\n\n1. EDA and manual batch detection\n2. Trend removal\n3. Noise detection and filtering\n4. Unsupervised cluster learning\n5. Target prediction","85a729ab":"###### Fitting final single GM model for all samples","bd9de295":"#### Naive filtering: setting frequency amp to zero","5f10c7ef":"- 50Hz local peaks in train and test signals","d5362465":"## 3. Noise detection and filtering\n\nCore ideas used in this section:\n\n- usage of Fast Fourier Transform (FFT) to detect the local peaks in the signal and the target;\n- if the local peak is present in the signal and absent in the target - it's a noise (systematic error) and has to be filtered","a630a81c":"- there is no 50Hz local peak in target","006d7318":"- no 50Hz or 0.04Hz local peaks in target","d997ba09":"###### Target classes distributions in batches"}}