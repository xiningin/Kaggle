{"cell_type":{"27e649b3":"code","b250fc21":"code","2601ce8d":"code","c241f04b":"code","745a9c90":"code","168b65a7":"code","ed741d2e":"code","371298ed":"code","7a17ff1a":"code","ede48d9e":"code","e59ecc50":"code","0c04ff8c":"code","108533fa":"code","aa4a4752":"code","f48a3258":"code","6563d26b":"code","d340aeb9":"code","a54c9c40":"markdown","2fd5014d":"markdown","4c283e72":"markdown","91b510b5":"markdown","5f11e0c2":"markdown","12a3685b":"markdown","ccdd249d":"markdown"},"source":{"27e649b3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# Any results you write to the current directory are saved as output.","b250fc21":"import os\nimport zipfile\nimport random\nimport tensorflow as tf\nimport shutil\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom shutil import copyfile\nfrom os import getcwd","2601ce8d":"test_zip = '\/kaggle\/input\/dogs-vs-cats\/test1.zip'\ntrain_zip = '\/kaggle\/input\/dogs-vs-cats\/train.zip'\nzip_ref = zipfile.ZipFile(test_zip, 'r')\nzip_ref.extractall('\/kaggle\/temp')\nzip_ref.close()\n\nzip_ref = zipfile.ZipFile(train_zip, 'r')\nzip_ref.extractall('\/kaggle\/temp')\nzip_ref.close()","c241f04b":"print(len(os.listdir('\/kaggle\/temp\/train')))\nprint(len(os.listdir('\/kaggle\/temp\/test1')))\nos.listdir(\"\/kaggle\/temp\")","745a9c90":"from tensorflow.keras.preprocessing import image\nimport matplotlib.pyplot as plt\n\ncount=0\nplt.figure(figsize=(10,10))\nfor i in np.random.randint(1, 500, size=25):\n    count += 1\n    image_path = '\/kaggle\/temp\/train\/dog.'+ str(i) +'.jpg'\n    img = image.load_img(image_path, target_size=(150, 150))\n    x = image.img_to_array(img)\n\n    x = tf.expand_dims(x, axis=0)\n    x = x \/ 255.0\n    plt.subplot(5, 5, count)\n    plt.axis('off')\n    plt.imshow(x[0])","168b65a7":"orig_train_data_dir = \"\/kaggle\/temp\/train\"\norig_test_data_dir = \"\/kaggle\/temp\/test1\"\n\ntry:\n    base_dir = \"\/kaggle\/Cats_vs_Dogs\/\"\n    os.mkdir(base_dir)\n\n    train_dir = os.path.join(base_dir, 'train')\n    os.mkdir(train_dir)\n    test_dir = os.path.join(base_dir, 'test')\n    os.mkdir(test_dir)\n\n    train_dogs_dir = os.path.join(train_dir, 'dogs')\n    train_cats_dir = os.path.join(train_dir, 'cats')\n    os.mkdir(train_dogs_dir)\n    os.mkdir(train_cats_dir)\n\n    test_dogs_dir = os.path.join(test_dir, 'dogs')\n    test_cats_dir = os.path.join(test_dir, 'cats')\n    os.mkdir(test_dogs_dir)\n    os.mkdir(test_cats_dir)\n\n    fnames = ['cat.{}.jpg'.format(i) for i in range(6250)] \n    for fname in fnames:\n        src = os.path.join(orig_train_data_dir, fname) \n        dst = os.path.join(train_cats_dir, fname) \n        shutil.copyfile(src, dst)\n        \n    fnames = ['dog.{}.jpg'.format(i) for i in range(6250)] \n    for fname in fnames:\n        src = os.path.join(orig_train_data_dir, fname) \n        dst = os.path.join(train_dogs_dir, fname) \n        shutil.copyfile(src, dst)\nexcept:\n    pass        \nfnames = os.listdir(orig_test_data_dir) \nfor fname in fnames:\n    src = os.path.join(orig_test_data_dir, fname) \n    dst = os.path.join(test_dir, fname) \n    shutil.copyfile(src, dst)\n","ed741d2e":"train_datagen = ImageDataGenerator(rescale=1.\/255, validation_split=0.2)\ntest_datagen = ImageDataGenerator(rescale=1.\/255)\n\ntrain_generator = train_datagen.flow_from_directory(\n        train_dir, \n        target_size=(150, 150),\n        batch_size=250,\n        class_mode = 'binary',\n        subset='training')\n\nvalidation_generator = train_datagen.flow_from_directory(\n        train_dir,\n        target_size=(150, 150),\n        batch_size=250,\n        class_mode='binary',\n        subset='validation')\n\ntest_generator = test_datagen.flow_from_directory(\n        test_dir,\n        target_size=(150, 150),\n        batch_size=250)","371298ed":"for data_batch, labels_batch in train_generator:\n    print(data_batch.shape)\n    print(labels_batch.shape)\n    break","7a17ff1a":"from tensorflow.keras import models\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import Callback, LearningRateScheduler\nfrom tensorflow.keras.applications import Xception\n\n\nconv_base = Xception(weights=\"\/kaggle\/input\/xception\/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\", include_top=False, input_shape=(150, 150, 3))","ede48d9e":"conv_base.summary()","e59ecc50":"batch_size = 250\n\ndef extract_features(generator, sample_count):\n    features = np.zeros(shape=(sample_count, 5, 5, 2048))\n    labels = np.zeros(shape=(sample_count))\n    i = 0\n    for inputs_batch, labels_batch in generator:\n        features_batch = conv_base.predict(inputs_batch)\n        features[i * batch_size : (i+1) * batch_size] = features_batch\n        labels[i * batch_size : (i+1) * batch_size] = labels_batch\n        i += 1\n        if i * batch_size >= sample_count:\n            break\n    return features, labels\n\ntrain_features, train_labels = extract_features(train_generator, 10000) \nvalidation_features, validation_labels = extract_features(validation_generator, 2500)","0c04ff8c":"from tensorflow.keras.optimizers import Adam\n\nclass myCallback(Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        if (logs.get('accuracy') > 0.999):\n            self.model.stop_training = True\n            \ncallback = myCallback()\n\nmodel = models.Sequential([\n    layers.Flatten(input_shape=(5, 5, 2048)),\n    layers.Dense(256, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=1e-5),metrics=['accuracy'])","108533fa":"history = model.fit(train_features, train_labels, epochs=50, batch_size=batch_size, validation_data=(validation_features, validation_labels),callbacks=[callback])","aa4a4752":"acc = history.history[\"accuracy\"]\nval_acc = history.history[\"val_accuracy\"]\nloss = history.history[\"loss\"]\nval_loss = history.history[\"val_loss\"]\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, \"bo\", label=\"Training accuracy\")\nplt.plot(epochs, val_acc, \"b\", label=\"Validation accuracy\")\nplt.title(\"Training and validation accuracy\")\nplt.legend()\n\nplt.figure()\nplt.plot(epochs, loss, \"bo\", label=\"Training loss\")\nplt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\nplt.title(\"Training and validation loss\")\nplt.legend()","f48a3258":"test_features = conv_base.predict(test_generator)","6563d26b":"pred = model.predict(test_features)","d340aeb9":"ans=[]\nfor i in range(pred.shape[0]):\n    if pred[i] > 0.5:\n        ans.append(1)\n    else:\n        ans.append(0)","a54c9c40":"Since this project requires CPU alone, feature extraction will be the most computational task.","2fd5014d":"<h2>Data generation:<\/h2>\nFirst step is creating folders for data generation","4c283e72":"Creating a validation and training data generator","91b510b5":"Check few images:","5f11e0c2":"Becuase we are using transfer learning, we need only half of training data to create a model with hight accuracy.","12a3685b":"<h2>Applying transfer learning<\/h2>","ccdd249d":"# <h2>Importing data: <\/h2>\nWe begin with importing the essential libraries for our project"}}