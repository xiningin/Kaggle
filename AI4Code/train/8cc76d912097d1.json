{"cell_type":{"885164b8":"code","a2e85fe0":"code","4981c391":"code","b94bcaeb":"code","b0544fd9":"code","1195657c":"code","5c865e19":"code","05ddd02f":"code","e082ec5f":"code","ec7a2444":"code","62120274":"code","dc5d8530":"code","2b192060":"code","7a47f6e9":"code","1eb16f00":"markdown","a2c7029d":"markdown","9cf35886":"markdown","c3115ca5":"markdown","d9cf131f":"markdown","5abedf6b":"markdown","e4ca9aa5":"markdown","b66caff6":"markdown","520c5859":"markdown","b7bc90e5":"markdown","92c48166":"markdown","c4981f86":"markdown","5663148e":"markdown","d8a79bc6":"markdown","7d81c13a":"markdown","18bdfab2":"markdown","2d0b57da":"markdown"},"source":{"885164b8":"import math\nimport numpy as np\nimport pandas as pd\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\n\n# ;-)\nseed = sum(map(ord,\"JF\"))","a2e85fe0":"# get data \ndf_train = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\ny_train = df_train[\"label\"].to_numpy()\nx_train = df_train.drop(labels=[\"label\"], axis=1).to_numpy()\ndel df_train\n\n# normalize \nx_train = x_train\/255.0\n\n# split\nx_train, x_valid, y_train, y_valid = \\\n    train_test_split(x_train, y_train, test_size=0.1, random_state=seed)\n\n# shapes\nprint(x_train.shape, y_train.shape)\nprint(x_valid.shape, y_valid.shape)","4981c391":"def plot_images(images, num_images):\n    # images\n    num_rows = math.floor(math.sqrt(num_images))\n    num_cols = math.ceil(num_images\/num_rows)\n    for i in range(num_images):\n        reshaped_image = images[i].reshape(28,28)\n        plt.subplot(num_rows, num_cols, i+1)\n        plt.imshow(reshaped_image, cmap=plt.cm.Greys_r)\n        plt.axis('off')\n    plt.show()\n    \n# images \nplot_images(x_train, 20)\n# plot_images(x_valid, 20)","b94bcaeb":"def plot_labels(labels):\n    # labels\n    plt.title(\"Labels\")\n    sns.countplot(x=labels)\n    plt.show()\n\n# labels\nplot_labels(y_train)\n# plot_labels(y_valid)","b0544fd9":"# features reduced from 784 to 32\npca = PCA(n_components=32, random_state=seed)\n\nx_train_pca = pca.fit_transform(x_train)\nx_valid_pca = pca.transform(x_valid)\n\n# new shapes\nprint(x_train_pca.shape)\nprint(x_valid_pca.shape)","1195657c":"def plot_pca_decomposition(pca_features, labels, c1=0, c2=1):\n    plt.figure(figsize=(12,6)); plt.title(\"PCA decomposition\")\n    plt.xlabel('component 1');  plt.ylabel('component 2')\n    sns.scatterplot(x=pca_features[:,0], y=pca_features[:,1], hue=labels, \n                    palette=sns.color_palette(\"Spectral\", 10), alpha=0.9)\n\n# plot pca\nplot_pca_decomposition(x_train_pca, y_train)\n# plot_pca_decomposition(x_valid_pca, y_valid)","5c865e19":"# create model & train\nlinear = SVC(C=0.1, kernel='linear', random_state=seed)\nlinear.fit(x_train_pca, y_train)\n\n# score ~0.925\nprint(f'{linear.score(x_valid_pca, y_valid):.4f}')","05ddd02f":"# create model & train\npoly = SVC(C=1, kernel='poly', random_state=seed, probability=True)\npoly.fit(x_train_pca, y_train)\n\n# score ~0.979\nprint(f'{poly.score(x_valid_pca, y_valid):.4f}')","e082ec5f":"# create model & train\nrbf = SVC(C=10, kernel='rbf', random_state=seed, probability=True)\nrbf.fit(x_train_pca, y_train)\n\n# score ~0.985\nprint(f'{rbf.score(x_valid_pca, y_valid):.4f}')","ec7a2444":"def avg_probs(x):\n    return np.mean((poly.predict_proba(x)*0.2,\n                    rbf.predict_proba(x)*0.8), axis=0)\n\nmeta = SVC(C=0.1, kernel='sigmoid', random_state=seed)\nmeta.fit(avg_probs(x_train_pca), y_train)\n\ny_preds = meta.predict(avg_probs(x_valid_pca))\nprint(f'{accuracy_score(y_valid, y_preds):.4f}')","62120274":"def plot_confusion_mtx(mtx):\n    plt.figure(figsize=(12,6)); plt.title(\"Confusion matrix\")\n    sns.heatmap(mtx, annot=True, cmap=\"Blues\", fmt=\"d\");\n    plt.ylabel(\"True label\"); plt.xlabel(\"Predicted label\")\n\n# main issues: 5 vs 3 & 4 vs 9\nplot_confusion_mtx(confusion_matrix(y_valid, y_preds))","dc5d8530":"# let's see some of the misclassified digits\nindices = (y_preds != y_valid) \nplot_images(x_valid[indices], 6)\nprint(f'Correct: {y_valid[indices][:6]}, Predicted: {y_preds[indices][:6]}') ","2b192060":"# get & normalize test data\nx_test = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\nx_test = x_test\/255.0\n\n# apply pca & predict\nx_test_pca = pca.transform(x_test)\nresults = meta.predict(avg_probs(x_test_pca))\n\n# store submission file\nsubmission = pd.concat([pd.Series(range(1, 28001), name=\"ImageId\"), \n                        pd.Series(results, name=\"Label\")], axis=1)\nsubmission.to_csv(\"digit-recognizer-submit.csv\", index=False)","7a47f6e9":"submission.head()","1eb16f00":"### SVM with RBF Kernel\n\nSame as above but using an RBF Kernel, see: [Radial_basis_function](https:\/\/en.wikipedia.org\/wiki\/Radial_basis_function). This is a more powerful model, but it easily overfits, leading to poor generalization, that's why the C penalty is incremented.","a2c7029d":"### SVM with polynomial (degree=3) Kernel\n\nNow we are applying the kernel trick, see: [Polynomial_kernel](https:\/\/en.wikipedia.org\/wiki\/Polynomial_kernel). A degree=3 poly kernel is used and better accuracy is achieved.\n\nNote: the 'probability=True' flag, it's for later usage.","9cf35886":"The aim of this notebook is not to achieve a perfect score, but rather to 'play' with principal component analysis + support vector machines, see how well they perform on this task, and establish a base-line benchmark for more complex methods. If you're interested in a 'decent' accuracy (CNNs) go to my other notebook:\n[Digit Recognizer - part 2: using CNNs](https:\/\/www.kaggle.com\/jorgefalcon\/digit-recognizer-cnn)","c3115ca5":"# Digit Recognizer - part 1: SVM with PCA","d9cf131f":"### Visualize PCA results","5abedf6b":"<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/2\/27\/MnistExamples.png\" alt=\"MNIST\" width=\"500\"\/>","e4ca9aa5":"### Plot confusion matrix","b66caff6":"Linear SVMs are amongst the simplest yet useful techniques you can employ in machine learning. See: [Support-vector_machine](https:\/\/en.wikipedia.org\/wiki\/Support-vector_machine).\n\nSince it is a multiclass classification problem, we are employing the 'OVR' strategy, which means that behind the scenes there are actually 10 SVMs with a yes \/ no task, one to identify each number, Sklearn (as well as Keras) really shine to hide the complexity, which is wonderful in general, but not that great for learning.\n\nNote that the input to our linear model is the transformed PCA representation of the original raw feature vectors.","520c5859":"### Meta model\n\nFinally, we take the weighted averages of the Polynomial and RBF probabilities (that's why we activate the probability=True flag) and feed them as input to a sigmoid SVM, which acts as a metamodel - it trains on the previous model's outputs and the expected targets -.\n\n**An accuracy of 98.6% is archived**, which is 'just ok' for the kind of models we are dealing with, but unacceptable for more complex architectures (at least for this 'toy problem').","b7bc90e5":"### Visualize data","92c48166":"### Fit a basic linear SVM","c4981f86":"### Apply dimensionality reduction\n\nWe are reducing the flattened version of each image - a 784 feature vector - to a more compact 32-dimensional representation, trying to keep as much information as possible, see: [Principal_component_analysis](https:\/\/en.wikipedia.org\/wiki\/Principal_component_analysis). This will help our SVM train better. \n\nNote: PCA is often not done when using larger NNs since part of each layer's job is to find a useful and more compact representation of the data. However, there are instances where preprocessing the data with some sort of transformation (PCA, Autoencoder, Embedding...) proves to be useful even for larger architectures.","5663148e":"### Get training & validation data","d8a79bc6":"<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/7\/72\/SVM_margin.png\" alt=\"Linear SVM\" width=\"400\"\/>","7d81c13a":"To check where our model failed to get it right, a confusion matrix visualization comes in handy. Here, we clearly see that 5 vs 3 and 4 vs 9 were the more problematic cases.","18bdfab2":"### Submit results!","2d0b57da":"Please don't ;-) we can do way better... check my other notebook:\n[Digit Recognizer - part 2: using CNNs](https:\/\/www.kaggle.com\/jorgefalcon\/digit-recognizer-cnn)"}}