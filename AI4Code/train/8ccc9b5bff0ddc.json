{"cell_type":{"43a9a5e5":"code","06da5129":"code","04f44510":"code","e22938b6":"code","6e084556":"code","5edfff81":"code","870c96fb":"code","bf436284":"code","2836d014":"code","bbaca473":"code","dd4a5dc3":"code","1d564b59":"code","bab7cd32":"code","6f82cddb":"code","b2223129":"code","548506de":"code","c372bc9a":"code","24dda599":"code","b56a251b":"code","e4acfa06":"code","33ced70b":"code","a977b0e7":"code","e37ae8e6":"code","9a1c6521":"markdown","42727b87":"markdown","305f09a5":"markdown","87ec1575":"markdown","82ad1a72":"markdown","af935943":"markdown","8e871791":"markdown","d44f83fd":"markdown","dfc45606":"markdown","82b36bb9":"markdown","3bf59521":"markdown","6ab874de":"markdown","f6d1a335":"markdown","5872f638":"markdown","2fafe65c":"markdown"},"source":{"43a9a5e5":"import os\nprint(os.listdir(\"..\/input\"))","06da5129":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error\nimport gc\nimport time\nfrom pandas.core.common import SettingWithCopyWarning\nimport warnings\nimport lightgbm as lgb\nfrom sklearn.model_selection import GroupKFold\n\n# I don't like SettingWithCopyWarnings ...\nwarnings.simplefilter('error', SettingWithCopyWarning)\ngc.enable()\n%matplotlib inline","04f44510":"train = pd.read_csv('..\/input\/create-extracted-json-fields-dataset\/extracted_fields_train.gz', \n                    dtype={'date': str, 'fullVisitorId': str, \"visitId\":str, 'sessionId':str}, nrows=None)\ntest = pd.read_csv('..\/input\/create-extracted-json-fields-dataset\/extracted_fields_test.gz', \n                   dtype={'date': str, 'fullVisitorId': str, \"visitId\":str, 'sessionId':str}, nrows=None)\n\ntrain.shape, test.shape","e22938b6":"def get_folds(df=None, n_splits=5):\n    \"\"\"Returns dataframe indices corresponding to Visitors Group KFold\"\"\"\n    # Get sorted unique visitors\n    unique_vis = np.array(sorted(df['fullVisitorId'].unique()))\n\n    # Get folds\n    folds = GroupKFold(n_splits=n_splits)\n    fold_ids = []\n    ids = np.arange(df.shape[0])\n    for trn_vis, val_vis in folds.split(X=unique_vis, y=unique_vis, groups=unique_vis):\n        fold_ids.append(\n            [\n                ids[df['fullVisitorId'].isin(unique_vis[trn_vis])],\n                ids[df['fullVisitorId'].isin(unique_vis[val_vis])]\n            ]\n        )\n\n    return fold_ids","6e084556":"y_reg = train['totals.transactionRevenue'].fillna(0)","5edfff81":"train['target'] = y_reg\ndef extract_new_feature(df): \n    print(\"Start extract date...\")\n    df['date'] = pd.to_datetime(df['visitStartTime'], unit='s')\n    df['day_of_week'] = df['date'].dt.dayofweek\n    df['hour'] = df['date'].dt.hour\n    df['day'] = df['date'].dt.day\n    df['month'] = df['date'].dt.month\n    print(\"Finished extract date...\")\nextract_new_feature(train)\nextract_new_feature(test)\n\ndef add_time_period_of_same_ID(df): \n    print(\"Start add time period feature...\")\n    df.sort_values(['fullVisitorId', 'date'], ascending=True, inplace=True)\n    df['next_revisit_time'] = (\n        df['date'] - df[['fullVisitorId', 'date']].groupby('fullVisitorId')['date'].shift(1)\n    ).astype(np.int64) \/\/ 1e9 \/\/ 60 \/\/ 60\n    df['prev_revisit_time'] = (\n        df['date'] - df[['fullVisitorId', 'date']].groupby('fullVisitorId')['date'].shift(-1)\n    ).astype(np.int64) \/\/ 1e9 \/\/ 60 \/\/ 60\n    print(\"Finished time periodfeature...\")\n    \nadd_time_period_of_same_ID(train)\nadd_time_period_of_same_ID(test)\ny_reg = train['target']\ndel train['target']","870c96fb":"categorical_features_train = train.select_dtypes(include=[np.object])\ncategorical_features_test = test.select_dtypes(include=[np.object])\ncategorical_features_train.columns","bf436284":"train['totals.pageviews']=train['totals.pageviews'].astype('float64')\ntrain['totals.hits']=train['totals.hits'].astype('float64')\ntest['totals.pageviews']=test['totals.pageviews'].astype('float64')\ntest['totals.hits']=test['totals.hits'].astype('float64')","2836d014":"df_combine=pd.concat([train,test],ignore_index=True)\nprint(df_combine.shape)\n#Find One_hot features that unique count <15\none_hot_features = df_combine[list(categorical_features_test)].nunique().reset_index()\none_hot_features.columns = ['features','unique_count']\none_hot_features = one_hot_features.loc[one_hot_features['unique_count'] < 10,\"features\"]\none_hot_features = list(one_hot_features)","bbaca473":"#Process one_hot_features\nfor i in one_hot_features:\n    print(\"Process feature =====>\"+str(i))\n    df_combine[\"one_hot_feature\"] = df_combine[i]\n    df_combine[\"one_hot_feature\"] =  str(i) + \".\" + df_combine[\"one_hot_feature\"].astype('str')\n    one_hot_combine = pd.get_dummies(df_combine[\"one_hot_feature\"])\n    print(one_hot_combine.shape)\n    df_combine = df_combine.join(one_hot_combine)\n    del df_combine[\"one_hot_feature\"]\n    del df_combine[i]\n    del one_hot_combine\n    print(df_combine.shape)","dd4a5dc3":"train = df_combine[:len(train)]\nprint(train.shape)\ntest = df_combine[len(train):]\nprint(test.shape)\ndel df_combine","1d564b59":"excluded_features = [\n    'date', 'fullVisitorId', 'sessionId', 'totals.transactionRevenue', \n    'visitId', 'visitStartTime']\ncategorical_larger_15_feature = [i for i in categorical_features_train if i not in one_hot_features and i not in excluded_features]\nprint(categorical_larger_15_feature)","bab7cd32":"not_do_ranking =['trafficSource.referralPath','geoNetwork.networkDomain']\nranking_feature = [i for i in categorical_larger_15_feature if i not in not_do_ranking]\nprint(ranking_feature)","6f82cddb":"def Ranking_process(df,df_test):\n    for col in ranking_feature:\n        if df[col].dtype=='object':\n            print(\"Process Ranking of \"+str(col)+\" feature...\")\n            df[col].fillna('others',inplace=True)\n            col_list=[col,'totals.transactionRevenue']\n            df_gropby=df[col_list].fillna(0).groupby(col).mean().reset_index()\n            df_gropby.columns = col_list\n            df_gropby['rank']=df_gropby['totals.transactionRevenue'].rank(ascending=1)\n            replace_dict={}\n            final_dict={}\n            for k,col_val in enumerate(df_gropby[col].values):\n                replace_dict[col_val]=df_gropby.iloc[k,2]\n            final_dict[col]=replace_dict\n            df[col]=df[col].map(replace_dict)\n            df_test[col]=df_test[col].map(replace_dict)\n            #df.replace(final_dict,inplace=True)\n            del df_gropby,replace_dict,final_dict\n            gc.collect()\n            print(\"Finished process Ranking of \"+str(col)+\" feature\")","b2223129":"Ranking_process(train,test)","548506de":"excluded_features = [\n    'date', 'fullVisitorId', 'sessionId', 'totals.transactionRevenue', \n    'visitId', 'visitStartTime'\n]\n\ncategorical_features = [\n    _f for _f in train.columns\n    if (_f not in excluded_features) & (train[_f].dtype == 'object')\n]\nprint(categorical_features)","c372bc9a":"\nfor f in categorical_features:\n    train[f], indexer = pd.factorize(train[f])\n    test[f] = indexer.get_indexer(test[f])\n","24dda599":"train.shape, test.shape","b56a251b":"folds = get_folds(df=train, n_splits=5)\n\ntrain_features = [_f for _f in train.columns if _f not in excluded_features]","e4acfa06":"importances = pd.DataFrame()\noof_reg_preds = np.zeros(train.shape[0])\nsub_reg_preds = np.zeros(test.shape[0])\nfor fold_, (trn_, val_) in enumerate(folds):\n    trn_x, trn_y = train[train_features].iloc[trn_], y_reg.iloc[trn_]\n    val_x, val_y = train[train_features].iloc[val_], y_reg.iloc[val_]\n    \n    reg = lgb.LGBMRegressor(\n        num_leaves=31,\n        learning_rate=0.03,\n        n_estimators=1000,\n        subsample=.9,\n        colsample_bytree=.9,\n        random_state=1\n    )\n    reg.fit(\n        trn_x, np.log1p(trn_y),\n        eval_set=[(val_x, np.log1p(val_y))],\n        early_stopping_rounds=50,\n        verbose=100,\n        eval_metric='rmse'\n    )\n    imp_df = pd.DataFrame()\n    imp_df['feature'] = train_features\n    imp_df['gain'] = reg.booster_.feature_importance(importance_type='gain')\n    \n    imp_df['fold'] = fold_ + 1\n    importances = pd.concat([importances, imp_df], axis=0, sort=False)\n    \n    oof_reg_preds[val_] = reg.predict(val_x, num_iteration=reg.best_iteration_)\n    oof_reg_preds[oof_reg_preds < 0] = 0\n    _preds = reg.predict(test[train_features], num_iteration=reg.best_iteration_)\n    _preds[_preds < 0] = 0\n    sub_reg_preds += np.expm1(_preds) \/ len(folds)\n    \nmean_squared_error(np.log1p(y_reg), oof_reg_preds) ** .5\n","33ced70b":"import warnings\nwarnings.simplefilter('ignore', FutureWarning)\n\nimportances['gain_log'] = np.log1p(importances['gain'])\nmean_gain = importances[['gain', 'feature']].groupby('feature').mean()\nimportances['mean_gain'] = importances['feature'].map(mean_gain['gain'])\n\nplt.figure(figsize=(8, 12))\nsns.barplot(x='gain_log', y='feature', data=importances.sort_values('mean_gain', ascending=False))","a977b0e7":"train['predictions'] = oof_reg_preds\ntest['predictions'] = np.log1p(sub_reg_preds)","e37ae8e6":"test_result = test[['fullVisitorId','predictions']].groupby('fullVisitorId').sum().reset_index()\ntrain_result = train[['fullVisitorId','predictions']].groupby('fullVisitorId').sum().reset_index()\ntest_result.columns = ['fullVisitorId','PredictedLogRevenue']\ntrain_result.columns = ['fullVisitorId','PredictedLogRevenue']\ntest_result.to_csv('Ranking_onehot_test.csv',index = False)\ntrain_result.to_csv('Ranking_onehot_train.csv',index = False)","9a1c6521":"### Get the extracted data\n* preprocessed dataset by olivier https:\/\/www.kaggle.com\/ogrellier\/create-extracted-json-fields-dataset","42727b87":"## Do ranking encoding  for categorical unique count >10\n* Idea comes from rahal's kernel : https:\/\/www.kaggle.com\/rahullalu\/gstore-eda-lgbm-baseline-1-4260","305f09a5":"## Training model with K cross validation (Light GBM)\n* reference this kernel : https:\/\/www.kaggle.com\/ogrellier\/i-have-seen-the-future","87ec1575":"## Do one hot encoding for categorical unique count <10","82ad1a72":"## Add Date feature\n* add revisit features\n* add data extract feature","af935943":"### Display feature importances","8e871791":"* Cause referral Path and networkDomain have many unique count, let's bypass it ","d44f83fd":"## Factorize other categoricals features","dfc45606":"### Train a model at Visitor level","82b36bb9":"## Process totals features\n* fill nan feature to 0","3bf59521":"## Model used in this kernel\nLightGBM with K-Cross validation, K=5","6ab874de":"### Introduction\nYou can refer the data analysis with this kernel : \nhttps:\/\/www.kaggle.com\/super13579\/basic-feature-analysis-date-categorical-revenue\n* Keep try different feature to find the key feature on training\n* Use K cross validate to find the best score\n* After find the best score , try to do ensemble learning\n","f6d1a335":"## Feature process in this kernel\n* Process Date feature (add day, week, hour, \"revisit time\")\n* Totals feature don't do anything, only fillna(0)\n* Do one hot encoding for categorical feature that have unique value <15\n* Do ranking encoding for categorical feature that have unique value >15","5872f638":"## Find categorical features","2fafe65c":"### Create user level predictions"}}