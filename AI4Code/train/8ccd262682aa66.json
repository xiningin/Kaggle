{"cell_type":{"8bdbd6f5":"code","b0467cac":"code","9e4ed7e4":"code","d6a250d5":"code","8fb1b519":"code","ce86669f":"code","40f68264":"code","52ff1cd3":"code","7f3136ef":"code","8e36bc2b":"code","c545fa53":"code","3c8f6a2b":"code","e836bec4":"code","d21ea212":"code","e177a05a":"code","1e2adef0":"code","ad12d9c9":"code","020b2f5c":"code","804f1f54":"code","562b4f5a":"code","265b1a8b":"code","4a96b0f6":"code","87087a8b":"code","61d134fc":"code","26d3cda3":"code","af43ad12":"code","9a6044ad":"code","0f415147":"code","23cbe1bb":"code","458b3128":"code","8881ce0e":"code","4d1ddf0e":"code","d3c6e64b":"code","5bf68f87":"code","90d50b9c":"code","aed7a62a":"code","77b8af41":"code","72f9c683":"code","1f39582c":"code","3ee6a0ca":"code","bb226de6":"code","2bad849a":"code","b6045b6c":"code","9e18e7b3":"code","159fe0d5":"code","82a10120":"code","696eeb53":"code","0a990b6e":"code","6a7d762b":"code","8db40c35":"code","144f80f7":"code","f9e5d295":"code","691d9967":"code","422e82fb":"code","fd3ee87f":"code","e7f0c349":"code","a30118be":"code","fc172dda":"code","88ae57d5":"code","4040b84c":"code","74f1c54b":"code","9a00a5f6":"code","06f2af21":"code","42600edd":"code","4403c408":"markdown","e5e5ff00":"markdown","354b2e0a":"markdown","74b152d8":"markdown","9c3966eb":"markdown","f78b0104":"markdown","ba0bb966":"markdown","8104d237":"markdown","e1bea4e4":"markdown","5b6827c1":"markdown","0eb5d8fc":"markdown","8269553d":"markdown","10c79748":"markdown","744fcf7c":"markdown","fe2a9f89":"markdown","5e39e8c2":"markdown","595c90cd":"markdown","c16b9e32":"markdown","09e966ad":"markdown","3b3c8c4f":"markdown","3b8bfcbe":"markdown","458093ab":"markdown","11ad6ccf":"markdown","53a312ff":"markdown","3ef7af66":"markdown","b6b8a4f4":"markdown","c41d0429":"markdown","354b5e5a":"markdown","ddd3a37a":"markdown"},"source":{"8bdbd6f5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b0467cac":"import pandas as pd\nimport numpy as np","9e4ed7e4":"product_sales_df = pd.read_csv('\/kaggle\/input\/summer-products-and-sales-in-ecommerce-wish\/summer-products-with-rating-and-performance_2020-08.csv')\ncat_count_df = pd.read_csv('\/kaggle\/input\/summer-products-and-sales-in-ecommerce-wish\/unique-categories.sorted-by-count.csv')","d6a250d5":"product_sales_df.head()","8fb1b519":"product_sales_df.columns","ce86669f":"target_col = 'units_sold'\n\nprint(f\"Shape of dataframe {product_sales_df.shape}\")\n\nrows = []\nfor col in product_sales_df.columns:\n    if product_sales_df[col].isin([0, 1, np.nan]).all():\n        row_dict = {'ColumnName': col, 'DataType': 'binary', 'HasMissing':product_sales_df.isnull().any().loc[col],\n                    'NumberOfMissingCells': product_sales_df.isnull().sum().loc[col], 'CorrelationWithTarget': product_sales_df.corr()[target_col].loc[col],\n                    'Mean': np.nan, 'Median': np.nan,'Mode': product_sales_df.mode()[col].loc[0], 'MinValue': np.nan, 'MaxValue': np.nan }\n    \n    elif product_sales_df.dtypes.loc[col] == 'int64' or product_sales_df.dtypes.loc[col] == 'float64':\n        \n        row_dict = {'ColumnName': col, 'DataType': product_sales_df.dtypes.loc[col], 'HasMissing':product_sales_df.isnull().any().loc[col],\n                    'NumberOfMissingCells': product_sales_df.isnull().sum().loc[col], 'CorrelationWithTarget': product_sales_df.corr()[target_col].loc[col],\n                    'Mean': product_sales_df.mean().loc[col], 'Median':product_sales_df.median().loc[col], 'Mode': product_sales_df.mode()[col].loc[0],\n                    'MinValue': product_sales_df.min().loc[col], 'MaxValue': product_sales_df.max().loc[col] }\n        \n    else:\n        row_dict = {'ColumnName': col, 'DataType': product_sales_df.dtypes.loc[col], 'HasMissing':product_sales_df.isnull().any().loc[col],\n                    'NumberOfMissingCells': product_sales_df.isnull().sum().loc[col], 'CorrelationWithTarget': np.nan, 'Mean': np.nan, 'Median': np.nan,\n                    'Mode': product_sales_df.mode()[col].loc[0], 'MinValue': np.nan, 'MaxValue': np.nan }\n        \n    rows.append(row_dict)\n    \n        \ninfo_df = pd.DataFrame(rows, columns=['ColumnName', 'DataType', 'HasMissing', 'NumberOfMissingCells', 'CorrelationWithTarget', 'Mean', 'Median', 'Mode', 'MinValue', 'MaxValue'])\n\ninfo_df.set_index('ColumnName', inplace=True)\n    \ninfo_df = info_df.sort_values('CorrelationWithTarget', ascending=False, na_position='last')\n\nprint(\"FOR NUMERICAL COLUMNS\")\ninfo_df[info_df['DataType']!='object']","40f68264":"print(\"\\nFor categorical\/non-numeric columns\")\ninfo_df[info_df['DataType']=='object'].drop(['CorrelationWithTarget', 'Mean', 'Median', 'MinValue', 'MaxValue'], axis=1)","52ff1cd3":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(8, 12))\nheatmap = sns.heatmap(product_sales_df.corr()[[target_col]].sort_values(by=target_col, ascending=False), vmin=-1, vmax=1, annot=True, cmap='BrBG')\nheatmap.set_title(f'Features Correlating with {target_col}', fontdict={'fontsize':18}, pad=16);","7f3136ef":"product_sales_df['currency_buyer'].unique()","8e36bc2b":"count = product_sales_df['product_color'].value_counts()\ncount","c545fa53":"np.sort(product_sales_df['product_color'].dropna().unique())","3c8f6a2b":"product_sales_df['product_color'] = product_sales_df['product_color'].str.lower()","e836bec4":"product_sales_df[product_sales_df['product_color'].str.contains('&', na=False)]['product_color'].unique()","d21ea212":"shade_to_colour = {\n    'navyblue': 'blue', 'lightblue': 'blue', 'skyblue': 'blue', 'lakeblue': 'blue', 'darkblue': 'blue', 'denimblue': 'blue', 'navy blue': 'blue', 'prussianblue': 'blue',\n    'navy': 'blue',\n    'armygreen': 'green', 'army green': 'green', 'fluorescentgreen': 'green', 'mintgreen': 'green', 'light green': 'green', 'lightgreen': 'green',\n    'applegreen': 'green', 'darkgreen': 'green', 'army': 'green', 'khaki': 'green', 'lightkhaki': 'green',\n    'lightyellow': 'yellow', \n    'winered': 'red', 'wine red': 'red', 'lightred': 'red', 'coralred': 'red', 'rose red': 'red', 'watermelonred': 'red', 'orange-red': 'red', 'rosered': 'red',\n    'claret': 'red', 'burgundy': 'red', \n    'gray': 'grey', 'silver': 'grey','lightgray': 'grey', 'lightgrey': 'grey', 'greysnakeskinprint': 'grey',\n    'coffee': 'brown', 'camel': 'brown', 'tan': 'brown', \n    'offwhite': 'white', 'ivory': 'white', 'nude': 'white',\n    'lightpink': 'pink', 'dustypink':'pink', 'rosegold': 'pink',\n    'lightpurple': 'purple', 'coolblack': 'black', 'apricot': 'orange', 'offblack': 'black'\n}\n\ndef update_color(col):\n    if shade_to_colour.get(col, False):\n        return shade_to_colour.get(col)\n    elif '&' in col:\n        return 'dual'\n    elif col in shade_to_colour.values():\n        return col\n    else:\n        return 'other'\n\nproduct_sales_df['product_color'].replace(np.nan, 'others', inplace=True)\n\nproduct_sales_df['product_color'] = product_sales_df.product_color.apply(update_color)","e177a05a":"count = product_sales_df['product_color'].value_counts()\ncount","1e2adef0":"col_df = product_sales_df.groupby('product_color').agg('sum')['units_sold'].to_frame()\ncol_df.reset_index(level=0, inplace=True)\ncol_df","ad12d9c9":"fig = plt.gcf()\nfig.set_size_inches( 16, 10)\nsns.barplot(x=\"product_color\", y=\"units_sold\", data=col_df)","020b2f5c":"product_sales_df['tags_count'] = product_sales_df['tags'].str.split(',').str.len()\n\nfig = plt.gcf()\nfig.set_size_inches( 16, 10)\nsns.lineplot(data=product_sales_df, x=\"tags_count\", y=\"units_sold\", ci=None)","804f1f54":"product_sales_df[product_sales_df['tags_count']<=10]","562b4f5a":"from wordcloud import WordCloud\nustr = \" \".join(product_sales_df['tags'].str.lower().str.split(',').sum())\n\nfig = plt.gcf()\nfig.set_size_inches( 16, 10)\nwordcloud = WordCloud(background_color='white').generate(ustr) \nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","265b1a8b":"product_sales_df['product_variation_size_id'].unique()","4a96b0f6":"product_sales_df['product_variation_size_id'].value_counts().head(50)","87087a8b":"product_sales_df['product_variation_size_id'] = product_sales_df['product_variation_size_id'].str.lower().str.replace('.', '').str.replace('size--', '').str.replace('size -', '').str.replace('size\/', '').str.replace('size ', '').str.replace('size-', '')","61d134fc":"product_sales_df['product_variation_size_id'].unique()","26d3cda3":"product_sales_df['product_variation_size_id'] = product_sales_df['product_variation_size_id'].replace('2xl', 'xl')\nproduct_sales_df['product_variation_size_id'] = product_sales_df['product_variation_size_id'].replace('3xl', 'xl')\nproduct_sales_df['product_variation_size_id'] = product_sales_df['product_variation_size_id'].replace('4xl', 'xl')\nproduct_sales_df['product_variation_size_id'] = product_sales_df['product_variation_size_id'].replace('5xl', 'xl')\nproduct_sales_df['product_variation_size_id'] = product_sales_df['product_variation_size_id'].replace('6xl', 'xl')\nproduct_sales_df['product_variation_size_id'] = product_sales_df['product_variation_size_id'].replace('x   l', 'xl')\nproduct_sales_df['product_variation_size_id'] = product_sales_df['product_variation_size_id'].replace('sizel', 'l')\nproduct_sales_df['product_variation_size_id'] = product_sales_df['product_variation_size_id'].replace('size4xl', 'xl')\nproduct_sales_df['product_variation_size_id'] = product_sales_df['product_variation_size_id'].replace('x   l', 'xl')\nproduct_sales_df['product_variation_size_id'] = product_sales_df['product_variation_size_id'].replace('1 pc - xl', 'xl')","af43ad12":"def change_size(cl):\n    if cl in 'xl,l,s,xs,m,xxl,xxxs,xxxxxl,xxxxl'.split(','):\n        return cl\n    else:\n        return 'other'\n\nproduct_sales_df['product_variation_size_id'] = product_sales_df['product_variation_size_id'].replace(np.nan, 'OTHER')\nproduct_sales_df['product_variation_size_id'] = product_sales_df['product_variation_size_id'].apply(change_size)","9a6044ad":"fig = plt.gcf()\nfig.set_size_inches( 16, 10)\nsns.barplot(x=\"product_variation_size_id\", y=\"units_sold\", data=product_sales_df)","0f415147":"fig = plt.gcf()\nfig.set_size_inches( 16, 10)\nsns.countplot('product_variation_size_id',\n              order = product_sales_df['product_variation_size_id'].value_counts().index,\n              data = product_sales_df)\nplt.show()","23cbe1bb":"product_sales_df.groupby('shipping_option_name').agg(['count', 'sum'])['units_sold']","458b3128":"fig = plt.gcf()\nfig.set_size_inches( 25, 16)\nsns.barplot(x=\"shipping_option_name\", y=\"units_sold\", data=product_sales_df)\nplt.show()","8881ce0e":"product_sales_df.urgency_text.value_counts()","4d1ddf0e":"product_sales_df.groupby('origin_country').agg(['count', 'sum'])['units_sold']","d3c6e64b":"list_of_na_merchants = product_sales_df[product_sales_df['origin_country'].isna()]['merchant_id'].values\n\nfor m in list_of_na_merchants:\n    print(\"merchant title \" + m)\n    print(product_sales_df[product_sales_df['merchant_id']==m]['origin_country'])","5bf68f87":"product_sales_df['origin_country'].fillna('CN', inplace=True)\n","90d50b9c":"product_sales_df['success'] = product_sales_df['units_sold'].apply(lambda x: 1 if x>1000 else 0)","aed7a62a":"fig = plt.gcf()\nfig.set_size_inches( 16, 10)\nsns.scatterplot(data=product_sales_df, x=\"rating_count\", y=\"units_sold\", hue='success', size='units_sold', sizes=(10, 200))","77b8af41":"product_sales_df.update(product_sales_df[['rating_five_count', 'rating_four_count', 'rating_three_count','rating_two_count', 'rating_one_count']].fillna(0))\nproduct_sales_df.loc[product_sales_df['rating_count']==0, 'rating'] = 0","72f9c683":"product_sales_df[['rating_five_count', 'rating_four_count', 'rating_three_count','rating_two_count', 'rating_one_count', 'rating_count', 'rating']].head()","1f39582c":"sns.barplot(x='success', y='rating_five_count', data=product_sales_df)","3ee6a0ca":"sns.barplot(x='success', y='rating_one_count', data=product_sales_df)","bb226de6":"product_sales_df['rating_three_count_prop'] = product_sales_df['rating_three_count']\/product_sales_df['rating_count']\nproduct_sales_df['rating_four_count_prop'] = product_sales_df['rating_four_count']\/product_sales_df['rating_count']\nproduct_sales_df['rating_five_count_prop'] = product_sales_df['rating_five_count']\/product_sales_df['rating_count']\nproduct_sales_df['rating_two_count_prop'] = product_sales_df['rating_two_count']\/product_sales_df['rating_count']\nproduct_sales_df['rating_one_count_prop'] = product_sales_df['rating_one_count']\/product_sales_df['rating_count']\n# to remove nan due to zero division\nproduct_sales_df.update(product_sales_df[['rating_five_count_prop', 'rating_four_count_prop', 'rating_three_count_prop','rating_two_count_prop', 'rating_one_count_prop']].fillna(0))\n","2bad849a":"sns.barplot(x='success', y='rating_one_count_prop', data=product_sales_df)","b6045b6c":"sns.barplot(x='success', y='rating_five_count_prop', data=product_sales_df)","9e18e7b3":"sns.barplot(x='success', y='rating_four_count_prop', data=product_sales_df)","159fe0d5":"fig = plt.gcf()\nfig.set_size_inches( 16, 10)\nsns.countplot(data=product_sales_df, x='merchant_has_profile_picture', hue='success')","82a10120":"fig = plt.gcf()\nfig.set_size_inches( 16, 10)\nsns.violinplot(data=product_sales_df, y='merchant_rating', x='success')","696eeb53":"product_sales_df['has_urgency_banner'] = product_sales_df['has_urgency_banner'].fillna(0)","0a990b6e":"## dropping unnecessary columns\nproduct_sales_df = product_sales_df.drop(['crawl_month','product_id','product_picture', 'product_url', 'merchant_profile_picture', 'merchant_id',\n                       'currency_buyer', 'theme','urgency_text', 'merchant_title', 'merchant_name', 'merchant_info_subtitle',\n                      'title','title_orig','tags', 'shipping_option_name', \"inventory_total\" , \"badge_fast_shipping\" ,\n                       \"badge_local_product\" , \"shipping_is_express\", \"units_sold\"], axis = 1)\n\n","6a7d762b":"product_sales_df = product_sales_df.drop([\"rating_five_count\",\"rating_four_count\",\"rating_three_count\", \"rating_two_count\", \"rating_one_count\"], axis = 1)","8db40c35":"product_sales_df = pd.get_dummies(product_sales_df, \n                           columns = ['product_color'],\n                           prefix = 'color_',\n                           drop_first = True)\nproduct_sales_df.head()","144f80f7":"product_sales_df = pd.get_dummies(product_sales_df, \n                           columns = ['product_variation_size_id'],\n                           prefix = 'size_',\n                           drop_first = True)\nproduct_sales_df.head()","f9e5d295":"product_sales_df = pd.get_dummies(product_sales_df, \n                           columns = ['origin_country'],\n                           prefix = 'country_',\n                           drop_first = True)\nproduct_sales_df.head()","691d9967":"product_sales_df.dtypes","422e82fb":"product_sales_df.isna().any().to_frame()","fd3ee87f":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(product_sales_df.drop(['success'], axis=1), product_sales_df['success'],\ntest_size=0.2, random_state=1, stratify=product_sales_df['success'])\n","e7f0c349":"from sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(random_state=1)\nclf.fit(X_train, y_train)\n\ny_pred = clf.predict(X_test)\n\naccuracy_score(y_pred, y_test)","a30118be":"features = pd.DataFrame()\nfeatures['feature'] = X_train.columns\nfeatures['importance'] = clf.feature_importances_\nfeatures.sort_values(by=['importance'], ascending=True, inplace=True)\nfeatures.set_index('feature', inplace=True)\n\nfeatures.plot(kind='barh', figsize=(25, 25))","fc172dda":"features = pd.DataFrame()\nfeatures['feature'] = X_train.columns\nfeatures['importance'] = clf.feature_importances_\nfeatures = features[features['feature']!='rating_count']\nfeatures.sort_values(by=['importance'], ascending=True, inplace=True)\nfeatures.set_index('feature', inplace=True)\n\nfeatures.plot(kind='barh', figsize=(25, 25))","88ae57d5":"from sklearn.feature_selection import SelectFromModel\n\nmodel = SelectFromModel(clf, prefit=True)\ntrain_reduced = model.transform(X_train)\nprint(train_reduced.shape)\n\ntest_reduced = model.transform(X_test)\nprint(test_reduced.shape)","4040b84c":"# from sklearn.preprocessing import StandardScaler\n# from sklearn.decomposition import PCA\n\n# sc = StandardScaler()\n# train_scaled = sc.fit_transform(X_train)\n# test_scaled = sc.transform(X_test)\n\n# pca = PCA(n_components=20)\n# train_reduced = pca.fit_transform(train_scaled)\n# test_reduced = pca.transform(test_scaled)\n# print(train_reduced.shape)\n# print(test_reduced.shape)","74f1c54b":"from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom xgboost import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.model_selection import cross_val_score","9a00a5f6":"def compute_score(clf, X, y, scoring='accuracy'):\n    xval = cross_val_score(clf, X, y, cv = 5, scoring=scoring)\n    return np.mean(xval)","06f2af21":"import warnings\nwarnings.filterwarnings('ignore')\n\nlogreg = LogisticRegression()\nlogreg_cv = LogisticRegressionCV()\nrf = RandomForestClassifier()\ngboost = GradientBoostingClassifier()\ngnb = GaussianNB()\nsvm = SVC()\nknn = KNeighborsClassifier()\nxgboost = XGBClassifier()\ndc = DecisionTreeClassifier()\nadc = AdaBoostClassifier()\nmodels = [logreg, logreg_cv, rf, gboost, gnb, svm, knn, xgboost, dc, adc]\n\nfor model in models:\n    print('Cross-validation of : {0}'.format(model.__class__))\n    score = compute_score(clf=model, X=train_reduced, y=y_train, scoring='accuracy')\n    print('CV score = {0}'.format(score))\n    print('****')","42600edd":"clf = XGBClassifier()\nclf.fit(train_reduced, y_train)\n\ny_pred = clf.predict(test_reduced)\n\naccuracy_score(y_pred, y_test)","4403c408":"We can see that a merchant is more likely to be successful if he has a profile picture\nThus the correlation with units_sold is high too\n\n## merchant_rating","e5e5ff00":"# EDA of Sales of summer clothes in E-commerce Wish dataset\n\nFirst of all we will build a function to analyse the sales data for us and provide important information such as:\n* Data type of each field\n* Which columns has missing data and number of missing records in each column\n* What is the correlation of all the other numeric columns with the target column \n* Now for the numeric features: the mean, median and mode\n* For categorical columns: mode\n","354b2e0a":"Thus normalising the data really solves the problem\n\n## merchant_rating_count\n\nhigh correlation and no missing values\n\n## merchant_has_profile_picture\n","74b152d8":"Thus we can see that the number of rating affects the success of a product greatly.\n\n## 'rating_five_count', 'rating_four_count', 'rating_three_count','rating_two_count', 'rating_one_count'\n\nThey all have very high correlation with the target variable and have 45 missing values each.\nIt turns out that these products with missing values for these 5 columns have no ratings (rating_count=0) \nSo let's replace these values with zero. Also we find that the value of 'rating' column for these products is 5 which is unlikely given the products have zero rating\n","9c3966eb":"We will now fit our data into diferent classfiers and choose the best model","f78b0104":"Let's drop  urgency_text,\ntitle, title_orig, currency_buyer, urgency_tex, merchant_title,merchant_name, \nmerchant_id,merchant_profile_picture,product_url\tobject, product_picture, \nproduct_id, theme and crawl_month","ba0bb966":"## urgency_text","8104d237":"## 3. product_variation_size_id\n14 missing values\nmost common value is 'S'","e1bea4e4":"As we can see there are only 27 products with tags count less than 10 and only 2 with sales of 20000 and rest have sales like 50, 100, 1000, 5000\nSo these two are outlires and thus reson for spike","5b6827c1":"Lets try to reduce the number of sizes here. ","0eb5d8fc":"Let's have a look at the most common tags with the help of a wordcloud","8269553d":"So merchants which have missing origin country cannot be replaced by looking at the origin_country of the same merchant for another product.\nThis is based on the assumnption that a country of origin for a merchant should have the same value across product.\n\nWe will replace this with CN as its the most frequent value.\n","10c79748":"Products with tags more than 35 are more discoverable and are thus bought more often.There is a sudden spike at just below 10 tags so let's investigate if that's an outlier\n","744fcf7c":"Random Forests Classifier gives us a really good accuracy of 96% on the test set. But let's check this with k fold cross validation and see if there is overfitting.\nWe can also checkout the important features (columns) in this dataset using the RandomForestClassifier as decision trees are one of the great ways to do so.","fe2a9f89":"Our next step is to normalise these columns as rating count for 'five' has a similar correlation with rating count for 'one' \nThis is unlikely as product with low ratings is less likely to be successful","5e39e8c2":"It looks like rating count has too much effect on our target variable.\nLets try to remove this see the graph again","595c90cd":"shipping_option_name","c16b9e32":"## origin_country","09e966ad":"After applying the necesssary transformation of the colour column we can see that black has sold most units followe by white.","3b3c8c4f":"## Let's first start with categorical features and process them\n\n### 1. currency_buyer\nIt has no missing values and the only value is 'EUR'","3b8bfcbe":"## 2. product_color\nHas 41 missing values and most common value is black","458093ab":"There are so many colours, let's see if we can combine different shades of a colour into one colour like : example navy blue, blue and light blue into just blue","11ad6ccf":"Here we can see that the majority of the products are of size 'S' but size 'M' has the most units_sold","53a312ff":"## rating_count\nno missing values, highest correlation with target","3ef7af66":"Lets try removing the unnecessary features and see how it affects our accuracy. We will be using Sklearn's SelectFromModel","b6b8a4f4":"As we can still few are left.","c41d0429":"The result are not that different, lets try PCA and see if we can improve the accuracy ","354b5e5a":"We are done with processing the categorical columns.\nNow lets start  with the numerical data\n\nThe first column is 'units_sold' which is our target variable. But we don't want to predict the number of sales for a product but rather if the product has been successful on the Wish.com platform.\nSo we will start by converting the numerical data to a binary one. (successful or not)\nunits_sold has no missing values and has a median of 1000. So we will consider products to be successful if they have sales greater than 1000.\nNow we are using median because because it not affected by very high outliers\n\n## units_sold","ddd3a37a":"## Tags\nNo missing values\n "}}