{"cell_type":{"85beecf8":"code","f9a0b923":"code","9011524d":"code","680fcee0":"code","948a66ba":"code","ff37793c":"code","3bd47150":"code","99d69ea7":"code","a993a819":"code","286f4b97":"code","97423c09":"code","0387a762":"code","70c1d39d":"code","cd6b4a8f":"code","0c60d136":"code","5433109b":"code","fb3fc318":"code","adbf7c45":"code","4621e2e9":"code","3ff68463":"code","5331f090":"code","53de4de2":"code","e600488c":"code","999bbc40":"code","7ed71aea":"code","482e06cd":"code","9110bc35":"code","ee68c135":"code","f8d33c2a":"code","d0b95ac6":"code","8efb9346":"code","caff7d56":"code","0e653218":"code","ec12787f":"code","46efaa3e":"code","f108fdbd":"code","1c1b554f":"code","72202d5c":"code","ec795d56":"code","ffda0fae":"code","9c930bdd":"code","414bb576":"code","9c19d058":"code","6a9bbc30":"code","d89baeb8":"code","cc41bceb":"code","958d40a5":"code","1ff7da67":"code","2a43b95c":"code","3d22628d":"markdown","17eee1b1":"markdown","01870f61":"markdown","178f1cb5":"markdown","920d1886":"markdown","a0af9f6f":"markdown","17d4fb6f":"markdown","d1420b5e":"markdown","8faffd33":"markdown","d5827729":"markdown","4e540248":"markdown","31815ccd":"markdown","5acf9540":"markdown","665e50ed":"markdown","931fc10c":"markdown","c1720650":"markdown","bb273191":"markdown","4308247b":"markdown","1eb31cad":"markdown","3071a3d3":"markdown","4ee18f90":"markdown","3e9d1f5c":"markdown","aef8472f":"markdown","8498096b":"markdown","509e49b5":"markdown","4d0c6e2b":"markdown","576de105":"markdown","bb70c149":"markdown","a1b6ea63":"markdown","c63b888b":"markdown","cd8860cd":"markdown","dd09742b":"markdown"},"source":{"85beecf8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\npd.set_option('display.float_format',lambda x: '%.2f' %x)\npd.set_option('display.max_columns',200)\npd.set_option('display.max_rows',1000)\nimport sklearn\n\nimport scipy.sparse \nimport matplotlib.pyplot as plt\n%matplotlib inline \nimport lightgbm as lgb\nimport seaborn as sns\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nimport catboost\nfrom catboost import Pool\nfrom catboost import CatBoostRegressor\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f9a0b923":"base_path = \"\/kaggle\/input\/competitive-data-science-predict-future-sales\/\"\nshops = pd.read_csv(base_path+\"shops.csv\");\ntest = pd.read_csv(base_path+\"test.csv\");\nsample_submission = pd.read_csv(base_path+\"sample_submission.csv\")\nsales_train = pd.read_csv(base_path+\"sales_train.csv\")\ncategories = pd.read_csv(base_path+\"item_categories.csv\")\nitems = pd.read_csv(base_path+\"items.csv\")\n\nbase_path_trans = \"\/kaggle\/input\/predict-future-sales-translated-dataset\/\"\nshops_en = pd.read_csv(base_path_trans+\"shops_en.csv\");\ncategories_en = pd.read_csv(base_path_trans+\"item_categories_en.csv\")\nitems_en = pd.read_csv(base_path_trans+\"items_en.csv\")\n","9011524d":"# Print column names and columns type\nprint(\"** Items **\\n\",items.dtypes)\nprint(\"\\n** Categories **\\n\",categories.dtypes)\nprint(\"\\n** Shops **\\n\",shops.dtypes)\nprint(\"\\n** Sales **\\n\",sales_train.dtypes)","680fcee0":"# Join data frames\nsales_train = sales_train.join(items, on='item_id', rsuffix='_').join(categories, on='item_category_id', rsuffix='_').drop(['item_id_', 'item_category_id_'], axis=1)\n\n# Describe dataframe\nsales_train.describe().T","948a66ba":"sales_train.head(3).T","ff37793c":"categories_en","3bd47150":"shops_en.sort_values('shop_name')","99d69ea7":"test[test[\"shop_id\"].isin([0,57,1,58,10,11,39,40])].groupby([\"shop_id\"])[\"item_id\"].agg(\"count\")\n","a993a819":"print(\"Missing values\\n\", sales_train.isnull().sum()) \nprint(\"\\nInvalid Values\")\nprint(\"item_price: \",sales_train[sales_train.item_price<0].item_price.size)","286f4b97":"# Remove data with negative price\nsales_train.drop(sales_train[sales_train['item_price'] < 0].index, inplace = True)\n\n# Convert date column format to format that may be better for analysis splitting in diferent features\nsales_train['date'] = pd.to_datetime(sales_train['date'], format=\"%d.%m.%Y\")\n# Split date\nsales_train['year'] = pd.DatetimeIndex(sales_train['date']).year\nsales_train['month'] = pd.DatetimeIndex(sales_train['date']).month\nsales_train['day'] = pd.DatetimeIndex(sales_train['date']).day\n\n# Combining data\nsales_train.loc[sales_train['shop_id']==11,'shop_id'] = 10\nsales_train.loc[sales_train['shop_id']==0,'shop_id'] = 57\nsales_train.loc[sales_train['shop_id']==1,'shop_id'] = 58\nsales_train.loc[sales_train['shop_id']==40,'shop_id'] = 39\n\nsales_train.describe().T","97423c09":"plt.hist(sales_train[\"item_id\"],bins=50)\nplt.xlabel(\"item_id\")\nplt.ylabel(\"count\")\nplt.show()","0387a762":"plt.hist(sales_train[\"shop_id\"], bins=60)\nplt.xlabel(\"shop_id\")\nplt.ylabel(\"count\")\nplt.show()","70c1d39d":"\nplt.hist(sales_train[\"item_category_id\"], bins=84)\nplt.xlabel(\"item_category_id\")\nplt.ylabel(\"count\")\nplt.figure(figsize=(3,4))\nplt.show()","cd6b4a8f":"shop_per_cat = sales_train.groupby(['item_category_id'])['item_cnt_day'].agg(\"sum\")\nshop_per_cat.columns=[\"item_category_id\",\"cnt_per_cate\"]\nshop_per_cat.plot()\nplt.title(\"Shops per category\")\nplt.show()","0c60d136":"# Daily Shops\ndaily_shops= sales_train.groupby(['date'])['item_cnt_day'].agg(\"sum\")\ndaily_shops.columns=[\"date\",\"cnt_day\"]\ndaily_shops.plot()\nplt.title(\"Shops per day\")\nplt.ylabel(\"sum_cnt\")\nplt.show()","5433109b":"# Monthly shops\nmonthly_shops= sales_train.groupby(['month'])['item_cnt_day'].agg(\"sum\")\nmonthly_shops.columns=[\"date\",\"cnt_day\"]\nmonthly_shops.plot()\nplt.title(\"Shops per month\")\nplt.show()","fb3fc318":"# Shops per block num\nshops_per_block = sales_train.groupby(['date_block_num'], as_index=False)['item_cnt_day'].agg(\"sum\")\n#shops_per_block.columns=[\"date_block_num\",\"item_cnt\"]\nshops_per_block['item_cnt_day'].plot()\nplt.xticks (shops_per_block['date_block_num'])\nplt.title(\"Sales per block num\")\nplt.show()","adbf7c45":"# Sales per week day\nsales_train[\"week_day\"] = pd.DatetimeIndex(sales_train['date']).dayofweek\nsales_per_wd = sales_train.groupby(['week_day'])['item_cnt_day'].agg(\"sum\")\nsales_per_wd.plot()\nplt.show()","4621e2e9":"def days_of_month_rate(year,month):\n    import calendar\n    import datetime\n    days = calendar.monthrange(year,month)[1]\n    date =  datetime.date(year, month, 1)\n    days_of_month = 0   \n   \n    for x in range(1,8):\n        if(date.weekday()<4):\n            days_of_month = days_of_month + 1\n        else:\n            days_of_month = days_of_month + 1\n        date = date + datetime.timedelta(1)\n    \n    days_of_month = days_of_month * 4    \n    if(days<=29):\n        return days_of_month;\n    date =  datetime.date(year, month, 29)\n    \n    for x in range(29, days+1):        \n        if(date.weekday()<4):\n            days_of_month = days_of_month + 1\n        else:\n            days_of_month = days_of_month + 1 \n        date = date + datetime.timedelta(1)\n    return days_of_month\n\n    return days_of_month\n","3ff68463":"plt.scatter(sales_train['item_price'], sales_train['item_cnt_day'])\nplt.ylabel(\"item_cnt_day\")\nplt.xlabel(\"item_price\")\nplt.show()","5331f090":"sales_train = sales_train[(sales_train['item_price']<40000)&(sales_train['item_cnt_day']<=100)]\nsales_train.loc[sales_train['item_cnt_day']<0, 'item_cnt_day'] = 0\nplt.scatter(sales_train['item_price'], sales_train['item_cnt_day'])\nplt.ylabel(\"item_cnt_day\")\nplt.xlabel(\"item_price\")\nplt.show()","53de4de2":"test_shop_ids = test['shop_id'].unique()\ntest_item_ids = test['item_id'].unique()\n\n# Only shops and items that exist in test set.\nwork_data = sales_train[sales_train['shop_id'].isin(test_shop_ids)]\nwork_data = work_data[work_data['item_id'].isin(test_item_ids)]\n\nprint('Data set size before :', sales_train.shape[0])\nprint('Data set size after:', work_data.shape[0])\nwork_data.describe().T","e600488c":"# Group data and generate feautures from price and sales per day\ntrain_monthly = work_data.groupby(\n    [\"date_block_num\",\"shop_id\",\"item_id\",\"item_category_id\",\"year\",\"month\"], as_index=False).agg({\n    'item_price':['sum', 'mean'], 'item_cnt_day':['sum', 'mean','count']#, \"week_day\" : ['count', 'mean']\n});\n\n# Rename features.\ntrain_monthly.columns = ['date_block_num', 'shop_id', 'item_id','item_category_id', 'year', 'month','sum_item_price', 'mean_item_price', 'item_cnt_month','mean_item_cnt', 'sales']\n\n\n# Add custom feature: month_rate\ntrain_monthly['month_rate'] = train_monthly.apply(lambda x: days_of_month_rate(x['year'].astype(int),x['month'].astype(int)),axis=1)\n\n\ntrain_monthly.describe().T","999bbc40":"# Build a empty data set with possible combinations for date_block_num, shop_id and tem_id\nshop_ids = train_monthly['shop_id'].unique()\nitem_ids = train_monthly['item_id'].unique()\nempty_df = []\nfor i in range(34):\n    for shop in shop_ids:\n        for item in item_ids:\n            empty_df.append([i, shop, item])\n    \nempty_df = pd.DataFrame(empty_df, columns=['date_block_num','shop_id','item_id'])","7ed71aea":"train_monthly = pd.merge(empty_df, train_monthly, on=['date_block_num','shop_id','item_id'], how='left')\ntrain_monthly.fillna(0, inplace=True)\ntrain_monthly.describe().T","482e06cd":"train_monthly['item_cnt_next_month'] = train_monthly.sort_values('date_block_num').groupby(['shop_id', 'item_id'])['item_cnt_month'].shift(-1)","9110bc35":"# Features based on item prices\n#hist_item_price_ft = work_data.groupby(['item_id'], as_index=False).agg({'item_price':['min', 'max']})\n#hist_item_price_ft.columns = ['item_id',\"hist_min_price\",\"hist_max_price\"]\n#hist_item_price_ft","ee68c135":"# Aggregation functions\ngetMin = lambda x: x.rolling(window=3, min_periods=1).min()\ngetMax = lambda x: x.rolling(window=3, min_periods=1).max()\ngetMean = lambda x: x.rolling(window=3, min_periods=1).mean()\ngetStd = lambda x: x.rolling(window=3, min_periods=1).std()\n\nfn_lst = [getMin, getMax, getMean, getStd]\nfn_name = ['min', 'max', 'mean', 'std']\n\nfor i in range(len(fn_lst)):\n    train_monthly[('item_cnt_hist_%s' % fn_name[i])] = train_monthly.sort_values('date_block_num').groupby(['shop_id', 'item_category_id', 'item_id'])['item_cnt_month'].apply(fn_lst[i])\n\n# Fill the empty std features with 0\ntrain_monthly['item_cnt_hist_std'].fillna(0, inplace=True)\ntrain_monthly.describe().T","f8d33c2a":"train_monthly[\"item_cnt_shifted_1\"] = train_monthly.sort_values('date_block_num').groupby(['shop_id', 'item_category_id', 'item_id'])['item_cnt_month'].shift(1)\ntrain_monthly[\"item_cnt_shifted_1\"].fillna(0, inplace=True)\ntrain_monthly[\"item_cnt_shifted_2\"] = train_monthly.sort_values('date_block_num').groupby(['shop_id', 'item_category_id', 'item_id'])['item_cnt_month'].shift(2)\ntrain_monthly[\"item_cnt_shifted_2\"].fillna(0, inplace=True)\ntrain_monthly[\"item_cnt_shifted_3\"] = train_monthly.sort_values('date_block_num').groupby(['shop_id', 'item_category_id', 'item_id'])['item_cnt_month'].shift(3)\ntrain_monthly[\"item_cnt_shifted_3\"].fillna(0, inplace=True)","d0b95ac6":"train_monthly['item_trend'] = train_monthly['item_cnt_month']\ntrain_monthly['item_trend'] -= (train_monthly[\"item_cnt_shifted_1\"]+train_monthly[\"item_cnt_shifted_2\"]+train_monthly[\"item_cnt_shifted_3\"])\n\ntrain_monthly['item_trend'] \/= 4","8efb9346":"train_monthly['date_block_num'].values.max()\n# Take months from 3 to 28. 3 first months where take for aggregation functions\ntrain_data = train_monthly[(train_monthly['date_block_num']<=28) & (train_monthly['date_block_num']>=3)]\n# Take months from 29 to 32\nval_data = train_monthly[(train_monthly['date_block_num']<=32) & (train_monthly['date_block_num']>=29)]\n# Last block is for test\ntest_data = train_monthly[train_monthly['date_block_num']==33]\nTotalData = train_monthly.shape[0]\nprint('number of validation samples: %d (%f%%)'%(val_data.shape[0],(val_data.shape[0]\/TotalData*100)))\nprint(\"number of training samples: %d (%f%%)\"%(train_data.shape[0],(train_data.shape[0]\/TotalData*100)))\nprint(\"number of test samples: %d (%f%%)\"%(test_data.shape[0],(test_data.shape[0]\/TotalData*100)))\n","caff7d56":"# Item mean\nhist_item_mean = train_data.groupby(['item_id']).agg({'item_cnt_next_month': ['mean']})\nhist_item_mean.columns = ['hist_item_mean']\nhist_item_mean.reset_index(inplace=True)\n# Shop mean\nhist_shop_mean = train_data.groupby(['shop_id']).agg({'item_cnt_next_month': ['mean']})\nhist_shop_mean.columns = ['hist_shop_mean']\nhist_shop_mean.reset_index(inplace=True)\n\n#Item-shop mean\nhist_item_shop_mean = train_data.groupby(['shop_id', 'item_id']).agg({'item_cnt_next_month': ['mean']})\nhist_item_shop_mean.columns = ['hist_shop_item_mean']\nhist_item_shop_mean.reset_index(inplace=True)\n# Year mean\nhist_year_mean = train_data.groupby(['year']).agg({'item_cnt_next_month': ['mean']})\nhist_year_mean.columns = ['hist_year_mean']\nhist_year_mean.reset_index(inplace=True)\n# Month mean\nhist_month_mean = train_data.groupby(['month']).agg({'item_cnt_next_month': ['mean']})\nhist_month_mean.columns = ['hist_month_mean']\nhist_month_mean.reset_index(inplace=True)\n\n# Add mean encoding features to train set.\ntrain_data = pd.merge(train_data, hist_item_mean, on=['item_id'], how='left')\ntrain_data = pd.merge(train_data, hist_shop_mean, on=['shop_id'], how='left')\ntrain_data = pd.merge(train_data, hist_item_shop_mean, on=['shop_id', 'item_id'], how='left')\ntrain_data = pd.merge(train_data, hist_year_mean, on=['year'], how='left')\ntrain_data = pd.merge(train_data, hist_month_mean, on=['month'], how='left')\n# Add meand encoding features to validation set.\nval_data = pd.merge(val_data, hist_item_mean, on=['item_id'], how='left')\nval_data = pd.merge(val_data, hist_shop_mean, on=['shop_id'], how='left')\nval_data = pd.merge(val_data, hist_item_shop_mean, on=['shop_id', 'item_id'], how='left')\nval_data = pd.merge(val_data, hist_year_mean, on=['year'], how='left')\nval_data = pd.merge(val_data, hist_month_mean, on=['month'], how='left')","0e653218":"train_data.describe().T","ec12787f":"#train_data.columns\ntarget_feature = ['item_cnt_next_month']\nall_features = ['date_block_num', 'shop_id','item_id','item_category_id','year','month',\n                'sum_item_price','mean_item_price','item_cnt_month','mean_item_cnt',\n                'sales','month_rate','item_cnt_hist_min','item_cnt_hist_max',\n                'item_cnt_hist_mean','item_cnt_hist_std','hist_item_mean','hist_shop_mean',\n                'hist_shop_item_mean','hist_year_mean','hist_month_mean',\n               'item_cnt_shifted_1','item_cnt_shifted_2','item_cnt_shifted_3','item_trend']\n\nx_train = train_data[all_features]\ny_train = train_data[target_feature]\n\nx_val = val_data[all_features]\ny_val = val_data[target_feature]","46efaa3e":"test = pd.read_csv(base_path+\"test.csv\");\nlatest_records = pd.concat([train_data, val_data]).drop_duplicates(subset=['shop_id', 'item_id'], keep='last')\nx_test = pd.merge(test, latest_records, on=['shop_id', 'item_id'], how='left', suffixes=['', '_'])\nx_test['year'] = 2015\nx_test['month'] = 9\nx_test['month_rate'] = days_of_month_rate(2015, 11)\n#x_test[int_features] = X_test[int_features].astype('int32')\nx_test = x_test[x_train.columns]","f108fdbd":"datasets = [x_train,x_val, x_test]\n\n          \nfor dataset in datasets:\n    for shop_id in dataset['shop_id'].unique():\n        for column in dataset.columns:\n            shop_median = dataset[(dataset['shop_id'] == shop_id)][column].median()\n            dataset.loc[(dataset[column].isnull()) & (dataset['shop_id'] == shop_id), column] = shop_median\n            \n# Fill remaining missing values on test set with mean.\nx_test.fillna(x_test.mean(), inplace=True)","1c1b554f":"# x_train.columns\n#lre_model_features = ['shop_id','item_id','month', \n#                      'hist_year_mean', 'hist_month_mean','item_cnt_hist_mean',\n#                      'item_cnt_hist_std','item_cnt_hist_max','mean_item_price','month_rate'] #0.6663\n\n#lre_model_features = ['item_cnt_month', 'item_cnt_shifted_3', 'item_trend', \n#                      'mean_item_cnt', 'hist_shop_mean','mean_item_price', 'item_cnt_hist_std'] #0.71459\n\nlre_model_features = ['item_cnt_month', 'item_cnt_shifted_3', 'item_trend', 'month', 'month_rate','item_cnt_hist_max'\n                      'mean_item_cnt', 'hist_shop_mean','mean_item_price', 'item_cnt_hist_std']\n\nlre_x_train = x_train[lre_model_features]\nlre_x_val = x_val[lre_model_features]\n\n# Normalization\nscaler = sklearn.preprocessing.MinMaxScaler()\nscaler.fit(lre_x_train)\nlre_x_train = scaler.transform(lre_x_train)\nlre_x_val = scaler.transform(lre_x_val)\n\n# Modeling\nfrom sklearn.linear_model import LinearRegression\nlre_model = LinearRegression()\nlre_model.fit(lre_x_train, y_train)\nlre_model.score(lre_x_val, y_val)\nlre_val_pred = lre_model.predict(lre_x_val)\nlre_val_pred","72202d5c":"#print(\"x_train\", x_train.shape)\n#print(\"lre_x_train\", lre_x_train.shape)\n#print(\"y_train\", y_train.shape)\n#print(\"lre_x_val\", lre_x_val.shape)\n#print(\"y_val\", lre_x_train.shape)","ec795d56":"# x_train.columns\n# Features to use with random forest\n#rfr_model_features = ['shop_id','item_id','month', \n#                      'hist_year_mean', 'hist_month_mean','item_cnt_hist_mean',\n#                      'item_cnt_hist_std','item_cnt_hist_max','mean_item_price']\n# Score 0.7008\n# \nrfr_model_features = ['shop_id', 'item_id', 'item_cnt_month', 'sales', 'year',\n                      'item_cnt_hist_mean', 'item_cnt_hist_std', 'item_cnt_shifted_1',\n                      'hist_shop_mean', 'item_trend', 'mean_item_cnt'] # Score: 0.7249\/100 est\n\nrfr_x_train = x_train[rfr_model_features]\nrfr_x_val = x_val[rfr_model_features]\n\nrfr_model = RandomForestRegressor(n_estimators=500, max_depth=7, random_state=0, n_jobs=-1)\nrfr_model.fit(rfr_x_train, y_train)\nrfr_model.score(rfr_x_val, y_val)","ffda0fae":"# Features to use with XBOST\n#print(\"x_train\", x_train.columns)\n#xgb_model_features = ['item_cnt_month','item_cnt_hist_min', 'item_cnt_hist_std',\n#                      'hist_shop_mean', 'hist_shop_item_mean'] # validation_1-rmse:1.83021\n\nxgb_model_features = ['item_cnt_month','item_cnt_hist_min', 'item_cnt_hist_std',\n                      'item_cnt_shifted_2', 'item_cnt_shifted_3',#'item_cnt_shifted_1',\n                      'hist_shop_mean', 'hist_shop_item_mean', 'item_trend', 'mean_item_cnt'] # 100 - Est --> validation_1-rmse:1.70364\n\nxgb_x_train = x_train[xgb_model_features]\nxgb_x_val = x_val[xgb_model_features]\n\nxgb_model = XGBRegressor(max_depth=8, n_estimators=500, min_child_weight=1000,\n                         colsample_bytree=0.7, subsample=0.7, eta=0.3, seed=12345)\n\nxgb_model.fit(xgb_x_train, y_train, \n              eval_set=[(xgb_x_train, y_train), (xgb_x_val, y_val)],\n              eval_metric='rmse',               \n              verbose=True, \n              early_stopping_rounds=20)\n\nxgb_model.score","9c930bdd":"#x_train.isna().sum()\n#cat_features = [0, 1,2,3]\n#cbt_model_features = ['item_cnt_month','item_cnt_hist_mean', 'item_cnt_hist_std',\n#                      'item_cnt_shifted_2', 'item_cnt_shifted_3',\n#                      'hist_shop_mean', 'hist_shop_item_mean', 'item_trend', 'mean_item_cnt'] \n\ncbt_model_features = ['month','year','shop_id','item_id']\n\ncbt_x_train = x_train[cbt_model_features]\ncbt_x_val = x_val[cbt_model_features]\n\ncbt_x_train['month'] = cbt_x_train['month'].astype(np.int) \ncbt_x_val['month'] = cbt_x_val['month'].astype(np.int) \ncbt_x_train['year'] = cbt_x_train['year'].astype(np.int) \ncbt_x_val['year'] = cbt_x_val['year'].astype(np.int) \ncbt_x_train['shop_id'] = cbt_x_train['shop_id'].astype(np.int) \ncbt_x_val['shop_id'] = cbt_x_val['shop_id'].astype(np.int) \ncbt_x_train['item_id'] = cbt_x_train['item_id'].astype(np.int) \ncbt_x_val['item_id'] = cbt_x_val['item_id'].astype(np.int)\n\ncatboost_model = CatBoostRegressor(\n    iterations=100,\n    max_ctr_complexity=4,\n    random_seed=0,\n    od_type='Iter',\n    od_wait=25,\n    verbose=50,\n    depth=4\n)\n\ncatboost_model.fit(\n    cbt_x_train, y_train,\n    cat_features=cbt_model_features,\n    eval_set=(cbt_x_val, y_val)\n)\n\ncatboost_model.score","414bb576":"#Prediction\ny_pred=rfr_model.predict(x_val)\n# a data frame with actual and predicted values of y\nevaluate = pd.DataFrame({'Actual': y_val.values.flatten(), 'Predicted': y_pred.flatten()})\nevaluate.head(10).T","9c19d058":"y_test = x_test\ny_test[target_feature]=0\ny_test = y_test[target_feature]\nrfr_x_test = x_test[rfr_model_features]\n\nprint(\"y_test\", y_test.shape)\nprint(\"rfr_x_test\", rfr_x_test.shape)\nprint(\"x_test\", x_test.shape)\n\n","6a9bbc30":"y_test = x_test\ny_test[target_feature]=0\ny_test = y_test[target_feature]\n\nrfr_x_test = x_test[rfr_model_features]\nlre_x_test = x_test[lre_model_features]\nlre_x_test = scaler.transform(lre_x_test) # Normalized\nxgb_x_test = x_test[xgb_model_features]\n\nrfr_val_preds = rfr_model.predict(rfr_x_val)\nlre_val_preds = lre_model.predict(lre_x_val)\nxgb_val_preds = xgb_model.predict(xgb_x_val)\n\nrfr_test_preds = rfr_model.predict(rfr_x_test)\nlre_test_preds = lre_model.predict(lre_x_test)\nxgb_test_preds = xgb_model.predict(xgb_x_test)\n\n\nstacked_val_predictions = np.column_stack((rfr_val_preds,lre_val_preds, xgb_val_preds))\nstacked_test_predictions = np.column_stack((rfr_test_preds,lre_test_preds, xgb_test_preds))\n\nmetaModel = LinearRegression()\nmetaModel.fit(stacked_val_predictions, y_val)\nfinal_predictions = metaModel.predict(stacked_test_predictions)","d89baeb8":"#final_predictions\n#rfr_test_preds\n#lre_test_preds\n#xgb_test_preds\n#lre_x_test","cc41bceb":"test2 = pd.read_csv(base_path+\"test.csv\");\nprediction_df = pd.DataFrame(test2['ID'], columns=['ID'])\nprediction_df['item_cnt_month'] = final_predictions.clip(0., 20.)\nprediction_df.to_csv('submission_06.csv', index=False)\nprediction_df.head(10)","958d40a5":"df = pd.DataFrame({\"Col1\": [10, 20, 15, 30, 45],\n                   \"Col2\": [13, 23, 18, 33, 48],\n                   \"Col3\": [17, 27, 22, 37, 52]},\n                  index=pd.date_range(\"2020-01-01\", \"2020-01-05\"))\n\n\ndf","1ff7da67":"# Prueba para enviar a la plataforma\ntest = pd.read_csv(base_path+\"test.csv\");\ncopy = sales_train\ncopy = copy[copy[\"item_price\"]<=40000]\ncopy = copy[copy[\"item_cnt_day\"]<=50]\ncopy = copy[copy[\"item_cnt_day\"]>0]\nsum_by_month = copy.groupby([\"shop_id\",\"item_id\",\"date_block_num\"] , as_index=False)[\"item_cnt_day\"].agg(\"sum\")\nmean_expected = sum_by_month.groupby([\"shop_id\",\"item_id\"] , as_index=False)[\"item_cnt_day\"].agg(\"mean\")\nprint(mean_expected.shape)\nprint(test.shape)\nresult = pd.merge(test, mean_expected, on=['shop_id','item_id',], how='left')\n","2a43b95c":"test[test['shop_id']==10]","3d22628d":"**Data leakages**\n\nSome data of train set does not exists in test data, so, is not necesary have all of this information. Next it will be removed data that not exists in test set.","17eee1b1":"**Catboost**","01870f61":"# Stacking","178f1cb5":"**Validating models**\n\nIt will be split training data set into training data and validation data in order to validate model. It will be used last month data as validationd data set. Considering it is a problem of forecasting it will be taken last tree months to estimate some data for validation data set.","920d1886":"Building validation\/training data","a0af9f6f":"**TO DO**\n* Data Standardization\n* Data Normalization\n* Binning","17d4fb6f":"It's highly probable that all items did not sell all months in all stores. So, we will complete missing values with zero.","d1420b5e":"# Test predictions\n","8faffd33":"# Data Acquisition\n\nLoad datasets into our Jupyter Notebook. Datasets are given in CSV (comma separated value) format.\n","d5827729":"Check some summary statistics for Sales, excluding NaN (Not a Number) values. Join sales to item data set in order to have category information","4e540248":"Now we plot items count vs prices to identify how prices influence sales.","31815ccd":"On previous data we can see that some shops have similar name (0=57, 1=58, 10=11 and 39 = 40) maybe they are duplicated and it wil be necesary merge transactions of duplicate in only one shop. I will check if they appear in test data set.","5acf9540":"In test data set there are not shops 0, 1, 11, 40, so their data will be moved to the shop which name matches.","665e50ed":"Considering that the objective is predict sales for a item, in a shop, during a month, data will be aggruped by these attributes in addition to item_category_id, year and month.\n\nWith aggregation it will be added new features in order to improve predictions.\n* **sum_item_price**: sum of item prices by year-month, item, shop_id\n* **mean_item_price**: mean of item prices by year-month, item, shop_id\n* **item_cnt_month**: sum of items sold by year-month, item, shop_id\n* **mean_item_cnt**: men of items sold by year-month, item, shop_id\n* **transactions**: number of transactions by year-month, item, shop_id\n* **month_rate**: qualification given to month according to weekend days and business days it has","931fc10c":"Exploring categories","c1720650":"Random Forest","bb273191":"The figure above shows that there are more sales on wekends (friday, saturday and sunday). We can include a feature that sum the number of days that a month have, given a higher weight if day is weekend and lower if it is not weekend. Next it is defined a custom function that could be use as lambda function to rate month. It will be used in next steps.","4308247b":"# Exploratory Data Analysis","1eb31cad":"# Basic exploring of data (EDA)\n\nBasic Insight of Dataset. Exploring of data in order to understand how it composed.","3071a3d3":"Previous plot shows that some categories have more transacctions. Below I explore in detail shops per item category.","4ee18f90":"Considering it is a problem of forecasting it will be taken last tree months as base to forecast next month behavior.","3e9d1f5c":"Exploring shops","aef8472f":"Plots above shows that sales tend to decline in recent months. Sales depends on months. We can see that some months have more sales than other. Also, price is an important feature to consider in the prediction of target value. Now we will evaluate de behavior of sales according to week day the sale is done","8498096b":"**Modelo de regresi\u00f3n lineal**\n\n> Normalizaci\u00f3n\n","509e49b5":"**Build test data set**\n\nWe want to predict last date_block_num (34), so, we will use data of previuos block (33)","4d0c6e2b":"Previous plot shows that shops are made mostly for 8 items categories. Now we will analize shops behavior in time for diferent periods.","576de105":"**XGBoost**","bb70c149":"Probabily in 3 months windows we did does not find data for all combinations of item_id, shop_id, category_id. If that case occurs data will be completed with historical mean.","a1b6ea63":"**Submission**","c63b888b":"**Outliers**\n\nGraph above shows some outliers. There are items with prices out of common and there are transactions with a lot of items buyed. We will remove sales where item price is greater than $40.000 and item_cnt_day > 100.\nThis plot also reveals that the cheaper item is, the more sells it has.","cd8860cd":"It will be added a new column that will have the target value of next month, considering that its a problem of forecasting","dd09742b":"# Data Wrangling\n\n1. Identify and deal with missing values\n2. Identify and deal with data with invalid values. As seen in summary statistics some items have price -1.\n2. Converting data from the initial format to a format that may be better for analysis."}}