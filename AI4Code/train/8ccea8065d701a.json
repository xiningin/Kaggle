{"cell_type":{"8cf4304e":"code","8d8e8eb2":"code","6f02bcb0":"code","21c4f6a1":"code","098baf99":"code","799db850":"code","c9452dcf":"code","495b7948":"code","e58cb544":"code","bcc18dd8":"code","b7ce29ad":"code","b2a81c1a":"code","a0b2140a":"markdown","45fc23fa":"markdown","6064d23d":"markdown","6ab63a6e":"markdown","ec185cf5":"markdown","60b34ebe":"markdown","8371521a":"markdown","617f74d2":"markdown","5824f22a":"markdown","222f4526":"markdown"},"source":{"8cf4304e":"import pandas as pd\nimport librosa\nimport soundfile\nimport os, glob, pickle\nimport numpy as np\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings('ignore')","8d8e8eb2":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","6f02bcb0":"def extract_feature(file_name):\n    X, sample_rate = librosa.load(file_name)\n    stft=np.abs(librosa.stft(X))\n    result=np.array([])\n    mfccs=np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0)\n    result=np.hstack((result, mfccs))\n    chroma=np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n    result=np.hstack((result, chroma))\n    mel=np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n    result=np.hstack((result, mel))\n    return result\n","21c4f6a1":"emotions={\n  '01':'neutral',\n  '02':'calm',\n  '03':'happy',\n  '04':'sad',\n  '05':'angry',\n  '06':'fearful',\n  '07':'disgust',\n  '08':'surprised'\n}\n\ndef gender(g):\n    if int(g[0:2]) % 2 == 0:\n        return 'female'\n    else:\n        return 'male'\n","098baf99":"def load_data(test_size=0.2):\n    x,y=[],[]\n    for file in tqdm(glob.glob(\"..\/input\/ravdess-emotional-speech-audio\/Actor_*\/*.wav\")):\n        file_name=os.path.basename(file)\n        emotion=emotions[file_name.split(\"-\")[2]] + '_' + gender(file_name.split(\"-\")[-1])\n        feature=extract_feature(file)\n        x.append(feature)\n        y.append(emotion)\n    return train_test_split(np.array(x), y, test_size=test_size, random_state=1)","799db850":"X_train, X_val, y_train, y_val = load_data()","c9452dcf":"print((X_train.shape[0], X_val.shape[0]))","495b7948":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_val = scaler.transform(X_val)","e58cb544":"print(f'Features extracted: {X_train.shape[1]}')","bcc18dd8":"from sklearn.neural_network import MLPClassifier\n\nmodel=MLPClassifier(alpha=0.01, batch_size=256, epsilon=1e-08, hidden_layer_sizes=(300,), learning_rate='adaptive', max_iter=500)\nmodel.fit(X_train,y_train)\nprint(model.score(X_train, y_train))","b7ce29ad":"y_pred=model.predict(X_val)\nprint(model.score(X_val, y_val))","b2a81c1a":"df=pd.DataFrame({'Actual': y_val, 'Predicted':y_pred})\ndf","a0b2140a":"Import libraries","45fc23fa":"Dictionary of emotions","6064d23d":"Make predictions on validation set","6ab63a6e":"Split dataset for training and validating","ec185cf5":"Select model","60b34ebe":"Scale data","8371521a":"Function to extract features","617f74d2":"Function to load data","5824f22a":"Problem statement:-\n\nSpeech emotion recognition, the best ever python mini project. The best example of it can be seen at call centers. If you ever noticed, call centers employees never talk in the same manner, their way of pitching\/talking to the customers changes with customers. Now, this does happen with common people too, but how is this relevant to call centers? Here is your answer, the employees recognize customers\u2019 emotions from speech, so they can improve their service and convert more people. In this way, they are using speech emotion recognition.","222f4526":"Check shapes"}}