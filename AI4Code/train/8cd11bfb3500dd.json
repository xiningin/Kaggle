{"cell_type":{"e4f86a66":"code","7570052e":"code","31869eec":"code","ba769c31":"code","ce6c8293":"code","fca61631":"code","cf50c43a":"code","05ab9421":"code","40657d29":"code","f65364cf":"code","813894bb":"code","09ad78fa":"code","bdffe68c":"code","e7ee2085":"code","dad354a8":"code","82a514a6":"code","a0b16fe2":"code","8d9e5b0b":"code","5c1920bb":"code","69ae6160":"code","c136f24a":"code","b0bc2b79":"code","6721e019":"code","e8a90c77":"markdown","1dda4367":"markdown","c24a8cda":"markdown","0de570da":"markdown","1f5d179d":"markdown","abe12b06":"markdown","ec96aaa9":"markdown","ead68d2a":"markdown","b9271901":"markdown","5b37ab8b":"markdown"},"source":{"e4f86a66":"! pip install -q efficientnet","7570052e":"# Basic library\nimport numpy as np \nimport pandas as pd \nimport os\n\n# Data preprocessing\nfrom sklearn.model_selection import train_test_split\n\n# Visualization\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\n\n# tensorflow\nimport tensorflow as tf\nimport tensorflow.keras.layers as l\nimport efficientnet.tfkeras as efn\n\n# data set\nfrom kaggle_datasets import KaggleDatasets","31869eec":"# TPU setting\n# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","ba769c31":"print(tpu.master())\nprint(tpu_strategy.num_replicas_in_sync)","ce6c8293":"# For tensorflow dataset\nAUTO = tf.data.experimental.AUTOTUNE\nignore_order = tf.data.Options()\nignore_order.experimental_deterministic = False\n\n# Pass\ngcs_path = KaggleDatasets().get_gcs_path()","fca61631":"# Sample dataframe\nsample = pd.read_csv(\"\/kaggle\/input\/alaska2-image-steganalysis\/sample_submission.csv\")","cf50c43a":"# batch size in tpu\nBATCH_SIZE = 32 * tpu_strategy.num_replicas_in_sync","05ab9421":"# Directrory and file name\n# Drop csv from dir_name\ndir_name = ['Test', 'JUNIWARD', 'JMiPOD', 'Cover', 'UERD']\ndf = pd.DataFrame({})\n\n# Create empty dataframe and list\nlists = []\ncate = []\n\n# get the filenames\nfor dir_ in dir_name:\n    # file name\n    list_ = os.listdir(\"\/kaggle\/input\/alaska2-image-steganalysis\/\"+dir_+\"\/\")\n    lists = lists+list_\n    # category name\n    cate_ = np.tile(dir_,len(list_))\n    cate = np.concatenate([cate,cate_])\n    \n# insert dataframe\ndf[\"cate\"] = cate\ndf[\"name\"] = lists","40657d29":"# path line\ndf[\"path\"] = [str(os.path.join(gcs_path,cate,name)) for cate, name in zip(df[\"cate\"], df[\"name\"])]","f65364cf":"# Labeling positive and negative\ndef cate_label(x):\n    if x[\"cate\"] == \"Cover\":\n        res = 0\n    else:\n        res = 1\n    return res\n\n# Test dataframe and Train dataframe\nTest_df = df.query(\"cate=='Test'\").sort_values(by=\"name\")\nTrain_df = df.query(\"cate!='Test'\")\n# Apply the function\nTrain_df[\"flg\"] = df.apply(cate_label, axis=1)","813894bb":"# label_counts\nTrain_df[\"cate\"].value_counts()","09ad78fa":"Train_df = Train_df.sample(80000)\n# label_counts\nTrain_df[\"cate\"].value_counts()","bdffe68c":"# Create train data and val data\nX = Train_df[\"path\"]\ny = Train_df[\"flg\"]\n\n# split train and val data\nX_train, X_val, y_train, y_val = train_test_split(X,y, test_size=0.2, random_state=10)","e7ee2085":"# Change to numpy array\nX_train, X_val, y_train, y_val = np.array(X_train), np.array(X_val), np.array(y_train), np.array(y_val)","dad354a8":"# Create test data\nX_test = np.array(Test_df[\"path\"])","82a514a6":"def decode_image(filename, label=None, image_size=(512,512)):\n    bits = tf.io.read_file(filename)\n    image = tf.image.decode_jpeg(bits, channels=3)\n    image = tf.cast(image, tf.float32)\/255.0\n    image = tf.image.resize(image, image_size)\n    \n    if label is None:\n        return image\n    else:\n        return image, label","a0b16fe2":"AUTO = tf.data.experimental.AUTOTUNE\nignore_order = tf.data.Options()\nignore_order.experimental_deterministic = False\n\n# Build input pipeline\ntrain_dataset = (tf.data.Dataset.from_tensor_slices((X_train, y_train)).prefetch(AUTO).with_options(ignore_order)\n                 .map(decode_image, num_parallel_calls=AUTO).shuffle(512).batch(BATCH_SIZE).repeat())\n\nvalid_dataset = (tf.data.Dataset.from_tensor_slices((X_val, y_val)).map(decode_image, num_parallel_calls=AUTO)\n                    .cache().batch(BATCH_SIZE).prefetch(AUTO))\n\ntest_dataset = (tf.data.Dataset.from_tensor_slices((X_test)).map(decode_image, num_parallel_calls=AUTO)\n                    .batch(BATCH_SIZE))","8d9e5b0b":"with tpu_strategy.scope():\n    model_b7 = tf.keras.Sequential([\n        efn.EfficientNetB7(input_shape=(512,512,3),weights='imagenet',include_top=False),\n        l.GlobalAveragePooling2D(),\n        l.Dense(1, activation=\"sigmoid\")\n    ])\n    \n    model_b7.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n    model_b7.summary()","5c1920bb":"STEPS_PER_EPOCH = X_train.shape[0] \/\/ BATCH_SIZE\ncallbacks = [tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)]\n\nEPOCHS = 5\nhist_b7 = model_b7.fit(train_dataset, epochs=EPOCHS,\n                   steps_per_epoch=STEPS_PER_EPOCH, validation_data=valid_dataset, callbacks=callbacks, workers=4, use_multiprocessing=True)","69ae6160":"# Prediction\npred_b7 = model_b7.predict(test_dataset, verbose=1)","c136f24a":"# training history\ntrain_loss = hist_b7.history[\"loss\"]\nval_loss = hist_b7.history[\"val_loss\"]\ntrain_acc = hist_b7.history[\"accuracy\"]\nval_acc = hist_b7.history[\"val_accuracy\"]\n\nfig, ax = plt.subplots(1,2,figsize=(10,6))\nax[0].plot(range(len(train_loss)), train_loss, label=\"train_loss\")\nax[0].plot(range(len(val_loss)), val_loss, label=\"val_loss\")\nax[0].set_xlabel(\"epochs\")\nax[0].set_ylabel(\"loss\")\nax[0].set_title(\"EfficientNetB7 loss\")\nax[0].legend()\n\nax[1].plot(range(len(train_acc)), train_acc, label=\"train_accuracy\")\nax[1].plot(range(len(val_acc)), val_acc, label=\"val_accuracy\")\nax[1].set_xlabel(\"epochs\")\nax[1].set_ylabel(\"accuracy\")\nax[1].set_title(\"EfficientNetB7 accurary\")\nax[1].legend()","b0bc2b79":"# EfficientNetB7\nsample_7 = sample.copy()\nsample_7[\"Label\"] = pred_b7\nsample_7.to_csv(\"submission_B7.csv\", index=False)","6721e019":"sample_7[\"Label\"].describe()","e8a90c77":"# Data and path preprocessing","1dda4367":"Model : EfficientNetB7<br>","c24a8cda":"TPU Setting","0de570da":"# Prediction","1f5d179d":"Since I need to keeping memory and running time over, the number of samples was smalled 60000 data wset.","abe12b06":"### Calculation","ec96aaa9":"# Alaska2,Try to predict by EfficientNet","ead68d2a":"Data set loadng","b9271901":"# Modeling","5b37ab8b":"As my first try for the ALSKA2 competition, I made predictions using tensorflow efficient net B7 model.<br>"}}