{"cell_type":{"040834a3":"code","f96b5f52":"code","bda7ddb4":"code","ed7b250b":"code","6fd2e960":"code","74fb367d":"code","7c477ad7":"code","abddf21c":"code","98e5bd9f":"code","51ef2b24":"code","132bae42":"code","028a9f2d":"code","0ae52a1e":"code","ae1ba63c":"code","1d901772":"code","44c8710b":"code","8fda697f":"code","3904cc66":"code","c5e7afea":"code","bc9bf743":"code","362976e2":"code","84984ea0":"code","ac69e9c1":"markdown","aae5bbc1":"markdown","61825015":"markdown","76e6bf18":"markdown","3f61d5b7":"markdown","4ce67013":"markdown","85b4f05b":"markdown","2c48cb5a":"markdown","18b3be92":"markdown","d5079bde":"markdown","99ac920f":"markdown","59a9b92a":"markdown","ddbcd0aa":"markdown","cbad248d":"markdown","8db5615e":"markdown","cae7f2d9":"markdown","c7b5f46e":"markdown","5637a8cc":"markdown","be0b9809":"markdown","d28f0e9f":"markdown","4dad2979":"markdown","cbcb620e":"markdown","1fb54a1f":"markdown","ea5d0a69":"markdown","d97c7f5d":"markdown","cf308f38":"markdown","03558d1a":"markdown","fb3814e9":"markdown","1634cf0e":"markdown","be668df5":"markdown","5f0a2c80":"markdown","69724c9a":"markdown","51233511":"markdown","39db0249":"markdown","8f8f4387":"markdown","ed5d39cd":"markdown","198b8148":"markdown","81393d1b":"markdown","7557dcb2":"markdown","cfad8d83":"markdown","7f5d860f":"markdown","9c838d26":"markdown","e26ba7f3":"markdown","9b0bf080":"markdown","b4acdc59":"markdown","a4c21f75":"markdown","3cca5eba":"markdown","3800f840":"markdown"},"source":{"040834a3":"import itertools\nfrom tqdm import tqdm, tnrange, tqdm_notebook\nfrom mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np \nimport seaborn as sns\nfrom scipy import stats\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nfrom statsmodels.graphics.regressionplots import *\nfrom sklearn.linear_model import LinearRegression, TheilSenRegressor\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom IPython.core.interactiveshell import InteractiveShell\n\n\nInteractiveShell.ast_node_interactivity = \"all\"  # make multiple printed outputs possible from one cell\n\n","f96b5f52":"# Read data\n\ndf = pd.read_csv(\"..\/input\/Admission_Predict.csv\")\n\ndf.shape  # small dataset\n\ndf.info()","bda7ddb4":"df.head(n=10)","ed7b250b":"df.describe()","6fd2e960":"df.nunique()","74fb367d":"df.drop([\"Serial No.\"],axis=1,inplace=True)\ncoa = df[[\"Chance of Admit \"]]\ndf.drop([\"Chance of Admit \"],axis=1,inplace=True)\ndf[\"Chance_of_Admit\"] = coa\nlor = df[[\"LOR \"]]\ndf.drop([\"LOR \"],axis=1,inplace=True)\ndf[\"LOR\"] = lor\ngre = df[[\"GRE Score\"]]\ndf.drop([\"GRE Score\"],axis=1,inplace=True)\ndf[\"GRE_Score\"] = gre\ntoe = df[[\"TOEFL Score\"]]\ndf.drop([\"TOEFL Score\"],axis=1,inplace=True)\ndf[\"TOEFL_Score\"] = toe\nur = df[\"University Rating\"] \ndf.drop([\"University Rating\"],axis=1,inplace=True)\ndf[\"University_Rating\"] = ur","7c477ad7":"df.head()","abddf21c":"def heatMap(df, mirror):\n\n   # Create Correlation df\n   corr = df.corr()\n   # Plot figsize\n   fig, ax = plt.subplots(figsize=(10, 10))\n   # Generate Color Map\n   colormap = sns.diverging_palette(220, 10, as_cmap=True)\n   \n   if mirror == True:\n      #Generate Heat Map, allow annotations and place floats in map\n      sns.heatmap(corr, cmap=colormap, annot=True, fmt=\".2f\")\n      #Apply xticks\n      plt.xticks(range(len(corr.columns)), corr.columns);\n      #Apply yticks\n      plt.yticks(range(len(corr.columns)), corr.columns)\n      #show plot\n\n   else:\n      # Drop self-correlations\n      dropSelf = np.zeros_like(corr)\n      dropSelf[np.triu_indices_from(dropSelf)] = True\n      # Generate Color Map\n      colormap = sns.diverging_palette(220, 10, as_cmap=True)\n      # Generate Heat Map, allow annotations and place floats in map\n      sns.heatmap(corr, cmap=colormap, annot=True, fmt=\".2f\", mask=dropSelf)\n      # Apply xticks\n      plt.xticks(range(len(corr.columns)), corr.columns);\n      # Apply yticks\n      plt.yticks(range(len(corr.columns)), corr.columns)\n   # show plot\n   plt.show()\n\nheatMap(df,True)","98e5bd9f":"sns.pairplot(df)","51ef2b24":"## plots to help ascertain if linear regression assumptions being violated\n\nfull_X = df.drop([\"Chance_of_Admit\"],axis=1)\nfull_y = df[[\"Chance_of_Admit\"]]\n\n\ndef LR_assump(X,y):\n    cX = sm.add_constant(X)\n    ols_model = sm.OLS(y, cX)\n\n    ols_results = ols_model.fit()\n    infl = ols_results.get_influence()\n    measuredf = infl.summary_frame()\n    Tresid = measuredf[\"student_resid\"]\n    residuals = ols_results.resid\n    fig, axs = plt.subplots(2, 2,figsize=(20,20))\n    axs[0,0].set_title('Normal QQ Plot')\n    qq = stats.probplot(residuals,dist=\"norm\",plot=axs[0,0])  # to suppress array being printed\n    axs[0,1].set_title(\"Index Plot of Studentised Residuals\")\n    axs[0,1].set_ylabel('Tresid')\n    axs[0,1].set_xlabel('Index')\n    axs[0,1].axhline(y=2,xmin=0,xmax=500)  # cutoffs for studentised resid, find outliers this way\n    axs[0,1].axhline(y=-2,xmin=0,xmax=500)\n    sns.scatterplot(np.arange(len(Tresid)), Tresid, ax=axs[0,1])\n    axs[1,0].set_title(\"Histogram of Studentised Residuals\")\n    axs[1,0].set_xlabel('Tresid')\n    axs[1,0].set_ylabel('Frequency')\n    sns.distplot(Tresid,ax=axs[1,0])\n    fv = ols_results.fittedvalues\n    axs[1,1].set_title(\"Plot of Studentised Residuals vs Fitted Values\")\n    axs[1,1].set_ylabel('Tresid')\n    axs[1,1].set_xlabel('Fitted Values')\n    axs[1,1].axhline(y=2,xmin=0,xmax=500)  # cutoffs for studentised resid, find outliers this way\n    axs[1,1].axhline(y=-2,xmin=0,xmax=500)\n    sns.scatterplot(fv, Tresid, ax=axs[1,1])\nLR_assump(full_X,full_y)","132bae42":"# Measures to determine influential points\nm = ols(formula='Chance_of_Admit ~ GRE_Score + TOEFL_Score + SOP + LOR + CGPA + Research + University_Rating', data=df).fit()\ninfl = m.get_influence()\nfig, axs = plt.subplots(2, 2,figsize=(20,20))\n\n# Set up values\n(c, p) = infl.cooks_distance\n(d, p) = infl.dffits\nlev = infl.hat_matrix_diag\n\n\naxs[0,0].set_title('Index Plots of Cook\\'s Distance')\naxs[0,0].set_ylabel('Cook\\'s Distance Value')\naxs[0,0].set_xlabel('Index')\nsns.scatterplot(np.arange(len(c)), c, ax=axs[0,0])\n\naxs[0,1].set_title('Index Plots of DFITS')\naxs[0,1].set_ylabel('DFITS Value')\naxs[0,1].set_xlabel('Index')\ndfitsCutOff = 2*(np.sqrt((df.shape[1])\/(df.shape[0]-df.shape[1]-2)))\ndfitsCutOff\naxs[0,1].axhline(y=0.255,xmin=0,xmax=500)\nsns.scatterplot(np.arange(len(d)), d, ax=axs[0,1])\n\n\naxs[1,0].set_title('Index Plots of Leverage Values')\nlevcutoff = 2*(7+1)\/full_X.shape[0]\naxs[1,0].set_ylabel('Leverage Values')\naxs[1,0].set_xlabel('Index')\naxs[1,0].axhline(y=levcutoff,xmin=0,xmax=500)\nsns.scatterplot(np.arange(len(lev)), lev, ax=axs[1,0])\n\nPi = lev\/(1-lev)\nSSE = m.mse_total * len(full_y)\ndi = m.resid\/(SSE**(1\/2))\nDi=(7+1)\/(1-lev)*(di**2\/(1-di**2))\n\n\naxs[1,1].set_title('Potential Residual Plot')\naxs[1,1].set_xlabel('Potential Function Values')\naxs[1,1].set_ylabel('Scaled Residuals')\nsns.scatterplot(Pi, Di, ax=axs[1,1])","028a9f2d":"# tank yew stats models \n\nfig = plt.figure(figsize=(12, 20))\nfig = sm.graphics.plot_ccpr_grid(m, fig=fig)","0ae52a1e":"# Full model\n\nX = df.drop([\"Chance_of_Admit\"],axis=1)\ny = df[[\"Chance_of_Admit\"]]\n\nlr = LinearRegression()\nlr.fit(X,y)\npred_y = lr.predict(X)\nr2_score(y, pred_y)  # initial r2 score\n\n","ae1ba63c":"# Build step forward feature selection\n\nlr = LinearRegression()\n\nsfs1 = SFS(estimator=lr, \n           k_features=(1, 7),\n           forward=True, \n           floating=False, \n           scoring='r2',\n           cv=10)\n\nsfs1.fit(X, y)\n\nprint('best combination (ACC: %.3f): %s\\n' % (sfs1.k_score_, sfs1.k_feature_idx_))\nprint('all subsets:\\n', sfs1.subsets_)\nplot_sfs(sfs1.get_metric_dict(), kind='std_err');   # lower R^2 than above with cross validation","1d901772":"# may not be accurate mallows cp \n\nfull_X = df.drop([\"Chance_of_Admit\"],axis=1)\nfull_y = df[[\"Chance_of_Admit\"]]\nfX_train, fX_test, fy_train, fy_test = train_test_split(full_X,y, test_size=0.3,train_size=0.7,random_state=42)\n\n\ndef Cp(y,pred,p,n,model):\n    model.fit(full_X,full_y.values.ravel())\n    SSEfull = mean_squared_error(full_y,model.predict(full_X)) * len(full_y)   # subset Sum squared error\n    sigma_hat_squared = SSEfull\/(len(full_y)-len(full_X.columns)-1)\n    SSEp = mean_squared_error(y,pred) * len(y)\n    return (SSEp\/sigma_hat_squared) - n + 2*p    ## closer to 0 the better \n    ","44c8710b":"\ndef LRfit(X,y):\n    #Fit linear regression model and return RSS and R squared values\n    \n    lr = LinearRegression(fit_intercept = True)\n    lr.fit(X,y)\n    pred = lr.predict(X)\n    RSS = mean_squared_error(y,pred) * len(y)\n    SSE = ((y-pred)**2)[\"Chance_of_Admit\"].sum()\n    SST = ((y-y.mean())**2)[\"Chance_of_Admit\"].sum()\n    R_squared = 1 -(SSE\/SST)\n    adjR_sq = 1 - (1-R_squared)*((X.shape[0]-1)\/(X.shape[0]-len(X.columns)-1))\n    p = len(X.columns)\n    n = len(y)\n    cp = abs(Cp(y,pred,p,n,LinearRegression(fit_intercept = True)) - p - 1) \n    return RSS,SSE, R_squared, adjR_sq, cp\n\n\n#Init variables\n\nRSS_list, SSE_list, R_squared_list, adjR_squared_list, Cp_list, feature_list = [],[],[],[],[],[]\nnum_features = []\n\n#Looping over k = 1 to k = 7 features in X\nfor k in tnrange(1,len(full_X.columns) + 1, desc = 'Loop...'):\n\n    #Looping over all possible combinations\n    for combo in itertools.combinations(full_X.columns,k):\n        results = LRfit(full_X[list(combo)],full_y)\n        RSS_list.append(results[0])\n        SSE_list.append(results[1])                  #Append lists\n        R_squared_list.append(results[2])\n        adjR_squared_list.append(results[3])\n        Cp_list.append(results[4])\n        feature_list.append(combo)\n        num_features.append(len(combo))   \n\n#Store in DataFrame\nLRdf = pd.DataFrame({'num_features': num_features,\"RSS\": RSS_list,'SSE': SSE_list,'R_squared':R_squared_list,\n                     'Adj R_squared':adjR_squared_list,\"|Cp-p-1|\": Cp_list,'features':feature_list})\nLRdf.sort_values(\"|Cp-p-1|\",axis=0,ascending=True)","8fda697f":"## Define function to fit Theil Sen for easier, repeatable subset fitting\n\ndef TSfit(X,y):\n    ts = TheilSenRegressor(random_state=0)\n    ts.fit(X,y.values.ravel())\n    pred = ts.predict(X)\n    SSE = mean_squared_error(y.values.ravel(),pred) * len(y)\n    SST = ((y-y.mean())**2)[\"Chance_of_Admit\"].sum()\n    R_squared = 1 -(SSE\/SST)\n    adjR_sq = 1 - (1-R_squared)*((X.shape[0]-1)\/(X.shape[0]-len(X.columns)-1))\n    p = len(X.columns)\n    n = len(y)\n    cp = abs(Cp(y,pred,p,n,TheilSenRegressor(random_state=0)) - p - 1) \n    return SSE, R_squared, adjR_sq, cp\n\n","3904cc66":"\n#Init variables\n\nSSE_list, R_squared_list, adjR_squared_list, Cp_list, feature_list = [],[],[],[],[]\nnum_features = []\n\n#Looping over k = 1 to k = 7 features in X\nfor k in tnrange(1,len(full_X.columns) + 1, desc = 'Loop...'):\n\n    #Looping over all possible combinations\n    for combo in itertools.combinations(full_X.columns,k):\n        results = TSfit(full_X[list(combo)],y)\n        \n        SSE_list.append(results[0])                  #Append lists\n        R_squared_list.append(results[1])\n        adjR_squared_list.append(results[2])\n        Cp_list.append(results[3])\n        feature_list.append(combo)\n        num_features.append(len(combo))   \n\n#Store in DataFrame\nTSdf = pd.DataFrame({'num_features': num_features,'SSE': SSE_list,'R_squared':R_squared_list, 'Adj R_squared':adjR_squared_list,\"|Cp-p-1|\": Cp_list,'features':feature_list})\nTSdf.sort_values(\"|Cp-p-1|\",axis=0,ascending=True)\n","c5e7afea":"full_X = df.drop([\"Chance_of_Admit\"],axis=1)\nfull_y = df[[\"Chance_of_Admit\"]]**2\n\nLR_assump(full_X,full_y)","bc9bf743":"RSS_list, SSE_list, R_squared_list, adjR_squared_list, Cp_list, feature_list = [],[],[],[],[],[]\nnum_features = []\n\n#Looping over k = 1 to k = 7 features in X\nfor k in tnrange(1,len(full_X.columns) + 1, desc = 'Loop...'):\n\n    #Looping over all possible combinations\n    for combo in itertools.combinations(full_X.columns,k):\n        results = LRfit(full_X[list(combo)],full_y)\n        RSS_list.append(results[0])\n        SSE_list.append(results[1])                  #Append lists\n        R_squared_list.append(results[2])\n        adjR_squared_list.append(results[3])\n        Cp_list.append(results[4])\n        feature_list.append(combo)\n        num_features.append(len(combo))   \n\n#Store in DataFrame\nLRdf = pd.DataFrame({'num_features': num_features,\"RSS\": RSS_list,'SSE': SSE_list,'R_squared':R_squared_list,\n                     'Adj R_squared':adjR_squared_list,\"|Cp-p-1|\": Cp_list,'features':feature_list})\nLRdf.sort_values(\"|Cp-p-1|\",axis=0,ascending=True)","362976e2":"SSE_list, R_squared_list, adjR_squared_list, Cp_list, feature_list = [],[],[],[],[]\nnum_features = []\n\n#Looping over k = 1 to k = 7 features in X\nfor k in tnrange(1,len(full_X.columns) + 1, desc = 'Loop...'):\n\n    #Looping over all possible combinations\n    for combo in itertools.combinations(full_X.columns,k):\n        results = TSfit(full_X[list(combo)],full_y)\n        \n        SSE_list.append(results[0])                  #Append lists\n        R_squared_list.append(results[1])\n        adjR_squared_list.append(results[2])\n        Cp_list.append(results[3])\n        feature_list.append(combo)\n        num_features.append(len(combo))   \n\n#Store in DataFrame\nTSdf = pd.DataFrame({'num_features': num_features,'SSE': SSE_list,'R_squared':R_squared_list, 'Adj R_squared':adjR_squared_list,\"|Cp-p-1|\": Cp_list,'features':feature_list})\nTSdf.sort_values(\"|Cp-p-1|\",axis=0,ascending=True)\n","84984ea0":"X = df.drop([\"Chance_of_Admit\"],axis=1)\ny = df[[\"Chance_of_Admit\"]]\n\nX = X[[\"CGPA\", \"LOR\", \"GRE_Score\", \"TOEFL_Score\"]]\n\nts = TheilSenRegressor(random_state=0)\nts.fit(X,y.values.ravel())\nts.coef_","ac69e9c1":"## Feature Selection","aae5bbc1":"First, we detect the presence of high leverage points by using the leverage index plot. We can observe from the plot that some observations lie further away from the others, suggesting that there may be high leverage observations in our data. We use the\ncutoff value of 2(p+1)\/n to reveal points with high leverage observations. \n\nBased on the leverage index plot we can observe a few high leverage points among our data. We can conclude that there are high leverage observations in our data. Second, we would like to detect the outliers in the response variable. We can see from our index plot of studentised residuals that there are a few observations that are far from the main trend and we can thus conclude that there are a few outliers in the response variable, i.e. The Chance of Admission. \n\nThird, we would like to detect whether there is any influential point. We will use 2 methods, namely Cook's Distance and the Welsch and Kuh Measure (DFITS) to detect the influential points in the data.  \n\nWe can observe from the Cook's Distance plot (upper left) that there are some observations that lie further away from the others, and to confirm our intuition, we determine a cutoff value. We use the cutoff value of $F(p + 1; n - p - 1; 0:5),$ (again, from the textbook: \n\n    \"It has been suggested that points with Ci values greater than the 50% point of the F distribution with p + 1 and (n - p - 1) degrees of freedom be classified as influential points.\"\n\n) which is 0.9192, to determine whether there are any influential points based on Cook's Distance. As we can see, all the points in our Index Plot of Cook's Distance lie within the cutoff value range, which indicates that there are an insignificant number of influential observations. However, from the plot itself, we can see some points are further away from others, suggesting that some observations are larger than others. \n\nThe second method used to detect influential points is the Welsch and Kuh Measure. This measure is implemented by plotting a graph of DFIT against the observation number (index). The cutoff value for this plot is defined to be 2*sqrt(p+1\/n-p-1) , which is 0.255 for our data. We observe from the DFITS plot (upper right) that there are only 5 influential points that lie beyond the range of the cutoff value of 0.255. Thus, we can conclude that there are an insignificant number of influential points based on the DFITS method. The Cook's Distance measure and DFITS measure gives similar conclusions for detecting influential points since they are approximately monotonically transformed from each other.\n\nNow we will examine the potential-residual plot to detect whether there are any unusual observations of more than one kind, such as high leverage points, outliers, or influential points. The potential-residual plot is obtained by plotting the potential function against the scaled residual function. \n\nFrom the Regression Analysis by Example Textbook:\n\nThe Potential-Residual plot helps to:\n\n    \"aid in classifying unusual observations as high-leverage points, outliers, or a combination of both\"\n\nBased on the scatterplot, we observe that there are several points scattered further away from the others. The points that are further away along the y-axis, Di, shows the existence of outliers in our observations, while the points that lie further along the x-axis, Pi, shows the presence of high leverage points in our observations. \n\nWe do not seem to have points that are both outliers and high leverage. We can conclude based on the plot above that there may be some outliers and high leverage points in our observations. However, the conclusion made based on the plot is only an estimation to detect whether there are any unusual observations, since there is no strong evidence that could support it.\n\nIn conclusion, there are only a few outliers and high leverage points, and no points that are both. Our data, although not perfect, does not have too many influential points, and linear regression is still very viable and probably a good choice for our data.\n","61825015":"## Understand Basics of Data","76e6bf18":"### Re-Testing assumptions","3f61d5b7":"Now that we have tested our assumptions, we first fit the base model and obtain the $R^2$ score of our prediction from sklearn, then run forward selection on our model to do feature selection.","4ce67013":"So far, we have been fitting our model and going through feature selection, without an exact goal in mind for our model. Do we want a final model that explains the most variation in our data? A model that has the best accuracy? The lowest Mean Squared Error? However, we must remember that our goal is to help prospective graduate students. Therefore, we should optimise for prediction accuracy, or in other words, we want to make future predictions as accurate as possible, and minimise Mean Sqaure Prediction Error (MSPE). However, MSPE is a theoretical value, and we have to approximate it. \n\n\nAccording to $Regression$ $Analysis$ $by$ $Example$, a metric known as Mallow's Cp is known to be a good estimate of the MSPE: \n\n    \"The Cp statistic therefore measures the performance of the variables in terms of the standardized total mean square error of prediction for the observed data points irrespective of the unknown true model\". (pg 286)\n\nThe closer |Cp - p - 1| is to 0, the better the MSPE, and consquently, the more accurate future predictions will be. In this section, we define a function that can help our model calculate the |Cp - p - 1|.\n\n\nCp = SSEp\/S^2 - n + 2p\n\nWhere SSEp is the sum of squared errors of the submodel,\n\nand $S^2 = (SSEfull)\/n-p-1$ as this is an unbiased estimator of sigma hat squared, the noise variance estimator.","85b4f05b":"# Assessing our model ","2c48cb5a":"Here we run feature selection with cross validation and wish to choose the best set of features, using R^2 as a placeholder metric to obtain a possible look at our important features.","18b3be92":"### Correlation Heatmap","d5079bde":"##  Chance of Admit = 0.1283821 * CGPA + 0.02203629 * LOR + 0.001663 * GRE Score +                                                       0.00346812 * TOEFL Score","99ac920f":"## Best Subset for Linear Regression: Theil-Sen Regressor w Box-Cox","59a9b92a":"Here, if you refer to the above residual plots, we can see that while the assumptions of linear regression are still not perfectly fulfilled, it is an improvement, and our data seems to violate linear regression assumptions less. The QQ-plot seems to be less of a curve, and there are less points beyond the (-2,2) margin in both residual scatterplots.\n\nOur simple Box-Cox transformation has paid off, in terms of our assumptions we test.\n\nLet's move on to finding the best global subset for both linear estimators and compare the |Cp-p-1| of the best global subset obtained with and without the transformation.","ddbcd0aa":"## Finding Influential Points: High Leverage Points & Outliers","cbad248d":"## Matrix Plots","8db5615e":"# References\n\n\n\n\nClaeskens, G., & Hjort, N. (2008). Model Selection and Model Averaging (Cambridge Series in Statistical and Probabilistic Mathematics). Cambridge: Cambridge University Press. doi:10.1017\/CBO9780511790485\n\n\nDang Xin, H Peng, X Wang and H Zhang (2008), Theil-Sen Estimators in a Multiple Linear Regression Model. Submitted paper. Available on http:\/\/home.olemiss.edu\/~xdang\/papers\/ \n","cae7f2d9":"## Boxcox Transformation","c7b5f46e":"## Best Subset for Linear Regression: Theil-Sen Regressor","5637a8cc":"# Linear Regression Diagnostics: Detection of Model Violations\n\nBefore applying our model, we need to check that our data does not violate the assumptions of linear regression, so as to ensure that linear regression is appropriate for the data we have. For this we use 2 sets of plots: the matrix plots of all X against y, and the plots of residuals.","be0b9809":"\nKeeping our main goal of being able to accurately predict graduate admission chances, we want to obtain the best model possible, or the model that has the lowest |Cp-p-1|. However, forward selection won't suffice here. Forward selection merely finds the local best model according to a certain statistic, but may fail to find the the globally best subset of features that obtains the global minimum for |Cp-p-1|. \n\nIt is for this reason that we would like to test all possible models: fit them to our regression model and calculate the |Cp-p-1|, SSE (Sum of Squared Errors, lower the better), and $R^2$ of each model (explained variance, higher the better), then take the model with the lowest |Cp-p-1|. \n\nWe choose to calculate our own $R^2$ as the r2 function in sklearn does not seem to be accurate, capable of producing negative values which should not be possible by definition of the squared coefficient of determination, $R^2$. \n\nHowever, $R^2$ naturally increases as more features are added to the model, regardless of whether the feature is actually significant. Thus, we also calculate adj $R^2$ as it allows us to compare explained variation between models of different number of features. \n\nFurther, we want to find a better alternative to the OLS linear estimator in linear regression, since linear regression is not a perfect fit of our data, since our data violates heterodascity and has quite a few outliers that could influence the fitted line of our OLS estimator.\n\nThus, I propose the Theil-Sen Estimator, which could be a better alternative.\n\nAccording to $Theil-Sen$ $Estimators$ $in$ $a$ $Multiple$ $Linear$ $Regression$ $Model$, the Theil Sen Estimator is \n\n    \"robust, consistent and asymptotically normal under mild conditions, and super-efficient when the error distribution is discontinuous\" \n\nand works by calculating the gradient of the line obtained from fitting a line to every possible pair of data points, or , for each $(xi,yi)$, the best fit line is the median m of the slopes $(yj \u2212 yi)\/(xj \u2212 xi)$ then taking the median of the lines fitted as the best fit line.\n\nThis estimator has the potential to be more effective than the usual OLS estimator which is not robust to outliers, and so let us find the globally best subset for both OLS and Theil-Sen estimators.\n\n\n","d28f0e9f":"## Understanding Predictors","4dad2979":"Now that we have found a better estimator, let us look at the data itself. Is there anything we can do about the assumptions our data violates? \n\nWe can try to use a Box-Cox transformation to 'squish' our residuals so that more residual points are between the (-2,2) margin, and thus reduce the violation of our linear regression assumptions by improving heterodascity of our data. We can apply a simple box-cox transformation by squaring our response variable.\n\nFirst we test our assumptions again to see if our data violates linear regression assumptions less.","cbcb620e":"## Mallows Cp","1fb54a1f":"## Role of variables: Component\/Component plus Residual Plots\n\nBefore fitting our model, we will conduct one final check on the linearity assumption for the response and predictor variables. We will use a more sophisticated plot called the residual plus component \/ partial residual plot. \n\n    The residual plus component plot for X j is a scatter plot of (e + betajXj) vs Xj, where e is the ordinary least squares residuals when Y is regressed on all predictor variables and ,betaj is the coefficient of Xj in this regression.\n\n                                                                                          - Regression Analysis by Example        \nThe slope of the points in these plots is betaj, the regression coefficient of Xj. The residual plus component plot is particularly sensitive in detecting non-linearity between X and y, and a linear relationship here will confirm that the use of linear regression with our data is a good fit.","ea5d0a69":"### Normalise Data, remove spaces in var names","d97c7f5d":"## Key Information about Dataset\n\n- 9 variables, all numerical.\n- Serial Number indicates the index of the data, it can be dropped.\n- University Rating and Research seem to be categorical data, made into numbers.\n- SOP and LOR seem to be on some kind of scale.\n- Chance of Admit is our dependent variable (y var), is a probability between 0 and 1.\n","cf308f38":"# Closing Comments","03558d1a":"As we can see our independent variable, chance of admit is most highly correlated to CGPA (0.87), GRE Score (0.80), and TOEFL Score (0.79). We shall keep this in mind when running feature selection, but first let us check whether our data can be modelled by linear regression by checking whether our data violates the assumptions of our linear model.\n","fb3814e9":"# Data Preparation","1634cf0e":"# Modelling the Data","be668df5":"That seems to be the only data cleaning we need to do.","5f0a2c80":"## Residual Plots","69724c9a":"Best Features so far:\n\nSOP, CGPA, Research, GRE_Score","51233511":"## Data Cleaning","39db0249":"From the residual plus component plots, there is a linear relationship between all the variables and the chance of admission, and there seems to be no non-linear pattern, which is strong evidence that linear regression is a good fit for our data.\n\n\nIn conclusion, after all the linear regression diagnostics, we can conclude that linear regression is still appropriate and quite likely very useful. Although our data does slightly violate some assumptions, this does not invalidate the use of linear regression, as the general trend observed is linear, and even if our assumptions are slightly violated, linear regression can still be applied to great effect.","8f8f4387":"For a more in-depth investigation of whether linear regression fits our data, we need to ensure that the best fit line is not overly determined by 1 or a few observations. These points may exercise undue influence on our best fit line, and causes changes in the fit when the point is deleted. These points are known influential points: High leverage points if in the X direction and Outliers if in the Y direction.\n\nAccording to Regression Analysis by Example:\n\nInfluential points in the response variable:\n\n    Observations with large standardized residuals are outliers in the response variable because they lie far from the fitted equation in the Y-direction ..... points with standardized residuals larger than 2 or 3 standard deviation away from the mean (zero) are called outliers. Outliers may indicate a model failure for those points.\n    \nInfluential points in the independent variables:\n\n    Outliers can also occur in the predictor variables (the X-space). They can also affect the regression results. The leverage values pii, described earlier, can be used to measure outlyingness in the X-space.....Observations that are outliers in the X-space are known as high leverage points to distinguish them from observations that are outliers in the response variable (those with large standardized residuals).\n\nThus, we want to find the existence and number of points there are in our data that are influential to assess whether our best fit line determined by our linear regression model might be inaccurate due to these influential observations.","ed5d39cd":"### Checking Linearity Assumptions\n\nWe can use the last row of the Matrix plot to check whether Chance of Admit increases as our predictors increase. For this assumption to be fulfilled by our data, we should see a diagonal pattern between chance of admit and all our predictors. From the last row of the matrix plot, we can see that all our predictors seem to have a positive linear relationship with chance of admit, GRE, TOEFL and CGPA are continous vars that seem to have a strong linear relationship with chance of admit, and the ordinal predictors seem to have a better chance of admit with higher categories on the ordinal scale, as well as a 1 in research having a higher chance of admit. This makes intuitive sense, as we expect exam and test scores to be taken into account for graduate admissions, and thus the better the scores, the better the chance. Thus the linearity assumption is satisfied.\n\n### Checking Independent Variable Assumptions\n\n\n\n- Multicolinearity \n\nWe can extract information about this assumption from the first 7 rows of the matrix plot, i.e the plots of features vs the rest of the features. We can see that some predictors are colinear (GRE and TOEFL and CGPA), which also makes intuitive sense as if a student is particularly gifted or hardworking, he\/she would more likely do well for all tests. This assumption is not entirely satisfied, but not entirely violated, as the colinearity is around 0.7-0.8, and not high at 0.9+, which is still acceptable. We can try to work around this violation by combining some features by PCA to get rid of any colinearity in our model.\n\n\n- Non-random and No Measurement error Assumptions\n\nThere's no information about the data being random and how it was measured, but it is safe to assume that these assumptions are not violated, as that is unlikely.\n\n\n### Measurement Error Asssumptions\n\n\n- Normality of Errors Asssumption\n\n\nThe normality assumption seems to be violated from looking at both the normal Q-Q plot and the histogram plot. In the normal Q-Q plot, the points deviate from the straight line near the left end. The histogram plot is slightly left-skewed, further confirming that some violations exists in the model.\n\n\n- Errors have Mean 0\n\nFrom both studentised residual scatter plots, we can see that the distribution of points is scattered around 0. This satisfies the assumption that the errors have mean 0.\n\n\n- Homogeneity of Variance of Errors\n\n- Independence of Errors\n\nThe homogeneity assumption and the independence assumption seems to be violated from both the scatter plots. In the Index Plot of Studentised residuals, the band of points is larger near the left end of the plot. In the Studentised residuals vs Fitted values plot, the band of points seem to be larger on the left and the band narrows around 0 as the Fit value increases. This seems to indicate that as the chance of admission of a student increases, there is less variance in the error of prediction of chances of admissions.\n\n\n\n### Outlier Assumption\n\n\nFinally, from our studentised residual scatter plots, there exists quite a few points outside the [-2,2] range, which is not ideal, as linear regression assumes that there are no outliers, but a good majority of our data is between our [-2,2] margin, and thus linear regression is still applicable here. We shall try to deal with our outliers by trying another estimator that is more robust to outliers than linear regression. \n\n","198b8148":"# Final Model","81393d1b":"## Best Model \n\nAfter exploring all subsets with both estimators with and without our box cox transformation, our best model is still the Theil-Sen Regressor with our original data, with it's lowest |Cp-p-1| of 0.075. However, there is an improvement \n\n\nhas 4 features: \n\nCGPA, LOR, GRE_Score, TOEFL_Score\n","7557dcb2":"## Best Subset for Linear Regression: OLS ","cfad8d83":"In this notebook, I hoped to give linear regression comprehensive attention, from assumption testing to the metrics used to evaluate the models made, and try a new method that I felt would work for our graduate data, and I am glad that my intuition paid off. \n\nI hope this notebook was accurate and that all my functions and metrics I calculated were correct and if they are not, please point them out to me :)\n\n\nThere areas to improve on, as there are many linear estimators, and while it is ideal to try them all, I simply do not have the time. Additionally, another metric I could have used to rank models is AIC. I aim to revisit this project and try regularisation, as well as try to use PCA.\n\n\nThanks for reading, and I hope you enjoyed my exploration of using linear models to predict the chance of graduate admission, if you did, please give it an upvote! (Esp if you used code from my notebook here :) )\n\n","7f5d860f":"## Best Subset for Linear Regression: OLS w Box-Cox","9c838d26":"Huzzah! \n\nThe introduction of the Theil-Sen estimator has paid off! The best combination of features for our Theil-Sen estimator has the lowest |Cp-p-1| statistic at 0.07509 whereas our usual OLS linear regression estimator has a lowest |Cp-p-1| of only 1.5639. Furthermore, $R^2$ for the best subsets of both estimators are quite close, at 0.7935 for the Theil Sen estimator and 0.7997 for OLS estimator. \n\nThese high $R^2$ scores ensure that both models have substantial explanatory power. We might think that we should rank models according to how much variation in data our model can explain, but a higher $R^2$ does not indicate accuracy in future predictions, as our data has outliers. \n\nIn the context of graduate admissions, the outliers in our data might be prospective students that did not have great test scores or GPA but were outstanding in other areas or had connections to the admission committee, or have good test scores but not gain admission due to intense competition or other factors. \n\nThese data points, though valid, are not part of the main trend, and should not be a part of our model, as these are rare occurences and we are predicting chance of admission for the general populace of prospective students. This is perhaps why the Theil-Sen Estimator does better: it is robust to these outliers that represent rare cases, and is thus more accurately able to predict for the general trend of people, as these rare cases may not ever come along again.\n\n\nSo far, our best model is the Theil-Sen Regressor with these 4 features: CGPA, LOR, GRE_Score, TOEFL_Score.\n\n","e26ba7f3":"## Checking Assumptions","9b0bf080":"To further our investigation of how good a fit linear regression is for our data, we can delve further into detecting outliers: checking for outliers in both the X and y direction.","b4acdc59":"## Preliminary Findings","a4c21f75":"# Improving on our model","3cca5eba":"## Baseline Model: Fit all Features","3800f840":"# Problem Statement\n\n\nGetting into a good graduate program is important for many people pursuing further studies beyond their Bachelor's, and without prior knowledge of how much a chance they stand to enter the university of their choice, applying for a program can be a very stressful experience.\n\nWell, no fear, data science is here! Using the data we have here, we can train a model to predict a prospective graduate student's chance of admission given a few factors indicated in our dataset. Moreover, we can also perhaps divine what tests and portions of the admission procedure have the greatest impact on the outcome of the admission. \n\nIntuitively, a linear model makes sense, as the higher your test scores and the more related projects you do, the more likely you are to be a better graduate student, and thus the higher your chance of admission. Thus, we will be using linear regression for predicting the chance of admission. Furthermore, we want to make sure linear regression is a viable model for our data, and thus we will work at testing the assumptions of linear regression, and if the assumptions are fulfilled or not violated terribly, we will fit our data to linear regression, then attempt to improve our model by testing a different linear estimator, or with data transformation.\n\nAfter we test all this, we shall present our final model: the estimator used and the final equation we use to predict the chance of admission. I choose to stick with linear regression mainly because I wish to be comprehensive in this one method, and I feel there is a lack of a proper treatment of linear regression. As with any model, we should:\n\n- First understand our data,\n\n- Clean, preprocess it.\n\n- Make sure our model is right for the data.\n\n- Choose the appropriate metrics to assess our models.\n\n- Then try to tune the model or improve on it, in our case by using another linear estimator.\n\n- Make final improvements before presenting and explaining our final model.\n\n\nFor fitting our model, I avoid train test split in favor of training on the entire dataset to minimise a theoretical Mean Square Prediction Error, and use this to select the best model. \n\nAdditionally, I don't really take into consideration the context of each variable used here such as a specific test, as I do not have any prior information about the importance of the particular graduate tests, of college GPA, or of doing research to get into a graduate programme, and you will not find me professing any intuition about which features we must include in our final model. I aim only to make sure I am comprehensive, and obtain the best model I can according to a certain metric.\n\nMuch of the content and theory here is taken from Regression Analysis by Example (I can't vouch for the Theil-Sen Estimator being in the book though)."}}