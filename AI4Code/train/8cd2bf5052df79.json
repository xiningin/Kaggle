{"cell_type":{"62698fc9":"code","95345c21":"code","208e3b2d":"code","692d0e2f":"code","59973820":"code","19fb6308":"code","e2e8a98d":"code","748d80f1":"code","ba2b7986":"code","5bbf8341":"code","58cbe38e":"code","b67ec9e0":"code","1334df60":"code","3f0a6117":"code","294e9603":"code","80d404a7":"code","49c8fe63":"code","1b4e0b31":"code","683888b1":"code","fd483403":"code","4ade26c9":"code","d2744768":"code","eee875d3":"code","ab2be1a3":"code","c008779d":"code","c2f2b5aa":"code","0fe2626c":"code","2c588a4d":"code","538bbb4d":"code","915ee1fb":"code","6fdcf1c9":"code","ee8f6236":"code","3d09729d":"code","7a023672":"code","7279c2bd":"code","b2797607":"code","309419e5":"code","617e8f1f":"code","b2fe212b":"code","5d6843ef":"code","fac7c646":"code","2a874a72":"code","cb9b4366":"code","001238a8":"code","71afb254":"code","b75e491f":"code","20f38eac":"code","81a00be5":"code","86dcc10f":"code","4b43339c":"code","a458f1f4":"code","66fcff69":"code","8fd1b12a":"code","d7649a1e":"code","055d308f":"code","8a40980a":"code","304bed93":"code","8d742df5":"code","4aaae334":"code","72edae02":"code","1c79a375":"code","0a4d50ba":"code","b3bf5f6c":"code","67dbae7c":"code","ffbe180d":"code","aeb8978c":"code","c8626781":"code","143c966a":"code","b0b39758":"code","b382cc28":"code","80a7e95d":"code","9ca1b71c":"code","9249ab9d":"code","02b63f02":"code","e922e981":"code","2c25d119":"code","efb06e14":"code","718cd35f":"code","8493adf5":"code","82f358f4":"code","b2aa8c72":"code","812b3602":"code","7db58e10":"code","ffbd9ad6":"code","d14f7dbf":"code","e5eda4bb":"code","b975391d":"markdown","69e6d24d":"markdown","05d352f2":"markdown","2e93fb4e":"markdown","c51331b0":"markdown","b7d0c028":"markdown","67b7e8a3":"markdown","f9db1635":"markdown","3827dadd":"markdown","2a4d81a3":"markdown","7dc022e7":"markdown","6bcaed78":"markdown","abec5b8e":"markdown","386db80d":"markdown","6288619d":"markdown","31b1c521":"markdown","2bfe7e86":"markdown","ddc12aad":"markdown","ab3d9cbd":"markdown","2111d5e5":"markdown","c5d8adfb":"markdown","3ef03d78":"markdown","42c20a6b":"markdown","63dd04f2":"markdown","38f55103":"markdown","192b668f":"markdown","88b672b1":"markdown","7e418496":"markdown","0c7880c7":"markdown","409a9d3c":"markdown","9c1bcbab":"markdown","75f4e759":"markdown","1e1e1355":"markdown","9299ab3a":"markdown","d75ea5eb":"markdown","d2229971":"markdown","29eb6dfe":"markdown","d4374b8e":"markdown","51fd3336":"markdown","3faa5f9b":"markdown","69e9e582":"markdown","b1df5dc8":"markdown","005c8110":"markdown","0c117531":"markdown","6b247fea":"markdown","45c8e790":"markdown","7d136b1f":"markdown","86faf401":"markdown","40e5b2ce":"markdown","fe2b9a4d":"markdown","31092d5b":"markdown","1ff3e278":"markdown","db8de405":"markdown","cf89c6c7":"markdown","e55a4058":"markdown","8f5168c5":"markdown","563a1996":"markdown","b797dd7f":"markdown","65ab0790":"markdown","005775f1":"markdown","05f00c81":"markdown","6d207724":"markdown","1edbac1d":"markdown","bbd2943e":"markdown","fc84ccc7":"markdown","6d692cf4":"markdown","8818ce48":"markdown","ff8db738":"markdown","109c2272":"markdown","ac4819a4":"markdown","79e97747":"markdown","53f46c6a":"markdown","ca37b5e2":"markdown","3ae18162":"markdown","342ed2f0":"markdown","590af1de":"markdown","460c7e80":"markdown","3cfb104f":"markdown","d304d4bf":"markdown","a74962b3":"markdown","21af42bd":"markdown","2022f631":"markdown","e35c5493":"markdown","00683f76":"markdown","eb3a494e":"markdown","7eccf926":"markdown","eea3aa97":"markdown","62bd7c23":"markdown","f0ee30cd":"markdown","6c426c69":"markdown","128cc722":"markdown","5402da92":"markdown","b9fc0c90":"markdown","4d896cff":"markdown","b1fa249c":"markdown","84e9a214":"markdown","ccc657f2":"markdown","870d90e2":"markdown","cced8da9":"markdown","0547403a":"markdown","ba0f0b8d":"markdown","39d00516":"markdown","afed2f04":"markdown","cecf130d":"markdown","ec23dc19":"markdown","47ece93e":"markdown","69d55a04":"markdown","71ba2041":"markdown","062ff684":"markdown","5b5b9d36":"markdown","0f258290":"markdown","8ec4468c":"markdown"},"source":{"62698fc9":"import pandas as pd\nimport numpy as np\nimport pandas_profiling\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport warnings\nwarnings.filterwarnings('ignore')","95345c21":"data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ndata.head()","208e3b2d":"data.profile_report()","692d0e2f":"labels = ['Non-Survived', 'Survived']\nvalues = [list(data['Survived'].value_counts())[0], list(data['Survived'].value_counts())[1]]\n\nfig = go.Figure(data=[go.Pie(labels=labels, values=values, pull=[0,0.05], title=\"Survived and Non-Survied\")])\nfig.show()","59973820":"def label(x):\n    if x==1:\n        return 'Survived'\n    else:\n        return 'Non-Survived'\ndata['Survived-copy'] = data['Survived'].apply(lambda x: label(x))","19fb6308":"gender_count = dict(data['Sex'].value_counts())\ngender_count","e2e8a98d":"labels = ['Male', 'Female']\nvalues = [gender_count['male'], gender_count['female']]\n\nfig = go.Figure(data=[go.Pie(labels=labels, values=values, pull=[0,0.05], title=\"Male and Female\")])\nfig.show()","748d80f1":"plt.figure(figsize=(12, 8))\nsns.heatmap(data.corr(), annot=True, cmap='coolwarm')","ba2b7986":"plt.figure(figsize=(12, 8))\nsns.countplot(x='Survived-copy', hue='Sex', data=data)\nplt.show()","5bbf8341":"gender_survived = dict(data['Sex'][data['Survived']==1].value_counts())\ngender_survived","58cbe38e":"labels = ['Male', 'Female']\nvalues = [gender_survived['male'], gender_survived['female']]\n\nfig = go.Figure(data=[go.Pie(labels=labels, values=values, pull=[0,0.05], title=\"Survived Male and Female\")])\nfig.show()","b67ec9e0":"gender_non_survived = dict(data['Sex'][data['Survived']==0].value_counts())\nlabels = ['Male', 'Female']\nvalues = [gender_non_survived['male'], gender_non_survived['female']]\n\nfig = go.Figure(data=[go.Pie(labels=labels, values=values, pull=[0,0.05], title=\"Non Survived Male and Female\")])\nfig.show()","1334df60":"classes_survived = dict(data['Pclass'][data['Survived']==1].value_counts())\nlabels = ['1st Class', '2nd Class' ,'3rd Class']\nvalues = [classes_survived[1], classes_survived[2], classes_survived[3]]\n\nfig = go.Figure(data=[go.Pie(labels=labels, values=values, pull=[0,0, 0.05], title=\"Survived Class Distribution\")])\nfig.show()","3f0a6117":"classes_non_survived = dict(data['Pclass'][data['Survived']==0].value_counts())\nlabels = ['1st Class', '2nd Class' ,'3rd Class']\nvalues = [classes_non_survived[1], classes_non_survived[2], classes_non_survived[3]]\n\nfig = go.Figure(data=[go.Pie(labels=labels, values=values, title=\"Non-Survived Class Distribution\")])\nfig.show()","294e9603":"f,ax=plt.subplots(1,2,figsize=(18,8))\ndata['Pclass'].value_counts().plot.bar(ax=ax[0])\nax[0].set_title('Number Of Passengers By Pclass')\nax[0].set_ylabel('Count')\nsns.countplot('Pclass',hue='Survived',data=data,ax=ax[1])\nax[1].set_title('Pclass Survived vs Non-Survived')\nplt.show()","80d404a7":"crosstab = pd.crosstab([data.Sex,data['Survived-copy']],data.Pclass)\nsns.heatmap(crosstab, cmap='coolwarm', annot=True,  fmt='g')\nplt.title(\"Crosstab of Sex vs Pclass with Survival\")\nsns.factorplot(x='Pclass',y='Survived',hue='Sex',data=data)\nplt.show()","49c8fe63":"fig = px.treemap(data, path=[\"Survived-copy\",'Sex','Pclass' ],\n                  color='Survived', hover_data=['Sex'],\n                  color_continuous_scale='Bluered_r', title=\"Ditribution of Survived and Non-Survived Passenger, their Genders with their class.\")\nfig.show()","1b4e0b31":"plt.figure(figsize=(12, 8))\nsns.countplot(x='SibSp', data=data)\nplt.show()","683888b1":"plt.figure(figsize=(12, 8))\ncrosstab = pd.crosstab(data.SibSp,data['Survived-copy'])\nsns.heatmap(crosstab, cmap='coolwarm', annot=True,  fmt='g')\nplt.title(\"Crosstab of Survival and SibSp\")\nplt.show()\nsns.factorplot(x='SibSp',y='Survived', hue=\"Sex\", data=data)\nplt.title(\"Factorplot for SibSp and Survived\")\nplt.show()","fd483403":"plt.figure(figsize=(12, 8))\nsns.heatmap(pd.crosstab([data.SibSp,data['Survived-copy']],data.Pclass,margins=True), cmap='coolwarm', annot=True, fmt='g')\nplt.title(\"Crosstab for Survived, SibSp and Pclass feature\")\nplt.show()","4ade26c9":"f,ax=plt.subplots(1,2,figsize=(18,8))\nsns.countplot(data['Parch'], ax=ax[0])\nax[0].set_title('CountPlot for Parch feature')\nsns.countplot('Parch',hue='Sex',data=data,ax=ax[1])\nax[1].set_title('Pclass Survived vs Non-Survived')\nplt.show()","d2744768":"sns.heatmap(pd.crosstab(data['Parch'], data['Survived-copy']), annot=True, cmap='coolwarm', fmt='g')\nplt.title(\"Crosstab for Survied and Parch features\")\nsns.factorplot(x=\"Parch\", y=\"Survived\", data=data )\nplt.title(\"Factorplot for Parch and Survived features\")","eee875d3":"sns.factorplot(x=\"Parch\", y=\"Survived\", hue='Sex', data=data )\nplt.title(\"Factorplot for Parch and Survived with Gender features\")","ab2be1a3":"plt.figure(figsize=(12, 8))\nsns.heatmap(pd.crosstab([data.Parch,data['Survived-copy']],data.Pclass,margins=True), cmap='coolwarm', annot=True, fmt='g')\nplt.title(\"Crosstab for Survived, SibSp and Pclass feature\")\nplt.show()","c008779d":"plt.figure(figsize=(12, 8))\nsns.kdeplot(data[\"Fare\"][data[\"Survived\"]==0], shade=True, label=\"Non Survived\")\nsns.kdeplot(data[\"Fare\"][data[\"Survived\"]==1], shade=True, label=\"Survived\")\nplt.title(\"Survival vs Fare\")","c2f2b5aa":"data[data['Fare']>280]","0fe2626c":"plt.figure(figsize=(12, 8))\nsns.kdeplot(data[\"Fare\"][data[\"Pclass\"]==1], shade=True, label=\"Pclass 1\")\nsns.kdeplot(data[\"Fare\"][data[\"Pclass\"]==2], shade=True, label=\"Pclass 2\")\nsns.kdeplot(data[\"Fare\"][data[\"Pclass\"]==3], shade=True, label=\"Pclass 3\")","2c588a4d":"sns.heatmap(pd.crosstab(data['Embarked'], data['Sex']), annot=True, cmap='coolwarm', fmt='g')\nplt.title(\"Crrostab Embarked and Sex\")","538bbb4d":"sns.heatmap(pd.crosstab(data['Embarked'], data['Survived-copy']), cmap='coolwarm', annot=True, fmt='g')\nplt.title('Crosstab for Survived and Embarked')\nsns.factorplot(x='Embarked', y='Survived', data=data)\nplt.title('factorplot Embarked vs Survival')","915ee1fb":"sns.heatmap(pd.crosstab(data['Embarked'], [data['Survived-copy'],data['Pclass']]), cmap='coolwarm', annot=True, fmt='g')\nplt.title(\"Crosstab for Embarked Survival and Pclass\")","6fdcf1c9":"sns.factorplot('Pclass','Survived',hue='Sex',col='Embarked',data=data)","ee8f6236":"plt.figure(figsize=(20, 8))\nsns.distplot(data['Age'], bins=[i for i in range(0, 81,2)])\nplt.title(\"Age distribution on the Ship\")","3d09729d":"plt.figure(figsize=(20, 8))\nsns.distplot(data[\"Age\"][data[\"Survived\"]==0], label=\"Non Survived\", bins=[i for i in range(0, 81,2)], kde=False)\nsns.distplot(data[\"Age\"][data[\"Survived\"]==1], label=\"Survived\", bins=[i for i in range(0, 81,2)], kde=False)\nplt.xlabel('Age')\nplt.legend()\nplt.title(\"Hisdtogram for Age distribution of Survived and Non Survived Passengers\")","7a023672":"plt.figure(figsize=(12, 8))\nsns.distplot(data[\"Age\"][data[\"Survived\"]==0][data['Age']<15], label=\"Non Survived\", bins=[i for i in range(0, 81,2)], kde=False)\nsns.distplot(data[\"Age\"][data[\"Survived\"]==1][data['Age']<15], label=\"Survived\", bins=[i for i in range(0, 81,2)], kde=False)\nplt.xlabel('Age')\nplt.legend()","7279c2bd":"plt.figure(figsize=(12, 8))\nsns.violinplot(x='Sex', y='Age', hue=\"Survived\", data=data, split=True)","b2797607":"plt.figure(figsize=(12, 8))\nsns.violinplot(x='Pclass', y='Age', hue=\"Survived\", data=data, split=True)","309419e5":"data[\"CabinBool\"] = (data[\"Cabin\"].notnull().astype('int'))","617e8f1f":"test[\"CabinBool\"] = (test[\"Cabin\"].notnull().astype('int'))","b2fe212b":"sns.factorplot(x='CabinBool', y='Survived', data=data)","5d6843ef":"data = data.drop(['Cabin'], axis=1)\ntest = test.drop(['Cabin'], axis=1)","fac7c646":"print(data['Age'].isna().sum(), test['Age'].isna().sum())","2a874a72":"data[\"Age\"] = data[\"Age\"].fillna(-0.5)\ntest[\"Age\"] = test[\"Age\"].fillna(-0.5)","cb9b4366":"bins = [-1, 0, 5, 12, 18, 24, 35, 60, np.inf]\nlabels = ['Unknown', 'Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Senior']\ndata['AgeGroup'] = pd.cut(data[\"Age\"], bins, labels=labels)\ntest['AgeGroup'] = pd.cut(test[\"Age\"], bins, labels=labels)","001238a8":"plt.figure(figsize=(12, 8))\nsns.countplot(data['AgeGroup'])","71afb254":"plt.figure(figsize=(12, 8))\nsns.barplot(x=\"AgeGroup\", y=\"Survived\", data=data)","b75e491f":"combine = [data, test]","20f38eac":"for dataset in combine:\n    dataset['Title'] = dataset['Name'].str.extract(\", ([A-Za-z]+)\\.\", expand=False)","81a00be5":"plt.figure(figsize=(12, 8))\nsns.heatmap(pd.crosstab(data['Title'], data['Sex']), annot=True, cmap='coolwarm', fmt='g')","86dcc10f":"for dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(\n        ['Lady', 'Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Jonkheer', 'Dona'],\n        'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Sir', 'Royal')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')","4b43339c":"plt.figure(figsize=(12, 8))\nsns.heatmap(pd.crosstab(data['Title'], data['Sex']), annot=True, cmap='coolwarm', fmt='g')","a458f1f4":"title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Royal\": 5, \"Rare\": 6}\n\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)","66fcff69":"plt.figure(figsize=(12, 8))\nsns.heatmap(pd.crosstab(data['Title'], data['AgeGroup']), annot=True, cmap='coolwarm', fmt='g')","8fd1b12a":"age_title_mapping = {1: \"Young Adult\", 2: \"Student\",3: \"Adult\", 4: \"Baby\", 5: \"Adult\", 6: \"Adult\"}\n# Created this dictionary after checking the results from above crosstab\n\nfor x in range(len(data[\"AgeGroup\"])):\n    if data[\"AgeGroup\"][x] == \"Unknown\":\n        data[\"AgeGroup\"][x] = age_title_mapping[data[\"Title\"][x]]","d7649a1e":"for x in range(len(test[\"AgeGroup\"])):\n    if test[\"AgeGroup\"][x] == \"Unknown\":\n        test[\"AgeGroup\"][x] = age_title_mapping[test[\"Title\"][x]]","055d308f":"data['AgeGroup'][data['AgeGroup']=='Unknown'].count()","8a40980a":"age_mapping = {'Baby': 1, 'Child': 2, 'Teenager': 3,\n               'Student': 4, 'Young Adult': 5,\n               'Adult': 6, 'Senior': 7}","304bed93":"data['AgeGroup'] = data['AgeGroup'].map(age_mapping)\ntest['AgeGroup'] = test['AgeGroup'].map(age_mapping)","8d742df5":"data = data.drop(['Age'], axis=1)\ntest = test.drop(['Age'], axis=1)","4aaae334":"data = data.drop(['Ticket'], axis=1)\ntest = test.drop(['Ticket'], axis=1)","72edae02":"data = data.drop(['Name'], axis=1)\ntest = test.drop(['Name'], axis=1)","1c79a375":"sex_mapping = {\"male\": 0, \"female\": 1}\ndata['Sex'] = data['Sex'].map(sex_mapping)\ntest['Sex'] = test['Sex'].map(sex_mapping)","0a4d50ba":"data[data['Embarked'].isna()]","b3bf5f6c":"plt.figure(figsize=(12, 8))\nsns.boxplot(x='Embarked', y='Fare',hue='Pclass', data=data)","67dbae7c":"data['Embarked'] = data['Embarked'].fillna('C')","ffbe180d":"embarked_mapping = {\"S\": 1, \"C\": 2, \"Q\": 3}\ndata['Embarked'] = data['Embarked'].map(embarked_mapping)\ntest['Embarked'] = test['Embarked'].map(embarked_mapping)","aeb8978c":"data.Fare.isna().sum(), test.Fare.isna().sum()","c8626781":"for x in range(len(test[\"Fare\"])):\n    if pd.isnull(test[\"Fare\"][x]):\n        pclass = test[\"Pclass\"][x]\n        test[\"Fare\"][x] = round(data[data[\"Pclass\"] == pclass][\"Fare\"].mean(), 2)","143c966a":"data['FareBand'] = pd.qcut(data['Fare'], 4,labels=[1, 2, 3, 4])","b0b39758":"plt.figure(figsize=(20, 8))\nsns.distplot(data[data['FareBand']==1].Fare, kde=False, label='FareBand 1')\nsns.distplot(data[data['FareBand']==2].Fare, kde=False, label='FareBand 2')\nsns.distplot(data[data['FareBand']==3].Fare, kde=False, label='FareBand 3')\nsns.distplot(data[data['FareBand']==4].Fare, kde=False, label='FareBand 4')\nplt.title(\"Distribution of the values of Fare for FareBand Feature\")\nplt.legend()\nplt.show()","b382cc28":"data['FareBand'] = data['FareBand'].astype('int')","80a7e95d":"test['FareBand'] = pd.qcut(test['Fare'], 4,\n                           labels=[1, 2, 3, 4])","9ca1b71c":"data = data.drop(['Fare'], axis=1)\ntest = test.drop(['Fare'], axis=1)","9249ab9d":"data=data.drop('Survived-copy', axis=1)","02b63f02":"data=data.drop('PassengerId', axis=1)","e922e981":"test=test.drop('PassengerId', axis=1)","2c25d119":"data.head()","efb06e14":"test.head()","718cd35f":"plt.figure(figsize=(12, 8))\nsns.heatmap(data.corr(), annot=True, cmap='coolwarm')","8493adf5":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split","82f358f4":"input_predictors = data.drop(['Survived'], axis=1)\noutput_target = data[\"Survived\"]","b2aa8c72":"num_folds = 10\nkfold = KFold(num_folds)","812b3602":"from sklearn.preprocessing import Normalizer\ninput_predictors = Normalizer().fit_transform(input_predictors)","7db58e10":"def spot_check(x_train, y_train):\n    l=dict()\n    c=dict()\n    \n    # MODEL-1) LogisticRegression\n    # ------------------------------------------\n    from sklearn.linear_model import LogisticRegression\n    \n    logreg = LogisticRegression()\n    \n    algos=('newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga')\n    panalties = ['l1', 'l2']\n    grid = {'penalty': panalties, 'solver': algos}\n    grid = GridSearchCV(estimator=logreg, param_grid=grid, cv=kfold, scoring='accuracy')\n    grid.fit(x_train, y_train)\n    logreg = grid.best_estimator_\n    print(\"MODEL-1: Accuracy of LogisticRegression : \", round(grid.best_score_ * 100,2))\n    l['acc_logreg']=round(grid.best_score_ * 100,2)\n    c['acc_logreg']=logreg\n\n    # MODEL-2) Gaussian Naive Bayes\n    # ------------------------------------------\n    from sklearn.naive_bayes import GaussianNB\n    \n    gaussian = GaussianNB()\n    \n    results = cross_val_score(gaussian, x_train, y_train, cv=kfold, scoring='accuracy')\n    \n    acc_gaussian = round(results.mean() * 100, 2)\n    print(\"MODEL-2: Accuracy of GaussianNB : \", acc_gaussian)\n    l['acc_gaussian']=acc_gaussian\n    c['acc_gaussian']=gaussian\n    \n    \n    # MODEL-3) Support Vector Classifier\n    # -------------------------------------------\n\n    from sklearn.svm import SVC\n    \n    kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n    grid = {'kernel': kernels}\n    svc = SVC()\n    grid = GridSearchCV(estimator=svc, param_grid=grid, cv=kfold, scoring='accuracy')\n    grid.fit(x_train, y_train)\n    svc = grid.best_estimator_\n    print(\"MODEL-3: Accuracy of Support Vector Machines : \", round(grid.best_score_ * 100,2))\n    l['acc_svc']=round(grid.best_score_ * 100,2)\n    c['acc_svc']=svc\n\n#    MODEL-4) Linear SVC\n#   ------------------------------------------\n\n    from sklearn.svm import LinearSVC\n   \n    linear_svc = LinearSVC(random_state=7)\n    results = cross_val_score(linear_svc, x_train, y_train, cv=kfold, scoring='accuracy')\n    acc_linear_svc = round(results.mean() * 100, 2)\n    print(\"MODEL-4: Accuracy of LinearSVC : \", acc_linear_svc)\n    l['acc_linear_svc'] =acc_linear_svc\n    c['acc_linear_svc']=linear_svc\n    \n    \n#   MODEL-5) Perceptron\n#   ------------------------------------------\n\n    from sklearn.linear_model import Perceptron\n    panalties = ['l1', 'l2']\n    grid = {'penalty': panalties}\n    perceptron = Perceptron(random_state=7)\n    grid = GridSearchCV(estimator=perceptron, param_grid=grid, cv=kfold, scoring='accuracy', n_jobs=-1)\n    grid.fit(x_train, y_train)\n    perceptron = grid.best_estimator_\n    print(\"MODEL-5: Accuracy of Perceptron : \", round(grid.best_score_ * 100,2))\n    l['acc_perceptron']=round(grid.best_score_ * 100,2)\n    c['acc_perceptron']=perceptron\n    \n\n\n#   MODEL-6) Multiplayer Perceptron\n#   ------------------------------------------\n    \n    from sklearn.neural_network import MLPClassifier\n    \n    activation = ('identity', 'logistic', 'tanh', 'relu')\n    solver = ('lbfgs', 'sgd', 'adam')\n    learning_rate = ('constant', 'invscaling', 'adaptive')\n    grid = {'activation': activation, 'solver': solver, 'learning_rate': learning_rate}\n    perceptron = MLPClassifier(random_state=7)\n    grid = GridSearchCV(estimator=perceptron, param_grid=grid, cv=kfold, scoring='accuracy', n_jobs=-1)\n    grid.fit(x_train, y_train)\n    perceptron = grid.best_estimator_\n    print(\"MODEL-6: Accuracy of MLPerceptron : \", round(grid.best_score_ * 100,2))\n    l['acc_mlperceptron']= round(grid.best_score_ * 100,2)\n    c['acc_mlperceptron']=perceptron\n\n    # MODEL-7) Decision Tree Classifier\n    # ------------------------------------------\n    \n    from sklearn.tree import DecisionTreeClassifier\n    \n    criteria = ('gini', 'entropy')\n    max_depths = [i for i in range(2, 10)]\n    grid = {'criterion': criteria, 'max_depth': max_depths}\n    decisiontree = DecisionTreeClassifier()\n    grid = GridSearchCV(estimator=decisiontree, param_grid=grid, cv=kfold, scoring='accuracy', n_jobs=-1)\n    grid.fit(x_train, y_train)\n    decisiontree = grid.best_estimator_\n    print(\"MODEL-7: Accuracy of DecisionTreeClassifier : \", round(grid.best_score_ * 100,2))\n    l['acc_decisiontree']= round(grid.best_score_ * 100,2)\n    c['acc_decisiontree']=decisiontree\n    \n    # MODEL-8) Random Forest\n    # ------------------------------------------\n    from sklearn.ensemble import RandomForestClassifier\n    criteria = ('gini', 'entropy')\n    n_estimators = [i for i in range(20, 201, 20)]\n    max_depths = [i for i in range(2, 20)]\n    grid = {'criterion': criteria, 'max_depth': max_depths, 'n_estimators': n_estimators}\n    randomforest = RandomForestClassifier(random_state=7)\n    grid = GridSearchCV(estimator=randomforest, param_grid=grid, cv=kfold, scoring='accuracy', n_jobs=-1)\n    grid.fit(x_train, y_train)\n    randomforest = grid.best_estimator_\n    print(\"MODEL-8: Accuracy of RandomForestClassifier : \", round(grid.best_score_ * 100,2))\n    l['acc_randomforest']= round(grid.best_score_ * 100,2)\n    c['acc_randomforest']=randomforest\n\n    # MODEL-9) KNN or k-Nearest Neighbors\n    # ------------------------------------------\n\n    from sklearn.neighbors import KNeighborsClassifier\n    algorithm = ('auto', 'ball_tree', 'kd_tree', 'brute')\n    n_neighbors = [i for i in range(1,30)]\n    grid = {'algorithm': algorithm, 'n_neighbors': n_neighbors}\n    knn = KNeighborsClassifier()\n    grid = GridSearchCV(estimator=knn, param_grid=grid, cv=kfold, scoring='accuracy', n_jobs=-1)\n    grid.fit(x_train, y_train)\n    knn = grid.best_estimator_\n    print(\"MODEL-9: Accuracy of k-Nearest Neighbors : \",round(grid.best_score_ * 100,2))\n    l['acc_knn']= round(grid.best_score_ * 100,2)\n    c['acc_knn']=knn\n    \n    \n\n    # MODEL-10) Stochastic Gradient Descent\n    # ------------------------------------------\n    \n    from sklearn.linear_model import SGDClassifier\n    \n    sgd = SGDClassifier(random_state=7)\n    results = cross_val_score(sgd, x_train, y_train, cv=kfold, scoring='accuracy')\n    acc_sgd = round(results.mean() * 100, 2)\n    print(\"MODEL-10: Accuracy of Stochastic Gradient Descent : \", acc_sgd)\n    l['acc_sgd']= acc_sgd\n    c['acc_sgd']=sgd\n\n    # MODEL-11) ADABoost Classifier\n    # __________________________________________\n    \n    from sklearn.ensemble import AdaBoostClassifier\n    n_estimators = [100,140,145,150,160, 170,175,180,185];\n    learning_r = [0.1,1,0.01,0.5]\n\n    grid = {'n_estimators':n_estimators, 'learning_rate':learning_r}\n    \n    grid = GridSearchCV(AdaBoostClassifier(base_estimator= None),param_grid=grid,cv=kfold,scoring='accuracy',n_jobs = -1)\n    grid.fit(x_train, y_train) \n    \n    ada = grid.best_estimator_\n    print(\"MODEL-11: Accuracy of adaboosgt : \", round(grid.best_score_ * 100,2))\n    l['acc_adaboost']=round(grid.best_score_ * 100,2)\n    c['acc_adaboost']=ada\n\n\n    # MODEL-12) Gradient Boosting Classifier\n    # ------------------------------------------\n    from sklearn.ensemble import GradientBoostingClassifier\n    \n    gbk = GradientBoostingClassifier()\n    results = cross_val_score(gbk, x_train, y_train, cv=kfold, scoring='accuracy')\n    acc_gbk = round(results.mean() * 100, 2)\n    print(\"MODEL-12: Accuracy of GradientBoostingClassifier : \", acc_gbk)\n    l['acc_gbk']= acc_gbk\n    c['acc_gbk']=gbk\n    \n    # MODEL-13) XGBoost\n    # ------------------------------------------\n    \n    from xgboost import XGBClassifier\n    classifier = XGBClassifier()\n    results = cross_val_score(classifier, x_train, y_train, cv=kfold, scoring='accuracy')\n    acc_xgb=round(results.mean() * 100, 2)\n    print(\"MODEL-13: Accuracy of XGBoost : \", acc_xgb)\n    l['acc_xgb']= acc_xgb\n    c['acc_xgb']=classifier\n    \n    return l, c\n","ffbd9ad6":"accuracy_list, classifiers = spot_check(input_predictors, output_target.values.reshape(-1,1))","d14f7dbf":"plt.figure(figsize=(12, 8))\nplt.plot(list(accuracy_list.values()))\nplt.xticks(ticks=[i for i in range(0,13)],labels=list(accuracy_list.keys()), rotation=45)\nplt.show()","e5eda4bb":"classifiers['acc_randomforest']","b975391d":"Deleting Cabin feature beacuse now we have CabinBool.","69e6d24d":"### Treemap for the distribution of Survived and Non-Survived Passengers, with their Classes and Genders","05d352f2":"From the above graph we can observe that the Median value of Fare of Pclass 1 passengers embarked from C is closest to 80. So I'll fill the missing Embarked values with C in data cleaning part.","2e93fb4e":"From the above heatmap and factorplot we can observe that females without any Sibling were most likely to survive but male with 1 Sibling or Spouse has highest survival rate.<br>\nOverall, passengers with 0 1 or 2 SibSp has the highest survival rate.<br>\nSeems like this feature also matters in predicting survival.","c51331b0":"Here we can see that passengers with cabin had far more survival rate than passengers without cabin.","b7d0c028":"It is time separate the fare values into some logical groups as well as filling in the single missing value in the test dataset.","67b7e8a3":"### Male and female distribution in Survived Passengers","f9db1635":"Create a combined group of both datasets","3827dadd":"Features **Parch** and **SibSp** has a positive correlation of **0.41**","2a4d81a3":"Here we can observe that most of the passengers aboard without any Parent or Children.","7dc022e7":"### After analyzing *Sex* and *Pclass* features, I came up with following rsults:\n- Looking at the **CrossTab** and the **FactorPlot**, we can easily infer that survival for Women from Pclass1 is about **95-96%**, as only **3** out of **94** Women from Pclass1 died.\n- Women were given first priority while rescue. Even Men from Pclass1 have a very low survival rate.\n- So here Pclass and Sex both are very important features.","6bcaed78":"We dont need this feature to predict the survival of a pasenger. So we should drop this.","abec5b8e":"### Male and female distribution in Non-Survived Passengers","386db80d":"1. Logistic Regression\n2. Gaussian Naive Bayes\n3. Support Vector Classifier\n4. Linear SVC\n5. Perceptron\n6. Multiplayer Perceptron\n7. Decision Tree Classifier\n8. Random Forest\n9. KNN or k-Nearest Neighbors\n10. Stochastic Gradient Descent\n11. ADABoost Classifier\n12. Gradient Boosting Classifier\n13. XGBoost","6288619d":"I'll check the accuracies of all the models with KFold Cross Validation and choosing best hyperparameters with GridSerachCV","31b1c521":"# Choosing the Best Model","2bfe7e86":"Next we'll fill in the missing values in the Age feature.<br>\nSince a higher percentage of values are missing, it would be illogical to fill all of them with the same value.<br>\nInstead, let's try to find a way to predict the missing ages.","ddc12aad":"Majroty of the titles are Mrs, Mr and Miss","ab3d9cbd":"### Feature: Ticket","2111d5e5":"Now that we've filled in the missing values at least somewhat accurately, it is time to map each age group to a numerical value.","c5d8adfb":"Most of the children were of pclass 3 and most of them survived. <br>\nPassenger of Pclass 1 between age 20 to 45 were more likely to survive.<br>\n","3ef03d78":"### Feature: Fare","42c20a6b":"Also deleting Survived-copy feature because I made it just for EDA. ","63dd04f2":"### Distribution of Classes of Non-Survived passengers","38f55103":"In Non-Survived Gender distribution, there were 85% male and only 15% female.","192b668f":"We have 177 mssing values in training set and 86 missing values in testing set.","88b672b1":"## Analyzing Features: Sex and Pclass","7e418496":"We can drop the Ticket feature since it's unlikely to yield any useful information.\n","0c7880c7":"### Here we are getting 84% accuracy with RandomForest Classifier.","409a9d3c":"NOTICE the values of last newly added feature 'Title'<br>\nNext, we'll try to predict the missing Age values from the most common age for their Title.\n","9c1bcbab":"Also we can drop the name feature since it contains no more useful information.","75f4e759":"### Feature Description:\nNumber of siblings \/ spouses aboard the Titanic.<br>\nThe dataset defines family relations in this way...<br>\nSibling = brother, sister, stepbrother, stepsister<br>\nSpouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n","1e1e1355":"Survival rate of females of Pclass 1 and 2 is almost one 1 embarked from all stations.<br>\nSurvival rate of Pclass 3 is lowest for both male and female from all stations.<br>\nMale of all Pclasses embarked at Q had lowest survival rate because most of them were of Pclass 3.","9299ab3a":"So there are only 3 passengers who paid more than 280, i.e 512 as Fare.<br>\n","d75ea5eb":"Map values in test set also.","d2229971":"Here we can observe that there were total 94 1st class women in the ship and out of 94, 91 females survived. #racism","29eb6dfe":"From above factorplot and heatmap we can observe that most of the passengers were without any Parent or Children. If we check the survival rate, passengers with 3 Parch had hoghest survival rate but there were only 5 passengers with 3 Parch out of which three of them survived. As the number of passenger were very less in that category so we shoul consider passengers with 1 Parch has highest survival rate. <br>\nThe number of survived Passengers of Parch 0 was highest but the rate of their survival was low because there were total 678 passengers with 0 Parch out of which only 233 passengers survived.","d4374b8e":"There are no Unknown value in AgeGroup feature.","51fd3336":"### Observations:\n* Highest positive correlation of 0.54 is between Sex and Survived.\n* Title ans Sex also have a positive correlation of 0.45\n* Parch and SibSp also have a positive correlation of 0.41","3faa5f9b":"Here we can see almost all of the children till age 15 survived.","69e9e582":"From this heatmap we can observe that only passengers of class 3 had more than 2 Parch.<br>\nThe impact of socianomic class of Passenger is also visible here. If you see the 1 Pclass, for each and every Parch number, the number of Survived is more than the number of Non-Survived people. and in Pclass 3, the number of non-Survived is more than the number of Survived. \n","b1df5dc8":"Now, We have to map each of the title groups to a numerical value.","005c8110":"# Data Cleaning and Feature Engineering\n### This part contains some part of EDA after cleaning features.","0c117531":"There were **almost twice the number of males than female** present in the ship but if we see the survival distribution of the male and female, **68% females** and only **32% male** survived the disaster.","6b247fea":"From the above two graphs we can observe that most of the children aboard survived.<br>\nMOst of the passenger survived were between 18 to 36. and Most of the non Survived passengers were also between 18 to 32.\n","45c8e790":"## *If you like this notebook please consider upvote!!*","7d136b1f":"Replaced all available values with 1 and all the missing values filled with 0 in new feature CabinBool.","86faf401":"### Distribution of Classes of Survived passengers ","40e5b2ce":"Here we can observer that a large group of 3rd class Passengers died in the tragedy.","fe2b9a4d":"Now, I am going to replae Unknown values in AgeGroup feature of both train and tes set.","31092d5b":"Majority of the passengers Embarked from S and almost all of the passengers embarked from Q were of Pclass 3, this was also a reason of less survival rate of passnegers embarked from  S and Q as compared to C.\nOnly for class 1 there was more survived passengers than non survived Embarked from all Stations.","1ff3e278":"At First, filling all the missing values with -0.5","db8de405":"Deleting Fare feature because now we have FareBand.","cf89c6c7":"Females from 18 to 30 were most likely to survive.<br>\nMale from 25 to 35 were more likely to survive.<br>\nSurvival of male child was more than the female child.","e55a4058":"Here we can observe that most of the passengers within range 0 to 60 didnt survived. After ~280 there is no orange or blue line but after 500 orange graph appers. It seems like there are some outliers in the Fare feature.<br>\n","8f5168c5":"## Analyzing Features","563a1996":"Lets check thaht people with cabin were more likely to survive or not.","b797dd7f":"In above graph we can observe that the as the Pclass increases the range of Fare decreases. For Pclass 1 the range if from 0 to more than 500(Blue Line). For Pclass 2 the range is 0 to ~80(Orange Line) and for Pclass 3 the range is from 0 to ~75(Green Line).","65ab0790":"### Analyzing Feature: Fare","005775f1":"Now we can drop the Age feature from the dataset as we are using AgeGroup now.","05f00c81":"### pandas_profiling\nTo explore the data I am generating a report with **pandas_profiling**. It generates profile reports from a pandas DataFrame. The pandas df.describe() function is great but a little basic for serious exploratory data analysis. pandas_profiling extends the pandas DataFrame with df.profile_report() for quick data analysis. <br><br>\nFor each column the following statistics - if relevant for the column type - are presented in an interactive HTML report:\n- Type inference: detect the types of columns in a dataframe.\n- Essentials: type, unique values, missing values\n- Quantile statistics like minimum value, Q1, median, Q3, maximum, range, interquartile range\n- Descriptive statistics like mean, mode, standard deviation, sum, median absolute deviation, coefficient of variation, kurtosis, skewness\n- Most frequent values\n- Histogram\n- Correlations highlighting of highly correlated variables, Spearman, Pearson and Kendall matrices\n- Missing values matrix, count, heatmap and dendrogram of missing values\n- Text analysis learn about categories (Uppercase, Space), scripts (Latin, Cyrillic) and blocks (ASCII) of text data.\n<br><br>Read more about pandas_profiling [here](https:\/\/github.com\/pandas-profiling\/pandas-profiling)","6d207724":"Most of the Passengers came **without any sibling or spouse**.","1edbac1d":"### Now we have cleaned our whole dataset.","bbd2943e":"Passengers of class 1 and 2 did not had more than 3 Siblings, this is also a reason of their high survival rate as we have seen above that passengers with less Siblings had high Survival rate.","fc84ccc7":"Babies had the highest and the seniors had the lowest survival rate.","6d692cf4":"### Scaling the Data","8818ce48":"### Crosstab and FactorPlot Survived vs Pclass","ff8db738":"#### Feature Desciption:\nNumber of parents \/ children aboard the Titanic<br>\nThe dataset defines family relations in this way...<br>\nParent = mother, father<br>\nChild = daughter, son, stepdaughter, stepson<br>\nSome children travelled only with a nanny, therefore parch=0 for them.<br>","109c2272":"Filling the missing Fare value in test set based on mean fare for that Pclass","ac4819a4":"I'll use Kfold cross vaidation to evaluate the model, scoring method wil be 'acuracy', so lets initialize Kfolds first. ","79e97747":"Extract a title for each Name in the train and test datasets","53f46c6a":"Most of the Titles are Master, Miss, Mr and Mrs. Now we have to replace titles with most common names.","ca37b5e2":"Feature Description:<br>\nPort of Embarkation<br>\nC = Cherbourg, Q = Queenstown, S = Southampton\n","3ae18162":"### Some Observations from above output\n* There are a total of 891 passengers in our training set.\n* The Age feature is missing approximately 19.9% of its values. Age feature is pretty important to predict survival,so we should probably attempt to fill these gaps.\n* The Cabin feature is missing approximately 77.1% of its values. Since so much of the feature is missing, it would be hard to fill in the missing values. I'll probably drop these values from our dataset.\n* The Embarked feature is missing only 2 passeners, which should be relatively harmless.\n* The Name and Ticket features has very high cardinality, so many distict values are there in that feature. I'll probably drop these features.","342ed2f0":"Pandas_profiling saves lot of time while explorng the data. It shows all of the statistical moments, histogram of common values etc. for each feature but only when the notebook is in **Edit Mode**. That's why I've vizualize some important graph again in the further sections.","590af1de":"Sorting the ages into logical categories","460c7e80":"### Analyzing Feature: Age","3cfb104f":"Map each Embarked value to a numerical value","d304d4bf":"Range of :<br>\nFareBand 1: 0.0 -- 7.89   \nNo. of values: 223 <br><br>\nFareBand 2: 7.92 -- 14.45<br>\nNo. of values: 224<br><br>\nFareBand 3: 14.45 -- 31.0 <br>\nNo. of values: 222<br><br>\nFareBand 4: 31.27 -- 512 <br>\nNo. of values: 222","a74962b3":"Best parameters were :<br>\ncriterion--> entropy<br>\nn_estimators--> 40<br>\nmax_depth--> 10<br>\n","21af42bd":"From the above crosstab we can observe that most of the passengers embarked from Southampton, so we can fill missing values with S in this feature but we will check more about this in Data Cleaning steps.","2022f631":"Both passengers with missing embarked were of Pclass 1 and paid Fare 80.","e35c5493":"### Analyzing Feature: Embarked","00683f76":"## Final Observations:\n* Survival Rate of female is higher than male.\n* Pclass 1 passengers had highest and Pclass 3 passengers had lowest lowest survival rate.\n* Almost all of the female of Pclass 1 and 2 survived.\n* Almost all of the children till age 15 survived.\n* Passengers embarked form C had highest survival rate.\n* Passengers with 0 to 2 SibSp had highest survival rate.\n* Passengers with 0 to 3 Parch had highest survival rate.\n\n","eb3a494e":"In order to get rid of outliers and make data more usable. I am going to separate the fare values into some logical groups.\nMap Fare values into groups of numerical values and storing in a new feature \"FareBand\".","7eccf926":"Passengers Embarked at Cherbourg having highest Survival rate.","eea3aa97":"### Lets have a look on the concept of **KFold** Cross Validation.<br>\nCross Validation is one of the most important concepts in any type of data modelling. Itsimply says, try to leave a sample on which you do not train the model and test the modelon this sample before finalizing the model.<br>\nHere I am giving an example of KFold Cross Valdation with k=7:<br>\n![Screenshot%20%28568%29.png](attachment:Screenshot%20%28568%29.png)\nIn this what goes on behind the scene : we divide the entire dataset into 7 equal samples. Now we train models on 6samples (Green boxes) and validate on 1 sample (grey box). Then,at the second iteration we train the model with a different sample held as validation. In 7 iterations, we have basically built model on each sample and held each of them as validation. This is a way to reduce the selection bias and reduce the variance in prediction power. Once we have all the 7 models, we take average of the error terms to find which of the models is best. k-fold cross validation is widely used to check whether a model is an overfit or not. If theperformance metrics at each of the k times modelling are close to each other and the meanof metric is highest. In a Kaggle competition, you might rely more on the cross validationscore and not on the Kaggle public score. This way you will be sure that the Public score isnot just by chance","62bd7c23":"By observing the above piecharts and countplots, we can say that passenger of class 1 were given high priority and 3rd class passenger were given lowest priority while rescue.","f0ee30cd":"Lets check the correlation between different festures of our data.","6c426c69":"Here, I'll clean both test and training set.","128cc722":"Making a function that conatins all the classifiers.<br>\nEValuating model with Kfold cross validation.","5402da92":"### How many Passengers Survived?","b9fc0c90":"# Titanic \n\n","4d896cff":"RMS Titanic was a British passenger liner operated by the White Star Line that sank in the North Atlantic Ocean in the early morning hours of 15 April 1912, after striking an iceberg during her maiden voyage from Southampton to New York City. Of the estimated 2,224 passengers and crew aboard, more than 1,500 died, making the sinking one of modern history's deadliest peacetime commercial marine disasters. RMS Titanic was the largest ship afloat at the time she entered service and was the second of three Olympic-class ocean liners operated by the White Star Line. She was built by the Harland and Wolff shipyard in Belfast. Thomas Andrews, chief naval architect of the shipyard at the time, died in the disaster.<br>[Source](https:\/\/www.history.com\/topics\/early-20th-century-us\/titanic)\n<br>\n**Here I will explore and build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc). **\n","b1fa249c":"Ignore the unknown column<br>\nFrom the above graph we can observe that<br>\nMost of the AgeGroup with title 1 were Young Adults.<br>\nTitle 2 were Student<br>\nTitle 3 were Adult<br>\nTitle 4 were Baby<br>\nTitle 5 were Adult (There was only one passenger with title 5.)<br>\nTitle 6 were also Adult<br>","84e9a214":"I am grouping age with following categories:<br>\n-1 to 0 : Unknown<br>\nAfter 0 to 5 : Baby<br>\nAfter 5 to 12 : Child<br>\nAfter 12 to 18 : Teenager<br>\nAfter 18 to 24 : Student<br>\nAfter 24 to 35 : Young Adult<br>\nAfter 35 to 60 : Adult<br>\nAfter 60 to inf : Senior<br>\n","ccc657f2":"## Analyzing feature: Sibsp","870d90e2":"### Spot Checking 13 Differnet Classfiers","cced8da9":"# Import Necessary Libraries","0547403a":"### Feature: Embraked","ba0f0b8d":"### Feature: Cabin","39d00516":"Map each Sex value to a numerical value","afed2f04":"### Feature: Sex","cecf130d":"### Number of Survived and NonSurvived with Gender","ec23dc19":"### Male and Female Distribution on the ship","47ece93e":"In case of female passengers, Parch 0 passengers had highest survival rate but in case of male, Parch 1 had highest survival rate.","69d55a04":"# Read In and Exploring the Historic Data (EDA)","71ba2041":"### Analyzing Feature: Parch","062ff684":"MOst of the passengers were of Age 17 to 40.","5b5b9d36":"### Feature: Passenger Id","0f258290":"### Feature: Age","8ec4468c":"Most of the passngers were of AgeGroup Young Adults and Adults."}}