{"cell_type":{"3a0f1b2c":"code","df9fb16e":"code","7f5a2640":"code","1ba2ce21":"code","64eb0f61":"code","df5ab3f6":"code","3d15ad78":"code","9f655adf":"code","4b3b81d2":"code","1b07d626":"code","51342e49":"code","abdbd224":"code","2707bc0f":"code","6f3a5076":"code","11086819":"code","28fe47d8":"code","a7c8eaa9":"code","54dff62b":"code","01b6bcf3":"code","2fbab2cb":"code","cb147e1c":"code","c20391eb":"code","2122805d":"code","c5c46aae":"code","2db26580":"code","34939526":"code","d06dd49d":"code","1f542cfc":"code","d17b5598":"code","941d9934":"markdown","52b0071d":"markdown","c2c27b3c":"markdown","229dc154":"markdown","91822a8a":"markdown","0ea69c46":"markdown","ff743511":"markdown","0283eebc":"markdown","dc832abc":"markdown","78b843c4":"markdown","46bca62d":"markdown","019ebc1f":"markdown","e775afea":"markdown","66d18b04":"markdown","d4f04f14":"markdown","c4dfb05d":"markdown","a9fb8b0d":"markdown","f057cf1b":"markdown"},"source":{"3a0f1b2c":"# !pip install xlrd\n!pip install openpyxl\n!pip install pmdarima","df9fb16e":"# Library Import\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport missingno\nfrom datetime import date\n%config Completer.use_jedi = False\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\nfrom sklearn.preprocessing import MinMaxScaler\nfrom statsmodels.tsa.api import VAR\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tools.eval_measures import aic\nfrom pmdarima import auto_arima\n\nimport seaborn as sns # for plot visualization\nscaler = MinMaxScaler()\npd.options.mode.chained_assignment = None","7f5a2640":"df = pd.read_excel('..\/input\/forecasting-usecase-task\/history_and_forecast_data.xlsx')","1ba2ce21":"ts = df.set_index('Time series ID')\nts.index.name = 'Date'\nts.columns = ts.columns.astype(str)","64eb0f61":"train_set, test_set = ts.loc[:\"2018-12-01\",], ts.loc[\"2019-01-01\":,]","df5ab3f6":"missingno.matrix(train_set.replace(0, np.nan), figsize=(14, 6), labels=True, sparkline=False, fontsize=8);","3d15ad78":"train_set.replace(0, np.nan).isna().sum().to_csv('products_unavailability_count.csv')\n# test_set.replace(0, np.nan).isna().sum().plot(kind='bar', figsize=(14,7));","9f655adf":"# Selecting products with max six months of missing information.\ngroup1_products = pd.DataFrame()\ngroup2_products = pd.DataFrame()\ngroup3_products = pd.DataFrame()\n\nfor col in train_set.columns.unique():\n    prod_df = train_set[col].reset_index().copy()\n    prod_df.rename(columns={\n        f'{col}': 'qty' \n    }, inplace=True)\n    prod_df['product_id'] = col\n    # At least 2 years of available record    \n    if len(train_set[train_set[col]!=0]) >= 24:\n        group1_products = pd.concat([group1_products, prod_df], axis=0)\n\n    # 1-2 years of available record\n    elif len(train_set[train_set[col]!=0]) in range(12, 24):\n        group2_products = pd.concat([group2_products, prod_df], axis=0)\n    \n    # if <1 year of record is available\n    else:\n        group3_products = pd.concat([group3_products, prod_df], axis=0)\n        ","4b3b81d2":"print(f'Group 1 products: {group1_products.product_id.unique()}')\nprint(f'Group 2 products: {group2_products.product_id.unique()}')\nprint(f'Group 3 products: {group3_products.product_id.unique()}')","1b07d626":"train_set[group1_products.product_id.unique()].corr().to_csv('.\/group1_correlation.csv')","51342e49":"###\n# {\n#   subset-i: productids\n# }\n###\n\nmultivariate_model_subset_dict = {\n    1: ['1', '4', '9'],\n    2: ['2', '3', '5', '7'],\n    3: ['16', '17'],\n    4: ['35', '37', '39', '40'],\n    5: ['22', '36']\n}\n\nmultivariate_product_set = set([prod for prod_set in multivariate_model_subset_dict.values() for prod in prod_set ])","abdbd224":"for key, prod_ids in multivariate_model_subset_dict.items():\n    for prod_id in prod_ids:\n        product = group1_products[group1_products.product_id==prod_id]\n        product.loc[:,'qty'] = scaler.fit_transform(product['qty'].values.reshape(-1, 1))\n        ax = product.set_index('Date').qty.plot(figsize=(14,6), marker='o', label=f'prod-{prod_id}');\n        ax.legend(loc='upper left', frameon=False)\n\n    plt.title(f'Product Group 1 (subset {key})')\n    plt.ylabel('Quantity')\n    plt.show()","2707bc0f":"# Imputation Technique\n# Give a try to MICE\n\ndef impute_missing_values(prods_df):\n    prods_df[prods_df <= 0] = np.nan\n    prods_df.interpolate(method='spline', order=2, limit_direction='both', axis=0, inplace=True)\n\n    return prods_df","6f3a5076":"# Perform ADF Test\ndef adfuller_test(series, signif=0.05, verbose=False):\n    \"\"\"\n    This function performs ADF test on input\n    series to and checks for stationarity\n    Parameters:\n    series: train set timeseries (all indices)\n    signif: significance level\n    verbose: Flag to handle print statemetns\n    Returns:\n    is_stationary: Flag, to show if TS is stationary or not\n    \"\"\"\n    is_stationary = False\n    r = adfuller(series, autolag='AIC')\n    output = dict([('test_statistic', round(r[0], 4)),\n                   ('pvalue', round(r[1], 4)),\n                   ('n_lags', round(r[2], 4)), ('n_obs', r[3])])\n    p_value = output['pvalue']\n    if p_value <= signif:\n        is_stationary = True\n        if verbose:\n            print(f\" --> P-Value = {p_value}. Rejecting Null Hypothesis.\")\n            print(f\" --> Series is Stationary.\")\n    else:\n        if verbose:\n            print(f\"\"\" --> P-Value =\n            {p_value}. Weak evidence to reject the Null Hypothesis.\"\"\")\n            print(f\" --> Series is Non-Stationary.\")\n    return is_stationary","11086819":"def invert_transformation(df_train, df_forecast, used_diff_order):\n    \"\"\"\n    Function performs invert transformation\n    Parameters:\n    df_train: train set,\n    df_forecast: forecast values,\n    used_diff_order: used differencing order to make series stationary\n    Returns:\n    df_fc: inverted forecasted series\n    \"\"\"\n    df_fc = df_forecast.copy()\n    columns = df_train.columns\n\n    for col in columns:\n        # Roll back 2nd Diff\n        if used_diff_order == 2:\n            df_fc[str(col) + '_1'] = (df_train[col].iloc[-1] -\n                                      df_train[col].iloc[-2]) + \\\n             df_fc[str(col) + '_2'].cumsum()\n\n        # Roll back 1st Diff\n        df_fc[str(col) + '_forecast'] = df_train[col].iloc[-1] + \\\n            df_fc[str(col) + '_1'].cumsum()\n\n    return df_fc.drop(\n        columns=([col for col in df_fc.columns if\n                  ('_1' in col) or ('_2' in col)]))","28fe47d8":"MAX_DIFF_ORDER = 2\ndef multivariate_modelling(train, test):\n    train = impute_missing_values(train)\n    # we will perform differencing\n    stationary_df = train.copy()\n    stationary_check_rounds = 0\n    # Stationarity check\n    while stationary_check_rounds <= MAX_DIFF_ORDER:\n        is_ts_stationary = []\n        stationary_check_rounds += 1\n        for name, column in stationary_df.iteritems():\n            is_col_stationary = adfuller_test(column)\n            is_ts_stationary.append(is_col_stationary)\n        print(f'''\\n Stationarity check round:\n              {stationary_check_rounds} complete.\\n product-ids{train.columns}:\n              {is_ts_stationary}''')\n        # if any of the column is non-stationary then we'll\n        if False not in is_ts_stationary:\n            print(f'''\\n\\n\\n *** All Timeseries columns are stationary;\n                  Required Order of differencing:{stationary_check_rounds-1}\n                  ***\\n\\n\\n''')\n            break\n        # else\n        print(f'''\\n\\n ** After {stationary_check_rounds}\n        order differencing ** \\n\\n''')\n        stationary_df = stationary_df.diff().dropna()\n\n    # Fitting VAR Model\n    var_model = VAR(stationary_df)\n    used_diff_order = ((stationary_check_rounds - 1)\n                       if stationary_check_rounds > 1 else 'forecast')\n\n    best_p_order = 4\n    model_fitted = var_model.fit(best_p_order)\n    lag_order = model_fitted.k_ar\n    forecast_input = stationary_df.values[-lag_order:]\n    forecasted_vals = model_fitted.forecast(y=forecast_input, steps=12)\n    test_df = pd.DataFrame(forecasted_vals, columns=test.columns+f'_{used_diff_order}', index=test.index)\n\n    if used_diff_order != 'forecast':\n        test_df = (invert_transformation(train,\n                            test_df, used_diff_order))\n    test_df = abs(test_df)\n    return test_df","a7c8eaa9":"multivariate_forecast_df = pd.DataFrame()\n# Iterate over related product subsets\nfor key, value in multivariate_model_subset_dict.items():\n    print(f'\\n\\n****** Modelling set:{key}, products:{value} ******\\n\\n')\n    prod_set_forecast = multivariate_modelling(train_set[value], test_set[value])\n    multivariate_forecast_df = pd.concat([multivariate_forecast_df, prod_set_forecast], axis=1)","54dff62b":"multivariate_forecast_df.head()","01b6bcf3":"for prod_id in multivariate_product_set:\n    \n    prod_train_df = group1_products[group1_products.product_id==prod_id].set_index('Date').qty\n\n    prod_train_df.plot(label='Actual', figsize=(13,6))\n    ax = forecast_df[f'{prod_id}_forecast'].plot(label=f'Forecast', figsize=(13,6))\n    ax.legend(loc='upper left', frameon=False);\n\n    plt.title(f'Product-{prod_id}')\n    plt.ylabel('Quantity')\n    plt.show()","2fbab2cb":"# productIds with at least 2 years of record availability\ngroup1_product_set = set(group1_products.product_id.unique())\n\n# Identifying products which could not become a part of multivariate modelling\nunivariate_prod_list = list(group1_product_set - multivariate_product_set)\n\n# putting group1 left out products and group2 products for univariate modelling\nunivariate_model_prods = list(univariate_prod_list) + list(group2_products.product_id.unique())\nunivariate_model_prods","cb147e1c":"# Auto ARIMA Function\ndef univariate_modelling(prod_id):\n    train = train_set[prod_id].copy()\n    test = test_set[prod_id].copy()\n\n    model = auto_arima(train, start_p=5, max_p=8, d=2, start_q=3,\n                       trace=True, error_action='ignore', suppress_warnings=True, \n                      n_fits=100,information_criterion='bic')\n    model.fit(train)\n\n    forecasted_vals = model.predict(n_periods=12)\n    test_df = pd.DataFrame(forecasted_vals, columns=[f'{prod_id}_forecast'], index=test.index)\n    test_df = abs(test_df)\n\n    return test_df","c20391eb":"uni_forecast_df = pd.DataFrame()\n# Iterating over few group1 and all groups 3 product ids\nfor prod_id in univariate_model_prods:\n    print(f'\\n\\n****** products:{prod_id} ******\\n\\n')\n    prod_forecast = univariate_modelling(prod_id)\n    uni_forecast_df = pd.concat([uni_forecast_df, prod_forecast], axis=1)","2122805d":"\nfor prod_id in sorted(univariate_model_prods):\n    \n    prod_train_df = train_set[prod_id]\n\n    prod_train_df.plot(label='Actual', figsize=(13,6))\n    ax = uni_forecast_df[f'{prod_id}_forecast'].plot(label=f'Forecast', figsize=(13,6))\n    ax.legend(loc='upper left', frameon=False);\n\n    plt.title(f'Product-{prod_id}')\n    plt.ylabel('Quantity')\n    plt.show()","c5c46aae":"group3_products.product_id.unique()","2db26580":"products_corr_with_group3 = train_set.corr()[group3_products.product_id.unique()]","34939526":"# DataFrame which will hold complete forecast results\ncomplete_forecast = pd.concat([uni_forecast_df, multivariate_forecast_df], axis=1)","d06dd49d":"# We are finding the most similar products (from better groups), using med-high correlation.\nfor p_id in products_corr_with_group3.columns:\n    related_prods = products_corr_with_group3[products_corr_with_group3[p_id]>0.5].sort_values(by=p_id)\n    related_prods_in_better_grps = set(related_prods.index)-set(group3_products.product_id.unique())    \n    complete_forecast[f'{p_id}_forecast'] = complete_forecast[list(related_prods_in_better_grps)[0]+'_forecast']","1f542cfc":"complete_forecast.columns = complete_forecast.columns.str.replace('_forecast', '')\ncomplete_forecast.columns = complete_forecast.columns.astype(int)\ncomplete_forecast = complete_forecast[range(complete_forecast.columns.min(), complete_forecast.columns.max()+1)]","d17b5598":"complete_forecast.to_csv('forecasting_results.csv')","941d9934":"### Visualize multivariate forecast","52b0071d":"### group3_products forecasting","c2c27b3c":"***Go ahead.. fork and explore on your own and let me know your suggestions and doubts in a comment section. Don't forget to upvote if you find it helpful.***","229dc154":"## Multivariate Modelling\n\nThese product subsets are identified based on prodct correlation report we generated earlier.\n\nGrangers Causality would make a more sense if we had more data points.","91822a8a":"### Visualize univariate forecasts","0ea69c46":"### Group 1 - Multivariate product demand data preparation:","ff743511":"### Analysing demand unavailability for products:","0283eebc":"Hello guys, here we are going to look into a business requirement, where product demand forecasting is required. Problem is that, we have 'n' number of products, out of which, oldest product demand history contains 3 years of monthly numbers, and newly launched product contains as less as 1 month observation. \n\nNow, we are required to give a forecast for next 12 months, technique and methodology is completely up to us. We basically need to make sure that our modelling technique selects the products dynamically, means that, we may want to deploy multiple modelling strategies to make sure that we are getting a benefit of multivariate forecasting, wherever possible.","dc832abc":"We are done with Group1 and Group 2 products' forecasting.","78b843c4":"### Read product demand history:","46bca62d":"### Visualize all Group 1, product subsets for Multivariate modelling","019ebc1f":"### Product group formation (based on data availability):","e775afea":"### Identify products best suited for univariate modelling:","66d18b04":"### Train Auto-ARIMA Model:","d4f04f14":"### Train VAR (Vector autoregression) Model:","c4dfb05d":"**Scope of Improvement:**\n\n- We can use MICE for data imputation, idea is to perform imputation based on correlated product groups.\n\n- For group3 products, we should scale forecast values to the corresponding product's historical levels.\n\n- For group2 poorly forecasted products, do 'Why analysis' on results to understand the root cause behind the poor forecasts. Once root cause is identified, then after making required adjustments, switch to any other forecasting technique.\n\n- Identify more variables, i.e. market and demand leading indicators. to better perform Multivariate forecasting.","a9fb8b0d":"As explained in use case document, we can find the closet product (from group 1 and group 2), for each groups 3 products. And consider forecast obtained for them.","f057cf1b":"## Univariate Modelling"}}