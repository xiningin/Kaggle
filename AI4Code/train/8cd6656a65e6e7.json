{"cell_type":{"f5f43648":"code","57927ade":"code","2d1eb32c":"code","a2439ad1":"code","cce527d4":"code","4545a9e4":"code","827e895c":"code","b9468fcc":"code","871d401c":"code","cbbdc568":"code","437f42ad":"markdown","332e5e43":"markdown","ae26264d":"markdown","e4892361":"markdown","922346bc":"markdown","40b89dc7":"markdown","fb0d4209":"markdown","55863bcb":"markdown","5c01f990":"markdown","2b773cb1":"markdown"},"source":{"f5f43648":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.utils import shuffle\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\nfrom pylab import *\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_probability as tfp\n\nimport warnings\nwarnings.filterwarnings('ignore')","57927ade":"path = '..\/input\/water-potability\/water_potability.csv'\ndf = pd.read_csv(path)\ndf.fillna(df.mean(), inplace=True)\ndf.head()","2d1eb32c":"features = df.drop('Potability', axis=1)\n\nscaler = StandardScaler()\nstand_features = scaler.fit_transform(features)\nstand_features = stand_features.astype(np.float32)\n\nlabels = df['Potability']","a2439ad1":"pca = PCA(n_components=2)\npca.fit(stand_features)\nX = pca.transform(stand_features)\n\n# colors indices\nidx_one = list(df[df['Potability'] == 1].index)\nidx_zero = list(df[df['Potability'] == 0].index)","cce527d4":"plt.figure(figsize=(14,8))\nplt.plot(X[idx_one][:,0], X[idx_one][:,1], 'bo')\nplt.plot(X[idx_zero][:,0], X[idx_zero][:,1], 'ro')","4545a9e4":"tfb = tfp.bijectors\ntfd = tfp.distributions","827e895c":"class RealNVP(tf.keras.models.Model):\n\n    def __init__(self, *, output_dim, num_masked, **kwargs):\n        super().__init__(**kwargs)\n        self.output_dim = output_dim\n        self.nets=[]\n\n        bijectors=[]\n        num_blocks = 5\n        h = 32 \n        for i in range(num_blocks): \n            net = tfb.real_nvp_default_template([h, h])\n            bijectors.append(\n                tfb.RealNVP(shift_and_log_scale_fn=net, \n                            num_masked=num_masked))\n            bijectors.append(tfb.Permute([1,0]))\n            self.nets.append(net) \n        bijector = tfb.Chain(list(reversed(bijectors[:-1])))\n\n        self.flow = tfd.TransformedDistribution(#G\n            distribution=tfd.MultivariateNormalDiag(loc=[0., 0.]), \n            bijector=bijector)\n        \n    def call(self, *inputs): \n        return self.flow.bijector.forward(*inputs)","b9468fcc":"model = RealNVP(output_dim=2, num_masked=1)\n_ = model(X) \nprint(model.summary())","871d401c":"optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n# needs to be called other-wise @tf.function has problem\n-tf.reduce_mean(model.flow.log_prob(X)) \n\n@tf.function\ndef train_step(X): \n    with tf.GradientTape() as tape:\n        loss = -tf.reduce_mean(model.flow.log_prob(X)) \n    gradients = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n    return loss      \n\nfrom time import time\nstart = time()\n\nfor i in range(len(df)):\n\n    loss = train_step(X)\n    if (i % 100 == 0):\n        print(\"i:{:4d}, loss:{:1.3f}, time:{:1.3f}\"\n                .format(i ,loss.numpy(), (time()-start)))\n        start = time()","cbbdc568":"# generate random data\nZ = np.random.normal(0,1,(5000,2))\n\nplt.figure(figsize=(14,6))\nplt.subplot(1,2,1)\nplt.plot(Z[:,0], Z[:,1], 'bo')\nplt.title('$Z \\sim N(0,1)$')\nplt.xlabel('$z_1$')\nplt.ylabel('$z_2$')\n\n# predict dimensions\nXs = model(Z).numpy()\n\nplt.subplot(1,2,2)\nplt.title('Transformed distribution')\nplt.xlabel('$x_1$')\nplt.ylabel('$x_2$')\nplt.plot(Xs[idx_one][:,0], Xs[idx_one][:,1], 'bo')\nplt.plot(Xs[idx_zero][:,0], Xs[idx_zero][:,1], 'ro')\nplt.xlim(-5.,5.)\nplt.ylim(-4.,4.)","437f42ad":"## Standardize","332e5e43":"<h1 id=\"analysis\" style=\"color:blue; background:white; border:0.5px dotted cyan;\"> \n    <center>Analysis\n        <a class=\"anchor-link\" href=\"#analysis\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","ae26264d":"<div style=\"width: 100%\">\n    <img style=\"width: 100%\" src=\"https:\/\/storage.googleapis.com\/kaggle-datasets-images\/681739\/1196904\/5c9764c44d37ca06ae29daeaa405e3a3\/dataset-cover.jpg\"\/>\n<\/div>","e4892361":"## Plot the 2D graph","922346bc":"<h1 id=\"dataset\" style=\"color:blue; background:white; border:0.5px dotted cyan;\"> \n    <center>Dataset\n        <a class=\"anchor-link\" href=\"#dataset\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","40b89dc7":"## Dimension reduction","fb0d4209":"<h1 id=\"real\" style=\"color:blue; background:white; border:0.5px dotted cyan;\"> \n    <center>Real NVP\n        <a class=\"anchor-link\" href=\"#real\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","55863bcb":"## Density Estimation using Real NVP\n\nUnsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations. ","5c01f990":"## Model","2b773cb1":"## Training"}}