{"cell_type":{"3819465f":"code","96683b11":"code","bd0c2862":"code","3b37c537":"code","f4ff2671":"code","38ebdd4c":"code","33cca549":"code","ade6c7bd":"code","2128f7a1":"code","7e757958":"code","844bfa18":"code","3f1828d2":"code","c7b478ed":"code","607288f4":"code","47a3c6fc":"code","72c057be":"code","be42e254":"code","4943f7ce":"code","8bdb62ae":"code","f33271e8":"code","4e4ad1a2":"code","4f090cce":"code","573c0039":"code","6c36dabd":"code","0168e2b8":"code","39b692e2":"code","4af58381":"code","2941b445":"code","49317f38":"code","ea91210d":"code","344b4263":"code","7850888c":"code","7afdd1e3":"code","58a291ed":"code","a845c111":"code","36bfa733":"code","89842c55":"code","c122496a":"code","9eeace3d":"code","8b54de5d":"code","95df4dc0":"code","169d034a":"code","ed38a14d":"code","558195e8":"code","d282c9f7":"code","65f8abcf":"code","cff3619b":"code","94e1a64d":"markdown","c5e6c026":"markdown","882304e9":"markdown","e655fda2":"markdown","4a19062a":"markdown","f050e932":"markdown","ff89069e":"markdown","c39a3775":"markdown","c09118cd":"markdown","f1c0560a":"markdown"},"source":{"3819465f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","96683b11":"train_df = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")","bd0c2862":"text = \"natural language processing and machine learning is fun and exciting\"\ncorpus = [[word.lower() for word in text.split()]]\n","3b37c537":"settings = {\n'window_size': 2,# context window +- center word\n'n': 10,# dimensions of word embeddings, also refer to size of hidden layer\n'epochs': 50,# number of training epochs\n'learning_rate': 0.01# learning rate\n}","f4ff2671":"!python --version","38ebdd4c":"import re  # For preprocessing\nimport pandas as pd  # For data handling\nfrom time import time  # To time our operations\nfrom collections import defaultdict  # For word frequency\n\nimport spacy  # For preprocessing\n\nimport logging  # Setting up the loggings to monitor gensim","33cca549":"train_df = pd.DataFrame(train_df)\ntest_df = pd.DataFrame(test_df)","ade6c7bd":"train_df.shape","2128f7a1":"test_df.shape","7e757958":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict\nfrom collections import  Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam\n","844bfa18":"import pandas as pd\n","3f1828d2":"tweet= pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest= pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ntweet.head(3)","c7b478ed":"x=tweet.target.value_counts()\nsns.barplot(x.index,x)\nplt.gca().set_ylabel('samples')","607288f4":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=tweet[tweet['target']==1]['text'].str.len()\nax1.hist(tweet_len,color='red')\nax1.set_title('disaster tweets')\ntweet_len=tweet[tweet['target']==0]['text'].str.len()\nax2.hist(tweet_len,color='green')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Characters in tweets')\nplt.show()","47a3c6fc":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=tweet[tweet['target']==1]['text'].str.split().map(lambda x: len(x))\nax1.hist(tweet_len,color='red')\nax1.set_title('disaster tweets')\ntweet_len=tweet[tweet['target']==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(tweet_len,color='green')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Words in a tweet')\nplt.show()","72c057be":"def create_corpus(target):\n    corpus=[]\n    \n    for x in tweet[tweet['target']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","be42e254":"corpus=create_corpus(0)\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n        \ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] ","4943f7ce":"x,y=zip(*top)\nplt.bar(x,y)","8bdb62ae":"corpus=create_corpus(1)\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n\ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \n    \n\n\nx,y=zip(*top)\nplt.bar(x,y)","f33271e8":"df=pd.concat([tweet,test])\ndf.shape","4e4ad1a2":"example=\"New competition launched :https:\/\/www.kaggle.com\/c\/nlp-getting-started\"","4f090cce":"def remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\nremove_URL(example)","573c0039":"df['text']=df['text'].apply(lambda x : remove_URL(x))\n","6c36dabd":"example = \"\"\"<div>\n<h1>Real or Fake<\/h1>\n<p>Kaggle <\/p>\n<a href=\"https:\/\/www.kaggle.com\/c\/nlp-getting-started\">getting started<\/a>\n<\/div>\"\"\"","0168e2b8":"def remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\nprint(remove_html(example))","39b692e2":"df['text']=df['text'].apply(lambda x : remove_html(x))","4af58381":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\nremove_emoji(\"Omg another Earthquake \ud83d\ude14\ud83d\ude14\")","2941b445":"df['text']=df['text'].apply(lambda x: remove_emoji(x))\n","49317f38":"def remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\nexample=\"I am a #king\"\nprint(remove_punct(example))","ea91210d":"df['text']=df['text'].apply(lambda x : remove_punct(x))\n","344b4263":"!pip install pyspellchecker\n","7850888c":"from spellchecker import SpellChecker\n\nspell = SpellChecker()\ndef correct_spellings(text):\n    corrected_text = []\n    misspelled_words = spell.unknown(text.split())\n    for word in text.split():\n        if word in misspelled_words:\n            corrected_text.append(spell.correction(word))\n        else:\n            corrected_text.append(word)\n    return \" \".join(corrected_text)\n        \ntext = \"corect me plese\"\ncorrect_spellings(text)","7afdd1e3":"#GloVe for Vectorization","58a291ed":"def create_corpus(df):\n    corpus=[]\n    for tweet in tqdm(df['text']):\n        words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in stop))]\n        corpus.append(words)\n    return corpus","a845c111":"corpus = create_corpus(df)","36bfa733":"embedding_dict={}\nwith open('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.100d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word=values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()","89842c55":"MAX_LEN=50\ntokenizer_obj=Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\nsequences=tokenizer_obj.texts_to_sequences(corpus)\n\ntweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')\n","c122496a":"word_index=tokenizer_obj.word_index\nprint('Number of unique words:',len(word_index))","9eeace3d":"num_words=len(word_index)+1\nembedding_matrix=np.zeros((num_words,100))\n\nfor word,i in tqdm(word_index.items()):\n    if i > num_words:\n        continue\n    \n    emb_vec=embedding_dict.get(word)\n    if emb_vec is not None:\n        embedding_matrix[i]=emb_vec\n            ","8b54de5d":"# BaseLine Model","95df4dc0":"model=Sequential()\n\nembedding=Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),\n                   input_length=MAX_LEN,trainable=False)\n\nmodel.add(embedding)\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\n\noptimzer=Adam(learning_rate=1e-5)\n\nmodel.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])","169d034a":"model.summary()\n","ed38a14d":"train=tweet_pad[:tweet.shape[0]]\ntest=tweet_pad[tweet.shape[0]:]","558195e8":"X_train,X_test,y_train,y_test=train_test_split(train,tweet['target'].values,test_size=0.15)\nprint('Shape of train',X_train.shape)\nprint(\"Shape of Validation \",X_test.shape)","d282c9f7":"history=model.fit(X_train,y_train,batch_size=4,epochs=15,validation_data=(X_test,y_test),verbose=1)","65f8abcf":"sample_sub=pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')\n","cff3619b":"y_pre=model.predict(test)\ny_pre=np.round(y_pre).astype(int).reshape(3263)\nsub=pd.DataFrame({'id':sample_sub['id'].values.tolist(),'target':y_pre})\nsub.to_csv('submission.csv',index=False)","94e1a64d":"# Making our submission","c5e6c026":"# Cleaning","882304e9":"Here we will use GloVe pretrained corpus model to represent our words.It is available in 3 varieties :50D ,100D and 200 Dimentional.We will try 100 D here.","e655fda2":"We will Start with a tutorial on Word2Vec to understand how to work with this.","4a19062a":"# References\nhttps:\/\/www.kaggle.com\/pierremegret\/gensim-word2vec-tutorial\n","f050e932":"<b>\nNumber of words in a tweet","ff89069e":"**Common stopwords in tweets**","c39a3775":"**Number of characters in tweets**","c09118cd":"**Spelling correction**","f1c0560a":"2. Hyperparameters\n\nBefore we jump into the actual implementation, let us define some of the hyperparameters we need later."}}