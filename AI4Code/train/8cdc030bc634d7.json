{"cell_type":{"8d38c431":"code","c87ea9f6":"code","9732110e":"code","205bb1b3":"code","32c961b8":"code","8f14ac55":"code","7eac6d48":"code","0dd2eb99":"code","972a1a21":"code","42d86ad6":"code","30bc83d3":"code","a4d72eb3":"code","35e49f87":"code","15238ac5":"code","0af7b33e":"code","bbb474c7":"code","69dcebc2":"code","bc255cd6":"code","71bc61d7":"code","a79d84ce":"code","1fd78876":"code","0a51446b":"code","1ab150a3":"code","8a4d677e":"code","0c9846bc":"code","40124b6f":"code","33bf6d82":"code","323ca16b":"code","254316fd":"code","f3b551b1":"code","d57de7e6":"code","67515f77":"code","5276e870":"code","3585532e":"code","04c489e2":"code","eae1db5a":"code","a28c0e9f":"code","3dad3da0":"code","4513e944":"code","e70a6e9e":"code","6e0284ed":"code","400e248c":"code","28eacae2":"code","e3b18636":"code","28adc237":"code","91979f32":"code","bd7ef5dd":"code","d604631b":"code","262a4036":"code","01a7237a":"code","8c1ad840":"code","c349e0c1":"code","621f9b68":"code","7653adc6":"code","c03b4995":"code","525fb415":"code","cde2bde4":"code","5f7c965d":"code","fcbf7f5d":"code","4e28a9d4":"markdown","02f027a5":"markdown","3ea7ae15":"markdown","37900fba":"markdown","52d8a42e":"markdown","4f77bb2f":"markdown","6bcd860d":"markdown","3436fb8c":"markdown","d2816e93":"markdown","e4422ad5":"markdown","779fa1f0":"markdown","b6f9b7c3":"markdown","4703baf3":"markdown","d6ebd216":"markdown","49bddbd4":"markdown","ad15596e":"markdown","cc62d54d":"markdown","511f95c5":"markdown","a974f4ad":"markdown","541391e7":"markdown","4bc62d59":"markdown"},"source":{"8d38c431":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c87ea9f6":"import pandas as pd\n\ntrain = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")\n\ntrain.shape, test.shape\n#\ubcc0\uc218\ub97c \uc904\uc5ec\uc57c \uaca0\ub2e4. \uc5b4\ub5a4 \ubcc0\uc218\ub97c \uc904\uc5ec\uc57c \ud560\uae4c ?","9732110e":"train.info()\n#80\uac1c \uceec\ub7fc, SalePrice(\ub3c5\ub9bd\ubcc0\uc218)\ub97c \uc81c\uac70 \ud558\uace0\ub294 \ub098\uba38\uc9c0\uac00 \uc885\uc18d\ubcc0\uc218 = \ub108\ubb34 \ub9ce\ub2e4 !! ","205bb1b3":"train.drop(train[(train['OverallQual'] < 5) & (train['SalePrice']> 200000)].index, inplace = True)\ntrain.reset_index(drop = True, inplace = True)\ntrain.shape","32c961b8":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint(\"The value of mu before log transformation is:\", mu)\nprint(\"The value of sigma before log transformation is:\", sigma)\n\nfig, ax = plt.subplots(figsize=(10, 6))\nsns.histplot(train['SalePrice'], color=\"b\", stat=\"probability\")\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"SalePrice\")\nax.set(title=\"SalePrice distribution\")\n\nplt.axvline(mu, color='r', linestyle='--')\nplt.text(mu + 10000, 0.11, 'Mean of SalePrice', rotation=0, color='r')\nfig.show()","8f14ac55":"import numpy as np \n\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"]) # \ub85c\uadf8 \ubcc0\ud658 \ud6c4 \uc885\uc18d \ubcc0\uc218 \uc2dc\uac01\ud654 \n\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint(\"The value of mu before log transformation is:\", mu)\nprint(\"The value of sigma before log transformation is:\", sigma)\n\nfig, ax = plt.subplots(figsize=(10, 6))\nsns.histplot(train['SalePrice'], color=\"b\", stat=\"probability\")\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"SalePrice\")\nax.set(title=\"SalePrice distribution\")\n\nplt.axvline(mu, color='r', linestyle='--')\nplt.text(mu + 0.05, 0.111, 'Mean of SalePrice', rotation=0, color='r')\nplt.ylim(0, 0.12)\nfig.show()","7eac6d48":"train_ID = train['Id']\ntest_ID = test['Id']\ntrain.drop(['Id'], axis=1, inplace=True)\ntest.drop(['Id'], axis=1, inplace=True)\ntrain.shape, test.shape","0dd2eb99":"# y \uac12 \ucd94\ucd9c, dataset \ubd84\ub9ac\ud560\ub54c \uc0ac\uc6a9\ny = train['SalePrice'].reset_index(drop=True)\n\n# \ubf51\uace0 \ub098\uba74 \uc6d0\ub798 df\uc5d0\uc11c \uc81c\uac70\ntrain = train.drop('SalePrice', axis = 1)\ntrain.shape, test.shape, y.shape\n\n","972a1a21":"# data \ud569\uce58\uae30 \n# - train data \uc640  Test\ub97c \uac19\uc774 \uc804\ucc98\ub9ac \ud558\uae30 \uc704\ud574 \n\nall_df = pd.concat([train, test]).reset_index(drop=True)\nall_df.shape","42d86ad6":"#\uacb0\uce21\uce58 \ud655\uc778 \n\ndef check_na(data, head_num = 6):\n  isnull_na = (data.isnull().sum() \/ len(data)) * 100\n  data_na = isnull_na.drop(isnull_na[isnull_na == 0].index).sort_values(ascending=False)\n  missing_data = pd.DataFrame({'Missing Ratio' :data_na, \n                               'Data Type': data.dtypes[data_na.index]})\n  print(\"\uacb0\uce21\uce58 \ub370\uc774\ud130 \uceec\ub7fc\uacfc \uac74\uc218:\\n\", missing_data.head(head_num))\n\ncheck_na(all_df, 20)","30bc83d3":"all_df.drop(['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'LotFrontage'], axis=1, inplace=True)\ncheck_na(all_df)\n\n#\uc544\uc9c1\ub3c4 \uacb0\uce21\uce58\uac00 \ub9ce\uc774 \uc788\ub2e4. ","a4d72eb3":"#a = all_df['BsmtCond'].value_counts().mode() #mode() : \ucd5c\ube48\uac12 \uac00\uc7a5 \ube48\ub3c4\uc218\uac00 \ub192\uc740 \uac12 \ucc3e\uae30\n#a\n\nprint(all_df['BsmtCond'].value_counts())\nprint()\nprint(all_df['BsmtCond'].mode()[0])\n\n#object column, \uac2f\uc218 \ud655\uc778\nimport numpy as np\ncat_all_vars = train.select_dtypes(exclude=[np.number]) #\uc22b\uc790\uc778 \uac83\uc744 \uc81c\uc678\ud55c type column \uc774\ub984 \ucd94\ucd9c\nprint(\"The whole number of all_vars(\ubb38\uc790\ud615data)\", len(list(cat_all_vars)))\n\n#column \uc774\ub984 \ubf51\uc544\ub0b4\uae30 \nfinal_cat_vars = []\nfor v in cat_all_vars:\n    if v not in ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'LotFrontage']:\n        final_cat_vars.append(v)\n\nprint(\"The whole number of final_cat_vars\", len(final_cat_vars))\n\n#\ucd5c\ube48\uac12\uc744 \ucc3e\uc544 \ub123\uc5b4\uc8fc\uae30\nfor i in final_cat_vars:\n  all_df[i] = all_df[i].fillna(all_df[i].mode()[0])\n\ncheck_na(all_df, 20)\nprint(\"\uc22b\uc790\ud615 data set\uc758 \uacb0\uce21\uce58\ub9cc \ub0a8\uc740 \uac83\uc744 \uc54c \uc218 \uc788\ub2e4. \")","35e49f87":"import numpy as np\nnum_all_vars = list(train.select_dtypes(include=[np.number]))\nprint(\"The whole number of all_vars\", len(num_all_vars))\n\nnum_all_vars.remove('LotFrontage')\n\nprint(\"The whole number of final_cat_vars\", len(num_all_vars))\nfor i in num_all_vars:\n  all_df[i].fillna(value=all_df[i].median(), inplace=True)\n\nprint(\"\uacb0\uce21\uce58\uac00 \uc874\uc7ac \ud558\uc9c0 \uc54a\uc740 \uac83\uc744 \uc54c \uc218 \uc788\ub2e4. \")\ncheck_na(all_df, 20)","15238ac5":"all_df.info()","0af7b33e":"from scipy.stats import skew\n\n#\uc678\ub3c4 \ud310\uc815\uc744 \ubc1b\uc744 \ub9cc\ud55c data set\uc744 \ud655\uc778 \ndef find_skew(x):\n  return skew(x)\n#\uc55e\uc5d0\uc11c \ubf51\uc740 numeric columns : num_all_vars\n#\uc0ac\uc6a9\uc790 \uc815\uc758\ud568\uc218\ub97c \uc4f0\uae30 \uc704\ud574 apply(find_skew)\ub97c \uc0ac\uc6a9, \uc624\ub984\ucc28\uc21c\uc815\ub82c\n\nskewness_features = all_df[num_all_vars].apply(find_skew).sort_values(ascending=False)\nskewness_features\n\n#high_skew = skew_valrs[slew_var > 1]\n\n#0~1\uc0ac\uc774\uc5d0 \uc788\ub294 \uac83\uc774 \uae30\uc900. \uae30\uc900 \ubc16\uc73c\ub85c \ub098\uac04 \uacbd\uc6b0 \uc870\uc815\uc774 \ud544\uc694(\uc815\uaddc\ubd84\ud3ec\ub97c \ub9cc\ub4e4\uc5b4 \uc8fc\uae30 \uc704\ud574)\n# 1. \ubc15\uc2a4\ucf54\uc2a4 \ubcc0\ud658 : ML -> RMSE (2.5) \n# 2. \ub85c\uadf8\ubcc0\ud658     : ML -> RMSE (2.1)\n# => RMSE\ub294 \uc801\uc740 \uac83\uc774 \uc88b\uae30 \ub54c\ubb38\uc5d0, \ub85c\uadf8 \ubcc0\ud658\uc73c\ub85c \uc0ac\uc6a9 \ud558\ub294 \uac83\uc774 \uc88b\ub2e4. ","bbb474c7":"skewnewss_index = list(skewness_features.index)\nskewnewss_index.remove('LotArea')\n#\uc678\ub3c4 \uc815\ub3c4\uac00 \ub108\ubb34 \ub192\uc740 LotArea\ub97c \ub0a0\ub824\uc8fc\ub294 \uac83.\nall_numeric_df = all_df.loc[:, skewnewss_index]\n\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.set_xlim(0, all_numeric_df.max().sort_values(ascending=False)[0])\nax = sns.boxplot(data=all_numeric_df[skewnewss_index] , orient=\"h\", palette=\"Set1\")\nax.xaxis.grid(False)\nax.set(ylabel=\"Feature names\")\nax.set(xlabel=\"Numeric values\")\nax.set(title=\"Numeric Distribution of Features Before Box-Cox Transformation\")\nsns.despine(trim=True, left=True)","69dcebc2":"from scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\nhigh_skew = skewness_features[skewness_features > 1]\nhigh_skew_index = high_skew.index\n\nprint(\"The data before Box-Cox Transformation: \\n\", all_df[high_skew_index].head())\n\nfor num_var in high_skew_index:\n  all_df[num_var] = boxcox1p(all_df[num_var], boxcox_normmax(all_df[num_var] + 1))\n\nprint(\"The data after Box-Cox Transformation: \\n\", all_df[high_skew_index].head())","bc255cd6":"fig, ax = plt.subplots(figsize=(10, 6))\nax.set_xscale('log')\nax = sns.boxplot(data=all_df[high_skew_index] , orient=\"h\", palette=\"Set1\")\nax.xaxis.grid(False)\nax.set(ylabel=\"Feature names\")\nax.set(xlabel=\"Numeric values\")\nax.set(title=\"Numeric Distribution of Features Before Box-Cox Transformation\")\nsns.despine(trim=True, left=True)","71bc61d7":"#\uc9d1\uc758 \uce35\uc218 \ub97c \ub354\ud574\uc11c \uc804\uccb4\uba74\uc801\uc774\ub77c\ub294 \ubcc0\uc218\ub97c \ub3c4\ucd9c \nall_df['TotalSF'] = all_df['TotalBsmtSF'] + all_df['1stFlrSF'] + all_df['2ndFlrSF']\nall_df = all_df.drop(['TotalBsmtSF', '1stFlrSF', '2ndFlrSF'], axis=1)\nprint(all_df.shape)","a79d84ce":"all_df['Total_Bathrooms'] = (all_df['FullBath'] + (0.5 * all_df['HalfBath']) + all_df['BsmtFullBath'] + (0.5 * all_df['BsmtHalfBath']))\nall_df['Total_porch_sf'] = (all_df['OpenPorchSF'] + all_df['3SsnPorch'] + all_df['EnclosedPorch'] + all_df['ScreenPorch'])\nall_df = all_df.drop(['FullBath', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath', 'OpenPorchSF', '3SsnPorch', 'EnclosedPorch', 'ScreenPorch'], axis=1)\nprint(all_df.shape)","1fd78876":"# \uc5f0\ub3c4\uc640 \uad00\ub828\ub41c.\nnum_all_vars = list(train.select_dtypes(include=[np.number]))\nyear_feature = []\nfor var in num_all_vars:\n  if 'Yr' in var:\n    year_feature.append(var)\n  elif 'Year' in var:\n    year_feature.append(var)\n  else:  \n    print(var, \"is not related with Year\")\nprint(year_feature)","0a51446b":"fig, ax = plt.subplots(3, 1, figsize=(10, 6), sharex=True, sharey=True)\nfor i, var in enumerate(year_feature):\n  if var != 'YrSold':\n    ax[i].scatter(train[var], y, alpha=0.3)\n    ax[i].set_title('{}'.format(var), size=15)\n    ax[i].set_ylabel('SalePrice', size=15, labelpad=12.5)\nplt.tight_layout()\nplt.show()","1ab150a3":"all_df = all_df.drop(['YearBuilt', 'GarageYrBlt'], axis=1)\nprint(all_df.shape)","8a4d677e":"# \ub9ac\ubaa8\ub378\ub9c1 \uc2dc\uc810\uc73c\ub85c \ubd80\ud130 \uc5bc\ub9c8\ub098 \ub418\uc5c8\ub098 + \ud314\ub9ac\ub294\uac70\nYearsSinceRemodel = train['YrSold'].astype(int) - train['YearRemodAdd'].astype(int)\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.scatter(YearsSinceRemodel, y, alpha=0.3)\nfig.show()","0c9846bc":"all_df['YearsSinceRemodel'] = all_df['YrSold'].astype(int) - all_df['YearRemodAdd'].astype(int)\nall_df = all_df.drop(['YrSold', 'YearRemodAdd'], axis=1)\nprint(all_df.shape)","40124b6f":"all_df['PoolArea'].value_counts()\n#0\uacfc \ub2e4\ub978\uac12\ub4e4.. \uc73c\ub85c \ub418\uc5b4\uc788\uc5b4\uc11c ","33bf6d82":"# 0\uacfc 1\ub85c \ub098\ub204\uc5b4 \uc801\uc6a9\ndef count_dummy(x):\n   # ```\n   # 0\uacfc 1\ub85c \ub098\ub220\uc8fc\ub294 \ud568\uc218 \n   # ```\n  if x > 0:\n    return 1\n  else:\n    return 0","323ca16b":"all_df['PoolArea'] = all_df['PoolArea'].apply(count_dummy)\nall_df['PoolArea'].value_counts()\n\n# \uc804\uccb4 \uacbd\ud5a5 \ub4f1\uc5d0 \uac70\uc758 \uc601\ud5a5\uc744 \uc8fc\uc9c0 \uc54a\uc74c ","254316fd":"all_df['GarageArea'] = all_df['GarageArea'].apply(count_dummy)\nall_df['GarageArea'].value_counts()","f3b551b1":"all_df['Fireplaces'] = all_df['Fireplaces'].apply(count_dummy)\nall_df['Fireplaces'].value_counts()","d57de7e6":"from sklearn.preprocessing import LabelEncoder\nimport pandas as pd\n\ntemp = pd.DataFrame({'Food_Name': ['Apple', 'Chicken', 'Broccoli'], \n                     'Calories': [95, 231, 50]})\n\nencoder = LabelEncoder()\nencoder.fit(temp['Food_Name'])\nlabels = encoder.transform(temp['Food_Name'])\nprint(list(temp['Food_Name']), \"==>\", labels)","67515f77":"from sklearn.preprocessing import OrdinalEncoder\nimport pandas as pd\n\ntemp = pd.DataFrame({'Food_Name': ['Apple', 'Chicken', 'Broccoli'], \n                     'Calories': [95, 231, 50]})\n\nencoder = OrdinalEncoder()\nlabels = encoder.fit_transform(temp[['Food_Name']])\nprint(list(temp['Food_Name']), \"==>\", labels.tolist())\n\n","5276e870":"# import pandas as pd\n# temp = pd.DataFrame({'Food_Name': ['Apple', 'Chicken', 'Broccoli'], \n#                      'Calories': [95, 231, 50]})\n\n# temp[['Food_No']] = temp.Food_Name.replace(['Chicken', 'Broccoli', 'Apple'],[1, 2, 3])\n\n# print(temp[['Food_Name', 'Food_No']])\n\n#ValueError: Columns must be same length as key\n","3585532e":"import pandas as pd\n\ntemp = pd.DataFrame({'Food_Name': ['Apple', 'Chicken', 'Broccoli'], \n                     'Calories': [95, 231, 50]})\n\ntemp = pd.get_dummies(temp)\nprint(temp)\nprint(temp.shape)","04c489e2":"all_df = pd.get_dummies(all_df).reset_index(drop=True)\nall_df.shape","eae1db5a":"X = all_df.iloc[:len(y), :] #Train data \uc758 \uac2f\uc218 \nX_test = all_df.iloc[len(y):, :] #\uc2dc\uc791\uc810 \ubd80\ud130 \ub098\uba38\uc9c0 \nX.shape, y.shape, X_test.shape","a28c0e9f":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.27, random_state = 0)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","3dad3da0":"#\uc218\ub3d9\uc73c\ub85c \uc4f0\ub294 MAE : \nimport numpy as np\n\ndef mean_absolute_error(y_true, y_pred):\n\n  error = 0\n  for yt, yp in zip(y_true, y_pred):\n    error = error + np.abs(yt-yp)\n  \n  mae = error \/ len(y_true)\n  return mae","4513e944":"import numpy as np\n\ndef mean_squared_error(y_true, y_pred):\n   \n  error = 0\n  for yt, yp in zip(y_true, y_pred):\n    error = error + (yt - yp) ** 2\n  \n  mse = error \/ len(y_true)\n  return mse","e70a6e9e":"from sklearn.metrics import mean_squared_error\ndef rmse (y_true, y_pred):\n    rmse_val = np.sqrt(mean_squared_error(y_true, y_pred))\n    return rmse_val\n    \n#      \"\"\"RMSE \ud568\uc218\n#      AGF : \n#          y_true(list): \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130\uc758 \uc2e4\uc82f\uac12\n#          y_pred(list): \ubaa8\ud615\uc744 \ud1b5\ud574 \uc5bb\uc5b4\uc9c4 \uc608\uce21\uac12\n#     Returns : RMSE_value\n#     \"\"\"\n        \n","6e0284ed":"y_true = [400, 300, 800, 900]\ny_pred = [300, 320, 777, 600]\nrmse(y_true, y_pred)","400e248c":"import numpy as np\n\ndef root_rmse_squared_error(y_true, ypred):\n  error = 0\n  \n  for yt, yp in zip(y_true, y_pred):\n    error = error + (yt - yp) ** 2\n  \n  mse = error \/ len(y_true)\n  rmse = np.round(np.sqrt(mse), 3)\n  return rmse\n\nrmse(y_true, y_pred)","28eacae2":"y_true = [400, 300, 800]\ny_pred = [380, 320, 777]\n\nprint(\"MAE:\", mean_absolute_error(y_true, y_pred))\nprint(\"MSE:\", mean_squared_error(y_true, y_pred))\nprint(\"RMSE:\", root_rmse_squared_error(y_true, y_pred))","e3b18636":"y_true = [400, 300, 800, 900]\ny_pred = [380, 320, 777, 600]\n\nprint(\"MAE:\", mean_absolute_error(y_true, y_pred))\nprint(\"MSE:\", mean_squared_error(y_true, y_pred))\nprint(\"RMSE:\", root_rmse_squared_error(y_true, y_pred))","28adc237":"from sklearn.metrics import mean_squared_error\n\ndef rmsle(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))","91979f32":"from sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold, cross_val_score\n#Kfold : \uad50\ucc28\uac80\uc99d\uc2dc \ud3c9\uac00 \uba54\ud2b8\ub9ad \ud568\uc218 \nfrom sklearn.linear_model import LinearRegression\n#shuffle data \uc11e\uae30\ndef cv_rmse(model, n_folds=5):\n    cv = KFold(n_splits=n_folds, random_state=42, shuffle=True)\n    rmse_list = np.sqrt(-cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=cv))\n    print('CV RMSE value list:', np.round(rmse_list, 4))\n    print('CV RMSE mean value:', np.round(np.mean(rmse_list), 4))\n    return (rmse_list)\n#neg_mean_squared_error = MSE -> RMSE","bd7ef5dd":"n_folds = 5\nrmse_scores = {}\nlr_model = LinearRegression()\nscore = cv_rmse(lr_model, n_folds)\n#n_folds : \uc5ec\uae30\uc11c \uc870\uc815 \uac00\ub2a5 \nprint(\"linear regression - mean: {:.4f} (std: {:.4f})\".format(score.mean(), score.std()))\nrmse_scores['linear regression'] = (score.mean(), score.std())","d604631b":"from sklearn.model_selection import cross_val_predict\n\nX = all_df.iloc[:len(y), :]\nX_test = all_df.iloc[len(y):, :]\nX.shape, y.shape, X_test.shape\n\nlr_model_fit = lr_model.fit(X, y)\n\n# y \ub85c\uadf8\uac12\uc73c\ub85c \ub4e4\uc5b4\uac00\uc11c np.expml : \ub2e4\uc2dc \uc6d0\ub798 data \ud615\ud0dc\ub85c \ub418\uac8c \ud574\uc90c\nfinal_preds = np.floor(np.expm1(lr_model_fit.predict(X_test)))\nprint(final_preds)","262a4036":"submission = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\nsubmission.iloc[:,1] = final_preds\nprint(submission.head())\nsubmission.to_csv(\"submission.csv\", index=False)","01a7237a":"from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LinearRegression\n\n# LinearRegresison\nlr_model = LinearRegression()\n\n# Tree Decision \ntree_model = DecisionTreeRegressor()\n\n# Random Forest Regressor\nrf_model = RandomForestRegressor()\n\n# Gradient Boosting Regressor\ngbr_model = GradientBoostingRegressor()","8c1ad840":"score = cv_rmse(lr_model, n_folds)\nprint(\"linear regression - mean: {:.4f} (std: {:.4f})\".format(score.mean(), score.std()))\nrmse_scores['linear regression'] = (score.mean(), score.std())","c349e0c1":"score = cv_rmse(tree_model, n_folds)\nprint(\"Decision Tree Regressor - mean: {:.4f} (std: {:.4f})\".format(score.mean(), score.std()))\nrmse_scores['Decision Tree Regressor'] = (score.mean(), score.std())","621f9b68":"score = cv_rmse(rf_model, n_folds)\nprint(\"RandomForest Regressor - mean: {:.4f} (std: {:.4f})\".format(score.mean(), score.std()))\nrmse_scores['RandomForest Regressor'] = (score.mean(), score.std())","7653adc6":"score = cv_rmse(gbr_model, n_folds)\nprint(\"Gradient Boosting Regressor - mean: {:.4f} (std: {:.4f})\".format(score.mean(), score.std()))\nrmse_scores['Gradient Boosting Regressor'] = (score.mean(), score.std())","c03b4995":"fig, ax = plt.subplots(figsize=(10, 6))\n\nax = sns.pointplot(x=list(rmse_scores.keys()), y=[score for score, _ in rmse_scores.values()], markers=['o'], linestyles=['-'], ax=ax)\nfor i, score in enumerate(rmse_scores.values()):\n    ax.text(i, score[0] + 0.002, '{:.4f}'.format(score[0]), horizontalalignment='left', size='large', color='black', weight='semibold')\n\nax.set_ylabel('Score (RMSE)', size=20, labelpad=12.5)\nax.set_xlabel('Model', size=20, labelpad=12.5)\nax.tick_params(axis='x', labelsize=13.5, rotation=10)\nax.tick_params(axis='y', labelsize=12.5)\nax.set_ylim(0, 0.25)\nax.set_title('Rmse Scores of Models without Blended_Predictions', size=20)\n\nfig.show()","525fb415":"lr_model_fit = lr_model.fit(X, y)\ntree_model_fit = tree_model.fit(X, y)\nrf_model_fit = rf_model.fit(X, y)\ngbr_model_fit = gbr_model.fit(X, y)\n\ndef blended_learning_predictions(X): \n  blended_score = (0.3 * lr_model_fit.predict(X)) + \\\n  (0.1 * tree_model_fit.predict(X)) + \\\n  (0.3 * gbr_model_fit.predict(X)) + \\\n  (0.3* rf_model_fit.predict(X))\n  return blended_score","cde2bde4":"blended_score = rmsle(y, blended_learning_predictions(X))\nrmse_scores['blended'] = (blended_score, 0)\nprint('RMSLE score on train data:')\nprint(blended_score)","5f7c965d":"fig, ax = plt.subplots(figsize=(10, 6))\n\nax = sns.pointplot(x=list(rmse_scores.keys()), y=[score for score, _ in rmse_scores.values()], markers=['o'], linestyles=['-'], ax=ax)\nfor i, score in enumerate(rmse_scores.values()):\n    ax.text(i, score[0] + 0.002, '{:.4f}'.format(score[0]), horizontalalignment='left', size='large', color='black', weight='semibold')\n\nax.set_ylabel('Score (RMSE)', size=20, labelpad=12.5)\nax.set_xlabel('Model', size=20, labelpad=12.5)\nax.tick_params(axis='x', labelsize=13.5, rotation=10)\nax.tick_params(axis='y', labelsize=12.5)\nax.set_ylim(0, 0.25)\n\nax.set_title('Rmse Scores of Models with Blended_Predictions', size=20)\n\nfig.show()","fcbf7f5d":"submission.iloc[:,1] = np.floor(np.expm1(blended_learning_predictions(X_test)))\nsubmission.to_csv(\"The_second_regression.csv\", index=False)","4e28a9d4":"## \uacb0\uce21\uce58 \ud655\uc778 \n- \uacb0\uce21\uce58 \ucc98\ub9ac \n    1. \uc81c\uac70\ud558\uae30 : column \uc81c\uac70, \ud2b9\uc815 \ud589\ub9cc \uc81c\uac70\ud558\uae30 \n    2. \ucc44\uc6b0\uae30 : 1) numeric(\uc218\uce58\ud615) : \ud3c9\uade0 \ub610\ub294 \uc911\uac04\uac12\uc73c\ub85c  \ucc44\uc6b0\uae30 \n               2) String(\ubb38\uc790\ud615) : \ucd5c\ube48\uac12\uc73c\ub85c \ucc44\uc6b0\uae30 \n    3. \ud1b5\uacc4 \uae30\ubc95\uc774\uc6a9, \ucc44\uc6b0\uae30 **(data \ubcf4\uac04)**\n        - \uc2e4\ubb34\uc5d0\uc11c\ub294 (KNNImput)\ub4f1, \uc2dc\uacc4\uc5f4 \uc790\ub8cc or \uc0b0\uc5c5\uad70\uc5d0 \ub530\ub77c \ub2e4\ub974\ubbc0\ub85c \uac00\uc11c \ubc30\uc6cc\ub77c.\n        - ","02f027a5":"### \ud3c9\uac00\uc9c0\ud45c\n#### MAE","3ea7ae15":"## \ub370\uc774\ud130 \ub2e4\uc6b4\ub85c\ub4dc \ubc0f \ubd88\ub7ec\uc624\uae30\n","37900fba":"#### RMSE with Sklean","52d8a42e":"### \ub3c4\ucd9c \ubcc0\uc218\n\nFeature Engineering \uc758 Key step\n- \ud310\ub9e4\ub7c9, \ub2e8\uac00, *\ub9e4\ucd9c\uc561 X*\n- \ud310\ub9e4\ub7c9 X \ub2e8\uac00 = \ub9e4\ucd9c\uc561(New Value) : \ub3c4\ucd9c \ubcc0\uc218\n    + ML\uc740 \uc218\uc2dd\uc774\uae30 \ub54c\ubb38\uc5d0 \ub3c4\ucd9c\ubcc0\uc218\uac00 \uc0dd\uc131 \ub418\ub294\uac83\uc740 \uc5f0\uc0b0\uc758 \uc99d\uac00\ub85c \uc774\uc5b4\uc9c4\ub2e4. \n    + \uc2dc\uac04\uc774 \uc624\ub798 \uac78\ub9b0\ub2e4.\n    + \uacb0\ub860 : \ubcc0\uc218\ub97c \uc904\uc774\ub294 \uac83\uc774 \uc88b\ub2e4. \n    ","4f77bb2f":"## EDA\n - \uc774\uc0c1\uce58\uacfc \uc911\ubcf5\uac12 \uc81c\uac70\n - overallQual (\uc8fc\ud0dd\uc758 \uc0c1\ud0dc\ub97c 1~10\ub4f1\uae09\uc73c\ub85c \ucc45\uc815)\n - \ud3c9\uc810 1 : \ud310\ub9e4\uac00\uac00 \ub192\uc74c = \uc774\uc0c1\uce58\ub77c\uace0 \ud310\ub2e8 \ud560 \uc218 \uc788\ub530. : \uc774\uac78 \uc81c\uac70 \ud574 \uc918\uc57c \ud568.\n ","6bcd860d":"## \uba38\uc2e0\ub7ec\ub2dd \ubaa8\ud615 \ud559\uc2b5 \ubc0f \ud3c9\uac00\n\n#### \ub370\uc774\ud130\uc14b \ubd84\ub9ac \ubc0f \uad50\ucc28 \uac80\uc99d","3436fb8c":"### \uc65c\ub3c4(Skewnewss) \ucc98\ub9ac\ud558\uae30 : \uc815\uaddc \ubd84\ud3ec\ub97c \uc774\ub8f0 \uc218 \uc788\uac8c (\uc124\ubb38\uc870\uc0ac \ub17c\ubb38 \ud1b5\uacc4\uc758 \uacbd\uc6b0 -1< \uc678\ub3c4 <1)\n\n- boxcose\ub97c \uc0ac\uc6a9 \ud560 \uc608\uc815\n- \uc65c\ub3c4\uac00 \uc591\uc218\uc77c\ub54c, \uc74c\uc218\uc77c\ub54c (\uc88c, \uc6b0\ub85c \uce58\uc6b0\uce5c \uc815\ub3c4)\n- \ucca8\ub3c4\uac00 \uc591\uc218\uc77c\ub54c, \uc74c\uc218\uc77c\ub54c (\ubf40\uc871\ud55c \uc815\ub3c4)\n\n- RMSE\ub97c \ucd5c\uc801(\ub0ae\uac8c)\uc73c\ub85c \ub9cc\ub4e4\uae30 \uc704\ud574 \uc870\uc815.","d2816e93":"### \ubaa8\ud615 \uc54c\uace0\ub9ac\uc998 \ucd94\uac00","e4422ad5":"## data feature \uc81c\uac70\n1. \ubaa8\ud615 \ud559\uc2b5 \uc2dc\uac04 \uac10\uc18c \n2. \uc5f0\uc0b0 \uc2dc noise \uac10\uc18c\n\n\n\uadf8\ub798\uc11c : train ID \ub97c \ube7c\uc8fc\uae30\ub85c\ud568 ","779fa1f0":"- RMSE \uac00 \uc801\uc740 \uac83\uc774 \uc88b\ub2e4. : \uc608\uce21\uc774 \uc798 \ub41c Model\uc774\ub77c\uace0 \ud560 \uc218 \uc788\ub2e4. ","b6f9b7c3":"- \ub530\ub77c\uc11c data \uc815\uc758\uc11c \uba3c\uc800 \ubd10\uc57c \ud55c\ub2e4. : data description.txt\ub97c \uba3c\uc800 \ubd10\uc57c \ud55c\ub2e4. !!! (\uc2e4\ubb34\uc5d0\uc11c\ub294 \uc5c6\ub294 \uacbd\uc6b0\uac00 \ub9ce\ub2e4.) \n- \uc2dc\uac01\ud654 : \uac01\uac01\uc758 data \ubb34\ud55c\uc791\uc5c5, \ub3c4\uba54\uc778 \uacf5\ubd80 \n- ","4703baf3":"## \ucc44\uc6b0\uae30 \n\n    1. \ubb38\uc790\uc5f4 \ucc44\uc6b0\uae30 \n    2. \n    \n1. object column \ucd94\ucd9c ","d6ebd216":"## \uc885\uc18d\ubcc0\uc218 \uc2dc\uac01\ud654","49bddbd4":"#### RMSE\n\n- RMSE \ub294 \ub9cc\ub4e4\uc5b4 \uc8fc\uc9c0 \uc54a\ub294\ub2e4. ","ad15596e":"## \ub354\ubbf8\ubcc0\uc218 \n\n### String data (non-Numeric) \n\n-  \uba85\ubaa9\ud615      : \ub0a8\ud559\uc0dd, \uc5ec\ud559\uc0dd...  \n-  \uc11c\uc5f4\ud615(\uc21c\uc11c) : 1\ub4f1\uae09, 2\ub4f1\uae09, 3\ub4f1\uae09 (\uac00\uc911\uce58, \ub4f1\uae09\uc22b\uc790 \ub4f1\uc73c\ub85c \ubc14\uafc0 \uc218 \uc788\ub2e4. )\n\n![image.png](attachment:b795960f-cac3-4486-9bcf-502ce4e4bb84.png)\n\n\n- \uc138\ubd80\uc801\uc73c\ub85c customize \ud558\ub294 \uac83\uc774 \ub0ab\ub2e4. \n- \uba85\ubaa9\ud615 series\uc5d0 \ub530\ub77c 17\uac1c\uc758  model\uc744 \uac1c\ubcc4\uc801\uc73c\ub85c  \ub9cc\ub4e4\ub418, \ud558\ub098\uc758 \ubaa8\ub378\ucc98\ub7fc \ubcf4\uc774\uac8c \uc2dc\uac01\ud654 \ub300\uc2dc\ubcf4\ub4dc\ub85c \ub9cc\ub4e4\uc5b4 \uc918\uc57c \ud569\ub2c8\ub2e4. \n","cc62d54d":"#### Test1","511f95c5":"### \ubaa8\ud615 \uc815\uc758 \ubc0f \uac80\uc99d \ud3c9\uac00\n#### \uad50\ucc28\uac80\uc99d\uc744 \uc704\ud55c cv_rmse() \ud568\uc218\n\n- \ubaa8\ud615\uc758 \uc548\uc815\uc131 : \uc99d\uac00   \/ \uadf8\ub7ec\ub098 \uc2dc\uac04\uc774 \ub9ce\uc774 \uac78\ub824\uc11c \uc2e4\ubb34\uc5d0\uc11c \uc4f0\uae30 \ubab9\uc2dc \ud798\ub4e4\ub2e4. \n- \uac80\uc99ddt :1\/5 (random\uc73c\ub85c \uc870\uac01\uc744 \ubf51\ub294\ub2e4 \/5\uac1c\uc911)\n- \ud6c8\ub828dt : 1\/5 *4\n\n    - RMSE1 : 2.0\n    - ....\n    - RMSE5 : 1.5\n        = EVR RMSE : 1.7\n\n= LinearModels \ud3c9\uade0 RMSE 1.7 : ","a974f4ad":"### \uccab\ubc88\uc9f8 \ucd5c\uc885 \uc608\uce21 \uac12 \uc81c\ucd9c","541391e7":"### Label Encoding, Ordinal Encoding, One-Hot Encoding\n\n- Label Encoding : \uc885\uc18d\ubcc0\uc218\uc5d0\ub9cc \n- Ordinal Encoding : \ub3c5\ub9bd\ubcc0\uc218\uc5d0\ub9cc\n- \uc368\uc57c \ud558\uc9c0\ub9cc, \uac1c\ub150\uc740 \uac19\ub2e4. \n- One-Hot Encoding :","4bc62d59":"#### Test2"}}