{"cell_type":{"fa630fd0":"code","0116377c":"code","2001a96d":"code","62d4fb0b":"code","629163ca":"code","8006db58":"code","e2bba4b6":"code","fb3dc88d":"code","717093ef":"code","b4e4936e":"code","31093232":"code","90be5cb2":"code","e0a41725":"code","d78cb440":"code","bd659942":"markdown","b361b9dc":"markdown","35be483a":"markdown","0ffc15d6":"markdown","656625f0":"markdown","287335e5":"markdown","30d460e0":"markdown","ef4e1462":"markdown","af4ac671":"markdown","6eef2f6b":"markdown","6dda58f6":"markdown"},"source":{"fa630fd0":"!pip install spacy_cld","0116377c":"import numpy as np \nimport pandas as pd\nimport re\nimport spacy\nfrom spacy_cld import LanguageDetector\n\n\nimport langdetect\nimport langid\n\nimport os\nprint(os.listdir(\"..\/input\"))","2001a96d":"# function for data cleaning..\ndef remove_xml(text):\n    return re.sub(r'<[^<]+?>', '', text)\n\ndef remove_newlines(text):\n    return text.replace('\\n', ' ') \n    \n\ndef remove_manyspaces(text):\n    return re.sub(r'\\s+', ' ', text)\n\ndef clean_text(text):\n    text = remove_xml(text)\n    text = remove_newlines(text)\n    text = remove_manyspaces(text)\n    return text","62d4fb0b":"df = pd.read_csv('..\/input\/data.csv')\ndf.head()","629163ca":"#Loading dataset  in tweets.\ntweets    = df['tweets']","8006db58":"for line in tweets:\n    line=clean_text(line)\ndf.head()","e2bba4b6":"\"\"\"\nresult = str(result[0])[:2] : keeping the most dominant language wich is situated \nin the 1st index, and we store the first 2 characters\n\"\"\"\n\nlanguages_langdetect = []\n\n# the try except blook because there is some tweets contain links\nfor line in tweets:\n    try:\n        result = langdetect.detect_langs(line)\n        result = str(result[0])[:2]\n    except:\n        result = 'unknown'\n    \n    finally:\n        languages_langdetect.append(result)","fb3dc88d":"nlp = spacy.load('en')\nlanguage_detector = LanguageDetector()\nnlp.add_pipe(language_detector)","717093ef":"\"\"\"\ndoc._.languages returns : list of str\nlike : ['fr'] -> french\n       ['en'] -> english\n       [] -> empty\n       ['fr','en'] -> french (the most dominant in a tweet) and english (least dominant)\n\"\"\"\n\ntweets          = df['tweets']\nlanguages_spacy = []\n\nfor e in tweets:\n    doc = nlp(e)\n    # cheking if the doc._.languages is not empty\n    # then appending the first detected language in a list\n    if(doc._.languages):\n        languages_spacy.append(doc._.languages[0])\n    # if it is empty, we append the list by unknown\n    else:\n        languages_spacy.append('unknown')","b4e4936e":"df['languages_spacy'] = languages_spacy\ndf['languages_langdetect'] = languages_langdetect","31093232":"df.head()","90be5cb2":"df['languages_spacy'].value_counts()","e0a41725":"df['languages_langdetect'].value_counts()","d78cb440":"df.to_csv('Detected_Languages.csv',index=False)","bd659942":"Here is the view of samll portion of cleaned dataset taken for language detection.\n#### the code below is needed in the next sections :","b361b9dc":"# <a id=\"c\">3. Implementation with langdetect <\/a>","35be483a":"# <a id=\"a\">1. Needed libraries<\/a>","0ffc15d6":"### Adding a column in the dataframe containing the language of the tweet","656625f0":"<a href=\"#a\">1. Needed Libraries<\/a><br>\n<a href=\"#b\">2. Dataset<\/a><br>\n<a href=\"#c\">3. Implementation with langdetect<\/a><br>\n<a href=\"#d\">4. Implementation with spacy<\/a><br>\n<a href=\"#e\">5. Conclusion<\/a><br>","287335e5":"# <a id=\"b\">2. Dataset<\/a>","30d460e0":"Cleaning the Data in the dataset by removing XML, extra spaces, and newlines.","ef4e1462":"# <a id=\"e\">5. Conclusion<\/a>\n<li>spacy returns 1582 en tweets <\/li>\n<li>langdetect returns 1018 en tweets<\/li>","af4ac671":"* **tweets :** denotees the tweets of the dataset\n* **languages_spacy :** It shows the sentence which were found to be in english (en) by spacy model.\n* **languages_langdetect :** It shows the language of the sentence.","6eef2f6b":"# <a id=\"d\">4. Implementation with spacy <\/a>","6dda58f6":"# Language Detection using spacy\n<li> our main goal is to keep english tweets collected from twitter about Devoxx France event<\/li>\n<li> the tweets in our dataset are expressed in many languages such as french, english... <\/li>\n<li> Here we are using per trained [spaCy models](https:\/\/spacy.io\/) to detect lenguage of the tweets.<\/li>"}}