{"cell_type":{"30159dd9":"code","c4200f97":"code","b7f38966":"code","9960a0d8":"code","8f9f9cd9":"code","739cbc93":"code","ed2be504":"code","c3e619f9":"code","b55a0105":"code","10f81287":"code","0b9c492a":"code","aef7509b":"code","edd0d4ed":"code","3cd132d9":"code","49a33348":"code","a14adc39":"code","0ca75b2a":"code","fa8aae29":"code","4cb3f75b":"code","b4b62330":"code","2abf509e":"code","188671c0":"code","15f143a4":"code","8fe98b23":"code","c42329de":"code","c2b1138a":"code","8f263e1e":"code","968537d4":"code","28318507":"markdown","c686751a":"markdown","6a7f0272":"markdown","0c111ac1":"markdown","f1643149":"markdown","a200674b":"markdown","3807ffc6":"markdown","c4eca840":"markdown","fbd974cf":"markdown","5ee3af28":"markdown"},"source":{"30159dd9":"# Import the required tools\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nimport plotly.express as px\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\nimport spacy\nimport re\nimport pyLDAvis.gensim\nfrom wordcloud import WordCloud\nimport warnings\nwarnings.filterwarnings(\"ignore\")","c4200f97":"train = pd.read_csv(\"\/kaggle\/input\/tweet-sentiment-extraction\/train.csv\")","b7f38966":"train.head()","9960a0d8":"# Check for data imbalance\nsns.countplot(train[\"sentiment\"]);","8f9f9cd9":"# Check for missing values\ntrain.isna().sum()","739cbc93":"train.dropna(inplace=True)\ntrain = train.reset_index(drop=True)","ed2be504":"def clean_punc(text):\n  cleaned_text = re.sub(r'[^\\w\\s]', '', text)\n  cleaned_text = re.sub(r'[0-9]', r'', cleaned_text)\n  return cleaned_text","c3e619f9":"train[\"cleaned_text\"] = np.vectorize(clean_punc)(train[\"text\"])","b55a0105":"# Remove stopwords\nstop_words = set(stopwords.words(\"english\"))\ntrain[\"cleaned_text\"] = train[\"cleaned_text\"].apply(lambda x : \" \".join([w.lower() for w in x.split() if w not in stop_words and len(w) > 3]))","10f81287":"texts = train[\"cleaned_text\"].apply(lambda x : x.split())","0b9c492a":"lemma = WordNetLemmatizer()\nlemming_texts = texts.apply(lambda x:[lemma.lemmatize(i) for i in x])\nlemming_texts","aef7509b":"for i in range(len(lemming_texts)):\n  lemming_texts[i] = \" \".join(lemming_texts[i])\n\ntrain[\"cleaned_text\"] = lemming_texts","edd0d4ed":"train","3cd132d9":"positive_text = train[\"cleaned_text\"][train[\"sentiment\"]==\"positive\"].apply(lambda x : x.split())\nnegative_text = train[\"cleaned_text\"][train[\"sentiment\"]==\"negative\"].apply(lambda x: x.split())\nneutral_text = train[\"cleaned_text\"][train[\"sentiment\"]==\"neutral\"].apply(lambda x:x.split())","49a33348":"# Merge all the lists into one common list\npositive_text = sum(positive_text, [])\nnegative_text = sum(negative_text, [])\nneutral_text = sum(neutral_text, [])","a14adc39":"# Plot the most common words in positive sentiment\nfreq_pos = nltk.FreqDist(positive_text)\npos_df = pd.DataFrame({\n    \"words\":list(freq_pos.keys()),\n    \"Count\":list(freq_pos.values())\n})\ncommon_pos = pos_df.nlargest(columns=\"Count\", n=30)\nfig = px.bar(common_pos, x=\"words\", y=\"Count\", labels={\"words\": \"Words\", \"Count\":\"Frequency\"})\nfig.show();","0ca75b2a":"# Plot the most common words in negative sentiment\nfreq_neg = nltk.FreqDist(negative_text)\nneg_df = pd.DataFrame({\n    \"words\":list(freq_neg.keys()),\n    \"Count\":list(freq_neg.values())\n})\ncommon_neg = neg_df.nlargest(columns=\"Count\", n=30)\nfig = px.bar(common_neg, x=\"words\", y=\"Count\", labels={\"words\": \"Words\", \"Count\": \"Frequency\"})\nfig.show();","fa8aae29":"# Plot the most common words in neutral sentiment\nfreq_ntl = nltk.FreqDist(neutral_text)\nntl_df = pd.DataFrame({\n    \"words\":list(freq_ntl.keys()),\n    \"Count\":list(freq_ntl.values())\n})\ncommon_ntl = ntl_df.nlargest(columns=\"Count\", n=30)\nfig = px.bar(common_ntl, x=\"words\", y=\"Count\", labels={\"words\": \"Words\", \"Count\":\"Frequency\"})\nfig.show();","4cb3f75b":"# Most common words in the texts\nwords = \" \".join([text for text in train[\"cleaned_text\"]])\nwordclouds = WordCloud(width=900, height=600, random_state=42, max_font_size=110).generate(words)\n\nplt.figure(figsize=(10, 10))\nplt.imshow(wordclouds, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","b4b62330":"# Wordcloud for positive words\npositive_words = \" \".join([word for word in train[\"cleaned_text\"][train[\"sentiment\"] == \"positive\"]])\nwordcloud_pos = WordCloud(width=900, height=700, random_state=42, max_font_size=100).generate(positive_words)\n\nplt.figure(figsize=(10, 10))\nplt.imshow(wordcloud_pos)\nplt.axis(\"off\")\nplt.show()","2abf509e":"nlp = spacy.load(\"en_core_web_sm\")","188671c0":"ners = []\nfor text in train[\"cleaned_text\"].values:\n  doc = nlp(text)\n\n  for entity in doc.ents:\n    ners.append((entity.text, entity.label_))","15f143a4":"name_entity_df = pd.DataFrame(ners, columns = [\"Entity Name\", \"Entity Label\"])\nentity_df = name_entity_df.groupby(by=[\"Entity Name\", \"Entity Label\"]).size().sort_values(ascending=False).reset_index().rename(columns = {0: \"Frequency\"})","8fe98b23":"# Plot the first 50 entities\nfigure = px.bar(x=entity_df[\"Entity Label\"][:50], y=entity_df[\"Frequency\"][:50]) \nfigure.show()","c42329de":"texts = train[\"cleaned_text\"].apply(lambda x: x.split())\ntexts.head()","c2b1138a":"# Build the dictionary of words from the document\nfrom gensim import corpora\ndictionary = corpora.Dictionary(texts)\ncorpus = [dictionary.doc2bow(text) for text in texts]","8f263e1e":"# Build the LDA Model \nimport gensim\ntopics = 10\nlda = gensim.models.ldamodel.LdaModel(corpus, num_topics=topics, id2word=dictionary, passes=15)\nlda.save(\"LDA_model.gensim\")","968537d4":"display = pyLDAvis.gensim.prepare(lda, corpus, dictionary)\npyLDAvis.display(display)","28318507":"## Tokenize the tweet","c686751a":"## Lemmatizing the text","6a7f0272":"## Name Entity Recognition","0c111ac1":"## Preprocessing","f1643149":"## Topic Modelling","a200674b":"## Wordcloud","3807ffc6":"## Data Visualization","c4eca840":"> Topic Modelling is the task of extracting the main topics from the documents.\n\n>For this task LDA is used. LDA is used to classify the text in a document to a particular topic","fbd974cf":"# EDA \n> This notebook contains preprocessing and visualization of the texts in the data.\n\n> For the visualization spacy, gensim and pyLDAvis is used.\n\n> Spacy: Spacy is an library which is used for advance natural processing. It also contains tokenization, Name Entity Recognition(NER), Parts of Speech tagging(POS), etc\n\n> Gensim: Gensim is used for unsupervised topic modelling and  finding similarity.\n\n> pyLDAviz: It is used to visualize the topic in the LDA model that has been fitted to large corpus. It creates clusters of similar topics. Two or more clusters overlapping means that they are similar. You can change the number of clusters if you want.","5ee3af28":"## Remove punctuations and numbers from the data"}}