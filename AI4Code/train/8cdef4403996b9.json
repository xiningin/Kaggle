{"cell_type":{"189998ea":"code","55d302e7":"code","2440fa37":"code","e0f3f9fb":"code","6068050e":"code","3f3b41d7":"code","c6350e2a":"code","a0eecbda":"code","320b9146":"code","5c99ee5c":"code","936748e0":"code","070132d2":"code","e91bd718":"code","f8fd0de5":"code","d370f0fc":"code","d6ab49cf":"code","f1a9bd54":"code","f38c9037":"code","4b315858":"code","d6717ada":"code","9127ae16":"code","f39f5dbf":"code","ad54f07a":"code","8bfe38ca":"code","46145b7d":"code","02135384":"code","00a7674a":"code","3d8d53b0":"code","14bf084d":"code","99e9ce60":"code","dd1fc911":"code","ba2ba2e1":"code","75ba295d":"code","2a09fb38":"code","0d24b202":"code","f88fdeaa":"code","1e099fe9":"markdown","96bf146c":"markdown","0ee2b421":"markdown","22d14df9":"markdown","31f308c4":"markdown","a77d8f3f":"markdown","9fdd0a0d":"markdown","5056aec3":"markdown","6db0e932":"markdown","2a7e5ce2":"markdown","d5a705ba":"markdown","65f4cb26":"markdown","8c2f8e98":"markdown","47395a76":"markdown","7db7fcb3":"markdown","c8d68a5a":"markdown","39e31489":"markdown","72048e3d":"markdown","f85cc6d9":"markdown","ed67594e":"markdown"},"source":{"189998ea":"def lower_text(text):\n  text = text.lower()\n  return text","55d302e7":"text = \"India is the second most populated country after China and the fourth largest economy after China  other two are USA, Russia.\"\noutput_str = lower_text(text)\nprint(output_str)","2440fa37":"import re\n\ndef remove_nembers(text):\n  input_str = re.sub(r'\\d+', \"\", input)\n  return input_str","e0f3f9fb":"text = \"There are 20 stairs for the first floor while 21 for the second floor but in total there are 44 stairs how ?\"\ninput_str = remove_nembers(text)\ninput_str","6068050e":"def remove_Punctuation(string): \n  \n    punctuations = '''!()-[]{};:'\"\\,<>.\/?@#$%^&*_~'''\n    for x in string.lower(): \n        if x in punctuations: \n            string = string.replace(x, \"\") \n    return string","3f3b41d7":"input_str = \"This &is [an] example? {of} string. with.? punctuation!!!!\" # Sample string\noutput_str =  remove_Punctuation(input_str) \noutput_str","c6350e2a":"def remove_whitespace(input_str):\n    input_str = input_str.strip()\n    \n    return input_str","a0eecbda":"text = \" \\t a string example\\t \"\noutput_str = remove_whitespace(text)\noutput_str","320b9146":"\nimport nltk\nnltk.download('punkt')\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\ndef remove_stopwords(text):\n  stop_words = set(stopwords.words('english'))\n  tokens = word_tokenize(text)\n  result = [i for i in tokens if not i in stop_words]\n  return \" \".join(result)","5c99ee5c":"\ninp_str = \"NLTK is a leading platform for building Python programs to work with human language data.\"\nopt_str = remove_stopwords(inp_str)\nprint(opt_str)","936748e0":"from nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize\n\ndef stemming(text):\n  tokens = []\n  stemmer= PorterStemmer()\n  tokenize_word=word_tokenize(text)\n  for word in tokenize_word:\n      tokens.append(stemmer.stem(word))\n  return \" \".join(tokens)","070132d2":"input_str=\"There are several types of stemming algorithms.\"\nstemming(input_str) ","e91bd718":"from nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nimport nltk\nnltk.download('wordnet')\n\ndef lemmatization(text):\n  tokens = []\n  lemmatizer=WordNetLemmatizer()\n  input_str=word_tokenize(text)\n  for word in input_str:\n      tokens.append(lemmatizer.lemmatize(word))\n  return \" \".join(tokens)","f8fd0de5":"text=\"been had done languages cities mice\"\noutput_str = lemmatization(text)\noutput_str","d370f0fc":"import re\n\ndef cleaning_html_tags(raw_html):\n  cleanr = re.compile('<.*?>')\n  cleantext = re.sub(cleanr, '', raw_html)\n  return cleantext","d6ab49cf":"text = \"The tag is <html> <\/html> removed\"\ncleaning_html_tags(text)","f1a9bd54":"import re\n\ndef remove_urls(text):\n    url_pattern = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url_pattern.sub(r'', text)","f38c9037":"text  = \"https:\/\/github.com\/KnitVikas\"\noutput_utls = remove_urls(text)\noutput_utls","4b315858":"text = \"text does not https:\/\/github.com\/KnitVikas coantain any urls\"\noutput_text = remove_urls(text)\noutput_text # so this is still removing only the urls fro the text","d6717ada":"import re\n\ndef remove_emojis(text):\n  print(text) # with emoji\n  emoji_pattern = re.compile(\"[\"\n          u\"\\U0001F600-\\U0001F64F\"  # emoticons\n          u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n          u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n          u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                            \"]+\", flags=re.UNICODE)\n  return emoji_pattern.sub(r'', text)","9127ae16":"text = u'This dog \\U0001f602'\noutput_text = remove_emojis(text)\noutput_text","f39f5dbf":"def remove_emoticons(text):\n    emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in EMOTICONS) + u')')\n    return emoticon_pattern.sub(r'', text)","ad54f07a":"remove_emoticons(\"Hello :-)\")","8bfe38ca":"def convert_emojis(text):\n    for emot in UNICODE_EMO:\n        text = re.sub(r'('+emot+')', \"_\".join(UNICODE_EMO[emot].replace(\",\",\"\").replace(\":\",\"\").split()), text)\n    return text","46145b7d":"text = \"game is on \ud83d\udd25\"\nconvert_emojis(text)","02135384":"def convert_emoticons(text):\n    for emot in EMOTICONS:\n        text = re.sub(u'('+emot+')', \"_\".join(EMOTICONS[emot].replace(\",\",\"\").split()), text)\n    return text","00a7674a":"text = \"Hello :-) :-)\"\nconvert_emoticons(text)","3d8d53b0":"import nltk\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nfrom nltk import word_tokenize \nfrom nltk import pos_tag\n\n\ndef pos_tagging(text):\n  text = word_tokenize(text)\n  return nltk.pos_tag(text)","14bf084d":"text = \"Parts of speech examples: an article, to write, interesting, easily, and, of\"\noutput_text = pos_tagging(text)\nprint(output_text)","99e9ce60":"import spacy \nnlp = spacy.load('en_core_web_sm') \n\ndef named_entity_recognition(text):\n  doc = nlp(text) \n  text = \"\"  \n  for ent in doc.ents: \n    text = text + \"\" + ent.text + \"--> \" + ent.label_ +\" \"\n  return text","dd1fc911":"text = \"Apple is looking at buying U.K. startup for $1 billion\"\noutput_str = named_entity_recognition(text)\noutput_str","ba2ba2e1":"# !pip install symspellpy","75ba295d":"import pkg_resources\nfrom symspellpy import SymSpell, Verbosity\n\ndef spellcorrector(text):\n  sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n  dictionary_path = pkg_resources.resource_filename(\n      \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n  bigram_path = pkg_resources.resource_filename(\n      \"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")\n  # term_index is the column of the term and count_index is the\n  # column of the term frequency\n  sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n  sym_spell.load_bigram_dictionary(bigram_path, term_index=0, count_index=2)\n  # lookup suggestions for multi-word input strings (supports compound\n  # splitting & merging)\n  # max edit distance per lookup (per single word, not per whole input string)\n  suggestions = sym_spell.lookup_compound((text), max_edit_distance=2)\n  # display suggestion term, edit distance, and term frequency\n  corrected_text = \"\"\n  for suggestion in suggestions:\n      corrected_text =corrected_text + str(suggestion).split(\",\")[0]\n\n  return corrected_text","2a09fb38":"wrong_text = \"whereis th elove hehad dated forImuch of thepast who couqdn'tread in sixtgrade and ins pired him\"\ncorrect_text = spellcorrector(wrong_text)\ncorrect_text","0d24b202":"import requests\n\ndef chat_words_conversion(slangText):\n  prefixStr = '<div class=\"translation-text\">'\n  postfixStr = '<\/div'\n  r = requests.post('https:\/\/www.noslang.com\/', {'action': 'translate', 'p': \n  slangText, 'noswear': 'noswear', 'submit': 'Translate'})\n  startIndex = r.text.find(prefixStr)+len(prefixStr)\n  endIndex = startIndex + r.text[startIndex:].find(postfixStr)\n  return r.text[startIndex:endIndex]","f88fdeaa":"Text = \"one minute BRB\"\nchat_words_conversion(Text)","1e099fe9":"# Conversion of Emoticon to Words\nIn the previous step, we have removed the emoticons. In case of use cases like sentiment analysis, the emoticons give some valuable information and so removing them might not be a good solution. What can we do in such cases?\nOne way is to convert the emoticons to word format so that they can be used in downstream modeling processes. Thanks for Neel again for the wonderful dictionary that we have used in the previous step. We are going to use that again for conversion of emoticons to words.\n","96bf146c":"# Part of speech tagging (POS)\nPart-of-speech tagging aims to assign parts of speech to each word of a given text (such as nouns, verbs, adjectives, and others) based on its definition and its context. \nThere are many tools containing POS taggers including NLTK, spaCy, TextBlob, Pattern, Stanford CoreNLP, Memory-Based Shallow Parser (MBSP), Apache OpenNLP, Apache Lucene, \n\nread more about it [here](http:\/\/www.nltk.org\/book\/ch05.html) \n","0ee2b421":"let us see what it does if urls are in between the text","22d14df9":"# Lemmatization \nThe aim of lemmatization, like stemming, is to reduce inflectional forms to a common base form. As opposed to stemming, lemmatization does not simply chop off inflections. Instead it uses lexical knowledge bases to get the correct base forms of words. Lemmatization tools are presented libraries described above: NLTK (WordNet Lemmatizer), spaCy, TextBlob, Pattern, gensim, Stanford CoreNLP, Memory-Based Shallow Parser (MBSP), Apache OpenNLP, Apache Lucene, General Architecture for Text Engineering (GATE), Illinois Lemmatizer, and DKPro Core.","31f308c4":"# Chat Words Conversion\n This is an important text preprocessing step if we are dealing with chat data. People do use a lot of abbreviated words in chat and so it might be helpful to expand those words for our analysis purposes.\n Got a good list of chat slang words from this repo. We can use this for our conversion here. We can add more words to this list.\n\n","a77d8f3f":"# Removing urls\nSome times the text may ccontains the url in it. This may be useful for some cases but sometimes we may need to remove the urls for better analysis.\ncan do it again using regular expressions:\n","9fdd0a0d":"# Named entity recognition\nNamed-entity recognition (NER) aims to find named entities in text and classify them into \npre-defined categories (names of persons, locations, organizations, times, etc.).\n\n**Installations:**\n*\npip install spacy\n\n\npython -m spacy download en_core_web_sm*\n*","5056aec3":"# Removal of Emoticons\n This is what we did in the last step right? Nope. We did remove emojis in the last step but not emoticons. There is a minor difference between emojis and emoticons.\n From Grammarist.com, emoticon is built from keyboard characters that when put together in a certain way represent a facial expression, an emoji is an actual image.\n :-) is an emoticon \ud83d\ude00 is an emoji\n Thanks to [NeelShah](http:\/\/github.com\/NeelShah18\/emot\/blob\/master\/emot\/emo_unicode.py) for such a huge collection of emoticons, we are going to use them to remove emoticons.\n","6db0e932":"# Removing Punctuations\nThe following code removes this set of symbols [!\u201d#$%&\u2019()*+,-.\/:;<=>?@[\\]^_`{|}~]:","2a7e5ce2":"# Removal of Whitespaces\nTo remove leading and ending spaces, you can use the strip() function:","d5a705ba":"# Stemming\n\nStemming is a process of reducing words to their word stem, base or root form (for example, books \u2014 book, looked \u2014 look). \nThe main algorithms used for stemming is Porter stemming algorithm.","65f4cb26":"# Conversion of emojis to text\n\nsome times we dont need emojis to removes insteed we want text representeing that emojis.\nNeel Shah has put together a list of emojis with the corresponding words as well as part of his [Github repo ](http:\/\/github.com\/NeelShah18\/emot\/blob\/master\/emot\/emo_unicode.py).We are going to make use of this dictionary\nto convert the emojis to corresponding words.\n","8c2f8e98":"# Lower Casing\n\nThis is the first and formost important step in text cleaning.As before applying any \nother techniques to the text it must be  lower cased.The idea is to convert the input text into same casing format so that 'vikas', 'Vikas' and 'VIKAS' are treated the same way.\n","47395a76":"# Remove Numbers\n\nRemove numbers if these are not relevant to your work. Usually, regular expressions are used to remove numbers.\nsometimes they may be useful it depends on your analysis","7db7fcb3":"****Getting Start**** \n\nPreprocessing the data is the first and formost important step towards machine learning.It becomes more imoprtant \nwhen you have unstructured data like text.\n\nMy objective for this kernel is to cover maximum text preprocessing techniques along with code.\n\nSome of the common text preprocessing steps are as follows:\n\n\n* Lower casing the letters\n* Removal of Numbers\n* Removal of Puntuations\n* Removal of whitespaces\n* Removal of StopWords\n* Stemming\n* Lemmatization\n* Removal of HTML Tags\n* Removal ofURLs\n* Removal of Emojis\n* Removal of Emoticans\n* Conversion of Emojis to text\n* Conversion of Emoticans to text\n* POS Tagging\n* Named Entity Recognition\n* Spelling Correction\n* Chat words conversion","c8d68a5a":"# Removal of Stop Words\n\nstopwords are the unnecessory words which occure frequently in the text. They dont provide insight of text so we need to remove them.\n\nNLTK is one of the most important library used for text cleaning such as for tokenization, ner, pos tagging stopwords removing etc.\nHere is will use it for importing the stopwords. Colud be done in given way:\n\nwe need to tokenize the text first than wll remove stopwords from the obtained tokens .\n","39e31489":"***Reference and further readings***\n\nThis is my first notebook Please upvote and share the notebook if you like it any suggestions are welcomed.\n\nSpecial thanks:    [1.](http:\/\/www.kaggle.com\/sudalairajkumar\/getting-started-with-text-preprocessing)  [2.](http:\/\/www.analyticsvidhya.com\/blog\/2019\/09\/introduction-information-extraction-python-spacy)  [3.](http:\/\/medium.com\/@datamonsters\/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908)\n","72048e3d":"# Removal of html tags\n\nmany of times, text may contains html tags this may generally happens when we scrape text from websites then it may contain htmls tags in it.\nWe may remove them using regular explression presents in them as follows: ","f85cc6d9":"# Removing emojis\n\nsometimes the text may contains emojis text may be from the source such as the chat or comment data may contains emojis.\nthese emojis could be romoved as follows:","ed67594e":"\n# Spelling Correction\nOne another important text preprocessing step is spelling correction. Typos are common in text data and we might want to correct those spelling mistakes before we do our analysis.\n\nIn this notebook, let us use the python package Symspellpy for our spelling correction.\nThis is really interesting spellcorrecting library kindly check it out [here](http:\/\/symspellpy.readthedocs.io\/en\/latest\/examples\/lookup_compound.html#basic-usage)\n\n\n**Intallation required**\n\n!pip install symspellpy\n"}}