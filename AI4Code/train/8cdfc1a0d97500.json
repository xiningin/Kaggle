{"cell_type":{"c6db0af6":"code","c0c8545c":"code","a357dec4":"code","417826ca":"code","62c141da":"code","e73bd089":"code","bbf0f70c":"code","840fff63":"code","d54d8a35":"code","cc3bf35f":"code","5ebe4499":"code","5766c035":"code","33fb803b":"code","1d5c238d":"code","accd03ef":"code","b470780e":"code","4d4cde70":"code","73443507":"code","656cc55a":"code","fc7195c7":"code","87a6f7cd":"code","8be0f537":"code","9517688c":"code","26844562":"code","b0c6387f":"code","b5f2bdeb":"code","45e0bcc7":"code","9e4ea5b0":"code","acb5a619":"code","0b050043":"code","451706c6":"code","cc5eae1a":"code","0bf81bbf":"code","9cd137a2":"code","bf05a18e":"code","df7ff55e":"code","49dfe27c":"code","bc4419d1":"code","4f3fcb0c":"code","5be7cc5f":"code","b126729d":"code","777f745f":"code","71b441d3":"code","a5ad013a":"code","db219794":"code","85d10a8d":"code","0d1c24cd":"code","dcdfa483":"code","0ca7bfec":"code","c6fb0407":"code","772cd280":"code","054e7698":"code","c60b88c1":"code","15938e53":"code","0f24ec23":"code","a64aad86":"code","74e2d868":"code","fa8af5a8":"code","bde2a758":"code","5ecadc01":"code","1338350f":"code","758038dd":"code","78e93211":"code","f6aca48c":"code","4b96ba88":"code","ddfff371":"code","f835f95a":"code","40f9beae":"code","212c5ff6":"code","d127838c":"code","6e5d0b05":"code","e6deef76":"code","55a2f22f":"code","4d72a05c":"code","60ee6c80":"code","c6a1f22a":"code","6fd0c9c9":"code","1e4c54ce":"code","cc120855":"code","990121fc":"code","8aa483a8":"code","35884148":"code","6ea36cce":"code","e1f8ecd4":"code","03211514":"code","ed7519dd":"code","fda383ee":"code","bac7c375":"code","5f8b8abe":"code","004cf3f2":"code","31d674b7":"code","c591cd20":"code","eae80aba":"code","fd7d9713":"code","1a6df572":"code","64a2f30c":"code","ed0650e1":"code","2ee91079":"code","af9e2c9f":"code","2e907f18":"code","d31d4c66":"code","14a5624e":"code","50945a25":"code","c74e7196":"code","5a398394":"code","034dffbe":"code","8ac971ec":"code","1eead3d4":"code","ec2bad4b":"code","ce96055b":"code","fe70863c":"code","29fbcd87":"code","322cbf51":"code","bd7bc4fd":"markdown","51a2631c":"markdown","250c94e4":"markdown","30b944df":"markdown","365ea154":"markdown","e45e3346":"markdown","7fee6d48":"markdown","1980bfe3":"markdown","7ed775df":"markdown","f7e67f2b":"markdown","7cf17151":"markdown","8e7dc475":"markdown","4e59488a":"markdown","984b92ca":"markdown","bf414567":"markdown","1bed3960":"markdown","af00d023":"markdown","3b3d3168":"markdown","435bb99a":"markdown","ec8b6e20":"markdown","c7096e87":"markdown","9bd97442":"markdown","39c913a8":"markdown","527b0eb3":"markdown","8fe20139":"markdown","ca85a597":"markdown","4efc6425":"markdown","21b3bd88":"markdown","61757116":"markdown","dc51a292":"markdown","fcbe5731":"markdown","452298ae":"markdown","8f5550ad":"markdown","3f95fa1b":"markdown","68f53f47":"markdown","ad422d03":"markdown","7f9cd820":"markdown","19d75767":"markdown","9f32644a":"markdown","e7fc0573":"markdown","6c02a719":"markdown","1b99759d":"markdown","9724eaac":"markdown","25b756d5":"markdown","997d11a1":"markdown","569bf21d":"markdown","907d2750":"markdown","abf3e756":"markdown","6f1f5514":"markdown","cea6a4d3":"markdown","17bc83bf":"markdown","b24c540d":"markdown","06a7819d":"markdown","e2f01a2e":"markdown","f023c0ae":"markdown","e773cd98":"markdown","90e5d9b4":"markdown","06fe0249":"markdown","0280994e":"markdown","74926561":"markdown","259122d4":"markdown","464c78d7":"markdown","7d5a0620":"markdown","f7ec1f0b":"markdown","05010fc8":"markdown","9b9f1e81":"markdown","28dde977":"markdown","53efc7ae":"markdown","ea0d3e61":"markdown","2b11a5dc":"markdown","0c48e3d3":"markdown","60468f21":"markdown","97477490":"markdown","8576ed15":"markdown","c96b69ed":"markdown","37c8e3e6":"markdown","93da7294":"markdown","0be7947b":"markdown","652a2445":"markdown","2b1236da":"markdown","dfeaefba":"markdown","4ff02bc4":"markdown","c22eb467":"markdown","023f543b":"markdown","46444989":"markdown","b9c99438":"markdown","2dc20cf5":"markdown","5d796ca6":"markdown","b2a3f3ee":"markdown","ce0eb4b5":"markdown","48172afa":"markdown","7af23a55":"markdown","214f0824":"markdown","47227ebf":"markdown","cddda2a5":"markdown"},"source":{"c6db0af6":"# packages to load \n# Check the versions of libraries\n# Python version\nimport warnings\nwarnings.filterwarnings('ignore')\nimport sys\nprint('Python: {}'.format(sys.version))\n# scipy\nimport scipy\nprint('scipy: {}'.format(scipy.__version__))\nimport numpy\n# matplotlib\nimport matplotlib\nprint('matplotlib: {}'.format(matplotlib.__version__))\n# numpy\nimport numpy as np # linear algebra\nprint('numpy: {}'.format(np.__version__))\n# pandas\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nprint('pandas: {}'.format(pd.__version__))\nimport seaborn as sns\nprint('seaborn: {}'.format(sns.__version__))\nsns.set(color_codes=True)\nimport matplotlib.pyplot as plt\nprint('matplotlib: {}'.format(matplotlib.__version__))\n%matplotlib inline\n# scikit-learn\nimport sklearn\nprint('sklearn: {}'.format(sklearn.__version__))\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport os\n%matplotlib inline\nfrom sklearn.metrics import accuracy_score\n# Importing metrics for evaluation\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\n","c0c8545c":"pd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points\n\n","a357dec4":"from subprocess import check_output\n\n","417826ca":"#print(check_output([\"ls\", \"\/Users\/ghostdev\/Desktop\/REML\"]).decode(\"utf8\")) #check the files available in the directory\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\")) #check the files available in the directory\n","62c141da":"# import Dataset to play with it\n#train = pd.read_csv('\/Users\/ghostdev\/Desktop\/REML\/train.csv')\n#test= pd.read_csv('\/Users\/ghostdev\/Desktop\/REML\/test.csv')\n# import Dataset to play with it\ntrain = pd.read_csv('..\/input\/train.csv')\ntest= pd.read_csv('..\/input\/test.csv')","e73bd089":"type(train)\n","bbf0f70c":"type(test)\n","840fff63":"# shape\nprint(train.shape)","d54d8a35":"# shape\nprint(test.shape)","cc3bf35f":"#columns*rows\ntrain.size","5ebe4499":"#columns*rows\ntest.size","5766c035":"print(train.info())\n","33fb803b":"train['Fence'].unique()\n","1d5c238d":"train['SaleType'].unique()\n","accd03ef":"train[\"HouseStyle\"].value_counts()\n","b470780e":"train[\"SaleType\"].value_counts()\n","4d4cde70":"train.head(5) \n","73443507":"train.tail(5) \n","656cc55a":"train.sample(5) \n","fc7195c7":"train.describe() \n","87a6f7cd":"train.isnull().sum()\n","8be0f537":"train.groupby('SaleType').count()\n","9517688c":"train.columns\n","26844562":"type((train.columns))\n","b0c6387f":"train[train['SalePrice']>700000]\n","b5f2bdeb":"numberic_features=train.select_dtypes(include=[np.number])\ncategorical_features=train.select_dtypes(include=[np.object])","45e0bcc7":"train['SalePrice'].describe()\n","9e4ea5b0":"sns.distplot(train['SalePrice']);\n","acb5a619":"#skewness and kurtosis\nprint(\"Skewness: %f\" % train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train['SalePrice'].kurt())","0b050043":"# Modify the graph above by assigning each species an individual color.\ncolumns = ['SalePrice','OverallQual','TotalBsmtSF','GrLivArea','GarageArea','FullBath','YearBuilt','YearRemodAdd']\nsns.FacetGrid(train[columns], hue=\"OverallQual\", size=5) \\\n   .map(plt.scatter, \"OverallQual\", \"SalePrice\") \\\n   .add_legend()\nplt.show()","451706c6":"train[columns].plot(y='SalePrice',x='OverallQual',kind='box')\nplt.figure()\n#This gives us a much clearer idea of the distribution of the input attributes:","cc5eae1a":"train[columns].plot(y='SalePrice',x='OverallQual',kind='box')\nplt.figure()\n#This gives us a much clearer idea of the distribution of the input attributes:","0bf81bbf":"data = pd.concat([train['SalePrice'], train['OverallQual']], axis=1)\nf, ax = plt.subplots(figsize=(12, 8))\nfig = sns.boxplot(x='OverallQual', y=\"SalePrice\", data=data)","9cd137a2":"# Use Seaborn's striplot to add data points on top of the box plot \n# Insert jitter=True so that the data points remain scattered and not piled into a verticle line.\n# Assign ax to each axis, so that each plot is ontop of the previous axis. \n\nax= sns.boxplot(x=\"OverallQual\", y=\"SalePrice\", data=train[columns])\nax= sns.stripplot(x=\"OverallQual\", y=\"SalePrice\", data=train[columns], jitter=True, edgecolor=\"gray\")\nplt.show()","bf05a18e":"# Tweek the plot above to change fill and border color color using ax.artists.\n# Assing ax.artists a variable name, and insert the box number into the corresponding brackets\n\nax= sns.boxplot(x=\"OverallQual\", y=\"SalePrice\", data=train[columns])\nax= sns.stripplot(x=\"OverallQual\", y=\"SalePrice\", data=train[columns], jitter=True, edgecolor=\"gray\")\n\nboxtwo = ax.artists[2]\nboxtwo.set_facecolor('red')\nboxtwo.set_edgecolor('black')\nboxthree=ax.artists[1]\nboxthree.set_facecolor('yellow')\nboxthree.set_edgecolor('black')\n\nplt.show()","df7ff55e":"# histograms\ntrain.hist(figsize=(15,20))\nplt.figure()","49dfe27c":"# scatter plot matrix\npd.plotting.scatter_matrix(train[columns],figsize=(10,10))\nplt.figure()","bc4419d1":"# violinplots on petal-length for each species\nsns.violinplot(data=train,x=\"Functional\", y=\"SalePrice\")","4f3fcb0c":"# Using seaborn pairplot to see the bivariate relation between each pair of features\nsns.set()\ncolumns = ['SalePrice','OverallQual','TotalBsmtSF','GrLivArea','GarageArea','FullBath','YearBuilt','YearRemodAdd']\nsns.pairplot(train[columns],size = 2 ,kind ='scatter')\nplt.show()","5be7cc5f":"# seaborn's kdeplot, plots univariate or bivariate density estimates.\n#Size can be changed by tweeking the value used\ncolumns = ['SalePrice','OverallQual','TotalBsmtSF','GrLivArea','GarageArea','FullBath','YearBuilt','YearRemodAdd']\nsns.FacetGrid(train[columns], hue=\"OverallQual\", size=5).map(sns.kdeplot, \"YearBuilt\").add_legend()\nplt.show()","b126729d":"# seaborn's kdeplot, plots univariate or bivariate density estimates.\n#Size can be changed by tweeking the value used\ncolumns = ['SalePrice','OverallQual','TotalBsmtSF','GrLivArea','GarageArea','FullBath','YearBuilt','YearRemodAdd']\nsns.FacetGrid(train[columns], hue=\"OverallQual\", size=10).map(sns.kdeplot, \"YearBuilt\").add_legend()\nplt.show()","777f745f":"# seaborn's kdeplot, plots univariate or bivariate density estimates.\n#Size can be changed by tweeking the value used\ncolumns = ['SalePrice','OverallQual','TotalBsmtSF','GrLivArea','GarageArea','FullBath','YearBuilt','YearRemodAdd']\nsns.FacetGrid(train[columns], hue=\"OverallQual\", size=5).map(sns.kdeplot, \"YearBuilt\").add_legend()\nplt.show()","71b441d3":"# Use seaborn's jointplot to make a hexagonal bin plot\n#Set desired size and ratio and choose a color.\ncolumns = ['SalePrice','OverallQual','TotalBsmtSF','GrLivArea','GarageArea','FullBath','YearBuilt','YearRemodAdd']\nsns.jointplot(x=\"OverallQual\", y=\"SalePrice\", data=train[columns], size=10,ratio=10, kind='hex',color='green')\nplt.show()","a5ad013a":"#In Pandas use Andrews Curves to plot and visualize data structure.\n#Each multivariate observation is transformed into a curve and represents the coefficients of a Fourier series.\n#This useful for detecting outliers in times series data.\n#Use colormap to change the color of the curves\ncolumns = ['SalePrice','OverallQual','TotalBsmtSF','GrLivArea','GarageArea','FullBath','YearBuilt','YearRemodAdd']\nfrom pandas.tools.plotting import andrews_curves\nandrews_curves(train[columns], \"YearBuilt\",colormap='rainbow')\nplt.show()","db219794":"# we will use seaborn jointplot shows bivariate scatterplots and univariate histograms with Kernel density \n# estimation in the same figure\ncolumns = ['SalePrice','OverallQual','TotalBsmtSF','GrLivArea','GarageArea','FullBath','YearBuilt','YearRemodAdd']\nsns.jointplot(x=\"SalePrice\", y=\"YearBuilt\", data=train[columns], size=6, kind='kde', color='#800000', space=0)","85d10a8d":"plt.figure(figsize=(7,4)) \ncolumns = ['SalePrice','OverallQual','TotalBsmtSF','GrLivArea','GarageArea','FullBath','YearBuilt','YearRemodAdd']\nsns.heatmap(train[columns].corr(),annot=True,cmap='cubehelix_r') #draws  heatmap with input as the correlation matrix calculted by(iris.corr())\nplt.show()","0d1c24cd":"# A final multivariate visualization technique pandas has is radviz\n# Which puts each feature as a point on a 2D plane, and then simulates\n# having each sample attached to those points through a spring weighted\n# by the relative value for that feature\nfrom pandas.tools.plotting import radviz\ncolumns = ['SalePrice','OverallQual','TotalBsmtSF','GrLivArea','GarageArea','FullBath','YearBuilt','YearRemodAdd']\nradviz(train[columns], \"OverallQual\")","dcdfa483":"# Save Id and drop it\ntrain_ID=train['Id']\ntest_ID=test['Id']\ntrain.drop('Id',axis=1,inplace=True)\ntest.drop('Id',axis=1,inplace=True)","0ca7bfec":"## 6-3-2 Noise filtering","c6fb0407":"fig, ax = plt.subplots()\nax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","772cd280":"#Deleting outliers\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n\n#Check the graphic again\nfig, ax = plt.subplots()\nax.scatter(train['GrLivArea'], train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","054e7698":"sns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","c60b88c1":"#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\n#Check the new distribution \nsns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","15938e53":"ntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.SalePrice.values\nall_data = pd.concat((train, test)).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","0f24ec23":"all_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data","a64aad86":"f, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=all_data_na.index, y=all_data_na)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","74e2d868":"#Correlation map to see how features are correlated with SalePrice\ncorrmat = train.corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=0.9, square=True)","fa8af5a8":"all_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")\n","bde2a758":"all_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")\n\n","5ecadc01":"all_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")\n","1338350f":"all_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")\n","758038dd":"#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))","78e93211":"for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_data[col] = all_data[col].fillna('None')","f6aca48c":"for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0)","4b96ba88":"for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)\n","ddfff371":"for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[col] = all_data[col].fillna('None')","f835f95a":"all_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)\n","40f9beae":"all_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\n","212c5ff6":"all_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")\n","d127838c":"all_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\n","6e5d0b05":"all_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\n","e6deef76":"all_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])","55a2f22f":"all_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\n","4d72a05c":"all_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")\n","60ee6c80":"all_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")\n","c6a1f22a":"#Check remaining missing values if any \nall_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head()","6fd0c9c9":"all_data[\"Utilities\"] = all_data[\"Utilities\"].fillna(\"AllPub\")\n","1e4c54ce":"#Check remaining missing values if any \nall_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head()\n","cc120855":"#MSSubClass=The building class\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n\n\n#Changing OverallCond into a categorical variable\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\n\n\n#Year and month sold are transformed into categorical features.\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","990121fc":"from sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))\n\n# shape        \nprint('Shape all_data: {}'.format(all_data.shape))","8aa483a8":"# Adding total sqfootage feature \nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']","35884148":"numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)","6ea36cce":"skewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    #all_data[feat] += 1\n    all_data[feat] = boxcox1p(all_data[feat], lam)","e1f8ecd4":"all_data = pd.get_dummies(all_data)\nprint(all_data.shape)","03211514":"train = all_data[:ntrain]\ntest = all_data[ntrain:]","ed7519dd":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import roc_curve\nimport xgboost as xgb\nimport lightgbm as lgb","fda383ee":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","bac7c375":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n","5f8b8abe":"KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n","004cf3f2":"GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)","31d674b7":"## XGBoost","c591cd20":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)","eae80aba":"## LightGBM","fd7d9713":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","1a6df572":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n","64a2f30c":"score = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","ed0650e1":"score = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","2ee91079":"score = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","af9e2c9f":"score = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","2e907f18":"score = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","d31d4c66":"score = rmsle_cv(model_lgb)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))\n","14a5624e":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1) ","50945a25":"averaged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))\n\nscore = rmsle_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","c74e7196":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)\n","5a398394":"stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),\n                                                 meta_model = lasso)\n\nscore = rmsle_cv(stacked_averaged_models)\nprint(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","034dffbe":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","8ac971ec":"#StackedRegressor\n#Final Training and Prediction\nstacked_averaged_models.fit(train.values, y_train)\nstacked_train_pred = stacked_averaged_models.predict(train.values)\nstacked_pred = np.expm1(stacked_averaged_models.predict(test.values))\nprint(rmsle(y_train, stacked_train_pred))","1eead3d4":"#XGBoost\nmodel_xgb.fit(train, y_train)\nxgb_train_pred = model_xgb.predict(train)\nxgb_pred = np.expm1(model_xgb.predict(test))\nprint(rmsle(y_train, xgb_train_pred))","ec2bad4b":"#lightGBM\nmodel_lgb.fit(train, y_train)\nlgb_train_pred = model_lgb.predict(train)\nlgb_pred = np.expm1(model_lgb.predict(test.values))\nprint(rmsle(y_train, lgb_train_pred))","ce96055b":"'''RMSE on the entire Train data when averaging'''\n\nprint('RMSLE score on train data:')\nprint(rmsle(y_train,stacked_train_pred*0.70 +\n               xgb_train_pred*0.15 + lgb_train_pred*0.15 ))","fe70863c":"## Ensemble prediction","29fbcd87":"ensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15\n","322cbf51":"sub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = ensemble\nsub.to_csv('submission.csv',index=False)\n","bd7bc4fd":"## LASSO Regression \nIn statistics and machine learning, lasso (least absolute shrinkage and selection operator)  is a **regression analysis** method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the statistical model it produces.  Lasso was originally formulated for least squares models and this simple case reveals a substantial amount about the behavior of the estimator, including its relationship to ridge regression and best subset selection and the connections between lasso coefficient estimates and so-called soft thresholding. It also reveals that (like standard linear regression) the coefficient estimates need not be unique if covariates are collinear.\n\nThis model may be very sensitive to outliers. So we need to made it more robust on them. For that we use the sklearn's Robustscaler() method on pipeline","51a2631c":"It looks like perhaps two of the input variables have a Gaussian distribution. This is useful to note as we can use algorithms that can exploit this assumption.\n","250c94e4":"FireplaceQu : data description says NA means \"no fireplace\"","30b944df":"LotFrontage : Since the area of each street connected to the house property most likely have a similar area to other houses in its neighborhood , we can fill in missing values by the median LotFrontage of the neighborhood.","365ea154":"<a id=\"10\"><\/a> <br>\n## 5-Installation\n#### Windows:\n* Anaconda (from https:\/\/www.continuum.io) is a free Python distribution for SciPy stack. It is also available for Linux and Mac.\n* Canopy (https:\/\/www.enthought.com\/products\/canopy\/) is available as free as well as commercial distribution with full SciPy stack for Windows, Linux and Mac.\n* Python (x,y) is a free Python distribution with SciPy stack and Spyder IDE for Windows OS. (Downloadable from http:\/\/python-xy.github.io\/)\n#### Linux\nPackage managers of respective Linux distributions are used to install one or more packages in SciPy stack.\n\nFor Ubuntu Users:\nsudo apt-get install python-numpy python-scipy python-matplotlibipythonipythonnotebook\npython-pandas python-sympy python-nose","e45e3346":"Beginner Working on Python Comprehensive ML Workflow for House Prices\nForked from the following workflow template \"https:\/\/www.kaggle.com\/mjbahmani\/a-comprehensive-ml-workflow-for-house-prices\/notebook\"\nMy goal is to add an LSTM model to the analysis. ","7fee6d48":"<a id=\"12\"><\/a> <br>\n## 5-2 Kaggle Kernel\nKaggle kernel is an environment just like you use jupyter notebook, it's an **extension** of the where in you are able to carry out all the functions of jupyter notebooks plus it has some added tools like forking et al.","1980bfe3":"##Stacking models","7ed775df":"## 6-3-4 Feature selection\nlet's first concatenate the train and test data in the same dataframe","f7e67f2b":"<a id=\"16\"><\/a> <br>\n## 6- Exploratory Data Analysis(EDA)\n In this section, you'll learn how to use graphical and numerical techniques to begin uncovering the structure of your data. \n \n* Which variables suggest interesting relationships?\n* Which observations are unusual?\n\nBy the end of the section, you'll be able to answer these questions and more, while generating graphics that are both insightful and beautiful.  then We will review analytical and statistical operations:\n\n*   5-1 Data Collection\n*   5-2 Visualization\n*   5-3 Data Preprocessing\n*   5-4 Data Cleaning\n<img src=\"http:\/\/s9.picofile.com\/file\/8338476134\/EDA.png\">","7cf17151":"## 6-1-2Select numberical features and categorical features","8e7dc475":"Exterior1st and Exterior2nd : Again Both Exterior 1 & 2 have only one missing value. We will just substitute in the most common string","4e59488a":"<a id=\"29\"><\/a> <br>\n### 6-2-11 radviz","984b92ca":"**Box Cox Transformation of (highly) skewed features\n\nWe use the scipy function boxcox1p which computes the Box-Cox transformation of \n1+x\n1+x\n.\nNote that setting \n\u03bb=0\n\u03bb=0\nis equivalent to log1p used above for the target variable.\nSee this page for more details on Box Cox Transformation as well as the scipy function's page","bf414567":"<a id=\"28\"><\/a> <br>\n### 6-2-10 Heatmap","1bed3960":"Train has one column more than test why?   (yes ==>> **target value**)","af00d023":"## 6-3-3 Target Variable\nSalePrice is the variable we need to predict. So let's do some analysis on this variable first.","3b3d3168":"BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2 : For all these categorical basement-related features, NaN means that there is no basement.","435bb99a":" <a id=\"1\"><\/a> <br>\n## 1- Introduction\nThis is a **A Comprehensive ML Workflow for House Prices** data set, it is clear that everyone in this community is familiar with house prices dataset but if you need to review your information about the dataset please visit this [link](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/data).\n\nI have tried to help **Fans of Machine Learning**  in Kaggle how to face machine learning problems. and I think it is a great opportunity for who want to learn machine learning workflow with python **completely**.\n\nI have covered most of the methods that are implemented for house prices until **2018**, you can start to learn and review your knowledge about ML with a simple dataset and try to learn and memorize the workflow for your journey in Data science world.\n\nI am open to getting your feedback for improving this **kernel**\n","ec8b6e20":"## Gradient Boosting Regression\nWith huber loss that makes it robust to outliers","c7096e87":"Is there any remaining missing value ?","9bd97442":"to give a statistical summary about the dataset, we can use **describe()","39c913a8":"FireplaceQu : data description says NA means \"no fireplace\"","527b0eb3":"**<< Note 2 >>**\nin pandas's data frame you can perform some query such as \"where\"","8fe20139":"<a id=\"19\"><\/a> <br>\n### 6-2-1 Scatter plot\n\nScatter plot Purpose To identify the type of relationship (if any) between two quantitative variables\n\n","ca85a597":"## 6-5 Skewed features","4efc6425":"Note the diagonal grouping of some pairs of attributes. This suggests a high correlation and a predictable relationship.","21b3bd88":"<a id=\"22\"><\/a> <br>\n### 6-2-4 Multivariate Plots\nNow we can look at the interactions between the variables.\n\nFirst, let\u2019s look at scatterplots of all pairs of attributes. This can be helpful to spot structured relationships between input variables.","61757116":"<a id=\"17\"><\/a> <br>\n## 6-1 Data Collection\n**Data collection** is the process of gathering and measuring data, information or any variables of interest in a standardized and established manner that enables the collector to answer or test hypothesis and evaluate outcomes of the particular collection.[techopedia]\n\n\n> **<< Note >>**\n\n> **The rows being the samples and the columns being attributes**\n","dc51a292":"<a id=\"26\"><\/a> <br>\n### 6-2-8 jointplot","fcbe5731":"## 7-1 Define a cross validation strategy","452298ae":"to pop up 5 random rows from the data set, we can use **sample(5)**  function","8f5550ad":"We use the cross_val_score function of **Sklearn**. However this function has not a shuffle attribut, we add then one line of code, in order to shuffle the dataset prior to cross-validation","3f95fa1b":"<a id=\"30\"><\/a> <br>\n## 6-3 Data Preprocessing\n**Data preprocessing** refers to the transformations applied to our data before feeding it to the algorithm.\n \nData Preprocessing is a technique that is used to convert the raw data into a clean data set. In other words, whenever the data is gathered from different sources it is collected in raw format which is not feasible for the analysis.\nthere are plenty of steps for data preprocessing and we just listed some of them :\n* removing Target column (id)\n* Sampling (without replacement)\n* Making part of iris unbalanced and balancing (with undersampling and SMOTE)\n* Introducing missing values and treating them (replacing by average values)\n* Noise filtering\n* Data discretization\n* Normalization and standardization\n* PCA analysis\n* Feature selection (filter, embedded, wrapper)","68f53f47":"GarageYrBlt, GarageArea and GarageCars : Replacing missing data with 0 (Since No garage = no cars in such garage.)","ad422d03":"## Notebook  Content\n*   1-  [Introduction](#1)\n*   2- [Machine learning workflow](#2)\n*   3- [Problem Definition](#3)\n*       3-1 [Problem feature](#4)\n*       3-2 [Aim](#5)\n*       3-3 [Variables](#6)\n*   4-[ Inputs & Outputs](#7)\n*   4-1 [Inputs ](#8)\n*   4-2 [Outputs](#9)\n*   5- [Installation](#10)\n*       5-1 [ jupyter notebook](#11)\n*       5-2[ kaggle kernel](#12)\n*       5-3 [Colab notebook](#13)\n*       5-4 [install python & packages](#14)\n*       5-5 [Loading Packages](#15)\n*   6- [Exploratory data analysis](#16)\n*       6-1 [Data Collection](#17)\n*       6-2 [Visualization](#18)\n*           6-2-1 [Scatter plot](#19)\n*           6-2-2 [Box](#20)\n*           6-2-3 [Histogram](#21)\n*           6-2-4 [Multivariate Plots](#22)\n*           6-2-5 [Violinplots](#23)\n*           6-2-6 [Pair plot](#24)\n*           6-2-7 [Kde plot](#25)\n*           6-2-8 [Joint plot](#26)\n*           6-2-9 [Andrews curves](#27)\n*           6-2-10 [Heatmap](#28)\n*           6-2-11 [Radviz](#29)\n*       6-3 [Data Preprocessing](#30)\n*       6-4 [Data Cleaning](#31)\n*   7- [Model Deployment](#32)\n*   8- [Conclusion](#53)\n*  9- [References](#54)","7f9cd820":"We impute them by proceeding sequentially through features with missing values\n\nPoolQC : data description says NA means \"No Pool\". That make sense, given the huge ratio of missing value (+99%) and majority of houses have no Pool at all in general. ","19d75767":"<a id=\"27\"><\/a> <br>\n###  6-2-9 andrews_curves","9f32644a":"**if you want see the type of data and unique value of it you use following script**","e7fc0573":"\nWe can get a quick idea of how many instances (rows) and how many attributes (columns) the data contains with the shape property.\n\n**You should see 1460 instances and  81  attributes for train and 1459 instances and  80 attributes for test**\n\nfor getting some information about the dataset you can use **info()** command","6c02a719":"From the plot, we can see that the species setosa is separataed from the other two across all feature combinations\n\nWe can also replace the histograms shown in the diagonal of the pairplot by kde.","1b99759d":"## 3- Problem Definition\nI think one of the important things when you start a new machine learning project is Defining your problem.\n\nProblem Definition has four steps that have illustrated in the picture below:\n<img src=\"http:\/\/s8.picofile.com\/file\/8338227734\/ProblemDefination.png\">\n\n### 3-1 Problem Feature\nwe will use the house prices data set. This dataset contains information about house prices and the target value is:\n\n* SalePrice\n\n<img src=\"https:\/\/kaggle2.blob.core.windows.net\/competitions\/kaggle\/5407\/media\/housesbanner.png\"><\/img>\n**Why am I  using House price dataset:**\n\n1- This is a good project because it is so well understood.\n\n2- Attributes are numeric and categurical so you have to figure out how to load and handle data.\n\n3- It is a Regression problem, allowing you to practice with perhaps an easier type of supervised learning algorithm.\n\n4- This is a perfect competition for data science students\u00a0who have completed an online course in machine learning and are looking to expand their skill set before trying a featured competition.\u00a0\n\n5-Creative feature engineering\u00a0.\n\n\n### 3-2 Aim\nIt is your job to predict the sales price for each house.\u00a0For each Id in the test set, you must predict the value of the SalePrice variable.\u00a0\n<img src=\"https:\/\/totalbitcoin.org\/wp-content\/uploads\/2015\/12\/Bitcoin-price-projections-for-2016.png\"><\/img>\n\n\n### 3-3 Variables\nThe variables are :\n* SalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict.\n* MSSubClass: The building class\n* MSZoning: The general zoning classification\n* LotFrontage: Linear feet of street connected to property\n* LotArea: Lot size in square feet\n* Street: Type of road access\n* Alley: Type of alley access\n* LotShape: General shape of property\n* LandContour: Flatness of the property\n* Utilities: Type of utilities available\n* LotConfig: Lot configuration\n* LandSlope: Slope of property\n* Neighborhood: Physical locations within Ames city limits\n* Condition1: Proximity to main road or railroad\n* Condition2: Proximity to main road or railroad (if a second is present)\n* BldgType: Type of dwelling\n* HouseStyle: Style of dwelling\n* OverallQual: Overall material and finish quality\n* OverallCond: Overall condition rating\n* YearBuilt: Original construction date\n* YearRemodAdd: Remodel date\n* RoofStyle: Type of roof\n* RoofMatl: Roof material\n* Exterior1st: Exterior covering on house\n* Exterior2nd: Exterior covering on house (if more than one material)\n* MasVnrType: Masonry veneer type\n* MasVnrArea: Masonry veneer area in square feet\n* ExterQual: Exterior material quality\n* ExterCond: Present condition of the material on the exterior\n* Foundation: Type of foundation\n* BsmtQual: Height of the basement\n* BsmtCond: General condition of the basement\n* BsmtExposure: Walkout or garden level basement walls\n* BsmtFinType1: Quality of basement finished area\n* BsmtFinSF1: Type 1 finished square feet\n* BsmtFinType2: Quality of second finished area (if present)\n* BsmtFinSF2: Type 2 finished square feet\n* BsmtUnfSF: Unfinished square feet of basement area\n* TotalBsmtSF: Total square feet of basement area\n* Heating: Type of heating\n* HeatingQC: Heating quality and condition\n* CentralAir: Central air conditioning\n* Electrical: Electrical system\n* 1stFlrSF: First Floor square feet\n* 2ndFlrSF: Second floor square feet\n*  LowQualFinSF: Low quality finished square feet (all floors)\n* GrLivArea: Above grade (ground) living area square feet\n* BsmtFullBath: Basement full bathrooms\n* BsmtHalfBath: Basement half bathrooms\n* FullBath: Full bathrooms above grade\n* HalfBath: Half baths above grade\n* Bedroom: Number of bedrooms above basement level\n* Kitchen: Number of kitchens\n* KitchenQual: Kitchen quality\n* TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n* Functional: Home functionality rating\n* Fireplaces: Number of fireplaces\n* FireplaceQu: Fireplace quality\n* GarageType: Garage location\n* GarageYrBlt: Year garage was built\n* GarageFinish: Interior finish of the garage\n* GarageCars: Size of garage in car capacity\n* GarageArea: Size of garage in square feet\n* GarageQual: Garage quality\n* GarageCond: Garage condition\n* PavedDrive: Paved driveway\n* WoodDeckSF: Wood deck area in square feet\n* OpenPorchSF: Open porch area in square feet\n* EnclosedPorch: Enclosed porch area in square feet\n* 3SsnPorch: Three season porch area in square feet\n* ScreenPorch: Screen porch area in square feet\n* PoolArea: Pool area in square feet\n* PoolQC: Pool quality\n* Fence: Fence quality\n* MiscFeature: Miscellaneous feature not covered in other categories\n* MiscVal: $Value of miscellaneous feature\n* MoSold: Month Sold\n* YrSold: Year Sold\n* SaleType: Type of sale\n* SaleCondition: Condition of sale","9724eaac":"Fence : data description says NA means \"no fence\"","25b756d5":"## 6-4-2 More features engeneering\n\nTransforming some numerical variables that are really categorical","997d11a1":"<a id=\"32\"><\/a> <br>\n## 7- Model Deployment\nIn this section have been applied more than **20 learning algorithms** that play an important rule in your experiences and improve your knowledge in case of ML technique.\n\n> **<< Note 3 >>** : The results shown here may be slightly different for your analysis because, for example, the neural network algorithms use random number generators for fixing the initial value of the weights (starting points) of the neural networks, which often result in obtaining slightly different (local minima) solutions each time you run the analysis. Also note that changing the seed for the random number generator used to create the train, test, and validation samples can change your results.","569bf21d":"## 6-1-1 Statistical Summary\n1- Dimensions of the dataset.\n\n2- Peek at the data itself.\n\n3- Statistical summary of all attributes.\n\n4- Breakdown of the data by the class variable.[7]\n\nDon\u2019t worry, each look at the data is **one command**. These are useful commands that you can use again and again on future projects.","907d2750":"<a id=\"20\"><\/a> <br>\n### 6-2-2 Box\nIn descriptive statistics, a **box plot** or boxplot is a method for graphically depicting groups of numerical data through their quartiles. Box plots may also have lines extending vertically from the boxes (whiskers) indicating variability outside the upper and lower quartiles, hence the terms box-and-whisker plot and box-and-whisker diagram.[wikipedia]","abf3e756":"BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath : missing values are likely zero for having no basement","6f1f5514":"**to check out last 5 row of the data set, we use tail() function**","cea6a4d3":"<a id=\"15\"><\/a> <br>\n## 5-5 Loading Packages\nIn this kernel we are using the following packages:\n    \n <img src=\"http:\/\/s8.picofile.com\/file\/8338227868\/packages.png\">\n Now we import all of them     ","17bc83bf":"<a id=\"7\"><\/a> <br>\n## 4- Inputs & Outputs\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/b\/bc\/Input-Output.JPG\"><\/img>\n\n<a id=\"8\"><\/a> <br>\n### 4-1 Inputs\n* train.csv - the training set\n\n<a id=\"9\"><\/a> <br>\n\n### 4-2 Outputs\n* sale prices for every record in test.csv","b24c540d":"MiscFeature : data description says NA means \"no misc feature\"","06a7819d":"MSSubClass : Na most likely means No building class. We can replace missing values with None","e2f01a2e":"## 6-3-1 removing ID","f023c0ae":"GarageType, GarageFinish, GarageQual and GarageCond : Replacing missing data with None","e773cd98":"Adding one more important feature\n\nSince area related features are very important to determine house prices, we add one more feature which is the total area of basement, first and second floor areas of each house","90e5d9b4":"## Elastic Net Regression \nthe elastic net is a regularized regression method that linearly combines the L1 and L2 penalties of the lasso and ridge methods.\nagain made robust to outliers","06fe0249":"**<< Note 1 >>**\n\n* Each row is an observation (also known as : sample, example, instance, record)\n* Each column is a feature (also known as: Predictor, attribute, Independent Variable, input, regressor, Covariate)\n\nAfter loading the data via **pandas**, we should checkout what the content is, description and via the following:","0280994e":"Log-transformation of the target variable","74926561":"We can see at the bottom right two with extremely large GrLivArea that are of a low price. These values are huge oultliers. Therefore, we can safely delete them.","259122d4":"**to check the first 5 rows of the data set, we can use head(5).**","464c78d7":"Simplest Stacking approach : Averaging base models\nWe begin with this simple approach of averaging base models. We build a new class to extend scikit-learn with our model and also to laverage encapsulation and code reuse (inheritance)","7d5a0620":"## 6-4-1 Imputing missing values","f7ec1f0b":"<a id=\"24\"><\/a> <br>\n### 6-2-6 pairplot","05010fc8":"<a id=\"54\"><\/a> <br>\n# 9- References\n* [1] [https:\/\/skymind.ai\/wiki\/machine-learning-workflow](https:\/\/skymind.ai\/wiki\/machine-learning-workflow)\n* [2] [Problem-define](https:\/\/machinelearningmastery.com\/machine-learning-in-python-step-by-step\/)\n* [3] [Sklearn](http:\/\/scikit-learn.org\/)\n* [4] [machine-learning-in-python-step-by-step](https:\/\/machinelearningmastery.com\/machine-learning-in-python-step-by-step\/)\n* [5] [Data Cleaning](http:\/\/wp.sigmod.org\/?p=2288)\n* [6] [kaggle kernel](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard)\n* [7] https:\/\/www.kaggle.com\/mjbahmani\/a-comprehensive-ml-workflow-for-house-prices\/notebook\n\n","9b9f1e81":"Alley : data description says NA means \"no alley access\"","28dde977":"The skew seems now corrected and the data appears more normally distributed.","53efc7ae":"## Kernel Ridge Regression \nKernel ridge regression (KRR)  combines Ridge Regression (linear least squares with l2-norm regularization) with the kernel trick. It thus learns a linear function in the space induced by the respective kernel and the data. For non-linear kernels, this corresponds to a non-linear function in the original space.","ea0d3e61":"## 6-1-3 Target Value Analysis\nas you know **SalePrice** is our target value that we should predict it then now we take a look at it","2b11a5dc":"<a id=\"11\"><\/a> <br>\n## 5-1 Jupyter notebook\nI strongly recommend installing **Python** and **Jupyter** using the **[Anaconda Distribution](https:\/\/www.anaconda.com\/download\/)**, which includes Python, the Jupyter Notebook, and other commonly used packages for scientific computing and data science.\n<img src=\"https:\/\/s3-ap-south-1.amazonaws.com\/av-blog-media\/wp-content\/uploads\/2018\/04\/jupyter-768x382.png\"><\/img>\nFirst, download Anaconda. We recommend downloading Anaconda\u2019s latest Python 3 version.\n\nSecond, install the version of Anaconda which you downloaded, following the instructions on the download page.\n\nCongratulations, you have installed Jupyter Notebook! To run the notebook, run the following command at the Terminal (Mac\/Linux) or Command Prompt (Windows):\n    ","0c48e3d3":"to print dataset **columns**, we can use columns atribute","60468f21":"Averaged base models class","97477490":"## 7-2 Model","8576ed15":"Getting the new train and test sets.","c96b69ed":"Getting dummy categorical features","37c8e3e6":"KitchenQual: Only one NA value, and same as Electrical, we set 'TA' (which is the most frequent) for the missing value in KitchenQual.","93da7294":"Electrical : It has one NA value. Since this feature has mostly 'SBrkr', we can set that for the missing value.","0be7947b":"<a id=\"2\"><\/a> <br>\n## 2- Machine Learning Workflow\nIf you have already read some [machine learning books](https:\/\/towardsdatascience.com\/list-of-free-must-read-machine-learning-books-89576749d2ff). You have noticed that there are different ways to stream data into machine learning.\n\nmost of these books share the following steps:\n*   Define Problem\n*   Specify Inputs & Outputs\n*   Exploratory data analysis\n*   Data Collection\n*   Data Preprocessing\n*   Data Cleaning\n*   Visualization\n*   Model Design, Training, and Offline Evaluation\n*   Model Deployment, Online Evaluation, and Monitoring\n*   Model Maintenance, Diagnosis, and Retraining\n\n**You can see my workflow in the below image** :\n <img src=\"http:\/\/s9.picofile.com\/file\/8338227634\/workflow.png\" \/>\n\n## 2-1 How to solve Problem?\n**Data Science has so many techniques and procedures that can confuse anyone.**\n\n**Step 1**: Translate your business problem statement into technical one\n\nAnalogous to any other software problem, data science aims at solving a business problem. Most of the times, business problem statements are vague and can be interpreted in multiple ways. This occurs mostly because we generally use qualitative words in our language which cannot be directly translated into a machine readable code.\n\nEg. Let\u2019s say we need to develop a solution to reduce crime rate of a city. The term \u201creduce\u201d can be interpreted as:\n\nDecreasing crime rate of areas with high crime rate\nDecreasing crime rate of the most common type of crime\nIt is a good practice to circle back with the client or the business team who define the problem to decide on the right interpretation.\n\n**Step 2**: Decide on the supervised learning technique\n\nThe end goal of almost every data science problem is usually classification or regression. Deciding the supervised technique for the problem will help you get more clarity on the business statement.\n\nEg. Let\u2019s look at our problem of reducing crime rate. While the problem of reducing crime rate is more of a policy decision, depending on the choice above, we would have to decide if we need to do classification or regression.\n\nIf we need to decrease crime rate of areas with high crime rate, we would need to determine the crime rate rate of an area. This is a regression problem.\nIf we need to decrease crime rate of most common type of crime, we would need to determine the most common type of crime in an area. This is a classification problem.\nAgain it is a good practice to circle back with the client or the business team who define the problem requirements to clarify on the exact requirement.\n\n**Step 3**: Literature survey\n\nLiterature Survey is one of the most important step (and often most ignored step) to approach any problem. If you read any article about components of Data Science, you will find computer science, statistics \/ math and domain knowledge. As it is quite inhuman for someone to have subject expertise in all possible fields, literature survey can often help in bridging the gaps of inadequate subject expertise.\n\nAfter going through existing literature related to a problem, I usually try to come up with a set of hypotheses that could form my potential set of features. Going through existing literature helps you understand existing proofs in the domain serving as a guide to take the right direction in your problem. It also helps in interpretation of the results obtained from the prediction models.\n\nEg. Going back to our problem of reducing crime rate, if you want to predict crime rate of an area, you would consider factors from general knowledge like demographics, neighboring areas, law enforcement rules etc. Literature survey will help you consider additional variables like climate, mode of transportation, divorce rate etc.\n\n**Step 4**: Data cleaning\n\nIf you speak with anyone who has spent some time in data science, they will always say that most of their time is spent on cleaning the data. Real world data is always messy. Here are a few common discrepancies in most data-sets and some techniques of how to clean them:\n\nMissing values\nMissing values are values that are blank in the data-set. This can be due to various reasons like value being unknown, unrecorded, confidential etc. Since the reason for a value being missing is not clear, it is hard to guess the value.\n\nYou could try different techniques to impute missing values starting with simple methods like column mean, median etc. and complex methods like using machine leaning models to estimate missing values.\n\nDuplicate records\nThe challenge with duplicate records is identifying a record being duplicate. Duplicate records often occur while merging data from multiple sources. It could also occur due to human error. To identify duplicates, you could approximate a numeric values to certain decimal places and for text values, fuzzy matching could be a good start. Identification of duplicates could help the data engineering team to improve collection of data to prevent such errors.\n\nIncorrect values\nIncorrect values are mostly due to human error. For Eg. If there is a field called age and the value is 500, it is clearly wrong. Having domain knowledge of the data will help identify such values. A good technique to identify incorrect values for numerical columns could be to manually look at values beyond 3 standard deviations from the mean to check for correctness.\n\n**Step 5**: Feature engineering\n\nFeature Engineering is one of the most important step in any data science problem. Good set of features might make simple models work for your data. If features are not good enough, you might need to go for complex models. Feature Engineering mostly involves:\n\nRemoving redundant features\nIf a feature is not contributing a lot to the output value or is a function of other features, you can remove the feature. There are various metrics like AIC and BIC to identify redundant features. There are built in packages to perform operations like forward selection, backward selection etc. to remove redundant features.\n\nTransforming a feature\nA feature might have a non linear relationship with the output column. While complex models can capture this with enough data, simple models might not be able to capture this. I usually try to visualize different functions of each column like log, inverse, quadratic, cubic etc. and choose the transformation that looks closest to a normal curve.\n\n**Step 6**: Data modification\n\nOnce the data is cleaned, there are a few modifications that might be needed before applying machine learning models. One of the most common modification would be scaling every column to the same range in order to give same weight to all columns. Some of the other required modifications might be data specific Eg. If output column is skewed, you might need to up-sample or down-sample.\n\nSteps 7 through 9 are iterative.\n\n**Step 7**: Modelling\n\nOnce I have the data ready, I usually start with trying all the standard machine learning models. If it is a classification problem, a good start will beLogistic Regression, Naive Bayes, k-Nearest Neighbors, Decision Tree etc. If it is a regression problem, you could try linear regression, regression tree etc. The reason for starting with simple models is that simple models have lesser parameters to alter. If we start with a complex model like Neural Network orSupport Vector Machines, there are so many parameters that you could change that trying all options exhaustively might be time consuming.\n\nEach of the machine learning models make some underlying assumptions about the data. For Eg. Linear Regression \/ Logistic Regression assumes that the data comes from a linear combination of input parameters. Naive Bayes makes an assumption that the input parameters are independent of each other. Having the knowledge of these assumptions can help you judge the results of the different models. It is often helpful to visualize the actual vs predicted values to see these differences.\n\n**Step 8**: Model comparison\n\nOne of the most standard technique to evaluate different machine learning models would be through the process of cross validation. I usually choose 10-fold cross validation but you may choose the right cross validation split based on the size of the data. Cross validation basically brings out an average performance of a model. This can help eliminate choosing a model that performs good specific to the data or in other words avoid over-fitting. It is often a good practice to randomize data before cross validation.\n\nA good technique to compare performance of different models is ROC curves. ROC curves help you visualize performance of different models across different thresholds. While ROC curves give a holistic sense of model performance, based on the business decision, you must choose the performance metric like Accuracy, True Positive Rate, False Positive Rate, F1-Score etc.\n\n**Step 9**: Error analysis\n\nAt this point, you have tried a bunch of machine learning models and got the results. It is a good usage of time to not just look at the results like accuracy or True Positive Rate but to look at the set of data points that failed in some of the models. This will help you understand the data better and improve the models faster than trying all possible combinations of models. This is the time to try ensemble models like Random Forest, Gradient Boosting or a meta model of your own [Eg. Decision tree + Logistic Regression]. Ensemble models are almost always guaranteed to perform better than any standard model.\n\n**Step 10**: Improving your best model\n\nOnce I have the best model, I usually plot training vs testing accuracy [or the right metric] against the number of parameters. Usually, it is easy to check training and testing accuracy against number of data points. Basically this plot will tell you whether your model is over-fitting or under-fitting. This articleDetecting over-fitting vs under-fitting explains this concept clearly.\n\nUnderstanding if your model is over-fitting or under-fitting will tell you how to proceed with the next steps. If the model is over-fitting, you might consider collecting more data. If the model is under-fitting, you might consider making the models more complex. [Eg. Adding higher order terms to a linear \/ logistic regression]\n\n**Step 11**: Deploying the model\n\nOnce you have your final model, you would want the model to be deployed so that it automatically predicts output for new data point without retraining. While you can derive a formula for simple models like Linear Regression, Logistic Regression, Decision Tree etc. , it is not so straight forward for complex models like SVM, Neural Networks, Random Forest etc. I\u2019m not very familiar with other languages but Python has a library called pickle which allows you to save models and use it to predict output for new data.\n\n**Step 12**: Adding feedback\n\nUsually, data for any data science problem is historical data. While this might be similar to the current data up-to a certain degree, it might not be able to capture the current trends or changes. For Eg. If you are using population as an input parameter, while population from 2015\u20132016 might vary slightly, if you use the model after 5 years, it might give incorrect results.\n\nOne way to deal with this problem is to keep retraining your model with additional data. This might be a good option but retraining a model might be time consuming. Also, if you have applications in which data inflow is huge, this might need to be done at regular intervals. An alternative and a better option would be to use active learning. Active learning basically tries to use real time data as feedback and automatically update the model. The most common approaches to do this are Batch Gradient Descent and Stochastic Gradient Descent. It might be appropriate to use the right approach based on the application.\n\nConcluding remarks\n\nThe field of data science is really vast. People spend their lifetime researching on individual topics discussed above. As a data scientist, you would mostly have to solve business problems than researching on individual subtopics. Additionally, you will have to explain the technical process and results to business teams who might not have enough technical knowledge. Thus, while you might not need a very in-depth knowledge of every technique, you need to have enough clarity to abstract the technical process and results and explain it in business terms.[3]\n","652a2445":"\n## Base models scores\n\nLet's see how these base models perform on the data by evaluating the cross-validation rmsle error","2b1236da":"<a id=\"13\"><\/a> <br>\n## 5-3 Colab notebook\n**Colaboratory** is a research tool for machine learning education and research. It\u2019s a Jupyter notebook environment that requires no setup to use.\n### 5-3-1 What browsers are supported?\nColaboratory works with most major browsers, and is most thoroughly tested with desktop versions of Chrome and Firefox.\n### 5-3-2 Is it free to use?\nYes. Colaboratory is a research project that is free to use.\n### 5-3-3 What is the difference between Jupyter and Colaboratory?\nJupyter is the open source project on which Colaboratory is based. Colaboratory allows you to use and share Jupyter notebooks with others without having to download, install, or run anything on your own computer other than a browser.","dfeaefba":"<a id=\"31\"><\/a> <br>\n## 6-4 Data Cleaning\nWhen dealing with real-world data, dirty data is the norm rather than the exception. We continuously need to predict correct values, impute missing ones, and find links between various data artefacts such as schemas and records. We need to stop treating data cleaning as a piecemeal exercise (resolving different types of errors in isolation), and instead leverage all signals and resources (such as constraints, available statistics, and dictionaries) to accurately predict corrective actions.\n\nThe primary goal of data cleaning is to detect and remove errors and anomalies to increase the value of data in analytics and decision making. While it has been the focus of many researchers for several years, individual problems have been addressed separately. These include missing value imputation, outliers detection, transformations, integrity constraints violations detection and repair, consistent query answering, deduplication, and many other related problems such as profiling and constraints mining.[8]\n","4ff02bc4":"<a id=\"23\"><\/a> <br>\n### 6-2-5 violinplots","c22eb467":"<a id=\"18\"><\/a> <br>\n## 6-2 Visualization\n**Data visualization**  is the presentation of data in a pictorial or graphical format. It enables decision makers to see analytics presented visually, so they can grasp difficult concepts or identify new patterns.\n\nWith interactive visualization, you can take the concept a step further by using technology to drill down into charts and graphs for more detail, interactively changing what data you see and how it\u2019s processed.[SAS]\n\n In this section I show you  **11 plots** with **matplotlib** and **seaborn** that is listed in the blew picture:\n <img src=\"http:\/\/s8.picofile.com\/file\/8338475500\/visualization.jpg\" \/>\n","023f543b":"<a id=\"21\"><\/a> <br>\n### 6-2-3 Histogram\nWe can also create a **histogram** of each input variable to get an idea of the distribution.\n\n","46444989":"The target variable is right skewed. As (linear) models love normally distributed data , we need to transform this variable and make it more normally distributed.\n","b9c99438":"MasVnrArea and MasVnrType : NA most likely means no masonry veneer for these houses. We can fill 0 for the area and None for the type. ","2dc20cf5":"to check out how many null info are on the dataset, we can use **isnull().sum()","5d796ca6":"Label Encoding some categorical variables that may contain information in their ordering set","b2a3f3ee":"MSZoning (The general zoning classification) : 'RL' is by far the most common value. So we can fill in missing values with 'RL'","ce0eb4b5":"\nAveraged base models score\n\nWe just average four models here ENet, GBoost, KRR and lasso. Of course we could easily add more models in the mix.","48172afa":"Functional : data description says NA means typical","7af23a55":"SaleType : Fill in again with most frequent which is \"WD\"","214f0824":"Flexibly plot a univariate distribution of observations.\n","47227ebf":"<a id=\"25\"><\/a> <br>\n###  6-2-7 kdeplot","cddda2a5":"### 6-2-12 Conclusion\nwe have used Python to apply data visualization tools to the House prices dataset. Color and size changes were made to the data points in scatterplots. I changed the border and fill color of the boxplot and violin, respectively."}}