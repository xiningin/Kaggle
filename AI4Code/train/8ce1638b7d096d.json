{"cell_type":{"8d1b33a8":"code","0e22a953":"code","a529c1ae":"code","d355d9f9":"code","bff54681":"code","58a1c860":"code","a8ff7751":"code","107aa9dd":"code","c660b3eb":"code","f0095729":"code","c7dc6baa":"code","7586e0f3":"code","4954bda6":"code","a4c32036":"code","a8615653":"code","fcb716fd":"code","6ed34472":"code","d54abc19":"code","4cea7121":"code","99662329":"code","087e9a89":"code","33b2e90f":"code","06d9c1c1":"code","4ae40004":"code","d107ae4c":"code","20746a88":"code","5f23de8e":"code","cdabf80e":"code","553e2546":"code","a7fd017b":"code","35ec359f":"code","65e06fc4":"code","30410444":"code","dcb6c8e4":"code","91e40723":"code","7bf1efb0":"code","a5ae0e8e":"code","2a5b259d":"code","474ad81a":"code","fc7b07b4":"markdown","56d8ecfa":"markdown","356452ce":"markdown","eb852940":"markdown","a2e8fc3a":"markdown","50bfbe10":"markdown"},"source":{"8d1b33a8":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nimport transformers\nimport missingno as msno\nimport re\nimport spacy\nimport nltk\nfrom wordcloud import WordCloud\n\nfrom sklearn.model_selection import KFold\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.metrics import RootMeanSquaredError\nfrom nltk.tokenize import sent_tokenize, word_tokenize \nfrom transformers import TFAutoModel,AutoTokenizer,TFAutoModelForSequenceClassification\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import make_pipeline\nfrom collections import Counter\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nsns.set_palette('husl')\n\n\n\n","0e22a953":"print(tf.__version__)","a529c1ae":"df=pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ndf.head()","d355d9f9":"df.info()","bff54681":"print(f'length of df : {len(df)}')","58a1c860":"msno.bar(df,sort=\"ascending\", figsize=(10,5))","a8ff7751":"df=df.iloc[:,1:]","107aa9dd":"def my_plot(df,row):\n  idx=0\n  j=0\n  feat=['target','standard_error']\n  plt.rcParams['figure.figsize'] = (15,5)\n  fig,axes=plt.subplots(row,2)\n  plt.subplots_adjust(top = 1.95)  \n  for i in range(row):\n      axes[i,j].axvline(df[feat[idx]].mean(), linestyle=':', linewidth=2)\n      sns.kdeplot(df[feat[idx]],color='red',ax=axes[i,j])   \n      axes[i,j].set_title(feat[idx])\n      j+=1\n      sns.violinplot(df[feat[idx]],color='red',ax=axes[i,j])     \n      axes[i,j].set_title(feat[idx])\n      idx+=1\n      j=0\n\n","c660b3eb":"my_plot(df,2)","f0095729":"df[df['url_legal'].notnull()].head()   #url legal","c7dc6baa":"len(df['license'].unique())    # licenses","7586e0f3":"df['license'][df['license'].notnull()]","4954bda6":"plt.figure(figsize=(20,10))\nplt.xticks(rotation=90)\nsns.countplot(df['license'][df['license'].notnull()])","a4c32036":"df=df[['excerpt','target']]                 \nprint(f'range of target values : ({df.target.min()},{df.target.max()})')","a8615653":"df=df.rename(columns={'excerpt':'text'})\nnltk.download('stopwords')\nstopwords = nltk.corpus.stopwords.words(\"english\")","fcb716fd":"df['target'].skew","6ed34472":"cloud=WordCloud(background_color = 'black',stopwords=stopwords,max_words=200,max_font_size = 40,scale=3).generate(str(df['text']))\n\ntitle='word count'\nfig = plt.figure(figsize=(15,15))\nfig.subplots_adjust(top = 2.25)\nfig.suptitle(title, fontsize = 20)\nplt.imshow(cloud)","d54abc19":"def text_clean(text):\n  pattern=re.compile(\"[^a-zA-Z]|https?:\/\/\\S+|www\\.\\S+\")\n  return pattern.sub(r' ',text)\n  ","4cea7121":"x_data=df['text'].apply(lambda text:text_clean(text).strip())","99662329":"x_data=[\" \".join(data.split()) for data in x_data]\nx_data[:10]","087e9a89":"#using spacy we do lemmatization and singularization\ntrain_data=[]\nnlp=spacy.load('en_core_web_sm')\nfor data in x_data:\n  doc = nlp(data)\n  train_data.append(\" \".join([str(token.lemma_) for token in doc]))","33b2e90f":"train_data=[' '.join([word for word in data.split() if '-PRON-'!=word]) for data in train_data]\ntrain_data=[' '.join([word for word in data.split() if word not in stopwords]) for data in train_data]\ny_data=df['target'].values","06d9c1c1":"x_data[:2],y_data[:2]","4ae40004":"train_data[:2],y_data[:2]","d107ae4c":"class DATASET:\n  def __init__(self,train_data,y_data):\n    \n    self.train_data=train_data\n    self.y_data=y_data\n  \n  def __call__(self,pad_sequences,train_test_split,model_name,roberta_tokenizer):\n\n    if model_name=='LR':\n\n      x_train,x_test,y_train,y_test=train_test_split(self.train_data,self.y_data,test_size=0.3)\n      tfidf=TfidfVectorizer(analyzer='word', ngram_range=(1,3))\n      table_c=tfidf.fit_transform(list(x_train)+list(x_test))\n      train_table_data=tfidf.transform(x_train)\n      test_table_data=tfidf.transform(x_test)\n      \n      return train_table_data,test_table_data,y_train,y_test\n\n    elif model_name=='roberta':\n      sequences=[]\n      length=[]\n      for text in self.train_data:\n        tokens=roberta_tokenizer.encode(text,add_special_tokens=True, truncation=True)\n        sequences.append(tokens)\n\n      \n      roberta_data=pad_sequences(sequences,maxlen=200,padding='pre',value=roberta_tokenizer.encode('<pad>')[1])  #roberta_tokenizer.encode('<pad>')[1] is the token value for padding\n      return roberta_data,self.y_data\n","20746a88":"roberta_tokenizer=AutoTokenizer.from_pretrained('roberta-base')","5f23de8e":"data=DATASET(train_data,y_data)\nx_train_data,x_test_data,y_train_data,y_test_data=data(pad_sequences,train_test_split,'LR',roberta_tokenizer)\nx_roberta_data,y_roberta_data=data(pad_sequences,train_test_split,'roberta',roberta_tokenizer)","cdabf80e":"print(f'for linear reg training sample set: {x_train_data.shape,y_train_data.shape} and for roberta whole dataset : {x_roberta_data.shape,y_roberta_data.shape}')","553e2546":"class Linear_Model(tf.keras.Model):\n  def __init__(self,x_train_data,y_train_data):\n    self.x_train_data=x_train_data\n    self.y_train_data=y_train_data\n    self.lreg=LinearRegression()\n\n  def linear_regression_result(self,x_test_data):\n    self.lreg.fit(self.x_train_data,self.y_train_data)     #train                    \n    \n    #predict\n    return self.lreg.predict(x_test_data)\n\n  ","a7fd017b":"linear_model=Linear_Model(x_train_data,y_train_data)\nlr_y_pred=linear_model.linear_regression_result(x_test_data)","35ec359f":"#performance metric evaluation on linear regression\nfrom sklearn.metrics import mean_squared_error\nprint(f'RMSE Score {mean_squared_error(lr_y_pred,y_test_data,squared=False)}')","65e06fc4":"class Custom_roberta(tf.keras.Model):\n\n  def __init__(self):\n    super(Custom_roberta,self).__init__()\n    self.roberta_model = TFAutoModelForSequenceClassification.from_pretrained('roberta-base',output_hidden_states=False, output_attentions=False, num_labels=1)\n\n  def call(self,input_ids):\n    x=self.roberta_model(input_ids)\n    \n\n    return x","30410444":"def loss_func(y_true,y_pred):  #root mean sqruared error (RMSE) \n  return tf.sqrt(tf.reduce_mean(tf.square(y_pred-y_true)))\n\n","dcb6c8e4":"#with fit function use simple scheduler\n#constant lr for first 10 epochs and then lr is decreased exponentially \n\ndef schedule(epochs,lr):\n    if epochs<10:\n        return lr\n        \n    else:\n        return lr * tf.math.exp(-0.01)\n    ","91e40723":"#train_Size of the data for training and dev\n\ntrain_size=int(0.8*(len(x_roberta_data)))","7bf1efb0":"x_roberta_data.shape,y_roberta_data.shape","a5ae0e8e":"#k folds\ndef train_in_folds(x_roberta_data,y_roberta_data,folds):\n\n    # initiate the kfold class from model_selection module\n    kf = KFold(n_splits=folds,shuffle=True)\n    \n    for (fold, (train_index, test_index)) in enumerate(kf.split(x_roberta_data)):\n        print(f'for fold : {fold+1}\\n')\n        x_train,x_test=x_roberta_data[train_index],x_roberta_data[test_index]\n        y_train,y_test=y_roberta_data[train_index],y_roberta_data[test_index]\n\n        x_test,y_test=tf.convert_to_tensor(x_test),tf.convert_to_tensor(y_test)\n        x_train,y_train=tf.convert_to_tensor(x_train),tf.convert_to_tensor(y_train)\n        \n\n        model=Custom_roberta()\n        optimizer = tf.keras.optimizers.Adam(lr=2e-5)\n        callback_1=tf.keras.callbacks.LearningRateScheduler(schedule)\n        model.compile(optimizer=optimizer,loss='mse',metrics=[RootMeanSquaredError()])\n        callback_2=tf.keras.callbacks.EarlyStopping(monitor='val_root_mean_squared_error',mode='min',restore_best_weights=True,patience=2)\n        \n\n        r=model.fit(x_train,y_train,batch_size=10,validation_data=(x_test,y_test),epochs=6,callbacks=[callback_1,callback_2],verbose=1)\n        print(f\"best rmse : {np.min(r.history['val_root_mean_squared_error']):.4f}\")\n        print('\\n')   \n\n","2a5b259d":"train_in_folds(x_roberta_data,y_roberta_data,folds=5)","474ad81a":"#### supervised learning using transfer learming , hence used for downstream task only and warnings can be ignored","fc7b07b4":"### Dataset prep for linear reg and Roberta","56d8ecfa":"### Linear Regression model vs transformer model (Roberta-base) performance ","356452ce":"### Roberta-base ","eb852940":"We can clearly see that transformer model outperforming simple linear model in terms of performance in terms of RMSE metric.In next Update i will try to optimize the RMSE Score","a2e8fc3a":"## Linear reg model","50bfbe10":"### word frequency excerpt visualization in word cloud"}}