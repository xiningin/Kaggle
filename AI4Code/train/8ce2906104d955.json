{"cell_type":{"970a3663":"code","ef56c6c0":"code","76259bf3":"code","7de10141":"code","2a326cb4":"code","410b3df6":"code","ffe7c66f":"code","053c07de":"code","a3e4eb16":"code","581fe46e":"code","c8ba28cd":"code","4efa4bf0":"code","eb41853c":"code","e538039c":"code","e14cc961":"code","8d6161d2":"code","b46a64ed":"code","4c7b4dfd":"code","6e2c677a":"code","bd71214a":"code","6e2304a2":"code","2be74507":"code","aacf9edd":"code","efb3a3a7":"code","1d68fa4f":"code","b7378f38":"code","1b509cba":"code","667eaa87":"code","9c608df3":"code","aa81b0ae":"code","ba922214":"code","7d8315f2":"code","d5207eff":"code","655621ed":"code","38aa5fbc":"code","40537956":"code","5b1138d0":"code","5b46d264":"code","8b982ee8":"code","101117bf":"code","39c10dbb":"code","c6b45a5f":"code","e7b45672":"code","3016cc2e":"code","b8ae8562":"code","7b0da1ab":"code","dbfd9e26":"code","655dff00":"code","275729e6":"code","31ca3884":"code","9921a7f7":"code","6bf8cbab":"code","8d6b350c":"code","7370ad27":"code","f078cbce":"code","3a44db91":"code","9d179559":"code","11db16df":"code","750e66a6":"code","d8e4b616":"code","187b16b7":"code","dc73303e":"code","f0950a7a":"code","062eac0d":"code","c608c10a":"code","51db7377":"code","54ccb99c":"code","faecbc6b":"markdown","f66e8de6":"markdown","251faa6e":"markdown","825d94c1":"markdown","8ac6c6c6":"markdown","58a586cf":"markdown","1aba0d17":"markdown","64cc57b7":"markdown","540ff02f":"markdown","0ff42a5f":"markdown","c655063e":"markdown","dee81d81":"markdown","32eea3b4":"markdown","0cc0d3e5":"markdown","4eb44ece":"markdown","7340d55a":"markdown","1bd47533":"markdown","9f1b7a85":"markdown","d69673ed":"markdown","2471b4b8":"markdown","ada5bdcb":"markdown","c6837191":"markdown","84e1d0dc":"markdown","ff890ace":"markdown","ef9861a9":"markdown","6362943b":"markdown","67c988e1":"markdown","b6b404ef":"markdown","1b5856fb":"markdown","549c3fc7":"markdown","ce6f7151":"markdown","3c23299c":"markdown","4e7d3f0f":"markdown","8a54fcd1":"markdown","3dd6ab16":"markdown","c4ded9a1":"markdown","4f64395b":"markdown","7f3c9bdb":"markdown","7d6a25c3":"markdown","bf374142":"markdown","b3c16bf4":"markdown","24bd36a2":"markdown","d8626ed0":"markdown","7ecc026d":"markdown","a5c7f496":"markdown","1ecbf594":"markdown","4879c2d6":"markdown","019965eb":"markdown","aae583c8":"markdown","85533e0d":"markdown","cdbedff8":"markdown"},"source":{"970a3663":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ef56c6c0":"data = pd.read_csv('\/kaggle\/input\/ames-housing-dataset\/AmesHousing.csv')","76259bf3":"data = data.sort_values(\"Yr Sold\")","7de10141":"pd.options.display.max_columns = None","2a326cb4":"data.head()","410b3df6":"data.info()","ffe7c66f":"data.shape","053c07de":"data.columns","a3e4eb16":"data.select_dtypes(object).columns","581fe46e":"data.select_dtypes([np.int64, np.float64]).columns","c8ba28cd":"data.duplicated().sum()","4efa4bf0":"data.isnull().sum()","eb41853c":"plt.figure(figsize=(12,6))\n# sns.histplot(data['SalePrice'])\nsns.displot(data['SalePrice'], height=7, aspect=1.7, color='brown')\nplt.title('SalePrice Distribution')\n\nplt.show()","e538039c":"data.columns","e14cc961":"null_count = data.isnull().sum()\ndlt = null_count[null_count > data.shape[0]*0.05].index\ndlt","8d6161d2":"data.drop(dlt, axis=1, inplace=True)","b46a64ed":"num_null = data.select_dtypes([np.int64, np.float64]).isnull().sum()\nmissing_num = num_null[num_null>0].index\nmissing_num","4c7b4dfd":"fill = data[missing_num].mode().to_dict(orient = \"record\")[0]\nfill","6e2c677a":"data.fillna(fill, inplace = True)","bd71214a":"obj = data.select_dtypes(object)\nobj.head()","6e2304a2":"uniq = obj.apply(lambda col: len(col.unique())).sort_values(ascending = False)\nuniq","2be74507":"rmv_uniq = uniq[uniq>10].index\ndata.drop(rmv_uniq, axis = 1, inplace = True)","aacf9edd":"obj_col = data.select_dtypes(object).columns\nobj_col","efb3a3a7":"years_sold = data['Yr Sold'] - data['Year Built']\nyears_sold [years_sold<0]","1d68fa4f":"years_rmd = data['Yr Sold'] - data['Year Remod\/Add']\nyears_rmd[years_rmd<0]","b7378f38":"data.drop([1702, 2180,2181], axis = 0,inplace = True)","1b509cba":"data[\"Years Before Sale\"] = years_sold\ndata[\"Years Since Remod\"] = years_rmd","667eaa87":"data.drop(['Year Built','Year Remod\/Add'], axis = 1, inplace = True)","9c608df3":"data.drop([\"Order\"], axis = 1, inplace = True)","aa81b0ae":"data_leak = ['Mo Sold', 'Yr Sold', 'Sale Type','Sale Condition']\ndata.drop(data_leak,axis = 1, inplace = True)","ba922214":"c = data.corr()\nplt.figure(figsize=(15,10))\n# sns.set(font_scale = 1)\nsns.heatmap(c)","7d8315f2":"obj_col = data.select_dtypes(object).columns\nobj_col","d5207eff":"data[obj_col] = data[obj_col].astype('category')","655621ed":"for i in obj_col:\n    data[i] = data[i].cat.codes","38aa5fbc":"plt.figure(figsize=(4,30))\n\ncor_df = pd.DataFrame({'SalePrice' : data.corr()['SalePrice'].values},\n                     index = data.corr()['SalePrice'].index)\n\nsns.heatmap(cor_df, annot=True, cmap='viridis', annot_kws={\"fontsize\":17})\nsns.set(font_scale = 1.5)\n\nplt.show()","40537956":"cor = data.corr()[\"SalePrice\"].abs().sort_values(ascending = False)\nretained = cor[cor>0.25].index\nretained","5b1138d0":"data = data[retained]","5b46d264":"s = c.unstack()\nso = s.sort_values(kind=\"quicksort\")","8b982ee8":"fin = so[(so > 0.78) & (so < 1)].sort_values(ascending=False)\nfin","101117bf":"f_l = list(fin.index)\nf_l","39c10dbb":"lst_del = []\nfor index, value in enumerate(f_l):\n    if index%2 != 0:\n        lst = value[0]\n        lst_del.append(lst)\nlst_del","c6b45a5f":"data.drop(['Garage Cars', 'TotRms AbvGrd', '1st Flr SF'], axis=1, inplace=True)","e7b45672":"hetro = data.copy()\nhetro = (hetro-hetro.min())\/(hetro.max()- hetro.min())\nvar = hetro.var().sort_values(ascending = False)\nvar","3016cc2e":"final_col = var[var>0.01].index\nfinal_col","b8ae8562":"data = data[final_col]","7b0da1ab":"data.head()","dbfd9e26":"data.columns","655dff00":"sale_price = data[\"SalePrice\"]","275729e6":"data = (data-data.min())\/(data.max()-data.min())","31ca3884":"data[\"SalePrice\"] = sale_price","9921a7f7":"data.head()","6bf8cbab":"data = data.sample(frac=1, random_state=123)\ndata.head()","8d6b350c":"data.shape","7370ad27":"indx = int(2927*0.75)\n\ntrain = data[:indx]\ntest = data[indx:]","f078cbce":"features = train.columns.drop('SalePrice')\ntarget = ['SalePrice']","3a44db91":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error\n\n\nmodel = LinearRegression()\nmodel.fit(train[features], train[target])\n\nprediction = model.predict(test[features])\n\nmae = mean_absolute_error(test[target], prediction)\n\nprint('Linear Regression')\nprint(f'Mean Absolute Error: {mae}')\n\n","9d179559":"from sklearn.model_selection import KFold\n\nmodel = LinearRegression()\nmaes = []\n\nkf = KFold(5, True, random_state=123)\n\nfor train_index, test_index in kf.split(data):\n    train = data.iloc[train_index]\n    test = data.iloc[test_index]\n    model.fit(train[features], train[target])\n    prediction = model.predict(test[features])\n    mae = mean_absolute_error(prediction, test[target])\n    maes.append(mae)\n\nprint(f'Mean Absolute Error: {np.mean(maes)}')","11db16df":"from sklearn.linear_model import LinearRegression\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor \nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import KFold\n\nLG = LinearRegression()\nSV = SVR()\nKN = KNeighborsRegressor()\nDT = DecisionTreeRegressor(random_state=123)\nGB = GradientBoostingRegressor(random_state=123)\nRF = RandomForestRegressor(random_state=123)\n\nmodels = [LG, SV, KN, DT, GB, RF,]\nmodel_name = [ 'Linear Regression', 'Support Vector Regression', 'K Nearest Neighbor', \n              'Decision Tree', 'Gradient Boost', 'Random Forest' ]\n\n\nmeans = []\nr2_score_ = []\nkf = KFold(5, True, random_state=123)\n\nfor i in range(len(models)):\n    maes = []\n    r2s = []\n    model = models[i]\n\n    for train_index, test_index in kf.split(data):\n        train = data.iloc[train_index]\n        test = data.iloc[test_index]\n        model.fit(train[features], train[target])\n        prediction = model.predict(test[features])\n        mae = mean_absolute_error(prediction, test[target])\n        r2 = r2_score(test[target], prediction)\n        maes.append(mae)\n        r2s.append(r2)\n        \n    means.append(np.mean(maes))\n    r2_score_.append(np.mean(r2s))\n    \n    \nmod_comp_def = pd.DataFrame({'Models' : model_name, 'Mean Absolute Error' : means,\n                            'R2_Score' : r2_score_}).set_index('Models')\nmod_comp_def","750e66a6":"# #LG = LinearRegression\n# RV = \n# KN = K Nearest Neighbor\n# DT = DecisionTree\n# GB = GradientBoost\n# RF = RandomForest\n\nparameter_space_LG = {\n    'fit_intercept' : [True, False] ,\n    'normalize' : [True, False] ,\n    'copy_X' : [True, False] ,\n    'positive' : [True, False]\n}\n\nparameter_space_SV = {\n    \"kernel\": [\"poly\", \"linear\", \"rbf\", \"sigmoid\"],\n        \"degree\": [3, 5],\n        \"coef0\": [0, 3, 7],\n        \"gamma\":[1e-3, 1e-1, 1\/train[features].shape[1]],\n        \"C\": [1, 10, 100],\n}\n\nparameter_space_RI = {\n    \"alpha\": [1, 10, 100, 290, 500],\n    \"fit_intercept\": [True, False],\n    \"solver\": ['svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'],\n    'normalize': [True, False],\n    'copy_X' : [True, False],\n    'max_iter' : [10, 100, 500, 1000]\n}\n\nparameter_space_EN = {\n    'alpha' : [300, 500,1000,1500] ,\n    'l1_ratio' : [0.1, 0.5, 1] ,\n    'fit_intercept' : [True, False] ,\n    'normalize' : [True, False] ,\n    'max_iter' : [10, 100, 500, 1000],\n    'selection' : ['cyclic', 'random'],\n}\n\nparameter_space_KN = {\n    'n_neighbors' : [1,5,10,20,30,40,50],\n    'weights' : ['uniform', 'distance'],\n    'algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute'],\n    'leaf_size' : [1,2,20,50,200],\n    'p' : [1,2],\n}\n\nparameter_space_DT = {\n    'criterion' : ['mse', 'friedman_mse', 'mae', 'poisson'] ,\n    'splitter' : ['best', 'random'],\n    'max_depth' : [5,10,20,50],\n}\n\nparameter_space_GB = {\n    'loss' : ['ls', 'lad', 'huber', 'quantile'],\n    'learning_rate' : [0.1,0.2, 0.5],\n    'n_estimators' : [180, 200,300],\n    'criterion' : ['friedman_mse', 'mse', 'mae'],\n}\n\nparameter_space_RF = {\n    'n_estimators' : [100,120],\n    'criterion' : ['mse', 'mae'],\n    'max_depth' : [10,15,30],\n}","d8e4b616":"from sklearn.model_selection import GridSearchCV\n\n\nLG = LinearRegression()\nSV = SVR()\nKN = KNeighborsRegressor()\nDT = DecisionTreeRegressor(random_state=123)\nGB = GradientBoostingRegressor(random_state=123)\nRF = RandomForestRegressor(random_state=123)\n\nmodels = [LG, SV, KN, DT, GB, RF,]\nmodel_name = [ 'Linear Regression', 'Support Vector Regression', 'K Nearest Neighbor', \n              'Decision Tree', 'Gradient Boost', 'Random Forest' ]\nparameter_space = [parameter_space_LG, parameter_space_SV, parameter_space_KN, \n                  parameter_space_DT, parameter_space_GB, parameter_space_RF]\n\nfor i in range(6):\n    clf = GridSearchCV(models[i],parameter_space[i] , n_jobs=4,\n                   cv=None, scoring=\"neg_mean_absolute_error\")\n\n    clf.fit(train[features], train[target])\n    print(f'{model_name[i]}:')\n    print(\"Best parameters:\")\n    print(clf.best_params_)\n    print('')","187b16b7":"LG_ = LinearRegression(copy_X=True, fit_intercept=True, normalize=True, positive=False, )\nSV_ = SVR(C=100, coef0=7, degree=5, gamma=0.1, kernel='poly', )\nKN_ = KNeighborsRegressor(algorithm='ball_tree', leaf_size=200,\n                         n_neighbors=10, p=1, weights='distance')\nDT_ = DecisionTreeRegressor(criterion='mae', max_depth=5, splitter='best', random_state=123)\nGB_ = GradientBoostingRegressor(criterion='mse', learning_rate=0.1, \n                                loss='huber', n_estimators=200, random_state=123)\nRF_ = RandomForestRegressor(criterion='mae', max_depth=15, n_estimators=100, random_state=123)\n\nmodels = [LG_, SV_, KN_, DT_, GB_, RF_,]\nmodel_name = [ 'Linear Regression', 'Support Vector Regression', 'K Nearest Neighbor', \n              'Decision Tree', 'Gradient Boost', 'Random Forest' ]\n\n\nmeans = []\nr2_score_ = []\nkf = KFold(5, True, random_state=123)\n\nfor i in range(len(models)):\n    maes = []\n    r2s = []\n    model = models[i]\n\n    for train_index, test_index in kf.split(data):\n        train = data.iloc[train_index]\n        test = data.iloc[test_index]\n        model.fit(train[features], train[target])\n        prediction = model.predict(test[features])\n        mae = mean_absolute_error(prediction, test[target])\n        r2 = r2_score(test[target], prediction)\n        maes.append(mae)\n        r2s.append(r2)\n        \n    means.append(np.mean(maes))\n    r2_score_.append(np.mean(r2s))\n    \n    \nmod_comp = pd.DataFrame({'Models' : model_name, 'Mean Absolute Error' : means,\n                            'R2_Score' : r2_score_}).set_index('Models')\nmod_comp","dc73303e":"# MAES with default parameters\nmod_comp_def","f0950a7a":"mod_comp = mod_comp.sort_values('Mean Absolute Error')\nmod_comp_def = mod_comp_def.sort_values('Mean Absolute Error')","062eac0d":"mod_comp_def","c608c10a":"final = mod_comp.copy()\nfinal['Mean Absolute Error_Before'] = mod_comp_def['Mean Absolute Error']\nfinal['R2_Score_Before'] = mod_comp_def['R2_Score']\nfinal.reset_index(inplace=True)\nfinal","51db7377":"sns.set(font_scale=1.5)\nmy_ticks = ['Gradient Boost', 'Random Forest', 'SVR', 'KNN', 'Linear Reg.', 'Decision Tree']\n\nmylegends = ['Mean Absolute Error_After', 'Mean Absolute Error_Before']\nax = final[['Mean Absolute Error', 'Mean Absolute Error_Before']].plot.bar(figsize=(15,9), \n                                                                           color = ['SteelBlue', 'SeaGreen'])\nax = final['Mean Absolute Error'].plot(ls='--', lw=3, marker='o', color='SteelBlue')\nax = final['Mean Absolute Error_Before'].plot(ls='-.', lw=3, marker='o', color='SeaGreen')\nax.set_xticklabels(my_ticks)\nax.legend(title='MAE', labels=mylegends)\n\nplt.xticks(rotation=-30)\n\nplt.show()","54ccb99c":"sns.set(font_scale=1.5)\nmy_ticks = ['Gradient Boost', 'Random Forest', 'SVR', 'KNN', 'Linear Reg.', 'Decision Tree']\n\nmylegends = ['R2_Score_After', 'R2_Score_Before']\nax = final[['R2_Score', 'R2_Score_Before']].plot.bar(figsize=(15,12), color = ['LightSalmon', 'Teal'])\nax = final['R2_Score'].plot(ls='--', lw=3, marker='o', color = 'DarkSalmon')\nax = final['R2_Score_Before'].plot(ls='-.', lw=3, marker='o', color='Teal')\nax.set_xticklabels(my_ticks)\nax.legend(title='R2_Score', labels=mylegends)\n\nplt.xticks(rotation=-30)\n\nplt.show()","faecbc6b":"#### **As you can see that after parameters' hypertuning Gradient Boost performs best among other regression models, having lowest Mean Absolute Error and Highest r2_Score**","f66e8de6":"Here we will be using GridSearchCV of SciKit Learn Library to find out the better parameter values for respective models which give us the optimum result.","251faa6e":"**Splitting the Data into 75% and 25%**","825d94c1":"### **Standardizing the Data (Normalization)**","8ac6c6c6":"#### **1 - Dropping Features having Null values greater than 5% of dataset size**","58a586cf":"## **Hyperparameter Tuning**","1aba0d17":"### **Data Cleaning**","64cc57b7":"We will be deleting the following features from our dataset.","540ff02f":"**We'll be dropping features where there is no or very little variation, b\/c these features are of no use for our model. Therefore we'll be keeping features only with variance greater than 0.01**","0ff42a5f":"### **Converting Object Feature into Numerical Form**","c655063e":"#### **r2_Score Comparison Before and After Hyperparmeter Tuning.**","dee81d81":"**Since we cannot perform standardization to our target label ('SalePrice'). Therefore for the time being we are saving it into a variable, then we'll replace it with the original one**","32eea3b4":"### **Checking Variance**","0cc0d3e5":"#### **2 - Handling Numerical Features**","4eb44ece":"**2.3-Filling them with Mode**","7340d55a":"##### **Merging Both Results**","1bd47533":"## **Handling Missing Values**","9f1b7a85":"### **Loading and Understanding the Data**","d69673ed":"##### **Applying these combination in our model using GridsearchCV**","2471b4b8":"### **Model Comparision with Default Parameters**","ada5bdcb":"**2.1-Checking Numerical Features with Null Values**","c6837191":"These three rows must be dropped b\/c they should not be negative.","84e1d0dc":"#### **Linear Regression**","ff890ace":"**2.2-Finding out the Mode for each Feature**","ef9861a9":"**Saving into New Column**","6362943b":"### **K-Fold Validation**","67c988e1":"**Finding out Uniques Values for each column**","b6b404ef":"##### **Checking Duplicates if Any**","1b5856fb":"- We are more interested in **'Years Before Sale'** and **'Years Since Remodelled**' of a car rather than **'Year Built'**, **'Yr Sold'** or **'Year Remod\/Add'**.\n- Because before purchasing we are more concerned about **'how much time has passed after remodelling of car'** or **'how many years the car has passed before it was sold'**. So for that we are transforming into our required form.","549c3fc7":"#### **3 - Handling Non-Numerical (Object) Features**","ce6f7151":"**Now we will be comparing different regression models with their default parameters along with k-fold validation.**","3c23299c":"'Year Built' will always be smaller than 'Yr Sold', which means 2180 row has wrong values. So we'll have to drop it.","4e7d3f0f":"**We can also see straight away some leaking feature i.e. we have some columns which are leaking our target values. Which may result in bad prediction because of the seasonality present in them. Since we are looking towards modelling a general price prediction model, so we better drop these features**","8a54fcd1":"**We have to remove any one feature from each pair.**","3dd6ab16":"##### **After hyperparameter tuning we can see that:**\n- Performance of each model except Linear Regression has been improved.\n- SVR performance has improved alot.\n- Among all models that we have used in this ML, Gradient Boost has performed the best with minimum MAE and highest r2_Score.","c4ded9a1":"**We've decided to take the features having correlation with SalePrice > 0.25**","4f64395b":"#### **Dropping Columns Having Unique Values Greater than 10**","7f3c9bdb":"**'Order' is just representing the row order, we don't need it in our dataset**","7d6a25c3":"**Since the size of our dataset is not too big, therefore we should apply k-fold validation method**","bf374142":"After performing Grid Search to our parameters combinations, we can conclude that:\n- Linear Regression will have highest accuracy with parameters: {'copy_X': True, 'fit_intercept': True, 'normalize': True, 'positive': False}\n- Support Vector Regression will have its highest accuracy among the given combination with parameters taken as: {'C': 100, 'coef0': 7, 'degree': 5, 'gamma': 0.1, 'kernel': 'poly'}\n- K Nearest Neighbor's optimum parameterss are: {'algorithm': 'ball_tree', 'leaf_size': 200, 'n_neighbors': 10, 'p': 1, 'weights': 'distance'}\n- Decision Tree has following optimum parameter values: {'criterion': 'mae', 'max_depth': 5, 'splitter': 'best'}\n- For Gradient Boost to perform best, the parameters will be: {'criterion': 'mse', 'learning_rate': 0.1, 'loss': 'huber', 'n_estimators': 200}\n- Random Forest will be having its best performance with parameters: {'criterion': 'mae', 'max_depth': 15, 'n_estimators': 100}","b3c16bf4":"**Below are the features' pair having correlation greater than 0.78**","24bd36a2":"### **- Using GridSearchCV**","d8626ed0":"#### **Getting Information from Data**","7ecc026d":"### **Removing other Unnecessary Features**","a5c7f496":"**Shuffling the DataSet before splitting**","1ecbf594":"#### **Mean Absolute Error Comparison Before and After Hyperparameter Tuning**","4879c2d6":"#### **Plotting Correlation wrt to SalePrice (Target Label)**","019965eb":"**Removing Columns with Unique Values greater than 10**","aae583c8":"**Removing Old Features**","85533e0d":"Below we are defining various combinations","cdbedff8":"### **Checking Multi-Collinearity**"}}