{"cell_type":{"5b6e1b09":"code","6f5b08c7":"code","21d435c2":"code","5c8a0986":"code","649f94dd":"code","c036e9a1":"code","c30f8a64":"code","12a11eaa":"code","40cab006":"code","dfa0c7f6":"code","a41aa659":"code","84e76112":"code","a86faeb8":"code","0ff17c6b":"code","913ab2a6":"code","89352c4e":"code","e9d9015a":"code","32e8c394":"code","407d5650":"code","6b459696":"code","8fae4097":"code","bab9669b":"code","1271bf49":"code","2537c7aa":"code","a711820d":"code","cd16bb00":"code","62a72334":"code","437e3622":"code","d5fe17ee":"code","97c390b1":"code","a5f7db79":"code","e9d1025a":"code","7c83ebdd":"code","e325f454":"markdown","a6b253a8":"markdown","62d19623":"markdown","989348c0":"markdown","2322c628":"markdown","4677d497":"markdown","8f4fee0c":"markdown","bc7538d5":"markdown","354f40f4":"markdown","83cac5ad":"markdown","c5340b70":"markdown","4557b191":"markdown","d590fe60":"markdown","081a9aa0":"markdown","b4b2935b":"markdown","c539ed9f":"markdown","daf23e6d":"markdown","f4635fec":"markdown","6e396feb":"markdown","f8d44306":"markdown","476da2dc":"markdown","2a92fcaa":"markdown","825a1ad4":"markdown","c1766edc":"markdown","ad331553":"markdown","8ba87ab1":"markdown","56415a9d":"markdown","573a2668":"markdown","1cde408f":"markdown","ae52ba58":"markdown","cdcd0665":"markdown"},"source":{"5b6e1b09":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np \nimport pandas as pd \n\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.feature_selection import RFE\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score \nfrom sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve, auc, log_loss\n\nimport matplotlib.pyplot as plt \nplt.rc(\"font\", size=14)\n\nimport seaborn as sns\nsns.set(style=\"white\")\nsns.set(style=\"whitegrid\", color_codes=True)\n\nimport warnings\nwarnings.simplefilter(action='ignore')\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\n# Drop a column from a DataFrame, where it exists\ndef dropColumnsFromDataFrame(dataFrame,columnNames=[]):\n    for columnName in columnNames:\n        if columnName in dataFrame.columns: \n            dataFrame.drop(columnName,axis=1,inplace=True)\n\n# Annotate plot -  utility function\ndef annotatePlot(label,value,plt,ax,pointPos,textPos,color='black'):\n    \"\"\"Return x plus y, optional\"\"\"\n    ax.annotate(label, xy=pointPos, xytext=textPos,\n            arrowprops=dict(facecolor='black', shrink=0.05),\n            )\n    # vertical dotted line originating at the value\n    plt.axvline(value, linestyle='dashed', linewidth=2,color=color)","6f5b08c7":"# Read in training and test data CSV\ntrain_csv = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_csv = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntrain_csv.head()","21d435c2":"#Number of empty value by field \ntrain_csv.isnull().sum()","5c8a0986":"# Percentage of missing values\ntrain_csv.isnull().sum()*100\/train_csv.shape[0]","649f94dd":"# Plot Age to get a feel its underlying distribution\nplt.figure(figsize=(20,9))\nage = train_csv['Age']\nax = age.hist(bins=20,density=True,stacked=True,color='teal',alpha=0.6)\nage.plot(kind='density',color='teal')\nax.set(xlabel = 'Age')\nmean_age_val = age.mean(skipna=True)\nmedian_age_val = age.median(skipna=True)\nannotatePlot(f\"mean: {mean_age_val:.2f}\",mean_age_val,plt,ax,color='blue',textPos=(mean_age_val+10, 0.010),pointPos=(mean_age_val, 0.008))\nannotatePlot(f\"median: {median_age_val:.2f}\",median_age_val,plt,ax,color='red',textPos=(median_age_val-30, 0.008),pointPos=(median_age_val, 0.004))\nplt.xlim(-5,90)\nplt.show()","c036e9a1":"# Before we make any changes to the data, let's make a copy of the training data\ntraining_data = train_csv.copy()","c30f8a64":"# Replace missing age values with the median age.\ntraining_data[\"Age\"].fillna(median_age_val,inplace=True)","12a11eaa":"# Plot the adjusted Age\nplt.figure(figsize=(20,9))\nage = training_data['Age']\nax = age.hist(bins=20,density=True,stacked=True,color='orange',alpha=0.6)\nage.plot(kind='density',color='orange')\nax.set(xlabel = 'Age - After')\nmean_age_val = age.mean(skipna=True)\nmedian_age_val = age.median(skipna=True)\nannotatePlot(f\"mean: {mean_age_val:.2f}\",mean_age_val,plt,ax,color='red',textPos=(mean_age_val+10, 0.010),pointPos=(mean_age_val, 0.008))\nannotatePlot(f\"median: {median_age_val:.2f}\",median_age_val,plt,ax,color='green',textPos=(median_age_val-30, 0.008),pointPos=(median_age_val, 0.004))\nplt.xlim(-5,90)\nplt.show()","40cab006":"# Plot age distributions from before and after replacing missing values with the median age.\n#Excluding missing values\nplt.figure(figsize=(20,9))\nax = train_csv['Age'].hist(bins=20,density=True,stacked=True,color='teal',alpha=0.4)\ntrain_csv['Age'].plot(kind='density',color='teal')\n#Missing values replaced\nax = training_data['Age'].hist(bins=20,density=True,stacked=True,color='orange',alpha=0.6)\ntraining_data['Age'].plot(kind='density',color='orange')\nax.legend(['Age - Before','Age - After'])\nax.set(xlabel = 'Age')\nplt.xlim(-5,90)\nplt.show()","dfa0c7f6":"# Drop the Cabin colum\ndropColumnsFromDataFrame(columnNames=['Cabin'],dataFrame=training_data)\ntraining_data.head() ","a41aa659":"# Drop the Ticket and Name colum\ndropColumnsFromDataFrame(columnNames=['Ticket','Name'],dataFrame=training_data)\ntraining_data.head()","84e76112":"# Create categorical columns from the **Sex** column. This will create a column for each unique value in the **Sex** column, one for 'male' and one for 'female'. \n# These two new columns will have a 0 or 1 value depending on whether the passenger in question was male or female.\ntraining_data = pd.get_dummies(training_data,columns=['Sex'],prefix=['Sex'])\ntraining_data.head()","a86faeb8":"# Drop the 'Sex_male' column\ndropColumnsFromDataFrame(columnNames=['Sex_male'],dataFrame=training_data)\ntraining_data.rename(columns={'Sex_female':'Female'},inplace=True)\ntraining_data.head()","0ff17c6b":"# If the number of parents and siblings that a person had on the boat was greater than 0, then they weren't travelling alone. \n# Next we create a new column, **Travelling_Alone** to tell us if the passenger was travelling with family.\ntraining_data['Travelling_Alone'] = training_data.apply(lambda x:  1 if x['SibSp'] + x['Parch'] > 0 else 0, axis=1)\ntraining_data.head()","913ab2a6":"# Plot % of passengers survived vs. passenger travelling alone (0: false, 1: true)\nplot = sns.barplot(\"Travelling_Alone\",\"Survived\",data=training_data)","89352c4e":"# Plot % of passengers survived vs. passenger travelling alone (0: false, 1: true)\nplot = sns.barplot(\"Travelling_Alone\",\"Survived\",hue='Female' ,data=training_data)\n# Relabel the legend\nplot.legend_.set_title('Sex')\nnew_labels = ['Male', 'Female']\nfor t, l in zip(plot.legend_.texts, new_labels): t.set_text(l)","e9d9015a":"# Plot % of passengers survived vs. sex\nplot = sns.barplot(\"Sex\",\"Survived\",data=train_csv)","32e8c394":"# Finally we drop the 'SibSp' and 'Parch' columns\ndropColumnsFromDataFrame(columnNames=['SibSp','Parch'],dataFrame=training_data)\ntraining_data.head()","407d5650":"# Create categorical columns from the **Pclass** column. This will create a column for each unique value in the **Pclass** column; passengers could hold a class 1,2 or 3 ticket. \n# These two new columns will have a 0 or 1 value depending on whether the passenger in question held a ticket of that class.\ntraining_data = pd.get_dummies(training_data,columns=['Pclass'],prefix=['Pclass'])\ntraining_data.head()","6b459696":"# Plot total number of passengers that embarked at each of the three ports. \nplot = sns.countplot(\"Embarked\",data=training_data)","8fae4097":"# Replace any empty 'Embarked' fields with Southhampton ('S')\ntraining_data['Embarked'].fillna('S',inplace=True)\n# Create one category per port \ntraining_data = pd.get_dummies(training_data,columns=['Embarked'],prefix=['Embarked'])\n# We can also drop the 'Embarked' column as we don't need it any more.\ndropColumnsFromDataFrame(columnNames=['Embarked'],dataFrame=training_data)\ntraining_data.isnull().sum()","bab9669b":"# Density plot of the ages of passengers that either survived or died.\nplt.figure(figsize=(20,9))\nax = sns.kdeplot(training_data['Age'].where(training_data['Survived']==1),shade=True)\nax = sns.kdeplot(training_data['Age'].where(training_data['Survived']==0),shade=True)\nplt.legend(['Survived','Died'])\nplt.title('Density plot of passenger ages for those that either survived or died.')\nax.set(xlabel='Age')\nplt.show()","1271bf49":"# Plot the survival rate for each passenger age group\nplt.figure(figsize=(20,9))\nbins = [0,3,12,17,35,55,training_data.Age.max()]\nlabels = ['infant','child','adolescent','young adult','adult','elderly']\nage_groups = pd.cut(training_data.Age, bins=bins,labels=labels)\ntraining_data['Age_Group'] = age_groups\nax = sns.barplot(x='Age_Group',y='Survived',data=training_data)\nplt.show()","2537c7aa":"# Create a new field 'Age_Lte_17' for all passengers of age 17 or less.\ntraining_data['Age_Lte_17'] = np.where(training_data.Age <= 17,1,0)\n# We can also drop the 'Age_Group' column as we don't need it any more.\ndropColumnsFromDataFrame(columnNames=['Age_Group'],dataFrame=training_data)\ntraining_data.head()","a711820d":"#Number of empty value by field\ntesting_data = test_csv.copy()\ntesting_data.isnull().sum()","cd16bb00":"dropColumnsFromDataFrame(columnNames=['Cabin','Name','Ticket'],dataFrame=testing_data)\ntesting_data.head()","62a72334":"# Create a new field 'Age_Lte_17' for all passengers of age 17 or less.\ntesting_data['Age_Lte_17'] = np.where(testing_data.Age <= 17,1,0)\n# Create one category per port \ntesting_data = pd.get_dummies(testing_data,columns=['Embarked'],prefix=['Embarked'])\n# Next we create a new column, **Travelling_Alone** to tell us if the passenger was travelling with family.\ntesting_data['Travelling_Alone'] = testing_data.apply(lambda x:  1 if x['SibSp'] + x['Parch'] > 0 else 0, axis=1)\n# Drop the 'SibSp' and 'Parch' columns\ndropColumnsFromDataFrame(columnNames=['SibSp','Parch'],dataFrame=testing_data)\n# Create categorical columns from the **Sex** column. This will create a column for each unique value in the **Sex** column, one for 'male' and one for 'female'. \n# These two new columns will have a 0 or 1 value depending on whether the passenger in question was male or female.\ntesting_data = pd.get_dummies(testing_data,columns=['Sex'],prefix=['Sex'])\n# Drop the 'Sex_male' column\ndropColumnsFromDataFrame(columnNames=['Sex_male'],dataFrame=testing_data)\n# Rename the 'Sex_female' column to 'Female'\ntesting_data.rename(columns={'Sex_female':'Female'},inplace=True)\n# Replace missing age values with the median age.\ntesting_data.Age.fillna(testing_data.Age.median(),inplace=True)\n# Create categorical columns from the **Pclass** column. This will create a column for each unique value in the **Pclass** column; passengers could hold a class 1,2 or 3 ticket. \n# These two new columns will have a 0 or 1 value depending on whether the passenger in question held a ticket of that class.\ntesting_data = pd.get_dummies(testing_data,columns=['Pclass'],prefix=['Pclass'])\ntesting_data.head()","437e3622":"# Train the model using recursive feature elimination\nfeatures = ['Age','Fare','Age_Lte_17','Embarked_C','Embarked_S','Travelling_Alone','Female','Pclass_1','Pclass_2']\nX = training_data[features]\ny = training_data['Survived']\n# Create a model\nmodel = LogisticRegression()\n# Use 'recursive feature elimination' to select only 8 features to use when training our model\n# Specify how many features we want to train our model on\nrfe = RFE(model,8)\nrfe = rfe.fit(X,y)\n# Show the selected features\nprint(\"Optimal number of features : %d\" % rfe.n_features_)\nprint(\"Selected features : %s\"%list(X[features].columns[rfe.support_]))","d5fe17ee":"# Train the model using recursive feature elimination with cross validation\nrfecv = RFECV(estimator=LogisticRegression(), step=1, cv=5,scoring='accuracy')\nrfecv.fit(X, y)\nselected_features = list(X[features].columns[rfecv.support_])\nprint(\"Optimal number of features : %d\" % rfecv.n_features_)\nprint(\"Selected features : %s\"%selected_features)\n# Plot number of features VS. cross-validation scores\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score (nb of correct classifications)\")\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.show()","97c390b1":"# Train the model using the selected features and 'train'\/'test' split\n# Prepare our data  \nX = training_data[selected_features]\ny = training_data['Survived']\n#  Split the training data into 'train'\/'test' groups\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n# Train model using 'train' data\nmodel = LogisticRegression()\nmodel.fit(X_train,y_train)\ny_prediction = model.predict(X_test)\ny_prediction_probability = model.predict_proba(X_test)[:,1]\nmodel_name = model.__class__.__name__\nprint(\"%s accuracy %.4f\"%(model_name,accuracy_score(y_test,y_prediction)))\nprint(\"%s precision %.4f\"%(model_name,precision_score(y_test,y_prediction)))\nprint(\"%s recall %.4f\"%(model_name,recall_score(y_test,y_prediction)))\nprint(\"%s log_loss %.4f\" % (model_name,log_loss(y_test, y_prediction_probability)))\n\n# R.O.C. Curve\nfpr, tpr, thresholds =  roc_curve(y_test,y_prediction_probability)\nprint(\"%s auc %.4f\" % (model_name,auc(fpr,tpr)))\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr, color='darkorange',\n         lw=lw, label='R.O.C. curve (area = %0.2f)' % auc(fpr,tpr))\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic (R.O.C)')\nplt.legend(loc=\"lower right\")\nplt.show()","a5f7db79":"# Logistic Regression with 10-fold cross validation\nmodel = LogisticRegression()\nscoring = {'accuracy': 'accuracy', 'log_loss': 'neg_log_loss', 'auc': 'roc_auc'}\nscores = cross_validate(model, X, y, cv=10,scoring=list(scoring.values()))\nmodel_name = model.__class__.__name__\nfor sc in range(len(scoring)):\n    print(model_name+\" mean %s: %.4f +\/-%.4f\" % (\n        list(scoring.keys())[sc], \n        -scores['test_%s' % list(scoring.values())[sc]].mean() if list(scoring.values())[sc]=='neg_log_loss' else scores['test_%s' % list(scoring.values())[sc]].mean(),\n        scores['test_%s' % list(scoring.values())[sc]].std()))","e9d1025a":"# Set up the logistic regression function\nC = np.arange(1e-05, 5.5, 0.1)\nscoring = {'Accuracy': 'accuracy', 'AUC': 'roc_auc', 'Log_loss': 'neg_log_loss'}\nlogreg = LogisticRegression()\n# Set up a standard scaler\nstandard_scaler = StandardScaler(with_mean=False, with_std=False)\n# Define stratified k-folds strategy\nn_splits=5\nn_repeats=5\nrandom_state=2\nrepeatedStratifiedKFolds = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=random_state)\n# Create a pipeline\npipeline = Pipeline(steps=[('scale',standard_scaler), ('clf',logreg)])\n# Select the optimal model using GridSearch\nmodel = GridSearchCV(estimator=pipeline, cv=repeatedStratifiedKFolds,\n              scoring=scoring, return_train_score=True,\n              param_grid=dict(clf__C=C), refit='Accuracy')\n# Fit the model\nmodel.fit(X, y)\nresults = model.cv_results_\n# Show the optimal estimator, its parameters and score \nprint(\"Best estimator is: \" + str(model.best_estimator_))\nprint(\"Best parameters are: \" + str(model.best_params_))\nprint('Best Score:', model.best_score_)","7c83ebdd":"testing_data['Survived'] = model.predict(testing_data[selected_features])\nsubmission = testing_data[['PassengerId','Survived']]\nsubmission.to_csv(\"submission.csv\", index=False)\nsubmission.tail()","e325f454":"What the above shows us is that of the infant group, more were more likely to survive than not. Conversely, young adults in the approximate age range of 25-35 were more likely to perish than not.   ","a6b253a8":"Let's see what the survival outcomes for passengers look like when contrasted against the size of their entourage.  ","62d19623":"Again, we can drop the **Cabin**,**Ticket** and **Name** columns.","989348c0":"<a id=\"t2.3.\"><\/a>\n## 2.3. Ticket and Name\nWe can take a similar approach with the **Ticket** and **Name** fields.","2322c628":"As expected, the median age of 28 is unchanged, however the mean has been reduced marginally, from 29.7 to 29.36. Finally, we can compare age distributions from before and after our change. ","4677d497":"<a id=\"t3.1.\"><\/a>\n## 3.1 Recursive Feature Elimination ","8f4fee0c":"<a id=\"t2.9.\"><\/a>\n## 2.9. Test Data\nGreat, I think that's enough work on the training data. Let's check out our test data.","bc7538d5":"<a id=\"t2.5.\"><\/a>\n## 2.5. Parents and Siblings\nMy deep, varied experience with all things Titanic tells me that any 'Rose' without a 'Jack' probably had a hard time getting off the sinking vessel; In other words, if you were travelling alone, you were less likely to have made it off the boat. Let's test out my hair brained theory!  ","354f40f4":"<a id=\"t3.4.\"><\/a>\n## 3.4 K-folds Cross Validation","83cac5ad":"Clearly, most passengers boarded in Southhampton. Let's update any missing entries in **Emabrked** with Southhampton and convert the **Embarked** column into three new categorical columns, one for each port.","c5340b70":"Next, we need to do something about the missing Cabin values. Here, 77% of the passenger records are missing a value for **Cabin** . It's a reasonable assumption that the  **Class** of cabin and the **Fare** paid are highly correlated with the **Cabin** occupied by each passenger. As a result, we can safely drop the **Cabin** field altogether from our training data set.  ","4557b191":"<a id=\"t2.4.\"><\/a>\n## 2.4. Sex\nNext we'll tackle the **Sex** column. Here we have two values 'male' and 'female'. What we'd like is a categorical field, that is a field with 0s or 1s. We can easily create such columns from any other column in a data frame by using the [pandas.get_dummies()](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.get_dummies.html) function. ","d590fe60":"Now that we've replaced missing age values with the median age, let us see what the new age distribution looks like.","081a9aa0":"It's obvious from the above that female passengers were much more likely to survive irrespective of whether they were travelling alone or with family. Female were much more likely to survive, as we can see below.","b4b2935b":"Because the mean of the age distribution is shifted to the right, replacing missing age values with the mean will potentially skew the distribution of the data even further to the right. Instead we should use the median instead, noting that the median sits slightly to the left of the mean.","c539ed9f":"<a id=\"t2.7.\"><\/a>\n## 2.7. Port of Embarkment\nWe should now examine the embarkement data; 0.224 % of passenger records did not record an emabarkement port. Let's see what the data in hand looks like. Given that we're only missing entries for a small number of passengers, we'll assume that those with no port recorded boarded at the most common port.  ","daf23e6d":"Contrary to my initial hypothesis, we can see that people who were travelling alone were ***more** likely* to have survived than those who were accompanied. Let's see if a passenger's **Sex** further affected their odds of survival.    ","f4635fec":"# **Introduction**\n\n1. [Import Data and Packages](#t1.)\n2. [Data Analysis and Synthesis](#t2.)\n    * 2.1. [Age](#t2.1.)\n    * 2.2. [Cabin](#t2.2.)\n    * 2.3. [Ticket and Name](#t2.3.)\n    * 2.4. [Sex](#t2.4.)\n    * 2.5. [Parents and Siblings](#t2.5.)\n    * 2.6. [Passenger Class](#t2.6.)\n    * 2.7. [Port of Embarkment](#t2.7.)\n    * 2.8. [Age](#t2.8.)\n    * 2.9. [Test Data](#t2.9.)\n3. [Logistic Regression](#t3.)\n    * 3.1. [Recursive Feature Elimination](#t3.1.)\n    * 3.2. [Recursive Feature Elimination with Cross Validation](#t3.2.)\n    * 3.3. [Train-Test Split](#t3.3.)\n    * 3.4. [K-folds Cross Validation](#t3.4.)\n    * 3.5. [GridSearchCV, Repeated Stratified K-folds with Preprocessing Pipeline](#t3.5.)","6e396feb":"<a id=\"t3.5.\"><\/a>\n## 3.5 GridSearchCV, Repeated Stratified K-folds with Preprocessing Pipeline","f8d44306":"<a id=\"t3.3.\"><\/a>\n## 3.3 Train-Test Split","476da2dc":"<a id=\"t1.\"><\/a>\n# 1. Import Data and Packages","2a92fcaa":"<a id=\"t3.\"><\/a>\n# 3. Logistic Regression\nOk, let's get to building our predictive model using [logistic regression](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html)!","825a1ad4":"Here's another way to look at passenger survival, this time by age group. Again, what stands out is the high survival rate of infants,children and adolescents and the slightly depressed survival rate for young adults and the elderly. The relatively small size of the whiskers on the young adult cohort indicates a higher confidence in the survival estimate for that groups. It's worth creating a new category in our data: 'Age_Lte_17' (Age Less Than or Equal to 17) to cover infants,children and adolescents.","c1766edc":"<a id=\"t2.1.\"><\/a>\n## 2.1. Age","ad331553":"<a id=\"t2.6.\"><\/a>\n## 2.6. Passenger Class\nThe **Pclass** column can also be transformed into a 3 categorical columns, one per ticket class. ","8ba87ab1":"We'll then make all the remaining changes that we made to our training data set.","56415a9d":"<a id=\"t2.8.\"><\/a>\n## 2.8. Age\nGreat, so there are no more empty fields in any column in our training data.Before we finish up, I'd like to see how passenger survival varied by passenger age. It's not unreasonable to think that extremely young or old passengers may have been had very different survival odds when compared to the average. ","573a2668":"<a id=\"t3.2.\"><\/a>\n## 3.2 Recursive Feature Elimination with Cross Validation","1cde408f":"<a id=\"t2.\"><\/a>\n# 2. Data Analysis and Synthesis","ae52ba58":"<a id=\"t2.2.\"><\/a>\n## 2.1. Cabin","cdcd0665":"Notice the two new colums that have been created - **Sex_female** and **Sex_male**. We can now drop either the **Sex_male** or **Sex_female** column. It's enough to know if a person was or wasn't either one of female or male."}}