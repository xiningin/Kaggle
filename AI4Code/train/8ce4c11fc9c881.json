{"cell_type":{"955e7b26":"code","1139dfd4":"code","0d27493e":"code","f6c8695b":"code","8a0d716b":"code","7d9a427e":"code","f365593e":"code","0bc7d265":"code","7abc4f1a":"code","393f2101":"code","5ebc7b2c":"code","9f44385f":"code","e0dcab43":"code","62208157":"code","0b632d54":"code","9e91eabc":"markdown","e0aaef4d":"markdown","2082517e":"markdown","ab0f98c4":"markdown","dd028b16":"markdown","ddadd5e8":"markdown","2596f184":"markdown","ef93d371":"markdown","4b84c570":"markdown","917c5ac0":"markdown","c48e51c4":"markdown"},"source":{"955e7b26":"# Import and read Metadata\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom os import listdir\nfrom os.path import isfile, join\nfrom os import listdir\nfrom tqdm import tqdm\nimport numpy as np\n\n# Key Data Paths\ndata_path = '..\/input\/CORD-19-research-challenge'\n# data_path = '.\/551982_1230614_bundle_archive\/'\npriority_question_path = data_path + '\/Kaggle\/target_tables\/2_relevant_factors\/'","1139dfd4":"# Loading the meta data and relative questions path\n\nmeta_data=pd.read_csv(data_path + '\/metadata.csv')\nprint(\"Column names: {}\".format(meta_data.columns))\nprint(\"number of rows: \", len(meta_data))\nmeta_data.head(5)","0d27493e":"plt.figure(figsize=(20,10))\nna_analysis = meta_data.isna().sum()\nna_analysis.sort_values().plot(kind='bar', stacked=True, x = 'columns', y = 'count')","f6c8695b":"question_1 = 'Effectiveness of a multifactorial strategy to prevent secondary transmission.csv'\npriority_question_1 =pd.read_csv(priority_question_path + question_1,index_col=0)\nprint(\"Column names: {}\".format(priority_question_1.columns))\nprint(\"number of rows: \", len(priority_question_1))\npriority_question_1.head(10)\npriority_question_1.shape","8a0d716b":"# Testing match between metadata and question 1\nmatch_list = []\nfor index, row in priority_question_1.iterrows():\n    url_row = meta_data.loc[meta_data['url'] == row['Study Link']]['url'].any()\n    title_row = meta_data.loc[meta_data['title'] == row['Study']]['title'].any()\n    if url_row != False or title_row != False:\n        match_list.append(True)\n    else:\n        match_list.append(False)\nprint(\"percentage of matched list is\", sum(match_list), \"\/\", len(match_list))","7d9a427e":"# Testing the duplication relationship between priority question 1 and literatures\nduplicateRowsDF = priority_question_1[priority_question_1.duplicated(['Study'])]\nprint(\"Duplicate Rows based on a single column are:\", duplicateRowsDF, sep='\\n')","f365593e":"# Listing all the priority questions \npriority_question_list = [f for f in listdir(priority_question_path) if isfile(join(priority_question_path, f))]\nfor question in priority_question_list:\n    match_list = []\n    priority_question_df = pd.read_csv(priority_question_path + question,index_col=0)\n    for index, row in priority_question_df.iterrows():\n        url_row = meta_data.loc[meta_data['url'] == row['Study Link']]['url'].any()\n        title_row = meta_data.loc[meta_data['title'] == row['Study']]['title'].any()\n        if url_row != False or title_row != False:\n            match_list.append(True)\n        else:\n            match_list.append(False)\n    print(question)\n    print(\"percentage of matched list is\", sum(match_list), \"\/\", len(match_list), \" = \", sum(match_list)\/len(match_list))","0bc7d265":"# Checking the quality of the csv file for each questions\nfor question in priority_question_list:\n    print(\"file name is: \",question)\n    priority_question_df = pd.read_csv(priority_question_path + question,index_col=0)\n    priority_question_df.info()","7abc4f1a":"# Obtaining all files names from all priority_question list into one dataframe\n# Metadata -> We want pdf_json_files, pmc_json_files\n\npdf_json_files_list = [];\npmc_json_files_list = [];\nresearch_topic_list = [];\ntopic_id_list = [];\ntopic_id = 1\ncombined_ques_literature = [];\nfor question in tqdm(priority_question_list):\n    print(\"file name is: \",question)\n    priority_question_df = pd.read_csv(priority_question_path + question,index_col=0)\n    combined_ques_literature.append(priority_question_df)\n    for index, row in priority_question_df.iterrows():\n        # Match using URL or title\n        url_match_row = meta_data.loc[meta_data['url'] == row['Study Link']]\n        title_match_row = meta_data.loc[meta_data['title'] == row['Study']]\n        if not url_match_row.empty or not title_match_row.empty:\n            if not url_match_row.empty:\n                pdf_json_files_list.append(url_match_row['pdf_json_files'].values[0])\n                pmc_json_files_list.append(url_match_row['pmc_json_files'].values[0])\n            else:\n                pdf_json_files_list.append(title_match_row['pdf_json_files'].values[0])\n                pmc_json_files_list.append(title_match_row['pmc_json_files'].values[0])\n        else:\n            pdf_json_files_list.append(\" \")\n            pmc_json_files_list.append(\" \")\n        topic_id_list.append(topic_id)\n        research_topic_list.append(question.split('.')[0])\n    topic_id = topic_id + 1\n\n# combining all datarfame into one big dataframe\ncombined_ques_literature = pd.concat(combined_ques_literature);","393f2101":"# Checking if previous result provides the correct dimension for the output\nprint(len(topic_id_list), len(pmc_json_files_list), len(pdf_json_files_list), len(research_topic_list))\nprint(\"before adding columns: \", combined_ques_literature.shape)\nprint(combined_ques_literature.columns)\ncombined_ques_literature.insert(1, \"topic_id\", topic_id_list, True)\ncombined_ques_literature.insert(2, \"research_topic\", research_topic_list, True)\ncombined_ques_literature.insert(3, \"pdf_json_files\", pdf_json_files_list, True)\ncombined_ques_literature.insert(4, \"pmc_json_files\", pmc_json_files_list, True)\n\nprint(\"after adding columns: \", combined_ques_literature.shape)","5ebc7b2c":"combined_ques_literature[['Influential','Infuential','Influential (Y\/N)']].info()\ncombined_ques_literature[['Factors', 'Factors Described']].info()\ncombined_ques_literature[['Date', 'Date Published']].info()","9f44385f":"combined_ques_literature['Influential'] = combined_ques_literature['Influential'].fillna(combined_ques_literature['Infuential'])\ncombined_ques_literature['Influential'] = combined_ques_literature['Influential'].fillna(combined_ques_literature['Influential (Y\/N)'])            \ncombined_ques_literature['Factors'] = combined_ques_literature['Factors'].fillna(combined_ques_literature['Factors Described'])\ncombined_ques_literature['Date'] = combined_ques_literature['Date'].fillna(combined_ques_literature['Date Published'])\ncombined_ques_literature = combined_ques_literature.drop(['Infuential', 'Influential (Y\/N)', 'Factors Described', 'Date Published'], axis=1)","e0dcab43":"print(combined_ques_literature.info())","62208157":"# Drop all the na and spaces on the pdf_json_file to make sure all labelled data is linked to a literature stored in database\n\nscoped_categorised_literature = combined_ques_literature.dropna(subset=['pdf_json_files'])\nscoped_categorised_literature = scoped_categorised_literature[~scoped_categorised_literature['pdf_json_files'].str.isspace()] \nscoped_categorised_literature.info()","0b632d54":"scoped_categorised_literature.to_pickle(\".\/1_scoped_cat_lit.pkl\")","9e91eabc":"### 1.2.2. Checking any N\/As in the priority questions list\nFrom the observations below, there are some missing values for certain questions in Measure of Evidence","e0aaef4d":"### 1.5. Exporting the cleansed and mapped data into a pickle file","2082517e":"### 1.3.2. Observation on Duplicated Rows\nMerging the Influential columns together will yield a complete 547 rows of non- null values. which is good. Same Approach will be peformed for observations","ab0f98c4":"## 1.3. Matching metadata and Priority question data by URL or the Title of the article\nTrying to find a match between the question file and the metadata to find whether the downloaded database contains the file, if not then web scraping might be required to obtain the information from the URL provided","dd028b16":"## 1.2. Priority Question (Target_table) Analysis\nKey Observations: This is a One to many relationship between a priority question to the literatures.\nA literature can be referenced multiple times within a question.\nThis observation provides us with the information that we will need to break down each document into sections","ddadd5e8":"### 1.3.1. Observation on merging metadata and priority questions\nBased on observations:\n1. the tables within priority questions are inconsistent with each other -> i.e. the columns are referring to the same thing\nFactors, Factors Described\nInfluential,Infuential,Influential (Y\/N)\n\n<br> Hence, next step would be to perform some cleaning to merge the duplicated rows ","2596f184":"### 1.2.1 Matching between Metadata and Priority data\nkey observation: Not all literatures referenced in Priority data is in the Metadata. We might need to acquire the literature from external sources","ef93d371":"# 1. Data Exploratory and Cleansing of Labelled Data\n**Prerequisite**: Please Download the CORD-19 Dataset at https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge\/tasks?taskId=882\n<br> **File name**: 551982_1230614_bundle_archive\n<br>**Key files \/ folders used for the Project:**\n1.     **Meta data**                : metadata.csv\n2.     **Priority Questions files** : Kaggle -> target_tables -> 2_relevant_factors\n3.     **Research Paper Locations**    : document_parses -> pdf_json & pmc_json","4b84c570":"## 1.1. Metadata Analysis \n<br> Key Observations: there are lots of missing values even from the source json files. This means that the database is not up to date and some literatures might require exteral resources i.e. web scrapping to obtain the information.\n<br> This could Impact on whether we will be able to collect enough information from this dataset","917c5ac0":"#### 1.2.1.1. Observation from above few observations can be made\n1. A literature can be repeated multiple times within each priority question - therefore the articles are segmented into sections for analysis\n2. matching meta data with question using both title and url as criteria -> if found then we will be able to identify the JSON within our database\n3. there is only one mismatch in priority_question1 that does not have the article within the database\n4. Number of rows for each topics are different - data imbalancing.\n","c48e51c4":"### 1.4. Scoping and finalising the Categorisation the data\nSince Not all rows contains the literature within the database, we will remove the labelled data for ones that does not have data in pdf_json_files. This is to simplify this solution for the purpose of machine learning\nThis can potentially be future work to develop a methodology for web scrapping techniques to obtain the literature from external sources outside CORD-19"}}