{"cell_type":{"6fbd7cf2":"code","37d65001":"code","398eaa23":"code","6e6ac82d":"code","7d29dc32":"code","407d8f34":"code","9d6d060a":"code","c9077116":"code","d0e55134":"code","599bc8d6":"code","0a2bd529":"code","336aa6e8":"code","d14b2e4d":"code","5b3ba792":"code","a1b53202":"code","ec8d1ec6":"code","10f88274":"code","5a9f066a":"code","cbf48c46":"code","d51ca6a2":"code","100ba37e":"code","4caddeea":"code","1e7e949b":"code","987b2fb7":"code","1dfefd52":"code","1d1c428c":"code","c9811b47":"markdown","ae3bcf81":"markdown","de1f031d":"markdown","1ad3cb8d":"markdown","79785462":"markdown","0957fc5e":"markdown","03d6baa7":"markdown","3ea8b2a7":"markdown","b6164806":"markdown","9829f4bb":"markdown","40cd006b":"markdown","7b0accfc":"markdown","23fd03e8":"markdown","1a916c40":"markdown","f286f485":"markdown","cde631f9":"markdown","02a7110e":"markdown"},"source":{"6fbd7cf2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# plotly\nimport plotly.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nfrom plotly import tools\n\n# matplotlib\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","37d65001":"# Reading the train and the test data.\ntrainData = pd.read_csv('..\/input\/train.csv')\ntestData = pd.read_csv('..\/input\/test.csv')\n\n# Displaying a sample of the train data to get more detailed info\ntrainData.head()","398eaa23":"trainData.describe()","6e6ac82d":"trainData.dtypes","7d29dc32":"trainData.apply(lambda x: x.isnull().any())","407d8f34":"pd.DataFrame({'percent_missing': trainData.isnull().sum() * 100 \/ len(trainData)})","9d6d060a":"pd.DataFrame({'percent_unique': trainData.apply(lambda x: x.unique().size\/x.size*100)})","c9077116":"# Names of the features extarcted from the data\nselFeatures = list(trainData.columns.values)\n# Removing the target variable from the column values\ntargetCol = 'Survived'\nselFeatures.remove(targetCol)\n\n# Removing features with unique values\nfor i in selFeatures:\n    if trainData.shape[0] == len(pd.Series(trainData[i]).unique()) :\n        selFeatures.remove(i)\n        \n# Removing features with high percentage of missing values\nselFeatures.remove('Cabin')","d0e55134":"import seaborn as sns\nsns.set(style=\"ticks\")\nplotFeatures = [x for x in selFeatures]\nplotFeatures.append(\"Survived\")\nsns.pairplot(trainData[plotFeatures], hue=\"Survived\")","599bc8d6":"targetClass = trainData.Survived.value_counts().values.tolist()\ndata = [go.Pie(labels=['Died','Survived'], values=targetClass,\n              hoverinfo='label+percent', textinfo='value')]\nlayout = dict(\n        title = \"Comparison of Classes (Died\/Survived)\",\n        autosize=False,\n        width=500,\n        height=500\n    )\n\nfig = dict(data=data, layout=layout)\niplot(fig)","0a2bd529":"def plotGraph(plotData,msg):\n    trace1 = go.Bar(\n    x=plotData.columns.values,\n    y=plotData.values[0],\n    name='No'\n    )\n    trace2 = go.Bar(\n        x=plotData.columns.values,\n        y=plotData.values[1],\n        name='Yes'\n    )\n    data = [trace1, trace2]\n    layout = dict(\n        title = msg,\n        xaxis= dict(title = plotData.columns.name),\n        yaxis= dict(title= 'Number of people'),\n        barmode='group',\n        autosize=False,\n        width=800,\n        height=500\n    )\n    fig = dict(data=data, layout=layout)\n    iplot(fig)","336aa6e8":"pclass = pd.crosstab([trainData.Survived], trainData.Pclass)\nplotGraph(pclass,'Survived based on Pclass')","d14b2e4d":"sex = pd.crosstab([trainData.Survived], trainData.Sex)\nplotGraph(sex, 'Survived based on sex')","5b3ba792":"embarked = pd.crosstab([trainData.Survived], trainData.Embarked)\nplotGraph(embarked, 'Survived based on embarked')","a1b53202":"SibSp = pd.crosstab([trainData.Survived], trainData.SibSp)\nplotGraph(SibSp, 'Survived based on SibSp')","ec8d1ec6":"Parch = pd.crosstab([trainData.Survived], trainData.Parch)\nplotGraph(Parch, 'Survived based on Parch')","10f88274":"def plotLine(plotData,msg):\n    trace1 = go.Scatter(\n    x=plotData.columns.values,\n    y=plotData.values[0],\n    mode='lines',\n    name='No'\n    )\n    trace2 = go.Scatter(\n        x=plotData.columns.values,\n        y=plotData.values[1],\n        mode='lines',\n        name='Yes'\n    )\n    data = [trace1, trace2]\n    layout = dict(\n        title = msg,\n        xaxis= dict(title = plotData.columns.name),\n        yaxis= dict(title= 'Number of people'),\n        autosize=False,\n        width=800,\n        height=500\n    )\n    fig = dict(data=data, layout=layout)\n    iplot(fig)","5a9f066a":"Age = pd.crosstab([trainData.Survived],trainData.Age)\nplotLine(Age,'Survival based on Age')","cbf48c46":"Fare = pd.crosstab([trainData.Survived],trainData.Fare)\nplotLine(Fare,'Survival based on Fare')","d51ca6a2":"# Also removing cabin and ticket features for the initial run.\nselFeatures.remove('Ticket')\n        \nprint(\"Target Class: '\"+ targetCol + \"'\")\nprint('Features to be investigated: ')\nprint(selFeatures)","100ba37e":"def handle_categorical_na(df):\n    ## replacing the null\/na\/nan values in 'Cabin' attribute with 'X'\n#     df.Cabin = df.Cabin.fillna(value='X')\n#     ## Stripping the string data in 'Cabin' and 'Ticket' features of numeric values and duplicated characters\n#     df.Cabin = [''.join(set(filter(str.isalpha, s))) for s in df.Cabin]\n#     df.Ticket = [''.join(set(filter(str.isalpha, s))) for s in df.Ticket]\n#     ## replacing the '' values in 'Ticket' attribute with 'X'\n#     df.Ticket.replace(to_replace='',value='X',inplace=True)\n    ## Imputing the null\/na\/nan values in 'Age' attribute with its mean value \n    df.Age.fillna(value=df.Age.mean(),inplace=True)\n    ## replacing the null\/na\/nan values in 'Embarked' attribute with 'X'\n    df.Embarked.fillna(value='X',inplace=True)\n    return df","4caddeea":"from sklearn.model_selection import train_test_split\nseed = 7\nnp.random.seed(seed)\nX_train, X_test, Y_train, Y_test = train_test_split(trainData[selFeatures], trainData.Survived, test_size=0.2)\n\nX_train = handle_categorical_na(X_train)\nX_test = handle_categorical_na(X_test)\n\n## using One Hot Encoding for handling categorical data\nX_train = pd.get_dummies(X_train,columns=['Embarked','Sex'],prefix=['Embarked','Sex'])\nX_test = pd.get_dummies(X_test,columns=['Embarked','Sex'],prefix=['Embarked','Sex'])","1e7e949b":"common_col = [x for x in X_test.columns if x in X_train.columns]\nX_test = X_test[common_col]\n\nmissing_col = [x for x in X_train.columns if x not in X_test.columns]\n## Inserting missing columns in test data\nfor val in missing_col:\n    X_test.insert(X_test.shape[1], val, pd.Series(np.zeros(X_test.shape[0])))","987b2fb7":"from sklearn.model_selection import GridSearchCV\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.constraints import maxnorm\n\nin_shape = X_train.shape[1]\n\ndef create_model(optimizer='Adam', neurons=50):\n    # Initialize the constructor\n    model = Sequential()\n    # Input - Layer\n    model.add(Dense(neurons, input_dim=in_shape, activation=activation))\n    # Hidden - Layers\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(neurons, activation = activation))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(neurons, activation = activation))\n    # Output- Layer\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    return model\n\ndef nn_hyperparameter_optimization():\n    model = KerasClassifier(build_fn=create_model, verbose=0)\n    # defining the grid search parameters\n    neurons = [65, 75, 85]\n    batch_size= [10, 20, 30, 40]\n    epochs= [10, 20, 30, 40]\n    optimizer = ['RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n    activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n    dropout_rate = [0.1, 0.2, 0.3, 0.4, 0.5]\n    param_grid = dict(neurons=neurons,\n                      optimizer=optimizer,\n                      batch_size=batch_size,\n                      epochs=epochs,\n                      activation=activation,\n                      dropout_rate=dropout_rate)\n    grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=5)\n    grid_result = grid.fit(X_train, Y_train)\n\n    # summarize results\n    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n    means = grid_result.cv_results_['mean_test_score']\n    stds = grid_result.cv_results_['std_test_score']\n    params = grid_result.cv_results_['params']\n    for mean, stdev, param in zip(means, stds, params):\n        print(\"%f (%f) with: %r\" % (mean, stdev, param))\n    return grid_result\n\n## based on the hyper parameter optimization, the below model is built.\nnn_model = Sequential()\n# Input - Layer\nnn_model.add(Dense(65, input_dim=in_shape, activation='relu'))\n# Hidden - Layers\nnn_model.add(Dropout(0.2))\nnn_model.add(Dense(65, activation = 'relu'))\nnn_model.add(Dropout(0.2))\nnn_model.add(Dense(65, activation = 'relu'))\n# Output- Layer\nnn_model.add(Dense(1, activation='sigmoid'))\nnn_model.compile(loss='binary_crossentropy', optimizer='Nadam', metrics=['accuracy'])\nnn_model.fit(X_train, Y_train,\n          batch_size=20,\n          epochs=20,\n          verbose=1,\n          validation_data=(X_test, Y_test))\n\nscore = nn_model.evaluate(X_test, Y_test, verbose=2)\nscore_nn = score[1]\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","1dfefd52":" def rf_hyperparameter_optimization():\n    from sklearn.model_selection import RandomizedSearchCV\n    # Number of trees in random forest\n    n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n    # Number of features to consider at every split\n    max_features = ['auto', 'sqrt']\n    # Maximum number of levels in tree\n    max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n    max_depth.append(None)\n    # Minimum number of samples required to split a node\n    min_samples_split = [2, 5, 10]\n    # Minimum number of samples required at each leaf node\n    min_samples_leaf = [1, 2, 4]\n    # Method of selecting samples for training each tree\n    bootstrap = [True, False]\n    # Create the random grid\n    random_grid = {'n_estimators': n_estimators,\n                   'max_features': max_features,\n                   'max_depth': max_depth,\n                   'min_samples_split': min_samples_split,\n                   'min_samples_leaf': min_samples_leaf,\n                   'bootstrap': bootstrap}\n\n    from sklearn.ensemble import RandomForestRegressor\n    # Using the random grid to search for best hyperparameters\n    # Creating the base model to tune\n    rf = RandomForestRegressor()\n    # Random search of parameters, using 5 fold cross validation, \n    rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 5, verbose=2, random_state=42, n_jobs = -1)\n    # Fit the random search model\n    rf_random.fit(X_train, Y_train)\n    return rf_random.best_params_\n\n## based on the hyper parameter optimization, the below model is built.\nfrom sklearn.ensemble import RandomForestClassifier\nrf_model = RandomForestClassifier(n_estimators=1400, min_samples_split=5, min_samples_leaf=4, max_features= 'sqrt', max_depth= 80, bootstrap= True)\nrf_model.fit(X_train,Y_train)\n# Fetching predictions\nY_pred = rf_model.predict(X_test)\n# Calculating the test accuracy\nfrom sklearn import metrics\nscore_rf = metrics.accuracy_score(Y_test, Y_pred)\nprint(\"Test Accuracy:\",score_rf)","1d1c428c":"xTest = testData[selFeatures]\nxTest = handle_categorical_na(xTest)\n## using One Hot Encoding for handling categorical data\nxTest = pd.get_dummies(xTest,columns=['Embarked','Sex'],prefix=['Embarked','Sex'])\ncommon_col = [x for x in xTest.columns if x in X_train.columns]\nxTest = xTest[common_col]\nmissing_col = [x for x in X_train.columns if x not in xTest.columns]\n## Inserting missing columns in test data\nfor val in missing_col:\n    xTest.insert(xTest.shape[1], val, pd.Series(np.zeros(xTest.shape[0])))\ncol_names = xTest.columns\nfrom sklearn.impute import SimpleImputer\nmy_imputer = SimpleImputer()\nxTest = my_imputer.fit_transform(xTest)\nxTest = pd.DataFrame(xTest)\nxTest.columns = col_names\n\nsubmission = pd.DataFrame()\n## Comparing and submitting the best result\nif score_nn>score_rf:\n    predictions = nn_model.predict_classes(xTest)\n    predictions = [x[0] for x in predictions]\nelse:\n    predictions = rf_model.predict(xTest)\nsubmission = pd.DataFrame({'PassengerId': testData.PassengerId, 'Survived': predictions})\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()\n","c9811b47":"**4.2 Random Forest**\n\nUsing Random Forest to build a model. The model uses Random Search to find the best hyperparameter values.\n\nThe optimized parameters are:\n* Number of trees\n* Number of features to consider at every split\n* Maximum number of levels in tree\n* Minimum number of samples required to split a node\n* Minimum number of samples required at each leaf node\n* Method of selecting samples for training each tree\n","ae3bcf81":"**1.2  Summary of the data:**","de1f031d":"**1.3 Some insights :**\n* The data set consists of 12 attributes (PassengerId, Survived, Pclass, Name, Sex, Age, SibSp, Parch, Ticket, Fare, Cabin, Embarked)\n    1. PassengerId - Numeric data, it is unique for each passenger, not useful.\n    2. Survived - Target data to be predicted, with incators 0 (Passenger Died) or 1 (Passenger Survived).\n    3. Pclass - Numeric data, has 3 categories (1st, 2nd or 3rd), useful.\n    4. Name - String data type, mostly unique for each passenger, not useful.\n    5. Sex - Categorical data, has 2 categories (male, female), useful.\n    6. Age - Numeric data, max age is 80, it contains few nulls (can be imputed), useful.\n    7. SibSp - Numeric data, max number of siblings \/ spouses aboard is 8, useful.\n    8. Parch - Numeric data, max number of parents \/ children aboard is 6, useful.\n    9. Ticket - Categorical data, it has a prefix to the number,  it can be useful.\n    10. Fare - Numeric data, higher the class, higher is the fare (need to check), max fare is 512,32 (currency is $ I guess) , useful.\n    11. Cabin - Categorical data, it contains few nulls (can be imputed), it can be useful.\n    12. Embarked - Categorical data, with 3 categories, C = Cherbourg, Q = Queenstown, S = Southampton, it contains few nulls (can be imputed), useful. \n* The attribute 'Survived' would be the attribute to be predicted, i.e, target attribute.\n* Based on the values in the target attribute, i.e., 0\/1 which states if the passenger survived or not, it is a Binary Classification.","1ad3cb8d":"## 3. Data Preparation","79785462":"## Titanic: Machine Learning from Disaster\n\nThis kernel uses solve the prediction of survival of a person on the Titanic, using a Neural Network of Sequential Model from Keras API for TensorFlow.\n\nFurther, I will be using Grid Search for Hyperparameter tuning of the model.","0957fc5e":"## 1. Data Understanding\n\n[](http:\/\/)First let us take a look at how our data looks like, to get more details on the characteristics of the attributes\/features.","03d6baa7":"**3.1 Feature Selection**\n\nSelecting the columns required for building the model based on EDA.","3ea8b2a7":"**2.1 Data Types:**\n\nLet us take a look at the type of data being handled.","b6164806":"**2.3 Unique Values:**\n\nChecking for the amount of unique values in each feature.","9829f4bb":"## 2. Exploratory Data Analysis","40cd006b":"**2.4 Visualizations:**\n\nVisualizing the data using interactive plots. Starting with a matrix scatter plot showing the relation between the features that will be used for the training. Followed by plots to check the survival rate based on features.","7b0accfc":"From the results of sections 2.2 and 2.3, we see that the features\/attributes Cabin, PassengerId and Name can be eliminated from the required features to build the model since, the feature Cabin has 77.1% of missing values, and the features PassengerId\/Name has 100% unique features.","23fd03e8":"**4.1 Neural Network**\n\nUsing Sequential model of Keras API to build a neural network. The model is tuned and optimezed using Grid Serach.\n\nThe optimized parameters are:\n* Number of neurons in input and hidden layers\n* Activation type\n* Optimizer \n* Epochs\n* Batch size\n* Dropout ratio\n\nDue to increased processing time, the best parameters were found and used. ","1a916c40":"## 4. Modeling","f286f485":"**1.1 Peek into the Data:**","cde631f9":"**2.2 Missing Data:**\n\nChecking and calculating the amount of missing values in the dataset.\n\nThese missing values will be handled later.","02a7110e":"**4.3 Submit Predictions**\n\nComparing the accuracy of neural network and random forest models and submitting the predictions from the best model."}}