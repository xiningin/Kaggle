{"cell_type":{"07df11e6":"code","74ca89a1":"code","65b37bfc":"code","b686be97":"code","3e21e3eb":"code","445fbe94":"code","a12d41e3":"code","13df792c":"code","3132c0b8":"code","07998727":"code","0f50656f":"code","1849e425":"code","014cdd5e":"code","d1231f4b":"code","e52d08aa":"code","42737216":"code","c6a66140":"code","204d9547":"code","e1001bd6":"code","4f52a0d2":"code","90ec6f59":"code","e6376a7c":"code","fe596882":"code","30bb9fbf":"code","e7fdf40b":"code","463f23fa":"code","979575c2":"code","4f6c187b":"code","678d090b":"code","c7e67e37":"code","156f131b":"code","0f816941":"code","4a1282c8":"code","52ffb523":"code","d9291e7d":"code","f905cfb1":"code","3d397ada":"code","4b7603e0":"code","09b46693":"code","63ca7f12":"code","72fa881c":"code","d77b3975":"code","2fcb5e47":"code","5ec47bed":"code","6c0ccbf1":"code","c67f4ceb":"code","6b227058":"code","b0c19c37":"code","48036535":"code","ec2bedcf":"code","e37d3e34":"code","ab4de592":"code","aae15346":"code","35b83c0e":"code","05512ee4":"code","6dd3ca1b":"code","42823f96":"code","e83888e3":"code","af9bc3fd":"code","dc8eeb64":"code","11b53cfe":"code","1b6bae96":"code","8ee28ce5":"code","94d5c5e3":"code","c2b3732a":"code","95cd05e2":"code","a609e6c6":"code","8349e311":"code","26a68cb6":"code","f1f558a4":"code","e538c636":"code","d52d80e7":"code","16dc5be0":"code","642f3553":"code","4d5f58b0":"code","de486979":"code","11ac057d":"code","f66e576d":"code","609eff6e":"code","d653167f":"code","15eef5d0":"code","a54feaed":"code","ca506389":"code","f45d9de7":"code","1b1b3273":"code","007fcb6f":"code","f81bd92a":"code","d0f3106c":"code","2ccf7ba5":"code","1a953756":"code","496ab6fd":"code","1a8b8fc7":"code","1f0856a5":"code","43ee72f0":"code","513a1c32":"code","de1397b3":"code","b8ddbd72":"code","90d5b8ec":"code","b107ad97":"code","0cc44eb2":"markdown","ca85e678":"markdown","1f1770f9":"markdown","83dccff5":"markdown","ff8715ba":"markdown","1c81e6b2":"markdown","e423dc1d":"markdown","7b45853e":"markdown","70f69020":"markdown","a2a24666":"markdown","7db9a127":"markdown","2ad59c8d":"markdown","0b1b7ad1":"markdown","23a56edb":"markdown","4dfa4573":"markdown","bc33d2bb":"markdown","14254092":"markdown","f17b2bd6":"markdown","5050beec":"markdown","ed84bf51":"markdown","246ff184":"markdown","5dff81ab":"markdown","3e4693d5":"markdown","ecccda87":"markdown","2fdf1de7":"markdown","7fa25a0b":"markdown","63f3ee3e":"markdown","d588c033":"markdown","e60aed6a":"markdown","64bd4616":"markdown","5824d956":"markdown","0b6a3864":"markdown","3a21246c":"markdown","c31e5e6c":"markdown","c914a4a6":"markdown","4ba9f113":"markdown","37fe4d8f":"markdown","88a56144":"markdown","ff7be3f9":"markdown","a301aced":"markdown","98cf0255":"markdown","cd4027c4":"markdown","55fa6162":"markdown","a7a88f3e":"markdown","ead9c229":"markdown","48882305":"markdown","0dc023d8":"markdown","83ecb32c":"markdown","239b206d":"markdown","efb1e7cd":"markdown","2b574922":"markdown","333892ff":"markdown"},"source":{"07df11e6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","74ca89a1":"#Importing libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nimport pylab as py\nimport warnings\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold, StratifiedKFold, train_test_split\nfrom sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, roc_curve, precision_score, recall_score, precision_recall_curve\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=UserWarning)\nwarnings.filterwarnings('ignore')\nsns.set(style = 'white')","65b37bfc":"df = pd.read_csv(\"..\/input\/banking-churn-prediction\/Banking_churn_prediction.csv\") ","b686be97":"df.head()","3e21e3eb":"df.shape","445fbe94":"#Printing all the columns presenr in the data\ndf.columns","a12d41e3":"#Find out the data types of different variables\ndf.dtypes","13df792c":"#identify list of variables with data type = int64\ndf.dtypes[df.dtypes=='int64']","3132c0b8":"#Converting required variabes to \"Category\" datatype.\ndf['churn'] = df['churn'].astype('category')\ndf['branch_code']=df['branch_code'].astype('category')\ndf['customer_nw_category']=df['customer_nw_category'].astype('category')","07998727":"df.dtypes[df.dtypes=='int64']","0f50656f":"#FLOAT DATA TYPE\ndf.dtypes[df.dtypes=='float64']","1849e425":"df['dependents'].value_counts(normalize = True)","014cdd5e":"fil = (df['dependents'] == 4) | (df['dependents'] == 5) | (df['dependents'] == 6) | (df['dependents'] == 7) | (df['dependents'] == 32) | (df['dependents'] == 50) | (df['dependents'] == 36) | (df['dependents'] == 52) | (df['dependents'] == 8) | (df['dependents'] == 9) | (df['dependents'] == 25)\ndf.loc[fil, 'dependents'] = 3\ndf['dependents'].value_counts(normalize = True)","d1231f4b":"#Converint required variables to \"Category\" and \"Integer\" datatypes.\ndf['dependents'] = df['dependents'].astype('category')\ndf['city']=df['city'].astype('category')\ndf[['city','dependents']].dtypes","e52d08aa":"df.dtypes","42737216":"#variables like 'GENDER','OCCUPATION'and 'LAST_TRANSACTION' are of object type. This means that PANDAS WAS NOT ABLE TO recognise \n#THE datatypes OF these three variables\ndf[['gender','occupation','last_transaction']].head(7)","c6a66140":"df['gender']=df['gender'].astype('category')\ndf['occupation'] = df['occupation'].astype('category')\ndf[['gender','occupation']].dtypes","204d9547":"#Creating an instance(data) of Datetimeindex class using \"last_transaction\"\ndf['last_transaction']=pd.DatetimeIndex(df['last_transaction'])","e1001bd6":"#extracting new columns from \"last_transaction\"\ndate = pd.DatetimeIndex(df['last_transaction'])\n#LAST DAY OF YEAR WHEN TRANSACTION WAS DONE\ndf['doy_ls_tran'] =date.dayofyear\n#WEEK OF YEAR WHEN LAST TRANSACTION WAS DONE\ndf['woy_ls_tran']=date.weekofyear\n#MONTH OF YEAR WHEN LAST TRANSACTION WAS DONE\ndf['moy_ls_tran']=date.month\n#DAY OF WEEK WHEN LAST TRANSACTION WAS DONE\ndf['dow_ls_tran']=date.dayofweek\n","4f52a0d2":"df[['last_transaction','doy_ls_tran','woy_ls_tran','moy_ls_tran','dow_ls_tran']].head()","90ec6f59":"df.dtypes","e6376a7c":"df.select_dtypes(include=['int64','float64','Int64']).dtypes","fe596882":"# segregating variables into groups\ncustomer_details = ['customer_id','vintage', 'age']\ncurrent_month = ['current_balance','current_month_credit','current_month_debit','current_month_balance']\nprevious_month = ['previous_month_end_balance','previous_month_credit','previous_month_debit','previous_month_balance']\nprevious_quarters = ['average_monthly_balance_prevQ','average_monthly_balance_prevQ2']\ntransaction_date = ['doy_ls_tran','woy_ls_tran','moy_ls_tran','dow_ls_tran']","30bb9fbf":"# custom function for easy and efficient analysis of numerical univariate\ndef UVA_numeric(df, var_group):\n  '''\u00a0\n  Univariate_Analysis_numeric\n  takes a group of variables (INTEGER and FLOAT) and plot\/print all the descriptives and properties along with KDE.\n\n  Runs a loop: calculate all the descriptives of i(th) variable and plot\/print it\n  '''\n\n  size = len(var_group)\n  plt.figure(figsize = (7*size,3), dpi = 100)\n  \n  #looping for each variable\n  for j,i in enumerate(var_group):\n    \n    # calculating descriptives of variable\n    mini = df[i].min()\n    maxi = df[i].max()\n    ran = df[i].max()-df[i].min()\n    mean = df[i].mean()\n    median = df[i].median()\n    st_dev = df[i].std()\n    skew = df[i].skew()\n    kurt = df[i].kurtosis()\n\n    # calculating points of standard deviation\n    points = mean-st_dev, mean+st_dev\n\n    #Plotting the variable with every information\n    plt.subplot(1,size,j+1)\n    sns.kdeplot(df[i], shade=True)\n    sns.lineplot(points, [0,0], color = 'black', label = \"std_dev\")\n    sns.scatterplot([mini,maxi], [0,0], color = 'orange', label = \"min\/max\")\n    sns.scatterplot([mean], [0], color = 'red', label = \"mean\")\n    sns.scatterplot([median], [0], color = 'blue', label = \"median\")\n    plt.xlabel('{}'.format(i), fontsize = 20)\n    plt.ylabel('density')\n    plt.title('std_dev = {}; kurtosis = {};\\nskew = {}; range = {}\\nmean = {}; median = {}'.format((round(points[0],2),round(points[1],2)),\n                                                                                                   round(kurt,2),\n\n                                                                                                   round(skew,2),\n                                                                                                   (round(mini,2),round(maxi,2),round(ran,2)),\n                                                                                                   round(mean,2),\n                                                                                                   round(median,2)))\n","e7fdf40b":"# Univariate Analysis of \"Customer Details\"\nUVA_numeric(df, customer_details)","463f23fa":"#droping \"customer_id\" from the dataframe\ndf.drop(['customer_id'], axis = 1, inplace  = True) ","979575c2":"#Univariate analysis of customer details\nUVA_numeric(df, current_month)","4f6c187b":"#Univariate analysis of previous month\nUVA_numeric(df,previous_month)","678d090b":"# Univariate analysis of transaction date\nUVA_numeric(df,transaction_date)","c7e67e37":"def UVA_category(df, var_group):\n\n  '''\n  Univariate_Analysis_categorical\n  takes a group of variables (category) and plot\/print all the value_counts and barplot.\n  '''\n  # setting figure_size\n  size = len(var_group)\n  plt.figure(figsize = (7*size,5), dpi = 100)\n\n  # for every variable\n  for j,i in enumerate(var_group):\n    norm_count = df[i].value_counts(normalize = True)\n    n_uni = df[i].nunique()\n\n  #Plotting the variable with every information\n    plt.subplot(1,size,j+1)\n    sns.barplot(norm_count, norm_count.index , order = norm_count.index)\n    plt.xlabel('fraction\/percent', fontsize = 20)\n    plt.ylabel('{}'.format(i), fontsize = 20)\n    plt.title('n_uniques = {} \\n value counts \\n {};'.format(n_uni,norm_count))","156f131b":"UVA_category(df, ['occupation', 'gender', 'customer_nw_category','dependents'])    ","0f816941":"UVA_category(df, ['city', 'branch_code'])","4a1282c8":"#CHURN\n\nUVA_category(df, ['churn'])","52ffb523":"\n# finding number of missing values in every variable\ndf.isnull().sum()","d9291e7d":"# Calculating Outliers\ndef UVA_outlier(df, var_group, include_outlier = True):\n  '''\n  Univariate_Analysis_outlier:\n  takes a group of variables (INTEGER and FLOAT) and plot\/print boplot and descriptives\\n\n  Runs a loop: calculate all the descriptives of i(th) variable and plot\/print it \\n\\n\n\n  data : dataframe from which to plot from\\n\n  var_group : {list} type Group of Continuous variables\\n\n  include_outlier : {bool} whether to include outliers or not, default = True\\n\n  '''\n\n  size = len(var_group)\n  plt.figure(figsize = (7*size,4), dpi = 100)\n  \n  #looping for each variable\n  for j,i in enumerate(var_group):\n    \n    # calculating descriptives of variable\n    quant25 = df[i].quantile(0.25)\n    quant75 = df[i].quantile(0.75)\n    IQR = quant75 - quant25\n    med = df[i].median()\n    whis_low = med-(1.5*IQR)\n    whis_high = med+(1.5*IQR)\n\n    # Calculating Number of Outliers\n    outlier_high = len(df[i][df[i]>whis_high])\n    outlier_low = len(df[i][df[i]<whis_low])\n\n    if include_outlier == True:\n      print(include_outlier)\n      #Plotting the variable with every information\n      plt.subplot(1,size,j+1)\n      sns.boxplot(df[i], orient=\"v\")\n      plt.ylabel('{}'.format(i))\n      plt.title('With Outliers\\nIQR = {}; Median = {} \\n 2nd,3rd  quartile = {};\\n Outlier (low\/high) = {} \\n'.format(\n                                                                                                   round(IQR,2),\n                                                                                                   round(med,2),\n                                                                                                   (round(quant25,2),round(quant75,2)),\n                                                                                                   (outlier_low,outlier_high)\n                                                                                                   ))\n      \n    else:\n      # replacing outliers with max\/min whisker\n      data = df[var_group][:]\n      data[i][data[i]>whis_high] = whis_high+1\n      data[i][data[i]<whis_low] = whis_low-1\n      \n      # plotting without outliers\n      plt.subplot(1,size,j+1)\n      sns.boxplot(data[i], orient=\"v\")\n      plt.ylabel('{}'.format(i))\n      plt.title('Without Outliers\\nIQR = {}; Median = {} \\n 2nd,3rd  quartile = {};\\n Outlier (low\/high) = {} \\n'.format(\n                                                                                                   round(IQR,2),\n                                                                                                   round(med,2)\n                                                                                                   (round(quant25,2),round(quant75,2)),\n                                                                                                   (outlier_low,outlier_high)\n                                                                                                   ))  \n  \n","f905cfb1":"UVA_outlier(df, current_month)","3d397ada":"UVA_outlier(df, previous_month)","4b7603e0":"UVA_outlier(df,previous_quarters)","09b46693":"customer_details = ['vintage', 'age',]","63ca7f12":"UVA_outlier(df,customer_details)","72fa881c":"numerical = df.select_dtypes(include=['int64','float64','Int64'])\nnumerical.dtypes\n","d77b3975":"### CORRELATION MATRIX\n# calculating correlation\ncorrelation = numerical.dropna().corr()\ncorrelation\n","2fcb5e47":"### HEATMAP\n# plotting heatmap usill all methods for all numerical variables\nplt.figure(figsize=(36,6), dpi=140)\nfor j,i in enumerate(['pearson','kendall','spearman']):\n  plt.subplot(1,3,j+1)\n  correlation = numerical.dropna().corr(method=i)\n  sns.heatmap(correlation, linewidth = 2)\n  plt.title(i, fontsize=18)","5ec47bed":"     var = []\nvar.extend(previous_month)\nvar.extend(current_month)\nvar.extend(previous_quarters)  \n\n# plotting heatmap usill all methods for all transaction variables\nplt.figure(figsize=(36,6), dpi=140)\nfor j,i in enumerate(['pearson','kendall','spearman']):\n  plt.subplot(1,3,j+1)\n  correlation = numerical[var].dropna().corr(method=i)\n  sns.heatmap(correlation, linewidth = 2)\n  plt.title(i, fontsize=18)","6c0ccbf1":"def TwoSampZ(X1, X2, sigma1, sigma2, N1, N2):\n  '''\n  takes mean, standard deviation, and number of observations and returns p-value calculated for 2-sampled Z-Test\n  '''\n  from numpy import sqrt, abs, round\n  from scipy.stats import norm\n  ovr_sigma = sqrt(sigma1**2\/N1 + sigma2**2\/N2)\n  z = (X1 - X2)\/ovr_sigma\n  pval = 2*(1 - norm.cdf(abs(z)))\n  return pval\n","c67f4ceb":"def TwoSampT(X1, X2, sd1, sd2, n1, n2):\n  '''\n  takes mean, standard deviation, and number of observations and returns p-value calculated for 2-sample T-Test\n  '''\n  from numpy import sqrt, abs, round\n  from scipy.stats import t as t_dist\n  ovr_sd = sqrt(sd1**2\/n1 + sd2**2\/n2)\n  t = (X1 - X2)\/ovr_sd\n  df = n1+n2-2\n  pval = 2*(1 - t_dist.cdf(abs(t),df))\n  return pval","6b227058":"def Bivariate_cont_cat(df, cont, cat, category):\n  #creating 2 samples\n  x1 = df[cont][df[cat]==category][:]\n  x2 = df[cont][~(df[cat]==category)][:]\n  \n  #calculating descriptives\n  n1, n2 = x1.shape[0], x2.shape[0]\n  m1, m2 = x1.mean(), x2.mean()\n  std1, std2 = x1.std(), x2.mean()\n  \n  #calculating p-values\n  t_p_val = TwoSampT(m1, m2, std1, std2, n1, n2)\n  z_p_val = TwoSampZ(m1, m2, std1, std2, n1, n2)\n\n  #table\n  table = pd.pivot_table(data=df, values=cont, columns=cat, aggfunc = np.mean)\n\n  #plotting\n  plt.figure(figsize = (15,6), dpi=140)\n  \n  #barplot\n  plt.subplot(1,2,1)\n  sns.barplot([str(category),'not {}'.format(category)], [m1, m2])\n  plt.ylabel('mean {}'.format(cont))\n  plt.xlabel(cat)\n  plt.title('t-test p-value = {} \\n z-test p-value = {}\\n {}'.format(t_p_val,\n                                                                z_p_val,\n                                                                table))\n\n  # boxplot\n  plt.subplot(1,2,2)\n  sns.boxplot(x=cat, y=cont, data=df)\n  plt.title('categorical boxplot')\n","b0c19c37":"### 1. Are vintage customers less likely to churn?\nBivariate_cont_cat(df, 'vintage', 'churn', 1)  ","48036535":"### 2. Are customers with higher average balance less likely to churn?\nBivariate_cont_cat(df, 'average_monthly_balance_prevQ', 'churn', 1)","ec2bedcf":"#### 2. Are customers with higher average balance in previour quarter 2 less likely to churn?\nBivariate_cont_cat(df, 'average_monthly_balance_prevQ2', 'churn', 1)","e37d3e34":"#Are customers with higher customer balance less likely to churn\nBivariate_cont_cat(df, 'current_month_balance', 'churn', 1)","ab4de592":"### 3. Are customers dropping monthly balance highly likely to churn?\n\n# Extracting drop of balance in previous and current month\ndifference = df[['churn','previous_month_balance','current_month_balance']][:]\ndifference['bal_diff'] = difference['current_month_balance']-difference['previous_month_balance']","aae15346":"Bivariate_cont_cat(difference, 'bal_diff', 'churn', 1)","35b83c0e":"Bivariate_cont_cat(df, 'age', 'churn', 1)","05512ee4":"#defining function fot categorical-categorical bivariate analysis\ndef BVA_categorical_plot(data, tar, cat):\n  '''\n  take data and two categorical variables,\n  calculates the chi2 significance between the two variables \n  and prints the result with countplot & CrossTab\n  '''\n  #isolating the variables\n  data = df[[cat,tar]][:]\n\n  #forming a crosstab\n  table = pd.crosstab(df[tar],df[cat],)\n  f_obs = np.array([table.iloc[0][:].values,\n                    table.iloc[1][:].values])\n\n  #performing chi2 test\n  from scipy.stats import chi2_contingency\n  chi, p, dof, expected = chi2_contingency(f_obs)\n  \n  #checking whether results are significant\n  if p<0.05:\n    sig = True\n  else:\n    sig = False\n\n  #plotting grouped plot\n  sns.countplot(x=cat, hue=tar, data=data)\n  plt.title(\"p-value = {}\\n difference significant? = {}\\n\".format(round(p,8),sig))\n\n  #plotting percent stacked bar plot\n  #sns.catplot(ax, kind='stacked')\n  ax1 = df.groupby(cat)[tar].value_counts(normalize=True).unstack()\n  ax1.plot(kind='bar', stacked='True',title=str(ax1))\n  int_level = df[cat].value_counts()","6dd3ca1b":"#Are females less likely to churn than males?  \nBVA_categorical_plot(df, 'churn', 'gender')","42823f96":"#Dependents  \nBVA_categorical_plot(df, 'churn', 'dependents')","e83888e3":"#Occupation  \nBVA_categorical_plot(df, 'churn', 'occupation')","af9bc3fd":"#CUSTOMER FROM LOW INCOME BRACKET MORE LIKELY TO CHURN\nBVA_categorical_plot(df, 'churn', 'customer_nw_category')","dc8eeb64":"# segregating customers into segments\n#Are young customers more likely to churn\n\ndf['age_group'] = \" \"\ndf['age_group'] = 'categorical'\ndf['age_group'][df['age']>=80] = 'very old'\ndf['age_group'][(df['age']<80) & (df['age']>=60)] = 'senior citizen'\ndf['age_group'][(df['age']<60) & (df['age']>=18)] = 'adult'\ndf['age_group'][df['age']<18] = 'young'","11b53cfe":"BVA_categorical_plot(df,'churn','age_group')","1b6bae96":"#Possibilities that cities and branch code with very few accounts may lead to churning\n# getting city codes which have less than 280 (1%) of accounts\ntmp = df['city'].value_counts()[:]\ncities = tmp[tmp<280].index\ndf['city_cat'] = ' '\ndf['city_cat'] = None\ndf['city_cat'][df['city'].isin(cities[:])] = 'low accounts'\ndf['city_cat'][~df['city'].isin(cities[:])] = 'high accounts'","8ee28ce5":"BVA_categorical_plot(df, 'churn', 'city_cat')","94d5c5e3":"#Branch Code\n# getting branch codes with more than 0.5% of total accounts\ntmp = df['branch_code'].value_counts()[:]\nbranch = tmp[tmp<140].index\n","c2b3732a":"# making two segments\ndf['branch_cat'] = ' ' \ndf['branch_cat'][df['branch_code'].isin(branch[:])] = 'low accounts'\ndf['branch_cat'][~df['branch_code'].isin(branch[:])] = 'high accounts'","95cd05e2":"BVA_categorical_plot(df, 'churn', 'branch_cat')","a609e6c6":"pd.isnull(df).sum()","8349e311":"df['gender'].value_counts()","26a68cb6":"dict_gender = {'Male': 1, 'Female':0}\ndf.replace({'gender': dict_gender}, inplace = True)\ndf['gender'] = df['gender'].fillna(-1)","f1f558a4":"df['dependents'].value_counts()","e538c636":"df['dependents'] = df['dependents'].fillna(0)","d52d80e7":"df['occupation'].value_counts()","16dc5be0":"df['occupation'] = df['occupation'].fillna('self_employed')","642f3553":"df['city'].value_counts()","4d5f58b0":"df['city'].mode()","de486979":"#Similarly City can also be imputed with most common category 1020\ndf['city'] = df['city'].fillna(1020)\n","11ac057d":"df['last_transaction'] = df['last_transaction'].fillna(999)","f66e576d":"df.dtypes","609eff6e":"df['gender']=df['gender'].astype('category')","d653167f":"df1 = df[['vintage', 'age', 'gender', 'dependents', 'occupation',\n       'customer_nw_category','current_balance',\n       'previous_month_end_balance', 'average_monthly_balance_prevQ',\n       'average_monthly_balance_prevQ2', 'current_month_credit',\n       'previous_month_credit', 'current_month_debit', 'previous_month_debit',\n       'current_month_balance', 'previous_month_balance', 'churn','age_group','city_cat']]","15eef5d0":"### Preprocessing\n#Now, before applying linear model such as logistic regression, we need to\n#scale the data and keep all features as numeric strictly.\n\n#DUMMIES WITH MULTIPLE CATEGORIES\n# Convert occupation to one hot encoded features\ndf1 = pd.concat([df1,pd.get_dummies(df1['occupation'],prefix = str('occupation'),prefix_sep='_')],axis = 1)\ndf1= pd.concat([df1,pd.get_dummies(df1['gender'],prefix = str('gender'),prefix_sep='_')],axis = 1)\ndf1= pd.concat([df1,pd.get_dummies(df1['customer_nw_category'],prefix = str('customer_nw_category'),prefix_sep='_')],axis = 1)\ndf1= pd.concat([df1,pd.get_dummies(df1['dependents'],prefix = str('dependents'),prefix_sep='_')],axis = 1)\ndf1= pd.concat([df1,pd.get_dummies(df1['age_group'],prefix = str('age_group'),prefix_sep='_')],axis = 1)\ndf1= pd.concat([df1,pd.get_dummies(df1['city_cat'],prefix = str('city_cat'),prefix_sep='_')],axis = 1)","a54feaed":"num_cols = [ 'current_balance','vintage',\n            'previous_month_end_balance', 'average_monthly_balance_prevQ2', 'average_monthly_balance_prevQ',\n            'current_month_credit','previous_month_credit', 'current_month_debit', \n            'previous_month_debit','current_month_balance', 'previous_month_balance']","ca506389":"for i in num_cols:\n    df1[i] = np.log(df1[i] + 17000)","f45d9de7":"std = StandardScaler()\nscaled = std.fit_transform(df1[num_cols])\nscaled = pd.DataFrame(scaled,columns=num_cols)\ndf_df_og = df1.copy()\ndf1 = df1.drop(columns = num_cols,axis = 1)\ndf1= df1.merge(scaled,left_index=True,right_index=True,how = \"left\")","1b1b3273":"y_all = df1.churn","007fcb6f":"df1 = df1.drop(['churn','occupation','gender','dependents','customer_nw_category','city_cat','age_group'],axis = 1)","f81bd92a":"df1.columns","d0f3106c":"baseline_cols = ['current_month_debit', 'previous_month_debit','current_balance','previous_month_end_balance','vintage'\n                 ,'occupation_retired', 'occupation_salaried','occupation_self_employed', 'occupation_student','age','gender_-1.0', 'gender_0.0', 'gender_1.0', 'customer_nw_category_1',\n       'customer_nw_category_2', 'customer_nw_category_3', 'dependents_0.0',\n       'dependents_1.0', 'dependents_2.0', 'dependents_3.0', 'age_group_adult',\n       'age_group_senior citizen', 'age_group_very old', 'age_group_young','city_cat_high accounts', 'city_cat_low accounts']","2ccf7ba5":"df_baseline = df1[baseline_cols]","1a953756":"xtrain, xtest, ytrain, ytest = train_test_split(df_baseline,y_all,test_size=1\/3, random_state=11, stratify = y_all)\nmodel = LogisticRegression()\nmodel.fit(xtrain,ytrain)\npred = model.predict_proba(xtest)[:,1]","496ab6fd":"from sklearn.metrics import roc_curve\nfpr, tpr, _ = roc_curve(ytest,pred) \nauc = roc_auc_score(ytest, pred) \nplt.figure(figsize=(12,8)) \nplt.plot(fpr,tpr,label=\"Validation AUC-ROC=\"+str(auc)) \nx = np.linspace(0, 1, 1000)\nplt.plot(x, x, linestyle='-')\nplt.xlabel('False Positive Rate') \nplt.ylabel('True Positive Rate') \nplt.legend(loc=4) \nplt.show()","1a8b8fc7":"# Confusion Matrix\npred_val = model.predict(xtest)\n\nlabel_preds = pred_val\n\ncm = confusion_matrix(ytest,label_preds)\n\n\ndef plot_confusion_matrix(cm, normalized=True, cmap='bone'):\n    plt.figure(figsize=[7, 6])\n    norm_cm = cm\n    if normalized:\n        norm_cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        sns.heatmap(norm_cm, annot=cm, fmt='g', xticklabels=['Predicted: No','Predicted: Yes'], yticklabels=['Actual: No','Actual: Yes'], cmap=cmap)\n\nplot_confusion_matrix(cm, ['No', 'Yes'])","1f0856a5":"recall_score(ytest,pred_val)","43ee72f0":"### Cross validation\n\n\n#Cross Validation is one of the most important concepts in any type of data modelling. It simply says, try to leave a sample on which you do not train the model and test the model on this sample before finalizing the model.\n#We divide the entire population into k equal samples. Now we train models on k-1 samples and validate on 1 sample. Then, at the second iteration we train the model with a different sample held as validation. \n#In k iterations, we have basically built model on each sample and held each of them as validation. This is a way to reduce the selection bias and reduce the variance in prediction power.\n#Since it builds several models on different subsets of the dataset, we can be more sure of our model performance if we use CV for testing our models.\n\n\ndef cv_score(ml_model, rstate = 12, thres = 0.5, cols = df1.columns):\n    i = 1\n    cv_scores = []\n    df2 = df1.copy()\n    df2 = df1[cols]\n    \n    # 5 Fold cross validation stratified on the basis of target\n    kf = StratifiedKFold(n_splits=5,random_state=rstate,shuffle=True)\n    for df_index,test_index in kf.split(df2,y_all):\n        print('\\n{} of kfold {}'.format(i,kf.n_splits))\n        xtr,xvl = df2.loc[df_index],df2.loc[test_index]\n        ytr,yvl = y_all.loc[df_index],y_all.loc[test_index]\n            \n        # Define model for fitting on the training set for each fold\n        model = ml_model\n        model.fit(xtr, ytr)\n        pred_probs = model.predict_proba(xvl)\n        pp = []\n         \n        # Use threshold to define the classes based on probability values\n        for j in pred_probs[:,1]:\n            if j>thres:\n                pp.append(1)\n            else:\n                pp.append(0)\n         \n        # Calculate scores for each fold and print\n        pred_val = pp\n        roc_score = roc_auc_score(yvl,pred_probs[:,1])\n        recall = recall_score(yvl,pred_val)\n        precision = precision_score(yvl,pred_val)\n        sufix = \"\"\n        msg = \"\"\n        msg += \"ROC AUC Score: {}, Recall Score: {:.4f}, Precision Score: {:.4f} \".format(roc_score, recall,precision)\n        print(\"{}\".format(msg))\n         \n         # Save scores\n        cv_scores.append(roc_score)\n        i+=1\n    return cv_scores\n","513a1c32":"baseline_scores = cv_score(LogisticRegression(), cols = baseline_cols)","de1397b3":"#There is some improvement in both ROC AUC Scores and Precision\/Recall Scores. Now we can try backward selection to select the best subset of features which give the best score. \nall_feat_scores = cv_score(LogisticRegression())","b8ddbd72":"#Reverse Feature Elimination and Backward Selection\nfrom sklearn.feature_selection import RFE\nimport matplotlib.pyplot as plt","90d5b8ec":"#just copies the data frame to a new place\ndf2 = df1\ndf2.dtypes\n#just drop all the object categories\n# Create the RFE object and rank each feature\nmodel = LogisticRegression()\nrfe = RFE(estimator=model, n_features_to_select=1, step=1)\nrfe.fit(df1, y_all)\nranking_df = pd.DataFrame()\nranking_df['Feature_name'] = df1.columns\nranking_df['Rank'] = rfe.ranking_\n\nranked = ranking_df.sort_values(by=['Rank'])\nranked\n\n#The balance features are proving to be very important as can be seen from the table. The RFE function can also be used to select features. Lets select the top 10 features from this table and check score.\n\nrfe_top_10_scores = cv_score(LogisticRegression(), cols = ranked['Feature_name'][:10].values)\n\n#Wow, the top 10 features obtained using the reverse feature selection are giving a much better score than any of our earlier attempts. This is the power of feature selection and it especially works well in case of linear models as tree based models are in itself to some extent capable of doing feature selection.\n#The recall score here is quite low. We should play around with the threshold to get a better recall score. AUC ROC depends on the predicted probabilities and is not impacted by the threshold. Let us try 0.2 as threshold which is close to the overall churn rate\ncv_score(LogisticRegression(), cols = ranked['Feature_name'][:10].values, thres=0.14)\n\n#We observe that there is continuous improvement in the Recall Score. However, clearly precision score is going down. On the basis of business requirement the bank can take a call on deciding the threshold. Without knowing the metrics relevant to the business, our best course of action is to optimize for AUC ROC Score so as to find the best probabilites here.\n#Comparison of Different model fold wise\n#Let us visualise the cross validation scores for each fold for the following 3 models and observe differences:\n","b107ad97":"#Baseline Model\n#Model based on all features\n#Model based on top 10 features obtained from RFE\nresults_df = pd.DataFrame({'baseline':baseline_scores, 'all_feats': all_feat_scores, 'rfe_top_10': rfe_top_10_scores})\n\nresults_df.plot(y=[\"baseline\", \"all_feats\", \"rfe_top_10\"], kind=\"bar\")\n#Here, we can see that the model based on RFE is giving the best result for each fold and students are encouraged to try more feature selection techniques and fine tune to get the best results.","0cc44eb2":"#So there is a good mix of males and females and arguably missing values\n\n#cannot be filled with any one of them. We could create a seperate category\n\n#by assigning the value -1 for all missing values in this column.\n\n#Before that, first we will convert the gender into 0\/1 and then replace missing values with -1","ca85e678":"#Summary\n1) dependent is expected to be a whole number. Therefore we should first count the frequency of different numbers and than using feature engineering technique \"Combining Sparse Category\" should combine the lease happening whole numbers and than change it to category data type.\n2) city varibale is also a unique code of a city represented by some integr number. Should be converted to \"Category\" type.\n3) Rest of the variables are best represented by float64.\n","1f1770f9":"# List of Hypothesis and investigation to perform under this combination.\n1) Are vintage customers less likely to churn?\n2) Are customers with higher average balance less likely to churn?\n3) Are customers dropping monthly balance highly likely to churn?\n4) Do age affect the decisions of churning?","83dccff5":"Summary\n1)If we look at corresponding plots in the outputs above, there seems to be a strong relation between the corresponding plots of previous_month and current_month variables.\n\n2)Outliers are significant in number and very similar in number between corresponding plots. Which indicates some inherent undiscovered behviour of Outliers\n","ff8715ba":"# Summary of Customer_Information:\n\n1) customer_id:\n#variable is unique for every customer, Hence uniform distribution.\n#This variable does not contribute any information can be eliminated from data\n\n2) age:\n#Median Age = 46\n#Most customers age between 30 to 66\n#skewness +0.33 : customer age is negligibly biased towards younger age\n#kurtosis = -0.17; very less likely to have extreme\/outlier values.\n\n3) vintage:\n#Most customers joined between 2100 and 2650 days from the day of data extraction.\n#skewness -1.42 : this is left skewed, vintage variable is significantly biased towards longer association of customers.\n#Kurtosis = 2.93: Extreme values and Outliers are very likely to be present in vintage.\n","1c81e6b2":"#We can see that people who churned actually had significantly highe balance during their previous two quarters.","e423dc1d":"#Outliers in previous two quarters are very similar but significantly large in number.\n \n#Investigation directions from Univariate Analysis\n#customer_id variable can be dropped.\n#Is there there any common trait\/relation between the customers who are performing high transaction credit\/debits?\n#customer_nw_category might explain that.\n#Occupation = Company might explain them\n#popular cities might explain this\n#4Customers whose last transaction was 6 months ago, did all of them churn? \n#Possibility that cities and branch code with very few accounts may lead to churning.","7b45853e":"# There seems to be significant between the churning rate for different dependents categories. Persons with no dependents are more likely to churn.","70f69020":"CONTINUOUS CATEGORICAL VARIABLES","a2a24666":"\"CATEGORICAL CATEGORICAL\"","7db9a127":"#There seems to be statistically significant difference between the churning rate in different occupation categories. Also, self employed has highest probability to churn.","2ad59c8d":"#Summary\n\n#Occupation\n60% of people are self_employed.\nThere are 1% Company Accounts. Might explain Outliers Extreme values in credit\/debit.\n\n#Gender:\nMales accounts are 1.5 times in number than Female Accounts i.e. we have 60% males with 40% females\n#customer_nw_category:\nHalf of all the accounts belong to the 3rd net worth category.\nLess than 15% belong to the highest net worth category.\n\n#dependents : \nMajority of the population has 0 dependents. This means they have no one to take care off on their expenses.\n","0b1b7ad1":"#Inferences\n\n#Vintage customers churned more, but results are not significantly different\n#Boxplot shows very similar distribution with outliers on the lower end.\n#Result\n#Because of higher Pvalue we can safely reject the alternative hypothesis that vintage customers are less likely to churn.\n","23a56edb":"# ANALYSIS of  MISSING VALUES","4dfa4573":"# Univariate analysis Categorical variables","bc33d2bb":"#Summary\n\n\n#Day_of_Year:\n1) most of the last transactions were made in the last 60 days of the extraction of data.There are transactions which were made more than an year ago.\n#Week_of_year and Month_of_year:\n1)These variable validate the findings from the day_of_year.\n#date of week figure shows no transaction on sunday as we all know the banks are closed on sunday. We can see high number of transaction on Monday.\n","14254092":"#Result:\n#Different income brackets have significant effect on the churn rate.\n","f17b2bd6":"1) gender and occupation variables belong to a categorical data types.\n2)  last_transaction should be a datetime variable.","5050beec":"# PREPROCESSING","ed84bf51":"# CHURN PREDICTION","246ff184":"#Scaling Numerical Features for Logistic Regression\n\n#Now, we remember that there are a lot of outliers in the dataset especially\n\n#when it comes to previous and current balance features. Also, the\n\n#distributions are skewed for these features if you recall from the EDA.\n\n#We will take 2 steps to deal with that here:\n\n#Log Transformation\n\n#Standard Scaler\n\n#Standard scaling is anyways a necessity when it comes to linear models\n\n#and we have done that here after doing log transformation on all balance\n\n#features.","5dff81ab":"#There is no significant difference between the high account count branches and low account count branches.","3e4693d5":"#We can see that their is a significant diffetence in the age of customets who chur and who do not churn.","ecccda87":"# \"VARIABLE IDENTIFICATION\"","2fdf1de7":"\"CONTINUOUS VARIABLES\"","7fa25a0b":"1) The first column is the complete data of the last transaction which was was done by the given customer.\n2) The next column represent the day of year, week of year, month of year, day of week when last transaction was done. Breaking down the  date variable into these granual information will help us understand when the lasr transaction was done from different perspectives.Now that we have extracted the essentials from the \n","63f3ee3e":"#The result of this function shows that there are quite a few missing values\n\n#in columns gender, dependents, city, days since last transaction and Percentage change in credits.\n\n#Let us go through each of them 1 by 1 to find the appropriate missing value imputation strategy for each of them.","d588c033":"#Inferences\n\n#Customers who churned had significantly high balance throughout the previous two quarters and previous month. But their average balance reduced significantly in the current month.\n","e60aed6a":"#Inference\n#Customers who churned had a very high drop in their balance which is signified by the negative value in this bar plot.\n","64bd4616":"#Transaction variables like credit\/debit have a strong correlation among themselves.\n#Balance variables have strong correlation among themselves.\n#Transaction variables like credit\/debit have insignificant or no correlation with the Balance variables.","5824d956":"#Summary\n1) Considering the kurtosis and skewness value  for all 4 of these plots. Outliers\/Extreme values are present.\n2) Need to Remove Outliers to visulaise these plots**","0b6a3864":"#Vintage has some outliers at the lower side.\n#Age seems to have no outliers.","3a21246c":"#Age group has significant effect on churning rate. However we can see major churning customers lies in adult and senior age group","c31e5e6c":"# Here we complete our first step of \"Exploratory Data Analytics\" i.e. \"Variable Identification\"","c914a4a6":"# BIVARIATE ANALYSIS","4ba9f113":"# UNIVARIATE ANALYSIS","37fe4d8f":"#Number of people who churned are 1\/4 times of the people who did not churn in the given data.","88a56144":"NUMERICAL NUMERICAL","ff7be3f9":"#Days since Last Transactions\n#A fair assumption can be made on this column as this is number of days \n#since last transaction in 1 year, we can substitute missing values with a \n#value greater than 1 year say 999","a301aced":"# \"OUTLIER ANALYSIS","98cf0255":"#Due to high p value we can say that the current balance value do not significantly differentiate between churning and non churning customers.","cd4027c4":"#### List of Hypothesis to check under this combination\n#Are females less likely to churn than males?\n#Are young customers more likely to churn?\n#Are customers in the lower income bracket more likely to churn?\n#Are customers with dependent(s) less likely to churn?\n#Customers with an average family size less than 4 are more likely to churn?\n#Possibility that cities and branch code with very few accounts may lead to churning.","55fa6162":"#Result:\n#the difference between the males and females customer churning is significant and more males customers churn.","a7a88f3e":"Missing Values\n#Before we go on to build the model, we must look for missing values within\n\n#the dataset as treating the missing values is a necessary step before we\n\n#fit a logistic regression model on the dataset.","ead9c229":"PROBLEM STATEMENT\nWHAT CUSTOMER SEGEMETNS ARE MORE LIKELY TO CHURN BALANCES IN THE NEXT QUARTER BY AT LEAST 50% CONSIDERING CURRENT QUARTER.","48882305":"#Summary:\n1) customer_id is a unique assigned to customers. So, integers datatype is correctly specified.\n2) branch_code again represents different branches, therefore it should be converted as \"Category datatype.\n3) Age and vintage are also numbers and hence we are okay wit them as integers.\n4) Customer _networth_category is supposed to be an ordinal category, should be converted to \"Category\" datatype.\n5) Churn 1 represents the churn and 0 represents not churn.However, there is no comparision between the two categories .This needs to be converted to each \"Category\" datatype.\n","0dc023d8":"#Things to investigate further down:**\n#Gender:\n1)do missing values have any relation with churn?\n\n#Dependents:\n1) Missing values might be similar to zero dependents\n2) do missing values have any relation with churn?\n\n#Occupation:\n1)Do missing values have similar behaviour to any other occupation\n2)do they have some relation with churn?\n\n#city:\n1)the respective cities can be found using branch_code\n\n#last_transaction:\n1)checking their previous month and current month and previous_quarter activity might give insight on their last transaction.\n2)Due to missing values in last_transactions we are seeing missing values in \"doy_ls_tran\",\"woy_ls_tran\",\"moy_ls_tran\",\"dow_ls_tran\"\n","83ecb32c":"#Summary\n1) Kendall and Spearman correlation seem to have very similar pattern between them, except the slight variation in magnitude of correlation.\n2) Too many variables with insignificant correlation.\n3) Major correlation lies between the transaction variables and balance variables.  \n4) extracting transaction information of current and previous months","239b206d":"#Customers who churned have significantly higher balance during immediate preceeding quarter.\n#Smaller Pvalue suggests that we reject null hypothesis.","efb1e7cd":"#Summary of previous_month\n1) This looks very similar to current_month. Most of the customers perform low amount transactions. Therefore we need to find out whether there exists a relationship beetween current month and previous month balances. This will done in Bivariate analysis.\n","2b574922":"#cities having less than 1 percent of the total have significantly different churn rates as compared to the cities with more accounts.","333892ff":"#Now, before applying linear model such as logistic regression, we need to\n#scale the data and keep all features as numeric strictly.\n"}}