{"cell_type":{"3c91accc":"code","784da439":"code","e13a8bf9":"code","836ac589":"code","b208bd7b":"code","afeb6fb3":"code","0e2db729":"code","c8706760":"code","affbb408":"markdown","df0fe930":"markdown","77997db2":"markdown","28d6eaa5":"markdown","835b57f0":"markdown"},"source":{"3c91accc":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport os\n\n# Importing Deep Learning Libraries\n\nfrom keras.preprocessing.image import load_img, img_to_array\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Dense,Input,Dropout,GlobalAveragePooling2D,Flatten,Conv2D,BatchNormalization,Activation,MaxPooling2D\nfrom keras.models import Model,Sequential\nfrom keras.optimizers import Adam,SGD,RMSprop","784da439":"picture_size = 48\nfolder_path = \"..\/input\/face-expression-recognition-dataset\/images\/\"","e13a8bf9":"\nexpression = 'angry'\nplt.style.use('dark_background')\nplt.figure(figsize= (12,12))\nfor i in range(1, 10, 1):\n    plt.subplot(3,3,i)\n    img = load_img(folder_path+\"train\/\"+expression+\"\/\"+\n                  os.listdir(folder_path + \"train\/\" + expression)[i], target_size=(picture_size, picture_size))\n    plt.imshow(img)   \nplt.show()","836ac589":"batch_size  = 128   # files in each iteration\n\ndatagen_train  = ImageDataGenerator()\ndatagen_val = ImageDataGenerator()\n\ntrain_set = datagen_train.flow_from_directory(folder_path+\"train\",\n                                              target_size = (picture_size,picture_size),\n                                              color_mode = \"grayscale\",\n                                              batch_size=batch_size,\n                                              class_mode='categorical',\n                                              shuffle=True)\n\n\ntest_set = datagen_val.flow_from_directory(folder_path+\"validation\",\n                                              target_size = (picture_size,picture_size),\n                                              color_mode = \"grayscale\",\n                                              batch_size=batch_size,\n                                              class_mode='categorical',\n                                              shuffle=False)\n","b208bd7b":"no_of_classes = 7    \n\nmodel = Sequential()\n\n#1st CNN layer\nmodel.add(Conv2D(64,(3,3),padding = 'same',input_shape = (48,48,1)))    # (3,3) is the kernel size, 64 is number of filters\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size = (2,2)))   # extract important feature from specified area\nmodel.add(Dropout(0.25))\n\n#2nd CNN layer\nmodel.add(Conv2D(128,(5,5),padding = 'same'))   # 128 number of filters\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size = (2,2)))\nmodel.add(Dropout (0.25))\n\n#3rd CNN layer\nmodel.add(Conv2D(512,(3,3),padding = 'same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size = (2,2)))\nmodel.add(Dropout (0.25))\n\n#4th CNN layer\nmodel.add(Conv2D(512,(3,3), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())   # flatten layer, collapse input size to 1 dimension\n\n#Fully connected 1st layer\nmodel.add(Dense(256))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))\n\n\n# Fully connected layer 2nd layer\nmodel.add(Dense(512))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))\n\nmodel.add(Dense(no_of_classes, activation='softmax'))\n\n\n\nopt = Adam(lr = 0.0001)\nmodel.compile(optimizer=opt,loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()\n ","afeb6fb3":"from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n\n\n\ncheckpoint = ModelCheckpoint(\"model.h5\", monitor='val_acc', verbose=1, save_best_only=True, mode='max')   # check model \n\nearly_stopping = EarlyStopping(monitor='val_loss',\n                          min_delta=0,\n                          patience=3,\n                          verbose=1,\n                          restore_best_weights=True\n                          )     # getting best model and accuracy not increasing then stop from over fitting, restire best weights\n\nreduce_learningrate = ReduceLROnPlateau(monitor='val_loss',\n                              factor=0.2,\n                              patience=3,\n                              verbose=1,\n                              min_delta=0.0001)      # model not cope up with particular defined learning rate \n\ncallbacks_list = [early_stopping,checkpoint,reduce_learningrate]\n\nepochs = 48\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer = Adam(lr=0.001),\n              metrics=['accuracy'])\n\nhistory = model.fit_generator(generator=train_set,\n                                steps_per_epoch=train_set.n\/\/train_set.batch_size,\n                                epochs=epochs,\n                                validation_data = test_set,\n                                validation_steps = test_set.n\/\/test_set.batch_size,\n                                callbacks=callbacks_list\n                                )","0e2db729":"history = model.fit_generator(generator=train_set,\n                                steps_per_epoch=train_set.n\/\/train_set.batch_size,\n                                epochs=epochs,\n                                validation_data = test_set,\n                                validation_steps = test_set.n\/\/test_set.batch_size,\n                                callbacks=callbacks_list\n                                )","c8706760":"plt.style.use('dark_background')\n\nplt.figure(figsize=(20,10))\nplt.subplot(1, 2, 1)\nplt.suptitle('Optimizer : Adam', fontsize=10)\nplt.ylabel('Loss', fontsize=16)\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.legend(loc='upper right')\n\nplt.subplot(1, 2, 2)\nplt.ylabel('Accuracy', fontsize=16)\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.show()\n","affbb408":"\n****Model Building****","df0fe930":"**Plotting Accuracy & Loss**","77997db2":"**Displaying Images**","28d6eaa5":"**Fitting the Model with Training and Validation Data**","835b57f0":"**Making Training and Validation Data**"}}