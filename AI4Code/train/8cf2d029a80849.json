{"cell_type":{"f831ab8f":"code","daf2e3ab":"code","9ddc4d8a":"code","67893506":"code","0bdde20a":"code","f5534926":"code","f13dc825":"code","fe2952d4":"code","e6c08a3b":"code","90817478":"code","7e1af93c":"code","3d3cbb54":"code","64427756":"code","d532ccae":"code","8241fb57":"code","d435145a":"code","a874277e":"code","5f15b28b":"code","e07612dd":"code","a520a155":"code","b5d80ef0":"code","b450fc33":"code","b9534051":"code","6c7b6078":"code","8c5f0718":"code","a4a7a224":"code","0235b1e6":"code","ee8bc98d":"code","89022692":"code","bb7f7976":"code","677428d5":"code","f15d2403":"code","8f2eab71":"code","7bea1213":"code","d3f5090b":"code","0097dc1a":"code","3ae20d31":"code","ea7aae91":"code","4de99ce3":"code","d6aa114d":"code","edcbd09a":"code","e04fd92b":"code","e0818917":"code","ac03a445":"markdown","78c1d3cb":"markdown","5b74ce90":"markdown","f790d042":"markdown","4739734d":"markdown","96d9af01":"markdown","128a2a9f":"markdown","35e5a866":"markdown","9e41c35d":"markdown","935fddac":"markdown","199daa34":"markdown","59bd4378":"markdown","092ff993":"markdown","6d82cc00":"markdown","b385819b":"markdown","98e736a7":"markdown","c17cf8a4":"markdown","38ea1299":"markdown","bf3ded11":"markdown","b1cfca6f":"markdown","cadb6175":"markdown","e2156bb6":"markdown","8af0ac3a":"markdown","ec635426":"markdown","a69bd83b":"markdown","9e7067a1":"markdown","61551f4f":"markdown","90e3448f":"markdown"},"source":{"f831ab8f":"# import and check tensorflow\nimport tensorflow as tf\nprint(\"TF version:\", tf.__version__)","daf2e3ab":"import tensorflow as tf\nimport tensorflow_hub as hub\n\nprint(\"TF version:\", tf.__version__)\nprint(\"Hub version:\", hub.__version__)\n\n# Check for GPU\nif tf.config.list_physical_devices(\"GPU\"):\n    print(\"GPU is available\")\nelse:\n    print(\"not available\")","9ddc4d8a":"\nimport pandas as pd\nlabels = pd.read_csv(\"..\/input\/dog-breed-identification\/labels.csv\")\nprint(labels.head())","67893506":"#columns of the labels.csv file\nlabels.columns","0bdde20a":"#displaying an image from the data\nfrom IPython.display import display, Image\nImage(\"..\/input\/dog-breed-identification\/train\/001cdf01b096e06d78e9e5112d419397.jpg\")","f5534926":"# Creating pathnames\nfilenames = []\nfor name in labels[\"id\"]:\n    filenames.append(\"..\/input\/dog-breed-identification\/train\/\" + name + \".jpg\")\n\n# Check the first 10 filenames\nfilenames[:10]","f13dc825":"# Check whether number of filenames matches number of actual image files\nimport os\nif len(os.listdir(\"..\/input\/dog-breed-identification\/train\/\")) == len(filenames):\n  print(\"There are no missing files.\")\nelse:\n  print(\"Files missing\")","fe2952d4":"# Check an image directly from a filepath\nImage(filenames[60])","e6c08a3b":"import numpy as np\nlabel = labels[\"breed\"].to_numpy() # convert labels column to NumPy array\nlabel[:10]","90817478":"# See if number of labels matches the number of filenames\nif len(label) == len(filenames):\n  print(\"Exact number of files\")\nelse:\n  print(\"Some files are missing\")","7e1af93c":"# Find the number of unique label\nunique = np.unique(label)\nlen(unique)","3d3cbb54":"# Turn every label into a boolean array\nboolean_label = [element == np.array(unique) for element in label]\nboolean_label[:1]","64427756":"print(label[0]) # original label\nprint(np.where(unique == label[0])[0][0]) # index where label occurs in the number array\nprint(boolean_label[0].argmax()) # index where label occurs in boolean array\nprint(boolean_label[0].astype(int))","d532ccae":"# Setup X & y variables\nx = filenames\ny = boolean_label","8241fb57":"# Set number of images to use\nNUM_IMAGES = 1000\n","d435145a":"# Import train_test_split from Scikit-Learn\nfrom sklearn.model_selection import train_test_split\n\n# Split them into training and validation using NUM_IMAGES \nx_train, x_val, y_train, y_val = train_test_split(x[:NUM_IMAGES],\n                                                  y[:NUM_IMAGES], \n                                                  test_size=0.2,\n                                                  random_state=42)\n\nlen(x_train), len(y_train), len(x_val), len(y_val)","a874277e":"# Convert image to NumPy array\nfrom matplotlib.pyplot import imread\nimage = imread(filenames[1])\nimage.shape# read in an image","5f15b28b":"tf.constant(image)[:1]","e07612dd":"# Define image dimension\nIMG_SIZE = 224\n\ndef process(image_path):\n  \n  # Read in image file\n  image = tf.io.read_file(image_path)\n\n  # Turn the jpeg image into numerical Tensor with 3 colour channels (Red, Green, Blue)\n  image = tf.image.decode_jpeg(image, channels=3)\n\n  # Convert the colour channel values from 0-225 values to 0-1 values\n  image = tf.image.convert_image_dtype(image, tf.float32)\n\n  # Resize the image to our desired size (224, 244)\n  image = tf.image.resize(image, size=[IMG_SIZE, IMG_SIZE])\n  return image","a520a155":"# Creating function to return a tuple (image, label)\ndef image_label(path, label):\n \n  image = process(path)\n  return image, label","b5d80ef0":"# defining batch dimensions\nBATCH = 32\n\n# Create a function to turn data into batches\ndef data_batches(x, y=None, batch_size=BATCH , valid_data=False, test_data=False):\n    \n  # If the data is a test dataset\n  if test_data:\n    print(\"Test data batches created\")\n    data = tf.data.Dataset.from_tensor_slices((tf.constant(x))) # only filepaths\n    data_batch = data.map(process).batch(BATCH)\n    return data_batch\n  \n  # If the data if a valid dataset\n  elif valid_data:\n    \n    data = tf.data.Dataset.from_tensor_slices((tf.constant(x), # filepaths\n                                               tf.constant(y))) # labels\n    data_batch = data.map(image_label).batch(BATCH)\n    print(\"Validation data batches created\")\n    return data_batch\n\n  else:\n    # If the data is a training dataset, we shuffle it\n    \n    # Turn filepaths and labels into Tensors\n    data = tf.data.Dataset.from_tensor_slices((tf.constant(x), # filepaths\n                                              tf.constant(y))) # labels\n    \n    # Shuffling pathnames and labels\n    data = data.shuffle(buffer_size=len(x))\n\n    # Create (image, label) tuples\n    data = data.map(image_label)\n\n    # Turn the data into batches\n    data_batch = data.batch(BATCH)\n    print(\"Training data batches created\")\n  return data_batch","b450fc33":"# Create training and validation data batches\ntrain_data = data_batches(x_train, y_train)\nval_data = data_batches(x_val, y_val, valid_data=True)","b9534051":"# attributes of our data batches\ntrain_data.element_spec, val_data.element_spec","6c7b6078":"import matplotlib.pyplot as plt\n\n# Create a function for viewing images in a data batch\ndef show_images(images, labels):\n    \n  # Setup the figure\n  plt.figure(figsize=(20, 20))\n    \n  # Loop through 25 (for displaying 25 images)\n  for i in range(25):\n        \n    # Create subplots (5 rows, 5 columns)\n    ax = plt.subplot(5, 5, i+1)\n    # Display an image\n    plt.imshow(images[i])\n    # Add the image label as the title\n    plt.title(unique[labels[i].argmax()])\n    # Turn gird lines off\n    plt.axis(\"off\")","8c5f0718":"# Visualize training images from the training data batch\ntrain_images, train_labels = next(train_data.as_numpy_iterator())\nshow_images(train_images, train_labels)","a4a7a224":"# Visualize validation images from the validation data batch\nval_images, val_labels = next(val_data.as_numpy_iterator())\nshow_images(val_images, val_labels)","0235b1e6":"# Setup input shape\nINPUT_SHAPE = [None, IMG_SIZE, IMG_SIZE, 3] # batch, height, width, colour channels\n\n# Setup output shape\nOUTPUT_SHAPE = len(unique) # number of unique labels\n\n# model URL from TensorFlow Hub\nMODEL_URL = \"https:\/\/tfhub.dev\/google\/imagenet\/mobilenet_v2_130_224\/classification\/4\"","ee8bc98d":"# Create a function which builds a Keras model\ndef modelling(input_shape=INPUT_SHAPE, output_shape=OUTPUT_SHAPE, model_url=MODEL_URL):\n\n  # Setup the model layers\n  model = tf.keras.Sequential([\n    hub.KerasLayer(MODEL_URL), # Layer 1 (input layer)\n    tf.keras.layers.Dense(units=OUTPUT_SHAPE, \n                          activation=\"softmax\") # Layer 2 (output layer)\n  ])\n\n  # Compile the model\n  model.compile(\n      loss=tf.keras.losses.CategoricalCrossentropy(), # loss function to be reduced\n      optimizer=tf.keras.optimizers.Adam(), # Adam optimizer\n      metrics=[\"accuracy\"] \n  )\n\n  # Build the model\n  model.build(INPUT_SHAPE) \n  \n  return model","89022692":"# Create a model and check its details\ncreated_model = modelling()\ncreated_model.summary()","bb7f7976":"# Create early stopping\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\",\n                                                  patience=3) # stops after 3 rounds of no improvements","677428d5":"# Number of times our model passes through the data\nEPOCHS = 100 ","f15d2403":"# Build a function to train and return a trained model\ndef train_model():\n    \n  # Create a model\n  model = modelling()\n\n  # Fit the model to the data passing it the callbacks we created\n  model.fit(x=train_data,\n            epochs=EPOCHS,\n            validation_data=val_data,\n            validation_freq=1, # check validation metrics every epoch\n            callbacks=[early_stopping])\n  \n  return model","8f2eab71":"# Fit the model to the data\nmodel = train_model()","7bea1213":"# Make predictions on the validation data\npredictions = model.predict(val_data, verbose=1) # verbose shows us how long there is to go\npredictions","d3f5090b":"predictions.shape","0097dc1a":"# First prediction\nprint(predictions[0])\nprint(f\"Max value (probability of prediction): {np.max(predictions[0])}\") # the max probability value predicted by the model\nprint(f\"Sum: {np.sum(predictions[0])}\")\nprint(f\"Max index: {np.argmax(predictions[0])}\") # the index of where the max value in predictions[0] occurs\nprint(f\"Predicted label: {unique[np.argmax(predictions[0])]}\") # the predicted label","3ae20d31":"\ndef pred_label(prediction_probabilities):\n\n  return unique[np.argmax(prediction_probabilities)]\n\nprediction_label = pred_label(predictions[0])\nprediction_label","ea7aae91":"# function to unbatch a batched dataset\ndef unbatchify(data):\n  \n  images = []\n  labels = []\n\n  # Loop through unbatched data\n  for image, label in data.unbatch().as_numpy_iterator():\n    images.append(image)\n    labels.append(unique[np.argmax(label)])\n  return images, labels\n\n# Unbatchify the validation data\nval_images, val_labels = unbatchify(val_data)\nval_images[0], val_labels[0]","4de99ce3":"def plot_pred(prediction_probabilities, labels, images, n=1):\n \n  pred_prob, true_label, image = prediction_probabilities[n], labels[n], images[n]\n  \n  # Get the pred label\n  prediction_label = pred_label(pred_prob)\n  \n  # Plot image & remove ticks\n  plt.imshow(image)\n  plt.xticks([])\n  plt.yticks([])\n\n  # Change the color of the title depending on if the prediction is right or wrong\n  if prediction_label == true_label:\n    color = \"green\"\n  else:\n    color = \"red\"\n\n  plt.title(\"{} {:2.0f}% ({})\".format(prediction_label,\n                                      np.max(pred_prob)*100,\n                                      true_label),\n                                      color=color)","d6aa114d":"# example prediction, original image and truth label\nplot_pred(prediction_probabilities=predictions,\n          labels=val_labels,\n          images=val_images)","edcbd09a":"def plot_pred_conf(prediction_probabilities, labels, n=1):\n \n  pred_prob, true_label = prediction_probabilities[n], labels[n]\n\n  # Get the predicted label\n  prediction_label = pred_label(pred_prob)\n\n  # Find the top 7 prediction confidence indexes\n  pred_indexes_7 = pred_prob.argsort()[-7:][::-1]\n    \n  # Find the top 7 prediction confidence values\n  pred_values_7 = pred_prob[pred_indexes_7]\n    \n  # Find the top 7 prediction labels\n  pred_labels_7 = unique[pred_indexes_7]\n\n  # Setup plot\n  top_plot = plt.bar(np.arange(len(pred_labels_7)), \n                     pred_values_7, \n                     color=\"purple\")\n  plt.xticks(np.arange(len(pred_labels_7)),\n             labels=pred_labels_7,\n             rotation=\"vertical\")\n\n  # Change color of true label\n  if np.isin(true_label, pred_labels_7):\n    top_plot[np.argmax(pred_labels_7 == true_label)].set_color(\"orange\")\n  else:\n    pass","e04fd92b":"plot_pred_conf(prediction_probabilities=predictions,\n               labels=val_labels,\n               n=10)","e0818917":"# few predictions and their different values\nmult = 0\nrows = 2\ncols = 2\nnum_images = rows*cols\nplt.figure(figsize=(5*1.5*cols, 5*rows))\nfor i in range(num_images):\n  plt.subplot(rows, 2*cols, 2*i+1)\n  plot_pred(prediction_probabilities=predictions,\n            labels=val_labels,\n            images=val_images,\n            n=i+mult)\n  plt.subplot(rows, 2*cols, 2*i+2)\n  plot_pred_conf(prediction_probabilities=predictions,\n                labels=val_labels,\n                n=i+mult)\nplt.tight_layout(h_pad=1.0)\nplt.show()","ac03a445":"### Turning images into Tensors\n\nDefine a function that:\n1. Takes an image filename as input.\n2. Uses TensorFlow to read the file and save it to a variable, `image`.\n3. Turn our `image` into Tensors.\n4. Resize the `image` to be of shape (224, 224).\n5. Return the modified `image`.","78c1d3cb":"## Predictions using the trained model\n","5b74ce90":"That's for one example, let's do the whole thing.","f790d042":"Starting with 1000 images","4739734d":"Since we have shuffled the train data in the function , everytime we run he function we will get a different set of pictures for the training data","96d9af01":"Thankyou","128a2a9f":"3 is the height of the image (red,green,blue)","35e5a866":"### Creating our own validation set\n\nUsing scikit learn train_test_split to split the data into train and validation set.","9e41c35d":"Wonderful! Now we've got a list of all different predictions our model has made, we'll do the same for the validation images and validation labels.\n\nRemember, the model hasn't trained on the validation data, during the `fit()` function, it only used the validation data to evaluate itself. So we can use the validation images to visually compare our models predictions with the validation labels.\n\nSince our validation data (`val_data`) is in batch form, to get a list of validation images and labels, we'll have to unbatch it (using [`unbatch()`](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/data\/Dataset#unbatch)) and then turn it into an iterator using [`as_numpy_iterator()`](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/data\/Dataset#as_numpy_iterator).\n\nLet's make a small function to do so.","935fddac":"Transfer Learning and TensorFlow 2.0 to Classify Different Dog Breeds\n\n\n\n\nIn this project we're going to be using deep learning to help us identify different breeds of dogs.\n\nTo do this, we'll be using data from the [Kaggle dog breed identification competition](https:\/\/www.kaggle.com\/c\/dog-breed-identification\/overview).\n\nIt is an example of multi class classification.\n\nSince the most important step in a deep learng problem is getting the data ready (turning it into numbers), that's what we're going to start with.\n\nWe're going to go through the following TensorFlow\/Deep Learning workflow:\n1. Get data ready (download from Kaggle, store, import).\n2. Prepare the data (preprocessing, the 3 sets, X & y).\n3. Choosing a model.\n4. Evaluating a model.\n5. Improve the model through experimentation (start with 1000 images, make sure it works, increase the number of images).\n","199daa34":"# Creating and training a model\n\nUsing a pretrained machine learning model is often referred to as **transfer learning**.\n\n\n We will use [mobilenet_v2_130_224](https:\/\/tfhub.dev\/google\/imagenet\/mobilenet_v2_130_224\/classification\/4)\n","59bd4378":"Wonderful, now lets do the same thing as before, compare the amount of labels to number of filenames.","092ff993":"Predicting dog breed","6d82cc00":"Making predictions with our model returns an array with a different value for each label.\n\nIn this case, making predictions on the validation data (200 images) returns an array (`predictions`) of arrays, each containing 120 different values (one for each unique dog breed).\n\nThese different values are the probabilities or the likelihood the model has predicted a certain image being a certain breed of dog. The higher the value, the more likely the model thinks a given image is a specific breed of dog.\n\nLet's see how we'd convert an array of probabilities into an actual label.","b385819b":"Cute!\n\nNow we've got our image filepaths together, let's get the labels.\n\nWe'll take them from `labels_csv` and turn them into a NumPy array.","98e736a7":"Visualization","c17cf8a4":"### Creating data batches\n\nIn deep learning, instead of finding patterns in an entire dataset at the same time, you often find them one batch at a time.\nHere we will create batches of size 32.","38ea1299":"Now let's split our data into training and validation sets. We'll use and 80\/20 split (80% training data, 20% validation data).","bf3ded11":"Training model to fit training data and callback","b1cfca6f":"Visualizing one of the images from the created list","cadb6175":"### Images and labels\n\nCreating list of images from their label IDs ","e2156bb6":"Check if all files are complete","8af0ac3a":"Visualizing the predictions","ec635426":"\n\n### Accessing the data\n\nImport the labels.csv file","a69bd83b":"Converting array of numbers to boolean array","9e7067a1":"#### Early Stopping Callback\n\nEarly stopping helps to maintain the quality of output of our model and if it starts to go down , it stops the model. \n\n","61551f4f":"The model is clearly overfitting as it performs far better on the training data than on validation data","90e3448f":"Looking at this, we can see there are 10222 different ID's (meaning 10222 different images) and 120 different breeds.\n\nLet's figure out how many images there are of each breed."}}