{"cell_type":{"183ad55c":"code","4ca760af":"code","0fa89848":"code","98fda00f":"code","1f6edb77":"code","b4987916":"code","6b172610":"code","d3b27730":"code","fe4597fc":"code","10a8af3e":"code","de10cba9":"code","08830a24":"code","1c63034a":"code","b6323dba":"code","e7a4934e":"code","1661d088":"code","cc1fa730":"markdown","9c95ba43":"markdown"},"source":{"183ad55c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# visualization\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4ca760af":"# Read in dataset - create Pandas Training & Test data objects to work with\n\ntrain_df = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\ntest_df = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/test.csv')\n","0fa89848":"train_df.info(null_counts=True) #Shows each Column name, Non-Null Count and Object type for that Column","98fda00f":"test_df.info(null_counts=True) #Shows each Column name, Non-Null Count and Object type for that Column","1f6edb77":"# Analyze dataset as needed to identify possible corrections (for clearly-incorrect entries), estimates \n# (for missing data i.e. null entries) as well as notable trends that may guide the type of predictor used.\n\ntrain_df.describe(include='all')","b4987916":"# Pivot Item Count data based on various inputs, e.g. Item Price to look for patterns\n\ntrain_df[['item_price', 'item_cnt_day']].groupby(['item_price'], as_index=False).mean().sort_values(by='item_cnt_day',ascending=False)","6b172610":"# Pivot Item Count data based on various inputs, e.g. Item ID to look for patterns\n\ntrain_df[['item_id', 'item_cnt_day']].groupby(['item_id'], as_index=False).mean().sort_values(by='item_cnt_day',ascending=False)","d3b27730":"# Pivot Item Count data based on various inputs, e.g. Shop ID to look for patterns\n\ntrain_df[['shop_id', 'item_cnt_day']].groupby(['shop_id'], as_index=False).mean().sort_values(by='item_cnt_day',ascending=False)","fe4597fc":"# Pivot Item Count data based on various inputs, e.g. Shop ID & Item ID to look for patterns\n\ntrain_df[['shop_id','item_id', 'item_cnt_day']].groupby(['shop_id','item_id'], as_index=False).mean().sort_values(by='item_cnt_day',ascending=False)","10a8af3e":"# Analyze data visually to look for patterns\n\n#g = sns.FacetGrid(train_df)\n#g.map(plt.hist, 'item_price', bins=500)","de10cba9":"# Prepare dataset as needed; combine Training & Test data if similar formatting changes needed\n#ENSURE CATEGORICAL COLUMNS CONVERTED TO NUMERICAL (OR DROPPED IF NOT NEEDED), USING ONE-HOT OR OTHER ENCODING METHOD!!!\nimport datetime\n\n#combine = [train_df, test_df]\n\n# Use copy of Training data to adjust\/convert\/etc. so that initial dataset remains intact\ntrain_df_final = train_df.copy()\n\n# Convert 'date' Column into numerical values (Try to find conversion to int equivalent, like MS Excel). Consider dropping 'date_block_num' if not relevant to sales prediction. \nint_date = pd.to_datetime(train_df['date'], format='%d.%m.%Y')\ntrain_df_final['date'] = int_date.astype(int)\n\ntrain_df_final.head()\n#train_df_final.tail()\n#train_df_final.info(null_counts=True) #Shows each Column name, Non-Null Count and Object type for that Column\n#train_df_final.describe() #Shows Count\/Mean\/STD\/Quartile\/Min-Max data for Numeric Columns\n#train_df_final.shape #Shows Row-Column counts","08830a24":"# Choose model (or group of models) to test\n\nfrom sklearn.linear_model import LogisticRegression\n#from sklearn.svm import SVC, LinearSVC\n#from sklearn.ensemble import RandomForestClassifier\n#from sklearn.neighbors import KNeighborsClassifier\n#from sklearn.naive_bayes import GaussianNB\n#from sklearn.linear_model import Perceptron\n#from sklearn.linear_model import SGDClassifier\n#from sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.model_selection import train_test_split\n\n#Set up Training & Test data objects to use for training & testing, including making training\/validation splits\n\nX_train, X_val, y_train, y_val = train_test_split(train_df_final.drop(\"item_cnt_day\", axis=1), train_df_final[\"item_cnt_day\"], random_state=1)\nX_test = test_df.copy()\n\nlogreg = LogisticRegression()\n","1c63034a":"# Fit model (if doing supervised learning)\n\n#logreg.fit(X_train, y_train)\n","b6323dba":"# Make predictions\n\n#y_pred = logreg.predict(X_val)\n","e7a4934e":"# Measure model accuracy\n\nfrom sklearn.metrics import mean_absolute_error\n\n#mae = mean_absolute_error(y_pred, y_val)\n#print(mae)\n\n#acc_log = round(logreg.score(X_train, y_train) * 100, 2)\n\n#acc_log","1661d088":"# Look for ways to improve accuracy w\/o overfitting\n","cc1fa730":"Ranges:\ndate_block_num - 0 - 33;\nshop_id - 0 - 59;\nitem_id = 0 - 22169;\nitem_price - -1 - 307,980;\nitem_cnt_day - -22 - 2,169\n\nNotable info:\nAt least 75% of entries have Item Prices of 249+.\nAt least 75% of entries have 1 or less Items Sold that day.\n","9c95ba43":"HELPFUL HINT: If the \"Non-Null Count\" is equal for all of the columns, then no data is missing. If a small %\nof values from one column is missing (e.g <=25%), then you can estimate\/impute the missing values using \nSKLearn's SimpleImputer or another impute tool. If the % of missing values is large, you may need to consider\neither dropping the rows with missing data (if the remaining dataset would still be large-enough for your \nanalysis) or dropping the column altogether."}}