{"cell_type":{"dab500d3":"code","740d2981":"code","616a71be":"code","49466faf":"code","575162d5":"code","5a1c6996":"code","624c50d3":"code","dfc14aff":"code","3d0e3aaa":"code","9b216c24":"code","a540ad98":"code","e3c33d88":"code","9bbd3a03":"code","f41ecb1b":"code","edc4e7ae":"code","536c6bba":"code","56ba3369":"code","eb540d5b":"code","01c45d1b":"code","70ff65ab":"code","e90d012e":"markdown","4f7d4ae6":"markdown","7e1fb50e":"markdown","21595da1":"markdown","4dbd48fe":"markdown","3f7edd0d":"markdown","e0a60755":"markdown","ad865760":"markdown","eedeb100":"markdown","d9439b9a":"markdown","5592af21":"markdown","3f52a9c8":"markdown","0314a1bf":"markdown","3cd0685f":"markdown","a68582d5":"markdown","0d2d42ff":"markdown"},"source":{"dab500d3":"# ----------------------------\n#\u00a0BASE\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport datetime as dt\n\n# ----------------------------\n# INSTALL\n# !pip install mlxtend\n\n# ----------------------------\n#\u00a0TRANSACTION ENCODER\nfrom mlxtend.preprocessing import TransactionEncoder\n\n# ----------------------------\n#\u00a0APRIORI FUNCTION \nfrom mlxtend.frequent_patterns import apriori, association_rules\n\n# ----------------------------\n#\u00a0ITERTOOLS \nimport itertools\n\n# ----------------------------\n#\u00a0CONFIGURATION\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\npd.set_option('display.max_columns', None)\npd.options.display.float_format = '{:.2f}'.format","740d2981":"df = pd.read_csv(\"..\/input\/supermarket\/GroceryStoreDataSet.csv\",names=['products'],header=None)\ndf","616a71be":"df.shape","49466faf":"data = list(df[\"products\"].apply(lambda x:x.split(',')))\n\nfrom mlxtend.preprocessing import TransactionEncoder\nte = TransactionEncoder()\nte_data = te.fit(data).transform(data)\ndf = pd.DataFrame(te_data,columns=te.columns_).astype(int)\n\ndf","575162d5":"p = df.copy()\nfor i in range(1, len(p.columns)+1):\n    p[\"Cat\"] = np.where(p[p.columns[i]] == 1, 1, 0)\n    p[p.columns[i]]= i\n    g =sns.scatterplot(p.index, p[p.columns[i]], hue = p.Cat, legend = False)\n    g.yaxis.set_label_text('Products')\n    g.set_yticks(np.arange(1, len(p.columns)))\n    g.set_xticks(df.index)\n    g.set_yticklabels(df.columns)\n    plt.title(\"Data Structure\")\n","5a1c6996":"# Find Frequency of Items\ndf.sum()","624c50d3":"#\u00a0Product Frequency \/ Total Sales\nfirst = pd.DataFrame(df.sum() \/ df.shape[0], columns = [\"Support\"]).sort_values(\"Support\", ascending = False)\nfirst","dfc14aff":"# Elimination by Support Value\nfirst[first.Support >= 0.15]","3d0e3aaa":"second = list(itertools.combinations(first.index, 2))\nsecond = [list(i) for i in second]\n#\u00a0Sample of combinations\nsecond[:10]","9b216c24":"# Finding support values\nvalue = []\nfor i in range(0, len(second)):\n    temp = df.T.loc[second[i]].sum() \n    temp = len(temp[temp == df.T.loc[second[i]].shape[0]]) \/ df.shape[0]\n    value.append(temp)\n#\u00a0Create a data frame            \nsecondIteration = pd.DataFrame(value, columns = [\"Support\"])\nsecondIteration[\"index\"] = [tuple(i) for i in second]\nsecondIteration['length'] = secondIteration['index'].apply(lambda x:len(x))\nsecondIteration = secondIteration.set_index(\"index\").sort_values(\"Support\", ascending = False)\n# Elimination by Support Value\nsecondIteration = secondIteration[secondIteration.Support > 0.1]\nsecondIteration","a540ad98":"def ar_iterations(data, num_iter = 1, support_value = 0.1, iterationIndex = None):\n    \n    # Next Iterations\n    def ar_calculation(iterationIndex = iterationIndex): \n        # Calculation of support value\n        value = []\n        for i in range(0, len(iterationIndex)):\n            result = data.T.loc[iterationIndex[i]].sum() \n            result = len(result[result == data.T.loc[iterationIndex[i]].shape[0]]) \/ data.shape[0]\n            value.append(result)\n        #\u00a0Bind results\n        result = pd.DataFrame(value, columns = [\"Support\"])\n        result[\"index\"] = [tuple(i) for i in iterationIndex]\n        result['length'] = result['index'].apply(lambda x:len(x))\n        result = result.set_index(\"index\").sort_values(\"Support\", ascending = False)\n        # Elimination by Support Value\n        result = result[result.Support > support_value]\n        return result    \n    \n    # First Iteration\n    first = pd.DataFrame(df.T.sum(axis = 1) \/ df.shape[0], columns = [\"Support\"]).sort_values(\"Support\", ascending = False)\n    first = first[first.Support > support_value]\n    first[\"length\"] = 1\n    \n    if num_iter == 1:\n        res = first.copy()\n        \n    # Second Iteration\n    elif num_iter == 2:\n        \n        second = list(itertools.combinations(first.index, 2))\n        second = [list(i) for i in second]\n        res = ar_calculation(second)\n        \n    # All Iterations > 2\n    else:\n        nth = list(itertools.combinations(set(list(itertools.chain(*iterationIndex))), num_iter))\n        nth = [list(i) for i in nth]\n        res = ar_calculation(nth)\n    \n    return res","e3c33d88":"iteration1 = ar_iterations(df, num_iter=1, support_value=0.1)\niteration1","9bbd3a03":"iteration2 = ar_iterations(df, num_iter=2, support_value=0.1)\niteration2","f41ecb1b":"iteration3 = ar_iterations(df, num_iter=3, support_value=0.01,\n              iterationIndex=iteration2.index)\niteration3","edc4e7ae":"iteration4 = ar_iterations(df, num_iter=4, support_value=0.01,\n              iterationIndex=iteration3.index)\niteration4","536c6bba":"# Apriori\nfreq_items = apriori(df, min_support = 0.1, use_colnames = True, verbose = 1)\nfreq_items.sort_values(\"support\", ascending = False)","56ba3369":"freq_items.sort_values(\"support\", ascending = False).head(5)","eb540d5b":"freq_items.sort_values(\"support\", ascending = False).tail(5)","01c45d1b":"# Association Rules & Info\ndf_ar = association_rules(freq_items, metric = \"confidence\", min_threshold = 0.5)\ndf_ar","70ff65ab":"df_ar[(df_ar.support > 0.15) & (df_ar.confidence > 0.5)].sort_values(\"confidence\", ascending = False)","e90d012e":"<a id='ar'><\/a>\n<h1 style=\"color:forestgreen\" >6. Association Rules<\/h1> \n\nThere are two main functions here. \n- apriori() function evaluate support value for each product.\n- association_rules() function help us to understand relationship between antecedents and consequences products. It gives some remarkable information about products.\n\nEspecially, \"antecedent support\", \"consequent support\", \"support\", \"confidence\" and\t\"lift\" variables are very important to make some business decisions.","4f7d4ae6":"<a id='libraries'><\/a>\n<h1 style=\"color:forestgreen\" >Association Rules? What's that!<\/h1> \n\nAssociation rule learning is a rule-based machine learning method for discovering interesting relations between variables in large databases. It is intended to identify strong rules discovered in databases using some measures of interestingness.\n\nBased on the concept of strong rules, Rakesh Agrawal, Tomasz Imieli\u0144ski and Arun Swam introduced association rules for discovering regularities between products in large-scale transaction data recorded by point-of-sale (POS) systems in supermarkets. For example, the rule {onions,potatoes} ->{burger} found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as, e.g., promotional pricing or product placements.\n\nIn addition to the above example from market basket analysis association rules are employed today in many application areas including Web usage mining, intrusion detection, continuous production, and bioinformatics. In contrast with sequence mining, association rule learning typically does not consider the order of items either within a transaction or across transactions. [Wikipedia] \n\nFirst of all, we need to know some terms about Association Rules.\n\n- **Support:** Support is an indication of how frequently the itemset appears in the dataset.\n\nSupport is a so-called frequency constraint. Its main feature is that it possesses the property of down-ward closure which means that all sub sets of a frequent set (support > min. support threshold) are also frequent. This property (actually, the fact that no super set of a infrequent set can be frequent) is used to prune the search space (usually a tree of item sets with increasing size) in level-wise algorithms (e.g., the APRIORI algorithm). The disadvantage of support is the rare item problem. Items that occur very infrequently in the data set are pruned although they would still produce interesting and potentially valuable rules.\n\n- **Confidence:** Confidence is an indication of how often the rule has been found to be true.\n\nConfidence is not down-ward closed and was developed together with support (the so-called support-confidence framework). While support is used to prune the search space and only leave potentially interesting rules, confidence is used in a second step to filter rules that exceed a min. confidence threshold. A problem with confidence is that it is sensitive to the frequency of the consequent (Y) in the data set. Caused by the way confidence is calculated, Ys with higher support will automatically produce higher confidence values even if they exists no association between the items.\n\n- **Lift:**  The ratio of the observed support to that expected if X and Y were independent.\n\nLeverage measures the difference of X and Y appearing together in the data set and what would be expected if X and Y where statistically dependent. The rational in a sales setting is to find out how many more units (items X and Y together) are sold than expected from the independent sells. Using min. leverage thresholds at the same time incorporates an implicit frequency constraint. E.g., for setting a min. leverage thresholds to 0.01% (corresponds to 10 occurrence in a data set with 100,000 transactions) one first can use an algorithm to find all itemsets with min. support of 0.01% and then filter the found item sets using the leverage constraint. Because of this property leverage also can suffer from the rare item problem.\n\n- **Conviction:** The ratio of the expected frequency that X occurs without Y (that is to say, the frequency that the rule makes an incorrect prediction) if X and Y were independent divided by the observed frequency of incorrect predictions.\n\nConviction compares the probability that X appears without Y if they were dependent with the actual frequency of the appearance of X without Y. In that respect it is similar to lift (see section about lift on this page), however, it contrast to lift it is a directed measure. Furthermore, conviction is monotone in confidence and lift.\n\n- **Leverage:** Leverage measures the difference of X and Y appearing together in the data set and what would be expected if X and Y where statistically dependent.\n\nThe rational in a sales setting is to find out how many more units (items X and Y together) are sold than expected from the independent sells. Using min. leverage thresholds at the same time incorporates an implicit frequency constraint. E.g., for setting a min. leverage thresholds to 0.01% (corresponds to 10 occurrence in a data set with 100,000 transactions) one first can use an algorithm to find all itemsets with min. support of 0.01% and then filter the found item sets using the leverage constraint. Because of this property leverage also can suffer from the rare item problem. [Michael Hahsler]\n\n\n","7e1fb50e":"<center><img\nsrc=\"https:\/\/lh3.googleusercontent.com\/proxy\/Cb3QDxNobx1fBiPevV54WqsodXpXyrNuUQ1J00viqe5-ep2pgTUy-TVlYinwcTmdG_Wu5bcxxZY1q8XxVNtL6COkUP8UtcswpbepNoYt8TCqMfaa3SBV_w8UG5ZSx9KMMMEMTwHO_hXs30FsbvqbevAK_HM\" style=\"width:40%;height:40%;\">\n<\/center>","21595da1":"If we divide all items with row number, we can find Support value. Our threshold value is 0.2 for Support value.","4dbd48fe":"- Antecedent support variable tells us probability of antecedent products alone\n- Consequents support variable tells us probability of consequents products alone\n- The support value is the value of the two products (Antecedents and Consequents)\n- Confidence is an indication of how often the rule has been found to be true.\n- The ratio of the observed support to that expected if X and Y were independent.","3f7edd0d":"**First Iteration:** Find support values for each product.\n\n- n: 20 (df.shape[0])","e0a60755":"<center> <h1 style=\"background-color:seagreen; color:white\" >Apriori Association Rules<\/h1> \n\n<center><img\nsrc=\"https:\/\/media-exp1.licdn.com\/dms\/image\/C4D12AQESnIfDsWzHgw\/article-cover_image-shrink_600_2000\/0?e=1603929600&v=beta&t=QFUbJY8HYJi2KYoxlEur1w5M5NPaU2h-S0OBlPKA8mo\" style=\"width:70%;height:40%;\">\n<\/center>\n    \n<br>    \n    \n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n  <h3 class=\"list-group-item list-group-item-action active\" style=\"background-color:seagreen; color:white\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\">Notebook Content!<\/h3>  \n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#libraries\" role=\"tab\" aria-controls=\"profile\" style=\"color:forestgreen\">Import Libraries<span class=\"badge badge-primary badge-pill\" style=\"background-color:khaki; color:gray\">1<\/span><\/a>\n  <a id=\"section2\" class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#load\" role=\"tab\" aria-controls=\"messages\" style=\"color:forestgreen\">Load Data<span class=\"badge badge-primary badge-pill\" style=\"background-color:khaki; color:gray\">2<\/span><\/a>\n  <a class=\"list-group-item list-group-item-action\"  data-toggle=\"list\" href=\"#tidydata\" role=\"tab\" aria-controls=\"settings\" style=\"color:forestgreen\">Tidy Data for Association Rules<span class=\"badge badge-primary badge-pill\" style=\"background-color:khaki; color:gray\">3<\/span><\/a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#step\" role=\"tab\" aria-controls=\"settings\" style=\"color:forestgreen\">Step by Step<span class=\"badge badge-primary badge-pill\" style=\"background-color:khaki; color:gray\">4<\/span><\/a> \n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#myfunction\" role=\"tab\" aria-controls=\"settings\" style=\"color:forestgreen\">My Function<span class=\"badge badge-primary badge-pill\" style=\"background-color:khaki; color:gray\">5<\/span><\/a>\n    <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#ar\" role=\"tab\" aria-controls=\"settings\" style=\"color:forestgreen\"> Association Rules<span class=\"badge badge-primary badge-pill\" style=\"background-color:khaki; color:gray\">6<\/span><\/a>\n    <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#notebooks\" role=\"tab\" aria-controls=\"settings\" style=\"color:forestgreen\">My Notebooks<span class=\"badge badge-primary badge-pill\" style=\"background-color:khaki; color:gray\">7<\/span><\/a> \n        <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#ref\" role=\"tab\" aria-controls=\"settings\" style=\"color:forestgreen\">References<span class=\"badge badge-primary badge-pill\" style=\"background-color:khaki; color:gray\">8<\/span><\/a> ","ad865760":"<a id='myfunction'><\/a>\n<h1 style=\"color:forestgreen\" >5. My Function<\/h1> \n\nmlxtend package helps us to analyze association rules problems. In this content, I tried to create my own function for association rules.","eedeb100":"**Second Iteration:** Find support values for pair product combinations.","d9439b9a":"<a id='notebooks'><\/a>\n<h1 style=\"color:forestgreen\" >7. My Notebooks<\/h1> \n\n\n<div class=\"row\">\n  <div class=\"col-sm-3\">\n    <div class=\"card\">\n      <div class=\"card-body\" style=\"width: 18rem;\">\n        <h5 class=\"card-title\">Marvel Universe: Civil War<\/h5>\n          <img src=\"https:\/\/i.pinimg.com\/originals\/5d\/02\/9a\/5d029aa478d79c8e7bcc99d020bad47b.jpg\" class=\"card-img-top\" alt=\"...\">\n        <hr>\n        <a href=\"https:\/\/www.kaggle.com\/ekrembayar\/marvel-universe-civil-war\" class=\"btn btn-primary\" style=\"color:white;\">Go to Notebook<\/a>\n      <\/div>\n    <\/div>\n  <\/div>\n  <div class=\"col-sm-3\">\n    <div class=\"card\">\n      <div class=\"card-body\" style=\"width: 18rem;\">\n        <h5 class=\"card-title\">THE LAST DANCE<\/h5>\n          <img src=\"https:\/\/www.kolpaper.com\/wp-content\/uploads\/2020\/05\/The-Last-Dance-Wallpaper-2.jpg\" class=\"card-img-top\" alt=\"...\">\n        <hr>\n        <a href=\"https:\/\/www.kaggle.com\/ekrembayar\/the-last-dance-data-visualization\" class=\"btn btn-primary\" style=\"color:white;\">Go to Notebook<\/a>\n      <\/div>\n    <\/div>\n  <\/div>\n  <div class=\"col-sm-3\">\n    <div class=\"card\">\n      <div class=\"card-body\" style=\"width: 18rem;\">\n        <h5 class=\"card-title\">FBI Data Cleaning<\/h5>\n          <img src=\"http:\/\/idora.gazetevatan.com\/vatanmediafile\/Haber598x362\/2020\/07\/10\/fbi-sosyal-medya-fenomenini-kacirdi--4618794.Jpeg\" class=\"card-img-top\" alt=\"...\">\n        <hr>\n        <a href=\"https:\/\/www.kaggle.com\/ekrembayar\/hadley-wickham-s-tidy-principles-fbi-data-cleaning\" class=\"btn btn-primary\" style=\"color:white;\">Go to Notebook<\/a>\n      <\/div>\n    <\/div>\n  <\/div>\n  <div class=\"col-sm-3\">\n    <div class=\"card\">\n      <div class=\"card-body\" style=\"width: 18rem;\">\n        <h5 class=\"card-title\">Chess EDA with R<\/h5>\n          <img src=\"https:\/\/thebridge.in\/wp-content\/uploads\/2020\/03\/Chess-Image-Hans-India-1280x720.jpg\" class=\"card-img-top\" alt=\"...\">\n        <hr>\n        <a href=\"https:\/\/www.kaggle.com\/ekrembayar\/chess-eda-with-r\" class=\"btn btn-primary\" style=\"color:white;\">Go to Notebook<\/a>\n      <\/div>\n    <\/div>\n  <\/div>\n    \n    \n ","5592af21":"<a id='step'><\/a>\n<h1 style=\"color:forestgreen\" >4. Step by Step<\/h1> \n","3f52a9c8":"###\u00a0Support value gives us these information:\n\n**Head 5**\n- 65 percent of 100 purchases are \"BREAD\"\n- 40 percent of 100 purchases are \"COFFEE\"\n- 35 percent of 100 purchases are \"BISCUIT\"\n- 35 percent of 100 purchases are \"TEA\"\n- 30 percent of 100 purchases are \"CORNFLAKES\"\n\n**Tail 5**\n- 10 percent of 100 purchases are \"MAGGI\" and \"BISCUIT\"\n- 10 percent of 100 purchases are \"MILK\" and \"BISCUIT\"\n- 10 percent of 100 purchases are \"TEA\" and \"BISCUIT\"\n- 10 percent of 100 purchases are \"JAM\" and \"BISCUIT\"\n- 10 percent of 100 purchases are \"COCK\", \"COFFEE\", \"BISCUIT\" and \"CORNFLAKES\"","0314a1bf":"<a id='load'><\/a>\n<h1 style=\"color:forestgreen\" >2. Data<\/h1> ","3cd0685f":"<h3 style=\"color:forestgreen\" >If you like the notebook, don\u2019t forget upvote! :)<\/h3> \n\n<a id='ref'><\/a>\n<h1 style=\"color:forestgreen\" >8. References<\/h1> \n\n\n- https:\/\/www.kaggle.com\/mariekaram\/apriori-association-rule\n- https:\/\/www.kdnuggets.com\/2016\/04\/association-rules-apriori-algorithm-tutorial.html\n- https:\/\/towardsdatascience.com\/association-rules-2-aa9a77241654\n- https:\/\/michael.hahsler.net\/research\/recommender\/associationrules.html\n- https:\/\/en.wikipedia.org\/wiki\/Association_rule_learning#:~:text=Association%20rules%20are%20usually%20required,frequent%20itemsets%20in%20a%20database.","a68582d5":"<a id='libraries'><\/a>\n<h1 style=\"color:forestgreen\" >1. Packages<\/h1> ","0d2d42ff":"<a id='tidydata'><\/a>\n<h1 style=\"color:forestgreen\" >3. Tidy Data for Association Rules<\/h1> "}}