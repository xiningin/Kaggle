{"cell_type":{"2a213e2e":"code","9ebf99d6":"code","51eba6af":"code","9e77bf3b":"code","4f98dbea":"code","25236510":"code","d2658827":"code","a56f7033":"code","254c28b0":"code","6b19edf0":"code","c801f37d":"code","b423f496":"code","05e7855b":"markdown","9a8f4cab":"markdown","f1190cc2":"markdown","c25161ba":"markdown","324848bb":"markdown","2e5c6206":"markdown","2c94249a":"markdown","e2be5057":"markdown"},"source":{"2a213e2e":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import KFold,StratifiedKFold,train_test_split\nfrom sklearn.metrics import mean_squared_error as mse\n\nimport h5py\n\nimport tensorflow as tf \nfrom tensorflow.keras.layers import Input,LSTM,Bidirectional,Embedding,Dense, Conv1D, Dropout , MaxPool1D , MaxPooling1D, GlobalAveragePooling2D , GlobalAveragePooling1D\nfrom tensorflow.keras.metrics import RootMeanSquaredError\nfrom tensorflow.keras.models import Model,load_model,save_model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau,ModelCheckpoint, EarlyStopping ,LearningRateScheduler\nfrom tensorflow.keras import backend as K\n\nfrom transformers import RobertaTokenizerFast , TFRobertaModel\n\n","9ebf99d6":"max_len = 256\nbatch_size = 32\nAUTOTUNE = tf.data.AUTOTUNE\n\nMODEL=['bert-base-uncased' ,'roberta-base']\n\nimport os\nos.makedirs(\".\/result\")\n\nsave_dir=\".\/result\"","51eba6af":"paths=[\"\/kaggle\/input\/commonlitreadabilityprize\/sample_submission.csv\",\n\"\/kaggle\/input\/commonlitreadabilityprize\/train.csv\",\n\"\/kaggle\/input\/commonlitreadabilityprize\/test.csv\"]\n\ndf_train=pd.read_csv(paths[1])\ndf_test=pd.read_csv(paths[2])\ndf_ss=pd.read_csv(paths[0])","9e77bf3b":"df_train = df_train.drop(['url_legal','license','standard_error'],axis='columns')\ndf_test = df_test.drop(['url_legal','license'],axis='columns')","4f98dbea":"df_train.columns, df_test.columns","25236510":"X= df_train['excerpt']\ny=df_train['target']\n\nX_test = df_test['excerpt']","d2658827":"tokenizer = RobertaTokenizerFast.from_pretrained(MODEL[1])\ntokenizer.save_pretrained(\".\/result\/roberta-tokenizer\")","a56f7033":"@tf.function\ndef map_function(encodings , target):\n    input_ids = encodings['input_ids']\n    attention_mask = encodings['attention_mask']\n    \n    target = tf.cast(target, tf.float32 )\n    \n    return {'input_ids': input_ids , 'attention_mask': attention_mask}, target","254c28b0":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","6b19edf0":"def create_model(roberta_model):\n  input_layer_id = Input(shape=(max_len,) ,dtype=tf.int32, name = 'input_ids')\n  input_layer_mask = Input(shape=(max_len,) ,dtype=tf.int32, name = 'attention_mask')\n    \n  roberta = roberta_model.roberta(input_ids = input_layer_id , attention_mask = input_layer_mask)[0]\n  roberta_output = roberta[:,0,:]\n  x= Dropout(0.2)(roberta_output)\n  predictions = Dense(1,activation='linear')(x)\n    \n  model = Model(inputs=[input_layer_id, input_layer_mask] , outputs=predictions)\n  model.compile(\n      optimizer = Adam(learning_rate=1e-5),\n      metrics = RootMeanSquaredError(),\n      loss = \"mse\"\n  )\n  return model\n\n","c801f37d":"with strategy.scope():\n  roberta_model = TFRobertaModel.from_pretrained(MODEL[1])\n  model = create_model(roberta_model)\n\nmodel.summary()","b423f496":"scores=[]\niterations = 1\nkfold = KFold(n_splits=5, shuffle= True , random_state = 2021)\nfor train_idx, test_idx in kfold.split(X,y):\n    print(\"************** iteration\",iterations,\"**************\")\n    X_train = X.loc[train_idx]\n    X_test = X.loc[test_idx]\n    y_train = y.loc[train_idx]\n    y_test = y.loc[test_idx]\n    \n    X_train = X_train.tolist()\n    X_test = X_test.tolist()\n\n    y_train = y_train.tolist()\n    y_test = y_test.tolist()\n\n    #tokenization\n    print('tokenization')\n    train_embeddings = tokenizer(X_train, truncation = True , padding = True , max_length=max_len)\n    test_embeddings = tokenizer(X_test , truncation = True , padding =True , max_length = max_len)\n    \n    print(train_embeddings.keys())\n    train = tf.data.Dataset.from_tensor_slices((train_embeddings,y_train))\n\n    train = (\n            train\n            .shuffle(1024)\n            .map(map_function, num_parallel_calls=AUTOTUNE)\n            .batch(16)\n            .prefetch(AUTOTUNE)\n        )\n\n\n    test = tf.data.Dataset.from_tensor_slices((test_embeddings , y_test))\n    test = (\n        test\n        .map(map_function, num_parallel_calls = AUTOTUNE)\n        .batch(16)\n        .prefetch(AUTOTUNE)\n    )\n    #Clearing backend session\n    K.clear_session()\n    print(\"Backend Cleared\")\n    \n    early_stopping=EarlyStopping(monitor=\"val_root_mean_squared_error\",min_delta=0,patience=5,verbose=0,mode=\"min\",restore_best_weights=True)\n    reduce_lr=ReduceLROnPlateau(monitor=\"val_root_mean_squared_error\",factor=0.2,patience=5,min_lr=0.00001)\n\n    hist=model.fit(train,validation_data=test,epochs=20, callbacks=[early_stopping, reduce_lr])\n\n    #prediction\n    print(\"predicting\")\n    y_pred = model.predict(test)\n    print(np.sqrt(mse(y_pred,y_test)))\n    scores.append(np.sqrt(mse(y_pred,y_test)))\n      \n    #saving model\n    print(\"saving model\")\n    localhost_save_option = tf.saved_model.SaveOptions(experimental_io_device=\"\/job:localhost\")\n    model.save(f'{save_dir}\/roberta_{iterations}', options=localhost_save_option)\n    \n    iterations+=1\nprint(\"the final average rmse is \", np.mean(scores))","05e7855b":"# \u2699\ufe0f Parameters","9a8f4cab":"# \ud83d\udcca Dataset Prep Function","f1190cc2":"# Abstract\n\nTrain and prototype your models quickly by using TPUs. \nThis notebook shows easy and quick way to train \ud83e\udd17Transformers on TPUs.\n\n# Versions \n * Version 2 : Basic roberta on TPU, CV :- 0.425 , LB:- 0.543 \n * Version 3 : Correction in KFold\n#### Future Work \n* To improve input pipeline by incorporating TFrecords \n","c25161ba":"# Imports","324848bb":"# \ud83e\udde0 Modelling","2e5c6206":"# Define Tokenizer","2c94249a":"# References\n1. [Notebook 1 ](https:\/\/www.kaggle.com\/miklgr500\/jigsaw-tpu-bert-with-huggingface-and-keras)\n\n\nThanks for viewing, drop your suggestions down in the comments below. \ud83d\ude42","e2be5057":"# \ud83d\udd04 Kfold Training"}}