{"cell_type":{"7727335d":"code","afa90686":"code","3336a263":"code","b848d946":"code","684a41b8":"code","8843d43d":"code","6b7e4de5":"code","387f45db":"code","ecaf5a2d":"code","ec7bf460":"code","9b584d34":"code","d3ca7fe9":"code","f7e89f2f":"code","14e50d5c":"code","ebade2c9":"code","ec31e688":"code","41a749dd":"code","16f2a771":"code","d6d6b1ed":"code","e20a0b2c":"code","04bec49c":"code","d4959d5a":"code","aa4c20cf":"code","be83fbbc":"code","fe6cddb4":"code","356bec40":"code","78d83b71":"code","d2969546":"code","bd438064":"code","ed7a6d2a":"code","3d4dcd71":"code","6d2f83f3":"code","0d87abca":"code","728356b8":"code","a016fc78":"code","afadb697":"code","6fce8949":"code","f0313cdb":"code","5cde3c96":"code","5c0722f8":"code","3a635606":"code","28e4afb9":"code","a53a66e9":"code","5d9f871f":"code","64d6e46f":"code","d5fee357":"code","71e391b1":"code","1e66166c":"code","7a7eeb9c":"code","d02d6d64":"code","bc8f6d8f":"code","d3e2cb21":"code","8c538fa5":"code","f69da0f4":"code","91147ff6":"code","19d06178":"code","4e1883c5":"code","8bf696c7":"code","a0ba6a07":"markdown","4bb85dce":"markdown","4896b0fc":"markdown","cd3e50f7":"markdown","c5c5b2a8":"markdown","2cd6db56":"markdown","2208ac0f":"markdown","deffce62":"markdown","18601343":"markdown","3ef61f89":"markdown","a74aebc3":"markdown","af916bff":"markdown","71e985c4":"markdown","745aad77":"markdown","1c76fd0e":"markdown","4685a243":"markdown","86189fa3":"markdown","34321b7f":"markdown","5854e4a5":"markdown","de343197":"markdown","27f1950f":"markdown","eb2e4418":"markdown","93caf13b":"markdown","2a422e74":"markdown","bca8611e":"markdown","61d42386":"markdown","acf1b8fb":"markdown","25c45f7d":"markdown","9f92830f":"markdown","498b6c4e":"markdown","53ccfd25":"markdown","b2250a66":"markdown","0b843342":"markdown","c993b2c6":"markdown","f4c4a6a2":"markdown","e5b9eafa":"markdown","1ca16e93":"markdown"},"source":{"7727335d":"import pandas as pd\nimport numpy as np\nimport seaborn as sns; sns.set(color_codes=True)","afa90686":"one = pd.read_csv('..\/input\/activity-recognition\/ActivityAccelerometer\/1.csv', names=[\"Sequence\", \"x_acceleration\", \"y_acceleration\", \"z_acceleration\",\"Labels\"])\none.drop('Sequence', axis = 1, inplace = True)\n\ntwo = pd.read_csv('..\/input\/activity-recognition\/ActivityAccelerometer\/2.csv', names=[\"Sequence\", \"x_acceleration\", \"y_acceleration\", \"z_acceleration\",\"Labels\"])\ntwo.drop('Sequence', axis = 1, inplace = True)\n\nthree = pd.read_csv('..\/input\/activity-recognition\/ActivityAccelerometer\/3.csv', names=[\"Sequence\", \"x_acceleration\", \"y_acceleration\", \"z_acceleration\",\"Labels\"])\nthree.drop('Sequence', axis = 1, inplace = True)\n\nfour = pd.read_csv('..\/input\/activity-recognition\/ActivityAccelerometer\/4.csv', names=[\"Sequence\", \"x_acceleration\", \"y_acceleration\", \"z_acceleration\",\"Labels\"])\nfour.drop('Sequence', axis = 1, inplace = True)\n\nfive = pd.read_csv('..\/input\/activity-recognition\/ActivityAccelerometer\/5.csv', names=[\"Sequence\", \"x_acceleration\", \"y_acceleration\", \"z_acceleration\",\"Labels\"])\nfive.drop('Sequence', axis = 1, inplace = True)\n\nsix = pd.read_csv('..\/input\/activity-recognition\/ActivityAccelerometer\/6.csv', names=[\"Sequence\", \"x_acceleration\", \"y_acceleration\", \"z_acceleration\",\"Labels\"])\nsix.drop('Sequence', axis = 1, inplace = True)\n\nseven = pd.read_csv('..\/input\/activity-recognition\/ActivityAccelerometer\/7.csv', names=[\"Sequence\", \"x_acceleration\", \"y_acceleration\", \"z_acceleration\",\"Labels\"])\nseven.drop('Sequence', axis = 1, inplace = True)\n\neight = pd.read_csv('..\/input\/activity-recognition\/ActivityAccelerometer\/8.csv', names=[\"Sequence\", \"x_acceleration\", \"y_acceleration\", \"z_acceleration\",\"Labels\"])\neight.drop('Sequence', axis = 1, inplace = True)\n\nnine = pd.read_csv('..\/input\/activity-recognition\/ActivityAccelerometer\/9.csv', names=[\"Sequence\", \"x_acceleration\", \"y_acceleration\", \"z_acceleration\",\"Labels\"])\nnine.drop('Sequence', axis = 1, inplace = True)\n\nten = pd.read_csv('..\/input\/activity-recognition\/ActivityAccelerometer\/10.csv', names=[\"Sequence\", \"x_acceleration\", \"y_acceleration\", \"z_acceleration\",\"Labels\"])\nten.drop('Sequence', axis = 1, inplace = True)\n\neleven = pd.read_csv('..\/input\/activity-recognition\/ActivityAccelerometer\/11.csv', names=[\"Sequence\", \"x_acceleration\", \"y_acceleration\", \"z_acceleration\",\"Labels\"])\neleven.drop('Sequence', axis = 1, inplace = True)\n\ntwelve = pd.read_csv('..\/input\/activity-recognition\/ActivityAccelerometer\/12.csv', names=[\"Sequence\", \"x_acceleration\", \"y_acceleration\", \"z_acceleration\",\"Labels\"])\ntwelve.drop('Sequence', axis = 1, inplace = True)\n\nthirteen = pd.read_csv('..\/input\/activity-recognition\/ActivityAccelerometer\/13.csv', names=[\"Sequence\", \"x_acceleration\", \"y_acceleration\", \"z_acceleration\",\"Labels\"])\nthirteen.drop('Sequence', axis = 1, inplace = True)\n\nfourteen = pd.read_csv('..\/input\/activity-recognition\/ActivityAccelerometer\/14.csv', names=[\"Sequence\", \"x_acceleration\", \"y_acceleration\", \"z_acceleration\",\"Labels\"])\nfourteen.drop('Sequence', axis = 1, inplace = True)\n\nfifteen = pd.read_csv('..\/input\/activity-recognition\/ActivityAccelerometer\/15.csv', names=[\"Sequence\", \"x_acceleration\", \"y_acceleration\", \"z_acceleration\",\"Labels\"])\nfifteen.drop('Sequence', axis = 1, inplace = True)","3336a263":"one['Person'] = 1\ntwo['Person'] = 2\nthree['Person'] = 3\nfour['Person'] = 4\nfive['Person'] = 5\nsix['Person'] = 6\nseven['Person'] = 7\neight['Person'] = 8\nnine['Person'] = 9\nten['Person'] = 10\neleven['Person'] = 11\ntwelve['Person'] = 12\nthirteen['Person'] = 13\nfourteen['Person'] = 14\nfifteen['Person'] = 15\n","b848d946":"frames = [one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve ,thirteen,fourteen , fifteen]\n\ndf= pd.concat(frames)\n\ndf.shape","684a41b8":"df_rnd = df.sample(n=100000, random_state=6758)","8843d43d":"df_rnd.dtypes","6b7e4de5":"df_rnd.isna().sum()","387f45db":"df_rnd = df_rnd[df_rnd.Labels != 0] #Removing rows which had label zero ","ecaf5a2d":"df_rnd['Labels'].value_counts()","ec7bf460":"df_rnd.describe()","9b584d34":"means = pd.DataFrame(columns = ['x_acceleration_mean','y_acceleration_mean','z_acceleration_mean','Labels'])\ngrouped = df.groupby(df.Labels)\n\n\nlst = []\nlst2 = []\nlst3 = []\nlst4 = []\nfor val in range(1,8):\n    label = grouped.get_group(val)\n    lst.append(label['x_acceleration'].mean())\n    lst2.append(label['y_acceleration'].mean())\n    lst3.append(label['z_acceleration'].mean())\n    lst4.append(val)\n\nmeans['x_acceleration_mean'] = lst\nmeans['y_acceleration_mean'] = lst2\nmeans['z_acceleration_mean'] = lst3\nmeans['Labels'] = lst4\n\nmeans","d3ca7fe9":"import matplotlib.pyplot as plt\n%matplotlib inline \n%config InlineBackend.figure_format = 'retina'\nplt.style.use(\"ggplot\")","f7e89f2f":"df_rnd.boxplot(column=['x_acceleration','y_acceleration','z_acceleration']);","14e50d5c":"from scipy import stats\ntemp = df[(np.abs(stats.zscore(df)) < 3).all(axis=1)]","ebade2c9":"temp.boxplot(column=['x_acceleration','y_acceleration','z_acceleration']);","ec31e688":"means = pd.DataFrame(columns = ['x_acceleration_mean','y_acceleration_mean','z_acceleration_mean','Labels'])\ngrouped = temp.groupby(temp.Labels)\n\n\nlst = []\nlst2 = []\nlst3 = []\nlst4 = []\nfor val in range(1,8):\n    label = grouped.get_group(val)\n    lst.append(label['x_acceleration'].mean())\n    lst2.append(label['y_acceleration'].mean())\n    lst3.append(label['z_acceleration'].mean())\n    lst4.append(val)\n\nmeans['x_acceleration_mean'] = lst\nmeans['y_acceleration_mean'] = lst2\nmeans['z_acceleration_mean'] = lst3\nmeans['Labels'] = lst4\n\nmeans","41a749dd":"df_rnd['Labels'].value_counts().plot(kind='pie',autopct='%.2f')\nplt.figlegend()\nplt.show()","16f2a771":"df_rnd.groupby('Labels')['x_acceleration'].plot.kde();\nplt.autoscale(enable=True, axis= 'both',tight=None)\nplt.figlegend();","d6d6b1ed":"sns.kdeplot(df_rnd['x_acceleration'],shade = True, color = 'blue')","e20a0b2c":"df_rnd.groupby('Labels')['y_acceleration'].plot.kde();\nplt.figlegend();","04bec49c":"sns.kdeplot(df_rnd['y_acceleration'],shade = True, color = 'red')","d4959d5a":"df_rnd.groupby('Labels')['z_acceleration'].plot.kde();\nplt.autoscale(enable=True, axis= 'both',tight=None)\nplt.figlegend();","aa4c20cf":"sns.kdeplot(df_rnd['z_acceleration'],shade = True, color = 'green')","be83fbbc":"from pandas.plotting import scatter_matrix\ncolors_palette = {1: 'red', 2:'green',3:'blue',4:'orange',5:'yellow',6:'cyan',7:'magenta'}\ncolors = [colors_palette[c] for c in df_rnd['Labels']]\nscatter_matrix(df_rnd, alpha = 0.2, figsize = (16,16), diagonal = 'hist',c=colors)\nplt.show()","fe6cddb4":"df_rnd.corr(method='pearson')","356bec40":"sns.heatmap(df_rnd.corr(method='pearson'))","78d83b71":"df['Labels'].value_counts()","d2969546":"Data = df_rnd.drop(columns = ['Person','Labels']).values\ntarget = df_rnd['Labels'].values","bd438064":"from sklearn import preprocessing\n\ntarget =  preprocessing.LabelEncoder().fit_transform(target)","ed7a6d2a":"np.unique(target, return_counts=True)","3d4dcd71":"#Using standard scaling techniques\n\nData = preprocessing.StandardScaler().fit_transform(Data)","6d2f83f3":"from sklearn.model_selection import train_test_split\n\nD_train, D_test, t_train, t_test = train_test_split(Data,\n                                                   target,\n                                                   test_size = 0.3,\n                                                   random_state = 6758)","0d87abca":"from sklearn.neighbors import KNeighborsClassifier\n\nknn_classifier = KNeighborsClassifier()\nknn_classifier.fit(D_train, t_train)\nkNN = knn_classifier.score(D_test, t_test)\nprint('KNN Classifier : ', kNN)","728356b8":"from sklearn.tree import DecisionTreeClassifier\n\ndt_classifier = DecisionTreeClassifier(criterion='entropy', max_depth=4, random_state= 6758)\ndt_classifier.fit(D_train, t_train)\ndT = dt_classifier.score(D_test, t_test)\nprint('DecisionTreeClassifier : ', dT)","a016fc78":"from sklearn.model_selection import RepeatedStratifiedKFold\n\ncv_method = RepeatedStratifiedKFold(n_splits=5, \n                                    n_repeats=3, \n                                    random_state= 6758)","afadb697":"import numpy as np\nparams_KNN = {'n_neighbors': [1,3, 5, 7, 11, 15, 20, 25], \n              'p': [1, 2, 5]}","6fce8949":"from sklearn.model_selection import GridSearchCV\n\ngs_KNN = GridSearchCV(estimator=KNeighborsClassifier(), \n                      param_grid=params_KNN, \n                      cv=cv_method,\n                      verbose=1,  # verbose: the higher, the more messages\n                      scoring='accuracy',\n                      n_jobs= -2,\n                      return_train_score=True)","f0313cdb":"gs_KNN.fit(Data, target);","5cde3c96":"gs_KNN.best_params_","5c0722f8":"gs_KNN.best_score_","3a635606":"gs_KNN.cv_results_['mean_test_score']","28e4afb9":"import pandas as pd\n\nresults_KNN = pd.DataFrame(gs_KNN.cv_results_['params'])","a53a66e9":"results_KNN['test_score'] = gs_KNN.cv_results_['mean_test_score']","5d9f871f":"results_KNN['metric'] = results_KNN['p'].replace([1,2,5], [\"Manhattan\", \"Euclidean\", \"Minkowski\"])\nresults_KNN","64d6e46f":"import altair as alt\nalt.renderers.enable('html')\n\nalt.Chart(results_KNN, \n          title='KNN Performance Comparison'\n         ).mark_line(point=True).encode(\n    alt.X('n_neighbors', title='Number of Neighbors'),\n    alt.Y('test_score', title='Mean CV Score', scale=alt.Scale(zero=False)),\n    color='metric'\n).interactive()","d5fee357":"from sklearn.tree import DecisionTreeClassifier\n\ndf_classifier = DecisionTreeClassifier(random_state=6758)\n\nparams_DT = {'criterion': ['gini', 'entropy'],\n             'max_depth': [5, 6, 7, 8, 10 , 12, 15, 17],\n             'min_samples_split': [2, 3]}\n\ngs_DT = GridSearchCV(estimator=df_classifier, \n                     param_grid=params_DT, \n                     cv=cv_method,\n                     verbose=1,\n                     n_jobs=-2,\n                     scoring='accuracy')\n\ngs_DT.fit(Data, target);","71e391b1":"gs_DT.best_params_","1e66166c":"gs_DT.best_score_","7a7eeb9c":"results_DT = pd.DataFrame(gs_DT.cv_results_['params'])\nresults_DT['test_score'] = gs_DT.cv_results_['mean_test_score']\nresults_DT.columns","d02d6d64":"alt.Chart(results_DT, \n          title='DT Performance Comparison'\n         ).mark_line(point=True).encode(\n    alt.X('max_depth', title='Maximum Depth'),\n    alt.Y('test_score', title='Mean CV Score', aggregate='average', scale=alt.Scale(zero=False)),\n    color='criterion'\n).interactive()","bc8f6d8f":"t_pred_knn = gs_KNN.predict(D_test)","d3e2cb21":"from sklearn import metrics\nprint('KNN Predicted accuracy score: ',metrics.accuracy_score(t_test, t_pred_knn))","8c538fa5":"t_pred_dt = gs_DT.predict(D_test)","f69da0f4":"print('DecisionTree accuracy score : ',metrics.accuracy_score(t_test,t_pred_dt))","91147ff6":"# Classification report for KNN \n\nprint(metrics.classification_report(t_test, t_pred_knn))","19d06178":"print(metrics.confusion_matrix(t_test, t_pred_knn))","4e1883c5":"#Classification report for Decision tree\n\nprint(metrics.classification_report(t_test, t_pred_dt, labels=np.unique(t_pred_dt)))","8bf696c7":"print(metrics.confusion_matrix(t_test, t_pred_dt))","a0ba6a07":"Now let us see what happens if we remove the outliers. Doing this and seeing if there was any change to our means table. ","4bb85dce":"## Retrieving Data and Data Preparation","4896b0fc":"### Scaling Features","cd3e50f7":"Taking each of the labels and looking at the columns ","c5c5b2a8":"As we can see, as the maximum depth increases the mean CV score also increases. An accuracy of 66.7% after hyper parameter tuning is good. ","2cd6db56":"### Predicting ","2208ac0f":"### Exploration between attributes","deffce62":"It looks like there was no point removing the outliers as the average of each of the acceleration's are still the same. Thus we are going to proceed with the df_rnd where the outliers were not removed","18601343":"<br>From the above means, it looks like y_acceleration is always more irrespective of the activity done. Looks like activity number 2 (Standing up, Walking and Going up\\down the stairs is the least for all parameters). The label looks very inconclusive considering it has all the activities in it. \n<br>The highest value for z_acceleration is when the person is going up\/down the stairs (label 5) \n<br>The hypothesis from y_acceleration mean is very inconclusive\n<br> x_acceleration is the highest when the person in walking and going up\/down the stairs (label 5,6) ","3ef61f89":"Scaling each of these descriptive feature is done by :  $\\mbox{scaled_value} = \\frac{\\mbox{value - mean}}{\\mbox{std. dev.}}$","a74aebc3":"The sampled dataset which is present right now is a numerical descriptive feature. So thereby scaling descriptive features, we can then train them on a classification model. \n\n","af916bff":"The F1 score for DT in label 4, 5 has a lower F1 score. Whilst the model is performing fairly well when looking at the other labels. ","71e985c4":"### Data Preparation","745aad77":"###  Data Preparation","1c76fd0e":"Comparing both these graphs, we can see that x_acceleration is the between 1600 - 2300. This is true for all the labels. However as we can see the pink line represents label 7 which is talking while standing. This has the highest probability for higher values. ","4685a243":"As we can see, the majority of the values are between 1800 to 2200. The highest spike of accelerometer is noticed when standing up.","86189fa3":"## Data Exploration","34321b7f":"### Hyper-Parameter Tuning ","5854e4a5":"## Data Modeling","de343197":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Retrieving-Data-and-Data-Preparation\" data-toc-modified-id=\"Retrieving-Data-and-Data-Preparation-1\">Retrieving Data and Data Preparation<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Data-Preparation\" data-toc-modified-id=\"Data-Preparation-1.1\">Data Preparation<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Data-Exploration\" data-toc-modified-id=\"Data-Exploration-2\">Data Exploration<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Exploration-between-attributes\" data-toc-modified-id=\"Exploration-between-attributes-2.1\">Exploration between attributes<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Data-Modeling\" data-toc-modified-id=\"Data-Modeling-3\">Data Modeling<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Data-Preparation\" data-toc-modified-id=\"Data-Preparation-3.1\">Data Preparation<\/a><\/span><\/li><li><span><a href=\"#Scaling-Features\" data-toc-modified-id=\"Scaling-Features-3.2\">Scaling Features<\/a><\/span><\/li><li><span><a href=\"#Fitting-a-classifier\" data-toc-modified-id=\"Fitting-a-classifier-3.3\">Fitting a classifier<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#K-Nearest-Neighbor-Classifier\" data-toc-modified-id=\"K-Nearest-Neighbor-Classifier-3.3.1\">K-Nearest-Neighbor Classifier<\/a><\/span><\/li><li><span><a href=\"#Decision-Tree-Classifier\" data-toc-modified-id=\"Decision-Tree-Classifier-3.3.2\">Decision Tree Classifier<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Hyper-Parameter-Tuning\" data-toc-modified-id=\"Hyper-Parameter-Tuning-3.4\">Hyper-Parameter Tuning<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#K-Nearest-Neighbor-Classifier\" data-toc-modified-id=\"K-Nearest-Neighbor-Classifier-3.4.1\">K-Nearest-Neighbor Classifier<\/a><\/span><\/li><li><span><a href=\"#Decision-Tree-Classifier\" data-toc-modified-id=\"Decision-Tree-Classifier-3.4.2\">Decision Tree Classifier<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Predicting\" data-toc-modified-id=\"Predicting-3.5\">Predicting<\/a><\/span><\/li><\/ul><\/li><\/ul><\/div>","27f1950f":"We can see that the accuracy is very less for both the cases without feeding in any parameters. Let us fine tune our parameters to see if we can fair better. ","eb2e4418":"Feeding in 1-7 nearest neighbors and giving Euclidean, Manhattan and Minkowski distance as p values","93caf13b":"### Fitting a classifier","2a422e74":"Since we could conclude that the accuracy for KNN is much higher than Decision Tree, predicting the values with KNN is much more efficient. But however we will predict using both to compare both the scores. ","bca8611e":"We can see from the heat map that there is close to 0.5 correlation on an average between the accelerations and close to 0.1 correlation between the labels and the acceleration. ","61d42386":"     1) We can notice from the scatter plots of each of the acceleration and between each other, there is not much                  correlation neither positively nor negatively\n     2) Comparing the labels and the acceleration : we can notice sitting in the computer (label 1) has the most                    acceleration followed by talking while standing (label 7)\n","acf1b8fb":"#### K-Nearest-Neighbor Classifier","25c45f7d":"#### Decision Tree Classifier","9f92830f":"We can see that label encoding has been done properly where 1 from  df_rnd is 0 in target, 2 from df_rnd is 1 and so on  ","498b6c4e":"#### K-Nearest-Neighbor Classifier","53ccfd25":"#### Decision Tree Classifier","b2250a66":"Looking at the F1-scores, we can see the model is not very good when it comes to label 1,4,5. While it is doing extremely well for the other labels","0b843342":"<h1><center>Activity Recognition from Single Chest-Mounted Accelerometer Data Set<\/center><h1>","c993b2c6":"Populating a dataframe with means of each of the label. Let us call it means","f4c4a6a2":"We can see that most of the y_acceleration values lie from 2300 - 2700. Label 1 and 7 have the highest spike in the graph","e5b9eafa":"Here we can see that most of the values given are for \n<br>label 1 - Working on computer\n<br>label 7 - Talking while standing \n<br>label 4 - Walking ","1ca16e93":"We can see that the mean CV score always increases with respect to number of neighbors. This can lead to overfitting which should be avoided. "}}