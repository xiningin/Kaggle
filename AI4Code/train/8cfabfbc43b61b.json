{"cell_type":{"cf0d0eb6":"code","3d6c7a87":"code","f5cb638c":"code","2785d4fa":"code","95f1c14e":"code","deaf59ce":"code","cfda42cc":"markdown","1fa4c510":"markdown","be5022c1":"markdown","8377db59":"markdown","a948a5a9":"markdown"},"source":{"cf0d0eb6":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n## line 1066 has misterious values -> ignored those values\nexpression=pd.read_table(\"\/kaggle\/input\/gene-expression-of-schizophrenia-and-bipolar\/Adjusted_expression_values.txt\",index_col=0,usecols=list(range(0,547))).T\nexpression.index=expression.index.astype('int64')\nexpression.shape\n\nindividuals=pd.read_table(\"\/kaggle\/input\/gene-expression-of-schizophrenia-and-bipolar\/E-MTAB-8018.sdrf.txt\",index_col=0)\n\n#examine what's inside individuals\n'''\nprint(individuals.shape)\n\n#unique values for each column\nfor col in individuals:\n    no_of_unique_values=len(np.unique(individuals[col]))\n    \n    if no_of_unique_values>=10: #if there are too many unique values I will just print out the summary\n        print(individuals[col].name,': ','no. of unique values= ',no_of_unique_values)\n\nprint('===========================================================================')\n        \nfor col in individuals:\n    no_of_unique_values=len(np.unique(individuals[col]))\n    \n    if no_of_unique_values<10: #if there are too many unique values I will just print out the summary        \n        print(individuals[col].value_counts())\n'''\n\ndf=individuals.join(expression,how='inner')\n\n#Columns in addition to gene expression I think should remain in the model: sex (column 2, 0 indexed), age (column 3, 0 indexed)\ncol_in_model=[2,3]\ncol_in_model.extend(list(range(28,47311)))\nX=df.iloc[:,col_in_model]\nX=pd.get_dummies(X) #get dummy variables for (male,female) => after this gender becomes (0,1) at the last column\n\n#response variable (target)\ny=np.array(df['Characteristics[disease]'])\ny=np.where(y!='normal',1,0)\n\n# Show how many NAs are in the data (X)\n#no_of_na=np.array(X.isna().sum())\n#print('No. of NAs in the data: ',sum(no_of_na[no_of_na!=0])) #there's only 1 -> impute by means of that feature column\n\n#impute NA by mean of each feature \nfrom sklearn.impute import SimpleImputer\n\ndef impute_by_mean(data):\n    imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean') \n    imp_mean.fit(data)\n    return(pd.DataFrame(imp_mean.transform(data)))\n\nX=impute_by_mean(X) \n# Show how many NAs are in the data (X) after imputation\n#no_of_na=np.array(X.isna().sum())\n#print('No. of NAs after imputation: ',sum(no_of_na[no_of_na!=0]))","3d6c7a87":"#feature selection\ntop_features=100 #no.of features I want to use\n\n#I used these 2 scripts to get the p-values using t test\n#https:\/\/github.com\/peterwu19881230\/test_repo\/blob\/master\/generate_X_and_binaryLabel.py\n#https:\/\/github.com\/peterwu19881230\/test_repo\/blob\/master\/find_diff_expressed_genes.R\n    \n\n#load p-values, rank for feature selection\npvals=np.array(pd.read_csv('https:\/\/raw.githubusercontent.com\/peterwu19881230\/test_repo\/master\/pvals.csv')).ravel()\n\ndef rank_array(array):\n    temp = array.argsort()\n    ranks = np.empty_like(temp)\n    ranks[temp] = np.arange(len(array))\n    return(ranks)\n\npval_ranks=rank_array(pvals)\n\nselected=np.where(pval_ranks<top_features,True,False) #note: ranking starts from 0\n\nage_sex=X.iloc[:,[0,-1]]\ngene_expression=X.iloc[:,1:-1]\nselected_gene_expression=gene_expression.iloc[:,selected]\n\nX=pd.concat([age_sex,selected_gene_expression],axis=1)","f5cb638c":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n\ndef my_inner_cv(X,y,model,cv,param_grid,test_size,random_state,train_test_boostrap=1):\n  \n  results=[]\n  for b in range(train_test_boostrap):        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, random_state=random_state,stratify=y)\n        grid_search = GridSearchCV(model, param_grid=param_grid,cv=cv,iid=False) \n        grid_search.fit(X_train, y_train)\n\n        accuracy=accuracy_score(y_test,grid_search.best_estimator_.predict(X_test))\n        result=[grid_search.best_estimator_,grid_search.best_params_,accuracy]\n        results.append(result)\n        \n        random_state=random_state+1\n        \n  return(results)\n\n\ndef my_logistic_regression(X,y,cv=5,param_grid={},test_size=0.2,random_state=101,train_test_boostrap=1):\n  model=LogisticRegression(solver='newton-cg',multi_class='ovr',penalty='l2')\n  result=my_inner_cv(X,y,model,cv,param_grid,test_size,random_state,train_test_boostrap)\n  return(result)\n\ndef my_lasso(X,y,cv=5,param_grid={},test_size=0.2,random_state=101,train_test_boostrap=1):\n  model=LogisticRegression(multi_class='ovr',penalty='l1',solver='liblinear',max_iter=1000)\n  result=my_inner_cv(X,y,model,cv,param_grid,test_size,random_state,train_test_boostrap)\n  return(result)","2785d4fa":"#set some parameters\ncv=3\ntest_size=0.2\nrandom_state=101\ntrain_test_boostrap=10","95f1c14e":"#logistic regression\nresult=my_logistic_regression(X.iloc[:,0:2],y,cv=cv,test_size=test_size,random_state=random_state,train_test_boostrap=10)\n\naccuracies=[result[2] for result in result] \n##->I have verified that each bootstrap train\/test sample are different. However, they seem to get the same accuracy\n\nprint('Bootstrapped accuracies= ',accuracies) \nprint('Average accuracy=',np.mean(accuracies))","deaf59ce":"# logtistic regression with L1 penalty (Lasso)\nC=[0.0001,0.001,0.01,0.1,1,10,100,1000,10000]\nresult=my_lasso(X,y,cv=cv,test_size=test_size,param_grid={'C':C},random_state=random_state,train_test_boostrap=10)\n\nhyper_params=[result[1] for result in result]\naccuracies=[result[2] for result in result]\n##->I have verified that each bootstrap train\/test sample are different. However, they seem to get the same accuracy\n\nprint('Bootstrapped best hyper parameters: ',hyper_params)\nprint('Bootstrapped accuracies= ',accuracies)\nprint('Average accuracy=',np.mean(accuracies))","cfda42cc":"## Conclusion:\n\nIn classifying diseased (either bipolar disorder, schizophrenia or schizoaffective disorder) \/non-diseased people. 100% accuracy was achieved by using Lasso on top 100 differentially expressed genes","1fa4c510":"## Prepare data","be5022c1":"## Introduction\n\nI have downloaded a RNA expression dataset from immortalized lymphocytes of schizophrenia and bipolar disorders patients and that of their non-affected relatives. From the analysis below, I have succesfully classified diseased and non-diseased people based on the top differentially expressed genes with an average accuray of 100% using Lasso.\n\n**Reference paper**: https:\/\/www.nature.com\/articles\/s41431-019-0526-y\n\n**Data repository**: https:\/\/www.ebi.ac.uk\/arrayexpress\/experiments\/E-MTAB-8018\/","8377db59":"## Train machine learning models","a948a5a9":"# Classification of schizophrenia, bipolar disorder, normal people using Lasso on RNA expression data"}}