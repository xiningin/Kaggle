{"cell_type":{"38785a16":"code","4b834f1d":"code","850e2a51":"code","1ac9e86c":"code","8c991929":"code","ad8b08ae":"code","dc465859":"code","49c01157":"code","2410a8a2":"code","e6f6f3fc":"code","708bc188":"code","aba6ae81":"code","a0f17f27":"code","51663f80":"code","aad71314":"code","a290db1a":"code","69ef81ff":"code","af53ce20":"code","894cb9ce":"code","d5f68d9f":"code","16e6672c":"code","e904184e":"code","c6240846":"code","36823ebf":"code","c7cdb4dd":"code","7b01ed09":"code","109f5c92":"code","ce749be0":"code","0691c232":"code","05e6b896":"code","34f85cd8":"code","a3df2aab":"code","e0b29543":"code","fc99f848":"code","bed21e34":"code","79f664b6":"code","cf0204f1":"code","7ffab471":"code","33b3c706":"code","920bc7f8":"markdown","23a7b458":"markdown","0ea66cd9":"markdown","27493643":"markdown","e179dad9":"markdown","5a4f768f":"markdown","06a749ec":"markdown","6dba1992":"markdown","3c5a1120":"markdown","2f56bddd":"markdown"},"source":{"38785a16":"#!pip install -U gensim\n#import nltk\n#nltk.download()","4b834f1d":"\nimport warnings\nwarnings.filterwarnings('ignore')\n","850e2a51":"# import packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport string\nimport sqlite3\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import SnowballStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,confusion_matrix,classification_report,f1_score\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn import cross_validation\nfrom sklearn.cross_validation import cross_val_score\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport gensim\nfrom sklearn.model_selection import train_test_split","1ac9e86c":"# creating sql connection string\ncon = sqlite3.connect('..\/input\/database.sqlite')","8c991929":"#Positive Review - Rating above 3\n#Negative Review - Rating below 3\n#Ignoring Reviews with 3 Rating\n\nfiltered_data = pd.read_sql_query('SELECT * from Reviews WHERE Score != 3',con)","ad8b08ae":"# mapping ratings above 3 as Positive and below 3 as Negative\n\nactual_scores = filtered_data['Score']\npositiveNegative = actual_scores.map(lambda x: 'Positive' if x>3 else 'Negative')\nfiltered_data['Score'] = positiveNegative","dc465859":"final = filtered_data.drop_duplicates(subset= { 'UserId', 'ProfileName', 'Time',  'Text'})","49c01157":"print('Rows dropped : ',filtered_data.size - final.size)\nprint('Percentage Data remaining after dropping duplicates :',(((final.size * 1.0)\/(filtered_data.size * 1.0) * 100.0)))","2410a8a2":"# Dropping rows where HelpfulnessNumerator < HelpfulnessDenominator\nfinal = final[final.HelpfulnessDenominator >= final.HelpfulnessNumerator]","e6f6f3fc":"print('Number of Rows remaining in the Dataset: ',final.size)","708bc188":"# Checking the number of positive and negative reviews\nfinal['Score'].value_counts()","aba6ae81":"# Data Sampling\nfinal = final.iloc[:50000,:]","a0f17f27":"# Checking the number of positive and negative reviews\n\nClass_Count  = final['Score'].value_counts()\n\nplt.figure()\nflatui = [\"#15ff00\", \"#ff0033\"]\nsns.set_palette(flatui)\nsns.barplot(Class_Count.index, Class_Count.values, alpha=0.8 )\nplt.title('Positive Class Count vs Negative Class Count')\nplt.ylabel('Count', fontsize=12)\nplt.xlabel('Class', fontsize=12)\nplt.show()\n\nprint(final['Score'].value_counts())","51663f80":"# Sorting values according to Time for Time Based Slicing\nfinal = final.sort_values('Time',kind = 'quicksort')","aad71314":"# Function to Remove HTML Tags\ndef cleanhtml(sentence):\n    cleaner = re.compile('<.*?>')\n    cleantext = re.sub(cleaner,\"\",sentence)\n    return cleantext","a290db1a":"# Function to clean punctuations and special characters\n\ndef cleanpunct(sentence):\n    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n    cleaned = re.sub(r'[.|,|)|(|\\|\/]',r' ',cleaned)\n    return  cleaned\n","69ef81ff":"# Initialize Stop words and PorterStemmer and Lemmetizer\nstop = set(stopwords.words('english'))\nsno = SnowballStemmer('english')\n\n\n#print(stop)\n#print('*' * 100)\n#print(sno.stem('tasty'))","af53ce20":"# Cleaning HTML and non-Alphanumeric characters from the review text\ni=0\nstr1=' '\nfinal_string=[]\nall_positive_words=[] # store words from +ve reviews here\nall_negative_words=[] # store words from -ve reviews here.\ns=''\nfor sent in final['Text'].values:\n    filtered_sentence=[]\n    #print(sent);\n    sent=cleanhtml(sent) # remove HTMl tags\n    for w in sent.split():\n        for cleaned_words in cleanpunct(w).split():\n            if((cleaned_words.isalpha()) & (len(cleaned_words)>2)):    \n                if(cleaned_words.lower() not in stop):\n                    s=(sno.stem(cleaned_words.lower())).encode('utf8')\n                    filtered_sentence.append(s)\n                    if (final['Score'].values)[i] == 'Positive': \n                        all_positive_words.append(s) #list of all words used to describe positive reviews\n                    if(final['Score'].values)[i] == 'Negative':\n                        all_negative_words.append(s) #list of all words used to describe negative reviews reviews\n                else:\n                    continue\n            else:\n                continue \n    #print(filtered_sentence)\n    str1 = b\" \".join(filtered_sentence) #final string of cleaned words\n    #print(\"***********************************************************************\")\n    \n    final_string.append(str1)\n    i+=1","894cb9ce":"final['CleanedText']=final_string\nfinal.head(5)","d5f68d9f":"#Dictionary for storing Metrics\nFinal_Metrics =pd.DataFrame()","16e6672c":"\n# Function for KNN\ndef runKNN(X_Train,X_Test,y_train,y_test,VectorizationType,algo):\n    global Final_Metrics\n    cv_scores = []\n    k_value = []\n    Train_Scores = []\n    Test_Scores = []\n    algorithm = ['kd_tree','brute']\n    Cnf_Mtx = []\n    Algo_Temp = 'Dummy'\n    \n        #print(algo)\n        # kd_tree cannot consume Sparse Matrix. Converting Sparse Matrix to Dense using Truncated SVD.\n    if algo == 'kd_tree':\n      Algo_Temp = algo\n      svd = TruncatedSVD()\n      X_Train = svd.fit_transform(X_Train)\n      X_Test = svd.fit_transform(X_Test)\n           \n            \n    print('*' * 300)        \n    j=0\n    for i in range(1,30,2):\n    # instantiate learning model (k = 30)\n      knn = KNeighborsClassifier(n_neighbors=i,algorithm = algo)\n      scores = cross_val_score(knn, X_Train, y_train, cv=10, scoring='accuracy')\n      cv_scores.append(scores.mean())\n      k_value.append(i)\n      \n      print('For K = ', i,'Accuracy Score = ', cv_scores[j])\n      j+=1\n       \n    plt.plot(k_value,cv_scores,'-o')\n    plt.xlabel('K-Value')\n    plt.ylabel('CV-Scores')\n    plt.title('K-Value vs CV-Scores')\n    print('*' * 300)\n               \n        #print(cv_scores)\n        #print(max(cv_scores))\n    k_optimum = k_value[cv_scores.index(max(cv_scores))]\n\n    knn = KNeighborsClassifier(n_neighbors=k_optimum,algorithm = algo)\n        # fitting the model on crossvalidation train\n    knn.fit(X_Train, y_train)\n\n        # predict the response on the crossvalidation train\n    pred = knn.predict(X_Test)\n    knn.fit(X_Train, y_train).score(X_Train, y_train)\n    Train_Scores.append(knn.score(X_Train, y_train))\n    Test_Scores.append(knn.score(X_Test, y_test))\n      \n    Temp_List = [algo,VectorizationType,k_optimum,knn.score(X_Train, y_train)*100,knn.score(X_Test, y_test)*100]\n        #print(Temp_List)\n    Final_Metrics = Final_Metrics.append({'Algorithm': algo,'Vectorization':VectorizationType,'HyperParameter':k_optimum,\n                                              'Training Accuracy Score': knn.score(X_Train, y_train)*100,\n                                              'Testing Accuracy Score':knn.score(X_Test, y_test)*100},\n                                            ignore_index=True)\n\n        # evaluate CV accuracy\n        #acc = accuracy_score(y_cv_input, pred, normalize=True) * float(100)\n        \n    print('\\nDetails for ',VectorizationType,'Vectorization:')\n        \n      #print('Accuracy for',algo,' algorithm with alpha =',alpha_optimum,' is ' ,np.round((accuracy_score(y_cv_input, pred)*100),decimals = 2))\n    print('Accuracy for',algo,' algorithm with K =',k_optimum,' is ' ,np.round((accuracy_score(y_test, pred)*100),decimals = 2))\n    print('F1 score for',algo,' algorithm with K =',k_optimum,' is ' , np.round((f1_score(y_test, pred,average= 'macro')*100),decimals = 2))\n    print('Recall for',algo,' agorithm with K =',k_optimum,' is ' , np.round((recall_score(y_test, pred,average= 'macro')*100),decimals = 2))\n    print('Precision for',algo,' algorithm with K =',k_optimum,' is ' , np.round((precision_score(y_test, pred,average= 'macro')*100),decimals = 2))\n    print ('\\n Clasification report for',algo,' algorithm with K =',k_optimum,' is \\n ' , classification_report(y_test,pred))\n        #print ('\\n Confusion matrix for',algo,' algorithm with K =',k_optimum,' is \\n' ,confusion_matrix(y_test, pred))\n    Cnf_Mtx = [pred]\n    \n    plt.figure()\n    confusion_matrix_Plot = confusion_matrix(y_test,pred)\n    heatmap = sns.heatmap(confusion_matrix_Plot, annot=True, fmt=\"d\")\n    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right')\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    \n    print('The Confusion Matrix for ',algo,' Algorithm')\n             ","e904184e":"#Splitting into Training and Testing Set, and using only Training set for Word2Vec Training\nX_Train,X_Test,y_train,y_test = train_test_split(final['CleanedText'],final['Score'])","c6240846":"# BoW Vectorization\n\nvect = CountVectorizer().fit(X_Train)\nX_Train = vect.transform(X_Train)\nX_Test = vect.transform(X_Test)","36823ebf":"runKNN(X_Train,X_Test,y_train,y_test,'Bag of Words','kd_tree')\n","c7cdb4dd":"runKNN(X_Train,X_Test,y_train,y_test,'Bag of Words','brute')","7b01ed09":"#Splitting into Training and Testing Set, and using only Training set for Word2Vec Training\nX_Train,X_Test,y_train,y_test = train_test_split(final['Text'],final['Score'])","109f5c92":"# TF-IDF weighted Word2Vec\nvect_tfidf = TfidfVectorizer(min_df = 5).fit(X_Train)\ntfidf_feat = vect_tfidf.get_feature_names() # tfidf words\/col-names\n#print(tfidf_feat)\n# final_tf_idf is the sparse matrix with row= sentence, col=word and cell_val = tfidf\n\ntfidf_sent_vectors = []; # the tfidf-w2v for each sentence\/review is stored in this list\nrow=0;\nfor sent in X_Train: # for each review\/sentence\n    sent_vec = np.zeros(50) # as word vectors are of zero length\n    weight_sum =0; # num of words with a valid vector in the sentence\/review\n    for word in sent: # for each word in a review\/sentence\n        try:\n            vec = w2v_model.wv[word]\n            # obtain the tf_idfidf of a word in a sentence\/review\n            tfidf = vect_tfidf[row, tfidf_feat.index(word)]\n            sent_vec += (vec * tf_idf)\n            weight_sum += tf_idf\n        except:\n            pass\n    \n    #print(type(sent_vec))\n    try:\n        sent_vec \/= weight_sum\n    except:\n        pass\n    \n    tfidf_sent_vectors.append(sent_vec)\n    row += 1\nX_train_Vectorised = tfidf_sent_vectors\n\n\ntfidf_sent_vectors = []; # the tfidf-w2v for each sentence\/review is stored in this list\nrow=0;\nfor sent in X_Test: # for each review\/sentence\n    sent_vec = np.zeros(50) # as word vectors are of zero length\n    weight_sum =0; # num of words with a valid vector in the sentence\/review\n    for word in sent: # for each word in a review\/sentence\n        try:\n            vec = w2v_model.wv[word]\n            # obtain the tf_idfidf of a word in a sentence\/review\n            tfidf = vect_tfidf[row, tfidf_feat.index(word)]\n            sent_vec += (vec * tf_idf)\n            weight_sum += tf_idf\n        except:\n            pass\n    \n    #print(type(sent_vec))\n    try:\n        sent_vec \/= weight_sum\n    except:\n        pass\n    \n    tfidf_sent_vectors.append(sent_vec)\n    row += 1\n\nX_test_Vectorised = tfidf_sent_vectors\n    \nX_train_Vectorised = np.nan_to_num(X_train_Vectorised)\nX_test_Vectorised = np.nan_to_num(X_test_Vectorised)\n    \n\n    ","ce749be0":"runKNN(X_train_Vectorised,X_test_Vectorised,y_train,y_test,'TF-IDF Weighted Word2Vec','kd_tree')","0691c232":"runKNN(X_train_Vectorised,X_test_Vectorised,y_train,y_test,'TF-IDF Weighted Word2Vec','brute')","05e6b896":"#Splitting into TRaining and Testing Set, and using only Training set for Word2Vec Training\nX_Train,X_Test,y_train,y_test = train_test_split(final['Text'],final['Score'])\n\n\n# Train your own Word2Vec model using your own text corpus\n\ni=0\nlist_of_sent=[]\nfor sent in X_Train.values:\n    filtered_sentence=[]\n    sent=cleanhtml(sent)\n    for w in sent.split():\n        for cleaned_words in cleanpunct(w).split():\n            if(cleaned_words.isalpha()):    \n                filtered_sentence.append(cleaned_words.lower())\n            else:\n                continue \n    list_of_sent.append(filtered_sentence)\n    ","34f85cd8":"'''print(final['Text'].values[0])\nprint(\"*****************************************************************\")\nprint(list_of_sent[0])'''","a3df2aab":"w2v_model=gensim.models.Word2Vec(list_of_sent,min_count=5,size=50, workers=4)    \nwords = list(w2v_model.wv.vocab)\n#print(len(words))","e0b29543":"# average Word2Vec\n# compute average word2vec for each review.\nsent_vectors = []; # the avg-w2v for each sentence\/review is stored in this list\nfor sent in X_Train: # for each review\/sentence\n    sent_vec = np.zeros(50) # as word vectors are of zero length\n    cnt_words =0; # num of words with a valid vector in the sentence\/review\n    for word in sent: # for each word in a review\/sentence\n        try:\n            vec = w2v_model.wv[word]\n            sent_vec += vec\n            cnt_words += 1\n        except:\n            pass\n    sent_vec \/= cnt_words\n    sent_vectors.append(sent_vec)\n\nX_train_Vectorised = sent_vectors\n\n\n\nsent_vectors = []; # the avg-w2v for each sentence\/review is stored in this list\nfor sent in X_Test: # for each review\/sentence\n    sent_vec = np.zeros(50) # as word vectors are of zero length\n    cnt_words =0; # num of words with a valid vector in the sentence\/review\n    for word in sent: # for each word in a review\/sentence\n        try:\n            vec = w2v_model.wv[word]\n            sent_vec += vec\n            cnt_words += 1\n        except:\n            pass\n    sent_vec \/= cnt_words\n    sent_vectors.append(sent_vec)\n\nX_test_Vectorised = sent_vectors\nprint(len(X_train_Vectorised))\nprint(len(X_test_Vectorised))\n\n#X_1, X_test, y_1, y_test = cross_validation.train_test_split(sent_vectors, final['Score'], random_state = 0,test_size = 0.3)\n#print('X_train first entry: \\n\\n', X_1[0])\n#print('\\n\\nX_train shape: ', X_1.shape)\n\n# split the train data set into cross validation train and cross validation test\n#X_tr, X_cv, y_tr, y_cv = cross_validation.train_test_split(X_1, y_1, test_size=0.3)\n\nnp.where(np.isnan(X_test_Vectorised))\nX_train_Vectorised = np.nan_to_num(X_train_Vectorised)\nX_test_Vectorised = np.nan_to_num(X_test_Vectorised)\n#np.nan_to_num(X_test_Vectorised)\n\n","fc99f848":"runKNN(X_train_Vectorised,X_test_Vectorised,y_train,y_test,'Average Word2Vec','kd_tree')","bed21e34":"runKNN(X_train_Vectorised,X_test_Vectorised,y_train,y_test,'Average Word2Vec','brute')","79f664b6":"#Splitting into TRaining and Testing Set, and using only Training set for Word2Vec Training\nX_Train,X_Test,y_train,y_test = train_test_split(final['Text'],final['Score'])\n\n# TF-IDF weighted Word2Vec\nvect_tfidf = TfidfVectorizer(min_df = 5).fit(X_Train)\ntfidf_feat = vect_tfidf.get_feature_names() # tfidf words\/col-names\n#print(tfidf_feat)\n# final_tf_idf is the sparse matrix with row= sentence, col=word and cell_val = tfidf\n\ntfidf_sent_vectors = []; # the tfidf-w2v for each sentence\/review is stored in this list\nrow=0;\nfor sent in X_Train: # for each review\/sentence\n    sent_vec = np.zeros(50) # as word vectors are of zero length\n    weight_sum =0; # num of words with a valid vector in the sentence\/review\n    for word in sent: # for each word in a review\/sentence\n        try:\n            vec = w2v_model.wv[word]\n            # obtain the tf_idfidf of a word in a sentence\/review\n            tfidf = vect_tfidf[row, tfidf_feat.index(word)]\n            sent_vec += (vec * tf_idf)\n            weight_sum += tf_idf\n        except:\n            pass\n    \n    #print(type(sent_vec))\n    try:\n        sent_vec \/= weight_sum\n    except:\n        pass\n    \n    tfidf_sent_vectors.append(sent_vec)\n    row += 1\nX_train_Vectorised = tfidf_sent_vectors\n\n\ntfidf_sent_vectors = []; # the tfidf-w2v for each sentence\/review is stored in this list\nrow=0;\nfor sent in X_Test: # for each review\/sentence\n    sent_vec = np.zeros(50) # as word vectors are of zero length\n    weight_sum =0; # num of words with a valid vector in the sentence\/review\n    for word in sent: # for each word in a review\/sentence\n        try:\n            vec = w2v_model.wv[word]\n            # obtain the tf_idfidf of a word in a sentence\/review\n            tfidf = vect_tfidf[row, tfidf_feat.index(word)]\n            sent_vec += (vec * tf_idf)\n            weight_sum += tf_idf\n        except:\n            pass\n    \n    #print(type(sent_vec))\n    try:\n        sent_vec \/= weight_sum\n    except:\n        pass\n    \n    tfidf_sent_vectors.append(sent_vec)\n    row += 1\n\nX_test_Vectorised = tfidf_sent_vectors\n    \nX_train_Vectorised = np.nan_to_num(X_train_Vectorised)\nX_test_Vectorised = np.nan_to_num(X_test_Vectorised)\n    \n\n    ","cf0204f1":"runKNN(X_train_Vectorised,X_test_Vectorised,y_train,y_test,'TF-IDF Weighted Word2Vec','kd_tree')","7ffab471":"runKNN(X_train_Vectorised,X_test_Vectorised,y_train,y_test,'TF-IDF Weighted Word2Vec','brute')","33b3c706":"Final_Metrics","920bc7f8":"**4.4 Using TF-IDF Weighted Word2Vec**","23a7b458":"**4.2 Using TF-IDF**","0ea66cd9":"**2. Importing Dataset from database.sqlite and ignoring reviews with Score  = 3 as they represent a neutral view**","27493643":"**4.1 Using Bag of Words**","e179dad9":"**4.3 Using Average Word2Vec**","5a4f768f":"**4.  KNN with KD Tree and Brute Force Algorithm**","06a749ec":"**Conclusion**","6dba1992":">    **KNN on Amazon Fine Foods Reviews**\n\n**Objective** - Run KNN Algorithms on Amazon Fine Foods Review Dataset using BoW,TF-IDF,Avg Word2Vec and TF-IDF weighed Word2Vec vectorization methods. Also to report the metrics for each iteration. Time based splitting to be followed.\n\n\n**KNN Algorithms to be used** - KD TREE and BRUTE\n\n**Kaggle Dataset Location** - https:\/\/www.kaggle.com\/snap\/amazon-fine-food-reviews\/data\n\n","3c5a1120":"**1. Importing required packages**","2f56bddd":"**3. Data Preprocessing**"}}