{"cell_type":{"a8a211ef":"code","2bf7b361":"code","8ea93c84":"code","ee05c94d":"code","7b357b2c":"code","7e37ddc7":"code","546292d2":"code","1461cff3":"code","00e741e4":"code","71739da8":"code","e3846153":"code","b7662520":"code","7cba8a67":"code","00ff5fc0":"code","5ff36d53":"code","96e946cb":"code","d59dc9ae":"code","4f1b2813":"code","bc8511e6":"code","7c3c07ce":"code","42b58acb":"code","a8856119":"code","1ac2b202":"code","25d9cbb3":"code","f9ff2b1e":"code","b91a875a":"code","52d846da":"code","245a0008":"code","22ddf06f":"code","d70a6403":"code","7ad90414":"markdown","63fa4ec0":"markdown","c1d35ecd":"markdown","5b304e70":"markdown","14e1cd98":"markdown","dd5dee8f":"markdown","3da11f40":"markdown","1faa201b":"markdown","d73367d2":"markdown","212cdb33":"markdown","1773ba0a":"markdown","8b1a0acd":"markdown","01c9fd3d":"markdown","de2185ae":"markdown","d4eb0e90":"markdown","ee4a2611":"markdown","cdf33385":"markdown","5d39dea6":"markdown","1b68fd33":"markdown","10c8d4da":"markdown","012ed7e3":"markdown"},"source":{"a8a211ef":"import pandas as pd\nimport numpy as np\nimport nltk\nimport re\nnltk.download('punkt')","2bf7b361":"df = pd.read_csv('..\/input\/olympic-news-dataset\/olympic_news.csv',encoding= 'unicode_escape')","8ea93c84":"df.head(10)","ee05c94d":"df.info()","7b357b2c":"#I will drop the article_title column.\n# Reason: Well I am trying to keep things simple and easy.","7e37ddc7":"df.drop(['article_title'], axis = 1, inplace=True)","546292d2":"df.head()","1461cff3":"#Now I am looking at first 3 article_text using the while loop. \n#It will help me in getting proper understanding of the article text.\ni = 0\nwhile (i < 3):   \n    i = i + 1\n    print(df['article_text'][i], sep=' ')","00e741e4":"from nltk.tokenize import sent_tokenize\nsentences = [sent_tokenize(s) for s in df['article_text']]\n\nsentences = [y for x in sentences for y in x] # flatten list\n\n# Above I have used list comprehension technique instead of conventional for loop method.\n#checking the first 3 sentences.\nsentences[:3]","71739da8":"#downloading the \"glove.6B.100d.txt\"\n!wget http:\/\/nlp.stanford.edu\/data\/glove.6B.zip\n!unzip glove*.zip","e3846153":"word_embeddings = {}\nf = open('glove.6B.100d.txt', encoding='utf-8')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    word_embeddings[word] = coefs\nf.close()","b7662520":"len(word_embeddings)","7cba8a67":"clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \",  regex=True)","00ff5fc0":"print(clean_sentences[0])\nprint(clean_sentences[1])\nprint(clean_sentences[2])","5ff36d53":"clean_sentences = [s.lower() for s in clean_sentences]","96e946cb":"nltk.download('stopwords')","d59dc9ae":"def remove_stopwords(sen):\n    sen_new = \" \".join([i for i in sen if i not in stop_words])\n    return sen_new","4f1b2813":"from nltk.corpus import stopwords\nstop_words = stopwords.words('english')","bc8511e6":"clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]","7c3c07ce":"clean_sentences[0:5]","42b58acb":"word_embeddings = {}\nf = open('glove.6B.100d.txt', encoding='utf-8')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    word_embeddings[word] = coefs\nf.close()","a8856119":"#vector representation is prerequiste for applying similarity matrix.","1ac2b202":"sentence_vectors = []\nfor i in clean_sentences:\n  if len(i) != 0:\n    v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])\/(len(i.split())+0.001)\n  else:\n    v = np.zeros((100,))\n  sentence_vectors.append(v)","25d9cbb3":"from sklearn.metrics.pairwise import cosine_similarity","f9ff2b1e":"similarity_matrix = np.zeros([len(sentences), len(sentences)])\n# The above code will help me in forming the matrix of the size of sentences. ","b91a875a":"for i in range(len(sentences)):\n  for j in range(len(sentences)):\n    if i != j:\n      similarity_matrix[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]","52d846da":"print(similarity_matrix.shape)","245a0008":"import networkx as nx\n\nnx_graph = nx.from_numpy_array(similarity_matrix)\nscores = nx.pagerank(nx_graph)","22ddf06f":"ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)","d70a6403":"# Extract top 10 sentences as the summary\nfor i in range(10):\n  print(ranked_sentences[i][1])","7ad90414":"# **1. TOKENIZATION (Spliting the whole paragraph into sentence)**","63fa4ec0":"# **5. Vector representation of sentences**","c1d35ecd":"# **8. Summarization**","5b304e70":"# Loading the library","14e1cd98":"**What are the stop words?**\n\nThese are actually the most common words in any language (like articles, prepositions, pronouns, conjunctions, etc) and does not add much information to the text. Examples of a few stop words in English are \u201cthe\u201d, \u201ca\u201d, \u201can\u201d, \u201cso\u201d, \u201cwhat\u201d.\n\n**Why we remove the stop words?**\n\nStop words are available in abundance in any human language. By removing these words, we remove the low-level information from our text in order to give more focus to the important information. In order words, we can say that the removal of such words does not show any negative consequences on the model we train for our task.\nRemoval of stop words definitely reduces the dataset size and thus reduces the training time due to the fewer number of tokens involved in the training.","dd5dee8f":"# **4. Removing stops words**","3da11f40":"# **2. WORD EMBEDDING (Then spliting the sentecnec into words.)**","1faa201b":"The nodes of this graph will represent the sentences and the edges will represent the similarity scores between the sentences. On this graph, we will apply the PageRank algorithm to arrive at the sentence rankings","d73367d2":"Doing this will help in processing the text faster.","212cdb33":"what is tokenization\n\nTokenization is a way of separating a piece of text into smaller units called tokens. \nHere, tokens can be either words, characters, or subwords. \nHence, tokenization can be broadly classified into 3 types\n1.word, 2.character, and 3.subword (n-gram characters) tokenization.","1773ba0a":"I will use cosine similarity for finding the similarity between the sentecnes. Sentences which has highest similairyt will be of more importance and we will rank them according to that and later on we will form the summarization using that. \n\n[Read more on cosine similarity.](https:\/\/www.machinelearningplus.com\/nlp\/cosine-similarity\/#:~:text=Cosine%20similarity%20is%20a%20metric,in%20a%20multi%2Ddimensional%20space.&text=The%20smaller%20the%20angle%2C%20higher%20the%20cosine%20similarity.)","8b1a0acd":"# laoding the datset","01c9fd3d":"In very simplistic terms, Word Embeddings are the texts converted into numbers and there may be different numerical representations of the same text.\nWord embeddings are a type of word representation that allows words with similar meaning to have a similar representation.","de2185ae":"Sorting the sentences on the basis of highest score","d4eb0e90":"# **6. Similarity matrix**","ee4a2611":"# **Preprocessing**","cdf33385":"# **7. Converting similarity matrix sim_mat into a graph**","5d39dea6":"In this case we are splitting the paragraph into sentences.","1b68fd33":"# **3. Remove punctuations, special characters and numbers.**","10c8d4da":"I am going to use Glove for word embedding.\nGloVe is an unsupervised learning algorithm for obtaining vector representations for words. \nTraining is performed on aggregated global word-word co-occurrence statistics from a corpus, \nand the resulting representations showcase interesting linear substructures of the word vector space\n\nRead more here https:\/\/nlp.stanford.edu\/projects\/glove\/","012ed7e3":"**converting to lower case**\n\n**Reason:**\n\nI think for your particular use-case, it would be better to convert it to lowercase because ultimately, you will need to predict the words given a certain context. You probably won't be needing to predict sentence beginnings in your use-case. Also, if a noun is predicted you can capitalize it later. However consider the other way round. (Assuming your corpus is in English) Your model might treat a word which is in the beginning of a sentence with a capital letter different from the same word which appears later in the sentence but without any capital latter. This might lead to decline in the accuracy. Whereas I think, lowering the words would be a better trade off."}}