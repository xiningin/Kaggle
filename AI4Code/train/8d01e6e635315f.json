{"cell_type":{"215d67b5":"code","76627a73":"code","0d11da54":"code","0c66173d":"code","6eb6f87a":"code","b77e3f4b":"code","b301a045":"code","135e8f58":"code","c544b87e":"code","c4cf644c":"code","9d25a3a5":"code","49fe3b96":"markdown","b1c746d5":"markdown","857c9f50":"markdown","0f7917b9":"markdown","223c489d":"markdown","52a26a80":"markdown","1715edd7":"markdown","a8874433":"markdown","8973bec5":"markdown"},"source":{"215d67b5":"!pip install ekphrasis","76627a73":"import os\nimport re\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Module\nfrom torch.utils.data import TensorDataset, Dataset, DataLoader, random_split\n\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import RegexpTokenizer\nfrom ekphrasis.classes.preprocessor import TextPreProcessor\nfrom ekphrasis.classes.tokenizer import SocialTokenizer\nfrom ekphrasis.dicts.emoticons import emoticons\nfrom keras.preprocessing.sequence import pad_sequences\n\nfrom tqdm import tqdm, trange, tqdm_notebook\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.model_selection import StratifiedKFold\n\nimport gensim\nfrom gensim.models import KeyedVectors\n\nfrom IPython.core.debugger import set_trace","0d11da54":"args = {\n    'data_dir': '..\/input\/nlp-getting-started\/',\n    'output_dir': '..\/working\/',\n    'train_file': 'train.csv',\n    'test_file': 'test.csv',\n    'w2v_pretrain': '..\/input\/glove-twitter-pretraining-for-word-embedding\/glove.twitter.27B.50d.txt',\n    \n    'hidden_dim': 256,\n    'target_dim': 1,\n    'max_len': 20,\n    'batch_size': 32,\n    'n_epoch': 10,\n    'n_splits': 5,\n    'learning_rate': 1e-3,\n    'seed': 1234,\n}","0c66173d":"def search_threshold(y_true, preds):\n    '''Find the best threshold value'''\n    best_thresh = 0.5\n    best_score = 0.0\n    for thresh in np.arange(0.3, 0.501, 0.01):\n        thresh = np.round(thresh, 2)\n        score = f1_score(y_true, (preds > thresh).astype(int))\n        if score > best_score:\n            best_thresh = thresh\n            best_score = score\n    return best_thresh, best_score\n\ndef seed_everything(seed=1234):\n    '''Setting seed values to get reproducible results'''\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\nseed_everything(args['seed'])","6eb6f87a":"# Load word embedding.\nword_vectors = KeyedVectors.load_word2vec_format(args['w2v_pretrain'], binary=False)\nembed_dim = word_vectors.vector_size","b77e3f4b":"# Define pre-process functions for Tweet content.\ndef create_text_processor():\n    text_processor = TextPreProcessor(\n            normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n                       'time', 'date', 'number'],\n            fix_html=True,\n            segmenter=\"twitter\",\n            corrector=\"twitter\",\n\n            unpack_hashtags=True,\n            unpack_contractions=True,\n            spell_correct_elong=True,\n\n            # tokenizer=SocialTokenizer(lowercase=True).tokenize,\n            tokenizer=RegexpTokenizer(r'\\w+').tokenize,\n\n            dicts=[emoticons]\n        )\n\n    return text_processor\n\ndef remove_stopword(tokens):\n    stop_words = stopwords.words('english')\n    return [word for word in tokens if word not in stop_words]\n\ndef stemming(tokens, ps):\n    tokens = [ps.stem(token) for token in tokens]\n    return tokens\n\ndef lemmatizer(tokens, wn):\n    tokens = [wn.lemmatize(token) for token in tokens]\n    return tokens\n    \ndef pre_process(s):\n    text = s.text\n    text = text.replace(\"\\n\", \" \")\n    text = text.replace(\"\\\/\", '\/')\n    text = text.lower()\n\n    tokens = text_processor.pre_process_doc(text)\n    tokens = remove_stopword(tokens)\n    tokens = stemming(tokens, ps)\n    tokens = lemmatizer(tokens, wn)\n    return tokens\n\ndef token_to_index(s):\n    '''Convert token to its index in the word embedding model'''\n    tokens = s.tokens\n    return [word_vectors.vocab.get(token).index if token in word_vectors else 0 for token in tokens]","b301a045":"# Load data, pre-process text and convert tokens to indices \ndef load_data():\n    train_df = pd.read_csv(os.path.join(args['data_dir'], args['train_file']))\n    test_df = pd.read_csv(os.path.join(args['data_dir'], args['test_file']))\n    \n    train_df['tokens'] = train_df.apply(pre_process, axis=1)\n    test_df['tokens'] = test_df.apply(pre_process, axis=1)\n    train_df['token_index'] = train_df.apply(token_to_index, axis=1)\n    test_df['token_index'] = test_df.apply(token_to_index, axis=1)\n    \n    train_X = pad_sequences(train_df['token_index'].values, maxlen=args['max_len'])\n    test_X = pad_sequences(test_df['token_index'].values, maxlen=args['max_len'])\n    train_y = train_df['target'].values\n    \n    return train_X, test_X, train_y","135e8f58":"# Define training model\nclass Net(Module):\n    def __init__(self, hidden_dim, target_size):\n        super(Net, self).__init__()\n        weights = torch.FloatTensor(word_vectors.vectors)\n        oov_vector = torch.zeros([1, weights.shape[1]])\n        weights = torch.cat((weights, oov_vector), 0)\n\n        self.embedding = nn.Embedding.from_pretrained(weights)\n        self.embedding.weight.requires_grad = False\n        \n        self.dropout_1 = nn.Dropout(p=0.2)\n        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=1, batch_first=True)\n        self.dropout_2 = nn.Dropout(p=0.2)\n        self.linear_out = nn.Linear(hidden_dim, target_size)\n        \n    def forward(self, content):\n        embed_vector = self.embedding(content)\n        lstm_output, lstm_hidden = self.lstm(self.dropout_1(embed_vector))\n        last_hidden = torch.squeeze(lstm_hidden[0], 0)\n        output = self.linear_out(self.dropout_2(last_hidden))\n        \n        return output","c544b87e":"text_processor = create_text_processor()\nps = nltk.PorterStemmer()\nwn = nltk.WordNetLemmatizer()\n\ntrain_X, test_X, train_y = load_data()\n\ntrain_X_tensor = torch.tensor(train_X, dtype=torch.long)\ntrain_y_tensor = torch.tensor(train_y[:, np.newaxis], dtype=torch.float32)\ntest_X_tensor = torch.tensor(test_X, dtype=torch.long)","c4cf644c":"test_loader = DataLoader(test_X_tensor, batch_size=args['batch_size'], shuffle=False, num_workers=8)\n\nsplits = list(StratifiedKFold(n_splits=args['n_splits'], shuffle=True, random_state=args['seed']).split(train_X_tensor, train_y))\n\nval_preds = np.zeros(train_y.shape)\ntest_preds = []\nfor idx, (train_idx, val_idx) in enumerate(splits):\n    print('Train Fold {}\/{}'.format(idx, args['n_splits']))\n    \n    train_dataset = TensorDataset(train_X_tensor[train_idx], train_y_tensor[train_idx])\n    train_loader = DataLoader(train_dataset, batch_size=args['batch_size'], shuffle=True, num_workers=8)\n\n    valid_dataset = TensorDataset(train_X_tensor[val_idx], train_y_tensor[val_idx])\n    valid_loader = DataLoader(valid_dataset, batch_size=args['batch_size'], shuffle=False, num_workers=8)\n    \n    model = Net(args['hidden_dim'], args['target_dim'])\n    model.cuda()\n    criterion = torch.nn.BCEWithLogitsLoss()\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=args['learning_rate'], weight_decay=0.0)\n    \n    model.zero_grad()\n    for epoch in range(args['n_epoch']):\n        print('Epoch {}\/{}'.format(epoch, args['n_epoch']))\n        model.train()\n        tr_loss = 0.0\n        \n        for index, (features, label) in enumerate(train_loader):\n            features = features.cuda()\n            label = label.cuda()\n            preds = model(features)\n            loss = criterion(preds, label)\n            \n            loss.backward()\n            optimizer.step()\n            model.zero_grad()\n\n            tr_loss += loss.item()\n\n        train_loss = tr_loss \/ len(train_loader)\n        print(f\"---Train loss {train_loss}\")\n        \n        # Evaluation\n        model.eval()\n        vl_loss = 0.0\n        fold_val_preds = None\n        for features, label in valid_loader:\n            features = features.cuda()\n            label = label.cuda()\n            \n            preds = model(features)\n            loss = criterion(preds, label)\n            vl_loss += loss.item()\n            \n            if epoch == args['n_epoch'] - 1:\n                if fold_val_preds is None:\n                    fold_val_preds = torch.sigmoid(preds).detach().cpu().numpy().squeeze()\n                else:\n                    fold_val_preds = np.append(fold_val_preds, torch.sigmoid(preds).detach().cpu().numpy().squeeze(), axis=0)\n            \n        valid_loss = vl_loss \/ len(valid_loader)\n        print(f\"---Valid loss {valid_loss}\")\n    \n    val_preds[val_idx] = fold_val_preds\n        \n    # Test\n    model.eval()\n    fold_test_preds = None\n    for features in test_loader:\n        features = features.cuda()\n        preds = model(features)\n        \n        if fold_test_preds is None:\n            fold_test_preds = torch.sigmoid(preds).detach().cpu().numpy().squeeze()\n        else:\n            fold_test_preds = np.append(fold_test_preds, torch.sigmoid(preds).detach().cpu().numpy().squeeze(), axis=0)\n            \n    test_preds.append(fold_test_preds)","9d25a3a5":"# Search the best threshold\nthreshold, val_score = search_threshold(train_y, val_preds)\nprint('Val Score: {} - Best threshold: {}'.format(val_score, threshold))\n# Get mean predicted value of all folds, then convert to label\ntest_preds_mean = np.mean(test_preds, axis=0)\npred_result = (test_preds_mean > threshold).astype(int)\n\n# Export submission file\nsubmit_df = pd.read_csv(os.path.join(args['data_dir'], 'sample_submission.csv'))\nsubmit_df[\"target\"] = pred_result\nsubmit_df.to_csv(os.path.join(args['output_dir'], 'submission.csv'), index=False)","49fe3b96":"# Model","b1c746d5":"# Train","857c9f50":"Install ekphrasis library, which is a text processing tool, developed towards pre-process text from social networks, such as Twitter or Facebook.\n\n[https:\/\/github.com\/cbaziotis\/ekphrasis](http:\/\/)","0f7917b9":"# Export","223c489d":"# Process Data","52a26a80":"This notebook shows basic steps for classifying disaster tweets by using LSTM model in Pytorch.","1715edd7":"Load Data","a8874433":"Train K-fold Model","8973bec5":"# Import"}}