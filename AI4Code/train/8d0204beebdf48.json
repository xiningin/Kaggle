{"cell_type":{"496e621b":"code","cf2f2855":"code","c442161e":"code","fb7caab5":"code","d1cc5f68":"code","793a3045":"code","618b0746":"code","3d550049":"code","c4862b50":"code","fc8565d6":"code","72229d6c":"code","97a478f1":"code","3569d7a4":"code","0d198f3c":"code","a715e489":"code","2b88a858":"code","e350b3e2":"code","84289d8c":"code","1b32f9ed":"code","67c0fc83":"code","5dd1ee1e":"code","97d5c35f":"code","ef057190":"code","7b835da7":"code","665edbf8":"code","cf224e93":"code","a3b02131":"code","fa2f22bc":"code","4c2e1e88":"code","7252ad52":"code","0ba2afe0":"code","881f12df":"code","6e41434b":"code","9b4e714c":"code","090ecf5c":"code","45369b15":"code","d2dfe683":"code","bd6e120d":"code","0276e602":"code","562857eb":"code","7c792ad9":"code","d7675fbf":"code","b9ee3b8a":"code","4bd28056":"code","0a2056ca":"code","d6e3d7e7":"code","72203cbc":"code","4a3d4499":"code","4319cd04":"code","b491fa8b":"code","954bba4b":"code","5dbba718":"code","e93e10a0":"code","ff2bd660":"code","6d901019":"code","e6411eb5":"code","e3956c68":"code","f10dc1bf":"code","840ef8cb":"code","c3861fdf":"code","2c5c9698":"code","1168b31a":"code","8571bb8b":"code","807d8daf":"code","ea6d966b":"code","2893d974":"code","e8f57b22":"code","db228e90":"code","c4cc222e":"code","08ac69c7":"code","2aab0087":"code","7b6be7b4":"code","1963173c":"code","7e5eb3b2":"code","69f5359a":"code","13f50006":"code","5d990306":"code","f2da602b":"code","36fa5c5f":"code","c23f63fc":"code","882bc652":"code","39c33bf2":"code","54cc453e":"code","c72397eb":"code","0fbe69bd":"code","52734c00":"code","fec0935c":"code","d14c15c9":"code","9e5944f3":"markdown","539853e0":"markdown","3b1fc17e":"markdown","2eb19723":"markdown","54a30c29":"markdown"},"source":{"496e621b":"!pip install pandas\n!pip install numpy\n!pip install sklearn\n!pip install nltk\n!pip install re\n","cf2f2855":"import numpy as np \nimport pandas as pd\nimport re\nimport gc\nimport os\nprint(os.listdir(\"..\/input\"))\nimport fileinput\nimport string\nimport tensorflow as tf\nimport zipfile\nimport datetime\nimport sys\nfrom tqdm  import tqdm\ntqdm.pandas()\nfrom nltk.tokenize import wordpunct_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score, roc_auc_score\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\nfrom keras.layers.embeddings import Embedding\nfrom sklearn.metrics import classification_report","c442161e":"import pandas as pd\nimport numpy as np\nimport sklearn\nimport re, nltk\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,f1_score,confusion_matrix,classification_report\nfrom sklearn.naive_bayes import MultinomialNB","fb7caab5":"import os","d1cc5f68":"import warnings\nwarnings.filterwarnings(\"ignore\")\n","793a3045":"import pandas as pd","618b0746":"total_dataset = pd.read_csv(r'..\/input\/totaldata.tsv',sep='\\t',encoding='utf-8')\n","3d550049":"total_dataset.shape","c4862b50":"total_dataset['Content'].head(3)","fc8565d6":"total_dataset['AuthorName'].value_counts()","72229d6c":"total_dataset.drop_duplicates(subset=None, keep='first', inplace=True)","97a478f1":"def missing_values_table(df):\n        mis_val = df.isnull().sum()\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        return mis_val_table_ren_columns","3569d7a4":"#calculate percentage of nan, 0.1 and 0.3\nmissing_values_table(total_dataset)","0d198f3c":"#dropping null\ntotal_dataset = total_dataset.dropna(how='any',axis=1) \n","a715e489":" total_dataset.isnull().sum(axis = 0)","2b88a858":"total_dataset.shape","e350b3e2":"for item in total_dataset.head(2).values:\n  print (item)","84289d8c":"print(total_dataset.shape)","1b32f9ed":"list(total_dataset.columns.values)","67c0fc83":"#distribution of Class\ntotal_dataset['AuthorName'].value_counts()","5dd1ee1e":"total_dataset['Content'][0:5]","97d5c35f":"punctuation_marks = ['\u060c','!','\u201d','\u201c','\u061f',':','\u06d4']","ef057190":"punctuation_marks","7b835da7":"def Coloum_preprocessing(content):\n    content_clean = re.sub(r\"http\\S+\", \"\", content)\n    cleanText = re.sub(r\"[a-zA-Z0-9.@#_:)(-]\",\"\",content_clean)\n    #p = re.compile(r'<.*?>')\n    #cleanText = re.sub(nonalpha,'',tweet_clean).strip()#remove english alphabets\n    #cleanText = re.sub(p,'',cleanText).strip()#remove english alphabets\n    words = [w for w in cleanText.split() if w not in punctuation_marks]\n    #print(words)\n    return \" \".join(words)","665edbf8":"sample = total_dataset.iloc[3]['Content']","cf224e93":"print(sample)","a3b02131":"clean = Coloum_preprocessing(sample)","fa2f22bc":"total_dataset['Content'] = total_dataset['Content'].apply(Coloum_preprocessing)","4c2e1e88":"print(clean)","7252ad52":"labels = total_dataset['AuthorName']\nlabels.head()","0ba2afe0":"total_dataset.head()","881f12df":"X_train, X_test, y_train, y_test = train_test_split(total_dataset, labels, test_size=0.25, random_state=42)","6e41434b":"X_train[['Content','AuthorName']][0:5]","9b4e714c":"X_train.shape","090ecf5c":"X_test.shape","45369b15":"vectorizer = CountVectorizer()\ntrain_feats = vectorizer.fit_transform(X_train['Content'])\ntest_feats = vectorizer.transform(X_test.Content)","d2dfe683":"train_feats.shape","bd6e120d":"features = vectorizer.vocabulary_","0276e602":"features","562857eb":"train_feats.A[0,450:500]","7c792ad9":"# Using Random Forest","d7675fbf":"rf_model =  RandomForestClassifier()\nrf_model.fit(train_feats,y_train)","b9ee3b8a":"X_test['predictedAuthor'] = rf_model.predict(test_feats)","4bd28056":"print(accuracy_score(X_test['AuthorName'],X_test['predictedAuthor']))","0a2056ca":"print(f1_score(X_test['AuthorName'],X_test['predictedAuthor'],average='weighted'))","d6e3d7e7":"y_train.unique()","72203cbc":"print(classification_report(X_test['AuthorName'],X_test['predictedAuthor']))","4a3d4499":"nb_model = MultinomialNB()\nnb_model.fit(train_feats,y_train)","4319cd04":"X_test['predictedAuthor'] = nb_model.predict(test_feats)","b491fa8b":"print(accuracy_score(X_test['AuthorName'],X_test['predictedAuthor']))","954bba4b":"print(f1_score(X_test['AuthorName'],X_test['predictedAuthor'],average='weighted'))","5dbba718":"lr = LogisticRegression()\nlr.fit(train_feats,y_train)\nX_test['predictedAuthor'] = lr.predict(test_feats)\nprint(accuracy_score(X_test['AuthorName'],X_test['predictedAuthor']))","e93e10a0":"for x in range(50,500,50):\n    print(\"=============================\")\n    print(\"Max Features %d\"%x)\n    vectorizer = CountVectorizer(max_features=x)\n    train_feats = vectorizer.fit_transform(X_train['Content'])\n    test_feats = vectorizer.transform(X_test.Content)\n    print(\"Using Random Forest\")\n    rf_model =  RandomForestClassifier()\n    rf_model.fit(train_feats,y_train)\n    X_test['predictedAuthor'] = rf_model.predict(test_feats)\n    print(accuracy_score(X_test['AuthorName'],X_test['predictedAuthor']))\n    print(\"Using Naive Bayes\")\n    nb_model = MultinomialNB()\n    nb_model.fit(train_feats,y_train)\n    X_test['predictedAuthor'] = nb_model.predict(test_feats)\n\n    print(accuracy_score(X_test['AuthorName'],X_test['predictedAuthor']))\n    \n    print(\"Using Logistic Regression\")\n    lr = LogisticRegression()\n    lr.fit(train_feats,y_train)\n    X_test['predictedAuthor'] = lr.predict(test_feats)\n    print(accuracy_score(X_test['AuthorName'],X_test['predictedAuthor']))\n    ","ff2bd660":"for x in range(10,150,10):\n    print(\"=============================\")\n    print(\"Max Features %d\"%x)\n    vectorizer = CountVectorizer(max_features=x)\n    train_feats = vectorizer.fit_transform(X_train['Content'])\n    test_feats = vectorizer.transform(X_test.Content)\n    print(\"Using Random Forest\")\n    rf_model =  RandomForestClassifier()\n    rf_model.fit(train_feats,y_train)\n    X_test['predictedAuthor'] = rf_model.predict(test_feats)\n    print(accuracy_score(X_test['AuthorName'],X_test['predictedAuthor']))\n    print(\"Using Naive Bayes\")\n    nb_model = MultinomialNB()\n    nb_model.fit(train_feats,y_train)\n    X_test['predictedAuthor'] = nb_model.predict(test_feats)\n\n    print(accuracy_score(X_test['AuthorName'],X_test['predictedAuthor']))\n    \n    print(\"Using Logistic Regression\")\n    lr = LogisticRegression()\n    lr.fit(train_feats,y_train)\n    X_test['predictedAuthor'] = lr.predict(test_feats)\n    print(accuracy_score(X_test['AuthorName'],X_test['predictedAuthor']))\n    ","6d901019":"vectorizer = TfidfVectorizer(ngram_range=(1,3),max_features=80)\ntrain_feats = vectorizer.fit_transform(X_train['Content'])\ntest_feats = vectorizer.transform(X_test.Content)\nprint(\"Using Random Forest\")\nrf_model =  RandomForestClassifier()\nrf_model.fit(train_feats,y_train)\nX_test['predictedAuthor'] = rf_model.predict(test_feats)\nprint(accuracy_score(X_test['AuthorName'],X_test['predictedAuthor']))\nprint(\"Using Naive Bayes\")\nnb_model = MultinomialNB()\nnb_model.fit(train_feats,y_train)\nX_test['predictedAuthor'] = nb_model.predict(test_feats)\nprint(accuracy_score(X_test['AuthorName'],X_test['predictedAuthor']))\n\nprint(\"Using Logistic Regression\")\nlr = LogisticRegression()\nlr.fit(train_feats,y_train)\nX_test['predictedAuthor'] = lr.predict(test_feats)\nprint(accuracy_score(X_test['AuthorName'],X_test['predictedAuthor']))\n    ","e6411eb5":"vectorizer.vocabulary_","e3956c68":"X_train.iloc[0]['Content']","f10dc1bf":"train_feats.A[0,50:200]","840ef8cb":"len(vectorizer.vocabulary_)","c3861fdf":"from nltk import word_tokenize\nfrom collections import defaultdict","2c5c9698":"def count_top_x_words(corpus, top_x, skip_top_n):\n    count = defaultdict(lambda: 0)\n    for c in corpus:\n        for w in word_tokenize(c):\n            count[w] += 1\n    count_tuples = sorted([(w, c) for w, c in count.items()], key=lambda x: x[1], reverse=True)\n    return [i[0] for i in count_tuples[skip_top_n: skip_top_n + top_x]]\n\n\ndef replace_top_x_words_with_vectors(corpus, top_x):\n    topx_dict = {top_x[i]: i for i in range(len(top_x))}\n\n    return [\n        [topx_dict[w] for w in word_tokenize(s) if w in topx_dict]\n        for s in corpus\n    ], topx_dict\ndef filter_to_top_x(corpus, n_top, skip_n_top=0):\n    top_x = count_top_x_words(corpus, n_top, skip_n_top)\n    return replace_top_x_words_with_vectors(corpus, top_x)","1168b31a":"from keras.models import Sequential\nfrom keras.layers import Dense, Conv1D, Flatten\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing import sequence\nfrom keras.utils import to_categorical\nimport pandas as pd\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split","8571bb8b":"!pip install lib\n\n#from lib.get_top_xwords import filter_to_top_x","807d8daf":"df2=total_dataset","ea6d966b":"counter = Counter(df2['AuthorName'].tolist())\ntop_10_varieties = {i[0]: idx for idx, i in enumerate(counter.most_common(10))}\ndf = df2[df2['AuthorName'].map(lambda x: x in top_10_varieties)]","2893d974":"description_list = df['Content'].tolist()\nmapped_list, word_list = filter_to_top_x(description_list, 2500, 10)\nvarietal_list_o = [top_10_varieties[i] for i in df['AuthorName'].tolist()]\nvarietal_list = to_categorical(varietal_list_o)\n","e8f57b22":"max_review_length = 600\n\nmapped_list = sequence.pad_sequences(mapped_list, maxlen=max_review_length)\ntrain_x, test_x, train_y, test_y = train_test_split(mapped_list, varietal_list, test_size=0.3)","db228e90":"max_review_length = 600\n\nembedding_vector_length = 64\nmodel = Sequential()\n\nmodel.add(Embedding(2500, embedding_vector_length, input_length=max_review_length))\nmodel.add(Conv1D(50, 5))\nmodel.add(Flatten())\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(max(varietal_list_o) + 1, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.fit(train_x, train_y, epochs=3, batch_size=64)","c4cc222e":"y_score = model.predict(test_x)\ny_score = [[1 if i == max(sc) else 0 for i in sc] for sc in y_score]\nn_right = 0\nfor i in range(len(y_score)):\n    if all(y_score[i][j] == test_y[i][j] for j in range(len(y_score[i]))):\n        n_right += 1\n\nprint(\"Accuracy: %.2f%%\" % ((n_right\/float(len(test_y)) * 100)))","08ac69c7":"!wget https:\/\/storage.googleapis.com\/bert_models\/2018_11_23\/multi_cased_L-12_H-768_A-12.zip\n!wget https:\/\/raw.githubusercontent.com\/google-research\/bert\/master\/modeling.py \n!wget https:\/\/raw.githubusercontent.com\/google-research\/bert\/master\/optimization.py \n!wget https:\/\/raw.githubusercontent.com\/google-research\/bert\/master\/run_classifier.py \n!wget https:\/\/raw.githubusercontent.com\/google-research\/bert\/master\/tokenization.py ","2aab0087":"import modeling\nimport optimization\nimport run_classifier\nimport tokenization","7b6be7b4":"folder = 'model_folder'\nwith zipfile.ZipFile(\"multi_cased_L-12_H-768_A-12.zip\",\"r\") as zip_ref:\n    zip_ref.extractall(folder)","1963173c":"BERT_MODEL = 'multi_cased_L-12_H-768_A-12'\nBERT_PRETRAINED_DIR = f'{folder}\/multi_cased_L-12_H-768_A-12'\nOUTPUT_DIR = f'{folder}\/outputs'\nprint(f'>> Model output directory: {OUTPUT_DIR}')\nprint(f'>>  BERT pretrained directory: {BERT_PRETRAINED_DIR}')","7e5eb3b2":"df2 = total_dataset\n\nlabel=total_dataset['AuthorName']\nlabel.head()","69f5359a":"df2[\"Text\"] = total_dataset[\"Content\"]\n\ndf2[\"Label\"] = LabelEncoder().fit_transform(label)\n\ndf2.head()","13f50006":"df2 = df2.drop(\"Content\", axis=1)","5d990306":"df2.head()","f2da602b":"df2 = df2.drop(\"AuthorName\", axis=1)","36fa5c5f":"df2.head()","c23f63fc":"X_train, X_test, y_train, y_test = train_test_split(df2[\"Text\"].values, df2[\"Label\"].values, test_size=0.2, random_state=42)","882bc652":"def create_examples(lines, set_type, labels=None):\n#Generate data for the BERT model\n    guid = f'{set_type}'\n    examples = []\n    if guid == 'train':\n        for line, label in zip(lines, labels):\n            text_a = line\n            label = str(label)\n            examples.append(\n              run_classifier.InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n    else:\n        for line in lines:\n            text_a = line\n            label = '0'\n            examples.append(\n              run_classifier.InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n    return examples\n\n# Model Hyper Parameters\nTRAIN_BATCH_SIZE = 32\nEVAL_BATCH_SIZE = 8\nLEARNING_RATE = 1e-5\nNUM_TRAIN_EPOCHS = 3.0\nWARMUP_PROPORTION = 0.1\nMAX_SEQ_LENGTH = 50\n# Model configs\nSAVE_CHECKPOINTS_STEPS = 100000 #if you wish to finetune a model on a larger dataset, use larger interval\n# each checpoint weights about 1,5gb\nITERATIONS_PER_LOOP = 100000\nNUM_TPU_CORES = 8\nVOCAB_FILE = os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt')\nCONFIG_FILE = os.path.join(BERT_PRETRAINED_DIR, 'bert_config.json')\nINIT_CHECKPOINT = os.path.join(BERT_PRETRAINED_DIR, 'bert_model.ckpt')\nDO_LOWER_CASE = BERT_MODEL.startswith('multi')\nlabel_list = [str(num) for num in range(13)]\ntokenizer = tokenization.FullTokenizer(vocab_file=VOCAB_FILE, do_lower_case=DO_LOWER_CASE)\ntrain_examples = create_examples(X_train, 'train', labels=y_train)\n\ntpu_cluster_resolver = None #Since training will happen on GPU, we won't need a cluster resolver\n#TPUEstimator also supports training on CPU and GPU. You don't need to define a separate tf.estimator.Estimator.\nrun_config = tf.contrib.tpu.RunConfig(\n    cluster=tpu_cluster_resolver,\n    model_dir=OUTPUT_DIR,\n    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n    tpu_config=tf.contrib.tpu.TPUConfig(\n        iterations_per_loop=ITERATIONS_PER_LOOP,\n        num_shards=NUM_TPU_CORES,\n        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\nnum_train_steps = int(\n    len(train_examples) \/ TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\nnum_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n\nmodel_fn = run_classifier.model_fn_builder(\n    bert_config=modeling.BertConfig.from_json_file(CONFIG_FILE),\n    num_labels=len(label_list),\n    init_checkpoint=INIT_CHECKPOINT,\n    learning_rate=LEARNING_RATE,\n    num_train_steps=num_train_steps,\n    num_warmup_steps=num_warmup_steps,\n    use_tpu=False, #If False training will fall on CPU or GPU, depending on what is available  \n    use_one_hot_embeddings=True)\n\nestimator = tf.contrib.tpu.TPUEstimator(\n    use_tpu=False, #If False training will fall on CPU or GPU, depending on what is available \n    model_fn=model_fn,\n    config=run_config,\n    train_batch_size=TRAIN_BATCH_SIZE,\n    eval_batch_size=EVAL_BATCH_SIZE)\n","39c33bf2":"print('Please wait...')\ntrain_features = run_classifier.convert_examples_to_features(\n    train_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\nprint('>> Started training at {} '.format(datetime.datetime.now()))\nprint('  Num examples = {}'.format(len(train_examples)))\nprint('  Batch size = {}'.format(TRAIN_BATCH_SIZE))\ntf.logging.info(\"  Num steps = %d\", num_train_steps)\ntrain_input_fn = run_classifier.input_fn_builder(\n    features=train_features,\n    seq_length=MAX_SEQ_LENGTH,\n    is_training=True,\n    drop_remainder=True)\nestimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\nprint('>> Finished training at {}'.format(datetime.datetime.now()))","54cc453e":"def input_fn_builder(features, seq_length, is_training, drop_remainder):\n  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n\n  all_input_ids = []\n  all_input_mask = []\n  all_segment_ids = []\n  all_label_ids = []\n\n  for feature in features:\n    all_input_ids.append(feature.input_ids)\n    all_input_mask.append(feature.input_mask)\n    all_segment_ids.append(feature.segment_ids)\n    all_label_ids.append(feature.label_id)\n\n  def input_fn(params):\n    \"\"\"The actual input function.\"\"\"\n    print(params)\n    batch_size = 500\n\n    num_examples = len(features)\n    d = tf.data.Dataset.from_tensor_slices({\n        \"input_ids\":\n            tf.constant(\n                all_input_ids, shape=[num_examples, seq_length],\n                dtype=tf.int32),\n        \"input_mask\":\n            tf.constant(\n                all_input_mask,\n                shape=[num_examples, seq_length],\n                dtype=tf.int32),\n        \"segment_ids\":\n            tf.constant(\n                all_segment_ids,\n                shape=[num_examples, seq_length],\n                dtype=tf.int32),\n        \"label_ids\":\n            tf.constant(all_label_ids, shape=[num_examples], dtype=tf.int32),\n    })\n\n    if is_training:\n      d = d.repeat()\n      d = d.shuffle(buffer_size=100)\n\n    d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n    return d\n  return input_fn","c72397eb":"predict_examples = create_examples(X_test, 'test')\n\npredict_features = run_classifier.convert_examples_to_features(\n    predict_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n\npredict_input_fn = input_fn_builder(\n    features=predict_features,\n    seq_length=MAX_SEQ_LENGTH,\n    is_training=False,\n    drop_remainder=False)\n\nresult = estimator.predict(input_fn=predict_input_fn)","0fbe69bd":"preds = []\nfor prediction in result:\n      preds.append(np.argmax(prediction['probabilities']))","52734c00":"from sklearn.metrics import accuracy_score","fec0935c":"print(\"Accuracy of BERT is:\",accuracy_score(y_test,preds))","d14c15c9":"print(classification_report(y_test,preds))","9e5944f3":"## Experiment Using BERT for classification","539853e0":"## Experiment 1: Using Count Vectorizer without Stopwords Removal with Unigrams","3b1fc17e":"## Experiment with TfIdf","2eb19723":"## Experiment of Building a Text Classification Model using keras","54a30c29":"## Experiment 2: Using Count Vectorizer using Max Features without Stopwords Removal with Unigrams"}}