{"cell_type":{"b7bdc293":"code","0729c0c3":"code","500ae86e":"code","631a8005":"code","c2db621e":"code","1ae28cbe":"code","09db56c1":"code","242120db":"code","e9be0e51":"code","42fb06a4":"code","d635f48b":"code","5a8f9844":"code","393829a8":"code","475f1695":"code","5f8aab8c":"code","c8517c6d":"code","657629df":"code","15022cb5":"code","0a7b7cd9":"code","cdb712c8":"code","ccc48f25":"code","79e5cfdc":"code","10fc3940":"code","acbb986c":"code","4c8c26be":"code","cfca499d":"code","371d6a0a":"code","72b1dd98":"code","453ab266":"markdown","58317bb1":"markdown","5cab714f":"markdown","4850868e":"markdown","fcc3ef4f":"markdown","20413831":"markdown","848d04fc":"markdown","e0f5b537":"markdown","a3062d1c":"markdown","4c6dfc2d":"markdown","6d4f84fc":"markdown","53622156":"markdown","bd2d4331":"markdown","9f400dd9":"markdown","bb8b049c":"markdown","04993f64":"markdown","46b0e892":"markdown","30b73fdc":"markdown"},"source":{"b7bdc293":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns \nimport matplotlib.pyplot as plt\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0729c0c3":"import numpy as np\nimport pandas as pd\nimport seaborn as sns \n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.pipeline import Pipeline\n","500ae86e":"df = pd.read_csv('..\/input\/weather-dataset-rattle-package\/weatherAUS.csv')","631a8005":"df.describe()","c2db621e":"zeros_cnt = df.isnull().sum().sort_values(ascending=False)\npercent_zeros = (df.isnull().sum() \/ df.isnull().count()).sort_values(ascending=False)\n\nmissing_data = pd.concat([zeros_cnt, percent_zeros], axis=1, keys=['Total', 'Percent'])\nmissing_data\n#missing_data.T","1ae28cbe":"dropList = list(missing_data[missing_data['Percent'] > 0.15].index)\ndropList\ndf.drop(dropList, axis=1, inplace=True)","09db56c1":"df['Location'].unique()","242120db":"#df.head()\ndf.shape","e9be0e51":"sns.pairplot(df[:1000])","42fb06a4":"df.head()\ndf.drop(['Date'], axis=1, inplace=True)\ndf.drop(['Location'], axis=1, inplace=True)","d635f48b":"df.info()","5a8f9844":"ohe = pd.get_dummies(data=df, columns=['WindGustDir','WindDir9am','WindDir3pm'])\nohe.info()","393829a8":"from sklearn import preprocessing\nfrom numpy import array\n\nohe['RainToday'] = df['RainToday'].astype(str)\nohe['RainTomorrow'] = df['RainTomorrow'].astype(str)\n\nlb = preprocessing.LabelBinarizer()\n\nohe['RainToday'] = lb.fit_transform(ohe['RainToday'])\nohe['RainTomorrow'] = lb.fit_transform(ohe['RainTomorrow'])","475f1695":"ohe = ohe.dropna()\n#ohe.drop('Location', axis=1, inplace=True)\ny = ohe['RainTomorrow']\nX = ohe.drop(['RainTomorrow'], axis=1)","5f8aab8c":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)","c8517c6d":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV","657629df":"X_train.info()","15022cb5":"#param_grid = { \n#    'n_estimators': [100, 200],\n#    'max_features': ['auto'],\n#    'max_depth' : [4,5,8,10],\n#    'criterion' :['gini', 'entropy']\n#}\n#RFC = RandomForestClassifier()\n\n#cv_RFC = GridSearchCV(estimator=RFC, param_grid=param_grid, cv=2)\n#cv_RFC.fit(X_train, y_train)","0a7b7cd9":"#cv_RFC.best_params_\n#sorted(zip(cv_RFC.best_estimator_.feature_importances_,ohe.columns))","cdb712c8":"pipe = Pipeline([('scaler', StandardScaler()), ('RFC', RandomForestClassifier(criterion='gini', \n                                                                              max_depth=10, \n                                                                              max_features='auto',\n                                                                              n_estimators=200))])","ccc48f25":"pipe.fit(X_train, y_train)","79e5cfdc":"pipe.score(X_train, y_train)","10fc3940":"from sklearn.model_selection import cross_val_score\ncross_val_score(pipe, X, y, cv=3)","acbb986c":"y_pred = pipe.predict(X_test)\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\n\n#confusion_matrix(y_test, y_pred)\naccuracy_score(y_test, y_pred)","4c8c26be":"from sklearn.metrics import precision_score, recall_score, f1_score\n\n#recall_score(y_test, y_pred)\n#precision_score(y_test, y_pred)\nf1_score(y_test, y_pred)","cfca499d":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n\nns_probs = [0 for _ in range(len(y_test))]\nlr_probs = pipe.predict_proba(X_test)\nlr_probs = lr_probs[:, 1]\n\nns_auc = roc_auc_score(y_test, ns_probs)\nlr_auc = roc_auc_score(y_test, lr_probs)\n\nprint('No Skill: ROC AUC=%.3f' % (ns_auc))\nprint('RFC: ROC AUC=%.3f' % (lr_auc))\n\n# calculate roc curves\nns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\nlr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)\n# plot the roc curve for the model\nplt.plot(ns_fpr, ns_tpr, linestyle='--', label='Dummy Classifer')\nplt.plot(lr_fpr, lr_tpr, marker='.', label='RFC')\n# axis labels\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\n# show the legend\nplt.legend()\n# show the plot\nplt.show()","371d6a0a":"from sklearn.metrics import precision_recall_curve\ny_scores = pipe.predict_proba(X_train)[:,1]\n#y_scores\n\nprecisions, recalls, thresholds = precision_recall_curve(y_train, y_scores)\n\ndef plot_prc (precisions, recalls, thresholds):\n    plt.plot(thresholds, precisions[:-1], 'b--', label='Precision')\n    plt.plot(thresholds, recalls[:-1], 'g-', label='Recall')\n    plt.xlabel('Thresholds')\n    plt.legend(loc='center left')\n    plt.ylim([0,1])\n\nplot_prc(precisions, recalls, thresholds)","72b1dd98":"#y_pred = clf.predict(X_test)  # default threshold is 0.5\ny_pred1 = (pipe.predict_proba(X_train)[:,1] >= 0.8).astype(int) # set threshold as 0.3\nprecision_score(y_train, y_pred1)","453ab266":"<img src=\"https:\/\/www.abc.net.au\/cm\/rimage\/11665138-3x2-xlarge.jpg?v=3\" height=500 width=500>","58317bb1":"# 6. Conclusion","5cab714f":"**Let's plot a graph to identify the threshold influence on the scores**","4850868e":"# Rain prediction in Australia\n\n**Task type:** Classification\n\n**ML algorithm used:** Random Forest Classifier\n\n**Metrics:** Accuracy, ROC-AUC\n\n**Other methods used:** GridSearchCV, precision\/recall balancing","fcc3ef4f":"**A pairplot helps visualize dependencies and correlation between features. Some of them have quite obvious links.**","20413831":"**Let's drop those features where the missing\/total coefficient is higher than 15%.**","848d04fc":"**Cross validation scores on the whole dataset:**","e0f5b537":"**So, we have build a quite simple Random Forest Classifier using the features from dataset applying one-hot-encoding to the categorical features.**\n\n**The accuracy-score for the out-of-the-box model is around 85% which is not bad. The AUC score is 0.862.**\n\n**We have also conducted an experiment with shifting the decision boundary for the model which resulted in a precision score spike. This is the technique you can use to manually set the threshold for the trained classifier.**","a3062d1c":"# 1. Load data","4c6dfc2d":"# 3. Model building","6d4f84fc":"**Let's encode categorical features using one-hot-encoding.**","53622156":"**Here we can clearly see the balance between precision & recall. \nSo if we want a higher recall, we can shift a threshold to a higher value.**\n\n**However, you should decide on the threshold with a thorough analysis not to miss-out on the model performance later.**","bd2d4331":"# 2. Data preprocessing","9f400dd9":"# 5. Plotting precision-recall & ROC curves.","bb8b049c":"**Let's see how many zero values are there for each column.**","04993f64":"**We can see some pretty straightforward correlations with almost linear-shaped distribution.**","46b0e892":"# 4. Model evaluation","30b73fdc":"**Please uncomment this part of code to use grid search for hyperparameter tuning for the model. The model below uses the outcome of the GridSearch operation with best parameters.**"}}