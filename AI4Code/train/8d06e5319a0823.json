{"cell_type":{"f45432bb":"code","11ba30fc":"code","8ec956d4":"code","f2ba58f1":"code","dc0b0368":"code","2ccaa539":"code","5181ff54":"code","4f296871":"code","8db5cb32":"code","7f1bc0c9":"code","e80053cc":"code","dee38858":"code","d0e8db3d":"code","2bb7a9f4":"code","68b145a4":"code","cd679df2":"code","f87aaab1":"markdown","ae890ab9":"markdown","f8bd7514":"markdown","78be67a3":"markdown","835959e7":"markdown","fedb2577":"markdown","0d785ba1":"markdown"},"source":{"f45432bb":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\nimport datetime\nfrom sklearn.metrics import accuracy_score\nfrom kaggle.competitions import twosigmanews\nimport gc","11ba30fc":"env = twosigmanews.make_env()\n(market_train, news_train) = env.get_training_data()\ngc.enable()","8ec956d4":"#10:00\u4e4b\u540e\u7684\u7b97\u6210\u4e0b\u4e00\u5929\uff0c\u4f3c\u4e4e\u6709\u4e0d\u597d\u7684\u5f71\u54cd\n# index = news_train['time'][news_train['time'].dt.hour > 22].index\n# news_train.loc[index,'time']  = news_train.loc[index,'time'].dt.ceil('d')\nnews_train['time'] = news_train['time'].dt.floor('d')\ncols = ['sentimentNegative','sentimentNeutral','sentimentPositive','relevance','companyCount','bodySize','sentenceCount','wordCount','firstMentionSentence',\n        'sentimentWordCount','takeSequence','sentimentClass','noveltyCount12H', 'noveltyCount24H','noveltyCount3D', 'noveltyCount5D', 'noveltyCount7D', \n        'volumeCounts12H','volumeCounts24H', 'volumeCounts3D', 'volumeCounts5D','volumeCounts7D']\n\nnews_total = news_train[['time','assetName'] + cols].copy()\ndel news_train\ngc.collect()\nnews_train = news_total\nprint(news_train.columns)\n","f2ba58f1":"import warnings\nwarnings.filterwarnings(action ='ignore',category = DeprecationWarning)\n\n#\u76f4\u63a5\u76f8\u4e58\u5185\u5b58\u4f1a\u7206\u6389\uff0c\u6210\u4e4b\u540e\uff0c\u53d8\u6210\u4e860.66912\uff0c\u6bd4\u6700\u597d0.66972\u5dee\u4e86\u4e00\u70b9\uff0c\u6682\u65f6\u4e0d\u6210\n# for col in cols:\n#     if col != 'relevance':\n#         print(col)\n#         news_train[col] = news_train[col] * news_train['relevance']\n#\u805a\u5408\u6bcf\u4e00\u4e2a\u65e5\u671f\u524d\u4e09\u5929\u5185\u7684\u65b0\u95fb\u6570\u636e\uff0c\u5f71\u54cd\u80a1\u4ef7\u8d70\u52bf\n#\u4e4b\u524d\u7684\u7248\u672c\uff0c\u76f4\u63a5\u590d\u5236\u51e0\u4efd\uff0c\u7136\u540e\u548cmarket_train\u8fdb\u884cjoin\uff0c\u4ee3\u4ef7\u8f83\u5927\n#\u76f4\u63a5\u8fdb\u884cnews data\u7684join\ndef get_news_train(raw_data,days = 5):\n    news_last = pd.DataFrame()\n    #\u8870\u51cf\u7cfb\u6570\n    rate = 1.0\n    for i in range(days):\n        cur_train = raw_data[cols] * rate \n        rate *= 0.7\n        cur_train['time'] = raw_data['time'] + datetime.timedelta(days = i,hours=22)\n        cur_train['key'] = cur_train['time'].astype(str)+ raw_data['assetName'].astype(str)\n        #cur_train\u7684groupby\u662f\u88ab\u8feb\u7684\u64cd\u4f5c\uff0c\u5904\u7406new_train\uff0c6\u5929\u4e4b\u5185\u7684\uff0c\u5185\u5b58\u4e0d\u8db3\uff0c\u4e0b\u9762\u7684groupby\n        cur_train = cur_train[['key'] + cols].groupby('key').sum()\n        cur_train['key'] = cur_train.index.values\n        news_last = pd.concat([news_last, cur_train[['key'] + cols]])\n        del cur_train\n        gc.collect()\n        print(\"after concat the shape is:\",news_last.shape)\n        news_last = news_last.groupby('key').sum()\n        news_last['key'] = news_last.index.values\n        print(\"the result shape is:\",news_last.shape)\n       \n    del news_last['key']\n    return news_last\n\nnews_last = get_news_train(news_train)\nprint(news_last.shape)\nprint(news_last.head())\nprint(news_last.dtypes)\n","dc0b0368":"market_train['key'] = market_train['time'].astype(str) + market_train['assetName'].astype(str)\nmarket_train = market_train.join(news_last,on = 'key',how='left')\nprint(market_train['sentimentNeutral'].isnull().value_counts())\nmarket_train.head()","2ccaa539":"# print(market_train['assetName'].nunique())\n# print(news_train['assetName'].nunique())\n# \u901a\u8fc7assetName \u5224\u65ad\u6709market \u4e2d\u670912\u4e07\u4e2aexample\u6ca1\u5728 news\u4e2d\u51fa\u73b0,\u901a\u8fc7\u65f6\u95f4\u8fdb\u884cjoin\uff0c\u4ea4\u96c6\u592a\u5c11\uff0c\u76ee\u524d\u611f\u89c9\u4f7f\u7528\n# assetName\u6bd4\u8f83\u5408\u9002\n# print(market_train['assetName'].isin(news_train['assetName']).value_counts())\n# print(market_train['time'].nunique())\n# print(news_train['time'].nunique())\n# print(news_train['time'].describe())","5181ff54":"cat_cols = ['assetCode','assetName']\nnum_cols = ['volume', 'close', 'open', 'returnsClosePrevRaw1', 'returnsOpenPrevRaw1', 'returnsClosePrevMktres1',\n                    'returnsOpenPrevMktres1', 'returnsClosePrevRaw10', 'returnsOpenPrevRaw10', 'returnsClosePrevMktres10',\n                    'returnsOpenPrevMktres10','sentimentNegative','sentimentNeutral','sentimentPositive','relevance','companyCount','bodySize',\n            'sentenceCount','wordCount','firstMentionSentence', 'sentimentWordCount','takeSequence','sentimentClass','noveltyCount12H', \n            'noveltyCount24H','noveltyCount3D', 'noveltyCount5D', 'noveltyCount7D', 'volumeCounts12H','volumeCounts24H', 'volumeCounts3D',\n            'volumeCounts5D','volumeCounts7D']\n","4f296871":"from sklearn.model_selection import train_test_split\ntrain_indices, val_indices = train_test_split(market_train.index.values,test_size=0.25, random_state=23)","8db5cb32":"def encode(encoder, x):\n    len_encoder = len(encoder)\n    try:\n        id = encoder[x]\n    except KeyError:\n        id = len_encoder\n    return id\nencoders = [{} for i in range(len(cat_cols))]\nfor i, cat in enumerate(cat_cols):\n    print('encoding %s ...' % cat, end=' ')\n    encoders[i] = {l: id for id, l in enumerate(market_train.loc[train_indices, cat].unique())}\n    market_train[cat] = market_train[cat].astype(str).apply(lambda x: encode(encoders[i], x))\n    print('Done')\n\nembed_sizes = [len(encoder) + 1 for encoder in encoders] #+1 for possible unknown assets","7f1bc0c9":"from sklearn.preprocessing import StandardScaler \nimport matplotlib\n# market_train[num_cols] = market_train[num_cols].fillna(0)\n#\u5f02\u5e38\u70b9\u8fc7\u6ee4\n# print(market_train['close'][market_train['close'] > 1000].count())\n# print(market_train['open'][market_train['open'] > 1000].count())\n# print(market_train['volume'][market_train['volume'] > 1e+08].count())\n\nmarket_train['close'].clip(upper = 1000, inplace = True)\nmarket_train['open'].clip(upper = 1000, inplace = True)\nmarket_train['volume'].clip(upper = 1e+08, inplace = True)\n\n# matplotlib.rcParams['figure.figsize'] = (12.0, 6.0)\n# prices = pd.DataFrame({\"close\":market_train[\"close\"], \"log(close + 1)\":np.log1p(market_train[\"close\"])})\n# prices.hist(bins = 10)\n# \u4e00\u5b9a\u540c\u65f6\u8fdb\u884c\u5f85\u9884\u6d4b\u6570\u636e\u96c6\u7684\u5bf9\u6570\u8f6c\u6362\n# \u5f00\u76d8\u4ef7\uff0c\u6536\u76d8\u4ef7\u4e0d\u592a\u7b26\u5408\u6b63\u6001\u5206\u5e03\uff0c\u8fdb\u884c\u4e00\u4e2a\u5bf9\u6570\u8f6c\u6362\n# market_train['close'] = np.log1p(market_train['close'])\n# market_train['open'] = np.log1p(market_train['open'])\n\nprint('scaling numerical columns')\nscaler = StandardScaler()\ncol_mean = market_train[num_cols].mean()\nmarket_train[num_cols]=market_train[num_cols].fillna(col_mean)\n\nscaler = StandardScaler()\nmarket_train[num_cols] = scaler.fit_transform(market_train[num_cols])\n# market_train.describe()\n# market_train[num_cols].isna()\n# market_train['returnsClosePrevMktres1'].isnull().value_counts()\n","e80053cc":"def get_input(market_train, indices):\n    X = market_train.loc[indices, num_cols]\n    for cat in cat_cols:\n        X[cat] = market_train.loc[indices, cat].values\n    y = (market_train.loc[indices,'returnsOpenNextMktres10'] >= 0).values\n    r = market_train.loc[indices,'returnsOpenNextMktres10'].values\n    u = market_train.loc[indices, 'universe']\n    d = market_train.loc[indices, 'time'].dt.date\n    return X,y,r,u,d\n\n# r, u and d are used to calculate the scoring metric\nX_train,y_train,r_train,u_train,d_train = get_input(market_train, train_indices)\n\nX_valid,y_valid,r_valid,u_valid,d_valid = get_input(market_train, val_indices)\nX_train.shape\nprint(X_valid.shape)","dee38858":"\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nfrom functools import partial\nfrom hyperopt import hp, fmin, tpe\nfrom sklearn.metrics import mean_squared_error\nalgo = partial(tpe.suggest, n_startup_jobs=10)\n# def auto_turing(args):\n#     #model = XGBClassifier(n_jobs = 4, n_estimators = args['n_estimators'],max_depth=6)\n#     model = lgb.LGBMClassifier(n_estimators=args['n_estimators'])\n#     model.fit(X_train,y_train.astype(int))\n#     confidence_valid = model.predict(X_valid)*2 -1\n#     score = accuracy_score(confidence_valid>0,y_valid)\n#     print(args,score)\n#     return -score\n# space = {\"n_estimators\":hp.choice(\"n_estimators\",range(20,200))}\n# print(fmin)\n# best = fmin(auto_turing, space, algo=algo,max_evals=30)\n# print(best)\n\n# \u5355\u673axgb\u7a0b\u5e8f\n# model = XGBClassifier(n_jobs = 4, n_estimators = 80, max_depth=6, subsample = 0.66,colsample_bytree = 0.66,learning_rate = 0.1)\n# model.fit(X_train,y_train.astype(int))\n# confidence_valid = model.predict(X_valid)*2 -1\n# score = accuracy_score(confidence_valid>0,y_valid)\n# print(score)\n# print(\"MSE\")\n# print(mean_squared_error(confidence_valid > 0, y_valid.astype(float)))\n# \u5355\u673algb\u7a0b\u5e8f,\u8bad\u7ec3\u6bd4xgb\u5feb\n# import lightgbm as lgb\n# model = lgb.LGBMClassifier(num_threads = 4, n_estimators=100, feature_fraction = 0.66, bagging_fraction = 0.66,\n#                            early_stopping_rounds = 10,valid_sets = [X_valid, y_valid.astype(int)],objective = 'binary', metric='binary_logloss')\n# model.fit(X_train,y_train.astype(int))\n# confidence_valid = model.predict(X_valid)*2 -1\n# score = accuracy_score(confidence_valid>0,y_valid)\n# print(score)\n# print(\"MSE\",mean_squared_error(confidence_valid > 0, y_valid.astype(float)))\n# custom function to run light gbm model\ndef run_lgb(train_X, train_y, val_X, val_y,args):\n    params = {\n        \"objective\" : \"binary\",\n        \"metric\" : \"binary_logloss\", \n        \"num_leaves\" : args['num_leaves'],\n        \"min_child_samples\" : args['min_child_samples'],\n        \"learning_rate\" : args['learning_rate'],\n        \"bagging_fraction\" : 0.7,\n        \"feature_fraction\" : 0.66,\n        \"bagging_frequency\" : 5,\n        \"bagging_seed\" : 2018,\n        \"verbosity\" : -1\n    }\n    \n    lgtrain = lgb.Dataset(train_X, label=train_y)\n    lgval = lgb.Dataset(val_X, label=val_y)\n    model = lgb.train(params, lgtrain, args['n_estimators'], valid_sets=[lgval], early_stopping_rounds=50, verbose_eval=100)\n    \n#     pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n    pred_val_y = model.predict(val_X, num_iteration=model.best_iteration)\n    confidence_valid = model.predict(X_valid)*2 -1\n    score = accuracy_score(confidence_valid > 0 , y_valid)\n    print(score)\n    mse = mean_squared_error(confidence_valid > 0, y_valid.astype(float))\n    print(\"MSE\", mse)\n    print(\"args\",args)\n    return model, mse\n\n# def auto_turing(args):\n#     model, mse = run_lgb(X_train, y_train.astype(int), X_valid, y_valid.astype(int),args)\n#     return mse\n# space = {\"n_estimators\":hp.choice('n_estimators',range(100,1000)),\n#          \"num_leaves\":hp.choice('num_leaves',range(20,100)),\n#          \"min_child_samples\":hp.choice(\"min_child_samples\",range(20,2000)),\n#          'learning_rate':hp.loguniform('learning_rate',0.01,0.3),\n#          'max_depth': hp.choice('max_depth', range(3,8))\n#         }\n# print(fmin)\n# best = fmin(auto_turing, space, algo=algo,max_evals=100)\n# print(best)\nargs = {'learning_rate': 1.0958730495793214, 'max_depth': 7, 'min_child_samples': 301, 'n_estimators': 439, 'num_leaves': 43}\nmodel, _ = run_lgb(X_train, y_train.astype(int), X_valid, y_valid.astype(int), args)\n\n# from sklearn.ensemble import RandomForestClassifier\n# distribution of confidence that will be used as submission\n# plt.hist(confidence_valid, bins='auto')\n# plt.title(\"predicted confidence\")\n# plt.show()\n# these are tuned params I found\ngc.collect()\n ","d0e8db3d":"# calculation of actual metric that is used to calculate final score\nconfidence_valid = model.predict(X_valid)*2 -1\nr_valid = r_valid.clip(-1,1) # get rid of outliers. Where do they come from??\nx_t_i = confidence_valid * r_valid * u_valid\ndata = {'day' : d_valid, 'x_t_i' : x_t_i}\ndf = pd.DataFrame(data)\nx_t = df.groupby('day').sum().values.flatten()\nmean = np.mean(x_t)\nstd = np.std(x_t)\nscore_valid = mean \/ std\nprint(score_valid)\nmarket_train.describe()","2bb7a9f4":"days = env.get_prediction_days()","68b145a4":"n_days = 0\npredicted_confidences = np.array([])\nfrom collections import deque\nnews_pre = deque()\nnews_all = pd.DataFrame()\nBaseMod = 50\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    n_days +=1\n    print(n_days,end=' ')\n    news_all = pd.concat([news_all,news_obs_df])\n    if n_days >= BaseMod and n_days % BaseMod >= 0 and n_days % BaseMod < 8:\n        news_pre.append(news_obs_df)\n    elif n_days >= BaseMod and n_days % BaseMod == 8:\n        del news_all\n        gc.collect()\n        news_all = pd.DataFrame()\n        for item in news_pre:\n            news_all = pd.concat([news_all,item])\n        news_pre.clear()\n    \n#     index = news_all['time'][news_all['time'].dt.hour > 22].index\n#     news_all.loc[index,'time']  = news_all.loc[index,'time'].dt.ceil('d')\n    news_all['time'] = news_all['time'].dt.floor('d')\n    news_last = pd.DataFrame()\n    \n#     for col in cols:\n#         if col != 'relevance':\n#             print(col)\n#             news_all[col] = news_all[col] * news_all['relevance']\n    #\u805a\u5408\u6bcf\u4e00\u4e2a\u65e5\u671f\u524d\u4e09\u5929\u5185\u7684\u65b0\u95fb\u6570\u636e\uff0c\u5f71\u54cd\u80a1\u4ef7\u8d70\u52bf\n    news_last = get_news_train(news_all)\n\n    market_obs_df['key'] = market_obs_df['time'].astype(str) + market_obs_df['assetName'].astype(str)\n    market_obs_df = market_obs_df.join(news_last,on = 'key',how='left')\n    \n    #\u5f02\u5e38\u70b9\u8fc7\u6ee4\n    market_obs_df['close'].clip(upper = 1000, inplace = True)\n    market_obs_df['open'].clip(upper = 1000, inplace = True)\n    market_obs_df['volume'].clip(upper = 1e+08, inplace = True)\n    \n    # \u5bf9\u6570\u8f6c\u6362\n#     market_obs_df['close'] = np.log1p(market_obs_df['close'])\n#     market_obs_df['open'] = np.log1p(market_obs_df['open'])\n    \n#     col_mean = [num_cols].mean()\n    #\u5f52\u4e00\u5316\n    market_obs_df[num_cols]=market_obs_df[num_cols].fillna(col_mean)\n    market_obs_df[num_cols] = scaler.transform(market_obs_df[num_cols])\n    X_test = market_obs_df[num_cols]\n    X_test['assetCode'] = market_obs_df['assetCode'].apply(lambda x: encode(encoders[0], x)).values\n    X_test['assetName'] = market_obs_df['assetName'].apply(lambda x: encode(encoders[1], x)).values\n\n    \n    market_prediction = model.predict(X_test)*2 -1\n    predicted_confidences = np.concatenate((predicted_confidences, market_prediction))\n\n    preds = pd.DataFrame({'assetCode':market_obs_df['assetCode'],'confidence':market_prediction})\n    # insert predictions to template\n    predictions_template_df = predictions_template_df.merge(preds,how='left').drop('confidenceValue',axis=1).fillna(0).rename(columns={'confidence':'confidenceValue'})\n    env.predict(predictions_template_df)\n    del news_last\n    gc.collect()\n\nenv.write_submission_file()","cd679df2":"# distribution of confidence as a sanity check: they should be distributed as above\nplt.hist(predicted_confidences, bins='auto')\nplt.title(\"predicted confidence\")\nplt.show()","f87aaab1":"# Handling numerical variables","ae890ab9":"# Train  model using hyperopt to auto hyper_parameters turing","f8bd7514":"Result validation","78be67a3":"# Prepare data","835959e7":"# Prediction","fedb2577":"# Handling categorical variables","0d785ba1":"# Market Data Only Baseline\n\nUsing a lot of ideas from NN Baseline Kernel.\nsee. https:\/\/www.kaggle.com\/christofhenkel\/market-data-nn-baseline"}}