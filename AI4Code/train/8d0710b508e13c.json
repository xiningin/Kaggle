{"cell_type":{"42d465b3":"code","233f0042":"code","50f48e5f":"code","13fb93ba":"code","2a911e40":"code","e809cf50":"code","4c05c84e":"code","ac965540":"code","2e6d79c1":"code","01e1a552":"code","a3d049ee":"code","eb80c42a":"code","4446817d":"code","b485a079":"code","c09e42a6":"code","44268273":"code","0d54b023":"code","98d0260c":"code","015669bd":"code","148a9e72":"code","2e6c1890":"code","3a4169ba":"code","f527dd76":"code","5ec3d470":"markdown","f1cce97e":"markdown","007b55fd":"markdown","8f1e9b4e":"markdown","0dedffaf":"markdown","49f25176":"markdown","3ecb9af2":"markdown","391eafb5":"markdown","941f2abf":"markdown","9ae21072":"markdown","f06db5a0":"markdown","77ed7ee7":"markdown","9f6dcb97":"markdown","d0e830af":"markdown","5e67e985":"markdown","0a44913d":"markdown","42191e88":"markdown","7dd6bd23":"markdown","bc8cf64e":"markdown","99093dfa":"markdown","ecf2f330":"markdown","006c963b":"markdown","3890dcca":"markdown","779228b7":"markdown","635b29ed":"markdown","4536aaad":"markdown","bec6431d":"markdown","5360d4ff":"markdown","e7855c55":"markdown"},"source":{"42d465b3":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport cv2\nimport os\nfrom tqdm import tqdm\nimport tensorflow as tf\nimport pickle\nimport time\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\ntrain = pd.read_csv('..\/input\/petfinder-pawpularity-score\/train.csv')\ntrain_images_path = '..\/input\/petfinder-pawpularity-score\/train'\ntrain_images_list = os.listdir(train_images_path)\nprint('Total Number of Training Images : ',len(train_images_list))","233f0042":"train_images = []\nfor i in tqdm(range(len(train_images_list))):\n    path = os.path.join(train_images_path,train_images_list[i])\n    image = cv2.imread(path)\n    image = image \/ 255\n    image = cv2.resize(image,(128,128))\n    train_images.append(image)\ntrain_images = np.array(train_images)  \n\ntrain_label = train['Pawpularity'] \/ 100","50f48e5f":"from tensorflow.keras.applications.vgg16 import VGG16;\nmodel = VGG16(include_top = False,input_shape = (128,128,3),weights = '..\/input\/vgg16-no-top-weights\/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5')\nfor layer in model.layers:\n    layer.trainable = False\n    \ntrain_feature_extractor = model.predict(train_images)\ntrain_features = train_feature_extractor.reshape(train_feature_extractor.shape[0],-1)","13fb93ba":"print('Input to Feature Extractor Shape : ',train_images.shape)\nprint('Output of Feature Extractor Shape : ',train_feature_extractor.shape)\nprint('Input to Machine Learning Algorithm Shape',train_features.shape)","2a911e40":"from sklearn.model_selection import KFold\nkf = KFold(n_splits = 4)\n\nx_train_cv,x_test_cv,y_train_cv,y_test_cv = train_test_split(train_features,train_label,test_size = 0.75)","e809cf50":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()","4c05c84e":"for train_index,test_index in tqdm(kf.split((train_images))):\n    \n    lr.fit(train_features[train_index],train_label[train_index])\n    pred = lr.predict(train_features[test_index]);y_true = train_label[test_index]\n    print('RMSE {:.2f}'.format(mean_squared_error(pred,y_true,squared = False)))","ac965540":"with open('lr_pickle.pkl','wb') as f:\n    pickle.dump(lr,f)","2e6d79c1":"from sklearn.svm import SVR\nsvr = SVR()","01e1a552":"param_tuning = {\n        'kernel': ['linear','poly'],\n        'C': [1,0.1,0.01] }\n\ngsearch = GridSearchCV(estimator = svr,\n                       param_grid = param_tuning,                        \n                       cv = 5,\n                       n_jobs = -1,\n                       verbose = 1)\ngsearch.fit(x_train_cv,y_train_cv)\ngsearch.best_params_","a3d049ee":"svr = SVR(C=0.01,kernel='poly')\n\nfor train_index,test_index in tqdm(kf.split((train_images))):\n    \n    svr.fit(train_features[train_index],train_label[train_index])\n    pred = svr.predict(train_features[test_index]);y_true = train_label[test_index]\n    print('RMSE {:.2f}'.format(mean_squared_error(pred,y_true,squared = False)))","eb80c42a":"with open('svr_pickle.pkl','wb') as f:\n    pickle.dump(svr,f)","4446817d":"import xgboost \nxgb = xgboost.XGBRegressor()","b485a079":"param_tuning = {\n        'learning_rate': [0.001,0.01, 0.1],\n        'max_depth': [4,5,6],\n        'n_estimators' : [100,200]}\n\ngsearch = GridSearchCV(estimator = xgb,\n                       param_grid = param_tuning,                        \n                       cv = 5,\n                       n_jobs = -1,\n                       verbose = 1)\ngsearch.fit(x_train_cv,y_train_cv)\ngsearch.best_params_","c09e42a6":"xgb = xgboost.XGBRegressor(learning_rate= 0.01,max_depth= 3,\n                             n_estimators = 100)\n\nfor train_index,test_index in tqdm(kf.split((train_images))):\n    \n    xgb.fit(train_features[train_index],train_label[train_index])\n    pred = xgb.predict(train_features[test_index]);y_true = train_label[test_index]\n    print('RMSE {:.2f}'.format(mean_squared_error(pred,y_true,squared = False)))","44268273":"with open('xgb_pickle.pkl','wb') as f:\n    pickle.dump(xgb,f)","0d54b023":"from lightgbm import LGBMRegressor\nlgbm = LGBMRegressor() ","98d0260c":"param_tuning = {\n        'learning_rate': [0.001,0.01, 0.1],\n        'max_depth': [4,5,6],\n        'n_estimators' : [100,200]}\n\ngsearch = GridSearchCV(estimator = lgbm,\n                       param_grid = param_tuning,                        \n                       cv = 5,\n                       n_jobs = -1,\n                       verbose = 1)\ngsearch.fit(x_train_cv,y_train_cv)\ngsearch.best_params_","015669bd":"lgbm = LGBMRegressor(learning_rate = 0.01,max_depth = 3,n_estimators = 100)\n\nfor train_index,test_index in tqdm(kf.split((train_images))):\n\n    lgbm.fit(train_features[train_index],train_label[train_index])\n    pred = lgbm.predict(train_features[test_index]);y_true = train_label[test_index]\n    print('RMSE {:.2f}'.format(mean_squared_error(pred,y_true,squared = False)))","148a9e72":"with open('lgbm_pickle.pkl','wb') as f:\n    pickle.dump(lgbm,f)","2e6c1890":"from mlxtend.regressor import StackingCVRegressor\nstack = StackingCVRegressor(regressors = (lr,svr,xgb,lgbm),meta_regressor = lr)","3a4169ba":"for train_index,test_index in tqdm(kf.split((train_images))):\n\n    stack.fit(train_features[train_index],train_label[train_index])\n    pred = stack.predict(train_features[test_index]);y_true = train_label[test_index]\n    print('RMSE {:.2f}'.format(mean_squared_error(pred,y_true,squared = False)))","f527dd76":"with open('stack_pickle.pkl','wb') as f:\n    pickle.dump(stack,f)","5ec3d470":"## <center>Feature Extractor : VGG16 + ML Algorithm [Training Notebook]<\/center>\n\n### <center>Welcome Curious Reader!<\/center>\n\n- You are now going to explore and understand this Training Notebook created for the competition : [PetFinder.my - PawPularity](https:\/\/www.kaggle.com\/c\/petfinder-pawpularity-score)  \n\n- This competition opens the door for facing the challenge of using Image Data & Categorical Data to predict a Continuous Value. \n\n- **Aim** : To understand the approach of using Feature Extractor in combination with a Machine Learning Algorithm to predict the output feature only using Image Data for training the models.","f1cce97e":"## <center>If you like the content of the notebook, please do upvote!<\/center>\n### <center>Feedback is appreciated!<\/center>\n### <center>Stay Safe!<\/center>","007b55fd":"### <center>Pickle File<\/center>","8f1e9b4e":"#### <center>Hyperparamter Tuning + Cross Validation<\/center>","0dedffaf":"- Redefining the model with tuned hyperparameters & training the model.\n- Average RMSE : 0.2125","49f25176":"## <center>Linear Regression<\/center>","3ecb9af2":"## <center>LGBM Regressor<\/center>","391eafb5":"### <center>Pickle File<\/center>","941f2abf":"## <center>Support Vector Regressor<\/center>","9ae21072":"### <center>Pickle File<\/center>","f06db5a0":"### Common Steps used for all the Regression Algorithms :\n\n1. Define the model.\n\n2. Using hyperparamter tuning & cross validation in combination with train_test_split, find the best parameters for the model.\n\n3. Fit the model on complete dataset using the combinations generated by the KFold.\n\n4. Pickle file of the model is generated and it is then referenced in the inference notebook for prediction purposes. ","77ed7ee7":"## <center>Xgboost Regressor<\/center>","9f6dcb97":"## <center>Create Train Image Batches & Output Feature Variable<\/center>","d0e830af":"### <center>Pickle File<\/center>","5e67e985":"## <center>Feature Extractor : VGG16<\/center>\n\n- We are using VGG16 as Image Feature Extractor which requires a batch of images as input.\n    \n- To define VGG16, we use [Keras Applications](https:\/\/keras.io\/api\/applications\/) to call VGG16 and set its parameters.","0a44913d":"## <center>Regression Algorithms\n\n- **Machine Learning Algorithms** : Linear Regression, Support Vector Regressor, Random Forest Regressor, Xgboost Regressor, LGBM Regressor, Stack of Regressors.\n\n- KFold is used to divide the dataset into sections which assists to make the model robust by training and testing on all the images.\n\n- **train_test_split** from sklearn was used for hyperparameter tuning & cross validation purposes with a high test size of 75 %.\n\n- This was done to get a random 25% of dataset.[Just an approach! You can try different methods aswell!]<\/center>","42191e88":"- Redefining the model with tuned hyperparameters & training the model.\n- Average RMSE : 0.205","7dd6bd23":"- Redefining the model with tuned hyperparameters & training the model.\n- Average RMSE : 0.205","bc8cf64e":"### <font color = 'red'>Note<\/font> : Below cells were executed at different times, these cells were commented & executed once again before publishing. Thus, you won't be able to see the outputs of print statements and model definitions. ","99093dfa":"- Normalizing the output feature variable by dividing it by 100 to bring their values in the range of [0 - 1]. ","ecf2f330":"### <center>Pickle File<\/center>","006c963b":"- Used default parameters,could not judge which paramters to tune. \n- RMSE [Average] : 1.56","3890dcca":"- VGG16 model is assigned the weights of the VGG16 model which does not have the dense layers for classification purpose manually.\nFor more information on this part, checkout [Keras Applications](https:\/\/keras.io\/api\/applications\/)\n\n- Source for downloading weights can be found [here!](https:\/\/github.com\/fchollet\/deep-learning-models\/releases\/tag\/v0.1)\n\n- To make sure that we don't lose the learnings of the weights, we use a for loop and assign **layer.trainable = False** which makes sure that we don't overwrite the weights set for the individual layers.","779228b7":"## <center>Stack : Linear Regression, Support Vector Regressor, Xgboost Regressor, LGBM Regressor<\/center>","635b29ed":"## <center>Conclusion<\/center>\n\n1. Huge amount of features generated by VGG16.\n2. Model fitting & hyperparameter tuning were time consuming tasks.\n3. Model Training Performances :\n\n| Model  |  RMSE |\n| :--- | :---: |\n|   LR   | 1.56  |\n|  SVR   | 0.205 |\n|  XGB   | 0.2125|\n|  LGBM  | 0.205 |\n|  STACK | 0.205 |\n\n\n\nLinks: \n\n1. [Inference Notebook](https:\/\/www.kaggle.com\/tanmay111999\/starter-feature-extractor-ml-algo-infer)\n2. [Discussion Post](https:\/\/www.kaggle.com\/c\/petfinder-pawpularity-score\/discussion\/279212)","4536aaad":"## <center>Import The Necessary Libraries & Define Data Access Variables <\/center>","bec6431d":"#### <center>Hyperparamter Tuning + Cross Validation<\/center>","5360d4ff":"- Did not carry out hyperparameter tuning!\n- Average RMSE : 0.205","e7855c55":"### <center>Hyperparamter Tuning + Cross Validation<\/center>"}}