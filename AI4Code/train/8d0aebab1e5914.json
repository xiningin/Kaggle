{"cell_type":{"fb6e5d83":"code","45bd7aa2":"code","157e92c3":"code","9de7d3c9":"code","3ed137a8":"code","3e064a87":"code","ad976d2d":"code","0274189f":"code","adf7cd78":"code","44e1fcc3":"code","fb787152":"code","92b76277":"code","9f4df1b6":"code","4b7840c2":"code","2214623d":"code","8577b2ce":"code","c8416b2f":"code","71d1c615":"code","a15fc0a2":"code","ef058f26":"code","bea367ea":"code","c9e689aa":"code","f0c455bd":"code","e3ff407e":"code","8bbe90de":"code","ca0ae504":"code","4c075d96":"markdown","2aaea69a":"markdown","7d3006da":"markdown","464848de":"markdown","b16a93fc":"markdown","743b2f76":"markdown","4d415c90":"markdown","52f49d12":"markdown","7b260236":"markdown","c961f187":"markdown","171a6e7f":"markdown","94e8c9f1":"markdown","662ad531":"markdown","53775100":"markdown","3a0e4d61":"markdown"},"source":{"fb6e5d83":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","45bd7aa2":"import matplotlib.pyplot as plt\nimport seaborn as sns","157e92c3":"# Load MNIST Data (train.csv)\nd0 = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')","9de7d3c9":"# print first five rows of d0\nd0.head()","3ed137a8":"# save the labels into a variable l.\n\nl = d0.label","3e064a87":"# Drop the label feature from d0 and store the pixel data in d\nd = d0.drop('label',axis=1)","ad976d2d":"#print shape of pixel and label data\nprint(l.shape)\nprint(d.shape)","0274189f":"idx = 1\n# print label value for index 1\nprint(l[idx])","adf7cd78":"d.head()","44e1fcc3":"plt.figure(figsize=(2,2))\n\n# reshape d from 1d to 2d pixel array for given idx ( prefer 28 X 28)\ngrid_data = d.loc[idx].values.reshape(28,28)\n\n#plot above grid image with cmap as gray and interpoltion as none\nplt.imshow(grid_data,interpolation='none',cmap='gray')\n\n#display plot\nplt.show()","fb787152":"# Pick first 15K data-points to work on for time-effeciency\n#Excercise: Perform the same analysis on all of 42K data-points\n\nlabels = l.head(15000)#labels with 15k data points\ndata = d.head(15000)#data with 15k data points\n\nprint(\"the shape of sample data = \",data.shape)","92b76277":"d.head()","9f4df1b6":"l.head()","4b7840c2":"# import standard scalar\nfrom sklearn.preprocessing import StandardScaler\n\n#fit transform data\nstandardized_data = StandardScaler().fit_transform(data)\n\n#print shape of standardized_data\ndata.shape","2214623d":"sample_data = standardized_data\n\n#use matrix multiplication on sample_data using numpy to find covariance matrix\ncovar_matrix = np.matmul(sample_data.T,sample_data)\n\n\n#print shape of covar_matrix\nprint ( \"The shape of variance matrix \\n\",covar_matrix.shape)","8577b2ce":"covar_matrix","c8416b2f":"# finding the top two eigen-values and corresponding eigen-vectors \n# for projecting onto a 2-Dim space\n\nfrom scipy.linalg import eigh \n\n# the parameter 'eigvals' is defined (low value to heigh value) \n# eigh function will return the eigen values in asending order\n\n# this code generates only the top 2 (782 and 783) eigenvalues\nvalues, vectors = eigh(covar_matrix,eigvals=(782,783))\n\nprint(\"Shape of eigen vectors = \",vectors.shape)\nprint(vectors)\n\n# converting the eigen vectors into (2,d) shape for easyness of further computations\nvectors = vectors.T\nprint(\"Updated shape of eigen vectors = \",vectors.shape)\nprint(vectors)\n# here the vectors[1] represent the eigen vector corresponding 1st principal eigen vector\n# here the vectors[0] represent the eigen vector corresponding 2nd principal eigen vector","71d1c615":"# multiplication of two principal eigen vectors with transposed sample_data to get 2d projected data\nnew_coordinates = np.matmul(vectors,sample_data.T)\n\nprint (\" resultanat new data points' shape \",vectors.shape, \"X\",sample_data.T.shape,\" = \",new_coordinates.shape)\n","a15fc0a2":"# appending label to the 2d projected data \nnew_coordinates = np.vstack((new_coordinates,labels)).T\n\n# creating a new data frame for ploting the labeled points.\ndataframe = pd.DataFrame(data=new_coordinates,columns=('1st_principal','2nd_principal','label'))\n\n#print dataframe head\ndataframe.head()","ef058f26":"sns.FacetGrid(dataframe,hue='label',size=8).map(plt.scatter,'1st_principal','2nd_principal','label').add_legend()\n\nplt.show()","bea367ea":"# initializing the pca\nfrom sklearn import decomposition\n\n\npca = decomposition.PCA()","c9e689aa":"# the number of components = 2\npca.n_components = 2\n\n# fit transform sample data using pca \npca_data = pca.fit_transform(sample_data)\n\n# pca_reduced will contain the 2-d projects of simple data\nprint(\"shape of pca_reduced.shape = \",pca_data.shape)","f0c455bd":" pd.DataFrame(data=np.vstack((pca_data.T,labels)).T,columns=('1st_principal','2nd_principal','label'))","e3ff407e":"# attaching the label for each 2-d data point (Hint: Use np.vstack)\npca_data = np.vstack((pca_data.T,labels)).T\n\n# creating a new data fram which help us in ploting the result data\npca_df = pd.DataFrame(data=pca_data,columns=('1st_principal','2nd_principal','label'))\n\n#plotting the 2d data points\nsns.FacetGrid(pca_df,hue='label',size=8).map(plt.scatter,'1st_principal','2nd_principal','label').add_legend()\nplt.show()","8bbe90de":"# the number of components = 784\npca.n_components = 784\n\n\n# fit transform sample data using pca \npca_data = pca.fit_transform(sample_data)\n\n\n#calculating percentage of variance explained in the data\npercentage_var_explained = pca.explained_variance_ \/ np.sum(pca.explained_variance_)\n\n#cumulative sum of the percentage_var_explained\ncumulative_explained_variance = np.cumsum(percentage_var_explained)","ca0ae504":"# Plot the PCA spectrum\nplt.figure(figsize=(6,4))\nplt.plot(cumulative_explained_variance,linewidth=4)\nplt.grid()\n\nplt.xlabel('n_components')\nplt.ylabel('Cumulative_explained_variance')\nplt.show()","4c075d96":"# PCA using Scikit-Learn","2aaea69a":"# PCA for dimensionality redcution (not for visualization)\nDistribution of explained variance for each principal component gives a sense of how much information will be represented and how much lost when the full, 64-dimensional input is reduced using a principal component model (i.e., a model that utilizes only the first N principal components).","7d3006da":"# 2D Visualization using PCA","464848de":"### Display or Plot above label","b16a93fc":"**`projecting the original data sample on the plane formed by multiplication of two principal eigen vectors with transposed sample_data`**","743b2f76":"# Find the co-variance matrix which is : \n                                                            A^T * A`\n* Find covariance matrix of dataset by multiplying the matrix of features by its transpose \n* It is a measure of how much each of the dimensions vary from the mean with respect to each other\n\n\nCovariance is measured between 2 dimensions to see if there is a relationship between the 2 dimensions, e.g., relationship between height and weight of students\n\n* Positive value of covariance indicates that both dimensions are directly proportional to each other, where if one dimension increases the other dimension increases accordingly\n\n* Negative value of covariance indicates that both dimensions are indirectly proportional to each other, where if one dimension increases then other dimension decreases accordingly\n\n* If in case covariance is zero, then the two dimensions are independent of each other","4d415c90":"----\n----\n\n# My Other Notebook:\n* [Simple Linear Regression](https:\/\/www.kaggle.com\/mukeshmanral\/linear-regression-basic)\n* [Multiple Linear Regression](https:\/\/www.kaggle.com\/mukeshmanral\/multiple-linear-regression-basic)\n* [Polynomial Regression](https:\/\/www.kaggle.com\/mukeshmanral\/polynomial-regression-basic)\n* [Advanced Linear Regression](https:\/\/www.kaggle.com\/mukeshmanral\/advance-linear-regression-basic-gridsearchcv-hpt)\n\n----\n* [Feature Engineering 1](https:\/\/www.kaggle.com\/mukeshmanral\/feature-engineering-diff-dataset-1)\n* [Feature Engineering 2](https:\/\/www.kaggle.com\/mukeshmanral\/feature-engineering-diff-dataset-2)\n* [Feature Engineering 3](https:\/\/www.kaggle.com\/mukeshmanral\/feature-engineering-diff-dataset-3)\n* [Feature Engineering 4](https:\/\/www.kaggle.com\/mukeshmanral\/feature-engineering-diff-dataset-4)\n\n----\n\n* [How KNN-Algorith(1) Works (Basic)](https:\/\/www.kaggle.com\/mukeshmanral\/k-nn-algorithm-1-basic)\n* [How KNN-Algorith(2) Works (Basic)](https:\/\/www.kaggle.com\/mukeshmanral\/k-nn-algorithm-2-basic)\n\n____\n\n* [Ensemble-Bagging-Random Forest-Extra Tree Basic](https:\/\/www.kaggle.com\/mukeshmanral\/bagging-ensemble-concept-rf-extree-basic) \n\n____\n____","52f49d12":"### configuring the parameteres","7b260236":"# Data-preprocessing\n### Standardizing data","c961f187":"# PCA is a method that brings together:\n* A measure of how each variable is associated with one another. (Covariance matrix.)\n\n* The directions in which our data are dispersed. (Eigenvectors.)\n\n* The relative importance of these different directions. (Eigenvalues.)\n\n* PCA combines our predictors and allows us to drop the Eigenvectors that are relatively unimportant.\n\n","171a6e7f":"----\n----\n\n# My Other Notebook:\n* [Simple Linear Regression](https:\/\/www.kaggle.com\/mukeshmanral\/linear-regression-basic)\n* [Multiple Linear Regression](https:\/\/www.kaggle.com\/mukeshmanral\/multiple-linear-regression-basic)\n* [Polynomial Regression](https:\/\/www.kaggle.com\/mukeshmanral\/polynomial-regression-basic)\n* [Advanced Linear Regression](https:\/\/www.kaggle.com\/mukeshmanral\/advance-linear-regression-basic-gridsearchcv-hpt)\n\n----\n\n* [Feature Engineering 1](https:\/\/www.kaggle.com\/mukeshmanral\/feature-engineering-diff-dataset-1)\n* [Feature Engineering 2](https:\/\/www.kaggle.com\/mukeshmanral\/feature-engineering-diff-dataset-2)\n* [Feature Engineering 3](https:\/\/www.kaggle.com\/mukeshmanral\/feature-engineering-diff-dataset-3)\n* [Feature Engineering 4](https:\/\/www.kaggle.com\/mukeshmanral\/feature-engineering-diff-dataset-4)\n\n----\n\n* [How KNN-Algorith(1) Works (Basic)](https:\/\/www.kaggle.com\/mukeshmanral\/k-nn-algorithm-1-basic)\n* [How KNN-Algorith(2) Works (Basic)](https:\/\/www.kaggle.com\/mukeshmanral\/k-nn-algorithm-2-basic)\n\n____\n\n* [Ensemble-Bagging-Random Forest-Extra Tree Basic](https:\/\/www.kaggle.com\/mukeshmanral\/bagging-ensemble-concept-rf-extree-basic) \n\n____\n____","94e8c9f1":"# Computing Eigenvectors and Eigenvalues\nEigenvectors and eigenvalues of a covariance (or correlation) matrix represent the \u201ccore\u201d of a PCA: \n* Eigenvectors (principal components) determine the directions of new feature space\n* Eigenvalues determine their magnitude <br>\nIn other words,`eigenvalues explain variance of data along new feature axes`\n\nEigenvectors and Eigenvalues of covariance matrix will give the principal components and a vector that we can use to project high-dimensional inputs to lower-dimensional subspace","662ad531":"**`ploting the 2d data points with seaborn`**","53775100":"From above you can see that if we take 200-dimensions, approx. 90% of variance is expalined.\n\nOur intention with princpal component analysis is to reduce the high-dimensional input to a low-dimensional input \n\nUltimately that low-dimensional input is intended for use in a model, since adding more components increases the cost and the accuracy","3a0e4d61":"# `One can Understand Basic of PCA by this implementation`"}}