{"cell_type":{"ff4f0a6e":"code","207df5dd":"code","ac03ca78":"code","97905222":"code","4ce3c554":"code","641f42d3":"code","03f95fff":"code","5f99750e":"code","02b36944":"code","a717ef88":"code","b087ce41":"code","92238472":"code","55c2d74e":"code","25e3a96e":"code","63ecd357":"code","8adca9fb":"code","fea0f282":"code","70b75b0e":"code","9d945b8a":"code","3286de4d":"code","ec6d6b00":"code","1d7f3b92":"code","c9dc51ae":"code","fd7ea221":"code","ed3d4eee":"code","0cda89af":"code","318bd5af":"code","d490a3dc":"code","8936fc2e":"code","6ad56208":"code","41db443f":"code","92486328":"code","4095df22":"code","9696bce2":"code","f818e803":"code","3ebcad55":"code","38ef6c0f":"code","188a243e":"code","7461fd5e":"code","bd8effb5":"code","57d3ac34":"code","8f877c41":"code","b559aa63":"code","84830db2":"code","2c536834":"code","e19a2a50":"code","1f5c71e8":"code","22ba6505":"code","3659befb":"markdown","c143593e":"markdown","6262f1a1":"markdown","b7da6ed0":"markdown","f8465ce8":"markdown","9fd45b59":"markdown","5f960d9d":"markdown","c7a9334c":"markdown","96d244a3":"markdown","7907cce7":"markdown","e50ef719":"markdown","cd76cd9a":"markdown","674ecd07":"markdown","9021523f":"markdown","4743784f":"markdown","3a09c987":"markdown","89a3cf79":"markdown","b285fc42":"markdown","afd5fa61":"markdown","669676db":"markdown","5a20b463":"markdown","ccb2c3bc":"markdown"},"source":{"ff4f0a6e":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\n\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.utils import to_categorical, Sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.optimizers import RMSprop,Adam\nfrom keras.applications import VGG19, VGG16, ResNet50\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","207df5dd":"path = '\/kaggle\/input\/landmark-recognition-2021\/'\nos.listdir(path)","ac03ca78":"train_data = pd.read_csv(path+'train.csv')\nsamp_subm = pd.read_csv(path+'sample_submission.csv')","97905222":"train_data.head()","4ce3c554":"samp_subm.head()","641f42d3":"def plot_examples(landmark_id=1):\n    \"\"\" Plot 5 examples of images with the same landmark_id \"\"\"\n    \n    fig, axs = plt.subplots(1, 5, figsize=(25, 12))\n    fig.subplots_adjust(hspace = .2, wspace=.2)\n    axs = axs.ravel()\n    for i in range(5):\n        idx = train_data[train_data['landmark_id']==landmark_id].index[i]\n        image_id = train_data.loc[idx, 'id']\n        file = image_id+'.jpg'\n        subpath = '\/'.join([char for char in image_id[0:3]])\n        img = cv2.imread(path+'train\/'+subpath+'\/'+file)\n        axs[i].imshow(img)\n        axs[i].set_title('landmark_id: '+str(landmark_id))\n        axs[i].set_xticklabels([])\n        axs[i].set_yticklabels([])","03f95fff":"print('Samples train:', len(train_data))\nprint('Samples test:', len(samp_subm))","5f99750e":"train_data.head()","02b36944":"len(train_data['landmark_id'].unique())","a717ef88":"samp_subm.head()","b087ce41":"train_data.head()","92238472":"image_id = train_data.loc[0, 'id']\nfile = image_id+'.jpg'\nsubpath = '\/'.join([char for char in image_id[0:3]]) ","55c2d74e":"file","25e3a96e":"subpath","63ecd357":"file in os.listdir(path+'train\/'+subpath)","8adca9fb":"path","fea0f282":"img = cv2.imread(path+'train\/'+subpath+'\/'+file)\nplt.imshow(img)\nplt.show()","70b75b0e":"img.shape","9d945b8a":"plot_examples(landmark_id = 138982)","3286de4d":"plot_examples(landmark_id = 126637)","ec6d6b00":"plot_examples(landmark_id = 83144)","1d7f3b92":"plot_examples(landmark_id = 83145)","c9dc51ae":"idx = train_data[train_data['landmark_id']==83145].index[0]\nprint(idx)\nprint('-------')\nprint(train_data.loc[idx])\nprint('-------')\nimage_id = train_data.loc[idx, 'id']\nimage_id","fd7ea221":"train_data.index[0:3]","ed3d4eee":"list(train_data.index)[:15]","0cda89af":"list_IDs_trainA, list_IDs_valA = train_test_split(list(train_data.index)[:15], test_size=0.33, random_state=2021)\nprint('---- list_IDs_trainA -----')\nprint(list_IDs_trainA)\nprint('---list_IDs_valA-----')\nprint(list_IDs_valA)\nprint('-----------')\nlist_IDs_testA = list(samp_subm.index)[:15]\nprint('---- list_IDs_testA-----')\nprint(list_IDs_testA)\nprint('-----------')","318bd5af":"train_data.iloc[train_data.index[0:3]]","d490a3dc":"#500000\nlist_IDs_train, list_IDs_val = train_test_split(list(train_data.index)[:100000], test_size=0.33, random_state=2021)\nlist_IDs_test = list(samp_subm.index)","8936fc2e":"print('Number train samples:', len(list_IDs_train))\nprint('Number val samples:', len(list_IDs_val))\nprint('Number test samples:', len(list_IDs_test))","6ad56208":"img_size = 32\nimg_channel = 3\nbatch_size = 64\n\nnum_classes = len(train_data['landmark_id'].value_counts())","41db443f":"num_classes","92486328":"class DataGenerator(Sequence):\n    def __init__(self, path, list_IDs, data, img_size, img_channel, batch_size):\n        self.path = path\n        self.list_IDs = list_IDs\n        self.data = data\n        self.img_size = img_size\n        self.img_channel = img_channel\n        self.batch_size = batch_size\n        self.indexes = np.arange(len(self.list_IDs))\n        \n    def __len__(self):\n        len_ = int(len(self.list_IDs)\/self.batch_size)\n        if len_*self.batch_size < len(self.list_IDs):\n            len_ += 1\n        return len_\n    \n    def __getitem__(self, index):\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n        X, y = self.__data_generation(list_IDs_temp)\n        return X, y\n            \n    \n    def __data_generation(self, list_IDs_temp):\n        X = np.zeros((self.batch_size, self.img_size, self.img_size, self.img_channel))\n        y = np.zeros((self.batch_size, 1), dtype=int)\n        for i, ID in enumerate(list_IDs_temp):\n            \n            image_id = self.data.loc[ID, 'id']\n            file = image_id+'.jpg'\n            subpath = '\/'.join([char for char in image_id[0:3]]) \n            \n            img = cv2.imread(self.path+subpath+'\/'+file)\n            \n            img = cv2.resize(img, (self.img_size, self.img_size))\n            X[i, ] = img\/255\n            if self.path.find('train')>=0:\n                y[i, ] = self.data.loc[ID, 'landmark_id']\n            else:\n                y[i, ] = 0\n        return X, y","4095df22":"train_generator = DataGenerator(path+'train\/', list_IDs_train, train_data, img_size, img_channel, batch_size)\nval_generator = DataGenerator(path+'train\/', list_IDs_val, train_data, img_size, img_channel, batch_size)\ntest_generator = DataGenerator(path+'test\/', list_IDs_test, samp_subm, img_size, img_channel, batch_size)","9696bce2":"train_generator.data[:5]","f818e803":"# data_generation(list_IDs_train)\n","3ebcad55":"weights='..\/input\/models\/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\nconv_base = ResNet50(weights=weights,\n                     include_top=False,\n                     input_shape=(img_size, img_size, img_channel))\nconv_base.trainable = True","38ef6c0f":"model = Sequential()\nmodel.add(conv_base)\nmodel.add(Flatten())\n#model.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(num_classes, activation='softmax'))\n\nmodel.compile(optimizer = Adam(lr=1e-4),\n              loss=\"sparse_categorical_crossentropy\",\n              metrics=['sparse_categorical_accuracy'])\n\nmodel.summary()","188a243e":"epochs = 1","7461fd5e":"history = model.fit_generator(generator=train_generator,\n                              validation_data=val_generator,\n                              epochs = epochs, workers=4)","bd8effb5":"print(\"Train done\")","57d3ac34":"# list all data in history\nprint(history.history.keys())","8f877c41":"history.history['loss']","b559aa63":"plt.plot(history.history['loss'])","84830db2":"y_pred = model.predict_generator(test_generator, verbose=1)","2c536834":"y_pred.shape","e19a2a50":"for i in range(len(samp_subm.index)):\n    category = np.argmax(y_pred[i])\n    score = y_pred[i][np.argmax(y_pred[i])].round(2)\n    samp_subm.loc[i, 'landmarks'] = str(category)+' '+str(score)","1f5c71e8":"samp_subm.head()","22ba6505":"samp_subm.to_csv('submission.csv', index=False)","3659befb":"# Path","c143593e":"Look on the image shape:","6262f1a1":"Define Model","b7da6ed0":"# Split Data\nWe define train, validation and test data.","f8465ce8":"There are 81313 unique classes:","9fd45b59":"# Model","5f960d9d":"# Predict Test Data","c7a9334c":"# Libraries\nWe use some standard python packages and the libraries of scikit learn and keras. ","96d244a3":"For each test image, we have to predict one landmark label and a corresponding confidence score. ","7907cce7":"# Intro\nWelcome to the [Google Landmark Recognition 2021](https:\/\/www.kaggle.com\/c\/landmark-recognition-2021) compedition\n![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/29762\/logos\/header.png)\n\nThis notebook will give you a guideline to start step by step with this compedition. We focus on:\n* the underlying structure of the data,\n* a data generator to load the image data on demand during the prediction process.\n\nWe use a simple model with a pretrained model on a subset of the train data to clarify the workflow. Additionally we recommend to use the power of GPU.\n\n\n<span style=\"color: royalblue;\">Please vote the notebook up if it helps you. Feel free to leave a comment above the notebook. Thank you. <\/span>","e50ef719":"# Export","cd76cd9a":"Is the file located in the subpath?","674ecd07":"# Functions","9021523f":"Plot the image:","4743784f":"# DataGenerator","3a09c987":"# Load Data","89a3cf79":"Load pretrained model:","b285fc42":"# Plot Some Examples\nWe plot some examples of images with the same **landmark_id** in a row.","afd5fa61":"# Data Generator\n\nWe use a data generator to load the data on demand.","669676db":"Use the DataGenerator class to define the data generators for train, validation and test data:","5a20b463":"# Find Image\nWe consider the first image of the train data set and plot it. The first 3 characters ares used for the subpath which is the location of the image. ","ccb2c3bc":"# Overview\nFirst we look on the size of the dataset:"}}