{"cell_type":{"02d165fc":"code","7b6f1a30":"code","6dd498f8":"code","5a0d4702":"code","47ce5d0e":"code","8ad3f818":"code","67958a26":"code","8eb4d685":"code","d5fec832":"markdown","53dfb226":"markdown","be62e193":"markdown","633bcc07":"markdown","cf137f35":"markdown","510ba6ef":"markdown","cea8b305":"markdown","42fbcd83":"markdown","d657c1ad":"markdown","7277363a":"markdown","c5741a79":"markdown"},"source":{"02d165fc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","7b6f1a30":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","6dd498f8":"predictions = pd.read_csv(\"..\/input\/compare-errors-made-by-different-models\/error_analysis.csv\")\npredictions.head()","5a0d4702":"sns.heatmap(predictions, cmap = \"pink\", cbar=False,  yticklabels=False)\nplt.title('Comparison of Predictions made by 3 models')\nplt.show()","47ce5d0e":"#sort predictions\npredictions = predictions.sort_values(by=['True Label', 'SVM_prediction', 'XGBoost_prediction', 'MLP_prediction' ])\npredictions.head()","8ad3f818":"#make heatmap after sorting\nsns.heatmap(predictions, cmap = \"pink\", cbar=False,  yticklabels=False)\nplt.title('Comparison of Predictions made by 3 models')\nplt.show()","67958a26":"#Keep only those samples for which at least one of the models makes an incorrect prediction\nerrors = predictions.loc[(predictions['True Label'] != predictions['SVM_prediction']) | \n                         (predictions['True Label'] != predictions['XGBoost_prediction']) | \n                         (predictions['True Label'] != predictions['MLP_prediction'])]","8eb4d685":"sns.heatmap(errors, cmap = \"pink\", cbar=False,  yticklabels=False)\nplt.title('Incorrect predictions comparison for 3 models')\nplt.show()","d5fec832":"I hope this helps you for your own analysis!\n\nIf you have suggestions for improvement, let me know!","53dfb226":"Suppose you are working on a binary classification problem. You trained 3 different models on your data: say SVM,XGBoost and a Multi-Layer Perceptron (MLP). Using a command like `model.predict(x_val)`, you can get predictions made by a each of the models for your validation samples:\n\n```python\nsvm_preds = svm.predict(x_val)\nxgb_preds = xgb.predict(x_val)\nmlp_preds = mlp.predict(x_val)\n```\n\nAlso, you have access to the true labels for the validation data: `y_val`.\n\n**You want to understand how the errors made by the different models compare to each other. Do the models make mistakes on the same samples? Or does each model make mistakes on different samples?**  \n\nTo answer this question, we'll use a simple but beautiful plot. But first, we need some data.\n\nYou can write the errors made by the different models to a DataFrame, like this:\n```python\nimport pandas as pd\n#Create a dictionary, keys are models, values are the predictions.\npredictions = {'True': y_val, \n     'svm_preds': svm_preds,\n     'xgb_preds': xgb_preds,\n     'mlp_preds': mlp_preds}\n#Create df\npredictions = pd.DataFrame.from_dict(d)\n\n#Write df to csv\npredictions.to_csv('error_analysis.csv')\n```\n\nYou can easily create the above DataFrame using your own data and models. \n\n\nFor the purpose of illustrarion, I've included an `error_analysis.csv` file, which we'll use to see how we can visualize the errors made by different models.\n\n\n","be62e193":"The above plot is not very helpful. but if we first sort `predictions`, we can get a much more expressive plot:","633bcc07":"## Import data","cf137f35":"This plot tells us several things:\n1. Different models misclassify different samples. For example, the first block of samples (*black* for first 3 columns, *white* for MLP) is classified correctly by SVM and XGB, but misclassified by MLP.\n2. There are some samples for which all three models misclassify. See the blocks in the middle where true label is *black* (0), but all 3 models predict *white* (1), and vice versa. It's interesting to note that there are a lot more samples with true label *black* (0) which are misclassified by all 3 models, as compared to samples with true label *white* (1) which are misclassifed by all 3 models. Does this mean classifying samples from class *black* (0) in inherently harder than classifying samples from class *white* (1)?","510ba6ef":"This heatmap is much better than the last one. *Black* is for `0`s and *white* is for 1s.\n\nFor most of the samples, all rows are either completely black or completely white, which means we're making **correct predictions**. The models seem to be doing pretty well!\n\nBut for some rows in the middle, we see that the `True label` and model predictions have different colors. These are the **misclassified samples**. Let's take a closer look at the samples which are misclassified by at least one of the models (unmatching colored ones in the middle).","cea8b305":"## Samples misclassified by at least one model","42fbcd83":"## Introduction","d657c1ad":"From above, we see that the `predictions` df contains the true label as well as the predictions made by 3 different models for each given sample. \n\nLet's visualize `predictions` using the `heatmap` function from `sns`.","7277363a":"# Comparing errors made by different models while predicting on same data","c5741a79":"## The importance of sorting"}}