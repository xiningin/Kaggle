{"cell_type":{"78f6d1c6":"code","30b04a29":"code","95d503ad":"code","f392c190":"code","ef5cec44":"code","44fb2cca":"code","db8abaaa":"code","514b9cc2":"code","82163960":"code","bba8cabd":"code","4205bcdb":"code","89ee4326":"code","88c1df88":"code","afab6cd9":"code","33590b4a":"code","c010dd44":"code","9caab320":"code","870bc6cb":"code","aadb3dd7":"code","f082b6d5":"code","b3ed2bf0":"markdown","7d82410b":"markdown"},"source":{"78f6d1c6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","30b04a29":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nimport time\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPool2D, Dropout, BatchNormalization, LeakyReLU\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n\nimport matplotlib.pyplot as plt","95d503ad":"train_data = pd.read_csv('\/kaggle\/input\/Kannada-MNIST\/train.csv')\ntest_data = pd.read_csv(\"\/kaggle\/input\/Kannada-MNIST\/test.csv\")\ndig_data = pd.read_csv(\"\/kaggle\/input\/Kannada-MNIST\/Dig-MNIST.csv\")","f392c190":"print(\"Train set shape = \" +str(train_data.shape))\nprint(\"Test set shape = \" +str(test_data.shape))\nprint(\"Dif set shape = \" +str(dig_data.shape))","ef5cec44":"val_count=train_data.label.value_counts()\nsns.barplot(val_count.index, val_count)","44fb2cca":"#Slicing and Reshaping training images\ndata_train = train_data.iloc[:,1:].values \nx_train = data_train.reshape(data_train.shape[0], 28, 28, 1)\n#Slicing training labels and applying one hot encoding\ntrain_label = train_data.iloc[:,0].values \ny_train = tf.keras.utils.to_categorical(train_label, 10)\nprint(x_train.shape, y_train.shape)","db8abaaa":"#Slicing and Reshaping validation images\ndata_val=dig_data.drop('label',axis=1).iloc[:,:].values\nx_val = data_val.reshape(data_val.shape[0], 28, 28,1)\n#Slicing validation labels and applying one hot encoding\nval_label=dig_data.label\ny_val = tf.keras.utils.to_categorical(val_label, 10) \nprint(x_val.shape, y_val.shape)","514b9cc2":"#procesing  test data\nx_test=test_data.drop('id', axis=1).iloc[:,:].values\nx_test = x_test.reshape(x_test.shape[0], 28, 28,1)\nx_test.shape","82163960":"model = Sequential()\n\nmodel.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation = 'relu', input_shape = (28,28,1)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation = 'relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation = 'relu'))\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation = 'relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv2D(filters = 128, kernel_size = (3,3), padding = 'Same', activation = 'relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(filters = 128, kernel_size = (3,3), padding = 'Same', activation = 'relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.2))\n\nmodel.add(Flatten())\nmodel.add(Dense(64, activation = 'relu'))\nmodel.add(Dense(32, activation = 'relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(10, activation = 'softmax'))","bba8cabd":"model.summary()","4205bcdb":"input_shape = (28, 28, 1)\nlearning_rate=0.001\nbatch_size = 512\nepochs = 280\noptimizer = Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999)","89ee4326":"def lr_decay(epoch):\n    return learning_rate * 0.99 ** epoch","88c1df88":"train_data_generator = ImageDataGenerator(rescale = 1.\/255.,\n                                        rotation_range = 20,\n                                        width_shift_range = 0.1,\n                                        height_shift_range = 0.1,\n                                        shear_range = 0.1,\n                                        zoom_range = [0.2, 1.2],\n                                        horizontal_flip = False)\nval_data_generator = ImageDataGenerator(rescale=1.\/255)","afab6cd9":"model.compile(optimizer=optimizer, loss=['categorical_crossentropy'], metrics=['accuracy'])","33590b4a":"#Featching the start time and tracking it to check if the training hour passed more than 2 hours or not\nstart = time.time()\ntime_spent = 0\naccuracy = []\nval_accuracy = []\nfor epoch in range (220):\n    time_spent = time.time()-start\n    if time_spent >= 7150:  # as training time limit >=2 hours (2 * 60 * 60 = 7200s)\n        break\n    else:\n        epoch += 1\n        print('epoch:', epoch)\n        history = model.fit_generator(\n          train_data_generator.flow(x_train,y_train, batch_size=batch_size),\n          steps_per_epoch=100,\n          epochs=1,\n          callbacks=[LearningRateScheduler(lr_decay)],\n          validation_data=val_data_generator.flow(x_val,y_val),\n          validation_steps=50,  \n          verbose=1, shuffle = True)\n        accuracy.append(history.history['accuracy'])\n        val_accuracy.append(history.history['val_accuracy'])\n\n","c010dd44":"def plot_accuracy(accuracy, val_accuracy):\n    epochs = range(len(accuracy))\n    plt.plot(epochs, accuracy, 'r')\n    plt.plot(epochs, val_accuracy, 'b')\n    plt.title('Training accuracy')\n    plt.legend(['train', 'val'], loc='lower right')\n    plt.grid()\n    plt.show()","9caab320":"plot_accuracy(accuracy, val_accuracy)","870bc6cb":"x_test = x_test\/255\n\npredictions = model.predict_classes(x_test)\nsubmission = pd.read_csv('..\/input\/Kannada-MNIST\/sample_submission.csv')\nsubmission['label'] = predictions\nsubmission.head()","aadb3dd7":"submission.to_csv(\"submission.csv\",index=False)","f082b6d5":"print('end...')","b3ed2bf0":"Here I am using the image generator given by Keras(tensorflow.keras.preprocessing.image.ImageDataGenerator) and I u**sed the Dig-MNIST as my validation data**. I started with training my model for 50 epochs and got an accuracy 0.98120. Then I started to increase the number of epochs to determine how my model performs if I train it for a greater number. I trained it for 100, 150 and 200 epochs and got accuracy 0.98280, 0.98320 and 0.98620  accordingly. As the training accuracy is increased by training more, I am training this for 220 epochs. After this 220 epoch, the accuracy does not improve that much. \nI have tried to keep my number of the parameter as low as I can with a good score.","7d82410b":"** As there is a time limit for 2 hours, I am checking after every epoch whether I am running out of time or not.**"}}