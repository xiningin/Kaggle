{"cell_type":{"08550d9f":"code","5cfedcb3":"code","78ccea8a":"code","36a64194":"code","b2060723":"code","cd6fd930":"code","cd9a634d":"code","eb06e35f":"code","f4d8a5b5":"code","89e20abc":"code","c00d3cc0":"code","58c115ee":"code","9eae3b55":"code","7126c2db":"code","fda06053":"code","561e20ba":"code","8cb75a73":"code","9248e7fa":"code","c2aeb894":"code","4dfedc15":"code","5634d0aa":"code","2047c99a":"code","0d324205":"code","8ea35e18":"code","1a46dbb3":"code","1b44d479":"code","eb4ae62e":"markdown","4faf3411":"markdown","24f9ee05":"markdown","cbb23ba6":"markdown","f423d0cd":"markdown","33907772":"markdown","8f7bd8f5":"markdown"},"source":{"08550d9f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5cfedcb3":"import seaborn as sns\nimport matplotlib.pyplot as plt","78ccea8a":"data=pd.read_csv(\"\/kaggle\/input\/water-potability\/water_potability.csv\")\ndata","36a64194":"data.shape","b2060723":"import missingno as msno\nmsno.bar(data)","cd6fd930":"data.isnull().sum()","cd9a634d":"data.info()","eb06e35f":"data['ph']=data['ph'].fillna(data['ph'].mean())\ndata['Sulfate']=data['Sulfate'].fillna(data['Sulfate'].mean())\ndata['Trihalomethanes']=data['Trihalomethanes'].fillna(data['Trihalomethanes'].mean())","f4d8a5b5":"data","89e20abc":"sns.countplot(x=data[\"Potability\"])","c00d3cc0":"features_num = ['ph', 'Hardness', 'Solids', 'Chloramines', 'Sulfate', \n                'Conductivity', 'Organic_carbon', 'Trihalomethanes',\n                'Turbidity']","58c115ee":"corr = data[features_num].corr()\n\nplt.figure(figsize=(16,6))\nax1 = plt.subplot(1,2,1)\nsns.heatmap(corr, annot=True, cmap='turbo')","9eae3b55":"sns.pairplot(data=data,hue=\"Potability\")","7126c2db":"# plot distribution of numerical features\nfor f in features_num:\n    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10,6), sharex=True)\n    ax1.hist(data[f], bins=30)\n    ax1.grid()\n    ax1.set_title(f)\n    # for boxplot we need to remove the NaNs first\n    feature_wo_nan = data[~np.isnan(data[f])][f]\n    ax2.boxplot(feature_wo_nan, vert=False)\n    ax2.grid()\n    ax2.set_title(f + ' - boxplot')\n    plt.show()","fda06053":"data.columns","561e20ba":"from sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nX =  pd.DataFrame(sc_X.fit_transform(data.drop([\"Potability\"],axis = 1)),\n        columns=['ph', 'Hardness', 'Solids', 'Chloramines', 'Sulfate', 'Conductivity',\n       'Organic_carbon', 'Trihalomethanes', 'Turbidity'])","8cb75a73":"y=data[\"Potability\"]","9248e7fa":"from sklearn.model_selection import train_test_split                #splitting the dataset\n                                                                 \nX_train,X_test,Y_train,Y_test = train_test_split(X,y,test_size=.50,random_state=3)","c2aeb894":"from sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nlog_reg = LogisticRegression()\nlog_reg.fit(X_train,Y_train)\n\nlog_acc=accuracy_score(Y_test,log_reg.predict(X_test))\n\n\nprint(\"Train Set Accuracy:\"+str(accuracy_score(Y_train,log_reg.predict(X_train))*100))\nprint(\"Test Set Accuracy:\"+str(accuracy_score(Y_test,log_reg.predict(X_test))*100))","4dfedc15":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=9)                #knn classifier\nknn.fit(X_train,Y_train)\n\nknn_acc = accuracy_score(Y_test,knn.predict(X_test))\n\n\nprint(\"Train Set Accuracy:\"+str(accuracy_score(Y_train,knn.predict(X_train))*100))\nprint(\"Test Set Accuracy:\"+str(accuracy_score(Y_test,knn.predict(X_test))*100))","5634d0aa":"from sklearn.svm import SVC\n\nsvm = SVC()\nsvm.fit(X_train,Y_train)    \n\nsvm_acc= accuracy_score(Y_test,svm.predict(X_test))\n\n\nprint(\"Train Set Accuracy:\"+str(accuracy_score(Y_train,svm.predict(X_train))*100))\nprint(\"Test Set Accuracy:\"+str(accuracy_score(Y_test,svm.predict(X_test))*100))","2047c99a":"from sklearn.tree import DecisionTreeClassifier\n\ndtc = DecisionTreeClassifier(criterion='entropy',max_depth=5)\ndtc.fit(X_train, Y_train)\n\n\ndtc_acc= accuracy_score(Y_test,dtc.predict(X_test))\n\nprint(\"Train Set Accuracy:\"+str(accuracy_score(Y_train,dtc.predict(X_train))*100))\nprint(\"Test Set Accuracy:\"+str(accuracy_score(Y_test,dtc.predict(X_test))*100))","0d324205":"from sklearn.ensemble import GradientBoostingClassifier\n\ngbc = GradientBoostingClassifier()\ngbc.fit(X_train, Y_train)\n\n\ngbc_acc=accuracy_score(Y_test,gbc.predict(X_test))\n\nprint(\"Train Set Accuracy:\"+str(accuracy_score(Y_train,gbc.predict(X_train))*100))\nprint(\"Test Set Accuracy:\"+str(accuracy_score(Y_test,gbc.predict(X_test))*100))","8ea35e18":"from xgboost import XGBClassifier\n\nxgb = XGBClassifier(booster = 'gbtree', learning_rate = 0.1, max_depth=6,n_estimators = 10)\nxgb.fit(X_train,Y_train)\n\nxgb_acc= accuracy_score(Y_test,xgb.predict(X_test))\n\nprint(\"Train Set Accuracy:\"+str(accuracy_score(Y_train,xgb.predict(X_train))*100))\nprint(\"Test Set Accuracy:\"+str(accuracy_score(Y_test,xgb.predict(X_test))*100))","1a46dbb3":"models = pd.DataFrame({\n    'Model': ['Logistic','KNN', 'SVC',  'Decision Tree Classifier',\n             'Gradient Boosting Classifier',  'XgBoost'],\n    'Score': [ log_acc,knn_acc, svm_acc, dtc_acc, gbc_acc, xgb_acc]\n})\n\nmodels.sort_values(by = 'Score', ascending = False)","1b44d479":"colors = [\"purple\", \"green\", \"orange\", \"magenta\",\"blue\",\"black\"]\n\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(16,8))\nplt.ylabel(\"Accuracy %\")\nplt.xlabel(\"Algorithms\")\nsns.barplot(x=models['Model'],y=models['Score'], palette=colors )\nplt.show()","eb4ae62e":"# 3. Support Vector Classifer","4faf3411":"# 4. Decision Tree Classifier","24f9ee05":"# 5. XGB CLASSIFIER","cbb23ba6":"**Now I am going to Calculate accuray score of the split data using Various Models**","f423d0cd":"# 1. LOGISTIC REGRESSION","33907772":"# 2. K NEIGHBORS CLASSIFIER","8f7bd8f5":"# Comparing their results"}}