{"cell_type":{"18227163":"code","d48fe90d":"code","666272b1":"code","dfe19dc4":"code","89fde05d":"code","d5f4d1f9":"code","8ca93658":"code","872f898c":"code","bebf4204":"code","71b6f512":"code","00111c0a":"code","3ffb4c85":"code","511c8814":"code","7ac9ed18":"code","45c4ab1a":"code","5f109d5e":"code","cc84ed27":"code","df3e1a0f":"code","e1fb2f25":"code","950730bd":"code","726c4ce8":"code","fbd64cc5":"code","1869b6bb":"code","cbb6050b":"code","48550b9c":"code","85c5f354":"code","9620844d":"code","ff575616":"code","25f2e47a":"code","7f95280f":"code","701dace9":"code","3fe1f8f6":"code","cda7837a":"code","917b0b3e":"code","b83e9bea":"code","88ae902d":"code","97579439":"code","8a9e9e41":"code","0aabdaaa":"code","011c0c7a":"code","f4fb3a92":"code","715e3e2d":"code","68a825aa":"code","1251429d":"code","133faac1":"code","01c80236":"code","15a18a17":"code","0019eca0":"code","d05b9186":"code","f343897b":"code","295e4de7":"code","03fcd4f8":"code","30ec4079":"code","91b2a3c6":"code","23c90fc2":"code","434c515d":"code","fc002a40":"code","96df862d":"code","1027ebb9":"code","2509e839":"code","da8037ab":"code","6ba44409":"code","57a99962":"code","cb381c3a":"code","7570de53":"code","10b2a704":"code","4340737f":"code","2255bb65":"code","5838af9a":"code","d3b49ca3":"code","fb266e96":"code","afd0e532":"code","9d1c5931":"code","4beb19ca":"code","d40199b4":"code","05f1dbfd":"code","6256be90":"code","101a78ac":"code","44be9091":"code","a4cfaacf":"code","5910a2da":"code","453597ed":"code","d740f7f8":"code","23e32a5f":"code","7f5ae19d":"code","9e64ae4a":"code","cea8124c":"code","2a2942fa":"code","3a576f09":"code","9461e103":"code","4faf1ddc":"code","2e21a372":"code","e186d852":"code","62374f36":"code","55ca88e1":"markdown","bb2f8bb8":"markdown","4b8cc19a":"markdown","676c8872":"markdown"},"source":{"18227163":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\nimport sklearn","d48fe90d":"print(sklearn.__version__)\nprint(np.__version__)\nprint(pd.__version__)","666272b1":"### importing the automobile data\ndf = pd.read_csv(\"..\/input\/data-for-reg\/auto-mpg.csv\")","dfe19dc4":"df.head()","89fde05d":"df.sample(10)","d5f4d1f9":"df.shape","8ca93658":"df.isnull().sum()","872f898c":"##replace the missing \"?\" with the np.nan value\ndf = df.replace('?',np.nan)\ndf.isna().sum()","bebf4204":"df.isnull().sum()\n","71b6f512":"df = df.dropna()","00111c0a":"df.head()","3ffb4c85":"df.isna().sum()","511c8814":"## origin and car name has no predictive powed on mpg\ndf.columns","7ac9ed18":"df = df[['mpg', 'cylinders', 'displacement', 'horsepower', 'weight',\n       'acceleration', 'model year']]","45c4ab1a":"df.head()","5f109d5e":"## we make the model year with the age of the car\n## prepending 19 so we got the full year\ndf['model year'] = '19'+df['model year'].astype(str)","cc84ed27":"df.head()","df3e1a0f":"## now making  an age column based on the model year\ncurrent_year         = datetime.datetime.now().year\ncar_manufacture_year = pd.to_numeric(df['model year'])\ndf['age'] = current_year - car_manufacture_year","e1fb2f25":"df.head()","950730bd":"df = df[['mpg', 'cylinders', 'displacement', 'horsepower', 'weight',\n       'acceleration', 'age']]","726c4ce8":"df.head()","fbd64cc5":"df.dtypes","1869b6bb":"## so there is one object field make it numerical\ndf['horsepower'] = pd.to_numeric(df['horsepower'])","cbb6050b":"df.dtypes","48550b9c":"df.head()","85c5f354":"## now we got all the neumeric value\n## check the relationship between the age and mpg\nplt.scatter(df['age'],df['mpg'])\nplt.xlabel('age')\nplt.ylabel('mpg')","9620844d":"## so we can see a downward trend that means more the year less \n## mgg and thats ovious during this time engine become advance","ff575616":"plt.scatter(df['horsepower'],df['mpg'])\nplt.xlabel('howrsepower')\nplt.ylabel('mpg')","25f2e47a":"plt.scatter(df['weight'],df['mpg'])\nplt.xlabel('weight')\nplt.ylabel('mpg')","7f95280f":"plt.scatter(df['acceleration'],df['mpg'])\nplt.xlabel('acceleration')\nplt.ylabel('mpg')","701dace9":"df.corr()['mpg'].plot(kind='bar')","3fe1f8f6":"df.corr()['mpg'].plot()","cda7837a":"df.corr()","917b0b3e":"sns.heatmap(df.corr(),annot=True)","b83e9bea":"df = df.sample(frac=1).reset_index(drop=True)","88ae902d":"df.head()","97579439":"### risk in multiple  regression\n## building simple linear regression\ndata = df.copy()","8a9e9e41":"from sklearn.model_selection import train_test_split\nX = data[['horsepower']]\ny = data[['mpg']]","0aabdaaa":"x_train,x_test,y_train,y_test = train_test_split(X,y)","011c0c7a":"from sklearn.linear_model import LinearRegression\nlinear_model = LinearRegression(normalize=True).fit(np.array(x_train),np.array(y_train))","f4fb3a92":"print(\"Train Score : {}\".format(linear_model.score(x_train,y_train)))\nprint(\"Test  Score : {}\".format(linear_model.score(x_test,y_test)))","715e3e2d":"y_pred = linear_model.predict(x_test)","68a825aa":"from sklearn.metrics import r2_score\nprint(\"Testing Score : {}\".format(r2_score(y_test,y_pred)))","1251429d":"plt.scatter(x_test,y_test)\nplt.plot(x_test,y_pred,color='r')\nplt.xlabel(\"Horsepower\")\nplt.ylabel('mpg')\n","133faac1":"from sklearn.model_selection import train_test_split\nX = data[['age']]\ny = data[['mpg']]\nx_train,x_test,y_train,y_test = train_test_split(X,y)\nfrom sklearn.linear_model import LinearRegression\nlinear_model = LinearRegression(normalize=True).fit(np.array(x_train),np.array(y_train))\nprint(\"Train Score : {}\".format(linear_model.score(x_train,y_train)))\nprint(\"Test  Score : {}\".format(linear_model.score(x_test,y_test)))\ny_pred = linear_model.predict(x_test)\nfrom sklearn.metrics import r2_score\nprint(\"Testing Score : {}\".format(r2_score(y_test,y_pred)))\nplt.scatter(x_test,y_test)\nplt.plot(x_test,y_pred,color='r')\nplt.xlabel(\"age\")\nplt.ylabel('mpg')\n","01c80236":"from sklearn.model_selection import train_test_split\nX = data[['acceleration']]\ny = data[['mpg']]\nx_train,x_test,y_train,y_test = train_test_split(X,y)\nfrom sklearn.linear_model import LinearRegression\nlinear_model = LinearRegression(normalize=True).fit(np.array(x_train),np.array(y_train))\nprint(\"Train Score : {}\".format(linear_model.score(x_train,y_train)))\nprint(\"Test  Score : {}\".format(linear_model.score(x_test,y_test)))\ny_pred = linear_model.predict(x_test)\nfrom sklearn.metrics import r2_score\nprint(\"Testing Score : {}\".format(r2_score(y_test,y_pred)))\nplt.scatter(x_test,y_test)\nplt.plot(x_test,y_pred,color='r')\nplt.xlabel(\"age\")\nplt.ylabel('mpg')\n","15a18a17":"from sklearn.model_selection import train_test_split\nX = data[['displacement','horsepower','weight']]\ny = data[['mpg']]\nx_train,x_test,y_train,y_test = train_test_split(X,y)\nfrom sklearn.linear_model import LinearRegression\nlinear_model = LinearRegression(normalize=True).fit(np.array(x_train),np.array(y_train))\nprint(\"Train Score : {}\".format(linear_model.score(x_train,y_train)))\nprint(\"Test  Score : {}\".format(linear_model.score(x_test,y_test)))\ny_pred = linear_model.predict(x_test)\nfrom sklearn.metrics import r2_score\nprint(\"Testing Score : {}\".format(r2_score(y_test,y_pred)))\n# plt.scatter(x_test,y_test)\n# plt.plot(x_test,y_pred,color='r')\n# plt.xlabel(\"age\")\n# plt.ylabel('mpg')","0019eca0":"plt.scatter(x_test['displacement'],y_test)\nplt.scatter(x_test['horsepower'],y_test)\nplt.scatter(x_test['weight'],y_test)\nplt.plot(x_test['displacement'],y_pred,color='r')\nplt.plot(x_test['horsepower'],y_pred,color='b')\nplt.plot(x_test['weight'],y_pred,color='g')\n","d05b9186":"## find the coefficient of the attributes we use\npredictors = x_train.columns","f343897b":"coff = pd.Series(linear_model.coef_[0],predictors).sort_values()","295e4de7":"coff","03fcd4f8":"y_pred = linear_model.predict(x_test)","30ec4079":"actual = []\nfor item in y_pred:\n    actual.append(item[0])","91b2a3c6":"y_test['predicted'] = np.array(actual)","23c90fc2":"r2_score(y_test['mpg'],y_test['predicted'])","434c515d":"df = pd.read_csv(\"..\/input\/data-for-reg\/exams.csv\")","fc002a40":"df.head()","96df862d":"df['gender'].unique()","1027ebb9":"d1= {'male':0,'female':1}\ndf['gender'] = df['gender'].map(d1)\n","2509e839":"df.head()","da8037ab":"df['race\/ethnicity'].unique()","6ba44409":"d2 = {'group E':0, 'group C':1, 'group B':2, 'group D':3, 'group A':4}","57a99962":"df['race\/ethnicity'] = df['race\/ethnicity'].map(d2)","cb381c3a":"df.head()","7570de53":"df['parental level of education'].unique()","10b2a704":"d3 = {\"associate's degree\":0, 'some college':1, 'high school':2,\n       \"bachelor's degree\":3, 'some high school':4, \"master's degree\":5}","4340737f":"df['parental level of education'] = df['parental level of education'].map(d3)","2255bb65":"df.head()","5838af9a":"print(df['lunch'].unique())\nprint(df['test preparation course'].unique())","d3b49ca3":"d4 = {'standard':0 ,'free\/reduced':1}\nd5 = {'none':0, 'completed':1}\ndf['lunch'] = df['lunch'].map(d4)\ndf['test preparation course'] = df['test preparation course'].map(d5)","fb266e96":"df.dtypes","afd0e532":"df.sample(10)","9d1c5931":"X = df.drop('math score',axis=1)\ny = df[['math score']]","4beb19ca":"from sklearn.model_selection import train_test_split","d40199b4":"x_train,x_test,y_train,y_test = train_test_split(X,y)","05f1dbfd":"print(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","6256be90":"linear_model = LinearRegression(fit_intercept=True).fit(x_train,y_train)","101a78ac":"linear_model.score(x_test,y_test)","44be9091":"from sklearn.metrics import r2_score","a4cfaacf":"r2_score(y_test,linear_model.predict(x_test))","5910a2da":"plt.plot(range(len(y_test)),y_test)\nplt.plot(range(len(y_test)),linear_model.predict(x_test))","453597ed":"### import all kinds of linear regression\nfrom sklearn.linear_model import LinearRegression,Lasso,Ridge,ElasticNet,Lars,SGDRegressor","d740f7f8":"from sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor","23e32a5f":"def model_fc(model,X_train,y_train):\n    model.fit(X,y)\n    return model\n\ndef build_model(model,X_test,y_test):\n    model = model\n    print(\"MODEL ACCURACY : {}\".format(model.score(X_test,y_test)));\n    y_pred = model.predict(X_test)\n    plt.plot(range(len(y_pred)),y_pred)\n    plt.plot(range(len(y_test)),y_test)\n    plt.show()\n    ","7f5ae19d":"lr = LinearRegression()\nlas = Lasso()\nri = Ridge()\nel = ElasticNet()\nla = Lars()\nsg = SGDRegressor()\nsv = SVR()\nkn = KNeighborsRegressor()\ndc = DecisionTreeRegressor()\nra = RandomForestRegressor()","9e64ae4a":"models  = { lr : 'LinearRegression',\nlas : 'Lasso',\nri : 'Ridge',\nel : 'ElasticNet',\nla : 'Lars',\nsg : 'SGDRegressor',\nsv : 'SVR',\nkn : 'KNeighborsRegressor',\ndc : 'DecisionTreeRegressor',\nra : 'RandomForestRegressor'} ","cea8124c":"models","2a2942fa":"for model in models:\n    model = model_fc(model,x_train,y_train)\n    print(str(model))\n    build_model(model,x_test,y_test)","3a576f09":"## Hyparameter tuning\nfrom sklearn.model_selection  import GridSearchCV\nparameters = {'alpha':[.2,.4,.6,.7,.8,.9,1]}\ngrid_search = GridSearchCV(las,parameters,cv=5,return_train_score=True)","9461e103":"grid_search.fit(x_train,y_train)","4faf1ddc":"grid_search.best_params_","2e21a372":"d = grid_search.best_estimator_","e186d852":"d.fit(x_train,y_train)","62374f36":"d.score(x_test,y_test)","55ca88e1":"### Regularaization (adding additional penalty) to deal with the overfitted data elastic regression is the combination of the both they are different based on their penalty score\n \n","bb2f8bb8":"# Simpl regression with categorical value","4b8cc19a":"## choosing the right algorithm for the data\n\n\n1. If you have a few data < 100k and few feature then you choose\n        *) lasso  \n        *) Elastic Net regression\n2. if you have data <100k and lots of feature you choose\n        *) Ridge Regression \n        *) SVR(kernel='linear')\n   and if does not woek you can also use\n        *) SVR(kernel=\"rbf\")\n        *) Ensamble Regression\n3. if more than 100K then use \n        * SGD model","676c8872":"## the penalty of the regression algorithm\n1. lasso regression give penalty\n    * mse + alpha(|A|+|B|) where alpha is the hyperparameter\n2. rdige regression give penalty\n    * mse +  alpha(|A|^2+|B|^2)\n3. Elast tic net is the\n    * mse + alpha(|A|+|B|) + alpha(|A|^2+|B|^2)"}}