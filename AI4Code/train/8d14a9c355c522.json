{"cell_type":{"840d3b4c":"code","1424fde4":"code","e6f0af84":"code","4ed18b5c":"code","4fc80231":"code","58e666f2":"code","0ecdc314":"code","121de43a":"code","536632a5":"code","0f0a5784":"code","fd68ebf8":"code","99eb4829":"code","3044d740":"code","4232bfec":"code","06411ab8":"code","50f43dbb":"code","0831798f":"code","a38a1115":"code","29414c51":"code","cc1d13f6":"code","030d61b6":"markdown","a17d61d0":"markdown","00ae968f":"markdown","81fa7cef":"markdown","b9220435":"markdown","08dc2d4f":"markdown","76598d3e":"markdown","56eed742":"markdown","be221e09":"markdown","52d89c0b":"markdown","157e4cc6":"markdown","5be0a172":"markdown","b9339070":"markdown","11879c20":"markdown","fe7bad5e":"markdown","d97bf6b5":"markdown","2997ba36":"markdown"},"source":{"840d3b4c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\nplt.rcParams['figure.figsize'] = (20,5)  #defualt figure size\n\ninput_path = \"..\/input\/m5-forecasting-accuracy\/\"\n\nsell_prices = pd.read_csv(input_path+'sell_prices.csv')\ncalendar = pd.read_csv(input_path+'calendar.csv')\nsales = pd.read_csv(input_path+'sales_train_validation.csv')\nsample_output = pd.read_csv(input_path+'sample_submission.csv')\n#print(sell_prices.head())\n#print(calendar.head())\n#print(sales.head())\n#print(sample_output.head())","1424fde4":"#Sales Catogery\nsales.groupby('cat_id').count()['id'].sort_values().plot(kind='barh',figsize=(15,2), title='Sales By Catogory',width=0.5)\nplt.show()","e6f0af84":"#Sales By Department\nsales.groupby('dept_id').count()['id'].sort_values().plot(kind='barh',figsize=(15,3), title='Sales By Department')\nplt.show()","4ed18b5c":"#Sales By State\nsales.groupby('state_id').count()['id'].sort_values().plot(kind='barh',figsize=(15,2), title='Sales By State')\nplt.show()","4fc80231":"date_columns = [c for c in sales.columns if 'd_' in c] # select date columns\ngouped_by_cat_totals = sales.groupby(['cat_id']).sum().T  #get sum and trasnpose\n#print(gouped_by_cat_totals.columns)\ngouped_by_cat_totals.plot(figsize=(20,5),title=\"Total Sales By Catogory\")\ncal_columns = ['d','month']\nmonthPosition = np.arange(3,2000,30) #Roughly\nfor xc in monthPosition:\n    plt.axvline(x=xc, color='k', linestyle='--')\nplt.show()","58e666f2":"cal_columns = ['date','d','month','year','wday','event_type_1','event_type_2']\ncalendar_selected = calendar[cal_columns].set_index('d')\ntotal_sales_OverCalendar = pd.concat([calendar_selected,gouped_by_cat_totals],axis=1,sort=False)\nprint(total_sales_OverCalendar['event_type_1'].unique())\nprint(total_sales_OverCalendar['event_type_2'].unique())\ntotal_sales_OverCalendar.head()","0ecdc314":"total_sales_OverCalendar['dayOfYear'] = total_sales_OverCalendar['date'].str.slice(5,10)\ndef plot_pivoted_year(data,name,num):\n    plt.figure(figsize=(20,10))\n    plt.subplot(3,1,num)\n    plt.title(name+\" Sales Over ther Year\")\n    pivoted = data.pivot_table(index='dayOfYear',columns='year',values=name)\n    plt.grid()\n    plt.plot(pivoted)\n    plt.legend()\n    plt.show()\n    \nplot_pivoted_year(total_sales_OverCalendar,'FOODS',1)\nplot_pivoted_year(total_sales_OverCalendar,'HOBBIES',2)\nplot_pivoted_year(total_sales_OverCalendar,'HOUSEHOLD',3)","121de43a":"#Yearly growth\ngouped_yearly = total_sales_OverCalendar.groupby('year')['FOODS','HOBBIES','HOUSEHOLD'].mean().T\ngouped_yearly.plot(kind='bar',title='Total Average Sales by year',figsize=(10,5))\nplt.show()","536632a5":"total_sales_OverCalendar['dayOfMonth'] = total_sales_OverCalendar['date'].str.slice(8,10)\ndef plot_pivoted_month(data,name,num):\n    plt.figure(figsize=(20,10))\n    plt.subplot(3,1,num)\n    plt.title(name+\" Sales Over Month\")\n    pivoted = data.pivot_table(index='dayOfMonth',columns='month',values=name)\n    plt.grid()\n    plt.plot(pivoted)\n    plt.legend()\n    plt.show()\n    \nplot_pivoted_month(total_sales_OverCalendar,'FOODS',1)\nplot_pivoted_month(total_sales_OverCalendar,'HOBBIES',2)\nplot_pivoted_month(total_sales_OverCalendar,'HOUSEHOLD',3)","0f0a5784":"#Monthly growth\ngouped_monthly = total_sales_OverCalendar.groupby('month')['FOODS','HOBBIES','HOUSEHOLD'].mean().T\ngouped_monthly.plot(kind='bar',title='Total Average Sales by Month',figsize=(10,5))\nplt.legend(loc='best')\nplt.show()","fd68ebf8":"def plotSalesAndEvents(data,eventData,col_name):\n    eventData.plot(kind='bar',figsize=(20,5),title='Event count vs Sales('+col_name+')',stacked=True)\n    data[col_name].plot(secondary_y=True,figsize=(20,5),linewidth=4)\n    plt.grid()\n    plt.show()\n    \ndef getEventData(data,eventType):\n    eventData = data[data[eventType].notnull()].pivot_table(index='month',columns=eventType,values='wday',aggfunc=len)\n    eventData = eventData.fillna(0)\n    eventData = eventData.reset_index('month')\n    eventData = eventData.set_index('month')\n    return eventData\n    \n#Chose complete year data. 2011 and 2016 we don;t have whole year data.\ncomplete_year_data = total_sales_OverCalendar[ (total_sales_OverCalendar['year']>2011) & (total_sales_OverCalendar['year']<2016)]\ngouped_monthly = complete_year_data.groupby('month')['FOODS','HOBBIES','HOUSEHOLD'].mean()\ngouped_monthly = gouped_monthly.reset_index('month')\ndata_2012 = total_sales_OverCalendar[total_sales_OverCalendar['year']==2012]\neventData = getEventData(data_2012,'event_type_1')\nplotSalesAndEvents(gouped_monthly,eventData,'FOODS')\nplotSalesAndEvents(gouped_monthly,eventData,'HOBBIES')\nplotSalesAndEvents(gouped_monthly,eventData,'HOUSEHOLD')","99eb4829":"def plot_timeseries_stat(timeseries):\n    rollingMean = timeseries.rolling(window=30,center=False).mean()\n    rollingStd = timeseries.rolling(window=30,center=False).std()\n    plt.figure(figsize=(20,8))\n    ori = plt.plot(timeseries,color='blue',label='Original')\n    mean = plt.plot(rollingMean,color='red',label='Rolling Mean')\n    std = plt.plot(rollingStd,color='black',label='Rolling Std')\n    plt.legend(loc='best')\n    plt.show(block=False)\n\nplot_timeseries_stat(total_sales_OverCalendar['FOODS'])\nplot_timeseries_stat(total_sales_OverCalendar['HOUSEHOLD'])\nplot_timeseries_stat(total_sales_OverCalendar['HOBBIES'])","3044d740":"#Making time series stationary\nfrom statsmodels.tsa.stattools import adfuller, acf, pacf\nfrom statsmodels.tsa.arima_model import ARIMA\nimport math\n\ndef test_stationarityDF(timeseries): ##Dickey-Fuller Test\n    dftest = adfuller(timeseries,autolag='AIC')\n    dfoutput=pd.Series(dftest[0:4],index=['Test Statistic','p-value','#Lags Used','No of Observesations Used'])\n    \n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print(dfoutput)\n\nfood_Series = total_sales_OverCalendar['FOODS']\nfood_Series.fillna(food_Series.mean(),inplace=True)\ntest_stationarityDF(food_Series)\n\nmovingAverage = food_Series.rolling(window=30).mean()\nmovingSTD = food_Series.rolling(window=30).std()\nplt.figure(figsize=(20,6))\nplt.plot(food_Series)\nplt.plot(movingAverage,color='red')\nplt.plot(movingSTD,color='black')\nplt.show()","4232bfec":"#Make stationarry\nfoodSeriesDiff = food_Series-movingAverage\nplt.figure(figsize=(20,5))\nplt.plot(foodSeriesDiff)\nplt.show()\npd.set_option('display.float_format', '{:.5f}'.format)\nfoodSeriesDiff.fillna(foodSeriesDiff.mean(),inplace=True)\ntest_stationarityDF(foodSeriesDiff)","06411ab8":"plt.plot(np.arange(0,31,1),acf(foodSeriesDiff,nlags=30))\nplt.axhline(y=0,linestyle='--',color='gray')\nplt.axhline(y=-7.96\/np.sqrt(len(foodSeriesDiff)),linestyle='--',color='gray')\nplt.axhline(y=7.96\/np.sqrt(len(foodSeriesDiff)),linestyle='--',color='gray')\nplt.title('Autocorrelation Function')\nplt.grid()\nplt.show()","50f43dbb":"plt.plot(np.arange(0,31,1),pacf(foodSeriesDiff,nlags=30))\nplt.axhline(y=0,linestyle='--',color='gray')\nplt.axhline(y=-7.96\/np.sqrt(len(foodSeriesDiff)),linestyle='--',color='gray')\nplt.axhline(y=7.96\/np.sqrt(len(foodSeriesDiff)),linestyle='--',color='gray')\nplt.title('Partial Autocorrelation Function')\nplt.grid()\nplt.show()","0831798f":"model = ARIMA(foodSeriesDiff,order=(2,2,0))\nresults_ARIMA = model.fit(disp=-1)\nplt.figure(figsize=(20,5))\nplt.plot(foodSeriesDiff)\nplt.plot(results_ARIMA.fittedvalues,color='red')\nplt.show()","a38a1115":"results_ARIMA.plot_predict('d_1800','d_1900',dynamic=False,ax=ax)","29414c51":"#Plot a portion of data to clear visulization\nplt.figure(figsize=(20,5))\nori = plt.plot(foodSeriesDiff.iloc[30:90],label='Original Diff')\n#plt.plot(results_ARIMA.fittedvalues.iloc[0:60],color='red')\n#plt.plot(results_ARIMA.fittedvalues.iloc[0:60].cumsum(),color='black')\n##ARIMA order is 2. show results lags 2 values\n#shfited['predicShfited2'] = pd.Series(results_ARIMA.fittedvalues,copy=True)\nshfited = pd.DataFrame({'predicShfited2':pd.Series(results_ARIMA.fittedvalues,copy=True),'day':foodSeriesDiff.index[0:1967]})\nshfited = shfited.set_index('day')\npre = plt.plot(shfited['predicShfited2'].iloc[30:90],color='green',label='Predicted Diff')\n#pre = plt.plot(results_ARIMA.fittedvalues.iloc[0:60],color='green',label='Predicted Diff')\nplt.legend(loc='best')\nplt.grid()\nplt.show()","cc1d13f6":"predictions_ARIMA_final = pd.Series(food_Series.at['d_2'],index=food_Series.index)\nshfited.loc['d_1'] = 0\nshfited.loc['d_2'] = 0\nmovingAverage.fillna(0)\npredictVsActual = pd.DataFrame({'actual':food_Series,'diffMean':foodSeriesDiff,\n                                'predictDiffOri':shfited['predicShfited2'],\n                                'predictDiff':shfited['predicShfited2'],\n                                'base':movingAverage})\npredictVsActual['predict'] = predictVsActual.loc[:,['predictDiff','base']].sum(axis=1)\npredictVsActual['error'] = predictVsActual['actual'] - predictVsActual['predict']\nplt.figure(figsize=(20,5))\nplt.plot(predictVsActual['actual'].iloc[1800:1950],label='Actual')\nplt.plot(predictVsActual['predict'].iloc[1800:1950],label='Predicted')\n#plt.plot(predictVsActual[['actual','predict']])\nplt.legend(loc='best')\nplt.title('Original vs predicted. RMSE: %4f'%np.sqrt(sum(predictVsActual['error']**2)\/len(predictVsActual)))\nplt.show()","030d61b6":"Correlation function cross upper confident value between 1 and 2. Hence chose 2 as p for ARIMA","a17d61d0":"### Monthly trend analysis","00ae968f":"#### Prediction","81fa7cef":"### *Please help by upvoting this kernel to keep motivate me. *","b9220435":"We can observe that Test Statistic is way less than 1% of critical value. So, we can conculde that the above Series is 99% stationary.","08dc2d4f":"### Time Series Data Analysis\n\nTo get an idea of the modeling, we will try to pridict result using Time series.\nHere, I'm tring to predict total sales for food catogory. ","76598d3e":"### Feature Selection for Complex Models\n\n\nWill update this kernel on below section on upcomming days.\n\n1) Feature Selection\n\n2) Various models\n\n3) Evaluate Results","56eed742":"Merge the Calandar data with the sales data","be221e09":"Data discription we can find in the data section of the problem statement, \nSo, we are not analyzing data again. As per the data discription only event related feilds having missing data.\nTo start with, we will use item catogery visulizations. That will be an effective representaton of the whole dataset.","52d89c0b":"We can clearly see yearly similar trend on each catogory. We can see total sales growth in each year.\n2016 we have only half of the years data","157e4cc6":"Partial Autocorrelation function drop to 0 when value is between 1 and 2. choose 2 as q value ","5be0a172":"We can observe that Test Statistic is not below the 1% of the Critical value. So,Series is not stationary.","b9339070":"### Exploratory Data Analysis\n\nFeature Enginering is a crucial part in machine learning.\nBefore start predicting we need to analyze the trend and select features based on the observations.\nFor that purpose we will start with Exploratory data analysis.","11879c20":"Events can affect sales. Above graphs we can see less number of event seasons people tend to buy more FOOD and household items. People tend to buy HOBIIES catogry when their are more events.\n\nWe can clearly see that month 6 having highest sales on HOBIIES catogory where we have highest number of Sporting events for the same month.\nMonth 8 having highes FOOD and HOUSEHOLD sales where lowest events occur for the same month","fe7bad5e":"ARIMA model is based on time series values. But we have more features than the time shift effect.\nAs a example, events, day of week, month can be considered as few more additional features.","d97bf6b5":"### Modeling start with ARIMA","2997ba36":"Events more like to be annual. We will analyze event effect on Sales on following section "}}