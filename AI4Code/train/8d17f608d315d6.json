{"cell_type":{"aa963aff":"code","6256c895":"code","93fe3e15":"code","155ed014":"code","6f01db19":"code","61197f81":"code","25b972c0":"markdown","5906b56c":"markdown","249774c6":"markdown","82ac16d1":"markdown","87c8ab49":"markdown","673804b7":"markdown","a4d01124":"markdown","47fe332f":"markdown","fe8ceb4c":"markdown","fe717903":"markdown","16c22d08":"markdown","92e3e9ed":"markdown","77560005":"markdown"},"source":{"aa963aff":"import numpy as np\nimport gym               #Gym is our environment\nimport random\nimport time\nfrom IPython.display import clear_output","6256c895":"\"\"\"First we will make our environment.There are many other environment.\nYou can visit https:\/\/gym.openai.com\/docs\/#environments for many game interface. \"\"\" \n    \nenv = gym.make(\"FrozenLake-v0\")     ","93fe3e15":"#Initializing the parameters on enviroment\naction_space_size = env.action_space.n          #Defining our action space and state space.\nstate_space_size = env.observation_space.n  \n\nq_table = np.zeros((state_space_size,action_space_size))  #Our grid where our agent will move.You will get more intution in the next.\nprint(q_table)","155ed014":"#Setting up the environment parameters\n\nnum_episodes = 10000               #The no. epsiode we want our agent to play during training.\nmax_steps_per_episode = 100       #Max no. of steps agent allowed to take during single episode.\n\nlearning_rate = 0.1\ndiscount_rate = 0.99\n\n\n#Parameters related to exploitation greedy-strategy.\n\nexploration_rate = 1                  #You will get more intitution about this in the next cell\nmax_exploration_rate = 1 \nmin_exploration_rate = 0.0001\nexploration_decay_rate = 0.001","6f01db19":"rewards_all_episodes = []\n#Q-learning Algorithm\nfor episode in range(num_episodes):\n \n    \"\"\"\n    For each episode we are going\n    back to starting state.\n    \"\"\"\n    \n    state = env.reset()\n    done = False                     #Just to keep track that whether our episode is finished or not.\n    rewards_current_episode = 0      #Reset reward\n    \n    \n    for step in range(max_steps_per_episode):\n         \n        \"\"\"\n        Nested loop which run for each timestamp\n        within an episode.\n        \"\"\"\n        \n        #Exploration-rate trade off:\n        exploration_rate_threshold = random.uniform(0,1)\n        if exploration_rate_threshold > exploration_rate:\n            action = np.argmax(q_table[state,:])\n        else:\n            action = env.action_space.sample()\n            \n        new_state , reward , done, info = env.step(action)\n        \n        # Update Q-table for (s,a)\n        # Update Q-table for Q(s,a)\n        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + \\\n        learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n        \n        state = new_state\n        rewards_current_episode += reward\n        \n        if done == True:\n            break\n    \n    \n    #Exploration rate decay\n    exploration_rate = min_exploration_rate + \\\n    (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n    rewards_all_episodes.append(rewards_current_episode)\n    \n# Calculate and print the average reward per thousand reward\nrewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes),num_episodes\/1000)\ncount = 1000\n\nprint(\"********Average reward per thousand episodes********\\n\")\nfor r in rewards_per_thousand_episodes:\n    print(count, \": \", str(sum(r\/1000)))\n    count += 1000\n    \n# Print updated Q-table\nprint(\"\\n\\n********Q-table********\\n\")\nprint(q_table)","61197f81":"# Watch our agent play Frozen Lake by playing the best action \n# from each state according to the Q-table\n\nfor episode in range(3):\n    state = env.reset()\n    done = False\n    print(\"*****EPISODE \", episode+1, \"*****\\n\\n\\n\\n\")\n    time.sleep(1)\n\n    for step in range(max_steps_per_episode):        \n        # Show current state of environment on screen\n        # Choose action with highest Q-value for current state       \n        # Take new action\n        clear_output(wait=True)\n        env.render()\n        time.sleep(0.3)\n        \n        action = np.argmax(q_table[state,:])        \n        new_state, reward, done, info = env.step(action)\n\n        if done:\n            clear_output(wait=True)\n            env.render()\n            if reward == 1:\n                print(\"****You reached the goal!****\")\n                time.sleep(3)\n            else:\n                print(\"****You fell through a hole!****\")\n                time.sleep(3)\n                clear_output(wait=True)\n            break\n        state = new_state    \n\nenv.close()","25b972c0":"# Notebook Content\n\n**INTRODUCTION TO REINFORCEMENT LEARNING**\n1. Markov Decision Processes (MDPs)\n2. Introduction to MDPs\n3. Policies and value functions\n4. Learning optimal policies and value functions\n\n**Q-LEARNING**\n1. Introduction to Q-learning with value iteration\n2. Implementing an epsilon greedy strategy","5906b56c":"# Markov Decision Processes (MDPs)\n\nMarkov decision processes give us a way to formalize sequential decision making. This formalization is the basis for <br>\nstructuring problems that are solved with reinforcement learning.\n\n**Components of an MDP:<br>\n1.Agent <br>\n2.Environment<br> \n3.State  -    S<br>\n4.Action -    A<br>\n5.Reward -    R<br>\n6.Time   -    T<br>**\n\n\n![image.png](attachment:image.png)<br>\n\n***It is the agent\u2019s goal to maximize the expected discounted return of rewards.***<br>\n\n**What's expected discounted return?**<br>\nTo understand that first we need to know what is episodic task and what is continous task!<br>\n\n**Episodic Task** : Each episode ends in a terminal state at time T , which is followed by resetting the environment to some <br>\n                standard starting state or to a random sample from a distribution of possible starting states. The next <br>                         episode then begins independently from how the previous episode ended. E.g Levels in mario etc.<br>\n\n**Continious Task**: There exists other types of tasks though where the agent-environment interactions don\u2019t break up <br>\n                 naturally into episodes, but instead continue without limit. These types of tasks are called continuing tasks.<br>\n                 \nDiscounted Return: \\begin{eqnarray*} G_{t} &=&R_{t+1}+\\gamma R_{t+2}+\\gamma ^{2}R_{t+3}+\\cdots \\\\ &=&\\sum_{k=0}^{\\infty }\\gamma ^{k}R_{t+k+1}\\text{.} \\end{eqnarray*}\n\nThe G at (t) is called reward. Which our agent need to maximize.<br>\nThis definition of the discounted return makes it to where our agent will care more about the immediate reward over future <br>\nrewards since future rewards will be more heavily discounted. <br>\nSo, while the agent does consider the rewards it expects to receive in the future, the more immediate rewards have more <br>\ninfluence when it comes to the agent making a decision about taking a particular action.<br>\n                 ","249774c6":"# Policies\nA policy is a function that maps a given state to probabilities of selecting each possible action from that state. We will use <br>\nthe symbol <math>\n  <mi>&#x3C0;<\/mi>\n<\/math> to denote a policy.<br>\nWhich means that, at time t, under policy <math>\n  <mi>&#x3C0;<\/mi>\n<\/math>, the probability of taking action a in state s is <math>\n  <mi>&#x3C0;<\/mi>\n  <mo stretchy=\"false\">(<\/mo>\n  <mi>a<\/mi>\n  <mrow class=\"MJX-TeXAtom-ORD\">\n    <mo stretchy=\"false\">|<\/mo>\n  <\/mrow>\n  <mi>s<\/mi>\n  <mo stretchy=\"false\">)<\/mo>\n<\/math>.<br>\n\n# Value Function\nValue functions are functions of states, or of state-action pairs, that estimate how good it is for an agent to be in <br> \na given state, or how good it is for the agent to perform a given action in a given state.\n\n# State-Value Function\nIt gives us the value of a state under <math>\n  <mi>&#x3C0;<\/mi>\n<\/math>.\nFormally, the value of state s under policy <math>\n  <mi>&#x3C0;<\/mi>\n<\/math> is the expected return from starting from state s at time t and following policy <math>\n  <mi>&#x3C0;<\/mi>\n<\/math> thereafter. Mathematically we define  as \\begin{eqnarray*} v_{\\pi }\\left( s\\right) &=&E_{\\pi}\\left[\n                                                \\rule[-0.05in]{0in}{0.2in}G_{t}\\mid S_{t}=s\\right] \\\\ &=&E_{\\pi }\\left[ \\sum_{k=0}^{\\infty }\\gamma ^{k}R_{t+k+1}\\mid S_{t}=s\\right] \\text{.} \\end{eqnarray*}\n                                                \n                                                \n                                                \n# Action-Value Function                                                \nIt gives us the value of an action under \\pi. Formally, the value of action a in state s under policy <math>\n  <mi>&#x3C0;<\/mi>\n<\/math> is the expected return from starting from state s at time t , taking action a , and following policy <math>\n  <mi>&#x3C0;<\/mi>\n<\/math> thereafter. Mathematically, we define  as <math>\n  <msub>\n    <mi>q<\/mi>\n    <mi>&#x3C0;<\/mi>\n  <\/msub>\n  <mo stretchy=\"false\">(<\/mo>\n  <mi>s<\/mi>\n  <mo>,<\/mo>\n  <mi>a<\/mi>\n  <mo stretchy=\"false\">)<\/mo>\n<\/math>\n\n\\begin{eqnarray*} q_{\\pi }\\left( s,a\\right) &=&E_{\\pi }\\left[ G_{t}\\mid S_{t}=s,A_{t}=a \\rule[-0.05in]{0in}{0.2in}\\right] \\\\ &=&E_{\\pi }\\left[ \\sum_{k=0}^{\\infty }\\gamma ^{k}R_{t+k+1}\\mid S_{t}=s,A_{t}=a\\right] \\text{.} \\end{eqnarray*}\n\n\nConventionally, the action-value function <math>\n  <msub>\n    <mi>q<\/mi>\n    <mi>&#x3C0;<\/mi>\n  <\/msub>\n<\/math> is referred to as the Q-function, and the output from the function for any given state-action pair is called a Q-value. The letter \u201c Q\u201d is used to represent the quality of taking a given action in a given state. We\u2019ll be working with Q-value functions a lot going forward.\n\n\n**Optimal Policy**\nIn terms of return, a policy <math>\n  <mi>&#x3C0;<\/mi>\n<\/math> is considered to be better than or the same as policy <math>\n  <msup>\n    <mi>&#x3C0;<\/mi>\n    <mi mathvariant=\"normal\">&#x2032;<\/mi>\n  <\/msup>\n<\/math> if the expected return of <math>\n  <mi>&#x3C0;<\/mi>\n<\/math> is greater than or equal to the expected return of  for all states. In other words,\n\n\u03c0\u03c0\nRemember, \u03c0 gives the expected return for starting in state  and following  thereafter. A policy that is better than or at least the same as all other policies is called the optimal policy.\n                                               \n# Bellman Optimality Equation For Q*\n\n\\begin{eqnarray*} q_{\\ast }\\left( s,a\\right) &=&E\\left[ R_{t+1}+\\gamma \\max_{a^{\\prime }}q_{\\ast }\\left( s^\\prime,a^{\\prime }\\right)\\right] \\end{eqnarray*}\n\nIt states that, for any state-action pair (s,a) at time t, the expected return from starting in state s, selecting action a and following the optimal policy thereafter (AKA the Q-value of this pair) is going to be the expected reward we get from taking action a in state s, which is R(t+1) , plus the maximum expected discounted return that can be achieved from any possible next state-action pair (s',a') .\n","82ac16d1":"In the layout above:<br>\n**S - Starting Point <br>\nH - Hole<br>\nF - Frozen Lake<br>\nG - Ground<br>**\n\nOur agent will have to decide its path to reach ground from Hole!<br>\nDon't worry we will train him like a soldier....","87c8ab49":"# Setting up the environment","673804b7":"Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend. The surface is described using a grid like the following:","a4d01124":"# Exploration Vs. Exploitation\nExploration is the act of exploring the environment to find out information about it. Exploitation is the act of exploiting the information that is already known about the environment in order to maximize the return.\n\n# Q-Learning - Choosing Actions With An Epsilon Greedy Strategy\nTo get this balance between exploitation and exploration, we use what is called an epsilon greedy strategy.\n\nWith this strategy, we define an exploration rate <math>\n  <mi>&#x3F5;<\/mi>\n<\/math> that we initially set to 1 . This exploration rate is the probability that our agent will explore the environment rather than exploit it. With <math>\n  <mi>&#x3F5;<\/mi>\n  <mo>=<\/mo>\n  <mn>1<\/mn>\n<\/math>, it is 100% certain that the agent will start out by exploring the environment.\n\nAs the agent learns more about the environment, at the start of each new episode, <math>\n  <mi>&#x3F5;<\/mi>\n<\/math> will decay by some rate that we set so that the likelihood of exploration becomes less and less probable as the agent learns more and more about the environment. The agent will become \u201cgreedy\u201d in terms of exploiting the environment once it has had the opportunity to explore and learn more about it.\n\nTo determine whether the agent will choose exploration or exploitation at each time step, we generate a random number between 0 and 1. If this number is greater than epsilon, then the agent will choose its next action via exploitation, i.e. it will choose the action with the highest Q-value for its current state from the Q-table. Otherwise, its next action will be chosen via exploration, i.e. randomly choosing its action and exploring what happens in the environment.\n\nif random_num > epsilon:\n  choose action via exploitation\nelse:\n  choose action via exploration\n  \nSince we set exploration rate to 1. It means it will surely explore the environment first. After that we update Q-value.\n\n# Updating The Q-Value\nTo update the Q-value for the action of moving right taken from the previous state, we use the Bellman equation that we highlighted previously: \n\n\\begin{eqnarray*} q_{\\ast }\\left( s,a\\right) &=&E\\left[ R_{t+1}+\\gamma \\max_{a^{\\prime }}q_{\\ast }\\left( s^\\prime,a^{\\prime }\\right)\\right] \\end{eqnarray*}\n\n\nHence we update by using:\n\n\\begin{eqnarray*} q_{\\ast }\\left( s,a\\right) - q(s,a)&=&loss \\\\E\\left[ R_{t+1}+\\gamma \\max_{a^{\\prime }}q_{\\ast }\\left( s^\\prime,a^{\\prime }\\right)\\right] - E\\left[ \\sum_{k=0}^{\\infty }\\gamma ^{k}R_{t+k+1}\\right]&=&loss \\end{eqnarray*}","47fe332f":"Resources : https:\/\/deeplizard.com\/learn\/video\/nyjbcRQ-uQ8\n","fe8ceb4c":"![image.png](attachment:image.png)","fe717903":"# About the game","16c22d08":"# **What Is Reinforcement Learning?**\n\nReinforcement learning (RL) is an area of machine learning that focuses on how you, or how some thing, might act in an environment in order to maximize some given reward. Reinforcement learning algorithms study the behavior of subjects in such environments and learn to optimize that behavior.\n\n\nThroughout the notebook we will learn concept along with the code.","92e3e9ed":"![image.png](attachment:image.png)","77560005":"\n|STATE    |DESCRIPTION                      |REWARD    |\n|:--|:--|--:|\n| S       |Agent\u2019s starting point - safe    |   0      |\n| F       |Frozen surface - safe            |   0      |\n| H       |Hole - game over                 |   0      |\n| G       |Goal - game over                 |   1      |"}}