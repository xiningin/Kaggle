{"cell_type":{"9f3cba89":"code","2a7da3ca":"code","007dd54f":"code","534bf1fa":"code","3c12a7fa":"code","4738967b":"code","073f3344":"code","122abd43":"code","be02f6ca":"code","29ddaca4":"code","271a67ea":"code","a6e0f73f":"code","f29392bc":"code","39314bc9":"code","6708e129":"code","93ce5d61":"code","e423392a":"code","79cfc3f7":"code","66d17636":"code","40776735":"code","1fb6ec0d":"code","f2be90e6":"code","d340bd99":"code","8e82090c":"code","5c133c2c":"code","4a325741":"code","c2847749":"code","da8ab421":"code","3d012306":"code","172046c4":"code","cbb26617":"code","1f68b7e6":"code","16a324ae":"code","011987b3":"code","8f43cc03":"code","8182c0e2":"code","77915c1c":"code","81b53ed7":"code","ac8d6a19":"code","f8ee1046":"code","c43dd8d0":"code","8adbccfd":"code","53eeaf77":"code","bd9edf15":"code","83cecf6f":"code","ac291dec":"code","27d1430b":"code","f993fe96":"code","6f5f317b":"code","5fe5e204":"code","88e511a3":"code","cc1224ac":"code","e1c2d6af":"code","1c12d000":"code","25ede8ec":"code","03f7c90f":"code","70d0be51":"code","9594e221":"code","983f0303":"code","4461216d":"code","4a83c5f0":"code","421e4f01":"code","a36c060c":"code","f4a161a4":"code","13221186":"code","300028d5":"code","fd4e8b4b":"code","7fdc8919":"code","790580ff":"code","b18baf52":"code","8267b586":"code","9f1e02b6":"code","fd811b4e":"markdown","b98b2135":"markdown","9baeb752":"markdown","73f86a25":"markdown","d3517a60":"markdown","6fe850ee":"markdown","cd0fb877":"markdown","b903398b":"markdown","a6dd4d59":"markdown","4369ea0c":"markdown","01307253":"markdown","c3864261":"markdown","435d8798":"markdown","a330727d":"markdown","765548b6":"markdown","6aea4a67":"markdown","a1ba8390":"markdown","3a732351":"markdown","622a4181":"markdown","5ecd4cba":"markdown","fc0d8f8c":"markdown","3517380c":"markdown","d5b2238c":"markdown","40774d13":"markdown","b122e17d":"markdown","e4178ea9":"markdown","d95cda39":"markdown"},"source":{"9f3cba89":"import pandas as pd\n\nimport numpy as np\nfrom scipy import stats\nfrom math import ceil\nfrom scipy.stats import norm\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import GridSearchCV, RepeatedKFold, cross_val_score\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import explained_variance_score\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import r2_score\nfrom sklearn import linear_model\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.preprocessing import PolynomialFeatures","2a7da3ca":"import warnings\nwarnings.filterwarnings('ignore')\n\ntrain_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')","007dd54f":"train_df.shape # 1460,81\ntest_df.shape # 1459,80","534bf1fa":"train_df.info()\n#checking all non-numerical columns\nfor c in train_df.columns:\n    col_type = train_df[c].dtype\n    if col_type != 'int64' and col_type != 'float64':\n        print(c)","3c12a7fa":"# Plot the Correlation map to see how features are correlated with target: SalePrice\ncorr_matrix = train_df.corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(corr_matrix, vmax=0.9, square=True)","4738967b":"#scatterplot\nsns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars','GarageArea' ,'TotalBsmtSF', 'FullBath', 'YearBuilt','TotRmsAbvGrd']\nsns.pairplot(train_df[cols], size = 2.5)\nplt.show();","073f3344":"\n#saleprice correlation matrix\nk = 10 #number of variables for heatmap\ncols = corr_matrix.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train_df[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","122abd43":"# first variable : GrLivArea\nplt.subplots(figsize=(15, 5))\n\nplt.subplot(1, 2, 1)\ng = sns.regplot(x=train_df['GrLivArea'], y=train_df['SalePrice'], fit_reg=False).set_title(\"Before\")\n\n# Delete outliers\nplt.subplot(1, 2, 2)                                                                                \ntrain_df = train_df.drop(train_df[(train_df['GrLivArea']>4000) & (train_df['SalePrice']<300000)].index)\ng = sns.regplot(x=train_df['GrLivArea'], y=train_df['SalePrice'], fit_reg=False).set_title(\"After\")","be02f6ca":"# Next Up: TotalBsmtSF\nplt.subplots(figsize=(15, 5))\n\nplt.subplot(1, 2, 1)\ng = sns.regplot(x=train_df['TotalBsmtSF'], y=train_df['SalePrice'], fit_reg=False).set_title(\"Before\")\n\n# Delete outliers\nplt.subplot(1, 2, 2)                                                                                \ntrain_df = train_df.drop(train_df[(train_df['TotalBsmtSF']>3000)].index)\ng = sns.regplot(x=train_df['TotalBsmtSF'], y=train_df['SalePrice'], fit_reg=False).set_title(\"After\")","29ddaca4":"# Next Up : OverallQual\nplt.subplots(figsize=(15, 5))\n\nplt.subplot(1, 2, 1)\ng = sns.regplot(x=train_df['OverallQual'], y=train_df['SalePrice'], fit_reg=False).set_title(\"Before\")\n\n# Delete outliers\nplt.subplot(1, 2, 2)                                                                                \ntrain_df = train_df.drop(train_df[(train_df['OverallQual']>9) & (train_df['SalePrice']>700000)].index)\ng = sns.regplot(x=train_df['OverallQual'], y=train_df['SalePrice'], fit_reg=False).set_title(\"After\")","271a67ea":"train_df.shape # 1453,81\n# 7 rows deleted","a6e0f73f":"#check if there are columns that are present in train and not in test\n# if there are any, we will have to drop them from train\nextra_train_cols = set( train_df.columns ) - set( test_df.columns )\nextra_train_cols # no columns that are present in train and not in test","f29392bc":"# basic states of 'SalePrice'\ntrain_df['SalePrice'].describe()\n# here we see min= 34900 and max= 625000\n# to get a better understanding, we will plot a distribution curve","39314bc9":"#distribution plot- histogram\nsns.distplot(train_df['SalePrice']).set_title(\"Distribution of SalePrice\")\n\n# probability plot\nfig = plt.figure()\nres = stats.probplot(train_df['SalePrice'], plot=plt)","6708e129":"#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\ntrain_df[\"SalePrice\"] = np.log1p(train_df[\"SalePrice\"])\n\n#Check the new distribution \nsns.distplot(train_df['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train_df['SalePrice'])\nprint( '\\n mean = {:.2f} and std dev = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('Distribution of Log SalePrices')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train_df['SalePrice'], plot=plt)\nplt.show()","93ce5d61":"# let us also store the target variable\ny_train = train_df.SalePrice.values\ntrain_df.head()","e423392a":"# store the unique ids of training dataset\ntrain_ids = train_df.index    \n# store the unique ids of test dataset\ntest_ids = test_df.index\n\n# combine train and test datas in to one dataframe\ntotal_df = pd.concat([train_df,test_df]).reset_index(drop=True)\n#total_df.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"Shape of total_df : {}\".format(total_df.shape))\ntotal_df.isnull().sum()","79cfc3f7":"feature_drop1= ['GarageYrBlt','TotRmsAbvGrd'] # will remove 1stFlrSF and GarageArea later-- after creating additional features","66d17636":"#removing features-- with multicollinearity or low correlation with target variable\ntotal_df.drop(feature_drop1,\n              axis=1, inplace=True)\ntotal_df.head()","40776735":"#Checking for missing data\nNAs = pd.concat([train_df.isnull().sum(), test_df.isnull().sum()], axis=1, keys=['Train', 'Test'])\nNAs[NAs.sum(axis=1) > 0]","1fb6ec0d":"# find missing values as percentage of data length\ntotal_na = (total_df.isnull().sum() \/ len(total_df)) * 100\ntotal_na = total_na.drop(total_na[total_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data_perc = pd.DataFrame({'Missing Ratio' :total_na})\nmissing_data_perc","f2be90e6":"# columns with attributes like Pool, Fence etc. marked as NaN indicate the absence of these features.\nattributes_with_na = ['PoolQC','MiscFeature','Alley','Fence','MasVnrType','FireplaceQu',\n               'GarageQual','GarageCond','GarageFinish','GarageType',\n               'BsmtExposure','BsmtCond','BsmtQual','BsmtFinType1','BsmtFinType2']\n\n# replace 'NaN' with 'None' in these columns\nfor col in attributes_with_na:\n    total_df[col].fillna('None',inplace=True)\n    \n#NAs in basement related columns will indicate no masonry veneer. Thus replacing MasVnr Area with 0\ntotal_df.MasVnrArea.fillna(0,inplace=True)\n\n#NAs in basement related columns will indicate no basement. Thus replacing all areas and count column with 0 \ntotal_df.BsmtFullBath.fillna(0,inplace=True)\ntotal_df.BsmtHalfBath.fillna(0,inplace=True)\ntotal_df.BsmtFinSF1.fillna(0,inplace=True)\ntotal_df.BsmtFinSF2.fillna(0,inplace=True)\ntotal_df.BsmtUnfSF.fillna(0,inplace=True)\ntotal_df.TotalBsmtSF.fillna(0,inplace=True)\n\n#Similarly for Garage Cars-- fill with 0; cause if no garage, no cars can parked in it\ntotal_df.GarageCars.fillna(0,inplace=True)   \n# doing the same for GarageArea\ntotal_df.GarageArea.fillna(0,inplace=True)   ","d340bd99":"total_df.isnull().sum()","8e82090c":"# Let us look at perc of nulls again\n# find missing values as percentage of data length\ntotal_na_rev = (total_df.isnull().sum() \/ len(total_df)) * 100\ntotal_na_rev = total_na_rev.drop(total_na_rev[total_na_rev == 0].index).sort_values(ascending=False)[:30]\nmissing_data_perc_rev = pd.DataFrame({'Missing Ratio' :total_na_rev})\nmissing_data_perc_rev","5c133c2c":"# Let us first focus on the lesser null percentages (except LoTFrontage)\n# Let us see the distribution of data across these fields\n\n# first up: Utilities\ntotal_df.groupby(['Utilities']).size() # only one NoSeWa value and 2 nulls \ntrain_df.groupby(['Utilities']).size() # train data contains the 'NoSeWa'i.e. Test has no NoSeWa value\n# 2 null values come from Test data\n## intuitively this will not play a significant role in our model prediction\n# for now let us populate the nulls with the most frequent value 'AllPub'-- can drop it later\ntotal_df['Utilities'] = total_df['Utilities'].fillna(total_df['Utilities'].mode()[0]) ","4a325741":"# next is: Functional\n# Similarly for Functional\n#Functional : by the definition of the column, 'NA' means typical\ntotal_df.groupby(['Functional']).size() # typ has 2717 as of now\n# Since 'typ' is also the most frequent value, let us replace 'NA' with 'typ'\ntotal_df[\"Functional\"] = total_df[\"Functional\"].fillna(\"Typ\")\ntotal_df.groupby(['Functional']).size() # typ= 2719 now","c2847749":"# Let us now look at: Electrical\ntotal_df.groupby(['Electrical']).size() # this has one missing value in Train i.e. SBrKr (currently 2671)\n# Let us just populate the NA with the most frequent entry\ntotal_df['Electrical'] = total_df['Electrical'].fillna(total_df['Electrical'].mode()[0])\ntotal_df.groupby(['Electrical']).size() # now SBrKr= 2672","da8ab421":"# Like Electrical, KitchenQual has 1 missing value\ntotal_df.groupby(['KitchenQual']).size() # the missing value is in Test; most frequent value is 'TA'= 1492\n# Let us just replace null with 'TA'\ntotal_df['KitchenQual'] = total_df['KitchenQual'].fillna(total_df['KitchenQual'].mode()[0])\ntotal_df.groupby(['KitchenQual']).size() # 'TA'= 1493","3d012306":"# The next column is SaleType\ntotal_df.groupby(['SaleType']).size() # one NA in Test, most frequent value is 'WD'=2525\n#populating nulls with the most frequent values\ntotal_df['SaleType'] = total_df['SaleType'].fillna(total_df['SaleType'].mode()[0])\ntotal_df.groupby(['SaleType']).size() # 'WD'= 2526","172046c4":"# Doing the same thing for Exterior1st and 2nd\ntotal_df['Exterior1st'] = total_df['Exterior1st'].fillna(total_df['Exterior1st'].mode()[0])\ntotal_df['Exterior2nd'] = total_df['Exterior2nd'].fillna(total_df['Exterior2nd'].mode()[0])","cbb26617":"# Moving on to the higher null percentages: MSZoninng\ntotal_df.groupby(['MSZoning']).size() #most frequent value is 'RL'=2265\n# Let us just substitute the 4 nulls with the most frequent values\ntotal_df['MSZoning'] = total_df['MSZoning'].fillna(total_df['MSZoning'].mode()[0])\ntotal_df.groupby(['MSZoning']).size() ","1f68b7e6":"#Checking for missing data once again\nNAs_again = pd.concat([total_df.isnull().sum()], axis=1)\nNAs_again[NAs_again.sum(axis=1) > 0] # just one column LotFrontage-- has 486 missing values ","16a324ae":"# function to scale a column\ndef norm_minmax(col):\n    return (col-col.min())\/(col.max()-col.min())","011987b3":"# By business definition, LotFrontage is the area of each street connected to the house property\n# Intuitively it should be highly correlated to variables like LotArea\n# It should also depend on LotShape, LotConfig\n# Let us make a simple Linear regressor to get the most accurate values\n\n# convert categoricals to dummies\n#also dropping the target 'SalePrice' for now as the target currently is 'LotFrontage'\ntotal_df_dummy = pd.get_dummies(total_df.drop('SalePrice',axis=1))\n# scaling all numerical columns\nfor col in total_df_dummy.drop('LotFrontage',axis=1).columns:\n    total_df_dummy[col] = norm_minmax(total_df_dummy[col])\n\nfrontage_train = total_df_dummy.dropna()\nfrontage_train_y = frontage_train.LotFrontage\nfrontage_train_X = frontage_train.drop('LotFrontage',axis=1)  \n\n# fit model\nlin_reg= linear_model.LinearRegression()\nlin_reg.fit(frontage_train_X, frontage_train_y)\n\n# check model results\nlr_coefs = pd.Series(lin_reg.coef_,index=frontage_train_X.columns)\nprint(lr_coefs.sort_values(ascending=False))","8f43cc03":"# use model predictions to populate nulls\nnulls_in_lotfrontage = total_df.LotFrontage.isnull()\nfeatures = total_df_dummy[nulls_in_lotfrontage].drop('LotFrontage',axis=1)\ntarget = lin_reg.predict(features)\n\n# fill nan values\ntotal_df.loc[nulls_in_lotfrontage,'LotFrontage'] = target","8182c0e2":"#Checking for missing data once again\nNAs_again = pd.concat([total_df.isnull().sum()], axis=1)\nNAs_again[NAs_again.sum(axis=1) > 0] # just one column LotFrontage-- has 486 missing values ","77915c1c":"train_subset=total_df[total_df['SalePrice'].notnull()]\nntrain= train_subset.shape[0]","81b53ed7":"# Let us start with the variables having highest correlation with the target variable\n# looking at OverallQual, GrLivArea, GarageCars and TotalBsmtSF\n\n# Since it is one of the highest correlated variables with the response, we can create a quadratic variable that might be a part of the regression equation\ntotal_df[\"OverallQual_2\"] = total_df[\"OverallQual\"].astype(int) ** 2\n#also creating cubic\ntotal_df[\"OverallQual_3\"] = total_df[\"OverallQual\"].astype(int) ** 3\n# another sqrt transformation\ntotal_df[\"OverallQual_sqrt\"] = np.sqrt(total_df[\"OverallQual\"].astype(int))\n\n# OverallQual is just a categorical variable in guise of integers\n# Changing OverallQual into a categorical variable\ntotal_df['OverallQual'] = total_df['OverallQual'].astype(str)","ac8d6a19":" # next variable: GrLivArea\n # creating the polynomial variables from here as well\ntotal_df[\"GrLivArea_2\"] = total_df[\"GrLivArea\"] ** 2\n#also creating cubic\ntotal_df[\"GrLivArea_3\"] = total_df[\"GrLivArea\"] ** 3\n# another sqrt transformation\ntotal_df[\"GrLivArea_sqrt\"] = np.sqrt(total_df[\"GrLivArea\"])","f8ee1046":"# let us check the distribution of GrLivArea \n#distribution and probability plots\n#distribution plot- histogram\nsns.distplot(total_df['GrLivArea']).set_title(\"Distribution of GrLivArea\")\n\n# probability plot\nfig = plt.figure()\nres = stats.probplot(total_df['GrLivArea'], plot=plt)","c43dd8d0":"# log transformed\ntotal_df['GrLivArea_log'] = np.log1p(total_df['GrLivArea'])","8adbccfd":"# we can also create buckets on GrLivArea\ntotal_df['GrLivArea_Band'] = pd.cut(total_df['GrLivArea'], 6,labels=[\"1\", \"2\", \"3\",\"4\",\"5\",\"6\"])\nprint(total_df['GrLivArea_Band'].unique())\n\n# since these are essential categorical variables,\n# let us convert them to string\ntotal_df['GrLivArea_Band'] = total_df['GrLivArea_Band'].astype(str)","53eeaf77":"total_df.head()","bd9edf15":"# creating polynomial features from TotalBsmtSF\ntotal_df[\"TotalBsmtSF_2\"] = total_df[\"TotalBsmtSF\"] ** 2\n#also creating cubic\ntotal_df[\"TotalBsmtSF_3\"] = total_df[\"TotalBsmtSF\"] ** 3\n# another sqrt transformation\ntotal_df[\"TotalBsmtSF_sqrt\"] = np.sqrt(total_df[\"TotalBsmtSF\"])\n\n# log transformed variable\ntotal_df['TotalBsmtSF_log'] = np.log1p(total_df['TotalBsmtSF'])","83cecf6f":"# also creating a 1-0 flag called 'HasBsmt' using 'TotalBsmtSF'\n#if area>0 it is 'Y', else 'N'\ntotal_df['HasBsmt'] = np.where(total_df['TotalBsmtSF']>0, 'Y', 'N')","ac291dec":"# we can also create buckets on GrLivArea\ntotal_df['TotalBsmtSF_Band'] = pd.cut(total_df['TotalBsmtSF'], 3,labels=[\"1\", \"2\", \"3\"])\nprint(total_df['TotalBsmtSF_Band'].unique())\n\n# since these are essential categorical variables,\n# let us convert them to string\ntotal_df['TotalBsmtSF_Band'] = total_df['TotalBsmtSF_Band'].astype(str)","27d1430b":"# creating polynomial features from GarageCars\ntotal_df[\"GarageCars_2\"] = total_df[\"GarageCars\"] ** 2\n#also creating cubic\ntotal_df[\"GarageCars_3\"] = total_df[\"GarageCars\"] ** 3\n# another sqrt transformation\ntotal_df[\"GarageCars_sqrt\"] = np.sqrt(total_df[\"GarageCars\"])\n\n# log transformed variable\ntotal_df['GarageCars_log'] = np.log1p(total_df['GarageCars'])","f993fe96":"# OverallCond is again just a rating- categorical variable. let us first convert the datatype\ntotal_df['OverallCond'] = total_df['OverallCond'].astype(str)\n# use OverallQual and OverallCond to get a total home quality-- averaging both\ntotal_df['TotalHomeQual'] = (total_df['OverallCond'].astype(int) + total_df['OverallQual'].astype(int))\/2\ntotal_df['TotalHomeQual'] = total_df['TotalHomeQual'].astype(str) # converted to string\ntotal_df[:5]","6f5f317b":"# Adding all floors SF\ntotal_df['AllFlrs_SF'] = total_df['TotalBsmtSF'] + total_df['1stFlrSF'] + total_df['2ndFlrSF']\n\n# creating features with finish type fraction of basement SF\n# create separate columns for area of each possible\n# basement finish type\nbsmt_fin_cols = ['BsmtGLQ','BsmtALQ','BsmtBLQ',\n                 'BsmtRec','BsmtLwQ']\n\nfor col in bsmt_fin_cols:\n    # initialise as columns of zeros\n    total_df[col+'SF'] = 0\n\n# fill remaining finish type columns\nfor row in total_df.index:\n    fin1 = total_df.loc[row,'BsmtFinType1']\n    if (fin1!='None') and (fin1!='Unf'):\n        # add area (SF) to appropriate column\n        total_df.loc[row,'Bsmt'+fin1+'SF'] += total_df.loc[row,'BsmtFinSF1']\n        \n    fin2 = total_df.loc[row,'BsmtFinType2']\n    if (fin2!='None') and (fin2!='Unf'):\n        total_df.loc[row,'Bsmt'+fin2+'SF'] += total_df.loc[row,'BsmtFinSF2']\n\n\n# remove initial BsmtFin columns\ntotal_df.drop(['BsmtFinType1','BsmtFinSF1','BsmtFinType2','BsmtFinSF2'], axis=1, inplace=True)\n\n# already have BsmtUnf column in dataset\nbsmt_fin_cols.append('BsmtUnf')\n\n# also create features representing the fraction of the basement that is each finish type\nfor col in bsmt_fin_cols:\n    total_df[col+'Frac'] = total_df[col+'SF']\/total_df['TotalBsmtSF']\n    # replace any nans with zero (for properties without a basement)\n    total_df[col+'Frac'].fillna(0,inplace=True)","5fe5e204":"#creating a feature on LivingAreaSF\ntotal_df['LivingAreaSF'] = total_df['1stFlrSF'] + total_df['2ndFlrSF'] + total_df['BsmtGLQSF'] + total_df['BsmtALQSF'] + total_df['BsmtBLQSF']","88e511a3":"# removing all individual SF variables used to create the above features\nSF_feature_drop= ['1stFlrSF','2ndFlrSF','BsmtGLQSF','BsmtALQSF','BsmtBLQSF','GarageArea']\n#removing features\ntotal_df.drop(SF_feature_drop,\n              axis=1, inplace=True)\ntotal_df.head()\n\ntotal_df[:5]","cc1224ac":"# timeline related variables\n# age at time of selling\ntotal_df['age_at_selling_point']= total_df['YrSold']-total_df['YearBuilt']\n\n# time since last remodel\ntotal_df['time_since_remodel']= total_df['YrSold']-total_df['YearRemodAdd']\n\n# create a flag feature whether the house was remodelled\ntotal_df['remodelled_after']= total_df['YearRemodAdd']-total_df['YearBuilt']\ntotal_df['HasBeenRemodelled'] = np.where(total_df['remodelled_after']>0, 'Y', 'N')\n\n# create feature on decade of selling\ntotal_df['DecadeSold']= (total_df['YrSold']\/\/10)*10\n# convert this to char\ntotal_df['DecadeSold'] = total_df['DecadeSold'].astype(str)\n\n# create feature on decade of building\ntotal_df['DecadeBuilt']= (total_df['YearBuilt']\/\/10)*10\n# convert this to char\ntotal_df['DecadeBuilt'] = total_df['DecadeBuilt'].astype(str)","e1c2d6af":"# drop the time fields used to create above features\n# removing all individual SF variables used to create the above features\ntime_feature_drop= ['YrSold','YearRemodAdd','remodelled_after','YearBuilt']\n#removing features\ntotal_df.drop(time_feature_drop,\n              axis=1, inplace=True)\ntotal_df.head()\n","1c12d000":"#MSSubClass is a categorical variable. Let us change the data type\ntotal_df['MSSubClass'] = total_df['MSSubClass'].astype(str)","25ede8ec":"list(total_df)","03f7c90f":"#Month sold are transformed into categorical features.\ntotal_df['MoSold'] = total_df['MoSold'].astype(str)\n\n# doing similar exercise for several other columns\nqual_cols= ['HeatingQC','KitchenQual','FireplaceQu','GarageQual','PoolQC','ExterQual','BsmtQual','Fence','BsmtCond','GarageCond','ExterCond','GarageCond']\n\nfor c in qual_cols:\n    total_df[c] = total_df[c].astype(str)","70d0be51":"total_df.info()\n#checking all non-numerical columns\nfor c in total_df.columns:\n    col_type = total_df[c].dtype\n    if col_type != 'int64' and col_type != 'float64':\n        print(c)","9594e221":"# create a list of ordinal variables\nordinal_variables=['HeatingQC','KitchenQual','FireplaceQu','GarageQual','PoolQC','ExterQual','BsmtQual','Fence','BsmtCond','GarageCond','ExterCond','GarageCond','OverallCond','OverallQual','TotalHomeQual']\n# label encoder\nle = preprocessing.LabelEncoder()\n\nfor c in ordinal_variables:\n    le.fit(total_df[c])\n    total_df[c] = le.transform(total_df[c])","983f0303":"# create a list of categorical columns for one hot encoding\ncat_variables= ['MSSubClass','MSZoning','Street','Alley','LotShape','LotConfig','LandContour','BsmtExposure','BldgType','CentralAir','Condition1','Condition2','Electrical','Exterior1st','Exterior2nd','Foundation','Functional','GarageFinish','GarageType','Heating','HouseStyle','LandSlope','SaleCondition','Utilities','RoofStyle','HasBsmt','RoofMatl','MasVnrType','HasBeenRemodelled','DecadeBuilt','DecadeSold','MoSold','Neighborhood','PavedDrive','MiscFeature','GrLivArea_Band','TotalBsmtSF_Band','SaleType']\n\n# One-Hot encoding to convert categorical columns to numeric\nprint('start one-hot encoding')\n\ntotal_df = pd.get_dummies(total_df, prefix = cat_variables,\n                         columns = cat_variables)\n\nprint('one-hot encoding done')\n\n# dropping SalePrice\ntotal_df.drop(['SalePrice'], axis=1, inplace=True)\n\n# normalize the variables to values from 0 to 1\nnormalized_total_df = pd.DataFrame(preprocessing.normalize(total_df))","4461216d":"print(total_df.shape)\nprint(normalized_total_df.shape)\nnormalized_total_df.columns= list(total_df)\nnormalized_total_df","4a83c5f0":"print(normalized_total_df.shape)\nnormalized_total_df.info()","421e4f01":"# Splitting the train and test datasets\ntrain = normalized_total_df[:ntrain]\ntest = normalized_total_df[ntrain:]","a36c060c":"# metric for evaluation\ndef rmse(y_true, y_pred):\n    diff = y_pred - y_true\n    sum_sq = sum(diff**2)    \n    n = len(y_pred)   \n    \n    return np.sqrt(sum_sq\/n)","f4a161a4":"# LGBM Regression\nlgb_model = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n\nlgb_model.fit(train.values, y_train)\nlgb_train_pred = lgb_model.predict(train.values)\nlgb_pred = np.expm1(lgb_model.predict(test.values))","13221186":"print('score=',lgb_model.score(train.values,y_train))\nprint('rmse=',rmse(y_train, lgb_train_pred))\nprint('explained variance score=',explained_variance_score(y_train, lgb_train_pred))\nprint('mae=',mean_absolute_error(y_train, lgb_train_pred))\nprint('r-squared=',r2_score(y_train, lgb_train_pred))","300028d5":"# XGB regressor\nxgb_model = xgb.XGBRegressor(max_depth= 3, learning_rate= 0.05, n_estimators= 800, booster= 'gbtree', gamma= 0, reg_alpha= 0.1,\n                  reg_lambda=0.7, max_delta_step= 0, min_child_weight=1, colsample_bytree=0.5, colsample_bylevel=0.2,\n                  scale_pos_weight=1)\n\nxgb_model.fit(train.values, y_train)\nxgb_train_pred = xgb_model.predict(train.values)\nxgb_pred = np.expm1(xgb_model.predict(test.values))","fd4e8b4b":"print('score=',xgb_model.score(train.values,y_train))\nprint('rmse=',rmse(y_train, xgb_train_pred))\nprint('explained variance score=',explained_variance_score(y_train, xgb_train_pred))\nprint('mae=',mean_absolute_error(y_train, xgb_train_pred))\nprint('r-squared=',r2_score(y_train, xgb_train_pred))","7fdc8919":"#KNearestNeighbours\nknn_model= KNeighborsRegressor()\n\nknn_model.fit(train.values, y_train)\nknn_train_pred = knn_model.predict(train.values)\nknn_pred = np.expm1(knn_model.predict(test.values))","790580ff":"print('score=',knn_model.score(train.values,y_train))\nprint('rmse=',rmse(y_train, knn_train_pred))\nprint('explained variance score=',explained_variance_score(y_train, knn_train_pred))\nprint('mae=',mean_absolute_error(y_train, knn_train_pred))\nprint('r-squared=',r2_score(y_train, knn_train_pred))","b18baf52":"# best model was xgb with max r-squared and min error\nprint(\"number of predictions=\",xgb_pred.shape[0],\"rows\") # shape of predictions is equal to shape of test dataset\n\n#hence appending\n# storing in best model predictions\nbest_model_prediction= xgb_pred\n# concat\ntest_df['SalePrice']= best_model_prediction","8267b586":"final_submission=test_df.copy()\nfinal_submission = final_submission[['Id','SalePrice']]\nprint(final_submission.shape)\nfinal_submission[:10]","9f1e02b6":"final_submission.to_csv('Final_submission.csv', encoding='utf-8',header=True ,index=False)","fd811b4e":"A few of these packages might not being used right now. Will use as I build more models","b98b2135":"**GrLivArea**","9baeb752":"### Missing Value Treatment","73f86a25":"### Outlier Removal","d3517a60":"### Importing packages","6fe850ee":"We see a positive skewness. This is not exactly a normal distribution. Also, the balues do not follow the linear trend here.  \nNow, if we observe a positive or a right skewness, log transformation is a good option\n\n#### Why log?\nThe non-linear trend shows 'SalePrice' has some sort of exponential relationship with the independent variables. Applying a log function on these values should give a linear trend and convert the set of values into 'normally distributed' values.\n\nFurthermore,  for this problem, \"submissions are evaluated on **Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price**. (Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.)\"\n\nAll this would just make sense if we replace the values in 'SalePrice' column by the corresponding log values","cd0fb877":"**TotalBsmtSF**","b903398b":"Let us first look at outliers in the numerical variables with the highest correlation to 'SalePrice'. Those are the ones that should make the most difference to 'SalePrice' predictions. ","a6dd4d59":"### Data Exploration and Understanding","4369ea0c":"**GarageCars**","01307253":"### Concatenating train and test to create total_df (aggregated dataset)","c3864261":"**OverallQual**","435d8798":"#### KNeighbours Regression","a330727d":"Thus final list of variables to be definitely considered and variables to be excluded:\n\n1. Variable with highest correlation with 'SalePrice' is 'OverallQual'-----> Retain 'OverallQual'\n\n2. High correlation between 'GarageCars' and 'GarageArea'  + High correlation between 'GarageCars' and 'SalePrice' ----> Keep 'GarageCars'; remove 'GarageArea'\n\n3. 'TotalBsmtSF' and '1stFlrSF' have high correlation and are equally correlated with 'SalePrice'-----> Randomly selecting 'TotalBsmtSF', remove '1stFlrSF'\n\n4. Of 'GarageYrBlt' and 'YearBuilt', 'YearBuilt' has lower missing values and higher correlation with 'SalePrice'-----> retain 'YearBuilt', remove 'GarageYrBlt'\n\n5. Strong correlation between 'TotRmsAbvGrd' and 'GrLivArea' + higher correlation between 'GrLivArea'  and 'SalePrice'-------> Keep 'GrLivArea' ; remove 'TotRmsAbvGrd'\n\n6. Retain 'FullBath' as we did not see correlation with any other variable but it has a significant association with 'SalePrice'","765548b6":"Following are the major observations from here:\n1. GrLivArea and TotRmsAbvGrd show high linear relationship. \n2. Let us shift focus to GrLivArea and TotalBsmtSF show a linear relationship with almost a boundary defining the plot. This basically indicates that GrLivArea defined the higher limit for the TotalBsmtSF (Basement area). Not many houses will have basements larger than the ground floor living area.\n3. 'SalePrice' shows almost a steep increase with 'YearBuilt', basically indicating that prices increase (almost eponentially) as the houses decrease in age. Most recent houses are highly priced.\n\nWe shall just zoom into the correlation matrix with 'SalePrice' and a few other features:\n","6aea4a67":"#### treating all object variables","a1ba8390":"### Modelling","3a732351":"Let us now start creating some features from multiple base variables.","622a4181":"We see a similar positive skewness in this variable. Let us create a log-transformed variable from GrLivArea","5ecd4cba":"#### Tree Based Regression","fc0d8f8c":"### Appending predictions to test","3517380c":"Creating a list of features to be removed \n","d5b2238c":"#### What are we doing?\nWe are investigating the nature of the target or response variable here; i.e. 'SalePrice'. On the basis of our findings here, we can transform the variable.\n\n#### Why are we doing this?\nThis is to ensure the model predictions behave better. What this means is, in regression it is necessary that the residuals follow a normal distribution. Now, if the predicted values are normally distributed then the residuals are as well and vice versa. For a detailed explaination refer to this [link here](https:\/\/stats.stackexchange.com\/questions\/60410\/normality-of-dependent-variable-normality-of-residuals).","40774d13":"### Investigation of Target Variable","b122e17d":"Of the above variables, a few are ordinal, others are categorical\nWe need to LabelEncode the ordinal features and One-hot encode the categorical features","e4178ea9":"### Feature Engineering or Feature Manipulation","d95cda39":"Let us do some feature selection on the basis of the correlations above and some general understanding of the problem:\n\n1. If we look at the heatmap above, all white squares indicate high correlation between the corresponding variables\n2. 'GarageCars' and 'GarageArea' show high correlation and is aligned with our intuitive thinking as well. More the area of the Garage , more the number of cars. Furthermore, both seem to have a similar (and relatively high) correlation with 'SalePrice'. This shows a clear case of multicollinearity. Thus we can remove one of them and retain the other.\n3. Furthermore, 'TotalBsmtSF' and '1stFlrSF' show high correlation again indicating multicollinearity. We should thus remove one of these.\n4. If we look at the correlation between 'TotalBsmtSF' and 'SalePrice', we see a white square i.e. high correlation. This indicates that TotalBsmtSF should be retained as it can help with the SalePrice prediction\n5. Another set of variables that show high correlation are 'YearBuilt' and 'GarageYrBlt'. Let us look at two other aspects: percentage of missing values in 'GarageYrBlt' and the correlation between 'YearBuilt' and 'SalePrice'. 'GarageYrBlt' has over a 5% missing values. Also, 'YearBuilt' seems to have a decent (around 0.5) correlation with 'SalePrice'. It seems like we should retain 'YearBuilt' and let go of 'GarageYrBlt'\n6. Also, besides 'YearBuilt', ( one of 'GarageCars' or 'GarageArea') and 'TotalBsmtSF', we should keep in mind four other variables that seem to have good correlation with 'SalePrice': 'OverallQual', 'GrLivArea', 'FullBath' and 'TotRmsAbvGrd'\n7. However, if we look at the correlation of 'TotRmsAbvGrd' with other variables, we see a high correlation with 'GrLivArea'. To be able to make a call on which variable to remove, let us look at some more analysis."}}