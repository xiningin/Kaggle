{"cell_type":{"3377081e":"code","28cb1ad8":"code","8cd07e8f":"code","54dd0f66":"code","00e149b1":"code","0ef9b885":"code","a704bf21":"code","92822e4b":"code","89e277a9":"code","c272501c":"code","f7e2d23c":"code","ccae2ab7":"code","500cb081":"code","ea176d77":"code","a5cc9d7f":"code","0946cf1a":"code","354408fa":"code","c0e85dc4":"code","0fe54b63":"code","2c0b1b0a":"code","227993bc":"code","d7f8acba":"code","aa3b57e9":"code","0244fa2e":"code","689db392":"code","515bd61f":"code","34de3eaa":"code","bbf45c2a":"code","5e9523c4":"code","cd5a09a7":"code","a1015dc3":"code","12591562":"code","4e07e3ad":"code","865c2b57":"code","97c3438f":"code","5ede06e1":"code","ecbbe633":"markdown","beb08b66":"markdown"},"source":{"3377081e":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm, skew, kurtosis\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\nimport os\nprint(os.listdir('..\/input'))","28cb1ad8":"df_train = pd.read_csv('..\/input\/train.csv')","8cd07e8f":"df_train.info()","54dd0f66":"df_train.head(10)","00e149b1":"df_train.columns","0ef9b885":"#check if there any zero in minimal of the price\ndf_train['SalePrice'].describe()","a704bf21":"#https:\/\/seaborn.pydata.org\/tutorial\/distributions.html#plotting-univariate-distributions\nsns.distplot(df_train['SalePrice'], bins=20, rug=True);","92822e4b":"#Skew = ambience distributions data (0=evenly distributed)\n#Kurt = to check the outlier data (3=standart value)\nprint(\"Skewness: %f\" %df_train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" %df_train['SalePrice'].kurt())","89e277a9":"#Annotation Heatmap https:\/\/seaborn.pydata.org\/examples\/heatmap_annotation.html\n\nHeatmap_Annotation = df_train.corr()\nf, ax = plt.subplots(figsize=(9, 6))\nsns.heatmap(Heatmap_Annotation, \n            vmax=.8, square=True)","c272501c":"#Diagonal Correlation https:\/\/seaborn.pydata.org\/examples\/many_pairwise_correlations.html\n\nsns.set(style='white')\nDiagonal_Corr = df_train.corr()\nmask = np.zeros_like(Diagonal_Corr, dtype=np.bool)\nmask[np.tril_indices_from(mask)] = True\nf, ax = plt.subplots(figsize=(11,9))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(Diagonal_Corr, mask=mask, cmap=cmap,\n           vmax=.8, center=0,square=True,\n           linewidths=5, cbar_kws={\"shrink\":.5})","f7e2d23c":"#SalePrice Correlation Matrix\nk=10\nsns.set(font_scale=1.25)\ncorrmat=df_train.corr()\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(df_train[cols].values.T)\nsns.heatmap(cm, annot=True, square=True,\n           fmt='.2f', annot_kws={'size':10},\n           yticklabels=cols.values,\n           xticklabels=cols.values)\n","ccae2ab7":"#Scatterplot https:\/\/seaborn.pydata.org\/tutorial\/regression.html\ncols = ['SalePrice', 'OverallQual','GrLivArea',\n       'GarageCars', 'GarageArea', 'TotalBsmtSF',\n       '1stFlrSF', 'FullBath', 'TotRmsAbvGrd',\n       'YearBuilt']\nsns.pairplot(df_train[cols], size =5)","500cb081":"#missing data\ntotal = df_train.isnull().sum().sort_values(ascending=False)\npercent = ((df_train.isnull().sum()\/df_train.isnull().count())*100).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","ea176d77":"f, ax = plt.subplots(figsize=(15,12))\nplt.xticks(rotation='90')\nsns.barplot(x=total.index, y=total)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values',fontsize=15)\nplt.title('Percent missing data by feature',fontsize=15)","a5cc9d7f":"#drop columns\/keys that have more than 50% of null values\ndf_train = df_train.drop((missing_data[missing_data['Percent'] > 50 ]).index,1)\ndf_train.isnull().sum().sort_values(ascending=False) #check","0946cf1a":"#FireplaceQu : data description says Null means \"no fireplace\"\ndf_train['FireplaceQu'] = df_train['FireplaceQu'].fillna('None')","354408fa":"#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\ndf_train[\"LotFrontage\"] = df_train.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))","c0e85dc4":"for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', \n            'BsmtFinType1', 'BsmtFinType2',\n            'GarageType', 'GarageFinish', 'GarageQual', \n            'GarageCond'):\n    df_train[col] = df_train[col].fillna('None')","0fe54b63":"#GarageYrBlt replacing missing data with 0\ndf_train['GarageYrBlt'] = df_train['GarageYrBlt'].fillna(0)","2c0b1b0a":"df_train[\"MasVnrType\"] = df_train[\"MasVnrType\"].fillna(\"None\")\ndf_train[\"MasVnrArea\"] = df_train[\"MasVnrArea\"].fillna(0)","227993bc":"#Electrical : It has one NA value. \n#Since this feature has mostly 'SBrkr', we can set that for the missing value.\ndf_train['Electrical'] = df_train['Electrical'].fillna(df_train['Electrical'].mode()[0])","d7f8acba":"df_train.isnull().sum().sort_values(ascending=False) #check","aa3b57e9":"#SalePrice Correlation Matrix\nk=10\nsns.set(font_scale=1.5)\ncorrmat=df_train.corr()\ncols = corrmat.nsmallest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(df_train[cols].values.T)\nsns.heatmap(cm, annot=True, square=True,\n           fmt='.2f', annot_kws={'size':10},\n           yticklabels=cols.values,\n           xticklabels=cols.values)\n\n\n","0244fa2e":"#deleting uncorrelate colomns\nUncor = ['EnclosedPorch', 'OverallCond', \n        'YrSold', 'LowQualFinSF', 'Id', \n         'MiscVal', 'BsmtHalfBath', 'BsmtFinSF2']\ndf_train.drop(Uncor, axis=1, inplace=True)\ndf_train.info()","689db392":"#More features engineering\n#Transforming some numerical variables that are really\ndf_train['MSSubClass'] = df_train['MSSubClass'].astype(str)\ndf_train['MoSold'] = df_train['MoSold'].astype(str)","515bd61f":"# Adding total sqfootage feature \ndf_train['TotalSF'] = df_train['TotalBsmtSF'] + df_train['1stFlrSF'] + df_train['2ndFlrSF']","34de3eaa":"#Univariate analysis\n#Detect and exclude outlier in numeric dtype\n#low 0.05 and high 0.90 quantile\nfrom pandas.api.types import is_numeric_dtype\ndef remove_outlier(df_train):\n    low = .05\n    high = .90\n    quant_df = df_train.quantile([low, high])\n    for name in list(df_train.columns):\n        if is_numeric_dtype(df_train[name]):\n            df_train = df_train[(df_train[name] > quant_df.loc[low, name]) & (df_train[name] < quant_df.loc[high, name])]\n    return df_train\n\nremove_outlier(df_train).head()","bbf45c2a":"#check the standardizing data\nfor name in list(df_train.columns):\n    if is_numeric_dtype(df_train[name]):\n        saleprice_scaled = StandardScaler().fit_transform(df_train[name][:,np.newaxis]);\n        low_range = saleprice_scaled[saleprice_scaled[:,0].argsort()][:5]\n        high_range= saleprice_scaled[saleprice_scaled[:,0].argsort()][-5:]\n        print('outer range (low) of the distribution:',name)\n        print(low_range)\n        print('\\nouter range (high) of the distribution:',name)\n        print(high_range)","5e9523c4":"#Bivariate\/Multivariate outlier checking with scatter plot\nfor name in list(df_train.columns):\n    if is_numeric_dtype(df_train[name]):\n        data = pd.concat([df_train['SalePrice'], df_train[name]], axis=1)\n        data.plot.scatter(x=name, y='SalePrice', ylim=(0,800000))","cd5a09a7":"#Dropping the outlier\n#Only on the Feature that perform linear regression dot in the scatter plot\ndf_train = df_train.drop(df_train[df_train['LotFrontage'] > 300].index)\ndf_train = df_train.drop(df_train[df_train['LotArea'] > 60000].index)\ndf_train = df_train.drop(df_train[(df_train['OverallQual'] > 9) & (df_train['SalePrice'] < 200000)].index)\ndf_train = df_train.drop(df_train[df_train['MasVnrArea'] > 1500].index)\ndf_train = df_train.drop(df_train[df_train['TotalBsmtSF'] > 3000].index)\ndf_train = df_train.drop(df_train[df_train['1stFlrSF'] > 2500].index)\ndf_train = df_train.drop(df_train[df_train['BsmtFullBath'] > 2.5].index)\ndf_train = df_train.drop(df_train[df_train['HalfBath'] > 1.5].index)\ndf_train = df_train.drop(df_train[df_train['BedroomAbvGr'] > 4].index)\ndf_train = df_train.drop(df_train[df_train['TotRmsAbvGrd'] > 13].index)\ndf_train = df_train.drop(df_train[df_train['Fireplaces'] > 2.5].index)\ndf_train = df_train.drop(df_train[df_train['GarageCars'] > 3].index)\ndf_train = df_train.drop(df_train[df_train['GarageArea'] >= 1250].index)\n","a1015dc3":"#skewed features\nnumeric_feats = df_train.dtypes[df_train.dtypes != \"object\"].index\n# Check the skew of all numerical features\nskewed_feats = df_train[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(20)","12591562":"sns.distplot(df_train['SalePrice'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_train['SalePrice'], plot=plt)","4e07e3ad":"skewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    #df_train[feat] += 1\n    df_train [feat] = boxcox1p(df_train[feat], lam)\n    \n#df_train[skewed_features] = np.log1p(df_train[skewed_features])","865c2b57":"#check\nsns.distplot(df_train['SalePrice'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_train['SalePrice'], plot=plt)","97c3438f":"from sklearn.preprocessing import LabelEncoder\ncolomns = df_train.dtypes[df_train.dtypes == \"object\"].index\n# process columns, apply LabelEncoder to categorical features\nfor name in colomns:\n    lbl = LabelEncoder() \n    lbl.fit(list(df_train[name].values)) \n    df_train[name] = lbl.transform(list(df_train[name].values))\n\n# shape        \nprint('Shape of df_train: {}'.format(df_train.shape))","5ede06e1":"#Dummy categorical features\ndf_train = pd.get_dummies(df_train)\nprint(df_train.shape)\ndf_train.head(20) #please compare the data after engineering and before engineering","ecbbe633":"BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2 : For all these categorical basement-related features, NaN means that there is no basement.\n\nGarageType, GarageFinish, GarageQual and GarageCond : Replacing missing data with None\n\n","beb08b66":"My first Kernels-practice Kernels"}}