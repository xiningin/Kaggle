{"cell_type":{"a5724e83":"code","37dcc030":"code","20b8a5bd":"code","27d3d9dc":"code","2208d330":"code","e399602c":"code","66434f4c":"code","be293816":"code","39e78572":"code","9f7e7449":"code","b8ceb339":"code","a9f6a89a":"code","8479af94":"code","60655d1e":"code","43717085":"code","cad6e525":"code","309501f5":"code","bf12891a":"code","cccdd1d5":"code","336c0be3":"code","37ca9c05":"code","4d9c7bcc":"code","7b474622":"code","192ab0d9":"code","0cf95a74":"code","55bc821e":"code","83b76703":"markdown","a08242dd":"markdown","3cb6c521":"markdown","866e51bd":"markdown","ab4823e2":"markdown","65631282":"markdown"},"source":{"a5724e83":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","37dcc030":"!pip install tensorflow","20b8a5bd":"amazon_df=pd.read_csv(\"\/kaggle\/input\/sentiment-labelled-sentences-data-set\/sentiment labelled sentences\/amazon_cells_labelled.txt\",delimiter='\\t',\n                        header=None, \n                        names=['review', 'sentiment'])\n\nimdb_df = pd.read_csv(\"\/kaggle\/input\/sentiment-labelled-sentences-data-set\/sentiment labelled sentences\/sentiment labelled sentences\/imdb_labelled.txt\", \n                        delimiter='\\t', \n                        header=None, \n                        names=['review', 'sentiment'])\n\nyelp_df = pd.read_csv(\"\/kaggle\/input\/sentiment-labelled-sentences-data-set\/sentiment labelled sentences\/sentiment labelled sentences\/yelp_labelled.txt\", \n                        delimiter='\\t', \n                        header=None, \n                        names=['review', 'sentiment'])\n\n","27d3d9dc":"amazon_df.head()","2208d330":"imdb_df.head()","e399602c":"yelp_df.head()","66434f4c":"data=pd.concat([amazon_df,yelp_df,imdb_df])","be293816":"data.reset_index(drop='True',inplace=True)\n","39e78572":"data","9f7e7449":"#Extracting Reviews and Sentiments\nsentences=data['review'].tolist()\nlabel=data['sentiment'].tolist()","b8ceb339":"# print some examples of sentences and labels\n\nprint(\"Sentences\")\nfor i in range(10):\n    #print(sentences[i],end=\"\\n\")\n    print(\"{} {}\".format(sentences[i],label[i]))\n    \n","a9f6a89a":"!pip install tensorflow_datasets","8479af94":"#Create A Subword Datasets\nimport tensorflow_datasets as tfds\nimport tensorflow as tf\n\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nvocab_size=1000\ntokenizer=tfds.features.text.SubwordTextEncoder.build_from_corpus(sentences,vocab_size,max_subword_length=5)","60655d1e":"print(\"vocab size is\",vocab_size)\n\n#check the tokenizer words\nnum=1\nprint(sentences[num])\nencoded_sentence=tokenizer.encode(sentences[num])\nprint(encoded_sentence)","43717085":"for i in encoded_sentence:\n    print(tokenizer.decode([i]))","cad6e525":"for i,sent in enumerate(sentences):\n    sentences[i]=tokenizer.encode(sent)","309501f5":"#print some encoded text\nfor i in range(10):\n    print(sentences[i],end=\"\\n\")","bf12891a":"import numpy as np\nmax_length =50\ntrunc_type='post'\npadding_type='post'\n\n#pad all Sequence\n\nsequence_added=pad_sequences(sentences,maxlen=max_length,padding =padding_type,truncating=trunc_type)","cccdd1d5":"#Separate the separate and Sentences on training and test data sets\n\ntraining_size=int(len(sentences)*0.8)\ntrain_seq=sequence_added[:training_size]\ntrain_labels=label[:training_size]\n\ntest_seq=sequence_added[training_size:]\ntest_labels=label[training_size:]\n\ntrain_labels=np.array(train_labels)\ntest_labels=np.array(test_labels)","336c0be3":"print(\"Total no of Training Sequence are\",len(train_seq))\nprint(\"Total no of Test Sequence are\",len(test_seq))","37ca9c05":"#Create a Model\nembedding_dim=16\nmodel=tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size,embedding_dim,input_length=max_length),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim,return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)),\n    tf.keras.layers.Dense(6,activation='relu'),\n    tf.keras.layers.Dense(1,activation='sigmoid')\n])\n\n","4d9c7bcc":"#fit a model\nmodel.compile(loss=\"binary_crossentropy\",optimizer='adam',metrics=['accuracy'])\nmodel.summary()","7b474622":"history=model.fit(train_seq,train_labels,epochs=50,validation_data=(test_seq,test_labels))","192ab0d9":"import matplotlib.pyplot as plt\n\n\ndef plot_graphs(history, string):\n        plt.plot(history.history[string])\n        plt.plot(history.history['val_'+string])\n        plt.xlabel(\"Epochs\")\n        plt.ylabel(string)\n        plt.legend([string, 'val_'+string])\n        plt.show()\n\nplot_graphs(history, \"accuracy\")\nplot_graphs(history, \"loss\")","0cf95a74":"def predict_review(model, new_sentences, maxlen=max_length, show_padded_sequence=True ):\n    # Keep the original sentences so that we can keep using them later\n    # Create an array to hold the encoded sequences\n    new_sequences = []\n\n    # Convert the new reviews to sequences\n    for i, frvw in enumerate(new_sentences):\n        new_sequences.append(tokenizer.encode(frvw))\n\n    trunc_type='post' \n    padding_type='post'\n\n    # Pad all sequences for the new reviews\n    new_reviews_padded = pad_sequences(new_sequences, maxlen=max_length, \n                                 padding=padding_type, truncating=trunc_type)             \n\n    classes = model.predict(new_reviews_padded)\n\n    # The closer the class is to 1, the more positive the review is\n    for x in range(len(new_sentences)):\n\n        # We can see the padded sequence if desired\n        # Print the sequence\n        if (show_padded_sequence):\n              print(new_reviews_padded[x])\n        # Print the review as text\n        print(new_sentences[x])\n        # Print its predicted class\n        print(classes[x])\n        print(\"\\n\")","55bc821e":"# Use the model to predict some reviews   \nfake_reviews = [\"I love this phone\", \n                \"Everything was cold\",\n                \"Everything was hot exactly as I wanted\", \n                \"Everything was green\", \n                \"the host seated us immediately\",\n                \"they gave us free chocolate cake\", \n                \"we couldn't hear each other talk because of the shouting in the kitchen\"\n              ]\n\npredict_review(model, fake_reviews)","83b76703":"We are Using Bidirectional LSTM Model","a08242dd":"Define a function to predict the sentiment of reviews\nWe'll be creating models with some differences and will use each model to predict the sentiment of some new reviews.\n\nTo save time, create a function that will take in a model and some new reviews, and print out the sentiment of each reviews.\n\nThe higher the sentiment value is to 1, the more positive the review is.","3cb6c521":"Now we split the data sets into training and test data\n","866e51bd":"Now, we'll create the sequences to be used for training by actually encoding each of the individual sentences. This is equivalent to text_to_sequences with the Tokenizer","ab4823e2":"**Plotting the accuracy and loss**\n\n\n","65631282":"In this case each Sentence have different encoded Length so we need to pad the sequence to make the length of all sentences same\n**"}}