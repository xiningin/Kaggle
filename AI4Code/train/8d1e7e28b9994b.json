{"cell_type":{"f6cc2acf":"code","6b0a2f2a":"code","a665e28a":"code","d1520cf8":"code","41b7588b":"code","6c359fd9":"code","885ac68f":"code","675c1d67":"code","b5267036":"code","ba96908d":"code","15285d47":"code","20d0991b":"code","240d798c":"code","67609f57":"code","3898b6b1":"markdown","98d959f0":"markdown","95c32d38":"markdown","6779adad":"markdown","3068201a":"markdown"},"source":{"f6cc2acf":"import pyspark\nimport pyspark.sql\nimport pyspark.streaming\nimport pyspark.mllib\nimport pyspark.ml\nfrom pyspark.sql import SparkSession","6b0a2f2a":"import warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)","a665e28a":"spark = SparkSession.builder \\\n    .appName('spark_for_conservative_credit_score') \\\n    .master('local[*]') \\\n    .config('spark.driver.memory','8G') \\\n    .config('spark.driver.maxResultSize', '2G') \\\n    .config('spark.serializer', 'org.apache.spark.serializer.KryoSerializer') \\\n    .config('spark.kryoserializer.buffer.max', '800M')\\\n    .getOrCreate()","d1520cf8":"from pyspark.sql.types import IntegerType\nfrom pyspark.sql.types import DoubleType","41b7588b":"train = spark.read.csv('\/data\/paper_train1.csv', header=True)","6c359fd9":"# loading train data set\nfile_location = \"\/data\/paper_train1.csv\"\nfile_type = \"csv\"\n# CSV options\ninfer_schema = \"false\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ntrain = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location).cache()\n\n# loading test data set\nfile_location = \"\/data\/paper_valid1.csv\"\nfile_type = \"csv\"\n# CSV options\ninfer_schema = \"false\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\nvalid = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location).cache()","885ac68f":"from pyspark.sql.functions import *\ntrain = train.withColumn(\"loan_amnt\", train.loan_amnt.cast(\"float\"))\\\n             .withColumn(\"emp_length\", train.emp_length.cast(\"float\"))\\\n             .withColumn(\"annual_inc\", train.annual_inc.cast(\"float\"))\\\n             .withColumn(\"dti\", train.dti.cast(\"float\"))\\\n             .withColumn(\"delinq_2yrs\", train.delinq_2yrs.cast(\"float\"))\\\n             .withColumn(\"revol_util\", regexp_replace(\"revol_util\", \"%\", \"\").cast(\"float\"))\\\n             .withColumn(\"total_acc\", train.total_acc.cast(\"float\"))\\\n             .withColumn(\"credit_length_in_years\", train.credit_length_in_years.cast(\"float\"))\\\n             .withColumn(\"int_rate\", regexp_replace(\"int_rate\", \"%\", \"\").cast(\"float\"))\\\n             .withColumn(\"remain\", train.remain.cast(\"float\"))\\\n             .withColumn(\"issue_year\", train.issue_year.cast(\"float\"))\\\n             .withColumn(\"phi_loan_amnt\", train.phi_loan_amnt.cast(\"float\"))\\\n             .withColumn(\"phi_emp_length\", train.phi_emp_length.cast(\"float\"))\\\n             .withColumn(\"phi_annual_inc\", train.phi_annual_inc.cast(\"float\"))\\\n             .withColumn(\"phi_dti\", train.phi_dti.cast(\"float\"))\\\n             .withColumn(\"phi_delinq_2yrs\", train.phi_delinq_2yrs.cast(\"float\"))\\\n             .withColumn(\"phi_revol_util\", regexp_replace(\"phi_revol_util\", \"%\", \"\").cast(\"float\"))\\\n             .withColumn(\"phi_total_acc\", train.phi_total_acc.cast(\"float\"))\\\n             .withColumn(\"phi_credit_length_in_years\", train.phi_credit_length_in_years.cast(\"float\"))\\\n             .withColumn(\"phi_int_rate\", regexp_replace(\"phi_int_rate\", \"%\", \"\").cast(\"float\"))\\\n             .withColumn(\"CRI\", train.CRI.cast(\"float\"))\\\n             .withColumn(\"train_flag\", train.train_flag.cast(\"float\"))\n\nvalid = valid.withColumn(\"loan_amnt\", valid.loan_amnt.cast(\"float\"))\\\n             .withColumn(\"emp_length\", valid.emp_length.cast(\"float\"))\\\n             .withColumn(\"annual_inc\", valid.annual_inc.cast(\"float\"))\\\n             .withColumn(\"dti\", valid.dti.cast(\"float\"))\\\n             .withColumn(\"delinq_2yrs\", valid.delinq_2yrs.cast(\"float\"))\\\n             .withColumn(\"revol_util\", regexp_replace(\"revol_util\", \"%\", \"\").cast(\"float\"))\\\n             .withColumn(\"total_acc\", valid.total_acc.cast(\"float\"))\\\n             .withColumn(\"credit_length_in_years\", valid.credit_length_in_years.cast(\"float\"))\\\n             .withColumn(\"int_rate\", regexp_replace(\"int_rate\", \"%\", \"\").cast(\"float\"))\\\n             .withColumn(\"remain\", valid.remain.cast(\"float\"))\\\n             .withColumn(\"issue_year\", valid.issue_year.cast(\"float\"))\\\n             .withColumn(\"phi_loan_amnt\", valid.phi_loan_amnt.cast(\"float\"))\\\n             .withColumn(\"phi_emp_length\", valid.phi_emp_length.cast(\"float\"))\\\n             .withColumn(\"phi_annual_inc\", valid.phi_annual_inc.cast(\"float\"))\\\n             .withColumn(\"phi_dti\", valid.phi_dti.cast(\"float\"))\\\n             .withColumn(\"phi_delinq_2yrs\", valid.phi_delinq_2yrs.cast(\"float\"))\\\n             .withColumn(\"phi_revol_util\", regexp_replace(\"phi_revol_util\", \"%\", \"\").cast(\"float\"))\\\n             .withColumn(\"phi_total_acc\", valid.phi_total_acc.cast(\"float\"))\\\n             .withColumn(\"phi_credit_length_in_years\", valid.phi_credit_length_in_years.cast(\"float\"))\\\n             .withColumn(\"phi_int_rate\", regexp_replace(\"phi_int_rate\", \"%\", \"\").cast(\"float\"))\\\n             .withColumn(\"CRI\", valid.CRI.cast(\"float\"))\\\n             .withColumn(\"train_flag\", valid.train_flag.cast(\"float\"))","675c1d67":"train.registerTempTable(\"train\")\ntrain.write.parquet('AA_DFW_ALL.parquet', mode='overwrite')\nvalid.registerTempTable(\"valid\")\nvalid.write.parquet('AA_DFW_ALL.parquet', mode='overwrite')\n\nprint(\" >>>>>>> \" + str(train.count())+ \" loans opened by TRAIN data_set for model training!\")\nprint(\" >>>>>>> \" + str(valid.count())+ \" loans opened by VALID data_set for model validation!\")","b5267036":"print(\" == imbalancement of the loan train and valid datasets  ==\")\nprint(\" >>>>>>> Train dataset: \" + str(train.groupby('default_loan').count().collect()))\nprint(\" >>>>>>> Test dataset: \" + str(valid.groupby('default_loan').count().collect()))","ba96908d":"# Set the response and predictor variables and set up regression models with train and test datasets.\nY = \"default_loan\"\n\ncategoricals = [\"phi_term_month\", \"home_ownership\", \"purpose\", \"addr_state\", \"verification_status\", \"application_type\"]\nnumerics = [\"CRI\", \"phi_loan_amnt\", \"phi_emp_length\", \"phi_annual_inc\", \"phi_dti\", \"phi_delinq_2yrs\", \"phi_revol_util\", \"phi_total_acc\", \"phi_credit_length_in_years\", \"phi_int_rate\"]\nX = categoricals + numerics\n\n# now we can save the valid to use it as an imput data for our final model","15285d47":"# We use mlflow and mlflow.spark. Before importing them into our program, we have to install PyPI and mlflow[extras] into our cluster","20d0991b":"# (1) define the model function\n# to build Grid of GLM models and Standardization + CrossValidation\n\nimport sklearn.metrics as metrics\nimport pandas as pd\nfrom plotnine import *\n#from plotnine.data import meat\nfrom mizani.breaks import date_breaks\nfrom mizani.formatters import date_format\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StandardScaler, StringIndexer, OneHotEncoder, Imputer, VectorAssembler\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nimport mlflow\nimport mlflow.spark\nfrom pyspark.mllib.evaluation import BinaryClassificationMetrics\nfrom pyspark.ml.linalg import Vectors\n\n# setting the parameters\nmaxIter = 10\nelasticNetParam = 0.5\nregParam = 0.3\n  \n  ## we start with mlflow.start_run() which essentially start tracking what we are doing in this notebook in databricks\nwith mlflow.start_run():\n    labelCol = \"default_loan\"\n    indexers = list(map(lambda c: StringIndexer(inputCol=c, outputCol=c+\"_idx\", handleInvalid = \"keep\"), categoricals))\n    ohes = list(map(lambda c: OneHotEncoder(inputCol=c + \"_idx\", outputCol=c+\"_class\"), categoricals))\n    imputers = Imputer(inputCols = numerics, outputCols = numerics)\n    featureCols = list(map(lambda c: c+\"_class\", categoricals)) + numerics\n    model_matrix_stages = indexers + ohes + \\\n                          [imputers] + \\\n                          [VectorAssembler(inputCols=featureCols, outputCol=\"features\"), \\\n                           StringIndexer(inputCol= labelCol, outputCol=\"label\")]\n    \n    scaler = StandardScaler(inputCol=\"features\",\n                            outputCol=\"scaledFeatures\",\n                            withStd=True,\n                            withMean=True)\n    \n    ## here, we build the logistic regression model with parameters equal to variables for elasticNet regression\n    lr = LogisticRegression(maxIter=maxIter, elasticNetParam=elasticNetParam, regParam=regParam, featuresCol = \"scaledFeatures\")\n    \n    ##now, we define a pipline which includes everything from standardazing the data, imputing missing values and encoding for categorical columns\n    pipeline = Pipeline(stages=model_matrix_stages+[scaler]+[lr])\n    \n    glm_model = pipeline.fit(train)\n    \n    ## Log Params and Model\n    ## The important part for mlflow of model tracking and reproduceability of the input parameters that we may want to review and take an action.  \n    mlflow.log_param(\"algorithm\", \"SparkML_GLM_regression\") # we put a name for the algorithm that we used\n    mlflow.log_param(\"regParam\", regParam)\n    mlflow.log_param(\"maxIter\", maxIter)\n    mlflow.log_param(\"elasticNetParam\", elasticNetParam)\n    mlflow.spark.log_model(glm_model, \"glm_model\")           # here we log the model itself\n    \n    ##Evaluate and Log ROC Curve\n    lr_summary = glm_model.stages[len(glm_model.stages)-1].summary\n    roc_pd = lr_summary.roc.toPandas()\n    fpr = roc_pd[\"FPR\"]\n    tpr = roc_pd[\"TPR\"]\n    roc_auc = metrics.auc(roc_pd[\"FPR\"], roc_pd[\"TPR\"])\n   \n    ## Set Max F1 Threshold  (for predicting the loan default with a balance between true-positives and false-positives)\n    fMeasure = lr_summary.fMeasureByThreshold\n    maxFMeasure = fMeasure.groupBy().max(\"F-Measure\").select(\"max(F-Measure)\").head()\n    madFMeasure = maxFMeasure[\"max(F-Measure)\"]\n    fMeasure = fMeasure.toPandas()\n    bestThreshold = float ( fMeasure[ fMeasure[\"F-Measure\"] == maxFMeasure] [\"threshold\"])\n    lr.setThreshold(bestThreshold)\n    \n    \n     ## Evaluate and Log Metrics  (here we score the customers)\n    def extract(row):\n      return (row.remain,) + tuple(row.probability.toArray().tolist()) + (row.label,) + (row.prediction,)\n\n    def score(model,data):\n      pred = model.transform(data).select(\"remain\", \"probability\", \"label\", \"prediction\")\n      pred = pred.rdd.map(extract).toDF([\"remain\", \"p0\", \"p1\", \"label\", \"prediction\"])\n      return pred\n\n    def auc(pred):\n      metric = BinaryClassificationMetrics(pred.select(\"p1\", \"label\").rdd)\n      return metric.areaUnderROC\n    \n   \n    glm_train = score(glm_model, train)\n    glm_valid = score(glm_model, valid)\n    \n    glm_train.registerTempTable(\"glm_train\")\n    glm_valid.registerTempTable(\"glm_valid\")\n    \n    print( \"GLM Training AUC :\" + str( auc(glm_train)))\n    print( \"GLM Validation AUC :\" + str(auc(glm_valid)))\n    \n    ## here we log the auc values and the area under the curve for the models metrics as we defined before for training as well as validation dataset\n    mlflow.log_metric(\"train_auc\", auc(glm_train))\n    mlflow.log_metric(\"valid_auc\", auc(glm_valid))","240d798c":"pandas_df = glm_valid.toPandas()\r\ntxt = 'This table represents the \"CONFUSION MATRIX\" from Elastic Net Regression'\r\nprint(txt.title())\r\npd.crosstab(pandas_df.label, pandas_df.prediction, values=pandas_df.remain, aggfunc=\"count\").round(2)","67609f57":"pandas_df_sum_net = glm_valid.groupBy(\"label\", \"prediction\").agg((sum(col(\"remain\"))).alias(\"sum_net\")).toPandas()\r\ntxt = 'This table represents the \"SUM NET\" from Elastic Net Regression'\r\nprint(txt.title())\r\npd.crosstab(pandas_df_sum_net.label, pandas_df_sum_net.prediction, values=pandas_df_sum_net.sum_net , aggfunc=\"sum\").round(2)","3898b6b1":"# Feature Engineering","98d959f0":"# Elastic net regression ","95c32d38":"# Machine Learning Codes for Credit Scoring - Elastic net regression!\n\nFor citation: https:\/\/doi.org\/10.1016\/j.eswa.2021.114835 (Journal - Expert Systems with Applications.)\n\nAsk for full-text in [ResearchGate](https:\/\/www.researchgate.net\/profile\/Afshin-Ashofteh-2)\n\nAfshin Ashofteh [email](aashofteh@novaims.unl.pt)\n\nSubject: Credit Risk and Credit Scoring.\n\nDatasource: loan.csv - Each loan includes applicant information provided by the applicant as well as current loan status (Current, Late, Fully Paid, etc.) and latest payment information.\n","6779adad":"Links: \n*[ResearchGate](https:\/\/www.researchgate.net\/profile\/Afshin-Ashofteh-2)\n*[Kaggle](https:\/\/www.kaggle.com\/aashofteh)\n*[Google Scholar](https:\/\/scholar.google.com\/citations?user=oIa1W0gAAAAJ&hl=en)\n*[Data Science Discussion Group](https:\/\/www.linkedin.com\/groups\/12420006)\n*[email](aashofteh@novaims.unl.pt)","3068201a":"## credit scoring (GLM models)."}}