{"cell_type":{"7d72c5ed":"code","6b5cf48b":"code","78779889":"code","9dcee1df":"code","bd961bbe":"code","2eccd27b":"code","91e879be":"code","c6d81d4d":"code","7b454ed0":"code","74c1fa3f":"code","27493240":"code","dc275dfb":"code","7d0e1fb8":"code","7bff51ec":"code","8832fb8e":"code","51f4558f":"code","cb29bfad":"code","424b7eff":"code","15ba1aab":"code","a82739a7":"code","e4d0e383":"code","0dec9c33":"code","1bfcc8b2":"code","441c9fdc":"code","677a185a":"markdown","fc7e0994":"markdown","cc26536a":"markdown","e4da9c53":"markdown","61585b82":"markdown","93bd0b6a":"markdown","a94a53a0":"markdown","afbd40f2":"markdown","40483698":"markdown","3e369097":"markdown","1131f639":"markdown","eda93880":"markdown","95c47cb6":"markdown","16947326":"markdown","7ad2868d":"markdown","8ea3c764":"markdown","b00ccef7":"markdown"},"source":{"7d72c5ed":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')\n%matplotlib inline","6b5cf48b":"df = pd.read_csv('\/kaggle\/input\/time-series-data-1\/uspopulation.csv',index_col='DATE',parse_dates=True)\ndf.index.freq = 'MS'\n\ndf['PopEst'].plot(figsize=(12,6), title='Yearly estimated population in US');","78779889":"train = df.iloc[:84]\ntest = df.iloc[84:]","9dcee1df":"from statsmodels.tsa.ar_model import AR\n\nmodel = AR(train).fit(maxlag=1, method='cmle')\n\nprint(f'p used: {model.k_ar}\\n')\nprint(f'AIC : {model.aic}\\n')\nprint(f'Coefficients:\\n{model.params}')","bd961bbe":"def plot_result(pred, model):\n    fig, ax = plt.subplots(figsize=(12,6))\n    train['PopEst'].iloc[-12:].plot(label='train', ax=ax)\n    test['PopEst'].plot(label='test', ax=ax)\n    pred.plot(ax=ax, label=f'{model} predicted')\n    plt.legend();\n\npred = model.predict(start='2018-01-01', end='2018-12-01')    \nplot_result(pred, \"AR(p=1)\")","2eccd27b":"model = AR(train).fit(maxlag=2, method='cmle')\n\npred = model.predict(start='2018-01-01', end='2018-12-01')\nplot_result(pred, \"AR(p=2)\")","91e879be":"model = AR(train).fit(ic='AIC', method='cmle')\n\nprint(f'p used: {model.k_ar}\\n')\nprint(f'AIC : {model.aic}\\n')\nprint(f'Coefficients:\\n{model.params}')","c6d81d4d":"pred = model.predict(start='2018-01-01', end='2018-12-01')\nplot_result(pred, f'AR(p={model.k_ar})')","7b454ed0":"!pip install pmdarima","74c1fa3f":"from pmdarima import auto_arima\n\n'''\nBasic params: start_p, start_q, max_p, max_q, \n        max_d, max_D, start_P, start_Q, max_P, max_Q\n\nd : The order of first-differencing. \n    If None (by default), the value will automatically be selected based on the results of the `test`\nD : The order of the seasonal differencing. \n    If None (by default, the value will automatically be selected based on the results of the `seasonal_test`\ntest : Type of unit root test to use in order to detect stationarity\n    (default='kpss')\nm : (default=1) The period for seasonal differencing.\n    If `m` == 1 (i.e., is non-seasonal), `seasonal` will be set to False. \nseasonal :  Whether to fit a SARIMA.\n    bool (default=True)\nstationary : Whether the time-series is stationary. If so, `d` should be set to zero.\n    bool, optional (default=False)\n        \n'''\n\nresult = auto_arima(train['PopEst'], \n                  start_p=0, start_q=0,\n                  max_p=6, max_q=3, #m=12,\n                  seasonal=False,  # ARIMA\n                  d=None,trace=True) \nresult.summary()","27493240":"from statsmodels.tsa.arima_model import ARIMA\n\nmodel = ARIMA(train['PopEst'],order=(4,1,0))\nresult = model.fit()","dc275dfb":"start=len(train)\nend=len(train)+len(test)-1\n\n'''\ndynamic = False means that forecasts at each point are generated using the full history up to that point (all lagged values).\ntyp ='levels' predicts the levels of the original endogenous variables. \n    If we'd used the default typ='linear' we would have seen linear predictions in terms of the differenced endogenous variables.\n'''\npredictions = result.predict(start=start, end=end, dynamic=False, typ='levels')\n\n# plot the result\ntrain['PopEst'].iloc[-12:].plot(label='train', figsize=(12,6))\ntest['PopEst'].plot(label='test')\npredictions.plot(label='ARIMA prediction')\nplt.legend();","7d0e1fb8":"model = ARIMA(df['PopEst'],order=(4,1,0))\nresult = model.fit()\n\npredictions = result.predict(start=end, end=end+12, dynamic=False, typ='levels')\n\n# plot the result\ndf['PopEst'].iloc[-36:].plot(label='original_data', figsize=(12,6))\npredictions.plot(label='ARIMA forecast')\nplt.legend();","7bff51ec":"df2 = pd.read_csv('\/kaggle\/input\/time-series-data-1\/TradeInventories.csv',index_col='Date',parse_dates=True)\ndf2.index.freq='MS'\ndf2['Inventories'] = df2['Inventories'].astype('float64')\n\ndf2.plot(figsize=(12,6));","8832fb8e":"from statsmodels.tsa.seasonal import seasonal_decompose\n\nresult = seasonal_decompose(df2['Inventories'], model='additive')  # model='add' also works\nresult.plot();","51f4558f":"from statsmodels.tsa.stattools import adfuller\n\nadf, pval, _, _, _, _ = adfuller(df2['Inventories'], autolag='AIC')\nprint(f'p-value = {pval}')","cb29bfad":"auto_arima(df2['Inventories'],seasonal=False).summary()","424b7eff":"# Split\ntrain = df2.iloc[:252]\ntest = df2.iloc[252:]\n\n# Build a model\nmodel = ARIMA(train['Inventories'],order=(0,1,0))\nresults = model.fit()\n\n# Predict\nstart=len(train)\nend=len(train)+len(test)-1\npredictions = results.predict(start=start, end=end, dynamic=False, typ='levels')","15ba1aab":"# plot the result\ntest['Inventories'].plot(figsize=(12,6), label='Test')\ntrain['Inventories'].iloc[-24:].plot(label='Train')\npredictions.plot(label='ARIMA prediction');\nplt.legend();","a82739a7":"df = pd.read_csv('\/kaggle\/input\/time-series-data-1\/airline_passengers.csv')\ndf['Month'] = pd.to_datetime(df['Month'])\n\ndf.set_index('Month', inplace=True)\ndf.index.freq = 'MS'\ndf.plot(figsize=(12,6));","e4d0e383":"adf, pval, _, _, _, _ = adfuller(df.iloc[:,0], autolag='AIC')\nprint(f'Adjusted Dickey-Fuller p-value = {pval}')\n\nseasonal_decompose(df['Thousands of Passengers'], model='add').plot();","0dec9c33":"train = df.iloc[:132]\ntest = df.iloc[132:]\n\nauto_arima(\n    train,\n    seasonal=True, \n    m=12\n).summary()","1bfcc8b2":"from statsmodels.tsa.statespace.sarimax import SARIMAX\n\nmodel = SARIMAX(train['Thousands of Passengers'], \n                order=(3,0,0), \n                seasonal_order=(0,1,0,12))\nresults = model.fit()","441c9fdc":"# Obtain predicted values\nstart=len(train)\nend=len(train)+len(test)-1\npredictions = results.predict(start=start, end=end, dynamic=False, typ='levels')\n\ntest.plot(label='Test')\npredictions.plot(label='SARIMA predictions')\nplt.legend();","677a185a":"## 3.3) Building ARIMA model","fc7e0994":"## 3.1) Choosing ARIMA order manually\n\nIt is usually not possible to tell, simply from a time plot, what values of $p$ and $q$ are appropriate for the data. However, it is sometimes possible to use the ACF and PACF plot to determine appropriate values for $p$ and $q$.","cc26536a":"## 1.3) Let Statsmodels choose p\n\nIn `.fit()`, there is a param `ic` : Criterion used for selecting the optimal lag length.","e4da9c53":"# 2) Moving Average\n\nRather than using past values of the forecast variable in a autoregression, a moving average model uses <u>past forecast errors<\/u> in a regression-like model.\n$$y_{t} = c + \\varepsilon_t + \\theta_{1}\\varepsilon_{t-1} + \\theta_{2}\\varepsilon_{t-2} + \\dots + \\theta_{q}\\varepsilon_{t-q},$$\nWhere \\varepsilon_t is white noise. We refer to this as an $MA(q)$. Notice that each value of $y_t$ can be thought of as a weighted moving average of the past few forecast errors.\n\n<div class=\"alert alert-warning\">\nWe should not be confused with the moving average smoothing or EWMA stuff. Those smoothing stuff is used for estimating the trend-cycle of past values. While a moving average model is used for forecasting future values.\n<\/div>\n\n* When $|\\theta| > 1$, the weights increase as lags increase, so the more distant the observations the greater their influence on the current error\n* When $|\\theta| > 1$, the most recent observations have higher weight than observations from the more distant past.","61585b82":"## 3.5) Another Example\nThis time we will see how well ARIMA perform on dataset with seasonality. ARIMA models are also capable of modelling a wide range of seasonal data.","93bd0b6a":"For more information on `.predict()` arguments visit https:\/\/www.statsmodels.org\/stable\/generated\/statsmodels.tsa.arima_model.ARIMAResults.predict.html","a94a53a0":"# 1) Autoregessive Model\n\nIn a multiple regression model, we forecast the variable of interest using a linear combination of predictors. In an autoregression model, we forecast the variable of interest using a linear combination of past values \n\nan autoregressive model of order $p$ can be written as : \n$$y_{t} = c + \\phi_{1}y_{t-1} + \\phi_{2}y_{t-2} + \\dots + \\phi_{p}y_{t-p} + \\varepsilon_{t}$$\nWhere $c$ is an average of $y_{t-1}, ..,y_{t-p}$ , $p$ is a number of lag we use, $\\phi_i$ is a lag coef. up to order $p$ and $\\varepsilon_t$ is a white noise. We refer this model as $AR(p)$\n","afbd40f2":"# Introduction to ARIMA Models\n\nExponential smoothing and ARIMA models are the two most widely used approaches to time series forecasting. While exponential smoothing models are based on a description of the trend and seasonality in the data, ARIMA models aim to describe the autocorrelations in the data.\n\n### Stationality\n---\n<u>A stationary time series<\/u> is one whose properties do not depend on the time at which the series is observed. it does not matter when you observe it, it should look much the same at any point in time. In general, a stationary time series will have no predictable patterns in the long-term. \n\nACF plot is also useful for identifying non-stationary time series. For a stationary time series, the ACF will drop to zero relatively quickly, while the ACF of non-stationary data decreases slowly. Also, for non-stationary data, the value of autocorrelation at lag 1 is often large and positive.\n\n### Differencing\n---\nIs one way to make a non-stationary time series stationary.\n- Transformations such as logarithms can help to stabilise the variance of a time series. \n- Differencing can help stabilise the mean of a time series by removing changes in the level of a time series, and therefore eliminating (or reducing) trend and seasonality.\n\nIn practice, it is almost never necessary to go beyond second-order differences.\n\n### White Noise\n---\nTime series that show no autocorrelation (close to zero) are called white noise\n\n### Random Walk\n---\nThe differenced series degree 1 can be written as : $y^{'}_t = y_t - y_{t-1}$ Let's say we get a stationary time-series after 1 differencing. Thus, $y_t - y_{t-1}$ is a white noise.\n$$\\epsilon_t = y_t - y_{t-1}$$\nWhere $\\epsilon_t$ denotes *white noise*.\n\n-> **Random walk model** : $y_t = y_{t-1} + \\epsilon_t$","40483698":"# 3) ARIMA model\n\n$$\\begin{equation}\n  y'_{t} = c + \\phi_{1}y'_{t-1} + \\cdots + \\phi_{p}y'_{t-p}\n     + \\theta_{1}\\varepsilon_{t-1} + \\cdots + \\theta_{q}\\varepsilon_{t-q} + \\varepsilon_{t}\n\\end{equation}$$\n\nWhere $y'_{t}$ is the differenced series (it may have been differenced more than once) <br>\n- $p$ : <u>*order of the autoregressive part*<\/u> (How many lag to be taken into account?)\n- $d$ : <u>*degree of first differencing involved*<\/u> (How many time should we perform differencing to make data stationary?) \n- $q$ : <u>*order of the moving average part*<\/u>\n\n<div class=\"alert alert-info\">\n    <h3>ARMA model:<\/h3>\n    <body>Autoregression Moving Average : ARMA(1,1) can be written as<\/body>\n    $$y_{t} = c + \\phi_{1}y_{t-1} + \\theta_{1}\\varepsilon_{t-1} + \\varepsilon_{t}$$\n    <body>Where $y_t$ is time-series data at time $t$. ARMA model always assume data to be stationary. For non-stationary data, we must apply ARIMA model.<\/body>\n<\/div>","3e369097":"By seeing ETS decomposition and augmented Dickey-Fuller test, we can confirm again that data is non-stationary. Note that from ETS we see the seasonality and trend.\n\nNext, we perform autoarima to identify ARIMA orders.","1131f639":"## 3.2) Automatic approach","eda93880":"## 3.4) Forecasting\n\nFor forcasting, we retrain the model using all data, and forecast into the future.","95c47cb6":"# 4) SARIMA model\n\nSo far, we have restricted our attention to non-seasonal data and non-seasonal ARIMA models. However, ARIMA models are also capable of modelling a wide range of seasonal data.\n$$\\text{SARIMA} : \\underbrace{(p,d,q)}_{\\text{Non-seasonal part}}  \\underbrace{(P,D,Q),m}_{\\text{Seasonal part}}$$\nwhere $m$ = number of observations per season. The additional seasonal terms are simply multiplied by the non-seasonal terms.\n\n","16947326":"## 1.2) Building AR(p=2) model","7ad2868d":"From Dickey-Fuller test, we can conclude that the data is non-stationary. Additionally, from ETS plot we clearly see the **annual seasonality** indicating that we should use $m = 12$  ","8ea3c764":"## 1.1) Building AR(p=1) model","b00ccef7":"Obviously, we see the upward trend. Thus, this is non-stationary data, and we can not use ARMA model. Instead, we'll use ARIMA model."}}