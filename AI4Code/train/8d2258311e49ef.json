{"cell_type":{"b8aabfd3":"code","3ba62338":"code","c1ae32d3":"code","7c5b1762":"code","69662a46":"code","1a59bd75":"code","91597647":"code","7e6e1576":"code","9e04def6":"code","9954d5de":"code","3b869969":"code","eea90c0a":"code","c5976a87":"code","881d6112":"code","a6c4971a":"code","751da0d3":"code","7d6e8ce5":"code","a61f97cf":"code","6ac5dea9":"code","2eff0766":"code","6fcf39e2":"code","d5bad1ba":"code","b48d3b52":"code","635f8286":"code","bba04bd8":"code","eac880c5":"code","06f03c7f":"code","efaddfd8":"code","468e7557":"code","4927831a":"code","a50e2b70":"code","1cd350f7":"code","3ea36688":"code","234ee9b6":"code","16fa24fe":"code","86f079cf":"code","09c0f940":"code","804280a5":"code","09b6df7d":"code","55588551":"code","9ce38758":"code","4fd144c2":"code","d204e835":"code","e1ad9e5f":"code","8c1115bc":"code","c3f9e038":"code","74debc8c":"code","7208af80":"code","9f0d8a8d":"code","9b5dca7a":"code","b454d1c4":"code","45145510":"code","e3d918b3":"code","846c7e31":"code","6f92cf24":"code","dd61210f":"code","a0019e2f":"code","9c5dc107":"code","19713d76":"code","3856d023":"code","870ea661":"code","1660360b":"code","3c2b335d":"code","91e7cb61":"code","c05a149a":"code","dc38488d":"code","346cca89":"code","ed5ac22f":"code","46cc6603":"code","2fed7bf9":"code","1f9e7c4e":"code","f2265006":"code","cc5758c1":"code","d71c9fa8":"code","9f9ff12a":"code","c9a831d9":"code","4b9b42ce":"code","610d059f":"code","a5778d41":"code","b5b692eb":"code","db2b09c1":"code","bba47618":"code","df8c8be7":"code","f44d2d70":"markdown","25cfeb1f":"markdown","da1f3a0f":"markdown","8ff0e4d7":"markdown","17727e03":"markdown","987512f2":"markdown","5b6501c6":"markdown","6d5e685d":"markdown","eca83567":"markdown","ce175be4":"markdown","27207221":"markdown","9555c37e":"markdown","3985ec6d":"markdown","d743df11":"markdown","ccca03c9":"markdown","97d0e1e6":"markdown","a31fc4b5":"markdown","de0662a4":"markdown","92518d25":"markdown","eb80606e":"markdown","dbca2a78":"markdown","d8380e0d":"markdown","15949faa":"markdown","3fac5e23":"markdown","3e33e9c5":"markdown","341f89b3":"markdown","1b279279":"markdown","ad18ec53":"markdown","ab056876":"markdown","82725e8a":"markdown","e35f23e7":"markdown","8f434864":"markdown","3557b3eb":"markdown","c0935843":"markdown","2f0a0edd":"markdown","1dfc89d6":"markdown","0febacfc":"markdown","0919805a":"markdown","7c120f01":"markdown","e4cbf7a6":"markdown","e6d2fa30":"markdown","82da323c":"markdown","819cfdd1":"markdown","73d7ab94":"markdown","1c0bff92":"markdown","295ac0a0":"markdown","72348e4c":"markdown","b8551138":"markdown","e07ea676":"markdown","9e3e2866":"markdown","26260d2a":"markdown","6f0bc058":"markdown","6bd7754f":"markdown","f5f1fb00":"markdown","cff024a3":"markdown","c088ed2a":"markdown","2e08f038":"markdown"},"source":{"b8aabfd3":"#Visualization and display option packages\n%matplotlib inline\nfrom IPython.display import display\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime as dt \nfrom datetime import timedelta, date\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn import tree\nsns.set(font_scale=2)\n\n#Math\nimport numpy as np\n\n#Table manipulation\nimport pandas as pd \n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n#Model Selection and parameter tuning\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nimport sklearn.model_selection as cv\n\n# import the metrics class\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\n\n#Supress deprecated\/future warnings \nimport warnings\ndef fxn():\n    warnings.warn(\"deprecated\", DeprecationWarning)\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    fxn()","3ba62338":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","c1ae32d3":"#Import data\ntrain = pd.read_csv('\/kaggle\/input\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/test.csv')","7c5b1762":"train.head()","69662a46":"#Basic info on train set\ntrain.info()","1a59bd75":"#Summary stats for numerical features\ntrain.describe()","91597647":"#Summary stats for categorical features\ntrain.describe(include=['O'])","7e6e1576":"g = sns.countplot(x='Survived', data=train)\n\n\nprint('Mean Survival Rate: ', str(round(train.Survived.mean()*100,2))+'%')","9e04def6":"g = sns.catplot(x = 'Survived', col = 'Sex', kind = 'count', data = train)\ng.set_xticklabels([\"No\", \"Yes\"])\ng.set_titles(\"{col_name}\", fontsize = 50)\n\n\n#Mean survival rate by sex\nprint('Mean Survival Rate (%): \\n', train.groupby('Sex')['Survived'].mean()*100)","9954d5de":"sns.catplot(x = 'Survived', col = 'Pclass', kind = 'count', data = train)\n\ng.set_xticklabels([\"No\", \"Yes\"])\ng.set_titles(\"{col_name}\", fontsize = 50)\n\n\n#Mean survival rate by sex\nprint('Mean Survival Rate (%): \\n', train.groupby('Pclass')['Survived'].mean()*100)","3b869969":"g = sns.FacetGrid(train, col='Survived', height = 10)\ng.map(plt.hist, 'Age', bins=20)\ng.set_titles(\"Survived = {col_name}\", fontsize = 50)\n","eea90c0a":"g = sns.catplot(x=\"Pclass\", y ='Survived', hue=\"Embarked\", kind=\"point\", edgecolor=\".6\",\n            data=train.groupby(['Pclass', 'Embarked']).mean().reset_index(), height = 7)\n\ng.set_titles(\"Survived\", fontsice = 50)","c5976a87":"#Create copy to change sex column to a boolean 'female': 1, 'male': 0\ntrain_copy = train.copy()\ntrain_copy['Sex'] = train_copy['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n\n#Plot pairplot of numerical components \nsns.pairplot(train_copy.iloc[:, [1,2,4,5,6,7,9]], hue = 'Survived', markers=[\"o\", \"s\"])","881d6112":"#Combine train and test for feature engineering \ntrain_test_df = [train, test]","a6c4971a":"for dataset in train_test_df:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\ntrain['Title'].value_counts()","751da0d3":"#Reduce the number of groups to 5 groups\nfor dataset in train_test_df:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ntrain[['Title', 'Survived']].groupby(['Title'], as_index=False).mean().sort_values(by = 'Survived',ascending = False)","7d6e8ce5":"#Change to ordinal \ntitle_mapping = {\"Mr\": 0, \"Miss\": 1, \"Mrs\": 2, \"Master\": 3, \"Rare\": 4}\nfor dataset in train_test_df:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n\ntrain.head()","a61f97cf":"#Drop the Name column from both datasets\nfor dataset in train_test_df:\n    dataset.drop('Name', axis = 1, inplace = True)\n\ntrain_test_df = [train, test]\ntrain.head()","6ac5dea9":"for dataset in train_test_df:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n\ntrain_test_df = [train, test]\n\ntrain.head()","2eff0766":"#TRAIN MISSING VALUES\ntotal_num = train.isnull().sum().sort_values(ascending=False)\nperc = train.isnull().sum()\/train.isnull().count() *100\nperc1 = (round(perc,2).sort_values(ascending=False))\n\n# Creating a data frame:\ntrain_null = pd.concat([total_num, perc1], axis =1 , keys =[\"Total Missing Values\", \"Percentage %\"]).sort_values(by =\"Percentage %\", ascending = False)\n\n#Top nulls\ntop_null = train_null[train_null[\"Percentage %\"]>0]\ntop_null.reset_index(inplace=True)\ntop_null","6fcf39e2":"#Visualising missing data \n\nsns.set(font_scale = 1)\nfig, axes = plt.subplots(2,1, figsize = (15,12))\nsns.heatmap(train.isnull(), cbar = False, yticklabels=False, cmap=\"magma\", ax=axes[0])\nsns.heatmap(test.isnull(), cbar = False, yticklabels=False, cmap=\"magma\", ax=axes[1])\n\n#Reset to 2\nsns.set(font_scale = 2)","d5bad1ba":"# grid = sns.FacetGrid(train_df, col='Pclass', hue='Gender')\ng = sns.FacetGrid(train, row='Title', height=4, aspect=1.6)\ng.map(plt.hist, 'Age', alpha=.5, bins=20)\ng.add_legend()\ng.set_ylabels('Count')","b48d3b52":"train.groupby(['Title'])['Age'].median()","635f8286":"for dataset in train_test_df:\n    dataset['Age'] = dataset.groupby(['Title'])['Age'].transform(lambda x: x.fillna(x.median()))","bba04bd8":"#Let us create Age bands and determine correlations with Survived.\ntrain['AgeBand'] = pd.cut(train['Age'], 5)\ntrain.groupby(['AgeBand'])['Survived'].mean().sort_values(ascending=True).to_frame().style.background_gradient(cmap='summer_r')","eac880c5":"#Let us replace Age with ordinals based on these bands.\nfor dataset in train_test_df:    \n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4\n    dataset['Age'] = dataset['Age'].astype(int)\n    \n    train.head()","06f03c7f":"#Drop ageband column from train dataset\ntrain = train.drop('AgeBand', axis=1)\n\ntrain_test_df = [train, test]","efaddfd8":"g = sns.catplot(x = 'Pclass',  y ='Survived', hue=\"Age\", kind=\"point\", edgecolor=\".6\",\n            data=train.groupby(['Pclass', 'Age']).mean().reset_index(), height = 7)\n\ng.set_titles(\"Survived\", fontsice = 50)","468e7557":"for dataset in train_test_df:\n    dataset['Age*Class'] = dataset.Age * dataset.Pclass\n    dataset['Age'] = dataset['Age'].astype(int)\n    dataset['Age*Class'] = dataset['Age*Class'].astype(int)\n    \ntrain.loc[:, ['Age*Class', 'Age', 'Pclass']].head(10)","4927831a":"train_test_df = [train, test]\n\nfor dataset in train_test_df:\n    dataset['is_child'] = dataset['Age'].apply(lambda x: 1 if x == 0 else 0)\n\ntrain_test_df = [train, test]\n\ntrain.head()","a50e2b70":"train.groupby(['is_child'])['Survived'].mean().sort_values(ascending=False).to_frame().style.background_gradient(cmap='summer_r')","1cd350f7":"mode_embarked = train.Embarked.dropna().mode()[0]\nmode_embarked ","3ea36688":"for dataset in train_test_df:\n    dataset['Embarked'] = dataset['Embarked'].fillna(mode_embarked)\n    \ntrain.groupby(['Embarked'])['Survived'].mean().sort_values(ascending=False).to_frame().style.background_gradient(cmap='summer_r')","234ee9b6":"#The null entry in \ntest[test.Fare.isnull()] #Pclass = 3","16fa24fe":"test['Fare'] = test['Fare'].fillna(test.loc[test.Pclass == 3 ,'Fare'].dropna().median())","86f079cf":"train['Fare_Band']=pd.qcut(train['Fare'],4)\ntrain.groupby(['Fare_Band'])['Survived'].mean().to_frame().style.background_gradient(cmap='summer_r')","09c0f940":"#Convert the Fare feature to ordinal values based on the FareBand.\n\nfor dataset in train_test_df:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n\ntrain = train.drop('Fare_Band', axis=1)\ntrain_test_df = [train, test]\n    \ntrain.head(10)","804280a5":"#Create new feature combining existing features\n\n#We can create a new feature for FamilySize which combines Parch and SibSp. \n#This will enable us to drop Parch and SibSp from our datasets.\n\nfor dataset in train_test_df:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch']\n\ntrain[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","09b6df7d":"#We can group FamilySize into four groups \n\nfor dataset in train_test_df:\n    dataset.loc[(dataset['FamilySize'] >= 1) & (dataset['FamilySize'] <= 2), 'FamilySize'] = 1\n    dataset.loc[dataset['FamilySize'] == 3, 'FamilySize'] = 2\n    dataset.loc[dataset['FamilySize'] >= 4, 'FamilySize'] = 3\n    \ntrain.groupby(['FamilySize'])['Survived'].mean().sort_values(ascending=False).to_frame().style.background_gradient(cmap='summer_r')","55588551":"g = sns.catplot(x = 'FamilySize', y=\"Survived\", kind=\"bar\", edgecolor=\".6\",\n            data=train.groupby(['FamilySize']).mean().reset_index(), height = 7)\n","9ce38758":"#Let us drop Parch, SibSp features in favor of Family Size\n\nfor dataset in train_test_df:\n    dataset.drop(['Parch', 'SibSp'], axis=1, inplace = True)\n\ntrain_test_df = [train, test]\n\ntrain.head()","4fd144c2":"train_test_df = [train, test]\n#We can create another feature called IsAlone.\nfor dataset in train_test_df:\n    dataset['Alone'] = 0\n    dataset.loc[dataset['FamilySize'] == 0, 'Alone'] = 1\n\ntrain[['Alone', 'Survived']].groupby(['Alone'], as_index=False).mean()","d204e835":"for dataset in train_test_df:\n    dataset['Has_Cabin'] = ~dataset.Cabin.isnull()\n    dataset['Has_Cabin'] = dataset['Has_Cabin'].astype(int)","e1ad9e5f":"#Let us drop Cabin  in favor of Has_Cabin\nfor dataset in train_test_df:\n    dataset.drop('Cabin', axis=1, inplace = True)\n\ntrain_test_df = [train, test]\ntrain.head()\ntrain.head()","8c1115bc":"print(train[['Has_Cabin', 'Survived']].groupby(['Has_Cabin'], as_index=False).mean().sort_values(by='Survived', ascending=False))\n\ndisplay(g = sns.catplot(x = 'Has_Cabin', y=\"Survived\", kind=\"bar\", edgecolor=\".6\",\n            data=train.groupby(['Has_Cabin']).mean().reset_index(), height = 7))","c3f9e038":"g = sns.catplot(x = 'Has_Cabin', y=\"Survived\",hue = 'Pclass', kind=\"bar\", edgecolor=\".6\",\n            data=train, height = 7)","74debc8c":"for dataset in train_test_df:\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n\ntrain.head()","7208af80":"train_test_df = [train, test]\n\nfor dataset in train_test_df:\n    dataset['Embarked*Pclass'] = dataset['Embarked']*dataset['Pclass']\n\ntrain.head()","9f0d8a8d":"#looking at the percentage of unique values for ticket in both datasets\n\nprint('Percent of unique ticket values in train: ',round(train.Ticket.nunique()\/len(train)*100,2), '%')\nprint('Percent of unique ticket values in test: ', round(test.Ticket.nunique()\/len(test)*100,2),'%')","9b5dca7a":"for dataset in train_test_df:\n    dataset.drop('Ticket', axis = 1, inplace = True)\n\ntrain_test_df = [train, test]\n\ntrain.head()","b454d1c4":"train = train.drop('PassengerId', axis = 1)","45145510":"# extract target variable (interest rate) from training data\ntarget = train[\"Survived\"]\n\n# remove interest rate column from training data\npredictors = train.drop(\"Survived\", axis=1)","e3d918b3":"predictors.head()","846c7e31":"#SET SEED \nSEED = 1 # We will be using this seed for all models that include a random_state parameter.\n\n# Split dataset into 80% train and 20% test\nX_train, X_test, Y_train, Y_test = \\\ntrain_test_split(predictors, target,\ntest_size=0.2,\nrandom_state=SEED)","6f92cf24":"#TRAIN Accuracy score\nlogreg = LogisticRegression(C=4, solver= 'lbfgs', random_state=SEED)\nlogreg.fit(X_train, Y_train)\nY_pred_train = logreg.predict(X_train)\nacc_log_train = round(logreg.score(X_train, Y_train) * 100, 2)\n\n\nprint(metrics.classification_report(Y_train, Y_pred_train))\nprint(\"Accuracy:\",acc_log_train,'%') #Total accuracy ","dd61210f":"#TEST Accuracy score\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_test, Y_test) * 100, 2)\n\n\nprint(metrics.classification_report(Y_test, Y_pred))\nprint(\"Accuracy:\",acc_log,'%') #Total accuracy ","a0019e2f":"cnf_matrix = metrics.confusion_matrix(Y_test, Y_pred)\n\n# create heatmap\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","9c5dc107":"#Logistic coefficients \ncoeff_df = pd.DataFrame(train.columns.delete(0))\ncoeff_df.columns = ['Feature']\ncoeff_df[\"Correlation\"] = pd.Series(logreg.coef_[0])\n\ncoeff_df.sort_values(by='Correlation', ascending=False)","19713d76":"#TRAIN Accuracy score\nsvc = SVC(gamma=0.01,C=100, random_state=SEED)\nsvc.fit(X_train, Y_train)\nY_pred_train = svc.predict(X_train)\nacc_svc_train = round(svc.score(X_train, Y_train) * 100, 2)\n\nprint(metrics.classification_report(Y_train, Y_pred_train))\nprint(\"Accuracy:\",acc_svc_train,'%') #Total accuracy ","3856d023":"#TEST Accuracy score\nsvc.fit(X_train, Y_train)\nY_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_test, Y_test) * 100, 2)\n\nprint(metrics.classification_report(Y_test, Y_pred))\nprint(\"Accuracy:\",acc_svc,'%') #Total accuracy ","870ea661":"# Set the parameters by cross-validation\n#tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-2, 1e-3, 1e-4, 1e-5],\n#                     'C': [0.001, 0.10, 0.1, 10, 25, 50, 100, 1000]},\n#                    {'kernel': ['sigmoid'], 'gamma': [1e-2, 1e-3, 1e-4, 1e-5],\n#                     'C': [0.001, 0.10, 0.1, 10, 25, 50, 100, 1000]},\n#                    {'kernel': ['linear'], 'C': [0.001, 0.10, 0.1, 10, 25, 50, 100, 1000]}\n#                   ]\n\n#scores = ['precision', 'recall']\n\n#for score in scores:\n#    print(\"# Tuning hyper-parameters for %s\" % score)\n#    print()\n\n#    clf = GridSearchCV(SVC(C=1), tuned_parameters, cv=5,\n#                       scoring='%s_macro' % score)\n#    clf.fit(X_train, Y_train)\n#\n#    print(\"Best parameters set found on development set:\")\n#    print()\n#    print(clf.best_params_)\n#    print()\n#    print(\"Grid scores on development set:\")\n#    print()\n#    means = clf.cv_results_['mean_test_score']\n#    stds = clf.cv_results_['std_test_score']\n#    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n#        print(\"%0.3f (+\/-%0.03f) for %r\"\n#              % (mean, std * 2, params))\n#    print()","1660360b":"cnf_matrix = metrics.confusion_matrix(Y_test, Y_pred)\n\n# create heatmap\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","3c2b335d":"#TRAIN Accuracy score\nknn = KNeighborsClassifier(n_neighbors = 5)\nknn.fit(X_train, Y_train)\nY_pred_train = knn.predict(X_train)\nacc_knn_train = round(knn.score(X_train, Y_train) * 100, 2)\n\nprint(metrics.classification_report(Y_train, Y_pred_train))\nprint(\"Accuracy:\",acc_knn_train,'%') #Total accuracy ","91e7cb61":"#TEST Accuracy score\nknn.fit(X_train, Y_train)\nY_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_test, Y_test) * 100, 2)\n\nprint(metrics.classification_report(Y_test, Y_pred))\nprint(\"Accuracy:\",acc_knn,'%') #Total accuracy ","c05a149a":"cnf_matrix = metrics.confusion_matrix(Y_test, Y_pred)\n\n# create heatmap\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","dc38488d":"#TRAIN Accuracy score\ndecision_tree = DecisionTreeClassifier(max_depth=6, ## Model complexity\n                           min_samples_leaf=20,\n                           random_state=SEED, criterion='entropy')\ndecision_tree.fit(X_train, Y_train)\nY_pred_train = decision_tree.predict(X_train)\nacc_decision_tree_train = round(decision_tree.score(X_train, Y_train) * 100, 2)\n\nprint(metrics.classification_report(Y_train, Y_pred_train))\nprint(\"Accuracy:\",acc_decision_tree_train,'%') #Total accuracy ","346cca89":"#TEST Accuracy score\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_test, Y_test) * 100, 2)\n\nprint(metrics.classification_report(Y_test, Y_pred))\nprint(\"Accuracy:\",acc_decision_tree,'%') #Total accuracy ","ed5ac22f":"cnf_matrix = metrics.confusion_matrix(Y_test, Y_pred)\n\n# create heatmap\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","46cc6603":"from sklearn.tree import DecisionTreeClassifier, export_graphviz\nfrom sklearn import tree\nfrom sklearn.datasets import load_wine\nfrom IPython.display import SVG\nfrom graphviz import Source\nfrom IPython.display import display\n\ngraph = Source(tree.export_graphviz(decision_tree, out_file=None\n   , feature_names=X_train.columns, class_names=['0', '1'] \n   , filled = True))\ndisplay(SVG(graph.pipe(format='svg')))","2fed7bf9":"#TRAIN Accuracy score\nrandom_forest = RandomForestClassifier(n_estimators=100, \n                                       min_samples_leaf=25,\n                                       min_samples_split=0.1,\n                                       random_state=SEED, \n                                       criterion='gini')\nrandom_forest.fit(X_train, Y_train)\nY_pred_train = random_forest.predict(X_train)\nacc_random_forest_train = round(random_forest.score(X_train, Y_train) * 100, 2)\n\nprint(metrics.classification_report(Y_train, Y_pred_train))\nprint(\"Accuracy:\",acc_random_forest_train,'%') #Total accuracy ","1f9e7c4e":"#TEST Accuracy score\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nacc_random_forest = round(random_forest.score(X_test, Y_test) * 100, 2)\n\nprint(metrics.classification_report(Y_test, Y_pred))\nprint(\"Accuracy:\",acc_random_forest,'%') #Total accuracy ","f2265006":"cnf_matrix = metrics.confusion_matrix(Y_test, Y_pred)\n\n# create heatmap\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","cc5758c1":"import pandas as pd\nimport matplotlib.pyplot as plt\n# Create a pd.Series of features importances\nimportances_rf = pd.Series(random_forest.feature_importances_,\nindex = X_train.columns)\n# Sort importances_rf\nsorted_importances_rf = importances_rf.sort_values()\n# Make a horizontal bar plot\nsorted_importances_rf.plot(kind='barh', color='blue')\nplt.show()","d71c9fa8":"from sklearn.model_selection import cross_val_score\n\nlogref_score = cross_val_score(logreg, X_train, Y_train, cv = 5, scoring='balanced_accuracy')\n\nlogref_score_mean = round(logref_score.mean() * 100, 2)\n\nprint('Logreg Accuracy : {:.2f}% (+\/- {:.3f}%)'.format(logref_score_mean, logref_score.std()*100*1.96))","9f9ff12a":"KNN_score = cross_val_score(knn, X_train, Y_train, cv = 5, scoring='balanced_accuracy')\nKNN_score_mean = round(KNN_score.mean()*100, 2)\n\nprint('KNN Accuracy : {:.2f}% (+\/- {:.3f}%)'.format(KNN_score_mean, KNN_score.std()*100*1.96))","c9a831d9":"RF_score = cross_val_score(random_forest, X_train, Y_train, cv = 5, scoring='balanced_accuracy')\nRF_score_mean = round(RF_score.mean()*100, 2)\n\nprint('RF Accuracy : {:.2f}% (+\/- {:.3f}%)'.format(RF_score_mean, RF_score.std()*100*1.96))","4b9b42ce":"svc_score = cross_val_score(svc, X_train, Y_train, cv = 5, scoring='balanced_accuracy')\nsvc_score_mean = round(svc_score.mean()*100, 2)\n\nprint('SVM Accuracy : {:.2f}% (+\/- {:.3f}%)'.format(svc_score_mean, svc_score.std()*100*1.96))","610d059f":"DT_score = cross_val_score(decision_tree, X_train, Y_train, cv = 5, scoring='balanced_accuracy')\nDT_score_mean = round(DT_score.mean()*100, 2)\n\nprint('DT Accuracy : {:.2f}% (+\/- {:.3f}%)'.format(DT_score_mean, DT_score.std()*100*1.96))","a5778d41":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', \n              'Decision Tree'],\n    'Train Score': [acc_svc_train, acc_knn_train, acc_log_train, \n              acc_random_forest_train, acc_decision_tree_train],\n    'Test Score': [acc_svc, acc_knn, acc_log, \n              acc_random_forest, acc_decision_tree],\n    'CV Score': [svc_score_mean, KNN_score_mean, logref_score_mean,\n                RF_score_mean, DT_score_mean]})\n\nmodels.sort_values(by='CV Score', ascending=False).style.background_gradient(cmap='RdYlGn')","b5b692eb":"holdout_test = test.drop(['PassengerId'], axis =1)\n\nholdout_test.head()","db2b09c1":"# Support Vector Machines\nsvc = SVC(gamma=0.01,C=100, kernel='rbf')\nsvc.fit(predictors, target)\nY_pred = svc.predict(holdout_test)","bba47618":"submission = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })","df8c8be7":"#Save result to CSV\n#submission.to_csv('submission.csv', index=False)","f44d2d70":"# Create has cabin indicator\n\nAlthough we see that 77.10% of cabin rows are NaN in our training set, this could be indicative of the fact that these passengers were not in a superiod class to have their cabin recorded. Let us create a dummy variable to track whether the passenger had a cabin recorded or not. ","25cfeb1f":"# Logistic Regression\n\nAfter applying some hyperparameter tuning to the Inverse of regularization strength parameter (C) the best value that balanced Test\/train accuracy was 4.","da1f3a0f":"## Let us visualize our decision tree to get an idea of how our classification is being made","8ff0e4d7":"# Evaluating all models accuracy","17727e03":"## Convert Embarked feature to numeric","987512f2":"**Based on this decision tree it is clear that title seems to be playing the most important role here, given it is the root node. Other important variables include 'Has_Cabin', 'Pclass' & 'Sex'.**","5b6501c6":"### Mapping titles categories to numeric\n#### Title mapping Mr =  0, Miss =  1, Mrs = 2, Master =  3, Rare = 4","6d5e685d":"### Now that we have obtained our own accuracy metrics using train-test split, let us check what scores we get from CV. In theory, these scores should give us a more representative and unbiased measure of our accuracy since we are using K number of train-test splits and averaging their results. ","eca83567":"Looking at survival rates by class and embarked\/port it looks like there may some interaction between these two variables. It is clear that Southhampton had a lower survival rate on average than the other locations. ","ce175be4":"#### Survival by Sex","27207221":"## Add embarked * Pclass interaction term\n\nAs our EDA showed earlier, there seems to be an interaction between the embarked location and pclass. Let us highlight this to our models by creating an interaction term between the two.","9555c37e":"# Random Forest","3985ec6d":"There seems to be quite significant survival rates just based on the passengers title alone!\n\nThese values seem to confirm some of our previous observations, with women having higher survival than men & wealthier sounding titles also surviving better than regular sounding ones. ","d743df11":"It looks like the survival rate in the train set was quite low, overall 38.38%.","ccca03c9":"# Finding interaction terms","97d0e1e6":"### Load Packages","a31fc4b5":"### Fill NAs of Fare in test with the median\n\nThere is only one value missing, we can resort to just filling this fare value with the median of its Pclass.","de0662a4":"### Exploratory Data Analysis","92518d25":"### Variable Definitions:\n![image.png](attachment:image.png)","eb80606e":"# Support Vector Machines\n\nSimilarly for SVM, after applying some hyperparameter tuning to the optimal gamma and C values were used in our models. The code for parameter optimazation was included below. ","dbca2a78":"### Fill NAs of Embarked with mode (most frequent)","d8380e0d":"Wow, there are many titles on this ship. Let us reduce these to five groups (Miss, Mrs, Master, Mr & Rare). ","15949faa":"# Create Alone dummy variable","3fac5e23":"### Interesting insights from the coefficients of the logistic regression\n\n1. Sex is highest positivie coefficient, implying as the Sex value increases (male: 0 to female: 1), the probability of Survived=1 increases the most.\n<br>\n     \n2. Title has a large positive coefficient. Looking at the order of our variables this also makes sense as the larger indicators values for Miss(1)\/Mrs(2)\/Master(3)\/Rare(4) should have a better chance of survivival than Mr(0). Since the first four are either indicative of being women\/wealthy and the last one is indicative of being an older man.\n<br>\n\n3. As Pclass increases, probability of Survived=1 decreases. Which makes sense since we saw lower class passengers had lower survival rates. \n<br>\n\n4. Alone seems to be a good feature to have included in the model as it has second highest negative correlation with Survived and suggests that people traveling alone were more likely to die. \n<br>\n\n5. is_child is also a good feature to have added as it is very positively correlated with survival as we saw from the EDA\n<br>\n\n6. Interestingly, Has_Cabin also provided a good predictor of survival. This agrees with the analysis we did previously that showed that those with non-null cabin had significantly higher survival rates.","3e33e9c5":"#### Let us start by preparing an empty array to contain guessed Age values based on Pclass x Gender combinations.\n\n","341f89b3":"#### Dropping the ticket column from both datasets","1b279279":"# Step 3: Apply Best Model to Holdout_Test set\n\nTest your models using the data found within the \"Holdout_testing\" file. Save the results of the final model (remember you will only predict the Survived column in holdout test set with your best model results) in a single, separate CSV titled \"Titanic Results from\" *insert your name or UChicago net ID.","ad18ec53":"Looking at the distribution of ages between the survived\/dead group it looks like the survived distribution has a stronger left skew, suggesting that young people represent a large proportion of those who survived than those who didn't.","ab056876":"It looks like the main missing values in our datasets are age and cabin. With a couple of values for embarked and fare. \n\nLet us take care of those now. ","82725e8a":"# Final Write Up \n\nGiven this was a classification problem I chose to use the main supervised algorithms I am currently familiar with. The results from the five models used to classify the titanic survivors was quite surprising. Firstly, I expected that the logisitic regression would not perform as well as it did because it assumes a linear relationship between the independent and dependent variables, homoskedastic errors  and no collinearity between independent variables. Many of these assumptions were violated, yet the model still outperformed more complex non-linear models. \n\nGiven the non-linear aspects of the problem, I expected decision trees\/random forest to show superior performance. This is because DT\/RF require no significant preprocessing, no assumptions on distribution and automatically take care of colinearity. However they seemed to underperform. This may be due to having overly complex models which caused them to overfit. Nevertheless, I was unable to stablize the train\/test\/CV accuracy measures any further given my time constraint. \n\nSVM performed the best in my case, this was not suprising in retrospect as it is a non-linear model so it can handle many different features\/high dimensional data. Furthermore, the kernel trick is real strength of SVM as it is very flexible classifier and does not solve for local optima. Lastly, the risk of over-fitting is relatively small in SVM. However, the disadvantage of this model is that it is difficult to understand and interpret the final model. Unlike logisitic regression or decision trees the variable weights and individual impact are not easy to extract and visualize. ","e35f23e7":"Similarly there are significant differences in the survival rate between ticket classes, with higher classes being more likely to survive. ","8f434864":"### Create FareBand to segment different fare values and create ordinal categorical variable","3557b3eb":"## Read the data ","c0935843":"#### Dropping the passengerID column from the train data (will keep it for test to show results)","2f0a0edd":"It looks like all three classes had some cabins registered but overall those with recorded cabins had higher overall survival rates across all classes.","1dfc89d6":"## Train-Test Split of training set to perform CV and measure accuracy.","0febacfc":"#### Survival by Pclass","0919805a":"## Plotting pairplot to look for other interesting relationships in our features.","7c120f01":"# K-Nearest Neighbors ","e4cbf7a6":"### Create FamilySize to replace sibsp and parch\n\nSince there wasnt significant relationship between each of these features with survival, let us combine them to try and get a more meaningful metric.","e6d2fa30":"### Converting Sex column to dummy for female\n#### Sex to boolean Female = 1 and Male = 0","82da323c":"### Survival Age distribution","819cfdd1":"# Final Mapping Dictionary\n\n#### Sex:\n- Female = 1\n- Male = 0\n\n#### Fare:\n- Fare <=7.91 : 0 \n- 7.91 < Fare <= 14.454 : 1\n- 14.454 < Fare <= 31 : 2\n- 31 < Fare : 3\n\n#### Age:  \n- Age <= 16 : 0 \n- 16 < Age <= 32 : 1\n- 32 < Age <= 48 : 2\n- 48 < Age <= 64 : 3\n- 64 < Age <= 64 : 4\n\n#### Embarked: \n- S =  0\n- C = 1\n- Q = 2\n\n#### Title:  \n- Mr =  0\n- Miss =  1 \n- Mrs = 2\n- Master =  3\n- Rare = 4\n\n#### is_child\t\n- Age <=16 : 1\n- Age > 16 : 0\n\n#### FamilySize\n- sum of SibSp + Parch\n\n#### Alone \n- = 1 if FamilySize = 0, 0 otherwise\n\n#### Has_Cabin \n- 1 for passengers with a cabin value and 0 otherwise","73d7ab94":"# Model Building ","1c0bff92":"## Dropping Columns","295ac0a0":"### Create title column from Name ","72348e4c":"#### Overall Survival (train set)","b8551138":"# Titanic Survival Classification - Nazih Kalo","e07ea676":"# Decision Tree","9e3e2866":"# Feature Engineering \n\nBased on the analysis above we can now move on to feature engineering to keep\/build features that seem to be useful.","26260d2a":"# Cross validation Scores\n\nWe will be using the **balanced accuracy** scoring metric for our cross validation. The reasoning for this is that balanced accuracy provides a better measure when **there is a large class imbalance**, which is the case here. There are a lot more people that didn't survive than those who did. A model can predict the value of the majority class for all predictions and achieve a high classification accuracy, the problem is that this model is not useful in the problem domain, so we will use balanced accuracy.","6f0bc058":"### Fill Age NA using median from Title\n","6bd7754f":"### Visualizing our missing data","f5f1fb00":"### Create interaction term between Age and Pclass \n\n##### Rationale for this is that the effect of Age varies with Pclass.  For example, older people in first class are more likely to survive than old people in lower classes.","cff024a3":"# Create is_child indicator","c088ed2a":"The survival rate was significantly higher among females with 78% vs. 18% for males. This suggests that sex will be an important feature for our model.","2e08f038":"This pairplot contains a lot of information about our features. \n\nFirst it confirms the idea that the age of survivors was lower overall (in the Age dist plot). \n\nSecond it looks like the fare among survivors has a stronger right skew (in the fare hist), this suggests that those who paid higher fares (and likely were in the higher classes) were more likely to survive. This makes sense given our analysis of pclass.\n\nThirdly, it looks like younger passengers tended to have higher more sibling and\/or spouses on the ship with them.\n\nLastly, there seems to be a small relationship between the SibSp and survival, with larger sibsp numbers resulting in higher survival odds. "}}