{"cell_type":{"208a1826":"code","35a818ff":"code","22ca607a":"code","e00bc178":"code","abec2c46":"code","92ca8b98":"code","4887b419":"code","9613fe2c":"code","f25d801b":"code","5781c922":"code","ef8dc0b5":"code","bba7e65c":"code","fc1b2d6b":"code","820497c9":"code","0e2228c1":"code","2e6715ba":"code","3cddd3af":"code","f1ee46eb":"code","4359858a":"code","48dfeeba":"code","bcadbd72":"code","4671a01f":"code","33384f9a":"code","b7e640ed":"code","46f90c46":"code","cba48a04":"code","c928ef6c":"code","6c692b40":"code","3c71e4fa":"code","b186abec":"code","7203bb2f":"code","2801fe22":"code","5790aa27":"code","e2c4050a":"code","56d53433":"code","99b7200d":"code","730e188c":"code","9c374f89":"code","0d234b38":"code","645bd932":"code","5e999c7a":"code","121c7f83":"code","78349d0e":"code","1951bbce":"code","5264667d":"code","31c1514a":"code","1cdcdf2e":"code","41b5cc3f":"markdown","fd999c2a":"markdown","7afb417a":"markdown","5f147084":"markdown","9c50d6ed":"markdown","4e02427f":"markdown","adde5457":"markdown","4ec8356f":"markdown","d09187e6":"markdown","1326d755":"markdown","fc28efb9":"markdown","aa26f418":"markdown","b9473445":"markdown","8cd88673":"markdown","2aac830b":"markdown","62d0e972":"markdown","8adaaeb9":"markdown","51bce871":"markdown","29a5dd4a":"markdown","a6875153":"markdown","1c1bee57":"markdown","487449a4":"markdown","e9fa3e66":"markdown","be9d1026":"markdown","21795b39":"markdown","175464b1":"markdown","de451ed6":"markdown","bbbad1fe":"markdown","0bb2683f":"markdown","30885c27":"markdown","5f3f045a":"markdown","40eb1eee":"markdown","2be9a074":"markdown","27b322a5":"markdown","44431aaa":"markdown","de752459":"markdown","a73bf15b":"markdown","cf83f106":"markdown","94edb001":"markdown","58406af6":"markdown","76bd024b":"markdown","e2d28a24":"markdown","014ce546":"markdown","b1825230":"markdown","0e348d2e":"markdown","effce4fe":"markdown","0f000df8":"markdown","202b3a1d":"markdown","9efe4c29":"markdown","2635cdb0":"markdown","ef25ef3d":"markdown","e52796ac":"markdown","3c5cc076":"markdown","1475d103":"markdown","a2b232d8":"markdown","b4a6d7c0":"markdown","f6a6cfda":"markdown","72b0bc21":"markdown","898719b2":"markdown","34e388af":"markdown","3a4d674f":"markdown","269ecd4b":"markdown","9aadfd42":"markdown","4f897706":"markdown","10318cea":"markdown","c76c680f":"markdown","94d059f1":"markdown","c3f0664b":"markdown","821535ca":"markdown","736e0cb2":"markdown","48d71d1b":"markdown","6ef62485":"markdown","9173fe05":"markdown","c2f0c1f4":"markdown","002c32c0":"markdown","c0b96df3":"markdown","31d3aafe":"markdown","d8a2e13c":"markdown","dd06e16d":"markdown","d2b81e92":"markdown","50f2a42b":"markdown","bee44fd0":"markdown","0acf27c7":"markdown","1eccbaec":"markdown","112e5d10":"markdown","0ec8fe69":"markdown","64d5bdf3":"markdown","d6cff593":"markdown"},"source":{"208a1826":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\ncolor=sns.color_palette()","35a818ff":"data=pd.read_csv('..\/input\/heart.csv')\ndata.head()","22ca607a":"fig,ax=plt.subplots(1, 2, figsize = (14,5))\nsns.countplot(data=data, x='target', ax=ax[0],palette='Set2')\nax[0].set_xlabel(\"Disease Count \\n [0]->No [1]->Yes\")\nax[0].set_ylabel(\"Count\")\nax[0].set_title(\"Heart Disease Count\")\ndata['target'].value_counts().plot.pie(explode=[0.1,0.0],autopct='%1.1f%%',ax=ax[1],shadow=True, cmap='Greens')\nplt.title(\"Heart Disease\")","e00bc178":"fig,ax=plt.subplots(1,2,figsize=(14,5))\nsns.countplot(x='sex',data=data,hue='target',palette='Set1',ax=ax[0])\nax[0].set_xlabel(\"0 ->Female , 1 ->Male\")\ndata.sex.value_counts().plot.pie(ax=ax[1],autopct='%1.1f%%',shadow=True, explode=[0.1,0], cmap='Reds')\nax[1].set_title(\"0 ->Female , 1 -> Male\")","abec2c46":"fig,ax=plt.subplots(1,2,figsize=(14,5))\nsns.countplot(x='fbs',data=data,hue='target',palette='Set3',ax=ax[0])\nax[0].set_xlabel(\"0-> fps <120 , 1-> fps>120\",size=12)\ndata.fbs.value_counts().plot.pie(ax=ax[1],autopct='%1.1f%%',shadow=True, explode=[0.1,0],cmap='Oranges')\nax[1].set_title(\"0 -> fps <120 , 1 -> fps>120\",size=12)","92ca8b98":"fig,ax=plt.subplots(1,2,figsize=(14,5))\nsns.countplot(x='restecg',data=data,hue='target',palette='Set3',ax=ax[0])\nax[0].set_xlabel(\"resting electrocardiographic\",size=12)\ndata.restecg.value_counts().plot.pie(ax=ax[1],autopct='%1.1f%%',shadow=True,\n                                     explode=[0.005,0.05,0.05],cmap='Blues')\nax[1].set_title(\"resting electrocardiographic\",size=12)","4887b419":"fig,ax=plt.subplots(1,2,figsize=(14,5))\nsns.countplot(x='slope',data=data,hue='target',palette='Set1',ax=ax[0])\nax[0].set_xlabel(\"peak exercise ST segment\",size=12)\ndata.slope.value_counts().plot.pie(ax=ax[1],autopct='%1.1f%%',shadow=True,explode=[0.005,0.05,0.05],cmap='Blues')\n\nax[1].set_title(\"peak exercise ST segment \",size=12)","9613fe2c":"fig,ax=plt.subplots(1,2,figsize=(14,5))\nsns.countplot(x='ca',data=data,hue='target',palette='Set2',ax=ax[0])\nax[0].set_xlabel(\"number of major vessels colored by flourosopy\",size=12)\ndata.ca.value_counts().plot.pie(ax=ax[1],autopct='%1.1f%%',shadow=True,cmap='Oranges')\nax[1].set_title(\"number of major vessels colored by flourosopy\",size=12)","f25d801b":"fig,ax=plt.subplots(1,2,figsize=(14,5))\nsns.countplot(x='thal',data=data,hue='target',palette='Set2',ax=ax[0])\nax[0].set_xlabel(\"number of major vessels colored by flourosopy\",size=12)\ndata.thal.value_counts().plot.pie(ax=ax[1],autopct='%1.1f%%',shadow=True,cmap='Greens')\nax[1].set_title(\"number of major vessels colored by flourosopy\",size=12)","5781c922":"fig,ax=plt.subplots(1,2,figsize=(14,5))\nsns.countplot(x='cp',data=data,hue='target',palette='Set3',ax=ax[0])\nax[0].set_xlabel(\"Chest Pain\")\ndata.cp.value_counts().plot.pie(ax=ax[1],autopct='%1.1f%%',explode=[0.01,0.01,0.01,0.01],shadow=True, cmap='Blues')\nax[1].set_title(\"Chest pain\")","ef8dc0b5":"fig,ax=plt.subplots(2,2,figsize=(14,10))\nsns.boxplot(y='trestbps',data=data,x='sex',hue='target',palette='Set2',ax=ax[0,0])\nax[0,0].set_title(\"Trestbps V\/S Sex\")\nsns.factorplot(y='trestbps',data=data,x='cp',hue='target',ax=ax[0,1],palette='Set2')\nax[0,1].set_title(\"Trestbps V\/S Chest Pain\")\nsns.violinplot(y='trestbps',data=data,x='exang',hue='target',ax=ax[1,0],palette='Set2')\nax[1,0].set_title(\"Trestbps V\/S Exang\")\nsns.swarmplot(y='trestbps',data=data,x='ca',hue='target',ax=ax[1,1],palette='Set2')\nax[1,1].set_title(\"Trestbps V\/S CA (Major Vessel Coloured)\")","bba7e65c":"fig,ax=plt.subplots(2,2,figsize=(14,10))\nsns.boxplot(y='chol',data=data,x='sex',hue='target',palette='Set3',ax=ax[0,0])\nax[0,0].set_title(\"Cholestrol V\/S Sex\")\nsns.boxplot(y='chol',data=data,x='cp',hue='target',ax=ax[0,1],palette='Set3')\nax[0,1].set_title(\"Cholestrol V\/S Chest Pain\")\nsns.swarmplot(y='chol',data=data,x='thal',hue='target',ax=ax[1,0],palette='Set3')\nax[1,0].set_title(\"Cholestrol V\/S Thal\")","fc1b2d6b":"fig,ax=plt.subplots(2,2,figsize=(14,10))\nsns.boxplot(y='oldpeak',data=data,x='sex',hue='target',palette='Set1',ax=ax[0,0])\nax[0,0].set_title(\"oldpeak V\/S Sex\")\nsns.boxplot(y='oldpeak',data=data,x='cp',hue='target',ax=ax[0,1],palette='Set1')\nax[0,1].set_title(\"oldpeak V\/S Chest Pain\")\nsns.swarmplot(y='oldpeak',data=data,x='thal',hue='target',ax=ax[1,0],palette='Set1')\nax[1,0].set_title(\"oldpeak V\/S Thal\")\nsns.factorplot(y='oldpeak',data=data,x='ca',hue='target',ax=ax[1,1],palette='Set1')\nax[1,1].set_title(\"oldpeak V\/S CA\")","820497c9":"fig,ax=plt.subplots(4,3,figsize=(15,15))\nfor i in range(12):\n    plt.subplot(4,3,i+1)\n    sns.distplot(data.iloc[:,i],kde=True, color='blue')","0e2228c1":"fig,ax=plt.subplots(1,1,figsize=(15,5))\nfeatures = data.columns\nsns.distplot(data[features].mean(axis=1),kde=True,bins=30,color='red')","2e6715ba":"fig,ax=plt.subplots(1,1,figsize=(15,5))\nfeatures = data.columns\nsns.distplot(data[features].std(axis=1),kde=True,bins=30,color='green')","3cddd3af":"fig,ax=plt.subplots(figsize=(15,5))\nsns.heatmap(data.isnull(), annot=True)","f1ee46eb":"fig=plt.figure(figsize=(18,18))\nsns.heatmap(data.corr(), annot= True, cmap='Blues')","4359858a":"data.sex=data.sex.astype('category')\ndata.cp=data.cp.astype('category')\ndata.fbs=data.fbs.astype('category')\ndata.restecg=data.restecg.astype('category')\ndata.exang=data.exang.astype('category')\ndata.ca=data.ca.astype('category')\ndata.slope=data.slope.astype('category')\ndata.thal=data.thal.astype('category')","48dfeeba":"data_label=data['target']\ndel data['target']\ndata_label=pd.DataFrame(data_label)","bcadbd72":"data=pd.get_dummies(data,drop_first=True)\ndata.head(),data_label.head()","4671a01f":"from sklearn.preprocessing import StandardScaler, MinMaxScaler\ndata_scaled=MinMaxScaler().fit_transform(data)\ndata_scaled=pd.DataFrame(data=data_scaled, columns=data.columns)","33384f9a":"data_scaled.head()","b7e640ed":"from sklearn.model_selection import train_test_split\nXtrain,Xtest,Ytrain,Ytest = train_test_split(data_scaled, data_label, test_size=0.20,\n                                             stratify=data_label,random_state=9154)","46f90c46":"from sklearn.linear_model import SGDClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n#from sklearn.ensemble import StackingClassifier Need to update sklearn to use inbuilt stacking classifier\nfrom sklearn.ensemble import VotingClassifier","cba48a04":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import f1_score","c928ef6c":"def CrossVal(dataX,dataY,mode,cv=3):\n    score=cross_val_score(mode,dataX , dataY, cv=cv, scoring='accuracy')\n    return(np.mean(score))","6c692b40":"def plotting(true,pred):\n    fig,ax=plt.subplots(1,2,figsize=(10,5))\n    precision,recall,threshold = precision_recall_curve(true,pred[:,1])\n    ax[0].plot(recall,precision,'g--')\n    ax[0].set_xlabel('Recall')\n    ax[0].set_ylabel('Precision')\n    ax[0].set_title(\"Average Precision Score : {}\".format(average_precision_score(true,pred[:,1])))\n    fpr,tpr,threshold = roc_curve(true,pred[:,1])\n    ax[1].plot(fpr,tpr)\n    ax[1].set_title(\"AUC Score is: {}\".format(auc(fpr,tpr)))\n    ax[1].plot([0,1],[0,1],'k--')\n    ax[1].set_xlabel('False Positive Rate')\n    ax[1].set_ylabel('True Positive Rate')","3c71e4fa":"sgd=SGDClassifier(tol=1e-10, random_state=23,loss='log', penalty= \"l2\", alpha=0.2)\nscore_sgd=CrossVal(Xtrain,Ytrain,sgd)\nprint(\"Accuracy is : \",score_sgd)\nsgd.fit(Xtrain,Ytrain)\nplotting(Ytest,sgd.predict_proba(Xtest))\n\nfig=plt.figure()\nsns.heatmap(confusion_matrix(Ytest,sgd.predict(Xtest)), annot= True, cmap='Oranges')\nsgd_f1=f1_score(Ytest,sgd.predict(Xtest))\nplt.title('F1 Score = {}'.format(sgd_f1))","b186abec":"k=KNeighborsClassifier(algorithm='auto',n_neighbors= 19)\nscore_k=CrossVal(Xtrain,Ytrain,k)\nprint(\"Accuracy is : \",score_k)\nk.fit(Xtrain,Ytrain)\nplotting(Ytest,k.predict_proba(Xtest))\n\n\nfig=plt.figure()\nsns.heatmap(confusion_matrix(Ytest,k.predict(Xtest)), annot= True, cmap='Reds')\nk_f1=f1_score(Ytest,k.predict(Xtest))\nplt.title('F1 Score = {}'.format(k_f1))","7203bb2f":"lr=LogisticRegression(class_weight='balanced', tol=1e-10)\nscore_lr=CrossVal(Xtrain,Ytrain,lr)\nprint(\"Accuracy is : \",score_lr)\nlr.fit(Xtrain,Ytrain)\nplotting(Ytest,lr.predict_proba(Xtest))\n\n\nfig=plt.figure()\nsns.heatmap(confusion_matrix(Ytest,lr.predict(Xtest)), annot= True, cmap='Greens')\nlr_f1=f1_score(Ytest,lr.predict(Xtest))\nplt.title('F1 Score = {}'.format(lr_f1))","2801fe22":"dtc=DecisionTreeClassifier(max_depth=6)\nscore_dtc=CrossVal(Xtrain,Ytrain,dtc)\nprint(\"Accuracy is : \",score_dtc)\ndtc.fit(Xtrain,Ytrain)\nplotting(Ytest,dtc.predict_proba(Xtest))\n\nfig=plt.figure()\nsns.heatmap(confusion_matrix(Ytest,dtc.predict(Xtest)), annot= True, cmap='Blues')\n\ndtc_f1=f1_score(Ytest,dtc.predict(Xtest))\nplt.title('F1 Score = {}'.format(dtc_f1))","5790aa27":"svc=SVC(C=0.2,probability=True,kernel='rbf',gamma=0.1)\nscore_svc=CrossVal(Xtrain,Ytrain,svc)\nprint(\"Accuracy is : \",score_svc)\nsvc.fit(Xtrain,Ytrain)\nplotting(Ytest,svc.predict_proba(Xtest))\n\nfig=plt.figure()\nsns.heatmap(confusion_matrix(Ytest,svc.predict(Xtest)), annot= True, cmap='Greys')\nsvc_f1=f1_score(Ytest,svc.predict(Xtest))\nplt.title('F1 Score = {}'.format(svc_f1))","e2c4050a":"rf=RandomForestClassifier(n_estimators=200, n_jobs=-1, random_state=97)\nscore_rf= CrossVal(Xtrain,Ytrain,rf)\nprint('Accuracy is:',score_rf)\nrf.fit(Xtrain,Ytrain)\nplotting(Ytest,rf.predict_proba(Xtest))\n\nfig=plt.figure()\nsns.heatmap(confusion_matrix(Ytest,rf.predict(Xtest)), annot= True, cmap='Oranges')\n\nrf_f1=f1_score(Ytest,rf.predict(Xtest))\nplt.title('F1 Score = {}'.format(rf_f1))","56d53433":"etc=ExtraTreesClassifier(n_estimators=200, n_jobs=-1, random_state=2)\nscore_etc= CrossVal(Xtrain,Ytrain,etc)\nprint('Accuracy is:',score_etc)\netc.fit(Xtrain,Ytrain)\nplotting(Ytest,etc.predict_proba(Xtest))\n\nfig=plt.figure()\nsns.heatmap(confusion_matrix(Ytest,etc.predict(Xtest)), annot= True, cmap='Greens')\n\netc_f1=f1_score(Ytest,etc.predict(Xtest))\nplt.title('F1 Score = {}'.format(etc_f1))","99b7200d":"abc=AdaBoostClassifier(sgd,n_estimators=100, random_state=343, learning_rate=0.012)\nscore_ada= CrossVal(Xtrain,Ytrain,abc)\nprint('Accuracy is:',score_ada)\nabc.fit(Xtrain,Ytrain)\nplotting(Ytest,abc.predict_proba(Xtest))\n\nfig=plt.figure()\nsns.heatmap(confusion_matrix(Ytest,abc.predict(Xtest)), annot= True, cmap='Reds')\n\nabc_f1=f1_score(Ytest,abc.predict(Xtest))\nplt.title('F1 Score = {}'.format(abc_f1))","730e188c":"gbc=GradientBoostingClassifier(n_estimators=100, random_state=43, learning_rate = 0.01)\nscore_gbc= CrossVal(Xtrain,Ytrain,gbc)\nprint('Accuracy is:',score_gbc)\ngbc.fit(Xtrain,Ytrain)\nplotting(Ytest,gbc.predict_proba(Xtest))\n\nfig=plt.figure()\nsns.heatmap(confusion_matrix(Ytest,gbc.predict(Xtest)), annot= True, cmap='Blues')\n\ngbc_f1=f1_score(Ytest,gbc.predict(Xtest))\nplt.title('F1 Score = {}'.format(gbc_f1))","9c374f89":"bc=BaggingClassifier(lr,max_samples=23, bootstrap=True, n_jobs= -1)\nscore_bc= CrossVal(Xtrain,Ytrain,gbc)\nprint('Accuracy is:',score_bc)\nbc.fit(Xtrain,Ytrain)\nplotting(Ytest,bc.predict_proba(Xtest))\n\nfig=plt.figure()\nsns.heatmap(confusion_matrix(Ytest,bc.predict(Xtest)), annot= True, cmap='Greys')\n\nbc_f1=f1_score(Ytest,bc.predict(Xtest))\nplt.title('F1 Score = {}'.format(bc_f1))","0d234b38":"fig= plt.figure(figsize=(10,10))\nimportant=pd.Series(rf.feature_importances_, index=Xtrain.columns)\nsns.set_style('whitegrid')\nimportant.sort_values().plot.barh()\nplt.title('Feature Importance')","645bd932":"model_accuracy = pd.Series(data=[score_sgd, score_k, score_lr, score_dtc, score_svc, score_rf, score_etc, \n                           score_ada, score_gbc, score_bc], \n                           index=['Stochastic GD','KNN','logistic Regression','decision tree', 'SVM', 'Random Forest',\n                            'Extra Tree', 'Ada Boost' , 'Gradient Boost','Bagging Classfier'])\nfig= plt.figure(figsize=(8,8))\nmodel_accuracy.sort_values().plot.barh()\nplt.title('Model Accracy')","5e999c7a":"model_f1_score = pd.Series(data=[sgd_f1, k_f1, lr_f1, dtc_f1, svc_f1, rf_f1, etc_f1, \n                           abc_f1, gbc_f1, bc_f1], \n                           index=['Stochastic GD','KNN','logistic Regression','decision tree', 'SVM', 'Random Forest',\n                                'Extra Tree', 'Ada Boost' , 'Gradient Boost', 'Bagging Classfier'])\nfig= plt.figure(figsize=(8,8))\nmodel_f1_score.sort_values().plot.barh()\nplt.title('Model F1 Score Comparison')","121c7f83":"vc=VotingClassifier(estimators=[('knn',k),('SGD',sgd),('lr',lr)],\n                    voting='soft')\nscore_vc= CrossVal(Xtrain,Ytrain,vc)\nprint('Accuracy is:',score_vc)\nvc.fit(Xtrain,Ytrain)\nplotting(Ytest,vc.predict_proba(Xtest))\n\nfig=plt.figure()\nsns.heatmap(confusion_matrix(Ytest,vc.predict(Xtest)), annot= True, cmap='Greys')\n\nvc_f1=f1_score(Ytest,vc.predict(Xtest))\nplt.title('F1 Score = {}'.format(vc_f1))","78349d0e":"from sklearn.model_selection import StratifiedKFold\nk=StratifiedKFold(n_splits= 5, shuffle=False, random_state=6)","1951bbce":"def stacking(model, Xtrain, Ytrain, Xtest, name):\n    prediction_train = np.zeros(len(Xtrain))\n    prediction_test = np.zeros((len(Xtest)))\n    for train_index, test_index in k.split(Xtrain,Ytrain):\n        trainset, trainset_label =  Xtrain.iloc[train_index,:], Ytrain.iloc[train_index]\n        cv_set, cv_label =  Xtrain.iloc[test_index,:], Ytrain.iloc[test_index]\n        \n        model.fit(trainset, trainset_label)\n        prediction_train[test_index] = model.predict(cv_set)\n        \n    prediction_test = model.predict(Xtest)\n    return (pd.DataFrame({name:prediction_train}),pd.DataFrame({name:prediction_test}))                               ","5264667d":"# stacking SGD , Logistic regression, voting classifier\nsgd_train, sgd_test = stacking(sgd, Xtrain, Ytrain, Xtest, 'sgd')\nlr_train, lr_test = stacking(lr, Xtrain, Ytrain, Xtest, 'logistic')\nvc_train, vc_test = stacking(vc, Xtrain, Ytrain, Xtest, 'voting') ","31c1514a":"# Combining prediction made by all the three classifiers\ntrainset = pd.concat([sgd_train,lr_train,vc_train],axis=1)\ntestset = pd.concat([sgd_test,lr_test,vc_test],axis=1)\n\n# checking correlation \nsns.heatmap(trainset.corr(), annot =True, cmap='Greens')","1cdcdf2e":"# meta classifeir\nlr=LogisticRegression(class_weight='balanced', tol=1e-20)\nscore_lr=CrossVal(trainset,Ytrain,lr)\nprint(\"Accuracy is : \",score_lr)\nlr.fit(trainset,Ytrain)\nplotting(Ytest,lr.predict_proba(testset))\n\n\nfig=plt.figure()\nsns.heatmap(confusion_matrix(Ytest,lr.predict(testset)), annot= True, cmap='Greens')\nlr_f1=f1_score(Ytest,lr.predict(testset))\nplt.title('F1 Score = {}'.format(lr_f1))","41b5cc3f":"### c) resting electrocardiographic results (values 0,1,2) (Category)","fd999c2a":"### Attribute Information:\n    1. age                                                  2. sex\n    3. chest pain type (4 values)                           4. resting blood pressure\n    5. serum cholestoral in mg\/dl                           6. fasting blood sugar > 120 mg\/dl\n    7. resting electrocardiographic results (values 0,1,2)  8. maximum heart rate achieved\n    9. exercise induced angina                              10. oldpeak = ST depression induced by exercise relative to rest\n    11. the slope of the peak exercise ST segment           12. number of major vessels (0-3) colored by flourosopy\n    13. thal: 3 = normal; 6 = fixed defect; \n            7 = reversable defect                           14. target column","7afb417a":"From above graph we can say that more than half of the population suffering from Heart Disease with parcentage of 54.5%. ","5f147084":"### Feel free to ask any doubt\/question\/ or to give any suggestion :)","9c50d6ed":"So after modeling we can say that **thalach, thal_2 and oldeak** are most important feature in prediction","4e02427f":"## K) Voting Classifier ","adde5457":"# Stacking models","4ec8356f":"Female have **higher cholestrol level** than Men. Chances of **Heart Diseases** decreases with decrease in **Cholestrol level**.","d09187e6":"### Let's explore Continuous data now with categorical and ordinal data","1326d755":"### g) Chest Pain (category)","fc28efb9":"Based on above plots we can comclude that if Old peak is less then people will have more chances of **having heart diseases**","aa26f418":"4-Levels of chest pain given in data where 3 is highest","b9473445":"## Part3: Predictive Modeling","8cd88673":"This is weird **People having fps < 120 have more chance of having Heart Disease than people havnig fps >120**","2aac830b":"### PLOT WITH RESPECT TO MEAN OF EACH ROW","62d0e972":"### h) Trestbps (continuous feature)","8adaaeb9":"### i) Cholestrol (continuous feature)","51bce871":"### Now Let's combine best classifier from above results and feed into Voting Classifier","29a5dd4a":"### Distribution of each features","a6875153":"### Handling Missing Data","1c1bee57":"### h) Ada Boost Classifier","487449a4":"Lets Move to other features","e9fa3e66":"Now we have to combine the prediction made by our best classifier which will use to feed to meta classifier","be9d1026":"### How many people are suffering from Heart Disease ?","21795b39":"### c) Logistic Regression","175464b1":"### Now let's work on each feature conversion","de451ed6":"Feature (the peak exercise ST segment slope) has three symbolic values (flat, up sloping, downsloping)\n","bbbad1fe":"Seems pretty good !!!! Now we have to feed this data to our meta classifier. For this i am going to use Logistic Regression. Let's see what it does","0bb2683f":"# EDA + Model building in Depth On Heart Diseases","30885c27":"**THIS seems to be important info from data** ","5f3f045a":"### e)  number of major vessels colored by flourosopy (category)","40eb1eee":"#### AS FURTHER COMPARISON BETWEEN MODEL PLOTTING F1 SCORE","2be9a074":"### Cross validation helper function","27b322a5":"#### MOST IMPORTANT NOTE HERE IS :Voting Classifier will perform better if all of the classifier which are choosen for voting are making different mistakes. So Voting Classifier will not make that mistake by choosing most voted class.","44431aaa":"Based on above heatmap we can say **most of the features are in high correlation with each other**.","de752459":"**This is interesting**","a73bf15b":"### Model accuracy plot","cf83f106":"### Importing ML libraries","94edb001":"### Stay tuned for more updates. And don't forget to give an upvote if you like it ","58406af6":"If you consider accuracy then Stochastic, Logistic Regression and K-Nearest Neighbours are doing better than other ML algorithms. ","76bd024b":"### Splittting data into test and train set","e2d28a24":"An electrocardiogram (ECG) is a test which measures the electrical activity of your heart to show whether or not it is working normally. An ECG records the heart's rhythm and activity on a moving strip of paper or a line on a screen. -> **THANKS GOOGLE**","014ce546":"### f) thal 3 = normal, 6 = fixed defect, 7 = reversable defect (category feature)","b1825230":"### Evalutation metrics to check model performance","0e348d2e":"Now we will stack our best models (base models) then after that we will use one meta model which will uses predictions made by base models and try to improve evaluation metrics.","effce4fe":"## Part2: Data Cleaning","0f000df8":"### Function to plot ROC and Precision Recall Curve ","202b3a1d":"### Objective : \nMain obejctive behind this notebook is to give an idea along with workflow of Machine Learning Processes.\n\nStarting from **Getting data informaion to Exploratory Data Analysis, Data Manipulation, Building and then Validation of Model.**\n\nI am trying to keep it as **simple** as i can so that newbie can also understand the workflow.\n\nIf you learn anything useful from this notebook then **Give Upvote :)\n","9efe4c29":"With above graph as a refrence we can **if resting electrocardiographic is 1 then person have more chances of suffering from Heart Disease**","2635cdb0":"![](https:\/\/cdn-images-1.medium.com\/max\/800\/0*GHYCJIjkkrP5ZgPh.png)","ef25ef3d":"This insight will be very usefull for our model","e52796ac":"This insight will also be useful for our model","3c5cc076":"**Number of Women suffering from Heart Disease are more than Men** but **Men population is more than Women**. We will use these insight for our model developement.","1475d103":"#### Finally after stacking model we got 80 and 80 F1 and AUC score it is still very less than Voting Classifier. Therefore it is no guranteed that you will get best result after stacking sometimes simple model can outperform complex models.","a2b232d8":"### a)Stochastic Gradient Descent ","b4a6d7c0":"### IMOPORTANT FEATURE\n","f6a6cfda":"### e) Support vector machine","72b0bc21":"### Plot with respect to Standard Deviation per Row","898719b2":"With 2nd Graph (Cholestrol V\/S Chest Pain) we can say that if **cholestrol is less than 240 approx** and **Chest pain is at level 3~4 then chances of having heart diseases are higher**","34e388af":"### By combining KNN + SGD + Logistic Regression we got 81.0 F1 score with 87 AUC","3a4d674f":"### Normalization (To get value b\/w 0 and 1)","269ecd4b":"THAT'S Good. **No Missing Values**","9aadfd42":"### a)->SEX (Category)","4f897706":"* ## Contents of the Notebook:\n\n#### Part1: Exploratory Data Analysis(EDA)\n1) Analysis of the features.\n\n2) Finding any relations or trends considering multiple features.\n#### Part2: Data Cleaning:\n1) Adding any few features if any.\n\n2) Removing redundant features.\n\n3) Converting features into suitable form for modeling.\n#### Part3: Predictive Modeling\n1) Running Basic Algorithms.\n\n2) Cross Validation.\n\n3) Important Features Extraction.\n\n4) Plotting ROC Curve, Precision\/Recall Curve, AUC\n\n5) Model Comparison (Accuracy + F1 Score)\n\n6) Ensemble model","10318cea":"Let's explore more feature to get more insight from dataset","c76c680f":"### d) Decision Tree Classifier","94d059f1":"Let's explore more","c3f0664b":"### g) Extra Trees Classifier","821535ca":"Since **Fluoroscopy** use to  produce x-ray which will makes possible to see internal organs in motion. Fluoroscopy uses x-ray to produce real-time video images.","736e0cb2":"**People who are on 3rd level of chest pain are very less as compared to people who are on 2nd level of chest pain**. \nI guess **Most people died after 2nd level of chest pain**","48d71d1b":"Based on above analysis we can say that Gender plays minor role with respect to Blood Pressure (trestbps). But **Chest Pain play's Vital Role** . As Chest pain increases Blood Pressure will also increases along with chances of Heart Diseases.","6ef62485":"But for Classification task **ACCURACY is not important**.  Instead of accuracy model should be judged on basis of **AUC (Area under curve), ROC CURVE, High Precision and High Recall values**. **F1 score** also play imporant role which is equals to **2\/(1\/precision + 1\/Recall) score**","9173fe05":"Let's explore other feature","c2f0c1f4":"### Let's do some Advanced EDA now ","002c32c0":"### b) K-Nearest Neighbors","c0b96df3":"### i) Gradient Boosting Classifier ","31d3aafe":"### d) the slope of the peak exercise ST segment (slope)(Category)","d8a2e13c":"### Creating dummies variables","dd06e16d":"### j) Bagging Classifier ","d2b81e92":"This is the power of ensembling. In this case Voting classifier is classifing instance based on Votes. For eg. Out of 3 classifier suppose if 2 classifier voting for postive class and other one is for negative class then Voting classifier will choose positive class for that instance.   ","50f2a42b":"Let's see **correlation between different features**","bee44fd0":"### j) Oldpeak (continuous feature)","0acf27c7":"### b)-> fasting blood sugar (Fbs) (Category)","1eccbaec":"### Now Sklearn also provide StackingClassifier() as well as StackingRegressor() under ensemble module. Update Sklearn to use those libraries","112e5d10":"## Part1: Exploratory Data Analysis(EDA)","0ec8fe69":"### f) Random Forest Classifier","64d5bdf3":"### Feature analysis","d6cff593":"Therefore **People having up sloping are more prone to Heart Disease than flat and downsloping**. This is useful for our model"}}