{"cell_type":{"01d7d0a1":"code","4ee281f8":"code","1d770a3d":"code","1293cbd9":"code","449b1d32":"code","7701d001":"code","1decbb35":"code","b5f3f365":"code","c5752ab3":"code","e76d3f35":"code","8c452d11":"code","102e7ff2":"code","476a5046":"code","6ef0eeae":"code","99a2026d":"code","d4a3214e":"code","e67d31a1":"code","bc709d39":"code","14d519b6":"code","a9ee9463":"code","393d3f03":"code","a8563ce5":"code","287b9fb6":"markdown","36322f1e":"markdown","71743ca3":"markdown","ce54d961":"markdown","791575f9":"markdown","2f027bb7":"markdown","b8ae996e":"markdown","217cb0f5":"markdown","efa71fda":"markdown","3f6c1386":"markdown","5cf6d9aa":"markdown","a6460954":"markdown"},"source":{"01d7d0a1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","4ee281f8":"# importing libraries\n# keras module for building LSTM\n\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, Dense, Dropout, LSTM\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport keras.utils as ku\nfrom keras.callbacks import EarlyStopping\n\n# set seeds for reproducability\n\nfrom tensorflow import set_random_seed\nfrom numpy.random import seed\nset_random_seed(2)\nseed(1)\n\nimport string\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.simplefilter(action='ignore',  category=FutureWarning)","1d770a3d":"currr_dir = \"..\/input\/\"\nall_headlines = []\n\nx=0\n\nfor filename in os.listdir(currr_dir):\n    if 'Articles' in filename:\n        \n        if x==0:\n            print(filename)\n            \n        article_df = pd.read_csv(currr_dir+filename)\n        \n        if x==0:\n            print(article_df.shape)\n            print(article_df.columns)\n            print(article_df.head(5))\n            print(article_df.tail(5))\n            \n        all_headlines.extend(list(article_df.headline.values))\n        \n        if x==0:\n            print(article_df.headline)\n            print(article_df.headline.values)\n            \n        x=1\n        \n        break\n\nall_headlines = [ h for h in all_headlines if h != 'Unknown' ]\nprint(len(all_headlines))\nprint(all_headlines[:5])","1293cbd9":"def clean_text(txt):\n    txt = \"\".join(w for w in txt if w not in string.punctuation).lower()\n    txt = txt.encode(\"utf8\").decode(\"ascii\",\"ignore\")\n    return txt","449b1d32":"print(clean_text(\"Questions for: \u2018Colleges Discover the Rural St..\"))","7701d001":"print(string.punctuation)","1decbb35":"corpus = [clean_text(x) for x in all_headlines]\nprint(corpus[:10])","b5f3f365":"tokenizer = Tokenizer()\ndef get_sequence_of_tokens(corpus):\n    q=0\n    # tokenization\n    tokenizer.fit_on_texts(corpus)\n    total_words = len(tokenizer.word_index) + 1\n    \n    if q==0:\n        print(total_words)\n    \n    # convert data into sequence of tokens\n    input_sequences = []\n    for line in corpus:\n        \n        if q==0:\n            print(line)\n            \n        token_list = tokenizer.texts_to_sequences([line])[0]\n        \n        if q==0:\n            print(token_list)\n            print(len(token_list))\n            \n        for i in range(1, len(token_list)):\n            n_gram_sequence = token_list[:i+1]\n            \n            if q==0:\n                print(n_gram_sequence)\n                \n            input_sequences.append(n_gram_sequence)\n            \n            if q==0:\n                print(input_sequences)\n                \n            q=1\n    \n    return input_sequences, total_words","c5752ab3":"input_sequence, total_words = get_sequence_of_tokens(corpus)\nprint(total_words)","e76d3f35":"(input_sequence[:10])","8c452d11":"def generate_padded_sequences(input_sequence):\n    max_sequence_len = max([len(x) for x in input_sequence])\n    input_sequences = np.array(pad_sequences(input_sequence, maxlen=max_sequence_len, padding='pre'))\n    predictors, labels = input_sequences[:,:-1], input_sequences[:,-1]\n    labels = ku.to_categorical(labels, num_classes=total_words)\n    \n    return predictors, labels, max_sequence_len","102e7ff2":"predictors, labels, max_seq_len = generate_padded_sequences(input_sequence)","476a5046":"len(predictors)","6ef0eeae":"len(labels)","99a2026d":"(max_seq_len)","d4a3214e":"print(total_words)","e67d31a1":"def create_model(max_seq_len, total_words):\n    input_len = max_seq_len - 1\n    \n    model = Sequential()\n    \n    # input: embedding layer\n    model.add(Embedding(total_words, 10, input_length=input_len))\n    \n    # hidden: lstm layer\n    model.add(LSTM(100))\n    model.add(Dropout(0.1))\n    \n    # output layer\n    model.add(Dense(total_words, activation='softmax'))\n    \n    # compile the model\n    model.compile(loss='categorical_crossentropy', optimizer='adam')\n    \n    return model","bc709d39":"model = create_model(max_seq_len, total_words)\nmodel.summary()","14d519b6":"# fitting (training) the model by passing predictors and labels as training data\n\nmodel.fit(predictors, labels, epochs=100, verbose=5)","a9ee9463":"def generate_text(seed_text, next_words, model, max_seq_len):\n    w=0\n    for _ in range(next_words):\n        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n        \n        if w==0:\n            print(token_list)\n            \n        token_list = pad_sequences([token_list], maxlen=max_seq_len-1, padding='pre')\n        \n        if w==0:\n            print(token_list)\n        \n        predicted = model.predict_classes(token_list, verbose=0)\n        \n        if w==0:\n            print(predicted)\n        \n        output_word = ''\n        \n        for word,index in tokenizer.word_index.items():\n            if index == predicted:\n                output_word = word\n                break\n                \n        seed_text = seed_text + \" \" + output_word\n        \n        w=1\n        \n    return seed_text.title()","393d3f03":"print(generate_text(\"united states\", 5, model, max_seq_len))","a8563ce5":"print (generate_text(\"united states\", 5, model, max_seq_len))\nprint (generate_text(\"preident trump\", 4, model, max_seq_len))\nprint (generate_text(\"donald trump\", 4, model, max_seq_len))\nprint (generate_text(\"india and china\", 4, model, max_seq_len))\nprint (generate_text(\"new york\", 4, model, max_seq_len))\nprint (generate_text(\"science and technology\", 5, model, max_seq_len))","287b9fb6":"Generating News headlines\n\nIn this kernel, I will be using the dataset of New York Times Comments and Headlines to train a text generation language model which can be used to generate News Headlines","36322f1e":"Generating Sequence of N-gram Tokens\n\nLanguage modelling requires a sequence input data, as given a sequence (of words\/tokens) the aim is the predict next word\/token.\n\nThe next step is Tokenization. Tokenization is a process of extracting tokens (terms \/ words) from a corpus. Python\u2019s library Keras has inbuilt model for tokenization which can be used to obtain the tokens and their index in the corpus. After this step, every text document in the dataset is converted into sequence of tokens","71743ca3":"LSTMs have an additional state called \u2018cell state\u2019 through which the network makes adjustments in the information flow. The advantage of this state is that the model can remember or forget the leanings more selectively.","ce54d961":"Padding the Sequences and obtain Variables : Predictors and Target\n\nNow that we have generated a data-set which contains sequence of tokens, it is possible that different sequences have different lengths. Before starting training the model, we need to pad the sequences and make their lengths equal. We can use pad_sequence function of Kears for this purpose. To input this data into a learning model, we need to create predictors and label. We will create N-grams sequence as predictors and the next word of the N-gram as label","791575f9":"Text Generation using LSTMs\n\nText Generation is a type of Language Modelling problem. Language Modelling is the core problem for a number of of natural language processing tasks such as speech to text, conversational system, and text summarization. A trained language model learns the likelihood of occurrence of a word based on the previous sequence of words used in the text. Language models can be operated at character level, n-gram level, sentence level or even paragraph level. In this notebook, I will explain how to create a language model for generating natural language text by implement and training state-of-the-art Recurrent Neural Network","2f027bb7":"Input Layer : Takes the sequence of words as input\n\nLSTM Layer : Computes the output using LSTM units. I have added 100 units in the layer, but this number can be fine tuned later.\n\nDropout Layer : A regularisation layer which randomly turns-off the activations of some neurons in the LSTM layer. It helps in preventing over fitting. (Optional Layer)\n\nOutput Layer : Computes the probability of the best possible next word as output","b8ae996e":"Unlike Feed-forward neural networks in which activation outputs are propagated only in one direction, the activation outputs from neurons propagate in both directions (from inputs to outputs and from outputs to inputs) in Recurrent Neural Networks. This creates loops in the neural network architecture which acts as a \u2018memory state\u2019 of the neurons. This state allows the neurons an ability to remember what have been learned so far.","217cb0f5":"Generating the text\n\nGreat, our model architecture is now ready. \n\nNext lets write the function to predict the next word based on the input words (or seed text). We will first tokenize the seed text, pad the sequences and pass into the trained model to get predicted word. The multiple predicted words can be appended together to get predicted sequence.","efa71fda":"now we can obtain the input vector X and the label vector Y which can be used for the training purposes. Recent experiments have shown that recurrent neural networks have shown a good performance in sequence to sequence learning and text data applications","3f6c1386":"The memory state in RNNs gives an advantage over traditional neural networks but a problem called Vanishing Gradient is associated with them. In this problem, while learning with a large number of layers, it becomes really hard for the network to learn and tune the parameters of the earlier layers. To address this problem, A new type of RNNs called LSTMs (Long Short Term Memory) Models have been developed.","5cf6d9aa":"Reference:\n    \ncomplete code and explaination from https:\/\/www.kaggle.com\/shivamb\/beginners-guide-to-text-generation-using-lstms\/notebook","a6460954":"Dataset preparation\n\nDataset cleaning\n\nIn dataset preparation step, we will first perform text cleaning of the data which includes removal of punctuations and lower casing all the words"}}