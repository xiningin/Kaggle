{"cell_type":{"b5a66207":"code","934f0513":"code","3d451953":"code","2fe4d8ea":"code","1919aa8c":"code","1e8e7e6c":"code","7df75a35":"code","3c6acf16":"code","c12c012d":"code","13d5b656":"code","32c75304":"code","8fe249de":"code","c305d338":"code","d64b7514":"code","e2d7378f":"code","750ced2c":"code","4590a23f":"code","a0e11c6a":"code","3654d9ef":"code","f648b533":"code","240af64c":"code","8d40c97d":"code","5c0c7666":"code","36eb628f":"code","881d1bfd":"code","2e83bd3c":"code","c6fa2c36":"code","65368220":"code","ea538565":"code","ec4f4290":"code","6dbaa597":"code","aa010bca":"code","12b78fae":"code","8ee7558e":"code","6bf2c6d8":"markdown","aea36cc8":"markdown","a0a4094d":"markdown","cb51106b":"markdown","155d54ef":"markdown","c98626ca":"markdown","86e4f96d":"markdown","6bb4b0da":"markdown","80a9794c":"markdown","ea8885c5":"markdown","2636b37b":"markdown","3218a969":"markdown","fee312d6":"markdown","bb581892":"markdown","864a27aa":"markdown","7365cb0c":"markdown","abf93495":"markdown","04811dbb":"markdown","eeac5aeb":"markdown","358139b2":"markdown","4fc8a089":"markdown","537e21b9":"markdown","b90a9e9a":"markdown","b1a36a29":"markdown","ebfd7f05":"markdown","fa331de1":"markdown","68b9fdd5":"markdown","9a9c9954":"markdown","d3520c79":"markdown","01063186":"markdown","a8921530":"markdown","a5ee88c1":"markdown","e0644a44":"markdown","aef27c00":"markdown","c9efd528":"markdown","1de45442":"markdown","b7801e55":"markdown","4ae6dafd":"markdown"},"source":{"b5a66207":"import numpy as np \nimport pandas as pd \n\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt \nplt.rc(\"font\", size=14)\nimport seaborn as sns\nsns.set(style=\"white\") #white background style for seaborn plots\nsns.set(style=\"whitegrid\", color_codes=True)\n\nimport warnings\nwarnings.simplefilter(action='ignore')","934f0513":"# Read CSV train data file into DataFrame\ntrain_df = pd.read_csv(\"..\/input\/train.csv\")\n\n# Read CSV test data file into DataFrame\ntest_df = pd.read_csv(\"..\/input\/test.csv\")\n\n# preview train data\ntrain_df.head()","3d451953":"print('The number of samples into the train data is {}.'.format(train_df.shape[0]))","2fe4d8ea":"# preview test data\ntest_df.head()","1919aa8c":"print('The number of samples into the test data is {}.'.format(test_df.shape[0]))","1e8e7e6c":"# check missing values in train data\ntrain_df.isnull().sum()","7df75a35":"# percent of missing \"Age\" \nprint('Percent of missing \"Age\" records is %.2f%%' %((train_df['Age'].isnull().sum()\/train_df.shape[0])*100))","3c6acf16":"ax = train_df[\"Age\"].hist(bins=15, density=True, stacked=True, color='teal', alpha=0.6)\ntrain_df[\"Age\"].plot(kind='density', color='teal')\nax.set(xlabel='Age')\nplt.xlim(-10,85)\nplt.show()","c12c012d":"# mean age\nprint('The mean of \"Age\" is %.2f' %(train_df[\"Age\"].mean(skipna=True)))\n# median age\nprint('The median of \"Age\" is %.2f' %(train_df[\"Age\"].median(skipna=True)))","13d5b656":"# percent of missing \"Cabin\" \nprint('Percent of missing \"Cabin\" records is %.2f%%' %((train_df['Cabin'].isnull().sum()\/train_df.shape[0])*100))","32c75304":"# percent of missing \"Embarked\" \nprint('Percent of missing \"Embarked\" records is %.2f%%' %((train_df['Embarked'].isnull().sum()\/train_df.shape[0])*100))","8fe249de":"print('Boarded passengers grouped by port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton):')\nprint(train_df['Embarked'].value_counts())\nsns.countplot(x='Embarked', data=train_df, palette='Set2')\nplt.show()","c305d338":"print('The most common boarding port of embarkation is %s.' %train_df['Embarked'].value_counts().idxmax())","d64b7514":"train_data = train_df.copy()\ntrain_data[\"Age\"].fillna(train_df[\"Age\"].median(skipna=True), inplace=True)\ntrain_data[\"Embarked\"].fillna(train_df['Embarked'].value_counts().idxmax(), inplace=True)\ntrain_data.drop('Cabin', axis=1, inplace=True)","e2d7378f":"# check missing values in adjusted train data\ntrain_data.isnull().sum()","750ced2c":"# preview adjusted train data\ntrain_data.head()","4590a23f":"plt.figure(figsize=(15,8))\nax = train_df[\"Age\"].hist(bins=15, density=True, stacked=True, color='teal', alpha=0.6)\ntrain_df[\"Age\"].plot(kind='density', color='teal')\nax = train_data[\"Age\"].hist(bins=15, density=True, stacked=True, color='orange', alpha=0.5)\ntrain_data[\"Age\"].plot(kind='density', color='orange')\nax.legend(['Raw Age', 'Adjusted Age'])\nax.set(xlabel='Age')\nplt.xlim(-10,85)\nplt.show()","a0e11c6a":"## Create categorical variable for traveling alone\ntrain_data['TravelAlone']=np.where((train_data[\"SibSp\"]+train_data[\"Parch\"])>0, 0, 1)\ntrain_data.drop('SibSp', axis=1, inplace=True)\ntrain_data.drop('Parch', axis=1, inplace=True)","3654d9ef":"#create categorical variables and drop some variables\ntraining=pd.get_dummies(train_data, columns=[\"Pclass\",\"Embarked\",\"Sex\"])\ntraining.drop('Sex_female', axis=1, inplace=True)\ntraining.drop('PassengerId', axis=1, inplace=True)\ntraining.drop('Name', axis=1, inplace=True)\ntraining.drop('Ticket', axis=1, inplace=True)\n\nfinal_train = training\nfinal_train.head()","f648b533":"test_df.isnull().sum()","240af64c":"test_data = test_df.copy()\ntest_data[\"Age\"].fillna(train_df[\"Age\"].median(skipna=True), inplace=True)\ntest_data[\"Fare\"].fillna(train_df[\"Fare\"].median(skipna=True), inplace=True)\ntest_data.drop('Cabin', axis=1, inplace=True)\n\ntest_data['TravelAlone']=np.where((test_data[\"SibSp\"]+test_data[\"Parch\"])>0, 0, 1)\n\ntest_data.drop('SibSp', axis=1, inplace=True)\ntest_data.drop('Parch', axis=1, inplace=True)\n\ntesting = pd.get_dummies(test_data, columns=[\"Pclass\",\"Embarked\",\"Sex\"])\ntesting.drop('Sex_female', axis=1, inplace=True)\ntesting.drop('PassengerId', axis=1, inplace=True)\ntesting.drop('Name', axis=1, inplace=True)\ntesting.drop('Ticket', axis=1, inplace=True)\n\nfinal_test = testing\nfinal_test.head()","8d40c97d":"plt.figure(figsize=(15,8))\nax = sns.kdeplot(final_train[\"Age\"][final_train.Survived == 1], color=\"darkturquoise\", shade=True)\nsns.kdeplot(final_train[\"Age\"][final_train.Survived == 0], color=\"lightcoral\", shade=True)\nplt.legend(['Survived', 'Died'])\nplt.title('Density Plot of Age for Surviving Population and Deceased Population')\nax.set(xlabel='Age')\nplt.xlim(-10,85)\nplt.show()","5c0c7666":"plt.figure(figsize=(20,8))\navg_survival_byage = final_train[[\"Age\", \"Survived\"]].groupby(['Age'], as_index=False).mean()\ng = sns.barplot(x='Age', y='Survived', data=avg_survival_byage, color=\"LightSeaGreen\")\nplt.show()","36eb628f":"final_train['IsMinor']=np.where(final_train['Age']<=16, 1, 0)\n\nfinal_test['IsMinor']=np.where(final_test['Age']<=16, 1, 0)","881d1bfd":"plt.figure(figsize=(15,8))\nax = sns.kdeplot(final_train[\"Fare\"][final_train.Survived == 1], color=\"darkturquoise\", shade=True)\nsns.kdeplot(final_train[\"Fare\"][final_train.Survived == 0], color=\"lightcoral\", shade=True)\nplt.legend(['Survived', 'Died'])\nplt.title('Density Plot of Fare for Surviving Population and Deceased Population')\nax.set(xlabel='Fare')\nplt.xlim(-20,200)\nplt.show()","2e83bd3c":"sns.barplot('Pclass', 'Survived', data=train_df, color=\"darkturquoise\")\nplt.show()","c6fa2c36":"sns.barplot('Embarked', 'Survived', data=train_df, color=\"teal\")\nplt.show()","65368220":"sns.barplot('TravelAlone', 'Survived', data=final_train, color=\"mediumturquoise\")\nplt.show()","ea538565":"sns.barplot('Sex', 'Survived', data=train_df, color=\"aquamarine\")\nplt.show()","ec4f4290":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE\n\ncols = [\"Age\",\"Fare\",\"TravelAlone\",\"Pclass_1\",\"Pclass_2\",\"Embarked_C\",\"Embarked_S\",\"Sex_male\",\"IsMinor\"] \nX = final_train[cols]\ny = final_train['Survived']\n# Build a logreg and compute the feature importances\nmodel = LogisticRegression()\n# create the RFE model and select 8 attributes\nrfe = RFE(model, 8)\nrfe = rfe.fit(X, y)\n# summarize the selection of the attributes\nprint('Selected features: %s' % list(X.columns[rfe.support_]))","6dbaa597":"from sklearn.feature_selection import RFECV\n# Create the RFE object and compute a cross-validated score.\n# The \"accuracy\" scoring is proportional to the number of correct classifications\nrfecv = RFECV(estimator=LogisticRegression(), step=1, cv=10, scoring='accuracy')\nrfecv.fit(X, y)\n\nprint(\"Optimal number of features: %d\" % rfecv.n_features_)\nprint('Selected features: %s' % list(X.columns[rfecv.support_]))\n\n# Plot number of features VS. cross-validation scores\nplt.figure(figsize=(10,6))\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score (nb of correct classifications)\")\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.show()","aa010bca":"Selected_features = ['Age', 'TravelAlone', 'Pclass_1', 'Pclass_2', 'Embarked_C', \n                     'Embarked_S', 'Sex_male', 'IsMinor']\nX = final_train[Selected_features]\n\nplt.subplots(figsize=(8, 5))\nsns.heatmap(X.corr(), annot=True, cmap=\"RdYlGn\")\nplt.show()","12b78fae":"from sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score \nfrom sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve, auc, log_loss\n\n# create X (features) and y (response)\nX = final_train[Selected_features]\ny = final_train['Survived']\n\n# use train\/test split with different random_state values\n# we can change the random_state values that changes the accuracy scores\n# the scores change a lot, this is why testing scores is a high-variance estimate\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n\n# check classification scores of logistic regression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\ny_pred_proba = logreg.predict_proba(X_test)[:, 1]\n[fpr, tpr, thr] = roc_curve(y_test, y_pred_proba)\nprint('Train\/Test split results:')\nprint(logreg.__class__.__name__+\" accuracy is %2.3f\" % accuracy_score(y_test, y_pred))\nprint(logreg.__class__.__name__+\" log_loss is %2.3f\" % log_loss(y_test, y_pred_proba))\nprint(logreg.__class__.__name__+\" auc is %2.3f\" % auc(fpr, tpr))\n\nidx = np.min(np.where(tpr > 0.95)) # index of the first threshold for which the sensibility > 0.95\n\nplt.figure()\nplt.plot(fpr, tpr, color='coral', label='ROC curve (area = %0.3f)' % auc(fpr, tpr))\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot([0,fpr[idx]], [tpr[idx],tpr[idx]], 'k--', color='blue')\nplt.plot([fpr[idx],fpr[idx]], [0,tpr[idx]], 'k--', color='blue')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate (1 - specificity)', fontsize=14)\nplt.ylabel('True Positive Rate (recall)', fontsize=14)\nplt.title('Receiver operating characteristic (ROC) curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n\nprint(\"Using a threshold of %.3f \" % thr[idx] + \"guarantees a sensitivity of %.3f \" % tpr[idx] +  \n      \"and a specificity of %.3f\" % (1-fpr[idx]) + \n      \", i.e. a false positive rate of %.2f%%.\" % (np.array(fpr[idx])*100))","8ee7558e":"# 10-fold cross-validation logistic regression\nlogreg = LogisticRegression()\n# Use cross_val_score function\n# We are passing the entirety of X and y, not X_train or y_train, it takes care of splitting the data\n# cv=10 for 10 folds\n# scoring = {'accuracy', 'neg_log_loss', 'roc_auc'} for evaluation metric - althought they are many\nscores_accuracy = cross_val_score(logreg, X, y, cv=10, scoring='accuracy')\nscores_log_loss = cross_val_score(logreg, X, y, cv=10, scoring='neg_log_loss')\nscores_auc = cross_val_score(logreg, X, y, cv=10, scoring='roc_auc')\nprint('K-fold cross-validation results:')\nprint(logreg.__class__.__name__+\" average accuracy is %2.3f\" % scores_accuracy.mean())\nprint(logreg.__class__.__name__+\" average log_loss is %2.3f\" % -scores_log_loss.mean())\nprint(logreg.__class__.__name__+\" average auc is %2.3f\" % scores_auc.mean())","6bf2c6d8":"<a id=\"t4.2.\"><\/a>\n## 4.2. Review of model evaluation procedures\n\nMotivation: Need a way to choose between machine learning models\n* Goal is to estimate likely performance of a model on out-of-sample data\n\nInitial idea: Train and test on the same data\n* But, maximizing training accuracy rewards overly complex models which overfit the training data\n\nAlternative idea: Train\/test split\n* Split the dataset into two pieces, so that the model can be trained and tested on different data\n* Testing accuracy is a better estimate than training accuracy of out-of-sample performance\n* Problem with train\/test split\n    * It provides a high variance estimate since changing which observations happen to be in the testing set can significantly change testing accuracy\n    * Testing accuracy can change a lot depending on a which observation happen to be in the testing set\n\nReference: <br>\nhttp:\/\/www.ritchieng.com\/machine-learning-cross-validation\/ <br>","aea36cc8":"Individuals traveling without family were more likely to die in the disaster than those with family aboard. Given the era, it's likely that individuals traveling alone were likely male.","a0a4094d":"According to the Kaggle data dictionary, both SibSp and Parch relate to traveling with family.  For simplicity's sake (and to account for possible multicollinearity), I'll combine the effect of these variables into one categorical predictor: whether or not that individual was traveling alone.","cb51106b":"<a id=\"t1.\"><\/a>\n# 1. Import Data & Python Packages","155d54ef":"~20% of entries for passenger age are missing. Let's see what the 'Age' variable looks like in general.","c98626ca":"<a id=\"t3.4.\"><\/a>\n## 3.4. Exploration of Embarked Port","86e4f96d":"Considering the survival rate of passengers under 16, I'll also include another categorical variable in my dataset: \"Minor\"","6bb4b0da":"This is a very obvious difference.  Clearly being female greatly increased your chances of survival.","80a9794c":"<a id=\"t2.1.\"><\/a>\n## 2.1.    Age - Missing Values","ea8885c5":"Passengers who boarded in Cherbourg, France, appear to have the highest survival rate.  Passengers who boarded in Southhampton were marginally less likely to survive than those who boarded in Queenstown.  This is probably related to passenger class, or maybe even the order of room assignments (e.g. maybe earlier passengers were more likely to have rooms closer to deck). <br> It's also worth noting the size of the whiskers in these plots.  Because the number of passengers who boarded at Southhampton was highest, the confidence around the survival rate is the highest.  The whisker of the Queenstown plot includes the Southhampton average, as well as the lower bound of its whisker.  It's possible that Queenstown passengers were equally, or even more, ill-fated than their Southhampton counterparts.","2636b37b":"<a id=\"t3.2.\"><\/a>\n## 3.2. Exploration of Fare","3218a969":"<a id=\"t3.\"><\/a>\n# 3. Exploratory Data Analysis","fee312d6":"<a id=\"t4.2.2.\"><\/a>\n### 4.2.2. Model evaluation based on K-fold cross-validation using `cross_val_score()` function ","bb581892":"<a id=\"t4.2.1.\"><\/a>\n### 4.2.1. Model evaluation based on simple train\/test split using `train_test_split()` function","864a27aa":"77% of records are missing, which means that imputing information and using this variable for prediction is probably not wise.  We'll ignore this variable in our model.","7365cb0c":"Since \"Age\" is (right) skewed, using the mean might give us biased results by filling in ages that are older than desired. To deal with this, we'll use the median to impute the missing values. ","abf93495":"I'll also create categorical variables for Passenger Class (\"Pclass\"), Gender (\"Sex\"), and Port Embarked (\"Embarked\"). ","04811dbb":"<font color=red>  Note: there is no target variable into test data (i.e. \"Survival\" column is missing), so the goal is to predict this target using different machine learning algorithms such as logistic regression. <\/font>","eeac5aeb":"The age distribution for survivors and deceased is actually very similar.  One notable difference is that, of the survivors, a larger proportion were children.  The passengers evidently made an attempt to save children by giving them a place on the life rafts. ","358139b2":"Unsurprisingly, being a first class passenger was safest.","4fc8a089":"<a id=\"t3.6.\"><\/a>\n## 3.6. Exploration of Gender Variable","537e21b9":"As we see, eight variables were kept. ","b90a9e9a":"As the distributions are clearly different for the fares of survivors vs. deceased, it's likely that this would be a significant predictor in our final model.  Passengers who paid lower fare appear to have been less likely to survive.  This is probably strongly correlated with Passenger Class, which we'll look at next.","b1a36a29":"<a id=\"t4.1.2.\"><\/a>\n### 4.1.2. Feature ranking with recursive feature elimination and cross-validation\n\nRFECV performs RFE in a cross-validation loop to find the optimal number or the best number of features. Hereafter a recursive feature elimination applied on logistic regression with automatic tuning of the number of features selected with cross-validation.","ebfd7f05":"<a id=\"t2.\"><\/a>\n# 2. Data Quality & Missing Value Assessment","fa331de1":"### Now, apply the same changes to the test data. <br>\nI will apply to same imputation for \"Age\" in the Test data as I did for my Training data (if missing, Age = 28).  <br> I'll also remove the \"Cabin\" variable from the test data, as I've decided not to include it in my analysis. <br> There were no missing values in the \"Embarked\" port variable. <br> I'll add the dummy variables to finalize the test set.  <br> Finally, I'll impute the 1 missing value for \"Fare\" with the median, 14.45.","68b9fdd5":"<a id=\"t2.2.\"><\/a>\n## 2.2. Cabin - Missing Values","9a9c9954":"<a id=\"t4.1.\"><\/a>\n## 4.1. Feature selection\n\n<a id=\"t4.1.1.\"><\/a>\n### 4.1.1. Recursive feature elimination\n\nGiven an external estimator that assigns weights to features, recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a `coef_ attribute` or through a `feature_importances_` attribute. Then, the least important features are pruned from current set of features.That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.\n\nReferences: <br>\nhttp:\/\/scikit-learn.org\/stable\/modules\/feature_selection.html <br>","d3520c79":"There are only 2 (0.22%) missing values for \"Embarked\", so we can just impute with the port where most people boarded.","01063186":"<a id=\"t2.4.1.\"><\/a>\n## 2.4.1. Additional Variables","a8921530":"Based on my assessment of the missing values in the dataset, I'll make the following changes to the data:\n* If \"Age\" is missing for a given row, I'll impute with 28 (median age).\n* If \"Embarked\" is missing for a riven row, I'll impute with \"S\" (the most common boarding port).\n* I'll ignore \"Cabin\" as a variable. There are too many missing values for imputation. Based on the information available, it appears that this value is associated with the passenger's class and fare paid.","a5ee88c1":"<a id=\"t2.3.\"><\/a>\n## 2.3. Embarked - Missing Values","e0644a44":"<a id=\"t2.4.\"><\/a>\n## 2.4. Final Adjustments to Data (Train & Test)","aef27c00":"<a id=\"t3.5.\"><\/a>\n## 3.5. Exploration of Traveling Alone vs. With Family","c9efd528":"<a id=\"t3.3.\"><\/a>\n## 3.3. Exploration of Passenger Class","1de45442":"<a id=\"t4.\"><\/a>\n# 4. Logistic Regression and Results","b7801e55":"By far the most passengers boarded in Southhampton, so we'll impute those 2 NaN's w\/ \"S\".","4ae6dafd":"<a id=\"t3.1.\"><\/a>\n## 3.1. Exploration of Age"}}