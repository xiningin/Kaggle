{"cell_type":{"8891a161":"code","b95c8824":"code","d904c670":"code","8fd1da7c":"code","51b5412a":"code","0a7143af":"code","f39b0fac":"code","6009dac0":"code","74086d23":"code","46e25209":"code","e99a11b9":"code","ee416f12":"code","cba47fb3":"code","0431a9db":"code","93c87b69":"code","adb32b13":"code","a7438430":"code","547729da":"code","284de8f4":"code","8a5a45e8":"code","9b326cb3":"code","f261a388":"code","8ff1ca72":"code","96dbf9bd":"code","a28d32ec":"code","a3e5e98a":"code","614bde17":"code","c0559265":"code","e65331db":"code","784ab905":"code","4071c14f":"code","df40bb72":"code","6470afb1":"code","9c7930a2":"code","67ce52e4":"code","5063291e":"code","6282fde8":"code","9c18f7ba":"code","5be3dbd6":"code","2e8eadb7":"code","a39b67b9":"code","286b0ddf":"code","cca11560":"code","d3bad780":"code","a9bfc6ba":"code","7d4eea65":"code","51756ed0":"code","8ebe5c64":"code","42733c3c":"code","64349bea":"code","c0478aa9":"code","0e0048ad":"code","9994e3a0":"code","698f29e6":"code","f1c935d9":"code","c69d68d1":"code","139ea804":"code","f214e5d0":"code","e1c87e5d":"code","308b7436":"code","ea521909":"code","d6f6bda0":"code","b7a62e7b":"code","8911959a":"code","b505f51c":"code","7330faf3":"code","274a4bb9":"code","228edf03":"markdown","87d06d6b":"markdown","afbae060":"markdown","3e05f6c7":"markdown","5e04241d":"markdown","5895031e":"markdown","fbf5937f":"markdown","02dd22e7":"markdown","355afeed":"markdown","54d9bab6":"markdown","44cc5048":"markdown","50b2695e":"markdown","c9916b16":"markdown","83e400a7":"markdown","6b385114":"markdown","703eaa86":"markdown","dc0ee028":"markdown","cf453afe":"markdown","b31e1d55":"markdown","89192023":"markdown","86a9512c":"markdown","ab7260e4":"markdown","ea8f97c8":"markdown","2d6bf0ea":"markdown","c0218a0c":"markdown","95b8b204":"markdown","d54bc218":"markdown","f16391ce":"markdown","a00b9c61":"markdown","a53b224d":"markdown","77ea9136":"markdown","a0c68833":"markdown","e659d708":"markdown","b6926b16":"markdown","de730721":"markdown","1a1c5285":"markdown","76c3fd94":"markdown","a3c4e64e":"markdown","86afeadc":"markdown","5bd139f1":"markdown","2071a5c5":"markdown","d3a78a58":"markdown","4c1f3574":"markdown","081a7c1b":"markdown","8c8ca0f9":"markdown"},"source":{"8891a161":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b95c8824":"train_df = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_PassengerId = test_df[\"PassengerId\"]","d904c670":"train_df.shape","8fd1da7c":"train_df.head()","51b5412a":"train_df.info()","0a7143af":"train_df.describe()","f39b0fac":"def bar_plot(column_name):\n\n    # count number of categorical variable(value\/sample)\n    number_of_values=train_df[column_name].value_counts()\n\n    # visualize\n    plt.figure(figsize=(9,3))\n    plt.bar(number_of_values.index,number_of_values)\n    plt.xticks(number_of_values.index, number_of_values.index.values)\n    plt.ylabel(\"Frequency\")\n    plt.title(column_name)\n    plt.show()\n    print(\"{}: \\n {}\".format(column_name,number_of_values))","6009dac0":"cat_variables_sense=[\"Survived\", \"Sex\", \"Pclass\", \"Embarked\", \"SibSp\", \"Parch\"]\nfor i in cat_variables_sense:\n    bar_plot(i)","74086d23":"cat_variables_nonsense = [\"Cabin\", \"Name\", \"Ticket\"]\ncat_variables= cat_variables_sense + cat_variables_nonsense","46e25209":"def hist_plot(column_name):\n    \n    plt.figure(figsize=(9,3))\n    plt.hist(train_df[column_name],bins=50)\n    plt.xlabel(\"column_name\")\n    plt.ylabel(\"Frequency\")\n    plt.title(\"{} distribution with hist\".format(column_name))\n    plt.show()","e99a11b9":"num_variables=[\"Fare\", \"Age\",\"PassengerId\"]\nfor i in num_variables:\n    hist_plot(i)","ee416f12":"train_df[[\"Pclass\",\"Survived\"]].groupby(\"Pclass\").mean().sort_values(by=\"Survived\",ascending=False)","cba47fb3":"train_df[[\"Sex\",\"Survived\"]].groupby(\"Sex\").mean().sort_values(by=\"Survived\",ascending=False)","0431a9db":"train_df[[\"SibSp\",\"Survived\"]].groupby(\"SibSp\").mean().sort_values(by=\"Survived\",ascending=False)","93c87b69":"train_df[[\"Parch\",\"Survived\"]].groupby(\"Parch\").mean().sort_values(by=\"Survived\",ascending=False)","adb32b13":"def outlier_detection(df,feature):\n    df[feature]=sorted(df[feature])\n    Q1, Q3 = np.percentile(df[feature] , [25,75])\n    IQR = Q3 - Q1\n    lower_range = Q1 - (1.5 * IQR)\n    upper_range = Q3 + (1.5 * IQR)\n    outliers=[]\n    for i in df[feature]:\n        if (i < lower_range) | (i > upper_range):\n            outliers.append(i)\n    print(len(outliers),\" values detected and fixed of\", feature, \"feature.\")\n    df[feature][df.loc[:,feature]<lower_range]=lower_range\n    df[feature][df.loc[:,feature]>upper_range]=upper_range\n    \n    return df[feature]","a7438430":"for col in num_variables:\n    train_df[col]= outlier_detection(train_df,col) # Main DF CHANGED\n    test_df[col]=outlier_detection(test_df,col) # Main DF CHANGED","547729da":"train_df_len = len(train_df)\ntrain_df = pd.concat([train_df,test_df],axis = 0).reset_index(drop = True)  # Main DF CHANGED","284de8f4":"import missingno as msno\nmsno.matrix(train_df);","8a5a45e8":"train_df.isnull().sum()","9b326cb3":"import statistics as stats\ntrain_df[\"Embarked\"] = train_df[\"Embarked\"].fillna(stats.mode(train_df[\"Embarked\"])) # Main DF CHANGED\ntrain_df[train_df[\"Embarked\"].isnull()]","f261a388":"train_df[\"Fare\"] = train_df[\"Fare\"].fillna(stats.mode(train_df[\"Fare\"]))","8ff1ca72":"train_df[\"Sex\"] = [1 if i == \"male\" else 0 for i in train_df[\"Sex\"]] # Main DF CHANGED\nsns.heatmap(train_df[[\"Age\",\"Sex\",\"SibSp\",\"Parch\",\"Pclass\"]].corr(), annot = True, cmap = \"coolwarm\")\nplt.show()","96dbf9bd":"index_nan_age = list(train_df[\"Age\"][train_df[\"Age\"].isnull()].index)\nfor i in index_nan_age:\n    age_pred = (train_df[\"Age\"][((train_df[\"SibSp\"] == train_df.iloc[i][\"SibSp\"]) &(train_df[\"Parch\"] == train_df.iloc[i][\"Parch\"]) & \\\n                                 (train_df[\"Pclass\"] == train_df.iloc[i][\"Pclass\"]))].median())\n    \n    # sometimes some values can't fill with upper method. so we will impute them with that column's mean\n    age_med = train_df[\"Age\"].median()\n    if not np.isnan(age_pred):\n        train_df[\"Age\"].iloc[i] = age_pred # Main DF CHANGED\n    else:\n        train_df[\"Age\"].iloc[i] = age_med # Main DF CHANGED\n","a28d32ec":"train_df[train_df[\"Age\"].isnull()]","a3e5e98a":"len(train_df.Cabin), train_df.Cabin.isnull().sum()","614bde17":"train_df.drop(labels = [\"Cabin\"], axis = 1, inplace = True) # Main DF CHANGED","c0559265":"g = sns.factorplot(x = \"SibSp\", y = \"Survived\", data = train_df, kind = \"bar\", size = 6)\ng.set_ylabels(\"Survived Probability\")\nplt.show()","e65331db":"g = sns.factorplot(x = \"Parch\", y = \"Survived\", kind = \"bar\", data = train_df, size = 6)\ng.set_ylabels(\"Survived Probability\")\nplt.show()","784ab905":"train_df[\"FamilyMembers\"]=train_df[\"Parch\"]+train_df[\"SibSp\"]+1 # Main DF CHANGED\ntrain_df.drop(columns=[\"Parch\",\"SibSp\"], axis=1, inplace=True)\ntrain_df","4071c14f":"g = sns.factorplot(x = \"FamilyMembers\", y = \"Survived\", data = train_df, kind = \"bar\")\ng.set_ylabels(\"Survival\")\nplt.show()","df40bb72":"train_df[\"FamilyMembers\"]=[1 if i < 4.5 else 0 for i in train_df[\"FamilyMembers\"]] # Main DF CHANGED\ntrain_df[\"FamilyMembers\"].head()","6470afb1":"sns.countplot(x = \"FamilyMembers\", data = train_df)\nplt.show()","9c7930a2":"g = sns.factorplot(x = \"Pclass\", y = \"Survived\", data = train_df, kind = \"bar\", size = 6)\ng.set_ylabels(\"Survived Probability\")\nplt.show()","67ce52e4":"train_df.Pclass.dtypes","5063291e":"train_df[\"Pclass\"] = train_df[\"Pclass\"].astype(\"category\") \ntrain_df = pd.get_dummies(train_df, columns= [\"Pclass\"]) # Main DF CHANGED\ntrain_df.head()","6282fde8":"g = sns.FacetGrid(train_df, col = \"Survived\")\ng.map(sns.distplot, \"Age\", bins = 25)\nplt.show()","9c18f7ba":"train_df[\"Title\"] = [i.split(\".\")[0].split(\",\")[-1].strip() for i in train_df[\"Name\"]] # Main DF CHANGED","5be3dbd6":"train_df[\"Title\"].head()","2e8eadb7":"sns.countplot(x=\"Title\", data = train_df)\nplt.xticks(rotation = 60)\nplt.show()","a39b67b9":"# convert to categorical\ntrain_df[\"Title\"] = train_df[\"Title\"].replace([\"Lady\",\"the Countess\",\"Capt\",\"Col\",\"Don\",\"Dr\",\"Major\",\"Rev\",\"Sir\",\"Jonkheer\",\"Dona\"],\"other\") # Main DF CHANGED\ntrain_df[\"Title\"] = ( [0 if i == \"Master\" else 1 if i == \"Miss\" or i == \"Ms\" or \\\n                       i == \"Mlle\" or i == \"Mrs\" else 2 if i == \"Mr\" else 3 for i in train_df[\"Title\"]] )  # Main DF CHANGED\ntrain_df[\"Title\"].head()","286b0ddf":"sns.countplot(x=\"Title\", data = train_df)\nplt.xticks(rotation = 60)\nplt.show()","cca11560":"train_df.drop(labels = [\"Name\"], axis = 1, inplace = True) # Main DF CHANGED","d3bad780":"train_df = pd.get_dummies(train_df,columns=[\"Title\"]) # Main DF CHANGED\ntrain_df.head()","a9bfc6ba":"sns.countplot(x = \"Embarked\", data = train_df);","7d4eea65":"train_df=pd.get_dummies(train_df, columns=[\"Embarked\"]) # Main DF CHANGED\ntrain_df","51756ed0":"train_df.Ticket.head(15)","8ebe5c64":"tickets = []\nfor i in list(train_df.Ticket):\n    if not i.isdigit():\n        tickets.append(i.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(\" \")[0])\n    else:\n        tickets.append(\"X\")\ntrain_df[\"Ticket\"] = tickets # Main DF CHANGED\ntrain_df[\"Ticket\"].unique()","42733c3c":"from category_encoders import BinaryEncoder\nencoder=BinaryEncoder(cols=[\"Ticket\"])\ntrain_df=encoder.fit_transform(train_df) # Main DF CHANGED\ntrain_df","64349bea":"sns.countplot(x=\"Sex\", data=train_df);","c0478aa9":"train_df[\"Sex\"] = train_df[\"Sex\"].astype(\"category\")\ntrain_df = pd.get_dummies(train_df, columns=[\"Sex\"]) # Main DF CHANGED\ntrain_df.head()","0e0048ad":"train_df.drop(labels = [\"PassengerId\"], axis = 1, inplace = True) # Main DF CHANGED","9994e3a0":"test = train_df[train_df_len:]\ntest.drop(labels = [\"Survived\"],axis = 1, inplace = True) # Main DF CHANGED","698f29e6":"from sklearn.model_selection import train_test_split\ntrain = train_df[:train_df_len] # Main DF CHANGED\nX_train = train.drop(labels = \"Survived\", axis = 1) # Main DF CHANGED\ny_train = train[\"Survived\"] # Main DF CHANGED\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.33, random_state = 42) # Main DF CHANGED\n\nprint( \"X_train\", len(X_train), \"\\nX_test\", len(X_test), \"\\ny_train\", len(y_train), \"\\ny_test\", len(y_test), \"\\ntest\", len(test) )","f1c935d9":"def models(X_train,Y_train):\n    \n    #use logistic regression\n    from sklearn.linear_model import LogisticRegression\n    log=LogisticRegression(random_state=42)\n    log.fit(X_train,Y_train)\n    \n    #use KNeighbors\n    from sklearn.neighbors import KNeighborsClassifier\n    knn=KNeighborsClassifier(n_neighbors=5,metric=\"minkowski\",p=2)\n    knn.fit(X_train,Y_train)\n    \n    #use SVC (linear kernel)\n    from sklearn.svm import SVC\n    svc_lin=SVC(kernel=\"linear\",random_state=42,probability=True)\n    svc_lin.fit(X_train,Y_train)\n    \n    #use SVC (RBF kernel)\n    svc_rbf=SVC(kernel=\"rbf\",random_state=42,probability=True)\n    svc_rbf.fit(X_train,Y_train)\n    \n    #use GaussianNB\n    from sklearn.naive_bayes import GaussianNB\n    gauss=GaussianNB()\n    gauss.fit(X_train,Y_train)\n    \n    #use Decision Tree\n    from sklearn.tree import DecisionTreeClassifier\n    tree=DecisionTreeClassifier(criterion=\"entropy\",random_state=42)\n    tree.fit(X_train,Y_train)\n    \n    #use Random Forest Classifier\n    from sklearn.ensemble import RandomForestClassifier\n    forest=RandomForestClassifier(n_estimators=10,criterion=\"entropy\",random_state=42)\n    forest.fit(X_train,Y_train)\n    \n    # use Hist Gradient Boosting Classifier\n    from sklearn.experimental import enable_hist_gradient_boosting\n    from sklearn.ensemble import HistGradientBoostingClassifier\n    histgrad=HistGradientBoostingClassifier()\n    histgrad.fit(X_train,y_train)\n    \n    # use GBM\n    from sklearn.ensemble import GradientBoostingClassifier\n    gbm=GradientBoostingClassifier()\n    gbm.fit(X_train,y_train)\n    \n    # use XGBoost\n    #!pip install xgboost\n    from xgboost import XGBClassifier\n    xgboost=XGBClassifier()\n    xgboost.fit(X_train,y_train)\n    \n    # use LightGBM\n    #!conda install -c conda-forge lightgbm\n    from lightgbm import LGBMClassifier\n    lightgbm=LGBMClassifier()\n    lightgbm.fit(X_train,y_train)\n\n    #print the training scores for each model\n    print('[0] Logistic Regression Training Score:',log.score(X_train,Y_train))\n    print('\\n[1] K Neighbors Training Score:',knn.score(X_train,Y_train))\n    print('\\n[2] SVC Linear Training Score:',svc_lin.score(X_train,Y_train))\n    print('\\n[3] SVC RBF Training Score:',svc_rbf.score(X_train,Y_train))\n    print('\\n[4] Gaussian Training Score:',gauss.score(X_train,Y_train))\n    print('\\n[5] Decision Tree Training Score:',tree.score(X_train,Y_train))\n    print('\\n[6] Random Forest Training Score:',forest.score(X_train,Y_train))\n    print('\\n[7] Hist Gradient Boosting Training Score:',histgrad.score(X_train,Y_train))\n    print('\\n[8] Gradient Boosting Training Score:',gbm.score(X_train,Y_train))\n    print('\\n[9] XGBoost Training Score:',xgboost.score(X_train,Y_train))\n    print('\\n[10] Light GBM Training Score:',lightgbm.score(X_train,Y_train))\n    \n    return log,knn,svc_lin,svc_rbf,gauss,tree,forest,histgrad,gbm,xgboost,lightgbm","c69d68d1":"log,knn,svc_lin,svc_rbf,gauss,tree,forest,histgrad,gbm,xgboost,lightgbm=models(X_train,y_train)","139ea804":"models=[log,knn,svc_lin,svc_rbf,gauss,tree,forest,histgrad,gbm,xgboost,lightgbm]\nmodels_name=[\"log\",\"knn\",\"svc_lin\",\"svc_rbf\",\"gauss\",\"tree\",\"forest\",\"histgrad\",\"gbm\",\"xgboost\",\"lightgbm\"]","f214e5d0":"before_tune_accuracy_score={}\ndef accuracy_score_calculator(model,model_name):\n    from sklearn.metrics import accuracy_score\n    y_pred=model.predict(X_test)\n    before_tune_accuracy_score[model_name]=accuracy_score(y_test,y_pred)","e1c87e5d":"before_tune_roc_score={}\ndef roc_score_calculator(model,model_name):\n    from sklearn.metrics import roc_auc_score\n    y_pred=model.predict_proba(X_test)[:,1]\n    before_tune_roc_score[model_name]=roc_auc_score(y_test,y_pred)","308b7436":"for i in range(len(models)):\n    roc_score_calculator(models[i],models_name[i])\n    accuracy_score_calculator(models[i],models_name[i])","ea521909":"size=np.arange(len(models))\nplt.bar(size-0.2, before_tune_roc_score.values(), color='g', width=0.4, tick_label=models_name)\nplt.bar(size+0.2, before_tune_accuracy_score.values(),color='b', width=0.4, tick_label=models_name)\nplt.legend([\"Before Roc Score\", \"Before Accuracy Score\"]);","d6f6bda0":"from sklearn.metrics import plot_roc_curve\nplt.figure(figsize=(10,10))\nax = plt.gca()\nlog_disp = plot_roc_curve(log, X_test, y_test, ax=ax, alpha=0.8)\nknn_disp = plot_roc_curve(knn, X_test, y_test, ax=ax, alpha=0.8)\nsvc_lin_disp = plot_roc_curve(svc_lin, X_test, y_test, ax=ax, alpha=0.8)\nsvc_rbf_disp = plot_roc_curve(svc_rbf, X_test, y_test, ax=ax, alpha=0.8)\ngauss_disp = plot_roc_curve(gauss, X_test, y_test, ax=ax, alpha=0.8)\ntree_disp = plot_roc_curve(tree, X_test, y_test, ax=ax, alpha=0.8)\nforest_disp = plot_roc_curve(forest, X_test, y_test, ax=ax, alpha=0.8)\nhistgrad_disp = plot_roc_curve(histgrad, X_test, y_test, ax=ax, alpha=0.8)\ngbm_disp = plot_roc_curve(gbm, X_test, y_test, ax=ax, alpha=0.8)\nxgboost_disp = plot_roc_curve(xgboost, X_test, y_test, ax=ax, alpha=0.8)\nlightgbm_disp = plot_roc_curve(lightgbm, X_test, y_test, ax=ax, alpha=0.8)\nplt.legend(loc = 'lower right', prop={'size': 16})\nplt.show()","b7a62e7b":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nclassifier = [ SVC(random_state = 42) , LogisticRegression(random_state = 42), \n              GradientBoostingClassifier(random_state = 42) ]\n\ngbm_param_grid = { \"loss\":[\"deviance\"], \"learning_rate\": [0.01, 0.05, 0.1, 0.2], \n                  \"min_samples_split\": np.linspace(0.1, 0.5, 12), \"min_samples_leaf\": np.linspace(0.1, 0.5, 12), \n                  \"max_depth\":[3,5,8], \"subsample\":[0.5, 0.8, 0.9, 1.0], \"n_estimators\":[10] }\n\nsvc_param_grid = {\"kernel\" : [\"linear\"], \"probability\":[True], \"gamma\": [0.001, 0.01, 0.1, 1] }\n\nlogreg_param_grid = {\"C\":np.logspace(-3,3,7), \"penalty\": [\"l1\",\"l2\"], \"solver\": ['liblinear'] }\n\nclassifier_param = [svc_param_grid, logreg_param_grid, gbm_param_grid ]","8911959a":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\n\ncv_result = []\nbest_estimators = []\nfor i in range(len(classifier)):\n    clf = GridSearchCV(classifier[i], param_grid=classifier_param[i], cv = StratifiedKFold(n_splits = 10), scoring = \"accuracy\", n_jobs = -1, verbose=1)\n    clf.fit(X_train,y_train)\n    cv_result.append(clf.best_score_)\n    best_estimators.append(clf.best_estimator_)\n    print(cv_result[i])","b505f51c":"cv_results = pd.DataFrame({\"Cross Validation Means\":cv_result, \n                           \"ML Models\":[\"LogisticRegression\", \"GradientBoostingClassifier\", \"SVC\"]})\n\ng = sns.barplot(\"Cross Validation Means\", \"ML Models\", data = cv_results)\ng.set_xlabel(\"Mean Accuracy\")\ng.set_title(\"Cross Validation Scores\");","7330faf3":"from sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import VotingClassifier\nvotingC = VotingClassifier(estimators = [(\"dt\",best_estimators[0]),\n                                        (\"rfc\",best_estimators[1]),\n                                        (\"lr\",best_estimators[2])],\n                                        voting = \"soft\", n_jobs = -1)\nvotingC = votingC.fit(X_train, y_train)\naccuracy_score(votingC.predict(X_test),y_test)","274a4bb9":"test_survived = pd.Series(votingC.predict(test), name = \"Survived\").astype(int)\nresults = pd.concat([test_PassengerId, test_survived],axis = 1)\nresults.to_csv(\"titanic.csv\", index = False)","228edf03":"We don't plot some of them ( \"Cabin\", \"Name\", \"Ticket\" ) because won't make any sense.","87d06d6b":"Now I will visualize all accuracy and roc auc score with histogram graph.","afbae060":"We are gonna get PassengerId column from test data because when we submit our project, we will need them and their possibility of staying alive.","3e05f6c7":"Now we will implement binary encoding to them. Because this feature has more than 5 unique values.","5e04241d":"<a id = \"13\"><\/a><br>\n## Parch and SibSp-- Survived","5895031e":"<a id = \"4\"><\/a><br>\n## Variable Analysis\n1. Categorical Variables: Survived, Sex, Pclass, Embarked, Cabin, Name, Ticket, SibSp and Parch\n2. Numerical Variables: Fare, age and passengerId","fbf5937f":"<a id = \"2\"><\/a><br>\n# Variable Description","02dd22e7":"Here also having less children increase ratio of survive. So we can concat these two features as FamilyMembers.","355afeed":"<a id = \"20\"><\/a><br>\n## Drop PassengerID\n\nI will drop Passenger ID feature because it doesn't make any sense to person's life.","54d9bab6":"As we see, having less then 3 children increase ratio of survive.","44cc5048":"I chose some features which I think related with Age feature. So let's compare their correlation.","50b2695e":"<a id = \"18\"><\/a><br>\n## Ticket","c9916b16":"Here we will see all roc auc scores in a roc_curve graph.","83e400a7":"\n* PassengerId => ID number of passenger\n* Survived => After crash, Alive (1) or Dead (0)\n* Pclass => 1. Class, 2. Class, 3. Class\n* Name => Name of passenger\n* Sex => Gender of passenger\n* Age => Age of passenger\n* SibSp => Number of siblings of passenger in ship\n* Parch => Number of children of passenger in ship\n* Ticket => Ticket number\n* Fare => Price of ticket\n* Cabin => Number of Cabin\n* Embarked => Name of port ( C = Cherbourg, Q = Queenstown, S = Southampton )","6b385114":"<a id = \"1\"><\/a> \n# Import Data and Libraries","703eaa86":"<a id = \"16\"><\/a><br>\n## Name -- Title\n\nWe can't predict the person could stay alive or not because of their name but we can predict by their title. So now we are gonna get every name's title instead of name.","dc0ee028":"There is many different unique values. At the first part of ticket feature, there are some string and then some digits. So let's take their only string part. Because digit part probably unique values and meaningles as name feature.\n\nNow we will replace \".\" and \"\/\" characters with \"\". Then will delete their spaces and split and take first value of list. If there is only digit value, we will call them as \"X\". ","cf453afe":"<a id = \"17\"><\/a><br>\n## Embarked\n\nOne-hot encoding for Embarked feature.","b31e1d55":"<font color = 'red'>\nContent: \n\n1. [Import Data and Libraries](#1)\n2. [Variable Description and Analysis](#2)\n    * [Feature Types](#3)\n    * [Variable Analysis](#4)\n        * [Categorical Variable](#5)\n        * [Numerical Variable](#6)\n3. [Basic Data Analysis](#7)\n4. [Outlier Detection](#8)\n5. [Handle Missing Value](#9)\n    * [Finding Missing Value](#10)\n    * [Filling Missing Value](#11)\n6. [Visualization and Feature Engineering](#12)\n    * [Parch and SibSp -- Survived](#13)\n    * [Pclass -- Survived](#14)\n    * [Age -- Survived](#15)\n    * [Name -- Title](#16)\n    * [Embarked](#17)\n    * [Ticket](#18)\n    * [Sex](#19)\n    * [Drop Passenger ID](#20)\n\n7. [Train Test Split](#21)\n8. [Modeling](#22)\n    * [Hyperparameter Tuning -- Grid Search -- Cross Validation](#23) \n    * [Ensemble Modeling](#24)\n    * [Prediction and Submission](#25)","89192023":"<a id = \"25\"><\/a><br>\n## Prediction and Submission\n\nHere we will get csv file to make submission after commit.","86a9512c":"<a id = \"11\"><\/a><br>\n## Fill Missing Value\n\nEmbarked has 2 missing value<br\/>\nFare has only 1","ab7260e4":"As we see, %77 of Cabin values are NaN and I think Cabin feature is meaningless as the name of passengers. So I will drop Cabin feature.","ea8f97c8":"<a id = \"9\"><\/a><br>\n# Missing Value\n\nLet's concat both datas before missing imputation.","2d6bf0ea":"<a id = \"15\"><\/a><br>\n## Age -- Survived","c0218a0c":"<a id = \"10\"><\/a><br>\n## Find Missing Value","95b8b204":"<a id = \"3\"><\/a><br>\n## Feature types\n* float64(2) => Fare and Age\n* int64(5) => Pclass, sibsp, parch, passengerId and survived\n* object(5) => Cabin, embarked, ticket, name and sex","d54bc218":"Now we will drop the Name feature.","f16391ce":"<a id = \"8\"><\/a><br>\n# Outlier Detection\n\nNow, we will find the outliers and change them with upper or lower bounds. (IQR Method)","a00b9c61":"# Introduction\n\nEverybody knows something about Titanic. And unfortunately it's accident. Here I try to test, if I would be on that ship with my family, could I stay alive. So could you? Let's find out.\n\n##        Which models used\nWe used the following models on this notebook:\n\n* Logistic Regression\n* KNeighbors Classifier\n* SVC (Linear)\n* SVC (RBF)\n* Naive Bayes\n* Decision Tree Classifier\n* Random Forest Classifier\n* Hist Gradient Boosting Classifier\n* Gradient Boosting Classifier\n* XGBoost Classifier\n* Ligth GBM Classifier","a53b224d":"<a id = \"6\"><\/a><br>\n### Numerical Variable","77ea9136":"<a id = \"7\"><\/a><br>\n# Basic Data Analysis\n\nWill try to find relation between some features.\n\n* Pclass - Survived\n* Sex - Survived\n* SibSp - Survived\n* Parch - Survived","a0c68833":"Here we found Logistic Regression, SVC Linear and GBM models best fits for our data. So let's tune them with ensemble learning way.","e659d708":"<a id = \"22\"><\/a><br>\n# Modeling\n\nHere I have a function to get 11 different models' score.","b6926b16":"<a id = \"5\"><\/a><br>\n### Categorical Variable","de730721":"Pclass feature is actually categorical but it's type int64. So let's convert it to category then implement one-hot encoding.","1a1c5285":"<a id = \"14\"><\/a><br>\n## Pclass -- Survived","76c3fd94":"<a id = \"23\"><\/a><br>\n## Hyperparameter Tuning -- Grid Search -- Cross Validation","a3c4e64e":"<a id = \"19\"><\/a><br>\n## Sex\n\nImplementing one-hot encoding to Sex feature either.","86afeadc":"<a id = \"24\"><\/a><br>\n## Ensemble Modeling","5bd139f1":"<a id = \"12\"><\/a><br>\n# Visualization and Feature Engineering","2071a5c5":"As we see after 4 member, the survival ratio is decrease. So let's take a threshold ( for this example, I will take 4.5 ) and categorize them lower than 4.5 and upper than 4.5.","d3a78a58":"We see here that around 30 age people couldn't survive mostly. Most passengers are between 15-40 age range.","4c1f3574":"Age is not correlated with sex but it is correlated with Parch, Sibsp and Pclass. <br\/>\nSo let's impute missing values of Age feature according to these 3 features' mean.","081a7c1b":"<a id = \"21\"><\/a><br>\n# Train - Test Split","8c8ca0f9":"Let's check relation between FamilySize and Survived features."}}