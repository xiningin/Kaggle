{"cell_type":{"c0f70628":"code","014f47f5":"code","c205ff99":"code","02ead131":"code","8b97badd":"code","edff02eb":"code","8658986a":"code","d4a2c797":"code","f36a1563":"code","0a58f77f":"code","13598758":"code","1d977c6e":"code","f09ed55d":"code","2220fd12":"code","1e4f554a":"code","68f98fe1":"code","af967b20":"code","7ddadb11":"code","3bac6244":"code","667a19bc":"code","84b87f6c":"code","0f940fbf":"code","703efe96":"markdown","4f6a5aef":"markdown","63758efb":"markdown","20412c8f":"markdown","5871ba69":"markdown","eb8c2768":"markdown","5420c601":"markdown","731b18b3":"markdown","0e621d20":"markdown","8ebb3f39":"markdown","5a7458ce":"markdown","a34319e4":"markdown","8ada6ab8":"markdown","09df912b":"markdown","44fafbc1":"markdown","ad1facd8":"markdown","1ac658e9":"markdown","165f95d7":"markdown","b176bad9":"markdown"},"source":{"c0f70628":"import numpy as np \nimport pandas as pd \nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nprint(os.listdir(\"..\/input\"))","014f47f5":"path = \"..\/input\/\"","c205ff99":"df_pokemons = pd.read_csv(path+'pokemon.csv')\ndf_battles =pd.read_csv(path+'battles.csv')\ndf_test = pd.read_csv(path+'test.csv')","02ead131":"df_pokemons = df_pokemons.set_index('#')\ndf_pokemons = df_pokemons.fillna({'Type 1': 'None', 'Type 2': 'None'})","8b97badd":"df_pokemons.head()","edff02eb":"df_battles.head()","8658986a":"stats = ['HP', 'Attack', 'Defense', 'Sp. Atk', 'Sp. Def', 'Speed']","d4a2c797":"new_columns = []\nfor pokemon in ['First_pokemon', 'Second_pokemon']:\n    \n    for stat in stats:\n        df_battles[pokemon+stat] = df_pokemons.loc[df_battles[pokemon]][stat].values\n        df_test[pokemon+stat] = df_pokemons.loc[df_test[pokemon]][stat].values\n        \n    for column_to_include in ['Type 1', 'Type 2']:\n        new_column = pokemon+'_'+column_to_include\n        new_columns.append(new_column)\n        df_test[new_column] = df_pokemons.loc[df_test[pokemon]][column_to_include].values\n        df_battles[new_column] = df_pokemons.loc[df_battles[pokemon]][column_to_include].values","f36a1563":"df_battles.head()","0a58f77f":"df_test.head()","13598758":"train_df = df_battles.copy()\ntest_df = df_test.copy()\ntarget = train_df['Winner']","1d977c6e":"import lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score as sk_metric\nimport time","f09ed55d":"exclude_columns = ['battle_number', 'First_pokemon', 'Second_pokemon', 'Winner']\nfeatures = [column for column in df_battles.columns if column not in exclude_columns]\ncategorical_columns = ['First_pokemon_Type 1', 'First_pokemon_Type 2', 'Second_pokemon_Type 1', 'Second_pokemon_Type 2']","2220fd12":"for feature in categorical_columns:\n    train_df[feature] = train_df[feature].astype('category')\n    test_df[feature] = test_df[feature].astype('category')","1e4f554a":"folds = StratifiedKFold(n_splits=5, shuffle=True)","68f98fe1":"param = {\n#         'num_leaves': 150,\n         'objective':'binary',\n         'max_depth': -1,\n         'learning_rate': 0.001,\n         \"boosting\": \"gbdt\",\n#          \"feature_fraction\": 0.8,\n#          \"bagging_freq\": 1,\n#          \"bagging_fraction\": 0.8 ,\n#          \"bagging_seed\": 11,\n         \"metric\": ['binary_logloss', 'binary_error'],\n#          \"lambda_l1\": 0.1,\n#          \"lambda_l2\": 0.1,\n         'num_rounds': 90000,\n        \"early_stopping\": 1000}","af967b20":"oof = np.zeros(len(train_df))  #Predicciones del set de train\npredictions = np.zeros(len(test_df)) #Predicciones del set de test\nstart_time = time.time()\nfeature_importance_df = pd.DataFrame()\nscore = [0 for _ in range(folds.n_splits)]","7ddadb11":"for n_fold, (train_idx, val_idx) in enumerate(folds.split(train_df, target)):\n    \n    print(\"Fold: {}\".format(n_fold))\n    \n    #Para usar LIGHTGBM hay que usar sus propios datasets, entonces hay que crearlos.\n    train_data = lgb.Dataset(train_df.iloc[train_idx][features], label=target.iloc[train_idx], categorical_feature=categorical_columns)\n    val_data = lgb.Dataset(train_df.iloc[val_idx][features], label=target.iloc[val_idx], categorical_feature=categorical_columns)\n    \n    #Training\n    clf = lgb.train(param, train_data, valid_sets=[val_data], verbose_eval=1000)\n    \n    oof[val_idx] = clf.predict(train_df.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    \n    #Al ser \u00e1rboles de decisi\u00f3n podemos ver cuanto colabora cada feature a la hora de decidir la predicc\u00f3n, vamos a aprovecharlo e ir guardando estos valores tambi\u00e9n\n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance(importance_type='gain')\n    fold_importance_df[\"fold\"] = n_fold + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n\n    #Por cada Fold predecimos el conjunto de test\n    current_pred = clf.predict(test_df[features], num_iteration=clf.best_iteration)\n    predictions += current_pred \/ folds.n_splits\n   \n    print(\"time elapsed: {:<5.2}min \\n\".format((time.time() - start_time) \/ 60))\n    \n    #El accuracy con el que trabaja Sklearn solo funciona si le pasamos 1 o 0 directamente, por lo tanto tenemos que transofrmar un poco las predicciones que nos devuelve LIGHTGBM\n    sk_predictions = oof[val_idx]  \n    sk_predictions[sk_predictions >= .5] = 1 \n    sk_predictions[sk_predictions < .5] = 0\n    score[n_fold] = sk_metric(target.iloc[val_idx], sk_predictions)\n    print(\"Fold score: {}\".format(score[n_fold]))\n    \nprint(\"CV score: {:<8.5f}\".format(sum(score) \/ folds.n_splits))\n","3bac6244":"metrics = []\nfor x in np.arange(.4,.7, .01):\n    t = oof.copy()\n    t[t >= x] = int(1)\n    t[t < x] = int(0)\n    metrics.append((x,sk_metric(target, t)))\nthreshold = max(metrics, key=lambda item:item[1])[0]\n#Este ser\u00e1 el umbral que usemos para decidir si la batalla la gana o pierde un pokemon\nprint(threshold)","667a19bc":"cols = (feature_importance_df[[\"feature\", \"importance\"]]\n        .groupby(\"feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:1000].index)\n\nbest_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n\nplt.figure(figsize=(14,25))\nsns.barplot(x=\"importance\",\n            y=\"feature\",\n            data=best_features.sort_values(by=\"importance\",\n                                           ascending=False))\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()","84b87f6c":"sub_df = pd.DataFrame({\"battle_number\": test_df[\"battle_number\"].values})\npredictions[predictions >= threshold] = 1\npredictions[predictions < threshold] = 0\nsub_df['Winner'] = predictions.astype(int)\nsub_df[:10]","0f940fbf":"sub_df.to_csv(\"submit.csv\", index=False)","703efe96":"Preparamos nuestro dataset con nuestro umbral definido y ya lo tenemos listo para enviar a Kaggle","4f6a5aef":"Copiamos los datasets y renombramos, m\u00e1s que nada por si tenemos que repetir el experimento no cargarnos los datasets originales. Consejo de cafre.","63758efb":"Cargamos los datasets, nada nuevo:","20412c8f":"En este caso sale justo lo que pensamos que tiene que ser, pero no os confi\u00e9is.","5871ba69":"Aqu\u00ed tendr\u00e9is que echar un rato en comprender lo que se hace. Pero b\u00e1sicamente es por cada Fold entrenaremos un LIGHTGBM, intentaremos predecir las batallas de test y guardarlas en prediction y luego dividir por el n\u00famero de folds. \nLIGHTGBM en modo binario devuelve la probabilidades no directamente 'clases' 1 o 0, se ver\u00e1s m\u00e1s adelante.","eb8c2768":"## MODEL","5420c601":"Uh esos dos .head() nos han quedado muy bonitos","731b18b3":"Importamos las librer\u00edas necesarias","0e621d20":"Rellenamos los valores faltantes de tipos de Pokemon. En el Slack se comenta que falta un nombre, pero como realmente no lo vamos a usar que le vayan dando","8ebb3f39":"A ver si conseguimos animar al amigo Santi XD","5a7458ce":"Aqu\u00ed hacemos la selecci\u00f3n de features con las que trabajar\u00e1 LIGHTGBM. Tenemos que decirle que variables son categ\u00f3ricas y ponerlas como tal en el dataset","a34319e4":"Vale, parece que ha quedado todo m\u00e1s o menos como queremos. Vamos a aplicar LIGHTGBM","8ada6ab8":"Vemos que la velocidad es de las cosas que mas deciden un combate, como probablemente era de esperar, seguidamente del ataque del pokemon 1. Ya cada uno que saque sus conclusiones","09df912b":"Vamos con el CrossValidation. Un split de 5 deber\u00eda de ser suficiente.","44fafbc1":"Bueno y si ten\u00e9is alguna duda ya sab\u00e9is. Os jod\u00e9is. Que estoy muy liado troleando en el Slack como para andar respondiendo preguntas XD","ad1facd8":"Combinamos ambos datasets, elegimos las Stats que nos interesen de nuestros pokemon (todas en este caso salvo Legendary y Generation, si hubieramos creado alguna extra pues tambi\u00e9n entrar\u00eda)\n\nPD: Recordad que los cambios que hagamos en el dataset de train (en este caso es battles) tambi\u00e9n hay que aplicarlos en test.","1ac658e9":"Vale, sabemos que LIGHTGBM nos devuelve realmente probabilidades, por tanto puede que el umbral \u00f3ptimo en el cual decir si el Winner es 1 o 0 no tiene por qu\u00e9 ser 0.5. Vamos a intentar encontrar el umbral que nos d\u00e9 mejores resultados en train al menos.","165f95d7":"Vamos a ver cual es realmente la importancia de cada una de las columnas","b176bad9":"Vale, parte importante de LIGHTGBM, los par\u00e1metros. Se le pasa como diccionario de Python. Se explican solos y sino en la docu de la librer\u00eda vienen genial. Los que vienen por defecto suelen funcionar muy bien, as\u00ed que los dejo comentados para que pod\u00e1is ver como usarlos.\n\nObviamente al el objetivo es binario, y usaremos dos m\u00e9tricas para evaluar el algoritmo junto con el accuracy importado anteriormente de SKLearn."}}