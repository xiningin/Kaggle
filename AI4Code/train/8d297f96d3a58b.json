{"cell_type":{"5e97169f":"code","fb82809f":"code","a759471a":"code","127de708":"code","428fb83d":"code","031e65ca":"code","6607cb8e":"code","a99301d7":"code","4f1b1bee":"code","f3d0772e":"code","76a7f1cc":"code","d00c2e58":"code","5fde6e87":"code","30b6087a":"code","46af868b":"code","6e5db200":"markdown","d528df44":"markdown","a48cc48c":"markdown","f2bbedfb":"markdown","4a115331":"markdown","1aa60f14":"markdown","57e7ccb6":"markdown","83412625":"markdown","eb2056fd":"markdown","6a32d177":"markdown","42e35a2b":"markdown","8d644eba":"markdown","317971ec":"markdown","a017cf50":"markdown","913a723d":"markdown","582ef66c":"markdown","2c9a12bb":"markdown","078ecb24":"markdown","3f05aee7":"markdown","2d29482d":"markdown","b1ffd0f3":"markdown","215dee14":"markdown","33a899a3":"markdown","e054d0cf":"markdown","abd00e59":"markdown","3e9c0ce5":"markdown","09855a80":"markdown","bb64caa1":"markdown","1b8af1c0":"markdown","49d913b1":"markdown","53ab0236":"markdown","8e3b2e90":"markdown","6c50b50d":"markdown","3b1e3318":"markdown","ea612069":"markdown","f743436f":"markdown","05b8361e":"markdown","2001ea02":"markdown","dc5e0e9f":"markdown","20620e21":"markdown","fa9c9684":"markdown","fb22f839":"markdown","2ea56f27":"markdown","6260a5d7":"markdown","6cd0e92e":"markdown","04218cf4":"markdown","7207f5bd":"markdown","b6d4843b":"markdown","2da3aa62":"markdown","babec82f":"markdown","f4ce40b4":"markdown","7d835076":"markdown","e21d1bd4":"markdown","f334f6da":"markdown","7fb2c114":"markdown","3949c5f3":"markdown","0353cd94":"markdown","6bd73eb9":"markdown","f2430209":"markdown","a998b65a":"markdown","ae6978bb":"markdown","5b2509b0":"markdown","3b0065b5":"markdown","fba04695":"markdown","8e91a34f":"markdown","bc6ba6c6":"markdown","3558b6ec":"markdown","cdaddb83":"markdown","e50c709c":"markdown","508ff800":"markdown"},"source":{"5e97169f":"import numpy as np\nimport pandas as pd","fb82809f":"class Module:\n    def __init__(self):\n        self._train = True\n\n    def forward(self, input):\n        raise NotImplementedError\n\n    def backward(self, input, grad_output):\n        raise NotImplementedError\n\n    def parameters(self):\n        \"\"\"\n        Returns list of its parameters\n        \"\"\"\n        return []\n\n    def grad_parameters(self):\n        \"\"\"\n        Returns list of tensors gradients of its parameters\n        \"\"\"\n        return []\n\n    def train(self):\n        self._train = True\n\n    def eval(self):\n        self._train = False","a759471a":"class Criterion:\n    def forward(self, input, target):\n        raise NotImplementedError\n\n    def backward(self, input, target):\n        raise NotImplementedError","127de708":"class Linear(Module):\n    def __init__(self, dim_in, dim_out):\n        super().__init__()\n        self.W = np.random.randn(dim_in, dim_out)\n        self.b = np.random.randn(1, dim_out)\n\n    def forward(self, input):\n        self.output = (np.dot(input, self.W) + self.b)\n        return self.output\n\n    def backward(self, input, grad_output):\n        self.grad_b = np.mean(grad_output, axis=0)\n        self.grad_W = np.dot(input.T, grad_output)\n\n        self.grad_W \/= input.shape[0]\n\n        grad_input = np.dot(grad_output, self.W.T)\n\n        return grad_input\n\n    def parameters(self):\n        return [self.W, self.b]\n\n    def grad_parameters(self):\n        return [self.grad_W, self.grad_b]","428fb83d":"def softmax(xs):\n    xs = np.subtract(xs, xs.max(axis=1, keepdims=True))\n    xs = np.exp(xs) \/ np.sum(np.exp(xs), axis=1, keepdims=True)\n    return xs\n\n\nclass CrossEntropy(Criterion):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, input, target):\n        eps = 1e-9\n        predictions = np.clip(input, eps, 1. - eps)\n        N = predictions.shape[0]\n        ce = -np.sum(target * np.log(predictions))\n        return ce \/ N\n\n\n    def backward(self, input, target):\n        eps = 1e-9\n        input_clamp = np.clip(input, eps, 1 - eps)\n        return softmax(input_clamp) - target","031e65ca":"class Sequential(Module):\n    def __init__(self, *layers):\n        super().__init__()\n        self.layers = layers\n\n    def forward(self, input):\n        for layer in self.layers:\n            input = layer.forward(input)\n\n        self.output = input\n        return self.output\n\n    def backward(self, input, grad_output):\n        for i in range(len(self.layers) - 1, 0, -1):\n            grad_output = self.layers[i].backward(self.layers[i-1].output, grad_output)\n\n        grad_output = self.layers[0].backward(input, grad_output)\n\n        return grad_output\n\n    def parameters(self):\n        res = []\n        for l in self.layers:\n            res += l.parameters()\n        return res\n\n    def grad_parameters(self):\n        res = []\n        for l in self.layers:\n            res += l.grad_parameters()\n        return res\n\n    def train(self):\n        for layer in self.layers:\n            layer.train()\n\n    def eval(self):\n        for layer in self.layers:\n            layer.eval()","6607cb8e":"def sigmoid(x):\n    return 1 \/ (1 + np.exp(-x))\n\n\nclass Sigmoid(Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, input):\n        self.output = sigmoid(input)\n        return self.output\n\n    def backward(self, input, grad_output):\n        grad_input = sigmoid(input) * (1 - sigmoid(input)) * grad_output\n        return grad_input","a99301d7":"class SoftMax(Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, input):\n        self.output = np.subtract(input, input.max(axis=1, keepdims=True))\n        self.output = np.exp(self.output) \/ np.sum(np.exp(self.output), axis=1, keepdims=True)\n        return self.output\n\n    def backward(self, input, grad_output):\n        return grad_output","4f1b1bee":"def DataLoader(X, Y, batch_size=32):\n    n = X.shape[0]\n    indices = np.arange(n)\n    np.random.shuffle(indices)\n    for start in range(0, n, batch_size):\n        end = min(start + batch_size, n)\n        batch_idx = indices[start:end]\n        yield X[batch_idx], Y[batch_idx]","f3d0772e":"def accuracy_score(y_true, y_pred):\n    a = np.argmax(y_true, axis=1)\n    b = np.argmax(y_pred, axis=1)\n    return np.count_nonzero(a == b) \/ y_true.shape[0]","76a7f1cc":"class Adam:\n    def __init__(self, model):\n        self.prev_m = None\n        self.prev_v = None\n        self.model = model\n        self.t = 1\n\n    def step(self, lr, beta1, beta2):\n        prev_m_tmp = []\n        prev_v_tmp = []\n        eps = 1e-7\n\n        for i, (weights, gradient) in enumerate(zip(self.model.parameters(), self.model.grad_parameters())):\n            if self.prev_m and self.prev_v:\n                m = beta1 * self.prev_m[i] + (1 - beta1) * gradient\n                v = beta2 * self.prev_v[i] + (1 - beta2) * gradient ** 2\n                m_hat = m \/ (1 - beta1 ** self.t)\n                v_hat = v \/ (1 - beta2 ** self.t)\n            else:\n                m = beta1 * 0 + (1 - beta1) * gradient\n                v = beta2 * 0 + (1 - beta2) * gradient ** 2\n                m_hat = m \/ (1 - beta1 ** self.t)\n                v_hat = v \/ (1 - beta2 ** self.t)\n\n            weights -= lr * m_hat \/ (np.sqrt(v_hat) + eps)\n\n            prev_m_tmp.append(m)\n            prev_v_tmp.append(v)\n\n        self.prev_m = prev_m_tmp\n        self.prev_v = prev_v_tmp\n\n        self.t += 1","d00c2e58":"df = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\n\ndf","5fde6e87":"from sklearn.model_selection import train_test_split\n\nX, y = df.iloc[:, 1:].to_numpy(), pd.get_dummies(df.iloc[:, 0]).to_numpy()\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)","30b6087a":"model = Sequential(\n    Linear(784, 512),\n    Sigmoid(),\n\n    Linear(512, 256),\n    Sigmoid(),\n\n    Linear(256, 128),\n    Sigmoid(),\n\n    Linear(128, 64),\n    Sigmoid(),\n\n    Linear(64, 10),\n\n    SoftMax(),\n)\n\n\nepochs = 35\neval_every = 1\nbatch_size = 1024\ncriterion = CrossEntropy()\noptimizer = Adam(model)\n\nfor epoch in range(epochs):\n    for x, y in DataLoader(X_train, y_train, batch_size=batch_size):\n        model.train()\n\n        y_pred = model.forward(x)\n        grad = criterion.backward(y_pred, y)\n        model.backward(x, grad)\n\n        optimizer.step(lr=0.003, beta1=0.9, beta2=0.999)\n\n    if (epoch + 1) % eval_every == 0:\n        model.eval()\n        y_train_pred = model.forward(X_train)\n        y_test_pred = model.forward(X_test)\n        loss_train = criterion.forward(y_train_pred, y_train)\n        loss_test = criterion.forward(y_test_pred, y_test)\n        print(f'Epoch: {epoch + 1}\/{epochs}')\n        print(f'Train Loss: {loss_train} Train Accuracy: {accuracy_score(y_train, y_train_pred)}')\n        print(f'Test Loss: {loss_test} Test Accuracy: {accuracy_score(y_test, y_test_pred)} \\n')","46af868b":"df_test = pd.read_csv('..\/input\/digit-recognizer\/test.csv')\n\noutput = model.forward(df_test)\n\noutput = np.argmax(output, axis=1)\n\nImageId = df_test.index + 1\n\nsubmission = pd.DataFrame({'ImageId': ImageId, 'Label': output})\n\nsubmission.to_csv('submission.csv', index=False)","6e5db200":"$$\\large\n\\newcommand{\\pd}[2]{\\frac{\\partial #1}{\\partial #2}}\n  \\pd{L}{w _ {1,1}}\n  = \\pd{L}{Y}\\cdot\\pd{Y}{w _ {1,1}}\n  = \\begin{pmatrix}\n         \\pd{L}{y _ {1,1}} & \\pd{L}{y _ {1,2}} & \\pd{L}{y _ {1,3}}\n    \\\\ \\pd{L}{y _ {2,1}} & \\pd{L}{y _ {2,2}} & \\pd{L}{y _ {2,3}}\n  \\end{pmatrix}\n  \\cdot\n  \\begin{pmatrix}\n    x _ {1,1} & 0 & 0 \\\\ x _ {2,1} & 0 & 0\n  \\end{pmatrix}\n  = \\pd{L}{y _ {1,1}}x _ {1,1} + \\pd{L}{y _ {2,1}}x _ {2,1}\n$$","d528df44":"Combining Equations $2$ and $3$ now gives:","a48cc48c":"$$\\Large\nx_1w_1 + x_2w_2 + x_3w_3 + b = a_1\n$$","f2bbedfb":"We can combine these six results into a single expression for  $\\large \\frac{\\partial L}{\\partial W}$:","4a115331":"In the above equation $L$ and $x_{1, 1}$ are scalars so $\\large \\frac{\\partial L}{\\partial x_{1, 1}}$ is also a scalar. If we view $Y$ not as a matrix but as a collection of intermediate scalar variables, then we can use the chain rule to write $\\large \\frac{\\partial L}{\\partial x_{1, 1}}$ solely in terms of scalar derivatives.\n\n\nTo avoid working with sums, it is convenient to collect all terms $\\large \\frac{\\partial L}{\\partial y_{i, j}}$ into a single matrix $\\large \\frac{\\partial L}{\\partial Y}$. Here $L$ is a scalar and $Y$ is a matrix, so $\\large \\frac{\\partial L}{\\partial Y}$ has the same shape as $Y (N \\times M)$, where each element of $\\large \\frac{\\partial L}{\\partial Y}$ gives the derivative of $L$ with respect to one element of $Y$. We similarly collect all terms $\\large \\frac{\\partial y_{i, j}}{\\partial x_{1, 1}}$ into a single matrix $\\large \\frac{\\partial Y}{\\partial x_{1, 1}}$. Since $Y$ is a matrix and $x_{1, 1}$ is a scalar, $\\large \\frac{\\partial Y}{\\partial x_{1, 1}}$ is a matrix with the same shape as $Y (N \\times M)$.\n\nSince $\\large \\frac{\\partial L}{\\partial x_{1, 1}}$ is a scalar , we know that the product of $\\large \\frac{\\partial L}{\\partial Y}$ and $\\large \\frac{\\partial Y}{\\partial x_{1, 1}}$ must be a scalar; by inspecting the expression using only scalar derivatives, it is clear that in this context the product of $\\large \\frac{\\partial L}{\\partial Y}$ and $\\large \\frac{\\partial Y}{\\partial x_{1, 1}}$ must be a dot product.\n\nIn the backward pass we are already given $\\large \\frac{\\partial L}{\\partial Y}$, so we only need to compute $\\large \\frac{\\partial L}{\\partial x_{1,1}}$.\n\nRecall that $Y$ is defined as:","1aa60f14":"$$\\large\n  Y = XW = \\begin{pmatrix}\n         x_{1,1}w_{1,1} + x_{1,2}w_{2,1} & \n         x_{1,1}w_{1,2} + x_{1,2}w_{2,2} &\n         x_{1,1}w_{1,3} + x_{1,2}w_{2,3}\n    \\\\ x_{2,1}w_{1,1} + x_{2,2}w_{2,1} & \n         x_{2,1}w_{1,2} + x_{2,2}w_{2,2} &\n         x_{2,1}w_{1,3} + x_{2,2}w_{2,3}\n  \\end{pmatrix}\n$$","57e7ccb6":"We can then compute the required partial derivatives:","83412625":"Think of it is like we are doing 2 linear regressions here, for $a_1$ and for $a_2$, and because of this we have 2 biases. Then as before we plug output from each linear regression to non linear function, and since we have 2 linear regressions, we will have 2 functions after we apply non linear functions to each of them. \n\nDo you see what we have just got? I'm speaking about these $s_1$ and $s_2$, you would not believe me, but we can interpret them as out new features! Think of $s_1$ and $s_2$ as inputs, and what are we doing with features? Right, we feeding this to neural net! So let's do this!","eb2056fd":"And again we get our output! You can actually do this as many times as you with, and this is called layers.","6a32d177":"$$\\Large\n\\sigma(x) = \\frac{1}{1 + e^{-(x_1w_1 + x_2w_2 + x_3w_3 + b)}}\n$$","42e35a2b":"$$\\large\n\\newcommand{\\pd}[2]{\\frac{\\partial #1}{\\partial #2}}\n  \\pd{L}{w _ {2,2}}\n  = \\pd{L}{Y}\\cdot\\pd{Y}{w _ {2,2}}\n  = \\begin{pmatrix}\n         \\pd{L}{y _ {1,1}} & \\pd{L}{y _ {1,2}} & \\pd{L}{y _ {1,3}}\n    \\\\ \\pd{L}{y _ {2,1}} & \\pd{L}{y _ {2,2}} & \\pd{L}{y _ {2,3}}\n  \\end{pmatrix}\n  \\cdot\n  \\begin{pmatrix}\n    0 & x _ {1,2} & 0 \\\\ 0 & x _ {2,2} & 0\n  \\end{pmatrix}\n  = \\pd{L}{y _ {1,2}}x _ {1,2} + \\pd{L}{y _ {2,2}}x _ {2,2}\n$$","8d644eba":"This allows us to compute the downstream gradient $\\large \\frac{\\partial L}{\\partial W}$ as the matrix-matrix product of $X^T$ and the upstream gradient $\\large \\frac{\\partial L}{\\partial Y}$. We never need to explicitly form the Jacobian $\\large \\frac{\\partial Y}{\\partial W}$!","317971ec":"Our derivation covers the specific case where $X$ has shape $2 \\times 2$ and $W$ has shape $2 \\times 3$, but in fact the expressions in Equation $26$ hold in general for matrices of any shape.","a017cf50":"$$\\large\n\\begin{align}\n  \\pd{L}{X} &= \\begin{pmatrix}\n        \\pd{L}{x_{1,1}} & \\pd{L}{x_{1,2}}\n    \\\\\\pd{L}{x_{2,1}} & \\pd{L}{x_{2,2}}\n  \\end{pmatrix}\n  \\\\[2ex] &= \\begin{pmatrix}\n        \\pd{L}{y_{1,1}} w_{1,1}+ \\pd{L}{y_{1,2}} w_{1,2}+ \\pd{L}{y_{1,3}} w_{1,3}&\n        \\pd{L}{y_{1,1}} w_{2,1}+ \\pd{L}{y_{1,2}} w_{2,2}+ \\pd{L}{y_{1,3}} w_{2,3}\n    \\\\\\pd{L}{y_{2,1}} w_{1,1}+ \\pd{L}{y_{2,2}} w_{1,2}+ \\pd{L}{y_{2,3}} w_{1,3}&\n        \\pd{L}{y_{2,1}} w_{2,1}+ \\pd{L}{y_{2,2}} w_{2,2}+ \\pd{L}{y_{2,3}} w_{2,3}\n  \\end{pmatrix}\n  \\\\[2ex] &= \\begin{pmatrix}\n        \\pd{L}{y_{1,1}} & \\pd{L}{y_{1,2}} & \\pd{L}{y_{1,3}}\n    \\\\\\pd{L}{y_{2,1}} & \\pd{L}{y_{2,2}} & \\pd{L}{y_{2,3}}\n  \\end{pmatrix}\n  \\begin{pmatrix}\n        w_{1,1} & w_{2,1}\n    \\\\w_{1,2} & w_{2,2}\n    \\\\w_{1,3} & w_{2,3}\n  \\end{pmatrix}\n  \\\\[2ex] &= \\boxed{\\pd{L}{Y}W^T} \\label{eq:dLdX-backprop}\n\\end{align}\n$$","913a723d":"$$\\large\n  Y = XW = \\begin{pmatrix}\n         x_{1,1}w_{1,1} + x_{1,2}w_{2,1} & \n         x_{1,1}w_{1,2} + x_{1,2}w_{2,2} &\n         x_{1,1}w_{1,3} + x_{1,2}w_{2,3}\n    \\\\ x_{2,1}w_{1,1} + x_{2,2}w_{2,1} & \n         x_{2,1}w_{1,2} + x_{2,2}w_{2,2} &\n         x_{2,1}w_{1,3} + x_{2,2}w_{2,3}\n  \\end{pmatrix}\n$$","582ef66c":"We can now combine Equations $6, 9, 13,$ and $16$ to give a single expression for $\\large \\frac{\\partial L}{\\partial X}$ in terms of $W$ and $\\large \\frac{\\partial L}{\\partial Y}$:","2c9a12bb":"# Backward Pass\n\nDuring the backward pass through the linear layer, we assume that the upstream gradient $\\large \\frac{\\partial L}{\\partial Y}$ has already been computed. For example if the linear layer is part of a linear classifier, then the matrix $Y$ gives class scores, these scores are fed to a loss function (such as the softmax or multiclass SVM loss) which computes the scalar loss $L$ and derivative $\\large \\frac{\\partial L}{\\partial Y}$ of the loss with respect to the scores.\n\nSince $L$ is a scalar and $Y$ is a matrix of shape $N \\times M$, the gradient $\\large \\frac{\\partial L}{\\partial Y}$ will be a matrix the same shape as $Y$, where each element of $\\large \\frac{\\partial L}{\\partial Y}$ gives the derivative of the loss $L$ with respect to one element of $Y$","078ecb24":"$$\\large\n\\begin{equation}\n  \\boxed{\\pd{L}{X} = \\pd{L}{Y}W^T} \\hspace{6pc} \\boxed{\\pd{L}{W} = X^T\\pd{L}{Y}}\n  \\label{eq:final}\n\\end{equation}\n$$","3f05aee7":"$$\\large\n\\begin{align}\n  \\pd{L}{x_{1,2}} &= \\pd{L}{Y}\\cdot\\pd{Y}{x_{1,2}}\n  \\\\[2ex] &= \\begin{pmatrix}\n    \\pd{L}{y_{1,1}} & \\pd{L}{y_{1,2}} & \\pd{L}{y_{1,3}}\n    \\\\ \\pd{L}{y_{2,1}} & \\pd{L}{y_{2,2}} & \\pd{L}{y_{2,3}}\n  \\end{pmatrix}\\cdot\n  \\begin{pmatrix}\n    w_{2,1} & w_{2,2} & w_{2,3}\n    \\\\ 0 & 0 & 0 \n  \\end{pmatrix}\n  \\\\[2ex] &= \\pd{L}{y_{1,1}}w_{2,1} + \\pd{L}{y_{1,2}}w_{2,2} + \\pd{L}{y_{1,3}}w_{2,3}\n    \\label{eq:dLdX12}\n\\end{align}\n$$","2d29482d":"The terms $\\large \\frac{\\partial Y}{\\partial X}$ and $\\large \\frac{\\partial Y}{\\partial W}$ in this equation are Jacobians containing the partial derivative of each element of $Y$ with respect to each element of the inputs $X$ and $W$. These equations thus tell us that the downstream gradients can be computed using a matrix-vector product between the Jacobians and the upstream gradients.\n\nHowever we do not want to form the Jacobian matrices $\\large \\frac{\\partial Y}{\\partial X}$ and $\\large \\frac{\\partial Y}{\\partial W}$ explicitly, because they will be very large. In a typical neural network we might have $N = 64$ and $M = D = 4096$. Then $\\large \\frac{\\partial Y}{\\partial X}$ consists of $64 \\cdot 4096 \\cdot 64 \\cdot 4096$ scalar values. This is more than 68 billion numbers: using 32-bit floating point numbers, this Jacobian matrix will take 256 GB of memory to store. Therefore it is completely hopeless to try and explicitly store and manipulate the Jacobian matrix.\n\nHowever it turns out that for most common neural network layers, we can derive expressions that compute the product $\\large \\frac{\\partial Y}{\\partial X} \\frac{\\partial L}{\\partial Y}$ without explicitly forming the Jacobian $\\large \\frac{\\partial Y}{\\partial X}$. Even better, we can typically derive this expression without even computing an explicit expression for the Jacobian $\\large \\frac{\\partial Y}{\\partial X}$. In many cases we can work out a small case on paper and then infer the general formula.\n\nLet\u2019s see how this works out for our specific case of $N = 2, D = 2, M = 3$. We first tackle $\\large \\frac{\\partial L}{\\partial X}$ . Again, we know that $\\large \\frac{\\partial L}{\\partial X}$ must have the same shape as $X$:","b1ffd0f3":"# Computing $\\frac{\\partial L}{\\partial x_{1, 1}}$\n\nWe can proceed one element of a time. First we will compute $\\large \\frac{\\partial L}{\\partial x_{1, 1}}$ . By the chain rule, we know that","215dee14":"$$\\large\n\\newcommand{\\pd}[2]{\\frac{\\partial #1}{\\partial #2}}\n\\pd{Y}{w _ {2,1}} = \\begin{pmatrix}\n         x _ {1,2} & 0 & 0\n    \\\\ x _ {2,2} & 0 & 0\n  \\end{pmatrix}\n  \\hspace{4pc}\n  \\pd{Y}{w _ {2,2}} = \\begin{pmatrix}\n         0 & x _ {1,2} & 0\n    \\\\ 0 & x _ {2,2} & 0\n  \\end{pmatrix}\n  \\hspace{4pc}\n  \\pd{Y}{w _ {2,3}} = \\begin{pmatrix}\n         0 & 0 & x _ {1,2}\n    \\\\ 0 & 0 & x _ {2,2}\n  \\end{pmatrix}\n  $$","33a899a3":"for $\\large \\frac{\\partial L}{\\partial x_{2, 1}}$ we compute:","e054d0cf":"Therefore we can easily compute that $\\large \\frac{\\partial L}{\\partial x_{1,1}}$ is:","abd00e59":"# Forward Pass\n\nDuring the forward pass, the linear layer takes an input $X$ of shape $N \\times D$ and a weight matrix $W$ of shape $D \\times M$, and computes an output $Y = XW$ of shape $N \\times M$ by computing the matrix product of the two inputs. To make things even more concrete, we will consider the case $N = 2, D = 2, M = 3$\n\nWe can then write out the forward pass in terms of the elements of the inputs:\n","3e9c0ce5":"$$\\large\n\\newcommand{\\pd}[2]{\\frac{\\partial #1}{\\partial #2}}\n  \\pd{L}{w _ {2,1}}\n  = \\pd{L}{Y}\\cdot\\pd{Y}{w _ {2,1}}\n  = \\begin{pmatrix}\n         \\pd{L}{y _ {1,1}} & \\pd{L}{y _ {1,2}} & \\pd{L}{y _ {1,3}}\n    \\\\ \\pd{L}{y _ {2,1}} & \\pd{L}{y _ {2,2}} & \\pd{L}{y _ {2,3}}\n  \\end{pmatrix}\n  \\cdot\n  \\begin{pmatrix}\n    x _ {1,2} & 0 & 0 \\\\ x _ {2,2} & 0 & 0\n  \\end{pmatrix}\n  = \\pd{L}{y _ {1,1}}x _ {1,2} + \\pd{L}{y _ {2,1}}x _ {2,2}\n$$","09855a80":"$$\\large\n\\begin{equation}\n  \\pd{L}{w _ {i,j}}\n  = \\sum _ {i\u2019=1}^N\\sum _ {j\u2019=1}^M\\pd{L}{y _ {i\u2019,j\u2019}}\\pd{y _ {i\u2019,j\u2019}}{w _ {i,j}}\n  = \\pd{L}{Y} \\cdot \\pd{Y}{w _ {i,j}}\n  \\label{eq:dLdW-chain}\n\\end{equation}\n$$","bb64caa1":"# Submit to competition","1b8af1c0":"# Solve MNIST \ud83d\udd22\n\nLet's test our model in MNIST dataset!","49d913b1":"Let's plug output from linear regression","53ab0236":"![Linear%20Regression%20Diagram.drawio.svg](attachment:Linear%20Regression%20Diagram.drawio.svg)","8e3b2e90":"![Linear%20Regression%20with%20sigmoid.drawio.svg](attachment:Linear%20Regression%20with%20sigmoid.drawio.svg)","6c50b50d":"$$\\large\n\\begin{align}\n  \\pd{L}{W} &= \\begin{pmatrix}\n         \\pd{L}{w _ {1,1}} & \\pd{L}{w _ {1,2}} & \\pd{L}{w _ {1,3}}\n    \\\\ \\pd{L}{w _ {2,1}} & \\pd{L}{w _ {2,2}} & \\pd{L}{w _ {2,3}}\n  \\end{pmatrix}\n  \\\\[2ex] &= \\begin{pmatrix}\n         \\pd{L}{y _ {1,1}}x_ {1,1} + \\pd{L}{y _ {2,1}}x _ {2,1} &\n         \\pd{L}{y _ {1,2}}x_ {1,1} + \\pd{L}{y _ {2,2}}x _ {2,1} &\n         \\pd{L}{y _ {1,3}}x_ {1,1} + \\pd{L}{y _ {2,3}}x _ {2,1} &\n    \\\\ \\pd{L}{y _ {1,1}}x_ {1,2} + \\pd{L}{y _ {2,1}}x _ {2,2} &\n         \\pd{L}{y _ {1,2}}x_ {1,2} + \\pd{L}{y _ {2,2}}x _ {2,2} &\n         \\pd{L}{y _ {1,3}}x_ {1,2} + \\pd{L}{y _ {2,3}}x _ {2,2} &\n  \\end{pmatrix}\n  \\\\[2ex] &= \\begin{pmatrix}\n    x _ {1,1} & x _ {2,1} \\\\ x _ {1,2} & x _ {2,2}\n  \\end{pmatrix}\n  \\begin{pmatrix}\n         \\pd{L}{y _ {1,1}} & \\pd{L}{y _ {1,2}} & \\pd{L}{y _ {1,3}}\n    \\\\ \\pd{L}{y _ {2,1}} & \\pd{L}{y _ {2,2}} & \\pd{L}{y _ {2,3}}\n  \\end{pmatrix}\n  \\\\[2ex] &= \\boxed{X^T\\pd{L}{Y}}\n\\end{align}\n$$","3b1e3318":"And and equation for the linear regression looks like","ea612069":"$$\\large\nY = XW = \\begin{pmatrix}\n         x_{1,1}w_{1,1} + x_{1,2}w_{2,1} & \n         x_{1,1}w_{1,2} + x_{1,2}w_{2,2} &\n         x_{1,1}w_{1,3} + x_{1,2}w_{2,3}\n    \\\\ x_{2,1}w_{1,1} + x_{2,2}w_{2,1} & \n         x_{2,1}w_{1,2} + x_{2,2}w_{2,2} &\n         x_{2,1}w_{1,3} + x_{2,2}w_{2,3}\n  \\end{pmatrix}\n $$","f743436f":"As before, the term $\\large \\frac{\\partial L}{\\partial Y}$ are the upstream gradients of the loss with respect to the layer outputs. We assume these have already been computed. To compute the $\\large \\frac{\\partial Y}{\\partial w_{i, j}}$ terms we recall that","05b8361e":"$$\\large\n\\newcommand{\\pd}[2]{\\frac{\\partial #1}{\\partial #2}}\n  \\pd{L}{w _ {1,3}}\n  = \\pd{L}{Y}\\cdot\\pd{Y}{w _ {1,3}}\n  = \\begin{pmatrix}\n         \\pd{L}{y _ {1,1}} & \\pd{L}{y _ {1,2}} & \\pd{L}{y _ {1,3}}\n    \\\\ \\pd{L}{y _ {2,1}} & \\pd{L}{y _ {2,2}} & \\pd{L}{y _ {2,3}}\n  \\end{pmatrix}\n  \\cdot\n  \\begin{pmatrix}\n    0 & 0 & x _ {1,1} \\\\ 0 & 0 & x _ {2,1}\n  \\end{pmatrix}\n  = \\pd{L}{y _ {1,3}}x _ {1,1} + \\pd{L}{y _ {2,3}}x _ {2,1}\n$$","2001ea02":"$$\\large\n\\newcommand{\\pd}[2]{\\frac{\\partial #1}{\\partial #2}}\n  \\pd{L}{w _ {1,2}}\n  = \\pd{L}{Y}\\cdot\\pd{Y}{w _ {1,2}}\n  = \\begin{pmatrix}\n         \\pd{L}{y _ {1,1}} & \\pd{L}{y _ {1,2}} & \\pd{L}{y _ {1,3}}\n    \\\\ \\pd{L}{y _ {2,1}} & \\pd{L}{y _ {2,2}} & \\pd{L}{y _ {2,3}}\n  \\end{pmatrix}\n  \\cdot\n  \\begin{pmatrix}\n    0 & x _ {1,1}  & 0 \\\\ 0 & x _ {2,1} & 0\n  \\end{pmatrix}\n  = \\pd{L}{y _ {1,2}}x _ {1,1} + \\pd{L}{y _ {2,2}}x _ {2,1}\n$$","dc5e0e9f":"$$\\large\nX = \\begin{pmatrix}\n         x_{1,1} & x_{1,2}\n    \\\\ x_{2,1} & x_{2,2}\n\\end{pmatrix}\n\\hspace{2pc}\nW = \\begin{pmatrix}\n         w_{1,1} & w_{1,2} & w_{1,3}\n    \\\\ w_{2,1} & w_{2,2} & w_{2,3}\n\\end{pmatrix}\n$$","20620e21":"$$\\large\n\\newcommand{\\pd}[2]{\\frac{\\partial #1}{\\partial #2}}\n\\pd{L}{X} = \\pd{Y}{X}\\pd{L}{Y} \\hspace{4pc} \\pd{L}{W} = \\pd{Y}{W}\\pd{L}{Y}\n$$","fa9c9684":"$$\\large\n\\begin{equation}\n  \\label{eq:dYdX11}\n  \\pd{Y}{x _ {1,1}} =\n  \\begin{pmatrix}\n    w_{1,1} & w_{1,2} & w_{1,3}\n    \\\\ 0 & 0 & 0\n  \\end{pmatrix}\n\\end{equation}\n$$","fb22f839":"# Hidden layers\n\nYou have seen that simple neural network looks like linear regression with some nonlinear function, but you probably have seen some deep and huge neural networks. The key point is that it's all the same, it does not matter how deep your neural net, it uses the same components, we will talk about it now.\n\nImage such neural net with one hidden layer. The $w_{ij}$ means that weight goes from input $i$ to neuron $j$. For example $w_{21}$ will connect $x_2$ and $a_1$","2ea56f27":"$$\\large\n\\begin{align}\n  \\pd{L}{x_{2,2}} &= \\pd{L}{Y}\\cdot\\pd{Y}{x_{2,2}}\n  \\\\[2ex] &= \\begin{pmatrix}\n    \\pd{L}{y_{1,1}} & \\pd{L}{y_{1,2}} & \\pd{L}{y_{1,3}}\n    \\\\ \\pd{L}{y_{2,1}} & \\pd{L}{y_{2,2}} & \\pd{L}{y_{2,3}}\n  \\end{pmatrix}\\cdot\n  \\begin{pmatrix}\n    0 & 0 & 0\n    \\\\ w_{2,1} & w_{2,2} & w_{2,3}\n  \\end{pmatrix}\n  \\\\[2ex] &= \\pd{L}{y_{2,1}}w_{2,1} + \\pd{L}{y_{2,2}}w_{2,2} + \\pd{L}{y_{2,3}}w_{2,3}\n    \\label{eq:dLdX22}\n\\end{align}\n$$","6260a5d7":"After the forward pass, we assume that the output will be used in other parts of the network, and will eventually be used to compute a scalar loss $L$","6cd0e92e":"In Equation $20$, Recall that $\\large \\frac{\\partial L}{\\partial Y}$ is a matrix of shape $N \\times M$ and $W$ is a matrix of shape $D \\times M$, thus $\\large \\frac{\\partial L}{\\partial X} = \\frac{\\partial L}{\\partial Y} W^T$ has shape $N \\times D$ which is the same shape as $X$.\n\nThe expression $\\large \\frac{\\partial L}{\\partial X} = \\frac{\\partial L}{\\partial Y} W^T$ allows us to backpropagate without explicitly forming the Jacobian $\\large \\frac{\\partial Y}{\\partial X}$!","04218cf4":"# Summary\n\nIn summary, we have derived the backpropagation expressions for the matrix-matrix product $Y = XW$:","7207f5bd":"$$\\large\n\\newcommand{\\pd}[2]{\\frac{\\partial #1}{\\partial #2}}\n  \\pd{L}{w _ {2,3}}\n  = \\pd{L}{Y}\\cdot\\pd{Y}{w _ {2,3}}\n  = \\begin{pmatrix}\n         \\pd{L}{y _ {1,1}} & \\pd{L}{y _ {1,2}} & \\pd{L}{y _ {1,3}}\n    \\\\ \\pd{L}{y _ {2,1}} & \\pd{L}{y _ {2,2}} & \\pd{L}{y _ {2,3}}\n  \\end{pmatrix}\n  \\cdot\n  \\begin{pmatrix}\n    0 & 0 & x _ {1,2} \\\\ 0 & 0 & x _ {2,2}\n  \\end{pmatrix}\n  = \\pd{L}{y _ {1,3}}x _ {1,2} + \\pd{L}{y _ {2,3}}x _ {2,2}\n$$","b6d4843b":"During the backward pass our goal is to use $\\large \\frac{\\partial L}{\\partial Y}$ in order to compute the downstream gradients $\\large \\frac{\\partial L}{\\partial X}$ and $\\large \\frac{\\partial L}{\\partial W}$. Again, since $L$ is a scalar we know that $\\large \\frac{\\partial L}{\\partial X}$ must have the same shape as $X (N \\times D)$ and $\\large \\frac{\\partial L}{\\partial W}$ must have the same shape as $W (D \\times M)$.\n\nBy the chain rule, we know that:","2da3aa62":"## Computing $\\frac{\\partial L}{\\partial W}$\n\nWe can follow a similar strategy to derive an expression for $\\large \\frac{\\partial L}{\\partial W}$ without forming the Jacobian $\\large \\frac{\\partial Y}{\\partial W}$.\n\nRecall that $X$ has a shape $N \\times D$, $W$ has shape $N \\times M$, and $Y = XW$ has shape $N \\times M$.\n\nThe gradient $\\large \\frac{\\partial L}{\\partial W}$ gives the effect on each element of $W$ om the scalar loss $L$, so it should be a matrix of partial derivatives of the same shape as $W$:","babec82f":"$$\\large\n\\begin{equation}\n  \\label{eq:dLdX}\n  X = \\begin{pmatrix} \n         x_{1,1} & x_{1,2}\n    \\\\ x_{2,1} & x_{2,2}\n  \\end{pmatrix}\n  \\implies\n  \\pd{L}{X} = \\begin{pmatrix}\n         \\pd{L}{x_{1,1}} & \\pd{L}{x_{1,2}}\n    \\\\ \\pd{L}{x_{2,1}} & \\pd{L}{x_{2,2}}\n  \\end{pmatrix}\n\\end{equation}\n$$","f4ce40b4":"$$\\large\n\\begin{align}\n  \\pd{L}{x_{1,1}} &= \\pd{L}{Y}\\cdot\\pd{Y}{x_{1,1}}\n  \\\\[2ex] &= \\begin{pmatrix}\n         \\pd{L}{y_{1,1}} & \\pd{L}{y_{1,2}} & \\pd{L}{y_{1,3}}\n    \\\\ \\pd{L}{y_{2,1}} & \\pd{L}{y_{2,2}} & \\pd{L}{y_{2,3}}\n  \\end{pmatrix}\\cdot\n  \\begin{pmatrix}\n    w_{1,1} & w_{1,2} & w_{1,3}\n    \\\\ 0 & 0 & 0 \n  \\end{pmatrix}\n  \\\\[2ex] &= \\pd{L}{y_{1,1}}w_{1,1} + \\pd{L}{y_{1,2}}w_{1,2} + \\pd{L}{y_{1,3}}w_{1,3}\n  \\label{eq:dLdX11}\n\\end{align}\n$$","7d835076":"# Linear Layer\n\nIn these notes we will explicitly derive the equations to use when backpropagating through a linear layer, using minibatches. Following a similar thought process can help you backpropagate through other types of computations involving matrices and tensors.","e21d1bd4":"# Coding the Fully Connected Layer\nWe can now write some python code to bring this math to life!","f334f6da":"![Neural%20network%20with%20hidden%20layer%20%5B2%5D.drawio.svg](attachment:Neural%20network%20with%20hidden%20layer%20%5B2%5D.drawio.svg)","7fb2c114":"# Good Old Linear Regression - Perceptron\n\nRemember this one? If not I highly reccomend you to brush up on this! If you are ready, we are good to go!\n\nThe ordinary linear regression looks like this, you have features, weights and bias, and then you want multiply all this stuff, also don't forget to add bias and get the result. If we would like to draw linear regression, it would look just like this.","3949c5f3":"<div style='text-align: center'>\n    <img src='https:\/\/www.pngall.com\/wp-content\/uploads\/3\/Portal-PNG.png' width='500' \/>\n<\/div>","0353cd94":"Similar to $\\large \\frac{\\partial L}{\\partial X}$, we can proceed one element at a time. We need to compute $\\large \\frac{\\partial L}{\\partial w_{i, j}}$. We can use the chain rule to derive an expression similar to Equation $2$:","6bd73eb9":"for $\\large \\frac{\\partial L}{\\partial x_{2, 2}}$ we compute:","f2430209":"We can now apply Equation $21$ to each of these six terms to derive expressions for each element of $\\large \\frac{\\partial L}{\\partial W}$:","a998b65a":"$$\\large\n\\newcommand{\\pd}[2]{\\frac{\\partial #1}{\\partial #2}}\n  \\pd{Y}{w _ {1,1}} = \\begin{pmatrix}\n         x _ {1,1} & 0 & 0\n    \\\\ x _ {2, 1} & 0 & 0\n  \\end{pmatrix}\n  \\hspace{4pc}\n  \\pd{Y}{w _ {1,2}} = \\begin{pmatrix}\n         0 & x _ {1,1} & 0\n    \\\\ 0 & x _ {2,1} & 0\n  \\end{pmatrix}\n  \\hspace{4pc}\n  \\pd{Y}{w _ {1,3}} = \\begin{pmatrix}\n         0 & 0 & x _ {1,1}\n    \\\\ 0 & 0 & x _ {2,1}\n  \\end{pmatrix}\n$$","ae6978bb":"Now our linear regression with sigmoid looks like this","5b2509b0":"$$\\large\n\\begin{equation}\n  \\label{eq:dLdX11-chain}\n  \\pd{L}{x _ {1,1}} \n  = \\sum _ {i=1}^N\\sum _ {j=1}^M \\pd{L}{y _ {i,j}}\\pd{y _ {i,j}}{x _ {1,1}}\n  = \\pd{L}{Y} \\cdot \\pd{Y}{x _ {1,1}}\n\\end{equation}\n$$","3b0065b5":"$$\\Large\n\\sigma(x) = \\frac{1}{1 + e^{-x}}\n$$","fba04695":"$$\\large\n\\newcommand{\\pd}[2]{\\frac{\\partial #1}{\\partial #2}}\n  \\pd{L}{Y} = \\begin{pmatrix}\n         \\pd{L}{y_{1,1}} & \\pd{L}{y_{1,2}} & \\pd{L}{y_{1,3}}\n    \\\\ \\pd{L}{y_{2,1}} & \\pd{L}{y_{2,2}} & \\pd{L}{y_{2,3}}\n  \\end{pmatrix}\n$$","8e91a34f":"There is one problem with linear regression, it's linear. We will never be able to learn nonlinear dependencies. Now, stop and try to think how we could over come such problem.\nThe answer is we need to add some nonlinearity, let's plug output from linear function to some nonlinear function, so that now we can create nonlinear function.\nOne of the most famous ninlinear function in deep learning is sigmoid, that looks like this","bc6ba6c6":"## Computing $\\frac{\\partial L}{\\partial X}$\n\nWe can now repeat the same computation to derive expressions for the other entries of $\\large \\frac{\\partial L}{\\partial X}$.\n \nfor $\\large \\frac{\\partial L}{\\partial x_{1, 2}}$ we compute:","3558b6ec":"$$\\large\n\\begin{align}\n  \\\\ \\pd{L}{x_{2,1}} &= \\pd{L}{Y}\\cdot\\pd{Y}{x_{2,1}}\n  \\\\[2ex] &= \\begin{pmatrix}\n    \\pd{L}{y_{1,1}} & \\pd{L}{y_{1,2}} & \\pd{L}{y_{1,3}}\n    \\\\ \\pd{L}{y_{2,1}} & \\pd{L}{y_{2,2}} & \\pd{L}{y_{2,3}}\n  \\end{pmatrix}\\cdot\n  \\begin{pmatrix}\n    0 & 0 & 0\n    \\\\ w_{1,1} & w_{1,2} & w_{1,3}\n  \\end{pmatrix}\n  \\\\[2ex] &= \\pd{L}{y_{2,1}}w_{1,1} + \\pd{L}{y_{2,2}}w_{1,2} + \\pd{L}{y_{2,3}}w_{1,3}\n    \\label{eq:dLdX21}\n\\end{align}\n$$","cdaddb83":"$$\\large\nW = \\begin{pmatrix}\n       w _ {1,1} & w _ {1,2} & w _ {1,3} \n  \\\\ w _ {2,1} & w _ {2,2} & w _ {2,3} \n\\end{pmatrix}\n\\implies\n\\pd{L}{W} = \\begin{pmatrix}\n      \\pd{L}{w _ {1,1}} & \\pd{L}{w _ {1,2}} & \\pd{L}{w _ {1, 3}}\n  \\\\\\pd{L}{w _ {1,1}} & \\pd{L}{w _ {1,2}} & \\pd{L}{w _ {1, 3}}\n\\end{pmatrix}\n$$","e50c709c":"![Neural%20Network%20with%201%20hidden%20layer.drawio.svg](attachment:Neural%20Network%20with%201%20hidden%20layer.drawio.svg)","508ff800":"<h1 style='text-align: center'>Neural network from scratch \ud83d\udd2e<\/h1>\n\n<p  style='text-align: center'>\nThis notebook is in <span style='color: green; font-weight: 700'>Active<\/span> state of development! Check out this notebook to see some updates as I update new stuff as oftern as I learn it!\n<a style='font-weight:700' href='https:\/\/github.com\/LilDataScientist\/Neural-Network'> Code on GitHub! <\/a><\/p>"}}