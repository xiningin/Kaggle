{"cell_type":{"c7909414":"code","17f41755":"code","396b7d4b":"code","644be9ad":"code","99ab79f3":"code","28d67c21":"code","bb9761e2":"code","39e96d4d":"code","42d22442":"code","df47b8bb":"code","d12e8511":"code","f97c4142":"code","81c13c03":"code","9198b938":"code","3a632a06":"code","e5bafdcd":"code","0499fd4d":"code","b6845e98":"code","1e01d3e7":"code","fa5466b0":"code","f913463b":"code","47791635":"code","f71a799b":"code","fbd07084":"code","3090a103":"code","72657078":"code","025a71d5":"code","2363dfa6":"code","843c42ff":"code","852105d8":"code","08b47e85":"code","191ee5a1":"code","eb6efc3f":"code","5254a3bb":"code","ea248556":"code","2fe7fe80":"code","371e83c6":"code","1f7cfe61":"code","21135422":"code","1d46181c":"code","e6aedc12":"code","c9de3e64":"code","bb257b83":"code","09a46093":"code","67d50757":"code","b2b4a9a6":"code","e12cf692":"code","3274ab65":"code","09085a9f":"code","6596613b":"code","21dc8fab":"code","e8328d92":"code","7238571d":"code","8471c089":"code","b7cbbbc7":"code","008cbf9b":"code","8fa59ddc":"code","31121326":"code","c255876f":"code","38710655":"code","1a0e9b4c":"code","d4ba968a":"code","e157da60":"code","b9eebfad":"code","cd442d37":"code","a16f2ce1":"code","c901c4a5":"code","d75f729b":"code","f63761a5":"code","f8a3d875":"code","a4b8a624":"code","b1e091c7":"code","039d535d":"code","be8f7a8c":"code","8b1e4e49":"code","2408043c":"code","347a5127":"code","adf1863e":"code","87bf642e":"code","4e0b39d8":"code","5d9bde00":"code","bc937f9d":"code","bc493307":"code","e2611d9f":"markdown","00c5e9b4":"markdown","7a19b5cd":"markdown"},"source":{"c7909414":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\n\nimport gc\ngc.collect()","17f41755":"!pip install pretrainedmodels\n\n%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n\n!pip install fastai==1.0.52\nimport fastai\n\nfrom fastai import *\nfrom fastai.vision import *\nfrom fastai.text import *\n\nfrom torchvision.models import *\nimport pretrainedmodels\n\nfrom utils import *\nimport sys\n\nfrom fastai.callbacks.tracker import EarlyStoppingCallback\nfrom fastai.callbacks.tracker import SaveModelCallback","396b7d4b":"%%bash\npip install pytorch-pretrained-bert","644be9ad":"from pytorch_pretrained_bert import BertTokenizer\nbert_tok = BertTokenizer.from_pretrained(\n    \"bert-base-uncased\",\n)","99ab79f3":"class FastAiBertTokenizer(BaseTokenizer):\n    \"\"\"Wrapper around BertTokenizer to be compatible with fast.ai\"\"\"\n    def __init__(self, tokenizer: BertTokenizer, max_seq_len: int=128, **kwargs):\n        self._pretrained_tokenizer = tokenizer\n        self.max_seq_len = max_seq_len\n\n    def __call__(self, *args, **kwargs):\n        return self\n\n    def tokenizer(self, t:str) -> List[str]:\n        \"\"\"Limits the maximum sequence length\"\"\"\n        return [\"[CLS]\"] + self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2] + [\"[SEP]\"]","28d67c21":"fastai_bert_vocab = Vocab(list(bert_tok.vocab.keys()))","bb9761e2":"fastai_tokenizer = Tokenizer(tok_func=FastAiBertTokenizer(bert_tok, max_seq_len=256), pre_rules=[], post_rules=[])","39e96d4d":"class BertTokenizeProcessor(TokenizeProcessor):\n    def __init__(self, tokenizer):\n         super().__init__(tokenizer=tokenizer, include_bos=False, include_eos=False)\n\nclass BertNumericalizeProcessor(NumericalizeProcessor):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, vocab=Vocab(list(bert_tok.vocab.keys())), **kwargs)\n\n\ndef get_bert_processor(tokenizer:Tokenizer=None, vocab:Vocab=None):\n    \"\"\"\n    Constructing preprocessors for BERT\n    We remove sos\/eos tokens since we add that ourselves in the tokenizer.\n    We also use a custom vocabulary to match the numericalization with the original BERT model.\n    \"\"\"\n    return [BertTokenizeProcessor(tokenizer=tokenizer),\n            NumericalizeProcessor(vocab=vocab)]","42d22442":"class BertDataBunch(TextDataBunch):\n    @classmethod\n    def from_df(cls, path:PathOrStr, train_df:DataFrame, valid_df:DataFrame, test_df:Optional[DataFrame]=None,\n                tokenizer:Tokenizer=None, vocab:Vocab=None, classes:Collection[str]=None, text_cols:IntsOrStrs=1,\n                label_cols:IntsOrStrs=0, label_delim:str=None, **kwargs) -> DataBunch:\n        \"Create a `TextDataBunch` from DataFrames.\"\n        p_kwargs, kwargs = split_kwargs_by_func(kwargs, get_bert_processor)\n        # use our custom processors while taking tokenizer and vocab as kwargs\n        processor = get_bert_processor(tokenizer=tokenizer, vocab=vocab, **p_kwargs)\n        if classes is None and is_listy(label_cols) and len(label_cols) > 1: classes = label_cols\n        src = ItemLists(path, TextList.from_df(train_df, path, cols=text_cols, processor=processor),\n                        TextList.from_df(valid_df, path, cols=text_cols, processor=processor))\n        src = src.label_for_lm() if cls==TextLMDataBunch else src.label_from_df(cols=label_cols, classes=classes)\n        if test_df is not None: src.add_test(TextList.from_df(test_df, path, cols=text_cols))\n        return src.databunch(**kwargs)","df47b8bb":"np.random.seed(42)","d12e8511":"import pandas as pd\ndf = pd.read_json('..\/input\/Sarcasm_Headlines_Dataset_v2.json', lines =True)\ndf.head()","f97c4142":"df.shape","81c13c03":"df['headline'][0]","9198b938":"from sklearn.model_selection import train_test_split\ndf_train, df_valid = train_test_split(df, stratify=df['is_sarcastic'], test_size = 0.10, random_state=42)","3a632a06":"df_train.shape, df_valid.shape","e5bafdcd":"import numpy as np\nimport matplotlib.pyplot as plt","0499fd4d":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline","b6845e98":"path = Path('..\/input\/')","1e01d3e7":"path.ls()","fa5466b0":"src = ItemLists(path, TextList.from_df(df_train, path=\".\", cols=1),\n                        TextList.from_df(df_valid, path=\".\", cols=1))","f913463b":"data=src.label_from_df(cols=2).databunch(bs=48)","47791635":"data.show_batch()","f71a799b":"data.vocab.itos[:10]","fbd07084":"data.train_ds[0][0]","3090a103":"data.train_ds[0][0].data[:10]","72657078":"bs = 32","025a71d5":"src_lm = ItemLists(path, TextList.from_df(df_train, path=\".\", cols=1),\n                        TextList.from_df(df_valid, path=\".\", cols=1))","2363dfa6":"data_lm = src_lm.label_for_lm().databunch(bs=bs)","843c42ff":"data_lm.show_batch()","852105d8":"data_lm.vocab.itos[:20]","08b47e85":"data_lm.train_ds[0][0].data[:10]","191ee5a1":"class FlatLoss(nn.NLLLoss):\n    def forward(self, input:Tensor, target:Tensor) -> Rank0Tensor:\n        return super().forward(input.view(-1), target.view(-1))","eb6efc3f":"learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3, model_dir = \"\/temp\/model\/\")","5254a3bb":"learn.lr_find()\nlearn.recorder.plot(suggestion=True)","ea248556":"learn.fit_one_cycle(2, 5e-2, moms=(0.8, 0.7))","2fe7fe80":"learn.save('fit_head')\nlearn.load('fit_head')","371e83c6":"learn.unfreeze()\nlearn.lr_find()\nlearn.recorder.plot(suggestion=True)","1f7cfe61":"learn.fit_one_cycle(12, max_lr=slice(1e-5, 1e-3), moms=(0.8, 0.7))","21135422":"learn.save('fine_tuned')\nlearn.load('fine_tuned')","1d46181c":"learn.save_encoder('fine_tuned_enc')","e6aedc12":"TEXT = \"He screamed like\"\nN_WORDS = 10\nN_SENTENCES = 2","c9de3e64":"print(\"\\n\".join(learn.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)))","bb257b83":"src_clas = ItemLists(path, TextList.from_df(df_train, path=\".\", cols=1, vocab=data_lm.vocab),\n                        TextList.from_df(df_valid, path=\".\", cols=1, vocab=data_lm.vocab))","09a46093":"data_clas = src_clas.label_from_df(cols=2).databunch(bs=bs)","67d50757":"data_clas.show_batch()","b2b4a9a6":"learn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5, model_dir='\/temp\/model')\nlearn.load_encoder('fine_tuned_enc')","e12cf692":"learn.lr_find()","3274ab65":"learn.recorder.plot(suggestion=True)","09085a9f":"learn.fit_one_cycle(4, slice(3e-2), moms=(0.8, 0.7))","6596613b":"learn.save('first')\nlearn.load('first')","21dc8fab":"learn.freeze_to(-2)\nlearn.fit_one_cycle(4, slice(3e-2\/(2.6**4),3e-2), moms=(0.8,0.7))","e8328d92":"learn.save('second')\nlearn.load('second')","7238571d":"learn.freeze_to(-3)\nlearn.fit_one_cycle(4, slice(3e-2\/(2.6**4),3e-2), moms=(0.8,0.7))","8471c089":"learn.save('third')\nlearn.load('third')","b7cbbbc7":"learn.unfreeze()\nlearn.fit_one_cycle(10, slice(3e-2\/(2.6**4),3e-2), moms=(0.8,0.7))","008cbf9b":"learn.predict(\"Miracle cure kills fifth patient\")","8fa59ddc":"learn.loss_func","31121326":"processor = get_bert_processor(tokenizer=fastai_tokenizer)","c255876f":"src_clas = ItemLists(path, TextList.from_df(df_train, path=\".\", cols=1, vocab=fastai_bert_vocab, processor=processor),\n                        TextList.from_df(df_valid, path=\".\", cols=1, vocab=fastai_bert_vocab, processor=processor))","38710655":"data_clas = src_clas.label_from_df(cols=2).databunch(bs=32)","1a0e9b4c":"data_clas.show_batch()","d4ba968a":"databunch = TextDataBunch.from_df(\".\", df_train, df_valid,\n     tokenizer=fastai_tokenizer, vocab=fastai_bert_vocab, include_bos=True, include_eos=True,\n                                  text_cols=1, label_cols=2, bs=32, collate_fn=partial(pad_collate, pad_first=False))","e157da60":"databunch.show_batch()","b9eebfad":"from pytorch_pretrained_bert.modeling import BertConfig, BertForSequenceClassification, BertForNextSentencePrediction, BertForMaskedLM","cd442d37":"from pytorch_pretrained_bert.optimization import BertAdam, ConstantLR, WarmupCosineSchedule, WarmupConstantSchedule, WarmupLinearSchedule, WarmupCosineWithWarmupRestartsSchedule, WarmupCosineWithHardRestartsSchedule","a16f2ce1":"model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)","c901c4a5":"def bert_clas_split(self) -> List[nn.Module]:\n    \n    bert = model.bert\n    embedder = bert.embeddings\n    pooler = bert.pooler\n    encoder = bert.encoder\n    classifier = [model.dropout, model.classifier]\n    n = len(encoder.layer)\/\/3\n    print(n)\n    groups = [[embedder], list(encoder.layer[:n]), list(encoder.layer[n+1:2*n]), list(encoder.layer[(2*n)+1:]), [pooler], classifier]\n    return groups","d75f729b":"x = bert_clas_split(model)","f63761a5":"x[0]","f8a3d875":"x[3]","a4b8a624":"from fastai.callbacks import *\n\nlearner = Learner(\n    databunch, model,\n    model_dir='\/temp\/model', metrics=accuracy\n)","b1e091c7":"learner.split([x[0], x[1], x[2], x[3], x[5]])","039d535d":"learner.freeze_to(-1)","be8f7a8c":"learner.lr_find()\nlearner.recorder.plot(suggestion=True)","8b1e4e49":"learner.fit_one_cycle(2, max_lr=slice(1e-4), moms=(0.8,0.7), pct_start=0.2, wd =(1e-7, 1e-5, 1e-4, 1e-3, 1e-2))","2408043c":"learner.save('first-head')\nlearner.load('first-head')","347a5127":"x[5]","adf1863e":"learner.freeze_to(-2)\nlearner.fit_one_cycle(2, max_lr=slice(1e-4\/(2.6**4), 1e-4), moms=(0.8, 0.7), wd =(1e-7, 1e-5, 1e-4, 1e-3, 1e-2))","87bf642e":"learner.save('second')\nlearner.load('second')","4e0b39d8":"learner.freeze_to(-3)\nlearner.fit_one_cycle(2, max_lr=slice(1e-4\/(2.6**4), 1e-4), moms=(0.8, 0.7), wd =(1e-7, 1e-5, 1e-4, 1e-3, 1e-2))","5d9bde00":"learner.save('third')\nlearner.load('third')","bc937f9d":"learner.unfreeze()\nlearner.lr_find()\nlearner.recorder.plot(suggestion=True)","bc493307":"learner.fit_one_cycle(2, max_lr=slice(1e-6\/(2.6**4), 1e-6), moms=(0.8, 0.7), wd =(1e-7, 1e-5, 1e-4, 1e-3, 1e-2))","e2611d9f":"# LANGUAGE MODEL - Fastai","00c5e9b4":"# BERT","7a19b5cd":"# CLASSIFIER MODEL - Fastai"}}