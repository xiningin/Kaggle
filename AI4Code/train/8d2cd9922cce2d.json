{"cell_type":{"1c400397":"code","469a87ff":"code","297bc99b":"code","d0810dd5":"code","5d151d9b":"code","f7975de8":"code","bfc55e86":"code","86af5383":"code","9bb40579":"code","647829a8":"code","51b74e2e":"code","628517ac":"code","14514c41":"code","97ef841e":"code","70d03349":"code","8e7ebe27":"code","60cafe20":"code","dd001183":"code","7e878d20":"code","66d9f7ef":"code","0d0384a5":"code","d00b0104":"code","3a4ec24d":"code","e5b95a12":"code","716b6f98":"code","bbb358b1":"code","b9b12f28":"code","cdcdcb77":"code","271b7a07":"code","18e56bc4":"code","64bb725b":"code","2aa94fdf":"code","46834ce2":"code","4d3f0221":"code","adf8dccf":"code","84225cce":"code","8855680a":"markdown","f8d7ce5d":"markdown","838c9407":"markdown","e5af811f":"markdown","94fa4cb9":"markdown","93992bf5":"markdown","54c617af":"markdown","efdefbdf":"markdown","e8851f6c":"markdown","de446e42":"markdown","9dd88329":"markdown"},"source":{"1c400397":"import pandas as pd\nimport numpy as np\n\nimport lightgbm as lgb\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn import model_selection, preprocessing, metrics\nimport datetime\\\n\nimport os\nimport json\nimport numpy as np\nimport pandas as pd\nfrom pandas.io.json import json_normalize","469a87ff":"def load_df(csv_path='..\/input\/train.csv', nrows=None):\n    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n    \n    df = pd.read_csv(csv_path, \n                     converters={column: json.loads for column in JSON_COLUMNS}, \n                     dtype={'fullVisitorId': str}, # Important!!\n                     nrows=nrows)\n    \n    for column in JSON_COLUMNS:\n        column_as_df = json_normalize(df[column])\n        column_as_df.columns = [f\"{column}_{subcolumn}\" for subcolumn in column_as_df.columns]\n        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n    print(f\"Loaded {os.path.basename(csv_path)}. Shape: {df.shape}\")\n    return df","297bc99b":"%%time\ndf_train = load_df()\ndf_test = load_df(\"..\/input\/test.csv\")","d0810dd5":"df_train[\"totals_transactionRevenue\"] = df_train[\"totals_transactionRevenue\"].astype('float')","5d151d9b":"const_cols = [c for c in df_train.columns if df_train[c].nunique(dropna=False)==1 ]\nconst_cols","f7975de8":"cols_to_drop = const_cols + ['sessionId']\n\ndf_train = df_train.drop(cols_to_drop + [\"trafficSource_campaignCode\"], axis=1)\ndf_test = df_test.drop(cols_to_drop, axis=1)","bfc55e86":"print(df_train.shape, df_test.shape)","86af5383":"# Impute 0 for missing target values\ndf_train[\"totals_transactionRevenue\"].fillna(0, inplace=True)\ntrain_y = df_train[\"totals_transactionRevenue\"].values\ntrain_id = df_train[\"fullVisitorId\"].values\ntest_id = df_test[\"fullVisitorId\"].values\n\n\n# label encode the categorical variables and convert the numerical variables to float\ncat_cols = [\"channelGrouping\", \"device_browser\", \n            \"device_deviceCategory\", \"device_operatingSystem\", \n            \"geoNetwork_city\", \"geoNetwork_continent\", \n            \"geoNetwork_country\", \"geoNetwork_metro\",\n            \"geoNetwork_networkDomain\", \"geoNetwork_region\", \n            \"geoNetwork_subContinent\", \"trafficSource_adContent\", \n            \"trafficSource_adwordsClickInfo.adNetworkType\", \n            \"trafficSource_adwordsClickInfo.gclId\", \n            \"trafficSource_adwordsClickInfo.page\", \n            \"trafficSource_adwordsClickInfo.slot\", \"trafficSource_campaign\",\n            \"trafficSource_keyword\", \"trafficSource_medium\", \n            \"trafficSource_referralPath\", \"trafficSource_source\",\n            'trafficSource_adwordsClickInfo.isVideoAd',\n            'trafficSource_isTrueDirect', 'device_isMobile']","9bb40579":"df_train['date'] = df_train['date'].apply(lambda x: datetime.date(int(str(x)[:4]), int(str(x)[4:6]), int(str(x)[6:])))\ndf_train['date'] = pd.to_datetime(df_train['date'])","647829a8":"num_cols = [\"totals_hits\", \"totals_pageviews\", \n            \"visitNumber\", \"visitStartTime\", \n            'totals_bounces',  'totals_newVisits']    \n\nfor col in num_cols:\n    df_train[col] = df_train[col].astype(float)\n    df_test[col] = df_test[col].astype(float)","51b74e2e":"df_train_copy = df_train.copy()\ndf_test_copy = df_test.copy()\n\ndf_train = df_train_copy.copy()\ndf_test = df_test_copy.copy()","628517ac":"train_dates = df_train['date'].copy()","14514c41":"for col in cat_cols:\n    print(col)\n    lbl = preprocessing.LabelEncoder()\n    lbl.fit(list(df_train[col].values.astype('str')) + list(df_test[col].values.astype('str')))\n    df_train[col] = lbl.transform(list(df_train[col].values.astype('str')))\n    df_test[col] = lbl.transform(list(df_test[col].values.astype('str')))","97ef841e":"df_train.shape","70d03349":"for col in df_train.columns:\n    if col not in num_cols and col not in cat_cols:\n        print(col)","8e7ebe27":"not_use_cols = ['date', 'fullVisitorId', 'visitId', 'totals_transactionRevenue']","60cafe20":"len(cat_cols) + len(num_cols)","dd001183":"len(not_use_cols)","7e878d20":"# Split the train dataset into development and valid based on time \ndev_df = df_train[df_train['date']<=datetime.date(2017,5,31)]\nval_df = df_train[df_train['date']>datetime.date(2017,5,31)]\ndev_y = np.log1p(dev_df[\"totals_transactionRevenue\"].values)\nval_y = np.log1p(val_df[\"totals_transactionRevenue\"].values)\n\nuse_cols = [col for col in df_train.columns if col not in not_use_cols]\n\ndev_X = dev_df[use_cols] \nval_X = val_df[use_cols] \ntest_X = df_test[use_cols] \n\n# custom function to run light gbm model\ndef run_lgb(train_X, train_y, val_X, val_y, test_X):\n    params = {\n        \"objective\" : \"regression\",\n        \"metric\" : \"rmse\", \n        \"num_leaves\" : 30,\n        \"min_child_samples\" : 100,\n        \"learning_rate\" : 0.1,\n        \"bagging_fraction\" : 0.7,\n        \"feature_fraction\" : 0.5,\n        \"bagging_frequency\" : 5,\n        \"bagging_seed\" : 1989,\n        \"verbosity\" : -1,\n        'seed': 1989\n    }\n    \n    lgtrain = lgb.Dataset(train_X, label=train_y)\n    lgval = lgb.Dataset(val_X, label=val_y)\n    model = lgb.train(params, lgtrain, 10000, valid_sets=[lgtrain, lgval], early_stopping_rounds=500, verbose_eval=100)\n    \n    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n    return pred_test_y, model\n\n# Training the model #\npred_test, model = run_lgb(dev_X, dev_y, val_X, val_y, test_X)","66d9f7ef":"sub_df = pd.DataFrame({\"fullVisitorId\":test_id})\npred_test[pred_test<0] = 0\nsub_df[\"PredictedLogRevenue\"] = np.expm1(pred_test)\nsub_df = sub_df.groupby(\"fullVisitorId\")[\"PredictedLogRevenue\"].sum().reset_index()\nsub_df.columns = [\"fullVisitorId\", \"PredictedLogRevenue\"]\nsub_df[\"PredictedLogRevenue\"] = np.log1p(sub_df[\"PredictedLogRevenue\"])\nsub_df.to_csv(\"Label_encoding.csv\", index=False)","0d0384a5":"def frequency_encoding(frame, col):\n    freq_encoding = frame.groupby([col]).size()\/frame.shape[0] \n    freq_encoding = freq_encoding.reset_index().rename(columns={0:'{}_Frequency'.format(col)})\n    return frame.merge(freq_encoding, on=col, how='left')","d00b0104":"df_train = df_train_copy.copy()\ndf_test = df_test_copy.copy()","3a4ec24d":"len_train = df_train.shape[0]\ndf_all = pd.concat([df_train, df_test])\n\nfor col in tqdm(cat_cols):\n    df_all = frequency_encoding(df_all, col)\n\ndf_train = df_all[:len_train]\ndf_test = df_all[len_train:]\n\nprint(df_train.shape, df_test.shape)","e5b95a12":"df_train.drop(cat_cols, axis=1, inplace=True)\ndf_test.drop(cat_cols, axis=1, inplace=True)\n\nfreq_cat_cols = ['{}_Frequency'.format(col) for col in cat_cols]\n\n# Split the train dataset into development and valid based on time \ndev_df = df_train[train_dates<=datetime.date(2017,5,31)]\nval_df = df_train[train_dates>datetime.date(2017,5,31)]\n\ndev_y = np.log1p(dev_df[\"totals_transactionRevenue\"].values)\nval_y = np.log1p(val_df[\"totals_transactionRevenue\"].values)\n\nuse_cols = [col for col in df_train.columns if col not in not_use_cols]\n\ndev_X = dev_df[use_cols] \nval_X = val_df[use_cols] \ntest_X = df_test[use_cols]  \n\n# Training the model #\npred_test, model = run_lgb(dev_X, dev_y, val_X, val_y, test_X)","716b6f98":"sub_df = pd.DataFrame({\"fullVisitorId\":test_id})\npred_test[pred_test<0] = 0\nsub_df[\"PredictedLogRevenue\"] = np.expm1(pred_test)\nsub_df = sub_df.groupby(\"fullVisitorId\")[\"PredictedLogRevenue\"].sum().reset_index()\nsub_df.columns = [\"fullVisitorId\", \"PredictedLogRevenue\"]\nsub_df[\"PredictedLogRevenue\"] = np.log1p(sub_df[\"PredictedLogRevenue\"])\nsub_df.to_csv(\"Freq_encoding.csv\", index=False)","bbb358b1":"from sklearn.model_selection import KFold","b9b12f28":"def mean_k_fold_encoding(col, alpha):\n    target_name = 'totals_transactionRevenue'\n    target_mean_global = df_train[target_name].mean()\n    \n    nrows_cat = df_train.groupby(col)[target_name].count()\n    target_means_cats = df_train.groupby(col)[target_name].mean()\n    target_means_cats_adj = (target_means_cats*nrows_cat + \n                             target_mean_global*alpha)\/(nrows_cat+alpha)\n    # Mapping means to test data\n    encoded_col_test = df_test[col].map(target_means_cats_adj)\n    \n    kfold = KFold(n_splits=5, shuffle=True, random_state=1989)\n    parts = []\n    for trn_inx, val_idx in kfold.split(df_train):\n        df_for_estimation, df_estimated = df_train.iloc[trn_inx], df_train.iloc[val_idx]\n        nrows_cat = df_for_estimation.groupby(col)[target_name].count()\n        target_means_cats = df_for_estimation.groupby(col)[target_name].mean()\n\n        target_means_cats_adj = (target_means_cats * nrows_cat + \n                                target_mean_global * alpha) \/ (nrows_cat + alpha)\n\n        encoded_col_train_part = df_estimated[col].map(target_means_cats_adj)\n        parts.append(encoded_col_train_part)\n        \n    encoded_col_train = pd.concat(parts, axis=0)\n    encoded_col_train.fillna(target_mean_global, inplace=True)\n    encoded_col_train.sort_index(inplace=True)\n    \n    return encoded_col_train, encoded_col_test","cdcdcb77":"df_train = df_train_copy.copy()\ndf_test = df_test_copy.copy()","271b7a07":"for col in tqdm(cat_cols):\n    temp_encoded_tr, temp_encoded_te = mean_k_fold_encoding(col, 5)\n    new_feat_name = 'mean_k_fold_{}'.format(col)\n    df_train[new_feat_name] = temp_encoded_tr.values\n    df_test[new_feat_name] = temp_encoded_te.values","18e56bc4":"df_train.drop(cat_cols, axis=1, inplace=True)\ndf_test.drop(cat_cols, axis=1, inplace=True)\n\nmean_cat_cols = ['mean_k_fold_{}'.format(col) for col in cat_cols]\n\n# Split the train dataset into development and valid based on time \ndev_df = df_train[train_dates<=datetime.date(2017,5,31)]\nval_df = df_train[train_dates>datetime.date(2017,5,31)]\n\ndev_y = np.log1p(dev_df[\"totals_transactionRevenue\"].values)\nval_y = np.log1p(val_df[\"totals_transactionRevenue\"].values)\n\nuse_cols = [col for col in df_train.columns if col not in not_use_cols]\n\ndev_X = dev_df[use_cols] \nval_X = val_df[use_cols] \ntest_X = df_test[use_cols] \n\n# Training the model #\npred_test, model = run_lgb(dev_X, dev_y, val_X, val_y, test_X)","64bb725b":"sub_df = pd.DataFrame({\"fullVisitorId\":test_id})\npred_test[pred_test<0] = 0\nsub_df[\"PredictedLogRevenue\"] = np.expm1(pred_test)\nsub_df = sub_df.groupby(\"fullVisitorId\")[\"PredictedLogRevenue\"].sum().reset_index()\nsub_df.columns = [\"fullVisitorId\", \"PredictedLogRevenue\"]\nsub_df[\"PredictedLogRevenue\"] = np.log1p(sub_df[\"PredictedLogRevenue\"])\nsub_df.to_csv(\"mean_encoding.csv\", index=False)","2aa94fdf":"df_train = df_train_copy.copy()\ndf_test = df_test_copy.copy()\n\nlen_train = df_train.shape[0]\ndf_all = pd.concat([df_train, df_test])\n\nfor col in tqdm(cat_cols):\n    df_all = frequency_encoding(df_all, col)\n\ndf_train = df_all[:len_train]\ndf_test = df_all[len_train:]\n\nprint(df_train.shape, df_test.shape)\n\nfor col in tqdm(cat_cols):\n    temp_encoded_tr, temp_encoded_te = mean_k_fold_encoding(col, 5)\n    new_feat_name = 'mean_k_fold_{}'.format(col)\n    df_train[new_feat_name] = temp_encoded_tr.values\n    df_test[new_feat_name] = temp_encoded_te.values","46834ce2":"for col in cat_cols:\n    print(col)\n    lbl = preprocessing.LabelEncoder()\n    lbl.fit(list(df_train[col].values.astype('str')) + list(df_test[col].values.astype('str')))\n    df_train[col] = lbl.transform(list(df_train[col].values.astype('str')))\n    df_test[col] = lbl.transform(list(df_test[col].values.astype('str')))","4d3f0221":"freq_cat_cols = ['{}_Frequency'.format(col) for col in cat_cols]\nmean_cat_cols = ['mean_k_fold_{}'.format(col) for col in cat_cols]","adf8dccf":"print(df_train.shape, df_test.shape)\n\n# Split the train dataset into development and valid based on time \ndev_df = df_train[train_dates<=datetime.date(2017,5,31)]\nval_df = df_train[train_dates>datetime.date(2017,5,31)]\ndev_y = np.log1p(dev_df[\"totals_transactionRevenue\"].values)\nval_y = np.log1p(val_df[\"totals_transactionRevenue\"].values)\n\nuse_cols = [col for col in df_train.columns if col not in not_use_cols]\n\ndev_X = dev_df[use_cols] \nval_X = val_df[use_cols] \ntest_X = df_test[use_cols]  \n\n# Training the model #\npred_test, model = run_lgb(dev_X, dev_y, val_X, val_y, test_X)","84225cce":"sub_df = pd.DataFrame({\"fullVisitorId\":test_id})\npred_test[pred_test<0] = 0\nsub_df[\"PredictedLogRevenue\"] = np.expm1(pred_test)\nsub_df = sub_df.groupby(\"fullVisitorId\")[\"PredictedLogRevenue\"].sum().reset_index()\nsub_df.columns = [\"fullVisitorId\", \"PredictedLogRevenue\"]\nsub_df[\"PredictedLogRevenue\"] = np.log1p(sub_df[\"PredictedLogRevenue\"])\nsub_df.to_csv(\"all.csv\", index=False)","8855680a":"# Label-encoding","f8d7ce5d":"# Mean encoding","838c9407":"# Frequency encoding","e5af811f":"- Of course, it's not easy to say the all(Label + Fre + Mean) is the best choice.\n- But, adding other encoding performs better than only using label encoding.","94fa4cb9":"# Result\n| Encoding  | Training RMSE  | VALID RMSE  |  RMSE(Tr) \/ RMSE(vld)  | LB \n|---|---|---|---|---|\n| Label encoding |  1.52503  | 1.69546  |  1.111755 |  1.4470\n|  Frequency encoding | 1.52039  | 1.69291 | 1.113471 | 1.4545\n|  Mean encoding |  1.52247 | 1.6955  | 1.113651  |  1.4448\n|Label + Fre + Mean (All) | 1.51965 | 1.69179 | 1.1132761 |  1.4417","93992bf5":"# Background\n- As you know, there are features which have high cardinality in this competition.\n- I've studied and read some discussions, blogs and articles about high cardinality.\n- In this kernel, I'll experiment to see which encoding works better.\n- Label encoding, Frequency encoding and Mean encoding will be tested.\n- Because I'm student, I welcome your feedback on anything of this contents.!\n- Ok, Let's see!.\n- To compare, I forked great kernel https:\/\/www.kaggle.com\/sudalairajkumar\/simple-exploration-baseline-ga-customer-revenue.\n- I set the same rrandom number and same parameters for each cases.\n- I want to recommend you, this kernel. https:\/\/www.kaggle.com\/vprokopev\/mean-likelihood-encodings-a-comprehensive-study. \n- This kernel experiments and explains these encodings. Very useful!","54c617af":"# Label + Frequency + Mean encoding","efdefbdf":"# More\n- I know, this experiment is so simple. The result could be changed depending on hyper-parameters and algoritm.\n- But, I think adding various encoding give us better performance than using only one encoding strategy because applying various approachs (like ensemble)  commoly shows good result.","e8851f6c":"- Traning RMSE: All  < Freq < Mean < Label\n- Valid RMSE: All < Freq < Label < Mean\n- Tr\/vld ratio: Label < All < Freq < Mean","de446e42":"- Freq, Mean encoding tend to overfit to training set.\n- But LB of Mean encoding is lower than LB of Freq.","9dd88329":"# Conclusion"}}