{"cell_type":{"fdba626a":"code","b054bfca":"code","fbca09bc":"code","7312f92a":"code","07b4ce0f":"code","a3ca544b":"code","eb122b97":"code","a319a430":"code","6d553c64":"code","e2460f9e":"code","a12b99cd":"code","84ee726a":"code","76ff9fd2":"code","38742edd":"code","3ea13b96":"code","0dd2b91d":"code","a65f4eab":"code","29f2e299":"code","41e5e701":"code","375ec318":"code","d1a8245b":"code","56f60a02":"code","261fbd50":"code","27eb175c":"code","b453ca97":"code","95f9b3d8":"code","4edf9fe4":"code","00bc7cf0":"code","c95dd6f7":"code","e9d5e7bc":"code","aafd6bb9":"code","2d71b2e6":"code","085c9aac":"code","30f4d6c8":"code","8d4c3a1d":"code","acf1d400":"code","9803fa7d":"code","86d015ba":"code","d079b89e":"code","7f41598a":"markdown","066e4a89":"markdown","6d713498":"markdown","ac8b5250":"markdown","91a4bfe8":"markdown","c6f4a0f1":"markdown","99a4194f":"markdown","ed391694":"markdown","43777504":"markdown","6cce23f0":"markdown","0c16cf45":"markdown","590be958":"markdown","f66de59a":"markdown","5b7435ed":"markdown","7e6c4f21":"markdown"},"source":{"fdba626a":"import numpy as np\nimport pandas as pd\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import normalize\nfrom sklearn.manifold import TSNE\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score,log_loss\nfrom sklearn.metrics import plot_confusion_matrix\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn import svm\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\n\nimport math\n\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.linear_model import LogisticRegression\n\n","b054bfca":"data=pd.read_csv('..\/input\/forest-cover-type-dataset\/covtype.csv')\nprint(\"number of datapoints\",data.shape[0])\nprint(\"number of features\",data.shape[1])\ndata.head()","fbca09bc":"data.describe()","7312f92a":"data.info()","07b4ce0f":"data[data.isnull().any(axis=1)]","a3ca544b":"data.isnull().sum()","eb122b97":"data['Cover_Type']=data['Cover_Type']-1\ndata['Cover_Type'].value_counts()","a319a430":"data.Cover_Type.unique()","6d553c64":"ax = sns.countplot(x=\"Cover_Type\", data=data)","e2460f9e":"data.corr()[\"Cover_Type\"]","a12b99cd":"plt.hist(data[\"Cover_Type\"])\n","84ee726a":"data.corr()[\"Cover_Type\"].plot(kind=\"bar\")\n","76ff9fd2":"df1=data.iloc[:,0:14]\ndf2=data['Cover_Type']\ndf1=df1.join(df2)\ndf1.head()","38742edd":"df1.corr()","3ea13b96":"fig = plt.subplots(figsize=(10,10))\nsns.heatmap(df1.corr(),vmax=0.5,square=True,annot=True,cmap='Blues')\nplt.xticks(rotation=90)\nplt.yticks(rotation=0)","0dd2b91d":"fig,axs=plt.subplots(ncols=3)\nsns.boxplot(x='Cover_Type',y='Elevation',data=data,ax=axs[0])#highest in 1 & 7 lowest in 4\nsns.boxplot(x='Cover_Type',y='Aspect',data=data,ax=axs[1])\nsns.boxplot(x='Cover_Type',y='Slope',data=data,ax=axs[2])","a65f4eab":"X=data.drop('Cover_Type',axis=1)\ny=data['Cover_Type']","29f2e299":"X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2,random_state=42)\ntrain_df, cv_df, y_train, y_cv = train_test_split(X_train, y_train, stratify=y_train, test_size=0.2)","41e5e701":"alpha = [100,200]\nmax_depth = [5, 10]\ncv_log_error_array = []\nfor i in alpha:\n    for j in max_depth:\n        print(\"for n_estimators =\", i,\"and max depth = \", j)\n        clf = RandomForestClassifier(n_estimators=i, criterion='gini', max_depth=j, random_state=42, n_jobs=-1)\n        clf.fit(train_df,y_train)\n        sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n        sig_clf.fit(train_df,y_train)\n        sig_clf_probs = sig_clf.predict_proba(cv_df)\n        cv_log_error_array.append(log_loss(y_cv, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n        print(\"Log Loss :\",log_loss(y_cv, sig_clf_probs)) \n\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = RandomForestClassifier(n_estimators=alpha[int(best_alpha\/2)], criterion='gini', max_depth=max_depth[int(best_alpha%2)], random_state=42, n_jobs=-1)\nclf.fit(train_df, y_train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_df,y_train)\n\npredict_y = sig_clf.predict_proba(train_df)\nprint('For values of best estimator = ', alpha[int(best_alpha\/2)], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(cv_df)\nprint('For values of best estimator = ', alpha[int(best_alpha\/2)], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(X_test)\nprint('For values of best estimator = ', alpha[int(best_alpha\/2)], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))","375ec318":"clf = RandomForestClassifier(n_estimators=alpha[int(best_alpha\/2)], criterion='gini', max_depth=max_depth[int(best_alpha%2)], random_state=42, n_jobs=-1)\nclf.fit(train_df,y_train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_df,y_train)\npred_y = sig_clf.predict(X_test)","d1a8245b":"accuracy_score(y_test, pred_y)","56f60a02":"cf_matrix=confusion_matrix(y_test,pred_y)\nprint(cf_matrix)","261fbd50":"f, ax = plt.subplots(figsize=(16, 12))\nsns.heatmap(cf_matrix,annot=True)","27eb175c":"X=data.drop('Cover_Type',axis=1)\ny=data['Cover_Type']","b453ca97":"scaler=StandardScaler()\nx=pd.DataFrame(scaler.fit_transform(X),columns=X.columns)","95f9b3d8":"from sklearn.decomposition import PCA\npca = PCA(n_components=13)\nprinciple=pca.fit_transform(x)\n","4edf9fe4":"x=pd.DataFrame(data=principle,columns=['pca1','pca2','pca3','pca4','pca5','pca6','pca7','pca8','pca9','pca10','pca11','pca12','pca13'])\nx.head()","00bc7cf0":"X_train, X_test, y_train, y_test = train_test_split(x, y,test_size=0.2,random_state=42)\ntrain_df, cv_df, y_train, y_cv = train_test_split(X_train, y_train, stratify=y_train, test_size=0.2)","c95dd6f7":"x.head()","e9d5e7bc":"alpha = [5, 11, 15, 21, 31, 41, 51, 99]\ncv_log_error_array = []\nfor i in alpha:\n    print(\"for alpha =\", i)\n    clf = KNeighborsClassifier(n_neighbors=i)\n    clf.fit(train_df,y_train)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_df,y_train)\n    sig_clf_probs = sig_clf.predict_proba(cv_df)\n    cv_log_error_array.append(log_loss(y_cv, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n    # to avoid rounding error while multiplying probabilites we use log-probability estimates\n    print(\"Log Loss :\",log_loss(y_cv, sig_clf_probs)) \n\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],str(txt)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = KNeighborsClassifier(n_neighbors=alpha[best_alpha])\nclf.fit(train_df, y_train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_df,y_train)\n\npredict_y = sig_clf.predict_proba(train_df)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(cv_df)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(X_test)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))","aafd6bb9":"clf = KNeighborsClassifier(n_neighbors=alpha[best_alpha])\nclf.fit(train_df,y_train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_df,y_train)\npred_y = sig_clf.predict(X_test)","2d71b2e6":"accuracy_score(y_test, pred_y)","085c9aac":"cf_matrix=confusion_matrix(y_test,pred_y)\nprint(cf_matrix)","30f4d6c8":"f, ax = plt.subplots(figsize=(16, 12))\nsns.heatmap(cf_matrix,annot=True)","8d4c3a1d":"from sklearn.linear_model import SGDClassifier\nalpha = [1, 10, 100, 1000]\ncv_log_error_array = []\nfor i in alpha:\n    print(\"for C =\", i)\n    clf = SGDClassifier( class_weight='balanced', alpha=i, penalty='l2', loss='hinge', random_state=42)\n    clf.fit(train_df,y_train)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_df,y_train)\n    sig_clf_probs = sig_clf.predict_proba(cv_df)\n    cv_log_error_array.append(log_loss(y_cv, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n    print(\"Log Loss :\",log_loss(y_cv, sig_clf_probs)) \n\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],str(txt)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='hinge', random_state=42)\nclf.fit(train_df, y_train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_df,y_train)\n\npredict_y = sig_clf.predict_proba(train_df)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(cv_df)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(X_test)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))","acf1d400":"clf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='hinge', random_state=42)\nclf.fit(train_df,y_train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_df,y_train)\npred_y = sig_clf.predict(X_test)","9803fa7d":"accuracy_score(y_test, pred_y)","86d015ba":"cf_matrix=confusion_matrix(y_test,pred_y)\nprint(cf_matrix)","d079b89e":"f, ax = plt.subplots(figsize=(16, 12))\nsns.heatmap(cf_matrix,annot=True)","7f41598a":"# **Importing Library**","066e4a89":"# **RANDOM FOREST CLASSIFIER WITH HYPERPARAMETER TUNING**","6d713498":"# **Exploratory Data Analysis**","ac8b5250":"# **KNearest Neighbors With Hyperparameter Tuning**","91a4bfe8":"**Training and Testing the model with best hyperparameter - RF**","c6f4a0f1":"# **Data Overview**","99a4194f":"# **Check for missing values & variable types**","ed391694":"# **Train Test and Cross Validation**","43777504":"\n**Training and Testing the model with best hyper paramters -KNN**","6cce23f0":"**Training and testing the model with best hyperparameter - SVM**","0c16cf45":"# **Support Vector Machine With Hyperparameter Tuning**","590be958":"# **Principal Component Analysis**","f66de59a":"**Elevation** - Elevation in meters\n\n**Aspect** - Aspect in degrees azimuth\n\n**Slope** - Slope in degrees\n\n**Horizontal_Distance_To_Hydrology ** -** Horz Dist to nearest surface water features**\n\n**Vertical_Distance_To_Hydrology** - Vert Dist to nearest surface water features\n\n**Horizontal_Distance_To_Roadways** - Horz Dist to nearest roadway\n\n**Hillshade_9am (0 to 255 index)** - Hillshade index at 9am, summer solstice\n\n**Hillshade_Noon (0 to 255 index)** - Hillshade index at noon, summer solstice\n\n**Hillshade_3pm (0 to 255 index)** - Hillshade index at 3pm, summer solstice\n\n**Horizontal_Distance_To_Fire_Points** - Horz Dist to nearest wildfire ignition points\n\n**Wilderness_Area (4 binary columns, 0 = absence or 1 = presence)** - Wilderness area designation\n\n**Soil_Type (40 binary columns, 0 = absence or 1 = presence)** - Soil Type designation\n\n**Cover_Type (7 types, integers 1 to 7)** - Forest Cover Type designation","5b7435ed":"# **Reading Forest Cover Data**","7e6c4f21":"# **Standadization**"}}