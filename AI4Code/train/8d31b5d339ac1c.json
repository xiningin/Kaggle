{"cell_type":{"79595fde":"code","5704e462":"code","ed39e9c7":"code","57e86658":"code","5c55866f":"code","0072011e":"code","83b60648":"code","3f82c845":"code","4ebf66f3":"code","7d121846":"code","2a84f298":"markdown","a56dd0d7":"markdown","34a36940":"markdown","6141df57":"markdown","b982458e":"markdown","e394f3af":"markdown","ce37f4c0":"markdown","53eee1fb":"markdown","a1cec64f":"markdown"},"source":{"79595fde":"!git clone https:\/\/github.com\/ayulockin\/client\n%cd client\n!pip -qq install .\n%cd ..","5704e462":"import os\nimport json\nimport time\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\n\nimport xgboost as xgb\nfrom xgboost.callback import EarlyStopping\nfrom sklearn.metrics import mean_squared_error","ed39e9c7":"import wandb\nfrom wandb.xgboost import WandbCallback\n\n# Login\nwandb.login()","57e86658":"crypto_df = pd.read_csv('..\/input\/g-research-crypto-forecasting\/train.csv')\nassets = pd.read_csv('..\/input\/g-research-crypto-forecasting\/asset_details.csv').sort_values(\"Asset_ID\").reset_index(drop=True)\ncrypto_df.head()","5c55866f":"crypto_df['datetime'] = pd.to_datetime(crypto_df['timestamp'], unit='s')\ntrain_df = crypto_df[crypto_df['datetime'] < '2021-06-13 00:00:00']\nvalid_df = crypto_df[crypto_df['datetime'] >= '2021-06-13 00:00:00']\n\nprint(\"Number of samples in train_df: \", len(train_df))\nprint(\"Number of samples in valid_df: \", len(valid_df))","0072011e":"# Features\nfeatues_col = [\"Count\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"VWAP\"]\n\ndef upper_shadow(df):\n    return df['High'] - np.maximum(df['Close'], df['Open'])\n\ndef lower_shadow(df):\n    return np.minimum(df['Close'], df['Open']) - df['Low']\n\ndef log_return(series, periods=1):\n    return np.log(series).diff(periods=periods)\n\ndef fill_nan_inf(df):\n    # Fill NaN values\n    df = df.fillna(0)\n    # Fill Inf values\n    df = df.replace([np.inf, -np.inf], 0)\n    \n    return df\n\ndef create_features(df, label=False):\n    \"\"\"\n    Create time series features\n    \"\"\"\n    # Build features\n    up_shadow = upper_shadow(df)\n    low_shadow = lower_shadow(df)    \n    five_min_log_return = log_return(df.VWAP, periods=5)\n    abs_one_min_log_return = log_return(df.VWAP,periods=1).abs()    \n    features = df[featues_col]\n\n    # Concat all the features into one dataframe\n    X = pd.concat([features, up_shadow, low_shadow, \n                   five_min_log_return, abs_one_min_log_return], \n                  axis=1)\n    \n    # Rename feature columns\n    X.columns = featues_col+[\"up_shadow\", \"low_shadow\", \"five_min_log_return\", \"abs_one_min_log_return\"]\n    \n    # Fill NaN and Inf\n    X = fill_nan_inf(X)\n    \n    if label:\n        y = df.Target\n        # Fill NaN and Inf\n        y = fill_nan_inf(y)\n        \n        return X, y\n    \n    return X","83b60648":"# Get single crypto trading data\nbtc_train = train_df[train_df.Asset_ID==1]\nbtc_valid = valid_df[valid_df.Asset_ID==1]\n\n# Fill missing value\nbtc_train = btc_train.reindex(range(btc_train.index[0],btc_train.index[-1]+60,60),method='pad')\nbtc_valid = btc_valid.reindex(range(btc_valid.index[0],btc_valid.index[-1]+60,60),method='pad')\n\n# Create features\nX_train, y_train = create_features(btc_train, label=True)\nX_valid, y_valid = create_features(btc_valid, label=True)","3f82c845":"def train():\n    with wandb.init() as run:\n        bst_params = {\n            'objective': 'reg:squarederror', \n            'n_estimators': 60,\n            'booster': run.config.booster,\n            'learning_rate': run.config.learning_rate,     \n            'gamma': run.config.gamma,\n            'max_depth': run.config.max_depth,\n            'min_child_weight': run.config.min_child_weight,  \n            'eval_metric': ['rmse'],\n            'tree_method': 'gpu_hist',\n        }\n\n        # Initialize the XGBoostClassifier\n        xgbmodel = xgb.XGBRegressor(**bst_params)\n\n        # Train the model, using the wandb_callback for logging\n        xgbmodel.fit(X_train, y_train, \n                     eval_set=[(X_valid, y_valid)],\n                     callbacks=[\n                         WandbCallback(log_model=True,\n                                       log_feature_importance=False,\n                                       define_metric=True)\n                     ],\n                     verbose=False)\n        \n        preds = xgbmodel.predict(X_valid)\n        rmse = np.sqrt(mean_squared_error(y_valid, preds))\n        print(\"RMSE: %f\" % (rmse))\n        wandb.log({\"Valid_RMSE\": rmse})","4ebf66f3":"sweep_config = {\n  \"name\" : \"btc_hyperparam_search\",\n  \"method\" : \"random\",\n  \"parameters\" : {\n    \"booster\": {\n        \"values\": [\"gbtree\", \"gblinear\"]\n    },\n    \"learning_rate\": {\n      \"min\": 0.001,\n      \"max\": 1.0\n    },\n    \"gamma\": {\n      \"min\": 0.001,\n      \"max\": 1.0\n    },\n    \"max_depth\": {\n        \"values\": [3, 5, 7]\n    },\n    \"min_child_weight\": {\n      \"min\": 1,\n      \"max\": 150\n    },\n    \"early_stopping_rounds\": {\n      \"values\" : [10, 20, 40, 40,]\n    },\n  }\n}\n\nsweep_id = wandb.sweep(sweep_config, project='btc_hyperparam_search')","7d121846":"wandb.agent(sweep_id, project='btc_hyperparam_search', function=train)","2a84f298":"# Features","a56dd0d7":"In this notebook we will be using Weights and Biases (W&B) integration for XGBoost for experiment tracking and use W&B Sweep for hyperparameter sweep. \n\nNote that I am using my own [fork](https:\/\/github.com\/ayulockin\/client) of the `wandb\/client` repo where I have improved the existing integration for XGBoost. You can find the pending PR [here](https:\/\/github.com\/wandb\/client\/pull\/2929). It is expected to be merged soon. I hope this notebook shows you the benefits of using this callback. ","34a36940":"We will take just one crypto asset and find the best combination of hyperparameters to forecast the target for the validation set. \n\nThere are two reasons to do so:\n\n* `MultiOutputRegressor` wrapper for `XGBRegressor` is limited. We can't perform multi-output prediction\/evaluation using this wrapper. Check out the GitHub issue [here](https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/15953). \n\n* We will use Bitcoin which is responsible to move the target because of having the hightest weightage (6.779922). ","6141df57":"# Load Dataset\n\nIf you haven't already check out the [Tutorial to the G-Research Crypto Competition](https:\/\/www.kaggle.com\/cstein06\/tutorial-to-the-g-research-crypto-competition).","b982458e":"With just 2-3 lines of extra code you could monitor so much more and make sense of the most important hyperparameters for your XGBoost model. \n\n## [Check out the Sweeps page here $\\rightarrow$](https:\/\/wandb.ai\/ayut\/btc_hyperparam_search\/sweeps\/c1pgsztw?workspace=user-ayut)\n\n<!-- ![img](https:\/\/media.giphy.com\/media\/2ji2l1fsec75l6yoFD\/giphy.gif) -->\n![sweepdemo_4.gif](https:\/\/s10.gifyu.com\/images\/sweepdemo_4.gif)","e394f3af":"# Prepare Split","ce37f4c0":"# Hyperparameter Tuning","53eee1fb":"# Imports and Setup","a1cec64f":"The existing integration of XGBoost (`wandb_callback`) uses an old style callback that will be deprecated in favor of `WandbCallback`. "}}