{"cell_type":{"d8abaa9c":"code","596ffc4e":"code","754cc315":"code","47bdb3eb":"code","a15e64ac":"code","4f0ae3e9":"code","658bda35":"code","39e64ec6":"code","fc1756bc":"code","658752ee":"code","79f0497f":"code","f76833c9":"code","b53c0fca":"code","4317ca25":"code","53fd3f4e":"code","5c107d18":"code","ea126659":"code","a2140595":"code","6fe5a32e":"code","6aa692d6":"code","3141d40e":"code","106eb05f":"code","f5e978c8":"code","2762b914":"code","c1ddc7f5":"code","cd4cefda":"markdown","8a6566ff":"markdown","82f5c499":"markdown","da303864":"markdown","18812ba1":"markdown","45f678d0":"markdown","046f5c6d":"markdown","d852e665":"markdown"},"source":{"d8abaa9c":"import json\nimport nltk\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndf=pd.read_csv(\"..\/input\/CORD-19-research-challenge\/metadata.csv\")\n","596ffc4e":"df.journal.isnull().value_counts()\ndf.publish_time.isnull().value_counts()","754cc315":"# 20 most popular journals\ndf.journal.value_counts()[0:20].plot(kind='bar')\nplt.grid()\nplt.show()","47bdb3eb":"biorxiv=pd.read_csv(\"..\/input\/cord-19-eda-parse-json-and-generate-clean-csv\/biorxiv_clean.csv\")\ncomm_use=pd.read_csv(\"..\/input\/cord-19-eda-parse-json-and-generate-clean-csv\/clean_comm_use.csv\")\nnoncomm_use=pd.read_csv(\"..\/input\/cord-19-eda-parse-json-and-generate-clean-csv\/clean_noncomm_use.csv\")\npmc=pd.read_csv(\"..\/input\/cord-19-eda-parse-json-and-generate-clean-csv\/clean_pmc.csv\")\n","a15e64ac":"import nltk\nfrom nltk.corpus import stopwords\n\nstop_words=stopwords.words(\"english\")\n\n#some preprocessing\ndef preprocess_text(text):\n    #lower\n    text=str(text).lower()\n    #tokenize\n    token = nltk.RegexpTokenizer(r'\\w+')\n    tk = token.tokenize(text)\n    #remove numbers\n    no_num = [word for word in tk if not word.isnumeric()]\n    #remove stopwords\n    no_stop = [word for word in no_num if word not in stop_words]\n    #lemmatize\n    lemmatizer=nltk.stem.WordNetLemmatizer()\n    lem = [lemmatizer.lemmatize(word) for word in no_stop]\n    return lem\n\n#example\ntext=biorxiv[\"abstract\"][4]\n\npreprocess_text(text)[:4]","4f0ae3e9":"def search_by_word(csv,word_list):\n    paper_list=[]\n    \n    lemmatizer=nltk.stem.WordNetLemmatizer()\n    words = [lemmatizer.lemmatize(word) for word in word_list]\n    \n    for index, paper in csv.iterrows():\n        if all([word in preprocess_text(paper.abstract) for word in words]):\n            paper_list.append(paper.paper_id)\n            \n    return paper_list\n\nsearch = search_by_word(biorxiv,[\"proton\"])\nprint(len(search))","658bda35":"br_surf = search_by_word(biorxiv,[\"decontamination\"])\n#noncomm_use_surf=search_by_word(comm_use,[\"adhesion\"])\n#comm_use_surf=search_by_word(noncomm_use,[\"adhesion\"])\npmc_surf=search_by_word(pmc,[\"decontamination\"])\nprint(\"number of articles for keyword surface in biorxiv are:\",len(br_surf))\n#print(\"number of articles for keyword surface in non_comm_use are:\",len(noncomm_use_surf))\n#print(\"number of articles for keyword surface in non_comm_use are:\",len(comm_use_surf))\nprint(\"number of articles for keyword surface in non_comm_use are:\",len(pmc_surf))\n","39e64ec6":"#keep only papers of interest\n\ndef papers_of_interest(csv,word_list):\n    paper_list=search_by_word(csv,word_list)\n    poi = pd.DataFrame.copy(csv)\n    for index,paper in csv.iterrows():\n        if paper.paper_id not in paper_list:\n            poi=poi.drop(index)\n    return poi\npoi = papers_of_interest(biorxiv,[\"proton\"])\nprint(poi)","fc1756bc":"from collections import Counter\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\n\ndef word_bar_graph_function(df,column,title):\n    # adapted from https:\/\/www.kaggle.com\/benhamner\/most-common-forum-topic-words\n    topic_words = [ z.lower() for y in\n                       [ preprocess_text(x) for x in df[column] if isinstance(x, str)]\n                       for z in y]\n    word_count_dict = dict(Counter(topic_words))\n    #filtering additional stopwords. \n    # TODO: Find better way than hardcode\n    add_stopwords = [\"conclusion\",\"preprint\",\"http\",\"doi\",\"biorxiv\",\"medrxiv\"]\n    stop_words = list(set(stopwords.words(\"english\"))|set(add_stopwords))\n    popular_words = sorted(word_count_dict, key = word_count_dict.get, reverse = True)\n    popular_words_nonstop = [w for w in popular_words if w not in stop_words]\n    plt.barh(range(50), [word_count_dict[w] for w in reversed(popular_words_nonstop[0:50])])\n    plt.yticks([x + 0.5 for x in range(50)], reversed(popular_words_nonstop[0:50]))\n    plt.title(title)\n    plt.show()\n\n\n            \n\nplt.figure(figsize=(10,10))\nword_bar_graph_function(poi,\"abstract\", \"Most common words in abstracts with environment & transmission\")\n\n","658752ee":"from gensim.models import Word2Vec","79f0497f":"def gen_train(): #ttype - texttype : choose abstract or text (for the full text)\n    journals = [biorxiv]#,comm_use,noncomm_use,pmc]\n    train_set = []\n    for element in journals:\n        for index,paper in element.iterrows():\n            doc = preprocess_text(paper.text)\n            train_set.append(doc) \n    return train_set\n\n# generate train set\ntrain = gen_train()\n","f76833c9":"model = Word2Vec(train,min_count=1,window=10,size=100)","b53c0fca":"model.most_similar(\"charge\")\n","4317ca25":"import numpy as np\nimport gensim\nimport time\n\ndef similar_count_dic(model,word):\n    similars = model.most_similar(word)\n    sim_new = {}\n    sim_new[word] = 0\n    for i in range(len(similars)):\n        sim_new[ similars[i][0] ] = 0\n        \n    return sim_new\n\ndef get_tk2id(paper_text):\n    \n    text_tk = preprocess_text(paper_text)\n    dictionary = gensim.corpora.Dictionary([text_tk])#wo_stop)\n    corpus = dictionary.doc2bow(text_tk)#title) for title in wo_stop]\n    try:\n        dictionary[0]\n    except KeyError:\n        pass\n    tk2id=dictionary.token2id\n    \n    return tk2id,corpus\n\n#note that only the paper_id is output without the information where it can be found (e.g. biorxiv)\n\ndef search_by_similarity(model,word):\n    sim_count = similar_count_dic(model,word)\n    results={}\n    journals = [biorxiv,noncomm_use,comm_use,pmc]\n    for element in journals:\n        for index,paper in element.iterrows():\n            tk2id , corpus = get_tk2id(paper.text)\n            counter = 0\n            for word in sim_count.keys():\n                try: \n                    if corpus[tk2id[word]][1]>0:\n                        counter += corpus[tk2id[word]][1]\n                except KeyError:\n                    pass\n            if counter > 0:\n                results[paper.paper_id] = counter\n    return results\n        \n","53fd3f4e":"t=time.time()\nresults = search_by_similarity(model,\"charge\")\nt1=time.time()\nprint(t1-t)","5c107d18":"def filter_results(results):\n    del_list = [ key for key in results if results[key]<140 ]\n    for key in del_list:\n        del results[key]\n        \n    return results\n\ndef id_from_meta(paper_id):\n    journals = [biorxiv,noncomm_use,comm_use,pmc]\n    subset_names = [\"biorxiv\",\"noncomm_use\",\"comm_use\",\"pmc\"]\n    res = []\n    for element in journals:\n        for index, paper in element.iterrows():\n            if paper.paper_id == paper_id:\n                return (subset_names[journals.index(element)],index)\n                break\n    print(\"paper_id not found\")\n        \n        \n            \nr=filter_results(results)\nid_from_meta(list(r.keys())[0])","ea126659":"with open(\"..\/input\/CORD-19-research-challenge\/biorxiv_medrxiv\/biorxiv_medrxiv\/\"+\"e7c3ec3dcb3469ee4d608029e7aa3068a4c90c54\"+\".json\") as f:\n    data=json.load(f)\ndata[\"metadata\"]","a2140595":"#compare with most similar words\n#model.most_similar(\"charge\")","6fe5a32e":"def highlight(text):\n    return \" \\033[1;41m \" + text + \" \\033[m \"  \n\ndef mark_passages(text,pass_nums):\n    \n    text = nltk.sent_tokenize(text)\n    for i in range(len(text)):\n        if i in pass_nums:\n            text[i] = highlight( text[i] )\n        print(text[i])\n\ndef preprocess_doc2vec(text):\n    text = nltk.sent_tokenize(text)\n    token = nltk.RegexpTokenizer(r'\\w+')\n    out = [token.tokenize(line) for line in text]\n    return out\n\ntrain_ex = preprocess_doc2vec(biorxiv[\"text\"][631])\ntrain_ex[:2];","6aa692d6":"import nltk\nfrom nltk.corpus import stopwords\nfrom gensim.models.doc2vec import Doc2Vec,TaggedDocument\n\n#some preprocessing\n\n#train_set = gen_train_doc()\n","3141d40e":"\ndef tagged_arts(train):\n    tagged_arts = []\n    for j in range(len(train)):\n        tagged_doc = [TaggedDocument(\n                     words=[word for word in document],\n                     tags=[i]\n                 ) for i, document in enumerate(train)]\n        tagged_arts.append(tagged_doc)\n    return tagged_arts\n\ntagged_arts = tagged_arts(train_ex)\n\n\n","106eb05f":"model = Doc2Vec(tagged_arts[0], vector_size=50, window=10, min_count=1, workers=4,train_epochs=50)","f5e978c8":"train_ex[0];\ninferred_vector = model.infer_vector(nltk.sent_tokenize(\"ion misses force to push into gate\"))\nsims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\nsims\n","2762b914":"train_ex[14];\n#biorxiv[\"abstract\"][0]","c1ddc7f5":"mark_passages(biorxiv[\"text\"][631],[10])","cd4cefda":"**Number of articles which contain publish_time information**\n\nRoughly 18k articles got published in a journal, while for the other articles (roughly 11k) this information is missing.\nThis can have various reasons (just missing, not yet published\/peer-reviewed, rejected (which itself can have various reasons)).\n","8a6566ff":"In this part I will train a doc2vec model to find parts of an article most similar to a particular question.\nThen the whole text is output with these particular parts highlighted. In this way, relevant information can be found and processed quickly. However, we do not throw away the context. If the researcher feels that the highlighted sentence does not hold all relevant information, the sorrounding sentences are still there (in contrast to summarising the article or only displaying the highlighted sentences). This part is still work in progress.\n\n","82f5c499":"Loading output of [xhlulu's](https:\/\/www.kaggle.com\/xhlulu\/cord-19-eda-parse-json-and-generate-clean-csv) kernel.","da303864":"I want to use a slightly altered version of the word count function from [Paul Mooney](https:\/\/www.kaggle.com\/paultimothymooney\/most-common-words-in-the-cord-19-dataset).","18812ba1":"Following the kernel of [Patrick S\u00e1nchez](https:\/\/www.kaggle.com\/saga21\/cord-19-data-extraction-functions) we will use a search by keywords. However, instead of the full text, I will search the abstracts for the keywords. Using only the abstract is faster than using the full text, but also leads to a much smaller number of \"hits\". These should still contain the most relevant articles, since the keywords appear in the abstract.","45f678d0":"**Word2Vec search by similar words**\n\nThe following method uses a word2vec model, which is trained on the complete set of articles text's, to search the articles not only by one keyword, but by all words that are similar to this keyword according to the word2vec model. I hoped that in this way one gets articles that really cover the desired topic. Unfortunately, this is pretty slow. When filtering all articles where the wordcount of all words similar to the keyword is higher than 50, we get a few articles that seem to touch the topic.\n\nTurns out I forgot to search by the keyword itself. Added it, it works better now. Its still slow, though.","046f5c6d":"##############################################################################################################\n","d852e665":"**This notebook provides easy-to-use functions mainly for searching the articles by keyword plus some initial thoughts on the dataset.**\n\n\n* some initial thoughts\n* easy-to-use function to browse abstracts by keywords\n* search all articles by all words similar to the keyword (word2vec) -- Disclaimer: slow and not super-useful\n* doc2vec model to search a particular paper and mark relevant passages -- still work in progress\n\n\n"}}