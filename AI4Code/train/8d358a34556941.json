{"cell_type":{"f17e5dc4":"code","2167ce67":"code","97bc4a4d":"code","22dd6968":"code","a5c325d1":"code","f219ac3c":"code","cd016c7f":"code","1b7578ff":"code","7c7ee033":"code","967fece8":"code","05ee97c1":"code","86f3d5df":"code","211ae603":"code","c2ef9e35":"code","d6d35535":"code","64ea61e1":"code","76023ad4":"code","b43c8921":"code","eedc13af":"code","0e1f9d90":"code","03ab3c1d":"code","d804e118":"code","aaca43ce":"code","ccf82b3f":"code","da1ec457":"code","0b409f68":"code","8c1a2923":"code","0071a7d7":"code","2c0f2d7d":"code","2371958d":"code","7a45b801":"code","e4b5dc3d":"code","5e3a7fd9":"code","834cf3c9":"code","159c3eb3":"code","5dbec52b":"code","9b79e078":"code","206d6dfc":"code","5c018de3":"code","29d42695":"code","e7e369a7":"code","10420e79":"code","1a4832aa":"code","878d617c":"code","7e2554ff":"code","19b41125":"code","79bdec70":"code","54000165":"code","d4c2861d":"code","4311957a":"code","967d46c5":"code","edae71cf":"code","f847de09":"code","29e930b3":"code","a331ad17":"code","17e4ad8a":"code","97172cd8":"code","c707154f":"code","0a9284c6":"markdown","ad744648":"markdown","c6476ba3":"markdown","fc775d6b":"markdown","73802002":"markdown","294250f3":"markdown","3d14da6f":"markdown","d02d61cc":"markdown","1f309334":"markdown","614036b5":"markdown","6682f0bf":"markdown","16b7cac9":"markdown","8180dea0":"markdown","951b56dd":"markdown","3b7b670d":"markdown","af25deb7":"markdown","a57b4b9a":"markdown","2535d949":"markdown","0ff72f92":"markdown","dd08159d":"markdown","5828e970":"markdown","0c9ede23":"markdown","8c26c24d":"markdown","9d869af2":"markdown","addd2b2d":"markdown","e23eb8d9":"markdown"},"source":{"f17e5dc4":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","2167ce67":"#reads tabular data and gives in tabular form\niris = pd.read_csv(\"..\/input\/iris\/Iris.csv\")\n#iris=pd.read_excel(\"\")","97bc4a4d":"print(iris)","22dd6968":"arr = np.array(iris)\n\nprint(arr)\n\n","a5c325d1":"print(type(arr))","f219ac3c":"print(arr[0])","cd016c7f":"print(arr[2] + arr[3])","1b7578ff":"print(arr[1:5])","7c7ee033":"print(arr.shape)","967fece8":"print(arr[4:])","05ee97c1":"print(arr.dtype)","86f3d5df":"iris.head()","211ae603":"iris.head(10)","c2ef9e35":"iris[20:30]","d6d35535":"iris.iloc[0:50,2]","64ea61e1":" species= iris[\"Species\"]\nprint(species)\ntype(species)","76023ad4":"species.shape","b43c8921":"id_species = iris[[\"Id\", \"Species\"]]\nprint(id_species)\nid_species.head(15)","eedc13af":"iris.SepalWidthCm.mean()","0e1f9d90":"max(iris.SepalWidthCm)","03ab3c1d":"iris.loc[iris['SepalLengthCm'] > 6]","d804e118":"iris.loc[iris['SepalLengthCm'] > 6, ['Species']]","aaca43ce":"iris[\"PetalLengthCm\"].max()","ccf82b3f":"iris.tail(-20)","da1ec457":"iris.shape","0b409f68":"print(iris)","8c1a2923":"#A statistical summary of the data\niris.describe()","0071a7d7":"iris.info()","2c0f2d7d":"iris.columns.unique()","2371958d":"iris.Species.unique()","7a45b801":"iris.isna()","e4b5dc3d":"print(iris.isna().sum())","5e3a7fd9":"iris.isnull().sum()","834cf3c9":"iris.dtypes","159c3eb3":"iris['Species'].value_counts()","5dbec52b":"iris.isnull().values.any()","9b79e078":"#This is a multivariate plot\nsns.pairplot(iris.drop(\"Id\", axis=1), hue=\"Species\", size=3)","206d6dfc":"#Checking for outliers\n#These are univariate plots\nplt.title(\"Sepal Length and petal Length Variation\")\nplt.figure(1)\nplt.boxplot([iris['SepalLengthCm']])\nplt.figure(2)\nplt.boxplot([iris['SepalWidthCm']])\nplt.show()","5c018de3":"plt.figure(figsize=(14,10))\nplt.title(\"Variation of Petal width in comparison with sepal length\")\nplt.xlabel(\"Length of Sepal\")\nplt.ylabel(\"Width of Petal\")\nplt.scatter(x=iris.SepalLengthCm,y=iris.SepalWidthCm,color='r')\nplt.show()","29d42695":"plt.figure(figsize=(20,10))\niris.hist()\nplt.show()","e7e369a7":"plt.figure(figsize=(30,20))\niris.plot(kind ='density',subplots = True, layout =(3,3),sharex = False)","10420e79":"sns.lmplot(data=iris, x=\"SepalLengthCm\", y=\"PetalWidthCm\", hue=\"Species\")","1a4832aa":"# Import label encoder \nfrom sklearn import preprocessing \n \n# label_encoder object knows how to understand word labels. \nlabel_encoder = preprocessing.LabelEncoder() \n  \n# Encode labels in column 'species'. \niris['Species']= label_encoder.fit_transform(iris['Species']) \n  \niris['Species'].unique()","878d617c":"iris.Species.dtypes","7e2554ff":"iris.tail(20)","19b41125":"from sklearn.model_selection import train_test_split\ntrain, test = train_test_split(iris, test_size = 0.25)\nprint(train.shape)\nprint(test.shape)","79bdec70":"#Scaling data\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\n\nsc = StandardScaler()\nsc.fit(train)\ntrain_X_std = sc.transform(train)\ntest_X_std = sc.transform(test)","54000165":"train_X = train[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm',\n                 'PetalWidthCm']]\ntrain_y = train.Species\n\ntest_X = test[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm',\n                 'PetalWidthCm']]\ntest_y = test.Species","d4c2861d":"train_X.head(10)","4311957a":"test_y.head()","967d46c5":"#Applying SVC (Support Vector Classification)\nfrom sklearn.svm import SVC\n\nsvm = SVC(kernel='rbf', random_state=0, gamma=.10, C=1.0)\nsvm.fit(train_X_std, train_y)\nprint('The accuracy of the SVM classifier on training data is {:.2f}'.format(svm.score(train_X_std, train_y)))\nprint('The accuracy of the SVM classifier on test data is {:.2f}'.format(svm.score(test_X_std,test_y)))","edae71cf":"#Applying Knn\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors = 7, p = 2, metric='minkowski')\nknn.fit(train_X_std, train_y)\n\nprint('The accuracy of the Knn classifier on training data is {:.2f}'.format(knn.score(train_X_std, train_y)))\nprint('The accuracy of the Knn classifier on test data is {:.2f}'.format(knn.score(test_X_std, test_y)))","f847de09":"#Applying XGBoost\nimport xgboost as xgb\n\nxgb_clf = xgb.XGBClassifier()\nxgb_clf = xgb_clf.fit(train_X_std, train_y)\n\nprint('The accuracy of the XGBoost classifier on training data is {:.2f}'.format(xgb_clf.score(train_X_std, train_y)))\nprint('The accuracy of the XGBoost classifier on test data is {:.2f}'.format(xgb_clf.score(test_X_std,test_y)))","29e930b3":"#Applying Decision Tree\nfrom sklearn import tree\n\n#Create tree object\ndecision_tree = tree.DecisionTreeClassifier(criterion='gini')\n\n#Train DT based on scaled training set\ndecision_tree.fit(train_X_std, train_y)\n\n#Print performance\nprint('The accuracy of the Decision Tree classifier on training data is {:.2f}'.format(decision_tree.score(train_X_std, train_y)))\nprint('The accuracy of the Decision Tree classifier on test data is {:.2f}'.format(decision_tree.score(test_X_std, test_y)))","a331ad17":"#Applying RandomForest\nfrom sklearn.ensemble import RandomForestClassifier\n\n#Create Random Forest object\nrandom_forest = RandomForestClassifier()\n\n#Train model\nrandom_forest.fit(train_X_std, train_y)\n\n#Print performance\nprint('The accuracy of the Random Forest classifier on training data is {:.2f}'.format(random_forest.score(train_X_std, train_y)))\nprint('The accuracy of the Random Forest classifier on test data is {:.2f}'.format(random_forest.score(test_X_std,test_y)))","17e4ad8a":"predictions1 = svm.predict(test_X_std)\nprint(predictions1)","97172cd8":"predictions2= decision_tree.predict(test_X_std)\nprint(predictions2)","c707154f":"predictions3= random_forest.predict(test_X_std)\nprint(predictions2)","0a9284c6":"# Define the Models","ad744648":"**Pandas Example**","c6476ba3":"*NumPy is a Python library used for working with arrays.\nIt also has functions for working in domain of linear algebra, fourier transform, and matrices.\nNumPy was created in 2005 by Travis Oliphant. It is an open source project and you can use it freely.\nNumPy stands for Numerical Python.*\n\n> In computer programming, pandas is a software library written for the Python programming language for data manipulation and analysis. In particular, it offers data structures and operations for manipulating numerical tables and time series. The package is known for a very useful data structure called the pandas DataFrame. Pandas also allows Python developers to easily deal with tabular data (like spreadsheets) within a Python script.","fc775d6b":"# Importing Necessary Libraries","73802002":"![maxresdefault.jpg](attachment:maxresdefault.jpg)","294250f3":"*Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.*","3d14da6f":"# Exploratory Data Analysis\n**We will explore the data to find useful information and insights from the data which will help us to prepare our machine learning model**","d02d61cc":"**Numpy Examples**","1f309334":"# Define, Train and test the model","614036b5":"![Decision-tree-for-Iris-dataset.png](attachment:Decision-tree-for-Iris-dataset.png)","6682f0bf":"**Random forest is a supervised learning algorithm. The \"forest\" it builds, is an ensemble of decision trees, usually trained with the \u201cbagging\u201d method. The general idea of the bagging method is that a combination of learning models increases the overall result.\nActually random forest builds multiple decision trees and merges them together to get a more accurate and stable prediction.**","16b7cac9":"# Data Preprocessing  - or preparing the dataset for training\n","8180dea0":"![Difference-between-Supervised-Unsupervised-amd-reinforcement-learning.jpg](attachment:Difference-between-Supervised-Unsupervised-amd-reinforcement-learning.jpg)","951b56dd":"*Selecting a subset of dataframe*","3b7b670d":"**Gradient boosting is one of the most powerful techniques for building predictive models. Gradient boosting classifiers are a group of machine learning algorithms that combine many weak learning models together to create a strong predictive model. Decision trees are usually used when doing gradient boosting. Gradient boosting models are becoming popular because of their effectiveness at classifying complex datasets, and have recently been used to win many Kaggle data science competitions.**","af25deb7":"# Data Visualization\n\n***We will know plot some graphs to get more insights from the data***","a57b4b9a":"# Loading the data","2535d949":"*Thanks a lot to [The Tech Lab](https:\/\/www.facebook.com\/thetechlab) for providing me the opportunity to take this course and to allow me to interact with some amazing minds. You can also follow me on [Facebook](https:\/\/www.facebook.com\/salmaneunus27) for future updates*","0ff72f92":"![Difference-Between-Classification-and-Regression-Comparison-Summary.jpg](attachment:Difference-Between-Classification-and-Regression-Comparison-Summary.jpg)","dd08159d":"![iris-machinelearning.png](attachment:iris-machinelearning.png)","5828e970":"**SVM is a supervised machine learning algorithm which can be used for classification or regression problems. It uses a technique called the kernel trick to transform your data and then based on these transformations it finds an optimal boundary between the possible outputs. Simply put, it does some extremely complex data transformations, then figures out how to seperate your data based on the labels or outputs you've defined.**","0c9ede23":"![SL-type.png](attachment:SL-type.png)","8c26c24d":"# Decison Tree Classifier","9d869af2":"*K-nearest neighbors (KNN) algorithm is a type of supervised ML algorithm which can be used for both classification as well as regression predictive problems. However, it is mainly used for classification predictive problems in industry.*","addd2b2d":"**Standardize features by removing the mean and scaling to unit variance**","e23eb8d9":"![EDA.jpg](attachment:EDA.jpg)"}}