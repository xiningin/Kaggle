{"cell_type":{"6833e7f7":"code","ccd6a16a":"code","3bcba12e":"code","f04d02ce":"code","5aa26969":"code","b900310f":"code","330fc632":"code","4ab62c93":"code","f2c46057":"code","3c332525":"code","d7498b48":"code","7719745f":"code","7f69377e":"code","b2f29f96":"code","db68d2a4":"code","6c102bec":"code","12c57a24":"code","39f97277":"code","cd61a230":"code","3f8bb6f4":"code","366a5ed7":"code","f04a8f29":"code","f0bad4a0":"code","c9eb9b5e":"code","efa2a869":"code","34952763":"code","08c8798b":"code","77d8f804":"code","b3c6654a":"code","fc90f5a9":"code","207ea9b6":"code","9349e20c":"code","23c22ae1":"code","73832d04":"code","a57029c1":"code","45950872":"code","9c0c30c7":"code","f05d52a4":"code","2e2596d5":"code","374878a3":"code","ed354d13":"code","6bdd1783":"code","ab2d2011":"code","7f0a9247":"code","5ffbfbd3":"code","b5eca8c5":"code","566ebbae":"code","06bf6642":"code","f7c54fc9":"code","03555a08":"code","d134d829":"code","72004fd3":"code","6bdb65bd":"code","414b8a19":"code","b47ea216":"code","28e37778":"code","1d3efb3f":"code","87ea1562":"code","265ec305":"code","01787eda":"code","8fe69a23":"code","6400b5de":"code","66f70ca8":"code","9cc545bf":"code","734ec36f":"code","bc4df3f7":"code","a8f2110e":"code","90ddc22c":"markdown","c14499bf":"markdown","d837a599":"markdown","aaa72c34":"markdown","229895cb":"markdown","b949dc50":"markdown","4c351709":"markdown","91741d4c":"markdown","f85db481":"markdown","5ac04771":"markdown","9edb616f":"markdown","1757fdb9":"markdown"},"source":{"6833e7f7":"import numpy as np\n\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\n\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(22,22)})\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, StratifiedShuffleSplit, KFold, GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score, precision_score, mean_squared_error\nfrom sklearn.linear_model import LogisticRegression\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport feather\n\nfrom xgboost import XGBClassifier","ccd6a16a":"train = pd.read_csv(\"..\/input\/wec-ml-mentorship-contest\/WEC_Contest_sample_submission.csv\", low_memory=False)\ntrain.head()","3bcba12e":"train = pd.read_csv(\"..\/input\/wec-ml-mentorship-contest\/WEC_Contest_train.csv\", low_memory=False)\nlabel = train[\"Label\"]\ntrainRows = train.shape[0]\n\ntest = pd.read_csv(\"..\/input\/wec-ml-mentorship-contest\/WEC_Contest_test.csv\", low_memory=False)\n\nsample = pd.read_csv(\"..\/input\/wec-ml-mentorship-contest\/WEC_Contest_sample_submission.csv\", low_memory=False)\n\ntest.to_feather(\"test\")\ntrain.to_feather(\"train\")","f04d02ce":"test.shape","5aa26969":"sample.shape","b900310f":"train.head()","330fc632":"train.info()","4ab62c93":"colInfo = {\"I\"       :\"continuous\",\n\"II\"      :\"categorical\",\n\"III\"     :\"categorical\",\n\"IV\"      :\"categorical\",\n\"V\"       :\"continuous\",\n\"VI\"      :\"continuous\",\n\"VII\"     :\"continuous\",\n\"VIII\"    :\"continuous\",\n\"IX\"      :\"continuous\",\n\"X\"       :\"continuous\",\n\"XI\"      :\"boolean\",\n\"XII\"     :\"categorical\",\n\"XIII\"    :\"continuous\",\n\"XIV\"     :\"boolean\",\n\"XV\"      :\"boolean\",\n\"XVI\"     :\"categorical\",\n\"XVII\"    :\"continuous\",\n\"XVIII\"   :\"continuous\",\n\"XIX\"     :\"continuous\",\n\"XX\"      :\"continuous\"} ","f2c46057":"def feature_types():\n    '''\n    Creates lists for boolean, categorical, continuous features.\n    Prints and returns these lists.\n    '''\n    boolean = []\n    categorical = []\n    continuous = []\n\n    for col in colInfo:\n        if (colInfo[col] == \"boolean\"):\n            boolean.append(col)\n        elif (colInfo[col] == \"categorical\"):\n            categorical.append(col)\n        else:\n            continuous.append(col)\n    \n    print(\"boolean: {}\".format(boolean))\n    print(\"categorical: {}\".format(categorical))\n    print(\"continuous: {}\".format(continuous))\n\n    return boolean, categorical, continuous","3c332525":"boolean, categorical, continuous = feature_types()        ","d7498b48":"train.shape","7719745f":"test.shape","7f69377e":"def null_columns(data):\n    '''\n    returns a list of columns with NaN values\n    \n    Arguements\n    data: the dataframe that you want to check\n    \n    Returns:\n    list of columns containing null values\n    '''\n    l = [col for col in data.columns if data[col].isnull().sum() > 0]\n    \n    if not l:\n        print(\"No NaN columns!\")\n\n    return l","b2f29f96":"def plot_dist(col, train, test):\n    '''\n    Plots the distribution of train and test set.\n    With the col values sorted in ascending order\n    \n    Arguements:\n    col: takes the column name of the DataFrame\n    train: train DataFrame\n    test: test DataFrame\n    \n    Returns:\n    nothing\n    '''\n    _, axes = plt.subplots(2, 1)\n\n    plot1 = sns.countplot(x=col, data=train, ax=axes[0])\n    plot1.set_title(\"Traning Set\")\n    plot1.set_xticklabels(plot1.get_xticklabels(), rotation=60, ha=\"right\")\n\n    plot2 = sns.countplot(x=col, data=test, ax=axes[1])\n    plot2.set_title(\"Test Set\")\n    plot2.set_xticklabels(plot1.get_xticklabels(), rotation=60, ha=\"right\")\n\n    plt.tight_layout()\n    plt.show()\n    print(\"\\n\\n\\n\")\n    ","db68d2a4":"def plot_line(col, train, test):\n    '''\n    Plots the distribution of train and test set.\n    With the col values sorted in ascending order\n    \n    Arguements:\n    col: takes the column name of the DataFrame\n    train: train DataFrame\n    test: test DataFrame\n    \n    Returns:\n    nothing\n    '''\n    _, axes = plt.subplots(2, 1)\n\n#     plot1 = sns.lineplot(x=col, data=train, ax=axes[0])\n#     plot1.set_title(\"Traning Set\")\n#     plot1.set_xticklabels(plot1.get_xticklabels(), rotation=60, ha=\"right\")\n\n#     plot2 = sns.lineplot(x=col, data=test, ax=axes[1])\n#     plot2.set_title(\"Test Set\")\n#     plot2.set_xticklabels(plot1.get_xticklabels(), rotation=60, ha=\"right\")\n\n\n    plot1 = train[col].plot.hist(ax=axes[0])\n    plot1.set_title(\"Traning Set\")\n    plot1.set_xticklabels(plot1.get_xticklabels(), rotation=60, ha=\"right\")\n\n    plot2 = test[col].plot.hist(ax=axes[1])\n    plot2.set_title(\"Test Set\")\n    plot2.set_xticklabels(plot1.get_xticklabels(), rotation=60, ha=\"right\")\n    \n    plt.tight_layout()\n    plt.show()\n    print(\"\\n\\n\\n\")","6c102bec":"def split_null(data, col):\n    '''\n    splits data into null and non-null dataframes and col based on the col\n    \n    Arguements:\n        data: a pd.DataFrame\n        col: a pd.Series that has NaN values\n        \n    Return:\n        returns a nonNullData, nullData, nonNullCol, nullCol\n    \n    '''\n    nonNullData = data.loc[~col.isnull()]\n    nonNullCol = col[~col.isnull()]\n    \n    nullData = data.loc[col.isnull()]\n    nullCol = col[col.isnull()]\n    \n    return nonNullData, nullData, nonNullCol, nullCol","12c57a24":"def impute_nans(col, colNull):\n    '''\n    Returns a column with NaN values removed\n    \n    Arguments:\n        col: pd.Series whose NaN values you want to impute\n        colNull: a pd.Series whose indices corresspond to indices of NaN values\n                    and values are the impute values\n                    \n    Returns:\n        col: a pd.Series with all the NaN values imputed\n    '''\n    for i in range(colNull.shape[0]):\n        col[colNull.index[i]] = colNull.values[i]\n        print(\"col[colNull.index[i]]: {}\".format(col[colNull.index[i]]))\n    return col","39f97277":"def corr_matrix(data, threshold):\n    '''\n    Displays the correlation for DatFrame data. Prints and returns the correlations of features above threshold\n    \n    Arguements:\n        data: the DataFrame\n        threshold: the correlation thershold value\n        \n    Returns:\n        corrList: contains a list of indices and correlation value for features above the correlation threshold\n    '''\n    corr = train.corr()\n    \n    numberOfCols = len(data.columns)\n    corrList = [ [i, j, corr.iloc[i,j]] for i in range(0, numberOfCols -1) for j in range(i+1, numberOfCols) if (abs(corr.iloc[i,j]) >= threshold)]\n    \n    corrList.sort(key=lambda x: x[2], reverse=True)\n    for c in corrList:\n        print(\"{}\\tand\\t{}:\\t{:0.3f}\".format(c[0], c[1], c[2]))\n        \n    corr.index = train.columns\n    sns.heatmap(corr, annot = True, cmap='RdYlGn', vmin=-1, vmax=1)\n    plt.title(\"Correlation Heatmap\", fontsize=6)\n    plt.show()\n    \n    \n    return corrList\n    ","cd61a230":"def plot_pair(data, corrList):\n    '''\n    Plots seaborn pairplots for a given corrList\n    \n    Arguements:\n        data: a DataFrame\n        corrList: a corrList from corr_matirx()\n        \n    Returns:\n        nothing\n    '''\n    cols = data.columns\n    for i, j, _ in corrList:\n        sns.pairplot(data, size=6, x_vars=cols[i],y_vars=cols[j] )\n        plt.show()","3f8bb6f4":"def score_stats(scores):\n    print(\"scores: {}\".format(scores*100))\n    print(\"scores.mean(): {}\".format(scores.mean()*100))\n    print(\"scores.std(): {}\".format(scores.std()))\n    print(\"scores.median(): {}\".format(np.median(scores)*100)) \n    print()","366a5ed7":"def label_list(npArray):\n    labelList = []\n    for arr in npArray:\n        labelList.append(str(str(arr[0])+\" \"+str(arr[1])+\" \"+str(arr[2])))\n\n    return labelList","f04a8f29":"def write_to_file(args, fileName):\n    submission = pd.DataFrame(data={\"Id\":sample[\"Id\"].values, \"Expected\": label_list(args)} )\n    submission.to_csv(fileName, index=False)","f0bad4a0":"null_columns(train)","c9eb9b5e":"null_columns(test)","efa2a869":"print(\"VIII: {}\".format(colInfo[\"VIII\"]))\nprint(\"XVI: {}\".format(colInfo[\"XVI\"]))","34952763":"plot_dist(\"VIII\", train, test)","08c8798b":"plot_dist(\"XVI\", train, test)","77d8f804":"train[\"Label\"].value_counts().sort_values()","b3c6654a":"plot1 = sns.countplot(x=\"Label\", data=train)\nplot1.set_title(\"Traning Set\")\nplot1.set_xticklabels(plot1.get_xticklabels(), rotation=60, ha=\"right\")\n\n\nplt.tight_layout()\nplt.show()","fc90f5a9":"for col in continuous:\n    plot_dist(col, train, test)","207ea9b6":"colInfo[\"XX\"] = colInfo[\"XIII\"] = colInfo[\"XVIII\"] = colInfo[\"X\"] = \"categorical\"\n\n# to change the feature types for one hot encoding\nboolean, categorical, continuous = feature_types()","9349e20c":"train[continuous].describe()","23c22ae1":"train[continuous] = np.log1p(train[continuous])\ntest[continuous] = np.log1p(test[continuous])\ntrain.to_feather(\"train\")\ntest.to_feather(\"test\")\ntrain[continuous].describe()","73832d04":"train.head()","a57029c1":"for col in continuous:\n    plot_line(col, train, test)","45950872":"for col in categorical:\n    plot_dist(col, train, test)","9c0c30c7":"for col in boolean:\n    plot_dist(col, train=train, test=test)","f05d52a4":"train = feather.read_dataframe(\"train\")\ntest = feather.read_dataframe(\"test\")\ncombined = pd.concat([train, test], ignore_index=True)\n\ncol8 = combined[\"VIII\"]\ncol16 = combined[\"XVI\"]\npd.Series(col16).rename(columns={\"0\":\"XVI\"}, inplace=True)\ncombined.drop(columns=[\"Label\", \"VIII\", \"XVI\"], inplace=True)\n\ncombined.to_feather(\"combined\")\n\ntry:\n    categorical.remove(\"XVI\")\nexcept:\n    pass\n\ncombined = pd.get_dummies(combined, columns=categorical)\n\nscaler = StandardScaler()\ncombined[combined.columns]= scaler.fit_transform(combined)\n# df[df.columns] = scaler.fit_transform(df[df.columns])\n\ndisplay(combined.head())\nprint(\"combined.shape: {}\".format(combined.shape))","2e2596d5":"train[\"VIII\"].describe()","374878a3":"test[\"VIII\"].describe()","ed354d13":"nonNullCombined16, nullCombined16, nonNullCol16, nullCol16 = split_null(combined, col16)","6bdd1783":"skf = StratifiedKFold(n_splits=10, random_state=42)\nknn = KNeighborsClassifier(n_neighbors=1, n_jobs=-1)\nscores = cross_val_score(knn, nonNullCombined16, nonNullCol16, scoring=\"f1_micro\", cv=skf.split(nonNullCombined16, nonNullCol16))\n\nscore_stats(scores)\n\nknn.fit(nonNullCombined16, nonNullCol16)\n\ncol16Pred = knn.predict(nullCombined16)","ab2d2011":"nullCol16 = pd.concat([nullCol16, pd.Series(col16Pred, index=nullCol16.index, name=\"temp\")], axis=1, ignore_index=False)\nnullCol16.drop(columns=[\"XVI\"], inplace=True)\nnullCol16.rename(columns={\"temp\":\"XVI\"}, inplace=True)\n\ncol16 = impute_nans(col16, nullCol16)\n\ncombined = feather.read_dataframe(\"combined\")\ncombined16 = pd.concat([combined, col16], axis=1)\ndisplay(combined16.head())\n\ncombined16.to_feather(\"combined\")","7f0a9247":"categorical.append(\"XVI\")\ncategorical = list(set(categorical))","5ffbfbd3":"combined16 = pd.get_dummies(combined16, columns=categorical)\n\nscaler = StandardScaler()\ncombined16[combined16.columns]= scaler.fit_transform(combined16)\n\ncombined16.head()","b5eca8c5":"nonNullCombined8, nullCombined8, nonNullCol8, nullCol8 = split_null(combined, col8)","566ebbae":"skf = KFold(n_splits=10, random_state=42)\nknnReg = KNeighborsRegressor(n_neighbors=1, n_jobs=-1)\nscores = cross_val_score(knnReg, nonNullCombined8, nonNullCol8, scoring=\"neg_mean_squared_error\", cv=skf.split(nonNullCombined8, nonNullCol8))\n\nscore_stats(scores)\n\nknnReg.fit(nonNullCombined8, nonNullCol8)\n\ncol8Pred = knnReg.predict(nullCombined8)","06bf6642":"nullCol8 = pd.concat([nullCol8, pd.Series(col8Pred, index=nullCol8.index, name=\"temp\")], axis=1, ignore_index=False)\nnullCol8.drop(columns=[\"VIII\"], inplace=True)\nnullCol8.rename(columns={\"temp\":\"VIII\"}, inplace=True)\n\ncol8 = impute_nans(col8, nullCol8)\n\ncombined = feather.read_dataframe(\"combined\")\ncombined8 = pd.concat([combined, col8], axis=1)\ncombined8.head()","f7c54fc9":"colNames = combined8.columns.tolist()\ndel colNames[len(colNames) - 1]\n\ncolNames.append(\"VIII\")\ncolNames.append(\"XXI\")\ncolNames.append(\"XXII\")\nprint(colNames)","03555a08":"len(colNames)","d134d829":"train = feather.read_dataframe(\"train\")\ntest = feather.read_dataframe(\"test\")\ncombined = pd.concat([train, test],ignore_index=True)\n\ncol8 = combined[\"VIII\"].isnull() * 1\ncol16 = combined[\"XVI\"].isnull() * 1\ncombined8 = pd.concat([combined8, col8.rename(\"VIII_nan\"), col16.rename(\"XVI_nan\")], axis=1,ignore_index=True)","72004fd3":"combined8.head()","6bdb65bd":"combined8.shape","414b8a19":"combined8.columns = colNames","b47ea216":"combined8.to_feather(\"combinedFinal\")","28e37778":"combined8 = pd.get_dummies(data=combined8, columns=categorical)","1d3efb3f":"trainNew = combined8.iloc[:trainRows, :]\ntestNew = combined8.iloc[trainRows:, :]\n\ntrainNew.reset_index(inplace=True, drop=True)\ntestNew.reset_index(inplace=True, drop=True)\n\ntrainNew.to_feather(\"trainNew\")\ntestNew.to_feather(\"testNew\")","87ea1562":"print(trainNew.shape, testNew.shape)","265ec305":"trainNew[trainNew.columns] = scaler.fit_transform(trainNew[trainNew.columns])\ntestNew[testNew.columns] = scaler.transform(testNew[testNew.columns])","01787eda":"trainNew.head()","8fe69a23":"%%time\nskf = StratifiedKFold(n_splits=10, random_state=42)\nknn = KNeighborsClassifier(n_neighbors=1, n_jobs=-1, weights=\"uniform\")\nscores = cross_val_score(knn, trainNew, label, scoring=\"f1_macro\", cv=skf.split(trainNew, label), n_jobs=-1)\n\nscore_stats(scores)\n\nknn.fit(trainNew, label)\nknnPred = knn.predict_proba(testNew)\n# print(np.argsort(-knnPred, axis=1)[0][:3])\nknnArgs = np.argsort(-knnPred, axis=1)[:, :3]\n\n# 0.19","6400b5de":"%%time\nrf = RandomForestClassifier(n_estimators=100, min_samples_leaf=1, oob_score=True\n                            , class_weight=\"balanced\", min_samples_split=2)\nscores = cross_val_score(rf, trainNew, label, scoring=\"f1_macro\", cv=skf.split(trainNew, label), n_jobs=-1)\n\nscore_stats(scores)\n\nrf.fit(trainNew, label)\n\nprint(rf.oob_score_)\n\nrfPred = rf.predict_proba(testNew)\n# print(np.argsort(-rfPred, axis=1)[0][:3])\nrfArgs = np.argsort(-rfPred, axis=1)[:, :3]","66f70ca8":"%%time\nxgb = XGBClassifier(n_estimators=200, max_depth=5, learning_rate=1, n_jobs=2, random_state=42)\nscores = cross_val_score(xgb, trainNew, label, scoring=\"f1_macro\", cv=skf.split(trainNew, label), n_jobs=-1)\n\nscore_stats(scores)\n\nxgb.fit(trainNew, label)\n\nxgbPred = xgb.predict_proba(testNew)\n# print(np.argsort(-xgbPred, axis=1)[0][:3])\nxgbArgs = np.argsort(-xgbPred, axis=1)[:, :3]","9cc545bf":"%%time\nlogit = LogisticRegression(max_iter=1000, C=1)\nscores = cross_val_score(logit, trainNew, label, scoring=\"f1_macro\", cv=skf.split(trainNew, label), n_jobs=-1)\n\nscore_stats(scores)\n\nlogit.fit(trainNew, label)\n\nlogitPred = logit.predict_proba(testNew)\n# print(np.argsort(-logitPred, axis=1)[0][:3])\nlogitArgs = np.argsort(-logitPred, axis=1)[:, :3]","734ec36f":"knnArgs","bc4df3f7":"logitArgs","a8f2110e":"write_to_file(knnArgs, \"knn.csv\")","90ddc22c":"**Skewed classes.**","c14499bf":"### Looking at categorical variable distributions","d837a599":"None of the feature look correlated using the pariplot ","aaa72c34":"Both features VIII and XVI have missing values in the train and test sets","229895cb":"Very small number of rows in training set maybe go for a gradient model ???","b949dc50":"### Looking at categorical variable distributions","4c351709":"# Working on Imputing","91741d4c":"**Insight:** looks like if `XI == 1` then it's likely going to be `\"Label\" == 0`","f85db481":"### HOW DO  I MAKE DISTRIBUTIONS MORE NORMAL? WHY DO I WANT TO DO THAT?","5ac04771":"###  A correlation matrix that would pick out correlation > `threshold` and then print the pairplots of those features to look for actual correlation","9edb616f":"Fields **XX, XIII, XVIII, X** are categorical. Only because they have the exact same values and more or less the same distribution could be a wrong assumption :\/","1757fdb9":"### Looking at boolean variable distributions"}}