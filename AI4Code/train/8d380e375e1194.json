{"cell_type":{"a0e29dd5":"code","a3b728c3":"code","82e878c2":"code","8a7d6141":"code","519ef9e5":"code","6307e7b9":"code","b5bc52e1":"code","64a4dede":"code","bcb0615f":"code","7966bf84":"code","eecc59f5":"code","9b675ef7":"code","ba623c43":"code","a672f5f4":"code","e29d0600":"code","2bcb74de":"code","0bad3c87":"code","d5c692bb":"code","ad771779":"code","731f1612":"code","0622a808":"code","a8c81eb6":"code","1d8344c3":"code","fe643593":"code","3fa52881":"code","a317f7d8":"code","904b1c0d":"code","3782d689":"code","c4279cf9":"code","8b1c32fa":"code","1702101c":"code","dbc219d1":"code","4c61d592":"code","1a61f039":"code","5deeda16":"code","4f61e05a":"code","c65b23e2":"code","4d29acb8":"code","8569b170":"code","e5434b6d":"code","cc45774f":"code","68d8bac4":"code","5378cb98":"code","30fbc1e6":"code","52fb8c5f":"code","7f464215":"code","6209132e":"code","5ac47311":"code","e055ed50":"code","d386210a":"code","0dc7a590":"code","53a04985":"code","205220a4":"code","976eadb9":"code","0b231bf1":"code","3ca5002d":"code","8b16edc9":"code","7bc214f7":"code","c6e46c9f":"code","ac3745cc":"markdown","18f0cdc8":"markdown","180e60a0":"markdown","f4342cf8":"markdown","aa040a2c":"markdown","2944c9b9":"markdown","7a9c91a8":"markdown","9fbc7de8":"markdown","c6e6941d":"markdown","04b141af":"markdown","a497e22e":"markdown","d68df267":"markdown","68ba5d72":"markdown","de25f1f7":"markdown","47865466":"markdown","2e226f61":"markdown","504c5ad8":"markdown","687c6008":"markdown","935bf62a":"markdown","fe0fe5c6":"markdown","ba44ce82":"markdown","961e4d54":"markdown","1a5d6383":"markdown","bfedbef9":"markdown","3a2077d3":"markdown","0eb99c92":"markdown","4d2a47d7":"markdown","190cfe05":"markdown","cdd31892":"markdown","c5551689":"markdown","188421d9":"markdown","ad85e9af":"markdown","af8a09a4":"markdown","9e54bc9d":"markdown"},"source":{"a0e29dd5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a3b728c3":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import RFE\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import tree\nfrom sklearn import linear_model","82e878c2":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nprint(f'Train size: {len(train)}\\nTest Size: {len(test)}')","8a7d6141":"train_df = pd.DataFrame(train)\ntest_df = pd.DataFrame(test)","519ef9e5":"train_df.head(10)","6307e7b9":"test_df.head(5)","b5bc52e1":"keys_train = sorted(train.keys())\nkeys_test = sorted(test.keys())","64a4dede":"uncommon_key = []\nfor key in keys_train:\n    if not key in keys_test:\n        uncommon_key.append(key)\nprint(uncommon_key)","bcb0615f":"train_df.describe()","7966bf84":"# Define a function which create and plot a new DF based on 'Survived' values.\ndef plot_by_surv_died(data, based_on, stacked=True, colors_= ['purple','black']):\n    \n    # Obtain survivors and died counts\n    survive = data[data['Survived']==1][based_on].value_counts()\n    dead = data[data['Survived']==0][based_on].value_counts()\n    \n    # Place these values on a new DF\n    surv_dea_df = pd.DataFrame([survive, dead])\n    surv_dea_df.index = ['Survived','Died']\n    surv_dea_df.plot(kind='bar', stacked=stacked, color=colors_)","eecc59f5":"survive = train_df[train_df['Survived']==1]['Sex'].value_counts()\ndead = train_df[train_df['Survived']==0]['Sex'].value_counts()","9b675ef7":"plot_by_surv_died(train_df, 'Sex')","ba623c43":"print(f'---> Survive \u0295-\u1d25-\u0294 <---\\n\\n{survive}')","a672f5f4":"sns.violinplot(x='Sex', y='Age', hue='Survived', data=train_df, split=True, palette='Set1')","e29d0600":"figure = plt.figure(figsize=(15,15))\nplt.hist([train_df[train_df['Survived']==0]['Fare'],\n          train_df[train_df['Survived']==1]['Fare']],\n         stacked=True,\n         color = ['Red','Green'],\n         bins = 50,\n         label = ['Died', 'Survived']\n        )\nplt.xlabel('Fare')\nplt.grid()\nplt.ylabel('# of Passengers')\nplt.legend()","2bcb74de":"plot_by_surv_died(train_df, 'Pclass', colors_=['#FFD700','#C0C0C0','#8C7853'])\nplt.legend(['1st class','2nd class','3rd class'])","0bad3c87":"train_df.info()","d5c692bb":"print(train_df.isnull())","ad771779":"print(train_df.isnull().any())","731f1612":"def plotting_missing_data(dataset):\n    # Conf\n    fig, ax = plt.subplots(figsize=(5,5))\n    plt.title('Missing data')\n    # Hearmap without color bar\n    sns.heatmap(dataset.isnull(), cbar=False, \n                cmap=sns.color_palette(\"rocket_r\", as_cmap=True) )","0622a808":"plotting_missing_data(train_df)","a8c81eb6":"label_encoder = preprocessing.LabelEncoder()\nencoder_sex = label_encoder.fit_transform(train_df['Sex'])\ntrain_df['Sex'] = encoder_sex\ntrain_df.head(5)","1d8344c3":"label_encoder_test= preprocessing.LabelEncoder()\nencoder_sex_test = label_encoder_test.fit_transform(test_df['Sex'])\ntest_df['Sex'] = encoder_sex_test\ntest_df.head(5)","fe643593":"train['Age'] = train['Age'].fillna(train['Age'].median())\nprint(train_df.isnull().any())","3fa52881":"plotting_missing_data(train_df)","a317f7d8":"train_df['Embarked'].isnull().sum()","904b1c0d":"train_df['Embarked'] = train_df['Embarked'].fillna('S')","3782d689":"plotting_missing_data(train_df)","c4279cf9":"train_df['Cabin'] = train_df['Cabin'].fillna('U')","8b1c32fa":"plotting_missing_data(train_df)","1702101c":"print(test_df.isnull().any())","dbc219d1":"plotting_missing_data(test_df)","4c61d592":"test_df['Age'] = test_df['Age'].fillna(test_df['Age'].median())","1a61f039":"test_df['Fare'] = test_df['Fare'].fillna(test_df['Fare'].median())","5deeda16":"test_df['Cabin'] = test_df['Cabin'].fillna('U')","4f61e05a":"plotting_missing_data(test_df)","c65b23e2":"train_df","4d29acb8":"test_df","8569b170":"train_predictors = train_df.drop(['Name', 'PassengerId', 'Survived', 'Cabin', 'Ticket'], axis = 1) #Axis = 1 refers to Columns\ntest_predictors =  test_df.drop(['Name', 'PassengerId', 'Cabin', 'Ticket'], axis = 1)","e5434b6d":"categorical = [i for i in train_predictors.columns if\n               train_predictors[i].nunique() and\n               train_predictors[i].dtype == 'object']\n\n\ncategorical2 = [i for i in test_predictors.columns if\n               test_predictors[i].nunique() and\n               test_predictors[i].dtype == 'object']\ncategorical","cc45774f":"categorical2","68d8bac4":"numerical = [i for i in train_predictors.columns if\n             train_predictors[i].dtype in ['int64','float64']\n            ]\nnumerical","5378cb98":"dummy_encoded_train_predictors = pd.get_dummies(train_predictors)\ndummy_encoded_train_predictors","30fbc1e6":"dummy_encoded_test_predictors = pd.get_dummies(test_predictors)\ndummy_encoded_test_predictors","52fb8c5f":"y_target = train_df['Survived'].values","7f464215":"x_features_one = dummy_encoded_train_predictors\nx_features_one","6209132e":"x_train, x_validation, y_train, y_validation = train_test_split(x_features_one, y_target, \n                                                                 test_size = 0.25, random_state=1)","5ac47311":"tree_one = tree.DecisionTreeClassifier()","e055ed50":"tree_one = tree_one.fit(x_train, y_train)","d386210a":"tree_one_accuracy = round(tree_one.score(x_validation, y_validation), 3)\ntree_one_accuracy","0dc7a590":"from xgboost import XGBRegressor #pip install xgboost\nfrom sklearn.metrics import r2_score","53a04985":"X_train = dummy_encoded_train_predictors","205220a4":"y_train = train_df['Survived'].values","976eadb9":"X_test = dummy_encoded_test_predictors","0b231bf1":"model = tree.DecisionTreeClassifier()","3ca5002d":"model.fit(X_train, y_train)","8b16edc9":"predictions = model.predict(X_test)","7bc214f7":"len(predictions)","c6e46c9f":"output = pd.DataFrame({'PassengerId': test_df.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","ac3745cc":"We have different variables (columns) that measure distinct parameters and all of them are in different scales so, our first step would be standardized this values. To complete this task we will use `sklearn.preprocessing.StandardScaler()` class, which standardize features by removing the mean and scaling to unit variance.","18f0cdc8":"Dataset available to download on:  [kaggle: Titanic Competition Data](https:\/\/www.kaggle.com\/c\/titanic\/data) \ud83d\udc9c","180e60a0":"Then, we need to decompose our categorical variable into numeric using `.get_dummies()`, which convert categorical variable into dummy\/indicator variables.","f4342cf8":"## Removing missing values and unnecessary data \ud83d\udd28","aa040a2c":"In this part we'll separate our numerical from categorical data with the purpose of after that, evaluate and  remove unnecessary parameters.\n\nBefore that, we can obviate some parameters that we are sure that cannot affect our predictions, like name, ticket number, etc and the variable that we want to predict `Survived` to can compare later our model fit:","2944c9b9":"We can see in the `count` row that there are some missing values in our `Age` column. In fact, there are 177 missing values (891 - 714); it will be important later and we'll need to do something similar with categorical variables.","7a9c91a8":"It's interesting to see how in men case the difference of surviving probabilities had reduced in base on how old they were, more specifically, men between 20's 35's tended to survive more than older men (40's - 80's). By the other hand, in the women case they survived compararively well on all the age groups.","9fbc7de8":"## Creating model \ud83e\udde0","c6e6941d":"#### Testing with local data","04b141af":"Print which columns have null (Zero or NaN) values.","a497e22e":"#### The same process in test data:","d68df267":"Next, we'll return to check our missing data before to remove unnecessary variables.","68ba5d72":"### By: Abdiel Guerrero \ud83d\udc9c","de25f1f7":"but first of all, let's try to convert our values `male`, `female` into 0, 1.","47865466":"Then, let's do something similar with categorical data (`Cabin` and `Embarked`). We'll start with `Embarked` because there are very few non values:","2e226f61":"Now, let's try to visualize our `Survived` data grouped by gender and age:","504c5ad8":"## Processing data \u2699","687c6008":"# Exploratory Data Analysis \ud83d\udd0e","935bf62a":"In this case we have, `men` =1 and `female` = 0.","fe0fe5c6":"For the beginning, we'll check our numerical data:","ba44ce82":"Now, let's to check our data's keys (columns) and what the differences between `train_df` and `test_df` are (variable(s) to predict).","961e4d54":"---","1a5d6383":"Now it's time to clean out null values. We will start with our numerical data, we could replace them with mean values.","bfedbef9":"The last one, is our Cabin column. In this case we have a lot of missing values so, trying to not change our data, we will replace the missing cabbing with letter U referring to `Unknown`.","3a2077d3":"---","0eb99c92":"There are only 2 non values, so, we'll fill there with a 'S' value:","4d2a47d7":"---","190cfe05":"We'll start reading our .csv files and put them in DataFrames to manage to work on Pandas.","cdd31892":"We have some missing values on `Age`, `Cabin` and `Embarked`. Let's to visualize it:","c5551689":"## Exploring and plotting \ud83d\udcca","188421d9":"---","ad85e9af":"We will use class `sklearn.preprocessing.LabelEncoder` which encode target labels with value between 0 and n_classes-1; in this case, between $0$ and $1$.","af8a09a4":"Now, why not try to visualice if there ir an corelation between ticket `fare` and probabilities to survive? In this case we could imagine these probabilities are based on their position on the ship or their priority to be rescued.","9e54bc9d":"Despite of there was a very little part of expensive tickets passengers, the most part of them survived. By the other hand, more than half people who bought the cheapest tickets died. Were there a kind of priority of rescue based on their social status, or it's more kind of a statistical mean based on the quantity of passengers in each fare kind?"}}