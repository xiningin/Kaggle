{"cell_type":{"020450f4":"code","ca69687a":"code","7e1f2e18":"code","e97d440b":"code","500e9074":"code","d0ef4e2b":"code","100a153a":"code","3c8f3e6b":"code","04993294":"code","6ba1dcdf":"code","aa962f11":"code","c62a644c":"code","ae7a7f12":"code","168410df":"code","c959bca7":"code","ee8cec9d":"code","9be82904":"code","3fc5fff8":"code","c25dd405":"code","9e77c5fc":"code","6c73a893":"code","d13e57b9":"code","c67b9186":"code","727c3018":"code","2c40959d":"markdown","2207f037":"markdown","454674de":"markdown","a5f9d320":"markdown","0ec2f45d":"markdown","f577011d":"markdown","f40d7620":"markdown","14b8cf75":"markdown","bc1879eb":"markdown","31124a79":"markdown","53051f17":"markdown","504d17aa":"markdown","9d0ec484":"markdown"},"source":{"020450f4":"# let's get lazypredict\n!pip3 install -U lazypredict","ca69687a":"!pip3 install -U pandas==1.2.3 # I need this version of Pandas for now\nimport numpy as np             \nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\ntrain_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\n#train_data.head()\ntest_data.head()","7e1f2e18":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\n# load data into X and y\ny = train_data[\"Survived\"]\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = train_data[features]\n#Prepare test data\nX_submission = test_data[features]","e97d440b":"# Select columns\nnumerical_cols = [cname for cname in X.columns if X[cname].dtype in ['int64', 'float64']]\ncategorical_cols = [cname for cname in X.columns if X[cname].nunique() < 10 and \n                        X[cname].dtype == \"object\"]\n\n# Preprocessing for numerical data\nnumerical_transformer = SimpleImputer(strategy='most_frequent')\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n\n!pip3 install -U pandas==1.0.5  \nimport pandas as pd \nfrom lazypredict.Supervised import LazyClassifier \nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                                random_state=0)\nclf = LazyClassifier(verbose=0,ignore_warnings=True, custom_metric=None)\n\n!pip install -U pandas==1.2.3 #hopefully the last one\nimport pandas as pd\ntrain,test= clf.fit(X_train, X_test, y_train, y_test)\nprint(train)","500e9074":"from sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import ExtraTreesClassifier\n\npipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                             ('model', ExtraTreesClassifier(n_estimators=100,\n                                                              max_depth=8,\n                                                              random_state=0))\n                          ])\n\nscores = cross_val_score(pipeline, X, y,\n                              cv=5,\n                              scoring='accuracy')\n\nprint(\"Accuracy scores:\\n\", scores, \"\\nAn average of: \", sum(scores) \/ len(scores))","d0ef4e2b":"pipeline.fit(X,y)\npredictions = pipeline.predict(X_submission)\npd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('ExtraTrees_submission.csv', index=False)\nprint(\"ExtraTrees submission was successfully saved!\")","100a153a":"from sklearn.model_selection import cross_val_score\nfrom lightgbm import LGBMClassifier\n\npipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                             ('model', LGBMClassifier(boosting_type='goss',\n                                                          n_estimators=100,\n                                                          max_depth=5,\n                                                          random_state=0))\n                          ])\n\nscores = cross_val_score(pipeline, X, y,\n                              cv=5,\n                              scoring='accuracy')\n\nprint(\"Accuracy scores:\\n\", scores, \"\\nAn average of: \", sum(scores) \/ len(scores))","3c8f3e6b":"pipeline.fit(X,y)\npredictions = pipeline.predict(X_submission)\npd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('LGBM_submission.csv', index=False)\nprint(\"LGBM submission was successfully saved!\")","04993294":"from sklearn.ensemble import AdaBoostClassifier\npipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                             ('model', AdaBoostClassifier(n_estimators=100,\n                                                              random_state=0))\n                          ])\n\nscores = cross_val_score(pipeline, X, y,\n                              cv=5,\n                              scoring='accuracy')\n\nprint(\"Accuracy scores:\\n\", scores, \"\\nAn average of: \", sum(scores) \/ len(scores))","6ba1dcdf":"pipeline.fit(X,y)\npredictions = pipeline.predict(X_submission)\npd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('AdaBoost_submission.csv', index=False)\nprint(\"AdaBoost submission was successfully saved!\")","aa962f11":"from sklearn.linear_model import LogisticRegression\npipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                             ('model', LogisticRegression(max_iter=100,\n                                                          solver='sag',\n                                                              random_state=0))\n                          ])\n\nscores = cross_val_score(pipeline, X, y,\n                              cv=5,\n                              scoring='accuracy')\n\nprint(\"Accuracy scores:\\n\", scores, \"\\nAn average of: \", sum(scores) \/ len(scores))","c62a644c":"pipeline.fit(X,y)\npredictions = pipeline.predict(X_submission)\npd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('LR_submission.csv', index=False)\nprint(\"LR submission was successfully saved!\")","ae7a7f12":"from sklearn.svm import NuSVC\npipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                             ('model', NuSVC(\n                                                          random_state=0))\n                          ])\n\nscores = cross_val_score(pipeline, X, y,\n                              cv=5,\n                              scoring='accuracy')\n\nprint(\"Accuracy scores:\\n\", scores, \"\\nAn average of: \", sum(scores) \/ len(scores))","168410df":"pipeline.fit(X,y)\npredictions = pipeline.predict(X_submission)\npd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('NuSVC_submission.csv', index=False)\nprint(\"NuSVC submission was successfully saved!\")","c959bca7":"from sklearn.neighbors import KNeighborsClassifier\npipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                             ('model', KNeighborsClassifier(n_neighbors=5,\n                                                              leaf_size=30))\n                          ])\n\nscores = cross_val_score(pipeline, X, y,\n                              cv=5,\n                              scoring='accuracy')\n\nprint(\"Accuracy scores:\\n\", scores, \"\\nAn average of: \", sum(scores) \/ len(scores))","ee8cec9d":"pipeline.fit(X,y)\npredictions = pipeline.predict(X_submission)\npd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('KNeighborsClassifier_submission.csv', index=False)\nprint(\"KNeighborsClassifier submission was successfully saved!\")","9be82904":"from sklearn.ensemble import BaggingClassifier\npipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                             ('model', BaggingClassifier(n_estimators=10,\n                                                              random_state=0))\n                          ])\n\nscores = cross_val_score(pipeline, X, y,\n                              cv=5,\n                              scoring='accuracy')\n\nprint(\"Accuracy scores:\\n\", scores, \"\\nAn average of: \", sum(scores) \/ len(scores))","3fc5fff8":"pipeline.fit(X,y)\npredictions = pipeline.predict(X_submission)\npd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('BaggingClassifier_submission.csv', index=False)\nprint(\"BaggingClassifier submission was successfully saved!\")","c25dd405":"from xgboost import XGBClassifier\npipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                             ('model', XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n                                                           gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=10,\n                                                           min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n                                                           objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n                                                           scale_pos_weight=1, seed=0, subsample=1))\n                          ])\n\nscores = cross_val_score(pipeline, X, y,\n                              cv=5,\n                              scoring='accuracy')\n\nprint(\"Accuracy scores:\\n\", scores, \"\\nAn average of: \", sum(scores) \/ len(scores))","9e77c5fc":"pipeline.fit(X,y)\npredictions = pipeline.predict(X_submission)\npd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('XGBClassifier_submission.csv', index=False)\nprint(\"XGBClassifier submission was successfully saved!\")","6c73a893":"from sklearn.semi_supervised import LabelPropagation\npipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                             ('model', LabelPropagation(n_neighbors=7,\n                                                              max_iter=1000))\n                          ])\n\nscores = cross_val_score(pipeline, X, y,\n                              cv=5,\n                              scoring='accuracy')\n\nprint(\"Accuracy scores:\\n\", scores, \"\\nAn average of: \", sum(scores) \/ len(scores))","d13e57b9":"pipeline.fit(X,y)\npredictions = pipeline.predict(X_submission)\npd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('LabelProp_submission.csv', index=False)\nprint(\"LabelProp submission was successfully saved!\")","c67b9186":"from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\npipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                             ('model', QuadraticDiscriminantAnalysis())\n                          ])\n\nscores = cross_val_score(pipeline, X, y,\n                              cv=5,\n                              scoring='accuracy')\n\nprint(\"Accuracy scores:\\n\", scores, \"\\nAn average of: \", sum(scores) \/ len(scores))","727c3018":"pipeline.fit(X,y)\npredictions = pipeline.predict(X_submission)\npd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('QDA_submission.csv', index=False)\nprint(\"QDA submission was successfully saved!\")","2c40959d":"The features `['Pclass', 'Sex', 'SibSp', 'Parch', 'Age']` has made ExtraTreesClassifier the most accurate model for us, tied with LGBM classifier. So let's experiment with both and see which comes out on top. ","2207f037":"# Playing for a better score\n\nOur results weren't that great, so we'll instead try a shotgun approach where we make predictions with a few of the different options LazyPredict gave us that were within the 0.75-0.85 range. ","454674de":"# Titanic Survival Prediction with LazyPredict\n\nI saw a post detailing LazyPredict and how it works, and it seemed pretty simple to implement, so I tried it out in this notebook. \n\nThis notebook will first use LazyPredict to see what our options are, and then it will output ten different submissions using 9 different models that were LazyPredicted to be our top choices. We end up with a bunch of different models that we know are generally good, allowing the parameters of each to easily be tinkered with, and also feature one that was predicted to be one of the worst performers. \n\nThere's definitely a better solution to the pip statements everywhere with the different versions of dependancies required to play nice, but in keeping with the theme of LazyPredict, I'm just going to be lazy.","a5f9d320":"# LogisticRegression","0ec2f45d":"# Bagging Classifier","f577011d":"# KNeighborsClassifier","f40d7620":"# Label Propagation","14b8cf75":"# AdaBoostClassifier","bc1879eb":"# LGBMClassifier","31124a79":"# The Poor Scorer\nAccording to LazyPredict with the features `['Pclass', 'Sex', 'SibSp', 'Parch', 'Age']` selected, `QuadraticDiscriminantAnalysis` is one of the worst options we have. Let's try changing the parameters a bit to see how good we can get it. ","53051f17":"# XGBClassifier","504d17aa":"# ExtraTreesClassifier","9d0ec484":"# NuSVC"}}