{"cell_type":{"e3f5ed86":"code","6e7c99d1":"code","993a4902":"code","f6bce69d":"code","81ab87f7":"code","ca729b76":"code","cb108723":"code","080c1973":"code","ed391710":"code","105c652c":"code","efee341b":"code","98fc6ca7":"code","b8f251b8":"code","5c367fdd":"code","73ef7d93":"code","e9d9f840":"code","63af8beb":"code","cf0fff9c":"code","18928902":"code","007009ac":"code","7fd58c49":"code","8043ad24":"code","1d9fa543":"code","e89e3807":"code","4d571544":"code","7b35394f":"code","54ba7cf7":"code","4b7fef0e":"code","301000ff":"markdown","7922ab56":"markdown","2f0116ab":"markdown","1d57b7ca":"markdown","0b6ee262":"markdown","9166b873":"markdown","1f9fccad":"markdown","d11e08fe":"markdown","051ceceb":"markdown","d30f178d":"markdown","988c3fbe":"markdown","df4a3570":"markdown","97227d6c":"markdown","76c0dc8e":"markdown","bddc9995":"markdown","a72de01a":"markdown","a098b9aa":"markdown","9accae69":"markdown","352bf5fc":"markdown","94b372fc":"markdown","011a2a97":"markdown","b3308f24":"markdown","9580b1c7":"markdown","ef17c9e9":"markdown","ce84109d":"markdown","50a85632":"markdown","b9e07bcd":"markdown","5f4a29b3":"markdown","f42b5358":"markdown"},"source":{"e3f5ed86":"import numpy as np\nimport pandas as pd\nfrom keras import models, layers\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\n\nimport matplotlib.font_manager as fm\nbest_font = fm.FontProperties(fname='..\/input\/staatfont\/Staatliches-Regular.ttf')","6e7c99d1":"data =pd.read_csv(\"..\/input\/bitcoin-historical-data\/bitstampUSD_1-min_data_2012-01-01_to_2021-03-31.csv\")\ndata = data[['Timestamp','Open']]\ndata['Timestamp'] = pd.to_datetime(data['Timestamp'],unit='s').dt.date\ndata = data.dropna()\ndata = data.groupby('Timestamp').mean()\n\ndata = data.reset_index(drop=True)\ndata.columns=['price']\n\nscaler = MinMaxScaler()\ndata['price'] = scaler.fit_transform(np.array(data['price']).reshape(-1,1))\n\nX = []\ny = []\nfor i in range(len(data)-5):\n    X.append(list(data.loc[i:i+4,\"price\"]))\n    y.append(data.loc[i+5,\"price\"])\n    \nX = np.array(X)\ny = np.array(y)\nX = X.reshape(-1,5,1)\n\nmodel = models.Sequential()\nmodel.add(layers.LSTM(5,input_shape=X.shape[1:]))\nmodel.add(layers.Dense(1))\nmodel.compile(optimizer='adam',loss='MSE')\nmodel.summary()\nmodel.fit(X, y, epochs=5,verbose=2,validation_split=0.3)","993a4902":"names = [weight.name for layer in model.layers for weight in layer.weights]\nweights = model.get_weights()\n\nnp.set_printoptions(suppress=True)\nfor name, weight in zip(names, weights):\n    print(name, weight.shape)\n    print(weight)\n\n    layer_type = name.split('\/')[1]\n    if layer_type == 'kernel:0':\n        kernel_0 = weight\n    if layer_type == 'recurrent_kernel:0':\n        recurrent_kernel_0 = weight\n    elif layer_type == 'bias:0':\n        bias_0 = weight\n    print()","f6bce69d":"kernel_weights = weights[0]\nrecurrent_kernel_weights = weights[1]\nbias = weights[2]\n\nn = 1\nunits = 5  # LSTM layers\n\n# (1, 20) embedding dims, units * 4\nWi = kernel_weights[:, 0:units]\nWf = kernel_weights[:, units:2 * units]\nWc = kernel_weights[:, 2 * units:3 * units]\nWo = kernel_weights[:, 3 * units:]\n\n# (5, 20) units, units * 4\nUi = recurrent_kernel_weights[:, 0:units]\nUf = recurrent_kernel_weights[:, units:2 * units]\nUc = recurrent_kernel_weights[:, 2 * units:3 * units]\nUo = recurrent_kernel_weights[:, 3 * units:]\n\n# (20,) units * 4\nbi = bias[0:units]\nbf = bias[units:2 * units]\nbc = bias[2 * units:3 * units]\nbo = bias[3 * units:]\n\ndef sigmoid(x):\n    return 1 \/ (1 + np.exp(-x))","81ab87f7":"print('Wf: ',Wf,'\\nWi: ',Wi,'\\nWo: ',Wo,'\\nWc: ',Wc,)\n\nprint('\\nUf: ',Uf,'\\nUi: ',Ui,'\\nUo: ',Uo,'\\nUc: ',Uc,)\n\nprint('\\nbf: ',bf,'\\nbi: ',bi,'\\nbo: ',bo,'\\nbc: ',bc,)","ca729b76":"ht_1 = np.zeros(n * units).reshape(n, units)\nCt_1 = np.zeros(n * units).reshape(n, units)\n\nresults = []\nfor t in range(0, len(X[2000,:])):\n    xt = np.array(X[2000,t])\n    ft = sigmoid(np.dot(xt, Wf) + np.dot(ht_1, Uf) + bf)  # forget gate\n    it = sigmoid(np.dot(xt, Wi) + np.dot(ht_1, Ui) + bi)  # input gate\n    ot = sigmoid(np.dot(xt, Wo) + np.dot(ht_1, Uo) + bo)  # output gate\n    Ct = ft * Ct_1 + it * np.tanh(np.dot(xt, Wc) + np.dot(ht_1, Uc) + bc)\n    ht = ot * np.tanh(Ct)\n\n    ht_1 = ht  # hidden state, previous memory state\n    Ct_1 = Ct  # cell state, previous carry state\n\n    results.append(ht)\n    print(t,': ht', ht)","cb108723":"model.predict(X[2000:2001])","080c1973":"ht_1 = np.zeros(n * units).reshape(n, units)\nCt_1 = np.zeros(n * units).reshape(n, units)\n\nh_t_value = []\n\ninfluence_h_t_value = []\nfor t in range(0, len(X[1000,:])):\n    xt = np.array(X[1000,t])\n    ft = sigmoid(np.dot(xt, Wf) + np.dot(ht_1, Uf) + bf)  # forget gate\n    influence_ft = (np.dot(ht_1, Uf))\/(np.dot(xt, Wf) + np.dot(ht_1, Uf) + bf) * ft\n\n    it = sigmoid(np.dot(xt, Wi) + np.dot(ht_1, Ui) + bi)  # input gate\n    influence_it = (np.dot(ht_1, Ui))\/(np.dot(xt, Wi) + np.dot(ht_1, Ui) + bi) * it\n\n    ot = sigmoid(np.dot(xt, Wo) + np.dot(ht_1, Uo) + bo)  # output gate\n    influence_ot = np.dot(ht_1, Uo) \/ (np.dot(xt, Wo) + np.dot(ht_1, Uo) + bo) * ot\n\n    gt =  np.tanh(np.dot(xt, Wc) + np.dot(ht_1, Uc) + bc)\n    influence_gt =np.dot(ht_1, Uc) \/ (np.dot(xt, Wc) + np.dot(ht_1, Uc) + bc) * gt\n    \n    Ct = ft * Ct_1 + it * gt\n    influence_ct = influence_ft * Ct_1 + influence_it * influence_gt\n    ht = ot * np.tanh(Ct)\n    influence_ht = influence_ot * (influence_ct\/Ct) * ht\n    \n    influence_h_t_value.append(influence_ht)\n\n    ht_1 = ht  # hidden state, previous memory state\n    Ct_1 = Ct  # cell state, previous carry state\n    \n    h_t_value.append(ht)\n    \ninfluence_h_t_value.append(h_t_value[-1])\nfor i in range(len(influence_h_t_value)-1,0,-1):\n    influence_h_t_value[i] = influence_h_t_value[i] - influence_h_t_value[i-1]\n    \ninfluence_h_t_value = influence_h_t_value[1:]","ed391710":"impact_columns = np.dot(influence_h_t_value,weights[3]) + (weights[4]\/5)\n\nfor i in range(len(impact_columns)):\n    print('columns_number : ',i, 'impact value : ', float(impact_columns[i]))\n    \nprint('\\nSum of value : ', float(sum(np.dot(influence_h_t_value,weights[3]) + (weights[4]\/5))))\n\nprint('\\nkeras model_predict : ', float(model.predict(X[1000:1001])))","105c652c":"data = pd.read_csv('..\/input\/womens-ecommerce-clothing-reviews\/Womens Clothing E-Commerce Reviews.csv')\ndata = data[['Review Text','Rating']]\ndata = data.dropna()\npositive = data[(data['Rating'] == 5)].sample(2370,random_state=100)\nnegative = data[(data['Rating'] == 2) | (data['Rating'] == 1)]\ndata = pd.concat([negative,positive])\ndata = data.reset_index(drop=True)\n\ndata['Rating'] = data['Rating'].apply(lambda x : 1 if x==5 else 0)","efee341b":"data.head(5)","98fc6ca7":"data.tail(5)","b8f251b8":"import re\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\n\ndef data_processing(text):\n    return_arr = []\n    text = re.sub(r\"[^a-zA-Z]\",\" \",text)\n    text = re.sub(r\" {2,}\",\" \",text)\n    text = text.lower()\n    words = word_tokenize(text)\n    s = PorterStemmer()\n    stopword = stopwords.words('english')\n    for t in words:\n        if t not in stopword:\n            return_arr.append(t)\n            \n    return_arr = [s.stem(w) for w in return_arr]\n    return return_arr\n\ndata['processing'] = data['Review Text'].apply(data_processing)","5c367fdd":"len_arr = []\nfor i in range(len(data)):\n    len_arr.append(len(data.loc[i,'processing']))\n    \nimport seaborn as sns\nsns.histplot(len_arr)","73ef7d93":"def data_processing2(text):\n    return_arr = []\n    text = re.sub(r\"[^a-z]\",\" \",text)\n    text = re.sub(r\" {2,}\",\" \",text)\n    return text\n\nnegative_sentences = data[data['Rating'] ==0]['processing']\npositive_sentences = data[data['Rating'] ==1]['processing']\n\nnegative_sentences = str(list(negative_sentences))\nnegative_sentences = data_processing2(negative_sentences)\n\nnegative_sentences = negative_sentences.split(' ')\nnegative_sentences = pd.DataFrame(negative_sentences)[0].value_counts()\nnegative_sentences = pd.DataFrame(negative_sentences)\n\npositive_sentences = str(list(positive_sentences))\npositive_sentences = data_processing2(positive_sentences)\n\npositive_sentences = positive_sentences.split(' ')\npositive_sentences = pd.DataFrame(positive_sentences)[0].value_counts()\npositive_sentences = pd.DataFrame(positive_sentences)\n\ncon = pd.concat([negative_sentences,positive_sentences],axis=1)\n\ncon.columns = ['negative','positive']\n\ncon = con.dropna()\n\ncon['negative_value'] = con[['negative','positive']].apply(lambda x : x[0]\/(x[0]+x[1]) *-1,axis=1)\ncon['positive_value'] = con[['negative','positive']].apply(lambda x : x[1]\/(x[0]+x[1]),axis=1)\n\ncon['total_value'] = con['negative_value'] + con['positive_value']\ncon = con.reset_index()\ncon = con.drop(['negative','positive','negative_value','positive_value'],axis=1)\nword_index =con['index'].to_list()\ncon = np.array(con)","e9d9f840":"con[:10]","63af8beb":"from tqdm import tqdm\n\nX = []\nfor sen in tqdm(data['processing']):\n    word_arr =[]\n    for word in sen:\n        if word in word_index:\n            word_arr.append(float(con[con[:,0] ==word,1]))\n        else:\n            word_arr.append(0)\n    X.append(word_arr)","cf0fff9c":"X = pd.DataFrame(X)\nX = X.fillna(0)\nX = X.loc[:,:50]\nX= X.to_numpy()\ny = np.array(data['Rating'])\nX = X.reshape(4740,51,1)\nX.shape, y.shape","18928902":"from tensorflow.keras.layers import Embedding, Dense,LSTM\nfrom tensorflow.keras.models import Sequential\n\nmodel = Sequential()\nmodel.add(LSTM(51,input_shape=X.shape[1:],activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\nmodel.summary()","007009ac":"model.fit(X,y, batch_size=128, epochs=15, validation_split=0.3)","7fd58c49":"names = [weight.name for layer in model.layers for weight in layer.weights]\nweights = model.get_weights()\n\nkernel_weights = weights[0]\nrecurrent_kernel_weights = weights[1]\nbias = weights[2]\n\nn = 1\nunits = 51  # LSTM layers\n\nWi = kernel_weights[:, 0:units]\nWf = kernel_weights[:, units:2 * units]\nWc = kernel_weights[:, 2 * units:3 * units]\nWo = kernel_weights[:, 3 * units:]\n\nUi = recurrent_kernel_weights[:, 0:units]\nUf = recurrent_kernel_weights[:, units:2 * units]\nUc = recurrent_kernel_weights[:, 2 * units:3 * units]\nUo = recurrent_kernel_weights[:, 3 * units:]\n\nbi = bias[0:units]\nbf = bias[units:2 * units]\nbc = bias[2 * units:3 * units]\nbo = bias[3 * units:]","8043ad24":"def make_plot(number):\n    ht_1 = np.zeros(n * units).reshape(n, units)\n    Ct_1 = np.zeros(n * units).reshape(n, units)\n\n    h_t_value = []\n\n    influence_h_t_value = []\n    for t in range(0, len(X[number,:])):\n        xt = np.array(X[number,t])\n        ft = sigmoid(np.dot(xt, Wf) + np.dot(ht_1, Uf) + bf)  # forget gate\n        influence_ft = (np.dot(ht_1, Uf))\/(np.dot(xt, Wf) + np.dot(ht_1, Uf) + bf) * ft\n\n        it = sigmoid(np.dot(xt, Wi) + np.dot(ht_1, Ui) + bi)  # input gate\n        influence_it = (np.dot(ht_1, Ui))\/(np.dot(xt, Wi) + np.dot(ht_1, Ui) + bi) * it\n\n        ot = sigmoid(np.dot(xt, Wo) + np.dot(ht_1, Uo) + bo)  # output gate\n        influence_ot = np.dot(ht_1, Uo) \/ (np.dot(xt, Wo) + np.dot(ht_1, Uo) + bo) * ot\n\n        gt =  np.tanh(np.dot(xt, Wc) + np.dot(ht_1, Uc) + bc)\n        influence_gt =np.dot(ht_1, Uc) \/ (np.dot(xt, Wc) + np.dot(ht_1, Uc) + bc) * gt\n\n        Ct = ft * Ct_1 + it * gt\n        influence_ct = influence_ft * Ct_1 + influence_it * influence_gt\n        ht = ot * np.tanh(Ct)\n        influence_ht = influence_ot * (influence_ct\/Ct) * ht\n\n        influence_h_t_value.append(influence_ht)\n\n        ht_1 = ht  # hidden state, previous memory state\n        Ct_1 = Ct  # cell state, previous carry state\n\n        h_t_value.append(ht)\n\n    influence_h_t_value.append(h_t_value[-1])\n    for i in range(len(influence_h_t_value)-1,0,-1):\n        influence_h_t_value[i] = influence_h_t_value[i] - influence_h_t_value[i-1]\n\n    influence_h_t_value = influence_h_t_value[1:]\n\n    impact_columns = np.dot(influence_h_t_value,weights[3]) + (weights[4]\/units)\n\n    if model.predict(X[number:number+1]) > 0.5:\n        b_color = 'lightgreen'\n    else:\n        b_color ='lightcyan'\n\n    fig = plt.figure(figsize=(15,3),facecolor=b_color)\n\n    for k in range(len(data.loc[number,'processing'])):\n        s = data.loc[number,'processing'][k]\n        va = round(float(impact_columns[k]),2)\n        if va > 0.5:\n            color ='green'\n        elif va< -0.3:\n            color ='blue'\n        else:\n            color ='black'\n\n        if k < 17:\n            plt.text(s=s, x=k*0.7, y=0,font=best_font,fontsize=20,color=color,va='center',ha='center')\n            plt.text(s=va,x=k*0.7, y=-0.1,font=best_font,fontsize=20,color=color,va='center',ha='center')\n        elif k < 34:\n            plt.text(s=s, x=k*0.7 - 17*0.7, y=-0.2,font=best_font,fontsize=20,color=color,va='center',ha='center')\n            plt.text(s=va,x=k*0.7- 17*0.7, y=-0.3,font=best_font,fontsize=20,color=color,va='center',ha='center')\n        else:\n            plt.text(s=s, x=k*0.7 - 34*0.7, y=-0.4,font=best_font,fontsize=20,color=color,va='center',ha='center')\n            plt.text(s=va,x=k*0.7- 34*0.7, y=-0.5,font=best_font,fontsize=20,color=color,va='center',ha='center')\n\n    plt.xlim(0,10)\n    plt.ylim(-0.5,0.1)\n    plt.axis('off')\n    plt.show()","1d9fa543":"make_plot(489)","e89e3807":"make_plot(2243)","4d571544":"make_plot(2378)","7b35394f":"make_plot(2628)","54ba7cf7":"make_plot(4560)","4b7fef0e":"make_plot(4339)","301000ff":"### If influence value is more than 0.5 : Green color\n### If influence value is lower than -0.5 : blue color\n\n### The background color mean too","7922ab56":"![](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/2db2cba6a0d878e13932fa27ce6f3fb71ad99cf1)","2f0116ab":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:blue;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n<h1 style=\"text-align: center;\n           padding: 10px;\n              color:white\">\nTrying on NLP ! ( Review Data )\n<\/h1>\n<\/div>","1d57b7ca":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:blue;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n<h1 style=\"text-align: center;\n           padding: 10px;\n              color:white\">\nLooking LSTM Detailly !\n<\/h1>\n<\/div>","0b6ee262":"### Using weights, We can know Cell_state values(Ct) and Hidden_state(ht) values","9166b873":"### I want to know that Which columns have the biggest impact.\n### So, I calculate this LSTM process","1f9fccad":"### I used Womens Clothing E-Commerce Reviews dataset","d11e08fe":"### I just try that on NLP","051ceceb":"### reference\n* http:\/\/docs.likejazz.com\/lstm\/\n* http:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/","d30f178d":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:blue;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n<h1 style=\"text-align: center;\n           padding: 10px;\n              color:white\">\nData processing and make simple model\n<\/h1>\n<\/div>","988c3fbe":"### Just use simple LSTM Model","df4a3570":"### ( I don't consider activation function like sigmoid )","97227d6c":"### And, I just use, Rate of Negative words, and Positive word \n\n### It mean\n\n### Negative Rate : -1 * Negatvie Counts \/ (Negative Counts + Positive Counts)\n### Positive Rate : 1 * Positive Counts \/ (Negative Counts + Positive Counts)\n\n### After Sum Two : Negative Rate + Positive Rate","76c0dc8e":"### I use bitcoin price dataset.\n### LSTM Showing great accuracy in Time series predict!","bddc9995":"### Using this calculation, We can get same values with model's predict\n### last hidden_state value(ht[4]) == model.predict value","a72de01a":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:blue;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n<h1 style=\"text-align: center;\n           padding: 10px;\n              color:white\">\nImport Libraries and Load Datasets\n<\/h1>\n<\/div>","a098b9aa":"![](https:\/\/www.oreilly.com\/library\/view\/neural-networks-and\/9781492037354\/assets\/mlst_1413.png)","9accae69":"### Rating mean that 0 is Negative review, and 1 is Positive review","352bf5fc":"![](https:\/\/www.csail.mit.edu\/sites\/default\/files\/2020-08\/FedTech-DeepLearning.jpg)","94b372fc":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:blue;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n<h1 style=\"text-align: center;\n           padding: 10px;\n              color:white\">\nWhich column will have the biggest impact?\n<\/h1>\n<\/div>","011a2a97":"### Then, Why am i obsessed with the impact?","b3308f24":"### This image help to understand LSTM's contruction","9580b1c7":"### I expect if you use word2vec or FastText. etc Can get more meaningful accuracy.\n### If you have Good idea, feel free to review comments plz.\n### I always welcom feedback","ef17c9e9":"### I use only Price data and Windowing size is 5\n### I just make simple model.\n### If you want more great performance, Try increase Windowing size and NN","ce84109d":"### I only use alphabet text, (delete special character or number)\n### And, delete stopwords, changing analogous term.","50a85632":"### The LSTM Model has 3 weights\n### 1. kernel\n### 2. recurrent_kernel\n### 3. bias","b9e07bcd":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:blue;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n<h1 style=\"text-align: center;\n           padding: 10px;\n              color:white\">\nLook at model's Weights\n<\/h1>\n<\/div>","5f4a29b3":"### I've used three days to do this. \ud83d\ude34","f42b5358":"### This time, I try Looking LSTM Detailly ( Especially Model's Weights )! "}}