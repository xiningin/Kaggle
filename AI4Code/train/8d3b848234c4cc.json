{"cell_type":{"dc7c8efa":"code","e5eb9517":"code","d0cfe1eb":"code","85c2da6c":"code","4a918bd5":"code","8c3b5119":"code","74fada83":"code","1551c211":"code","fea4e65f":"code","cc9c6c97":"code","28b06d38":"code","1b9c326c":"code","3750e898":"code","9d06260b":"code","023b21b0":"code","e4ee9d7b":"code","c0e9b7e3":"code","a42ac42e":"code","f2e3d445":"code","aeef7e48":"code","8e618f0c":"code","ae9b149f":"code","1bdab8dd":"code","c29487e2":"code","23350b08":"code","46057995":"code","1b419ebe":"code","8299c860":"code","086148f9":"code","55df33bc":"code","59ff9137":"code","dd4c1426":"code","ccb68bf9":"code","b1389fd6":"code","5a40b800":"code","3800faf9":"code","6c75bc05":"markdown","2212301a":"markdown","7b8e1315":"markdown","b014b744":"markdown","b6b59abb":"markdown","6853c6fd":"markdown","cdf4ae07":"markdown","1aec7cdd":"markdown","b5f2d123":"markdown","f8b2c7e2":"markdown","6287826f":"markdown","e578dfcf":"markdown","8ced69b1":"markdown","b623c444":"markdown","2e44257f":"markdown","313cec50":"markdown","3eb56b8f":"markdown","a57ac3d1":"markdown","163940ce":"markdown","f3bc631a":"markdown","8c71c08e":"markdown","49fa280b":"markdown","fcfe9c6d":"markdown","33c56a5a":"markdown","6ef5bbf7":"markdown","2a560879":"markdown","eef6b65a":"markdown"},"source":{"dc7c8efa":"# Import necessary libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt   # plotting\nimport seaborn as sns   # plotting heatmap\n\n%matplotlib inline","e5eb9517":"# Import data, convert string dates to 'datetime64' and set the date column as index:\ndf = pd.read_csv('..\/input\/test_task_data.csv',\n                 parse_dates=['date'],\n                 infer_datetime_format=True,\n                 index_col='date',\n                 thousands=',',\n                 decimal='.'\n                )","d0cfe1eb":"#  Review the general info on data, paying attention to missing values and dtypes\ndf.info()","85c2da6c":"# Let's remove the empty column and look at some examples of data:\ndf = df.drop(columns='Unnamed: 17')\nprint(f'data shape = {df.shape}')\ndf.head()","4a918bd5":"# It appears that 'feature_5' has missing values up to 2012-10-18\n# let's fill them backwards\ndf.feature_5 = df.feature_5.fillna(method='bfill')","8c3b5119":"# Basic statistics of the data:\ndf.describe()","74fada83":"# Plot the time series\nplt.style.use('fivethirtyeight')\ndf.plot(subplots=True,\n        layout=(6, 3),\n        figsize=(22,22),\n        fontsize=10, \n        linewidth=2,\n        sharex=False,\n        title='Visualization of the original Time Series')\nplt.show()","1551c211":"# Let's also draw a heatmap visualization of the correlation matrix\ncorr_matrix = df.corr(method='spearman')\nf, ax = plt.subplots(figsize=(16,8))\nsns.heatmap(corr_matrix, annot=True, fmt='.2f', linewidth=0.4,\n            annot_kws={\"size\": 10}, cmap='coolwarm', ax=ax)\nplt.xticks(fontsize=10)\nplt.yticks(fontsize=10)\nplt.show()","fea4e65f":"# We want to keep original time series for the EDA in APPENDIX\n# So we apply the 'pc_change()' transformation to a copy of the original time series\ndf_transform = df.loc[:,:'target_value'].copy().pct_change(1)\ndf_transform.head()","cc9c6c97":"# fill the 1st row with NA data\ndf_transform.fillna(method='bfill', inplace=True)\ndf_transform.head()","28b06d38":"# Plot the transformed time series\nplt.style.use('fivethirtyeight')\ndf_transform.plot(subplots=True,\n                  layout=(6, 3),\n                  figsize=(24,24),\n                  fontsize=10, \n                  linewidth=2, \n                  title='Visualization of the transformed Features')\nplt.show()","1b9c326c":"# Split train and test data\ntrain_features = df_transform.loc['2012-01-02':'2016-12-31']\ntrain_labels = df.loc['2012-01-02':'2016-12-31', 'target_class']\n\ntest_features = df_transform.loc['2017-01-02':'2018-06-19']\ntest_labels = df.loc['2017-01-02':'2018-06-19', 'target_class']\n\n# I want to use a T-days window of input data for predicting target_class\n# It means I need to prepend (T-1) last train records to the 1st test window\nT = 45  # my choice of the timesteps window\n\nprepend_features = train_features.iloc[-(T-1):]\ntest_features = pd.concat([prepend_features, test_features], axis=0)\n\ntrain_features.shape, train_labels.shape, test_features.shape, test_labels.shape","3750e898":"# Rescale the features\nfrom sklearn.preprocessing import StandardScaler  # MinMaxScaler\n\nscaler = StandardScaler()  # MinMaxScaler(feature_range=(-1,1))\nscaled_train_features = pd.DataFrame(scaler.fit_transform(train_features.values),\n                                     index=train_features.index,\n                                     columns=train_features.columns)\n# The Scaler is fit on the training set and then applied to the test set\nscaled_test_features = pd.DataFrame(scaler.transform(test_features.values),\n                                    index=test_features.index,\n                                    columns=test_features.columns)\n\nscaled_train_features.shape, scaled_test_features.shape","9d06260b":"# Plot the rescaled_train_features\nplt.style.use('fivethirtyeight')\nscaled_train_features.plot(subplots=True,\n                           layout=(6, 3),\n                           figsize=(24,24),\n                           fontsize=10, \n                           linewidth=2, \n                           title='Visualization of the scaled Train Features')\nplt.show()","023b21b0":"# Create sequences of T timesteps\nX_train, y_train = [], []\nfor i in range(train_labels.shape[0] - (T-1)):\n    X_train.append(scaled_train_features.iloc[i:i+T].values)\n    y_train.append(train_labels.iloc[i + (T-1)])\nX_train, y_train = np.array(X_train), np.array(y_train).reshape(-1,1)\nprint(f'Train data dimensions: {X_train.shape}, {y_train.shape}')\n\nX_test, y_test = [], []\nfor i in range(test_labels.shape[0]):\n    X_test.append(scaled_test_features.iloc[i:i+T].values)\n    y_test.append(test_labels.iloc[i])\nX_test, y_test = np.array(X_test), np.array(y_test).reshape(-1,1)  \n\nprint(f'Test data dimensions: {X_test.shape}, {y_test.shape}')","e4ee9d7b":"# Import Keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, BatchNormalization\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom keras.regularizers import l2\nfrom time import time","c0e9b7e3":"# Let's make a list of CONSTANTS for modelling:\nLAYERS = [8, 8, 8, 1]                # number of units in hidden and output layers\nM_TRAIN = X_train.shape[0]           # number of training examples (2D)\nM_TEST = X_test.shape[0]             # number of test examples (2D),full=X_test.shape[0]\nN = X_train.shape[2]                 # number of features\nBATCH = M_TRAIN                          # batch size\nEPOCH = 50                           # number of epochs\nLR = 5e-2                            # learning rate of the gradient descent\nLAMBD = 3e-2                         # lambda in L2 regularizaion\nDP = 0.0                             # dropout rate\nRDP = 0.0                            # recurrent dropout rate\nprint(f'layers={LAYERS}, train_examples={M_TRAIN}, test_examples={M_TEST}')\nprint(f'batch = {BATCH}, timesteps = {T}, features = {N}, epochs = {EPOCH}')\nprint(f'lr = {LR}, lambda = {LAMBD}, dropout = {DP}, recurr_dropout = {RDP}')\n\n# Build the Model\nmodel = Sequential()\nmodel.add(LSTM(input_shape=(T, N), units=LAYERS[0],\n               activation='tanh', recurrent_activation='hard_sigmoid',\n               kernel_regularizer=l2(LAMBD), recurrent_regularizer=l2(LAMBD),\n               dropout=DP, recurrent_dropout=RDP,\n               return_sequences=True, return_state=False,\n               stateful=False, unroll=False\n              ))\nmodel.add(BatchNormalization())\nmodel.add(LSTM(units=LAYERS[1],\n               activation='tanh', recurrent_activation='hard_sigmoid',\n               kernel_regularizer=l2(LAMBD), recurrent_regularizer=l2(LAMBD),\n               dropout=DP, recurrent_dropout=RDP,\n               return_sequences=True, return_state=False,\n               stateful=False, unroll=False\n              ))\nmodel.add(BatchNormalization())\nmodel.add(LSTM(units=LAYERS[2],\n               activation='tanh', recurrent_activation='hard_sigmoid',\n               kernel_regularizer=l2(LAMBD), recurrent_regularizer=l2(LAMBD),\n               dropout=DP, recurrent_dropout=RDP,\n               return_sequences=False, return_state=False,\n               stateful=False, unroll=False\n              ))\nmodel.add(BatchNormalization())\nmodel.add(Dense(units=LAYERS[3], activation='sigmoid'))\n\n# Compile the model with Adam optimizer\nmodel.compile(loss='binary_crossentropy',\n              metrics=['accuracy'],\n              optimizer=Adam(lr=LR))\nprint(model.summary())\n\n# Define a learning rate decay method:\nlr_decay = ReduceLROnPlateau(monitor='loss', \n                             patience=1, verbose=0, \n                             factor=0.5, min_lr=1e-8)\n# Define Early Stopping:\nearly_stop = EarlyStopping(monitor='val_acc', min_delta=0, \n                           patience=30, verbose=1, mode='auto',\n                           baseline=0, restore_best_weights=True)\n# Train the model. \n# The dataset is small for NN - let's use test_data for validation\nstart = time()\nHistory = model.fit(X_train, y_train,\n                    epochs=EPOCH,\n                    batch_size=BATCH,\n                    validation_split=0.0,\n                    validation_data=(X_test[:M_TEST], y_test[:M_TEST]),\n                    shuffle=True,verbose=0,\n                    callbacks=[lr_decay, early_stop])\nprint('-'*65)\nprint(f'Training was completed in {time() - start:.2f} secs')\nprint('-'*65)\n# Evaluate the model:\ntrain_loss, train_acc = model.evaluate(X_train, y_train,\n                                       batch_size=M_TRAIN, verbose=0)\ntest_loss, test_acc = model.evaluate(X_test[:M_TEST], y_test[:M_TEST],\n                                     batch_size=M_TEST, verbose=0)\nprint('-'*65)\nprint(f'train accuracy = {round(train_acc * 100, 4)}%')\nprint(f'test accuracy = {round(test_acc * 100, 4)}%')\nprint(f'test error = {round((1 - test_acc) * M_TEST)} out of {M_TEST} examples')\n\n# Plot the loss and accuracy curves over epochs:\nfig, axs = plt.subplots(nrows=1, ncols=2, figsize=(18,6))\naxs[0].plot(History.history['loss'], color='b', label='Training loss')\naxs[0].plot(History.history['val_loss'], color='r', label='Validation loss')\naxs[0].set_title(\"Loss curves\")\naxs[0].legend(loc='best', shadow=True)\naxs[1].plot(History.history['acc'], color='b', label='Training accuracy')\naxs[1].plot(History.history['val_acc'], color='r', label='Validation accuracy')\naxs[1].set_title(\"Accuracy curves\")\naxs[1].legend(loc='best', shadow=True)\nplt.show()","a42ac42e":"y_hat = model.predict_classes(X_test, batch_size=M_TEST, verbose=1)\n#score = sum(y_hat == y_test) \/ len(y_test)\n#print(f'Prediction accuracy = {score*100}%')\nindex = pd.date_range(start='2017-01-02', end='2018-06-19', freq='B')\nfor i in range(y_hat.shape[0]):\n    print(index[i], y_hat[i], y_test[i])","f2e3d445":"# Let's define a EDA function for repeated calls on individual time series:\n\nimport statsmodels.api as sm  # seasonal trend decomposition\nfrom statsmodels.graphics import tsaplots   # autocorrelation\n\ndef eda(df_name, ts_name, decomp_model='additive'):\n    \"\"\" \n    Inputs: df_name - name of the dataframe\n            ts_name - name of the time series in the dataframe\n            decomp_model - 'additive'\/'multiplicative'\n    Outputs: EDA statistics and plots for individual time series in df_name\n    \"\"\"\n    # Statistics\n    print(f'Statistic of {ts_name} time series')\n    print(df_name[ts_name].describe())\n    \n    # Plotting\n    fig, axs = plt.subplots(nrows=4, ncols=2, figsize=(24,24))\n    fig.suptitle(f'Visualization of the \"{ts_name}\" time series', fontsize=24)\n        \n    # Observed values of the time series against target_class values\n    df_name[ts_name].plot(ylim=[df_name[ts_name].min(), df_name[ts_name].max()],\n                          linewidth=2, ax=axs[0,0])\n    axs[0,0].set_title('Observed values (red\/green where target_class=0\/1)')\n    axs[0,0].set_xlabel('')\n    axs[0,0].fill_between(df_name.index, df_name[ts_name], \n                          where=(df_name.target_class==0),\n                          facecolor='red', alpha=0.5)\n    axs[0,0].fill_between(df_name.index, df_name[ts_name], \n                          where=(df_name.target_class==1),\n                          facecolor='green', alpha=0.5)\n    axs[0,0].axvline('2017-01-01', color='red', linestyle='dashed')\n    \n    # Seasonality, trend and noise in time series data\n    decomp = sm.tsa.seasonal_decompose(df_name[ts_name],\n                                       model=decomp_model)\n    decomp.trend.plot(linewidth=2, ax=axs[0,1])\n    axs[0,1].set_title('Trend values')\n    axs[0,1].set_xlabel('')\n    decomp.seasonal.plot(linewidth=2, ax=axs[1,0])\n    axs[1,0].set_title('Seasonal values')\n    axs[1,0].set_xlabel('')\n    decomp.resid.plot(linewidth=2, ax=axs[1,1])\n    axs[1,1].set_title('Residual values')\n    axs[1,1].set_xlabel('')\n    \n    # Distribution of values of time series\n    df_name[ts_name].plot.hist(bins=30, ax=axs[2,0])\n    axs[2, 0].set_title('Histogram')\n    df_name[[ts_name]].boxplot(ax=axs[2,1])\n    axs[2, 1].set_title('Boxplot')\n        \n    # Autocorrelation of time series\n    tsaplots.plot_acf(df_name[ts_name], lags=40, ax=axs[3,0])\n    tsaplots.plot_pacf(df_name[ts_name], lags=40, ax=axs[3,1])\n    \n    plt.tight_layout(rect=[0, 0, 1, 0.95])\n    plt.show()","aeef7e48":"# Call EDA function to explore the time series\neda(df, 'target_value', 'multiplicative')","8e618f0c":"# Call EDA function to explore the time series\neda(df, 'feature_1', 'multiplicative')","ae9b149f":"# Call EDA function to explore the time series\neda(df, 'feature_2', 'multiplicative')","1bdab8dd":"# Call EDA function to explore the time series\neda(df, 'feature_3', 'multiplicative')","c29487e2":"# Call EDA function to explore the time series\neda(df, 'feature_4', 'multiplicative')","23350b08":"# Call EDA function to explore the time series\neda(df, 'feature_5')","46057995":"# Call EDA function to explore the time series\neda(df, 'feature_6')","1b419ebe":"# Call EDA function to explore the time series\neda(df, 'feature_7')","8299c860":"# Call EDA function to explore the time series\neda(df, 'feature_8')","086148f9":"# Call EDA function to explore the time series\neda(df, 'feature_9')","55df33bc":"# Call EDA function to explore the time series\neda(df, 'feature_10')","59ff9137":"# Call EDA function to explore the time series\neda(df, 'feature_11')","dd4c1426":"# Call EDA function to explore the time series\neda(df, 'feature_12')","ccb68bf9":"# Call EDA function to explore the time series\neda(df, 'feature_13')","b1389fd6":"# Call EDA function to explore the time series\neda(df, 'feature_14')","5a40b800":"# Call EDA function to explore the time series\neda(df, 'feature_15')","3800faf9":"# Call EDA function to explore the time series\neda(df, 'feature_16')","6c75bc05":"# <a name=\"5\"><\/a> 5. APPENDIX - Exploratory Data Analysis for Individual Time Series\n### <a name=\"index\"><\/a> INDEX:\n\nReference | Reference | Reference\n:-- | :-- | :--\n[EDA Function](#eda_function) | [A-5. feature_5](#feature_5) | [A-11. feature_11](#feature_11) \n[A-0. target_value](#target_value) | [A-6. feature_6](#feature_6)  | [A-12. feature_12](#feature_12) \n[A-1. feature_1](#feature_1) | [A-7. feature_7](#feature_7) | [A-13. feature_13](#feature_13) \n[A-2. feature_2](#feature_2) | [A-8. feature_8](#feature_8) | [A-14. feature_14](#feature_14) \n[A-3. feature_3](#feature_3) | [A-9. feature_9](#feature_9) | [A-15. feature_15](#feature_15) \n[A-4. feature_4](#feature_4) | [A-10. feature_10](#feature_10) | [A-16. feature_16](#feature_16) ","2212301a":"# <a name=\"feature_6\"><\/a> A-6. feature_6\n[Back to INDEX](#index)","7b8e1315":"# <a name=\"feature_10\"><\/a> A-10. feature_10\n[Back to INDEX](#index)","b014b744":"\n### Conclusion: \n* **target_value** is unimodal bell-shaped (slightly skewed to the right); \n* decomposed into trend and stochastic noise;\n* percentage change is likely to have stationarity property\n* alternative - log transformation of the variable\n* it is possible to apply z-score standartization (sklearn StandardScaler)","b6b59abb":"# <a name=\"4\"><\/a> 4. LSTM Model - Batch Training and Predictiction\n-------------------------","6853c6fd":"# <a name=\"feature_15\"><\/a> A-15. feature_15\n[Back to INDEX](#index)","cdf4ae07":"# <a name=\"feature_7\"><\/a> A-7. feature_7\n[Back to INDEX](#index)","1aec7cdd":"# <a name=\"feature_2\"><\/a> A-2. feature_2\n[Back to INDEX](#index)","b5f2d123":"# <a name=\"feature_12\"><\/a> A-12. feature_12\n[Back to INDEX](#index)","f8b2c7e2":"# <a name=\"target_value\"><\/a> A-0. target_value \n[Back to INDEX](#index)","6287826f":">  ## An alternative simplified transformation of the time-series (just to keep in mind): \n> Normalize sequences X = X\/X_0 - 1, where X_0 is 1st example in the series:\n> * X_0_train, X_0_test = X_train[ : , 0], X_test[ : , 0]\n> * X_train = X_train\/X_0_train[ : , None, :] - 1\n> * X_test = X_test\/X_0_test[ : , None, :] - 1","e578dfcf":"# <a name=\"feature_3\"><\/a> A-3. feature_3\n[Back to INDEX](#index)","8ced69b1":"# <a name=\"1\"><\/a> 1. Load and Review Data\n------------------","b623c444":"# <a name=\"feature_11\"><\/a> A-11. feature_11\n[Back to INDEX](#index)","2e44257f":"# <a name=\"feature_8\"><\/a> A-8. feature_8\n[Back to INDEX](#index)","313cec50":"# <a name=\"feature_16\"><\/a> A-16. feature_16\n[Back to INDEX](#index)","3eb56b8f":"# <a name=\"feature_14\"><\/a> A-14. feature_14\n[Back to INDEX](#index)","a57ac3d1":"# <a name=\"feature_1\"><\/a> A-1. feature_1\n[Back to INDEX](#index)","163940ce":"### (!) We see that the mean and variance look almost constant except for a few outliers, but it is better to rescale the transformed time series either with StandardScaler or MinMaxScaler(-1, 1) for efficient model learning. The scalers are fit on the traning data and then applied to the test data.","f3bc631a":"# <a name=\"feature_13\"><\/a> A-13. feature_13\n[Back to INDEX](#index)","8c71c08e":"<h1 id=\"eda_function\"> EDA Function <\/h1>","49fa280b":"# <a name=\"2\"><\/a> 2. Feature Engineering\n-------\n### What do we have from the raw data review?\n* Exploratory Data Analysis of individual time series is visualized in the [APPENDIX](#index) below.\n* The raw data contain stochastic time series, including 'target_value'. Predicting\/ making classification based on stochastic variable values may force the model to learn the 'persistence' mode (i.e. yhat(t+1) = y(t)), resulting in little predictive power. Defining the model to predict (make classification from) the difference in values between the time steps rather than value itself, is a stronger test of its predictive power. \n* The raw data are weakly correleted with the target_value and the target_class and among each other with rare exceptions. The predictive power could be in temporal effects. \n* The raw data are at different scales, therefore one more review after data transformation is needed to check if a standartization (for ~normal distribution of values) \/ normalization (for other distributions) is required to ensure efficient learning of NN models.\n\nPossible nature | TS_name | Description | Transformation | Rescaling after transformation\n:---: | :-- | :-- | :-- | :--\nclass | target_class | binary (48%-1s, 52%-0s) | None | None\nstock index or indicator like RSI | [target_value](#target_value) | unimodal, bell-shaped, skewed to the right, stochastic trend with values in [48, 91], order +2 autocorr | pc_change or log diff | Standard\nstock index | [feature_1](#feature_1) | unimodal, bell-shaped, slightly skewed to the right, stochastic trend with values in [1432, 2539], order +2 autocorr | pc_change or log diff | Standard\nstock index | [feature_2](#feature_2) | bimodal, stochastic trend with values in [27, 126 ], order +2 autocorr | pc_change or log diff | Standard or MinMax\nstock index | [feature_3](#feature_3) | unimodal, strongly skewed to the right,  stochastic trend with values in [315, 830], order +2 autocorr  | pc_change or log diff | Standard or MinMax\nstock index | [feature_4](#feature_4) | unimodal, skewed to the right,  stochastic trend with values in [1, 6 ], order +2 autocorr | pc_change or log diff | Standard\ntechnical indicator | [feature_5](#feature_5) | unimodal, 5 descrete values  in [100+-0.00002], ordr +2 autocorr |  pc_change or log diff | Standard\ntechnical indicator | [feature_6](#feature_6) | unimodal, bell-shaped, ranging in [100+- 0.04], order +2 autocorr | pc_change or log diff | Standard\ntechnical indicator | [feature_7](#feature_7) | unimodal, skewed to the right, ranging in [100+-0.04], order -29\/+31 autocorr  | pc_change or log diff | Standard\ntechnical indicator | [feature_8](#feature_8) | ranging around 100, occasional spikes in range [-340, +780] - [Q1'12, Q4'16], order 2 autocorr, negative values! | pc_change | Standard or MinMax\ntechnical indicator | [feature_9](#feature_9) | ranging around 100, occasional spikes in range [-3413, +2626] - [Q1'12, Q4'16], order 2 autocorr, negative values! | pc_change | Standard or MinMax\ntechnical indicator | [feature_10](#feature_10) | ranging around 100, occasional spikes in range [-2104, +2206] - [Q1'12, Q4'16], order 2 autocorr, negative values! | pc_change | Standard or MinMax\ntechnical indicator | [feature_11](#feature_11) | ranging around 100, occasional spikes in range [-1321, +1213 ] - [Q4'12-Q1'13], order 2 autocorr, negative values! | pc_change | Standard or MinMax\ntechnical indicator | [feature_12](#feature_12) | ranging around 100, occasional spikes in range [-2933, +2462 ] - [Q4'12-Q1'13], order 2 autocorr, negative values! | pc_change | Standard or MinMax\ntechnical indicator | [feature_13](#feature_13) | ranging around 100, occasional spikes in range [-3206, +2687 ] - [Q4'12-Q1'13], order 2 autocorr, negative values! | pc_change | Standard or MinMax\ntechnical indicator (oscillator) | [feature_14](#feature_14) | unimodal, skewed to the right, ranging in  [100+-0.02], autocorr +32 | pc_change or log diff | Standard\ntechnical indicator (oscillator) | [feature_15](#feature_15) | unimodal, bell-shaped, skewed to the left, ranging in [100+-0.02], autocorr -38 | pc_change or log diff | Standard\ntechnical indicator (oscillator) | [feature_16](#feature_16) | unimodal, bell-shaped, ranging in [100+-0.01], autocorr +14 | pc_change or log diff | Standard\n\n### Let's try a simple approach first and apply the pc_change to all *features* and *target_value* time series.","fcfe9c6d":"# <a name=\"feature_4\"><\/a> A-4. feature_4\n[Back to INDEX](#index)","33c56a5a":"# 0. Problem description\n--------------------------\nI've got this dataset of financial time series from my freinds at TenViz who's job is the magic of predicting stock market movements. The problem was formulated as follows:\n> ... to predict *target_class* based on values of *target_variable* & available features (dataset 'test_task_data.csv'). Before building a classifier, please pay attention to the nature of features and specific aspects of working with time series. Also, you can use *target_values* to derive useful information and additional features. As a train set use dataset from 2012-01-01 till 2016-12-31, as a test set used from 2017-01-02 till 2018-06-19. Finally, evaluate your model & provide analysis with short comments.\n> The results of the work should contain:\n> * Description of the steps of the solution of the task.\n> * Runnable implementation code in Python.\n> * PDF with the charts\n\nIn this kernel\/ notebook I review the raw time series data, apply necessery transformations and scaling, formulate a machine learning problem and build a classifier based on a stacked LSTM RNN.\n\n## Contents:\n1. [Load and Review Data](#1)\n2. [Feature Engineering](#2)\n3. [Data Pre-processing for LSTM Model](#3)\n4. [LSTM Model - Batch Training and Predictiction](#4)\n5. [APPENDIX - EDA for Individual Time Series](#5)","6ef5bbf7":"# <a name=\"feature_5\"><\/a> A-5. feature_5\n[Back to INDEX](#index)","2a560879":"# <a name=\"3\"><\/a> 3. Data Pre-processing for LSTM Model\n------------------------------------\nInput data for the Keras LSTM layer has 3 dimensions: (M, T, N), where \n* M - number of examples (2D: sequences of timesteps x features), \n* T - sequence length (timesteps) and \n* N - number of features (input_dim)","eef6b65a":"# <a name=\"feature_9\"><\/a> A-9. feature_9\n[Back to INDEX](#index)"}}