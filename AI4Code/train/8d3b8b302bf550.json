{"cell_type":{"d64232b0":"code","08f4f909":"code","beac7586":"code","e74c9055":"code","a91099e9":"code","e2edd469":"code","ad6032e4":"code","05e037b2":"code","6c418385":"code","f08fade4":"code","2d4380da":"code","997a40b7":"code","46632de9":"code","a66bc23a":"code","add9da40":"code","6e0dfaa6":"code","f2311b92":"code","06d5ad94":"code","7773209d":"code","06d56af7":"code","20f59bcc":"code","f8138e62":"code","adeacc08":"markdown","77e1cbac":"markdown","4bf6f88d":"markdown","991ae975":"markdown","6699d33a":"markdown","df163ac2":"markdown","50ca9371":"markdown","af23c7da":"markdown","fd862873":"markdown"},"source":{"d64232b0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt;\n\nimport warnings;\n# filter warnings\nwarnings.filterwarnings(\"ignore\");\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","08f4f909":"x_l = np.load(\"..\/input\/sign-language-digits-dataset\/X.npy\");\ny_l = np.load(\"..\/input\/sign-language-digits-dataset\/Y.npy\");\nimgSize = 64;\nplt.subplot(1, 2, 1);\nplt.imshow(x_l[260].reshape(imgSize, imgSize));\nplt.axis(\"off\");\nplt.subplot(1, 2, 2);\nplt.imshow(x_l[900].reshape(imgSize, imgSize));\nplt.axis(\"off\");","beac7586":"x = np.concatenate((x_l[204:409], x_l[822:1027]), axis = 0);\ny = np.concatenate((np.zeros(205), np.ones(205)), axis = 0).reshape(x.shape[0], 1);\nprint(x.shape);\nprint(y.shape);","e74c9055":"from sklearn.model_selection import train_test_split;\nxTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size = 0.15, random_state = 42);\nnumberOfTrainData = xTrain.shape[0];\nnumberOfTestData = xTest.shape[0];","a91099e9":"xTrainFlatten = xTrain.reshape(numberOfTrainData, xTrain.shape[1]*xTrain.shape[2]);\nxTestFlatten = xTest.reshape(numberOfTestData, xTest.shape[1]*xTest.shape[2]);\nprint(f\"Shape of xTrain: {xTrainFlatten.shape}\");\nprint(f\"Shape of xTest: {xTestFlatten.shape}\");","e2edd469":"xTrain = xTrainFlatten.T;\nxTest = xTestFlatten.T;\nyTrain = yTrain.T;\nyTest = yTest.T;\n","ad6032e4":"# Initialize weights and bias\n\ndef InitWeightAndBias(dimension):\n    w = np.full((dimension, 1), 0.01);\n    b = 0.0;\n    return w, b;\n\n#w, b = InitWeightAndBias(xTrain.shape[0]);","05e037b2":"# Sigmoid Fcn\n\ndef Sigmo(value):\n    result = 1 \/ (1 + np.exp(-value));\n    return result; ","6c418385":"def FwdAndBwdProp(w, b, xTrain, yTrain):\n    # Forward Propagation\n    z = np.dot(w.T, xTrain) + b;\n    yHead = Sigmo(z);\n    loss = -yTrain*np.log(yHead) - (1-yTrain)*np.log(1-yHead);\n    cost = np.sum(loss) \/ xTrain.shape[1];\n    \n    # Backward Propagation\n    dw = (np.dot(xTrain, ((yHead - yTrain).T)))\/xTrain.shape[1];\n    db = np.sum(yHead - yTrain)\/xTrain.shape[1];\n    grads = {\"dw\": dw, \"db\": db};\n        \n    return cost, grads;","f08fade4":"# Updating weights\n\ndef UpdWeightAndBias(w, b, xTrain, yTrain, learningRate, numOfIters):\n    \n    costList = [];\n    for iterId in range(0, numOfIters):\n        cost, grads = FwdAndBwdProp(w, b, xTrain, yTrain);\n        costList.append(cost);\n        \n        w -= learningRate * grads[\"dw\"];\n        b -= learningRate * grads[\"db\"];\n        \n    params = {\"w\": w, \"b\": b};\n    plt.plot(range(0, numOfIters), costList);\n    plt.xticks(range(0, numOfIters), rotation = \"vertical\");\n    plt.xlabel(\"Iterations\");\n    plt.ylabel(\"Cost\");\n    \n    return params, grads, costList;","2d4380da":"# Prediction level\n\ndef Predict(w, b, xTest):\n    z = np.dot(w.T, xTest) + b;\n    yHead = Sigmo(z);\n    yPred = np.zeros((1, xTest.shape[1]));\n    \n    for iter in range(yHead.shape[1]):\n        if yHead[0, iter] >= 0.5:\n            yPred[0, iter] = 1;\n        \n    return yPred;","997a40b7":"# Logistic Regression\n\ndef LogisticRegression(xTrain, xTest, yTrain, yTest, learningRate, numOfIters):\n    \n    dimension = xTrain.shape[0];\n    \n    w, b = InitWeightAndBias(dimension);\n    params, grads, costList = UpdWeightAndBias(w, b, xTrain, yTrain, learningRate, numOfIters);\n    w = params[\"w\"];\n    b = params[\"b\"];\n    \n    yPred = Predict(w, b, xTest);\n    \n    accuracy = 100 - np.mean(np.abs(yPred - yTest)) * 100;\n    print(f\"Accuracy of Logistic Regression: {accuracy}\");\n    return;\n\nlearningRate = 0.01;\nnumOfIters = 300;\nLogisticRegression(xTrain, xTest, yTrain, yTest, learningRate, numOfIters);","46632de9":"from sklearn import linear_model\nlogreg = linear_model.LogisticRegression(random_state = 42,max_iter= 150)\n\n#print(\"test accuracy: {} \".format(logreg.fit(xTrain.T, yTrain.T).score(xTest.T, yTest.T)))\n#print(\"train accuracy: {} \".format(logreg.fit(xTrain.T, yTrain.T).score(xTrain.T, yTrain.T)))","a66bc23a":"# Initialize params and layer sizes\n\ndef InitParamsAndLayerSizesANN(xTrain, yTrain):\n    hiddenNodeCount = 3;\n    params = {\"w1\": np.random.randn(hiddenNodeCount, xTrain.shape[0])*0.1,\n              \"b1\": np.zeros((hiddenNodeCount,1)),\n              \"w2\": np.random.randn(yTrain.shape[0], hiddenNodeCount)*0.1,\n              \"b2\": np.zeros((yTrain.shape[0],1))};\n    return params;","add9da40":"# Forward Propagation\n\ndef FwdPropANN(params, xTrain):\n    z1 = np.dot(params[\"w1\"], xTrain) + params[\"b1\"];\n    a1 = np.tanh(z1);\n    z2 = np.dot(params[\"w2\"], a1) + params[\"b2\"];\n    a2 = Sigmo(z2);\n    \n    cache = {\"z1\": z1, \"a1\": a1, \"z2\": z2, \"a2\": a2};\n    return a2, cache;","6e0dfaa6":"# Compute cost fcn\n\ndef ComputeCostANN(a2, yTrain):\n    logRes = np.multiply(np.log(a2), yTrain);\n    cost = -1*np.sum(logRes) \/ yTrain.shape[1];\n    \n    return cost; ","f2311b92":"# Back propagation\n\ndef BwdPropANN(params, cache, xTrain, yTrain):\n    dz2 = cache[\"a2\"] - yTrain;\n    dw2 = np.dot(dz2, cache[\"a1\"].T)\/xTrain.shape[1];\n    db2 = np.sum(dz2, axis = 1, keepdims = True)\/xTrain.shape[1];\n    dz1 = np.dot(params[\"w2\"].T, dz2)*(1-np.power(cache[\"a1\"], 2));\n    dw1 = np.dot(dz1, xTrain.T)\/xTrain.shape[1];\n    db1 = np.sum(dz1, axis = 1, keepdims = True)\/xTrain.shape[1];\n    \n    grads = {\"dw1\": dw1, \"db1\": db1, \"dw2\": dw2, \"db2\": db2};\n    return grads;","06d5ad94":"# Update weights\n\ndef UpdateWeightsANN(grads, params, learningRate = 0.01):\n    params[\"w1\"] -= learningRate * grads[\"dw1\"];\n    params[\"b1\"] -= learningRate * grads[\"db1\"];\n    params[\"w2\"] -= learningRate * grads[\"dw2\"];\n    params[\"b2\"] -= learningRate * grads[\"db2\"];\n\n    return params;","7773209d":"# Make Prediction\n\ndef PredictANN(params, xTest):\n    a2, cache = FwdPropANN(params, xTest);\n    yHead = np.zeros((1, xTest.shape[1]));\n    \n    for item in range(a2.shape[1]):\n        if a2[0, item] > 0.5:\n            yHead[0, item] = 1;\n            \n    return yHead;","06d56af7":"# Run 2Layer NN\n\ndef TwoLayerANN(xTrain, yTrain, xTest, yTest, learningRate, numIters):\n    costList = [];\n    \n    params = InitParamsAndLayerSizesANN(xTrain, yTrain);\n    \n    for iterCntr in range(0, numIters):\n        a2, cache = FwdPropANN(params, xTrain);\n        if iterCntr % 100 == 0:\n            costList.append(ComputeCostANN(a2, yTrain));\n            \n        grads = BwdPropANN(params, cache, xTrain, yTrain);\n        params = UpdateWeightsANN(grads, params, learningRate);\n        \n        \n    plt.plot(costList);\n    plt.xlabel(\"Itertions\");\n    plt.ylabel(\"Cost\");\n    plt.show();\n\n    yHead = PredictANN(params, xTest);\n    print(f\"Accuracy: {np.sum(yHead == yTest) \/ yHead.shape[1] * 100}\")\n    \nlearningRate = 0.01;\nnumIters = 3500;\nTwoLayerANN(xTrain, yTrain, xTest, yTest, learningRate, numIters);\n    \n    ","20f59bcc":"x_train, x_test, y_train, y_test = xTrain.T, xTest.T, yTrain.T, yTest.T;\n\nimport tensorflow as tf\n\nfrom tensorflow.python.keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom tensorflow.python.keras.models import Sequential # initialize neural network library\nfrom tensorflow.python.keras.layers import Dense # build our layers library\n\ndef build_classifier():\n    classifier = Sequential() # initialize neural network\n    classifier.add(Dense(units = 8, kernel_initializer = 'uniform', activation = 'relu', input_dim = x_train.shape[1]))\n    classifier.add(Dense(units = 4, kernel_initializer = 'uniform', activation = 'relu'))\n    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n    return classifier\n\nclassifier = KerasClassifier(build_fn = build_classifier, epochs = 100)\naccuracies = cross_val_score(estimator = classifier, X = x_train, y = y_train, cv = 3)\nmean = accuracies.mean()\nvariance = accuracies.std()\nprint(\"Accuracy mean: \"+ str(mean))\nprint(\"Accuracy variance: \"+ str(variance))","f8138e62":"import torch;","adeacc08":"# Logistic Regression with SKLearn","77e1cbac":"# Get & Prepare the Data for Processing","4bf6f88d":"# L Layer with Keras","991ae975":"# 2 Layer ANN","6699d33a":"## Prepare the test and train data","df163ac2":"# Logistic Regression","50ca9371":"## Concatenate the data","af23c7da":"## Flatten the data","fd862873":"## Take the transpose of the matrices\/vectors to prepare for deep learning"}}