{"cell_type":{"c712ba4a":"code","e44e32ba":"code","4ffe540a":"code","88c20337":"code","afa0aae0":"code","9d99023a":"code","e92a9ece":"code","8dad0f14":"code","ff1ef168":"code","7815eb5f":"code","0057be00":"code","52cd077a":"code","f815135e":"code","98cd67d7":"code","1b979eb8":"code","c576060f":"code","df170535":"code","341c2b18":"code","4f67206a":"code","d6dd45f7":"code","12349ae1":"code","3429b7e1":"code","eda3696b":"code","31fd363e":"code","0f8323e1":"code","8589810e":"code","0d0e071c":"code","ab1e0601":"code","38957ceb":"code","832ebcb0":"code","042bb7ac":"code","8811e418":"code","b8eaf56f":"code","a5435e44":"code","5a4176dc":"code","9f21cde1":"code","66b1ff6f":"code","8fd2ddb4":"code","4a0ebcf8":"code","a9775613":"code","474f61f3":"code","260e103f":"code","f8957ca9":"code","de66ec61":"code","a9b65e2e":"code","87d9b48f":"code","9f3f7462":"code","29707388":"code","2988d15e":"code","61e7d5b4":"code","dc463cae":"code","34f39bf7":"code","056fbaf9":"code","5c16dffd":"code","6d33b59b":"code","eca13881":"code","4f723371":"code","6166f2ed":"code","169c641b":"code","f159b9c5":"code","bddb9753":"code","43cc7ebd":"code","b4bb6368":"code","ede88cef":"code","8391b956":"markdown","1dbbc732":"markdown","c2fba65c":"markdown","64f5b3d1":"markdown","2757c7c6":"markdown","fd645ddb":"markdown","f3721048":"markdown","50c4d8e0":"markdown","6827a2db":"markdown","6778438b":"markdown","483be61d":"markdown","3de7d363":"markdown","32af85b6":"markdown","a69c0efa":"markdown","534d53c1":"markdown","91909845":"markdown","f8af738a":"markdown","e5ba5479":"markdown","d2f14dc9":"markdown","8790564c":"markdown","7daa2c8b":"markdown","49f68d8c":"markdown","fad5f0e6":"markdown","145eafce":"markdown","fae84df5":"markdown","84f22274":"markdown","9605273d":"markdown","b42937ce":"markdown","927b1c6d":"markdown","d3b2d2a5":"markdown","a069e09a":"markdown","4d0f0bce":"markdown","a88bea98":"markdown","e164879a":"markdown","63997e0f":"markdown","080bbc2f":"markdown","e90a1297":"markdown","595c456e":"markdown","fb2b4c9d":"markdown","9c2e5ec6":"markdown","ad01fcab":"markdown","cf411b43":"markdown","9f2dae16":"markdown","ba2a20d3":"markdown","4e7e920b":"markdown","59125d73":"markdown","18443fbf":"markdown","5d0fd512":"markdown","9a5fccec":"markdown","46bad0da":"markdown"},"source":{"c712ba4a":"import pandas as pd\n\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\ndf = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\ndf.sample(5)","e44e32ba":"df.describe()","4ffe540a":"df.isnull().sum()","88c20337":"df['Class'].value_counts(normalize=True)*100","afa0aae0":"df['Class'].value_counts(sort=False).plot.bar(\n    title=\"Class Frequency Distribution\\n0 = Normal\\n1 = Fraud\", \n    color=['grey','red'],\n    rot=0,\n    figsize=(13,5)\n);","9d99023a":"df['Time'].plot.hist(\n    title=\"Time Frequency Distribution\",\n    bins=50, \n    color='grey',\n    figsize=(13,5)\n);","e92a9ece":"df['Amount'].plot.hist(\n    title=\"Amount Frequency Distibution\",\n    bins=100,\n    color='grey',\n    figsize=(13,5),\n    density=True\n);","8dad0f14":"df.drop(['Time','Amount','Class'],axis=1).boxplot(figsize=(13,5));","ff1ef168":"df.drop(['Time','Amount','Class'],axis=1).hist(\n    figsize=(13,15), \n    ylabelsize=False,\n    grid=False\n);","7815eb5f":"df.V4.hist(\n    bins=50,\n    figsize=(13,5),\n    grid=False\n);","0057be00":"df.V12.hist(\n    bins=50,\n    figsize=(13,5),\n    grid=False\n);","52cd077a":"df.V28.hist(\n    bins=50,\n    figsize=(13,5),\n    grid=False\n);","f815135e":"original = df.copy()","98cd67d7":"import matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(13,5))\naxes[0].get_xaxis().set_visible(False)\naxes[0].set_title('Amount')\naxes[0].boxplot(df['Amount'])\naxes[1].get_xaxis().set_visible(False)\naxes[1].set_title('Time')\naxes[1].boxplot(df['Time']);","1b979eb8":"df['hour'] = pd.to_timedelta(df['Time'], unit='s').dt.components.hours\ndf['minute'] = pd.to_timedelta(df['Time'], unit='s').dt.components.minutes\ndf['second'] = pd.to_timedelta(df['Time'], unit='s').dt.components.seconds","c576060f":"from sklearn.preprocessing import RobustScaler\n\nscaler = RobustScaler()\nhours_scaled = scaler.fit_transform(df['hour'].values.reshape(-1,1))\nminutes_scaled = scaler.fit_transform(df['minute'].values.reshape(-1,1))\nseconds_scaled = scaler.fit_transform(df['second'].values.reshape(-1,1))\namount_scaled = scaler.fit_transform(df['Amount'].values.reshape(-1,1))\ndf.drop(['Time','Amount','hour','minute','second'], axis=1, inplace=True)\ndf.insert(0, 'hour_scaled', hours_scaled)\ndf.insert(1, 'minute_scaled', minutes_scaled)\ndf.insert(2, 'second_scaled', seconds_scaled)\ndf.insert(3, 'amount_scaled', amount_scaled)","df170535":"df['Class'].value_counts()","341c2b18":"from imblearn.over_sampling import SMOTE\nseed = 1618 # random state for reproducability \n\nfraud = df[df['Class']==1]\nnormal = df[df['Class']==0].sample(len(fraud)*100, random_state=seed)\ndf = pd.concat([normal,fraud])\nsm = SMOTE(random_state=seed)\nx = df.drop(['Class'], axis = 1)\ny = df[\"Class\"]\nx_res, y_res = sm.fit_resample(x, y)\ndf = x_res\ndf['Class'] = y_res\ndf = df.sample(frac=1, random_state=seed)","4f67206a":"df['Class'].value_counts(sort=False).plot.bar(\n    title=\"Balanced Class Distribution\\n0 = Normal\\n1 = Fraud\", \n    color=['grey','red'],\n    rot=0,\n    figsize=(13,5)\n);","d6dd45f7":"import numpy as np\n\noutliers = {}\nfor col in df.drop(['hour_scaled','minute_scaled','second_scaled','Class'], axis=1).columns:\n    q25 = np.percentile(df[col].loc[df['Class'] == 1].values, 25) \n    q75 = np.percentile(df[col].loc[df['Class'] == 1].values, 75)\n    iqr = q75 - q25\n    upper = q75 + (iqr * 3)\n    lower = q25 - (iqr * 3)\n    outliers[col] = [x for x in df[col].loc[df['Class'] == 1].values \n                if x < lower or x > upper]\n    df[col].replace(df[(df[col] > upper) | (df[col] < lower)].index, np.nan, inplace=True)","12349ae1":"from sklearn.experimental import enable_iterative_imputer  \nfrom sklearn.impute import IterativeImputer\n\nimputer = IterativeImputer(max_iter=10, random_state=seed)\nimp_arr = imputer.fit_transform(df)\ndf = pd.DataFrame(imp_arr, columns=df.columns)","3429b7e1":"fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(13,10));\naxes[0].set_title('Original');\noriginal.drop(['Time','Amount','Class'],axis=1).boxplot(figsize=(13,5), ax=axes[0]);\naxes[1].set_title('Cleaned');\ndf.drop(['hour_scaled',\n         'minute_scaled',\n         'second_scaled',\n         'amount_scaled',\n         'Class'],axis=1).boxplot(figsize=(13,5), ax=axes[1]);","eda3696b":"!pip install seaborn --upgrade","31fd363e":"import seaborn as sns\n\ncorr = df.corr()\nfig = plt.figure(figsize=(13,13))\nsns.heatmap(\n    corr, \n    cmap=\"RdBu\",\n    xticklabels=corr.columns,\n    yticklabels=corr.columns\n);","0f8323e1":"print(corr['Class'].sort_values())","8589810e":"import statsmodels.api as sm\nsm_model = sm.Logit(df['Class'],df.drop(['Class'], axis=1)).fit(disp=0)\nsm_model.summary()","0d0e071c":"def AIC(true,pred,cols):\n    resid = true - pred\n    sse = sum(resid**2)\n    k = len(cols)\n    AIC = 2*k - 2*np.log(sse)\n    \n    return AIC","ab1e0601":"y_val_base = sm_model.predict(df.drop(['Class'],axis=1))\nAIC_base = AIC(df['Class'],y_val_base,df.drop(['Class'],axis=1).columns)\nprint('Baseline AIC: ', AIC_base)","38957ceb":"irr_vars = list(corr['Class'][abs(round(corr['Class'].sort_values(),2))<=0.10].index)\ndf.drop(irr_vars, axis=1, inplace=True)","832ebcb0":"df.boxplot(['V14','V12'], by='Class', figsize=(13,5), grid=False)\ndf.boxplot(['V10','V16'], by='Class', figsize=(13,5), grid=False)\ndf.boxplot(['V3','V9'], by='Class', figsize=(13,5), grid=False);","042bb7ac":"df.boxplot(['V4', 'V11'], by='Class', figsize=(13,5), grid=False)\ndf.boxplot(['V2', 'V19'], by='Class', figsize=(13,5), grid=False);","8811e418":"exp_vars = df.drop(['Class'], axis=1).columns\nfig, axes = plt.subplots(nrows=len(exp_vars), ncols=1, figsize=(13,len(exp_vars)*5));\nfor var in exp_vars:\n    sns.histplot(\n        data=df, \n        x=var, \n        bins=50, \n        kde=True, \n        hue='Class', \n        palette=['grey','red'],\n        ax=axes[list(exp_vars).index(var)]\n    );","b8eaf56f":"from sklearn.model_selection import train_test_split\n\ntrain_df, test_df = train_test_split(df, test_size=0.20, shuffle=True, random_state=seed)\nx_train = train_df.drop(['Class'], axis = 1)\ny_train = train_df[\"Class\"]\nx_test = test_df.drop(['Class'], axis = 1)\ny_test = test_df[\"Class\"]","a5435e44":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import recall_score, precision_score, f1_score, classification_report\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import uniform as sp_randFloat\nfrom scipy.stats import randint as sp_randInt    \nimport time\n\nparams_lr = {\n    'penalty' : ['l1', 'l2'],\n    'C' : sp_randFloat(0, 4),\n    'solver' : ['lbfgs',],\n    'random_state': [seed]\n}\n\nlr = RandomizedSearchCV(\n    LogisticRegression(), \n    param_distributions=params_lr, \n    cv=5, \n    n_iter=5, \n    n_jobs=-1 \n)\n\nt1 = time.time()\nbest_lr = lr.fit(x_train, y_train)\nt2 = time.time()\ny_pred_lr = best_lr.predict(x_test)\nt3 = time.time()\ny_val_lr = best_lr.predict(x_train)\nt_fit_lr = t2 - t1\nt_pred_lr = t3 - t2\n\nprint('The best parameters across ALL searched params:\\n', best_lr.best_params_)","5a4176dc":"f1_test_lr = f1_score(y_test, y_pred_lr)\nf1_train_lr = f1_score(y_train, y_val_lr)\nf1_delta_lr = round(f1_test_lr-f1_train_lr,5)\nprint('F1 Score Delta: {}'.format(f1_delta_lr))","9f21cde1":"print(classification_report(y_test, y_pred_lr, target_names=['Normal', 'Fraud'], digits=5))","66b1ff6f":"AIC_lr = AIC(y_test,y_pred_lr,x_train.columns)\nprint('LR AIC: ', AIC_lr)","8fd2ddb4":"sm_model = sm.Logit(y_train, x_train).fit(disp=0)\nsm_model.summary()","4a0ebcf8":"y_val_sm = sm_model.predict(x_test)\nAIC_sm = AIC(y_test, y_val_sm, x_train.columns)\nprint('Simplified AIC: ', AIC_sm)","a9775613":"from sklearn.ensemble import GradientBoostingClassifier\n\nparams_gbm = {\n    'learning_rate': sp_randFloat(),\n    'subsample'    : sp_randFloat(),\n    'n_estimators' : sp_randInt(100, 1000),\n    'max_depth'    : sp_randInt(4, 10),\n    'random_state': [seed] \n}\n\ngbm = RandomizedSearchCV(\n    GradientBoostingClassifier(), \n    param_distributions=params_gbm, \n    cv=5, \n    n_iter=5, \n    n_jobs=-1\n)\n\nt1 = time.time()\nbest_gbm = gbm.fit(x_train, y_train)\nt2 = time.time()\ny_pred_gbm = best_gbm.predict(x_test)\nt3 = time.time()\ny_val_gbm = best_gbm.predict(x_train)\nt_fit_gbm = t2 - t1\nt_pred_gbm = t3 - t2\n\nprint('The best parameters across ALL searched params:\\n', best_gbm.best_params_)","474f61f3":"f1_test_gbm = f1_score(y_test, y_pred_gbm)\nf1_train_gbm = f1_score(y_train, y_val_gbm)\nf1_delta_gbm = round(f1_test_gbm-f1_train_gbm,5)\nprint('F1 Score Delta: {}'.format(f1_delta_gbm))","260e103f":"print(classification_report(y_test, y_pred_gbm, target_names=['Normal', 'Fraud'], digits=5))","f8957ca9":"AIC_gbm = AIC(y_test,y_pred_gbm,x_train.columns)\nprint('GBM AIC: ', AIC_gbm)","de66ec61":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\n\nparams_bdt = {\n    'learning_rate': sp_randFloat(),\n    'n_estimators' : sp_randInt(100, 1000),\n    'random_state': [seed]\n}\n\nbdt = RandomizedSearchCV(\n    AdaBoostRegressor(DecisionTreeRegressor()),\n    param_distributions=params_bdt, \n    cv=5, \n    n_iter=5, \n    n_jobs=-1\n)\n\nt1 = time.time()\nbest_bdt = bdt.fit(x_train, y_train)\nt2 = time.time()\ny_pred_bdt = best_bdt.predict(x_test)\nt3 = time.time()\ny_val_bdt = best_bdt.predict(x_train)\nt_fit_bdt = t2 - t1\nt_pred_bdt = t3 - t2\n\nprint('The best parameters across ALL searched params:\\n', best_bdt.best_params_)","a9b65e2e":"f1_test_bdt = f1_score(y_test, y_pred_bdt)\nf1_train_bdt = f1_score(y_train, y_val_bdt)\nf1_delta_bdt = round(f1_test_bdt-f1_train_bdt,5)\nprint('F1 Score Delta: {}'.format(f1_delta_bdt))","87d9b48f":"print(classification_report(y_test, y_pred_bdt, target_names=['Normal', 'Fraud'], digits=5))","9f3f7462":"AIC_bdt = AIC(y_test, y_pred_bdt, x_train.columns)\nprint('BDT AIC: ', AIC_bdt)","29707388":"import xgboost as xgb\n\nparam_xgb = {\n    'n_estimators': sp_randInt(150, 1000),\n    'learning_rate': sp_randFloat(0.01, 0.6),\n    'subsample': sp_randFloat(0.3, 0.9),\n    'max_depth': [3, 4, 5, 6, 7, 8, 9],\n    'colsample_bytree': sp_randFloat(0.5, 0.9),\n    'min_child_weight': [1, 2, 3, 4],\n    'random_state': [seed]\n}\n\nxgb = RandomizedSearchCV(\n    xgb.XGBClassifier(objective = 'binary:logistic'), \n    param_distributions = param_xgb,\n    cv = 5,  \n    n_iter = 5, \n    n_jobs = -1\n)\n\n\nt1 = time.time()\nbest_xgb = xgb.fit(x_train, y_train)\nt2 = time.time()\ny_pred_xgb = best_xgb.predict(x_test)\nt3 = time.time()\ny_val_xgb = best_xgb.predict(x_train)\nt_fit_xgb = t2 - t1\nt_pred_xgb = t3 - t2\n\nprint('The best parameters across ALL searched params:\\n', best_bdt.best_params_)","2988d15e":"f1_test_xgb = f1_score(y_test, y_pred_xgb)\nf1_train_xgb = f1_score(y_train, y_val_xgb)\nf1_delta_xgb = round(f1_test_xgb-f1_train_xgb,5)\nprint('F1 Score Delta: {}'.format(f1_delta_xgb))","61e7d5b4":"print(classification_report(y_test, y_pred_xgb, target_names=['Normal', 'Fraud'], digits=5))","dc463cae":"AIC_xgb = AIC(y_test, y_pred_xgb, x_train.columns)\nprint('XGB AIC: ', AIC_xgb)","34f39bf7":"from sklearn.ensemble import RandomForestClassifier\n\nparams_rf = { \n    'n_estimators':  sp_randInt(100, 1000),\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth': sp_randInt(4, 10),\n    'bootstrap': [True, False],\n    'random_state': [seed]\n}\n\nrf = RandomizedSearchCV(\n    RandomForestClassifier(),\n    param_distributions=params_rf, \n    cv=5, \n    n_iter=5, \n    n_jobs=-1\n)\n\nt1 = time.time()\nbest_rf = rf.fit(x_train, y_train)\nt2 = time.time()\ny_pred_rf = best_rf.predict(x_test)\nt3 = time.time()\ny_val_rf = best_rf.predict(x_train)\nt_fit_rf = t2 - t1\nt_pred_rf = t3 - t2\n\nprint('The best parameters across ALL searched params:\\n', best_rf.best_params_)","056fbaf9":"f1_test_rf = f1_score(y_test, y_pred_rf)\nf1_train_rf = f1_score(y_train, y_val_rf)\nf1_delta_rf = round(f1_test_rf-f1_train_rf,5)\nprint('F1 Score Delta: {}'.format(f1_delta_rf))","5c16dffd":"print(classification_report(y_test, y_pred_rf, target_names=['Normal', 'Fraud'], digits=5))","6d33b59b":"AIC_rf = AIC(y_test, y_pred_rf, x_train.columns)\nprint('RF AIC: ', AIC_rf)","eca13881":"from sklearn.neural_network import MLPClassifier\nparams_nn = { \n    'hidden_layer_sizes':  [(sp_randInt.rvs(10,200,1),\n                             sp_randInt.rvs(10,200,1),),\n                            (sp_randInt.rvs(10,200,1),)],\n    'activation': ['tanh', 'relu', 'logistic'],\n    'solver': ['sgd', 'adam', 'lbfgs'],\n    'alpha': sp_randFloat(0.0001, 0.05),\n    'learning_rate': ['constant','adaptive'],\n    'random_state': [seed]\n}\n\nnn = RandomizedSearchCV(\n    MLPClassifier(),\n    param_distributions=params_nn, \n    cv=5, \n    n_iter=5, \n    n_jobs=-1\n)\n\n\nt1 = time.time()\nbest_nn = nn.fit(x_train,y_train)\nt2 = time.time()\ny_pred_nn = best_nn.predict(x_test)\nt3 = time.time()\ny_val_nn = best_nn.predict(x_train)\nt_fit_nn = t2 - t1\nt_pred_nn = t3 - t2\n\nprint('The best parameters across ALL searched params:\\n', best_nn.best_params_)","4f723371":"f1_test_nn = f1_score(y_test, y_pred_nn)\nf1_train_nn = f1_score(y_train, y_val_nn)\nf1_delta_nn = round(f1_test_nn-f1_train_nn,5)\nprint('F1 Score Delta: {}'.format(f1_delta_nn))","6166f2ed":"print(classification_report(y_test, y_pred_nn, target_names=['Normal', 'Fraud'], digits=5))","169c641b":"AIC_nn = AIC(y_test, y_pred_nn, x_train.columns)\nprint('NN AIC: ', AIC_nn)","f159b9c5":"def bold(var):\n    s = '\\033[1m' + str(round(var,5)) + '\\033[0m'\n    return s\n\nlr_str = 'Logistic Regression\\nF1 score: {:.5f}\\tF1 delta: {}\\tAIC: {}\\n'\ngbm_str = 'Gradient Boosting Machine\\nF1 score: {:.5f}\\tF1 delta: {:.5f}\\tAIC: {:.5f}\\n'\nbdt_str = 'Boosted Decision Tree\\nF1 score: {}\\tF1 delta: {}\\tAIC: {:.5f}\\n'\nxgb_str = 'Extreme Gradient Boost\\nF1 score: {:.5f}\\tF1 delta: {}\\tAIC: {:.5f}\\n'\nrf_str = 'Random Forest\\nF1 score: {:.5f}\\tF1 delta: {:.5f}\\tAIC: {:.5f}\\n'\nnn_str = 'Neural Network\\nF1 score: {:.5f}\\tF1 delta: {:.5f}\\tAIC: {:.5f}\\n'\n\nprint(lr_str.format(f1_test_lr, bold(f1_delta_lr), bold(AIC_lr)))\nprint(gbm_str.format(f1_test_gbm, f1_delta_gbm, AIC_gbm))\nprint(bdt_str.format(bold(f1_test_bdt), f1_delta_bdt, AIC_bdt))\nprint(xgb_str.format(f1_test_xgb, f1_delta_xgb, AIC_xgb))\nprint(rf_str.format(f1_test_rf, f1_delta_rf, AIC_rf))\nprint(nn_str.format(f1_test_nn, f1_delta_nn, AIC_nn))","bddb9753":"fit_times = [t_fit_lr, t_fit_gbm, t_fit_bdt, t_fit_xgb, t_fit_rf, t_fit_nn]\npred_times = [t_pred_lr, t_pred_gbm, t_pred_bdt, t_pred_xgb, t_pred_rf, t_pred_nn]\ntick_label=['LR','GBM','BDT','XGB','RF','NN']\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(13,5));\naxes[0].set_title('Time to Fit');\naxes[0].bar(x=[l for l in range(6)], height=fit_times, width=0.8, tick_label=tick_label)\nfor i, v in enumerate(fit_times):\n    axes[0].annotate(str(round(v,2)),(i, v), ha='center', va='bottom')\naxes[1].set_title('Time to Predict');\naxes[1].bar(x=[l for l in range(6)], height=pred_times, width=0.8, tick_label=tick_label)\nfor i, v in enumerate(pred_times):\n    axes[1].annotate(str(round(v,4)),(i, v), ha='center', va='bottom')\nplt.show();","43cc7ebd":"final_model = LogisticRegression(\n    C=0.35957637139353915, \n    penalty='l2', \n    solver='lbfgs',\n    random_state=seed\n).fit(x_train,y_train)\ncoefs = pd.DataFrame(final_model.coef_,columns=x_train.columns, index=['coefs'])\nodds_ratios = pd.DataFrame(np.exp(final_model.coef_),columns=x_train.columns, index=['odds_ratio']) \npd.concat([coefs,odds_ratios])","b4bb6368":"from sklearn.inspection import plot_partial_dependence, partial_dependence\n\nplot_partial_dependence(final_model, x_train, ['V4'])\nV4_AME = round(np.mean(partial_dependence(final_model, x_train, [4])[0]),4)\n\nprint('Average Marginal Effect of V4: {}'.format(V4_AME))","ede88cef":"feature_importance = abs(final_model.coef_[0])\nfeature_importance = 100.0 * (feature_importance \/ feature_importance.max())\nsorted_idx = np.argsort(feature_importance)\npos = np.arange(sorted_idx.shape[0]) + .75\n\nfig = plt.figure(figsize=(13,5))\nax = fig.add_subplot(1, 1, 1)\nax.barh(pos, feature_importance[sorted_idx], align='center')\nax.set_yticks(pos)\nax.set_yticklabels(np.array(x_train.columns)[sorted_idx])\nax.set_title('Relative Feature Importance')\nplt.show();","8391b956":"<a id=\"eda\"><\/a>\n## Exploratory Data Analysis (EDA)\n\nFirst we need to explore the data set to **understand** our variables,\n**clean** our dataset, and **analyze relationships** between variables. Ultimately EDA should allow us to answer the following questions from Daniel Bourke's article [A Gentle Introduction to Exploratory Data Analysis](https:\/\/towardsdatascience.com\/a-gentle-introduction-to-exploratory-data-analysis-f11d843b8184)[$^{5}$](#5):\n* *What question(s) are you trying to solve (or prove wrong)?*\n* *What kind of data do you have and how do you treat different types?*\n* *What\u2019s missing from the data and how do you deal with it?*\n* *Where are the outliers and why should you care about them?*\n* *How can you add, change or remove features to get more out of your data?*\n\nWhile we won't explicitly answer these questions in this paper we will approach understanding, cleaning, and analyzing with them in mind. Before we begin it is important to remember that except for `Time` and `Amount` all explanatory variables are transformed due to privacy reasons. The explanatory variables were transformed through [prinicpal component analysis or PCA](https:\/\/en.wikipedia.org\/wiki\/Principal_component_analysis) and no further information can be provided regarding features `V1, V2, ..., V28`.\n\n`Time` contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature `Amount` is the transaction amount, this feature can be used for example-dependant cost-senstive learning. Finally the feature `Class` is the response variable and it takes value 1 in case of fraud and 0 otherwise.","1dbbc732":"While the principal components are unknown due to the anonymization process behind PCA we can see from the coefficients that the log-odds of a transaction being fraudulent is **1.040989** for a transaction having 1 unit increase of `V4` holding all other variables constant. Another way of describing this relationship is with odds ratio in which case we would state 1 unit increase of `V4` increases the odds of a transaction being fraudulent by **2.83** times. While odds ratio descriptions are significantly easier to understand than log-odds, this is still a bit ambiguous when compared to linear regression interpretations. Marginal effects are an alternative metric that describe the impact of the predictor on the outcome variable with the Average Marginal Effect (AME) representing the impact on the probability of the outcome.[$^{44}$](#44) Using the marginal effect approach we can interpret 1 unit increase of `V4` increasing the probability of fraud by **73.52%**.","c2fba65c":"The next boosting model we will evaluate AdaBoostRegressor which uses adaptive boosting is an ensemble technique that uses the weighted sum of the various weak learners. The weak learners in this specific example are DecisionTreeRegressor instances a standard decision tree model.[$^{37}$](#37)\n\n<p><img src=\"https:\/\/miro.medium.com\/max\/1700\/0*paPv7vXuq4eBHZY7.png\" \/><\/p>","64f5b3d1":"Before we scale however we need to take a moment to work through some feature engineering to extract the various time components (e.g. hour, minute, second) from the `Time` variable","2757c7c6":"<a id=\"objective\"><\/a>\n## Objective\nThe primary goal of this notebook is to develop a highly performant model for detecting fraudulent credit card transactions that could be utilized by our client the credit card company. Along the way we will outline a generalized end to end process for completing a machine learning project from data load to model validation and insights derived from the ultimate solution. ","fd645ddb":"All of our PCA variables,`V1, V2, ..., V28`, as mentioned before have been transformed.","f3721048":"Now we will revisit the variables `Time` and `Amount` to extract and scale them. [Feature scaling](https:\/\/en.wikipedia.org\/wiki\/Feature_scaling) is a set of methods used to normalize the range of explanatory variables.[$^{8}$](#8) The primary methods are: \n* [rescaling (min-max normalization)](https:\/\/en.wikipedia.org\/wiki\/Feature_scaling#Rescaling_(min-max_normalization))\n* [mean normalization](https:\/\/en.wikipedia.org\/wiki\/Feature_scaling#Mean_normalization)\n* [standardization (z-score normalization)](https:\/\/en.wikipedia.org\/wiki\/Feature_scaling#Standardization_(Z-score_Normalization))\n* [scaling to unit length](https:\/\/en.wikipedia.org\/wiki\/Feature_scaling#Scaling_to_unit_length)\n\nBaijayanta Roy does a great job outlining several reasons why scaling is desirable in support of some machine learning algorithms. In [All about Feature Scaling](https:\/\/towardsdatascience.com\/all-about-feature-scaling-bcc0ad75cb35) Roy notes that \"Neural network gradient descent converge much faster with feature scaling than without it.\"\n\nHe continues, \"Scaling is critical while performing **Principal Component Analysis (PCA)**. PCA tries to get the features with maximum variance, and the variance is high for high magnitude features and skews the PCA towards high magnitude features.\" [$^{9}$](#9)\n\nGiven the use of PCA in our dataset it seems highly advisable to use feature scaling for the `Time` and `Amount` variables. `Time` doesn't have any substantial outliers, however `Amount` is heavily right skewed resulting in a large number of outliers as seen in the [Matplotlib](https:\/\/matplotlib.org) boxplots below.[$^{10}$](#10) ","50c4d8e0":"<a id=\"model\"><\/a>\n## Modeling\n\nBefore we begin generating various predictive models from our Binary Classification problem it is important to discuss how we can assess model performance. [Amazon Machine Learning Developer Guide](https:\/\/docs.aws.amazon.com\/machine-learning\/latest\/dg\/binary-classification.html) indicates \"typical metrics are accuracy (ACC), precision, recall, false positive rate, and F1-measure\". [$^{21}$](#21)\n\nThese evaluation metrics are calculated from four basic combinations of actual data cateogry and assigned category:\n* **True Positive (TP)** - correct positive assignment\n* **True Negative (TN)** - correct negative assignment\n* **False Positive (FP)**  \\[Type I Error\\] - incorrect positive assignment   \n* **False Negative (FN)**  \\[Type II Error\\] - incorrect negative assignment    \n\nA confusion matrix, also known as an error matrix, is a 2x2 table that reflects these assignments. Each row of the matrix represents the instances in an actual class while each column represents the instances in a predicted class (or vice versa)[$^{29}$](#29)\n<p><\/p>\n\n<img src=\"https:\/\/2.bp.blogspot.com\/-EvSXDotTOwc\/XMfeOGZ-CVI\/AAAAAAAAEiE\/oePFfvhfOQM11dgRn9FkPxlegCXbgOF4QCLcBGAs\/s1600\/confusionMatrxiUpdated.jpg\" \/>\n\n\nAccuracy (ACC) is the fraction of correct assignment (TP + TN)\/(TP + TN + FP + FN), precision is the fraction of TP\/(TP + FP), recall a.k.a. sensitivity is the fraction TP\/(TP + FN), false positive rate is FP\/(TN + FP), and finally F1-measure or F1-score is the \"harmonic mean of precision and recall\"[$^{22}$](#22)[$^{23}$](#23) \n\n<p><center>$\\normalsize F_1= \\frac{2}{recall^{-1} + precision^{-1}} = 2  \\frac{precision * recall}{precision + recall}=\\frac{TP}{TP + \\frac{1}{2}(FP + FN)}$<\/center><\/p>\n\nDepending on the use case one might be more interested in a model that performs well for a specific metric or subset of metrics over others. For credit card fraud it is important that we have both high precision and high recall. F1-measure is often considered to be the most relevant measure in fraud detection due to its combination of of precision and recall into a single metric.[$^{24}$](#24) Now that we know we need to evaluate our models to favor high recall or high sensitivity we need to split our data into train and test sets.","6827a2db":"<a id=\"understand\"><\/a>\n### Understand - Structure & Variable Distribution\nWe start by reading in the credit card transaction data set CSV file as a pandas [DataFrame](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.html) object and look a sample of some records using the [sample](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.sample.html) method. Then begin exploring using the [describe](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.describe.html) method to obtain descriptive statistics about each of the available variables.[$^{6}$](#6)","6778438b":"Our final boosting technique, described as an \"efficient and scalable implementation of gradient boosting framwork by Friedman et al.\" [$^{45}$](#45), eXtreme Gradient Boost (XGB) uses a \"more regularized model formalization to control over-fitting, which gives it better performance\" [$^{46}$](#46) compared with Gradient Boosting Machine.","483be61d":"Next we will look to balance the dataset to avoid overfitting in our downstream models. \"Many real world classification problems are represented by highly unbalanced data sets, in which, the number of samples from one class is much smaller than from another. This is known as class imbalance problem and is often reported as an obstacle to constructing a model that can successfully discriminate the minority samples from the majority samples.\"[$^{12}$](#12)\n\nTo address the class imbalance problem we will utilize a combination of Random Under Sampling and SMOTE for over sampling. Random Under Sampling, a technique where we identify _n_, the minimum observations between our response variable classes, then take a random sample of size _n_ from the class with more observations to have a resulting 50\/50 ratio. It should be noted that this process can result in loss of information but depending on the use case may have good results.[$^{13}$](#13) \n\n<img src=https:\/\/miro.medium.com\/max\/3348\/1*P93SeDGPGw0MhwvCcvVcXA.png \/>\n\nSMOTE, which stands for Synthetic Minority Over-sampling Technique, over-samples the minority class by taking each minority class sample and introducing synthetic examples along the line segments joining any\/all of the _k_ minority class nearest neighbors. Synthetic samples are generated by taking the difference between the sample under consideration and its nearest neighbor, multiplying the difference by a random number between 0 and 1 then adding it to the sample under consideration. This technique ultimately forces the decision region of the minority class to become more general.[$^{25}$](#25)  \n\n<img src=https:\/\/miro.medium.com\/max\/734\/1*yRumRhn89acByodBz0H7oA.png \/>\n\nWe happen to have **492 Fraud** transactions and **284315 Normal** transactions so we will use Random Under Sampling to sample 49200 observations from the normal subset and SMOTE to generate 49200 Fraud observations. ","3de7d363":"Looking closer we can see some variety in the distributions even though all the PCA variables have mean values of `0`. Distributions of variables `V4`, `V12`, and `V28` are shown below. `V4` shows a slightly right skewed, `V12` appears to be left skewed, and `V28` shows a very narrow distribution. We will look at these distributions with respect to our target variable `Class` later as it might be that the normal transactions follow a normal distribution and that the fraudulent transactions are what are skewing these plots.","32af85b6":"We will be using RobustScaler from the [Scikit-learn](https:\/\/scikit-learn.org\/stable\/index.html) package because it is as the name suggests robust to outliers. According to the [sklearn.preprocessing.RobustScaler](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.RobustScaler.html) documentation, \n\n\"This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile).\n\nCentering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method.\"[$^{11}$](#11)","a69c0efa":"<h1 align=\"center\">Credit Card Fraud Detection<\/h1>","534d53c1":"<a id=\"intro\"><\/a>\n## Introduction\n\nAccording to the Federal Bureau of Investigation, \"Credit card fraud is the unauthorized use of a credit or debit card, or similar payment tool (ACH, EFT, recurring charge, etc.), to fraudulently obtain money or property. Credit and debit card numbers can be stolen from unsecured websites or can be obtained in an identity theft scheme.\"[$^{1}$](#1)\n\nIn the FTC's Consumer Sentinel Network Data Book 2019 it was noted that \"Credit card fraud tops the list of identity theft reports in 2019. The FTC received more than 271,000 reports from people who said their information was misused on an existing account or to open a new credit card account.\"[$^{2}$](#2) The Nilson Report, the leading global card and mobile payments trade publication, releases annual fraud statistics and indicated in the November 2019 issue that global losses had amounted to \\\\$27.85 billion. The United States which represents 21.54% of transactions accounted for \\\\$9.47 or 33.99% of the gross worldwide losses.[$^{3}$](#3)  \n\nWith data breaches becoming more common in recent years we can imagine that these numbers will continue to rise presenting an opportunity for credit card companies to take a proactive stance on detecting fraudulent charges quickly.[$^{4}$](#4) We will explore various models utilizing a credit card transaction data set from Kaggle and evaluating which has the best performance metrics for classifying an incoming transaction as fraudulent or normal. \n\nThe data set is provided by [The Machine Learning Group (MLG)](https:\/\/mlg.ulb.ac.be\/wordpress\/) of [Universit\u00e9 Libre de Bruxelles (ULB)](https:\/\/www.ulb.be) and contains transactions made by european cardholders in September 2013.","91909845":"The variables with largest negative correlations are `V14, V12, V10, V16, V3, V9` and the ones with the largest positive correlations are `V4, V11, V2, V19`. We will look at each of these next to understand how the transactions compare across the two classes. ","f8af738a":"`Amount` is a highly skewed variable with maximum observed value of `25,691.16` however most transactions averaging at only `88`. Again because of the wide distribution of values we will scale the values down and align with the PCA variables. We should also note that `Amount` does not have any unit associated with it. Given the fact that the data was sourced in Europe which has many currencies and the fact that we will be scaling the variable later we will leave the numeric value unitless.","e5ba5479":"Weak learners or base models are the building blocks for designing more complex models in ensemble theory. Since most errors in modeling are sourced from variance, noise, and bias ensemble methods that focus on reducing these errors typically fall into three \"meta-algorithms\", boosting, bagging, and stacking. The next couple of models will focus on boosting a technique where learners are combined sequentially.[$^{35}$](#35)[$^{36}$](#36) There are different approaches to optimizing boosting algorithms, hence the variety of boosting models available, however that is beyond scope for this notebook.\n\n<p><img src=\"https:\/\/pluralsight2.imgix.net\/guides\/81232a78-2e99-4ccc-ba8e-8cd873625fdf_2.jpg\" \/><\/p>","d2f14dc9":"The data set consists of `284807` observations of `31` variables, `30` explanatory and `1` response. Our data set does not contain any missing values making [imputation](https:\/\/en.wikipedia.org\/wiki\/Imputation_(statistics)) irrelevant. It should be noted that \"the problem of missing data is relatively common in almost all research and can have a significant effect on the conclusions that can be drawn from the data.\" [$^{7}$](#7) Various imputation methods can be taken when less than 30% of observations are missing at random, however, for scenarios where more than 30% of observations are missing or observations are not missing at random it is best practice to remove the variable(s) in question. ","8790564c":"Random Forest models are in a different \"meta-algorithm\" class than the previous boosting models called bagging. Bagging is a method where weak learners are created in parallel, as opposed to boosting's sequential approach, and ultimately aggregated to create a kind of average of their individual outputs.[$^{37}$](#37) Random Forests utilize parallel decision trees as shown in the image below. \n\n<p><img src=\"https:\/\/miro.medium.com\/max\/1306\/0*f_qQPFpdofWGLQqc.png\" \/><\/p>","7daa2c8b":"`Time` appears to be cyclical with periods every approximately `86000` seconds (24 hours). We will execute simple feature engineering to break out the various time components, e.g. hour, min, second. If the data contained at least 2 weeks worth of observations we could possibly extract the day of week component as there is often a weekly seasonality in transaction or sales data.[$^{30}$](#30) We will subsequently scale the new time component features to align with the PCA variables.  ","49f68d8c":"<a id=\"nn\"><\/a>\n### Neural Network\nScikit-learn's implementation of an artificial neural network is the MLPClassifier. Artifical neural networks, often referred to simply as neural networks, are \"a wide class of flexible nonlinear regression and discriminant models, data reduction models, and nonlinear dynamic systems\" inspired by biological neural networks. Typically represented  as layers of nodes connected to one another like neurons connected through synapses. Each node representing some simple mathematical process of its input. The layers are commonly designated as the input layer (first layer), output layer (last layer), and hidden layer(s) (inner layer(s)). Most neural networks are fully connected from one layer to another with particular weights applied to the connections. [$^{39}$](#39) [$^{40}$](#40)\n\n<p><img src=\"https:\/\/www.gyansetu.in\/wp-content\/uploads\/2020\/02\/Neural-Network-in-AI-min.png\" \/><\/p>\n\nNeural networks are amongst some of the most complex models in use at scale across industries. This complexity of course comes at a cost according to Geman et al. however recent papers have been investigating the U-shaped test error curve and refute this claim.[$^{41}$](#41) [$^{42}$](#42) Whether or not the bias-variance tradeoff is present in a neural network, one thing is certain, computational resources are required to execute and this can significantly increase the time for delivering a prediction. Our use case of credit card fraud may prohibit the use of this model but for the sake of evaluating the accuracy and its comparison to the previous models we will proceed.\n\n<p><img src=\"https:\/\/www.bradyneal.com\/img\/bias-variance\/fortmann-roe-bias-variance.png\" \/><\/p>","fad5f0e6":"Next up in the clean phase is outlier handling. Outliers can come in a variety of forms, the most common are **Global Anomalies** a.k.a. point anomalies, **Contextual Anomalies** a.k.a. conditional anomalies, and **Collective Anomalies**. These outliers are often the result of data entry errors, measurement errors, or are simply natural outliers.[$^{14}$](#14)\n\nJim Frost in [Guidelines for Removing and Handling Outliers in Data](https:\/\/statisticsbyjim.com\/basics\/remove-outliers\/) notes, \"Sometimes it\u2019s best to keep outliers in your data. They can capture valuable information that is part of your study area.\"[$^{15}$](#15) Others such as Tam\u00e1s Nyitrai and Mikl\u00f3s Vir\u00e1g in [The effects of handling outliers on the performance of bankruptcy prediction models](https:\/\/doi.org\/10.1016\/j.seps.2018.08.004) indicate that \"There seems to be consensus in the literature on the necessity of handling outliers, at the same time, it is not clear how to define extreme values to be handled in order to maximize the predictive power of models.\"[$^{16}$](#16)\n\nGiven the seemingly conflicting ideas around outlier handling we will opt to look at very extreme values and use imputation to replace the removed values which should provide us with overall improvements to our model accuracy metrics. While there are a number of different outlier detection techniques we will be using NumPy[$^{28}$](#28) and the IQR method, specifically targeting the most extreme values (3X IQR) applying a univariate approach. For imputation we will leverage [IterativeImputer](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.impute.IterativeImputer.html#sklearn.impute.IterativeImputer) from Scikit-learn to replace the removed values. \n\nIterativeImputer uses \"\\[a\\] strategy for imputing missing values by modeling each feature with missing values as a function of other features in a round-robin fashion\", which was inspired by the R MICE package (Multivariate Imputation by Chained Equations). This multivariate approach for imputation typically results in a significantly lower MSE when compared to simple univariate mean imputation. \n\nFor a summary of several outlier detection techniques including Z-score method, Winsorization method, Isolation Forest, and more see Naresh Bhat's [Outlier!!! The Silent Killer](https:\/\/www.kaggle.com\/nareshbhat\/outlier-the-silent-killer) notebook on Kaggle.[$^{17}$](#17) ","145eafce":"<a id=\"bdt\"><\/a>\n### Boosted Decision Tree","fae84df5":"Given that the goal of our project was to determine the most performative model it is worthwhile for us to assess the speed of the models for both fitting and predicting. We should also note here that this dataset contains just two days of transactions and the true population size is unknown however we can assume that the population is considerably larger. This is important to know because some hyperparameter changes would likely be required for implementation, specifically the LR **solver** and **n_jobs** parameters. As noted in the Scikit-learn documentation _\"For small datasets, \u2018liblinear\u2019 is a good choice, whereas \u2018sag\u2019 and \u2018saga\u2019 are faster for large ones.\"_ ","84f22274":"Another way to compare the distributions of the remaining variables with respect to `Class` is by using histogram plots with kernel density estimates (KDE) overlay. We have color coded normal transactions in <span style=\"color:gray\"> **gray** <\/span> and fraud transactions in <span style=\"color:red\"> **red** <\/span> as we have done earlier in this notebook and as indicated in the legend for each plot. <a id=\"kde\"><\/a>","9605273d":"<a id=\"forest\"><\/a>\n### Random Forest","b42937ce":"We continue by looking at the correlation across our explanatory variables. \"Correlation analysis is a statistical method used to evaluate the strength of relationship between two quantitative variables. A high correlation means that two or more variables have a strong relationship with each other, while a weak correlation means that the variables are hardly related.\"[$^{18}$](#18) \n\nWe will be using the Pearson correlation coefficient (PCC) as our statistic for measuring the relationships between our variables. PCC, also referred to as Pearson's _r_, is a statistic that measures the linear correlation between two variables _X_ and _Y_ and has a value between -1 and +1. Variables with a positive correlation (PCC > 0) behave in a manner in which the increase in one of the variables results in an increase in the other. The opposite can be said of variables with a negative correlation (PCC < 0), an increase in one of the variables results in a decrease in the other. Variables with PCC close to 0 have no linear correlation.[$^{19}$](#19)\n\nBelow is a heatmap generated using the [Seaborn](https:\/\/seaborn.pydata.org\/index.html) data visualization library and reflecting the Pearson correlation coefficient between all pairs of variables.[$^{20}$](#20) <span style=\"color:darkblue\"> **Dark blue**<\/span> indicating strong positive correlation (PCC $\\approx$ +1) and <span style=\"color:darkred\"> **dark red**<\/span> indicating strong negative correlation (PCC $\\approx$ -1).","927b1c6d":"Feature scaling particularly benefits machine learning models because it allows for improved gradient descent requiring less iterations to arrive at minimum cost $ J(\\theta)$. There are a number of great resources around cost functions, $ J(\\theta)$, including [Andrew Ng's Machine Learning](https:\/\/www.coursera.org\/learn\/machine-learning) course on Coursera. \"Put simply, a cost function is a measure of how wrong the model is in terms of its ability to estimate the relationship between X and y.\"[$^{38}$](#38) Gradient descent is an optimization algorithm that aims at finding the local or global minima of the aforementioned cost function. \n\n<img src=\"https:\/\/static.commonlounge.com\/fp\/600w\/HUvkPMB4OlJqQU7LzAZXdWzgw1556694254_kc\" \/>","d3b2d2a5":"Before we proceed to clean our data set we will make a copy to compare with later. ","a069e09a":"Using the statsmodels[$^{32}$](#32) library to do a basic logistic regression we can see all the variables are significant with _p-value_ < _0.05_ and the model has a resulting **_AIC_ = 46.95** which we will use as a baseline for comparing against a simplified logisitic regression model later.","4d0f0bce":"**NOTE:** _We will be using a random search approach for selecting optimal model hyperparameters for all models shown below. Additionally, we will be looking at the delta (difference) of F1 scores between train and test sets. If F1 delta is > 0 then we can conclude the model does not overfit and can generalize for unseen observations. If F1 delta is < 0 then we would say the model is overfitting and does not generalize for unseen observations. For more details on hyperparameter tuning please refer to \"Hypterparameter Tuning for Machine Learning Models\" by Jeremy jordan and \"Random Search for Hyper-Parameter Optimization\" by James Bergstra et al. [$^{33}$](#33)[$^{34}$](#34) For more details on overfitting please refer to \"The Problem of Overfitting\" by Douglas M. Hawkins._ [$^{31}$](#31)","a88bea98":"Using the same statsmodels approach as we did for the basic logistic regression in the [analyze relationships - correlation](#relate) section in the two cells below the resulting **_AIC_ = _25.1_**. This AIC value is very similar to that of the AIC from the Scikit-learn logisitc regression model above which had the value of **_25.9_**. This demonstrates that both the statsmodels library and the Scikit-learn library have similiar outcomes and that our efforts for feature reduction through the correlation analysis was the correct approach as these models fit our data better than the basic logisitic regression. ","e164879a":"<a id=\"gbm\"><\/a>\n### Gradient Boosting Machine","63997e0f":"### Logisitic Regression\n<a id=\"logistic\"><\/a>\n\nLogistic model uses the logistic or inverse logit function to model a binary or dichotomous variable. Similar to linear regression, logistic regression fits the data to a line however unlike linear regression the line is not straight but rather an S shaped curved called **Sigmoid**. The S shaped curve is bound on the Y axis between 0 and 1 which fits our goal of binary classification. The sigmoid function (shown below) provides us with a probability of an observation belonging to one of the two classes. [$^{26}$](#26) [$^{27}$](#27)\n\n<p><center>$\\normalsize sigmoid(x)= \\frac{1}{1+e^{-x}} = \\frac{e^{x}}{e^{x}+1}$<\/center><\/p>\n<p><\/p>\nRepresentative plots for a linear regression and logistic regression can be seen below.\n<p><img src=\"https:\/\/miro.medium.com\/max\/1400\/1*dm6ZaX5fuSmuVvM4Ds-vcg.jpeg\" \/><\/p>","080bbc2f":"<a id=\"clean\"><\/a>\n### Clean - Feature Engineer, Scale, Balance, & Outlier Handling\n","e90a1297":"<a id=\"outro\"><\/a>\n## Conclusions\nAfter evaluating all of our models and exclusively looking at their F1 scores, the **Boosted Decision Tree**, which leveraged an adaptive boosting technique performed the best. Interestingly if we were to use traditional statistics for evaluating model goodness of fit the **Logistic Regression** would be selected based on it minimizing the AIC. The LR model was also the only one that did not show signs of overfitting as seen with an F1 delta > 0 although the XGB model did have the smallest measure of overfitting.","595c456e":"---\n### [Introduction](#intro)\n### [Objective](#objective)\n### [Exploratory Data Analysis (EDA)](#eda)\n* [Understand - Structure & Variable Distribution](#understand)\n* [Clean - Feature Engineer, Scale, Balance, & Outlier Handling](#clean)\n* [Analyze Relationships - Correlation](#relate)\n\n### [Modeling](#model)\n* [Logistic Regression](#logistic)\n* [Gradient Boosting Machine](#gbm)\n* [Boosted Decision Tree](#bdt)\n* [Extreme Gradient Boost](#xgb)\n* [Random Forest](#forest)\n* [Neural Network](#nn)\n\n### [Conclusions](#outro)\n### [Citations](#citations)\n---","fb2b4c9d":"The variables with the lowest correlations (+\/- 0.1) can be removed as they will not provide any valuable information in our models because the distributions are too similar between the two classes. ","9c2e5ec6":"We will create a simple function for investigating goodness of fit using traditional AIC statistic. This statistic is a small component for comparing models we will discuss others in the modeling section. ","ad01fcab":"<a id=\"xgb\"><\/a>\n### Extreme Gradient Boost","cf411b43":"We wrap up the notebook with a quick review of feature importance. Scikit-learn's LogisticRegression model does not directly output feature importance like the ExtraTreesClassifier, RandomForestClassifier, or RecursiveFeatureElimination methods, however, one can calculate the relative feature importance as shown below. This approach uses the relative weights of the coefficients produced by the model. The top 5 important features are `V14, V12, V4, V10, V17` which unsurprisingly are also the 5 variables with the largest separation between the fraud and normal distributions as seen in the [histogram\/KDE plots](#kde) from the [analyze relationships - correlation section](#relate). ","9f2dae16":"<a id=\"relate\"><\/a>\n### Analyze Relationships - Correlation","ba2a20d3":"Looking at the response variable `Class` we see the dataset is very imbalanced with only **0.17%** of observations result in a fraudulent transactions. This is to be expected, if fraudulent transactions were very common credit card companies would lose money and likely cease to operate. The topic of balancing will be explored the next section but lets continue reviewing the structure and distribution of our available variables.","4e7e920b":"While not in scope for our discussions there is a whole field of study around distributed computing and the architecture behind deploying ML solutions at scale that could shift practitioners decisions on the optimal solution.[$^{43}$](#43) Based on our criterion for model comparison and evaluation the **Logistic Regression** has been identified as the best solution for our credit card fraud detection problem. Lets dive a bit deeper into this solution and take a look into the effects quantification and feature importance. ","59125d73":"Listing the PCC for all explanatory variables with respect to our dependent variable `Class` in ascending order","18443fbf":"Below we compare boxplots from our original dataframe and the now cleaned dataframe. Notice the y-axis scale is significantly reduced in our cleaned dataframe and the boxes are actually visible for most of our variables.","5d0fd512":"<div style=\"text-align:center\"><img src=\"https:\/\/cdn.cliqz.com\/wp-content\/uploads\/2018\/12\/Blog_trackers-who-steal-uai-1440x900.png\" \/><\/div>","9a5fccec":"The first boosting model we will evaluate is GradientBoostingClassier sometimes referred to as Gradient Boosting Machine which operates in the purest sense of the boosting method building weak learners on the previous weak learner.[$^{37}$](#37)","46bad0da":"#### <a id=\"citations\"><\/a>\n## Citations\n\n<a id=1><\/a>$^{1}$Credit Card Fraud. 15 June 2016, www.fbi.gov\/scams-and-safety\/common-scams-and-crimes\/credit-card-fraud. \n\n<a id=2><\/a>$^{2}$Federal Trade Commission . (2019). Consumer Sentinel Network data book for January\u2013December 2019. Washington, DC. Retrieved from https:\/\/www.ftc.gov\/system\/files\/documents\/reports\/consumer-sentinel-network-data-book-2019\/consumer_sentinel_network_data_book_2019.pdf\n\n<a id=3><\/a>$^{3}$\"The Nilson Report Newsletter Archive\", The Nilson Report, November 2019, https:\/\/nilsonreport.com\/upload\/content_promo\/The_Nilson_Report_Issue_1164.pdf.\n\n<a id=4><\/a>$^{4}$Sobers, Rob. \u201c107 Must-Know Data Breach Statistics for 2020.\u201d Varonis, 24 Sept. 2020, www.varonis.com\/blog\/data-breach-statistics\/\n\n<a id=5><\/a>$^{5}$Bourke, Daniel. \u201cA Gentle Introduction to Exploratory Data Analysis.\u201d Medium, Towards Data Science, 13 Feb. 2019, towardsdatascience.com\/a-gentle-introduction-to-exploratory-data-analysis-f11d843b8184. \n\n<a id=6><\/a>$^{6}$[Data structures for statistical computing in python](https:\/\/conference.scipy.org\/proceedings\/scipy2010\/pdfs\/mckinney.pdf), McKinney, Proceedings of the 9th Python in Science Conference, Volume 445, 2010.\n\n<a id=7><\/a>$^{7}$Kang H. The prevention and handling of the missing data. Korean J Anesthesiol. 2013;64(5):402-406. doi: [10.4097\/kjae.2013.64.5.402](https:\/\/doi.org\/10.4097\/kjae.2013.64.5.402)\n\n<a id=8><\/a>$^{8}$Wikipedia contributors. \"Feature scaling.\" Wikipedia, The Free Encyclopedia. Wikipedia, The Free Encyclopedia, 28 Oct. 2020. Web. 1 Nov. 2020.\n\n<a id=9><\/a>$^{9}$Roy, Baijayanta. \u201cAll about Feature Scaling.\u201d Medium, Towards Data Science, 7 Apr. 2020, towardsdatascience.com\/all-about-feature-scaling-bcc0ad75cb35. \n\n<a id=10><\/a>$^{10}$J. D. Hunter, \"Matplotlib: A 2D Graphics Environment,\" in Computing in Science & Engineering, vol. 9, no. 3, pp. 90-95, May-June 2007, doi: [10.1109\/MCSE.2007.55](https:\/\/doi.org\/10.1109\/MCSE.2007.55)\n\n<a id=11><\/a>$^{11}$[Scikit-learn: Machine Learning in Python](https:\/\/jmlr.csail.mit.edu\/papers\/v12\/pedregosa11a.html), Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.\n\n<a id=12><\/a>$^{12}$Muhammad Atif Tahir, Josef Kittler, Fei Yan,\nInverse random under sampling for class imbalance problem and its application to multi-label classification, Pattern Recognition, Volume 45, Issue 10, 2012, Pages 3738-3750,ISSN 0031-3203, doi: [10.1016\/j.patcog.2012.03.014](https:\/\/doi.org\/10.1016\/j.patcog.2012.03.014)\n\n<a id=13><\/a>$^{13}$Dataman, Dr. \u201cUsing Under-Sampling Techniques for Extremely Imbalanced Data.\u201d Medium, Medium, 27 Oct. 2020, medium.com\/@Dataman.ai\/sampling-techniques-for-extremely-imbalanced-data-part-i-under-sampling-a8dbc3d8d6d8. \n\n<a id=14><\/a>$^{14}$Cohen, Ira. \u201cOutliers Explained: a Quick Guide to the Different Types of Outliers.\u201d Medium, Towards Data Science, 8 Nov. 2018, towardsdatascience.com\/outliers-analysis-a-quick-guide-to-the-different-types-of-outliers-e41de37e6bf6. \n\n<a id=15><\/a>$^{15}$Frost, Jim. \u201cGuidelines for Removing and Handling Outliers in Data.\u201d Statistics By Jim, 6 June 2020, statisticsbyjim.com\/basics\/remove-outliers\/. \n\n<a id=16><\/a>$^{16}$Nyitrai, Tam\u00e1s, and Mikl\u00f3s Vir\u00e1g. \u201cThe Effects of Handling Outliers on the Performance of Bankruptcy Prediction Models.\u201d Socio-Economic Planning Sciences, vol. 67, 2019, pp. 34\u201342., doi:[10.1016\/j.seps.2018.08.004](https:\/\/doi.org\/10.1016\/j.seps.2018.08.004) \n\n<a id=17><\/a>$^{17}$Bhat, Naresh. \u201cOutlier!!! The Silent Killer.\u201d Kaggle, Kaggle, 26 Oct. 2020, www.kaggle.com\/nareshbhat\/outlier-the-silent-killer. \n\n\n<a id=18><\/a>$^{18}$Franzese, Monica, and Antonella Iuliano. \u201cCorrelation Analysis.\u201d Encyclopedia of Bioinformatics and Computational Biology, 2019, pp. 706\u2013721., doi:[10.1016\/b978-0-12-809633-8.20358-0](https:\/\/doi.org\/10.1016\/b978-0-12-809633-8.20358-0) \n\n<a id=19><\/a>$^{19}$Wikipedia contributors. \"Pearson correlation coefficient.\" Wikipedia, The Free Encyclopedia. Wikipedia, The Free Encyclopedia, 31 Oct. 2020. Web. 2 Nov. 2020.\n\n<a id=20><\/a>$^{20}$Waskom, Michael et al. \u201cMwaskom\/Seaborn.\u201d 0.11.0, Zenodo, Sept. 2020, doi.org\/10.5281\/zenodo.592845. \n\n<a id=21><\/a>$^{21}$Mitchell, Tom M. \u201cMachine Learning.\u201d Amazon, McGraw Hill, 2017, docs.aws.amazon.com\/machine-learning\/latest\/dg\/binary-classification.html. \n\n<a id=22><\/a>$^{22}$Wikipedia contributors. \"Sensitivity and specificity.\" Wikipedia, The Free Encyclopedia. Wikipedia, The Free Encyclopedia, 20 Oct. 2020. Web. 2 Nov. 2020.\n\n<a id=23><\/a>$^{23}$Wikipedia contributors. \"F-score.\" Wikipedia, The Free Encyclopedia. Wikipedia, The Free Encyclopedia, 22 Oct. 2020. Web. 2 Nov. 2020.\n\n<a id=24><\/a>$^{24}$Pozzolo, Andrea Dal, et al. \u201cLearned Lessons in Credit Card Fraud Detection from a Practitioner Perspective.\u201d Expert Systems with Applications, vol. 41, no. 10, 2014, pp. 4915\u20134928., doi:[10.1016\/j.eswa.2014.02.026](https:\/\/doi.org\/10.1016\/j.eswa.2014.02.026) \n\n<a id=25><\/a>$^{25}$Chawla, N. V., et al. \u201cSMOTE: Synthetic Minority Over-Sampling Technique.\u201d Journal of Artificial Intelligence Research, vol. 16, 2002, pp. 321\u2013357., doi:[10.1613\/jair.953](https:\/\/doi.org\/10.1613\/jair.953) \n\n<a id=26><\/a>$^{26}$Zornoza, Jaime. \u201cLogistic Regression Explained.\u201d Medium, Towards Data Science, 13 Oct. 2020, towardsdatascience.com\/logistic-regression-explained-9ee73cede081. \n\n<a id=27><\/a>$^{27}$Chao-Ying Joanne Peng, Kuk Lida Lee & Gary M. Ingersoll (2002) An Introduction to Logistic Regression Analysis and Reporting, The Journal of Educational Research, 96:1, 3-14, DOI: [10.1080\/00220670209598786](https:\/\/doi.org\/10.1080\/00220670209598786) \n\n<a id=28><\/a>$^{28}$Harris, C.R., Millman, K.J., van der Walt, S.J. et al. Array programming with NumPy. Nature 585, 357\u2013362 (2020). DOI: [0.1038\/s41586-020-2649-2](https:\/\/doi.org\/0.1038\/s41586-020-2649-2). ([Publisher link](https:\/\/www.nature.com\/articles\/s41586-020-2649-2))\n\n<a id=29><\/a>$^{29}$Powers, David M W (2011). \"Evaluation: From Precision, Recall and F-Measure to ROC, Informedness, Markedness & Correlation\". Journal of Machine Learning Technologies. 2 (1): 37\u201363. S2CID 55767944\n\n<a id=30><\/a>$^{30}$Choi, Tsan-Ming, et al. \u201cA Hybrid SARIMA Wavelet Transform Method for Sales Forecasting.\u201d Decision Support Systems, vol. 51, no. 1, 2011, pp. 130\u2013140., doi:[10.1016\/j.dss.2010.12.002](https:\/\/doi.org\/10.1016\/j.dss.2010.12.002) \n\n<a id=31><\/a>$^{31}$Hawkins, Douglas M. \u201cThe Problem of Overfitting.\u201d Journal of Chemical Information and Computer Sciences, vol. 44, no. 1, 2004, pp. 1\u201312., doi:[10.1021\/ci0342472](https:\/\/doi.org\/10.1021\/ci0342472)\n\n<a id=32><\/a>$^{32}$Seabold, Skipper, and Josef Perktold. \u201c[statsmodels: Econometric and statistical modeling with python.](http:\/\/conference.scipy.org\/proceedings\/scipy2010\/pdfs\/seabold.pdf)\u201d Proceedings of the 9th Python in Science Conference. 2010.\n\n<a id=33><\/a>$^{33}$Jordan, Jeremy. \u201cHyperparameter Tuning for Machine Learning Models.\u201d Jeremy Jordan, Jeremy Jordan, 5 Dec. 2018, www.jeremyjordan.me\/hyperparameter-tuning\/. \n\n<a id=34><\/a>$^{34}$Bergstra, James, and Yoshua Bengio. \u201cRandom Search for Hyper-Parameter Optimization.\u201d The Journal of Machine Learning Research, 1 Feb. 2012, [dl.acm.org\/doi\/abs\/10.5555\/2188385.2188395](https:\/\/dl.acm.org\/doi\/abs\/10.5555\/2188385.2188395). \n\n<a id=35><\/a>$^{35}$DeFilippi, Robert R.F. \u201cBoosting, Bagging, and Stacking - Ensemble Methods with Sklearn and Mlens.\u201d Medium, Medium, 4 Aug. 2018, medium.com\/@rrfd\/boosting-bagging-and-stacking-ensemble-methods-with-sklearn-and-mlens-a455c0c982de. \n\n<a id=36><\/a>$^{36}$T. M. Khoshgoftaar, J. Van Hulse and A. Napolitano, \"Comparing Boosting and Bagging Techniques With Noisy and Imbalanced Data,\" in IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans, vol. 41, no. 3, pp. 552-568, May 2011, doi: 10.1109\/TSMCA.2010.2084081.\n\n<a id=37><\/a>$^{37}$Rocca, Joseph. \u201cEnsemble Methods: Bagging, Boosting and Stacking.\u201d Medium, Towards Data Science, 5 May 2019, towardsdatascience.com\/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205. \n\n<a id=38><\/a>$^{38}$McDonald, Conor. \u201cMachine Learning Fundamentals (I): Cost Functions and Gradient Descent.\u201d Medium, Towards Data Science, 5 Nov. 2018, towardsdatascience.com\/machine-learning-fundamentals-via-linear-regression-41a5d11f5220. \n\n<a id=39><\/a>$^{39}$Sarle, Warren S. \"Neural Networks and Statistical Models\". SAS Institute Inc., 1994, http:\/\/citeseerx.ist.psu.edu\/viewdoc\/download?doi=10.1.1.1066.9086&rep=rep1&type=pdf\n\n<a id=40><\/a>$^{40}$Marr, Bernard. \u201cWhat Are Artificial Neural Networks - A Simple Explanation For Absolutely Anyone.\u201d Forbes, Forbes Magazine, 24 Sept. 2018, www.forbes.com\/sites\/bernardmarr\/2018\/09\/24\/what-are-artificial-neural-networks-a-simple-explanation-for-absolutely-anyone\/?sh=39cd8be51245. \n\n<a id=41><\/a>$^{41}$Geman, Stuart, et al. \u201cNeural Networks and the Bias\/Variance Dilemma.\u201d Neural Computation, vol. 4, no. 1, 1992, pp. 1\u201358., doi:[10.1162\/neco.1992.4.1.1](https:\/\/doi.org\/10.1162\/neco.1992.4.1.1). \n\n<a id=42><\/a>$^{42}$Neal, Brady, et al. \u201cA Modern Take on the Bias-Variance Tradeoff in Neural Networks.\u201d ArXiv.org, 18 Dec. 2019, arxiv.org\/abs\/1810.08591. \n\n<a id=43><\/a>$^{43}$Buyya, Rajkumar, et al. \u201cPrinciples of Parallel and Distributed Computing.\u201d Mastering Cloud Computing, Morgan Kaufmann, 12 Apr. 2013, www.sciencedirect.com\/science\/article\/pii\/B9780124114548000024. \n\n<a id=44><\/a>$^{44}$Raoniar, Rahul. \u201cFitting MLR and Binary Logistic Regression Using Python (Research-Oriented Modeling &amp;...\u201d Medium, Towards Data Science, 9 Oct. 2020, towardsdatascience.com\/fitting-mlr-and-binary-logistic-regression-using-python-research-oriented-modeling-dcc22f1f0edf. \n\n<a id=45><\/a>$^{45}$Chen, Tianqi, and Tong He. \u201cXgboost: EXtreme Gradient Boosting.\u201d The Comprehensive R Archive Network, 2 Sept. 2020, cran.r-project.org\/web\/packages\/xgboost\/vignettes\/xgboost.pdf. \n\n<a id=46><\/a>$^{46}$Chen, Tianqi. \u201cWhat Is the Difference between the R Gbm (Gradient Boosting Machine) and Xgboost (Extreme Gradient Boosting)?\u201d Quora, 3 Sept. 2015, www.quora.com\/What-is-the-difference-between-the-R-gbm-gradient-boosting-machine-and-xgboost-extreme-gradient-boosting\/answer\/Tianqi-Chen-1?srid=8Ze. \n\nAndrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. [Calibrating Probability with Undersampling for Unbalanced Classification](https:\/\/www.researchgate.net\/publication\/283349138_Calibrating_Probability_with_Undersampling_for_Unbalanced_Classification). In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015\n\nDal Pozzolo, Andrea [Adaptive Machine learning for credit card fraud detection](http:\/\/di.ulb.ac.be\/map\/adalpozz\/pdf\/Dalpozzolo2015PhD.pdf) ULB MLG PhD thesis (supervised by G. Bontempi)\n\nCarcillo, Fabrizio; Dal Pozzolo, Andrea; Le Borgne, Yann-A\u00ebl; Caelen, Olivier; Mazzer, Yannis; Bontempi, Gianluca. [Scarff: a scalable framework for streaming credit card fraud detection with Spark](https:\/\/www.researchgate.net\/publication\/319616537_SCARFF_a_Scalable_Framework_for_Streaming_Credit_Card_Fraud_Detection_with_Spark), Information fusion,41, 182-194,2018,Elsevier\n\nCarcillo, Fabrizio; Le Borgne, Yann-A\u00ebl; Caelen, Olivier; Bontempi, Gianluca. [Streaming active learning strategies for real-life credit card fraud detection: assessment and visualization](https:\/\/www.researchgate.net\/publication\/332180999_Deep-Learning_Domain_Adaptation_Techniques_for_Credit_Cards_Fraud_Detection), International Journal of Data Science and Analytics, 5,4,285-300,2018,Springer International Publishing\n\nBertrand Lebichot, Yann-A\u00ebl Le Borgne, Liyun He, Frederic Obl\u00e9, Gianluca Bontempi [Deep-Learning Domain Adaptation Techniques for Credit Cards Fraud Detection](https:\/\/www.researchgate.net\/publication\/332180999_Deep-Learning_Domain_Adaptation_Techniques_for_Credit_Cards_Fraud_Detection), INNSBDDL 2019: Recent Advances in Big Data and Deep Learning, pp 78-88, 2019\n\nFabrizio Carcillo, Yann-A\u00ebl Le Borgne, Olivier Caelen, Frederic Obl\u00e9, Gianluca Bontempi [Combining Unsupervised and Supervised Learning in Credit Card Fraud Detection](https:\/\/www.researchgate.net\/publication\/333143698_Combining_Unsupervised_and_Supervised_Learning_in_Credit_Card_Fraud_Detection) Information Sciences, 2019\n\nDal Pozzolo, Andrea; Boracchi, Giacomo; Caelen, Olivier; Alippi, Cesare; Bontempi, Gianluca. [Credit card fraud detection: a realistic modeling and a novel learning strategy](https:\/\/www.researchgate.net\/publication\/319867396_Credit_Card_Fraud_Detection_A_Realistic_Modeling_and_a_Novel_Learning_Strategy), IEEE transactions on neural networks and learning systems,29,8,3784-3797,2018,IEEE"}}