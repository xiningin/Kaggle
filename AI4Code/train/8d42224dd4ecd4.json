{"cell_type":{"865f239b":"code","e2991082":"code","af44bf7a":"code","f1291735":"code","34c528ee":"code","fcde5ef4":"code","88c27da8":"code","44f7facc":"code","4da92ff3":"code","57d1e292":"code","c9b7eb86":"code","ae94304d":"code","f7da85a5":"code","061f87e7":"code","7f59fd65":"code","c3f8b09d":"code","47e3ddb5":"code","70c6e7c8":"code","9f7a8a64":"code","5d2fcdf6":"code","7a3aab31":"code","c6355bd7":"markdown","e4efc03a":"markdown","78dde1a3":"markdown","9ab6ce8c":"markdown","dd1465f4":"markdown"},"source":{"865f239b":"import torch\nfrom torch import nn, optim\nimport torch.nn.functional as F\nfrom torchvision.transforms import transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom nltk.tokenize import word_tokenize\nfrom tqdm.auto import tqdm\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","e2991082":"class Flickr8k(Dataset):\n    \n    def __init__(self, root_file, captions, transform=None):\n        \n        self.transform = transform\n        self.root = root_file\n        self.ids = captions\n        \n    def __getitem__(self, idx):\n        \n        image_path, caption = self.ids[idx]\n        image = Image.open(self.root+image_path)\n        if self.transform: \n            image = self.transform(image)\n\n        return image, caption\n    \n    def __len__(self):\n        return self.ids.shape[0]\n        \n        ","af44bf7a":"def build_datasets_vocab(root_file, captions_file, transform, split=0.15):\n    df = pd.read_csv(captions_file)\n    \n    vocab = {}\n    def create_vocab(caption):\n        tokens = [token.lower() for token in word_tokenize(caption)]\n        for token in tokens: \n            if token not in vocab: \n                vocab[token] = len(vocab)\n            \n    df[\"caption\"].apply(create_vocab)\n    \n    train, valid = train_test_split(df, test_size=split, random_state=42)\n    return Flickr8k(root_file, train.values, transform), \\\n           Flickr8k(root_file, valid.values, transform), \\\n           vocab\n    \n        ","f1291735":"transform = transforms.Compose([\n    transforms.Resize((128, 128)),\n    transforms.ToTensor()\n])\ntrain_dataset, valid_dataset, vocab = build_datasets_vocab(\"..\/input\/flickr8k\/Images\/\", \n                                              \"..\/input\/flickr8k\/captions.txt\",\n                                              transform)\n\nid_to_word = {id_: word for word, id_ in vocab.items()}","34c528ee":"df = pd.read_csv(\"..\/input\/flickr8k\/captions.txt\")\n# MAX_CAPTION_LEN = df[\"caption\"].apply(lambda x: len(word_tokenize(x))).max()\nMAX_CAPTION_LEN = 38","fcde5ef4":"def transform_captions(captions):\n    \n    transformed = [[vocab[word.lower()] for word in word_tokenize(caption)] for caption in captions]\n    padded = [transform + [vocab[\".\"]]*(MAX_CAPTION_LEN - len(transform)) for transform in transformed]\n    \n    return padded","88c27da8":"def get_caption(caption_sequence):\n    \n    return \" \".join([id_to_word[id_] for id_ in caption_sequence if id_ != vocab[\".\"]])","44f7facc":"# Constants\n\nPOOLING_FACTOR = 32\n","4da92ff3":"class ConvLeak(nn.Module):\n    \n    def __init__(self, in_channels, out_channels, kernel_size=5):\n        \n        super().__init__()\n        self.layer = nn.Sequential(\n            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, \n                      kernel_size=kernel_size, padding=(kernel_size-1)\/\/2),\n            nn.LeakyReLU(),\n        )\n        \n    def forward(self, x):\n        return self.layer(x)\n    \n    \nclass ConvTransposeLeak(nn.Module):\n    \n    def __init__(self, in_channels, out_channels, kernel_size=5):\n        super().__init__()\n        self.layer = nn.Sequential(\n            nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels, \n                               kernel_size=kernel_size, padding=(kernel_size-1)\/\/2),\n            nn.LeakyReLU(),\n        )\n        \n    def forward(self, x):\n        return self.layer(x) ","57d1e292":"class Encoder(nn.Module):\n    \n    def __init__(self, in_channels, out_channels, image_dim, latent_dim):\n        \n        super().__init__()\n        \n        # Some constants\n        \n        iW, iH = image_dim\n        hW, hH = iW\/\/POOLING_FACTOR, iH\/\/POOLING_FACTOR\n        vec_dim = out_channels * hW * hH\n        \n        self.layer1 = nn.Sequential(\n            ConvLeak(in_channels=in_channels, out_channels=48),\n            ConvLeak(in_channels=48, out_channels=48)\n        )\n        self.layer2 = nn.Sequential(\n            ConvLeak(in_channels=48, out_channels=84),\n            ConvLeak(in_channels=84, out_channels=84)\n        )\n        self.layer3 = nn.Sequential(\n            ConvLeak(in_channels=84, out_channels=128),\n            ConvLeak(in_channels=128, out_channels=128)\n        )\n#         self.layer4 = nn.Sequential(\n#             ConvLeak(in_channels=128, out_channels=192),\n#             ConvLeak(in_channels=192, out_channels=192)\n#         )\n        self.layer4 = nn.Sequential(\n            ConvLeak(in_channels=128, out_channels=out_channels),\n            nn.Flatten()\n        )\n        \n        self.pooling = nn.MaxPool2d(4, return_indices=True)\n        self.pooling_2 = nn.MaxPool2d(2, return_indices=True) \n        \n        \n        self.hidden = nn.Sequential(\n            nn.Linear(in_features = vec_dim, out_features=latent_dim),\n            nn.LeakyReLU(),\n            nn.Linear(in_features=latent_dim, out_features=latent_dim),\n            nn.Tanh()\n        )\n            \n        self.encoder_mean = nn.Linear(in_features = latent_dim, out_features = vec_dim)\n        self.encoder_logstd = nn.Linear(in_features = latent_dim, out_features = vec_dim)\n        \n        \n    def generate_code(self, mean, log_std):\n        \n        sigma = torch.exp(log_std)\n        epsilon = torch.randn_like(mean)\n        return (sigma * epsilon) + mean \n        \n        \n    def forward(self, x):\n        \n        x = self.layer1(x)\n        x, indices_1 = self.pooling(x)\n        x = self.layer2(x)\n        x, indices_2 = self.pooling(x)\n        x = self.layer3(x)\n        x, indices_3 = self.pooling_2(x)\n        x = self.layer4(x)\n        \n        hidden = self.hidden(x)\n        mean, log_std = self.encoder_mean(hidden), self.encoder_logstd(hidden)\n        c = self.generate_code(mean, log_std)\n        \n        return c, indices_1, indices_2, indices_3, mean, log_std","c9b7eb86":"class Decoder(nn.Module):\n    \n    def __init__(self, in_channels, out_channels, image_dim):\n        \n        super().__init__()\n        \n        iW, iH = image_dim\n        hW, hH = iW\/\/POOLING_FACTOR, iH\/\/POOLING_FACTOR\n        \n        self.layer4 = nn.Sequential(\n            nn.Unflatten(1, unflattened_size=(in_channels, hW, hH)),\n            ConvTransposeLeak(in_channels=in_channels, out_channels=128)\n        )\n        \n#         self.layer4 = nn.Sequential(\n#             ConvTransposeLeak(192, 192),\n#             ConvTransposeLeak(192, 128)\n#         )\n        self.layer3 = nn.Sequential(\n            ConvTransposeLeak(128, 128),\n            ConvTransposeLeak(128, 84)\n        )\n        self.layer2 = nn.Sequential(\n            ConvTransposeLeak(84, 84),\n            ConvTransposeLeak(84, 48)\n        )\n        self.layer1 = nn.Sequential(\n            ConvTransposeLeak(48, 48),\n            ConvTransposeLeak(48, 3)\n        )\n        \n        self.unpooling = nn.MaxUnpool2d(4)\n        self.unpooling_2 = nn.MaxUnpool2d(2)\n        \n        self.precision = nn.Parameter(torch.rand(1))\n        \n        \n    def generate_data(self, mean, precision):\n        \n        # Precision is 1\/variance\n        sigma = torch.exp(-precision)\n        epsilon = torch.randn_like(mean)\n        return (sigma * epsilon) + mean\n        \n    def forward(self, x, indices_1, indices_2, indices_3):\n        \n#         x = self.layer5(x)\n#         x = self.unpooling_2(x, indices_4)\n        x = self.layer4(x)\n        x = self.unpooling_2(x, indices_3)\n        x = self.layer3(x)\n        x = self.unpooling(x, indices_2)\n        x = self.layer2(x)\n        x = self.unpooling(x, indices_1)\n        x = self.layer1(x)\n        \n        return x\n#         return self.generate_data(x, self.precision)\n        ","ae94304d":"class CaptionRNN(nn.Module):\n    \n    CAPTION_LIMIT = MAX_CAPTION_LEN\n    \n    def __init__(self, input_size, vocab_size, embedding_size, hidden_size, stop_index):\n        super().__init__()\n        \n        \n        self.mlp_l1 = nn.Sequential(\n            nn.Linear(in_features=input_size, out_features=input_size),\n            nn.LeakyReLU(),\n            nn.Linear(in_features=input_size, out_features=hidden_size),\n            nn.Tanh()\n        )\n        \n        self.mlp_l2 = nn.Sequential(\n            nn.Linear(in_features=hidden_size, out_features=hidden_size),\n            nn.LeakyReLU(),\n            nn.Linear(in_features=hidden_size, out_features=vocab_size),\n        )\n        \n#         self.mlp_l1 = nn.Linear(in_features=input_size, out_features=hidden_size)\n#         self.mlp_l2 = nn.Linear(in_features=hidden_size, out_features=vocab_size)\n        \n        self.gru = nn.GRU(input_size=embedding_size, hidden_size=hidden_size, batch_first=True)\n        self.embedding = nn.Embedding(vocab_size, embedding_size)\n        \n        self.stop_index = stop_index\n        \n        \n    def generate_caption(self, code):\n        \n#         h_1 = torch.tanh(self.mlp_l1(code))\n        h_1 = self.mlp_l1(code)\n        prob_1 = F.softmax(self.mlp_l2(h_1), dim=-1)\n        y_1 = torch.multinomial(prob_1, 1)\n        \n        words = [y_1.item()]\n        w_t = self.embedding(y_1)\n        y_t = y_1\n        h_t = h_1\n        \n#         while len(words) < CaptionRNN.CAPTION_LIMIT:\n        while len(words) < CaptionRNN.CAPTION_LIMIT and y_t.item() != self.stop_index:\n            h_t = self.gru(w_t.unsqueeze(0), h_t.unsqueeze(0).unsqueeze(0))[0]\n            h_t = h_t.squeeze(0).squeeze(0)\n            prob_t = F.softmax(self.mlp_l2(h_t), dim=-1)\n            y_t = torch.multinomial(prob_t, 1)\n            words.append(y_t.item())\n            w_t = self.embedding(y_t)\n        \n        return words\n        \n        \n    def caption_prob(self, code, caption):\n#         hidden_1 = torch.tanh(self.mlp_l1(code))\n        hidden_1 = self.mlp_l1(code)\n        probs_1 = F.softmax(self.mlp_l2(hidden_1), dim=1)\n        weights = self.embedding(caption)\n        output, hidden = self.gru(weights, hidden_1.unsqueeze(0))\n        probs_2_above = F.softmax(self.mlp_l2(output[:, :-1]), dim=-1)\n        return torch.cat([probs_1.unsqueeze(1), probs_2_above], dim=1)\n        \n        \n        ","f7da85a5":"class VAECaptioner(nn.Module):\n    \n    def __init__(self, in_channel, code_channels, image_dim, vocab):\n        super().__init__()\n        \n        LATENT_DIM = 300\n        EMBEDDING_SIZE = 600\n        HIDDEN_SIZE = 512\n        CODE_FLAT = code_channels*((image_dim[0]*image_dim[1])\/\/(POOLING_FACTOR**2))\n        \n        self.vocab = vocab\n        \n        self.encoder = Encoder(in_channel, code_channels, image_dim, LATENT_DIM)\n        self.decoder = Decoder(code_channels, in_channel, image_dim)\n        self.captionr = CaptionRNN(CODE_FLAT, len(vocab), EMBEDDING_SIZE, HIDDEN_SIZE, vocab[\".\"])\n        \n    def forward(self, x, y):\n        \n        c, indices_1, indices_2, indices_3, mean, log_std = self.encoder(x)\n        reconstructed = self.decoder(c, indices_1, indices_2, indices_3)\n        caption_prob = self.captionr.caption_prob(c, y)\n        \n        return reconstructed, caption_prob, mean, log_std\n    \n    def generate_caption(self, x):\n        \n        c, indices_1, indices_2, indices_3, mean, log_std = self.encoder(x)\n        return self.captionr.generate_caption(c[0])","061f87e7":"EPOCHS = 3\nBATCH_SIZE = 32\ntrain_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nvalid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)\nmodel = VAECaptioner(3, 128, (128, 128), vocab).to(device)\noptimizer = optim.AdamW(model.parameters(), lr=0.0002)\ncriterion = nn.MSELoss(reduction=\"sum\")\ncriterion2 = nn.CrossEntropyLoss(reduction=\"sum\")","7f59fd65":"def calculate_loss(reconstructed, caption_prob, images, captions_transformed, mean, log_std):\n    \n    size = captions_transformed.shape[0]\n    reconstruction_error = criterion(reconstructed, images)\n    likelihoods = torch.stack([\n        caption_prob[i, np.arange(MAX_CAPTION_LEN), captions_transformed[i]] for i in range(size)])\n    \n    \n    log_likelihoods = -torch.log(likelihoods).sum()\n    KL_divergence = - (1 - mean.pow(2) - torch.exp(2 * log_std) + (2 *log_std)).sum()\n    \n    return reconstruction_error + (log_likelihoods) + KL_divergence, log_likelihoods\n    ","c3f8b09d":"losses = []\ncaption_losses = []\nval_losses = []\nval_caption_losses = []\nfor epoch in range(EPOCHS):\n    t = tqdm(train_dataloader, desc=f\"Train: Epoch {epoch}\")\n    \n    for images, captions in t:\n        images = images.to(device)\n        captions_transformed = torch.LongTensor(transform_captions(captions)).to(device)\n        reconstructed, caption_prob, mean, log_std = model(images, captions_transformed)\n        \n        loss, caption_loss = calculate_loss(reconstructed, caption_prob, images, captions_transformed, mean, log_std)\n        loss.backward()\n        \n        optimizer.step()\n        optimizer.zero_grad()\n        losses.append(loss.item())\n        caption_losses.append(caption_loss.item())\n    \n    v = tqdm(valid_dataloader, desc=f\"Valid: Epoch {epoch}\")\n    with torch.no_grad():\n        for images, captions in v: \n            images = images.to(device)\n            captions_transformed = torch.LongTensor(transform_captions(captions)).to(device)\n            reconstructed, caption_prob, mean, log_std = model(images, captions_transformed)\n            loss, caption_loss = calculate_loss(reconstructed, caption_prob, images, captions_transformed, mean, log_std)\n            \n            val_losses.append(loss.item())\n            val_caption_losses.append(caption_loss.item())\n            ","47e3ddb5":"_ = plt.plot(losses)","70c6e7c8":"_ = plt.plot(caption_losses)","9f7a8a64":"_ = plt.imshow(images[2].to(\"cpu\").permute(1, 2, 0))","5d2fcdf6":"_ = plt.imshow(reconstructed[2].detach().to(\"cpu\").permute(1,2,0))","7a3aab31":"plt.imshow(images[1].to(\"cpu\").permute(1, 2, 0))\nplt.axis(\"off\")\n_ = plt.title(get_caption(model.generate_caption(images[1].unsqueeze(0))))","c6355bd7":"# Training the model","e4efc03a":"# Defining the model","78dde1a3":"## Create the datasets and preprocessing tools","9ab6ce8c":"# Introduction\n\nThis notebook tries to recreate the paper \"Variational Autoencoder for Deep Learning of Images, Labels and Captions\" (Yunchen Pu, Zhe Gan, Ricardo Henao, Xin Yuan, Chunyuan Li, Andrew Stevens and Lawrence Carin) as much as possible. Please view the paper [here](https:\/\/proceedings.neurips.cc\/paper\/2016\/file\/eb86d510361fc23b59f18c1bc9802cc6-Paper.pdf).\n\nThere is an example of a generated caption below (to mixed results).","dd1465f4":"## Importing the libraries"}}