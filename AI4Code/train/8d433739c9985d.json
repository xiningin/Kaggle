{"cell_type":{"31df09a6":"code","ac208bc3":"code","f6862268":"code","ea1c7708":"code","fea7785d":"code","ad75965c":"code","28e81d0f":"code","1241d9e6":"code","dbed3373":"code","6d45adc3":"code","4e9aef35":"code","8112782a":"code","11415809":"code","b6913ca1":"code","d7d1870d":"code","c60991a6":"code","830b16b9":"code","88051bde":"code","312e2b39":"code","a1a3f447":"code","e6468ef4":"code","53c71580":"code","48dc9f44":"code","89f35d70":"code","ebf9af4e":"code","e493c3d5":"code","b1ee286c":"code","57801b98":"code","e9323dff":"code","3c259909":"code","b1a771c8":"code","138b7de9":"code","c1b057f9":"code","22926e16":"code","6796e466":"code","29bd89c1":"code","12ce3319":"code","d8f6f85d":"code","46fc94f4":"code","aa42e762":"code","5db378f6":"code","8666b463":"code","33a82910":"code","9e22a90a":"code","6422eeb5":"code","65bb7179":"code","ca37fd60":"code","248ee7cd":"code","36c85f04":"markdown","0de5d934":"markdown","74b5813e":"markdown","2fb818dc":"markdown","aa753a3f":"markdown","a854a156":"markdown","64b2e1c1":"markdown","6e5d28fe":"markdown","4cd3ee96":"markdown","8864e92b":"markdown","81f2f943":"markdown","7a9c36f2":"markdown","51ea2db6":"markdown","bd177c3e":"markdown","dcf544b1":"markdown","77f93a12":"markdown","7d8d6716":"markdown","cee80f2a":"markdown","68ed74ff":"markdown","de956274":"markdown","1da4f3e8":"markdown","aaa5a735":"markdown","68e8b952":"markdown","169c7482":"markdown","f7dc60a8":"markdown","ed2fcbac":"markdown","e28fd78c":"markdown","b0137c32":"markdown","2c7fd250":"markdown","9713b320":"markdown","eaa2f286":"markdown","a9779455":"markdown","3c54f28d":"markdown","2681db38":"markdown","8746ea82":"markdown","199353db":"markdown","737439af":"markdown","94d2fcba":"markdown"},"source":{"31df09a6":"#First, we import the libraries we need\n\n%matplotlib inline\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns","ac208bc3":"df_census = pd.read_csv(\"..\/input\/census.csv\")\ndf_census_test = pd.read_csv(\"..\/input\/census_test.csv\")\ndf_census.head(10)","f6862268":"df_census.dtypes","ea1c7708":"df_census.greater_than_50k.unique()","fea7785d":"#For df_census\ndf_census_selected = df_census.select_dtypes(include = [\"object\"])\ndf_census[df_census_selected.columns] = df_census_selected.apply(lambda x: x.str.strip())\n\n#For df_cesus_test\n\ndf_census_selected = df_census_test.select_dtypes(include = [\"object\"])\ndf_census_test[df_census_selected.columns] = df_census_selected.apply(lambda x: x.str.strip())","ad75965c":"df_census = df_census.astype(\n{\n        \"workclass\" : \"category\", \n        \"education\" : \"category\",\n        \"marital_status\" : \"category\",\n        \"occupation\" : \"category\",\n        \"relationship\" : \"category\",\n        \"race\" : \"category\",\n        \"native_country\" : \"category\"\n}\n)\n\ndf_census_test = df_census_test.astype(\n{\n        \"workclass\" : \"category\", \n        \"education\" : \"category\",\n        \"marital_status\" : \"category\",\n        \"occupation\" : \"category\",\n        \"relationship\" : \"category\",\n        \"race\" : \"category\",\n        \"native_country\" : \"category\"\n}\n)","28e81d0f":"for col_name in df_census.columns:\n    if(isinstance(df_census[col_name].dtype, pd.core.dtypes.dtypes.CategoricalDtype)):\n        print(\"Catergories for {} are:\".format(col_name))\n        for category in list(df_census[col_name].cat.categories):\n            print(\"  -\", category) ","1241d9e6":"pd.concat([df_census[[\"age\", \"hours_per_week\"]].describe(), df_census_test[[\"age\", \"hours_per_week\"]].describe() ], axis = 1)","dbed3373":"sns.distplot(df_census[\"age\"])","6d45adc3":"import scipy.stats as stats\nprint(\"H0 hypothesis test:\",stats.normaltest(df_census[\"age\"]))","4e9aef35":"sns.distplot(df_census_test[\"age\"])","8112782a":"sns.distplot(df_census[\"hours_per_week\"])","11415809":"print(\"H0 hypothesis test:\",stats.normaltest(df_census[\"hours_per_week\"]))","b6913ca1":"sns.distplot(df_census_test[\"hours_per_week\"])","d7d1870d":"print(\"Proportions of 1s for census: \", df_census.greater_than_50k.mean())\nprint(\"Proportions of 1s for census test: \", df_census_test.greater_than_50k.mean())","c60991a6":"df_1 = pd.DataFrame({ \"census\": (df_census.workclass.value_counts()\/df_census.workclass.count())})\ndf_2 = pd.DataFrame({ \"census\": (df_census_test.workclass.value_counts()\/df_census_test.workclass.count())})\ndf_2 =df_2.rename(columns = {\"census\" : \"census_test\"})\npd.concat([df_1, df_2], axis = 1)","830b16b9":"df_census_mean = pd.DataFrame({\"avg_over_50k\" : df_census[[\"workclass\",  \"greater_than_50k\"]].groupby([\"workclass\"])[\"greater_than_50k\"].mean()})\ndf_census_mean_test = pd.DataFrame({\"avg_over_50k_test\" : df_census_test[[\"workclass\",  \"greater_than_50k\"]].groupby([\"workclass\"])[\"greater_than_50k\"].mean()})\npd.concat([df_census_mean, df_census_mean_test], axis = 1)","88051bde":"df_census_test[[\"workclass\",  \"greater_than_50k\"]].groupby([\"workclass\"])[\"greater_than_50k\"].value_counts()","312e2b39":"df_census_mean = pd.DataFrame({\"avg_over_50k\" : df_census[[\"education\",  \"greater_than_50k\"]].groupby([\"education\"])[\"greater_than_50k\"].mean()})\ndf_census_mean_test = pd.DataFrame({\"avg_over_50k_test\" : df_census_test[[\"education\",  \"greater_than_50k\"]].groupby([\"education\"])[\"greater_than_50k\"].mean()})\npd.concat([df_census_mean, df_census_mean_test], axis = 1)","a1a3f447":"df_census_mean = pd.DataFrame({\"avg_over_50k\" : df_census[[\"marital_status\",  \"greater_than_50k\"]].groupby([\"marital_status\"])[\"greater_than_50k\"].mean()})\ndf_census_mean_test = pd.DataFrame({\"avg_over_50k_test\" : df_census_test[[\"marital_status\",  \"greater_than_50k\"]].groupby([\"marital_status\"])[\"greater_than_50k\"].mean()})\npd.concat([df_census_mean, df_census_mean_test], axis = 1)","e6468ef4":"df_census_test[[\"marital_status\",  \"greater_than_50k\"]].groupby([\"marital_status\"])[\"greater_than_50k\"].value_counts()","53c71580":"df_census_mean = pd.DataFrame({\"avg_over_50k\" : df_census[[\"occupation\",  \"greater_than_50k\"]].groupby([\"occupation\"])[\"greater_than_50k\"].mean()})\ndf_census_mean_test = pd.DataFrame({\"avg_over_50k_test\" : df_census_test[[\"occupation\",  \"greater_than_50k\"]].groupby([\"occupation\"])[\"greater_than_50k\"].mean()})\npd.concat([df_census_mean, df_census_mean_test], axis = 1)","48dc9f44":"df_census[\"occupation\"].value_counts()","89f35d70":"gr_occupation = sns.barplot(y = \"occupation\", x =\"greater_than_50k\", data=df_census, estimator = np.mean)","ebf9af4e":"gr_occupation = sns.barplot(y = \"occupation\", x =\"greater_than_50k\", data=df_census_test, estimator = np.mean)","e493c3d5":"df_census_mean = pd.DataFrame({\"avg_over_50k\" : df_census[[\"relationship\",  \"greater_than_50k\"]].groupby([\"relationship\"])[\"greater_than_50k\"].mean()})\ndf_census_mean_test = pd.DataFrame({\"avg_over_50k_test\" : df_census_test[[\"relationship\",  \"greater_than_50k\"]].groupby([\"relationship\"])[\"greater_than_50k\"].mean()})\npd.concat([df_census_mean, df_census_mean_test], axis = 1)","b1ee286c":"df_census_mean = pd.DataFrame({\"avg_over_50k\" : df_census[[\"race\",  \"greater_than_50k\"]].groupby([\"race\"])[\"greater_than_50k\"].mean()})\ndf_census_mean_test = pd.DataFrame({\"avg_over_50k_test\" : df_census_test[[\"race\",  \"greater_than_50k\"]].groupby([\"race\"])[\"greater_than_50k\"].mean()})\npd.concat([df_census_mean, df_census_mean_test], axis = 1)","57801b98":"df_census_mean = pd.DataFrame({\"avg_over_50k\" : df_census[[\"gender\",  \"greater_than_50k\"]].groupby([\"gender\"])[\"greater_than_50k\"].mean()})\ndf_census_mean_test = pd.DataFrame({\"avg_over_50k_test\" : df_census_test[[\"gender\",  \"greater_than_50k\"]].groupby([\"gender\"])[\"greater_than_50k\"].mean()})\npd.concat([df_census_mean, df_census_mean_test], axis = 1)","e9323dff":"df_census_mean = pd.DataFrame({\"avg_over_50k\" : df_census[[\"native_country\",  \"greater_than_50k\"]].groupby([\"native_country\"])[\"greater_than_50k\"].mean()})\ndf_census_mean_test = pd.DataFrame({\"avg_over_50k_test\" : df_census_test[[\"native_country\",  \"greater_than_50k\"]].groupby([\"native_country\"])[\"greater_than_50k\"].mean()})\npd.concat([df_census_mean, df_census_mean_test], axis = 1)","3c259909":"sns.countplot(y='occupation', hue='greater_than_50k', data=df_census)","b1a771c8":"sns.countplot(y='occupation', hue='greater_than_50k', data=df_census_test)","138b7de9":"%matplotlib inline\nimport scipy.stats as stats\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split","c1b057f9":"sns.heatmap(df_census.isnull())","22926e16":"df_census.dropna(inplace = True)","6796e466":"from sklearn.model_selection import train_test_split\nfrom warnings import simplefilter\n\nsimplefilter(action='ignore', category=FutureWarning)\n","29bd89c1":"df_census_dummies = pd.get_dummies(df_census[['workclass', 'education', 'marital_status',\n       'occupation', 'relationship', 'race', 'gender',\n       'native_country']])","12ce3319":"df_census.drop(['workclass', 'education', 'marital_status',\n       'occupation', 'relationship', 'race', 'gender',\n       'native_country'], axis = 1, inplace=True)","d8f6f85d":"#We get together original dataset with dummies\n\ndf_census_train = pd.concat([df_census_dummies, df_census], axis = 1)","46fc94f4":"X_train, X_test, y_train, y_test = train_test_split(df_census_train.drop('greater_than_50k',axis=1), \n                                                    df_census_train['greater_than_50k'], test_size=0.30, \n                                                    random_state=101)","aa42e762":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nlogmodel = LogisticRegression()\nlogmodel.fit(X_train,y_train)","5db378f6":"predictions = logmodel.predict(X_test)","8666b463":"from sklearn.metrics import classification_report\nprint(classification_report(y_test,predictions))","33a82910":"# Our accuracy is:\nlogmodel_score = logmodel.score(X_test, y_test)\nprint(\"Model Score: \" ,logmodel_score)","9e22a90a":"pd.DataFrame(metrics.confusion_matrix(y_test, predictions), columns = [\"PREDICTED_FALSE\",\"PREDICTED_TRUE\" ], index = [\"ACTUAL_FALSE\", \"ACTUAL_TRUE\"])","6422eeb5":"a = logmodel.predict_proba(X_train)[:,1]\nb = y_train\n\ntot_bads=1.0*sum(b)\ntot_goods=1.0*(len(b)-tot_bads)\nelements_df = pd.DataFrame({'probability': a,'gbi': b})\npivot_elements_df = pd.pivot_table(elements_df, values='gbi', index=['probability'], aggfunc=[sum,len]).fillna(0)\nmax_ks = perc_goods = perc_bads = cum_perc_bads = cum_perc_goods = 0\ncum_perc_bads_list = [0.0]\ncum_perc_goods_list = [0.0]\ncum_cp_minus = [0.0]\ncum_cp_plus = [0.0]\n\nfor i in range(len(pivot_elements_df)):\n    perc_goods =  ((pivot_elements_df['len'].iloc[i]['gbi'] - pivot_elements_df['sum'].iloc[i]['gbi']) \/ tot_goods)\n    perc_bads = float(pivot_elements_df['sum']['gbi'].iloc[i]\/ tot_bads)\n    cum_perc_goods += perc_goods   \n    cum_perc_bads += perc_bads\n\n    \n    cum_perc_bads_list.append(cum_perc_bads)\n    cum_perc_goods_list.append(cum_perc_goods)\n    cum_diff = cum_perc_bads-cum_perc_goods\n\n    cum_cp_minus.append(0.0)    \n    cum_cp_minus[-1] = cum_perc_bads_list[-1] - cum_perc_bads_list[-2]\n\n    cum_cp_plus.append(0.0)\n    cum_cp_plus[-1] = cum_perc_goods_list[-1] + cum_perc_goods_list[-2]\n    \n    \n    if abs(cum_diff) > max_ks:\n        max_ks = abs(cum_diff)\n\nprint('KS=',max_ks)","65bb7179":"z_score = 0\nfor i in range(len(cum_cp_plus)):\n    try:\n        z_score +=  cum_cp_minus[i] * cum_cp_plus[i]\n    except:\n        pass\nprint('GINI=',1- z_score\/100.0)","ca37fd60":"a = logmodel.predict_proba(X_test)[:,1]\nb = y_test\n\ntot_bads=1.0*sum(b)\ntot_goods=1.0*(len(b)-tot_bads)\nelements_df = pd.DataFrame({'probability': a,'gbi': b})\npivot_elements_df = pd.pivot_table(elements_df, values='gbi', index=['probability'], aggfunc=[sum,len]).fillna(0)\nmax_ks = perc_goods = perc_bads = cum_perc_bads = cum_perc_goods = 0\ncum_perc_bads_list = [0.0]\ncum_perc_goods_list = [0.0]\ncum_cp_minus = [0.0]\ncum_cp_plus = [0.0]\n\nfor i in range(len(pivot_elements_df)):\n    perc_goods =  ((pivot_elements_df['len'].iloc[i]['gbi'] - pivot_elements_df['sum'].iloc[i]['gbi']) \/ tot_goods)\n    perc_bads = float(pivot_elements_df['sum']['gbi'].iloc[i]\/ tot_bads)\n    cum_perc_goods += perc_goods   \n    cum_perc_bads += perc_bads\n\n    \n    cum_perc_bads_list.append(cum_perc_bads)\n    cum_perc_goods_list.append(cum_perc_goods)\n    cum_diff = cum_perc_bads-cum_perc_goods\n\n    cum_cp_minus.append(0.0)    \n    cum_cp_minus[-1] = cum_perc_bads_list[-1] - cum_perc_bads_list[-2]\n\n    cum_cp_plus.append(0.0)\n    cum_cp_plus[-1] = cum_perc_goods_list[-1] + cum_perc_goods_list[-2]\n    \n    \n    if abs(cum_diff) > max_ks:\n        max_ks = abs(cum_diff)\n\nprint('KS=',max_ks)","248ee7cd":"z_score = 0\nfor i in range(len(cum_cp_plus)):\n    try:\n        z_score +=  cum_cp_minus[i] * cum_cp_plus[i]\n    except:\n        pass\nprint('GINI=',1- z_score\/100.0)","36c85f04":"KS and GINI for test","0de5d934":"From the previous types, we see that:\n    <ul>\n    <li>age: discrete variable.<\/li> \n    <li>workclass: numeric variable.<\/li>\n    <li>education: nominal variable.<\/li>\n    <li>education_num: ordinal variable.<\/li>\n    <li>marital_status: nominal variable<\/li>\n    <li>occupation: nominal variable.<\/li>\n    <li>relationship: nominal variable<\/li>\n    <li>race: nominal variable.<\/li>\n    <li>gender: nominal variable<\/li>\n    <li>hours_per_week: discrete variable.<\/li>\n    <li>native_country: nominal variable.<\/li>\n    <li>greater_than_50k: binary data.<\/li>\n    <\/ul>","74b5813e":"We are given two datasets: census and census_test. Applying EDA, we have to guess if census_test was randomly selected from census. If not, it may be biased. \n\nFirst of all, well import census dataset and give a quick view to the variables it contains","2fb818dc":"<b> We are going to calculate a logistic regression using sklearn <\/b>","aa753a3f":"<b>Education <\/b>","a854a156":"First of all, we are going to clean all trailing and leading whitespaces","64b2e1c1":"# EDA + LOGISTIC REGRESSION","6e5d28fe":"From the previous comparison, we see that there isn't such a big difference between census and census_test. From this perspective, the census_test may seem randomly selected. Let's see what happens with categorical data.\n\nTo do that, we are going to calculate proportions for each value\n","4cd3ee96":"Even if there are 66% of Armed-Forces from test dataset have an income over 50K, it may be that the sample took more armed forces observations, since there are few of them.","8864e92b":"<b>Race <\/b>","81f2f943":"<b> Now let's calculate KS and GINI index <\/b>","7a9c36f2":"The former distribution isn't normal too","51ea2db6":"From the former graphics, we see that the margin of error in Armed-Forces is high, because there are very few observations. Because of that we can't state that census.csv is manipulated.","bd177c3e":"<b>Gender <\/b>","dcf544b1":"The former distributionn is not normal","77f93a12":"We'll check dtypes that were automatically guessed by pandas library","7d8d6716":"<p> In order to understand the dataset, we'll check out the categories in each variable <\/p>","cee80f2a":"<b>Marital status <\/b>","68ed74ff":"# Exercise 1: EDA analysis","de956274":"<b> We can state that census_test.csv is not manipulated. Data distribution is very similar for both datasets and when there is a significant difference, the margin of error is high because there are very few observations (as in Armed-foces). Also, it seems that there is an error because there are people workclass Without-pay that have an income over 50K, while in the original dataset (census.csv), there were 0 people. ","1da4f3e8":"Because we can't compute independent categorical variables in a machine learning algorithm, we have to use the pandas method get_dummies.","aaa5a735":"KS and GINI for train","68e8b952":"So far, so good, but we still have more work to do. We will group variables in order to\nanalise the proportion of paychecks greater than 50K","169c7482":"We'll split census dataset for training and test","f7dc60a8":"We'll check other variables","ed2fcbac":"Next, we are going to compare statistics for numerical data.","e28fd78c":"<b>Native country <\/b>","b0137c32":"Also, it seems that there is a 12% of difference between Married-AF-spouse. But this is not so relevant.","2c7fd250":"We have 0 vs 28% of people in our test dataset that have a income greater than 50K without being paid!!! <b> How's that possible :S !<\/b>\n\nEither it's manually modified or there's is an error in the dataset. Anyway, we should check the number of observations, because the fewer the number of observations, the greater the margin of error.","9713b320":"<b>Occupation <\/b>","eaa2f286":"<b>For workclass <\/b>","a9779455":"<p> We are given two datasets: census.csv and census_test.csv. We'll have two determine applyin EDA if census_test is either a random sample from census.csv or if it's manipulated. Once we've guessed that, we'll apply a logistic regression and measure it with different statistics. <\/p>","3c54f28d":"First of all, we have to deal with null values. A good aproach to deal with null values\nis applyin a heatmap","2681db38":"One last plot to show the proportions","8746ea82":"<b>Relationship <\/b>","199353db":"<b>For greater_than_50k <\/b>","737439af":"Since there's categorical data, we'll take advantage of the \"category\" dtype pd.Series","94d2fcba":"# Exercise 2: applying logistic regresion"}}