{"cell_type":{"d8d71b35":"code","106959fd":"code","fe24982c":"code","9ac29d33":"code","dcd82378":"code","bc37969a":"markdown","6deaa982":"markdown","e8127bc3":"markdown"},"source":{"d8d71b35":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\n\ndata = pd.read_csv('..\/input\/train.csv')\ndata.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = data.SalePrice\nX = data.drop(['SalePrice'], axis=1).select_dtypes(exclude=['object'])\ntrain_X, test_X, train_y, test_y = train_test_split(X.as_matrix(), y.as_matrix(), test_size=0.25)\n\nmy_imputer = SimpleImputer()\ntrain_X = my_imputer.fit_transform(train_X)\ntest_X = my_imputer.transform(test_X)\n","106959fd":"from xgboost import XGBRegressor\n\nmy_model = XGBRegressor()\n# Add silent=True to avoid printing out updates with each cycle\nmy_model.fit(train_X, train_y, verbose=False)\n","fe24982c":"# make predictions\npredictions = my_model.predict(test_X)\n\nfrom sklearn.metrics import mean_absolute_error\nprint(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_y)))","9ac29d33":"my_model = XGBRegressor(n_estimators=1000)\nmy_model.fit(train_X, train_y, early_stopping_rounds=5, \n             eval_set=[(test_X, test_y)], verbose=False)\n","dcd82378":"my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05)\nmy_model.fit(train_X, train_y, early_stopping_rounds=5, \n             eval_set=[(test_X, test_y)], verbose=False)\n","bc37969a":"When using **early_stopping_rounds**, you need to set aside some of your data for checking the number of rounds to use.  If you later want to fit a model with all of your data, set **n_estimators** to whatever value you found to be optimal when run with early stopping.\n\n### learning_rate\nHere's a subtle but important trick for better XGBoost models:\n\nInstead of getting predictions by simply adding up the predictions from each component model, we will multiply the predictions from each model by a small number before adding them in.  This means each tree we add to the ensemble helps us less.  In practice, this reduces the model's propensity to overfit.\n\nSo, you can use a higher value of **n_estimators** without overfitting.  If you use early stopping, the appropriate number of trees will be set automatically.\n\nIn general, a small learning rate (and large number of estimators) will yield more accurate XGBoost models, though it will also take the model longer to train since it does more iterations through the cycle.\n\nModifying the example above to include a learing rate would yield the following code:","6deaa982":"We similarly evaluate a model and make predictions as we would do in scikit-learn.","e8127bc3":"We build and fit a model just as we would in scikit-learn."}}