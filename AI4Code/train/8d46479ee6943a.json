{"cell_type":{"13785b3e":"code","d9454c1c":"code","4a9d2089":"code","81b45065":"code","6459b6e2":"code","5a3385ad":"code","b3092da5":"code","269308e5":"code","f0d0f66f":"code","fe05e75f":"code","52e35a0f":"code","9665ce82":"code","c4bdd1bb":"code","d59f64d4":"code","ce690041":"code","6feead5f":"code","a10c719e":"code","1f440d5b":"code","07558bd4":"code","69dbf427":"markdown","947b2f25":"markdown","f21b95c5":"markdown","87c6af8b":"markdown","46b6178d":"markdown","03973710":"markdown","79849a3b":"markdown","1b6e5a8b":"markdown","1728ffbe":"markdown","cbcdb28d":"markdown","783bab6d":"markdown","ca3d000a":"markdown","7de0f41c":"markdown","40ab5d63":"markdown","517f71fe":"markdown","9bc24bdb":"markdown","b9aee5b9":"markdown","e78739a6":"markdown"},"source":{"13785b3e":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport pandas_profiling \nfrom pandas_profiling import ProfileReport \nimport plotly.graph_objects as go\n\nimport networkx as nx\nimport math as math\nimport time\nplt.style.use('seaborn')\nplt.rcParams['figure.figsize'] = [14,14]","d9454c1c":"data=pd.read_csv(\"..\/input\/netflix-shows\/netflix_titles.csv\")","4a9d2089":"ProfileReport(data)","81b45065":"old = data.sort_values(\"release_year\", ascending = True)\nold = old[old['duration'] != \"\"]\nold = old[old['type'] !=\"TV Show\"]\nold[['title', \"release_year\",\"country\",\"duration\"]][:15]","6459b6e2":"old = data.sort_values(\"release_year\", ascending = True)\nold = old[old['duration'] != \"\"]\nold = old[old['type'] !=\"Movie\"]\nold[['title', \"release_year\",\"country\",\"duration\"]][:15]","5a3385ad":"data.type.value_counts(normalize=True)","b3092da5":"colors = ['gold', 'mediumturquoise']\ntypes = ['Movie','TV Show']\npercentage = [0.6841,0.3158]\n\n# Use `hole` to create a donut-like pie chart\nfig = go.Figure(data=[go.Pie(labels=types, values=percentage, hole=.6)])\nfig.update_traces(hoverinfo='label', textinfo='label', textfont_size=10,\n                  marker=dict(colors=colors, line=dict(color='#000000', width=1)))\nfig.update_layout(\n    autosize=False,\n    width=500,\n    height=500,\n    margin=dict(\n        l=50,\n        r=50,\n        b=0,\n        t=0,\n        pad=0\n    ),\n    paper_bgcolor=\"LightSteelBlue\",\n)\nfig.show()","269308e5":"data['country'].value_counts()","f0d0f66f":"country  = data['country'].value_counts()\ncountry = country[:15,]\nfig = px.bar(x=country.index, y=country.values, color=country.values,\n             hover_data=[country.index, country.values],labels={'country':'Frequency'}, height=400)\nfig.show()","fe05e75f":"#Most Popular Titles\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nplt.rcParams['figure.figsize'] = (13, 13)\nwordcloud = WordCloud(stopwords=STOPWORDS,background_color = 'black', width = 1000,  height = 1000, max_words = 121).generate(' '.join(data['title']))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","52e35a0f":"release_year  = data['release_year'].value_counts()\nfig = px.bar(x=release_year.index, y=release_year.values, color=release_year.values,\n             hover_data=[release_year.index, release_year.values],labels={'country':'Frequency'}, height=400)\n# Change\nfig.update_layout(barmode='stack')\nfig.show()","9665ce82":"df1 = data[data[\"type\"] == \"TV Show\"]\ndf2 = data[data[\"type\"] == \"Movie\"]\ntemp_df1 = df1['release_year'].value_counts().reset_index()\ntemp_df2 = df2['release_year'].value_counts().reset_index()\ntrace1 = go.Bar(x = temp_df1['index'],y = temp_df1['release_year'],\n                name=\"TV Shows\",marker = dict(color = 'rgb(249, 6, 6)'))\ntrace2 = go.Bar(x = temp_df2['index'],y = temp_df2['release_year'],\n                name = \"Movies\",\n                marker = dict(color = 'rgb(26, 118, 255)'))\nlayout = go.Layout(template= \"plotly\" , xaxis = dict(title = 'Year'), yaxis = dict(title = 'Count'))\nfig = go.Figure(data = [trace1, trace2], layout = layout)\nfig.update_layout(barmode='stack')\nfig.show()","c4bdd1bb":"df=data\n# convert to datetime\ndata[\"date_added\"] = pd.to_datetime(data['date_added'])\ndata['year'] = data['date_added'].dt.year\ndata['month'] = data['date_added'].dt.month\ndata['day'] = data['date_added'].dt.day\n# convert columns \"director, listed_in, cast and country\" in columns that contain a real list\n# the strip function is applied on the elements\n# if the value is NaN, the new column contains a empty list []\ndf['directors'] = df['director'].apply(lambda l: [] if pd.isna(l) else [i.strip() for i in l.split(\",\")])\ndf['categories'] = df['listed_in'].apply(lambda l: [] if pd.isna(l) else [i.strip() for i in l.split(\",\")])\ndf['actors'] = df['cast'].apply(lambda l: [] if pd.isna(l) else [i.strip() for i in l.split(\",\")])\ndf['countries'] = df['country'].apply(lambda l: [] if pd.isna(l) else [i.strip() for i in l.split(\",\")])\n\ndf.head()","d59f64d4":"#KMeans clustering with TF-IDF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel\nfrom sklearn.cluster import MiniBatchKMeans\n\n# Build the tfidf matrix with the descriptions\nstart_time = time.time()\ntext_content = df['description']\nvector = TfidfVectorizer(max_df=0.4,         # drop words that occur in more than X percent of documents\n                             min_df=1,      # only use words that appear at least X times\n                             stop_words='english', # remove stop words\n                             lowercase=True, # Convert everything to lower case \n                             use_idf=True,   # Use idf\n                             norm=u'l2',     # Normalization\n                             smooth_idf=True # Prevents divide-by-zero errors\n                            )\ntfidf = vector.fit_transform(text_content)\n\n# Clustering  Kmeans\nk = 200\nkmeans = MiniBatchKMeans(n_clusters = k)\nkmeans.fit(tfidf)\ncenters = kmeans.cluster_centers_.argsort()[:,::-1]\nterms = vector.get_feature_names()\n\n# print the centers of the clusters\n# for i in range(0,k):\n#     word_list=[]\n#     print(\"cluster%d:\"% i)\n#     for j in centers[i,:10]:\n#         word_list.append(terms[j])\n#     print(word_list) \n    \nrequest_transform = vector.transform(df['description'])\n# new column cluster based on the description\ndf['cluster'] = kmeans.predict(request_transform) \n\ndf['cluster'].value_counts().head()","ce690041":"#\u00abcolumn cluster are not going to be used because clusters are two unbalanced\n#But tfidf will be used in order to find similar description\u00bb\n# Find similar : get the top_n movies with description similar to the target description \ndef find_similar(tfidf_matrix, index, top_n = 5):\n    cosine_similarities = linear_kernel(tfidf_matrix[index:index+1], tfidf_matrix).flatten()\n    related_docs_indices = [i for i in cosine_similarities.argsort()[::-1] if i != index]\n    return [index for index in related_docs_indices][0:top_n]  ","6feead5f":"G = nx.Graph(label=\"MOVIE\")\nstart_time = time.time()\nfor i, rowi in df.iterrows():\n    if (i%1000==0):\n        print(\" iter {} -- {} seconds --\".format(i,time.time() - start_time))\n    G.add_node(rowi['title'],key=rowi['show_id'],label=\"MOVIE\",mtype=rowi['type'],rating=rowi['rating'])\n#    G.add_node(rowi['cluster'],label=\"CLUSTER\")\n#    G.add_edge(rowi['title'], rowi['cluster'], label=\"DESCRIPTION\")\n    for element in rowi['actors']:\n        G.add_node(element,label=\"PERSON\")\n        G.add_edge(rowi['title'], element, label=\"ACTED_IN\")\n    for element in rowi['categories']:\n        G.add_node(element,label=\"CAT\")\n        G.add_edge(rowi['title'], element, label=\"CAT_IN\")\n    for element in rowi['directors']:\n        G.add_node(element,label=\"PERSON\")\n        G.add_edge(rowi['title'], element, label=\"DIRECTED\")\n    for element in rowi['countries']:\n        G.add_node(element,label=\"COU\")\n        G.add_edge(rowi['title'], element, label=\"COU_IN\")\n    \n    indices = find_similar(tfidf, i, top_n = 5)\n    snode=\"Sim(\"+rowi['title'][:15].strip()+\")\"        \n    G.add_node(snode,label=\"SIMILAR\")\n    G.add_edge(rowi['title'], snode, label=\"SIMILARITY\")\n    for element in indices:\n        G.add_edge(snode, df['title'].loc[element], label=\"SIMILARITY\")\nprint(\" finish -- {} seconds --\".format(time.time() - start_time)) ","a10c719e":"def get_all_adj_nodes(list_in):\n    sub_graph=set()\n    for m in list_in:\n        sub_graph.add(m)\n        for e in G.neighbors(m):        \n                sub_graph.add(e)\n    return list(sub_graph)\ndef draw_sub_graph(sub_graph):\n    subgraph = G.subgraph(sub_graph)\n    colors=[]\n    for e in subgraph.nodes():\n        if G.nodes[e]['label']==\"MOVIE\":\n            colors.append('blue')\n        elif G.nodes[e]['label']==\"PERSON\":\n            colors.append('red')\n        elif G.nodes[e]['label']==\"CAT\":\n            colors.append('green')\n        elif G.nodes[e]['label']==\"COU\":\n            colors.append('yellow')\n        elif G.nodes[e]['label']==\"SIMILAR\":\n            colors.append('orange')    \n        elif G.nodes[e]['label']==\"CLUSTER\":\n            colors.append('orange')\n\n    nx.draw(subgraph, with_labels=True, font_weight='bold',node_color=colors)\n    plt.show()\nlist_in=[\"Ocean's Twelve\",\"Ocean's Thirteen\"]\nsub_graph = get_all_adj_nodes(list_in)\ndraw_sub_graph(sub_graph)","1f440d5b":"#The recommendation function\n#Explore the neighborhood of the target film \u2192 this is a list of actor, director, country, categorie\n#Explore the neighborhood of each neighbor \u2192 discover the movies that share a node with the target field\n#Calcul Adamic Adar measure \u2192 final results\ndef get_recommendation(root):\n    commons_dict = {}\n    for e in G.neighbors(root):\n        for e2 in G.neighbors(e):\n            if e2==root:\n                continue\n            if G.nodes[e2]['label']==\"MOVIE\":\n                commons = commons_dict.get(e2)\n                if commons==None:\n                    commons_dict.update({e2 : [e]})\n                else:\n                    commons.append(e)\n                    commons_dict.update({e2 : commons})\n    movies=[]\n    weight=[]\n    for key, values in commons_dict.items():\n        w=0.0\n        for e in values:\n            w=w+1\/math.log(G.degree(e))\n        movies.append(key) \n        weight.append(w)\n    \n    result = pd.Series(data=np.array(weight),index=movies)\n    result.sort_values(inplace=True,ascending=False)        \n    return result;","07558bd4":"result = get_recommendation(\"The Perfect Date\")\nresult2 = get_recommendation(\"Ocean's Thirteen\")\nresult3 = get_recommendation(\"The Devil Inside\")\nresult4 = get_recommendation(\"Stranger Things\")\nprint(\"*\"*40+\"\\n Recommendation for 'Ocean's Twelve'\\n\"+\"*\"*40)\nprint(result.head())\nprint(\"*\"*40+\"\\n Recommendation for 'Ocean's Thirteen'\\n\"+\"*\"*40)\nprint(result2.head())\nprint(\"*\"*40+\"\\n Recommendation for 'Belmonte'\\n\"+\"*\"*40)\nprint(result3.head())\nprint(\"*\"*40+\"\\n Recommendation for 'Stranger Things'\\n\"+\"*\"*40)\nprint(result4.head())","69dbf427":"### Q6:What is the distribution of movies duration??","947b2f25":"### Q5: What are most popular titles??","f21b95c5":"## 2.1 Importing Dataset\ud83d\udcda","87c6af8b":"### Q2: What are the oldest TV shows available on Netflix??","46b6178d":"### Building a recomendation engine based on graph using adamic bar measure\nThe more the measure is high, the closest are the two nodes.\nThe measures between all movies are NOT pre-calculated, in order to determine the list of recommendation films, we are going to explore the neighborhood of the target film","03973710":"### Q3:Percentage of TV shows to movie??","79849a3b":"![](https:\/\/i.imgur.com\/dP7i2ya.gif)\n<h1><center>\ud83c\udf7fNetflix-EDA+ConsumptionAnalysis\ud83c\udf7f<\/center><\/h1>","1b6e5a8b":"## Credits:\n1. https:\/\/www.kaggle.com\/shivamb\/netflix-shows-and-movies-exploratory-analysis\n2. https:\/\/www.kaggle.com\/vikassingh1996\/netflix-movies-and-shows-plotly-recommender-sys\n3. https:\/\/www.kaggle.com\/yclaudel\/recommendation-engine-with-networkx -Thanks Yann I am learning a lot from your networkx work..","1728ffbe":"# 1.Introduction\n- Netflix, Inc. is an American Technology & media-services provider and production company headquartered in Los Gatos, California, founded in 1997 by Reed Hastings and Marc Randolph in Scotts Valley, California. \n- The company's primary business is its subscription-based streaming service which offers online streaming of a library of films and television programs, including those produced in-house. \n- As of April 2020, Netflix had over 193 million paid subscriptions worldwide, including 73 million in the United States.It is available worldwide except in the following: mainland China (due to local restrictions), Iran, Syria, North Korea, and Crimea (due to U.S. sanctions). The company also has offices in France, United States, United Kingdom, Brazil, the Netherlands, India, Japan, and South Korea.Netflix is a member of the Motion Picture Association of America (MPAA). Today, the company produces and distributes content from countries all over the globe.\n- This dataset contains the meta details about the movies and tv shows.","cbcdb28d":"## Recomendation Engine.This is Yann's work (NOT MINE).Let's give it go..","783bab6d":"### Q7:How much content is produced today in comparison to earlier decades??","ca3d000a":"### Q4:What are country wise representation of all content??","7de0f41c":"## 3.1 General EDA","40ab5d63":"1. Essentials: type, unique values, missing values.\n2. Quantile statistics like minimum value, Q1, median, Q3, maximum, range, interquartile range.\n3. Descriptive statistics like mean, mode, standard deviation, sum, median absolute deviation, coefficient of variation, kurtosis, skewness.\n4. Most frequent values.\n5. Histogram.\n6. Correlations show the correlated variables, Spearman and Pearson matrices.\n7. Sample of dataset","517f71fe":"## 2. Importing Libraries \ud83d\udcda","9bc24bdb":"## Profile Profiling is much faster way to do basic general EDA","b9aee5b9":"### Q1:Which are Oldest Movies available on Netflix??","e78739a6":"## 3. Exploratory Data Analysis\ud83d\udd0e\nIn statistics, exploratory data analysis (EDA) is an approach to analyzing data sets to summarize their main characteristics, often with visual methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling or hypothesis testing task. "}}