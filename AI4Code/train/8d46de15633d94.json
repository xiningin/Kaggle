{"cell_type":{"bdc8d730":"code","93acff0a":"code","9306e070":"code","24524cd9":"code","f4e4843c":"code","d700913e":"code","6ded0644":"code","72a12e5c":"code","cb5c8359":"code","e55a91d3":"code","e2da62de":"code","09afada9":"code","3d694e91":"code","f42170b4":"code","f197f272":"code","0593e24a":"code","a1721b4a":"code","b753bbb4":"code","8ac12dd0":"code","d2578b6d":"code","172f23d3":"code","625139eb":"code","a3e4ac12":"markdown","7b774f9a":"markdown","02908e31":"markdown","e0c4c550":"markdown","df9a5687":"markdown","e7a6e08c":"markdown","3e1fb98a":"markdown","bd265769":"markdown","25c89218":"markdown","a53e67b4":"markdown","ea93c7a3":"markdown","b6a5b921":"markdown","ed643d39":"markdown","f18fc5eb":"markdown","f3474ec1":"markdown","459c4991":"markdown","21e32b23":"markdown","a9c6d69f":"markdown"},"source":{"bdc8d730":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.colors\nimport os","93acff0a":"os.listdir('..\/input\/')","9306e070":"\ndata = pd.read_csv('..\/input\/blobs.csv')\n\nprint(data.groupby('class').head(2))\n","24524cd9":"X = data[['feature1', 'feature2']].values\ny = data['class'].values","f4e4843c":"colors=['green','blue']\ncmap = matplotlib.colors.ListedColormap(colors)\n#Plot the figure\nplt.figure()\nplt.title('Non-linearly separable classes')\nplt.scatter(X[:,0], X[:,1], c=y,\n           marker= 'o', s=50,cmap=cmap,alpha = 0.5 )\nplt.show()","d700913e":"from sklearn.linear_model import LogisticRegression\nlr_model = LogisticRegression()\n_ = lr_model.fit(X, y)","6ded0644":"def plot_decision_boundary( X, y, model):\n    plt.clf()\n    # Set min and max values and give it some padding\n    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1\n    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1   \n    colors=['blue','green']\n    cmap = matplotlib.colors.ListedColormap(colors)   \n    h = 0.01\n    # Generate a grid of points with distance h between them\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    # Predict the function value for the whole grid\n    A = model(np.c_[xx.ravel(), yy.ravel()])\n    A = A.reshape(xx.shape)\n    # Plot the contour and training examples\n    plt.contourf(xx, yy, A, cmap=\"spring\")\n    plt.ylabel('x2')\n    plt.xlabel('x1')\n    plt.scatter(X[0, :], X[1, :], c=y, s=8,cmap=cmap)\n    plt.title(\"Decision Boundary for learning rate:\")\n    plt.show()","72a12e5c":"plot_decision_boundary(X.T,y,lambda x: lr_model.predict(x))","cb5c8359":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.25,random_state = 25)\n\nprint(\"shape of X_train:{} shape 0f Y_train:{}\".format(X_train.shape, Y_train.shape))  \nprint(\"shape of X_test:{} shape 0f Y_test:{}\".format(X_test.shape, Y_test.shape))  ","e55a91d3":"X_train = X_train.T\n\nY_train = Y_train.reshape(1, len(Y_train))\n\nX_test = X_test.T\n\nY_test = Y_test.reshape(1, len(Y_test))\n\n\n\nprint(\"shape of X_train:{} shape 0f Y_train:{} after transformation\".format(X_train.shape, Y_train.shape))","e2da62de":"layer_dims = [2,9,9,1]","09afada9":"import tensorflow as tf","3d694e91":"def placeholders(num_features):\n  A_0 = tf.placeholder(dtype = tf.float64, shape = ([num_features,None]))\n  Y = tf.placeholder(dtype = tf.float64, shape = ([1,None]))\n  return A_0,Y","f42170b4":"def initialize_parameters_deep(layer_dims):\n  L = len(layer_dims)\n  parameters = {}\n  print(\"Initializing parameters \\n L : {}\".format(L))\n  for l in range(1,L):\n    print(\"l : {}\".format(l))\n    parameters['W' + str(l)] = tf.Variable(initial_value=tf.random_normal([layer_dims[l], layer_dims[l-1]], dtype=tf.float64)* 0.01)\n    parameters['b' + str(l)]=tf.Variable(initial_value=tf.zeros([layer_dims[l],1],dtype=tf.float64) * 0.01)\n  print(parameters)\n  return parameters ","f197f272":"def linear_forward_prop(A_prev,W,b, activation):\n  Z = tf.add(tf.matmul(W, A_prev), b)\n  if activation == \"sigmoid\":\n    A = Z\n  elif activation == \"relu\":\n    A = tf.nn.relu(Z)                                           #apply relu activation on Z using tf.nn.relu() function\n  return A","0593e24a":"def l_layer_forwardProp(A_0, parameters):\n  A = A_0\n  L = len(parameters)\/\/2\n  for l in range(1,L):\n    A_prev = A\n    A = linear_forward_prop(A_prev,parameters['W' + str(l)],parameters['b' + str(l)], \"relu\")\n  A_final = linear_forward_prop(A, parameters['W' + str(L)], parameters['b' + str(L)], \"sigmoid\" )\n  return A_final","a1721b4a":"def deep_layer_network( X_data,y_data,  layer_dims, learning_rate, num_iter, X_test=np.nan,Y_test=np.nan):\n  num_features = layer_dims[0]\n  A_0, Y =  placeholders(num_features)                                    #call placeholder function to initialize placeholders A_0 and Y\n  parameters = initialize_parameters_deep(layer_dims)                                    #Initialse Weights and bias using initialize_parameters_deep() with layer_dims as parameters  \n  Z_final =   l_layer_forwardProp(A_0, parameters)                                     #call the function l_layer_forwardProp() to define the final output\n  cost =   tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Z_final,labels=Y))\n  #define the mean cost using sigmoid cross entropy function\n  train_net = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n  #call tensorflow's gradient descent optimizer function with minimize cost  \n  init = tf.global_variables_initializer()    \n  \n  with tf.Session() as sess:\n    sess.run(init)\n    for i in range(num_iter):\n      _,c = sess.run([train_net, cost], feed_dict={A_0: X_data, Y: y_data})\n      if i % 1000 == 0:\n        print(c)\n    if X_test.any() and Y_test.any():\n        correct_prediction = tf.equal(tf.round(tf.sigmoid(Z_final)), Y)\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n    \n        print(\"Accuracy on test set:\", accuracy.eval({A_0: X_test, Y: Y_test}))\n    with open(\"Output.txt\", \"w\") as text_file:\n      text_file.write(\"c= %f\\n\" % c)\n    params = sess.run(parameters)\n  return params","b753bbb4":"parameters = deep_layer_network(X_train,Y_train, layer_dims, learning_rate=0.3, num_iter=10000, X_test=X_test, Y_test= Y_test)    #fill the parameters ","8ac12dd0":"def predict(A_0, parameters):\n  with tf.Session() as sess:\n    Z = l_layer_forwardProp(A_0, parameters)\n    A = sess.run(tf.round(tf.sigmoid(Z)))\n  return A","d2578b6d":"plot_decision_boundary(X.T,y,lambda x: predict(x.T,parameters))","172f23d3":"print(X.T.shape, y.shape, X_train.shape, Y_train.reshape(-1).shape)","625139eb":"plot_decision_boundary(X_train,Y_train.reshape(-1),lambda x: predict(x.T,parameters))","a3e4ac12":"## Define DNN\n- Train the deep neural network with learning rate 0.3 and number of iterations to 10000\n- Use X_data and Y_data to train the network","7b774f9a":"- Run the cell below cell to plot the decision boundary perdicted by logistic regression model","02908e31":"## Plotting Training Data\n- Run the below piece of  code to visualize the data in x-y plane.The green and blue dots corresponds to class 0 and 1 respectively\n- You can see that the data is not linearly seperable i.e you cannot draw one specific boundary to classify the data.","e0c4c550":"## Experimenting with Logistic Regression\n- Before diving into deep neural network lets try to classify the data using simple logistic regression.    \n- The code for logistic regression has been written for you.\n- Run the below cell to build a simple logistic regression model","df9a5687":"## Initialize weights and bias for each layer\nDefine function named initialize_parameters_deep() to initialize weights and bias for each layer\n- Use tf.random_normal() to initialise weights and tf.zeros() to initialise bias. Set datatype as float64\n- Parameters - layer_dims\n- Returns - dictionary of weights and bias","e7a6e08c":"Import tensorflow package as tf","3e1fb98a":"## Plotting Decision Boundary for LR\n- Run the below cell to define the method to plot the decision boundary.The code for visualization has been written for you.","bd265769":"## Define Predict Method \nRun the cell below to define the method to predict outputof the model for given input and parameters.The code has been written for you","25c89218":"- From the above plot you can say that simple logistic regression poorly perfroms in classifying the data since the decision boundary is not able to effectively classify the two classes.\n- Now build a deep neural network to classify the same data.\n\n## Preparing the data \nPreparing the data:  \n    - Transpose X so that rows represents the features and column represents samples. Assig the resulting array to variable X_data\n    - Reshape data y to a row vector whose length equal to number of samples.Assign the resulting vector to variable y_data","a53e67b4":"The following code extacts features and target variable and assign it to variable X and y respectively","ea93c7a3":"## Define forward propagation for entire network\nDefine forward propagation for entire network as l_layer_forward()\n- Parameters: A_0(input data), parameters(dictionary of weights and bias)\n- returns: A(output from final layer)","b6a5b921":"## Defining Placeholders for Input and Output\nDefine a function named placeholders to return two placeholders one for input data as A_0 and one for output data as Y.\n- Set the datatype of placeholders as float64\n- parameters - num_features\n- Returns - A_0 with shape (num_feature, None) and Y with shape(1,None)\n","ed643d39":"## Loading Input Data\nThe data is provided as file named 'blobs.csv'.  \nUsing pandas read the csv file and assign the resulting dataframe to variable 'data'   \nfor example if file name is 'xyz.csv' read file as **pd.read_csv('xyz.csv')** ","f18fc5eb":"## Plot the DNN Decision Boundary\nRun the below cell to plot the decision boundary predicted by the deep nural network","f3474ec1":"## Define model\n- Define the model as deep_layer_network()\n- Parameters: X(input data), Y(output data), layer_dims, learning_rate, num_iter(number of iterations)\n- returns: parameters ( dictionary of updated weights and bias)","459c4991":"## Defining Layer Dimension\nDefine the layer dimension as an array  called **'layer_dims'**  with one input layer **equal to number of features**, **two hidden layer** with **nine nodes** each and **one final output layer** with** one node**.\n","21e32b23":"## Define forward propagation For Current Layer\nDefine functon named linear_forward_prop() to define forward propagation for a given layer.\n- parameters: A_prev(output from previous layer), W(weigth matrix of current layer), b(bias vector for current layer),activation(type of activation to be used for out of current layer)  \n- returns: A(output from the current layer)\n- Use relu activation for hidden layers and for final output layer return the output unactivated i.e if activation is sigmoid","a9c6d69f":"## Previous Recommended Reading\n- https:\/\/www.kaggle.com\/jsrshivam\/shallow-network-tutorial-using-tensorflow\n\n***\n## Introduction\n- In this handson you will be building a deep neural network using tensorflow for binary classification\n- The dataset has two features('feature1' and 'feature2') and one target variable\n- The target variable(named as 'class') maps each record to either 0 or 1\n- Some of the necessary pacakges required to read file and data visualization has been imported for you\n***\n"}}