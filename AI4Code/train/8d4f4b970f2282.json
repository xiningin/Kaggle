{"cell_type":{"4048831d":"code","4ec50b9e":"code","9ffebde1":"code","a1d6def4":"code","0745c3f7":"code","4936e0f7":"code","ee938e9d":"code","c524ad59":"code","ecd7588b":"code","2338cbc6":"code","4554b46e":"code","2ab3f7c4":"code","1118c4a7":"code","5633c510":"code","c99b7ecc":"code","602779ea":"code","05a1833b":"code","9f254e2f":"markdown","1ccb863b":"markdown","686e4dff":"markdown","94a4f4e3":"markdown","1c833d79":"markdown","b1264e04":"markdown","61e6169a":"markdown","5022d93d":"markdown","6aec6d6f":"markdown","42bafff3":"markdown","a2e8556b":"markdown","577c56ec":"markdown","1a94ec8a":"markdown","9b3fde6f":"markdown","0c05ccf2":"markdown","21063571":"markdown","5ccda37d":"markdown","7de33eb6":"markdown","5b6fab91":"markdown","af4820d0":"markdown","b25c3acb":"markdown","6e31493f":"markdown"},"source":{"4048831d":"#Data Wrangling\nimport numpy as np\nimport pandas as pd\n\n#Data Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Model selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.pipeline import make_pipeline\n\n#Model evaluation\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV","4ec50b9e":"df = pd.read_csv(\"..\/input\/rock-density-xray\/rock_density_xray.csv\")","9ffebde1":"df.head()","a1d6def4":"df.info()","0745c3f7":"df.describe().transpose()","4936e0f7":"df.isna().sum()","ee938e9d":"plt.figure(figsize = (8,4), dpi = 100)\nsns.scatterplot(data = df, x = df.columns[0], y = df.columns[1])\nplt.show()","c524ad59":"# Train Test Split\nX = df['Rebound Signal Strength nHz'].values.reshape(-1,1)  \ny = df['Rock Density kg\/m3']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=101)","ecd7588b":"def run_model(model,X_train,y_train,X_test,y_test):\n    \n    # Fit Model\n    model.fit(X_train,y_train)\n    \n    # Get Metrics\n    \n    preds = model.predict(X_test)\n    \n    rmse = np.sqrt(mean_squared_error(y_test,preds))\n    mean = df[df.columns[1]].mean()\n    print(f'RMSE : {rmse}')\n    print(f'MEAN Y : {mean}')\n    \n    \n    # Plot results\n    signal_range = np.arange(0,100)\n    output = model.predict(signal_range.reshape(-1,1))\n    \n    \n    plt.figure(figsize=(12,6),dpi=150)\n    sns.scatterplot(x=df.columns[0],y=df.columns[1],data=df,color='black')\n    plt.plot(signal_range,output)","2338cbc6":"lr = LinearRegression()\nrun_model(lr,X_train,y_train,X_test,y_test)","4554b46e":"#Third degree polynomial: play around with the degree yourself\npipe = make_pipeline(PolynomialFeatures(3),LinearRegression())\nrun_model(pipe,X_train,y_train,X_test,y_test)","2ab3f7c4":"#Third degree polynomial: play around with the degree yourself\npipe = make_pipeline(PolynomialFeatures(7),LinearRegression())\nrun_model(pipe,X_train,y_train,X_test,y_test)","1118c4a7":"k_values = [1,5,10]\nfor n in k_values:\n    model = KNeighborsRegressor(n_neighbors=n)\n    run_model(model,X_train,y_train,X_test,y_test)","5633c510":"model = DecisionTreeRegressor()\nrun_model(model,X_train,y_train,X_test,y_test)","c99b7ecc":"trees = [10,50,100]\nfor n in trees:\n    model = RandomForestRegressor(n_estimators=n)\n    run_model(model,X_train,y_train,X_test,y_test)","602779ea":"model = GradientBoostingRegressor()\nrun_model(model,X_train,y_train,X_test,y_test)","05a1833b":"model = AdaBoostRegressor()\nrun_model(model,X_train,y_train,X_test,y_test)","9f254e2f":"#### **5. Random Forest Regression**","1ccb863b":"#### **6. Gradient Boosting**","686e4dff":"The Roote Mean Square Error (RMSE) is 0.257. When compare to a mean of 2.225 it seems that the model is doing a very good job. But, when we look at the graph we can see that the model is not picking any singal from the data i.e it did a very poor job. ","94a4f4e3":"#### **7. Adaboost**","1c833d79":"<h2 style=\"background-color:#f15a39; padding: 20px; font-family:cursive\">Outline<\/h2>","b1264e04":"<a id = \"compare\"><\/a>\n<h2 style=\"background-color:#f15a39; padding: 20px; font-family:cursive;\">Comparing the performance of each model<\/h2>","61e6169a":"10 trees is very low, and 100 is ver large. 50 trees seems to be a good choice. ","5022d93d":"In this notebook we tried different regression techniques on a very simple data set in order to illusterate how each regression model differs from the rest. each of them have a very close performance. But ensemble methods seems to be the best, specifically Gradient Boosting. We did not do any hyper parameter tuning, this might be done in following notebooks.","6aec6d6f":"#### **2. Polynomial Regression**","42bafff3":"<a id = \"check\"><\/a>\n<h2 style=\"background-color:#f15a39; padding: 20px; font-family:cursive;\">Quick quality check<\/h2>","a2e8556b":"The model has only one dependent variable, therefore there is no space for hyper prameter tuning. Although the RMSE is minimized, the model is picking so much noise, the model is not likely to generalize well on new data with larger scale. ","577c56ec":"At number of neighbors = 1 the model is picking so much noise, as it increased to 10 we allowed more bias and thus the model started to work better.","1a94ec8a":"We will build a fuction to make the comparison between models easier.The function will tanke in the model, the training and test data. It will return the evaluation metric \"Root Mean Square Error\" and a scatter plot of the data with the regression line. ","9b3fde6f":"#### **4. Decision Tree Regression**","0c05ccf2":"### **Results**:\n1. The data is clean\n2. All features are in the right type and fromat\n3. No missing values\n4. No outliers","21063571":"When it comes to predicting continuous outcomes, Linear Regression is the most popular algorithm. However many of us did no know that there is might be other more efficient Algorithms in this regard. \n\nAlgorithms such as KNN, Descision Tress, and Gradient Boosting are very famous for their usage in classification problems. But, it turns out that they can do a very good job in regression problems. In this notebook we will show case the usage of these algorithms and evaluate their performance.","5ccda37d":"For the 6th degree polynomial, the Roote Mean Square Error (RMSE) is 0.136, the model is doing a very good job. At the same time, graph also shows that the model is fitting the data pretty well. ","7de33eb6":"<a id = \"pipe\"><\/a>\n<h2 style=\"background-color:#f15a39; padding: 20px; font-family:cursive;\">Building a pipline function<\/h2>","5b6fab91":"#### **3. KNN Regression**","af4820d0":"#### **1. Linear Regression**","b25c3acb":"1. [Package imports](#imports)\n2. [Quick quality check](#check)\n3. [Building a pipline function](#pipe)\n4. [Comparing the performance of each model](#compare)","6e31493f":"<a id = \"imports\"><\/a>\n<h2 style=\"background-color:#f15a39; padding: 20px; font-family:cursive;\">Package imports<\/h2>"}}