{"cell_type":{"15ebdfe7":"code","c9b0d1cd":"code","9e5791b9":"code","b7fca844":"code","a15ac349":"code","da04c99b":"code","b68ca637":"code","dfe40779":"code","d58c3c6e":"code","40c0b67d":"code","e5c0caa5":"code","fb16ac5c":"code","1741051d":"code","1179b618":"code","c8cd18e6":"code","0a32a40a":"code","1224fa75":"code","e1db328f":"code","0244d5f5":"code","81b50246":"code","f6438911":"code","d7294d8f":"code","79de32db":"code","ea04df9b":"code","1d5a2eae":"code","4feef8ef":"code","d6952373":"markdown","8c234f52":"markdown"},"source":{"15ebdfe7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","c9b0d1cd":"x=pd.read_csv(\"..\/input\/amazon.csv\",header=None,encoding=\"latin1\")","9e5791b9":"x.head()","b7fca844":"x.info()","a15ac349":"y=pd.read_csv(\"..\/input\/imdb.csv\",sep=\";\",header=None,encoding=\"latin1\")\n","da04c99b":"y.head()","b68ca637":"y.info()","dfe40779":"z=pd.read_csv(\"..\/input\/yelp.csv\",sep=\";\",header=None,encoding=\"latin1\")","d58c3c6e":"z.head()","40c0b67d":"z.info()","e5c0caa5":"#concanate all the data in a variable.\ndata=pd.concat([x,y,z],axis=0)\ndata.shape","fb16ac5c":"#first 5 samples \ndata.head()","1741051d":"#Change the columns name\ndata.columns=[\"sentences\",\"sinif\"]\ndata.head()","1179b618":"data.isnull().sum()\n#data information\ndata.info()","c8cd18e6":"data.index=range(0,len(data),1)","0a32a40a":"import re\nimport nltk as nlp\ndescription_list = []\nfor description in data.sentences:\n    description = re.sub(\"[^a-zA-Z]\",\" \",description)#we remove the words \"[a-zA-Z]\" inside the sentence.\n    description = description.lower()   # Capitalize letters  to lower case\n    description = nlp.word_tokenize(description)#we divide each sentence into words\n    #description = [ word for word in description if not word in set(stopwords.words(\"english\"))]\n    lemma = nlp.WordNetLemmatizer() # it helps us find the root of the word.\n    description = [ lemma.lemmatize(word) for word in description]\n    description = \" \".join(description) #we have created a sentence by combining words\n    description_list.append(description)\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n # the method we use to create bag of words.\nmax_features= 7000 #get the most used 7000 words.\n\n#Stopwords.\ncount_vectorizer = CountVectorizer(max_features=max_features,stop_words = \"english\")\n\n#translates sentences into codes of 0 and 1\nsparce_matrix = count_vectorizer.fit_transform(description_list).toarray()  \n ","1224fa75":"y = data.iloc[:,1].values \nx = sparce_matrix","e1db328f":"#split data\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.1, random_state = 42)\n","0244d5f5":"#Navie Bayes Classification Algorithm\nfrom sklearn.naive_bayes import GaussianNB\nnb=GaussianNB()\n#E\u011fitim\nnb.fit(x_train,y_train)\n#Test\ny_pred=nb.predict(x_test)\nprint(\"Navie Bayes algorithm accuracy:\",nb.score(x_test,y_test)*100)\n\n#Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm_nb=confusion_matrix(y_test,nb.predict(x_test))\nf,ax=plt.subplots(figsize=(5,5))\nsns.heatmap(cm_nb,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)","81b50246":"#KNN Classification Algorithm\nfrom sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier(n_neighbors=2)\nknn.fit(x_train,y_train)\ny_pred=knn.predict(x_test)\nprint(\"Knn algorithm accuracy=\",knn.score(x_test,y_test)*100)\n\n#Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm_knn=confusion_matrix(y_test,knn.predict(x_test))\nf,ax=plt.subplots(figsize=(5,5))\nsns.heatmap(cm_knn,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)","f6438911":"#Decision Tree Classification Algorithm\nfrom sklearn.tree import DecisionTreeClassifier\ndt=DecisionTreeClassifier()\ndt.fit(x_train,y_train)\ny_pred=dt.predict(x_test)\nprint(\"Decision Tree algorithm accuracy=\",dt.score(x_test,y_test)*100)\n#Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm_dt=confusion_matrix(y_test,dt.predict(x_test))\nf,ax=plt.subplots(figsize=(5,5))\nsns.heatmap(cm_dt,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)","d7294d8f":"#Random Forest Classification Algorithm\nfrom sklearn.ensemble import RandomForestClassifier\nrf=RandomForestClassifier(n_estimators=100,random_state=1)\nrf.fit(x_train,y_train)\nprint(\"Random Forest algorithm accuracy =\",rf.score(x_test,y_test)*100)\n\n#Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm_rf=confusion_matrix(y_test,rf.predict(x_test))\nf,ax=plt.subplots(figsize=(5,5))\nsns.heatmap(cm_rf,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)","79de32db":"#Logistic Regression Classification Algorithm\nfrom sklearn.linear_model import LogisticRegression\nlr=LogisticRegression()\nlr.fit(x_train,y_train)\nprint(\"Logistic Regression algorithm accuracy =\",lr.score(x_test,y_test)*100)\n\n#Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm_lr=confusion_matrix(y_test,lr.predict(x_test))\nf,ax=plt.subplots(figsize=(5,5))\nsns.heatmap(cm_lr,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)","ea04df9b":"#Support Vector Machine Classification Algorithm\nfrom sklearn.svm import SVC\nsvm=SVC()\nsvm.fit(x_train,y_train)\nprint(\"SVM algorithm accuracy\",svm.score(x_test,y_test)*100)\n\n#Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm_svm=confusion_matrix(y_test,svm.predict(x_test))\nf,ax=plt.subplots(figsize=(5,5))\nsns.heatmap(cm_svm,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)","1d5a2eae":"plt.figure(figsize=(24,24))\n\nplt.suptitle(\"Confusion Matrixes\",fontsize=24)\n#Logistic Regression Confusion Matrix\nplt.subplot(2,3,1)\nplt.title(\"Logistic Regression Confusion Matrix\")\nsns.heatmap(cm_lr,cbar=False,annot=True,linewidths=2,linecolor=\"orange\",fmt=\".0f\")\n\n#Decision Tree Confusion Matrix\nplt.subplot(2,3,2)\nplt.title(\"Decision Tree Classifier Confusion Matrix\")\nsns.heatmap(cm_dt,cbar=False,annot=True,linewidths=2,linecolor=\"orange\",fmt=\".0f\")\n\n#K Nearest Neighbors Confusion Matrix\nplt.subplot(2,3,3)\nplt.title(\"K Nearest Neighbors Confusion Matrix\")\nsns.heatmap(cm_knn,cbar=False,annot=True,linewidths=2,linecolor=\"orange\",fmt=\".0f\")\n\n#Naive Bayes Confusion Matrix\nplt.subplot(2,3,4)\nplt.title(\"Naive Bayes Confusion Matrix\")\nsns.heatmap(cm_nb,cbar=False,annot=True,linewidths=2,linecolor=\"orange\",fmt=\".0f\")\n\n#Random Forest Confusion Matrix\nplt.subplot(2,3,5)\nplt.title(\"Random Forest Confusion Matrix\")\nsns.heatmap(cm_rf,cbar=False,annot=True,linewidths=2,linecolor=\"orange\",fmt=\".0f\")\n\n#Support Vector Machine Confusion Matrix\nplt.subplot(2,3,6)\nplt.title(\"Support Vector Machine Confusion Matrix\")\nsns.heatmap(cm_svm,cbar=False,annot=True,linewidths=2,linecolor=\"orange\",fmt=\"d\")\n\nplt.show()\n","4feef8ef":"algorithms=[\"Navie Bayes\",\"KNN\",\"DecisionTree\",\"Random Forest\",\"Logistic Regression\",\"Support Vector Machine\"]\naccuracy_score=[nb.score(x_test,y_test)*100,knn.score(x_test,y_test)*100,dt.score(x_test,y_test)*100,rf.score(x_test,y_test)*100,lr.score(x_test,y_test)*100,svm.score(x_test,y_test)*100]\n\n\ntrace1=go.Bar(\n        x=algorithms,\n        y=accuracy_score,\n        name=\"Classification Algorithms\",\n        marker=dict(color=\"rgba(147, 255, 128, 0.5)\",\n                    line=dict(color=\"rgb(0,0,0)\",width=1.5)))\n\ndata=[trace1]\nlayout=dict(title=\"Positive and Negative Sentences Classification\",\n            xaxis=dict(title=\"Classification Algorithms\",ticklen=5,zeroline=False),\n            yaxis=dict(title=\"Accuracy Score\"))\n\nfig=dict(data=data,layout=layout)\niplot(fig)","d6952373":"**Conclusion**\n\nI made the text classification according to the Amazon, yelp and imdb comments. I showed the accuracy rate and comparison matrix according to the classification algorithms. \nAccording to these results, the highest score was found in the logistic regression algorithm.\nFor this study, instead of using the Support Vector Machine algorithm, you may want to estimate yourself. :D","8c234f52":"** In this study,I made text classification. The aim of the study is to classify the sentences as negative and positive.**\n\n**Let's start!**"}}