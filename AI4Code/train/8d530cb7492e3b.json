{"cell_type":{"98f33b23":"code","92d80a41":"code","beaf65eb":"code","29f068c3":"code","c90eedab":"code","4636c615":"code","cf9d0744":"code","ff260da5":"code","c355f71d":"code","5803d5ea":"code","346fa8ad":"code","343acdb7":"code","388dd905":"code","7c6a3e3a":"code","960a9ada":"code","d7c06546":"code","ec6b1c42":"code","45df9f98":"code","562a0442":"code","55205a3b":"markdown","35217bd6":"markdown","b3e1c5de":"markdown","24dee8b2":"markdown","be18d967":"markdown","50e40472":"markdown","95794dcb":"markdown","68f90a1d":"markdown","a6afc1b6":"markdown","0c58dd50":"markdown","783ab3cf":"markdown","a0349318":"markdown"},"source":{"98f33b23":"import numpy as np\nimport os\nimport glob\nimport re\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom random import shuffle\nfrom skimage.util.shape import view_as_blocks\nfrom skimage import io, transform\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation, Flatten\nfrom keras.layers.convolutional import Convolution2D, MaxPooling2D\nimport warnings\nwarnings.filterwarnings('ignore')","92d80a41":"train_size = 5000\ntest_size = 1500\n\ntrain = glob.glob(\"..\/input\/dataset\/train\/*.jpeg\")\ntest = glob.glob(\"..\/input\/dataset\/test\/*.jpeg\")\n\nshuffle(train)\nshuffle(test)\n\ntrain = train[:train_size]\ntest = test[:test_size]\n\npiece_symbols = 'prbnkqPRBNKQ'","beaf65eb":"def fen_from_filename(filename):\n  base = os.path.basename(filename)\n  return os.path.splitext(base)[0]","29f068c3":"print(fen_from_filename(train[0]))\nprint(fen_from_filename(train[1]))\nprint(fen_from_filename(train[2]))","c90eedab":"f, axarr = plt.subplots(1,5, figsize=(20, 30))\n\nfor i in range(0,5):\n    axarr[i].set_title(fen_from_filename(train[i]), fontsize=10, pad=20)\n    axarr[i].imshow(mpimg.imread(train[i]))\n    axarr[i].axis('off')","4636c615":"#NORNAL\ndef images(images_path, image_height, image_width):\n    imges_list = []\n\n    for image in tqdm(os.listdir(images_path)):\n        path = os.path.join(images_path, image)\n\n        image = cv2.imread(path)\n        image = cv2.resize(image , (image_height, image_width))\n        imges_list.append([np.array(image)])\n    shuffle(imges_list)\n\n   #Convert List Into Array\n    array_image = np.array(imges_list)\n\n    #Removed Dimention \n    images = array_image[:,0,:,:]\n    return(images)","cf9d0744":"def onehot_from_fen(fen):\n    eye = np.eye(13)\n    output = np.empty((0, 13))\n    fen = re.sub('[-]', '', fen)\n\n    for char in fen:\n        if(char in '12345678'):\n            output = np.append(\n              output, np.tile(eye[12], (int(char), 1)), axis=0)\n        else:\n            idx = piece_symbols.index(char)\n            output = np.append(output, eye[idx].reshape((1, 13)), axis=0)\n\n    return output\n\ndef fen_from_onehot(one_hot):\n    output = ''\n    for j in range(8):\n        for i in range(8):\n            if(one_hot[j][i] == 12):\n                output += ' '\n            else:\n                output += piece_symbols[one_hot[j][i]]\n        if(j != 7):\n            output += '-'\n\n    for i in range(8, 0, -1):\n        output = output.replace(' ' * i, str(i))\n\n    return output","ff260da5":"def process_image(img):\n    downsample_size = 200\n    square_size = int(downsample_size\/8)\n    img_read = io.imread(img)\n    img_read = transform.resize(\n      img_read, (downsample_size, downsample_size), mode='constant')\n    tiles = view_as_blocks(img_read, block_shape=(square_size, square_size, 3))\n    tiles = tiles.squeeze(axis=2)\n    return tiles.reshape(64, square_size, square_size, 3)","c355f71d":"def train_gen(features, labels, batch_size):\n    for i, img in enumerate(features):\n        y = onehot_from_fen(fen_from_filename(img))\n        x = process_image(img)\n        yield x, y\n\ndef pred_gen(features, batch_size):\n    for i, img in enumerate(features):\n        yield process_image(img)","5803d5ea":"img_rows = 25\nimg_cols = 25\n\n# Setup hyper parameters for deep learning\nEPOCHS = 100\nBATCH_SIZE = 10\nVERBOSE = 1\nNB_CLASSES = 13\nVALIDATION_SPLIT = 0.2","346fa8ad":"model = Sequential()","343acdb7":"from keras.layers import Dense, Conv2D, Flatten, MaxPool2D\n","388dd905":"# Add a convolutional layer\nmodel.add(Convolution2D(15, kernel_size=2, activation='relu', \n                 input_shape=(img_rows, img_cols, 3)))\n\n\n# Add a pooling operation\nmodel.add(MaxPool2D(2))\n\n# Add another convolutional layer\nmodel.add(Convolution2D(5, kernel_size=2, activation='relu'))\n\n# Flatten and feed to output layer\nmodel.add(Flatten())\n# Add a dropout layer\nmodel.add(Dropout(0.3))\nmodel.add(Dense(13, activation='softmax'))\nmodel.summary()","7c6a3e3a":"# Compile the model \nmodel.compile(optimizer='adam', \n              loss='categorical_crossentropy', \n              metrics=['accuracy'])","960a9ada":"# Fit parameters\nprint(\"\\nTraining Progress:\\n------------------------\")\nmodel.fit_generator(train_gen(train, None, 64), steps_per_epoch=train_size)","d7c06546":"# Evaluate the model on test data\nres = (\n  model.predict_generator(pred_gen(test, 64), steps=test_size)\n  .argmax(axis=1)\n  .reshape(-1, 8, 8)\n)\nres","ec6b1c42":"pred_fens = np.array([fen_from_onehot(one_hot) for one_hot in res])\ntest_fens = np.array([fen_from_filename(fn) for fn in test])\n\nfinal_accuracy = (pred_fens == test_fens).astype(float).mean()\n\nprint(\"Final Accuracy: {:1.5f}%\".format(final_accuracy))","45df9f98":"def display_with_predicted_fen(image):\n    pred = model.predict(process_image(image)).argmax(axis=1).reshape(-1, 8, 8)\n    fen = fen_from_onehot(pred[0])\n    imgplot = plt.imshow(mpimg.imread(image))\n    plt.axis('off')\n    plt.title(fen)\n    plt.show()","562a0442":"display_with_predicted_fen(test[0])\ndisplay_with_predicted_fen(test[1])\ndisplay_with_predicted_fen(test[2])","55205a3b":"#### Examples:","35217bd6":"### Sample images and display predicted FEN ","b3e1c5de":"### Functions for sampling batches for training and evaluation:","24dee8b2":"### Functions to convert FEN to one-hot encoded vectors and vice-versa","be18d967":"### Data Import","50e40472":"#### Function to proccess an image:\n-  downsample an image to 200 by 200 pixel\n-  split an image of the chess board to 64 images of individual squares\n-  drop redundant dimensions, reshape","95794dcb":"### Define a model:","68f90a1d":"### Train the model:","a6afc1b6":"### Testing the model:","0c58dd50":"### Calculating an accuracy of the model:","783ab3cf":"### Plotting samples","a0349318":"### Function to extract FEN from filename"}}