{"cell_type":{"b6a023fb":"code","bd72e4b1":"code","f8e666c2":"code","7e684348":"code","8c4c86a7":"code","ab5e4c74":"code","26b50a73":"code","756dd2d9":"code","2e1e2985":"code","c9a4b005":"code","ed4ecbd8":"code","cc35326a":"code","f4315939":"code","9a65744a":"code","53621df9":"code","82b3dba3":"code","214744b2":"code","4b1400a8":"code","90af6417":"code","8fdec496":"code","e34afc02":"code","561c40c0":"code","c3052635":"code","0507880a":"code","1a6a5166":"code","b09101ab":"code","4361c79b":"code","262c8367":"code","92c730ff":"code","937d58f9":"code","bc70d086":"code","a518c067":"code","0600a300":"code","90265950":"code","10966070":"code","f5729f84":"code","57e06e8a":"code","9bdb84af":"code","68d6c348":"code","f433a852":"code","870a8e8d":"code","9c58b2d2":"code","a9c5167d":"code","1b762e4e":"code","cce5e8b8":"code","373f337e":"code","26878723":"code","7d749bb0":"code","ff6ea033":"code","8649e842":"code","01313f34":"code","453db36b":"code","cef50e39":"markdown","1b2fc81c":"markdown","2426993b":"markdown","47b1b17f":"markdown","f8cb633b":"markdown","223310c6":"markdown","6da8ac0d":"markdown","9185b91b":"markdown","b2e24428":"markdown","9a590805":"markdown","b08b9f3f":"markdown","29e85a13":"markdown","f47b7307":"markdown","0b535552":"markdown","bc90c79f":"markdown","2d2adf73":"markdown","ae96d5c4":"markdown","7176cb3c":"markdown","ce0da394":"markdown","7280f9cd":"markdown","ee727095":"markdown","a8d608ed":"markdown","09b746b3":"markdown","a79cb918":"markdown","2478a5ea":"markdown","91a79ac7":"markdown","a199a18b":"markdown","211cfa1f":"markdown","f4f188a2":"markdown","7a272a31":"markdown","e8b1235a":"markdown","d5cabfce":"markdown","08399e24":"markdown","fcd4c212":"markdown","6e370cbb":"markdown","6efd8114":"markdown","28fc84b6":"markdown","d5eca22b":"markdown","ec33ce9d":"markdown","8e161c8a":"markdown"},"source":{"b6a023fb":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline ","bd72e4b1":"# path of data \nfilename = '..\/input\/auto-eda\/automobileEDA.csv'\ndf = pd.read_csv(filename)\ndf.head()","f8e666c2":"df.shape","7e684348":"df.isnull().sum().sum()","8c4c86a7":"df1=df[df.isna().any(axis=1)]\ndf1","ab5e4c74":"df.dropna(inplace=True)","26b50a73":"df.isnull().sum().sum()","756dd2d9":"df.dtypes.value_counts()","2e1e2985":"df.describe(include='object')","c9a4b005":"df.describe()","ed4ecbd8":"(df.select_dtypes(include=['object'])).columns","cc35326a":"sns.boxplot(x='body-style',y='price',data=df)","f4315939":"featurecols=df.drop(['price'],axis=1)\nlabel=df['price']","9a65744a":"featurecols.corrwith(label)","53621df9":"abs(featurecols.corrwith(label)).sort_values(ascending=False)","82b3dba3":"from sklearn.linear_model import LinearRegression","214744b2":"lm = LinearRegression()\nlm","4b1400a8":"X = df[['highway-mpg']]\nY = df['price']","90af6417":"lm.fit(X,Y)","8fdec496":"Yhat=lm.predict(X)\nYhat[0:5]  ","e34afc02":"width = 12\nheight = 10\nplt.figure(figsize=(width, height))\nsns.regplot(x=\"highway-mpg\", y=\"price\", data=df)\nplt.ylim(0,)","561c40c0":"width = 12\nheight = 10\nplt.figure(figsize=(width, height))\nsns.residplot(df['highway-mpg'], df['price'])\nplt.show()","c3052635":"lm.intercept_","0507880a":"lm.coef_","1a6a5166":"from sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\n\nprint('The R-square is: ', r2_score(Y, Yhat))\nmse = mean_squared_error(Y, Yhat)\nprint('The mean square error of price and predicted value is: ', mse)","b09101ab":"X = df[['highway-mpg']]\nY = df['price']","4361c79b":"from sklearn.preprocessing import PolynomialFeatures\n\npr=PolynomialFeatures(degree=6)  #Defining our function to convert our feature to a 6th degree polynomial\npoly_feat=pr.fit_transform(X)","262c8367":"X","92c730ff":"pd.DataFrame(poly_feat)","937d58f9":"lm_poly=LinearRegression()\nlm_poly.fit(poly_feat,Y)\npoly_pred=lm_poly.predict(poly_feat)","bc70d086":"from sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\n\nprint('The R-square is: ', r2_score(label, poly_pred))\nmse = mean_squared_error(label, poly_pred)\nprint('The mean square error of price and predicted value is: ', mse)","a518c067":"def PlotPolly(model, independent_variable, dependent_variabble, Name):\n    x_new = np.linspace(15, 55, 100)\n    y_new = model(x_new)\n\n    plt.plot(independent_variable, dependent_variabble, '.', x_new, y_new, '-')\n    plt.title('Polynomial Fit with Matplotlib for Price ~ Length')\n    ax = plt.gca()\n    ax.set_facecolor((0.898, 0.898, 0.898))\n    fig = plt.gcf()\n    plt.xlabel(Name)\n    plt.ylabel('Price of Cars')\n\n    plt.show()\n    plt.close()","0600a300":"x = df['highway-mpg']\ny = df['price']","90265950":"f = np.polyfit(x, y, 6)\np = np.poly1d(f)\nprint(p)","10966070":"PlotPolly(p, x, y, 'highway-mpg')","f5729f84":"numerical_cols=featurecols.select_dtypes(exclude=['object']) #Select all columns which are not object type.","57e06e8a":"lm2=LinearRegression()    #We will use the same object because the only thing different than before is the multiple predictors. \nlm2","9bdb84af":"lm2.fit(numerical_cols, label) #Fitting our numerical columns as predictors and price as label.","68d6c348":"lm2.intercept_","f433a852":"lm2.coef_","870a8e8d":"Y_predicted = lm2.predict(numerical_cols)","9c58b2d2":"plt.figure(figsize=(width, height))\n\n\nax1 = sns.distplot(df['price'], hist=False, color=\"r\", label=\"Actual Value\")\nsns.distplot(Y_predicted, hist=False, color=\"b\", label=\"Fitted Values\" , ax=ax1)\n\n\nplt.title('Actual vs Fitted Values for Price')\nplt.xlabel('Price (in dollars)')\nplt.ylabel('Proportion of Cars')\n\nplt.show()\nplt.close()","a9c5167d":"from sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\n\nprint('The R-square is: ', r2_score(label, Y_predicted))\nmse = mean_squared_error(label, Y_predicted)\nprint('The mean square error of price and predicted value is: ', mse)","1b762e4e":"from sklearn.preprocessing import PolynomialFeatures","cce5e8b8":"pr=PolynomialFeatures(degree=2)\npr","373f337e":"poly_feat=pr.fit_transform(numerical_cols)","26878723":"numerical_cols.shape","7d749bb0":"poly_feat.shape","ff6ea033":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\npoly_feat=scaler.fit_transform(poly_feat)  #Apply standardization to our polynomial features ","8649e842":"lm4=LinearRegression()\nlm4.fit(poly_feat,label)   \nYpoly_predicted = lm4.predict(poly_feat)","01313f34":"plt.figure(figsize=(width, height))\n\n\nax1 = sns.distplot(df['price'], hist=False, color=\"r\", label=\"Actual Value\")\nsns.distplot(Ypoly_predicted, hist=False, color=\"b\", label=\"Fitted Values\" , ax=ax1)\n\n\nplt.title('Actual vs Fitted Values for Price')\nplt.xlabel('Price (in dollars)')\nplt.ylabel('Proportion of Cars')\n\nplt.show()\nplt.close()","453db36b":"from sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\n\nprint('The R-square is: ', r2_score(label, Ypoly_predicted))\nmse = mean_squared_error(label, Ypoly_predicted)\nprint('The mean square error of price and predicted value is: ', mse)","cef50e39":"# 1. Linear Regression and Polynomial Regression","1b2fc81c":"#### What is the value of the Slope (b)?","2426993b":"Above we confirm the increase in accuracy was because the function fits more the data points, improving the prediction.","47b1b17f":"**Now we could predict the price of a car with a relatively high accuracy by only giving the features of the new one and use these in the argument of the function lm4.predict( )**","f8cb633b":"#### What is the value of the intercept(a)?","223310c6":"### Hello!, Welcome to my first project!, today I will show you how to explore your data and build some basic models using sklearn library in order to predict car price based on using one or several features.\n\nFor this project we will use a dataset created by an automobile importer, which stores several characteristics of cars and its corresponding prices. \n### Let's get started!","6da8ac0d":"Now fit the linear model using highway-mpg as feature and price as label.","9185b91b":"As this is a linear regression function, we expect to obtain its intercept and slope:","b2e24428":"# Polynomial features \nConsidered as a particular case of the general linear regression model or multiple linear regression models, we get non-linear relationships by squaring or setting higher-order terms of the predictor variables.","9a590805":"We see the performance of this last model is almost perfect!, both curves are almost the same and we can see the difference between them quantified by computing the error metrics.","b08b9f3f":"### Now, we are going to create polynomial features, then standardize every column and finally feed our linear model with these features:","29e85a13":"Using seaborn library we can easily make a regression plot of these:","f47b7307":"### Error metrics for regression model:\nAs we are dealing with predicting a continuous value, we calculate the errors using mean squared error and coefficient of determination (R2 score):","0b535552":"### Looking for nan or null values:","bc90c79f":"# 2. Multiple linear Regression\nAs we want to get the highest possible accurary from our models we will achieve this when we make use of all posible variables incluiding the categorical (nominal and ordinal), to achieve this we must transform these features using LabelBinarizer, LabelEncoder and OneHotEncoder, but for the current project we will only use numerical features so as to keep our focus on developing our models. We will deal with categorical variables the next project in which we will build more complex models.","2d2adf73":"In models with more accuracy we can expect the residual plot to concentrate much more points near zero in the y-axis. ","ae96d5c4":"We saw earlier in the two plots and now in error metrics that a linear model did not provide the best fit while using highway-mpg as the predictor variable, but we could improve this accuracy by transforming this feature to a polynomial type. ","7176cb3c":"Now we see the error metrics has improved considerably, making our model much more accurate.","ce0da394":"We have to consider this last one as our new \"predictor variable\" despite the fact that it contains 7 columns, we we fit a new linear regression and predict as before:","7280f9cd":"Initially we had 18 features to use as predictors and after converting them to polynomial 2nd degree we see below the total number of features now has increased to 190.","ee727095":"### Now let's fit a model with all numerical features:","a8d608ed":"### Fit the linear model using all of our numeric features above.","09b746b3":"### Removing nan values:","a79cb918":"#### How could Highway-mpg help us predict car price?","2478a5ea":"The regression plot does not seem too accurate for this feature, we can see several points far from the line, which is indicative of underfitting, for this reason we will use a residual plot from seaborn which measures and plots the difference between the predicted and the actual point: ","91a79ac7":"# Import libraries","a199a18b":"#### Create the linear regression object","211cfa1f":"#### What is the value of the intercept (a)?","f4f188a2":"We will use the following code to plot the polynomial function: ","7a272a31":"Finally we will use this processed features to feed our model and expect a much better performance: ","e8b1235a":"#### What are the values of the coefficients (b1, b2, b3, b4, ... , bn)?","d5cabfce":"We can see that the fitted values are reasonably close to the actual values, since the two distributions overlap a bit. However, there is definitely some room for improvement.","08399e24":"How do we visualize a model for Multiple Linear Regression? This gets a bit more complicated because we can't visualize it with regression or residual plot as before.\n\nOne way to look at the fit of the model is by looking at the distribution plot: We can look at the distribution of the fitted values that result from the model and compare it to the distribution of the actual values.","fcd4c212":"To make a prediction of the model we have to use the '.predict( )' and using X as its argument.","6e370cbb":"### Load data and store in dataframe df:","6efd8114":"### Let's see distribution of column types in our dataframe:","28fc84b6":"### Let's transform our feature to polynomial and fit a new model:","d5eca22b":"#### Let's load the modules for linear regression","ec33ce9d":"Below we see our actual feature, an then the 6th degree polynomial created from this: ","8e161c8a":"#### As we know the function of our model will have one coefficient by each feature\nWe should get a final linear function with the following structure:\n$$\nYhat = a + b_1 X_1 + b_2 X_2 + b_3 X_3 + b_4 X_4 + ... + b_n X_n\n$$"}}