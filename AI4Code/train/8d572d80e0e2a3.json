{"cell_type":{"101fb21e":"code","59027059":"code","0c5a63a1":"code","1ae84af7":"code","1f4e697f":"code","61d80353":"code","b83e1731":"code","447f2e44":"markdown","6875eeb4":"markdown","c853b2a8":"markdown","bf7d612b":"markdown","29198009":"markdown","f08f111a":"markdown","8ffba890":"markdown","6e91336e":"markdown","44ccce22":"markdown","ab251c9c":"markdown","ee8c6c95":"markdown"},"source":{"101fb21e":"# standard Python tools\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as ss   # need this for chi-squared function\n\n# special tools for working in Kaggle\nimport joblib   # save and load ML models\nimport gc       # garbage collection\nimport os \n\n# preprocessing steps\nfrom sklearn import preprocessing\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\n\n# machine learning models and tools\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n# surely there will be a lot more packages loaded by the time we are done!","59027059":"MainDir = \"..\/input\/..\/input\/home-credit-default-risk\"\nprint(os.listdir(MainDir))\n\n# Main table\ntrain = pd.read_csv(f'{MainDir}\/application_train.csv')\n\n# Supplemental data - we can create additional feature sets by analyzing these.\nbureau = pd.read_csv(f'{MainDir}\/bureau.csv')\n","0c5a63a1":"train.head(5)","1ae84af7":"print(bureau.shape)                                        # size of table - 17 columns x 1.72 million rows\nprint(bureau.SK_ID_CURR.nunique(), \"unique SK_ID_CURR\")    # number of unique SK_ID_CURR is 305,811, similar to size of train.csv\n\n# identifying column types\ntypes = np.array([z for z in bureau.dtypes])               # array([dtype('float64'), dtype('float64'), dtype('O'), dtype('O') ...])\nall_columns = bureau.columns.values                        # list of all column names\nis_num = types != 'object'                                 # returns array([False, False, False, False,  True,  True, ...) where True is a numeric variable\nnum_features = all_columns[is_num].tolist()                # list of all numeric columns\ncat_features = all_columns[~is_num].tolist()               # list of all categorical columns\nprint(len(num_features), \"numeric features\")               # looks like we have 14 numeric features (including the two key fields)\nprint(len(cat_features), \"categorical features\")           # ... and three categorical features\n\nbureau.head(5)\n# SK_ID_CURR key field will let us merge this data into the train.csv table; SK_ID_BUREAU key field will let us merge with bureau_balance.csv.","1f4e697f":"# check for missing values:\nbureau.isna().sum().to_frame().sort_values(0, ascending = False)\n\n# no categorical variables have missing values. For now, let's assume that missing numeric values are zeroes.\nbureau.fillna(0, inplace = True)\n\n# let's get some stats grouped on SK_ID_CURR\nGrouped = (bureau\n           .groupby('SK_ID_CURR')\n           .agg(\n               {'SK_ID_CURR': 'count', \n                'AMT_CREDIT_SUM': 'sum',\n                'AMT_CREDIT_SUM_DEBT': 'sum',\n                'AMT_CREDIT_SUM_OVERDUE': 'sum',\n                'AMT_ANNUITY' : 'sum',\n                'DAYS_CREDIT': 'max',\n                'CREDIT_DAY_OVERDUE' : 'max',\n                'AMT_CREDIT_MAX_OVERDUE' : 'sum'\n               }\n           )\n          )\nGrouped","61d80353":"#Merge into train.csv\ntrain['SK_ID_CURR'] = train['SK_ID_CURR'].astype(str)\nGrouped['SK_ID_CURR'] = Grouped['SK_ID_CURR'].astype(str)\n\ntrain = train.merge(Grouped, on = 'SK_ID_CURR', how = 'left')\n\n# getting all NA on merged columns\ntrain.isna().sum().to_frame().sort_values(0, ascending = False)\n","b83e1731":"# Description table contains characters that are unprintable with UTF8 encoding, so we need to open it this way:\n\nwith open(f'{MainDir}\/HomeCredit_columns_description.csv', 'r', encoding = 'ISO-8859-1') as csvfile:\n    desc = pd.read_csv(csvfile)\npd.set_option(\"display.max_rows\", None)               # print entire thing, not just first and last rows\npd.options.display.max_colwidth = 100                 # description column\ndesc","447f2e44":"### Import packages","6875eeb4":"## Home Credit Default Risk - Team 3 (Kahsai, Nichols, Pellerito)","c853b2a8":"### Reminder - these are the columns in train","bf7d612b":"# Bureau table","29198009":"# Appendix - data descriptions","f08f111a":"# First look at training data set","8ffba890":"### merge our new features into the training data","6e91336e":"### first look - head of bureau table, size and shape, etc.","44ccce22":"### Some Visualizations","ab251c9c":"### wrangling new features from bureau.csv table","ee8c6c95":"### Read the training data"}}