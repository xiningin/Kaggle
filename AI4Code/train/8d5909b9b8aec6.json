{"cell_type":{"88e7be55":"code","a071c9eb":"code","703e2fa0":"code","4e4815aa":"code","570ea83d":"code","4243bded":"code","a11a9620":"code","dcb6f6be":"code","060c7079":"code","b1fc4a13":"code","05335491":"code","da5d3fc5":"code","561bfb89":"code","882627b8":"code","d11528b8":"code","9930001c":"code","5128df5c":"code","5538cbcd":"code","1388608c":"code","b18506d1":"code","85fa5a79":"code","8cb7d8a9":"code","254a9805":"code","c45a4e78":"code","0934f501":"code","b8366b0f":"code","3e312c2e":"code","16e3d7c7":"code","0ee9059d":"code","6ff5c0e8":"markdown","72b09a53":"markdown","32904cec":"markdown","c0353282":"markdown","5382b80e":"markdown","1f141c69":"markdown","4bf70aeb":"markdown","6dca4c47":"markdown","207f84b7":"markdown","46db2884":"markdown"},"source":{"88e7be55":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Dense, LSTM, Embedding\nimport matplotlib.pyplot as plt\nimport os","a071c9eb":"file_URL = \"https:\/\/storage.googleapis.com\/download.tensorflow.org\/data\/shakespeare.txt\"\nfile_name= \"shakespeare.txt\"\n# get the file path\npath = keras.utils.get_file(file_name, file_URL)","703e2fa0":"raw = open(path, 'rb').read()\nprint(raw[250:400])","4e4815aa":"text = raw.decode(encoding='utf-8')\nprint(text[250:400])","570ea83d":"len(text)","4243bded":"# unique characters\nvocabulary = np.array(sorted(set(text)))\nlen(vocabulary)","a11a9620":"# assign an integer to each character\ntokenizer = {char:i for i,char in enumerate(vocabulary)}","dcb6f6be":"# check characters and its corresponding integer\n\nfor i in range(20):\n    char = vocabulary[i]\n    token = tokenizer[char]\n    print('%4s : %4d'%(repr(char),token))","060c7079":"vector = np.array([tokenizer[char] for char in text])\n\nprint('\\nSample Text \\n')\nprint('-'*70)\nprint(text[:100])\nprint('-'*70)\nprint('\\n\\nCorresponding Integer Vector \\n')\nprint('-'*70)\nprint(vector[:100])\nprint('-'*70)\n","b1fc4a13":"# convert into tensors\nvector = tf.data.Dataset.from_tensor_slices(vector)\n\n# make sequences each of length 100 characters\nsequences = vector.batch(100, drop_remainder=True)","05335491":"def prepare_dataset(seq):\n    input_vector = seq[:-1]\n    target_vector = seq[1:]\n    return input_vector, target_vector\n\ndataset = sequences.map(prepare_dataset)","da5d3fc5":"# check how it looks\n\nfor inp, tar in dataset.take(1):\n    print(inp.numpy())\n    print(tar.numpy())\n    inp_text = ''.join(vocabulary[inp])\n    tar_text = ''.join(vocabulary[tar])\n    print(repr(inp_text))\n    print(repr(tar_text))","561bfb89":"# number of batched sequences\nlen(sequences)\/\/64","882627b8":"AUTOTUNE = tf.data.AUTOTUNE\n# buffer size 10000\n# batch size 64\ndata = dataset.batch(64, drop_remainder=True).repeat()\ndata = data.prefetch(AUTOTUNE)\n# steps per epoch is number of batches available\nSTEPS_PER_EPOCH = len(sequences)\/\/64","d11528b8":"for inp, tar in data.take(1):\n    print(inp.numpy().shape)\n    print(tar.numpy().shape)","9930001c":"model = keras.Sequential([\n    # Embed len(vocabulary) into 64 dimensions\n    Embedding(len(vocabulary), 64, batch_input_shape=[64,None]),\n    # LSTM RNN layers\n    LSTM(512, return_sequences=True, stateful=True),\n    LSTM(512, return_sequences=True, stateful=True),\n    # Classification head\n    Dense(len(vocabulary))\n])","5128df5c":"model.summary()","5538cbcd":"keras.utils.plot_model(model, show_shapes=True, dpi=64)","1388608c":"# test whether model performs good\n\nfor example_inp, example_tar in data.take(1):\n    example_pred = model(example_inp)\n    print(example_tar.numpy().shape)\n    print(example_pred.shape)\n    ","b18506d1":"ids = tf.random.categorical(example_pred[0], num_samples=1)\nids.shape","85fa5a79":"ids[0][-1].numpy()","8cb7d8a9":"# callback to save checkpoints\ncheckpoint_path = os.path.join(\".\/checkpoints\", \"ckpt_{epoch}\")\ncheckpoint_callback = keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True)","254a9805":"model.compile(optimizer='adam', \n              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n\nhistory = model.fit(data, \n                    epochs=10, \n                    steps_per_epoch=STEPS_PER_EPOCH, \n                    callbacks=[checkpoint_callback])","c45a4e78":"plt.plot(history.history['loss'], '+-r')\nplt.title('Performance Analysis', size=16, color='green')\nplt.xlabel('Epochs', size=14, color='blue')\nplt.ylabel('Loss', size=14, color='blue')\nplt.xticks(range(10))\nplt.show()","0934f501":"# reset previous states of model\nmodel.reset_states()","b8366b0f":"sample = 'ANTHONIO:'\n# vectorize the string\nsample_vector = [tokenizer[s] for s in sample]\npredicted = sample_vector\n# convert into tensor of required dimensions\nsample_tensor = tf.expand_dims(sample_vector, 0) \n# broadcast to first dimension to 64 \nsample_tensor = tf.repeat(sample_tensor, 64, axis=0)\n\n# predict next 1000 characters\n# temperature is a sensitive variable to adjust prediction\ntemperature = 0.6\nfor i in range(1000):\n    pred = model(sample_tensor)\n    # reduce unnecessary dimensions\n    pred = pred[0].numpy()\/temperature\n    pred = tf.random.categorical(pred, num_samples=1)[-1,0].numpy()\n    predicted.append(pred)\n    sample_tensor = predicted[-99:]\n    sample_tensor = tf.expand_dims([pred],0)\n    # broadcast to first dimension to 64 \n    sample_tensor = tf.repeat(sample_tensor, 64, axis=0)\n    \n","3e312c2e":"pred_char = [vocabulary[i] for i in predicted]\ngenerated = ''.join(pred_char)\nprint(generated)","16e3d7c7":"sample = 'ANTHONIO:'\n# vectorize the string\nsample_vector = [tokenizer[s] for s in sample]\npredicted = sample_vector\n# convert into tensor of required dimensions\nsample_tensor = tf.expand_dims(sample_vector, 0) \n# broadcast to first dimension to 64 \nsample_tensor = tf.repeat(sample_tensor, 64, axis=0)\n\n# predict next 1000 characters\n# vary temperature to change randomness\ntemperature = 0.8\nfor i in range(1000):\n    pred = model(sample_tensor)\n    # reduce unnecessary dimensions\n    pred = pred[0].numpy()\/temperature\n    pred = tf.random.categorical(pred, num_samples=1)[-1,0].numpy()\n    predicted.append(pred)\n    sample_tensor = predicted[-99:]\n    sample_tensor = tf.expand_dims([pred],0)\n    # broadcast to first dimension to 64 \n    sample_tensor = tf.repeat(sample_tensor, 64, axis=0)\n    \n","0ee9059d":"pred_char = [vocabulary[i] for i in predicted]\ngenerated = ''.join(pred_char)\nprint(generated)","6ff5c0e8":"### vary temperature to see yet different prediction","72b09a53":"# Build Model","32904cec":"# Vectorize Word Characters into Integers","c0353282":"# Create the Environment","5382b80e":"# Inference - Next Character Prediction","1f141c69":"# Train the Model","4bf70aeb":"# Performance Evaluation","6dca4c47":"### Thank You For Your Time!","207f84b7":"# Download Text Data","46db2884":"# Batch and Prefetch Dataset"}}