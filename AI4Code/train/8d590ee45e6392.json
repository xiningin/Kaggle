{"cell_type":{"978446f3":"code","d7394f77":"code","ea46e219":"code","b04ac656":"code","938fde08":"code","e164eb49":"code","1da1555c":"code","5a156781":"code","ea241f7b":"code","b5b8fbd0":"code","b69e8a23":"code","e4ba6d9a":"code","afce5c4a":"code","12cb2869":"code","d7cc9bd3":"code","befd5710":"code","5eca2cbe":"code","b73c2bb6":"code","4d2189d9":"code","b100d1eb":"markdown","7ec4b9d3":"markdown","73b41059":"markdown","2574becc":"markdown","d07180b7":"markdown","a8493a1a":"markdown","7a6c37dc":"markdown"},"source":{"978446f3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d7394f77":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","ea46e219":"from sklearn.datasets import load_breast_cancer\ncancer = load_breast_cancer()\n\nprint(cancer.DESCR)","b04ac656":"df = pd.DataFrame(cancer.data, columns=cancer.feature_names)\ndf['class'] = cancer.target\n\ndf.tail()","938fde08":"print(df.columns)\nprint(df.columns[:10])\nprint(list(df.columns[:10]))\nprint(df[['class'] + list(df.columns[:10])])","e164eb49":"sns.pairplot(df[['class'] + list(df.columns[:10])])\nplt.show()","1da1555c":"sns.pairplot(df[['class'] + list(df.columns[10:20])])\nplt.show()","5a156781":"sns.pairplot(df[['class'] + list(df.columns[20:30])])\nplt.show()","ea241f7b":"print(df.columns[:10])\nprint(df.columns[10:20])\nprint(df.columns[20:30])\ncols = ['mean radius', 'mean texture','mean smoothness', \n        'mean compactness', 'mean concave points',\n        'worst radius', 'worst texture', 'worst smoothness',\n        'worst texture','worst concave points',\n        'class']","b5b8fbd0":"for c in cols[:-1]:\n    sns.histplot(df, x=c, hue=cols[-1], bins=50, stat='probability')\n    plt.show()","b69e8a23":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim","e4ba6d9a":"# df[cols].values : double float - float64\ndata = torch.from_numpy(df[cols].values).float() # single float32 \n\ndata.shape","afce5c4a":"# Split x and y\nx = data[:, :-1]\ny = data[:, -1:]\n\nprint(x.shape, y.shape)","12cb2869":"# define configuration\nn_epochs = 200000\nlearning_rate = 1e-2\nprint_interval = 10000","d7cc9bd3":"# Define costum model\nclass MyModel(nn.Module):\n    \n    def __init__(self, input_dim, output_dim):\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        \n        super().__init__()\n        \n        self.linear = nn.Linear(input_dim, output_dim)\n        self.act = nn.Sigmoid()\n        \n    def forward(self, x):\n        # \/x\/ = (batch_size, input_dim)\n        y = self.act(self.linear(x))\n        # \/y\/ = (batch_size, output_dim)\n        \n        return y","befd5710":"model = MyModel(input_dim=x.size(-1), # x.size() : torch.Size([569, 10]), x.size(-1) : 10\n                output_dim=y.size(-1))\ncrit = nn.BCELoss() # Define BCELoss instead of MSELoss\n\noptimizer = optim.SGD(model.parameters(), # model.parameters() = (w, b)\n                     lr=learning_rate)","5eca2cbe":"for i in range(n_epochs):\n    ## Order is important\n    \n    # 1) y_hat = f(x)\n    y_hat = model(x)\n    \n    # 2) Calculate BCE\n    loss = crit(y_hat, y)\n    \n    ##### Different between ML and DL : use and update gradient \n    \n    # 3) Reset gradient (for only this term)\n    optimizer.zero_grad()\n    \n    # 4) Differential of Loss\n    loss.backward()\n    \n    # 5) Update Gradient with learning_rate, w grad, b grad \n    optimizer.step()\n    \n    if (i + 1) % print_interval == 0:\n        print(\"Epoch %d: loss=%.4e\" % (i + 1, loss))","b73c2bb6":"correct_cnt = (y == (y_hat > .5)).sum() # True == True, False == False \uc778 \uac83, \uc989 \uc81c\ub300\ub85c \uc608\uce21\ud55c \uac1c\uc218 \ud655\uc778\nprint(y.size()) # torch.Size([569, 1])\ntotal_cnt = float(y.size(0)) # 569\n\nprint('Accuracy: %.4f' % (correct_cnt \/ total_cnt))","4d2189d9":"# torch.cat with dim=1 means [(569, 1); (569, 1)], Thus, (569, 2)\ndf = pd.DataFrame(torch.cat([y, y_hat], dim=1).detach().numpy(),\n                  columns=['y', 'y_hat'])\n\nsns.histplot(df, x='y_hat', hue='y', bins=50, stat='probability')\nplt.show()","b100d1eb":"## Let's see the result!","7ec4b9d3":"# **Logistic Regression**","73b41059":"## Load Dataset from sklearn","2574becc":" ## Order is important\n    \n    # 1) y = f(x)\n    y_hat = model(x) # Theta w and b are determined by the dimensions of x and y\n    \n    # 2) Calculate MSE \n    loss = F.mse_loss(y_hat, y)\n    \n    # 3) Reset gradient\n    optimizer.zero_grad() \n    \n    # 4) differential of Loss\n    loss.backward() # w.grad, b.grad have value (w and b saved gradient value)\n    \n    # 5) Gradient descent with learning_rate, w grad, b grad\n    optimizer.step() # Gradientdescent","d07180b7":"## Select features","a8493a1a":"## Train Model with PyTorch","7a6c37dc":"### Pair plot with mean features"}}