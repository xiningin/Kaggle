{"cell_type":{"f866a42b":"code","63025b9d":"code","55cd8cac":"code","9c35a16a":"code","281a8e86":"code","3e127e82":"code","a16aaee5":"code","5c365fe2":"code","2ecc597e":"code","cc9546c0":"code","a885f343":"code","d1a936f9":"code","c2822fd0":"code","27ee6f47":"code","45148aaa":"code","e63803aa":"code","dc224eda":"code","36158de1":"code","ec4515bd":"code","b2bd58b6":"code","270cd756":"code","202103b1":"code","14a7afe5":"code","ae146ba9":"code","c0d1a41a":"code","d6e3a72d":"code","73d5088e":"code","37af2633":"code","9f5eca30":"code","7fdf2e0e":"code","1802419b":"code","019c5e0d":"code","965deac9":"code","830d293a":"code","fc9690c5":"code","51fc1c27":"code","79940405":"code","c5f44a6d":"code","22d37c7f":"code","32a04771":"code","4fbe579d":"code","b03bfe47":"code","d6669fe4":"code","078541c6":"code","c82f35cd":"code","4f51f35d":"code","a47a4cc3":"code","8d6d1f92":"code","4d40db7d":"code","a593556e":"code","8384a681":"code","df868f5c":"code","5c1c9809":"code","98a8db5b":"code","e87ab00d":"markdown","71dbae9c":"markdown","aeaa6abb":"markdown","f7593e26":"markdown","b3cca488":"markdown","5982c6de":"markdown","591cd548":"markdown","4ca0d270":"markdown","67850541":"markdown","29070a6d":"markdown","b2261ad9":"markdown","3b4441cc":"markdown","32c5360a":"markdown","508787ac":"markdown","2c0d0d40":"markdown","d47f844c":"markdown","582549f2":"markdown","79051d17":"markdown","8c70b53d":"markdown","c62a6693":"markdown","0a65fd50":"markdown","976c64f2":"markdown","ef00c45f":"markdown","80824ecc":"markdown","ebac7850":"markdown","2af337a3":"markdown"},"source":{"f866a42b":"# Libraries\nimport tensorflow as tf\nfrom tensorflow import keras\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom keras.models import Sequential\nfrom keras.layers import Dense","63025b9d":"# Import data from Kaggle\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","55cd8cac":"# Set training and testing data \ntrain = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\") \ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv') \nexample_test = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv') ","9c35a16a":"print(\"Train shape: {}\".format(train.shape))\nprint(\"Test shape: {}\".format(test.shape))","281a8e86":"# Saving 'PassengerId' and 'Survived' of train in y_train\ny_train = train.loc[:,['PassengerId', 'Survived']]\n\n# Saving 'PassengerId' of test in test_passengerId\ntest_passengerId = test.loc[:, 'PassengerId']\n\n# Before concatenate, I will drop 'Survived' column from train\ntrain.drop('Survived', axis = 1, inplace = True)\ntrain.shape","3e127e82":"# Convert test_passengerId into Dataframe\ntest_passengerId = pd.DataFrame(test_passengerId)\ntest_passengerId = test_passengerId.set_index(test_passengerId['PassengerId'])\ntest_passengerId.head()","a16aaee5":"# Concatenate train and test\ndf_titanic = pd.concat([train, test])\ndf_titanic.shape","5c365fe2":"# How many passengers are in data?\nprint(\"On the Titanic were {} passengers. At least, quantified in this dataset.\". format(df_titanic.shape[0]))","2ecc597e":"df_titanic.head()","cc9546c0":"# Are there duplicates ID's?\nprint(\"There are {} duplicates ID's\".format(len(df_titanic['PassengerId'])-len(df_titanic['PassengerId'].drop_duplicates())))","a885f343":"df_titanic.describe()","d1a936f9":"# How many nulls are?\ndf_titanic.isnull().sum()","c2822fd0":"# Age distribution\nplt.figure(figsize=(12,6))\nsns.distplot(df_titanic['Age'].dropna(),kde=False,bins=30)","27ee6f47":"# Age and classes\nplt.figure(figsize=(12,7))\nsns.boxplot(x='Pclass', y ='Age', data= df_titanic)","45148aaa":"# Set values to age in order to the differents classes\ndf_titanic['Age'] = df_titanic.apply(lambda x:25 if (pd.isna(x['Age']) and x['Pclass']==3) \n                                    else (29 if (pd.isna(x['Age']) and x['Pclass']==2) \n                                      else (39 if (pd.isna(x['Age']) and x['Pclass']==1) \n                                        else x['Age'])),axis=1)\n\n# Check if there are any nulls in \"Age\"\ndf_titanic['Age'].isnull().sum()","e63803aa":"df_titanic['Embarked'].value_counts()","dc224eda":"df_titanic[df_titanic['Embarked'].isnull()]","36158de1":"# Fill with mode\ndf_titanic['Embarked'] = df_titanic['Embarked'].fillna(df_titanic['Embarked'].mode()[0]) \n\n# Count values again\ndf_titanic['Embarked'].value_counts() # There are 2 more in \"S\"","ec4515bd":"df_titanic[df_titanic['Fare'].isnull()]","b2bd58b6":"# Let's see fare's distribution for each class\nplt.figure(figsize=(12,7))\nsns.boxplot(x='Pclass', y ='Fare', data= df_titanic)","270cd756":"# Mean of fare for Class = 3\nthird_class = df_titanic[df_titanic['Pclass'] == 3]\nmean_fare_third_class = third_class['Fare'].mean()\nmean_fare_third_class","202103b1":"# Filling 'Fare'\ndf_titanic['Fare'] = df_titanic['Fare'].fillna(third_class['Fare'].mean()) ","14a7afe5":"# How many nulls are in 'Cabin'?\nnulls_cabin = df_titanic['Cabin'].isnull().sum()\n\nprint(\"There are {} missings in 'Cabin' and there are {} passengers\".format(nulls_cabin, df_titanic.shape[0]))","ae146ba9":"# Dropping 'Cabin'\ndf_titanic.drop('Cabin', inplace = True ,axis = 1)","c0d1a41a":"# Transformation of \"Ticket\"\nticket_vc = df_titanic['Ticket'].value_counts()\nvc_ticket_1 = ticket_vc[ticket_vc==1] # How many with count values = 1\nvc_ticket = vc_ticket_1.reset_index()\n\n# Save in a list tickets with count value = 1\nindividual_tickets = list(vc_ticket['index'])","d6e3a72d":"# Create ticket_list: if is an individual ticket, then 1. Else = 0\nticket_list = []\n\nfor i in df_titanic['Ticket']:\n    if i in individual_tickets:\n        ticket_list.append(1)\n    else:\n        ticket_list.append(0)\n        \n# Replace in the original column \"Ticket\"\ndf_titanic['Ticket'] = ticket_list\n\n# Value_counts\ndf_titanic['Ticket'].value_counts()","73d5088e":"# Female = 1, Male = 0\ndf_titanic['Sex'] = df_titanic['Sex'].map({'female':1,'male':0}).astype(int)","37af2633":"# Apply a Label Encoder for Embarked values\ntransform1 = LabelEncoder()\ntransform1.fit_transform(['Q','C','S'])\n\nembarked1 = transform1.transform(df_titanic['Embarked'])\n\n# Replace in the original column\ndf_titanic['Embarked'] = embarked1","9f5eca30":"# Get title for 'Name' (Mr, Miss, Mrs, Dr, etc)\ndf_titanic['Title'] = df_titanic['Name'].str.extract('([A-Za-z]+)\\.', expand= False)\ndf_titanic['Title'].value_counts()","7fdf2e0e":"# Uniform\ndf_titanic['Title'] = df_titanic['Title'].replace('Mme', 'Mrs')\ndf_titanic['Title'] = df_titanic['Title'].replace('Ms', 'Miss')\ndf_titanic['Title'] = df_titanic['Title'].replace('Don', 'Mr')\ndf_titanic['Title'] = df_titanic['Title'].replace('Dona', 'Mrs')\ndf_titanic['Title'] = df_titanic['Title'].replace('Mlle', 'Miss')\ndf_titanic['Title'] = df_titanic['Title'].replace('Lady', 'Miss')\ndf_titanic['Title'] = df_titanic['Title'].replace(['Dr','Rev','Major','Sir','Capt','Jonkheer','Col'],'Mr')\ndf_titanic['Title'] = df_titanic['Title'].replace('Countess', 'Mrs')","1802419b":"# Replacing with numbers\ndf_titanic['Title'] = df_titanic['Title'].map({'Mr':0,'Mrs':1,'Miss':2,'Master':3,})\n\n# Drop 'Name' column\ndf_titanic.drop('Name',axis=1,inplace=True)","019c5e0d":"df_titanic.dtypes","965deac9":"# Checking if there are not missing values\ndf_titanic.isnull().sum()","830d293a":"df_titanic.set_index(['PassengerId'], inplace=True)","fc9690c5":"print(\"Shape of train: {}\".format(train.shape))\nprint(\"Shape of test: {}\".format(test.shape))","51fc1c27":"# Set PassengerId as index in y_train\ny_train.set_index(['PassengerId'], inplace=True)","79940405":"# Merge on index with y_train to set original train (inner join)\ntrain = pd.merge(df_titanic, y_train, left_index=True, right_index=True, how = \"inner\")\ntrain.head()","c5f44a6d":"train.shape","22d37c7f":"df_titanic.shape","32a04771":"# Merge on index with y_train to set original test (outer join)\ntest = pd.merge(df_titanic, test_passengerId, left_index=True, right_index=True, how = \"inner\")\n\n# Set PassengerId as index\ntest.set_index(['PassengerId'], inplace=True)\n\ntest.head()","4fbe579d":"test.shape","b03bfe47":"train.head()","d6669fe4":"# Reset index\ntrain = train.reset_index()\ntest = test.reset_index()\n\nfeatures = train[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Embarked', 'Title']]\ntarget = train['Survived'].values\n\n# Drop column survived\ntrain = train.drop(columns = 'Survived')","078541c6":"# Splitting\nX_train, X_test, y_train, y_test = train_test_split(features, target, \n                                                    test_size = 0.25, random_state = 42)\n\nprint(\"X_train shape: {}\".format(X_train.shape))\nprint(\"X_test shape: {}\".format(X_test.shape))\nprint(\"y_train shape: {}\".format(y_train.shape))\nprint(\"y_test shape: {}\".format(y_test.shape))","c82f35cd":"sc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(test)","4f51f35d":"from tensorflow.keras import regularizers\nregularizador_l2 = regularizers.l2(9e-5)\n\nmodel = Sequential()\nmodel.add(Dense(64, input_shape=(9, ), kernel_initializer=\"uniform\", activation='relu'))\nmodel.add(keras.layers.Dropout(0.2))\n\nmodel.add(keras.layers.Dense(100, kernel_initializer=\"uniform\", kernel_regularizer = regularizador_l2, activation='relu'))\nmodel.add(keras.layers.Dropout(0.2))\n\nmodel.add(keras.layers.Dense(84, kernel_initializer=\"uniform\", activation='relu'))\nmodel.add(keras.layers.BatchNormalization())\n\nmodel.add(keras.layers.Dense(84, kernel_initializer=\"uniform\", activation='relu'))\n\nmodel.add(keras.layers.Dense(84, kernel_initializer=\"uniform\", activation='relu'))\n\nmodel.add(keras.layers.Dense(1, kernel_initializer=\"uniform\", activation='sigmoid'))\n\nmodel.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\nmodel.summary()\n\n# Stopping callback\ncallback = keras.callbacks.EarlyStopping(\n    monitor='val_loss',  \n    patience = 20,  \n    verbose = 1\n    )","a47a4cc3":"model.fit(features,\n          target,\n          epochs=200,\n          batch_size=20,\n          validation_split=0.33,\n          verbose=1,\n          callbacks = [callback]) ","8d6d1f92":"results = model.evaluate(features, target, verbose=1)\nprint('Test Loss: {}'.format(results))","4d40db7d":"# Preparing the test set for prediction\n\npassengerid_test = test['PassengerId']\n\ntest.drop('PassengerId', axis = 1, inplace = True)\ntest.head()","a593556e":"pd.options.display.float_format = '{:20,.2f}'.format # Supressing scientific notation in pandas\n\n\n# Getting predictions of test data\nprediction = model.predict(test).tolist()\n\n# List to series\nseries = pd.Series(prediction)\n\n# Creating new column of predictions in test dataframe\ntest['probability'] = series\ntest['probability'] = test['probability'].str.get(0)","8384a681":"# If probability >= 0.5 then survived (1), else, not survived (0)\n\nseries = []\n\nfor val in test.probability:\n    if val >= 0.5:\n        series.append(1)\n    else:\n        series.append(0)\n        \ntest['Survived'] = series","df868f5c":"# Concatenate Passenger Id to the table\ntest['PassengerId'] = passengerid_test\ntest.head()","5c1c9809":"test.shape","98a8db5b":"# Submission\nsubmission = test.loc[:, ['PassengerId', 'Survived']]\n\nsubmission.to_csv('my_submission.csv', index=False)","e87ab00d":"#### **3.4 To-Do with 'Name'**","71dbae9c":"#### **3.5 Checking if column's types are correct**","aeaa6abb":"#### **3.1 To-Do with 'Ticket'**","f7593e26":"So I transformed the variables in test and train together. Now it's time to make predictions.","b3cca488":"#### **2.2.2 Filling \"Embarked\"**","5982c6de":"Before the exploring and transforming data, I will concatenate train and test and after transformations, I will separate again train and test to make the submission.","591cd548":"#### **3.2 To-Do with 'Sex'**","4ca0d270":"# 4. Separate train and test (to have the originals)","67850541":"# 6. Final thoughts ","29070a6d":"# **Titanic with Deep Learning**","b2261ad9":"# 5. Making predictions with Deep Learning - TensorFlow","3b4441cc":"# 3. More feature engineering","32c5360a":"#### **5.1 Separate train and test to build the model**","508787ac":"#### **2.2.1 Filling \"Age\"**\n\n","2c0d0d40":"#### **5.2 Building the model**","d47f844c":"Results are not really great (around 75%-77% of score). Best option to improve this score is to use Machine Learning (logistic regression, SVM, Decision Tree, etc) instead of Deep Learning because this dataset has few data to work with Neural Networks in my opinion.\n\nUr suggestions are welcome!","582549f2":"### 2.1 Exploring data","79051d17":"#### **3.3 To-Do with 'Embarked'**","8c70b53d":"#### **2.2.3 Filling 'Fare'**","c62a6693":"# 2. Exploring and transforming data (missing values)","0a65fd50":"#### **2.2.4 To-do with 'cabin'**","976c64f2":"#### **5.2 Normalize data**","ef00c45f":"#### **3.6 Select 'PassengerId' as index**","80824ecc":"# 1. Importing libraries and dataset","ebac7850":"### 2.2 Transforming data (missing values)","2af337a3":"There are too many missings, so I will delete 'Cabin'."}}