{"cell_type":{"31322bfb":"code","fb519c4f":"code","0c8f50ee":"code","c689275a":"code","ac4afa85":"code","b6b9cec7":"code","27f9f790":"code","fdc22f26":"code","29126bce":"code","c885bd20":"code","72fe1afa":"code","71f312b3":"code","d9d97a61":"code","a7bc8ec5":"code","df750cfe":"code","f52119ed":"code","fbb9274d":"code","db0b3337":"code","4325ab4d":"code","44597c52":"code","9b993685":"code","97b12819":"code","4f1ac623":"code","7ff174f7":"code","b4784e7d":"code","1e81945b":"code","10f01437":"code","e9425418":"code","3774aa64":"code","f6433f3f":"code","605d79aa":"code","90b9c306":"code","bb605719":"code","f5a87fe8":"code","d9657c91":"code","5fb38f67":"code","3aa590d0":"code","ed2b0bf7":"code","91deae4b":"code","e11f719e":"code","b2ad2416":"code","bde96cc0":"code","37e6cb2a":"code","b328c182":"code","c09282e9":"code","380770ac":"code","a76fa6cc":"code","800f14de":"code","4b184613":"code","4efb0120":"code","ea105d0d":"code","28f4ca09":"code","3e045ad1":"code","93d09c06":"code","24727e23":"code","4f153987":"code","0346f915":"code","596f4330":"code","f5238b1b":"code","0650687a":"code","fbb7afa3":"code","58850fd8":"code","4265ab09":"code","0500c22f":"code","61b549df":"code","57494038":"code","b83d3db5":"code","4f3bb841":"code","f7efb5cd":"code","64d82e8b":"code","7269e76a":"code","49b703cf":"code","49726585":"code","f2f37b26":"code","d504b52e":"code","d1087f41":"code","e84126e5":"code","29c29f60":"code","acf4662f":"code","ba36a526":"code","0aec8fe1":"code","81f9855d":"markdown","15bf52a9":"markdown","d6722431":"markdown","44e24073":"markdown","571dfa52":"markdown","ac26eff7":"markdown","5bdf4d68":"markdown","b9aae994":"markdown","d770735f":"markdown","a307a5b7":"markdown","497c57ab":"markdown","9e16f937":"markdown","3c76424c":"markdown","b99745b2":"markdown","bd7ef08e":"markdown","c7d81d7e":"markdown","feefbe78":"markdown","593e19ba":"markdown","e11239a4":"markdown","9be3f7a2":"markdown","073944f7":"markdown","de0a7917":"markdown","6e9b2873":"markdown","ce1e17b7":"markdown","09d815f8":"markdown","4487da2b":"markdown","a4a767b5":"markdown","c6a826aa":"markdown","f64c9c18":"markdown"},"source":{"31322bfb":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n%matplotlib inline  \nimport seaborn as sns\nfrom pandas.plotting import scatter_matrix\nfrom sklearn import linear_model\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom sklearn import model_selection #import cross_val_score, StratifiedKFold\nfrom sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, export_graphviz\nfrom sklearn import metrics  # mean_squared_error, mean_absolute_error, median_absolute_error, explained_variance_score, r2_score\nfrom sklearn.feature_selection import SelectFromModel, RFECV\nfrom sklearn.metrics import max_error\nfrom sklearn.decomposition import PCA\nfrom sklearn import preprocessing","fb519c4f":"#dataset column names:\n\ncol_names = ['id','cycle','setting1','setting2','setting3','s1','s2','s3','s4','s5','s6','s7','s8','s9','s10','s11','s12','s13','s14','s15','s16','s17','s18','s19','s20','s21','s22','s23']\n","0c8f50ee":"#load training data\n\ndf_train_raw = pd.read_csv('..\/input\/PM_train.txt', sep = ' ', header=None)\ndf_train_raw.head()","c689275a":"\n#assign column names\n\ndf_train_raw.columns = col_names\ndf_train_raw.head()\n","ac4afa85":"df_train_raw.id.unique()","b6b9cec7":"#drop extra space columnn\n\ndf_train_raw=df_train_raw.drop(columns=['s22','s23'])\ndf_train_raw.head()","27f9f790":"# get some stat\n\ndf_train_raw.describe()","fdc22f26":"# check the data types\n\ndf_train_raw.dtypes","29126bce":"df_train_raw.isnull().sum()","c885bd20":"#load test data\n\ndf_test_raw = pd.read_csv('..\/input\/PM_test.txt', sep = ' ', header=None)\ndf_test_raw.head()","72fe1afa":"# #drop extra space columnn\n# df_test_raw.drop([26,27], axis=1, inplace='True')\n\n#assign column names\ndf_test_raw.columns = col_names\ndf_test_raw.head()\n","71f312b3":"#drop extra space columnn\n\ndf_test_raw=df_test_raw.drop(columns=['s22','s23'])","d9d97a61":"# get some stat on test data\n\ndf_test_raw.describe()","a7bc8ec5":"df_test_raw.isnull().sum()","df750cfe":"# Load the truth data - actual 'ttf' for test data\n\ndf_truth = pd.read_csv('..\/input\/PM_truth.txt', sep = ' ', header=None)\ndf_truth.head()","f52119ed":"#drop extra empty column in the truth data and rename remaining 'ttf'\n\n# df_truth.drop([1], axis=1, inplace='True')\ndf_truth.columns = ['ttf','1']\ndf_truth=df_truth.drop(columns=['1'])\ndf_truth.head()","fbb9274d":"#get some stat on truth data\n\ndf_truth.describe()","db0b3337":"\ndef add_features(df_in, rolling_win_size):\n    \n    \"\"\"Add rolling average and rolling standard deviation for sensors readings using fixed rolling window size.\n    \n    Args:\n            df_in (dataframe)     : The input dataframe to be proccessed (training or test) \n            rolling_win_size (int): The window size, number of cycles for applying the rolling function\n        \n    Reurns:\n            dataframe: contains the input dataframe with additional rolling mean and std for each sensor\n    \n    \"\"\"\n    \n    sensor_cols = ['s1','s2','s3','s4','s5','s6','s7','s8','s9','s10','s11','s12','s13','s14','s15','s16','s17','s18','s19','s20','s21']\n    \n    sensor_av_cols = [nm.replace('s', 'av') for nm in sensor_cols]\n    sensor_sd_cols = [nm.replace('s', 'sd') for nm in sensor_cols]\n    \n    df_out = pd.DataFrame()\n    \n    ws = rolling_win_size\n    \n    #calculate rolling stats for each engine id\n    \n    for m_id in pd.unique(df_in.id):\n    \n        # get a subset for each engine sensors\n        df_engine = df_in[df_in['id'] == m_id]\n        df_sub = df_engine[sensor_cols]\n\n    \n        # get rolling mean for the subset\n        av = df_sub.rolling(ws, min_periods=1).mean()\n        av.columns = sensor_av_cols\n    \n        # get the rolling standard deviation for the subset\n        sd = df_sub.rolling(ws, min_periods=1).std().fillna(0)\n        sd.columns = sensor_sd_cols\n    \n        # combine the two new subset dataframes columns to the engine subset\n        new_ftrs = pd.concat([df_engine,av,sd], axis=1)\n    \n        # add the new features rows to the output dataframe\n        df_out = pd.concat([df_out,new_ftrs])\n        \n    return df_out","4325ab4d":"\ndef prepare_train_data (df_in, period):\n    \n    \"\"\"Add regression and classification labels to the training data.\n\n        Regression label: ttf (time-to-failure) = each cycle# for an engine subtracted from the last cycle# of the same engine\n        Binary classification label: label_bnc = if ttf is <= parameter period then 1 else 0 (values = 0,1)\n        Multi-class classification label: label_mcc = 2 if ttf <= 0.5* parameter period , 1 if ttf<= parameter period, else 2\n        \n      Args:\n          df_in (dataframe): The input training data\n          period (int)     : The number of cycles for TTF segmentation. Used to derive classification labels\n          \n      Returns:\n          dataframe: The input dataframe with regression and classification labels added\n          \n    \"\"\"\n    \n    #create regression label\n    \n    #make a dataframe to hold the last cycle for each enginge in the dataset\n    df_max_cycle = pd.DataFrame(df_in.groupby('id')['cycle'].max())\n    df_max_cycle.reset_index(level=0 , inplace=True)\n    df_max_cycle.columns = ['id', 'last_cycle']\n\n    #add time-to-failure ttf as a new column - regression label\n    df_in = pd.merge(df_in, df_max_cycle, on='id')\n    df_in['ttf'] = df_in['last_cycle'] - df_in['cycle']\n    df_in.drop(['last_cycle'], axis=1 , inplace=True)\n    \n#     #create binary classification label\n#     df_in['label_bnc'] = df_in['ttf'].apply(lambda x: 1 if x <= period else 0)\n    \n#     #create multi-class classification label\n#     df_in['label_mcc'] = df_in['ttf'].apply(lambda x: 2 if x <= period\/2 else 1 if x <= period else 0)\n    \n    return df_in\n    ","44597c52":"df_max_cycle = pd.DataFrame(df_train_raw.groupby('id')['cycle'].max())\ndf_max_cycle.reset_index(level=0 , inplace=True)\ndf_max_cycle.columns = ['id', 'last_cycle']\ndf_max_cycle.describe()","9b993685":"df_max_cycle","97b12819":"df_max_cycle.describe()","4f1ac623":"# https:\/\/github.com\/Samimust\/predictive-maintenance\ndef prepare_test_data(df_test_in, df_truth_in, period):\n    \n    \"\"\"Add regression and classification labels to the test data.\n\n        Regression label: ttf (time-to-failure) = extract the last cycle for each enginge and then merge the record with the truth data\n        Binary classification label: label_bnc = if ttf is <= parameter period then 1 else 0 (values = 0,1)\n        Multi-class classification label: label_mcc = 2 if ttf <= 0.5* parameter period , 1 if ttf<= parameter period, else 2\n        \n      Args:\n          df_in (dataframe): The input training data\n          period (int)     : The number of cycles for TTF segmentation. Used to derive classification labels\n          \n      Returns:\n          dataframe: The input dataframe with regression and classification labels added\n    \n\n    \n    \"\"\"\n    \n    df_tst_last_cycle = pd.DataFrame(df_test_in.groupby('id')['cycle'].max())\n    \n    df_tst_last_cycle.reset_index(level=0, inplace=True)\n    df_tst_last_cycle.columns = ['id', 'last_cycle']\n#     , inplace=True\n    df_test_in = pd.merge(df_test_in, df_tst_last_cycle, on='id')\n\n\n    df_test_in = df_test_in[df_test_in['cycle'] == df_test_in['last_cycle']]\n\n    df_test_in.drop(['last_cycle'], axis=1, inplace=True)\n    \n    df_test_in.reset_index(drop=True, inplace=True)\n    \n    df_test_in = pd.concat([df_test_in, df_truth], axis=1)\n    \n#     #create binary classification label\n#     df_test_in['label_bnc'] = df_test_in['ttf'].apply(lambda x: 1 if x <= period else 0)\n    \n#     #create multi-class classification label\n#     df_test_in['label_mcc'] = df_test_in['ttf'].apply(lambda x: 2 if x <= period\/2 else 1 if x <= period else 0)\n\n    return df_test_in","7ff174f7":"# add extracted features to training data\n\ndf_train_fx = add_features(df_train_raw, 5)\ndf_train_fx.head()","b4784e7d":"#add labels to training data using period of 30 cycles for classification\n\ndf_train = prepare_train_data (df_train_fx, 30)\ndf_train.head()","1e81945b":"df_train.describe()","10f01437":"x=df_train[df_train.ttf==30]\ny=x[x.id<10]\ny","e9425418":"df_train.dtypes","3774aa64":"# save the training data to csv file for later use\n\ndf_train.to_csv('train.csv', index=False)","f6433f3f":"# add extracted features to test data\n\ndf_test_fx = add_features(df_test_raw, 5)\ndf_test_fx.head()","605d79aa":"df_test_fx['id'].value_counts()","90b9c306":"# df_test_fx=df_test_fx[df_test_fx.id==1]\n# df_test_fx=df_test_fx[df_test_fx.cycle==31]\n# df_test_fx","bb605719":"#add labels to test data using period of 30 cycles for classification\ndf_test = prepare_test_data(df_test_fx, df_truth, 30)\ndf_test.head()","f5a87fe8":"df_test.dtypes","d9657c91":"# save the test data to csv file for later use\n\ndf_test.to_csv('test.csv', index=False)","5fb38f67":"df_tr_lbl = pd.read_csv('train.csv')\ndf_tr_lbl.head(10)\n","3aa590d0":"df_tr_lbl.info()","ed2b0bf7":"#exclude enging id and cycle number from the input features:\n\nfeaturs = ['setting1','setting2','setting3','s1','s2','s3','s4','s5','s6','s7','s8','s9','s10','s11','s12','s13','s14','s15','s16','s17','s18','s19','s20','s21']","91deae4b":"#plot and compare the standard deviation of input features:\n\ndf_tr_lbl[featurs].std().plot(kind='bar', figsize=(10,6), title=\"Features Standard Deviation\")","e11f719e":"df_tr_lbl[featurs].std().sort_values(ascending=False)","b2ad2416":"# get ordered list features correlation with regression label ttf\ndf_tr_lbl[featurs].corrwith(df_tr_lbl.ttf).sort_values(ascending=False)","bde96cc0":"df_tr_lbl[df_tr_lbl.s5==14.62][\"s5\"].value_counts()","37e6cb2a":"df_tr_lbl[\"s5\"].std()","b328c182":"# list of features having low or no correlation with regression label ttf and very low or no variance\n# These features will be target for removal in feature selection\nlow_cor_featrs = ['setting3', 's1', 's10', 's18','s19','s16','s5', 'setting2', 'setting1']\ndf_tr_lbl[low_cor_featrs].describe()","c09282e9":"# list of features having high correlation with regression label ttf\n\ncorrel_featurs = ['s12', 's7', 's21', 's20', 's6', 's14', 's9', 's13', 's8', 's3', 's17', 's2', 's15', 's4', 's11']\n\ndf_tr_lbl[correl_featurs].describe()","380770ac":"# add the regression label 'ttf' to the list of high corr features \n\ncorrel_featurs_lbl = correl_featurs + ['ttf']\ncorrel_featurs_lbl","a76fa6cc":"df_tr_lbl[correl_featurs_lbl].corr()","800f14de":"# # most correlated features\n# corrmat = df_tr_lbl.corr()\n# top_corr_features = corrmat.index[abs(corrmat[\"ttf\"])>0.6]\n# plt.figure(figsize=(20,10))\n# g = sns.heatmap(df_tr_lbl[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","4b184613":"# plot a heatmap to display +ve and -ve correlation among features and regression label:\n\nimport seaborn as sns\ncm = np.corrcoef(df_tr_lbl[correl_featurs_lbl].values.T)\nsns.set(font_scale=1.0)\nfig = plt.figure(figsize=(10, 8))\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 8}, yticklabels=correl_featurs_lbl, xticklabels=correl_featurs_lbl)\nplt.title('Features Correlation Heatmap')\nplt.show()","4efb0120":"def explore_col(s):\n    \n    \"\"\"Plot 4 main graphs for a single feature.\n    \n        plot1: histogram \n        plot2: boxplot \n        plot3: line plot (time series over cycle)\n        plot4: scatter plot vs. regression label ttf\n        \n    Args:\n        s (str): The column name of the feature to be plotted.\n        e (int): The number of random engines to be plotted for plot 3. Range from 1 -100, 0:all engines, >100: all engines.\n\n    Returns:\n        plots\n    \n    \"\"\"\n    \n    fig = plt.figure(figsize=(10, 8))\n\n\n    sub1 = fig.add_subplot(221) \n    sub1.set_title(s +' histogram') \n    sub1.hist(df_tr_lbl[s])\n\n    sub2 = fig.add_subplot(222)\n    sub2.set_title(s +' boxplot')\n    sub2.boxplot(df_tr_lbl[s])\n    \n    sub3 = fig.add_subplot(224)\n    sub3.set_title(\"scatter: \"+ s + \" \/ttf (regr label)\")\n    sub3.set_xlabel('ttf')\n    sub3.scatter(df_tr_lbl['ttf'],df_tr_lbl[s])\n    plt.tight_layout()\n    plt.show()","ea105d0d":"explore_col(\"s12\")","28f4ca09":"explore_col(\"av11\")","3e045ad1":"df_test = pd.read_csv('test.csv')\ndf_test","93d09c06":"dt=df_test[\"cycle\"]+df_test[\"ttf\"]\ndt.describe()","24727e23":"df_test.describe()","4f153987":"#Prepare data for regression model\n\n# original features\nfeatures_orig = ['setting1','setting2','setting3','s1','s2','s3','s4','s5','s6','s7','s8','s9','s10','s11','s12','s13','s14','s15','s16','s17','s18','s19','s20','s21']\n\n# original + extracted fetures\nfeatures_adxf = ['setting1', 'setting2', 'setting3', 's1', 's2', 's3', 's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14', 's15', 's16', 's17', 's18', 's19', 's20', 's21', 'av1', 'av2', 'av3', 'av4', 'av5', 'av6', 'av7', 'av8', 'av9', 'av10', 'av11', 'av12', 'av13', 'av14', 'av15', 'av16', 'av17', 'av18', 'av19', 'av20', 'av21', 'sd1', 'sd2', 'sd3', 'sd4', 'sd5', 'sd6', 'sd7', 'sd8', 'sd9', 'sd10', 'sd11', 'sd12', 'sd13', 'sd14', 'sd15', 'sd16', 'sd17', 'sd18', 'sd19', 'sd20', 'sd21']\n\n# features with low or no correlation with regression label\nfeatures_lowcr = ['setting3', 's1', 's10', 's18','s19','s16','s5', 'setting1', 'setting2']\n\n# features that have correlation with regression label\nfeatures_corrl = ['s2', 's3', 's4', 's6', 's7', 's8', 's9', 's11', 's12', 's13', 's14', 's15', 's17', 's20','s21']\n# features_impor = ['s4', 's7', 's9', 's11', 's12']\n# a variable to hold the set of features to experiment with\n\nfeatures = features_corrl\nX_train = df_train[features]\ny_train = df_train['ttf']\n\nX_test = df_test[features]\ny_test = df_test['ttf']\n# from sklearn.preprocessing import MinMaxScaler\n# min_max_scaler = preprocessing.MinMaxScaler()\n# X_train = pd.DataFrame(min_max_scaler.fit_transform(X_train),columns=features_corrl)\n# X_test = pd.DataFrame(min_max_scaler.fit_transform(X_test),columns=features_corrl )","0346f915":"def get_regression_metrics(model, actual, predicted):\n    \n    \"\"\"Calculate main regression metrics.\n    \n    Args:\n        model (str): The model name identifier\n        actual (series): Contains the test label values\n        predicted (series): Contains the predicted values\n        \n    Returns:\n        dataframe: The combined metrics in single dataframe\n    \n    \n    \"\"\"\n    x= np.mean(np.abs((actual - predicted) \/ actual)) * 100\n    regr_metrics = {\n                        'Root Mean Squared Error' : metrics.mean_squared_error(actual, predicted)**0.5,\n                        'Mean Absolute Error' : metrics.mean_absolute_error(actual, predicted),\n                        'R^2' : metrics.r2_score(actual, predicted),\n                        'Explained Variance' : metrics.explained_variance_score(actual, predicted),\n                        'Max Error' : metrics.max_error(actual, predicted),\n                        'Mean absolute percentage error': x\n               \n                   }\n\n    #return reg_metrics\n    df_regr_metrics = pd.DataFrame.from_dict(regr_metrics, orient='index')\n    df_regr_metrics.columns = [model]\n    return df_regr_metrics","596f4330":"def plot_features_weights(model, weights, feature_names, weights_type='c'):\n    \n    \"\"\"Plot regression coefficients weights or feature importance.\n    \n    Args:\n        model (str): The model name identifier\n        weights (array): Contains the regression coefficients weights or feature importance\n        feature_names (list): Contains the corresponding features names\n        weights_type (str): 'c' for 'coefficients weights', otherwise is 'feature importance'\n        \n    Returns:\n        plot of either regression coefficients weights or feature importance\n        \n    \n    \"\"\"\n    (px, py) = (8, 10) if len(weights) > 30 else (8, 5)\n    W = pd.DataFrame({'Weights':weights}, feature_names)\n    W.sort_values(by='Weights', ascending=True).plot(kind='barh', color='r', figsize=(px,py))\n    label = ' Coefficients' if weights_type =='c' else ' Features Importance'\n    plt.xlabel(model + label)\n    plt.gca().legend_ = None","f5238b1b":"def plot_residual(model, y_train, y_train_pred, y_test, y_test_pred):\n    \n    \"\"\"Print the regression residuals.\n    \n    Args:\n        model (str): The model name identifier\n        y_train (series): The training labels\n        y_train_pred (series): Predictions on training data\n        y_test (series): The test labels\n        y_test_pred (series): Predictions on test data\n        \n    Returns:\n        Plot of regression residuals\n    \n    \"\"\"\n    \n    plt.scatter(y_train_pred, y_train_pred - y_train, c='blue', marker='o', label='Training data')\n    plt.scatter(y_test_pred, y_test_pred - y_test, c='lightgreen', marker='s', label='Test data')\n    plt.xlabel('Predicted Values')\n    plt.ylabel('Residuals')\n    plt.legend(loc='upper left')\n    plt.hlines(y=0, xmin=-50, xmax=400, color='red', lw=2)\n    plt.title(model + ' Residuals')\n    plt.show()\n    ","0650687a":"#try linear regression\n\nlinreg = linear_model.LinearRegression()\nlinreg.fit(X_train, y_train)\n\ny_test_predict = linreg.predict(X_test)\ny_train_predict = linreg.predict(X_train)\n\nprint('R^2 training: %.3f, R^2 test: %.3f' % (\n      (metrics.r2_score(y_train, y_train_predict)), \n      (metrics.r2_score(y_test, y_test_predict))))\n\nlinreg_metrics = get_regression_metrics('Linear Regression', y_test, y_test_predict)\nlinreg_metrics\n","fbb7afa3":"plot_features_weights('Linear Regression', linreg.coef_, X_train.columns, 'c')","58850fd8":"plot_residual('Linear Regression', y_train_predict, y_train, y_test_predict, y_test)","4265ab09":"#try LASSO\n\nlasso = linear_model.Lasso(alpha=0.001)\nlasso.fit(X_train, y_train)\n\ny_test_predict = lasso.predict(X_test)\ny_train_predict = lasso.predict(X_train)\n\nprint('R^2 training: %.3f, R^2 test: %.3f' % (\n      (metrics.r2_score(y_train, y_train_predict)), \n      (metrics.r2_score(y_test, y_test_predict))))\n\nlasso_metrics = get_regression_metrics('LASSO', y_test, y_test_predict)\n\nlasso_metrics","0500c22f":"plot_features_weights('LASSO', lasso.coef_, X_train.columns, 'c')","61b549df":"plot_residual('LASSO', y_train_predict, y_train, y_test_predict, y_test)","57494038":"#try ridge\n\nrdg = linear_model.Ridge(alpha = 0.01)\nrdg.fit(X_train, y_train)\n\ny_test_predict = rdg.predict(X_test)\ny_train_predict = rdg.predict(X_train)\n\nprint('R^2 training: %.3f, R^2 test: %.3f' % (\n      (metrics.r2_score(y_train, y_train_predict)), \n      (metrics.r2_score(y_test, y_test_predict))))\n\nrdg_metrics = get_regression_metrics('Ridge Regression', y_test, y_test_predict)\nrdg_metrics","b83d3db5":"plot_features_weights('Ridge Regression', rdg.coef_, X_train.columns, 'c')","4f3bb841":"plot_residual('Ridge Regression', y_train_predict, y_train, y_test_predict, y_test)","f7efb5cd":"#try Polynomial Regression\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\npoly = PolynomialFeatures(degree=2)\n\nX_train_poly = poly.fit_transform(X_train)\nX_test_poly = poly.fit_transform(X_test)\n\n\npolyreg = linear_model.LinearRegression()\npolyreg.fit(X_train_poly, y_train)\n\ny_test_predict = polyreg.predict(X_test_poly)\ny_train_predict = polyreg.predict(X_train_poly)\n\nprint('R^2 training: %.3f, R^2 test: %.3f' % (\n      (metrics.r2_score(y_train, y_train_predict)), \n      (metrics.r2_score(y_test, y_test_predict))))\n\npolyreg_metrics = get_regression_metrics('Polynomial Regression', y_test, y_test_predict)\npolyreg_metrics","64d82e8b":"plot_residual('Polynomial Regression', y_train_predict, y_train, y_test_predict, y_test)","7269e76a":"#try Decision Tree regressor\n\n#dtrg = DecisionTreeRegressor(max_depth=8, max_features=5, random_state=123) # selected features\ndtrg = DecisionTreeRegressor(max_depth=7, random_state=123)\ndtrg.fit(X_train, y_train)\n\ny_test_predict = dtrg.predict(X_test)\ny_train_predict = dtrg.predict(X_train)\n\nprint('R^2 training: %.3f, R^2 test: %.3f' % (\n      (metrics.r2_score(y_train, y_train_predict)), \n      (metrics.r2_score(y_test, y_test_predict))))\n\ndtrg_metrics = get_regression_metrics('Decision Tree Regression', y_test, y_test_predict)","49b703cf":"plot_features_weights('Decision Tree Regressor', dtrg.feature_importances_, X_train.columns, 't' )","49726585":"plot_residual('Decision Tree Regression', y_train_predict, y_train, y_test_predict, y_test)","f2f37b26":"# from sklearn.ensemble import RandomForestRegressor\n# from sklearn.model_selection import GridSearchCV\n# param_grid = {\n#     'bootstrap': [True],\n#     'max_depth': [4, 5, 6, 10],\n#     'max_features': [2, 3],\n#     'min_samples_leaf': [3, 4, 5],\n#     'min_samples_split': [8, 10, 12],\n#     'n_estimators': [100, 200, 300, 1000]\n# }\n# # Create a based model\n# rf = RandomForestRegressor()\n# # Instantiate the grid search model\n# grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n#                           cv = 3, n_jobs = -1, verbose = 2)\n# grid_search.fit(X_train, y_train)\n# grid_search.best_params_","d504b52e":"#try Random Forest\n\n#rf = RandomForestRegressor(n_estimators=100, max_features=2, max_depth=4, n_jobs=-1, random_state=1) # selected features\nrf1 = RandomForestRegressor(n_estimators=100, max_features=3, max_depth=4, n_jobs=-1, random_state=1) # original features\n#rf = RandomForestRegressor(n_estimators=100, max_features=3, max_depth=7, n_jobs=-1, random_state=1) # orig + extrcted \n\nrf1.fit(X_train, y_train)\n\ny_test_predict = rf1.predict(X_test)\ny_train_predict = rf1.predict(X_train)\n\nprint('R^2 training: %.3f, R^2 test: %.3f' % (\n      (metrics.r2_score(y_train, y_train_predict)), \n      (metrics.r2_score(y_test, y_test_predict))))\n\nrf1_metrics = get_regression_metrics('Random Forest Regression', y_test, y_test_predict)\nrf1_metrics","d1087f41":"rf2 = RandomForestRegressor(n_estimators=1000, max_features=3, max_depth=10, n_jobs=-1, random_state=1) # original features\n#rf = RandomForestRegressor(n_estimators=100, max_features=3, max_depth=7, n_jobs=-1, random_state=1) # orig + extrcted \n\nrf2.fit(X_train, y_train)\n\ny_test_predict = rf2.predict(X_test)\ny_train_predict = rf2.predict(X_train)\n\nprint('R^2 training: %.3f, R^2 test: %.3f' % (\n      (metrics.r2_score(y_train, y_train_predict)), \n      (metrics.r2_score(y_test, y_test_predict))))\n\nrf2_metrics = get_regression_metrics('Random Forest Regression', y_test, y_test_predict)\nrf2_metrics","e84126e5":"plot_residual('Random Forest Regression', y_train_predict, y_train, y_test_predict, y_test)","29c29f60":"# try recursive feature elimination\n\nkfold = model_selection.KFold(n_splits=5, random_state=10)\n\ndtrg = DecisionTreeRegressor(max_depth=7)\n\nrfecv = RFECV(estimator=dtrg, step=1, cv=kfold, scoring='neg_mean_squared_error', n_jobs=-1)\nrfecv.fit(X_train, y_train)\n\nprint(\"Optimal number of features : %d\" % rfecv.n_features_)\n\nsel_features = [f for f,s in zip(X_train.columns, rfecv.support_) if s]\nprint('The selected features are: {}'.format(sel_features))\n\n# Plot number of features VS. cross-validation scores\nplt.figure()\nplt.xlabel(\"Number of features selected (RFE)\")\nplt.ylabel(\"Cross validation score (mse)\")\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.show()","acf4662f":"# view predictions vs actual\n\nrf_pred_dict = {\n                'Actual' : y_test,\n                'Prediction' : y_test_predict\n            }\n    \nrf_pred = pd.DataFrame.from_dict(rf_pred_dict)\nabs(rf_pred.Actual-rf_pred.Prediction).hist(bins=10)","ba36a526":"#regression metrics comparison before feature engineering\n\nreg_metrics_bfe = pd.concat([linreg_metrics, lasso_metrics, rdg_metrics, dtrg_metrics, polyreg_metrics, rf1_metrics,rf2_metrics], axis=1)\nreg_metrics_bfe","0aec8fe1":"# from sklearn.model_selection import GridSearchCV\n# from sklearn.linear_model import Lasso\n\n# alpha = [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]\n\n# lasso = Lasso()\n\n# parameters = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]}\n\n# ridge_regressor = GridSearchCV(lasso, parameters,scoring='neg_mean_squared_error', cv=5)\n\n# ridge_regressor.fit(X_train, y_train)","81f9855d":"Create helper function to create features based on smoothing the time series for sensors by adding rolling mean and rolling standard deviation","15bf52a9":"There is a very high correlation (> 0.8) between some features: (s14, s9), (s11, s4), (s11, s7), (s11, s12), (s4, s12), (s8,s13), (s7, s12)\nThis may hurt the performance of some ML algorithms. \nSo, some of the above features will be target for removal in feature selection","d6722431":"### Prepare the Training Data:","44e24073":"### Load Truth Data:","571dfa52":"All data columns are numeric.","ac26eff7":"Using the above functions let us model and evaluate some regression algorithms","5bdf4d68":"There are 100 engines. each engine has between 1 to 362 cycles (average of 108 cycles per engine). The last cycle for each engine represents the cycle when failure had happened.","b9aae994":"To get meaningful test data, we need to merge the truth data (TTF) with last cycle for each engine in the test data. This will give us a test set of 100 engines with their TTF data. Will do that later when we create regression and classification labels for both training and test data. \n\nBut now let us add some features to smooth the sensors reading: rolling average and rolling standard deviation.\n","d770735f":"### Load Test Data:","a307a5b7":"create helper function to add the regression and classification labels to the training data","497c57ab":"# Regression Modelling:","9e16f937":"Create a helper function to calculate regression metrics","3c76424c":"### Data Source ###\n\n___Training Data:___  The aircraft engine run-to-failure data.\n[download trianing data](http:\/\/azuremlsamples.azureml.net\/templatedata\/PM_train.txt)  \n___Test Data:___ The aircraft engine operating data without failure events recorded.\n[download test data](http:\/\/azuremlsamples.azureml.net\/templatedata\/PM_test.txt)  \n___Ground Truth Data:___ The true remaining cycles for each engine in the testing data.\n[download truth data](http:\/\/azuremlsamples.azureml.net\/templatedata\/PM_truth.txt)  \n\nFor simplicity, data files have been downloaded to local Data folder.","b99745b2":"# Load train data\n","bd7ef08e":"### Data Columns\n\n\u2022\t__id__: is the engine ID, ranging from 1 to 100  \n\u2022\t__cycle__: per engine sequence, starts from 1 to the cycle number where failure had happened (trining data only) \n\u2022\t__setting1__ to __setting3__: engine operational settings  \n\u2022\t__s1__ to __s21__: sensors measurements  \n","c7d81d7e":"### Feature Extraction:","feefbe78":"\nLet us create a helper function to ease exploration of each feature invidually:","593e19ba":"Create a helper function to plot the regression residuals","e11239a4":"Same as training data, there are 100 engines, each engine has between 1 to 303 cycles (average of 76 cycles per engine). But this time, failure cycle was not provided.  \n\nFailure events for test data - remaining cycles before failure (TTF) - were provided in a separate truth file.","9be3f7a2":"With the help of these functions, let us prepare training and test data by adding features and labels","073944f7":"# Load test data","de0a7917":"### Prepare the Test Data:","6e9b2873":"Rolling average, rolling standard deviation, regression labels, and classification labels have been added to the training data.  \n\nLet us save the dataframe for later use in data exploration and modeling phases.","ce1e17b7":"create helper function to add the regression and classification labels to the training data","09d815f8":"No missing values. This is a clean dataset!","4487da2b":"## Compare all regression models tested","a4a767b5":"Rolling average, rolling standard deviation, regression labels, and classification labels have been added to the test data.  \n\nLet us save the dataframe for later use in data exploration and modeling phases","c6a826aa":"### Load Training Data:","f64c9c18":"['s12', 's7', 's21', 's20', 's6', 's14', 's9', 's13', 's8', 's3', 's17', 's2', 's15', 's4', 's11'] could be target for feature selection during modeling since their correlation with TTF is higher than other features.  \n\nLet us disply this correlation in heatmap"}}