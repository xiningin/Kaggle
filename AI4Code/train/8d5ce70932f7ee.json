{"cell_type":{"48f0a22b":"code","b507c768":"code","7b58af81":"code","b87653e2":"code","fe74d2f9":"code","a2eaae32":"code","7558e29a":"code","d1054f41":"code","4c0c8f6b":"code","a9ddc103":"code","85a9db72":"code","cfe97cb9":"code","9474eeea":"code","81598ccc":"code","7f694568":"code","af3eae88":"code","4d5dff37":"code","b8e2c6d1":"code","89b269ea":"code","463951d7":"code","8ef7a9f2":"code","8f7e7550":"code","43837e20":"code","b1506326":"code","9df17bc3":"code","19a03ca7":"code","b9354f08":"code","3d1a82c1":"code","d82066c1":"code","009663b8":"code","50b74466":"code","3d18a079":"code","30b5d41e":"markdown","e3d55d48":"markdown","977780da":"markdown","c00678ed":"markdown","46860950":"markdown","fe2ba911":"markdown","185598b6":"markdown","bf427476":"markdown","2311fbbc":"markdown","5d063116":"markdown","432a8776":"markdown","6dd784ed":"markdown","703078db":"markdown","92d457b7":"markdown"},"source":{"48f0a22b":"import pandas as pd\nimport numpy as np\nimport warnings  \nwarnings.filterwarnings('ignore')\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport itertools\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import  accuracy_score, f1_score, precision_score,confusion_matrix, recall_score, roc_auc_score\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\nfrom sklearn.svm import SVC \nplt.style.use('fivethirtyeight')","b507c768":"df=pd.read_csv('..\/input\/audit_data.csv')","7b58af81":"df.head()","b87653e2":"df.columns","fe74d2f9":"df.tail()","a2eaae32":"cols_del=['LOCATION_ID','TOTAL']\n\ndf.drop(cols_del, axis=1, inplace=True)","7558e29a":"df.head()","d1054f41":"df.info()","4c0c8f6b":"df.describe()","a9ddc103":"df.isna().sum()","85a9db72":"df['Money_Value'].fillna((df['Money_Value'].mean()), inplace=True)","cfe97cb9":"df.isna().sum()","9474eeea":"sns.countplot(df['Risk'], label = \"Count\") ","81598ccc":"X=df.drop(['Risk'],axis=1)","7f694568":"X.corrwith(df.Risk).plot.bar(\n        figsize = (20, 10), title = \"Correlation with Churn\", fontsize = 20,\n        rot = 90, grid = True)","af3eae88":"X.corr(method='pearson').style.format(\"{:.2}\").background_gradient(cmap=plt.get_cmap('coolwarm'), axis=1)","4d5dff37":"X=X.drop(['Detection_Risk'],axis=1)","b8e2c6d1":"X.columns","89b269ea":"df1=df[df['Risk']==1]\ncolumns=df1.columns[:21]\nplt.subplots(figsize=(18,15))\nlength=len(columns)\nfor i,j in itertools.zip_longest(columns,range(length)):\n    plt.subplot((length\/2),3,j+1)\n    plt.subplots_adjust(wspace=0.2,hspace=0.5)\n    df1[i].hist(bins=20,edgecolor='black')\n    plt.title(i)\nplt.show()","463951d7":"y=df['Risk']","8ef7a9f2":"from sklearn.model_selection import train_test_split,cross_val_score\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,stratify=y, random_state = 123)","8f7e7550":"from sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nX_train_scaled = pd.DataFrame(sc_X.fit_transform(X_train))\nX_test_scaled = pd.DataFrame(sc_X.transform(X_test))","43837e20":"logi = LogisticRegression(random_state = 0, penalty = 'l1')\nlogi.fit(X_train_scaled, y_train)","b1506326":"kfold = model_selection.KFold(n_splits=10, random_state=7)\nscoring = 'accuracy'\n\nacc_logi = cross_val_score(estimator = logi, X = X_train_scaled, y = y_train, cv = kfold,scoring=scoring)\nacc_logi.mean()","9df17bc3":"y_predict_logi = logi.predict(X_test_scaled)\nacc= accuracy_score(y_test, y_predict_logi)\nroc=roc_auc_score(y_test, y_predict_logi)\nprec = precision_score(y_test, y_predict_logi)\nrec = recall_score(y_test, y_predict_logi)\nf1 = f1_score(y_test, y_predict_logi)\n\nresults = pd.DataFrame([['Logistic Regression',acc, acc_logi.mean(),prec,rec, f1,roc]],\n               columns = ['Model', 'Accuracy','Cross Val Accuracy', 'Precision', 'Recall', 'F1 Score','ROC'])\nresults","19a03ca7":"random_forest_e = RandomForestClassifier(n_estimators = 100,criterion='entropy', random_state = 47)\nrandom_forest_e.fit(X_train_scaled, y_train)","b9354f08":"acc_rande = cross_val_score(estimator = random_forest_e, X = X_train_scaled, y = y_train, cv = kfold, scoring=scoring)\nacc_rande.mean()","3d1a82c1":"y_predict_r = random_forest_e.predict(X_test_scaled)\nroc=roc_auc_score(y_test, y_predict_r)\nacc = accuracy_score(y_test, y_predict_r)\nprec = precision_score(y_test, y_predict_r)\nrec = recall_score(y_test, y_predict_r)\nf1 = f1_score(y_test, y_predict_r)\n\nmodel_results = pd.DataFrame([['Random Forest',acc, acc_rande.mean(),prec,rec, f1,roc]],\n               columns = ['Model', 'Accuracy','Cross Val Accuracy', 'Precision', 'Recall', 'F1 Score','ROC'])\nresults = results.append(model_results, ignore_index = True)\nresults","d82066c1":"from sklearn import metrics\nimport matplotlib.pyplot as plt\n\nplt.figure()\n\n# Add the models to the list that you want to view on the ROC plot\nmodels = [\n{\n    'label': 'Logistic Regression',\n    'model': LogisticRegression(random_state = 0, penalty = 'l1'),\n},\n    {\n    'label': 'Random Forest Entropy',\n    'model': RandomForestClassifier(n_estimators = 100,criterion='entropy', random_state = 47),\n},\n    \n]\n\n# Below for loop iterates through your models list\nfor m in models:\n    model = m['model'] # select the model\n    model.fit(X_train_scaled, y_train) # train the model\n    y_pred=model.predict(X_test_scaled) # predict the test data\n# Compute False postive rate, and True positive rate\n    fpr, tpr, thresholds = metrics.roc_curve(y_test, model.predict_proba(X_test_scaled)[:,1])\n# Calculate Area under the curve to display on the plot\n    auc = metrics.roc_auc_score(y_test,model.predict(X_test_scaled))\n# Now, plot the computed values\n    plt.plot(fpr, tpr, label='%s ROC (area = %0.2f)' % (m['label'], auc))\n# Custom settings for the plot \nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('1-Specificity(False Positive Rate)')\nplt.ylabel('Sensitivity(True Positive Rate)')\nplt.title('Receiver Operating Characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()","009663b8":"cm_logi = confusion_matrix(y_test, y_predict_logi)\nplt.title('Confusion matrix of the Logistic classifier')\nsns.heatmap(cm_logi,annot=True,fmt=\"d\")\nplt.show()","50b74466":"cm_r = confusion_matrix(y_test, y_predict_r)\nplt.title('Confusion matrix of the Random Forest classifier')\nsns.heatmap(cm_r,annot=True,fmt=\"d\")\nplt.show()","3d18a079":"importances = random_forest_e.feature_importances_\nindices = np.argsort(importances)[::-1]\n\n# Rearrange feature names so they match the sorted feature importances\nnames = [X.columns[i] for i in indices]\n\n# Create plot\nplt.figure()\n\n# Create plot title\nplt.title(\"Feature Importance\")\n\n# Add bars\nplt.bar(range(X.shape[1]), importances[indices])\n\n# Add feature names as x-axis labels\nplt.xticks(range(X.shape[1]), names, rotation=90)\n\n# Show plot\nplt.show()","30b5d41e":"# Feature Scaling","e3d55d48":"# Cross Validation","977780da":"**As we can see that Money_Value has one null value so I am imputing it with the mean value of this feature.","c00678ed":"# Model evaluation","46860950":"# Confusion Matrix","fe2ba911":"# Feature Importance Plot","185598b6":"# Model Evaluation","bf427476":"# Applying Random Forest","2311fbbc":"# Applying Base Model : Logistic Regression","5d063116":"# Plotting ROC AUC Curve","432a8776":"# Cross Validation","6dd784ed":"**As we have seen there is no correlation of Detection_Risk so it will be deleted before model building process as it is not contributing well enough for prediction process.**","703078db":"**I am deleting TOTAL, which is just the total of PARA_A and PARA_B and LOCATION_ID as these are not significant for prediction process**","92d457b7":"# Train Test Split"}}