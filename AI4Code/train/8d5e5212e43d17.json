{"cell_type":{"f39b6721":"code","c7a37c53":"code","6ca8d5c6":"code","d280ead7":"code","e4bd425e":"code","2ebf4cb6":"code","b2ad9bbb":"code","4357bfa9":"code","7b69882e":"code","283adc3b":"code","1324f5cf":"code","4687bc6e":"code","9ce91c0a":"code","2e526715":"code","045c675d":"code","10c0e66e":"code","02956a62":"code","8c586bca":"code","6aba7489":"code","46006751":"code","8f77fcb4":"markdown","1f8473b4":"markdown","dc5d1823":"markdown","d800fe4e":"markdown","1ddc2c13":"markdown","0e44b36f":"markdown","234c4530":"markdown","a8fa1490":"markdown","e1d6ddf9":"markdown","aacec6cc":"markdown","91711d07":"markdown","2e40cb6c":"markdown","44a56c15":"markdown","33e2a75a":"markdown","79696ef1":"markdown"},"source":{"f39b6721":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n #   for filename in filenames:\n        #print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c7a37c53":"import os\nimport glob\nimport cv2\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization\nfrom tensorflow.keras.layers import Dropout, Flatten, Input, Dense\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom keras.models import *\nfrom keras.layers import *\nfrom keras.optimizers import *\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler\nfrom keras import backend as keras\nfrom sklearn.metrics import accuracy_score","6ca8d5c6":"image_path = os.path.join(\"\/kaggle\/input\/chest-xray-masks-and-labels\/Lung Segmentation\/CXR_png\")\nmask_path = os.path.join(\"\/kaggle\/input\/chest-xray-masks-and-labels\/data\/Lung Segmentation\/masks\")\nimages = os.listdir(image_path)\nmask = os.listdir(mask_path)\nprint(len(images), len(mask))","d280ead7":"mask2 = [fName.split(\".png\")[0] for fName in mask]\nimage_file_name = [fName.split(\"_mask\")[0] for fName in mask2]\nprint(len(image_file_name), len(mask))","e4bd425e":"check = [i for i in mask if \"mask\" in i]\nprint(len(check))","2ebf4cb6":"dsize = (256, 256)\ndef image_resize(img):\n    return cv2.resize(img,dsize )","b2ad9bbb":"#print(mask)\nx = np.array([np.array(np.stack(( image_resize(cv2.imread(os.path.join(image_path,filename.split(\"_mask\")[0]+\".png\"),  0)),), axis=-1)) for filename in image_file_name])\ny= np.array([np.array(np.stack(( image_resize(cv2.imread(os.path.join(mask_path,filename),  0)),), axis=-1)) for filename in mask])\n","4357bfa9":"print(x.shape, y.shape)","7b69882e":"def show_random_examples(x, y, p):\n    indices = np.random.choice(range(x.shape[0]), 10, replace=False)\n    x = x[indices]\n    y = y[indices]\n    p = p[indices]\n    dsize2 =(400,400)\n    plt.figure(figsize=(10, 5))\n    for i in range(10):\n        plt.subplot(2, 5, i + 1)\n        xi = cv2.resize(x[i], dsize2)\n        plt.imshow(xi)\n        plt.xticks([])\n        plt.yticks([])\n#        col = 'green' if np.argmax(y[i]) == np.argmax(p[i]) else 'red'\n#        plt.xlabel(class_names[np.argmax(p[i])], color=col)\n    plt.show()    \n    for i in range(10):\n        plt.subplot(2, 5, i + 1)\n        xi = cv2.resize(y[i], dsize2)\n        plt.imshow(xi)\n        plt.xticks([])\n        plt.yticks([])\n#        col = 'green' if np.argmax(y[i]) == np.argmax(p[i]) else 'red'\n#        plt.xlabel(class_names[np.argmax(p[i])], color=col)\n    plt.show()    \n    for i in range(10):\n        plt.subplot(2, 5, i + 1)\n        xi = cv2.resize(p[i], dsize2)\n        plt.imshow(xi)\n        plt.xticks([])\n        plt.yticks([])\n#        col = 'green' if np.argmax(y[i]) == np.argmax(p[i]) else 'red'\n#        plt.xlabel(class_names[np.argmax(p[i])], color=col)\n    plt.show()   ","283adc3b":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train_m, y_test_m = train_test_split(x, y, test_size=0.2, random_state=123)\nx2 = np.flip(X_train , axis = 2)\ny2 = np.flip(y_train_m , axis = 2)\nX_train= np.append(X_train, x2,axis=0)\ny_train_m = np.append(y_train_m, y2,axis=0)\nprint(X_train.shape,y_train_m.shape )\nshow_random_examples(X_train, y_train_m, y_train_m)","1324f5cf":"def unet(pretrained_weights = None,input_size = (256,256,1)):\n    inputs = Input(input_size)\n    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n    drop4 = Dropout(0.5)(conv4)\n    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n\n    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n    drop5 = Dropout(0.5)(conv5)\n\n    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n    merge6 = concatenate([drop4,up6], axis = 3)\n    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n\n    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n    merge7 = concatenate([conv3,up7], axis = 3)\n    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n\n    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n    merge8 = concatenate([conv2,up8], axis = 3)\n    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n\n    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n    merge9 = concatenate([conv1,up9], axis = 3)\n    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n\n    model = Model(inputs, conv10)\n\n    model.compile(optimizer = Adam(lr = 1e-3), loss = 'binary_crossentropy', metrics = ['accuracy'])\n    \n    #model.summary()\n\n   # if(pretrained_weights):\n    #    model.load_weights(pretrained_weights)\n\n    return model","4687bc6e":"model_1 = unet()\nmodel_1.summary()","9ce91c0a":"y_test_m2 = (y_test_m\/255.> .5).astype(int)","2e526715":"y_train_m2 = (y_train_m\/255.> .5).astype(int)\nmodel_checkpoint = ModelCheckpoint('\/kaggle\/working\/unet_membrane_a1.hdf5', monitor='loss',verbose=2, save_best_only=True)\n\nh = model_1.fit(\n    X_train\/255., y_train_m2,   \n    callbacks=[model_checkpoint],\n    validation_data=(X_test\/255., y_test_m2),\n    epochs=15, batch_size=20,\n    #validation_split=0.2,\n    #shuffle=True,\n)","045c675d":"plt.plot(h.history['accuracy'])\nplt.plot(h.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(h.history['loss'])\nplt.plot(h.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n\n\nlosses = h.history['loss']\naccs = h.history['accuracy']\nval_losses = h.history['val_loss']\nval_accs = h.history['val_accuracy']\nepochs = len(losses)\n\npreds = model_1.predict(X_test\/255.)*255\n\nshow_random_examples(X_test, y_test_m, preds)","10c0e66e":"from keras.callbacks import ReduceLROnPlateau\nlearning_rate_decay = ReduceLROnPlateau(monitor='loss', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)","02956a62":"h2 = model_1.fit(\n    X_train\/255., y_train_m2,\n    validation_data=(X_test\/255., y_test_m2),\n    epochs=50, batch_size=16,\n    shuffle=True,\n    verbose=2,\n    callbacks=[learning_rate_decay]\n)\n","8c586bca":"plt.plot(h2.history['accuracy'])\nplt.plot(h2.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(h2.history['loss'])\nplt.plot(h2.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n\n\nlosses = h2.history['loss']\naccs = h2.history['accuracy']\nval_losses = h2.history['val_loss']\nval_accs = h2.history['val_accuracy']\nepochs = len(losses)\n\npreds2 = model_1.predict(X_test\/255.)*255\n\nshow_random_examples(X_test, y_test_m, preds2)","6aba7489":"test_path = os.path.join(\"\/kaggle\/input\/chest-xray-masks-and-labels\/data\/Lung Segmentation\/test\")\ntest = os.listdir(test_path)\n#print(test)\ny= np.array([np.array(np.stack(( image_resize(cv2.imread(os.path.join(test_path,filename),  0)),), axis=-1)) for filename in test])\n\npreds3 = model_1.predict(y\/255.)*255\n","46006751":"show_random_examples(y, y, preds3)","8f77fcb4":"Model traing with decreaing Learning rate","1f8473b4":"Code to create train and valuation set. \nAugmentation on Train DATA set","dc5d1823":"Below program is to run the with decaying learning rate","d800fe4e":"**Here to learn Please let me know if I am doing something wrong**","1ddc2c13":"Input and Output both are images.\nIn This Python code I have writen logic to Get the mask from Chest X-ray.\nI have used only one type of augmentation 180 degree rotation or horizontal flip.\nIn valation data I am getting Around 98.17 % accuracy. ","0e44b36f":"Model creation","234c4530":"I am asuming the all the Mask have a file.\n","a8fa1490":"Plot after traing the dataset with decaying learning rate","e1d6ddf9":"Training the data model","aacec6cc":"Below is code to Show random exmaple ","91711d07":"AS from above images are 800 images (CXR_PNG) and 704(Mask).\nSo i have to map those. ","2e40cb6c":"Loading and Predition with test data set","44a56c15":"Graph for accuracy and loss for Val and traiing data set","33e2a75a":"TEST DATA Result","79696ef1":"Unet Model"}}