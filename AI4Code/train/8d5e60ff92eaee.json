{"cell_type":{"e6ebb542":"code","dd5a132f":"code","99c05076":"code","e3879c72":"code","c505eee7":"code","49a66dd0":"code","3bbbc417":"code","7c900f7b":"code","112743d9":"code","a50ab3df":"code","eeae0b87":"code","89d20593":"code","0dc4a939":"code","f9b0db13":"code","38894115":"code","39ef1122":"code","d58a2eff":"code","c1b68588":"code","41767aed":"code","1ceb4dea":"markdown","292c4dbd":"markdown","c03a4469":"markdown","9ecbeb3c":"markdown","a3bb94d3":"markdown","4f121ae5":"markdown","b034eb14":"markdown","e060ba92":"markdown","9af49b49":"markdown","1bc25646":"markdown","c683ccf7":"markdown","4dde139f":"markdown"},"source":{"e6ebb542":"!pip install nb-black > \/dev\/null\n%load_ext lab_black","dd5a132f":"import pandas as pd\nimport numpy as np\nfrom itertools import cycle\nimport matplotlib.pylab as plt\nfrom matplotlib.patches import Rectangle\nimport subprocess\nfrom tqdm.notebook import tqdm\n\nimport cv2\nfrom cv2 import VideoWriter, VideoWriter_fourcc\nimport os\nfrom IPython.display import Video\n\n\nplt.style.use(\"ggplot\")\ncolor_pal = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\ncolor_cycle = cycle(plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"])","99c05076":"train = pd.read_csv(\"..\/input\/tensorflow-great-barrier-reef\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tensorflow-great-barrier-reef\/test.csv\")\nss = pd.read_csv(\"..\/input\/tensorflow-great-barrier-reef\/example_sample_submission.csv\")\n\ntrain.shape, test.shape","e3879c72":"import greatbarrierreef\n\nenv = greatbarrierreef.make_env()  # initialize the environment\niter_test = (\n    env.iter_test()\n)  # an iterator which loops over the test set and sample submission\nfor (pixel_array, sample_prediction_df) in iter_test:\n    break\n    sample_prediction_df[\n        \"annotations\"\n    ] = \"0.5 0 0 100 100\"  # make your predictions here\n    env.predict(sample_prediction_df)  # register your predictions","c505eee7":"plt.style.use(\"default\")\nfig, ax = plt.subplots(figsize=(15, 10))\nax.imshow(pixel_array)\nax.axis(\"off\")\nax.set_title(\"Example Image from the Barrier Reef Dataset\", fontsize=14)\nplt.show()","49a66dd0":"plt.style.use(\"ggplot\")\nfig, axs = plt.subplots(3, 1, figsize=(15, 10), sharex=True, sharey=True)\n\nfor video in [0, 1, 2]:\n    for sequence, d in train.query(\"video_id == @video\").groupby(\"sequence\"):\n        d[\"sequence_frame\"].plot(ax=axs[video], label=f\"Sequence {sequence}\")\n    axs[video].set_title(f\"Video {video}: Sequence Frame vs Video Frame\")\n    axs[video].set_xlabel(\"Video Frame\")\n    axs[video].set_ylabel(\"Sequence Frame\")\n    axs[video].legend(bbox_to_anchor=(1, 1), loc=\"upper left\")\nplt.show()","3bbbc417":"train[\"n_annotations\"] = train[\"annotations\"].apply(lambda x: len(eval(x)))\ntrain[\"video_sequence\"] = (\n    train[\"video_id\"].astype(\"str\") + \"_\" + train[\"sequence\"].astype(\"str\")\n)\n\nax = (\n    train.groupby([\"video_sequence\"])[\"sequence_frame\"]\n    .max()\n    .sort_values()\n    .plot(kind=\"barh\", figsize=(12, 7), title=\"Length of Sequences\")\n)\nax.set_xlabel(\"Number of Frames in the Seqence\")","7c900f7b":"fig, axs = plt.subplots(1, 2, figsize=(15, 5))\ntrain.groupby([\"video_sequence\"])[\"n_annotations\"].mean().sort_values().plot(\n    kind=\"barh\", title=\"Avg of Annotations\", ax=axs[0], color=next(color_cycle)\n)\n\ntrain.groupby([\"video_sequence\"])[\"n_annotations\"].sum().sort_values().plot(\n    kind=\"barh\", title=\"Total Annotations\", ax=axs[1], color=next(color_cycle)\n)\nplt.show()","112743d9":"fig, axs = plt.subplots(1, 3, figsize=(15, 5), sharey=True)\nfor i, d in train.groupby([\"video_id\", \"sequence\"]):\n    d.set_index(\"sequence_frame\")[\"n_annotations\"].plot(ax=axs[i[0]])\n    axs[i[0]].set_title(f\"Video ID: {i[0]} - Sequence {i[1]}\")\nfig.suptitle(\"Number of Annotations per Frame for each Sequence\")\nplt.show()","a50ab3df":"plt.style.use(\"default\")\n\n\ndef plot_reef_image(\n    image_id,\n    df,\n    ax=None,\n    show_annotations=True,\n    line_width=1,\n    line_color=\"red\",\n    figsize=(30, 5),\n    image_dir=\"..\/input\/tensorflow-great-barrier-reef\/train_images\/\",\n):\n    \"\"\"\n    Plot reef image. If `show_annotations` is True, create boxes\n    with the annotations for starfish.\n    \"\"\"\n\n    example = df.query(\"image_id == @image_id\")\n    video = example[\"video_id\"].values[0]\n    frame = example[\"video_frame\"].values[0]\n    annotations = eval(example[\"annotations\"].values[0])\n\n    img = plt.imread(f\"{image_dir}video_{video}\/{frame}.jpg\")\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n    ax.imshow(img)\n    ax.axis(\"off\")\n\n    n_annotations = len(annotations)\n    ax.set_title(f\"image_id: {image_id} ({n_annotations} Starfish)\", fontsize=12)\n\n    if show_annotations:\n        for a in annotations:\n            ax.add_patch(\n                Rectangle(\n                    (a[\"x\"], a[\"y\"]),\n                    a[\"width\"],\n                    a[\"height\"],\n                    lw=line_width,\n                    facecolor=\"none\",\n                    edgecolor=line_color,\n                )\n            )\n\n    return ax","eeae0b87":"# Find image with the most number of annotations as an example\nimage_id = train.sort_values(\"n_annotations\").tail(1)[\"image_id\"].values[0]\nax = plot_reef_image(image_id, train, line_color=\"red\")","89d20593":"fig, axs = plt.subplots(2, 4, figsize=(20, 10))\naxs = axs.flatten()\nimage_ids = train.sample(8, random_state=529)[\"image_id\"].values\n\nfor i, image_id in enumerate(image_ids):\n    plot_reef_image(image_id, train, ax=axs[i])\nplt.tight_layout()\nplt.show()","0dc4a939":"fig, axs = plt.subplots(2, 4, figsize=(20, 10))\naxs = axs.flatten()\nimage_ids = (\n    train.query(\"n_annotations >= 5\").sample(8, random_state=529)[\"image_id\"].values\n)\n\nfor i, image_id in enumerate(image_ids):\n    plot_reef_image(image_id, train, ax=axs[i])\nplt.tight_layout()\nplt.show()","f9b0db13":"fig, axs = plt.subplots(2, 4, figsize=(20, 10))\naxs = axs.flatten()\nimage_ids = (\n    train.query(\"n_annotations == 0\").sample(8, random_state=529)[\"image_id\"].values\n)\n\nfor i, image_id in enumerate(image_ids):\n    plot_reef_image(image_id, train, ax=axs[i])\nplt.tight_layout()\nplt.show()","38894115":"def add_annotations(img, annotations, color=\"red\", thickness=3):\n    \"\"\"\n    Adds annotations to an image using cv2.\n\n    annotations: [list] of dictionaries with the annoation details\n    \"\"\"\n    if color == \"red\":\n        box_color = (0, 0, 255)  # Red\n    elif color == \"black\":\n        box_color = (0, 0, 0)  # Black\n    for a in annotations:\n        cv2.rectangle(\n            img,\n            (a[\"x\"], a[\"y\"]),\n            (a[\"x\"] + a[\"width\"], a[\"y\"] + a[\"height\"]),\n            box_color,\n            thickness=thickness,\n        )\n\n    return img\n\n\ndef create_reef_video(\n    train,\n    video_id,\n    start_video_frame,\n    end_video_frame,\n    annotate=True,\n    output_filename=\".\/test.mp4\",\n    FPS=30,\n    image_dir=\"..\/input\/tensorflow-great-barrier-reef\/train_images\/\",\n):\n\n    width = 1280\n    height = 720\n\n    fourcc = VideoWriter_fourcc(*\"mp4v\")\n\n    temp_fn = output_filename.replace(\".mp4\", \"\") + \"_temp.mp4\"\n\n    video_file = VideoWriter(temp_fn, fourcc, float(FPS), (width, height))\n\n    subset_df = (\n        train.query(\n            \"video_id == @video_id and video_frame >= @start_video_frame and video_frame <= @end_video_frame\"\n        )\n        .reset_index(drop=True)\n        .copy()\n    )\n    for i, example in tqdm(subset_df.iterrows(), total=len(subset_df)):\n        video = example[\"video_id\"]\n        frame = example[\"video_frame\"]\n        image_fn = f\"{image_dir}video_{video}\/{frame}.jpg\"\n        img = cv2.imread(image_fn)\n        if annotate:\n            annotations = eval(example[\"annotations\"])\n            img = add_annotations(img, annotations)\n        video_file.write(img)\n\n    video_file.release()\n\n    subprocess.run(\n        [\n            \"ffmpeg\",\n            \"-i\",\n            temp_fn,\n            \"-crf\",\n            \"18\",\n            \"-preset\",\n            \"veryfast\",\n            \"-vcodec\",\n            \"libx264\",\n            output_filename,\n            \"-loglevel\",\n            \"error\",\n        ]\n    )\n\n    os.remove(temp_fn)\n\n    return output_filename","39ef1122":"create_reef_video(\n    train,\n    output_filename=\"example-1.mp4\",\n    annotate=True,\n    video_id=1,\n    start_video_frame=9090,\n    end_video_frame=9172,\n)\nVideo(\"example-1.mp4\", width=800)","d58a2eff":"create_reef_video(\n    train,\n    output_filename=\"example-2.mp4\",\n    annotate=True,\n    video_id=2,\n    start_video_frame=5600,\n    end_video_frame=5800,\n)\nVideo(\"example-2.mp4\", width=900)","c1b68588":"create_reef_video(\n    train,\n    output_filename=\"example-3.mp4\",\n    annotate=True,\n    video_id=0,\n    start_video_frame=4500,\n    end_video_frame=4700,\n)\nVideo(\"example-3.mp4\", width=900)","41767aed":"\nfor video, data in train.groupby(\"video_id\"):\n    print(f'======== Creating Annotated Video {video} ========')\n    start_frame = data[\"video_frame\"].min()\n    end_frame = data[\"video_frame\"].max()\n    create_reef_video(\n        train,\n        output_filename=f\"full_video{video}_annotated.mp4\",\n        annotate=True,\n        video_id=video,\n        start_video_frame=start_frame,\n        end_video_frame=end_frame,\n    )","1ceb4dea":"# Examples of Annotations\nThe below function allows us to plot an image with its annotations","292c4dbd":"## Example of using the submission package.\nThe `greatbarrierreef` package loops over the test set. We have to predict each sample before we can see the next. This will impact how we must design our model.","c03a4469":"## Example Image. Can you see the starfish?","9ecbeb3c":"# Create a video by merging images","a3bb94d3":"## Example of Sequences within the training videos\n\n- Do we actually have 3 different videos?\n- Are the videos just subsets of a single long video?","4f121ae5":"# First Look at the Starfish Dataset\n\nThis notebook was created during a live coding session on twitch. Follow here: https:\/\/www.twitch.tv\/medallionstallion_\/","b034eb14":"# Create Full Annotated Videos","e060ba92":"# How many Annotations per Frame?\n- Is it different in each video?\n- Is it different in each sequence within a video?","9af49b49":"## Examples with >= 5 Annotations","1bc25646":"## Examples with 0 annotations","c683ccf7":"# Plot A Bunch of Random Images with Annotations","4dde139f":"# Training Metadata\n- `video_id` - Unique per video. We only have 3 videos in the training dataset.\n- `sequence` - random number to identify a group within the video of uncut footage.\n- `video_frame` - frame number within the entire video\n- `squence_frame` - frame number within the sequence (shot clip from within the video)\n- `image_id` - a combination of video_id + video_frame. Links to the image in the training images directory.\n- `annotations` - bounding boxes for starfish within the given frame."}}