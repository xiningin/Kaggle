{"cell_type":{"92112403":"code","1fb783a9":"code","a3ef6007":"code","d330ee07":"code","ea345237":"code","31d8675c":"code","1c3d38e9":"code","fc0086f3":"code","2fc131de":"code","0c0f3011":"code","3bc2a79e":"code","5125138a":"code","6b33a89b":"code","0201ef26":"code","883a78a4":"code","8fff3702":"code","ef500cfb":"code","93c70aca":"code","6a08e094":"code","c754a6ed":"code","bda27a0a":"code","63539bb8":"code","f664d201":"markdown","94f9bf80":"markdown","2ccab037":"markdown","5a5301da":"markdown","eda89e41":"markdown","ba9016dd":"markdown","fc144a2b":"markdown","5e23ef7d":"markdown","832e9444":"markdown","55b6910f":"markdown","28f32a17":"markdown","97e28806":"markdown","e80413cf":"markdown","8e193d9e":"markdown","42d8f57c":"markdown","97ba8acd":"markdown","60073cf4":"markdown","7f89ea2a":"markdown","a450e80b":"markdown","20df9faf":"markdown","e8bfd4a3":"markdown","5f83f4ca":"markdown","ef0043d6":"markdown","ebeef909":"markdown","a9ea9021":"markdown","55f6c137":"markdown","814bd2c8":"markdown","fd3924f6":"markdown","453b299a":"markdown","d1814810":"markdown","0386f8ee":"markdown","bd677690":"markdown","5fb945f1":"markdown","4623ecf4":"markdown","3ee552d6":"markdown","39cf8307":"markdown","d47382d3":"markdown","52e21fe6":"markdown","842c54dc":"markdown","afd81cde":"markdown","1ee01e81":"markdown","94333095":"markdown","ed51e281":"markdown","b8efa237":"markdown","bfd5e8de":"markdown"},"source":{"92112403":"#packages\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as sns\nfrom scipy import stats\n\n\n#importing data \ndata=pd.read_csv('..\/input\/train.csv')\nanwsering_this=pd.read_csv('..\/input\/test.csv')\nsubmission_id=anwsering_this.Id.values","1fb783a9":"#inspecting data \ndata.head()","a3ef6007":"data.info()","d330ee07":"#check \"Id\" column whether or not have duplicates\nif len(set(data.Id))==len(data.Id):\n    print(\"No duplicates for Id column\")\n\n#drop \"Id\" column\ndata.drop(\"Id\", axis = 1, inplace = True)\n","ea345237":"f,a=plt.subplots(figsize=(8,6))\nplt.scatter(data.GrLivArea, data.SalePrice, c = \"blue\",marker='s')\nplt.xlabel(\"GrLivArea\")\nplt.ylabel(\"SalePrice\")\ndata=data[data.GrLivArea < 4000]\nprint(data.shape)\nprint(\"Four instances droped\")","31d8675c":"#target\nprint(data['SalePrice'].describe())\n\n#plotting target column\nplt.style.use('seaborn')\nf,a=plt.subplots(figsize=(8,6))\nplt.title('Before log(1+x)')\nsns.distplot(data['SalePrice'],fit=stats.norm)\nf,a=plt.subplots(figsize=(8,6))\nstats.probplot(data['SalePrice'], plot=plt)\n\n\n","1c3d38e9":"#log it \ndata['SalePrice'] = np.log1p(data['SalePrice'])\n\n#plot again\nf,a=plt.subplots(figsize=(8,6))\nplt.title('After log(1+x)')\nsns.distplot(data['SalePrice'],fit=stats.norm)\n\nf,a=plt.subplots(figsize=(8,6))\n\nstats.probplot(data['SalePrice'],plot=plt)\n\n","fc0086f3":"#correlation analysis\ncorr=data.corr()\nf,a=plt.subplots(figsize=(30,26))\nsns.heatmap(corr,annot=True)\nprint('correlation with SalePrice Rank list')\nprint(corr['SalePrice'].sort_values(ascending=False))\n","2fc131de":"# concatenate train and test datasets\n# store the location of separate\nsep=data.shape[0]\ndata_fts=data.drop(['SalePrice'],axis=1)\ntest_fts=anwsering_this.drop(['Id'],axis=1)\nall_instances=pd.concat([data_fts,test_fts]).reset_index(drop=True)\n\n\n#count missing values\nnull_counts=all_instances.isnull().sum().sort_values(ascending=False)\nprint(null_counts.head(38))","0c0f3011":"# Filling missing values\n\n\n# NaN of PoolQC means \"No Pool\"\nall_instances.loc[:,'PoolQC']=all_instances.loc[:,'PoolQC'].fillna('No')\n# NaN of MiscFeature means \"No Miscellaneous feature\"\nall_instances.loc[:,'MiscFeature']=all_instances.loc[:,'MiscFeature'].fillna('No')\n# NaN of Alley means \"No alley access\"\nall_instances.loc[:,'Alley']=all_instances.loc[:,'Alley'].fillna('No')\n# NaN of Fence means \"No Fence\"\nall_instances.loc[:,'Fence']=all_instances.loc[:,'Fence'].fillna('No')\n# NaN of FireplaceQu means \"No Fireplace\"\nall_instances.loc[:,'FireplaceQu']=all_instances.loc[:,'FireplaceQu'].fillna('No')\n# NaN of LotFrontage means \"No street connected to property\"\nall_instances.loc[:,'LotFrontage']=all_instances.loc[:,'LotFrontage'].fillna(0)\n# NaN of GarageCond means \"No Garage\"\nall_instances.loc[:,'GarageCond']=all_instances.loc[:,'GarageCond'].fillna('No')\n# NaN of GarageQual means \"No Garage\"\nall_instances.loc[:,'GarageQual']=all_instances.loc[:,'GarageQual'].fillna('No')\n# NaN of GarageYrBlt means \"No Garage\"\nall_instances.loc[:,'GarageYrBlt']=all_instances.loc[:,'GarageYrBlt'].fillna(0)\n# NaN of GarageFinish means \"No Garage\"\nall_instances.loc[:,'GarageFinish']=all_instances.loc[:,'GarageFinish'].fillna('No')\n# NaN of GarageType means \"No Garage\"\nall_instances.loc[:,'GarageType']=all_instances.loc[:,'GarageType'].fillna('No')\n# NaN of BsmtCond means \"No Basement\"\nall_instances.loc[:,'BsmtCond']=all_instances.loc[:,'BsmtCond'].fillna('No')\n# NaN of BsmtExposure means \"No Basement\"\nall_instances.loc[:,'BsmtExposure']=all_instances.loc[:,'BsmtExposure'].fillna('No_B')\n# NaN of BsmtQual means \"No Basement\"\nall_instances.loc[:,'BsmtQual']=all_instances.loc[:,'BsmtQual'].fillna('No')\n# NaN of BsmtFinType2 means \"No Basement\"\nall_instances.loc[:,'BsmtFinType2']=all_instances.loc[:,'BsmtFinType2'].fillna('No')\n# NaN of BsmtFinType1 means \"No Basement\"\nall_instances.loc[:,'BsmtFinType1']=all_instances.loc[:,'BsmtFinType1'].fillna('No')\n# NaN of MasVnrType means \"No Masonry veneer\"\nall_instances.loc[:,'MasVnrType']=all_instances.loc[:,'MasVnrType'].fillna('No')\n# NaN of MasVnrArea means \"No Masonry veneer\" \nall_instances.loc[:,'MasVnrArea']=all_instances.loc[:,'MasVnrArea'].fillna(0)\n\n# NaN of MSZoning fill with \"RL\",Mode\nall_instances.loc[:,'MSZoning']=all_instances.loc[:,'MSZoning'].fillna(\"RL\")\n\n# NaN of BsmtHalfBath means \"No Basement\"\nall_instances.loc[:,'BsmtHalfBath']=all_instances.loc[:,'BsmtHalfBath'].fillna(0)\n# NaN of Utilities fill with \"AllPub\",Mode\nall_instances.loc[:,'Utilities']=all_instances.loc[:,'Utilities'].fillna(\"AllPub\")\n# NaN of Functional fill with \"Typ\",Mode\nall_instances.loc[:,'Functional']=all_instances.loc[:,'Functional'].fillna(\"Typ\")\n# NaN of BsmtFullBath fill with 0\nall_instances.loc[:,'BsmtFullBath']=all_instances.loc[:,'BsmtFullBath'].fillna(0)\n# NaN of BsmtFinSF2 fill with 0\nall_instances.loc[:,'BsmtFinSF2']=all_instances.loc[:,'BsmtFinSF2'].fillna(0)\n# NaN of BsmtFinSF1 fill with 0 \nall_instances.loc[:,'BsmtFinSF1']=all_instances.loc[:,'BsmtFinSF1'].fillna(0) \n# NaN of Exterior2nd fill with \"VinylSd\",Mode\nall_instances.loc[:,'Exterior2nd']=all_instances.loc[:,'Exterior2nd'].fillna(\"VinylSd\") \n# NaN of BsmtUnfSF fill with 0\nall_instances.loc[:,'BsmtUnfSF']=all_instances.loc[:,'BsmtUnfSF'].fillna(0) \n# NaN of TotalBsmtSF fill with 0\nall_instances.loc[:,'TotalBsmtSF']=all_instances.loc[:,'TotalBsmtSF'].fillna(0) \n# NaN of Exterior1st fill with \"VinylSd\",Mode\nall_instances.loc[:,'Exterior1st']=all_instances.loc[:,'Exterior1st'].fillna(\"VinylSd\") \n# NaN of SaleType fill with \"WD\",Mode\nall_instances.loc[:,'SaleType']=all_instances.loc[:,'SaleType'].fillna(\"WD\") \n# NaN of Electrical fill with \"SBrkr\",Mode\nall_instances.loc[:,'Electrical']=all_instances.loc[:,'Electrical'].fillna(\"SBrkr\")\n# NaN of KitchenQual fill with \"TA\",Mode\nall_instances.loc[:,'KitchenQual']=all_instances.loc[:,'KitchenQual'].fillna(\"TA\")\n# NaN of GarageArea fill with 0\nall_instances.loc[:,'GarageArea']=all_instances.loc[:,'GarageArea'].fillna(0)\n# NaN of GarageCars fill with 0\nall_instances.loc[:,'GarageCars']=all_instances.loc[:,'GarageCars'].fillna(0)\n\n# check if any missing values remain\nprint(\"Remain \"+str(all_instances.isnull().sum().sum())+\" missing values.\")\n\n\n\n\n\n\n","3bc2a79e":"\n\n# numeric features suppose to be categorical feature\nn2c=['MSSubClass','MoSold']\nfor i in n2c:\n    all_instances[i]=all_instances[i].astype(str)\n    \n\n# summerize categorical features suppose to be numeric feature\nc2n_=['GarageCond','GarageQual','GarageFinish','FireplaceQu','KitchenQual','HeatingQC','BsmtFinType2','BsmtFinType1','BsmtExposure','BsmtCond','BsmtQual','ExterCond','ExterQual','Fence']\n\n# sequential categorical features can be transform to numerical feature\nlv4_NA=['PoolQC']\nlv5_full=['ExterQual','ExterCond','HeatingQC','KitchenQual']\nlv5_NA=['BsmtQual','BsmtCond','FireplaceQu','GarageQual','GarageCond']\nBsmt_Type=['BsmtFinType2','BsmtFinType1']\n\n# lv4_NA\nall_instances['PoolQC']=all_instances['PoolQC'].replace({'No':0,'Fa':1,'TA':2,'Gd':3,'Ex':4})\n\n# lv5_full\nfor i in lv5_full:\n    all_instances[i]=all_instances[i].replace({'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})\n\n# lv5_NA    \nfor i in lv5_NA:\n    all_instances[i]=all_instances[i].replace({'No':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})\n\n# Bsmt_Type\nfor i in Bsmt_Type:\n    all_instances[i]=all_instances[i].replace({'No':0,'Unf':1,'LwQ':2,'Rec':3,'BLQ':4,'ALQ':5,'GLQ':6})\n\n\n# rest of it \nall_instances['GarageFinish']=all_instances['GarageFinish'].replace({'No':0,'Unf':1,'RFn':2,'Fin':3})\nall_instances['BsmtExposure']=all_instances['BsmtExposure'].replace({'No_B':0,'No':1,'Mn':2,'Av':3,'Gd':4})\nall_instances['Fence']=all_instances['Fence'].replace({'No':0,'MnWw':1,'GdWo':2,'MnPrv':3,'GdPrv':4})\n\n\n","5125138a":"# Combine features, you can combine more if you like.\n# Overall combination\nall_instances[\"OverallRate\"] = all_instances[\"OverallQual\"] * all_instances[\"OverallCond\"]\n# Garage combination\nall_instances[\"GarageRate\"] = all_instances[\"GarageQual\"] * all_instances[\"GarageCond\"]\n# Exterior combination\nall_instances[\"ExterRate\"] = all_instances[\"ExterQual\"] * all_instances[\"ExterCond\"]\n# Bsmt combination\nall_instances[\"BsmtRate\"]=all_instances[\"BsmtQual\"] * all_instances[\"BsmtCond\"]\n\n# Pool combination\nall_instances[\"PoolRate\"]=all_instances[\"PoolQC\"] * all_instances[\"PoolArea\"]\n\n# Make polynomial features \n# Adding Polynomial features for top 10 correlated features\ntop_10_corr=list(corr['SalePrice'].sort_values(ascending=False).index)[1:11]\nfor i in top_10_corr:\n    all_instances[i+'_s2']=all_instances[i] ** 2\n    all_instances[i+'_s3']=all_instances[i] ** 3\n    all_instances[i+'_sqrt']=np.sqrt(all_instances[i])\nprint(all_instances.shape)","6b33a89b":"from scipy.stats import skew\n\n# Seperate numeric columns and object columns.\ncategorical_features = all_instances.select_dtypes(include = [\"object\"]).columns\nnumerical_features = all_instances.select_dtypes(exclude = [\"object\"]).columns\n\nprint(\"Numerical features : \" + str(len(numerical_features)))\nprint(\"Categorical features : \" + str(len(categorical_features)))\nall_instances_num = all_instances[numerical_features]\nall_instances_cat = all_instances[categorical_features]\n\n\n# log(1+x) transform all skewed columns\nskewness = all_instances_num.apply(lambda x: skew(x))\nskewness = skewness[abs(skewness) > 0.5]\nprint(str(skewness.shape[0]) + \" skewed numerical features to log transform\")\nskewed_features = skewness.index\nall_instances_num[skewed_features] = np.log1p(all_instances_num[skewed_features])\n\n# concate them back\nall_instances = pd.concat([all_instances_num, all_instances_cat], axis = 1)\nprint(all_instances.shape)\n","0201ef26":"all_instances=pd.get_dummies(all_instances,drop_first=True)\nprint(all_instances.shape)","883a78a4":"from sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV,Ridge,Lasso,ElasticNet\nfrom sklearn.metrics import mean_squared_error, make_scorer\n\n# Slicing our pre-processed dataset as before we combine train dataset and test dataset\nnew_data=all_instances[:sep]\ny=data['SalePrice']\nnew_test=all_instances[sep:]\n","8fff3702":"# split dataset first\nX_train,X_test,y_train,y_test=train_test_split(new_data,y,test_size=0.3,random_state=111)\n\n# Standardize our train dataset and test dataset separatly\nstdSc = StandardScaler()\n\n# X_train dataset we use \"fit_transform\" and X_test we use \"transform\" because in that case means we use the \"fit\"(mean and std) from X_train to transform\n# our X_test,this make sure our data is equally rescaled.\n\n# Another thing,StandardScaler will automatic converte values to numeric,so we need serparate numeric and other type of columns. \nX_train.loc[:, numerical_features] = stdSc.fit_transform(X_train.loc[:, numerical_features])\nX_test.loc[:, numerical_features] = stdSc.transform(X_test.loc[:, numerical_features])\n","ef500cfb":"# sign-flip the RMSE for parameter search\nscorer = make_scorer(mean_squared_error, greater_is_better = False)\n\n# real cross validation score \ndef rmse_cv_train(model):\n    rmse= np.sqrt(-cross_val_score(model, X_train, y_train, scoring = scorer, cv = 10))\n    return(rmse)\n\ndef rmse_cv_test(model):\n    rmse= np.sqrt(-cross_val_score(model, X_test, y_test, scoring = scorer, cv = 10))\n    return(rmse)\n","93c70aca":"# Linear Regression\nlr = LinearRegression()\n\n# Look at predictions on training and validation set\nprint(\"RMSE on Training set :\", rmse_cv_train(lr).mean())\nprint(\"RMSE on Test set :\", rmse_cv_test(lr).mean())\n","6a08e094":"# RigdeCV implements ridge regression with built-in cross-validation of the alpha parameter, similar to GridSearchCV to Parameter optimize.\nridge = RidgeCV(alphas = [0.01, 0.04, 0.08, 0.1, 0.4, 0.8, 1, 4, 8, 10, 40, 80])\nridge.fit(X_train, y_train)\nalpha= ridge.alpha_\nprint(\"Best alpha :\", alpha)\n\nprint(\"Try again for more precision with alphas centered around \" + str(alpha))\nridge = RidgeCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85, \n                          alpha * .9, alpha * .95, alpha, alpha * 1.05, alpha * 1.1, alpha * 1.15,\n                          alpha * 1.25, alpha * 1.3, alpha * 1.35, alpha * 1.4], \n                cv = 10)\nridge.fit(X_train, y_train)\nalpha_ridge = ridge.alpha_\nprint(\"Best alpha :\", alpha_ridge)\n\nprint(\"Ridge RMSE on Training set :\", rmse_cv_train(ridge).mean())\nprint(\"Ridge RMSE on Test set :\", rmse_cv_test(ridge).mean())\n\n# Final scoring\nridge_=Ridge(alpha=alpha_ridge)\nridge_.fit(X_train,y_train)\nridge_predict=ridge_.predict(X_test)\nridge_RMSE=np.sqrt(mean_squared_error(ridge_predict,y_test))\nprint(\"Ridge regression RMSE :\",ridge_RMSE)\n\n\n\n\n\n","c754a6ed":"# LassoCV similar to RigdeCV\nlasso = LassoCV(alphas = [0.0001, 0.0004, 0.0008, 0.001, 0.004, 0.008, 0.01, 0.04, 0.08, 0.1, \n                          0.4, 0.8, 1], \n                max_iter = 50000, cv = 10)\nlasso.fit(X_train, y_train)\nalpha = lasso.alpha_\nprint(\"Best alpha :\", alpha)\n\nprint(\"Try again for more precision with alphas centered around \" + str(alpha))\nlasso = LassoCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, \n                          alpha * .85, alpha * .9, alpha * .95, alpha, alpha * 1.05, \n                          alpha * 1.1, alpha * 1.15, alpha * 1.25, alpha * 1.3, alpha * 1.35, \n                          alpha * 1.4], \n                max_iter = 50000, cv = 10)\nlasso.fit(X_train, y_train)\nalpha_lasso = lasso.alpha_\nprint(\"Best alpha :\", alpha_lasso)\n\nprint(\"Lasso RMSE on Training set :\", rmse_cv_train(lasso).mean())\nprint(\"Lasso RMSE on Test set :\", rmse_cv_test(lasso).mean())\n\n# Final scoring\nlasso_=Lasso(alpha=alpha_lasso)\nlasso_.fit(X_train,y_train)\nlasso_predict=lasso_.predict(X_test)\nlasso_RMSE=np.sqrt(mean_squared_error(lasso_predict,y_test))\nprint(\"Lasso regression RMSE :\",lasso_RMSE)","bda27a0a":"elasticNet = ElasticNetCV(l1_ratio = [0.1, 0.3, 0.5, 0.6, 0.7, 0.8, 0.85, 0.9, 0.95, 1],\n                          alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, \n                                    0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6], \n                          max_iter = 50000, cv = 10)\nelasticNet.fit(X_train, y_train)\nalpha = elasticNet.alpha_\nratio = elasticNet.l1_ratio_\nprint(\"Best l1_ratio :\", ratio)\nprint(\"Best alpha :\", alpha )\n\nprint(\"Try again for more precision with l1_ratio centered around \" + str(ratio))\nelasticNet = ElasticNetCV(l1_ratio = [ratio * .85, ratio * .9, ratio * .95, ratio, ratio * 1.05, ratio * 1.1, ratio * 1.15],\n                          alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6], \n                          max_iter = 50000, cv = 10)\nelasticNet.fit(X_train, y_train)\nif (elasticNet.l1_ratio_ > 1):\n    elasticNet.l1_ratio_ = 1    \nalpha = elasticNet.alpha_\nratio = elasticNet.l1_ratio_\nprint(\"Best l1_ratio :\", ratio)\nprint(\"Best alpha :\", alpha )\n\nprint(\"Now try again for more precision on alpha, with l1_ratio fixed at \" + str(ratio) + \n      \" and alpha centered around \" + str(alpha))\nelasticNet = ElasticNetCV(l1_ratio = ratio,\n                          alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85, alpha * .9, \n                                    alpha * .95, alpha, alpha * 1.05, alpha * 1.1, alpha * 1.15, alpha * 1.25, alpha * 1.3, \n                                    alpha * 1.35, alpha * 1.4], \n                          max_iter = 50000, cv = 10)\nelasticNet.fit(X_train, y_train)\nif (elasticNet.l1_ratio_ > 1):\n    elasticNet.l1_ratio_ = 1    \nalpha_eln = elasticNet.alpha_\nratio_eln = elasticNet.l1_ratio_\nprint(\"Best l1_ratio :\", ratio_eln)\nprint(\"Best alpha :\", alpha_eln )\n\nprint(\"ElasticNet RMSE on Training set :\", rmse_cv_train(elasticNet).mean())\nprint(\"ElasticNet RMSE on Test set :\", rmse_cv_test(elasticNet).mean())\n\n# Final scoring\nelasticNet_=ElasticNet(l1_ratio=ratio_eln,alpha=alpha_eln)\nelasticNet_.fit(X_train,y_train)\nelasticNet_predict=elasticNet_.predict(X_test)\nelasticNet_RMSE=np.sqrt(mean_squared_error(elasticNet_predict,y_test))\nprint(\"elasticNet RMSE :\",elasticNet_RMSE)","63539bb8":"# create new model object first\nFinal_model=Ridge(alpha=alpha_ridge)\n\n# fit all data and Standardize first\n\nstdSc_ = StandardScaler()\nnew_data.loc[:, numerical_features] = stdSc_.fit_transform(new_data.loc[:, numerical_features])\n\n# Standardize test\nnew_test.loc[:, numerical_features] = stdSc_.transform(new_test.loc[:, numerical_features])\n\nFinal_model.fit(new_data,y)\nsubmission=Final_model.predict(new_test)\n\n# reverse our data since we log(1+x) transformed before\nsubmission=np.expm1(submission)\n\ndf_submission=pd.DataFrame({'Id':submission_id,'SalePrice':submission})\n\ndf_submission.to_csv('submission.csv',index=False)","f664d201":"## Correlation analysis","94f9bf80":"To explain why Standardize data,I simply copy the documentation from help(Standardize):\n\n*Standardization of a dataset is a common requirement for many\nmachine learning estimators: they might behave badly if the\nindividual features do not more or less look like standard normally\ndistributed data (e.g. Gaussian with 0 mean and unit variance).*\n \n*For instance many elements used in the objective function of\na learning algorithm (such as the RBF kernel of Support Vector\nMachines or the L1 and L2 regularizers of linear models) assume that\nall features are centered around 0 and have variance in the same\norder. If a feature has a variance that is orders of magnitude larger\nthat others, it might dominate the objective function and make the\nestimator unable to learn from other features correctly as expected.*","2ccab037":"## Decreasing skewness","5a5301da":"# **0. Introduction**","eda89e41":"Rigde regression add L1 penalty to Linear Regression cost function.","ba9016dd":"First Seperate numeric columns and object columns to process numeric column,when |skewness| >0.5 we call it skewed,and log(1+x) transform them more close [](http:\/\/)to normal distribution.","fc144a2b":"[Documentation](https:\/\/ww2.amstat.org\/publications\/jse\/v19n3\/decock.pdf) indicates we have some outliers,and the author recommends removing houses have more than 4000 square feet' from the dataset,so we should remove them.","5e23ef7d":"# **4. Submission**","832e9444":"## Imputing Missing values","55b6910f":"**Reducing all numeric columns skewness.**","28f32a17":"First,let's check out simple Linear Regression performance.","97e28806":"Our models only take numeric values,all categorical features have to be encoding as numerical values.Concretly use two ways to encoding categorical features.\n* **Label encoder**: Concretly encode our features as int numbers such as 0,1,2,3,4...,usually for encoding those sequential categorical features\n* **One-hot encoder**:Concretly encode our features as binarays,one category will add one column to dataset.0 means instance not belong to this category 1 on contrary.\n\n\n(*\"Concretly\" via Andrew Ng*)","e80413cf":"**My English is not very good. There may be many grammatical mistakes. I hope you guys can understand what I am trying to say ,and if any suggestion plz tell me ! (..\u2022\u02d8_\u02d8\u2022..).**","8e193d9e":"**This part I simply add few features,notice that you can use your imgination to create more features as you wish.**","42d8f57c":"**Slicing dataset**","97ba8acd":"Before fill missing values and feature engineering we should first concatenate train and test datasets,by doing this can avoid we do the same work to test dataset again when predicting.","60073cf4":"## **Standardize**","7f89ea2a":"# **1. Exploratory data analysis and some pre-processing**","a450e80b":"For numeric variable usually use column mean or median,use mean will considering the variable popluation but easy to influenced by outliers,use median will ignore the impact of outliers but will lost lots of information from  the variable popluation.","20df9faf":"Use log(1+x) fuction to make our target varible more close to Normal Distribution,for Linear Model is good to get high score.","e8bfd4a3":"**Label encoding**","5f83f4ca":"## **ElasticNet**","ef0043d6":"Rigde regression add L2 penalty to Linear Regression cost function , so it able to regularize.","ebeef909":"That is **Explosive Errors !!!** ,I think we should just move on ,if anyone know what just happened,please tell me...","a9ea9021":"## **Score Definition**\n\nStandard to evaluate our models.","55f6c137":"# **2. Imputing missing values and Feature engineering**","814bd2c8":"To Be Continue...","fd3924f6":"**Use pd.get_dummies to One-hot encoding.**\n","453b299a":"**Notice when we Standardize our data, we first need to split our train and test datasets,because Standardize formula : z = (x - u) \/ s . it use the mean an std of our data population,if we don't split our data,our train dataset will obtain the some information about our \"unseen\" data test dataset which we use to evaluate our model performance.so we should Standardize our train dataset and test dataset separatly.**","d1814810":"\nFirst think first,let's look at the target column distribution.","0386f8ee":"**Grateful to these kernels!**\n1. [A study on Regression applied to the Ames dataset](https:\/\/www.kaggle.com\/juliencs\/a-study-on-regression-applied-to-the-ames-dataset)\n2. [Comprehensive data exploration with Python](https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python)","bd677690":"Hello kagglers!\nI'm new here,I love Data Science and Machine Learning,after I  aqcuire linear regression basic knowledges,I dive into kaggle to make my first ML model,This is my first \u201cserious\u201d kernel,House Price Pridiction is a very classic regression problem,For this kernel I only use **Linear regression\/Ridge regression\/Lasso regression\/ElasticNet regularization** (Basicly compare regularized linear model with non-regularized linear model).After this kernel I hope myself master the basic process about **EDA\/Clean data\/Feature engineering\/Model Evaluation\/Parameter optimize. **","5fb945f1":"## **Ridge regression**","4623ecf4":"## Inspecting data","3ee552d6":"There are lots of missing values in the dataset,we need to handle it.\nThe principle we doing it always follow the \"data_description.txt\".","39cf8307":"## Feature engineering","d47382d3":"## Remove outliers","52e21fe6":"## **Linear Regression without regularization**","842c54dc":"For categorical variable we fill with strings,maybe Mode of the column.","afd81cde":"**When fill the missing values:**","1ee01e81":"## **Lasso regression**","94333095":"# 3. Modeling and Model Evaluation\n","ed51e281":"Compare final score, choose Ridge regression model for submission.","b8efa237":"**Manually add some feature**\n\nsometimes we just need more features,we can't collect more feature,so we create them:\n* Adding Polynomial features,Liner Model Regression it is a polynomial curve to fit dataset,so we manually create some  polynomial features.\n* Combine features to create more feature.","bfd5e8de":"ElasticNet add L1 penalty and L2 penalty to Linear Regression cost function."}}