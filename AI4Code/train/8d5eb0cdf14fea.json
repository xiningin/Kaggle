{"cell_type":{"572671e7":"code","2d052f05":"code","3982e73f":"code","c577e113":"code","5c337b13":"code","2d00b8f2":"code","ee6c40ce":"code","4cb36d5f":"code","291ac19a":"code","7e9a7825":"code","dc4c765e":"code","cc5d23eb":"code","0890b89a":"code","9bcaace8":"code","ff7aa4cc":"code","97494319":"code","8d1c391a":"code","a79644c2":"code","b78a1b3f":"code","032f64ac":"markdown","babf1bb4":"markdown","a246a020":"markdown","2829526a":"markdown","2c65d2d7":"markdown","c468fd0c":"markdown","023ff5d1":"markdown"},"source":{"572671e7":"import numpy as np \nimport pandas as pd\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn import svm\n\n\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\n%matplotlib inline\nimport seaborn as sns","2d052f05":"data = pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")","3982e73f":"data.head()","c577e113":"class_names = {0:'Not Fraud', 1:'Fraud'}\nprint(data.Class.value_counts().rename(index = class_names))\nprint('Not Fraud', round(data['Class'].value_counts()[0]\/len(data) * 100,2), '% data')\nprint('Fraud', round(data['Class'].value_counts()[1]\/len(data) * 100,2), '% data ')","5c337b13":"colors = [\"red\", \"green\"]\n\nsns.countplot('Class', data=data , palette=colors)\nplt.title(' Not Fraud=0   Fraud=1', fontsize=11);","2d00b8f2":"fig, ax = plt.subplots(1, 2, figsize=(18,4))\namount = data['Amount'].values\ntime = data['Time'].values\nsns.distplot(amount, ax=ax[0], color='green')\nax[0].set_title('Distribution of the transaction amount', fontsize=11)\nax[0].set_xlim([min(amount), max(amount)])\nsns.distplot(time, ax=ax[1], color='red')\nax[1].set_title('Transaction time distribution', fontsize=11)\nax[1].set_xlim([min(time), max(time)])\nplt.show()","ee6c40ce":"data['Amount'] = np.log(data.Amount + 0.01)\ndata['Time'] = np.log(data.Time + 0.01)\nfig, ax = plt.subplots(1, 2, figsize=(20,10))\nsns.distplot(data[\"Amount\"],ax=ax[0],color=\"green\")\nsns.distplot(data[\"Time\"],ax=ax[1],color=\"red\")\nax[0].set_title(\"Distribution of Transaction Amount\")\nax[1].set_title(\"Distribution of Transaction Time\")","4cb36d5f":"V = data.iloc[:, 1:29].columns\n\nplt.figure(figsize=(13, 30*4))\ngs = gridspec.GridSpec(28, 1)\nfor i, j in enumerate(data[V]):\n    ax = plt.subplot(gs[i])\n    sns.distplot(data[j][data.Class==1], bins=50, label='fraud', color='r')\n    sns.distplot(data[j][data.Class==0], bins=50, label='Not', color='g')\n    ax.set_xlabel('')\n    ax.legend()\n    ax.set_title('fraud and No fraud distribution for each variable: ' + str(j))\nplt.legend()\nplt.show();","291ac19a":"f, (ax1) = plt.subplots(1, 1, figsize=(12,10))\n\nbalanced_corr = data.corr(method='spearman')\nsns.heatmap(balanced_corr, cmap='coolwarm_r', annot_kws={'size':12}, ax=ax1)\nax1.set_title('Data Correlation Matrix', fontsize=14)\nplt.show()","7e9a7825":"#Creation of our neural network\nmodel = Sequential()\n#The input layer contains 30 variables and the second one contains 64 neurons\nmodel.add(Dense(64,input_shape=(30,)))\n# The third layer contains 64 neurons\nmodel.add(Dense(64, activation='relu'))\n#The output layer contains 1 neuron corresponding to the probability to have a fraud \nmodel.add(Dense(1, activation='sigmoid'))\n#We use the Adam optimizer and cross entropy \nmodel.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0001), metrics=['accuracy'])\n\n\n#Class is the variable that we want to predict\nX = data.drop(columns=['Class'] , axis=1)\ny = data['Class']\n#Tranformation of tablles to arrays \nX = np.asarray(X)\ny = np.asarray(y)\n#Splitting data into testing and training data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)","dc4c765e":"#Training the model for 5 epochs\nmodel.fit(X_train, y_train, batch_size=128, epochs=5, verbose=1)","cc5d23eb":"model.evaluate(X_test, y_test)","0890b89a":"y_test_pred = (model.predict(X_test)> 0.5)*1  \nconfusion_matrix(y_test, y_test_pred)","9bcaace8":"sns.heatmap(confusion_matrix(y_test, y_test_pred), annot=True, cmap='Blues')\nplt.xlabel('Predicted label')\nplt.ylabel('True label')\nfigsize=(5, 4)\nplt.show()","ff7aa4cc":"#Implementation of the logistic regression model for the training data\nlogistic_model = LogisticRegression(max_iter=1000)\nlogistic_model.fit(X_train,y_train)","97494319":"#Implementation of the logistic regression model for the testing data\ny_test_pred = logistic_model.predict(X_test)  \nconfusion_matrix(y_test, y_test_pred)","8d1c391a":"#implementation of the confusion Matrix, \nfig, ax = plt.subplots(figsize=(5, 4))    \ndisp = metrics.plot_confusion_matrix(logistic_model, X_test, y_test,cmap='Blues',ax=ax)   \ndisp.figure_.suptitle(\"Confusion Matrix\") ","a79644c2":"#implementation of th SVM model for our training data\nsvm_model = svm.SVC()\nsvm_model.fit(X_train, y_train)","b78a1b3f":"fig, ax = plt.subplots(figsize=(5, 4))    \ndisp = metrics.plot_confusion_matrix(svm_model, X_test, y_test,cmap='Blues',ax=ax)   \ndisp.figure_.suptitle(\"Confusion Matrix\")","032f64ac":"# 2. Logistic Regression","babf1bb4":"# 3. SVM Support Vector Machines","a246a020":"#  Models implemented\n","2829526a":"# 1. Creation of neural network","2c65d2d7":"Introduction\n\nThis paper presents the implementation of many algorithms : Neural Networks Models, Logistic Regression, and Support Vector Machine. In fact, the objective of this project is for credit card establishments to be able to recognize fraudulent credit cards.\n\nRegarding the dataset used, it contains unknown numeric input variables which are the result of a PCA transformation for confidentiality reasons. The characteristics V1 ... V28 are the main components obtained by a PCA transformation, and \"Time\", which refers to the seconds elapsed between each transaction and the first transaction in the dataset, and \"Amount\", which refers to the amount of the transaction, are the only characteristics which have not been transformed by PCA.","c468fd0c":"# Correlation between characteristics","023ff5d1":"# **Data statistics & analysis**"}}