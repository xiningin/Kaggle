{"cell_type":{"69aa996c":"code","0c84e704":"code","abd1890f":"code","ac533155":"code","690962c1":"code","d4f7ac59":"code","ea0810b8":"code","2e06c1c8":"code","047221f0":"code","a79f7906":"code","c1a53c1e":"code","89f8b3ab":"code","efeea579":"code","5eda2767":"code","5fd19ea2":"code","2e98d428":"code","dd530407":"code","eb0391ca":"code","89eeb7bd":"code","363ee37a":"code","fd7232bf":"code","ada7ed1b":"code","274ceff0":"code","49910300":"code","9cf22b45":"code","e728f983":"code","a8af6049":"code","9fe0bb5f":"code","10ada4ad":"code","6b3f69f1":"code","d91e992d":"code","ae7601b2":"code","34ad5c1a":"code","2666e1e8":"code","f3e8808a":"code","8b4d1caf":"code","2b1a2d54":"code","ee87cbd6":"code","748150ee":"code","969ecb64":"code","4e1cfd74":"code","f6b0c037":"code","2e26fe65":"code","691333cf":"code","47e6000e":"code","5ea3f9d0":"code","6011f0a4":"code","d6c10872":"code","c134c4ab":"code","61662e19":"code","d4ad2e60":"code","00adf221":"code","293958ae":"code","7fce058a":"code","0423a97f":"code","9f3f057e":"code","8abcf994":"code","17ab6a68":"code","1cb3c2a1":"code","5b43f53f":"markdown","2fe184f1":"markdown","e2c8d8a9":"markdown","a2093fc8":"markdown","61bd1984":"markdown","61fbfedd":"markdown","d9f7be12":"markdown","1c72c2d6":"markdown","88f7e605":"markdown","e875aee6":"markdown","620c380a":"markdown","324b6220":"markdown","ade9b6e4":"markdown","d572bc37":"markdown","61ccd89c":"markdown","166349b4":"markdown","6957c155":"markdown","577b81db":"markdown","cbe802f1":"markdown","fa3f8bb9":"markdown","9e4fb4be":"markdown","70e6f39a":"markdown","6673fcc9":"markdown","bfa80b42":"markdown","bdd6506c":"markdown","2d06a894":"markdown","7b296edc":"markdown","994155a1":"markdown","e256e238":"markdown","0c871792":"markdown","d04bb32f":"markdown","f9980b84":"markdown","bfb5dd06":"markdown","f042d1b7":"markdown","56e9963c":"markdown","fd20ecac":"markdown"},"source":{"69aa996c":"!pip install logitboost","0c84e704":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import roc_auc_score, confusion_matrix, roc_curve\nimport matplotlib.pyplot as plt\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom logitboost import LogitBoost\nimport seaborn as sns\nfrom category_encoders import TargetEncoder, HashingEncoder, LeaveOneOutEncoder\nfrom sklearn.naive_bayes import ComplementNB, GaussianNB, MultinomialNB, BernoulliNB\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler\nimport string\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","abd1890f":"train = pd.read_csv('..\/input\/cat-in-the-dat\/train.csv')\ntest = pd.read_csv('..\/input\/cat-in-the-dat\/test.csv')","ac533155":"train.head()","690962c1":"train.shape","d4f7ac59":"train.isna().sum().sort_values(ascending = False)","ea0810b8":"train.describe(include='all')","2e06c1c8":"# Get list of categorical variables\ns = (train.dtypes == 'object')\nobject_cols = list(s[s].index)\n\nprint(\"Categorical variables:\")\nprint(object_cols)","047221f0":"target = train['target']","a79f7906":"replace_xor = lambda x: 'xor' if x in xor_values else x","c1a53c1e":"print(set(train['ord_4'].unique()))\nprint(set(test['ord_4'].unique()))\n","89f8b3ab":"columns_to_test = ['ord_5', 'ord_4', 'ord_3']\nfor column in columns_to_test:\n    xor_values = set(train[column].unique()) ^ set(test[column].unique())\n    if xor_values:\n        print('Column', column, 'has', len(xor_values), 'XOR values')\n        train[column] = train[column].apply(replace_xor)\n        test[column] = test[column].apply(replace_xor)\n    else:\n        print('Column', column, 'has no XOR values')","efeea579":"#train[\"ord_5a\"]=train[\"ord_5\"].str[0]\n#train[\"ord_5b\"]=train[\"ord_5\"].str[1]\n#train.drop(['ord_5'], axis=1, inplace = True)\n","5eda2767":"#test[\"ord_5a\"]=test[\"ord_5\"].str[0]\n#test[\"ord_5b\"]=test[\"ord_5\"].str[1]\n#test.drop(['ord_5'], axis=1, inplace = True)","5fd19ea2":"def date_cyc_enc(df, col, max_vals):\n    df[col + '_sin'] = np.sin(2 * np.pi * df[col]\/max_vals)\n    df[col + '_cos'] = np.cos(2 * np.pi * df[col]\/max_vals)\n    return df","2e98d428":"map_to_ascii_index = lambda x: string.ascii_letters.index(x)","dd530407":"replace_xor = lambda x: 'xor' if x in xor_values else x","eb0391ca":"train['merge_col1'] =  train[['nom_0', 'nom_1']].apply(lambda x: ''.join(x), axis=1)\ntest['merge_col1'] =  test[['nom_0', 'nom_1']].apply(lambda x: ''.join(x), axis=1)\n\ntrain['merge_col2'] =  train[['nom_1', 'nom_2']].apply(lambda x: ''.join(x), axis=1)\ntest['merge_col2'] =  test[['nom_1', 'nom_3']].apply(lambda x: ''.join(x), axis=1)\n\ntrain['merge_col3'] =  train[['nom_2', 'nom_3']].apply(lambda x: ''.join(x), axis=1)\ntest['merge_col3'] =  test[['nom_2', 'nom_3']].apply(lambda x: ''.join(x), axis=1)\n\ntrain['merge_col4'] =  train[['nom_3', 'nom_4']].apply(lambda x: ''.join(x), axis=1)\ntest['merge_col4'] =  test[['nom_3', 'nom_4']].apply(lambda x: ''.join(x), axis=1)","89eeb7bd":"# Binary encoding\ntrain['bin_3'] = [0 if x == 'F' else 1 for x in train['bin_3']]\ntrain['bin_4'] = [0 if x == 'N' else 1 for x in train['bin_4']]\n\n#Hard coded Label encoding\ntrain['ord_1'] = [0 if x == 'Novice' else 1 if x == 'Contributor' else 2 if x == 'Expert' else 3 if x == 'Master' else 4 for x in train['ord_1']]\ntrain['ord_2'] = [0 if x == 'Freezing' else 1 if x == 'Cold' else 2 if x == 'Warm' else 3 if x == 'Hot' else 4 if x == 'Boiling Hot' else 5 for x in train['ord_2']]\n\n# Label encoding via LabelEncoder class\nlabel_encoder = LabelEncoder()\ntrain['ord_3'] = label_encoder.fit_transform(train['ord_3'])\ntest['ord_3'] = label_encoder.transform(test['ord_3'])\n\ntrain['ord_4'] = label_encoder.fit_transform(train['ord_4'])\ntest['ord_4'] = label_encoder.transform(test['ord_4'])\n\ntrain['ord_5'] = label_encoder.fit_transform(train['ord_5'])\ntest['ord_5'] = label_encoder.transform(test['ord_5'])\n\n#train['ord_5b'] = label_encoder.fit_transform(train['ord_5b'])\n\n#train['ord_3'] = train['ord_3'].apply(map_to_ascii_index)\n#train['ord_4'] = train['ord_4'].apply(map_to_ascii_index)\n#train['ord_5'] = label_encoder.fit_transform(train['ord_5'])\n\n\n#train = train.drop('ord_5b', axis=1)\n\ntrain = date_cyc_enc(train, 'day', 7)\ntrain = date_cyc_enc(train, 'month', 12)\ntrain.drop(['day', 'month'], axis=1, inplace = True)\n\n#Leave one out encoding high cardinal variables\nhigh_cardinal_vars = ['nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']\n\n#trgt_encoder = TargetEncoder(cols=high_cardinal_vars, smoothing=0, return_df=True)\n#hashing_encoder = HashingEncoder(cols = high_cardinal_vars)\nloo_encoder = LeaveOneOutEncoder(cols=high_cardinal_vars)\ntrain = loo_encoder.fit_transform(train.drop(['target'], axis = 1), train['target'])\n\n# Same for test data\ntest['bin_3'] = [0 if x == 'F' else 1 for x in test['bin_3']]\ntest['bin_4'] = [0 if x == 'N' else 1 for x in test['bin_4']]\ntest['ord_1'] = [0 if x == 'Novice' else 1 if x == 'Contributor' else 2 if x == 'Expert' else 3 if x == 'Master' else 4 for x in test['ord_1']]\ntest['ord_2'] = [0 if x == 'Freezing' else 1 if x == 'Cold' else 2 if x == 'Warm' else 3 if x == 'Hot' else 4 if x == 'Boiling Hot' else 5 for x in test['ord_2']]\n\n#test = test.drop('ord_5b', axis=1)\n\n#test['ord_3'] = test['ord_3'].apply(map_to_ascii_index)\n#test['ord_4'] = test['ord_4'].apply(map_to_ascii_index)\n#test['ord_5b'] = label_encoder.fit_transform(test['ord_5b'])\n\n#for column in ['ord_3', 'ord_4', 'ord_5a']:\n#    train[column] = train[column].apply(map_to_ascii_index)\n#    test[column] = test[column].apply(map_to_ascii_index)\n\n#For cyclic data we convert it into sin and cosine values\ntest = date_cyc_enc(test, 'day', 7)\ntest = date_cyc_enc(test, 'month', 12)\ntest.drop(['day', 'month'], axis=1, inplace = True)\n\ntest = loo_encoder.transform(test)\n","363ee37a":"# One Hot encoding other nominal columns\ntrain_df = pd.get_dummies(train, drop_first=True)\n\n# Same for test data\n#test_modified = test.drop(nominal_variables, axis = 1)\ntest_df = pd.get_dummies(test, drop_first=True)","fd7232bf":"print(train_df.shape)\nprint(test_df.shape)","ada7ed1b":"cor = train_df.corr()","274ceff0":"f, ax = plt.subplots(figsize=(25, 25))\nsns.heatmap(cor, annot=False, ax=ax)","49910300":"X = train_df.drop(['id'], axis=1)\ny = target","9cf22b45":"x=y.value_counts()\nplt.bar(x.index,x)\nplt.gca().set_xticks([0,1])\nplt.title('distribution of target variable')\nplt.show()","e728f983":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, train_size=0.7, random_state=1)","a8af6049":"sm = SMOTE(kind = \"regular\")\nX_tr,y_tr = sm.fit_sample(X_train,y_train)\n","9fe0bb5f":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()","10ada4ad":"# Function for comparing different approaches\ndef score_dataset(X_train, X_test, y_train, y_test, model):\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    draw_roc(y_test, preds)\n    return roc_auc_score(y_test, preds)","6b3f69f1":"scaler = StandardScaler()\nX_tr = scaler.fit_transform(X_tr)\nX_test = scaler.transform(X_test)","d91e992d":"lr = LogisticRegression(random_state=0, solver = 'lbfgs')\nprint('AUC score with Logistic Regression :- ', score_dataset(X_tr, X_test, y_tr, y_test, lr))","ae7601b2":"rft = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0)\nprint('AUC score with RandomForest :- ', score_dataset(X_tr, X_test, y_tr, y_test, rft))","34ad5c1a":"dt = DecisionTreeClassifier(random_state=0)\nprint('AUC score with Decision Tree :- ', score_dataset(X_tr, X_test, y_tr, y_test, dt))","2666e1e8":"gaussianNB = GaussianNB(priors=None, var_smoothing=1e-09)\nprint('AUC score with gausian Naive Bayes :- ', score_dataset(X_tr, X_test, y_tr, y_test, gaussianNB))","f3e8808a":"# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparam_grid = {\n    'max_depth': range(1, 5),\n    'min_samples_leaf': range(25, 175, 50),\n    'min_samples_split': range(50, 150, 50)\n}","8b4d1caf":"# uncomment if you want to see hyper parameter tuning. Although it takes some good amount of time\n'''\n# instantiate the model\ndt = DecisionTreeClassifier()\n\n# fit tree on training data\ngrid_search_dt = GridSearchCV(estimator = dt, param_grid = param_grid, \n                          cv = n_folds, verbose = 1, n_jobs = -1, scoring=\"roc_auc\")\ngrid_search_dt.fit(X_tr, y_tr)\n'''","2b1a2d54":"# uncomment to see the results\n'''\ncv_results_dt = pd.DataFrame(grid_search_dt.cv_results_)\n# printing the optimal accuracy score and hyperparameters\nprint(\"Decison Tree grid search Accuracy : \", grid_search_dt.best_score_)\nprint(grid_search_dt.best_estimator_)\n'''","ee87cbd6":"param_grid['n_estimators']  = range(50, 200, 50)","748150ee":"# uncomment if you want to see hyper parameter tuning. Although it takes some good amount of time\n'''\n# instantiate the model\nrft = RandomForestClassifier(n_jobs= -1)\n\n# fit tree on training data\ngrid_search_rft = GridSearchCV(estimator = rft, param_grid = param_grid, \n                          cv = n_folds, verbose = 1, n_jobs = -1, scoring=\"roc_auc\")\ngrid_search_rft.fit(X_tr, y_tr)\n'''","969ecb64":"# uncomment to see the results\n'''\ncv_results_rft = pd.DataFrame(grid_search_rft.cv_results_)\n# printing the optimal accuracy score and hyperparameters\nprint(\"Random Forest grid search Accuracy : \", grid_search_rft.best_score_)\n# Best estimators\nprint(grid_search_rft.best_estimator_)\n'''","4e1cfd74":"\nlogit_param_grid = {\n    'C': [0.100, 0.150, 0.120, 0.125, 0.130, 0.135, 0.140, 0.145, 0.150]\n}\n\nlogit_grid = GridSearchCV(estimator = lr, param_grid = logit_param_grid,\n                          scoring='roc_auc', cv=5, n_jobs=-1, verbose=0)\nlogit_grid.fit(X_tr, y_tr)\n\nbest_C = logit_grid.best_params_['C']\n# best_C = C = 0.125\n\nprint('Best C:', best_C)\n","f6b0c037":"rft = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n                       max_depth=4, max_features='auto', max_leaf_nodes=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=25, min_samples_split=50,\n                       min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=-1,\n                       oob_score=False, random_state=None, verbose=0,\n                       warm_start=False)\nprint('AUC score with RandomForest :- ', score_dataset(X_tr, X_test, y_tr, y_test, rft))","2e26fe65":"dt = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n                       max_features=None, max_leaf_nodes=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=25, min_samples_split=50,\n                       min_weight_fraction_leaf=0.0, presort=False,\n                       random_state=None, splitter='best')\nprint('AUC score with Decision Tree :- ', score_dataset(X_tr, X_test, y_tr, y_test, dt))","691333cf":"lr = LogisticRegression(solver='lbfgs', random_state = 0, C=best_C)\nprint('AUC score with Losgistic Regression :- ', score_dataset(X_tr, X_test, y_tr, y_test, lr))","47e6000e":"catboost = CatBoostClassifier(iterations=20,learning_rate=1,depth=2, custom_metric=['AUC'])\nprint('AUC score with Catboost classifier :- ', score_dataset(X_tr, X_test, y_tr, y_test, catboost))","5ea3f9d0":"lboost = LogitBoost(n_estimators=200, random_state=0)\nprint('AUC score with Logitboost classifier :- ', score_dataset(X_tr, X_test, y_tr, y_test, lboost))","6011f0a4":"xgboost = XGBClassifier(random_state=0)\nprint('AUC score with Xgboost classifier :- ', score_dataset(X_tr, X_test, y_tr, y_test, xgboost))","d6c10872":"test_df.drop(['id'], axis=1, inplace = True)","c134c4ab":"test_df = scaler.transform(test_df)","61662e19":"X = scaler.transform(X)","d4ad2e60":"lr = lr.fit(X,y)\ndt = dt.fit(X,y)\nrft = rft.fit(X,y)\ncatboost = catboost.fit(X,y)\nlboost = lboost.fit(X,y)\ngaussianNB = gaussianNB.fit(X,y)","00adf221":"y_test_final_lr = lr.predict(test_df)\ny_test_final_dt = dt.predict(test_df)\ny_test_final_rft = rft.predict(test_df)\ny_test_NB = gaussianNB.predict(test_df)\ny_test_final_catboost = catboost.predict(test_df)\ny_test_final_logitboost = lboost.predict(test_df)","293958ae":"y_test_prob_lr = lr.predict_proba(test_df)[:, 1]\ny_test_prob_dt = dt.predict_proba(test_df)[:, 1]\ny_test_prob_rft = rft.predict_proba(test_df)[:, 1]\ny_test_prob_NB = gaussianNB.predict_proba(test_df)[:, 1]\ny_test_prob_catboost = catboost.predict_proba(test_df)[:, 1]\ny_test_prob_logitboost = lboost.predict_proba(test_df)[:, 1]","7fce058a":"submission = pd.DataFrame({\n        \"id\": test[\"id\"],\n        \"target\": y_test_prob_lr\n    })\nsubmission.to_csv('LogisticRegression.csv',header=True, index=False)","0423a97f":"submission = pd.DataFrame({\n        \"id\": test[\"id\"],\n        \"target\": y_test_prob_dt\n    })\nsubmission.to_csv('DecisonTree.csv',header=True, index=False)","9f3f057e":"submission = pd.DataFrame({\n        \"id\": test[\"id\"],\n        \"target\": y_test_prob_rft\n    })\nsubmission.to_csv('RandomForest.csv',header=True, index=False)","8abcf994":"submission = pd.DataFrame({\n        \"id\": test[\"id\"],\n        \"target\": y_test_prob_NB\n    })\nsubmission.to_csv('GaussianNB.csv',header=True, index=False)","17ab6a68":"submission = pd.DataFrame({\n        \"id\": test[\"id\"],\n        \"target\": y_test_prob_catboost\n    })\nsubmission.to_csv('Catboost.csv',header=True, index=False)","1cb3c2a1":"submission = pd.DataFrame({\n        \"id\": test[\"id\"],\n        \"target\": y_test_prob_logitboost\n    })\nsubmission.to_csv('Logitboost.csv',header=True, index=False)","5b43f53f":"## Logitboost","2fe184f1":"# Boosting algorithms","e2c8d8a9":"Lets scale test data first","a2093fc8":"# Encoding data","61bd1984":"Use the best submission file to submit your score. any more suggestions welcome. Will still try to imporve this kernel","61fbfedd":"# Encoding techniques\n\nTaken reference from https:\/\/www.kaggle.com\/discdiver\/category-encoders-examples\n\n1. bin_3, bin_4 :- Convert Y\/N and T\/F to 1\/0\n2. nom_0 -  nom_4 :-  Encode using One hot encoding\n3. nom_5 - nom_9 :- Target encode them as they are high cardinal variables\n4. ord_1, ord_2 :- Convert into numerical order using hard coded values as Label encoder might not be able to understand the order\n5. ord_3 - ord_4 :-  Encode using ascii as they are alphabetical values\n6. ord_5 :- Separate two alphabets and then do label encoding\n7. day, month:- Encode using sin and cosine values as they are cyclic in nature\n","d9f7be12":"## Lets predict for test data","1c72c2d6":"## Catboost","88f7e605":"# Analysing imbalanace in dataset","e875aee6":"We will be using all these models to calculate scores","620c380a":"# Logistic Regression","324b6220":"Before starting with encoding lets create some other features","ade9b6e4":"Lets see how the train dataset looks like","d572bc37":"## Decison Tree","61ccd89c":"Lets identify the uncommon columns between test and train data. Replace uncommon columns with a common value","166349b4":"Lets fit for entire training set before making predictions. But before that we have to scale entire train data also","6957c155":"Lets save target variable somewhere","577b81db":"# Model Refinement","cbe802f1":"We can clearly see that correlation between data points is quite less here. So lets keep all these features and move ahead with our classification\n","fa3f8bb9":"Lets try this with a boosting algorithm also . We will use **CatBoostClassifier** . First lets try with Vanilla model version","9e4fb4be":"Clearly there is imbalance in dataset. We need to cater this implance using SMOTE technique","70e6f39a":"So we observed somethings\n\n1. There are no null values in train dataset\n2. There are multiple categorical variables which are as follows\n    1. bin_3, bin_4 :-  binary cols\n    2. nom_0 -  nom_4 :-  nominal columns ( with no order)\n    3. nom_5 - nom_9 :- nominal columns with high cardinality\n    4. ord_1 - ord_5 :-  Ordered columns\n    \nWe have to use different ways to treat these columns and convert them into numerical data","6673fcc9":"## Xgboost","bfa80b42":"Lets plot roc curver and calculate score for all models","bdd6506c":"Now lets do the necessary train_test_split","2d06a894":"# Scaling data","7b296edc":"## Random Forest","994155a1":"Lets load the datasets first","e256e238":"# Analysing Categorical variables","0c871792":"Lets cater the imbabalnce in dataset","d04bb32f":"# Testing with vanilla version of models","f9980b84":"## Model fitting with tuned hyper parameters","bfb5dd06":"**Problem Statement:-**\n\nA common task in machine learning pipelines is encoding categorical variables for a given algorithm in a format that allows as much useful signal as possible to be captured.\n\nWe have to handle different types of categorical data columns using multiple techniques in order to get best results.\n\n![Lets Categorize](https:\/\/media3.giphy.com\/media\/WYEWpk4lRPDq0\/giphy.gif)\nLets begin.\n\nTypes of categorical data given to us \n- binary features\n- low- and high-cardinality nominal features\n- low- and high-cardinality ordinal features\n- (potentially) cyclical features","f042d1b7":"Now we have got our tuned Random Forest and Decison Tree. Lets take them and fit with our data","56e9963c":"# Importing libraries","fd20ecac":"Lets create a function to test our dataset and calculate ROC-AUC score for multiple models as param"}}