{"cell_type":{"35d79b08":"code","2bd068bb":"code","f043e5ad":"code","da02577b":"code","b77ac934":"code","d5c2a029":"code","a21ec209":"code","fbfec9bd":"code","f6e6b8ff":"code","0ab9d7fc":"code","8b7371e1":"code","363fd6d2":"code","cf888370":"code","855a8168":"code","d7c47773":"markdown","71f48411":"markdown","1fe928db":"markdown","3ee6fc57":"markdown","07433a30":"markdown","9cccec53":"markdown","2f6a79ed":"markdown","a2319d13":"markdown","afa4b261":"markdown","cf9d34f0":"markdown","ad2579a6":"markdown","5d75a769":"markdown","ea95a480":"markdown","e95afd2a":"markdown","7c53bf7e":"markdown","8360a0c9":"markdown","86fb976b":"markdown"},"source":{"35d79b08":"import numpy as np # Linear algebra\nimport pandas as pd # For data manipulation\nimport json\nimport os\nimport matplotlib.pyplot as plt # For visualization\nfrom sklearn.neighbors import KNeighborsClassifier # For modelling\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, StratifiedKFold # For evaluation and hyperparameter tuning\nfrom sklearn.metrics import confusion_matrix, classification_report # For evaluation\nfrom scipy.ndimage import shift, rotate, zoom # For data augmentation\nfrom IPython.display import FileLink # For downloading the output file\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","2bd068bb":"train_df = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/test.csv\")\nsubmission_df = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/sample_submission.csv\")","f043e5ad":"train_df.info()","da02577b":"test_df.info()","b77ac934":"X_train = train_df.iloc[:, 1:].values\ny_train = train_df.iloc[:, 0].values\nX_test = test_df.values\n\nprint(f\"X_train shape: {X_train.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")\nprint(f\"X_test shape: {X_test.shape}\")","d5c2a029":"some_digit = X_train[40]\n\nsome_digit_image = some_digit.reshape(28, 28)\nprint(f\"Label: {y_train[40]}\")\nplt.imshow(some_digit_image, cmap=\"binary\")\nplt.show()","a21ec209":"stratified_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor fold, indices in enumerate(stratified_fold.split(X_train, y_train)):\n    # Creating datasets for training and testing the model \n    X_train_, y_train_ = X_train[indices[0]], y_train[indices[0]]\n    X_test_, y_test_ = X_train[indices[1]], y_train[indices[1]]\n    \n    estimator = KNeighborsClassifier()\n    estimator.fit(X_train_, y_train_)\n    predictions = estimator.predict(X_test_)\n    \n    print(f\"Classification report for Fold {fold + 1}:\")\n    print(classification_report(y_test_, predictions, digits=3), end=\"\\n\\n\")\n    \n    print(f\"Confusion Matrix for Fold {fold + 1}:\")\n    print(confusion_matrix(y_test_, predictions), end=\"\\n\\n\")\n    \n    del X_train_\n    del X_test_\n    del y_train_\n    del y_test_","fbfec9bd":"grid_params = {\n    \"weights\": ['uniform', 'distance'],\n    \"n_neighbors\": [3, 4, 5, 6, 8, 10]\n}\n\nestimator = KNeighborsClassifier()\ngrid_estimator = GridSearchCV(estimator, # Base estimator\n                              grid_params, # Parameters to tune\n                              cv=stratified_fold, # cross-validation stratergy\n                              verbose=2, # Verbosity of the logs\n                              n_jobs=-1) # Number of jobs to be run concurrently with -1 meaning all the processors\n\n\"\"\"\nCommenting this block to reduce the notebook execution time\n\"\"\"\n\n# Fitting the estimator with training data\n# grid_estimator.fit(X_train, y_train)\n\n# print(f\"Best Score: {grid_estimator.best_score_}\", end=\"\\n\\n\")\n# print(f\"Best Parameters: \\n{json.dumps(grid_estimator.best_params_, indent=4)}\",\n#       end=\"\\n\\n\")\n# print(\"Grid Search CV results:\")\n# results_df = pd.DataFrame(grid_estimator.cv_results_)\n# results_df","f6e6b8ff":"estimator = KNeighborsClassifier(n_neighbors=4, weights='distance')\nestimator.fit(X_train, y_train)\npredictions = estimator.predict(X_test)","0ab9d7fc":"def shift_in_one_direction(image, direction):\n    \"\"\"\n    Shifts an image by one pixel in the specified direction\n    \"\"\"\n    if direction == \"DOWN\":\n        image = shift(image, [1, 0])\n    elif direction == \"UP\":\n        image = shift(image, [-1, 0])\n    elif direction == \"LEFT\":\n        image = shift(image, [0, -1])\n    else:\n        image = shift(image, [0, 1])\n\n    return image\n\n\ndef shift_in_all_directions(image):\n    \"\"\"\n    Shifts an image in all the directions by one pixel\n    \"\"\"\n    reshaped_image = image.reshape(28, 28)\n\n    down_shifted_image = shift_in_one_direction(reshaped_image, \"DOWN\")\n    up_shifted_image = shift_in_one_direction(reshaped_image, \"UP\")\n    left_shifted_image = shift_in_one_direction(reshaped_image, \"LEFT\")\n    right_shifted_image = shift_in_one_direction(reshaped_image, \"RIGHT\")\n\n    return (down_shifted_image, up_shifted_image,\n            left_shifted_image, right_shifted_image)\n\n\ndef rotate_in_all_directions(image, angle):\n    \"\"\"\n    Rotates an image clockwise and anti-clockwise\n    \"\"\"\n    reshaped_image = image.reshape(28, 28)\n    \n    rotated_images = (rotate(reshaped_image, angle, reshape=False),\n                      rotate(reshaped_image, -angle, reshape=False))\n    \n    return rotated_images\n\n\ndef clipped_zoom(image, zoom_ranges):\n    \"\"\"\n    Clips and zooms an image at the specified zooming ranges\n    \"\"\"\n    reshaped_image = image.reshape(28, 28)\n    \n    h, w = reshaped_image.shape\n    \n    zoomed_images = []\n    for zoom_range in zoom_ranges:\n        zh = int(np.round(h \/ zoom_range))\n        zw = int(np.round(w \/ zoom_range))\n        top = (h - zh) \/\/ 2\n        left = (w - zw) \/\/ 2\n        \n        zoomed_images.append(zoom(reshaped_image[top:top+zh, left:left+zw],\n                                  zoom_range))\n    \n    return zoomed_images\n\ndef alter_image(image):\n    \"\"\"\n    Alters an image by shifting, rotating, and zooming it\n    \"\"\"\n    shifted_images = shift_in_all_directions(image)\n    rotated_images = rotate_in_all_directions(image, 10)\n    zoomed_images = clipped_zoom(image, [1.1, 1.2])\n            \n    return np.r_[shifted_images, rotated_images, zoomed_images]\n\nX_train_add = np.apply_along_axis(alter_image, 1, X_train).reshape(-1, 784)\ny_train_add = np.repeat(y_train, 8)\n\nprint(f\"X_train_add shape: {X_train_add.shape}\")\nprint(f\"y_train_add shape: {y_train_add.shape}\")","8b7371e1":"X_train_combined = np.r_[X_train, X_train_add]\ny_train_combined = np.r_[y_train, y_train_add]\n\ndel X_train\ndel X_train_add\ndel y_train\ndel y_train_add\n\nprint(f\"X_train_combined shape: {X_train_combined.shape}\")\nprint(f\"y_train_combined shape: {y_train_combined.shape}\")","363fd6d2":"cdata_estimator = KNeighborsClassifier(n_neighbors=4, weights='distance')\ncdata_estimator.fit(X_train_combined, y_train_combined)\ncdata_estimator_predictions = cdata_estimator.predict(X_test)","cf888370":"submission_df[\"Label\"] = predictions\nsubmission_df.to_csv('submission.csv', index=False)\nFileLink('submission.csv')","855a8168":"submission_df[\"Label\"] = cdata_estimator_predictions\nsubmission_df.to_csv('cdata_submission.csv', index=False)\nFileLink('cdata_submission.csv')","d7c47773":"**Peeking the data**","71f48411":"Combining the synthesized data with the actual training data","1fe928db":"Fitting a new model with the tuned hyperparameters to the combined dataset","3ee6fc57":"Fitting a new model with the found hyperparameter values to the training data and making predictions on the test data","07433a30":"Visualizing a digit from the training data as a 28 X 28 image","9cccec53":"Fine-tuning the model by finding the best values for the hyperparameters (weights, n_neighbors) using GridSearchCV","2f6a79ed":"**Data Augmentation**","a2319d13":"Knowing about the features in the datasets","afa4b261":"## Digit Recognizer\n\nAlthough this is a computer vision problem, I created a simple model using **K-Nearest Neighbors** algorithm in this notebook to be a good starting point knowing that CNN would be a much better option. I used the **GridSearchCV** to fine tune the hyperparameters such as *\"n_neighbors\", and \"weights\"* and to perform cross-validation. Furthermore, I have used **Data Augmentation** or **Artificial Data Synthesis** technique in this notebook to boost the model's performance on the test set.\n\nPlease **upvote** if you like this notebook and share your valuable feedback.\n\nYou can find my other notebooks below:\n\n* [Disaster Tweets Classification](https:\/\/www.kaggle.com\/gauthampughazh\/disaster-or-not-plotly-use-tfidf-h2o-ai-automl)\n* [House Sales Price Prediction](https:\/\/www.kaggle.com\/gauthampughazh\/house-sales-price-prediction-svr)\n* [Titanic Survival Classification](https:\/\/www.kaggle.com\/gauthampughazh\/titanic-survival-prediction-pandas-plotly-keras)","cf9d34f0":"Using **StratifiedKFold** to ensure that the test data represents samples from all classes (digits) and for cross-validating the model. Using the classification report and confusion matrix to understand the model's performance on each fold.","ad2579a6":"Each image in the training set is \n\n* shifted down, up, left and right by one pixel\n* rotated clockwise and anti-clockwise \n* clipped and zoomed at two different ranges\n\ngenerating eight different images. The image is clipped before zooming to preserve the image size.","5d75a769":"Generating the submission file","ea95a480":"**Note:** With **Data Augmentation** the accuracy jumped from 97.185% to 98.128% on the test data.","e95afd2a":"**Best parameter values found:** {n_neighbors: 4, weights: 'distance'}","7c53bf7e":"Loading the datasets into dataframes","8360a0c9":"**Model Selection**","86fb976b":"Converting the train and test dataframes into numpy arrays"}}