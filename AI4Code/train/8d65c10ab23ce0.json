{"cell_type":{"7c08b702":"code","c2d136dd":"code","e4e6d96a":"code","57233f4c":"code","243c30f5":"code","5325ffeb":"code","1eb26ab5":"code","eb748429":"code","5f1dd52a":"code","31728911":"code","54fe72d1":"code","c32722d9":"code","c2aaf024":"code","7b82696f":"code","6c83fc6c":"code","7a9dc84f":"code","83005e85":"code","8d0dc095":"code","d72a825f":"code","f7719e94":"code","383c6893":"code","973eb77d":"code","2f1fd847":"code","502a8917":"code","71c6c8d5":"code","3de92865":"code","f363a960":"code","a1bb94c2":"code","718f5c56":"code","e47ea106":"code","ae1e40e6":"code","cffee5b0":"code","9089481f":"code","095408d5":"code","5e041e67":"code","1ba923cb":"code","1676aca7":"code","1b612615":"code","c0d90304":"code","de1e5661":"code","0ca953b7":"code","6caf7fda":"code","8aa31b0e":"code","dff19b8e":"code","18774223":"code","51e94cc7":"code","406fd162":"code","5aafad17":"code","3625f1ff":"code","62b44f80":"code","fb375f86":"code","8331f773":"code","593dd32a":"code","2f9cca44":"code","e830bf1c":"code","e22f5c67":"code","0121d76d":"code","2dcc4e72":"code","4d5e21fa":"code","9cc703df":"code","997cbfd4":"markdown","cf450967":"markdown","ad076864":"markdown","850fdf5b":"markdown","8d097237":"markdown","f99705c0":"markdown","cd1361b5":"markdown","1f6f5c30":"markdown","bfca197c":"markdown","5fa6fbe8":"markdown","5a92a1ad":"markdown","13a79c0c":"markdown","7346dee4":"markdown","9534c21b":"markdown","3d03eb87":"markdown","381a0935":"markdown","a5563248":"markdown","dd09253d":"markdown"},"source":{"7c08b702":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c2d136dd":"from nltk.corpus import stopwords\nstop = stopwords.words('english')\nimport re\nfrom keras import Sequential, Model\nfrom keras.layers import Embedding, GlobalAveragePooling1D, Dense,InputLayer\nfrom keras.layers import concatenate,BatchNormalization\nfrom tensorflow.keras import regularizers\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import classification_report","e4e6d96a":"df=pd.read_csv('\/kaggle\/input\/deepn\/Train_Data.csv')","57233f4c":"df_test=pd.read_csv('\/kaggle\/input\/deepn\/Test_Data.csv')","243c30f5":"df","5325ffeb":"df_test","1eb26ab5":"df.info()","eb748429":"df.iloc[:,-3].unique()","5f1dd52a":"df[df.iloc[:,-3]=='en']","31728911":"df.head()","54fe72d1":"def data_clean(df):\n    for ind,row in df.iterrows():\n        while((row['text'].split(\" \")[-1]=='&amp') | (row['text'].split(\" \")[-1]=='&gt')):\n            flag=1\n            row.iloc[1]+=row.iloc[2]\n            for i in range(2,len(df.columns)-1):\n                row.iloc[i]=row.iloc[i+1]\n  \n        df.iloc[ind]=row\n    df['text']=df['text'].replace(r'&amp','',regex=True).replace(r'http\\S+', '',regex=True)\n    df['text']=df['text'].replace(r'&gt','',regex=True)\n    \n    df['screen_and_username']=df['screen_and_username'].apply(lambda x: re.sub(r'\\\/(.+)',\"\",x))\n   \n    return df","c32722d9":"df=data_clean(df)","c2aaf024":"df.tweet_language.unique()\ndf.type_of_tweet.value_counts()","7b82696f":" df.drop(columns=['type_of_tweet','tweet_language'])","6c83fc6c":"df","7a9dc84f":"df=df[(df['rumour_identification']=='0') | (df['rumour_identification']=='1')]","83005e85":"df_test=data_clean(df_test)","8d0dc095":"df","d72a825f":"df.loc[569,'text']","f7719e94":"df","383c6893":"data=df\ndata_test=df_test","973eb77d":"# data[['type_of_tweet','num_followers','num_friends','num_retweets','rumour_identification']]=df[['type_of_tweet','num_followers','num_friends','num_retweets','rumour_identification']].apply(pd.to_numeric,errors='coerce')\n# data_test[['type_of_tweet','num_followers','num_friends','num_retweets']]=df_test[['type_of_tweet','num_followers','num_friends','num_retweets']].apply(pd.to_numeric,errors='coerce')","2f1fd847":"data=data.iloc[:,:10]\ndata_test=data_test.iloc[:,:10]","502a8917":"data_test.info()","71c6c8d5":"data","3de92865":"def remove_non_alpha(data):\n    data['text']=data['text'].apply(lambda x: re.sub(r\"[^a-zA-z# ]+\",\"\",x.lower())) #using regex to select only alphabetical char.\n    data['text_stop_rm'] = data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)])) #removing stop words from text field\n    return data","f363a960":"data=remove_non_alpha(data)\ndata_test=remove_non_alpha(data_test)","a1bb94c2":"data","718f5c56":"##creating list for creating mapping\nheaders=data.header.value_counts().index.tolist()\nusername=data.screen_and_username.value_counts().index.tolist()\nheaders_test=data_test.header.value_counts().index.tolist()\nusername_test=data_test.screen_and_username.value_counts().index.tolist()","e47ea106":"#creating dict for mapping\nusername_dict = {}\ni = 0\nfor word in username:\n    if word not in username_dict:\n        username_dict[word] = i\n        i += 1\nprint(username_dict)\nheader_dict = {}\ni = 0\nfor word in headers:\n    if word not in header_dict:\n        header_dict[word] = i\n        i += 1\nprint(header_dict)\n\nusername_dict_t = {}\ni = 0\nfor word in username_test:\n    if word not in username_dict_t:\n        username_dict_t[word] = i\n        i += 1\n\nheader_dict_t = {}\ni = 0\nfor word in headers_test:\n    if word not in header_dict_t:\n        header_dict_t[word] = i\n        i += 1\n","ae1e40e6":"#func to mapping the col with dict\ndef mapping(data,header_dict,username_dict):\n    data['header']=data['header'].map(header_dict)\n    data['screen_and_username']=data['screen_and_username'].map(username_dict)\n    return data","cffee5b0":"data=mapping(data,header_dict,username_dict)\ndata_test=mapping(data_test,header_dict_t,username_dict_t)","9089481f":"data_train=data.copy()\ndata_testt=data_test.copy()","095408d5":"data[['type_of_tweet','num_followers','num_friends','num_retweets','rumour_identification']]=data[['type_of_tweet','num_followers','num_friends','num_retweets','rumour_identification']].apply(pd.to_numeric,errors='coerce')\ndata_test[['type_of_tweet','num_followers','num_friends','num_retweets']]=data_test[['type_of_tweet','num_followers','num_friends','num_retweets']].apply(pd.to_numeric,errors='coerce')","5e041e67":"data_testt.info()","1ba923cb":"data.isnull().values.any()","1676aca7":"data.screen_and_username.value_counts().keys()","1b612615":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences","c0d90304":"tokenizer = Tokenizer(num_words=1000)\ntokenizer.fit_on_texts(data['text_stop_rm'])","de1e5661":"X_train = tokenizer.texts_to_sequences(data['text_stop_rm'])\nX_test=tokenizer.texts_to_sequences(data_test['text_stop_rm'])\nvocab_size = len(tokenizer.word_index) + 1","0ca953b7":"data.rumour_identification.value_counts()","6caf7fda":"vocab_size","8aa31b0e":"#padding the tokens of array for text field s.t. they could be of same dimension before training.\nX_train = pad_sequences(X_train, padding='post', maxlen=vocab_size)\nX_test = pad_sequences(X_test, padding='post', maxlen=vocab_size)","dff19b8e":"X_train_r=data[['header','screen_and_username','num_followers','num_friends','num_retweets']]","18774223":"X_test_r=data_test[['header','screen_and_username','num_followers','num_friends','num_retweets']]","51e94cc7":"X_train_r['screen_and_username']\/=data['screen_and_username'].max()\nX_train_r['num_followers']\/=data['num_followers'].max()\nX_train_r['num_friends']\/=data['num_friends'].max()\nX_train_r['num_retweets']\/=data['num_retweets'].max()","406fd162":"X_test_r['screen_and_username']\/=data_test['screen_and_username'].max()\nX_test_r['num_followers']\/=data_test['num_followers'].max()\nX_test_r['num_friends']\/=data_test['num_friends'].max()\nX_test_r['num_retweets']\/=data_test['num_retweets'].max()","5aafad17":"X_train_r","3625f1ff":"left_branch = Sequential()          \nleft_branch.add(InputLayer(input_shape=(6441)))  ##layer for text field\nleft_branch.add(Dense(5500, activation='relu'))\nleft_branch.add(BatchNormalization())\nleft_branch.add(Dense(1500, activation='relu',activity_regularizer=regularizers.l2(0.01)))\nleft_branch.add(Dense(500, activation='relu'))\nleft_branch.add(Dense(50, activation='tanh'))\n\nright_branch = Sequential()\nright_branch.add(InputLayer(input_shape=(5)))      ## layers for other attributes\nright_branch.add(Dense(50, activation='tanh'))\nmerged=concatenate([left_branch.output, right_branch.output],axis=-1)   ##merging both the layers \nmerged=Dense(10, activation='relu')(merged)\nmerged=Dense(1, activation='sigmoid')(merged)\nmodel = Model(inputs=[left_branch.input,right_branch.input], outputs=merged)\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n","62b44f80":"model.summary()","fb375f86":"model.fit([X_train, X_train_r],data['rumour_identification'], batch_size=32, epochs=100,validation_split=0.25,steps_per_epoch=25)","8331f773":"te=model.predict([X_test,X_test_r])","593dd32a":"cols=[\"Identity\",'header','num_followers', 'num_friends', 'num_retweets']","2f9cca44":"X_train, X_test, y_train, y_test = train_test_split( data_train[cols], data_train[\"rumour_identification\"], random_state=42)\nRF=RandomForestClassifier()\nRF.fit(X_train,y_train)\ny_pred=RF.predict(X_test)\nprint(classification_report(y_test,y_pred))","e830bf1c":"RF.fit(data_train[cols], data_train[\"rumour_identification\"])","e22f5c67":"data_testt[cols].info()","0121d76d":"data_testt['num_followers'] = pd.to_numeric(data_testt['num_followers'], errors='coerce').fillna(0)\ndata_testt['num_friends'] = pd.to_numeric(data_testt['num_friends'], errors='coerce').fillna(0)\ndata_testt['num_retweets'] = pd.to_numeric(data_testt['num_retweets'], errors='coerce').fillna(0)","2dcc4e72":"pred=RF.predict(data_testt[cols])","4d5e21fa":"final=pd.DataFrame({\"Identity\":data_testt[\"Identity\"].values,\"rumour_identification\":np.array(pred)})","9cc703df":"final.to_csv(\"final.csv\",index=False)","997cbfd4":"getting only classification value (Y) for training.","cf450967":"# creating model for training","ad076864":"As the number of params for text field is 6441 ,i can train them in one model and other attributes in other model and then merge them for final classification.","850fdf5b":"We'll first map the userhandles and header col.","8d097237":"`tweet_language` and `type_of_tweet` contains redundant values which does not contribute in leaning the model so we can drop them.","f99705c0":"Those are some random value i.e. some string type and some int type that doesn't clearly define what it represents","cd1361b5":"Defining data_clean function for shifting back the col to its proper position and then removing symbol `&amp`, `&gt` and hyperlink also because most people scroll down sites and only see the info in the heading of tweet. Also extracting the handle name of the users.","1f6f5c30":"copying dataframe to new variable so that further changes could be altered again if anything wents wrong.","bfca197c":"'type_of_tweet','num_followers','num_friends','num_retweets','rumour_identification' are object col(string type) but we need to include them for training model so we need to convert them to either float or int type.\nAlso dropping unnamed col.","5fa6fbe8":"# Tokenize\nWe need to tokenize values because we need to performa mathematical calculation like forward propagation(W.X_t+b) then backward progation(dl\/dw,dl\/dz,dl\/dA) for updating weights to train our model,but the text and user name are in string so we'll map them to numbers and can proceed further.\nBut as we can see from that their are some non alphabetical char like emoji,punctuation etc. so we need to remove them before tokenizing them for mapping.","5a92a1ad":"There are some unnamed column ,those contain some value so we cannot directly delete them","13a79c0c":"Random classifier give best score ,so training model on whole data now and predicting class for submission","7346dee4":"Create 2 model : one for text field and another for remaining cols and then finally merging them .","9534c21b":"Normalizing other attributes such that they could converge fast.","3d03eb87":"Their could be lot of words in total text so i am using Tokenizer to create token and map the words for text field.","381a0935":"# Data cleaning\nFrom Above we can see that value for those rows are shifted by two col and this is same for unnamed col 10 and 11 ,so we need to put them at their right col for further process.\nIn order to proceed ,for some row we need to shift them by 1 col ,some by 2 and by3,so if one observe closely their are some value at the end to `text` and `header` col , this could be a symbol that represents that some string has been truncated and shift to next col ,so we can check wheather last value of text is *&amp* or *&gt* ,which shows string has beed truncated and shifted to next col.  ","a5563248":"The accuracy from above model does not comes to be fine ,so lets go with ML libs . I am using Randclassifier.","dd09253d":"`screen_and_username` contains handle and user name both , we can also tag or recognize user with their handle name."}}