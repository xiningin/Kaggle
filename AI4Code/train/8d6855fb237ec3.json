{"cell_type":{"b5db8771":"code","d6539b9b":"code","281f168e":"code","252a68d5":"code","5daf8801":"code","6e3eec47":"code","2349c5ac":"code","0a87706f":"code","87b980ae":"code","f1484d84":"code","87a599cc":"code","ed7563a3":"code","48484d3d":"code","8e9ab73a":"code","449433a5":"code","cf605681":"code","593188e5":"code","92174918":"code","b4b22c7d":"code","d8153139":"code","7bdfe135":"code","40c389d3":"code","b7eb952f":"code","3c262c1d":"code","c456494c":"code","35f1869e":"code","1ba87ad3":"code","52bd2b74":"code","9d58ac80":"code","e5b443a3":"code","4841fed3":"code","bcbeb5be":"code","9ef34498":"code","c21f32b9":"code","4b00d5ea":"code","337c8938":"code","d68cae1f":"code","f394aa51":"code","6c8ed73d":"code","8001acf5":"code","288c76bd":"code","37973ce8":"code","b316f625":"code","61ae712b":"code","ca9bf2ee":"code","9b88e119":"code","3ac5a0d0":"code","d69e52ae":"code","6d6fe58e":"code","f4e9abad":"code","e786bb72":"code","e52ff65a":"code","34ff5f0c":"code","52e4684e":"code","c10f1d41":"code","aa7e818c":"code","de3cf19a":"code","b4dbdbce":"code","fdfc76da":"code","3916fc02":"code","e5b941d7":"code","bda3b5b8":"code","434d8641":"code","e4eac3c8":"code","1493a7a2":"code","3ab9225a":"code","21beaf6f":"code","e91f9311":"code","3504dcae":"code","e2829959":"code","f3b5faad":"code","b2b756dd":"code","e9db29c2":"code","4a0f3092":"code","eee3f018":"code","52a9e05f":"code","10059cc2":"code","f73ed538":"code","a40c8d5c":"code","3d321dac":"code","2d1c07fd":"code","5da7562a":"code","c59434af":"code","1e0f50ca":"code","b715cb55":"code","bb3cf945":"code","5f242b9e":"code","6388ae5b":"code","c551d108":"code","8bec415b":"code","847c60a4":"code","99b152a2":"code","946cf56b":"code","7a3301db":"code","02de0759":"code","4e9a41f8":"markdown","3c64fbe5":"markdown","36bf2a5d":"markdown","a7f5b5dc":"markdown","74043217":"markdown","5da0242b":"markdown","7b81c935":"markdown","e8ed6011":"markdown","719e1773":"markdown","3025d8bc":"markdown","7920f656":"markdown","8b3c0bde":"markdown","ff2c2435":"markdown","d31bc81d":"markdown","72bf4774":"markdown","8ef48b29":"markdown","0db52a13":"markdown","df8502e2":"markdown","bbcac608":"markdown","f54b7f26":"markdown","8ad87d7c":"markdown","2a3982f2":"markdown","7882a67e":"markdown","3672d68b":"markdown","48e0167f":"markdown","7c2fc03e":"markdown","3686d08d":"markdown","9bd0f305":"markdown","277314e4":"markdown","165ed77b":"markdown","1f156a69":"markdown","acaad249":"markdown","09eba4f1":"markdown","448a01e5":"markdown","3d69273c":"markdown","e5e8aec5":"markdown","1e1266ec":"markdown","227c648f":"markdown","8f7fcd4c":"markdown","d96fb3ee":"markdown","e4336037":"markdown","e2fe6717":"markdown","39a1c96e":"markdown","ad278cad":"markdown","317df8df":"markdown","7638f9c7":"markdown","20a1e7dc":"markdown","70feee14":"markdown","f4b92737":"markdown","2c138b10":"markdown","625c1ce7":"markdown","2a0d60bc":"markdown","64702f5c":"markdown","4bcf0ad5":"markdown","b7bd07d1":"markdown","b22954eb":"markdown","ccc476e6":"markdown","e7aa5a31":"markdown","2eba2867":"markdown","3be573b6":"markdown","95b685e7":"markdown","92c380e3":"markdown","2a0ca80d":"markdown","40bcdf1b":"markdown","6efe1cd4":"markdown","2156aa09":"markdown","46bf3cfa":"markdown","69d40875":"markdown","795c8db4":"markdown","b4315640":"markdown","0b956807":"markdown","dfe77c5e":"markdown","7861b666":"markdown","28e6f76b":"markdown","04ed3b62":"markdown","50c07880":"markdown","dd09f420":"markdown","ed96cf53":"markdown","7013fd5c":"markdown","5c1f804a":"markdown","44eb0098":"markdown","7931342d":"markdown","e3407941":"markdown","43ea33fa":"markdown","40d65a47":"markdown","66eb6253":"markdown","1b0c1edb":"markdown","364ceb57":"markdown","989fbba6":"markdown","5613bcb0":"markdown","620c52eb":"markdown","3a81f8df":"markdown","4112fd05":"markdown","eb41ad2d":"markdown","77d14ffb":"markdown","5d075c9c":"markdown","949334de":"markdown","ad4e3520":"markdown","ba82977d":"markdown","3271a97c":"markdown","9eaea01d":"markdown","cf5746bb":"markdown","2f2ce69a":"markdown","a2743123":"markdown"},"source":{"b5db8771":"import seaborn as sns\nfrom matplotlib import pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n%config InlineBackend.figure_format = 'retina'\nsns.set(style=\"ticks\")\nplt.rc('figure', figsize=(6, 3.7), dpi=100)\nplt.rc('axes', labelpad=20, facecolor=\"#ffffff\", \n       linewidth=0.4, grid=True, labelsize=10)\nplt.rc('patch', linewidth=0)\nplt.rc('xtick.major', width=0.2)\nplt.rc('ytick.major', width=0.2)\nplt.rc('grid', color='#EEEEEE', linewidth=0.25)\nplt.rc('font', family='Arial', weight='400', size=10)\nplt.rc('text', color='#282828')\nplt.rc('xtick', labelsize=10)\nplt.rc('ytick', labelsize=10)\nplt.rc('savefig', pad_inches=0.3, dpi=300)","d6539b9b":"import pandas as pd\nimport numpy as np\n\ndataset = pd.read_csv(\"..\/input\/AmesHousing.csv\")","281f168e":"# Configuring float numbers format\npd.options.display.float_format = '{:20.2f}'.format\ndataset.head(n=5)","252a68d5":"dataset.describe(include=[np.number], percentiles=[.5]) \\\n    .transpose().drop(\"count\", axis=1)","5daf8801":"dataset.describe(include=[np.object]).transpose() \\\n    .drop(\"count\", axis=1)","6e3eec47":"# Getting the number of missing values in each column\nnum_missing = dataset.isna().sum()\n# Excluding columns that contains 0 missing values\nnum_missing = num_missing[num_missing > 0]\n# Getting the percentages of missing values\npercent_missing = num_missing * 100 \/ dataset.shape[0]\n# Concatenating the number and perecentage of missing values \n# into one dataframe and sorting it\npd.concat([num_missing, percent_missing], axis=1, \n          keys=['Missing Values', 'Percentage']).\\\n          sort_values(by=\"Missing Values\", ascending=False)","2349c5ac":"dataset[\"Pool Area\"].value_counts()","0a87706f":"dataset[\"Pool QC\"].fillna(\"No Pool\", inplace=True)","87b980ae":"dataset[\"Misc Val\"].value_counts()","f1484d84":"dataset['Misc Feature'].fillna('No feature', inplace=True)","87a599cc":"dataset['Alley'].fillna('No Alley', inplace=True)\ndataset['Fence'].fillna('No Fence', inplace=True)\ndataset['Fireplace Qu'].fillna('No Fireplace', inplace=True)","ed7563a3":"dataset['Lot Frontage'].fillna(0, inplace=True)","48484d3d":"garage_columns = [col for col in dataset.columns if col.startswith(\"Garage\")]\ndataset[dataset['Garage Cars'].isna()][garage_columns]","8e9ab73a":"dataset[~pd.isna(dataset['Garage Type']) & \n        pd.isna(dataset['Garage Qual'])][garage_columns]","449433a5":"dataset['Garage Cars'].fillna(0, inplace=True)\ndataset['Garage Area'].fillna(0, inplace=True)\n\ndataset.loc[~pd.isna(dataset['Garage Type']) & \n            pd.isna(dataset['Garage Qual']), \"Garage Type\"] = \"No Garage\"\n\nfor col in ['Garage Type', 'Garage Finish', 'Garage Qual', 'Garage Cond']:\n    dataset[col].fillna('No Garage', inplace=True)\n    \ndataset['Garage Yr Blt'].fillna(0, inplace=True)","cf605681":"bsmt_columns = [col for col in dataset.columns if \"Bsmt\" in col]\ndataset[dataset['Bsmt Half Bath'].isna()][bsmt_columns]","593188e5":"dataset[~pd.isna(dataset['Bsmt Cond']) & \n        pd.isna(dataset['Bsmt Exposure'])][bsmt_columns]","92174918":"dataset[~pd.isna(dataset['Bsmt Cond']) & \n        pd.isna(dataset['BsmtFin Type 2'])][bsmt_columns]","b4b22c7d":"for col in [\"Bsmt Half Bath\", \"Bsmt Full Bath\", \"Total Bsmt SF\", \n            \"Bsmt Unf SF\", \"BsmtFin SF 2\", \"BsmtFin SF 1\"]:\n    dataset[col].fillna(0, inplace=True)\n\ndataset.loc[~pd.isna(dataset['Bsmt Cond']) & \n            pd.isna(dataset['Bsmt Exposure']), \"Bsmt Exposure\"] = \"No\"\ndataset.loc[~pd.isna(dataset['Bsmt Cond']) & \n            pd.isna(dataset['BsmtFin Type 2']), \"BsmtFin Type 2\"] = \"Unf\"\n\nfor col in [\"Bsmt Exposure\", \"BsmtFin Type 2\", \n            \"BsmtFin Type 1\", \"Bsmt Qual\", \"Bsmt Cond\"]:\n    dataset[col].fillna(\"No Basement\", inplace=True)","d8153139":"dataset['Mas Vnr Area'].fillna(0, inplace=True)\ndataset['Mas Vnr Type'].fillna(\"None\", inplace=True)","7bdfe135":"dataset['Electrical'].fillna(dataset['Electrical'].mode()[0], inplace=True)","40c389d3":"dataset.isna().values.sum()","b7eb952f":"from matplotlib import pyplot as plt\nimport seaborn as sns\n\nplt.scatter(x=dataset['Gr Liv Area'], y=dataset['SalePrice'], \n            color=\"orange\", edgecolors=\"#000000\", linewidths=0.5);\nplt.xlabel(\"Gr Liv Area\"); plt.ylabel(\"SalePrice\");","3c262c1d":"outlirt_columns = [\"Gr Liv Area\"] + \\\n                  [col for col in dataset.columns if \"Sale\" in col]\ndataset[dataset[\"Gr Liv Area\"] > 4000][outlirt_columns]","c456494c":"dataset = dataset[dataset[\"Gr Liv Area\"] < 4000]","35f1869e":"plt.scatter(x=dataset['Gr Liv Area'], y=dataset['SalePrice'], \n            color=\"orange\", edgecolors=\"#000000\", linewidths=0.5);\nplt.xlabel(\"Gr Liv Area\"); plt.ylabel(\"SalePrice\");","1ba87ad3":"dataset.reset_index(drop=True, inplace=True)","52bd2b74":"dataset.drop(['Order', 'PID'], axis=1, inplace=True)","9d58ac80":"sns.violinplot(x=dataset['SalePrice'], inner=\"quartile\", color=\"#36B37E\");","e5b443a3":"sns.boxplot(dataset['SalePrice'], whis=10, color=\"#00B8D9\");","4841fed3":"sns.distplot(dataset['SalePrice'], kde=False, \n             color=\"#172B4D\", hist_kws={\"alpha\": 0.8});\nplt.ylabel(\"Count\");","bcbeb5be":"fig, ax = plt.subplots(figsize=(12,9))\nsns.heatmap(dataset.corr(), ax=ax);","9ef34498":"sns.distplot(dataset['SalePrice'], kde=False, \n             color=\"#172B4D\", hist_kws={\"alpha\": 0.8});\nplt.ylabel(\"Count\");","c21f32b9":"sns.distplot(dataset['Overall Qual'], kde=False, \n             color=\"#172B4D\", hist_kws={\"alpha\": 1});\nplt.ylabel(\"Count\");","4b00d5ea":"plt.scatter(x=dataset['Overall Qual'], y=dataset['SalePrice'], \n            color=\"orange\", edgecolors=\"#000000\", linewidths=0.5);\nplt.xlabel(\"Overall Qual\"); plt.ylabel(\"SalePrice\");","337c8938":"sns.distplot(dataset['Gr Liv Area'], kde=False, \n             color=\"#172B4D\", hist_kws={\"alpha\": 0.8});\nplt.ylabel(\"Count\");","d68cae1f":"plt.scatter(x=dataset['Gr Liv Area'], y=dataset['SalePrice'], \n            color=\"orange\", edgecolors=\"#000000\", linewidths=0.5);\nplt.xlabel(\"Gr Liv Area\"); plt.ylabel(\"SalePrice\");","f394aa51":"fig, axes = plt.subplots(1, 4, figsize=(18,5))\nfig.subplots_adjust(hspace=0.5, wspace=0.6)\nfor ax, v in zip(axes.flat, [\"Year Built\", \"Year Remod\/Add\", \n                             \"Mas Vnr Area\", \"Total Bsmt SF\"]):\n    sns.distplot(dataset[v], kde=False, color=\"#172B4D\", \n                 hist_kws={\"alpha\": 0.8}, ax=ax)\n    ax.set(ylabel=\"Count\");","6c8ed73d":"x_vars = [\"Year Built\", \"Year Remod\/Add\", \"Mas Vnr Area\", \"Total Bsmt SF\"]\ng = sns.PairGrid(dataset, y_vars=[\"SalePrice\"], x_vars=x_vars);\ng.map(plt.scatter, color=\"orange\", edgecolors=\"#000000\", linewidths=0.5);","8001acf5":"fig, axes = plt.subplots(1, 4, figsize=(18,5))\nfig.subplots_adjust(hspace=0.5, wspace=0.6)\nfor ax, v in zip(axes.flat, [\"1st Flr SF\", \"Full Bath\", \n                             \"Garage Cars\", \"Garage Area\"]):\n    sns.distplot(dataset[v], kde=False, color=\"#172B4D\", \n                 hist_kws={\"alpha\": 0.8}, ax=ax);\n    ax.set(ylabel=\"Count\");","288c76bd":"x_vars = [\"1st Flr SF\", \"Full Bath\", \"Garage Cars\", \"Garage Area\"]\ng = sns.PairGrid(dataset, y_vars=[\"SalePrice\"], x_vars=x_vars);\ng.map(plt.scatter, color=\"orange\", edgecolors=\"#000000\", linewidths=0.5);","37973ce8":"sns.distplot(dataset['TotRms AbvGrd'], kde=False, \n             color=\"#172B4D\", hist_kws={\"alpha\": 0.8});\nplt.ylabel(\"Count\");","b316f625":"plt.rc(\"grid\", linewidth=0.05)\nfig, axes = plt.subplots(1, 2, figsize=(15,5))\nfig.subplots_adjust(hspace=0.5, wspace=0.4)\nh1 = axes[0].hist2d(dataset[\"Garage Cars\"], \n                    dataset[\"Garage Area\"],\n                    cmap=\"viridis\");\naxes[0].set(xlabel=\"Garage Cars\", ylabel=\"Garage Area\")\nplt.colorbar(h1[3], ax=axes[0]);\nh2 = axes[1].hist2d(dataset[\"Gr Liv Area\"], \n                    dataset[\"TotRms AbvGrd\"],\n                    cmap=\"viridis\");\naxes[1].set(xlabel=\"Gr Liv Area\", ylabel=\"TotRms AbvGrd\")\nplt.colorbar(h1[3], ax=axes[1]);\nplt.rc(\"grid\", linewidth=0.25)","61ae712b":"fig, axes = plt.subplots(1, 3, figsize=(16,5))\nfig.subplots_adjust(hspace=0.5, wspace=0.6)\nfor ax, v in zip(axes.flat, [\"Bsmt Unf SF\", \"BsmtFin SF 1\", \"Bsmt Full Bath\"]):\n    sns.distplot(dataset[v], kde=False, color=\"#172B4D\", \n                 hist_kws={\"alpha\": 0.8}, ax=ax);\n    ax.set(ylabel=\"Count\")","ca9bf2ee":"fig, axes = plt.subplots(1, 2, figsize=(15,5))\nfig.subplots_adjust(hspace=0.5, wspace=0.4)\naxes[0].scatter(dataset[\"Bsmt Unf SF\"], dataset[\"BsmtFin SF 1\"],\n                color=\"orange\", edgecolors=\"#000000\", linewidths=0.5);\naxes[0].set(xlabel=\"Bsmt Unf SF\", ylabel=\"BsmtFin SF 1\");\naxes[1].scatter(dataset[\"Bsmt Unf SF\"], dataset[\"Bsmt Full Bath\"],\n                color=\"orange\", edgecolors=\"#000000\", linewidths=0.5);\naxes[1].set(xlabel=\"Bsmt Unf SF\", ylabel=\"Bsmt Full Bath\");","9b88e119":"for f in [\"Overall Qual\", \"Gr Liv Area\"]:\n    dataset[f + \"_p2\"] = dataset[f] ** 2\n    dataset[f + \"_p3\"] = dataset[f] ** 3\ndataset[\"OverallQual_GrLivArea\"] = \\\n    dataset[\"Overall Qual\"] * dataset[\"Gr Liv Area\"]","3ac5a0d0":"dataset.drop([\"Garage Cars\", \"TotRms AbvGrd\"], axis=1, inplace=True)","d69e52ae":"print(\"Unique values in 'Bsmt Cond' column:\")\nprint(dataset['Bsmt Cond'].unique().tolist())","6d6fe58e":"mp = {'Ex':4,'Gd':3,'TA':2,'Fa':1,'Po':0}\ndataset['Exter Qual'] = dataset['Exter Qual'].map(mp)\ndataset['Exter Cond'] = dataset['Exter Cond'].map(mp)\ndataset['Heating QC'] = dataset['Heating QC'].map(mp)\ndataset['Kitchen Qual'] = dataset['Kitchen Qual'].map(mp)\n\nmp = {'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1,'No Basement':0}\ndataset['Bsmt Qual'] = dataset['Bsmt Qual'].map(mp)\ndataset['Bsmt Cond'] = dataset['Bsmt Cond'].map(mp)\ndataset['Bsmt Exposure'] = dataset['Bsmt Exposure'].map(\n    {'Gd':4,'Av':3,'Mn':2,'No':1,'No Basement':0})\n\nmp = {'GLQ':6,'ALQ':5,'BLQ':4,'Rec':3,'LwQ':2,'Unf':1,'No Basement':0}\ndataset['BsmtFin Type 1'] = dataset['BsmtFin Type 1'].map(mp)\ndataset['BsmtFin Type 2'] = dataset['BsmtFin Type 2'].map(mp)\n\ndataset['Central Air'] = dataset['Central Air'].map({'Y':1,'N':0})\ndataset['Functional'] = dataset['Functional'].map(\n    {'Typ':7,'Min1':6,'Min2':5,'Mod':4,'Maj1':3,\n     'Maj2':2,'Sev':1,'Sal':0})\ndataset['Fireplace Qu'] = dataset['Fireplace Qu'].map(\n    {'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1,'No Fireplace':0})\ndataset['Garage Finish'] = dataset['Garage Finish'].map(\n    {'Fin':3,'RFn':2,'Unf':1,'No Garage':0})\ndataset['Garage Qual'] = dataset['Garage Qual'].map(\n    {'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1,'No Garage':0})\ndataset['Garage Cond'] = dataset['Garage Cond'].map(\n    {'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1,'No Garage':0})\ndataset['Pool QC'] = dataset['Pool QC'].map(\n    {'Ex':4,'Gd':3,'TA':2,'Fa':1,'No Pool':0})\ndataset['Land Slope'] = dataset['Land Slope'].map(\n    {'Sev': 2, 'Mod': 1, 'Gtl': 0})\ndataset['Fence'] = dataset['Fence'].map(\n    {'GdPrv':4,'MnPrv':3,'GdWo':2,'MnWw':1,'No Fence':0})","f4e9abad":"dataset[['Paved Drive']].head()","e786bb72":"dataset = pd.get_dummies(dataset)","e52ff65a":"pavedDrive_oneHot = [c for c in dataset.columns if c.startswith(\"Paved\")]\ndataset[pavedDrive_oneHot].head()","34ff5f0c":"dataset[['SalePrice']].head()","52e4684e":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n# We need to fit the scaler to our data before transformation\ndataset.loc[:, dataset.columns != 'SalePrice'] = scaler.fit_transform(\n    dataset.loc[:, dataset.columns != 'SalePrice'])","c10f1d41":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    dataset.drop('SalePrice', axis=1), dataset[['SalePrice']], \n    test_size=0.25, random_state=3)","aa7e818c":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import Ridge\n\nparameter_space = {\n    \"alpha\": [1, 10, 100, 290, 500],\n    \"fit_intercept\": [True, False],\n    \"solver\": ['svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'],\n}\n\nclf = GridSearchCV(Ridge(random_state=3), parameter_space, n_jobs=4,\n                   cv=3, scoring=\"neg_mean_absolute_error\")\n\nclf.fit(X_train, y_train)\nprint(\"Best parameters:\")\nprint(clf.best_params_)","de3cf19a":"ridge_model = Ridge(random_state=3, **clf.best_params_)","b4dbdbce":"ridge_model.fit(X_train, y_train);","fdfc76da":"from sklearn.metrics import mean_absolute_error\n\ny_pred = ridge_model.predict(X_test)\nridge_mae = mean_absolute_error(y_test, y_pred)\nprint(\"Ridge MAE =\", ridge_mae)","3916fc02":"from sklearn.linear_model import ElasticNet\n\nparameter_space = {\n    \"alpha\": [1, 10, 100, 280, 500],\n    \"l1_ratio\": [0.5, 1],\n    \"fit_intercept\": [True, False],\n}\n\nclf = GridSearchCV(ElasticNet(random_state=3), parameter_space, \n                   n_jobs=4, cv=3, scoring=\"neg_mean_absolute_error\")\n\nclf.fit(X_train, y_train)\nprint(\"Best parameters:\")\nprint(clf.best_params_)","e5b941d7":"elasticNet_model = ElasticNet(random_state=3, **clf.best_params_)","bda3b5b8":"elasticNet_model.fit(X_train, y_train);","434d8641":"y_pred = elasticNet_model.predict(X_test)\nelasticNet_mae = mean_absolute_error(y_test, y_pred)\nprint(\"Elastic Net MAE =\", elasticNet_mae)","e4eac3c8":"from sklearn.neighbors import KNeighborsRegressor\n\nparameter_space = {\n    \"n_neighbors\": [9, 10, 11,50],\n    \"weights\": [\"uniform\", \"distance\"],\n    \"algorithm\": [\"ball_tree\", \"kd_tree\", \"brute\"],\n    \"leaf_size\": [1,2,20,50,200]\n}\n\nclf = GridSearchCV(KNeighborsRegressor(), parameter_space, cv=3, \n                   scoring=\"neg_mean_absolute_error\", n_jobs=4)\n\nclf.fit(X_train, y_train)\nprint(\"Best parameters:\")\nprint(clf.best_params_)","1493a7a2":"knn_model = KNeighborsRegressor(**clf.best_params_)","3ab9225a":"knn_model.fit(X_train, y_train);","21beaf6f":"y_pred = knn_model.predict(X_test)\nknn_mae = mean_absolute_error(y_test, y_pred)\nprint(\"K-Nearest Neighbors MAE =\", knn_mae)","e91f9311":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.svm import SVR\n\nparameter_space = \\\n    {\n        \"kernel\": [\"poly\", \"linear\", \"rbf\", \"sigmoid\"],\n        \"degree\": [3, 5],\n        \"coef0\": [0, 3, 7],\n        \"gamma\":[1e-3, 1e-1, 1\/X_train.shape[1]],\n        \"C\": [1, 10, 100],\n    }\n\nclf = GridSearchCV(SVR(), parameter_space, cv=3, n_jobs=4,\n                   scoring=\"neg_mean_absolute_error\")\n\nclf.fit(X_train, y_train)\nprint(\"Best parameters:\")\nprint(clf.best_params_)","3504dcae":"svr_model = SVR(**clf.best_params_)","e2829959":"svr_model.fit(X_train, y_train);","f3b5faad":"y_pred = svr_model.predict(X_test)\nsvr_mae = mean_absolute_error(y_test, y_pred)\nprint(\"Support Vector Regression MAE =\", svr_mae)","b2b756dd":"from sklearn.tree import DecisionTreeRegressor\n\nparameter_space = \\\n    {\n        \"criterion\": [\"mse\", \"friedman_mse\", \"mae\"],\n        \"min_samples_split\": [5, 18, 29, 50],\n        \"min_samples_leaf\": [3, 7, 15, 25],\n        \"max_features\": [20, 50, 150, 200, X_train.shape[1]],\n    }\n\nclf = GridSearchCV(DecisionTreeRegressor(random_state=3), parameter_space, \n                   cv=3, scoring=\"neg_mean_absolute_error\", n_jobs=4)\n\nclf.fit(X_train, y_train)\nprint(\"Best parameters:\")\nprint(clf.best_params_)","e9db29c2":"dt_model = DecisionTreeRegressor(**clf.best_params_)","4a0f3092":"dt_model.fit(X_train, y_train);","eee3f018":"y_pred = dt_model.predict(X_test)\ndt_mae = mean_absolute_error(y_test, y_pred)\nprint(\"Decision Tree MAE =\", dt_mae)","52a9e05f":"from sklearn.neural_network import MLPRegressor\n\nparameter_space = \\\n    {\n        \"hidden_layer_sizes\": [(7,)*3, (19,), (100,), (154,)],\n        \"activation\": [\"identity\", \"logistic\", \"tanh\", \"relu\"],\n        \"solver\": [\"lbfgs\"],\n        \"alpha\": [1, 10, 100],\n    }\n\nclf = GridSearchCV(MLPRegressor(random_state=3), parameter_space, \n                   cv=3, scoring=\"neg_mean_absolute_error\", n_jobs=4)\n\nclf.fit(X_train, y_train)\nprint(\"Best parameters:\")\nprint(clf.best_params_)","10059cc2":"nn_model = MLPRegressor(**clf.best_params_)","f73ed538":"nn_model.fit(X_train, y_train);","a40c8d5c":"y_pred = nn_model.predict(X_test)\nnn_mae = mean_absolute_error(y_test, y_pred)\nprint(\"Neural Network MAE =\", nn_mae)","3d321dac":"from sklearn.ensemble import RandomForestRegressor\n\nparameter_space = \\\n    {\n        \"n_estimators\": [10, 100, 300, 600],\n        \"criterion\": [\"mse\", \"mae\"],\n        \"max_depth\": [7, 50, 254],\n        \"min_samples_split\": [2, 5],\n        \"min_samples_leaf\": [1, 5],\n        \"max_features\": [19, 100, X_train.shape[1]],\n        \"bootstrap\": [True, False],\n    }\n\nclf = RandomizedSearchCV(RandomForestRegressor(random_state=3), \n                         parameter_space, cv=3, n_jobs=4,\n                         scoring=\"neg_mean_absolute_error\", \n                         n_iter=10, random_state=3)\n\nclf.fit(X_train, y_train)\nprint(\"Best parameters:\")\nprint(clf.best_params_)","2d1c07fd":"rf_model = RandomForestRegressor(**clf.best_params_)","5da7562a":"rf_model.fit(X_train, y_train);","c59434af":"y_pred = rf_model.predict(X_test)\nrf_mae = mean_absolute_error(y_test, y_pred)\nprint(\"Random Forest MAE =\", rf_mae)","1e0f50ca":"from xgboost import XGBRegressor\n\nparameter_space = \\\n    {\n        \"max_depth\": [4, 5, 6],\n        \"learning_rate\": [0.005, 0.009, 0.01],\n        \"n_estimators\": [700, 1000, 2500],\n        \"booster\": [\"gbtree\",],\n        \"gamma\": [7, 25, 100],\n        \"subsample\": [0.3, 0.6],\n        \"colsample_bytree\": [0.5, 0.7],\n        \"colsample_bylevel\": [0.5, 0.7,],\n        \"reg_alpha\": [1, 10, 33],\n        \"reg_lambda\": [1, 3, 10],\n    }\n\nclf = RandomizedSearchCV(XGBRegressor(random_state=3), \n                         parameter_space, cv=3, n_jobs=4,\n                         scoring=\"neg_mean_absolute_error\", \n                         random_state=3, n_iter=10)\n\nclf.fit(X_train, y_train)\nprint(\"Best parameters:\")\nprint(clf.best_params_)","b715cb55":"xgb_model = XGBRegressor(**clf.best_params_)","bb3cf945":"xgb_model.fit(X_train, y_train);","5f242b9e":"y_pred = xgb_model.predict(X_test)\nxgb_mae = mean_absolute_error(y_test, y_pred)\nprint(\"XGBoost MAE =\", xgb_mae)","6388ae5b":"x = ['KNN', 'Decision Tree', 'Neural Network', 'Ridge', \n     'Elastic Net', 'Random Forest', 'SVR', 'XGBoost']\ny = [22780.14, 20873.95, 15656.38, 15270.46, 14767.91,\n     14506.46, 12874.93, 12556.68]\ncolors = [\"#392834\", \"#5a3244\", \"#7e3c4d\", \"#a1484f\", \n          \"#c05949\", \"#d86f3d\", \"#e88b2b\", \"#edab06\"]\nfig, ax = plt.subplots()\nplt.barh(y=range(len(x)), tick_label=x, width=y, height=0.4, color=colors);\nax.set(xlabel=\"MAE (smaller is better)\", ylabel=\"Model\");","c551d108":"sns.violinplot(x=dataset['SalePrice'], inner=\"quartile\", color=\"#36B37E\");","8bec415b":"sns.boxplot(dataset['SalePrice'], whis=10, color=\"#00B8D9\");","847c60a4":"sns.distplot(dataset['SalePrice'], kde=False,\n             color=\"#172B4D\", hist_kws={\"alpha\": 0.8});","99b152a2":"y_train.describe(include=[np.number])","946cf56b":"xgb_feature_importances = xgb_model.feature_importances_\nxgb_feature_importances = pd.Series(\n    xgb_feature_importances, index=X_train.columns.values\n    ).sort_values(ascending=False).head(15)\n\nfig, ax = plt.subplots(figsize=(7, 5))\nsns.barplot(x=xgb_feature_importances, \n            y=xgb_feature_importances.index, \n            color=\"#003f5c\");\nplt.xlabel('Feature Importance');\nplt.ylabel('Feature');","7a3301db":"rf_feature_importances = rf_model.feature_importances_\nrf_feature_importances = pd.Series(\n    rf_feature_importances, index=X_train.columns.values\n    ).sort_values(ascending=False).head(15)\n\nfig, ax = plt.subplots(figsize=(7,5))\nsns.barplot(x=rf_feature_importances, \n            y=rf_feature_importances.index, \n            color=\"#ffa600\");\nplt.xlabel('Feature Importance');\nplt.ylabel('Feature');","02de0759":"common_imp_feat = [x for x in xgb_feature_importances.index \n                   if x in rf_feature_importances.index]\ncommImpFeat_xgb_scores = [xgb_feature_importances[x] \n                          for x in common_imp_feat]\ncommImpFeat_rf_scores = [rf_feature_importances[x] \n                         for x in common_imp_feat]\n\nind = np.arange(len(commImpFeat_xgb_scores))\nwidth = 0.35\n\nfig, ax = plt.subplots()\nax.bar(ind - width\/2, commImpFeat_xgb_scores, width,\n       color='#003f5c', label='XGBoost');\nax.bar(ind + width\/2, commImpFeat_rf_scores, width, \n       color='#ffa600', label='Random Forest')\nax.set_xticks(ind);\nax.set_xticklabels(common_imp_feat);\nax.legend();\nplt.xticks(rotation=90);","4e9a41f8":"That means that the prediction type that is appropriate to our problem is **regression**.\n\nNow we move to choose the modeling techniques we want to use. There are a lot of techniques available for regression problems like Linear Regression, Ridge Regression, Artificial Neural Networks, Decision Trees, Random Forest, etc. In this project, we will test many modeling techniques, and then choose the technique(s) that yield the best results. The techniques that we will try are:\n\n### 1. Linear Regression\n\nThis technique models the relationship between the target variable and the independent variables (predictors). It fits a linear model with coefficients to the data in order to minimize the residual sum of squares between the target variable in the dataset, and the predicted values by the linear approximation.\n\n### 2. Nearest Neighbors\n\nNearest Neighbors is a type of instance-based learning. For this technique, the model tries to find a number (k) of training examples closest in distance to a new point, and predict the output for this new point from these closest neighbors. k can be a user-defined number (k-nearest neighbors), or vary based on the local density of points (radius-based neighbors). The distance metric used to measure the closeness is mostly the Euclidean distance.\n\n### 3. Support Vector Regression\n\nSupport vector machines (SVM) are a set of methods that can be used for classification and regression problems. When they are used for regression, we call the technique Support Vector Regression.\n\n### 4. Decision Trees\n\nFor this technique, the goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. An example of a simple decision tree for predicting who survived when the Titanic sank is shown in Figure 8:\n\n![Figure 8: Predicting who survived when the Titanic sank](https:\/\/s3.eu-north-1.amazonaws.com\/ammar-files\/kaggle-kernels\/House+Price+Prediction+%7C+An+End-to-End+Machine+Learning+Project\/8.png)\n\n### 5. Neural Networks\n\nNeural network is a machine learning model that tries to mimic the way of working of the biological brain. A neural network consists of multiple layers. Each layer consists of a number of nodes. The nodes of each layer are connected to the nodes of adjacent layers. Each node can be activated or not based on its inputs and its activation function. An example of a neural network is shown in Figure 9:\n\n![Figure 9: A neural network](https:\/\/s3.eu-north-1.amazonaws.com\/ammar-files\/kaggle-kernels\/House+Price+Prediction+%7C+An+End-to-End+Machine+Learning+Project\/9.png)\n\n### 6. Random Forest\n\nBagging is an ensemble method where many base models are used with a randomized subset of data to reduce the variance of a the base model.\n\n### 7. Gradient Boosting\n\nBoosting is also an ensemble method where weak base models are used to create a strong model that reduces bias and variance of the base model.\n\nEach one of these techniques has many algorithmic implementation. *We will choose algorithm(s) for each of these techniques* in the next section.\n\n<h1 id=\"model-building\">Model Building and Evaluation<\/h1>\n\nIn this part, we will build our prediction model: we will choose algorithms for each of the techniques we mentioned in the previous section. After we build the model, we will evaluate its performance and results.\n\n## Feature Scaling\n\nIn order to make all algorithms work properly with our data, we need to scale the features in our dataset. For that, we will use a helpful function named `StandardScaler()` from the popular Scikit-Learn Python package. This function standardizes features by subtracting the mean and scaling to unit variance. It works on each feature independently. For a value $x$ of some feature $F$, the `StandardScaler()` function performs the following operation:\n\n\\begin{equation*}\nz = \\frac{x - \\mu}{s}\n\\end{equation*}\n\nwhere $z$ is the result of scaling $x$, $\\mu$ is the mean of feature $F$, and $s$ is the standard deviation of $F$. ","3c64fbe5":"##### Garage Cond, Garage Qual, Garage Finish, Garage Yr Blt, Garage Type, Garage Cars, and Garage Area\n\nAccording to the dataset documentation, `NA` in `Garage Cond`, `Garage Qual`, `Garage Finish`, and `Garage Type` indicates that there is no garage in the house. So we fill in the missing values in these columns with `\"No Garage\"`. We notice that `Garage Cond`, `Garage Qual`, `Garage Finish`, `Garage Yr Blt` columns have 159 missing values, but `Garage Type` has 157 and both `Garage Cars` and `Garage Area` have one missing value. Let's take a look at the row that contains the missing value in `Garage Cars`:","36bf2a5d":"Then we train our model using our training set (`X_train` and `y_train`):","a7f5b5dc":"To avoid problems in modeling later, we will reset our dataset index after removing the outlier rows, so no gaps remain in our dataset index:","74043217":"In the table we got, `count` represents the number of non-null values in each column, `unique` represents the number of unique values, `top` represents the most frequent element, and `freq` represents the frequency of the most frequent element.\n\n## Data Cleaning\n\n### Dealing with Missing Values\n\nWe should deal with the problem of missing values because some machine learning models don't accept data with missing values. Firstly, let's see the number of missing values in our dataset. We want to see the number and the percentage of missing values for each column that actually contains missing values.","5da0242b":"We will replace the values of `Garage Type` with `\"No Garage\"` in these two rows also.\n\nFor `Garage Yr Blt`, we will fill in missing values with 0 since this is a numerical column:","7b81c935":"## Splitting the Dataset\n\nAs usual for supervised machine learning problems, we need a training dataset to train our model and a test dataset to evaluate the model. So we will split our dataset randomly into two parts, one for training and the other for testing. For that, we will use another function from Scikit-Learn called `train_test_split()`:","e8ed6011":"Finally, we test our model on `X_test`. Then we evaluate the model performance by comparing its predictions with the actual true values in `y_test` using the MAE metric as we described above:","719e1773":"We defined the parameter space above using reasonable values for chosen parameters. Then we used `GridSearchCV()` with 3 folds (`cv=3`). Now we build our XGBoost model with the best parameters found:","3025d8bc":"## Composition of Models and Feature Engineering to Win Algorithmic Trading Challenge\n\nA study done by de Abril and Sugiyama (2013) introduced the techniques and ideas used to win Algorithmic Trading Challenge, a competition held on Kaggle. The goal of the competition was to develop a model that can predict the short-term response of order-driven markets after a big liquidity shock. A liquidity shock happens when a trade or a sequence of trades causes an acute shortage of liquidity (cash for example). \n\nThe challenge data contains a training dataset and a test dataset. The training dataset has around `754,000` records of trade and quote observations for many securities of London Stock Exchange before and after a liquidity shock. A trade event happens when shares are sold or bought, whereas a quote event happens when the ask price or the best bid changes.\n\nA separate model was built for bid and another for ask. Each one of these models consists of `K` random-forest sub-models. The models predict the price at a particular future time. \n\nThe authors spent much effort on feature engineering. They created more than `150` features. These features belong to four categories: price features, liquidity-book features, spread features (bid\/ask spread), and rate features (arrival rate of orders\/quotes). They applied a feature selection algorithm to obtain the optimal feature set ($F_b$) for bid sub-models and the optimal feature set ($F_a$) of all ask sub-models. The algorithm applied eliminates features in a backward manner in order to get a feature set with reasonable computing time and resources.\n\nThree instances of the final model proposed in the study were trained on three datasets; each one of them consists of `50,000` samples sampled randomly from the training dataset. Then, the three models were applied to the test dataset. The predictions of the three models were then averaged to obtain the final prediction. The proposed method achieved a RMSE score of `0.77` approximately.\n","7920f656":"We can see that the mean is `179,846.69` and the median is `159,895`. We can see also that the first quartile is `128,500`; this means that 75% of the data is larger than this number. Now looking at XGBoost error of `12,556.68`, we can say that an error of about `12,000` is good for data whose mean is `159,895` and whose 75% of it is larger than `128,500`.","8b3c0bde":"We see that `Overall Qual` takes an integer value between 1 and 10, and that most houses have an overall quality between 5 and 7. Now we plot the scatter plot of `SalePrice` and `Overall Qual` to see the relationship between them:","ff2c2435":"### One-Hot Encoding For Categorical Features\n\nMachine learning models accept only numbers as input, and since our dataset contains categorical features, we need to encode them in order for our dataset to be suitable for modeling. We will encode our categorical features using one-hot encoding technique which transforms the categorical variable into a number of binary variables based on the number of unique categories in the categorical variable; each of the resulting binary variables has only 0 and 1 as its possible values. Pandas package provides a convenient function `get_dummies()` that can be used for performing one-hot encoding on our dataset.\n\nTo see what will happen to our dataset, let us take for example the variable `Paved Drive` which indicates how the driveway is paved. It has three possible values: `Y` which means for \"Paved\", `P` which means \"Partial Pavement\", and `N` which means \"Dirt\/Gravel\". Let us take a look at `Paved Drive` value for the first few rows in our dataset:","d31bc81d":"### Support Vector Regression\n\nFor Support Vector Regression (SVR), we will use one of three implementations provided by the Scikit-Learn package.\n\nThe SVR model has the following syntax:\n\n```py\nSVR(kernel=\u2019rbf\u2019, degree=3, gamma=\u2019auto_deprecated\u2019, coef0=0.0, tol=0.001, \n    C=1.0, epsilon=0.1, shrinking=True, cache_size=200, verbose=False, max_iter=-1)\n```\n\nFirstly, we will use `GridSearchCV()` to search for the best model parameters in a parameter space provided by us. The parameter `kernel` specifies the kernel type to be used in the algorithm, `degree` represents the degree of the polynomial kernel `poly`, `gamma` is the kernel coefficient for `rbf`, `poly` and `sigmoid` kernels, `coef0` is independent term in kernel function, and `C` is the penalty parameter of the error term.","72bf4774":"Now, we perform one-hot encoding:","8ef48b29":"We can see that they are truly positively correlated; generally, as the overall quality increases, the sale price increases too. This verfies what we got from the heatmap above.\n\nNow, we want to see the relationship between the target variable and `Gr Liv Area` variable which represents the living area above ground. Let us first see the distribution of `Gr Liv Area`:","0db52a13":"We can see for example that a value of `P` in the original `Paved Drive` column is converted to 1 in `Paved Drive_P` and zeros in `Paved Drive_N` and `Paved Drive_Y` after one-hot encoding.\n\nAll categorical column are converted in the same way.\n\nNow, after we have cleaned and prepared our dataset, it is ready for modeling.\n\n<h1 id=\"pred-type\">Prediction Type and Modeling Techniques<\/h1>\n\nIn this section, we choose the type of machine learning prediction that is suitable to our problem. We want to determine if this is a ragression problem or a classification problem. In this project, we want to predict the *price* of a house given information about it. The price we want to predict is a continuous value; it can be any real number. This can be seen by looking at the target vatiable in our dataset `SalePrice`:","df8502e2":"Now we remove them:","bbcac608":"We defined the parameter space above using reasonable values for chosen parameters. Then we used `GridSearchCV()` with 3 folds (`cv=3`). Now we build our Neural Network model with the best parameters found:","f54b7f26":"Then we train our model using our training set (`X_train` and `y_train`):","8ad87d7c":"## Deleting Some Unimportant Columns\n\nWe will delete columns that are not useful in our analysis. The columns to be deleted are `Order` and `PID`:","2a3982f2":"We can see that there are 2917 entries in `Pool Area` column that have a value of 0. This verfies our hypothesis that each house without a pool has a missing value in `Pool QC` column and a value of 0 in `Pool Area` column. So let's fill the missing values in `Pool QC` column with `\"No Pool\"`:","7882a67e":"The scatter plot above shows clearly the strong positive correlation between `Gr Liv Area` and `SalePrice` verifying what we found with the heatmap.\n\n#### Moderate Positive Correlation\n\nNext, we want to visualize the relationship between the target variable and the variables that are positively correlated with it, but the correlation is not very strong. Namely, these variables are `Year Built`, `Year Remod\/Add`, `Mas Vnr Area`, `Total Bsmt SF`, `1st Flr SF`, `Full Bath`, `Garage Cars`, and `Garage Area`. We start with the first four. Let us see the distribution of each of them:","3672d68b":"From the table above, we can see, for example, that the average lot area of the houses in our dataset is 10,147.92 ft<sup>2<\/sup> with a standard deviation of 7,880.02 ft<sup>2<\/sup>. We can see also that the minimum lot area is 1,300 ft<sup>2<\/sup> and the maximum lot area is 215,245 ft<sup>2<\/sup> with a median of 9,436.5 ft<sup>2<\/sup>. Similarly, we can get a lot of information about our dataset variables from the table.\n\nThen, we move to see statistical information about the non-numerical columns in our dataset:","48e0167f":"<h1 id=\"introduction\">Introduction<\/h1>\n\nThousands of houses are sold everyday. There are some questions every buyer asks himself like: What is the actual price that this house deserves? Am I paying a fair price? In this paper, a machine learning model is proposed to predict a house price based on data related to the house (its size, the year it was built in, etc.). During the development and evaluation of our model, we will show the code used for each step followed by its output. This will facilitate the reproducibility of our work. In this study, Python programming language with a number of Python packages will be used.\n\n## Goals of the Study\n\nThe main objectives of this study are as follows:\n\n- To apply data preprocessing and preparation techniques in order to obtain clean data\n- To build machine learning models able to predict house price based on house features\n- To analyze and compare models performance in order to choose the best model\n\n## Paper Organization\n\nThis paper is organized as follows: in the next section, section 2, we examine studies related to our work from scientific journals. In section 3, we go through data preparation including data cleaning, outlier removal, and feature engineering. Next in section 4, we discuss the type of our problem and the type of machine-learning prediction that should be applied; we also list the prediction techniques that will be used. In section 5, we choose algorithms to implement the techniques in section 4; we build models based on these algorithms; we also train and test each model. In section 6, we analyze and compare the results we got from section 5 and conclude the paper.","7c2fc03e":"From the plots, we can see the negative correlation between each pair of these variables.\n\nWe will use the information we got from exploratory data analysis in this section, we will use it in feature engineering in the next section.\n\n## Feature Engineering\n\nIn this section, we will use the insights from Exploratory Data Analysis section to engineer the features of our dataset.\n\n### Creating New Derived Features\n\nFirstly, we noticed a high positive correlation between the target variable `SalePrice` and each of `Overall Qual` and `Gr Liv Area`. This gives an indication that the latter two features are very important in predicting the sale price. So, we will create polynomial features out of these features: For each one of these features, we will derive a feature whose values are the squares of original values, and another feature whose values are the cubes of original values. Moreover, we will create a feature whose values are the product of our two features values:","3686d08d":"Then we train our model using our training set (`X_train` and `y_train`):","9bd0f305":"## Feature Importances\n\nSome of the models we used provide the ability to see the importance of each feature in the dataset after fitting the model. We will look at the feature importances provided by both XGBoost and Random Forest models. We have 242 features in our data which is a big number, so we will take a look at the 15 most important features.\n\n### XGBoost\n\nLet's discover the most important features as determined by XGBoost model:","277314e4":"We can see from the plot that most house prices fall between 100,000 and 250,000. The dashed lines represent the locations of the three quartiles Q1, Q2 (the median), and Q3. Now let's see the box plot of `SalePrice`:","165ed77b":"We defined the parameter space above using reasonable values for chosen parameters. Then we used `GridSearchCV()` with 3 folds (`cv=3`). Now we build our Ridge model with the best parameters found:","1f156a69":"Then we train our model using our training set (`X_train` and `y_train`):","acaad249":"Then we train our model using our training set (`X_train` and `y_train`):","09eba4f1":"Now, we visualize the relationship between each pair using scatter plots:","448a01e5":"<h1 id=\"data-prep\">Data  Preparation<\/h1>\n\nIn this study, we will use a housing dataset presented by De Cock (2011). This dataset describes the sales of residential units in Ames, Iowa starting from 2006 until 2010. The dataset contains a large number of variables that are involved in determining a house price. We obtained a csv copy of the data from https:\/\/www.kaggle.com\/prevek18\/ames-housing-dataset.\n\n## Data Description\n\nThe dataset contains `2930` records (rows) and `82` features (columns).\n\nHere, we will provide a brief description of dataset features. Since the number of features is large (82), we will attach the original data description file to this paper for more information about the dataset (It can be downloaded also from https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/data). Now, we will mention the feature name with a short description of its meaning.\n\n|Feature|Description|\n|-------|-----------|\n|MSSubClass| The type of the house involved in the sale|\n|MSZoning| The general zoning classification of the sale|\n|LotFrontage| Linear feet of street connected to the house|\n|LotArea| Lot size in square feet|\n|Street| Type of road access to the house|\n|Alley| Type of alley access to the house|\n|LotShape| General shape of the house|\n|LandContour| House flatness|\n|Utilities| Type of utilities available|\n|LotConfig| Lot configuration|\n|LandSlope| House Slope|\n|Neighborhood| Locations within Ames city limits|\n|Condition1| Proximity to various conditions|\n|Condition2| Proximity to various conditions (if more than one is present)|\n|BldgType| House type|\n|HouseStyle| House style|\n|OverallQual| Overall quality of material and finish of the house|\n|OverallCond| Overall condition of the house|\n|YearBuilt| Construction year|\n|YearRemodAdd| Remodel year (if no remodeling nor addition, same as YearBuilt)|\n|RoofStyle| Roof type|\n|RoofMatl| Roof material|\n|Exterior1st| Exterior covering on house|\n|Exterior2nd| Exterior covering on house (if more than one material)|\n|MasVnrType| Type of masonry veneer|\n|MasVnrArea| Masonry veneer area in square feet|\n|ExterQual| Quality of the material on the exterior|\n|ExterCond| Condition of the material on the exterior|\n|Foundation| Foundation type|\n|BsmtQual| Basement height|\n|BsmtCond| Basement Condition|\n|BsmtExposure| Refers to walkout or garden level walls|\n|BsmtFinType1| Rating of basement finished area|\n|BsmtFinSF1| Type 1 finished square feet|\n|BsmtFinType2| Rating of basement finished area (if multiple types)|\n|BsmtFinSF2| Type 2 finished square feet|\n|BsmtUnfSF| Unfinished basement area in square feet|\n|TotalBsmtSF| Total basement area in square feet|\n|Heating| Heating type|\n|HeatingQC| Heating quality and condition|\n|CentralAir| Central air conditioning|\n|Electrical| Electrical system type|\n|1stFlrSF| First floor area in square feet|\n|2ndFlrSF| Second floor area in square feet|\n|LowQualFinSF| Low quality finished square feet in all floors|\n|GrLivArea| Above-ground living area in square feet|\n|BsmtFullBath| Basement full bathrooms|\n|BsmtHalfBath| Basement half bathrooms|\n|FullBath| Full bathrooms above ground|\n|HalfBath| Half bathrooms above ground|\n|Bedroom| Bedrooms above ground|\n|Kitchen| Kitchens above ground|\n|KitchenQual| Kitchen quality|\n|TotRmsAbvGrd| Total rooms above ground (excluding bathrooms)|\n|Functional| Home functionality|\n|Fireplaces| Number of fireplaces|\n|FireplaceQu| Fireplace quality|\n|GarageType| Garage location|\n|GarageYrBlt| Year garage was built in|\n|GarageFinish| Interior finish of the garage|\n|GarageCars| Size of garage (in car capacity)|\n|GarageArea| Garage size in square feet|\n|GarageQual| Garage quality|\n|GarageCond| Garage condition|\n|PavedDrive| How driveway is paved|\n|WoodDeckSF| Wood deck area in square feet|\n|OpenPorchSF| Open porch area in square feet|\n|EnclosedPorch| Enclosed porch area in square feet|\n|3SsnPorch| Three season porch area in square feet|\n|ScreenPorch| Screen porch area in square feet|\n|PoolArea| Pool area in square feet|\n|PoolQC| Pool quality|\n|Fence| Fence quality|\n|MiscFeature| Miscellaneous feature|\n|MiscVal| Value of miscellaneous feature|\n|MoSold| Sale month|\n|YrSold| Sale year|\n|SaleType| Sale type|\n|SaleCondition| Sale condition|","3d69273c":"Finally, we test our model on `X_test`. Then we evaluate the model performance by comparing its predictions with the actual true values in `y_test` using the MAE metric as we described above:","e5e8aec5":"#### 2. Elastic Net\n\nThis model has the following syntax:\n\n```py\nElasticNet(alpha=1.0, l1_ratio=0.5, fit_intercept=True, normalize=False, \n           precompute=False, max_iter=1000, copy_X=True, tol=0.0001, \n           warm_start=False, positive=False, random_state=None, selection=\u2019cyclic\u2019)\n```\n\nFirstly, we will use `GridSearchCV()` to search for the best model parameters in a parameter space provided by us. The parameter `alpha` is a constant that multiplies the penalty terms, `l1_ratio` determines the amount of L1 and L2 regularizations, `fit_intercept` is the same as Ridge's.","1e1266ec":"##### Electrical\n\nThis column has one missing value. We will fill in this value with the mode of this column:","227c648f":"We will fill in the missing values in `Bsmt Exposure` for these three rows with `\"No\"`. According to the dataset documentation, `\"No\"` for `Bsmt Exposure` means \"No Exposure\":","8f7fcd4c":"Finally, we test our model on `X_test`. Then we evaluate the model performance by comparing its predictions with the actual true values in `y_test` using the MAE metric as we described above:","d96fb3ee":"We can see that `Misc Val` column has 2827 entries with a value of 0. `Misc Feature` has 2824 missing values. Then, as with `Pool QC`, we can say that each house without a \"miscellaneous feature\" has a missing value in `Misc Feature` column and a value of 0 in `Misc Val` column. So let's fill the missing values in `Misc Feature` column with `\"No Feature\"`:","e4336037":"Then we train our model using our training set (`X_train` and `y_train`):","e2fe6717":"Let's now take a look at the row where `BsmtFin Type 2` is null while `BsmtFin Type 1`, `Bsmt Qual`, and `Bsmt Cond` are not null:","39a1c96e":"### Random Forest\n\nFor Random Forest (RF), we will use an implementations provided by the Scikit-Learn package.\n\nThe Random Forest model has the following syntax:\n\n```py\nRandomForestRegressor(n_estimators=\u2019warn\u2019, criterion=\u2019mse\u2019, max_depth=None, \n                      min_samples_split=2, min_samples_leaf=1, \n                      min_weight_fraction_leaf=0.0, max_features=\u2019auto\u2019, \n                      max_leaf_nodes=None, min_impurity_decrease=0.0, \n                      min_impurity_split=None, bootstrap=True, oob_score=False, \n                      n_jobs=None, random_state=None, verbose=0, warm_start=False)\n```\n\nFirstly, we will use `GridSearchCV()` to search for the best model parameters in a parameter space provided by us. The parameter `n_estimators` specifies the number of trees in the forest, `bootstrap` determines whether bootstrap samples are used when building trees. `criterion`, `max_depth`, `min_samples_split`, `min_samples_leaf`, `max_features` are the same as those of the decision tree model.","ad278cad":"And now let us see their relationships with the target variable:","317df8df":"We can see that most house prices fall between 100,000 and 200,000. We see also that there is a number of expensive houses to the right of the plot. Now, we move to see the distribution of `Overall Qual` variable:","7638f9c7":"Next, we move to the last four. Let us see the distribution of each of them:","20a1e7dc":"This means that our dataset is now complete; it doesn't contain any missing value anymore.\n\n## Outlier Removal\n\nIn the paper in which our dataset was introduced by De Cock (2011), the author states that there are five unusual values and outliers in the dataset, and encourages the removal of these outliars. He suggested plotting `SalePrice` against `Gr Liv Area` to spot the outliers. We will do that now:","70feee14":"Also, we noticed that there are some predictor features that are highly correlated with each other. To avoid the [Multicollinearity](https:\/\/en.wikipedia.org\/wiki\/Multicollinearity) problem, we will delete one feature from each pair of highly correlated predictors. We have two pairs: the first consists of `Garage Cars` and `Garage Area`, and the other consists of `Gr Liv Area` and `TotRms AbvGrd`. For the first pair, we will remove `Garage Cars` feature; from the second pair, we will remove `TotRms AbvGrd` feature:","f4b92737":"# House Price Prediction | An End-to-End Machine Learning Project\n---\n <h3 style=\"font-family:Times New Roman; font-style:italic\">By <a href=\"http:\/\/ammar-alyousfi.com\">Ammar Alyousfi<\/a> (December, 2018). <a href=\"https:\/\/s3.eu-north-1.amazonaws.com\/ammar-files\/ammar-website\/House+Price+Prediction+Using+Machine+Learning+Techniques.pdf\">PDF<\/a><\/h3>\n<p>&nbsp;<\/p>\n\n![](https:\/\/s3.eu-north-1.amazonaws.com\/ammar-files\/kaggle-kernels\/House+Price+Prediction+%7C+An+End-to-End+Machine+Learning+Project\/header-img.jpg)\n\n<p>&nbsp;<\/p>\n\n## Contents\n\n<a href=\"#introduction\">Introduction<\/a>\n\n<a href=\"#lr\">Literature Review<\/a>\n\n<a href=\"#data-prep\">Data Preparation<\/a>\n\n<a href=\"#eda\">Exploratory Data Analysis<\/a>\n\n<a href=\"#pred-type\">Prediction Type and Modeling Techniques<\/a>\n\n<a href=\"#model-building\">Model Building and Evaluation<\/a>\n\n<a href=\"#analysis-comparison\">Analysis and Comparison<\/a>\n\n<a href=\"#comparison\">Conclusion<\/a>\n\n<a href=\"#ref\">References<\/a>","2c138b10":"We defined the parameter space above using reasonable values for chosen parameters. Then we used `GridSearchCV()` with 3 folds (`cv=3`). Now we build our Support Vector Regression model with the best parameters found:","625c1ce7":"Now, let's get statistical information about the numeric columns in our dataset. We want to know the mean, the standard deviation, the minimum, the maximum, and the 50th percentile (the median) for *each numeric column* in the dataset:","2a0d60bc":"We defined the parameter space above using reasonable values for chosen parameters. Then we used `GridSearchCV()` with 3 folds (`cv=3`). Now we build our Ridge model with the best parameters found:","64702f5c":"## Getting A Feel of the Dataset\n\nLet's display the first few rows of the dataset to get a feel of it:","4bcf0ad5":"##### Misc Feature\n\nThe percentage of missing values in Pool QC column is 96.38% which is very high also. Let's take a look at the values of `Misc Val` column:","b7bd07d1":"<h1 id=\"eda\">Exploratory Data Analysis<\/h1>\n\nIn this section, we will explore the data using visualizations. This will allow us to understand the data and the relationships between variables better, which will help us build a better model.\n\n## Target Variable Distribution\n\nOur dataset contains a lot of variables, but the most important one for us to explore is the target variable. We need to understand its distribution. First, we start by plotting the violin plot for the target variable. The width of the violin represents the frequency. This means that if a violin is the widest between 300 and 400, then the area between 300 and 400 contains more data points than other areas:","b22954eb":"Where \"Gd\" means \"Good\", \"TA\" means \"Typical\", \"Po\" means \"Poor\", \"Fa\" means \"Fair\", and \"Ex\" means \"Excellent\" according to the dataset documentation. But the problem is that machine learning models will not know that this feature represents a ranking; it will be treated as other categorical features. So to solve this issue, we will map each one of the possible values of this feature to a number. We will map `\"No Basement\"` to 0, `\"Po\"` to 1, `\"Fa\"` to 2, `\"TA\"` to 3, `\"Gd\"` to 4, and `\"Ex\"` to 5.\n\nThe ordinal features in the dataset are: `Exter Qual`, `Exter Cond`, `Bsmt Qual`, `Bsmt Cond`, `Bsmt Exposure`, `BsmtFin Type 1`, `BsmtFin Type 2`, `Heating QC`, `Central Air`, `Kitchen Qual`, `Functional`, `Fireplace Qu`, `GarageFinish`, `Garage Qual`, `Garage Cond`, `Pool QC`, `Land Slope` and `Fence`. We will map the values of each of them to corresponding numbers as described for `Bsmt Cond` above and in accordance with the dataset documentation:","ccc476e6":"We will fill in the missing value in `BsmtFin Type 2` for this row with `\"Unf\"`. According to the dataset documentation, `\"Unf\"` for `BsmtFin Type 2` means \"Unfinished\":","e7aa5a31":"<h1 id=\"conclusions\">Conclusion<\/h1>\n\nIn this paper, we built serveral regression models to predict the price of some house given some of the house features. We eveluated and compared each model to determine the one with highest performance. We also looked at how some models rank the features according to their importance. In this paper, we followed the data science process starting with getting the data, then cleaning and preprocessing the data, followed by exploring the data and building models, then evaluating the results and communicating them with visualizations.\n\nAs a recommendation, we advise to use this model (or a version of it trained with more recent data) by people who want to buy a house in the area covered by the dataset to have an idea about the actual price. The model can be used also with datasets that cover different cities and areas provided that they contain the same features. We also suggest that people take into consideration the features that were deemed as most important as seen in the previous section; this might help them estimate the house price better.\n\n<h1 id=\"ref\">References\n\nAlkhatib, K., Najadat, H., Hmeidi, I., & Shatnawi, M. K. A. (2013). Stock price prediction using k-nearest neighbor (kNN) algorithm. International Journal of Business, Humanities and Technology, 3(3), 32-44.\n\nde Abril, I. M., & Sugiyama, M. (2013). Winning the kaggle algorithmic trading challenge with the composition of many models and feature engineering. IEICE transactions on information and systems, 96(3), 742-745.\n\nFeng, Y., & Jones, K. (2015, July). Comparing multilevel modelling and artificial neural networks in house price prediction. In Spatial Data Mining and Geographical Knowledge Services (ICSDM), 2015 2nd IEEE International Conference on (pp. 108-114). IEEE.\n\nHegazy, O., Soliman, O. S., & Salam, M. A. (2014). A machine learning model for stock market prediction. arXiv preprint arXiv:1402.7351.\n\nTicknor, J. L. (2013). A Bayesian regularized artificial neural network for stock market forecasting. Expert Systems with Applications, 40(14), 5501-5506.\n\nDe Cock, D. (2011). Ames, Iowa: Alternative to the Boston housing data as an end of semester regression project. Journal of Statistics Education, 19(3).\n","2eba2867":"<h1 id=\"analysis-comparison\">Analysis and Comparison<\/h1>\n\nIn the previous section, we created many models: for each model, we searched for good parameters then we constructed the model using those parameters, then trained (fitted) the model to our training data (`X_train` and `y_train`), then tested the model on our test data (`X_test`) and finally, we evaluated the model performance by comparing the model predictions with the true values in `y_test`. We used the mean absolute error (MAE) to evaluate model performance. \n\nUsing the results we got in the previous section, we present a table that shows the mean absolute error (MAE) for each model when applied to the test set `X_test`. The table is sorted ascendingly according to MAE score.\n\n|Model                            |MAE      |\n|---------------------------------|---------|\n|XGBoost                          |12556.68 |\n|Support Vector Regression (SVR)  |12874.93 |\n|Random Forest                    |14506.46 |\n|Elastic Net                      |14767.91 |\n|Ridge                            |15270.46 |\n|Neural Network                   |15656.38 |\n|Decision Tree                    |20873.95 |\n|K-Nearest Neighbors (KNN)        |22780.14 |\n\nWe also present a graph that visualizes the table contents:","3be573b6":"Now, we visualize the relationship between `Garage Cars` and `Garage Area` and between `Gr Liv Area` and `TotRms AbvGrd`:","95b685e7":"### Gradient Boosting\n\nFor Gradient Boosting (GB), we will use the renowned XGBoost implementations.\n\nXGBoost model has the following syntax:\n\n```py\nXGBRegressor(max_depth=3, learning_rate=0.1, n_estimators=100, silent=True, \n             objective='reg:linear', booster='gbtree', n_jobs=1, nthread=None, \n             gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, \n             colsample_bytree=1, colsample_bylevel=1, reg_alpha=0, reg_lambda=1, \n             scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, \n             missing=None, importance_type='gain', **kwargs)\n```\n\nFirstly, we will use `GridSearchCV()` to search for the best model parameters in a parameter space provided by us. The parameter `max_depth` sets the maximum depth of a tree, `learning_rate` represents the step size shrinkage used in updating weights, `n_estimators` specifies the number of boosted trees to fit, `booster` determines which booster to use, `gamma` specifies the minimum loss reduction required to make a further partition on a leaf node of the tree, `subsample` is subsample ratio of the training instances; this subsampling will occur once in every boosting iteration, `colsample_bytree` specifies the subsample ratio of columns when constructing each tree, `colsample_bylevel` specifies the subsample ratio of columns for each split, in each level, `reg_alpha` is L1 regularization term, and `reg_lambda` is L2 regularization term.","92c380e3":"## Stock Market Prediction Using A Machine Learning Model\n\nIn another study done by Hegazy, Soliman, and Salam (2014), a system was proposed to predict daily stock market prices. The system combines particle swarm optimization (PSO) and least square support vector machine (LS-SVM), where PSO was used to optimize LV-SVM.\n\nThe authors claim that in most cases, artificial neural networks (ANNs) are subject to the overfitting problem. They state that support vector machines algorithm (SVM) was developed as an alternative that doesn't suffer from overfitting. They attribute this advantage to SVMs being based on the solid foundations of VC-theory. They further elaborate that LS-SVM method was reformulation of traditional SVM method that uses a regularized least squares function with equality constraints to obtain a linear system that satisfies Karush-Kuhn-Tucker conditions for getting an optimal solution.\n\nThe authors describe PSO as a popular evolutionary optimization method that was inspired by organism social behavior like bird flocking. They used it to find the optimal parameters for LS-SVM. These parameters are the cost penalty $C$, kernel parameter $\\gamma$, and insensitive loss function $\\epsilon$.\n\nThe model proposed in the study was based on the analysis of historical data and technical financial indicators and using LS-SVM optimized by PSO to predict future daily stock prices. The model input was six vectors representing the historical data and the technical financial indicators. The model output was the future price. The model used is represented in Figure 2.\n\n![Figure 2: The structure of the model used](https:\/\/s3.eu-north-1.amazonaws.com\/ammar-files\/kaggle-kernels\/House+Price+Prediction+%7C+An+End-to-End+Machine+Learning+Project\/2.png)\n\nRegarding the technical financial indicators, five were derived from the raw data: relative strength index (RSI), money flow index (MFI), exponential moving average (EMA), stochastic oscillator (SO), and moving average convergence\/divergence (MACD). These indicators are known in the domain of stock market.\n\nThe model was trained and tested using datasets taken from https:\/\/finance.yahoo.com\/. The datasets were from Jan 2009 to Jan 2012 and include stock data for many companies like Adobe and HP. All datasets were partitioned into a training set with `70%` of the data and a test set with `30%` of the data. Three models were trained and tested: LS-SVM-PSO model, LS-SVM model, and ANN model. The results obtained in the study showed that LS-SVM-PSO model had the best performance. Figure 3 shows a comparison between the mean square error (MSE) of the three models for the stocks of many companies.\n\n![Figure 3: MSE comparison](https:\/\/s3.eu-north-1.amazonaws.com\/ammar-files\/kaggle-kernels\/House+Price+Prediction+%7C+An+End-to-End+Machine+Learning+Project\/3.png)","2a0ca80d":"#### 1. Ridge Regression\n\nThis model has the following syntax:\n\n```py\nRidge(alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, \n      max_iter=None, tol=0.001, solver=\u2019auto\u2019, random_state=None)\n```\n\nFirstly, we will use `GridSearchCV()` to search for the best model parameters in a parameter space provided by us. \nThe parameter `alpha` represents the regularization strength, `fit_intercept` determines whether to calculate the intercept for this model, and `solver` controls which solver to use in the computational routines.","40bcdf1b":"### Common Important Features\n\nNow, let us see which features are among the most important features for both XGBoost and Random Forest models, and let's find out the difference in their importance regarding the two models:","6efe1cd4":"##### Alley,  Fence, and Fireplace Qu\n\nAccording to the dataset documentation, `NA` in `Alley`, `Fence`, and `Fireplace Qu` columns denotes that the house doesn't have an alley, fence, or fireplace. So we fill in the missing values in these columns with `\"No Alley\"`, `\"No Fence\"`, and `\"No Fireplace\"` accordingly:","2156aa09":"### Neural Network\n\nFor Neural Network (NN), we will use an implementations provided by the Scikit-Learn package.\n\nThe Neural Network model has the following syntax:\n\n```py\nMLPRegressor(hidden_layer_sizes=(100, ), activation=\u2019relu\u2019, solver=\u2019adam\u2019, \n             alpha=0.0001, batch_size=\u2019auto\u2019, learning_rate=\u2019constant\u2019, \n             learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, \n             random_state=None, tol=0.0001, verbose=False, warm_start=False, \n             momentum=0.9, nesterovs_momentum=True, early_stopping=False, \n             validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, \n             n_iter_no_change=10)\n```\n\nFirstly, we will use `GridSearchCV()` to search for the best model parameters in a parameter space provided by us. The parameter `hidden_layer_sizes` is a list where its ith element represents the number of neurons in the ith hidden layer, `activation` specifies the activation function for the hidden layer, `solver` determines the solver for weight optimization, and `alpha` represents L2 regularization penalty.","46bf3cfa":"We can see that this is the same row that contains the missing value in `Garage Area`, and that all garage columns except `Garage Type` are null in this row, so we will fill the missing values in `Garage Cars` and `Garage Area` with 0.","69d40875":"From the plots above, we can see that these eight variables are truly positively correlated with the target variable. However, it's apparent that they are not as highly correlated as `Overall Qual` and `Gr Liv Area`.\n\n### Relatioships Between Predictor Variables\n\n#### Positive Correlation\n\nApart from the target variable, when we plotted the heatmap, we discovered a high positive correlation between `Garage Cars` and `Garage Area` and between `Gr Liv Area` and `TotRms AbvGrd`. We want to visualize these correlations also. We've already seen the distribution of each of them except for `TotRms AbvGrd`. Let us see the distribution of `TotRms AbvGrd` first:","795c8db4":"## Correlation Between Variables\n\nWe want to see how the dataset variables are correlated with each other and how predictor variables are correlated with the target variable. For example, we would like to see how `Lot Area` and `SalePrice` are correlated: Do they increase and decrease together (positive correlation)? Does one of them increase when the other decrease or vice versa (negative correlation)? Or are they not correlated?\n\nCorrelation is represented as a value between -1 and +1 where +1 denotes the highest positive correlation, -1 denotes the highest negative correlation, and 0 denotes that there is no correlation.\n\nWe will show correlation between our dataset variables (numerical and boolean variables only) using a heatmap graph:","b4315640":"## Using K-Nearest Neighbours for Stock Price Prediction\n\nAlkhatib, Najadat, Hmeidi, and Shatnawi (2013) have done a study where they used the k-nearest neighbours (KNN) algorithm to predict stock prices. In this study, they expressed the stock prediction problem as a similarity-based classification, and they represented the historical stock data as well as test data by vectors.\n\nThe authors listed the steps of predicting the closing price of stock market using KNN as follows:\n\n* The number of neaerest neighbours is chosen\n* The distance between the new record and the training data is computed\n* Training data is sorted according to the calculated distance\n* Majority voting is applied to the classes of the k nearest neighbours to determine the predicted value of the new record\n\nThe data used in the study is stock data of five companies listed on the Jordanian stock exchange. The data range is from 4 June 2009 to 24 December 2009. Each of the five companies has around `200` records in the data. Each record has three variables: closing price, low price, and high price. The author stated that the closing price is the most important feature in determining the prediction value of a stock using KNN.\n\nAfter applying KNN algorithm, the authors summarized the prediction performance evaluation using different metrics in a the table shown in Figure 5. \n\n![Figure 5: Prediction performance evaluation](https:\/\/s3.eu-north-1.amazonaws.com\/ammar-files\/kaggle-kernels\/House+Price+Prediction+%7C+An+End-to-End+Machine+Learning+Project\/5.png)\n\nThe authors used lift charts also to evaluate the performance of their model. Lift chart shows the improvement obtained by using the model compared to random estimation. As an example, the lift graph for AIEI company is shown in Figure 6. The area between the two lines in the graph is an indicator of the goodness of the model.\n\n![Figure 6: AIEI lift graph](https:\/\/s3.eu-north-1.amazonaws.com\/ammar-files\/kaggle-kernels\/House+Price+Prediction+%7C+An+End-to-End+Machine+Learning+Project\/6.png)\n\nFigure 7 shows the relationship between the actual price and predicted price for one year for the same company.\n\n![Figure 7: Relationship between actual and predicted price for AIEI](https:\/\/s3.eu-north-1.amazonaws.com\/ammar-files\/kaggle-kernels\/House+Price+Prediction+%7C+An+End-to-End+Machine+Learning+Project\/7.png)\n","0b956807":"Finally, we test our model on `X_test`. Then we evaluate the model performance by comparing its predictions with the actual true values in `y_test` using the MAE metric as we described above:","dfe77c5e":"Now we start dealing with these missing values.\n\n##### Pool QC\n\nThe percentage of missing values in `Pool QC` column is 99.56% which is very high. We think that a missing value in this column denotes that the corresponding house doesn't have a pool. To verify this, let's take a look at the values of `Pool Area` column:","7861b666":"We can clearly see the five values meant by the authour in the plot above. Now, we will remove them from our dataset. We can do so by keeping data points that have `Gr Liv Area` less than 4,000. But first we take a look at the dataset rows that correspond to these unusual values:","28e6f76b":"Finally, we test our model on `X_test`. Then we evaluate the model performance by comparing its predictions with the actual true values in `y_test` using the MAE metric as we described above:","04ed3b62":"We specified the size of the test set to be 25% of the whole dataset. This leaves 75% for the training dataset. Now we have four subsets: `X_train`, `X_test`, `y_train`, and `y_test`. Later we will use `X_train` and `y_train` to train our model, and `X_test` and `y_test` to test and evaluate the model. `X_train` and `X_test` represent features (predictors); `y_train` and `y_test` represent the target. From now on, we will refer to `X_train` and `y_train` as the training dataset, and to `X_test` and `y_test` as the test dataset. Figure 10 shows an example of what `train_test_split()` does.\n\n![Figure 10: train_test_split() operation](https:\/\/s3.eu-north-1.amazonaws.com\/ammar-files\/kaggle-kernels\/House+Price+Prediction+%7C+An+End-to-End+Machine+Learning+Project\/10.png)\n\n## Modeling Approach\n\nFor each one of the techniques mentioned in the previous section (Linear Regression, Nearest Neighbor, Support Vector Machines, etc.), we will follow these steps to build a model:\n\n- Choose an algorithm that implements the corresponding technique\n- Search for an effective parameter combination for the chosen algorithm\n- Create a model using the found parameters\n- Train (fit) the model on the training dataset\n- Test the model on the test dataset and get the results\n\n### Searching for Effective Parameters\n\nUsing Scikit-Learn, we can build a decision-tree model for example as follows:\n\n```py\nmodel = DecisionTreeRegressor(max_depth=14, min_samples_split=5, max_features=20)\n```\n\nWe can do this but to probably achieve a better performance if we choose better values for the parameters `max_depth`, `min_samples_split`, and `max_features`. To do so, we will examine many parameter combinations and choose the combination that gives the best score. Scikit-Learn provides a useful function for that purpose: `GridSearchCV()`. So for the example above, we will do the following:\n\n```py\nparameter_space = {\n    \"max_depth\": [7, 15],\n    \"min_samples_split\": [5, 10],\n    \"max_features\": [30, 45]\n}\n\nclf = GridSearchCV(DecisionTreeRegressor(), parameter_space, cv=4, \n                   scoring=\"neg_mean_absolute_error\")\n                   \nclf.fit(X_train, y_train)\n```\n\nThe code above will test the decision-tree model using all the parameter combinations. It will use *cross validation* with 4 folds and it will use the mean absolute error for scoring and comparing different parameter combinations. At the end, it will provide us with the best parameter combination that achieved the best score so we can use it to build our model.\n\nSometimes, when the number of parameter combinations is large, `GridSearchCV()` can take very long time to run. So in addition to `GridSearchCV()`, we will sometimes use `RandomizedSearchCV()` which is similar to `GridSearchCV()` but instead of using all parameter combinations, it picks a number of random combinations specified by `n_iter`. For the example above, we can use `RandomizedSearchCV()` as follows:\n\n```py\nclf = RandomizedSearchCV(DecisionTreeRegressor(), parameter_space, cv=4, \n                         scoring=\"neg_mean_absolute_error\", n_iter=100)\n```\nThis will make `RandomizedSearchCV()` pick 100 parameter combinations randomly.\n\n## Performance Metric\n\nFor evaluating the performance of our models, we will use mean absolute error (MAE). If $\\hat{y}_i$ is the predicted value of the $i$-th element, and $y$ is the corresponding true value, then for all $n$ elements, RMSE is calculated as:\n\n\\begin{equation*}\n\\text{MAE}(y, \\hat{y}) = \\frac{1}{n} \\sum_{i=1}^{n} \\left| y_i - \\hat{y}_i \\right|.\n\\end{equation*}\n\n## Modeling\n\n### Linear Regression\n\nFor Linear Regression, we will choose three algorithmic implementations: Ridge Regression and Elastic Net. We will use the implementations provided in the Scikit-Learn package of these algorithms.","50c07880":"Now let us see their relationships with the target variable using scatter plots:","dd09f420":"By looking at the table and the graph, we can see that XGBoost model has the smallest MAE, `12556.68` followed by Support Vector Regression model with a little larger error of `12974.93`. After that, Random Forest and Elastic Net models come with similar errors: `14506.46` and `14767.91` respectively. Then come Ridge and Neural Network models with close errors: `15270.46` and `15656.38` respectively. Then comes Decision Tree model with MAE of `20873.95`, and at last, the K-Nearest Neighbors model with an error of `22780.14`.\n\nSo, in our experiment, the best model is XGBoost and the worst model is K-Nearest Neighbors. We can see that the difference in MAE between the best model and the worst model is significant; the best model has almost half of the error of the worst model.\n\n## Performance Interpretation\n\nWe chose the mean absolute error (MAE) as our performance metric to evaluate and compare models. MAE presents a value that is easy to understand; it shows the average value of model error. For example, for our XGBoost model, its MAE is `12556.68` which means that on average, XGBoost will predict a value that is bigger or smaller than the true value by `12556.68`. Now to understand how good this MAE is, we need to know the range and distribution of the data. In our case, we need to see the values of the target variable `SalePrice` which contains the actual house prices. Let's see the violin plot, box plot, and histogram of `SalePrice` in our dataset:","ed96cf53":"### Nearest Neighbors\n\nFor Nearest Neighbors, we will use an implementation of the k-nearest neighbors (KNN) algorithm provided by Scikit-Learn package.\n\nThe KNN model has the following syntax:\n\n```py\nKNeighborsRegressor(n_neighbors=5, weights=\u2019uniform\u2019, algorithm=\u2019auto\u2019, \n                    leaf_size=30, p=2, metric=\u2019minkowski\u2019, metric_params=None, \n                    n_jobs=None, **kwargs)\n```\n\nFirstly, we will use `GridSearchCV()` to search for the best model parameters in a parameter space provided by us. The parameter `n_neighbors` represents `k` which is the number of neighbors to use, `weights` determines the weight function used in prediction: `uniform` or `distance`, `algorithm` specifies the algorithm used to compute the nearest neighbors, `leaf_size` is passed to `BallTree` or `KDTree` algorithm. It can affect the speed of the construction and query, as well as the memory required to store the tree.","7013fd5c":"From the three plots above, we can understand the distribution of `SalePrice`. Now let's get some numerical statistical information about it:","5c1f804a":"Now let's check if there is any remaining missing value in our dataset:","44eb0098":"##### Lot Frontage\n\nAs we saw previously, `Lot Frontage` represents the linear feet of street connected to the house. So we assume that the missing values in this column indicates that the house is not connected to any street, and we fill in the missing values with 0:","7931342d":"We can see that there are many correlated variables in our dataset. Wwe notice that `Garage Cars` and `Garage Area` have high positive correlation which is reasonable because when the garage area increases, its car capacity increases too. We see also that `Gr Liv Area` and `TotRms AbvGrd` are highly positively correlated which also makes sense because when living area above ground increases, it is expected for the rooms above ground to increase too. \n\nRegarding negative correlation, we can see that `Bsmt Unf SF` is negatively correlated with `BsmtFin SF 1`, and that makes sense because when we have more unfinished area, this means that we have less finished area. We note also that `Bsmt Unf SF` is negatively correlated with `Bsmt Full Bath` which is reasonable too.\n\nMost importantly, we want to look at the predictor variables that are correlated with the target variable (`SalePrice`). By looking at the last row of the heatmap, we see that the target variable is highly positively correlated with `Overall Qual` and `Gr Liv Area`. We see also that the target variable is positively correlated with `Year Built`, `Year Remod\/Add`, `Mas Vnr Area`, `Total Bsmt SF`, `1st Flr SF`, `Full Bath`, `Garage Cars`, and `Garage Area`.\n\n### Relatioships Between the Target Variable and Other Varibles\n\n#### High Positive Correlation\n\nFirstly, we want to visualize the relationships between the target variable and the variables that are highly and positively correlated with it, according to what we saw in the heatmap. Namely, these variables are `Overall Qual` and `Gr Liv Area`. We start with the relatioship between the target variable and `Overall Qual`, but before that, let's see the distribution of each of them. Let's start with the target variable `SalePrice`:","e3407941":"##### Mas Vnr Area and Mas Vnr Type\n\nEach of these two columns have 23 missing values. We will fill in these missing values with `\"None\"` for `Mas Vnr Type` and with 0 for `Mas Vnr Area`. We use `\"None\"` for `Mas Vnr Type` because in the dataset documentation, `\"None\"` for `Mas Vnr Type` means \"None\" (i.e. no masonry veneer):","43ea33fa":"We can see that these are the same rows that contain the missing values in `Bsmt Full Bath`, and that one of these two rows is contains the missing value in each of `Total Bsmt SF`, `Bsmt Unf SF`, `BsmtFin SF 2`, and `BsmtFin SF 1` columns. We notice also that `Bsmt Exposure`, `BsmtFin Type 2`, `BsmtFin Type 1`, `Bsmt Qual`, and `Bsmt Cond` are null in these rows, so we will fill the missing values in `Bsmt Half Bath`, `Bsmt Full Bath`, `Total Bsmt SF`, `Bsmt Unf SF`, `BsmtFin SF 2`, and `BsmtFin SF 1` columns with 0.\n\nWe saw that there are 3 rows where `Bsmt Exposure` is null while `BsmtFin Type 1`, `Bsmt Qual`, and `Bsmt Cond` are not null. Let's take a look at these three rows:","40d65a47":"Then we train our model using our training set (`X_train` and `y_train`):","66eb6253":"##### Bsmt Exposure, BsmtFin Type 2, BsmtFin Type 1, Bsmt Qual, Bsmt Cond, Bsmt Half Bath, Bsmt Full Bath, Total Bsmt SF, Bsmt Unf SF, BsmtFin SF 2, and BsmtFin SF 1\n\n\nAccording to the dataset documentation, `NA` in any of the first five of these columns indicates that there is no basement in the house. So we fill in the missing values in these columns with `\"No Basement\"`. We notice that the first five of these columns have 80 missing values, but `BsmtFin Type 2` has 81, `Bsmt Exposure` has 83, `Bsmt Half Bath` and `Bsmt Full Bath` each has 2, and each of the others has 1. Let's take a look at the rows where `Bsmt Half Bath` is null:","1b0c1edb":"We can see that the above-ground living area falls approximately between 800 and 1800 ft<sup>2<\/sup>. Now, let us see the relationship between `Gr Liv Area` and the target variable:","364ceb57":"We defined the parameter space above using reasonable values for chosen parameters. Then we used `GridSearchCV()` with 3 folds (`cv=3`). Now we build our Ridge model with the best parameters found:","989fbba6":"Then we train our model using our training set (`X_train` and `y_train`):","5613bcb0":"We defined the parameter space above using reasonable values for chosen parameters. Then we used `GridSearchCV()` with 3 folds (`cv=3`). Now we build our Random Forest model with the best parameters found:","620c52eb":"This shows us the minimum and maximum values of `SalePrice`. It shows us also the three quartiles represented by the box and the vertical line inside of it. Lastly, we plot the histogram of the variable to see a more detailed view of the distribution:","3a81f8df":"### Decision Tree\n\nFor Decision Tree (DT), we will use an implementations provided by the Scikit-Learn package.\n\nThe Decision Tree model has the following syntax:\n\n```py\nDecisionTreeRegressor(criterion=\u2019mse\u2019, splitter=\u2019best\u2019, max_depth=None, \n                      min_samples_split=2, min_samples_leaf=1, \n                      min_weight_fraction_leaf=0.0, max_features=None, \n                      random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, \n                      min_impurity_split=None, presort=False)\n```\n\nFirstly, we will use `GridSearchCV()` to search for the best model parameters in a parameter space provided by us. The parameter `criterion` specifies the function used to measure the quality of a split, `min_samples_split` determines the minimum number of samples required to split an internal node, `min_samples_leaf` determines the minimum number of samples required to be at a leaf node, and `max_features` controls the number of features to consider when looking for the best split.","4112fd05":"## House Price Prediction Using Multilevel Model and Neural Networks\n\nA different study was done by Feng and Jones (2015) to preduct house prices. Two models were built: a multilevel model (MLM) and an artificial neural network model (ANN). These two models were compared to each other and to a hedonic price model (HPM). \n\nThe multilevel model integrates the micro-level that specifies the relationships between houses within a given neighbourhood, and the macro-level equation which specifies the relationships between neighbouhoods. The hedonic price model is a model that estimates house prices using some attributes such as the number of bedrooms in the house, the size of the house, etc.\n\nThe data used in the study contains house prices in Greater Bristol area between 2001 and 2013. Secondary data was obtained from the Land Registry, the Population Census and Neighbourhood Statistics to be used in order to make the models suitable for national usage. The authors listed many reasons on why they chose the Greater Bristol area such as its diverse urban and rural blend and its different property types. Each record in the dataset contains data about a house in the area: it contains the address, the unit postcode, property type, the duration (freehold or leasehold), the sale price, the date of the sale, and whether the house was newly-built when it was sold. In total, the dataset contains around `65,000` entries. To enable model training and testing, the dataset was divided into a training set that contains data about house sales from 2001 to 2012, and a test set that contains data about house sales in 2013.\n\nThe three models (MLM, ANN, and HPM) were tested using three senarios. In the first senario, locational and measured neighbourhood attributes were not included in the data. In the second senario, grid references of house location were included in the data. In the third senario, measured neighbourhood attributes were included in the data. The models were compared in goodness of fit where $R^2$ was the metric, predictive accuracy where mean absolute error (MAE) and mean absolute percentage error (MAPE) were the metrics, and explanatory power. HPM and MLM models were fitted using MLwiN software, and ANN were fitted using IBM SPSS software. Figure 4 shows the performance of each model regarding fit goodness and predictive accuracy. It shows that MLM model has better performance in general than other models.\n\n![Figure 4: Model performance comparison](https:\/\/s3.eu-north-1.amazonaws.com\/ammar-files\/kaggle-kernels\/House+Price+Prediction+%7C+An+End-to-End+Machine+Learning+Project\/4.png)\n","eb41ad2d":"Finally, we test our model on `X_test`. Then we evaluate the model performance by comparing its predictions with the actual true values in `y_test` using the MAE metric as we described above:","77d14ffb":"### Dealing with Ordinal Variables\n\nThere are some ordinal features in our dataset. For example, the `Bsmt Cond` feature has the following possible values: ","5d075c9c":"We defined the parameter space above using reasonable values for chosen parameters. Then we used `GridSearchCV()` with 3 folds (`cv=3`). Now we build our Decision Tree model with the best parameters found:","949334de":"Finally, we test our model on `X_test`. Then we evaluate the model performance by comparing its predictions with the actual true values in `y_test` using the MAE metric as we described above:","ad4e3520":"<h1 id=\"lr\">Literature Review<\/h1>\n\nIn this section, we look at five recent studies that are related to our topic and see how models were built and what results were achieved in these studies.\n\n## Stock Market Prediction Using Bayesian-Regularized Neural Networks\nIn a study done by Ticknor (2013), he used Bayesian regularized arti\ufb01cial neural network to predict the future operation of financial market. Specifically, he built a model to predict future stock prices. The input of the model is previous stock statistics in addition to some financial technical data. The output of the model is the next-day closing price of the corresponding stocks.\n\nThe model proposed in the study is built using Bayesian regularized neural network. The weights of this type of networks are given a probabilistic nature. This allows the network to penalize very complex models (with many hidden layers) in an automatic manner. This in turn will reduce the overfitting of the model.\n\nThe model consists of a feedforward neural network which has three layers: an input layer, one hidden layer, and an output layer. The author chose the number of neurons in the hidden layer based on experimental methods.The input data of the model is normalized to be between `-1` and `1`, and this opertion is reversed for the output so the predicted price appears in the appropriate scale.\n\nThe data that was used in this study was obtained from Goldman Sachs Group (GS), Inc. and Microsoft Corp. (MSFT) . The data covers 734 trading days (4 January 2010 to 31 December 2012). Each instance of the data consisted of daily statistics: low price, high price, opening price, close price, and trading volume. To facilitate the training and testing of the model, this data was split into training data and test data with `80%` and `20%` of the original data, respectively. In addition to the daily-statistics variables in the data, six more variables were created to reflect financial indicators.\n\nThe performance of the model were evaluated using mean absolute percentage error (MAPE) performance metric. MAPE was calculated using this formula:\n\n\\begin{align}\nMAPE = \\frac{\\sum_{i=1}^r (\\text{abs}( y_i-p_i)\/ y_i )}{r}\\times100%\n\\end{align}\n\nwhere $p_i$ is the predicted stock price on day $i$, $y_i$ is the actual stock price on day $i$, and $r$ is the number of trading days.\n\nWhen applied on the test data, The model achieved a MAPE score of `1.0561` for MSFT part, and `1.3291` for GS part. Figure 1 shows the actual values and predicted values for both GS and MSFT data.\n\n![Figure 1: Predicted vs. actual price](https:\/\/s3.eu-north-1.amazonaws.com\/ammar-files\/kaggle-kernels\/House+Price+Prediction+%7C+An+End-to-End+Machine+Learning+Project\/1.png)","ba82977d":"We saw that there are 2 rows where `Garage Type` is not null while `Garage Cond`, `Garage Qual`, `Garage Finish`, and `Garage Yr Blt` columns are null. Let's take a look at these two rows:","3271a97c":"## Reading the Dataset\n\nThe first step is reading the dataset from the csv file we downloaded. We will use the `read_csv()` function from `Pandas` Python package:","9eaea01d":"We can see the strong correlation between each pair. For `Garage Cars` and `Garage Area`, we see that the highest concentration of data is when `Garage Cars` is 2 and `Garage Area` is approximately between 450 and 600 ft<sup>2<\/sup>. For `Gr Liv Area` and `TotRms AbvGrd`, we notice that the highest concentration is when `Garage Liv Area` is roughly between 800 and 2000 ft<sup>2<\/sup> and `TotRms AbvGrd` is 6.\n\n#### Negative Correlation\n\nWhen we plotted the heatmap, we also discovered a significant negative correlation between `Bsmt Unf SF` and `BsmtFin SF 1`, and between `Bsmt Unf SF` and `Bsmt Full Bath`. We also want to visualize these correlations. Let us see the distribution of these variables first:","cf5746bb":"### Random Forest\n\nNow, let's see the most important features as for Random Forest model:","2f2ce69a":"Let us see what has happened to the `Paved Drive` variable by looking at the same rows above: ","a2743123":"Finally, we test our model on `X_test`. Then we evaluate the model performance by comparing its predictions with the actual true values in `y_test` using the MAE metric as we described above:"}}