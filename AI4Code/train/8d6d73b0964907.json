{"cell_type":{"007aa496":"code","b691c9be":"code","f7c5c140":"code","d70e0b98":"code","9cb10417":"code","16f4f922":"code","30718da0":"code","2a47a942":"code","2074f102":"code","86f68f97":"code","e6d07231":"code","e6cdcd70":"code","685a8b18":"code","4a5126cf":"code","ea641dc6":"code","7fadf2a7":"code","fb2d8b4a":"code","81e1670a":"code","220d9caa":"code","fad0c1df":"code","a3bbe26d":"code","b7549f33":"code","1db55a83":"code","dabfe401":"code","4e74c53d":"code","798b70d0":"code","84b36142":"code","479f76f1":"code","494743b3":"code","3806b578":"code","37830535":"code","2d9ca652":"markdown","724a89a2":"markdown","8b1f1ea5":"markdown"},"source":{"007aa496":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\n# You can write up to 20GB to the current directory (\/kaggle\/working \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b691c9be":"!pip install -U torchvision","f7c5c140":"import torch\nimport torchvision\nfrom torchvision import datasets, models\nfrom torchvision.transforms import functional as FT\nfrom torchvision.transforms import transforms as T\nfrom torch import nn, optim\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader, sampler, random_split, Dataset\nimport copy\nimport math\nimport cv2\nfrom PIL import Image\nimport albumentations as A\nimport cv2\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","d70e0b98":"import warnings\nwarnings.filterwarnings(\"ignore\")\nfrom collections import defaultdict, deque\nimport datetime\nimport time\nfrom tqdm import tqdm\nfrom torchvision.utils import draw_bounding_boxes","9cb10417":"print(torch.__version__)\nprint(torchvision.__version__)","16f4f922":"!pip install pycocotools\nfrom pycocotools.coco import COCO","30718da0":"def collate_fn(batch):\n    return tuple(zip(*batch))","2a47a942":"from albumentations.pytorch import ToTensorV2","2074f102":"def get_albumentation(train):\n    if train:\n        transform = A.Compose([\n            A.Resize(600, 600),\n            A.HorizontalFlip(p=0.5),\n            A.VerticalFlip(p=0.5),\n            A.RandomBrightnessContrast(p=0.2),\n            A.ColorJitter(p=0.1),\n            ToTensorV2()\n        ], bbox_params=A.BboxParams(format='coco'))\n    else:\n        transform = A.Compose([\n            A.Resize(600, 600),\n            ToTensorV2()\n        ], bbox_params=A.BboxParams(format='coco'))\n    return transform","86f68f97":"class AquariumDetection(datasets.VisionDataset):\n    def __init__(\n        self,\n        root: str,\n        split = \"train\",\n        transform= None,\n        target_transform = None,\n        transforms = None,\n    ) -> None:\n        super().__init__(root, transforms, transform, target_transform)\n        self.split = split\n        self.coco = COCO(os.path.join(root, split, \"_annotations.coco.json\"))\n        self.ids = list(sorted(self.coco.imgs.keys()))\n        self.ids = [id for id in self.ids if (len(self._load_target(id)) > 0)]\n\n    def _load_image(self, id: int) -> Image.Image:\n        path = self.coco.loadImgs(id)[0][\"file_name\"]\n        image = cv2.imread(os.path.join(self.root, self.split, path))\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        return image\n\n    def _load_target(self, id: int):\n        return self.coco.loadAnns(self.coco.getAnnIds(id))\n\n    def __getitem__(self, index: int):\n        id = self.ids[index]\n        image = self._load_image(id)\n        target = copy.deepcopy(self._load_target(id))\n        \n        boxes = [t['bbox'] + [t['category_id']] for t in target]\n        if self.transforms is not None:\n            transformed = self.transforms(image=image, bboxes=boxes)\n        \n        image = transformed['image']\n        boxes = transformed['bboxes']\n        new_boxes = []\n        for box in boxes:\n            xmin =  box[0]\n            ymin = box[1]\n            xmax = xmin + box[2]\n            ymax = ymin + box[3]\n            new_boxes.append([xmin, ymin, xmax, ymax])\n        \n        boxes = torch.tensor(new_boxes, dtype=torch.float32)\n        \n        targ = {}\n        targ[\"boxes\"] = boxes\n        targ[\"labels\"] = torch.tensor([t[\"category_id\"]  for t in target], dtype=torch.int64)\n        targ[\"image_id\"] = torch.tensor([t[\"image_id\"]  for t in target])\n        targ[\"area\"] = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        targ[\"iscrowd\"] = torch.tensor([t[\"iscrowd\"]  for t in target], dtype=torch.int64)\n\n        return image.div(255), targ\n\n\n    def __len__(self) -> int:\n        return len(self.ids)","e6d07231":"dataset_path = \"\/kaggle\/input\/aquarium-dataset\/Aquarium Combined\/\"","e6cdcd70":"coc = COCO(os.path.join(dataset_path, \"train\", \"_annotations.coco.json\"))\ncategories = coc.cats\nn_classes = len(categories.keys())\nn_classes, categories","685a8b18":"classes = []\nfor i in categories.items():\n    classes.append(i[1][\"name\"])","4a5126cf":"train_dataset = AquariumDetection(root=dataset_path, transforms=get_albumentation(True))\nval_dataset = AquariumDetection(root=dataset_path, split=\"valid\", transforms=get_albumentation(False))\ntest_dataset = AquariumDetection(root=dataset_path, split=\"test\", transforms=get_albumentation(False))","ea641dc6":"sample = train_dataset[12]\nimg_int = torch.tensor(sample[0] * 255, dtype=torch.uint8)\nplt.imshow(draw_bounding_boxes(img_int, \n                               sample[1]['boxes'], \n                               [classes[i] for i in sample[1]['labels']],\n                               width=4).permute(1, 2, 0)\n          )","7fadf2a7":"len(train_dataset), len(val_dataset), len(test_dataset)","fb2d8b4a":"## USE FASTERRCNN MOBILENET\nmodel = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)\nin_features = model.roi_heads.box_predictor.cls_score.in_features\nmodel.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, n_classes) ","81e1670a":"# backbone = torchvision.models.detection.backbone_utils.mobilenet_backbone(\"mobilenet_v3_large\", pretrained=True, fpn=True)\n# anchor_generator = torchvision.models.detection.anchor_utils.AnchorGenerator([[2, 3] for _ in range(6)], min_ratio=0.2, max_ratio=0.95)\n","220d9caa":"# ## Not working RETINANET ##\n# backbone = torchvision.models.efficientnet_b0(pretrained=True).features\n# backbone.out_channels = 1280\n# anchor_generator = torchvision.models.detection.anchor_utils.AnchorGenerator(sizes=((32, 64, 128, 256, 512),), aspect_ratios=((0.5, 1.0, 2.0),))\n# model = models.detection.RetinaNet(backbone, num_classes=n_classes, min_size=600,\n#                                    anchor_generator=anchor_generator)","fad0c1df":"train_loader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=4, shuffle=True, num_workers=4,\n    collate_fn=collate_fn\n)\nval_loader = torch.utils.data.DataLoader(\n    val_dataset, batch_size=4, shuffle=True, num_workers=4,\n    collate_fn=collate_fn\n)\ntest_loader = torch.utils.data.DataLoader(\n    test_dataset, batch_size=4, shuffle=True, num_workers=4,\n    collate_fn=collate_fn\n)","a3bbe26d":"images,targets = next(iter(train_loader))\nimages = list(image for image in images)\ntargets = [{k: v for k, v in t.items()} for t in targets]\noutput = model(images,targets)","b7549f33":"x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\nmodel.eval()\npredictions = model(x) ","1db55a83":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\ndevice","dabfe401":"model = model.to(device)","4e74c53d":"params = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.01,\n                            momentum=0.9, nesterov=True, weight_decay=1e-4)\n\n# and a learning rate scheduler which decreases the learning rate by\n# 10x every 3 epochs\nlr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[16, 22], gamma=0.1)","798b70d0":"from tqdm import tqdm","84b36142":"def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq):\n    model.to(device)\n    model.train()\n    header = 'Epoch: [{}]'.format(epoch)\n\n    lr_scheduler = None\n    if epoch == 0:\n        warmup_factor = 1.0 \/ 1000\n        warmup_iters = min(1000, len(data_loader) - 1)\n\n        lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n            optimizer, start_factor=warmup_factor, total_iters=warmup_iters\n        )\n\n    all_losses = []\n    all_losses_dict = []\n    \n    for images, targets in tqdm(data_loader):\n        images = list(image.to(device) for image in images)\n        targets = [{k: torch.tensor(v).to(device) for k, v in t.items()} for t in targets]\n\n        loss_dict = model(images, targets)\n\n        losses = sum(loss for loss in loss_dict.values())\n\n        loss_dict_append = {k: v.item() for k, v in loss_dict.items()}\n        losses_reduced = sum(loss for loss in loss_dict.values())\n\n        loss_value = losses_reduced.item()\n        \n        all_losses.append(loss_value)\n        all_losses_dict.append(loss_dict_append)\n        \n        if not math.isfinite(loss_value):\n            print(\"Loss is {}, stopping training\".format(loss_value))\n            print(loss_dict)\n            sys.exit(1)\n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n\n        if lr_scheduler is not None:\n            lr_scheduler.step()\n    \n    all_losses_dict = pd.DataFrame(all_losses_dict)\n    print(\"Epoch {}: lr: {:.6f} loss: {:.6f}, loss_classifier: {:.6f}, loss_box_reg: {:.6f}, loss_rpn_box_reg: {:6f}, loss_objectness: {:.6f}\".format(\n        epoch, optimizer.param_groups[0][\"lr\"], np.mean(all_losses), \n        all_losses_dict[\"loss_classifier\"].mean(),\n        all_losses_dict[\"loss_box_reg\"].mean(),\n        all_losses_dict[\"loss_rpn_box_reg\"].mean(),\n        all_losses_dict[\"loss_objectness\"].mean(),\n    ))\n    \n","479f76f1":"# let's train it for 10 epochs\nnum_epochs = 7\n\nfor epoch in range(num_epochs):\n    train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq=28)\n    lr_scheduler.step()","494743b3":"model.eval()\ntorch.cuda.empty_cache()","3806b578":"img, _ = test_dataset[32]\nimg_int = torch.tensor(img * 255, dtype=torch.uint8)\nwith torch.no_grad():\n    prediction = model([img.to(device)])\n    pred = prediction[0]","37830535":"fig = plt.figure(figsize=(14, 10))\nplt.imshow(draw_bounding_boxes(img_int, \n                               pred['boxes'][pred['scores'] > 0.8], \n                               [classes[i] for i in pred['labels'][pred['scores'] > 0.8].tolist()],\n                               width=4).permute(1, 2, 0)\n          )","2d9ca652":"## Dataset","724a89a2":"## Model","8b1f1ea5":"## Transforms"}}