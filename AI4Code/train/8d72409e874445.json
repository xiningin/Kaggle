{"cell_type":{"e17773df":"code","83df88b8":"code","d2a691fa":"code","97e376ba":"code","2969b35a":"code","edb8df05":"code","ff295a8d":"code","d026d002":"code","4b61983f":"code","99b83b5b":"code","3b0815a1":"code","d2056016":"code","d6440f7c":"markdown","0277f601":"markdown","c6f200a7":"markdown"},"source":{"e17773df":"import os\nimport random\nimport math\nimport sys\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport lightgbm as lgb\nfrom cuml.svm import SVR\nimport tensorflow as tf\nfrom tensorflow.keras import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, Input, Concatenate, BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt","83df88b8":"# Constants\nAUTOTUNE = tf.data.experimental.AUTOTUNE  \nimg_size = 224\nchannels = 3\nBatch_size = 16\n\n# Directory for dataset\ntrain_dir = \"\/kaggle\/input\/petfinder-pawpularity-score\/train\/\"\ntest_dir = \"\/kaggle\/input\/petfinder-pawpularity-score\/test\/\"\n\ndef seed_everything():\n    os.environ['PYTHONHASHSEED'] = str(123)\n    np.random.seed(123)\n    random.seed(123)\n    tf.random.set_seed(123)\n    os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '2'\n    os.environ['PYTHONHASHSEED'] = str(123)\n\nseed_everything()","d2a691fa":"# Reading dataset train, test in df and df_test respectively\ndf = pd.read_csv(\"\/kaggle\/input\/petfinder-pawpularity-score\/train.csv\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/petfinder-pawpularity-score\/test.csv\")\nId = df_test[\"Id\"].copy()\n\n\n# Converting Id column for taking images\ndf[\"Id\"] = df[\"Id\"].apply(lambda x : \"\/kaggle\/input\/petfinder-pawpularity-score\/train\/\" + x + \".jpg\")\ndf_test[\"Id\"] = df_test[\"Id\"].apply(lambda x : \"\/kaggle\/input\/petfinder-pawpularity-score\/test\/\" + x + \".jpg\")","97e376ba":"# Defining functions for reading and augmentation of images\n# A seperate function for creating dataset\n\n# Augmenting the image\ndef image_preprocess(is_labelled):  \n    def augment(image):\n        image = tf.image.random_flip_left_right(image)\n        image = tf.image.random_flip_up_down(image)\n        image = tf.image.random_contrast(image, 0.95, 1.05)\n        return image\n    \n    def can_be_augmented(img, label):\n        return augment(img), label\n    \n#   If record has label both image and lable will be returned\n    return can_be_augmented if is_labelled else augment\n\n\n\n# Reading and rescaling images\ndef image_read(is_labelled):\n    def decode(path):\n        image = tf.io.read_file(path)\n        image = tf.image.decode_jpeg(image, channels=channels)\n        image = tf.cast(image, tf.float32)\n        image = tf.image.resize(image, (img_size, img_size))\n        image = tf.keras.applications.efficientnet.preprocess_input(image)\n        return image\n    \n    def can_be_decoded(path, label):\n        return decode(path), label\n    \n#   If record has label both image and lable will be returned\n    return can_be_decoded if is_labelled else decode\n\n\n# Creating the dataset\ndef create_dataset(df, df_meta, batch_size, is_labelled = False, augment = False, shuffle = False):\n    image_read_fn = image_read(is_labelled)\n    image_preprocess_fn = image_preprocess(is_labelled)\n    \n    if is_labelled:\n        dataset = tf.data.Dataset.from_tensor_slices((df[\"Id\"].values, df_meta.values))\n    else:\n        dataset = tf.data.Dataset.from_tensor_slices((df[\"Id\"].values))\n    \n    dataset = dataset.map(image_read_fn, num_parallel_calls=AUTOTUNE)\n    dataset = dataset.map(image_preprocess_fn, num_parallel_calls=AUTOTUNE) if augment else dataset\n#     dataset = dataset.shuffle(1024, reshuffle_each_iteration=True) if shuffle else dataset\n#     dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(AUTOTUNE)\n    return dataset\n\ndef create_metadata_dataset(df, batch_size):\n    dataset = tf.data.Dataset.from_tensor_slices((df[\"Pawpularity\"].values))\n#     dataset = dataset.shuffle(1024, reshuffle_each_iteration=True)\n#     dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(AUTOTUNE)\n    return dataset \n\ndef create_metadata_dataset_test(df, batch_size):\n    dataset = tf.data.Dataset.from_tensor_slices((df.values))\n#     dataset = dataset.shuffle(1024, reshuffle_each_iteration=True)\n#     dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(AUTOTUNE)\n    return dataset\n\ndef create_metadata(df, lab,batch_size):\n    dataset = tf.data.Dataset.from_tensor_slices((df.values, lab['Pawpularity'].values))\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(AUTOTUNE)\n    return dataset\n\ndef create_metadata_test(df,batch_size):\n    dataset = tf.data.Dataset.from_tensor_slices((df.values))\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(AUTOTUNE)\n    return dataset","2969b35a":"# Defining train, validation and test set \ntrn = df.iloc[:9000]\nval = df.iloc[9001:]\ntes = np.zeros((df_test.shape[0],),dtype='float32')\ntes = pd.DataFrame(tes)\n\nxtrain_att_trn = trn.drop([\"Id\", \"Pawpularity\"],axis=1)\nxtrain_att_val = val.drop([\"Id\", \"Pawpularity\"],axis=1)\ntest_att = df_test.drop([\"Id\"],axis=1)\nxtrain_att_trn.astype('float32')\nxtrain_att_val.astype('float32')\ntest_att.astype('float32')\ntrain = create_dataset(trn, xtrain_att_trn,Batch_size, is_labelled = True, augment = False, shuffle = False)\nvalidation = create_dataset(val, xtrain_att_val,Batch_size, is_labelled = True, augment = False, shuffle = False)\ntest = create_dataset(df_test, test_att,Batch_size, is_labelled = True, augment = False, shuffle=False)\ntrain_att = create_metadata(xtrain_att_trn, trn,Batch_size)\ntest_att = create_metadata_test(test_att,Batch_size)\n\n# Defining xtrain_att, ytrain_att for MLP\n\nytrain_trn = create_metadata_dataset(trn, Batch_size)\nytrain_val = create_metadata_dataset(val, Batch_size)\ntest_val = create_metadata_dataset_test(tes, Batch_size)\n\ntrain = tf.data.Dataset.zip((train, ytrain_trn)).batch(Batch_size)\nvalidation = tf.data.Dataset.zip((validation, ytrain_val)).batch(Batch_size)\ntest = tf.data.Dataset.zip((test, test_val)).batch(Batch_size)","edb8df05":"# Loading pretrained EfficientNet\nimg_mod = \"\/kaggle\/input\/keras-applications-models\/EfficientNetB0.h5\"\nefnet = tf.keras.models.load_model(img_mod)\nefnet.trainable = False","ff295a8d":"# Using pretrained EfficientNet with image size of (224,224,3)\n# Using relu activation function because it a regression problem\n\ninp1 = Input(shape=(img_size, img_size, channels))\nx = efnet(inp1)\nx = BatchNormalization()(x)\nx = Dropout(0.2)(x)\ninp2 = Input(shape=(12))\ncon = Concatenate()([x, inp2])\nx = Dense(units = 64, activation=\"relu\")(con)\nout = Dense(units = 1)(x)\n\nmodel = Model([inp1, inp2], out)","d026d002":"# Early stopping helps as it stops training if val_loss(validation score) does not decrease.\nearly_stopping = EarlyStopping(patience = 5,restore_best_weights=True)\n\n\nlr_schedule = ExponentialDecay(\n    initial_learning_rate=1e-3,\n    decay_steps=100, decay_rate=0.96,\n    staircase=True)","4b61983f":"# Compiling and Fitting the model\nmodel.compile(loss=\"mse\", \n              optimizer = tf.keras.optimizers.Adam(learning_rate = lr_schedule), \n              metrics=[tf.keras.metrics.RootMeanSquaredError()])\n\npredictor = model.fit(train,\n                      epochs=25, \n                      validation_data = validation,\n                      callbacks = [early_stopping])","99b83b5b":"mlp = Sequential([\n    Input(shape=(12,)),\n    Dense(128, activation=\"relu\"),\n    Dense(64, activation=\"relu\"),\n    Dense(1, activation=\"relu\")\n])\n\nmlp.compile(loss=\"mse\", \n              optimizer = tf.keras.optimizers.Adam(learning_rate = lr_schedule), \n              metrics=[tf.keras.metrics.RootMeanSquaredError()])\n\nmlp_predictor = mlp.fit(train_att,\n                      epochs=25, \n                      callbacks = [early_stopping])","3b0815a1":"# Making prediction on test set\npred1 = model.predict(test)\npred2 = mlp.predict(test_att)\n\nfinal=pd.DataFrame()\nfinal['Id']=Id\nfinal['Pawpularity']=(pred1 + pred2) \/ 2\nfinal.to_csv('submission.csv',index=False)","d2056016":"final.head()","d6440f7c":"## Prediction","0277f601":"\n## CNN for Images","c6f200a7":"## Preprocessing"}}