{"cell_type":{"4035193e":"code","0941cdb5":"code","e5b587fe":"code","7d9352da":"code","cefb041b":"code","c34d068b":"code","c65e568b":"code","e622f8c2":"code","8991293c":"code","d8ce1691":"code","58563007":"code","05c7318d":"code","392b0fec":"code","6056a142":"code","6d883c48":"code","b697f36a":"code","46c5812e":"code","60d11680":"code","ad803719":"code","ae772f23":"code","33bc7560":"code","6bdd6d6f":"code","5728d8f6":"code","4414d808":"code","4bfce467":"code","bcaf8402":"code","9be09010":"code","2e5d8914":"code","071a21c1":"markdown","028a7bbb":"markdown","5bfdeea3":"markdown","58a3db17":"markdown","a398e00c":"markdown","4a4b4782":"markdown","4e2fd511":"markdown","c8d30d08":"markdown","221d5659":"markdown","644614cc":"markdown","0aee986e":"markdown","9c455134":"markdown","6114e21e":"markdown","a33c580d":"markdown","72d4b041":"markdown","24cd58c3":"markdown","b34c60d2":"markdown","5418bcb2":"markdown","33fe1239":"markdown","3722b1a2":"markdown"},"source":{"4035193e":"import os\nimport gc\nimport datetime\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\nimport lightgbm as lgb\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n","0941cdb5":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df\n\ndef import_data(file):\n    \"\"\"create a dataframe and optimize its memory usage\"\"\"\n    df = pd.read_csv(file, parse_dates=True, keep_date_col=True, index_col='TransactionID')\n    df = reduce_mem_usage(df)\n    return df\n\n\ndef aggreg(columns, userid, aggr='mean'):\n    \"\"\"\n       Grouping the selected variables according to \"userids\", taking the averages and \n       assigning them to the new variable according to userids\n       \n    \"\"\"\n    \n    \n    for col in columns:\n        # create the new colunm name\n        new_col_name = col+'_'+userid+'_'+aggr \n        df_temp = pd.concat([X_train[[userid, col]], X_test[[userid,col]]]) \n        df_temp.loc[df_temp[col]==-1,col] = np.nan \n        \n        # grouping column by userid\n        df_temp = df_temp.groupby(userid)[col].agg([aggr]).reset_index().rename(columns={aggr: new_col_name})\n        df_temp.index = list(df_temp[userid]) \n        df_temp = df_temp[new_col_name].to_dict()  \n        \n        # Add these average values to Train and Test sets according to userid with the name \"new_col_name\"\n        X_train[new_col_name] = X_train[userid].map(df_temp).astype('float32')\n        X_test[new_col_name]  = X_test[userid].map(df_temp).astype('float32')\n        \n        # Writes -1 instead of \"nan\" values in newly created variables.\n        X_train[new_col_name].fillna(-1,inplace=True)\n        X_test[new_col_name].fillna(-1,inplace=True)\n      \n\n\n    \ndef aggreg_uniq(columns, userid):\n    \n    \"\"\"\n        Variables in columns are grouped by userid and unique values \u200b\u200bin this column are counted and\n        the total number of each unique value is assigned across the \"userid\" in Test and Train sets.\n    \"\"\"\n    for col in columns:  \n        df = pd.concat([X_train[[userid,col]],X_test[[userid,col]]],axis=0)\n        uniq = df.groupby(userid)[col].agg(['nunique'])['nunique'].to_dict()\n        \n        X_train[col+'_count'] = X_train[userid].map(uniq).astype('float32')\n        X_test[col+'_count'] = X_test[userid].map(uniq).astype('float32')\n    \n    \ndef num_positiv(X_train,X_test):\n    \n    \"\"\"\n       We increase each value by the minimum value in the Train and Test set, so there is no negative value \n       and the minimum value becomes 0. The purpose in doing this is when we assign -1 to NAN values, \n       it can be perceived as a separate class.\n    \"\"\"\n    for f in X_train.columns:  \n        \n        if f not in ['TransactionAmt','TransactionDT',\"isFraud\"]: \n            mn = np.min((X_train[f].min(),X_test[f].min())) \n            X_train[f] -= np.float32(mn)  \n            X_test[f] -= np.float32(mn)\n            \n            X_train[f].fillna(-1,inplace=True)  \n            X_test[f].fillna(-1,inplace=True)  \n            \n\ndef class_freq(cols):\n    \"\"\" \n       The \"class_freq\" function normalizes the specified columns in the entered data sets,\n       converts their types to \"float32\" and adds them to the data sets as a new variable with \"_freq\" extension.\n    \"\"\"\n    \n    for col in cols:\n        df = pd.concat([X_train[col],X_test[col]])\n        vc = df.value_counts(dropna=True).to_dict()  \n        vc[-1] = -1  \n        nm = col+'_freq' \n        X_train[nm] = X_train[col].map(vc)  \n        X_test[nm] = X_test[col].map(vc) \n        del df; x=gc.collect()\n        \n\n        \ndef factorize_categoric():    \n    \n    \"\"\"\n       Factorizing process is performed for all categoric (object) variables, and\n       factorize function keeps nan values as -1.\n    \"\"\"\n    for col in X_train.select_dtypes(include=['category','object']).columns:\n        df = pd.concat([X_train[col],X_test[col]])\n        df,_ = df.factorize(sort=True)\n        X_train[col] = df[:len(X_train)].astype('int32')\n        X_test[col] = df[len(X_train):].astype('int32')\n        del df; x=gc.collect()        \n        \n\n        \n\ndef user_id(col1,col2):\n    \n    \"\"\"\n       Converts the values \u200b\u200bin 2 columns to string and combines them \n       with \"_\" to create a string type new variable.\n       \n    \"\"\"\n    us_id = col1+'_'+col2\n    \n    X_train[us_id] = X_train[col1].astype(str)+'_'+X_train[col2].astype(str)\n    X_test[us_id] = X_test[col1].astype(str)+'_'+X_test[col2].astype(str)","e5b587fe":"%%time\n\nprint('Loading data...')\n\ntrain_id = import_data(\"..\/input\/ieee-fraud-detection\/train_identity.csv\")\nprint('\\tSuccessfully loaded train_identity!')\n\nX_train = import_data('..\/input\/ieee-fraud-detection\/train_transaction.csv')\nprint('\\tSuccessfully loaded train_transaction!')\nX_train = X_train.merge(train_id, how='left', left_index=True, right_index=True) \n\ntest_id = import_data('..\/input\/ieee-fraud-detection\/test_identity.csv')\nprint('\\tSuccessfully loaded test_identity!')\n\nX_test = import_data('..\/input\/ieee-fraud-detection\/test_transaction.csv')\nprint('\\tSuccessfully loaded test_transaction!')\n\ntest_id.columns = train_id.columns\nX_test = X_test.merge(test_id, how='left', left_index=True, right_index=True)  \n\npd.set_option('max_columns', None)\n\n# TARGET\ny_train = X_train['isFraud'].copy()  \n\nprint('Data was successfully loaded!\\n')","7d9352da":"nan_groups={}\nv_cols = ['V'+str(i) for i in range(1,340)]\nfor i in X_train.columns:\n    nan_sum = X_train[i].isna().sum()\n    try:\n        nan_groups[nan_sum].append(i)\n    except:\n        nan_groups[nan_sum]=[i]\n\nfor i,j in nan_groups.items():\n    print('The Sum of the NaN Values =',i)\n    print(j)\n    \n    \n\nnon_group_list=list()\nfor i,j in nan_groups.items():\n    if len(j)>5:\n        if i != 0:\n            non_group_list.append(i)\n            \n            \n# Variable groups with a correlation value of more than 0.70 within the groups\n\n# V1 - V11 \ngrp1 = [[1],[2,3],[4,5],[6,7],[8,9],[10,11]]\n# V12 - V34\ngrp2 = [[12,13],[14],[15,16,17,18,21,22,31,32,33,34],[19,20],[23,24],[25,26],[27,28],[29,30]]\n# V35 - V52\ngrp3 = [[35,36],[37,38],[39,40,42,43,50,51,52],[41],[44,45],[46,47],[48,49]]\n# V53 - V74\ngrp4 = [[53,54],[55,56],[57,58,59,60,63,64,71,72,73,74],[61,62],[65],[66,67],[68],[69,70]]\n# V74 - V94\ngrp5 = [[75,76],[77,78],[79,80,81,84,85,92,93,94],[82,83],[86,87],[88],[89],[90,91]]\n# V95 - V107\ngrp6 = [[95,96,97,101,102,103,105,106],[98],[99,100],[104]]\n# V107 - V123\ngrp7 = [[107],[108,109,110,114],[111,112,113],[115,116],[117,118,119],[120,122],[121],[123]]\n# V124 - V137\ngrp8 = [[124,125],[126,127,128,132,133,134],[129],[130,131],[135,136,137]]\n# V138 - V163\ngrp9 = [[138],[139,140],[141,142],[146,147],[148,149,153,154,156,157,158],[161,162,163]]\n# V167 - V183\ngrp10 = [[167,168,177,178,179],[172,176],[173],[181,182,183]]\n# V184 - V216\ngrp11 = [[186,187,190,191,192,193,196,199],[202,203,204,211,212,213],[205,206],[207],[214,215,216]]\n# V217 - V238\ngrp12 = [[217,218,219,231,232,233,236,237],[223],[224,225],[226],[228],[229,230],[235]]\n# V240 - V262\ngrp13 = [[240,241],[242,243,244,258],[246,257],[247,248,249,253,254],[252],[260],[261,262]]\n# V263 - V278\ngrp14 = [[263,265,264],[266,269],[267,268],[273,274,275],[276,277,278]]\n# V220 - V272\ngrp15 = [[220],[221,222,227,245,255,256,259],[234],[238,239],[250,251],[270,271,272]]\n# V279 - V299\ngrp16 = [[279,280,293,294,295,298,299],[284],[285,287],[286],[290,291,292],[297]]\n# V302 - V321\ngrp17 = [[302,303,304],[305],[306,307,308,316,317,318],[309,311],[310,312],[319,320,321]]\n# V281 V315\ngrp18 = [[281],[282,283],[288,289],[296],[300,301],[313,314,315]]\n# V322 - V339\ngrp19 = [[322,323,324,326,327,328,329,330,331,332,333],[325],[334,335,336],[337,338,339]]\n\n\ngrp_list = [grp1,grp2,grp3,grp4,grp5,grp6,grp7,grp8,grp9,grp10,\n            grp11,grp12,grp13,grp14,grp15,grp16,grp17,grp18,grp19]\n\n\n\n\ndef clip_group(group,df):\n    \"\"\"\n      Selects the higher number of unique values from the same correlated variables\n      \n    \"\"\"\n    clipped_list = []\n    for i in group:\n        maximum = 0; \n        V_num = i[0]\n        for j in i:\n            n = df['V'+str(j)].value_counts().count()\n            if n>maximum:\n                maximum = n\n                V_num = j\n            \n        clipped_list.append(V_num)\n    \n        \n    print('Variables in the clipped_list: ',clipped_list)\n    return clipped_list\n\n\n\n# V variables that were decided to be used in the model as a result of the correlation were kept in the V_clipped_cols variable.\nV_clipped_cols = list()\nfor i in grp_list:\n    for j in clip_group(i,X_train):\n        V_clipped_cols.append(\"V\"+str(j))\n        \n\nfor i in range (1, 339):\n    name = \"V\"+str(i)\n    if name not in V_clipped_cols:\n        X_train.drop(\"V\"+str(i),axis=1, inplace=True)\n        X_test.drop(\"V\"+str(i),axis=1, inplace=True)","cefb041b":"valid_card = pd.concat([X_train[['card1']], X_test[['card1']]])\nvalid_card = valid_card['card1'].value_counts()\nvalid_card_std = valid_card.values.std()\n\ninvalid_cards = valid_card[valid_card<=2]\n\nvalid_card = valid_card[valid_card>2]\nvalid_card = list(valid_card.index)\n\nX_train['card1'] = np.where(X_train['card1'].isin(X_test['card1']), X_train['card1'], np.nan)\nX_test['card1']  = np.where(X_test['card1'].isin(X_train['card1']), X_test['card1'], np.nan)\n\nX_train['card1'] = np.where(X_train['card1'].isin(valid_card), X_train['card1'], np.nan)\nX_test['card1']   = np.where(X_test['card1'].isin(valid_card), X_test['card1'], np.nan)\n\n\n# Making values \"nan\" if a value is not common in the Train and Test set\nfor col in ['card2','card3','card4','card5','card6']: \n    X_train[col] = np.where(X_train[col].isin(X_test[col]), X_train[col], np.nan)\n    X_test[col]  = np.where(X_test[col].isin(X_train[col]), X_test[col], np.nan)\n","c34d068b":"col_1 = 'card1'\ncol_2 = 'P_emaildomain'\ncol_3 = 'addr1'\n\n\nuser_id(col_1,col_2)\nuser_id(col_1+'_'+col_2,col_3)\nX_train.drop(col_1+'_'+col_2, axis = 1, inplace=True)\nX_test.drop(col_1+'_'+col_2, axis = 1, inplace=True)\n\nus_id = col_1 + '_' + col_2 + '_' + col_3\nX_train.rename(columns={us_id: 'userid'}, inplace=True)\nX_test.rename(columns={us_id: 'userid'}, inplace=True)","c65e568b":"for df in [X_train,X_test]:\n\n    df['OS_id_30'] = df['id_30'].str.split(' ', expand=True)[0]\n    df['version_id_30'] = df['id_30'].str.split(' ', expand=True)[1]\n\n    df['browser_id_31'] = df['id_31'].str.split(' ', expand=True)[0]\n    df['version_id_31'] = df['id_31'].str.split(' ', expand=True)[1]","e622f8c2":"for df in [X_train,X_test]:\n\n    df['TransactionAmt'] = df['TransactionAmt'].astype('float32')\n    df['Trans_min_std'] = (df['TransactionAmt'] - df['TransactionAmt'].mean()) \/ df['TransactionAmt'].std()","8991293c":"X_train[\"lastest_browser\"] = np.zeros(X_train.shape[0])\nX_test[\"lastest_browser\"] = np.zeros(X_test.shape[0])\n\ndef setBrowser(df):\n    \n    df.loc[df[\"id_31\"]==\"samsung browser 7.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"opera 53.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"mobile safari 10.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"google search application 49.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"firefox 60.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"edge 17.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 69.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 67.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 63.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 63.0 for ios\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 64.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 64.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 64.0 for ios\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 65.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 65.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 65.0 for ios\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 66.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 66.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 66.0 for ios\",'lastest_browser']=1\n    return df\n\nX_train=setBrowser(X_train)\nX_test=setBrowser(X_test)","d8ce1691":"us_emails = ['gmail', 'net', 'edu']\n\nfor df in [X_train,X_test]:\n    for c in ['P_emaildomain', 'R_emaildomain']:\n\n        df[c + '_suffix'] = df[c].map(lambda x: str(x).split('.')[-1])\n        df[c + '_suffix'] = df[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')","58563007":"# Predicting foreign countries by exchange rate\nfor df in [X_train,X_test]:\n    \n    df['TransactionAmt_decimal_lenght'] = df['TransactionAmt'].astype(str).str.split('.', expand=True)[1].str.len()\n    df['cents'] = (df['TransactionAmt'] - np.floor(X_train['TransactionAmt'])).astype('float32')","05c7318d":"p = 'P_emaildomain'\nr = 'R_emaildomain'\nunknown = 'email_not_provided'\n\ndef setDomain(df):\n    df[p] = df[p].astype('str')\n    df[r] = df[r].astype('str')\n    \n    df[p] = df[p].fillna(unknown)\n    df[r] = df[r].fillna(unknown)\n    \n    df['email_check'] = np.where((df[p]==df[r])&(df[p]!=unknown),1,0)\n\n    df[p+'_prefix'] = df[p].apply(lambda x: x.split('.')[0])\n    df[r+'_prefix'] = df[r].apply(lambda x: x.split('.')[0])\n    \n    return df\n    \nX_train=setDomain(X_train)\nX_test=setDomain(X_test)","392b0fec":"# Listing dates between '2017-10-01' and '2019-01-01 \ndates_range = pd.date_range(start='2017-10-01', end='2019-01-01')\n\n# US national holidays are listed between '2017-10-01' and '2019-01-01 \nus_holidays = calendar().holidays(start=dates_range.min(), end=dates_range.max())\n\nSTART_DATE = '2017-12-01'\nstartdate = datetime.datetime.strptime(START_DATE, \"%Y-%m-%d\")\n\n\n# The variable of the hour of day, day of week and day of month and month of year were created.\nfor df in [X_train,X_test]:\n    \n    df[\"Date\"] = df['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds=x)))\n    df['_Weekdays'] = df['Date'].dt.dayofweek\n    df['_Dayhours'] = df['Date'].dt.hour\n    df['_Monthdays'] = df['Date'].dt.day\n    df['_Yearmonths'] = (df['Date'].dt.month).astype(np.int8) \n\n    \n    # Is the transaction done on holiday?\n    df['is_holiday'] = (df['Date'].dt.date.astype('datetime64').isin(us_holidays)).astype(np.int8)\n\n    df.drop(\"Date\", axis=1,inplace=True)","6056a142":"for col in ['ProductCD','M4']:\n    temp_dict = X_train.groupby([col])['isFraud'].agg(['mean']).reset_index().rename(columns={'mean': col+'_target_mean'})\n    temp_dict.index = temp_dict[col].values\n    temp_dict = temp_dict[col+'_target_mean'].to_dict()\n    \n    if col=='ProductCD':\n        X_train['ProductCD_1'] = X_train[col].map(temp_dict)\n        X_test['ProductCD_1']  = X_test[col].map(temp_dict)\n    else:\n        X_train['M4_1'] = X_train[col].map(temp_dict)\n        X_test['M4_1']  = X_test[col].map(temp_dict)\n        \n        \n# Dropping 'ProductCD' and 'M4'\nX_train.drop(['ProductCD','M4'], axis=1,inplace=True)\nX_test.drop(['ProductCD','M4'], axis=1,inplace=True)","6d883c48":"for i in range(1,16):\n    if i in [1,2,3,5,9]:\n        continue\n    X_train['D'+str(i)] =  X_train['D'+str(i)] - X_train.TransactionDT\/np.float32(24*60*60)\n    X_test['D'+str(i)] = X_test['D'+str(i)] - X_test.TransactionDT\/np.float32(24*60*60)","b697f36a":"for df in [X_train,X_test]:\n\n    df['mean_last'] = df['TransactionAmt'] - df.groupby('userid')['TransactionAmt'].transform(lambda x: x.rolling(10, 1).mean())\n    df['min_last'] = df.groupby('userid')['TransactionAmt'].transform(lambda x: x.rolling(10, 1).min())\n    df['max_last'] = df.groupby('userid')['TransactionAmt'].transform(lambda x: x.rolling(10, 1).max())\n    df['std_last'] = df['mean_last'] \/ df.groupby('userid')['TransactionAmt'].transform(lambda x: x.rolling(10, 1).std())\n\n    df['mean_last'].fillna(0, inplace=True, )\n    df['std_last'].fillna(0, inplace=True)\n\n    df['TransactionAmt_to_mean_card_id'] = df['TransactionAmt'] - df.groupby(['userid'])['TransactionAmt'].transform('mean')\n    df['TransactionAmt_to_std_card_id'] = df['TransactionAmt_to_mean_card_id'] \/ df.groupby(['userid'])['TransactionAmt'].transform('std')\n    \n    \n    # Replaces infinite values with 999\n    df = df.replace(np.inf,999)","46c5812e":"factorize_categoric()\n\nnum_positiv(X_train,X_test)\n\nclass_freq(['addr1','card1','card2','card3','P_emaildomain'])\n\naggreg(['TransactionAmt','D4','D9','D10','D11','D15'],'userid','mean')\n\naggreg(['TransactionAmt','D4','D9','D10','D11','D15','C14'],'userid','std')\n\naggreg(['C'+str(x) for x in range(1,15) if x!=3],'userid','mean')\n\naggreg(['M'+str(x) for x in range(1,10) if x!=4],'userid','mean')\n\naggreg_uniq(['P_emaildomain','dist1','id_02','cents','C13','V314','V127','V136','V309','V307','V320'],'userid')","60d11680":"START_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')\nX_train['DT_M'] = X_train['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\nX_train['DT_M'] = (X_train['DT_M'].dt.year-2017)*12 + X_train['DT_M'].dt.month \n\nX_test['DT_M'] = X_test['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\nX_test['DT_M'] = (X_test['DT_M'].dt.year-2017)*12 + X_test['DT_M'].dt.month ","ad803719":"# Reducing the memory usage\nX_train = reduce_mem_usage(X_train)\nX_test = reduce_mem_usage(X_test)","ae772f23":"# Dropping userid to prevent overfitting\nX_train.drop(\"userid\", axis=1, inplace=True)\nX_test.drop(\"userid\", axis=1, inplace=True)\n\n# Dropping target variable from Train set\nX_train.drop(\"isFraud\", axis=1, inplace=True)","33bc7560":"# Splitting Train set\nX_train1, X_test1, y_train1, y_test1 = train_test_split(X_train, y_train, test_size=0.33, random_state=42)","6bdd6d6f":"# Parameter Tuning\nmodel = lgb.LGBMClassifier()\nparam_dist = {\"max_depth\": [5,10,15],\n              \"learning_rate\" : [0.1,0.15,0.3],\n              \"num_leaves\": [32,150,200],\n              \"n_estimators\": [300,400],\n              'is_unbalance': [True],\n              'boost_from_average': [False],\n              'device': ['gpu'],\n              'gpu_platform_id': [0],\n              'gpu_device_id': [0],\n              \"random_state\": [2]}\n\n\ngrid_search = GridSearchCV(model, param_grid=param_dist, cv = 3, \n                                   verbose=10, n_jobs=-1)\n\n\n\n\ngrid_search.fit(X_train1, y_train1)","5728d8f6":"# Best values of the hyperparameters\nmax_depth_best = grid_search.best_estimator_.max_depth\nnum_leaves_best = grid_search.best_estimator_.num_leaves\nn_estimators_best = grid_search.best_estimator_.n_estimators\nlearning_rate_best = grid_search.best_estimator_.learning_rate","4414d808":"del model \nx=gc.collect()","4bfce467":"#df_train = X_train.drop(X_train.index[590520:])\n#y_train_drop = y_train.drop(y_train.index[590520:])\n","bcaf8402":"groups = X_train['DT_M']\nkf = KFold(n_splits=6, random_state=42)\nkf.get_n_splits(X_train, y_train, groups)\n\npreds = np.zeros(len(X_test))\ncount = 0\n\nfor train_index, test_index in kf.split(X_train, y_train, groups):\n    print(\"\\nTRAIN:\", train_index, \"TEST:\", test_index)\n    X_train_df, X_test_df = X_train.iloc[train_index], X_train.iloc[test_index]\n    y_train_df, y_test_df = y_train.iloc[train_index], y_train.iloc[test_index]\n    print(X_train_df.shape)\n    print(X_test_df.shape)\n    \n    \n    \n    clf = lgb.LGBMClassifier(max_depth = max_depth_best,\n                          num_leaves = num_leaves_best, \n                          n_estimators = n_estimators_best,\n                          n_jobs = -1 , \n                          verbose = 1,\n                          learning_rate= learning_rate_best,\n                          eval_metric='auc',\n                          nthread=4,\n                          is_unbalance = True,\n                          boost_from_average = False,\n                          device = 'gpu',\n                          gpu_platform_id = 0,\n                          gpu_device_id = 0\n                          )\n    \n    h = clf.fit(X_train_df, y_train_df, eval_set=[(X_test_df,y_test_df)],verbose=100, early_stopping_rounds=200)\n    \n   \n    preds += clf.predict_proba(X_test)[:,1]\/kf.n_splits\n    \n \n    \n    count = count + 1\n    if count <=5:\n        del h, clf\n    \n        x = gc.collect()\nprint('#'*20)\n","9be09010":"#\u00a0Confusion matrix and Classification report\npred1 =  clf.predict(X_test_df)\nfpr, tpr, thresholds = metrics.roc_curve(y_test_df, pred1, pos_label=2)\n\nprint(metrics.auc(fpr, tpr))\n\nprint(metrics.confusion_matrix(y_test_df, pred1))\nprint(metrics.classification_report(y_test_df, pred1))","2e5d8914":"sample_submission = pd.read_csv('..\/input\/ieee-fraud-detection\/sample_submission.csv')\nsample_submission.isFraud = preds\nsample_submission.to_csv('sub_lgbm_kfold.csv',index=False)\n","071a21c1":"### Extracting browser and versions\nIt separates the browser, device and versions of the processes and assigns them to new variables.","028a7bbb":"### Latest Version Control\nChecking the devices if they have the latest version or not. If they use the latest version, they are assigned as 1 otherwise 0.","5bfdeea3":"### Valid and invalid cards\nCards with less than 2 frequencies are defined as invalid cards while those with more than 2 were defined as valid. If a value is not common in the Train and Test set, these values were named as \"nan\". Finally, we named all invalid and \"nan\" values as \"nan\" while the others were valid.","58a3db17":"# Rolling Window Aggregations of Last Transactions\n\nRolling window aggregations of window size 10 for the transaction amount are performed using these formulas. These \nfeatures are important as they give almost all the necessary information about the distribution of the user\u2019s last 10\ntransaction amounts. Setting minimum acceptable window size as 1 provides us not to have too many missing values for \nthese features, as there is the observation itself in case all the previous observations are missing.\n","a398e00c":"# Model  and Hyperparameters Tuning","4a4b4782":"### Creating Userid\nDetermining the person to whom each process belongs to similar values by converting the features to string and combining them with \"_\".","4e2fd511":"# Model ","c8d30d08":"#\u00a0Creating New Features by Using Aggregation Functions","221d5659":"### Defining Time Variable and US Holidays","644614cc":"### Mean Encoding\n\nFraud ratios with respect to ProductCD and M4 categories. By doing this we used the target variable as the basis to generate the new encoded feature. ","0aee986e":"### Matching Receiver and Purchaser Email Domains\n\nR_emaildomain and p_emaildomain were compared. If P_emaildomain and R_emaildomain are the same and are not null, we assign 1. Otherwise we assign 0.","9c455134":"# Submitting the Predicted Values","6114e21e":"# Import Libraries","a33c580d":"#\u00a0Importing Data","72d4b041":"# Grouping the V-variables \n\nWe grouped the V variables according to the NaN values they contained and looked at the correlations of the V variables in the same group. we determined 70% as the correlation value.  Re-grouped those with the same correlations. We selected variables with the highest number of unique value among the variables with the same correlation and removed the others from the data set.\n","24cd58c3":"### Standardization of TransactionAmt\nConverts the type of TransactionAmt to float32. Subtracts the mean from the values in TransactionAmt and divides it by its standard deviation. In this way, standardization has been made.","b34c60d2":"# Functions","5418bcb2":"# Feature Engineering \n\n","33fe1239":"### Rescaling D columns\n\nThe D Columns are \"time deltas\" belong to the past. Therfore they are normalized to  transform the D Columns into their point in the past.","3722b1a2":"### Country Extraction\n\nDetermining the countries where the transactions are made according to the mail extensions."}}