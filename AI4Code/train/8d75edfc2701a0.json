{"cell_type":{"dc5d5282":"code","54526ca0":"code","69e20686":"code","6db408c3":"code","6c02bc74":"code","8d613f7c":"code","65a495f1":"code","0c369599":"code","b08be13b":"code","d7d6df3c":"code","3c3d8066":"code","fed514c6":"code","ddb386f8":"code","95f5f0ea":"code","5836526b":"code","22f0c096":"code","8cfe3a5c":"code","bd66baa4":"code","e885e429":"code","a6174124":"code","a847ce67":"code","bcdb7833":"code","847fefac":"code","9052ee2a":"code","d5171bd9":"code","9a227369":"code","af30423b":"code","e4fad780":"code","b476c405":"code","01b17a76":"code","b7bc0857":"code","48b45260":"code","df51883c":"code","a05d0979":"code","417c37a8":"code","5365c00d":"code","a13fd3e2":"code","e8038fc2":"code","7cd653cd":"code","0e8b6090":"code","23836bbb":"code","fb3b3de9":"code","f63c6121":"code","566f3566":"code","a82973a2":"code","a1412841":"code","b8125694":"code","84b092ef":"code","dbdbd8b8":"code","7a5a16ab":"code","19ad714d":"code","f4568575":"code","31903c5c":"code","cd775e1d":"code","36cbbc11":"code","3b1482ea":"code","e573817f":"code","d8058586":"code","bec203fb":"code","d8ee9b03":"code","5edf21cb":"code","36d62e48":"code","2ceff276":"code","4e5129a6":"code","8c017f5d":"code","9e13b7d9":"code","58c49e99":"code","58aa4eca":"code","dff6cde1":"code","00f8510d":"code","b81ee342":"code","a366d577":"code","67da7779":"code","db3a2c01":"code","4897f6cf":"code","bbb6da88":"code","4e05104f":"code","d3600a2a":"code","f96a1344":"code","adbd9ad1":"code","aa54fc27":"code","c957cc08":"markdown","99951103":"markdown","15fd5779":"markdown","1c7a694b":"markdown","3906f0b5":"markdown","3c47fe95":"markdown","bbc7261f":"markdown","4c69a8f8":"markdown","d1dec5db":"markdown","43002f79":"markdown","bf825371":"markdown","9628e285":"markdown","edd35f81":"markdown","00949372":"markdown","d5d2708d":"markdown","65110aec":"markdown","bb345868":"markdown","80115a1e":"markdown","a706edbc":"markdown","4408acc3":"markdown","884ee13f":"markdown","af79ae76":"markdown","577dcbd3":"markdown","54876452":"markdown","a6aecb87":"markdown","1d4a757d":"markdown","ab8f14c8":"markdown","3670f31a":"markdown","4d451d18":"markdown","f24b23dd":"markdown","50152559":"markdown","2f84434c":"markdown","a7ab504f":"markdown","5535bee4":"markdown","07b3ccd0":"markdown","3970d882":"markdown","6aee7d09":"markdown","9500954a":"markdown","224b9b92":"markdown","93ff1d89":"markdown","13cbcb0d":"markdown","80ff74f0":"markdown","b59a3dc7":"markdown","95610ab1":"markdown","24c6fbd2":"markdown","b7bda5fe":"markdown","592396ab":"markdown","e7794a34":"markdown","2a2ad5c4":"markdown","420c19ea":"markdown","541bc7f7":"markdown","5fcd504f":"markdown","0f8e9802":"markdown","558512d8":"markdown","d277bbb8":"markdown","1d04ab3e":"markdown","02164d4c":"markdown","9fa22761":"markdown"},"source":{"dc5d5282":"# Import required packages\nimport pandas as pd\nimport numpy as np\nimport pprint, time\nfrom collections import Counter\nimport re\nimport time\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.probability import FreqDist\n\nfrom sklearn.model_selection import train_test_split\n\nimport warnings\n\n\nwarnings.filterwarnings(\"ignore\")\n\nnp.random.seed(0)","54526ca0":"start = time.time()","69e20686":"# reading the universal tag dataset\nnltk_data = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))","6db408c3":"# first few tagged sentences\nprint(nltk_data[:3])","6c02bc74":"# make train test split with test_size as 0.05 for 5%\ntrain_set, test_set = train_test_split(nltk_data,random_state=42,test_size=0.05)\n\nlen(train_set),len(test_set)","8d613f7c":"# preivew the sentence in a DF\npd.DataFrame(train_set)","65a495f1":"# Getting list of tagged words to a flattern list of tupples\ntrain_tokens = [token_pair for sentence in train_set for token_pair in sentence]\nlen(train_tokens),train_tokens[:5]","0c369599":"# extract the words from the train_tokens\ntokens = [pair[0] for pair in train_tokens]\ntokens[:10]","b08be13b":"# preview the frequence of word occurace\nFreqDist(tokens)","d7d6df3c":"# set the wors to vocabulary variable with unique count\nvocabulary = set(tokens)\nlen(vocabulary)","3c3d8066":"# extract the tags from train_tokens\ntags = [pair[1] for pair in train_tokens]\nlen(tags)","fed514c6":"# preview the number of tag occuraces\nFreqDist(tags)","ddb386f8":"# set the tag to a unique set variable\ntag_set = sorted(set(tags))\nlen(tag_set)","95f5f0ea":"tag_set","5836526b":"# get the tags counts to a variable for easy access to total tag count for a tag\ntag_cnt = Counter(tags)","22f0c096":"word_given_tags = pd.Series(FreqDist(train_tokens)).reset_index()\\\n                                .rename(columns={'level_0':'vocabulary','level_1':'tag',0:'count'})\\\n                                .reset_index(drop=True).sort_values(by='count',ascending=False)\nword_given_tags","8cfe3a5c":"# method to compute word given tag: count\ndef word_given_tag(word, tag):\n    word_given_tag_count = word_given_tags.loc[(word_given_tags['vocabulary'] == word) & (word_given_tags['tag'] == tag),'count']    \n    count = word_given_tag_count.to_list()\n    return word_given_tag_count.to_list()[0] if len(count) != 0 else 0","bd66baa4":"# examples\n\nprint(\"\\n\",\"Occurances of Word for given tags :-\",\"\\n\")\n\n# large\nprint(\"Company\",\"\\n\")\nprint(\"As NOUN : \"+str(word_given_tag('company', 'NOUN')), \"\\n\")\n\n# will\nprint(\"\\n\",\"Will\",\"\\n\")\nprint(\"As NOUN : \"+str(word_given_tag('will', 'NOUN')))\nprint(\"As VERB : \"+str(word_given_tag('will', 'VERB')))\n\n# book\nprint(\"\\n\\n\",\"Book\",\"\\n\")\nprint(\"As NOUN : \"+str(word_given_tag('book', 'NOUN')))\nprint(\"As VERB : \"+str(word_given_tag('book', 'VERB')))","e885e429":"# convetr the above dataframe to pivot table with index as vocaublary and colunms as tag highlighting the probability as values\nemission_probability = pd.pivot_table(word_given_tags,columns='tag',index='vocabulary',values='count',fill_value=0.0)\nemission_probability = emission_probability.apply(lambda x: x\/x.sum(),axis=0)\nemission_probability","a6174124":"word_given_tag('zone', 'NOUN')","a847ce67":"tag_cnt['NOUN']","bcdb7833":"word_given_tag('zone', 'NOUN')\/tag_cnt['NOUN']","847fefac":"# compute occurance of a tag1 followed by tag2: tag2(t2) given tag1 (t1), i.e. Transition Probability\ndef t2_given_t1(t2, t1, train_bag = train_tokens):\n    \n    # get tag count for t1\n    count_t1 = tag_cnt[t1]\n    count_t2_t1 = 0\n    \n    # iterate through all the tags\n    for index in range(len(tags)-1):\n        \n        # check if t1 is followed by t2 in the on each iterated tag index\n        if tags[index]==t1 and tags[index+1] == t2:\n            count_t2_t1 += 1\n            \n    return (count_t2_t1, count_t1) ","9052ee2a":"# examples\nprint(t2_given_t1('NOUN', 'ADJ'))\nprint(t2_given_t1('NOUN', 'DET'))\nprint(t2_given_t1('.', 'NOUN'))\nprint(t2_given_t1('NOUN', 'PRT'))\nprint(t2_given_t1('PRON', 'VERB'))\nprint(t2_given_t1('VERB', 'DET'))","d5171bd9":"# creating t x t transition matrix of tags\n# each column is t2, each row is t1\n# thus M(i, j) represents P(tj given ti)\n\n# initialize txt matrix with zeroes\ntags_matrix = np.zeros((len(tag_set), len(tag_set)), dtype='float32')\n\n# get the transition probability of each tag followin another tag\nfor i, t1 in enumerate(list(tag_set)):\n    for j, t2 in enumerate(list(tag_set)): \n        tags_matrix[i, j] = t2_given_t1(t2, t1)[0]\/tag_cnt[t2]","9a227369":"transition_probability = pd.DataFrame(tags_matrix, columns = list(tag_set), index=list(tag_set))","af30423b":"transition_probability","e4fad780":"sns.set_style('darkgrid')\nsns.set(palette='viridis')","b476c405":"# heatmap of tags matrix\n# T(i, j) means P(tag j given tag i)\nplt.figure(figsize=(18, 12))\nsns.heatmap(transition_probability,annot=True,cmap='viridis')\nplt.show()","01b17a76":"#Lets filter out all the probabilities below 0.5\ntags_frequent = transition_probability[transition_probability>0.5]\nplt.figure(figsize=(18, 12))\nsns.heatmap(tags_frequent,annot=True,cmap='viridis')\nplt.show()","b7bc0857":"# created a vairbale to view all the unknown words for prediction\nunknow_words = set()\n\n# Viterbi Heuristic\ndef Viterbi(words, train_bag = train_tokens):\n    state = []\n    T = list(set([pair[1] for pair in train_bag]))\n    for key, word in enumerate(words):\n        #initialise list of probability column for a given observation\n        p = [] \n        for tag in T:\n            \n            # for the first tage the tag t2 will always be '.'\n            if key == 0:\n                transition_p = transition_probability.loc['.', tag]\n            else:\n                transition_p = transition_probability.loc[state[-1], tag]\n                \n            # compute emission probabilities\n            if word in vocabulary:\n                emission_p = emission_probability.loc[word,tag]\n            else:\n                unknow_words.add(word)\n                # emission probability will be 0 for unknown\n                emission_p = 0 \n            # state probability of Viterbi is emiision probability * transition probability\n            state_probability = emission_p * transition_p    \n            p.append(state_probability)\n            \n        pmax = max(p)\n        # getting state for which probability is maximum\n        state_max = T[p.index(pmax)]\n        state.append(state_max)\n    return list(zip(words, state))","48b45260":"# list of tagged words\ntest_run = [tup for sent in test_set for tup in sent]\n\n# list of untagged words\ntest_tagged_words = [tup[0] for tup in test_run]","df51883c":"# check the number  of sentence in validation set \nlen([sent for sent in test_set])","a05d0979":"# check the number of tagged words in validation set\nlen(test_tagged_words)","417c37a8":"# tagging the test sentences\ntagged_seq = Viterbi(test_tagged_words)\n# accuracy check\ncheck = [i for i, j in zip(tagged_seq, test_run) if i == j] ","5365c00d":"# view the unknow words that encountered zero probabilities in viterbi\nlist(unknow_words)[:5]","a13fd3e2":"# create a DataFrame for accuracy \naccuracy_table = pd.DataFrame({'Method':[],'Accuracy':[]})","e8038fc2":"# define a method to calculate the accuracy score \ndef calculate_accuacy(tagged_sq,check,method):\n    accuracy = len(check)\/len(tagged_seq)\n    print(\"Accuracy for \"+method+\" : \"+str(accuracy))\n    global accuracy_table\n    accuracy_table = accuracy_table.append(pd.Series([method,accuracy],index=accuracy_table.columns),ignore_index=True)","7cd653cd":"# calculate the accuracy score for vanilla viterbi\ncalculate_accuacy(tagged_seq,check,'Vanilla Viterbi')","0e8b6090":"# view the correclty tagged sequence\ncorrect_tagged_cases = [j for i, j in enumerate(zip(tagged_seq, test_run)) if j[0]==j[1]]\ncorrect_tagged_cases[:5]","23836bbb":"# extract all the incorrect predictions\nincorrect_tagged_cases = [j for i, j in enumerate(zip(tagged_seq, test_run)) if j[0]!=j[1]]\nincorrect_tags_df = pd.DataFrame(Counter([tag_sets[0][1] for i,tag_sets in enumerate(incorrect_tagged_cases)]).items()).set_index(0)\nincorrect_tagged_cases[:5]","fb3b3de9":"# plot a barchart for tags that where incorrectly predicted\nax = sns.barplot(y=list(map(int,incorrect_tags_df.values)),x=incorrect_tags_df.index.values,palette='viridis')\nax.set(title='Number of incorrect Predictions for Vanilla Viterbi',xlabel='Tags',ylabel='Count');","f63c6121":"# Viterbi Heuristic\ndef Viterbi_I(words, train_bag = train_tokens):\n    state = []\n    T = list(set([pair[1] for pair in train_bag]))\n    for key, word in enumerate(words):\n        #initialise list of probability column for a given observation\n        p = [] \n        for tag in T:\n            if key == 0:\n                transition_p = transition_probability.loc['.', tag]\n            else:\n                transition_p = transition_probability.loc[state[-1], tag]\n                \n            # compute emission and state probabilities\n            if word in vocabulary:\n                emission_p = emission_probability.loc[word,tag]\n            else:\n                # Use transition Probability for unknown by making emission probabilty as a constant 1\n                emission_p = 1\n            state_probability = emission_p * transition_p    \n            p.append(state_probability)\n            \n        pmax = max(p)\n        # getting state for which probability is maximum\n        state_max = T[p.index(pmax)]\n        state.append(state_max)\n    return list(zip(words, state))","566f3566":"# tagging the test sentences\ntagged_seq_I = Viterbi_I(test_tagged_words)\ntagged_seq_I[:5]","a82973a2":"# accuracy\ncheck_I = [i for i, j in zip(tagged_seq_I, test_run) if i == j] ","a1412841":"len(check_I),len(tagged_seq_I)","b8125694":"# calculate the accuracy score for viterbi Apprach I\ncalculate_accuacy(tagged_seq_I,check_I,'Viterbi Approach I - Using Transition probability when emission probability is 0')","84b092ef":"# view the correclty tagged sequence\ncorrect_tagged_cases_I = [j for i, j in enumerate(zip(tagged_seq_I, test_run)) if j[0]==j[1]]\ncorrect_tagged_cases_I[:5]","dbdbd8b8":"# extract all the incorrect predictions\nincorrect_tagged_cases_I = [j for i, j in enumerate(zip(tagged_seq_I, test_run)) if j[0]!=j[1]]\nincorrect_tags_I_df = pd.DataFrame(Counter([tag_sets[0][1] for i,tag_sets in enumerate(incorrect_tagged_cases_I)]).items()).set_index(0)\nincorrect_tagged_cases_I[:5]","7a5a16ab":"# plot a barchart for tags that where incorrectly predicted\nsns.set(palette='Spectral')\nax = sns.barplot(y=list(map(int,incorrect_tags_I_df.values)),x=incorrect_tags_I_df.index.values,palette='viridis')\nax.set(title='Number of incorrect Predictions for Viterbi I',xlabel='Tags',ylabel='Count');","19ad714d":"patterns = [\n    (r'\\*\\-.*','X'),                                         # handle text that has '*-' as numbers ex. *-235 ,*-*T23\n    (r'[0-9]+[\\.\\-]?[0-9]*s?','NUM'),                        # regex for predicting numbers ex. 29.3 , 29-03 or 29\n    (r'[A-Z]{,2}\\.?[A-Z]?\\.?','NOUN'),                       # pattern for finding Titles or Codes as Nouns ex. Mr. , U.S. , TX , Dr\n    (r'^([Mm]ore|less|later|earl[a-z]+|middle|most)$','ADJ'),# this can identify the given text as Adjective\n    (r'^([Tt]hat|so|though|before|about|as)$','ADP'),        # handles the given text as ADP\n    (r'^[a-z]+(ed|ing)$','VERB'),                            # pattern to predict all the owrds ending in ing,ed as verbs ex. wased , running , leaving\n    (r'^[a-z]+ly$','ADV'),                                   # regex to identify all the words ending in ly as Adverb ex. successfully, hardly\n    (r'^[A-z]+(ds|ive?s|ist?s|ent|ial|ers)$','NOUN'),        # this can find the all words eding in given pattern as NOUN ex. active,partial\n    (r'^[A-z]{3,}s$', 'NOUN'),                               # Assign all the words that are above three characters as NOUNS\n    (r'.*','NOUN')                                           # assigns the remaing words as NOUN\n]","f4568575":"regexp_tagger = nltk.RegexpTagger(patterns)\nregexp_tagger.evaluate(test_set)","31903c5c":"unigram_tagger = nltk.UnigramTagger(train_set,backoff=regexp_tagger)\nunigram_tagger.evaluate(test_set)","cd775e1d":"bigram_tagger = nltk.BigramTagger(train_set,backoff=unigram_tagger)\nbigram_tagger.evaluate(test_set)","36cbbc11":"trigram_tagger = nltk.TrigramTagger(train_set,backoff=bigram_tagger)\ntrigram_tagger.evaluate(test_set)","3b1482ea":"# Bigram tagger backed up by the rule-based tagger\nregex_bigram_tagger = nltk.BigramTagger(train_set, backoff = unigram_tagger)","e573817f":"# Viterbi Heuristic\ndef Viterbi_II(words, train_bag = train_tokens):\n    state = []\n    T = list(set([pair[1] for pair in train_bag]))\n    for key, word in enumerate(words):\n        #initialise list of probability column for a given observation\n        if word in vocabulary:\n            p = []\n            for tag in T:\n                \n                if key == 0:\n                    transition_p = transition_probability.loc['.', tag]\n                else:\n                    transition_p = transition_probability.loc[state[-1], tag]\n\n                # compute emission and state probabilities\n                emission_p = emission_probability.loc[word,tag]\n                state_probability = emission_p * transition_p    \n\n                p.append(state_probability)\n            pmax = max(p)\n\n            # getting state for which probability is maximum\n            state_max = T[p.index(pmax)] \n            state.append(state_max)\n        else:\n            # USING BIGRAMTAGGER FOR UNKNOWN\n            state_max = regex_bigram_tagger.tag_sents([[(word)]])\n            state.append(state_max[0][0][1])\n            \n    return list(zip(words, state))","d8058586":"# tagging the test sentences\ntagged_seq_II = Viterbi_II(test_tagged_words)\ncheck_II = [i for i, j in zip(tagged_seq_II, test_run) if i == j]","bec203fb":"len(check_II),len(tagged_seq_II)","d8ee9b03":"# calculate the accuracy score for viterbi approach II\ncalculate_accuacy(tagged_seq_II,check_II,'Viterbi Approach II - Bigram Tagger with backoff for unknown + Viterbi')","5edf21cb":"# view the correclty tagged sequence\ncorrect_tagged_cases_II = [j for i, j in enumerate(zip(tagged_seq_II, test_run)) if j[0]==j[1]]\ncorrect_tagged_cases_II[:5]","36d62e48":"# extract all the incorrect predictions\nincorrect_tagged_cases_II = [j for i, j in enumerate(zip(tagged_seq_II, test_run)) if j[0]!=j[1]]\nincorrect_tags_II_df = pd.DataFrame(Counter([tag_sets[0][1] for i,tag_sets in enumerate(incorrect_tagged_cases_II)]).items()).set_index(0)\nincorrect_tagged_cases_II[:5]","2ceff276":"# plot a barchart for tags that where incorrectly predicted\nax = sns.barplot(y=list(map(int,incorrect_tags_II_df.values)),x=incorrect_tags_II_df.index.values,palette='viridis')\nax.set(title='Number of incorrect Predictions for Viterbi II',xlabel='Tags',ylabel='Count');","4e5129a6":"accuracy_table","8c017f5d":"# display bar plot for accuracy\nax = sns.barplot(y=accuracy_table.Accuracy,x=accuracy_table.Method,palette='viridis')\n\n# set titlte and labels\nax.set(ylim=[0.8,1],title='Accuracy Comparison',ylabel='Accuracy Score',xlabel='Method')\n\n# despine the boundaries\nsns.despine(right=True,top=True)\n\n# set tick labels\nax.set_xticklabels(['Vanilla Viterbi','Viterbi Approach I','Viterbi Approach II'],rotation=0)\n\n# set text values for bars\nfor patch in ax.patches:\n    height = round(patch.get_height(),3)\n    ax.text(patch.get_x()+patch.get_width()\/3,0.81,height,fontproperties=dict(size=12,weight='bold'),color='w')","9e13b7d9":"incorrect_tags = pd.DataFrame(list(set(incorrect_tagged_cases)),columns=['Vanilla Viterbi','original'])","58c49e99":"trained_tags = pd.DataFrame(list(incorrect_tags.original),columns=['Vocabulary','Trained Tag']).set_index('Vocabulary')","58aa4eca":"vanilla_tags = pd.DataFrame(list(incorrect_tags['Vanilla Viterbi']),columns=['Vocabulary','Vanilla Viterbi']).set_index('Vocabulary')\nviterbi_I_tags = pd.DataFrame(set(tagged_seq_I),columns=['Vocabulary','Viterbi I']).set_index('Vocabulary')\nviterbi_II_tags = pd.DataFrame(set(tagged_seq_II),columns=['Vocabulary','Viterbi II']).set_index('Vocabulary')","dff6cde1":"df = pd.merge(pd.merge(pd.merge(trained_tags,vanilla_tags,left_index=True,right_index=True,how='inner'),\\\n              viterbi_I_tags,left_index=True,right_index=True,how='inner'),\\\n        viterbi_II_tags,left_index=True,right_index=True,how='inner')","00f8510d":"df_style = df.reset_index().sample(25).style.set_table_attributes(\"style='display:inline'\").set_caption('Tag prediction of Viterbi I and II for incorreclty predicted tags of Vanilla Viterbi')\ndf_style","b81ee342":"try:\n    f = open(\"Test_sentences.txt\", \"r\")\n    senteces = f.readlines()\n    f.close()\nexcept:\n    senteces = ['Android is a mobile operating system developed by Google.\\n',\n         'Android has been the best-selling OS worldwide on smartphones since 2011 and on tablets since 2013.\\n',\n         \"Google and Twitter made a deal in 2015 that gave Google access to Twitter's firehose.\\n\",\n         'Twitter is an online news and social networking service on which users post and interact with messages known as tweets.\\n',\n         'Before entering politics, Donald Trump was a domineering businessman and a television personality.\\n',\n         'The 2018 FIFA World Cup is the 21st FIFA World Cup, an international football tournament contested once every four years.\\n',\n         'This is the first World Cup to be held in Eastern Europe and the 11th time that it has been held in Europe.\\n',\n         'Show me the cheapest round trips from Dallas to Atlanta\\n',\n         'I would like to see flights from Denver to Philadelphia.\\n',\n         'Show me the price of the flights leaving Atlanta at about 3 in the afternoon and arriving in San Francisco.\\n',\n         'NASA invited social media users to experience the launch of ICESAT-2 Satellite.\\n',\n         '\\n',\n         '\\n',\n         '\\n']","a366d577":"senteces","67da7779":"senteces = pd.Series(senteces).str.replace('\\n','')","db3a2c01":"# define a method to display a table displaying all approache's tag prediction for a given sentence\ndef test_sample_data_sent(n):\n    # get the sentence by index\n    sample_sent=senteces[n]\n    \n    # get the tag sequence and convert it to dataframe with vocabulary as index and column values as tags\n    # get the tag sequence for the train dataset\n    tag_seq=pd.DataFrame(set(train_tokens)).set_index(0)\n    \n    # get actual tag sequence for the given sentence from nltk universal tagset\n    actual_tags = pd.DataFrame(list(pd.DataFrame(nltk.pos_tag_sents(list(map(word_tokenize,word_tokenize(senteces[n]))), tagset='universal'))[0])).drop_duplicates().set_index(0)\n\n    # get tag sequence for the given sentence from vanilla viterbi\n    tag_seq_vanilla_viterbi=pd.DataFrame(Viterbi(list(word_tokenize(sample_sent)))).drop_duplicates().set_index(0)\n\n    # get tag sequence for the given sentence from viterbi I technique\n    tag_seq_viterbi_I=pd.DataFrame(Viterbi_I(list(word_tokenize(sample_sent)))).drop_duplicates().set_index(0)\n\n    # get tag sequence for the given sentence from viterbi II technique\n    tag_seq_viterbi_II=pd.DataFrame(Viterbi_II(list(word_tokenize(sample_sent)))).drop_duplicates().set_index(0)\n\n    # Merge the tag sequence based on the vocabulary index from all the approaches to a single DataFrame\n    df_sent= tag_seq_vanilla_viterbi.merge(tag_seq_viterbi_I,left_index=True,right_index=True,how='left')\n    df_sent= df_sent.merge(tag_seq_viterbi_II,left_index=True,right_index=True,how='left')\n    df_sent= df_sent.merge(tag_seq,left_index=True,right_index=True,how='left')\n    df_sent= df_sent.merge(actual_tags,left_index=True,right_index=True,how='left')\n    \n    # set the index name from 0 to Vocabulary\n    df_sent.index.name = 'Vocabulary'\n\n    # set the column names from 1 to algorithms names in order of merge\n    df_sent.columns = ['Vanilla Viterbi','Viterbi I','Viterbi II','TrainDataset Tags','Actual Tag']\n    \n    # Our TrainData set might have a vocabulary tagged under different tags to consolidate it to one row we get the all duplicate index\n    # and concat all the tag to one array of strings and then remove the duplicates and asign the list of values to the vocabulary with multi trained tag\n    df_dup_sent = df_sent[df_sent.index.value_counts() > 1]\n    if len(df_dup_sent):\n        df_dup_sent = df_dup_sent.groupby('Vocabulary')['TrainDataset Tags'].transform(lambda x : str(list(x.values))).to_frame()\n        df_sent = df_sent[~df_sent.index.duplicated(keep='first')]\n        df_sent.loc[df_dup_sent.index,'TrainDataset Tags'] = df_dup_sent['TrainDataset Tags'].values\n        \n    # return the dataFrame in the orer fo this columns\n    return df_sent[['Actual Tag','TrainDataset Tags','Vanilla Viterbi','Viterbi I','Viterbi II']]","4897f6cf":"print(senteces[0])\ntest_sample_data_sent(0)","bbb6da88":"print(senteces[3])\ntest_sample_data_sent(3)","4e05104f":"print(senteces[5])\ntest_sample_data_sent(5)","d3600a2a":"from IPython.core.display import HTML\n# display the tablelayout and caption\/title in center\nHTML(\"\"\"\n<style>\ncaption {\n    text-align: center;\n    margin: 0;\n    display: block;\n    width: 100%;\n}\n.jp-RenderedHTMLCommon.jp-RenderedHTML\n{\n    text-align: center;\n}\n.jp-RenderedHTMLCommon.jp-RenderedHTML th.col_heading \n{\n    text-align:center;\n}\n<\/style>\n\n\"\"\")","f96a1344":"# analysze all the given sample sentences\nprint('Note :- NaNs in Trained Tag denotes the unknown words in our Train dataset split')\nfor i,sentence in enumerate(senteces):\n    if len(sentence) > 0:\n        print(\"\\n\\n\")\n        display(test_sample_data_sent(i).reset_index().style.set_table_attributes(\"style='display:inline'\").set_caption(\"<b>Sentence \"+str(i+1)+\" : <\/b>\"+sentence+\"<br\/><br\/>\"))","adbd9ad1":"time.time() - start","aa54fc27":"display(accuracy_table.style.set_table_attributes(\"style='display:inline'\"))","c957cc08":"### Evaluating tagging accuracy I","99951103":"### List down cases which were incorrectly tagged by original POS tagger and got corrected by your modifications for Validation dataset","15fd5779":"### Vanilla Viterbi Algorithm","1c7a694b":"#### TrigramTagger backoff BigramTagger","3906f0b5":"#### Combining Taggers with backoff","3c47fe95":"#### UnigramTragger backoff RegexTagger","bbc7261f":"### Data Preparation","4c69a8f8":"**Observation**\n\nThe Viterbi I has almost same number of inaccurate tags but is not specific to a single tag, hence the error rate is even across the tags","d1dec5db":"Similaraly we extract the tagsequnce from other approaches for the incorrect tags words of vanilla viterbi","43002f79":"Lets check if the values are mapped right","bf825371":"### Compare the tagging accuracies of the modifications with the vanilla Viterbi algorithm","9628e285":"We need to split in a ratio of 95% for train and 5% for test","edd35f81":"### Transition Probabilities","00949372":"## Conclusion","d5d2708d":"### Viterbi Modified I ( Using Transisition Probability for unknown)","65110aec":"Lets create a dataframe to preview the number of occurance of a tag word pair in train_tokens","bb345868":"### Lates take three random samples for the given test data","80115a1e":"## Model","a706edbc":"<h3>Observation<\/h3>\n\nFrom the table we can see that Viterbi I identifies most of the NOUNs correctly and Verterbi II identifies most of the NUMs correctly","4408acc3":"<center><h1>Syntactic Analysis<\/h1><\/center>","884ee13f":"<h2>Goals<\/h2>\n\nWe need to split the Treebank dataset into train and validation sets, using a sample size of 95:5 for training: validation sets, i.e. keeping the validation size small, else the algorithm will need a very high amount of runtime.\n\nWe need to accomplish the following in for the problem statement:\n\n1. Write the vanilla Viterbi algorithm for assigning POS tags (i.e. without dealing with unknown words) \n2. Solve the problem of unknown words using at least two techniques. These techniques can use any of the approaches discussed in the class - lexicon, rule-based, probabilistic etc. Note that to implement these techniques, you can either write separate functions and call them from the main Viterbi algorithm, or modify the Viterbi algorithm, or both.\n3. Compare the tagging accuracy after making these modifications with the vanilla Viterbi algorithm.\n4. List down at least three cases from the sample test file (i.e. unknown word-tag pairs) which were incorrectly tagged by the original Viterbi POS tagger and got corrected after your modifications.","af79ae76":"The Viterbi algorithm - for every word w in the sentence, a tag t is assigned to w such that it maximizes the likelihood of the occurrence of P(tag\u2502word).\n\nState Probability :\n> $ P(tag\u2502word) = EmissionProbability \u2217 TransitionProbability  = P(word\u2502tag)\u2217 P(tag\u2502previousTag) $\n\nIn other words, we assign the tag t to the word w which has the max\u2061 P(tag\u2502word)\n","577dcbd3":"Viterbi Heuristic is a greedy approach. The basic idea of the Viterbi algorithm is as follows - given a list of observations (words) $O_1 ,O_2 ....$ On to be tagged, rather than computing the probabilities of all possible tag sequences, you assign tags sequentially, i.e. assign the most likely tag to each word using the previous tag.\n\nMore formally, you assign the tag $T_i$ to each word $W_i$ such that it maximises the likelihood:\n\n$P(T_i | W_i ) = P(W_i |T_i ) * P(T_{i-1} |T_i ) $ ,\n\nwhere $T_{i-1}$ is the tag assigned to the previous word. The probability of a tag $T_i$ is assumed to be dependent only on the previous tag $T_{i-1}$, and hence the term  $P(T_i | T_{i-1}) $ - Markov Assumption.","54876452":"### Split the corups to train test sample","a6aecb87":"Checking emission probablity of zone","1d4a757d":"As we had a preview of the error counts, we could enhance our Viterbi II model by fixing the incorrect predicitions of NOUNs as its error rate was high by tuning our models regexTaggers and Bigram\/UnigarmTaggers focusing towards NOUN vocabularies","ab8f14c8":"**Observation**\n\nWe can see that most of the CONJ are not properly classfied and NUM are classifed properly","3670f31a":"### RegexTagger | Rulebased Tagger","4d451d18":"## Visualization","f24b23dd":"## Testing Our Viterbi Models on Sample data","50152559":"Display 25 random records to view the prediction made by Viterbi Approach I and II for incorrect prediction made by Vanilla Viterbi","2f84434c":"## Lexicon and Rule-Based Models for POS Tagging","a7ab504f":"<h2>Problem Statement<\/h2>\n\n<h3>HMMs and Viterbi algorithm for POS tagging<\/h3>\n\nThe Vanilla Viterbi algorithm results in ~87% accuracy. The approx. 13% loss of accuracy is majorly due to the fact that when the algorithm encounters an unknown word (i.e. not present in the training set, such as 'Twitter'), it assigns an incorrect tag arbitrarily. This is because, for unknown words, the emission probabilities for all candidate tags are 0, so the algorithm arbitrarily chooses (the first) tag.\n\n\nIn this assignment, we need to modify the Viterbi algorithm to solve the problem of unknown words using at least two techniques. Though there could be multiple ways to solve this problem, we will be using methods based on the following statements:\n\n* Which tag class do you think most unknown words belong to? \n* Can you identify rules (e.g. based on morphological cues) that can be used to tag unknown words? \n\n* Why does the Viterbi algorithm choose a random tag on encountering an unknown word? \n* Can you modify the Viterbi algorithm so that it considers only one of the transition or emission probabilities for unknown words?","5535bee4":"From our Observations we can conclude that Vanilla Veterbi has many incorrect predictions and has been enhanced with additional approaches\n\n**Approach I**\n\n>Viterbi Approach I - Using Transition probability when emission probability is 0\n\nIt performed well and had hiher accuray score than Vanilla Veterbi with a score of 91.3%\n\n\n**Approach II**\n\n>Viterbi Approach II - Bigram Tagger with backoff for unknowns on Vanilla Viterbi\n\nThis Technique did most of the predictions and had less error count than the previous two approaches with an accuracy score of 93.8%","07b3ccd0":"### Viterbi Modified II ( Using BigramTagger and RuleBased tagger for unknowns)","3970d882":"In the above POS Taggers we can see that \n\n    Unigram Tagger backed up by the rulebased gives an accuracy of about 94.3%. \n    Bigram Tagger backed up by the rulebased gives an accuracy of about 94.5%. \n    Trigram Tagger backed up by the rulebased gives an accuracy of about 94.5%. \n\nLet's try to modify the viterbi algorithm to using **bigram tagger** as a back-off for unknown words.","6aee7d09":"Lets collect all the incorrectly predicted tags from Vanilla viterbi","9500954a":"Now we can merge based on the vocabulary and view it as a dataframe","224b9b92":"We can see that Android, Google etc. are some unknown tags in our train dataset  and still prediction has been made\n\nAndroid as\n\n    -VERB in Vanilla\n    -CONJ in Viterbi I\n    -NOUN in Viterbi II\n    \nGoogle as\n\n    -VERB in Vanilla\n    -DET in Viterbi I\n    -NOUN in Viterbi II\n    \n  >Viterbi II has predicted the unkown NOUNs tags correctly","93ff1d89":"Lets calculate the transistion probability of the tags\n\n>Transition Probability : $P(t_1|t_2)$ = (Number of time $t_1$ is followed by $t_2$) \/ (Number of times $t_1$ appeared)","13cbcb0d":"#### BigramTagger backoff UnigramTragger","80ff74f0":"Lets define a method to get the count for a word given tag","b59a3dc7":"<h2>Data<\/h2>\n\nFor this problems statement, we\u2019ll use the Treebank dataset of NLTK with the 'universal' tagset. The Universal tagset of NLTK comprises only 12 coarse tag classes as follows: Verb, Noun, Pronouns, Adjectives, Adverbs, Adpositions, Conjunctions, Determiners, Cardinal Numbers, Particles, Other\/ Foreign words, Punctuations.\n\nNote that using only 12 coarse classes (compared to the 46 fine classes such as NNP, VBD etc.) will make the Viterbi algorithm faster as well.","95610ab1":"We can see that 2018, World etc. are some unknown tags in our train dataset and still prediction has been made\n\n2018 as\n\n    -VERB in Vanilla\n    -ADJ in Viterbi I\n    -NUM in Viterbi II\n    \nWorld as\n\n    -NOUN in Vanilla\n    -NOUN in Viterbi I\n    -NOUN in Viterbi II\n    \n> All the algorithms has precided the tag for 'World' correclty\n\n> 2018 has been correctly predicted as nUM by Viterbi II","24c6fbd2":"Lets extract all the tags for the vocabularies of incorrect_tags from oru train tagset","b7bda5fe":"## Future todo Steps","592396ab":"### Emission Probability","e7794a34":"In this approach we will again modify the vanilla Viterbi algorthim to handle unknown words. \n\nIf the word is not in vocabulary of testset then its calculate emission probabilty will be zero hence we use rule based tagger to map vocabulary to the approbriate Tag based on the rule algorithm.","2a2ad5c4":"**Observation**\n\nWe can see that most of the NOUNs has not been Predicted correctly and most of CONJs are identified correctly.\n\nIf we look closer the ratio of the tag error looks similar to vanilla Viterbi","420c19ea":"#### Now lets test on all sample sentences","541bc7f7":"Lets now calculate the emmission probability for word given tag\n\n>Emmission Probability : P(w|t) = (Number of times word has been tagged t) \/ (Number of times t appears)","5fcd504f":"In the first approach we will use the transition probability for unknown words when calculating the state probability of Viterbi. To make this possible we can ignore the emission probability by setting the probability to a constant 1, thus transition probability will be considered as it is for unknown","0f8e9802":"### Solve the problem of unknown words","558512d8":"### List down at least three cases from the sample test file (i.e. unknown word-tag pairs) which were incorrectly tagged by the original Viterbi POS tagger and got corrected after your modifications.","d277bbb8":"<h3> Tag Description<\/h3>\n\n. - punctuation\n\nADJ - adjectives\n\nADP - adpositions\n\nADV - adverbs\n\nCONJ - conjunctions\n\nDET - determiners\n\nNOUN - nouns\n\nNUM - cardinal numbers\n\nPRON - pronouns\n\nPRT - particles or other function words\n\nVERB - verbs    \n\nX - other: foreign words, typos, abbreviations\n","1d04ab3e":"We can see that Twitter, tweets etc. are some unknown tags in our train dataset and still prediction has been made\n\nTwitter as\n\n    -VERB in Vanilla\n    -CONJ in Viterbi I\n    -NOUN in Viterbi II\n    \ntweets as\n\n    -VERB in Vanilla\n    -DET in Viterbi I\n    -NOUN in Viterbi II\n\n> Again Viterbi II has got the prediction of the nouns of unknowns correclty","02164d4c":"### Build the vanilla Viterbi based POS tagger","9fa22761":"### Evaluation with Validation corpus datast"}}