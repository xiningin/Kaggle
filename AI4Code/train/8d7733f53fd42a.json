{"cell_type":{"2f21dc0e":"code","c5f59591":"code","e29101c5":"code","98c330d1":"code","7d68ee4f":"code","a1848d91":"code","c3b3e05f":"code","bb386d8f":"code","910162b4":"code","c613e4df":"code","732f35d8":"code","b89f227f":"code","9876a9f4":"code","b8589acf":"code","68baaf45":"code","e5e67bde":"code","34b2071c":"code","af056e36":"code","345feb0c":"code","5c3a2251":"code","234e7b82":"code","a7ad2633":"code","30f47d86":"code","504ebf60":"code","ddc03338":"code","e2eb6c4b":"code","4d1e9482":"code","13ef8e2b":"code","1318f479":"code","e3e5af16":"code","eab1bb70":"code","2f4cbec5":"code","b961717b":"code","fb4be16d":"code","126dacb2":"code","11a0fda3":"code","bbfaf25a":"code","a2f74a17":"code","ddbf41b6":"code","7b3a48fc":"code","8ddf9f38":"code","1ab3f388":"code","8311e050":"code","f40ac617":"code","36bc9320":"code","999afc3f":"code","4497c96c":"code","ae2365e9":"code","d45ae898":"code","050488b4":"code","cc7398c8":"code","14602965":"markdown","e166f4cd":"markdown","57b0e3b3":"markdown","8ee63153":"markdown","e8777e29":"markdown","daeec858":"markdown","eaad8460":"markdown","c25ba56a":"markdown","6751b76a":"markdown","39240e53":"markdown","da7fc5d6":"markdown","8735ea46":"markdown","c17563b1":"markdown","356dff17":"markdown","58f9dc7f":"markdown","5f56034b":"markdown","f7b18dd5":"markdown","df597565":"markdown","bb06e8ec":"markdown","9cd08184":"markdown","f0bd232d":"markdown","df08e827":"markdown","7dfaed65":"markdown","8eeff784":"markdown","f8d8d74a":"markdown","55e40ff2":"markdown","37b8ac6a":"markdown","baae4403":"markdown","e0bae2db":"markdown","7dd6755a":"markdown","bb74930b":"markdown","dad932e7":"markdown","45277685":"markdown","959ada4e":"markdown","1c5cdc6f":"markdown","9c62f4d6":"markdown","decf3c9a":"markdown","cfc1a579":"markdown","4a1ea7d4":"markdown","d7feefde":"markdown","1c44d35d":"markdown","cd1764f5":"markdown","3dc601ec":"markdown","19f4d8b0":"markdown","c4ef4549":"markdown","84a54bcf":"markdown","1b69d29d":"markdown","4fc1a6d0":"markdown","b3dafcc3":"markdown","0fe69e3e":"markdown","3fbfb339":"markdown","edcc30a6":"markdown","e25f5adc":"markdown","268c28d6":"markdown"},"source":{"2f21dc0e":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport os\nwarnings.filterwarnings(\"ignore\")","c5f59591":"train = pd.read_csv('..\/input\/titanic\/train.csv',sep = \",\", header = 0)\ntest = pd.read_csv('..\/input\/titanic\/test.csv',sep = \",\", header = 0)","e29101c5":"train.head()","98c330d1":"test.head()","7d68ee4f":"train.info()","a1848d91":"test.info()","c3b3e05f":"#Inject SalePrice in test\ntest['Survived'] = 0\nprint(\"Train Shape: \" + str(train.shape))\nprint(\"Test Shape: \" + str(test.shape))","bb386d8f":"full = pd.concat([train,test])\nfull.shape","910162b4":"full.head()","c613e4df":"full['Sex'] = full['Sex'].map({'male':0, 'female':1})","732f35d8":"full.info()","b89f227f":"full.drop(['Ticket','Cabin'], axis = 1, inplace = True)","9876a9f4":"sns.set_palette(\"GnBu_d\")\nplt.title(\"Missingess Map\")\nplt.rcParams['figure.figsize'] = (8.0, 5.0) #Adjust values as necessary\nsns.heatmap(full.isnull(), cbar=False)","b8589acf":"full['Age'].replace('',np.nan,inplace=True)\nfull['Age'].fillna(value=full['Age'].mean(),inplace = True)\nfull['Fare'].replace('',np.nan,inplace=True)\nfull['Fare'].fillna(value=full['Fare'].mean(),inplace = True)\nfull[['Age','Fare']].info()","68baaf45":"full['Embarked'].fillna(value=full['Embarked'].value_counts().idxmax(),inplace = True)\nfull[['Embarked']].info()","e5e67bde":"full.info()","34b2071c":"full['Survived'] = full['Survived'].astype('category')\nfull['Sex'] = full['Sex'].astype('category')\nfull['Embarked'] = full['Embarked'].astype('category')","af056e36":"full.info()","345feb0c":"train = full[0:891]\ntest = full[891:1309]","5c3a2251":"fig, axs = plt.subplots(ncols=4,nrows=1, figsize = (20,5),squeeze = False)\nsns.countplot(x = \"Survived\", data = train, ax = axs[0][0])\nsns.countplot(x = \"Pclass\", data = train, ax = axs[0][1])\nsns.countplot(x = \"Embarked\", data = train, ax = axs[0][2])\nsns.countplot(x = \"Sex\", data = train, ax = axs[0][3])","234e7b82":"colors = [\"#B83636\", \"#2FB756\"]\nsns.set_palette(sns.color_palette(colors))\nfig, axs = plt.subplots(ncols=3,nrows=1, figsize = (20,5), squeeze = False)\nsns.countplot(x = 'Pclass', data = train, hue = 'Survived', ax=axs[0][0])\nsns.countplot(x = 'Embarked', data = train, hue = 'Survived', ax=axs[0][1])\nsns.countplot(x = 'Sex', data = train, hue = 'Survived', ax=axs[0][2])","a7ad2633":"fig, axs = plt.subplots(ncols=4,nrows=2, figsize = (20,10))\nsns.boxplot(x=\"Survived\", y=\"Age\", data = train, ax=axs[0][0])\nsns.stripplot(x='Survived',y='Age', data=train, jitter=True, ax=axs[0][0])\n\nsns.boxplot(x=\"Survived\", y=\"Fare\", data = train, ax=axs[0][1])\nsns.stripplot(x='Survived',y='Fare', data=train, jitter=True, ax=axs[0][1])\n\nsns.boxplot(x=\"Survived\", y=\"SibSp\", data = train, ax=axs[0][2])\nsns.stripplot(x='Survived',y='SibSp', data=train, jitter=True, ax=axs[0][2])\n\nsns.boxplot(x=\"Survived\", y=\"Parch\", data = train, ax=axs[0][3])\nsns.stripplot(x='Survived',y='Parch',  data=train, jitter=True, ax=axs[0][3])\n\nsns.violinplot(x=\"Survived\", y=\"Age\", data = train, ax=axs[1][0])\nsns.violinplot(x=\"Survived\", y=\"Fare\", data = train, ax=axs[1][1])\nsns.violinplot(x=\"Survived\", y=\"SibSp\", data = train, ax=axs[1][2])\nsns.violinplot(x=\"Survived\", y=\"Parch\", data = train, ax=axs[1][3])","30f47d86":"fig, axs = plt.subplots(ncols=3,nrows=2, figsize = (20,12), squeeze = False)\nsns.distplot(train[(train['Survived'] == 0)]['Age'], kde=False, color = \"#2FB756\", ax=axs[0][0])\nsns.distplot(train[(train['Survived'] == 1)]['Age'], kde=False,color = \"#B83636\", ax=axs[0][1])\nsns.distplot(train[(train['Survived'] == 0)]['Age'], kde=True, color = \"#2FB756\", ax=axs[0][2])\nsns.distplot(train[(train['Survived'] == 1)]['Age'], kde=True,color = \"#B83636\", ax=axs[0][2])\nsns.distplot(train[(train['Survived'] == 0)]['Fare'], kde=False, color = \"#2FB756\", ax=axs[1][0])\nsns.distplot(train[(train['Survived'] == 1)]['Fare'], kde=False, color = \"#B83636\", ax=axs[1][1])\nsns.distplot(train[(train['Survived'] == 0)]['Fare'], kde=True, color = \"#2FB756\", ax=axs[1][2])\nsns.distplot(train[(train['Survived'] == 1)]['Fare'], kde=True, color = \"#B83636\", ax=axs[1][2])","504ebf60":"train['0to18'] = train['Age'].apply(lambda x: 1 if x <= 18 else 0)\ntrain['18to50'] = train['Age'].apply(lambda x: 1 if (x > 18 and x <=50) else 0)\ntrain['50above'] = train['Age'].apply(lambda x: 1 if x > 50 else 0)\ntest['0to18'] = test['Age'].apply(lambda x: 1 if x <= 18 else 0)\ntest['18to50'] = test['Age'].apply(lambda x: 1 if (x > 18 and x <=50) else 0)\ntest['50above'] = test['Age'].apply(lambda x: 1 if x > 50 else 0)","ddc03338":"train.info()","e2eb6c4b":"test.info()","4d1e9482":"fig, axs = plt.subplots(ncols=3,nrows=1, figsize = (20,5), squeeze = False)\nsns.countplot(x = '0to18', data = train, hue = 'Survived', ax=axs[0][0])\nsns.countplot(x = '18to50', data = train, hue = 'Survived', ax=axs[0][1])\nsns.countplot(x = '50above', data = train, hue = 'Survived', ax=axs[0][2])","13ef8e2b":"def extract_titles(name):\n    if '.' in name:\n        return name.split(',')[1].split('.')[0].strip()\n    else:\n        return 'Unknown'\n    \ndef replace_titles(x):\n    title = x['Title']\n    if title in ['Capt', 'Col', 'Don', 'Jonkheer', 'Major', 'Rev', 'Sir']:\n        return 'Mr'\n    elif title in ['the Countess', 'Mme', 'Lady']:\n        return 'Mrs'\n    elif title in ['Mlle', 'Ms']:\n        return 'Miss'\n    elif title =='Dr':\n        if x['Sex']=='male':\n            return 'Mr'\n        else:\n            return 'Mrs'\n    else:\n        return title\n\ntrain['Title'] = train['Name'].map(lambda x: extract_titles(x))\ntrain['Title'] = train.apply(replace_titles, axis=1)\ntest['Title'] = test['Name'].map(lambda x: extract_titles(x))\ntest['Title'] = test.apply(replace_titles, axis=1)","1318f479":"sns.countplot(x = 'Title', data = train, hue = 'Survived')","e3e5af16":"train['FamilySize'] = train['SibSp'] + train['Parch'] + 1\ntest['FamilySize'] = test['SibSp'] + test['Parch'] + 1","eab1bb70":"sns.countplot(x = 'Title', data = train, hue = 'Survived')","2f4cbec5":"print(train.info())\nprint(test.info())","b961717b":"todrop = ['Age','SibSp','Parch','Name','PassengerId']\ntodrop2 = ['Age','SibSp','Parch','Name','Survived']\ntrain.drop(todrop, axis = 1, inplace = True)\ntest.drop(todrop2, axis = 1, inplace = True)\ntrain['0to18'] = train['0to18'].astype('category')\ntest['0to18'] = test['0to18'].astype('category')\ntrain['18to50'] = train['18to50'].astype('category')\ntest['18to50'] = test['18to50'].astype('category')\ntrain['50above'] = train['50above'].astype('category')\ntest['50above'] = test['50above'].astype('category')\nprint(train.info())\nprint(test.info())","fb4be16d":"from sklearn.preprocessing import OneHotEncoder\ntodummify = list(train.select_dtypes(include=['object','category']).columns)\nbinaord = {'0to18','18to50','50above','Survived','Sex'}\ntodummify = [var for var in todummify if var not in binaord]\nenc = OneHotEncoder(handle_unknown='ignore')\nenc_train = pd.DataFrame(enc.fit_transform(train[todummify]).toarray(),\n                      columns=enc.get_feature_names(todummify))\ntrain = train.join(enc_train,how='inner')\ntrain.drop(todummify, axis = 1, inplace = True )\n\ntocategorify = [col for col in train.columns if '_' in col]\ntrain[tocategorify] = train[tocategorify].astype('category')\n\n\nenc_test = pd.DataFrame(enc.transform(test[todummify]).toarray(),\n                      columns=enc.get_feature_names(todummify))\ntest = test.join(enc_test,how='inner')\ntest.drop(todummify, axis = 1, inplace = True )\n\ntocategorify = [col for col in test.columns if '_' in col]\ntest[tocategorify] = test[tocategorify].astype('category')\n\n\nprint(train.info(verbose=True))\nprint(test.info(verbose=True))","126dacb2":"X = train.drop(['Survived'], axis = 1)\ny = train['Survived']\nprint(\"Dependent Variables\")\ndisplay(X.head())\nprint(\"Independent Variable\")\ndisplay(y.to_frame().head())","11a0fda3":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tools.tools import add_constant\nX_numeric = X._get_numeric_data() #drop non-numeric cols\nX_numeric = add_constant(X_numeric)\nVIF_frame = pd.Series([variance_inflation_factor(X_numeric.values, i) \n               for i in range(X_numeric.shape[1])], \n              index=X_numeric.columns).to_frame()\n\nVIF_frame.drop('const', axis = 0, inplace = True) \nVIF_frame.rename(columns={VIF_frame.columns[0]: 'VIF'},inplace = True)\nVIF_frame[~VIF_frame.isin([np.nan, np.inf, -np.inf]).any(1)]","bbfaf25a":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size = 0.25, \n                                                    random_state = 823)\nprint(\"X_train\")\nprint(X_train.head())\nprint(\" \")\nprint(\"X_test\")\nprint(X_test.head())","a2f74a17":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(random_state = 823)\nrfc.fit(X_train,y_train)\nfeatures = X_train.columns.tolist()\nfeature_value = rfc.feature_importances_\nd = {'Features' : features, 'Values' : feature_value}\nfi = pd.DataFrame(d).sort_values('Values', ascending = False).reset_index()\nfi\nplt.rcParams['figure.figsize'] = (25, 5.0)\nax = sns.barplot(x=fi['Features'], y = fi['Values'], data = fi, palette=\"Blues_d\")","ddbf41b6":"from sklearn.model_selection import GridSearchCV\n\nparameters = [{'n_estimators' : [10000], #number of trees in the forest\n               'criterion': ['gini'], #gini or entropy\n               'max_depth': [4], #The maximum depth of the tree. \n               'min_samples_split': [2], #The minimum number of samples required to split an internal node.\n               'min_samples_leaf': [2], #The minimum number of samples required to be at a leaf node.\n               'max_features': [5], #The number of features to consider when looking for the best split:\n               'max_leaf_nodes': [7]}] #Grow a tree with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.\n               #'ccp_alpha': np.arange(0.0,,0.01)}]\nrf_clf = GridSearchCV(rfc, parameters,scoring = 'accuracy', cv = 10)\nrf_clf.fit(X_train,y_train)\nprint(\"Best Parameter Values: \")\npd.DataFrame.from_dict(rf_clf.best_params_,orient='index',columns=['Values'])","7b3a48fc":"best_rfc_model = rf_clf.best_estimator_\nbest_rfc_model.fit(X_train,y_train)\npredictions = best_rfc_model.predict(X_test)\npredictions","8ddf9f38":"from sklearn.metrics import classification_report,confusion_matrix\ndata = confusion_matrix(y_test, predictions)\ndf_cm = pd.DataFrame(data, columns=np.unique(y_test), index = np.unique(y_test))\ndf_cm.index.name = 'Predicted'\ndf_cm.columns.name = 'Actual'\nplt.figure(figsize = (10,7))\nsns.set(font_scale=1.5)\nax = sns.heatmap(df_cm,cmap = 'Greens', annot=True,annot_kws={\"size\": 16}, fmt='g')\nax.set_title('Confusion Matrix')\nprint(\"Classification Report: \")\nprint(classification_report(y_test,predictions))","1ab3f388":"import scikitplot as skplt\nimport matplotlib.pyplot as plt\ny_true = y_test\ny_probas = best_rfc_model.predict_proba(X_test)\nskplt.metrics.plot_roc(y_true, y_probas, \n                             title = 'ROC Curve',\n                             figsize = (12,8))\nplt.grid(b = 'Whitegrid')","8311e050":"from sklearn.metrics import accuracy_score\nprint(\"Test Accuracy: \" + str(\"{:.2f}\".format(accuracy_score(y_test, predictions))))\npredictions2 = best_rfc_model.predict(X_train)\nprint(\"Train Accuracy: \" + str(\"{:.2f}\".format(accuracy_score(y_train, predictions2))))","f40ac617":"Survived = pd.Series(best_rfc_model.predict(test.drop('PassengerId',axis=1)),name='Survived')\nmy_solution = pd.concat([test['PassengerId'],Survived], axis=1)\nmy_solution.to_csv('my_output_rfc.csv',index=False)","36bc9320":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train_numeric = X_train._get_numeric_data()\nX_test_numeric = X_test._get_numeric_data()\ntest_copy = test.copy()\ntest_copy.drop('PassengerId', axis = 1, inplace = True)\ntest_numeric = test_copy._get_numeric_data()\nX_train_numeric_scaled = pd.DataFrame(scaler.fit_transform(X_train_numeric), \n                                      index=X_train.index,\n                                      columns=X_train_numeric.columns)\nX_test_numeric_scaled = pd.DataFrame(scaler.transform(X_test_numeric), \n                                     index = X_test.index, \n                                     columns=X_test_numeric.columns)\ntest_numeric_scaled = pd.DataFrame(scaler.transform(test_numeric), \n                                     index = test.index, \n                                     columns = test_numeric.columns)\nX_train.update(X_train_numeric_scaled)\nX_test.update(X_test_numeric_scaled)\ntest.update(test_numeric_scaled)\ndisplay(X_train.head())\ndisplay(X_test.head())\ndisplay(test.head())","999afc3f":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation,Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Dropout\n\nmodel = Sequential()\n\n##### Some References:\n# https:\/\/stats.stackexchange.com\/questions\/181\/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw\n# https:\/\/stats.stackexchange.com\/questions\/164876\/tradeoff-batch-size-vs-number-of-iterations-to-train-a-neural-network\n# https:\/\/datascience.stackexchange.com\/questions\/18414\/are-there-any-rules-for-choosing-the-size-of-a-mini-batch\n\n#Initializer\nmodel = Sequential()\n\n#Input Layer\nmodel.add(Dense(units = 14,activation='relu'))\nmodel.add(Dropout(0.5))\n\n#Hidden Layer\nmodel.add(Dense(units = 9,activation='relu'))\nmodel.add(Dropout(0.5))\n\n#Output Layer\nmodel.add(Dense(units = 1,activation='sigmoid'))\n\n#For binary classification problem, loss function is 'binary_crossentropy'\nmodel.compile(loss='binary_crossentropy', optimizer='adam')\n\n#EarlyStopping\nearly_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience = 25)\n\nmodel.fit(x=X_train, \n          y=y_train, \n          epochs=5000,\n          validation_data=(X_test, y_test), verbose=1,\n          callbacks=[early_stop])","4497c96c":"model.summary()","ae2365e9":"model_loss = pd.DataFrame(model.history.history)\nmodel_loss.plot()","d45ae898":"predictions = (model.predict(X_test) > 0.5).astype(\"int32\")\npredictions[0:5]","050488b4":"from sklearn.metrics import classification_report,confusion_matrix\ndata = confusion_matrix(y_test, predictions)\ndf_cm = pd.DataFrame(data, columns=np.unique(y_test), index = np.unique(y_test))\ndf_cm.index.name = 'Predicted'\ndf_cm.columns.name = 'Actual'\nplt.figure(figsize = (10,7))\nsns.set(font_scale=1.5)\nax = sns.heatmap(df_cm,cmap = 'Greens', annot=True,annot_kws={\"size\": 16}, fmt = 'g')# font size\nax.set_title('Confusion Matrix')\nprint(\"Classification Report: \")\nprint(classification_report(y_test,predictions))","cc7398c8":"Survived = (model.predict(test.drop('PassengerId',axis=1)) > 0.5).astype(\"int32\")\nSurvived = Survived.flatten()\nSurvived = pd.Series(Survived,name='Survived')\nmy_solution = pd.concat([test['PassengerId'],Survived], axis=1)\nmy_solution.to_csv('my_output_ann.csv',index=False)","14602965":"### Check for missing values","e166f4cd":"### Define IVs and DV (X & y)","57b0e3b3":"##### Impute the msising values for the Embarked class with the most frequent class","8ee63153":"##### Take a peek at the predictions","e8777e29":"## Data Preprocessing","daeec858":"### Remove MultiCollinearity Issues from Numeric Features using Variance Inflation Factor","eaad8460":"### Predict","c25ba56a":"##### Splitting back to train & test","6751b76a":"##### Pclass, Embarked and Sex definitely are important features in predicting survival","39240e53":"### Create and Train the Model\n##### To avoid overfitting, we will utilize early stopbacks and dropout layers","da7fc5d6":"### Evaluate the Model","8735ea46":"##### Plot the Model Loss","c17563b1":"##### Dropping unnecessary columns - these for me are unnecessary","356dff17":"## Random Forest","58f9dc7f":"### Split to Train & Test","5f56034b":"### Get an overview of the importance of each of the features","f7b18dd5":"##### Find the optimal values for the most commonly tuned hyperparameters for a Random Forest Classifier. One can refer to https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html for the complete documentation of the tunable hyperparameters","df597565":"##### Transforming necessary features to categorical type.\n##### Note that I've excluded Pclass because I want to treat it as a feature having ordinality","bb06e8ec":"##### Check the new Feature","9cd08184":"### Imputing Values","f0bd232d":"##### Impute the mean for the Age & Fare features","df08e827":"##### No collinearity issues detected","7dfaed65":"##### b.) ROC Curve & AUC","8eeff784":"##### Check the new features","f8d8d74a":"##### Feature Engineering # 2 - Extract the Titles from Names and replace it with common titles","55e40ff2":"## Artificial Neural Networks","37b8ac6a":"##### Feature Engineering 3 - Combine SibSp & Parch to a single feature called \"FamilySize\"","baae4403":"### Data Exploration & Feature Engineering","e0bae2db":"##### Combining for easier cleaning","7dd6755a":"### Predict","bb74930b":"### Encoding Categorical Features","dad932e7":"### Evaluate the Model","45277685":"##### Submission 1 - RFC","959ada4e":"##### Now we're ready to do some modeling","1c5cdc6f":"### Importing the Libraries","9c62f4d6":"Submissions' Scores:\n\n**RF - 0.77990** -> I believe this can be further improved by doing legit hyperparameter optimization in GridSearch (The code I used was just to make the run time faster because the main purpose of this notebook is just for demonstration. I remember I scored 0.8032 when I used R)\n\n**ANN - 0.79665** -> This can also be improved by tweaking some parameters in your network.\n\nThat's all!\nPlease upvote if you learned something from this notebook :), You can also view my other notebooks and hoefully learn something from there as well.","decf3c9a":"##### Feature Engineering #1 - Divide the Age Feature into 3 categories, 0-18 years old, 18-50 years old & 50+ years old","cfc1a579":"### This kernel is intended for beginners. The goal is demonstrate RF and ANN implementation in Titanic Dataset. If you find this kernel helpful, your upvotes would be highly appreciated :). You can also check out my other notebooks :)","4a1ea7d4":"##### Model Summary","d7feefde":"##### Take a peek at the Data","1c44d35d":"##### Count Plots","cd1764f5":"##### ANNs will require scaling, so we need to scale the numeric features","3dc601ec":"##### a.) Classification Report and Confusion Matrix","19f4d8b0":"##### c.) Overfitting Check - Train vs Test Accuracy","c4ef4549":"##### Final Cleaning - Drop all unnecessary Features & Other transformations","84a54bcf":"##### Submission 2 - ANN","1b69d29d":"##### sklearn's randomForest requires one hot encoding on categorical features.","4fc1a6d0":"##### Box Plots and Violin Plots","b3dafcc3":"##### Mapping values of the Sex feature","0fe69e3e":"##### Check the new Feature","3fbfb339":"##### Classification Report and Confusion Matrix","edcc30a6":"### Importing the Data","e25f5adc":"##### Although it seems obvious, let's check VIF values to see if there are collinearities that can affect the modelling just to be sure","268c28d6":"### Find the Optimal Model using GridSearchCV"}}