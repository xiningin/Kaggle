{"cell_type":{"45c85202":"code","9a350309":"code","c8db09ad":"code","0d3150dc":"code","67e6e7e8":"code","03cbadbb":"code","89c85269":"code","4e69581a":"code","f1030084":"code","86ddec9c":"code","4e7335f3":"code","2bbe7e84":"code","d937e80f":"code","9a6de859":"code","a1a4c151":"code","bfddefda":"code","f16267d6":"code","4fef9f57":"code","7e885331":"code","028d87a2":"code","cf077cd6":"code","948eb735":"code","bf1e3d5a":"code","00da0170":"code","0aa14164":"code","845faa83":"markdown","1212951a":"markdown","ed35c8ef":"markdown","34529777":"markdown","df5dd56b":"markdown"},"source":{"45c85202":"!pip install scikit-learn -U","9a350309":"!rm -r \/opt\/conda\/lib\/python3.7\/site-packages\/lightgbm","c8db09ad":"!git clone --recursive https:\/\/github.com\/Microsoft\/LightGBM","0d3150dc":"!apt-get install -y -qq libboost-all-dev","67e6e7e8":"# If you have trouble with cmake, run this: ldd \"$(type -p cmake)\"\n# and find out what library is missing or has a wrong version\n# !rm \/opt\/conda\/lib\/libcurl.so.4","03cbadbb":"# %%bash\n# cd LightGBM\n# mkdir build\n# cd build\n# cmake -DUSE_GPU=1 -DOpenCL_LIBRARY=\/usr\/local\/cuda\/lib64\/libOpenCL.so -DOpenCL_INCLUDE_DIR=\/usr\/local\/cuda\/include\/ ..\n# make -j$(nproc)","89c85269":"!mkdir -p \/etc\/OpenCL\/vendors && echo \"libnvidia-opencl.so.1\" > \/etc\/OpenCL\/vendors\/nvidia.icd\n!rm -r LightGBM","4e69581a":"!pip install autogluon ","f1030084":"# Importing core libraries\nimport numpy as np\nimport pandas as pd\nimport gc\n\n# Importing AutoGluon\nfrom autogluon.tabular import TabularDataset, TabularPredictor\n\n# Scikit Learn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.pipeline import Pipeline","86ddec9c":"# Derived from the original script https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage \n# by Guillaume Martin\n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","4e7335f3":"# Loading data \nX_train = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/train.csv\").set_index('id')\nX_test = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/test.csv\").set_index('id')\nX_train.head()","2bbe7e84":"# Feature engineering\n# unique_values = X_train.iloc[:1000].nunique() < 0\n# categoricals = [col for col in  unique_values.index[unique_values < 10] if col!='target']\nnumeric = [col for col in X_train.columns  if col!='target']\n# print(\"categoricals\",categoricals)\nprint(\"numeric\",numeric)\nX_train['mean_numeric'] = X_train[numeric].mean(axis=1)\nX_train['std_numeric'] = X_train[numeric].std(axis=1)\nX_train['min_numeric'] = X_train[numeric].min(axis=1)\nX_train['max_numeric'] = X_train[numeric].max(axis=1)\n\nX_test['mean_numeric'] = X_test[numeric].mean(axis=1)\nX_test['std_numeric'] = X_test[numeric].std(axis=1)\nX_test['min_numeric'] = X_test[numeric].min(axis=1)\nX_test['max_numeric'] = X_test[numeric].max(axis=1)\n\nnumeric+=['mean_numeric','std_numeric','min_numeric','max_numeric']\nprint(\"numeric\",numeric)","d937e80f":"X_train.head()\n# X_train.columns.tolist()","9a6de859":"# Feature selection\nfeatures = ['f0',\n 'f1',\n 'f2',\n 'f3',\n 'f4',\n 'f5',\n 'f6',\n 'f7',\n 'f8',\n 'f9',\n 'f10',\n 'f11',\n 'f12',\n 'f13',\n 'f14',\n 'f15',\n 'f16',\n 'f17',\n 'f18',\n 'f19',\n 'f20',\n 'f21',\n 'f22',\n 'f23',\n 'f24',\n 'f25',\n 'f26',\n 'f27',\n 'f28',\n 'f29',\n 'f30',\n 'f31',\n 'f32',\n 'f33',\n 'f34',\n 'f35',\n 'f36',\n 'f37',\n 'f38',\n 'f39',\n 'f40',\n 'f41',\n 'f42',\n 'f43',\n 'f44',\n 'f45',\n 'f46',\n 'f47',\n 'f48',\n 'f49',\n 'f50',\n 'f51',\n 'f52',\n 'f53',\n 'f54',\n 'f55',\n 'f56',\n 'f57',\n 'f58',\n 'f59',\n 'f60',\n 'f61',\n 'f62',\n 'f63',\n 'f64',\n 'f65',\n 'f66',\n 'f67',\n 'f68',\n 'f69',\n 'f70',\n 'f71',\n 'f72',\n 'f73',\n 'f74',\n 'f75',\n 'f76',\n 'f77',\n 'f78',\n 'f79',\n 'f80',\n 'f81',\n 'f82',\n 'f83',\n 'f84',\n 'f85',\n 'f86',\n 'f87',\n 'f88',\n 'f89',\n 'f90',\n 'f91',\n 'f92',\n 'f93',\n 'f94',\n 'f95',\n 'f96',\n 'f97',\n 'f98',\n 'f99',\n 'mean_numeric',\n 'std_numeric',\n 'min_numeric',\n 'max_numeric']\n\nX_train = X_train[features + ['target']]\nX_test = X_test[features]","a1a4c151":"### REDUCE MEMORY USAGE\nX_train = reduce_mem_usage(X_train)\nX_test = reduce_mem_usage(X_test)\ngc.collect()","bfddefda":"VALIDATION = False\nif VALIDATION is True:\n    X_train, X_val = train_test_split(X_train, test_size=int(len(X_train) * 0.2), random_state=42)\n    train_data = TabularDataset(X_train)\n    val_data = TabularDataset(X_val)\nelse:\n    train_data = TabularDataset(X_train)\n    val_data = TabularDataset(X_train.iloc[:100_000, :])\n\nSUBSAMPLE = False\nRANDOM_STATE = 0\nif SUBSAMPLE is True:\n    subsample_size = 100_000  # subsample subset of data for faster demo, try setting this to much larger values\n    train_data = train_data.sample(n=subsample_size, random_state=RANDOM_STATE)\n    \ntrain_data.head()","f16267d6":"label = 'target'\nprint(\"Summary of target variable: \\n\", train_data[label].describe())","4fef9f57":"!mkdir agModels","7e885331":"xgb_params = {\n    'objective': 'binary:logistic',\n    'eval_metric': 'auc',\n    'tree_method': 'gpu_hist',\n    'use_label_encoder': False,\n    'n_estimators': 10000,\n    'max_depth': 3,\n    'subsample': 0.5,\n    'colsample_bytree': 0.5,\n    'learning_rate': 0.01187,\n#     'gpu_id': 0,\n#     'predictor': 'gpu_predictor'\n}\n\ncb_params = {\n    'loss_function' : 'CrossEntropy',\n    'eval_metric' : 'AUC',\n    'iterations' : 10000,\n    'grow_policy' : 'SymmetricTree',\n    'use_best_model' : True,\n    'depth' : 5,\n    'l2_leaf_reg' : 3.0,\n    'random_strength' : 1.0,\n    'learning_rate' : 0.1,\n#     'task_type' : 'GPU',\n#     'devices' : '0',\n    'verbose' : 0\n}\n\nlgb_params = {\n    'objective' : 'binary',\n    'metric' : 'auc',\n    'max_depth' : 3,\n    'num_leaves' : 7,\n    'n_estimators' : 5000,\n    'colsample_bytree' : 0.3,\n    'subsample' : 0.5,\n    'reg_alpha' : 18,\n    'reg_lambda' : 17,\n    'learning_rate' : 0.095,\n#     'device' : 'gpu'\n}","028d87a2":"save_path = 'agModels'  # specifies folder to store trained models\npresets='best_quality'\nmetric = 'roc_auc'\nhours = 8.0\npredictor = (TabularPredictor(label=label, eval_metric=metric, path=save_path)\n             .fit(train_data,\n                  excluded_model_types = ['KNN', 'XT' ,'RF', 'NN', 'FASTAI'],\n                  hyperparameters = {'GBM': lgb_params, \n                                     'CAT': cb_params,\n                                     'XGB': xgb_params\n                                    },\n                  presets=presets,\n                  time_limit= int(60 * 60 * hours))\n            )\n","cf077cd6":"results = predictor.fit_summary(show_plot=True)","948eb735":"leaderboard = predictor.leaderboard(val_data)","bf1e3d5a":"test_data = TabularDataset(X_test)\ntest_preds = predictor.predict_proba(test_data)","00da0170":"# Predicting and submission\nsubmission = pd.DataFrame({'id':X_test.index, \n                           'target': test_preds.iloc[:,1].ravel()})\n\nsubmission.to_csv(\"submission_autogluon_PL.csv\", index=False)","0aa14164":"submission","845faa83":"You can actually use the optimized parameters that you can find on public kernels to boost your AutoGluon performances.\n\nFor instance these parameters are from the high scoring notebook https:\/\/www.kaggle.com\/dlaststark\/tps-1021-la-dee-da by DLASTSTARK","1212951a":"AutoGluon is an auto-ml package, developed by J Mueller, X Shi, A Smola:\n\nMueller, Jonas, Xingjian Shi, and Alexander Smola. \"Faster, Simpler, More Accurate: Practical Automated Machine Learning with Tabular, Text, and Image Data.\" Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 2020.\n\nFor tabular data, AutoGluon can produce models to predict the values in one column based on the values in the other columns. With just a single call to fit(), you can achieve high accuracy in standard supervised learning tasks (both classification and regression).\n\nIn the economy of a competition it can help you to create benchmarks, get insights on models' workings and accelerate your experimentations.","ed35c8ef":"Installing the latest Scikit-learn","34529777":"Installing LightGBM for GPU","df5dd56b":"Installing AutoGluon"}}