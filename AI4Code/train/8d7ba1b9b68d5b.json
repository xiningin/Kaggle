{"cell_type":{"ddc89912":"code","85b5f081":"code","9b3f98a4":"code","da36f7a5":"code","85139449":"code","f8ade523":"code","9fb09b56":"code","9179d076":"code","66cd16f1":"code","f940000b":"code","15f40577":"code","fef398a4":"code","ed41f914":"code","740d85a2":"code","5df5c9c0":"code","eaf84e61":"code","c1a5d012":"code","532d1831":"code","7eee3229":"code","048a2f8f":"code","fe980ad2":"code","cf7b6018":"code","ad22154f":"code","62243f77":"code","24613696":"code","b6ca339e":"code","1595fcd3":"code","3d00fcf9":"code","2da12327":"code","945edd56":"code","be441e7b":"code","6d794b1e":"code","2984ab74":"code","3bf32a8e":"code","2e168438":"code","b7e6c4bc":"code","6c8fceea":"code","2d52ddde":"code","f50ba36a":"code","67622451":"code","e503173a":"code","b8a080bb":"code","f412f856":"markdown","64858d85":"markdown","23bf5fd3":"markdown","132be844":"markdown","6b2d3fb1":"markdown","1e1bd575":"markdown","99e1f2d6":"markdown","4f89f957":"markdown","e8a8defc":"markdown","159abf01":"markdown","02a7fc1c":"markdown","59679fb4":"markdown","24692cba":"markdown","7beb1d91":"markdown","216cc8af":"markdown","a3a0c989":"markdown","7eb903d9":"markdown","9370f3b0":"markdown","203ab39c":"markdown","196e7d24":"markdown","b4aa4191":"markdown","dcfa61de":"markdown","d68d8ff6":"markdown","eb5e1635":"markdown","84a19d2a":"markdown","42398168":"markdown","db145628":"markdown","22cfe601":"markdown","dce330c1":"markdown","9b9d1d33":"markdown","51285ddd":"markdown","26177f1b":"markdown","d5d7b46d":"markdown","c9850631":"markdown","91375a75":"markdown","1a4bfddc":"markdown","69b18479":"markdown","f3e228c2":"markdown","dd8fe9fb":"markdown","cf43443b":"markdown","4e871d79":"markdown","23e2d81e":"markdown","df4bb0d4":"markdown","a3abc0ab":"markdown","a96e5772":"markdown","082f63c4":"markdown","ad26d245":"markdown","aae6c4c6":"markdown","227ddde8":"markdown","101358ab":"markdown","30e41b8a":"markdown","419b7548":"markdown","f9703cad":"markdown","71820fc2":"markdown","c54b21ea":"markdown","6c361f14":"markdown"},"source":{"ddc89912":"import numpy as np\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","85b5f081":"a = np.array([3, 2])\nprint(\"An array:\")\nprint(a)\nprint(\"Shape of a: (note the weird shape)\")\nprint(a.shape)\nprint(\"==================================\")\nx = np.array([[3, 2]])\nprint(\"A row vector:\")\nprint(x)\nprint(\"Shape of x:\")\nprint(x.shape)\nprint(\"==================================\")\ny = np.array([[3], [2]])\nprint(\"A column vector:\")\nprint(y)\nprint(\"Shape of y:\")\nprint(y.shape)","9b3f98a4":"X = np.array([[3, 2], [-1, 3]])\nprint(\"This is a matrix\")\nprint(X)\nprint(\"Shape of X:\")\nprint(X.shape)","da36f7a5":"print(X.reshape(1,4))\nprint(X.reshape(1,-1)) # Notice the -1","85139449":"a = np.arange(2,12)","f8ade523":"a","9fb09b56":"a[3:8] # Extract a subarray from the 3rd element to (but not include) the 8th one.","9179d076":"my_string = \"Hallo\"\nmy_substring = my_string[1:4] # instead of using some string.substring() function, if it even exists!\nprint(my_substring)","66cd16f1":"a = np.array([[1, 8, -3, 0], [2, 1, -1, 5], [6, 5, 7, 1], [-2, -1, 1,4]])\nprint(a[1:3,1:3])","f940000b":"print(a[1:3,:]) # take all the columns in the sliced rows\nprint(a[:,1:3]) # take all the rows in the sliced columns","15f40577":"a = np.array([[1, 8, -3, 0], [2, 1, -1, 5], [6, 5, 7, 1], [-2, -1, 1,4]])\na = np.tile(a, (3,1,1)).T\na","fef398a4":"plt.imshow(np.maximum(a\/np.max(a), 0))","ed41f914":"print(a[:,:,1]) # Get the green channel (every values for width and height)","740d85a2":"a[:,:,1] = np.zeros((4,4)) # Set the green channel to all 0s\na","5df5c9c0":"plt.imshow(np.maximum(a\/np.max(a), 0)) # red + blue = pink","eaf84e61":"a = np.array([[1, 8, -3, 0], [2, 1, -1, 5], [6, 5, 7, 1], [-2, -1, 1,4]])\na = np.tile(a, (3,1,1)).T\na = np.tile(a, (50,1,1,1))","c1a5d012":"print(a[5,:,:,1]) # Get the green channel (every values for width and height) of the $5^{th}$ image","532d1831":"print(np.sum(a, axis=3).shape)\nnp.sum(a, axis=3)","7eee3229":"print((np.sum(a, axis=(0))\/50).shape)\nnp.sum(a, axis=(0))\/50","048a2f8f":"np.sum(a, axis=(0,3))","fe980ad2":"a = np.array([[2, 1, -1, 0]]) # A row vector with the shape (1,4)\nb = 2 # A scalar value\n# What is a + b? Is it compatible to add a vector and a scalar?","cf7b6018":"c = a + b\nprint(\"a + b:\")\nprint(c)\nprint(a.shape==c.shape)\nbrd_b = np.array([2, 2, 2, 2]) # brd_b = np.repeat(b, 4, axis=0)\nprint(brd_b)\nprint(c)\nprint(np.array_equal(a + b, a + brd_b))\n","ad22154f":"X = np.arange(1,10).reshape(3,3)\nprint(\"X:\")\nprint(X)\nd = np.array([1,-1,1]).reshape(1,-1)  # Note how we reshaped d \nprint(\"d:\")\nprint(d)\nprint(\"X + d: (d is broadcasted along the first dimension)\")\nprint(X + d) \nprint(\"\\n========================\")\nd = np.array([1,-1,1]).reshape(-1,1)  # Note how we reshaped d \nprint(\"d:\")\nprint(d)\nprint(\"X + d: (d is broadcasted along the second dimension)\")\nprint(X + d)","62243f77":"a = np.arange(20, step=2)\nprint(a)\nprint(\"Min: \" + str(a.min()))\nprint(\"Max: \" + str(a.max()))\nprint(\"Mean: \" + str(a.mean()))\nprint(\"Standard Deviation: \" + str(a.std()))\nprint(\"Norm of vector a: \" + str(np.linalg.norm(a)))\n","24613696":"x = np.array([128, 1, -3, 2, 0.4, 1000]).reshape(-1,1)\nx_prime = (x - x.min()) \/ (x.max() - x.min())\nprint(x_prime)\nplt.bar(range(x.shape[0]), x_prime.ravel())","b6ca339e":"x = np.array([128, 1, -3, 2, 0.4, 1000]).reshape(-1,1)\nx_std = (x - x.mean()) \/ x.std()\nprint(x_std)\nplt.bar(range(x.shape[0]), x_std.ravel())","1595fcd3":"print(np.random.randn(2,4)) # You should use this for your weight initialization","3d00fcf9":"print(np.random.rand(2,4))","2da12327":"print(np.random.randint(0,10)) # note the upper limit","945edd56":"print(np.random.permutation([1, -4, 3, 2, -6]))\n\narr = np.arange(9).reshape((3, 3))\nprint(np.random.permutation(arr))","be441e7b":"print(\"When we do not specify the seed:\")\nfor i in range(3):\n    print(np.random.randint(0,10))\nprint(\"When we specify the same seed:\")\nfor i in range(3):\n    np.random.seed(1111) # set the same seed 1111 before every random generation\n    print(np.random.randint(0,10))\nprint(\"When we specify different seeds:\")\nfor i in range(3):\n    np.random.seed(i * 3)\n    print(np.random.randint(0,10))","6d794b1e":"n_in = 3 # the number of neurons from the previous layer\nn_out = 2 # the number of neurons from the current layer\nW = np.random.randn(n_out, n_in) * (0.1) \nprint(W)","2984ab74":"n_in = 3 # the number of neurons from the previous layer\nn_out = 2 # the number of neurons from the current layer\nW = np.random.randn(n_out, n_in) * (np.sqrt(2. \/ (n_in + n_out)))  # Xavier initialization\nprint(W)","3bf32a8e":"n_in = 3 # the number of neurons from the previous layer\nn_out = 2 # the number of neurons from the current layer\nW = np.random.randn(n_out, n_in) * (np.sqrt(1. \/ n_in))  # He initialization\nprint(W)","2e168438":"def sm_sample(pa = 0.3, pb = 0.6, pc = 0.1):\n\n    r = np.random.rand()\n    if (r < pa):\n        output = 'a'\n    elif (r < pa + pb):\n        output = 'b'\n    else:\n        output = 'c'\n    return output\n\noutputs = []\nfor i in range(10):\n    outputs.append(sm_sample())\nprint(outputs)\n\noutputs = []\n# Law of large numbers: 100000 is large enough for our sample to approximate the true distribution\nfor i in range(100000):  \n    outputs.append(sm_sample())\n\nfrom collections import Counter\nc_list = Counter(outputs)\nprint(c_list) ","b7e6c4bc":"def sm_sample_general(out, smp):\n    # out contains possible outputs\n    # smp contains the softmax output distributions\n    return np.random.choice(out, p = smp)\n\nout = ['a', 'b', 'c']\nsmp=np.array([0.3, 0.6, 0.1])\n\noutputs = []\nfor i in range(10):\n    outputs.append(sm_sample_general(out, smp))\nprint(outputs)\n\noutputs = []\n# Law of large numbers: 100000 is large enough for our sample to approximate the true distribution\nfor i in range(100000):  \n    outputs.append(sm_sample_general(out, smp))\n\nfrom collections import Counter\nc_list = Counter(outputs)\nprint(c_list) ","6c8fceea":"# The normal for-loop\ndef my_dot(a, b):\n    out = 0\n    #assert(a.shape[0]==b.shape[0])\n    for i in range(a.shape[0]):\n        out += a[i] * b[i]\n    return out","2d52ddde":"n = 1000\na = np.random.randn(n)\nb = np.random.randn(n)\n","f50ba36a":"%%timeit -n 10\nmy_dot(a,b)","67622451":"%%timeit -n 10\nnp.dot(a,b)","e503173a":"np.random.seed(1234)\n\ndropout_p = 0.8                         # keep around 80% number of neurons\n\nA_prev = np.random.randn(100, 50)       # Activations from the previous layer, mini-batch training\nW = np.random.randn(30, 100)            # the previous layer has 100 neuron, \n                                        # the current layer to be dropped-out has 30 neurons\nb = np.random.randn(30, 1)\n\n# Dropout\n# CODE HERE                             # Initialize a random, uniform-distribute vector with the same shape\n# CODE HERE                             # Mask the drop-out: True (1) if we want to keep the corresponding neuron\n                                        # False (0) if the neuron is dropped out\n# CODE HERE                             # Apply the mask\n# CODE HERE                             # Scale the activation after dropping a number of neurons\n\n# Do the forward pass\nA = np.tanh(np.dot(W, A_prev) + b)\n\nprint(A[3:6, 2:4])\n","b8a080bb":"np.random.seed(1234)\n\ndropout_p = 0.8                         # keep around 80% number of neurons\n\nA_prev = np.random.randn(100, 50)       # Activations from the previous layer, mini-batch training\nW = np.random.randn(30, 100)            # the previous layer has 100 neuron, \n                                        # the current layer to be dropped-out has 30 neurons\nb = np.random.randn(30, 1)\n\n# Dropout\ndropped = np.random.rand(100, 50)       # Initialize a random, uniform-distribute vector with the same shape\nmasked = (dropped < dropout_p)          # Mask the drop-out: True (1) if we want to keep the corresponding neuron\n                                        # False (0) if the neuron is dropped out\nA_prev = A_prev * masked                # Apply the mask\nA_prev = A_prev \/ dropout_p             # Scale the activation after dropping a number of neurons\n\n# Do the forward pass\nA = np.tanh(np.dot(W, A_prev) + b)\n\nprint(A[3:6, 2:4])\n","f412f856":"What if we want to aggregate the pixel intensities through the channels (the last dimension) and also all of the images?","64858d85":"__Pair-wise__: \n<br\/>\nThe element-wise arithmetic operation between two same-shape arrays (also counted the broadcasted array, so be careful). This one is quite easy to vectorize: $ u + v $, $ u * v $, $ numpy.maximum(u,v)$ (element-wise get the max, different from $ numpy.max(u) $),...\n<br\/>\n<br\/>\n__Element-wise application__: \n<br\/>\nThis one is similar to the other one, just find some numpy built-in function supporting vectorization, e.g. $np.exp(u)$, $np.tanh(u)$ (but numpy does not have the built-in $sigmoid(u)$, you can use $np.exp(u)$ to do that).\n<br\/>\n<br\/>\n__Aggregation function__: \n<br\/>\nThe operations belonging to this one often follows the __pair-wise__ (e.g., in $np.dot$) and they are the most difficult ones since we have to choose the correct axis to aggregate in case the array is multi-dimensional. Another notice is that you should not wrongly take them as their similar pair-wise functions. Let's go through the following example. \n\n\n","23bf5fd3":"# <center> KIT Praktikum Neuronale Netze: Numpy Tutorial <\/center>\n\n\nOn this exercise, you are warming up (again) with `Numpy`, especially with multi-dimentional arrays and vectorization your deep learning algorithm. Many methods from Deep Learning (DL) frameworks like `TensorFlow` or `Pytorch` work in the similar way to Numpy methods, so being familiar with `Numpy` brings you great advantages. Furthermore, some parts\/modules\/phases of your NN applications\/projects would use `Numpy` rather than the DL frameworks. Often, `Numpy` and DL codes are mixed in your implementation and you need to know when to use `Numpy` and when to use your DL frameworks.   \n\nYou will learn to:\n* Work with __multi-dimentional arrays__\n* Understand what is __broadcasting__ and how to make use of it (and how to avoid unwanted broadcasting)\n* Use `random` package\n* Do __vectorization__\n\n","132be844":"__Note__: The intuition behind those initialization methods comes from the fact that we try to force the distribution of the output in the current layer is close as much as possible to that of the input so that the learning is easier. Of course when doing so, we need to consider the activation function, _fan in_ (number of neurons from the previous layer), _fan out_ (number of neurons from the current layer) and even how we choose randomly number. For more intuition about those initializations, feel free to read this: http:\/\/andyljones.tumblr.com\/post\/110998971763\/an-explanation-of-xavier-initialization. And you might also want to look at the following table from https:\/\/mnsgrg.com\/2017\/12\/21\/xavier-initialization\/:\n\n<center><img src=\"https:\/\/github.com\/thanhleha-kit\/PraktikumNeuronaleNetze\/blob\/master\/Images\/initializations.png?raw=true\" style=\"width:900px;height:248px\"><\/center>\n","6b2d3fb1":"__Notes__:\n  1. At this time we work with vectors and matrices, so at the moment, you can think about printing out those to see them and understanding them as rows and columns __only for now__. Soon, however, we will work with multi-dimensional $n$-dim arrays ($n \\geq 3$), so do not heavily rely on your intuitions in the 2D or 3D space.\n  2. In deep learning\/machine learning, any vector should not be understood as a vector in a __geometric__ world but it is a __physical__ vector, i.e. a specific state of some attributes of an object, or an ordered list of attribute values in the __computer science__ world. For example, [2, 3, 100, 30000] is a vector denoting a house's attributes: it has 2 bathrooms, 3 bedrooms, its area is 100 squared meters and it costs 30000 EUR. And this understanding should be expanded to the more general multi-dimensional arrays. Another example: the 2D array, or matrix above, [[3, 2], [-1, 3]], might denote the grayscale values of a $2 \\times 2$-pixel image, but it might denote the weight matrix between a 2-neuron layer and another 2-neuron layer in a deep neural network, so again, o not heavily rely on your intuitions in the 2D or 3D space.\n\nLet's get used to this way of thinking by doing array slicing.\n  \n  ","1e1bd575":"You all know that __Dropout__[[Hinton et al., 2012](https:\/\/arxiv.org\/pdf\/1207.0580.pdf)] is a good regularization method. During the forward phase of our training, we drop some neurons by a probability $p$ so that there is no incoming and outgoing connections from those neurons, __only for that iteration__. In this example we are going to implement the forward step of the training for one layer with its activation ReLU. The following practical way works best with the activation functions which return 0 for the inputs of 0 (for almost good activation like ReLU, tanh,..), but just leave the generalization aside for now. The direction is to create some masked vector to decide which inputs to keep, which inputs we set to 0s. \nThe steps are:\n  1. Initialize a random, uniform-distribute vector whose shape is the same as our layer, e.g. 100 neurons.\n  2. Create the masked vector by comparing our initialized vector with the dropout probability $p$. (In convention, this is actually a keeping probability).\n  3. Based on the masked vector, keep or zero-out the inputs of the layer (which are actually the outputs of the previous layer, or the input features, in case this layer is the first layer). \n  4. (Optional but helpful) Scale the inputs to compensate the dropped neurons. \n  \n__Try to do it yourself first on the cell below (fill in the 5 lines of code marked__ _#CODE HERE_ __) before looking at the solution in the next cell__.","99e1f2d6":"However, a constant factor $\\alpha = 0.1$ is not good for all layer architectures. Often, __Xavier initialization__[[Glorot and Bengio, 2010](https:\/\/arxiv.org\/pdf\/1207.0580.pdf)] or __He initialization__[[He et al., 2015](https:\/\/www.cv-foundation.org\/openaccess\/content_iccv_2015\/papers\/He_Delving_Deep_into_ICCV_2015_paper.pdf)] is better, when they take the size of the layers into account (and to be more precisely, they also consider the activation functions).","4f89f957":"To help you build the nature adaption from for-loop operations to vectorization, let's try to classify the algebra arithmetic operations and focus on how to vectorize them. Besides some numpy array initialization functions, there are three main types of those operations: element-wise operations between two arrays (or I could call it _pair-wise_), _element-wise application_ of a function on an array and _aggregation_ operations on an array. ","e8a8defc":"\nSorry, the code above is just for illustration. If there are many possible outputs (not just 3 like in this case), we need to loop through the ifs. Fortunately, the random package has this sampling method already: `numpy.random.choice`:","159abf01":"__If you implement correctly, the output should be:__\n<br\/>\n[[ 1.&emsp;&emsp;&emsp;&emsp;&ensp;&ensp;-0.99841418]<br\/>\n [-0.89187979&emsp;0.99999996]<br\/>\n [ 1.&emsp;&emsp;&emsp;&emsp;&emsp;&ensp;0.99999776]]","02a7fc1c":"Generate a $2 \\times 4$ matrix (2D array) of random numbers in range [0,1) sampled from the (continuous) uniform distribution:","59679fb4":"# 2. Broadcasting","24692cba":"Generate a matrix (2D array) of random numbers sampled from the standard normal distribution:","7beb1d91":"**Broadcasting** in `Numpy` refers to the implicit compatiblization when doing array arithmetics on different-shaped vectors\/matrices. Of course it is a feature, not an error. If you are familiar with `Matlab\/Octave`, this one is the one you feel `Matlab\/Octave` are lacking of. Broadcasting makes the code more succinct and clear.","216cc8af":"You see? Don't think the first dimension as rows, or axis x, the second as column, or axis y, but just associate the dimensions with your concrete coordinate: the first one denoting a concrete image in your image set, the second one is something about width (actually the width index of a specific pixel), the last one is the different channel and so on. This way of thinking could also applied for cross-dimensional operations like the `numpy.sum()` and other aggregation operations. ","a3a0c989":"__Xavier initialization__:\n    \n$$\\alpha = \\displaystyle\\sqrt{\\frac{2}{n_{in} + n_{out}}}$$\n\nIt's good when use with `sigmoid` or `tanh` activation functions.","7eb903d9":"Now what if we want to find the \"average\" image? We'd do the sum of pixel intensities through all the images and divide to the number of images. Since we duplicated 1 image 50 times, the average image is exactly our initial image: ","9370f3b0":"When you use $numpy.sum()$, just stop for a second and think, which thing I want to sum now, then set the axis correspondingly. For example, what if we want to add the pixel intensities through the channels (the last dimension)?        ","203ab39c":"<center><img src=\"https:\/\/github.com\/thanhleha-kit\/PraktikumNeuronaleNetze\/blob\/master\/Images\/3DMatrixSlicing.png?raw=true\" style=\"width:366px;height:360px\"><\/center>","196e7d24":"<center><img src=\"https:\/\/github.com\/thanhleha-kit\/PraktikumNeuronaleNetze\/blob\/master\/Images\/dotparallelize.png?raw=true\" style=\"width:939px;height:289px\"><\/center>","b4aa4191":"Only care about the rows (blue) or the columns (yellow)? ","dcfa61de":"3D arrays? Like an image with its width, height and RGB channels? More difficult but still manage to do. Below is to extract the Green channel (the second channel), and it becomes the matrix similar to the above matrix, and we can continue our slicing as we wish: ","d68d8ff6":"## Example: Hidden Layer Weight Update\n<br\/>\nIn the slide 17 of the Back Propagation lecture, the update rule for the weights of a hidden layer is written like this:","eb5e1635":"__Standardization__:\n\nYou can see from the chart that min-Max Normalization is not a good normalization for gradient-based learning if the features are in very different ranges like the one above (it's good for our MNIST, however, since the features are pixel intensity in the same range [0,127]). Standardization is better: \n\n$$x' = \\displaystyle\\frac{x - \\bar{x}}{\\sigma(x)}$$\n<br\/>\nHere $\\bar{x}$ is the mean of x and $\\sigma(x)$ is the standard deviation.\n","84a19d2a":"## Example: Xavier Initialization and He Initialization\n\nFor a neural network to learn well, beside feature normalization and other things, we also need proper weight initialization: the weights should be randomly initialized, or at least different numbers (to break the symmetry), and they should be small. In the exercises of our Neuronale Netze course, we usually use the factor of 0.1:\n","42398168":"Broadcasting is extremely useful. However, since it allows some array arithmetics compatible even when the operands have incompatible shape, be careful with it. In the (binary) logistic regression code of the course Neuronale Netze, because we have only one output neuron, the weight matrix $W$ is actually a vector, and the bias $b$ is a scalar. Hence, some of you might initialize $b$ as a scalar. But when we generalize the logistic regression to produce multiple outputs for some multi-class classification problems, $b$ here should be a vector. Then if you initialized it as a scalar, the code still runs, but we have some __logical error__ here, which is difficult to find and debug. The scalar $b$ is broadcasted to be a $m\\times n$-matrix with the same scalar value, but it should be a $m\\times n$-matrix consisting of $m$ same $n$-vectors (with $n$ different scalar values). In other programming languages without broadcasting, an incompatible-shaped error might be raised instead.","db145628":"__Min-Max Normalization__ (or __Rescaling__ to [0,1] range):\n$$x' = \\displaystyle\\frac{x - min(x)}{max(x) - min(x)}$$\n<br\/>\nNote that $x$ here is a vector of input features, $min(x)$ and $max(x)$ are both scalars. Broadcasting allows us to simply do the normalization without replicating $min(x)$ or $max(x)$ to be a vector of same shape with $x$:\n","22cfe601":"So $b$ is broadcasted from a scalar to the vector having the same shape of $a$ by copying the scalar value ($2$) along the broadcasted dimension to make the addition possible and compatible. The similar thing happened to matrix and vector addition or the addition of 2 multi-dimensional numpy arrays:","dce330c1":"<center><img src=\"https:\/\/github.com\/thanhleha-kit\/PraktikumNeuronaleNetze\/blob\/master\/Images\/dropout.png?raw=true\" style=\"width:679px;height:321px\"><\/center>","9b9d1d33":"Calculate the error terms: $\\delta_j = o_j(1-o_j) \\displaystyle\\sum_{k \\in Downstream(j)}\\delta_kw_{kj}$, then $w_{ij} = w_{ij} + \\eta\\delta_jx_{ji}$","51285ddd":"## Example: Dropout\n\nIn this example, we will combine what we learned on $numpy.random$ package and _vectorization_ to implement Dropout.","26177f1b":"__How to vectorize those two steps?__ \n<br\/>\n  1. The first tricky part is $\\displaystyle\\sum_{k \\in Downstream(j)}\\delta_kw_{kj}$, we know it must be a dot product between $w_{kj}$ (the next layer's weights) and $\\delta_k$ (the error terms of the next layer - in _backprop_, it means we already have those error terms calculated). But what is the first argument of $np.dot()$ and what is the second argument? Doing trial and error on the compatibility of the shape? No, just assume that _you have to do the vectorization using pens and papers only_! This depends on your convention of input feature vector $\\textbf{x}$: whether it is a column vector or a row vector. Just assume that your convention says $\\textbf{x}$ is a column vector with the shape $(n,1)$, $n$ is the number of features. Then when you think the number of neurons in one hidden layer is also the number of higher-level features that your neural network learns up to that layers, so the activations of one layer also form a column vector, and the error terms of one layer also form a column vector. Hence, the shape of the vector $\\Delta_{next}$ formed by many error terms $\\delta_k$ of the next layer  is $(k,1)$, and the shape of that dot product output vector $\\Delta$ formed by many error terms $\\delta_j$ of the current layer is $(j,1)$, so $\\Delta = \\textbf{W}_{next}^T\\cdot\\Delta_{next}$ ($\\textbf{W}_{next}$ is the first argument of $numpy.dot()$).\n  2. The second tricky part is $\\eta\\delta_jx_{ij}$. I have seen some students use $numpy.dot()$ instead of element-wise multiplication here for $\\delta_jx_{ij}$ since they think it doesn't make sense if we do element-wise multiplication between a vector and a matrix, and they tested the $numpy.dot()$ and it ran correctly in the logistic regression exercise (because it is only one output so $numpy.sum()$ of it is equal to itself!). But now we learned about broadcasting and we knew that it should be an element-wise multiplication, not the $numpy.dot()$.\n<br\/>\n\nSo the final vectorized version is somehow like this: $\\textbf{W} = \\textbf{W} + \\eta * (\\textbf{o}*(1-\\textbf{o})*np.dot(\\textbf{W}_{next}^T,\\Delta_{next})) * \\textbf{X}$\n<br\/>\n\nFeel free to check the shape compatibility by pens and papers. Please note that $\\textbf{X}$ here is not the input vector $\\textbf{x}$ but it is the input of this current layer (which is the vector\/matrix of the activations from the previous layer). Of course, I don't recommend to include everything on one line like this, it's just served as an illustration for our vectorization.\n\n__Vectorization of $m$ training instances (mini-batch training)__:\n<br\/> We learned from the Neuronale Netze course that mini-batch training is the compromising solution between batch training and online training (or stochastic gradient descent - SGD). Mini-batch training is faster than batch training and it is more stable than SGD. And the nice thing is that we can also vectorize an iteration instead of using a for-loop through every instances of that mini-batch. Assume that mini-batch size is $m$ and the number of features for each instance is $n$, we can now have the matrix of inputs $\\textbf{X}$ with the shape $(n, m)$. Please remember this convention: the number of training instances is the number of columns in that matrix, and the number of row in that matrix is the number of input features. This convention is compatible with the convention we talked before as the input feature vector of one instance is a column vector!    ","d5d7b46d":"Get a permutation of a list or an array:\n","c9850631":"__He initialization__:\n    \n$$\\alpha = \\displaystyle\\sqrt{\\frac{1}{n_{in}}}$$\n\nIt's good when use with `ReLU` activation function.","91375a75":"When I started using `python`, I found slicing very useful. I have a numpy (1D) array or some python list, and I can use slicing to extract continuous elements of that array, similar but less convenient than chossing some page range to print in the PDF printing dialog:","1a4bfddc":"# 4. Vectorization","69b18479":"Let's reshape the matrix $X$ to be a vector (or sometimes we call it __flattening__: making a multi-dimensional array becomes a vector):","f3e228c2":"Let's import `numpy` and explore some basic methods: ","dd8fe9fb":"<center><img src=\"https:\/\/github.com\/thanhleha-kit\/PraktikumNeuronaleNetze\/blob\/master\/Images\/MatrixSlicing2.png?raw=true\" style=\"width:276px;height:282px\"><\/center>\n","cf43443b":"Now bunch of images like that? Should be a 4-dim array, the first one is the number of images. And then extract the green channel of the $m^{th}$ image:","4e871d79":"## Slicing","23e2d81e":"## Example: Feature Normalization\nFrom the Neuronale Netze lectures, you know that gradient-based learning would be better and faster if the input features are normalized. In this example we will implement two normalization methods: Min-Max Normalization and Standardization and you will see how broadcasting is useful.\n\nFirst, let's look at some aggregation (statistics) methods using numpy:","df4bb0d4":"Generating a random value or an array of random values is very important in deep learning, for example when we want to initialize the weights of our neural network, use dropout or sample the result of the output distribution. We use `numpy.random` package to generate an array of random values instead of looping through the random generation of one variable.  ","a3abc0ab":"\n## Example: Sampling from Softmax Distribution\n\nIn the output layer of a multi-class classifier, we often use `softmax` and choose the output that has the maximal output probability. It is not always the case, especially with sequence modeling problems. Sometimes we would like to draw from that distribution. Let's see how to do this using the random package.\n\nAssume that we have three possible outputs '_a_', '_b_', '_c_', for example, the output of an one-character handwritten recognition. The softmax layer produced the following distribution: $p$('a' $|$ input) = $0.3$, $p$('b' $|$ input) = $0.6$, $p$('c' $|$ input) = $0.1$ (note that sum of those probabilities is 1). How to sample from this distribution given the random methods we know?\n\nThe most intuitive solution is generating a random number in [0, 1), say $r$. If $r < 0.3 = p$('a' $|$ input), we generate 'a', if $ 0.3 \\leq r < 0.9 = p$('a' $|$ input) $+$ $p$('b' $|$ input), we generate 'b', elsewhere ($ 0.9 \\leq r < 1 = p$('a' $|$ input) $+$ $p$('b' $|$ input) $+$ $p$('c' $|$ input), we generate 'c'. ","a96e5772":"I assume that you are familiar with some basic numpy methods, or at least know how to look them up on the internet. Now we will focus on linear algebra.\n# 1. Multi-dimentional Arrays","082f63c4":"The random algorithm used for all of the methods above is a _pseudo random generating algorithm_. It is based on some initial state, or \"_seed_\" to generate random numbers. If you do not specify the seed, it can take some number elsewhere to be the seed. But if we specify the same seed every time we generate random numbers, those numbers will be the same. It is good for many cases, for example, replicating exactly the result of your neural network training even you randomly initialized your weights.","ad26d245":"And did you notice that when we \"calculated\" the average image in Section 1, we use broadcasting, but rather for the division of $m=50$ images than the addition? ","aae6c4c6":"Note the differences, while 1D arrays and vectors are mathematically similar concepts, in deep learning, we prefer to _understand vector as a matrix with either one row or one column_, not the 1D array.  ","227ddde8":"As you see, the for-loop way is much slower than the numpy one when we calculated the dot product between two 1000-dim vectors. Feel free to change n to bigger numbers, for example 5000, and you can see the gap is even bigger (~15000 times for 5000-dim vectors). ","101358ab":"# 3. Using `random` package","30e41b8a":"2D arrays a.k.a matrices? No problem! First slice the rows (blue cells), then the columns in those rows (green cells):","419b7548":"`Numpy` processes arrays much more faster and effectively than their python counterparts like lists since they are greatly optimized. __Thus, we should vectorize and avoid using loops as much as possible__. Let's compare two ways of calculating dot product between two vectors:","f9703cad":"Let's go to a specific example:","71820fc2":"## Axes on aggregation functions","c54b21ea":"In Deep Learning, the most obvious application of broadcasting might be when you calculate $Z = Wx + b$. Here, $Wx$  (the _dot product_ of $W$ and $x$) is a matrix and $b$ is a vector, so in a normal programming language, we should duplicate or repeat b several times along a suitable dimension to make the addition compatible, but broadcasting reduces that step. The general rules of broadcasting are fully mentioned here: https:\/\/docs.scipy.org\/doc\/numpy-1.13.0\/user\/basics.broadcasting.html","6c361f14":"Generate a random integer number between 0 and 9 uniformly:"}}