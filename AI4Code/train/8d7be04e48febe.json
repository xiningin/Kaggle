{"cell_type":{"4c9f6c0d":"code","5fd0a302":"code","5265ff14":"code","68469575":"code","2fd58a6e":"code","9b39a4fc":"code","cbf497b5":"code","00db2051":"code","128f377b":"code","5a5177e7":"code","62320b86":"code","d0340d56":"code","7c9d954d":"code","dbfc3784":"code","9527fe22":"code","f9b5ad65":"code","3c5f27c4":"code","600d3fb6":"code","b04b2ce7":"code","88ab73be":"code","2703f735":"code","9f662d59":"code","710856b6":"code","fd1b95ab":"code","6b84cb97":"code","4d44e645":"code","bc5b738d":"code","542b9cea":"code","bdf630e7":"code","ce1c3c1f":"code","ba2102b1":"code","8f5b578e":"code","0a194d6f":"code","7ac2581c":"code","d74cf2df":"code","7589abb8":"code","d3b0e785":"code","c930e203":"code","734f532a":"code","e7015b42":"code","67aa2f74":"code","88fbd819":"code","7f8b781e":"code","8d48ca5c":"code","2ff5726f":"code","72507a83":"code","2cb3166c":"code","a5ef4375":"code","23eab2dd":"code","93f3b112":"code","8266568f":"code","dae9346c":"code","16678702":"code","671e496f":"code","6193eab0":"code","536d975e":"code","dd88f3cf":"code","54493b14":"code","866df667":"code","84b812f3":"code","99cedf8f":"code","abd7bd3d":"code","ee1e55e6":"code","99b1631a":"code","4e3c8a50":"code","5bf7b2b9":"code","73fbfe46":"code","29e09099":"code","de29f6b6":"code","6de88197":"code","c13983c8":"code","a8487a3d":"code","e4129a79":"code","29a82f4f":"code","942a547d":"code","9a5c928a":"code","de1ad8dc":"code","c475e854":"code","80613f22":"code","b40728b1":"code","e7084d6d":"code","bf7467f9":"code","cc842bf1":"code","438b8f8b":"code","d52be530":"markdown","7cd22e04":"markdown","93e98605":"markdown","dbe33621":"markdown","18340c4d":"markdown","a794434f":"markdown","8247e1d7":"markdown","46ce903b":"markdown","08f01b2b":"markdown","a3893588":"markdown","a7cec67f":"markdown","e093e5c6":"markdown","73d10108":"markdown","fcf28725":"markdown","b7082c53":"markdown","589f3559":"markdown","2384ada9":"markdown","985461ae":"markdown","a447a186":"markdown","0a009385":"markdown","1c6c1488":"markdown","04e22a65":"markdown","6afce893":"markdown","91a3f3bc":"markdown","b1347563":"markdown","5e506b7a":"markdown","94c09d63":"markdown","d627cf8d":"markdown","79cb5cfe":"markdown","771baeb5":"markdown","18ab041b":"markdown","3cdfd77c":"markdown","20d6aa88":"markdown","587cbb3c":"markdown","a4afb0d5":"markdown","3957f221":"markdown","969af9f7":"markdown","e981cb14":"markdown","6c59160d":"markdown","7a3cba55":"markdown","1dc6de8f":"markdown","60d9940d":"markdown","8f194790":"markdown","7a8d7b68":"markdown","e5c60eb5":"markdown","8f7e5278":"markdown","be64bd1f":"markdown","5ae4d588":"markdown"},"source":{"4c9f6c0d":"import pandas as pd\n\ndata_temp = pd.read_csv('..\/input\/257k-gaiadr2-sources-with-photometry.csv', dtype={'source_id': str})","5fd0a302":"len(data_temp)","5265ff14":"data_temp.columns","68469575":"should_remove_set = set(pd.read_csv('..\/input\/257k-gaiadr2-should-remove.csv', dtype={'source_id': str})['source_id'])","2fd58a6e":"data_temp = data_temp[~data_temp['source_id'].isin(should_remove_set)]\ndata_temp.reset_index(inplace=True, drop=True)","9b39a4fc":"len(data_temp)","cbf497b5":"assert len(data_temp) == len(set(data_temp['source_id']))","00db2051":"import numpy as np\n\nnp.random.seed(2018080028)\n\ntrain_mask = np.random.rand(len(data_temp)) < 0.9\nwork_data = data_temp[train_mask]\nwork_data.reset_index(inplace=True, drop=True)\ntest_data = data_temp[~train_mask]\ntest_data.reset_index(inplace=True, drop=True)\ndata_temp = None # Get rid of big frame","128f377b":"len(work_data)","5a5177e7":"import inspect\n\npd_concat_argspec = inspect.getfullargspec(pd.concat)\npd_concat_has_sort = 'sort' in pd_concat_argspec.args\n\ndef pd_concat(frames):\n    # Due to Pandas versioning issue\n    new_frame = pd.concat(frames, sort=False) if pd_concat_has_sort else pd.concat(frames)\n    new_frame.reset_index(inplace=True, drop=True)\n    return new_frame\n    \ndef plt_hist(x, bins=30):\n    # plt.hist() can be very slow.\n    histo, edges = np.histogram(x, bins=bins)\n    plt.bar(0.5 * edges[1:] + 0.5 * edges[:-1], histo, width=(edges[-1] - edges[0])\/(len(edges) + 1))","62320b86":"import types\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler \n\ndef get_cv_model_transform(data_frame, label_extractor, var_extractor, trainer_factory, response_column='response', \n                           id_column='source_id', n_runs=2, n_splits=2, max_n_training=None, scale = False,\n                           trim_fraction=None):\n    '''\n    Creates a transform function that results from training a regression model with cross-validation.\n    The transform function takes a frame and adds a response column to it.\n    '''\n    default_model_list = []\n    sum_series = pd.Series([0] * len(data_frame))\n    for r in range(n_runs):\n        shuffled_frame = data_frame.sample(frac=1)\n        shuffled_frame.reset_index(inplace=True, drop=True)\n        response_frame = pd.DataFrame(columns=[id_column, 'response'])\n        kf = KFold(n_splits=n_splits)\n        first_fold = True\n        for train_idx, test_idx in kf.split(shuffled_frame):\n            train_frame = shuffled_frame.iloc[train_idx]\n            if trim_fraction is not None:\n                helper_labels = label_extractor(train_frame) if isinstance(label_extractor, types.FunctionType) else train_frame[label_extractor] \n                train_label_ordering = np.argsort(helper_labels)\n                orig_train_len = len(train_label_ordering)\n                head_tail_len_to_trim = int(round(orig_train_len * trim_fraction * 0.5))\n                assert head_tail_len_to_trim > 0\n                trimmed_ordering = train_label_ordering[head_tail_len_to_trim:-head_tail_len_to_trim]\n                train_frame = train_frame.iloc[trimmed_ordering]\n            if max_n_training is not None:\n                train_frame = train_frame.sample(max_n_training)\n            train_labels = label_extractor(train_frame) if isinstance(label_extractor, types.FunctionType) else train_frame[label_extractor]\n            test_frame = shuffled_frame.iloc[test_idx]\n            train_vars = var_extractor(train_frame)\n            test_vars = var_extractor(test_frame)\n            scaler = None\n            if scale:\n                scaler = StandardScaler()  \n                scaler.fit(train_vars)\n                train_vars = scaler.transform(train_vars)  \n                test_vars = scaler.transform(test_vars) \n            trainer = trainer_factory()\n            fold_model = trainer.fit(train_vars, train_labels)\n            test_responses = fold_model.predict(test_vars)\n            test_id = test_frame[id_column]\n            assert len(test_id) == len(test_responses)\n            fold_frame = pd.DataFrame({id_column: test_id, 'response': test_responses})\n            response_frame = pd_concat([response_frame, fold_frame])\n            if first_fold:\n                first_fold = False\n                default_model_list.append((scaler, fold_model,))\n        response_frame.sort_values(id_column, inplace=True)\n        response_frame.reset_index(inplace=True, drop=True)\n        assert len(response_frame) == len(data_frame), 'len(response_frame)=%d' % len(response_frame)\n        sum_series += response_frame['response']\n    cv_response = sum_series \/ n_runs\n    assert len(cv_response) == len(data_frame)\n    assert len(default_model_list) == n_runs\n    response_map = dict()\n    sorted_id = np.sort(data_frame[id_column].values) \n    for i in range(len(cv_response)):\n        response_map[str(sorted_id[i])] = cv_response[i]\n    response_id_set = set(response_map)\n    \n    def _transform(_frame):\n        _in_trained_set = _frame[id_column].astype(str).isin(response_id_set)\n        _trained_frame = _frame[_in_trained_set].copy()\n        _trained_frame.reset_index(inplace=True, drop=True)\n        if len(_trained_frame) > 0:\n            _trained_id = _trained_frame[id_column]\n            _tn = len(_trained_id)\n            _response = pd.Series([None] * _tn)\n            for i in range(_tn):\n                _response[i] = response_map[str(_trained_id[i])]\n            _trained_frame[response_column] = _response\n        _remain_frame = _frame[~_in_trained_set].copy()\n        _remain_frame.reset_index(inplace=True, drop=True)\n        if len(_remain_frame) > 0:\n            _unscaled_vars = var_extractor(_remain_frame)\n            _response_sum = pd.Series([0] * len(_remain_frame))\n            for _model_tuple in default_model_list:\n                _scaler = _model_tuple[0]\n                _model = _model_tuple[1]\n                _vars = _unscaled_vars if _scaler is None else _scaler.transform(_unscaled_vars)\n                _response = _model.predict(_vars)\n                _response_sum += _response\n            _remain_frame[response_column] = _response_sum \/ len(default_model_list)\n        _frames_list = [_trained_frame, _remain_frame]\n        return pd_concat(_frames_list)\n    return _transform","d0340d56":"import scipy.stats as stats\n\ndef print_evaluation(data_frame, label_column, response_column):\n    response = response_column(data_frame) if isinstance(response_column, types.FunctionType) else data_frame[response_column]\n    label = label_column(data_frame) if isinstance(label_column, types.FunctionType) else data_frame[label_column]\n    residual = label - response\n    rmse = np.sqrt(sum(residual ** 2) \/ len(data_frame))\n    correl = stats.pearsonr(response, label)[0]\n    print('RMSE: %.4f | Correlation: %.4f' % (rmse, correl,), flush=True)","7c9d954d":"def transform_init(data_frame):    \n    new_frame = data_frame.copy()\n    new_frame.reset_index(inplace=True, drop=True)\n    distance = 1000.0 \/ new_frame['parallax']\n    new_frame['distance'] = distance\n    new_frame['abs_mag_ne'] = new_frame['phot_g_mean_mag'] - 5 * (np.log10(distance) - 1)\n    new_frame['color_index'] = new_frame['phot_bp_mean_mag'] - new_frame['phot_rp_mean_mag']\n    return new_frame","dbfc3784":"work_data = transform_init(work_data)","9527fe22":"mag_column_groups = [\n    ['phot_g_mean_mag', 'phot_bp_mean_mag', 'phot_rp_mean_mag'],\n    ['gsc23_v_mag', 'gsc23_b_mag'],\n    ['ppmxl_b1mag', 'ppmxl_b2mag', 'ppmxl_r1mag', 'ppmxl_imag'],\n    ['tmass_j_m', 'tmass_h_m', 'tmass_ks_m'],\n    ['tycho2_bt_mag', 'tycho2_vt_mag'],\n]","f9b5ad65":"def populate_mag_columns(data_frame, feature_list):\n    for group in mag_column_groups:\n        len_group = len(group)\n        assert len_group >= 2\n        for i in range(1, len_group):\n            mag_diff = data_frame[group[i]] - data_frame[group[i - 1]]\n            feature_list.append(mag_diff)","3c5f27c4":"# Hyperparameters for gaussian transformations of distance-from-plane.\nPLANE_DENSITY_2T_VAR1 = 2 * 15 ** 2\nPLANE_DENSITY_2T_VAR2 = 2 * 50 ** 2","600d3fb6":"def extract_model_vars(data_frame):\n    distance = data_frame['distance'].values\n    log_distance = np.log(distance)\n    latitude_rad = np.deg2rad(data_frame['b'].values)\n    longitude_rad = np.deg2rad(data_frame['l'].values)\n    sin_lat = np.sin(latitude_rad)\n    cos_lat = np.cos(latitude_rad)\n    sin_long = np.sin(longitude_rad)\n    cos_long = np.cos(longitude_rad)\n    \n    distance_in_plane = np.abs(distance * cos_lat)\n    distance_from_plane_sq = (distance * sin_lat) ** 2\n    plane_density_feature1 = np.exp(-distance_from_plane_sq \/ PLANE_DENSITY_2T_VAR1)\n    plane_density_feature2 = np.exp(-distance_from_plane_sq \/ PLANE_DENSITY_2T_VAR2)\n    feature_list = [log_distance, distance, \n                    distance_in_plane, \n                    plane_density_feature1, plane_density_feature2,\n                    sin_lat, cos_lat, sin_long, cos_long\n                   ]\n    \n    populate_mag_columns(data_frame, feature_list)\n    mag_g = data_frame['phot_g_mean_mag']\n    mag_rp = data_frame['phot_rp_mean_mag']\n    mag_bp = data_frame['phot_bp_mean_mag']\n    feature_list.append(mag_g - data_frame['allwise_w2'])\n    feature_list.append(mag_bp - data_frame['tmass_j_m'])\n    feature_list.append(mag_bp - data_frame['gsc23_b_mag'])\n    feature_list.append(mag_g - data_frame['ppmxl_r1mag'])\n    feature_list.append(mag_rp - data_frame['ppmxl_imag'])\n    feature_list.append(mag_rp - data_frame['tycho2_bt_mag'])\n    feature_list.append(data_frame['tmass_j_m'] - data_frame['allwise_w2'])\n    feature_list.append(data_frame['gsc23_b_mag'] - data_frame['ppmxl_imag'])\n    \n    return np.transpose(feature_list)    ","b04b2ce7":"LABEL_COLUMN = 'phot_g_mean_mag'","88ab73be":"MAX_N_TRAINING = 50000","2703f735":"from sklearn.neural_network import MLPRegressor\n\ndef get_nn_trainer():\n    return MLPRegressor(hidden_layer_sizes=(60, 60), max_iter=400, alpha=0.1, random_state=np.random.randint(1,10000))","9f662d59":"def get_nn_transform(label_column):\n    return get_cv_model_transform(work_data, label_column, extract_model_vars, get_nn_trainer, \n        n_runs=3, n_splits=2, max_n_training=MAX_N_TRAINING, response_column='nn_' + label_column, scale=True)","710856b6":"transform_nn = get_nn_transform(LABEL_COLUMN)\nwork_data = transform_nn(work_data)","fd1b95ab":"print_evaluation(work_data, LABEL_COLUMN, 'nn_' + LABEL_COLUMN)","6b84cb97":"import lightgbm\n\ndef get_lgbm_trainer():\n    return lightgbm.LGBMRegressor(num_leaves=80, max_depth=-1, learning_rate=0.1, n_estimators=1000, \n        subsample_for_bin=50000, reg_alpha=0.03, reg_lambda=0.0,\n        random_state=np.random.randint(1,10000))","4d44e645":"def get_lgbm_transform(label_column):\n     return get_cv_model_transform(work_data, label_column, extract_model_vars, get_lgbm_trainer, \n        n_runs=2, n_splits=2, max_n_training=MAX_N_TRAINING, response_column='lgbm_' + label_column, scale=False)","bc5b738d":"transform_lgbm = get_lgbm_transform(LABEL_COLUMN)\nwork_data = transform_lgbm(work_data)","542b9cea":"print_evaluation(work_data, LABEL_COLUMN, 'lgbm_' + LABEL_COLUMN)","bdf630e7":"def extract_blend_vars(data_frame):\n    lgbm_responses = data_frame['lgbm_' + LABEL_COLUMN].values\n    nn_responses = data_frame['nn_' + LABEL_COLUMN].values\n    return np.transpose([lgbm_responses, nn_responses])","ce1c3c1f":"from sklearn import linear_model\n\ndef get_blend_trainer():\n    return linear_model.LinearRegression()","ba2102b1":"def get_blend_transform(label_column):\n    return get_cv_model_transform(work_data, label_column, extract_blend_vars, get_blend_trainer, \n        n_runs=3, n_splits=3, max_n_training=None, response_column='blend_' + label_column, scale=False)","8f5b578e":"transform_blend = get_blend_transform(LABEL_COLUMN)\nwork_data = transform_blend(work_data)","0a194d6f":"print_evaluation(work_data, LABEL_COLUMN, 'blend_' + LABEL_COLUMN)","7ac2581c":"def get_bres_label(data_frame):\n    return data_frame[LABEL_COLUMN] - data_frame['blend_' + LABEL_COLUMN]","d74cf2df":"def extract_bres_vars(data_frame):\n    distance = data_frame['distance'].values\n    latitude = np.deg2rad(data_frame['b'].values)\n    longitude = np.deg2rad(data_frame['l'].values)\n    position_z = distance * np.sin(latitude)\n    projection = distance * np.cos(latitude)\n    position_x= projection * np.cos(longitude)\n    position_y = projection * np.sin(longitude)\n    color_index = data_frame['color_index']\n    return np.transpose([position_x, position_y, position_z,\n                        color_index])    ","7589abb8":"from sklearn.ensemble import RandomForestRegressor\n\ndef get_bres_trainer():\n    return RandomForestRegressor(n_estimators=60, max_depth=18, min_samples_split=30, random_state=np.random.randint(1,10000))","d3b0e785":"transform_bres = get_cv_model_transform(work_data, get_bres_label, extract_bres_vars, get_bres_trainer, \n        n_runs=3, n_splits=2, max_n_training=MAX_N_TRAINING, response_column='modeled_bres', scale=False,\n        trim_fraction=0.003)","c930e203":"work_data = transform_bres(work_data)\nprint_evaluation(work_data, get_bres_label, 'modeled_bres')","734f532a":"def transform_final_model(data_frame):\n    new_frame = data_frame.copy()\n    new_frame['model_response'] = new_frame['blend_' + LABEL_COLUMN] + new_frame['modeled_bres']\n    return new_frame","e7015b42":"work_data = transform_final_model(work_data)\nprint_evaluation(work_data, LABEL_COLUMN, 'model_response')","67aa2f74":"def color_index(data_frame):\n    return data_frame['phot_bp_mean_mag'] - data_frame['phot_rp_mean_mag']","88fbd819":"def giant_separation_y(x):\n    return x * 40.0 - 25","7f8b781e":"import matplotlib.pyplot as plt","8d48ca5c":"work_data_sample = work_data.sample(2000)\nplt.rcParams['figure.figsize'] = (10, 5)\n_color_index = color_index(work_data_sample)\nplt.scatter(_color_index, work_data_sample['abs_mag_ne'] ** 2, s=2)\nplt.plot(_color_index, giant_separation_y(_color_index), '--', color='orange')\nplt.gca().invert_yaxis()\nplt.title('Pseudo H-R diagram for giant removal')\nplt.xlabel('BP - RP color index')\nplt.ylabel('Absolute magnitude squared')\nplt.show()","2ff5726f":"def transform_rm_giants(data_frame):\n    new_frame = data_frame[data_frame['abs_mag_ne'] ** 2 >= giant_separation_y(color_index(data_frame))]\n    new_frame.reset_index(inplace=True, drop=True)\n    return new_frame","72507a83":"work_data = transform_rm_giants(work_data)\nlen(work_data)","2cb3166c":"RESPONSE_COLUMN = 'model_response'","a5ef4375":"def transform_residual(data_frame):\n    new_frame = data_frame.copy()\n    new_frame['model_residual'] = data_frame[LABEL_COLUMN] - data_frame[RESPONSE_COLUMN]\n    return new_frame","23eab2dd":"work_data = transform_residual(work_data)","93f3b112":"mean_model_residual = np.mean(work_data['model_residual'].values)\n\ndef get_squared_res_label(data_frame):\n    return (data_frame['model_residual'] - mean_model_residual) ** 2","8266568f":"def extract_residual_vars(data_frame):\n    parallax = data_frame['parallax']\n    parallax_error = data_frame['parallax_error']\n    parallax_high = parallax + parallax_error\n    parallax_low = parallax - parallax_error\n    var_error_diff = np.log(parallax_high) - np.log(parallax_low)\n    \n    flux_error = data_frame['phot_g_mean_flux_error']\n    \n    latitude_rad = np.deg2rad(data_frame['b'].values)\n    longitude_rad = np.deg2rad(data_frame['l'].values)\n    sin_lat = np.sin(latitude_rad)\n    cos_lat = np.cos(latitude_rad)\n    sin_long = np.sin(longitude_rad)\n    cos_long = np.cos(longitude_rad)\n\n    distance = data_frame['distance']\n    distance_in_plane = np.abs(distance * cos_lat)\n    distance_from_plane_sq = (distance * sin_lat) ** 2\n    plane_density_feature1 = np.exp(-distance_from_plane_sq \/ PLANE_DENSITY_2T_VAR1)\n    plane_density_feature2 = np.exp(-distance_from_plane_sq \/ PLANE_DENSITY_2T_VAR2)\n    return np.transpose([\n        distance,\n        sin_lat, cos_lat, sin_long, cos_long,\n        plane_density_feature1, plane_density_feature2,\n        var_error_diff,\n        flux_error\n    ])","dae9346c":"def get_residual_trainer():\n    return RandomForestRegressor(n_estimators=60, max_depth=9, min_samples_split=10, random_state=np.random.randint(1,10000))","16678702":"transform_expected_res_sq = get_cv_model_transform(work_data, get_squared_res_label, extract_residual_vars, get_residual_trainer, \n        n_runs=4, n_splits=2, max_n_training=MAX_N_TRAINING, response_column='expected_res_sq', scale=False,\n        trim_fraction=0.003)","671e496f":"work_data = transform_expected_res_sq(work_data)","6193eab0":"print_evaluation(work_data, get_squared_res_label, 'expected_res_sq')","536d975e":"def transform_anomaly(data_frame):\n    new_frame = data_frame.copy()\n    new_frame_residual = new_frame['model_residual'].values\n    new_frame['anomaly'] = (new_frame_residual - mean_model_residual) \/ np.sqrt(new_frame['expected_res_sq'].astype(float))\n    return new_frame","dd88f3cf":"work_data = transform_anomaly(work_data)","54493b14":"np.std(work_data['anomaly'])","866df667":"transform_list = [transform_init,                          # extra info columns\n                  transform_lgbm, transform_nn,            # individual models\n                  transform_blend,                         # the blend\n                  transform_bres, transform_final_model,   # position-based residual correction\n                  transform_rm_giants,                     # removal of giants\n                  transform_residual,                      # add the residual column\n                  transform_expected_res_sq,               # regional residual variance\n                  transform_anomaly                        # anomaly metric\n                 ]","84b812f3":"def combined_transform(data_frame):\n    _frame = data_frame\n    for t in transform_list:\n        _frame = t(_frame)\n    return _frame","99cedf8f":"test_data = combined_transform(test_data)","abd7bd3d":"np.std(test_data['model_residual'])","ee1e55e6":"np.std(work_data['model_residual'])","99b1631a":"data = pd_concat([work_data, test_data])\nwork_data = None\ntest_data = None","4e3c8a50":"len(data)","5bf7b2b9":"data[data['source_id'] == '2081900940499099136'][\n    ['source_id', 'distance', 'abs_mag_ne', 'model_residual', 'anomaly']]","73fbfe46":"CAND_SD_THRESHOLD = 3.0","29e09099":"data_anomalies = data['anomaly']","de29f6b6":"anomaly_std = np.std(data_anomalies)","6de88197":"cand_threshold = anomaly_std * CAND_SD_THRESHOLD\ncandidates = data[data_anomalies >= cand_threshold]\nlen(candidates)","c13983c8":"bright_control_group = data.sort_values('anomaly', ascending=True).head(len(candidates))","a8487a3d":"normal_control_group = data[(data_anomalies < anomaly_std) & (data_anomalies > -anomaly_std)].sample(len(candidates))","e4129a79":"data_anomalies = None # Discard big array","29a82f4f":"def get_position_frame(data_frame):\n    new_frame = pd.DataFrame(columns=['source_id', 'x', 'y', 'z'])\n    new_frame['source_id'] = data_frame['source_id'].values\n    distance = data_frame['distance'].values\n    latitude = np.deg2rad(data_frame['b'].values)\n    longitude = np.deg2rad(data_frame['l'].values)\n    new_frame['z'] = distance * np.sin(latitude)\n    projection = distance * np.cos(latitude)\n    new_frame['x'] = projection * np.cos(longitude)\n    new_frame['y'] = projection * np.sin(longitude)\n    return new_frame","942a547d":"def get_sun():\n    new_frame = pd.DataFrame(columns=['source_id', 'x', 'y', 'z'])\n    new_frame.loc[0] = ['sun', 0.0, 0.0, 0.0]\n    return new_frame","9a5c928a":"candidates_wbstar = pd_concat([candidates, data[data['source_id'] == '2081900940499099136']])\ncandidates_pos_frame = pd_concat([get_position_frame(candidates_wbstar), get_sun()])","de1ad8dc":"import plotly.plotly as py\nimport plotly.offline as py\nimport plotly.graph_objs as go\n\npy.init_notebook_mode(connected=False)","c475e854":"def plot_pos_frame(pos_frame, star_color, sun_color = 'blue', bstar_color = 'black'):    \n    star_color = [(bstar_color if row['source_id'] == '2081900940499099136' else (sun_color if row['source_id'] == 'sun' else star_color)) for _, row in pos_frame.iterrows()]\n    trace1 = go.Scatter3d(\n        x=pos_frame['x'],\n        y=pos_frame['y'],\n        z=pos_frame['z'],\n        mode='markers',\n        text=pos_frame['source_id'],\n        marker=dict(\n            size=3,\n            color=star_color,\n            opacity=0.67\n        )\n    )\n    scatter_data = [trace1]\n    layout = go.Layout(\n        margin=dict(\n            l=0,\n            r=0,\n            b=0,\n            t=0\n        )\n    )\n    fig = go.Figure(data=scatter_data, layout=layout)\n    py.iplot(fig)","80613f22":"%%html\n<!-- Allow bigger output cells -->\n<style>\n.output_wrapper, .output {\n    height:auto !important;\n    max-height: 1500px;\n}\n<\/style>","b40728b1":"normal_control_group_wbstar = pd_concat([normal_control_group, data[data['source_id'] == '2081900940499099136']])\nnormal_control_group_pos_frame = pd_concat([get_position_frame(normal_control_group_wbstar), get_sun()])\nplot_pos_frame(normal_control_group_pos_frame, 'gray')","e7084d6d":"plot_pos_frame(candidates_pos_frame, 'green')","bf7467f9":"bright_control_group_wbstar = pd_concat([bright_control_group, data[data['source_id'] == '2081900940499099136']])\nbright_control_group_pos_frame = pd_concat([get_position_frame(bright_control_group_wbstar), get_sun()])\nplot_pos_frame(bright_control_group_pos_frame, 'red')","cc842bf1":"SAVED_COLUMNS = ['source_id', 'tycho2_id', 'ra', 'dec', 'pmra', 'pmdec', 'l', 'b', 'distance', 'color_index',\n                 LABEL_COLUMN, 'blend_' + LABEL_COLUMN, 'model_residual', 'anomaly']","438b8f8b":"data[SAVED_COLUMNS].to_csv('mag-modeling-results.csv', index=False)","d52be530":"## LightGBM model\nWe're usng [LightGBM](https:\/\/github.com\/Microsoft\/LightGBM) this time. It's fast and accurate.","7cd22e04":"## Output\nModel results and other dataset columns are made available in the output tab of this kernel. The *anomaly* column is the key result. Its standard deviation is roughly 1.0. Positive *anomaly* values indicate a star is dimmer than expected. ","93e98605":"## Anomaly metric\nThe *anomaly* metric is defined as the locally standardized model residual.","dbe33621":"Regression training is done over a sample of available data. At most we use these many records in each pass:","18340c4d":"These results are not that bad, if you consider we're modeling noise. We're basically using a random forest to estimate the regional variance (i.e. mean squared error) of magnitude residuals.","a794434f":"## Squared residual modeling\nWe corrected for position-based bias, but error also seems to depend on position in the sky.","8247e1d7":"The following function gets the within-group differences.","46ce903b":"The following function is similar to the one we used to train the original model, except this one averages out multiple runs of k-fold cross-validation. It also has a optional *trim_fraction* parameter that allows outliers of training subsets to be removed. The concept is similar to that of a trimmed average. We will use it in residual and squared error modeling.","08f01b2b":"The separation function we will use to remove giants is visually derived.","a3893588":"## Data\n\nData was obtained from the [Gaia Archive](https:\/\/gea.esac.esa.int\/archive\/), using its ADQL query tool, and made available in a [Kaggle dataset](https:\/\/www.kaggle.com\/solorzano\/257k-gaia-dr2-stars). In addition to Gaia DR2 parallax and photometry, the dataset includes magnitude observations from GSC 2.3, PPMXL, 2MASS, AllWISE, and Tycho2.","a7cec67f":"## Blend\nThe blend only has two models this time.","e093e5c6":"## Giant removal\nIn prior kernels, we found that the model struggles with giant stars. We do want the model to learn about the spectrophotometric characteristics of giant stars, but we'll remove them from further consideration at this stage of the analysis.","73d10108":"## Neural Network","fcf28725":"Next, we define a function that extracts the features used to train regression models. Note that in addition to calling *populate_mag_columns*, we've added some differences between Gaia magnitudes and specific magnitudes from other databases. We've found these additional differences improve model performance.\n\nA star's position in the sky is also a feature. We could presumably lose some information by including position, but it greatly improves model performance.\n\nIn addition to distance, we're including distance in the galactic plane and a couple gaussian transformations of the distance *from* the galactic plane, which seem to help the model resolve variations in extinction due to proximity to galactic latitude 0.","b7082c53":"The 'test' data we set aside has the following approximate RMSE:","589f3559":"## Train\/Test split","2384ada9":"What we're looking for here is that there a no beams of candidates aligned with Earth, which would indicate there are possible line-of-sight artifacts. Those kinds of artifacts seem to be largely removed.\n\nThe anomalously bright star control group is shown in red below. Since giants have been removed, these brighter-than-expected stars are largely in the main sequence.","985461ae":"The regression label will be defined by the following variable.","a447a186":"## Adjustment for position-based distortions\nThe base model already included position features, as they greatly improve model performance. It's usually possible to squeeze a bit more performance by modeling residuals.\n\nMore importantly, there's an issue that can be corrected here. Because the model includes position features, a extreme outlier can affect its neighbors. A concrete example is Gaia DR2 1596779097312755328 \u2014 an extremely bright outlier. Its neighbors will be unusually *dim* according to the model. So we'll take advantage of the *trim_fraction* parameter of the cross-validation modeling transform to ignore outliers when training this position-based correction.","0a009385":"We'll include our sun in visualizations. Look for the blue dot.","1c6c1488":"The *model_response* column will contain the response of the blend plus the response of the residual model we just trained. ","04e22a65":"Note that we're including *color_index* as a feature here, in case there are color-based dependencies to the distortions. It seems to help slightly.","6afce893":"## Helper functions used in modeling\nSome boilerplate:","91a3f3bc":"A random forest works well for this, because of the nature of the feature space.","b1347563":"## Extra columns\nWe'll add some columns for convenience and informational purposes.","5e506b7a":"## Anomalous and control group selection\nFor visualization purposes, let's get a list of dim \"outliers\" at a 3-sigma cut-off. We'll also get some bright and ordinary controls.","94c09d63":"A sample of ordinary stars is shown in gray.","d627cf8d":"## Acknowledgments\n\nThis work has made use of data from the European Space Agency (ESA) mission Gaia (https:\/\/www.cosmos.esa.int\/gaia), processed by the Gaia Data Processing and Analysis Consortium (DPAC, https:\/\/www.cosmos.esa.int\/web\/gaia\/dpac\/consortium). Funding for the DPAC has been provided by national institutions, in particular the institutions participating in the Gaia Multilateral Agreement.","79cb5cfe":"## Model features\nPrimarily, we want to use differences between magnitude observations made with different photometric filters. We've found it's better to separate them into groups first.","771baeb5":"This is a little surprising. It's unusual to find that a neural network does better than a GBM with a relatively small number of features.","18ab041b":"<a id='space_distribution_section'><\/a>\n## Space distribution of star groups\nThe following function calculates rectangular (x-y-z) coordinates for each star. *X* points to the galactic center and *Z* is perpendicular to the galactic plane.","3cdfd77c":"By concatenating *work_data* and *test_data* we end up with results for the whole dataset (minus giants).","20d6aa88":"## Note about AllWISE artifact\nIn an earlier version of the kernel we had removed the region around an apparent line-of-sight artifact. Anomalously dim stars would cluster near a plane given by:\n\n0.64X + 0.64Y - Z = 0\n\nUpon further analysis, we narrowed the issue down to the *allwise_w1* magnitude. Magnitudes *allwise_w3* and *allwise_w4* also seem to have their own systematics, based on cross-database analysis. We're only using *allwise_w2* at this point.","587cbb3c":"## Final transformation and validation","a4afb0d5":"## KIC 8462852\nKIC 8462852 is an enigmatic star (Boyajian et al. 2015) that happens to be in the dataset. Its model results follow.","3957f221":"Note that in addition to position features, we're including a *parallax_error*-derived feature. It seems reasonable to do that, even though it's not clear that it helps a whole lot.\n\nThe distance-from-plane transformations improve meta-model performance considerably.","969af9f7":"We'll use 90% of the data (*work_data*) for cross-validation and hype-parameter optimization. The remaining 10% (*test_data*) will be set aside for final confirmation and validation.","e981cb14":"This is lower than we saw previously, but that's because we removed giants. It actually is consistent with what we see in the *work_data* frame now:","6c59160d":"Anomalously dim stars are shown in green.","7a3cba55":"## Removal of duplicates and region with artifact\nThere are rows with duplicate *source_id* values (multiple Tycho2 matches) and some apparent line-of-sight artifacts, as explained in [*Removal of Gaia DR2 Stars With Apparent Systematic*](https:\/\/www.kaggle.com\/solorzano\/removal-of-gaia-dr2-stars-with-apparent-systematic). We'll just remove all of the rows that are potentially problematic.","1dc6de8f":"Note that, once again, we're using the *trim_fraction* parameter so that extreme outliers don't distort neighbor results.","60d9940d":"It should be noted that excluding position features from the base model and applying a correction on the residuals wouldn't work this well. This means there's likely a complex interaction between position and spectrophotometric features.","8f194790":"## References\n\nBoyajian, et al. (2015). _Planet Hunters X. KIC 8462852 - Where's the Flux?_ arXiv:1509.03622\n\nBradbury et al. (2011). _Dysonian Approach to SETI: A Fruitful Middle Ground?_ Journal of the British Interplanetary Society, vol. 64, p. 156-165\n\nZackrisson et al. (2018). _SETI with Gaia: The observational signatures of nearly complete Dyson spheres_. arXiv:1804.08351 ","7a8d7b68":"It's an ordinary star according to the model.","e5c60eb5":"We're also adding KIC 8462852 to the 3D scatter charts. Look for the black dot.","8f7e5278":"## Overview\nA model of stellar magnitude was presented in *[Dysonian SETI With Machine Learning](https:\/\/www.kaggle.com\/solorzano\/dysonian-seti-with-machine-learning)*. Some changes and improvements are introduced in this kernel:\n\n* We're using a [dataset of 257K Gaia DR2 stars](https:\/\/www.kaggle.com\/solorzano\/257k-gaia-dr2-stars) from the Northern and Southern hemispheres, with photometry from a number of other databases.\n* Some systematics\/artifacts are removed from the data.\n* Multiple runs of k-fold cross-validation are averaged out.\n* A position-based magnitude bias correction is applied to the residuals of the blend.\n* Model RMSE is ~0.115 magnitudes.\n* Giants are removed post-training, and RMSE is subsequently ~0.10. It's a sensitive model.\n\nSome outliers and a sample of ordinary stars are shown in <a href=\"#space_distribution_section\">3D scatter charts<\/a>. Model results are made available in the output tab. The *anomaly* metric is the key result. It has a standard deviation of ~1.0.\n\n**Follow-ups:**\n* [Multi-Stellar SETI Candidate Selection, Part 1](https:\/\/www.kaggle.com\/solorzano\/multi-stellar-seti-candidate-selection-part-1)\n* [Multi-Stellar SETI Candidate Selection, Part 2](https:\/\/www.kaggle.com\/solorzano\/multi-stellar-seti-candidate-selection-part-2)\n* [Multi-Stellar SETI Candidate Selection, Part 3](https:\/\/www.kaggle.com\/solorzano\/multi-stellar-seti-candidate-selection-part-3)\n\n**Updates:**\n* 9\/1\/2018: Removed region with apparent Gaia magnitude artifacts. Additionally, AllWISE magnitudes other than *allwise_w2* are no longer used because of line-of-sight artifacts.","be64bd1f":"Columns found in the data frame are:","5ae4d588":"The standard deviation of *anomaly* should be ~1.0, absent extreme outliers."}}