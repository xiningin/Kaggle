{"cell_type":{"a4a7f587":"code","34e8bcec":"code","cb659b96":"code","79064dc2":"code","667a1f03":"code","f64f63cd":"code","08d77519":"code","0a391b5b":"code","6ff0ff5b":"code","f6d83b28":"code","a9ad098b":"code","208c5aab":"code","df228789":"code","5362e825":"code","3a3241f5":"code","54bc5b99":"code","c844ed08":"code","fc583660":"code","86fbb2b6":"code","b319cf43":"code","7e3437c3":"markdown","5b0b10d2":"markdown","5c4dc7ac":"markdown"},"source":{"a4a7f587":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split,cross_val_score\nfrom sklearn.metrics import confusion_matrix,classification_report, roc_auc_score,matthews_corrcoef,precision_score, recall_score, f1_score, accuracy_score\n\n\nimport xgboost as xgb\n\nimport warnings\nwarnings.filterwarnings('ignore')","34e8bcec":"df = pd.read_csv('..\/input\/heart-attack-analysis-prediction-dataset\/heart.csv')\ndf = df.drop_duplicates().reset_index(drop=True)","cb659b96":"df.head()","79064dc2":"df.isna().sum()","667a1f03":"df.output.value_counts()","f64f63cd":"X,y = df.iloc[:,:-1],df['output']\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,stratify=y,random_state=42)","08d77519":"xgb_classifier = xgb.XGBClassifier(eval_metric='auc')","0a391b5b":"xgb_classifier.fit(X_train,y_train)","6ff0ff5b":" prediction = xgb_classifier.predict(X_test)","f6d83b28":"print(classification_report(prediction,y_test))","a9ad098b":"print(\"Precision:{}\".format(precision_score(prediction,y_test)))\nprint(\"Recall:{}\".format(recall_score(prediction,y_test)))\nprint(\"F1 Score:{}\".format((f1_score(prediction,y_test))))","208c5aab":"from bayes_opt import BayesianOptimization","df228789":"def bo_params_xgb(max_depth, gamma,learning_rate,n_estimators,subsample):\n    \n    params = {\n        'max_depth': int(max_depth),\n        'gamma': gamma,\n        'learning_rate':learning_rate,\n        'subsample': subsample,\n        'eval_metric': 'auc',\n        'n_estimators':int(n_estimators)\n    }\n    \n    scores = cross_val_score(xgb.XGBClassifier(random_state=123, **params,use_label_encoder=False),\n                             X_train, y_train,cv=5,scoring=\"f1\").mean()\n    return scores.mean()","5362e825":"xgb_bo = BayesianOptimization(bo_params_xgb, {'max_depth': (3, 10),\n                                             'gamma': (0, 1),\n                                             'learning_rate':(0,1),\n                                              'subsample':(0.5,1),\n                                              'n_estimators':(100,200)\n                                             })","3a3241f5":"results = xgb_bo.maximize(n_iter=200, init_points=20)","54bc5b99":"params = xgb_bo.max['params']\nparams['max_depth']= int(params['max_depth'])\nparams['n_estimators']= int(params['n_estimators'])\nparams['eval_metric'] = 'auc'\nprint(params)","c844ed08":"xgb_classifier2 = xgb.XGBClassifier(**params,use_label_encoder=False)\nxgb_classifier2.fit(X_train,y_train)","fc583660":" prediction = xgb_classifier2.predict(X_test)","86fbb2b6":"print(classification_report(prediction,y_test))","b319cf43":"print(\"Precision:{}\".format(precision_score(prediction,y_test)))\nprint(\"Recall:{}\".format(recall_score(prediction,y_test)))\nprint(\"F1 Score:{}\".format((f1_score(prediction,y_test))))","7e3437c3":"# Hyperparameter tuning\n\n\nReference: https:\/\/proceedings.neurips.cc\/paper\/2012\/file\/05311655a15b75fab86956663e1819cd-Paper.pdf\n\nMachine learning models have parameters and hyperparameters. Parameters are learnt from the data and hyperparameters are pre-defined for a model. For example, in linear\/logistic regression, the coefficients and biases are the parameters that is being optimised while training, whereas learning rate is one hyperparameter that is set to a constant before training the model. The process of finding the best hyperparameters from a range of values is called hyperparameter tuning.\n\n## Bayesian optimization for hyperparameter tuning\n\n\nBayesian optimization (BO) is an automated procedure for hyperparameter tuning. BO uses gaussian process to model mean and variance of the target function.\n\n### Gaussian process\n\nGuassian process is a prior distribution on functions, and in the case of hyperparameter tuning it will be of the form:\n\n$\n\\begin{align}\nf:\\chi \\rightarrow {\\mathbb{R}} \n\\end{align}\n$ <br>\n<br>\nWhere $\\chi$ is the space of hyperparameter set we are tuning, and $f$ is the target function decided based on the criteria on which the hyperparameters needs to be tuned. For a gaussian process, $p(f|\\chi)$ follows a normal distribution. In our problem, we define the mean of weighted F1 score from five fold cross validation as the target function. \n\n### Acquisition function\n\nIt is assumed that the function $f(x)$ is drawn from a gaussain process prior and our observation is of the form ${x_i,y_i}$, where <br>\n\n$ y_i \\thicksim N(f(x_i), \u03bd) $ , $\u03bd$ is the variance of noise introduced into the function observations.\n\nThis prior and the data induce a posterior over functions,the acquisition function, $a:\\chi \\rightarrow {\\mathbb{R^{+}}} \n$, determines what points in $\\chi$ to be evaluated next.\n\nThere are different choices for acquisition functions, mainly <br>\n\n1) Upper Confidence Bounds method <br>\n2) Expected improvement method <br>\n3) Probability Of Improvement method <br>\n\n\nAcquisiton function follows a greedy approach to find the optimum hyperparameters in each step, and with a fixed step of iteration, we are  expected to get the suitable hyperparameters for our model.\n\n\n\n\nFor understanding how bayesian optimization works,we will finetune for 'max_depth','gamma','n_estimators','n_estimators' and 'subsample' hyperparameters.","5b0b10d2":"# Problem Statement\n\nPredict the possibility of heart attack, with the given personal and health related informations of a person.","5c4dc7ac":"There is a significant improvement in the precision and F1 score."}}