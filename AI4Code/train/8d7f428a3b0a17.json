{"cell_type":{"8055e2aa":"code","c1f85ec5":"code","c0f65bd1":"code","bbb73f7c":"code","65d94658":"code","f111af6e":"code","439a6ded":"code","4cb53692":"code","7f8251c0":"code","06fd1c3f":"code","c696d823":"code","7eb707de":"code","233d09ad":"code","1e05e39f":"code","8d535d73":"code","fa8c8930":"code","5e34cdcf":"code","5963f252":"code","56ae1b5e":"markdown","10776e41":"markdown","7be887df":"markdown","dabbbfa6":"markdown","d3ac750e":"markdown","18edb222":"markdown","83ac7862":"markdown","fd933003":"markdown"},"source":{"8055e2aa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c1f85ec5":"CalendarDF=pd.read_csv(\"\/kaggle\/input\/m5-forecasting-accuracy\/calendar.csv\", header=0)\nSalesDF=pd.read_csv(\"\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_evaluation.csv\", header=0) #June 1st Dataset","c0f65bd1":"import os, psutil\n\npid = os.getpid()\npy = psutil.Process(pid)\nmemory_use = py.memory_info()[0] \/ 2. ** 30\nprint ('memory GB:' + str(np.round(memory_use, 2)))","bbb73f7c":"CalendarDF['date'] = pd.to_datetime(CalendarDF.date)\n\nTX_1_Sales = SalesDF[['TX_1' in x for x in SalesDF['store_id'].values]]\nTX_1_Sales = TX_1_Sales.reset_index(drop = True)\nTX_1_Sales.info()","65d94658":"# Generate MultiIndex for easier aggregration.\nTX_1_Indexed = pd.DataFrame(TX_1_Sales.groupby(by = ['cat_id','dept_id','item_id']).sum())\nTX_1_Indexed.info()","f111af6e":"# Aggregate total sales per day for each sales category\nFood = pd.DataFrame(TX_1_Indexed.xs('FOODS').sum(axis = 0))\nHobbies = pd.DataFrame(TX_1_Indexed.xs('HOBBIES').sum(axis = 0))\nHousehold = pd.DataFrame(TX_1_Indexed.xs('HOUSEHOLD').sum(axis = 0))\nFood.info()","439a6ded":"# Merge the aggregated sales data to the calendar dataframe based on date\nCalendarDF = CalendarDF.merge(Food, how = 'left', left_on = 'd', right_on = Food.index)\nCalendarDF = CalendarDF.rename(columns = {0:'Food'})\nCalendarDF = CalendarDF.merge(Hobbies, how = 'left', left_on = 'd', right_on = Hobbies.index)\nCalendarDF = CalendarDF.rename(columns = {0:'Hobbies'})\nCalendarDF = CalendarDF.merge(Household, how = 'left', left_on = 'd', right_on = Household.index)\nCalendarDF = CalendarDF.rename(columns = {0:'Household'})\nCalendarDF.head(10)","4cb53692":"# Drop dates with null sales data\nCalendarDF = CalendarDF.drop(CalendarDF.index[1941:])\nCalendarDF.reset_index(drop = True)","7f8251c0":"# Collect sales data from each category into one dataframe\ncategoriesDF = CalendarDF[['Food','Hobbies','Household']]\ncategoriesDF.corr(method = 'pearson')\ncategoriesDF.corr(method = 'spearman')\ncategoriesDF.corr(method = 'kendall')","06fd1c3f":"from statsmodels.tsa.seasonal import seasonal_decompose\n\nFood.index = CalendarDF['date']\n\n# Split food sales data into train and test \nfoodTrain = Food['20110129':'20160410']\nfoodTest = Food['20160411':'20160522']\n\n# Drop 0 sales values to prepare data for multiplicative seasonal decomposition\nfoodTrain = foodTrain[foodTrain[foodTrain.columns[0]] !=0]\n\n# Seasonal decomposition\nresult = seasonal_decompose(foodTrain, model = 'multiplicative', extrapolate_trend = 'freq', freq = 7) # frequency set to weekly\n\n# Store seasonality component of decomposition\nseasonal = result.seasonal.to_frame()\nseasonal_index = result.seasonal[-7:].to_frame()\n\n# Merge the train data and the seasonality \nfoodTrain = foodTrain.merge(seasonal, how = 'left', on = foodTrain.index , left_index = True, right_index = True)","c696d823":"# Building the SARIMAX model\n# I use the Pyramid Arima package to perform an auto-SARIMAX forecast\n\n!pip install pmdarima\nimport pmdarima as pm\n\n#SARIMAX Model setting the exogenous variable to weekly seasonality \nsxmodel = pm.auto_arima(foodTrain[foodTrain.columns[0]], exogenous= foodTrain[['seasonal']],\n                           start_p=1, start_q=1,\n                           test='adf',\n                           max_p=3, max_q=3, m=7,\n                           start_P=0, seasonal=True,\n                           d=None, D=1, trace=True,\n                           error_action='ignore',  \n                           suppress_warnings=True, \n                           stepwise=True)\n\nsxmodel.summary()","7eb707de":"# Forecasting using the SARIMAX model\nimport matplotlib.pyplot as plt\n\nn_periods = 42\nfitted, confint = sxmodel.predict(n_periods = n_periods,  exogenous= np.tile(seasonal_index['seasonal'], 6).reshape(-1,1),  return_conf_int = True)\n\nindex_of_fc = pd.date_range(foodTest.index[0], periods = n_periods, freq = 'D')\n\n# make series for plotting purpose\nfitted_series = pd.Series(fitted, index=index_of_fc)\nlower_series = pd.Series(confint[:, 0], index=index_of_fc)\nupper_series = pd.Series(confint[:, 1], index=index_of_fc)\n\n# Plot\nplt.plot(foodTest)\nplt.plot(fitted_series, color='darkgreen')\nplt.fill_between(lower_series.index, \n                 lower_series, \n                 upper_series, \n                 color='k', alpha=.15)\n\nplt.title(\"SARIMA - Total Sales of TX_1\")\nplt.show()","233d09ad":"# data engineering for event_name_1\nCalendarDF['isweekday'] = [1 if wday >= 3 else 0 for wday in CalendarDF.wday.values]\nCalendarDF['isweekend'] = [0 if wday > 2 else 1 for wday in CalendarDF.wday.values]\nCalendarDF['holiday_weekend'] = [1 if (we == 1 and h not in [np.nan]) else 0 for we,h in CalendarDF[['isweekend','event_name_1']].values]\nCalendarDF['holiday_weekday'] = [1 if (wd == 1 and h not in [np.nan]) else 0 for wd,h in CalendarDF[['isweekday','event_name_1']].values]\n\n# one-hot-encoding event_name_1\nCalendarDF = pd.get_dummies(CalendarDF, columns=['event_name_1'], prefix=['holiday'], dummy_na=True)\n\nFood = CalendarDF['Food']\nFood.index = CalendarDF['date']\n\n# Section out the columns created by encoding and concat with Food dataframe\ntemp = CalendarDF.iloc[:,16:50]\ntemp.index = CalendarDF['date']\nFood = pd.concat([Food, temp], axis = 1)\n\nfoodTrain = Food['20110129':'20160410']\nfoodTest = Food['20160411':'20160522']","1e05e39f":"# Build the SARIMAX model\nsxmodel_event = pm.auto_arima(foodTrain[foodTrain.columns[0]], exogenous= foodTrain.iloc[:,1:],\n                           start_p=1, start_q=1,\n                           test='adf',\n                           max_p=3, max_q=3, m=7,\n                           start_P=0, seasonal=True,\n                           d=None, D=1, trace=True,\n                           error_action='ignore',  \n                           suppress_warnings=True, \n                           stepwise=True)\n\nsxmodel_event.summary()","8d535d73":"# Forecast\nn_periods = 42\nevent_predict, confint = sxmodel_event.predict(n_periods = n_periods,  exogenous= foodTest.iloc[:,1:],  return_conf_int = True)\n\nindex_of_fc = pd.date_range(foodTest.index[0], periods = n_periods, freq = 'D')\n\n# make series for plotting purpose\nfitted_series = pd.Series(event_predict, index=index_of_fc)\nlower_series = pd.Series(confint[:, 0], index=index_of_fc)\nupper_series = pd.Series(confint[:, 1], index=index_of_fc)\n\n# Plot\n#plt.plot(foodTrain)\nplt.plot(foodTest)\nplt.plot(fitted_series, color='darkgreen')\nplt.fill_between(lower_series.index, \n                 lower_series, \n                 upper_series, \n                 color='k', alpha=.15)\n\nplt.title(\"SARIMA - Total Sales of TX_1\")\nplt.show()","fa8c8930":"#Accuracy metrics\ndef symmetric_mean_absolute_percentage_error(actual,forecast):\n    return 1\/len(actual) * np.sum(2 * np.abs(forecast-actual)\/(np.abs(actual)+np.abs(forecast)))\n\ndef mean_absolute_error(actual, forecast):\n    return np.mean(np.abs(actual - forecast))\n\ndef naive_forecasting(actual, seasonality):\n    return actual[:-seasonality]\n\ndef mean_absolute_scaled_error(actual, forecast, seasonality):\n    return mean_absolute_error(actual, forecast) \/ mean_absolute_error(actual[seasonality:], naive_forecasting(actual, seasonality))","5e34cdcf":"symmetric_mean_absolute_percentage_error(foodTest[foodTest.columns[0]], fitted) #sMAPE of SARIMAX with forced seasonality","5963f252":"symmetric_mean_absolute_percentage_error(foodTest[foodTest.columns[0]], event_predict) #sMAPE of SARIMAX with event_name_1","56ae1b5e":"# Load in relevant datasets","10776e41":"# M5 Forecasting Accuracy Research\n\nThis is a continuation of my work on analyzing the sales data of Walmart's TX_1 store (Version 1 found here:https:\/\/www.kaggle.com\/jimmyliuu\/m5-forecast-accuracy-research-version-1). This week, I add a couple exogenous variables into my SARIMA model in hopes of improving forecast accuracy. I chose to force weekly seasonality and event_name_1 into the model. \n\nI followed Jason Brownlee's \"How to Decompose Time Series Data into Trend and Seasonality\" to build the SARIMAX model with forced weekly seasonality (found here:https:\/\/machinelearningmastery.com\/decompose-time-series-data-trend-seasonality\/)\n\nI followed Oscar Arzamendia's \"Time Series Forecasting - A Getting Started Guide\" as a guide to feature engineering. I used one-hot-encoding on event_name_1 to feed it into the SARIMAX model (found here:https:\/\/towardsdatascience.com\/time-series-forecasting-a-getting-started-guide-c435f9fa2216#:~:text=An%20exogenous%20variable%20is%20one,without%20being%20affected%20by%20it.)","7be887df":"Here, I perform a couple of correlation tests between each of the sales categories (food, hobbies, and household).","dabbbfa6":"# Preparing the dataset","d3ac750e":"# Building the SARIMAX model using event_name_1 as an exogenous variable","18edb222":"# Building the SARIMAX model with forced weekly seasonality\n\nReference: https:\/\/machinelearningmastery.com\/decompose-time-series-data-trend-seasonality\/ Section 15","83ac7862":"Both SARIMAX models are slightly more accurate than the ARIMA model based on sMAPE.","fd933003":"# Comparing Results\n\nI will now compare the forecasting results from the ARIMA model, the SARIMA model, and the two SARIMAX models using the sMAPE and MASE functions. (reference: https:\/\/gist.github.com\/bshishov\/5dc237f59f019b26145648e2124ca1c9)"}}