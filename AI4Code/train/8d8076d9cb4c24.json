{"cell_type":{"2640e3f9":"code","ebd4ad96":"code","a85579c1":"code","8735b6d3":"code","e4018f8f":"code","84a63df6":"code","03b7e94a":"code","b2f84728":"code","c68e127a":"code","61a116e4":"code","2c9c8c72":"code","2fbc9d3e":"code","bcb47272":"code","f82f31c9":"code","d1458d89":"code","4170de35":"code","4dad2d9d":"code","73fc680d":"code","a85599e3":"code","4244693c":"code","16b4d7fa":"code","9c1c95ac":"code","ce4840b8":"code","330136ab":"code","c640d099":"code","ecc9fca3":"code","86590307":"code","5e09f926":"code","4cbca5a1":"code","7bb89dd1":"code","50cd57da":"code","39167603":"code","c076ae35":"code","8fb4afd3":"code","88d867c5":"code","c5514433":"code","b3539455":"code","354ddfdc":"code","9a1f52b2":"code","9188241b":"markdown","a51372ff":"markdown","d3982b05":"markdown","3c1cf05d":"markdown","4368bf21":"markdown","44378384":"markdown","75402068":"markdown","50a714b5":"markdown","f5ce4001":"markdown","efc109ed":"markdown","b7327278":"markdown","22e7aeb6":"markdown","2b83270e":"markdown","bfb20e90":"markdown","f7ff8374":"markdown","2773eadf":"markdown","c5ee2101":"markdown","e36219dd":"markdown","209a39be":"markdown"},"source":{"2640e3f9":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\n\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OrdinalEncoder\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\n\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, plot_confusion_matrix, roc_curve, roc_auc_score, auc\n\n%matplotlib inline","ebd4ad96":"DATA_PATH = \"..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv\"","a85579c1":"data = pd.read_csv(DATA_PATH)\ndata.head()","8735b6d3":"data.shape","e4018f8f":"print(f\"Number of uniqe id's in data: {len(data.id.unique())}\")","84a63df6":"# Dropping the id column\ndata.drop(columns = [\"id\"], inplace = True)\ndata.head()","03b7e94a":"all_columns = list(data.columns)\ncategorical_data_cols  = [column for column in all_columns if len(data[column].unique())<=5]\ncontinuous_data_cols  = [column for column in all_columns if column not in categorical_data_cols]\nprint(f\"Continuos Data Columns: {', '.join(continuous_data_cols)}\")\nprint(f\"Categorical Data Columns: {', '.join(categorical_data_cols)}\")","b2f84728":"stroke_val_counts = data[\"stroke\"].value_counts()\n\nprint(f\"Non Stroke: {stroke_val_counts[0] \/ sum(stroke_val_counts)}%\")\nprint(f\"Stroke: {stroke_val_counts[1] \/ sum(stroke_val_counts)}%\")\n\ndata[\"stroke\"].value_counts().plot(kind = \"bar\")\nplt.show()","c68e127a":"for column in categorical_data_cols[:-1]:\n    print(f\"Number of NaN values in {column}: {data[column].isnull().sum()}\")","61a116e4":"plt.figure(figsize = (17,19))\ni = 1\nfor column in categorical_data_cols[:-1]:\n    plt.subplot(4, 2, i)\n    sns.countplot(x = data[column], hue = data[\"stroke\"])\n    i+=1\nplt.show()","2c9c8c72":"for column in categorical_data_cols[:-1]:\n    plt.figure(figsize = (9,5))\n    type_count = data.groupby(column)[\"stroke\"].sum()\n    x = type_count.index\n    y = type_count.values\n    plt.barh(x, y)\n\n    for index, value in enumerate(y):\n        plt.text(value, index,\n                 value)\n\n    plt.title(f\"{column} vs Stroke\")\n    plt.show()","2fbc9d3e":"for column in categorical_data_cols:\n    print(f\"Unique values in {column} are: {', '.join([str(i) for i in data[column].unique()])}\")","bcb47272":"married_map = {\n    \"Yes\":1,\n    \"No\":0\n}\nresidence_map = {\n    \"Urban\":1,\n    \"Rural\":2\n}\n\nord_encoder = OrdinalEncoder()\ndata[\"smoking_status\"] = ord_encoder.fit_transform(data[\"smoking_status\"].values.reshape(-1, 1))\n\ndata[\"ever_married\"] = data[\"ever_married\"].map(married_map)\ndata[\"Residence_type\"] = data[\"Residence_type\"].map(residence_map)\n\ndata = pd.get_dummies(data, columns = [\"gender\", \"work_type\"], drop_first = True)\ndata.head()","f82f31c9":"for column in continuous_data_cols:\n    print(f\"Number of NaN values in {column}: {data[column].isnull().sum()}\")","d1458d89":"# checking the distribution of the data\nplt.figure(figsize = (11, 9))\ni = 1\nfor column in continuous_data_cols:\n    plt.subplot(2, 2, i)\n    sns.histplot(data[column], bins = 50)\n    i+=1\nplt.show()","4170de35":"# checking the distribution of the data\nplt.figure(figsize = (11, 9))\ni = 1\nfor column in continuous_data_cols:\n    plt.subplot(2, 2, i)\n    sns.kdeplot(data[column])\n    i+=1\nplt.show()","4dad2d9d":"# checking the outliers in the data\nplt.figure(figsize = (11, 9))\ni = 1\nfor column in continuous_data_cols:\n    plt.subplot(2, 2, i)\n    sns.boxplot(x = data[column])\n    i+=1\nplt.show()","73fc680d":"data[\"bmi\"].fillna(value = data[\"bmi\"].median(), inplace = True)\nprint(f\"Number of missing values in BMI: {data['bmi'].isnull().sum()}\")","a85599e3":"# checking the distribution of the data\nplt.figure(figsize = (11, 9))\ni = 1\nfor column in continuous_data_cols:\n    plt.subplot(2, 2, i)\n    sns.kdeplot(data[column])\n    i+=1\nplt.show()","4244693c":"data[\"bmi\"] = np.log(data[\"bmi\"])\nsns.kdeplot(data[\"bmi\"])\nplt.show()","16b4d7fa":"plt.figure(figsize = (7,7))\nsns.pairplot(data[continuous_data_cols+[\"stroke\"]], hue = \"stroke\")\nplt.show()","9c1c95ac":"plt.figure(figsize = (9,7))\nsns.heatmap(data[continuous_data_cols].corr(), annot = True, center = 0)\nplt.show()","ce4840b8":"data.head()","330136ab":"X = data.drop(columns = [\"stroke\"])\ny = data[\"stroke\"]","c640d099":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 24)\nprint(f\"Train Data: {X_train.shape}, {y_train.shape}\")\nprint(f\"Train Data: {X_test.shape}, {y_test.shape}\")","ecc9fca3":"counter = Counter(y_train)\nprint(f\"Before Upsampling: {counter}\")\n\nupsample = SMOTE()\nX_train, y_train = upsample.fit_resample(X_train, y_train)\ncounter = Counter(y_train)\nprint(counter)","86590307":"# After Upsampling\nprint(f\"Train Data: {X_train.shape}, {y_train.shape}\")\nprint(f\"Train Data: {X_test.shape}, {y_test.shape}\")","5e09f926":"std_scaler  = StandardScaler()\nX_train = std_scaler.fit_transform(X_train)\nX_test = std_scaler.transform(X_test)","4cbca5a1":"all_models = {\n    \"xgb_model\":XGBClassifier(eval_metric = \"logloss\",random_state=18,use_label_encoder=False),\n    \"rf_model\":RandomForestClassifier(random_state = 18),\n    \"logistic_model\":LogisticRegression(),\n    \"svm_model\":SVC(),\n    \"ada_model\":AdaBoostClassifier(RandomForestClassifier(random_state = 18))\n}\n\nfor model_name in all_models:\n    print(f\"Model Name: {model_name}\")\n    cv_score = cross_val_score(all_models[model_name],X_train, y_train, cv = 5)\n    print(cv_score)\n    print(f\"Mean Score: {np.mean(cv_score)}\")\n    print()","7bb89dd1":"svm_model = SVC()\nsvm_model.fit(X_train, y_train)\n\nprint(\"On Test Data\")\npredictions = svm_model.predict(X_test)\nprint(f\"Accuracy: {accuracy_score(y_test, predictions)}\")\nprint(f\"F1 Score: {f1_score(y_test, predictions)}\")\nprint(f\"Precision: {precision_score(y_test, predictions)}\")\nprint(f\"Recall: {recall_score(y_test, predictions)}\")\nplot_confusion_matrix(svm_model, X_test, y_test)\nplt.show()\n\nprint()\n\nprint(\"On Train Data\")\npredictions = svm_model.predict(X_train)\nprint(f\"Accuracy: {accuracy_score(y_train, predictions)}\")\nprint(f\"F1 Score: {f1_score(y_train, predictions)}\")\nprint(f\"Precision: {precision_score(y_train, predictions)}\")\nprint(f\"Recall: {recall_score(y_train, predictions)}\")\nplot_confusion_matrix(svm_model, X_train, y_train)\nplt.show()","50cd57da":"param_grid = {'C': [0.1, 1, 10, 100, 1000], \n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n              'kernel': ['rbf']} \n  \ngrid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 0)\ngrid.fit(X_train, y_train)\n\nprint(\"Best Params:\",grid.best_params_)\nprint(\"Best Estimator\", grid.best_estimator_)","39167603":"svm_model = SVC(C=10, gamma=1)\nsvm_model.fit(X_train, y_train)\n\nprint(\"On Test Data\")\npredictions = svm_model.predict(X_test)\nprint(f\"Accuracy: {accuracy_score(y_test, predictions)}\")\nprint(f\"F1 Score: {f1_score(y_test, predictions)}\")\nprint(f\"Precision: {precision_score(y_test, predictions)}\")\nprint(f\"Recall: {recall_score(y_test, predictions)}\")\nplot_confusion_matrix(svm_model, X_test, y_test)\nplt.show()\n\nprint()\n\nprint(\"On Train Data\")\npredictions = svm_model.predict(X_train)\nprint(f\"Accuracy: {accuracy_score(y_train, predictions)}\")\nprint(f\"F1 Score: {f1_score(y_train, predictions)}\")\nprint(f\"Precision: {precision_score(y_train, predictions)}\")\nprint(f\"Recall: {recall_score(y_train, predictions)}\")\nplot_confusion_matrix(svm_model, X_train, y_train)\nplt.show()","c076ae35":"def plot_roc_auc(model, X, y):\n    probs = model.predict_proba(X)\n    preds = probs[:,1]\n    fpr, tpr, threshold = roc_curve(y, preds)\n    roc_auc = auc(fpr, tpr)\n    \n    print(\"AUC Score\",roc_auc_score(y, preds))\n\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()","8fb4afd3":"error_rate = []\nfor i in range(1, 50):\n    pipeline = KNeighborsClassifier(n_neighbors = i)\n    pipeline.fit(X_train, y_train)\n    predictions = pipeline.predict(X_test)\n    accuracy = accuracy_score(y_test, predictions)\n    print(f\"Accuracy at k = {i} is {accuracy}\")\n    error_rate.append(np.mean(predictions != y_test))\n\nplt.figure(figsize=(10,6))\nplt.plot(range(1,50),error_rate,color='blue', linestyle='dashed', \n         marker='o',markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')\nprint(\"Minimum error:-\",min(error_rate),\"at K =\",error_rate.index(min(error_rate))+1)","88d867c5":"knn_model = KNeighborsClassifier(n_neighbors = 2)\nknn_model.fit(X_train, y_train)\n\nprint(\"On Test Data\")\npredictions = knn_model.predict(X_test)\nprint(f\"Accuracy: {accuracy_score(y_test, predictions)}\")\nprint(f\"F1 Score: {f1_score(y_test, predictions)}\")\nprint(f\"Precision: {precision_score(y_test, predictions)}\")\nprint(f\"Recall: {recall_score(y_test, predictions)}\")\nplot_confusion_matrix(knn_model, X_test, y_test)\nplt.show()\n\nprint()\n\nprint(\"On Train Data\")\npredictions = knn_model.predict(X_train)\nprint(f\"Accuracy: {accuracy_score(y_train, predictions)}\")\nprint(f\"F1 Score: {f1_score(y_train, predictions)}\")\nprint(f\"Precision: {precision_score(y_train, predictions)}\")\nprint(f\"Recall: {recall_score(y_train, predictions)}\")\nplot_confusion_matrix(knn_model, X_train, y_train)\nplt.show()","c5514433":"rf_model = RandomForestClassifier(random_state = 24)\nrf_model.fit(X_train, y_train)\n\nprint(\"On Test Data\")\npredictions = rf_model.predict(X_test)\nprint(f\"Accuracy: {accuracy_score(y_test, predictions)}\")\nprint(f\"F1 Score: {f1_score(y_test, predictions)}\")\nprint(f\"Precision: {precision_score(y_test, predictions)}\")\nprint(f\"Recall: {recall_score(y_test, predictions)}\")\nplot_confusion_matrix(rf_model, X_test, y_test)\nplt.show()\n\nplot_roc_auc(rf_model, X_test, y_test)\n\nprint()\n\nprint(\"On Train Data\")\npredictions = rf_model.predict(X_train)\nprint(f\"Accuracy: {accuracy_score(y_train, predictions)}\")\nprint(f\"F1 Score: {f1_score(y_train, predictions)}\")\nprint(f\"Precision: {precision_score(y_train, predictions)}\")\nprint(f\"Recall: {recall_score(y_train, predictions)}\")\nplot_confusion_matrix(rf_model, X_train, y_train)\nplt.show()\nplot_roc_auc(rf_model, X_train, y_train)","b3539455":"xgb_model = XGBClassifier(eval_metric = \"logloss\",random_state=18,use_label_encoder=False)\nxgb_model.fit(X_train, y_train)\n\nprint(\"On Test Data\")\npredictions =xgb_model.predict(X_test)\nprint(f\"Accuracy: {accuracy_score(y_test, predictions)}\")\nprint(f\"F1 Score: {f1_score(y_test, predictions)}\")\nprint(f\"Precision: {precision_score(y_test, predictions)}\")\nprint(f\"Recall: {recall_score(y_test, predictions)}\")\nplot_confusion_matrix(xgb_model, X_test, y_test)\nplt.show()\nplot_roc_auc(xgb_model, X_test, y_test)\n\n\nprint()\n\nprint(\"On Train Data\")\npredictions =xgb_model.predict(X_train)\nprint(f\"Accuracy: {accuracy_score(y_train, predictions)}\")\nprint(f\"F1 Score: {f1_score(y_train, predictions)}\")\nprint(f\"Precision: {precision_score(y_train, predictions)}\")\nprint(f\"Recall: {recall_score(y_train, predictions)}\")\nplot_confusion_matrix(xgb_model, X_train, y_train)\nplt.show()\nplot_roc_auc(xgb_model, X_train, y_train)","354ddfdc":"ada_model = AdaBoostClassifier()\nada_model.fit(X_train, y_train)\n\nprint(\"On Test Data\")\npredictions = ada_model.predict(X_test)\nprint(f\"Accuracy: {accuracy_score(y_test, predictions)}\")\nprint(f\"F1 Score: {f1_score(y_test, predictions)}\")\nprint(f\"Precision: {precision_score(y_test, predictions)}\")\nprint(f\"Recall: {recall_score(y_test, predictions)}\")\nplot_confusion_matrix(ada_model, X_test, y_test)\nplt.show()\nplot_roc_auc(ada_model, X_test, y_test)\n\n\nprint()\n\nprint(\"On Train Data\")\npredictions = ada_model.predict(X_train)\nprint(f\"Accuracy: {accuracy_score(y_train, predictions)}\")\nprint(f\"F1 Score: {f1_score(y_train, predictions)}\")\nprint(f\"Precision: {precision_score(y_train, predictions)}\")\nprint(f\"Recall: {recall_score(y_train, predictions)}\")\nplot_confusion_matrix(ada_model, X_train, y_train)\nplt.show()\nplot_roc_auc(ada_model, X_train, y_train)","9a1f52b2":"voting_model = VotingClassifier(\n    [\n        (\"svm_model\", SVC()),\n        (\"xgb_model\", XGBClassifier(eval_metric = \"logloss\",random_state=18,use_label_encoder=False)),\n        (\"ada_model\", AdaBoostClassifier())\n    ]\n)\n\nvoting_model.fit(X_train, y_train)\n\nprint(\"On Test Data\")\npredictions = voting_model.predict(X_test)\nprint(f\"Accuracy: {accuracy_score(y_test, predictions)}\")\nprint(f\"F1 Score: {f1_score(y_test, predictions)}\")\nprint(f\"Precision: {precision_score(y_test, predictions)}\")\nprint(f\"Recall: {recall_score(y_test, predictions)}\")\nplot_confusion_matrix(voting_model, X_test, y_test)\nplt.show()\n\n\nprint()\n\nprint(\"On Train Data\")\npredictions = voting_model.predict(X_train)\nprint(f\"Accuracy: {accuracy_score(y_train, predictions)}\")\nprint(f\"F1 Score: {f1_score(y_train, predictions)}\")\nprint(f\"Precision: {precision_score(y_train, predictions)}\")\nprint(f\"Recall: {recall_score(y_train, predictions)}\")\nplot_confusion_matrix(voting_model, X_train, y_train)\nplt.show()\n","9188241b":"## SVM Model","a51372ff":"## Scaling the Data","d3982b05":"<b>OBSERVATION: <\/b>We can notice that the number of unique id's is equal to the number of rows. Therfore there is no duplicacy in the data.","3c1cf05d":"## Upsampling using SMOTE","4368bf21":"## KNN Classifier","44378384":"## XGBoost Model","75402068":"## About the Data\n<b>Data Dictionary<\/b><br>\n1. id: unique identifier\n2. gender: \"Male\", \"Female\" or \"Other\"\n3. age: age of the patient\n4. hypertension: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension\n5. heart_disease: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease\n6. ever_married: \"No\" or \"Yes\"\n7. work_type: \"children\", \"Govt_jov\", \"Never_worked\", \"Private\" or \"Self-employed\"\n8. Residence_type: \"Rural\" or \"Urban\"\n9. avg_glucose_level: average glucose level in blood\n10. bmi: body mass index\n11. smoking_status: \"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"*\n12. stroke: 1 if the patient had a stroke or 0 if not","50a714b5":"<b>OBSERVATIONS:<\/b><br>\n1. The number of male and female having stroke are almost equal in number.\n2. The people suffering and not suffering with hypertension have almost same and no sign of heart stroke. This may be due to the fact that the number of records with stroke \"1\" is very less.\n3. The married people are showing more signs for heart stroke\n4. The people who are having private jobs are more prone to heart attack.","f5ce4001":"## Random Forest Model","efc109ed":"The BMI column is in the form of log normal distribution. Let us apply log transformation to convert it into Normal Distribution.","b7327278":"## Hyperparameter Tuning for SVM Model","22e7aeb6":"<b>OBSERVATION: <\/b>Since the data for BMI is right skewed let us fill the missing values in the bmi with median, since median is not affected by the ouliers.","2b83270e":"## Adaboost Model","bfb20e90":"## Checking for best baseline model using Cross Validation","f7ff8374":"## Univariate Analysis","2773eadf":"## Bivariate Analysis","c5ee2101":"## Splitting the data for training and testing","e36219dd":"<b>OBSERVATION: <\/b>From the above plot we can clearly notice that the dataset is higly imbalanced dataset. We need to do some upsampling to balance the data.","209a39be":"<b>OBSERVATION: <\/b>Here smoking status is an ordinal variable and remaining are nominal variables. Let us do ordinal encoding for the ordinal variable and one hot encoding for nominal variables."}}