{"cell_type":{"b5f9600d":"code","64a65dec":"code","d19e6c30":"code","a4eb912b":"code","a6559c18":"code","52732539":"code","e3acf379":"code","3c743666":"code","402843cc":"code","f81d6e45":"code","2bfee45d":"code","1b00d366":"code","da9851fe":"code","c01c49f0":"code","30dfccaf":"code","b74176ff":"code","f56d2ba4":"code","a7426d45":"code","60267504":"code","ff6f9d63":"code","679c199c":"code","c1cbd48e":"code","a940497f":"code","09ecbdc1":"code","fc60e919":"code","fa2d16a0":"code","cb80f2b6":"markdown","24676c98":"markdown"},"source":{"b5f9600d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","64a65dec":"!ls \/kaggle\/input","d19e6c30":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","a4eb912b":"# load the dataset into dataframe cc_df\ncc_df = pd.read_csv('\/kaggle\/input\/default-of-credit-card-clients-dataset\/UCI_Credit_Card.csv')","a6559c18":"cc_df.describe()","52732539":"cc_df.head()","e3acf379":"#No missing data as all the columns have 30000 entries\ncc_df.info()","3c743666":"plt.figure(figsize=(12,8))\ncc_df.corr()['default.payment.next.month'][:].sort_values().plot(kind='bar')","402843cc":"cc_df.hist(bins = 30, figsize = (20,20), color = 'b')\n","f81d6e45":"#Check how many are samples we have with default and no-default cases\n\ncc_df['default.payment.next.month'].value_counts()","2bfee45d":"#Check the correlations\n\ncorrelations = cc_df.corr()\nf, ax = plt.subplots(figsize=(20,20))\nsns.heatmap(correlations, annot=True)","1b00d366":"plt.figure(figsize=[20, 10])\nsns.countplot(x = 'AGE', hue = 'default.payment.next.month', data = cc_df)","da9851fe":"#Check the corelation with the categorical variables\n\nplt.figure(figsize=[20,20])\nplt.subplot(311)\nsns.countplot(x = 'EDUCATION', hue = 'default.payment.next.month', data = cc_df)\nplt.subplot(312)\nsns.countplot(x = 'SEX', hue = 'default.payment.next.month', data = cc_df)\nplt.subplot(313)\nsns.countplot(x = 'MARRIAGE', hue = 'default.payment.next.month', data = cc_df)\n","c01c49f0":"\nplt.figure(figsize=(12,7))\ncc_default_df        = cc_df[cc_df['default.payment.next.month'] == 1]\ncc_nodefault_df      = cc_df[cc_df['default.payment.next.month'] == 0]\n\nsns.histplot(cc_nodefault_df['LIMIT_BAL'], bins = 250, color = 'b')\nsns.histplot(cc_default_df['LIMIT_BAL'], bins = 250, color = 'r')\n","30dfccaf":"plt.figure(figsize=(12,7))\n\nsns.kdeplot(cc_nodefault_df['BILL_AMT1'], label = 'Customers who did not default', shade = True, color = 'b')\nsns.kdeplot(cc_default_df['BILL_AMT1'], label = 'Customers who defaulted', shade = True, color = 'r')\n","b74176ff":"# Let's see the impact of categorical variables on the balance\n\nplt.figure(figsize=[10,20])\nplt.subplot(311)\nsns.boxplot(x = 'SEX', y = 'LIMIT_BAL', data = cc_df, showfliers = False)\nplt.subplot(312)\nsns.boxplot(x = 'MARRIAGE', y = 'LIMIT_BAL', data = cc_df, showfliers = False)\nplt.subplot(313)\nsns.boxplot(x = 'EDUCATION', y = 'LIMIT_BAL', data = cc_df, showfliers = False)","f56d2ba4":"# We need to convert categorical variables into onehotencoding\ncats = cc_df[['SEX', 'EDUCATION', 'MARRIAGE']]\ncats","a7426d45":"#Convert the categorical variables into OneHot encoding\n\nfrom sklearn.preprocessing import OneHotEncoder\nonehotencoder = OneHotEncoder()\ncats = onehotencoder.fit_transform(cats).toarray()","60267504":"cats","ff6f9d63":"cats = pd.DataFrame(cats)\ncats","679c199c":"X = cc_df.drop(['ID', 'default.payment.next.month','SEX', 'EDUCATION', 'MARRIAGE' ], axis = 1)\nX","c1cbd48e":"X = pd.concat([cats, X], axis=1)\nX","a940497f":"#from sklearn.preprocessing import MinMaxScaler\n#scaler = MinMaxScaler()\n#X = scaler.fit_transform(X)","09ecbdc1":"y = cc_df['default.payment.next.month']\ny","fc60e919":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)\n\nparam_grid = {\n        'gamma': [0.5, 1, 5],   # regularization parameter \n        'subsample': [0.3,  0.7, 1.0], # % of rows taken to build each tree\n        'colsample_bytree': [0.6, 0.8, 1.0], # number of columns used by each tree\n        'max_depth': [ 5, 6] # depth of each tree\n        }\n\n\nimport xgboost as xgb\nxgb_model = xgb.XGBClassifier(learning_rate=0.01, n_estimators=100, eval_metric='logloss', use_label_encoder=False)\nfrom sklearn.model_selection import GridSearchCV\ngrid = GridSearchCV(xgb_model, param_grid, refit = True, verbose = 4)\ngrid.fit(X_train, y_train)","fa2d16a0":"# evaluate predictions\nfrom sklearn.metrics import confusion_matrix, average_precision_score, roc_auc_score, roc_curve, classification_report, precision_recall_curve, f1_score\n    \ny_prob=grid.predict_proba(X_test)\ny_pred = grid.predict(X_test)\naccuracy = metrics.accuracy_score(y_test, y_pred)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\nroc_xgb = roc_auc_score(y_test, y_prob[:,1])\nprint('ROC-AUC', roc_xgb)\nprint('='*20)\nprint('Confusion Matrix')\ncm_xgb = confusion_matrix(y_test, y_pred)\nprint(cm_xgb)\nsns.heatmap(cm_xgb, annot=True, cmap='viridis')\ncl_xgb = classification_report(y_test,y_pred )\nprint(cl_xgb)","cb80f2b6":"**It is slightly imbalanced dataset as we have 23364 for 0, and 6636 for 1 **","24676c98":"**Model with XGBoost and use grid search to optimize the hyperparameters"}}