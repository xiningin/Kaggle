{"cell_type":{"8c5499a3":"code","63af0e73":"code","aeb3803d":"code","a8337267":"code","e5eb6dea":"code","e2102d56":"code","d137c071":"code","de7c2e46":"code","61470d25":"code","67ee383c":"code","4ead9d6a":"code","8e330c22":"markdown","1bd9579a":"markdown","ff8cbfef":"markdown","2d3d1f6e":"markdown","60c56ce3":"markdown","bdeaab50":"markdown","1583d3ac":"markdown","83e38223":"markdown"},"source":{"8c5499a3":"#Importing all the necessary packages\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import PowerTransformer","63af0e73":"#First step in analysing any dataset is to check if it has any missing values. So let's do that first\n\ndata =pd.read_csv(\"..\/input\/CC GENERAL.csv\")\nmissing = data.isna().sum()\nprint(missing)","aeb3803d":"# Aha! Minimum payments and credit limit has missing values. We can fill in those missing values with either mean or median of its respective column.\n\ndata['MINIMUM_PAYMENTS'] = data['MINIMUM_PAYMENTS'].fillna(data['MINIMUM_PAYMENTS'].median())\ndata['CREDIT_LIMIT'] = data['CREDIT_LIMIT'].fillna(data['CREDIT_LIMIT'].median())\ndata = data.drop(['CUST_ID'],axis=1)","a8337267":"# Let's take a look at how our data looks\ndata.head()","e5eb6dea":"# It's always a good practice to remove unnecessary information from your dataset. This helps the algorithm converge better\n# To find out which features are important, let's take a look their variance. Variance prvoides a quick overview of how spread the feature is. \n# Lower the varaince, less important the feature is.\n\nfor j in list(data.columns.values):\n    print(\"Feature: {0}, Variance: {1}\".format(j,data[j].var()))","e2102d56":"# Let's get rid of the fatures with less variance\n\ndata = data.drop([\"BALANCE_FREQUENCY\",\"PURCHASES_FREQUENCY\",\"ONEOFF_PURCHASES_FREQUENCY\",\"PURCHASES_INSTALLMENTS_FREQUENCY\",\"CASH_ADVANCE_FREQUENCY\",\"CASH_ADVANCE_TRX\",\"PURCHASES_TRX\",\"PRC_FULL_PAYMENT\",\"TENURE\"],axis=1)\nfor j in list(data.columns.values):\n    print(\"Feature: {0}, Variance: {1}\".format(j,data[j].var()))","d137c071":"# Next step is to chekc if we are dealing with a lot of outliers. Having a lot of them can cause K means to perform poorly.\nplt.figure(figsize=(20,10))\nfor j in list(data.columns.values):\n    plt.scatter(y=data[j],x=[i for i in range(len(data[j]))],s=[20])\nplt.legend()","de7c2e46":"# Luckily, this dataset has very few outliers which we can ignore for now. However, one problem that is evident in the graph above is the scale of values\n# Any machine learning model would perform better over a scaled dataset compared to a non -scaled one. So let's get that done.\n\nX = PowerTransformer(method='yeo-johnson').fit_transform(data)","61470d25":"# Now that our data is ready, we can run our K-means algorithm over it. The trick here is to guess the number of clusters you want k-means to make.\n# This is called the elbow technique\n\nwcss = []\nfor ii in range( 1, 30 ):\n    kmeans = KMeans(n_clusters=ii, init=\"k-means++\", n_init=10, max_iter=300) \n    kmeans.fit_predict(X)\n    wcss.append( kmeans.inertia_ )\n    \nplt.plot( wcss, 'ro-', label=\"WCSS\")\nplt.title(\"Computing WCSS for KMeans++\")\nplt.xlabel(\"Number of clusters\")\nplt.ylabel(\"WCSS\")\nplt.show()","67ee383c":"# Somewhere near 7, the slope starts to flatten signifancty. So 7 is a good no of clusters to start with.\n\n\nkmeans = KMeans(n_clusters=7, init=\"k-means++\", n_init=10, max_iter=300) \ny_pred = kmeans.fit_predict(X)","4ead9d6a":"# Now that we have our clusters, the only thing left to do is find out what are the common traits amongst the members in each cluster\n# A pairplot is the simplest way to visualize this relationship.\n\ndata[\"cluster\"] = y_pred\ncols = list(data.columns)\nss = sns.pairplot( data[ cols ], hue=\"cluster\")\nplt.legend()","8e330c22":"## Clustering using Kmeans\n\nK-means clustering is a type of unsupervised learning, which is used when you have unlabeled data (i.e., data without defined categories or groups). The goal of this algorithm is to find groups in the data, with the number of groups represented by the variable K. The algorithm works iteratively to assign each data point to one of K groups based on the features that are provided. Data points are clustered based on feature similarity.\n\n![Image](https:\/\/www.jeremyjordan.me\/content\/images\/2016\/12\/kmeans.gif)","1bd9579a":"The goal was to segment the customers in order to define a marketing strategy. Unfortunately the colors of the plots change when this kernel is rerun - but here are some thoughts:\n\n* **Big Spenders with large Payments** - they make expensive purchases and have a credit limit that is between average and high.  This is only a small group of customers.\n* **Cash Advances with large Payments** - this group takes the most cash advances. They make large payments, but this appears to be a small group of customers.\n* **Medium Spenders with third highest Payments **- the second highest Purchases group (after the Big Spenders).\n* **Highest Credit Limit but Frugal** - this group doesn't make a lot of purchases. It looks like the 3rd largest group of customers.\n* **Cash Advances with Small Payments **- this group likes taking cash advances, but make only small payments. \n* **Small Spenders and Low Credit Limit** - they have the smallest Balances after the Smallest Spenders, their Credit Limit is in the bottom 3 groups, the second largest group of customers.\n* **Smallest Spenders and Lowest Credit Limit** - this is the group with the lowest credit limit but they don't appear to buy much. Unfortunately this appears to be the largest group of customers.\n* **Highest Min Payments** - this group has the highest minimum payments (which presumably refers to \"Min Payment Due\" on the monthly statement. This might be a reflection of the fact that they have the second lowest Credit Limit of the groups, so it looks like the bank has identified them as higher risk.)\n\nSo a marketing strategy that targeted the first five groups might be effective. ","ff8cbfef":"## Clustering","2d3d1f6e":"## Working of K means\n\nThe \u039a-means clustering algorithm uses iterative refinement to produce a final result. The algorithm inputs are the number of clusters \u039a and the data set. The data set is a collection of features for each data point. The algorithms starts with initial estimates for the \u039a centroids, which can either be randomly generated or randomly selected from the data set. The algorithm then iterates between two steps:\n\n1. Data assigment step:\n\nEach centroid defines one of the clusters. In this step, each data point is assigned to its nearest centroid, based on the squared Euclidean distance. More formally, if ci is the collection of centroids in set C, then each data point x is assigned to a cluster based on\n\n$$\\underset{c_i \\in C}{\\arg\\min} \\; dist(c_i,x)^2$$\n\nwhere dist( \u00b7 ) is the standard (L2) Euclidean distance. Let the set of data point assignments for each ith cluster centroid be Si.\n\n2. Centroid update step:\n\nIn this step, the centroids are recomputed. This is done by taking the mean of all data points assigned to that centroid's cluster.\n\n$$c_i=\\frac{1}{|S_i|}\\sum_{x_i \\in S_i x_i}$$\n\nThe algorithm iterates between steps one and two until a stopping criteria is met (i.e., no data points change clusters, the sum of the distances is minimized, or some maximum number of iterations is reached).\n\nThis algorithm is guaranteed to converge to a result. The result may be a local optimum (i.e. not necessarily the best possible outcome), meaning that assessing more than one run of the algorithm with randomized starting centroids may give a better outcome.\n\nChoosing K\nThe algorithm described above finds the clusters and data set labels for a particular pre-chosen K. To find the number of clusters in the data, the user needs to run the K-means clustering algorithm for a range of K values and compare the results. In general, there is no method for determining exact value of K, but an accurate estimate can be obtained using the following techniques.\n\nOne of the metrics that is commonly used to compare results across different values of K is the mean distance between data points and their cluster centroid. Since increasing the number of clusters will always reduce the distance to data points, increasing K will always decrease this metric, to the extreme of reaching zero when K is the same as the number of data points. Thus, this metric cannot be used as the sole target. Instead, mean distance to the centroid as a function of K is plotted and the \"elbow point,\" where the rate of decrease sharply shifts, can be used to roughly determine K.\n\nA number of other techniques exist for validating K, including cross-validation, information criteria, the information theoretic jump method, the silhouette method, and the G-means algorithm. In addition, monitoring the distribution of data points across groups provides insight into how the algorithm is splitting the data for each K.\n\n![Image](https:\/\/media.giphy.com\/media\/42dsvcMDP3diU\/giphy.gif)","60c56ce3":"Clustering is the task of dividing the population or data points into a number of groups such that data points in the same groups are more similar to other data points in the same group and dissimilar to the data points in other groups. It is basically a collection of objects on the basis of similarity and dissimilarity between them.\n\nFor Ex: The data points in the graph below clustered together can be classified into one single group. We can distinguish the clusters, and we can identify that there are 3 clusters in the below picture.\n\nSome commonly used clustering algorithms:\n1. K means\n2. DBSCAN\n3. Mean-Shift \n4. Hierarchical\n\n![Image](https:\/\/cdncontribute.geeksforgeeks.org\/wp-content\/uploads\/merge3cluster.jpg)\n","bdeaab50":"Unsupervised learning is the training of an artificial intelligence (AI) algorithm using information that is neither classified nor labeled and allowing the algorithm to act on that information without guidance.\n\nIn unsupervised learning, an AI system is presented with unlabeled, uncategorised data and the system\u2019s algorithms act on the data without prior training. The output is dependent upon the coded algorithms. Subjecting a system to unsupervised learning is one way of testing AI.\n\nUnsupervised learning algorithms can perform more complex processing tasks than supervised learning systems. However, unsupervised learning can be more unpredictable than the alternate model. While an unsupervised learning AI system might, for example, figure out on its own how to sort cats from dogs, it might also add unforeseen and undesired categories to deal with unusual breeds, creating clutter instead of order.","1583d3ac":"# Problem Statement\n\nThe data provider @arjunbhasin2013 says: \n> This case requires to develop a customer segmentation to define marketing strategy. The sample Dataset summarizes the usage behavior of about 9000 active credit card holders during the last 6 months. The file is at a customer level with 18 behavioral variables.\n","83e38223":"# Unsupervised Learning"}}