{"cell_type":{"94e13c7f":"code","5efbd49e":"code","1c70ffdf":"code","2a686842":"code","6cc0351f":"code","26cd5816":"code","2654c744":"code","28eb8d4b":"code","4d295eb3":"code","4d2551e6":"code","d1819738":"code","4d3c20c7":"code","ae602f36":"code","ce429e2d":"code","e9052df4":"code","445f96c5":"code","b34b73eb":"code","93f6974a":"code","cc4bbb26":"code","82b6c9bd":"code","bcb1332f":"code","6959ebb5":"code","47e450f2":"code","651ca543":"code","3ee40cba":"code","8d54afeb":"code","cbe81e2e":"code","d64c04c1":"code","08bb52c8":"code","f3de82cd":"code","1e43f640":"code","93ea1c7c":"code","ee129747":"code","fa21ffae":"code","32f442e3":"code","7a621fcc":"code","2c89630e":"code","5ca6b163":"code","3aa43058":"code","c25c0ef7":"code","153f4034":"code","1b47e71d":"code","f50f92ab":"markdown","fae78581":"markdown","eebc4788":"markdown"},"source":{"94e13c7f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5efbd49e":"import matplotlib.pyplot as plt\nfrom fbprophet import Prophet","1c70ffdf":"df = pd.read_csv(\"..\/input\/corn2015-2017\/corn2013-2017.txt\",sep=',',header=None, names=['date','price'])\ndf.head()","2a686842":"df.info()","6cc0351f":"df['date'] = pd.to_datetime(df['date'])","26cd5816":"df.head()","2654c744":"import plotly.express as px\n\nfig = px.line(df, x=\"date\", y=\"price\", title='Price Time Series')\nfig.show();","28eb8d4b":"df = df.rename(columns={'date':'ds', 'price':'y'})","4d295eb3":"model = Prophet()\nmodel.fit(df)","4d2551e6":"future = model.make_future_dataframe(periods=365)\nforecast = model.predict(future)","d1819738":"forecast.head()","4d3c20c7":"fig1 = model.plot(forecast)","ae602f36":"fig2 = model.plot_components(forecast)","ce429e2d":"!pip install pmdarima","e9052df4":"from statsmodels.tsa.arima_model import ARMA,ARMAResults,ARIMA,ARIMAResults\nfrom statsmodels.graphics.tsaplots import plot_acf,plot_pacf \nfrom pmdarima import auto_arima \nfrom statsmodels.tsa.statespace.sarimax import SARIMAX","445f96c5":"df = pd.read_csv(\"..\/input\/corn2015-2017\/corn2013-2017.txt\",sep=',',header=None, names=['date','price'])\ndf.head()","b34b73eb":"df['date'] = pd.to_datetime(df['date'])","93f6974a":"df.set_index(\"date\",inplace=True)","cc4bbb26":"df.index.freq='W'","82b6c9bd":"df['price'].plot(figsize=(12,5));","bcb1332f":"adf_test(df['price'])","6959ebb5":"auto_arima(df['price'],seasonal=True).summary()","47e450f2":"df.shape","651ca543":"train = df.iloc[:62]\ntest = df.iloc[62:]","3ee40cba":"model = SARIMAX(train['price'],order=(0,1,1))\nresults = model.fit()\nresults.summary()","8d54afeb":"start=len(train)\nend=len(train)+len(test)-1\npredictions = results.predict(start=start, end=end, dynamic=False, typ='levels').rename('SARIMAX(0,1,1) Predictions')","cbe81e2e":"model = SARIMAX(df['price'],order=(0,1,1))\nresults = model.fit()\nfcast = results.predict(len(df),len(df)+11,typ='levels').rename('SARIMA(0,1,1) Forecast')","d64c04c1":"title = 'Weekly Corn Price Prediction'\nylabel='Price'\nxlabel='Date'\n\nax = df['price'].plot(legend=True,figsize=(12,6),title=title)\nfcast.plot(legend=True)\nax.autoscale(axis='x',tight=True)\nax.set(xlabel=xlabel, ylabel=ylabel);","08bb52c8":"import math\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nimport warnings\nwarnings.filterwarnings('ignore')","f3de82cd":"df = pd.read_csv(\"..\/input\/corn2015-2017\/corn2013-2017.txt\",sep=',',header=None, names=['date','price'])\ndf.head()","1e43f640":"df['date'] = pd.to_datetime(df['date'])","93ea1c7c":"df = df.iloc[:,1].values\nplt.plot(df)\nplt.xlabel(\"date\")\nplt.ylabel(\"price\")\nplt.title(\"weekly corn price\")\nplt.show()","ee129747":"df = df.reshape(-1,1)\ndf = df.astype(\"float32\")\ndf.shape","fa21ffae":"# scaling \nscaler = MinMaxScaler(feature_range=(0, 1))\ndf = scaler.fit_transform(df)","32f442e3":"train_size = int(len(df) * 0.75)\ntest_size = len(df) - train_size\ntrain = df[0:train_size,:]\ntest = df[train_size:len(df),:]\nprint(\"train size: {}, test size: {} \".format(len(train), len(test)))","7a621fcc":"time_stamp = 10\n\ndataX = []\ndataY = []\n\nfor i in range(len(train)-time_stamp-1):\n    a = train[i:(i+time_stamp), 0]\n    dataX.append(a)\n    dataY.append(train[i + time_stamp, 0])\n    \ntrainX = np.array(dataX)\ntrainY = np.array(dataY)  ","2c89630e":"dataX = []\ndataY = []\nfor i in range(len(test)-time_stamp-1):\n    a = test[i:(i+time_stamp), 0]\n    dataX.append(a)\n    dataY.append(test[i + time_stamp, 0])\ntestX = np.array(dataX)\ntestY = np.array(dataY)  ","5ca6b163":"trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\ntestX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))","3aa43058":"model = Sequential()\n\nmodel.add(LSTM(10, input_shape=(1, time_stamp))) # 10 lstm neuron\nmodel.add(Dense(1))\n\nmodel.compile(loss='mean_squared_error', optimizer='adam')\n\nmodel.fit(trainX, trainY, epochs=50, batch_size=1)","c25c0ef7":"trainPredict = model.predict(trainX)\ntestPredict = model.predict(testX)\n\n# invert predictions\ntrainPredict = scaler.inverse_transform(trainPredict)\ntrainY = scaler.inverse_transform([trainY])\ntestPredict = scaler.inverse_transform(testPredict)\ntestY = scaler.inverse_transform([testY])\n\n# calculate root mean squared error\ntrainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\nprint('Train Score: %.2f RMSE' % (trainScore))\ntestScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\nprint('Test Score: %.2f RMSE' % (testScore))","153f4034":"# shifting train\ntrainPredictPlot = np.empty_like(df)\ntrainPredictPlot[:, :] = np.nan\ntrainPredictPlot[time_stamp:len(trainPredict)+time_stamp, :] = trainPredict\n\n# shifting test\ntestPredictPlot = np.empty_like(df)\ntestPredictPlot[:, :] = np.nan\ntestPredictPlot[len(trainPredict)+(time_stamp*2)+1:len(df)-1, :] = testPredict","1b47e71d":"plt.plot(scaler.inverse_transform(df))\nplt.plot(trainPredictPlot)\nplt.plot(testPredictPlot)\nplt.show()","f50f92ab":"### LSTM ","fae78581":"### Prophet Model","eebc4788":"### Statistical Time Series Model"}}