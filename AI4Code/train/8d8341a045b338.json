{"cell_type":{"9da01ab7":"code","8b26854c":"code","3799ae89":"code","41b0f756":"code","95ff7379":"code","21cd1a41":"code","b9d34eef":"code","af694220":"code","7c6a5ab6":"code","446d9938":"code","913269ed":"code","8769c2fb":"code","52e5ff27":"code","9d41947b":"code","ddfc6f1f":"code","e549f503":"code","ec4c282a":"code","fabc77d9":"code","9d5fd9a8":"code","036c75f1":"code","c88164ed":"code","ebd743ae":"code","4bf6fdf4":"code","6d3a294e":"code","937e0fc3":"code","8ee4dad0":"code","bb6b32a6":"code","0a5df974":"code","27b265d2":"code","a4d7dc12":"code","8ba06e2f":"code","7e6b23fd":"code","0199d2ae":"code","f17f2aaa":"code","89fcaad3":"code","4a73e6ce":"code","302168bf":"code","e2ad0e11":"code","c4e975e5":"code","09ab87be":"code","655c1edb":"code","7e92a8f5":"code","7433251b":"code","8fd67c00":"code","4edd50c3":"markdown","07fbaca4":"markdown","c392e5d2":"markdown"},"source":{"9da01ab7":"print('Tweets Real \/ Fake Classification......')","8b26854c":"import os\nimport pandas as pd\nlv_path = r'..\/input\/nlp-getting-started\/'\nprint(os.listdir(lv_path))\nimport numpy as np\nimport pandas as pd\nimport itertools\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix","3799ae89":"train_df = pd.read_csv(r'..\/input\/nlp-getting-started\/train.csv')\ntrain_df.head(5)","41b0f756":"print('training data shape: ', train_df.shape)","95ff7379":"sub_df = pd.read_csv(r'..\/input\/nlp-getting-started\/sample_submission.csv')\nsub_df.head(5)","21cd1a41":"test_df = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ntest_df.head(5)","b9d34eef":"train_df['target'].value_counts()","af694220":"import seaborn as sns\nsns.barplot(train_df['target'].value_counts().index,train_df['target'].value_counts(),palette='rocket')","7c6a5ab6":"# A disaster tweet\ndisaster_tweets = train_df[train_df['target']==1]['text']\ndisaster_tweets.values[1]","446d9938":"#not a disaster tweet\nnon_disaster_tweets = train_df[train_df['target']==0]['text']\nnon_disaster_tweets.values[1]","913269ed":"train_df.loc[train_df['text'].str.contains('disaster', na=False, case=False)].target.value_counts()","8769c2fb":"# A quick glance over the existing data\ntrain_df['text'][:5]","52e5ff27":"import re\nprint(train_df.text[3])\nre.sub(\"RT @[\\w]*:\", \"\" , train_df.text[3])","9d41947b":"def clean_data(tweet):\n    tweet = re.sub(\"RT @[\\w]*:\", \"\", tweet)\n    tweet = re.sub(\"@[\\w]*\", \"\", tweet)\n    tweet = re.sub(\"https:\/\/[A-Za-z0-9.\/]\", \"\", tweet)\n    tweet = re.sub(\"\\n\", \"\", tweet)\n    tweet = re.sub(\"&amp\", \"\", tweet)\n    tweet = re.sub(\"#\", \"\", tweet)\n    tweet = re.sub(r\"[^\\w]\", ' ', tweet )\n    return tweet","ddfc6f1f":"train_df['text'] = train_df['text'].apply(lambda x: clean_data(x))\ntest_df['text'] = test_df['text'].apply(lambda x: clean_data(x))","e549f503":"train_df.head()","ec4c282a":"test_df.head()","fabc77d9":"train_df['text'] = train_df['text'].apply(lambda x: x.lower())","9d5fd9a8":"train_df['text']","036c75f1":"import nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\ntrain_df['text'] = train_df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))","c88164ed":"train_df['text']","ebd743ae":"import matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfake_data = train_df[train_df[\"target\"] == 0]\nall_words = ' '.join([text for text in fake_data.text])\nwordcloud = WordCloud(width= 800, height= 500,\n                          max_font_size = 110,\n                          collocations = False).generate(all_words)\nplt.figure(figsize=(10,7))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","4bf6fdf4":"from wordcloud import WordCloud\nfake_data = train_df[train_df[\"target\"] == 1]\nall_words = ' '.join([text for text in fake_data.text])\nwordcloud = WordCloud(width= 800, height= 500,\n                          max_font_size = 110,\n                          collocations = False).generate(all_words)\nplt.figure(figsize=(10,7))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","6d3a294e":"from sklearn.feature_extraction.text import CountVectorizer","937e0fc3":"X, y = train_df['text'], train_df['target']","8ee4dad0":"# #DataFlair - Split the dataset\n# x_train,x_test,y_train,y_test=train_test_split(train_df['target'], labels, test_size=0.2, random_state=7)","bb6b32a6":"count_vect = CountVectorizer()","0a5df974":"x_train_df = count_vect.fit_transform(X)\n# x_train_tr = count_vect.fit_transform(train_df.target)","27b265d2":"# x_train_df","a4d7dc12":"x_train_df.shape, train_df.shape","8ba06e2f":"from sklearn.feature_extraction.text import TfidfTransformer","7e6b23fd":"tfidf = TfidfTransformer()","0199d2ae":"x_traintf = tfidf.fit_transform(x_train_df)","f17f2aaa":"x_traintf.shape","89fcaad3":"# labels = train_df.target\n# labels","4a73e6ce":"train_df.head(2)","302168bf":"from sklearn.naive_bayes import MultinomialNB","e2ad0e11":"clf = MultinomialNB().fit(x_traintf, y)","c4e975e5":"test_in = [\n#     'deeds reason earthquake may allah forgive us',\n#     'summer lovely',\n#     'damage school bus 80 multi car crash breaking',\n#     'man'\n'The U.S. Army released new guidelines for optimal soldier performance \u2014 and they include strategic and aggressive napping']\nX_test_in = count_vect.transform(test_df.text) #(test_in)","09ab87be":"x_test_tf = tfidf.transform(X_test_in)\nx_test_tf","655c1edb":"pred = clf.predict(x_test_tf)","7e92a8f5":"pred","7433251b":"train_df.head(20)","8fd67c00":"sub_df['target'] = pred.round().astype(int)\nsub_df.to_csv(r'Your System Path', index=False)","4edd50c3":"#### Import All required libraries ","07fbaca4":"### Predictions","c392e5d2":"### Test Dataset "}}