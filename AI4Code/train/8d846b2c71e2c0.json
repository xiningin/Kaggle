{"cell_type":{"b4c61576":"code","688947da":"code","1dd9fc92":"code","23870c5c":"code","18408cdf":"code","6693f6e6":"code","fca1e675":"code","ad435de3":"code","43145e3f":"code","4cfc6e79":"code","b916a213":"code","675e184c":"code","756c7a07":"code","4ac81edf":"code","8060aaa4":"code","4b689789":"code","4a7d3bbd":"code","7bd632c5":"code","93bcfb3e":"code","0d0e7d49":"code","232b675b":"code","2db00881":"code","6e352250":"code","07aedba3":"code","fff2b0f1":"code","9de4c683":"code","0a3770ee":"code","181bb476":"code","d8ca7cd0":"markdown"},"source":{"b4c61576":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","688947da":"# Read Data \ntrain_data = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ntest_data =  pd.read_csv('..\/input\/digit-recognizer\/test.csv')","1dd9fc92":"train_data.head()","23870c5c":"# Function To display Image\nimport matplotlib.pyplot as plt\n%matplotlib inline\ndef show_image(image):\n    plt.imshow(image,cmap = 'gray')\n    plt.show()\n    ","18408cdf":"show_image(train_data.values[0,1:].reshape((28,28)))\nprint('LABEL --> ',train_data['label'][0])","6693f6e6":"# Here I'm gonna build a convolution model for classification purpose using Tensorflow.\n# Lets import some packages.\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Conv2D,MaxPooling2D,Flatten,Dropout\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport tensorflow as tf","fca1e675":"total_training_data = len(train_data)\ntotal_testing_data = len(test_data)\nprint('TOTAL TRAINING DATA ---> ',total_training_data)\nprint('TOTAL TESTING DATA --->',total_testing_data)","ad435de3":"X_train_data = train_data.drop(labels = 'label',axis=1)","43145e3f":"Y_train_data = train_data['label']","4cfc6e79":"X_train = tf.keras.utils.normalize(X_train_data,axis = 1)\nX_test = tf.keras.utils.normalize(test_data,axis=1)","b916a213":"X_train = X_train.values.reshape((42000,28,28,1))","675e184c":"np.unique(Y_train_data)\nY_train = tf.keras.utils.to_categorical(Y_train_data,num_classes=10)","756c7a07":"from sklearn.model_selection import train_test_split\nx_train,x_val,y_train,y_val = train_test_split(X_train,Y_train,test_size = 0.1,random_state = 1)\nprint('Trainig data: ',x_train.shape,y_train.shape)\nprint('Validation data: ',x_val.shape,y_val.shape)","4ac81edf":"# Lets define the model\nmodel = Sequential()\nmodel.add(Conv2D(input_shape = (28,28,1),filters = 16,kernel_size = (3,3),strides = (2,2),padding = 'same',activation = 'relu'))\nmodel.add(MaxPooling2D(pool_size = (1,1)))\nmodel.add(Conv2D(filters = 32,kernel_size = (3,3),strides = (2,2),padding = 'same',activation = 'relu'))\nmodel.add(MaxPooling2D(pool_size = (1,1)))\nmodel.add(Conv2D(filters = 128,kernel_size = (3,3),strides = (2,2),padding = 'same',activation = 'relu'))\nmodel.add(MaxPooling2D(pool_size =  (1,1)))\nmodel.add(Flatten())\nmodel.add(Dense(10,activation = 'softmax'))\n","8060aaa4":"model.summary()","4b689789":"model.compile(optimizer = 'adam',loss = 'categorical_crossentropy',metrics = ['Accuracy'])","4a7d3bbd":"BATCH_SIZE = 64\nEPOCHS = 500","7bd632c5":"model.fit?","93bcfb3e":"hist = model.fit(x = X_train,y = Y_train,batch_size = BATCH_SIZE,validation_data=(x_val,y_val),epochs = EPOCHS,verbose = 2)","0d0e7d49":"plt.plot(hist.history['loss'],'red',label = 'Loss')\nplt.plot(hist.history['accuracy'],'blue',label = 'Accuracy')\nplt.plot(hist.history['val_loss'],'green',label = 'val_loss')\nplt.plot(hist.history['val_accuracy'],'pink',label = 'val_accuracy')\nplt.legend()\nplt.show()","232b675b":"X_test = X_test.reshape((-1,28,28,1))","2db00881":"X_test.shape","6e352250":"pred = model.predict(X_test)","07aedba3":"Y_out = []\nfor p in pred:\n    Y_out.append(np.argmax(p))","fff2b0f1":"show_image(X_test[0].reshape((28,28)))\nY_out[0]","9de4c683":"ImageId = [i for i in range(1,X_test.shape[0]+1)]","0a3770ee":"dict1 =  {'ImageId':ImageId,\n         'Label':Y_out}\ndf = pd.DataFrame(dict1)\n","181bb476":"df.to_csv('submission.csv',index=False)","d8ca7cd0":"Total data ---> 42000+28000 = 70000\nTest data percentage ---> 0.4%\nTraining data percentage ---> 0.6%\nLet's divide training data ratio in 0.5 +0.1 (Training + Validation)\nSo, Total validation data ---> 4200 (0.1%42000)\nAnd Training data will 37800."}}