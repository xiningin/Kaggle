{"cell_type":{"6bfcd1a8":"code","d3f46d3a":"code","3787b2d7":"code","187b4a03":"code","e39c03b4":"code","06fd975e":"code","44a8e40a":"code","d0245a70":"code","08101d12":"code","4a65dcc2":"code","8067196b":"code","f9dcbbee":"code","06e8dae4":"code","5037e562":"code","e26597c3":"code","bbcfc959":"code","605e5105":"code","523133c3":"code","c13d7a4a":"markdown","09cac3bb":"markdown","129a1dac":"markdown","cef8c9ca":"markdown","6fd9da6b":"markdown","6a479dd9":"markdown"},"source":{"6bfcd1a8":"### \ud328\ud0a4\uc9c0 \uc124\uce58 \nimport pandas as pd #Analysis \nimport matplotlib.pyplot as plt #Visulization\nimport seaborn as sns #Visulization\nimport numpy as np #Analysis \nfrom scipy.stats import norm #Analysis \nfrom sklearn.preprocessing import StandardScaler #Analysis \nfrom scipy import stats #Analysis \nimport warnings \nwarnings.filterwarnings('ignore')\n%matplotlib inline\nimport gc\n\nimport os\nimport string\ncolor = sns.color_palette()\n\n%matplotlib inline\n\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nfrom sklearn import model_selection, preprocessing, metrics, ensemble, naive_bayes, linear_model\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\n\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 999\n\nimport plotly.graph_objs as go\n\nimport time\nimport random","d3f46d3a":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport warnings\nimport gc\nimport time\nimport sys\nimport datetime\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.filterwarnings('ignore')\nfrom sklearn import metrics\n\nplt.style.use('seaborn')\nsns.set(font_scale=1)\npd.set_option('display.max_columns', 500)","3787b2d7":"train_df = pd.read_csv(\"..\/input\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/test.csv\")","187b4a03":"# Taking a look at how many rows and columns the train dataset contains\nrows1 = train_df.shape[0]; rows2 = test_df.shape[0]; \ncolumns1 = train_df.shape[1]; columns2 = test_df.shape[1]\nprint(\"The train dataset contains {0} rows and {1} columns\".format(rows1, columns1))\nprint(\"The test dataset contains {0} rows and {1} columns\".format(rows2, columns2))","e39c03b4":"train_df.head()","06fd975e":"data = [go.Bar(\n            x = train_df[\"target\"].value_counts().index.values,\n            y = train_df[\"target\"].value_counts().values,\n            text='Distribution of target variable'\n    )]\n\nlayout = go.Layout(\n    title='Target variable distribution'\n)\n\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig, filename='basic-bar')","44a8e40a":"#https:\/\/www.kaggle.com\/ashishpatel26\/bird-eye-view-of-two-sigma-nn-approach\ndef mis_value_graph(data):  \n    data = [\n    go.Bar(\n        x = data.columns,\n        y = data.isnull().sum(),\n        name = 'Counts of Missing value',\n        textfont=dict(size=20),\n        marker=dict(\n        line=dict(\n            color= generate_color(),\n            #width= 2,\n        ), opacity = 0.45\n    )\n    ),\n    ]\n    layout= go.Layout(\n        title= '\"Total Missing Value By Column\"',\n        xaxis= dict(title='Columns', ticklen=5, zeroline=False, gridwidth=2),\n        yaxis= dict(title='Value Count', ticklen=5, gridwidth=2),\n        showlegend=True\n    )\n    fig = go.Figure(data=data, layout=layout)\n    py.iplot(fig, filename='skin')\n    \ndef generate_color():\n    color = '#{:02x}{:02x}{:02x}'.format(*map(lambda x: random.randint(0, 255), range(3)))\n    return color\n\nmis_value_graph(train_df)","d0245a70":"#https:\/\/www.kaggle.com\/ashishpatel26\/bird-eye-view-of-two-sigma-nn-approach\ndef mis_value_graph(data):  \n    data = [\n    go.Bar(\n        x = data.columns,\n        y = data.isnull().sum(),\n        name = 'Counts of Missing value',\n        textfont=dict(size=20),\n        marker=dict(\n        line=dict(\n            color= generate_color(),\n            #width= 2,\n        ), opacity = 0.45\n    )\n    ),\n    ]\n    layout= go.Layout(\n        title= '\"Total Missing Value By Column\"',\n        xaxis= dict(title='Columns', ticklen=5, zeroline=False, gridwidth=2),\n        yaxis= dict(title='Value Count', ticklen=5, gridwidth=2),\n        showlegend=True\n    )\n    fig = go.Figure(data=data, layout=layout)\n    py.iplot(fig, filename='skin')\n    \ndef generate_color():\n    color = '#{:02x}{:02x}{:02x}'.format(*map(lambda x: random.randint(0, 255), range(3)))\n    return color\n\nmis_value_graph(test_df)","08101d12":"train_int = train_df.copy()\ndel train_int['ID_code']\ndata = [\n    go.Heatmap(\n        z= train_int.corr().values,\n        x= train_int.columns.values,\n        y= train_int.columns.values,\n        colorscale='Viridis',\n        reversescale = False,\n        #text = True ,\n        opacity = 1.0 )\n]\n\nlayout = go.Layout(\n    title='Pearson Correlation of Integer-type features',\n    xaxis = dict(ticks='', nticks=36),\n    yaxis = dict(ticks='' ),\n    width = 900, height = 700)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='labelled-heatmap')","4a65dcc2":"# https:\/\/www.kaggle.com\/fayzur\/customer-transaction-prediction-strong-baseline\n# Thanks fayzur. Nice Parameter \nparam = {\n        'num_leaves': 10,\n        'max_bin': 119,\n        'min_data_in_leaf': 11,\n        'learning_rate': 0.02,\n        'min_sum_hessian_in_leaf': 0.00245,\n        'bagging_fraction': 1.0, \n        'bagging_freq': 5, \n        'feature_fraction': 0.05,\n        'lambda_l1': 4.972,\n        'lambda_l2': 2.276,\n        'min_gain_to_split': 0.65,\n        'max_depth': 14,\n        'save_binary': True,\n        'seed': 1337,\n        'feature_fraction_seed': 1337,\n        'bagging_seed': 1337,\n        'drop_seed': 1337,\n        'data_random_seed': 1337,\n        'objective': 'binary',\n        'boosting_type': 'gbdt',\n        'verbose': 1,\n        'metric': 'auc',\n        'is_unbalance': True,\n        'boost_from_average': False,\n    }","8067196b":"features = [c for c in train_df.columns if c not in ['ID_code', 'target']]\ntarget = train_df['target']","f9dcbbee":"%%time\nfrom sklearn.metrics import roc_auc_score, roc_curve\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=2019)\noof = np.zeros(len(train_df))\npredictions = np.zeros(len(test_df))\nfeature_importance_df = pd.DataFrame()\n\nstart = time.time()\n\n\nfor fold_, (trn_idx, val_idx) in enumerate(skf.split(train_df.values, target.values)):\n    print(\"fold n\u00b0{}\".format(fold_))\n    trn_data = lgb.Dataset(train_df.iloc[trn_idx][features], label=target.iloc[trn_idx])\n    val_data = lgb.Dataset(train_df.iloc[val_idx][features], label=target.iloc[val_idx])\n\n    num_round = 10000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 100)\n    oof[val_idx] = clf.predict(train_df.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    predictions += clf.predict(test_df[features], num_iteration=clf.best_iteration) \/ 5\n\nprint(\"CV score: {:<8.5f}\".format(roc_auc_score(target, oof)))","06e8dae4":"cols = (feature_importance_df[[\"feature\", \"importance\"]]\n        .groupby(\"feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:1000].index)\nbest_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n\nplt.figure(figsize=(14,26))\nsns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\nplt.title('LightGBM Features (averaged over folds)')\nplt.tight_layout()\nplt.savefig('lgbm_importances.png')","5037e562":"##submission\nsub_df = pd.DataFrame({\"ID_code\":test_df[\"ID_code\"].values})\nsub_df[\"target\"] = predictions\nsub_df.to_csv(\"lgb_submission.csv\", index=False)","e26597c3":"## Catboost : https:\/\/www.kaggle.com\/wakamezake\/starter-code-catboost-baseline\nfrom catboost import Pool, CatBoostClassifier\nmodel = CatBoostClassifier(loss_function=\"Logloss\", eval_metric=\"AUC\")\nkf = KFold(n_splits=5, random_state=42, shuffle=True)\n\ny_valid_pred = 0 * target\ny_test_pred = 0\n\nfor idx, (train_index, valid_index) in enumerate(kf.split(train_df)):\n    y_train, y_valid = target.iloc[train_index], target.iloc[valid_index]\n    X_train, X_valid = train_df[features].iloc[train_index,:], train_df[features].iloc[valid_index,:]\n    _train = Pool(X_train, label=y_train)\n    _valid = Pool(X_valid, label=y_valid)\n    print( \"\\nFold \", idx)\n    fit_model = model.fit(_train,\n                          eval_set=_valid,\n                          use_best_model=True,\n                          verbose=200\n                         )\n    pred = fit_model.predict_proba(X_valid)[:,1]\n    print( \"  auc = \", roc_auc_score(y_valid, pred) )\n    y_valid_pred.iloc[valid_index] = pred\n    y_test_pred += fit_model.predict_proba(test_df[features])[:,1]\ny_test_pred \/= 5","bbcfc959":"##submission\nsub_df1 = pd.DataFrame({\"ID_code\":test_df[\"ID_code\"].values})\nsub_df1[\"target\"] = y_test_pred\nsub_df1.to_csv(\"cat_submission.csv\", index=False)","605e5105":"corr_df = pd.merge(sub_df,sub_df1,how='left',on='ID_code')\ncorr_df.corr()","523133c3":"##submission\nsub_df2 = pd.DataFrame({\"ID_code\":test_df[\"ID_code\"].values})\nsub_df2[\"target\"] = 0.5*sub_df[\"target\"] + 0.5*sub_df1[\"target\"]\nsub_df2.to_csv(\"lgb_cat_submission.csv\", index=False)","c13d7a4a":"## LightGBM BaseLine","09cac3bb":"Wow. All variable name is var_. it means that the variable is identifier !!!. https:\/\/www.kaggle.com\/c\/porto-seguro-safe-driver-prediction porto competition also has identifier variable. This link will help.","129a1dac":"There are some check point. \n- 1. The train and test row are similar.  \n- 2. The column size so many.  ","cef8c9ca":"Target is unbalanced. i'll try upsampling...!","6fd9da6b":"Train and Test has no missing value. Very Nice !!!. ","6a479dd9":"![](https:\/\/storage.googleapis.com\/kaggle-organizations\/141\/thumbnail.jpg?r=890)\n# Santander Customer Transaction Prediction\nCan you identify who will make a transaction?\n\nVersion6\n- Ensemble : LB 0.899\n- LightGBM : LB 0.898\n- Catboost : LB 0.898 "}}