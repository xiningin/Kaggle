{"cell_type":{"ac4eee6b":"code","673f7496":"code","84ce017c":"code","a92abed6":"code","691ebc6d":"code","c2571528":"code","e5d61db0":"code","afeb824c":"code","181f8671":"code","37997adb":"code","99a4d54b":"code","bf25d523":"code","d749aee7":"code","2c205191":"code","2fbd8896":"code","c34f2c71":"code","a149b0de":"code","15db27e3":"code","65a5ae39":"code","4ff729c9":"code","75f8c542":"code","33f280c1":"code","c0d3c984":"code","114698e7":"code","be6cba90":"code","e5da4d81":"code","da8282ce":"code","a019ee63":"code","bc470be5":"code","9b021ba7":"code","0195e91c":"code","1fd6a38e":"code","c1322e29":"markdown","fe8dc703":"markdown","4b9fec66":"markdown","47439c92":"markdown","d93723d6":"markdown","38f1b583":"markdown","a7d7e2d7":"markdown","81e1fe70":"markdown","78645036":"markdown","fa6ef3dd":"markdown","ecfff895":"markdown","ffacf586":"markdown","3a277179":"markdown","50c4d07b":"markdown","e39eb5a5":"markdown","89edbf38":"markdown","19f97596":"markdown","aa5bf740":"markdown","a8342bd6":"markdown","f4010d0f":"markdown","a2cf3cab":"markdown","042d3de8":"markdown","a27afc39":"markdown","61e75765":"markdown","ca61159a":"markdown","8ec44bf8":"markdown","90481e66":"markdown","ee4c27b3":"markdown"},"source":{"ac4eee6b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nplt.style.use('ggplot')\nsns.set()\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","673f7496":"mpl.rcParams['axes.titlesize'] = 20\nmpl.rcParams['axes.labelsize'] = 16","84ce017c":"def reduce_mem_usage(df):\n    \"\"\"\n    Reduce dataframe's memory usage\n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    numerics = ['int8', 'int16', 'int32', 'int64', 'float16',\n                'float32', 'float64', 'uint8', 'uint16', 'uint32', 'uint64']\n\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int' or str(col_type)[:4] == 'uint':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min >= np.iinfo(np.uint8).min and c_max < np.iinfo(np.uint8).max:\n                    df[col] = df[col].astype(np.uint8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min >= np.iinfo(np.uint16).min and c_max < np.iinfo(np.uint16).max:\n                    df[col] = df[col].astype(np.uint16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min >= np.iinfo(np.uint32).min and c_max < np.iinfo(np.uint32).max:\n                    df[col] = df[col].astype(np.uint32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n                elif c_min >= np.iinfo(np.uint64).min and c_max < np.iinfo(np.uint64).max:\n                    df[col] = df[col].astype(np.uint64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimisation is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(\n        100 * (start_mem - end_mem) \/ start_mem))\n\n    return df","a92abed6":"def corr_plot(data, title, method='pearson', figsize=(13,8)):\n    \"\"\"\n    Plot the correlation matrix\n    \"\"\"\n    mname = {\n        'pearson': 'Pearson correlation',\n        'kendall': 'Kendall Tau correlation',\n        'spearman': 'Spearman rank correlation'\n    }\n    corr = data.corr(method=method)\n    fig, (ax) = plt.subplots(1, 1, figsize=figsize)\n    ax.set_title(\"{} ({})\".format(title, mname[method]))\n    ax = sns.heatmap(\n        corr,\n        vmin=-1, vmax=1, center=0,\n        cmap=sns.diverging_palette(20, 220, n=200),\n        square=True\n    )\n    ax.set_yticklabels(\n        ax.get_yticklabels(),\n        rotation=0,\n        horizontalalignment='right'\n    )\n    ax.set_xticklabels(\n        ax.get_xticklabels(),\n        rotation=90,\n        horizontalalignment='right'\n    )\n    ax.set_ylim(corr.shape[0], 0)\n    return fig, (ax)","691ebc6d":"%%time\ndf = pd.read_csv(\"\/kaggle\/input\/us-accidents\/US_Accidents_May19.csv\").pipe(reduce_mem_usage)","c2571528":"# Lowercase all columns\ndf.columns = map(str.lower, df.columns)","e5d61db0":"df.describe(include='O').T","afeb824c":"df.describe().T","181f8671":"%%time\ndf = df.assign(\n    start_time=lambda df: pd.to_datetime(df.start_time),\n    end_time=lambda df: pd.to_datetime(df.end_time),\n    weather_timestamp=lambda df: pd.to_datetime(df.weather_timestamp),\n    time_span=lambda df: df.end_time - df.start_time,\n    time_span_hour=lambda df: df.time_span \/ np.timedelta64(1, 'h'),\n    time_span_minute=lambda df: df.time_span_hour * 60,\n    start_hour=lambda df: df.start_time.dt.hour,\n    start_month=lambda df: df.start_time.dt.month,\n    start_dow=lambda df: df.start_time.dt.weekday_name,\n    start_dom=lambda df: df.start_time.dt.day,\n    end_hour=lambda df: df.end_time.dt.hour,\n    end_month=lambda df: df.end_time.dt.month,\n    end_dow=lambda df: df.end_time.dt.dayofweek,\n    end_dom=lambda df: df.end_time.dt.day,\n)","37997adb":"_ = corr_plot(df.drop(['start_time', 'end_time', 'time_span', 'weather_timestamp'], axis=1), title='Correlation plot', method='pearson', figsize=(15, 10))","99a4d54b":"plt.figure(figsize=(12,8))\nax = df['source'].value_counts().plot(kind='bar')\nax.set_title(\"Report count by source\")\nax.set_xlabel(\"Source\")\nax.get_yaxis().set_major_formatter(\n    mpl.ticker.FuncFormatter(lambda x, p: format(int(x), ','))\n)\nfor tick in ax.get_xticklabels():\n    tick.set_rotation(0)","bf25d523":"plt.figure(figsize=(17,8))\nax = df['state'].value_counts().plot(kind='bar')\nax.set_title(\"Accident count by state\")\nax.set_xlabel(\"State\")\nax.get_yaxis().set_major_formatter(\n    mpl.ticker.FuncFormatter(lambda x, p: format(int(x), ','))\n)\nfor tick in ax.get_xticklabels():\n    tick.set_rotation(0)","d749aee7":"plt.figure(figsize=(17,8))\nax = df['state'].value_counts().sort_values(ascending=True).tail(10).plot(kind='barh')\nax.set_title(\"Accidents by state - Top 10 states with most accidents\")\nax.set_ylabel(\"State\")\nax.get_xaxis().set_major_formatter(\n    mpl.ticker.FuncFormatter(lambda x, p: format(int(x), ','))\n)\nfor tick in ax.get_xticklabels():\n    tick.set_rotation(0)","2c205191":"plt.figure(figsize=(17,8))\nax = df['state'].value_counts().sort_values(ascending=True).head(10).plot(kind='barh')\nax.set_title(\"Accidents by state - Top 10 states with the least number of accidents\")\nax.set_ylabel(\"State\")\nax.get_xaxis().set_major_formatter(\n    mpl.ticker.FuncFormatter(lambda x, p: format(int(x), ','))\n)\nfor tick in ax.get_xticklabels():\n    tick.set_rotation(0)","2fbd8896":"plt.figure(figsize=(17,8))\nax = df['severity'].value_counts().plot(kind='bar')\nax.set_title(\"Accident count by severity\")\nax.set_xlabel(\"Severity\")\nax.get_yaxis().set_major_formatter(\n    mpl.ticker.FuncFormatter(lambda x, p: format(int(x), ','))\n)\nfor tick in ax.get_xticklabels():\n    tick.set_rotation(0)","c34f2c71":"df.groupby('state').agg(\n    accident_count=('id','count'),\n    mean_severity=('severity', 'mean'),\n    median_severity=('severity', 'median'),\n    std_severity=('severity', 'std')\n).sort_values(by=['mean_severity', 'accident_count'], ascending=[False, False]).head(10)","a149b0de":"plt.figure(figsize=(14,8))\nax = df.plot(kind='scatter', x='start_lng', y='start_lat', label='Severity', c='severity', cmap=plt.get_cmap('jet'), colorbar=True,alpha=0.4, figsize=(14,8))\nax.set_title(\"Severity distribution by location\")\n# ax.set_xlabel(\"Severity\")\nax.legend()\nplt.ioff()","15db27e3":"plt.figure(figsize=(17,8))\nax = df['start_hour'].value_counts().sort_index().plot(kind='bar')\nax.set_title(\"Accident starts at which hour?\")\nax.set_xlabel(\"Hour\")\nax.get_yaxis().set_major_formatter(\n    mpl.ticker.FuncFormatter(lambda x, p: format(int(x), ','))\n)\nfor tick in ax.get_xticklabels():\n    tick.set_rotation(0)","65a5ae39":"plt.figure(figsize=(17,8))\nax = df['start_dow'].value_counts().plot(kind='bar')\nax.set_title(\"Accident starts at which day of week?\")\nax.set_xlabel(\"Hour\")\nax.get_yaxis().set_major_formatter(\n    mpl.ticker.FuncFormatter(lambda x, p: format(int(x), ','))\n)\nfor tick in ax.get_xticklabels():\n    tick.set_rotation(0)","4ff729c9":"df.time_span_hour.describe()","75f8c542":"df[df.time_span_hour < 0].id.count()","33f280c1":"df = df[(df['time_span_hour'] > 0)]","c0d3c984":"df.query(\"time_span_hour < 24\")['id'].count() \/ df.id.count()","114698e7":"plt.figure(figsize=(14,8))\n# ax = df.query(\"time_span_hour <= 24\")['time_span_hour'].hist(bins=50)\nax = sns.kdeplot(df.query(\"time_span_hour <= 24\")['time_span_hour'], shade=True)\nax.set_title(\"Accident length (in hour) - Accidents that happened within 24 hours\")\nax.get_yaxis().set_major_formatter(\n    mpl.ticker.FuncFormatter(lambda x, p: format(int(x), ','))\n)\nfor tick in ax.get_xticklabels():\n    tick.set_rotation(0)","be6cba90":"plt.figure(figsize=(14,8))\n# ax = df.query(\"time_span_hour <= 24\")['time_span_hour'].hist(bins=50)\nax = sns.kdeplot(df.query(\"time_span_hour <= 2\")['time_span_minute'], shade=True)\nax.set_title(\"Accident length (in minute) - Accidents that happened within 2 hours\")\nax.get_yaxis().set_major_formatter(\n    mpl.ticker.FuncFormatter(lambda x, p: format(int(x), ','))\n)\nfor tick in ax.get_xticklabels():\n    tick.set_rotation(0)","e5da4d81":"plt.figure(figsize=(13,8))\nax = sns.boxplot(x='severity', y='time_span_hour', data=df[df['time_span_hour'] <= 24])\nax.set_title(\"Duration distribution by severity\")","da8282ce":"plt.figure(figsize=(13,8))\nax = df['timezone'].value_counts().plot(kind='bar')\nax.set_title(\"Accident count by timezone\")\nax.set_xlabel(\"Timezone\")\nax.get_yaxis().set_major_formatter(\n    mpl.ticker.FuncFormatter(lambda x, p: format(int(x), ','))\n)\nfor tick in ax.get_xticklabels():\n    tick.set_rotation(0)","a019ee63":"df['distance(mi)'] = df['distance(mi)'].astype('float')","bc470be5":"df['distance(mi)'].describe()","9b021ba7":"df['distance(mi)'].quantile(.99)","0195e91c":"plt.figure(figsize=(14,8))\n# ax = df.query(\"time_span_hour <= 24\")['time_span_hour'].hist(bins=50)\nax = sns.kdeplot(df[df['distance(mi)'] < 5]['distance(mi)'], shade=True)\nax.set_title(\"Length of road affected (in mile)\")\nax.get_yaxis().set_major_formatter(\n    mpl.ticker.FuncFormatter(lambda x, p: format(int(x), ','))\n)\nfor tick in ax.get_xticklabels():\n    tick.set_rotation(0)","1fd6a38e":"plt.figure(figsize=(13,8))\nax = sns.boxplot(x='severity', y='distance(mi)', data=df[df['distance(mi)'] < 5])\nax.set_title(\"Distance distribution by severity\")","c1322e29":"Let's look at severity by state also. Here is the table showing top 10 states with highest average severity","fe8dc703":"There are only 13 records, let's remove them for now.","4b9fec66":"# Exploratory data analysis on US Accidents dataset","47439c92":"### Correlation","d93723d6":"We can see that 99% of the distances affected are less than 5 mile. So we filter out all of the distances more than 5 before plotting the distribution","38f1b583":"### Length of road extent affected","a7d7e2d7":"Let's look at the accidents with lengths less than 24h, which account for 99.95% of the data","81e1fe70":"Can we look deeper into the duration (in minutes) ? Let's look at accidents that happened within 2 hours","78645036":"The nulls are described clearly in the dataset description.","fa6ef3dd":"So most accidents occured during weekdays. How about the duration?","ecfff895":"Let's look at the discrepancies - negative time span?","ffacf586":"### Severity\nHow about the accidents' severity","3a277179":"### Check for nulls, missing values","50c4d07b":"Define some util functions","e39eb5a5":"The distribution has peaks at 7am-8am and 4pm-5pm. How about the days of week","89edbf38":"### Time columns engineering\nFor further timeseries manipulation, we need columns representing datetime components","19f97596":"## EDA ","aa5bf740":"## Load libraries","a8342bd6":"We can see the accidents with low severity levels have smaller time span in general.","f4010d0f":"And let's also visualise the duration distribution in different severity levels","a2cf3cab":"### Timezone","042d3de8":"How the distance differs for different severity levels?","a27afc39":"## Load and preprocess the dataset","61e75765":"And here is the distribution of severity based on geolocation (inspired by [this kernel](https:\/\/www.kaggle.com\/biphili\/road-accidents-in-us)","ca61159a":"### Accident duration","8ec44bf8":"### Accident time\nLet's see when did the accidents normally occur","90481e66":"### Source distribution\nLet's look at the main sources of the accident reports","ee4c27b3":"### State distribution\nNext let's see the distribution of accidents by state"}}