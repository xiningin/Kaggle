{"cell_type":{"ce1250df":"code","c9b23621":"code","fc855137":"code","9f813916":"code","aa0e5442":"code","5a8855cb":"code","bf538611":"code","951878e0":"code","c5d0b842":"code","af1e87aa":"code","8629ef70":"code","048a402b":"code","6ea896af":"code","7bd2ec56":"code","2f51f6b1":"code","cc48501b":"code","850e7bf2":"code","c6657aac":"code","a34ec7c0":"code","9ebcf078":"code","fe6d7fd7":"code","126c6a8c":"code","f2a065ef":"code","056b89bf":"markdown","fd64eb74":"markdown","bb3b583a":"markdown","2abe262f":"markdown","693d8502":"markdown","a5923b26":"markdown"},"source":{"ce1250df":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c9b23621":"import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import datasets\nfrom sklearn import svm\n\nX, y = datasets.load_iris(return_X_y=True)","fc855137":"X_train, X_test, y_train, y_test = train_test_split(\n    X, y, \n    test_size=0.4, \n    random_state=0,\n)","9f813916":"clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\nclf.score(X_test, y_test)","aa0e5442":"from sklearn.model_selection import cross_val_score\nclf = svm.SVC(kernel='linear', C=1, random_state=42)\nscores = cross_val_score(clf, X, y, cv=5)\nscores","5a8855cb":"from sklearn.model_selection import ShuffleSplit\nn_samples = X.shape[0]\ncv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\ncross_val_score(clf, X, y, cv=cv)","bf538611":"def custom_cv_2folds(X):\n    n = X.shape[0]\n    i = 1\n    while i <= 2:\n        idx = np.arange(n * (i - 1) \/ 2, n * i \/ 2, dtype=int)\n        yield idx, idx\n        i += 1\n\ncustom_cv = custom_cv_2folds(X)\ncross_val_score(clf, X, y, cv=custom_cv)","951878e0":"import numpy as np\nfrom sklearn.model_selection import KFold\n\nX = [\"a\", \"b\", \"c\", \"d\"]\nkf = KFold(n_splits=2)\nfor train, test in kf.split(X):\n    print(\"%s %s\" % (train, test))","c5d0b842":"from sklearn.model_selection import LeaveOneOut\n\nX = [1, 2, 3, 4]\nloo = LeaveOneOut()\nfor train, test in loo.split(X):\n    print(\"%s %s\" % (train, test))","af1e87aa":"from sklearn.model_selection import LeavePOut\n\nX = np.ones(4)\nlpo = LeavePOut(p=2)\nfor train, test in lpo.split(X):\n    print(\"%s %s\" % (train, test))","8629ef70":"from sklearn.model_selection import ShuffleSplit\nX = np.arange(10)\nss = ShuffleSplit(n_splits=5, test_size=0.25, random_state=0)\nfor train_index, test_index in ss.split(X):\n    print(\"%s %s\" % (train_index, test_index))","048a402b":"from sklearn.model_selection import StratifiedKFold, KFold\nimport numpy as np\nX, y = np.ones((50, 1)), np.hstack(([0] * 45, [1] * 5))\nskf = StratifiedKFold(n_splits=3)\nfor train, test in skf.split(X, y):\n    print('train -  {}   |   test -  {}'.format(\n        np.bincount(y[train]), np.bincount(y[test])))\n\n\n\nkf = KFold(n_splits=3)\nfor train, test in kf.split(X, y):\n    print('train -  {}   |   test -  {}'.format(\n        np.bincount(y[train]), np.bincount(y[test])))","6ea896af":"from sklearn.model_selection import GroupKFold\n\nX = [0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 8.8, 9, 10]\ny = [\"a\", \"b\", \"b\", \"b\", \"c\", \"c\", \"c\", \"d\", \"d\", \"d\"]\ngroups = [1, 1, 1, 2, 2, 2, 3, 3, 3, 3]\n\ngkf = GroupKFold(n_splits=3)\nfor train, test in gkf.split(X, y, groups=groups):\n    print(\"%s %s\" % (train, test))","7bd2ec56":"!pip install -U sklearn","2f51f6b1":"from sklearn.model_selection import TimeSeriesSplit\n\nX = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\ny = np.array([1, 2, 3, 4, 5, 6])\ntscv = TimeSeriesSplit(n_splits=3)\nprint(tscv)\n\nfor train, test in tscv.split(X):\n    print(\"%s %s\" % (train, test))","cc48501b":"import xgboost as xgb\nfrom sklearn.model_selection import cross_val_score\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv('..\/input\/mercedes-benz-greener-manufacturing\/train.csv.zip')\ny_train = data['y']\nX_train = data.drop('y', axis=1).select_dtypes(include=[np.number])\ncross_val_score(\n    estimator=xgb.XGBRegressor(), \n    X=X_train, \n    y=y_train,\n    cv=5, \n    scoring='r2'\n)","850e7bf2":"import numpy as np\nfrom sklearn.model_selection import KFold, cross_val_score\nscores = np.array([])\nfor i in range(10):\n    fold = KFold(\n        n_splits=5, \n        shuffle=True, \n        random_state=i\n    )\n    scores_on_this_split = cross_val_score(\n        estimator=xgb.XGBRegressor(), \n        X=X_train, \n        y=y_train,\n        cv=fold, \n        scoring='r2'\n    )\n    scores = np.append(scores, scores_on_this_split)","c6657aac":"scores","a34ec7c0":"import xgboost as xgb\nfrom sklearn.model_selection import cross_val_score, KFold\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import ttest_rel\ndata = pd.read_csv('..\/input\/mercedes-benz-greener-manufacturing\/train.csv.zip')\ny_train = data['y']\nX_train = data.drop('y', axis=1).select_dtypes(include=[np.number])\nscores_100_trees = np.array([])\nscores_110_trees = np.array([])","9ebcf078":"for i in range(10):\n    fold = KFold(\n        n_splits=5, \n        shuffle=True, \n        random_state=i\n    )\n    scores_100_trees_on_this_split = cross_val_score(\n        estimator=xgb.XGBRegressor(\n            n_estimators=100\n        ),\n        X=X_train, \n        y=y_train,\n        cv=fold, \n        scoring='r2'\n    )\n    scores_100_trees = np.append(\n        scores_100_trees,\n        scores_100_trees_on_this_split\n    )\n    scores_110_trees_on_this_split = cross_val_score(\n        estimator=xgb.XGBRegressor(\n            n_estimators=110\n        ),\n        X=X_train, \n        y=y_train,\n        cv=fold, \n        scoring='r2'\n    )\n    scores_110_trees = np.append(\n        scores_110_trees,\n        scores_110_trees_on_this_split\n    )","fe6d7fd7":"scores_100_trees","126c6a8c":"scores_110_trees","f2a065ef":"ttest_rel(scores_100_trees, scores_110_trees)","056b89bf":"![](https:\/\/scikit-learn.org\/stable\/_images\/sphx_glr_plot_cv_indices_013.png)","fd64eb74":"![](https:\/\/scikit-learn.org\/stable\/_images\/sphx_glr_plot_cv_indices_007.png)","bb3b583a":"![](https:\/\/scikit-learn.org\/stable\/_images\/sphx_glr_plot_cv_indices_008.png)","2abe262f":"![](https:\/\/scikit-learn.org\/stable\/_images\/grid_search_cross_validation.png)","693d8502":"![](https:\/\/scikit-learn.org\/stable\/_images\/sphx_glr_plot_cv_indices_009.png)","a5923b26":"![](https:\/\/scikit-learn.org\/stable\/_images\/sphx_glr_plot_cv_indices_006.png)"}}