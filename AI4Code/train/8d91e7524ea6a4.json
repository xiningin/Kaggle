{"cell_type":{"0b5d23f7":"code","13497cd6":"code","f38e7cbd":"code","2c455539":"code","aad6146e":"code","65a97e2c":"code","b48adb89":"code","0494c7ff":"code","147839a5":"code","d8f4236d":"code","373557e1":"code","1cc2123d":"code","fcc0d2d3":"code","58440b0a":"code","602a9df0":"code","ead8330d":"code","fec65927":"code","f6a93c12":"code","b593bcf2":"code","48b4038d":"code","92d96897":"code","96aefde1":"code","6edf969c":"code","8f44e151":"code","e70b3608":"code","8064758d":"code","179b9eb8":"code","71e00fd5":"code","8bc4d34e":"code","e925b2b0":"code","73775668":"code","07081e10":"code","c00af37f":"code","0a12a9a6":"code","eed689e7":"code","116e261f":"code","3ca345b9":"code","72713d05":"code","2693e531":"code","eaf1416d":"code","30a0515a":"code","dcb307ec":"code","86926fe5":"code","181705f2":"code","5deb4922":"code","c088c860":"code","1d3aec1c":"code","185cad3d":"code","7a9ca5a3":"code","0e65200b":"code","89b8dd25":"code","f671a6d6":"code","260934dc":"code","50ea7cef":"code","002720e4":"code","5d87a1f5":"code","46d01168":"code","82c6a60f":"code","91e6b914":"code","a4318685":"markdown","2a98f3a5":"markdown","4b083b08":"markdown","29ffbe6b":"markdown","8e16c5c5":"markdown","3bf60590":"markdown","860a0374":"markdown","f94b8608":"markdown","c366f478":"markdown","a7ef05a6":"markdown","1bf8e182":"markdown","c3e43e90":"markdown","e78783bb":"markdown","3eb6aa96":"markdown","eb4fddf6":"markdown","fe4a024a":"markdown","d4b6e00d":"markdown","ac9f35a4":"markdown","7d80c9f9":"markdown","59aae2c8":"markdown","4b06af3f":"markdown","5343f015":"markdown","d4afb3ff":"markdown","94d24488":"markdown","179dec0c":"markdown","6d36fcd9":"markdown","1a6d288d":"markdown","92b9ea78":"markdown","86dbcd19":"markdown","6daaf62b":"markdown","1f436121":"markdown","f1db2bf8":"markdown","ae46c13e":"markdown","b306ec97":"markdown","175c498e":"markdown","190c68ce":"markdown","79bbf842":"markdown","82bf5550":"markdown","7a7c0676":"markdown","4d13d29f":"markdown","a24a2d90":"markdown","828d1391":"markdown","26100823":"markdown","85c05c62":"markdown","ff5ffd73":"markdown","06fa154c":"markdown","d603d7a7":"markdown","779354b8":"markdown","f4d2e184":"markdown","864cbb0b":"markdown","8be8ff56":"markdown","b42d5448":"markdown","eccd414a":"markdown","54d243c3":"markdown","15b6f786":"markdown","f8ee369b":"markdown","7da540ef":"markdown","2dffd89c":"markdown","6e9571fb":"markdown","78f57520":"markdown","a2a6e0c8":"markdown","6a1fd7a5":"markdown","10f3e239":"markdown","33c1760d":"markdown","e71bed2e":"markdown","2ccd205b":"markdown","efb9c229":"markdown","aeabba53":"markdown","06d31c41":"markdown","b154aef3":"markdown"},"source":{"0b5d23f7":"import numpy as np\nimport pandas as pd\npd.options.display.max_columns=None\nimport matplotlib.gridspec as gridspec\nfrom scipy.stats import skew\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nstyle.use('fivethirtyeight')\nimport scipy.stats as stats\nfrom scipy.stats import norm\nfrom sklearn.linear_model import LinearRegression, Ridge, SGDRegressor, ElasticNet\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\nimport matplotlib.style as style\nimport seaborn as sns\nimport missingno as msno\nimport warnings\nwarnings.filterwarnings('ignore')\nimport csv","13497cd6":"with open('..\/input\/house-prices-advanced-regression-techniques\/data_description.txt', 'r') as f:\n    desc = f.readlines()\n\ndesc = [line.strip() for line in desc]\ndesc","f38e7cbd":"with open('..\/input\/house-prices-advanced-regression-techniques\/train.csv', 'r', encoding='utf-8') as f:\n    train = pd.read_csv(f)\n    \ntrain.name = 'Train'\ntrain.head(10)","2c455539":"with open('..\/input\/house-prices-advanced-regression-techniques\/test.csv', 'r', encoding='utf-8') as f:\n    test = pd.read_csv(f)\n\ntest.name='Test'\ntest.head()","aad6146e":"def df_shape(df):\n    \"\"\"Function is to display the database name along with the number of rows and columns within it\"\"\"\n    print(f\"{df.name} has {df.shape[0]} rows and {df.shape[1]} columns\")\n    return None\n\ndf_shape(train)\ndf_shape(test)","65a97e2c":"def missing_values(df):\n    \"\"\"Function to make a missing matrix graph for the table and also to make a table to count the number and percent of missing values \n        in each column that has 1 or more missing values in them\"\"\"\n    msno.matrix(df)\n    plt.title(f\"{df.name} dataset missing values matrix\", size=30)\n    plt.show()\n\n    total = df.isnull().sum().sort_values(ascending=False)[df.isnull().sum().sort_values(ascending=False) > 0]\n    percent = np.round(df.isnull().sum().sort_values(ascending=False) \/ len(df) * 100, 2)[df.isnull().sum().sort_values(ascending=False) > 0]\n    print(f\"Missing values for {df.name} table:\")\n    return pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])","b48adb89":"missing_values(train)","0494c7ff":"missing_values(test)","147839a5":"train.SalePrice.describe()","d8f4236d":"def plot_3(df, var):\n    \"\"\"Function to make a plot 3 charts to show the distribution, probability plot and the box plot of a variable in a dataset\"\"\"\n    fig = plt.figure(constrained_layout=True, figsize=(12, 9))\n    grid = gridspec.GridSpec(nrows=2, ncols=3, figure=fig)\n\n    ax1 = fig.add_subplot(grid[0, :2])\n    sns.distplot(df[var], fit=norm, ax=ax1, norm_hist=True, color='red')\n    ax1.set_title('Distribution')\n    plt.tight_layout()\n\n    ax2 = fig.add_subplot(grid[1, :2])\n    stats.probplot(df[var], plot=ax2)\n    plt.tight_layout()\n\n    ax3 = fig.add_subplot(grid[:2, 2])\n    sns.boxplot(y=df[var], ax=ax3, color='red')\n    ax3.set_title('Price density')\n    plt.tight_layout()\n    plt.show()\n    \nplot_3(train, 'SalePrice')","373557e1":"print('Skewness (SalePrice): {}'.format(train.SalePrice.skew()))\nprint('Kurtosis (SalePrice): {}'.format(train.SalePrice.kurt()))","1cc2123d":"train_corr = train.corr()\ntrain_corr = train_corr.sort_values(by='SalePrice', ascending=False)\ntrain_corr['SalePrice'] # correlation with SalePrice","fcc0d2d3":"plt.figure(figsize=(12, 12))\nsns.heatmap(train_corr, cmap='coolwarm', square=True, center=0)\nplt.title('Train set correlations')\nplt.tight_layout()\nplt.show()","58440b0a":"def custom_scatter(df, x1, x2):\n    fig = plt.figure(figsize=(11, 8.5), constrained_layout=True)\n    grid = gridspec.GridSpec(nrows=2, ncols=4, figure=fig)\n    \n    ax = fig.add_subplot(grid[0, :])\n    sns.regplot(x=x1, y='SalePrice', scatter_kws={'s':5, 'color':'black'}, line_kws={'color':'red'}, data=df, ax=ax)\n    plt.title(f\"SalePrice\/{x1}\", size=14)\n    plt.tight_layout()\n    \n    ax1 = fig.add_subplot(grid[1, :])\n    sns.regplot(x=x2, y='SalePrice', scatter_kws={'s':5, 'color':'black'}, line_kws={'color':'red'}, data=df, ax=ax1)\n    plt.title(f\"SalePrice\/{x2}\", size=14)\n    plt.tight_layout()\n    plt.show()","602a9df0":"custom_scatter(train, 'GrLivArea', 'OverallQual')","ead8330d":"custom_scatter(train, 'GarageArea', 'TotalBsmtSF')","fec65927":"custom_scatter(train, '1stFlrSF', 'YearBuilt')","f6a93c12":"fig, (ax1, ax2) = plt.subplots(figsize = (12,8), ncols=2, sharey=False)\nsns.regplot(x=train.GrLivArea, y=train.SalePrice, ax=ax1, color='black') # line of best linear fit plot\nplt.tight_layout()\n\nsns.regplot(x=train.MasVnrArea, y=train.SalePrice, ax=ax2, color='black')\nplt.tight_layout()","b593bcf2":"pretransformed_grlivarea = list(train.GrLivArea)\npretransformed_saleprice = list(train.SalePrice)\n\nfig, (ax1, ax2) = plt.subplots(figsize=(12, 6), ncols=2)\nsns.regplot(x=pretransformed_grlivarea, y=pretransformed_saleprice, color='black', ax=ax1)\nsns.residplot(x=pretransformed_grlivarea, y=pretransformed_saleprice, color='black', ax=ax2)\nplt.show()","48b4038d":"plot_3(train, 'SalePrice')","92d96897":"train.SalePrice = np.log(train.SalePrice) # log convert all the SalePrice in the train dataset\ntrain.SalePrice[:5]","96aefde1":"plot_3(train, 'SalePrice')","6edf969c":"fig = plt.figure(figsize=(12, 9), constrained_layout=True)\ngrid = gridspec.GridSpec(nrows=4, ncols=4, figure=fig)\n\nax1 = fig.add_subplot(grid[:2, :2])\nsns.regplot(x=pretransformed_grlivarea, y=pretransformed_saleprice, color='blue')\nax1.set_title('Pre log transform')\nax1.set_xlabel(xlabel='GrLivArea')\nax1.set_ylabel(ylabel='SalePrice')\nplt.tight_layout()\n\nax2 = fig.add_subplot(grid[:2, 2:])\nsns.residplot(x=pretransformed_grlivarea, y=pretransformed_saleprice, color='blue')\nax2.set_title('Pre log transform')\nax2.set_xlabel(xlabel='GrLivArea')\nax2.set_ylabel(ylabel='SalePrice')\nplt.tight_layout()\n\nax3 = fig.add_subplot(grid[2:, :2])\nsns.regplot(x='GrLivArea', y='SalePrice', data=train, color='red')\nax3.set_title('Post log transform')\nplt.tight_layout()\n\nax4 = fig.add_subplot(grid[2:, 2:])\nsns.residplot(x='GrLivArea', y='SalePrice', data=train, color='red')\nax4.set_title('Post log transform')\nplt.tight_layout()","8f44e151":"plt.figure(figsize=(30, 20))\nsns.heatmap(train.corr(), annot=True, cmap='coolwarm', fmt='.2f', annot_kws={'size':15}, center=0);\nplt.title('Heatmap of all features', size=20)\nplt.show()","e70b3608":"train.corr()['SalePrice'].sort_values(ascending=False)[:6]","8064758d":"sns.regplot(x='GrLivArea', y='SalePrice', data=train, color='black')\nplt.show()","179b9eb8":"train = train.drop(train[train.index== 1298].index) # removing the top 2 highest GrLivArea rows\ntrain = train.drop(train[train.index== 523].index)\nsns.regplot(x='GrLivArea', y='SalePrice', data=train, color='black')\nplt.show()","71e00fd5":"sns.scatterplot(x='GarageCars', y='SalePrice', data=train, color='black')\nplt.show()","8bc4d34e":"train = train.drop(train[train.GarageCars == 4.0].index)\nsns.scatterplot(x='GarageCars', y='SalePrice', data=train, color='black')\nplt.show()","e925b2b0":"train.drop(columns=['Id'], axis=1, inplace=True) # dropping the column ID from both databases\ntest.drop(columns=['Id'], axis=1, inplace=True)\n\ny = train['SalePrice'].reset_index(drop=True) # saving the SalePrice entries\ny","73775668":"orig_train = train.copy()\norig_train.SalePrice = orig_train.SalePrice.transform(lambda x: np.exp(x))\norig_train.head() # making a copy of the original train dataset","07081e10":"all_data = pd.concat((train, test)).reset_index(drop=True)\nall_data.drop('SalePrice', axis=1, inplace=True)\nall_data.name = 'All Data'\nall_data.head()","c00af37f":"missing_values(all_data)","0a12a9a6":"missing_val_col = [\"Alley\", \n                   \"PoolQC\", \n                   \"MiscFeature\",\n                   \"Fence\",\n                   \"FireplaceQu\",\n                   \"GarageType\",\n                   \"GarageFinish\",\n                   \"GarageQual\",\n                   \"GarageCond\",\n                   'BsmtQual',\n                   'BsmtCond',\n                   'BsmtExposure',\n                   'BsmtFinType1',\n                   'BsmtFinType2',\n                   'MasVnrType'] # dependent features\n\nfor col in missing_val_col:\n    all_data[col] = all_data[col].fillna('None')","eed689e7":"# filling empty MSZoning values with the most common value in each MSSubClass grouped unique value\nall_data.MSZoning = all_data.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))","116e261f":"all_data['Functional'] = all_data['Functional'].fillna('Typ') \nall_data['Utilities'] = all_data['Utilities'].fillna('AllPub') \nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0]) \nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\nall_data['KitchenQual'] = all_data['KitchenQual'].fillna(\"TA\") \nall_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\nall_data['Electrical'] = all_data['Electrical'].fillna(\"SBrkr\") ","3ca345b9":"missing_values(all_data)","72713d05":"more_cols = ['LotFrontage', \n                    'GarageYrBlt', \n                    'MasVnrArea', \n                    'BsmtFullBath', \n                    'BsmtHalfBath', \n                    'BsmtFinSF2', \n                    'GarageCars', \n                    'GarageArea', \n                    'BsmtUnfSF', \n                    'BsmtFinSF1', \n                    'TotalBsmtSF']\n\nfor col in more_cols:\n    all_data[col] = all_data[col].fillna(0)","2693e531":"missing_values(all_data)","eaf1416d":"all_data.shape","30a0515a":"all_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['TotalBsmtSF'] = all_data['TotalBsmtSF'].astype(float)\nall_data['MoSold'] = all_data['MoSold'].astype(str)\nall_data['MSSubClass'] = all_data['MSSubClass'].astype(str)","dcb307ec":"numeric_features = all_data.dtypes[all_data.dtypes != 'object'].index\nskewed_feats = all_data[numeric_features].apply(lambda x: skew(x)).sort_values(ascending=False)\nskewed_feats","86926fe5":"def fix_skew(df):\n    \"\"\"Function to take in a dataframe and log convert all numerical features with a positive skewness\"\"\"\n    numeric_feats = all_data.dtypes[all_data.dtypes != 'object'].index # getting the only the numerical data type features\n    skew_feats = df[numeric_feats].apply(lambda x: skew(x)).sort_values(ascending=False)\n    high_skew = skew_feats[skew_feats > 0.5].index\n    \n    for col in high_skew:\n        df[col] = np.log(df[col] + 1) # adding 1 to get approriate log values for 0\n        \n    return df\n\nfix_skew(all_data)","181705f2":"plot_3(all_data, '1stFlrSF') # checking the distribution after log transformation","5deb4922":"all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\nall_data['YrBltAndRemod'] = all_data.YearBuilt + all_data.YearRemodAdd\nall_data['TotalBathrooms'] = all_data.BsmtFullBath + (0.5 * all_data.BsmtHalfBath) + all_data.FullBath + (0.5* all_data.HalfBath)\nall_data['Total_sqr_footage'] = all_data['BsmtFinSF1'] + all_data['BsmtFinSF2'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\nall_data['Total_porch_sf'] = (all_data['OpenPorchSF'] + all_data['3SsnPorch'] + all_data['EnclosedPorch']\n                                              + all_data['ScreenPorch'] + all_data['WoodDeckSF'])","c088c860":"all_data['HasPool'] = all_data['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nall_data['Has2ndFlr'] = all_data['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nall_data['HasGarage'] = all_data['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nall_data['HasBsmt'] = all_data['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nall_data['HasFireplace'] = all_data['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\nall_data.shape","1d3aec1c":"all_data.drop(['PoolQC', 'Utilities', 'Street'], axis=1, inplace=True)\nall_data.shape","185cad3d":"X  = all_data.loc[:len(y) - 1  , :]\nX_sub = all_data.loc[len(y):, :]\nX.shape, X_sub.shape, y.shape","7a9ca5a3":"def reduce_overfitting(df):\n    \"\"\"Function to calculate the amount of 0s in a column and drop any columns in which 99.94 percent of the values are 0\"\"\"\n    overfit = []\n    \n    for col in df.columns:\n        zero_count = df[col].value_counts().iloc[0]\n        if zero_count \/ len(df) * 100 > 99.94:\n            overfit.append(col)\n    \n    overfit = list(overfit)\n    return overfit       \n        \noverfitted_features = reduce_overfitting(X)\nX.drop(overfitted_features, axis=1, inplace=True)\nX_sub.drop(overfitted_features, axis=1, inplace=True)\nX.shape,y.shape, X_sub.shape","0e65200b":"cat_cols = X.dtypes[X.dtypes == 'object'].index\n\nfor col in cat_cols:\n    encoder = LabelEncoder().fit(X[col])\n    X[col] = encoder.transform(X[col])\n    \nfor col in cat_cols:\n    encoder = LabelEncoder().fit(X_sub[col])\n    X_sub[col] = encoder.transform(X_sub[col])\n\nX.head()","89b8dd25":" X_train, X_test, y_train, y_test = train_test_split(X, y.values, test_size=.05, random_state=0)\n\nX_train.shape, y_train.shape, X_test.shape, y_test.shape","f671a6d6":"scaler = StandardScaler().fit(X_train)\n\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\nX_train[0]","260934dc":"high_score = 0\nhigh_alpha = 0\n\nfor x in range(0, 101):\n    rid = Ridge(alpha=x).fit(X_train, y_train)\n    score = rid.score(X_test, y_test)\n    if score > high_score:\n        high_score = score\n        high_alpha = x\n\nprint('Ridge Regression:')\nprint(f'Highest score = {np.round(high_score * 100, 2)}% \\nWith alpha = {high_alpha} ')","50ea7cef":"high_score = 0\nhigh_alpha = 0\n\nfor x in range(0, 101):\n    sgd = SGDRegressor(alpha=x, shuffle=False).fit(X_train, y_train)\n    score = sgd.score(X_test, y_test)\n    if score > high_score:\n        high_score = score\n        high_alpha = x\n    else:\n        continue\n\nprint('SGDRegressor:')\nprint(f'Highest score = {np.round(high_score * 100, 2)}% \\nWith alpha = {high_alpha} ')","002720e4":"X_sub = StandardScaler().fit_transform(X_sub)\nX_sub","5d87a1f5":"sgd = SGDRegressor(alpha=0, shuffle=False).fit(X_train, y_train)\npredicts = sgd.predict(X_sub)\npredicts = [np.round(np.exp(x), 2) for x in predicts] # undoing the log transformation previously \npredicts[:5]","46d01168":"len(predicts)","82c6a60f":"rows = list(zip(range(1461, 1461 + len(predicts)), predicts))\nrows = [list(x) for x in rows]\nrows[:5]","91e6b914":"#with open('submission.csv', 'w', newline='') as csvfile:\n    #writer = csv.writer(csvfile)\n    #writer.writerow(['ID', 'SalePrice'])\n    #writer.writerows(rows)","a4318685":"### Multivariate Normality (Normality of Errors)\nThe linear regression analysis requires the dependent variable to be multivariate normally distributed. A histogram, box plot, or a Q-Q-Plot can check if the target variable is normally distributed. The goodness of fit test, e.g., the Kolmogorov-Smirnov test can check for normality in the dependent variable. We already know that our target variable does not follow a normal distribution. Let's bring back the three charts to show our target variable.","2a98f3a5":"Skewness\n\n* is the degree of distortion from the symmetrical bell curve or the normal curve.\n* So, a symmetrical distribution will have a skewness of \"0\".\n* There are two types of Skewness: Positive and Negative.\n* Positive Skewness(similar to our target variable distribution) means the tail on the right side of the distribution is longer and fatter.\n* In positive Skewness the mean and median will be greater than the mode similar to this dataset. Which means more houses were sold by less than the average price.\n* Negative Skewness means the tail on the left side of the distribution is longer and fatter.\n* In negative Skewness the mean and median will be less than the mode.\n* Skewness differentiates in extreme values in one versus the other tail.\n\n\n![Skewness](https:\/\/cdn-images-1.medium.com\/max\/1600\/1*nj-Ch3AUFmkd0JUSOW_bTQ.jpeg)","4b083b08":"Now that the distribution for all numerical features is normalized. Time to create some new features\n\n## Creating new features","29ffbe6b":"This is a newer version of the previous notebook that is based on the same goal, to predict the house prices, by using the same databases. However, in this notebook the aim is to apply a deeper and more detailed understanding of how to analyze and clean up the data. While also making more intricate regression models.\n\n[Inspiration](https:\/\/www.kaggle.com\/masumrumi\/a-detailed-regression-guide-with-house-pricing\/comments)","8e16c5c5":"## Kurtosis","3bf60590":"## Splitting the data","860a0374":"##### IMOPRTANT TO ALWAYS LOOK AT THE DATABASE AND KEEP A KEEN EYE ON THE FEATURES AND THEIR DEPENDENCIES","f94b8608":"## Rules to follow when making a Linear Regression Model ","c366f478":"### Filling the rest\n\n##### The rest of the columns will be filled with their most common value or with their own respective value to indicate a lack of that particular feature","a7ef05a6":"# Importing libraries ","1bf8e182":"In this case, GrLivArea has a better linear relationship with SalePrice than MasVnrArea and SalePrice. Another **very important** thing, to check for outliers as the have a big detrimental effect to making an accurate linear regression model, we can alos use a residual plot to check the error variance across the true line. We can check the residual plot for SalePrice and GrLivArea","c3e43e90":"### Test ","e78783bb":"## Missing values","3eb6aa96":"As seen from the heatmap, you can see that there is multicollinearity in multiple variables, **though instead of cleaning them now, we can let the models do that for us (Regularisation models)**. We can see many features having good relationships with other independent features.\n(However, **it is best to only keep the features that have a good relationship with the target variable**)","eb4fddf6":"### OverallQual & GrLivArea","fe4a024a":"After the log transformation, the normality errors for SalePrice are now fixed, let's check the variance via residual plots now.","d4b6e00d":"# Fin :)","ac9f35a4":"* **Linearity (Correct functional form)**\n* **Homoscedasticity (Constant Error Variance vs Heteroscedasticity).**\n* **Independence of Errors vs Autocorrelation)**\n* **Multivariate Normality (Normality of Errors)**\n* **No or little Multicollinearity.**","7d80c9f9":"#### GrLivArea","59aae2c8":"## Seeing the highest correlations on a graph ","4b06af3f":"### MSSubClass and MSZoning\n\n##### Taking the most common MSZoning value in each group of MSSubClass using that to fill empty spots","5343f015":"# Column description ","d4afb3ff":"## Skewness","94d24488":"# Skewness and Kurtosis ","179dec0c":"As we can see from the above residual plot, the graph is in the shape of a cone, the variance increasing as the values increase, this is known as **heteroscedasticity**. This variance should be controlled a little, instead of having a cone shape, the shape of the variance should be more rectangular, this is **homoscedasticity**. The variance. \n\n### Homoscedasticity\n\n**Homoscedasticity ( Constant Variance )**: The assumption of Homoscedasticity is crucial to linear regression models. Homoscedasticity describes a situation in which the error term or variance or the \"noise\" or random disturbance in the relationship between the independent variables and the dependent variable is the same across all values of the independent variable. In other words, there is a constant variance present in the response variable as the predictor variable increases. If the \"noise\" is not the same across the values of an independent variable like the residual plot above, we call that Heteroscedasticity. As you can tell, it is the opposite of Homoscedasticity.","6d36fcd9":"## Building a model ","1a6d288d":"## Fixing data types of categorical features ","92b9ea78":"### Dependent features \n\n##### Looked through the features list and theire descriptions, the chosen columns were chosen because their value entirely depends on another relevant feature ","86dbcd19":"# Feature Engineering ","6daaf62b":"## Specific SalePrice correlations ","1f436121":"# Modeling the Data \n\n## Label encoding the data","f1db2bf8":"# Concluding the results \n\n## Scaling the test values ","ae46c13e":"## Fixing Skewness ","b306ec97":"# Train and test file opens ","175c498e":"#### GarageCars","190c68ce":"Dropping the Id columns on both databases","79bbf842":"# Handling missing values \n\n## Missing Matrix\n### Train ","82bf5550":"## Splitting train and test ","7a7c0676":"## Reducing overfitting ","4d13d29f":"### SGDRegressor ","a24a2d90":"### Linearity \nLinearity(Correct functional form): Linear regression needs the relationship between each independent variable and the dependent variable to be linear. The linearity assumption can be tested with scatter plots. The following two examples depict two cases, where no or little linearity is present.","828d1391":"Now for situations like this, simply log transforming the variable values will fix the distribution.","26100823":"3 models were implement in a very basic manner and their highest scores were noted down.","85c05c62":"## Filling in remaining empty values ","ff5ffd73":"Let's focus on SalePrice, which is the target variable, and make basic observations on it.","06fa154c":"### GarageArea & TotalBsmtSF","d603d7a7":"## Combining train and test ","779354b8":"### Inspecting outliers and removing them ","f4d2e184":"More features were made for the train and test set while trying to reduce overfitting","864cbb0b":"### No or Little multicollinearity\n\nMulticollinearity is when there is a strong correlation between independent variables. Linear regression or multilinear regression requires independent variables to have little or no similar features. Multicollinearity can lead to a variety of problems, including:\n\n* The effect of predictor variables estimated by our regression will depend on what other variables are included in our model.\n* Predictors can have wildly different results depending on the observations in our sample, and small changes in samples can result in very different estimated effects.\n* With very high multicollinearity, the inverse matrix, the computer calculates may not be accurate.\n* We can no longer interpret a coefficient on a variable as the effect on the target of a one-unit increase in that variable holding the other variables constant. The reason behind that is, when predictors are strongly correlated, there is not a scenario in which one variable can change without a conditional change in another variable.\n\nWe can use a heatmap to look for multicollinearity ","8be8ff56":"## Scaling the data","b42d5448":"##### There are no more empty values","eccd414a":"## Dropping Features ","54d243c3":"## Describing the datasets ","15b6f786":"## Model making and predicting","f8ee369b":"# Correlations ","7da540ef":"## Summary ","2dffd89c":"Taking rows with a skewness value of > 0.5 and applying log transformation to them.\n**Applying log transformation works to fix the distribution as long as the feature's skewness is positive**. We are only going to apply log transformation to non categorical features","6e9571fb":"* There is **heteroscedasticity** for both graphs in the pre log transformation graphs. The conical shape of the graph is still present.\n\n\n\n* However, there is **homoscedasticity** for both graphs in the post log transformation graphs, there is less variance as the values go up and the linear relationships are more tight than before.\n    \n    ","78f57520":"### 1stFlrSF & YearBuilt ","a2a6e0c8":"In probability theory and statistics, Kurtosis is the measure of the \"tailedness\" of the probability. distribution of a real-valued random variable. So, In other words, it is the measure of the extreme values(outliers) present in the distribution.\n\n* There are three types of Kurtosis: Mesokurtic, Leptokurtic, and Platykurtic.\n* Mesokurtic is similar to the normal curve with the standard value of 3. This means that the extreme values of this distribution are similar to that of a normal distribution.\n* Leptokurtic Example of leptokurtic distributions are the T-distributions with small degrees of freedom.\n* Platykurtic: Platykurtic describes a particular statistical distribution with thinner tails than a normal distribution. Because this distribution has thin tails, it has fewer outliers (e.g., extreme values three or more standard deviations from the mean) than do mesokurtic and leptokurtic distributions.\n\n\n![Kurtosis](attachment:image.png)","6a1fd7a5":"These charts tell us a couple of things:\n   \n   * The SalePrice is not normally distributed\n   * SalePrice is right skewed\n   * And there are quite a few outliers in SalePrice\n   * As the year increases, the price also increases","10f3e239":"### RidgeRegression ","33c1760d":"# Introduction ","e71bed2e":"## Submission file","2ccd205b":"## Observations \n* SalePrice shows an unequal level of variance across most predictor variables\n* There are quite a few outliers that don't necessarily follow the general trend","efb9c229":"# Target Variable ","aeabba53":"* From the missing matrix graph and the missing values tables, we can see that quite a few features have missing values, some a lot more than others.\n\n* There are many types of features","06d31c41":"Now to look at the distributions of SalePrice","b154aef3":"# Formatting data "}}