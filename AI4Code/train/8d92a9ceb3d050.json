{"cell_type":{"5cfd6c0a":"code","90046612":"code","a5dac0db":"code","1a209d0d":"code","829d9302":"code","9c42b9ba":"code","7903282f":"code","59f84618":"code","dcd8169a":"code","10b4a992":"code","19a60e4c":"code","884d0aac":"code","2a9c3f69":"code","61b6c06e":"code","5918fdfa":"code","f4d1d3e4":"code","09cd84df":"code","88dcb912":"code","6cd8dfea":"code","54a99eec":"code","0eb4448e":"code","93646061":"code","c4120bc5":"code","0a04f867":"code","edc0be48":"code","4ff2e412":"code","5940a8ff":"code","64ef6eae":"code","d3c346e7":"code","9760520c":"code","5d8cf64d":"code","3c9270e6":"code","59892c88":"code","d3ebf295":"code","8228e4fb":"code","3a73d34b":"code","33f97c0c":"code","48ed5fcf":"code","c5d392ca":"code","cdcd56e5":"code","c48ff855":"code","cd85e37e":"code","2f18d40d":"code","5b5cdd91":"code","46a58f93":"code","b109d3d3":"code","071e7c25":"code","c4e6aaaa":"code","2e8bcfd3":"code","a4816825":"code","d9257c52":"code","ba7962d5":"code","2a2742d9":"code","a65af657":"code","cc5d38f6":"code","5ee26591":"code","1b3c278a":"code","9ad6b4e7":"code","68a7865f":"code","d67d9ffc":"markdown","90965e70":"markdown","01f64a0c":"markdown","40a0bb34":"markdown","52776a37":"markdown","7cb22b49":"markdown","06c93d09":"markdown","77481386":"markdown","92d12ec5":"markdown","593488ff":"markdown","2cd9da86":"markdown","0aac6a55":"markdown","53b9813a":"markdown","ec453013":"markdown","8fe08e58":"markdown"},"source":{"5cfd6c0a":"# Importing few libraries\nimport os\nimport shutil\nimport random\nfrom tqdm import tqdm\n\nimport numpy as np\nimport pandas as pd\n\nimport PIL\nimport seaborn as sns\nimport matplotlib.pyplot as plt","90046612":"DATASET = \"..\/input\/2750\"\n\nLABELS = os.listdir(DATASET)\nprint(LABELS)","a5dac0db":"# plot class distributions of whole dataset\ncounts = {}\n\nfor l in LABELS:\n    counts[l] = len(os.listdir(os.path.join(DATASET, l)))\n\n    \nplt.figure(figsize=(12, 6))\n\nplt.bar(range(len(counts)), list(counts.values()), align='center')\nplt.xticks(range(len(counts)), list(counts.keys()), fontsize=12, rotation=40)\nplt.xlabel('class label', fontsize=13)\nplt.ylabel('class size', fontsize=13)\nplt.title('EUROSAT Class Distribution', fontsize=15);","1a209d0d":"img_paths = [os.path.join(DATASET, l, l+'_1000.jpg') for l in LABELS]\n\nimg_paths = img_paths + [os.path.join(DATASET, l, l+'_2000.jpg') for l in LABELS]\n\ndef plot_sat_imgs(paths):\n    plt.figure(figsize=(15, 8))\n    for i in range(20):\n        plt.subplot(4, 5, i+1, xticks=[], yticks=[])\n        img = PIL.Image.open(paths[i], 'r')\n        plt.imshow(np.asarray(img))\n        plt.title(paths[i].split('\/')[-2])\n\nplot_sat_imgs(img_paths)","829d9302":"import re\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom keras.preprocessing.image import ImageDataGenerator\n\nTRAIN_DIR = '..\/working\/training'\nTEST_DIR = '..\/working\/testing'\nBATCH_SIZE = 64\nNUM_CLASSES=len(LABELS)\nINPUT_SHAPE = (64, 64, 3)\nCLASS_MODE = 'categorical'\n\n# create training and testing directories\nfor path in (TRAIN_DIR, TEST_DIR):\n    if not os.path.exists(path):\n        os.mkdir(path)\n\n# create class label subdirectories in train and test\nfor l in LABELS:\n    \n    if not os.path.exists(os.path.join(TRAIN_DIR, l)):\n        os.mkdir(os.path.join(TRAIN_DIR, l))\n\n    if not os.path.exists(os.path.join(TEST_DIR, l)):\n        os.mkdir(os.path.join(TEST_DIR, l))","9c42b9ba":"# map each image path to their class label in 'data'\ndata = {}\n\nfor l in LABELS:\n    for img in os.listdir(DATASET+'\/'+l):\n        data.update({os.path.join(DATASET, l, img): l})\n\nX = pd.Series(list(data.keys()))\ny = pd.get_dummies(pd.Series(data.values()))\n\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=69)\n\n# split the list of image paths\nfor train_idx, test_idx in split.split(X, y):\n    \n    train_paths = X[train_idx]\n    test_paths = X[test_idx]\n\n    # define a new path for each image depending on training or testing\n    new_train_paths = [re.sub('\\.\\.\\\/input\\\/2750', '..\/working\/training', i) for i in train_paths]\n    new_test_paths = [re.sub('\\.\\.\\\/input\\\/2750', '..\/working\/testing', i) for i in test_paths]\n\n    train_path_map = list((zip(train_paths, new_train_paths)))\n    test_path_map = list((zip(test_paths, new_test_paths)))\n    \n    # move the files\n    print(\"moving training files..\")\n    for i in tqdm(train_path_map):\n        if not os.path.exists(i[1]):\n            if not os.path.exists(re.sub('training', 'testing', i[1])):\n                shutil.copy(i[0], i[1])\n    \n    print(\"moving testing files..\")\n    for i in tqdm(test_path_map):\n        if not os.path.exists(i[1]):\n            if not os.path.exists(re.sub('training', 'testing', i[1])):\n                shutil.copy(i[0], i[1])","7903282f":"# Create a ImageDataGenerator Instance which can be used for data augmentation\n\ntrain_gen = ImageDataGenerator(\n    rescale=1.\/255,\n    rotation_range=60,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    vertical_flip = True\n#   validation_split=0.2\n)\n\ntrain_generator = train_gen.flow_from_directory(\n    directory=TRAIN_DIR,\n    target_size=(64, 64),\n    batch_size=BATCH_SIZE,\n    class_mode=CLASS_MODE,\n    #subset='training',\n    color_mode='rgb',\n    shuffle=True,\n    seed=69\n)\n# The validation set is optional if we choose to do that\n\"\"\"\nvalid_generator = train_gen.flow_from_directory(\n    directory=TRAIN_DIR,\n    target_size=(64, 64),\n    batch_size=BATCH_SIZE,\n    class_mode=CLASS_MODE,\n    subset='validation',    \n    color_mode='rgb',\n    shuffle=True,\n    seed=69\n)\n\"\"\"\n# test generator for evaluation purposes with no augmentations, just rescaling\ntest_gen = ImageDataGenerator(\n    rescale=1.\/255,\n)\n\ntest_generator = test_gen.flow_from_directory(\n    directory=TEST_DIR,\n    target_size=(64, 64),\n    batch_size=BATCH_SIZE,\n    class_mode=CLASS_MODE,\n    color_mode='rgb',\n    shuffle=False,\n    seed=69\n)","59f84618":"print(train_generator.class_indices)","dcd8169a":"np.save('class_indices', train_generator.class_indices)","10b4a992":"# Using image data generator api in keras for making image dataset \nrf_gen = ImageDataGenerator(rescale=1.\/255)\n\nrf_train_generator = rf_gen.flow_from_directory(\n    directory=TRAIN_DIR,\n    target_size=(64, 64),\n    # by using batch_size as training data size we can extract data from this iterator\n    batch_size=21600, \n    class_mode=CLASS_MODE,\n    color_mode='rgb',\n    shuffle=False,\n    seed=7\n)\n\nrf_test_generator = rf_gen.flow_from_directory(\n    directory=TEST_DIR,\n    target_size=(64, 64),\n    batch_size=5400,\n    class_mode=CLASS_MODE,\n    color_mode='rgb',\n    shuffle=False,\n    seed=7\n)","19a60e4c":"train = rf_train_generator.next()\nX_train = train[0].reshape(21600, 12288)\ny_train = train[1]\n\ntest = rf_test_generator.next()\nX_test = test[0].reshape(5400, 12288)\ny_test = test[1]","884d0aac":"# Creating mini batches of training data as training on whole data is difficult\nids_1 = np.random.choice(len(X_train), size=21600, replace=False)\nX_train_mini_1 = X_train[ids_1]\ny_train_mini_1 = rf_train_generator.classes[ids_1]","2a9c3f69":"# import random forest classifier\nfrom sklearn import ensemble\nfrom sklearn.metrics import accuracy_score\n\n\nrf_clf = ensemble.RandomForestClassifier(n_estimators=20, n_jobs=-1, random_state=7)\nrf_clf.fit(X_train_mini_1, y_train_mini_1)\ny_pred = rf_clf.predict(X_test)\nacc = accuracy_score(y_pred, rf_test_generator.classes)\nprint(\"Accuracy Score: {0:.4}\".format(acc))","61b6c06e":"from __future__ import division\nfrom collections import Counter\nimport random\nimport numpy as np\nfrom scipy.stats import mode\n\n\ndef shuffle_in_unison(a, b):\n    \"\"\" Shuffles two lists of equal length and keeps corresponding elements in the same index. \"\"\"\n    rng_state = np.random.get_state()\n    np.random.shuffle(a)\n    np.random.set_state(rng_state)\n    np.random.shuffle(b)\n\n\ndef entropy(Y):\n    \"\"\" In information theory, entropy is a measure of the uncertanty of a random sample from a group. \"\"\"\n    \n    distribution = Counter(Y)\n    s = 0.0\n    total = len(Y)\n    for y, num_y in distribution.items():\n        probability_y = (num_y\/total)\n        s += (probability_y)*np.log(probability_y)\n    return -s\n\n\ndef information_gain(y, y_true, y_false):\n    \"\"\" The reduction in entropy from splitting data into two groups. \"\"\"\n    return entropy(y) - (entropy(y_true)*len(y_true) + entropy(y_false)*len(y_false))\/len(y)","5918fdfa":"# Implementing a Decision Tree Classifier\n\nclass DecisionTreeClassifier(object):\n    \"\"\" A decision tree classifier.\n    A decision tree is a structure in which each node represents a binary\n    conditional decision on a specific feature, each branch represents the\n    outcome of the decision, and each leaf node represents a final\n    classification.\n    \"\"\"\n\n    def __init__(self, max_features=lambda x: x, max_depth=10,\n                    min_samples_split=2):\n        \"\"\"\n        Args:\n            max_features: A function that controls the number of features to\n                randomly consider at each split. The argument will be the number\n                of features in the data.\n            max_depth: The maximum number of levels the tree can grow downwards\n                before forcefully becoming a leaf.\n            min_samples_split: The minimum number of samples needed at a node to\n                justify a new node split.\n        \"\"\"\n\n        self.max_features = max_features\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split\n\n\n    def fit(self, X, y):\n        \"\"\" Builds the tree by chooseing decision rules for each node based on\n        the data. \"\"\"\n\n        n_features = X.shape[1]\n        n_sub_features = int(self.max_features(n_features))\n        feature_indices = random.sample(range(n_features), n_sub_features)\n        \n        self.trunk = self.build_tree(X, y, feature_indices, 0)\n\n\n    def predict(self, X):\n        \"\"\" Predict the class of each sample in X. \"\"\"\n\n        num_samples = X.shape[0]\n        y = np.empty(num_samples)\n        for j in range(num_samples):\n            node = self.trunk\n\n            while isinstance(node, Node):\n                if X[j][node.feature_index] <= node.threshold:\n                    node = node.branch_true\n                else:\n                    node = node.branch_false\n            y[j] = node\n\n        return y\n\n\n    def build_tree(self, X, y, feature_indices, depth):\n        \"\"\" Recursivly builds a decision tree. \"\"\"\n\n        if depth is self.max_depth or len(y) < self.min_samples_split or entropy(y) is 0:\n            return mode(y)[0][0]\n        \n        feature_index, threshold = find_split(X, y, feature_indices)\n\n        X_true, y_true, X_false, y_false = split(X, y, feature_index, threshold)\n        if y_true.shape[0] is 0 or y_false.shape[0] is 0:\n            return mode(y)[0][0]\n        \n        branch_true = self.build_tree(X_true, y_true, feature_indices, depth + 1)\n        branch_false = self.build_tree(X_false, y_false, feature_indices, depth + 1)\n\n        return Node(feature_index, threshold, branch_true, branch_false)\n\n\ndef find_split(X, y, feature_indices):\n    \"\"\" Returns the best split rule for a tree node. \"\"\"\n\n    num_features = X.shape[1]\n\n    best_gain = 0\n    best_feature_index = 0\n    best_threshold = 0\n    for feature_index in feature_indices:\n        values = sorted(set(X[:, feature_index])) ### better way\n\n        for j in range(len(values) - 1):\n            threshold = (values[j] + values[j+1])\/2\n            X_true, y_true, X_false, y_false = split(X, y, feature_index, threshold)\n            gain = information_gain(y, y_true, y_false)\n\n            if gain > best_gain:\n                best_gain = gain\n                best_feature_index = feature_index\n                best_threshold = threshold\n\n    return best_feature_index, best_threshold\n\n\nclass Node(object):\n    \"\"\" A node in a decision tree with the binary condition xi <= t. \"\"\"\n\n    def __init__(self, feature_index, threshold, branch_true, branch_false):\n        self.feature_index = feature_index\n        self.threshold = threshold\n        self.branch_true = branch_true\n        self.branch_false = branch_false\n\n\ndef split(X, y, feature_index, threshold):\n    \"\"\" Splits X and y based on the binary condition xi <= threshold. \"\"\"\n\n    X_true = []\n    y_true = []\n    X_false = []\n    y_false = []\n\n    for j in range(len(y)):\n        if X[j][feature_index] <= threshold:\n            X_true.append(X[j])\n            y_true.append(y[j])\n        else:\n            X_false.append(X[j])\n            y_false.append(y[j])\n\n    X_true = np.array(X_true)\n    y_true = np.array(y_true)\n    X_false = np.array(X_false)\n    y_false = np.array(y_false)\n\n    return X_true, y_true, X_false, y_false","f4d1d3e4":"# Implementing Random Forest Clasifier\n\nclass RandomForestClassifier(object):\n    \"\"\" A random forest classifier.\n    A random forest is a collection of decision trees that vote on a\n    classification decision. Each tree is trained with a subset of the data and\n    features.\n    \"\"\"\n\n    def __init__(self, n_estimators=32, max_features=np.sqrt, max_depth=10,\n        min_samples_split=2, bootstrap=0.9):\n        \"\"\"\n        Args:\n            n_estimators: The number of decision trees in the forest.\n            max_features: Controls the number of features to randomly consider\n                at each split.\n            max_depth: The maximum number of levels that the tree can grow\n                downwards before forcefully becoming a leaf.\n            min_samples_split: The minimum number of samples needed at a node to\n                justify a new node split.\n            bootstrap: The fraction of randomly choosen data to fit each tree on.\n        \"\"\"\n        self.n_estimators = n_estimators\n        self.max_features = max_features\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split\n        self.bootstrap = bootstrap\n        self.forest = []\n\n\n    def fit(self, X, y):\n        \"\"\" Creates a forest of decision trees using a random subset of data and\n            features. \"\"\"\n        self.forest = []\n        n_samples = len(y)\n        n_sub_samples = round(n_samples*self.bootstrap)\n        \n        for i in range(self.n_estimators):\n            shuffle_in_unison(X, y)\n            X_subset = X[:n_sub_samples]\n            y_subset = y[:n_sub_samples]\n\n            tree = DecisionTreeClassifier(self.max_features, self.max_depth,\n                                            self.min_samples_split)\n            tree.fit(X_subset, y_subset)\n            self.forest.append(tree)\n\n\n    def predict(self, X):\n        \"\"\" Predict the class of each sample in X. \"\"\"\n        n_samples = X.shape[0]\n        n_trees = len(self.forest)\n        predictions = np.empty([n_trees, n_samples])\n        for i in range(n_trees):\n            predictions[i] = self.forest[i].predict(X)\n\n        return mode(predictions)[0][0]\n\n\n    def score(self, X, y):\n        \"\"\" Return the accuracy of the prediction of X compared to y. \"\"\"\n        y_predict = self.predict(X)\n        n_samples = len(y)\n        correct = 0\n        for i in range(n_samples):\n            if y_predict[i] == y[i]:\n                correct = correct + 1\n        accuracy = correct\/n_samples\n        return accuracy","09cd84df":"# Creating mini batches of training data as training on whole data is difficult\nids_2 = np.random.choice(len(X_train), size=2160, replace=False)\nX_train_mini_2 = X_train[ids_2]\ny_train_mini_2 = rf_train_generator.classes[ids_2]","88dcb912":"forest = RandomForestClassifier(n_estimators=2)\nforest.fit(X_train_mini_2, y_train_mini_2)","6cd8dfea":"accuracy = forest.score(X_test, rf_test_generator.classes)\nprint(\"Accuracy Score: {0:.4}\".format(accuracy))","54a99eec":"import tensorflow as tf\nfrom keras.models import Model\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom keras.optimizers import Adam\n\n\nfrom keras.applications import VGG16, VGG19\nfrom keras.applications import ResNet50, ResNet50V2, ResNet152V2\nfrom keras.applications import InceptionV3, Xception\n\nfrom sklearn.metrics import precision_recall_fscore_support, confusion_matrix, fbeta_score, accuracy_score","0eb4448e":"gpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n  # Restrict TensorFlow to only use the first GPU\n  try:\n    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")    \n  except RuntimeError as e:\n    # Visible devices must be set before GPUs have been initialized\n    print(e)\n    \ntf.config.set_soft_device_placement(True)","93646061":"# Note that for different CNN models we are using different setup of dense layers\ndef compile_model(cnn_base, input_shape, n_classes, optimizer, fine_tune=None):\n    \n    if (cnn_base == 'ResNet50') or (cnn_base == 'ResNet50V2') or (cnn_base == 'ResNet152V2'):\n        if cnn_base == 'ResNet50':\n            conv_base = ResNet50(include_top=False,\n                                 weights='imagenet', \n                                 input_shape=input_shape)\n        elif cnn_base == 'ResNet50V2':\n            conv_base = ResNet50V2(include_top=False,\n                                 weights='imagenet', \n                                 input_shape=input_shape)\n        else:\n            conv_base = ResNet152V2(include_top=False,\n                                 weights='imagenet', \n                                 input_shape=input_shape)\n        top_model = conv_base.output\n        top_model = Flatten()(top_model)\n        top_model = Dense(2048, activation='relu')(top_model)\n        top_model = Dropout(0.2)(top_model)\n       \n    \n    elif (cnn_base == 'VGG16') or (cnn_base == 'VGG19'):\n        if cnn_base == 'VGG16':\n            conv_base = VGG16(include_top=False,\n                              weights='imagenet', \n                              input_shape=input_shape)\n        else:\n            conv_base = VGG19(include_top=False,\n                              weights='imagenet', \n                              input_shape=input_shape)\n        top_model = conv_base.output\n        top_model = Flatten()(top_model)\n        top_model = Dense(2048, activation='relu')(top_model)\n        top_model = Dropout(0.2)(top_model)\n        top_model = Dense(2048, activation='relu')(top_model)\n        top_model = Dropout(0.2)(top_model)\n    \n    \n    output_layer = Dense(n_classes, activation='softmax')(top_model)\n    \n    model = Model(inputs=conv_base.input, outputs=output_layer)\n        \n    if type(fine_tune) == int:\n        for layer in conv_base.layers[fine_tune:]:\n            layer.trainable = True\n    else:\n        for layer in conv_base.layers:\n            layer.trainable = False\n\n    model.compile(optimizer=optimizer, loss='categorical_crossentropy',\n                 metrics=['categorical_accuracy'])\n    \n    return model\n\ndef plot_history(history):\n       \n    acc = history.history['categorical_accuracy']\n    val_acc = history.history['val_categorical_accuracy']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    \n    plt.figure(figsize=(10, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(acc)\n    plt.plot(val_acc)\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'val'], loc='upper left')\n    \n    plt.subplot(1, 2, 2)\n    plt.plot(loss)\n    plt.plot(val_loss)\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'val'], loc='upper left')\n    \n    plt.show();\n\ndef display_results(y_true, y_preds, class_labels):\n    \n    results = pd.DataFrame(precision_recall_fscore_support(y_true, y_preds),\n                          columns=class_labels).T\n    results.rename(columns={0: 'Precision',\n                           1: 'Recall',\n                           2: 'F-Score',\n                           3: 'Support'}, inplace=True)\n    \n    conf_mat = pd.DataFrame(confusion_matrix(y_true, y_preds), \n                            columns=class_labels,\n                            index=class_labels)    \n    f2 = fbeta_score(y_true, y_preds, beta=2, average='micro')\n    accuracy = accuracy_score(y_true, y_preds)\n    print(f\"Accuracy: {accuracy}\")\n    print(f\"Global F2 Score: {f2}\")    \n    return results, conf_mat\n\ndef plot_predictions(y_true, y_preds, test_generator, class_indices):\n\n    fig = plt.figure(figsize=(20, 10))\n    for i, idx in enumerate(np.random.choice(test_generator.samples, size=20, replace=False)):\n        ax = fig.add_subplot(4, 5, i + 1, xticks=[], yticks=[])\n        ax.imshow(np.squeeze(test_generator[idx]))\n        pred_idx = np.argmax(y_preds[idx])\n        true_idx = y_true[idx]\n                \n        plt.tight_layout()\n        ax.set_title(\"{}\\n({})\".format(class_indices[pred_idx], class_indices[true_idx]),\n                     color=(\"green\" if pred_idx == true_idx else \"red\"))    ","c4120bc5":"N_STEPS = train_generator.samples\/\/BATCH_SIZE\nN_VAL_STEPS = test_generator.samples\/\/BATCH_SIZE\nN_EPOCHS = 100\n\n# model callbacks\ncheckpoint = ModelCheckpoint(filepath='..\/working\/model.weights.best.hdf5',\n                        monitor='val_categorical_accuracy',\n                        save_best_only=True,\n                        verbose=1)\n\nearly_stop = EarlyStopping(monitor='val_categorical_accuracy',\n                           patience=10,\n                           restore_best_weights=True,\n                           mode='max')\n\nreduce_lr = ReduceLROnPlateau(monitor='val_categorical_accuracy', factor=0.5,\n                              patience=3, min_lr=0.00001)","0a04f867":"resnet50_model = compile_model('ResNet50', INPUT_SHAPE, NUM_CLASSES, Adam(lr=1e-2), fine_tune=None)\nresnet50_model.summary()","edc0be48":"train_generator.reset()\ntest_generator.reset()\n\nN_STEPS = train_generator.samples\/\/BATCH_SIZE\nN_VAL_STEPS = test_generator.samples\/\/BATCH_SIZE\nN_EPOCHS = 100\n\n# model callbacks\ncheckpoint = ModelCheckpoint(filepath='..\/working\/model.weights.best.hdf5',\n                        monitor='val_categorical_accuracy',\n                        save_best_only=True,\n                        verbose=1)\n\nearly_stop = EarlyStopping(monitor='val_categorical_accuracy',\n                           patience=10,\n                           restore_best_weights=True,\n                           mode='max')\n\nreduce_lr = ReduceLROnPlateau(monitor='val_categorical_accuracy', factor=0.5,\n                              patience=3, min_lr=0.00001)","4ff2e412":"# First Pretraining the dense layer\nresnet50_history = resnet50_model.fit_generator(train_generator,\n                             steps_per_epoch=N_STEPS,\n                             epochs=50,\n                             callbacks=[early_stop, checkpoint],\n                             validation_data=test_generator,\n                             validation_steps=N_VAL_STEPS)","5940a8ff":"# re-train whole network end2end \nresnet50_model = compile_model('ResNet50', INPUT_SHAPE, NUM_CLASSES, Adam(lr=1e-4), fine_tune=0)\n\nresnet50_model.load_weights('..\/working\/model.weights.best.hdf5')\n\ntrain_generator.reset()\ntest_generator.reset()\n\nresnet50_history = resnet50_model.fit_generator(train_generator,\n                             steps_per_epoch=N_STEPS,\n                             epochs=N_EPOCHS,\n                             callbacks=[early_stop, checkpoint, reduce_lr],\n                             validation_data=test_generator,\n                             validation_steps=N_VAL_STEPS)","64ef6eae":"plot_history(resnet50_history)","d3c346e7":"resnet50_model.load_weights('..\/working\/model.weights.best.hdf5')\n\nclass_indices = train_generator.class_indices\nclass_indices = dict((v,k) for k,v in class_indices.items())\n\ntest_generator_new = test_gen.flow_from_directory(\n    directory=TEST_DIR,\n    target_size=(64, 64),\n    batch_size=1,\n    class_mode=None,\n    color_mode='rgb',\n    shuffle=False,\n    seed=69\n)\n\npredictions = resnet50_model.predict_generator(test_generator_new, steps=len(test_generator_new.filenames))\npredicted_classes = np.argmax(np.rint(predictions), axis=1)\ntrue_classes = test_generator_new.classes\n\nprf, conf_mat = display_results(true_classes, predicted_classes, class_indices.values())\nprf","9760520c":"# Save the model and the weights\nresnet50_model.save('..\/working\/ResNet50_eurosat.h5')","5d8cf64d":"resnet50V2_model = compile_model('ResNet50V2', INPUT_SHAPE, NUM_CLASSES, Adam(lr=1e-2), fine_tune=None)\nresnet50V2_model.summary()","3c9270e6":"train_generator.reset()\ntest_generator.reset()\n\nN_STEPS = train_generator.samples\/\/BATCH_SIZE\nN_VAL_STEPS = test_generator.samples\/\/BATCH_SIZE\nN_EPOCHS = 100\n\n# model callbacks\ncheckpoint = ModelCheckpoint(filepath='..\/working\/model.weights.best.hdf5',\n                        monitor='val_categorical_accuracy',\n                        save_best_only=True,\n                        verbose=1)\n\nearly_stop = EarlyStopping(monitor='val_categorical_accuracy',\n                           patience=10,\n                           restore_best_weights=True,\n                           mode='max')\n\nreduce_lr = ReduceLROnPlateau(monitor='val_categorical_accuracy', factor=0.5,\n                              patience=3, min_lr=0.00001)","59892c88":"# First Pretraining the dense layer\nresnet50V2_history = resnet50V2_model.fit_generator(train_generator,\n                             steps_per_epoch=N_STEPS,\n                             epochs=50,\n                             callbacks=[early_stop, checkpoint],\n                             validation_data=test_generator,\n                             validation_steps=N_VAL_STEPS)","d3ebf295":"# re-train whole network end2end \nresnet50V2_model = compile_model('ResNet50V2', INPUT_SHAPE, NUM_CLASSES, Adam(lr=1e-4), fine_tune=0)\n\nresnet50V2_model.load_weights('..\/working\/model.weights.best.hdf5')\n\ntrain_generator.reset()\ntest_generator.reset()\n\nresnet50V2_history = resnet50V2_model.fit_generator(train_generator,\n                             steps_per_epoch=N_STEPS,\n                             epochs=N_EPOCHS,\n                             callbacks=[early_stop, checkpoint, reduce_lr],\n                             validation_data=test_generator,\n                             validation_steps=N_VAL_STEPS)","8228e4fb":"plot_history(resnet50V2_history)","3a73d34b":"resnet50V2_model.load_weights('..\/working\/model.weights.best.hdf5')\n\nclass_indices = train_generator.class_indices\nclass_indices = dict((v,k) for k,v in class_indices.items())\n\ntest_generator_new = test_gen.flow_from_directory(\n    directory=TEST_DIR,\n    target_size=(64, 64),\n    batch_size=1,\n    class_mode=None,\n    color_mode='rgb',\n    shuffle=False,\n    seed=69\n)\n\npredictions = resnet50V2_model.predict_generator(test_generator_new, steps=len(test_generator_new.filenames))\npredicted_classes = np.argmax(np.rint(predictions), axis=1)\ntrue_classes = test_generator_new.classes\n\nprf, conf_mat = display_results(true_classes, predicted_classes, class_indices.values())\nprf","33f97c0c":"# Save the model and the weights\nresnet50V2_model.save('..\/working\/ResNet50V2_eurosat.h5')","48ed5fcf":"resnet152V2_model = compile_model('ResNet152V2', INPUT_SHAPE, NUM_CLASSES, Adam(lr=1e-2), fine_tune=None)\nresnet152V2_model.summary()","c5d392ca":"train_generator.reset()\ntest_generator.reset()\n\nN_STEPS = train_generator.samples\/\/BATCH_SIZE\nN_VAL_STEPS = test_generator.samples\/\/BATCH_SIZE\nN_EPOCHS = 100\n\n# model callbacks\ncheckpoint = ModelCheckpoint(filepath='..\/working\/model.weights.best.hdf5',\n                        monitor='val_categorical_accuracy',\n                        save_best_only=True,\n                        verbose=1)\n\nearly_stop = EarlyStopping(monitor='val_categorical_accuracy',\n                           patience=10,\n                           restore_best_weights=True,\n                           mode='max')\n\nreduce_lr = ReduceLROnPlateau(monitor='val_categorical_accuracy', factor=0.5,\n                              patience=3, min_lr=0.00001)","cdcd56e5":"# First Pretraining the dense layer\nresnet152V2_history = resnet152V2_model.fit_generator(train_generator,\n                             steps_per_epoch=N_STEPS,\n                             epochs=50,\n                             callbacks=[early_stop, checkpoint],\n                             validation_data=test_generator,\n                             validation_steps=N_VAL_STEPS)","c48ff855":"# re-train whole network end2end \nresnet152V2_model = compile_model('ResNet152V2', INPUT_SHAPE, NUM_CLASSES, Adam(lr=1e-4), fine_tune=0)\n\nresnet152V2_model.load_weights('..\/working\/model.weights.best.hdf5')\n\ntrain_generator.reset()\ntest_generator.reset()\n\nresnet152V2_history = resnet152V2_model.fit_generator(train_generator,\n                             steps_per_epoch=N_STEPS,\n                             epochs=N_EPOCHS,\n                             callbacks=[early_stop, checkpoint, reduce_lr],\n                             validation_data=test_generator,\n                             validation_steps=N_VAL_STEPS)","cd85e37e":"plot_history(resnet152V2_history)","2f18d40d":"resnet152V2_model.load_weights('..\/working\/model.weights.best.hdf5')\n\nclass_indices = train_generator.class_indices\nclass_indices = dict((v,k) for k,v in class_indices.items())\n\ntest_generator_new = test_gen.flow_from_directory(\n    directory=TEST_DIR,\n    target_size=(64, 64),\n    batch_size=1,\n    class_mode=None,\n    color_mode='rgb',\n    shuffle=False,\n    seed=69\n)\n\npredictions = resnet152V2_model.predict_generator(test_generator_new, steps=len(test_generator_new.filenames))\npredicted_classes = np.argmax(np.rint(predictions), axis=1)\ntrue_classes = test_generator_new.classes\n\nprf, conf_mat = display_results(true_classes, predicted_classes, class_indices.values())\nprf","5b5cdd91":"# Save the model and the weights\nresnet152V2_model.save('..\/working\/ResNet152V2_eurosat.h5')","46a58f93":"vgg16_model = compile_model('VGG16', INPUT_SHAPE, NUM_CLASSES, Adam(lr=1e-2), fine_tune=None)\nvgg16_model.summary()","b109d3d3":"train_generator.reset()\ntest_generator.reset()\n\nN_STEPS = train_generator.samples\/\/BATCH_SIZE\nN_VAL_STEPS = test_generator.samples\/\/BATCH_SIZE\nN_EPOCHS = 100\n\n# model callbacks\ncheckpoint = ModelCheckpoint(filepath='..\/working\/model.weights.best.hdf5',\n                        monitor='val_categorical_accuracy',\n                        save_best_only=True,\n                        verbose=1)\n\nearly_stop = EarlyStopping(monitor='val_categorical_accuracy',\n                           patience=10,\n                           restore_best_weights=True,\n                           mode='max')\n\nreduce_lr = ReduceLROnPlateau(monitor='val_categorical_accuracy', factor=0.5,\n                              patience=3, min_lr=0.00001)","071e7c25":"train_generator.reset()\n# First Pretraining the dense layer\nvgg16_history = vgg16_model.fit_generator(train_generator,\n                             steps_per_epoch=N_STEPS,\n                             epochs=50,\n                             callbacks=[early_stop, checkpoint],\n                             validation_data=test_generator,\n                             validation_steps=N_VAL_STEPS)","c4e6aaaa":"# re-train whole network end2end \nvgg16_model = compile_model('VGG16', INPUT_SHAPE, NUM_CLASSES, Adam(lr=1e-4), fine_tune=0)\n\nvgg16_model.load_weights('..\/working\/model.weights.best.hdf5')\n\ntrain_generator.reset()\ntest_generator.reset()\n\nvgg16_history = vgg16_model.fit_generator(train_generator,\n                             steps_per_epoch=N_STEPS,\n                             epochs=N_EPOCHS,\n                             callbacks=[early_stop, checkpoint, reduce_lr],\n                             validation_data=test_generator,\n                             validation_steps=N_VAL_STEPS)","2e8bcfd3":"plot_history(vgg16_history)","a4816825":"vgg16_model.load_weights('..\/working\/model.weights.best.hdf5')\n\nclass_indices = train_generator.class_indices\nclass_indices = dict((v,k) for k,v in class_indices.items())\n\ntest_generator_new = test_gen.flow_from_directory(\n    directory=TEST_DIR,\n    target_size=(64, 64),\n    batch_size=1,\n    class_mode=None,\n    color_mode='rgb',\n    shuffle=False,\n    seed=69\n)\n\npredictions = vgg16_model.predict_generator(test_generator_new, steps=len(test_generator_new.filenames))\npredicted_classes = np.argmax(np.rint(predictions), axis=1)\ntrue_classes = test_generator_new.classes\n\nprf, conf_mat = display_results(true_classes, predicted_classes, class_indices.values())\nprf","d9257c52":"# Save the model and the weights\nvgg16_model.save('..\/working\/vgg16_eurosat.h5')","ba7962d5":"vgg19_model = compile_model('VGG19', INPUT_SHAPE, NUM_CLASSES, Adam(lr=1e-2), fine_tune=None)\nvgg19_model.summary()","2a2742d9":"train_generator.reset()\ntest_generator.reset()\n\nN_STEPS = train_generator.samples\/\/BATCH_SIZE\nN_VAL_STEPS = test_generator.samples\/\/BATCH_SIZE\nN_EPOCHS = 100\n\n# model callbacks\ncheckpoint = ModelCheckpoint(filepath='..\/working\/model.weights.best.hdf5',\n                        monitor='val_categorical_accuracy',\n                        save_best_only=True,\n                        verbose=1)\n\nearly_stop = EarlyStopping(monitor='val_categorical_accuracy',\n                           patience=10,\n                           restore_best_weights=True,\n                           mode='max')\n\nreduce_lr = ReduceLROnPlateau(monitor='val_categorical_accuracy', factor=0.5,\n                              patience=3, min_lr=0.00001)","a65af657":"train_generator.reset()\n# First Pretraining the dense layer\nvgg19_history = vgg19_model.fit_generator(train_generator,\n                             steps_per_epoch=N_STEPS,\n                             epochs=50,\n                             callbacks=[early_stop, checkpoint],\n                             validation_data=test_generator,\n                             validation_steps=N_VAL_STEPS)","cc5d38f6":"# re-train whole network end2end \nvgg19_model = compile_model('VGG19', INPUT_SHAPE, NUM_CLASSES, Adam(lr=1e-4), fine_tune=0)\n\nvgg19_model.load_weights('..\/working\/model.weights.best.hdf5')\n\ntrain_generator.reset()\ntest_generator.reset()\n\nvgg19_history = vgg19_model.fit_generator(train_generator,\n                             steps_per_epoch=N_STEPS,\n                             epochs=N_EPOCHS,\n                             callbacks=[early_stop, checkpoint, reduce_lr],\n                             validation_data=test_generator,\n                             validation_steps=N_VAL_STEPS)","5ee26591":"plot_history(vgg19_history)","1b3c278a":"vgg19_model.load_weights('..\/working\/model.weights.best.hdf5')\n\nclass_indices = train_generator.class_indices\nclass_indices = dict((v,k) for k,v in class_indices.items())\n\ntest_generator_new = test_gen.flow_from_directory(\n    directory=TEST_DIR,\n    target_size=(64, 64),\n    batch_size=1,\n    class_mode=None,\n    color_mode='rgb',\n    shuffle=False,\n    seed=69\n)\n\npredictions = vgg19_model.predict_generator(test_generator_new, steps=len(test_generator_new.filenames))\npredicted_classes = np.argmax(np.rint(predictions), axis=1)\ntrue_classes = test_generator_new.classes\n\nprf, conf_mat = display_results(true_classes, predicted_classes, class_indices.values())\nprf","9ad6b4e7":"# Save the model and the weights\nvgg19_model.save('..\/working\/vgg19_eurosat.h5')","68a7865f":"plot_predictions(true_classes, predictions, test_generator_new, class_indices)","d67d9ffc":"# Land-Cover Classification with EuroSAT Dataset\n\n\n![](https:\/\/raw.githubusercontent.com\/phelber\/EuroSAT\/master\/eurosat_overview_small.jpg) \n\n","90965e70":"## 4.5 VGG19 Model","01f64a0c":"## 4.3 ResNet152V2 Model","40a0bb34":"In this project, I'll be exploring the EUROSAT dataset. The EUROSAT dataset is composed of images taken from the Sentinel-2 satellite. This dataset lists images of the earth's surface into 10 different land cover labels. For this project, I will build an image classification model for predicting a land cover label, given an image. ","52776a37":"## 4.1 ResNet50 Model","7cb22b49":"I'd like to evaluate the performance of the model later on after training, so I'll perform a stratified shuffle-split using Scikit-learn to maintain class proportions. 30% of the dataset will be held for evaluation purposes. I'll be loading my data into the Keras model using the ImageDataGenerator class. I'll need the images to be in their own respective land cover directories. \n\nAfter splitting the dataset, I'll create some image augmentations using the generator and also denote a subset of the training data to be used as validation data during training. ","06c93d09":"The dataset is split into 10 classes of land cover. Each class varies in size, so I'll have to stratify later on when splitting the data into training, testing and validation sets. ","77481386":"## 4.2 ResNet50V2 Model","92d12ec5":"# II. Preprocessing","593488ff":"# IV. Deep Learning For Image Classification\n\nDeep Learning has highly influenced the field of computer vision when Convolutional Neural Networks (CNN) models were used in tasks like image classification, object detection, facial recognition etc. As discussed by authors of EuroSAT paper many deep learning models outperform the traditional non deep learning methods by a large margin. \n\nIn this section we will train many state of the art architectures which performed well on the ILSVRC challenge. The authors achieved an accuracy of 98.57% using a fine tuned ResNet50 model. Here, I will try to employ a similar strategy for training the models where initially the CNN part of the model will be frozen with imagenet weights and dense layers will be trained with a high learning rate of 0.01 and later we will train the whole model end-to-end i.e. fine tune by keeping a small learning rate between 0.001 to 0.0001\n\nBefore we start here is a list of CNN models which we will train:\n1. ResNet50\n2. ResNet50V2\n3. ResNet152V2\n3. VGG16\n4. VGG19\n\nIn future\n5. InceptionV3\n6. Xception","2cd9da86":"# III. Machine Learning for Image Classification\n\nFirst, we will see how a machine learning model performs by directly feeding it the whole image pixels (64x64x3 = 12288). From our 80-20 split on the 27000 samples dataset the training data is of size 21600 and test data is of size 5400. Now, here I will use some trick to utilize keras ImageDataGenerator such that we can obtain the image dataset as a numpy array which can be used by a machine learning model for training and testing. \n\nNow, here I will test Random Forest Classifier. First by using the direct implementation provided by scikit-learn and other implemented from scratch. This notebook is a part of my course project because of which I have included this additional implementation from scratch. Feel free to check it out or you can skip over it if you want.\n","0aac6a55":"## 4.4 VGG16 Model","53b9813a":"### 2. Random Forest Implementation from scratch\nThe dataset size is quite large (21600x12288).With scikit-learn's implementation of Random Forest we were barely able to train the model. The implementation from scratch is however not that efficient so we will only provide it with a fraction of training data. Still it can be observed that it produces reasonable accuracy considering for a 10 class classification problem a random guessing approach gives accuracy of 10%","ec453013":"# I. Data Exploration","8fe08e58":"### 1. Random Forest Scikit-Learn Implementation"}}