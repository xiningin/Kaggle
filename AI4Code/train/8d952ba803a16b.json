{"cell_type":{"fc427492":"code","854cb40d":"code","8b989ef1":"code","314992ae":"code","e2f7a239":"code","edc3fe14":"code","bf115d24":"code","3952e209":"code","4b454771":"code","d8d97ced":"code","b6378bd1":"code","1abf96b0":"code","0c03f0d2":"code","2f626bae":"code","ad1aebd3":"code","b88d56d7":"code","42f2911b":"code","b4020ec7":"code","79384cad":"code","76668a7c":"code","cbbc3934":"code","01fe50f2":"code","b1b13342":"code","e906486c":"code","4d48885a":"code","3485f2f2":"markdown","46270631":"markdown","dcc793df":"markdown","20fc5de8":"markdown"},"source":{"fc427492":"import numpy as np\nimport pandas as pd\n\nnp.random.seed(2019)\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nimport keras\nfrom keras import regularizers\nfrom keras.applications.vgg16 import VGG16\nfrom keras.models import Model\nfrom keras.layers import Dense, Dropout, Flatten\n\n\nimport os\nfrom tqdm import tqdm\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nimport cv2\nfrom keras.preprocessing.image import ImageDataGenerator","854cb40d":"df_train = pd.read_csv('..\/input\/dog-breed-identification\/labels.csv') # arquivo com os nomes das imagens e as suas ra\u00e7as para o treino\ndf_test = pd.read_csv('..\/input\/dog-breed-identification\/sample_submission.csv') # arquivo com os nomes das imagens de teste","8b989ef1":"df_train.head()","314992ae":"df_test.head()","e2f7a239":"targets_series = pd.Series(df_train['breed'])\none_hot = pd.get_dummies(targets_series, sparse = True)","edc3fe14":"one_hot_labels = np.asarray(one_hot)","bf115d24":"im_size = 197","3952e209":"x_train = []\ny_train = []\nx_test = []","4b454771":"i = 0 \nfor f, breed in tqdm(df_train.values):\n    img = cv2.imread('..\/input\/dog-breed-identification\/train\/{}.jpg'.format(f))\n    x_train.append(cv2.resize(img, (im_size, im_size)))\n    label = one_hot_labels[i]\n    y_train.append(label)\n    i += 1","d8d97ced":"# delete df_train to decrease memory usage\ndel df_train","b6378bd1":"for f in tqdm(df_test['id'].values):\n    img = cv2.imread('..\/input\/dog-breed-identification\/test\/{}.jpg'.format(f))\n    x_test.append(cv2.resize(img, (im_size, im_size)))","1abf96b0":"num_class = 120","0c03f0d2":"X_train, X_valid, Y_train, Y_valid = train_test_split(x_train, y_train,shuffle=True,  test_size=0.2, random_state=1)","2f626bae":"datagen = ImageDataGenerator(width_shift_range=0.2,\n                            height_shift_range=0.2,\n                            zoom_range=0.2,\n                            rotation_range=30,\n                            vertical_flip=False,\n                            horizontal_flip=True)\n\ndatagen.fit(X_train)","ad1aebd3":"train_generator = datagen.flow(np.array(X_train), np.array(Y_train), \n                               batch_size=32) ","b88d56d7":"def create_my_model(use_regularizer, optimizer):\n    base_model = VGG16(weights=\"..\/input\/keras-pretrained-models\/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\",include_top=False, input_shape=(im_size, im_size, 3))\n    dropout = base_model.output\n    dropout = Dropout(0.5)(dropout)\n    model_with_dropout = Model(inputs=base_model.input, outputs=dropout)\n        \n    x = base_model.output\n    x = Flatten()(x)\n    predictions = Dense(num_class, activation='softmax', kernel_regularizer=regularizers.l2(0.0015), activity_regularizer=regularizers.l1(0.0015))(x)\n    \n    my_model = Model(inputs=base_model.input, outputs=predictions)\n    \n#     for layer in my_model.layers:\n#         layer.treinable = False\n    \n    my_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n    return my_model\n","42f2911b":"def gerar_grafico(historia, titulo):\n    plt.plot(historia.history['acc'])\n    plt.plot(historia.history['val_acc'])\n    plt.title('Accuracy ' + titulo)\n    plt.ylabel('Accuracy')\n    plt.xlabel('Times')\n    plt.legend(['training', 'validation'], loc='upper left')\n    plt.show()\n    plt.plot(historia.history['loss'])\n    plt.plot(historia.history['val_loss'])\n    plt.title('Loss ' + titulo)\n    plt.ylabel('Loss')\n    plt.xlabel('Times')\n    plt.legend(['training', 'validation'], loc='upper left')\n    plt.show()","b4020ec7":"model_rmsprop_com_regularizador = create_my_model(use_regularizer=True, optimizer='rmsprop')\nmodel_sgd_com_regularizador = create_my_model(use_regularizer=True, optimizer='sgd')","79384cad":"history_rmsprop_com_regularizador = model_rmsprop_com_regularizador.fit_generator(\n    train_generator,\n    epochs=10, steps_per_epoch=len(X_train) \/ 18, #len(X_train) \/ 18,\n    validation_data=(np.array(X_train), np.array(Y_train)), validation_steps=len(X_valid) \/ 18)\n\npreds = model_rmsprop_com_regularizador.predict(np.array(x_test), verbose=1)\n\ngerar_grafico(history_rmsprop_com_regularizador, \n              \"VGG16 with RMSprop\")\n\nsub = pd.DataFrame(preds)\ncol_names = one_hot.columns.values\nsub.columns = col_names\nsub.insert(0, 'id', df_test['id'])\nsub.head(5)\n\nsub.to_csv(\"predictions_vgg16_with_RMSProp.csv\")\n\nmodel_rmsprop_com_regularizador.save('vgg16_with_RMSProp.h5')\n","76668a7c":"history_sgd_com_regularizador = model_sgd_com_regularizador.fit_generator(\n    train_generator,\n    epochs=10, steps_per_epoch=len(X_train) \/ 18, #len(X_train) \/ 18,\n    validation_data=(np.array(X_train), np.array(Y_train)), validation_steps=len(X_valid) \/ 18)\n\npreds = model_sgd_com_regularizador.predict(np.array(x_test), verbose=1)\n\ngerar_grafico(history_sgd_com_regularizador, \n              \"VGG16 com SGD\")\n\nsub = pd.DataFrame(preds)\ncol_names = one_hot.columns.values\nsub.columns = col_names\nsub.insert(0, 'id', df_test['id'])\nsub.head(5)\n\nsub.to_csv(\"output_sgd_v2_com_data_augmentation_e_sem_regularizador.csv\")\n\nmodel_sgd_com_regularizador.save('sgd_v2_com_data_augmentation_e_sem_regularizador.h5')","cbbc3934":"from sklearn.metrics import f1_score\n\nf1_score(Y_train, Y_valid, average='macro') ","01fe50f2":"f1_score(Y_train, Y_valid, average='micro') ","b1b13342":"f1_score(Y_train, Y_valid, average='weighted') ","e906486c":"f1_score(Y_train, Y_valid, average=None) ","4d48885a":"import matplotlib.pyplot as plt\n\nfrom sklearn import datasets\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (brier_score_loss, precision_score, recall_score,\n                             f1_score)\nfrom sklearn.calibration import CalibratedClassifierCV, calibration_curve\nfrom sklearn.model_selection import train_test_split\n\n\n# Create dataset of classification task with many redundant and few\n# informative features\n#X, y = datasets.make_classification(n_samples=100000, n_features=20,\n                                    #n_informative=2, n_redundant=10,\n                                    #random_state=42)\n#X_train, X_valid, Y_train, Y_valid # => \n#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.99,\n                                                    #random_state=42)\n\n\ndef plot_calibration_curve(est, name, fig_index):\n    \"\"\"Plot calibration curve for est w\/o and with calibration. \"\"\"\n    # Calibrated with isotonic calibration\n    isotonic = CalibratedClassifierCV(est, cv=2, method='isotonic')\n\n    # Calibrated with sigmoid calibration\n    sigmoid = CalibratedClassifierCV(est, cv=2, method='sigmoid')\n\n    # Logistic regression with no calibration as baseline\n    lr = LogisticRegression(C=1.)\n\n    fig = plt.figure(fig_index, figsize=(10, 10))\n    ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n    ax2 = plt.subplot2grid((3, 1), (2, 0))\n\n    ax1.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n    for clf, name in [(lr, 'Logistic'),\n                      (est, name),\n                      (isotonic, name + ' + Isotonic'),\n                      (sigmoid, name + ' + Sigmoid')]:\n        clf.fit(X_train, Y_train)\n        y_pred = clf.predict(X_valid)\n        if hasattr(clf, \"predict_proba\"):\n            prob_pos = clf.predict_proba(X_valid)[:, 1]\n        else:  # use decision function\n            prob_pos = clf.decision_function(X_valid)\n            prob_pos = \\\n                (prob_pos - prob_pos.min()) \/ (prob_pos.max() - prob_pos.min())\n\n        clf_score = brier_score_loss(Y_valid, prob_pos, pos_label=y.max())\n        print(\"%s:\" % name)\n        print(\"\\tBrier: %1.3f\" % (clf_score))\n        print(\"\\tPrecision: %1.3f\" % precision_score(Y_valid, y_pred))\n        print(\"\\tRecall: %1.3f\" % recall_score(Y_valid, y_pred))\n        print(\"\\tF1: %1.3f\\n\" % f1_score(Y_valid, y_pred))\n\n        fraction_of_positives, mean_predicted_value = \\\n            calibration_curve(Y_valid, prob_pos, n_bins=10)\n\n        ax1.plot(mean_predicted_value, fraction_of_positives, \"s-\",\n                 label=\"%s (%1.3f)\" % (name, clf_score))\n\n        ax2.hist(prob_pos, range=(0, 1), bins=10, label=name,\n                 histtype=\"step\", lw=2)\n\n    ax1.set_ylabel(\"Fraction of positives\")\n    ax1.set_ylim([-0.05, 1.05])\n    ax1.legend(loc=\"lower right\")\n    ax1.set_title('Calibration plots  (reliability curve)')\n\n    ax2.set_xlabel(\"Mean predicted value\")\n    ax2.set_ylabel(\"Count\")\n    ax2.legend(loc=\"upper center\", ncol=2)\n\n    plt.tight_layout()\n\n# Plot calibration curve for Gaussian Naive Bayes\nplot_calibration_curve(GaussianNB(), \"Naive Bayes\", 1)\n\n# Plot calibration curve for Linear SVC\nplot_calibration_curve(LinearSVC(max_iter=10000), \"SVC\", 2)\n\nplt.show()","3485f2f2":"The following will read the training and test images.","46270631":"Reading csv's to get the names of the images and their breeds.","dcc793df":"* [](http:\/\/)VGG16  com Regularization + Data Augmentation + Dropout 0.5","20fc5de8":"Races must be using * one-hot encode *"}}