{"cell_type":{"a67943d2":"code","51257267":"code","d411e6a2":"code","9a13ad18":"code","fac525e7":"code","1fe2a009":"code","3576845c":"code","58345660":"code","67e52171":"code","2c73f815":"code","b122ed3e":"code","26fbad4f":"code","4d6285b6":"code","1e3548c3":"code","2599dd17":"code","b069a6fc":"code","c3159d9f":"code","5d3e2364":"code","bd875576":"code","3c459fb0":"code","f156f212":"code","525b816f":"code","c5588d0d":"code","31d2ff32":"code","10e64e18":"code","43730e1f":"code","a8b56ab9":"code","5f14eff4":"code","bf278d83":"markdown","33b608c3":"markdown","18266463":"markdown","1ca13ab5":"markdown","a7878695":"markdown","e17c1e28":"markdown","908b4b2c":"markdown","0a1de7af":"markdown","481d5240":"markdown","018f8f9f":"markdown","8a696338":"markdown","72ded114":"markdown","62093a0c":"markdown","293ece3a":"markdown","39d0f090":"markdown","33e8bd42":"markdown","f612a8d1":"markdown","6daec37a":"markdown"},"source":{"a67943d2":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","51257267":"%matplotlib inline","d411e6a2":"# Setting plt and sns styles\nplt.style.use('seaborn-dark')\nsns.set(style='dark')","9a13ad18":"# Ignoring warnings\nimport warnings\nwarnings.filterwarnings('ignore')","fac525e7":"# Reading data\ndata = pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')","1fe2a009":"data.head()","3576845c":"data.describe()","58345660":"# Checking for missing data as NaN entries\ndata.isnull().sum()","67e52171":"features = list(data.columns)\nfeatures.remove('Outcome')","2c73f815":"# Calculating the relative size of each class\nN_TRUE = len(data[data['Outcome'] == 1])\nN_FALSE = len(data) - N_TRUE\n\nprint('N_TRUE = {}'.format(N_TRUE))\nprint('N_FALSE = {}'.format(N_FALSE))\nprint('N_FALSE fraction = {:.3f}'.format(N_FALSE\/(N_FALSE+N_TRUE)))","b122ed3e":"# Plotting distributions of features split by target\nfig, axs = plt.subplots(2, 4, figsize=(20, 10))\naxs = axs.flatten()\n\nfor ax, feat in zip(axs, features):\n    sns.histplot(data, x=feat, hue='Outcome', ax=ax);","26fbad4f":"# Counting examples with value 0 of Glucose, BloodPressure, BMI or Insulin.\ndata_with_zeros = data[\n    (data['Glucose'] == 0)\n    | (data['BMI'] == 0)\n    | (data['Insulin'] == 0)\n    | (data['BloodPressure'] == 0)\n]\nprint('N of examples with incomplete data = {}'.format(len(data_with_zeros)))","4d6285b6":"# Creating dummy variables to indicate whether Glucose, BloodPressure, BMI and Insulin data is valid.\nfor feat in ['Glucose', 'BloodPressure', 'BMI', 'Insulin']:\n    new_feat = 'Valid' + feat\n    data[new_feat] = data[feat].map(lambda d: int(d != 0))\n    features.append(new_feat)\nprint(features)","1e3548c3":"# Looking at the correlations\nfig, ax = plt.subplots(figsize=(10,10))\nsns.heatmap(data.corr(), annot=True, fmt='.2f', ax=ax);","2599dd17":"# Standard scaling\nfrom sklearn.preprocessing import StandardScaler","b069a6fc":"# Preparing X and y arrays \nX_train, y_train = data[features].to_numpy(), data['Outcome'].to_numpy()","c3159d9f":"scaler = StandardScaler().fit(X_train)\nX_train_std = scaler.transform(X_train)","5d3e2364":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.model_selection import KFold","bd875576":"# Building and fitting models using KFold with 5 splits.\n# The classifiers are LogisticRegression, RandomForestClassifier and DummyClassifier\n\nfrom collections import defaultdict\n\nkf = KFold(n_splits=5)\nresults = dict()\nresults['LogisticRegression'] = defaultdict(list)\nresults['RandomForest'] = defaultdict(list)\nresults['DummyClassifier'] = defaultdict(list)\n\nfor train_index, test_index in kf.split(X_train_std):\n    X_train_cv, X_val_cv = X_train_std[train_index], X_train_std[test_index]\n    y_train_cv, y_val_cv = y_train[train_index], y_train[test_index]\n    \n    # print('Fitting LogReg {}'.format(counter))\n    for key in results.keys():\n        if key == 'LogisticRegression':\n            clf = LogisticRegression(C=1).fit(X_train_cv, y_train_cv)\n        elif key == 'RandomForest':\n            clf = RandomForestClassifier(n_estimators=100).fit(X_train_cv, y_train_cv)\n        else:\n            clf = DummyClassifier(strategy='most_frequent').fit(X_train_cv, y_train_cv)\n    \n        results[key]['pred'].append(clf.predict(X_val_cv))\n        results[key]['prob'].append(clf.predict_proba(X_val_cv))\n        results[key]['true'].append(y_val_cv)\n        results[key]['accuracy'].append(clf.score(X_val_cv, y_val_cv))\n\nprint('Mean Accuracy')\nfor key in results.keys():\n    print('{} {:.3f}'.format(key, np.mean(results[key]['accuracy'])))","3c459fb0":"from sklearn.metrics import (\n    confusion_matrix,\n    accuracy_score, \n    recall_score, \n    f1_score,\n    precision_score,\n    auc,\n    roc_curve,\n    precision_recall_curve,\n    classification_report\n)","f156f212":"# Confusion Matrix\n# This calculation will use only one of the cross validation splits.\nnormalize = None\nfor key in results.keys():\n    results[key]['conf_mat'] = confusion_matrix(\n        results[key]['true'][0],\n        results[key]['pred'][0],\n        normalize=normalize\n    )","525b816f":"# Plotting confusion matrix for all classifiers\nfig, axs =plt.subplots(1, 3, figsize=(15, 4))\naxs = axs.flatten()\n\nfor key, ax in zip(results.keys(), axs):\n    ax.set_title(key)\n    sns.heatmap(results[key]['conf_mat'], annot=True, ax=ax)\n    ax.set_ylabel('true')\n    ax.set_xlabel('predicted')","c5588d0d":"# Calculating scores and storing them into a DataFrame in the results dict\nfor clf in results.keys():\n    \n    results[clf]['scores_df'] = pd.DataFrame(columns=['Accuracy', 'Precision', 'Recall', 'F1'])\n    \n    for y_true, y_pred in zip(results[clf]['true'], results[clf]['pred']):\n        acc = accuracy_score(y_true, y_pred)\n        prec = precision_score(y_true, y_pred)\n        rec = recall_score(y_true, y_pred)\n        f1 = f1_score(y_true, y_pred)\n        results[clf]['scores_df'] = results[clf]['scores_df'].append(\n                {'Accuracy': acc, 'Precision': prec, 'Recall': rec, 'F1': f1},\n                ignore_index=True\n        )","31d2ff32":"results['LogisticRegression']['scores_df'].head()","10e64e18":"# Plotting the scores as box plots \nfig, axs = plt.subplots(1, 3, figsize=(15, 5))\naxs = axs.flatten()\n\nfor clf, ax in zip(results.keys(), axs):\n    ax.set_title(clf)\n    sns.boxplot(data=results[clf]['scores_df'], ax=ax)\n    ax.set_ylim(0.5, 0.9)","43730e1f":"# Calculating recall and precision as a function of threshold\nrecall, precision, accuracy = list(), list(), list()\nthreshold = np.linspace(0, 0.99, 20)\n\nclf = 'LogisticRegression'\n\nfor thr in threshold:\n    y_true = results[clf]['true'][0]\n    y_pred = [int(r > thr) for r in results[clf]['prob'][0][:,1]] \n    \n    precision.append(precision_score(y_true, y_pred))\n    recall.append(recall_score(y_true, y_pred))\n    accuracy.append(accuracy_score(y_true, y_pred))    ","a8b56ab9":"# Plotting recall and precision x threshold\nax = plt.gca()\n\nax.plot(threshold, precision, label='precision')\nax.plot(threshold, recall, label='recall')\nax.plot(threshold, accuracy, label='accuracy')\n\nax.set_ylabel('recall\/precision')\nax.set_ylabel('threshold')\n\nax.plot([0.5, 0.5], [0, 1.0], color='k', linestyle='--')\nax.plot([0.3, 0.3], [0, 1.0], color='k', linestyle=':')\n\nax.legend();","5f14eff4":"# Plotting ROC curves\n\nfig, axs =plt.subplots(1, 2, figsize=(15, 6))\naxs = axs.flatten()\n\nfor ax, key in zip(axs, ('LogisticRegression', 'RandomForest')):\n\n    ax.set_title(key)\n    for y_true, y_prob in zip(results[key]['true'], results[key]['prob']):\n        fpr, tpr, thresholds = roc_curve(y_true, y_prob[:,1])\n        auc_score = auc(fpr, tpr)\n        results[key]['AUC'].append(auc_score)\n        sns.lineplot(fpr, tpr, ax=ax, label='AUC = {:.3f}'.format(auc_score));\n\n    ax.legend()\n\nprint('Mean AUC')\nfor key in ('LogisticRegression', 'RandomForest'):\n    print('{} {:.3f}'.format(key, np.mean(results[key]['AUC'])))\n","bf278d83":"# Introduction\n\nThis is a *binary classification* problem in which the aim is to predict which patients have diabetes based on a number of measurements. \n\nThe focus of this notebook is on the *binary classification metrics*, including confusion matrix, multiple kinds of scores (accuracy, precision, recall and F1 score), ROC curve and AUC.  \n\nThere are substantial fraction of missing data that needed to be handled. These data is indicated by examples with value 0 of Glucose, BloodPressure, BMI and Insulin. Instead of removing these examples, we created dummy variables to indicate whether of not these other variables have valid values. Apart from that, no other feature engineering step was required.\n\nA simple modeling using Logistic Regression and Random Forest is performed, without any kind of optimization.\n\nDataset details can be found [here](https:\/\/www.kaggle.com\/uciml\/pima-indians-diabetes-database).\n\n**This notebook is part of my learning process, therefore any feedback is welcome and appreciated =)**","33b608c3":"## Handling Missing Data","18266463":"# Data Exploration and Preparation","1ca13ab5":"- The feature distributions look all fine in general (nearly gaussian shape, no outliers)\n- There are spikes at 0 in a number of features (Glucose, BloodPressure, BMI and Insulin). We know that these measurements should never be zero, so these must be examples in which these **measurements are missing**. We are going to clean these examples next.","a7878695":"## Scores: Accuracy, Recall, Precision, F1 scores\n\nA good explanation about classification scores can be found [here](https:\/\/towardsdatascience.com\/accuracy-precision-recall-or-f1-331fb37c5cb9).\n\n- **Accuracy** is the overall rate of classes being correctly predicted. It is very sensitive to imbalances on the relative size of each class. \n\n- **Precision** is the fraction of predicted positives that are actually positives. It is important when the cost of false positive is high. E.g. email spam detection (we do not want to classify a real email as spam, but we do not mind classifying a spam as real one once in a while).\n\n- **Recall** is the fraction of true positives that were actually predicted as positives. It is important when the cost of false negatives are high. E.g. fraud or sickness detection.\n\n- **F1 Score** is given by 2 * (p x r)\/ (p + r), where p is precision and r is recall. It provides a balance between precision and recall. It is better than accuracy because it balances out the sizes of the samples for both classes.","e17c1e28":"## Rescaling","908b4b2c":"- The number of incomplete examples is too large to be simply removed. Instead we will created new dummy variables to indicate whether of not a certain features is valid. ","0a1de7af":"# Modeling\n\nA simple modeling will be performed, using only Logistic Regression and Random Forest classifiers.\n\nThe dataset is small (< 1000 examples) and the number of features not large (20), so computing performance should not be an issue.\n\nWe will use *cross validation* to evaluate the classifiers. All the metrics will be calculated using the validation datasets. Since this is only a simple analysis for learning purposes, we will not separate a test sample for a final evaluation. ","481d5240":"We can see from the plot that there is a trade off between recall and precision. On an extreme case, when the threshold is set to zero, all examples are classified as positives, which leads to recall equals to 1.0. This extreme scenario is clearly not a good option since we are not making use of all the information countained in the dataset to optimize the classification. Ideally, we would like to find a optimal threshold that still maintains a certain high level of accuracy.\n\nIn our plot we can see that between 0.3 and 0.6 the accuracy reaches a plateau. This means that varying the threshold within this range will only represent a trade between recall and precision without a lossof accuracy. Therefore, **the new threshold should be set to around 0.3**, where the recall reaches a maximum before the accuracy start dropping.","018f8f9f":"### Discussion\n\n- One can see that the dataset is *not balanced* with relation to each target class - number of target with value 0 (not diabetics) is ~ 3 x largen than 1 (diabetics). This unbalance will be important for the discussion of the scores in the next section.\n- The confusion matrix themselves are not good tools to evaluate the performance of the classifiers. A simpler method for that is to calculate the scores, that will be discussed next.","8a696338":"# Binary Classification Metrics\n\nIn this section we will calculate a number of different binary classification metrics: confusion matrix, scores, ROC and AUC. ","72ded114":"### Opmitizing Decision Threshold","62093a0c":"## Confusion Matrix\n\nThe good reference about confusion matrix can be found [here](https:\/\/towardsdatascience.com\/understanding-confusion-matrix-a9ad42dcfd62).","293ece3a":"### Discussion\n\nFirst we can see that both LogisticRegression and RandomForest show a very similar trend between the scores. LogisticRegression perform slighlty better than RandomForest on average and both perform much better than the DummyClassifier (~0.75 accuracy for LR and RF against <0.65 of the DC).\n\nSecond, the accuracy is always the highest score, which is expected since it is the overall \"rate of correctness\" of the model. Another expected feature is that the F1 score is an intermediate value between precision and recall, which makes sense by construction.\n\nComparing precision and recall we see that precision is significantly higher (~0.7 against ~0.55). For this specific problem, however, our priority is to find as many diabetics as possible, even if that means we willmisclassify some non-diabetics as diabetics. Therefore we want to optimize the *fraction of true positive that were actually predicted as positive*, the **recall**. The recall in our modelling is around 0.55 - 0.6, which is not that good.\n\nIn order to evaluate whether we can reach a better recall by changing the threshold, we will plot next the recall and precision vs threshold for the Logistic Regression model.","39d0f090":"- The ROC looks very similar for both models.\n- The AUC is an overall metric (like accuracy) and we see that the same trend is observed, where LogisticRegrssion performs slighlty better than RandomForest.","33e8bd42":"## ROC curve and AUC\n\nExplanations about ROC and AUC can be found [here](https:\/\/towardsdatascience.com\/understanding-auc-roc-curve-68b2303cc9c5).","f612a8d1":"## Distributions and Correlations","6daec37a":"## Variables - Summary\n\n- 8 numerical features: Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin, BMI, DiabetesPedigreeFunction, Age.\n- There is no NaN entry in the dataset.\n- No feature engineering seems to be necessary.\n- Outcome is the target variable (0 or 1).\n- 65% of the examples do not have diabetes, which will be our baseline for accuracy score of the classifiers.  "}}