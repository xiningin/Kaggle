{"cell_type":{"ba9f46b7":"code","3a4dc974":"code","ae361d36":"code","eac56261":"code","084ce0be":"code","f14a8fa5":"code","78acb665":"code","ced1bd8c":"code","37377be9":"code","0eedf5a0":"code","047423ec":"code","9d39e49b":"code","a02e322c":"code","aa2d53fd":"code","084062c9":"code","c387aa42":"code","0c448121":"markdown","7bda7be7":"markdown","fcd53a75":"markdown","65aa7e32":"markdown","74de0eeb":"markdown","a69d4ba5":"markdown","9a84fa13":"markdown","358d6742":"markdown","26de49d6":"markdown","ffd0fc45":"markdown","221f4af8":"markdown"},"source":{"ba9f46b7":"import pandas as pd\nimport numpy as np\n\ndf = pd.read_csv(\"..\/input\/autism-prediction\/Autism-prediction\/train.csv\")\ndf.dtypes","3a4dc974":"df.head()","ae361d36":"df = pd.get_dummies(df, columns=['gender', 'ethnicity', 'jaundice', 'austim', 'contry_of_res', 'used_app_before', 'age_desc', 'relation'])","eac56261":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\ndataset = df\ny = dataset[\"Class\/ASD\"]\nsns.displot(y)","084ce0be":"from sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\n\nX = dataset.select_dtypes(exclude=\"object\").copy()\nX.drop([\"Class\/ASD\",\"ID\"], axis=1, inplace=True)\nX = X.values\npoly = preprocessing.PolynomialFeatures(2)\nX = poly.fit_transform(X)\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42) ","f14a8fa5":"import sklearn\nfrom imblearn.over_sampling import RandomOverSampler \n\nrnd_over = RandomOverSampler(sampling_strategy=\"minority\")\nX_train, y_train = rnd_over.fit_resample(X_train, y_train)\nsns.displot(y_train)","78acb665":"scaler = preprocessing.StandardScaler().fit(X_train)\nX_train = scaler.transform(X_train)\nX_val = scaler.transform(X_val)","ced1bd8c":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\n\ntree_set = dataset.select_dtypes(exclude=\"object\").copy()\ntarget = tree_set[\"Class\/ASD\"]\ntree_set.drop([\"Class\/ASD\",\"ID\"], axis=1, inplace=True)\n\nclf = DecisionTreeClassifier(max_depth=3)\nclf.fit(tree_set, target)\nclf.score(tree_set, target)","37377be9":"fig = plt.figure(figsize=(25,20))\n_ = tree.plot_tree(clf, \n                   feature_names=df.columns,\n                   class_names=[\"Not Autistic\", \"Autistic\"],\n                   filled=True, \n                   fontsize=18)","0eedf5a0":"!pip install pycaret","047423ec":"from pycaret.classification import *\n\nsetup(data = dataset.select_dtypes(exclude=\"object\").copy(), \n             target = \"Class\/ASD\",\n             numeric_imputation = 'mean',\n             silent = True, normalize = True, session_id=42)\n\ntop3 = compare_models(exclude = ['catboost','xgboost','lightgbm'], n_select=3)","9d39e49b":"tune_model(top3[0])","a02e322c":"from sklearn.ensemble import ExtraTreesClassifier\n\nclf = ExtraTreesClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},\n                     criterion='entropy', max_depth=9, max_features='sqrt',\n                     max_leaf_nodes=None, max_samples=None,\n                     min_impurity_decrease=0, min_impurity_split=None,\n                     min_samples_leaf=6, min_samples_split=10,\n                     min_weight_fraction_leaf=0.0, n_estimators=230, n_jobs=-1,\n                     oob_score=False, random_state=42, verbose=0,\n                     warm_start=False)\nclf.fit(X_train, y_train)\nprint(clf.score(X_train, y_train))\nprint(clf.score(X_val, y_val))","aa2d53fd":"y = dataset[\"Class\/ASD\"]\nX = dataset.select_dtypes(exclude=\"object\").copy()\nX.drop([\"Class\/ASD\",\"ID\"], axis=1, inplace=True)\n\npoly = preprocessing.PolynomialFeatures(2)\nX = poly.fit_transform(X)\n\nscaler = preprocessing.StandardScaler().fit(X)\nX = scaler.transform(X)\n\nclf = ExtraTreesClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},\n                     criterion='entropy', max_depth=9, max_features='sqrt',\n                     max_leaf_nodes=None, max_samples=None,\n                     min_impurity_decrease=0, min_impurity_split=None,\n                     min_samples_leaf=6, min_samples_split=10,\n                     min_weight_fraction_leaf=0.0, n_estimators=230, n_jobs=-1,\n                     oob_score=False, random_state=42, verbose=0,\n                     warm_start=False)\nclf.fit(X, y)\nprint(clf.score(X, y))","084062c9":"X_test = pd.read_csv(\"..\/input\/autism-prediction\/Autism-prediction\/test.csv\")\nX_test = pd.get_dummies(X_test, columns=['gender', 'ethnicity', 'jaundice', 'austim', 'contry_of_res', 'used_app_before', 'age_desc', 'relation'])\nX_test = X_test.select_dtypes(exclude=\"object\").copy()\n\n# drop columns we didn't see in training\nX_test_copy = X_test.copy()\nfor col in X_test.columns:\n    if col not in dataset.columns:\n        X_test_copy = X_test_copy.drop(col, axis=1)\nX_test = X_test_copy\n# add columns we don't see in test but did in training\nX_test_copy = X_test.copy()\nfor col in dataset.columns:\n    if col not in X_test.columns:\n        X_test_copy[col] = 0\nX_test = X_test_copy","c387aa42":"X_test.drop([\"Class\/ASD\",\"ID\"], axis=1, inplace=True)\n\npoly = preprocessing.PolynomialFeatures(2)\nX_test = poly.fit_transform(X_test)\n\nX_test = scaler.transform(X_test)\npredicts = clf.predict(X_test)\nsubmissiondf = pd.DataFrame({'ID': pd.read_csv(\"..\/input\/autism-prediction\/Autism-prediction\/test.csv\")['ID'],\n                       'Class\/ASD': predicts})\nsubmissiondf.to_csv(\".\/submission.csv\", index=False)","0c448121":"# Process Data Create Y\n\nY will be the class column. Let's see what the balance is like.","7bda7be7":"Hmm a lot of objects not the most auspicious start for Exploratory Data Analysis. Often it is better to have floats or ints. Let's see if there is anything that can be improved.","fcd53a75":"# Create Decision Tree and Visualize Result","65aa7e32":"Scale data","74de0eeb":"# Introduction\n\nThis is an interesting dataset and I wanted to see how a simple approach fair on it. I will be doing feature engineering and then let Pycaret show me which of the simpler techniques will be best by excluding boosting. Hopefully we get something to show for it at the end.  \n\n# Load Data","a69d4ba5":"Quite a bit of imbalance we may need to look at ways to solve that.","9a84fa13":"# Run on Test\n\nInitially we need to train on the whole dataset for maximum accuracy when doing test.","358d6742":"# Split X","26de49d6":"# Use Pycaret","ffd0fc45":"# Conclusion\n\nWhile results could have been better. I was happy to see this through my chosen approach to its end result. If you have ideas on how I can improve please let me know in the comments. I'm here to learn :)  ","221f4af8":"# Random Over Sampling\n\nWe can balance the classes by over sampling from Y"}}