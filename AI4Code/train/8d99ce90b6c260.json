{"cell_type":{"4d561c4e":"code","6ddade42":"code","9897e86b":"code","ce6a4530":"code","969f342b":"code","534071f6":"code","7721daf2":"code","d2323ea2":"code","f4552b86":"code","ac7b88fa":"code","916eb471":"code","ccfff5a1":"code","61ae19cd":"code","b933216c":"code","138aabd4":"markdown","c2e1abea":"markdown","c937d1a8":"markdown","90930afb":"markdown","d251c900":"markdown","fcfd46d0":"markdown","6866ede7":"markdown","76f2c6d6":"markdown","ece4aa01":"markdown","04d9ba4b":"markdown","bfb67450":"markdown","2fddc562":"markdown","1bf88657":"markdown","2307ff45":"markdown","e47dce89":"markdown"},"source":{"4d561c4e":"from IPython.display import Image\nimport os\n!ls ..\/input\/","6ddade42":"Image('..\/input\/pictures1\/snn.png')","9897e86b":"#Import numpy for numerical calculations\n\nimport numpy as np","ce6a4530":"input_weights = np.around(np.random.uniform(-5,5,size=6), decimals=2)\nbias_weights = np.around(np.random.uniform(size=3), decimals=2)","969f342b":"print(input_weights)\nprint(bias_weights)","534071f6":"x_1 = 0.5 #input 1\nx_2 = 0.82 #input 2\n\nprint('Input x1 is {} and Input x2 is {}'.format(x_1,x_2))","7721daf2":"z_11 = x_1 * input_weights[0] + x_2 * input_weights[1] + bias_weights[0]\n\nprint('The linear combination of inputs at the first node of the hidden layer is {}'.format(z_11))","d2323ea2":"z_12 = x_1 * input_weights[2] + x_2 * input_weights[3] + bias_weights[1]\n\nprint('The linear combination of inputs at the second node of the hidden layer is {}'.format(z_12))","f4552b86":"Image('..\/input\/pictures1\/relu.png')","ac7b88fa":"a_11 = max(0.0, z_11)\n\nprint('The output of the activation function at the first node of the hidden layer is {}'.format(np.around(a_11, decimals=4)))","916eb471":"a_12 = max(0.0, z_12)\n\nprint('The output of the activation function at the second node of the hidden layer is {}'.format(np.around(a_12, decimals=4)))","ccfff5a1":"z_2 = a_11 * input_weights[4] + a_12 * input_weights[5] + bias_weights[2]\n\nprint('The linear combination of inputs at the output layer is {}'.format(z_2))","61ae19cd":"Image('..\/input\/pictures1\/sigmoid.png')","b933216c":"y = 1.0 \/ (1.0 + np.exp(-z_2))\n\nprint('The output of the network for the given inputs is {}'.format(np.around(y, decimals=6)))","138aabd4":"Let's calculate linear combination of inputs and their weights which will be assigned to **z_11**, the first node in the hidden layer.","c2e1abea":"# 5. Repeat the steps until you get your final output.","c937d1a8":"Here, we will be feeding this summation into a non-linear activation function known as sigmoid function which is best suited for Output Layer.","90930afb":"As you can see from the above formula, it's clear that **ReLU excludes any x values which are less than 0 and activates only when x values are greater than 0**.\n\nLet's compute the output of this activation function when z_11 is fed into it.","d251c900":"# 4. Calculate Output of Activation Function.\n\nNow that we're done with the summation, let's feed this into the activation functions. We're taking ReLU as our activation function for this hidden layer.\n\nLet's first visualize ReLU function and look at the formula.","fcfd46d0":"Now, these outputs serve as the inputs to the Output Layer. So, let's repeat the last few steps in assigning weights and computing linear combination of inputs and then passing that into the activation function.","6866ede7":"# 3. Calculate linear combination of inputs.","76f2c6d6":"By default, value for biases is equal to 1.","ece4aa01":"# 1. Randomly Initialize Weights.","04d9ba4b":"We have SNN (Shallow Neural Network) and ReLU function, which we will come back to later. Let's look at SNN.","bfb67450":"Let's see the random weights which were assigned to the inputs and the biases.","2fddc562":"# 2. Assign values to inputs.","1bf88657":"# Structure of Shallow Neural Network\n\nLet's see how a Shallow Neural Network looks like.","2307ff45":"For simplicity's sake, we are taking only **2 inputs** plus a default bias. In **total**, there are **9 weights** of which 6 correspond to the inputs and the other 3 are of bias. In the hidden layer, we have two nodes, namely, **z_11 and z_12**. These are the summation functions in the hidden layer which will then be fed into activation functions, respectively, **a_11 and a_12**. The output from these activation functions will again have different weights and have the final summation at **z_2**. This z_2 will again be fed into activation function which yields us the **final output**. \n\nLet's start coding.","e47dce89":"## *Welcome to FIO Labs*\n\n# This Tutorial is also available in Video format on our [YouTube Channel - FIOLabs](https:\/\/www.youtube.com\/channel\/UC6Vn_nUJeJ7PrvsayHPVOag) . Follow for more AI | ML | DL Tutorials.\n\n# Shallow Neural Network\n\nWe've done coding for a simple [Perceptron](https:\/\/www.kaggle.com\/prashfio\/perceptron#Structure-of-Perceptron)  in our last tutorial using just *numpy*. Let's build a Shallow Neural Network *from the scratch*, i.e., a Neural Network with one input layer, one hidden layer, and one output layer."}}