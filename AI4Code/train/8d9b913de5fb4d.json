{"cell_type":{"1083281b":"code","be878872":"code","42213860":"code","c25d9eda":"code","bed3e72b":"code","804d1a1c":"code","4905aa28":"code","5bbf1245":"code","90d9d492":"code","06423cc9":"code","75a76c05":"code","1df79328":"code","17af4fb7":"code","18eb86cb":"code","545f4be1":"code","39994aa2":"code","6ac602c6":"code","690deb7a":"code","3461580e":"code","04798867":"code","d80806ed":"code","55d0becd":"code","39cedbdf":"code","0b525461":"code","b252b20e":"code","6d226547":"code","2c3aea52":"code","53131ccf":"code","48e16e7a":"code","53b45507":"code","7a460e37":"code","3e95a2c1":"code","bde9da48":"code","aeff3f04":"code","4d9112b7":"code","8acddd00":"code","a81ace76":"code","89aa0471":"code","dd064607":"code","f3d2fc55":"code","8d095b8d":"code","d5a69e28":"code","9e805e31":"code","6c25a12a":"code","f18f8aac":"code","ff44a44a":"code","23417261":"code","07767420":"code","b0e569b0":"code","280e7740":"code","5d8ee0cd":"code","5528e8e3":"code","9766ba5b":"code","e7e6e2c3":"code","6613495c":"code","7981e83e":"code","e8d62840":"code","72e42cdc":"code","38551d8b":"code","d03bfa1a":"code","5a93f09d":"code","9a56d0b4":"code","cfa0ad15":"code","3ec0f8fc":"code","e4b57c48":"code","dedaf87c":"code","137f4197":"code","84a0739e":"code","48b2956f":"code","360ba8e1":"code","a2b0c5c7":"code","fa097482":"markdown","daa9e3ee":"markdown","23f2b0f7":"markdown","9eb703ed":"markdown"},"source":{"1083281b":"package_path = \"..\/input\/efficientnet-pytorch\/EfficientNet-PyTorch\/EfficientNet-PyTorch-master\/\"\nimport sys \nsys.path.append(package_path)\n\nimport os\nimport glob\nimport time\nimport random\n\nimport numpy as np\nimport pandas as pd\n\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nimport cv2\n\nimport torch\nfrom torch import nn\nfrom torch.utils import data as torch_data\nfrom torch.nn import functional as F\n\nimport efficientnet_pytorch\n\nfrom torch.utils.data import Dataset, DataLoader\n","be878872":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nseed = 123\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(seed)\n\nclass X3D:\n    XS=0\n    S=1\n    M=2\n    L=3\n    \nx3d_config = {\n    'input_clip_length': [4, 13, 16, 16],\n    'depth_factor': [2.2, 2.2, 2.2, 5.0],\n    'width_factor': [1, 1, 1, 2.9]\n}\n\nclass CFG:\n    img_size = 256\n    n_frames = 10\n    \n    cnn_features = 256\n    lstm_hidden = 32\n    \n    n_fold = 5\n    n_epochs = 10","42213860":"class CNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.map = nn.Conv2d(in_channels=4, out_channels=3, kernel_size=1)\n        self.net = efficientnet_pytorch.EfficientNet.from_name(\"efficientnet-b0\")\n#         checkpoint = torch.load(\"..\/input\/efficientnet-pytorch\/efficientnet-b0-08094119.pth\")\n#         self.net.load_state_dict(checkpoint)\n        \n        n_features = self.net._fc.in_features\n        \n#         print(n_features)\n        \n        self.net._fc = nn.Linear(in_features=n_features, out_features=CFG.cnn_features, bias=True)\n    \n    def forward(self, x):\n        x = F.relu(self.map(x))\n        out = self.net(x)\n        return out\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.cnn = CNN()\n        self.rnn = nn.LSTM(CFG.cnn_features, CFG.lstm_hidden, 2, batch_first=True)\n        self.fc = nn.Linear(CFG.lstm_hidden, 1, bias=True)\n\n    def forward(self, x):\n        # x shape: BxTxCxHxW\n        batch_size, timesteps, C, H, W = x.size()\n        c_in = x.view(batch_size * timesteps, C, H, W)\n        c_out = self.cnn(c_in)\n        r_in = c_out.view(batch_size, timesteps, -1)\n        output, (hn, cn) = self.rnn(r_in)\n        \n        out = self.fc(hn[-1])\n        return out","c25d9eda":"def load_dicom(path):\n    dicom = pydicom.read_file(path)\n    data = dicom.pixel_array\n    data = data - np.min(data)\n    if np.max(data) != 0:\n        data = data \/ np.max(data)\n        \n    data = np.float32(cv2.resize(data, (CFG.img_size, CFG.img_size)))\n    return torch.tensor(data)\n\n# def load_dicom_line(path):\n#     t_paths = sorted(\n#         glob.glob(os.path.join(path, \"*\")), \n#         key=lambda x: int(x[:-4].split(\"-\")[-1]),\n#     )\n#     images = []\n#     for filename in t_paths:\n#         data = load_dicom(filename)\n#         if data.max() == 0:\n#             continue\n#         images.append(data)\n        \n#     return images\n\n# def load_image(path):\n#     image = cv2.imread(path, 0)\n#     if image is None:\n#         return np.zeros((CFG.img_size, CFG.img_size))\n    \n#     image = cv2.resize(image, (CFG.img_size, CFG.img_size)) \/ 255\n#     return torch.tensor(image)\n\n# def get_valid_frames(t_paths):\n#     res = []\n#     for path in t_paths:\n#         img = load_dicom(path)\n#         if img.view(-1).mean(0) != 0:\n#             res.append(path)\n#     return res\n    \n\ndef uniform_temporal_subsample(x, num_samples):\n    '''\n        Moddified from https:\/\/github.com\/facebookresearch\/pytorchvideo\/blob\/d7874f788bc00a7badfb4310a912f6e531ffd6d3\/pytorchvideo\/transforms\/functional.py#L19\n        Args:\n            x: input list\n            num_samples: The number of equispaced samples to be selected\n        Returns:\n            Output list     \n    '''\n    t = len(x)\n    indices = torch.linspace(0, t - 1, num_samples)\n    indices = torch.clamp(indices, 0, t - 1).long()\n    return [x[i] for i in indices]","bed3e72b":"class TestDataRetriever(Dataset):\n    def __init__(self, paths, transform=None):\n        self.paths = paths\n        self.transform = transform\n          \n    def __len__(self):\n        return len(self.paths)\n    \n    def read_video(self, vid_paths):\n        video = [load_dicom(path) for path in vid_paths]\n        if len(video)==0:\n            video = torch.zeros(CFG.n_frames, CFG.img_size, CFG.img_size)\n        else:\n            video = torch.stack(video) # T * C * H * W\n#         video = torch.transpose(video, 0, 1) # C * T * H * W\n        return video\n    \n    def __getitem__(self, index):\n        _id = self.paths[index]\n        patient_path = f\"..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/test\/{str(_id).zfill(5)}\/\"\n        channels = []\n        for t in [\"FLAIR\",\"T1w\", \"T1wCE\", \"T2w\"]:\n            t_paths = sorted(\n                glob.glob(os.path.join(patient_path, t, \"*\")), \n                key=lambda x: int(x[:-4].split(\"-\")[-1]),\n            )\n            num_samples = CFG.n_frames\n#             t_paths = get_valid_frames(t_paths)\n            if len(t_paths) < num_samples:\n                in_frames_path = t_paths\n            else:\n                in_frames_path = uniform_temporal_subsample(t_paths, num_samples)\n            \n            channel = self.read_video(in_frames_path)\n            if channel.shape[0] == 0:\n                print(\"1 channel empty\")\n                channel = torch.zeros(num_samples, CFG.img_size, CFG.img_size)\n            channels.append(channel)\n        \n        channels = torch.stack(channels).transpose(0,1)\n        return {\"X\": channels.float(), \"id\": _id}","804d1a1c":"df = pd.read_csv(\"..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/train_labels.csv\")\ndf.head(10)\n# df.info()","4905aa28":"models = []\nfor i in range(1, CFG.n_fold+1):\n    model = Model()\n    model.to(device)\n    checkpoint = torch.load(f\"..\/input\/rnsa21-best-weights\/best-model-{i}.pth\")\n#     print(checkpoint[\"model_state_dict\"])\n    model.load_state_dict(checkpoint[\"model_state_dict\"])\n    model.eval()\n    \n    models.append(model)","5bbf1245":"submission = pd.read_csv(\"..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/sample_submission.csv\")\ntest_data_retriever = TestDataRetriever(\n    submission[\"BraTS21ID\"].values # ids in test data\n)\nprint(test_data_retriever.read_video)\n\ntest_loader = torch_data.DataLoader(\n    test_data_retriever,\n    batch_size=4,\n    shuffle=False,\n    num_workers=8,\n)\n","90d9d492":"print(len(test_data_retriever))\nprint(test_data_retriever[85]['X'].shape)","06423cc9":"test_loader.batch_size\nlen(test_loader) # no. of batches","75a76c05":"y_pred = []\nids = []\n\nfor e, batch in enumerate(test_loader):\n    print(f\"{e}\/{len(test_loader)}\", end=\"\\r\")\n    with torch.no_grad():\n        print(batch[\"X\"].shape)\n        tmp_pred = np.zeros((batch[\"X\"].shape[0], ))\n        for model in models:\n            tmp_res = torch.sigmoid(model(batch[\"X\"].to(device))).cpu().numpy().squeeze()\n#             print(tmp_pred)\n            tmp_pred += tmp_res\n            \n        tmp_pred = tmp_pred\/len(models)\n        y_pred.extend(tmp_pred)\n#         print(len(y_pred))\n        ids.extend(batch[\"id\"].numpy().tolist())","1df79328":"submission1 = pd.DataFrame({\"BraTS21ID\": ids, \"MGMT_value\": y_pred})\n# submission.to_csv(\"submission.csv\", index=False)","17af4fb7":"submission1","18eb86cb":"pip install '..\/input\/rsna-monai-packages\/monai-0.6.0-202107081903-py3-none-any.whl'","545f4be1":"import pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport os\n\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nimport glob","39994aa2":"import albumentations as A\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom albumentations.pytorch import ToTensorV2\nfrom sklearn.metrics import roc_auc_score\nfrom torch.optim import lr_scheduler\nfrom tqdm import tqdm\nimport re","6ac602c6":"NUM_IMAGES_3D = 64\nTRAINING_BATCH_SIZE = 8\nTEST_BATCH_SIZE = 8\nIMAGE_SIZE = 256\nN_EPOCHS = 15\ndo_valid = True\nn_workers = 4\ntype_ = \"T1wCE\"","690deb7a":"def load_dicom_image(path, img_size=IMAGE_SIZE, voi_lut=True, rotate=0):\n    dicom = pydicom.read_file(path)\n    data = dicom.pixel_array\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n\n    if rotate > 0:\n        rot_choices = [\n            0,\n            cv2.ROTATE_90_CLOCKWISE,\n            cv2.ROTATE_90_COUNTERCLOCKWISE,\n            cv2.ROTATE_180,\n        ]\n        data = cv2.rotate(data, rot_choices[rotate])\n\n    data = cv2.resize(data, (img_size, img_size))\n    return data","3461580e":"import random\n\nimport cv2\nfrom torch.utils.data import Dataset\n\n\nclass BrainRSNADataset(Dataset):\n    def __init__(\n        self, data, transform=None, target=\"MGMT_value\", mri_type=\"FLAIR\", is_train=True\n    ):\n        self.target = target\n        self.data = data\n        self.type = mri_type\n\n        self.transform = transform\n        self.is_train = is_train\n        self.folder = \"train\" if self.is_train else \"test\"\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        row = self.data.loc[index]\n        case_id = int(row.BraTS21ID)\n        target = int(row[self.target])\n        _3d_images = self.load_dicom_images_3d(case_id)\n        _3d_images = torch.tensor(_3d_images).float()\n        if self.is_train:\n            return {\"image\": _3d_images, \"target\": target}\n        else:\n            return {\"image\": _3d_images, \"case_id\": case_id}\n\n    def load_dicom_images_3d(\n        self,\n        case_id,\n        num_imgs=NUM_IMAGES_3D,\n        img_size=IMAGE_SIZE,\n        rotate=0,\n    ):\n        case_id = str(case_id).zfill(5)\n\n        path = f\"..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/{self.folder}\/{case_id}\/{self.type}\/*.dcm\"\n#         path = f\"..\/input\/brain-tumor-test\/test1\/test1\/{case_id}\/{self.type}\/*.dcm\"\n        \n        files = sorted(\n            glob.glob(path),\n            key=lambda var: [\n                int(x) if x.isdigit() else x for x in re.findall(r\"[^0-9]|[0-9]+\", var)\n            ],\n        )\n\n        middle = len(files) \/\/ 2\n        num_imgs2 = num_imgs \/\/ 2\n        p1 = max(0, middle - num_imgs2)\n        p2 = min(len(files), middle + num_imgs2)\n        image_stack = [load_dicom_image(f, rotate=rotate) for f in files[p1:p2]]\n        \n        img3d = np.stack(image_stack).T\n        if img3d.shape[-1] < num_imgs:\n            n_zero = np.zeros((img_size, img_size, num_imgs - img3d.shape[-1]))\n            img3d = np.concatenate((img3d, n_zero), axis=-1)\n#         print(img3d.shape)\n        if np.min(img3d) < np.max(img3d):\n            img3d = img3d - np.min(img3d)\n            img3d = img3d \/ np.max(img3d)\n\n        return np.expand_dims(img3d, 0)\n\n","04798867":"ls ..\/input\/","d80806ed":"import monai\n\n# model \nmodel = monai.networks.nets.resnet10(spatial_dims=3, n_input_channels=1, n_classes=1)\ndevice = torch.device(\"cuda\")\nmodel.to(device);\nall_weights = os.listdir(\"..\/input\/resnet10rsna\")\nfold_files = [f for f in all_weights if type_ in f]\n# print(np.array(fold_files).shape)\ncriterion = nn.BCEWithLogitsLoss()","55d0becd":"fold_files","39cedbdf":"sample = pd.read_csv(\"..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/sample_submission.csv\")","0b525461":"tta_true_labels = []\ntta_preds = []\ntest_dataset = BrainRSNADataset(data=sample, mri_type=type_, is_train=False)\ntest_dl = torch.utils.data.DataLoader(\n        test_dataset, batch_size=8, shuffle=False, num_workers=4\n    )\n\npreds_f = np.zeros(len(sample))\nfor fold in range(5):\n    image_ids = []\n    model.load_state_dict(torch.load(f\"..\/input\/resnet10rsna\/{fold_files[fold]}\"))\n    preds = []\n    epoch_iterator_test = tqdm(test_dl)\n    with torch.no_grad():\n        for  step, batch in enumerate(epoch_iterator_test):\n            model.eval()\n            images = batch[\"image\"].to(device)\n            print(batch[\"image\"].shape)\n            outputs = model(images)\n            preds.append(outputs.sigmoid().detach().cpu().numpy())\n            image_ids.append(batch[\"case_id\"].detach().cpu().numpy())\n    \n\n    preds_f += np.vstack(preds).T[0]\/5\n\n    ids_f = np.hstack(image_ids)","b252b20e":"# print(np.array(test_dataset).shape)\ntest_dataset[40]['image'].shape","6d226547":"sample[\"BraTS21ID\"] = ids_f\nsample[\"MGMT_value\"] = preds_f","2c3aea52":"submission2 = sample.sort_values(by=\"BraTS21ID\").reset_index(drop=True)","53131ccf":"# submission2.to_csv(\"submission.csv\", index=False)","48e16e7a":"submission2","53b45507":"# ====================================================\n# Directory settings\n# ====================================================\nimport os\n\nOUTPUT_DIR = '.\/'\nif not os.path.exists(OUTPUT_DIR):\n    os.makedirs(OUTPUT_DIR)\n    \n# TRAIN_PATH = '..\/input\/rsna-miccai-png\/train'\nTEST_PATH = '..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/test'","7a460e37":"# ====================================================\n# CFG\n# ====================================================\nclass CFG:\n    debug=False\n    num_workers=8\n    model_name='efficientnet_b3'\n    size=512\n    batch_size=4\n    seed=42\n    target_size=2\n    target_col='MGMT_value'\n    n_fold=5\n    trn_fold=[0,1,2,3,4]\n    inference=True","3e95a2c1":"# ====================================================\n# Imports\n# ====================================================\nimport sys\nsys.path.append('..\/input\/pytorch-image-models\/pytorch-image-models-master')\n\nimport os\nimport math\nimport time\nimport random\nimport shutil\nfrom pathlib import Path\nfrom contextlib import contextmanager\nfrom collections import defaultdict, Counter\n\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom tqdm.auto import tqdm\nfrom functools import partial\n\nimport cv2\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD\nimport torchvision.models as models\nfrom torch.nn.parameter import Parameter\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\n\nimport timm\n\nimport warnings \nwarnings.filterwarnings('ignore')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","bde9da48":"def seed_torch(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nseed_torch(seed=CFG.seed)","aeff3f04":"test = os.listdir('..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/test')\n# print(np.array(test).shape)\ntest = pd.DataFrame({'BraTS21ID' : test})\n# print(test)\ntest['BraTS21ID'] = test['BraTS21ID'].astype(int)\ntest","4d9112b7":"# ====================================================\n# Dataset\n# ====================================================\nclass TestDataset(Dataset):\n    def __init__(self, df, transform=None):\n        self.df = df\n        self.file_names = df['BraTS21ID'].values\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        root = f'{TEST_PATH}\/{str(self.file_names[idx]).zfill(5)}\/'\n        com = []\n        for typ in ['FLAIR', 'T1w', 'T1wCE', 'T2w']:\n            paths = os.listdir(root + typ)\n            rnd = random.sample(paths, min(10, len(paths)))\n            typ_imgs = []\n            for f in rnd:\n                file_path = f'{root}{typ}\/{f}'\n                dicom = pydicom.read_file(file_path)\n                data = apply_voi_lut(dicom.pixel_array, dicom)\n                if dicom.PhotometricInterpretation == \"MONOCHROME1\":\n                    data = np.amax(data) - data\n                data = data - np.min(data)\n                data = data \/ np.max(data)\n                image = (data * 255).astype(np.uint8)\n                typ_imgs.append(cv2.resize(image, (CFG.size, CFG.size)))\n            com.append(np.mean(typ_imgs, axis = 0))\n        image = np.array(com).transpose((1,2,0)) \/ 255\n        if self.transform:\n            augmented = self.transform(image=image)\n            image = augmented['image']\n            image = image.float()\n        return image","8acddd00":"# ====================================================\n# Transforms\n# ====================================================\ndef get_transforms(*, data):\n    if data == 'valid':\n        return A.Compose([\n            ToTensorV2(),\n        ])","a81ace76":"# ====================================================\n# MODEL\n# ====================================================\nclass CustomEfficientNet(nn.Module):\n    def __init__(self, model_name=CFG.model_name, pretrained=False):\n        super().__init__()\n        self.conv = nn.Conv2d(4,3,1)\n        self.model = timm.create_model(CFG.model_name, pretrained=pretrained)\n        n_features = self.model.classifier.in_features\n#         print(n_features)\n        self.model.classifier = nn.Linear(n_features, CFG.target_size)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.model(x)\n        return x","89aa0471":"# ====================================================\n# Helper functions\n# ====================================================\ndef load_state_eff(model_path):\n    state_dict = torch.load(model_path)['model']\n    return state_dict\n\ndef inference(model_eff, states, test_loader, device):\n    model_eff.to(device)\n    tk0 = tqdm(enumerate(test_loader), total=len(test_loader))\n    probs = []\n    for i, (images) in tk0:\n        images = images.to(device)\n        avg_preds = []\n        x=1\n        for state in states:\n            model_eff.load_state_dict(state)\n            model_eff.eval()\n            with torch.no_grad():\n                y_preds = model_eff(images)\n#                 print(images.shape) # (4, 4, 512, 512)\n                print(y_preds.shape) # (4, 2)\n            avg_preds.append(y_preds.softmax(1).to('cpu').numpy()) # append (4, 2) 5 times \n            x+=1\n#         print(np.array(avg_preds).shape)    # (5, 4, 2) -> 5 folds \n        avg_preds = np.mean(avg_preds, axis=0)\n        probs.append(avg_preds)\n    probs = np.concatenate(probs)\n    return probs","dd064607":"model_eff = CustomEfficientNet(CFG.model_name, pretrained=False)\n\nstates = [load_state_eff('..\/input\/rsnatraining\/efficientnet_b3_fold0_best.pth'),\n          load_state_eff('..\/input\/rsnatraining\/efficientnet_b3_fold1_best.pth'),\n          load_state_eff('..\/input\/rsnatraining\/efficientnet_b3_fold2_best.pth'),\n          load_state_eff('..\/input\/rsnatraining\/efficientnet_b3_fold3_best.pth'),\n          load_state_eff('..\/input\/rsnatraining\/efficientnet_b3_fold4_best.pth'),\n]\n","f3d2fc55":"# ====================================================\n# inference\n# ====================================================\n\ntest_dataset = TestDataset(test, transform=get_transforms(data='valid'))\ntest_loader = DataLoader(test_dataset, batch_size=CFG.batch_size, shuffle=False, \n                         num_workers=CFG.num_workers, pin_memory=True)\npredictions = inference(model_eff, states, test_loader, device)\n\n# submission\ntest['MGMT_value'] = predictions[:,1]\n# test[['BraTS21ID', 'MGMT_value']].to_csv(OUTPUT_DIR+'submission.csv', index=False)\n# test.head()\nsubmission3 = test[['BraTS21ID', 'MGMT_value']]\n# submission3.to_csv(OUTPUT_DIR+'submission.csv', index=False)\n# submission3.head()","8d095b8d":"submission3 = submission3.sort_values(by=\"BraTS21ID\").reset_index(drop=True)\nsubmission3","d5a69e28":"import os\nimport sys \nimport json\nimport glob\nimport random\nimport collections\nimport time\nimport re\n\nimport numpy as np\nimport pandas as pd\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nimport cv2\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport torch\nfrom torch import nn\nfrom torch.utils import data as torch_data\nfrom sklearn import model_selection as sk_model_selection\nfrom torch.nn import functional as torch_functional\nimport torch.nn.functional as F\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score","9e805e31":"if os.path.exists(\"..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\"):\n    data_directory = '..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification'\n    pytorch3dpath = \"..\/input\/efficientnetpyttorch3d\/EfficientNet-PyTorch-3D\"\nelse:\n    data_directory = '\/media\/roland\/data\/kaggle\/rsna-miccai-brain-tumor-radiogenomic-classification'\n    pytorch3dpath = \"EfficientNet-PyTorch-3D\"\n    \nmri_types = ['FLAIR','T1w','T1wCE','T2w']\nSIZE = 256\nNUM_IMAGES = 64\n\nsys.path.append(pytorch3dpath)\nfrom efficientnet_pytorch_3d import EfficientNet3D","6c25a12a":"def load_dicom_image(path, img_size=SIZE, voi_lut=True, rotate=0):\n    dicom = pydicom.read_file(path)\n    data = dicom.pixel_array\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n        \n    if rotate > 0:\n        rot_choices = [0, cv2.ROTATE_90_CLOCKWISE, cv2.ROTATE_90_COUNTERCLOCKWISE, cv2.ROTATE_180]\n        data = cv2.rotate(data, rot_choices[rotate])\n        \n    data = cv2.resize(data, (img_size, img_size))\n    return data\n\n\ndef load_dicom_images_3d(scan_id, num_imgs=NUM_IMAGES, img_size=SIZE, mri_type=\"FLAIR\", split=\"train\", rotate=0):\n\n    files = sorted(glob.glob(f\"{data_directory}\/{split}\/{scan_id}\/{mri_type}\/*.dcm\"), \n               key=lambda var:[int(x) if x.isdigit() else x for x in re.findall(r'[^0-9]|[0-9]+', var)])\n\n    middle = len(files)\/\/2\n    num_imgs2 = num_imgs\/\/2\n    p1 = max(0, middle - num_imgs2)\n    p2 = min(len(files), middle + num_imgs2)\n    img3d = np.stack([load_dicom_image(f, rotate=rotate) for f in files[p1:p2]]).T \n    if img3d.shape[-1] < num_imgs:\n        n_zero = np.zeros((img_size, img_size, num_imgs - img3d.shape[-1]))\n        img3d = np.concatenate((img3d,  n_zero), axis = -1)\n        \n    if np.min(img3d) < np.max(img3d):\n        img3d = img3d - np.min(img3d)\n        img3d = img3d \/ np.max(img3d)\n            \n    return np.expand_dims(img3d,0)\n\na = load_dicom_images_3d(\"00000\")\nprint(a.shape)\nprint(np.min(a), np.max(a), np.mean(a), np.median(a))","f18f8aac":"def set_seed(seed):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n\nset_seed(12)","ff44a44a":"train_df = pd.read_csv(f\"{data_directory}\/train_labels.csv\")\ndisplay(train_df)\n\ndf_train, df_valid = sk_model_selection.train_test_split(\n    train_df, \n    test_size=0.2, \n    random_state=12, \n    stratify=train_df[\"MGMT_value\"],\n)\n","23417261":"df_train.tail()","07767420":"class Dataset(torch_data.Dataset):\n    def __init__(self, paths, targets=None, mri_type=None, label_smoothing=0.01, split=\"train\", augment=False):\n        self.paths = paths\n        self.targets = targets\n        self.mri_type = mri_type\n        self.label_smoothing = label_smoothing\n        self.split = split\n        self.augment = augment\n          \n    def __len__(self):\n        return len(self.paths)\n    \n    def __getitem__(self, index):\n        scan_id = self.paths[index]\n        if self.targets is None:\n            data = load_dicom_images_3d(str(scan_id).zfill(5), mri_type=self.mri_type[index], split=self.split)\n        else:\n            if self.augment:\n                rotation = np.random.randint(0,4)\n            else:\n                rotation = 0\n\n            data = load_dicom_images_3d(str(scan_id).zfill(5), mri_type=self.mri_type[index], split=\"train\", rotate=rotation)\n\n        if self.targets is None:\n            return {\"X\": torch.tensor(data).float(), \"id\": scan_id}\n        else:\n            y = torch.tensor(abs(self.targets[index]-self.label_smoothing), dtype=torch.float)\n            return {\"X\": torch.tensor(data).float(), \"y\": y}\n","b0e569b0":"!pip install ..\/input\/einops-030\/einops-0.3.0-py2.py3-none-any.whl","280e7740":"from einops import rearrange, repeat\nfrom einops.layers.torch import Rearrange\n\n# helpers\n\ndef pair(t):\n    return t if isinstance(t, tuple) else (t, t)\n\n# classes\n\nclass PreNorm(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n        self.fn = fn\n    def forward(self, x, **kwargs):\n        return self.fn(self.norm(x), **kwargs)\n\nclass FeedForward(nn.Module):\n    def __init__(self, dim, hidden_dim, dropout = 0.):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(dim, hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, dim),\n            nn.Dropout(dropout)\n        )\n    def forward(self, x):\n        return self.net(x)\n\nclass Attention(nn.Module):\n    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n        super().__init__()\n        inner_dim = dim_head *  heads\n        project_out = not (heads == 1 and dim_head == dim)\n\n        self.heads = heads\n        self.scale = dim_head ** -0.5\n\n        self.attend = nn.Softmax(dim = -1)\n        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim),\n            nn.Dropout(dropout)\n        ) if project_out else nn.Identity()\n\n    def forward(self, x):\n        qkv = self.to_qkv(x).chunk(3, dim = -1)\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n\n        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n\n        attn = self.attend(dots)\n\n        out = torch.matmul(attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass Transformer(nn.Module):\n    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n        mlp_dim = 2048\n        for _ in range(depth):\n            #print (dim, mlp_dim)\n            self.layers.append(nn.ModuleList([\n                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n            ]))\n    def forward(self, x):\n        for attn, ff in self.layers:\n            x = attn(x) + x\n            x = ff(x) + x\n        return x\n\n# class ViT(nn.Module):\n#     def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n#         super().__init__()\n#         image_height, image_width = pair(image_size)\n#         patch_height, patch_width = pair(patch_size)\n\n#         assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n\n#         num_patches = (image_height \/\/ patch_height) * (image_width \/\/ patch_width)\n#         patch_dim = channels * patch_height * patch_width\n#         assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n\n#         self.to_patch_embedding = nn.Sequential(\n#             Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n#             nn.Linear(patch_dim, dim),\n#         )\n\n#         self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n#         self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n#         self.dropout = nn.Dropout(emb_dropout)\n\n#         self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n\n#         self.pool = pool\n#         self.to_latent = nn.Identity()\n\n#         self.mlp_head = nn.Sequential(\n#             nn.LayerNorm(dim),\n#             nn.Linear(dim, num_classes)\n#         )\n\n#     def forward(self, img):\n#         x = self.to_patch_embedding(img)\n#         b, n, _ = x.shape\n\n#         cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n#         x = torch.cat((cls_tokens, x), dim=1)\n#         x += self.pos_embedding[:, :(n + 1)]\n#         x = self.dropout(x)\n\n#         x = self.transformer(x)\n\n#         x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n\n#         x = self.to_latent(x)\n#         return self.mlp_head(x)","5d8ee0cd":"class Model(nn.Module):\n    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels = 3, dropout = 0., emb_dropout = 0.):\n        super().__init__()\n        assert image_size % patch_size == 0, 'image dimensions must be divisible by the patch size'\n        num_patches = (image_size \/\/ patch_size) *(image_size \/\/ patch_size)* 2\n        patch_dim = channels * patch_size ** 3\n\n        self.patch_size = patch_size\n\n        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n        self.patch_to_embedding = nn.Linear(patch_dim, dim)\n        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n        self.dropout = nn.Dropout(emb_dropout)\n        #print (mlp_dim)\n        self.transformer = Transformer(dim, depth, heads, mlp_dim, dropout)\n        #print (dim)\n        self.to_cls_token = nn.Identity()\n\n        self.mlp_head = nn.Sequential(\n            nn.LayerNorm(dim),\n            nn.Linear(dim, mlp_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(mlp_dim, num_classes),\n            nn.Dropout(dropout)\n        )\n\n    def forward(self, img, mask = None):\n        p = self.patch_size\n        #print (img.shape)\n        x = rearrange(img, 'b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1 = p, p2 = p, p3 = p)\n        #print (x.shape)\n        x = self.patch_to_embedding(x)\n        #print (x.shape)\n        cls_tokens = self.cls_token.expand(img.shape[0], -1, -1)\n        #print (cls_tokens.shape)\n        x = torch.cat((cls_tokens, x), dim=1)\n        #print (x.shape)\n        #print (self.pos_embedding.shape)\n        x += self.pos_embedding\n        x = self.dropout(x)\n\n        x = self.transformer(x)\n\n        x = self.to_cls_token(x[:, 0])\n        return self.mlp_head(x)","5528e8e3":"class Trainer:\n    def __init__(\n        self, \n        model, \n        device, \n        optimizer, \n        criterion\n    ):\n        self.model = model\n        self.device = device\n        self.optimizer = optimizer\n        self.criterion = criterion\n\n        self.best_valid_score = np.inf\n        self.n_patience = 0\n        self.lastmodel = None\n        \n    def fit(self, epochs, train_loader, valid_loader, save_path, patience):        \n        for n_epoch in range(1, epochs + 1):\n            self.info_message(\"EPOCH: {}\", n_epoch)\n            \n            train_loss, train_time = self.train_epoch(train_loader)\n            valid_loss, valid_auc, valid_time = self.valid_epoch(valid_loader)\n            \n            self.info_message(\n                \"[Epoch Train: {}] loss: {:.4f}, time: {:.2f} s            \",\n                n_epoch, train_loss, train_time\n            )\n            \n            self.info_message(\n                \"[Epoch Valid: {}] loss: {:.4f}, auc: {:.4f}, time: {:.2f} s\",\n                n_epoch, valid_loss, valid_auc, valid_time\n            )\n\n            # if True:\n            #if self.best_valid_score < valid_auc: \n            if self.best_valid_score > valid_loss: \n                self.save_model(n_epoch, save_path, valid_loss, valid_auc)\n                self.info_message(\n                     \"auc improved from {:.4f} to {:.4f}. Saved model to '{}'\", \n                    self.best_valid_score, valid_loss, self.lastmodel\n                )\n                self.best_valid_score = valid_loss\n                self.n_patience = 0\n            else:\n                self.n_patience += 1\n            \n            if self.n_patience >= patience:\n                self.info_message(\"\\nValid auc didn't improve last {} epochs.\", patience)\n                break\n            \n    def train_epoch(self, train_loader):\n        self.model.train()\n        t = time.time()\n        sum_loss = 0\n\n        for step, batch in enumerate(train_loader, 1):\n            X = batch[\"X\"].to(self.device)\n            targets = batch[\"y\"].to(self.device)\n            self.optimizer.zero_grad()\n            outputs = self.model(X).squeeze(1)\n            \n            loss = self.criterion(outputs, targets)\n            loss.backward()\n\n            sum_loss += loss.detach().item()\n\n            self.optimizer.step()\n            \n            message = 'Train Step {}\/{}, train_loss: {:.4f}'\n            self.info_message(message, step, len(train_loader), sum_loss\/step, end=\"\\r\")\n        \n        return sum_loss\/len(train_loader), int(time.time() - t)\n    \n    def valid_epoch(self, valid_loader):\n        self.model.eval()\n        t = time.time()\n        sum_loss = 0\n        y_all = []\n        outputs_all = []\n\n        for step, batch in enumerate(valid_loader, 1):\n            with torch.no_grad():\n                X = batch[\"X\"].to(self.device)\n                targets = batch[\"y\"].to(self.device)\n\n                outputs = self.model(X).squeeze(1)\n                loss = self.criterion(outputs, targets)\n\n                sum_loss += loss.detach().item()\n                y_all.extend(batch[\"y\"].tolist())\n                outputs_all.extend(torch.sigmoid(outputs).tolist())\n\n            message = 'Valid Step {}\/{}, valid_loss: {:.4f}'\n            self.info_message(message, step, len(valid_loader), sum_loss\/step, end=\"\\r\")\n            \n        y_all = [1 if x > 0.5 else 0 for x in y_all]\n        auc = roc_auc_score(y_all, outputs_all)\n        \n        return sum_loss\/len(valid_loader), auc, int(time.time() - t)\n    \n    def save_model(self, n_epoch, save_path, loss, auc):\n        self.lastmodel = f\"{save_path}-best.pth\"\n        torch.save(\n            {\n                \"model_state_dict\": self.model.state_dict(),\n                \"optimizer_state_dict\": self.optimizer.state_dict(),\n                \"best_valid_score\": self.best_valid_score,\n                \"n_epoch\": n_epoch,\n            },\n            self.lastmodel,\n        )\n    \n    @staticmethod\n    def info_message(message, *args, end=\"\\n\"):\n        print(message.format(*args), end=end)","9766ba5b":"# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# def train_mri_type(df_train, df_valid, mri_type):\n#     if mri_type==\"all\":\n#         train_list = []\n#         valid_list = []\n#         for mri_type in mri_types:\n#             df_train.loc[:,\"MRI_Type\"] = mri_type\n#             train_list.append(df_train.copy())\n#             df_valid.loc[:,\"MRI_Type\"] = mri_type\n#             valid_list.append(df_valid.copy())\n\n#         df_train = pd.concat(train_list)\n#         df_valid = pd.concat(valid_list)\n#     else:\n#         df_train.loc[:,\"MRI_Type\"] = mri_type\n#         df_valid.loc[:,\"MRI_Type\"] = mri_type\n\n#     print(df_train.shape, df_valid.shape)\n#     display(df_train.head())\n    \n#     train_data_retriever = Dataset(\n#         df_train[\"BraTS21ID\"].values, \n#         df_train[\"MGMT_value\"].values, \n#         df_train[\"MRI_Type\"].values,\n#         augment=True\n#     )\n\n#     valid_data_retriever = Dataset(\n#         df_valid[\"BraTS21ID\"].values, \n#         df_valid[\"MGMT_value\"].values,\n#         df_valid[\"MRI_Type\"].values\n#     )\n\n#     train_loader = torch_data.DataLoader(\n#         train_data_retriever,\n#         batch_size=4,\n#         shuffle=True,\n#         num_workers=8,pin_memory = True\n#     )\n\n#     valid_loader = torch_data.DataLoader(\n#         valid_data_retriever, \n#         batch_size=4,\n#         shuffle=False,\n#         num_workers=8,pin_memory = True\n#     )\n\n#     model = Model(\n#         image_size = 256,\n#         patch_size = 32,\n#         num_classes = 1,\n#         dim = 1024,\n#         depth = 2,\n#         heads = 16,\n#         mlp_dim = 2048,\n#         channels = 1,\n#         dropout = 0.1,\n#         emb_dropout = 0.1\n#     )\n#     model.to(device)\n\n#     #checkpoint = torch.load(\"best-model-all-auc0.555.pth\")\n#     #model.load_state_dict(checkpoint[\"model_state_dict\"])\n\n#     #print(model)\n\n#     optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n#     #optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n\n#     criterion = torch_functional.binary_cross_entropy_with_logits\n\n#     trainer = Trainer(\n#         model, \n#         device, \n#         optimizer, \n#         criterion\n#     )\n\n#     history = trainer.fit(\n#         10, \n#         train_loader, \n#         valid_loader, \n#         f\"{mri_type}\", \n#         10,\n#     )\n    \n#     return trainer.lastmodel\n\n# modelfiles = None\n\n# if not modelfiles:\n#     modelfiles = [train_mri_type(df_train, df_valid, m) for m in mri_types]\n#     print(modelfiles)","e7e6e2c3":"def predict(modelfile, df, mri_type, split):\n    print(\"Predict:\", modelfile, mri_type, df.shape)\n    df.loc[:,\"MRI_Type\"] = mri_type\n    data_retriever = Dataset(\n        df.index.values, \n        mri_type=df[\"MRI_Type\"].values,\n        split=split\n    )\n\n    data_loader = torch_data.DataLoader(\n        data_retriever,\n        batch_size=4,\n        shuffle=False,\n        num_workers=8,\n    )\n   \n    model = Model(\n        image_size = 256,\n        patch_size = 32,\n        num_classes = 1,\n        dim = 1024,\n        depth = 2,\n        heads = 16,\n        mlp_dim = 2048,\n        channels = 1,\n        dropout = 0.1,\n        emb_dropout = 0.1\n    )\n    model.to(device)\n    \n    checkpoint = torch.load(modelfile)\n    model.load_state_dict(checkpoint[\"model_state_dict\"])\n    model.eval()\n    \n    y_pred = []\n    ids = []\n\n    for e, batch in enumerate(data_loader,1):\n        print(f\"{e}\/{len(data_loader)}\", end=\"\\r\")\n        with torch.no_grad():\n            tmp_pred = torch.sigmoid(model(batch[\"X\"].to(device))).cpu().numpy().squeeze()\n            if tmp_pred.size == 1:\n                y_pred.append(tmp_pred)\n            else:\n                y_pred.extend(tmp_pred.tolist())\n            ids.extend(batch[\"id\"].numpy().tolist())\n            \n    preddf = pd.DataFrame({\"BraTS21ID\": ids, \"MGMT_value\": y_pred}) \n    preddf = preddf.set_index(\"BraTS21ID\")\n    return preddf","6613495c":"# df_valid = df_valid.set_index(\"BraTS21ID\")\n# df_valid[\"MGMT_pred\"] = 0\n# for m, mtype in zip(modelfiles,  mri_types):\n#     pred = predict(m, df_valid, mtype, \"train\")\n#     df_valid[\"MGMT_pred\"] += pred[\"MGMT_value\"]\n# df_valid[\"MGMT_pred\"] \/= len(modelfiles)\n# auc = roc_auc_score(df_valid[\"MGMT_value\"], df_valid[\"MGMT_pred\"])\n# print(f\"Validation ensemble AUC: {auc:.4f}\")\n# sns.displot(df_valid[\"MGMT_pred\"])","7981e83e":"# submission4 = pd.read_csv(f\"{data_directory}\/sample_submission.csv\", index_col=\"BraTS21ID\")\n\n# submission4[\"MGMT_value\"] = 0\n# for m, mtype in zip(modelfiles, mri_types):\n#     pred = predict(m, submission4, mtype, split=\"test\")\n#     submission[\"MGMT_value\"] += pred[\"MGMT_value\"]\n\n# submission4[\"MGMT_value\"] \/= len(modelfiles)\n# submission4[\"MGMT_value\"].to_csv(\"submission.csv\")","e8d62840":"# submission4","72e42cdc":"# sns.displot(submissio4n[\"MGMT_value\"])","38551d8b":"final = pd.read_csv(\"..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/sample_submission.csv\")\nfinal.head()","d03bfa1a":"final[\"MGMT_value\"] = (submission1[\"MGMT_value\"] + submission2[\"MGMT_value\"] + submission3[\"MGMT_value\"])\/3","5a93f09d":"final","9a56d0b4":"final.to_csv(\"submission.csv\", index=False)\nfinal","cfa0ad15":"# temp = pd.read_csv(\"..\/input\/brain-tumor-test\/train1_actual_labels.csv\")\n# actual_labels = temp[\"MGMT_value\"].values\n# predicted_labels = final[\"MGMT_value\"].values\n# temp","3ec0f8fc":"# pred = []\n# for i in predicted_labels:\n#     if i<0.5:\n#         pred.append(0)\n#     else:\n#         pred.append(1)\n","e4b57c48":"# from sklearn.metrics import precision_score\n# precision_score(actual_labels, pred, average=\"binary\")","dedaf87c":"# submission1","137f4197":"# submission2","84a0739e":"# submission3","48b2956f":"# submission3 = submission3.sort_values(by = 'BraTS21ID')\n# submission3","360ba8e1":"# final[\"MGMT_value\"] = (submission1[\"MGMT_value\"] + submission2[\"MGMT_value\"] + submission3[\"MGMT_value\"])\/3","a2b0c5c7":"# precision_score(actual_labels, pred, average=\"binary\")","fa097482":"# Model3 random","daa9e3ee":"# Model2 Fire Baba","23f2b0f7":"# Final Submission","9eb703ed":"# Model4 [MUEN]funkyboy"}}