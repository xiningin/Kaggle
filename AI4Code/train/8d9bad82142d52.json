{"cell_type":{"db4ae337":"code","c60573ed":"code","5b01ef2c":"code","5cd7c495":"code","f858b115":"code","b0f9a311":"code","7c83ec10":"code","08c20154":"code","b38a0188":"code","f0b0c59a":"code","039ae366":"code","0bfa108a":"code","37375d20":"code","2c02890a":"code","1a545a99":"code","7868cd36":"code","08ce16db":"code","39e6d232":"code","22b4c5fc":"code","6e9cf1bd":"code","dc2daa4f":"code","df9848f0":"code","370cd370":"code","6ff293fe":"code","83a654ac":"code","557f3677":"code","f9a6f32b":"code","d3ed2433":"code","81d11b46":"code","816702a0":"code","5eb0f65a":"code","3f057753":"code","087580dc":"code","5917a6dc":"code","f90f9161":"code","04653520":"code","93717ba0":"code","6abdfaf6":"code","8b38a85d":"code","93716e7e":"code","860f9a83":"code","0aa3a0d5":"markdown","94addda7":"markdown","5967b41b":"markdown","5f111779":"markdown","e4b9fb76":"markdown","3aa69e18":"markdown","a7a8de7b":"markdown","de64f51c":"markdown","8dadc4ec":"markdown","62a91df8":"markdown","f7f0ad5e":"markdown","a59f381e":"markdown","516fe9c9":"markdown","dafc2b0b":"markdown","9b2e932a":"markdown","1993776e":"markdown","c27834f5":"markdown","9697dd8d":"markdown","4070e1aa":"markdown","51855938":"markdown","beee3316":"markdown","af62b6c1":"markdown","01f27e38":"markdown","b76a4005":"markdown","dfcd44ad":"markdown","7398ade1":"markdown","0fcc497a":"markdown","d0d05e72":"markdown","8a7a9e36":"markdown","d8d335a0":"markdown","dd0175ff":"markdown","c84a78d7":"markdown","7912dcb7":"markdown","8d466b39":"markdown","15b296b6":"markdown","b04d6d30":"markdown","a975c849":"markdown","02f9c90b":"markdown","a5b1478d":"markdown","74d7ad1f":"markdown"},"source":{"db4ae337":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\n\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.preprocessing import OneHotEncoder, PowerTransformer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom category_encoders import TargetEncoder\nfrom scipy import stats\n\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.linear_model import Lasso\nfrom lightgbm import LGBMRegressor\n\nimport warnings\nwarnings.simplefilter('ignore')\nprint('Setup complete')","c60573ed":"# Read data\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntrain.head()","5b01ef2c":"def plot_numerical(col, discrete=False):\n    if discrete:\n        fig, ax = plt.subplots(1,2,figsize=(12,6))\n        sns.stripplot(x=col, y='SalePrice', data=train, ax=ax[0])\n        sns.countplot(train[col], ax=ax[1])\n        fig.suptitle(str(col) + ' analysis')\n    else:\n        fig, ax = plt.subplots(1,2,figsize=(12,6))\n        sns.scatterplot(x=col, y='SalePrice', data=train, ax=ax[0])\n        sns.distplot(train[col], kde=False, ax=ax[1])\n        fig.suptitle(str(col) + ' analysis')\n\ndef plot_categorical(col):\n    fig, ax = plt.subplots(1,2,figsize=(12,6), sharey=True)\n    sns.stripplot(x=col, y='SalePrice', data=train, ax=ax[0])\n    sns.boxplot(x=col, y='SalePrice', data=train, ax=ax[1])\n    fig.suptitle(str(col) + ' analysis')\n    \nprint('plot_numerical() & plot_categorical() are ready to use')","5cd7c495":"plt.figure(figsize=(8,5))\na = sns.distplot(train.SalePrice, kde=False)\nplt.title('SalePrice distribution')\na = plt.axvline(train.SalePrice.describe()['25%'], color='b')\na = plt.axvline(train.SalePrice.describe()['75%'], color='b')\nprint('SalePrice description:')\nprint(train.SalePrice.describe().to_string())","f858b115":"# Select numerical features only\nnum_features = [col for col in train.columns if train[col].dtype in ['int64', 'float64']]\n# Remove Id & SalePrice \nnum_features.remove('Id')\nnum_features.remove('SalePrice')\n# Create a numerical columns only dataframe\nnum_analysis = train[num_features].copy()\n# Impute missing values with the median just for the moment\nfor col in num_features:\n    if num_analysis[col].isnull().sum() > 0:\n        num_analysis[col] = SimpleImputer(strategy='median').fit_transform(num_analysis[col].values.reshape(-1,1))\n# Train a model   \nclf = ExtraTreesRegressor(random_state=42)\nh = clf.fit(num_analysis, train.SalePrice)\nfeature_imp = pd.DataFrame(sorted(zip(clf.feature_importances_,num_features)), columns=['Value','Feature'])\nplt.figure(figsize=(16,10))\nsns.barplot(x='Value', y='Feature', data=feature_imp.sort_values(by='Value', ascending=False))\nplt.title('Most important numerical features with ExtraTreesRegressor')\ndel clf, h;","b0f9a311":"plt.figure(figsize=(8,8))\nplt.title('Correlation matrix with SalePrice')\nselected_columns = ['OverallQual', 'GarageCars', 'GrLivArea', 'YearBuilt', 'FullBath', '1stFlrSF', 'TotalBsmtSF', 'GarageArea']\na = sns.heatmap(train[selected_columns + ['SalePrice']].corr(), annot=True, square=True)","7c83ec10":"plot_numerical('OverallQual', True)","08c20154":"plot_numerical('GarageCars', True)","b38a0188":"plot_numerical('GrLivArea')","f0b0c59a":"plot_numerical('YearBuilt')","039ae366":"plot_numerical('FullBath', True)","0bfa108a":"plot_numerical('1stFlrSF')","37375d20":"plot_numerical('TotalBsmtSF')","2c02890a":"plot_numerical('GarageArea')","1a545a99":"# Select categorical features only\ncat_features = [col for col in train.columns if train[col].dtype =='object']\n# Create a categorical columns only dataframe\ncat_analysis = train[cat_features].copy()\n# Impute missing values with NA just for the moment\nfor col in cat_analysis:\n    if cat_analysis[col].isnull().sum() > 0:\n        cat_analysis[col] = SimpleImputer(strategy='constant').fit_transform(cat_analysis[col].values.reshape(-1,1))\n# Target encoding\ntarget_enc = TargetEncoder(cols=cat_features)\ncat_analysis = target_enc.fit_transform(cat_analysis, train.SalePrice) \n# Train a model \nclf = ExtraTreesRegressor(random_state=42)\nh = clf.fit(cat_analysis, train.SalePrice)\nfeature_imp = pd.DataFrame(sorted(zip(clf.feature_importances_,cat_features)), columns=['Value','Feature'])\nplt.figure(figsize=(16,10))\nsns.barplot(x='Value', y='Feature', data=feature_imp.sort_values(by='Value', ascending=False))\nplt.title('Most important categorical features with ExtraTreesRegressor')\ndel clf, h;","7868cd36":"fig, ax = plt.subplots(1,2,figsize=(16,6), sharey=True)\nsns.stripplot(x='Neighborhood', y='SalePrice', data=train, ax=ax[0])\nsns.boxplot(x='Neighborhood', y='SalePrice', data=train, ax=ax[1])\nax[0].set_xticklabels(ax[0].get_xticklabels(), rotation=90)\nax[1].set_xticklabels(ax[1].get_xticklabels(), rotation=90)\nfig.suptitle('Neighborhood analysis')\nplt.show()","08ce16db":"plot_categorical('ExterQual')","39e6d232":"plot_categorical('BsmtQual')","22b4c5fc":"plot_categorical('KitchenQual')","6e9cf1bd":"fig, ax = plt.subplots(1,2,figsize=(16,6), sharey=True)\ntrain_missing = round(train.isnull().mean()*100, 2)\ntrain_missing = train_missing[train_missing > 0]\ntrain_missing.sort_values(inplace=True)\nsns.barplot(train_missing.index, train_missing, ax=ax[0], color='orange')\nax[0].set_xticklabels(ax[0].get_xticklabels(), rotation=90)\nax[0].set_ylabel('Percentage of missing values')\nax[0].set_title('Train set')\ntest_missing = round(test.isnull().mean()*100, 2)\ntest_missing = test_missing[test_missing > 0]\ntest_missing.sort_values(inplace=True)\nsns.barplot(test_missing.index, test_missing, ax=ax[1], color='orange')\nax[1].set_xticklabels(ax[1].get_xticklabels(), rotation=90)\nax[1].set_title('Test set')\nplt.show()","dc2daa4f":"plot_numerical('LotFrontage')\nprint('LotFrontage minimum:', train.LotFrontage.min())","df9848f0":"plot_categorical('FireplaceQu')","370cd370":"fig, ax = plt.subplots(2,2,figsize=(12,10), sharey=True)\nsns.stripplot(x='Fence', y='SalePrice', data=train, ax=ax[0][0])\nsns.stripplot(x='Alley', y='SalePrice', data=train, ax=ax[0][1])\nsns.stripplot(x='MiscFeature', y='SalePrice', data=train, ax=ax[1][0])\nsns.stripplot(x='PoolQC', y='SalePrice', data=train, ax=ax[1][1])\nfig.suptitle('Analysis of columns with more than 80% of missing values')\nplt.show()","6ff293fe":"# Fill high % missing columns with NA\nfor col in ['FireplaceQu', 'Fence', 'Alley', 'MiscFeature', 'PoolQC']:\n    train[col].fillna('NotAvailable', inplace=True)\n    test[col].fillna('NotAvailable', inplace=True)\n    \n# Fill Garage variables with NA\nfor col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n    train[col].fillna('NotAvailable', inplace=True)\n    test[col].fillna('NotAvailable', inplace=True)\n    \n# Fill GarageYrBlt with a new value\n# I chose the min over train + test set minus 1\ngarage_min = min(train['GarageYrBlt'].min(), test['GarageYrBlt'].min()) -1\ntrain['GarageYrBlt'].fillna(garage_min, inplace=True)\ntest['GarageYrBlt'].fillna(garage_min, inplace=True)\n\n# Fill these Garage columns with 0\nfor col in ['GarageArea', 'GarageCars']:\n    train[col].fillna(0, inplace=True)\n    test[col].fillna(0, inplace=True)\n    \n# Fill numerical Bsmt variables with 0\nfor col in ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath']:\n    train[col].fillna(0, inplace=True)\n    test[col].fillna(0, inplace=True)\n    \n# Fill categorical Bsmt variables with NA  \nfor col in ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']:\n    train[col].fillna('NotAvailable', inplace=True)\n    test[col].fillna('NotAvailable', inplace=True)\n    \n# Fill low % missing columns with the mode\nfor col in ['MasVnrType', 'MSZoning', 'Functional', 'Electrical', 'KitchenQual', 'Exterior1st',\n           'Exterior2nd', 'SaleType','MSSubClass', 'Utilities', 'MasVnrArea']:\n    imputer = SimpleImputer(strategy='most_frequent')\n    train[col] = imputer.fit_transform(train[col].values.reshape(-1,1))\n    test[col] = imputer.transform(test[col].values.reshape(-1,1))\n    \n# Fill LotFrontage with KNNImputer on LotArea\nimputer = KNNImputer(n_neighbors=5)\ntrain[['LotFrontage', 'LotArea']] = imputer.fit_transform(train[['LotFrontage', 'LotArea']])\ntest[['LotFrontage', 'LotArea']] = imputer.transform(test[['LotFrontage', 'LotArea']])\n\nprint('Imputation finished!')\nprint('Number of missing values in the train set:', train.isnull().sum().sum())\nprint('Number of missing values in the test set:', test.isnull().sum().sum())","83a654ac":"# Select categorical features\ncat_features = [col for col in train.columns if train[col].dtype =='object']\ncat_features = cat_features + ['MSSubClass','MoSold','YrSold']\n# Select numerical features\nnum_features = [col for col in train.columns if train[col].dtype in ['int64', 'float64'] and col not in cat_features]\n# Remove Id & SalePrice \nnum_features.remove('Id')\nnum_features.remove('SalePrice')\n# Encode categorical features\ntarget_enc = TargetEncoder(cols=cat_features)\ntrain_TE = train[num_features].join(target_enc.fit_transform(train[cat_features], train.SalePrice)) \n# Train a model\nclf = ExtraTreesRegressor(random_state=0)\nh = clf.fit(train_TE, train.SalePrice)\nfeature_imp = pd.DataFrame(sorted(zip(clf.feature_importances_,train_TE)), columns=['Value','Feature'])\nplt.figure(figsize=(20,14))\nsns.barplot(x='Value', y='Feature', data=feature_imp.sort_values(by='Value', ascending=False))\nplt.title('Feature importance with ExtraTreesRegressor')\ndel clf, h;","557f3677":"cols_to_keep = list(feature_imp.sort_values(by='Value', ascending=False).reset_index(drop=True).loc[:59, 'Feature'])\nprint('Keeping the first {:d} most informative features'.format(len(cols_to_keep)))","f9a6f32b":"fig, ax = plt.subplots(2,2,figsize=(12,10), sharey=True)\nplt.suptitle('Outliers analysis')\nsns.scatterplot(x='GrLivArea', y='SalePrice', data=train, ax=ax[0][0])\nsns.scatterplot(x='OverallQual', y='SalePrice', data=train, ax=ax[0][1])\nsns.scatterplot(x='LotFrontage', y='SalePrice', data=train, ax=ax[1][0])\na = sns.scatterplot(x='LotArea', y='SalePrice', data=train, ax=ax[1][1])","d3ed2433":"outliers_id = set(train.loc[train.GrLivArea > 4500, 'Id']) \nprint('Outliers saved in outliers_id')","81d11b46":"# Applies log to all elements of the target column\ny_train = train.loc[~train.Id.isin(outliers_id), 'SalePrice'].apply(lambda x: np.log(x)).reset_index(drop=True)\n# Plot the difference\nplt.figure(figsize=(16,6))\nplt.subplot(1, 2, 1)\nplt.title('Log transformed SalePrice')\nsns.distplot(y_train, fit=stats.norm)\n#Getting the parameters used by the function\n(mu, sigma) = stats.norm.fit(y_train)\nplt.legend(['Normal distr. ($\\mu=${:.2f}, $\\sigma=${:.2f})'.format(mu, sigma)])\nplt.ylabel('Density')\n#QQ-plot\nplt.subplot(1, 2, 2)\na = stats.probplot(y_train, plot=plt)","816702a0":"# Define the train set\nX_train = train.loc[~train.Id.isin(outliers_id), cols_to_keep].reset_index(drop=True)\nX_test = test[cols_to_keep]\n# Intersect cat_features and cols_to_keep\ncat_cols = [col for col in cols_to_keep if col in cat_features]\n# Intersect num_features and cols_to_keep\nnum_cols = [col for col in cols_to_keep if col not in cat_features]\n\n# Lasso preprocessing \nlasso_preprocessor = ColumnTransformer(transformers=[\n        ('num', PowerTransformer(standardize=True), num_cols),\n        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)\n])","5eb0f65a":"oof_lasso = np.zeros(len(X_train))\npreds_lasso = np.zeros(len(X_test))\nout_preds_lasso = np.zeros(2)\n\nkf = KFold(n_splits=5, random_state=42, shuffle=True)\nprint('Training {:d} Lasso models...\\n'.format(kf.n_splits))\n\nfor i, (idxT, idxV) in enumerate(kf.split(X_train, y_train)):\n    print('Fold', i)\n    print(' rows of train =', len(idxT),'rows of holdout =', len(idxV))\n  \n    train_prep = lasso_preprocessor.fit_transform(X_train.loc[idxT])\n    valid_prep = lasso_preprocessor.transform(X_train.loc[idxV])\n    out_prep = lasso_preprocessor.transform(train.loc[train.Id.isin(outliers_id), cols_to_keep])\n    test_prep = lasso_preprocessor.transform(X_test)\n    \n    clf = Lasso(alpha=0.0005)\n    h = clf.fit(train_prep, y_train.loc[idxT])\n    \n    oof_lasso[idxV] += clf.predict(valid_prep)\n    out_preds_lasso += clf.predict(out_prep)\/kf.n_splits\n    preds_lasso += clf.predict(test_prep)\/kf.n_splits\n    \n    del h, clf\n    \nprint ('\\nOOF CV:', mean_squared_error(y_train, oof_lasso, squared=False))","3f057753":"fig, ax = plt.subplots(1,2,figsize=(16,6))\nax[0].set_title('Lasso OOF predictions vs real SalePrice')\nax[0].set_xlabel('OOF predictions')\nsns.scatterplot(np.exp(oof_lasso), np.exp(y_train), ax=ax[0])\nx = np.linspace(0,700000, 100)\nax[0].plot(x, x, 'r-', lw=1)\n# Residuals plot\nax[1].set_title('Residuals')\nsns.scatterplot(x=np.exp(oof_lasso), y=np.exp(y_train)-np.exp(oof_lasso), ax=ax[1])\nax[1].set_ylabel('SalePrice - OOF predictions')\nax[1].set_xlabel('OOF predictions')\nplt.show()","087580dc":"plt.figure(figsize=(8,5))\nplt.title('Train vs predictions comparision')\nsns.scatterplot(x='GrLivArea', y='SalePrice', data=train, label='Real train')\nsns.scatterplot(x=train.loc[train.Id.isin(outliers_id), 'GrLivArea'], y = np.exp(out_preds_lasso), label='Outliers predictions')\na = sns.scatterplot(test.GrLivArea, np.exp(preds_lasso), label='Test predictions')","5917a6dc":"lasso_submission = np.exp(preds_lasso)\nlasso_submission[1089] = train.loc[train.GrLivArea>4500, 'SalePrice'].mean()","f90f9161":"tree_preprocessor = ColumnTransformer(transformers=[\n        ('num', 'passthrough', num_cols),\n        ('cat', TargetEncoder(cols=cat_cols), cat_cols)\n])","04653520":"oof_LGBM = np.zeros(len(X_train))\npreds_LGBM = np.zeros(len(X_test))\nout_preds_LGBM = np.zeros(2)\n\nkf = KFold(n_splits=5, random_state=42, shuffle=True)\nprint('Training {:d} LGBM models...\\n'.format(kf.n_splits))\n\nfor i, (idxT, idxV) in enumerate(kf.split(X_train, y_train)):\n    print('Fold', i)\n    print(' rows of train =', len(idxT),'rows of holdout =', len(idxV))\n  \n    train_prep = tree_preprocessor.fit_transform(X_train.loc[idxT], y_train.loc[idxT])\n    valid_prep = tree_preprocessor.transform(X_train.loc[idxV])\n    out_prep = tree_preprocessor.transform(train.loc[train.Id.isin(outliers_id), cols_to_keep])\n    test_prep = tree_preprocessor.transform(X_test)\n    \n    clf = LGBMRegressor(n_estimators=5000, random_state=42, num_leaves=8, colsample_bytree=0.8)\n    h = clf.fit(train_prep, y_train.loc[idxT], \n                eval_set=[(valid_prep, y_train.loc[idxV])],\n                verbose=300, early_stopping_rounds=200, eval_metric='rmse')\n    \n    oof_LGBM[idxV] += clf.predict(valid_prep)\n    out_preds_LGBM += clf.predict(out_prep)\/kf.n_splits\n    preds_LGBM += clf.predict(test_prep)\/kf.n_splits\n    \n    del h, clf\n    \nprint ('\\nOOF CV', mean_squared_error(y_train, oof_LGBM, squared=False))","93717ba0":"fig, ax = plt.subplots(1,2,figsize=(16,6))\nax[0].set_title('LGBM OOF predictions vs SalePrice')\nax[0].set_xlabel('OOF predictions')\nsns.scatterplot(np.exp(oof_LGBM), np.exp(y_train), ax=ax[0])\nx = np.linspace(0,650000, 100)\nax[0].plot(x, x, 'r-', lw=1)\n# Residuals plot\nax[1].set_title('Residuals')\nsns.scatterplot(x=np.exp(oof_LGBM), y=np.exp(y_train)-np.exp(oof_LGBM), ax=ax[1])\nax[1].set_ylabel('SalePrice - OOF predictions')\nax[1].set_xlabel('OOF predictions')\nplt.show()","6abdfaf6":"plt.figure(figsize=(8,5))\nplt.title('Train vs predictions comparision')\nsns.scatterplot(x='GrLivArea', y='SalePrice', data=train, label='Real train')\nsns.scatterplot(x=train.loc[train.Id.isin(outliers_id), 'GrLivArea'], y = np.exp(out_preds_LGBM), label='Outliers predictions')\na = sns.scatterplot(test.GrLivArea, np.exp(preds_LGBM), label='Test predictions')","8b38a85d":"LGBM_submission = np.exp(preds_LGBM)\nLGBM_submission[1089] = train.loc[train.GrLivArea>4500, 'SalePrice'].mean()","93716e7e":"all = []\nfor w in np.linspace(0,1,101):\n    ensemble_pred = w * oof_lasso + (1-w) * oof_LGBM\n    ensemble_rmsle = mean_squared_error(ensemble_pred, y_train, squared=False)\n    all.append(ensemble_rmsle)\n# Print the best weight\nbest_weight = np.argmin(all)\/100\nprint('Best weight =', best_weight)\n# Plot the ensemble_rmsle against w\nplt.figure(figsize=(8,5))\nplt.title('Ensemble RMSLE against weight $w$')\nplt.xlabel('$w$')\nplt.ylabel('Ensemble RMSLE')\na = sns.scatterplot(x=np.linspace(0,1,101), y=all, color='orange')\nplt.legend(['$w\\cdot$oof_lasso + $(1-w)\\cdot$oof_LGBM'])\nplt.show()","860f9a83":"kaggle_sub = best_weight*lasso_submission + (1-best_weight)*LGBM_submission\n# Save the output\noutput = pd.DataFrame({'Id': test.Id, 'SalePrice': kaggle_sub})\noutput.to_csv('my_submission.csv', index=False)\nprint('Your submission was successfully saved!')","0aa3a0d5":"Let's take a look at the first five rows of the training set: we already see some missing values and that we have an `Id` column that is not useful for predictions.  \nThe target `SalePrice` is the last column and it will not be present in the test set that will thus have 80 columns only. ","94addda7":"We spot four outliers on the right, but also the super expensive houses are really far from the regression line.  \nThe feature is also not normally distributed but has three (or four?) peaks.  \nLet's wrap this part up and move to analyze some categorical variables: I don't want to make the notebook excessively long and provide no useful information. \n# Best categorical feature analysis\nIn this section, we will analyze some of the most useful categorical variables.  \nWe still have missing values to impute and also we have to encode the columns such that a model can use them, so for now I will impute everything with a NA label and use target encoding.","5967b41b":"Again the blue and yellow clouds are perfectly stackable as they should be, but the two red dots are another time overestimated (less than before though).  \nFor this reason, we will proceed with changing the yellow outlier prediction and have our final LGBM submission ready.","5f111779":"We notice a very nice trend, with the minimum generally increasing.  \nHowever, when `OverallQual` is ten, we have two very cheap houses: these are definitely outliers that we should remove later.\n## GarageCars\nNo, they are not giving you a car inside the garage when you buy a house!  \nThis variable is just a way of measuring the size of the garage but in car capacity.  \nIt has actually a slightly better correlation (0.64) with the target than `GarageArea`, even though the values are discrete.","e4b9fb76":"# The target variable Saleprice\nAs you can see below, the sale prices are right-skewed: this was somewhat expected as few people can afford very expensive houses.  \nThe two vertical lines are the first and third quartiles: we see that the core of the distribution is in a `SalePrice` range between 129975 and 214000.  \nFor the moment, let's keep this in mind: we will eventually normalize before modeling.","3aa69e18":"# Feature selection\nNow that we have all the missing values imputed, we still have 79 explanatory variables and we should definitely make a selection to reduce the dimensionality of the dataset.  \nAs we did before, let's train an ExtraTreesRegressor to see the most important numerical and categorical variables combined: we will keep the features whose importance is greater than a certain threshold.  \nAfter a quick analysis, I decided to treat `MSSubClass`, `MoSold`, `YrSold` as categorical from now on because they are of int\/float type but they are not ordinal.","a7a8de7b":"Alongside some rare occurrences, from the plot we clearly see some neighborhoods as NridgHt, Blmngtn, Veenker, StoneBr, NoRidge where the baseline price for a house is over 150000.\n## ExterQual\nThe column name is quite self-explanatory as this variable describes the quality of the material on the exterior.  \nIt is an ordinal variable with \n```\nEx\tExcellent\nGd\tGood\nTA\tAverage\/Typical\nFa\tFair\nPo\tPoor\n```\nand that's exactly what we see in the plots, even if there are multiple outliers present.  \nNote that for every value the interquartile ranges are not overlapping: this is something that will eventually become false while we proceed in this analysis.","de64f51c":"## GarageArea\nThis is the last numerical variable we are looking at.  \nThe column name speaks for itself, so let's just see the plot and the correlation of 0.62 with the target `SalePrice`.  \nAccording to our model, this variable is four times less important than `GarageCars`, so it seems like the other garage feature is more informative.  \nAlso this time, multiple zero values are present as not all houses have a garage.","8dadc4ec":"## BsmtQual\nThis variable evaluates the height of the basement and it is ordinal with\n```\nEx\tExcellent (100+ inches)\t\nGd\tGood (90-99 inches)\nTA\tTypical (80-89 inches)\nFa\tFair (70-79 inches)\nPo\tPoor (<70 inches)\n```\nWe see the trend is correct from the boxplots, so there's nothing to point out here.","62a91df8":"We see that more recent houses generally have a higher price and the trend looks actually quadratic to me, but I may be tricked by the plot.  \nEven though there are some historical houses with a medium-high price, we see that when we talk about expensive houses (>400k) they are all built in the last 20 years except one.  \nWe also see that the minimum price for a house is increasing after the second post-war period with a fairly robust trend.\n## FullBath\nThis column indicates the total number of full bathrooms above grade.  \nIts values are discrete and it is the fifth variable by importance in our tree model.","f7f0ad5e":"Now we can validate our Lasso model using 5-fold cross-validation and extract the out of fold (OOF) predictions as explained [here](https:\/\/www.kaggle.com\/c\/siim-isic-melanoma-classification\/discussion\/175614) to understand where it is wrong.","a59f381e":"## LigthGBM\nNow, let's train a tree-based model: this time, our preprocessing pipeline does not modify the numerical columns and applies target encoding for the categorical attributes.  \nThis variety should definitely help to make different errors.","516fe9c9":"We are done with this part, let's move on to filling missing values. \n# Dealing with missing values\nLet's start by counting the percentage of missing values in the various columns for both train and test data.","dafc2b0b":"The trend looks good, but there are actually not many houses with 0 or 3 bathrooms, so it's a bit tricky.\n## 1stFlrSF\nThis column tells us the size of the first floor in square feet and it totally makes sense that is important for our tree model.  \nIt has a correlation with `SalePrice` of 0.61 too, which is not bad.  \nIf we want to tell all the truth, this variable is partially redundant as incorporated in `GrLivArea`, but it still plays an important role.  \nWe note that the two outliers are present also here and are the same as before.","9b2e932a":"We see that there are more columns with missing values in the test set, but most of them have very few missing entries which is a good thing.  \nApart from that, we recognize the same structure of the train data: we have a small number of missing values in the basement features, a greater amount in the garage features, and around 15% of data is missing in the `LotFrontage` column.  \nWe then have `FireplaceQu` with around 50% of missing values and four columns with more than 80% of missing values.  \nBefore imputing or deleting these columns, let's take a moment to figure out why the data is missing in the columns with a non-negligible amount of null entries.  \n## Basement and Garage variables\nFrom the data description, we see that null values in both basement and garage columns happen because a house does not have these ambients, so the correct way to impute them is not with the most common value but with a missing label (if categorical) or with a new value (if numerical).\n## LotFrontage\nWhat about `LotFrontage`? It should represent the linear feet of the street connected to the property, so a missing value could happen because it was not recorded.  \nLet's plot it so we can understand better if that's the case or not.","1993776e":"Wow, we see a nice parabola, which means great success on our side.  \nOnce we weight the predictions as well, we are ready to submit and climb the LB to the top!","c27834f5":"After some tests, I decided not to delete them forever but just in the training phase: we will see in a moment that in the validation part they give us the possibility to understand some errors and actively correct them to increase the model performances.  \nSo, let's just save their `Id` for now.","9697dd8d":"## TotalBsmtSF\nThis column contains the total square feet of the basement area.  \nIt has a very respectable correlation with the target of 0.61 and is the seventh most important variable for our model.  \nHere is the plot: of course, this time, we have some zero values because not every house has a basement.  \nIt seems like multiple outliers are present also here...","4070e1aa":"# Model training\nFor this competition, my approach was building two different models (Lasso & LightGBM), analyze their errors, correct them if necessary, and finally create an ensemble.  \nThis is way better than training only one model because averaging reduces a lot of the error and gives the possibility to go well under 0.12000 in the LB, something that for a single model is a bit difficult.\n## Lasso\nWe start with a regularized linear model, and since some features are very well correlated with the target, it should do very well.  \nOur preprocessing pipeline consists of a normalizer for the numerical columns and one-hot-encoding for the categorical attributes.","51855938":"I think deleting the least used columns will only help, so we will reduce the dimensionality of the data to 60 features and leave the rest of the work to the models themselves.","beee3316":"Now we will introduce a couple of functions that will be used frequently in the EDA part.  \nThe first one plots numerical variables (scatterplot or stripplot with the target `SalePrice` & the variable distribution), the latter plots categorical variables (stripplot & boxplot).","af62b6c1":"Again, we can validate our model using 5-fold cross-validation (with the same random seed as before, so they are exactly the same folds) and extract the out of fold predictions so we can ensemble the two models later.","01f27e38":"## Fence, Alley, MiscFeature, PoolQC\nAll these columns have more than 80% of missing values and in every case because the attribute they are describing is not present in the house.  \nUnlike the `FireplaceQu` feature, all these variables filled with a missing label were not used a lot by the model, so they should be not very informative.  \nFor completeness, we still analyze them to understand why this happens.","b76a4005":"## Ensemble using OOF predictions\nWe now proceed with the ensemble procedure: we loop through all 101 weights from 0.00 to 1.00 and search for the one that minimizes the weighted RMSLE of the two OOF predictions.  \nOnce we find it, it will be used to create a weighted submission file that should reflect the nice behavior of the OOF CV.  \nTo fully understand this code, I link again to this beautiful [discussion](https:\/\/www.kaggle.com\/c\/siim-isic-melanoma-classification\/discussion\/175614) by Chris Deotte which focuses very well on this method.","dfcd44ad":"# Conclusion\nFirst of all, thank you for reading my notebook: feel free to leave a comment if something isn't clear, I'm always trying to improve!  \nThat being said, I am very happy to reach the top of the LB also in this competition: surely it is possible to lower the MSLE again by ensembling more models, but I think the simplicity and effectiveness of this solution are fascinating.  \nHere is a histogram of House Prices LB scores in November 2020: you can see that our score of 0.11635 places between the orange and red line and it is pretty good (top 3%)! ![](attachment:HousePrices_LB.png)\nAs always, I learned a lot also from this experience, but now it's time to move on: see you in the next one and have a nice day!","7398ade1":"We see that the LGBM model is very similar to the Lasso one, but seems to make larger errors from the point cloud.  \nLet's see how it behaves when it encounters the outliers.","0fcc497a":"# Understand the competition metric\nThe metric for this competition (read [here](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/overview\/evaluation)) is Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price, much better known as Root-Mean-Squared-Log-Error (RMSLE).  \nIf $p_i$ and $a_i$ are respectively the $i$-th predicted and actual price of a house, the RMSLE of the submission will be\n$$ \n\\text{RMSLE} = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}{(\\log{p_i}-\\log{a_i})}^2}\n$$\nBecause most classifiers cannot minimize this metric by default and we have to define a custom one, we can kill two birds with one stone and take the log of `SalePrice` before training our model.  \nNot only this will give us the possibility to use MSE\/RMSE as a metric (taking the square root doesn't make any difference), but it will also transform the target distribution to look like a normal one avoiding further problems.","d0d05e72":"We will take a look at the four most useful categorical variables to complete our preliminary EDA and understand if they can really help our model or not.  \n## Neighborhood\nThis column represents the physical locations within Ames city limits and it is the most useful categorical variable according to our model.  \nI guess that's because there are obviously areas in the city that are particularly wealthy or deprived.","8a7a9e36":"# Remove outliers?\nAs we saw in the previous sections, there are some outliers in the most predictive features as well as in the less predictive ones.  \nHere I plotted some examples: we can see that large values of the features do not lead to an increase in `SalePrice`, but also that really high values of the target are some sort of outliers equally.  \nWhat should we do?","d8d335a0":"## KitchenQual\nWow, have you ever thought about this variable being a really important one?  \nThat's a bit weird, but having a nice looking kitchen actually seems a sign of an expensive house.  \nAgain, the variable is ordinal with \n```\nEx\tExcellent\nGd\tGood\nTA\tTypical\/Average\nFa\tFair\nPo\tPoor\n```  ","dd0175ff":"The blue and yellow clouds are perfectly stackable: the predictions for the test set are well aligned with the train one.  \nNow we focus our attention on the two red dots: the linear model predicted a really high `SalePrice` for them (over a milion!) but they should be much lower in the plot.  \nIn the same zone, we spot a yellow prediction for the test set: I bet its value should be much lower as well.  \nWe will thus proceed with changing this prediction and have our final Lasso submission ready.","c84a78d7":"We notice a strong positive correlation between `GarageArea` and `GarageCars`, as well as between `1stFlSF` and `TotalBsmtSF`.  \nSome of the variables are also well correlated with the target, even though some non-linear relationships may be hidden in the data.  \nLet's proceed with the analysis of one feature at a time.\n## OverallQual\nThis feature is the most correlated to the target (0.79) when we search for linear relationships and also by far the most predictive according to our tree-based model.    \nAs the data description says, `OverallQual` rates the overall material and finish of the house on a scale from 1 to 10.  \nLet's see what we are talking about.","7912dcb7":"We see that the Lasso model is pretty good and that most of the points are really close to the red diagonal line where they should be.  \nThe residuals plot looks also not bad: the model makes just a few large errors and the point cloud is almost symmetric.  \nNow, let's consider another important aspect which is the behavior of the model when encountering the outliers we set aside earlier: they were not used for training but we still have their predicted values for a quick comparison.  \nIn the train set, when `GrLivArea` passes 4500, the `SalePrice` drops for some reason and we will see in a moment that our linear model makes huge errors in these cases.  \nIn the test set we are lucky: there is only a house with `GrLivArea` over 5000 and I am pretty confident its price will be below 200k.","8d466b39":"We see there's nothing particularly interesting going on here: either the number of samples with non-null entries is too low to extract a meaningful pattern or the points overlap in a price range that's also extremely dense of houses.\n## Fill missing values\nAs described earlier, we now proceed with the column imputation: we will fill all columns with a low percentage of missing values with the most common one, `LotFrontage` using KNNImputer on `LotArea` (which appears to have a nice correlation with it) and the rest with NAs.","15b296b6":"![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/5407\/media\/housesbanner.png)\n# Introduction\nThis notebook contains a detailed EDA and a couple of baseline models for the [House Prices](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques) competition, the **regression** cousin for beginners of the [Titanic](https:\/\/www.kaggle.com\/c\/titanic).  \nThis dataset is perfect for experimenting with a lot of techniques and strengthening your knowledge: we are asked to predict the final price of 1459 houses given 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa.  \nWith missing values to impute properly, a ton of categorical columns to deal with, and the possibility to practice creative feature engineering, we have a long way to go before we deploy a reasonable model.  \nWithout further ado, let's get started!","b04d6d30":"# Best numerical feature analysis\nSince it is not the case to plot all the features, one thing we can do is actually taking a look at some of the most predictive ones using a model-based selection.  \nWe will start by analyzing the numerical columns, using an ExtraTreesRegressor to have an idea.  \nI want to mention that if you change the model, you will surely see a different feature importance plot due to the way it is built, but the result you'll get will be similar if the models have the same architecture.","a975c849":"We see the houses with a `GarageCars` value of 4 have all low prices, really particular.  \nFor the rest, the trend looks nice: having a `GarageCars` value of 3 seems the standard for very expensive homes with no exceptions.\n## GrLivArea\nThis value represents the above grade (ground) living area in square feet of the house and it is one of the most common measures people consider when buying.  \nWith no surprise, the correlation of this feature with `SalePrice` is 0.71, even though we notice two bad outliers on the bottom-right that should be removed.","02f9c90b":"We notice that the minimum value for `LotFrontage` is 21, so null values could also stand for 0 (=not present)!  \nHowever, due to zero values in other numerical features as `TotalBsmtSF` or `GarageArea` when they were not present, I think these values are missing because they were not recorded.  \nSince we have a lot of other columns to work with, filling with the median or mean would definitely be a coarse approach, even though this variable was not used much by our tree model in the previous section.  \nFor this reason, we will use sklearn's KNNImputer which will probably give better results.\n## FireplaceQu\nThis column indicates the fireplace quality.  \nSimilar to other features, we know these values are missing because a fireplace is not present in the house so the right way to impute this column is with a missing label.  \nI said impute and not drop because this feature was actually pretty important according to our tree-based model (it was the fifth most predictive variable), so it's a pity to delete it.  \nThis variable is supposed to be ordinal with \n```\nEx\tExcellent - Exceptional Masonry Fireplace\nGd\tGood - Masonry Fireplace in main level\nTA\tAverage - Prefabricated Fireplace in main living area or Masonry Fireplace in basement\nFa\tFair - Prefabricated Fireplace in basement\nPo\tPoor - Ben Franklin Stove\nNA\tNo Fireplace\n```\nand we can clearly recognize the order despite the presence of multiple outliers.","a5b1478d":"I think looking at the first eight features is enough to understand the data.  \nFirst, let's see the correlation heatmap between these columns and the target as some of them are likely to be correlated.","74d7ad1f":"## YearBuilt\nThis column is just the original construction date of the house and it is self-explanatory.  \nLet's plot it and we will make some considerations later."}}