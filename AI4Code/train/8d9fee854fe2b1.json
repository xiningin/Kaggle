{"cell_type":{"b54214d3":"code","52a1ad62":"code","ea730e0c":"code","a7bcbc7d":"code","477cb777":"code","81a235c2":"code","cb9de2a7":"code","6cb45c09":"code","893d6168":"code","38232e21":"code","64438eb7":"code","01c9ffd2":"code","c4b703b7":"code","ece56518":"code","dbf63fc7":"markdown","30c00fea":"markdown","d791ce34":"markdown","6711cbd9":"markdown","fb7daa87":"markdown","24f6da41":"markdown","865e2c12":"markdown","633bac3a":"markdown","dc98cce8":"markdown","394d6987":"markdown"},"source":{"b54214d3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","52a1ad62":"# import modules\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom matplotlib import pyplot as plt\nfrom datetime import datetime as dt\nimport seaborn as sns\n# set graphics dark mode\nplt.style.use('dark_background')\n# import dataset\ntrainset = pd.read_csv('\/kaggle\/input\/bike-sharing-demand\/train.csv')\ntestset = pd.read_csv('\/kaggle\/input\/bike-sharing-demand\/test.csv')\n# dataset quick view\ntrainset.head()","ea730e0c":"# create date feature from datetime\ntrainset.insert(1, 'date', pd.DataFrame([x[:10] for x in trainset.datetime]))\n# create time feature from datetime\ntrainset.insert(2, 'time', pd.DataFrame([x[11:] for x in trainset.datetime]))\n# convert datetime from string to datetime\ntrainset.date = [dt.strptime(x, '%Y-%m-%d').weekday() for x in trainset.date]\n# drop datetime column since we created two variables and casual and registered since their value is contained in count\ntrainset.drop(['datetime'], axis = 1, inplace = True)\n# trainset quick view\ntrainset.head()","a7bcbc7d":"# get index of the time elements in unique list\n_, idx = np.unique(trainset.time, return_inverse = True)\n# replace time feature with the index just computed\ntrainset.time = idx\n# trainset quick view\ntrainset.head()","477cb777":"# date - count boxplot\nplt.figure(), sns.boxplot(x = trainset['date'], y = trainset['count'])","81a235c2":"# replace date with weekday\ntrainset.date = [1 if x >= 0 and x < 6 else 0 for x in trainset.date]\n# replace feature name\ntrainset.rename(columns = {'date':'weekday'}, inplace = True)\n# trainset quick view\ntrainset.head()","cb9de2a7":"# check sum of nulls\ntrainset.isnull().sum()","6cb45c09":"# draw the pairplot of the variables\nplt.figure(), sns.pairplot(trainset)\n# check target boxplot to see outliers\nplt.figure(), sns.boxplot(trainset['count'])","893d6168":"# apply log transform to remove the number of outliers\ntrainset['count'] = np.log(trainset['count'])\n# repeat pairplot\nplt.figure(), sns.pairplot(trainset)\n# repeat boxplot\nplt.figure(), sns.boxplot(trainset['count'])","38232e21":"# variables correlation heatmap\nplt.figure(figsize = (10,10)), sns.heatmap(trainset.corr())","64438eb7":"# remove features highly correlated\ntrainset.drop(['casual','registered','temp'], axis = 1, inplace = True)\n# graph heatmap again\nplt.figure(figsize = (10,10)), sns.heatmap(trainset.corr())","01c9ffd2":"# group time values into day segments\ntrainset.time = [0 if x >= 0 and x < 6 else(1 if x > 5 and x < 13 else (2 if x > 12 and x < 19 else 3)) for x in trainset.time]\n# trainset quick view\ntrainset.head()","c4b703b7":"# get original datetime column for submission\ntestdates = testset.datetime\n# create date feature from datetime\ntestset.insert(1, 'date', pd.DataFrame([x[:10] for x in testset.datetime]))\n# create time feature from datetime\ntestset.insert(2, 'time', pd.DataFrame([x[11:] for x in testset.datetime]))\n# convert datetime from string to datetime\ntestset.date = [dt.strptime(x, '%Y-%m-%d').weekday() for x in testset.date]\n# drop datetime column since we created two variables and casual and registered since their value is contained in count\ntestset.drop(['datetime'], axis = 1, inplace = True)\n# get index of the time elements in unique list\n_, idx = np.unique(testset.time, return_inverse = True)\n# replace time feature with the index just computed\ntestset.time = idx\n# replace date with weekday\ntestset.date = [1 if x >= 0 and x < 6 else 0 for x in testset.date]\n# replace feature name\ntestset.rename(columns = {'date':'weekday'}, inplace = True)\n# remove features highly correlated\ntestset.drop(['temp'], axis = 1, inplace = True)\n# group time values into day segments\ntestset.time = [0 if x >= 0 and x < 6 else(1 if x > 5 and x < 13 else (2 if x > 12 and x < 19 else 3)) for x in testset.time]\n# testset quick view\ntestset.head()","ece56518":"# features\nXtrain = trainset.iloc[:,:-1]\nXtest = testset.iloc[:,:]\n# target\nytrain = trainset.iloc[:,-1]\n# standard scaler\nsca = StandardScaler().fit(Xtrain)\n# standarize features\nXtrain = sca.transform(Xtrain)\nXtest = sca.transform(Xtest)\n# classifier\nclf = RandomForestRegressor(random_state = 0)\n# regularization parameter range\nparam_grid = {'n_estimators': [25, 50, 100], 'max_features': [3, 6]}\n# grid search\ngrid = GridSearchCV(estimator = clf, scoring = 'neg_mean_squared_log_error', param_grid = param_grid)\n# training\nclf.fit(Xtrain, ytrain)\n# predictions\npreds = np.round(np.exp(clf.predict(Xtest)))\n# clip negatives in case there are\npreds[preds < 0] = 0\n# submission\npd.DataFrame({'datetime': testdates, 'count': preds}).to_csv('my_submission.csv', index = False)","dbf63fc7":"Model development and predictions","30c00fea":"Now that all the features are numerical and we checked there are no null values, let's check the correlation between variables.","d791ce34":"The datetime field contains the information about the date and the time in a single feature. It might be interesting to have this info in two separate variables, one for the date and other for the time. The reason is that it is more usual to rent a bike in the weekdays than in the weekend and in the morning than in the night, so having this two kind of information in two separate variables, might let the model to learn this trends.","6711cbd9":"So, actually, the rentals are higher in the weekdays than in the weekends, with no difference between the members of each group. Knowing this, we can change the date feature to a binary feature named weekday which is 1 from monday to friday and 0 from saturday to sunday.","fb7daa87":"We must convert time feature to numerical values. A simple approach is to create factors. For example, if we had the list: '01:00', '02:00', '03:00', '04:00' ... we can convert it to an int factor list where we use the index of each element in the original list: 0, 1, 2, 3...","24f6da41":"Check if there is a significative difference in the rentals for the different weekdays","865e2c12":"Let's group the time values (0 to 23) to \"day segments\" which we can define as 0 (dawn: 0 to 5), 1 (morning: 6 to 12), 2 (afternoon: 13 to 18) and 3 (evening: 19 to 23). This way we will have less sparse time information.","633bac3a":"Check the distrbution and relationships of the variables.","dc98cce8":"Prepare the test set applying the same transformations we applied for the training set.","394d6987":"Check if there are empty values in any of the samples"}}