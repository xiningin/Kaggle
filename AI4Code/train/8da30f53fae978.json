{"cell_type":{"cd10310f":"code","88e98e8e":"code","39400905":"code","86031c22":"code","289040d8":"code","aa5b5146":"code","fa5d6a61":"code","2913540a":"code","d14b2baa":"code","d5d68649":"code","4b095b46":"code","b66d6a32":"code","58ad1957":"code","8a2f6182":"code","2ed736da":"code","f408e0a5":"code","de4e0677":"code","83937fcf":"code","0dd76d8e":"code","544eace4":"code","170ef867":"code","fada4422":"markdown","653352a5":"markdown","bab06b5b":"markdown","9bb46f7c":"markdown","39a563b5":"markdown","965fac4b":"markdown","776b09ec":"markdown","decce081":"markdown","86c84c82":"markdown","0abcb042":"markdown","405de8a9":"markdown","40a23509":"markdown","4f7d9e6e":"markdown","057aedc0":"markdown","e70661c7":"markdown","1c4e484c":"markdown","e0be3f08":"markdown","efc639a7":"markdown","75cbd672":"markdown","ef02b7b7":"markdown","a759865f":"markdown","b1e9d503":"markdown","947c9119":"markdown","d558c724":"markdown","454fd05d":"markdown","d3526989":"markdown","8a2552f3":"markdown"},"source":{"cd10310f":"import warnings\nwarnings.filterwarnings('ignore')\n%load_ext autoreload\n%autoreload 2\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport random\nimport csv\nimport numpy as np\nimport sys\nimport pandas as pd","88e98e8e":"# function to 3D plot the changes in the quadcopter position\nfrom mpl_toolkits.mplot3d import Axes3D\ndef quadcopter_3d_plot(results, vars=['x', 'y', 'z'], title=''):\n    x = results[vars[0]]\n    y = results[vars[1]]\n    z = results[vars[2]]\n    c = results['time']\n    \n    fig = plt.figure(figsize=(8, 4), dpi=100)\n    ax = plt.axes(projection='3d')\n    cax = ax.scatter(x, y, z, c=c, cmap='YlGn')\n    ax.set(xlabel=vars[0], ylabel=vars[1], zlabel=vars[2], title=title)\n    fig.colorbar(cax, label='Time step (s)', pad=0.1, aspect=40)\n    plt.show();\n    \n# function to 3D plot the path of the Quadcopter\n\ndef show_flight_path(results, target=None):\n    results = np.array(results)\n    \n    ax = plt.axes(projection='3d')\n    ax.plot3D(results[:,0], results[:,1], results[:,2], 'gray')\n    if not target is None:\n        ax.scatter([target[0]], [target[1]], [target[2]], c='r', marker='o', s=30, label='Destination')\n    ax.scatter(results[0,0], results[0,1], results[0,2], c='g', marker='x', s=30, label='Starting Point')\n    ax.scatter(results[-1,0], results[-1,1], results[-1,2], c='b', marker='x', s=30, label='Ending point')\n    ax.legend()","39400905":"\nclass Basic_Agent():\n    def __init__(self, task):\n        self.task = task\n    \n    def act(self):\n        new_thrust = random.gauss(450., 25.)\n        return [new_thrust + random.gauss(0., 1.) for x in range(4)]","86031c22":"import os\nfrom shutil import copyfile\npath=os.getcwd()\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\nos.makedirs('.\/agents\/')\n\ncopyfile(src = \"..\/input\/reinforcementlearingfiles\/task.py\", dst = \".\/task.py\")\ncopyfile(src = \"..\/input\/reinforcementlearingfiles\/physics_sim.py\", dst = \".\/physics_sim.py\")\ncopyfile(src = \"..\/input\/agents\/agent.py\", dst = \".\/agents\/agent.py\")\ncopyfile(src = \"..\/input\/agents\/policy_search.py\", dst = \".\/agents\/policy_search.py\")\ncopyfile(src = \"..\/input\/reinforcementlearingfiles\/data.txt\", dst = \".\/data.txt\")\ncopyfile(src = \"..\/input\/reinforcementlearingfiles\/requirements.txt\", dst = \".\/requirements.txt\")\ncopyfile(src = \"..\/input\/reinforcementlearingfiles\/rewards.txt\", dst = \".\/rewards.txt\")","289040d8":"from task import Task\n\nruntime = 10.                                     # time limit of the episode\ninit_pose = np.array([0., 0., 13., 0., 0., 18.])  # initial pose\ninit_velocities = np.array([0., 8., 10.])         # initial velocities\ninit_angle_velocities = np.array([0., 77., 31.])   # initial angle velocities\nfile_output = '.\/data.txt'                         # file name for saved results\n\n# Setup\ntask = Task(init_pose, init_velocities, init_angle_velocities, runtime)\nagent = Basic_Agent(task)\ndone = False\nlabels = ['time', 'x', 'y', 'z', 'phi', 'theta', 'psi', 'x_velocity',\n          'y_velocity', 'z_velocity', 'phi_velocity', 'theta_velocity',\n          'psi_velocity', 'rotor_speed1', 'rotor_speed2', 'rotor_speed3', 'rotor_speed4']\nresults = {x : [] for x in labels}\n\n# Run the simulation, and save the results.\nwith open(file_output, 'w') as csvfile:\n    writer = csv.writer(csvfile)\n    writer.writerow(labels)\n    while True:\n        rotor_speeds = agent.act()\n        _, _, done = task.step(rotor_speeds)\n        to_write = [task.sim.time] + list(task.sim.pose) + list(task.sim.v) + list(task.sim.angular_v) + list(rotor_speeds)\n        for ii in range(len(labels)):\n            results[labels[ii]].append(to_write[ii])\n        writer.writerow(to_write)\n        if done:\n            break","aa5b5146":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.plot(results['time'], results['x'], label='x')\nplt.plot(results['time'], results['y'], label='y')\nplt.plot(results['time'], results['z'], label='z')\nplt.legend()\n_ = plt.ylim()","fa5d6a61":"plt.plot(results['time'], results['x_velocity'], label='x_hat')\nplt.plot(results['time'], results['y_velocity'], label='y_hat')\nplt.plot(results['time'], results['z_velocity'], label='z_hat')\nplt.legend()\n_ = plt.ylim()","2913540a":"plt.plot(results['time'], results['phi'], label='phi')\nplt.plot(results['time'], results['theta'], label='theta')\nplt.plot(results['time'], results['psi'], label='psi')\nplt.legend()\n_ = plt.ylim()","d14b2baa":"plt.plot(results['time'], results['phi_velocity'], label='phi_velocity')\nplt.plot(results['time'], results['theta_velocity'], label='theta_velocity')\nplt.plot(results['time'], results['psi_velocity'], label='psi_velocity')\nplt.legend()\n_ = plt.ylim()","d5d68649":"plt.plot(results['time'], results['rotor_speed1'], label='Rotor 1 revolutions \/ second')\nplt.plot(results['time'], results['rotor_speed2'], label='Rotor 2 revolutions \/ second')\nplt.plot(results['time'], results['rotor_speed3'], label='Rotor 3 revolutions \/ second')\nplt.plot(results['time'], results['rotor_speed4'], label='Rotor 4 revolutions \/ second')\nplt.legend()\n_ = plt.ylim()","4b095b46":"# the pose, velocity, and angular velocity of the quadcopter at the end of the episode\nprint(task.sim.pose)\nprint(task.sim.v)\nprint(task.sim.angular_v)","b66d6a32":"quadcopter_3d_plot(results, vars=['x', 'y', 'z'])","58ad1957":"quadcopter_3d_plot(results, vars=['rotor_speed1', 'rotor_speed2', 'rotor_speed3'])","8a2f6182":"from agents.policy_search import PolicySearch_Agent\nfrom task import Task\n\nnum_episodes = 2000\ntarget_pos = np.array([13., 8., 77.])\ntask = Task(target_pos=target_pos)\nagent = PolicySearch_Agent(task) \nrewards = []\nlabels = ['episode', 'total_reward']\nresults = {x : [] for x in labels}\nbest_flight_path = []\nfor i_episode in range(1, num_episodes+1):\n    state = agent.reset_episode() # start a new episode\n    flight_path = [ state ]\n    while True:\n        action = agent.act(state) \n        next_state, reward, done = task.step(action)\n        agent.step(reward, done)\n        state = next_state\n        flight_path.append(state)\n        if done:\n            rewards += [agent.score]\n            to_write = [i_episode] + [max(rewards)]\n            for k in range(len(labels)):\n                results[labels[k]].append(to_write[k])                 \n            print(\"\\rEpisode = {:4d}, score = {:7.3f} (best = {:7.3f}), noise_scale = {}\".format(\n                i_episode, agent.score, agent.best_score, agent.noise_scale), end=\"\")  # [debug]\n            rewards.append(np.mean(results['total_reward']))\n            if agent.score >= max(rewards):\n                best_flight_path = flight_path                \n            break\n    sys.stdout.flush()","2ed736da":"# avrage  of the last 10 episodes\nperformance = np.mean(rewards[-10:])\nprint(performance)","f408e0a5":"\"\"\"\n\n\nfrom agents.agent import DDPG\nfrom task import Task,Task_TakeOff\n\nnum_episodes = 2000\ntarget_pos = np.array([0., 0., 0.])\ninit_pos = np.array([0., 0., 0.])\ntask = Task_TakeOff(init_pose=init_pos, target_pos=target_pos)\nrewards = []\nlabels = ['episode', 'total_reward']\nresults = {x : [] for x in labels}\nagent = DDPG(task) \n\nbest_flight_path = []\nfor i_episode in range(1, num_episodes+1):\n    state = agent.reset_episode() # start a new episode\n    num_steps = 0\n    flight_path = [ state ]\n    while True:\n        action = agent.act(state) \n        next_state, reward, done = task.step(action)\n        agent.step(action, reward, next_state, done)\n        state = next_state\n        flight_path.append(state)\n        if done:\n            rewards += [agent.get_score()]\n            to_write = [i_episode] + [max(rewards)]\n            for k in range(len(labels)):\n                results[labels[k]].append(to_write[k])            \n            print(\"Episode = {:4d}, steps={:4d} reward = {:9.3f} (best = {:9.3f})  \".format(\n                i_episode, agent.num_steps, agent.get_score(), max(rewards)), end=\"\\r\")  # [debug]\n            rewards.append(np.mean(results['total_reward']))\n            if agent.get_score() >= max(rewards):\n                best_flight_path = flight_path                \n            break\n            \n    sys.stdout.flush()\n    \n    \"\"\"","de4e0677":"plt.plot(results['episode'], results['total_reward'])\n_ = plt.ylim()","83937fcf":"# avrage  of the last 10 episodes\nperformance = np.mean(rewards[-10:])\nprint(performance)","0dd76d8e":"\"\"\"\nfrom agents.agent import DDPG\nfrom task import Task_Hover\n\n\n# the values below gives the quadcopter different starting positions.\nruntime = 5.                                     # time limit of the episode\ninit_pose = np.array([0., 0., 0., 0., 0., 0.])  # initial pose\ninit_velocities = np.array([0., 0., 0.])         # initial velocities\ninit_angle_velocities = np.array([0., 0., 0.])   # initial angle velocities\nfile_output = 'rewards.txt'                      # file name for saved results\n\nnum_episodes = 2000\ntarget_pos = np.array([0., 0., 0.])\ntask = Task(target_pos=target_pos)\nagent = DDPG(task) \nrewards = []\nlabels = ['episod', 'total_reward']\nresults = {x : [] for x in labels}\n\nwith open(file_output, 'w') as csvfile:\n    writer = csv.writer(csvfile)\n    writer.writerow(labels)  \n    best_total_reward = 0\n    for i_episode in range(1, num_episodes+1):\n        state = agent.reset_episode() # start a new episode\n        total_reward = 0\n        while True:\n            action = agent.act(state) \n            next_state, reward, done = task.step(action)\n            total_reward += reward\n            if total_reward > best_total_reward:\n                best_total_reward = total_reward\n            agent.step(action, reward, next_state, done)\n            state = next_state\n            if done:\n                to_write = [i_episode] + [total_reward]\n                for ii in range(len(labels)):\n                    results[labels[ii]].append(to_write[ii])\n                writer.writerow(to_write)\n                print(\"\\rEpisode = {:4d}, total_reward = {:7.3f} (best = {:7.3f})\".format(\n                    i_episode, total_reward, best_total_reward), end=\"\")\n                rewards.append(np.mean(results['total_reward']))\n                break\n        sys.stdout.flush()\n        \n        \"\"\"","544eace4":"\"\"\"\nplt.plot(results['episod'], results['total_reward'])\n_ = plt.ylim()\n\n\"\"\"","170ef867":"# avrage  of the last 10 episodes\nperformance = np.mean(rewards[-10:])\nprint(performance)","fada4422":"## The Agent\n\nThe agent given in `agents\/policy_search.py` uses a very simplistic linear policy to directly compute the action vector as a dot product of the state vector and a matrix of weights. Then, it randomly perturbs the parameters by adding some Gaussian noise, to produce a different policy. Based on the average reward obtained in each episode (`score`), it keeps track of the best set of parameters found so far, how the score is changing, and accordingly tweaks a scaling factor to widen or tighten the noise.\n\nThe code below explains how the agent performs on the task.","653352a5":"## <font color='grey'>Preparation and importing libraries<\/font>","bab06b5b":"# Train a Quadcopter How to Fly\n#  Using Reinforcement Learing\n## By Yosry Negm\n\n<br><hr><br>\nThis Project introduces how to Design an agent to fly a quadcopter, and then train it using a reinforcement learning algorithm.","9bb46f7c":"Finally, we can use the code cell below to print the agent's choice of actions.  ","39a563b5":"The code below visualizes how the position of the quadcopter evolved during the simulation.","965fac4b":"## Average of the last 10 episodes","776b09ec":"## Ploting rewards for Hover Task","decce081":"The next code cell visualizes the velocity of the quadcopter.","86c84c82":"## <font color='grey'>Warm up<\/font>","0abcb042":"Next, we can plot the Euler angles (the rotation of the quadcopter over the $x$-, $y$-, and $z$-axes),","405de8a9":"## <font color='blue'>Take Off <\/font> Task","40a23509":"In this project, I chose to use<font color='green'><b> Actor-Critic Algorithm <\/b><\/font>to train my dsigned agent on the prposed tasks. It works better for me after tuning the algorithm hyper parameter such as \nchoosing discount factor (<font color='green'><b>gamma<\/b><\/font>) equal to 0.99 and <font color='green'><b>epislon<\/b><\/font> equal to 0.01 for <b>soft update of target parameters<\/b>.<br>\nI build the model by<b> two nural networks <\/b>one for the <u>actor<\/u> and the other one for the <u>crictic<\/u> with  <font color='green'><b>ReLu<\/b><\/font> activation function in the hidden layers and <font color='green'><b>Sigmoid <\/b><\/font>activation function in the output layer in each.<br>\n- <b><u>The Neural Network of the Actor<\/u><\/b> :-<br>\n    -  Dense :  32 hidden units Layer ,Two Layers with 64 hidden units each , 128 hidden units Layer (All with <b>ReLu<\/b> activation function) and Batch Normalization. \n    -  Dropout: 3 with keep_prob = 0.5 .\n    -  Output : on Layer with units equal to the action size with <b>Sigmoid<\/b> activation function.<br>\n- <b><u>The Neural Network of the Critic<\/u><\/b>:-<br>\n    - <b><u>State <\/u>:<\/b> <br>\n        -  Dense :  32 hidden units Layer , 64 hidden units Layer, 128 hidden units Layer (All with <b>ReLu<\/b> activation function) and Batch Normalization. \n        -  Dropout: 3 with keep_prob = 0.5.<br>\n    - <b><u>Action <\/u>:<\/b><br>\n        -  Dense : Two Layers with 64 hidden units each, 128 hidden units Layer (All with <b>ReLu<\/b> activation function) and Batch Normalization. \n        -  Dropout: 3 with keep_prob = 0.5.<br>\n- <p>When building this architecture, I put into consideration imporving policy gradient with a critic i.e running the policy over generating samples then fit the model to estimate the return and therefore, improving the policy.The used algorithms seems to be stable and simple despite of there is no shared features between the actor and the critic so I made it to work in Batch to get lower variance with virtue of critic trying to get rewards sooner as possible to obtain better performance.<\/p>","4f7d9e6e":"## Average of the last 10 episodes","057aedc0":"## Reflections\n\nI have chosen two tasks to train my designed agent, they are <b><font color='green'> Take-off task<\/font><\/b> and <b><font color='green'> Hover task<\/font><\/b> . I designed the reward function in both cases to <b>preserve the clipping of rewards between -1 and +1<\/b> trying to to<b> maximize the penality<\/b> whenever the distance between the pose and target of the quadcopter is large or the agent has run out of time.\n","e70661c7":"before plotting the velocities (in radians per second) corresponding to each of the Euler angles.","1c4e484c":"## Ploting rewards for Take Off Task","e0be3f08":"## Define the Task, Design the Agent, and Train Your Agent!\n\nNow we will amend `task.py` to specify a task of our choosing.  If we're unsure what kind of task to specify, we may like to teach our quadcopter to takeoff, hover in place, land softly, or reach a target pose.  \n\nAfter specifying our task, we will use the agent in `agents\/policy_search.py` as a template to define our new agent in `agents\/agent.py`.  we can borrow whatever we need from the that agent, including ideas on how you might modularize our code (using helper methods like `act()`, `learn()`, `reset_episode()`, etc.).\n\nNote that it is **highly unlikely** that the first agent and task that you specify will learn well.  You will likely have to tweak various hyperparameters and the reward function for your task until you arrive at reasonably good behavior.\n\nAs you develop your agent, it's important to keep an eye on how it's performing. Use the code above as inspiration to build in a mechanism to log\/save the total rewards obtained in each episode to file.  If the episode rewards are gradually increasing, this is an indication that your agent is learning.","efc639a7":"## <font color='blue'>Hover <\/font> Task","75cbd672":"### Also, the following is a  3D plot to visualize the four rotors speed changes","ef02b7b7":"The code below lets the agent select actions to control the quadcopter.  \nThe `labels` list below annotates statistics that are saved while running the simulation.  All of this information is saved in a text file `data.txt` and stored in the dictionary `results`.  ","a759865f":"Training the agent to fly the Quadcopter wasn't easy task, It took me extra time and lots of trail and error ,gussing and numerous tuning phases of the hyperparamers of the algorithm and the architecture as well as trying different intial states and redesigning the reward function several times until got gradual learning curve and better performance as indicated from the mean of the last 10 episodes.","b1e9d503":"## <font color='grey'>Declerations of utility routines<\/font>","947c9119":"In `task.py`, we use the 6-dimensional pose of the quadcopter to construct the state of the environment at each timestep.  However, when amending any more tasks, we could expand the size of the state vector by including the velocity information.  Also we can use any combination of the pose, velocity, and angular velocity.\n\n## The Task\n\nThe `__init__()` method in `task.py` is used to initialize several variables that are needed to specify the task.  \n- The simulator is initialized as an instance of the `PhysicsSim` class (from `physics_sim.py`).  \n- Inspired by the methodology in the original DDPG paper, we make use of action repeats.  For each timestep of the agent, we step the simulation `action_repeats` timesteps.  If you are not familiar with action repeats, please read the **Results** section in [the DDPG paper](https:\/\/arxiv.org\/abs\/1509.02971).\n- We set the number of elements in the state vector.  For the sample task, we only work with the 6-dimensional pose information.  To set the size of the state (`state_size`), we must take action repeats into account.  \n- The environment will always have a 4-dimensional action space, with one entry for each rotor (`action_size=4`). You can set the minimum (`action_low`) and maximum (`action_high`) values of each entry here.\n- The sample task in this provided file is for the agent to reach a target position.  We specify that target position as a variable.\n\nThe `reset()` method resets the simulator.  The agent should call this method every time the episode ends.  we can see an example of this in the code below.\n\nThe `step()` method is perhaps the most important.  It accepts the agent's choice of action `rotor_speeds`, which is used to prepare the next state to pass on to the agent.  Then, the reward is computed from `get_reward()`.  The episode is considered done if the time limit has been exceeded, or the quadcopter has travelled outside of the bounds of the simulation.\n\nIn the next section, we will show how to test the performance of an agent on this task.","d558c724":"First of all it was an excited experience for me, especially I am interested in the programming of autonomous vehicles. The project was a great chance to have hands and dig into field\nmore practically,The hardest part I faced was really how to define and design tasks it tooks me time to think about them also I found some difficulty in tuning the parameters and reward function to get better performance as well as inspecting the architecture serveral times.Plotting results was so easy and the project in general goes smooth with me.","454fd05d":"### In adition, the following 3D plot visualize the changes in the Quadcopter position","d3526989":"When specifying a task, we will derive the environment state from the simulator.  The code below prints the values of the following variables at the end of the simulation:\n- `task.sim.pose` (the position of the quadcopter in ($x,y,z$) dimensions and the Euler angles),\n- `task.sim.v` (the velocity of the quadcopter in ($x,y,z$) dimensions), and\n- `task.sim.angular_v` (radians\/second for each of the three Euler angles).","8a2552f3":"This agent should <b>perform very poorly<\/b> on this task.  And that's where you come in! <font color='red'> As shown below :-<\/b>"}}