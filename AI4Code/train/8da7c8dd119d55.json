{"cell_type":{"e0083a0b":"code","f6417f77":"code","54eb9933":"code","336fcdde":"code","fcbb9fd2":"code","02b36d07":"code","8c8b6461":"code","c4a5d557":"code","5a3baad4":"code","f8ac7a5c":"code","261b254d":"code","ed0d81dc":"code","37d1f2ca":"code","4bc7fa43":"code","1764fbbe":"markdown","d7d50825":"markdown","f49d6854":"markdown","6a5c8910":"markdown","ba00e15b":"markdown","3db30cc1":"markdown","20d6a3d8":"markdown","7204a86d":"markdown","fb657f53":"markdown","de2eb714":"markdown","89740a6e":"markdown","af8457e5":"markdown","70687ada":"markdown","d8848eba":"markdown"},"source":{"e0083a0b":"def run_spider():\n    import requests\n    from lxml import etree\n    import re\n    import time #\u6709\u5fc5\u8981\u65f6\u7528\u6765\u7761\u7720\n    import pandas as pd \n    # version1\uff1a\u5b58\u5728\u4e24\u4e2a\u95ee\u98981\u662flink\u6709\u65f6\u5019\u4e0d\u662fcqu\u800c\u5fae\u4fe1\u7b49\u6765\u6e902\u662f\u6700\u540e\u4e00\u6b21\u518d\u5199\u6587\u4ef6\u4e0d\u5b89\u5168\uff08\u6bcf\u9694\u51e0\u8f6e\u5c31\u4fdd\u5b58\uff09\n    # version2\uff1a\u4e5f\u6709\u4e24\u4e2a\u95ee\u98981\u662f\u901f\u5ea6\u592a\u6162\u53ef\u4ee5\u7528\u591a\u8fdb\u7a0b\u548c\u5f02\u6b65io\u89e3\u51b32\u662f\u8bfb\u5199\u6587\u4ef6\u6bcf\u6b21\u91cd\u5199csv\u53ef\u4ee5\u6539\u6210\u8ffd\u52a0\u4e14list\u53d8frame\u518d\u4fdd\u5b58\u592a\u9ebb\u70e6\u76f4\u63a5\u5199\u6216\u8005csv\u5e93\n    # version3\uff1a\u52a0\u5165\u4e86\u591a\u8fdb\u7a0b\u6ca1;\u5bf9\u6bd4\u4e86csv\u6a21\u5757pd\u66f4\u5feb\u5c24\u5176\u5b58pkl\u66f4\u5feb\uff1b\u4fee\u590d\u4e86nexturl\u4f1a\u6b7b\u5faa\u73af\uff1b\u4fee\u590d\u6d4f\u89c8\u6570\u4e3a\u96f6bug\n    # \u4f7f\u7528\u8ffd\u52a0\u800c\u975e\u5185\u5b58\u4fdd\u5b58\u5168\u90e8\u7684\u6570\u636e\uff1b\u4f46\u907f\u514d\u4e2d\u65ad\u8fd0\u884c\u540e\u8ffd\u52a0\u91cd\u590d\u547d\u540d\u52a0\u4e0a\u968f\u673a\n    start = time.time()\n    url=\"http:\/\/news.cqu.edu.cn\/newsv2\/news-126.html\"\n    url2 = \"http:\/\/news.cqu.edu.cn\/newsv2\/news-126.html?&page=590\" #\u6d4b\u8bd5url\n    url3 = \"https:\/\/news.cqu.edu.cn\/newsv2\/news-132.html\"\n\n    header = {\"User-Agent\":\"Mozilla\/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/79.0.3945.130 Safari\/537.36\"}\n    results=[]\n    flag = True\n    while flag:\n        html = requests.get(url, headers=header).text\n        # print(html)\n        listpage = etree.HTML(html)\n        # lis = listpage.xpath('\/html\/body\/div[3]\/div\/div[2]') \n        lists = listpage.xpath('\/\/div[@class=\"lists\"]\/div[@class=\"item\"]') #\u8fd9\u91cc\u9700\u8981\u6ce8\u610f\u9009\u62e9\u7684\u662fitem\n\n        # \u4e00\u4e2apage\u5185\u7684\u6293\u53d6\n        for oneSelector in lists: # \u6536\u96c6\u7684\u7ec6\u8282\n            # testSelector = oneSelector.xpath(\"@class\")[0]\n            # print(testSelector) #item\n\n            # \u4e0d\u540c\u677f\u5757\u7684div\u987a\u5e8f\u8fd8\u662f\u4e0d\u540c\u8981\u9009class\n            link = oneSelector.xpath(\"div[@class='content']\/div[@class='title']\/a\/@href\")[0]\n            if \"http:\/\/news.cqu.edu.cn\" not in link:\n                link = \"http:\/\/news.cqu.edu.cn\" + link\n            title = oneSelector.xpath(\"div[@class='content']\/div[@class='title']\/a\/text()\")[0]\n            abstract = oneSelector.xpath(\"div[@class='content']\/div[@class='abstract']\/text()\")[0]\n            try:\n                html_sub = requests.get(link, headers=header).text\n                newspage = etree.HTML(html_sub)\n                datetime = (newspage.xpath(\"\/\/div[@class='ibox']\/span[2]\/text()\"))[0]\n                author = newspage.xpath(\"\/\/div[@class='dinfoa']\/p\/a\/text()\") #a\u4e2a\u6570\u4e0d\u5b9a\u7684\u4f46\u53ef\u4ee5\u8fd9\u6837\u52a0\u5165\u5230list\n                # TODO: \u8fd9\u4e2acount\u4e0d\u80fd\u76f4\u63a5\u6293\u5f97\u627eapi\u53d1\u73b0\u5728html\u91cc\u6709\u8fd9\u4e2aapi\n                view_num = newspage.xpath(\"\/\/span[@id='hits']\/text()\")[0]\n\n                api = newspage.xpath('\/\/script[@language=\"JavaScript\"]\/@src')[0] #$('#todaydowns').html('227');$('#weekdowns').html('227');$('#monthdowns').html('227');$('#hits').html('227');\n                apiout = (requests.get(api,headers=header).text) \n                view_num = apiout.split(\"#hits').html('\")[-1].replace(\"');\", \"\") #\u66f4\u597d\u7684\u5bf9\u4e0a\u9762\u5b57\u7b26\u4e32\u63d0\u53d6\u662f\u6b63\u5219\n                print(view_num)\n                tags = newspage.xpath(\"\/\/div[@class='tags']\/a\/text()\")\n                # \u6709\u4e9b\u6b63\u6587\u7684p\u4e0d\u4e00\u6837\u8fd8\u6709\u542bimg\u7684 newsbody = newspage.xpath(\"\/\/p[@style='text-align: justify;']\/text()\")\n                newsbody = newspage.xpath(\"\/\/div[@class='acontent']\/p\/text()\")\n                results.append([datetime,title,author,abstract,link,tags,newsbody,view_num])\n\n            except:\n                print(\"Error:\",link)\n        next_url = listpage.xpath('\/\/div[@class=\"page\"]\/a[last()]\/@href')[0]\n        cur_page = listpage.xpath('\/\/div[@class=\"page\"]\/span\/text()')[0]\n\n        if next_url and cur_page==next_url.split(\"=\")[-1]:\n            # url = \"http:\/\/news.cqu.edu.cn\/newsv2\/\"+ next_url\n            print(cur_page,next_url.split(\"=\")[-1])\n            flag = False\n        url = next_url\n        print(\"crawl:\"+url)\n    filename = \"data_set\"+str(time.time())+\".pkl\"\n    df = pd.DataFrame(results,columns=['datetime','title','author','abstract','link','tags','newsbody','view_num']) #,columns=['link','title','abstract']\n    # df.to_csv(filename, mode='a',encoding='utf_8_sig')\n    df.to_pickle(filename)\n# run_spider()","f6417f77":"# import\nimport os\nfrom collections import Counter\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport jieba\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\nfrom sklearn.linear_model import LogisticRegression\nimport warnings\nwarnings.filterwarnings('ignore') \n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))","54eb9933":"def get_stopwords(stopwords_dir=\"..\/input\/cqunews\/stopwords\"):\n    \"\"\"\n       \u8bfb\u53d6\u505c\u7528\u8bcd\u5e93,\u8fd9\u91cc\u53ea\u662f\u7528\u4e86\u4e00\u4e2a\u505c\u7528\u8bcd\u5e93   \u5982\u679c\u8981\u9884\u6d4b\u7ed3\u679c\u66f4\u9ad8\u70b9\u7684\u8bdd\uff0c\u53ef\u4ee5\u5c06\u591a\u4e2a\u8bed\u6599\u5e93\u7ed3\u5408\u8d77\u6765\u4f7f\u7528\n       :return: set \u5305\u542b\u7684\u505c\u7528\u8bcd\n       \"\"\"\n    files = os.listdir(stopwords_dir)#\u8fd4\u56de\u8def\u5f84\u4e0b\u7684\u6240\u6709\u6587\u4ef6\n    stopwords = []\n    for f in files:\n        filepath = os.path.join(stopwords_dir, f)#\u628a\u6587\u4ef6\u62fc\u63a5\u6210\u6587\u4ef6\u8def\u5f84\n        with open(filepath, 'r', encoding='utf-8') as r:\n            w = r.read().split()\n        stopwords += w\n    return set(stopwords) #b\u628a\u6240\u6709\u505c\u7528\u8bcd\u6587\u4ef6\u91cc\u7684\u90fd\u53d6\u51fa\u6765\uff0c\u7136\u540e\u653e\u5230\u96c6\u5408\u91cc","336fcdde":"filepath = [r\"\/kaggle\/input\/cqunews\/zhaosheng.pkl\",\\\n            r\"\/kaggle\/input\/cqunews\/zonghe.pkl\"]\nstopwords = get_stopwords(\"\/kaggle\/input\/cqunews\/stopwords\")\ndata = (pd.concat([pd.read_pickle(file) for file in filepath]))\nX = data.drop(['view_num'],axis=1)\ny = data['view_num'].astype(int) #\u8f6c\u6362\u6570\u5b57\ndata.head() #\u4e00\u4e07\u516b\u6761 ","fcbb9fd2":"# max(y) 54514\n# min(y) 9\n# (y.mean()) 798\n# y.median() 489 \u4e2d\u4f4d\u6570\n# y.plot() \u8fd9\u4e2a\u56fex\u662f\u5e8f\u53f7\u6ca1\u4ec0\u4e48\u610f\u4e49\n# ct = Counter(y)\n# plt.bar(list(ct.keys()),list(ct.values()),label='xl1')#label\u4e3a\u8bbe\u7f6e\u56fe\u4f8b\u6807\u7b7e\uff0c\u9700\u8981\u914d\u5408legend\uff08\uff09\u51fd\u6570\u624d\u80fd\u663e\u793a\u51fa\n# (sorted(ct.items(), key=lambda data: data[1])) \u4e0d\u662f\u76f4\u65b9\u56fe\u4e5f\u4e0d\u662f\u6298\u7ebf\u56fe\n# y.plot(kind='bar') \u592a\u6162\u4e86\nplt.hist(y,range=(0,y.mean()*2),bins=10) # \u539f\u672c\u6c1b\u56f4\u662f0\u5230\u6700\u5927\u770b\u5230\u57fa\u672c\u5206\u5e03\u5728\u4e00\u4fa7\u4e8e\u662f\u786e\u5b9a\u8303\u56f4","02b36d07":"label = y.apply(lambda x:x>y.mean()) # \u770b\u56fe\u5c31\u77e5\u9053\u7528\u5747\u503c\u4e3a\u8d1f\u8f83\u591a\nfrom sklearn.preprocessing import StandardScaler\nstandardscaler = StandardScaler()\nystd = standardscaler.fit_transform(np.array(y).reshape(-1,1))\nCounter(label)","8c8b6461":"# \u4f7f\u7528title+tags\u5408\u5e76\uff0c\u4e14\u540e\u8005\u4e0d\u9700\u8981\u5206\u8bcd\nimport jieba.posseg as pseg\ndef content_to_word(line):\n    gen = pseg.cut(line)\n    words = []\n    for i in gen:\n        if i.flag != 'x' and i.word not in stopwords  and not i.word.isdigit(): #\u505c\u7528\u8bcd\n            words.append(i.word) #\u53ea\u52a0\u975e\u6807\u70b9\u7b26\u53f7\n    return ' '.join(words) #\u7a7a\u683c\u5206\u5272\u7684\u5b57\u7b26\u4e32\n# data['tags']+data['title'] \u9700\u8981\u5148tags\u5904\u7406\n# temp = data['tags'].map(lambda x: \" \".join(x)) #\u591a\u8fd0\u884c\u51e0\u6b21\u4f1a\u88ab\u5206\u8bcd\u5355\u5b57\u56e0\u4e3atags\u4f1a\u53cd\u590d\u4f7f\u7528\u6240\u6709\u6539\u7528temp\u5b58\u6570\u636e\n# data['title'] = data['title'].map(content_to_word)\n# X = data['title'] #+ temp\nX = data['abstract'].map(content_to_word)\nX","c4a5d557":"import wordcloud\ndef word_cloud_analyse(datas,stopwords):\n    X=datas['abstract']\n    words=[]\n    for i in range(len(X)):\n        l=[]\n        l=jieba.lcut(X.iloc[i])\n        l=[t for t in l  if t not in stopwords and t!=' ']\n        words+=l\n    words=' '.join(words)\n#     ,font_path=\"C:\\\\Windows\\\\Fonts\\\\HGWYS_CNKI.TTF\"\n    wd=wordcloud.WordCloud(background_color=\"white\")\n    wd.generate(words)\n    return wd\n        \n\nstopwords=get_stopwords()\nwd=word_cloud_analyse(data,stopwords)\n%pylab inline\nimport matplotlib.pyplot as plt\nplt.imshow(wd, interpolation='bilinear')\nplt.axis(\"off\")","5a3baad4":"# This function returns the test set training set after word vectorization\ndef tokenizer_data(X, y, MAX_SEQUENCE_LENGTH):\n    \n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(X)\n    joblib.dump(tokenizer, '.\/models\/Tokenizer')\n\n    sequences = tokenizer.texts_to_sequences(X)\n    word_index = tokenizer.word_index\n    print('\u6570\u636e\u96c6\u542b %s \u4e2a\u4e0d\u540c\u7684\u8bcd.' % len(word_index))\n    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n    labels = to_categorical(np.asarray(y)) # onehot \n    labels = np.asarray(y)\n    print('Shape of data tensor:', data.shape,\"[0]:\", data[0])\n    print('Shape of label tensor:', labels.shape,\"[0]:\", labels[0])\n\n\n    # \u7b2c\u4e00\u90e8\u5206\u5212\u5206\u5e94\u8be5\u653e\u8fd9\u513f\/\u6a21\u578bfit\u62a5\u9519\u5c31\u662f\u56e0\u4e3a\u76f4\u63a5\u7528\u7684\u539f\u59cb\u6570\u636e\n    X, X_test, y, y_test = train_test_split(data, labels, test_size=.20, random_state=2048)\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=.10, random_state=2048)\n    return word_index,X_train,y_train,X_val,y_val,X_test,y_test\nword_index, X_train, y_train, X_val, y_val, X_test, y_test = tokenizer_data(X,label,MAX_SEQUENCE_LENGTH) #\u7528ystd\u662f\u505a\u56de\u5f52\u7684","f8ac7a5c":"import joblib\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.layers import Dense, Input, Flatten, Dropout\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding, GlobalMaxPooling1D\nfrom tensorflow.keras.models import Sequential\n# train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.2)\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras import initializers\n\nMAX_SEQUENCE_LENGTH = 100\nEMBEDDING_DIM = 200\ndef train():\n    print('3. #############\u8f7d\u5165word2vec...##########')\n    embedding_layer = Embedding(len(word_index) + 1, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)\n    print('4. #############\u6a21\u578b\u53ca\u8bad\u7ec3...#############')\n\n    model = Sequential()\n    # \u4f7f\u7528\u4e0a\u9762\u6784\u9020\u7684layer\n    model.add(embedding_layer)\n    model.add(Dropout(0.3))\n    model.add(Conv1D(250, 3, padding='valid', activation='relu', strides=1)) \n    model.add(MaxPooling1D(3))\n#     model.add(Dropout(0.2))\n    model.add(Flatten())\n    model.add(Dense(EMBEDDING_DIM, activation='relu'))\n#     model.add(Dense(EMBEDDING_DIM, activation='relu',kernel_regularizer=regularizers.l2(0.01)))\n\n    # model.add(Dense(y_train.shape[1], activation='softmax')) #\u591a\u5206\u7c7bsoftmax\n    model.add(Dense(1, activation='sigmoid')) #\u4e8c\u5206\u7c7b\n#     model.add(Dense(1)) #\u56de\u5f52\n\n    model.summary()  \n    from tensorflow.python.keras.utils.vis_utils import plot_model\n    plot_model(model, to_file='cnn_model.png',show_shapes=True)\n    cce = tf.keras.losses.CategoricalCrossentropy()\n    bce = tf.keras.losses.BinaryCrossentropy()\n    # mse = tf.keras.losses.MSE()\n    opt = tf.keras.optimizers.SGD(learning_rate=0.1)\n    model.compile(loss=bce,\n                  optimizer='rmsprop', #rmsprop\n                  metrics=['acc']) #'mae', 'mse','acc'\n    print(model.metrics_names)\n    model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=5, batch_size=128) #loss\u4e3a\u96f6\n    model.save('.\/models\/cnn_word.h5')\n\n\n    print('4. #############\u8bc4\u4f30...#############')\n    print(model.evaluate(X_test, y_test))\n    return model\nmodel = train()","261b254d":"def word_to_vector(text, MAX_SEQUENCE_LENGTH=100):\n    tokenizer = joblib.load(\".\/models\/Tokenizer\")\n    word_index = tokenizer.word_index\n    sequences = tokenizer.texts_to_sequences([text])\n    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n#     data = pad_sequences(sequences)\n\n    return data\ndef predict(sentence):\n    model = tf.keras.models.load_model('.\/models\/cnn_word.h5')\n\n    gen = pseg.cut(sentence)\n    words = []\n    for i in gen:\n        if i.flag != 'x' and i.word not in stopwords and not i.word.isdigit() :\n            words.append(i.word)  \n    data = ' '.join(words) \n    data = word_to_vector(data)\n    print(((model.predict(data)))) #\u76f4\u63a5model\u8c03\u7528\npredict(\"\u4e3a\u5e86\u4ee3\uff0c2021\u5e745\u6708\uff0c\u5b66\u6821\u4e3e\u529e\u4e86\u201c\u4e2d\u56fd\u68a6\u2022\u52b3\u52a8\u2019\u7f8e\u4e2a\u5b66\u9662\u8e0a\u8dc3\u53c2\u52a0\u3002\")\npredict(\"6\u67083\u65e5\u4e0b\u5348\uff0c\u8054\u8c0a\u8d5b\u51b3\u8d5b\u5728\u91cd\u5927\u82b1\u56ed\u4e52\u4e53\u7403\u9986\u62c9\u5f00\u6218\u5e55\u3002\u4ece\u5b66\u9662\u6559\u804c\u5de5\u548c\u7855\u58eb\u7814\u7a76\u751f\u9009\u62d4\u51fa\u768416\u540d\u9009\u624b\u53c2\u52a0\u4e86\u51b3\u8d5b\")\n# Since we take the average above as a positive sample, we need the page views greater than 800 to be greater than 0.5","ed0d81dc":"data['abstract'].iloc[0]","37d1f2ca":"# Attempt decision tree \n# from sklearn.ensemble import ExtraTreesClassifier,RandomForestClassifier,AdaBoostClassifier #,DecisionTreeClassifier\n# from sklearn.tree import DecisionTreeClassifier\n# from sklearn.model_selection import cross_val_score\n# from sklearn.feature_selection import RFE\n# from sklearn.decomposition import PCA, TruncatedSVD, PCA\n# from mlxtend.classifier import EnsembleVoteClassifier \n# DTC_Classifier = DecisionTreeClassifier(criterion='entropy', random_state=0)\n# ETC_Classifier = ExtraTreesClassifier()\n# RTC_Classifier = RandomForestClassifier(criterion='entropy')\n# #  Gaussian Naive Baye Model\n# BNB_Classifier = GaussianNB()\n\n# # GBM\n# import xgboost as xgb\n# XGB_Classifier = xgb.XGBClassifier()\n# # ADA\n# ADA_Classifier = AdaBoostClassifier(\n#     DTC_Classifier,\n#     n_estimators=100,\n#     learning_rate=1.5)\n# import lightgbm as lgb\n# params = {\n#     \"objective\": 'binary',\n#     \"num_leaves\": 24,\n#     \"learning_rate\": 0.2,\n#     \"bagging_fraction\": 0.9440403047411987,\n#     \"bagging_freq\": 8\n# }\n# LGB_Classifier = lgb.LGBMClassifier(objective='binary',num_leaves=31,learning_rate=0.2,bagging_fraction=0.8963,bagging_freq=4)","4bc7fa43":"# # Evaluate Models\n# from sklearn.model_selection import cross_val_score\n# from sklearn import metrics\n\n# models = []\n# # models.append(('Decision Tree Classifier', DTC_Classifier))  #\u7b80\u76f4\u8fc7\u62df\u5408\u4e86\u8bad\u7ec3\u96c6\u4e0a\u8fd11\u7684acc\u4e14\u7edd\u5927\u591a\u6570\u7684\u9884\u6d4b\u6982\u7387\u76f4\u63a51\n# # models.append(('Random Forest Classifier', RTC_Classifier))\n# # models.append(('Extract Tree Classifier', ETC_Classifier))\n# models.append(('BNB Classifier', BNB_Classifier))\n\n\n# # models.append(('XGB Classifier', XGB_Classifier)) #xgb\u7684fit\u8017\u65f6\u5f88\u957f\u4f46\u6548\u679c\u6bd4lgb\u597d\u70b9\uff0c\u7ecf\u6d4bpredict\u9ad8\u70b9\n# # models.append(('LGB Classifier', LGB_Classifier))\n# # models.append(('LogisticRegression', LGR_Classifier))\n# def train_evaluate(x,y,istest=False):\n#     for i, v in models:\n#         if istest==False:\n#             v.fit(x,y)\n#         t1 = time.time()\n#         vpred = v.predict(x)\n# #         scores = cross_val_score(v, x, y, cv=10) #\u5355\u5c31\u8fd9\u4e00\u53e5\u5c31\u7528\u4e86cv\u6b21fit\u51fd\u6570\u592a\u8d39\u65f6\u95f4\n#         accuracy = metrics.accuracy_score(y, vpred) #\u548cmodel.score\u4e00\u6837\u4f5c\u7528\n#         cm = metrics.confusion_matrix(y, vpred)\n#         classification = metrics.classification_report(y, vpred)\n#         print('Cost',time.time()-t1)\n#         print('============================== {} Model Evaluation =============================='.format(i))\n#         print()\n# #         print (\"Cross Validation Mean Score:\" \"\\n\", scores.mean())\n#         print (\"Model Accuracy:\" \"\\n\", accuracy)\n#         print(f\"TNR:{cm[1][1]\/(cm[0][1]+cm[1][1])}\\nTPR:{cm[0][0]\/(cm[0][0]+cm[1][0])}\" \"\\n\")\n#         print(\"Classification report:\", classification)\n# #         show_feature_importance(v)\n#         fpr,tpr,thr = metrics.roc_curve(y,v.predict_proba(x)[:,1],1)\n# #         display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=metrics.auc(fpr,tpr),estimator_name=i)\n#         ac = metrics.auc(fpr,tpr)\n#         plt.plot(fpr,tpr,label=f\"{i}:AUC={ac:1.5f}\") #scatter\u753b\u70b9\u53ef\u89c1\u662f\u79bb\u6563\n#         print('FPR:\\t',fpr.shape)#\u662f\u4e00\u6bcf\u4e2a\n# #         metrics.plot_roc_curve(v,x,y) \u4e3a\u4e86\u753b\u5728\u4e00\u4e2a\u56fe\u91cc\u7528RocCurveDisplay(\u4e5f\u4e0d\u80fd) \u4f46\u8fd9\u4e2a\u662f\u6563\u70b9\u663e\u793a\u6298\u7ebf\uff08\u9ed8\u8ba4\u7684thr\u53d6\u503c\u662f\u6309\u7167score\u800c\u4e0d\u662f0-1\u4e4b\u95f4\u4f9d\u6b21\u53d6\uff09\n#         plt.legend() # \u8bbe\u7f6e\u4f4d\u7f6e\uff0c\u6ca1\u6709\u8fd9\u4e2alabel\u4e0d\u751f\u6548(\u8fd8\u5fc5\u987b\u5728\u6bcf\u6b64plot\u540e)\n#     plt.ylabel('TPR(\u771f\u6b63\u7c7b\u7387\uff1a\u5b9e\u9645\u6b63\u4e2d\u9884\u6d4b\u6b63)',fontsize='15')\n#     plt.xlabel('FPR(\u5047\u6b63\u7c7b\u7387\uff1a\u5b9e\u9645\u8d1f\u4e2d\u9884\u6d4b\u6b63)',fontsize='15')\n#     plt.title('ROC(\u63a5\u6536\u8005\u64cd\u4f5c\u7279\u5f81\uff0c\u6bcf\u4e2a\u9608\u503c\u786e\u5b9a\u4e2a\u70b9)',fontsize='20')\n#     plt.show()\n#     plt.savefig('roc.png')\n#     from IPython.display import FileLink\n#     FileLink('roc.png')\n# import time\n# word_index, X_train, y_train, X_val, y_val, X_test, y_test = tokenizer_data(X,y,MAX_SEQUENCE_LENGTH=80) #\u7528y\u662f\u505a\u56de\u5f52\u7684\n# X_test,y_test\n# Counter(label)\n# # train_evaluate(X_train,y_train) \n# # train_evaluate(X_test,y_test,True) ","1764fbbe":"- View the pageview distribution","d7d50825":"- Use Jieba for Chinese word segmentation and take out the X (We chose to focus only on the abstract column, and we also tried to merge the tags columns without much effect)\n>\u7279\u5f81\u9009\u62e9  \u53d6\u4e00\u5217\uff1f\u53d6\u591a\u5217\u5408\u5e76\u4e00\u5217\uff1f\u53d6\u591a\u5217\uff1f","f49d6854":"- This cell is about training CNN. If there is no GPU, it may be slow, so you can skip this part and use the trained files directly","6a5c8910":"![](cnn_model.png)","ba00e15b":"# Data collection and storage","3db30cc1":" Processing the label\n<!-- - \u79bb\u6563\u95ee\u9898\/\u4e8c\u5206\u7c7b\/\u591a\u5206\u7c7b\n- \u4fdd\u7559\u79bb\u6563\u6570\u636e\u5f53\u4f5c\u56de\u5f52\u9884\u6d4b\u6d4f\u89c8\u6570\n- \u6807\u7b7e\u4e8c\u503c\u5316\uff08\u6807\u51c6\uff1f\u5747\u503c\/\u4e2d\u4f4d\u6570\uff09\n- \u6807\u7b7e\u533a\u95f4\u5316\uff08\u591a\u5c11\u4e2a\uff1f\u6309\u7167\u6700\u5c0f\u6700\u5927\u5747\u5206\uff1f\u6307\u6570\u578b\u533a\u95f4\uff1f\u5747\u503c\u4e3a\u4e2d\u5fc3\u5206\uff1f\uff09 -->","20d6a3d8":"# Exploration and visualization\n- This section is going to visualize X and manipulate X to make it a better representation","7204a86d":"#### A summary of the model\n- We see that the loss of verification set increases first and then decreases in training, indicating that CNN overfits this model, which may be caused by label binarization and data volume\n- \u7528\u9519\u635f\u5931\u51fd\u6570\u4f1a\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u80fd\u8fdb\u884c\n- \u521d\u59cb\u4f7f\u7528cnn\u4f1a\u8fc7\u62df\u5408\u5373loss\u4e0b\u964dval loss\u5148\u964d\u540e\u5347\n- \u5c06\u6a21\u578b\u8f93\u51fa\u6539\u4e3a\u591a\u5206\u7c7b\uff08onehot\u7f16\u7801\u59821\u53d8\u62100 1\uff09\u6539\u52a8\u6ca1\u7528\n- \u6539\u6210\u56de\u5f52loss\u975e\u5e38\u5927\n- \u4f7f\u7528\u6b63\u5219\u5316\u9879\u53cd\u800c\u8bad\u7ec3\u6ca1\u6709\u6548\u679c [keras\u6587\u6863](https:\/\/keras-cn.readthedocs.io\/en\/latest\/other\/regularizers\/)\n- \u4f18\u5316\u5668\u65b9\u9762\uff1aSGD\u662f\u4f18\u5316\u4e0d\u4e86\uff0cadam\u867d\u7136\u66f4\u5feb\u4f46\u4e5f\u662f\u5148\u4e0b\u964d\u540e\u5347\n- \u60f3\u5230\u4e0d\u7ba1\u6807\u7b7e\u5982\u4f55\u5212\u5206\u4e8c\u7c7b\u53ea\u662f\u5f71\u54cdacc\u800closs\u53d8\u5316\u662f\u4e00\u6837\u7684\u6240\u4ee5\u5212\u5206\u6ca1\u591a\u5927\u610f\u4e49\uff0c\u53e6\u4e00\u65b9\u9762\u53d8\u6210\u56de\u5f52\u53ea\u6709loss\u6307\u6807\u4e14\u4e5f\u662f\u5148\u964d\u4f4e\u518d\u9707\u8361\n- \u6362\u6210abstract\u5217\u4ee3\u66fftitle\u4f1a\u597d\u4e9b\u53e6\u5916\u7528\u65b0\u95fb\u4e3b\u9898\u4ee3\u66ff\u5462","fb657f53":"# Experimentation and prediction\n- This section tries neural network CNN and machine learning Bayes and makes predictions","de2eb714":"# Data preparation\n- The data is saved in the PKL format because it is fast to read and write and is small in volume.I have also saved a copy of the CSV for easy viewing, so we can load the file directly here.","89740a6e":"- Here's how to make predictions","af8457e5":"- load data and stopwords","70687ada":"# Contents\n## 1. Data collection and storage\n- Crawler script key code\n- How to use\n\n## 2. Data preparation\n- Import Module\n- Load the data and fetch the data set\n- word segmentation\n\n## 3. Exploration and visualization\n- The distribution of views\n- The words in the training set\n\n## 4. Experimentation and prediction\n- Neural Networks: CNN (train and prediction)\n- Machine learning: Bayes","d8848eba":"- key code\n- you can run this cell but it will cost a lot of time, and you can also run spider.py directly"}}