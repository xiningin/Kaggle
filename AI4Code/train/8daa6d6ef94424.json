{"cell_type":{"a5f00277":"code","73623331":"code","76e0106a":"code","47ed03cc":"code","ef336065":"code","6eaf3cfa":"code","7948574c":"code","872c6507":"code","b82517e1":"code","171e0591":"code","7742c6da":"code","190cc613":"code","4bb4d776":"code","87cfddf2":"code","84f1ad6f":"code","fb2012db":"code","7549c2a9":"code","9888ea5b":"code","bc430592":"code","bea48308":"code","24422ea3":"code","5292b0e1":"code","1b3c131f":"code","78805954":"code","15930ee9":"code","d5b87827":"code","0d7e0744":"code","6aa930cf":"code","a9dcbb08":"code","0f65fd1c":"code","c07932c2":"code","4b943af5":"code","3fa5ada4":"code","626653a2":"code","94b9bb0f":"code","b659e8b7":"code","a969de06":"code","833c6d82":"code","36acbc48":"code","aa649130":"code","b6a261f2":"code","fe7369e4":"code","bfcab69b":"code","1a8d2e08":"code","d944d250":"code","73e355bc":"code","401ecdea":"code","fdba94c4":"code","636ab8e8":"code","41c8521b":"code","441a0195":"code","920aa33f":"code","96e1ce14":"code","b5c6eb2a":"code","03c96460":"markdown","d08d3bb1":"markdown","5dcf6825":"markdown","1896b528":"markdown","fe351d07":"markdown","07916434":"markdown","d15c6d0b":"markdown","5dc0b61e":"markdown","c4c0f701":"markdown","83697530":"markdown","bd443361":"markdown","695336ed":"markdown","23249cd1":"markdown","856be022":"markdown","9bca1a2b":"markdown","4a0c3f9b":"markdown","3d5d298e":"markdown","045645f9":"markdown","998f060f":"markdown","fc5a7432":"markdown","06239c7c":"markdown","561c4470":"markdown","d1571561":"markdown","20f5a84b":"markdown"},"source":{"a5f00277":"import pandas as pd\npd.options.display.max_columns = 999\nimport numpy as np\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import train_test_split\n#sklearn.metrics.accuracy_score(y_true, y_pred, normalize=True, sample_weight=None\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom pandas_profiling import ProfileReport\nimport warnings\nwarnings.simplefilter(action='ignore')\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder","73623331":"test = pd.read_csv(\"..\/input\/seleksidukungaib\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/seleksidukungaib\/train.csv\")\nsample_submission = pd.read_csv(\"..\/input\/seleksidukungaib\/sample_submission.csv\")","76e0106a":"train.columns","47ed03cc":"dropped_column = ['idx', 'userId', 'num_transfer_trx', 'max_transfer_trx',\n                  'min_transfer_trx', 'date', 'date_collected', 'isUpgradedUser']","ef336065":"data = pd.concat([train,test],ignore_index=True)\ndata = data.drop(dropped_column, axis = 1)","6eaf3cfa":"data = data.drop(['average_transfer_trx'], axis = 1)","7948574c":"data.info()","872c6507":"#10 row null values on column isActive, isVerifiedPhone, isVerifiedEmail, blocked, super, userLevel, pinEnabled\ndata.loc[data.isActive.isnull() == True]\n#is churned not null -> di data train -> drop 10 baris tersebut\ndata = data.dropna(subset=[\"isActive\"])","b82517e1":"#isi kolom bertipe object dengan modus\ndata['premium'] = data['premium'].fillna(data['premium'].mode())","171e0591":"# isi yang lainnya dengan median, kecuali isChurned\nfor column in data.columns:\n    if (column != \"isChurned\"):\n        data[column] = data[column].fillna(data[column].median())","7742c6da":"# semua data sudah tidak ada yang null","190cc613":"# encode data yang bertype object agar mudah saat dilakukan modelling (beberapa model hanya bisa menggunakan tipe numeric, seperti regression model)\ncategorical_features = ['premium', 'super', 'pinEnabled']\nle = LabelEncoder()\nfor col in categorical_features:\n    data[col] = le.fit_transform(list(data[col].values))","4bb4d776":"#cap data ke 0.85 persen","87cfddf2":"Q3 = data.quantile(0.85)","84f1ad6f":"Q3","fb2012db":"data.describe()","7549c2a9":"numerik_col = ['average_recharge_trx','average_topup_trx','max_recharge_trx','max_topup_trx',\n              'min_recharge_trx','min_topup_trx','num_recharge_trx','num_topup_trx','num_transaction',\n              'random_number','total_transaction']\n# for col in numerik_col:\n#     data.loc[data[col] >= Q3[col], col] = Q3[col]","9888ea5b":"# average_topup_trx berkorelasi tinggi dengan max_topup_trx\n# data['avg_topup_plus_max_topup'] = data['average_topup_trx'] + data['max_topup_trx']\n# data.drop(['max_topup_trx', 'average_topup_trx'], axis=1, inplace = True)\n# berdasar hasil experimen, lebih buruk jika digabung\n\n# num_transaction berkorelasi tinggi dengan num_recharge_trx\ndata['num_transaction_plus_num_recharge'] = data['num_transaction'] + data['num_recharge_trx']\ndata.drop(['num_transaction', 'num_recharge_trx'], axis=1, inplace = True)\n","bc430592":"# karena train dan test adalah data di bulan berbeda, maka normalisasi dilakukan di masing2 data (split dulu)\n\n## split data to train and test:\ntrain = data[~data.isChurned.isnull()]\ntest = data[data.isChurned.isnull()]\n\n# Normalisasi data numeric data:\n\nnumerik_col = ['max_recharge_trx','average_recharge_trx',\n               'average_topup_trx', 'max_topup_trx',\n              'min_recharge_trx','min_topup_trx','num_topup_trx',\n              'random_number','total_transaction',\n#               'num_recharge_trx','num_transaction'\n              ]\n\nfor col in (numerik_col):\n#     data[col]=((data[col]-data[col].min())\/(data[col].max()-data[col].min()))\n    train[col]=((train[col]-train[col].min())\/(train[col].max()-train[col].min()))\n    test[col]=((test[col]-test[col].min())\/(test[col].max()-test[col].min()))\n","bea48308":"train.describe()","24422ea3":"train.info()","5292b0e1":"print(len(train))\nprint(len(test))","1b3c131f":"# cek data duplikat \ntrain.duplicated().value_counts()\n\n#hasil data duplikat -> 8000 row , action : drop duplikat","78805954":"#drop data duplikat\ntrain.drop_duplicates(keep = 'first', inplace = True) \n  ","15930ee9":"### Cari korelasi data\ntrain.corr().style.background_gradient(cmap='coolwarm')","d5b87827":"# analisis yang korelasinya kurang dari 0.1 dan lebih dari -0.1 (kecil)\n# isActive, isVeriviedPhone, blocked, super, random_number\nmin_cor = ['isActive', 'isVerifiedPhone', 'blocked', 'super', 'random_number']\nfor col in min_cor:\n    print('====== ', col, \" ======\")\n    print(train[col].value_counts())\n","0d7e0744":"#isActive, isVerifiedPhone, blocked dan super memiliki varisi data yang sangat sedikit 90% ++ sama\n# random_number tidak memiliki \n# aksi : drop semua kolom tersebut\ndrop_from_cor = ['isActive', 'isVerifiedPhone', 'blocked', 'super', 'random_number']\ntrain.drop(drop_from_cor, axis = 1, inplace = True)\ntest.drop(drop_from_cor, axis = 1, inplace = True)","6aa930cf":"train.head()","a9dcbb08":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import ExtraTreeClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm.classes import OneClassSVM\nfrom sklearn.neural_network.multilayer_perceptron import MLPClassifier\nfrom sklearn.neighbors.classification import RadiusNeighborsClassifier\nfrom sklearn.neighbors.classification import KNeighborsClassifier\nfrom sklearn.multioutput import ClassifierChain\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.multiclass import OutputCodeClassifier\nfrom sklearn.multiclass import OneVsOneClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.linear_model.stochastic_gradient import SGDClassifier\nfrom sklearn.linear_model.ridge import RidgeClassifierCV\nfrom sklearn.linear_model.ridge import RidgeClassifier\nfrom sklearn.linear_model.passive_aggressive import PassiveAggressiveClassifier    \nfrom sklearn.gaussian_process.gpc import GaussianProcessClassifier\n# from sklearn.ensemble.voting_classifier import VotingClassifier\nfrom sklearn.ensemble.weight_boosting import AdaBoostClassifier\nfrom sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\nfrom sklearn.ensemble.bagging import BaggingClassifier\nfrom sklearn.ensemble.forest import ExtraTreesClassifier\nfrom sklearn.ensemble.forest import RandomForestClassifier\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.semi_supervised import LabelPropagation\nfrom sklearn.semi_supervised import LabelSpreading\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.naive_bayes import MultinomialNB  \nfrom sklearn.neighbors import NearestCentroid\nfrom sklearn.svm import NuSVC\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.svm import SVC\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\n# from sklearn.mixture import DPGMM\n# from sklearn.mixture import GMM \n# from sklearn.mixture import GaussianMixture\n# from sklearn.mixture import VBGMM","0f65fd1c":"Y = train[\"isChurned\"]\nX = train.drop([\"isChurned\"], axis = 1)","c07932c2":"random_state = 1\nX_train, X_valid, y_train, y_valid = train_test_split(X, Y, test_size = 0.2, random_state = random_state)","4b943af5":"def get_kfold():\n    return KFold(n_splits=5, shuffle=True, random_state=1)","3fa5ada4":"all_model = [RandomForestClassifier(),ExtraTreeClassifier(), LogisticRegression(),RidgeClassifier(),\n             DecisionTreeClassifier(), KNeighborsClassifier(), PassiveAggressiveClassifier(),\n#              BernoulliNB(), GaussianNB(), CalibratedClassifierCV(), \n#              AdaBoostClassifier(), GradientBoostingClassifier()\n#              LinearSVC(), NuSVC() , SVC()\n            ]","626653a2":"params = {'loss_function':'Logloss', # objective function\n          'eval_metric':'F1', # metric\n#           'cat_features' : categorical_features,\n          'iterations' : 1000,\n          'learning_rate': 0.01, \n#            'task_type': \"GPU\", # enable GPU\n          'verbose': 1000, # output to stdout info about training process every 1000 iterations\n          'random_seed': random_state\n         }\ncbc = CatBoostClassifier(**params)","94b9bb0f":"data_dmatrix = xgb.DMatrix(data=X,label=Y)","b659e8b7":"params = {\"objective\":\"binary:logistic\",'colsample_bytree': 0.3,'learning_rate': 0.1,\n                'max_depth': 10, 'alpha': 10}\n\n# cv_results = xgb.cv(dtrain=data_dmatrix, params=params, nfold=5,\n#                     num_boost_round=50,early_stopping_rounds=10,metrics=\"auc\", as_pandas=True, seed=123)","a969de06":"all_model.append(cbc)\nall_model.append(XGBClassifier())","833c6d82":"for model in all_model:\n    scores = cross_val_score(model, X, Y, cv=get_kfold(), scoring='f1')\n    print(\"Model : \" , model , \" has f1 score : \" , scores.mean())","36acbc48":"#setelah eksperimen dan percobaan submission, model logistic regression paling nggak overfitting\n# choose logistic regression for Hyperparameter tuning","aa649130":"from sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import GridSearchCV","b6a261f2":"model = LogisticRegression()\nsolvers = ['newton-cg', 'lbfgs', 'liblinear']\npenalty = ['l2']\nc_values = [100, 10, 1.0, 0.1, 0.01]","fe7369e4":"grid = dict(solver=solvers,penalty=penalty,C=c_values)\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='f1',error_score=0, verbose = 3)\ngrid_result = grid_search.fit(X, Y)\n","bfcab69b":"# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","1a8d2e08":"#use all data for final train, use best param from tuning\nmodel =  LogisticRegression(C= 1.0, penalty= 'l2', solver = 'newton-cg')","d944d250":"scores = cross_val_score(model, X, Y, cv=get_kfold(), scoring='f1')\nprint(scores.mean())","73e355bc":"model = model.fit(X,Y)","401ecdea":"test = test.drop(['isChurned'], axis = True)","fdba94c4":"prediction = model.predict(test)","636ab8e8":"prediction = prediction.astype(int)","41c8521b":"len(prediction)","441a0195":"len(sample_submission)","920aa33f":"#LogisticRegression\nsample_submission[\"isChurned\"] = prediction\nsample_submission[\"isChurned\"].value_counts()","96e1ce14":"sample_submission.to_csv(\"submission.csv\", index = False)","b5c6eb2a":"#best : logreg kfold => 0.894","03c96460":"### Load data","d08d3bb1":"#jalanin di local biar keliatan jelas\n#profile semua data \nProfileReport(data, title='All data Pandas Profiling Report')","5dcf6825":"### XGBoost Model","1896b528":"#### From all data :\n- average_transfer has 45% missing values : drop\n- average_topup_trx berkorelasi tinggi dengan max_topup_trx\n- num_transaction berkorelasi tinggi dengan num_recharge_trx","fe351d07":"# Feature Engineering","07916434":"# SELEKSI LAB GAIB\nMuhamamd Zunan Alfikri | 13518019","d15c6d0b":"#Jalanain di local biar keliatan jelas\ntest_profile = ProfileReport(test, title='Test Pandas Profiling Report')\ntest_profile","5dc0b61e":"# Exploratory Data Analysis","c4c0f701":"# Hyperparameter Tunning\nmencari parameter terbaik agar akurasi model meningkat, tuning menggunakan Gridsearch","83697530":"#jalanin di local biar keliatan jelas\ntrain_profile = ProfileReport(train, title='Train Pandas Profiling Report')\ntrain_profile","bd443361":"### Yang diperoleh dari pandas profiling test\n- date, date_collected, isUpgradeUser punya nilai yang konstan , aksi : drop kolom\n- user_id, idx memiliki nilai yang unik, aksi : drop column\n- num_transaction berkorelasi tinggi dengan num_recharge_trx","695336ed":"### Yang diperoleh dari pandas profiling train:\n- num_transfer_trx, max_transfer_trx, min_transfer_trx mempunyai nilai konstan, aksi : drop column\n- idx unique, aksi : drop column\n- Korelasi tinggi :\n    - max_topup_trx berkorelasi tinggi dengan average_topup_trx\n    - min_topup_trx berkolerasi tinggi dengan avarage_topup_trx\n    - num_transaction berkorelasi tinggi dengan num_recharge_trx\n- Missing :\n    - average_transfer_trx : 66% null values\n","23249cd1":"# Submission","856be022":"## 3. Label encoding","9bca1a2b":"## 4. Handling outliers","4a0c3f9b":"## 2. Handling Null Values","3d5d298e":"#### Aksi :\n- drop num_transfer_trx, max_transfer_trx, min_transfer_trx, date, date_collected, isUpgradeUser, idx, user_id","045645f9":"### Catboost Model","998f060f":"## KFOLD","fc5a7432":"# Modelling","06239c7c":"## Final Train : ","561c4470":"## 1. Profiling untuk melihat gambaran seluruh data","d1571561":"### Import dependencies","20f5a84b":"## Some Sklearn Model :"}}