{"cell_type":{"a0af08bc":"code","9bf8c520":"code","d4d8e79a":"code","3b3f2f83":"code","09006228":"code","2c10e12c":"code","62b4fe4a":"code","e27ca794":"code","4c4541ad":"code","3108ac88":"code","0ae63ace":"code","7a5a20ae":"code","a4e64b55":"code","44dae775":"code","b0570f89":"code","077efc6b":"code","e3975fb4":"code","c4e1bcd0":"code","12bdd47a":"code","85191f32":"code","f5018618":"code","a2f1e275":"code","8b159ff7":"code","42a5e141":"code","08c884cd":"code","898fdacc":"code","3c8c8277":"code","7ac44e26":"markdown","2310f1e9":"markdown","67efb9fd":"markdown","f7630479":"markdown","e6b8410c":"markdown"},"source":{"a0af08bc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","9bf8c520":"import numpy as np\nimport pandas as pd\nimport random\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt","d4d8e79a":"Path = \"\/kaggle\/input\/iris\/Iris.csv\"\nPath1 = \"C:\/Users\/wilkm\/Desktop\/aaaa\/iris1.xlsx\"\n\niris =  pd.read_csv(Path)\niris = iris.iloc[:, 1:6]\niris.head()","3b3f2f83":"# normalising features\niris_trans = iris\niris_trans.head()\ndata = iris_trans.iloc[:, 0:4]\nvalues = data.values\nscaler = MinMaxScaler()\nprint(scaler.fit(data))\nMinMaxScaler(copy=True, feature_range=(0, 1))\niris_trans.iloc[:, 0:4] = scaler.transform(data)\niris_trans.Species = pd.Categorical(iris_trans.Species)\niris_trans['categ'] = iris_trans.Species.cat.codes\niris_trans.head()","09006228":"iris_trans[\"categ\"].value_counts()\n# balanced dataset","2c10e12c":"#splitting data to training and test tests \ntrain_df, test_df = train_test_split(iris_trans, test_size = 0.25,random_state=42)","62b4fe4a":"# visualising data\ncateg = iris_trans.iloc[:, 5]\nfeat = iris_trans.iloc[:, 0:2]\ncateg = iris_trans.iloc[:, 5]\nfeat = iris_trans.iloc[:, 0:2]\nplt.scatter(feat.iloc[:,0], feat.iloc[:,1], c = categ)\nplt.title(\"Iris data set\")","e27ca794":"train_x = np.array(train_df.iloc[:, 0:4])\ntrain_x  = train_x[:, [0,1]]\ntest_x = np.array(test_df.iloc[:, 0:4])\ntest_x  = test_x[:, [0,1]]\ntrain_y = np.array(pd.get_dummies(train_df.iloc[:, 5]))\ntest_y = np.array(pd.get_dummies(test_df.iloc[:, 5]))","4c4541ad":"# First attempt to bulid neural network and to use the object-oriented programming \ndef sigmoid(x):\n        return 1 \/ (1 + np.exp(-x))\n\ndef deriv_sigmoid(x):\n    return sigmoid(x) * (1 - sigmoid(x))\n\ndef softmax(x):\n    expx= np.exp(x)\n    return expx \/ expx.sum(axis=1, keepdims=True)\n\n\n\n\n\nclass NeuralNetwork:\n    import matplotlib.pyplot as plt\n    def __init__(self, input_x, output_act, alpha, epochs):\n   \n        self.input_x      = input_x\n        self.output_act = output_act\n        \n        no_of_examp = self.input_x .shape[0]\n        \n        self.W1   = np.random.rand(self.input_x.shape[1],4) \n        self.B1   = np.random.randn(4)            \n        self.W2   = np.random.rand(4,self.output_act.shape[1]) \n        self.B2   = np.random.randn(self.output_act.shape[1])\n        self.output     = np.zeros(self.output_act.shape)\n        self.alpha = alpha\n        self.epochs = epochs\n        self.error = [] \n   \n     \n    \n    def feedforward(self, input_x):\n        self.v1 = np.dot(input_x, self.W1) + self.B1\n        self.layer1 = sigmoid(self.v1)\n        self.v2 = np.dot(self.layer1, self.W2) + self.B2      \n        self.output = softmax(self.v2)\n        return self.output\n    \n    def backprop(self, input_x, output_act, alpha):\n        out_error = self.output - self.output_act\n        der_W2 = np.dot(self.layer1.T, out_error)\n        der_B2 = out_error\n        \n        der_W1 = np.dot(self.input_x.T, ((self.layer1 *(1-self.layer1)) * np.dot(out_error , self.W2.T)))\n        der_B1 = (self.layer1 *(1-self.layer1)) * np.dot(out_error , self.W2.T)\n        \n        self.W1 -= alpha * der_W1\n        self.B1 -= alpha * der_B1.sum(axis=0)\n\n        self.W2 -= alpha * der_W2\n        self.B2 -= alpha * der_B2.sum(axis=0) \n        #return self.W1, self.B1, self.W2, self.B2 \n    def train(self, input_x, output_act, alpha, epochs):\n        self.error = []\n        for epoch in range(self.epochs):\n            self.feedforward(input_x)\n            self.backprop(input_x, output_act, alpha)\n            if epoch % 200 == 0:\n                loss = np.sum(-1*np.multiply(output_act, np.log(self.output)))\n                #loss = loss\/no_of_examp\n                print('Loss function value: ', loss)\n                self.error.append(loss)\n        plt.plot(self.error)\n        plt.title(\"Loss\")\n    def test(self, data, act_y, results = False):\n        res = np.around(self.feedforward(data))\n        x = 0\n        for i in range(len(res)):\n            if(all(act_y[i]==res[i])):\n                x+= 1\n        acc = round((x \/ len(res)), 2)\n        print('Accuracy: {0:2.2f}'.format(acc))\n        if results == True:\n            print(\"Predicted values: \", \"\\n\" ,res)\n       ","3108ac88":"# training the network\nnew_network = NeuralNetwork(train_x, train_y, 0.01, 5000)\nnew_network.train(train_x, train_y, 0.01, 5000)","0ae63ace":"# print training accuracy\nnew_network.test(train_x, train_y)","7a5a20ae":"# print test data accuracy and results\nnew_network.test(test_x,test_y, True)","a4e64b55":"import pandas as pd\nmushrooms = pd.read_csv(\"..\/input\/mushroom-classification\/mushrooms.csv\", header = 0)\nmushrooms.head()","44dae775":"print( mushrooms[\"class\"].value_counts()\/mushrooms.shape[0]*100)\n# looking an % values, this is a balanced data set","b0570f89":"print(mushrooms.shape)\n# there are 22 features in the set, and all features are categorical","077efc6b":"!pip install prince","e3975fb4":"import prince","c4e1bcd0":"mca = prince.MCA()\nmushrooms_mca_categ = mushrooms.iloc[:, 0] # extract labels\nmushrooms_mca = mushrooms.iloc[:, 1:] # features\nmushrooms_mca.head()","12bdd47a":"mca = mca.fit(mushrooms_mca) # same as calling ca.fs_r(1)\nmca = mca.transform(mushrooms_mca) # same as calling ca.fs_r_sup(df_new) for *another* test set.\nprint(mca.head(20))","85191f32":"print(mca.head(20))","f5018618":"mushrooms_new = pd.concat([mca, mushrooms_mca_categ], axis=1)\nmushrooms_new[\"class\"] = pd.Categorical(mushrooms_new[\"class\"])\nmushrooms_new[\"class_cat\"] = mushrooms_new[\"class\"].cat.codes\nmushrooms_new.head()\n","a2f1e275":"labels = mushrooms_new[\"class_cat\"]# poisonus1, edible0\nfeat1 = mushrooms_new[0]\nfeat2 = mushrooms_new[1]\nplt.scatter(feat1, feat2, c = labels)\nplt.title(\"Musrooms MCA features\")","8b159ff7":"mush_train, mush_test = train_test_split(mushrooms_new, test_size = 0.25, random_state = 42)\nmush_train_x = np.array(mush_train.iloc[:, 0:2])\nmush_train_y = np.array(pd.get_dummies(mush_train.iloc[:, 3]))\nmush_test_x = np.array(mush_test.iloc[:, 0:2])\nmush_test_y = np.array(pd.get_dummies(mush_test.iloc[:, 3]))","42a5e141":"new_network2 = NeuralNetwork(mush_train_x, mush_train_y, 0.0001, 5000)","08c884cd":"new_network2.train(mush_train_x, mush_train_y, 0.0001, 5000)","898fdacc":"new_network2.test(mush_train_x, mush_train_y)","3c8c8277":"new_network2.test(mush_test_x, mush_test_y)","7ac44e26":"The accuracy is very good, that can be expected for a well categorised data set","2310f1e9":"Performing multiple correspondence analysis wii reduce dimensionality\nIt is \n","67efb9fd":"This is my attempt to build the Neural Network from scratch and classify Iris species","f7630479":"After MCA classes look rather well separable","e6b8410c":"Both training and test results have very good accuracy"}}