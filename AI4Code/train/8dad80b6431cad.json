{"cell_type":{"bbeb1f33":"code","19e9b6b5":"code","a563dfd0":"code","ba9bd367":"code","72a45d50":"code","c906e91a":"code","772cb82f":"code","2a224dfb":"code","7a6cffe0":"code","c701597e":"code","d51955e5":"code","66e93c46":"code","5e62f850":"code","0399021b":"code","b257b147":"code","3052faa1":"code","87481fe5":"code","936d7efd":"code","20fc9bc4":"code","53801c59":"code","cd6dc49a":"markdown","c3513988":"markdown","180a4cb7":"markdown","663ac7a1":"markdown","48a4a0db":"markdown","5952b83a":"markdown","56adec7e":"markdown","bd9cefa0":"markdown","d230ab79":"markdown","5b5b893d":"markdown","9875a8eb":"markdown","ea9f6132":"markdown","79ab481a":"markdown","1ea928e7":"markdown","11d81ec7":"markdown"},"source":{"bbeb1f33":"import warnings\nwarnings.filterwarnings(\"ignore\")\n# Some Common Imports\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom subprocess import check_output\n%matplotlib inline\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport os\nimport gc\n\n# Imports for Preprocessing\nimport re                              # regular expressions for removing HTML tags etc.\nfrom nltk.corpus import stopwords      # stop words for filtering out useless wods ex. (as, a , the, etc.) \nimport distance                        # to mesure the distance ( in turn similarity )\nfrom nltk.stem import PorterStemmer    # to do stemming ( find the base word )\nfrom bs4 import BeautifulSoup\n\nfrom fuzzywuzzy import fuzz            # find the detail here ( http:\/\/chairnerd.seatgeek.com\/fuzzywuzzy-fuzzy-string-matching-in-python )\nfrom MulticoreTSNE import MulticoreTSNE as TSNE     # to visualize after dimansionality reduction (https:\/\/github.com\/DmitryUlyanov\/Multicore-TSNE)\n\nfrom wordcloud import WordCloud, STOPWORDS # to get an Image of most occuring words\nfrom os import path                        # to get files in the OS\nfrom PIL import Image                      # to work with Images\n\nfrom multiprocessing import Pool           # Very important to divide work in different cores\n","19e9b6b5":"import nltk\nnltk.download('stopwords')","a563dfd0":"df = pd.read_csv('..\/input\/quora-question-pairs-similarity-eda\/df_fe_without_preprocessing_train.csv')","ba9bd367":"df.head(2)","72a45d50":"# To get the results in 4 decemal points\nSAFE_DIV = 0.0001 \n\nSTOP_WORDS = stopwords.words(\"english\")\n\n\ndef preprocess(x):\n    x = str(x).lower()\n    x = x.replace(\",000,000\", \"m\").replace(\",000\", \"k\").replace(\"\u2032\", \"'\").replace(\"\u2019\", \"'\")\\\n                           .replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\")\\\n                           .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\")\\\n                           .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\")\\\n                           .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\")\\\n                           .replace(\"%\", \" percent \").replace(\"\u20b9\", \" rupee \").replace(\"$\", \" dollar \")\\\n                           .replace(\"\u20ac\", \" euro \").replace(\"'ll\", \" will\")\n    x = re.sub(r\"([0-9]+)000000\", r\"\\1m\", x)\n    x = re.sub(r\"([0-9]+)000\", r\"\\1k\", x)\n    \n    \n    porter = PorterStemmer()\n    pattern = re.compile('\\W')\n    \n    if type(x) == type(''):\n        x = re.sub(pattern, ' ', x)\n    \n    \n    if type(x) == type(''):\n        x = porter.stem(x)\n        example1 = BeautifulSoup(x)\n        x = example1.get_text()\n               \n    \n    return x\n    ","c906e91a":"def get_token_features(q1, q2):\n    token_features = [0.0]*10\n    \n    # Converting the Sentence into Tokens: \n    q1_tokens = q1.split()\n    q2_tokens = q2.split()\n\n    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n        return token_features\n    # Get the non-stopwords in Questions\n    q1_words = set([word for word in q1_tokens if word not in STOP_WORDS])\n    q2_words = set([word for word in q2_tokens if word not in STOP_WORDS])\n    \n    #Get the stopwords in Questions\n    q1_stops = set([word for word in q1_tokens if word in STOP_WORDS])\n    q2_stops = set([word for word in q2_tokens if word in STOP_WORDS])\n    \n    # Get the common non-stopwords from Question pair\n    common_word_count = len(q1_words.intersection(q2_words))\n    \n    # Get the common stopwords from Question pair\n    common_stop_count = len(q1_stops.intersection(q2_stops))\n    \n    # Get the common Tokens from Question pair\n    common_token_count = len(set(q1_tokens).intersection(set(q2_tokens)))\n    \n    \n    token_features[0] = common_word_count \/ (min(len(q1_words), len(q2_words)) + SAFE_DIV)\n    token_features[1] = common_word_count \/ (max(len(q1_words), len(q2_words)) + SAFE_DIV)\n    token_features[2] = common_stop_count \/ (min(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n    token_features[3] = common_stop_count \/ (max(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n    token_features[4] = common_token_count \/ (min(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n    token_features[5] = common_token_count \/ (max(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n    \n    # Last word of both question is same or not\n    token_features[6] = int(q1_tokens[-1] == q2_tokens[-1])\n    \n    # First word of both question is same or not\n    token_features[7] = int(q1_tokens[0] == q2_tokens[0])\n    \n    token_features[8] = abs(len(q1_tokens) - len(q2_tokens))\n    \n    #Average Token Length of both Questions\n    token_features[9] = (len(q1_tokens) + len(q2_tokens))\/2\n    return token_features\n\n# get the Longest Common sub string\n\ndef get_longest_substr_ratio(a, b):\n    strs = list(distance.lcsubstrings(a, b))\n    if len(strs) == 0:\n        return 0\n    else:\n        return len(strs[0]) \/ (min(len(a), len(b)) + 1)\n\ndef extract_features(df):\n    # preprocessing each question\n    df[\"question1\"] = df[\"question1\"].fillna(\"\").apply(preprocess)\n    df[\"question2\"] = df[\"question2\"].fillna(\"\").apply(preprocess)\n\n    print(\"token features...\")\n    \n    # Merging Features with dataset\n    \n    token_features = df.apply(lambda x: get_token_features(x[\"question1\"], x[\"question2\"]), axis=1)\n    \n    df[\"cwc_min\"]       = list(map(lambda x: x[0], token_features))\n    df[\"cwc_max\"]       = list(map(lambda x: x[1], token_features))\n    df[\"csc_min\"]       = list(map(lambda x: x[2], token_features))\n    df[\"csc_max\"]       = list(map(lambda x: x[3], token_features))\n    df[\"ctc_min\"]       = list(map(lambda x: x[4], token_features))\n    df[\"ctc_max\"]       = list(map(lambda x: x[5], token_features))\n    df[\"last_word_eq\"]  = list(map(lambda x: x[6], token_features))\n    df[\"first_word_eq\"] = list(map(lambda x: x[7], token_features))\n    df[\"abs_len_diff\"]  = list(map(lambda x: x[8], token_features))\n    df[\"mean_len\"]      = list(map(lambda x: x[9], token_features))\n   \n    #Computing Fuzzy Features and Merging with Dataset\n    \n    # do read this blog: http:\/\/chairnerd.seatgeek.com\/fuzzywuzzy-fuzzy-string-matching-in-python\/\n    # https:\/\/stackoverflow.com\/questions\/31806695\/when-to-use-which-fuzz-function-to-compare-2-strings\n    # https:\/\/github.com\/seatgeek\/fuzzywuzzy\n    print(\"fuzzy features..\")\n\n    df[\"token_set_ratio\"]       = df.apply(lambda x: fuzz.token_set_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n    # The token sort approach involves tokenizing the string in question, sorting the tokens alphabetically, and \n    # then joining them back into a string We then compare the transformed strings with a simple ratio().\n    df[\"token_sort_ratio\"]      = df.apply(lambda x: fuzz.token_sort_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n    df[\"fuzz_ratio\"]            = df.apply(lambda x: fuzz.QRatio(x[\"question1\"], x[\"question2\"]), axis=1)\n    df[\"fuzz_partial_ratio\"]    = df.apply(lambda x: fuzz.partial_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n    df[\"longest_substr_ratio\"]  = df.apply(lambda x: get_longest_substr_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n    return df","772cb82f":"def parallelize_dataframe(df, func, n_cores=4):\n    df_split = np.array_split(df, n_cores)\n    pool = Pool(n_cores)\n    df = pd.concat(pool.map(func, df_split))\n    pool.close()\n    pool.join()\n    return df","2a224dfb":"print(\"Extracting features for train:\")\ndf = pd.read_csv(\"..\/input\/quora-question-pairs-similarity-eda\/train.csv\")\ndf = parallelize_dataframe(df, extract_features)\ndf.to_csv(\"nlp_features_train.csv\", index=False)\ndf.head()","7a6cffe0":"df_duplicate = df[df['is_duplicate'] == 1]\ndfp_nonduplicate = df[df['is_duplicate'] == 0]\n\n# Converting 2d array of q1 and q2 and flatten the array: like {{1,2},{3,4}} to {1,2,3,4}\np = np.dstack([df_duplicate[\"question1\"], df_duplicate[\"question2\"]]).flatten()\nn = np.dstack([dfp_nonduplicate[\"question1\"], dfp_nonduplicate[\"question2\"]]).flatten()\n\nprint (\"Number of data points in class 1 (duplicate pairs) :\",len(p))\nprint (\"Number of data points in class 0 (non duplicate pairs) :\",len(n))\n\n#Saving the np array into a text file\nnp.savetxt('train_p.txt', p, delimiter=' ', fmt='%s')\nnp.savetxt('train_n.txt', n, delimiter=' ', fmt='%s')","c701597e":"# reading the text files and removing the Stop Words:\nd = path.dirname('.')\n\ntextp_w = open(path.join(d, 'train_p.txt')).read()\ntextn_w = open(path.join(d, 'train_n.txt')).read()\nstopwords = set(STOPWORDS)\nstopwords.add(\"said\")\nstopwords.add(\"br\")\nstopwords.add(\" \")\nstopwords.remove(\"not\")\n\nstopwords.remove(\"no\")\n#stopwords.remove(\"good\")\n#stopwords.remove(\"love\")\nstopwords.remove(\"like\")\n#stopwords.remove(\"best\")\n#stopwords.remove(\"!\")\nprint (\"Total number of words in duplicate pair questions :\",len(textp_w))\nprint (\"Total number of words in non duplicate pair questions :\",len(textn_w))","d51955e5":"wc = WordCloud(background_color=\"white\", max_words=len(textp_w), stopwords=stopwords)\nwc.generate(textp_w)\nprint (\"Word Cloud for Duplicate Question pairs\")\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","66e93c46":"wc = WordCloud(background_color=\"white\", max_words=len(textn_w),stopwords=stopwords)\n# generate word cloud\nwc.generate(textn_w)\nprint (\"Word Cloud for non-Duplicate Question pairs:\")\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","5e62f850":"n = df.shape[0]\nsns.pairplot(df[['ctc_min', 'cwc_min', 'csc_min', 'token_sort_ratio', 'is_duplicate']][0:n], hue='is_duplicate', vars=['ctc_min', 'cwc_min', 'csc_min', 'token_sort_ratio'])\nplt.show()","0399021b":"# Distribution of the token_sort_ratio\nplt.figure(figsize=(10, 8))\n\nplt.subplot(1,2,1)\nsns.violinplot(x = 'is_duplicate', y = 'token_sort_ratio', data = df[0:] , )\n\nplt.subplot(1,2,2)\nsns.distplot(df[df['is_duplicate'] == 1.0]['token_sort_ratio'][0:] , label = \"1\", color = 'red')\nsns.distplot(df[df['is_duplicate'] == 0.0]['token_sort_ratio'][0:] , label = \"0\" , color = 'blue' )\nplt.show()","b257b147":"plt.figure(figsize=(10, 8))\n\nplt.subplot(1,2,1)\nsns.violinplot(x = 'is_duplicate', y = 'fuzz_ratio', data = df[0:] , )\n\nplt.subplot(1,2,2)\nsns.distplot(df[df['is_duplicate'] == 1.0]['fuzz_ratio'][0:] , label = \"1\", color = 'red')\nsns.distplot(df[df['is_duplicate'] == 0.0]['fuzz_ratio'][0:] , label = \"0\" , color = 'blue' )\nplt.show()","3052faa1":"# Using TSNE for Dimentionality reduction for 15 Features(Generated after cleaning the data) to 3 dimention\n\nfrom sklearn.preprocessing import MinMaxScaler\n\ndfp_subsampled = df[0:5000]\nX = MinMaxScaler().fit_transform(dfp_subsampled[['cwc_min', 'cwc_max', 'csc_min', 'csc_max' , 'ctc_min' , 'ctc_max' , 'last_word_eq', 'first_word_eq' , 'abs_len_diff' , 'mean_len' , 'token_set_ratio' , 'token_sort_ratio' ,  'fuzz_ratio' , 'fuzz_partial_ratio' , 'longest_substr_ratio']])\ny = dfp_subsampled['is_duplicate'].values","87481fe5":"tsne2d = TSNE(\n    n_components=2,\n    init='random', # pca\n    random_state=101,\n    method='barnes_hut',\n    n_iter=5000,\n    verbose=2,\n    angle=0.5,\n    n_jobs=8\n).fit_transform(X)","936d7efd":"df = pd.DataFrame({'x':tsne2d[:,0], 'y':tsne2d[:,1] ,'label':y})\n\n# draw the plot in appropriate place in the grid\nsns.lmplot(data=df, x='x', y='y', hue='label', fit_reg=False, size=8,palette=\"Set1\",markers=['s','o'])\nplt.title(\"perplexity : {} and max_iter : {}\".format(30, 5000))\nplt.show()","20fc9bc4":"tsne3d = TSNE(\n    n_components=3,\n    init='random', # pca\n    random_state=101,\n    method='barnes_hut',\n    n_iter=5000,\n    verbose=2,\n    angle=0.5,\n    n_jobs = 8\n).fit_transform(X)","53801c59":"trace1 = go.Scatter3d(\n    x=tsne3d[:,0],\n    y=tsne3d[:,1],\n    z=tsne3d[:,2],\n    mode='markers',\n    marker=dict(\n        sizemode='diameter',\n        color = y,\n        colorscale = 'Portland',\n        colorbar = dict(title = 'duplicate'),\n        line=dict(color='rgb(255, 255, 255)'),\n        opacity=0.75\n    )\n)\n\ndata=[trace1]\nlayout=dict(height=800, width=800, title='3d embedding with engineered features')\nfig=dict(data=data, layout=layout)\npy.iplot(fig, filename='3DBubble')","cd6dc49a":"__ Word Clouds generated from non duplicate pair question's text __","c3513988":"# Quora Question Pair Similarity Featue Engineering\n","180a4cb7":"<h4> 3.5.1.2 Pair plot of features ['ctc_min', 'cwc_min', 'csc_min', 'token_sort_ratio'] <\/h4>","663ac7a1":"<h2>Preprocessing of Text <\/h2>","48a4a0db":"### Hope you enjoyed it, pls. upvote ifyou enjoyed \ud83d\udc4d\ud83d\udc4d","5952b83a":"- Creating Word Cloud of Duplicates and Non-Duplicates Question pairs\n- We can observe the most frequent occuring words","56adec7e":"Definition:\n- __Token__: You get a token by splitting sentence a space\n- __Stop_Word__ : stop words as per NLTK.\n- __Word__ : A token that is not a stop_word\n\n\nFeatures:\n- __cwc_min__ :  Ratio of common_word_count to min lenghth of word count of Q1 and Q2 <br>cwc_min = common_word_count \/ (min(len(q1_words), len(q2_words))\n<br>\n<br>\n- __cwc_max__ :  Ratio of common_word_count to max lenghth of word count of Q1 and Q2 <br>cwc_max = common_word_count \/ (max(len(q1_words), len(q2_words))\n<br>\n<br>\n- __csc_min__ :  Ratio of common_stop_count to min lenghth of stop count of Q1 and Q2 <br> csc_min = common_stop_count \/ (min(len(q1_stops), len(q2_stops))\n<br>\n<br>\n- __csc_max__ :  Ratio of common_stop_count to max lenghth of stop count of Q1 and Q2<br>csc_max = common_stop_count \/ (max(len(q1_stops), len(q2_stops))\n<br>\n<br>\n- __ctc_min__ :  Ratio of common_token_count to min lenghth of token count of Q1 and Q2<br>ctc_min = common_token_count \/ (min(len(q1_tokens), len(q2_tokens))\n<br>\n<br>\n\n- __ctc_max__ :  Ratio of common_token_count to max lenghth of token count of Q1 and Q2<br>ctc_max = common_token_count \/ (max(len(q1_tokens), len(q2_tokens))\n<br>\n<br>\n        \n- __last_word_eq__ :  Check if First word of both questions is equal or not<br>last_word_eq = int(q1_tokens[-1] == q2_tokens[-1])\n<br>\n<br>\n\n- __first_word_eq__ :  Check if First word of both questions is equal or not<br>first_word_eq = int(q1_tokens[0] == q2_tokens[0])\n<br>\n<br>\n        \n- __abs_len_diff__ :  Abs. length difference<br>abs_len_diff = abs(len(q1_tokens) - len(q2_tokens))\n<br>\n<br>\n\n- __mean_len__ :  Average Token Length of both Questions<br>mean_len = (len(q1_tokens) + len(q2_tokens))\/2\n<br>\n<br>\n\n\n- __fuzz_ratio__ :  https:\/\/github.com\/seatgeek\/fuzzywuzzy#usage\nhttp:\/\/chairnerd.seatgeek.com\/fuzzywuzzy-fuzzy-string-matching-in-python\/\n<br>\n<br>\n\n- __fuzz_partial_ratio__ :  https:\/\/github.com\/seatgeek\/fuzzywuzzy#usage\nhttp:\/\/chairnerd.seatgeek.com\/fuzzywuzzy-fuzzy-string-matching-in-python\/\n<br>\n<br>\n\n\n- __token_sort_ratio__ : https:\/\/github.com\/seatgeek\/fuzzywuzzy#usage\nhttp:\/\/chairnerd.seatgeek.com\/fuzzywuzzy-fuzzy-string-matching-in-python\/\n<br>\n<br>\n\n\n- __token_set_ratio__ : https:\/\/github.com\/seatgeek\/fuzzywuzzy#usage\nhttp:\/\/chairnerd.seatgeek.com\/fuzzywuzzy-fuzzy-string-matching-in-python\/\n<br>\n<br>\n\n\n\n\n\n- __longest_substr_ratio__ :  Ratio of length longest common substring to min lenghth of token count of Q1 and Q2<br>longest_substr_ratio = len(longest common substring) \/ (min(len(q1_tokens), len(q2_tokens))\n","bd9cefa0":"**<h3> Analysis of extracted features <\/h3>","d230ab79":"__ Word Clouds generated from  duplicate pair question's text __","5b5b893d":"- Function to Compute and get the features : With 2 parameters of Question 1 and Question 2","9875a8eb":"<h4>Plotting Word clouds<\/h4>","ea9f6132":"<h3> Visualization <\/h3>","79ab481a":"- Preprocessing:\n    - Removing html tags \n    - Removing Punctuations\n    - Performing stemming\n    - Removing Stopwords\n    - Expanding contractions etc.","1ea928e7":"<h2> Advanced Feature Extraction (NLP and Fuzzy Features) <\/h2>","11d81ec7":"This is a continuation of a previous kernel and hense I would recommend you have a look at it first - https:\/\/www.kaggle.com\/abhishekprajapat\/quora-question-pairs-similarity-eda"}}