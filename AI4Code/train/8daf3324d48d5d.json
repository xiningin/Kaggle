{"cell_type":{"0be12909":"code","a42bf131":"code","98eaa74e":"code","65361d42":"code","08f37acc":"code","5ae4cc50":"code","520c8767":"code","760da48b":"code","d38eb373":"code","0f154647":"code","22f917f2":"code","ca523030":"code","fec7582d":"code","559c212c":"code","b0ba420c":"code","edaa969a":"code","3d984722":"code","69c5bad3":"code","0d356d42":"code","f036a332":"code","7f3075c3":"code","d5ccf630":"code","cdd421d9":"code","99f49a79":"code","785a820d":"code","87f77159":"code","c27cd6ea":"code","1d1bb418":"code","c26a538d":"code","8df9ffa0":"code","61323765":"code","e4e64195":"code","40e5af3c":"code","a979d60a":"code","c0c581f6":"code","9a34b399":"code","a7fb0f2a":"code","39bceeb8":"code","0a7b501f":"code","37ac6d6d":"code","7d6598ff":"code","e118ebcf":"code","18a6a9c0":"code","2cbcbe91":"code","6a09daf7":"code","32d3b04f":"code","34d12566":"code","3d46cc21":"code","bd05fe36":"code","cb0666c4":"code","5dda6a1b":"code","3ff00758":"code","230324a7":"code","3239410e":"code","9eb437ce":"code","72f43947":"code","8d8c5461":"code","750b2ddf":"code","1d468856":"code","08b4577a":"code","8ce388e9":"code","d16a8fd4":"code","7479d1f0":"code","e3afc6e2":"code","855d5e69":"code","0315996c":"code","6d20b759":"markdown","b68c0719":"markdown","6ef111de":"markdown","4f302e74":"markdown","e5c02951":"markdown","b8fd5523":"markdown","66916acc":"markdown","3fcf42a3":"markdown","d6a7dc3b":"markdown","4517f31e":"markdown","0ebaffa3":"markdown"},"source":{"0be12909":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a42bf131":"!pip install ..\/input\/python-datatable\/datatable-0.11.0-cp37-cp37m-manylinux2010_x86_64.whl > \/dev\/null 2>&1","98eaa74e":"!pip install \/kaggle\/input\/pytorchtabnet\/pytorch_tabnet-2.0.1-py3-none-any.whl","65361d42":"from collections import defaultdict\nimport datatable as dt\n\nimport statsmodels.api as sm\nfrom sklearn.metrics import roc_auc_score\n\nfrom matplotlib import pyplot as plt\nimport riiideducation\nfrom pathlib import Path\nimport seaborn as sns\n\nfrom pytorch_tabnet.tab_model import TabNetClassifier","08f37acc":"path = Path('\/kaggle\/input')\nassert path.exists()","5ae4cc50":"%%time\n\ndata_types_dict = {\n    'user_id': 'int32', \n    'content_id': 'int16', \n    'answered_correctly': 'int8', \n    'prior_question_elapsed_time': 'float32', \n    'prior_question_had_explanation': 'bool'\n}\ntarget = 'answered_correctly'\ntrain_df = dt.fread(path\/\"riidtrainjay\/train.jay\", columns=set(data_types_dict.keys())).to_pandas()","520c8767":"%%time\n\ntrain_df = train_df[train_df[target] != -1].reset_index(drop=True)\ntrain_df.drop(columns=['timestamp'], inplace=True)","760da48b":"train_df['prior_question_had_explanation'].fillna(False, inplace=True)\ntrain_df['prior_question_had_explanation'] = train_df['prior_question_had_explanation'].astype('uint8')","d38eb373":"train_df['lag'] = train_df.groupby('user_id')[target].shift()\ntrain_df['lag'].fillna(0, inplace=True)","0f154647":"train_df.info()","22f917f2":"%%time\n\ncum = train_df.groupby(['user_id'])['lag'].agg(['cumsum', 'cumcount'])\ntrain_df['user_correctness'] = cum['cumsum'] \/ cum['cumcount']\ntrain_df.drop(columns=['lag'], inplace=True)\ndel cum","ca523030":"user_agg = train_df.groupby('user_id')[target].agg(['sum', 'count'])\ncontent_agg = train_df.groupby('content_id')[target].agg(['sum', 'count'])","fec7582d":"for f in ['prior_question_elapsed_time']:\n    train_df[f] = pd.to_numeric(train_df[f], downcast='float')","559c212c":"%%time\n\ntrain_df['residual'] =  train_df[target] - train_df['content_id'].map(content_agg['sum'] \/ content_agg['count'])\nresidual_agg = train_df.groupby('user_id')['residual'].agg(['sum'])","b0ba420c":"prior_question_elapsed_time_agg = train_df.groupby('user_id').agg({'prior_question_elapsed_time': ['sum', lambda x: len(x)]})\nprior_question_elapsed_time_agg.columns = ['sum', 'count']\nprior_question_elapsed_time_agg['count'] = prior_question_elapsed_time_agg['count'].astype('int32')\nprior_question_elapsed_time_agg.info()","edaa969a":"# Covariance between time and user mean\n\ntrain_df['user_prior_question_elapsed_time_diff'] = (train_df[target] - train_df['user_id'].map(user_agg['sum'] \/ user_agg['count'])) * (train_df['prior_question_elapsed_time'] - train_df['user_id'].map(prior_question_elapsed_time_agg['sum'] \/ prior_question_elapsed_time_agg['count']))\nuser_prior_question_elapsed_time_diff_agg = train_df.groupby('user_id')['user_prior_question_elapsed_time_diff'].agg(['sum'])\ntrain_df['user_prior_question_elapsed_time_diff_mean'] = train_df['user_id'].map(user_prior_question_elapsed_time_diff_agg['sum'] \/ user_agg['count'])","3d984722":"USER_TRIES = 70\n\nimport math\nVALID_TRIES = math.ceil(USER_TRIES \/ 10)","69c5bad3":"train_df = train_df.groupby('user_id').tail(USER_TRIES).reset_index(drop=True)","0d356d42":"train_df.shape","f036a332":"data_types_dict = {'question_id': 'int16', 'part': 'int8', 'bundle_id': 'int16', 'tags': 'string'}\n\nquestions_df = pd.read_csv(\n    path\/'riiid-test-answer-prediction\/questions.csv', \n    usecols=data_types_dict.keys(),\n    dtype=data_types_dict\n)","7f3075c3":"unique_tags_combos_keys = {v:i for i,v in enumerate(questions_df['tags'].unique())}\nquestions_df['tags_encoded'] = questions_df['tags'].apply(lambda x : unique_tags_combos_keys[x])\nquestions_df['tags_encoded'] = pd.to_numeric(questions_df['tags_encoded'], downcast='integer')\nquestions_df.info()","d5ccf630":"def extract_tag_factory(tag_pos):\n    def extract_tag(x):\n        if isinstance(x, str) and tag_pos < len(x.split()):\n            splits = x.split()\n            splits.sort()\n            return int(splits[tag_pos])\n        else:\n            return 255\n    return extract_tag\n        \nfor i in range(0, 3):\n    questions_df[f'tag_{i + 1}'] = questions_df['tags'].apply(extract_tag_factory(i))\n    questions_df[f'tag_{i + 1}'] = questions_df[f'tag_{i + 1}'].astype('uint8')","cdd421d9":"train_df = pd.merge(train_df, questions_df, left_on='content_id', right_on='question_id', how='left')\ntrain_df.drop(columns=['question_id'], inplace=True)","99f49a79":"train_df['content_count'] = train_df['content_id'].map(content_agg['count']).astype('int32')\ntrain_df['content_id'] = train_df['content_id'].map(content_agg['sum'] \/ content_agg['count'])","785a820d":"train_df['prior_question_elapsed_time_mean'] = train_df['user_id'].map(prior_question_elapsed_time_agg['sum'] \/ prior_question_elapsed_time_agg['count'])","87f77159":"train_df['residual_user_mean'] = train_df['user_id'].map(residual_agg['sum'] \/ user_agg['count'])","c27cd6ea":"train_df['prior_question_elapsed_time'].fillna(train_df['prior_question_elapsed_time'].mean(), inplace=True)\ntrain_df['user_correctness'].fillna(train_df['user_correctness'].mean(), inplace=True)","1d1bb418":"for f in ['user_correctness', 'content_id']:\n    train_df[f] = pd.to_numeric(train_df[f], downcast='float')","c26a538d":"valid_df = train_df.groupby('user_id').tail(VALID_TRIES)\n# train_df.drop(valid_df.index, inplace=True)","8df9ffa0":"train_df['user_correctness'] = train_df['user_correctness'].replace(train_df['user_correctness'].mean(), 0.0)\nvalid_df['user_correctness'] = valid_df['user_correctness'].replace(valid_df['user_correctness'].mean(), 0.0)","61323765":"train_df","e4e64195":"features = [\n    'prior_question_elapsed_time',\n    'prior_question_had_explanation',\n    'user_correctness',\n    'part',\n    'content_id',\n    'content_count',\n    'tags_encoded',\n    'tag_1',\n    'tag_2',\n    'prior_question_elapsed_time_mean',\n    'residual_user_mean'\n]","40e5af3c":"%%time\n\nmodel = sm.OLS(train_df[target], train_df[features])\nlin_model = model.fit()\nroc_auc_score(valid_df[target], lin_model.predict(valid_df[features]))","a979d60a":"lin_model.predict(valid_df[features].values[:10])","c0c581f6":"EPOCHS=5\nBATCH_SIZE=4096","9a34b399":"cat_features = ['tags_encoded', 'tag_1', 'tag_2']","a7fb0f2a":"cont_features = [x for x in features if x not in cat_features]","39bceeb8":"cont_features","0a7b501f":"# Check fast ai version\nimport fastai\nfrom fastai.tabular.all import *\n\nfastai.__version__","37ac6d6d":"train_df.info()","7d6598ff":"train_df[target] = train_df[target].astype('float32')","e118ebcf":"%%time\n\ndls = TabularDataLoaders.from_df(train_df, \n    procs=[Categorify, FillMissing, Normalize],\n    cat_names=cat_features, \n    cont_names=cont_features,\n    y_names=target, valid_idx=valid_df.index, bs=BATCH_SIZE)","18a6a9c0":"def my_auc(inp, targ):\n    \"Simple wrapper around scikit's roc_auc_score function for regression problems\"\n    inp,targ = flatten_check(inp,targ)\n    return roc_auc_score(targ.cpu().numpy(), inp.cpu().numpy())","2cbcbe91":"def bce(inp,targ):\n    \"Binary cross entropy\"\n    inp,targ = flatten_check(inp,targ)\n    loss = F.binary_cross_entropy(inp, targ)\n    return loss","6a09daf7":"learn = tabular_learner(dls, layers=[200,100], metrics=my_auc)","32d3b04f":"learn.model","34d12566":"learn.model.layers.add_module('sigmoid', nn.Sigmoid())\nlearn.loss_func = bce","3d46cc21":"lr_find_res = learn.lr_find()","bd05fe36":"%%time\n\nlearn.fit_one_cycle(3, lr=lr_find_res.lr_min)","cb0666c4":"def predict_batch(self, df):\n    dl = self.dls.test_dl(df)\n    dl.dataset.conts = dl.dataset.conts.astype(np.float32)\n    inp,preds,_,dec_preds = self.get_preds(dl=dl, with_input=True, with_decoded=True)\n    return preds.numpy()\n\nsetattr(learn, 'predict_batch', predict_batch)","5dda6a1b":"import lightgbm as lgb","3ff00758":"cat_features = ['tags_encoded', 'tag_1', 'tag_2']\n\nlgb_train = lgb.Dataset(train_df[features], train_df[target], categorical_feature = cat_features, free_raw_data=False)\nlgb_eval = lgb.Dataset(valid_df[features], valid_df[target], categorical_feature = cat_features, reference=lgb_train, free_raw_data=False)","230324a7":"METRICS = ['auc']\n\nparams = {\n    'objective': 'binary',\n    'seed': 42,\n    'metric': METRICS,\n    'learning_rate': 0.05,\n    'max_bin': 800,\n    'num_leaves': 80\n}","3239410e":"%%time\n\nNUM_BOOST_ROUNDS = 300\n\nevals_result = {}\n\nlgb_model = lgb.train (\n    params, \n    lgb_train, \n    valid_sets=[lgb_train, lgb_eval], \n    verbose_eval=20, \n    num_boost_round=NUM_BOOST_ROUNDS, \n    early_stopping_rounds=20,\n    evals_result=evals_result\n)","9eb437ce":"lgb.plot_importance(lgb_model)","72f43947":"import torch\n\n# Tabnet object\nclf_tabnet = TabNetClassifier(cat_idxs=[list(train_df[features].columns).index(x) for x in cat_features], \n                              scheduler_params={\"step_size\": 50, \"gamma\": 0.9},\n                                scheduler_fn=torch.optim.lr_scheduler.StepLR)\nclf_tabnet","8d8c5461":"%%time\n\n# Fit TabNet model\nclf_tabnet.fit(\n    X_train=train_df[features].values, y_train=train_df[target].values,\n    eval_set=[(valid_df[features].values, valid_df[target].values)],\n    max_epochs=2,\n    batch_size=8192 * 4\n)","750b2ddf":"preds = clf_tabnet.predict_proba(valid_df[features].values[:1000])\npreds[:,1].shape","1d468856":"user_sum_dict = user_agg['sum'].astype('int16').to_dict(defaultdict(int))\nuser_count_dict = user_agg['count'].astype('int16').to_dict(defaultdict(int))\ncontent_sum_dict = content_agg['sum'].astype('int32').to_dict(defaultdict(int))\ncontent_count_dict = content_agg['count'].astype('int32').to_dict(defaultdict(int))\nresidual_sum_dict = residual_agg['sum'].astype('float32').to_dict(defaultdict(int))","08b4577a":"prior_question_elapsed_time_sum_dict = prior_question_elapsed_time_agg['sum'].astype('int64').to_dict(defaultdict(int))\nprior_question_elapsed_time_count_dict = prior_question_elapsed_time_agg['count'].astype('int32').to_dict(defaultdict(int))","8ce388e9":"env = riiideducation.make_env()\niter_test = env.iter_test()\nprior_test_df = None","d16a8fd4":"def clip(count): return np.clip(count, 1e-8, np.inf)","7479d1f0":"for (test_df, sample_prediction_df) in iter_test:\n    if prior_test_df is not None:\n        prior_test_df[target] = eval(test_df['prior_group_answers_correct'].iloc[0])\n        prior_test_df = prior_test_df[prior_test_df[target] != -1].reset_index(drop=True)\n        \n        user_ids = prior_test_df['user_id'].values\n        content_ids = prior_test_df['content_id'].values\n        prior_question_elapsed_times = prior_test_df['prior_question_elapsed_time'].values\n        targets = prior_test_df[target].values\n        \n        for user_id, content_id, prior_question_elapsed_time, answered_correctly in zip(user_ids, content_ids, prior_question_elapsed_times, targets):\n            user_sum_dict[user_id] += answered_correctly\n            user_count_dict[user_id] += 1\n            content_sum_dict[content_id] += answered_correctly\n            content_count_dict[content_id] += 1\n            mean_accuracy = content_sum_dict[content_id] \/ clip(content_count_dict[content_id])\n            residual_sum_dict[user_id] += answered_correctly - mean_accuracy\n            \n            prior_question_elapsed_time_sum_dict[user_id] += 0 if np.isnan(prior_question_elapsed_time) else prior_question_elapsed_time\n            prior_question_elapsed_time_count_dict[user_id] += 0 if np.isnan(prior_question_elapsed_time) else 1\n    \n    prior_test_df = test_df.copy()\n    \n    test_df = pd.merge(test_df, questions_df, left_on='content_id', right_on='question_id', how='left')\n    test_df = test_df[test_df['content_type_id'] == 0].reset_index(drop=True)\n    \n    test_df['prior_question_had_explanation'] = test_df['prior_question_had_explanation'].fillna(False).astype('uint8')\n    \n    user_sum = np.zeros(len(test_df), dtype=np.int16)\n    user_count = np.zeros(len(test_df), dtype=np.int16)\n    res_sum = np.zeros(len(test_df), dtype=np.float32)\n    content_sum = np.zeros(len(test_df), dtype=np.int32)\n    content_count = np.zeros(len(test_df), dtype=np.int32)\n    prior_question_elapsed_time_sum = np.zeros(len(test_df), dtype=np.int32)\n    prior_question_elapsed_time_count = np.zeros(len(test_df), dtype=np.int32)\n    \n    for i, (user_id, content_id) in enumerate(zip(test_df['user_id'].values, test_df['content_id'].values)):\n        user_sum[i] = user_sum_dict[user_id]\n        user_count[i] = user_count_dict[user_id]\n        res_sum[i] = residual_sum_dict[user_id]\n        content_sum[i] = content_sum_dict[content_id]\n        content_count[i] = content_count_dict[content_id]\n        prior_question_elapsed_time_sum[i] = prior_question_elapsed_time_sum_dict[user_id]\n        prior_question_elapsed_time_count[i] = prior_question_elapsed_time_count_dict[user_id]\n\n    content_count = clip(content_count)\n    user_count = clip(user_count)\n    prior_question_elapsed_time_count = clip(prior_question_elapsed_time_count)\n    test_df['user_correctness'] = user_sum \/ user_count\n    test_df['residual_user_mean'] = res_sum \/ user_count\n    test_df['content_count'] = content_count\n    test_df['content_id'] = content_sum \/ content_count\n    test_df['prior_question_elapsed_time_mean'] = prior_question_elapsed_time_sum \/ prior_question_elapsed_time_count\n    \n    test_df['prior_question_elapsed_time'].fillna(train_df['prior_question_elapsed_time'].mean(), inplace=True)\n    \n    test_df[cat_features] = test_df[cat_features].apply(pd.to_numeric, downcast='integer')\n    test_df.fillna(0, inplace=True)\n       \n    test_df[target] = np.average([\n        clf_tabnet.predict_proba(test_df[features].values)[:,1],\n        lin_model.predict(test_df[features]),\n        learn.predict_batch(learn, test_df[features])[:,0],\n        lgb_model.predict(test_df[features])\n    ], weights=[0.25, 0.2, 0.25, 0.3], axis=0)\n    \n    env.predict(test_df[['row_id', target]])","e3afc6e2":"test_df[target] = np.average([\n    clf_tabnet.predict_proba(test_df[features].values)[:,1],\n    lin_model.predict(test_df[features]),\n    learn.predict_batch(learn, test_df[features])[:,0],\n    lgb_model.predict(test_df[features])\n], weights=[0.25, 0.2, 0.25, 0.3], axis=0)","855d5e69":"lgb_model.predict(test_df[features])","0315996c":"test_df","6d20b759":"### LightGBM","b68c0719":"##### Question related","6ef111de":"##### Fast AI","4f302e74":"##### Linear model","e5c02951":"### Predict","b8fd5523":"### Tabnet Starter\nSimple starter notebook, which uses for prediction a simple ensemble with tabnet and a linear model using the statsmodel library and another fast ai neural network.","66916acc":"### Create dataset","3fcf42a3":"### Feature generation","d6a7dc3b":"### Training","4517f31e":"#### Tabnet","0ebaffa3":"### Load data"}}