{"cell_type":{"59f7c5ef":"code","2f4548ac":"code","ad5b1dd0":"code","456b2b97":"code","78a6e7bc":"code","09132aba":"code","e6bd43e8":"code","b699cd4b":"code","f081b621":"code","5be7ea26":"code","8a105056":"code","ce2a884f":"code","185bd612":"code","7ac6ccbd":"code","a5d2fcd3":"code","afc32607":"code","b056fe20":"code","660d666c":"code","485835b2":"code","fe027ed7":"markdown","69e3fc81":"markdown","948295a8":"markdown","b87554cb":"markdown","cf7cb443":"markdown","a7920fe3":"markdown"},"source":{"59f7c5ef":"# Let's import all the necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import PolynomialFeatures, MinMaxScaler\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","2f4548ac":"# Model to predict the sales given the spend on marketing\ndata = {'Marketing Spend (Million $)' : [23, 26, 30, 34, 43, 48], 'Sales (Million $)': [651, 762, 856, 1063, 1190, 1298]}\ndata = pd.DataFrame(data)\ndata","ad5b1dd0":"# Plotting a scatter plot to visualize the data\nsns.scatterplot(data = data, x = 'Marketing Spend (Million $)', y = 'Sales (Million $)')\nplt.show()","456b2b97":"# Scaling the data between 0 and 1\nscaler = MinMaxScaler()\ndata[['Marketing Spend (Million $)', 'Sales (Million $)']] = \\\nscaler.fit_transform(data[['Marketing Spend (Million $)', 'Sales (Million $)']])\ndata","78a6e7bc":"# Plotting a scatter plot to visualize the data\nsns.scatterplot(data = data, x = 'Marketing Spend (Million $)', y = 'Sales (Million $)')\nplt.show()","09132aba":"# Read in the data\nX = data['Marketing Spend (Million $)'].values.reshape(-1,1)\ny = data['Sales (Million $)']\n\nreg = LinearRegression()\nreg.fit(X, y)\n\n# Predictions on the basis of the model\ny_pred = reg.predict(X)\ny_pred","e6bd43e8":"r2_score(y, y_pred) # Marketing spends account for 96.5% of the variation present in the Sales","b699cd4b":"# Residual Sum of Squares = Mean_Squared_error * Total number of datapoints\nrss = np.sum(np.square(y - y_pred))\nprint(rss)\nmse = mean_squared_error(y, y_pred)\nprint(mse)\n\n# Root Mean Squared Error\nrmse = mse ** 0.5\nprint(rmse)","f081b621":"# Plot for Predicted sales vs Marketing Spend\nplt.scatter(X, y, color = 'blue') # original datashown as blue points\nplt.plot(X, y_pred, color = 'red', linewidth = 3) # Fitted model in red\nplt.xlabel('Marketing Spend (Million $)')\nplt.ylabel('Sales (Million $)')\nplt.show()","5be7ea26":"# help(Polynomial Features)\nX = data['Marketing Spend (Million $)'].values.reshape(-1, 1)\nprint(X)","8a105056":"poly = PolynomialFeatures(3) # Want to generate features with degree less than or equal to the specifies degree\nY = poly.fit_transform(X) # Transform the variable X to 1, X, X^2, X^3\nprint(Y)","ce2a884f":"# Building the polynomial regression model with degree 5\ndegree = 5 # got this number through trial and this is the lowest value which fit the data perfectly\npolyreg5 = PolynomialFeatures(degree)\nX_poly5 = polyreg5.fit_transform(X) # Transform the variable X to 1, X, X^2, X^3, X^4, X^5\nlinreg5 = LinearRegression()\nlinreg5.fit(X_poly5, y)","185bd612":"# Plotting the polynomial regression(degree -5) and simple linear regression\nX_seq = np.linspace(X.min(), X.max(), 300).reshape(-1,1) # Between 0 and 1 we get 300 equally spaced values\n\n# Print(X_seq)\nplt.figure()\nplt.scatter(X, y)\nplt.plot(X_seq, linreg5.predict(polyreg5.fit_transform(X_seq)), color=\"black\") # model fit with polynomial regression\nplt.plot(X_seq, reg.predict(X_seq), color=\"red\") # model fit with linear regression\nplt.title(\"Polynomial regression with degree \" + str(degree))\nplt.xlabel(\"Marketing Spend (Million $)\")\nplt.ylabel(\"Predicted Sales (Million $)\")\nplt.show()","7ac6ccbd":"y_pred5 = linreg5.predict(polyreg5.fit_transform(X)) # store predictions from the polynomial regression in the variable y_pred5\nprint(r2_score(y, y_pred5))","a5d2fcd3":"# Metrics to asses model performance\nrss = np.sum(np.square(y - y_pred5))\nprint(rss)\nmse = mean_squared_error(y, y_pred5)\nprint(mse)\n\n# Root Mean Squared Error\nrmse = mse ** 0.5\nprint(rmse)","afc32607":"# Applying Ridge Regression with varying the hyperparameter 'lambda'\n\nX_seq = np.linspace(X.min(),X.max(),300).reshape(-1,1) # values to be considered for predictor variable\nlambdas = [0, 0.001, 0.01, 0.1, 1, 10, 100, 1000] # Higher the value of lambda, \n                                                  # more the regularization\nfor i in lambdas: # for each lambda we get different model coefficients\n    degree = 5 # Degree for polynomial regression - chose 5 since this is the lowest number that gave a perfect fit\n    # Creating degree 5 features\n    ridgecoef = PolynomialFeatures(degree)\n    # Transforming input features to polynomial features (1, x1, x2, x3, x4, x5)    \n    X_poly = ridgecoef.fit_transform(X)\n    ridgereg = Ridge(alpha = i) # Initialize the Ridge Regression model with a specific lambda\n    ridgereg.fit(X_poly, y) # fit the model on the polynomial features\n    \n    # Plotting the ridge regression model for each lambda\n    plt.figure()\n    plt.scatter(X,y)\n    plt.plot(X_seq,ridgereg.predict(ridgecoef.fit_transform(X_seq)),color=\"black\") # Polynomial Regression\n    plt.plot(X_seq,reg.predict(X_seq),color=\"red\") # Linear Regression\n    plt.title(\"Polynomial regression with degree \"+str(degree) + \" and lambda = \" + str(i))\n    plt.show()\n    \n    #Computing the r2 score\n    y_pred = ridgereg.predict(ridgecoef.fit_transform(X))\n    print(\"r2 score = \" + str(r2_score(y, y_pred))) \n    print(ridgereg.coef_) # model coefficients","b056fe20":"# Applying Lasso Regression with varying the hyperparameter 'lambda'\n\nlambdas = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\nfor i in lambdas:\n    degree = 5\n    # Creating degree 5 features\n    lassocoef = PolynomialFeatures(degree)\n    # Transforming input features to polynomial features (1, x1, x2, x3, x4, x5)\n    X_poly = lassocoef.fit_transform(X)\n    lassoreg = Lasso(alpha = i)\n    lassoreg.fit(X_poly, y)\n    # Plotting the ridge model\n    plt.figure()\n    plt.scatter(X,y)\n    plt.plot(X_seq,lassoreg.predict(lassocoef.fit_transform(X_seq)),color=\"black\")\n    plt.plot(X_seq,reg.predict(X_seq),color=\"red\")\n    plt.title(\"Polynomial regression with degree \"+str(degree) + \" and lambda = \" + str(i))\n    plt.show()\n    # Compute R^2 \n    y_pred = lassoreg.predict(lassocoef.fit_transform(X))\n    print(\"r2 score = \" + str(r2_score(y, y_pred)))\n    print(lassoreg.coef_)","660d666c":"# Get the model coefficients for specific lambda say 0.001\n\n# Ridge Regression\n\nridgecoef = PolynomialFeatures(degree, include_bias = True) # Creating degree 5 features\n# Transforming input features to polynomial features (1, x1, x2, x3, x4, x5)    \nX_poly = ridgecoef.fit_transform(X)\nridgereg = Ridge(alpha = 0.001) # Initialize the Ridge Regression model with a specific lambda\nridgereg.fit(X_poly, y) # fit the model on the polynomial features\nprint(ridgereg.coef_)    \ny_pred = ridgereg.predict(ridgecoef.fit_transform(X))\nprint(\"r2 score = \" + str(r2_score(y, y_pred)))\n\n# Lasso Regression\nlassocoef = PolynomialFeatures(degree) # Creating degree 5 features\n# Transforming input features to polynomial features (1, x1, x2, x3, x4, x5)\nX_poly = lassocoef.fit_transform(X)\nlassoreg = Lasso(alpha = 0.001)\nlassoreg.fit(X_poly, y)\nprint(lassoreg.coef_)\ny_pred = lassoreg.predict(lassocoef.fit_transform(X))\nprint(\"r2 score = \" + str(r2_score(y, y_pred)))","485835b2":"betas = pd.DataFrame(index=['1', 'x', 'x2', 'x3', 'x4', 'x5'], \n                     columns = ['Polynomial', 'Ridge', 'Lasso'])\nbetas['Polynomial'] = linreg5.coef_ # Polynomial Regression\nbetas['Ridge'] = ridgereg.coef_ # Ridge Regression\nbetas['Lasso'] = lassoreg.coef_ # Lasso Regression\nbetas","fe027ed7":"### Ridge Regression","69e3fc81":"#### Build the Linear Regression Model","948295a8":"#### Let us say that we want a better fit and hence we use higher degree ploynomials as predictors.","b87554cb":"### Ridge and Lasso Regression for specific lambda value","cf7cb443":"Ridge and Lasso regression are some of the simple techniques to reduce model complexity and prevent over-fitting which may result from simple linear regression.","a7920fe3":"### Lasso Regression"}}