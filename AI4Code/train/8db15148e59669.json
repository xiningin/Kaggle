{"cell_type":{"3ce39b8e":"code","56d2ca1f":"code","2dfc1277":"code","e48aaa5d":"code","5ac6be6c":"code","a5f271cf":"code","ae927855":"code","9e597d39":"code","525a44d4":"code","4800185e":"code","8febed8a":"code","257f41b3":"code","1fe4e0c7":"code","3a98dd04":"markdown"},"source":{"3ce39b8e":"\nimport numpy as np \nimport pandas as pd\n\n\nfrom keras_preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Flatten, Input, GlobalMaxPool2D\nfrom keras.optimizers import Adam\nimport matplotlib.pyplot as plt\nfrom keras.utils.vis_utils import plot_model\n\nimport seaborn as sn\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.utils import class_weight\n","56d2ca1f":"\nimg_path = \"..\/input\/plant-pathology-2020-fgvc7\/images\/\"\n\ntest              = pd.read_csv(\"..\/input\/plant-pathology-2020-fgvc7\/test.csv\")\ntrain             = pd.read_csv(\"..\/input\/plant-pathology-2020-fgvc7\/train.csv\")\n\ntrain[\"imaged_id_fileName\"] = train.image_id+\".jpg\"\ntest[\"imaged_id_fileName\"]  = test.image_id+\".jpg\"\n\ntrain.head()","2dfc1277":"\nimg_height = 100\nimg_width = 133\n\nbatch_size = 32\nlabels = [\"healthy\",\"multiple_diseases\",\"rust\",\"scab\"]\ntrain_dataGenerator = ImageDataGenerator(rescale=1.\/255,\n    shear_range=0,\n    zoom_range=(1, 1.3),\n    rotation_range = 360,\n    brightness_range = (0.7, 1.3),                                                   \n    horizontal_flip=True,\n    vertical_flip=True,\n    validation_split=0.2)\n\ntrain_generator = train_dataGenerator.flow_from_dataframe(\n    dataframe=train,x_col='imaged_id_fileName', y_col=labels,\n    directory=img_path, target_size=(img_height, img_width),\n    batch_size=batch_size,class_mode='raw', subset='training') \n\nvalidation_generator = train_dataGenerator.flow_from_dataframe(\n    dataframe=train,x_col='imaged_id_fileName', y_col=labels,\n    directory=img_path, target_size=(img_height, img_width),\n    batch_size=batch_size, class_mode='raw', subset='validation')\n\nvalidation_generator2 = train_dataGenerator.flow_from_dataframe(\n    dataframe=train,x_col='imaged_id_fileName', y_col=labels,\n    directory=img_path, target_size=(img_height, img_width),\n    batch_size=batch_size, class_mode='raw',shuffle=False,\n    sort = False, subset='validation') \n\ntrain_dataGenerator_full = ImageDataGenerator(rescale=1.\/255,\n    shear_range=0,\n    zoom_range=(1, 1.3),\n    rotation_range = 360,\n    brightness_range = (0.7, 1.3),                                                   \n    horizontal_flip=True,\n    vertical_flip=True,\n    validation_split=0) \n\ntrain_generator_full = train_dataGenerator_full.flow_from_dataframe(\n    dataframe=train,x_col='imaged_id_fileName', y_col=labels,\n    directory=img_path, target_size=(img_height, img_width),\n    batch_size=batch_size, class_mode='raw', subset='training') \ntest_dataGenerator = ImageDataGenerator(rescale=1.\/255)\n\ntest_generator = test_dataGenerator.flow_from_dataframe(\n    dataframe=test,x_col='imaged_id_fileName', y_col=labels,\n    directory=img_path, shuffle = False, sort = False,\n    target_size=(img_height, img_width), batch_size=1, class_mode=None)","e48aaa5d":"\nclassProb =np.zeros(len(labels))\nidx = 0\nfor k in labels:\n    print(f\"{k} contains {train[k].sum()} samples\")\n    classProb[idx] = train[k].sum()\n    idx+=1\n\nprint()\ncolor = ['#58508d','#bc5090','#ff6361', '#ffa600'] \nplt.figure(figsize=(15,7))\nplt.pie(classProb, shadow=True, explode=[0,0.5, 0, 0],labels=labels,\n        autopct='%1.2f%%', colors=color, startangle=-90,\n        textprops={'fontsize': 14})\n\nclass_weight_vect =np.square(1 \/ (classProb\/classProb.sum()) )\nclass_weight_vect=class_weight_vect\/np.min(class_weight_vect)           ","5ac6be6c":"\ndef plotImages(imgs):\n    col=5\n    row=2\n    fig, axes = plt.subplots(row, col, figsize=(25,25))  \n    axes = axes.flatten()\n    for k in range(10):\n        axes[k].imshow(imgs[k])\n    fig.subplots_adjust(hspace=-0.75, wspace=0.2) \n    plt.show()\n\n    \n#Apply augmentation to the same picture 10 times and plot the outcome:   \nplotImageAugmentation = [validation_generator2[1][0][0] for i in range(10)] #Using validation_generator2 for consitency since shuffle is turned off.\nplotImages(plotImageAugmentation)","a5f271cf":"\nmodel = Sequential()\nmodel.add(Conv2D(35, kernel_size=(3, 3), activation='relu', kernel_initializer='glorot_uniform', \n                 bias_initializer='zeros',  input_shape=(img_height, img_width, 3), padding='same'))\nmodel.add(Conv2D(35, (3, 3),kernel_initializer='glorot_uniform', bias_initializer='zeros', \n                 activation='relu', padding='same'))\nmodel.add(Dropout(0.1))\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(35, (3, 3),kernel_initializer='glorot_uniform', bias_initializer='zeros', activation='relu'))\nmodel.add(Conv2D(35, (3, 3),kernel_initializer='glorot_uniform', bias_initializer='zeros', activation='relu'))\nmodel.add(Dropout(0.1))\nmodel.add(MaxPool2D(pool_size=(5, 5)))\n\nmodel.add(Conv2D(50, (3, 3),kernel_initializer='glorot_uniform', bias_initializer='zeros', activation='relu'))\nmodel.add(Conv2D(50, (3, 3),kernel_initializer='glorot_uniform', bias_initializer='zeros', activation='relu'))\nmodel.add(Dropout(0.1))\nmodel.add(GlobalMaxPool2D())\n\nmodel.add(Dropout(0.1))\nmodel.add(Dense(4, activation='softmax'))\n\noptimizerAdam = Adam(lr=0.00125, amsgrad=True)\n\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=optimizerAdam, metrics=[\"accuracy\"])","ae927855":"\nmodel.summary()","9e597d39":"plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)","525a44d4":"\nnb_epochs = 100\nhistory = model.fit_generator(\n    train_generator,\n    steps_per_epoch = train_generator.samples \/\/ batch_size,\n    validation_data = validation_generator, \n    validation_steps = validation_generator.samples \/\/ batch_size,\n    epochs = nb_epochs,\n    class_weight=class_weight_vect)","4800185e":"\nfs = 17\nfig = plt.figure(figsize=(9,5))\nfig.patch.set_facecolor('xkcd:white')\nplt.plot(history.history['accuracy'], color=color[0])\nplt.plot(history.history['val_accuracy'], color=color[3])\nplt.ylabel('Accuracy',fontsize=fs)\nplt.xlabel('Epoch #',fontsize=fs)\nplt.legend(['training', 'validation'],fontsize=fs)\nplt.grid('both', linestyle='--')\nplt.xticks(fontsize = fs)\nplt.yticks(fontsize = fs)\nplt.show()\n\n\nfig = plt.figure(figsize=(9,5))\nfig.patch.set_facecolor('xkcd:white')\nplt.plot(history.history['loss'], color=color[0])\nplt.plot(history.history['val_loss'], color=color[3])\nplt.ylabel('Loss',fontsize=fs)\nplt.xlabel('Epoch #',fontsize=fs)\nplt.legend(['training', 'validation'],fontsize=fs)\nplt.grid('both', linestyle='--')\nplt.xticks(fontsize = fs)\nplt.yticks(fontsize = fs)\nplt.show()","8febed8a":"\nY_pred = model.predict(validation_generator2)\nY_pred_labels = np.argmax(Y_pred, axis=1)\ny_true = np.argmax(validation_generator.labels, axis=1 )\n\n\nlabels_num = [0,1,2,3]\ncm = confusion_matrix( y_true, Y_pred_labels, normalize='true')\nsn.set(font_scale=1.4) \nsn.heatmap(cm, annot=True, annot_kws={\"size\": 14}, cmap=\"YlGnBu\", xticklabels = labels, yticklabels = labels)\nplt.show()\n\nprint(classification_report(y_true, Y_pred_labels))","257f41b3":"nb_epochs = 50\nhistory = model.fit_generator(\n    train_generator_full,\n    steps_per_epoch = train_generator_full.samples \/\/ batch_size,\n    epochs = nb_epochs,\n    class_weight=class_weight_vect)","1fe4e0c7":"test_predictions = model.predict_generator(test_generator)","3a98dd04":"# This notebook implements classification using CNNs in Keras for the Plant Pathology 2020 Challenge \n\nThe main purpouos of this notebook is to strengthen my pratical knowledge of implementing different methods of deep learning. However, I decided share this notebook in hope that others can benefit from it. \n\n**Purpouse of this notebook:**\n* Learn how to implelemt data augmentation in Keras\n* Practice implementing CNNs in Keras\n* Learn how to use training with class weights for an imbalanced data set\n\n** This notebook does NOT attempt:**\n* Achieving top score on the dataset \n* Optimize learning speed\n\n**Comments:**\n* Achives approxmiately a score of 0.94 on the public test data.\n* Design to run with GPU accelerator on Kaggle. However, at the time of commit the GPU accleration is not working properly on Kaggle ( https:\/\/www.kaggle.com\/c\/liverpool-ion-switching\/discussion\/136058#833053 )\n    * Some failed commits due to running out of memory on CPU \n* Even with heavy class weighting, the CNN does not perform well for the class with low number of observations (5%), i.e. \"multiple_diseases\".\n \n "}}