{"cell_type":{"d6b98d7f":"code","95e4df52":"code","3e98fa2e":"code","18109c28":"code","3db88525":"code","9755c157":"code","35dc0a6a":"code","7abd073b":"code","79685a2f":"code","bd3a2419":"code","c73190f2":"code","56448527":"code","f2b6b472":"code","38a04ff2":"code","140e2c78":"code","05f6a2e6":"code","bf7108b2":"code","18f7db46":"code","a37edd14":"code","e6fddf91":"code","0769efff":"code","81459444":"code","b7d3c593":"code","a9b9386d":"code","fd7a15aa":"code","6181f6cd":"code","1d2e0842":"code","82e52267":"code","2a6ab5cc":"code","70fda054":"code","6a857924":"code","582853e1":"code","f765ed8d":"code","a9bac40c":"code","5314884f":"code","351e72b8":"code","3dcb5510":"code","8e9b2e85":"code","a6f30b0c":"code","48ad02a6":"code","954dc66b":"code","ef293690":"code","f100834b":"code","9eb2123b":"markdown","7ebb1f93":"markdown","986faffb":"markdown","6ed4a27e":"markdown","3d7bef7e":"markdown","3c66c4ec":"markdown","4afe82b6":"markdown","7b8a7228":"markdown","880bfcdc":"markdown","8f5e73c2":"markdown","569a415d":"markdown","92fefe6e":"markdown","a4c5de68":"markdown","24325b47":"markdown","c2b08cea":"markdown","a31184c3":"markdown","3dbd1b4c":"markdown","327c46a0":"markdown","9091b519":"markdown","0da3c1c2":"markdown","9815b9d4":"markdown","e0aeb598":"markdown"},"source":{"d6b98d7f":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom scipy.special import softmax\nfrom sklearn.model_selection import KFold\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\n\nfrom lightgbm import LGBMClassifier\n\nfrom joblib import dump, load\n\nimport tensorflow as tf","95e4df52":"# data path\ndata_path = '..\/input\/lish-moa\/'\nfeatures_file = 'train_features.csv'\ntargets_file = 'train_targets_scored.csv'\nno_targets_file = 'train_targets_nonscored.csv'\ntest_file = 'test_features.csv'","3e98fa2e":"# get train data\ndf_features = pd.read_csv(os.path.join(data_path, features_file))\ndf_targets = pd.read_csv(os.path.join(data_path, targets_file))\ndf_no_targets = pd.read_csv(os.path.join(data_path, no_targets_file))\n\n# get test data\ndf_test = pd.read_csv(os.path.join(data_path, test_file))","18109c28":"# keep columns names lists\n# columns names = 'sig_id' + 'cp_type' + features_quali + features_quanti + scored_targets + no_scored_targets\nscored_targets = list(set(df_targets.columns) - set(['sig_id']))\nno_scored_targets = list(set(df_no_targets.columns) - set(['sig_id']))\nfeatures_quali = ['cp_time', 'cp_dose']\nfeatures_quanti = list(set(df_features.columns)\n                       - set(scored_targets)\n                       - set(no_scored_targets)\n                       - set(features_quali)\n                       - set(['sig_id', 'cp_type']))\nprint('Scored targets count : {}'.format(len(scored_targets)))\nprint('No scored targets count : {}'.format(len(no_scored_targets)))\nprint('Features quali count : {}'.format(len(features_quali)))\nprint('Features quanti count : {}'.format(len(features_quanti)))","3db88525":"# separate features_quanti : gene expression and cell viability features\ncells = [feature_name for feature_name in features_quanti if feature_name.find(\n    'c-') != -1]\ngenes = [feature_name for feature_name in features_quanti if feature_name.find(\n    'g-') != -1]\nprint('Features genes count : {}'.format(len(genes)))\nprint('Features cells count : {}'.format(len(cells)))","9755c157":"# check sig_id is unique\ntest = df_features['sig_id'].is_unique\nprint('sig_id unique : {}'.format(test))","35dc0a6a":"# check nan\ntest = df_features.isnull().values.any()\nprint('Missing data : {}'.format(test))","7abd073b":"# merge features and targets\ndef merge_features_targets(df_features, df_targets, df_no_targets):\n    df_data = df_features.merge(\n        df_targets, how='left', on='sig_id', validate='one_to_one')\n    df_data = df_data.merge(df_no_targets, how='left',\n                            on='sig_id', validate='one_to_one')\n    print('- Merge features and targets')\n    print('   Data shape : {}'.format(df_data.shape))\n    return df_data","79685a2f":"# separate compound and control\ndef separate_compound_control(df_data):\n    '''\n    input : dataframe features and targets\n    '''\n    df_compound = df_data[df_data['cp_type'] == 'trt_cp']\n    df_control = df_data[df_data['cp_type'] == 'ctl_vehicle']\n    print('- Separate compound and control')\n    print('   Compound shape : {}'.format(df_compound.shape))\n    print('   Control shape : {}'.format(df_control.shape))\n    return df_compound, df_control","bd3a2419":"# onehot encoding qualitatives variables\ndef onehot(df_compound, features_quali, mean=None, std=None, train=True):\n    onehot_data = pd.get_dummies(\n        df_compound[features_quali], columns=['cp_dose'])\n    features_onehot = list(onehot_data.columns)\n    # standardisation 'cp_time'\n    if mean == None:\n        mean = onehot_data['cp_time'].mean()\n    if std == None:\n        std = onehot_data['cp_time'].std()\n    onehot_data['cp_time'] = (onehot_data['cp_time'] - mean) \/ std\n    # add onehot\n    if train:\n        df_compound = pd.concat([onehot_data, df_compound[[\n                                'sig_id'] + features_quanti + scored_targets + no_scored_targets]], axis=1)\n    else:\n        df_compound = pd.concat(\n            [onehot_data, df_compound[['sig_id'] + features_quanti]], axis=1)\n    print('- Onehot encoding qualitatives variables')\n    return df_compound, mean, std, features_onehot","c73190f2":"# train set : prepare data pipeline\ndf_data = merge_features_targets(df_features, df_targets, df_no_targets)\ndf_compound_train, df_control_train = separate_compound_control(df_data)\nfeatures = genes + cells\ndf_compound_train, mean, std, features_onehot = onehot(\n    df_compound_train, features_quali)\nprint('Train set : compound shape : {}'.format(df_compound_train.shape))","56448527":"# test set : prepare data pipeline\ndf_compound_test, df_control_test = separate_compound_control(df_test)\ndf_compound_test, mean, std, features_onehot = onehot(\n    df_compound_test, features_quali, mean, std, train=False)\nprint('Compound shape : {}'.format(df_compound_test.shape))","f2b6b472":"# keep features list\n#features = features_onehot + genes + cells + ['pca_cells']\nfeatures = features_onehot + genes + cells","38a04ff2":"# train set\nX_train = df_compound_train[['sig_id'] + features]\nprint(X_train.shape)\nY_train = df_compound_train[scored_targets]\nprint(Y_train.shape)","140e2c78":"# test set\nX_test = df_compound_test[['sig_id'] + features]\nprint(X_test.shape)","05f6a2e6":"# X_train = X_train.head(100)\n# Y_train = Y_train.head(100)","bf7108b2":"def moa_metric(y_true, y_pred):\n    y_true = y_true.astype('float64')\n    y_pred = y_pred.astype('float64')\n    y_pred = np.maximum(np.minimum(y_pred, 1. - 1e-15), 1e-15)\n    return - np.mean((y_true * np.log(y_pred)) + ((1. - y_true) * np.log(1. - y_pred)))\n\n\n# scikit scorer\nmetric = make_scorer(moa_metric, greater_is_better=False, needs_proba=True)","18f7db46":"# Best hyper-parameters\nparameters = {\n    'estimator__n_estimators': [100, 200, 500], \n    'estimator__learning_rate': [0.01, 0.001, 0.0001], \n    'estimator__max_depth': [4, 6, 8],\n    'estimator__subsample': [0.5, 0.75, 1.],  \n    'estimator__colsample_bytree': [0.6, 0.8, 1.]}  \n\n# LigtGBM classifier\nclf_gb = OneVsRestClassifier(LGBMClassifier(), n_jobs=-1)\n\nclf = GridSearchCV(clf_gb,\n                   cv=5,\n                   scoring=metric,\n                   verbose=1,\n                   n_jobs=-1,\n                   return_train_score=True,\n                   param_grid=parameters)\n\n# %time clf.fit(X_train[features].values[:200,...], Y_train[scored_targets].values[:200,...])\n# pd.DataFrame(clf.cv_results_)","a37edd14":"# print('Best hyper-parameters : {}'.format(clf.best_params_))\n# print('Metric on val set : {}'.format(clf.best_score_))\n\n# Best hyper-parameters : {'estimator__colsample_bytree': 0.6, 'estimator__learning_rate': 0.01, 'estimator__max_depth': 4, 'estimator__n_estimators': 100, 'estimator__subsample': 0.5}\n# Metric on val set : -0.05989294552932979","e6fddf91":"# folds\nn_splits = 5\nskf = KFold(n_splits=n_splits, random_state=1, shuffle=True)\n\nlgbm_models = {}\nfeatures_importances = {}\n\n# train each folds\nfor n_fold, (train_index, test_index) in enumerate(skf.split(X_train.values, Y_train.values)):\n    X_train_fold = X_train[features].values[train_index]\n    Y_train_fold = Y_train[scored_targets].values[train_index]\n    X_val_fold = X_train[features].values[test_index]\n    Y_val_fold = Y_train[scored_targets].values[test_index]\n    # define model\n    clf_lgbm = OneVsRestClassifier(LGBMClassifier(colsample_bytree=0.6, # best hyper-parameters\n                                                  learning_rate=0.01,\n                                                  max_depth=4,\n                                                  n_estimators=100,\n                                                  subsample=0.5), n_jobs=-1)\n    # train on train set fold\n    print('Train fold : {}'.format(n_fold + 1))\n    clf_lgbm.fit(X=X_train_fold,\n                 y=Y_train_fold)\n\n    # get features importance for each estimators\n    importances = np.zeros((X_train_fold.shape[1],))\n    for i in range(len(scored_targets)):\n        count_estimators = 0\n        try:  # sometimes no label in dataset and no estimator...\n            importances = importances + \\\n                clf_lgbm.estimators_[i].feature_importances_\n            count_estimators = count_estimators + 1\n        except:\n            pass\n    if count_estimators > 0:\n        importances = importances \/ count_estimators\n    else:\n        importances = None\n    # save features importance for each fold\n    features_importances['fold_{}'.format(n_fold + 1)] = importances\n\n    # evaluation on train fold\n    Y_pred_train_fold = clf_lgbm.predict_proba(X_train_fold)\n    metric_train_fold = moa_metric(Y_train_fold, Y_pred_train_fold)\n    print('Metric on train fold : {}'.format(metric_train_fold))\n\n    # prediction and evaluation on val fold\n    Y_pred_val_fold = clf_lgbm.predict_proba(X_val_fold)\n    metric_val_fold = moa_metric(Y_val_fold, Y_pred_val_fold)\n    Y_pred_val_fold = pd.DataFrame(Y_pred_val_fold, columns=scored_targets)\n    Y_pred_val_fold['sig_id'] = X_train['sig_id'].values[test_index]\n    print('Metric on validation fold : {}'.format(metric_val_fold))\n\n    # prediction on test set\n    Y_pred_test_fold = clf_lgbm.predict_proba(X_test[features])\n    Y_pred_test_fold = pd.DataFrame(Y_pred_test_fold, columns=scored_targets)\n    Y_pred_test_fold['sig_id'] = X_test['sig_id'].values\n\n    # keep predictions and metric\n    lgbm_models['fold_{}'.format(n_fold + 1)] = [metric_train_fold,\n                                                 metric_val_fold,\n                                                 Y_pred_val_fold[[\n                                                     'sig_id'] + scored_targets],\n                                                 Y_pred_test_fold[['sig_id'] + scored_targets]]","0769efff":"# get features importances (used by neural network model)\nimportances = np.array(\n    [importance_fold for importance_fold in features_importances.values()])\nimportances = np.sum(importances, axis=0) \/ importances.shape[0]\nimportances = importances \/ np.max(importances)\nimportances.shape","81459444":"plt.hist(importances)","b7d3c593":"features = features_onehot + genes + cells\ndf_importances = pd.DataFrame(\n    importances, index=features, columns=['importance'])\ndf_importances = df_importances.sort_values(by='importance', ascending=False)\ndf_plot_imp_max = df_importances.head(20)\ndf_plot_imp_min = df_importances.tail(20)","a9b9386d":"ax = df_plot_imp_max.plot.bar()\nax.set_title('Features importance from LGBM\\n(10 most important features)')\nax.set_ylabel('Importance')\nax.set_xlabel('Features')","fd7a15aa":"ax = df_plot_imp_min.plot.bar()\nax.set_title('Features importance from LGBM\\n(10 least important features)')\nax.set_ylabel('Importance')\nax.set_xlabel('Features')","6181f6cd":"# concatenate prediction on train set folds (used as features during stacking)\ndf_lgbm_pred = pd.concat([model[2] for model in lgbm_models.values()])\ndf_lgbm_pred.head(5)","1d2e0842":"# get moa weights from train set\n# weight : ]0,1] 1 is for less present class\noccurence = np.sum(Y_train[scored_targets].values, axis=0)\nmax_occurence = np.max(occurence)\nweights = 1 + (max_occurence - occurence) \/ max_occurence\nweights","82e52267":"# weight features\nX_train[features] = X_train[features].copy() * importances\nX_test[features] = X_test[features].copy() * importances","2a6ab5cc":"# nn architecture\n\n\ndef get_model(input_shape):\n\n    inputs = tf.keras.Input(input_shape)\n    x = tf.keras.layers.BatchNormalization()(inputs)\n    x = tf.keras.layers.Dropout(0.6)(x)\n    x = tf.keras.layers.Dense(2048, activation=\"relu\")(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(0.6)(x)\n    x = tf.keras.layers.Dense(1024, activation=\"relu\")(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(0.6)(x)\n    outputs = tf.keras.layers.Dense(\n        len(scored_targets), activation=\"sigmoid\")(x)\n\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n\n    return model","70fda054":"def tf_moa_metric(y_true, y_pred):\n    y_true = tf.cast(y_true, dtype='float64')\n    y_pred = tf.cast(y_pred, dtype='float64')\n    y_pred = tf.maximum(tf.minimum(y_pred, 1. - 1e-15), 1e-15)\n    return - tf.math.reduce_mean((y_true * tf.math.log(y_pred)) + ((1. - y_true) * tf.math.log(1. - y_pred)))","6a857924":"# custom loss = weighted loss\ndef tf_moa_loss(y_true, y_pred):\n    y_true = tf.cast(y_true, dtype='float64')\n    y_pred = tf.cast(y_pred, dtype='float64')\n    y_pred = tf.maximum(tf.minimum(y_pred, 1. - 1e-15), 1e-15)\n    log_loss = (y_true * tf.math.log(y_pred)) + \\\n        ((1. - y_true) * tf.math.log(1. - y_pred))\n    log_loss_weighted = log_loss * weights\n    return - tf.math.reduce_mean(log_loss_weighted)","582853e1":"def get_dataset(X_train, Y_train, X_test, Y_test, batch_size):\n    #  train dataset\n    ds_train = tf.data.Dataset.from_tensor_slices(\n        (X_train.astype(float), Y_train.astype(float)))\n    ds_train = ds_train.shuffle(X_train.shape[0])\n    ds_train = ds_train.batch(batch_size)\n    ds_train = ds_train.prefetch(batch_size * 2)\n    # test dataset\n    ds_test = tf.data.Dataset.from_tensor_slices(\n        (X_test.astype(float), Y_test.astype(float)))\n    ds_test = ds_test.batch(X_test.shape[0])\n\n    return ds_train, ds_test","f765ed8d":"batch_size = 32\nepochs = 75\n\nskf = KFold(n_splits=n_splits, random_state=1, shuffle=True)\nnn_models = {}\n\n# train each folds\nfor n_fold, (train_index, test_index) in enumerate(skf.split(X_train.values, Y_train.values)):\n    X_train_fold = X_train[features].values[train_index]\n    Y_train_fold = Y_train[scored_targets].values[train_index]\n    X_val_fold = X_train[features].values[test_index]\n    Y_val_fold = Y_train[scored_targets].values[test_index]\n\n    # get dataset\n    ds_train, ds_val = get_dataset(\n        X_train_fold, Y_train_fold, X_val_fold, Y_val_fold, batch_size)\n\n    # get model\n    model = get_model(X_train_fold.shape[1])\n\n    # optimizer\n    model.compile(optimizer=tf.keras.optimizers.Adam(0.01),\n        loss=tf_moa_loss,\n        metrics=[tf_moa_metric])\n\n    # callback\n    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, mode='min',\n                                                     patience=5, min_lr=0.00001, verbose=1)\n    checkpoint_path = 'weights_fold_{}.hdf5'.format(n_fold)\n    cb_checkpt = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, monitor='val_loss', verbose=0, save_best_only=True,\n                                                    save_weights_only=True, mode='min')\n\n    # train\n    print('Train fold : {}'.format(n_fold + 1))\n    history = model.fit(x=ds_train, epochs=epochs,\n                        validation_data=ds_val, callbacks=[reduce_lr, cb_checkpt])\n\n    # load best weights\n    model.load_weights(checkpoint_path)\n\n    # evaluate on train fold\n    Y_pred_train_fold = model.predict(X_train_fold)\n    metric_train_fold = tf_moa_metric(Y_train_fold, Y_pred_train_fold)\n    print('Metric on train fold : {}'.format(metric_train_fold))\n\n    # predict and evaluate on val fold\n    Y_pred_val_fold = model.predict(X_val_fold)\n    metric_val_fold = tf_moa_metric(Y_val_fold, Y_pred_val_fold)\n    Y_pred_val_fold = pd.DataFrame(Y_pred_val_fold, columns=scored_targets)\n    Y_pred_val_fold['sig_id'] = X_train['sig_id'].values[test_index]\n    print('Metric on validation fold : {}'.format(metric_val_fold))\n\n    # predict on test set\n    Y_pred_test_fold = model.predict(X_test[features])\n    Y_pred_test_fold = pd.DataFrame(Y_pred_test_fold, columns=scored_targets)\n    Y_pred_test_fold['sig_id'] = X_test['sig_id'].values\n\n    # keep predictions and metric\n    nn_models['fold_{}'.format(n_fold + 1)] = [metric_train_fold,\n                                               metric_val_fold,\n                                               Y_pred_val_fold[[\n                                                   'sig_id'] + scored_targets],\n                                               Y_pred_test_fold[['sig_id'] + scored_targets]]","a9bac40c":"# concatenate prediction on train set folds (used as features during stacking)\ndf_nn_pred = pd.concat([nn_model[2] for nn_model in nn_models.values()])\ndf_nn_pred.head(5)","5314884f":"# sort datasets before merge\ndf_nn_pred = df_nn_pred.sort_values(by='sig_id')\ndf_lgbm_pred = df_lgbm_pred.sort_values(by='sig_id')\n\n# add sig_id features to Y_train\nY_train['sig_id'] = df_compound_train['sig_id'].copy()\nY_train_stack = Y_train.sort_values(by='sig_id')\n\n# merge lgbm and nn\nX_train_stack = df_nn_pred.merge(\n    df_lgbm_pred, how='left', on='sig_id', validate='one_to_one')\nX_train_stack = X_train_stack.sort_values(by='sig_id')","351e72b8":"# define features names\n\nfeatures_stack = list(set(X_train_stack.columns) - set(['sig_id']))\nprint(len(features_stack))\n\nfeatures = list(set(df_lgbm_pred.columns) - set(['sig_id']))\nprint(len(features))","3dcb5510":"# concatenate and mean LGBM and NN predictions on test set\n\n# nn model\nX_test_nn = pd.concat([nn_model[3] for nn_model in nn_models.values()])\nX_test_nn = X_test_nn.groupby('sig_id').mean()\nX_test_nn = X_test_nn.reset_index(col_fill='sig_id')\n# lgbm model\nX_test_lgbm = pd.concat([lgbm_model[3] for lgbm_model in lgbm_models.values()])\nX_test_lgbm = X_test_lgbm.groupby('sig_id').mean()\nX_test_lgbm = X_test_lgbm.reset_index(col_fill='sig_id')\n\n# merge LGBM and NN features\nX_test_stack = X_test_nn.merge(\n    X_test_lgbm, how='left', on='sig_id', validate='one_to_one')","8e9b2e85":"# val metric nn\nnp.mean([nn_model[1] for nn_model in nn_models.values()])","a6f30b0c":"# val metric lgbm\nnp.mean([lgbm_model[1] for lgbm_model in lgbm_models.values()])","48ad02a6":"n_splits = 5\nskf = KFold(n_splits=n_splits, random_state=1, shuffle=True)\n\nmodels_stack = {}\n\nfor n_fold, (train_index, test_index) in enumerate(skf.split(X_train_stack.values, Y_train_stack.values)):\n    X_train_fold = X_train_stack[features_stack].values[train_index]\n    Y_train_fold = Y_train[scored_targets].values[train_index]\n    X_val_fold = X_train_stack[features_stack].values[test_index]\n    Y_val_fold = Y_train[scored_targets].values[test_index]\n\n    # get model\n    clf_rl = OneVsRestClassifier(LogisticRegression(n_jobs=-1))\n\n    # train\n    print('Train fold : {}'.format(n_fold + 1))\n    clf_rl.fit(X=X_train_fold,\n               y=Y_train_fold)\n\n    # evaluate on train fold\n    Y_pred_train_fold = clf_rl.predict_proba(X_train_fold)\n    metric_train_fold = moa_metric(Y_train_fold, Y_pred_train_fold)\n    print('Metric on train fold : {}'.format(metric_train_fold))\n\n    # predict and evaluate on val fold\n    Y_pred_val_fold = clf_rl.predict_proba(X_val_fold)\n    metric_val_fold = moa_metric(Y_val_fold, Y_pred_val_fold)\n    Y_pred_val_fold = pd.DataFrame(Y_pred_val_fold, columns=scored_targets)\n    Y_pred_val_fold['sig_id'] = X_train['sig_id'].values[test_index]\n    print('Metric on validation fold : {}'.format(metric_val_fold))\n\n    # predict on test set\n    Y_pred_test_fold = clf_rl.predict_proba(X_test_stack[features_stack])\n    Y_pred_test_fold = pd.DataFrame(Y_pred_test_fold, columns=scored_targets)\n    Y_pred_test_fold['sig_id'] = X_test_stack['sig_id'].values\n\n    # keep predictions and metric\n    models_stack['fold_{}'.format(n_fold + 1)] = [metric_train_fold,\n                                                  metric_val_fold,\n                                                  Y_pred_val_fold[[\n                                                      'sig_id'] + scored_targets],\n                                                  Y_pred_test_fold[['sig_id'] + scored_targets]]","954dc66b":"np.mean([model[0] for model in models_stack.values()])","ef293690":"# concatenate and mean prediction on test dataset\npred_compound = pd.concat([stack_model[3]\n                           for stack_model in models_stack.values()])\npred_compound = pred_compound.groupby('sig_id').mean()\npred_compound = pred_compound.reset_index(col_fill='sig_id')\n\n# add control prediction (equal 0 !) to compound prediction\nY_pred_control = np.zeros((df_control_test.shape[0], len(scored_targets)))\n\n# get sig_id\n#pred_compound = np.concatenate((np.expand_dims(df_compound_test['sig_id'].values, axis=1), Y_pred_compound[:,0:len(scored_targets)]), axis=1)\npred_control = np.concatenate((np.expand_dims(\n    df_control_test['sig_id'].values, axis=1), Y_pred_control), axis=1)\npred_control = pd.DataFrame(pred_control, columns=['sig_id'] + scored_targets)\n\n# merge control pred and control pred\ndf_pred = pd.concat([pred_compound, pred_control], axis=0)\n\n# write submission file\ndf_pred.to_csv('submission.csv', index=False)","f100834b":"# plot\nmodels = ['LGBM', 'NN', 'Stacking']\ntrain = [\n    np.mean([model[0] for model in lgbm_models.values()]),\n    np.mean([model[0] for model in nn_models.values()]),\n    np.mean([model[0] for model in models_stack.values()])\n]\nval = [\n    np.mean([model[1] for model in lgbm_models.values()]),\n    np.mean([model[1] for model in nn_models.values()]),\n    np.mean([model[1] for model in models_stack.values()])\n]\ntest = [0, 0, 0]\n\nx = np.arange(len(models))  # the label locations\nwidth = 1 \/ len(train)  # the width of the bars\n\nfig, ax = plt.subplots(figsize=(8, 5))\nrects1 = ax.bar(x - width, train, width, label='train')\nrects2 = ax.bar(x, val, width, label='val')\nrects3 = ax.bar(x + width, test, width, label='test')\n\nax.set_ylabel('loss')\nax.set_title('log loss for train, val and test dataset')\nax.set_xticks(x)\nax.set_xticklabels(models)\nax.legend()","9eb2123b":"## Pipeline","7ebb1f93":"## Train Logistic Regression on LGBM+NN features","986faffb":"# LightGBM","6ed4a27e":"## Train folds","3d7bef7e":"<h1>Sommaire<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import-data\" data-toc-modified-id=\"Import-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>Import data<\/a><\/span><\/li><li><span><a href=\"#Prepare-data\" data-toc-modified-id=\"Prepare-data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>Prepare data<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Prepare-functions\" data-toc-modified-id=\"Prepare-functions-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;<\/span>Prepare functions<\/a><\/span><\/li><li><span><a href=\"#Pipeline\" data-toc-modified-id=\"Pipeline-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;<\/span>Pipeline<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#LightGBM\" data-toc-modified-id=\"LightGBM-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>LightGBM<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Metric\" data-toc-modified-id=\"Metric-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;<\/span>Metric<\/a><\/span><\/li><li><span><a href=\"#Find-best-hyper-parameters-by-cross-validation-and-sub-sampling\" data-toc-modified-id=\"Find-best-hyper-parameters-by-cross-validation-and-sub-sampling-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;<\/span>Find best hyper-parameters by cross-validation and sub-sampling<\/a><\/span><\/li><li><span><a href=\"#Train-LGBM,-cross-validation-evaluation-and-predict-on-test-set\" data-toc-modified-id=\"Train-LGBM,-cross-validation-evaluation-and-predict-on-test-set-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;<\/span>Train LGBM, cross-validation evaluation and predict on test set<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Neural-network\" data-toc-modified-id=\"Neural-network-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;<\/span>Neural network<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Compute-MoA-weights-from-train-set\" data-toc-modified-id=\"Compute-MoA-weights-from-train-set-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;<\/span>Compute MoA weights from train set<\/a><\/span><\/li><li><span><a href=\"#Weight-features-from-LGBM-features-importances\" data-toc-modified-id=\"Weight-features-from-LGBM-features-importances-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;<\/span>Weight features from LGBM features importances<\/a><\/span><\/li><li><span><a href=\"#Define-model-and-weighted-loss\" data-toc-modified-id=\"Define-model-and-weighted-loss-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;<\/span>Define model and weighted loss<\/a><\/span><\/li><li><span><a href=\"#Dataset-generator\" data-toc-modified-id=\"Dataset-generator-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;<\/span>Dataset generator<\/a><\/span><\/li><li><span><a href=\"#Train-folds\" data-toc-modified-id=\"Train-folds-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;<\/span>Train folds<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Stacking-:-LGBM-+-NN\" data-toc-modified-id=\"Stacking-:-LGBM-+-NN-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;<\/span>Stacking : LGBM + NN<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Train-dataset\" data-toc-modified-id=\"Train-dataset-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;<\/span>Train dataset<\/a><\/span><\/li><li><span><a href=\"#Test-set\" data-toc-modified-id=\"Test-set-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;<\/span>Test set<\/a><\/span><\/li><li><span><a href=\"#Check-LGBM-and-NN-performance-on-val-folds\" data-toc-modified-id=\"Check-LGBM-and-NN-performance-on-val-folds-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;<\/span>Check LGBM and NN performance on val folds<\/a><\/span><\/li><li><span><a href=\"#Train-Logistic-Regression-on-LGBM+NN-features\" data-toc-modified-id=\"Train-Logistic-Regression-on-LGBM+NN-features-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;<\/span>Train Logistic Regression on LGBM+NN features<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Postprocess-prediction\" data-toc-modified-id=\"Postprocess-prediction-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;<\/span>Postprocess prediction<\/a><\/span><\/li><li><span><a href=\"#Plot-results\" data-toc-modified-id=\"Plot-results-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;<\/span>Plot results<\/a><\/span><\/li><\/ul><\/div>","3c66c4ec":"# Import data","4afe82b6":"## Dataset generator","7b8a7228":"# Prepare data","880bfcdc":"## Find best hyper-parameters by cross-validation and sub-sampling","8f5e73c2":"# Plot results","569a415d":"##\u00a0Train LGBM, cross-validation evaluation and predict on test set","92fefe6e":"## Test set","a4c5de68":"# Neural network","24325b47":"## Define model and weighted loss","c2b08cea":"## Train dataset","a31184c3":"# Stacking : LGBM + NN","3dbd1b4c":"## Prepare functions","327c46a0":"## Metric","9091b519":"## Check LGBM and NN performance on val folds","0da3c1c2":"## Compute MoA weights from train set\n(needed during loss)","9815b9d4":"## Weight features from LGBM features importances","e0aeb598":"# Postprocess prediction"}}