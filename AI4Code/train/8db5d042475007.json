{"cell_type":{"850ca539":"code","c251dff6":"code","e26115d7":"code","f813c8d9":"code","ef760025":"code","775a9852":"code","6c0d3584":"code","cc8d077c":"code","dfffd7e7":"code","d444dd3b":"code","cf8d152b":"code","ba7db709":"code","a117a1d5":"code","59ed7204":"code","6cac7eef":"code","387badb0":"code","433937f1":"code","b419a30c":"code","46d57470":"code","a87d956a":"code","c150638b":"code","4598163f":"code","1e4f0894":"code","c0cb0dcf":"code","11e292ff":"code","9e5818dd":"code","48b40113":"code","93278cc3":"code","e3527f3f":"code","19c792e7":"code","782a927d":"code","ff7324fd":"code","83bb398d":"code","6fb5bca1":"code","939a12de":"code","470364a3":"code","450bf2b7":"code","80429057":"code","1cc66d7b":"code","b857babe":"code","3dab21d7":"code","a6ea61a0":"code","7e04764a":"code","531d6e77":"code","807917cf":"code","c3a7cd3c":"code","a7827a62":"code","67df6f13":"code","27dd59d3":"code","67da4557":"code","c6cc6398":"code","42bb697b":"code","5a127207":"code","db7f579d":"code","79945208":"code","fe388e57":"code","5780fbb2":"code","176a9bc3":"code","a2a7336a":"code","69f3d7f8":"code","8854486f":"code","7e92a2cc":"code","f937bab1":"code","7d88d7b7":"code","2b87dbdf":"code","59c62997":"code","ad34571d":"code","7800c501":"code","ab1aa07c":"code","dc70c23d":"code","ed9dd719":"code","6c518fb7":"code","7493e085":"code","9af82abf":"code","e2716c71":"code","ec6c046f":"code","ab2fa022":"code","7dd5b214":"code","db1c63fc":"code","0a5ea568":"code","e3be0f4b":"code","47148077":"code","a8ff0a93":"code","f675474f":"code","23048678":"code","5f56f25b":"markdown","714ee710":"markdown","a6af4351":"markdown","195f5197":"markdown","b564d821":"markdown","728b952c":"markdown","51ddb6f9":"markdown","878f1840":"markdown","da881fd9":"markdown","cf92e005":"markdown","33d3a560":"markdown","572899a3":"markdown","5737be10":"markdown","e7e156b4":"markdown","35536314":"markdown","3828cdc3":"markdown","68f08081":"markdown","31ee4f80":"markdown","7e5e811c":"markdown","e042f392":"markdown","241c67cc":"markdown","3b47b2b1":"markdown","dd8c92e0":"markdown","e2887d3c":"markdown","afbd0802":"markdown","c61bce1d":"markdown","afeb7af3":"markdown","38335fea":"markdown","fe39a14d":"markdown","c60bb2c3":"markdown","57ddb3b7":"markdown"},"source":{"850ca539":"!pip install fastai==0.7.0","c251dff6":"#It will automatically reload the latest module when you start again\n%load_ext autoreload\n%autoreload 2\n#Used to display plots and graphs inside Jupyter\n%matplotlib inline","e26115d7":"#The following are fastAi imports\nfrom fastai.imports import * \nfrom fastai.structured import *\n\nfrom pandas_summary import DataFrameSummary\nfrom sklearn.ensemble import RandomForestRegressor\nfrom IPython.display import display\n\nfrom sklearn import metrics","f813c8d9":"?display","ef760025":"??display","775a9852":"import os\nprint(os.listdir(\"..\/input\"))","6c0d3584":"PATH = \"..\/input\/train\/\"","cc8d077c":"# ! says it is a bash command and not a jupyter command, and {} says it is a python variable\n!ls {PATH}","dfffd7e7":"!head -n 5 ..\/input\/train\/Train.csv","d444dd3b":"df_raw = pd.read_csv(PATH+ 'Train.csv', low_memory = False ,parse_dates=['saledate'])","cf8d152b":"df_raw","ba7db709":"#function to display all the data of the dataframe at one go\ndef display_all(df):\n    with pd.option_context(\"display.max_rows\", 1000):\n        with pd.option_context(\"display.max_columns\", 1000):\n            display(df)","a117a1d5":"display_all(df_raw.transpose())","59ed7204":"df_raw.SalePrice  = np.log(df_raw.SalePrice)","6cac7eef":"df_raw.saledate.head(5)","387badb0":"def add_datepart(df, fldname, drop=True, time=False):\n    \"Helper function that adds columns relevant to a date.\"\n    fld = df[fldname]\n    fld_dtype = fld.dtype\n    if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n        fld_dtype = np.datetime64\n\n    if not np.issubdtype(fld_dtype, np.datetime64):\n        df[fldname] = fld = pd.to_datetime(fld, infer_datetime_format=True)\n    targ_pre = re.sub('[Dd]ate$', '', fldname)\n    attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear',\n            'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n    if time: attr = attr + ['Hour', 'Minute', 'Second']\n    for n in attr: df[targ_pre + n] = getattr(fld.dt, n.lower())\n    df[targ_pre + 'Elapsed'] = fld.astype(np.int64) \/\/ 10 ** 9\n    if drop: df.drop(fldname, axis=1, inplace=True)","433937f1":"add_datepart(df_raw, 'saledate')\ndf_raw.saleYear.head(5)","b419a30c":"train_cats(df_raw) #behind scenes everything will be converted into numbers","46d57470":"df_raw.UsageBand.cat.categories","a87d956a":"#the UsageBand is in wired order, so we'll convert into order High Med Low\ndf_raw.UsageBand.cat.set_categories(['High', 'Medium', 'Low'],ordered = True ,inplace = True)","c150638b":"display_all(df_raw.isnull().sum().sort_index()\/len(df_raw))","4598163f":"df, y, nas = proc_df(df_raw, 'SalePrice')","1e4f0894":"nas #this is created by proc_df they are new columns along with there mean values","c0cb0dcf":"#all columns\ndf.columns","11e292ff":"#now check everything is numeric\ndf.head()","9e5818dd":"m = RandomForestRegressor(n_jobs=-1) #njobs = -1 use all the resouces for running our model\nm.fit(df, y)\nm.score(df, y)","48b40113":"def split_vals(a,n) : return a[:n].copy(), a[n:].copy()","93278cc3":"n_valid = 12000 #same as kaggle's test size\nn_trn = len(df) - n_valid\nraw_train, raw_valid = split_vals(df, n_trn)\nx_train, x_valid = split_vals(df, n_trn)\ny_train, y_valid = split_vals(y, n_trn)\n","e3527f3f":"x_train.shape, y_train.shape, x_valid.shape","19c792e7":"#the following code will print the score\ndef rmse(x, y): return math.sqrt(((x-y)**2).mean())\n\ndef print_score(m):\n    res = [rmse(m.predict(x_train), y_train), rmse(m.predict(x_valid), y_valid), m.score(x_train, y_train), m.score(x_valid, y_valid)]\n    if hasattr(m ,'obb_score_'): res.append(m.obb_score_)\n    print(res)","782a927d":"m = RandomForestRegressor(n_jobs=-1)\n%time m.fit(df,y)\nprint_score(m)","ff7324fd":"df_trn, y_trn, nas = proc_df(df_raw, 'SalePrice', subset=30000)\nx_train, _ = split_vals(df_trn, 20000)\ny_train, _ = split_vals(y_trn, 20000)","83bb398d":"m = RandomForestRegressor(n_jobs=-1)\n%time m.fit(x_train, y_train)\nprint_score(m)","6fb5bca1":"#RandomForest randomizes the things, so to stop it we set Bootstrap = False\n#n_estimator depicts that we are using 1 tree with max_depth of that tree 3\nm = RandomForestRegressor(n_jobs=-1, n_estimators=1, max_depth=3, bootstrap=False)\n%time m.fit(x_train, y_train)\nprint_score(m)","939a12de":"draw_tree(m.estimators_[0], df_trn, precision=3)","470364a3":"m = RandomForestRegressor(n_jobs=-1, n_estimators=40, max_depth=35)\n%time m.fit(x_train, y_train)\nprint_score(m)","450bf2b7":"#let's check how each tree is working differently and how's there predictions\npreds = np.stack([t.predict(x_valid)for t in m.estimators_])\npreds[:, 0], np.mean(preds[:, 0]), y_valid[0]","80429057":"preds.shape","1cc66d7b":"plt.plot([metrics.r2_score(y_valid, np.mean(preds[:i+1], axis = 0)) for i in range(40)])","b857babe":"m = RandomForestRegressor(n_jobs=-1, n_estimators=40,oob_score=True)\n%time m.fit(x_train, y_train)\nprint_score(m)","3dab21d7":"df_trn, y_trn, nas = proc_df(df_raw, 'SalePrice')\nx_train, _ = split_vals(df_trn, n_trn)\ny_train, _ = split_vals(y_trn, n_trn)","a6ea61a0":"set_rf_samples(20000)","7e04764a":"m = RandomForestRegressor(n_jobs=-1, n_estimators=40,oob_score=True)\n%time m.fit(x_train, y_train)\nprint_score(m)","531d6e77":"reset_rf_samples()","807917cf":"def dectree_max_depth(tree):\n    children_left = tree.children_left\n    children_right = tree.children_right\n    \n    def walk(node_id):\n        if(children_left[node_id] != children_right[node_id]):\n            left_max = 1 + walk(children_left[node_id])\n            righ_max = 1 + walk(children_right)\n            return max(left_max, right_max)\n        else:\n            return 1\n        root_node_id = 0\n        return walk(root_node_id)","c3a7cd3c":"m = RandomForestRegressor(n_jobs=-1, n_estimators=40,oob_score=True)\n%time m.fit(x_train, y_train)\nprint_score(m)","a7827a62":"t = m.estimators_[0].tree_","67df6f13":"t.max_depth","27dd59d3":"m = RandomForestRegressor(n_jobs=-1, n_estimators=40,oob_score=True, min_samples_leaf = 5)\n%time m.fit(x_train, y_train)\nprint_score(m)","67da4557":"t = m.estimators_[0].tree_\ndectree_max_depth(t)","c6cc6398":"m = RandomForestRegressor(n_jobs=-1, n_estimators=40,oob_score=True, min_samples_leaf = 5, max_features=0.5)\n%time m.fit(x_train, y_train)\nprint_score(m)","42bb697b":" set_rf_samples(50000)","5a127207":"def get_preds(t): return t.predict(x_valid)\n%time preds = np.stack(parallel_trees(m, get_preds))\nnp.mean(preds[:, 0]), np.std(preds[:,0])","db7f579d":"x = raw_valid.copy()\nx['pred_std'] = np.std(preds, axis = 0)\nx['pred'] = np.mean(preds, axis = 0)\nx.Enclosure.value_counts().plot.barh()","79945208":"flds = ['Enclosure','pred', 'pred_std']\nenc_summ = x[flds].groupby('Enclosure', as_index = False).mean()\nenc_summ","fe388e57":"raw_valid.ProductSize.value_counts().plot.barh()","5780fbb2":"fi = rf_feat_importance(m, df_trn)\nfi[:10]","176a9bc3":"fi.plot('cols', 'imp', figsize = (10,6), legend = False)","a2a7336a":"def plot_fi(fi) : return fi.plot('cols', 'imp', 'barh', figsize = (12,7), legend=False)","69f3d7f8":"plot_fi(fi[:30])","8854486f":"to_keep = fi[fi.imp > 0.005].cols\nlen(to_keep)","7e92a2cc":"df_keep = df_trn[to_keep].copy()\nx_train, x_valid = split_vals(df_keep, n_trn)","f937bab1":"m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(x_train, y_train)\nprint_score(m)","7d88d7b7":"fi = rf_feat_importance(m, df_keep)\nplot_fi(fi)","2b87dbdf":"df_raw.YearMade.head(5)\ndf_raw.Coupler_System.head(5)","59c62997":"plt.figure(figsize=(15,5))\n\nplt.subplot(1,2,1)\nplt.scatter(df_raw['YearMade'], df_raw['SalePrice'])\nplt.ylabel('SalePrice')\nplt.xlabel('YearMade')\n\nplt.subplot(1,2,2)\nsns.stripplot(df_raw['Coupler_System'], df_raw['SalePrice'])","ad34571d":"plt.figure(figsize=(15,5))\n\nplt.subplot(1,2,1)\nsns.stripplot(df_raw['ProductSize'], df_raw['SalePrice'],order = ['Large', 'Medium', 'Small', 'Mini', 'Compact'])\n\nplt.subplot(1,2,2)\nsns.stripplot(df_raw['fiProductClassDesc'], df_raw['SalePrice'])\n","7800c501":"plt.hist(df_raw['YearMade'], bins = 10)\nplt.show()","ab1aa07c":"df_raw['Coupler_System'].value_counts().plot(kind='bar')","dc70c23d":"df_raw['ProductSize'].value_counts().plot(kind='bar')","ed9dd719":"reset_rf_samples()","6c518fb7":"x_train, x_valid = split_vals(df_keep, n_trn)","7493e085":"m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(x_train, y_train)\nprint_score(m)","9af82abf":"set_rf_samples(50000)","e2716c71":"from scipy.cluster import hierarchy as hc","ec6c046f":"corr = np.round(scipy.stats.spearmanr(df_keep).correlation, 4)\ncorr_condensed = hc.distance.squareform(1-corr)\nz = hc.linkage(corr_condensed, method='average')\nplt.figure(figsize=(15,15))\ndendrogram = hc.dendrogram(z, labels=df_keep.columns, orientation='left', leaf_font_size = 16)\nplt.show()","ab2fa022":"def get_obb(df):\n    m = RandomForestRegressor(n_estimators=30,min_samples_leaf=5, max_features=0.6, n_jobs=-1, oob_score=True)\n    x, _ = split_vals(df, n_trn)\n    m.fit(x, y_train)\n    return m.oob_score_","7dd5b214":"get_obb(df_keep)","db1c63fc":"for c in ('saleYear', 'saleElapsed', 'fiModelDesc', 'fiBaseModel', 'Grouser_Tracks', 'Coupler_System'):\n    print(c, get_obb(df_keep.drop(c, axis = 1)))","0a5ea568":"to_drop = ['saleYear', 'fiBaseModel', 'Grouser_Tracks']\nget_obb(df_keep.drop(to_drop, axis = 1))","e3be0f4b":"df_keep.drop(to_drop, axis = 1, inplace=True)\nx_train, x_valid = split_vals(df_keep, n_trn)","47148077":"reset_rf_samples()\nm = RandomForestRegressor(n_estimators=60,min_samples_leaf=5, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(x_train, y_train)\nprint_score(m)","a8ff0a93":"df_raw.YearMade[df_raw.YearMade<1950] = 1950\ndf_keep['age'] = df_raw['age'] = df_raw.saleYear - df_raw.YearMade","f675474f":"x_train, x_valid = split_vals(df_keep, n_trn)\nm = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.6, n_jobs=-1)\nm.fit(x_train, y_train)\nplot_fi(rf_feat_importance(m, df_keep))","23048678":"m = RandomForestRegressor(n_estimators=100, min_samples_leaf=3, max_features=0.5, n_jobs=-1)\nm.fit(x_train, y_train)\nprint_score(m)","5f56f25b":"We revert to using a full bootstrap sample in order to show the impact of other over-fitting avoidance methods.\n","714ee710":"To check what each library do or it's source code just do the below thing\nto find what it does put ?\nto find it's source code put ??","a6af4351":"### Let's modify and tune the model to improve the predictions","195f5197":"Lets check the data using the head command(bash command)","b564d821":"## Removing redundant featire","728b952c":"### Inital Processing\nLets start with preprocessing, we'll check for datatypes, nan values ,outiers and many more things which will tell us a lot about data we have \n\n## Curse of dimensionality\n\u201cCurse of dimensionality\u201d \u2192 theoreticians don\u2019t like many dimensions (aka columns), but in practical use the more information the better because you don\u2019t know what might be.\n\nNo free lunch theorem \u2192 In theory, no one type of model will work well for any kind of data set, which true for random data sets\n\nJeremy\u2019s \u201cfree lunch theorem\u201d \u2192 in practice, random forest is the best model for most data b\/c most data is not random. A random-forest decision tree works for almost every structured-data problem.\n\nQ: if we have too many dimensions\/fields won\u2019t we run into co-linearity problems?\nRandom forests have almost no co-linearity problems\n\nWith completely off-the-shelf tools, i.e., without any customization, Jeremy\u2019s general purpose random forest analysis places around 100th place in Bulldozer competition (in the top 25%) of all submissions.\n\n### Let's convert the all variables which is categorical into continious\n\nNote : our model need all the inputs to be in continious form, so we have to explicity handle it \n\nLet's convert date into continious, like we can extract many things outof date, like year, day, was it a weekday etc\n\nSo in fastai there is a add_datepart function which finds many fields for us like month year week day, monthend start of month etc","51ddb6f9":"To check what a function do you can simple do to that function and click shift+tab, for more details click shift + tab + tab","878f1840":"## End\nSo this is the  notebook which explore the first 4 part of FastAI's machine learning course, we used here RandomForest as our model and trained the model on preprocessed data.\n\nStay tune for the next notebook which will cover next part of FastAI's series.\n\nUpvote the kernel if you find it helpful\n\nConnect on Linkedin - https:\/\/www.linkedin.com\/in\/savannahar\/","da881fd9":"So we can see that the score are pretty awesome after feature importance, it would have probably landed us in top 10","cf92e005":"We'll replace categories with numeric code ,handle missing continious values and split the dependent variable into a seperate variable\n\nEverything is done by proc_df","33d3a560":"Now like a single tree, let's create a forest this is called as\n## Bagging\nDecision tree methods are extremely flexible and easy to use, and when ensembled (using bagging or boosting) are the state of the art on many practical tasks\nfirst, by looking at the individual trees that make them up, then by learning about \u201cbagging\u201d, the simple trick that lets a random forest be much more accurate than any individual tree.","572899a3":"If you take a look at the metric of the competition then it is, RMSLE (Root mean squared log error) so let's convert the Y i.e target variable into it's log based values","5737be10":"Above we trained the model on entire data set ,so there is a possiblity that it may have overfitted the data and so the predictions are high, so let's split the data into train and validation set and check how our basic model is working on it","e7e156b4":"There are many missing values, which RandomForest cannot handle directly, so we'll handle it now","35536314":"Another way to reduce over-fitting is to grow our trees less deeply. We do this by specifying (with min_samples_leaf) that we require some minimum number of rows in every leaf node. This has two benefits:\n\nThere are less decision rules for each leaf node; simpler models should generalize better\nThe predictions are made by averaging more rows in the leaf node, resulting in less volatility\n\nWe can also increase the amount of variation amongst the trees by not only use a sample of rows for each tree, but to also using a sample of columns for each split. We do this by specifying max_features, which is the proportion of features to randomly select from at each split.\n\nNone\n0.5\n'sqrt'\n1, 3, 5, 10, 25, 100","3828cdc3":"## Interpreting Machine learning Model\n### Understanding the Data\nNow we'll understand the data deeper and quickly, other thing we'll learn is we'll handle large dataset\n\n* For unstructed data try deep learning, for structed mostly go with randomforest as it works almost all the time.","68f08081":"There are other many categorical data, we can directly handle them using train_cats function of fastai, it creates categorical varialbe for everything that is a string, it'll map int to sting","31ee4f80":"## Out of bag score\nNow what we could do is, we can pass few rows as validation set which were not used ealier, this way we have different validation set for each tree, and then we can average them\nKind of like crossvalidation\nSo it is like every row will be outofbag sample given to the tree\n\nSo if we pass obb_socre to RandomForest then it'll show us that score","7e5e811c":"We won't add more tree's as the accuracy is not increasing and the graph is flattening","e042f392":"Now let's use the data using pandas which help's us to better visualize the data and also perform various operations on it","241c67cc":"## Reducing Overfitting\n### Subsampling\n\nOverfitting does major damage to our predictions on test set\nWe can do subsampling to reduce overfitting by ","3b47b2b1":" ### Let's build a Single tree estimator\n the n_estimator in RandomForestRegressor tells us number of trees to use","dd8c92e0":"## lets run the model\nNow let's create our basic randomforest regressor with no hyper parameter tuning ","e2887d3c":"## Introduction to Random Forest\n\nIn this notebook, i'll try to explain you the fastAI's lecture series first topic.\n\nWe'll cover importance of Randomforest and why they are widely used today in all the competitions","afbd0802":"## Feature Importance\nIt's not normally enough to just to know that a model can make accurate predictions - we also want to know how it's makign predictions. he most important way to see this is with feature importance","c61bce1d":"## Partial Dependence\nWe can use ggplot to check the partial dependence","afeb7af3":"  So here we have train.csv and test.csv","38335fea":"### Imports\nCheck the comments about what each import is used for","fe39a14d":"## FastAI the latest favourite of Data Scientist\nIn this series, I'll try to walk you through fastAI series with detailed explaination.\n\nAlso the notebook will have the techniques taught in each part of course ,applied to come competition.\n\nLet's get started","c60bb2c3":"The basic idea is this: rather than limit the total amount of data that our model can access, let's instead limit it to a different random subset per tree. That way, given enough trees, the model can still see all the data, but for each individual tree it'll be just as fast as if we had cut down our dataset as before.\n","57ddb3b7":"### We'll be working with  Kaggle Blue Book Buldozers competiton so let's import the data"}}