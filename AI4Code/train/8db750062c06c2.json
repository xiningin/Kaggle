{"cell_type":{"e55a5e4c":"code","1775283a":"code","2ebbf5bf":"code","dfd5416e":"code","2aba2c89":"code","012a2625":"code","2dbfb74d":"code","a2317974":"code","4ff4e38b":"code","da7c123e":"code","44e5067f":"code","9b03cc04":"code","15f4f2cf":"code","c5851f0d":"code","19840b50":"code","415b737a":"code","076bf6ea":"code","645d73b4":"code","61802b74":"code","f58996bc":"code","42a642e3":"code","0dbc2e45":"code","d837116b":"code","4177c263":"code","46820f27":"code","d8fff007":"code","42f30f67":"code","3b43bcbd":"code","f6d51b0e":"code","a05a59a0":"code","b638f5c3":"code","efc442ce":"code","f9ee1972":"code","29d57209":"markdown","d18b38bd":"markdown","e1514e0e":"markdown","2d64d479":"markdown","0eb81694":"markdown","494fa3bf":"markdown","7dc24af0":"markdown","6e241146":"markdown","4bd56584":"markdown","b8fb6218":"markdown","7711887c":"markdown","9d281cce":"markdown","133d90de":"markdown"},"source":{"e55a5e4c":"import math\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport gc\nimport tensorflow as tf\nimport time\nimport keras\nimport cv2\nimport scipy.special\nfrom keras.optimizers import SGD\nfrom keras.models import Model, Sequential\nfrom keras.layers import Input, Dense, Activation, Add, GlobalAveragePooling2D, Dropout, Flatten, BatchNormalization, Lambda\nfrom keras.layers.convolutional import Conv2D, MaxPooling2D\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.regularizers import l2\nfrom keras.utils import plot_model\nfrom tqdm.auto import tqdm\n\nfrom PIL import Image\n\n# set max display columns and rows count\npd.set_option('display.max_columns', 10000)\npd.set_option('display.max_rows', 10000)","1775283a":"import os\n__print__ = print\ndef print_log(string):\n    os.system(f'echo \\\"{string}\\\"')\n    __print__(string)","2ebbf5bf":"# Install EfficientNet\n!pip install '\/kaggle\/input\/kerasefficientnetb3\/efficientnet-1.0.0-py3-none-any.whl'\nimport efficientnet.keras as efn","dfd5416e":"# class_map \ubd88\ub7ec\uc624\uae30(grapheme_root, vowel_diacritic, consonant_diacritic \uc885\ub958\uc640 \ub515\uc154\ub108\ub9ac\uac00 \ub4e4\uc5b4\uc788\uc74c)\nclass_map = pd.read_csv('\/kaggle\/input\/bengaliai-cv19\/class_map.csv')\n\n# class_map\uc5d0 \uc788\ub294 grapheme_root, vowel_diacritic, consonant_diacritic \uc815\ubcf4\ub97c \ubd84\ub9ac\ud558\uc5ec \ubcf4\uad00\ngrapheme_root = class_map[class_map['component_type'] == 'grapheme_root']['component'].values\nGRAPHEME_ROOT_NUM = grapheme_root.shape[0]\nvowel_diacritic = class_map[class_map['component_type'] == 'vowel_diacritic']['component'].values\nVOWEL_DIACRITIC_NUM = vowel_diacritic.shape[0]\nconsonant_diacritic = class_map[class_map['component_type'] == 'consonant_diacritic']['component'].values\nCONSONANT_DIACRITIC_NUM = consonant_diacritic.shape[0]","2aba2c89":"# train.csv \ud30c\uc77c \uacbd\ub85c\nTRAIN_META_PATH = '\/kaggle\/input\/bengaliai-cv19\/train.csv'\n\n# train_image_data.parquet \ud30c\uc77c \uacbd\ub85c\nTRAIN_IMG_PATH = ['\/kaggle\/input\/bengaliai-cv19\/train_image_data_0.parquet',\n           '\/kaggle\/input\/bengaliai-cv19\/train_image_data_1.parquet',\n           '\/kaggle\/input\/bengaliai-cv19\/train_image_data_2.parquet',\n           '\/kaggle\/input\/bengaliai-cv19\/train_image_data_3.parquet']\n\n# \uc774\ubbf8\uc9c0 \ud06c\uae30 \uc815\ubcf4(image height, image width)\nRAW_IMG_ROWS, RAW_IMG_COLUMNS = 137, 236","012a2625":"# \ud559\uc2b5 \ub808\uc774\ube14 \uac00\uc838\uc624\ub294 \ud568\uc218\n# split_y \uc778\uc790\ub97c \ud1b5\ud574 train_y\ub97c 3\uac1c\uc758 train_y_grapheme_root, train_y_vowel_diacritic, train_y_consonant_diacritic\ub85c \ub098\ub20c \uc218 \uc788\ub2e4\ndef get_train_y(train_meta_path, data_range, split_y=True):\n    train_meta_data = pd.read_csv(train_meta_path)\n    \n    # pandas \ub0b4\uc7a5 \ud568\uc218\ub97c \uc774\uc6a9\ud574\uc11c one hot encoding \uc801\uc6a9\n    train_y_grapheme_root       = pd.get_dummies(train_meta_data['grapheme_root']).to_numpy(dtype='float32')\n    train_y_vowel_diacritic     = pd.get_dummies(train_meta_data['vowel_diacritic']).to_numpy(dtype='float32')\n    train_y_consonant_diacritic = pd.get_dummies(train_meta_data['consonant_diacritic']).to_numpy(dtype='float32')\n    \n    # multiclassification: \ub9c8\uc9c0\ub9c9 \ub808\uc774\uc5b4\uc5d0 \ub4e4\uc5b4\uac08 \ub808\uc774\ube14\uc744 \uc11c\ub85c \ubd84\ub9ac\ud560 \uc9c0, \ud569\uce60 \uc9c0\ub97c \uc801\uc6a9\n    train_y = [train_y_grapheme_root[data_range], train_y_vowel_diacritic[data_range], train_y_consonant_diacritic[data_range]]\n    if (split_y is False):\n        train_y = np.concatenate(train_y, axis=1)\n        \n    return train_y\n\n# train_image_data_x.parquet\uc744 \ubd88\ub7ec\uc640\uc11c numpy \ubc30\uc5f4\ub85c \ub418\uc5b4 \uc788\ub294 \uc774\ubbf8\uc9c0\uc640, \uc2dc\uc791 Id, \uadf8\ub9ac\uace0 \ub4e4\uc5b4 \uc788\ub294 \uc774\ubbf8\uc9c0\uc758 \uac1c\uc218\ub97c \ubc18\ud658\ud55c\ub2e4.\ndef get_img_data(img_path):\n    data = pd.read_parquet(img_path)\n    \n    # train_image_data\uc758 \uc778\ub371\uc2a4 \uc815\ubcf4\uc640 Image_Id \uc815\ubcf4\uac00 \uc77c\uce58\ud558\uc9c0 \uc54a\uc744 \uc218 \uc788\uc74c(train_image_data_1.parquet\uc758 index\uac00 0\uc77c \ub54c\uc5d0\ub294 Image_Id\uac00 50210\uc774\ub2e4.)\n    # \ub530\ub77c\uc11c meta data\uc640 \uc0c1\uc751\ud55c \uc815\ubcf4\ub97c \uac00\uc838\uc624\uac8c \ud558\uae30 \uc704\ud574\uc11c train_image_data\uc758 \uccab id \uac12\uacfc \uc694\uc18c\uc758 \uac1c\uc218\ub97c \ubc18\ud658\ud558\ub3c4\ub85d \ud55c\ub2e4.\n    start_id    = int(data.iloc[0, 0].split('_')[1])\n    element_num = data.shape[0]\n    \n    # pandas\ub85c \ub370\uc774\ud130\ub97c \ubd88\ub7ec\uc640 numpy \ubc30\uc5f4\ub85c \ubc14\uafb8\ub294 \uc791\uc5c5\uc774 \ud544\uc694\ud558\ub2e4.\n    # pandas\ub85c \ub370\uc774\ud130\ub97c \uc870\uc791\ud560 \ub54c\uc5d0\ub294 \uc18d\ub3c4\uac00 \ub108\ubb34 \ub290\ub9ac\ub2e4(\ud2b9\ud788 iloc). \ub530\ub77c\uc11c numpy \ubc30\uc5f4\ub85c \ubc14\uafb8\uc5b4 \ubc18\ud658\ud574\uc11c \uc18d\ub3c4 \ud5a5\uc0c1\uc744 \uaf80\ud55c\ub2e4.\n    img_data = data[data.columns[1:]]\n    img_data = img_data.to_numpy(dtype='uint8')\n    img_data = img_data.reshape(-1, RAW_IMG_ROWS, RAW_IMG_COLUMNS)\n    \n    del data\n    gc.collect()\n    \n    return img_data, start_id, element_num","2dbfb74d":"# \ub370\uc774\ud130 \uc804\ucc98\ub9ac \ud568\uc218 Version 1\ndef load_train_data_v1(train_img_path, split_y=True):\n    print('\ud559\uc2b5 \uc774\ubbf8\uc9c0 \ubd88\ub7ec\uc624\ub294 \uc911 ...')\n    print('  \uacbd\ub85c:', train_img_path)\n    img_data, start_id, element_num = get_img_data(train_img_path)\n    print('  \ucd1d ', element_num, '\uac1c\uc758 \uc774\ubbf8\uc9c0')\n    print('\ud559\uc2b5 \uc774\ubbf8\uc9c0 \ubd88\ub7ec\uc624\uae30 \uc644\ub8cc !')\n    \n    # create train_y\n    print('\ud559\uc2b5 \ub808\uc774\ube14 \uc0dd\uc131 \uc911 ...')\n    train_y = get_train_y(TRAIN_META_PATH, range(start_id, start_id + element_num), split_y=split_y)\n    print('\ud559\uc2b5 \ub808\uc774\ube14 \uc0dd\uc131 \uc644\ub8cc !')\n\n    # create train_x\n    print('\ud559\uc2b5 \uc774\ubbf8\uc9c0 \uc0dd\uc131\uc911 ...')\n    # train_x \uc815\ubcf4 \uc0dd\uc131 \ubc0f \uc774\ubbf8\uc9c0 \ud06c\uae30 \uc870\uc808\n    img_rows = RAW_IMG_ROWS \/\/ 2\n    img_columns = RAW_IMG_COLUMNS \/\/ 2\n    train_x = np.empty(dtype='uint8', shape=(element_num, img_rows, img_columns))\n    \n    for i in range(train_x.shape[0]): # resize image\n        train_x[i] = cv2.resize(img_data[i], dsize=(img_columns, img_rows)) # width * height\n    print('  \ud559\uc2b5 \uc774\ubbf8\uc9c0 \ud06c\uae30:', '(' + str(img_rows) + ', ' + str(img_columns) + ')')\n    \n    # \uba54\ubaa8\ub9ac \uad00\ub9ac\n    del img_data # img_data\ub97c \ub354 \uc774\uc0c1 \uc548 \uc4f0\ubbc0\ub85c \uc77c\ub2e8 \uba54\ubaa8\ub9ac\uc5d0\uc11c \uc0ad\uc81c\n    gc.collect()\n\n    # keras CNN \ubaa8\ub378\uc5d0 \ub123\uae30 \uc704\ud55c \ucc28\uc6d0 \ubc0f \ub370\uc774\ud130\ud0c0\uc785 \uc815\ub9ac\n    train_x = train_x.reshape((-1, img_rows, img_columns, 1))\n    train_x = train_x.astype('float32')\n    train_x = train_x \/ 255.0\n    print('\ud559\uc2b5 \uc774\ubbf8\uc9c0 \uc0dd\uc131 \uc644\ub8cc !')\n\n    return train_x, train_y","a2317974":"# \ub370\uc774\ud130 \uc804\ucc98\ub9ac \ud568\uc218 Version 2\ndef load_train_data_v2(train_img_path, output_img_size, split_y=True):\n    print('\ud559\uc2b5 \uc774\ubbf8\uc9c0 \ubd88\ub7ec\uc624\ub294 \uc911 ...')\n    print('  \uacbd\ub85c:', train_img_path)\n    img_data, start_id, element_num = get_img_data(train_img_path)\n    print('  \ucd1d ', element_num, '\uac1c\uc758 \uc774\ubbf8\uc9c0')\n    print('\ud559\uc2b5 \uc774\ubbf8\uc9c0 \ubd88\ub7ec\uc624\uae30 \uc644\ub8cc !')\n    \n    # create train_y\n    print('\ud559\uc2b5 \ub808\uc774\ube14 \uc0dd\uc131 \uc911 ...')\n    train_y = get_train_y(TRAIN_META_PATH, range(start_id, start_id + element_num), split_y=split_y)\n    print('\ud559\uc2b5 \ub808\uc774\ube14 \uc0dd\uc131 \uc644\ub8cc !')\n    \n    # create train_x\n    print('\ud559\uc2b5 \uc774\ubbf8\uc9c0 \uc0dd\uc131\uc911 ...')\n    img_rows    = output_img_size[1]\n    img_columns = output_img_size[0]\n    train_x = np.empty(dtype='uint8', shape=(element_num, img_rows, img_columns))\n    \n    for i in tqdm(range(train_x.shape[0])):\n        img = img_data[i]\n        # get ROI(region of interest)\n        x1, y1, x2, y2 = get_img_roi(img)\n        # crop image\n        img = img[y1:y2, x1:x2]\n        # resize image\n        img = cv2.resize(img, dsize=(img_columns, img_rows)) # width * height\n        train_x[i] = img\n        \n    print('  \ud559\uc2b5 \uc774\ubbf8\uc9c0 \ud06c\uae30:', '(' + str(img_rows) + ', ' + str(img_columns) + ')')\n    \n    # keras CNN \ubaa8\ub378\uc5d0 \ub123\uae30 \uc704\ud55c \ucc28\uc6d0 \ubc0f \ub370\uc774\ud130\ud0c0\uc785 \uc815\ub9ac\n    train_x = train_x.reshape((-1, img_rows, img_columns, 1))\n    train_x = train_x.astype('float32')\n    train_x = train_x \/ 255.0\n    print('\ud559\uc2b5 \uc774\ubbf8\uc9c0 \uc0dd\uc131 \uc644\ub8cc!')\n    \n    return train_x, train_y\n\n# \uc774\ubbf8\uc9c0\uc758 \uad00\uc2ec \uc601\uc5ed\uc744 \uc88c\ud45c(x1, y1, x2, y2)\ub85c \ubc18\ud658\ud558\ub294 \ud568\uc218\ndef get_img_roi(img):\n    _, img_thres = cv2.threshold(img, 200, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n        \n    img_thres_sum_x = img_thres.sum(axis=0)\n    for x1 in range(0, RAW_IMG_COLUMNS, 1):\n        if (img_thres_sum_x[x1] > 0):\n            break\n\n    for x2 in range(RAW_IMG_COLUMNS-1, -1, -1):\n        if (img_thres_sum_x[x2] > 0):\n            break\n\n    img_thres_sum_y = img_thres.sum(axis=1)\n    for y1 in range(0, RAW_IMG_ROWS, 1):\n        if (img_thres_sum_y[y1] > 0):\n            break\n\n    for y2 in range(RAW_IMG_ROWS-1, -1, -1):\n        if (img_thres_sum_y[y2] > 0):\n            break\n            \n    return x1, y1, x2, y2","4ff4e38b":"IMG_ROWS    = 64\nIMG_COLUMNS = 64\n\ntrain_x, train_y = load_train_data_v2(TRAIN_IMG_PATH[0], output_img_size=(IMG_ROWS, IMG_COLUMNS), split_y=True)","da7c123e":"# \ud2b9\uc815 \ubcb5\uace8\uc5b4 \uc190\uae00\uc528 \ub370\uc774\ud130 \uc2dc\uac01\ud654(train set\uc5d0\uc11c\ub9cc \uc0ac\uc6a9)\ndef visualize_grapheme(grapheme, train_img_path):\n    meta_data = pd.read_csv(TRAIN_META_PATH)\n    img_data = pd.read_parquet(train_img_path)\n    \n    grapheme_id = meta_data[meta_data['grapheme'] == grapheme]['image_id'].values # train_data\uc5d0\uc11c \ud45c\uc2dc\ud560 \ubcb5\uace8\uc5b4\uc5d0 \ud574\ub2f9\ud558\ub294 image_id \uc120\ud0dd\n    grapheme_image = img_data[img_data['image_id'].isin(grapheme_id)] # train_image_data\uc5d0\uc11c \ud45c\uc2dc\ud560 \ubcb5\uace8\uc5b4\uc5d0 \ud574\ub2f9\ud558\ub294 \uc774\ubbf8\uc9c0 \uc120\ud0dd\n    size = len(grapheme_image.index) # \uc774\ubbf8\uc9c0 \uac1c\uc218\n\n    # matplotlib figure \uc124\uc815\n    columns = 8\n    rows = size \/ columns + 1\n    fig = plt.figure(figsize=(30, rows * 3))\n\n    # figure \uadf8\ub9ac\uae30(imshow \uc774\uc6a9, imshow\ub97c \uc0ac\uc6a9\ud560 \ub54c \ub370\uc774\ud130 \ud0c0\uc785\uc774 uint8\uc774\uc5b4\uc57c \ud55c\ub2e4.)\n    for i in range(size):\n        image_index = str(grapheme_image.iloc[i, 0]).split('_')[1]\n        image = grapheme_image.iloc[i].values[1:].astype('uint8').reshape(RAW_IMG_ROWS, RAW_IMG_COLUMNS)\n\n        ax = fig.add_subplot(rows, columns, i + 1)\n        ax.imshow(image, cmap='gray')\n        ax.set_xlabel(grapheme_id[i])\n\n    plt.show()\n    del(meta_data)\n    del(img_data)\n    gc.collect()\n            \n# \ubcb5\uace8\uc5b4 \uc190\uae00\uc528 \ub370\uc774\ud130 \uc2dc\uac01\ud654\ndef visualize_bengali(img_path, count, bias = 0):\n    img_data = pd.read_parquet(img_path)\n    \n    # \ubc45\uace8\uc5b4 \uc190\uae00\uc528 \ub370\uc774\ud130 \uc2dc\uac01\ud654 (matplotlib \ud55c \ud654\uba74\uc5d0 \uc5ec\ub7ec\uac1c \uadf8\ub798\ud504 \uadf8\ub9ac\uae30\ub97c \ud1b5\ud574)\n    columns = 8\n    rows = int(count \/ columns) + 1\n    fig = plt.figure(figsize=(30, int(rows * 3)))\n\n    for i in range(0, count):\n        image_index = str(img_data.iloc[bias + i, 0]).split('_')[1]\n        # imshow\ub97c \ud558\uae30 \uc704\ud574\uc11c\ub294 nparray\uc758 type\uc774 uint8\uc774\uc5b4\uc57c \ud55c\ub2e4.\n        image = img_data.iloc[bias + i].values[1:].astype('uint8').reshape(RAW_IMG_ROWS, RAW_IMG_COLUMNS)\n\n        ax = fig.add_subplot(rows, columns, i + 1)\n        ax.imshow(image, cmap='gray')\n        ax.set_xlabel(image_index)\n\n    plt.show()\n    del(img_data)\n    gc.collect()\n    \n# grapheme\ub97c \uc785\ub825\ud558\uba74 \uc2dc\uac01\ud654\ud574\uc8fc\uace0, grapheme\uc758 \uc815\ubcf4\ub97c \ucd9c\ub825\ud574\uc900\ub2e4.\ndef show_grapheme_info(x, y, num, size):\n    plt.imshow(x[num].reshape(size), cmap='gray')\n    if (y is not list):\n        y = np.concatenate(y, axis=1)\n    \n    print(class_map[y[num] == 1])","44e5067f":"show_grapheme_info(x=train_x, y=train_y, num=100, size=(IMG_ROWS, IMG_COLUMNS))\n# visualize_grapheme('\u09b2\u09be', TRAIN_IMG_PATH[0])\n# visualize_bengali(TRAIN_IMG_PATH[2], count=24, bias=11160)","9b03cc04":"INPUT_SHAPE = (IMG_ROWS, IMG_COLUMNS, 1)\n\n# activation functions\nrelu = lambda x: keras.activations.relu(x, alpha=0.0, max_value=None, threshold=0.0)\nswish = lambda x: x * keras.activations.sigmoid(x)","15f4f2cf":"# resnet convolution unit\ndef resnet_conv2d_unit(filters, kernel_size, strides):\n    return Conv2D(filters=filters,\n                  kernel_size=kernel_size,\n                  strides=strides,\n                  padding='same',\n                  kernel_initializer='he_normal',\n                  kernel_regularizer=l2(1e-4))","c5851f0d":"def get_resnet_v1_conv1_layer(x, activation):\n    x = resnet_conv2d_unit(filters=64, kernel_size=(7, 7), strides=(2, 2))(x)\n    x = BatchNormalization()(x)\n    x = Activation(activation)(x)\n    return x\n\ndef get_resnet_v1_conv2_layer(x, activation, count):\n    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n    \n    for i in range(count):\n        shortcut = x\n        \n        x = resnet_conv2d_unit(filters=64, kernel_size=(3, 3), strides=(1, 1))(x)\n        x = BatchNormalization()(x)\n        x = Activation(activation)(x)\n        \n        x = resnet_conv2d_unit(filters=64, kernel_size=(3, 3), strides=(1, 1))(x)\n        x = BatchNormalization()(x)\n        x = Add()([x, shortcut])\n        x = Activation(activation)(x)\n        \n    return x\n\ndef get_resnet_v1_conv3_to_5_layer(x, activation, filters, count):\n    for i in range(count):\n        shortcut = x\n        \n        if i == 0:\n            x = resnet_conv2d_unit(filters=filters, kernel_size=(3, 3), strides=(2, 2))(x)\n        else:\n            x = resnet_conv2d_unit(filters=filters, kernel_size=(3, 3), strides=(1, 1))(x)\n        x = BatchNormalization()(x)\n        x = Activation(activation)(x)\n        \n        x = resnet_conv2d_unit(filters=filters, kernel_size=(3, 3), strides=(1, 1))(x)\n        x = Dropout(rate=0.2)(x)\n        x = BatchNormalization()(x)\n        if i == 0:\n            shortcut = resnet_conv2d_unit(filters=filters, kernel_size=(1, 1), strides=(2, 2))(shortcut)\n            shortcut = BatchNormalization()(shortcut)\n        x = Add()([x, shortcut])\n        x = Activation(activation)(x)\n        \n    return x","19840b50":"# ResNet v1 \uad6c\ud604\ndef get_resnet_v1_model(layer_num=(2, 2, 2, 2)):\n    input_tensor = Input(shape=INPUT_SHAPE, dtype='float32')\n\n    layer = get_resnet_v1_conv1_layer(input_tensor, activation=swish)\n    layer = get_resnet_v1_conv2_layer(layer, activation=swish, count=layer_num[0]) # filter_size: 64\n    layer = get_resnet_v1_conv3_to_5_layer(layer, activation=swish, filters=128, count=layer_num[1])\n    layer = get_resnet_v1_conv3_to_5_layer(layer, activation=swish, filters=256, count=layer_num[2])\n    layer = get_resnet_v1_conv3_to_5_layer(layer, activation=swish, filters=512, count=layer_num[3])\n\n    layer = Flatten()(layer)\n    layer = BatchNormalization()(layer)\n    layer = Activation(activation=swish)(layer)\n    \n    layer = Dense(1024, activation=swish, kernel_initializer='he_normal', kernel_regularizer=l2(1e-4))(layer)\n    layer = Dropout(0.2)(layer)\n    layer = BatchNormalization()(layer)\n    layer = Activation(activation=swish)(layer)\n    \n    # !! \uc2dc\ud589 \ucc29\uc624 !!\n    # * \ub2e4\ub978 \uacf3\uc5d0\uc11c\ub294 \ucd08\uae30\uac12\uc744 \uc124\uc815\ud560 \ub54c he \ubc29\ubc95\uc744 \uc0ac\uc6a9\ud588\ub294\ub370, \ub9c8\uc9c0\ub9c9 output network\uc758 \uac00\uc911\uce58\ub97c \ucd08\uae30\ud654 \ud560 \ub54c\n    #   kernel_initializer='he_normal', kernel_regularizer=l2(1e-4)\ub97c \uc0ac\uc6a9\ud574\uc8fc\uc9c0 \uc54a\uc558\ub2e4.\n    #   \uac00\uc911\uce58\ub97c \uac80\uc99d\ub41c \ubc29\ubc95\uc744 \ucd08\uae30\ud654 \ud558\ub2c8\uae4c \ud6e8\uc52c \ub354 \uc218\ub834\uc774 \uc798 \ub418\uc5c8\ub2e4.\n\n    # grapheme_root\n    output_grapheme_root       = Dense(GRAPHEME_ROOT_NUM,\n                                       name='grapheme_root',\n                                       activation='softmax', kernel_initializer='he_normal', kernel_regularizer=l2(1e-4))(layer)\n    # vowel_diacritic\n    output_vowel_diacritic     = Dense(VOWEL_DIACRITIC_NUM,\n                                       name='vowel_diacritic',\n                                       activation='softmax', kernel_initializer='he_normal', kernel_regularizer=l2(1e-4))(layer)\n    # consonant_diacritic\n    output_consonant_diacritic = Dense(CONSONANT_DIACRITIC_NUM,\n                                       name='consonant_diacritic',\n                                       activation='softmax', kernel_initializer='he_normal', kernel_regularizer=l2(1e-4))(layer)\n\n    return Model(inputs=input_tensor,\n                 outputs=[output_grapheme_root, output_vowel_diacritic, output_consonant_diacritic])","415b737a":"# ResNet v2 first layer\n# (conv1: 7x7_64_stride2)\ndef get_resnet_v2_conv1_layer(x, activation):\n    layer = resnet_conv2d_unit(filters=64, kernel_size=(7, 7), strides=(2, 2))(x)\n    layer = BatchNormalization()(layer)\n    layer = Activation(activation)(layer)\n    return layer\n\n# ResNet v2 second layer\n# (conv2: 3x3maxpooling_stride2 -> 1x1_64 -> 3x3_64 -> 1x1_256)\ndef get_resnet_v2_conv2_layer(x, activation, count):\n    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n    \n    for i in range(count):\n        shortcut = x # for skip connection(\uc2a4\ud0b5 \uc5f0\uacb0)\n        \n        x = resnet_conv2d_unit(filters=64, kernel_size=(1, 1), strides=(1, 1))(x)\n        x = BatchNormalization()(x)\n        x = Activation(activation)(x)\n\n        x = resnet_conv2d_unit(filters=64, kernel_size=(3, 3), strides=(1, 1))(x)\n        x = BatchNormalization()(x)\n        x = Activation(activation)(x)\n\n        x = resnet_conv2d_unit(filters=256, kernel_size=(1, 1), strides=(1, 1))(x)\n        x = BatchNormalization()(x)\n\n        # Skip Connection \ud560 \ub54c\uc5d0\ub294 \ucd9c\ub825 \ud150\uc11c\uc758 \ucc28\uc6d0\uacfc shortcut \ud150\uc11c\uc758 \ucc28\uc6d0\uc774 \uc11c\ub85c \ud2c0\ub9ac\ubbc0\ub85c \uac19\uac8c \ub9cc\ub4e4\uc5b4\uc900\ub2e4.\n        if (i == 0):\n            shortcut = resnet_conv2d_unit(filters=256, kernel_size=(1, 1), strides=(1, 1))(shortcut) \n        \n        x = Add()([x, shortcut]) # Skip Connection\n        x = Activation(activation)(x)\n\n    return x\n\n# ResNet v2 third layer\n# (conv3: 1x1_128(stride2 at i==0) -> 3x3_128 -> 1x1_512\ndef get_resnet_v2_conv3_layer(x, activation, count):\n    for i in range(count):\n        shortcut = x # for skip connection(\uc2a4\ud0b5 \uc5f0\uacb0)\n        \n        # output size \ucd95\uc18c(\uac00\ub85c \uc138\ub85c \uc808\ubc18\uc73c\ub85c)\n        if (i == 0):\n            x = resnet_conv2d_unit(filters=128, kernel_size=(1, 1), strides=(2, 2))(x)\n        else:\n            x = resnet_conv2d_unit(filters=128, kernel_size=(1, 1), strides=(1, 1))(x)\n\n        x = BatchNormalization()(x)\n        x = Activation(activation)(x)\n\n        x = resnet_conv2d_unit(filters=128, kernel_size=(3, 3), strides=(1, 1))(x)\n        x = BatchNormalization()(x)\n        x = Activation(activation)(x)\n\n        x = resnet_conv2d_unit(filters=512, kernel_size=(1, 1), strides=(1, 1))(x)\n        x = BatchNormalization()(x)\n\n        # \ucc98\uc74c Skip Connection \ud560 \ub54c\uc5d0\ub294 \ucd9c\ub825 \ud150\uc11c\uc758 \ucc28\uc6d0\uacfc shortcut \ud150\uc11c\uc758 \ucc28\uc6d0\uc774 \uc11c\ub85c \ud2c0\ub9ac\ubbc0\ub85c \uac19\uac8c \ub9cc\ub4e4\uc5b4\uc900\ub2e4.\n        # output size\ub3c4 \ub2e4\ub974\ubbc0\ub85c stride\ub97c 2\ub85c \uc124\uc815\ud574\uc11c output size\uc758 \ud06c\uae30\ub97c 1\/4\ub85c \uc904\uc778\ub2e4(\uac00\ub85c\uc640 \uc138\ub85c\uac00 \ub458 \ub2e4 \ubc18\uc529 \uc904\uc5c8\uc73c\ubbc0\ub85c)\n        if (i == 0):\n            shortcut = resnet_conv2d_unit(filters=512, kernel_size=(1, 1), strides=(2, 2))(shortcut) \n\n        x = Add()([x, shortcut]) # Skip Connection\n        x = Activation(activation)(x)\n\n    return x\n\n# ResNet v2 fourth layer\n# (conv4: 1x1_256(stride2 at i==0) -> 3x3_256 -> 1x1_1024)\ndef get_resnet_v2_conv4_layer(x, activation, count):\n    for i in range(count):\n        shortcut = x # for skip connection(\uc2a4\ud0b5 \uc5f0\uacb0)\n        \n        # output size \ucd95\uc18c(\uac00\ub85c \uc138\ub85c \uc808\ubc18\uc73c\ub85c)\n        if (i == 0):\n            x = resnet_conv2d_unit(filters=256, kernel_size=(1, 1), strides=(2, 2))(x)\n        else:\n            x = resnet_conv2d_unit(filters=256, kernel_size=(1, 1), strides=(1, 1))(x)\n\n        x = BatchNormalization()(x)\n        x = Activation(activation)(x)\n\n        x = resnet_conv2d_unit(filters=256, kernel_size=(3, 3), strides=(1, 1))(x)\n        x = BatchNormalization()(x)\n        x = Activation(activation)(x)\n\n        x = resnet_conv2d_unit(filters=1024, kernel_size=(1, 1), strides=(1, 1))(x)\n        x = BatchNormalization()(x)\n\n        # \ucc98\uc74c Skip Connection \ud560 \ub54c\uc5d0\ub294 \ucd9c\ub825 \ud150\uc11c\uc758 \ucc28\uc6d0\uacfc shortcut \ud150\uc11c\uc758 \ucc28\uc6d0\uc774 \uc11c\ub85c \ud2c0\ub9ac\ubbc0\ub85c \uac19\uac8c \ub9cc\ub4e4\uc5b4\uc900\ub2e4.\n        # output size\ub3c4 \ub2e4\ub974\ubbc0\ub85c stride\ub97c 2\ub85c \uc124\uc815\ud574\uc11c output size\uc758 \ud06c\uae30\ub97c 1\/4\ub85c \uc904\uc778\ub2e4(\uac00\ub85c\uc640 \uc138\ub85c\uac00 \ub458 \ub2e4 \ubc18\uc529 \uc904\uc5c8\uc73c\ubbc0\ub85c)\n        if (i == 0):\n            shortcut = resnet_conv2d_unit(filters=1024, kernel_size=(1, 1), strides=(2, 2))(shortcut) \n\n        x = Add()([x, shortcut]) # Skip Connection\n        x = Activation(activation)(x)\n\n    return x\n\n# ResNet v2 fifth layer\n# (conv4: 1x1_512(stride2 at i==0) -> 3x3_512 -> 1x1_2048)\ndef get_resnet_v2_conv5_layer(x, activation, count):\n    for i in range(count):\n        shortcut = x # for skip connection(\uc2a4\ud0b5 \uc5f0\uacb0)\n        \n        # output size \ucd95\uc18c(\uac00\ub85c \uc138\ub85c \uc808\ubc18\uc73c\ub85c)\n        if (i == 0):\n            x = resnet_conv2d_unit(filters=512, kernel_size=(1, 1), strides=(2, 2))(x)\n        else:\n            x = resnet_conv2d_unit(filters=512, kernel_size=(1, 1), strides=(1, 1))(x)\n\n        x = BatchNormalization()(x)\n        x = Activation(activation)(x)\n\n        x = resnet_conv2d_unit(filters=512, kernel_size=(3, 3), strides=(1, 1))(x)\n        x = BatchNormalization()(x)\n        x = Activation(activation)(x)\n\n        x = resnet_conv2d_unit(filters=2048, kernel_size=(1, 1), strides=(1, 1))(x)\n        x = BatchNormalization()(x)\n\n        # \ucc98\uc74c Skip Connection \ud560 \ub54c\uc5d0\ub294 \ucd9c\ub825 \ud150\uc11c\uc758 \ucc28\uc6d0\uacfc shortcut \ud150\uc11c\uc758 \ucc28\uc6d0\uc774 \uc11c\ub85c \ud2c0\ub9ac\ubbc0\ub85c \uac19\uac8c \ub9cc\ub4e4\uc5b4\uc900\ub2e4.\n        # output size\ub3c4 \ub2e4\ub974\ubbc0\ub85c stride\ub97c 2\ub85c \uc124\uc815\ud574\uc11c output size\uc758 \ud06c\uae30\ub97c 1\/4\ub85c \uc904\uc778\ub2e4(\uac00\ub85c\uc640 \uc138\ub85c\uac00 \ub458 \ub2e4 \ubc18\uc529 \uc904\uc5c8\uc73c\ubbc0\ub85c)\n        if (i == 0):\n            shortcut = resnet_conv2d_unit(filters=2048, kernel_size=(1, 1), strides=(2, 2))(shortcut) \n\n        x = Add()([x, shortcut]) # Skip Connection\n        x = Activation(activation)(x)\n\n    return x","076bf6ea":"# ResNet \uad6c\ud604\n# learning rate reduction\ub3c4 \uc801\uc6a9\ud558\uae30\ndef get_resnet_v2_50_layer_model():\n    input_tensor = Input(shape=INPUT_SHAPE, dtype='float32')\n\n    layer = get_resnet_v2_conv1_layer(input_tensor, activation=swish)\n    layer = get_resnet_v2_conv2_layer(layer, activation=swish, count=3)\n    layer = get_resnet_v2_conv3_layer(layer, activation=swish, count=4)\n    layer = get_resnet_v2_conv4_layer(layer, activation=swish, count=6)\n    layer = get_resnet_v2_conv5_layer(layer, activation=swish, count=3)\n\n    layer = GlobalAveragePooling2D()(layer)\n\n    # !! \uc2dc\ud589 \ucc29\uc624 !!\n    # * \ub2e4\ub978 \uacf3\uc5d0\uc11c\ub294 \ucd08\uae30\uac12\uc744 \uc124\uc815\ud560 \ub54c he \ubc29\ubc95\uc744 \uc0ac\uc6a9\ud588\ub294\ub370, \ub9c8\uc9c0\ub9c9 output network\uc758 \uac00\uc911\uce58\ub97c \ucd08\uae30\ud654 \ud560 \ub54c\n    #   kernel_initializer='he_normal', kernel_regularizer=l2(1e-4)\ub97c \uc0ac\uc6a9\ud574\uc8fc\uc9c0 \uc54a\uc558\ub2e4.\n    #   \uac00\uc911\uce58\ub97c \uac80\uc99d\ub41c \ubc29\ubc95\uc744 \ucd08\uae30\ud654 \ud558\ub2c8\uae4c \ud6e8\uc52c \ub354 \uc218\ub834\uc774 \uc798 \ub418\uc5c8\ub2e4.\n\n    # grapheme_root\n    output_grapheme_root       = Dense(GRAPHEME_ROOT_NUM,\n                                       name='grapheme_root',\n                                       activation='softmax', kernel_initializer='he_normal', kernel_regularizer=l2(1e-4))(layer)\n    # vowel_diacritic\n    output_vowel_diacritic     = Dense(VOWEL_DIACRITIC_NUM,\n                                       name='vowel_diacritic',\n                                       activation='softmax', kernel_initializer='he_normal', kernel_regularizer=l2(1e-4))(layer)\n    # consonant_diacritic\n    output_consonant_diacritic = Dense(CONSONANT_DIACRITIC_NUM,\n                                       name='consonant_diacritic',\n                                       activation='softmax', kernel_initializer='he_normal', kernel_regularizer=l2(1e-4))(layer)\n\n    return Model(inputs=input_tensor,\n                 outputs=[output_grapheme_root, output_vowel_diacritic, output_consonant_diacritic])","645d73b4":"# VGGNet \uad6c\ud604\ndef get_vggnet_model():\n    activation = swish\n    input_tensor = Input(shape=INPUT_SHAPE, dtype='float32')\n    \n    # input layer\n    model = Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation=activation)(input_tensor)\n\n    # first block\n    model = Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation=activation)(model)\n    model = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')(model)\n    model = BatchNormalization(momentum=0.15)(model)\n\n    # second block\n    model = Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation=activation)(model)\n    model = Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation=activation)(model)\n    model = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')(model)\n    model = BatchNormalization(momentum=0.15)(model)\n    model = Dropout(rate=0.3)(model)\n\n    # third block\n    model = Conv2D(filters=128, kernel_size=(3, 3), padding='same', activation=activation)(model)\n    model = Conv2D(filters=128, kernel_size=(3, 3), padding='same', activation=activation)(model)\n    model = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')(model)\n    model = BatchNormalization(momentum=0.15)(model)\n    model = Dropout(rate=0.3)(model)\n\n    # forth block\n    model = Conv2D(filters=256, kernel_size=(3, 3), padding='same', activation=activation)(model)\n    model = Conv2D(filters=256, kernel_size=(3, 3), padding='same', activation=activation)(model)\n    model = Conv2D(filters=256, kernel_size=(3, 3), padding='same', activation=activation)(model)\n    model = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')(model)\n    model = BatchNormalization(momentum=0.15)(model)\n\n    # fifth block\n    model = Conv2D(filters=256, kernel_size=(3, 3), padding='same', activation=activation)(model)\n    model = Conv2D(filters=256, kernel_size=(3, 3), padding='same', activation=activation)(model)\n    model = Conv2D(filters=256, kernel_size=(3, 3), padding='same', activation=activation)(model)\n    model = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')(model)\n    model = Dropout(rate=0.3)(model)\n\n    # fully connected\n    model = Flatten()(model)\n    model = Dense(2048, activation=activation)(model)\n    model = Dropout(rate=0.2)(model)\n    model = Dense(1024, activation=activation)(model)\n    \n    # classification block\n    output_grapheme_root       = Dense(GRAPHEME_ROOT_NUM, name='grapheme_root', activation='softmax')(model)\n    output_vowel_diacritic     = Dense(VOWEL_DIACRITIC_NUM, name='vowel_diacritic', activation='softmax')(model)\n    output_consonant_diacritic = Dense(CONSONANT_DIACRITIC_NUM, name='consonant_diacritic', activation='softmax')(model)\n\n    return Model(inputs=input_tensor,\n                 outputs=[output_grapheme_root, output_vowel_diacritic, output_consonant_diacritic])","61802b74":"# Generalized mean pool - GeM\ngm_exp = tf.Variable(3.0, dtype = tf.float32)\ndef generalized_mean_pool_2d(X):\n    pool = (tf.reduce_mean(tf.abs(X**(gm_exp)), \n                        axis = [1, 2], \n                        keepdims = False) + 1.e-7)**(1.\/gm_exp)\n    return pool","f58996bc":"# EfficientNet \ub9cc\ub4e4\uae30(\uc678\ubd80 \ub370\uc774\ud130 \ubd88\ub7ec\uc640\uc11c)\ndef get_efficiennet_model():\n    activation = swish\n    input_tensor = Input(shape=INPUT_SHAPE, dtype='float32')\n    \n    # Create and Compile Model and show Summary\n    model = efn.EfficientNetB3(weights=None,\n                               include_top=False,\n                               input_tensor=input_tensor,\n                               pooling=None,\n                               classes=None)\n    \n    # UnFreeze all layers\n    for layer in model.layers:\n        layer.trainable = True\n    \n    # GeM\n    lambda_layer = Lambda(generalized_mean_pool_2d)\n    lambda_layer.trainable_weights.extend([gm_exp])\n    x = lambda_layer(model.output)\n    \n    # multi output\n    output_grapheme_root       = Dense(GRAPHEME_ROOT_NUM, name='grapheme_root', activation='softmax')(x)\n    output_vowel_diacritic     = Dense(VOWEL_DIACRITIC_NUM, name='vowel_diacritic', activation='softmax')(x)\n    output_consonant_diacritic = Dense(CONSONANT_DIACRITIC_NUM, name='consonant_diacritic', activation='softmax')(x)\n\n    return Model(inputs=input_tensor,\n                 outputs=[output_grapheme_root, output_vowel_diacritic, output_consonant_diacritic])","42a642e3":"# model = get_resnet_v1_model(layer_num=(2, 2, 2, 2)) # 18-layer\n# model = get_resnet_v1_model(layer_num=(3, 4, 6, 3)) # 34-layer\n# model = get_resnet_v2_50_layer_model() # 54-layer\nmodel = get_efficiennet_model() # efficient-net\nprint('total', model.count_params(), 'parameter(s)')\n\n# optimizer = SGD(lr=0.01, momentum=0.9) # Stochastic gradient descent\noptimizer = 'adam'\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])","0dbc2e45":"plot_model(model, to_file='model.png')","d837116b":"batch_size = 256\nepochs = 12\nvalidation_split = 0.2\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=0.001,\n                              verbose=1, mode='auto')\ncallbacks = [reduce_lr]\n\ndef start_train(train_x, train_y):\n    hist = model.fit(train_x, train_y,\n                     batch_size=batch_size,\n                     epochs=epochs,\n                     validation_split=validation_split,\n                     callbacks=callbacks,\n                    )\n\ndef start_train_with_augmentation(train_x, train_y):   \n    # \ub370\uc774\ud130 \uc99d\uac15(\uc2e4\uc2dc\uac04\uc73c\ub85c \ucc98\ub9ac) \uc694\uc18c\n    datagen = ImageDataGenerator(\n        rotation_range=10, # -10~10\ub3c4 \ud68c\uc804\n        zoom_range=0.3     # 0.7~1.3\ubc30 \ud655\ub300\n    )\n    \n    # Fit the model\n    history = model.fit_generator(datagen.flow(train_x, {'grapheme_root': train_y[0], 'vowel_diacritic': train_y[1], 'consonant_diacritic': train_y[2]}, batch_size=batch_size),\n                                  epochs = epochs, \n                                  steps_per_epoch=x_train.shape[0] \/\/ batch_size, \n                                  callbacks=callbacks)","4177c263":"# \ud559\uc2b5 \uc2dc\uc791\nprint_log('train 0 start')\nstartTime = time.time()\nstart_train(train_x, train_y)\n# start_train_with_augmentation(train_x, train_y)\ndel(train_x)\ndel(train_y)\ngc.collect()\nprint_log('train 0 end')\n\nfor i in range(1, 4):\n    print_log('train ' + str(i) + ' start')\n    _train_x, _train_y = load_train_data_v2(TRAIN_IMG_PATH[i], output_img_size=(IMG_ROWS, IMG_COLUMNS), split_y=True)\n    start_train(_train_x, _train_y)\n#     start_train_with_augmentation(_train_x, _train_y)\n    del(_train_x)\n    del(_train_y)\n    gc.collect()\n    print_log('train ' + str(i) + ' end')\n\nprint('train elapsed time:', time.time() - startTime)","46820f27":"model.save('efficientnet_twotimes.h5')","d8fff007":"# test_image_data.parquet \ud30c\uc77c \uacbd\ub85c\nTEST_IMG_PATH = ['\/kaggle\/input\/bengaliai-cv19\/test_image_data_0.parquet',\n           '\/kaggle\/input\/bengaliai-cv19\/test_image_data_1.parquet',\n           '\/kaggle\/input\/bengaliai-cv19\/test_image_data_2.parquet',\n           '\/kaggle\/input\/bengaliai-cv19\/test_image_data_3.parquet']","42f30f67":"# train_image_data_x.parquet\uc744 \ubd88\ub7ec\uc640\uc11c numpy \ubc30\uc5f4\ub85c \ub418\uc5b4 \uc788\ub294 \uc774\ubbf8\uc9c0\uc640, parquet \ud30c\uc77c\uc5d0 \uc788\ub294 Image_Id\ub97c \ubc18\ud658\ud55c\ub2e4.\ndef get_img_data_with_img_id(img_path):\n    data = pd.read_parquet(img_path)\n    \n    img_id = np.empty(shape=data.shape[0], dtype='object')\n    img_id = data['image_id']\n    \n    # pandas\ub85c \ub370\uc774\ud130\ub97c \ubd88\ub7ec\uc640 numpy \ubc30\uc5f4\ub85c \ubc14\uafb8\ub294 \uc791\uc5c5\uc774 \ud544\uc694\ud558\ub2e4.\n    # pandas\ub85c \ub370\uc774\ud130\ub97c \uc870\uc791\ud560 \ub54c\uc5d0\ub294 \uc18d\ub3c4\uac00 \ub108\ubb34 \ub290\ub9ac\ub2e4(\ud2b9\ud788 iloc). \ub530\ub77c\uc11c numpy \ubc30\uc5f4\ub85c \ubc14\uafb8\uc5b4 \ubc18\ud658\ud574\uc11c \uc18d\ub3c4 \ud5a5\uc0c1\uc744 \uaf80\ud55c\ub2e4.\n    img_data = data[data.columns[1:]]\n    img_data = img_data.to_numpy(dtype='uint8')\n    img_data = img_data.reshape(-1, RAW_IMG_ROWS, RAW_IMG_COLUMNS)\n    \n    del data\n    gc.collect()\n    \n    return img_data, img_id","3b43bcbd":"# \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130 \ubd88\ub7ec\uc624\uae30 Version 1\ndef load_test_data_v1(test_img_path):\n    img_data, start_id, element_num = get_img_data(test_img_path)\n    indices = np.array(range(start_id, start_id + element_num), dtype='uint32')\n\n    # create test_x\n    img_rows = RAW_IMG_ROWS \/\/ 2\n    img_columns = RAW_IMG_COLUMNS \/\/ 2\n    test_x = np.empty(dtype='uint8', shape=(element_num, img_rows, img_columns))\n    \n    for i in range(test_x.shape[0]): # resize image\n        test_x[i] = cv2.resize(img_data[i], dsize=(img_columns, img_rows)) # width * height\n    \n    del img_data\n    gc.collect()\n\n    # keras CNN \ubaa8\ub378\uc5d0 \ub123\uae30 \uc704\ud55c \ucc28\uc6d0 \ubc0f \ub370\uc774\ud130\ud0c0\uc785 \uc815\ub9ac\n    test_x = test_x.reshape((-1, img_rows, img_columns, 1))\n    test_x = test_x.astype('float32')\n    test_x = test_x \/ 255.0\n\n    return test_x, indices","f6d51b0e":"# \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130 \ubd88\ub7ec\uc624\uae30 Version 2\ndef load_test_data_v2(test_img_path, output_img_size):\n    img_data, img_id = get_img_data_with_img_id(test_img_path)\n    \n    element_num = img_data.shape[0]\n    img_rows    = output_img_size[1]\n    img_columns = output_img_size[0]\n    \n    # create test_x\n    test_x = np.empty(dtype='uint8', shape=(element_num, img_rows, img_columns))\n    for i in range(test_x.shape[0]):\n        img = img_data[i]\n        x1, y1, x2, y2 = get_img_roi(img) # get ROI(region of interest)\n        img = img[y1:y2, x1:x2] # crop image\n        img = cv2.resize(img, dsize=(img_columns, img_rows)) # resize image (width * height)\n        test_x[i] = img\n        \n    # keras CNN \ubaa8\ub378\uc5d0 \ub123\uae30 \uc704\ud55c \ucc28\uc6d0 \ubc0f \ub370\uc774\ud130\ud0c0\uc785 \uc815\ub9ac\n    test_x = test_x.reshape((-1, img_rows, img_columns, 1))\n    test_x = test_x.astype('float32')\n    test_x = test_x \/ 255.0\n        \n    return test_x, img_id","a05a59a0":"def predict_model(y_split=True):\n    # \ub9ac\uc2a4\ud2b8\uac00 \uc811\uadfc \uc18d\ub3c4\uac00 \ube60\ub984(pandas\uc758 dataframe\uc740 \ub108\ubb34 \ub290\ub9bc)\n    predict_result = list()\n    \n    for p in range(4):\n        test_x, img_id = load_test_data_v2(TRAIN_IMG_PATH[p], output_img_size=(IMG_ROWS, IMG_COLUMNS))\n        \n        # \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130 \uc608\uce21\n        predict = model.predict(test_x)\n        if y_split:\n            predict_consonant_diacritic = predict[2].argmax(axis=1)\n            predict_grapheme_root       = predict[0].argmax(axis=1)\n            predict_vowel_diacritic     = predict[1].argmax(axis=1)\n        else:\n            predict_consonant_diacritic = predict[:, 179:].argmax(axis=1)\n            predict_grapheme_root       = predict[:, :168].argmax(axis=1)\n            predict_vowel_diacritic     = predict[:, 168:179].argmax(axis=1)\n\n        # \uc608\uce21\uac12 \uae30\ub85d\n        for i in range(test_x.shape[0]):\n            row_id_prefix = img_id[i] + '_'\n\n            predict_result.append(row_id_prefix + 'consonant_diacritic')\n            predict_result.append(predict_consonant_diacritic[i])\n\n            predict_result.append(row_id_prefix + 'grapheme_root')\n            predict_result.append(predict_grapheme_root[i])\n\n            predict_result.append(row_id_prefix + 'vowel_diacritic')\n            predict_result.append(predict_vowel_diacritic[i])\n            \n        print('predict test image data:', p)\n\n    submission = pd.DataFrame(np.array(predict_result).reshape(-1, 2), columns=['row_id', 'target'])\n    submission.set_index('row_id', inplace=True)\n    \n    return submission","b638f5c3":"# \uc2dc\uac04 \uce21\uc815\nstartTime = time.time()\n\n# \uc608\uce21 \uc2dc\uc791\nsubmission = predict_model(y_split=True)\n\nprint('predict elapsed time:', time.time() - startTime)","efc442ce":"print(submission)\nsubmission.to_csv('submission.csv')","f9ee1972":"test_x, img_id = load_test_data_v2(TEST_IMG_PATH[p], output_img_size=(IMG_ROWS, IMG_COLUMNS))","29d57209":"## ResNet v2 \ubaa8\ub4c8\n![resnetv2layer](https:\/\/t1.daumcdn.net\/cfile\/tistory\/99167C335C47F0E315)","d18b38bd":"# 4. \ubaa8\ub378 \ud559\uc2b5\uc2dc\ud0a4\uae30","e1514e0e":"# 1. \ub370\uc774\ud130 \ubd84\uc11d \ubc0f \uac00\uacf5\n\n### \ub370\uc774\ud130 \ubd84\uc11d\n* class_map.csv \ud30c\uc77c\uc5d0\ub294 grapheme_root, vowel_diacritic, consonant_diacritic \uc815\ubcf4\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\ub2e4.\n* train.csv\uc640 test.csv \ud30c\uc77c\uc5d0\ub294 train_image_data_x.parquet\uc640 test_image_data.parquet \ub370\uc774\ud130\uc5d0 \ub300\ud55c \uba54\ud0c0 \ub370\uc774\ud130\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\ub2e4. grapheme_root, vowel_diacritic, consonant_diacritic\uac00 \uc5b4\ub5bb\uac8c \uc870\ud569\ub418\uc5b4 \uc788\ub294\uc9c0 class_map\uc5d0 \uc788\ub294 label\ub85c \ud45c\ud604\ub418\uc5b4 \uc788\ub2e4. \uadf8\ub9ac\uace0 \ucd5c\uc885\uc73c\ub85c \uc870\ud569\ub41c \uae00\uc790(grapheme)\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\ub2e4.\n* train_image_data_x.parquet\uc640 test_image_data.parquet \ud30c\uc77c\uc5d0\ub294 \ubc45\uace8\uc5b4 \uc190\uae00\uc528 \uc774\ubbf8\uc9c0\uac00 \uae30\ub85d\ub418\uc5b4 \uc788\ub2e4. \uc0ac\uc774\uc988\ub294 (137, 236)\uc774\ub2e4.\n\n### \ubc45\uace8\uc5b4 \uace0\ucc30\n* grapheme_root\uc640 vowel_diacritic \uadf8\ub9ac\uace0 consonant_diacritic\uc774 \uc870\ud569\ub418\uc5b4 \ud558\ub098\uc758 \uc790\uc18c(grapheme)\ub97c \ub9cc\ub4e0\ub2e4.","2d64d479":"# 3-3. \ubaa8\ub378 \ub9cc\ub4e4\uae30 (VGGNet)","0eb81694":"# 3-1. \ubaa8\ub378 \ub9cc\ub4e4\uae30 (ResNet-v1)\n<\ucc38\uc870\ud55c \ubb38\uc11c>\n* ResNet \ub17c\ubb38 (Deep Residual Learning for Image Recognition)\n* https:\/\/www.researchgate.net\/figure\/Proposed-Modified-ResNet-18-architecture-for-Bangla-HCR-In-the-diagram-conv-stands-for_fig1_323063171\n![resnet18forbangla](https:\/\/www.researchgate.net\/profile\/Muhammad_Hasan19\/publication\/323063171\/figure\/fig1\/AS:603178554904576@1520820382219\/Proposed-Modified-ResNet-18-architecture-for-Bangla-HCR-In-the-diagram-conv-stands-for.png)","494fa3bf":"## \ub370\uc774\ud130 \uc804\ucc98\ub9ac \ud568\uc218 Version 2\nVersion 1\uacfc\ub294 \ub2e4\ub974\uac8c \ubc45\uace8\uc5b4 \uc190\uae00\uc528\uc5d0 ROI(Region of Interest)\ub97c \uc801\uc6a9\ud574 \uc190\uae00\uc528 \ubd80\ubd84\ub9cc \ub0a8\uaca8\ub193\uace0 \uc774\ubbf8\uc9c0\ub97c \uc798\ub77c\ub0b8\ub2e4. \uadf8 \ub4a4\uc5d0 \uc77c\uc815\ud55c \ud06c\uae30(\uc608\ub97c \ub4e4\uc5b4 64x64)\ub85c Resize\ub97c \ud574\uc11c \ub370\uc774\ud130\ub97c \uc804\ucc98\ub9ac \ud55c\ub2e4.","7dc24af0":"# 5. \uc608\uce21 \ubc0f \uacb0\uacfc \uc81c\ucd9c","6e241146":"\uc2dc\ud589\ucc29\uc624 (Notebook Exceeded Allowed Compute \ubb38\uc81c \ud574\uacb0)\n1. test\uc758 paraquet\uc548\uc5d0 \ub4e4\uc5b4 \uc788\ub294 \uc815\ubcf4\uac00 \uc5bc\ub9c8 \ub418\uc9c0 \uc54a\uc74c. \ucd1d 12\uac1c.\n1. \uadf8\ub798\uc11c \uadf8\ub0e5 4\uac1c\uc758 \ub370\uc774\ud130 \ud30c\uc77c(test_image_data_x.parquet)\uc744 \ubaa8\ub450 \ud558\ub098\ub85c \ud569\ucce4\uc74c.\n1. \ucee4\ub110\uc740 \uba54\ubaa8\ub9ac \ubb38\uc81c \uc5c6\uc774 \uc798 \ub3cc\uc544\uac14\ub294\ub370, submission \ud560 \ub54c \uba54\ubaa8\ub9ac \ucd08\uacfc\uac00 \ub428.\n1. \uc0dd\uac01\ud574\ubcf4\ub2c8 submission\uc5d0\uc11c test data set\uc758 \uac1c\uc218\ub294 \ubaa8\ub984. \uc544\ub9c8 \uc5c4\uccad \ub9ce\uc744 \uac83\uc784.\n1. \uadf8\ub798\uc11c \uba54\ubaa8\ub9ac\uac00 \ubd80\uc871\ud55c \uac83\uc774\uc5c8\uace0, \uadf8\ub0e5 \ub530\ub85c \ub530\ub85c \ubd88\ub7ec\uc640\uc11c \ubaa8\ub378\ub85c \uc608\uce21\ud568.\n\n\uc2dc\ud589\ucc29\uc624 (Notebook Timeout \ubb38\uc81c \ud574\uacb0)\n1. pandas\uc758 dataframe\uc5d0\uc11c append \ud568\uc218\ub97c \uc4f0\ub294 \uac83\uc740 \uc2dc\uac04 \uc18c\ubaa8\uac00 \ub108\ubb34 \ud06c\ub2e4. \uc57d 5\ub9cc\uac1c\uc758 \ub370\uc774\ud130\ub97c append \ud558\ub294\ub370 \uac70\uc758 15\ubd84\uc774 \uac78\ub838\ub2e4.\n1. \ud558\uc9c0\ub9cc, \ud30c\uc774\uc36c\uc5d0\uc11c \uc81c\uacf5\ud558\ub294 list()\ub97c \uc774\uc6a9\ud574\uc11c \ub370\uc774\ud130\ub97c 1\ucc28\uc6d0\uc73c\ub85c append\ub97c \ud558\uace0, numpy\ub85c 2\ucc28\uc6d0 \ubc30\uc5f4\ub85c reshape \ud574\uc900\ub2e4.\n1. \uadf8\ub807\uac8c \ub9cc\ub4e0 numpy array\ub97c \ub2e4\uc2dc pandas\ub85c \ubcc0\ud658\ud574\uc11c csv \ud30c\uc77c\uc744 \ucd94\ucd9c\ud558\uba74 \ud6e8\uc2e0 \ube60\ub974\ub2e4. \uc57d 5\ub9cc\uac1c\uc758 \ub370\uc774\ud130\ub97c \ucc98\ub9ac\ud558\ub294\ub370 40\ucd08 \uac78\ub9bc. 23\ubc30 \uc2dc\uac04 \uc808\uc57d.","4bd56584":"# 3-2. \ubaa8\ub378 \ub9cc\ub4e4\uae30 (ResNet-v2)\n<\ucc38\uc870\ud55c \ubb38\uc11c>\n* ResNet \ub17c\ubb38 (Deep Residual Learning for Image Recognition)\n* https:\/\/eremo2002.tistory.com\/76\n* https:\/\/keras.io\/examples\/cifar10_resnet\/\n![resnetunit](https:\/\/t1.daumcdn.net\/cfile\/tistory\/99F0453F5C47F17413)","b8fb6218":"## \ub370\uc774\ud130 \uc804\ucc98\ub9ac \ud568\uc218 Version 1\n\uac1c\ubcc4 \ud559\uc2b5 \uc774\ubbf8\uc9c0 \ub370\uc774\ud130(train_image_data_x.parquet)\ub97c pandas\ub85c \ubd88\ub7ec\uc640 <U>\uac00\ub85c, \uc138\ub85c \ud06c\uae30\ub97c \uc808\ubc18\uc73c\ub85c \uc904\uc778\ub2e4<\/U>. \uc989, \uc774\ubbf8\uc9c0 \ud06c\uae30\ub294 1\/4\ubc30\uac00 \ub41c\ub2e4. \uc774\ub807\uac8c \ud558\ub294 \uc774\uc720\ub294 \uba54\ubaa8\ub9ac \ubb38\uc81c \ub54c\ubb38\uc778\ub370, keras\uc5d0 \ud559\uc2b5 \ub370\uc774\ud130\ub97c \ub123\uae30 \uc704\ud574 float32 \ud615\ud0dc\uc758 \ub370\uc774\ud130 \ud0c0\uc785\uc744 \uac00\uc838\uc57c \ud55c\ub2e4. train_image_data_x.parquet\uc744 \ubd88\ub7ec\uc624\uace0 uint8 \ub370\uc774\ud130\ud615 numpy \ubc30\uc5f4\uc744 \ub9cc\ub4e4 \ub54c\uc5d0 3GB \uc815\ub3c4 \uc0ac\uc6a9\ud558\ub294\ub370, float32 \ub370\uc774\ud130\ud615\uc73c\ub85c \ubc14\uafb8\uba74 \ud55c \uac1c\uc758 train_image_data_x.parquet \ub9c8\ub2e4 12GB\ub97c \uc18c\ubaa8\ud558\ub294 \ubb38\uc81c\uac00 \uc0dd\uae34\ub2e4. \uadf8\ub798\uc11c \uc774\ubbf8\uc9c0 \ud06c\uae30 \uc790\uccb4\ub97c \uc904\uc5ec \ub370\uc774\ud130\ub97c \uc804\ucc98\ub9ac\ud558\uae30\ub85c \ud588\ub2e4.","7711887c":"# 3-4. \ubaa8\ub378 \ub9cc\ub4e4\uae30 (EfficientNet)","9d281cce":"# 2. \ub370\uc774\ud130 \uc2dc\uac01\ud654","133d90de":"# 0. \ubc45\uace8\uc5b4 \uc190\uae00\uc528 \uc778\uc2dd"}}