{"cell_type":{"ab1323b4":"code","7adbab46":"code","791db2fc":"code","f752825b":"code","83db6fe9":"code","b5346ccd":"code","b104ceb8":"code","5bce68b2":"code","2cb3510b":"code","5d315229":"code","7f7520f4":"code","dd1c3660":"code","02f45f05":"code","82338892":"code","c92e7b8b":"code","beb9c7e6":"code","fa0ed283":"code","57bbd454":"code","5e7e5e41":"code","afcc1655":"code","223a92ed":"code","d0dc31f8":"code","dd11826a":"code","422b1188":"code","8f1606a0":"code","1a8957fe":"code","3383b1ac":"code","1646dd5a":"code","c21dd360":"code","51573d28":"code","d637d94c":"code","c78e97d2":"code","4afbcc9e":"code","1d146807":"markdown","4fe1fbf0":"markdown","8434b4de":"markdown","e2c882c0":"markdown","2e9ebf91":"markdown","d2f96b1a":"markdown","1b5500a6":"markdown","444e1821":"markdown","fbc2d74d":"markdown","10d9ce51":"markdown","8b2e1f8e":"markdown","c7cb4fde":"markdown","e485209d":"markdown","a82d9ace":"markdown","6c4b9e4a":"markdown","40b1bd54":"markdown","77e43c89":"markdown","18f6fb7e":"markdown","93cdbf97":"markdown","c81c5a4b":"markdown","b60b4822":"markdown","74fde003":"markdown","01bd34a9":"markdown","fa1f2a58":"markdown","44c42d7b":"markdown","f14f8c28":"markdown","9d6ee833":"markdown","6a776fa1":"markdown","2c30bc89":"markdown"},"source":{"ab1323b4":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats as ss\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics as mtr","7adbab46":"import warnings\nwarnings.filterwarnings('ignore')","791db2fc":"df = pd.read_csv('..\/input\/credit-card-customers\/BankChurners.csv')\nprint(f\"Data has {df.shape[0]} rows and {df.shape[1]} columns\")\n\ndf.head(5)","f752825b":"# Lets drop 2 unnessecery columns, or probably very useful,who know!!\ndf = df.drop(['Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1',\n'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2'],axis=1)","83db6fe9":"# Lets get info on columns\ndf.info()","b5346ccd":"print(\"Nubmber of Duplicate Rows: \",df.duplicated().sum())","b104ceb8":"print(f\"How pure is primary key 'CLIENTNUM': {len(df['CLIENTNUM'].unique())\/len(df) * 100}%\")","5bce68b2":"# Function to get Descriptive Analysis of Numeric columns\ndef Numeric_Analysis(x):\n    print('='*40)\n    print(f'Descriptive Statistics of {x.name}')\n    print('='*40)\n    print(x.describe())\n    print('='*40)\n    print(f\"Probability Density Plot for {x.name}\")\n    print('='*40)\n    ax,fig = plt.subplots(figsize=(8,4))\n    fig = sns.kdeplot(x.values,shade=True)\n    fig = plt.xlabel(x.name)\n    plt.show()\n\n# Function to get Descriptive Analysis of Categorical columns\ndef Categorical_Analysis(x):\n    print('='*40)\n    print(f'One-Way Frequency Table of {x.name}')\n    print('='*40)\n    desc = pd.DataFrame(x.value_counts())\n    desc.columns = ['Frequency']\n    desc['Percentage'] = np.round((x.value_counts()\/len(df) * 100).values,3)\n    print(desc)\n    print('='*40)\n    fig,ax = plt.subplots(figsize=(8,6))\n    print(f'One-Way Frequency Plot of {x.name}')\n    print('='*40)\n    fig = sns.barplot(x=desc.index,y=desc['Percentage'].values)\n    fig.plot()\n    fig = plt.ylabel('Percentage')\n    fig = plt.xlabel(x.name)\n    plt.show()","2cb3510b":"for col, dtyp in dict(df.drop(['CLIENTNUM'], axis=1).dtypes).items():\n    if dtyp.str == '|O':\n        Categorical_Analysis(df[col])\n    else:\n        Numeric_Analysis(df[col])\n    print(\"X--------------X\"*6,)","5d315229":"num_cols = [col for col in df.drop(['CLIENTNUM','Attrition_Flag'], axis=1).columns if df[col].dtype.str != '|O']\ncat_cols = [col for col in df.drop(['CLIENTNUM','Attrition_Flag'], axis=1).columns if df[col].dtype.str == '|O']","7f7520f4":"fig, ax = plt.subplots(figsize=(12,12))\nfig = sns.heatmap(df[num_cols].corr(), fmt='.1', cmap='Reds', annot=True)\nfig = plt.xticks(rotation=70)","dd1c3660":"# Getting Cramer's V for Categorical Correlation\ndef cramers_v(x, y):\n    confusion_matrix = pd.crosstab(x,y)\n    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n    n = confusion_matrix.sum().sum()\n    phi2 = chi2\/n\n    r,k = confusion_matrix.shape\n    phi2corr = max(0, phi2-((k-1)*(r-1))\/(n-1))\n    rcorr = r-((r-1)**2)\/(n-1)\n    kcorr = k-((k-1)**2)\/(n-1)\n    return np.sqrt(phi2corr\/min((kcorr-1),(rcorr-1)))","02f45f05":"n = len(cat_cols)\ncat_corr = np.zeros([n,n])\nfor i in range(n):\n    for j in range(n):\n        cat_corr[i,j] = cramers_v(df[cat_cols[i]],df[cat_cols[j]])\n\ncat_corr = pd.DataFrame(cat_corr, index=cat_cols, columns=cat_cols)","82338892":"fig, ax = plt.subplots(figsize=(8,8))\nfig = sns.heatmap(cat_corr, fmt='.2', cmap='Reds', annot=True)\nfig = plt.yticks(rotation=0)","c92e7b8b":"pd.crosstab(df['Gender'],df['Income_Category'], margins='row')","beb9c7e6":"# Combine the Customer_Age and Months_on_book to get a new column so we can drop one of them and avoid correlation\nPer_of_life_as_cust = df['Months_on_book']\/(df['Customer_Age']*12) * 100\ndf['Per_of_life_as_cust'] = Per_of_life_as_cust\ndf[['Customer_Age','Per_of_life_as_cust']].corr()","fa0ed283":"# Average Transaction amount gives us a better feature and also avoids correlation\nAvg_Trans_Amt = df['Total_Trans_Amt']\/df['Total_Trans_Ct']\ndf['Avg_Trans_Amt'] = Avg_Trans_Amt\ndf[['Avg_Trans_Amt','Total_Trans_Ct']].corr()","57bbd454":"df = df.drop(['Total_Trans_Amt','Months_on_book','Avg_Open_To_Buy','Card_Category'], axis=1)","5e7e5e41":"X = df.drop(['CLIENTNUM','Attrition_Flag'], axis=1).copy()\ny = (df['Attrition_Flag'].copy() == 'Attrited Customer')*1\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=55, test_size=0.3)","afcc1655":"to_standardize = ['Customer_Age','Credit_Limit','Total_Revolving_Bal','Total_Amt_Chng_Q4_Q1','Total_Trans_Ct',\n                  'Total_Ct_Chng_Q4_Q1','Avg_Utilization_Ratio','Per_of_life_as_cust','Avg_Trans_Amt']\nto_woe = [col for col in X_train.columns if col not in to_standardize]","223a92ed":"ss = StandardScaler()\nss.fit(X_train[to_standardize])\nX_train[to_standardize] = ss.transform(X_train[to_standardize])\nX_test[to_standardize] = ss.transform(X_test[to_standardize])","d0dc31f8":"target_flag = y_train == 1\nN = len(y_train)\ndef WOE_Calculator(x):\n    rows = list()\n    #print(x.name)\n    for attr in list(x.unique()):\n        #print(attr)\n        x_at = x[x == attr]\n        n = len(x_at)\n        good_per = (len(x_at[~target_flag])+0.5)\/N * 100\n        bad_per = (len(x_at[target_flag])+0.5)\/N * 100\n        woe = np.log(good_per\/bad_per)\n        iv = ((good_per - bad_per)\/100)*woe\n        rows.append([x.name, attr, n, good_per, bad_per, woe, iv])\n    return pd.DataFrame(rows, columns=['Feature', 'Attribute', 'Count', 'Good%', 'Bad%', 'WOE', 'IV'])","dd11826a":"df_iv = pd.DataFrame(columns=['Feature', 'Attribute', 'Count', 'Good%', 'Bad%', 'WOE', 'IV'])\nfor col in X_train[to_woe].columns:\n    df_iv = pd.concat([WOE_Calculator(X_train[col]), df_iv])","422b1188":"df_iv.sort_values(by='WOE').head(10)","8f1606a0":"df_iv.groupby('Feature')['IV'].sum()","1a8957fe":"for col in X_train[to_woe].columns:\n    woe_dict = dict(df_iv[df_iv['Feature'] == col][['Attribute','WOE']].set_index('Attribute')['WOE'])\n    X_train[col] = X_train[col].apply(lambda x : woe_dict[x])\n    X_test[col] = X_test[col].apply(lambda x : woe_dict[x])","3383b1ac":"clf = LogisticRegression(solver='lbfgs')\nclf.fit(X_train, y_train)","1646dd5a":"y_pred = clf.predict(X_test)\nprint(mtr.classification_report(y_test, y_pred))\nprint('AUC of  ROC is : ',mtr.roc_auc_score(y_test, y_pred))\nfig = sns.heatmap(mtr.confusion_matrix(y_test, y_pred), fmt='', cmap='Blues', annot=True)","c21dd360":"def gains_table(y_true, y_pred):\n    y = pd.DataFrame(np.c_[y_true,y_pred[:,1]],columns=['y_true','y_pred']).sort_values(by='y_pred')\n    y_true = y['y_true']\n    y_pred = y['y_pred']\n    n = len(y_true)\n    y_pred = pd.Series(y_pred*100).sort_values(ascending=False)\n    bins = [i for i in range(0,n-int(n\/10),int(n\/10))] + [n]\n    rows = []\n    cum_good = 0\n    cum_bad = 0\n    good_n = (y_true == 0).sum()\n    bad_n = (y_true == 1).sum()\n    for i in range(0,10):\n        x = y_pred[bins[i]:bins[i+1]]\n        max_prob = np.max(x)\n        min_prob = np.min(x)\n        mean_prob = np.mean(x)\n        count = len(x)\n        good = len(x[y_true == 0])\n        bad = len(x[y_true == 1])\n        good_per = np.round(good\/count * 100,2)\n        bad_per = np.round(bad\/count * 100,2)\n        cum_good = cum_good + good\n        cum_bad = cum_bad + bad\n        if bad == 0:\n            fpr = np.inf\n        else:\n            fpr = good\/bad\n        cum_good_per = np.round(cum_good \/ good_n * 100,2)\n        cum_bad_per = np.round(cum_bad \/ bad_n * 100,2)\n        ks = cum_bad_per - cum_good_per\n        rows.append([max_prob, mean_prob, min_prob, count, good, bad, good_per, bad_per,cum_good, cum_bad, fpr,\n                     cum_good_per, cum_bad_per, ks])\n    return pd.DataFrame(rows, columns=['Max Proba', 'Mean Proba', 'Min Proba', 'Count', 'Good', 'Bad', 'Good%', 'Bad%',\n                                       'Cumalative Good', 'Cumalative Bad', 'False Positive Rate', 'Cumalative Good%',\n                                       'Cumalative Bad%', 'KS'])","51573d28":"y_pred_prob = clf.predict_proba(X_test)\ngains_table(y_test.values, y_pred_prob)","d637d94c":"y_pred2 = (y_pred_prob[:,1] >= 0.11)*1","c78e97d2":"print(mtr.classification_report(y_test, y_pred2))\nprint('AUC of  ROC is : ',mtr.roc_auc_score(y_test, y_pred2))\nfig = sns.heatmap(mtr.confusion_matrix(y_test, y_pred2), fmt='', cmap='Blues', annot=True)","4afbcc9e":"model_coef = pd.Series(dict(zip(list(X_train.columns),list(clf.coef_[0])))).sort_values()\nfig,ax = plt.subplots(figsize=(6,6))\nfig = sns.barplot(x=model_coef.values, y=model_coef.index)","1d146807":"<h3>Insights<\/h3>","4fe1fbf0":"<b>We can replace the categorical features with there respective WOE, rather than using dummy variables. This helps us avoid increasing dimensionality of our data<\/b>","8434b4de":"## Data Loading","e2c882c0":"### What is Gains Table","2e9ebf91":"# Credit Card Customer Churn Analysis","d2f96b1a":"## Insigts of Data From Descriptive Analysis","1b5500a6":"![image.png](attachment:image.png)","444e1821":"#### This data seems heavely modified and engineered already, we must carefully avoid the trap of multicollinearity","fbc2d74d":"### What is IV?","10d9ce51":"### What is Weight of Evidence?","8b2e1f8e":"So lets put on our analytics hats and help out our friend here, below are the steps and we will try to find a viable solution to this guys problem in the smallest amount of time possible, dont wanna spend entire day here do we.","c7cb4fde":"![image.png](attachment:image.png)","e485209d":"<b>Therefore a positive WOE mean more \"Good Customers\" than \"Bad Customers\", and vice versa for negative WOE. Absolute value of WOE tells us the seperation of two.<\/b>","a82d9ace":"No major correlation, other than <b>\"Income_Category -vs- Gender\"<\/b>,although majority of card holders are females, there income category is mostly on lower end, or unknonw. We will have to find a way to combine the two columns, to avoid unstable models.","6c4b9e4a":"<b>IV stands for Information Value, it is useful in determining the predictive power of a feature based on there class seperation, using WOE<\/b>","40b1bd54":"Hello There!! Been a long time since I uploaded anything, might have some rough edges, apologies for that, do point out any issues in comments.","77e43c89":"## Univariant Analysis","18f6fb7e":"![image.png](attachment:image.png)","93cdbf97":"<ul>\n    <li><b>CLIENTNUM: <\/b>Primary Key, No duplicates Not of much use as of now<\/li>\n    <li><b>Attrition_Flag: <\/b>Target Feature, Categorical, Very unbalanced distribution of data, will see if we can use some kind of sampling technique to improve<\/li>\n    <li><b>Customer_Age: <\/b>Discret Numerical Feature, I was expecting a skewd distribution, as older population are more prone to avoid using credit cards then the younger one.<\/li>\n    <li><b>Gender: <\/b>Not being sexist, but its a little weird that there are more 'Female' Card holders than 'Men'.<\/li>\n    <li><b>Dependent_count: <\/b>Discret Numerical Feature, an average family of 4 or less seems to be the case.<\/li>\n    <li><b>Education: <\/b>Categorical Feature, gradutes seems to be most represented category, we might be able to combine a few categories into one based on there bad rate.<\/li>\n    <li><b>Marital_Status: <\/b>Married people seems to be most represented here, it is possible that one customer might have more than one card in family, and drop a few to reduce liability<\/li>\n    <li><b>Income_Category :<\/b>It is already binned, which causes some loss of info, and also I expected normal distribution, but it seems to be linear descending distribution as income increases.<\/li>\n    <li><b>Card_Category :<\/b>Blue is overally dominant here, and others are way to small to add any useful info, we might drop this<\/li>\n    <li><b>Months_on_book :<\/b>Normally distributed as expected, except for that weird peak, which might suggest tampering with original data, probably someone replace missing values with mean of data, causing such peak at mean, this is not good.<\/li>\n    <li><b>Total_Relationship_Count :<\/b>Number of products owned by customer, we will see how it relates with attrition.<\/li>\n    <li><b>Months_Inactive_12_mon :<\/b>It seems Inactivie users mostly comeback after 3 months max or probably drop out, a good hypothisis to check.<\/li>\n    <li><b>Contacts_Count_12_mon :<\/b>Very similar to last column, as most people comeback by 3 months, most contact is done during that period to bring user back to spending, its possible that users who do not respond even after 3 months are more probable to drop off permanently<\/li>\n    <li><b>Credit_Limit :<\/b>Has a weird bump at the end of the tail, worth checking further.<\/li>\n    <li><b>Total_Revolving_Bal :<\/b> Total Revolving balance means, how much balance is left over after each last payment made by customer, or pending debt, seems most people payoff there debt or have none, but a large portion seems to carry a huge amount.<\/li>\n    <li><b>Avg_Open_To_Buy :<\/b>Average open credit line over last 12 months, distribution very similar to credit_limit, might be correlated and thus redundunt<\/li>\n    <li><b>Total_Amt_Chng_Q4_Q1 :<\/b>Not quite sure what it means, perhaps the percentage change in last 1 year in overall balance.<\/li>\n    <li><b>Total_Trans_Amt :<\/b>Very uneven distribution,  perhaps will work better if we just bin it.<\/li>\n    <li><b>Total_Trans_Ct :<\/b>A double hump  camel, this tells us there are high frequency users and low frequency users in our data, usually its the low frequency users who sticks with a bank longer, as they have less issues with there cards.<\/li>\n    <li><b>Total_Ct_Chng_Q4_Q1 :<\/b>Not quite sure what this is, lets assume its point change in total transaction count<\/li>\n    <li><b>Avg_Utilization_Ratio :<\/b>Card Utilization Ratio is debt\/credit_limit at any given time,I am asummin the average is over 12 months, which would simply be<b> (Credit_Limit - Avg_Open_To_Buy)\/Credit_Limit)<\/b> seems we already have some engineered columns here<\/li>\n<\/ul>","c81c5a4b":"So this Dataset really drew my attention in last few days, as someone working in Finance, it seemed really intreasting to look into, and man did I enjoy working on it...\n\nSo the basic idea is as below.\n\n<I><b>Business manager of a consumer credit card portfolio of a bank is facing severe customer attrition problem in the recent months. This is impacting the business. The business manager wants to leverage the power of data analytics to understand the primary reasons of attrition. She also wants to have an ability to understand the customers who are likely to close their accounts with the bank in near future, so that she can focus her efforts, well in advance to retain those customers.<\/b><\/I>","b60b4822":"#### No Null, No Duplicates, No Overlapping keys, this is what analytics heaven must feel like","74fde003":"## Model Development & Validation","01bd34a9":"<h4>\n<ol>\n    <li>Data Loading<\/li>\n    <li>Data Cleaning<\/li>\n    <li>Univariant Analysis<\/li>\n    <li>Multivariant Analysis<\/li>\n    <li>Feature Engineering<\/li>\n    <li>Model Development<\/li>\n    <li>Results Analysis<\/li>\n    <li>Conclusion<\/li>\n<\/ol>\n<\/h4>","fa1f2a58":"## Data Cleaning","44c42d7b":"## Multivariant Analysis","f14f8c28":"## Feature Engineering","9d6ee833":"**Gains table are mostly used in scorecard based models to identify model performance across different probability score intervals. Gain is defined as cumalatve bad capture rate at any interval, in simple words,  if we set our threshold their such that any observation with probaility score higher is positive, what percentage of true postive we will be able to capture.**","6a776fa1":"<b>The weight of evidence tells the predictive power of an independent variable in relation to the dependent variable. Since it evolved from credit scoring world, it is generally described as a measure of the separation of good and bad customers. \"Bad Customers\" refers to the customers who left company, and \"Good Customers\" refers to the customers who continued to use credit card.<\/b>","2c30bc89":"<h3>Insights:<\/h3>\n\n<ul>\n    <li><b>Months_on_books-vs-Customer_Age:<\/b> The older the customer, the longer they have been with company, very strongly correlated, either have to drop one or find a way to combine that<\/li>\n    <li><b>Credit_Limit -vs- Average_Open_To_Buy: <\/b> As expected, this is way similar as credit_limit, we can just drop one of the columns.<\/li>\n    <li><b>Total_Trans_Amt -vs- Total_Trans_Ct: <\/b>The more transactions you do, the more amount you generate in debt, no surprise there.<\/li>\n<\/ul>"}}