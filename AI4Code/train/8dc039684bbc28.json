{"cell_type":{"07c2d67b":"code","5e05e95d":"code","4d93bc45":"code","7dd8327a":"code","484880a1":"code","d88f113e":"code","8ad66217":"code","7f9541d3":"code","35a9a36f":"code","719599c0":"code","92f298af":"code","0e2626a0":"code","665a9c66":"code","967fa7f6":"code","01d5b7fb":"code","8c37cc62":"code","7a47b18a":"code","11b0c425":"code","391ffa02":"code","85e483b4":"code","dfbe1c1b":"code","24d30970":"code","7af2f97d":"code","50477b13":"code","df9e440c":"code","40604bde":"code","b44c366f":"code","86ace81b":"code","0439b6de":"code","8a3ebcb6":"code","48ed470e":"code","01f6489a":"code","69b7b3fc":"code","77abf1a5":"code","f11a4e11":"code","88b0c428":"code","b9b1c40c":"code","24ddf5f8":"code","04d5a18b":"code","8b271b4d":"code","c4bf6cb1":"code","b51e618e":"code","a08d4702":"code","74e8deca":"code","1c45bcd5":"code","7fab8b35":"code","547dddba":"code","8658c116":"code","a5e2e31f":"code","be20180c":"code","12434b57":"code","0845e55e":"code","d82f1e97":"code","40369e56":"code","8cbb248f":"code","eee68c33":"code","dea70e59":"code","6d639884":"code","b152b4d3":"code","5c109d45":"code","43fd575d":"code","d2e1ed6e":"code","e311fa44":"code","6db9d163":"code","16e92bea":"code","f2609a45":"code","473cb42d":"code","f58d5f8e":"code","ccb339fe":"code","de0d1cc6":"code","89d60d15":"code","1924071c":"code","7020b30f":"code","d3b6b62e":"code","940e97a1":"code","6a1ffcdf":"code","b5413db0":"code","ed64a962":"code","e4c2ad35":"code","55e5ca76":"code","ec1b6811":"code","33823bf0":"code","269ee926":"code","de38afce":"code","48d6adc8":"code","d739344e":"code","a342ce25":"code","4d6517af":"code","7c5efba7":"code","a6e6d6c7":"code","4c8f03a0":"code","f74ce3a8":"code","aee23399":"code","5a6e92e0":"code","a96da150":"code","aea91e44":"code","27e5bc1c":"code","821c5587":"code","40d3712b":"markdown","05a07d7f":"markdown","415907a8":"markdown","fc4ee8f5":"markdown","eff4a7d1":"markdown","6d6540eb":"markdown","8e588393":"markdown","62937e60":"markdown","0321df7a":"markdown","3284cf3d":"markdown","4c58abc8":"markdown","618e268c":"markdown","a2ac2485":"markdown","5efecb1c":"markdown","534e08ec":"markdown","49c12ce7":"markdown"},"source":{"07c2d67b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5e05e95d":"# To print multiple output in a cell\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = 'all'","4d93bc45":"train=pd.read_csv('..\/input\/analytics-vidhya-janatahack-customer-segmentation\/Train_aBjfeNk.csv')\n\ntest=pd.read_csv('..\/input\/analytics-vidhya-janatahack-customer-segmentation\/Test_LqhgPWU.csv')\n\nsample=pd.read_csv('..\/input\/analytics-vidhya-janatahack-customer-segmentation\/sample_submission_wyi0h0z.csv')","7dd8327a":"train.head()\ntrain.shape\n\nprint('-----------------'*5)\ntest.head()\ntest.shape\n\nprint('------------------'*5)\n\nsample.shape","484880a1":"train.isnull().sum()\ntest.isnull().sum()","d88f113e":"# join test and train data\ntrain['train_or_test']='train'\ntest['train_or_test']='test'\ndf=pd.concat([train,test])","8ad66217":"df.shape","7f9541d3":"import cufflinks  as cf\n\n#We set the all charts as public\ncf.set_config_file(sharing='public',theme='pearl',offline=False)\ncf.go_offline()","35a9a36f":"df.head()\ndf.shape","719599c0":"# Let's see which gender is in the majority.\n\n\nlayout1 = cf.Layout(\n    height=400,\n    width=800\n)\n\ndf['Gender'].value_counts().iplot(kind='bar',layout=layout1.to_plotly_json())","92f298af":"# Count for missing values in the train and test data\n\ndf.isnull().sum().iplot(kind='bar',color='blue')","0e2626a0":"# Age distribution \n\ndf['Age'].iplot(kind='hist',color='red')","665a9c66":"# check for the outliers in the age\ndf['Age'].iplot(kind='box',color='green')","967fa7f6":"train.head()","01d5b7fb":"df['Spending_Score'].value_counts()","8c37cc62":"# let's see which profession have high spending score\n\ndf.groupby(['Profession'])['Spending_Score'].value_counts().iplot(kind='bar',color='#0A0DE5 ')","7a47b18a":"plt.subplots(figsize=(8,8))\ndf['Ever_Married'].value_counts().plot(kind='pie')","11b0c425":"plt.subplots(figsize=(8,8))\ndf['Var_1'].value_counts().plot(kind='pie',wedgeprops=dict(width=0.45), startangle=-40)","391ffa02":"df.head()","85e483b4":"# df['Living_with_parents']\n\ndf['Married_Since']=df['Work_Experience']-1\n\n# df['Married_Since'].value_counts()\n# df['Married_Since'].unique()\n\n# df['Married_Since']","dfbe1c1b":"# created this feature but didn't see any improvement in accuracy.\n\n# df['Living_with_parents']=df['Family_Size']-2\n# df['Living_with_parents']=df['Living_with_parents'].apply(lambda x:'yes' if x>0 else 'No' )","24d30970":"df.head()","7af2f97d":"\ndef id_features(data):\n    df = data.copy()\n    df['week'] = df['ID']%7\n    df['month'] = df['ID']%30\n    df['year'] = df['ID']%365\n    df['num_weeks'] = df['ID']\/\/7\n    df['num_year'] = df['ID']\/\/365\n    df['num_quarter'] = df['ID']\/\/90\n    df['quarter'] = df['ID']%90\n    df['num_days'] = df['ID'].values - 458982\n    df['num_weeks_2'] = (df['ID'].values - 458982)\/\/7\n    df['num_months_2'] = (df['ID'].values - 458982)\/\/30\n\n    return df","50477b13":"df=id_features(df)\ndf.head()","df9e440c":"Repeated_ids=df['ID'].value_counts().sort_values(ascending=False)","40604bde":"c=Repeated_ids.index\nd=Repeated_ids.values","b44c366f":"Repeated_ids.values","86ace81b":"dataframe=pd.DataFrame({'ID':c, 'count':d})","0439b6de":"new_data=dataframe[dataframe['count']==2]","8a3ebcb6":"len(new_data['ID'].values)","48ed470e":"rows=new_data['ID'].values\nrows","01f6489a":"df.loc[df['ID'].isin(rows)]","69b7b3fc":"df[df['ID']==460661]","77abf1a5":"df[df['ID']==467958]","f11a4e11":"df[df['ID']==467856]","88b0c428":"# repeated Id in the train data . we replace the segmentation of train with the test\n\ndf_train=train[train['ID'].isin(new_data['ID'].values)]\ndf_train.shape","b9b1c40c":"# df_train\ndf_test=test[test['ID'].isin(new_data['ID'].values)]\ndf_test.shape\nlen(df_test['ID'].unique())","24ddf5f8":"# do the cross check again\nvalue_list=[]\nfor i in df_train['ID'].values:\n    if i in df_test['ID'].values:\n        value_list.append(i)","04d5a18b":"len(value_list)","8b271b4d":"len(df_train['ID'].unique())","c4bf6cb1":"df.isnull().sum()","b51e618e":"# df.corr().iplot(kind='heatmap',colorscale='ylgn')","a08d4702":"# list_d=df['ID'].value_counts().sort_values(ascending=False).values==2","74e8deca":"# value_2=[]\n# for i in list_d:\n#     if i==True:\n#         value_2.append(i)","1c45bcd5":"# len(value_2)","7fab8b35":"df.info()","547dddba":"df.isnull().sum()","8658c116":"df['Ever_Married'].value_counts()","a5e2e31f":"df['Ever_Married'].mode()","be20180c":"# df['Ever_Married']=df['Ever_Married'].fillna(df['Ever_Married'].mode()[0])\n\ndf['Ever_Married']=df['Ever_Married'].fillna('About_TO')","12434b57":"# treating Graduated\n\n# df['Graduated']=df['Graduated'].fillna(df['Graduated'].mode()[0])\n\ndf['Graduated']=df['Graduated'].fillna(\"InMid\")","0845e55e":"# df['Profession']=df['Profession'].fillna(df['Profession'].mode()[0])\n\ndf['Profession']=df['Profession'].fillna('Other')\n# df['Profession'].value_counts()","d82f1e97":"# df['Work_Experience']=df['Work_Experience'].fillna(df['Work_Experience'].mean())\n\ndf['Work_Experience']=df['Work_Experience'].fillna(-999)\n\n# df['Work_Experience'].value_counts()","40369e56":"# df['Family_Size']=df['Family_Size'].fillna(df['Family_Size'].mean())\n\ndf['Family_Size']=df['Family_Size'].fillna(-999)","8cbb248f":"# df['Var_1']=df['Var_1'].fillna(df['Var_1'].mode()[0])\n\ndf['Var_1']=df['Var_1'].fillna('New_char')\n\n# df['Var_1'].value_counts()","eee68c33":"df.isnull().sum()","dea70e59":"df.head()","6d639884":"# Encoding the variable.\n\nfrom sklearn.preprocessing import LabelEncoder\n\nencoder=LabelEncoder()","b152b4d3":"# cat_col=['Gender', 'Ever_Married', 'Graduated', 'Profession', 'Spending_Score',\n#        'Var_1','Living_with_parents']\n\n\ncat_col=['Gender', 'Ever_Married', 'Graduated', 'Profession', 'Spending_Score',\n       'Var_1']","5c109d45":"df.isnull().sum()","43fd575d":"for col in cat_col:\n    df[col] = encoder.fit_transform(df[col])","d2e1ed6e":"df.dtypes\ndf.head(2)","e311fa44":"df.dtypes","6db9d163":"# seperate train and test data\n\n# Seperate the train and test data before training.\n\ntrain_n=df.loc[df.train_or_test.isin(['train'])]\ntest_n=df.loc[df.train_or_test.isin(['test'])]\ntrain_n.drop(columns={'train_or_test'},axis=1,inplace=True)\ntest_n.drop(columns={'train_or_test'},axis=1,inplace=True)","16e92bea":"train_n.head()\ntest_n.head()","f2609a45":"train_n.drop('ID',axis=1,inplace=True)\n\ntest_n.drop(['ID','Segmentation'],axis=1,inplace=True)\n","473cb42d":"train_n.shape\ntest_n.shape","f58d5f8e":"train_n['Segmentation'].unique()","ccb339fe":"train_n['Segmentation']=train_n['Segmentation'].map({'A':0,'B':1,'C':2,'D':3})","de0d1cc6":"# Model training importations\n\nimport lightgbm as lgb\n\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n\nfrom sklearn.metrics import mean_squared_log_error, mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import classification_report\nimport lightgbm as lgb\n\n\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import f1_score\nfrom imblearn.over_sampling import SMOTE\nfrom catboost import CatBoostClassifier\n\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance","89d60d15":"train_n.head(2)\ntest_n.head(3)","1924071c":"# X=train_n.drop('Segmentation',axis=1)\n# y=train_n['Segmentation']\n# X_test=test_n\nlabel_col='Segmentation'","7020b30f":"df_train, df_eval = train_test_split(train_n, test_size=0.30, random_state=42, shuffle=True, stratify=train_n[label_col])","d3b6b62e":"feature_cols = train_n.columns.tolist()\nfeature_cols.remove('Segmentation')","940e97a1":"# train['Segmentation'].unique()\n# train['Segmentation'].value_counts()","6a1ffcdf":"params = {}\nparams['learning_rate'] = 0.15\nparams['max_depth'] = 10\nparams['n_estimators'] = 10000\nparams['objective'] = 'multiclass'\nparams['boosting_type'] = 'gbdt'\nparams['subsample'] = 0.7\nparams['random_state'] = 42\nparams['colsample_bytree']=0.7\nparams['min_data_in_leaf'] = 25\nparams['reg_alpha'] = 1.7\nparams['reg_lambda'] = 1.11","b5413db0":"params","ed64a962":"clf = lgb.LGBMClassifier(**params)","e4c2ad35":"clf.fit(df_train[feature_cols], df_train[label_col], early_stopping_rounds=1000, eval_set=[(df_train[feature_cols], df_train[label_col]), (df_eval[feature_cols], df_eval[label_col])], eval_metric='multi_error', verbose=True, categorical_feature=cat_col)","55e5ca76":"eval_score = accuracy_score(df_eval[label_col], clf.predict(df_eval[feature_cols]))\n\nprint('Eval ACC: {}'.format(eval_score))","ec1b6811":"best_iter = clf.best_iteration_\nparams['n_estimators'] = best_iter\nprint(params)","33823bf0":"df_train = pd.concat((df_train, df_eval))","269ee926":"clf = lgb.LGBMClassifier(**params)\n\nclf.fit(df_train[feature_cols], df_train[label_col], eval_metric='multi_error', verbose=False, categorical_feature=cat_col)\n\n# eval_score_auc = roc_auc_score(df_train[label_col], clf.predict(df_train[feature_cols]))\neval_score_acc = accuracy_score(df_train[label_col], clf.predict(df_train[feature_cols]))\n\nprint('ACC: {}'.format(eval_score_acc))","de38afce":"preds = clf.predict(test_n[feature_cols])","48d6adc8":"submission = pd.DataFrame({'ID':test['ID'], 'Segmentation':preds})","d739344e":"## Needs to reverse the Segmentation.\n\n\nsubmission['Segmentation']=submission['Segmentation'].map({0:'A',1:'B',2:'C',3:'D'})","a342ce25":"plt.rcParams['figure.figsize'] = (12, 6)\nlgb.plot_importance(clf)\nplt.show()","4d6517af":"df_train.head()\ndf_train.shape","7c5efba7":"submission.head()","a6e6d6c7":"# df_train[df_train['ID']==459001]['Segmentation'].values","4c8f03a0":"# ne=[]\n\n# for i in df_train['ID'].values:\n#     if i in submission['ID'].values:\n#         ne.append(i)\n#         submission[submission['ID']==i]['Segmentation']=df_train[df_train['ID']==i]['Segmentation'].values\n","f74ce3a8":"# df_train.to_csv('df_train.csv',index=False,encoding='utf-8')","aee23399":"len(ne)\n# ne","5a6e92e0":"# submission[submission['ID']==459001]","a96da150":"# df_train.head()","aea91e44":"submission.to_csv('submission22.csv',index=False)","27e5bc1c":"submission['Segmentation'].value_counts()","821c5587":"submission.head()","40d3712b":"- Most number of people belongs to the category of ```Cat_6```.","05a07d7f":"- We can see there is a repeation of IDs.\n- Which are supposed to be unique for the data.","415907a8":"| <font color='blue'><h2>Column Name<\/h2><\/font>                       | <font color='blue'><h2>Description<\/h2><\/font>                                                                                        |\n| ------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------:|\n| Id                       | UniqueID                                                                                                   | \n| Gender                          | \tGender of the customer                                                                  | \n| Ever_Married                            | Marital status of the customer                                                                                    | \n| Age                              | Age of the customer                                                                                 | \n| Graduated                               | Is the customer a graduate?                                                                                   |\n| Profession                               | Profession of the customer                                                                                   |\n|Work_Experience                             |  Work Experience in years                   |\n| Spending_Score                             | Spending score of the customer                                          |\n| Family_Size                             | Number of family members for the customer (including the customer)\n                                         |\n|Var_1                              |   Anonymised Category for the customer        |\n| <b><u>Segmentation<\/u><\/b>         |\t<b><u>(target) Customer Segment of the customer<\/u><\/b>                                                 |","fc4ee8f5":"- This is the very basic version so here i am simply filling all these values with the very naive approach.\n\n- But later on we will imputer all these values with a specific imputation strategy.","eff4a7d1":"## Problem Statement\n\nAn automobile company has plans to enter new markets with their existing products (P1, P2, P3, P4 and P5). After intensive market research, they\u2019ve deduced that the behavior of new market is similar to their existing market. \n\nIn their existing market, the sales team has classified all customers into 4 segments (A, B, C, D ). Then, they performed segmented outreach and communication for different segment of customers. This strategy has work exceptionally well for them. They plan to use the same strategy on new markets and have identified 2627 new potential customers. \n\n- ```You are required to help the manager to predict the right group of the new customers```.","6d6540eb":"- For me this information seems like a great one & let's see how you and me utilize this to create predictions. ","8e588393":"## Imputing missing values","62937e60":"## Data Description","0321df7a":"## Data Visualization","3284cf3d":"## Let's Begin\n\nHi, My name is ```vinay vikram```, \n\nSince after this COVID-19 pendemic i started participating in Analytics Vidhya Hackathon and trust me i am loving them. \n\nFrom last few hackathon i got in to this idea of making my notebook public since from beginning without runing the competitive sprit.\n\n        - Here i do share my approach theoritically without opensourcing my code.\n        - So let's come together and work together with mutual knowledge sharing.\n        - If you have better ideas share yours and if you have any suggestion regarding my work share that too.\n        \n \n## Kindly do Upvote & Comment\n        \n\n> In this notebook i provide you some hints if you implement them in your notebook,Surely gonna get better results.(because i already implemented and bla.. bla... ....).\n> \n> So keep your eye on given hints.","4c58abc8":"<img src='https:\/\/datahack-prod.s3.ap-south-1.amazonaws.com\/__sized__\/contest_cover\/Customer_Segmentation-thumbnail-1200x1200-90.jpg' height=600 width=750\/>","618e268c":"- After seeing all the IDs i think there is something hidden pattern in the IDs but for now i will drop them.\n- Further i will see what i can do with it.\n","a2ac2485":"# <center>JanataHack - Customer Segmentation<\/center>","5efecb1c":"Here as you can see train and test data have only around 10,000 rows.\n\n- So need to be cautious while applying high end Gradient boosting algorithm.","534e08ec":"--------------------------start_here-------------------------------------","49c12ce7":"- From the plot seems like Artist have the highest and lowest spending score."}}