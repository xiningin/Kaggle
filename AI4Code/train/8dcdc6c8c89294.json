{"cell_type":{"917b4e17":"code","26fda90b":"code","e2913ecc":"code","7531ca46":"code","9e5f1c52":"code","28cf7635":"code","b8fa1674":"code","fc69886d":"code","be0ebb19":"code","ca7da999":"code","6681aaf0":"code","6d05db4d":"code","da6053b7":"code","bcdeb6fa":"code","c1ec8b56":"code","bbd630e0":"code","209e9b15":"code","42cd3211":"code","9c14ffb8":"code","efaecf5f":"code","0badd196":"code","a4b37e63":"code","1d948ab9":"markdown"},"source":{"917b4e17":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nfrom matplotlib import rcParams\nrcParams['figure.figsize']=10,6\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.recurrent import LSTM\nfrom keras.models import Sequential\nimport time\nfrom sklearn.metrics import mean_squared_error\nfrom matplotlib import pyplot\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","26fda90b":"dataset=pd.read_csv(\"..\/input\/rainfall in india 1901-2015.csv\",encoding = \"ISO-8859-1\")\ndataset.dtypes","e2913ecc":"groups = dataset.groupby('SUBDIVISION')['YEAR','JAN','FEB','MAR','APR','MAY','JUN','JUL','AUG','SEP','NOV','DEC']\ndata=groups.get_group(('BIHAR'))\ndata.head()","7531ca46":"data=data.melt(['YEAR']).reset_index()\ndata.head()\n","9e5f1c52":"df= data[['YEAR','variable','value']].reset_index().sort_values(by=['YEAR','index'])\ndf.head()","28cf7635":"df.columns=['INDEX','YEAR','Month','avg_rainfall']","b8fa1674":"df.head()","fc69886d":"d={'JAN':1,'FEB':2,'MAR' :3,'APR':4,'MAY':5,'JUN':6,'JUL':7,'AUG':8,'SEP':9,\n   'OCT':10,'NOV':11,'DEC':12}\ndf['Month']=df['Month'].map(d)\ndf.head(12)","be0ebb19":"df['Date']=pd.to_datetime(df.assign(Day=1).loc[:,['YEAR','Month','Day']])\ndf.head(12)","ca7da999":"cols=['avg_rainfall']\ndataset=df[cols]\ndataset.head()","6681aaf0":"series=dataset\nseries.head()","6d05db4d":"series.shape","da6053b7":"pyplot.figure(figsize=(20,6))\npyplot.plot(series.values)\npyplot.show()","bcdeb6fa":"# Get the raw data values from the pandas data frame.\ndata_raw = series.values.astype(\"float32\")\n\n# We apply the MinMax scaler from sklearn\n# to normalize data in the (0, 1) interval.\nscaler = MinMaxScaler(feature_range = (0, 1))\ndataset = scaler.fit_transform(data_raw)\n\n# Print a few values.\ndataset[0:5]","c1ec8b56":"# Using 60% of data for training, 40% for validation.\nTRAIN_SIZE = 0.80\n\ntrain_size = int(len(dataset) * TRAIN_SIZE)\ntest_size = len(dataset) - train_size\ntrain, test = dataset[0:train_size, :], dataset[train_size:len(dataset), :]\nprint(\"Number of entries (training set, test set): \" + str((len(train), len(test))))","bbd630e0":"# FIXME: This helper function should be rewritten using numpy's shift function. See below.\ndef create_dataset(dataset, window_size = 1):\n    data_X, data_Y = [], []\n    for i in range(len(dataset) - window_size - 1):\n        a = dataset[i:(i + window_size), 0]\n        data_X.append(a)\n        data_Y.append(dataset[i + window_size, 0])\n    return(np.array(data_X), np.array(data_Y))","209e9b15":"# Create test and training sets for one-step-ahead regression.\nwindow_size = 1\ntrain_X, train_Y = create_dataset(train, window_size)\ntest_X, test_Y = create_dataset(test, window_size)\nprint(\"Original training data shape:\")\nprint(train_X.shape)\n\n# Reshape the input data into appropriate form for Keras.\ntrain_X = np.reshape(train_X, (train_X.shape[0], 1, train_X.shape[1]))\ntest_X = np.reshape(test_X, (test_X.shape[0], 1, test_X.shape[1]))\nprint(\"New training data shape:\")\nprint(train_X.shape)","42cd3211":"def fit_model(train_X, train_Y, window_size = 1):\n    model = Sequential()\n    \n    model.add(LSTM(2000,activation = 'tanh', inner_activation = 'hard_sigmoid', input_shape = (1, window_size)))\n    model.add(Dropout(0.2))\n    model.add(Dense(500))\n    model.add(Dropout(0.4))\n    model.add(Dense(500))\n    model.add(Dropout(0.4))\n    model.add(Dense(400))\n    model.add(Dropout(0.4))\n    model.add(Dense(1, activation = 'linear'))\n    model.compile(loss = \"mean_squared_error\", \n                  optimizer = \"adam\")\n    model.fit(train_X, \n              train_Y, \n              epochs = 10, \n              batch_size = 64, \n              )\n    \n    return(model)\n\n# Fit the first model.\nmodel1 = fit_model(train_X, train_Y, window_size)","9c14ffb8":"import math\ndef predict_and_score(model, X, Y):\n    # Make predictions on the original scale of the data.\n    pred = scaler.inverse_transform(model.predict(X))\n    # Prepare Y data to also be on the original scale for interpretability.\n    orig_data = scaler.inverse_transform([Y])\n    # Calculate RMSE.\n    score = math.sqrt(mean_squared_error(orig_data[0], pred[:, 0]))\n    return(score, pred)\n\nrmse_train, train_predict = predict_and_score(model1, train_X, train_Y)\nrmse_test, test_predict = predict_and_score(model1, test_X, test_Y)\n\nprint(\"Training data score: %.2f RMSE\" % rmse_train)\nprint(\"Test data score: %.2f RMSE\" % rmse_test)","efaecf5f":"# Start with training predictions.\ntrain_predict_plot = np.empty_like(dataset)\ntrain_predict_plot[:, :] = np.nan\ntrain_predict_plot[window_size:len(train_predict) + window_size, :] = train_predict\n\n# Add test predictions.\ntest_predict_plot = np.empty_like(dataset)\ntest_predict_plot[:, :] = np.nan\ntest_predict_plot[len(train_predict) + (window_size * 2) + 1:len(dataset) - 1, :] = test_predict\n\n# Create the plot.\nplt.figure(figsize = (18, 8))\nplt.plot(scaler.inverse_transform(dataset), label = \"True value\",color='red')\nplt.plot(train_predict_plot, label = \"Training set prediction\",color='yellow')\nplt.plot(test_predict_plot, label = \"Test set prediction\")\nplt.xlabel(\"Months\")\n\n\nplt.legend()\nplt.show()","0badd196":"test_predict","a4b37e63":"train_predict","1d948ab9":"Indian Government has undertaken many research studies to analyze the impact of global warming and climate change on rainfall pattern in India. The analyses were made using observed rainfall data from more than 3000 rain-gauge stations spread over the country for 115 years (1901-2015). The major inferences from these studies based on the 115 years of rainfall data are as follows:\n\n The analysis of 115 years of monsoon rainfall data suggests that there is no long term change or trend in the monsoon rainfall averaged over the country.\n Even though, there are no changes in the all-India rainfall, there are significant changes in annual rainfall in some meteorological sub-divisions. Rainfall over Kerala, East Madhya Pradesh, Jharkhand, Arunachal Pradesh and Nagaland, Manipur, Mizoram and Tripura (NMMT) show decreasing trends. However, rainfall over coastal Karnataka, Maharashtra and Jammu and Kashmir show an increasing trend.\n![](https:\/\/krishijagran.com\/media\/3122\/weather-forecast-for-the-week-in-india.png)\n\nThere is a general tendency of increasing frequency of extreme rainfall (heavy rainfall events) over India, especially over the central parts of India during the southwest (June- September) monsoon season.\n There is no evidence of global warming on the observed changes in annual or seasonal rainfall over India. However, there is growing evidence suggesting that increasing frequency of extreme rainfall is due to global warming.\n The climate change assessment made by the Intergovernmental Panel on Climate Change (IPCC) suggest that in future, frequency of extreme rainfall may increase over India due to increase in global warming. However,  there are NO other long term changes\/trends in rainfall over India which can be attributed to global warming. The Indian Monsoon is found to be a stable system.\n \n With this data with more variations of average rainfall, it is very difficult for a statistical model to predict the required data point.Here we implement neural networks to predict the avg rainfall, the neural net is used to create multiple features that helps in predicting the data points with more seasonal variations."}}