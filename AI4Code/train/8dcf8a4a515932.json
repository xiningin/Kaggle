{"cell_type":{"4d22353f":"code","e0667588":"code","ff062561":"code","7fa3ac2c":"code","686dce6d":"code","347cd081":"code","2e9f5710":"code","3d481c0a":"code","4a393314":"code","425d38d6":"code","607542d8":"code","808bb4a3":"code","ca053d35":"code","1fcd5de8":"code","eb9d3f1a":"code","7d05c68f":"code","3324f641":"markdown","1ae5f89e":"markdown","c885dd7d":"markdown","e4f79dd5":"markdown","2a49b41c":"markdown"},"source":{"4d22353f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e0667588":"from xgboost import XGBRegressor,XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder","ff062561":"X = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ny = X[\"Survived\"]\nX.drop(\"Survived\",axis=1,inplace=True)","7fa3ac2c":"x_train,x_valid,y_train,y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                      random_state=0)","686dce6d":"cols_with_missing = [col for col in x_train.columns\n                     if x_train[col].isnull().any()]\ncols_with_missing","347cd081":"numeric_col = \"Age\"\ncat_col= \"Embarked\"","2e9f5710":"numerical_transformer = SimpleImputer()\ncategorical_transformer = SimpleImputer(strategy='most_frequent')","3d481c0a":"x_train[\"imputed_\"+numeric_col] = numerical_transformer.fit_transform(x_train[[numeric_col]])\ndf_test[\"imputed_\"+numeric_col] = numerical_transformer.fit_transform(df_test[[numeric_col]])\nx_valid[\"imputed_\"+numeric_col] = numerical_transformer.transform(x_valid[[numeric_col]])\nx_train[\"imputed_\"+cat_col] = categorical_transformer.fit_transform(x_train[[cat_col]])\nx_valid[\"imputed_\"+cat_col] = categorical_transformer.transform(x_valid[[cat_col]])\ndf_test[\"imputed_\"+cat_col] = categorical_transformer.transform(df_test[[cat_col]])","4a393314":"x_train.drop(cols_with_missing,axis=1,inplace=True)\nx_valid.drop(cols_with_missing,axis=1,inplace=True)\ndf_test.drop(cols_with_missing,axis=1,inplace=True)","425d38d6":"OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nx_train[\"Sex\"] = OH_encoder.fit_transform(x_train[[\"Sex\"]])\nx_valid[\"Sex\"] = OH_encoder.fit_transform(x_valid[[\"Sex\"]])\ndf_test[\"Sex\"] = OH_encoder.fit_transform(df_test[[\"Sex\"]])","607542d8":"feature_cols = [\"Pclass\",\"Sex\"]","808bb4a3":"x_train = x_train[feature_cols]\nx_valid = x_valid[feature_cols]\nx_test = df_test[feature_cols]","ca053d35":"titanic_model = XGBClassifier(n_estimators=700,learning_rate=0.05,n_jobs=4)","1fcd5de8":"titanic_model.fit(x_train, y_train, \n             early_stopping_rounds=10, \n             eval_set=[(x_valid, y_valid)])","eb9d3f1a":"pred_test = titanic_model.predict(x_test,ntree_limit=titanic_model.best_ntree_limit)","7d05c68f":"output = pd.DataFrame({'PassengerId': df_test['PassengerId'],\n                       'Survived': pred_test})\noutput.to_csv('submission.csv', index=False)","3324f641":"Find out columns with missing values,these will be imputed later on","1ae5f89e":"Load the test and train data,separate out the target variable from train data","c885dd7d":"Split the training dataset into train and valid","e4f79dd5":"Dropping the column \"Cabin\" as only 163 entries are non null","2a49b41c":"Download necessary packages"}}