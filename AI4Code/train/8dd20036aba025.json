{"cell_type":{"ebf24871":"code","ab54c5e1":"code","a24a8297":"code","15857689":"code","3c218423":"code","29d9ea60":"code","ff5049a5":"code","4a329bc6":"code","5f8ce929":"code","d8f04ac8":"code","d229e578":"code","44313936":"code","a6023816":"code","2bb6254c":"code","2e39763f":"code","3a5b21cf":"code","c8551243":"code","37b09e0d":"code","92b214b8":"code","95ec0527":"code","25d5589b":"code","66d6d8c3":"code","46f6694b":"code","f260cf0d":"code","64bc9efb":"code","2e881e36":"code","cb5e78c4":"code","846ab33e":"code","f5eac55f":"code","edd246df":"code","a4156dc6":"code","7a21178a":"code","d0b19d09":"code","7c7b3e89":"code","b2d18e8d":"code","a4a61e66":"code","6ebcfefa":"code","c4bc9e29":"code","1f572a24":"code","cb673c00":"code","95d04317":"code","cb09c221":"code","b812b16b":"markdown","8a6ec7c0":"markdown","25c93805":"markdown","ce34c468":"markdown","ff9e9461":"markdown","64a5df4e":"markdown","73184e82":"markdown","5b2c9f68":"markdown","115eb28b":"markdown","f072c5d7":"markdown","fc8221e5":"markdown","80232227":"markdown"},"source":{"ebf24871":"!pip install mplfinance\n!pip install bokeh","ab54c5e1":"import math\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt \nimport matplotlib.dates as mdates\nimport mplfinance as fplt\n\nfrom keras.models import Sequential\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import LSTM, Dense, Dropout\nfrom sklearn.preprocessing import RobustScaler\nfrom datetime import date, timedelta, datetime\nfrom pandas.plotting import register_matplotlib_converters\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error","a24a8297":"import matplotlib\nmatplotlib.use('nbagg')\nsns.set(rc={'figure.figsize':(15, 5)})","15857689":"df = pd.read_csv('..\/input\/historical-bitcoin-prices-btc\/BTCUSDT_1h.csv')\ndf","3c218423":"df.isna().sum()","29d9ea60":"df.info()","ff5049a5":"df.info()","4a329bc6":"df.describe()","5f8ce929":"df[\"close_time\"].min(), df[\"close_time\"].max()","d8f04ac8":"# Removing the localization in 'close_time'\ndf['close_time'] = pd.to_datetime(df['close_time'], errors='coerce')\ndf['close_time'] = df['close_time'].dt.tz_localize(None)\ndf","d229e578":"# close_time to Index --> For visualization\ndf_plot = df.copy()\ndf_plot = df_plot.set_index('close_time')\ndf_plot","44313936":"# Using mplfinance to cisualizes the data as candlesticks (somewhat)\n# Note that it will take some time to render, since there are too many records\n# Ignore the warning\n\n%matplotlib widget\n%matplotlib inline\n\nfplt.plot(\n    df_plot,\n    figsize=(20,5) ,\n    type='candle',\n    style='charles',\n    title='BTC - 2017 ~ 2022',\n    ylabel='Price ($)',\n    volume=True,\n    ylabel_lower='Shares\\nTraded',\n    show_nontrading=True)","a6023816":"from math import pi\nfrom bokeh.plotting import figure\nfrom bokeh.io import output_notebook,show\nfrom bokeh.resources import INLINE\n\noutput_notebook(resources=INLINE)\n\ninc = df_plot.close > df_plot.open\ndec = df_plot.open > df_plot.close\n\nw = 12*30*30*100\n\np = figure(x_axis_type=\"datetime\", plot_width=1000, plot_height=500, title = \"BTC, 2017 ~ 2022\")\n\np.segment(df_plot.index, df_plot.high, df_plot.index, df_plot.low, color=\"black\")\n\np.vbar(df_plot.index[inc], w, df_plot.open[inc], df_plot.close[inc], fill_color=\"lawngreen\", line_color=\"red\")\n\np.vbar(df_plot.index[dec], w, df_plot.open[dec], df_plot.close[dec], fill_color=\"tomato\", line_color=\"lime\")\n\nshow(p)","2bb6254c":"def createFeatures(df):\n    df = pd.DataFrame(df)\n        \n    # Moving averages - different periods\n    df['MA200'] = df['close'].rolling(window=200).mean() \n    df['MA100'] = df['close'].rolling(window=100).mean() \n    df['MA50'] = df['close'].rolling(window=50).mean() \n    df['MA26'] = df['close'].rolling(window=26).mean()            \n    df['MA20'] = df['close'].rolling(window=20).mean() \n    df['MA12'] = df['close'].rolling(window=12).mean() \n    \n    # SMA Differences - different periods\n    df['DIFF-MA200-MA50'] = df['MA200'] - df['MA50']\n    df['DIFF-MA200-MA100'] = df['MA200'] - df['MA100']\n    df['DIFF-MA200-CLOSE'] = df['MA200'] - df['close']            \n    df['DIFF-MA100-CLOSE'] = df['MA100'] - df['close']\n    df['DIFF-MA50-CLOSE'] = df['MA50'] - df['close']\n    \n    # Moving Averages on high, lows, and std - different periods\n    df['MA200_low'] = df['low'].rolling(window=200).min()\n    df['MA14_low'] = df['low'].rolling(window=14).min()\n    df['MA200_high'] = df['high'].rolling(window=200).max()       \n    df['MA14_high'] = df['high'].rolling(window=14).max()\n    df['MA20dSTD'] = df['close'].rolling(window=20).std() \n    \n    # Exponential Moving Averages (EMAS) - different periods\n    df['EMA12'] = df['close'].ewm(span=12, adjust=False).mean()\n    df['EMA20'] = df['close'].ewm(span=20, adjust=False).mean()\n    df['EMA26'] = df['close'].ewm(span=26, adjust=False).mean()   \n    df['EMA100'] = df['close'].ewm(span=100, adjust=False).mean()\n    df['EMA200'] = df['close'].ewm(span=200, adjust=False).mean()\n\n    # Shifts (one day before and two days before)\n    df['close_shift-1'] = df.shift(-1)['close']\n    df['close_shift-2'] = df.shift(-2)['close']                   \n\n    # Bollinger Bands\n    df['Bollinger_Upper'] = df['MA20'] + (df['MA20dSTD'] * 2)    \n    df['Bollinger_Lower'] = df['MA20'] - (df['MA20dSTD'] * 2)     \n    \n    \n    df['K-ratio'] = 100*((df['close'] - df['MA14_low']) \/ (df['MA14_high'] - df['MA14_low']) )\n    \n    # Relative Strength Index (RSI)\n    df['RSI'] = df['K-ratio'].rolling(window=3).mean()            \n \n    # Moving Average Convergence\/Divergence (MACD)\n    df['MACD'] = df['EMA12'] - df['EMA26']                        \n    \n    # Replace NA's\n    nareplace = df.at[df.index.max(), 'close']                     \n    df.fillna((nareplace), inplace=True)\n    \n    return df","2e39763f":"# List of considered Features\n\nFEATURES = [\n#             'High',\n#             'Low',\n#             'Open',\n              'close',\n#             'Volume',\n              'close_time',\n#             'Day',\n#             'Month',\n#             'Year',\n#             'Adj Close',\n#             'close_shift-1',\n#             'close_shift-2',\n              'MACD',\n              'RSI',\n#             'MA200',\n#             'MA200_high',\n#             'MA200_low',\n              'Bollinger_Upper',                                      \n              'Bollinger_Lower',\n#             'MA100',            \n#             'MA50',\n#             'MA26',\n#             'MA14_low',\n#             'MA14_high',\n#             'MA12',\n#             'EMA20',\n#             'EMA100',\n#             'EMA200',\n#             'DIFF-MA200-MA50',\n#             'DIFF-MA200-MA10',\n#             'DIFF-MA200-CLOSE',\n#             'DIFF-MA100-CLOSE',\n#             'DIFF-MA50-CLOSE'\n           ]","3a5b21cf":"# Dataset with features\ndata = createFeatures(df)                               \n\n# Enter YOUR date\n# Since if we use the entire dataset, it won't be plotted properly, I will use tha data from past 2 months\n# Feel free to change the 'use_start_date'\nuse_start_date = pd.to_datetime(\"2021-11-01\" )\n\n# Shift the timeframe\ndata = data[data['close_time'] > use_start_date].copy()             \n\n# Filter the data to the list of FEATURES\ndata_filtered = data[FEATURES]                                \n\n# We add a prediction column and set dummy values to prepare the data for scaling\ndata_filtered_ext = data_filtered.copy()\ndata_filtered_ext['Prediction'] = data_filtered_ext['close']  \nprint(data_filtered_ext.tail().to_string())\n\n# remove Date column before training \ndfs = data_filtered_ext.copy()\ndel dfs[('close_time')]                                             \ndel dfs[('Prediction')]\n\n# Register matplotlib converters\nregister_matplotlib_converters()\n\nnrows = dfs.shape[1]\n\n# Plot parameters\nfig, ax = plt.subplots(figsize=(18, 8))                       \nx = data_filtered_ext['close_time'] \nassetname_list = []\n\nfor i in range(nrows):\n    assetname = dfs.columns[i-1]\n    y = data_filtered_ext[assetname]\n    \n    # Plotting each column\n    ax.plot(x, y, label=assetname, linewidth=1.0)             \n    assetname_list.append(assetname)\n\n  \nax.set_title('BTC - 2017~2022')\nax.legend()\nax.tick_params(axis=\"x\", rotation=90, labelsize=10, length=0)   \nplt.show","c8551243":"df_lstm = df.copy()\ndel df_lstm[('close_time')]\n\nnrows = df_lstm.shape[0]\nnp_data_unscaled = np.reshape(np.array(df_lstm), (nrows, -1))\nprint(np_data_unscaled.shape)\n\nscaler = RobustScaler()\nnp_data = scaler.fit_transform(np_data_unscaled)\n\nscaler_pred = RobustScaler()\ndf_Close = pd.DataFrame(df_lstm['close'])\nnp_Close_scaled = scaler_pred.fit_transform(df_Close)","37b09e0d":"sequence_length = 50\n\n# 80-20 split for training and test sets -- feel free to change this\ntrain_data_length = math.ceil(np_Close_scaled.shape[0] * 0.8)\n\n# Create the training and test data\ntrain_data = np_Close_scaled[0:train_data_length, :]\ntest_data = np_Close_scaled[train_data_length - sequence_length:, :]\n\ndef partition_dataset(sequence_length, data):\n    x, y = [], []\n    data_len = data.shape[0]\n\n    for i in range(sequence_length, data_len):\n        x.append(data[i-sequence_length:i,:]) \n        y.append(data[i, 0]) \n    \n    x = np.array(x)\n    y = np.array(y)\n    return x, y\n\nx_train, y_train = partition_dataset(sequence_length, train_data)\nx_test, y_test = partition_dataset(sequence_length, test_data)\n\n\nprint(x_train.shape, y_train.shape)\nprint(x_test.shape, y_test.shape)\n\nprint(x_train[1][sequence_length-1][0])\nprint(y_train[0])","92b214b8":"model = Sequential()\n\nn_neurons = x_train.shape[1] * x_train.shape[2]\nprint('timesteps: ' + str(x_train.shape[1]) + ',' + ' features: ' + str(x_train.shape[2]) + ', neurons: ' + str(n_neurons))\n\n# The Model\nmodel.add(LSTM(64, return_sequences=True, input_shape=(x_train.shape[1], x_train.shape[2]))) \n\nmodel.add(LSTM(128, return_sequences=True))\n\nmodel.add(LSTM(64, return_sequences=False))\n\nmodel.add(Dense(32))\nmodel.add(Dense(1, activation='relu'))\n \n# Hyperparameters\noptimizer='adam'; loss='mean_squared_error'; epochs = 100; batch_size = 64; patience = 6;\nparameter_list = ['epochs ' + str(epochs), 'batch_size ' + str(batch_size), 'patience ' + str(patience), 'optimizer ' + str(optimizer), 'loss ' + str(loss)]\nprint('Parameters: ' + str(parameter_list))\n\nmodel.compile(optimizer=optimizer, loss=loss)\n\nearly_stop = EarlyStopping(monitor='loss', \n                           patience=patience, \n                           verbose=1)\n\nmodel.summary()","95ec0527":"history = model.fit(x_train,\n                    y_train,\n                    validation_data=(x_test, y_test),\n                    batch_size=batch_size, \n                    epochs=epochs, \n                    callbacks=[early_stop], \n                    shuffle = True)","25d5589b":"fig, ax = plt.subplots(figsize=(10, 10), sharex=True)\n\nplt.plot(history.history[\"loss\"])\nplt.plot(history.history[\"val_loss\"])\n\nplt.title(\"Model loss\")\nplt.ylabel(\"Loss\")\nplt.xlabel(\"Epoch\")\nplt.legend([\"Train\", \"Test\"], loc=\"upper right\")\nplt.show()","66d6d8c3":"# Save the model if you want\n# model.save('YOUR_FILE_NAME.h5')","46f6694b":"train_df = df_plot.sort_values(by=['close_time']).copy()    \ndate_index = train_df.index                       \ndate_index_df = pd.DataFrame(date_index)","f260cf0d":"y_pred_scaled = model.predict(x_test)\n\n# Unscale the predicted values\ny_pred = scaler_pred.inverse_transform(y_pred_scaled)\ny_test_unscaled = scaler_pred.inverse_transform(y_test.reshape(-1, 1))\ny_test_unscaled.shape\n\nMAE = mean_absolute_error(y_test_unscaled, y_pred)\nprint(f'Median Absolute Error (MAE): {np.round(MAE, 2)}')\n\nMAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)\/ y_test_unscaled))) * 100\nprint(f'Mean Absolute Percentage Error (MAPE): {np.round(MAPE, 2)} %')\n\nMDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)\/ y_test_unscaled)) ) * 100\nprint(f'Median Absolute Percentage Error (MDAPE): {np.round(MDAPE, 2)} %')\n\n# Again, change the date if you want\ndisplay_start_date = \"2021-11-01\" \nuse_start_date = pd.to_datetime(\"2021-11-01\" )\n\ndata_filtered_sub = df_lstm.copy()\ndate_index = date_index_df[date_index_df['close_time'] > use_start_date].copy()\ndata_filtered_sub['close_time'] = date_index\n\ntrain = data_filtered_sub[:train_data_length + 1]\nvalid = data_filtered_sub[train_data_length:]\nvalid.insert(1, \"Predictions\", y_pred, True)\nvalid.insert(1, \"Difference\", valid[\"Predictions\"] - valid[\"close\"], True)\n\n\nvalid = valid[valid['close_time'] > display_start_date]\ntrain = train[train['close_time'] > display_start_date]\n\nfig, ax = plt.subplots(figsize=(18, 10), sharex=True)\n\nplt.title(\"Predictions vs. Ground Truth\", fontsize=20)\nplt.ylabel('BTC', fontsize=18)\nplt.plot(train[\"close\"], color=\"#039dfc\", linewidth=1.0)\nplt.plot(valid[\"Predictions\"], color=\"#E91D9E\", linewidth=1.0)\nplt.plot(valid[\"close\"], color=\"black\", linewidth=1.0)\nplt.legend([\"Train\", \"Test Predictions\", \"Ground Truth\"], loc=\"upper left\")\n\nvalid.loc[valid[\"Difference\"] >= 0, 'diff_color'] = \"#2BC97A\"\nvalid.loc[valid[\"Difference\"] < 0, 'diff_color'] = \"#C92B2B\"\nplt.bar(valid.index, valid[\"Difference\"], width=0.8, color=valid['diff_color'])\n\nplt.show()","64bc9efb":"df_t = pd.read_csv('..\/input\/historical-bitcoin-prices-btc\/BTCUSDT_1h.csv')\n\n# Removing the localization in 'close_time'\ndf_t['close_time'] = pd.to_datetime(df_t['close_time'], errors='coerce')\ndf_t['close_time'] = df_t['close_time'].dt.tz_localize(None)\ndf_t","2e881e36":"# Normalize 'close' value --> Target\n\ndf_t['return'] = df_t['close'] - df_t['close'].shift(1)\nreturn_range = df_t['return'].max() - df_t['return'].min() \ndf_t['return'] = df_t['return'] \/ return_range\n\ndf_t.plot(x='close_time', y='return', figsize=(18,4))","cb5e78c4":"# Make label, 1 as rising price, 0 as falling price\n\ndf_t['label'] = df_t['return'].shift(-1)\ndf_t['label'] = df_t['label'].apply(lambda x: 1 if x>0.0 else 0)\ndf_t.tail()","846ab33e":"df1 = df_t[df_t['label'] == 1]\ndf2 = df_t[df_t['label'] == 0]\n\nprint('df1 ',df1.shape)\nprint('df2 ',df2.shape)","f5eac55f":"df1 =df1.sample(n=18773)\ndf1","edd246df":"df_merge = pd.concat([df1, df2])\ndf_merge","a4156dc6":"df_merge = df_merge.sort_values(\"close_time\")\ndf_merge","7a21178a":"df_merge.isna().sum()","d0b19d09":"df_merge.drop(index=df_merge.index[0], axis=0,inplace=True)","7c7b3e89":"df_merge.isna().sum()","b2d18e8d":"df_merge","a4a61e66":"# Make training dataset\n# This cell will take some time to execute, because of vertical stacking\n\n# Number of features --> taking last 60 days for each tree\nn_features = 60 \n\n# Empty array of size 60\ntrain_x = np.array([]).reshape([-1,n_features]) \ntrain_y = np.array([]).reshape([-1,1])\nfor index, row in df_merge.iterrows():\n    i = df_merge.index.get_loc(index)  #index\n    if i<n_features:\n        continue\n    _x = np.array(df_t[i-n_features+1:i+1]['return']).T.reshape([1, -1]) \n    _y = df_t.loc[i]['label']\n    train_x = np.vstack((train_x, _x))\n    train_y = np.vstack((train_y, _y))\n    \ntrain_y = train_y.reshape([-1])\n\nprint(train_x.shape)\nprint(train_y.shape)\n\nprint('%% of Class_0 : %f' % (np.count_nonzero(train_y == 0)\/float(len(train_y))))\nprint('%% of Class_1 : %f' % (np.count_nonzero(train_y == 1)\/float(len(train_y))))","6ebcfefa":"# 90% for training, 10% for testing\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nclf = GradientBoostingClassifier(random_state=0, n_estimators=4)\n\ntrain_len = int(len(train_x)*0.9)\nclf.fit(train_x[:train_len], train_y[:train_len])\n\naccuracy = clf.score(train_x[train_len:], train_y[train_len:])\nprint('Testing Accuracy: %f' % accuracy)","c4bc9e29":"train_x[train_len:] = np.nan_to_num(train_x[train_len:])\nnp.any(np.isnan(train_x[train_len:]))","1f572a24":"del df_trade\ndel pred\ndel train_x\ndel train_y","cb673c00":"# Predict test data\npred = clf.predict(train_x[train_len:])\n\n\n# Calculate equity\ncontracts  = 10000.0\ncommission = 0.0\nflag = 0\n\ndf_trade = pd.DataFrame(train_x[train_len:,-1], columns=['return'])\ndf_trade['label']  = train_y[train_len:]\ndf_trade['pred']   = pred\ndf_trade['won']    = df_trade['label'] == df_trade['pred']\ndf_trade['return'] = df_trade['return'].shift(-1) * return_range\ndf_trade.drop(df_trade.index[len(df_trade)-1], inplace=True)\n\ndef calc_profit(row):\n    global flag\n    if flag == 0:\n        eq = contracts\n        flag = 1\n    else:\n        eq = 0 \n        \n    if row['won']:\n        return abs(row['return']) + eq - commission\n    else:\n        return -abs(row['return']) + eq - commission\n\ndf_trade['pnl'] = df_trade.apply(lambda row: calc_profit(row), axis=1)\ndf_trade['equity'] = df_trade['pnl'].cumsum()\n\ndisplay(df_trade.tail())\ndf_trade.plot(y='equity', figsize=(20,5), title='Backtest with $10,000 initial capital')\nplt.xlabel('Trades')\nplt.ylabel('Equity (USD)')\nfor r in df_trade.iterrows():\n    if r[1]['won']:\n        plt.axvline(x=r[0], linewidth=0.5, alpha=0.8, color='g')\n    else:\n        plt.axvline(x=r[0], linewidth=0.5, alpha=0.8, color='r')","95d04317":"# The first 20 trades\n\ndf_trade.head(20)","cb09c221":"# Calculate summary of trades\n\nn_win_trades = float(df_trade[df_trade['pnl']>0.0]['pnl'].count())\nn_los_trades = float(df_trade[df_trade['pnl']<0.0]['pnl'].count())\nprint(\"Net Profit            : $%.2f\" % df_trade.tail(1)['equity'])\nprint(\"Number Winning Trades : %d\" % n_win_trades)\nprint(\"Number Losing Trades  : %d\" % n_los_trades)\nprint(\"Percent Profitable    : %.2f%%\" % (100*n_win_trades\/(n_win_trades + n_los_trades)))\nprint(\"Avg Win Trade         : $%.3f\" % df_trade[df_trade['pnl']>0.0]['pnl'].mean())\nprint(\"Avg Los Trade         : $%.3f\" % df_trade[df_trade['pnl']<0.0]['pnl'].mean())\nprint(\"Largest Win Trade     : $%.3f\" % df_trade[df_trade['pnl']>0.0]['pnl'].max())\nprint(\"Largest Los Trade     : $%.3f\" % df_trade[df_trade['pnl']<0.0]['pnl'].min())\nprint(\"Profit Factor         : %.2f\" % abs(df_trade[df_trade['pnl']>0.0]['pnl'].sum()\/df_trade[df_trade['pnl']<0.0]['pnl'].sum()))\n\ndf_trade['pnl'].hist(bins=20)","b812b16b":"# **Pre-processing**","8a6ec7c0":"## **Short-term Analysis**\n## **If you want to perfrom the *Long-term Analysis*, change the date in *'use_start_date'***","25c93805":"### For the following cell, if you get 'Input contains NaN, infinity or a value too large for dtype('float32').' error, run the this:\ntrain_x[train_len:] = np.nan_to_num(train_x[train_len:])train_x[train_len:] = np.nan_to_num(train_x[train_len:])","ce34c468":"# **RNN (LSTM)**","ff9e9461":"## **Prediction**\n### **Change the date same as before. I went with 2021-11-01 ~ 2022-01-10. Note that the train line will not be shown given this date** ","64a5df4e":"## **Scaling & Transforming the Data**","73184e82":"## **Model Training**\n### **Hyperparameters:**\n* **Optimizer: Adam**\n* **Loss Function: MSE**\n* **Epochs: 100**\n* **Batch Size: 64**\n* **Early Stopping: Patience of 6 epochs**\n","5b2c9f68":"### **1. Using mplfinance to cisualizes the data as candlesticks (somewhat)**\n### **2. Note that it will take some time to render, since there are too many records**\n### **3. Ignore the warning**","115eb28b":"# **Trade Trend Prediction (Gradient Boosting)**\n## **Note that at this point, the main dataframe (*df*) has features from technical analysis. So keep that in mind if you want to use those featues, because there are some missing values. Here, I create a new instance of the dataset.","f072c5d7":"# **Conclusion**\n## Overall, the model performed very well. If we had used this model for trading BTC for the past 3 years or so, our \\\\$10K capital would be around \\\\$50K on Januray 10, 2022. Of course, real trading would much, much diffrent than I put it, and would require a lot more to do to make use of models like this one.","fc8221e5":"# **Technical Analysis**\n### From [Here](https:\/\/github.com\/dharun-narayanan\/Forex_Predictive_Model\/blob\/main\/forex_prediction_final.ipynb)","80232227":"### **Using Bokeh to visualize the data -- It's chart has zooming abilities**\n### **You might to zoom in on candles to properly see them OR you can plot less data, i.e. 2021 ~ 2022**\n### **Feel free to try other values and arguments -- for example, by changing the variable '*w*' you can change candlesticks properties**"}}