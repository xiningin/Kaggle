{"cell_type":{"121dc7e1":"code","3cc2d065":"code","4349c40f":"code","23a83ff5":"code","33c1160c":"code","1721542b":"code","cdc73a11":"code","c4ef18a2":"code","bae999e6":"code","62f39b6e":"code","7c6c8f8c":"code","1394746d":"code","0d385c18":"code","bddcb229":"code","11cb53e4":"code","2994dd78":"code","873e4b45":"code","56bde507":"code","a6b6a4e9":"code","f571d78b":"code","36248d34":"code","20d7ff71":"code","63160cf3":"code","0e8229fc":"code","816fb28c":"code","7353bd70":"code","c979f952":"code","55154b1e":"code","4c0a6c72":"code","6cf7d9e6":"code","abae31db":"code","aa200d87":"code","b8308e6c":"code","dcdca086":"code","6984082d":"code","8aa6f3cb":"code","262486e2":"code","1dbcaaa1":"code","3b1dfeea":"code","6221dae9":"code","af8472ed":"code","2eb86f53":"code","0e008d5c":"code","e05ac1aa":"code","8b568211":"code","144b33f2":"code","c243fbfa":"code","5b4ef450":"code","4f131ce1":"code","d92d7439":"code","868ee6d8":"code","bf2f2a4e":"code","7e2a5207":"code","5155ba7a":"code","94a3b4a2":"code","f2230e10":"code","35d4e0aa":"code","35644863":"code","8c84959e":"code","f280c658":"code","df064624":"code","dd78412f":"code","221cab0b":"code","6b7a4b61":"code","6ffb8ab8":"code","de2dd4d6":"code","e1fcbc7e":"code","4c7c090d":"code","cdb21c80":"code","098871b1":"code","bb3737a1":"code","1d3e0321":"code","bf64c33e":"code","ec494ae3":"code","06d7a4ca":"code","19b3cfa2":"code","2f29a265":"code","ee9219c4":"markdown","703457ff":"markdown","554b7657":"markdown","5e841d9f":"markdown","89881654":"markdown","7002783b":"markdown","30fa2e77":"markdown","ae9338ef":"markdown","94966aa1":"markdown","6ec28b6e":"markdown","6700d7ee":"markdown","7a411506":"markdown","9cf6fea8":"markdown","4ae46366":"markdown","6e68f667":"markdown","8b2b4e0e":"markdown","7218bb0d":"markdown","06edf046":"markdown","e1bf0601":"markdown","4ada87e1":"markdown","cc680285":"markdown","6e4d3fdb":"markdown","05e587a9":"markdown","90c6c214":"markdown"},"source":{"121dc7e1":"import numpy as np\nimport pandas as pd\nimport xgboost as xgb\nfrom xgboost import plot_importance, plot_tree\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom google.cloud import bigquery","3cc2d065":"train = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-1\/train.csv\")\ntest = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-1\/test.csv\")","4349c40f":"test","23a83ff5":"%%time\nclient = bigquery.Client()\ndataset_ref = client.dataset(\"noaa_gsod\", project=\"bigquery-public-data\")\ndataset = client.get_dataset(dataset_ref)\n\ntables = list(client.list_tables(dataset))\n\ntable_ref = dataset_ref.table(\"stations\")\ntable = client.get_table(table_ref)\nstations_df = client.list_rows(table).to_dataframe()\n\ntable_ref = dataset_ref.table(\"gsod2020\")\ntable = client.get_table(table_ref)\ntwenty_twenty_df = client.list_rows(table).to_dataframe()\n\nstations_df['STN'] = stations_df['usaf'] + '-' + stations_df['wban']\ntwenty_twenty_df['STN'] = twenty_twenty_df['stn'] + '-' + twenty_twenty_df['wban']\n\ncols_1 = ['STN', 'mo', 'da', 'temp', 'min', 'max', 'stp', 'wdsp', 'prcp', 'fog']\ncols_2 = ['STN', 'country', 'state', 'call', 'lat', 'lon', 'elev']\nweather_df = twenty_twenty_df[cols_1].join(stations_df[cols_2].set_index('STN'), on='STN')\n\nweather_df.tail(10)","33c1160c":"from scipy.spatial.distance import cdist\n\nweather_df['day_from_jan_first'] = (weather_df['da'].apply(int)\n                                   + 31*(weather_df['mo']=='02') \n                                   + 60*(weather_df['mo']=='03')\n                                   + 91*(weather_df['mo']=='04')  \n                                   )\n\nmo = train['Date'].apply(lambda x: x[5:7])\nda = train['Date'].apply(lambda x: x[8:10])\ntrain['day_from_jan_first'] = (da.apply(int)\n                               + 31*(mo=='02') \n                               + 60*(mo=='03')\n                               + 91*(mo=='04')  \n                              )\n\nC = []\nfor j in train.index:\n    df = train.iloc[j:(j+1)]\n    mat = cdist(df[['Lat','Long', 'day_from_jan_first']],\n                weather_df[['lat','lon', 'day_from_jan_first']], \n                metric='euclidean')\n    new_df = pd.DataFrame(mat, index=df.Id, columns=weather_df.index)\n    arr = new_df.values\n    new_close = np.where(arr == np.nanmin(arr, axis=1)[:,None],new_df.columns,False)\n    L = [i[i.astype(bool)].tolist()[0] for i in new_close]\n    C.append(L[0])\n    \ntrain['closest_station'] = C\n\ntrain = train.set_index('closest_station').join(weather_df[['temp', 'min', 'max', 'stp', 'wdsp', 'prcp', 'fog']], ).reset_index().drop(['index'], axis=1)\ntrain.sort_values(by=['Id'], inplace=True)\ntrain.head()","1721542b":"from scipy.spatial.distance import cdist\n\nweather_df['day_from_jan_first'] = (weather_df['da'].apply(int)\n                                   + 31*(weather_df['mo']=='02') \n                                   + 60*(weather_df['mo']=='03')\n                                   + 91*(weather_df['mo']=='04')  \n                                   )\n\nmo = test['Date'].apply(lambda x: x[5:7])\nda = test['Date'].apply(lambda x: x[8:10])\ntest['day_from_jan_first'] = (da.apply(int)\n                               + 31*(mo=='02') \n                               + 60*(mo=='03')\n                               + 91*(mo=='04')  \n                              )\n\nC = []\nfor j in test.index:\n    df = test.iloc[j:(j+1)]\n    mat = cdist(df[['Lat','Long', 'day_from_jan_first']],\n                weather_df[['lat','lon', 'day_from_jan_first']], \n                metric='euclidean')\n    new_df = pd.DataFrame(mat, index=df.ForecastId, columns=weather_df.index)\n    arr = new_df.values\n    new_close = np.where(arr == np.nanmin(arr, axis=1)[:,None],new_df.columns,False)\n    L = [i[i.astype(bool)].tolist()[0] for i in new_close]\n    C.append(L[0])\n    \ntest['closest_station'] = C\n\ntest = test.set_index('closest_station').join(weather_df[['temp', 'min', 'max', 'stp', 'wdsp', 'prcp', 'fog']], ).reset_index().drop(['index'], axis=1)\ntest.sort_values(by=['ForecastId'], inplace=True)\ntest.head()","cdc73a11":"train[\"wdsp\"] = pd.to_numeric(train[\"wdsp\"])\ntest[\"wdsp\"] = pd.to_numeric(test[\"wdsp\"])","c4ef18a2":"train[\"fog\"] = pd.to_numeric(train[\"fog\"])\ntest[\"fog\"] = pd.to_numeric(test[\"fog\"])","bae999e6":"X_train = train.drop([\"Fatalities\", \"ConfirmedCases\"], axis=1)","62f39b6e":"countries = X_train[\"Country\/Region\"]","7c6c8f8c":"countries.unique()","1394746d":"X_train = X_train.drop([\"Id\"], axis=1)\nX_test = test.drop([\"ForecastId\"], axis=1)","0d385c18":"X_train.dtypes","bddcb229":"X_train['Date']= pd.to_datetime(X_train['Date']) \nX_test['Date']= pd.to_datetime(X_test['Date']) ","11cb53e4":"X_train = X_train.set_index(['Date'])\nX_test = X_test.set_index(['Date'])","2994dd78":"def create_time_features(df):\n    \"\"\"\n    Creates time series features from datetime index\n    \"\"\"\n    df['date'] = df.index\n    df['hour'] = df['date'].dt.hour\n    df['dayofweek'] = df['date'].dt.dayofweek\n    df['quarter'] = df['date'].dt.quarter\n    df['month'] = df['date'].dt.month\n    df['year'] = df['date'].dt.year\n    df['dayofyear'] = df['date'].dt.dayofyear\n    df['dayofmonth'] = df['date'].dt.day\n    df['weekofyear'] = df['date'].dt.weekofyear\n    \n    X = df[['hour','dayofweek','quarter','month','year',\n           'dayofyear','dayofmonth','weekofyear']]\n    return X","873e4b45":"create_time_features(X_train)\ncreate_time_features(X_test)","56bde507":"X_train","a6b6a4e9":"X_test","f571d78b":"X_train.drop(\"date\", axis=1, inplace=True)\nX_test.drop(\"date\", axis=1, inplace=True)","36248d34":"world_happiness_index = pd.read_csv(\"..\/input\/world-bank-datasets\/World_Happiness_Index.csv\")","20d7ff71":"world_happiness_grouped = world_happiness_index.groupby('Country name').nth(-1)","63160cf3":"world_happiness_grouped.drop(\"Year\", axis=1, inplace=True)","0e8229fc":"X_train = pd.merge(left=X_train, right=world_happiness_grouped, how='left', left_on='Country\/Region', right_on='Country name')\nX_test = pd.merge(left=X_test, right=world_happiness_grouped, how='left', left_on='Country\/Region', right_on='Country name')","816fb28c":"X_train","7353bd70":"malaria_world_health = pd.read_csv(\"..\/input\/world-bank-datasets\/Malaria_World_Health_Organization.csv\")","c979f952":"X_train = pd.merge(left=X_train, right=malaria_world_health, how='left', left_on='Country\/Region', right_on='Country')\nX_test = pd.merge(left=X_test, right=malaria_world_health, how='left', left_on='Country\/Region', right_on='Country')","55154b1e":"X_train","4c0a6c72":"X_train.drop(\"Country\", axis=1, inplace=True)\nX_test.drop(\"Country\", axis=1, inplace=True)","6cf7d9e6":"human_development_index = pd.read_csv(\"..\/input\/world-bank-datasets\/Human_Development_Index.csv\")","abae31db":"X_train = pd.merge(left=X_train, right=human_development_index, how='left', left_on='Country\/Region', right_on='Country')\nX_test = pd.merge(left=X_test, right=human_development_index, how='left', left_on='Country\/Region', right_on='Country')","aa200d87":"X_train","b8308e6c":"X_train.drop([\"Country\", \"Gross national income (GNI) per capita 2018\"], axis=1, inplace=True)\nX_test.drop([\"Country\", \"Gross national income (GNI) per capita 2018\"], axis=1, inplace=True)","dcdca086":"night_ranger_predictors = pd.read_csv(\"..\/input\/covid19-demographic-predictors\/covid19_by_country.csv\")","6984082d":"#There is a duplicate for Georgia in this dataset from Night Ranger, causing merge issues so we will just drop the Georgia rows\nnight_ranger_predictors = night_ranger_predictors[night_ranger_predictors.Country != \"Georgia\"]","8aa6f3cb":"X_train = pd.merge(left=X_train, right=night_ranger_predictors, how='left', left_on='Country\/Region', right_on='Country')\nX_test = pd.merge(left=X_test, right=night_ranger_predictors, how='left', left_on='Country\/Region', right_on='Country')","262486e2":"X_train","1dbcaaa1":"X_train.drop([\"Country\", \"Restrictions\",\"Quarantine\", \"Schools\", \"Total Infected\", \"Total Deaths\"], axis=1, inplace=True)\nX_test.drop([\"Country\", \"Restrictions\",\"Quarantine\", \"Schools\", \"Total Infected\", \"Total Deaths\"], axis=1, inplace=True)","3b1dfeea":"X_train.info(verbose=True)\n","6221dae9":"X_test","af8472ed":"X_train = pd.concat([X_train,pd.get_dummies(X_train['Province\/State'], prefix='ps')],axis=1)\nX_train.drop(['Province\/State'],axis=1, inplace=True)\nX_test = pd.concat([X_test,pd.get_dummies(X_test['Province\/State'], prefix='ps')],axis=1)\nX_test.drop(['Province\/State'],axis=1, inplace=True)","2eb86f53":"X_train = pd.concat([X_train,pd.get_dummies(X_train['Country\/Region'], prefix='cr')],axis=1)\nX_train.drop(['Country\/Region'],axis=1, inplace=True)\nX_test = pd.concat([X_test,pd.get_dummies(X_test['Country\/Region'], prefix='cr')],axis=1)\nX_test.drop(['Country\/Region'],axis=1, inplace=True)","0e008d5c":"y_train = train[\"Fatalities\"]","e05ac1aa":"y_train","8b568211":"X_train","144b33f2":"reg = xgb.XGBRegressor(n_estimators=1000)","c243fbfa":"reg.fit(X_train, y_train, verbose=True)","5b4ef450":"plot = plot_importance(reg, height=0.9, max_num_features=20)","4f131ce1":"y_train = train.groupby([\"Country\/Region\"]).Fatalities.pct_change(periods=1)","d92d7439":"y_train = y_train.replace(np.nan, 0)","868ee6d8":"y_train = y_train.replace(np.inf, 0)","bf2f2a4e":"reg = xgb.XGBRegressor(n_estimators=1000)","7e2a5207":"reg.fit(X_train, y_train, verbose=True)","5155ba7a":"plot = plot_importance(reg, height=0.9, max_num_features=20)","94a3b4a2":"y_train = train[\"ConfirmedCases\"]","f2230e10":"reg = xgb.XGBRegressor(n_estimators=1000)","35d4e0aa":"reg.fit(X_train, y_train, verbose=True)","35644863":"plot = plot_importance(reg, height=0.9, max_num_features=20)","8c84959e":"y_train = train.groupby([\"Country\/Region\"]).ConfirmedCases.pct_change(periods=1)","f280c658":"y_train = y_train.replace(np.nan, 0)","df064624":"y_train = y_train.replace(np.inf, 0)","dd78412f":"reg = xgb.XGBRegressor(n_estimators=1000)","221cab0b":"reg.fit(X_train, y_train, verbose=True)","6b7a4b61":"plot = plot_importance(reg, height=0.9, max_num_features=20)","6ffb8ab8":"y_train = train[\"ConfirmedCases\"]\nconfirmed_reg = xgb.XGBRegressor(n_estimators=1000)\nconfirmed_reg.fit(X_train, y_train, verbose=True)\npreds = confirmed_reg.predict(X_test)\npreds = np.array(preds)\npreds[preds < 0] = 0\npreds = np.round(preds, 0)","de2dd4d6":"preds = np.array(preds)","e1fcbc7e":"preds","4c7c090d":"submissionOrig = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-1\/submission.csv\")","cdb21c80":"submissionOrig[\"ConfirmedCases\"]=pd.Series(preds)","098871b1":"submissionOrig","bb3737a1":"test = test.join(submissionOrig[\"ConfirmedCases\"])\ntest[\"Difference\"] = test.groupby([\"Country\/Region\"])[\"ConfirmedCases\"].apply(lambda x: x.shift(1) - x)\nfor index, row in test.iterrows():\n    if index>0:\n        if row[\"Difference\"] < 0:\n            test.at[index,\"ConfirmedCases\"] = test.iloc[index-1][\"ConfirmedCases\"]","1d3e0321":"submissionOrig[\"ConfirmedCases\"] = test[\"ConfirmedCases\"]\ntest.drop(\"ConfirmedCases\", axis=1, inplace=True)","bf64c33e":"y_train = train[\"Fatalities\"]\nconfirmed_reg = xgb.XGBRegressor(n_estimators=1000)\nconfirmed_reg.fit(X_train, y_train, verbose=True)\npreds = confirmed_reg.predict(X_test)\npreds = np.array(preds)\npreds[preds < 0] = 0\npreds = np.round(preds, 0)\nsubmissionOrig[\"Fatalities\"]=pd.Series(preds)","ec494ae3":"test = test.join(submissionOrig[\"Fatalities\"])\ntest[\"Difference\"] = test.groupby([\"Country\/Region\"])[\"Fatalities\"].apply(lambda x: x.shift(1) - x)\nfor index, row in test.iterrows():\n    if index>0:\n        if row[\"Difference\"] < 0:\n            test.at[index,\"Fatalities\"] = test.iloc[index-1][\"Fatalities\"]","06d7a4ca":"submissionOrig[\"Fatalities\"] = test[\"Fatalities\"]\ntest.drop(\"Fatalities\", axis=1, inplace=True)","19b3cfa2":"submissionOrig","2f29a265":"submissionOrig.to_csv('submission.csv',index=False)","ee9219c4":"# Add Weather Data","703457ff":"For now lets drop some of the columns I am not quite sure how to implement in my analysis yet. These may be very good variables to use but I will have to spend more time thinking about how to use them. Also MAKE SURE TO REMOVE the \"Total Infected\", \"Total Deaths\" columns as these are what we are trying to get our model to predict.","554b7657":"There are also issues with pct_change function sometimes returning \"inf\" when going from 0 to 0, so just change these to 0","5e841d9f":"# Try running on test data and submitting results","89881654":"# **INTRODUCTION:**\n\nThis is a very simple implementation of XGBoost for this data. One of the great features of XGBoost is that it has built in functionality to easily see the top features F score. This means we can add a lot of features to the model and then easily see what really makes a difference in prediction. This is extremely important for this challenge since it is not really about the leaderboard results, it is more about determining useful features for models. \n\nI am working on collecting publicly available datasets to continue to add in new variables (features) to see what might be useful.","7002783b":"Drop the two \"y\" columns","30fa2e77":"Change the Date column to be a datetime","ae9338ef":"So one further issue with the predictions is that XGBoost does not know that the predictions should be cumulative, so for each country we always want the predictions to be greater than or equal to the previous days. To fix this issue we should attached the predictions back to the original test dataframe so that we can grouby country\/state and then for each grouping we need to make sure that the predicted variable is always equal the previous days prediction or greater.","94966aa1":"# Load the data","6ec28b6e":"# Adding more variables to the mix","6700d7ee":"# One hot encode the Provice\/State and the Country\/Region columns","7a411506":"I think now would be a good time to add a couple more variables from outside datasets to this mix to see if any of their data could provide further insight in our predictions. I have collected a couple from the World Bank as well as the UN. These datasets are nice since they have data listed for almost all countries in the world.\n\nLets start with the World Happiness Index dataset from the UN. It has some information related to GINI Coefficients, \"social support\", \"Healthy Life Expectancy at Birth\", Generosity, and Perceptions of Corruption. These indicators could capture some ideas around the healthcare setups in each country and also broad societal differences. They are very generic and broad so I wouldnt expect them to be extremely useful, and even if they show up as very informative on our predictions it would be tough to really break out true actionable insights from them but it is somewhere to start.\n\nWe will just grab the most recent value for each country (most this is 2018) to begin with. If you wanted to get a little more in depth you could probably take an average of the last 5 years or something like that but for now we will stay simple.","9cf6fea8":"We will be doing this using the technique outlined in the great notebook https:\/\/www.kaggle.com\/davidbnn92\/weather-data?scriptVersionId=30695168","4ae46366":"There are issues with pct_change function returning NaN when doing percentage change from 0 to 0, so just change these to 0.","6e68f667":"Drop the Id column","8b2b4e0e":"# Grab the \"y\" variable we want to predict","7218bb0d":"# Create time features based on the new Date index","06edf046":"# Change y variable to Confirmed Cases\n\nWe will use the same train data as above but lets change the y variable to be Confirmed Cases and see if anything changes.","e1bf0601":"# Use percentage change in the y variable instead of raw numbers\n\nI think another interesting way to look at this might be through percentage change of the y variable. We really care about the percentage change in fatalities from day to day not the total number. This is because we know areas where the infection has been for a longer period of time would automatically have a higher total number, while areas with relatively new infection would have a lower total number of deaths but quite possible a higher percentage change since the virus is spreading more rapidly.","4ada87e1":"One thing to note is that fatalities will always go up, so we will want to adjust any prediction that is less than the previous days prediction to be equal to the previous day. Also the same is true with confirmed cases.","cc680285":"Get the percentage change for each country, using one day lag. Would be interesting to play around with the lag time (periods) to see if this changes the analysis, my first thought would be to change this to 7 day (one week).","6e4d3fdb":"Set the index to the date","05e587a9":"Issue with wdsp and fog column being objects and not numeric, so change this","90c6c214":"Check the datatypes, they need to all be int, float, or bool for XGBoost"}}