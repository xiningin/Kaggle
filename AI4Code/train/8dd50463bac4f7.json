{"cell_type":{"012a7157":"code","025037f9":"code","689ceee7":"code","e004c82d":"code","c12c923b":"code","19348415":"code","fa6c5360":"code","092f6080":"code","e0cba271":"code","9df0b84c":"code","38388b12":"code","c7d2b49f":"code","6127462c":"code","b00bdd09":"code","e3ddd172":"code","037f5b92":"code","4b0b5d8a":"code","002d1658":"code","a7450bc6":"code","39f7870c":"code","4026ffc9":"code","439337cd":"code","9da0149d":"code","8ab239b4":"code","4d1ed83c":"code","70aeaacb":"code","3539ea11":"code","a4e3cb16":"code","37dd687b":"code","e0277868":"code","9d940705":"code","236008b6":"code","50b20baf":"code","52c78aa0":"code","761c0470":"code","722bc328":"code","57bb2cad":"code","1ea39011":"markdown","69c3e048":"markdown","3dd943a6":"markdown","711eae9f":"markdown","309b2390":"markdown","c17f64b4":"markdown","0d185753":"markdown","054d9158":"markdown","4352c4ec":"markdown","28c9615f":"markdown","4b178fb2":"markdown","f273f380":"markdown","d78b21a5":"markdown","5fc2ccac":"markdown","27ec08f8":"markdown","a25ae131":"markdown","17e2c1ec":"markdown","3b400924":"markdown","3bfd7bce":"markdown","8a403dfb":"markdown","34aa2341":"markdown","607d75e4":"markdown","6b66e708":"markdown","9ac20ddc":"markdown","0d2929b4":"markdown","0c60774f":"markdown","cad30ba4":"markdown","eccb5652":"markdown","48f934d1":"markdown"},"source":{"012a7157":"print('Loading packages')\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nprint('These are the files to use: ',os.listdir(\"..\/input\"))\nfrom sklearn import preprocessing\nfrom statistics import mean\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport random\nfrom matplotlib import rcParams\n%matplotlib inline\nle = preprocessing.LabelEncoder()\nimport re","025037f9":"print('reading input files..')\ndata = pd.read_csv('..\/input\/train.csv')\nsampl = pd.read_csv('..\/input\/gender_submission.csv')","689ceee7":"test  = pd.read_csv('..\/input\/test.csv')","e004c82d":"data.head()","c12c923b":"# Appending test data with train data, since both dataset can have related values like family name and ticket\ndf = data.append(test, sort = False)","19348415":"totalt = df.isnull().sum().sort_values(ascending=False)\npercent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([totalt, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(6)","fa6c5360":"# Creating a TicketId feature, it will tell which person was part of that Family or group\nticketNum = pd.DataFrame(df.Ticket.value_counts())\nticketNum.rename(columns = {'Ticket' : 'TicketNum'}, inplace = True)\nticketNum['TicketId'] = pd.Categorical(ticketNum.index).codes\nticketNum.loc[ticketNum.TicketNum < 3, 'TicketId'] = -1\ndf = pd.merge(left = df, right = ticketNum, left_on = 'Ticket', \n              right_index = True, how = 'left', sort = False)\ndf = df.drop(['TicketNum'],axis=1)\ndf.head()","092f6080":"# Separating FamilyName\ndf['FamilyName'] = df.Name.apply(lambda x : str.split(x, ',')[0])","e0cba271":"# Lets create one more feature FamilySurvival\ndf['FamilySurv'] = 0.5\nfor _, grup in df.groupby(['FamilyName','Fare']):\n    if len(grup) != 1:\n        for index, row in grup.iterrows():\n            smax = grup.drop(index).Survived.max()\n            smin = grup.drop(index).Survived.min()\n            pid = row.PassengerId\n            \n            if smax == 1:\n                df.loc[df.PassengerId == pid, 'FamilySurv'] = 1.0\n            elif smin == 0:\n                df.loc[df.PassengerId == pid, 'FamilySurv'] = 0.0\nfor _, grup in df.groupby(['Ticket']):\n    if len(grup) != 1:\n        for index, row in grup.iterrows():\n            if (row.FamilySurv == 0.0 or row.FamilySurv == 0.5):\n                smax = grup.drop(index).Survived.max()\n                smin = grup.drop(index).Survived.min()\n                pid  = row.PassengerId\n\n                if smax == 1:\n                    df.loc[df.PassengerId == pid, 'FamilySurv'] = 1.0\n                elif smin == 0:\n                    df.loc[df.PassengerId == pid, 'FamilySurv'] = 0.0\ndf.FamilySurv.value_counts()","9df0b84c":"# CabinNum (Finding, how many cabin a person has)\ndef CabinNum(data):\n    data.Cabin = data.Cabin.fillna('0')\n    regex = re.compile('\\s*(\\w+)\\s*')\n    data['CabinNum'] = data.Cabin.apply(lambda x : len(regex.findall(x)))\nCabinNum(df)","38388b12":"df.CabinNum.value_counts()","c7d2b49f":"# Creating Feature Title, since Higher Rank people has more survival chances, should give hit and trail\ndef TitleFunc(data):\n    sub = {'Col.','Rev.', 'Mr.','Sir.','Jonkheer.', 'Don.','Dona.','Capt.',\n           'General.','Major.'}\n    sub1 = {'Miss.','Mme.','Mlle.','Ms.'}\n    sub2 = {'Mrs.','Countess.','Lady.'}\n    sub3 = {'Master.'}\n    sub4 = {'Dr.'}\n    pattern, pattern1, pattern2, pattern3 = '|'.join(sub), '|'.join(sub1), '|'.join(sub2), '|'.join(sub3)\n    pattern4 = '|'.join(sub4)\n    data['Title'] = ''\n    data.loc[data['Name'].str.contains(pattern),'Title'] = 'Mr.'\n    data.loc[data['Name'].str.contains(pattern1),'Title'] = 'Miss.'\n    data.loc[data['Name'].str.contains(pattern2),'Title'] = 'Mrs.'\n    data.loc[data['Name'].str.contains(pattern3),'Title'] = 'Master.'\n    data.loc[(data['Name'].str.contains(pattern)) & (data['Age'] <=13),'Title'] = 'Master.'\n    data.loc[(data['Name'].str.contains(pattern4)) & (data['Sex'] == 'female'),'Title'] = 'Dr.f'\n    data.loc[(data['Name'].str.contains(pattern4)) & (data['Sex'] == 'male'),'Title'] = 'Dr.m'\nTitleFunc(df)","6127462c":"#Lets see Who Survived most\ntrain1 = df[0:891].copy()\nsns.set(style=\"whitegrid\")\nplt.figure(figsize=(10,3))\nax = sns.barplot(x=\"Title\", y=\"Survived\", data=train1)\n#ax = sns.barplot(x=\"Title\", y=\"Survived\",hue='Title', data=train1)","b00bdd09":"# Lets first check missing fare\ndf.loc[df['Fare'].isnull()]","e3ddd172":"#Let's find simlar data, and fill that for missing fare\ndf.loc[(df['Age'] >= 60) & (df['Pclass'] ==3) & (df['Sex'] == 'male') & (df['Embarked'] =='S')]","037f5b92":"df.loc[df['Fare'].isnull(), 'Fare'] = 7            #First fill missing fare by least value","4b0b5d8a":"# Creating FareCat Title, since High Fare people has more survival chances\ndef FareFunc(data):\n    data['FareCat'] = 0\n    data.loc[data['Fare'] < 8, 'FareCat'] = 0\n    data.loc[(data['Fare'] >= 8 ) & (data['Fare'] < 16),'FareCat' ] = 1\n    data.loc[(data['Fare'] >= 16) & (data['Fare'] < 30),'FareCat' ] = 2\n    data.loc[(data['Fare'] >= 30) & (data['Fare'] < 45),'FareCat' ] = 3\n    data.loc[(data['Fare'] >= 45) & (data['Fare'] < 80),'FareCat' ] = 4\n    data.loc[(data['Fare'] >= 80) & (data['Fare'] < 160),'FareCat' ] = 5\n    data.loc[(data['Fare'] >= 160) & (data['Fare'] < 270),'FareCat' ] = 6\n    data.loc[(data['Fare'] >= 270), 'FareCat'] = 7\nFareFunc(df)","002d1658":"train1 = df[0:891].copy()\nsns.set(style=\"whitegrid\")\nplt.figure(figsize=(14,3.5))\nax = sns.barplot(x=\"FareCat\", y=\"Survived\",hue='Title', data=train1)","a7450bc6":"# Creating FamlSize Feature, since Very big family dint survive as per data\ndef FamlSize(data):\n    data['FamlSize'] = 0\n    data['FamlSize'] = data['SibSp'] + data['Parch'] + 1\nFamlSize(df)","39f7870c":"df.head(1)","4026ffc9":"def LablFunc(data):\n    lsr = {'Title','Cabin'}\n    for i in lsr:\n        le.fit(data[i].astype(str))\n        data[i] = le.transform(data[i].astype(str))\nLablFunc(df)","439337cd":"## Lets predict the age of a person and fill the missing Age\nfeatures = ['Pclass','SibSp','Parch','TicketId','Fare','CabinNum','Title']\nfrom sklearn.ensemble import ExtraTreesRegressor as ETRg\ndef AgeFunc(df):\n    Etr = ETRg(n_estimators = 200, random_state = 2)\n    AgeX_Train = df[features][df.Age.notnull()]\n    AgeY_Train = df['Age'][df.Age.notnull()]\n    AgeX_Test = df[features][df.Age.isnull()]\n    \n    Etr.fit(AgeX_Train,np.ravel(AgeY_Train))\n    AgePred = Etr.predict(AgeX_Test)\n    df.loc[df.Age.isnull(), 'Age'] = AgePred\n    \nAgeFunc(df)","9da0149d":"# Lets derive AgeGroup feature from age\ndef AgeCat(data):\n    data['AgeCat'] = 0\n    data.loc[(data['Age'] <= 5), 'AgeCat'] = 0\n    data.loc[(data['Age'] <= 12) & (data['Age'] > 5), 'AgeCat'] = 1\n    data.loc[(data['Age'] <= 18) & (data['Age'] > 12), 'AgeCat'] = 2\n    data.loc[(data['Age'] <= 22) & (data['Age'] > 18), 'AgeCat'] = 3\n    data.loc[(data['Age'] <= 32) & (data['Age'] > 22), 'AgeCat'] = 4\n    data.loc[(data['Age'] <= 45) & (data['Age'] > 32), 'AgeCat'] = 5\n    data.loc[(data['Age'] <= 60) & (data['Age'] > 45), 'AgeCat'] = 6\n    data.loc[(data['Age'] <= 70) & (data['Age'] > 60), 'AgeCat'] = 7\n    data.loc[(data['Age'] > 70), 'AgeCat'] = 8\nAgeCat(df)","8ab239b4":"train1 = df[0:891].copy()\nsns.set(style=\"whitegrid\")\nplt.figure(figsize=(14,3.5))\nax = sns.barplot(x=\"AgeCat\", y=\"Survived\",hue='Sex', data=train1)","4d1ed83c":"df.loc[df['Embarked'].isnull()]","70aeaacb":"sns.set(style=\"whitegrid\")\nplt.figure(figsize=(12,2))\nax = sns.barplot(x=\"Embarked\", y=\"Survived\",hue='Pclass', data=df)","3539ea11":"def FillEmbk(data):\n    var = 'Embarked'\n    data.loc[(data.Embarked.isnull()),'Embarked']= 'C'\nFillEmbk(df)","a4e3cb16":"# Label Encode Embarked\ndef LablFunc(data):\n    lst = {'Embarked','Sex'}\n    for i in lst:\n        le.fit(data[i].astype(str))\n        data[i] = le.transform(data[i].astype(str))\nLablFunc(df)","37dd687b":"df.columns","e0277868":"from sklearn.preprocessing import StandardScaler\ntarget = data['Survived'].values\nselect_features = ['Pclass', 'Age','AgeCat','SibSp', 'Parch', 'Fare', \n                   'Embarked', 'TicketId', 'CabinNum', 'Title','Cabin',\n                   'FareCat', 'FamlSize','FamilySurv','Sex']\nscaler = StandardScaler()\ndfScaled = scaler.fit_transform(df[select_features])\ntrain = dfScaled[0:891].copy()\ntest = dfScaled[891:].copy()","9d940705":"from sklearn.feature_selection import SelectKBest, f_classif\nselector = SelectKBest(f_classif, len(select_features))\nselector.fit(train, target)\nscores = -np.log10(selector.pvalues_)\nindices = np.argsort(scores)[::-1]\n\nprint('Features importance:')\nfor i in range(len(scores)):\n    print('%.2f %s' % (scores[indices[i]], select_features[indices[i]]))","236008b6":"def FeatFunc(t_data,model):\n    names = t_data.columns.values\n    print(\"Features sorted by their score:\")\n    print(sorted(zip(map(lambda x: round(x, 4), model.feature_importances_), names), \n                 reverse=True))","50b20baf":"from sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier","52c78aa0":"SrchRFC = RandomForestClassifier(max_depth = 5, min_samples_split = 4, n_estimators = 500,\n                                 random_state = 20, n_jobs = -1)          # Initialize\nSrchRFC.fit(train, target)          # Train","761c0470":"prc = SrchRFC.predict(train)\naccuracy_score(target,prc)","722bc328":"prdt2 = SrchRFC.predict(test)   #using Random Forest Classifier\nprint('Predicted result: ', prdt2)","57bb2cad":"sampl['Survived'] = pd.DataFrame(prdt2)\nsampl.to_csv('submission.csv', index=False)","1ea39011":"### Let's Predict the outcome on Test data","69c3e048":"### Finding out missing values in Embarked column","3dd943a6":"### Define Feture importance function","711eae9f":"#### Submit the file to competition","309b2390":"**Find out missing data**","c17f64b4":"### Lable Encoding Categorical Data","0d185753":"### Lable Encoding","054d9158":"### Lets Check first from where 1st Class passesnger Came","4352c4ec":"**Creating New Features**","28c9615f":"**More Feature Creation**","4b178fb2":"Oh Wow... Doctor of female category survived most and after that Mrs and after that Miss and then Master and then Male Doctors","f273f380":"### Visualize data","d78b21a5":"Oh!!!, we have **Cabin, Age, Embarked and Fare** has missing values after combining both training and test data.","5fc2ccac":"Bravo!! Mrs. Misss. and female Dr. survived even in zero fare category. However Male Survived in higher Fare category only..","27ec08f8":"If you are a begginer, you can leave this portion of creating 'FamilySurv, and come back later when start unserstanding.","a25ae131":"### Lets check which Fare class Survived along with their title","17e2c1ec":"\n## Please upvote this Kernel, if you find it useful.\n","3b400924":"### Checking best features","3bfd7bce":"Value NaN means null value. It looks like data has some **missing values**. We need to **first check which columns have missing values**, and then we will fill them accordingly","8a403dfb":"First, **let's combine both** train and test data, to make it easy to fill values and make operation and create relation between train and test data.","34aa2341":"### Predict the values on Train data to check Accuracy Score","607d75e4":"## Fill missing Age","6b66e708":"### Importing RandomForestClassifier","9ac20ddc":"#### from 'C' high number of 1st Pclass people Survived, lets fill 'C' in missing value","0d2929b4":"## Lets check which Fare class Survived along with their title","0c60774f":"**In this kernel we will learn machine learning step by step:**\n1. **Find out missing data** columns in train and test data\n2. **Merging datasets**\n3. **Filling missing values** in datasets\n4. **Creating new features** (columns) from existing features\n5. **Visualizing data** using **matplotlib**\n6. **Label Encoding** the categorical data (**A method to convert catagorical text data into numerical data**)\n7. Checking approximate **feature importance**\n8. **Scaling up data** using Standard Scaler\n9. Initializing **simple random forest** model to use\n10. **Train** the Model\n11. **Predict the outcome** from the model\n11. Submit the **output** file to competition","cad30ba4":"### Scaling up data","eccb5652":"### Initializing Model and Training the the Model","48f934d1":"**Fill Missing data**"}}