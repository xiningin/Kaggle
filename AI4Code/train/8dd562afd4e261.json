{"cell_type":{"8fddfc48":"code","208669d4":"code","225718a2":"code","5efc8f93":"code","64a48479":"code","8233f2b7":"code","09d842ab":"code","f7ab2075":"code","907ab436":"code","30d234ab":"code","2aea96fe":"code","eb56e8ae":"code","82e46974":"code","94fd0dde":"code","3de46cee":"code","3418396d":"code","05d8617e":"code","bf152deb":"code","b3925ce6":"code","ec9b5e58":"code","3545e0f8":"code","d9b92f4f":"code","10b1c502":"code","0564899c":"code","dca161d4":"code","8b245ee4":"code","74135f85":"code","b8dd2d21":"code","9f05371f":"code","0036e9df":"code","0b2e22f5":"code","f8a2e2cf":"code","0e8a9401":"code","c9462465":"code","696063ea":"code","a7c91e63":"code","a5158c33":"code","eb783243":"code","def90f08":"code","acb97aad":"code","028deb33":"code","b2a338d9":"code","4bab60bd":"code","d71a0060":"code","6551ef07":"code","b4239d4b":"code","849cf7ba":"code","600278b2":"code","442a609e":"code","6f05800f":"code","451304de":"code","fb96ac8b":"code","dbad6097":"code","743656f4":"code","dafee3fe":"code","f6448081":"code","b5799bca":"code","d49732a8":"code","2d055f89":"code","606ee052":"code","072f473a":"code","a851ae97":"code","37e101fa":"code","ccae4cc7":"code","ad1d4f32":"code","5c86549c":"code","65dbe4ec":"code","5c3a2c9c":"code","271ef185":"code","5682af2d":"code","04f92856":"code","ebdf5ac4":"markdown","8c746603":"markdown","41d3ac17":"markdown","df256b92":"markdown","89dc8885":"markdown","d165afaa":"markdown","0e118e12":"markdown","d66c1406":"markdown","4abc358c":"markdown","692da3f5":"markdown","0e8c70fe":"markdown","2d492067":"markdown","cef3ca7c":"markdown","0be9d835":"markdown","10fb7395":"markdown","b8e30f01":"markdown","8b127e00":"markdown","805af369":"markdown","4ab0a365":"markdown","e4bbdbb2":"markdown","2ffb7ae4":"markdown","c7729807":"markdown","ff1b88ab":"markdown","6e5bbaca":"markdown","3f744097":"markdown","9fe58b66":"markdown","2614b7ad":"markdown","ecd23341":"markdown","2542fd3f":"markdown","634d9073":"markdown"},"source":{"8fddfc48":"from datetime import datetime\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.rc(\"font\", size=14)\nimport warnings\nimport pandas_profiling \nwarnings.filterwarnings('ignore')\npd.set_option('display.max_colwidth', 200)\npd.set_option('display.max_rows', 10000)\npd.set_option('display.max_columns', 200)\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n\nfrom scipy.stats import randint as sp_randint\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score, confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import label_binarize\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\n\nfrom sklearn import tree\nfrom sklearn import ensemble\nfrom sklearn import linear_model\nfrom sklearn import model_selection\n\nfrom keras.utils import to_categorical\nfrom sklearn.preprocessing import LabelEncoder","208669d4":"iris = pd.read_csv('\/kaggle\/input\/iris\/Iris.csv')\niris.head()","225718a2":"iris.shape","5efc8f93":"iris.info()","64a48479":"sns.countplot(iris.Species)","8233f2b7":"iris.describe()","09d842ab":"iris = iris.drop(['Id'], axis=1)","f7ab2075":"dataset_na = (iris.isnull().sum() \/ len(iris)) * 100\ndataset_na = dataset_na.drop(dataset_na[dataset_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :dataset_na})\nmissing_data.head(20)","907ab436":"plt.figure(figsize=(6,4))\nsns.heatmap(iris.corr(),annot=True) \nplt.ylim(4,0)","30d234ab":"sns.FacetGrid(iris, hue=\"Species\", size=5) \\\n   .map(plt.scatter, \"PetalLengthCm\", \"PetalWidthCm\") \\\n   .add_legend()","2aea96fe":"sns.FacetGrid(iris, hue=\"Species\", size=5) \\\n   .map(plt.scatter, \"PetalLengthCm\", \"SepalLengthCm\") \\\n   .add_legend()","eb56e8ae":"sns.FacetGrid(iris, hue=\"Species\", size=5) \\\n   .map(plt.scatter, \"PetalWidthCm\", \"SepalLengthCm\") \\\n   .add_legend()","82e46974":"sns.pairplot(iris, hue=\"Species\", size=3)","94fd0dde":"le = LabelEncoder()\niris.Species = le.fit_transform(iris.Species)","3de46cee":"iris['Species'].value_counts()","3418396d":"iris.head()","05d8617e":"X = iris.loc[:,iris.columns != 'Species']\nX.head()","bf152deb":"y = iris[\"Species\"]\ny[:5]","b3925ce6":"print(X.shape)\nprint(y.shape)","ec9b5e58":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)","3545e0f8":"print(X_train.shape)\nprint(y_train.shape)","d9b92f4f":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","10b1c502":"lr = LogisticRegression(random_state = 0)\nlr.fit(X_train,y_train)","0564899c":"y_pred_train_lr = lr.predict(X_train)\ny_pred_test_lr = lr.predict(X_test)","dca161d4":"# Accuracy for Logistic Regression\nprint('Accuracy score for train data is:', accuracy_score(y_train,y_pred_train_lr))\nprint('Accuracy score for test data is:', accuracy_score(y_test,y_pred_test_lr))","8b245ee4":"# from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix = pd.DataFrame(confusion_matrix(y_test, y_pred_test_lr))\nconfusion_matrix.index = ['Actual 1','Actual 2','Actual 3']\nconfusion_matrix.columns = ['Predicted 1','Predicted 2','Predicted 3']\nprint(confusion_matrix)","74135f85":"lr_report = classification_report(y_test, y_pred_test_lr)\nprint(lr_report)","b8dd2d21":"dt = tree.DecisionTreeClassifier(random_state = 0)\ndt.fit(X_train, y_train)","9f05371f":"y_pred_train_dt = dt.predict(X_train)  \ny_pred_test_dt = dt.predict(X_test)  ","0036e9df":"# Accuracy for Decision Tree\nprint('Accuracy score for train data is:', accuracy_score(y_train,y_pred_train_dt))\nprint('Accuracy score for test data is:', accuracy_score(y_test,y_pred_test_dt))","0b2e22f5":"dt_report = classification_report(y_test, y_pred_test_dt)\nprint(dt_report)","f8a2e2cf":"rf = RandomForestClassifier(random_state = 0)\nrf.fit(X_train, y_train)","0e8a9401":"y_pred_train_rf = rf.predict(X_train)\ny_pred_test_rf = rf.predict(X_test)","c9462465":"# Accuracy for Random Forest\nprint('Accuracy score for train data:', accuracy_score(y_train,y_pred_train_rf))\nprint('Accuracy score for test data using the model without parameter specification:', accuracy_score(y_test,y_pred_test_rf))","696063ea":"rf_report = classification_report(y_test, y_pred_test_rf)\nprint(rf_report)","a7c91e63":"knn=KNeighborsClassifier(n_neighbors=3) \nknn.fit(X_train,y_train) ","a5158c33":"y_pred_train_knn = knn.predict(X_train)\ny_pred_test_knn = knn.predict(X_test)","eb783243":"# Accuracy for KNN\nprint('Accuracy score for train data:', accuracy_score(y_train,y_pred_train_knn))\nprint('Accuracy score for test data using the model without parameter specification:', accuracy_score(y_test,y_pred_test_knn))\n","def90f08":"knn_report = classification_report(y_test, y_pred_test_knn)\nprint(knn_report)","acb97aad":"nb = GaussianNB()\nnb.fit(X_train, y_train)","028deb33":"y_pred_train_nb = nb.predict(X_train)\ny_pred_test_nb = nb.predict(X_test)","b2a338d9":"# Accuracy for Naive Bayes\nprint('Accuracy score for train data:', accuracy_score(y_train,y_pred_train_nb))\nprint('Accuracy score for test data using the model without parameter specification:', accuracy_score(y_test,y_pred_test_nb))\n","4bab60bd":"nb_report = classification_report(y_test, y_pred_test_nb)\nprint(nb_report)","d71a0060":"svc = SVC()\nsvc.fit(X_train,y_train)","6551ef07":"y_pred_train_svc = svc.predict(X_train)\ny_pred_test_svc = svc.predict(X_test)","b4239d4b":"# Accuracy for Support Vector\nprint('Accuracy score for train data:', accuracy_score(y_train,y_pred_train_svc))\nprint('Accuracy score for test data using the model without parameter specification:', accuracy_score(y_test,y_pred_test_svc))\n","849cf7ba":"svc_report = classification_report(y_test, y_pred_test_svc)\nprint(svc_report)","600278b2":"model_names = ['--------------------Logistic Regression---------------------\\n',  \n               '\\n------------------Decsision Classifier------------------\\n', \n               '\\n----------------Random Forest Classifier------------\\n',\n              '\\n--------------------KNN Classifier------------\\n',\n              '\\n-----------------Naive Bayes Classifier------------\\n',\n              '\\n--------------Support Vector Classifier------------\\n']\nreport = model_names[0] + lr_report + model_names[1] + dt_report + model_names[2] + rf_report \\\n+ model_names[3] + knn_report + model_names[4] + nb_report + model_names[5] + svc_report\nprint(report)\n","442a609e":"MLA = [\n    #Ensemble Methods\n    ensemble.RandomForestClassifier(),\n\n    \n    #Linear Model\n    linear_model.LogisticRegression(),\n    \n    #Tree   \n    tree.DecisionTreeClassifier()  ,\n    \n    \n    ]","6f05800f":"#split dataset in cross-validation with this splitter class: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit\n#note: this is an alternative to train_test_split\ncv_split = model_selection.ShuffleSplit(n_splits = 5, test_size = .3, train_size = .7, random_state = 0 ) # run model 10x with 60\/30 split intentionally leaving out 10%\n","451304de":"#create table to compare MLA metrics\nMLA_columns = ['MLA Name', 'MLA Parameters','MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time', 'TrainTestDifference']\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\n#create table to compare MLA predictions\nMLA_predict = iris[[\"Species\"]]  # Y \n\n#index through MLA and save performance to table\nrow_index = 0\nFeature_Importance = {}","fb96ac8b":"Y = iris[\"Species\"].values.reshape(-1, 1)","dbad6097":"for alg in MLA:\n\n    #set name and parameters\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n    \n    #score model with cross validation: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n    cv_results = model_selection.cross_validate(alg, X, Y, cv  = cv_split,return_train_score=True,scoring='f1_weighted')\n\n    \n    # cv_result is a dictionary -> All the results of diff models are saved \n    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n    MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()   \n    #if this is a non-bias random sample, then +\/-3 standard deviations (std) from the mean, should statistically capture 99.7% of the subsets\n    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3   #let's know the worst that can happen!\n    #MLA_compare.loc[row_index, 'TrainTestDifference'] = cv_results['train_score'].mean() - cv_results['test_score'].mean() \n\n    #save MLA predictions - see section 6 for usage\n    alg.fit(X, Y)\n\n    try:\n      Feature_Importance[MLA_name] = alg.feature_importances_\n    except AttributeError:\n      pass\n      \n    MLA_predict[MLA_name] = alg.predict(X)\n    \n    row_index+=1\n    \n#print and sort table: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.sort_values.html\nMLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\nMLA_compare\nMLA_compare['TrainTestDifference'] = (MLA_compare['MLA Test Accuracy Mean']-MLA_compare['MLA Train Accuracy Mean'])*100\nMLA_compare   ","743656f4":"#class 0 [0,0,1]\n#class 1 [0,1,0]\n#class 2 [0,0,1]","dafee3fe":"y = to_categorical(y)","f6448081":"y.shape","b5799bca":"X = X.values","d49732a8":"X.shape","2d055f89":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)","606ee052":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler().fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","072f473a":"from keras.models import Sequential\nfrom keras.layers import Dense","a851ae97":"model=Sequential()\nmodel.add(Dense(16, activation='relu', input_dim=4))\nmodel.add(Dense(32, activation='relu', input_dim=4))\nmodel.add(Dense(3, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\nmodel.summary()             ","37e101fa":"model.fit(X_train, y_train,epochs=150, verbose=2)","ccae4cc7":"predictions = model.predict_classes(X_test)","ad1d4f32":"y_test.argmax(axis=1)","5c86549c":"# Accuracy for Random Forest\nprint('Accuracy score for test data:', accuracy_score(y_test.argmax(axis=1),predictions))\n","65dbe4ec":"nn_report = classification_report(y_test.argmax(axis=1), predictions)\nprint(nn_report)","5c3a2c9c":"# from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import confusion_matrix\ncm = pd.DataFrame(confusion_matrix(y_test.argmax(axis=1), predictions))\ncm.index = ['Actual 1','Actual 2','Actual 3']\ncm.columns = ['Predicted 1','Predicted 2','Predicted 3']\nprint(cm)","271ef185":"model.save('iris_nn_model.h5')","5682af2d":"from keras.models import load_model\nnew_model = load_model('iris_nn_model.h5')","04f92856":"new_model.predict_classes(X_test)","ebdf5ac4":"<a id=section1><\/a> \n# 1. Problem Statement","8c746603":"<a id=section407><\/a> \n## 4.7 Naive Bayes Classifier ","41d3ac17":"<a id=section401><\/a> \n## 4.1 Preparing X and y using pandas","df256b92":"<a id=section5><\/a> \n# 5. Model Evaluation","89dc8885":"### Scatter Plots","d165afaa":"## Standardizing the variables","0e118e12":"# Iris Classifier","d66c1406":"<a id=section2><\/a> \n## 2. Load the packages and data\n<section1>\n<br\/>","4abc358c":"Given the dataset containing Iris Species of flowers, different Machine Learning models to predict the species of a flower","692da3f5":"<a id=section303><\/a> \n## 3.3 Handling Categorical Features ","0e8c70fe":"#### Looking at the count plot,  It is an balanced dataset","2d492067":"<a id=section301><\/a> \n## 3.1 Missing Values","cef3ca7c":"<a id=section403><\/a> \n## 4.3 Logistic Regression ","0be9d835":"<a id=section8><\/a> \n# 8. Conclusion","10fb7395":"<a id=section406><\/a> \n## 4.6 KNN Classifier ","b8e30f01":"<a id=section3><\/a> \n# 3. Preprocessing","8b127e00":"There is no missing value in the dataset","805af369":"**KNN Classifier is the best model in terms of accuracy comparison to other models evaluated**","4ab0a365":"<a id=section302><\/a> \n## 3.2 Find Correlation","e4bbdbb2":"### Removing id column","2ffb7ae4":"<a id=section404><\/a> \n## 4.4 Decision Tree Classifier","c7729807":"<a id=section4><\/a> \n# 4. Model Development","ff1b88ab":"<a id=section7><\/a> \n# 7. Neural Network Approach","6e5bbaca":"<a id=section6><\/a> \n# 6. Auto ML\n","3f744097":"<a id=section402><\/a> \n## 4.2 Splitting X and y into training and test datasets.\n ","9fe58b66":"<a id=section405><\/a> \n## 4.5 Random Forest Classifier ","2614b7ad":"<a id=section408><\/a> \n## 4.8 Support Vector Classifier ","ecd23341":"# Table of Contents\n-  [Problem Statement](#section1)<\/br>\n-  [Load the packages and data](#section2)<\/br>\n-  [Preprocessing](#section3)<\/br>\n    - [Missing Values](#section301)<br\/>\n    - [Correlation between Target and Features](#section302)<br\/>\n    - [Handling Categorical Features](#section303)<br\/>\n-  [Model Development](#section4)<\/br>\n    - [Preparing X and y using pandas](#section401)<br\/>\n    - [Splitting X and y into training and test datasets](#section402)<br\/>\n    - [Logistic Regression](#section403)<br\/>\n    - [Decision Tree Classifier](#section404)<br\/>\n    - [Random Forest Classifier](#section405)<br\/>\n    - [KNN Classifier](#section406)<br\/>\n    - [Naive Bayes Classifier](#section407)<br\/>\n    - [Support Vector Classifier](#section408)<br\/>\n-  [Model Evaluation](#section5)<\/br>\n-  [Auto ML](#section6)<\/br>\n-  [Neural Network Approach](#section7)<br\/>\n-  [Conclusion](#section8)<br\/>","2542fd3f":"#### Looking at the coorelation heatmap\n- PetalLengnthCm and PetalWidthCm are positively correlated\n- PetalWidthCm and SepalLengthCm are positively correlated\n- SepalLengthCm and PetalLengnthCm are positively coorelated","634d9073":"#### Encoding the Target Variable - Species"}}