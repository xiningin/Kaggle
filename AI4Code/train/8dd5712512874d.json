{"cell_type":{"cf3b6ddc":"code","9cb81c73":"code","f8529845":"code","63b4ae5b":"code","06b3aab2":"code","48d1615b":"code","8366f17e":"code","f891683f":"code","4b0dd40c":"code","8fc3b55c":"code","7e9d66e8":"code","1c00674f":"code","c0e14f86":"code","c5f65341":"code","8e355d58":"code","1976e9dc":"code","2c0540d6":"code","94159b01":"code","e51da17a":"code","be4df735":"code","2c0e4134":"code","589306df":"code","4eab518c":"code","9a6c66b7":"code","dd7f6a04":"code","bd1dca61":"code","4ee94b57":"markdown","d7a0bf70":"markdown","4bb4131d":"markdown"},"source":{"cf3b6ddc":"!pip freeze > requirements.txt","9cb81c73":"# Standard Imports for working with data\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pickle\nimport scipy.io\nimport tarfile\nimport csv\nimport sys\nimport os\n\n\n# Frameworks for working with Neural Networks\nimport tensorflow as tf\nimport random\nfrom keras.models import Model\nfrom keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding, BatchNormalization\nfrom keras.preprocessing import sequence\n\nfrom sklearn.model_selection import train_test_split\nimport warnings\n\n# Default parameters for visualizations\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 10, 5\n%config InlineBackend.figure_format = 'svg'\n%matplotlib inline\nwarnings.filterwarnings('ignore')\n\n\n# Datasets available in directory\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n\n# Print modules versions\nprint(f\"\"\"\nPython: {sys.version[0:3]}\nTensorflow: {tf.__version__}\nKeras: {tf.keras.__version__}\n\"\"\")","f8529845":"# SETUP\nSEED = 42\nrandom.seed = SEED\nnp.random.seed(seed = SEED)\n\n\n# MODEL\nBATCH_SIZE = 128\nEPOCH = 15\nVAL_SPLIT = 0.2\n\n# TOKENIZER\n# the maximum number of words to be used. (most frequent)\nMAX_WORDS = 20000\n# Max number of words in each complaint\nMAX_SEQUENCE_LENGTH = 150\n\nDATA_PATH = '\/kaggle\/input\/sf-dl-movie-genre-classification\/'","63b4ae5b":"train_df = pd.read_csv(DATA_PATH + 'train.csv')","06b3aab2":"train_df.head(3)","48d1615b":"train_df.info()","8366f17e":"train_df.genre.value_counts(normalize = True).plot(kind = 'bar', figsize = (12, 4), fontsize = 8)\nplt.xticks(rotation = 60)\nplt.xlabel(\"Genres\", fontsize = 8)\nplt.ylabel(\"Percentage from total dist., %\", fontsize = 8)","f891683f":"# TEST \ntest_df = pd.read_csv(DATA_PATH + 'test.csv')\ntest_df.head(4)","4b0dd40c":"# TARGET VARIABLE DUMMIFY\nY = pd.get_dummies(train_df.genre)\nCLASS_NUM = Y.shape[1]\nprint(\"Shape of the label tensor: \", Y.shape)","8fc3b55c":"Y.head()","7e9d66e8":"# to build a dictionary we will use all text\nall_text = train_df.text.append(test_df.text, ignore_index = True)","1c00674f":"from tensorflow.keras.preprocessing.text import Tokenizer","c0e14f86":"%%time \ntokenize = Tokenizer(num_words = MAX_WORDS)\ntokenize.fit_on_texts(all_text)","c5f65341":"# Now we will translate our text to the vector\n\nsequences = tokenize.texts_to_sequences(train_df.text)\nsequences_matrix = sequence.pad_sequences(sequences, maxlen = MAX_SEQUENCE_LENGTH)\nprint(sequences_matrix.shape)","8e355d58":"print(train_df.text[1])\nprint(sequences_matrix[1])","1976e9dc":"def RNN():\n    inputs = Input(name = 'inputs', shape = [MAX_SEQUENCE_LENGTH])\n    layer = Embedding(MAX_WORDS, 50, input_length = MAX_SEQUENCE_LENGTH)(inputs)\n    layer = LSTM(100)(layer)\n    layer = Dense(256, activation = 'elu', name = 'FC1')(layer)\n    layer = Dropout(0.5)(layer)\n    layer = Dense(CLASS_NUM, activation = 'sigmoid', name = 'out_layer')(layer)\n    model = Model(inputs = inputs, outputs = layer)\n    return model","2c0540d6":"model = RNN()\nmodel.summary()\nmodel.compile(loss = 'categorical_crossentropy', optimizer = 'Adamax', metrics = ['accuracy'])","94159b01":"history = model.fit(sequences_matrix, Y, batch_size = BATCH_SIZE, epochs = EPOCH, validation_split = VAL_SPLIT)","e51da17a":"def plot_history(history):\n    plt.figure(figsize = (10, 5))\n    plt.style.use('dark_background')\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    \n    epochs = range(len(acc))\n    \n    plt.plot(epochs, acc, 'b', label = 'Training accuracy')\n    plt.plot(epochs, val_acc, 'r', label = 'Validation accuracy')\n    plt.title(\"Training and validation accuracy\")\n    plt.legend()\n    \n    \n    plt.figure(figsize = (10,5))\n    plt.style.use('dark_background')\n    plt.plot(epochs, loss, 'b', label = 'Training loss')\n    plt.plot(epochs, val_loss, 'r', label = 'Validation loss')\n    plt.title('Training and Validation loss')\n    plt.legend()\n    \n    plt.show()\n","be4df735":"plot_history(history)","2c0e4134":"model.save('keras_nlp_lstm.h5')","589306df":"test_sequences = tokenize.texts_to_sequences(test_df.text)\ntest_sequences_matrix = sequence.pad_sequences(test_sequences, maxlen = MAX_SEQUENCE_LENGTH)","4eab518c":"%%time\npredict_proba = model.predict(test_sequences_matrix)","9a6c66b7":"# always save predict proba to build ensembles in future\npredict_proba = pd.DataFrame(predict_proba, columns = Y.columns)\npredict_proba.to_csv('predict_proba.csv', index = False)\npredict_proba.head()","dd7f6a04":"predict_genre = Y.columns[np.argmax(predict_proba.values, axis=1)]","bd1dca61":"submission = pd.DataFrame({'id':range(1, len(predict_genre)+1), \n                           'genre':predict_genre}, \n                          columns=['id', 'genre'])\n\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()\n","4ee94b57":"#### MODEL","d7a0bf70":"## 1st Attempt\n### **Preparation of the DataSets for modelling.**","4bb4131d":"#### Data Tokenization\n\n\nData tokenization is the first step in text preprocessing. We divide long sentences of the text to the small ones (corpus to sentences, sentences to words and etc.). As the result we get a kind of the dictionary which will help us to vectorize the initial text.\nKeras has Tokenizer method that helps to preprocess the text by one-line script. It is easy and intuitive to use.\nhttps:\/\/machinelearningmastery.com\/prepare-text-data-deep-learning-keras\/"}}