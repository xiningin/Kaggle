{"cell_type":{"1f3912b5":"code","51539c06":"code","d8e15a9d":"code","f6e5b3ff":"code","0200a90e":"code","5b6ea9d4":"code","9aec67d0":"code","fb3c4e34":"code","fd1c89d7":"code","9550a838":"code","6d4b8e30":"code","39b9a93c":"code","46a802b9":"code","0ed600f3":"code","477c5708":"code","dbe5d96a":"markdown","43ab361d":"markdown"},"source":{"1f3912b5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","51539c06":"import os\nimport sys\nimport tarfile\nimport torch\nimport collections\nimport torchvision\nfrom torchvision.datasets import VisionDataset\nfrom torchvision.transforms import functional as F\n\n\nif sys.version_info[0] == 2:\n    import xml.etree.cElementTree as ET\nelse:\n    import xml.etree.ElementTree as ET\n\nfrom PIL import Image\nfrom torchvision.datasets.utils import download_url, check_integrity, verify_str_arg","d8e15a9d":"classes = [\"background\", 'crack', 'bullseye', 'scratch']","f6e5b3ff":"# Defining Dataset Class\nclass VOCDetection(VisionDataset):\n    def __init__(self,root,year='2012',image_set='train',download=False,transform=None,target_transform=None,transforms=None):\n        super(VOCDetection, self).__init__(root, transforms, transform, target_transform)\n\n        image_dir = os.path.join(\"..\/input\/internvoc\/intern\", 'images')\n        annotation_dir = os.path.join(\"..\/input\/internvoc\/intern\", 'annotations')\n\n        self.images = [os.path.join(image_dir, x) for x in os.listdir(image_dir)]\n        self.annotations = [os.path.join(annotation_dir, x ) for x in os.listdir(annotation_dir)]\n        assert (len(self.images) == len(self.annotations))\n\n    def __getitem__(self, index):\n        \"\"\"\n        Args:\n            index (int): Index\n        Returns:\n            tuple: (image, target) where target is a dictionary of the XML tree.\n        \"\"\"\n        img = Image.open(self.images[index]).convert('RGB')\n        raw_target = self.parse_voc_xml(ET.parse(self.annotations[index]).getroot())\n        \n        target = {}\n        boxes = []\n        labels = []\n        try:\n            for that in raw_target[\"annotation\"][\"object\"]:\n                boxes.append(list(map(float, list(that[\"bndbox\"].values()))))\n                labels.append(classes.index(that[\"name\"]))\n            target[\"boxes\"] = torch.tensor(boxes)\n            target[\"labels\"] = torch.tensor(labels)\n        except TypeError:\n            boxes.append(list(map(float, list(raw_target[\"annotation\"][\"object\"][\"bndbox\"].values()))))\n            labels.append(classes.index(raw_target[\"annotation\"][\"object\"][\"name\"]))\n            target[\"boxes\"] = torch.tensor(boxes)\n            target[\"labels\"] = torch.tensor(labels)\n        target[\"image_id\"] = torch.tensor([int(raw_target[\"annotation\"][\"filename\"].split(\".\")[0])])\n        \n    \n        if self.transforms :\n            img, target = self.transforms(img, target)\n            \n        return img, target\n\n    def __len__(self):\n        return len(self.images)\n\n    def parse_voc_xml(self, node):\n        voc_dict = {}\n        children = list(node)\n        if children:\n            def_dic = collections.defaultdict(list)\n            for dc in map(self.parse_voc_xml, children):\n                for ind, v in dc.items():\n                    def_dic[ind].append(v)\n            if node.tag == 'annotation':\n                def_dic['object'] = [def_dic['object']]\n            voc_dict = {\n                node.tag:\n                    {ind: v[0] if len(v) == 1 else v\n                     for ind, v in def_dic.items()}\n            }\n        if node.text:\n            text = node.text.strip()\n            if not children:\n                voc_dict[node.tag] = text\n        return voc_dict\n\n\ndef download_extract(url, root, filename, md5):\n    download_url(url, root, filename, md5)\n    with tarfile.open(os.path.join(root, filename), \"r\") as tar:\n        tar.extractall(path=root)","0200a90e":"datadict = VOCDetection(\"..\/input\/internvoc\/intern\", year=\"2007\", download=False, transforms=False)","5b6ea9d4":"!git clone https:\/\/github.com\/adityak2920\/FasterRCNN\n!pip install pycocotools","9aec67d0":"import sys\nsys.path.append(\"FasterRCNN\/src\/\")\n\nimport transforms as T\nimport utils\nfrom engine import train_one_epoch, evaluate\n\ndef get_transform(train):\n    transforms = []\n    transforms.append(T.ToTensor())\n    if train:\n        transforms.append(T.RandomHorizontalFlip(0.5))\n    return T.Compose(transforms)","fb3c4e34":"def faster_rcnn_model(num_classes, device):\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n\n    # get number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n    model.to(device)\n    \n    return model","fd1c89d7":"from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","9550a838":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\ndataset = VOCDetection(\"..\/input\/internvoc\/intern\", year=\"2007\", download=False, transforms=get_transform(train=True))\nindices = torch.randperm(len(dataset)).tolist()\ndataset = torch.utils.data.Subset(dataset, indices)\n\n\ndata_loader = torch.utils.data.DataLoader(\n    dataset, batch_size=4, shuffle=True, num_workers=8,\n    collate_fn=utils.collate_fn)\n\n\nnum_classes = 4  #  classes + background \nmodel = faster_rcnn_model(num_classes, device)\n\n#defining paremeters for training\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005,\n                          momentum=0.9, weight_decay=0.0005)\n# and a learning rate scheduler\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n                                             step_size=3,\n                                             gamma=0.1)\n\n\nnum_epochs = 10\n\nfor epoch in range(num_epochs):\n  # train for one epoch, printing every 10 iterations\n  train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n  # update the learning rate\n  lr_scheduler.step()\n  # evaluate on the test dataset\n#       evaluate(model, data_loader_test, device=device)\n\nprint(\"That's it!\")\n","6d4b8e30":"model.eval()\nevalt = T.ToTensor()\nannot = model([F.to_tensor(Image.open(\"..\/input\/internvoc\/intern\/images\/46.jpg\")).cuda()])","39b9a93c":"annot","46a802b9":"x1 = annot[0][\"boxes\"].cpu().detach().numpy()[0][0]\ny1 = annot[0][\"boxes\"].cpu().detach().numpy()[0][1]\nx2 = annot[0][\"boxes\"].cpu().detach().numpy()[0][2]\ny2 = annot[0][\"boxes\"].cpu().detach().numpy()[0][3]","0ed600f3":"classes[int(annot[0][\"labels\"])]","477c5708":"import matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport numpy as np\n\nimg = np.array(Image.open(\"..\/input\/internvoc\/intern\/images\/46.jpg\"), dtype=np.uint8)\n# Create figure and axes\nfig,ax = plt.subplots(1)\n\n# Display the image\nax.imshow(img)\n\nwidth = x2 - x1\nheight = y2 - y1\nrect = patches.Rectangle((x1,y1),width,height,linewidth=1,edgecolor='r',facecolor='none', label = classes[int(annot[0][\"labels\"])])\nax.add_patch(rect)\n\nplt.show()","dbe5d96a":"## Training","43ab361d":"## Defining dataset class"}}