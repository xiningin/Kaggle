{"cell_type":{"9cd23992":"code","8fea166b":"code","a5972c0f":"code","656954b7":"code","c2fadc83":"code","ffd33ce5":"code","467b06d2":"code","0529f86a":"code","db7c7edc":"code","3b9189f0":"code","9887e630":"code","3f0c45b6":"code","3272bb3b":"code","1368db4b":"code","c4c2891b":"code","682cd4b5":"code","2c2c965f":"code","05ab5346":"code","2b2aa00c":"code","5cfb9b67":"code","1a11cba1":"code","53a1c05c":"code","3a7cd014":"code","64819c7a":"code","a34d32ce":"code","b3e75c4b":"code","c13cd5d3":"code","7be11dbd":"code","13ebaa25":"code","3438923a":"code","67b5aeaa":"code","7f15e9a0":"code","16aa8d33":"code","af54c647":"code","b21924f1":"code","70376a48":"code","8cee77e6":"code","06456c59":"code","192bc03c":"code","28397038":"code","5576f58b":"code","e493a5bb":"code","107b0fdb":"code","c1147efe":"code","19eb64f9":"code","47a9064b":"code","204b0cf3":"code","67dfc505":"code","283944d4":"code","e137e8fa":"code","04c2458b":"code","46e21428":"code","94c49647":"code","556aaea0":"code","f2bd0bb2":"code","cb5ef744":"code","7559311f":"code","91784eb1":"code","29c2e895":"code","b6c92859":"code","a6d9204f":"code","0e189595":"code","5c24eb59":"code","57017667":"code","a0c347b7":"code","be4ff127":"code","db63b8bf":"code","a4877295":"code","243848e2":"code","83d3946a":"code","4f06ae18":"code","a7a2ad3f":"code","8e0fcade":"code","11311d28":"code","a04aa25a":"code","cedf1e47":"code","2a0eda77":"code","f98a413d":"code","b79e46f7":"code","2a66f4bd":"code","0c2d6d68":"code","86719e25":"code","f12dddde":"code","84ec238b":"code","9a6b04e0":"code","6789dae1":"code","974a8e5a":"code","1b5852a0":"code","4950ae96":"code","14bcddd5":"code","c512e603":"code","8d7f03e9":"code","46c3515b":"code","8f6d23f6":"code","08a32618":"code","c82d1682":"code","aa608f46":"code","f14da28d":"code","3839c702":"code","da50bd69":"code","4557752c":"code","0069266b":"code","102b1f9c":"code","1cdf7557":"code","8a9ed1f9":"code","f45f0c8b":"code","b442f958":"code","f6df78f1":"code","ac15d026":"code","3a33ef65":"code","f31619ae":"code","562b85f6":"code","97259af2":"code","5804e2f0":"code","41c460eb":"code","5abec71b":"code","658f39cf":"code","c4560c1f":"code","c5d4a05c":"code","8d66e2d1":"code","9171d2b7":"code","27eca9e3":"code","0484f249":"code","84ed1dba":"code","22079e2b":"code","0270769c":"code","eb1b98cd":"code","973bcb61":"code","902533a2":"code","d57bd266":"code","d2cc74e5":"code","e57b3d63":"code","6e876fef":"code","88bcd8f9":"code","af6a7138":"code","0a025ee3":"code","c6997cc7":"code","90893e14":"code","c6fa4235":"code","55a7a773":"code","d898a1c5":"code","4703185e":"code","1cd1c17a":"code","3348a6e1":"code","da5d7838":"code","9d5495b6":"code","aa18abdf":"code","82afb248":"code","21c547b5":"code","92a28da4":"code","62170f24":"code","eaee76a1":"code","c39beac4":"code","75f353b6":"code","a4f116f6":"code","cccc8b5b":"code","0e0a8c1c":"code","568e601d":"code","053bc56d":"code","71e08887":"code","1f2408a6":"code","16c1a075":"code","a74edb10":"code","a9dec553":"code","c7cd8ae2":"code","5ecf9ca9":"code","cfdcd188":"code","2fe78949":"code","045797b1":"code","1f171e42":"code","0e0b8d0b":"code","e1fc1173":"code","03ef5411":"code","31e1aa68":"code","64bb1204":"markdown","6d1e1819":"markdown","f53a0491":"markdown","09ed9579":"markdown","3510bab3":"markdown","b584a0ad":"markdown","1cd030d9":"markdown","48aee8f6":"markdown","876258cd":"markdown","d40d359a":"markdown","b90e88d8":"markdown","87824d53":"markdown","958d5859":"markdown","d895a9e0":"markdown","c4e24490":"markdown","31d469e0":"markdown","4f7a7335":"markdown","bcb5b7cc":"markdown","0ebe2363":"markdown","3f0a9c49":"markdown","eecd467c":"markdown","4c09be58":"markdown","d68e4db8":"markdown","cd3c709a":"markdown","24c3f8fa":"markdown","429fae4a":"markdown","a69beab1":"markdown","cd9880d5":"markdown","0f8abf1d":"markdown","18a44f05":"markdown","3150c333":"markdown","05e37f50":"markdown","e09180f4":"markdown","78b257eb":"markdown","8edb9366":"markdown","903e401c":"markdown","a595630b":"markdown","65012314":"markdown","98852bb4":"markdown","75de1aa6":"markdown"},"source":{"9cd23992":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","8fea166b":"transactions=pd.read_csv(\"\/kaggle\/input\/ucrfinal\/datasetForFinalAssignment.csv\")","a5972c0f":"transactions.info()","656954b7":"test=pd.read_csv(\"\/kaggle\/input\/ucrfinal\/datasetForFinalTest.csv\")","c2fadc83":"test.info()","ffd33ce5":"transactions.rename(columns={'signup_time-purchase_time':'TimeBetween',\n                          'N[device_id]':'TimesDeviceUsed',\n                          }, \n                 inplace=True)","467b06d2":"TimesIPaddress = transactions['ip_address'].value_counts()\nTimesIPaddress.values","0529f86a":"transactions[\"TimesIPaddress\"] = transactions.groupby('ip_address')['user_id'].transform('count')","db7c7edc":"pd.pivot_table(transactions, index=\"class\", values=[\"TimeBetween\",\"TimesDeviceUsed\",\"TimesIPaddress\",\"purchase_value\",\"age\"], aggfunc=\"mean\")","3b9189f0":"E = transactions[transactions['class'] == 1]\nF= transactions[transactions['class'] == 0]\nE[\"purchase_value\"].describe()","9887e630":"F[\"purchase_value\"].describe()","3f0c45b6":"transactions['class'].value_counts().plot.bar()\ntransactions['class'].value_counts()","3272bb3b":"(7007\/74691)*100","1368db4b":"sns.countplot(x='source',hue=\"class\",data=transactions) ","c4c2891b":"sns.countplot(x='browser', hue=\"class\",data=transactions)\nplt.xticks(rotation=-45)","682cd4b5":"sns.countplot(x='sex', hue=\"class\",data=transactions) \n","2c2c965f":"import seaborn as sns\nsns.boxplot(x=\"class\", y=\"TimesDeviceUsed\", data=transactions) ","05ab5346":"sns.boxplot(x=\"class\", y=\"purchase_value\", data=transactions)","2b2aa00c":"sns.boxplot(x=\"class\", y=\"age\", data=transactions)","5cfb9b67":"sns.boxplot(x=\"class\", y=\"TimeBetween\", data=transactions) ","1a11cba1":"sns.countplot(data=transactions, x=\"age\",hue=\"class\")","53a1c05c":"from sklearn.preprocessing import LabelEncoder\nlb_make = LabelEncoder()\ntransactions['sex'] = lb_make.fit_transform(transactions['sex'])\nsourcedummies=pd.get_dummies(transactions[\"source\"], columns=[\"source\"])\nbrowserdummies=pd.get_dummies(transactions[\"browser\"], columns=[\"browser\"])\n\n","3a7cd014":"transactions = pd.concat([transactions, sourcedummies], axis=1)\ntransactions = pd.concat([transactions, browserdummies], axis=1)\n\n\n","64819c7a":"transactions.info()","a34d32ce":"y = transactions[\"class\"]\n\nX = transactions[[\"age\",\"sex\",\"TimeBetween\",\"purchase_value\",\"TimesDeviceUsed\",\"TimesIPaddress\",\"Chrome\",\"FireFox\",\"IE\",\"Opera\",\"Safari\",\"Ads\",\"Direct\",\"SEO\"]]\n\n","b3e75c4b":"I=X.copy(deep=True)\n","c13cd5d3":"from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nX = pd.DataFrame(ss.fit_transform(X),columns = X.columns)\nX.head()","7be11dbd":"plt.figure(figsize = (14,14))\nplt.title(' Transactions features correlation plot (Pearson)')\ncorr = I.corr()\nsns.heatmap(corr,xticklabels=corr.columns,yticklabels=corr.columns,linewidths=.1,cmap=\"Reds\")\nplt.show()","13ebaa25":"from sklearn.model_selection import train_test_split\n\n\nX_train, X_test, y_train, y_test =   train_test_split(X,y,test_size = 0.1, stratify = y)","3438923a":"X_train.shape, X_test.shape","67b5aeaa":"type(X_test)","7f15e9a0":"indexA=X_test.index.values\nindexA","16aa8d33":"A=I.iloc[indexA]\n","af54c647":"from imblearn.over_sampling import SVMSMOTE\n#sm = SMOTE(sampling_strategy='str', random_state=None, k_neighbors=2, m_neighbors='deprecated', out_step='deprecated', kind='deprecated', svm_estimator='deprecated', n_jobs=1, ratio=1.0)\nsm=SVMSMOTE(sampling_strategy='auto', random_state=None, k_neighbors=5, n_jobs=1, m_neighbors=10, svm_estimator=None, out_step=0.5)\nfrom imblearn.over_sampling import SMOTE, ADASYN\n#sm = SMOTE(random_state=1)\nX_res, y_res = sm.fit_sample(X_train, y_train)\n#ada = ADASYN(random_state=42)\n#X_res, y_res = ada.fit_sample(X_train, y_train)\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n#X_res = scaler.fit_transform(X_res)\n#X_test = scaler.fit_transform(X_test)  \n","b21924f1":"X_res.shape, y_res.shape","70376a48":"\n                   \nnp.sum(y_res)\/len(y_res)","8cee77e6":"from sklearn.tree import  DecisionTreeClassifier as dt\nfrom sklearn.ensemble import RandomForestClassifier as rf\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier as gbm\nfrom xgboost.sklearn import XGBClassifier\nimport lightgbm as lgb\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn import svm\nfrom lightgbm import LGBMClassifier\nimport xgboost as xgb\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import average_precision_score\nimport sklearn.metrics as metrics\nimport time\npd.set_option('display.max_columns', 100)\n\n\nRFC_METRIC = 'gini'  #metric used for RandomForrestClassifier\nNUM_ESTIMATORS = 100 #number of estimators used for RandomForrestClassifier\nNO_JOBS = 4 #number of parallel jobs used for RandomForrestClassifier\n\n\n","06456c59":"from sklearn.tree import ExtraTreeClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm.classes import OneClassSVM\nfrom sklearn.neural_network.multilayer_perceptron import MLPClassifier\nfrom sklearn.neighbors.classification import RadiusNeighborsClassifier\nfrom sklearn.neighbors.classification import KNeighborsClassifier\nfrom sklearn.multioutput import ClassifierChain\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.multiclass import OutputCodeClassifier\nfrom sklearn.multiclass import OneVsOneClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.linear_model.stochastic_gradient import SGDClassifier\nfrom sklearn.linear_model.ridge import RidgeClassifierCV\nfrom sklearn.linear_model.ridge import RidgeClassifier\nfrom sklearn.linear_model.passive_aggressive import PassiveAggressiveClassifier    \nfrom sklearn.gaussian_process.gpc import GaussianProcessClassifier\nfrom sklearn.ensemble.weight_boosting import AdaBoostClassifier\nfrom sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\nfrom sklearn.ensemble.bagging import BaggingClassifier\nfrom sklearn.ensemble.forest import ExtraTreesClassifier\nfrom sklearn.ensemble.forest import RandomForestClassifier\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.semi_supervised import LabelPropagation\nfrom sklearn.semi_supervised import LabelSpreading\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.naive_bayes import MultinomialNB  \nfrom sklearn.neighbors import NearestCentroid\nfrom sklearn.svm import NuSVC\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.svm import SVC\n#from sklearn.mixture import DPGMM\n#from sklearn.mixture import GMM \nfrom sklearn.mixture import GaussianMixture\n#from sklearn.mixture import VBGMM\n","192bc03c":"clfdt =dt(criterion='gini',\n    splitter='best',\n    max_depth=None,\n    min_samples_split=2,\n    min_samples_leaf=1,\n    min_weight_fraction_leaf=0.0,\n    max_features=None,\n    random_state=None,\n    max_leaf_nodes=None,\n    min_impurity_decrease=0.0,\n    min_impurity_split=None,\n    class_weight=None,\n    presort=False,)","28397038":"start = time.time()\nclfdt = clfdt.fit(X_res,y_res)\nend = time.time()\n(end-start)\/60                   \n","5576f58b":"classes = clfdt.predict(X_test)\nclasses","e493a5bb":"f  = confusion_matrix( y_test, classes )#confusion_matrix(y_true, y_pred)\nf\n","107b0fdb":"tn,fp,fn,tp = f.ravel() #tn, fp, fn, tp = f.ravel()\n(tn, fp, fn, tp)","c1147efe":"precision = tp\/(tp+fp)\nprecision                    \n","19eb64f9":"recall = tp\/(tp + fn)\nrecall ","47a9064b":"accuracy=(tp+tn)\/(tp+tn+fp+fn)\naccuracy","204b0cf3":"from sklearn import metrics\nclfdt.score(X_res,y_res)","67dfc505":"from sklearn.metrics import r2_score\nr2_score(y_test,classes )","283944d4":"from sklearn.metrics import f1_score\nf1_score(y_test, classes, average='binary')","e137e8fa":"\nA['y_test'] = y_test.tolist()","04c2458b":"A[\"classes\"]=classes.tolist()","46e21428":"P=A.query(\" classes==0\")\n","94c49647":"Z=P.query(\"y_test!= classes\")\n","556aaea0":"CostOfFalseNegative=Z[\"purchase_value\"].sum(axis = 0, skipna = True) \nCostOfFalseNegative","f2bd0bb2":"costOfFaslePositive=8*fp\ncostOfFaslePositive\n","cb5ef744":"costOfWorngPrediction=costOfFaslePositive+CostOfFalseNegative\ncostOfWorngPrediction","7559311f":"clfrf =rf(n_estimators='warn',\n    criterion='entropy',\n    max_depth=None,\n    min_samples_split=2,\n    min_samples_leaf=1,\n    min_weight_fraction_leaf=0.0,\n    max_features='auto',\n    max_leaf_nodes=None,\n    min_impurity_decrease=0.0,\n    min_impurity_split=None,\n    bootstrap=True,\n    oob_score=False,\n    n_jobs=None,\n    random_state=None,\n    verbose=0,\n    warm_start=False,\n    class_weight=None,)","91784eb1":"start = time.time()\nclfrf = clfrf.fit(X_res,y_res)\nend = time.time()\n(end-start)\/60 ","29c2e895":"classes = clfrf.predict(X_test)\nclasses","b6c92859":"f  = confusion_matrix( y_test, classes )#confusion_matrix(y_true, y_pred)\nf","a6d9204f":"tn,fp,fn,tp = f.ravel() #tn, fp, fn, tp = f.ravel()\n(tn, fp, fn, tp)","0e189595":"precision = tp\/(tp+fp)\nprecision","5c24eb59":"recall = tp\/(tp + fn)\nrecall","57017667":"accuracy=(tp+tn)\/(tp+tn+fp+fn)\naccuracy","a0c347b7":"from sklearn.metrics import r2_score\nr2_score(y_test,classes )","be4ff127":"from sklearn import metrics\nclfrf.score(X_res,y_res)","db63b8bf":"from sklearn.metrics import f1_score\nf1_score(y_test, classes, average='binary')","a4877295":"A['y_test'] = y_test.tolist()\nA[\"classes\"]=classes.tolist()\nP=A.query(\" classes==0\")\nZ=P.query(\"y_test!= classes\")\nCostOfFalseNegative=Z[\"purchase_value\"].sum(axis = 0, skipna = True) \nCostOfFalseNegative","243848e2":"costOfFaslePositive=8*fp\ncostOfFaslePositive","83d3946a":"costOfWorngPrediction=costOfFaslePositive+CostOfFalseNegative\ncostOfWorngPrediction","4f06ae18":"clfetc=ExtraTreesClassifier( n_estimators='warn',\n    criterion='gini',\n    max_depth=None,\n    min_samples_split=2,\n    min_samples_leaf=1,\n    min_weight_fraction_leaf=0.0,\n    max_features='auto',\n    max_leaf_nodes=None,\n    min_impurity_decrease=0.0,\n    min_impurity_split=None,\n    bootstrap=False,\n    oob_score=False,\n    n_jobs=None,\n    random_state=None,\n    verbose=0,\n    warm_start=False,\n    class_weight=None,)","a7a2ad3f":"start = time.time()\nclfetc = clfetc.fit(X_res,y_res)\nend = time.time()\n(end-start)\/60 ","8e0fcade":"classes = clfetc.predict(X_test)\nclasses","11311d28":"f  = confusion_matrix( y_test, classes )#confusion_matrix(y_true, y_pred)\ntn,fp,fn,tp = f.ravel() #tn, fp, fn, tp = f.ravel()\n(tn, fp, fn, tp)","a04aa25a":"precision = tp\/(tp+fp)\nrecall = tp\/(tp + fn)\naccuracy=(tp+tn)\/(tp+tn+fp+fn)\nprint( precision, recall, accuracy)\n","cedf1e47":"from sklearn import metrics\nclfetc.score(X_res,y_res)","2a0eda77":"from sklearn.metrics import r2_score\nfrom sklearn.metrics import f1_score\nr2_score(y_test,classes ),f1_score(y_test, classes, average='binary')","f98a413d":"A['y_test'] = y_test.tolist()\nA[\"classes\"]=classes.tolist()\nP=A.query(\" classes==0\")\nZ=P.query(\"y_test!= classes\")\nCostOfFalseNegative=Z[\"purchase_value\"].sum(axis = 0, skipna = True) \nCostOfFalseNegative\ncostOfFaslePositive=8*fp\ncostOfFaslePositive\ncostOfWorngPrediction=costOfFaslePositive+CostOfFalseNegative\ncostOfWorngPrediction","b79e46f7":"from sklearn.linear_model.logistic import LogisticRegression\nclassifier=LogisticRegression()\nclassifier.fit(X_res,y_res)\n","2a66f4bd":"classes = classifier.predict(X_test)\nf  = confusion_matrix( y_test, classes )#confusion_matrix(y_true, y_pred)\ntn,fp,fn,tp = f.ravel() #tn, fp, fn, tp = f.ravel()\n(tn, fp, fn, tp)","0c2d6d68":"precision = tp\/(tp+fp)\nrecall = tp\/(tp + fn)\naccuracy=(tp+tn)\/(tp+tn+fp+fn)\nprint( precision, recall, accuracy)","86719e25":"from sklearn import metrics\nclassifier.score(X_res,y_res)","f12dddde":"A['y_test'] = y_test.tolist()\nA[\"classes\"]=classes.tolist()\nP=A.query(\" classes==0\")\nZ=P.query(\"y_test!= classes\")\nCostOfFalseNegative=Z[\"purchase_value\"].sum(axis = 0, skipna = True) \nCostOfFalseNegative\ncostOfFaslePositive=8*fp\ncostOfFaslePositive\ncostOfWorngPrediction=costOfFaslePositive+CostOfFalseNegative\ncostOfWorngPrediction","84ec238b":"clfkn=KNeighborsClassifier(n_neighbors=2)","9a6b04e0":"start = time.time()\nclfkn = clfkn.fit(X_res,y_res)\nend = time.time()\n(end-start)\/60","6789dae1":"classes = clfkn.predict(X_test)\nf  = confusion_matrix( y_test, classes )#confusion_matrix(y_true, y_pred)\ntn,fp,fn,tp = f.ravel() #tn, fp, fn, tp = f.ravel()\n(tn, fp, fn, tp)","974a8e5a":"precision = tp\/(tp+fp)\nrecall = tp\/(tp + fn)\naccuracy=(tp+tn)\/(tp+tn+fp+fn)\nprint( precision, recall, accuracy)","1b5852a0":"clfkn.score(X_res,y_res)","4950ae96":"from sklearn.metrics import r2_score\nfrom sklearn.metrics import f1_score\nr2_score(y_test,classes ),f1_score(y_test, classes, average='binary')","14bcddd5":"A['y_test'] = y_test.tolist()\nA[\"classes\"]=classes.tolist()\nP=A.query(\" classes==0\")\nZ=P.query(\"y_test!= classes\")\nCostOfFalseNegative=Z[\"purchase_value\"].sum(axis = 0, skipna = True) \nCostOfFalseNegative\ncostOfFaslePositive=8*fp\ncostOfFaslePositive\ncostOfWorngPrediction=costOfFaslePositive+CostOfFalseNegative\ncostOfWorngPrediction","c512e603":"clfgbm = gbm(\n    loss='exponential',\n    learning_rate=0.1,\n    n_estimators=100,\n    subsample=1.0,\n    criterion='friedman_mse',\n    min_samples_split=2,\n    min_samples_leaf=1,\n    min_weight_fraction_leaf=0.0,\n    max_depth=3,\n    min_impurity_decrease=0.0,\n    min_impurity_split=None,\n    init=None,\n    random_state=None,\n    max_features=None,\n    verbose=0,\n    max_leaf_nodes=None,\n    warm_start=False,\n    presort='auto',\n    validation_fraction=0.1,\n    n_iter_no_change=None,\n    tol=0.0001,\n)","8d7f03e9":"start = time.time()\nclfgbm = clfgbm.fit(X_res,y_res)\nend = time.time()\n(end-start)\/60","46c3515b":"classes = clfgbm.predict(X_test)\nf  = confusion_matrix( y_test, classes )#confusion_matrix(y_true, y_pred)\ntn,fp,fn,tp = f.ravel() #tn, fp, fn, tp = f.ravel()\n(tn, fp, fn, tp)","8f6d23f6":"precision = tp\/(tp+fp)\nrecall = tp\/(tp + fn)\naccuracy=(tp+tn)\/(tp+tn+fp+fn)\nprint( precision, recall, accuracy)","08a32618":"from sklearn import metrics\nclfgbm.score(X_res,y_res)","c82d1682":"from sklearn.metrics import r2_score\nfrom sklearn.metrics import f1_score\nr2_score(y_test,classes ),f1_score(y_test, classes, average='binary')","aa608f46":"A['y_test'] = y_test.tolist()\nA[\"classes\"]=classes.tolist()\nP=A.query(\" classes==0\")\nZ=P.query(\"y_test!= classes\")\nCostOfFalseNegative=Z[\"purchase_value\"].sum(axis = 0, skipna = True) \nCostOfFalseNegative\ncostOfFaslePositive=8*fp\ncostOfFaslePositive\ncostOfWorngPrediction=costOfFaslePositive+CostOfFalseNegative\ncostOfWorngPrediction","f14da28d":"clfxgb=XGBClassifier(criterion=\"gini\", max_depth=4, max_features=14, min_weight_fraction_leaf=0.05997, n_estimators=499)","3839c702":"start = time.time()\nclfxgb = clfxgb.fit(X_res,y_res)\nend = time.time()\n(end-start)\/60  ","da50bd69":"classes = clfxgb.predict(X_test.values)\nf  = confusion_matrix( y_test, classes )#confusion_matrix(y_true, y_pred)\ntn,fp,fn,tp = f.ravel() #tn, fp, fn, tp = f.ravel()\n(tn, fp, fn, tp)","4557752c":"precision = tp\/(tp+fp)\nrecall = tp\/(tp + fn)\naccuracy=(tp+tn)\/(tp+tn+fp+fn)\nprint( precision, recall, accuracy)","0069266b":"from sklearn import metrics\nclfxgb.score(X_res,y_res)","102b1f9c":"from sklearn.metrics import r2_score\nfrom sklearn.metrics import f1_score\nr2_score(y_test,classes ),f1_score(y_test, classes, average='binary')","1cdf7557":"A['y_test'] = y_test.tolist()\nA[\"classes\"]=classes.tolist()\nP=A.query(\" classes==0\")\nZ=P.query(\"y_test!= classes\")\nCostOfFalseNegative=Z[\"purchase_value\"].sum(axis = 0, skipna = True) \nCostOfFalseNegative\ncostOfFaslePositive=8*fp\ncostOfFaslePositive\ncostOfWorngPrediction=costOfFaslePositive+CostOfFalseNegative\ncostOfWorngPrediction","8a9ed1f9":"clflgb = lgb.LGBMClassifier(criterion=\"gini\", max_depth=5, max_features=14, min_weight_fraction_leaf=0.01674, n_estimators=499)","f45f0c8b":"start = time.time()\nclflgb = clflgb.fit(X_res,y_res)\nend = time.time()\n(end-start)\/60                   \n\n","b442f958":"classes = clflgb.predict(X_test)\nf  = confusion_matrix( y_test, classes )#confusion_matrix(y_true, y_pred)\ntn,fp,fn,tp = f.ravel() #tn, fp, fn, tp = f.ravel()\n(tn, fp, fn, tp)","f6df78f1":"precision = tp\/(tp+fp)\nrecall = tp\/(tp + fn)\naccuracy=(tp+tn)\/(tp+tn+fp+fn)\nprint( precision, recall, accuracy)","ac15d026":"from sklearn import metrics\nclflgb.score(X_res,y_res)","3a33ef65":"from sklearn.metrics import r2_score\nfrom sklearn.metrics import f1_score\nr2_score(y_test,classes ),f1_score(y_test, classes, average='binary')","f31619ae":"A['y_test'] = y_test.tolist()\nA[\"classes\"]=classes.tolist()\nP=A.query(\" classes==0\")\nZ=P.query(\"y_test!= classes\")\nCostOfFalseNegative=Z[\"purchase_value\"].sum(axis = 0, skipna = True) \nCostOfFalseNegative\ncostOfFaslePositive=8*fp\ncostOfFaslePositive\ncostOfWorngPrediction=costOfFaslePositive+CostOfFalseNegative\ncostOfWorngPrediction","562b85f6":"clfac = AdaBoostClassifier(base_estimator=None,\n    n_estimators=50,\n    learning_rate=1.0,\n    algorithm='SAMME.R',\n    random_state=None,)\n","97259af2":"start = time.time()\nclfac = clfac.fit(X_res,y_res)\nend = time.time()\n(end-start)\/60  ","5804e2f0":"classes = clfac.predict(X_test)\nf  = confusion_matrix( y_test, classes )#confusion_matrix(y_true, y_pred)\ntn,fp,fn,tp = f.ravel() #tn, fp, fn, tp = f.ravel()\n(tn, fp, fn, tp)","41c460eb":"precision = tp\/(tp+fp)\nrecall = tp\/(tp + fn)\naccuracy=(tp+tn)\/(tp+tn+fp+fn)\nprint( precision, recall, accuracy)","5abec71b":"from sklearn import metrics\nclfac.score(X_res,y_res)","658f39cf":"from sklearn.metrics import r2_score\nfrom sklearn.metrics import f1_score\nr2_score(y_test,classes ),f1_score(y_test, classes, average='binary')","c4560c1f":"A['y_test'] = y_test.tolist()\nA[\"classes\"]=classes.tolist()\nP=A.query(\" classes==0\")\nZ=P.query(\"y_test!= classes\")\nCostOfFalseNegative=Z[\"purchase_value\"].sum(axis = 0, skipna = True) \nCostOfFalseNegative\ncostOfFaslePositive=8*fp\ncostOfFaslePositive\ncostOfWorngPrediction=costOfFaslePositive+CostOfFalseNegative\ncostOfWorngPrediction","c5d4a05c":"clfcb = CatBoostClassifier( )","8d66e2d1":"start = time.time()\nclfcb = clfcb.fit(X_res,y_res)\nend = time.time()\n(end-start)\/60  ","9171d2b7":"classes = clfcb.predict(X_test)\nf  = confusion_matrix( y_test, classes )#confusion_matrix(y_true, y_pred)\ntn,fp,fn,tp = f.ravel() #tn, fp, fn, tp = f.ravel()\n(tn, fp, fn, tp)","27eca9e3":"precision = tp\/(tp+fp)\nrecall = tp\/(tp + fn)\naccuracy=(tp+tn)\/(tp+tn+fp+fn)\nprint( precision, recall, accuracy)","0484f249":"from sklearn import metrics\nclfcb.score(X_res,y_res)","84ed1dba":"from sklearn.metrics import r2_score\nfrom sklearn.metrics import f1_score\nr2_score(y_test,classes ),f1_score(y_test, classes, average='binary')","22079e2b":"A['y_test'] = y_test.tolist()\nA[\"classes\"]=classes.tolist()\nP=A.query(\" classes==0\")\nZ=P.query(\"y_test!= classes\")\nCostOfFalseNegative=Z[\"purchase_value\"].sum(axis = 0, skipna = True) \nCostOfFalseNegative\ncostOfFaslePositive=8*fp\ncostOfFaslePositive\ncostOfWorngPrediction=costOfFaslePositive+CostOfFalseNegative\ncostOfWorngPrediction","0270769c":"from sklearn.naive_bayes import GaussianNB \ngnb = GaussianNB() \ngnb.fit(X_res, y_res)","eb1b98cd":"classes = gnb.predict(X_test) \nf  = confusion_matrix( y_test, classes )#confusion_matrix(y_true, y_pred)\ntn,fp,fn,tp = f.ravel() #tn, fp, fn, tp = f.ravel()\n(tn, fp, fn, tp)","973bcb61":"precision = tp\/(tp+fp)\nrecall = tp\/(tp + fn)\naccuracy=(tp+tn)\/(tp+tn+fp+fn)\nprint( precision, recall, accuracy)","902533a2":"from sklearn import metrics\ngnb.score(X_res,y_res)","d57bd266":"A['y_test'] = y_test.tolist()\nA[\"classes\"]=classes.tolist()\nP=A.query(\" classes==0\")\nZ=P.query(\"y_test!= classes\")\nCostOfFalseNegative=Z[\"purchase_value\"].sum(axis = 0, skipna = True) \nCostOfFalseNegative\ncostOfFaslePositive=8*fp\ncostOfFaslePositive\ncostOfWorngPrediction=costOfFaslePositive+CostOfFalseNegative\ncostOfWorngPrediction","d2cc74e5":"from sklearn.svm import LinearSVC\nfrom sklearn.metrics import accuracy_score\n#create an object of type LinearSVC\nsvc_model = LinearSVC(random_state=0)\n#train the algorithm on training data and predict using the testing data\nclasses = svc_model.fit(X_res, y_res).predict(X_test)\n#print the accuracy score of the model\nprint(\"LinearSVC accuracy : \",accuracy_score(y_test, classes, normalize = True))","e57b3d63":"f  = confusion_matrix( y_test, classes )#confusion_matrix(y_true, y_pred)\ntn,fp,fn,tp = f.ravel() #tn, fp, fn, tp = f.ravel()\n(tn, fp, fn, tp)","6e876fef":"precision = tp\/(tp+fp)\nrecall = tp\/(tp + fn)\naccuracy=(tp+tn)\/(tp+tn+fp+fn)\nprint( precision, recall, accuracy)","88bcd8f9":"from sklearn import metrics\nsvc_model.score(X_res,y_res)","af6a7138":"A['y_test'] = y_test.tolist()\nA[\"classes\"]=classes.tolist()\nP=A.query(\" classes==0\")\nZ=P.query(\"y_test!= classes\")\nCostOfFalseNegative=Z[\"purchase_value\"].sum(axis = 0, skipna = True) \nCostOfFalseNegative\ncostOfFaslePositive=8*fp\ncostOfFaslePositive\ncostOfWorngPrediction=costOfFaslePositive+CostOfFalseNegative\ncostOfWorngPrediction","0a025ee3":"y_res_onehot = pd.get_dummies(y_res)\n","c6997cc7":"from sklearn.utils import class_weight\nclass_weights = class_weight.compute_class_weight('balanced',\n                                                 np.unique(y_train),\n                                                 y_train)\nfrom keras.models import Sequential\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation,Dropout,Flatten\n\n\nmodel = Sequential()\nmodel.add(Dense(32, input_shape=(14,)))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(16))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(2))\nmodel.add(Activation('softmax'))\nmodel.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy'])\n        \nmodel.fit(X_res, y_res_onehot,class_weight=class_weights, epochs=6, batch_size=32)","90893e14":"classes = model.predict_classes(X_test)\nf  = confusion_matrix( y_test, classes )#confusion_matrix(y_true, y_pred)\ntn,fp,fn,tp = f.ravel() #tn, fp, fn, tp = f.ravel()\n(tn, fp, fn, tp)","c6fa4235":"precision = tp\/(tp+fp)\nrecall = tp\/(tp + fn)\naccuracy=(tp+tn)\/(tp+tn+fp+fn)\nprint( precision, recall, accuracy)","55a7a773":"from sklearn import metrics\nsvc_model.score(X_res,y_res)","d898a1c5":"from sklearn.metrics import r2_score\nfrom sklearn.metrics import f1_score\nr2_score(y_test,classes ),f1_score(y_test, classes, average='binary')","4703185e":"A['y_test'] = y_test.tolist()\nA[\"classes\"]=classes.tolist()\nP=A.query(\" classes==0\")\nZ=P.query(\"y_test!= classes\")\nCostOfFalseNegative=Z[\"purchase_value\"].sum(axis = 0, skipna = True) \nCostOfFalseNegative\ncostOfFaslePositive=8*fp\ncostOfFaslePositive\ncostOfWorngPrediction=costOfFaslePositive+CostOfFalseNegative\ncostOfWorngPrediction","1cd1c17a":"from sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.ensemble import RandomForestClassifier\nfrom mlxtend.classifier import StackingClassifier\nimport numpy as np\nimport warnings\nfrom sklearn.svm import LinearSVC\nfrom sklearn.svm import SVC\nwarnings.simplefilter('ignore')","3348a6e1":"clf1 = XGBClassifier(criterion=\"gini\", max_depth=4, max_features=4, min_weight_fraction_leaf=0.05997, n_estimators=499)\nclf2=lgb.LGBMClassifier(criterion=\"gini\", max_depth=5, max_features=53, min_weight_fraction_leaf=0.01674, n_estimators=499)\nclf3 = LinearSVC(random_state=0)\nclf4 = GaussianNB()\nclf5=LogisticRegression() \nclf6=AdaBoostClassifier()\nlr = gbm() \n\nsclf = StackingClassifier(classifiers=[clf1,clf2,clf3,clf4,clf5,clf6],\n                          meta_classifier=lr)","da5d7838":"start = time.time()\nsclf = sclf.fit(X_test.values,y_test)\nend = time.time()\n(end-start)\/60","9d5495b6":"classes = sclf.predict(X_test.values)\nf  = confusion_matrix( y_test, classes )#confusion_matrix(y_true, y_pred)\ntn,fp,fn,tp = f.ravel() #tn, fp, fn, tp = f.ravel()\n(tn, fp, fn, tp)","aa18abdf":"precision = tp\/(tp+fp)\nrecall = tp\/(tp + fn)\naccuracy=(tp+tn)\/(tp+tn+fp+fn)\nprint( precision, recall, accuracy)","82afb248":"from sklearn import metrics\nsclf.score(X_res,y_res)","21c547b5":"from sklearn.metrics import r2_score\nfrom sklearn.metrics import f1_score\nr2_score(y_test,classes ),f1_score(y_test, classes, average='binary')","92a28da4":"A['y_test'] = y_test.tolist()\nA[\"classes\"]=classes.tolist()\nP=A.query(\" classes==0\")\nZ=P.query(\"y_test!= classes\")\nCostOfFalseNegative=Z[\"purchase_value\"].sum(axis = 0, skipna = True) \nCostOfFalseNegative\ncostOfFaslePositive=8*fp\ncostOfFaslePositive\ncostOfWorngPrediction=costOfFaslePositive+CostOfFalseNegative\ncostOfWorngPrediction","62170f24":"clf1 = XGBClassifier(criterion=\"gini\", max_depth=4, max_features=4, min_weight_fraction_leaf=0.05997, n_estimators=499)\nclf2=lgb.LGBMClassifier(criterion=\"gini\", max_depth=5, max_features=53, min_weight_fraction_leaf=0.01674, n_estimators=499)\nclf3 = LinearSVC(random_state=0)\nclf4 = GaussianNB()\nclf5=LogisticRegression() \nclf6=AdaBoostClassifier()\nlr = gbm() \n\nsclf = StackingClassifier(classifiers=[clf1,clf2,clf3,clf4,clf5,clf6],\n                          meta_classifier=lr)","eaee76a1":"start = time.time()\nsclf = sclf.fit(X_test.values,y_test)\nend = time.time()\n(end-start)\/60","c39beac4":"classes = sclf.predict(X_test.values)\nf  = confusion_matrix( y_test, classes )#confusion_matrix(y_true, y_pred)\ntn,fp,fn,tp = f.ravel() #tn, fp, fn, tp = f.ravel()\n(tn, fp, fn, tp)","75f353b6":"precision = tp\/(tp+fp)\nrecall = tp\/(tp + fn)\naccuracy=(tp+tn)\/(tp+tn+fp+fn)\nprint( precision, recall, accuracy)","a4f116f6":"from sklearn.metrics import r2_score\nfrom sklearn.metrics import f1_score\nr2_score(y_test,classes ),f1_score(y_test, classes, average='binary')","cccc8b5b":"A['y_test'] = y_test.tolist()\nA[\"classes\"]=classes.tolist()\nP=A.query(\" classes==0\")\nZ=P.query(\"y_test!= classes\")\nCostOfFalseNegative=Z[\"purchase_value\"].sum(axis = 0, skipna = True) \nCostOfFalseNegative\ncostOfFaslePositive=8*fp\ncostOfFaslePositive\ncostOfWorngPrediction=costOfFaslePositive+CostOfFalseNegative\ncostOfWorngPrediction","0e0a8c1c":"Specificity = tn \/ (tn + fp)\nSpecificity","568e601d":"Sensitivity = tp \/ (tp + fn)\nSensitivity","053bc56d":"from sklearn.metrics import cohen_kappa_score\ncohen_kappa_score(y_test,classes)","71e08887":"from sklearn.metrics import roc_auc_score\nroc_auc_score(y_test, classes)","1f2408a6":"# calculate roc curve\nfrom sklearn.metrics import roc_curve\nfpr, tpr, thresholds = roc_curve(y_test, classes)","16c1a075":"from sklearn.metrics import average_precision_score\naverage_precision_score(y_test, classes)","a74edb10":"from sklearn.metrics import log_loss\nlog_loss(y_test, classes, eps=1e-15, normalize=True, sample_weight=None, labels=None)","a9dec553":"from matplotlib import pyplot\nfrom sklearn.metrics import roc_curve, auc\n# plot no skill\npyplot.plot([0, 1], [0, 1], linestyle='--')\n# plot the roc curve for the model\npyplot.plot(fpr, tpr, marker='.')\n# show the plot\npyplot.show()","c7cd8ae2":"from sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import average_precision_score\nfrom matplotlib import pyplot\nprecision, recall, thresholds = precision_recall_curve(y_test, classes)\n# calculate F1 score\nf1=f1_score(y_test, classes, average='binary')\n# calculate precision-recall AUC\nauc = auc(recall, precision)\n# calculate average precision score\nap = average_precision_score(y_test, classes)\nprint('f1=%.3f auc=%.3f ap=%.3f' % (f1, auc, ap))\n# plot no skill\npyplot.plot([0, 1], [0.5, 0.5], linestyle='--')\n# plot the precision-recall curve for the model\npyplot.plot(recall, precision, marker='.')\n# show the plot\npyplot.show()","5ecf9ca9":"from sklearn.metrics import precision_recall_curve\nimport matplotlib.pyplot as plt\nfrom inspect import signature\n\nprecision, recall, _ = precision_recall_curve(y_test, classes)\n\nstep_kwargs = ({'step': 'post'}\n               if 'step' in signature(plt.fill_between).parameters\n               else {})\nplt.step(recall, precision, color='b', alpha=0.2,\n         where='post')\nplt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\n\n","cfdcd188":"cm = pd.crosstab(y_test.values, classes, rownames=['Actual'], colnames=['Predicted'])\nfig, (ax1) = plt.subplots(ncols=1, figsize=(5,5))\nsns.heatmap(cm, \n            xticklabels=['Not Fraud', 'Fraud'],\n            yticklabels=['Not Fraud', 'Fraud'],\n            annot=True,ax=ax1,\n            linewidths=.2,linecolor=\"Darkblue\", cmap=\"Blues\")\nplt.title('Confusion Matrix', fontsize=14)\nplt.show()","2fe78949":"test.rename(columns={'signup_time-purchase_time':'TimeBetween',\n                          'N[device_id]':'TimesDeviceUsed',\n                          }, \n                 inplace=True)","045797b1":"TimesIPaddress = test['ip_address'].value_counts()\ntest[\"TimesIPaddress\"] = test.groupby('ip_address')['user_id'].transform('count')\n","1f171e42":"from sklearn.preprocessing import LabelEncoder\nlb_make = LabelEncoder()\ntest['sex'] = lb_make.fit_transform(test['sex'])\nsourcedummies=pd.get_dummies(test[\"source\"], columns=[\"source\"])\nbrowserdummies=pd.get_dummies(test[\"browser\"], columns=[\"browser\"])\ntest = pd.concat([test, sourcedummies], axis=1)\ntest = pd.concat([test, browserdummies], axis=1)\n","0e0b8d0b":"testdata = transactions[[\"sex\",\"age\",\"TimeBetween\",\"purchase_value\",\"TimesDeviceUsed\",\"TimesIPaddress\",\"Chrome\",\"FireFox\",\"IE\",\"Opera\",\"Safari\",\"Ads\",\"Direct\",\"SEO\"]]","e1fc1173":"from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\ntestdata = pd.DataFrame(ss.fit_transform(testdata),columns = testdata.columns)\ntestdata.head()","03ef5411":"classes = sclf.predict(testdata.values)\nclasses","31e1aa68":"pd.DataFrame(classes).to_csv(\"CapstoneFinal_Nagesh_Anumala.csv\",header=None,index=None)","64bb1204":"Let us see what are the features available at final for building models","6d1e1819":"#### Read the data","f53a0491":"The minimum, maximum & average purchase_value for both fraudulent & non-fraudulent transactions is almost same. Let us explore is there any imbalance in data between fradulent & non-fradulent transactions. ","09ed9579":"### Naive Bayes Classifier","3510bab3":"### KNeighborsClassifier","b584a0ad":"#### Load packages","1cd030d9":"Observing the above models. I want to ensamble my own model using Stacking Classifier","48aee8f6":"#### MY FINAL MODEL SELECTED\nAdboost Classifier,Neural Networks,and Stacking Classifier ensembled by me are performing well.Comparing the above models, I decided to select Stacking Classifier as my best model. ","876258cd":"#### XGBOOST CLASSIFIER","d40d359a":"##### Separating Target & Predictors\nThis time for seperating predictors & target \ny=Target\nX=Predictors (selected features)\nA combination of features selected & tried and finally it is decided to select these features for building model.","b90e88d8":"### Adaboost classifier","87824d53":"#### Neural Networks using Keras","958d5859":"There is a big imbalance between fraudulent (class=1) and normal(class=0) transactions. Only 9.38% of the total transactions are fraudulent transactions. Any model built with this data will give high accuracy and low recall. we need to balance  this by overampling using SMOTE when building models. ","d895a9e0":"##### Rename Features\nI will rename some of the given features for convience as follows\nsignup_time-purchase_time as TimeBetween\nN[device_id] as TimesDeviceUsed\n","c4e24490":"### Let us load packages","31d469e0":"#### Data Exploration\nLet us explore the data","4f7a7335":"#### GradientBoostingClassifier","bcb5b7cc":"###### Visual Explore of Data\nLet us explore visually by creating some bar plots & box plots","0ebe2363":"##### Selection of Features for model building\nThe given features like Column 1& user_id are for identification of transaction. We had already derived new features using  signup_time,purchase_time, device_id & ip_address. Therefore, we need not select these features. I will try selecting combination of other features to get more recall & precision.","3f0a9c49":"##### RandomForestClassifier","eecd467c":"Given Features\n\nColumn 1                     74691 non-null int64\nuser_id                      74691 non-null int64\nsignup_time                  74691 non-null object\npurchase_time                74691 non-null object\nsignup_time-purchase_time    74691 non-null int64\npurchase_value               74691 non-null int64\ndevice_id                    74691 non-null object\nsource                       74691 non-null object\nbrowser                      74691 non-null object\nsex                          74691 non-null object\nage                          74691 non-null int64\nip_address                   74691 non-null float64\nclass                        74691 non-null int64\nN[device_id]                 74691 non-null int64\n\nThere are 74691 transactions with 14 features including the target variable \"class\". There is no missing data in the entire dataset.\n","4c09be58":"##### Create New Features\nIt looks like some IP addresses were used more than once, this could have an impact on the type of transaction. Let's create a new feature (new column) to use notate how many times the same IP address was used.\nI will create a new fature number of transactions taken from same IP address as TimesIPaddress","d68e4db8":"# Let us use this model for predicting the test data","cd3c709a":"### LinearSVC","24c3f8fa":"#### CatBoostClassifier","429fae4a":"###### Let us copy the X_test data for using at a later stage","a69beab1":"### Lightgbm classifier","cd9880d5":"##### Converting categorical variables into numerical\nWe need to convert categorical variables into numerical variables for using in model building. Let us do that","0f8abf1d":"## Extra Tress Classifier","18a44f05":"###### Spliting Train & Validation data\nNow time for splitting the data into train & validation. For  convienance the nomenclature of validation is used as test. The data is split into 70:30, 80:20 & 90:10 ratios and tried and finally decided to split in the ratio of 90:10 for getting more recall & precision.","3150c333":"## ENSEMBLED Stacking Classifier","05e37f50":"##### Balancing the validation data before modelling\nTried with SMOTE, ADASYN, SMOTENC, BorderLineSMOTE,SVMSMOTE for over sampling and finally decided to use SMOTE. ","e09180f4":"There are 13413 transactions with 13 features excluding the target variable \"class\" in our test dataset. There is no missing data in the entire dataset.","78b257eb":"#### DecisionTreeClassifier","8edb9366":"##### Let us see the Correlation Plot of our selected features ","903e401c":"### LogisticRegression","a595630b":"Given Datasets\nTrain Dataset: datasetForFinalAssignment.csv\nTest Dataset: datasetForFinalTest\n\n Let us first import the main libraries & read the train dataset & test dataset.","65012314":"# MODEL BUILDING\nLet us start building the models.\nI tried with the following models,selecting combination of features,using scalled & unscalled data, combination of parameters,tuned the parameters using Bayesian optimization. However, the final selected models presented here.\n1.Decision Tree Classifier\n2.Random Forest Classifier\n3.Extra Tress Classifier\n4.KNeighborsClassifier\n5.GradientBoostingClassifier\n6.XGBClassifier\n7.lightgbm classifier\n9.Adaboost classifier\n10.CatBoostClassifier\n11.Neural Networks using Keras\n\nThe following matrics caluculated for evaluating the models\n(a)Precision\n(b)Recall\n(c)Accuracy\n(d)Sensitivity\n(e)Specificity\n(f)r2_score\n(g)f1_score\n(h)cohen_kappa_score\n(i)roc_auc_score\n(j)average_precision_score\n(k)Plotted roc_curve and precision_recall_curve\n\nand estimated the total cost to company \n\nFor keeping simplicity of presentation, presented only Precision,Recall,Accuracy,r2_score,f1_score and cost to company to all models tried and all paramers mentioned above for the final selected model.\n","98852bb4":"It appears that TimeBetween,TimesDeviceUsed & TimesIPaddress have some impact on class. There is no significant impact of purchase_value & sex. Let us explore more.","75de1aa6":"##                  Business Analytics Capstone\n                         Nagesh Kumar Anumala.\nFinal Project: Build a predictive model for determining if a customer transaction is fraudulent\n\nOur customer is an e-commerce site that sells wholesale electronics. You have been contracted to build a model that predicts whether a given transaction is fraudulent or not. We  only have information about each user\u2019s first transaction on the company's website. If we fail to identify a fraudulent transaction, the company loses money equivalent to the price of the fraudulently purchased product. If we incorrectly flag a real transaction as fraudulent, it inconveniences the customers whose valid transactions are flagged\u2014a cost our client values at $8. The task is to build a model that's predictive but also minimizes to cost to the company not only by correctly flagging and identifying fraudulent transactions but also to minimize the cost of wrong predictions since each wrong prediction costs the company $8."}}