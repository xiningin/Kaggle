{"cell_type":{"b2f27fac":"code","58fa82ec":"code","80b0d04e":"code","5b37b454":"code","d19d4738":"code","1bc435e6":"code","c17ec095":"code","b46a9283":"code","04744d13":"code","fee214fe":"code","f363b549":"code","6f11914b":"code","e4a1788e":"code","0828f98d":"code","a39cf084":"code","5140fb0f":"code","69d36541":"code","bc7f51e7":"code","aa1ccf0a":"code","e36aa016":"code","1b7e24c3":"code","1420991c":"code","989f400a":"code","f141f09e":"code","2775a1f6":"code","8de7496f":"code","cc3d709c":"code","7291a7bf":"code","f865ebf8":"code","1cfde5da":"code","0ae20889":"code","bfb64ff0":"code","5f26b6a6":"code","b0aa869a":"code","153447e4":"code","5f411fbd":"code","b0b4c22d":"code","9fb53494":"code","a22b6341":"code","1ee39f33":"code","764cf219":"code","756c1c96":"code","545dcd8f":"code","ca2417d4":"code","585b17e4":"code","0df2c61f":"code","7b64c3a6":"code","7216d7de":"code","4a5a01ae":"code","268fd6e4":"code","a71a31fc":"code","e1cc0787":"code","b3c47bf1":"code","dd1257d9":"code","64edce66":"code","4137bd01":"code","91214a04":"code","e632cb67":"code","961436af":"code","d67ccd6f":"markdown","ef002b09":"markdown","4eef9fc6":"markdown","8d7e8d22":"markdown","82cf4d70":"markdown","e9d830d9":"markdown","02f3ad79":"markdown","f4409b1f":"markdown","59c7d8d2":"markdown","d4964a06":"markdown","c3c664f1":"markdown","d32a0aa5":"markdown","1a074b06":"markdown","e68c8fb9":"markdown","6b7e1fc3":"markdown","397554b4":"markdown","c170eb73":"markdown","e31e5490":"markdown","89ff5afe":"markdown","362f48e0":"markdown","bc4b173c":"markdown","32b001e3":"markdown","1a4302f9":"markdown","357975a4":"markdown","a24a0b4e":"markdown","05f48f70":"markdown","e6a7acef":"markdown"},"source":{"b2f27fac":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport time\nimport gc\nprint(os.listdir(\"..\/input\/\"))\n\nimport PIL\nimport cv2","58fa82ec":"#\ub370\uc774\ud130 path\npath = '..\/input\/2019-3rd-ml-month-with-kakr\/'\ntrain_img_path = os.path.join(path, 'train')\ntest_img_path = os.path.join(path, 'test')\n\n#seed\nseed = 119","80b0d04e":"train_df = pd.read_csv(os.path.join(path, 'train.csv'))\ntest_df = pd.read_csv(os.path.join(path, 'test.csv'))\nclass_df = pd.read_csv(os.path.join(path, 'class.csv'))\nprint(train_df.head())","5b37b454":"print('Total class is ', len(class_df))\nprint(class_df.head())","d19d4738":"assert len(train_df) == len(os.listdir(train_img_path)) #\ud30c\uc77c \uc218\uc640 \ub9ac\uc2a4\ud2b8 \uc218\uac00 \ub3d9\uc77c\ud55c\uc9c0 \ud655\uc778\nprint('Train Image is : ', len(train_df))\n\nassert len(test_df) == len(os.listdir(test_img_path))\nprint('Test Image is : ', len(test_df))","1bc435e6":"#\ud074\ub798\uc2a4 \ubd84\ud3ec \ud655\uc778\nplt.figure(figsize=(12, 6))\nsns.countplot(train_df[\"class\"], order=train_df[\"class\"].value_counts(ascending=True).index)\nplt.title(\"Number of data per each class\")\nplt.show()","c17ec095":"train_df['class'].value_counts().describe()[['min', 'mean', 'max']]","b46a9283":"#\uc774\ubbf8\uc9c0 \ucd9c\ub825\ud568\uc218 \uc815\uc758\ndef plot_img(img_list, path=train_img_path):\n    plt.figure(figsize=(20,20))\n    \n    for num, (index, img) in enumerate(img_list.iterrows()):\n        image = cv2.imread(os.path.join(train_img_path, img.img_file))   #image \ud30c\uc77c load\n        image = cv2.rectangle(image, (img.bbox_x1, img.bbox_y1), (img.bbox_x2, img.bbox_y2), color=(255,255,0), thickness= 5) #bounding box \uc0dd\uc131\n        height, width = image.shape[:2] #image\uc758 shape \uc800\uc7a5\n        plt.subplot(5, 5, num+1) # 10\uac1c\uc758 subplot \uc911 (num+1) \ubc88\uca30 subplot \uc9c0\uc815\n        plt.imshow(image)\n        plt.title('%s, (%i * %i)' % (img.img_file, width, height))\n        plt.axis('off')\n        \nimg_list = train_df[:10]\nplot_img(img_list)","04744d13":"#\uc774\ubbf8\uc9c0\uc758 width\uc640 height\uc744 \uc800\uc7a5\ud558\ub294 \ud568\uc218\ndef width_height_img(train_df):\n    start = time.time()\n    w = []  #width\ub97c \uc800\uc7a5\ud558\ub294 list\n    h = []  #height\ub97c \uc800\uc7a5\ud558\ub294 list\n\n    for num, df in train_df.iterrows():\n        image = cv2.imread(os.path.join(train_img_path, df.img_file))\n        height, width = image.shape[:2]\n        w.append(width)\n        h.append(height)\n\n    train_df['width'] = w\n    train_df['height'] = h\n    train_df['pixel'] = train_df['width'] * train_df['height']\n    print(time.time() -start)\n    return train_df\n\ntrain_df = width_height_img(train_df)\nprint(train_df.head())","fee214fe":"plot_img(train_df.loc[train_df.pixel == min(train_df.pixel)])","f363b549":"plot_img(train_df.loc[train_df.pixel == max(train_df.pixel)])","6f11914b":"temp = train_df.loc[train_df['class'] == 115]\nplot_img(temp[:15])","e4a1788e":"ratio = train_df['width'] \/ train_df['height']","0828f98d":"plt.hist(ratio, bins=20)\nplt.xlim(0.5, 2.5)\nplt.ylim(0, 8000)\nplt.title('Ratio histogram')\nplt.show()","a39cf084":"#Bounding box image\ub97c \uc800\uc7a5\ud558\ub294 \ud568\uc218\ndef save_cropped_img(train_df=train_df, path=path, save_path = None):\n    for num, df in train_df.iterrows():\n        img = cv2.imread(os.path.join(path, df.img_file))\n        #print(img.shape)\n        img = img[df.bbox_y1 : df.bbox_y2, df.bbox_x1: df.bbox_x2]\n        #print(img.shape)\n        name = save_path + df.img_file\n        cv2.imwrite(name, img)","5140fb0f":"tt = train_df[:1]\n\nfor num, df in tt.iterrows():\n    print(df.img_file)\n    img = cv2.imread(os.path.join(train_img_path, df.img_file))\n    print(img.shape)\n    img = img[df.bbox_y1 : df.bbox_y2, df.bbox_x1: df.bbox_x2]\n    plt.imshow(img)","69d36541":"#\uc0c8\ub85c \ub9cc\ub4e0 \uc774\ubbf8\uc9c0\ub97c \uc800\uc7a5\ud560 path\ub97c \uc0dd\uc131\n'''%%time\n!mkdir \/crop_train\n#crop_train_path = '\/crop_train\/'\nsave_cropped_img(train_df, path=train_img_path, save_path=crop_train_path)'''","bc7f51e7":"'''#\ubaa8\ub4e0 \uc774\ubbf8\uc9c0\uac00 \ubcf5\uc0ac\ub42c\ub294\uc9c0 \ud655\uc778\nassert len(os.listdir(train_img_path)) == len(os.listdir(crop_train_path))\nprint(len(os.listdir(crop_train_path)))'''","aa1ccf0a":"#Test \uc774\ubbf8\uc9c0\ub3c4 \uc800\uc7a5\ud569\ub2c8\ub2e4.\n'''%%time\n!mkdir \/crop_test\ncrop_test_path = '\/crop_test\/'\nsave_cropped_img(test_df, path=test_img_path, save_path=crop_test_path)'''","e36aa016":"crop_train_path = '..\/input\/2019-3rd-ml-month-with-kakr\/train\/'\nassert len(os.listdir('..\/input\/crop-image\/train_crop\/')) == len(os.listdir(crop_train_path))\nprint('Train images : ', len(os.listdir('..\/input\/crop-image\/train_crop\/')))\n\ncrop_test_path = '..\/input\/2019-3rd-ml-month-with-kakr\/test\/'\nassert len(os.listdir('..\/input\/crop-image\/test_crop\/\/')) == len(os.listdir(crop_test_path))\nprint(\"Test images : \", len(os.listdir('..\/input\/crop-image\/test_crop\/\/')))","1b7e24c3":"#'class' \uac00 int64 type\uc73c\ub85c \uc800\uc7a5\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.\ntrain_df.info()","1420991c":"#type \ubcc0\ud658\ntrain_df['class'] = train_df['class'].astype('str')","989f400a":"from sklearn.model_selection import train_test_split\n\ntr_data, val_data = train_test_split(train_df[['img_file', 'class']], train_size=0.9, random_state=seed, stratify=train_df['class'])\ntest_data = test_df[['img_file']]\nprint(len(tr_data), len(val_data))","f141f09e":"assert tr_data['class'].nunique() == val_data['class'].nunique()\nprint('All class image in val_data')\nsns.countplot(val_data['class'])","2775a1f6":"from keras.applications.mobilenet import preprocess_input #\uc0ac\uc6a9\ud560 \ubaa8\ub378\uba85\nfrom keras.preprocessing.image import ImageDataGenerator","8de7496f":"#Hyper Parameter\ninput_size = (224,224) \nbatch_size = 32\nepochs = 22","cc3d709c":"#preprocessing\ub9cc \ubcc0\uacbd\ud574\uc11c \uc0ac\uc6a9\ud560 \uc218 \uc788\ub3c4\ub85d \ud568\uc218\ub85c \uc815\uc758\ndef make_generator(input_size= input_size, batch_size=batch_size, preprocessing_function=preprocess_input):\n    #ImageDataGenerator \uc124\uc815\uc744 \uc815\uc758\ud569\ub2c8\ub2e4.\n    train_datagen = ImageDataGenerator(horizontal_flip=True,    #\uc218\ud3c9 \ubc18\uc804\n                                       zoom_range= 0.15,        #\ud655\ub300 & \ucd95\uc18c\n                                       width_shift_range= 0.1,  #\uc218\ud3c9\ubc29\ud5a5 \uc774\ub3d9\n                                       height_shift_range=0.1,  #\uc218\uc9c1\ubc29\ud5a5 \uc774\ub3d9\n                                       preprocessing_function= preprocessing_function  \n                                      )\n\n    val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n    test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n\n    #Train Generator\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4.\n    train_generator = train_datagen.flow_from_dataframe(dataframe=tr_data, \n                                                        directory=crop_train_path,  #path\ub294 \ub9de\uac8c \uc218\uc815\n                                                        x_col='img_file', \n                                                        y_col='class', \n                                                        target_size=input_size, \n                                                        color_mode='rgb',  #RGB\ub77c\uba74 'rgb'\n                                                        class_mode='categorical', \n                                                        batch_size=batch_size, \n                                                        #shuffe = True,\n                                                        seed=seed)\n\n    #Valid Generator\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4.\n    valid_generator = val_datagen.flow_from_dataframe(dataframe=val_data, \n                                                     directory=crop_train_path, #path\ub294 \ub9de\uac8c \uc218\uc815\n                                                     x_col= 'img_file',\n                                                     y_col= 'class',\n                                                     target_size= input_size,\n                                                     color_mode= 'rgb',\n                                                     class_mode= 'categorical',\n                                                     batch_size= batch_size,\n                                                     shuffle=True,\n                                                     seed = seed)\n\n    #Test Generator\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4.\n    test_generator = test_datagen.flow_from_dataframe(dataframe=test_data, \n                                                        directory=crop_test_path, #path\ub294 \ub9de\uac8c \uc218\uc815\n                                                        x_col='img_file',\n                                                        y_col=None,              #\uc5c6\ub294 \ub370\uc774\ud130\uc774\ubbc0\ub85c None\n                                                        target_size=input_size, \n                                                        color_mode='rgb', \n                                                        class_mode=None, \n                                                        batch_size=batch_size,\n                                                        shuffle=False,\n                                                        seed=seed)\n    \n    return train_generator, valid_generator, test_generator","7291a7bf":"train_generator, valid_generator, test_generator = make_generator()","f865ebf8":"from keras.applications import MobileNet\nfrom keras.layers import Dense, GlobalAveragePooling2D #, ZeroPadding2D, Conv2D\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping","1cfde5da":"#\uae30\ubcf8 \ubaa8\ub378\uc744 \ubd88\ub7ec\uc635\ub2c8\ub2e4.\nbase_model = MobileNet(input_shape=(224,224,3), \n                         weights='imagenet', #imagenet pretrained weight \ubd88\ub7ec\uc624\uae30\n                         include_top=False) #include_top\ub85c \ub05d\ubd80\ubd84 \ubbf8\ud3ec\ud568","0ae20889":"#\ubaa8\ub378\uc758 \ub9c8\uc9c0\ub9c9 \ubd80\ubd84\uc744 \uc644\uc131\ud569\ub2c8\ub2e4. fully_connected layer\uc778 Dense\uc548\uc5d0\ub294 \ubd84\ub958\ud558\uace0\uc790\ud558\ub294 \uc804\uccb4 class \uc218\uc758 \ub123\uc2b5\ub2c8\ub2e4.\n#activation\uc740 class\uc758 \uc218\uac00 \uc5ec\ub7ec\uac1c(multi) \uc77c\ub584\ub294 softmax\ub97c, \uc774\uc9c4\ubd84\ub958(binary)\uc778 \uacbd\uc6b0\ub294 sigmoid\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\npred = Dense(196, activation='softmax', kernel_initializer='he_normal')(x)","bfb64ff0":"#\ucd5c\uc885 \ubaa8\ub378 \uc0dd\uc131\nmodel = Model(inputs=base_model.input, output=pred)","5f26b6a6":"'''for layer in base_model.layers:\n    layer.trainable = False\n    \n#\ub9c8\uc9c0\ub9c9 Dense layer\uc758 \ud30c\ub77c\ubbf8\ud130\ub9cc \ud559\uc2b5\ub429\ub2c8\ub2e4.\nprint(model.trainable_weights)'''","b0aa869a":"model.compile(optimizer=Adam(lr=0.001, epsilon=1e-08), loss='categorical_crossentropy', metrics=['acc'])","153447e4":"#steps_per_epoch \uc124\uc815 \ud568\uc218\ndef get_steps(num_samples, batch_size):\n    if (num_samples % batch_size) > 0 :\n        return (num_samples \/\/ batch_size) + 1\n    else :\n        return num_samples \/\/ batch_size","5f411fbd":"#callback \uc124\uc815\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', patience=3,\n                                           verbose=1, factor=0.5, min_lr=0.0001)\nfilepath = 'model_{val_acc:.2f}_{val_loss:.2f}.h5'\nmodel_ckpt = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True)\n#es = EarlyStopping(monitor='val_acc', min_delta=0, patience=3, verbose=1, mode='auto')\n\ncallbacks = [learning_rate_reduction, model_ckpt]","b0b4c22d":"#history\ub85c \ud559\uc2b5\uacfc\uc815 \ub370\uc774\ud130\ub97c \uc800\uc7a5\ud569\ub2c8\ub2e4.\nhistory = model.fit_generator(train_generator, \n                              steps_per_epoch=get_steps(len(tr_data), batch_size), \n                              validation_data= valid_generator,\n                              validation_steps= get_steps(len(val_data), batch_size),\n                              epochs=epochs,\n                              callbacks=callbacks, \n                              verbose=1)\ngc.collect()","9fb53494":"# Plot training & validation accuracy values\nfig, ax = plt.subplots()\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Val'], loc='upper left')\nplt.show()","a22b6341":"test_generator.reset()    #Generator \ucd08\uae30\ud654\nprediction = model.predict_generator(\n    generator = test_generator,\n    steps = get_steps(len(test_data), batch_size),\n    verbose=1\n)","1ee39f33":"predicted_class_indices=np.argmax(prediction, axis=1)\n\n# Generator class dictionary mapping\nlabels = (train_generator.class_indices)\nlabels = dict((v,k) for k,v in labels.items())\npredictions = [labels[k] for k in predicted_class_indices]\n\nsubmission = pd.read_csv(os.path.join(path, 'sample_submission.csv'))\nsubmission[\"class\"] = predictions\nsubmission.to_csv(\"submission.csv\", index=False)\nsubmission.head()","764cf219":"from keras.applications.xception import Xception\nfrom keras.applications.xception import preprocess_input","756c1c96":"base_model_2 = Xception(input_shape=(224,224,3), weights= 'imagenet', include_top=False) \nx = base_model_2.output\nx = GlobalAveragePooling2D()(x)\npred = Dense(196, activation='softmax', kernel_initializer='he_normal')(x)\n\nmodel_2 = Model(inputs=base_model_2.input, output=pred)\nmodel_2.compile(optimizer=Adam(lr=0.001, epsilon=1e-08), loss='categorical_crossentropy', metrics=['acc'])","545dcd8f":"#callback \uc124\uc815\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', patience=3,\n                                           verbose=1, factor=0.5, min_lr=0.0001)\nfilepath = 'model_2_{val_acc:.2f}_{val_loss:.2f}.h5'\nmodel_ckpt = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True)\n#es = EarlyStopping(monitor='val_acc', min_delta=0, patience=3, verbose=1, mode='auto')\n\ncallbacks = [learning_rate_reduction, model_ckpt]","ca2417d4":"from random import randint\n\nhistory_2 = []\nfor i in range(6):\n    seed = randint(0, 200)\n    tr_data, val_data = train_test_split(train_df[['img_file', 'class']], train_size=0.9, random_state=seed, stratify=train_df['class'])\n    train_generator, valid_generator, test_generator = make_generator(preprocessing_function=preprocess_input)\n    \n    history = model_2.fit_generator(train_generator, \n                                      steps_per_epoch=get_steps(len(tr_data), batch_size), \n                                      validation_data= valid_generator,\n                                      validation_steps= get_steps(len(val_data), batch_size),\n                                      epochs=4,\n                                      callbacks=callbacks, \n                                      verbose=1)\n    history_2.append(history)","585b17e4":"acc_list = []\nval_acc_list = []\nfor _, i in enumerate(history_2):\n    acc_list.append(i.history['acc'])\n    val_acc_list.append(i.history['val_acc'])","0df2c61f":"fig, ax = plt.subplots()\nplt.plot(acc_list)\nplt.plot(val_acc_list)\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'val'], loc='upper left')\nplt.show()","7b64c3a6":"test_generator.reset()\nprediction = model_2.predict_generator(\n    generator = test_generator,\n    steps = get_steps(len(test_df), batch_size),\n    verbose=1\n)","7216d7de":"predicted_class_indices=np.argmax(prediction, axis=1)\n\n# Generator class dictionary mapping\nlabels = (train_generator.class_indices)\nlabels = dict((v,k) for k,v in labels.items())\npredictions = [labels[k] for k in predicted_class_indices]\n\nsubmission = pd.read_csv(os.path.join(path, 'sample_submission.csv'))\nsubmission[\"class\"] = predictions\nsubmission.to_csv(\"submission2.csv\", index=False)\nsubmission.head()","4a5a01ae":"from keras.applications import MobileNet\nfrom keras.applications.mobilenet import preprocess_input","268fd6e4":"def gray_preprocess(img):\n    img = preprocess_input(img)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    img = np.repeat(img[:,:, np.newaxis], 3, axis=2)\n    return img","a71a31fc":"seed = 119\ntr_data, val_data = train_test_split(train_df[['img_file', 'class']], train_size=0.9, random_state=seed, stratify=train_df['class'])","e1cc0787":"train_generator, valid_generator, test_generator = make_generator(preprocessing_function=gray_preprocess)","b3c47bf1":"for i in train_generator:\n    print(i[0][0].shape)\n    plt.imshow(i[0][0])\n    break","dd1257d9":"base_model_3 = MobileNet(input_shape=(224,224,3), weights='imagenet', include_top=False)\n\nx = base_model_3.output\nx = GlobalAveragePooling2D()(x)\npred = Dense(196, activation='softmax')(x)\n\nmodel_3 = Model(inputs=base_model_3.input, output=pred)\nmodel_3.compile(optimizer=Adam(lr=0.001, epsilon=1e-08), loss='categorical_crossentropy', metrics=['accuracy'])","64edce66":"#callback \uc124\uc815\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', patience=3,\n                                           verbose=1, factor=0.5, min_lr=0.0001)\nfilepath = 'model_3_{val_acc:.2f}_{val_loss:.2f}.h5'\nmodel_ckpt = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True)\n#es = EarlyStopping(monitor='val_acc', min_delta=0, patience=3, verbose=1, mode='auto')\n\ncallbacks = [learning_rate_reduction, model_ckpt]","4137bd01":"#history\ub85c \ud559\uc2b5\uacfc\uc815 \ub370\uc774\ud130\ub97c \uc800\uc7a5\ud569\ub2c8\ub2e4.\nhistory_3 = model_3.fit_generator(train_generator, \n                              steps_per_epoch=get_steps(len(tr_data), batch_size), \n                              validation_data= valid_generator,\n                              validation_steps= get_steps(len(val_data), batch_size),\n                              epochs=epochs,\n                              callbacks=callbacks, \n                              verbose=1)","91214a04":"# Plot training & validation accuracy values\nfig, ax = plt.subplots()\nplt.plot(history_3.history['acc'])\nplt.plot(history_3.history['val_acc'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Val'], loc='upper left')\nplt.show()","e632cb67":"test_generator.reset()\nprediction = model_3.predict_generator(\n    generator = test_generator,\n    steps = get_steps(len(test_df), batch_size),\n    verbose=1\n)","961436af":"predicted_class_indices=np.argmax(prediction, axis=1)\n\n# Generator class dictionary mapping\nlabels = (train_generator.class_indices)\nlabels = dict((v,k) for k,v in labels.items())\npredictions = [labels[k] for k in predicted_class_indices]\n\nsubmission = pd.read_csv(os.path.join(path, 'sample_submission.csv'))\nsubmission[\"class\"] = predictions\nsubmission.to_csv(\"submission3.csv\", index=False)\nsubmission.head()","d67ccd6f":"\ud074\ub798\uc2a4\uc758 \ubd84\ud3ec\ub97c \ud655\uc778\ud55c \uacb0\uacfc, class \ub2f9 \ud3c9\uade0 51\uc7a5\uc758 \uc774\ubbf8\uc9c0\uac00 \uc788\uc73c\uba70 \ub300\uccb4\uc801\uc73c\ub85c \uace0\ub974\uac8c \uc774\ubbf8\uc9c0\uac00 \ubd84\ud3ec\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.","ef002b09":"\ub2e4\uc74c\uc73c\ub85c validation dataset\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4. \n\n\uac04\ud639 validation dataset\uc5d0 \ubaa8\ub4e0 class\uac00 \ud3ec\ud568\ub418\uc9c0 \uc54a\uc73c\uba74 \ubc11\uc5d0\uc11c \uc5d0\ub7ec\uac00 \ubc1c\uc0dd\ud558\uac8c \ub429\ub2c8\ub2e4. \uc774 \ubb38\uc81c\ub97c \ubc29\uc9c0\ud558\uae30 \uc704\ud574 `train_test_split`\uc758 `stratify` \ud30c\ub77c\ubbf8\ud130 \uac12\uc5d0 target\uac12(class)\ub97c \ub123\uc73c\uba74 target \uac12\uc774 \uace0\ub974\uac8c \ubf51\ud78c \ub370\uc774\ud130\uc14b\uc744 \uc5bb\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4.","4eef9fc6":"\ub370\uc774\ud130\ub294 \ucd1d 196\uac1c\uc758 class\ub85c \uc774\ub8e8\uc5b4\uc9c4 10016\uc7a5\uc758 Trianing \uc774\ubbf8\uc9c0, 6169\uc7a5\uc758 Test \uc774\ubbf8\uc9c0\uc785\ub2c8\ub2e4.","8d7e8d22":"\ud30c\uc77c\uc744 \uc9c1\uc811 \ubc1b\uc544\uc11c \uc0ac\uc6a9\ud558\ub294 \uacbd\uc6b0, \ud30c\uc77c\uc758 \uc218\uac00 \uc11c\ub85c \ub9de\ub294\uc9c0 \ud655\uc778\ud569\ub2c8\ub2e4.","82cf4d70":"## 3. Model training \n\n\uc774\uc81c \ubaa8\ub378\uc744 \ud559\uc2b5\ud558\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4. \uc704\uc5d0\uc11c \uc81c\ub108\ub808\uc774\ud130\ub97c \uc0ac\uc6a9\ud558\ubbc0\ub85c `fit_generator`\ub85c \ud559\uc2b5\uc744 \uc9c4\ud589\ud569\ub2c8\ub2e4. \uc774\ub54c`callback`\uc744 \uc124\uc815\ud574\uc11c \ud559\uc2b5 \uc9c4\ud589\uc911\uc5d0 learning rate\ub97c \ubcc0\uacbd\ud558\uac70\ub098 earlystopping \uc744 \uc124\uc815\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. ","e9d830d9":"\uc774\ubbf8\uc9c0\uc758 \uc885\ud6a1\ube44\ub97c \uc0b4\ud3b4\ubcf4\uba74 \uc885\ud6a1\ube44\uac00 1 \uc774\uc0c1\uc758 \uc218\uce58\ub85c \ubd84\ud3ec\ub418\ub294 \uac83\uc73c\ub85c \ubcf4\uc544, \ub300\ubd80\ubd84 \uc774\ubbf8\uc9c0\uc758 width\uc758 \uae38\uc774\uac00 \ub354 \uae34 \uac83\uc744 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4.","02f3ad79":"\ub9c8\uc9c0\ub9c9\uc73c\ub85c optimizer\uc640 loss, metrics\uc744 \uc815\ud574\uc11c \ubaa8\ub378\uc744 \uc644\uc131\ud569\ub2c8\ub2e4.","f4409b1f":"CSV \ud30c\uc77c\uc5d0\ub294 \ud30c\uc77c\uba85\uacfc class\uba85, bounding_box\uc758 \uc88c\ud45c\uac12\uc774 \ub4e4\uc5b4\uc788\uc2b5\ub2c8\ub2e4.\n\n* img_file - \ub370\uc774\ud130 \uc14b\uc758 \uac01 \ub85c\uc6b0\uc640 \uc5f0\uacb0\ub418\ub294 \uc774\ubbf8\uc9c0 \ud30c\uc77c \uc774\ub984\n* bbox_x1 - \ubc14\uc6b4\ub529 \ubc15\uc2a4 x1 \uc88c\ud45c (\uc88c\uc0c1\ub2e8 x)\n* bbox_y1 - \ubc14\uc6b4\ub529 \ubc15\uc2a4 y1 \uc88c\ud45c (\uc88c\uc0c1\ub2e8 y)\n* bbox_x2 - \ubc14\uc6b4\ub529 \ubc15\uc2a4 x2 \uc88c\ud45c (\uc6b0\ud558\ub2e8 x)\n* bbox_y2 - \ubc14\uc6b4\ub529 \ubc15\uc2a4 y2 \uc88c\ud45c (\uc6b0\ud558\ub2e8 y)\n* class - \uc608\uce21\ud558\ub824\ub294 \ucc28\uc885(Target)\n* id - \uac01 \ub370\uc774\ud130 \uc14b\uc5d0 \uae30\uc785 \ub418\uc5b4 \uc788\ub294 \ud074\ub798\uc2a4 id\n* name - \ud074\ub798\uc2a4 id\uc5d0 \ub300\uc751\ub418\ub294 \uc2e4\uc81c \ucc28\uc885 \ub808\uc774\ube14","59c7d8d2":"## 2. Create Model\n\n\ub370\uc774\ud130\ub97c \uc900\ube44\ud588\ub2e4\uba74 \ubaa8\ub378\uc744 \ub9cc\ub4e4\uc5b4\ubd05\uc2dc\ub2e4. \ubaa8\ub378\uad6c\uc870\ub294 \uc9c1\uc811 \ub9cc\ub4e4\uc218\ub3c4 \uc788\uc9c0\ub9cc Keras\uc5d0\uc11c \ub300\ud45c\uc801\uc778 \uba87\uac00\uc9c0 \ubaa8\ub378\uc744 \uc774\uc6a9\ud560 \uc218 \uc788\ub3c4\ub85d \uc81c\uacf5\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4. \ubaa8\ub378\uc758 \uc885\ub958\ub294 [Keras application documnent](https:\/\/keras.io\/applications\/)\uc5d0\uc11c \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uad6c\uc870\ubfd0\ub9cc \uc544\ub2c8\ub77c imagenet \ub370\uc774\ud130\uc14b\uc73c\ub85c \ubbf8\ub9ac \ud559\uc2b5\uc2dc\ud0a8 \ubaa8\ub378\uc758 weight\ub97c \uc0ac\uc6a9\ud560 \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4. \n\n\uc5ec\uae30\uc11c\ub294 Mobilenet\uc774\ub77c\ub294 \uad6c\uc870\ub97c \uc0ac\uc6a9\ud558\uba70, pretrained weight\uc744 \ubd88\ub7ec\uc640\uc11c \ubaa8\ub378\uc744 \ub9cc\ub4e4\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4. \n* \ubaa8\ub378\uc744 \ub370\uc774\ud130(class \uc218)\uc5d0 \ub9de\ucd94\uae30 \uc704\ud574 \ubaa8\ub378\uc758 \ub05d\ubd80\ubd84\uc740 \uac00\uc838\uc624\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.\n* \ubaa8\ub378\uc744 \ub9cc\ub4e4\uace0 \ud559\uc2b5\uc2dc\ud0ac(\uc5c5\ub370\uc774\ud2b8\ud560) weight \uc601\uc5ed\uc744 \uc9c0\uc815\ud569\ub2c8\ub2e4.\n* Optimizer\uc640 loss, metrics\ub97c \uc124\uc815\ud569\ub2c8\ub2e4.","d4964a06":"## 2. Image data\n\n\uc804\uccb4 \uc774\ubbf8\uc9c0 \uc911 \uc77c\ubd80\ub97c \ucd9c\ub825\ud574\uc11c \uc774\ubbf8\uc9c0\ub97c \ud655\uc778\ud574\ubcf4\ub3c4\ub85d\ud558\uaca0\uc2b5\ub2c8\ub2e4. \ub178\ub791\uc0c9 \uc601\uc5ed\uc740 Bounding box\ub85c \ucd94\ucd9c\ud588\uc744 \ub54c\uc758 \uc601\uc5ed\uc785\ub2c8\ub2e4.","c3c664f1":"### **\ubc88\uc678 2. grayscale \uc801\uc6a9\ud558\uae30 with MoblieNet\n\nKeras application\uc758 \ubaa8\ub378 \uad6c\uc870\ub294 input\uc73c\ub85c rgb \uac12\uc774 \uc788\ub294 \uc774\ubbf8\uc9c0\ub9cc \ubc1b\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub2e8\uc21c\ud788 grayscale\ub9cc \uc801\uc6a9\ud560 \uacbd\uc6b0, input\uc5d0\uc11c \uc5d0\ub7ec\uac00 \ubc1c\uc0dd\ud558\uac8c \ub429\ub2c8\ub2e4. \ud574\uacb0\ubc29\ubc95 \uc911 \ud558\ub098\ub85c \uc774\ubbf8\uc9c0\ub97c grayscale\ub85c \ubc14\uafbc \ud6c4 \uac19\uc740 \uac12\uc744 \ubcf5\uc0ac\ud558\uc5ec 3\ucc44\ub110\uc758 \uc774\ubbf8\uc9c0\ucc98\ub7fc \ub9cc\ub4e4\uc5b4 \ubaa8\ub378\uc744 \ud559\uc2b5\uc2dc\ucf1c\ubcf4\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4.\n\n** \ud639\uc2dc \ub2e4\ub978 \ubc29\ubc95\uc774 \uc788\uac70\ub098, \ubaa8\ub378\uc758 input layer \uad6c\uc870\ub9cc \ubcc0\uacbd\ud558\ub294 \ubc29\ubc95\uc744 \uc54c\uace0 \uacc4\uc2e0\ubd84\uc740 \ub313\uae00 \ub0a8\uaca8\uc8fc\uc2dc\uba74 \uac10\uc0ac\ud558\uaca0\uc2b5\ub2c8\ub2e4!\n\n\n* \uacb0\uacfc\uce58\n    - \uae30\ubcf8 \ubaa8\ub378 : 0.87315\n    - grayscale \ubcc0\uacbd \ubaa8\ub378 : 0.85407","d32a0aa5":"### Set up","1a074b06":"# <a id='1'>Data exploration<\/a>","e68c8fb9":"## 1. Csv data","6b7e1fc3":"## <a>Introduction<\/a>\n\n\uc774\ubc88 \ucef4\ud398\ud2f0\uc158\uc5d0\uc11c\ub294, \uc8fc\uc5b4\uc9c4 \uc790\ub3d9\ucc28 \uc774\ubbf8\uc9c0\ub97c \ucd1d 196\uac1c\uc758 \ud074\ub798\uc2a4\ub85c \ubd84\ub958 \uc608\uce21\ud558\ub294 \uac83\uc774 \ubaa9\ud45c\uc785\ub2c8\ub2e4.  \nsample_submission\uc5d0 \ub9e4\ud551\ub41c \ud14c\uc2a4\ud2b8 \uc774\ubbf8\uc9c0\uc758 \ud074\ub798\uc2a4\ub97c \uc608\uce21\ud558\uc5ec \uc81c\ucd9c\ud558\uc2dc\uba74 \ub429\ub2c8\ub2e4.\n\n\ub9ce\uc740 \ub0b4\uc6a9\uc740 [\uae40\ud0dc\uc9c4\ub2d8\uc758 [3rd ML Month] Car Model Classification Baseline](https:\/\/www.kaggle.com\/fulrose\/3rd-ml-month-car-model-classification-baseline)\uc744 \ucc38\uace0\ud558\uc600\uc2b5\ub2c8\ub2e4. \ubd80\uc871\ud558\uac70\ub098 \ud2c0\ub9b0 \ubd80\ubd84\uc788\uc73c\uba74 \ub313\uae00 \ub0a8\uaca8\uc8fc\uc2dc\uba74 \uac10\uc0ac\ud558\uaca0\uc2b5\ub2c8\ub2e4 :)\n\n### Contents\n\n1. Data exploration\n2. Image Classification","397554b4":"\uc81c\ub108\ub808\uc774\ud130\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4. \ub370\uc774\ud130\uc14b \ubcc4\ub85c \ub098\ub220\uc11c \uc0dd\uc131\ud558\uba74 \ub429\ub2c8\ub2e4.\n\n** base_line \ucee4\ub110\uc758 \ucf54\ub4dc\ub97c \uadf8\ub300\ub85c \uc0ac\uc6a9\ud558\ub294 \uacbd\uc6b0, test image\ub97c \uc21c\uc11c\ub300\ub85c \ubd88\ub7ec\uc640\uc57c \uacb0\uacfc\uac12\uc774 \uc81c\ub300\ub85c \uc800\uc7a5\ub418\uae30 \ub54c\ubb38\uc5d0 `shuffle=False`\ub97c \ubc18\ub4dc\uc2dc \uc0ac\uc6a9\ud574\uc57c \ud569\ub2c8\ub2e4. ","c170eb73":"pretrained weight \ubd80\ubd84\uc744 \ud559\uc2b5\uc2dc\ud0ac\uc9c0 \uc124\uc815\ud569\ub2c8\ub2e4. \uae30\ubcf8\uac12\uc740 `True`\uc774\ubbc0\ub85c \uc804\uccb4\ubaa8\ub378\uc744 \ud559\uc2b5\ud560 \ub584\ub294 \uc124\uc815\ud560 \ud544\uc694\uac00 \uc5c6\uc9c0\ub9cc, \ud2b9\uc815 \ubd80\ubd84\uc758 weight\ub97c \uace0\uc815\ud558\uace0 \uc2f6\uc73c\uba74 `False`\ub85c \ubc14\uafd4\uc918\uc57c\ud569\ub2c8\ub2e4.","e31e5490":"\ud558\ub098\uc758 class \uc774\ubbf8\uc9c0\ub9cc \uc0b4\ud3b4\ubcf4\uc558\uc2b5\ub2c8\ub2e4. \ucc28\uc758 \uc815\uba74, \uce21\uba74, \ud6c4\uba74\uae4c\uc9c0 \ub2e4\uc591\ud55c \ubaa8\uc2b5\uc774 \uc800\uc7a5\ub418\uc5b4 \uc788\uc73c\uba70, \ud2b9\ud788 \ucc28\ub7c9\uc758 \uc0c9\uc774 \ub2ec\ub77c\ub3c4 \uac19\uc740 class\ub85c \uc800\uc7a5\ub418\uc5b4 \uc788\ub294 \uc810\uc744 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4.","89ff5afe":"\ud53d\uc140 \uae30\uc900\uc73c\ub85c \uc0b4\ud3b4\ubcf4\uc558\uc744 \ub54c, \uac00\uc7a5 \uc791\uc740 \uc774\ubbf8\uc9c0\uc758 \ud06c\uae30\ub294 `78 * 58` \uc774\uba70 \uac00\uc7a5 \ud070 \uc774\ubbf8\uc9c0\ub294 `5616 * 3744`\uc785\ub2c8\ub2e4. ","362f48e0":"### **\ubc88\uc678 1. Xception \ubaa8\ub378 with cross validation\n\n\ub2e8\uc21c\ud558\uac8c train, validation dataset\ub97c \ub2e4\uc2dc \uad6c\uc131\ud558\ub294 \ubc29\ubc95\uc744 \ucd94\uac00\ud558\uc5ec \ubaa8\ub378 \ud559\uc2b5\uc744 \uc9c4\ud589\ud569\ub2c8\ub2e4.","bc4b173c":"# <a id='0'>3rd ML Month - Car Model Classification<\/a>","32b001e3":"## Crop Image\n\n\ud559\uc2b5 \ub370\uc774\ud130\uc758 \uc815\ud654\ub3c4\ub97c \ub192\uc774\uae30\uc704\ud574 \uc8fc\uc5b4\uc9c4 Boundind box \uc88c\ud45c\ub97c \uc774\uc6a9\ud574\uc11c \ud574\ub2f9 \uc601\uc5ed\uc758 \uc774\ubbf8\uc9c0\ub9cc\uc744 \uc0ac\uc6a9\ud558\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4. Bounding box \uc601\uc5ed\uc744 \ubf51\uc544\ub0b4\ub294 \ubc29\ubc95\uc740 \ub2e8\uc21c\ud788 image\uc5d0\uc11c \ud574\ub2f9 \uc88c\ud45c\uc601\uc5ed\ub9cc \uc120\ud0dd\ud558\uba74 \ub429\ub2c8\ub2e4.\n\n* csv \uc548\uc5d0 \uc788\ub294 \ud30c\uc77c\uba85\uc744 \uadf8\ub300\ub85c \uc0ac\uc6a9\ud558\uae30\uc704\ud574 \uc218\uc815 \ud6c4 \uc774\ubbf8\uc9c0\ud30c\uc77c\uba85\uc740 \ub530\ub85c \ubcc0\uacbd\ud558\uc9c0 \uc54a\uaca0\uc2b5\ub2c8\ub2e4.\n* [\ud5c8\ud0dc\uba85\ub2d8\uc758 [3rd ML Month] Car Image Cropping](https:\/\/www.kaggle.com\/tmheo74\/3rd-ml-month-car-image-cropping)\uc758 output file\uc744 \uac00\uc838\uc640\uc11c \uc0ac\uc6a9\ud558\uba74 \uc774 \uacfc\uc815\uc744 \uc2a4\ud0b5\ud574\ub3c4 \ub429\ub2c8\ub2e4!","1a4302f9":"# <a id='2'>Image classification<\/a>","357975a4":"`preprocessing_function` \ubd80\ubd84\uc5d0\uc11c grayscale \ubcc0\ud658 \ud6c4 \ubcf5\uc0ac\ud558\ub3c4\ub85d \uc218\uc815","a24a0b4e":"## 4. Prediction\n\n\ud559\uc2b5\ud55c \ubaa8\ub378\ub85c test \uc774\ubbf8\uc9c0\ub97c \ub123\uc5b4 \uc608\uce21\uac12\uc744 \uad6c\ud558\uace0 \uacb0\uacfc\uac12\uc744 \uc800\uc7a5\ud574\uc11c `submission` \ud30c\uc77c\uc744 \ub9cc\ub4ed\ub2c8\ub2e4.","05f48f70":"## 1. Data preprocessing\n\n\uc804\ucc98\ub9ac \uacfc\uc815\uc5d0\uc11c\ub294 \ubaa8\ub378\uc5d0 \ub123\uc744 \uc774\ubbf8\uc9c0\uc758 \ud06c\uae30(input size)\uc640 RGB \ucc44\ub110, Augmentation \uc0ac\uc6a9 \uc5ec\ubd80\ub97c \uc9c0\uc815\ud569\ub2c8\ub2e4. Keras\uc5d0\uc11c \uc81c\uacf5\ud558\ub294 \ubaa8\ub378\uad6c\uc870\ub97c \uc4f8 \uacbd\uc6b0\uc5d0\ub294 `preprocess_input`\ub97c import\ud558\uc5ec \uc27d\uac8c \ub370\uc774\ud130 \uc804\ucc98\ub9ac\ub97c \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub610\ud55c \uc81c\ub108\ub808\uc774\ud130(generator)\ub97c \uc0ac\uc6a9\ud55c\ub2e4\uba74 \uc774 \uacfc\uc815\uc744 \uc81c\ub108\ub808\uc774\ud130 \uc0dd\uc131 \ud30c\ub77c\ubbf8\ud130\ub85c \ub123\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n\uc5ec\uae30\uc11c\ub294 \uc8fc\uc5b4\uc9c4 csv \ud30c\uc77c\uc758 \ub370\uc774\ud130\ub97c \uc774\uc6a9\ud574\uc11c \uc81c\ub108\ub808\uc774\ud130\ub97c \uc0dd\uc131\ud558\uc5ec \uc774\ubbf8\uc9c0\ub97c \ubd88\ub7ec\uc624\ub294 \ubc29\ubc95\uc744 \uc0ac\uc6a9\ud558\uaca0\uc2b5\ub2c8\ub2e4. \uc81c\ub108\ub808\uc774\ud130\uc5d0 \ub300\ud55c \uc124\uba85 \ubc0f \ucf54\ub4dc\ub294  [[3rd ML Month] Car Model Classification Baseline](https:\/\/www.kaggle.com\/fulrose\/3rd-ml-month-car-model-classification-baseline)\uc5d0 \uc790\uc138\ud788 \ub098\uc640\uc788\ub124\uc694. baseline \ucee4\ub110\uc5d0\ub3c4 \uc5b8\uae09\ub418\uc5b4 \uc788\ub4ef\uc774 kaggle \ucee4\ub110\uc5d0\uc11c \uc804\uccb4 \uc774\ubbf8\uc9c0\ub97c \uba54\ubaa8\ub9ac\uc5d0 \uc62c\ub9b4\uc218 \uc5c6\uc73c\ubbc0\ub85c \uaf2d \uc788\uc5b4\uc57c\ud569\ub2c8\ub2e4!\n\n* dataframe\uc73c\ub85c \ubd88\ub7ec\uc628 csv \ud30c\uc77c\uc744 \uc774\uc6a9\ud558\uae30 \uc704\ud574\uc11c\ub294 `flow_from_dataframe` \uba54\uc11c\ub4dc\ub97c \uc0ac\uc6a9\ud574\uc11c \uc81c\ub108\ub808\uc774\ud130\ub97c \ub9cc\ub4ed\ub2c8\ub2e4.\n* \uc774\ub54c \uc0ac\uc6a9\ud560 `class` \ub370\uc774\ud130 type\uc774 string \uc774\uc5ec\uc57c\ud569\ub2c8\ub2e4. \ub530\ub77c\uc11c dataframe\uc5d0\uc11c class\uc758 \ub370\uc774\ud130 \ud0c0\uc785\uc744 \ud655\uc778\ud558\uace0 string\uc774 \uc544\ub2c8\ub77c\uba74 \ubcc0\uacbd\ud558\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4.\n* \uc81c\ub108\ub808\uc774\ud130 \ud074\ub798\uc2a4\ub97c \uc815\uc758\ud558\ub294 \uacfc\uc815\uc5d0\uc11c agumentation \ubc29\ubc95 \uc124\uc815\uc740 [\uae40\ud0dc\uc601\uc758 \ucf00\ub77c\uc2a4 imagegenerator](https:\/\/tykimos.github.io\/2017\/06\/10\/CNN_Data_Augmentation\/)\uc5d0 \uc0c1\uc138\ud788 \ub098\uc640\uc788\uc2b5\ub2c8\ub2e4.","e6a7acef":"image classification \uacfc\uc815\uc740 \ub2e4\uc74c\uacfc \uac19\uc774 \uc9c4\ud589\ud569\ub2c8\ub2e4.\n\n    1. \ub370\uc774\ud130 \uc804\ucc98\ub9ac\n    2. \ubaa8\ub378\uc120\ud0dd \ubc0f \uc644\uc131(\uad6c\uc870, optimizer, loss \ub4f1)\n    3. \ubaa8\ub378 \ud559\uc2b5\n    4. \uc608\uce21\n"}}