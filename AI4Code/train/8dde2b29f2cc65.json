{"cell_type":{"7a3a0447":"code","35a6a5a3":"code","a766721e":"code","cec9f0f7":"code","1c26d1ec":"code","c4d22fb5":"code","e3950535":"code","45d71811":"code","18ad7ea7":"code","f7c5bcf3":"code","c12662e8":"code","1e877ca6":"code","75cc1234":"code","6aa8baa4":"code","eb624657":"code","bf27f1b1":"code","33ed1a98":"code","0050013b":"code","f8062436":"code","2b693565":"code","d61aa06d":"code","53f5ffe9":"code","61b97916":"code","9dc57a52":"code","097a38e3":"code","1af7d7ae":"code","fa7ccc0c":"code","9e0b94c2":"code","6ed9995b":"code","169f5520":"code","3e3f61cc":"code","3c28bfd7":"code","070a26ad":"code","c766d728":"code","b9c19ba4":"code","e5fa2825":"code","e8a062b1":"code","47573ab0":"code","d2fa7971":"code","b2f05e77":"code","e16cc57a":"code","6d846372":"code","6ba5f5a5":"code","8092071b":"code","a867d48d":"code","8771a7b8":"code","8488b8fa":"code","c0d6392c":"code","e1085df3":"code","2e6217d8":"code","29154ee4":"code","a9d0f9d2":"code","81190a47":"code","ea39c61c":"markdown","0cd6a241":"markdown","bbff07ae":"markdown","bbb08c7c":"markdown","af316b8b":"markdown","6a2cf698":"markdown","be75f49a":"markdown","b107ed2e":"markdown","960e1899":"markdown","30c8b3b7":"markdown","8153ef1a":"markdown","4498f8a6":"markdown","ddaa82b6":"markdown","8bda7a02":"markdown","8f7403a4":"markdown"},"source":{"7a3a0447":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","35a6a5a3":"df= pd.read_csv('\/kaggle\/input\/real-estate-price-prediction\/Real estate.csv')","a766721e":"df.head()","cec9f0f7":"df.shape","1c26d1ec":"df.info()","c4d22fb5":"df.describe()","e3950535":"sns.pairplot(df)","45d71811":"plt.figure(figsize=(8,5))\nsns.displot(df['Y house price of unit area'] , bins=30 , kde=True )","18ad7ea7":"sns.heatmap(df.corr(), annot=True,cmap='RdYlGn')","f7c5bcf3":"plt.figure(figsize=(16,10))\nfor i in range (len(df.columns)):\n    plt.subplot(3,5,i+1)\n    sns.boxplot(df[df.columns[i]])","c12662e8":"X= df.drop('Y house price of unit area', axis=1)\ny=df['Y house price of unit area']","1e877ca6":"from sklearn.preprocessing import PolynomialFeatures","75cc1234":"polynomial_converter= PolynomialFeatures(degree=3, include_bias=False)","6aa8baa4":"poly_features= polynomial_converter.fit_transform(X)","eb624657":"poly_features.shape","bf27f1b1":"from sklearn.model_selection import train_test_split","33ed1a98":"X_train, X_test, y_train, y_test = train_test_split(poly_features, y, test_size=0.3, random_state=101)","0050013b":"print(X_train)\nprint(y_train)\nprint(X_test)\nprint(y_test)","f8062436":"from sklearn.preprocessing import StandardScaler","2b693565":"scaler= StandardScaler()","d61aa06d":"scaler.fit(X_train)","53f5ffe9":"X_train= scaler.transform(X_train)\nX_test= scaler.transform(X_test)","61b97916":"print(X_train)\nprint(y_train)\nprint(X_test)\nprint(y_test)","9dc57a52":"#Train the Model\nfrom sklearn.linear_model import Ridge","097a38e3":"ridge_model= Ridge(alpha=10)","1af7d7ae":"ridge_model.fit(X_train, y_train)","fa7ccc0c":"#predict Test Data\ny_pred= ridge_model.predict(X_test)","9e0b94c2":"#Evaluating the Model\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\nMAE= mean_absolute_error(y_test, y_pred)\nMSE= mean_squared_error(y_test, y_pred)\nRMSE= np.sqrt(MSE)","6ed9995b":"pd.DataFrame([MAE, MSE, RMSE], index=['MAE', 'MSE', 'RMSE'], columns=['metrics'])","169f5520":"#Train the Model\nfrom sklearn.linear_model import RidgeCV","3e3f61cc":"ridge_cv_model=RidgeCV(alphas=(0.5, 1.0, 10.0), scoring='neg_mean_absolute_error')","3c28bfd7":"ridge_cv_model.fit(X_train, y_train)","070a26ad":"ridge_cv_model.alpha_","c766d728":"#Predicting Test Data\ny_pred_ridge= ridge_cv_model.predict(X_test)","b9c19ba4":"MAE_ridge= mean_absolute_error(y_test, y_pred_ridge)\nMSE_ridge= mean_squared_error(y_test, y_pred_ridge)\nRMSE_ridge= np.sqrt(MSE_ridge)","e5fa2825":"pd.DataFrame([MAE_ridge, MSE_ridge, RMSE_ridge], index=['MAE', 'MSE', 'RMSE'], columns=['Ridge Metrics'])","e8a062b1":"ridge_cv_model.coef_","47573ab0":"from sklearn.linear_model import LassoCV","d2fa7971":"lasso_cv_model= LassoCV(eps=0.01, n_alphas=100, cv=5)","b2f05e77":"lasso_cv_model.fit(X_train, y_train)","e16cc57a":"lasso_cv_model.alpha_","6d846372":"y_pred_lasso= lasso_cv_model.predict(X_test)","6ba5f5a5":"MAE_Lasso= mean_absolute_error(y_test, y_pred_lasso)\nMSE_Lasso= mean_squared_error(y_test, y_pred_lasso)\nRMSE_Lasso= np.sqrt(MSE_Lasso)","8092071b":"pd.DataFrame([MAE_Lasso, MSE_Lasso, RMSE_Lasso], index=['MAE', 'MSE', 'RMSE'], columns=['Lasso Metrics'])","a867d48d":"lasso_cv_model.coef_","8771a7b8":"from sklearn.linear_model import ElasticNetCV","8488b8fa":"elastic_model= ElasticNetCV(l1_ratio=[0.1, 0.5, 0.7, 0.9, 0.95, 0.99, 1],cv=5, max_iter=150000)","c0d6392c":"elastic_model.fit(X_train, y_train)","e1085df3":"elastic_model.l1_ratio_","2e6217d8":"y_pred_elastic=elastic_model.predict(X_test)","29154ee4":"MAE_Elastic= mean_absolute_error(y_test, y_pred_elastic)\nMSE_Elastic= mean_squared_error(y_test, y_pred_elastic)\nRMSE_Elastic= np.sqrt(MSE_Elastic)","a9d0f9d2":"pd.DataFrame([MAE_Elastic, MSE_Elastic, RMSE_Elastic], index=['MAE', 'MSE', 'RMSE'], columns=['Elastic Metrics'])","81190a47":"elastic_model.coef_","ea39c61c":"# Regularization (Ridge - LASSO - ElasticNet)\nIn mathematics, statistics, finance, computer science, particularly in machine learning and inverse problems, regularization is the process of adding information in order to solve an ill-posed problem or to prevent overfitting.  Regularization can be applied to objective functions in ill-posed optimization problems. The regularization term, or penalty, imposes a cost on the optimization function to make the optimal solution unique.  Independent of the problem or model, there is always a data term, that corresponds to a likelihood of the measurement and a regularization term that corresponds to a prior. By combining both using Bayesian statistics, one can compute a posterior, that includes both information sources and therefore stabilizes the estimation process. By trading off both objectives, one choses to be more addictive to the data or to enforce generalization (to prevent overfitting). There is a whole research branch dealing with all possible regularizations. The work flow usually is, that one tries a specific regularization and then figures out the probability density that corresponds to that regularization to justify the choice. It can also be physically motivated by common sense or intuition, which is more difficult.  In machine learning, the data term corresponds to the training data and the regularization is either the choice of the model or modifications to the algorithm. It is always intended to reduce the generalization error, i.e. the error score with the trained model on the evaluation set and not the training data.  One of the earliest uses of regularization is related to the method of least squares. The figured out probability density is the gaussian distribution, which is now known under the name \"Tikhonov regularization\".\n\n","0cd6a241":"### 3: Elastic Net\nIn statistics and, in particular, in the fitting of linear or logistic regression models, the elastic net is a regularized regression method that linearly combines the L1 and L2 penalties of the lasso and ridge methods.","bbff07ae":"###  Scaling the Data","bbb08c7c":"Ridge regression is a method of estimating the coefficients of multiple-regression models in scenarios where independent variables are highly correlated. It has uses in fields including econometrics, chemistry, and engineering.\n\nRidge regression was developed as a possible solution to the imprecision of least square estimators when linear regression models have some multicollinear (highly correlated) independent variables\u2014by creating a ridge regression estimator (RR). This provides a more precise ridge parameters estimate, as its variance and mean square estimator are often smaller than the least square estimators previously derived.","af316b8b":"### Split the Data to Train & Test","6a2cf698":"###  Regularization","be75f49a":"###  Import the Data","b107ed2e":"###  Determine the Features & Target Variable (Lable)","960e1899":"###  EDA","30c8b3b7":"### Ridge Regression (Coosing an alpha value with Cross-Validation)","8153ef1a":"### 1: Ridge Regression","4498f8a6":"###  Preprocessing (Polynomial Conversion)","ddaa82b6":"###  Data Overview","8bda7a02":"###  Import all Necessary Libraries","8f7403a4":"### 2: Lasso Regression\nRegression is a modeling task that involves predicting a numeric value given an input.\nLinear regression is the standard algorithm for regression that assumes a linear relationship between inputs and the target variable. An extension to linear regression invokes adding penalties to the loss function during training that encourages simpler models that have smaller coefficient values. These extensions are referred to as regularized linear regression or penalized linear regression.\n\nLasso Regression is a popular type of regularized linear regression that includes an L1 penalty. This has the effect of shrinking the coefficients for those input variables that do not contribute much to the prediction task. This penalty allows some coefficient values to go to the value of zero, allowing input variables to be effectively removed from the model, providing a type of automatic feature selection. "}}