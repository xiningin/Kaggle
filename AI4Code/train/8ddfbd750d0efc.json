{"cell_type":{"ebfb5de2":"code","54086028":"code","3a4b40e8":"code","33f338ee":"code","45641e85":"code","eb9c6df6":"code","ff382e17":"code","31f87023":"code","ae2f0ed5":"code","43237877":"code","885ac692":"code","1c5bb17b":"code","6ee1b9a7":"code","5d8890bf":"code","90f83ece":"code","cd8255f5":"code","be77deb9":"code","02fb7870":"code","c360c5dd":"code","c147ff57":"code","319b5f33":"code","bbc4eb35":"code","de9c8f06":"code","3ad7011f":"code","6f208971":"code","7bee07d9":"code","2e79af1a":"code","182249f7":"code","47bf3ef7":"code","a7fd888a":"code","e75c750d":"code","cb9c5010":"code","b55ad375":"code","9592a64d":"code","18501a62":"code","e156494b":"markdown","8487440e":"markdown","658ad4db":"markdown","06103740":"markdown","093d718b":"markdown","4c92fe02":"markdown","2e1841f7":"markdown","ef3d98ad":"markdown","c1ca68ae":"markdown","b240b6c2":"markdown","6bd37f26":"markdown","f4d4a4ef":"markdown","a4ba3d0b":"markdown","a4342310":"markdown","4715b676":"markdown","4d95612d":"markdown","d7359714":"markdown","d9a1cec6":"markdown","f026cf38":"markdown","edb9b185":"markdown","aa78a9d7":"markdown","e3f0393b":"markdown","a717f12b":"markdown","07eaf0f0":"markdown","3fb29022":"markdown","1b968656":"markdown","5ea03865":"markdown","03e19ac6":"markdown","fff2cd0a":"markdown","b967797d":"markdown","c570c535":"markdown","9d077f2d":"markdown","6e0cb744":"markdown"},"source":{"ebfb5de2":"import numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfrom os import listdir\nfrom os.path import join\nfrom scipy.io import wavfile\n\nimport IPython.display as ipd\nfrom librosa.feature import melspectrogram\nfrom librosa import power_to_db\nfrom librosa.effects import trim\n\n# plotting utilities\n%matplotlib inline\nplt.rcParams[\"figure.figsize\"] = (8, 4)\nplt.rcParams[\"figure.titleweight\"] = 'bold' \nplt.rcParams[\"figure.titlesize\"] = 'large'\nplt.rcParams['figure.dpi'] = 120\nplt.style.use('fivethirtyeight')\n\nrs = 99","54086028":"files = '\/kaggle\/input\/free-spoken-digit-dataset-fsdd\/recordings\/'\nds_files = listdir(files)\n\nX = []\ny = []\nfor file in ds_files:\n    label = int(file.split(\"_\")[0])\n    rate, data = wavfile.read(join(files, file))\n    X.append(data.astype(np.float16))\n    y.append(label)","3a4b40e8":"len(X), len(y)","33f338ee":"np.unique(y, return_counts = True)","45641e85":"rate = 8000\ndef show_length_distribution(signals, rate = 8000):\n    sampel_times = [len(x)\/rate for x in signals]\n\n\n    f, (ax_box, ax_hist) = plt.subplots(2, sharex=True, gridspec_kw={\"height_ratios\": (.20, .80)})\n\n    # Add a graph in each part\n    sns.boxplot(x = sampel_times, ax=ax_box, linewidth = 0.9, color=  '#9af772')\n    sns.histplot(x = sampel_times, ax=ax_hist, bins = 'fd', kde = True)\n\n    # Remove x axis name for the boxplot\n    ax_box.set(xlabel='')\n\n\n    title = 'Audio signal lengths'\n    x_label = 'duration (seconds)'\n    y_label = 'count'\n\n    plt.suptitle(title)\n    ax_hist.set_xlabel(x_label)\n    ax_hist.set_ylabel(y_label)\n    plt.show()\n    return sampel_times\n\n\nlengths = show_length_distribution(X)","eb9c6df6":"q = 90\nnp.percentile(lengths, q)","ff382e17":"tot_outliers = sum(map(lambda x: x > np.percentile(lengths, q), lengths))\nprint(f'Values outside {q} percentile: {tot_outliers}')","31f87023":"Longest_audio = np.argmax([len(x) for x in X])\nplt.plot(X[Longest_audio])\nplt.title(\"Longest audio signal\");\n\n\nipd.Audio(X[Longest_audio], rate=rate)\n","ae2f0ed5":"Shortest_audio = np.argmin([len(x) for x in X])\nplt.plot(X[Shortest_audio])\nplt.title(\"Shortest audio signal\");\n\nipd.Audio(X[Shortest_audio], rate=rate)\n","43237877":"# by default anything below 10 db is considered as silence\ndef remove_silence(sample, sr= 8000, top_db = 10):\n    \"\"\"This function removes trailing and leading silence periods of audio signals.\n    \"\"\"\n    y = np.array(sample, dtype = np.float64)\n    # Trim the beginning and ending silence\n    yt, _ = trim(y, top_db= top_db)\n    return yt","885ac692":"X_tr = [remove_silence(x) for x in X]\n\nshow_length_distribution(X_tr);","1c5bb17b":"plt.plot(X_tr[Longest_audio])\nplt.title(\"Longest recording after trimming\");\n\n\nipd.Audio(X_tr[Longest_audio], rate=rate)","6ee1b9a7":"N = int(rate * 0.8) # 0.8 is the upper limit of trimmed audio length\nX_uniform = []\nfor x in X_tr:\n    if len(x) < N:\n        X_uniform.append(np.pad(x, (0, N - len(x)), constant_values = (0, 0)))\n    else:\n        X_uniform.append(x[:N])","5d8890bf":"def into_bins(X, bins = 20):\n    \"\"\"This functions creates bins of same width and computes mean and standard deviation on those bins\n    \"\"\"\n    X_mean_sd = []\n    for x in X:\n        x_mean_sd = []\n        As = np.array_split(np.array(x), 20)\n        for a in As:\n            mean = np.round(a.mean(dtype=np.float64), 4)\n            sd = np.round(a.std(dtype=np.float64), 4)\n            x_mean_sd.extend([mean, sd])\n\n        X_mean_sd.append(x_mean_sd)\n    return np.array(X_mean_sd)","90f83ece":"from sklearn.ensemble import RandomForestClassifier as RFC\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n\nfrom sklearn import svm\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline","cd8255f5":"for bins in range(20,101,20):\n    X_mean_sd = into_bins(X_uniform, bins)\n    X_train, X_test, y_train, y_test = train_test_split(X_mean_sd, y, test_size = 0.20, random_state = rs)\n    clf = RFC()\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    acc = accuracy_score(y_test, y_pred)\n    p,r,f,s = precision_recall_fscore_support(y_test, y_pred)\n    print(f\"for {bins} bins, f-macro average:{f.mean()}, accuracy: {acc}\")","be77deb9":"X_time = into_bins(X_uniform, 60)\nX_train, X_test, y_train, y_test = train_test_split(X_time, y, test_size = 0.20, random_state = rs)","02fb7870":"param_grid = {\n    \"n_estimators\": [100,150,200],\n    \"criterion\": [\"gini\", \"entropy\"],\n    \"min_impurity_decrease\": [0.0,0.05,0.1]\n}\n\nclf = RFC(random_state = rs, n_jobs = -1 )\ngrid_search = GridSearchCV(clf, param_grid, scoring = \"f1_macro\", cv = 5)\ngrid_search.fit(X_train, y_train)\n\nprint(\"best Parameters for RF model:\\n\", grid_search.best_params_)\nprint(\"best score:\", grid_search.best_score_)\nprint(\"\\n\\n Results on test dataset:\\n\\n\")\ny_pred = grid_search.predict(X_test)\nprint(classification_report(y_test, y_pred))","c360c5dd":"steps = [('scaler', StandardScaler()), ('SVM', svm.SVC())]\npipeline = Pipeline(steps)\n\nparameteres = {'SVM__C':[5,10,20], 'SVM__kernel':[\"linear\", \"poly\", \"rbf\"]}\ngrid_search = GridSearchCV(pipeline, param_grid=parameteres, cv=5)\ngrid_search.fit(X_train, y_train)\n\nprint(\"best Parameters for RF model:\\n\", grid_search.best_params_)\nprint(\"best score:\", grid_search.best_score_)\nprint(\"\\n\\n Results on test dataset:\\n\\n\")\ny_pred = grid_search.predict(X_test)\nprint(classification_report(y_test, y_pred))","c147ff57":"# Plot the spectrogram of power on log scale\n\n# fig, ax = plt.subplots(figsize = (8,6))\n\npowerSpectrum, freqenciesFound, time, imageAxis = plt.specgram(X[np.random.randint(100)], Fs=rate, scale = \"dB\")\ncbar = plt.gcf().colorbar(imageAxis)\ncbar.set_label('db')\nplt.grid()\nplt.suptitle(\"Spectrogram of a signal\")\nplt.xlabel('Time')\nplt.ylabel('Frequency')\nplt.show()   ","319b5f33":"def ft_mean_std(X, n, f_s = 8000):\n    \"\"\"Computes mean and std of each n x n block of spectrograms of X\n       empty bins contains mean values of that column matrices\n       \n    Parameters:\n        X: 2-d sampling array\n        n: number of rows or columns to split spectogram\n    Returns:\n        A 2-d numpy array - feature Matrix with n x 2 x n features as columns\n    \"\"\"\n    X_sp = [] #feature matrix\n    for x in X:\n        sp = power_to_db(melspectrogram(x, n_fft= len(x)), np.mean)\n        x_sp = [] #current feature set\n        # split the rows\n        for v_split in np.array_split(sp, n, axis = 0):\n            # split the columns\n            for h_split in np.array_split(v_split, n, axis = 1):\n                if h_split.size == 0: #happens when number of culumns < n\n                    m = np.median(v_split).__round__(4)\n                    sd = np.std(v_split).__round__(4)\n                else:\n                    m = np.mean(h_split).__round__(4)\n                    sd = np.std(h_split).__round__(4)\n                x_sp.extend([m,sd])\n                \n        X_sp.append(x_sp)\n\n    return np.array(X_sp)","bbc4eb35":"X_ft = ft_mean_std(X, 10)\nlen(X_ft)","de9c8f06":"models = {\n    \"rfc\": RFC(random_state=rs),\n    \"svm\": Pipeline([('scaler', StandardScaler()), ('SVM', svm.SVC())])\n}\nscores = {}\nfor n in range(3,20,2):\n    X_ft = ft_mean_std(X, n)\n    X_train, X_test, y_train, y_test = train_test_split(X_ft, y, test_size = 0.20, random_state = rs)\n    score = []\n    for model in models:\n        clf = models[model]\n        clf.fit(X_train, y_train)\n        y_pred = clf.predict(X_test)\n        p,r,f,s = precision_recall_fscore_support(y_test, y_pred)\n        score.append((model, np.mean(f)))\n    scores[n] = score","3ad7011f":"rf_scores = [x[0][1] for x in scores.values()]\nsvm_scores = [x[1][1] for x in scores.values()]\nx = scores.keys()\n\nplt.plot(x, rf_scores, label = 'RF')\nplt.plot(x, svm_scores, label= 'SVM')\n\nplt.legend(loc = (1,.8))\nplt.suptitle(\"Model evaluation on different n. of bins\")\nplt.xlabel(\"n. of bins\")\nplt.ylabel('mean f-score')\nplt.show()","6f208971":"X_ft = ft_mean_std(X, 10)\nX_train, X_test, y_train, y_test = train_test_split(X_ft, y, test_size = 0.20, random_state = rs)","7bee07d9":"param_grid = {\n    \"n_estimators\": [100,150,200],\n    \"criterion\": [\"gini\", \"entropy\"],\n    \"min_impurity_decrease\": [0.0,0.05,0.1]\n}\n\nclf = RFC(random_state = rs, n_jobs = -1 )\nrf_search = GridSearchCV(clf, param_grid, scoring = \"f1_macro\", cv = 5)\nrf_search.fit(X_train, y_train)\n\nprint(\"best Parameters for RF model:\\n\", rf_search.best_params_)\nprint(\"best score:\", rf_search.best_score_)\nprint(\"\\n\\n Results on test dataset:\\n\\n\")\ny_pred = rf_search.predict(X_test)\nprint(classification_report(y_test, y_pred))","2e79af1a":"rfc = RFC(n_estimators= 200, criterion= 'gini', min_impurity_decrease= 0.0,random_state = rs, n_jobs = -1 )\nscores = cross_val_score(rfc, X_ft, y, cv=10, scoring = 'accuracy', n_jobs = -1)\nreport = f\"\"\"Average accuracy of Random Forest model: {np.mean(scores):.2f}\nwith a standard deviation of {np.std(scores):.2f}\n\"\"\"\nprint(report)","182249f7":"steps = [('scaler', StandardScaler()), ('SVM', svm.SVC())]\npipeline = Pipeline(steps)\n\nparameteres = {'SVM__C':[5,10,20], 'SVM__kernel':[\"linear\", \"poly\", \"rbf\"]}\nsvm_search = GridSearchCV(pipeline, param_grid=parameteres, cv=5)\nsvm_search.fit(X_train, y_train)\n\nprint(\"best Parameters for RF model:\\n\", svm_search.best_params_)\nprint(\"best score:\", svm_search.best_score_)\nprint(\"\\n\\n Results on test dataset:\\n\\n\")\ny_pred = svm_search.predict(X_test)\nprint(classification_report(y_test, y_pred))","47bf3ef7":"steps = [('scaler', StandardScaler()), ('SVM', svm.SVC(C= 20, kernel= 'rbf'))]\npipeline = Pipeline(steps)\nscores = cross_val_score(pipeline, X_ft, y, cv=10, scoring = 'accuracy', n_jobs = -1)\nreport = f\"\"\"Average accuracy of SVM model: {np.mean(scores):.2f}\nwith a standard deviation of {np.std(scores):.2f}\n\"\"\"\nprint(report)","a7fd888a":"def split(array,w_bins):\n    \"\"\"Split a matrix into sub-matrices of equal size.\"\"\"\n\n    # original dimensions\n    rows, cols = array.shape\n    # size of sub matrices\n    sub_rows = rows\/\/w_bins + 1 * rows%w_bins\n    sub_cols = cols\/\/w_bins + 1 * cols%w_bins\n    # padding to properly fit\n    pad_rows = sub_rows*w_bins - rows\n    pad_cols = sub_cols*w_bins - cols\n    padded_array = np.pad(array, ((0,pad_rows), (0, pad_cols)))\n    \n    rows, cols = padded_array.shape\n    return (padded_array.reshape(rows\/\/sub_rows, sub_rows, -1, sub_cols)\n                 .swapaxes(1, 2)\n                 .reshape(-1, sub_rows, sub_cols))\n\n\ndef split_ft_mean_std(X, n):\n    \"\"\" Computes mean and std of each n x n block of spectrograms of X\n        bins are padded with zeros to equaly divide in n x n matrices.\n        \n    Parameters:\n        X: 2-d sampling array\n        n: number of rows or columns to split spectogram\n    Returns:\n        A 2-d numpy array - feature Matrix with n x n x 2 features\n    \"\"\"\n    f_s = 8000\n    X_sp = [] #feature matrix\n    for x in X:\n        sp = power_to_db(melspectrogram(x, n_fft= len(x)), np.mean)\n        blocks = split(sp,n)\n        mean = blocks.mean(axis = (-1,-2))\n        std = blocks.std(axis = (-1,-2))\n        X_sp.append(np.hstack((mean,std)))\n    return np.array(X_sp)","e75c750d":"%timeit -n2 -r1 ft_mean_std(X, 10)","cb9c5010":"%timeit -n2 -r1 split_ft_mean_std(X, 10)","b55ad375":"steps = [('scaler', StandardScaler()), ('SVM', svm.SVC())]\npipeline = Pipeline(steps)\n\nparameteres = {'SVM__C':[5,10,20], 'SVM__kernel':[\"linear\", \"poly\", \"rbf\"]}\n","9592a64d":"X_ft = split_ft_mean_std(X, 10)\nX_train, X_test, y_train, y_test = train_test_split(X_ft, y, test_size = 0.20, random_state = rs)\n\nsvm_search = GridSearchCV(pipeline, param_grid=parameteres, cv=5)\nsvm_search.fit(X_train, y_train)\n\nprint(\"best Parameters for RF model:\\n\", svm_search.best_params_)\nprint(\"best score:\", svm_search.best_score_)\nprint(\"\\n\\n Results on test dataset:\\n\\n\")\ny_pred = svm_search.predict(X_test)\nprint(classification_report(y_test, y_pred))","18501a62":"steps = [('scaler', StandardScaler()), ('SVM', svm.SVC(C= 20, kernel= 'rbf'))]\npipeline = Pipeline(steps)\nscores = cross_val_score(pipeline, X_ft, y, cv=10, scoring = 'accuracy', n_jobs = -1)\nreport = f\"\"\"Average accuracy of SVM model: {np.mean(scores):.2f}\nwith a standard deviation of {np.std(scores):.2f}\n\"\"\"\nprint(report)","e156494b":"# Concluisons\n\nThe proposed approach obtains results that are outperforming naive baseline we defined in the beginning.  \nIt does so by leveraging both timeand frequency-based features.  \nWe have empirically shown that the selected classifiers perform similarly for this specific task, achieving satisfactory results in terms of macro f1 score and accuracy.\nWe can further improve by using different set of hyperparameters. The results obtained, however, are already very promising. This classification problem is indeed quite easy and the datasets available are very limited.  \n ","8487440e":"# Time domain analysis\n## Feature Extraction from time domain:\n\n99 percentile of audio length is around 0.92 seconds.  \nWe will remove the leading and trailing silence from signals to see if we get different distribution of length.","658ad4db":"We get comparable results but model is more efficient now.  \nThe reuslts can be improved by tuning the appropriate number of splits (as we did earlier).","06103740":"## In particular\nwe shall see:\n1. How to play audio files in Python\n2. How to sample audio signal into digital form\n3. How to remove leading and trailing noise (e.g. silence) from audio\n4. How to set a naive baseline (use of time domain)\n5. How to combine time and frequency domain features (Spectogram)\n6. How to train, build and predict using different machine learning models","093d718b":"## Hyperparameters tuning","4c92fe02":"# Results\n\nAlthough results are quite satisfactory, we can use other techniques to split the spectogram matrix.  \nOne other way of splitting spectorgram is to _pad_ it such that each sub matrix has identical shape.  \nThis way we also avoid the for-loops which is performance killer. ","2e1841f7":"### Support vector classifier","ef3d98ad":"In this notebook, I introduce a possible approach to the *Free Spoken Digit Dataset* classification problem.  \nThe machine learning models are able to predict audio labels with an accuracy of 98%.\n","c1ca68ae":"# Basic EDA","b240b6c2":"### Random Forest Classifier","6bd37f26":"We will create a matrix with uniform length of columns to allign all recordings.  \nAll signals will have rate*0.8 data points.","f4d4a4ef":"# Classification models","a4ba3d0b":"As expected, this new method is twice as fast as the previous one.  \nLet's compare the results:","a4342310":"We can explore different recordings to see how they are trimmed.","4715b676":"### Number of bins","4d95612d":"With 60 bins we are able to get comparable results.  \nNow we will train two models via grid search to optimize the configuration.","d7359714":"## Hyperperameter tuning","d9a1cec6":"### Load data\n\nThe Free Spoken Digit Dataset is a collection of audio recordings of utterances of digits (\u201czero\u201d to \u201cnine\u201d) from different people.  \nThe goal of this competition is to correctly identify the digit being uttered in each recording. ","f026cf38":"Number of bins is an hyperparameter.  \nWe will try different n. of bins with default configurations of Random Forest Classifier.","edb9b185":"### Support Vector machines","aa78a9d7":"# Abstract","e3f0393b":"We have seen that both time and frequency domains contain useful information regarding the recordings.  \nWe can leverage both by using the spectrogram of each signal.\n  \nTo extract features from a specogram of given signal, we divide it into N x N sub matrices of nearly identical shape.  \nLater, we compute mean and standard deviation of these submatrices and consider them as features set.  \nNumber of sub matrices is considered as an hyperparameter for classifier.","a717f12b":"### Import Necessary Libraries","07eaf0f0":"# Spectorgrams","3fb29022":"## Model building","1b968656":"## Feature extraction from Power spectogram","5ea03865":"We can select 10 as initial number of bins. Both models are stable in the neighborhood of 10.  \nwe can check the performance of models with their optimal configurations.\n","03e19ac6":"![feature%20extraction.jpg](attachment:feature%20extraction.jpg)","fff2cd0a":"The problem is well balanced: for each of the classes we have 300 samples in dataset.  \nAll recordings are sampled at the rate of 8 kHZ\n\nAudio signals have different length.  \nSome of them have leading and silence intervals. Let's analyze that first.","b967797d":"In a spectoral representation of audio signals, we get time on x-axis and different frequencies on y-axis.\nValues in the matrix represent different properties of audio singal related to particular time and frequency. (amplitude, power ecc)","c570c535":"These outliers will be later handled according to the proposed solutions.  \n\n__We can look at some extreme cases:__","9d077f2d":"__Results:__  \n\nWe are able to set a baseline for other models. The baseline accuracy and f1 macro average are 0.59 and 0.58 respectively.\n","6e0cb744":"### Random forest classifier"}}