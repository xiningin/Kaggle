{"cell_type":{"5f1ee938":"code","dd30e7b5":"code","bd340f0b":"code","72d0ad92":"code","286a0d08":"code","f85157c3":"code","ad6f9a25":"code","6896d088":"code","b8fc0260":"code","88538232":"code","be836a15":"markdown","bbcab190":"markdown","1988f3f8":"markdown","b83c2213":"markdown","48059ba7":"markdown","cdccdbe0":"markdown","f6146e54":"markdown","1b0f06d1":"markdown","913477cd":"markdown"},"source":{"5f1ee938":"# ! pip install txtai","dd30e7b5":"# import the required packages and function\nimport numpy as np\nfrom txtai.embeddings import Embeddings\nembed = Embeddings({\"method\": \"transformers\", \"path\": \"sentence-transformers\/bert-base-nli-mean-tokens\"})","bd340f0b":"# Please provide your own API in the below input text box:\nimport wandb\napi = wandb.Api()\n\n# you can register for free on https:\/\/wandb.ai\/ \n# and then can find unique API under \"settings\"","72d0ad92":"# some random examples\nTxtSections = [\"India is on second spot with close to million confirmed corona virus cases\",\n               \"US President Donald Trump on Sunday declared himself immune from Covid-19 as he prepares to return to the campaign trail in a fight to regain ground against surging White House rival Joe Biden\",\n               \"If climate change was a somewhat abstract notion a decade ago, today it is all too real for Californians fleeing wildfires and smothered in a blanket of smoke, the worst year of fires on record\",\n               \"Apple Reaches $2 Trillion. Apple is the first U.S. company to hit that value, a staggering ascent that began in the pandemic\",\n               \"Firefighters in Australia are trying to take advantage of cooler and damper weather to slow the spread of the devastating fires\",\n               \"Oscar for Parasite, the film received Academy Awards for best picture and best director, the honors set off cheers and a burst of pride in a country fearful of being overlooked\"]","286a0d08":"TxtSections","f85157c3":"print(\"%-20s %s\" % (\"Input Query\", \"Best Match\"))\nprint(\"-\" * 100)\n\nfor input_query in (\"film awards\", \"happy story\",\"tech news\",\"weather report\",\"health\",\"tragedy\", \"asia\"):\n    # Get index of best text section that best matches the input query\n    indx = np.argmax(embed.similarity(input_query, TxtSections))\n    # print the input query and the best match\n    print(\"%-20s %s\" % (input_query, TxtSections[indx]))\n","ad6f9a25":"# Create an index for the list of sections\nembed.index([(idx, text, None) for idx, text in enumerate(TxtSections)])\n\nprint(\"%-20s %s\" % (\"Input Query\", \"Best Match\"))\nprint(\"-\" * 100)\n\n\n# Run an embeddings search for each query\nfor input_query in (\"film awards\", \"happy story\",\"tech news\",\"weather report\",\"health\",\"tragedy\", \"asia\"):\n    # Extract uid of first result\n    # search result format: (uid, score)\n    indx = embed.search(input_query, 1)[0][0]\n    \n    # print the input query and the best match\n    print(\"%-20s %s\" % (input_query, TxtSections[indx]))\n    \n","6896d088":"# save the index\nembed.save(\"index\")\n\n# view\n!ls","b8fc0260":"# load the index\nembeddings = Embeddings()\nembeddings.load(\"index\")","88538232":"indx = embeddings.search(\"climate change\", 1)[0][0]\nprint(TxtSections[indx])","be836a15":"## Step 3:\n\n**Building an Embeddings Index**\n\nFor small list of texts, the above code works fine. But for large dataset of documents, it doesn't make sense to tokenize each of the words and convert to embeddings and pass into query. \n\nLuckily, to overcome above challenge, TxtAI supports building pre-computed indices which can significantly improve performance of the process.\n\nSo in the following examplewe will build an index that stores the text embeddings.","bbcab190":"### **Installation**\n\n**Since TxtAI was developed on python (supports python version 3.6+), you can easily install this library via pip and PyPI.**\n\nTo install this library:\n","1988f3f8":"Awesome! \n\nWe can see that the input query has been correctly matched with the respective keyword of the examples.\n\nThe example above shows for almost all of the queries, the actual text isn't stored in the list of text sections. This is the true power of transformer models over token based search.","b83c2213":"**Txtai is an AI-powered search engine that is built based on indexing over text sections.**\n\n**It supports building text indices to perform similarity searches and between the sections of the text and the query typed in the search bar.**\n\n**Txtai can also be used to build an interactive question and answer machine.**\n\n**Referece: https:\/\/github.com\/neuml\/txtai**\n\n","48059ba7":"**Now, we will use the similarity method to identify the similarities between the search word and the random examples given above.** ","cdccdbe0":"### **Saving the embeddings**\n\nAt the time of writing this tutorial, indices are not created incrementally, therefore, the index needs a fullto be rebuilt to incorporate new data. In future, this might be improved.\n\nSo let's learn how to save the above created index.\n\nNow we will save the above embeddings indices onto the disk so that it can be reloaded again whenever we require to do so. \n\n","f6146e54":"# **What is TxtAI?**","1b0f06d1":"## Step 2:\n\n### Running similarity queries\n\nAn embedding instance relies on the underlying transformer model to build text embeddings.\n\nLet's now create a list of a few random sentences as shown in the example below:","913477cd":"## Step 1:\n### **Create an Embeddings instance**\n\nThe main entry point of the TxtAI are the embedding instances. An Embeddings instance defines the method used to tokenize and convert a text section into an embeddings vector using transformers. \n\nFirst transformers helps in converting text into tokens, and then into embedding vectors. \nTherefore, whenever we enter words in the search bar, TxtAI understands the information by tokenizing it and retrieves the correct and relevant information for it while using much less memory. \n\nAs first step, let's implement a simple in-memory embedding instance to understand this concept better. ****"}}