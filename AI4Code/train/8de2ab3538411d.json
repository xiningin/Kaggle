{"cell_type":{"c675056a":"code","510db759":"code","cd389b60":"code","0641a6e6":"code","e808a5c2":"code","4e16ece1":"code","4e1b0f81":"code","f17b8e86":"code","96421efc":"code","66d11b46":"code","783bc8f2":"code","5c70128a":"code","49890c13":"code","a829208c":"code","2c9985c2":"code","fd16e1bd":"code","4ccccee3":"code","3e9943dc":"code","c6550608":"code","34b2b87a":"code","e6e62e91":"code","4e151dcc":"code","3a232767":"code","face212a":"code","8ac9d222":"code","d1253c18":"code","06f4604d":"code","6e3682c3":"code","f8311674":"code","94d0a473":"code","aec417b6":"code","0fb9453d":"code","aaf41136":"code","4513595c":"code","b59cf86a":"code","74cf0520":"code","332debf5":"code","af98be70":"code","53264fa0":"code","29cd5407":"code","bee89d90":"code","2b4e9130":"code","c9768525":"code","6869608b":"code","dd3ebc24":"code","4926bab5":"code","a0ad7290":"code","028124d1":"code","f7d32a73":"code","37d57a5c":"code","c89e1b01":"code","79577c6f":"code","f28429a6":"code","1f6a52ba":"code","1617471e":"code","42083379":"code","ce306e17":"code","98485c92":"markdown","6c6936ce":"markdown","59109d56":"markdown","be8f418c":"markdown","b539d13a":"markdown","64090811":"markdown","8443a9a8":"markdown","0aed27ab":"markdown","e69170fa":"markdown","d06acb2e":"markdown","fcb0d0be":"markdown","faf1c86a":"markdown","e73291c0":"markdown","d4fdcd71":"markdown","7b8cb982":"markdown","bc2cb51a":"markdown","58e0b926":"markdown","ca546ffc":"markdown","5f792199":"markdown","212244c4":"markdown","7a35b0f9":"markdown","1c7ec5ea":"markdown","8de170b6":"markdown","4081d179":"markdown","96e8d109":"markdown","0ddeb814":"markdown","03489290":"markdown","0541f067":"markdown","203e6959":"markdown","711652a0":"markdown","65ca08cf":"markdown","7251a956":"markdown","c7e16a58":"markdown","30df5b5f":"markdown","bb830276":"markdown","09f00b1e":"markdown","8f60eeb1":"markdown","065b882a":"markdown","e4aaaa57":"markdown","32c59e51":"markdown","5bd318ac":"markdown","adcf6ea6":"markdown","2f4fb049":"markdown","73f48455":"markdown","9f0bf1b4":"markdown","2aaf8aca":"markdown","c2264ac9":"markdown","de0ce379":"markdown","b0f8aa76":"markdown","f51d3d7e":"markdown"},"source":{"c675056a":"import pandas as pd\nimport numpy as np\nimport scipy as sp\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set()\nfrom time import time\nimport random\nrandom.seed(11)\n\n# suppress warnings: only necessary on Kaggle kernel (because of some spotty package updates)\n# shouldn't be necessary on a properly updated local machine\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=UserWarning)","510db759":"import os\nprint(os.listdir(\"..\/input\"))","cd389b60":"opinions_df = pd.read_csv('..\/input\/opinions_since_1970.csv')\n\n# eliminate a few hundred mini-opinions that are basically short comments on the majority opinion\nopinions_df = opinions_df[opinions_df.text.map(lambda x: len(x) > 3000)]\nprint(opinions_df.shape)\n\n# eliminate lingering per curiam opinions; eliminate the second_dissenting \/ dissenting distinction\nopinions_df = opinions_df[opinions_df.category != 'per_curiam']\nopinions_df.category = opinions_df.category.map(lambda x: x if x != 'second_dissenting' else 'dissenting')\n\n# drop least common authors (Roberts, Kagan, Black, Harlan)\nauthors_with_min_100 = opinions_df.author_name.value_counts()[opinions_df.author_name.value_counts() > 100].index\nopinions_df = opinions_df[opinions_df.author_name.isin(authors_with_min_100)]\n\n# drop justice douglas (213 opinions) for reasons detailed below\nopinions_df = opinions_df[opinions_df.author_name!='justice douglas']\n\nopinions_df.author_name.value_counts()","0641a6e6":"opinions_df.groupby('author_name').agg('mean').year_filed.astype(int).sort_values()","e808a5c2":"import matplotlib.pylab as pylab\nparams = {'legend.fontsize': 12,\n         'axes.labelsize': 12,\n         'axes.titlesize':14}\npylab.rcParams.update(params)","4e16ece1":"yearly_counts = opinions_df.groupby('year_filed').agg({'federal_cite_one': pd.Series.nunique})\nplt.figure(figsize=(10,6))\nplt.plot(yearly_counts.iloc[:46]) # index omits 2017(36) and 2018(1) which have not-yet-catalogued cases\nplt.title('Number of cases per year with at least one attributed opinion (i.e., not decided per curiam)', fontsize=14)\nplt.ylim((0,180))\nplt.xlabel('Year')\nplt.ylabel('Number of cases')\nplt.show()","4e1b0f81":"for category in ['majority', 'concurring', 'dissenting']:\n    plt.figure(figsize=(10,6))\n    sns.distplot(opinions_df[opinions_df.category==category].text.map(lambda x: len(x)))\n    plt.title('distribution of ' + category + ' opinion lengths')\n    plt.xlim((0,150000))\n    plt.xlabel('length (characters) of opinion')\n    plt.show()","f17b8e86":"temporal_sequence = opinions_df.groupby('author_name').agg('mean').year_filed.sort_values().index\n\n# bar graph of number of opinions by justice, arranged by year: absolute -- by category\nplt.figure(figsize=(16,8))\nsns.countplot('author_name', hue='category', order=temporal_sequence, data=opinions_df)\nplt.xticks(rotation=75)\nplt.xlabel('Justice (arranged chronologically by average year of opinion)')\nplt.title('Number of opinions written by each recent justice, by category')\nplt.show()","96421efc":"# bar graph of number of opinions by justice, arranged by year: per-year\nyearly_counts = opinions_df.groupby('author_name').agg(\n    {'year_filed': pd.Series.nunique,\n     'category': 'count' })\nyearly_counts['average'] = yearly_counts.category \/ yearly_counts.year_filed\nplt.figure(figsize=(12,7))\nsns.barplot('author_name','average', order=temporal_sequence, data=yearly_counts.reset_index())\nplt.xticks(rotation=75)\nplt.xlabel('Justice (arranged chronologically by average year of opinion)')\nplt.title('Average opinions per year by each recent justice')\nplt.show()","66d11b46":"opinions_df['word_count'] = opinions_df.text.map(lambda x: len(x.split()))\n\nplt.figure(figsize=(12,7))\nsns.barplot('author_name', 'word_count', order=temporal_sequence, ci=95, data=opinions_df)\nplt.xticks(rotation=75)\nplt.title('Average length of opinion(words) by each recent justice')\nplt.show()","783bc8f2":"# get selection index\ntest_index = np.random.choice(opinions_df.index, size=opinions_df.shape[0]\/\/4, replace=False)\ntrain_index = opinions_df.index[~opinions_df.index.isin(test_index)]\ntoy_index = np.random.choice(opinions_df.index, size=opinions_df.shape[0]\/\/200, replace=False)\n# create a version by position, for a re-indexed version\nre_train_index = [i for i in range(opinions_df.shape[0]) if opinions_df.index[i] in train_index]\nre_test_index = [i for i in range(opinions_df.shape[0]) if opinions_df.index[i] in test_index]\n\n# split data\nop_df = opinions_df.loc[train_index]\ntest_df = opinions_df.loc[test_index]\ntoy_df = opinions_df.loc[toy_index]","5c70128a":"import spacy\nparser = spacy.load('en')\n\nfrom spacy.lang.en.stop_words import STOP_WORDS\nstopword_additions = [\n    \"'s\",\n    '\u2019s',\n    '\\r',\n    'l.ed.2d',\n    'l.ed'\n]\nfor addition in stopword_additions:\n    STOP_WORDS.add(addition)","49890c13":"import re\n\ndef clean(text):\n    # recipe from https:\/\/stackoverflow.com\/questions\/6116978\/how-to-replace-multiple-substrings-of-a-string\n    rep = {\n        '\\s+':' ', # reduces any whitespace to a single space\n        '\\'s':'', # removes possessives, and there are virtually no contractions in the texts\n        '\u2019s':'',\n        '\\r':'',\n        'u. s. c.': 'u.s.c.', # federal statute citation\n        'u. s.': ''\n    }\n    rep = dict((re.escape(k), v) for k, v in rep.items())\n    pattern = re.compile(\"|\".join(rep.keys()))\n    text = pattern.sub(lambda m: rep[re.escape(m.group(0))], text)\n    text = re.sub('\\s+', ' ', text) # doesn't work in the rep dict for some reason...\n    return text\n\ndef parse_and_lemmatize(opinions, get_full_ops=True, get_nameless_ops=False, get_named_ents=False):\n    '''\n    Returns all three lists; if any are marked False, the returned list will be empty:\n        lemmatized_full_ops: a list of lists (one per opinion) of the lemmas in each opinion\n        lemmatized_nameless_ops: a lists of lists of the lemmas (excluding named entitites) in each opinion\n        named_entities: a list of lists of named entities identified in each opinion\n    '''\n    start = time()\n    lemmatized_full_ops = []\n    lemmatized_nameless_ops = []\n    named_entities = []\n    counter = 0\n    parser = spacy.load('en')\n    for opinion in opinions:\n        counter += 1\n        print('Parsing opinion {} of {}'.format(counter, len(opinions)), end='\\r')\n        parsed_opinion = parser(clean(opinion))\n        if get_full_ops:\n            lemmatized_opinion = \\\n                [token.lemma_ for token in parsed_opinion \\\n                 if not token.is_stop and not token.is_punct and not token.like_num]\n            lemmatized_full_ops.append(lemmatized_opinion)\n        if get_nameless_ops:\n            lemmatized_opinion = \\\n                [token.lemma_ for token in parsed_opinion \\\n                 if not token.is_stop and not token.is_punct and not token.like_num and token.ent_iob == 2]\n        lemmatized_nameless_ops.append(lemmatized_opinion)\n        if get_named_ents:\n            named_entities.append([(ent.text, ent.label_) for ent in parsed_opinion.ents])\n\n    print('  Total parsing time:', round((time()-start)\/60, 1), 'minutes    ')\n    \n    return lemmatized_full_ops, lemmatized_nameless_ops, named_entities\n\n\nall_lemmatized_opinions, all_lemmatized_opinions_nonames, named_entity_tuples  = parse_and_lemmatize(\n    opinions_df.text,\n    get_nameless_ops=True,\n    get_named_ents=True\n)\nlemmatized_opinions_with_names = [all_lemmatized_opinions[i] for i in re_train_index]\nlemmatized_opinions_with_names_test = [all_lemmatized_opinions[i] for i in re_test_index]\nlemmatized_opinions_nonames = [all_lemmatized_opinions_nonames[i] for i in re_train_index]\nlemmatized_opinions_nonames_test = [all_lemmatized_opinions_nonames[i] for i in re_test_index]\nnamed_entities = [tup[0] for opinion in named_entity_tuples for tup in opinion]","a829208c":"from collections import Counter\n\nall_words_with_names = [lemma for opinion in lemmatized_opinions_with_names for lemma in opinion]\nn_words = len(set(all_words_with_names))\ncounter = Counter(all_words_with_names)\ntwo_count = len([word for word in counter.keys() if counter[word] >= 2])\nthree_count = len([word for word in counter.keys() if counter[word] >= 3])\n\nprint(\"*** Stats with named entities: ***\")\nprint(\"Number of unique words (likely to include typos, etc.):\", n_words)\nprint(\"Number of words occurring at least twice:\", two_count)\nprint(\"Number of words occurring at least thrice:\", three_count)\n\nall_words_nonames = [lemma for opinion in lemmatized_opinions_nonames for lemma in opinion]\nn_words_2 = len(set(all_words_nonames))\ncounter_2 = Counter(all_words_nonames)\ntwo_count_2 = len([word for word in counter_2.keys() if counter_2[word] >= 2])\nthree_count_2 = len([word for word in counter_2.keys() if counter_2[word] >= 3])\n\nprint(\"\\n*** Stats without named entities: ***\")\nprint(\"Number of unique words (likely to include typos, etc.):\", n_words_2)\nprint(\"Number of words occurring at least twice:\", two_count_2)\nprint(\"Number of words occurring at least thrice:\", three_count_2)","2c9985c2":"# add back in two common general names, and all Xth Amendment references\nfor i, opinion in enumerate(named_entity_tuples):\n    keepnames = [tup[0] for tup in opinion if tup[0] in ['Indian', 'Miranda'] or tup[0].endswith('Amendment')]\n    all_lemmatized_opinions_nonames[i].extend(keepnames)\n\nlemmatized_opinions_nonames = [all_lemmatized_opinions_nonames[i] for i in re_train_index]\nlemmatized_opinions_nonames_test = [all_lemmatized_opinions_nonames[i] for i in re_test_index]","fd16e1bd":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nstart = time()\nall_words = [lemma for opinion in lemmatized_opinions_nonames for lemma in opinion]\ncommon_words = [tup[0] for tup in Counter(all_words).most_common(12000)[100:]] # omit most common 100 words\njoined_opinions = [\n    ' '.join([lemma for lemma in opinion]) # could remove with `if lemma in common_words`, but vectorizer vocab does so\n    for opinion in lemmatized_opinions_nonames]\nprint('Total common-word filtering time:', round((time()-start)\/60, 1), 'minutes')\n\nvectorizer_0 = TfidfVectorizer(\n    lowercase=True,\n    use_idf=True,\n    vocabulary=common_words\n)\n\ntfidf_mat = vectorizer_0.fit_transform(joined_opinions)\nprint('Total vectorizing time:', round((time()-start)\/60, 1), 'minutes')","4ccccee3":"from collections import Counter\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import Normalizer\n\n# remove_list = [\"'s\", '   ', '         ', '    ', '\\r ', '      ','  ', '\u2019s', ' ', '            ']\n\n# make pipeline\nvectorizer = TfidfVectorizer(\n    lowercase=True,\n    use_idf=True,\n    vocabulary=common_words # could omit an additional most common 200 words here if desired\n)\nsvd = TruncatedSVD(300)\nlsa = make_pipeline(vectorizer, svd, Normalizer(copy=False))\n\n# run pipeline\nstart=time()\nlsa_mat_300 = lsa.fit_transform(joined_opinions)\nlsa_mat_200 = lsa_mat_300[:,:200]\nlsa_mat_100 = lsa_mat_300[:,:100]\nprint('Total LSA time:', round((time()-start)\/60, 1), 'minutes')\nprint('lsa_mat shape:', lsa_mat_200.shape)","3e9943dc":"lsa_300_df = pd.concat(\n    [\n        op_df[['author_name','year_filed','case_name', 'category', 'word_count']].reset_index(drop=True), \n        pd.DataFrame(lsa_mat_300)\n    ],\n    axis=1)\n\nlsa_100_df = pd.concat(\n    [\n        op_df[['author_name','year_filed','case_name', 'category', 'word_count']].reset_index(drop=True), \n        pd.DataFrame(lsa_mat_300)\n    ],\n    axis=1)\n\nlsa_100_df.loc[:1]","c6550608":"from wordcloud import WordCloud\n\ndef visualize_components(components_df):\n    n_components = components_df.shape[0]\n    for j in range(n_components \/\/ 2):\n        plt.figure(figsize=(14,5))\n        i = 2*j\n        plt.subplot(121)\n        weights_dict = components_df.loc[i].sort_values(ascending=False)[:25].to_dict()\n        word_cloud = WordCloud(background_color='white', colormap='RdBu').generate_from_frequencies(weights_dict)\n        plt.imshow(word_cloud, interpolation='bilinear')\n        plt.title(\"FEATURE NUMBER \" + str(i+1))\n        plt.axis('off')\n\n        i += 1\n        plt.subplot(122)\n        weights_dict = components_df.loc[i].sort_values(ascending=False)[:25].to_dict()\n        word_cloud = WordCloud(background_color='white', colormap='RdBu').generate_from_frequencies(weights_dict)\n        plt.imshow(word_cloud, interpolation='bilinear')\n        plt.title(\"FEATURE NUMBER \" + str(i+1))\n        plt.axis('off')    \n\n        plt.show()","34b2b87a":"components_df = pd.DataFrame(svd.components_[:20,:], columns=vectorizer.vocabulary)\nvisualize_components(components_df)","e6e62e91":"from sklearn.decomposition import LatentDirichletAllocation\nfrom wordcloud import WordCloud\n\n# tf-idf\nvectorizer_2 = TfidfVectorizer(\n    lowercase=True,\n    use_idf=True,\n    vocabulary=common_words\n)\ntfidf_mat = vectorizer_2.fit_transform(joined_opinions)\n\n# LDA\nstart=time()\nlda_5 = LatentDirichletAllocation(\n    n_components=5,\n    max_iter=20,\n    random_state=11\n)\nlda_mat = lda_5.fit_transform(tfidf_mat)\nprint('Total LDA time:', round((time()-start)\/60, 1), 'minutes')\n\n# visual representation\ncomponents_df = pd.DataFrame(lda_5.components_, columns=vectorizer_2.vocabulary)\nfor i in range(5):\n    weights_dict = components_df.loc[i].sort_values(ascending=False)[:25].to_dict()\n    word_cloud = WordCloud(background_color='white', colormap='RdBu').generate_from_frequencies(weights_dict)\n    plt.figure(figsize=(8,5))\n    plt.imshow(word_cloud, interpolation='bilinear')\n    plt.title(\"FEATURE NUMBER \" + str(i+1))\n    plt.axis('off')\n    plt.show()","4e151dcc":"start=time()\nlda_10 = LatentDirichletAllocation(\n    n_components=10,\n    max_iter=20,\n    random_state=11\n)\nlda_mat = lda_10.fit_transform(tfidf_mat)\nprint('Total LDA time:', round((time()-start)\/60, 1), 'minutes')\n\n# visual representation\ncomponents_df = pd.DataFrame(lda_10.components_, columns=vectorizer_2.vocabulary)\nvisualize_components(components_df)","3a232767":"start=time()\nlda_30 = LatentDirichletAllocation(\n    n_components=30,\n    max_iter=50,\n    random_state=11\n)\nlda_mat = lda_30.fit_transform(tfidf_mat)\nprint('Total LDA time:', round((time()-start)\/60, 1), 'minutes')\n\n# visual representation\ncomponents_df = pd.DataFrame(lda_30.components_, columns=vectorizer_2.vocabulary)\nvisualize_components(components_df)","face212a":"# # LSA-filtered LDA:\n# components_df = pd.DataFrame(svd.components_[:2,:], columns=vectorizer.vocabulary)\n# filtered_words = components_df.loc[0].sort_values(ascending=False)[:25].index\n# print(filtered_words)\n\n# vectorizer_3 = TfidfVectorizer(\n#     lowercase=True,\n#     use_idf=True,\n#     vocabulary=[word for word in common_words if word not in filtered_words]\n# )\n# tfidf_mat_3 = vectorizer_3.fit_transform(joined_opinions)\n\n# lda_20 = LatentDirichletAllocation(\n#     n_components=20,\n#     max_iter=20,\n#     random_state=11\n# )\n# lda_mat = lda_20.fit_transform(tfidf_mat_3)","8ac9d222":"lda_mat.shape","d1253c18":"lda_df = pd.concat(\n    [\n        op_df[['author_name','year_filed','case_name', 'category', 'word_count']].reset_index(drop=True), \n        pd.DataFrame(lda_mat)\n    ],\n    axis=1)\n\nlda_by_year = lda_df[['year_filed'] + list(range(30))].groupby('year_filed').agg('mean')\nlda_by_year.head()","06f4604d":"x = lda_by_year.index[1:-1]\nplt.figure(figsize=(14,10))\n\nfor feature in [14,22,23]:\n    y_vals = list(lda_by_year[feature-1])\n    y = [(y_vals[j] + y_vals[j-1] + y_vals[j+1])\/3 for j in range(1, len(y_vals)-1)] # rolling mean for smoothing\n    plt.plot(x,y)\nplt.legend(labels=[\n    'religion in schools',\n    'unions and employers',\n    'immigration'\n])  \nplt.title(\"Topic prominence in SCOTUS opinions over time\")\nplt.show()","6e3682c3":"temporal_sequence = opinions_df.groupby('author_name').agg('mean').year_filed.sort_values().index\ncorr_df = lsa_300_df.drop('category', axis=1).groupby('author_name').agg('mean').T.corr()\ntypicality = corr_df.sum(axis=1)\n\nmask = np.zeros_like(corr_df, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.set(style=\"white\")","f8311674":"plt.figure(figsize=(10,10))\nsns.heatmap(\n    lsa_300_df.drop('category', axis=1).groupby('author_name').agg('mean').T.corr().loc[\n        temporal_sequence, temporal_sequence\n    ],\n    cmap='Greens',\n    square=True,\n    mask=mask\n)\nplt.title(\"Similarity heatmap of averaged LSA opinion vector for each justice (by year)\")\nplt.show()","94d0a473":"plt.figure(figsize=(10,10))\nsns.heatmap(\n    lsa_300_df.drop('category', axis=1).groupby('author_name').agg('mean').T.corr().loc[\n        typicality.sort_values().index, typicality.sort_values().index\n    ],\n    cmap='Greens',\n    square=True,\n    mask=mask\n)\nplt.title(\"Similarity heatmap of averaged LSA opinion vector for each justice (by distinctness)\")\nplt.show()","aec417b6":"from sklearn.cluster import KMeans\n\njustices_df = lsa_300_df.drop('category', axis=1).groupby('author_name').agg('mean')\n\nkmeans = KMeans(n_clusters=2)\nlabels = kmeans.fit_predict(justices_df)\n\njustices_df['cluster_labels'] = labels\njustices_df.cluster_labels.sort_values()","0fb9453d":"# plot explained variance\nsns.set()\nplt.figure(figsize=(8,6))\nplt.plot(svd.explained_variance_)\nplt.title(\"Explained variance by each successive feature\")\nplt.show()","aaf41136":"from sklearn.manifold import TSNE\ntsne = TSNE(random_state=12)\nstart = time()\ntsne_mat = tsne.fit_transform(lsa_mat_100[:,:65])\ntsne_df = pd.concat(\n    [lsa_100_df[['author_name','category','case_name','year_filed']].reset_index(drop=True), \n     pd.DataFrame(tsne_mat)], \n    axis=1\n) \nprint(\"Elapsed time:\", round((time()-start)\/60, 1), \"minutes\")\n\nplt.figure(figsize=(16,14))\nsns.scatterplot(tsne_df[0], tsne_df[1], s=25, hue=tsne_df.author_name)\nplt.show()","4513595c":"from sklearn.cluster import KMeans, AgglomerativeClustering, SpectralClustering\nfrom sklearn.metrics import adjusted_rand_score, silhouette_score, silhouette_samples\n\ninertia_scores = []\nari_scores = []\nsilhouettes = []\nkvals = range(6,37,3)\nfor k in kvals:\n    print('Processing k =', k, end='\\r')\n    kmeans = KMeans(n_clusters=k, random_state=11)\n    cluster_labels = kmeans.fit_predict(lsa_mat_300)\n    kmeans2 = KMeans(n_clusters=k, random_state=13)\n    cluster_labels2 = kmeans2.fit_predict(lsa_mat_300)\n    ari = adjusted_rand_score(cluster_labels, cluster_labels2)\n    ari_scores.append(ari)\n    inertia_scores.append(kmeans.inertia_)\n    silhouette = round(silhouette_score(lsa_mat_300, cluster_labels), 3)\n    silhouettes.append(silhouette)\n\nplt.figure(figsize=(10,7))\nplt.plot(kvals, inertia_scores \/ np.mean(inertia_scores), label='inertia')\nplt.plot(kvals, ari_scores, label='ARI')\nplt.plot(kvals, silhouettes \/ np.mean(silhouettes), label='silhouette')\nplt.title('Normalized metric scores vs k')\nplt.xlabel(\"k\")\nplt.ylabel('Mean-normalized scores')\nplt.legend()\nplt.show()","b59cf86a":"tfidf_df = pd.DataFrame(tfidf_mat.todense(), columns=vectorizer_2.vocabulary) # tfidf_mat must be the 400: version\n\ndef labels_to_keywords(labels, tfidf_df, n=2):\n    label_dict = {}\n    for label in np.unique(labels):\n        label_dict[label] = list(tfidf_df[labels==label].sum().sort_values(ascending=False).index[:n+3])\n        label_dict[label] = ', '.join([word for word in label_dict[label] if word not in ['ct', 'court', 'appeal']][:n])\n    return [label_dict[label] for label in labels]","74cf0520":"kmeans = KMeans(n_clusters=18, random_state=11)\nlabels = kmeans.fit_predict(lsa_mat_300)\nkmeans_15_labels = labels[:]\nplt.figure(figsize=(14,12))\nsns.scatterplot(tsne_df[0], tsne_df[1], s=25, hue=labels_to_keywords(labels, tfidf_df))\nplt.title('K-means clustering with k=18')\nplt.show()","332debf5":"kmeans = KMeans(n_clusters=24, random_state=11)\nlabels = kmeans.fit_predict(lsa_mat_300)\nkmeans_24_labels = labels[:]\nplt.figure(figsize=(16,14))\nsns.scatterplot(tsne_df[0], tsne_df[1], s=25, hue=labels_to_keywords(labels, tfidf_df))\nplt.title('K-means clustering with k=24')\nplt.show()","af98be70":"# components_df = pd.DataFrame(lda_12.components_, columns=vectorizer_2.vocabulary)\nlabel_dict = {}\nfor label in np.unique(labels):\n    label_dict[label] = list(tfidf_df[labels==label].sum().sort_values(ascending=False).index[:3])\n    label_dict[label] = ', '.join([word for word in label_dict[label] if word !='ct'][:2])\n    \ndef display_wordcloud(label):\n    print(\"LABEL {} ({})\".format(label, label_dict[label]))\n    print(\"examples:\")\n    print(' ','\\n  '.join(list(set(lsa_300_df.case_name[labels==label][:7].values))))\n    weights_dict = tfidf_df[labels==label].sum().sort_values(ascending=False)[:25].to_dict()\n    word_cloud = WordCloud(background_color='white', colormap='RdBu').generate_from_frequencies(weights_dict)\n    plt.figure(figsize=(8,5))\n    plt.imshow(word_cloud, interpolation='bilinear')\n    plt.title(\"LABEL NUMBER \" + str(i))\n    plt.axis('off')\n    plt.show()\n\nfor i in range(24):\n    display_wordcloud(i)","53264fa0":"agglomerative = AgglomerativeClustering(n_clusters=24, linkage='ward')\nlabels = agglomerative.fit_predict(lsa_mat_300)\nward_labels = labels[:]\nplt.figure(figsize=(14,12))\nsns.scatterplot(tsne_df[0], tsne_df[1], s=25, hue=labels_to_keywords(labels, tfidf_df))\nplt.title('Ward agglomerative clustering with n=24')\nplt.show()","29cd5407":"spectral = SpectralClustering(n_clusters=22)\nlabels = spectral.fit_predict(lsa_mat_300)\nspectral_labels = labels[:]\nplt.figure(figsize=(14,12))\nsns.scatterplot(tsne_df[0], tsne_df[1], s=25, hue=labels_to_keywords(labels, tfidf_df))\nplt.title('Spectral Clustering clustering with k=22')\nplt.show()","bee89d90":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\ndef test_models(X_train, X_test, y_train, y_test, mnb=True):\n    start = time()\n    # logistic regression\n    lr = LogisticRegression(solver='lbfgs', multi_class='multinomial', class_weight='balanced')\n    train = lr.fit(X_train, y_train)\n    print(\"**Logistic Regression**\")\n    print('Training set score:', round(lr.score(X_train, y_train), 4))\n    print('Test set score:', round(lr.score(X_test, y_test), 4))\n    \n    if mnb:  # multinomialNB must be non-negative, so can't take LSA components\n        mnb = MultinomialNB()\n        mnb.fit(X_train, y_train)\n        print(\"\\n**Multinomial Naive Bayes**\")\n        print('Training set score:', round(mnb.score(X_train, y_train), 4))\n        print('Test set score:', round(mnb.score(X_test, y_test), 4))\n\n    # linear SVM\n    svc = LinearSVC(class_weight='balanced')\n    svc.fit(X_train, y_train)\n    print(\"\\n**Linear SVM**\")\n    print('Training set score:', round(svc.score(X_train, y_train), 4))\n    print('Test set score:', round(svc.score(X_test, y_test), 4))\n\n    # random forest\n    rfc = RandomForestClassifier(n_estimators=20)\n    rfc.fit(X_train, y_train)\n    print(\"\\n**Random Forest**\")\n    print('Training set score:', round(rfc.score(X_train, y_train), 4))\n    print('Test set score:', round(rfc.score(X_test, y_test), 4))\n    print(\"\\n Total elapsed time:\", round((time()-start)\/60, 1), \"minutes\")","2b4e9130":"# make train\/test splits\ntest_index_2 = np.random.choice(np.arange(tfidf_mat.shape[0]), size=opinions_df.shape[0]\/\/4, replace=False)\ntrain_index_2 = np.setdiff1d(np.arange(tfidf_mat.shape[0]), test_index_2)\n\n# basic training sets\nX_train_tfidf = pd.DataFrame(tfidf_mat[train_index_2].todense()) # could remain csr_mat if I convert all the below to csrs\nX_test_tfidf = pd.DataFrame(tfidf_mat[test_index_2].todense())\nX_train_lsa = lsa_mat_200[train_index_2]\nX_test_lsa = lsa_mat_200[test_index_2]\ny_train = lsa_300_df['author_name'].loc[train_index_2]\ny_test = lsa_300_df['author_name'].loc[test_index_2]\n\n# advanced and cluster labels\nX_train_kmeans_15 = pd.get_dummies(pd.Series(kmeans_15_labels).astype(str)).loc[train_index_2]\nX_test_kmeans_15 = pd.get_dummies(pd.Series(kmeans_15_labels).astype(str)).loc[test_index_2]\nX_train_kmeans_24 = pd.get_dummies(pd.Series(kmeans_24_labels).astype(str)).loc[train_index_2]\nX_test_kmeans_24 = pd.get_dummies(pd.Series(kmeans_24_labels).astype(str)).loc[test_index_2]\nX_train_spectral = pd.get_dummies(pd.Series(spectral_labels).astype(str)).loc[train_index_2]\nX_test_spectral = pd.get_dummies(pd.Series(spectral_labels).astype(str)).loc[test_index_2]\nX_train_ward = pd.get_dummies(pd.Series(ward_labels).astype(str)).loc[train_index_2]\nX_test_ward = pd.get_dummies(pd.Series(ward_labels).astype(str)).loc[test_index_2]\nX_train_all_clusters = pd.concat([X_train_kmeans_24, X_train_spectral, X_train_ward], axis=1)\nX_test_all_clusters = pd.concat([X_test_kmeans_24, X_test_spectral, X_test_ward], axis=1)\nX_train_lda = pd.DataFrame(lda_mat[train_index_2])\nX_test_lda = pd.DataFrame(lda_mat[test_index_2])","c9768525":"test_models(X_train_lsa, X_test_lsa, y_train, y_test, mnb=False)","6869608b":"start = time()\ndef minitest(X_train, X_test, y_train, y_test):\n    lgbmc = LGBMClassifier(class_weight='balanced', min_data=1, min_data_in_bin=1)\n    lgbmc.fit(X_train, y_train)\n    print(\"\\n**LightGBM**\")\n    print('Training set score:', lgbmc.score(X_train, y_train))\n    print('Test set score:', lgbmc.score(X_test, y_test))\n\nminitest(X_train_lsa, X_test_lsa, y_train, y_test)\nprint(\"Elapsed time:\", round((time()-start)\/60, 1), \"minutes\")","dd3ebc24":"test_models(X_train_tfidf, X_test_tfidf, y_train, y_test, mnb=True)","4926bab5":"# decision tree\nfrom sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier(class_weight='balanced')\ndtc.fit(X_train_tfidf, y_train)\nprint(\"\\n**Decision Tree Classifier**\")\nprint('Training set score:', round(dtc.score(X_train_tfidf, y_train), 4))\nprint('Test set score:', round(dtc.score(X_test_tfidf, y_test), 4))","a0ad7290":"test_models(\n    pd.concat([X_train_tfidf, X_train_lda], axis=1),\n    pd.concat([X_test_tfidf, X_test_lda], axis=1),\n    y_train,\n    y_test,\n    mnb=True)","028124d1":"test_models(\n    pd.concat([X_train_tfidf.reset_index(drop=True), X_train_kmeans_15.reset_index(drop=True)], axis=1),\n    pd.concat([X_test_tfidf.reset_index(drop=True), X_test_kmeans_15.reset_index(drop=True)], axis=1),\n    y_train,\n    y_test,\n    mnb=False)","f7d32a73":"test_models(\n    pd.concat([X_train_tfidf.reset_index(drop=True), X_train_spectral.reset_index(drop=True)], axis=1),\n    pd.concat([X_test_tfidf.reset_index(drop=True), X_test_spectral.reset_index(drop=True)], axis=1),\n    y_train,\n    y_test,\n    mnb=True)","37d57a5c":"test_models(\n    pd.concat([X_train_tfidf.reset_index(drop=True), \n               X_train_all_clusters.reset_index(drop=True),\n              X_train_lda.reset_index(drop=True)], axis=1),\n    pd.concat([X_test_tfidf.reset_index(drop=True), \n               X_test_all_clusters.reset_index(drop=True),\n              X_test_lda.reset_index(drop=True)], axis=1),\n    y_train,\n    y_test,\n    mnb=True)","c89e1b01":"# random forest\nstart = time()\nrfc = RandomForestClassifier(n_estimators=500)\nrfc.fit(X_train_tfidf, y_train)\nprint(\"\\n**Random Forest**\")\nprint('Training set score:', round(rfc.score(X_train_tfidf, y_train), 4))\nprint('Test set score:', round(rfc.score(X_test_tfidf, y_test), 4))\nprint(\"\\n Total elapsed time:\", round((time()-start)\/60, 1), \"minutes\")","79577c6f":"# random forest\nX_train_temp = pd.concat(\n    [X_train_tfidf.reset_index(drop=True), \n    X_train_all_clusters.reset_index(drop=True),\n    X_train_lda.reset_index(drop=True)], \n    axis=1\n)\n\nX_test_temp = pd.concat(\n    [X_test_tfidf.reset_index(drop=True),\n     X_test_all_clusters.reset_index(drop=True),\n    X_test_lda.reset_index(drop=True)], \n    axis=1\n)\n\nstart = time()\nrfc = RandomForestClassifier(n_estimators=500)\nrfc.fit(X_train_temp, y_train)\nprint(\"\\n**Random Forest**\")\nprint('Training set score:', round(rfc.score(X_train_temp, y_train), 4))\nprint('Test set score:', round(rfc.score(X_test_temp, y_test), 4))\nprint(\"\\n Total elapsed time:\", round((time()-start)\/60, 1), \"minutes\")","f28429a6":"joined_opinions_test = [\n    ' '.join([lemma for lemma in opinion]) # could remove with `if lemma in common_words`, but vectorizer vocab does so\n    for opinion in lemmatized_opinions_nonames_test]\ntfidf_mat_test = vectorizer_2.transform(joined_opinions_test)\n\nstart=time()\nlsa_mat_test_300 = svd.transform(tfidf_mat_test)\nlsa_mat_test_200 = lsa_mat_test_300[:,:200]\nlsa_mat_test_100 = lsa_mat_test_300[:,:100]\nprint('Total LSA time:', round((time()-start)\/60, 1), 'minutes')\nprint('lsa_mat shape:', lsa_mat_test_200.shape)","1f6a52ba":"# TSNE\ntsne = TSNE(random_state=12)\nstart = time()\ntsne_mat_test = tsne.fit_transform(lsa_mat_test_300[:,:65])\ntsne_test_df = pd.concat(\n    [test_df[['author_name','category','case_name','year_filed']].reset_index(drop=True), \n     pd.DataFrame(tsne_mat_test)], \n    axis=1\n) \nprint(\"Elapsed time:\", round((time()-start)\/60, 1), \"minutes\")\n\nplt.figure(figsize=(14,12))\nsns.scatterplot(tsne_test_df[0], tsne_test_df[1], s=25, hue=tsne_test_df.author_name)\nplt.show()","1617471e":"tfidf_df_test = pd.DataFrame(tfidf_mat_test.todense(), columns=vectorizer_2.vocabulary) # tfidf_mat must be the 400: version\n\ndef labels_to_keywords(labels, tfidf_df, n=2):\n    label_dict = {}\n    for label in np.unique(labels):\n        label_dict[label] = list(tfidf_df[labels==label].sum().sort_values(ascending=False).index[:n+1])\n        label_dict[label] = ', '.join([word for word in label_dict[label] if word not in ['ct', 'court']][:n])\n    return [label_dict[label] for label in labels]","42083379":"kmeans = KMeans(n_clusters=24, random_state=11)\nlabels = kmeans.fit_predict(lsa_mat_test_300)\nplt.figure(figsize=(14,12))\nsns.scatterplot(tsne_test_df[0], tsne_test_df[1], s=25, hue=labels_to_keywords(labels, tfidf_df_test))\nplt.title('K-means clustering with k=24')\nplt.show()","ce306e17":"spectral = SpectralClustering(n_clusters=22)\nlabels = spectral.fit_predict(lsa_mat_test_300)\nspectral_labels = labels[:]\nplt.figure(figsize=(14,12))\nsns.scatterplot(tsne_test_df[0], tsne_test_df[1], s=25, hue=labels_to_keywords(labels, tfidf_df_test))\nplt.title('Spectral Clustering clustering with k=22')\nplt.show()","98485c92":"There's a lot of noise here because there are many factors affecting which cases reach the Supreme Court each year.  But some of the trends may be meaningful: union-related cases have declined with the decrease in unionization until the post-2008-crisis uptick in unionization.  Immigration seems to have peaked around 2000, just after the peak in Mexican immigrant numbers.  And religion in schools seems to have settled somewhat since the 80s and 00s.  (Other topics displayed less interpretable trends.)","6c6936ce":"### Agglomerative clustering\n\nAgglomerative clustering works reasonably well here too, though it tends more toward one big central cluster with spotted outliers.","59109d56":"# Basic data exploration\n\nOur opinions data now contains 7,578 opinions from 1970 through 2018.  Of these:\n- 4388 are majority opinions\n- 2427 are dissenting\n- 955 are concurring\n\nNote that these exclude per curiam (unanimous) agreements, in which cases the opinion is written by the court with no individual author cited.\n\nThe average date of opinion written by each justice is as follows:","be8f418c":"### Spectral clustering\n\nSpectral clustering produces clusters that are very nearly as clean as the k-means clusters, and seem to be picking out some different dimensionality through the middle.  It will definitely be worth taking a look at the interpretability of these clusters as topical clusters, as well as including them as features for supervised learning.","b539d13a":"Of these ten features, six are fairly clear:\n- Feature 1: agriculture law\n- Feature 2: ??\n- Feature 3: elections and voting\n- Feature 4: optometry (?!)\n- Feature 5: pollution\n- Feature 6: shipping and customs\n- Feature 7: employment benefits\n- Feature 8: criminal law and sentencing\n- Feature 9: ??\n- Feature 10: ??\n\nAs we increase n_components, the features are getting more specific (cf. optometry), but many are getting less comprehensible.","64090811":"### Latent Dirichlect Allocation\n\nBefore digging into the LDA results below, it's worth noting that tuning LDA is as much art as science.  For one thing, choosing the right number of components is highly empirical.  It's generally possible to use some heuristics based on the rate of change in the perplexity of the results (the RPC*); however, the perplexity metric built into sklearn's LDA is broken, and constructing my own isn't a priority for this capstone.  From empirical investigation of the resulting topical clusters over repeated runs, the topics seemed to be distinct and useful at low numbers (5-8), poor at small\/medium numbers (10-20), and fairly well again and medium\/large numbers (30-50).  I also tried tweaking the alpha and beta parameters, but the default values seemed to work best.\n\nLDA is also not especially stable between runs.  I've set the random state for the sake of consistency, but different runs did produce somewhat different groupings and results, although this variance was curtailed by increasing the max_iter parameter.  \n\nThe wordclouds here, as before, scale the words by the weight allotted to each of the 30 top-weighted terms in the LDA vector.  Below these, I'll point out some interesting commonalities and differences between these topical clusters.\n\n\\* https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC4597325\/","8443a9a8":"### Tests with all available data","0aed27ab":"# Clustering\n\nNow we'll move on to performing clustering on the opinions.  This has several goals, including:\n- revealing the innate structure and natural groups of the data\n- yielding alternate semantic clusters\n- finding cluster labels that may be likely to help with classification\n\nFirst we'll need to get a two-feature version of our data for visualization, using the density-based t-SNE method. This requires us to start with a smaller, more manageable set of features.  We'll take a quick look at the explained variance plot of our LSA features to decide how many features we should feed in, then proceed to performing the t-SNE and clustering.","e69170fa":"This isn't very conclusive, unfortunately: there are no clear elbows in inertia or silhouette scores, nor dramtic peaks in ARI. Given this, we'll mostly go off the shape of the data and the number of clear topics that emerged from LDA.  Based on these, something in the range of 18 to 24 would probably work well.","d06acb2e":"### LDA with 30 components","fcb0d0be":"### Examining the components (semantic clusters)\n\nSince we've got these SVD-reduced LSA features, let's take a look at the weightings of the original features, which are lemmas, for each of the top ten combined features.  Below, I've plotted wordclouds with the word sizes determined by those feature weights.","faf1c86a":"There is a notable trend here toward longer opinions in recent years.  Justice Thomas, whose opinions would have been perfectly average before 1990, now stands out as unusually terse among justices in the second half of this chart.  This trend is probably due at least in part to the fact that SCOTUS justices write fewer opinions per year than they used to (again, the shrinking docket).","e73291c0":"### t-SNE and plotting","d4fdcd71":"## Load and Clean\n","7b8cb982":"# Checking clustering on test set\n\nHere we'll return to the test data set aside earlier and see if the methods produce comparable results.","bc2cb51a":"### A note on dropping Justice Douglas\n\nDespite the fact that me meets the minimum opinion count, we're going to drop Justice Douglas from the dataset.  There are three core reasons:\n1. He doesn't really belong to the era this report is examining.  We are examining 1970-2016; Douglas was incapacitated in 1974 and retired in 1975.\n2. The meta-statistics of his opinions are quite anomalous. He wrote far shorter opinions, far more opinions per year, and nearly exclusively in dissent.\n3. The opinions he wrote were themselves highly unusual. \n\nThe last point requires some substantiation, and is in fact a major understatement.  Per Wikipedia: \n\n<blockquote>\"In general, legal scholars have noted that Douglas's judicial style was unusual in that he did not attempt to elaborate justifications for his judicial positions on the basis of text, history, or precedent. Douglas was known for writing short, pithy opinions which relied on philosophical insights, observations about current politics, and literature, as much as more conventional \"judicial\" sources. Douglas wrote many of his opinions in twenty minutes, often publishing the first draft.\"<\/blockquote>\n\nAll of the plots below marked Douglas as a major outlier when included, which is good intuitive confirmation of their usefulness.  Even better, the LSA dataset formulated later in this notebook uniquely identified him as unusual.  His outliership is so extreme that it washes out differences and trends among other justices.  For these reasons, we'll drop Douglas from the dataset moving forward.","58e0b926":"This gives us essentially what we expect: the more distinct authors (at the top) are either very similar to or very unlike each other (very dark or very light), whereas the more typical authors (at the bottom) have a moderate difference from all other authors.  By this metric, it appears that Justice Scalia is the prototypical opinion author.  This makes sense given his tendency to write roughly as much the fashion of his predecessors as in the fashion of his contemporaries.  It is surprising, however, given his reputation for stylistic flair.  (It may be that this reputation is owed to a small but vociferous minority of his opinions.)\n\nFinally, we can also try clustering the authors:","ca546ffc":"### Optimal model","5f792199":"Optimal model with all cluster values:","212244c4":"### Supervised learning conclusions\n\nBy and large, adding cluster labels doesn't actually help at all.  The Spectral clustering labels did help a little bit with the SVM, but they either didn't change or slightly lowered the other model scores.  It's a little bit surprising that none of the more advanced topical analysis seems to have helped at all.\n\nThe most accurate model here, seems to be Random Forest, topping out at around 37%.  For text classification in general, it's surprising that RF would be better than the canonical linear SVM (our second most accurate model here, at 31%).  However, it's easy to see why it might be more effective given the particulars of this data.  The data has a latent hierarchical structure, with authorship falling well below subject\/topic of the case.  The hierarchical structure of Random Forest allows it to account for this hierarchical structure in the data.\n\nThat said, all of these models are massively overfitting.  There may be parameter tweaking that could improve this.  But it also means that our core problem here is probably the limited data.  If we had ten times the data, we could probably predict authorship pretty accurately \u2013 but there's a very finite number of opinions authored by each SCOTUS justice.\n\nOverall, due to the various intrinsic difficulties of the dataset, our prediction rate will probably remain fairly low no matter what we try.","7a35b0f9":"# Supervised learning: author classification","1c7ec5ea":"Overall, while there are some real differences, these clustering methods result in the same general outline of clusters: a central, hard-to-separate crowd of business-related cases (agency, income, commerce, &c), and many outlying clusters of more specific civil and criminal topics.  While these label sets aren't perfect, they do provide a pretty sound taxonomy of coherent topics.\n\nThe topic clusters are also appropriately proximate to each other.  For instance, in K-means with k=24 (probably the best of these clusterings), \"sentence, sentencing\" is right next to \"offense, conviction,\" fairly close to \"juror, peremptory\", and very far away from the natural-resource-focused \"water, land.\"\n\nThese labels can hypothetically contribute to a supervised learning task (which we'll try below), but they are more useful for document classification and for tracking topic prominence against time or other factors (as in the LDA section).","8de170b6":"#### LDA with 10 components","4081d179":"## Author Similarity\nSince we've vectorized every opinion in our dataset, we can now average each author's opinion vectors and measure some differences.  The averages across the reduced featureset are all very similar overall, but there are also some interesting differences.\n\nFirst, let's try sequencing the rows and columns of the correlation heatmap by chronology (specifically, the mean year for each author).","96e8d109":"# Modeling and Analysis\n\nTo do anything with this data, we'll first need to vectorize it.  There are basically two canonical ways to do this:\n1. Lemmatize\/stem words and produce a bag-of-words or tf-idf vector for each document\n2. Get word embeddings and average or otherwise combine the embeddings of all words in each document\n\nFor this case, we'll mostly use 1, which is the most useful for conventional topic modeling techniques.  There are also some technical obstacles for option 2: the corpus is big enough that producing our own word embeddings would take considerable time on this machine, but also small enough and lexically diverse enough that these embeddings may still be rather flawed.  And downloading pre-trained embeddings may be worth a try, but domain specificity is a major concern, given the distinctive nature of legal writing.  Still, if time allows, it would be worthwhile to give some version of option 2 a shot as well.","0ddeb814":"If we want a more detailed interpretation of the clusters, we can also look at the full wordcloud for each cluster (the labels above just use the top two words).","03489290":"All five of these features are specific, coherent, and interpretable:\n- Feature 1: separation of church and state\n- Feature 2: nudity\/obscenity laws\n- Feature 3: employer \/ employee suits\n- Feature 4: police searches and fourth amendment rights\n- Feature 5: criminal sentencing\n\nSome of these features overlap with the LSA features, but features 1 and 2 are almost entirely new","0541f067":"### Plotting topics over time","203e6959":"### Tests with cluster labels","711652a0":"I won't go through all 40 topics here, but note that some of them are totally indecipherable, while others are both coherent and highly specific.  Of particular note:\n- Feature 6: television programming\n- Feature 7: waterlands\n- Feature 18: elections and voting\n- Feature 19: wildlife resources (hunting & fishing)\n- Feature 23: immigration\n- Feature 24: employee abuse\n- Feature 25: obscenity\n\nAt small numbers of topics (e.g., 5), LDA seems to work better here.  As the number of topics increases, the specificity increases but the consistency of coherence falls. Nevertheless, the 40-50% of the clusters that don't display any clear human-conceptual coherence may still represent significant semantic groupings not captured by our English vocabulary and concept-structures, or just not readily apparent.  \n\nIt is also possible as an optimization to filter out the top words selected by the first LSA feature, and then run LDA on the remaining datase, as in the commented-out code snippet below.  In practice, I didn't find that this made any noticeable improvement in topic distinctness or coherence.\n\nIt's worth noting that different runs of LDA with different settings produced results that differed in interesting ways.  It would be far too space-consuming to print out the results of every variant, but I'll mention a few of the differences here.  \n\nFirst, with 10-15 clusters, some runs grouped abortion with medical issues as in the groupings above, but others grouped it with fourth amendment rights and search legality.  That isn't the first connection most humans would think of, but it represents the fact that abortion has been treated in the courts as a right to privacy issue.  Such a grouping shows that the algorithm is drawing semantic clusters from how these topics are actually treated in the text, not just general language-use patterns.\n\nSecond, while almost every run of LDA included a topic cluster for criminal cases or sentencing and some version of finance and bankruptcy cases, the set of topic clusters varied widely.  Native American cases, religion, obscenity, segregation, immigration law, and maritime law all appeared in some runs and not others.  This demonstrates that the hyperparameters of LDA matter a great deal, and that it can be pretty unstable across different runs if max_iter is set fairly low.  But it also represents legitimate conceptual ambiguities.  Ten humans would almost certainly come up with ten different ways for enumerating and grouping the topics encompassed by these six thousand opinions. Is abortion a medical issue or a right to privacy issue, legally speaking?  Should trade be grouped with maritime law, or with immigration?  In a way, it's actually more reaffirming than otherwise that runs of LDA can differ on some of the same issues that humans may reasonably differ on.","65ca08cf":"This shows about what we would expect: majority opinions are usually the longest, since they lay all the groundwork; concurring opinions are shortest, since they build most off the majority opinions; dissenting opinions are in the middle, building somewhat off the majority opinion but having many different or opposing points to make.\n\nNow let's take a look at patterns by individual author.","7251a956":"### Clustering check conclusions:\n\nAll of these results are affirmative. The shape of the data through t-SNE is pretty similar to that of the test set, with some differences since it's a density-based algorithm.  More importantly, the topic clusters are nearly identical.  In short, the rerun shows that the clustering above was stable and consistent across the withheld test data as well.","c7e16a58":"## Setting aside test set\n\nI'll now set aside a quarter of our data for later use (primarily checking the stability of our unsupervised results).  This may be a little more rigor than is really necessary here, but it's one of the few ways to check the reliability of unsupervised methods.","30df5b5f":"## LSA\n\nThese basic tf-idf vectors will serve for some supervised learning processes and (when truncated) clustering.  But for more computationally expensive tasks, we'll need a reduced dataset.  Latent Semantic Analysis uses SVD to reduce the dataset down to a set of combined features that can be thought of as representing semantic clusters: our first piece of topic modeling in this notebook.","bb830276":"This preliminary plot nicely demonstrates the difficulty of the author-classification task: while the data does have some natural clustering, this clustering is definitively NOT around author (as we'll see below, it does seem to be driven by topic of the case).  This also means that including cluster labels in the data, which may allow an algorithm to account for topic, will be especially important for the classification task.","09f00b1e":"### Preliminary tests","8f60eeb1":"Here, with chronology, we see some patterning. As we would expect, the oldest justices are fairly similar to each other (top left corner is shaded dark) and the newer justices are fairly similar to each other (bottom right corner is shaded dark), while the differences are greater across a larger time difference (bottom left corner is lighter).  \n\nOne signficant deviation to this trend is Justice Thomas.  His opinions are actually more similar to justices thirty years prior than to most of his contemporaries - with the exception of Justice Scalia, who displays a similar (though less pronounced) tendency.  \n\nNow let's try sorting by distinctness (the sum of each author's similarity scores).","065b882a":"It looks like somewhere between 50 and 75 \u2013 toward the latter end of the elbow \u2013 would get us the best bang-for-buck.  For now we'll call it 65.","e4aaaa57":"These results show a drastic decrease in the size of the docket (i.e., cases heard per term) since the 1980s.  Caution is warranted here: the present data excludes cases decided per curiam (a small but significant number), a small number of cases featuring only opinions by non-prolific judges, and potential some with irregular formatting.  But external sources (e.g., <a href=https:\/\/www.nytimes.com\/2009\/09\/29\/us\/29bar.html>The New York Times<\/a>) confirm that the decrease in docket size is a real and widely noted phenomenon.  This decrease explains some of the trends visible in the individual justices' opinion statistics below.\n\nWe can also take a look at how long opinions tend to be by category (majority\/dissenting\/concurring):","32c59e51":"# Introduction\n\nThe rulings and opinions of the Supreme Court of the United States (SCOTUS) have been of obvious importance to developments in US law, especially in the past half-century.  While the outcome of the majority vote of the nine justices is the sole determinant of the case at hand, the written majority opinion \u2013 typically a substantial, multi-page document \u2013 determines the scope of and the justification for the precedent that the immediate ruling establishes.  So while the case-deciding power of the court rests with the rulings, the broader precedent-setting power of the court lies in the written opinions.  These precedents can shape the direction of litigation, legislation, and in particular future rulings at all levels of courts.\n\nGiven this context, exploratory analysis of this dataset should be both interesting and useful.  In this notebook, I'll examine about 7,500 opinions written by eighteen different justices between 1970 and 2016.  I'll perform the following types of analysis:\n- high-level statistical analysis of justices and opinion counts\/lengths\/types over the years\n- topic modeling of opinions\n- clustering opinions\n- quantitative and qualitative comparisons between different justices' work\n- correlations with known ideological tendencies\n\nI'll also make an attempt at classifying opinions by author.  However, note that there are numerous difficulties that will make this a very difficult task:\n- topics of opinions vary hugely based on the subject matter of the case\n- the official authoring justice is not the sole author of any opinion: opinions are typically first drafted by clerks, and revised through discussion and collaboration with other justices\n- the problem features a moderately high number of classes (18) with limited examples (minimum ~100)\n\nThat said, most of the interesting insights here will come from the unsupervised exploration of the data.\n\nFinally, two quick notes on adapting this notebook into a Kaggle kernel.  First, this notebook originally included a section on moral and political ideology, but because it used outside data sources (Martin-Quinn scores, Segal-Cover scores, Moral Foundations Theory keywords dictionary), so there's no straightforward way to run it in a Kaggle kernel.  Second, at time of posting, Kaggle's seaborn module is a version behind and doesn't include the scatterplot module used for the clustering plots here; it has to be manually updated per [Kaggle's instructions.](https:\/\/www.kaggle.com\/docs\/kernels#modifying-the-default-environment)","5bd318ac":"Collectively, the number of opinions authored - both total and per-year - shows the downward trend that we expected from the shrinking docket.  There are some notable individual discrepancies.  It is in particular worth noting some positive outliers:\n- **Justices Marshall and Rhenquist** both served (successively) as chief justice for the court.  The chief justice customarily writes the majority opinion if he or she votes on the prevailing side.\n- **Justices Brennan and Stevens** both (successively) held the position of senior associate justice, and both often voted liberal while the Chief Justice at the time often voted conservative. This positioned them almost as associate chief justices \/ minority leaders (not formal titles).\n- **Justice Powell**: the reasons for his outliership are unclear and may be simple a tendency of personality.\n- **Justice Scalia** was a vociferous and eloquent writer known for dissenting.\n\nThe composition of the opinions for each justice, (majority opinions vs. dissents) largely indicates whether or not the majority of the court tended to align with a particular justice's views during their tenure.","adcf6ea6":"It looks like the clustering tends to pick up mostly on era, with some weight toward ideology as well: older and\/or more conservative justices are grouped in cluster 0, while more recent and more progressive justices are grouped in label 1.","2f4fb049":"It's common with LSA for the first feature to serve as a filter for the most common and indiscriminate words across the board*, and we see some of that in Feature 0 here: a constitutional amendment, a counsel (attorney), a sentence, and an appeal are common to very many of these cases. Certainly these words don't clearly identify a distinct and coherent concept.\n\nSome of the other features, though, have a more clearly articulable focus:\n- Feature 1: [mixture\/filter]\n- Feature 2: criminal sentencing\n- Feature 3: police conduct and the legality of searches\n- Feature 4: religion in schools\n- Feature 5: employer-employee arbitration\n- Feature 6: Native American rights\n- Feature 7: [mixed - ?]\n- Feature 8: Native American cases\n- Feature 9: elections and discrimination\n- Feature 10: [mixed - ?]\n\nOverall, about three of the top ten features here are entertwined and not robustly distinct.  The blends don't fit any single human-language concept or category.  This doesn't preclude the usefulness of our LSA-reduced dataset for ongoing modeling.  For insightful topic modeling, however, it would also make sense to give Latent Dirichlet Analysis a try.  One of its chief advantages is the ability to create more distinct or coherent topic clusters than LSA.\n\n\\*  *\"Following the recom-mendation of Hu, Cai, Wiemer-Hastings, Graesser, & McNamara (2007), we discard the first dimension prior to computing similarity because this dimension is always positive and correlates with the under-lying terms\u2019 frequency in the corpus.\"  -- (PDF) Measuring Moral Rhetoric in Text. Available from: https:\/\/www.researchgate.net\/publication\/258698999_Measuring_Moral_Rhetoric_in_Text [accessed Oct 17 2018].*","73f48455":"This is a very promising result: these semantic clusters seem to be even more consistently distinct and interpretable than those produced by LDA.  Nearly every one matches an easily named natural topic.  ","9f0bf1b4":"### K-means\n\nTo choose the number of clusters for our K-means, I'll plot the entropy, silhouette score, and adjusted rand index (ARI, a measure of consistency with different starting centroids) across a range of n-values.  We're looking for inflection points in the entropy and silhouette score, plus a reasonably high adjusted rand index.","2aaf8aca":"#### LDA with 5 components","c2264ac9":"### Parsing and TF_IDF\n\nFirst we'll need to clean and lemmatize the opinions. This parsing will give us a version including named entities (people, organizations, etc.) and a version without them. \n\nMoving forward I'll work almost entirely with the version of the opinions with the named entities removed.  The main reason is that some of the topic analysis methods, especially LSA, end up catching a lot of names if they are included.  This leads to mostly uninterpretable topics clusters of people and organizations.  Names are also just not what we're interested in here: they tend to be restricted to the opinions belonging to just one case (sometimes two or three), and while they're frequent within that case, they're absent and useless for the rest of the dataset.","de0ce379":"# Conclusions\nIn this examination of SCOTUS opinions, we've generated insight into author tendencies and differences between justices, the structure of opinion data, and latent semantic clusters in the opinions.  The topical clusters produced by clustering seem to be the most coherent (even more than the topics from LDA), probably because the topic\/subject is the most prominent or determinative latent aspect of any given opinion.  The supervised task of predicting authorship remains extremely difficult, however, due to the diverse topics covered, the formulaic nature of the prose, and the collaborative methods by which the texts were generated.\n\nIn future work, topical clusters like these could easily be used for opinion classification.  Tracking political leanings within topical clusters could also prove more insightful than trying to do so across all topics.\n\n### Further work\nThere remains much to be done with this data that could not fit into this capstone project.  In particular:\n- A **more rigorous Moral Foundations Theory analysis** and certain types of **bias evaluation** using pre-trained word embeddings could lend useful insight.\n- **Sentiment analysis** using pre-trained word embeddings may yield interesting results, especially over the course of single opinions from paragraph to paragraph\n- **Textual entailment** could be used to attempt a mapping of argumentative structures in the opinions\n- **Mapping centers of tension**: we could use custom-generated word embeddings (with word2vec, or an inverted tf-idf + LSA) to measure which words are used most differently by different authors \u2013 especially justices of different political leanings.  This could identify ideological and linguistic centers of tension, which we could map over time.\n\nOverall, this project revealed a lot of information about the opinions themselves, but pointed even more toward interesting future work that could be built on this foundation.","b0f8aa76":"## Imports","f51d3d7e":"### LSA"}}