{"cell_type":{"a9037d22":"code","cf2f28cd":"code","7a17614e":"code","eb766d8c":"code","9ce7b04b":"code","9d27e52b":"code","0532828d":"code","72b57e96":"code","b36006d2":"code","0d31cba3":"code","f125b453":"code","b5467442":"code","ae1beebf":"code","ffd8d74b":"code","533b340c":"code","18d1d173":"code","b220820a":"code","99dd2101":"code","c4dffa1f":"code","864bd9c4":"code","9c1d1833":"code","ee8fe7cb":"code","766a63bc":"code","216b6085":"code","6991e536":"code","800e9f15":"code","d4d014ef":"code","663a0110":"code","0ed5f466":"markdown","a89399d5":"markdown","f542e035":"markdown","33e503de":"markdown","720b89e9":"markdown","ee5c90ab":"markdown","b82f3ec0":"markdown","9345d48a":"markdown","794d96f6":"markdown","4bcafd1b":"markdown","a75ff192":"markdown","0c508073":"markdown","05512097":"markdown","6341d263":"markdown","b9c4f6bf":"markdown","24c758f2":"markdown","7a91f63f":"markdown","487c2c5c":"markdown"},"source":{"a9037d22":"!pip install -q pydub","cf2f28cd":"import os\nimport gc\nimport cv2\nimport time\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nfrom joblib import Parallel, delayed\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport pydub\nimport librosa\nimport librosa.display\nfrom pydub import AudioSegment as AS\nfrom librosa.feature import melspectrogram\nfrom librosa.core import power_to_db as ptdb\n\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.sequence import pad_sequences as pad","7a17614e":"N = 8\nSR = 44100\nCHUNKS = 1\nTSR = 32000\nN_MELS = 128\nPOP_FRAC = 0.25\nMAXLEN = 1000000\nAMPLITUDE = 1000\nCHUNK_SIZE = 500000","eb766d8c":"os.listdir('..\/input')","9ce7b04b":"TEST_DATA_PATH = '..\/input\/birdsong-recognition\/test.csv'\nTRAIN_DATA_PATH = '..\/input\/birdsong-recognition\/train.csv'\nTEST_AUDIO_PATH = '..\/input\/birdsong-recognition\/test_audio\/'\nTRAIN_AUDIO_PATH = '..\/input\/birdsong-recognition\/train_audio\/'\nCHECKING_PATH = '..\/input\/prepare-check-dataset\/birdcall-check\/'","9d27e52b":"sub = os.path.exists(TEST_AUDIO_PATH)\nTEST_DATA_PATH = TEST_DATA_PATH if sub else CHECKING_PATH + 'test.csv'\nTEST_AUDIO_PATH = TEST_AUDIO_PATH if sub else CHECKING_PATH + 'test_audio\/'","0532828d":"test_df = pd.read_csv(TEST_DATA_PATH)\ntrain_df = pd.read_csv(TRAIN_DATA_PATH)","72b57e96":"test_df.head()","b36006d2":"train_df.head()","0d31cba3":"keys = set(train_df.ebird_code)\nvalues = np.arange(0, len(keys))\ncode_dict = dict(zip(sorted(keys), values))","f125b453":"def normalize(x):\n    return np.float32(x)\/2**15\n\ndef read(file, norm=False):\n    try:\n        a = AS.from_mp3(file)\n        a = a.set_frame_rate(TSR)\n    except:\n        return TSR, np.zeros(MAXLEN)\n\n    y = np.array(a.get_array_of_samples())\n    if a.channels == 2: y = y.reshape((-1, 2))\n    if norm: return a.frame_rate, normalize(y)\n    if not norm: return a.frame_rate, np.float32(y)\n\ndef write(file, sr, x, normalized=False):\n    birds_audio_bitrate, file_format = '320k', 'mp3'\n    ch = 2 if (x.ndim == 2 and x.shape[1] == 2) else 1\n    y = np.int16(x * 2 ** 15) if normalized else np.int16(x)\n    song = AS(y.tobytes(), frame_rate=sr, sample_width=2, channels=ch)\n    song.export(file, format=file_format, bitrate=birds_audio_bitrate)","b5467442":"def get_idx(length):\n    length = get_len(length)\n    max_idx = MAXLEN - CHUNK_SIZE\n    idx = np.random.randint(length + 1)\n    chunk_range = idx, idx + CHUNK_SIZE\n    chunk_idx = max([0, chunk_range[0]])\n    chunk_idx = min([chunk_range[1], max_idx])\n    return (chunk_idx, chunk_idx + CHUNK_SIZE)\n\ndef get_len(length):\n    if length > MAXLEN: return MAXLEN\n    if length <= MAXLEN: return int(length*POP_FRAC)","ae1beebf":"def get_chunk(data, length):\n    index = get_idx(length)\n    return data[index[0]:index[1]]\n\ndef get_signal(data):\n    length = max(data.shape)\n    data = data.T.flatten().reshape(1, -1)\n    data = np.float32(pad(data, maxlen=MAXLEN).reshape(-1))\n    return [get_chunk(data, length) for _ in range(CHUNKS)]","ffd8d74b":"def to_imagenet(X, mean=None, std=None, norm_max=None, norm_min=None, eps=1e-6):\n    mean = mean or X.mean()\n    X = X - mean\n    std = std or X.std()\n    Xstd = X \/ (std + eps)\n    _min, _max = Xstd.min(), Xstd.max()\n    norm_max = norm_max or _max\n    norm_min = norm_min or _min\n    if (_max - _min) > eps:\n        # Normalize to [0, 255]\n        V = Xstd\n        V[V < norm_min] = norm_min\n        V[V > norm_max] = norm_max\n        V = 255*((V - norm_min) \/ (norm_max - norm_min))\n    else:\n        # Just zero\n        V = np.zeros_like(Xstd, dtype=np.uint8)\n    return np.stack([V]*3, axis=-1)","533b340c":"def get_melsp(data):\n    melsp = melspectrogram(data, n_mels=N_MELS)\n    return to_imagenet(librosa.power_to_db(melsp))\n\ndef get_melsp_img(data):\n    data = get_signal(data)\n    return np.stack([get_melsp(point) for point in data])","18d1d173":"def save(indices, path):\n    folder = TRAIN_AUDIO_PATH\n\n    for index in tqdm(indices):\n        file_name = train_df.filename[index]\n        ebird_code = train_df.ebird_code[index]\n\n        default_signal = np.random.random(MAXLEN)*AMPLITUDE\n        default_values = SR, np.int32(np.round(default_signal))\n\n        values = read(folder + ebird_code + '\/' + file_name)\n        _, data = values if len(values) == 2 else default_values\n        \n        image = np.nan_to_num(get_melsp_img(data))[0]\n        cv2.imwrite(path + file_name + '.jpg', image); del image; gc.collect()","b220820a":"train_ids = np.array_split(np.arange(len(train_df)), 5)\ntrain_ids_1, train_ids_2, train_ids_3, train_ids_4, train_ids_5 = train_ids","99dd2101":"train_ids_1 = np.array_split(np.array(train_ids_1), N)\ntrain_ids_2 = np.array_split(np.array(train_ids_2), N)\ntrain_ids_3 = np.array_split(np.array(train_ids_3), N)\ntrain_ids_4 = np.array_split(np.array(train_ids_4), N)\ntrain_ids_5 = np.array_split(np.array(train_ids_5), N)","c4dffa1f":"!mkdir train_1\npath = \"train_1\/\"\nparallel = Parallel(n_jobs=N, backend=\"threading\")\nparallel(delayed(save)(ids, path) for ids in train_ids_1)","864bd9c4":"!zip -r train_1.zip train_1\n!rm -rf train_1","9c1d1833":"!mkdir train_2\npath = \"train_2\/\"\nparallel = Parallel(n_jobs=N, backend=\"threading\")\nparallel(delayed(save)(ids, path) for ids in train_ids_2)","ee8fe7cb":"!zip -r train_2.zip train_2\n!rm -rf train_2","766a63bc":"!mkdir train_3\npath = \"train_3\/\"\nparallel = Parallel(n_jobs=N, backend=\"threading\")\nparallel(delayed(save)(ids, path) for ids in train_ids_3)","216b6085":"!zip -r train_3.zip train_3\n!rm -rf train_3","6991e536":"!mkdir train_4\npath = \"train_4\/\"\nparallel = Parallel(n_jobs=N, backend=\"threading\")\nparallel(delayed(save)(ids, path) for ids in train_ids_4)","800e9f15":"!zip -r train_4.zip train_4\n!rm -rf train_4","d4d014ef":"!mkdir train_5\npath = \"train_5\/\"\nparallel = Parallel(n_jobs=N, backend=\"threading\")\nparallel(delayed(save)(ids, path) for ids in train_ids_5)","663a0110":"!zip -r train_5.zip train_5\n!rm -rf train_5","0ed5f466":"### Define paths\n\nThe paths used are documented below ~\n\n1. Metadata related\n\n    * <code>..\/input\/birdsong-recognition\/test.csv<\/code> contains the test metadata used for submission.\n    * <code>..\/input\/birdsong-recognition\/train.csv<\/code> contains the train metadata used for submission.\n    * <code>..\/input\/prepare-check-dataset\/test.csv<\/code> contains the test metadata used for committing.\n\n\n2. Audio data related\n\n    * <code>..\/input\/birdsong-recognition\/test_audio<\/code> contains the test audio used for submission.\n    * <code>..\/input\/birdsong-recognition\/train_audio<\/code> contains the train audio used for submission.\n    * <code>..\/input\/prepare-check-dataset\/test_audio<\/code> contains the test audio used for committing.","a89399d5":"### Check data available\n\nWe have two datasets at our disposal: <code>birdsong-recognition<\/code> and <code>prepare-check-dataset<\/code>.","f542e035":"# Acknowledgements\n\n1. [LibROSA](https:\/\/librosa.org\/librosa\/) ~ by the librosa team\n2. [Audio Data Analysis Using librosa \ud83d\udcc8](https:\/\/www.kaggle.com\/hamditarek\/audio-data-analysis-using-librosa) ~ by Tarek Hamdi\n3. [Understanding the Mel Spectrogram](https:\/\/medium.com\/analytics-vidhya\/understanding-the-mel-spectrogram-fca2afa2ce53) ~ by Leland Roberts\n4. [Bidirectional LSTM for audio labeling with Keras](https:\/\/www.kaggle.com\/carlolepelaars\/bidirectional-lstm-for-audio-labeling-with-keras) ~ by Carlo Lepelaars","33e503de":"### Define functions to calculate melspectrogram features\n\nBelow we define some functions to calculate the <code>melspectrogram<\/code> features from audio signals.","720b89e9":"## Load metadata from .csv files <a id=\"1.4\"><\/a>\n\n* Now we load the training and testing metadata.\n\n* We can see that the testing dataframe has only <code>3<\/code> rows in it.\n\n* This is only a dummy test dataframe. The actual testing data will be used during submission.","ee5c90ab":"## Define key hyperparameters and paths <a id=\"1.3\"><\/a>\n\n* Here we define the key hyperparameters: sequence length, train\/val split, batch size, epochs, LR.\n* We also specify the correct paths for loading data, training, inference, and finally submission to this competition.","b82f3ec0":"# Introduction\n\nWelcome to the \"Cornell Birdcall Identification\" challenge on Kaggle! In this challenge, contestants need to identify the species of birds involved in audio clips of them calling. In this kernel, I will generate <code>melspectrograms<\/code> from all the training audio clips and save them as images, so that training can be speeded up!","9345d48a":"### Define hyperparameters\n\nThe hyperparameters used are documented below ~\n\n1. Data processing related\n\n    * <code>N_MELS<\/code> is the number of <code>melspectrogram<\/code> features per time step.\n    * <code>AMPLITUDE<\/code> represents the default signal amplitude applied to unreadable files.\n    * <code>SR<\/code> is the sampling rate at which the audio is loaded (readings per second). It defaults to <code>44100 Hz<\/code>.\n    * <code>TSR<\/code> is the sampling rate at which the test audio clips are loaded. It defaults to <code>32000 Hz<\/code>.\n    * <code>MAXLEN<\/code> is the maximum number of readings. Longer clips will be trimmed and shorter ones will be padded.\n    * <code>SPLIT<\/code> represents the fraction of data to be used for training. The rest of the data is used for validation.\n    * <code>CHUNKS<\/code> represents the number of chunks that will be extracted from each signal. It defaults to <code>1<\/code> per signal.\n    * <code>CHUNK_SIZE<\/code> represents the sequence length of each audio chunk to be fed into the <code>melspectrogram<\/code> function.\n    * <code>POP_FRAC<\/code> is the maximum proportion of signal information to be ignored per chunk (defaults to <code>0.25<\/code>).","794d96f6":"### Define utility function to read audio\n\n* Now we define a function using <code>pydub<\/code> to read audio files into <code>numpy<\/code> arrays.\n* This implementation is significantly faster than <code>librosa.load<\/code> and <code>torchaudio.load<\/code>.","4bcafd1b":"<center><img src=\"https:\/\/i.imgur.com\/bIw8deA.jpg\" width=\"1000px\"><\/center>","a75ff192":"## Install additional packages <a id=\"1.1\"><\/a>\n\n* We will now install <code>pydub<\/code>\n* <code>pydub<\/code> will help us load audio data from <code>.mp3<\/code> files much faster than the <code>librosa<\/code> command: <code>librosa.load<\/code>","0c508073":"### Define functions to process audio signals\n\nThese are a set of functions which process the audio before the <code>melspectrogram<\/code> transformation.\n\nThe functions used are documented below ~\n\n* <code>get_idx<\/code> selects the start and end index of a given audio chunk.\n* <code>get_chunk<\/code> takes indices from <code>get_idx<\/code> and outputs a chunk of data between those indices.\n* <code>get_len<\/code> is a helper function which is used to decide possible chunk indices based on <code>POP_FRAC<\/code>.\n  \n  --> <code>If<\/code> the signal is longer than <code>MAXLEN<\/code>, it sets the maximum index to <code>MAXLEN<\/code>.<br>\n  --> <code>Else<\/code> it uses <code>POP_FRAC<\/code> to ensure chunks are centered around audio signal and not padding.\n\n\n* <code>get_signal<\/code> flattens the signal, pads it to <code>MAXLEN<\/code>, and stacks multiple chunks into one array.","05512097":"### Define spectrogram loading function\n\nNow we define a function that generates a <code>spectrogram<\/code> at a list of indices.","6341d263":"### Prepare the label dictionary\n\n* Next we prepare a dictionary linking each bird species to a unique integer.\n* This dictionary will help us when we need to one-hot encode our targets later.","b9c4f6bf":"### Load all training spectrograms and save with parallel processing\n\nNext we will use multi-threading to generate all the spectorgrams and save them quickly.","24c758f2":"# Preparing the ground <a id=\"1\"><\/a>\n\nIn this section, we will prepare the ground to train and test the model by installing packages, setting hyperparameters, and loading the data.","7a91f63f":"## Data processing <a id=\"3.1\"><\/a>\n\nThe first step to define important function to process the data and generate features.","487c2c5c":"## Import necessary libraries <a id=\"1.2\"><\/a>\n\n* Now, we import all the libraries we need.\n* <code>matplotlib<\/code> and <code>tqdm<\/code> for data analysis and visualization.\n* <code>librosa<\/code>, <code>pydub<\/code> and <code>keras<\/code> for model training and inference.\n* <code>numpy<\/code>, <code>pandas<\/code>, and <code>sklearn<\/code> for data processing and manipulation."}}