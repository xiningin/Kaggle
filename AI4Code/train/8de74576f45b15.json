{"cell_type":{"8fb03a5f":"code","e998d078":"code","4023ecad":"code","da021d25":"code","839c3ecd":"code","169aaa13":"code","50a4779e":"code","e92c4030":"code","a707e28d":"code","ec4d153b":"code","ae579e6b":"code","3f7bb98c":"code","bac0c859":"code","3d37ba90":"code","cce49dad":"code","70b155d5":"code","8b80d634":"code","5f107548":"code","5fbea691":"code","a5e64a43":"code","a93bd972":"code","2abafe8b":"code","cc5e0e45":"code","45617884":"code","6aaa1006":"code","e8648e4e":"code","fe13f0c8":"code","78be7623":"code","cdbbd5c0":"code","f6fab9cf":"code","5fc8b10d":"code","89405829":"code","6c7aea8a":"code","68ec988a":"code","375893a6":"code","f71bfa3c":"code","9a58e071":"code","66d9e15a":"code","2f2ccaf7":"code","74181af8":"code","7755373a":"code","6411a943":"code","ba618a96":"code","6549f94c":"code","06c8e0df":"code","d9c5e71e":"code","2784b9d4":"code","ac7eec8b":"code","9187cea1":"code","6b008acf":"code","feb9ed41":"code","84eb3c4a":"code","5f3ceb2f":"code","d367e5ab":"code","e0e81639":"code","ac7a83a7":"code","080b69e2":"code","34a83f4f":"code","ee67e2d3":"code","a2de1824":"code","164f7689":"code","ac29d03c":"code","759f54be":"code","22fd7d74":"code","a28672a8":"code","67249e96":"code","5c00eccb":"code","d7e93f9b":"code","69c3475e":"code","1a872055":"code","d03aecda":"code","d3dfcf58":"code","4d645d23":"code","78a0b22e":"code","323b6f72":"code","8523ae4d":"code","15f937c1":"code","25b894dd":"code","cf414fe6":"code","cdbf5f98":"code","e0901aa8":"code","233ffc1d":"code","76259c4d":"code","31d12be3":"code","efff1bcc":"code","e4810810":"code","7546511f":"code","eba62efb":"code","3743214a":"code","42bb1f99":"code","8013b0c5":"code","d83d94da":"code","2cd92229":"code","ff3a73fd":"code","db7f3a80":"code","2441e552":"code","2f77f205":"code","cea30e33":"code","b03fab80":"code","fffaa8a5":"code","3a288134":"code","36d3090f":"code","d2864a80":"code","f5dc1fad":"code","6fc6eef8":"code","c39d3f10":"code","fde96b26":"code","f5af8ba4":"code","dd3f1154":"code","c0e95a0b":"code","7577e3dc":"code","7655e5b5":"code","0179cd88":"code","401676dc":"code","33ea21ed":"code","740fa9de":"code","18aff509":"code","09cc5f54":"code","c53b9762":"code","d39cfa7a":"markdown","6ce6702d":"markdown","d86ad4bb":"markdown","30c1a08a":"markdown","231abc16":"markdown","15c074a7":"markdown","896a7295":"markdown","2f6be2ae":"markdown","2b157959":"markdown","563057d4":"markdown","d95995ae":"markdown","9e152326":"markdown","2f522186":"markdown","f9ee05ff":"markdown","5249aeb1":"markdown","ec2087b5":"markdown","cedc0d1f":"markdown","406d0e51":"markdown","9cd4a8a9":"markdown","44185e43":"markdown","886c98a5":"markdown","29283a69":"markdown","4934f5eb":"markdown","92696cd5":"markdown","39699d47":"markdown","4d724a32":"markdown","3f732484":"markdown","52d956ee":"markdown","8d37f606":"markdown","355dbae4":"markdown","c09fafb5":"markdown","2719958c":"markdown","6114c1cb":"markdown","b55db1d8":"markdown","ed14ff4a":"markdown","31e58aac":"markdown","63821f55":"markdown","61b74836":"markdown","4ee9adb8":"markdown","6759f800":"markdown","26be4585":"markdown","6ae74983":"markdown","3ce07787":"markdown","578728b6":"markdown","46d22026":"markdown","72c99948":"markdown","7e513462":"markdown","c62ef91e":"markdown","2c88944e":"markdown","ec4a57aa":"markdown","f3758afb":"markdown","c2049984":"markdown","cb95716e":"markdown","ccca6569":"markdown","7329d738":"markdown","50d4e228":"markdown","9b56f429":"markdown","582f4c96":"markdown","e5b016cd":"markdown","474c1e3c":"markdown","2b50948a":"markdown","479a5a1e":"markdown","6bbd7422":"markdown","80c415d2":"markdown","d2b0e4c4":"markdown","6075e5e3":"markdown","cfa5b8ad":"markdown","63a2b597":"markdown","d8865461":"markdown"},"source":{"8fb03a5f":"# Dependencies and setup\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.gridspec as gridspec\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedShuffleSplit, RandomizedSearchCV, cross_val_score, cross_validate\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.tree import DecisionTreeClassifier\nimport statsmodels.api as sm\nfrom sklearn.tree import plot_tree\nimport os\n%matplotlib inline","e998d078":"# Set maximum rows to a high number\npd.set_option('display.max_rows', 100)","4023ecad":"# Load datasets\ntraining_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv',index_col=0)\ntesting_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv',index_col=False)","da021d25":"# Make copies of the original DataFrames in order to prevent errors\ntraining_data_2 = training_data.copy()\ntesting_data_2 = testing_data.copy()","839c3ecd":"# Investigate shape of the training dataset\ntraining_data_2.shape","169aaa13":"# Investigate shape of the testing dataset\ntesting_data_2.shape","50a4779e":"# Investigate missing values in the training dataset\ntotal_missing_training = training_data_2.isnull().sum().sort_values (ascending = False)\npercent_missing_training = round((training_data_2.isnull().sum().sort_values(ascending = False)\/len(training_data_2))*100,2)\npd.concat([total_missing_training, percent_missing_training], axis = 1, keys = ['Total','Percent'])","e92c4030":"# Investigate missing values in the testing dataset\ntotal_missing_testing = testing_data_2.isnull().sum().sort_values (ascending = False)\npercent_missing_testing = round((testing_data_2.isnull().sum().sort_values(ascending = False)\/len(testing_data_2))*100,2)\npd.concat([total_missing_testing, percent_missing_testing], axis = 1, keys = ['Total','Percent'])","a707e28d":"# Plot a histogram of fares in the training dataset\ntesting_data_2.Fare.hist(bins=35, density = True, stacked = True, alpha = 0.6, color='royalblue')\ntesting_data_2.Fare.plot(kind = 'density', color = 'royalblue')\nplt.xlabel('Fare ($)')\nplt.xlim(-10,200)\nplt.title('Distribution of Fare')\nplt.show()","ec4d153b":"# Look at the line item with the missing Fare value\ntesting_data_2[testing_data_2['Fare'].isnull()]","ae579e6b":"# Fill in missing Fare values with the median Fare paid by customers of the same passenger class, sex, and port of embarkation\ntesting_data_2['Fare'] = testing_data_2.groupby(['Pclass','Sex','Embarked'])['Fare'].transform(lambda x: x.fillna(x.median()))","3f7bb98c":"# Look into what the 'Embarked' null rows look like\ntraining_data_2[training_data_2['Embarked'].isnull()]","bac0c859":"# Check if there are any other passengers with the same ticket number in the training dataset\ntraining_data_2[training_data_2['Ticket']=='113572']","3d37ba90":"# Check if there are any other passengers with the same ticket number in the testing dataset\ntesting_data_2[testing_data_2['Ticket']=='113572']","cce49dad":"# Check if there are any other passengers with the same cabin in the training dataset\ntraining_data_2[training_data_2['Cabin']=='B28']","70b155d5":"# Check if there are any other passengers with the same cabin in the testing dataset\ntesting_data_2[testing_data_2['Cabin']=='B28']","8b80d634":"# Look at the percentiles of fare prices paid for all first class women\ntraining_data_2[(training_data_2['Pclass'] == 1) & (training_data_2['Sex'] == 'female')].groupby('Embarked')['Fare'].describe()","5f107548":"# Look into the most common port of embarkation for people on cabin level B\ntraining_data_2[training_data_2['Cabin'].str[0]=='B']['Embarked'].value_counts()","5fbea691":"# Look at the most common ticket classes for individuals on cabin level B\ntraining_data_2[training_data_2['Cabin'].str[0]=='B']['Pclass'].value_counts()","a5e64a43":"# Look at the distribution of sexes of passengers inhabiting cabin level B \ntraining_data_2[training_data_2['Cabin'].str[0]=='B']['Sex'].value_counts()","a93bd972":"# Look at the percentiles of fare prices paid by women inhabiting cabin level B\ntraining_data_2[(training_data_2['Cabin'].str[0]=='B') & (training_data_2['Sex'] == 'female')].groupby('Embarked')['Fare'].describe()","2abafe8b":"# Replace the missing 'Embarked' datapoints with 'C' for 'Cherbourg'\ntraining_data_2.update(training_data_2['Embarked'].fillna('C'))","cc5e0e45":"# Plot a histogram of ages in the training dataset\ntraining_data_2.Age.hist(bins=15, density = True, stacked = True, alpha = 0.6, color='royalblue')\ntraining_data_2.Age.plot(kind = 'density', color = 'royalblue')\nplt.xlabel('Age')\nplt.xlim(-10,85)\nplt.show()","45617884":"# CREATE A FIGURE SHOWING THE COUNT OF PASSENGERS WITH AGE VALUES POPULATED BY CLASS, SURVIVORSHIP, AND SEX\n# Set up the figure with two subplots\nfig, (axis1,axis2,axis3) = plt.subplots(1, 3, figsize=(14,6))\n# Create a plot showing the count of observations with upper, middle, and lower classes\nsns.countplot(x = 'Pclass', data = training_data_2[training_data_2.Age.notnull()], palette = 'Blues_r', ax = axis1)\naxis1.set_xticklabels(['Upper','Middle','Lower'])\naxis1.set_xlabel('Ticket Class')\n# Create a plot showing the count of passengers who survived and did not survive\nsns.countplot(x = 'Survived', data = training_data_2[training_data_2.Age.notnull()], palette = 'Blues_r', ax = axis2)\naxis2.set_xticklabels(['No','Yes'])\naxis2.set_xlabel('Survived?')\n# Create a plot showing the count of observations by sex\nsns.countplot(x = 'Sex', data = training_data_2[training_data_2.Age.notnull()], palette = 'Blues_r', ax = axis3)\naxis3.set_xticklabels(['Male','Female'])\naxis3.set_xlabel('Sex')\n# Add title and show the graph\nplt.text(-3, 495, 'Count of Passengers With Ages', fontsize = 20)\nplt.show()","6aaa1006":"# CREATE A FIGURE SHOWING THE COUNT OF PASSENGERS WITH AGE VALUES EQUAL TO NULL BY CLASS, SURVIVORSHIP, AND SEX\n# Set up the figure with two subplots\nfig, (axis1,axis2,axis3) = plt.subplots(1, 3, figsize=(14,6))\n# Create a plot showing the count of observations with upper, middle, and lower classes\nsns.countplot(x = 'Pclass', data = training_data_2[training_data_2.Age.isnull()], palette = 'Oranges_r', ax = axis1)\naxis1.set_xticklabels(['Upper','Middle','Lower'])\naxis1.set_xlabel('Ticket Class')\n# Create a plot showing the count of passengers who survived and did not survive\nsns.countplot(x = 'Survived', data = training_data_2[training_data_2.Age.isnull()], palette = 'Oranges_r', ax = axis2)\naxis2.set_xticklabels(['No','Yes'])\naxis2.set_xlabel('Survived?')\n# Create a plot showing the count of observations by sex\nsns.countplot(x = 'Sex', data = training_data_2[training_data_2.Age.isnull()], palette = 'Oranges_r', ax = axis3)\naxis3.set_xticklabels(['Male','Female'])\naxis3.set_xlabel('Sex')\n# Add title and show the graph\nplt.text(-3, 140, 'Count of Passengers Without Ages', fontsize = 20)\nplt.show()","e8648e4e":"# Replace missing variables with the median of age for individuals grouped by ticket class, sex, and embarked location\ntraining_data_2['Age'] = training_data_2.groupby(['Pclass','Sex','Embarked'])['Age'].transform(lambda x: x.fillna(x.median()))\ntesting_data_2['Age'] = testing_data_2.groupby(['Pclass','Sex','Embarked'])['Age'].transform(lambda x: x.fillna(x.median()))","fe13f0c8":"# Plot histogram of ages before nulls were replaced and show this histogram in blue\ntraining_data.Age.hist(bins=15, density = True, stacked = True, alpha = 0.6, color='royalblue')\ntraining_data.Age.plot(kind = 'density', color = 'royalblue', label = 'before')\n# Plot histogram of ages after null values were replaced and show the histogram in orange\ntraining_data_2.Age.hist(bins=15, density = True, stacked = True, alpha = 0.5, color='orange')\ntraining_data_2.Age.plot(kind = 'density', color = 'orange', label = 'after')\n# Create legend and labels\nplt.legend()\nplt.xlabel('Age')\nplt.xlim(-10,85)\nplt.show()","78be7623":"# Set up the figure with two subplots\nfig, (axis1,axis2) = plt.subplots(1, 2, figsize=(14,6))\n# Create a plot showing the count of observations with upper, middle, and lower classes\nsns.countplot(x = 'Pclass', data = training_data_2, palette = 'Blues_r', ax = axis1)\naxis1.set_xticklabels(['Upper','Middle','Lower'])\naxis1.set_xlabel('Ticket Class')\n# Create a plot showing the proportion of people in each class who survived\nsns.barplot('Pclass', 'Survived', data = training_data_2, palette = 'Oranges_r', ax = axis2)\naxis2.set_xticklabels(['Upper','Middle','Lower'])\naxis2.set_xlabel('Ticket Class')\nplt.show()","cdbbd5c0":"# Set up the figure with two subplots\nfig, (axis1,axis2) = plt.subplots(1, 2, figsize=(14,6))\n# Create a plot showing the count of observations with upper, middle, and lower classes\nsns.countplot(x = 'Sex', data = training_data_2, palette = 'Blues_r', ax = axis1)\naxis1.set_xticklabels(['Male','Female'])\naxis1.set_xlabel('Sex')\n# Create a plot showing the proportion of people in each class who survive\nsns.barplot('Sex', 'Survived', data = training_data_2, palette = 'Oranges_r', ax = axis2)\naxis2.set_xticklabels(['Male','Female'])\naxis2.set_xlabel('Sex')\nplt.show()","f6fab9cf":"# Set up figure with two subplots\nplt.figure(figsize=(15,8))\n# Create a kernel density estimation plot showing the ages of passengers who survive the shipwreck and color the plot blue\nax1 = sns.kdeplot(training_data_2['Age'][training_data_2.Survived == 1], color = 'royalblue', shade=True)\n# Create a kernel density estimation plot showing the ages of passengers who did not survive the shipwreck and color the plot orange\nax2 = sns.kdeplot(training_data_2['Age'][training_data_2.Survived == 0], color = 'orange', shade=True)\n# Add titles and legend\nplt.legend(['Survived', 'Died'])\nplt.title('Density Plot of Age for Surviving Population and Deceased Population (Including Replaced Age Values)')\nax2.set(xlabel = 'Age')\nplt.show()","5fc8b10d":"# Set up figure with two subplots\nplt.figure(figsize=(15,8))\n# Create a kernel density estimation plot showing the ages of passengers who survive the shipwreck and color the plot blue\nax1 = sns.kdeplot(training_data['Age'][(training_data.Age.notnull()) & (training_data.Survived == 1)], color = 'royalblue', shade=True)\n# Create a kernel density estimation plot showing the ages of passengers who did not survive the shipwreck and color the plot orange\nax2 = sns.kdeplot(training_data['Age'][(training_data.Age.notnull()) & (training_data.Survived == 0)], color = 'orange', shade=True)\n# Add titles and legend\nplt.legend(['Survived', 'Died'])\nplt.title('Density Plot of Age for Surviving Population and Deceased Population (Only Including Passengers Whose Ages Originally Specified in the Data)')\nax2.set(xlabel = 'Age')\nplt.xlim(-10,85)\nplt.show()","89405829":"# Look at average survival and count of passengers by age\ntraining_data_2[['Age', 'Survived']].groupby(['Age'], as_index = False).agg(['mean', 'count'])","6c7aea8a":"# Set up the figure with two subplots\nfig, (axis1,axis2) = plt.subplots(1, 2, figsize=(14,6))\n# Create a plot showing the count of observations with different numbers of siblings \/ spouses on board the titanic\nsns.countplot(x = 'SibSp', data = training_data_2, palette = 'Blues_r', ax = axis1)\naxis1.set_xlabel('Number of Siblings \/ Spouses On Board Titanic')\n# Create a plot showing the proportion of with different numbers of siblings \/ spouses onboard the titanic who survive\nsns.barplot('SibSp', 'Survived', data = training_data_2, palette = 'Oranges_r', ax = axis2)\naxis2.set_xlabel('Number of Siblings \/ Spouses On Board Titanic')\nplt.show()","68ec988a":"# Set up the figure with two subplots\nfig, (axis1,axis2) = plt.subplots(1, 2, figsize=(14,6))\n# Create a plot showing the count of observations with different numbers of parents \/ children on board the titanic\nsns.countplot(x = 'Parch', data = training_data_2, palette = 'Blues_r', ax = axis1)\naxis1.set_xlabel('Number of Parents \/ Children On Board Titanic')\n# Create a plot showing the proportion of with different numbers of parents \/ children onboard the titanic who survive\nsns.barplot('Parch', 'Survived', data = training_data_2, palette = 'Oranges_r', ax = axis2)\naxis2.set_xlabel('Number of Parents \/ Children On Board Titanic')\nplt.show()","375893a6":"# Create a density plot showing the distribution of fares passengers paid\nplt.figure(figsize=(15,8))\nax1 = sns.kdeplot(training_data_2['Fare'], color='grey', shade=True)\nplt.title('Density Plot of Fare')\nax1.set(xlabel = 'Fare')\nplt.show()","f71bfa3c":"# Create a pivot table showing the counts of passengers per ticket number\ntraining_data_3 = training_data_2.copy()\ntraining_data_3.drop(columns = ['Survived'], inplace = True)\ncombined_data = training_data_3.append(testing_data_2, sort = True)\nticket_counts = combined_data.pivot_table(index = 'Ticket', values = ['Name'], aggfunc = {'Name':'count'})\nticket_counts = ticket_counts.rename(columns = {'Name': 'TicketCount'})","9a58e071":"# Merge the dataset with the pivot table in order to add a column showing the number of passengers assigned to the observation's ticket number\ntraining_data_2 = training_data_2.merge(ticket_counts, left_on='Ticket', right_on = 'Ticket')","66d9e15a":"# Look at the dataset with the new column added to see if ticket fares correspond to individual passengers or to all passengers in groups with the same ticket number\ntraining_data_2.sort_values(by=['TicketCount'], ascending = False).head(20)","2f2ccaf7":"# Drop the new ticket count column -- feature engineering is handled later in this notebook\ntraining_data_2.drop(columns = ['TicketCount'], inplace = True)","74181af8":"# Set up the figure with two subplots\nfig, (axis1,axis2) = plt.subplots(1, 2, figsize=(14,6))\n# Create a plot showing the count of observations of passengers leaving from different ports\nsns.countplot(x = 'Embarked', data = training_data_2, palette = 'Blues_r', ax = axis1)\naxis1.set_xlabel('Port of Embarkation')\naxis1.set_xticklabels(['Cherbourg', 'Queenstown', 'Southampton'])\n# Create a plot showing the proportion of with different numbers of passengers leaving from different ports who survive\nsns.barplot('Embarked', 'Survived', data = training_data_2, palette = 'Oranges_r', ax = axis2)\naxis2.set_xlabel('Port of Embarkation')\naxis2.set_xticklabels(['Cherbourg','Queenstown', 'Southampton'])\nplt.show()","7755373a":"# Define children as under 16 and adults as older than 16 or older\ntraining_data_2['ChildAdult'] = pd.cut(training_data_2['Age'],[0,16,81],  labels = ['child', 'adult'], right = False)\ntesting_data_2['ChildAdult'] = pd.cut(testing_data_2['Age'],[0,16,81],  labels = ['child', 'adult'], right = False)","6411a943":"# Set up the figure with two subplots\nfig, (axis1,axis2) = plt.subplots(1, 2, figsize=(14,6))\n# Create a plot showing the count of observations with different numbers of parents \/ children on board the titanic\nsns.countplot(x = 'ChildAdult', data = training_data_2, palette = 'Blues_r', ax = axis1)\naxis1.set_xlabel('Child or Adult')\n# Create a plot showing the proportion of with different numbers of family members onboard the titanic who survive\nsns.barplot('ChildAdult', 'Survived', data = training_data_2, palette = 'Oranges_r', ax = axis2)\naxis2.set_xlabel('Child or Adult')\nplt.show()","ba618a96":"# Create family size variable\ntraining_data_2['FamilySize'] = training_data_2['SibSp'] + training_data_2['Parch']\ntesting_data_2['FamilySize'] = testing_data_2['SibSp'] + testing_data_2['Parch']","6549f94c":"# Set up the figure with two subplots\nfig, (axis1,axis2) = plt.subplots(1, 2, figsize=(14,6))\n# Create a plot showing the count of observations with different numbers of family members on board the Titanic\nsns.countplot(x = 'FamilySize', data = training_data_2, palette = 'Blues_r', ax = axis1)\naxis1.set_xlabel('Family Size')\n# Create a plot showing the proportion of passengers with different numbers of family members onboard the Titanic who survived\nsns.barplot('FamilySize', 'Survived', data = training_data_2, palette = 'Oranges_r', ax = axis2)\naxis2.set_xlabel('Family Size')\nplt.show()","06c8e0df":"# Create 'WithFamily' variable and define passengers as being with family if family size is greater than 0 \ntraining_data_2['WithFamily'] = np.where((training_data_2['FamilySize'] > 0), 1, 0)\ntesting_data_2['WithFamily'] = np.where((testing_data_2['FamilySize'] > 0), 1, 0)","d9c5e71e":"# Set up the figure with two subplots\nfig, (axis1,axis2) = plt.subplots(1, 2, figsize=(14,6))\n# Create a plot showing the count of observations with different numbers of parents \/ children on board the Titanic\nsns.countplot(x = 'WithFamily', data = training_data_2, palette = 'Blues_r', ax = axis1)\naxis1.set_xlabel('Passengers')\naxis1.set_xticklabels(['Travel Alone','Travel with Family'])\n# Create a plot showing the proportion of passengers with different numbers of family members onboard the titanic who survived\nsns.barplot('WithFamily', 'Survived', data = training_data_2, palette = 'Oranges_r', ax = axis2)\naxis2.set_xlabel('Passengers')\naxis2.set_xticklabels(['Travel Alone', 'Travel with Family'])\nplt.show()","2784b9d4":"# Create variable IsChild that is equal to 1 when the passenger is <16 years old and 0 otherwise\ntraining_data_2['IsChild'] = np.where(training_data_2['ChildAdult'] == 'child', 1 , 0)\ntesting_data_2['IsChild'] = np.where(testing_data_2['ChildAdult'] == 'child', 1 , 0)\n# Create variable IsFemale that is equal to 1 when the passenger is female and 0 otherwise\ntraining_data_2['IsFemale'] = np.where(training_data_2['Sex'] == 'female', 1 , 0)\ntesting_data_2['IsFemale'] = np.where(testing_data_2['Sex'] == 'female', 1 , 0)","ac7eec8b":"# Create a pivot table that counts the number of women and number of children with each ticket number; This is to be used as a proxy to capture the number of women and children traveling in each group\ntraining_data_4 = training_data_2.copy()\ntraining_data_4.drop(columns = ['Survived'], inplace = True)\ncombined_data_2 = training_data_4.append(testing_data_2, sort = True)\nticket_counts_2 = combined_data_2.pivot_table(index = 'Ticket', values = ['Name','IsChild', 'IsFemale'], aggfunc = {'Name':'count', 'IsChild':'sum', 'IsFemale':'sum'})\nticket_counts_2 = ticket_counts_2.rename(columns = {'Name': 'TicketCount', 'IsChild':'NumberOfChildren','IsFemale':'NumberOfFemales'})","9187cea1":"# Merge the newly-created pivot tables with the training and testing dataset to access the count of women and children traveling in each group\ntraining_data_2 = training_data_2.merge(ticket_counts_2, left_on='Ticket', right_on = 'Ticket')\ntesting_data_2 = testing_data_2.merge(ticket_counts_2, left_on='Ticket', right_on = 'Ticket')","6b008acf":"# Create dummy variable indicating whether or not a passenger is an adult traveling with a child\ntraining_data_2['TravelWChild'] = np.where((training_data_2['NumberOfChildren']>0) & (training_data_2['ChildAdult'] == 'adult'),1,0)\ntesting_data_2['TravelWChild'] = np.where((testing_data_2['NumberOfChildren']>0) & (testing_data_2['ChildAdult'] == 'adult'),1,0)\n# Create dummy variable indicating whether or not a passenger is a man traveling with a woman\ntraining_data_2['TravelWFemale'] = np.where((training_data_2['NumberOfFemales']>0) & (training_data_2['Sex'] == 'male'),1,0)\ntesting_data_2['TravelWFemale'] = np.where((testing_data_2['NumberOfFemales']>0) & (testing_data_2['Sex'] == 'male'),1,0)","feb9ed41":"# Set up the figure with two subplots\nfig, (axis1,axis2) = plt.subplots(1, 2, figsize=(14,6))\n# Create a plot showing the count of observations of adults traveling with children and adults traveling without children\nsns.countplot(x = 'TravelWChild', data = training_data_2[training_data_2['ChildAdult']=='adult'], palette = 'Blues_r', ax = axis1)\naxis1.set_xlabel('Passengers')\naxis1.set_xticklabels(['Travel Without Child', 'Travel With Child'])\n# Create a plot showing the proportion of adults traveling with children and adults traveling without children who survived\nsns.barplot('TravelWChild', 'Survived', data = training_data_2[training_data_2['ChildAdult']=='adult'] , palette = 'Oranges_r', ax = axis2)\naxis2.set_xlabel('Passengers')\naxis2.set_xticklabels(['Travel Without Child', 'Travel With Child'])\nplt.show()","84eb3c4a":"# Set up the figure with two subplots\nfig, (axis1,axis2) = plt.subplots(1, 2, figsize=(14,6))\n# Create a plot showing the count of observations of men traveling with a female and men traveling without a female\nsns.countplot(x = 'TravelWFemale', data = training_data_2[training_data_2['Sex']=='male'], palette = 'Blues_r', ax = axis1)\naxis1.set_xlabel('Passengers')\naxis1.set_xticklabels(['Travel Without Female', 'Travel With Female'])\n# Create a plot showing the proportion of men traveling with women and men traveling without women who survive\nsns.barplot('TravelWFemale', 'Survived', data = training_data_2[training_data_2['Sex']=='male'], palette = 'Oranges_r', ax = axis2)\naxis2.set_xlabel('Passengers')\naxis2.set_xticklabels(['Travel Without Female', 'Travel With Female'])\nplt.show()","5f3ceb2f":"# Calculate 'FarePerPassenger' as 'Fare' \/ 'TicketCount' in order to capture the fare related to each individual observation\ntraining_data_2['FarePerPassenger'] = training_data_2['Fare'] \/ training_data_2['TicketCount']\ntesting_data_2['FarePerPassenger'] = testing_data_2['Fare'] \/ testing_data_2['TicketCount']","d367e5ab":"# Plot the density plot of fare per passenger\nplt.figure(figsize=(15,8))\nax1 = sns.kdeplot(training_data_2['FarePerPassenger'], color='grey', shade=True)\nplt.title('Density Plot of Fare per Passenger')\nax1.set(xlabel = 'Fare per Passenger')\nplt.show()","e0e81639":"# Plot density plots of fares paid per passenger for passengers who survive relative to passengers who die \nplt.figure(figsize=(15,8))\nax1 = sns.kdeplot(training_data_2['FarePerPassenger'][(training_data_2.Survived == 1)], color='royalblue', shade=True)\nax2 = sns.kdeplot(training_data_2['FarePerPassenger'][(training_data_2.Survived == 0)], color='orange', shade=True)\nplt.legend(['Survived', 'Died'])\nplt.title('Density Plot of Fare per Passenger for Surviving Population and Deceased Population')\nax2.set(xlabel = 'Fare per Passenger')\nplt.xlim(-10,80)\nplt.show()","ac7a83a7":"# Plot density plots of fares paid per passenger for passengers for upper, middle, and lower class passengers\nplt.figure(figsize=(15,8))\nax1 = sns.kdeplot(training_data_2['FarePerPassenger'][(training_data_2.Pclass == 1)], color = 'royalblue')\nax2 = sns.kdeplot(training_data_2['FarePerPassenger'][(training_data_2.Pclass == 2)], color = 'orange')\nax3 = sns.kdeplot(training_data_2['FarePerPassenger'][(training_data_2.Pclass == 3)], color = 'dimgray')\nplt.legend(['Upper Class', 'Middle Class', 'Lower Class'])\nplt.title('Density Plot of Fare per Passenger for People of Different Classes')\nax2.set(xlabel = 'Fare per Passenger')\nplt.xlim(-10,80)\nplt.show()","080b69e2":"# Plot density plots of fares paid per passenger for passengers who survive relative to passengers who die for upper class passengers only \nplt.figure(figsize=(15,8))\nax1 = sns.kdeplot(training_data_2['FarePerPassenger'][(training_data_2.Survived == 1) & (training_data_2.Pclass == 1)], color='royalblue', shade=True)\nax2 = sns.kdeplot(training_data_2['FarePerPassenger'][(training_data_2.Survived == 0) & (training_data_2.Pclass == 1)], color='orange', shade=True)\nplt.legend(['Survived', 'Died'])\nplt.title('Density Plot of Fare per Passenger for Surviving Population and Deceased Population of Upper Class Passengers')\nax2.set(xlabel = 'Fare per Passenger')\nplt.xlim(-10,80)\nplt.show()","34a83f4f":"# Plot density plots of fares paid per passenger for passengers who survive relative to passengers who die for middle class passengers only\nplt.figure(figsize=(15,8))\nax1 = sns.kdeplot(training_data_2['FarePerPassenger'][(training_data_2.Survived == 1) & (training_data_2.Pclass == 2)], color='royalblue', shade=True)\nax2 = sns.kdeplot(training_data_2['FarePerPassenger'][(training_data_2.Survived == 0) & (training_data_2.Pclass == 2)], color='orange', shade=True)\nplt.legend(['Survived', 'Died'])\nplt.title('Density Plot of Fare per Passenger for Surviving Population and Deceased Population of Middle Class Passengers')\nax2.set(xlabel = 'Fare per Passenger')\nplt.xlim(-3,19)\nplt.show()","ee67e2d3":"# Plot density plots of fares paid per passenger for passengers who survive relative to passengers who die for lower class passengers only\nplt.figure(figsize=(15,8))\nax1 = sns.kdeplot(training_data_2['FarePerPassenger'][(training_data_2.Survived == 1) & (training_data_2.Pclass == 3)], color='royalblue', shade=True)\nax2 = sns.kdeplot(training_data_2['FarePerPassenger'][(training_data_2.Survived == 0) & (training_data_2.Pclass == 3)], color='orange', shade=True)\nplt.legend(['Survived', 'Died'])\nplt.title('Density Plot of Fare per Passenger for Surviving Population and Deceased Population of Lower Class Passengers')\nax2.set(xlabel = 'Fare per Passenger')\nplt.xlim(-1,15)\nplt.show()","a2de1824":"# Since data has been cleaned and new features produces, create a new dataframe for the cleaned data\ntraining_data_cleaned = training_data_2.copy()\ntesting_data_cleaned = testing_data_2.copy()","164f7689":"# Create a dataframe for the data used in training the logistic regression model\nlog_res_training = training_data_cleaned.copy()","ac29d03c":"# Drop variables that are not being used\nlog_res_training.drop(columns = ['Cabin','Age','SibSp', 'Parch', 'FamilySize','Name', 'Ticket','Sex', 'ChildAdult','TicketCount','NumberOfChildren', 'NumberOfFemales', 'Fare'], inplace = True)","759f54be":"# Create binary variables for all categorical variables and drop the first category to avoid perfect multicollinearity\nlog_res_training = pd.get_dummies(log_res_training, columns = ['Embarked'], drop_first = True)","22fd7d74":"# Apply standard scaler to 'Fare' column\nfare_per_passenger = log_res_training[['FarePerPassenger']].values\nfare_per_passenger = StandardScaler().fit_transform(fare_per_passenger)\nlog_res_training['FarePerPassenger'] = fare_per_passenger","a28672a8":"# Take a look at the final dataset\nlog_res_training.head()","67249e96":"# Define the dependent and independent variables\ny_logistic = log_res_training.iloc[:,0].values\nX_logistic = log_res_training.iloc[:,1:].values","5c00eccb":"# Fit a logistic model to the data\nlogistic_regression_classifier = sm.Logit(endog = y_logistic, exog = X_logistic).fit()","d7e93f9b":"# Show coefficients of the logistic regression\nlogistic_regression_classifier.summary()","69c3475e":"# The 'TravelWChild' variable (Binary variable indicating whether or not the individual traveled with a child) has a high p-value revealing that the variable is not statistically significant in this model. Remove the 'TravelWChild' variable\nX_logistic = np.delete(X_logistic,4, axis = 1)","1a872055":"# Fit a logistic model to the data (with the 'TravelWChild' variable removed) \nlogistic_regression_classifier = sm.Logit(endog = y_logistic, exog = X_logistic).fit()","d03aecda":"# Show coefficients of the logistic regression\nlogistic_regression_classifier.summary()","d3dfcf58":"# The 'Embarked_Q' variable (Binary variable indicating whether or not the individual embarked the Titanic from the Queenstown port) has a high p-value revealing that the variable is not statistically significant in this model. Remove the 'Embarked_Q' variable\nX_logistic = np.delete(X_logistic,6, axis = 1)","4d645d23":"# Fit a logistic model to the data (with the 'TravelWChild' and 'Embarked_Q' variables removed)\nlogistic_regression_classifier = sm.Logit(endog = y_logistic, exog = X_logistic).fit()","78a0b22e":"# Show coefficients of the logistic regression\nlogistic_regression_classifier.summary()","323b6f72":"# The 'Embarked_S' variable (Binary variable indicating whether or not the individual embarked the Titanic from the Southampton port) has a high p-value revealing that the variable is not statistically significant in this model. Remove the 'Embarked_S' variable\nX_logistic = np.delete(X_logistic,6, axis = 1)","8523ae4d":"# Fit a logistic model to the data (with the 'TravelWChild', 'Embarked_Q', and 'Embarked_S' variables removed)\nlogistic_regression_classifier = sm.Logit(endog = y_logistic, exog = X_logistic).fit()","15f937c1":"# Show coefficients of the logistic regression\nlogistic_regression_classifier.summary()","25b894dd":"# The 'WithFamily' variable (binary variable indicating whether or not a passenger is traveling with family) has a high p-value revealing that the variable is not statistically significant in this model. Removing the 'WithFamily' variable\nX_logistic = np.delete(X_logistic,1, axis = 1)","cf414fe6":"# Fit a logistic model to the data (with the 'TravelWChild', 'Embarked_Q', 'Embarked_S', and 'WithFamily' variables removed)\nlogistic_regression_classifier = sm.Logit(endog = y_logistic, exog = X_logistic).fit()","cdbf5f98":"# Show coefficients of the logistic regression\nlogistic_regression_classifier.summary()","e0901aa8":"# The 'TravelWFemale' variable (variable indicating whether a male passenger is traveling with a female) has a high p-value revealing that the variable is not statistically significant in this model. Removing the 'TravelWFemale' variable\nX_logistic = np.delete(X_logistic,3, axis = 1)","233ffc1d":"# Fit a logistic model to the data (with the with the 'TravelWChild', 'Embarked_Q', 'Embarked_S', 'WithFamily', and 'TravelWFemale' variables removed)\nlogistic_regression_classifier = sm.Logit(endog = y_logistic, exog = X_logistic).fit()","76259c4d":"# Show coefficients of the logistic regression\nlogistic_regression_classifier.summary()","31d12be3":"# Fit the final logistic regression classifier\nlogistic_regression_classifier = LogisticRegression(random_state = 0)\nlogistic_regression_classifier.fit(X_logistic, y_logistic)","efff1bcc":"# Make a copy of the cleaned training data to avoid errors\ndec_tree_training = training_data_cleaned.copy()","e4810810":"# Drop variables not being used\ndec_tree_training.drop(columns = ['Cabin','SibSp', 'Parch','Name', 'Ticket','Sex', 'ChildAdult','IsChild','WithFamily','TicketCount','NumberOfChildren', 'NumberOfFemales', 'Fare'], inplace = True)","7546511f":"# Create dummy variables for nominal categorical variables\ndec_tree_training = pd.get_dummies(dec_tree_training, columns = ['Embarked'])","eba62efb":"# Look at the dataset\ndec_tree_training.head()","3743214a":"# Define x and y variables for the decision tree\ny_tree = dec_tree_training.iloc[:,0].values\nX_tree = dec_tree_training.iloc[:,1:].values","42bb1f99":"# Create a decision tree to the data without passing any parameters\ntree_1 = DecisionTreeClassifier()\ntree_1 = tree_1.fit(X_tree, y_tree)","8013b0c5":"# Show chart of this initial decision tree\nplt.figure(figsize=(75,40))\ntree_1_image = plot_tree(tree_1, \n              feature_names=dec_tree_training.iloc[:,1:].columns, \n              class_names={0:'Died',1:'Survived'},\n              filled=True, \n              rounded=True, \n              fontsize=14)","d83d94da":"# Define a list of max_depths, min_samples_leaves, and min_samples_splits\nmax_depths = list(range(1,41))\nmin_samples_leaves = list(range(1,41))\n# min_samples_splits = list(range(1,41))","2cd92229":"# Pass the parameters into GridSearchCV\ngrid_decision_tree = GridSearchCV(DecisionTreeClassifier(),{'max_depth': max_depths, 'min_samples_leaf': min_samples_leaves}, cv = 5,scoring = 'roc_auc', n_jobs = -1)","ff3a73fd":"# Use GridSearchCV to figure out the best possible parameters to pass\ngrid_decision_tree.fit(X_tree, y_tree)","db7f3a80":"# Fit the newly optimized decision tree\ntree_2 = DecisionTreeClassifier(max_depth = grid_decision_tree.best_params_['max_depth'], min_samples_leaf = grid_decision_tree.best_params_['min_samples_leaf'])\ntree_2 = tree_2.fit(X_tree, y_tree)","2441e552":"# Display the newly optimized decision tree\nplt.figure(figsize=(50,20))\ntree_2_image = plot_tree(tree_2, \n              feature_names=dec_tree_training.iloc[:,1:].columns, \n              class_names={0:'Died',1:'Survived'},\n              filled=True, \n              rounded=True, \n              fontsize=14)","2f77f205":"# Create copy of the training data to use for random forest model training\nrndm_frst_training = training_data_cleaned.copy()","cea30e33":"# Drop columns that are not being used in the model\nrndm_frst_training.drop(columns = ['Cabin','SibSp', 'Parch', 'WithFamily','Name', 'Ticket','Sex', 'ChildAdult','TicketCount','NumberOfChildren', 'NumberOfFemales', 'Fare', 'IsChild'], inplace = True)","b03fab80":"# Create dummy variables for the 'Embarked' feature\nrndm_frst_training = pd.get_dummies(rndm_frst_training, columns = ['Embarked'])","fffaa8a5":"# Look at the final dataset\nrndm_frst_training.head()","3a288134":"# Define x and y variables\ny_forest = rndm_frst_training.iloc[:,0].values\nX_forest = rndm_frst_training.iloc[:,1:].values","36d3090f":"# Define the ranges of parameter values to test\nn_estimators = list(range(1,126))\nmax_depths = list(range(1,34))\nmin_samples_splits = list(range(1,34))\nmin_samples_leaves = list(range(1,34))","d2864a80":"# Pass the parameters into RandomizedSearchCV\ngrid_random_forest = RandomizedSearchCV(RandomForestClassifier(),{'n_estimators':n_estimators, 'max_depth': max_depths, 'min_samples_leaf': min_samples_leaves, 'min_samples_split': min_samples_splits}, cv = 5,scoring = 'roc_auc', n_iter = 1000, n_jobs = -1, random_state = 0)","f5dc1fad":"# Use RandomizedSearchCV to figure out the best possible parameters to pass\ngrid_random_forest.fit(X_forest, y_forest)","6fc6eef8":"# Fit the newly optimized decision tree\nforrest_1 = RandomForestClassifier(random_state = 0, max_features = 3, n_estimators = grid_random_forest.best_params_['n_estimators'], max_depth = grid_random_forest.best_params_['max_depth'], min_samples_leaf = grid_random_forest.best_params_['min_samples_leaf'], min_samples_split = grid_random_forest.best_params_['min_samples_split'])\nforrest_1 = forrest_1.fit(X_tree, y_tree)","c39d3f10":"# Measure performance of the logistic regression model using 5 fold cross validation\nscores_logistic_regression = cross_validate(logistic_regression_classifier, X_logistic, y_logistic, cv = 5, scoring = ['accuracy','precision', 'recall', 'f1', 'roc_auc'])","fde96b26":"# Print the performance scores for the logistic regression\nprint('Logistic Regression Test Results')\nprint('--------------------------------------')\nprint('accuracy: ' + str(scores_logistic_regression['test_accuracy'].mean()))\nprint('precision score: ' + str(scores_logistic_regression['test_precision'].mean()))\nprint('recall score: ' + str(scores_logistic_regression['test_recall'].mean()))\nprint('f1 score: ' + str(scores_logistic_regression['test_f1'].mean()))","f5af8ba4":"# Measure performance of the decision tree model using 5 fold cross validation\nscores_decision_tree = cross_validate(tree_2, X_tree, y_tree, cv = 5, scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc'])","dd3f1154":"# Print the performance scores for the decision tree\nprint('Decision Tree Test Results')\nprint('--------------------------------------')\nprint('accuracy: ' + str(scores_decision_tree['test_accuracy'].mean()))\nprint('precision score: ' + str(scores_decision_tree['test_precision'].mean()))\nprint('recall score: ' + str(scores_decision_tree['test_recall'].mean()))\nprint('f1 score: ' + str(scores_decision_tree['test_f1'].mean()))","c0e95a0b":"# Measure performance of the random forest model using 5 fold cross validation\nscores_random_forest = cross_validate(forrest_1, X_forest, y_forest, cv = 5, scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc'])","7577e3dc":"# Print the performance scores for the random forest\nprint('Random Forest Test Results')\nprint('--------------------------------------')\nprint('accuracy: ' + str(scores_random_forest['test_accuracy'].mean()))\nprint('precision score: ' + str(scores_random_forest['test_precision'].mean()))\nprint('recall score: ' + str(scores_random_forest['test_recall'].mean()))\nprint('f1 score: ' + str(scores_random_forest['test_f1'].mean()))","7655e5b5":"# Create the submission as its own dataframe\ncompetition_submission = testing_data_cleaned.copy()","0179cd88":"# Drop columns that are not being used in the model\ncompetition_submission.drop(columns = ['Cabin','SibSp', 'Parch', 'WithFamily','Name', 'Ticket','Sex', 'ChildAdult','TicketCount','NumberOfChildren', 'NumberOfFemales', 'Fare', 'IsChild'], inplace = True)","401676dc":"# Create dummy variables for the 'Embarked' feature\ncompetition_submission = pd.get_dummies(competition_submission, columns = ['Embarked'])","33ea21ed":"# Look at the final dataset\ncompetition_submission.head()","740fa9de":"# Define the independent variables\nX_competition = competition_submission.iloc[:,1:].values","18aff509":"# Create survived column with the predictions included\ncompetition_submission['Survived'] = grid_random_forest.predict(X_competition)","09cc5f54":"# Drop the columns that are not needed\ncompetition_submission.drop(columns = ['Pclass', 'Age', 'TravelWFemale', 'FamilySize', 'IsFemale', 'TravelWChild', 'FarePerPassenger', 'Embarked_C', 'Embarked_Q', 'Embarked_S'], inplace = True)","c53b9762":"# Export the best performing model's predictions\ncompetition_submission.to_csv('\/kaggle\/working\/submission_rf.csv', index = False)","d39cfa7a":"In order to prepare the dataset for logistic regression, it is important to change the nominal variables ('Sex', 'Embarked', and 'ChildAdult') into separate binary variables for each of the classifications. For example, 'Embarked' has a separate binary variable for whether or not the passenger embarked from Cherbourg, Queenstown, or Southhampton. For logistic regression, it is critical to remove one of the binary variables from each category in order to prevent perfect multicollinearity across the independent variables. For example, if only binary variables indicating if the passenger embarked from 'Queenstown' and 'Southhampton' are included, it is not necessary to include a binary variable indicating if the passenger embarked from 'Cherbourg' because if both the Queenstown and Southhampton indicate a 0 value, then by default the passenger must have embarked from 'Cherbourg', which is captured by the intercept term.\n\nCoefficients in logistic regression are determined by minimizing the cost function (log los) using gradient descent. As a result, it is important to apply feature scaling to variables with wide ranges to ensure that the conversion to the global minimum (the gradient descent process that happens to establish the coefficients) happens more quickly.\n\n'Pclass' is a variable that indicates whether a passenger is upper class, middle class, or lower class. The variable is originally indicated as 1 (upper), 2 (middle), or 3 (lower). Since the range of values for this variable is relatively narrow and applying feaure scaling to this variable would mean losing interpretability of this variable, feature scaling is not applied to 'Pclass' in this model.\n\n'FarePerPassenger' is a variable that indicates the price the passenger paid for her\/his ticket. This variable has a wide range so this variable needs to be scaled down to make establishing the coefficients more efficient. The variable is standardized based on the normal distrbution so that the mean of the variable is ~0 and the standard deviation of the variable is ~1.","6ce6702d":"### 6a. Background","d86ad4bb":"Random forest takes longer to run than decision tree. Additionally, more parameters are being optimized for in this random forest. As a result, GridSearchCV takes too long to run. As a result, RandomizedSearchCV is used instead. Like GridSearchCV, RandomizedSearchCV performs cross validation to optimize the model across a variety of combinations of parameters. However, instead of testing every combination of every parameter, RandomizedSearchCV only tests a random sample of parameter value combinations.","30c1a08a":"### 3d. Explore Sibling\/Spouses On Board the Titanic","231abc16":"### 6c. Run initial decision tree","15c074a7":"### 5a. Background","896a7295":"### 8a. Background on performance metrics","2f6be2ae":"Logistic regression models the probability that the binary dependent variable Y falls into one of two categories. Logistic regression describes the relationship between the binary dependent variable and one or more independent variables. The model is is fit using maximum likelihood.","2b157959":"The dataset is composed of significantly more men than women. Women were much more likely to survive the shipwreck than men.","563057d4":"<a id=\"t1.\"><\/a>\n# 1. Import Data and Python Packages","d95995ae":"### 2d. Deal with missing 'Age' values","9e152326":"### 2c. Deal with missing 'Embarked' values","2f522186":"### 8b. Comparison","f9ee05ff":"### 6b. Prepare variables for decision tree","5249aeb1":"### 4c. Capture Residual Effects of Traveling with Women and Traveling with Children","ec2087b5":"One fare is missing from the test dataset.\n\n**Options for handling missing values:**\n1. Remove the field entirely\n    - Not a good solution because 'Fare' is a usable field\n    - The vast majority of observations for the 'Fare' field are present, making 'Fare' a very usable field that should not be removed\n2. Remove the observations that contain the missing values\n    - Not a good solution because the missing value exists in the test dataset\n    - The goal of the Titanic Problem is to predict whether or not each of the passengers in the test dataset survive. Removing one observation from the test dataset would mean there would be no basis for predicting survivorship for the removed observation\n3. Replace the missing values with approximations of the values\n    - The best solution in this case\n\n**Strategy for replacing the missing fare value:**\n\n'Fare' is a right skewed variable so it is preferable to replace the missing value with a median value rather than an average value so that the approximation is not overestimated. Ideally, the fare approximation should be based on a the fare paid by a set of passengers similar to the passenger whose fare paid is unknown. In this case, it is a good solution to replace the missing fare with the median fare paid by passengers with the same ticket class, sex, and port of embarkation as the passenger whose fare paid is unknown.","cedc0d1f":"### 3a. Explore Ticket Class","406d0e51":"### 2e. Deal with missing 'Cabin' values","9cd4a8a9":"The initial output reveals several independent variables with p-value > 0.05. The p-value for regression coefficients quantifies how much confidence there is that the independent variable impacts the dependent variable. The smaller the p value, the stronger the evidence that there is a relationship between the independent and dependent variable. A p-value threshold of 0.05 is used in this analysis. \n\nThere is not a good reason to keep insignificant variables in the model so a backward elimination approach is used to ensure that all variables used in the logistic regression have statistically significant relationships with the dependent variable. In the backward elimination approach, the model is run and the p-value of each independent variable is observed. Then, the independent variable with the largest p-value greater than 0.05 is removed from the model. This methodology is repeated until all remaining independent variables have p-values less than 0.05.","44185e43":"The following variables all have p-values less than 0.05 and thus remain in the logistic regression model:\n* 'Pclass': An ordinal variable indicating someone's passenger class using a rating of 1 (upper class), 2 (middle class), or 3 (lower class)\n* 'FarePerPassenger': The price that the passenger paid to board the ship (note that this variable was standardized to have a mean of 0 and a standard deviation of ~1)\n* 'isChild': A binary variable indicating whether or not the passenger is a child (defined as younger than 16 in this analysis). 1 indicates that the passenger is a child. 0 indicates that the passenger is an adult.\n* 'isFemale': A binary variable indicating whether or not the passenger is female. 1 indicates the passenger is female. 0 indicates the passenger is male","886c98a5":"### 7a. Background","29283a69":"### 4d. Turning 'Fare' feature into 'FarePerPassenger'","4934f5eb":"### 2b. Deal with missing 'Fare' values","92696cd5":"<a id=\"t8.\"><\/a>\n# 8. Evaluate Performance of Models","39699d47":"The training dataset has 11 fields and 891 observations. The testing dataset has 10 fields and 418 observations. The training dataset includes the field 'Survived' indicating whether or not the passenger survived while the testing dataset does not.\n\nThere are missing values in the 'Cabin', 'Age', 'Embarked', and 'Fare' columns. It is important to figure out a strategy to deal with these missing values so that machine learning algorithms will work.\n\nTo handle missing values, there are primarily three options:\n1. Remove the field entirely\n2. Remove the observations that contain the missing values\n3. Replace the missing values with approximations of the values","4d724a32":"Of the three models, the random forest model appears to perform the best. The random forest is used to predict the values in the test dataset.","3f732484":"<a id=\"t6.\"><\/a>\n# 6. Decision Tree","52d956ee":"### 3e. Explore Parents\/Children On Board the Titanic","8d37f606":"<a id=\"t2.\"><\/a>\n# 2. Perform Initial Observations and Deal with Missing Values","355dbae4":"### 5c. Run first logistic regression","c09fafb5":"<a id=\"t5.\"><\/a>\n# 5. Logistic Regression","2719958c":"### 5b. Prepare variables for logistic regression","6114c1cb":"Lower class ticket holders make up the bulk of the training dataset relative to middle and upper class ticket holders. People of higher classes tend to be more likely to survive the shipwreck.","b55db1d8":"The initial tree is very deep and is very difficult to explain. When no parameters are set, the decision tree grows until either all leaves are pure or until all leaves contain less than the min_samples_split samples (default = 2). Since the default min_samples_leaf value is 1, the algorithm is able to create splits until there is only one observation left in a leaf node. \n\nGiven the size and lack of interpretability of this initial decision tree, it is likely that this decision tree is overfitting the data. Overfitting occurs when a function is too closely fit to a limited set of data points. As a result, while overfit models tend to perform well on training data, but poorly on new observations (testing data).\n\nIn order to prevent overfitting, the model needs to be optimized. See below for definitions of the parameters that are optimized in section 6d:\n* max_depth: The default is set to 'None'. This defines the maximum depth of a tree. If None, the nodes are expanded until all leaves contain less than min_samples_split samples. The higher value of maximum depth causes overfitting and a lower value causes underfitting. If not set to a limit, the decision tree will split until all leaves are pure or until all leaves contain less than the min_samples_split (default = 2) samples\n* min_samples_leaf: The default is set to one. This defines the minimum number of samples required to be at a leaf node. \n* min_samples_split: The default is set to two. This defines the minimum number of samples required to split an internal node. ","ed14ff4a":"<a id=\"t4.\"><\/a>\n# 4. Feature Engineering","31e58aac":"In the above example, GridSearchCV is used to optimize the decision tree. GridSearchCV accepts a set of parameters, tries all possible combinations of those parameters, and keeps track of the resulting cross validation scores for all combinations of parameters. Max_depth, min_samples_leaf, and min_samples_split from 1 to 40 are all attempted at all combinations and the best performing combination of all parameters is selected.\n\nCross validation partitions the data into K bins of equal size. The algorithm uses this data to run K different learning experiments. Each learning experiement uses a different partition as the test set and the remaining K-1 partitions as the training set. After the algorithm runs the K experiments, the resulting performances from the K experiments are averaged to get the model's overall performance. In the case of the above example, cross validation is run using 5 partitions meaning that 20% of the data is tested in each of the 5 experiements. \n\nIn order to choose the optimal parameters of the model, 'ROC_AUC' is used to assess model performance. \n\nAn ROC graph shows the true positive rate (sensitivity or (TP)\/(TP+FN)) on the y-axis and the false positive rate (1 - specificity or (FP)\/(FP+TN)) on the x axis. Calculating sensitivity and 1-specificity for all possible thresholds for a given model creates the points for the ROC curve. A diagonal line marking instances in which the true positive rate = false postive rate provides a contrast to the ROC curve. The diagonal line marks instances in which the proportion of correctly classified positive samples is the same proportion of incorrectly classified negative samples. \n\nAUC measures the area under the ROC curve or the percentage of the overall graph beneath the ROC curve. Higher AUC scores indicate better performing models while lower AUC scores indicate poorer model performance. An AUC score of 0.5 means that AUC is equal to that of the diagonal line meaning that the model has no capacity to distinguish between the positive class and the negative class. An AUC score of 1.0 means the classifier is perfect with both high sensitivity and high specificity.\n\nIn the above diagram, there are some splits that yield the same predicted value in all divisions of a singular split. This is because splits are created to maximize node purity. Instances in which there is a final split that yields two leaf notes with identical predicted classifications are the result of one leaf node having more node purity than the other.\n\nWhile Decision Trees are easy to interpret, they tend to be inaccurate when making predictions on new data. Additionally, decision trees often suffer from high variance. For example, if the training data were split into two parts at random and a decision tree were fit to both halves, the results would be very different. To solve for this, a Random Forest Model is used below.","63821f55":"<a id=\"t9.\"><\/a>\n# 9. Make Predictions","61b74836":"It appears that women and children are more likely to survive the shipwreck than other passengers. It is also possible that women and children have a residual impact on the survivorship for the other passengers in their parties. For example, if a child were to board a lifeboard, perhaps the Titanic crew members require a parent\/guardian to chaperone the child on the lifeboat. So adults traveling with children may have higher odds of surviving the shipwreck.\n\nThe fields 'TravelWFemale' and 'TravelWChild' are created to capture this residual effect. 'TravelWFemale' captures men who share ticket numbers at least one woman. 'TravelWChild' captures adults who share ticket numbers with at least one child. \n\nPreliminary investigation shows that ~10-15% of adults travel with at least one child and ~25% of men travel with at least one woman. Adults traveling with at least one child have higher survivorship rates than adults traveling without a child and men traveling with at least one woman have higher survivorship rates than men traveling without any women. ","4ee9adb8":"'SibSp' and 'Parch' are combined to create a 'FamilySize' variable. Additionally in order to better capture passengers traveling alone versus passengers traveling with family, a 'WithFamily' variable is created.\n\nThe meaning behind 'SibSp' and 'Parch' on their own is unclear so the effect is best combined to capture the size of the passenger's family to make the data easier to understand and the results easier to interpret. Additionally, since it appears that passengers traveling along are significantly less likely to survive than passengers traveling with family, the 'WithFamily' variable is created to isolate the effects of passengers traveling alone versus with family.","6759f800":"### 7b. Prepare variables for random forest","26be4585":"### 5d. Run subsequent logistic regression models using a backward elimination approach","6ae74983":"### 8c. Conclusion","3ce07787":"### 7c. Create optimized random forest","578728b6":"The 'Fare' field captures the fare paid for each ticket. Since a ticket number can apply to an entire group of passengers, the fare paid applies to multiple people in many cases. In order to better capture the effect of fare paid at the passenger level, a 'FarePerPassenger' field is created as 'Fare' divided by the number of passengers corresponding to the applicable ticket number.\n\nIt appears that on a whole the distribution of fares paid per passenger is overall different for passengers who survive the shipwreck relative to passengers who do not survive the shipreck. However, when distributions are cut by passenger class, the differences in distributions of fares paid by passengers who survive and do not survive the shipwreck is less clear. This indicates that while fare paid per passenger could impact survivorship, fare paid per passenger may also be closely related to passenger class.","46d22026":"Important Terminology:\n* Parent Node: A node that is further divided into sub-nodes.\n* Child Node: A node that results from the division of a parent node.\n* Root Node: The very beginning of a decision tree that represents the entire population or sample. The root node is divided into two or more sets. The root node only has arrows pointing away from it meaning that it does not have a parent.\n* Internal Node \/ Node: A node within a decision tree that is beyond the root node and before the leaf nodes. Internal nodes have arrows pointing towards them and arrows pointing away from them. Internal nodes carry at least one child. \n* Leaf Node \/ Terminal Node: Nodes that do not split.  \n* Branches: Segments of the decision tree that connect the nodes.\n* Splitting: Process of dividing a node into two or more sub-nodes.\n* Pruning: Removing sub-nodes of a decision model. The opposite process of splitting.\n\nDecision trees involve stratifying or segmenting the predictor space into a number of simple regions. A decision tree begins with all data in the population \/ sample at the root node. All data in the population\/sample is split based on criteria (e.g. whether the passenger on the Titanic is male or female). Following the initial split, the algorithm performs subsequent splits (called internal nodes) until the algorithm arrives at points (at leaf nodes) where the algorithm cannot perform additional splits. When the algorithm arrives at a leaf node, the algorithm classifies the observation as the mode classification of the training observations that fall in the region (so if 9 total passengers in the training dataset fall into a leaf node and 5 of whom survive the shipwreck while 4 of whom do not survive the shipwreck then the algorithm classifies observations that fall into the leaf node as 'survived').\n\nThe decision tree algorithm itself creates splits based on the purity of the nodes directly after the split (most commonly using Gini index or entropy). Purity refers to the homogeneity of split observations based on the classification of interest. A node is 100% pure when all of its data belongs to a single class (so if a node contains 100% surviving passengers or 100% deceased passengers, the node is pure). A node becomes less pure when it contains observations corresponding to multiple classifications. A Gini Index of 0 means that a node is pure and a Gini Index of 1 means the elements within the node are randomly distributed across various classes. At each split, the algorithm compares the purity of the resulting nodes at all possible splits (so all possible independent variables to be used at the split and all possible values within each independent variable to be used at the split). The split that results in nodes that are overall most pure (using weighted average Gini index or entropy) is selected and used in the model. This process repeats for subsequent decision tree splits until the algorithm reaches a stopping point.","72c99948":"The vast majority of passengers leave from Cherbourg. It appears that passengers from some ports have higher survivorship than others.","7e513462":"### 2a. Investigate shape of dataset and whether or not there are any missing values","c62ef91e":"~20% of the age values are missing from both the training and testing datasets.\n\n**Options for handling missing values:**\n1. Remove the field entirely\n    - Not a good solution because 'Age' is likely a significant field\n    - During the Titanic Disaster, efforts were made to save children first so it is likely that age is an important field that helps determine survivorship. As a result, this field should not be removed\n2. Remove the observations that contain the missing values\n    - Not a good solution because this would mean removing a significant portion of the data\n    - Removing ~20% of the data, would likely adversely impact the model's predictive power because the model would have fewer examples of data points to train on\n    - Additionally, removing ~20% of the data would mean losing visibility on ~20% of the test observations meaning there would be little basis for predicting survivorship for ~20% of the test observations\n3. Replace the missing values with approximations of the values\n    - The best solution in this case\n\n**Strategy for replacing the missing age values:**\n\nThe age variable is right skewed so it is preferable to replace the missing value with a median value rather than an average value so that the approximation is not overestimated. Ideally, the age approximation should be based on a the age of a set of passengers similar to the passengers whose ages are unknown. In this case, it is a good solution to replace the missing ages with the median ages of passengers with the same ticket class, sex, and port of embarkation as the passengers whose ages are unknown. The histogram above shows the distribution of ages before and after replacements are made. This histogram shows that the distribution of ages is largely preserved after these replacements are made.","2c88944e":"### 4a. Isolate the Effect of Age","ec4a57aa":"The performance metrics used to compare classification the logistic regression, decision tree, and random forest models are:\n\n* Accuracy: (TP + TN) \/ (TP + TN + FP + FN); The proportion of correctly predicted classifications from the overall number of cases\n* Precision: (TP \/ (TP +FP)); True positives divided by predicted positives; Proportion of correct positive classifications from cases that are predicted as positive\n* Recall: (TP \/ (TP + FN)); True positives divided by actual positives; Measures the proportion of correct positive classifications from cases that are actually positive\n* F1 Score: The harmonic mean of precision and recall","f3758afb":"Two passengers have missing port of embarkation values in the training dataset.\n\n**Options for handling missing values:**\n1. Remove the field entirely\n    - Not a good solution because 'Embarked' is a usable field\n    - The vast majority of observations for the 'Embarked' field are present, making 'Embarked' a very usable field that should not be removed\n2. Remove the observations that contain the missing values\n    - A valid solution, but not the best solution because it is important to have as many usable training observations as possible\n    - Since the training dataset is already relatively small, observations should not be removed unless it is absolutely necessary\n3. Replace the missing values with approximations of the values\n    - The best solution in this case\n\n**Strategy for replacing the missing port of embarkation values:**\n\nBoth of the passengers with missing port of embarkation values have the same ticket number and cabin number. Both passengers are female first class passengers who paid the same fare. Both women reside on cabin floor B. No other passengers on the ship have the same ticket number as the two women with missing port of embarkation values, but several reside on the same cabin floor. \n\nFirst class women residing on cabin floor B who embarked from 'Cherbourg' generally paid fairs most similar to those of the two women with missing port of embarkation values. Therefore, it is best to replace the null port of embarkation values with 'C' for 'Cherbourg'.","c2049984":"### 3g. Explore Port of Embarkation","cb95716e":"### 4b. Resolve Parents\/Children on Board and Siblings\/Spouses on Board Variables","ccca6569":"~77% of the cabin number values are missing from both the training and testing datasets.\n\n**Options for handling missing values:**\n1. Remove the field entirely\n    - The best solution in this case\n2. Remove the observations that contain the missing values\n    - Not a good solution because this would mean removing a significant portion of the data\n    - Removing ~77% of the data, would adversely impact the model's predictive power because the model would have significantly fewer examples to train on\n    - Removing ~77% of the data would also mean losing visibility on ~77% of the test observations meaning there would be little basis for predicting Survivorship for the removed test observations\n3. Replace the missing values with approximations of the values\n    - Not a good solution because only ~23% of the observations have cabin numbers\n    - Given that very few observations have cabin numbers recorded, it would be very difficult to reaonably approximate the cabin numbers for the remaining ~77% of the data","7329d738":"A density plot of passenger fares shows a right skewed variable with some passengers paying more than $100 for tickets. Upon closer investigation, it is clear that passenger fare corresponds to the ticket number not to the individual passenger. So if passengers purchase tickets in a group, the fare applies to the fees paid by the entire group and not to the individual passengers. This issue is resolved in the feature engineering section of this notebook.","50d4e228":"Note that when dummy variables are created for the nominal categorical variables, a column is not deleted for the 'Embarked' data. Multicollinearity is no longer a concern for the Decision Tree Model because decision trees are not linear models. Additionally, scaling the fare paid per passenger is not necessary for decision trees.","9b56f429":"In the above graphs, density curves are used to show the different distributions of ages for passengers who survive the shipwreck and passengers who do not survive the shipwreck. The goal is to see whether or not the distribution of ages of the surviving population is different than the distribution of ages of the deceased population. If the distributions are different, it could mean that age plays a role in indicating a passenger's probability of surviving the shipwreck.\n\nThe first graph (titled 'Density Plot of Age for Surviving Population and Deceased Population (Including Replaced Age Values)') compares the distribution of ages for passengers who survive the shipwreck relative to passengers who do not survive the shipwreck. Note that in the first graph, the age distribution for deceased passengers and surviving passengers differs in two ways:\n\n    1. A higher proportion of surviving individuals are between the ages of ~0 and ~18\n    2. A higher proportion of deceased passengers are between the ages of ~18 and ~35\n    \nThe second difference is due to the methodology used to replace null ages. Null ages are replaced by median values grouped by different factors. Since passengers with missing age values tend to disproportionately exist in the deceased group, when missing values are replaced it slightly throws off the distribution of ages for deceased passengers. When the observations with missing ages are ignored, the deceased and surviving groups have similar distributions of ages apart from the ~0 - ~18 age group.\n\nThe key takeaway is that it appears that children are more likely to survive the shipwreck than adults. ","582f4c96":"<a id=\"t7.\"><\/a>\n# 7. Random Forest","e5b016cd":"### 6d. Create optimized decision tree","474c1e3c":"Random forests combine the simplicity of decision trees with flexibility, which generally results in a vast improvement in accuracy. Random forests are built from decision trees. \n\nInitially, the random forest algorithm takes a random sample with replacement from the dataset of the same size as the original dataset. This is called a bootstrapped dataset. When building these decision trees, each time a split in a tree is considered, a random sample of m predictors is chosen as split candidates from the full set of p predictors. The split is only allowed to use the m predictors. A fresh sample of m predictors is taken at each split. The number of predictors considered at each split is approximately equal to the square root of the total number of predictors. After the decision tree is created, the algorithm starts the steps over again (create a new bootstrapped dataset and create a decision tree using a subset of m features at each step in the decision tree), creating multiple trees. \n\nWhen new observations are fed to the random forest, the algorithm predicts each observation's classification using each individual tree in the forest. The most occuring prediction from all the trees in the forest for each observation is used as the predicted classification for each observation.\n\nIn building a random forest, the algorithm is not allowed to consider a majority of the available predictors. This ensures that the trees used in random forest are less correlated meaning that taking the average of the resulting trees is less variable and more reliable. The algorithm ultimately creates a wide variety of trees, which makes it more effective than individual decision trees.","2b50948a":"### 3c. Explore Age","479a5a1e":"The vast majority of passengers in the training dataset do not have a parent or child on board the Titanic.\n\nThere seems to generally be higher survivorship for passengers traveling with one or more parents or children relative to passengers traveling without any parents or children.","6bbd7422":"<a id=\"t3.\"><\/a>\n# 3. Explore Variables and their Impact on Survival","80c415d2":"### 3f. Explore Fare","d2b0e4c4":"### 3b. Expore Gender","6075e5e3":"# **Contents**\n1. [Import Data and Python Packages](#t1.)\n2. [Perform Initial Observations and Deal with Missing Values](#t2.)\n3. [Explore Variables and their Impact on Survival](#t3.)\n4. [Feature Engineering](#t4.)\n5. [Logistic Regression](#t5.)\n6. [Decision Tree](#t6.)\n7. [Random Forest](#t7.)\n8. [Evaluate Performance of Models](#t8.)\n9. [Make Predictions](#t9.)","cfa5b8ad":"The vast majority of passengers in the training dataset do not have a sibling or spouse on board the Titanic.\n\nThere seems to generally be higher survivorship for passengers traveling with one or more siblings or spouses relative to passengers traveling without any siblings or spouses.","63a2b597":"Children are defined as being under 16 while adults are defined as being 16 or older. Children appear to be more likely to survive the shipwreck than adults. ","d8865461":"Note that when dummy variables are created for the nominal categorical variables, a column is not deleted for the 'Embarked' data. Multicollinearity is no longer a concern for Random Forest because Random Forest is not a linear model. Additionally, scaling the fare paid per passenger is not necessary for this model."}}