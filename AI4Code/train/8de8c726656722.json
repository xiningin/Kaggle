{"cell_type":{"3b972b92":"code","8d033a82":"code","1817beb2":"code","ee90a25b":"code","982ac9ed":"code","0b3db781":"code","4a1d4015":"code","41fea22a":"code","b14895a1":"code","37d5b13c":"code","af3eb78c":"code","00a34d8f":"code","3d021243":"code","e0e87ff9":"code","80a89c76":"code","c388328b":"code","128bed1e":"code","e9e01eb1":"code","2dbce489":"code","96d69641":"code","a9fac195":"code","d75f38ec":"code","5ba12738":"code","6b5082be":"code","4cdddd41":"code","4029388f":"code","a24abff1":"code","0c66350a":"code","81d4ce07":"code","7392a466":"code","0f04ec15":"code","c95baf89":"code","75a323a9":"code","87dc62cf":"code","69bf4be5":"code","95e46e50":"code","dba609f3":"code","d6b02d54":"code","40f539c6":"code","fa3b1c32":"code","9fc000e2":"code","fc788559":"code","9513e70b":"code","000548bd":"code","8829de54":"code","7ba3f4c8":"code","d795b30f":"code","cda94a42":"code","cdfbe699":"code","6d2fc0bd":"code","d7eaadb5":"code","6d084539":"code","96cb33e0":"code","8cdc24e7":"code","16812737":"code","36a7659c":"code","c4c97ff7":"code","a190f6e8":"code","2e611b6e":"code","f394a643":"code","d14d11e8":"code","69e60be7":"code","31c42ab6":"code","330e5c5e":"code","0af616e3":"code","e45a5e12":"code","128a0614":"code","89690b75":"code","6cace69a":"code","5738c474":"code","977bf62c":"code","db5e3f70":"code","032f4d3d":"code","a4f7a8aa":"code","b01df6cf":"code","ad88dbdf":"code","adf1b21c":"markdown","6ef19f54":"markdown","4bf8e9b2":"markdown","3c161cd0":"markdown","85d07bb3":"markdown","13ccf71a":"markdown","c9f95adb":"markdown","35a4dc6b":"markdown","489557ee":"markdown","1bcd421f":"markdown","7ef76311":"markdown","64b68004":"markdown","d19b7291":"markdown","7487cd5a":"markdown","5734eacf":"markdown","d830bcf6":"markdown","555e9a2b":"markdown","b19d05bc":"markdown","7619c16c":"markdown","2aa819e5":"markdown","63598ee6":"markdown","dec3da98":"markdown","ca2391f9":"markdown","66e173ba":"markdown","b332400f":"markdown","8cbe179a":"markdown","74e24d62":"markdown","605324be":"markdown","f04ed2ba":"markdown","a8b07520":"markdown","1f9c4c67":"markdown","797a8e0f":"markdown","78ffd66d":"markdown","c1b4657c":"markdown","8f3ed5b5":"markdown","f91744d8":"markdown","66f0a53c":"markdown","8a4327a2":"markdown","a311d60b":"markdown","78b4fcc6":"markdown","e021e6bc":"markdown","d4d4b3f8":"markdown","5494c583":"markdown","fade4f36":"markdown","fe788624":"markdown"},"source":{"3b972b92":"import pandas as pd\nimport random\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import rcParams\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-muted')  #mudar estilo? plt.style.available\nimport re\n#mudar estilo? plt.style.available \n\ndef _shuffle(list):\n  c = plt.style.available\n  random.shuffle(list)\n  return list[0]\n\ndef substrings_in_string(big_string, substrings):\n    for substring in substrings:\n        if big_string.find(substring) != -1:\n            return substring\n    return np.nan\n\n","8d033a82":"dftrain = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndftest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nprint(\"test :\\t\",dftest.shape)\nprint(\"train:\\t\",dftrain.shape)\n#print(\"temp:\\t\",df.shape)","1817beb2":"print('Train')\ndftrain.info()","ee90a25b":"print('Test')\ndftest.info()","982ac9ed":"describe = dftrain.describe()\ndescribe.index = describe.index.map(str.upper)\ndescribe","0b3db781":"dftrain.head()","4a1d4015":"dfnulls = dftrain.isnull().sum()\ndfnulls[dfnulls.values>0]","41fea22a":"values = dftrain.Survived.value_counts()\nplt.clf()\nfig,ax=plt.subplots(1,0,figsize=(10,3))\nvalues.plot.pie(shadow=True,startangle=180,explode=[0,0.1],autopct='%1.2f%%')\nplt.show()","b14895a1":"dftrain[['Survived','Pclass']].groupby(['Survived','Pclass']).apply(lambda c: c.count())","37d5b13c":"classes = ['1\u00ba','2\u00ba','3\u00ba']\ndie =  dftrain[dftrain.Survived==0].Pclass.value_counts()\nsurvived =  dftrain[dftrain.Survived==1].Pclass.value_counts()\nfig,ax = plt.subplots(1,2,figsize=(6,5))\nsurvived.plot.pie(ax=ax[0],autopct='%1.1f%%',explode=[0.3,0.05,0.05],shadow=True)\nax[0].set_title('Survived')\nax[0].set_xlabel(classes)\nax[0].grid(True)\n\ndie.plot.pie(ax=ax[1],autopct='%1.1f%%',explode=[0.3,0.05,0.05],shadow=True)\nax[1].set_title('Die')\nax[1].set_ylabel('')\nax[1].grid(True)\n\nplt.show()","af3eb78c":"cats = ['Pclass','Sex','SibSp','Parch','Embarked']\ndftrain[cats].head()","00a34d8f":"fig,ax=plt.subplots(1,len(cats),figsize=(25,2))\ncolors =  ['blue','green','gray','gold','yellow', 'orange', 'red','purple','indigo','violet']\nsurvived = dftrain[dftrain.Survived==1].Survived.value_counts()\nfor i,c in enumerate(cats):\n  v = dftrain[c].value_counts() \n  ax[i].bar(v.index,v,color =_shuffle(colors),edgecolor='black')\n  ax[i].set_title(c.upper())\n  ax[i].set_ylabel('')\n  ax[i].grid(True)\nplt.show()","3d021243":"dftrain['Name'].head()","e0e87ff9":"#title_list=['Mrs', 'Mr', 'Master', 'Miss', 'Major', 'Rev','Dr', 'Ms', 'Mlle','Col', 'Capt', 'Mme', 'Countess','Don', 'Jonkheer']\n#tempo join\ndftrain['fork']=1\ndftest['fork']=2\ndftest['Survived'] = 2\ndftrain = pd.concat([dftrain, dftest], ignore_index=True, sort=False)\n\ndftrain['Title']=dftrain.Name.str.extract('([A-Za-z]+)\\.')\ndftrain['Title'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don'],['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr'],inplace=True)\n\n#dftrain['Title']=dftrain['Name'].map(lambda x: substrings_in_string(x, title_list))\n#dftrain['Title']=dftrain.apply(replace_titles, axis=1)\ndftrain.Title = dftrain.Title.str.strip()\ndftrain.Title = dftrain.Title.str.upper()\n\ndftrain.Title.unique()\n","80a89c76":"from wordcloud import WordCloud, STOPWORDS\nstopwords = set(STOPWORDS) \nnames = ' a '.join(dftrain.Title.values)\nwordcloud2 = WordCloud(background_color='white', stopwords=stopwords,min_font_size=10).generate(names)\nplt.figure(figsize = (20, 4), facecolor=None) \nplt.imshow(wordcloud2)\nplt.axis(\"off\")\nplt.tight_layout(pad = 0)\nplt.show()","c388328b":"print(len(wordcloud2.words_))\nwordcloud2.words_","128bed1e":"titles = dftrain.Title.value_counts()\nfig,ax = plt.subplots(1,1,figsize=(6,5))\ntitles.plot.pie(ax=ax,autopct='%1.1f%%',shadow=True,explode=[0.3,0.1,0.1,0.1,0.1,.1])\nax.grid(True)\nplt.show()","e9e01eb1":"chance = dftrain.groupby(\"Title\")[\"Survived\"].mean()\nfig,ax = plt.subplots(1,1,figsize=(6,5))\nchance.plot.pie(ax=ax,autopct='%1.1f%%',shadow=True,explode=[0.01,0.05,0.05,0.2,0.1,0.1])\nax.grid(True)\nplt.show()","2dbce489":"print('Survivors vs Title')\npd.crosstab(dftrain.Survived, dftrain.Title)","96d69641":"dftrain['Cabin'].fillna('Unknown',inplace=True)\ndftrain['Cabin'].head()","a9fac195":"cabin_list = ['A', 'B', 'C', 'D', 'E', 'F', 'T', 'G', 'Unknown']\ndftrain['Deck']=dftrain['Cabin'].map(lambda x: substrings_in_string(x, cabin_list))","d75f38ec":"deck = dftrain[(dftrain['fork'] ==1)].Deck.value_counts()\nfig,ax = plt.subplots(1,1,figsize=(10,3))\ndeck.plot.bar(ax=ax)#,shadow=True,explode=[0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1])\nax.grid(True)\nplt.show()\n","5ba12738":"chance = dftrain[(dftrain['fork'] ==1)].groupby(\"Deck\")[\"Survived\"].mean()\nfig,ax = plt.subplots(1,1,figsize=(6,3))\nchance.plot.bar(ax=ax)\nax.grid(True)\nplt.show()","6b5082be":"print('Survivors vs Deck')\npd.crosstab(dftrain[(dftrain['fork'] ==1)].Survived, dftrain[(dftrain['fork'] ==1)].Deck)","4cdddd41":"dftrain.Embarked.fillna('S',inplace=True)\nprint('Survivors vs Embarked')\npd.crosstab(dftrain[(dftrain['fork'] ==1)].Survived, dftrain[(dftrain['fork'] ==1)].Embarked)","4029388f":"embarked = dftrain[(dftrain['fork'] ==1)].Embarked.value_counts()\nfig,ax = plt.subplots(1,1,figsize=(6,5))\nembarked.plot.pie(ax=ax,autopct='%1.1f%%',shadow=True,explode=[0.2,0.1,0.1])\nax.grid(True)\nplt.show()","a24abff1":"chance = dftrain[(dftrain['fork'] ==1)].groupby(\"Embarked\")[\"Survived\"].mean()\nfig,ax = plt.subplots(1,1,figsize=(6,5))\nchance.plot.pie(ax=ax,autopct='%1.1f%%',shadow=True,explode=[0.3,0.05,0.05])\nax.grid(True)\nplt.show()","0c66350a":"dftrain.SibSp.isna().sum()\nprint('Survivors vs SibSp')\npd.crosstab(dftrain[(dftrain['fork'] ==1)].Survived, dftrain[(dftrain['fork'] ==1)].SibSp)","81d4ce07":"sibSp = dftrain.SibSp[(dftrain['fork'] ==1)].value_counts()\nfig,ax = plt.subplots(1,1,figsize=(10,3))\nsibSp.plot.bar(ax=ax)\nax.grid(True)\nplt.show()","7392a466":"\nchance = dftrain[(dftrain['fork'] ==1)].groupby(\"SibSp\")[\"Survived\"].mean()\nfig,ax = plt.subplots(1,1,figsize=(10,4))\nchance.plot.bar(ax=ax,color='g')\nax.axvline(x=chance.mean()*10,linewidth=5,color='r' )\nax.grid(True)\nplt.show()","0f04ec15":"dftrain.Parch.isna().sum()\nprint('Survivors vs Parch')\npd.crosstab(dftrain[(dftrain['fork'] ==1)].Survived, dftrain[(dftrain['fork'] ==1)].Parch)","c95baf89":"chance = dftrain[(dftrain['fork'] ==1)].groupby(\"Parch\")[\"Survived\"].mean()\nfig,ax = plt.subplots(1,1,figsize=(10,4))\nchance.plot.bar(ax=ax,color='g')\nax.axvline(x=chance.mean()*11,linewidth=5,color='r' )\nax.grid(True)\nplt.show()","75a323a9":"dftrain.Pclass.isna().sum()\nprint('Survivors vs Pclass')\npd.crosstab(dftrain[(dftrain['fork'] ==1)].Survived, dftrain[(dftrain['fork'] ==1)].Pclass)","87dc62cf":"pclass = dftrain[(dftrain['fork'] ==1)].Pclass.value_counts()\nfig,ax = plt.subplots(1,1,figsize=(6,5))\npclass.plot.pie(ax=ax,autopct='%1.1f%%',shadow=True,explode=[0.2,0.1,0.1])\nax.grid(True)\nplt.show()","69bf4be5":"chance = dftrain[(dftrain['fork'] ==1)].groupby(\"Pclass\")[\"Survived\"].mean()\nfig,ax = plt.subplots(1,1,figsize=(6,5))\nchance.plot.pie(ax=ax,autopct='%1.1f%%',shadow=True,explode=[0.2,0.05,0.05])\nax.grid(True)\nplt.show()","95e46e50":"cats = ['Age','Fare']\nfig,ax=plt.subplots(1,len(cats),figsize=(25,2))\ncolors =  ['blue','green','gray','gold','yellow', 'orange', 'red','purple','indigo','violet']\nsurvived = dftrain[dftrain.Survived==1 & (dftrain['fork'] ==1)].Survived.value_counts()\nfor i,c in enumerate(cats):\n  v = dftrain[c].value_counts() \n  ax[i].bar(v.index,v,color =_shuffle(colors),edgecolor='black')\n  ax[i].set_title(c.upper())\n  ax[i].set_ylabel('')\n  ax[i].grid(True)\nplt.show()","dba609f3":"dftrain[(dftrain['fork'] ==1)].Fare.describe()\n","d6b02d54":"plt.figure(figsize = (9,3))\nplt.hist(dftrain[(dftrain['fork'] ==1)][\"Fare\"], bins = 40)\nplt.xlabel(\"Fare\")\nplt.ylabel(\"Frequency\")\nplt.show()","40f539c6":"dftrain.Age.isnull().sum()","fa3b1c32":"dftrain.Age[(dftrain['fork'] ==1)].describe()","9fc000e2":"plt.figure(figsize = (9,3))\nplt.hist(dftrain[(dftrain['fork'] ==1)][\"Age\"], bins = 40)\nplt.xlabel(\"Age\")\nplt.ylabel(\"Frequency\")\nplt.show()","fc788559":"dftrain.Age.isna().sum()","9513e70b":"cols = dftrain.Title.value_counts().index\nfor c in  cols:\n  print(len(dftrain[(dftrain.Age.isnull()) & (dftrain.Title==c)]))\n  print(dftrain[(dftrain.Title==c)].Age.median(),dftrain[(dftrain.Title==c)].Age.mean())\n  dftrain.loc[(dftrain.Age.isnull()) & (dftrain.Title==c),\"Age\"] = dftrain[(dftrain.Title==c)].Age.mean() ","000548bd":"dftrain[\"Age_Range\"] = pd.cut(dftrain.Age, [0,15,25,35,45,55,65,81],  right=False)\ndftrain[\"Age_Range\"].head()","8829de54":"pd.crosstab(dftrain[(dftrain['fork'] ==1)].Age_Range, dftrain[(dftrain['fork'] ==1)].Survived).style.background_gradient(cmap='gray')\n","7ba3f4c8":"chance = dftrain[(dftrain['fork'] ==1)].groupby(\"Age_Range\")[\"Survived\"].mean()\nfig,ax = plt.subplots(1,1,figsize=(6,5))\nchance.plot.pie(ax=ax,autopct='%1.1f%%',shadow=True,explode=[0.3,0.1,0.1,0.1,0.1,0.1,0.1])\nax.grid(True)\nplt.show()","d795b30f":"dftrain[(dftrain['fork'] ==1)]['Age_Range'].value_counts().to_frame().style.background_gradient(cmap='gray')","cda94a42":"sns.factorplot('Age_Range','Survived',data=dftrain[(dftrain['fork'] ==1)],col='Pclass')\nplt.show()","cdfbe699":"dftrain.Age = dftrain.Age_Range\ndftrain.drop('Age_Range', axis=1, inplace=True)\ndftrain.head(1)","6d2fc0bd":"f,ax=plt.subplots(2,2,figsize=(20,5))\nsns.countplot('Embarked',data=dftrain[(dftrain['fork'] ==1)],ax=ax[0,0])\nax[0,0].set_title('No. Of Passengers Boarded')\nsns.countplot('Embarked',hue='Sex',data=dftrain[(dftrain['fork'] ==1)],ax=ax[0,1])\nax[0,1].set_title('Male-Female Split for Embarked')\nsns.countplot('Embarked',hue='Survived',data=dftrain[(dftrain['fork'] ==1)],ax=ax[1,0])\nax[1,0].set_title('Embarked vs Survived')\nsns.countplot('Embarked',hue='Pclass',data=dftrain[(dftrain['fork'] ==1)],ax=ax[1,1])\nax[1,1].set_title('Embarked vs Pclass')\nplt.subplots_adjust(wspace=0.2,hspace=0.5)\nplt.show()","d7eaadb5":"#https:\/\/triangleinequality.wordpress.com\/2013\/09\/08\/basic-feature-engineering-with-the-titanic-data\/\ndftrain['Family_Size']=dftrain['SibSp']+dftrain['Parch']\ndftrain.head(2)","6d084539":"dftrain.drop('Cabin', axis=1, inplace=True)","96cb33e0":"dftrain.loc[dftrain['Family_Size'] == 0, 'Family_Size'] = 1\n#dftrain[dftrain['Family_Size']==0]['Family_Size'] =1\ndftrain['Family_Size'].value_counts()","8cdc24e7":"dftrain.drop('Name', axis=1, inplace=True)","16812737":"dftrain['Fare_Per_Person']=dftrain['Fare']\/(dftrain['Family_Size']+1)\ndftrain['Fare_Per_Person'].value_counts()","36a7659c":"dftrain['Fare_Person_Range']=pd.qcut(dftrain['Fare_Per_Person'],5)\ndftrain['Fare_Person_Range'].value_counts()","c4c97ff7":"dftrain.drop('Fare_Per_Person', axis=1, inplace=True)","a190f6e8":"dftrain['Fare_Range']=pd.qcut(dftrain['Fare'],5)\ndftrain[(dftrain['fork'] ==1)]['Fare_Range'].value_counts()","2e611b6e":"dftrain.drop('Fare', axis=1, inplace=True)","f394a643":"dftrain[\"Fare\"] = dftrain.Fare_Range\ndftrain.drop('Fare_Range', axis=1, inplace=True)\n","d14d11e8":"dftrain['Sex'].replace(['male','female'],[0,1],inplace=True)\ndftrain['Embarked'].replace(['S','C','Q'],[0,1,2],inplace=True)\ndftrain['Title'].replace(['MR', 'MRS', 'MISS', 'MASTER'],[0,1,2,3],inplace=True)\ndftrain['Deck'].replace(['Unknown', 'C', 'E', 'G', 'D', 'A', 'B', 'F', 'T'],[0,1,2,3,4,5,6,7,8],inplace=True)","69e60be7":"dftrain.drop(['Ticket','PassengerId'],axis=1,inplace=True)","31c42ab6":"dftrain.head(3)","330e5c5e":"plt.gcf()\nc = dftrain[(dftrain['fork'] ==1)].copy()\nc.drop('fork', axis=1, inplace=True)\nsns.heatmap(c.corr(),annot=True,linewidths=2.2,annot_kws={'size':12})\nfig=plt.gcf()\nfig.set_size_inches(18,6)\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\nplt.show()","0af616e3":"dftrain.Pclass = dftrain.Pclass.astype(\"category\")\ndftrain.Sex = dftrain.Sex.astype(\"category\")\ndftrain.Age = dftrain.Age.astype(\"category\")\ndftrain.SibSp = dftrain.SibSp.astype(\"category\")\ndftrain.Parch = dftrain.Parch.astype(\"category\")\ndftrain.Embarked = dftrain.Embarked.astype(\"category\")\ndftrain.Title = dftrain.Title.astype(\"category\")\ndftrain.Fare = dftrain.Fare.astype(\"category\")\ndftrain.Deck = dftrain.Deck.astype(\"category\")\ndftrain.Family_Size = dftrain.Family_Size.astype(\"category\")","e45a5e12":"dftrain = pd.get_dummies(dftrain, columns= ['Family_Size'])\ndftrain.head()","128a0614":"dftrain = pd.get_dummies(dftrain, columns= ['Pclass','Sex','Age','SibSp','Parch','Embarked','Title','Fare','Deck'])\ndftrain.head()","89690b75":"dftrain.drop(['Fare_Person_Range'],axis=1,inplace=True)\ndftrain.head()","6cace69a":"test = dftrain[(dftrain['fork'] ==2)].copy()\ntrain = dftrain[(dftrain['fork'] ==1)].copy()\ntrain.drop(['fork'],axis=1,inplace=True)\ntest.drop(['fork'],axis=1,inplace=True)\ntest.drop(['Survived'],axis=1,inplace=True)\n\nx = train.drop(labels = \"Survived\", axis = 1).values\ny = train.Survived.values\n\nx.shape,test.shape,train.shape\n","5738c474":"import sklearn\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score,accuracy_score\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\nfrom xgboost import XGBClassifier\n\n#xgb = XGBClassifier(eta =0.0851,max_depth=10,gamma=0.6,alpha=0.5,scale_pos_weight =5,eval_metric='rmse')\nxgb = XGBClassifier(n_estimators=360, max_depth=200, learning_rate=0.1)\nscaler = MinMaxScaler()\nxscaled = scaler.fit_transform(x)\ntestscaled = scaler.transform(test.values)\nxtrain, xtest, ytrain, ytest = train_test_split(xscaled,y, test_size=0.35,stratify=y)\n","977bf62c":"xgb.fit(xtrain,ytrain)\nacc_train = round(xgb.score(xtrain, ytrain)*100,2) \nacc_test = round(xgb.score(xtest,ytest)*100,2)\nprint(\"Train:\\t{}% \".format(acc_train))\nprint(\"Test :\\t{}% \".format(acc_test))","db5e3f70":"predictions = xgb.predict(xtest)","032f4d3d":"print('precision_score:\\t',precision_score(predictions,ytest, average='macro'))","a4f7a8aa":"print('accuracy_score:\\t',accuracy_score(predictions,ytest))","b01df6cf":"predictions.shape, dftest.shape","ad88dbdf":"predictions = xgb.predict(testscaled)\ntest_survived = pd.Series(predictions, name = \"Survived\").astype(int)\nsubmit = pd.concat([dftest.PassengerId, test_survived],axis = 1)\nsubmit.shape \nsubmit.Survived.value_counts().plot.pie()\nsubmit.to_csv(\"titanic.csv\", index = False)","adf1b21c":"![](https:\/\/c4l.net\/wp-content\/uploads\/2019\/08\/Facts-About-Learning-Differences-Icon-hover.png)\n**Statistical data:** \nIn this phase we will verify that we have a sense of how our data is distributed.\n\n* Knowing our data ...\n* Counting values\n* Number of unique values\n* Higher value (more frequent)\n* Frequency of your primary value\n* Mean, standard deviation, minimum and maximum values\n* Percentiles of your data: 25%, 50%, 75% by default","6ef19f54":"Librarys","4bf8e9b2":"#### **Fare**\nPassenger fare\t","3c161cd0":"#### **Correlation**","85d07bb3":"**First look at the data...**\n\n* What types do we have here?\n* How many records?\n* How many columns?\n\n![](https:\/\/pt-static.z-dn.net\/files\/dc1\/f17c85f85405d14cbf4bb2bd8bebc29f.jpg)\n","13ccf71a":"#### **Age**\nAdjusting and creating age range","c9f95adb":"**Data Cleaning & Correlation between the features**","35a4dc6b":"**Accuracy classification score.**\n\nIn multilabel classification, this function computes subset accuracy: the set of labels predicted for a sample must exactly match the corresponding set of labels in y_true.","489557ee":"How is the distribution of survived vs a pclass?\n\nThe PClass feature is a proxy for socioeconomic status (SES)\n\nDid the money factor influence who survived?","1bcd421f":"**Modeling**","7ef76311":"**the passengers?** \nFor the training set, we provide the result (also known as ground truth) for each passenger.\n\n\nYour model will be based on \"features\" such as gender and class of passengers.\n* We can also use the  **[feature engineering](https:\/\/triangleinequality.wordpress.com\/2013\/09\/08\/basic-feature-engineering-with-the-titanic-data\/)** \n","64b68004":"#### **Parch**\nof parents \/ children aboard the Titanic","d19b7291":"![](https:\/\/images.twinkl.co.uk\/tw1n\/image\/private\/t_create_thumb\/create\/library\/Spinner-with-Pencil-and-Paperclip---PlanIt-Y1-Addition-and-Subtraction-Home-Learning-Tasks---KS1.png)\n\nBasic Feature Engineering with the Titanic Data\n\n\nFirst up the Name column is currently not being used, but we can at least extract the title from the name. There are quite a few titles going around, but I want to reduce them all to Mrs, Miss, Mr and Master.  To do this we\u2019ll need a function that searches for substrings. Thankfully the library \u2018strings\u2019 has just what we need.\nhttps:\/\/triangleinequality.wordpress.com\/2013\/09\/08\/basic-feature-engineering-with-the-titanic-data\/","7487cd5a":"#### Submission","5734eacf":"#### **Cabin**\nThis is going be very similar, we have a \u2018Cabin\u2019 column not doing much, only 1st class passengers have cabins, the rest are \u2018Unknown\u2019. A cabin number looks like \u2018C123\u2019. The letter refers to the deck, and so we\u2019re going to extract these just like the titles.\n\n\n**Turning cabin number into Deck**","d830bcf6":"apparently with 1 or 2 (siblings \/ spouses) the chance of surviving is greater.","555e9a2b":"**Note** \nPClass: A proxy for socioeconomic status (SES)\n>1st = Superior\n>2nd = Average\n>3rd = Low\n\nSibsp: The data set defines family relationships in this way ...\n\nBrothers = brother, sister, half-brother, half-sister\n\nSpouse = husband, wife (lovers and grooms were ignored)\n\nParch: The data set defines family relationships in this way ...\n\nParents = mother, father\n\nChild = daughter, son, stepdaughter,\n\nstepson Some children traveled only with the nanny, so parch = 0 for them.\n\nEmbarked:\n>C = Cherbourg\n>Q = Queenstown\n>S = Southampton","b19d05bc":"Load & Check","7619c16c":"It is evident that only 61.62% survived the accident; something curious is to see that of the only passengers that did not survive, most of them were passengers in the 3rd class.\n\n**What did I do in the Problem definition step?**\n\n![](https:\/\/images.twinkl.co.uk\/tw1n\/image\/private\/t_create_thumb\/create\/library\/Parent-and-Child-Doing-math-Activity---Thank-You-2020-Cards-Home-Learning-Classic-KS1.png)\n__\n\nI basically read the Data Description to get some insights; and I did, later on we will see other features of our datasets.\n\nThe columns that drew my attention to the Data description: Sex, Name, Age\n\n\n","2aa819e5":"*Okay, but the question that doesn't stop is:*\n\nAfter all, how is the distribution of people who survived?\n\n> **Survived**:\n> * **0** = No\n> * **1** = Yes\n","63598ee6":"* Are there columns with null values?\n* Which columns?\n![](https:\/\/cdn.iconscout.com\/icon\/premium\/png-128-thumb\/inference-1428533-1210761.png)","dec3da98":"Age, Cabin and Embarked features need to be adjusted.","ca2391f9":"#### **PClass**\nsocioeconomic status","66e173ba":"We noticed something very interesting here, 15 passengers have a fare 0","b332400f":"**Discovery types**\nIn this first moment, we will identify and treat our features\n\n","8cbe179a":"Creating new family_size column","74e24d62":"#### **XGBoost**\nIf things don\u2019t go your way in predictive modeling, use XGboost.  \n\nXGBoost algorithm has become the ultimate weapon of many data scientist. \n\nIt\u2019s a highly sophisticated algorithm, powerful enough to deal with all sorts of irregularities of data.","605324be":"**Data Analysis**","f04ed2ba":"**Numeric *(also known as Quantitative)*They are data with a sense of measurement involved. Their types can be continuous or discrete.\n\n> * **Discreet:** Accounting data; they assume a fixed number of distinct values.\n>> Example: SibSip, this feature tells us if the passenger is alone or in a family.\n>>> * sibsp: The dataset defines family relations in this way ...\n>>> * Sibling = brother, sister, stepbrother, stepsister\n>>> * Spouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\n> * **Continuous:** Data that can have an infinite number of values within a classification.\n>> * Example: Fare Passenger fare\n","a8b07520":"#### **SibSp**\nof siblings \/ spouses aboard the Titanic","1f9c4c67":"Chance to survive for each deck","797a8e0f":"#### **Embarked**","78ffd66d":"Chance to survive for each title","c1b4657c":"#### **Compute the precision**\nThe precision is the ratio tp \/ (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.\n\nThe best value is 1 and the worst value is 0.","8f3ed5b5":"#### **Age**\nAge in years\n\nage: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5","f91744d8":"![](https:\/\/learningboosters.com.au\/wp-content\/uploads\/2019\/05\/hobarts-best-tutoring-service-learning-boosters-are-knowledgeable-experienced.png)\n\n**Categorical** *(also known as Qualitative)*\n\nThis type of data represents the characteristics of an object; A variable that describes categorical data is called a categorical variable. These types of variables can have one of a limited number of values.\n\nThis type of variable is easy to understand.\n\nThere are some types of categorical variables.\n\n**Binary**: Being able to assume exactly two values, it is also called the dichotomous variable.\n>Example: Sex\n\n**Politomics**: Which can assume more than two possible values.\n>Example: PClass, Embarked\n\n","66f0a53c":"Chance to survive for each  Embarked","8a4327a2":"It seems that children had a **better chance of survival**! hufa! :)","a311d60b":"Now that I have them, I recombine them to the four categories.","78b4fcc6":"#### **Embarked**\nPort of Embarkatio","e021e6bc":"### Titanic Disaster \nExploratory Data Analysis - Fundamentals \n\nEDA is usually the first stage for data mining. It allows us to visualize data to understand it, as well as create hypotheses for future analysis. \n\nThe exploratory analysis revolves around the creation of a \"**prelude**\" to data. Its purpose is to highlight the truth about the content with no bias. Its use is focused on understanding the modeling that can result in creating **hypotheses**.\n\n![](https:\/\/www.eyedocshoppe.com\/images\/T\/xctmp4IlEPS.png)\n\nBefore we get into the problem proposed by the challenge, I will compile about the Exploratory Data Analysis (EDA). \n\n![](https:\/\/img.icons8.com\/ios-filled\/2x\/learning.png)\n\nMain stages of EDA. Some of its main aspects:\n\n**Data requirements:**\nIt is important to understand what kind of data is needed for an adequate understanding of our problem; so that we can collect, select and store the minimum amount of data with noise. \n\n**Data Collection:**\n> The data collected must be stored in the correct format. \n\n**Data processing:**\n> Pre-processing involves processing the data set's preset prior to its actual analysis. \nThe most common tasks at this stage involve the correct export of the dataset, placing it in a more harmonious structure.\n\n**Data cleaning:**\n>In this phase, the verification of duplicates, missing values, identifying inaccuracies in the dataset and etc. are applied to the correct transformations.\n\n","d4d4b3f8":"*Problem:* \nBefore trying to extract insights, it is essential to understand the problem to be solved; in our case, it is, predicting the survivors on the Titanic with the fundamentals of ML.","5494c583":"#### **Name** Feature","fade4f36":"*thanks for the support, you guys are great!*\n\nReferencies\n* @ash316 - [EDA To Prediction(DieTanic)](https:\/\/www.kaggle.com\/ash316\/eda-to-prediction-dietanic)\n* @kanncaa1 - [DataiTeam Titanic EDA ](https:\/\/www.kaggle.com\/kanncaa1\/dataiteam-titanic-eda)\n* [Exploratory Data Analysis](https:\/\/towardsdatascience.com\/exploratory-data-analysis-8fc1cb20fd15)\n* [Why EDA is necessary](https:\/\/medium.com\/@srimalashish\/why-eda-is-necessary-for-machine-learning-233b6e4d5083)\n","fe788624":"**The survival rate decreases with increasing age, regardless of your economic class.**"}}