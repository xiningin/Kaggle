{"cell_type":{"283cfbdc":"code","1912fb67":"code","f21b2bb6":"code","780f61bb":"code","d59e1c17":"code","0147976c":"code","075c26e9":"code","492b7a29":"code","f9cbb6e3":"code","0b667c2a":"code","7057dbe7":"code","601d5b70":"code","67471d6a":"code","6e4f47b6":"code","0762d2ee":"code","0650659d":"code","ca84e9c5":"code","c4b7ac00":"code","73e5e5e1":"code","8ead194e":"code","233c32fc":"code","331ec3de":"code","f64b4687":"code","5445c09b":"code","15046486":"code","f33b82eb":"code","8f73b3cf":"code","f11863c6":"code","511a928b":"code","84be9ffc":"code","3061ef07":"code","72eff70e":"code","24bc2854":"code","088e940a":"code","19991168":"markdown","8a7a01a2":"markdown","17b8c873":"markdown","3b7ef4d3":"markdown","1f55996f":"markdown","17773e78":"markdown","9496f5c8":"markdown","fac8424f":"markdown","9f91ac49":"markdown","4d2d5748":"markdown","7699e0a1":"markdown","2840d2c5":"markdown","7959edf4":"markdown","1ddaa4a5":"markdown","0cd46fc6":"markdown"},"source":{"283cfbdc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1912fb67":"# Import all the required libraries and modules\nimport requests # library to handle requests\nimport pandas as pd # library for data analsysis\nimport numpy as np # library to handle data in a vectorized manner\nimport random # library for random number generation\n\n!conda install -c anaconda lxml --yes\n!conda install -c anaconda beautifulsoup4 --yes\nfrom bs4 import BeautifulSoup\n\n# libraries for displaying images\nfrom IPython.display import Image \nfrom IPython.core.display import HTML \n\n# library for chart plotting\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler # Module from Scikit-learn for Data normalization\nfrom sklearn.cluster import KMeans # Module from Scikit-learn for clustering\nfrom sklearn.datasets.samples_generator import make_blobs\n!wget --quiet https:\/\/cocl.us\/Geospatial_data\n!conda install -c conda-forge folium=0.5.0 --yes\nimport folium\n    \n# tranforming json file into a pandas dataframe library\nfrom pandas.io.json import json_normalize\n\nprint('Folium installed')\nprint('Libraries imported.')","f21b2bb6":"cgn_df = pd.read_html('https:\/\/en.wikipedia.org\/wiki\/Districts_of_Cologne')[1]\ncgn_df = cgn_df.drop([9,10])\ncgn_df.drop(['Map','Coat','City parts','District Councils','Town Hall'], axis=1, inplace =True)\ncgn_df.rename(columns={'City district':'District','Population1':'Population','Pop. density':'Pop_Density'}, inplace=True)\ncgn_df['District'].replace({'District 1 K\u00f6ln-Innenstadt':'K\u00f6ln-Innenstadt','District 2 K\u00f6ln-Rodenkirchen':'K\u00f6ln-Rodenkirchen','District 3 K\u00f6ln-Lindenthal':'K\u00f6ln-Lindenthal','District 4 K\u00f6ln-Ehrenfeld':'K\u00f6ln-Ehrenfeld','District 5 K\u00f6ln-Nippes':'K\u00f6ln-Nippes','District 6 K\u00f6ln-Chorweiler':'K\u00f6ln-Chorweiler','District 7 K\u00f6ln-Porz':'K\u00f6ln-Porz','District 8 K\u00f6ln-Kalk':'K\u00f6ln-Kalk','District 9 K\u00f6ln-M\u00fclheim':'K\u00f6ln-M\u00fclheim'}, inplace=True)\ncgn_df.sort_values('Pop_Density', inplace=True, ascending=False)\ncgn_df['Pop_Density'] = cgn_df['Pop_Density'].str.replace('\/km\u00b2','').str.replace('.','').astype(float)\ncgn_df","780f61bb":"labels = ['K\u00f6ln-Innenstadt','K\u00f6ln-Ehrenfeld','K\u00f6ln-Nippes','K\u00f6ln-Lindenthal','K\u00f6ln-Kalk','K\u00f6ln-M\u00fclheim','K\u00f6ln-Rodenkirchen','K\u00f6ln-Porz','K\u00f6ln-Chorweiler']\n\nind = np.arange(len(cgn_df['Pop_Density']))  \nwidth = 0.3\n\nfig, ax = plt.subplots(figsize=(16,8))\nrects = ax.bar(ind, cgn_df['Pop_Density'], width, label=labels, color='#5bc0de')\nax.set_title(\"Cologne Neighborhoods - Population Density Bar Plot\", fontsize=16)\nax.set_xticks(ind)\nax.set_xticklabels((labels))\nplt.ylabel('Population Density (Pop.\/km\u00b2)')\nplt.xlabel('Districts')\nax.get_yaxis().set_visible(True)\nax.spines['top'].set_visible(False)\nax.spines['left'].set_visible(True)\nax.spines['right'].set_visible(False)\n\n\n\ndef autolabel(rects, xpos='center'):\n\n    ha = {'center': 'center', 'right': 'left', 'left': 'right'}\n    offset = {'center': 0, 'right': 1, 'left': -1}\n\n    for rect in rects:\n        height = rect.get_height().round(2)\n        ax.annotate('{}'.format(height),\n                    xy=(rect.get_x() + rect.get_width() \/ 2, height),\n                    xytext=(offset[xpos]*3, 3),  # use 3 points offset\n                    textcoords=\"offset points\",  # in both directions\n                    ha=ha[xpos], va='bottom', fontsize=14)\n        \nautolabel(rects, \"center\")\n\nfig.tight_layout()\n\nplt.show()","d59e1c17":"cgn_df.drop(cgn_df.index[4:], axis=0, inplace=True)\ncgn_df","0147976c":"!conda install -c conda-forge geopy --yes \nfrom geopy.geocoders import Nominatim # module to convert an address into latitude and longitude values","075c26e9":"geolocator = Nominatim(user_agent=\"Cologne Explorer\")\ncgn_df['Districts_Coord'] = cgn_df['District'].apply(geolocator.geocode).apply(lambda x: (x.latitude, x.longitude))\ncgn_df[['Latitude', 'Longitude']] = cgn_df['Districts_Coord'].apply(pd.Series)\ncgn_df.drop(['Districts_Coord'], axis=1, inplace=True)\ncgn_df","492b7a29":"rent_df = pd.read_html('https:\/\/www.koeln.de\/immobilien\/mietspiegel.html')[0]\nrent_df.drop(['3. Quartal 2018','Ver\u00e4nderung'], axis=1, inplace=True)\nrent_df.rename(columns={'Stadtteil':'District','4. Quartal 2018\/04':'Price_m2_2018'}, inplace=True)\nrent_df.head()","f9cbb6e3":"neigh_list = ['Innenstadt','Ehrenfeld','Nippes','Lindenthal']\nrent_array = rent_df[rent_df.District.isin(neigh_list)]\nrent_df2 = pd.DataFrame(rent_array, columns=['District','Price_m2_2018'])\nrent_df2['Price_m2_2018'] = rent_df2['Price_m2_2018'].str.replace('\u20ac','').str.replace(',','.').astype(float)\nrent_df2.sort_values('Price_m2_2018', inplace=True, ascending=False)\nrent_df2","0b667c2a":"# No term \"Innestadt\" found on rent_df, therefore district was missing on rent_df2. Being the Innenstadt district mainly composed of the Altstadt-S\u00fcd und Altstadt-Nord, it was taken the average of both to be appended into the new dataframe\ninnenstadt = pd.DataFrame({'District':['Innenstadt'],'Price_m2_2018':[rent_df.iloc[0:2]['Price_m2_2018'].str.replace('\u20ac','').str.replace(',','.').astype(float).mean()]}, columns=['District','Price_m2_2018'])\nrent_df2 = rent_df2.append(innenstadt)\nrent_df2.sort_values('Price_m2_2018', inplace=True, ascending=False)\nrent_df2['District'].replace({'Innenstadt':'K\u00f6ln-Innenstadt','Rodenkirchen':'K\u00f6ln-Rodenkirchen','Lindenthal':'K\u00f6ln-Lindenthal','Ehrenfeld':'K\u00f6ln-Ehrenfeld','Nippes':'K\u00f6ln-Nippes','Chorweiler':'K\u00f6ln-Chorweiler','Porz':'K\u00f6ln-Porz','Kalk':'K\u00f6ln-Kalk','M\u00fclheim':'K\u00f6ln-M\u00fclheim'}, inplace=True)\nrent_df2","7057dbe7":"labels = ['K\u00f6ln-Lindenthal', 'K\u00f6ln-Ehrenfeld', 'K\u00f6ln-Nippes', 'K\u00f6ln-Innenstadt']\n\nind = np.arange(len(rent_df2['Price_m2_2018']))  \nwidth = 0.3\n\nfig, ax = plt.subplots(figsize=(16,8))\nrects = ax.bar(ind, rent_df2['Price_m2_2018'], width, label=labels, color='#5cb85c')\nax.set_title(\"Cologne Neighborhoods - Rental Price 2018\", fontsize=16)\nax.set_xticks(ind)\nax.set_xticklabels((labels))\nplt.ylabel('Rental Price')\nplt.xlabel('Districts')\nax.get_yaxis().set_visible(True)\nax.spines['top'].set_visible(False)\nax.spines['left'].set_visible(True)\nax.spines['right'].set_visible(False)\n\nautolabel(rects, \"center\")\n\nfig.tight_layout()\n\nplt.show()","601d5b70":"cgn_df.drop([cgn_df.index[1],cgn_df.index[3]], axis=0, inplace=True)\ncgn_df","67471d6a":"CLIENT_ID = '2QLU5CN20BM5KPMDHX2QYYBWG13KZAYXAYOIQX0S0300KW01' # your Foursquare ID\nCLIENT_SECRET = 'RND2YVJCGI10NSERVO4LCVGFYETQUROC3OCBD0MMK1XPFKKA' # your Foursquare Secret\nVERSION = '20200429'\nLIMIT = 200\nprint('Your credentials:')\nprint('CLIENT_ID: ' + CLIENT_ID)\nprint('CLIENT_SECRET:' + CLIENT_SECRET)","6e4f47b6":"query_1 = 'Bar'\nquery_2 = 'Restaurant'\nradius = 1250\nprint('Queries OK!')","0762d2ee":"url_inne1 = 'https:\/\/api.foursquare.com\/v2\/venues\/search?client_id={}&client_secret={}&ll={},{}&v={}&query={}&radius={}&limit={}'.format(CLIENT_ID, CLIENT_SECRET, cgn_df.iloc[0][4], cgn_df.iloc[0][5], VERSION, query_1, radius, LIMIT)\nurl_nipp1 = 'https:\/\/api.foursquare.com\/v2\/venues\/search?client_id={}&client_secret={}&ll={},{}&v={}&query={}&radius={}&limit={}'.format(CLIENT_ID, CLIENT_SECRET, cgn_df.iloc[1][4], cgn_df.iloc[1][5], VERSION, query_1, radius, LIMIT)\n\nurl_inne2 = 'https:\/\/api.foursquare.com\/v2\/venues\/search?client_id={}&client_secret={}&ll={},{}&v={}&query={}&radius={}&limit={}'.format(CLIENT_ID, CLIENT_SECRET, cgn_df.iloc[0][4], cgn_df.iloc[0][5], VERSION, query_2, radius, LIMIT)\nurl_nipp2 = 'https:\/\/api.foursquare.com\/v2\/venues\/search?client_id={}&client_secret={}&ll={},{}&v={}&query={}&radius={}&limit={}'.format(CLIENT_ID, CLIENT_SECRET, cgn_df.iloc[1][4], cgn_df.iloc[1][5], VERSION, query_2, radius, LIMIT)\n\n\nprint(\"URLs created!\")","0650659d":"results_inne1 = requests.get(url_inne1).json()\nresults_nipp1 = requests.get(url_nipp1).json()\n\nresults_inne2 = requests.get(url_inne2).json()\nresults_nipp2 = requests.get(url_nipp2).json()\n\nprint(\"GET Request done, results generated\")","ca84e9c5":"venues_inne1 = results_inne1['response']['venues']\nvenues_nipp1 = results_nipp1['response']['venues']\n\nvenues_inne2 = results_inne2['response']['venues']\nvenues_nipp2 = results_nipp2['response']['venues']\n\nprint(\"JSON File sorted out\")","c4b7ac00":"df_inne1 = pd.json_normalize(venues_inne1)\ndf_inne1['District'] = 'K\u00f6ln-Innenstadt'\ndf_inne1['Category'] = 'Bar'\ndf_inne1.head()","73e5e5e1":"df_nipp1 = json_normalize(venues_nipp1)\ndf_nipp1['District'] = 'K\u00f6ln-Nippes'\ndf_nipp1['Category'] = 'Bar'","8ead194e":"df_inne2 = json_normalize(venues_inne2)\ndf_inne2['District'] = 'K\u00f6ln-Innenstadt'\ndf_inne2['Category'] = 'Restaurant'","233c32fc":"df_nipp2 = json_normalize(venues_nipp2)\ndf_nipp2['District'] = 'K\u00f6ln-Nippes'\ndf_nipp2['Category'] = 'Restaurant'","331ec3de":"df_merged = pd.concat([df_inne1, df_inne2, df_nipp1, df_nipp2])\ndf_clean = df_merged[['name','Category','District','location.address','location.postalCode','location.lat','location.lng','location.distance']]\ndf_clean.rename({'name':'Venue','location.address':'Address','location.postalCode':'PostalCode','location.lat':'LocationLatitude','location.lng':'LocationLongitude','location.distance':'LocationDistance'}, axis=1, inplace=True)\ndf_clean.head()","f64b4687":"# create a Stamen Toner map of the world centered around Cologne, Germany\ncologne_map = folium.Map(location=[50.936631, 6.958401], zoom_start=10)\n\n# instantiate a feature group for the incidents in the dataframe\nlocations = folium.map.FeatureGroup()\n\n# loop through the 100 crimes and add each to the incidents feature group\nfor lat, lng, dist in zip(df_clean.LocationLatitude, df_clean.LocationLongitude, df_clean.District):\n    if dist == 'K\u00f6ln-Innenstadt':\n        locations.add_child(\n        folium.features.CircleMarker(\n            [lat, lng],\n            radius=5, # define how big you want the circle markers to be\n            color='blue',\n            fill=True,\n            fill_color='blue',\n            fill_opacity=0.6))\n    \n    elif dist == 'K\u00f6ln-Nippes':\n        locations.add_child(\n        folium.features.CircleMarker(\n            [lat, lng],\n            radius=5, # define how big you want the circle markers to be\n            color='red',\n            fill=True,\n            fill_color='red',\n            fill_opacity=0.6))    \n\n    else:\n           pass\n            \ncologne_map.add_child(locations)","5445c09b":"nipp_df = pd.concat([df_nipp1, df_nipp2])\nnipp_df = nipp_df[['name','Category','District','location.address','location.postalCode','location.lat','location.lng','location.distance']]\nnipp_df.rename({'name':'Venue','location.address':'Address','location.postalCode':'PostalCode','location.lat':'LocationLatitude','location.lng':'LocationLongitude','location.distance':'LocationDistance'}, axis=1, inplace=True)\n\nnipp_df.head()","15046486":"nipp_df['Category'].value_counts()","f33b82eb":"inne_df = pd.concat([df_inne1, df_inne2])\ninne_df = inne_df[['name','Category','District','location.address','location.postalCode','location.lat','location.lng','location.distance']]\ninne_df.rename({'name':'Venue','location.address':'Address','location.postalCode':'PostalCode','location.lat':'LocationLatitude','location.lng':'LocationLongitude','location.distance':'LocationDistance'}, axis=1, inplace=True)\n\ninne_df.head()","8f73b3cf":"inne_df['Category'].value_counts()","f11863c6":"neigh_data = {'Bar': [50,17],\n        'Restaurant': [50,6]\n        }\n\ncomp_df = pd.DataFrame(neigh_data, columns = ['Bar','Restaurant'], index=['K\u00f6ln-Innenstadt','K\u00f6ln-Nippes'])\ncomp_df","511a928b":"labels =['K\u00f6ln-Innenstadt','K\u00f6ln-Nippes']\nbar = comp_df['Bar']\nrestaurant = comp_df['Restaurant']\n\n\nind = np.arange(len(bar))  \nwidth = 0.2\n\nfig, ax = plt.subplots(figsize=(10,8))\nrects1 = ax.bar(ind - width, bar, width, label='Bar', color='#5cb85c')\nrects2 = ax.bar(ind, restaurant, width, label='Restaurant', color='#5bc0de')\n\n\nax.set_title(\"Neighborhoods Benchmark: Venues Breakdown\", fontsize=16)\nax.set_xticks(ind)\nplt.ylabel('Number of Venues')\nax.set_xticklabels((labels))\nax.get_yaxis().set_visible(False)\nax.spines['top'].set_visible(False)\nax.spines['left'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.legend(fontsize=14)\n\nautolabel(rects1, \"center\")\nautolabel(rects2, \"center\")\n\n\nfig.tight_layout()\n\nplt.show()","84be9ffc":"X = df_clean[['LocationLatitude','LocationLongitude']].values[:,1:]\nX = np.nan_to_num(X)\ncluster_dataset = StandardScaler().fit_transform(X)","3061ef07":"num_clusters = 5\n\nk_means = KMeans(init=\"k-means++\", n_clusters=num_clusters, n_init=6)\nk_means.fit(cluster_dataset)\nlabels = k_means.labels_\n\nprint(labels)","72eff70e":"df_clean[\"ClusterLabels\"] = labels\ndf_clean.head(5)","24bc2854":"# create a Stamen Toner map of the world centered around Cologne, Germany\ncologne_map = folium.Map(location=[50.936631, 6.958401], zoom_start=10)\n\n# instantiate a feature group for the incidents in the dataframe\nlocations = folium.map.FeatureGroup()\n\n# loop through the 100 crimes and add each to the incidents feature group\nfor lat, lng, label in zip(df_clean.LocationLatitude, df_clean.LocationLongitude, df_clean.ClusterLabels):\n    if label == 0:\n        locations.add_child(\n        folium.features.CircleMarker(\n            [lat, lng],\n            radius=5, # define how big you want the circle markers to be\n            color='blue',\n            fill=True,\n            fill_color='blue',\n            fill_opacity=0.6))\n    elif label == 1:\n        locations.add_child(\n        folium.features.CircleMarker(\n            [lat, lng],\n            radius=5, # define how big you want the circle markers to be\n            color='yellow',\n            fill=True,\n            fill_color='yellow',\n            fill_opacity=0.6))\n    elif label == 2:\n        locations.add_child(\n        folium.features.CircleMarker(\n            [lat, lng],\n            radius=5, # define how big you want the circle markers to be\n            color='red',\n            fill=True,\n            fill_color='red',\n            fill_opacity=0.6))\n    elif label == 3:\n        locations.add_child(\n        folium.features.CircleMarker(\n            [lat, lng],\n            radius=5, # define how big you want the circle markers to be\n            color='green',\n            fill=True,\n            fill_color='green',\n            fill_opacity=0.6))\n    elif label == 4:\n        locations.add_child(\n        folium.features.CircleMarker(\n            [lat, lng],\n            radius=5, # define how big you want the circle markers to be\n            color='orange',\n            fill=True,\n            fill_color='orange',\n            fill_opacity=0.6))\n    elif label == 5:\n        locations.add_child(\n        folium.features.CircleMarker(\n            [lat, lng],\n            radius=5, # define how big you want the circle markers to be\n            color='pink',\n            fill=True,\n            fill_color='pink',\n            fill_opacity=0.6))\n    elif label == 6:\n        locations.add_child(\n        folium.features.CircleMarker(\n            [lat, lng],\n            radius=5, # define how big you want the circle markers to be\n            color='purple',\n            fill=True,\n            fill_color='purple',\n            fill_opacity=0.6))\n    elif label == 7:\n        locations.add_child(\n        folium.features.CircleMarker(\n            [lat, lng],\n            radius=5, # define how big you want the circle markers to be\n            color='brown',\n            fill=True,\n            fill_color='brown',\n            fill_opacity=0.6))\n    elif label == 8:\n        locations.add_child(\n        folium.features.CircleMarker(\n            [lat, lng],\n            radius=5, # define how big you want the circle markers to be\n            color='violet',\n            fill=True,\n            fill_color='violet',\n            fill_opacity=0.6))\n        \n\n    else:\n           pass\n            \ncologne_map.add_child(locations)","088e940a":"clust_df = round(df_clean.sort_values('ClusterLabels'),2)\nclust_df.head()","19991168":"#### Then, I've gathered data on the rent prices per squared meter for every district through official reports published on www.koeln.de","8a7a01a2":"#### One thing that calls attention is that there is a clear difference on the amount of venues between the two neighborhoods, which is in fact related to the next topic to be investigated.\n\n### **3.4 Competition benchmark**\n#### One key criterion established in order to choose which kind of venue and neighborhood would be chosen was competition. Let us look at the Neighborhoods number\u2019s breakdown:","17b8c873":"#### With those variables I was now able to create Data frames out of it, below one example of the coding used:","3b7ef4d3":"#### According to the resulting dataframe, two districts had already a rent price of more than 12\u20ac per squared meter. We will hence drop the respective rows in the main dataframe:","1f55996f":"## 2. Data Gathering and Sorting Criteria\n### 2.1 Description\n#### In order to perform the analysis, a combination of geolocation data called from Foursquare and external sources found on the internet will be used. In general, the required data to be collected is as follows:\n\n* #### Venues position and insights: Foursquare API;\n* #### Details on Cologne Districts: webscraping on the internet.\n\n#### Dataframes will be generated and cross-referenced in search for key information under the given criteria, which are:\n\n* #### Rent costs: no more than 12\u20ac per square meter;\n* #### Demographic: minimum population density of 3000 people per squared kilometer\n* #### Competition: the chosen district can't have more than 20 establishments of the same category as the one to be chosen\n* #### Location: as close to the city center as possible\n\n### 2.2 Considerations on data quality and source choices\n#### When it comes to the data quality and reliability throughout the analysis, the level of confidence on the accuracy of the datasets is assumed fairly high, given the sources used and broad availability of alternative pieces of data on the internet, that corroborate the pieces of information used. Here it is important to say that although the usage of Foursquare API as main source of geolocation data for the venues is imposed as one of the Capstone course requirements, the API is widely known for sourcing data to highly used and trusted applications, such as Apple\u2019s Maps and Foursquare City Guide itself.\n\n#### Regarding the data about the city, the accuracy of neighborhoods names, distribution, population density, latitude and longitude was assessed through a combination of cross-checking several sources and using Google Maps to validate that the geolocation was in order. The latter was necessary because Geocoder was used to find the respective coordinates and given it is essentially based on keyword searching out the Data Frame itself, mismatches could in a first moment occur.\n\n#### One key concern though, was the rent price data available, for a couple reasons. The first one is that there are substantial differences between sources (there is a lot of info available based on real estate listings and websites, which might be biased somehow). Another point was that quite some sources could not be used as there was no distribution per district, making a comparison not feasible. At last, data that was too old (such as 10+ years old reports or so) could also not be considered. Given that, the criteria applied to define where the rent data would come from was based on how reputable the website to be scraped was and from which year would the report be.\n\n## 3. Methodology and Exploratory Data Analysis\n### 3.1 Web scraping Cologne Districts Data and Cleaning\n#### First, I've collected a list of the districts of Cologne on Wikipedia, sorted, formatted and renamed as per our needs:","17773e78":"# Applied Data Science Capstone: A Peek into Cologne's Bars and Restaurants Scene\n#### For the last couple of months I've been taking a series of courses in the field of Data Analysis and one of them is the IBM Data Science professional certificate, provided by Coursera. The final task of the Capstone Project is to apply Data Science for a business problem while leveraging the use of geolocation data out of Foursquare API in combination with the methods and algorithms learned throughout the course. I've chosen to analyze data from Cologne, Germany, the city I've been living for the past 5 years.\n\n## 1. Introduction and Business Problem Statement\n\n#### Cologne, founded in 38 BC, has a population of more than 1 million people, making it the forth largest city in the country. The city is part of the state of North-Rhine Westfalia, forming together with cities like D\u00fcsseldorf, Essen, Bonn, and a couple others, the Rhine-Ruhr area, known as the largest metropolitan region in Germany. It is also located in the far west of the country, having the borders of Belgium and Netherlands within a 100km range and being well-centered in Western Europe.\n\n#### Home of the K\u00f6lner Dom, the tallest twin-spired Church in the world, Cologne is known for its openness and multicultural vibe, with a vibrant nightlife and a variety of options for going out, including bars, clubs and restaurants from all kinds of places in the world. It is home for some of the most relevant and biggest events around, besides being part of a circuit of best music concerts and tours one can find in the region.\n\n#### This mix of history, entertainment and privileged location makes it a very good destination for tourists and a nice place to live. In this context, investing in the entertainment business in the city is often considered a good option, but then this brings a couple of questions, such as: what kind of establishment makes more sense to be opened? which locations would be best to start such a business? How competitive will be the chosen market? These are some of the questions we propose to answer with this analysis.\n\n### 1.1 The Problem Statement: Which Cologne neighborhood is the best to start a bar or restaurant?\n\n#### A client is thinking about investing in opening an establishment in the city of Cologne, so he asked to analyze the available data and cross-reference it against some criteria in order to take a decision. Below further details:\n\n* #### Two types of establishment were defined as initial targets: a bar or restaurant. It must be one of this two categories, therefore the analysis will prioritize those;\n* #### It is required that the new establishment is located where there is a good amount of people, preferably not so far away from the city center. In order to have an objective target, it was defined a minimum population density of 3000 people per squared kilometer;\n* #### On the other hand it is important that his establishment has the opportunity to stand out, so a neighborhood with too much competition in the same category should be avoided;\n* #### The place will be rented and the client is willing to pay up to 12\u20ac per squared meter.\n\n### 1.2 The Target Audience\n#### Besides the obvious interest coming from the aforementioned client in taking a data-driven decision about where to invest his money on, this study can be helpful to a couple of other stakeholders, such as:\n\n* #### Other investors with a similar interest in investing in the entertainment and gastronomy business in the city of Cologne;\n* #### Fellow Data Scientists that are looking for ideas or references on how to carry out some tasks that might be applicable to their projects, not just for Cologne, but anywhere in the world;\n* #### Cologne Citizens in general, to get further insights on the city they live in and on which neighborhoods there are more options of venues to go and check out.","9496f5c8":"### 3.2 Getting Venues Data through Foursquare API","fac8424f":"#### The next step was to search for the respective coordinates of each district, which I did through Geocoder:","9f91ac49":"#### As you can see, K\u00f6ln-Innenstadt was not directly found through the listing scraped from the website, so I processed the data further taking the average of Altstadt-Nord and -S\u00fcd and used it as the data point for Innenstadt:","4d2d5748":"#### As shown on the cleaned dataframe, only 4 districts have more than 3000 people per squared kilometer, therefore all data not falling in the criteria needs to be dropped:","7699e0a1":"### **3.3 Cleaning the Venues data**\n#### After I called the data out of Foursquare, I still needed to clean and prepare the data. The two main tasks were to drop everything that would not be relevant and to blend the different data-frames created, as they were at this point separated.","2840d2c5":"#### Below the distribution of the venue locations on the city map, being K\u00f6ln-Nippes represented in red and K\u00f6ln-Innenstadt in blue:","7959edf4":"#### As mentioned before, as a requirement for this Capstone, we needed to call the venues data through Foursquare API. The process is straightforward, and it was made easier by the previous filtering done on the city data.\n#### Here we need to start taking a look into insights on both bars and restaurants, so I\u2019ve created a couple of variables to deal with each, query_1 and query_2 for the bar and restaurant queries, respectively and 4 for the URLs, 2 per district, one for each type of venue. The radius was established through a couple of iterations done during the data clustering. The method to do it was basically based on previous knowledge of the city, while checking on the map the most senseful way of distributing the data, bigger radius would return inaccuracy on to which neighborhood the venue would belong to, so I\u2019ve used different figures until the boundaries were respected accordingly:","1ddaa4a5":"#### Please note, that with the population density data already scraped it was possible to rule out a couple of neighborhoods that did not fall into the criteria. As a result, out of 9 neighborhoods that are part of the city of Cologne, remained:\n\n* #### K\u00f6ln-Innenstadt (also often referenced as the city center)\n\n* #### K\u00f6ln-Ehrenfeld\n\n* #### K\u00f6ln-Nippes\n\n* #### K\u00f6ln-Lindenthal","0cd46fc6":"### 4. Results\n#### According to the competition target defined in the beginning of the analysis, there should be no more than 20 venues falling into the same venue category in the chosen neighborhood, which necessarily means that between the two districts left, K\u00f6ln-Nippes have more appeal when considering the level of competition.\n\n#### There is still one decision to be taken: what would make more sense to be opened, a bar or restaurant? When looking at the data, there are 17 bars located in Nippes according to the Foursquare data, and 6 restaurants. In order to give perspective, a couple of considerations are due. First, about bars, the kind of entertainment is more uniform among themselves, unless of course the bar is thematic, is a franchise or is already well known and marketed. Restaurants, on the other hand, might not compete as strongly with each other due to the whole variety of typical food available to be chosen when considering such establishment, especially in such cosmopolitan town as Cologne. This may incur in an exception depending on several different factors that are not covered by this study, such as opening hours, pricing and reviews available on the internet. Under these assumptions, the recommendation proposed by the study is to open a restaurant in the neighborhood K\u00f6ln-Nippes.\n\n### 5. Discussion\n#### A couple of further considerations need to be mentioned. The first is that, although the data shows a clear path with a well-defined rationale in order to reach the conclusion mentioned above, several factors were not part of the criteria used in the analysis. This means that the business case could be slightly different depending on what kind of other targets were to be established. As an example, one key factor to make the decision to lean towards Nippes instead of Innenstadt was the competition targets. What would happen if we also consider internet reviews benchmark and more detailed rent data for every neighborhood? What about if the competition target was substantially higher than 20? Another aspect is the distance to the city center, which for our use case did not need to be checked deeper, as the neighborhoods that were more distant from downtown were ruled out early in the analysis. But what would happen if the average income were to be considered? K\u00f6ln-Lindenthal is known for being a higher end neighborhood, which means more expensive (this makes sense when we look at the rent data scraped during the study, this neighborhood was the most expensive). On the other hand, not being that close to the city center could be an asset, depending on a couple of non-tangible aspects that also were not part of this study. \n\n#### By considering more of these non-tangible aspects, not just Lindenthal, but also Ehrenfeld could be more attractive according to certain specific contexts. The bottom line here is that, although the study is completely data-driven, it serves as a good initial reference of what are key important aspects for similar business cases only, so in no way it can propose itself to be an ultimate reference for decision making when considering other kind of criteria. Here it is important to say that, for the purposes of avoiding scope creep and to keep the Capstone Project comprehensiveness as concise as possible, the number of variables and criteria were kept to a minimum.\n\n### 6. Conclusion\n#### In this analysis, I proposed a discussion, based on objective targets, on which neighborhood of the city of Cologne would have more appeal to open either a bar or a restaurant. After conducting this study, we can infer that there are substantial differences between different parts of the city of Cologne whenever considering starting a business in the entertainment and gastronomy industries. Through several different data analysis techniques and well-defined criteria, we were able to see in a step-by-step approach how to apply Data Science principles in order to take an informed decision. Being myself a citizen of Cologne and living in the city for a couple of years, conducting this study not just meant a lot of learning, but also brought joy and satisfaction not just because I could see several points of convergence in the reached conclusions with the daily routine, but also because of the opportunity to getting to know a bit more of the town I live in. If you are a fellow Data Science student going through the Capstone project and using this material as reference for your own analysis, I hope you find it useful and that you, the same way as me, can see the value in going through the whole process, and can have as much fun as I did by doing this work."}}