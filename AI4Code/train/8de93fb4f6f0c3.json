{"cell_type":{"f67f3c17":"code","6688f6e8":"code","c315537d":"code","f0915673":"code","13c02b00":"code","32a4aaf3":"code","cbba48a1":"code","a532c31c":"code","a8e35e55":"code","a19f3569":"code","0f262f12":"code","101a84a3":"code","40aeeba7":"code","11357158":"code","642ddee3":"code","e7ba6848":"code","aeb4f70c":"code","23cdf4ad":"code","7713031e":"code","de16c286":"code","55b08753":"code","eef2ffc5":"code","00cd8ae6":"code","ead1a61a":"code","91c5852a":"code","3d93dabc":"markdown","31982837":"markdown","74ba5d56":"markdown","997b55e8":"markdown","9d4c3f86":"markdown","aacd12c4":"markdown","d498a05d":"markdown","9527ec97":"markdown","f4efce7d":"markdown","01b33c93":"markdown","eedffbbf":"markdown","a4f2ea4b":"markdown","623c8422":"markdown"},"source":{"f67f3c17":"\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom transformers import AutoTokenizer\n\npd.options.display.max_colwidth=150\npd.options.display.min_rows=300","6688f6e8":"# Read the data \ndf_test = pd.read_csv(\"..\/input\/jigsaw-toxic-comment-classification-challenge\/test.csv\").sample(20000)\nprint(df_test.shape)\n\ndf = pd.read_csv('\/kaggle\/input\/jigsaw-toxic-severity-rating\/validation_data.csv')\nprint(df.shape)\n\n# Combine text comments into one column\ndata = pd.DataFrame({\"text\": df_test.comment_text.tolist() + \\\n                             df.more_toxic.tolist() + \\\n                             df.less_toxic.tolist() }).drop_duplicates()\nprint(data.shape)\n","c315537d":"# Train TFIDF on old competition data and extract tokenizer\n\ntoxic_df = pd.read_csv('\/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv')\ntfidf_ = TfidfVectorizer(max_features = 5000).fit(toxic_df.comment_text)\nprint(len(tfidf_.vocabulary_))\ntfidf_tokenizer = tfidf_.build_analyzer()\n\n\n# Load bert tokenizer\n\nbert_tokenizer = AutoTokenizer.from_pretrained('\/kaggle\/input\/distilbertbaseuncased\/')","f0915673":"# An Example\nfor id_ in [66,622]:\n    sample = data.text[id_]\n    print(\"\\n\\n\\n===== Actual comment =====\")\n    print(sample)\n\n    print(\"\\n===== Tfidf tokenized comment =====\")\n    print([x for x in tfidf_tokenizer(sample) if x in tfidf_.vocabulary_])\n\n    print(\"\\n===== Bert tokenized comment =====\")\n    print(bert_tokenizer.convert_ids_to_tokens(bert_tokenizer.encode(sample)))","13c02b00":"# Extract such cases\n\ntmp = data.text.str.extractall(r'([A-Za-z]+([A-Za-z])\\2{2,}[A-Za-z]+\\b)')\n\ntmp.head(20)","32a4aaf3":"# Top cases \ntmp[0].value_counts()","cbba48a1":"\n#data.text[data.text.str.contains(tmp[0].value_counts().index[2])].tolist()","a532c31c":"# How will these look after cleaning \n\n# Take in the ids of the cases with patterns\nidx = tmp.reset_index()['level_0'].drop_duplicates()\n# Save in df\ndata_with_patterns = pd.DataFrame({\"text\": data.loc[idx].text.tolist()})\n# Clean the pattern\ndata_with_patterns['cleaned'] = data_with_patterns.text.str.replace(r'([A-Za-z])\\1{2,}',r'\\1')\ndata_with_patterns.head(10)","a8e35e55":"tmp = data.text.str.extractall(r'([A-Za-z]{1,}([*!?\\'])\\2{2,}[A-Za-z]{1,})')\ntmp.head(20)\n","a19f3569":"# Top patterns\ntmp[0].value_counts()","0f262f12":"\n# Take in the ids of the cases with patterns\nidx = tmp.reset_index()['level_0'].drop_duplicates()\n# Save in df\ndata_with_patterns = pd.DataFrame({\"text\": data.loc[idx].text.tolist()})\n# Clean the pattern\ndata_with_patterns['cleaned'] = data_with_patterns.text.str.replace(r'([A-Za-z]{1,})([*!?\\'])\\2{2,}([A-Za-z]{1,})',r'\\1\\2\\3')\n\ndata_with_patterns.head(10)","101a84a3":"tmp = data.text.str.extractall(r'(\\b[A-Za-z]{1,2}([\\]*!?\\'])\\2{2,}\\b)')\ntmp.head(20)\n","40aeeba7":"tmp[0].value_counts()","11357158":"\ntmp = data.text.str.extractall(r'([^\\w ]{3,})')\ntmp.head(20)\n","642ddee3":"tmp[0].value_counts()","e7ba6848":"\ntmp = data.text.str.extractall(r'(([a-zA-Z]+)[\/!?.]([a-zA-Z]+))').reset_index()\ntmp.head()\n","aeb4f70c":"# Top combinations\npd.concat([ tmp[1], tmp[2]]).str.lower().value_counts().reset_index()[:20]","23cdf4ad":"\ntmp = data.text.str.extractall(r'(\\b([a-zA-Z] ){3,})').reset_index()\ntmp.head(20)\n","7713031e":"# Top patterns\ntmp[0].value_counts()","de16c286":"tmp[0].str.lower().str.replace(r'[ .-]','').value_counts()","55b08753":"\ntmp = data.text.str.extractall(r'(\\b([a-zA-Z][-.]){2,}[a-zA-Z]\\b)').reset_index()\ntmp.head(20)\n","eef2ffc5":"# Top patterns\ntmp[0].value_counts()","00cd8ae6":"# Cleaned distribution\ntmp[0].str.lower().str.replace(r'[ .-]','').value_counts()","ead1a61a":"\ntmp = data.text.str.extractall(r'([a-zA-Z]+[^\\w ]{3,})')\ntmp.head(20)\n","91c5852a":"tmp[0].value_counts()","3d93dabc":"# Pattern - words are mixed together with punctuations like \"word1\/word2\"","31982837":"# Pattern: words split into characters with spaces like \"F C K\"\n","74ba5d56":"## User generated text data can be __S#!~#@!!__\n\n### A quick glance into some __common patterns__ where text is written in S#!~#@ way\n\n- Pattern 1: characters repeated in the string like \"brooooo\", \"cooooool\", \"damn uuuuuuuuuuuu\", etc\n- Pattern 2: words with punctuations in between - \"f******r'\n- Pattern 2b: words with punctuations at the end - \"a**'\n- Pattern 3: Non words multiple times sequentially\n- Pattern 4: words are mixed together with punctuations like \"word1\/word2\"\n- Pattern 5: words split into characters with spaces like \"F C K\"\n- Pattern 6: words split into characters with punctutations like \"F-C-K\"\n","997b55e8":"### How will these look after cleaning \n#### Convert cases like \"saaaaaad\" to \"sad\"","9d4c3f86":"## We don't need no __$#!++Y__ data","aacd12c4":"# Pattern - words with punctuations at the end - \"f***'","d498a05d":"# Pattern - words with punctuations in between - \"f******r'","9527ec97":"### How will these look after cleaning \n#### Convert cases like \"f***cking\" or \"f******cking\" to \"f*cking\"","f4efce7d":"# Pattern: words split into characters with punctuations like \"N-O-N-S-E-N-S-E\"\n","01b33c93":"# Pattern: Non words multiple times sequentially","eedffbbf":"# pattern - \\w mixed with \\W","a4f2ea4b":"# Pattern: characters repeated in the string like \"brooooo\", \"cooooool\", \"damn uuuuuuuuuuuu\", etc\n","623c8422":"### Example of how different tokenizers will tokenize badly written strings - Information loss!"}}