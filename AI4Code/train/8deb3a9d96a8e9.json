{"cell_type":{"f6feee3f":"code","3c1c960e":"code","0199ec17":"code","2981e315":"code","ac6fd853":"code","695df59f":"code","f69cc574":"code","ddf97d51":"code","7d7fe5be":"code","f0928859":"code","6a12814b":"code","cf218d36":"code","031646ae":"code","f3d36cf2":"code","296df2d2":"code","c2d51f83":"markdown","8a0fbb0c":"markdown","d80ddddc":"markdown","299f887e":"markdown","12f6e5f7":"markdown","f568ac5c":"markdown","47363ae4":"markdown","24b5d5bd":"markdown","68a28017":"markdown","60b2262e":"markdown","e0d4ad17":"markdown","addd03fd":"markdown","b1eb3e9e":"markdown"},"source":{"f6feee3f":"import gc\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom tensorflow import random\n\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sn\n\n\nrandom.set_seed(5577)","3c1c960e":"%%time\ntrainDf = pd.read_csv('\/kaggle\/input\/jane-street-market-prediction\/train.csv')","0199ec17":"def reduce_memory_usage(df):\n    start_memory = df.memory_usage().sum() \/ 1024**2\n    print(f\"Memory usage of dataframe is {start_memory} MB\")\n\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != 'object':\n            c_min = df[col].min()\n            c_max = df[col].max()\n\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    pass\n        else:\n            df[col] = df[col].astype('category')\n\n    end_memory = df.memory_usage().sum() \/ 1024**2\n    print(f\"Memory usage of dataframe after reduction {end_memory} MB\")\n    print(f\"Reduced by {100 * (start_memory - end_memory) \/ start_memory} % \")\n    return df\n\ntrainDf = reduce_memory_usage(trainDf)","2981e315":"dropCols = [\"resp\", \"resp_1\", \"resp_2\", \"resp_3\", \"resp_4\", \"ts_id\"]\ntrainDf = trainDf.drop(columns=dropCols)","ac6fd853":"trainDf.isnull().sum()","695df59f":"trainDf.fillna(0, inplace=True)","f69cc574":"trainDfW = trainDf[trainDf[\"weight\"] > 0]\ntrainDfW.head()","ddf97d51":"trainDf.shape","7d7fe5be":"trainDfW.shape","f0928859":"trainDfW.head()","6a12814b":"trainDfW.describe()","cf218d36":"%%time\ncorrDfW = trainDfW.corr()\nfig, ax = plt.subplots(figsize=(25,25)) \nsn.heatmap(corrDfW, linewidths=.5, annot=False, ax=ax)\nplt.show()","031646ae":"%%time\nscaler = MinMaxScaler()\nscaledTrain = scaler.fit_transform(trainDfW)\n\npca = PCA().fit(scaledTrain)\nexCumul = np.cumsum(pca.explained_variance_ratio_)\npx.area(\n    x=range(1, exCumul.shape[0] + 1),\n    y=exCumul,\n    labels={\"x\": \"# Components\", \"y\": \"Explained Variance\"}\n)","f3d36cf2":"pca = PCA(n_components=2)\ndfComp = pca.fit_transform(scaledTrain)\n\ntotal_var = pca.explained_variance_ratio_.sum() * 100\nfig = px.scatter(dfComp, x=0, y=1, color=trainDfW['weight'], title=f'Total Explained Variance: {total_var:.3f}%', labels={'0': 'PC 1', '1': 'PC 2'})\nfig.show()","296df2d2":"dfNoF0 = trainDfW.drop(\"feature_0\", 1)\nscaledTrainNoF0 = scaler.fit_transform(dfNoF0)\npca = PCA(n_components=2)\ndfComp = pca.fit_transform(scaledTrainNoF0)\n\ntotal_var = pca.explained_variance_ratio_.sum() * 100\nfig = px.scatter(dfComp, x=0, y=1, color=trainDfW['weight'], title=f'Total Explained Variance: {total_var:.3f}%', labels={'0': 'PC 1', '1': 'PC 2'})\nfig.show()","c2d51f83":"### Correlation Matrix","8a0fbb0c":"## Data Loading","d80ddddc":"According to the data tab of the competition :\n> Trades with weight = 0 were intentionally included in the dataset for completeness, although such trades will not contribute towards the scoring evaluation.\n\nSo, I make a slice without those rows before looking into the details.","299f887e":"Now lets take a look of the two major principal components when we remove feature_0 from the dataset.","12f6e5f7":"## Data Understanding","f568ac5c":"## Data Exploration\n### Basics","47363ae4":"Although the matrix is quite heavy, it allows to identify some interesting clusters. Some features seems highly (positively or negatively) correlated. The next step would be to pinpoint those features and check from features.csv if they share the same tags... ToDo","24b5d5bd":"## Data Preparation","68a28017":"Here, we can see that :\n* One of the component accounts for a third (36.9%) of the total variance\n* The threshold of 90% variance is explained by 8 components\n* The threshold of 95% variance is explained by 11 components","60b2262e":"## Reducing Memory Usage\nI had trouble to use the dataset due to it using around 5GB of RAM just after being loaded. I found this function from [sbunzini](https:\/\/www.kaggle.com\/sbunzini\/reduce-memory-usage-by-75) to mitigate the issue.","e0d4ad17":"### PCA","addd03fd":"Filling \"na\" with 0 for starter. It might be wiser to use some other techniques (imputing, mean, ...) but for a first version, this will do the job.","b1eb3e9e":"Removing columns that will not be used as feature for the training phase."}}