{"cell_type":{"fcd928db":"code","437c37d0":"code","f6456500":"code","16acc5f6":"code","061ffd2b":"code","b7ec6641":"code","96af4ed9":"code","4649f9e7":"code","b42e8952":"code","2d7c084b":"code","9a023294":"code","f4fbedc5":"code","99df53c5":"code","52753a9d":"code","5c5faedb":"code","fa0d81bf":"code","b82d6f78":"code","3c5c8e08":"code","f48541ec":"code","f5616efc":"code","26722190":"code","72da32b8":"code","15b0e5e6":"code","37007972":"code","5aa176f1":"markdown","490c07bb":"markdown","3bb57b2a":"markdown"},"source":{"fcd928db":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","437c37d0":"import random\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\nfrom sklearn.model_selection import train_test_split\nimport re","f6456500":"! pip install gdown","16acc5f6":"# down load Thai-wiki QA\n!gdown --id 14BNqgCOPJdQzwjJWPZGbqKeYu-9MHC1r\n# !wget https:\/\/drive.google.com\/file\/d\/14BNqgCOPJdQzwjJWPZGbqKeYu-9MHC1r\/view?usp=sharing","061ffd2b":"!unzip -q .\/AIFORTHAI-ThaiWikiQA_NSC2020.zip\n\n!unzip -q .\/AIFORTHAI-ThaiWikiQA_NSC2020\/wiki-documents-nsc.zip","b7ec6641":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nrandom.seed(26)\nnp.random.seed(26)\ntorch.manual_seed(26)","96af4ed9":"model_name = \"wangchanberta-base-att-spm-uncased\" #@param [\"wangchanberta-base-att-spm-uncased\", \"xlm-roberta-base\", \"bert-base-multilingual-cased\", \"wangchanberta-base-wiki-newmm\", \"wangchanberta-base-wiki-syllable\", \"wangchanberta-base-wiki-sefr\", \"wangchanberta-base-wiki-spm\", \"deepset\/xlm-roberta-large-squad2\", \"monsoon-nlp\/bert-base-thai\", \"roberta-base\"]\n","4649f9e7":"model = AutoModelForSequenceClassification.from_pretrained(model_name)\nmodel.to(device) # Send the model to the GPU if we have one\ntokenizer = AutoTokenizer.from_pretrained(model_name) ","b42e8952":"def encode_data(tokenizer, questions, contexts, max_length):\n    input_ids = []\n    attention_masks = []\n\n    for question, context in zip(questions, contexts):\n        encoded_data = tokenizer.encode_plus(question, context, max_length=max_length, pad_to_max_length=True, truncation_strategy=\"longest_first\")\n        encoded_pair = encoded_data[\"input_ids\"]\n        attention_mask = encoded_data[\"attention_mask\"]\n\n        input_ids.append(encoded_pair)\n        attention_masks.append(attention_mask)\n\n    return np.array(input_ids), np.array(attention_masks)","2d7c084b":"# train test split\nyes_no_df = pd.read_csv('.\/AIFORTHAI-ThaiWikiQA_NSC2020\/ThaiQACorpus-DevelopmentDataset.csv')\nyes_no_df = yes_no_df[yes_no_df['question_type']==2]\n\nyes_no_df.head()","9a023294":"def clean_text(text):\n    regrex_pattern = re.compile(pattern = \"[\"\n        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           \"]+\", flags = re.UNICODE)\n    return regrex_pattern.sub(r'',text)\n\ndef remove_HTML_syntax(input_text):\n    text = ''\n    accumulate = False\n    output_text = input_text\n    for char in input_text:\n        if char == '<':\n            accumulate = True\n        elif char == '>':\n            output_text = output_text.replace(text + char, '')\n            text = ''\n            accumulate = False\n        \n        if accumulate:\n            text += char\n\n    return output_text\n\ndef context_and_hashtag(file_path):\n    context_and_hashtag = {}\n    with open(file_path, encoding=\"utf-8\") as f:\n        lines = f.readlines()\n        hashtag = \"\"\n        raw = \"\"\n        for line in lines:\n            if not line.startswith(\"#\") and len(line.strip()) >= 2:\n                line = clean_text(line)\n                # line = line.replace(\"\u201d\", \"\").replace(\"\u201c\", \"\")\n                line = line.strip()\n                raw += line\n            elif line.startswith(\"#\"):\n                 hashtag += line.strip()\n    context_and_hashtag[file_path] = {\"context\": raw, \"hashtag\": hashtag}\n    return context_and_hashtag\n    \ndef preprocessing(df):\n    # df[\"article_id\"] = df[\"article_path\"].apply(lambda x: x[-7:])\n    df['context_and_hashtag'] = [context_and_hashtag(x) for x in df[\"article_path\"]]\n    df[\"context\"] = [list(x.values())[0]['context'] for x in df['context_and_hashtag']]\n    df[\"hashtag\"] = [list(x.values())[0]['hashtag'] for x in df['context_and_hashtag']]\n    df[\"Predicted\"] = \"\"\n    df['context'] = df['context'].map(lambda x: remove_HTML_syntax(x))\n    return df","f4fbedc5":"text_yesno_path = '.\/wiki-documents-nsc'\n\nyes_no_df['article_path']  = [os.path.join(text_yesno_path, str(x)+'.txt') for x in yes_no_df['article_id']]\n\nbooleanDictionary = {'\u0e43\u0e0a\u0e48': True, '\u0e44\u0e21\u0e48\u0e43\u0e0a\u0e48': False}\nyes_no_df['answer'] = yes_no_df['answer'].replace(booleanDictionary)\n\nyes_no_df = preprocessing(yes_no_df)[['question_id', 'question', 'answer', 'article_id', 'article_path', 'context']]\nyes_no_df.head()","99df53c5":"train_yes_no, test_yes_no = train_test_split(yes_no_df, test_size=0.33, random_state=42)\ndisplay(train_yes_no.head())\ndisplay(test_yes_no.head())","52753a9d":"learning_rate = 1e-5\noptimizer = AdamW(model.parameters(), lr=learning_rate, eps=1e-8)","5c5faedb":"contexts_train = train_yes_no.context.values\nquestions_train = train_yes_no.question.values\nanswers_train = train_yes_no.answer.values.astype(int)\n\ncontexts_test = test_yes_no.context.values\nquestions_test = test_yes_no.question.values\nanswers_test = test_yes_no.answer.values.astype(int)","fa0d81bf":"# Encoding data\nmax_seq_length = 412\ninput_ids_train, attention_masks_train = encode_data(tokenizer, questions_train, contexts_train, max_seq_length)\ninput_ids_test, attention_masks_test = encode_data(tokenizer, questions_test, contexts_test, max_seq_length)\n\ntrain_features = (input_ids_train, attention_masks_train, answers_train)\ntest_features = (input_ids_test, attention_masks_test, answers_test)","b82d6f78":"batch_size = 4\n\ntrain_features_tensors = [torch.tensor(feature, dtype=torch.long) for feature in train_features]\ntest_features_tensors = [torch.tensor(feature, dtype=torch.long) for feature in test_features]\n\ntrain_dataset = TensorDataset(*train_features_tensors)\ntest_dataset = TensorDataset(*test_features_tensors)\n\ntrain_sampler = RandomSampler(train_dataset)\ndev_sampler = SequentialSampler(test_dataset)\n\ntrain_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)\ntest_dataloader = DataLoader(test_dataset, sampler=dev_sampler, batch_size=batch_size)","3c5c8e08":"import gc\ngc.collect()\ntorch.cuda.empty_cache() ","f48541ec":"epochs = 2\ngrad_acc_steps = 1\ndisplay_steps = 1\ntrain_loss_values = []\ntest_acc_values = []","f5616efc":"for epoch in range(epochs):\n    # Training\n    epoch_train_loss = 0 # Cumulative loss\n    model.train()\n    model.zero_grad()\n\n    for step, batch in tqdm(enumerate(train_dataloader), desc='Steps: '):\n\n        input_ids = batch[0].to(device)\n        attention_masks = batch[1].to(device)\n        labels = batch[2].to(device) \n\n        outputs = model(input_ids, token_type_ids=None, attention_mask=attention_masks, labels=labels)\n\n        loss = outputs[0]\n        loss = loss \/ grad_acc_steps\n        epoch_train_loss += loss.item()\n\n        loss.backward()\n\n        if (step+1) % grad_acc_steps == 0: # Gradient accumulation is over\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # Clipping gradients\n            optimizer.step()\n            model.zero_grad()\n        # torch.cuda.empty_cache() \n\n    epoch_train_loss = epoch_train_loss \/ len(train_dataloader)          \n    train_loss_values.append(epoch_train_loss)\n\n    if epoch % display_steps == 0:\n        print('Train loss: ', epoch_train_loss)\n\n    # Evaluation\n    epoch_dev_accuracy = 0 # Cumulative accuracy\n    model.eval()\n\n    for batch in test_dataloader:\n        input_ids = batch[0].to(device)\n        attention_masks = batch[1].to(device)\n        labels = batch[2]\n                \n        with torch.no_grad():        \n            outputs = model(input_ids, token_type_ids=None, attention_mask=attention_masks)\n                    \n        logits = outputs[0]\n        logits = logits.detach().cpu().numpy()\n\n        predictions = np.argmax(logits, axis=1).flatten()\n        labels = labels.numpy().flatten()\n\n        epoch_dev_accuracy += np.sum(predictions == labels) \/ len(labels)\n\n    epoch_dev_accuracy = epoch_dev_accuracy \/ len(test_dataloader)\n    test_acc_values.append(epoch_dev_accuracy)\n\n    if epoch % display_steps == 0:\n        print('Test acc: ', epoch_dev_accuracy)","26722190":"def predict(question, context, max_length=416):\n    sequence = tokenizer.encode_plus(question, context, return_tensors=\"pt\", max_length=max_length, truncation_strategy=\"longest_first\")['input_ids'].to(device)\n    logits = model(sequence)[0]\n    probabilities = torch.softmax(logits, dim=1).detach().cpu().tolist()[0]\n    prediction = np.array(probabilities).argmax()\n    return int(prediction) # 1->Yes, 0-> NO","72da32b8":"def run_yes_no(df):\n    yes_no_prediction = []\n    for row in tqdm(df.itertuples()):\n        question = str(row.question)\n        context = str(row.context)\n\n        if \"\u0e43\u0e0a\u0e48\u0e2b\u0e23\u0e37\u0e2d\u0e44\u0e21\u0e48\" in question or \"\u0e2b\u0e23\u0e37\u0e2d\u0e44\u0e21\u0e48\" in question or question in yes_no_df['question'].values:\n            yes_no_prediction.append(predict(question, context))\n        else:\n            yes_no_prediction.append(np.nan)\n    df['yes_no_prediction'] = yes_no_prediction\n    return df","15b0e5e6":"yes_no_df['id'] = yes_no_df['question_id']\npred_yes_no = run_yes_no(yes_no_df)\npred_yes_no","37007972":"from sklearn.metrics import accuracy_score, classification_report\npred_yes_no.dropna(inplace=True)\npred_yes_no['yes_no_prediction'] = pred_yes_no['yes_no_prediction'].astype(int)\npred_yes_no[\"answer\"] = pred_yes_no[\"answer\"].astype(int)\n\nprint(accuracy_score(pred_yes_no['answer'], pred_yes_no['yes_no_prediction']))\nprint(classification_report(pred_yes_no['answer'], pred_yes_no['yes_no_prediction']))","5aa176f1":"# Infer","490c07bb":"ref. https:\/\/medium.com\/illuin\/deep-learning-has-almost-all-the-answers-yes-no-question-answering-with-transformers-223bebb70189","3bb57b2a":"# Training"}}