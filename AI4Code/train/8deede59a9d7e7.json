{"cell_type":{"f45eec00":"code","333fbf03":"code","a111d7bd":"code","5166a6d4":"code","2a00db4f":"code","335a7d3a":"code","23a05bb7":"code","393a9459":"code","217cff45":"code","32fbc346":"code","d55428ed":"code","3bd9e3a8":"code","b220a577":"code","944e0bcb":"code","876d126f":"code","95aee160":"code","9ca346c0":"code","96e5d6c1":"code","df60d716":"code","0cd2502f":"code","6f065519":"code","2db4f400":"code","13b21e8a":"code","cd7c724d":"code","d32a9c18":"code","78de9f63":"code","e9c6f969":"code","5b3c9a80":"code","8ccac34e":"code","d06f7407":"code","150a12af":"code","fe92672e":"code","c80e5929":"code","47a86fa8":"code","e610af8d":"markdown","1c03b19b":"markdown","7d26a801":"markdown","ade40e2a":"markdown","5503d992":"markdown","f1854544":"markdown","d71e0b1c":"markdown","024a782a":"markdown","b44954b3":"markdown","9f18aca2":"markdown","2ca3f8da":"markdown","ec82560d":"markdown","33f0a558":"markdown","88755bc6":"markdown","dd67d022":"markdown","73c42786":"markdown","32a5d38b":"markdown","dd9d5972":"markdown","ab535d76":"markdown","e9817eef":"markdown","7690c425":"markdown","6ad4297b":"markdown","a542ffe4":"markdown","d2473509":"markdown","b996b6ae":"markdown"},"source":{"f45eec00":"# installing japanese font\nfrom matplotlib.font_manager import FontProperties\n!apt-get -y install fonts-ipafont-gothic\nfont_jp = FontProperties(fname=r'\/usr\/share\/fonts\/opentype\/ipafont-gothic\/ipagp.ttf',size=10)","333fbf03":"# Importing the necessary libraries.\nfrom tensorflow.compat.v1.keras.layers import CuDNNLSTM, CuDNNGRU\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom nltk.translate.bleu_score import sentence_bleu as bleu\nfrom janome.tokenizer import Tokenizer as janome_tokenizer\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.layers import Dense, Embedding\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.layers import TimeDistributed\nfrom tensorflow.keras.models import Model\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nimport tensorflow as tf\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport datetime\nimport warnings\nimport time\nimport re\nimport os\n# warnings.filterwarnings(\"ignore\", message=\"Glyph \\d+ missing from current font.\")\nwarnings.simplefilter(\"ignore\")\nsns.set()\ntf.compat.v1.enable_eager_execution()\ntf.keras.backend.clear_session()","a111d7bd":"f = open('..\/input\/english-to-japanese-50k-sentences\/jpn.txt').read() # reading data from file\ndata_ = [line.split('\\t')[:2] for line in f.split('\\n')[:-1]] # removing attribution\ndf = pd.DataFrame(data_, columns=['English', 'Japanese'])\nprint('Dimensions of the data as a dataframe:')\ndf.shape","5166a6d4":"df.head(10)","2a00db4f":"df.tail(10)","335a7d3a":"# For tokenizing the Japanese sentences\nfrom janome.tokenizer import Tokenizer as janome_tokenizer\ntoken_object = janome_tokenizer()","23a05bb7":"mispell_dict = {\"aren't\" : \"are not\", \"can't\" : \"cannot\", \"couldn't\" : \"could not\", \n                \"couldnt\" : \"could not\", \"didn't\" : \"did not\", \"doesn't\" : \"does not\", \n                \"doesnt\" : \"does not\", \"don't\" : \"do not\", \"hadn't\" : \"had not\", \n                \"hasn't\" : \"has not\", \"haven't\" : \"have not\", \"havent\" : \"have not\", \n                \"he'd\" : \"he would\", \"he'll\" : \"he will\", \"he's\" : \"he is\", \"i'd\" : \"I would\",\n                \"i'd\" : \"I had\", \"i'll\" : \"I will\", \"i'm\" : \"I am\", \"isn't\" : \"is not\", \n                \"it's\" : \"it is\", \"it'll\":\"it will\", \"i've\" : \"I have\", \"let's\" : \"let us\", \n                \"mightn't\" : \"might not\", \"mustn't\" : \"must not\", \"shan't\" : \"shall not\", \n                \"she'd\" : \"she would\", \"she'll\" : \"she will\", \"she's\" : \"she is\", \n                \"shouldn't\" : \"should not\", \"shouldnt\" : \"should not\", \"that's\" : \"that is\", \n                \"thats\" : \"that is\", \"there's\" : \"there is\", \"theres\" : \"there is\", \n                \"they'd\" : \"they would\", \"they'll\" : \"they will\", \"they're\" : \"they are\", \n                \"theyre\": \"they are\", \"they've\" : \"they have\", \"we'd\" : \"we would\", \n                \"we're\" : \"we are\", \"weren't\" : \"were not\", \"we've\" : \"we have\", \n                \"what'll\" : \"what will\", \"what're\" : \"what are\", \"what's\" : \"what is\", \n                \"what've\" : \"what have\", \"where's\" : \"where is\", \"who'd\" : \"who would\",\n                \"who'll\" : \"who will\", \"who're\" : \"who are\", \"who's\" : \"who is\", \n                \"who've\" : \"who have\", \"won't\" : \"will not\", \"wouldn't\" : \"would not\", \n                \"you'd\" : \"you would\", \"you'll\" : \"you will\", \"you're\" : \"you are\", \n                \"you've\" : \"you have\", \"'re\": \" are\", \"wasn't\": \"was not\", \"we'll\":\" will\", \n                \"didn't\": \"did not\", \"tryin'\":\"trying\"\n                }\n\ndef deconcate(sent):\n    keys = mispell_dict.keys()\n    cleaned_sentence = []\n    for word in sent.split(' '):\n        if word in keys:\n            cleaned_sentence.append(mispell_dict[word])\n        else:\n            cleaned_sentence.append(word)\n    return ' '.join(cleaned_sentence)","393a9459":"def preprocess(phrase):\n  phrase = phrase.strip().lower()\n  phrase = deconcate(phrase)\n  phrase = '<sos> '+ phrase.lower() +' <eos>'\n  return phrase","217cff45":"# preprocessing the sentences by stripping and adding <sos>, <eos>\nenglish, japanese = [], []\nfor line in tqdm(f.split('\\n')[:-1]):\n  english.append(preprocess(line.split('\\t')[0]))\n  jp_ = ' '.join([x.surface for x in token_object.tokenize(line.split('\\t')[1])])\n  japanese.append(preprocess(jp_))","32fbc346":"_eng_len = [len(x.split(' '))-2 for x in english]\n_jpn_len = [len(x.split(' '))-2 for x in japanese]","d55428ed":"sns.set()\nfig = plt.figure(figsize=(10,6))\nsns.distplot(_eng_len)\nsns.distplot(_jpn_len, color='orange')\nfig.legend(labels=['English sentence data','Japanese sentence data'])\nplt.xlabel('Sentence length')\nplt.ylabel('Number of sentences')\nplt.title('Histograms of sentence lengths')\nplt.show()","3bd9e3a8":"fig, ax = plt.subplots(1,2, figsize=(20, 10))\nsns.violinplot(_eng_len, ax=ax[0], orient='v')\nfig.suptitle('Violinplot of sentence lengths')\nax[0].set_xlabel('English Sentence length')\nax[0].set_ylabel('Number of sentences')\nsns.violinplot(_jpn_len, ax=ax[1], orient='v', color='orange')\nax[1].set_xlabel('Japanese Sentence length')\nax[1].set_ylabel('Number of sentences')\nfig.legend(labels=['English sentence data','Japanese sentence data'])\nplt.show();","b220a577":"# initializing tokenizer To convert the words into numerical representation\nfilters = \"+1234567890\\\"#$%&()*+,-\/:;=@[\\\\]^_`{|}~'\"\ntokenizer_eng = Tokenizer(split=' ', char_level=False, filters=filters)\n# fitting on english\ntokenizer_eng.fit_on_texts(english)\neng_tokens = tokenizer_eng.texts_to_sequences(english)\n# fitting on japanese\ntokenizer_jpn = Tokenizer(lower=True, split=' ', char_level=False, filters=filters)\ntokenizer_jpn.fit_on_texts(japanese)\njpn_tokens = tokenizer_jpn.texts_to_sequences(japanese)","944e0bcb":"# defining the vocabulary size of english and japanese language in the given data.\neng_vocab = len(tokenizer_eng.word_index.items()) + 1\njpn_vocab = len(tokenizer_jpn.word_index.items()) + 1","876d126f":"# defining the max length of english and japanese sentence in the given data (for padding).\neng_max = max([len(e) for e in eng_tokens])\njpn_max = max([len(i) for i in jpn_tokens])","95aee160":"# Padding the tokenized data\neng_padded = pad_sequences(eng_tokens, maxlen=eng_max, padding='post')\njpn_padded = pad_sequences(jpn_tokens, maxlen=jpn_max, padding='post')","9ca346c0":"# splitting data into train and test\neng_tr, eng_te, jpn_tr, jpn_te = train_test_split(eng_padded, jpn_padded, test_size=0.03, random_state=42)","96e5d6c1":"eng_tr.shape, jpn_tr.shape, eng_te.shape, jpn_te.shape","df60d716":"class Encoder(tf.keras.layers.Layer):\n    def __init__(self, vocab_size, embedding_dim, enc_units):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        self.enc_units = enc_units\n        \n    def build(self, input_shape):\n        self.embedding = Embedding(input_dim=self.vocab_size, output_dim=self.embedding_dim, name=\"embedding_layer_encoder\")\n        self.lstm = CuDNNLSTM(self.enc_units, return_state=True, return_sequences=True,\n                             recurrent_initializer='glorot_uniform', name=\"Encoder_LSTM\")\n        \n    def call(self, input_sentances, state_h, state_c, training=True):\n        input_embedd = self.embedding(input_sentances)\n        self.lstm_output, self.lstm_state_h, self.state_c = self.lstm(input_embedd, initial_state=[state_h, state_c])\n        return self.lstm_output, self.lstm_state_h\n\n    def initialize_hidden_state(self, batch_sz):\n        return [tf.zeros((batch_sz, self.enc_units)), tf.zeros((batch_sz, self.enc_units))]\n    \nclass Decoder(tf.keras.layers.Layer):\n    def __init__(self, vocab_size, embedding_dim, dec_units):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        self.dec_units = dec_units\n        self.dense = tf.keras.layers.Dense(vocab_size)\n        self.W1 = tf.keras.layers.Dense(self.dec_units)\n        self.W2 = tf.keras.layers.Dense(self.dec_units)\n        self.V = tf.keras.layers.Dense(1)\n\n    def build(self, input_shape):\n        self.embedding = Embedding(input_dim=self.vocab_size, output_dim=self.embedding_dim, name=\"embedding_layer_decoder\")\n        self.lstm = CuDNNLSTM(self.dec_units, return_state=True, return_sequences=True,\n                             recurrent_initializer='glorot_uniform', name=\"Encoder_LSTM\")\n        \n    def call(self, target_sentances, enc_output, state_h):\n        target_embedd = self.embedding(target_sentances)\n        # concat method as content-based function\n        hidden_with_time_axis = tf.expand_dims(state_h, 1) # expanding layer dimension for compatibility with encoder_output tensor\n        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis))) # calculating score\n        attention_weights = tf.nn.softmax(score, axis=1) # calculating attention_weights\n        context_vector = attention_weights * enc_output # calculating context_vector\n        context_vector = tf.reduce_sum(context_vector, axis=1) # preparing context vector to make it compatible with target_embedd\n        target_embedd = tf.concat([tf.expand_dims(context_vector, 1), target_embedd], axis=-1) # concatenating context vector and target_embeddings\n        lstm_output, state_h, state_c = self.lstm(target_embedd) # getting result from lstm\n        lstm_output = tf.reshape(lstm_output, (-1, lstm_output.shape[2])) # Making the output compatible for dense layer for (vocab representation)\n        dec_output = self.dense(lstm_output) # getting vocab size output\n        return  attention_weights, dec_output, state_h, attention_weights\n\n    def initialize_hidden_state(self, batch_sz):\n        return [tf.zeros((batch_sz, self.enc_units)), tf.zeros((batch_sz, self.enc_units))]","0cd2502f":"optimizer = tf.keras.optimizers.Adam()\ndef loss_function(real, pred):\n    mask = tf.cast(tf.cast(real, dtype=tf.bool), dtype=tf.float32) #mask to elemenate the loss due <pad> values\n    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n    return tf.reduce_mean(loss_)","6f065519":"# https:\/\/machinelearningmastery.com\/calculate-bleu-score-for-text-python\/\nwarnings.simplefilter(\"ignore\")\n# I'll be calculating the BLEU score in 1000 random datapoints from test data.\ndef bleu_score_calc(encoder, decoder):\n    size = 1000\n    pts = np.random.randint(0, jpn_te.shape[0], size)\n    q = jpn_te[pts]\n    questions = [t.split(' ')[1:-1] for t in tokenizer_jpn.sequences_to_texts(q)]\n\n    a = eng_te[pts]\n    answers = [t.split(' ')[1:-1] for t in tokenizer_eng.sequences_to_texts(a)]\n\n    inputs = tf.convert_to_tensor(q)\n    \n    state_h, state_c = [tf.zeros((size, units)), tf.zeros((size, units))]\n    encoder_output, state_h = encoder(inputs, state_h, state_c)\n    decoder_input = tf.expand_dims(tokenizer_jpn.texts_to_sequences(['<sos>'])[0]*size, 1)\n    \n    result = []\n    for i in range(eng_max):\n      attention_wts, decoder_output, state_h, _ = decoder(decoder_input, encoder_output, state_h)\n      predicted_word = tf.argmax(decoder_output, axis=1).numpy()\n      result.append(tokenizer_eng.sequences_to_texts(predicted_word.reshape(-1,1)))\n      decoder_input = tf.expand_dims(predicted_word, 1)\n    result = np.array(result).T\n    guesses = [x[:np.where(x=='<eos>')[0][0]] if ('<eos>' in x) else x for x in result]\n\n    return np.mean([bleu([x],y) for x,y in zip(answers, guesses)])","2db4f400":"font_jp = FontProperties(fname=r'\/usr\/share\/fonts\/opentype\/ipafont-gothic\/ipagp.ttf',size=10)\ndef plot_heatmap(df):\n  plt.figure(figsize=(16,10))\n  plt.title('heatmap of Annotation weights')\n  heatmap = plt.pcolor(df)\n\n  for y in range(df.shape[0]):\n      for x in range(df.shape[1]):\n          plt.text(x + 0.5, y + 0.5, '%.3f' % df.values[y, x],\n                  horizontalalignment='center',\n                  verticalalignment='center')\n\n  plt.colorbar(heatmap)\n  plt.yticks(np.arange(0.5, len(df.index), 1), df.index, fontproperties=font_jp)\n  plt.xticks(np.arange(0.5, len(df.columns), 1), df.columns, fontproperties=font_jp)\n  plt.show()","13b21e8a":"train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)","cd7c724d":"# Calculation and applying gradient descent on the trainable parameters of the model\n@tf.function\ndef train_step(encoder, decoder, state_h, state_c, inputs, target):\n    loss = 0\n    with tf.GradientTape() as tape:\n        encoder_output, state_h = encoder(inputs, state_h, state_c)\n        decoder_input = tf.expand_dims(tokenizer_jpn.texts_to_sequences(['<sos>'])[0] * BATCH_SIZE, 1)\n        for t in range(1, target.shape[1]):\n            attention_wts, decoder_output, state_h, score = decoder(decoder_input, encoder_output, state_h)\n            loss += loss_function(target[:, t], decoder_output)\n            decoder_input = tf.expand_dims(target[:, t], 1)\n    \n    batch_loss = (loss \/ int(target.shape[1]))\n    variables = encoder.variables + decoder.variables\n    gradients = tape.gradient(loss, variables)\n    optimizer.apply_gradients(zip(gradients, variables))\n    train_loss(batch_loss)\n    v_name = []\n    for v in variables:\n      v_name.append(v.name)\n\n    return batch_loss, v_name, gradients","d32a9c18":"# https:\/\/www.tensorflow.org\/tutorials\/text\/nmt_with_attention\nclass MyModel(Model):\n    def __init__(self, jpn_vocab, eng_vocab, embedding_dim, units):\n        super().__init__() # https:\/\/stackoverflow.com\/a\/27134600\/4084039\n        self.encoder = Encoder(jpn_vocab, embedding_dim, units)\n        self.decoder = Decoder(eng_vocab, embedding_dim, units)\n        self.optimizer = tf.keras.optimizers.Adam()\n        self.state_h = 0\n        self.state_c = 0\n        self.batch_loss = 0\n    \n    def fit(self, dataset, BATCH_SIZE, epochs):\n        tf.summary.trace_on(graph=True, profiler=False)\n        for epoch in range(epochs):\n            start = time.time()\n            total_loss = 0\n            self.state_h, self.state_c = self.encoder.initialize_hidden_state(BATCH_SIZE)\n            for (batch, (inputs, target)) in enumerate(dataset):\n                batch_loss, variables, gradients = train_step(self.encoder, self.decoder, self.state_h, self.state_c, inputs, target)\n                total_loss += batch_loss\n                if batch % 100 == 0:\n                    print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy()))\n\n            bleu_score = bleu_score_calc(self.encoder, self.decoder)\n\n            print('Epoch {} Loss {:.4f} BLEU-score {}'.format(epoch+1, total_loss\/N_BATCH, bleu_score))\n            print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n\n        tf.summary.trace_off()\n        print('training completed...!')\n        print('returning gradients...')\n        train_loss.reset_states()\n\n    # function for translating the input japanese sentence to english and plotting the annotation\/attention scores.\n    # If no sentence is passed, it picks a random sentence from Japanese test data\n    def test(self, input_sentence=''):\n        if input_sentence=='':\n          k = np.random.randint(len(jpn_te)) # a random number\n          w = jpn_tr[k] # a random sentence sequence from the input language\n          asked = tokenizer_jpn.sequences_to_texts([w])[0] # converting the input sequence into text\n          print(f\"input sentence: {' '.join(asked.split(' ')[1:-1])}\") # printing the input text sequence\n\n          o = eng_tr[k] # a random sentence sequence from the output language\n          o = tokenizer_eng.sequences_to_texts([o])[0] # converting the output sequence into text\n          print(f\"actual translation: {' '.join(o.split(' ')[1:-1])}\") # printing the output text sequence\n        \n        else:\n          inp = ' '.join([x.surface for x in token_object.tokenize(input_sentence)])\n          prep = preprocess(inp)\n          jp_tkn = tokenizer_jpn.texts_to_sequences([prep])\n          w = pad_sequences(jp_tkn, maxlen=jpn_max, padding='post')[0]\n          asked = tokenizer_jpn.sequences_to_texts([w])[0] # converting the input sequence into text\n          print(f\"input sentence: {' '.join(asked.split(' ')[1:-1])}\") # printing the input text sequence\n\n        # preparing input for attention model\n        inputs = tf.convert_to_tensor(w.reshape(1,-1)) # initializing the input as a tensor\n        state_h, state_c = [tf.zeros((1, units)), tf.zeros((1, units))] # initializing the hidden layers (all zeros)\n        encoder_output, state_h = self.encoder(inputs, state_h, state_c)\n        decoder_input = tf.expand_dims(tokenizer_jpn.texts_to_sequences(['<sos>'])[0], 0) # decoder initial input as all <sos>\n        # translating sentence\n        attention = []\n        result = ''\n        for i in range(eng_max):\n          attention_wts, decoder_output, state_h, _ = self.decoder(decoder_input, encoder_output, state_h)\n          attention_wts = attention_wts.numpy().flatten()\n          predicted_word = tf.argmax(decoder_output[0]).numpy()\n          if predicted_word == tokenizer_jpn.texts_to_sequences(['<eos>'])[0][0]:\n            break\n          result += tokenizer_eng.sequences_to_texts([[predicted_word]])[0] + ' '\n          decoder_input = tf.expand_dims([predicted_word], 0)\n          attention.append(attention_wts[1:len(asked.split(' '))-1])\n        print(f'predicted translation: {result}')\n\n        attention = np.array(attention)\n        index = result.split(' ')[:-1]\n        columns = asked.split(' ')[1:-1]\n\n        atten = pd.DataFrame(attention, columns=columns, index=index)\n        plot_heatmap(atten)","78de9f63":"BUFFER_SIZE = len(eng_tr)\nBATCH_SIZE = 128\nN_BATCH = BUFFER_SIZE\/\/BATCH_SIZE\nembedding_dim = 256\nunits = 1024\nEPOCHS = 20\nprint(N_BATCH)\n\ndataset = tf.data.Dataset.from_tensor_slices((jpn_tr, eng_tr)).shuffle(BUFFER_SIZE)\ndataset = dataset.batch(BATCH_SIZE, drop_remainder=True)","e9c6f969":"tf.keras.backend.clear_session()\nmodel = MyModel(jpn_vocab, eng_vocab, embedding_dim, units)\nmodel.fit(dataset, BATCH_SIZE, EPOCHS)\n# model.save_weights('jpn_to_eng.h5')","5b3c9a80":"model.fit(dataset, BATCH_SIZE, 10)","8ccac34e":"model.test()","d06f7407":"model.test()","150a12af":"model.test()","fe92672e":"# ... asks Orochimaru! \ud83d\udc0d\nmodel.test('\u6016\u3044\u306e\u304b\uff1f')","c80e5929":"# ... says Naruto! \ud83e\udd8a\nmodel.test('\u6016\u304f\u306a\u3044\uff01')","47a86fa8":"# Finally, *a long dialogue by* Itachi! \ud83d\udd34\nmodel.test('\u5f7c\u3089\u304c\u305d\u308c\u3092\u5931\u308f\u306a\u3044\u9650\u308a\u3001\u4f55\u304b\u306e\u771f\u306e\u4fa1\u5024\u3092\u5b9f\u73fe\u3057\u306a\u3044\u3053\u3068\u306f\u4eba\u9593\u306e\u672c\u6027\u3067\u3059\u3002')","e610af8d":"### \ud83d\udc81\ud83c\udffb It took around 100 mins to train the model on around 52k sentences and the BLEU score so far is 0.53 which is pretty good. Let's test the model on some random datapoints from the test data and check the attention scores.","1c03b19b":"### \ud83e\uddee Gradient calculation and update","7d26a801":"### \ud83c\udfcb Defining some more parameters and training the model","ade40e2a":"### \u24a1 BLEU score function","5503d992":"<img src=\"https:\/\/i.ibb.co\/YWXcL6v\/itachi.jpg\" alt=\"itachi\" border=\"0\">","f1854544":"### The model seems to perform pretty good on short sentences but seems lost while translating long sentences, probably because of the small dataset.\n### In the future version I'll improve the performance of this model by adding more data and by some fine-tuning. I'll also use the other scoring functions and see which one works the best.\n### Thanks for reading, hope the notebook was useful! Comments and feedbacks are most welcomed!\n### Stay strong! Keep on learning!\n### \u3055\u3088\u3046\u306a\u3089\uff01\u262e","d71e0b1c":"### \ud83d\udcc8 Function for plotting the heatmap of Annotation weights","024a782a":"### \ud83c\udf65 Finally, let's test the model on some of the quotes from Japanese manga Naruto.","b44954b3":"## Translating Japanese text to English using attention enabled Encoder-Decoder model.","9f18aca2":"### \ud83d\udcca Let's do some analysis on the sentence lengths.","2ca3f8da":"<img src=\"https:\/\/i.ibb.co\/W3TDNTj\/Naruto-Cinco-cosas-que-el-anime-mejoro-en-relacion-al-manga-6-1.jpg\" alt=\"Naruto-Cinco-cosas-que-el-anime-mejoro-en-relacion-al-manga-6-1\" border=\"0\">","ec82560d":"### \u2049\ufe0f Attention model:\n#### Attention is a mechanism that was developed to improve the performance of the Encoder-Decoder RNN on machine translation. It was proposed as a solution to the limitation of the Encoder-Decoder model encoding the input sequence to one *fixed length internal representation* from which to decode each output time step. This issue was believed to be more of a problem when decoding long sequences. Instead of encoding the input sequence into a single fixed context vector, the attention model develops a context vector that is filtered specifically for each output time step.\n![attention gif](https:\/\/labs.eleks.com\/wp-content\/uploads\/2019\/06\/image5.gif)\n#### As with the Encoder-Decoder paper, the technique is applied to a machine translation problem and uses GRU units rather than LSTM memory cells. But in this project I'll be using LSTM memory cells.\nreference 1: https:\/\/machinelearningmastery.com\/encoder-decoder-recurrent-neural-network-models-neural-machine-translation\/\n\nreference 2: https:\/\/machinelearningmastery.com\/how-does-attention-work-in-encoder-decoder-recurrent-neural-networks\/","33f0a558":"<img src=\"https:\/\/i.ibb.co\/CwPmH6n\/291689-orochimaru.jpg\" alt=\"291689-orochimaru\" border=\"0\">","88755bc6":"#### Remember in the step 2 'Score calculation' the scoring is performed using a function. Now there are many scoring functions out there but in this project I'll be using *general method* for score calculation.\n<img src=\"https:\/\/i.ibb.co\/dWB96PX\/Screen-Shot-2020-08-19-at-4-44-12-AM.png\" alt=\"Screen-Shot-2020-08-19-at-4-44-12-AM\" border=\"0\">","dd67d022":"<img src=\"https:\/\/i.ibb.co\/CVCNVxS\/Screen-Shot-2020-08-19-at-7-25-13-AM.png\" alt=\"Screen-Shot-2020-08-19-at-7-25-13-AM\" border=\"0\">","73c42786":"### \ud83d\udcc9 Defining the loss function","32a5d38b":"### \ud83e\udd14 What is Attention mechanism?\n#### Before understanding the attention mechanism, first let's take a quick look at Encoder-Decoder architecture.","dd9d5972":"### \ud83e\uddf9 Data preprocessing","ab535d76":"### \ud83d\udc81\ud83c\udffb The heatmap of attention scores tell us how the words in the input Japanese sentence and the predicted English sentence are related.","e9817eef":"### Let's get started straight away!","7690c425":"#### Hmmm... The Japanese sentences tend to have a longer word length than the English sentences.","6ad4297b":"### \u2338 General method for score calculation.\n#### Defining the encoder and decoder models","a542ffe4":"### \ud83d\udd75 How do Attention models work?\n#### Since the attention models are encoder-decoder models with attention mechanism, they function very similar to encoder-decoder models but with a catch. Given a problem of generating an output text sequence from input text sequence (Ex. converting a Japanese sentence to English), let's quickly go through the working of a encoder decoder model step by step:\n1. Encoding: The input sentence is first encoded as a single fixed-length vector.\n2. Score calculation: The encoded input sentence flows through the encoder and decoder. The decoder outputs one value at a time, which is passed on to perhaps more layers before finally outputting a prediction (y) for the current output time step. The alignment model scores (e) how well each encoded input (h) matches the current output of the decoder (s). The calculation of the score requires the output from the decoder from the previous output time step, e.g. s(t-1). When scoring the very first output for the decoder, this will be 0. Scoring is performed using a **function a()**.\n3. Annotation weights calculation: Next, the alignment scores are normalized using a softmax function. The normalization of the scores allows them to be treated like probabilities, indicating the likelihood of each encoded input time step (annotation) being relevant to the current output time step. These normalized scores are called annotation weights.\n4. Context-vector calculation: Next, each annotation (h) is multiplied by the annotation weights (a) to produce a new attended context vector from which the current output time step can be decoded.\n5. Decoding: Decoding is then performed as per the Encoder-Decoder model, although in this case using the attended context vector for the current time step.\n\nFind the original paper here: https:\/\/arxiv.org\/abs\/1706.03762","d2473509":"### \u329f Creating Attention model","b996b6ae":"### \ud834\udf2d Encoder-Decoder model:\n![encoder decoder image](https:\/\/queirozf.com\/images\/contents\/ZZqXAUp.png)\n\n#### The Encoder-Decoder architecture with recurrent neural networks has become an effective and standard approach for both neural machine translation (NMT) and sequence-to-sequence (seq2seq) prediction in general. The key benefits of the approach are the ability to train a single end-to-end model directly on source and target sentences and the ability to handle variable length input and output sequences of text.\n#### An Encoder-Decoder architecture was developed where an input sequence was read in entirety and encoded to a *fixed-length internal representation*. A decoder network then used this internal representation to output words until the end of sequence token was reached. LSTM networks were used for both the encoder and decoder.\n#### It works fairly well but without attention the performance drops as the length of the sentences increase.\n![bleu score encoder-decoder](https:\/\/3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com\/wp-content\/uploads\/2017\/10\/Loss-in-model-skill-with-increased-sentence-length.png)"}}