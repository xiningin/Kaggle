{"cell_type":{"4dc4b73f":"code","62f4c55e":"code","163efbda":"code","99d1123b":"code","dbde596e":"code","1a3a4bfe":"code","c71df681":"code","3d7501f8":"code","bbe739ec":"code","221e3ee0":"code","e5209f6e":"code","d1459d61":"code","5fce22e0":"code","44231b38":"code","ae358163":"code","3b359ca9":"code","8ea14bb7":"code","77b20bcd":"markdown","13f7142c":"markdown","c7c88498":"markdown","777ac736":"markdown","f6889770":"markdown","93d232d7":"markdown","c19c6bb2":"markdown"},"source":{"4dc4b73f":"# We will use the official tokenization script created by the Google team\n!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py","62f4c55e":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\n\nimport tokenization","163efbda":"def bert_encode(texts, tokenizer, max_len=512):\n    \n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n        \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n        ","99d1123b":"def build_model(bert_layer, max_len=512):\n    \n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n    \n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(clf_output)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    \n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model\n    ","dbde596e":"## Load bert from the tensorflow Hub\n\nmodule_url = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)","1a3a4bfe":"## Load CSV files containing data\n\ntrain = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")","c71df681":"## Importing required libaries for preprocssing\n\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nimport re \nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize","3d7501f8":"## Define regular expressions, stopwords and lemmatizer\n\nreplace_by_space_re = re.compile('[\/(){}\\[\\]\\|@,;]')\nbad_symbols_re = re.compile('[^0-9a-z ]')\n#links_re = re.compile('(www|http)\\S+')\nlinks_re = re.compile(r'http\\S+')\n\nStopwords = set(stopwords.words('english'))\nStopwords.remove('no')\nStopwords.remove('not')\n\nlemmatizer = nltk.stem.WordNetLemmatizer()","bbe739ec":"## Function to clean and prepare text\n\ndef text_prepare(text):\n    \"\"\"\n        text: a string\n        \n        return: modified initial string\n    \"\"\"\n    \n    text = text.lower()  # lowercase text\n    text = re.sub(replace_by_space_re,\" \",text) # replace symbols by space\n    text = re.sub(bad_symbols_re, \"\",text) # remove bad symbols\n    text = re.sub(links_re, \"\",text) # remove hyperlinks\n    \n    word_tokens = word_tokenize(text) # Creating word tokens out of the text\n    \n    filtered_tokens=[]\n    for word in word_tokens:\n        if word not in Stopwords:\n            filtered_tokens.append(lemmatizer.lemmatize(word))\n    \n    text = \" \".join(word for word in filtered_tokens)\n    return text","221e3ee0":"vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","e5209f6e":"train[\"text\"] = [text_prepare(x) for x in train[\"text\"]]\ntest[\"text\"] = [text_prepare(x) for x in test[\"text\"]]","d1459d61":"#train_input = bert_encode(train.text.values, tokenizer, max_len=160)\n#test_input = bert_encode(test.text.values, tokenizer, max_len=160)\n\ntrain_input = bert_encode(train[\"text\"], tokenizer, max_len=160)\ntest_input = bert_encode(test[\"text\"], tokenizer, max_len=160)\n\ntrain_labels = train.target.values","5fce22e0":"train_input[2].shape","44231b38":"model = build_model(bert_layer, max_len=160)\nmodel.summary()","ae358163":"checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\n\ntrain_history = model.fit(\n    train_input, train_labels,\n    validation_split=0.2,\n    epochs=5,\n    callbacks=[checkpoint],\n    batch_size=16\n)","3b359ca9":"model.load_weights('model.h5')\ntest_pred = model.predict(test_input)","8ea14bb7":"submission['target'] = test_pred.round().astype(int)\nsubmission.to_csv('submission_BERT.csv', index=False)","77b20bcd":"## Text Cleaning","13f7142c":"## Clean, Encode the text into tokens, masks, and segment flags","c7c88498":"## **Helper Functions**","777ac736":"## About this Kernel\n\nThis is the first time I am implementing the BERT model for NLP. I started with a simple logistic regreesion to solve this problem. Once done I was looking for ways to improve my model accuracy. Then I came across BERT, the state-of-the-art model for solving NLP problems. This solution gave me the public score of 0.828, which is 0.03 more than my vanilla logistic regreesion model\n\nSince this is my first imlementation of BERT, I have used most of the BERT code from [xhlulu](https:\/\/www.kaggle.com\/xhlulu)'s notebook [Disaster NLP: Keras BERT using TFHub](https:\/\/www.kaggle.com\/xhlulu\/disaster-nlp-keras-bert-using-tfhub\/input). THis is a wonderful yet simple notebook covering the core implementation of BERT. Please have a look at it.","f6889770":"## Model: Build, Train, Predict, Submit","93d232d7":"## Load tokenizer from the bert layer","c19c6bb2":"## Loading BERT layer and data"}}