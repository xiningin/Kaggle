{"cell_type":{"d0b90c46":"code","eaf1c7db":"code","ca8adf72":"code","d1b23969":"code","4bc267ae":"code","990a4713":"code","6f5dde3f":"code","ea364cf6":"code","262a6330":"code","06651248":"code","180d36cf":"code","a8069244":"code","6b9a7071":"code","752db312":"code","219d6bc5":"code","001ad554":"code","9922aa30":"markdown","18f3e9db":"markdown","cae5289c":"markdown","830b3a7b":"markdown","d0362421":"markdown","9d416eb7":"markdown","490430f7":"markdown","0a68c00e":"markdown","7c65d8ed":"markdown","719199d6":"markdown","6a11be83":"markdown","5c3f0cee":"markdown","43c94023":"markdown","53f8dfb4":"markdown"},"source":{"d0b90c46":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport os\nimport seaborn as sns\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\ndf = pd.read_csv('..\/input\/covid-vaccinations-by-country\/owid-covid-data.csv')","eaf1c7db":"df.head()","ca8adf72":"for i in df.columns:\n    print(i)\n    \nprint(df.columns.shape)","d1b23969":"USA_filter = df['location'] == 'United States'\nUSA_data = df[USA_filter]\n\nUSA_data.plot(x='date', y=[\"stringency_index\", 'human_development_index'], title=\"United States\")","4bc267ae":"df2 = df[['location', \n          'new_deaths_per_million', \n          'new_cases_per_million', \n          'new_vaccinations_smoothed_per_million',\n          'stringency_index',\n          'gdp_per_capita'\n           ]]","990a4713":"print(df2.shape)\ndf2.head()","6f5dde3f":"grouped = df2.groupby(\n   ['location']\n).agg(\n    {    'new_deaths_per_million':sum,    # Sum duration per group\n         'new_cases_per_million':sum,  # get the count of networks\n        'new_vaccinations_smoothed_per_million':sum,\n        'stringency_index':'mean',\n     'gdp_per_capita':'mean'\n    }\n)","ea364cf6":"grouped","262a6330":"corr = grouped.corr()\ncorr","06651248":"sns.set_theme(style=\"white\")\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(10, 6))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","180d36cf":"high_gdp_filter = grouped['gdp_per_capita']>40000\nhigh_gdp = grouped[high_gdp_filter]\nhigh_gdp.shape","a8069244":"highcorr = high_gdp.corr()\nhighcorr","6b9a7071":"sns.set_theme(style=\"white\")\n\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(highcorr, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(10, 5))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(highcorr, mask=mask, cmap=cmap, vmax=1, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","752db312":"low_gdp_filter = grouped['gdp_per_capita']<2000\nlow_gdp = grouped[low_gdp_filter]\nlow_gdp.shape","219d6bc5":"sns.set_theme(style=\"white\")\n\nlowcorr = low_gdp.corr()\nlowcorr","001ad554":"mask = np.triu(np.ones_like(lowcorr, dtype=bool))\n\nf, ax = plt.subplots(figsize=(10, 5))\n\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\nsns.heatmap(lowcorr, mask=mask, cmap=cmap, vmax=1, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","9922aa30":"I see that it's 193 rows, which is substantially less than the 600's I had before - this tells me that I should have only one of each country.","18f3e9db":"## Reflections and Limitations\n\n### Arbitrary designations of Low GDP and High GDP\nLike I mentioned before, there was definitely a more sophisticated way to find the number needed to find the top and bottom GDP countries. A simple analysis would have helped me determine which number would yield the bottom and top quartiles. \n\n### Low number of countries for analysis\nfor the correlation analyses, N was about 25. That's not at all an impressive sample size to try to make any sort of conjectures and conclusions. The reality of this, of course, was that there are only so many countries to explore.\n\n### So much missing vaccine data\nThis analyses didn't take into account how the vaccine effected the cases. It's important to point out this wasn't meant to analyze numbers over time - it was meant to analyze correlations. However, I think for a better overall picture, this should be a consideration.\n\n### Visualization Skills\nI definitely have a lot more to learn when it comes to data visualization - some of those nuances, like not having the dates crammed together like that first line plot, can make a huge difference in presentation.\n\n\n#### Overall, it was a fun way to practice newskills. Thanks for reading, feedback and suggestions welcome!","cae5289c":"It seems like there's even less to see here in terms of corellations than in the overall data. The new deaths and new cases are still strongly correlated. Not much else.","830b3a7b":"There are a few of these I'm curious about. For example, is the **stringency index** something that remains the same throughout the data set? Or does it fluctuate? Does the **human development index** stay the same or fluctuate?\n\nI'll explore that by slicing the data just for the United States, and plotting a line.","d0362421":"Now I'm interested in seeing correlations across those different numbers I selected. ","9d416eb7":"To pick the number to designate high and low GDP, I just resorted to trial and error. I know there was probably a more sophisticatd way to determine at what GDP I could capture the number of countries I wanted, but for this I just keept playing around with the number and then seeing how many rows the new DF resulted in. \n\nI decided on a GDP per Capita of 40,000 which captures the top 24 contries.","490430f7":"I see one strong correlations that is pretty obvious:\n* New Cases per Million and Deaths Per Million: .83\n\nBut I don't see a lot of other really compelling correlations.\n\nWhat I'd be even more intested in is if these correlations will change if we're dealing with high GDP countries vs low GDP coutnries. Would a high GDP country be **more** sensitive to stringency than a low GDP? Among lower GDP countries, would the GDP have **more** for an effect on cases?","0a68c00e":"## The Data\n[Our World in Data](https:\/\/ourworldindata.org\/coronavirus) generously provides data on COVID Numbers around the world. I thought this data would be a good opportunity to use some concepts I recently learned in Python.\n\n### Goals\nI go into this simple analysis with a few goals in mind\n* Find out how the data is arranged\n* Determine which variables I'm most interested in\n* Determine what kind of manipulation or aggregation needs to be done\n* Find some insights\n\n### Summary\nI do this first by exploring the data through a simple .head() command. I explore a variable that I need to understand better, which is the stringency index. I then extract the relevant variables into a new data frame, group and aggregate, and run and visualize a correlation analysis. I go over the limitations of my methods at the bottom.","7c65d8ed":"Interestingly, there is a LOT more red on this heat map. There is a lot of null data for vaccinations - it is predictable, although unfortunate, that countries with lower GDP's have less access to vaccines than the top GDP countries. \n\nBut the other measures, like new cases and new deaths seem to be ***more*** sensitive to things like stringency and GDP than were the high GDP countries. ","719199d6":"For the Low GDP filter I used the same method - trial and error to find a GDP per Capita number that would yield about 25 countries. That number ended up being 2000.","6a11be83":"## I'll start with see what the data looks like.","5c3f0cee":"It looks like the data is a daily update on things like **new cases**, **new deaths**, **new hospitalizations**. Glancing through, it also looks like the data has some more relative numbers like **\"new_cases_per_million\"** - this makes it useful for controlling for things like population.\n\nThere are a lot of columns. It's easier for me to look through and get sense of what I'm most interested in by having it iterated through and printed line by line, so that's what I'll do.","43c94023":"The data I've curated in this new dataframe consists of daily updates of these specific parameters. I'm intrested in aggregating this so each country has one number to reference. I will do this by grouping by **location** which is the country name, and then selecting the measure.\n\nFor the **deaths per million**, **new cases per million**, **new vaccinations per million**, I'm interested in the sum. However, with **stringency index** and **GDP per capita**, adding each one of those up will result in a huge number that doesn't tell us much. I want to get a sense o what those indeces have been throughout the time the data was collected. So for that, I'll make the measure a mean.","53f8dfb4":"It looks like over time, the human development index remains the same, while the **stringency index** increases. The **human development index** is a stagnant index, so I think I'm more interested in the **stringency index**.\n\nNext I'll create a dataframe with the columns I'm specifically interested in."}}