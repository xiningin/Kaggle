{"cell_type":{"c911438c":"code","d230c5ac":"code","a70b9c8c":"code","2d1f6062":"code","6a9f6331":"code","f6245450":"code","6f6bd964":"code","f8b12ec1":"code","4fa85900":"code","164c2b10":"code","1e0e2253":"code","94518d9c":"code","7e325b1d":"code","f3392013":"code","432ef922":"code","50aa297f":"markdown","ef4a94d0":"markdown","ab30cc0a":"markdown","07b5314e":"markdown","e44467a7":"markdown","782802d4":"markdown","a5376056":"markdown","36a12f7b":"markdown","6f528630":"markdown","9128f7c1":"markdown","bafeaeb3":"markdown"},"source":{"c911438c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d230c5ac":"dataset = pd.read_csv('\/kaggle\/input\/horse-colic\/horse.csv')\ndataset.head()","a70b9c8c":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.countplot(x='outcome',data=dataset,hue='age')","2d1f6062":"dataset.isnull().sum()","6a9f6331":"dataset.dtypes.value_counts()","f6245450":"from sklearn.impute import SimpleImputer\nimp_mode = SimpleImputer(strategy = 'most_frequent')\nfor colname in dataset.columns:\n    if dataset[colname].dtype == object:\n        dataset[colname] = pd.DataFrame(imp_mode.fit_transform(dataset[colname].values.reshape(-1,1)))\ndataset.head()","6f6bd964":"dataset.isnull().sum()","f8b12ec1":"Y = dataset['outcome']\ndataset.drop('outcome',axis= 1, inplace = True)\ndataset = pd.get_dummies(dataset, prefix_sep='_', drop_first=True,dummy_na=False)\ndataset.head()","4fa85900":"dataset = pd.DataFrame(imp_mode.fit_transform(dataset), columns= dataset.columns)\ndataset.head()","164c2b10":"dataset.isnull().sum()","1e0e2253":"dataset['outcome'] = Y\nfrom sklearn.preprocessing import LabelEncoder\nlabelencoder = LabelEncoder()\ndataset['outcome'] = labelencoder.fit_transform(dataset['outcome'])\ndataset.head()","94518d9c":"from sklearn.model_selection import train_test_split\n\nX = dataset.iloc[:,:-1].values\ny = dataset.iloc[:,-1].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 42)","7e325b1d":"from sklearn.tree import DecisionTreeClassifier\nclassifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 42)\nclassifier.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score,classification_report \nprint(\"Decision Tree Accuracy: \",accuracy_score(y_test, y_pred)*100)","f3392013":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred, digits=3))","432ef922":"from sklearn.ensemble import RandomForestClassifier\nclassifierRF = RandomForestClassifier(n_estimators = 100, criterion = 'entropy', random_state = 42)\nclassifierRF.fit(X_train, y_train)\ny_predRF = classifierRF.predict(X_test)\nfrom sklearn.metrics import accuracy_score,classification_report\nprint(\"Random Forest Accuracy: \",accuracy_score(y_test, y_predRF)*100)\nprint(classification_report(y_test, y_predRF, digits=3))","50aa297f":"Let's plot the outcome w.r.t age","ef4a94d0":"Filling NaN for categorical features (Object dtype)","ab30cc0a":"Filling NaN for numerical Features","07b5314e":"Adding outcome to dataset and putting label to it","e44467a7":"Fitting Decision Tree Classification to the Training set and prediction","782802d4":"Approach:\n    1. Identifying NaN columns and respective counts\n    2. Identifying dtype of columns\n    3. Labeling the categorical variables and each variable having n unique values, will be converted into (n-1) features to avoid dummy variable trap. Filling NaN values with Most frequent values of respective columns, This will be done in two steps.\n        * Fill the NaN of categorical features and put the lablel on them. This will enrich our data for most frequent column\n        * Fill the NaN of remaining features (Numerical).\n\n","a5376056":"Verifying the NaN change","36a12f7b":"Verifying there is no NaN","6f528630":"Fitting Random Forest Classification to the Training set","9128f7c1":"Splitting the dataset into the Training set and Test set","bafeaeb3":"Removing outcome from dataset and adding label to categorical features"}}