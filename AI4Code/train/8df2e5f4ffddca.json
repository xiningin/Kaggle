{"cell_type":{"c33ab91b":"code","3cf5e808":"code","38373a39":"code","65b42c4f":"code","78f5f527":"code","f44b73d9":"code","447e5f52":"code","8e7f050f":"code","8546a5b4":"code","287f1376":"code","5b450445":"code","ae6bee5a":"code","3d796429":"code","859aa879":"code","d5713104":"code","ce86f137":"code","634434fe":"code","2dfdaa56":"code","9cc844f9":"code","275b1b79":"code","28f48c13":"code","36afae3a":"code","bd769eb3":"code","6e85a33d":"code","249f620c":"markdown","f096db50":"markdown","c8652991":"markdown","b271a3a6":"markdown","42ea3897":"markdown","3f66b545":"markdown","056d2e85":"markdown","0db86926":"markdown","2d4e8f1e":"markdown","153fad57":"markdown","d601fbc2":"markdown","c30caf26":"markdown","0ef60fa4":"markdown","3e09cea2":"markdown"},"source":{"c33ab91b":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Machine learning libraries\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier\nfrom sklearn.neural_network import MLPClassifier\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","3cf5e808":"# Settings\ntrain_path = os.path.join('..', 'input', 'train.csv')\ntest_path = os.path.join('..', 'input', 'test.csv')\nsubmission_path = os.path.join('..', 'input', 'gender_submission.csv')\n\n# Data loading\ntrain_df = pd.read_csv(train_path)\ntest_df = pd.read_csv(test_path)\nsubmission = pd.read_csv(submission_path)","38373a39":"# Let's look at 10 random rows of train_df\ntrain_df.sample(n=10)","65b42c4f":"# Let's look at the distribution of passenger according to their survival\ntrain_df['Survived'].value_counts().plot.bar()","78f5f527":"# Let's separate our dataset between the ones who survived and the ones that did not\nsurvived_df = train_df[train_df['Survived'] == 1]\ndead_df = train_df[train_df['Survived'] == 0]\n\n# Let's plot a few insights\nfig, axes = plt.subplots(nrows=3, ncols=2, figsize=(15, 5))\nplt.subplots_adjust(hspace=0.6)\n\nsurvived_df['Pclass'].value_counts().sort_index().plot.bar(ax=axes[0][0], title='Survived Pclass')\ndead_df['Pclass'].value_counts().sort_index().plot.bar(ax=axes[0][1], title='Dead Pclass')\n\nsurvived_df['Embarked'].value_counts().sort_index().plot.bar(ax=axes[1][0], title='Survived embarked place')\ndead_df['Embarked'].value_counts().sort_index().plot.bar(ax=axes[1][1], title='Dead embarked place')\n\nsurvived_df['Sex'].value_counts().sort_index().plot.bar(ax=axes[2][0], title='Survived sex')\ndead_df['Sex'].value_counts().sort_index().plot.bar(ax=axes[2][1], title='Dead sex')\n\nplt.show()","f44b73d9":"# Let's plot the survival rate per slice of 5 years in age\nage_hist = []\nfor age in range(0, 80, 5):\n    tmp_df = train_df[train_df['Age'] < age + 5]\n    age_hist += [tmp_df[tmp_df['Age'] >= age]['Survived'].mean()]\n\nplt.plot([age + 2.5 for age in range(0, 80, 5)], age_hist)\nplt.xlabel('Age')\nplt.ylabel('Survival rate')\nplt.title('Survival age per age slices')\nplt.show()","447e5f52":"# While we are working on the age, we need to fill NaN values\n# For that, we will replace NaN values by the median age\nmedian_age = train_df['Age'].median()\nfor df in [train_df, test_df]:\n    df['Age'].fillna(value=median_age, inplace=True)\n\n    # is_child feature creation\n    df['is_child'] = 0\n    df.loc[train_df['Age'] < 15, 'is_child'] = 1\n    \ntrain_df.sample(n=10)","8e7f050f":"train_df['Embarked'].fillna(value=train_df['Embarked'].mode()[0])\ntest_df['Embarked'].fillna(value=test_df['Embarked'].mode()[0])\n\n\ntrain_df = pd.get_dummies(train_df, columns=['Pclass', 'Embarked'])\ntest_df = pd.get_dummies(test_df, columns=['Pclass', 'Embarked'])\n\ndef sex_one_hot_encoding(x):\n    if x == 'female':\n        return 1\n    else:\n        return 0\n    \ntrain_df['Sex'] = train_df['Sex'].apply(sex_one_hot_encoding)\ntest_df['Sex'] = test_df['Sex'].apply(sex_one_hot_encoding)\n\ntrain_df.sample(n=10)","8546a5b4":"for df in [train_df, test_df]:\n    df['family_size'] = df['SibSp'] + df['Parch']\n    \n    df['alone_passenger'] = 0\n    df.loc[df['family_size'] == 0, 'alone_passenger'] = 1\n\nsurvival_rate = []\nfor n in range(train_df['family_size'].max()):\n    survival_rate += [train_df[train_df['family_size'] == n]['Survived'].mean()]\n\n# This graph will give us some insights on the family size impact\nplt.plot([n for n in range(train_df['family_size'].max())], survival_rate)\nplt.xlabel('Family size')\nplt.ylabel('Survival rate')\nplt.title('Survival age per family size')\nplt.show()","287f1376":"def is_small_family(n):\n    if n <= 3:\n        return 1\n    else:\n        return 0\n    \ndef is_medium_family(n):\n    if n > 3 and n <= 5:\n        return 1\n    else:\n        return 0\n    \ndef is_big_family(n):\n    if n > 5:\n        return 1\n    else:\n        return 0\n\nfor df in [train_df, test_df]:\n    df['small_family'] = df['family_size'].apply(is_small_family)\n    df['medium_family'] = df['family_size'].apply(is_medium_family)\n    df['big_family'] = df['family_size'].apply(is_big_family)\n    ","5b450445":"survived = train_df['Survived']\n\ntrain_df.drop(axis=1, columns=['Survived', 'PassengerId', 'Ticket'], inplace=True)\ntest_df.drop(axis=1, columns=['PassengerId', 'Ticket'], inplace=True)\n\ntrain_df.sample(n=10)","ae6bee5a":"def get_deck(cab):\n    if cab[0] == 'T':  # no T deck on the Titanic...\n        return 'U'\n    else:\n        return cab[0]\n\nfor df in [train_df, test_df]:\n    df['Cabin'].fillna('U', inplace=True)\n\n    # is_child feature creation\n    df['Cabin'] = df['Cabin'].apply(get_deck)\n\ntrain_df = pd.get_dummies(train_df, columns=['Cabin'])\ntest_df = pd.get_dummies(test_df, columns=['Cabin'])\n\ntrain_df.sample(n=10)","3d796429":"# What could we create from the Name column ?\n# maybe a feature with the length of the name, one with the particle\ndef get_last_name_length(name):\n    return len(name.split(',')[0])\n\nparticles = ['Mr', 'Mrs', 'Dr', 'Master', 'Miss', 'Ms']\ndef get_particle(name):\n    name = name.split(',')[1]\n    particle = name.split('.')[0]\n    if particle in particles:\n        return particle\n    else:\n        return 'NO'\n\nfor df in [train_df, test_df]:\n    df['name_length'] = df['Name'].apply(get_last_name_length)\n    df['name_particle'] = df['Name'].apply(get_particle)\n    df.drop(axis=1, columns=['Name'], inplace=True)\n    \ntrain_df = pd.get_dummies(train_df, columns=['name_particle'])\ntest_df = pd.get_dummies(test_df, columns=['name_particle'])\n\ntrain_columns = train_df.columns.values\ntest_columns = test_df.columns.values\nfor c in train_columns:\n    if c not in test_columns:\n        test_df[c] = 0\n\nfor c in test_columns:\n    if c not in train_columns:\n        train_df[c] = 0\n\ntrain_df.sample(n=10)","859aa879":"# <=== Logistic Regression ===>\nlogistic_regression = LogisticRegression(solver='lbfgs', max_iter=1000)\nscores = cross_val_score(logistic_regression, train_df, survived, cv=5)\n\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std() * 2))","d5713104":"# <=== Linear SVC ===>\nlinear_svc = LinearSVC(loss='squared_hinge', max_iter=10000)\nscores = cross_val_score(linear_svc, train_df, survived, cv=5)\n\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std() * 2))","ce86f137":"# <=== RandomForestClassifier ===>\nrandom_forest = RandomForestClassifier(n_estimators=125)\nscores = cross_val_score(random_forest, train_df, survived, cv=5)\n\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std() * 2))","634434fe":"# <=== Logistic regression bagging ===>\nbase_clf = LogisticRegression(solver='lbfgs', max_iter=1000)\nlr_bagging = BaggingClassifier(base_clf, n_estimators=11)\nscores = cross_val_score(lr_bagging, train_df, survived, cv=5)\n\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std() * 2))","2dfdaa56":"# <=== Linear SVC bagging ===>\nbase_clf = LinearSVC(loss='squared_hinge', max_iter=10000)\nlsvc_bagging = BaggingClassifier(base_clf, n_estimators=11)\nscores = cross_val_score(lsvc_bagging, train_df, survived, cv=5)\n\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std() * 2))","9cc844f9":"# <=== MLP classifier ===>\nmlp = MLPClassifier(hidden_layer_sizes=(50, 25, 10), activation='relu', solver='adam', max_iter=1000)\nscores = cross_val_score(mlp, train_df, survived, cv=5)\n\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std() * 2))","275b1b79":"# <=== Logistic regression boosting ===>\nbase_clf = LogisticRegression(solver='lbfgs', max_iter=1000)\nlr_boosting = AdaBoostClassifier(base_clf, n_estimators=25)\nscores = cross_val_score(lr_boosting, train_df, survived, cv=5)\n\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std() * 2))","28f48c13":"# Parameter dicts for Grid Search\nlinear_svc_params = {\n    'loss': ['squared_hinge', 'hinge'],\n    'C': [0.25, 1.0, 2.5, 5.0]\n}\n\nrandom_forest_params = {\n    'n_estimators': [125, 151, 175],\n    'min_samples_split' : [2, 3, 4],\n    'min_samples_leaf': [1, 2, 3]\n}\n\nada_boost_params = {\n    'n_estimators': [17, 25, 37],\n}","36afae3a":"linear_svc = LinearSVC(loss='squared_hinge', max_iter=10000)\nlsvc_gs = GridSearchCV(linear_svc, linear_svc_params, n_jobs=4, cv=5)\nlsvc_gs.fit(train_df, survived)\nprint(\"Linear SVC best score : {} --> {}\".format(lsvc_gs.best_score_, lsvc_gs.best_params_))\npred_linear_svc = lsvc_gs.predict(test_df.fillna(value=0))\n\nrandom_forest = RandomForestClassifier(n_estimators=125)\nrf_gs = GridSearchCV(random_forest, random_forest_params, n_jobs=4, cv=5)\nrf_gs.fit(train_df, survived)\nprint(\"Random forest best score : {} --> {}\".format(rf_gs.best_score_, rf_gs.best_params_))\npred_random_forest = rf_gs.predict(test_df.fillna(value=0))\n\nbase_clf = LogisticRegression(solver='lbfgs', max_iter=1000)\nlr_boosting = AdaBoostClassifier(base_clf, n_estimators=25)\nab_gs = GridSearchCV(lr_boosting, ada_boost_params, n_jobs=4, cv=5)\nab_gs.fit(train_df, survived)\nprint(\"Ada boost best score : {} --> {}\".format(ab_gs.best_score_, ab_gs.best_params_))\npred_lr_boosting = ab_gs.predict(test_df.fillna(value=0))","bd769eb3":"# Ensemble voting between our 3 models\npred = pred_linear_svc + pred_random_forest + pred_lr_boosting\npred = (pred >= 2).astype(np.uint8)","6e85a33d":"submission['Survived'] = pred\nsubmission.to_csv('submission.csv', index=False)\n\nsubmission['Survived'] = pred_random_forest\nsubmission.to_csv('submission_rf.csv', index=False)","249f620c":"## Classification\n\nWe will train a few classifiers and score them with a cross-validation strategy\n","f096db50":"Now, let's store the Survived column in another variable and let's drop the *PassengerId* and the *Ticket* columns.","c8652991":"# Feature generation importance\nIn this kernel, I will try to generate useful and meaningful features from the Titanic dataset to boost a classifier.","b271a3a6":"For the cabin, the first letter indicates the deck. This can be an useful information. I will also give the letter U (Unknown) to NaN values as this lack of information may still be useful.","42ea3897":"For each passenger, we have 12 columns.\n* PassengerId : just a Id to differentiate passengers, we will drop that as it does not contain any information\n* Survived : our target column, that we will store in another variable\n* Pclass : categorical data, the class of the passenger\n* Name : the name of the passenger, we will need to work a bit on this column to use it\n* Sex : categorical data, sex of the passenger\n* Age : quantitative data, age of the passenger (WARNING : contains NaN values)\n* SibSp : quantitative data, number of siblings \/ spouses aboard the Titanic\n* Parch : quantitative data, number of parents \/ children aboard the Titanic\n* Ticket : ticket number, this will be hard to use ! Does it even contain any useful information ?\n* Fare : quantitative data\n* Cabin : categorical data, hard to use... maybe with some preporcessing according to the ship architecture\n* Embarked : categorical data, where the passenger embarked","3f66b545":"## Data insights\nTo start, I will look a bit into the given data.","056d2e85":"The two classes have roughly the same magnitude. This will be convenient for the training phase.","0db86926":"Let's do at bit of feature engineering on families : family size, alone passenger","2d4e8f1e":"Now, let's preprocess categorical columns thanks to the *get_dummies* pandas function. This will give us a one-hot encoded version of each of our categorical variables.","153fad57":"Now, we only have the Name column to process before starting the training of our classifier.","d601fbc2":"WIP : still to come --> feature importance, grid search,...","c30caf26":"Here, we will select, linear SVC classifier, the random forest classifier and the ada boost classifier. I will now train them on the whole training dataset and predict the output on the test dataset to get a submission.","0ef60fa4":"As we can see, women have a better survival chance, as a first class passenger also have a better survival chance. These features will be useful for our classifier.","3e09cea2":"Here we can see that children (under 15) have a higher survival rate : we should maybe create a categorical feature *is_child*\n\n## Feature creation\n\nFrom now on, we will always modify the test data as we modify the train data."}}