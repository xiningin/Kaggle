{"cell_type":{"78ce0376":"code","a040219d":"code","d6b48b69":"code","f4aab48c":"code","90446a78":"code","b57b280e":"code","5f7d3ef4":"code","6e92fa63":"code","9191fe68":"code","745e436b":"code","5ed91f55":"code","081c3b18":"code","959448f4":"code","6e57bc35":"code","cf15d997":"code","48088493":"code","c31d15ec":"code","1f0ad8a2":"code","7bf0511f":"code","98e08673":"markdown","f260a2fd":"markdown","01429ecd":"markdown","d2e2d048":"markdown","9eaed867":"markdown","91b44bcd":"markdown","4d15f910":"markdown","a4c9a294":"markdown","7512068e":"markdown","029450d9":"markdown","182a40bd":"markdown","eb9ba4e0":"markdown","07089f7d":"markdown","35884290":"markdown","720d4cfc":"markdown","7c00eb22":"markdown","c4b631ec":"markdown","537bbca1":"markdown","8e551174":"markdown","b8f8b8e1":"markdown","9dfea402":"markdown","dd38666a":"markdown"},"source":{"78ce0376":"#set up nltk\nimport nltk\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('movie_reviews')\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import movie_reviews\n\n#for setting up training and testing data\nimport random\n\n#useful other tools\nimport re\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom itertools import zip_longest\nfrom nltk.probability import FreqDist\nfrom nltk.classify.api import ClassifierI\n","a040219d":"def split_data(data, ratio=0.7): # when the second argument is not given, it defaults to 0.7\n    \"\"\"\n    Given corpus generator and ratio:\n     - partitions the corpus into training data and test data, where the proportion in train is ratio,\n\n    :param data: A corpus generator.\n    :param ratio: The proportion of training documents (default 0.7)\n    :return: a pair (tuple) of lists where the first element of the \n            pair is a list of the training data and the second is a list of the test data.\n    \"\"\"\n    \n    data = list(data)  \n    n = len(data)  \n    train_indices = random.sample(range(n), int(n * ratio))          \n    test_indices = list(set(range(n)) - set(train_indices))    \n    train = [data[i] for i in train_indices]           \n    test = [data[i] for i in test_indices]             \n    return (train, test)                       \n \n\ndef get_train_test_data():\n    \n    #get ids of positive and negative movie reviews\n    pos_review_ids=movie_reviews.fileids('pos')\n    neg_review_ids=movie_reviews.fileids('neg')\n   \n    #split positive and negative data into training and testing sets\n    pos_train_ids, pos_test_ids = split_data(pos_review_ids)\n    neg_train_ids, neg_test_ids = split_data(neg_review_ids)\n    #add labels to the data and concatenate\n    training = [(movie_reviews.words(f),'pos') for f in pos_train_ids]+[(movie_reviews.words(f),'neg') for f in neg_train_ids]\n    testing = [(movie_reviews.words(f),'pos') for f in pos_test_ids]+[(movie_reviews.words(f),'neg') for f in neg_test_ids]\n   \n    return training, testing","d6b48b69":"random.seed(133)#seeding the data so that the results won't change on multiple run on the cell\ntraining_data,testing_data=get_train_test_data()\nprint(\"The amount of training data is {}\".format(len(training_data)))\nprint(\"The amount of testing data is {}\".format(len(testing_data)))\nprint(\"The representation of a single data item is below\")\nprint(training_data[0])","f4aab48c":"from nltk.corpus import stopwords\nstop = stopwords.words('english')\n# function for normalising and filtering the training and testing data \ndef normalise(wordlist):\n    lowered=[word.lower() for word in wordlist] \n    filtered=[word for word in lowered if word.isalpha() and word not in stop]\n    return filtered","90446a78":"training_norm=[(FreqDist(normalise(wordlist)),label) for (wordlist,label) in training_data]\ntesting_norm=[(FreqDist(normalise(wordlist)),label) for (wordlist,label) in testing_data]\n\n#creates instances of FreqDist class\npos_freq_dist=FreqDist()\nneg_freq_dist=FreqDist()\n\n# positive and negative comments are classified into different frequency distribution using the lable\nfor reviewDist,label in training_norm:\n    if label=='pos':\n        pos_freq_dist+=reviewDist\n    else:\n        neg_freq_dist+=reviewDist\n        \n#the differnce of frequency of appearence of each word in the documents are classified into pos_diff and neg_diff\npos_diff = pos_freq_dist - neg_freq_dist\nneg_diff = neg_freq_dist - pos_freq_dist\n\npos_wordlist=[i[0] for i in pos_diff.most_common(10)]\nneg_wordlist=[i[0] for i in neg_diff.most_common(10)]\nprint('Most common positive words are {}'.format(pos_wordlist))\nprint('Most common negative words are {}'.format(neg_wordlist))","b57b280e":"from nltk.classify.api import ClassifierI\n\n#SimpleClassifier inherits ClassifierI(later used for `classify_many` method)\nclass SimpleClassifier(ClassifierI): \n\n    def __init__(self, pos, neg): \n        self._pos = pos \n        self._neg = neg \n\n    def classify(self, doc): \n        #doc is a FreqDist\n        score = 0\n        \n #calculates the total score of the document by anlysing each review classification       \n        for word,value in doc.items():\n            if word in self._pos:\n                score+=value\n            if word in self._neg:\n                score-=value\n        \n        return \"neg\" if score < 0 else \"pos\" \n\n     \n\n    def labels(self): \n        return (\"pos\", \"neg\")\n","5f7d3ef4":"WordListClassifier = SimpleClassifier(pos_wordlist,neg_wordlist)# created an instance of the classifier\nWordListClassifier.classify(FreqDist(training_norm[0][0])) # to check if the code is working as expected a frequency distribution is given to the classifier instance as a parameter","6e92fa63":"def classifier_evaluate(cls, test_data):\n    '''\n    cls: an instance of a classifier object which has a classify method which returns \"pos\" or \"neg\"\n    test_data: a list of pairs where each pair is a FreqDist rep of a doc and its label\n  \n    returns: float point number which is the accuracy of the classifier on the test data provided \n    '''\n    acc = 0 #initiating counter\n    docs,goldstandard=zip(*test_data) #to convert list of pair to pair of list\n    for prediction,goldlabel in zip(predictions,goldstandard):\n        if prediction==goldlabel:\n            acc+=1\n    \n    return acc \/ (len(test_data)) #returns the accuracy ","9191fe68":"docs,goldstandard = zip(*testing_norm) # to seperate the frequency distribution and the label\npredictions = WordListClassifier.classify_many(docs)\nAccuracy1 = classifier_evaluate(WordListClassifier,testing_norm)\nprint('The accuracy of the above classifier is {}'.format(Accuracy1))","745e436b":"class ConfusionMatrix: #defining our own confusion matrix for this analysis\n    def __init__(self,predictions,goldstandard,classes=(\"pos\",\"neg\")):\n    \n        (self.c1,self.c2)=classes\n        self.TP=0\n        self.FP=0\n        self.FN=0\n        self.TN=0\n#counts total number of True positives, True Negatives, False positives and False Negatives        \n        for p,g in zip(predictions,goldstandard):\n            if g==self.c1:\n                if p==self.c1:\n                    self.TP+=1\n                else:\n                    self.FN+=1\n        \n            elif p==self.c1:\n                self.FP+=1\n            else:\n                self.TN+=1\n        \n    \n    def precision(self):#calculates precision\n        p=0\n        \n        p = self.TP \/ (self.TP + self.FP)\n    \n        return p\n  \n    def recall(self):#calculates recall\n        r=0\n        \n        r = self.TP\/(self.TP + self.FN)\n    \n        return r\n  \n    def f1(self):#calculates f-1 score\n        f1=0\n        \n        f1= 2*self.precision()*self.recall()\/(self.precision()+self.recall())\n        return f1 ","5ed91f55":"ConfusionMatrix_1 = ConfusionMatrix(predictions,goldstandard) #instance of Confusion Matrix is generated\nprecision1 = ConfusionMatrix_1.precision()\nrecall1= ConfusionMatrix_1.recall()\nf11= ConfusionMatrix_1.f1()","081c3b18":"print('The Precision of the classifier is {}'.format(ConfusionMatrix_1.precision()))\nprint( 'The recall of the classifier is {}'.format(ConfusionMatrix_1.recall()))\nprint('The f1 of the score the classifier is {}'.format(ConfusionMatrix_1.f1()))\nprint('The accuracy of the classifier is {}'.format(Accuracy1))","959448f4":"#imports NaiveBayesClassifier form nltk module and imports pandas\nimport nltk\nfrom nltk.classify import NaiveBayesClassifier      \nnltk_NB = NaiveBayesClassifier.train(training_norm)\nimport pandas as pd","6e57bc35":"nltk_NB_predictions =nltk_NB.classify_many(docs) # this is the prediction from the naive bayes classifier\nConfusionMatrix_2 = ConfusionMatrix(nltk_NB_predictions,goldstandard) #Using a different instance of confusion matrix to calculate the scores\nprecision2 =ConfusionMatrix_2.precision()\nrecall2 = ConfusionMatrix_2.recall()\nf12 =ConfusionMatrix_2.f1()\nprint(\"The precision of the classifier is {}\".format(precision2))\nprint(\"The recall of the classifier is {}\".format(recall2))\nprint(\"The f1-score of the classifier is {}\".format(f12))","cf15d997":"data = {'WordlistClassifier':[precision1,recall1,f11],'NaiveBayesClassifier':[precision2,recall2,f12] } # creating a dictionary of results from both the classifier\ndf = pd.DataFrame(data, index = ['Precision','Recall','f1-score']) #creates pandas dataframe to compare the scores\ndf","48088493":"#initialising lists so that scores for each loop can be held\nlist_accuracy = []\nlist_precision =[]\nlist_recall = []\nlist_f1 = []\n\nfor i in range(10,101):\n  pos_wordlist =[j[0] for j in pos_diff.most_common(i)]\n  neg_wordlist =[j[0] for j in neg_diff.most_common(i)]\n  WordListClassifier = SimpleClassifier(pos_wordlist,neg_wordlist)#for each loop we have to initiate the instance of WordListClassifier since the lenth of wordlis is changing\n  predictions1 = WordListClassifier.classify_many(docs)\n  list_accuracy.append(classifier_evaluate(WordListClassifier,testing_norm))\n  ConfusionMatrix_3 = ConfusionMatrix(predictions1,goldstandard)\n  list_precision.append(ConfusionMatrix_3.precision())\n  list_recall.append(ConfusionMatrix_3.recall())\n  list_f1.append(ConfusionMatrix_3.f1())","c31d15ec":"wordlist_length = list(range(10,101))","1f0ad8a2":"%matplotlib inline\nfrom matplotlib import pyplot as plt\n#plots precision vs wordlist_length  \nplt.plot(wordlist_length,list_precision , 'go', label = 'Precision')\nplt.xlabel('Word List length')\nplt.legend()\nplt.show()\n\n","7bf0511f":"%matplotlib inline\n#plots recall vs wordlist_length\nplt.plot(wordlist_length,list_recall, 'r', label = 'Recall')\n#plots F1 Score vs Wordlist_length\nplt.plot(wordlist_length, list_f1, label = 'F1 score')\nplt.legend()\nplt.show()","98e08673":"If we had only taken the frequency count in which the words appear in positive and negative comments,then there are many words like movie, films etc. which appears frequently in both the lists and it is not logical to classify the reviews based on this. So, taking frequency difference is the better way to identify the content words indicating positive and negative reviews because the way we use language to express different emotions are different. Hence, the number of times one will use a perticular word in a postive comment will be most probably different from the number of words the person will use in a negative comment.\n","f260a2fd":"When you have run the cell below, your unique training and testing samples will be stored in `training_data` and `testing_data`","01429ecd":"For a future work I will probably recommed Naive bayes classifier over word list classifier assuming we have large enough data available. In word list classifier we are not considering the probability of getting a  positive and negative review. We just build the classifier without considering it. But in Naive Bayes classifier we even consider the data distribution by calculating the probability of occurance of the data. Also from this analysis we can see that the scores are better for naive bayes classifier\n\nSo, hence I recommend Naive Bayes classifier over word list classifier","d2e2d048":"Pandas data frame is used to make a comparision between `WordListClassifier` and the `nltk_NB` (Naive Bayes Classifier). Frome the generated data frame we can clearly observe that for all the parameters (Precision, Recall and F1-Score), NaiveBayesClassifier has the higher values, which clearly indicates that `NaiveBayesClassifier` is better compared to the created `WordListClassifier`\n\nThe highest improvement is result is seen in Recall which means that Naive bayes classifier does a good job in retrieving the relevent instances.","9eaed867":"# SENTIMENT CLASSIFICATION ANALYSIS USING WORDLIST CLASSIFIER AND NAIVE BAYES CLASSIFIER\nHere in this analysis,I am doing the comparison between wordlist classifier and Naive Bayes classifier for sentiment classification using the measurements **Precision**, **Recall**, **Accuracy** and **F-1 Score**. The data used for the analysis is `movie_reviews` from `nltk.corpus`","91b44bcd":"The Calculated Precision, Recall, f1 and accuracy is printed","4d15f910":"Accuracy alone is not a good measure to evaluate a classifier. For example, a classifier which always gives the output as positive review all the time and we are giving an input in which the data contains only 5% actual negative reviews, then, the accuracy of our classifier is 95%!. Accuracy could have been a good measure if the data was balanced. But in real world cases, mostly the data will be imbalanced and measuring only accuracy is not a good practice to evaluate the classifier.To tackle this problem we measure the above defined **precision**, **recall** and **f1-score**.","a4c9a294":"### WORDLIST CLASSIFIER","7512068e":"A `ConfusionMatrix` class is defined so that we can find out the counts of **True positive** , **False Negative** , **False Positive** and **True Negative** and finally calculate **Precision**, **Recall** and **f1** score. \n\n* $Precision\\, (p)$ = $\\frac{TP}{TP+FP}$ = fraction of relevant instances among the retrieved instances\n\n* $Recall\\, (r)$ = $\\frac{TP}{TP+FN}$ =  fraction of relevant instances that were retrieved\n\n* $f1\\, score $ = $\\frac{2\\times p\\times r}{p+r}$ = Harmonic mean of precision and recall","029450d9":"From nltk.corpus stopwords module has been imported and a function ```normalise\n``` has been created, which lowers each word in the wordlist and removes the common stopwords in the english language","182a40bd":"To calculate the accuracy of the classifier `WordListClassifier` a function `classifier_evaluate` is generated. The `classifier_evaluate` takes in the classifer we created and the test_data as parameters. From the test_data the Frequency distribution and the label ('pos' or 'neg') is extracted. For each prediction by the classifier, the prediction is compared with the actual label and if both are same, counter `acc` is added by one.\n\nThe accuracy of the classifier is defined as $\\frac{Number\\thinspace  of \\thinspace correct \\thinspace predictions}{length \\thinspace of \\, data}$","eb9ba4e0":"An instance of confusion matrix `ConfusionMatrix_1` is made and the precision, recall and f1 score is calculated by calling the corresponding classes in the `ConfusionMatrix`. The prediction by the `WordListClassifier` and the actual label is passed to the confusion matrix as the parameters to calculate the scores.","07089f7d":"To generate a wordlist classifier class, `ClassifierI` parent class imported from `nltk.classify.api` and new method `classify` is defined. The method takes the frequency distribution of the words as the parameter and increment or decrement a counter with the frequency of each word, depending upon whether the word actually belongs to the positive wordlist or negative wordlist. In this classifier I have not considered the the words which doesn't belong to either positive or negative wordlist. One way is to randomly assign those words as either positive or negative and count the score, but since the value we count is the frequency of the occurance of that word and not +1 or -1 it might create an unnecessary weightage to either positive or negative review if the freqency of the word which is not in either list is so large. \n\nAfter creating the classifier class, and instance `WordlistClassifier` is created.","35884290":"I have created a dictionary of results so that it can be easily converted to a pandas data frame and we can compare the wordlist classifier and Naive Bayes Classifier side by side","720d4cfc":"To Understand the impact of the length of the wordlists on the wordlist classifier, I have created 90 instances of positive and negative word lists using a for-loop with its length increasing by 1 each time. The words are selected in such that the high frequncy words are selected initially and as the word list grows it selects words with comparatively less freqency.\n\nIn each loop, for each word lists the **accuracy**, **Precision** , **Recall** and **F1-score** is calculated using instance of `ConfusionMatrix`. The results for each are stored in different lists using .append() method.","7c00eb22":"### CONCLUSION","c4b631ec":"`training_norm` and `testing_norm`, which are list of tuple containing frequency distribution and its correponding Label (ie positive or negative review) is created using list comprehension and then all the frequency distribution curresponding to positive and negative reviews are summed up. Finally, We finds the content words corresponding to positive and negative feedback by finding out the difference in frequencies of in which they occure in the training sample. The code is given below.","537bbca1":"#### IMPACT OF WORDLIST LENGTH ON WORDLIST CLASSIFIER","8e551174":"An instance of `ConfusionMatrix` is used calculate the **precision**, **recall** , and **f1-score** of the prediction by the Naive Bayes Classifier. Here we are using the same confusion matrix which we created earlier for calculating the different scores, but we could have also used the inbulit functions in the naive bayes classifier to calculate the same. Both will give the same results.","b8f8b8e1":"### NAIVE BAYES CLASSIFIER","9dfea402":"Using matplotlib the graphs for Accuracpy, Precision, Recall and F1 score is plotted and the following results were observed\n\n* The precision decreases from 0.59 to 0.54 and then increases again and maintains almost a steaddy value at 0.55 and then decreases again till 0.54\n\n* There is no much change in the F1 score and is almost steady at 0.70\n\n* The recall is increasing as the wordlist increases from 0.9 to 0.95\n","dd38666a":"From `nltk.classify` the ` NaiveBayesClassifier `is  imported and an instance of the imported class `nltk_NB` is created. The Naive Bayes Classifier is then trained using the training data and then the predictions are done on the test data using classify_many method of `NaiveBayesClassifier`"}}