{"cell_type":{"d8be98af":"code","cbd9419b":"code","772583e2":"code","6158885a":"code","90184806":"code","fa079b95":"code","cf6daf32":"code","22f1f60f":"code","06ccb5dc":"code","99fd598b":"code","f92519c1":"code","6a8d07ec":"code","fd6a6751":"code","77b181c2":"code","054cd01b":"code","2c80b6bf":"code","32f8218d":"markdown"},"source":{"d8be98af":"\"\"\" Training scrip(t in Tensorflow \"\"\"\nimport os\nimport logging\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras import optimizers, losses, metrics\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.applications.vgg16 import preprocess_input","cbd9419b":"train_dir = \"..\/input\/intel-image-classification\/seg_train\/seg_train\"\ntest_dir = \"..\/input\/intel-image-classification\/seg_test\/seg_test\"\nepochs=50\nimg_size = (150, 150)\nimg_shape = (150, 150, 3)\nbatch_size = 256\nnum_classes = len(os.listdir(train_dir))\nidx_to_name = os.listdir(train_dir)\nname_to_idx = dict([(v, k) for k, v in enumerate(idx_to_name)])","772583e2":"name_to_idx","6158885a":"def data_to_df(data_dir, subset=None, train_size=None):\n    df = pd.DataFrame()\n    filenames = []\n    labels = []\n    \n    for dataset in os.listdir(data_dir):\n        img_list = os.listdir(os.path.join(data_dir, dataset))\n        label = name_to_idx[dataset]\n        \n        for image in img_list:\n            filenames.append(os.path.join(data_dir, dataset, image))\n            labels.append(label)\n        \n    df[\"filenames\"] = filenames\n    df[\"labels\"] = labels\n    \n    if subset == \"train\":\n        train_df, val_df = train_test_split(df, train_size=0.8, shuffle=True,\n                                            random_state=10)\n        return train_df, val_df\n    \n    return df\n\nprint(\"Converting data directory to dataframe\")\ntrain_df, val_df = data_to_df(train_dir, subset=\"train\", train_size=0.8)","90184806":"class CustomDataGenerator(tf.keras.utils.Sequence):\n\n    ''' Custom DataGenerator to load img \n    \n    Arguments:\n        data_frame = pandas data frame in filenames and labels format\n        batch_size = divide data in batches\n        shuffle = shuffle data before loading\n        img_shape = image shape in (h, w, d) format\n        augmentation = data augmentation to make model rebust to overfitting\n    \n    Output:\n        Img: numpy array of image\n        label : output label for image\n    '''\n    \n    def __init__(self, data_frame, batch_size=10, img_shape=None, augmentation=True, num_classes=None):\n        self.data_frame = data_frame\n        self.train_len = self.data_frame.shape[0]\n        self.batch_size = batch_size\n        self.img_shape = img_shape\n        self.num_classes = num_classes\n        print(f\"Found {self.data_frame.shape[0]} images belonging to {self.num_classes} classes\")\n\n    def __len__(self):\n        ''' return total number of batches '''\n        self.data_frame = shuffle(self.data_frame)\n        return int(self.train_len\/self.batch_size)\n\n    def on_epoch_end(self):\n        ''' shuffle data after every epoch '''\n        # fix on epoch end it's not working, adding shuffle in len for alternative\n        pass\n    \n    def __data_augmentation(self, img):\n        ''' function for apply some data augmentation '''\n        img = tf.keras.preprocessing.image.random_shift(img, 0.2, 0.3)\n        img = tf.image.random_flip_left_right(img)\n        img = tf.image.random_flip_up_down(img)\n        return img\n        \n    def __get_image(self, file_id):\n        \"\"\" open image with file_id path and apply data augmentation \"\"\"\n        img = np.asarray(Image.open(file_id))\n        img = np.resize(img, self.img_shape)\n        img = self.__data_augmentation(img)\n        img = preprocess_input(img)\n\n        return img\n\n    def __get_label(self, label_id):\n        \"\"\" uncomment the below line to convert label into categorical format \"\"\"\n        #label_id = tf.keras.utils.to_categorical(label_id, num_classes)\n        return label_id\n\n    def __getitem__(self, idx):\n        batch_x = self.data_frame[\"filenames\"][idx * self.batch_size:(idx + 1) * self.batch_size]\n        batch_y = self.data_frame[\"labels\"][idx * self.batch_size:(idx + 1) * self.batch_size]\n        # read your data here using the batch lists, batch_x and batch_y\n        x = [self.__get_image(file_id) for file_id in batch_x] \n        y = [self.__get_label(label_id) for label_id in batch_y]\n\n        return np.array(x), np.array(y)","fa079b95":"train_data = CustomDataGenerator(train_df, \n                                 batch_size=batch_size, \n                                 img_shape=img_shape,\n                                 num_classes=num_classes)\nval_data = CustomDataGenerator(val_df, \n                               batch_size=batch_size, img_shape=img_shape, num_classes=num_classes)","cf6daf32":"def build_model(img_shape):\n    inputs = layers.Input(shape=img_shape)\n    out = layers.Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(inputs)\n    out = layers.Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(out)\n    out = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(out)\n\n    out = layers.Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(out)\n    out = layers.Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(out)\n    out = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(out)\n\n    out = layers.Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(out)\n    out = layers.Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(out)\n    out = layers.Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(out)\n    out = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(out)\n\n    out = layers.Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(out)\n    out = layers.Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(out)\n    out = layers.Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(out)\n    out = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(out)\n\n    out = layers.Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(out)\n    out = layers.Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(out)\n    out = layers.Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(out)\n    out = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(out)\n    out = layers.GlobalAveragePooling2D()(out)\n    out = layers.Dense(128, activation=\"relu\")(out)\n    out = layers.Dropout(0.5)(out)\n    outputs = layers.Dense(6, activation=\"softmax\")(out)\n    \n    return tf.keras.Model(inputs, outputs, name=\"VGG16\")","22f1f60f":"model = build_model(img_shape)","06ccb5dc":"model.summary()","99fd598b":"optimizer = optimizers.Adam()\nloss_fn = losses.SparseCategoricalCrossentropy(from_logits=True)\ntrain_acc_metrics = metrics.SparseCategoricalAccuracy()\nval_acc_metrics = metrics.SparseCategoricalAccuracy()","f92519c1":"@tf.function\ndef train_step(x, y):\n    with tf.GradientTape() as tape:\n        logits = model(x, training=True)\n        loss_value = loss_fn(y, logits)\n    grads = tape.gradient(loss_value, model.trainable_weights)\n    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n    train_acc_metrics.update_state(y, logits)\n    return loss_value\n\n@tf.function\ndef test_step(x, y):\n    val_logits = model(x, training=False)\n    val_acc_metrics.update_state(y, val_logits)","6a8d07ec":"epochs = 10\nimport time\nfor epoch in range(epochs):\n    print(f\"Epoch :{epoch}\/{epochs}\")\n    start_time = time.perf_counter()\n    for step, (x_batch_train, y_batch_train) in enumerate(train_data):\n        loss_value = train_step(x_batch_train, y_batch_train)\n        \n        if (step % 50) == 0: \n            print(f\"Step: {step} Training loss :{loss_value}\")\n            print(f\"Seen so far: {(step + 1) * batch_size} samples\") \n    \n    train_acc = train_acc_metrics.result()\n    print(\"Training Accuracy:\", float(train_acc))\n    train_acc_metrics.reset_states()\n\n    for x_batch_val, y_batch_val in val_data:\n        test_step(x_batch_val, y_batch_val)\n    \n    val_acc = val_acc_metrics.result()\n    print(\"Validation Accuracy: \", float(val_acc))\n    val_acc_metrics.reset_states()\n\n    print(\"Time Taken\", time.perf_counter() - start_time)\n    print(\"*\"*80)","fd6a6751":"model.compile(optimizer=optimizer, loss=loss_fn)\nmodel.save(\"saved_model\")","77b181c2":"test_df = data_to_df(test_dir)\ntest_data = CustomDataGenerator(test_df, \n                               batch_size=batch_size, img_shape=img_shape, num_classes=num_classes)","054cd01b":"def model_evalution(test_data):\n    \"\"\" function to test the loss and accuracy on validation data \"\"\"\n    for X_test, y_test in val_data:\n        y_pred = model(X_test, training=False)\n        val_acc_metrics.update_state(y_test, y_pred)\n        accuracy = val_acc_metrics.result()\n    \n    return float(accuracy)","2c80b6bf":"print(\"Accuracy on Test DataSet\", model_evalution(test_data))","32f8218d":"## Intel image classification\n\nIntel Image Classification Using Custom Dataloader and Tensorflow traning loop and model evlaution functoion to write code from scratch to understand what's going inside."}}