{"cell_type":{"d331ecbb":"code","5e56385b":"code","c5e0a9d0":"code","19550b21":"code","87da719c":"code","a5ab8497":"code","b5b327a8":"code","0d393020":"code","83622276":"code","860ff31f":"code","3ab7cd82":"code","eec86068":"code","17fa5215":"code","08f3fc2e":"code","2f1c7e03":"code","938851b9":"code","85f7b872":"code","8295c596":"code","0c5f7c50":"code","47e1550d":"code","c9410aa5":"code","1e4b4f14":"code","921dac68":"code","df4f2b41":"code","d298104a":"code","7e073f06":"code","6595ccd9":"code","711e7b27":"code","30ad732b":"code","cdc37d61":"code","636bee0d":"code","befb39a1":"code","78119337":"code","bfa501af":"code","9e3d1cef":"code","c15698a3":"code","30120fc2":"code","7be3b639":"code","19c4f66a":"code","e2f743b2":"code","2ded2589":"code","7d407368":"code","0f2b95c1":"code","34d49aa6":"code","393c165b":"code","c086375b":"code","cf5a29e3":"code","8316b85d":"code","1e74af60":"code","e9035940":"code","feb77893":"code","530d5243":"code","59df5d5f":"code","16cd09d2":"code","dcf8ae45":"code","833af7a3":"code","83dc0fb8":"code","ba0700af":"code","73b4e1c1":"code","bfb9e098":"code","b0e1a7e2":"code","76fa2dc1":"code","7201a4b3":"code","6319bae6":"code","7590999d":"markdown","c1d9194a":"markdown","068772d3":"markdown","451f2b69":"markdown","0ae2dc4e":"markdown","c44f5b86":"markdown","62ace45e":"markdown","b8f7e9a7":"markdown","4a44b1ef":"markdown","58bd8430":"markdown","7d6eb78b":"markdown","4600fb96":"markdown","3819b392":"markdown","597d3712":"markdown","f8ec7c9a":"markdown","807958d3":"markdown","8d294db4":"markdown","1af76177":"markdown","09f295ee":"markdown","2eb16e02":"markdown","e4b83d86":"markdown","6bc98836":"markdown","483b708a":"markdown","af8d956a":"markdown","39ade4d1":"markdown","58607a2c":"markdown","bd7bc142":"markdown","dd4c2982":"markdown","dabb9db0":"markdown","2001b91d":"markdown","fd7626f8":"markdown","a9efeac4":"markdown","6259b8c6":"markdown","2e338a49":"markdown","18088a15":"markdown","b83b675e":"markdown","02062071":"markdown","d1fccf3f":"markdown","b0c17a1d":"markdown","688c20f3":"markdown","103f0119":"markdown","4725a052":"markdown","aa7bf49c":"markdown","0258902d":"markdown","1c222008":"markdown","eb4f7a50":"markdown","19536f51":"markdown","33f75a7a":"markdown","a728dd13":"markdown","fddec81d":"markdown","2d448bed":"markdown","dff737fc":"markdown","0b5a9bbb":"markdown","798c2af8":"markdown","3172dd61":"markdown","1f2c50dd":"markdown","b833a084":"markdown","1bd4c442":"markdown","fe481fee":"markdown","d57c3091":"markdown"},"source":{"d331ecbb":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","5e56385b":"customer_info = pd.read_csv('..\/input\/customer-clustering\/segmentation data.csv')\ncustomer_info.sample(5)","c5e0a9d0":"customer_info.info()","19550b21":"num_columns = ['ID','Age','Income']\ncat_columns = ['Sex','Marital status','Education','Occupation','Settlement size']\n\ncustomer_info[cat_columns] = customer_info[cat_columns].astype('str')","87da719c":"customer_info.describe(include='object').T","a5ab8497":"customer_info.describe().T","b5b327a8":"for numerical in num_columns:\n    plt.figure(figsize=(8,4))\n    sns.histplot(data=customer_info, x=numerical)\n    plt.savefig(numerical + '.png')","0d393020":"for category in cat_columns:\n    plt.figure(figsize=(8,4))\n    sns.countplot(data=customer_info, x=category)","83622276":"sns.scatterplot(x='Age', y='Income', data=customer_info)","860ff31f":"import scipy.stats as stats\nprint(stats.pearsonr(customer_info['Age'], customer_info['Income']))","3ab7cd82":"sns.lmplot(x='Age', y='Income', data=customer_info)","eec86068":"for category in cat_columns:\n    for numerical in num_columns:\n        if numerical != 'ID':\n            plt.figure(figsize=(8,4))\n            sns.kdeplot(data=customer_info, x=numerical, hue=category) ## Non-normalized version of this plot by switching kdeplot for histplot","17fa5215":"cat_columns","08f3fc2e":"cat_aux = cat_columns.copy()\nfor category1 in cat_columns:\n    cat_aux.pop(0);\n    for category2 in cat_aux:\n        if category1 != category2:\n            plt.figure(figsize=(8,4))\n            sns.countplot(data=customer_info, x=category1, hue=category2)","2f1c7e03":"def bivariate_scatter(x, y, hue, df):\n    plt.figure(figsize=(6,6))\n    sns.scatterplot(x=x, y=y, data=df, hue=hue, alpha=0.85)","938851b9":"for cat in cat_columns:\n    bivariate_scatter('Age', 'Income', cat, customer_info)","85f7b872":"from scipy import stats","8295c596":"normaltest_result_income = stats.normaltest(customer_info['Income'])[1]\nnormaltest_result_age    = stats.normaltest(customer_info['Age'])[1]\n\nprint(f'The p-value for the null hypothesis of the Income being Normally distributed is {normaltest_result_income}')\nprint(f'The p-value for the null hypothesis of the Age    being Normally distributed is {normaltest_result_age}')","0c5f7c50":"def apply_log(column):\n    return np.log(column)\n\ndef normality_test(column):\n    return stats.normaltest(column)","47e1550d":"from sklearn.preprocessing import PowerTransformer","c9410aa5":"# PowerTransform data\nfeature = customer_info['Income'].to_numpy().reshape(-1,1)\n\npowtr = PowerTransformer()\nfeature_transf = powtr.fit_transform(feature)\narray_1d = feature_transf.flatten()\nfeature = pd.Series(data=array_1d, index=list(range(len(array_1d))))\n\n# Log Transform data\nlog_transformed_income = apply_log(customer_info['Income'])\n\n# Create axis for original data plot (ax1) and transformed data (ax2)\nfig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(15,4));\n\n# Plot original data & get metrics\ncustomer_info['Income'].plot(kind='hist', ax=ax1)\nax1.title.set_text('Original data')\nnorm_test1 = normality_test(customer_info['Income'])\n\n# Plot log transformed data & get metrics\nlog_transformed_income.plot(kind='hist', ax=ax2);\nax2.title.set_text('Log Transformed data')\nnorm_test2 = normality_test(log_transformed_income)\n\n# Plot power transformed data & get metrics\nfeature.plot(kind='hist', ax=ax3);\nax3.title.set_text('PowerTransformed data')\nnorm_test3 = normality_test(feature)\n\n# Create a DataFrame that shows normality test results for each tranformation\nnorm_results = [norm_test1, norm_test2, norm_test3]\nmetrics = pd.DataFrame(norm_results, index=['Original data', 'Log transform', 'PowerTransformer'])","1e4b4f14":"normality_test(log_transformed_income)","921dac68":"metrics","df4f2b41":"# PowerTransform data\nfeature2 = customer_info['Age'].to_numpy().reshape(-1,1)\n\npowtr = PowerTransformer()\nfeature_transf = powtr.fit_transform(feature2)\narray_1d = feature_transf.flatten()\nfeature2 = pd.Series(data=array_1d, index=list(range(len(array_1d))))\n\n# Log Transform data\nlog_transformed_age = apply_log(customer_info['Age'])\n\n# Create axis for original data plot (ax1) and transformed data (ax2)\nfig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(15,4));\n\n# Plot original data & get metrics\ncustomer_info['Age'].plot(kind='hist', ax=ax1)\nax1.title.set_text('Original data')\nnorm_test1 = normality_test(customer_info['Age'])\n\n# Plot log transformed data & get metrics\nlog_transformed_age.plot(kind='hist', ax=ax2);\nax2.title.set_text('Log Transformed data')\nnorm_test2 = normality_test(log_transformed_age)\n\n# Plot power transformed data & get metrics\nfeature2.plot(kind='hist', ax=ax3);\nax3.title.set_text('PowerTransformed data')\nnorm_test3 = normality_test(feature2)\n\n# Create a DataFrame that shows normality test results for each tranformation\nnorm_results = [norm_test1, norm_test2, norm_test3]\nmetrics = pd.DataFrame(norm_results, index=['Original data', 'Log transform', 'PowerTransformer'])","d298104a":"metrics","7e073f06":"customer_info['transf_income'] = feature\ncustomer_info['transf_age']    = log_transformed_age","6595ccd9":"customer_transformed = customer_info.drop(['Income', 'Age', 'ID'], axis=1)\ncustomer_transformed","711e7b27":"from sklearn.preprocessing import MinMaxScaler","30ad732b":"scaler = MinMaxScaler()\nX = scaler.fit_transform(customer_transformed)","cdc37d61":"X[0]","636bee0d":"from sklearn.cluster import KMeans","befb39a1":"clusters_range=[2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18]\ninertias=[]\n\nfor c in clusters_range:\n    kmeans=KMeans(n_clusters=c, random_state=0).fit(X)\n    inertias.append(kmeans.inertia_)\n\nplt.figure(figsize=(7,7))\nplt.plot(clusters_range,inertias, marker='o')","78119337":"from sklearn.metrics import silhouette_samples, silhouette_score\n\nclusters_range=range(2, 20)\nrandom_range  =range(0, 20)\nresults=[]\n\nfor c in clusters_range:\n    for r in random_range:\n        clusterer=KMeans(n_clusters=c, random_state=r)\n        cluster_labels=clusterer.fit_predict(X)\n        silhouette_avg=silhouette_score(X, cluster_labels)\n        results.append([c,r,silhouette_avg])\n\nresult  =pd.DataFrame(results, columns=[\"n_clusters\",\"seed\",\"silhouette_score\"])\npivot_km=pd.pivot_table(result, index=\"n_clusters\", columns=\"seed\",values=\"silhouette_score\")\n\nplt.figure(figsize=(15, 6))\nsns.heatmap(pivot_km, annot=True, linewidths=.5, fmt='.3f', cmap=sns.cm.rocket_r)\nplt.tight_layout()","bfa501af":"from sklearn.decomposition import PCA","9e3d1cef":"pca = PCA(n_components=3, random_state=42)\nX_pca = pca.fit_transform(X)","c15698a3":"X_pca_df = pd.DataFrame(data=X_pca, columns=['X1', 'X2', 'X3'])","30120fc2":"kmeans=KMeans(n_clusters=6, random_state=0).fit(X)","7be3b639":"labels = kmeans.labels_\nX_pca_df['Labels'] = labels","19c4f66a":"from plotly import __version__\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport cufflinks as cf\ninit_notebook_mode(connected=True)\ncf.go_offline()","e2f743b2":"X_pca_df.head()","2ded2589":"X_pca_df['Labels'] = X_pca_df['Labels'].astype(str)","7d407368":"import plotly.express as px\n\nfig = px.scatter_3d(X_pca_df, x='X1', y='X2', z='X3',\n              color=X_pca_df['Labels'])\nfig.show()","0f2b95c1":"results_df = customer_info.drop(['ID', 'transf_income', 'transf_age'], axis=1)\nresults_df['Labels'] = kmeans.labels_\nresults_df = results_df.astype({'Sex':'int32', 'Marital status':'int32', 'Education':'int32', 'Occupation':'int32', 'Settlement size':'int32'})\nresults_df.info()","34d49aa6":"summary = {}\n\nfor index in range(6):\n    summary[index] = results_df[results_df['Labels'] == index].describe().T  # .describe method provides general statistics about the data","393c165b":"summary[0]","c086375b":"results_df[results_df['Labels'] == 0].hist(figsize=(15,15));","cf5a29e3":"summary[3]","8316b85d":"results_df[results_df['Labels'] == 3].hist(figsize=(15,15));","1e74af60":"summary[1]","e9035940":"results_df[results_df['Labels'] == 1].hist(figsize=(15,15));","feb77893":"summary[2]","530d5243":"results_df[results_df['Labels'] == 2].hist(figsize=(15,15));\nplt.savefig('cluster4_results')","59df5d5f":"from scipy.stats import ttest_ind\nimport numpy as np\n\nage_cluster_0 = results_df[results_df['Labels'] == 0]['Age']\nage_cluster_1 = results_df[results_df['Labels'] == 1]['Age']\nage_cluster_2 = results_df[results_df['Labels'] == 2]['Age']\nage_cluster_3 = results_df[results_df['Labels'] == 3]['Age']\n\nfirst_clusters  = [1, 2]\nsecond_clusters = [0, 3]\nreal_clusters = [age_cluster_0, age_cluster_1, age_cluster_2, age_cluster_3]\n\nfor num_clust_1 in first_clusters:\n    for num_clust_2 in second_clusters:\n        ttest,pval = ttest_ind(real_clusters[num_clust_1], real_clusters[num_clust_2])\n        print(f'p-value of {num_clust_1} vs {num_clust_2} is:',pval)\n        if pval <0.05:\n            print(\"we reject null hypothesis\")\n        else:\n            print(\"we accept null hypothesis\")\n","16cd09d2":"summary[4]","dcf8ae45":"results_df[results_df['Labels'] == 4].hist(figsize=(15,15));","833af7a3":"summary[5]","83dc0fb8":"results_df[results_df['Labels'] == 5].hist(figsize=(15,15));","ba0700af":"centroids = kmeans.cluster_centers_\npd.DataFrame(centroids, columns = results_df.columns[:7], index = ['C0', 'C1', 'C2', 'C3', 'C4', 'C5'])","73b4e1c1":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\nimport graphviz","bfb9e098":"clf = DecisionTreeClassifier(max_depth = 4, min_samples_leaf = 5)","b0e1a7e2":"X_clusters = results_df.drop('Labels', axis=1)\ny_clusters = results_df['Labels']\n\nclf.fit(X_clusters, y_clusters)","76fa2dc1":"from sklearn.metrics import classification_report, confusion_matrix","7201a4b3":"predictions = clf.predict(X_clusters)\nprint(classification_report(y_clusters, predictions))","6319bae6":"# DOT data\ndot_data = tree.export_graphviz(clf, out_file=None, \n                                feature_names=results_df.columns[:7],  \n                                class_names=['Cluster 0', 'Cluster 1', 'Cluster 2', 'Cluster 3', 'Cluster 4', 'Cluster 5'],\n                                filled=True)\n\n# Draw graph\ngraph = graphviz.Source(dot_data, format=\"png\") \ngraph\n#plt.savefig('DecisionTree.png')","7590999d":"There are a few interesting relationships that can be drawn from the data:\n\n1) People with Occupation = '0' (unemployed) tend to live in small cities within the customers\n2) Married people tend to have either highschool-level or post-graduate-level educations within the customers\n3) Married people also tend to live in smaller cities\n4) Women within the customer list tend to be married more often than men\n5) Males tend to be employed more often within the customer list. Women within the dataset, on the other hand, tend to be unemployed more often","c1d9194a":"### Missing values\n\nWe've seen from the .info() method that there are no missing values.\n\n## EDA\n\nWe start by taking a look at the distributions of the numerical variables Age, ID and Income. \n\n- We notice ID has a uniform distribution, which makes sense since it is an identifier of the customer and it will be dropped below.\n- The variable 'Age' has a heavy right skew, generated because of the lower limit at zero of the variable. If we are using K-Means clustering, there will be no need to normalize the feature, but we may have to do so for other models.\n- The feature 'Income' has the same right skew problem as the 'Age' feature. We'll have to be wary of this depending on the model we select.","068772d3":"### Clusters according to the Decision Tree:\n\nBelow you can see the interpretation of each cluster depending on the features that separate the customers in either group. You can see that even though we used different methods to achieve the grouping descriptions, the results were almost identical. The interesting fact about the Decision Tree is that it can be pruned to take only the most important features into account, and so it provides a much simpler version that can be summarized better, as it is done below. We will stay with the clusters obtained by this method for the reasons explained before.\n\nNotice that there are clusters like number 4, where you only need to know 2 features about the customer to group them into cluster 4. This is a very simple rule to use in practice.\n\n### Summarized Clusters:\n\nCluster 0 --> Single + Small city + Male + Employed (employee\/self-employed) >>> (we ignore Income as both leafs return the Cluster 0 label)\n\nCluster 1 --> Non-single + Female + small city + unemployed or employees (there are very little cases of medium\/big city + unemployed so we discard them)\n                                                  \nCluster 2 --> Non-single + Female + medium\/large city + management\/self-employed (there are very little cases of small city + self-employed so we discard them)\n\nCluster 3 --> Single + Medium\/big city + Male + Employed (employee or self-employed)\n\nCluster 4 --> Non-single + Males\n\nCluster 5 --> Single + Small city + Female (there are very little cases of medium\/big city + Female so we discard them)","451f2b69":"We should first notice that the clusters are quite separated and the algorithm seems to be doing the cluster separation well, as the frontiers between clusters seem quite clear.\n\nNow we should create a Results DataFrame that includes the labels and apply filtering methods to infer information about the clusters provided by the K-Means model. Our objective is to define what type of customer is reflected in each cluster!","0ae2dc4e":"Above, we created a dictionary called summary which contains all the central tendency statistics we wanted to know. Min and Max statistics give us an idea of the range of the data, while the mean and median give us the central tendency of the data. Finally, the standard deviation gives us a notion of variance within the data.","c44f5b86":"### Multivariate Analysis","62ace45e":"### Interpreting results\n\nNow, let's try to interpret the results we got for each cluster with the help of this statistics and our general knowledge of the data. Remember we gathered some information about the data during the data visualization section that may come in handy here. For example, we noticed that 'Age' wasn't a good indicator of separation from the clusters as data points from this feature within the same age range belonged to different categories.\n\nWe will use **two strategies to interpret the data points**. The first is using the summary descriptive statistics from the **.describe()** method. This gives us a good general idea of the data within each label, but it's also incomplete, as we don't have an idea of distribution. Therefore, we will make use of the **.hist()** method for DataFrames to add another layer of depth into our analysis with the distribution of the data.\n\nNotice the clusters are not analyzed in order, due to the fact that I've already conducted a quick analysis to make sense of the data.","b8f7e9a7":"The model seems extremely accurate at predicting the values of the classes, so we can expect the split points to be quite accurate as well. We will proceed with the interpretation of the model using this technique.","4a44b1ef":"### Conclusions\n\nAfter running the tests, we notice that the data isn't normally distributed yet, so neither the log transformation or the PowerTransformer were able to get it to a full normal distribution. Even though we get data that still isn't normally distributed, it has improved significantly from the initial tests. This means that our transformed data is a better approximation to normally distributed data than the original data, so we will use this transformed data instead. \n\n- For the 'Age' feature, we transform the data using the Log Transform method with our function\n- For the 'Income' feature, we use the PowerTransformer from scikit learn","58bd8430":"The p-value for both features is extremely low, and verifies p-value(Income) << p-value(Age) << 0.05. This means that we can reject the hypothesis that the data is normally distributed, so we know with extreme certainty that the data is **NOT** normally distributed. \n\nThis means we need to transform the data and check for normallity against the transformed data to add it to the model. The K-Means clustering model assumes normallity of distributions, so we need the features to be normalized before feeding them to the model.","7d6eb78b":"#### Categorical vs Numerical","4600fb96":"#### Multivariate Conclusions\n\nBelow are the conclusions for the multivariate analysis.\n\n1) People who live in smaller cities tend to have lower income in the dataset\n2) People who have managerial jobs or own businesses have higher income than employees, who have higher income than unemployed\n3) Highly educated people tend to be older, but income doesn't seem to be influenced by education level\n4) Non-single people tend to have lower income and are younger than single people on average in the dataset\n5) Males tend to have a higher income than women when they are older","3819b392":"| Variable | Data Type | Range | Description |\n| --- | --- | --- | --- |\n| ID | numerical | Integer | Shows a unique identificator of a customer. |\n| Sex | categorical | {0,1} | Biological sex (gender) of a customer. 0 = male \/ 1 = female |\n| Marital status | categorical | {0,1} | Marital status of a customer. 0 = single \/ 1 = non-single |\n| Age | numerical | Integer | The age of the customer in years, calculated as current year minus the year of birth of the customer at the time of creation of the dataset (Min. age = 18 \/ Max. age = 78) |\n| Education | categorical | {0,1,2,3} | Level of education of the customer. 0=no education \/ 1=high-school \/ 2=university \/ 3=graduate |\n| Income | numerical | Real | Self-reported annual income in US dollars of the customer. |\n| Occupation | categorical | {0,1,2} | Category of occupation of the customer. 0=unemployed \/ 1=employee\/oficial \/ 2=management or self-employed |\n| Settlement size | categorical | {0,1,2} | The size of the city that the customer lives in. 0=small \/ 1=mid-size \/ 2=big |","597d3712":"#### Conclusions Cluster 5:\n\nCluster 5 is represented by single females of low education. The majority of them have a high-school level education, they live in small cities and are either unemployed or work as employees.","f8ec7c9a":"### Feature: Income\n\n##### Log Transformation","807958d3":"### Cluster 4","8d294db4":"## Summary statistics of each cluster\n\nNow let's build a program that calculates statistics for the data for each label. Our aim with this is to have a general sense of the values within each cluster, so we need central tendency statistics that look at the entire dataset we are interested in, like the mean, median, std. dev. and so forth. The .describe() method does precisely that for us. It shows the mean and std. dev., and the 50% column shows the median of the data.","1af76177":"### Definitive Feature transformations:\n\nWe broadcast the results of the previous section to our customer_info DataFrame. Finally, we rename the final transformed DataFrame that contains the final features to use in the model. We drop the variables we won't be introducing to the model, which are 'Income', 'Age' and 'ID'.","09f295ee":"### Feature Tranforming and Scaling\n\n#### Normality test for the data\n\nNumerical features showed a right skew in their distributions. We will check if the data is normally distributed or not, by using a normality test from scipy. From our visualizations we can already guess that the data isn't normally distributed, but we run the test to have evidence of precisely that.","2eb16e02":"#### Conclusions Cluster 3:\n\nThis cluster is also represented by single men, but with higher income and with management or self-employed occupations. They live in medium to large cities. Education is similar to cluster one, with most being educated up to high-school level or below. Ages seem to be within the same range as cluster one, so they won't be taken into account.","e4b83d86":"#### Categorical Variables Distribution\n\nNext, we take a look at categorical variables. Running univariant countplots, we reach the following conclusions:\n\n- Customer genders are quite balanced\n- Marital status is also balanced \n- Most customers have highschool level education. There are only 1,8% graduate students. We could join both categories ('2' and '3') in a category called university & over\n- Both the 'Occupation' and 'Settlement size' features seem to have a larger enough quantity of instances of each category.","6bc98836":"### Bivariate Analysis\n\n#### Numerical Features","483b708a":"#### Visualization\n\nWe have already clustered the data into 6 distinct groups and done PCA to get 3 features out of the 7 we originally had. It is always important to remember that using PCA inherently means a loss of information, so the projections of the data in the new features X1, X2 and X3 can have some overlapping points, but in reality, when using K-Means clustering the border points are clearly defined.","af8d956a":"When running the loop shown above, we notice that the elbow happens around 6-7 clusters, which would be a good approximation. The curve isn't very clear and you could also say that 12 is a good number as well, but you should understand that 12 clusters is generally too much, so we would rather lose some information about the groups our customers belong to, than gaining more accuracy in the clustering used.\n\nEven though we have a somewhat convincing result above, we will use the Silhouette scores to see if we can gain more insight on how many clusters should we use. See the procedure below:","39ade4d1":"### Cluster 0","58607a2c":"#### Applying K-Means with 6 clusters and visualizing the results with PCA decomposition","bd7bc142":"### Cluster 2:","dd4c2982":"**Imports:**","dabb9db0":"# Customer Segmentation Model\n\nThis notebook contains a clustering exercise used to practice model interpretation of grouped data. The hardest part about building a clustering model is generally understanding the results it outputs, as you don't have a target variable that you can compare your results with. \n\nTherefore, interpretation can become quite complex and so we explore a couple of alternatives for understanding your clusters once you have trained your model. Below are the steps taken to clean up the data, exploring it and finally fitting it to a K-Means clustering model and interpreting the results.\n\nI belive the most important part of this exercise is the clustering interpretation, as every time we perform this type of unsupervised task we should be aware that the end goal is to provide business insights through the results.\n\nIf you like the notebook, don't forget to **upvote!!**\n\n### Imports and information about the variables:\n\nVariable description including type, range and full description:","2001b91d":"After running the test, we notice that the data isn't normally distributed yet, so the log transformation was unable to get it to a full normal distribution. We will now test scikit learn's **PowerTransformer** to see if we reach better results.","fd7626f8":"#### PowerTransformer","a9efeac4":"#### Categorical vs Categorical","6259b8c6":"### Cluster 1","2e338a49":"#### Transformation cicle\n\nBelow, we will go over the same cicle over and over until we achieve a reasonable enough distribution. For that, we create a function that applies a transformation on the data by fitting its parameters and then transforming each value to a newer distribution. We will test a log and a PowerTransform, as the data seems to have a strong right skew, especially the Income feature.","18088a15":"### Loading the dataset","b83b675e":"#### Conclusion Cluster 0:\n\nThis cluster groups single men with medium income, generally unemployed who live in small cities. We could be tempted to say that their Age is between 20 and 40 primarily or that education is generally high-school level or below, but as we'll see further into the analysis, this happens in other clusters as well, so we conclude that age or education are good separators of clusters in this dataset for clusters 1 and 3.","02062071":"#### Numerical Variables Distribution","d1fccf3f":"#### Conclusions Cluster 1\n\nThis cluster is represented by non-single women with medium income and high-school or above-level education. They live in small cities and are either unemployed or work as employees\/officials. Age is still within the same ranges as before, so we won't take it into account.","b0c17a1d":"### Feature: Age","688c20f3":"### Using cluster centroids as a means of understanding the clusters themselves\n\nWhen we need to get a quick idea of the clusters' statistics by cluster, we can also use the centroids of each cluster, as we will show below. The problem with this method is that it provides a sense of central tendency, but it doesn't tell the whole story. Outliers may be affecting the values of each centroid, and so we have to handle this metric with care.\n\nHowever, it will be a good measure of accuracy from our already created clusters.","103f0119":"### Visualization of the Clustering Tree\n\nI'm calling the following tree a 'Clustering Tree' as it aids in defining the clustering algorithm parameters and gives and idea of how the data should be interpreted from our results. The clustering tree returns the results shown below, it is important to note that clusters are named in the same order as they were defined in previous sections.","4725a052":"#### Conclusions Cluster 4:\n\nCluster 4 is represented by non-single males with high-school or higher level education and low-medium income. They are mostly employed as employees\/officials. Note that this customers live in small, medium and large cities equally, so we are not taking it into account in this analysis. Age has the same problem as with other clusters, so it isn't taken into account.","aa7bf49c":"## Alternative ways to interpret clusters:","0258902d":"## Re-running the models for 6 and 7 clusters:\n\nBelow we create the models for 6 and 7 clusters using K-Means and try to understand the results. We will use PCA to visualize the data and determine if the 6-7 clusters we selected provide good inter-cluster cohesion and cluster separation. If not, we will re-run this approach with a Density-Based model and see if we get better results.","1c222008":"### Cluster 3:","eb4f7a50":"### Decision Tree as a method to interpret clusters:\n\nAn alternative way to visualize and understand clusters is by way of using decision trees. We can make a decision tree predict the labels of each cluster we have determined, and in doing so the tree will determine splitting points based on the features we pass to the model. In this way, we can create the cluster descriptions based on how the decision tree splits the data.\n\nWe will use graphviz as our tree visualization tool. If you haven't downloaded this library, but have matplotlib, there is also an option to plot trees with the matplotlib library, but the visualization is way clearer with graphviz!","19536f51":"#### Conclusions Cluster 2:\n\nCluster 2 is represented by non-single females with high-school or higher-level education, high income who are employed or self-employed. They live in medium to large cities. Again, the distribution of Age is similar in this cluster, so no extra information is obtained from this feature. \n\nYou could argue that the age of people in clusters 1 and 2 are generally younger (we use the median of the age to get 28.5 & 29 years in clusters 1 and 2 respectively) than people in clusters 0 and 3, which comparatively have older people in them (using the median again we get 36 and 37 for clusters 0 and 3 respectively). The difference is quite small, but not insignificant, as we'll check in the following cell. \n\nBelow we conduct a Hypothesis Test which assumes their data comes from the same sample, and we arrive at the conclusion that the means are significantly different, with a statistical significance of 95%. This means that we can assume that the ages of groups 0 and 3 are significantly different from ages in groups 1 and 2.\n\nDespite this effort, we still don't have a good grasp on the difference between the clusters regarding Age, so we will still ignore this feature.","33f75a7a":"There is some small correlation between the Age and Income features. We notice that the slope of the line is very small, which shows that the correlation between the features is low. Below we can see the calculation of the Pearson correlation factor, which states that values have a small correlation between them.","a728dd13":"#### Evaluating the model to make sure interpretations are accurate:","fddec81d":"### General info on the dataset:\n\nWe'll first run a quick statistical analysis of the data. We will check for null values, get the general sense of the data and the types, to see if they reflect what's shown in the table above. We use:\n\n- The .info() method on the DataFrame to understand the data types in each column. We notice there are no missing values and that all columns have numerical values in them.\n- The .describe() method on the DataFrame shows summary info on the regular statistics you may apply to categorical\/numerical data.","2d448bed":"### General Conclusions about the data\n\nEven though there was no missing data, we noticed a few trends that could be used or could be an expected outcome from the models. \n\nFirst, we notice that some numerical features (Age and Income) have a right-skewed normal distribution. We will have to correct that for the model to perform correctly, since it assumes normality in our features. Most likely, a **log transform** will correct this skew.\n\nSecond, about the data itself, we noticed that there is a small correlation between age and income, as expected. People in smaller cities have lower income in the dataset. Income is higher as occupation feature is higher. Non-singles (married, divorced, widowed or separated) tend to have lower income than single people, and for some strange reason tend to be younger than single people (in this dataset). When they are older, males tend to have higher income than females. Most unemployed people and married people in the dataset live in small cities. There are more unemployed women than men in the dataset.","dff737fc":"#### Silhouette scores:","0b5a9bbb":"#### Feature Scaling (MinMaxScaler)\n\nSince there are categorical features scaled between 0 and 1 ('Sex' feature), we will use scikit learn's MinMaxScaler to scale out data between 0 and 1. This is done so that the scale is equivalent to that of the 'Sex' feature (all values will be between zero and one). This is important for the K-Means clustering model, as it uses distance as a measure of similarity. Therefore, if we don't scale the features, we won't get accurate clusters.","798c2af8":"![mall image](https:\/\/mallofnorway.com\/content\/uploads\/2020\/06\/mall-of-norway-customer-service-1024x683.jpg)","3172dd61":"# Clustering Models\n\n## K-Means Clustering\n\nK-Means is a clustering algorithm based on distance to determine the similarity of different points. It creates clusters by assigning points to the cluster nearest to them.\n\n### Selecting the correct number of clusters\n\nThere are several ways to select the correct number of clusters, but they are all based in the amount of business insight we get from each cluster. It isn't useful to find a lot of clusters if we cannot interpret them, or gain nothing by separating them into different clategories. The number of clusters is an input to the model, but we cannot determine the number of clusters beforehand. So a good approximation is using the elbow method and selecting the number of groups that reduces a metric to a considerable amount; more wouldn't add much information and less would mean the metric could still be improved significantly.\n\n#### Elbow method","1f2c50dd":"From the graph shown above, we see that there is a spike in the Silhouette score for 7 clusters, which is coherent with the results we got from the elbow method. This results show that choosing either 6 or 7 clusters should result in somewhat separated groups, which is what we are looking for. We see again that more clusters would reduce the metrics even further, but interpretation for such a small quantity of features with so many clusters would be really hard.\n\nWe decide to create 6 and 7 clusters and use our business understanding to determine which classification provides more insights about the customers. Another option would be to select 14 or more, which is unrealistic given the size of the dataset.","b833a084":"### Cluster 5","1bd4c442":"We add the labels of each row to the DataFrame so we can make the plotting process simpler. In order to remember that we are talking about a categorical result in the labels columns, we will later convert them to string-types.","fe481fee":"You can use the information in the centroid to make some sense of the data, but as with the central tendency metrics, you will generally underfit your descriptions, as centroids don't necessarily show the whole picture of the data within each cluster. You can see that gender and marital status of most clusters will be correct, but numerical features, and even some categorical features are harder to interpret. this is why we prefer the Decision Tree method shown below.","d57c3091":"#### Creating a 3D dataset with PCA"}}