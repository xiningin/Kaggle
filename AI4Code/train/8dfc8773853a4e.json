{"cell_type":{"87f9d9ca":"code","598da5d6":"code","70bc1da2":"code","d3576f08":"code","2c79bc99":"code","455449f9":"code","c093a08f":"code","69d7c669":"code","80478291":"code","563b0b32":"code","c91b7d1d":"code","7045d627":"code","23cfb9c3":"code","de07a684":"code","895793b7":"markdown","1fa2c918":"markdown","78bc3065":"markdown","9ced44ae":"markdown","aa938ac0":"markdown","5e1ae3dc":"markdown","fceb1168":"markdown","83329bf9":"markdown","8bc13e85":"markdown","207ed7d9":"markdown","0f05d4d8":"markdown","44a1cc2a":"markdown","33289148":"markdown","06e91444":"markdown","d3c8507b":"markdown","95fb45da":"markdown","c23416f8":"markdown","ba5fe50c":"markdown","255fec9e":"markdown","00ff9433":"markdown"},"source":{"87f9d9ca":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","598da5d6":"#importing libraries\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import Pipeline","70bc1da2":"data = pd.read_csv('\/kaggle\/input\/us-yearly-electricity-consumption\/total-electricity-consumption-us.csv')\n\n#printing head\ndata.head()","d3576f08":"# number of observations\ndata.shape","2c79bc99":"# checking NA\n\ndata.isnull().sum()","455449f9":"# defining length of data index\nsize = len(data.index)\n\n# defining custom index\nindex = range(0,size,5)  #(start,stop,step)\n\n# train will not have same index which is defined above\ntrain = data[~data.index.isin(index)]\n\n# test will have same index which is defined above\ntest = data[data.index.isin(index)]","c093a08f":"#printing train and test shape\n\nprint(train.shape)\nprint(test.shape)","69d7c669":"#printing train and test length\n\nprint(len(train))\nprint(len(test))","80478291":"train.head()","563b0b32":"type(train.Year.values.reshape(-1,1))","c91b7d1d":"# making train two dimensional\nX_train = train.Year.values.reshape(-1,1)\n\n# defining y_train\ny_train = train.Consumption\n\n# making test two dimensional \nX_test = test.Year.values.reshape(-1,1)\n\n#Defining y_test\ny_test = test.Consumption","7045d627":"# defining empty list r2_train and r2_test\nr2_train = []\nr2_test = []\n\n# define degrees as list with 1,2 and 3 as elements \ndegrees = [1,2,3]","23cfb9c3":"# Iterating over each degree value defined above\nfor degree in degrees:\n# initialising pipeline\n    pipeline = Pipeline([('poly_features',PolynomialFeatures(degree=degree)),('model',LinearRegression())])\n#fitting pipeline with train and test\n    pipeline.fit(X_train,y_train)\n\n# test performance\n    y_pred_test = pipeline.predict(X_test)\n#appending r2_test with r2_score\n    r2_test.append(metrics.r2_score(y_test,y_pred_test))\n# training performance\n    y_pred_train = pipeline.predict(X_train)\n#appending r2_train with r2_score\n    r2_train.append(metrics.r2_score(y_train,y_pred_train))\n     \n\n## plot predictions and actual values against year\n    fig,ax = plt.subplots()\n    ax.set_title('Degree='+str(degree))\n    ax.set_xlabel('Year')\n    ax.set_ylabel('Power consumption')\n    \n    # train data \n    ax.scatter(X_train,y_train)\n    ax.plot(X_train,y_pred_train)\n    \n    # test data\n    ax.scatter(X_train,y_train)\n    ax.plot(X_test,y_pred_test)\n\n    plt.show()","de07a684":"# respective test r-squared scores of predictions for each degree\nprint(degrees,'\\n')\nprint(r2_train,'\\n')\nprint(r2_test)","895793b7":"![image.png](attachment:97941e63-8d00-4664-a93e-953bfae2f9ca.png)","1fa2c918":"* Degree = 1 => Linear fits => Linear Reg \n* Degree = 2 => Quadratic fits \n* Degree = 3 => Cubic fits","78bc3065":"![image.png](attachment:3736eaf3-f24b-4193-bf0a-589fc5036547.png)","9ced44ae":"**`Implementation of polynomial regression is a two-step process`**\nSee the `Pipeline` in Orange\n","aa938ac0":"---\n**`What is Polynomial Regression?`** <br>\nIt is a `special case of Linear Regression`,so polynomial regression differs in that it allows us to fit curved lines\/planes to our data","5e1ae3dc":"* `First One is Classification`\n* `Second One is Regression`","fceb1168":"![image.png](attachment:5b08e30c-6173-4344-98d4-f8fcd42844a8.png)","83329bf9":"No missing values in the dataset","8bc13e85":"Observe as polynomial degree increases, accuracy also increases but degree should also be decided based on checking condition of Underfitting and Overtting","207ed7d9":"* `First One is Simple Linear Regression`\n* `Second One is Polynomial Regression`\n\n---\n---","0f05d4d8":"### This notebook is a Continuation of [Advanced Linear Regression](https:\/\/www.kaggle.com\/mukeshmanral\/advance-linear-regression-basic-gridsearchcv-hpt) notebook","44a1cc2a":"Doing a Polynomial Regression: Comparing `Linear fits`, `Quadratic fits` and `Cubic fits`\n\n# Why Pipeline???\nPipeline `helps to associate two models or objects to be built sequentially with each other`, in this case `objects are PolynomialFeatures()` and `LinearRegression()`\n\n\n[Sklearn Pipeline Library](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.pipeline.Pipeline.html)","33289148":"**`converting train to a two dimensional array, as required by learning algorithm`**","06e91444":"* First One is `Under Fit` => `degree` = x\n* Second One is `Good Fit` => `degree` = x^3\n* Third One is `Over Fit` => `degree` = x^20\n\n`Polynomial Regression` is very sensitive to outliers, and one must take care when selecting `degree` to avoid overfitting\n\nWe just need to select `degree` of polynomial very wisely\n\n----","d3c8507b":"Observe in Polynomial Linear Regression the b2x1^2, its power goes 2 times","95fb45da":"# Polynomial Regression","c23416f8":"![image.png](attachment:2b0c87dc-b06a-4a15-830d-02920f48d5d4.png)","ba5fe50c":"![image.png](attachment:6ede7976-cb72-4e95-88e9-bfbb7abff717.png)","255fec9e":"#### In this notebook I am trying to Apply `Polynomial Regression` before I have applied [Simple Linear Regression](https:\/\/www.kaggle.com\/mukeshmanral\/linear-regression-basic), [Multiple Linear Regression](https:\/\/www.kaggle.com\/mukeshmanral\/multiple-linear-regression-basic) and [Advance Linear Regression with GridSearchCV and Hyperparameter Tuning](https:\/\/www.kaggle.com\/mukeshmanral\/advance-linear-regression-basic-gridsearchcv-hpt).\n\n**`Why Even I am bothered about Polynomial Regression??` Why Polynomial Regression** <br>\n`If Linear Regression Model cannot model relationship between Target Variable and Predictor Variable`, or can say `what if they don\u2019t have a Linear Relationship?`\n\nWell \u2013 that\u2019s where Polynomial Regression might be of assistance","00ff9433":"`In Supervised Learning we have`"}}