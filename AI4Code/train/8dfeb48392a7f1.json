{"cell_type":{"d0376a90":"code","2927c3ed":"code","37047481":"code","a7305346":"code","88485668":"code","073e66db":"code","8963cbc2":"code","048e7241":"code","6614ccf6":"code","03340972":"code","128c8167":"code","cf21c985":"code","d9ec617b":"code","19f5e8af":"code","31a0517d":"code","d15553ea":"code","0e6b0aed":"code","ff59f6d7":"code","eaffa024":"code","bf180ea6":"code","ba5983c8":"code","c9c0238e":"code","538b4123":"code","59aefc04":"code","584bd397":"code","c173f84d":"code","ba05ec13":"code","c09f4b6d":"code","0ff629bc":"markdown","c11b5445":"markdown","813d6a91":"markdown","3ee3738f":"markdown","40fd7a7f":"markdown","acebb9d8":"markdown","44b22873":"markdown","97e25145":"markdown","2a2e49e8":"markdown","e0afff6b":"markdown","1d9c6eb7":"markdown","e7722cdd":"markdown","3fd8ac5a":"markdown","f27cf341":"markdown","f10aab7e":"markdown","1e459524":"markdown","3ff63b0d":"markdown","781668d3":"markdown","bc6f1313":"markdown","8f81cd44":"markdown","5fd7d911":"markdown","f2bae947":"markdown"},"source":{"d0376a90":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom heapq import nlargest \nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns; sns.set()\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBRegressor\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nimport warnings\nwarnings.filterwarnings('ignore')","2927c3ed":"sample_submission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\n\ntrain.head()","37047481":"categorical = train.select_dtypes(include=['object']).columns\nnumerical = train.select_dtypes(include=['int64', 'float64']).columns\n\nprint(\"Categorical columns:\")\nprint(categorical)\nprint(\"\\nNumerical columns:\")\nprint(numerical)\n\n# Correlation matrix\n\nprint(\"\\nCorrelations with SalePrice: \")\ncorr = train.corr()\nax = sns.heatmap(\n    corr, \n    vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200),\n    square=True\n)\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right'\n);\n\nHighest_corr = corr.nlargest(20, 'SalePrice')['SalePrice']\nprint(Highest_corr)","a7305346":"f1, axes = plt.subplots(1, 3, figsize=(15,5))\nf1.subplots_adjust(hspace=0.4, wspace=0.8)\nsns.boxplot(x=train['OverallQual'], y=train['SalePrice'],orient='v', ax=axes[0])\nsns.boxplot(x=train['GarageCars'], y=train['SalePrice'], orient='v', ax=axes[1])\nsns.boxplot(x=train['Fireplaces'], y=train['SalePrice'], orient='v', ax=axes[2])","88485668":"train = train.drop(train[(train['GarageCars']>3) & (train['SalePrice']<300000)].index).reset_index(drop=True)","073e66db":"f2, axes = plt.subplots(1, 3, figsize=(15,5))\nf2.subplots_adjust(hspace=0.4, wspace=0.8)\nsns.boxplot(x=train['TotRmsAbvGrd'], y=train['SalePrice'],orient='v', ax=axes[0])\nsns.boxplot(x=train['FullBath'], y=train['SalePrice'], orient='v', ax=axes[1])\nsns.boxplot(x=train['HalfBath'], y=train['SalePrice'], orient='v', ax=axes[2])","8963cbc2":"sns.jointplot(x=train['GrLivArea'], y=train['SalePrice'], kind='reg').annotate(stats.pearsonr)\nsns.jointplot(x=train['GarageArea'], y=train['SalePrice'], kind='reg').annotate(stats.pearsonr)","048e7241":"train = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<250000)].index).reset_index(drop=True)\ntrain = train.drop(train[(train['GarageArea']>1100) & (train['SalePrice']<200000)].index).reset_index(drop=True)\nsns.jointplot(x=train['GrLivArea'], y=train['SalePrice'], kind='reg').annotate(stats.pearsonr)\nsns.jointplot(x=train['GarageArea'], y=train['SalePrice'], kind='reg').annotate(stats.pearsonr)","6614ccf6":"sns.jointplot(x=train['TotalBsmtSF'], y=train['SalePrice'], kind='reg').annotate(stats.pearsonr)\nsns.jointplot(x=train['1stFlrSF'], y=train['SalePrice'], kind='reg').annotate(stats.pearsonr)\nsns.jointplot(x=train['2ndFlrSF'], y=train['SalePrice'], kind='reg').annotate(stats.pearsonr)\nsns.jointplot(x=train['MasVnrArea'], y=train['SalePrice'], kind='reg').annotate(stats.pearsonr)","03340972":"sns.jointplot(x=train['YearBuilt'], y=train['SalePrice'], kind='reg').annotate(stats.pearsonr)\nsns.jointplot(x=train['YearRemodAdd'], y=train['SalePrice'], kind='reg').annotate(stats.pearsonr)\nsns.jointplot(x=train['GarageYrBlt'], y=train['SalePrice'], kind='reg').annotate(stats.pearsonr)","128c8167":"sns.jointplot(x=train['BsmtFinSF1'], y=train['SalePrice'], kind='reg').annotate(stats.pearsonr)\nsns.jointplot(x=train['LotFrontage'], y=train['SalePrice'], kind='reg').annotate(stats.pearsonr)\nsns.jointplot(x=train['WoodDeckSF'], y=train['SalePrice'], kind='reg').annotate(stats.pearsonr)\nsns.jointplot(x=train['OpenPorchSF'], y=train['SalePrice'], kind='reg').annotate(stats.pearsonr)","cf21c985":"y_train = train.SalePrice\njoin_data = pd.concat((train, test), sort=False).reset_index(drop=True)\ntrain_size = len(train)\ntest_size = len(test)\n\nprint(\"Train set size: \", train_size)\nprint(\"Test set size: \", test_size)\nprint(\"Train+test set size: \", len(join_data))","d9ec617b":"missings_count = {col:join_data[col].isnull().sum() for col in join_data.columns}\nmissings = pd.DataFrame.from_dict(missings_count, orient='index')\nprint(missings.nlargest(30, 0))","19f5e8af":"def fill_missings(data):\n    clean_data = data.copy()\n\n    # Replace missing categorical data with None (strings like Gd = Good, Wd = Wood, etc)\n    fill_with_nones = ['PoolQC','MiscFeature','Alley','Fence','FireplaceQu','GarageType','GarageFinish',\n          'GarageQual','GarageCond','BsmtExposure','BsmtQual','BsmtCond', 'BsmtFinType1', 'BsmtFinType2',\n          'MasVnrType']\n    for col in fill_with_nones:\n        clean_data[col] = clean_data[col].fillna(\"None\")\n\n    # Replace some numeric missings with 0\n    fill_with_0 = ['MasVnrArea','BsmtFullBath','BsmtHalfBath','BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','GarageArea','GarageCars','TotalBsmtSF','GarageYrBlt']\n    for col in fill_with_0:\n        clean_data[col] = clean_data[col].fillna(0)\n\n    # Replace LotFrontage with neighborhood mean\n    clean_data[\"LotFrontage\"] = clean_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))\n\n    # When applies or doubting how to replace, use mode\n    clean_data['MSZoning'].fillna(clean_data['MSZoning'].mode()[0])\n    clean_data['Electrical'].fillna(clean_data['Electrical'].mode()[0])\n    clean_data['Exterior1st'].fillna(clean_data['Exterior1st'].mode()[0])\n    clean_data['Exterior2nd'].fillna(clean_data['Exterior2nd'].mode()[0])\n    clean_data['KitchenQual'].fillna(clean_data['KitchenQual'].mode()[0])\n    clean_data['SaleType'].fillna(clean_data['SaleType'].mode()[0])\n    clean_data['Utilities'].fillna(clean_data['Utilities'].mode()[0])\n\n    # Ad-hoc replacement of the Functional column\n    clean_data['Functional'].fillna('Typ')\n    \n    return clean_data","31a0517d":"def create_additional_features(all_data):\n        \n    # Flags\n    all_data['has_pool'] = all_data['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n    all_data['has_2ndfloor'] = all_data['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n    all_data['has_garage'] = all_data['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n    all_data['has_fireplace'] = all_data['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n    all_data['has_bsmt'] = all_data['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n    \n    # Combine features\n    all_data['Year_BuiltAndRemod']=all_data['YearBuilt']+all_data['YearRemodAdd']\n    all_data['Total_SF']=all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n    all_data['Total_sqr_footage'] = (all_data['BsmtFinSF1'] + all_data['BsmtFinSF2'] + all_data['1stFlrSF'] + all_data['2ndFlrSF'])\n    all_data['Total_Bath'] = (all_data['FullBath'] + (0.5 * all_data['HalfBath']) + all_data['BsmtFullBath'] + (0.5 * all_data['BsmtHalfBath']))\n    all_data['Total_porch_SF'] = (all_data['OpenPorchSF'] + all_data['3SsnPorch'] + all_data['EnclosedPorch'] + all_data['ScreenPorch'] + all_data['WoodDeckSF'])\n    \n    return all_data","d15553ea":"def categorize_data(data):\n    categorized_data = data.copy()\n    categorized_data['MSSubClass'] = categorized_data['MSSubClass'].apply(str)\n    categorized_data['OverallCond'] = categorized_data['OverallCond'].astype(str)\n    categorized_data['YrSold'] = categorized_data['YrSold'].astype(str)\n    categorized_data['MoSold'] = categorized_data['MoSold'].astype(str)\n    \n    return categorized_data","0e6b0aed":"def encode_categories(data):\n\n    all_data = data.copy()\n    cols_to_encode = ('PoolQC', 'Alley', 'Fence', 'FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n            'ExterQual', 'ExterCond','HeatingQC', 'KitchenQual', 'BsmtFinType1', 'BsmtFinType2', 'Functional', \n            'BsmtExposure', 'GarageFinish', 'LandSlope','LotShape', 'PavedDrive', 'Street', 'CentralAir', 'MSSubClass', \n            'OverallCond', 'YrSold', 'MoSold')\n\n    # Process columns and apply LabelEncoder to categorical features\n    for c in cols_to_encode:\n        lbl = LabelEncoder() \n        lbl.fit(list(all_data[c].values)) \n        all_data[c] = lbl.transform(list(all_data[c].values))\n        \n    return all_data","ff59f6d7":"# Preprocessing\nall_data = fill_missings(join_data)\nall_data = create_additional_features(all_data)\nall_data = categorize_data(all_data)\nall_data = encode_categories(all_data)\n\n# Split again train-test data\nX = all_data[:train_size]\nX_test_full = all_data[train_size:]\n\nprint(len(all_data), len(X), len(X_test_full))\n\n# Remove rows with missing target, separate target from predictors\nX.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = X.SalePrice              \nX.drop(['SalePrice'], axis=1, inplace=True)\n\n# Deal with additional outliers (from an in depth and manual analysis)\noutliers = [30, 88, 462, 631, 1322]\nX = X.drop(X.index[outliers])\ny = y.drop(y.index[outliers])\n\n# Drop columns that would lead to overfitting (too much 0s)\noverfit = []\nfor i in X.columns:\n    counts = X[i].value_counts()\n    zeros = counts.iloc[0]\n    if zeros \/ len(X) * 100 > 99.94:\n        overfit.append(i)\n\noverfit = list(overfit)\nX = X.drop(overfit, axis=1)\n\n# Break off validation set from training data\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.99, test_size=0.01,\n                                                                random_state=0)\n\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\nlow_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n                        X_train_full[cname].dtype not in ['int64', 'float64']]\n\nprint(\"Low cardinality columns: \", low_cardinality_cols)\n\n# Select numeric columns\nnumeric_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n\nprint(\"Numeric columns: \", numeric_cols)\n\n# Keep selected columns only\nmy_cols = low_cardinality_cols + numeric_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\nX_test = X_test_full[my_cols].copy()\n\n# One-hot encode the data (to shorten the code, we use pandas)\nX_train = pd.get_dummies(X_train)\nX_valid = pd.get_dummies(X_valid)\nX_test = pd.get_dummies(X_test)\nX_train, X_valid = X_train.align(X_valid, join='left', axis=1)\nX_train, X_test = X_train.align(X_test, join='left', axis=1)","eaffa024":"skewed_feats = X_train[numeric_cols].apply(lambda x: stats.skew(x.dropna())).sort_values(ascending=False)\ny_train = y_train.apply(lambda x: np.log(x))\ny_valid = y_valid.apply(lambda x: np.log(x))\n\n# Apply log to all columns with |skewness|>0.8\nlog_col = []\nfor col in skewed_feats.index:\n    if(abs(skewed_feats[col])>0.8): \n        log_col.append(col)\n        X_train[col]=X_train[col].apply(lambda x: np.log(x))\n        X_valid[col]=X_valid[col].apply(lambda x: np.log(x))\n        X_test[col]=X_test[col].apply(lambda x: np.log(x))","bf180ea6":"def optimize_xgb(all_data): \n    xgb1 = XGBRegressor()\n    parameters = {'nthread':[1], #when use hyperthread, xgboost may become slower\n                  'objective':['reg:linear'],\n                  'learning_rate': [.02, .01, .0075, .005], #so called `eta` value\n                  'max_depth': [5, 6, 7],\n                  'min_child_weight': [3, 4, 5],\n                  'subsample': [0.7],\n                  'colsample_bytree': [0.7],\n                  'n_estimators': [2500, 5000]}\n\n    xgb_grid = GridSearchCV(xgb1,\n                            parameters,\n                            cv = 2,\n                            n_jobs = 5,\n                            verbose=True)\n\n    xgb_grid.fit(X_train, y_train)\n\n    print(xgb_grid.best_score_)\n    print(xgb_grid.best_params_)\n    \n# optimize_xgb(all_data)","ba5983c8":"# Define model with best MAE\nmodel = XGBRegressor(colsample_bytree=0.7, learning_rate=.01, max_depth=6, min_child_weight=3, n_estimators=3000, \n                     nthread=1, objective='reg:squarederror', subsample=0.7, random_state=21, \n                     early_stopping_rounds = 10, eval_set=[(X_valid, y_valid)], verbose=False)","c9c0238e":"# Train and test the model\n\nprint(\"Let's the training begin. Plase wait.\")\n\n# Bundle preprocessing and modeling code in a pipeline\nmy_pipeline = Pipeline(steps=[('model', model)])\nmy_pipeline.fit(X_train, y_train)\n\nprint(\"Training finished! Now let's predict test values.\")\n\npreds_test = my_pipeline.predict(X_test)\npreds_test = [np.exp(pred) for pred in preds_test] \n\n# Save test predictions to file\noutput = pd.DataFrame({'Id': X_test.index+9,\n                       'SalePrice': preds_test})\noutput.to_csv('submission.csv', index=False)\n\nprint(\"Everything finished correctly!\")\n\nprint(output.SalePrice)","538b4123":"print(len(test), len(X_test_full), len(output.SalePrice))","59aefc04":"#Check the distribution of SalePrice\nsns.distplot(y)\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\nprint(\"Skewness: %f\" % y.skew())\nprint(\"Kurtosis: %f\" % y.kurt())","584bd397":"# Correlation matrix\n\ncorr = all_data.corr()\nax = sns.heatmap(\n    corr, \n    vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200),\n    square=True\n)\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right'\n);\n\nHighest_corr = corr.nsmallest(40, 'SalePrice')['SalePrice']\nprint(Highest_corr)","c173f84d":"lightgbm = lgb.LGBMRegressor(objective='regression', \n                                       num_leaves=4,\n                                       learning_rate=0.01, \n                                       n_estimators=5000,\n                                       max_bin=200, \n                                       bagging_fraction=0.75,\n                                       bagging_freq=5, \n                                       bagging_seed=7,\n                                       feature_fraction=0.2,\n                                       feature_fraction_seed=7,\n                                       verbose=-1,\n                                       )\nlgb_model_full_data = lightgbm.fit(X_train, y_train)","ba05ec13":"xgboost = XGBRegressor(learning_rate=0.01,n_estimators=3460,\n                                     max_depth=3, min_child_weight=0,\n                                     gamma=0, subsample=0.7,\n                                     colsample_bytree=0.7,\n                                     objective='reg:linear', nthread=-1,\n                                     scale_pos_weight=1, seed=27,\n                                     reg_alpha=0.00006)\n\nxgb_model_full_data = xgboost.fit(X_train, y_train)","c09f4b6d":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef blend_models_predict(X):\n    return ((1 * xgb_model_full_data.predict(X)) + \\\n            (0 * lgb_model_full_data.predict(X)))\n\nprint('RMSLE score on train data:')\nprint(rmsle(y_valid, blend_models_predict(X_valid)))","0ff629bc":"Perfect! Our model is done and results submited. Now you can enjoy your results and play with parameters or additional feature engineering to obtain better results. Hope you enjoyed the kernel and see you soon!\n","c11b5445":"Let's continue analyzing other columns that may be correlated to SalePrice:","813d6a91":"# 7. Prediction and submit <a id=\"section7\"><\/a>\n\nFinally, define the model with the best parameters, introduce the objective function to minimize (in this case the squared error) and submit the results. Notice that we will use a pipeline, just as a best practices guideline.","3ee3738f":"# 4. Feature engineering <a id=\"section4\"><\/a>\n\nAlright, our data is now clean and we have more insight about data columns. However, a successful machine learning algorithm will need as much information as possible to improve its scoring. Hence, we will need to enrich a bit more our dataset.\n\nWhat we will do:\n* Create additional flag columns. Has_pool, has_garage, etc?\n* Combine features. For example, Total_SF = TotalBsmtSF + 1stFlrSF + 2ndFlrSF\n* Label encode categories\n* Label encode numerical data that is in fact categorical. For example, FireplaceQu, which is a quality measurement","40fd7a7f":"Some categorical columns refer to quality or type, and can be encoded into numeric values. We'll use a simple LabelEncoding, but TargetEncoding or CatBoost could perform better:","acebb9d8":"The following columns have poor corelation with SalePrice. It is expected to find a low Pearson coeff and hence we won't clean data from them.","44b22873":"## Optional code cells\n\n\nAll code cells below are completely optional. They help us to understand some changes done to data (i.e. skewness correction), see correlations and test alternative algorithms. ","97e25145":"# House prices: in-depth feature engineering + XGB\n\nThe aim of this notebook is to estimate house prices based on their characteristics. I think this competition is a **great starting point for anyone interested in feature engineering and data cleaning**, given the simplicity of the dataset but the considerable amount of feature engineering that can be performed in order to improve the results.\n\nTable of contents:\n1. [Load data](#section1)\n2. [Data exploration (EDA)](#section2)\n3. [Missings cleaning](#section3)\n4. [Feature engineering](#section4)\n5. [Data preparation](#section5)\n6. [Parametrization of the XGB model](#section6)\n7. [Prediction and submit](#section7)","2a2e49e8":"Once weird values and outliers have been removed from the training dataset, it's time to deal with missings and encoding","e0afff6b":"# 2. Data exploration (EDA) <a id=\"section2\"><\/a>\n\nThere are 81 columns in the original dataset. A big fraction of this columns seems to have null values, and there are different types of data (categorical, numerical, date related information, etc). Hence, we will analyze these features to look for high correlations with the target column (SalePrice), potential outliers and meaningful insights that could help us to engineer some additional features.\n\nSummary of the EDA step:\n* Define which columns are categorical or numerical\n* Generate a correlation matrix to detect potential dependencies between data columns\n* Analyze discrete\/categorical columns and drop outliers\n* Analyze continuous important data and drop outliers","1d9c6eb7":"First we analyze discrete columns:","e7722cdd":"Missings count in the train+test set:","3fd8ac5a":"# 6. Parametrization of the XGB model <a id=\"section6\"><\/a>\n\nEverything is ready for our final prediction. But first, we need to find the parameters that give the best accuracy. This is done by a simple grid search over 3 cross-validation folds.","f27cf341":"# 1. Load data <a id=\"section1\"><\/a>\n\nRead the files provided by the Kaggle competition: train, test and sample_submission.","f10aab7e":"# 3. Missings cleaning <a id=\"section3\"><\/a>\n \nFrom our data exploration, we know that there are several columns with missing data. We need to find out how many missings has each column, and how to deal with them. Since the nature of each column is different, we work [](http:\/\/)in-depth each of them while using the data_description.txt provided by the Kaggle competition to decide how to deal with them in each case.\n\nOur main tasks are:\n* Define number of missings for each data column\n* Fill missings with the help of the data_description.txt file (manual and precise work)","1e459524":"Proceed with rest of data using jointplots","3ff63b0d":"OverallQual and Fireplaces look fine, since the better the quality and number of fireplaces, the more expensive the house. However, a reasonable assumption is that the sale price of a house should be an increasing function of the garage cars capacity. \n\nHence, GaraceCars = 4 is an outlier. Drop outliers with GarageCars = 4 and cheap SalePrice","781668d3":"Before going to the final detection, there's an additional problem to address. \n\nWealth is a magnitude that follows a power law, and hence its distribution is not a normal distribution, and other colums may have similar behaviors. Since most machine learning algorithms -and definetly XGB is one of them- work better with normal distributions, we will analyze  highly non-normal distributed features with |skewness|>0.8.\n\nWe will modify them to be more \"normally distributed\". This can be done with several transformations, but I decided to use a simple logarithm.","bc6f1313":"OK, we have created several additional features that hopefully will help to improve the model's accuracy. Now we need tackle the label encoding.\n\nBut before that, there are 4 numerical columns that relate to \"types\" and hence should be considered categories:","8f81cd44":"# 5. Data preparation <a id=\"section5\"><\/a>\n\nFine, we are almost there. Once all previous functions are applied, data will be clean from missing values, enriched with additional features and encoded into numerical values. Let's just finish the last details and split the dataset.\n\nData preparation workflow:\n* Apply all previous functions to fill misings, create new features and encode categories\n* Split data set: train, validation and test\n* Deal with additional outliers. From a posterior manual analysis\n* Drop all columns that would lead to overfitting\n* One hot encode pending categorical data if needed","5fd7d911":"Remove the two outliers on the bottom-right corner (high GrLivArea\/GarageArea but cheap SalePrice)","f2bae947":"Most missings can be understood from the data_description.txt file (i.e. PoolQC missing means no pool). Let's fix this:"}}