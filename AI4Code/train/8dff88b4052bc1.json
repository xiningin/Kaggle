{"cell_type":{"069d75de":"code","ed9ed0ac":"code","eaee21e6":"code","578d2388":"code","093e7ea2":"code","9cdf64bc":"code","93290e69":"code","6d6ce94f":"code","836481d7":"code","c46ba067":"code","7d221acc":"code","b146bfe9":"code","b6942d41":"code","2669bab1":"code","ddc85071":"code","831872ed":"code","784782ac":"code","64ec8ed4":"code","3e125a9e":"code","78b06868":"code","52147dba":"code","f8c2351b":"code","774499f6":"code","bfed4a31":"code","8d22af9b":"code","39c2b446":"code","219e6f7f":"code","de9409fe":"code","6e0e8242":"code","d16887bf":"code","a7a1177e":"code","f5121f9f":"code","5c03f682":"code","a2d794e5":"code","8fa765ca":"code","dde3c8e6":"code","d47b08ec":"code","2081a0f8":"code","edbe56e0":"code","983252f4":"code","8b9ca808":"code","3b30a7f8":"code","ba90bfe1":"code","943f5036":"code","c2be649f":"code","eb2dd513":"code","0a777d30":"code","377a579c":"code","8589e6ef":"code","2b84d42d":"code","64cbfbe5":"code","53b9be0f":"code","53a39a7c":"code","dd77d508":"code","6eb89d74":"code","2d7cef5e":"markdown","c8c16674":"markdown","5617a573":"markdown","e7c6053a":"markdown","6bf6a413":"markdown","e7e5aaf6":"markdown","958ced2b":"markdown","02c38cbf":"markdown","cf55cf17":"markdown","40e525fd":"markdown","b6540cf5":"markdown","6f0cfac3":"markdown","c6a5d86a":"markdown","ebceb6dd":"markdown","44b66616":"markdown","083c65e9":"markdown","8b7731db":"markdown","88de0f50":"markdown","0c796e5f":"markdown","ab039039":"markdown","3ac1fdfa":"markdown"},"source":{"069d75de":"import numpy as np\nimport pandas as pd","ed9ed0ac":"path = '..\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv'\ntrain_data = pd.read_csv(path)\ntrain_data","eaee21e6":"path = '..\/input\/hr-analytics-job-change-of-data-scientists\/aug_test.csv'\ntest_data = pd.read_csv(path)\ntest_data","578d2388":"path = '..\/input\/hr-analytics-job-change-of-data-scientists\/sample_submission.csv'\ngender_submission_data = pd.read_csv(path)\ngender_submission_data","093e7ea2":"df = pd.concat([train_data, test_data], ignore_index=True)\ndf","9cdf64bc":"df['target'].replace(to_replace=0,  value='no', inplace=True)\ndf['target'].replace(to_replace=1,  value='yes', inplace=True)\ndf['target'] = df['target'].astype('object')\ndf['target'].value_counts(dropna=False)","93290e69":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nplt.figure(figsize=(20, 30))\nsns.heatmap(df.isnull(), cbar=False)","6d6ce94f":"def NaN_info(df):\n    global null_view\n    try:\n        null_view = df[[col for col in df.columns if df[col].isna().sum() > 0]].isna().sum().sort_values(ascending = True)\n        null_view = pd.DataFrame(null_view, columns=['NANs'])\n        null_view[['PERCENT']] = null_view.NANs.apply(lambda x: round((x\/len(df))*100, 2))\n        null_view[['TYPE']] = df.dtypes\n    except:\n        return null_view\n    return null_view\n\nNaN_info(df)","836481d7":"df.dtypes","c46ba067":"for el in list(df.columns):\n    print(f'======================= {el} =======================')\n    print(df[el].value_counts(dropna=False))","7d221acc":"indexes = list(df.index)\nfor el in indexes:\n    city = df.loc[el, 'city']\n    city = city.split(sep='_')    \n    df.loc[el, 'city_num'] = int(city[1])\n    \ndf.city_num = df.city_num.astype('int64')","b146bfe9":"df['experience'].replace(to_replace='>20', value=np.NaN, inplace=True)\ndf['experience'].replace(to_replace='<1', value=0.5, inplace=True)\ndf['experience'] = df['experience'].astype('float64')","b6942d41":"def change(x):\n    if x < 1:\n        x = 'trainee'\n    elif x < 2:\n        x = 'junior'\n    elif x < 5:\n        x = 'middle'\n    elif x < 8:\n        x = 'senior'\n    elif x < 15:\n        x = 'master'\n    elif x > 14:\n        x = 'grandmaster'\n    else:\n        np.nan\n    return x\n\ndf['experience_cat'] = df['experience'].apply(change)\n\ndf[0:10][['experience','experience_cat']]","2669bab1":"change={\n        '<10':10, \n        '10\/49':50, \n        '50-99':100, \n        '100-500':500, \n        '500-999':1000, \n        '1000-4999':5000, \n        '5000-9999':10000, \n        '10000+':100000,\n        }\ndf['company_size_num'] = df['company_size'].map(change)\ndf[0:10][['company_size','company_size_num']]","ddc85071":"df['last_new_job'].unique()","831872ed":"change={\n        'never': 0,\n        '1': 1,\n        '2': 2,\n        '3': 3,\n        '4': 4,\n        '>4': 5,\n        }\ndf['last_new_job_num'] = df['last_new_job'].map(change)\ndf[0:10][['last_new_job','last_new_job_num']]","784782ac":"df","64ec8ed4":"def NaN_info(df):\n    global null_view\n    try:\n        null_view = df[[col for col in df.columns if df[col].isna().sum() > 0]].isna().sum().sort_values(ascending = True)\n        null_view = pd.DataFrame(null_view, columns=['NANs'])\n        null_view[['PERCENT']] = null_view.NANs.apply(lambda x: round((x\/len(df))*100, 2))\n        null_view[['TYPE']] = df.dtypes\n    except:\n        return null_view\n    return null_view\n\nNaN_info(df)","3e125a9e":"def nan_predict(df,\n                skip_features_from_prediction_where_percent_missing_data_more_than = 100,\n                include_features_as_predictors_where_perc_missing_data_less_than = 50,\n                apply_fast_predictor_where_missing_data_less_than_percent = 100,\n                use_n_rows_for_train_not_more_than = 1000000000,    #  If your dataframe is large\n                randomizedSearchCV_iter_plus_perc_missing_data = 10,\n                n_estimators_parameter_for_LightGBM = 2000,\n                target_feature = None,   # For prediction at the end\n                ): \n    \n    import random\n    import pandas as pd\n    import numpy as np\n\n    # Disabling warnings\n    import sys\n    import warnings\n    if not sys.warnoptions:\n        warnings.simplefilter(\"ignore\")\n\n\n    from lightgbm import LGBMClassifier\n    from lightgbm import LGBMRegressor\n    from sklearn.model_selection import RandomizedSearchCV\n    from sklearn.model_selection import ShuffleSplit\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import mean_absolute_error\n    from sklearn.metrics import accuracy_score\n    from sklearn.metrics import f1_score\n    from sklearn.preprocessing import LabelEncoder\n    \n    \n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    %matplotlib inline\n    \n    \n\n    global counter_all_predicted_values\n    counter_all_predicted_values = 0\n    \n    global numeric_features\n    numeric_features = []\n    \n    global best_params\n    \n    \n    PARAMS  =  {'num_leaves': [12, 50, 120, 200, 300, 400, 500],   #np.arange(200, 600, step=100),\n                'max_depth': [4, 8, 12, 16],\n                'learning_rate': [0.001, 0.01, 0.1],\n                'n_estimators': [n_estimators_parameter_for_LightGBM],\n                'subsample': [0.1, 0.3, 0.5],\n                'feature_fraction': [0.1, 0.3, 0.5],\n                'bagging_fraction': [0.1, 0.3, 0.5],\n                'bagging_seed': np.arange(1, 3, step=1),\n                'lambda_l1': [0.2],\n                'lambda_l2': [0.1],\n                'min_child_samples': np.arange(2, 6, step=2),\n                'min_split_gain': [0.0001, 0.001]\n               }\n    \n    \n    CV = ShuffleSplit(n_splits=2, test_size=0.25, random_state=0)\n    \n    \n    \n\n    def NaN_info(df):\n        global null_view\n        try:\n            null_view = df[[col for col in df.columns if df[col].isna().sum() > 0]].isna().sum().sort_values(ascending = True)\n            null_view = pd.DataFrame(null_view, columns=['NANs'])\n            null_view[['PERCENT']] = null_view.NANs.apply(lambda x: round((x\/len(df))*100, 2))\n            null_view[['TYPE']] = df.dtypes\n        except:\n            return null_view\n        return null_view\n    \n    \n    def numeric_features(df):\n        num_features = [feature for feature in df.columns if df[feature].dtype in ['int64', 'float64']]\n        return num_features\n    \n    \n    def integer_features(df):\n        global int_features\n        int_features = [feature for feature in df.columns if df[feature].dtype in ['int64']]\n        return int_features\n\n\n    def encoding(work_predictors, df):\n        feature_power = 0.5          # Skew handling\n        for j in work_predictors:\n            el_type = df[j].dtype\n            if el_type == 'object':\n                df[j].replace(np.nan, 'NoNoNo', inplace=True)\n                labelencoder = LabelEncoder()\n                df.loc[:, j] = labelencoder.fit_transform(df.loc[:, j])\n            else:\n                df[j] = df[j]**feature_power\n        return df, work_predictors\n\n\n    def hyperparms_tuning(CV, X_train, X_test, y_train, y_test, n_iter_for_RandomizedSearchCV, PARAMS, alg, scoring):\n        global best_params\n        global pred_test_lgb\n\n        lgbm = alg(random_state = 0)\n        lgbm_randomized = RandomizedSearchCV(estimator=lgbm, \n                                            param_distributions=PARAMS, \n                                            n_iter=n_iter_for_RandomizedSearchCV, \n                                            scoring=scoring, \n                                            cv=CV, \n                                            verbose=0,\n                                            n_jobs = -1)\n\n        lgbm_randomized.fit(X_train, y_train)\n        \n        best_params = lgbm_randomized.best_params_\n        pred_test_lgb = lgbm_randomized.predict(X_test)\n        return best_params, pred_test_lgb\n\n    \n    def predict_regressor(best_params, X, y, miss_df):\n        print('Best parameters:')\n        print(best_params)\n        print('')\n        global pred_miss\n        lgbm = LGBMRegressor(**best_params, n_jobs=-1, random_state=0)\n        lgbm.fit(X, y)\n        pred_miss = list(lgbm.predict(miss_df))\n        print('-------------------------------')\n        print(f\"The first 100 predicted missing values: \\n{pred_miss[:100]}\")\n        return pred_miss\n\n\n    def predict_classifier(best_params, X, y, miss_df):\n        print('Best parameters:')\n        print(best_params)\n        print('')\n        global pred_miss\n        lgbm = LGBMClassifier(**best_params, n_jobs=-1, random_state=0)\n        lgbm.fit(X, y)\n        pred_miss = list(lgbm.predict(miss_df))\n        print('-------------------------------')\n        print(f\"The first 100 predicted missing values: \\n{pred_miss[:100]}\")\n        return pred_miss\n    \n    \n    def imput_missing_value_to_main_df(df, miss_indeces, pred_miss, el):\n        counter = 0\n        for idx in miss_indeces:\n            df.loc[idx, el] = pred_miss[counter]\n            counter += 1\n        return df\n    \n    \n    \n    # Go)\n\n    plt.figure(figsize=(20, 5))\n    sns.heatmap(df.isnull(), cbar=False)\n    \n    \n    print(NaN_info(df))\n    print('\\n\\n\\n')\n    \n    all_features = list(df.columns)\n    df_indeces = list(df.index)\n    df.reset_index(drop=True, inplace=True)\n    \n    integer_features(df)\n\n    delete_miss_features = list(\n        (null_view.loc[null_view['PERCENT'] > skip_features_from_prediction_where_percent_missing_data_more_than]).index)\n    print(f'Exclude from the prediction, because missing data more than \\\n    {skip_features_from_prediction_where_percent_missing_data_more_than}% :\\n{delete_miss_features}')\n    print('')\n    all_miss_features = list(null_view.index)\n\n    for delete_feature in delete_miss_features:\n        all_miss_features.remove(delete_feature)\n        \n    \n    if target_feature in all_miss_features:  # moving target_feature to end of the prediction\n        all_miss_features.append(all_miss_features.pop(all_miss_features.index(target_feature)))\n        \n    \n    for el in all_miss_features:\n        print('\\n\\n\\n\\n')\n        \n        # select features as predictors\n        NaN_info(df)\n        lot_of_miss_features = list(\n            (null_view.loc[null_view['PERCENT'] > include_features_as_predictors_where_perc_missing_data_less_than]).index)\n        now_predictors = list(set(all_features)-set(lot_of_miss_features))\n        work_predictors = list(set(now_predictors) - set([el]))\n\n        \n        # missing data (data for prediction)\n        miss_indeces = list((df[pd.isnull(df[el])]).index)\n        miss_df = df.iloc[miss_indeces][:]\n        miss_df = miss_df[work_predictors]\n        encoding(work_predictors, df=miss_df)\n\n        \n        # data without NaN rows (X data for train, evaluation of model)\n        work_indeces = list(set(df_indeces) - set(miss_indeces))\n        if len(work_indeces) > use_n_rows_for_train_not_more_than:\n            randomlist = random.sample(range(0, len(work_indeces)), use_n_rows_for_train_not_more_than)\n            work_indeces = [work_indeces[i] for i in randomlist]\n        \n        work_df = df.iloc[work_indeces][:] \n        encoding(work_predictors, df=work_df)\n        X = work_df[work_predictors]\n        y = work_df[el]\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n\n        \n        # Info\n        feature_type = df[el].dtypes\n        percent_missing_data = null_view['PERCENT'][el]\n        print(f'Feature: {el},   type: {feature_type},   missing values: {percent_missing_data}%\\n')    \n        print(f'Shape for train dataframe: {(X.shape)}')\n        print(f'Unused features as predictors, because missing data more than {include_features_as_predictors_where_perc_missing_data_less_than}% :')\n        print(lot_of_miss_features)\n        print('')\n        \n        \n        # PREDICTIONS\n        if percent_missing_data < apply_fast_predictor_where_missing_data_less_than_percent:\n            \n            # FAST Predictions without tuning hyperparameters\n            \n            print('FAST prediction without tuning hyperparameters, because missing data less than 1%\\n')\n            best_params = {}\n            if feature_type == 'object' or feature_type == 'bool':\n                print('FAST CLASSIFIER:')\n                labelencoder = LabelEncoder()\n                y_train = labelencoder.fit_transform(y_train)\n                y_test = labelencoder.fit_transform(y_test)\n                lgbm = LGBMClassifier(n_jobs=-1, random_state=0)\n                lgbm.fit(X_train, y_train)\n                pred_test_lgb_FAST = lgbm.predict(X_test)\n                accuracy = accuracy_score(y_test, pred_test_lgb_FAST)\n                print('Evaluations:')\n                print(f'first 10 y_test: {y_test[:10]}')\n                print(f'first 10 y_pred: {pred_test_lgb_FAST[:10]}\\n')\n                f1 = f1_score(y_test, pred_test_lgb_FAST, average='weighted')\n                print(f'accuracy_score:      {accuracy}')\n                print(f'f1_score (weighted): {f1}')\n                \n                predict_classifier(best_params, X, y, miss_df)\n                counter_all_predicted_values += len(miss_indeces)\n                imput_missing_value_to_main_df(df, miss_indeces, pred_miss, el)\n\n            elif feature_type == 'float64' or feature_type == 'int64':\n                print('FAST REGRESSOR:')\n                \n                lgbm = LGBMRegressor(n_jobs=-1, random_state=0)\n                lgbm.fit(X_train, y_train)\n                pred_test_lgb_FAST = lgbm.predict(X_test)\n                MAE = mean_absolute_error(y_test,pred_test_lgb_FAST)\n                y_te = list(round(y_test[:10], 1))\n                y_pred = list(np.round(pred_test_lgb_FAST[:10], 1))\n                print('Evaluations:')\n                print(f'first 10 y_test: {y_te}')\n                print(f'first 10 y_pred: {y_pred}\\n')\n                print(f'mean_absolute_error: {MAE}')\n                print(f'mean for {el}: {df[el].mean()}')\n                \n                predict_regressor(best_params, X, y, miss_df)\n                counter_all_predicted_values += len(miss_indeces)\n                imput_missing_value_to_main_df(df, miss_indeces, pred_miss, el)\n\n            else:\n                print(f\"unprocessed feature: {el} - {feature_type} type\")\n                \n                  \n        else:\n            \n            # ADVANCED Predictions with tuning hyperparameters\n            \n            n_iter_for_RandomizedSearchCV = int(randomizedSearchCV_iter_plus_perc_missing_data + percent_missing_data * 1)\n            print(f'Iteration for RandomizedSearchCV: {n_iter_for_RandomizedSearchCV}\\n')\n            \n            if feature_type == 'object' or feature_type == 'bool':\n                print('ADVANCED CLASSIFIER:')\n                labelencoder = LabelEncoder()\n                y_train = labelencoder.fit_transform(y_train)\n                y_test = labelencoder.fit_transform(y_test)\n                hyperparms_tuning(CV, X_train, X_test, y_train, y_test, n_iter_for_RandomizedSearchCV, PARAMS, alg=LGBMClassifier, scoring='f1_weighted')\n                accuracy = accuracy_score(y_test, pred_test_lgb)\n                print('Evaluations:')\n                print(f'first 10 y_test: {y_test[:10]}')\n                print(f'first 10 y_pred: {pred_test_lgb[:10]}\\n')\n                f1 = f1_score(y_test, pred_test_lgb, average='weighted')\n                print(f'accuracy_score:      {accuracy}')\n                print(f'f1_score (weighted): {f1}')\n                \n                predict_classifier(best_params, X, y, miss_df)\n                counter_all_predicted_values += len(miss_indeces)\n                imput_missing_value_to_main_df(df, miss_indeces, pred_miss, el)\n\n            elif feature_type == 'float64' or feature_type == 'int64':\n                print('ADVANCED REGRESSOR:')\n                hyperparms_tuning(CV, X_train, X_test, y_train, y_test, n_iter_for_RandomizedSearchCV, PARAMS, alg=LGBMRegressor, scoring='neg_mean_squared_error')\n                MAE = mean_absolute_error(y_test,pred_test_lgb)\n                y_te = list(round(y_test[:10], 1))\n                y_pred = list(np.round(pred_test_lgb[:10], 1))\n                print('Evaluations:')\n                print(f'first 10 y_test: {y_te}')\n                print(f'first 10 y_pred: {y_pred}\\n')\n                print(f'mean_absolute_error: {MAE}')\n                print(f'mean for {el}: {df[el].mean()}')\n                \n                predict_regressor(best_params, X, y, miss_df)\n                counter_all_predicted_values += len(miss_indeces)\n                imput_missing_value_to_main_df(df, miss_indeces, pred_miss, el)\n\n            else:\n                print(f\"unprocessed feature: {el} - {feature_type} type\")\n        \n        plt.figure(figsize=(20, 5))\n        sns.heatmap(df.isnull(), cbar=False)\n\n        \n    for feature in int_features:\n        df[[feature]] = df[[feature]].astype('int64')\n        \n    df.index = df_indeces\n\n    print('\\n\\n\\n')\n    print(f'These features have not been processed, because missing data more than {skip_features_from_prediction_where_percent_missing_data_more_than}%')\n    print(NaN_info(df))\n    print('\\n\\n\\n')\n    print(f'{counter_all_predicted_values} values have been predicted and replaced')\n    print('\\n')\n    \n    return df","78b06868":"nan_predict(df,\n            target_feature = 'target')     # For prediction at the end\n","52147dba":"int_features = ['training_hours', 'city_num', 'company_size_num', \n                'last_new_job_num', 'experience']\nfor feature in int_features:\n        df[[feature]] = df[[feature]].astype('int64')","f8c2351b":"target_column = ['target']\npredictors = list(set(list(df.columns))-set(target_column))","774499f6":"sns.set(font_scale=1.5)\n\nfor el in predictors:\n    plot_data = df[['target', el]]\n    try:\n        g = sns.pairplot(plot_data, hue='target', palette='Set1', height=10, aspect=2)\n        \n        handles = g._legend_data.values()\n        labels = g._legend_data.keys()\n        g.fig.legend(handles=handles, labels=labels, loc='upper center', ncol=1)\n    except:\n        pass","bfed4a31":"df.columns","8d22af9b":"plot_data = df[['company_size', 'target']]\nplt.figure(figsize=(20,10))\nsns.countplot(x='company_size', hue='target', data=plot_data, palette='Set1')","39c2b446":"plot_data = df[['major_discipline', 'target']]\nplt.figure(figsize=(20,20))\nsns.countplot(x='major_discipline', hue='target', data=plot_data, palette='Set1')","219e6f7f":"plot_data = df[['education_level', 'target']]\nplt.figure(figsize=(20,10))\nsns.countplot(x='education_level', hue='target', data=plot_data, palette='Set1')","de9409fe":"plot_data = df[['enrolled_university', 'target']]\nplt.figure(figsize=(20,10))\nsns.countplot(x='enrolled_university', hue='target', data=plot_data, palette='Set1')","6e0e8242":"plot_data = df[['relevent_experience', 'target']]\nplt.figure(figsize=(20,10))\nsns.countplot(x='relevent_experience', hue='target', data=plot_data, palette='Set1')","d16887bf":"plot_data = df[['last_new_job', 'target']]\nplt.figure(figsize=(20,10))\nsns.countplot(x='last_new_job', hue='target', data=plot_data, palette='Set1')","a7a1177e":"plot_data = df[['experience_cat', 'target']]\nplt.figure(figsize=(20,10))\nsns.countplot(x='experience_cat', hue='target', data=plot_data, palette='Set1')","f5121f9f":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n\ndf_permutation = df.copy()\n\ntarget = ['target']\npredictors = list(set(list(df.columns)) - set(target))\n\n\ndef encoding(df, columns):\n    feature_power = 0.5          # Skew handling\n    for j in columns:\n        el_type = df[j].dtype\n        if el_type == 'object':\n            labelencoder = LabelEncoder()\n            df.loc[:, j] = labelencoder.fit_transform(df.loc[:, j])\n        else:\n            df[j] = df[j]**feature_power\n    return df, columns\n\nencoding(df_permutation, df_permutation.columns)\n\n\nX = df_permutation[predictors]\ny = df_permutation[target]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","5c03f682":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\ndef permutation(X_train, X_test, y_train, y_test, alg):\n    model = alg(n_jobs=-1, random_state=0).fit(X_train, y_train)\n    perm = PermutationImportance(model, random_state=0).fit(X_test, y_test)\n    return eli5.show_weights(perm, feature_names = X_test.columns.tolist())","a2d794e5":"from lightgbm import LGBMClassifier\npermutation(X_train, X_test, y_train, y_test, LGBMClassifier)","8fa765ca":"from lightgbm import LGBMRegressor\npermutation(X_train, X_test, y_train, y_test, LGBMRegressor)","dde3c8e6":"from sklearn.ensemble import RandomForestClassifier\npermutation(X_train, X_test, y_train, y_test, RandomForestClassifier)","d47b08ec":"from sklearn.ensemble import RandomForestRegressor\npermutation(X_train, X_test, y_train, y_test, RandomForestRegressor)","2081a0f8":"df.columns","edbe56e0":"from matplotlib import pyplot as plt\nfrom pdpbox import pdp\nfrom lightgbm import LGBMClassifier\n\n\n\n\nmodel = LGBMClassifier(random_state=0).fit(X_train, y_train)\n\nfor feature in X_train.columns:\n    pdp_dist = pdp.pdp_isolate(model=model,\n                               dataset=X_test,\n                               model_features=X_test.columns, \n                               feature=feature)\n\n    pdp.pdp_plot(pdp_dist, feature)\n    plt.show()","983252f4":"features_to_plot = ['target', 'city_development_index']\ninter1  =  pdp.pdp_interact(model=model, \n                            dataset=df_permutation, \n                            model_features=X_test.columns, \n                            features=features_to_plot)\n\npdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=features_to_plot, plot_type='contour')\nplt.show()","8b9ca808":"import shap\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n\nmy_model = RandomForestClassifier(n_estimators=30, random_state=1).fit(X_train, y_train)\n\ndef shap_force_plot(X_test, model, row):\n    data_for_prediction = X_test.iloc[row,:]\n    explainer = shap.TreeExplainer(my_model)\n    shap_values = explainer.shap_values(data_for_prediction)\n    shap.initjs()\n    return shap.force_plot(explainer.expected_value[0], shap_values[0], data_for_prediction)   ","3b30a7f8":"shap_force_plot(X_test, model, 0)","ba90bfe1":"shap_force_plot(X_test, model, 100)","943f5036":"shap_force_plot(X_test, model, 1000)","c2be649f":"shap_force_plot(X_test, model, 2000)","eb2dd513":"from xgboost import XGBClassifier\n\n\nmodel = XGBClassifier(random_state=0).fit(X_train, y_train)\n\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X_test)\n\nshap.summary_plot(shap_values, X_test)","0a777d30":"from sklearn.ensemble import RandomForestClassifier\n\n\n\nmy_model = RandomForestClassifier(n_estimators=30, random_state=1).fit(X_train, y_train)\n\nexplainer = shap.TreeExplainer(my_model)\ndata_1 = pd.concat([X, y], axis=1)\ndata_1 = data_1.iloc[0:100,:]\nshap_values = explainer.shap_values(data_1)\nshap.dependence_plot('city_development_index', shap_values[1], data_1, interaction_index=\"target\")","377a579c":"shap_values = explainer.shap_values(data_1)\nshap.dependence_plot('city', shap_values[1], data_1, interaction_index=\"target\")","8589e6ef":"shap_values = explainer.shap_values(data_1)\nshap.dependence_plot('experience_cat', shap_values[1], data_1, interaction_index=\"target\")","2b84d42d":"shap_values = explainer.shap_values(data_1)\nshap.dependence_plot('experience', shap_values[1], data_1, interaction_index=\"target\")","64cbfbe5":"shap_values = explainer.shap_values(data_1)\nshap.dependence_plot('company_size', shap_values[1], data_1, interaction_index=\"target\")","53b9be0f":"df['target'].replace(to_replace='no',   value=0.0, inplace=True)\ndf['target'].replace(to_replace='yes',  value=1.0, inplace=True)\ndf['target'] = df['target'].astype('float64')\ndf['target'].value_counts(dropna=False)\n","53a39a7c":"result = df.loc[19158:, 'target']\npredictions = np.array(result)\npredictions","dd77d508":"submission = pd.DataFrame({'enrollee_id':test_data['enrollee_id'],'target':predictions})\nsubmission","6eb89d74":"submission.to_csv('submission.csv', index=False)\nprint('Finish')","2d7cef5e":"### Permutation Importance LGBMClassifier","c8c16674":"# Data overview, cleaning and preprocessing","5617a573":"# Summary Plot","e7c6053a":"### Permutation Importance LGBMRegressor","6bf6a413":"# Pre-Processing and Feature Engineering","e7e5aaf6":"# Permutation Importance","958ced2b":"### Permutation Importance RandomForestRegressor","02c38cbf":"### Permutation function","cf55cf17":"# NaN prediction and imputation","40e525fd":"# SHAP","b6540cf5":"# Partial Plots","6f0cfac3":"# EDA","c6a5d86a":"### Permutation Importance RandomForestClassifier","ebceb6dd":"### Contact for joint processing","44b66616":"# Who will move to a new job?","083c65e9":"### train data","8b7731db":"### test data","88de0f50":"# Dependence Contribution Plot","0c796e5f":"# Create feature with experience title","ab039039":"### sample_submission.csv","3ac1fdfa":"# Submission"}}