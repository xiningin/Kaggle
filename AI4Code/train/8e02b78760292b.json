{"cell_type":{"22f3d9d4":"code","de4c8252":"code","f8ff4dcc":"code","e0ac4fb3":"code","eab6669f":"code","17137847":"code","8eef7f48":"code","ca4f842a":"code","028c8c4c":"code","00fa0f5e":"code","88f55965":"code","743e6a5b":"code","05aa0e18":"code","d821ce71":"code","b6ef4205":"code","f79f3024":"code","eaf8c94e":"code","a565f966":"code","9ba36eca":"code","1fbbf880":"code","37704dbf":"code","d2d0e748":"code","bb86cae1":"code","6f543e82":"code","28c2cb03":"code","835c4b3b":"code","904c50eb":"code","7fa03495":"code","0dcd40c7":"code","f4fc40c6":"code","ad853856":"code","81713a4a":"code","44fa7f2f":"code","7b6a64f5":"code","4eb9ca57":"code","dd9f8cbf":"markdown","dca23150":"markdown","00bee9da":"markdown","95282cad":"markdown","bfe3151a":"markdown","dafc7976":"markdown","fd50c5ad":"markdown","6392f0ec":"markdown","651c9c6f":"markdown","b1045817":"markdown","982033fc":"markdown","c3aaa98a":"markdown","27794dee":"markdown","a38a13ab":"markdown","b51a0c26":"markdown","4875551c":"markdown","345e5e4e":"markdown","c2dd666f":"markdown","0303aa60":"markdown","2c7d7ced":"markdown","24a5e58b":"markdown","1afb3ccf":"markdown","dc61a432":"markdown","ca3fcf8b":"markdown","0e22ca2c":"markdown"},"source":{"22f3d9d4":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","de4c8252":"data=pd.read_csv('..\/input\/twitter-airline-sentiment\/Tweets.csv')","f8ff4dcc":"data.head(10)","e0ac4fb3":"data.isnull().sum() * 100 \/ len(data)","eab6669f":"data.columns","17137847":"data.shape","8eef7f48":"print(data['airline_sentiment'].value_counts())\nsns.countplot(data['airline_sentiment'])","ca4f842a":"print(data.airline.value_counts())\nsns.countplot(data['airline'])","028c8c4c":"#remove unwanted column from the data \ndel data['airline_sentiment_gold']\ndel data['negativereason_gold']\ndel data['tweet_coord']","00fa0f5e":"#calculate no of sentiment for each airline \nplt.figure(1,figsize=(10,10))\nairlines= ['US Airways','United','American','Southwest','Delta','Virgin America']\nfor i in airlines:\n    indices=airlines.index(i)\n    plt.subplot(2,3,indices+1)\n    new_value=data[data['airline']==i]\n    print(new_value['airline_sentiment'].value_counts(),i)\n    Index = [1,2,3]\n    sns.countplot(new_value['airline_sentiment'])\n    plt.title('Count of Moods of '+i)","88f55965":"#install library of nltk to  clean the text data\nimport nltk\nfrom wordcloud import WordCloud,STOPWORDS\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nimport re\n\nnltk.download('stopwords')\nstop_words = stopwords.words('english')","743e6a5b":"new_data=data[data['airline_sentiment']=='negative']\nwords = ' '.join(new_data['text'])\ncleaned_word=' '.join([word for word in words.split()\n                         if 'http' not in word\n                         and not word.startswith('@')\n                          and word !='RT'])\nwordcloud = WordCloud(stopwords=STOPWORDS,\n                      background_color='black',\n                      width=3000,\n                      height=2500\n                     ).generate(cleaned_word)\nplt.figure(1,figsize=(10,10))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","05aa0e18":"new_data=data[data['airline_sentiment']=='positive']\nwords = ' '.join(new_data['text'])\ncleaned_word=' '.join([word for word in words.split()\n                         if 'http' not in word\n                         and not word.startswith('@')\n                          and word !='RT'])\nwordcloud = WordCloud(stopwords=STOPWORDS,\n                      background_color='black',\n                      width=3000,\n                      height=2500\n                     ).generate(cleaned_word)\nplt.figure(1,figsize=(10,10))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","d821ce71":"neutral_text=data[data['airline_sentiment']=='neutral']\nwords=''.join(neutral_text['text'])\ncleaned_word=' '.join([word for word in words.split()\n                      if 'http' not in word\n                       and not word.startswith('@')\n                        and word!='RT'])\nwordcloud=WordCloud(stopwords=STOPWORDS,\n                    background_color='black',\n                      width=3000,\n                      height=2500).generate(cleaned_word)\nplt.figure(1,figsize=(10,10))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","b6ef4205":"print(data.negativereason.value_counts())\nplt.figure(figsize=(25,5))\nsns.countplot(data.negativereason)","f79f3024":"plt.figure(1,figsize=(35,36))\nairline=['US Airways','United','American','Southwest','Delta','Virgin America']\nfor i in airline:\n    indices=airline.index(i)\n    plt.subplot(3,2,indices+1)\n    new_values=data[data['airline']==i]\n    Index=[1,2,3,4,5,6,7,8,9,10]\n    sns.countplot(new_values['negativereason'])\n    plt.title('count of Moods of'+i)","eaf8c94e":"text_cleaning_re = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"","a565f966":"def preprocess(x,stem=False):\n    x=re.sub(text_cleaning_re,'  ',str(x).lower()).strip()\n    tokens=[]\n    for token in x.split('\\n'):\n        if token not in stop_words:\n            if stem:\n                tokens.append(stemmer.stem(token))\n            else:\n                tokens.append(token)\n        return '  '.join(tokens)\ndata.text=data.text.apply(lambda x:preprocess(x))","9ba36eca":"data['sentiment']=data['airline_sentiment'].apply(lambda x: 0 if x=='negative' else 1)","1fbbf880":"print(data.sentiment.value_counts())","37704dbf":"y=data.sentiment","d2d0e748":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(min_df=10)\nX= vectorizer.fit_transform(data.text)","bb86cae1":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2, random_state=0)","6f543e82":"model = sklearn.linear_model.LogisticRegression(penalty=\"l2\", C=0.1)\nmodel.fit(X_train, y_train)","28c2cb03":"score = model.score(X_test, y_test)\nscore","835c4b3b":"import shap\nexplainer = shap.LinearExplainer(model, X_train, feature_dependence=\"independent\")\nshap_values = explainer.shap_values(X_test)\nX_test_array = X_test.toarray() # we need to pass a dense version for the plotting functions","904c50eb":"shap.summary_plot(shap_values, X_test_array, feature_names=vectorizer.get_feature_names())","7fa03495":"from sklearn.metrics import classification_report, confusion_matrix\nsns.heatmap(confusion_matrix(y_test, model.predict(X_test)))\nprint(classification_report(y_test, model.predict(X_test)))","0dcd40c7":"from sklearn.ensemble import RandomForestClassifier\nmodels=RandomForestClassifier()\nmodel.fit(X_train, y_train)","f4fc40c6":"score = model.score(X_test, y_test)\nscore","ad853856":"from sklearn.metrics import classification_report, confusion_matrix\nprint(classification_report(y_test, model.predict(X_test)))\nsns.heatmap(confusion_matrix(y_test, model.predict(X_test)))","81713a4a":"from sklearn.svm import SVC\nsvm=SVC()\nsvm=SVC(kernel=\"rbf\", C=0.025, probability=True)","44fa7f2f":"svm.fit(X_train,y_train)","7b6a64f5":"score = model.score(X_test, y_test)\nscore","4eb9ca57":"from sklearn.metrics import classification_report, confusion_matrix\nprint(classification_report(y_test, model.predict(X_test)))\nsns.heatmap(confusion_matrix(y_test, model.predict(X_test)))","dd9f8cbf":"***Percentage of the negative reason***","dca23150":"# Clean the text data and then we check most postive and most negative comment on the text data","00bee9da":"**ANY FEEDBACK IN THE COMMENTS WILL BE HIGHLY APPRECIATED**","95282cad":"***Word cloud:-visual representation of the text data They are very popular because they help us easily spot the most frequently occurring words. The more frequent the word is used, the larger and bolder it is displayed. That the help to tackel the main problem ***","bfe3151a":"**using simple ml model to predict the sentiment of the user**\n","dafc7976":"*postive and neutrl sentiment aree coherent with each other thats why we using label encoding technique 1 denote postive and negative sentiment convert to the 0*","fd50c5ad":"![](https:\/\/www.kdnuggets.com\/images\/sentiment-fig-1-689.jpg)","6392f0ec":"**Data show that the when customer get not good Service from the airline then tweet about it the wrtite less twee when get good service from the airline**","651c9c6f":"***Neagative reason by airline that the get type of reviews gave by the customer***","b1045817":"Random Forest:-The Random Forest  classifiers are suitable for dealing with the high dimensional noisy data in text classification","982033fc":"# *null value in percentage*","c3aaa98a":"***Data processing technique remove noise from the text data ***\n**Different text processing techniques:**\n**Tokenization:-separating a piece of text into smaller units the are bascilly called the tokens Tokenization is essentially splitting a phrase, sentence, paragraph, or an entire text document into smaller units, such as individual words or terms**","27794dee":"**United, US Airways, American substantially get negative reactions.\nTweets for Virgin America are the most balanced.\n**","a38a13ab":"# Total no of tweet for each airline","b51a0c26":"*Neutral sentiment about the airline*","4875551c":"# Here we check airline sentiment for each airline","345e5e4e":"***machine can never stand the word thats why we convert word to vector that help to easy undderstand to the machine differnet type of vectorizaion technique used in ml .\nTF-IDF are sparse vectors where the number of non-zero values in the vector is equal to the number of unique words in the document***\n![](https:\/\/miro.medium.com\/max\/1838\/1*qQgnyPLDIkUmeZKN2_ZWbQ.png)","c2dd666f":"united airways get more tweet\nvirgin America get the least tweet","0303aa60":"# EDA OF THE data sets","2c7d7ced":"* Sentiment analysis :- Sentiment analysis help to get wider public opinion that used by businesses to detect sentiment in social data, gauge brand reputation, and understand customers allows brands to learn what makes customers happy or frustrated","24a5e58b":"HERe check about the airline sentiment about the user ","1afb3ccf":"**SVC:-is an algorithm that determines the best decision boundary between vectors that belong to a given group (or category) and vectors that do not belong to it.**","dc61a432":"***That\u2019s the way to calculate the Shapley value: It is the average of the marginal contributions across all permutations theoretic approach to explain the output of any machine learning model. It connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions (see papers for details and citations).***","ca3fcf8b":"***Logistic regression :-logistic regression basscially used in the sigmoid function that help to the where the user stasfied or not***","0e22ca2c":"*postive sentiment about the airline* "}}