{"cell_type":{"bf97c4e8":"code","83e26f3b":"code","5c6d8157":"code","0d7bef76":"code","a7a8e243":"code","439196e2":"code","665d08b6":"code","a0d4f73f":"code","a7015824":"code","943c17bb":"code","f317f433":"code","46b6db22":"code","2e0ba3f8":"markdown","f34242d2":"markdown","b722782f":"markdown","4eb485a2":"markdown"},"source":{"bf97c4e8":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom sklearn.datasets import make_regression","83e26f3b":"class linear_regression:\n    def __init__(self,features=1,epochs=1000,learning_rate=0.001,regularization=\"None\",lambda_=1):\n        self.features=features\n        ## Initializing weights and biases to 0\n        self.weights=np.zeros(features)\n        self.bias=0\n        self.epochs=epochs\n        self.regularization=regularization\n        self.lambda_=lambda_\n        self.learning_rate=learning_rate\n    def mse(self,y,y_hat):\n        diff=y-y_hat\n        reg=0.\n        if(self.regularization==\"L1\"):\n            reg=np.sum(np.abs(self.weights))*self.lambda_\n        if(self.regularization==\"L2\"):\n            reg=np.sum(self.weights**2)*self.lambda_\n        return np.sum(diff**2)\/(2*len(y))+reg\n    def fit(self,x,y,print_every_nth_epoch=1):\n        assert self.features==x.shape[1],\"Number of features don't match\"\n        assert y.shape[0]==x.shape[0],\"Number of entries don't match\"\n        n=x.shape[0]\n        for epoch in range(self.epochs):\n            y_bar=np.dot(x,self.weights)+self.bias\n            diff=y-y_bar\n            grad_w=np.dot(x.T,diff)*self.learning_rate\/n\n            if(self.regularization==\"L1\"):\n                sign=np.where(self.weights>0,1,-1)\n                grad_w+=sign*self.lambda_\n            if(self.regularization==\"L2\"):\n                grad_w+=self.lambda_*2*self.weights\n            self.weights+=grad_w\n            grad_b=np.sum(diff)*self.learning_rate\/n\n            self.bias+=grad_b\n            if((epoch+1)%print_every_nth_epoch==0):\n                print(\"--------- epoch {} -------> loss={} ----------\".format((epoch+1),self.mse(y,y_bar)))\n\n    def predict(self,x):\n        assert self.features==x.shape[1],\"Number of features don't match\"\n        return np.dot(x,self.weights)+self.bias","5c6d8157":"x,y=make_regression(n_features=1,n_samples=200,bias=20,noise=40)\nx = preprocessing.scale(x)\ny = preprocessing.scale(y)\nplt.scatter(x,y)","0d7bef76":"model=linear_regression(epochs=500,learning_rate=0.01)\nmodel.fit(x,y,print_every_nth_epoch=100)","a7a8e243":"test_x=np.array([[-1],[1.99]])\ntest_y=model.predict(test_x)\nplt.scatter(x,y,c='g')\nplt.plot(x,model.predict(x))\nplt.scatter(test_x,test_y,c='r',marker='*',s=300)\nplt.title(\"vanilla linear regression\")\nplt.show()","439196e2":"x1=pd.DataFrame(np.arange(-1,1,0.03))\ny1=x1**3+x1**2\nprint(x1.head())\nprint(y1.head())\nplt.scatter(x1,y1)\nx1[1]=x1[0]**2\nx1[2]=x1[0]**3\nx1=x1.to_numpy()\ny1=y1.to_numpy().T[0]","665d08b6":"model1=linear_regression(features=3,epochs=800,learning_rate=0.1)\nmodel1.fit(x1,y1,print_every_nth_epoch=100)","a0d4f73f":"plt.scatter(x1.T[0],y1)\nplt.plot(x1.T[0],model1.predict(x1),c=\"r\")\nplt.title(\"Polynomial Regression\")\nplt.show()","a7015824":"model2=linear_regression(regularization=\"L2\",lambda_=0.0001)\nmodel2.fit(x,y,print_every_nth_epoch=100)","943c17bb":"plt.plot(x,model2.predict(x),c=\"r\")\nplt.scatter(x,y,c='g')\n\nplt.title(\"Ridge Regression\")\nplt.show()","f317f433":"model3=linear_regression(regularization=\"L1\",lambda_=0.00001,features=1)\nmodel3.fit(x,y,print_every_nth_epoch=100)","46b6db22":"plt.scatter(x,y)\nplt.plot(x,model3.predict(x),c=\"r\")\nplt.title(\"Lasso Regression\")\nplt.show()","2e0ba3f8":"## Linear Regression using Gradient Descent\n\n- Best fit line represented by y = m1 * x1 + m2 * x2 + m3 * x3 + ... + c\n- Fit of line calculated by mean squared error\n- parameters computed using gradient descent","f34242d2":"### Lasso Regression\n- It is similar to ridge regression as it penalizes weights\n- It can minimize weights to 0\n- It can thus help in feature selection","b722782f":"### Trying polynomial regression\n- Multiple regression can be used for non linear data\n- We add polynomial features thus transforming data to higher dimension where it is linear","4eb485a2":"### Rigde Regression\n\n- We penalize weights by adding w^2 to loss function\n- It trades bias for lower variance"}}