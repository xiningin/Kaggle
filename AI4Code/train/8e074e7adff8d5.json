{"cell_type":{"2c8a5df0":"code","9cdb4b04":"code","5495a3cf":"code","b5b45221":"code","80192855":"code","147d3a80":"code","64b02f2f":"code","3940616b":"code","5390751d":"code","5ebb2126":"code","5ef1e82a":"code","631a0ebe":"code","08484aa0":"code","b7d5fced":"code","19d92b2a":"code","9cbddc4d":"code","ef68a432":"code","dfed6536":"code","a0d87525":"code","11e1c40b":"code","0c862bd3":"code","954d182c":"code","ef8d5c4c":"code","46543e69":"code","9ee6f58d":"code","5812a186":"code","3239dc77":"code","60a0a846":"code","19676ff3":"code","a5aa9232":"code","64386e53":"code","1edf111b":"code","5cb1f556":"code","c454b0cc":"code","747cd260":"code","b9721beb":"markdown","8dbac44f":"markdown","47ff779e":"markdown","958bb177":"markdown","2b348712":"markdown","0c21210a":"markdown","46b76b23":"markdown","70a9852c":"markdown","5216ffb4":"markdown","5b228f0f":"markdown","7794b6b8":"markdown","bf66f7cd":"markdown","ea8329a0":"markdown","c8d8ddbc":"markdown","203a04bd":"markdown","db72c958":"markdown","f51e22eb":"markdown","d49118e8":"markdown","7ae9cade":"markdown","0005f29c":"markdown","ab108a85":"markdown","0d4ef248":"markdown","2e32b091":"markdown","18e26bc3":"markdown","157337e6":"markdown","420b9ca8":"markdown","e30afce3":"markdown","fca1065f":"markdown","9046683c":"markdown","70a76567":"markdown","246e5256":"markdown","578a1e67":"markdown","eb11312c":"markdown","bf1e0fce":"markdown","7d342bc8":"markdown","ead57395":"markdown"},"source":{"2c8a5df0":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.rcParams['figure.figsize'] = (20.0, 10.0)","9cdb4b04":"df = pd.read_csv('..\/input\/apple-aapl-historical-stock-data\/HistoricalQuotes.csv')\ndf.head()","5495a3cf":"df.shape","b5b45221":"df.info()","80192855":"df1 = df.iloc[:,1]\ndf1.head()","147d3a80":"df1 = df1.replace('[\\$,]', '', regex=True).astype(float)\ndf1.head()","64b02f2f":"plt.plot(df1)","3940616b":"from statsmodels.tsa.seasonal import seasonal_decompose\n\nresult  = seasonal_decompose(pd.Series(df1),period=100)\n\nresult.plot()\nplt.title('Apple Stock')\nplt.show()","5390751d":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler(feature_range = (0,1))","5ebb2126":"df1.shape","5ef1e82a":"df1 = scaler.fit_transform(np.array(df1).reshape(-1,1))","631a0ebe":"print(df1)\nprint('\\n df1 present shape : ', df1.shape)","08484aa0":"training_size = int(len(df1)*0.7)\ntest_size = len(df1) - training_size\nprint('Training Size : ',training_size)\nprint('Test Size : ',test_size)","b7d5fced":"train_data, test_data = df1[0:training_size,:],df1[training_size:len(df1),:]\nprint('Training Data Shape : ', train_data.shape)\nprint('Test Data Shape: ', test_data.shape)","19d92b2a":"def create_dataset(dataset, window=1):\n    dataX, dataY= [], []\n    for i in range(len(dataset)-window-1):\n        a = dataset[i:(i+window),0]\n        dataX.append(a)\n        dataY.append(dataset[i+window,0])\n    return np.array(dataX), np.array(dataY)","9cbddc4d":"window = 100\nX_train, y_train = create_dataset(train_data, window=100)\nX_test, y_test = create_dataset(test_data, window=100)","ef68a432":"print(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)","dfed6536":"X_train = X_train.reshape(X_train.shape[0],X_train.shape[1],1)\nX_test = X_test.reshape(X_test.shape[0],X_test.shape[1],1)","a0d87525":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import LSTM","11e1c40b":"model = Sequential()\nmodel.add(LSTM(50, return_sequences = True, input_shape = (100,1)))\nmodel.add(LSTM(50, return_sequences = True))\nmodel.add(LSTM(50))\nmodel.add(Dense(1))\n\nmodel.compile(loss='mean_squared_error', optimizer = 'adam')","0c862bd3":"model.summary()","954d182c":"model.fit(X_train,y_train,validation_data = (X_test,y_test), epochs = 100, batch_size=64,verbose=1)","ef8d5c4c":"y_pred = scaler.inverse_transform(model.predict(X_test))","46543e69":"y_test = scaler.inverse_transform(y_test.reshape(-1, 1))","9ee6f58d":"y_pred[:5]","5812a186":"y_test[:5]","3239dc77":"import math\nfrom sklearn.metrics import mean_squared_error\n\nrmse_score = math.sqrt(mean_squared_error(y_test,y_pred))\nprint('Root Mean Squared Error(test) : ',rmse_score)","60a0a846":"from sklearn.metrics import r2_score\nprint('R-squared Score  : ',r2_score(y_test,y_pred))","19676ff3":"# shift train predictions for plotting\ntrain_predict = scaler.inverse_transform(model.predict(X_train))\n\nlook_back=100\ntrainPredictPlot = np.empty_like(df1)\ntrainPredictPlot[:, :] = np.nan\ntrainPredictPlot[look_back:len(train_predict)+look_back, :] = train_predict\n# shift test predictions for plotting\ntestPredictPlot = np.empty_like(df1)\ntestPredictPlot[:, :] = np.nan\ntestPredictPlot[len(train_predict)+(look_back*2)+1:len(df1)-1, :] = y_pred\n# plot baseline and predictions\nplt.plot(scaler.inverse_transform(df1))\nplt.plot(trainPredictPlot)\nplt.plot(testPredictPlot)\nplt.legend()\nplt.show()","a5aa9232":"x_input=test_data[len(test_data)-100:].reshape(1,-1)\nx_input.shape","64386e53":"temp_input=list(x_input)\ntemp_input=temp_input[0].tolist()","1edf111b":"# demonstrate prediction for next 30 days\nfrom numpy import array\n\nlst_output=[]\nn_steps=100\ni=0\nwhile(i<30):\n    \n    if(len(temp_input)>100):\n        #print(temp_input)\n        x_input=np.array(temp_input[1:])\n        #print(\"{} day input {}\".format(i,x_input))\n        x_input=x_input.reshape(1,-1)\n        x_input = x_input.reshape((1, n_steps, 1))\n        #print(x_input)\n        yhat = model.predict(x_input, verbose=0)\n        #print(\"{} day output {}\".format(i,yhat))\n        temp_input.extend(yhat[0].tolist())\n        temp_input=temp_input[1:]\n        #print(temp_input)\n        lst_output.extend(yhat.tolist())\n        i=i+1\n    else:\n        x_input = x_input.reshape((1, n_steps,1))\n        yhat = model.predict(x_input, verbose=0)\n        #print(yhat[0])\n        temp_input.extend(yhat[0].tolist())\n        #print(len(temp_input))\n        lst_output.extend(yhat.tolist())\n        i=i+1\n    \n\n#print(lst_output)","5cb1f556":"day_new=np.arange(1,101)\nday_pred=np.arange(101,131)","c454b0cc":"print(scaler.inverse_transform(lst_output))","747cd260":"plt.plot(day_new,scaler.inverse_transform(df1[len(df1)-100:]))\nplt.plot(day_pred,scaler.inverse_transform(lst_output))","b9721beb":"# Predict 30 Days future value","8dbac44f":"We will be using a 70%-30% train test split. But before applying splitting, we need to window the data. For that, we will store the training and testing data and their shapes in some variables.","47ff779e":"We will be using the **Sequential, Dense and LSTM layers** to build the model. Ous model will be a **stacked LSTM model**. That means there will be more than one LSTM layer. ","958bb177":"## Windowing Dataset\n\nFor better performance of any time series (univariate), it is necessary to use the **splitting window** on the dataset. The concept is simple. We will convert the dataset into several overlapping series. You will have an idea by seeing the picture below. \n![sliding%20window.JPG](attachment:sliding%20window.JPG)\nThe figure shows the window size = 2. We will be using suitable window size for the best performance. You can try with any number you want. It is a hyperparameter that is needed to be tuned. ","2b348712":"It is clear that the df1 is a vector. But the problem is MinMaxScaler works on numpy 2D arrays, not on vectors. So, we will convert df1 to 2D array using **np.array(df1).reshape(-1,1))** and then apply the scaling. ","0c21210a":"The training and testing data both have 100 columns in their independent variable (X) side. \nAt last, before passing the data to the model, we need to reshape the data to make is 3D as LSTM are build to only accept 3D data.  ","46b76b23":"# Visualization","70a9852c":"# Evaluation","5216ffb4":"We can see above that the data is now scaled and the shape of df1 is 2D. Now, we will move on to the train test splitting. ","5b228f0f":"We now train our model with the training data over 100 epochs and with a batch size = 64","7794b6b8":"# Introduction\n\nIn this notebook, I will guide you a step by step on a journey to predict the future stock price of Apple.Inc based on their previous 10 years' data. \n\n**N.B: Stock price is an uncertain thing. So, any prediction may drastically differ from the real world value. So, please do not use this for real stock market usage.**","bf66f7cd":"# Dataset\n\nWe are going to use the [Apple AAPL Historical Stock Dataset](https:\/\/www.kaggle.com\/tarunpaparaju\/apple-aapl-historical-stock-data) and store the data in df variable. We will have a look at the dataset using **df.head()**, it will show the first 5 entries of the dataset. ","ea8329a0":"# LSTM Model","c8d8ddbc":"Now, as the model is trained, lets check its prediction on the test data. We will store the prediction in **y_pred**, but as the data is scaled, so, we will inverse scale the data using **scaler.inverse_transform()** method. ","203a04bd":"For predicting the 30 days future values, we need to predict each days values first, then use that value along with 99 others to make a series of 100 (window size is set to 100). The following code does exactly that, and iteratively predicts the 30 days stock value. The values are stored and finally displayed and visualized.","db72c958":"# Data Preprocessing","f51e22eb":"# Train Test Split","d49118e8":"# Fitting the data to the model","7ae9cade":"Now, it is time to plot the data. Let's use plt.plot from the matplotlib library to plot the data and visualize the behavior. ","0005f29c":"We will now visualize the whole data, training and testing part in a single graph using the following code. ","ab108a85":"# Conclustion\n\nOur model performed good at predicting the Apple Stock price usign a Stacked LSTM model. This entire notebook can be reused in any stock price prediction with the proper window size. \n\nAt last, I would like to thank Mr. Krish Naik, who has showed me the guideline to make this work done. You can check his channel [here](youtube.com\/channel\/UCNU_lfiiWBdtULKOw6X0Dig). Also, you can watch his LSTM's video on APple Stock Price Prediction from [this link](https:\/\/www.youtube.com\/watch?v=H6du_pfuznE&t=1902s).\n\n**At last, I hope, you enjoyed my notebook. If so, then your appreciation will motivate me a lot to write such notebooks in the upcoming days. Thank you.**","0d4ef248":"To make sure that the sliding window worked, lets check the shape. If the dataset now do not have 100 columns then that means the windowing was not performed properly. ","2e32b091":"We can see the data in the selected column are not numbers, rather they are objects (currency) as they have Dollar sign as the prefix. Now, we will use regular expression to substitute the Dollar sign with a null '' and typecast the values to float. Let's see how they will look after this preprocessing  ","18e26bc3":"Lest choose **window size = 100** for now and apply the windowing on training and testing data. ","157337e6":"R-squared Score of 0.99 shows that our model predictionline is very well fitted to the actual line. ","420b9ca8":"We can see there are many features. Lets check the shape of the dataset using **df.shape**","e30afce3":"We are seeing that there are total 2518 rows and 6 columns in the dataset. Now, lets have a deeper look into our dataset to get some more information. We are going to use **df.info()**. This will help us view the datatypes, missing values and many more insights of the dataset and its features. ","fca1065f":"We can see the first 5 prediction of the model, the prices are very close to the real values. So, we can say that our model has a good accuracy in prediction. So, now lets mathematically check the **error** and **r2 score** to have a idea about how good are model is. ","9046683c":"Now, lets select one feature to make the problem a univariate one. We will be selecting the **Close\/Last** stock price. For that we will use the **df.iloc** and select all the rows and, only the 2nd column. Lets see the new dataframe, df1 using **df1.head()**","70a76567":"Now, before applying scaling, lets see the original data shape.","246e5256":"As it is a regression like problem, so we will be using the **loss='mean_squared_error'**","578a1e67":"Lets see the summary of the model.","eb11312c":"It seems that the data has some seasonality and a clear downward trend. But, to be sure about these **seasonality, trend and residuals**, lets use the **statsmodel** and use the **seasonal decompose** to visualize the components of this time series. ","bf1e0fce":"So, we have now a general idea of the stock price data's behavior. Now, lets Normalize the data. Normalization is a very important part for any **Recurrent Neural Network**. For our LSTM model, normalization will play an important role. Normalization using **MinMaxScaler** will bring our entire datapoints between a minimum and a maximum value. For this purpose, we will use the values **(0,1)**.","7d342bc8":"Lets also inverse scale the y_train data so that we can compare the data.","ead57395":"As the root mean squared error is only 1.66, it indicates that our model has predicted the stock prices very close to that real values. "}}