{"cell_type":{"07c9b499":"code","3caf767c":"code","0ea86673":"code","1eb147a6":"code","a821ba2b":"code","78487087":"code","d45c7193":"code","41e08293":"code","4b4f0552":"code","af55e2fb":"code","3399fcac":"code","9f11f259":"code","e80d4ab2":"code","e438f1c8":"code","7997322d":"code","2a0d7bd0":"code","a36c3e08":"code","282b3299":"code","92967293":"code","8c14e4d3":"code","87b426ed":"code","7e0464af":"code","cbb0ab11":"code","e1557363":"code","e1091c67":"code","02efc123":"code","8b7d1138":"code","bfeb6e6f":"code","2cc1e991":"code","d66a12eb":"code","03b6942d":"code","d05538b9":"code","6cd9334e":"code","76ea6866":"markdown","6b43c422":"markdown","5a1b4b8c":"markdown","df66e525":"markdown","ed47ecf9":"markdown","b2e92812":"markdown","f32af7e8":"markdown","c38ebd38":"markdown","b5dd4205":"markdown","cda623f1":"markdown","23722eed":"markdown","15a4a547":"markdown","72d2bca2":"markdown","6a6b93e4":"markdown","ca4a3978":"markdown","81b7ddba":"markdown","aa5bd0bb":"markdown","77671687":"markdown"},"source":{"07c9b499":"!pip install -q nlpretext loguru","3caf767c":"import os\nimport gc\nimport copy\nimport time\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n%config InlineBackend.figure_format = 'svg'\n\nfrom tqdm import tqdm\nfrom collections import defaultdict\n\nfrom loguru import logger\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","0ea86673":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda import amp\n\nimport transformers\nfrom transformers import BertTokenizer, BertModel, BertConfig\nfrom transformers import AdamW, get_linear_schedule_with_warmup\n\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom scipy.stats import ks_2samp","1eb147a6":"from nlpretext import Preprocessor\nfrom nlpretext.basic.preprocess import (normalize_whitespace, remove_punct, \n                                        remove_eol_characters, remove_stopwords, \n                                        lower_text, unpack_english_contractions)\n\nfrom nlpretext.social.preprocess import remove_html_tags","a821ba2b":"%%time\ndf_train = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")","78487087":"df_train.info()","d45c7193":"# Sample Text before preprocessing\n\ndf_train['excerpt'][0]","41e08293":"preprocessor = Preprocessor()\npreprocessor.pipe(unpack_english_contractions)\npreprocessor.pipe(remove_eol_characters)\npreprocessor.pipe(lower_text)\npreprocessor.pipe(normalize_whitespace)\npreprocessor.pipe(remove_punct)\n#preprocessor.pipe(remove_stopwords(lang='en'))","4b4f0552":"df_train['excerpt'] = df_train['excerpt'].apply(preprocessor.run)","af55e2fb":"df_test['excerpt'] = df_test['excerpt'].apply(preprocessor.run)","3399fcac":"# Sample Text after preprocessing\n\ndf_train['excerpt'][0]","9f11f259":"def max_words(df):\n    lengths = df['excerpt'].apply(lambda x: len(x.split()))\n    return max(lengths)","e80d4ab2":"excerpt_lenghts = df_train['excerpt'].apply(lambda x: len(x.split()))\nsns.histplot(excerpt_lenghts)","e438f1c8":"max(excerpt_lenghts)","7997322d":"class CFG:\n    \n    max_len = 218\n    \n    train_batch_size = 8\n    valid_batch_size = 16\n    \n    epochs = 5\n    learning_rate = 1e-5\n    n_accumulate = 1\n    folds = 5\n    \n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')","2a0d7bd0":"def create_folds(df, n_s=5, n_grp=None):\n    df['Fold'] = -1\n    \n    if n_grp is None:\n        skf = KFold(n_splits=n_s)\n        target = df.target\n    else:\n        skf = StratifiedKFold(n_splits=n_s)\n        df['grp'] = pd.cut(df.target, n_grp, labels=False)\n        target = df.grp\n    \n    for fold_no, (t, v) in enumerate(skf.split(target, target)):\n        df.loc[v, 'Fold'] = fold_no\n    return df","a36c3e08":"df_train = create_folds(df_train, n_s=CFG.folds, n_grp=100)\ndf_train['Fold'].value_counts()","282b3299":"def viz_tgt_folds(df):\n    fig, axs = plt.subplots(1, 5, sharex=True, sharey=True, figsize=(10,4))\n    for i, ax in enumerate(axs):\n        ax.hist(df[df.Fold == i]['target'], bins=100, density=True, label=f'Fold-{i}')\n        if i == 0:\n            ax.set_ylabel('Frequency')\n        if i == 2:\n            ax.set_xlabel(\"Target\")\n        ax.legend(frameon=False, handlelength=0)\n    plt.tight_layout()\n    plt.show()\n    \n    return None","92967293":"viz_tgt_folds(df)","8c14e4d3":"for fold in np.sort(df.Fold.unique())[1:]:\n    print(f'Fold 0 vs {fold}:', ks_2samp(df.loc[df.Fold==0,'target'], df.loc[df.Fold==fold,'target']))","87b426ed":"class BERTDset(Dataset):\n    def __init__(self, df, tokenizer, max_len):\n        self.text = df['excerpt'].values\n        self.target = df['target'].values\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n        \n    def __len__(self):\n        return len(self.text)\n    \n    def __getitem__(self, index):\n        text = self.text[index]\n        inputs = self.tokenizer.encode_plus(\n            text,\n            None,\n            truncation=True,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            return_token_type_ids=True\n        )\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n        token_type_ids = inputs[\"token_type_ids\"]\n        \n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n            'target': torch.tensor(self.target[index], dtype=torch.float)\n        }","7e0464af":"def eval_metrics(outputs, targets):\n    return torch.sqrt(nn.MSELoss()(outputs, targets))","cbb0ab11":"class BERTClass(nn.Module):\n    def __init__(self):\n        super(BERTClass, self).__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        self.dropout = nn.Dropout(0.3)\n        self.fc = nn.Linear(768, 1)\n    \n    def forward(self, ids, mask, token_type_ids):\n        _, output = self.bert(ids, attention_mask = mask, \n                              token_type_ids = token_type_ids, \n                              return_dict=False)\n        output = self.dropout(output)\n        output = self.fc(output)\n        return output\n\nmodel = BERTClass()\nmodel.to(CFG.device);","e1557363":"def training(model,optimizer,scheduler,dataloader,device,epoch):\n    model.train()\n    scaler = amp.GradScaler()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, data in bar:        \n        ids = data['ids'].to(device, dtype = torch.long)\n        mask = data['mask'].to(device, dtype = torch.long)\n        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n        targets = data['target'].to(device, dtype = torch.float)\n        \n        batch_size = ids.size(0)\n        \n        with amp.autocast(enabled=True):\n            outputs = model(ids, mask, token_type_ids)\n            loss = eval_metrics(outputs, targets)\n            loss = loss \/ CFG.n_accumulate\n            \n        scaler.scale(loss).backward()\n        \n        if (step + 1) % CFG.n_accumulate == 0:\n            scaler.step(optimizer)\n            scaler.update()\n            \n            # zero the parameter gradients\n            optimizer.zero_grad()\n            \n            if scheduler is not None:\n                scheduler.step()\n                \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        \n        epoch_loss = running_loss\/dataset_size\n        \n        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss,\n                        LR=optimizer.param_groups[0]['lr'])\n    gc.collect()\n    \n    return epoch_loss","e1091c67":"def inferance(model,optimizer,scheduler,dataloader,device,epoch):\n    \n    model.eval()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, data in bar:        \n        ids = data['ids'].to(device, dtype = torch.long)\n        mask = data['mask'].to(device, dtype = torch.long)\n        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n        targets = data['target'].to(device, dtype = torch.float)\n        \n        batch_size = ids.size(0)\n        \n        outputs = model(ids, mask, token_type_ids)\n        loss = eval_metrics(outputs, targets)\n        \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        \n        epoch_loss = running_loss\/dataset_size\n        \n        bar.set_postfix(Epoch=epoch, Valid_Loss=epoch_loss,\n                        LR=optimizer.param_groups[0]['lr'])   \n    gc.collect()\n    \n    return epoch_loss","02efc123":"@logger.catch\ndef run(model, optimizer, scheduler, device, num_epochs):    \n    start = time.time()\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_loss = np.inf\n    history = defaultdict(list)\n    \n    for epoch in range(1, num_epochs + 1): \n        gc.collect()\n        train_epoch_loss = training(model, optimizer, scheduler, dataloader=train_loader, \n                                           device=CFG.device, epoch=epoch)\n        \n        valid_epoch_loss = inferance(model, optimizer, scheduler, dataloader=valid_loader, \n                                           device=CFG.device, epoch=epoch)\n    \n        history['Train Loss'].append(train_epoch_loss)\n        history['Valid Loss'].append(valid_epoch_loss)\n        \n        # deep copy the model\n        if valid_epoch_loss <= best_loss:\n            print(f\"Validation Loss Improved ({best_loss} ---> {valid_epoch_loss})\")\n            best_loss = valid_epoch_loss\n            best_model_wts = copy.deepcopy(model.state_dict())\n            PATH = \"Loss{:.4f}_epoch{:.0f}.bin\".format(best_loss, epoch)\n            torch.save(model.state_dict(), PATH)\n            print(\"Model Saved\")\n    \n    print()\n    \n    end = time.time()\n    time_elapsed = end - start\n    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n        time_elapsed \/\/ 3600, (time_elapsed % 3600) \/\/ 60, (time_elapsed % 3600) % 60))\n    print(\"Best Loss: {:.4f}\".format(best_loss))\n    \n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    \n    return model, history","8b7d1138":"def prepare_data(df,fold):\n    \n    df_train = df[df.Fold == fold].reset_index(drop=True)\n    df_valid = df[df.Fold == fold].reset_index(drop=True)\n    \n    train_dataset = BERTDset(df_train, CFG.tokenizer, CFG.max_len)\n    valid_dataset = BERTDset(df_valid, CFG.tokenizer, CFG.max_len)\n\n    train_loader = DataLoader(train_dataset, batch_size=CFG.train_batch_size, \n                              num_workers=4, shuffle=True, pin_memory=True)\n    valid_loader = DataLoader(valid_dataset, batch_size=CFG.valid_batch_size, \n                              num_workers=4, shuffle=False, pin_memory=True)\n    \n    return train_loader, valid_loader","bfeb6e6f":"train_loader, valid_loader = prepare_data(df_train,fold=1)","2cc1e991":"for x in valid_loader:\n    print(x)\n    break","d66a12eb":"# Defining Optimizer with weight decay to params other than bias and layer norms\nparam_optimizer = list(model.named_parameters())\nno_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\noptimizer_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \n     'weight_decay': 0.0001},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \n     'weight_decay': 0.0}\n    ]  \n\noptimizer = AdamW(optimizer_parameters, lr=CFG.learning_rate)\n\n#Defining LR Scheduler\nscheduler = get_linear_schedule_with_warmup(\n    optimizer, \n    num_warmup_steps=0, \n    num_training_steps=len(train_loader)*CFG.epochs\n)","03b6942d":"lrs = []\nfor epoch in range(1, CFG.epochs + 1):\n    scheduler.step()\n    lrs.append(optimizer.param_groups[0][\"lr\"])\nplt.plot(lrs);","d05538b9":"model, history = run(model, optimizer, scheduler, device=CFG.device, num_epochs=CFG.epochs)","6cd9334e":"fig = plt.figure(figsize=(22,8))\nplt.plot(history['Train Loss'], label='Train Loss')\nplt.plot(history['Valid Loss'], label='Valid Loss')\nplt.legend()\nplt.title('Loss Curve');","76ea6866":"## Ensuring the distribution of Folds\n\nWe can compare any two folds with the **Kolmogorov-Smirnov** test to examine if the folds come from the same distribution. Let's compare all folds with the 1st fold for simplicity. The test results are given below. Indeed, the low KS (~0.0008) and high probability (1.0) values confirm that all folds come from the same distribution.","6b43c422":"### Observation :- \n#### Target distribution accross the folds looks normal and evenly distributed","5a1b4b8c":"<br>\n<h1 style = \"font-size:60px; font-family:Garamond ; font-weight : normal; background-color: #f6f5f7 ; color : #fe346e; text-align: center; border-radius: 100px 85;\">CommonLit Readability<br> BERT Model<\/h1>\n<br>","df66e525":"<br>\n<h1 style = \"font-size:60px; font-family:Garamond ; font-weight : normal; background-color: #f6f5f7 ; color : #fe346e; text-align: center; border-radius: 100px 85;\">CommonLit Readability<br> BERT + Roberta + Model Explainabilty<\/h1>\n<br>","ed47ecf9":"# Required Modeling Libraries","b2e92812":"# 3. Data Wrangling or Preprocessing","f32af7e8":"### ... In progress ","c38ebd38":"## Let See How the Taget is distribured accross folds","b5dd4205":"# 2. Data Loading","cda623f1":"### Observation :- \n#### We Can Statisticaly say that target distribution even where as PValue approaching 1 and Statistic is less than zero","23722eed":"## To know more details about nlpretext library \n\n[Link](https:\/\/medium.com\/artefact-engineering-and-data-science\/introducing-nlpretext-a8bb7c03df89)","15a4a547":"# NLP Preprocessing Libraries","72d2bca2":"# Sampling Strategy - KFOLD","6a6b93e4":"<br>\n<h1 style = \"font-size:60px; font-family:Garamond ; font-weight : normal; background-color: #f6f5f7 ; color : #fe346e; text-align: center; border-radius: 100px 85;\">BERT Model - Training & Validation <\/h1>\n<br>","ca4a3978":"## Loss Function","81b7ddba":"## Preparing Torch dataset for BERT ","aa5bd0bb":"# Required Data Loading and Util libraries ","77671687":"### Observation :- \n    For BERT , Its required to share the maximum words in the text "}}