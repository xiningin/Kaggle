{"cell_type":{"f21a4258":"code","662e75ce":"code","1f0eefa6":"code","499a5af0":"code","febe3d35":"code","b36a2180":"code","d5d365cd":"code","5fa41b69":"code","5738b681":"code","2bef5c44":"code","23f5656a":"code","ae2ec98d":"code","ef728dea":"code","bcdc6789":"code","ce37c18d":"code","f308aa3b":"code","56adb407":"code","94496805":"code","12e6ea94":"code","600ad9b8":"code","e787c45c":"code","48015636":"code","043971de":"code","9b5e6c2e":"code","3f300b06":"code","ed869e06":"code","213cfd39":"code","3072b806":"code","bcd6e667":"code","f7cb8cae":"code","77a0a26a":"code","c0258e0f":"code","8dac27c7":"code","2827bc40":"code","c77d1e2d":"code","85552f96":"code","35d6194d":"code","b4f48817":"code","6064d659":"code","d56304de":"code","ddc7695a":"code","68e15aef":"code","746b5da3":"code","e8203964":"code","bcf86385":"code","46f5fce9":"code","fc415010":"code","7d495567":"code","fc3036ff":"code","1bcb3b43":"code","8707f045":"code","8256ea44":"code","4698c2b7":"code","ba504b11":"code","dae4ba41":"code","cd977cd5":"code","df10212a":"code","13ce4655":"code","ea1ae777":"code","a8403745":"code","e59a03b4":"code","09dee116":"code","3aa62159":"code","99c85b82":"code","7c8dde2f":"code","ba1398bf":"code","30a35efc":"code","1093b46e":"code","a99c7b4b":"code","4ea48aa0":"code","f44f5c57":"code","468f6bfe":"code","927fc199":"code","2b651d5a":"code","24f5e3a5":"code","75616a63":"code","f5325949":"code","af0685f3":"code","8e446cde":"code","5b048146":"code","c9b5e8b3":"code","ec7239a8":"markdown","297fb723":"markdown","e0c5cf8a":"markdown","51c6b2ad":"markdown","f578c294":"markdown","31df4e52":"markdown","0858dc0c":"markdown","d4d92a9a":"markdown","e9ca1b23":"markdown","9f9007a7":"markdown","26e64b27":"markdown","657fa29c":"markdown","e90bdd75":"markdown","3d535c14":"markdown","3775532b":"markdown","49670d87":"markdown","1c5c21bb":"markdown","0339c781":"markdown","1677162d":"markdown"},"source":{"f21a4258":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas_profiling\nimport warnings","662e75ce":"df_train=pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","1f0eefa6":"pd.options.display.max_columns = None","499a5af0":"df_train","febe3d35":"df_test","b36a2180":"df_train.columns","d5d365cd":"df_test.columns","5fa41b69":"#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ndf_train.drop(\"Id\", axis = 1, inplace = True)\n","5738b681":"#Correlation map to see how features are correlated with SalePrice\ncorrmat = df_train.corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=0.9, square=True)","2bef5c44":"print(\"\\nThe train data size after dropping Id feature is : {} \".format(df_train.shape)) \nprint(\"The test data size after dropping Id feature is : {} \".format(df_test.shape))\n","23f5656a":" #most correlated features\ncorrmat = df_train.corr()\ntop_corr_features = corrmat.index[abs(corrmat[\"SalePrice\"])>0.5]\nplt.figure(figsize=(10,10))\ng = sns.heatmap(df_train[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","ae2ec98d":"sns.barplot(df_train.OverallQual,df_train.SalePrice)","ef728dea":"sns.barplot(df_train.GrLivArea,df_train.SalePrice)","bcdc6789":"sns.set()\ncols = [ 'SalePrice','OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\n\n    #plt.show()\nsns.pairplot(df_train[cols], size = 2.5)\nplt.show();","ce37c18d":"from scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\nsns.distplot(df_train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(df_train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\nprint(\"skewness of salesprice is :\", df_train[\"SalePrice\"].skew())\nfig = plt.figure()\nres = stats.probplot(df_train['SalePrice'], plot=plt)\nplt.show()","f308aa3b":"df_train.SalePrice = np.log1p(df_train.SalePrice )\ny = df_train.SalePrice\ny","56adb407":"print(\"skewness of salesprice is :\", df_train[\"SalePrice\"].skew())\n#train['SalePrice'] = np.log(train['SalePrice']+1)\nsns.distplot(df_train['SalePrice'],fit=norm)\n(mu, sig) = norm.fit(df_train['SalePrice'])\nplt.ylabel('Frequency')\nplt.legend([\"Skewness: {:.2f}\".format(df_train['SalePrice'].skew())])\nplt.show()\nfig = plt.figure()\nres = stats.probplot(df_train['SalePrice'], plot=plt)\nplt.show()","94496805":"plt.scatter(y =df_train.SalePrice,x = df_train.GrLivArea,c = 'black')\nplt.show()\n#we can see the outlier in the below image","12e6ea94":"train_nas = df_train.isnull().sum()\ntrain_nas = train_nas[train_nas>0]\ntrain_nas.sort_values(ascending=False)","600ad9b8":"test_nas = df_test.isnull().sum()\ntest_nas = test_nas[test_nas>0]\ntest_nas.sort_values(ascending = False)","e787c45c":"print(\"Find most important features relative to target\")\ncorr = df_train.corr()\ncorr.sort_values([\"SalePrice\"], ascending = False, inplace = True)\nprint(corr.SalePrice)\n#this you can see at the time of heatmap also","48015636":"# Differentiate numerical features (minus the target) and categorical features\ncategorical_features = df_train.select_dtypes(include=['object']).columns\ncategorical_features","043971de":"#numerical data\nnumerical_features = df_train.select_dtypes(exclude = [\"object\"]).columns\nnumerical_features","9b5e6c2e":"#numerical_features = numerical_features.drop(\"SalePrice\")\nprint(\"Numerical features : \" + str(len(numerical_features)))\nprint(\"Categorical features : \" + str(len(categorical_features)))\n#FOR NUMERIC\ntrain_num = df_train[numerical_features]\n#FOR CATEGORICAL\ntrain_cat = df_train[categorical_features]","3f300b06":"sns.set_style(\"whitegrid\")\nmissing = df_train.isnull().sum()\nmissing = missing[missing > 0]\nmissing.sort_values(inplace=True)\nmissing.plot.bar()","ed869e06":"all_data_na = (df_train.isnull().sum() \/ len(df_train)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head(20)\n","213cfd39":"# Handle remaining missing values for numerical features by using median as replacement\nprint(\"NAs for numerical features in train : \" + str(train_num.isnull().values.sum()))\ntrain_num = train_num.fillna(train_num.median())\nprint(\"Remaining NAs for numerical features in train : \" + str(train_num.isnull().values.sum()))","3072b806":"# Handle remaining missing values for catergorical features by using median as replacement\nprint(\"NAs for catergorical features in train : \" + str(train_cat.isnull().values.sum()))\ntrain_num = train_num.fillna(train_num.median())\nprint(\"Remaining NAs for catergorical features in train : \" + str(train_cat.isnull().values.sum()))","bcd6e667":"#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\ndf_train[\"LotFrontage\"] = df_train.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\n\n#GarageType, GarageFinish, GarageQual and GarageCond : Replacing missing data with None\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    df_train[col] = df_train[col].fillna('None')\n    \n#GarageYrBlt, GarageArea and GarageCars : Replacing missing data with 0 (Since No garage = no cars in such garage.)\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    df_train[col] = df_train[col].fillna(0)\n    \n#BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath :\n#missing values are likely zero for having no basement\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    df_train[col] = df_train[col].fillna(0)\n    \n#BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2 : \n#For all these categorical basement-related features, NaN means that there is no basement.\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    df_train[col] = df_train[col].fillna('None')\n    \n#MasVnrArea and MasVnrType : NA most likely means no masonry veneer for these houses.\n#We can fill 0 for the area and None for the type.\ndf_train[\"MasVnrType\"] = df_train[\"MasVnrType\"].fillna(\"None\")\ndf_train[\"MasVnrArea\"] = df_train[\"MasVnrArea\"].fillna(0)\n\n#MSZoning (The general zoning classification) : 'RL' is by far the most common value. \n#So we can fill in missing values with 'RL'\ndf_train['MSZoning'] = df_train['MSZoning'].fillna(df_train['MSZoning'].mode()[0])\n\n#Utilities : For this categorical feature all records are \"AllPub\", except for one \"NoSeWa\" and 2 NA . \n#Since the house with 'NoSewa' is in the training set, this feature won't help in predictive modelling. \n#We can then safely remove it.\ndf_train = df_train.drop(['Utilities'], axis=1)\n\n#Functional : data description says NA means typical\ndf_train[\"Functional\"] = df_train[\"Functional\"].fillna(\"Typ\")\n\n#Electrical : It has one NA value. Since this feature has mostly 'SBrkr', we can set that for the missing value.\ndf_train['Electrical'] = df_train['Electrical'].fillna(df_train['Electrical'].mode()[0])\n\n#KitchenQual: Only one NA value, and same as Electrical,\n#we set 'TA' (which is the most frequent) for the missing value in KitchenQual.\ndf_train['KitchenQual'] = df_train['KitchenQual'].fillna(df_train['KitchenQual'].mode()[0])\n\n#Exterior1st and Exterior2nd : Again Both Exterior 1 & 2 have only one missing value.\n#We will just substitute in the most common string\ndf_train['Exterior1st'] = df_train['Exterior1st'].fillna(df_train['Exterior1st'].mode()[0])\ndf_train['Exterior2nd'] = df_train['Exterior2nd'].fillna(df_train['Exterior2nd'].mode()[0])\n\n#SaleType : Fill in again with most frequent which is \"WD\"\ndf_train['SaleType'] = df_train['SaleType'].fillna(df_train['SaleType'].mode()[0])\n\n#MSSubClass : Na most likely means No building class. We can replace missing values with None\ndf_train['MSSubClass'] = df_train['MSSubClass'].fillna(\"None\")\n\ndf_train[\"PoolQC\"] = df_train[\"PoolQC\"].fillna(\"None\")\n#MiscFeature : data description says NA means \"no misc feature\"\n\ndf_train[\"MiscFeature\"] = df_train[\"MiscFeature\"].fillna(\"None\")\n#Alley : data description says NA means \"no alley access\"\ndf_train[\"Alley\"] = df_train[\"Alley\"].fillna(\"None\")\n#Fence : data description says NA means \"no fence\"\n\ndf_train[\"Fence\"] = df_train[\"Fence\"].fillna(\"None\")\n#FireplaceQu : data description says NA means \"no fireplace\"\ndf_train[\"FireplaceQu\"] = df_train[\"FireplaceQu\"].fillna(\"None\")\n","f7cb8cae":"# Some numerical features are actually really categories\ndf_train = df_train.replace({\"MSSubClass\" : {20 : \"SC20\", 30 : \"SC30\", 40 : \"SC40\", 45 : \"SC45\", \n                                       50 : \"SC50\", 60 : \"SC60\", 70 : \"SC70\", 75 : \"SC75\", \n                                       80 : \"SC80\", 85 : \"SC85\", 90 : \"SC90\", 120 : \"SC120\", \n                                       150 : \"SC150\", 160 : \"SC160\", 180 : \"SC180\", 190 : \"SC190\"},\n                       \"MoSold\" : {1 : \"Jan\", 2 : \"Feb\", 3 : \"Mar\", 4 : \"Apr\", 5 : \"May\", 6 : \"Jun\",\n                                   7 : \"Jul\", 8 : \"Aug\", 9 : \"Sep\", 10 : \"Oct\", 11 : \"Nov\", 12 : \"Dec\"}\n                      })","77a0a26a":" #Encode some categorical features as ordered numbers when there is information in the order\ndf_train = df_train.replace({\"Alley\" : {\"Grvl\" : 1, \"Pave\" : 2},\n                       \"BsmtCond\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"BsmtExposure\" : {\"No\" : 0, \"Mn\" : 1, \"Av\": 2, \"Gd\" : 3},\n                       \"BsmtFinType1\" : {\"No\" : 0, \"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3, \"BLQ\" : 4, \n                                         \"ALQ\" : 5, \"GLQ\" : 6},\n                       \"BsmtFinType2\" : {\"No\" : 0, \"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3, \"BLQ\" : 4, \n                                         \"ALQ\" : 5, \"GLQ\" : 6},\n                       \"BsmtQual\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"ExterCond\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5},\n                       \"ExterQual\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5},\n                       \"FireplaceQu\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"Functional\" : {\"Sal\" : 1, \"Sev\" : 2, \"Maj2\" : 3, \"Maj1\" : 4, \"Mod\": 5, \n                                       \"Min2\" : 6, \"Min1\" : 7, \"Typ\" : 8},\n                       \"GarageCond\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"GarageQual\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"HeatingQC\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"KitchenQual\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"LandSlope\" : {\"Sev\" : 1, \"Mod\" : 2, \"Gtl\" : 3},\n                       \"LotShape\" : {\"IR3\" : 1, \"IR2\" : 2, \"IR1\" : 3, \"Reg\" : 4},\n                       \"PavedDrive\" : {\"N\" : 0, \"P\" : 1, \"Y\" : 2},\n                       \"PoolQC\" : {\"No\" : 0, \"Fa\" : 1, \"TA\" : 2, \"Gd\" : 3, \"Ex\" : 4},\n                       \"Street\" : {\"Grvl\" : 1, \"Pave\" : 2},\n                       \"Utilities\" : {\"ELO\" : 1, \"NoSeWa\" : 2, \"NoSewr\" : 3, \"AllPub\" : 4}}\n                     )","c0258e0f":"# Differentiate numerical features (minus the target) and categorical features\ncategorical_features = df_train.select_dtypes(include = [\"object\"]).columns\nnumerical_features = df_train.select_dtypes(exclude = [\"object\"]).columns\nnumerical_features = numerical_features.drop(\"SalePrice\")\nprint(\"Numerical features : \" + str(len(numerical_features)))\nprint(\"Categorical features : \" + str(len(categorical_features)))\ntrain_num = df_train[numerical_features]\ntrain_cat = df_train[categorical_features]","8dac27c7":"df_test.drop(['Alley','MiscFeature','Fence','PoolQC'], axis=1, inplace=True)","2827bc40":"df_test[\"LotFrontage\"].fillna(df_test['LotFrontage'].mean(), inplace=True)\ndf_test[\"MasVnrType\"]=df_test[\"MasVnrType\"]=df_test[\"MasVnrType\"].replace(np.nan, \"unknown\")\ndf_test[\"Utilities\"]=df_test[\"Utilities\"].replace(np.nan, \"unknown\")\ndf_test[\"Exterior1st\"]=df_test[\"Exterior1st\"].replace(np.nan, \"unknown\")\ndf_test[\"Exterior2nd\"]=df_test[\"Exterior2nd\"].replace(np.nan, 'unknown')\ndf_test[\"MasVnrArea\"].fillna(df_test['MasVnrArea'].mean(), inplace=True) \ndf_test['BsmtQual']=df_test[\"BsmtQual\"].replace(np.nan, 'unknown')\ndf_test[\"BsmtFullBath\"].fillna(df_test[\"BsmtFullBath\"].mean(), inplace=True)\ndf_test[\"KitchenQual\"]=df_test[\"KitchenQual\"].replace(np.nan, 'unknown')\ndf_test[\"Functional\"]=df_test[\"Functional\"].replace(np.nan, 'unknown')\ndf_test[\"FireplaceQu\"]= df_test[\"FireplaceQu\"].replace(np.nan, 'unknown')\ndf_test[\"GarageType\"]=df_test['GarageType'].replace(np.nan, 'unknown')\ndf_test[\"GarageYrBlt\"].fillna(df_test['GarageYrBlt'].mean(), inplace=True)\ndf_test[\"GarageFinish\"]=df_test['GarageFinish'].replace(np.nan, 'unknown')\ndf_test[\"GarageCars\"].fillna(df_test['GarageCars'].mean(), inplace=True)\ndf_test[\"GarageArea\"].fillna(df_test['GarageArea'].mean(), inplace=True)\ndf_test[\"GarageQual\"]=df_test[\"GarageQual\"].replace(np.nan, 'unknown')\ndf_test[\"GarageCond\"]=df_test[\"GarageCond\"].replace(np.nan, 'unknown')\ndf_test[\"SaleType\"]=df_test[\"SaleType\"].replace(np.nan, 'unknown')\ndf_test[\"BsmtFinType1\"]=df_test[\"BsmtFinType1\"].replace(np.nan, \"unknown\")\ndf_test[\"BsmtFinType2\"]=df_test[\"BsmtFinType2\"].replace(np.nan, \"unknown\")\ndf_test[\"BsmtExposure\"]=df_test[\"BsmtExposure\"].replace(np.nan, \"unknown\")\ndf_test[\"MSZoning\"]=df_test[\"MSZoning\"].replace(np.nan, \"unknown\")\ndf_test[\"BsmtFinSF1\"].fillna(df_test['BsmtFinSF1'].mean(), inplace=True)\ndf_test[\"BsmtFinSF2\"].fillna(df_test['BsmtFinSF2'].mean(), inplace=True)\ndf_test[\"BsmtUnfSF\"].fillna(df_test['BsmtUnfSF'].mean(), inplace=True)\ndf_test[\"TotalBsmtSF\"].fillna(df_test['TotalBsmtSF'].mean(), inplace=True)\ndf_test[\"BsmtHalfBath\"].fillna(df_test['BsmtHalfBath'].mean(), inplace=True)\ndf_test[\"BsmtCond\"]=df_test[\"BsmtCond\"].replace(np.nan, \"unknown\")","c77d1e2d":"df_test.columns[df_test.isnull().any()]","85552f96":"sns.heatmap(df_test.isnull(), cbar=False)","35d6194d":"#for train set\nall_data_na = (df_train.isnull().sum() \/ len(df_train)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head(20)","b4f48817":"#for test set\nall_data_na = (df_test.isnull().sum() \/ len(df_test)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head(20)","6064d659":"df_train.isnull().sum().sort_values(ascending = False)","d56304de":" #Create dummy features for categorical values via one-hot encoding\ntrain_cat.shape","ddc7695a":"train_cat.head()\n#HERE IS THE VALUES CATEGORICAL","68e15aef":"train_num.head()\n#HERE IS THE VALUES NUMERIC","746b5da3":"str(train_cat.isnull().values.sum())","e8203964":"from sklearn.preprocessing import StandardScaler \nfrom sklearn.model_selection import train_test_split \nfrom sklearn.model_selection import KFold \nfrom sklearn.model_selection import cross_val_score \nfrom sklearn.model_selection import GridSearchCV \nfrom sklearn.linear_model import LinearRegression \nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet \nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR \nfrom sklearn.pipeline import Pipeline \nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor \nfrom sklearn.ensemble import ExtraTreesRegressor  \nfrom sklearn.metrics import mean_squared_error","bcf86385":"\ndf_train = pd.concat([train_cat,train_num],axis=1)\ndf_train.shape","46f5fce9":"df_train.tail()","fc415010":" ## Combining train and test datasets together so that we can do all the work at once. \nall_data = pd.concat((df_train, df_test)).reset_index(drop = True)\n#all_data.drop(['SalePrice'], axis = 1, inplace = True)\n#y = df_train['SalePrice'].reset_index(drop=True)","7d495567":"all_data= all_data.drop(['Street', 'PoolQC',], axis=1)\nall_data.shape","fc3036ff":"## Creating dummy variable \nfinal_features = pd.get_dummies(all_data).reset_index(drop=True)\nfinal_features.shape","1bcb3b43":"#filling NA's with the mean of the column:\nfinal_features = final_features.fillna(final_features.mean())\n","8707f045":"X = final_features.iloc[:len(y), :]\n\nX_sub = final_features.iloc[len(y):]","8256ea44":"outliers = [30, 88, 462, 631, 1322]\nX = X.drop(X.index[outliers])\ny = y.drop(y.index[outliers])","4698c2b7":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX=sc.fit_transform(X)\n","ba504b11":"from sklearn.model_selection import train_test_split\n## Train test split follows this distinguished code pattern and helps creating train and test set to build machine learning. \nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size = 0.3, random_state = 0)","dae4ba41":"X_train.shape, y_train.shape, X_test.shape, y_test.shape","cd977cd5":"y_train.values.reshape((-1,1))\ny_test.values.reshape((-1,1))\ny_train=y_train.transpose()\ny_test=y_test.transpose()","df10212a":"y_train.shape, y_test.shape","13ce4655":"## importing necessary models.\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\n## Call in the LinearRegression object\nlin_reg = LinearRegression(normalize=True, n_jobs=-1)\n## fit train and test data. \nlin_reg.fit(X_train, y_train)\n## Predict test data. \ny_pred = lin_reg.predict(X_test)","ea1ae777":"y_pred","a8403745":"# get average squared error(MSE) by comparing predicted values with real values. \nprint ('%.2f'%mean_squared_error(y_test, y_pred))","e59a03b4":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, KFold\nlin_reg = LinearRegression()\ncv = KFold(shuffle=True, random_state=2, n_splits=8)\nscores = cross_val_score(lin_reg, X,y,cv = cv, scoring = 'neg_mean_absolute_error')","09dee116":"print ('%.8f'%scores.mean())","3aa62159":"num_folds = 10 \nseed = 8\nscoring = 'neg_mean_squared_error'\nt = []\nt.append(('LR', LinearRegression()))\nt.append(('LASSO', Lasso()))\nt.append(('EN', ElasticNet()))\nt.append(('KNN', KNeighborsRegressor()))\nt.append(('CART', DecisionTreeRegressor())) \nt.append(('SVR', SVR()))\n\n\n\nresults = []\nnames = []\nfor name, model in t: \n    kfold = KFold(n_splits=num_folds, random_state=seed) \n    cv_results = cross_val_score(model, X_train,y_train, cv=kfold, scoring=scoring) \n    results.append(cv_results)\n    names.append(name) \n    print(name, cv_results.mean(), cv_results.std()) ","99c85b82":"import xgboost as xgb\n","7c8dde2f":"dtrain = xgb.DMatrix(X_train, label = y)\ndtest = xgb.DMatrix(X_test)\n\nparams = {\"max_depth\":2, \"eta\":0.1}\nmodel = xgb.cv(params, dtrain,  num_boost_round=500, early_stopping_rounds=100)","ba1398bf":"model.loc[30:,[\"test-rmse-mean\", \"train-rmse-mean\"]].plot()","30a35efc":"#from sklearn.ensemble import Lasso\nfrom sklearn.linear_model import Lasso\nlasso = Lasso(alpha=0.1, normalize=True) \nlasso.fit(X_train, y_train) \nlasso_pred = lasso.predict(X_test) ","1093b46e":"print(mean_squared_error(y_test,lasso_pred ))","a99c7b4b":" lasso.score(X_test, y_test) ","4ea48aa0":"from sklearn.linear_model import Ridge\nridge = Ridge(alpha=0.1, normalize=True) \nridge.fit(X_train, y_train) \nridge_pred = ridge.predict(X_test)\nprint(mean_squared_error(y_test,ridge_pred ))\nridge.score(X_test, y_test) \n","f44f5c57":"from sklearn.ensemble import RandomForestRegressor\nmy_model = RandomForestRegressor()\nmy_model.fit(X_train, y_train)","468f6bfe":"random_predicted_prices = my_model.predict(X_test)\nprint(mean_squared_error(y_test,random_predicted_prices ))","927fc199":"print(random_predicted_prices)","2b651d5a":"from sklearn.ensemble import GradientBoostingRegressor\nscaler = StandardScaler().fit(X_train) \nrescaledX = scaler.transform(X_train)\nmodel = GradientBoostingRegressor(random_state=seed, n_estimators=400)\nmodel.fit(rescaledX, y_train)","24f5e3a5":"rescaledValidationX = scaler.transform(X_test)\ngradient_predictions = model.predict(X_test) \nprint(mean_squared_error(y_test, gradient_predictions))","75616a63":"print(gradient_predictions)","f5325949":"prediction_list=[lasso_pred,random_predicted_prices,gradient_predictions,ridge_pred]\nfor i in prediction_list:\n    submit_test = pd.concat([df_test['Id'],pd.DataFrame(i)], axis=1)\n    submit_test.columns=['Id', 'SalePrice']","af0685f3":"submit_test","8e446cde":"submit_test1.to_csv('submission.csv', index=False )","5b048146":"def create_download_link(title = \"Download CSV file\", filename = \"submission.csv\"):  \n    html = '<a href={filename}>{title}<\/a>'\n    html = html.format(title=title,filename=filename)\n    return HTML(html)","c9b5e8b3":"from IPython.display import HTML\ncreate_download_link(filename='submission.csv')","ec7239a8":"NO MISSING RATIO","297fb723":"1. HERE SalePrice is mostly related to \n   OverallQual      \n   GrLivArea        \n   GarageCars       \n   GarageArea       \n   TotalBsmtSF      \n   1stFlrSF         \n   FullBath        \n   YearBuilt       \n   YearRemodAdd     ","e0c5cf8a":"* LotFrontage : Since the area of each street connected to the house property most likely have a similar area to other houses in its neighborhood , we can fill in missing values by the median LotFrontage of the neighborhood.\n* GarageType, GarageFinish, GarageQual and GarageCond : Replacing missing data with None\n* GarageYrBlt, GarageArea and GarageCars : Replacing missing data with 0 (Since No garage = no cars in such garage.)\n* BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath : missing values are likely zero for having no basement\n* BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2 : For all these categorical basement-related features, NaN means that there is no basement.\n* MasVnrArea and MasVnrType : NA most likely means no masonry veneer for these houses. We can fill 0 for the area and None for the type.\n* MSZoning (The general zoning classification) : 'RL' is by far the most common value. So we can fill in missing values with 'RL'\n* Utilities : For this categorical feature all records are \"AllPub\", except for one \"NoSeWa\" and 2 NA . Since the house with 'NoSewa' is in the training set, this feature won't help in predictive modelling. We can then safely remove it.\n* Functional : data description says NA means typical\n* Electrical : It has one NA value. Since this feature has mostly 'SBrkr', we can set that for the missing value.\n* KitchenQual: Only one NA value, and same as Electrical, we set 'TA' (which is the most frequent) for the missing value in KitchenQual.\n* Exterior1st and Exterior2nd : Again Both Exterior 1 & 2 have only one missing value. We will just substitute in the most common string\n* SaleType : Fill in again with most frequent which is \"WD\"\n* MSSubClass : Na most likely means No building class. We can replace missing values with None\n","51c6b2ad":"NO MISSING RATIO","f578c294":"# Exploratory Data Analysis(EDA)","31df4e52":"# L1 Regularisation (Lasso)\u000b(Least Absolute Shrinkage and Selection Operator)\n","0858dc0c":"* Quantitative: 1stFlrSF, 2ndFlrSF, 3SsnPorch, BedroomAbvGr, BsmtFinSF1, BsmtFinSF2, BsmtFullBath, BsmtHalfBath, BsmtUnfSF, EnclosedPorch, Fireplaces, FullBath, GarageArea, GarageCars, GarageYrBlt, GrLivArea, HalfBath, KitchenAbvGr, LotArea, LotFrontage, LowQualFinSF, MSSubClass, MasVnrArea, MiscVal, MoSold, OpenPorchSF, OverallCond, OverallQual, PoolArea, ScreenPorch, TotRmsAbvGrd, TotalBsmtSF, WoodDeckSF, YearBuilt, YearRemodAdd, YrSold\n\n* Qualitative: Alley, BldgType, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, BsmtQual, CentralAir, Condition1, Condition2, Electrical, ExterCond, ExterQual, Exterior1st, Exterior2nd, Fence, FireplaceQu, Foundation, Functional, GarageCond, GarageFinish, GarageQual, GarageType, Heating, HeatingQC, HouseStyle, KitchenQual, LandContour, LandSlope, LotConfig, LotShape, MSZoning, MasVnrType, MiscFeature, Neighborhood, PavedDrive, PoolQC, RoofMatl, RoofStyle, SaleCondition, SaleType, Street, Utilities,","d4d92a9a":"* Ridge regression is an L2 penalized model where we simply add the squared sum of the weights to our least-squares cost function:\n\n\n\n* By increasing the value of the hyperparameter \u03bb , we increase the regularization strength and shrink the weights of our model.\n","e9ca1b23":"# L2 Regularisation (Ridge)","9f9007a7":"# TIME FOR MODELING\n* Before modeling each algorithm, I would like to discuss them for a better understanding. This way I would review what I know and at the same time help out the community. \n* Introducing**Linear Regression**, one of the most basic and straightforward models. Many of us may have learned to show the relationship between two variable using something called \"y equals mX plus b.\" Let's refresh our memory and call upon on that equation.\n# >                   **y=mX+b**\n* m = slope of the regression line. It represents the relationship between X and y. In another word, it gives weight as to for each x(horizontal space) how much y(vertical space) we have to cover. In machine learning, we call it coefficient.\n* b = y-intercept.\n* x and y are the data points located in x_axis and y_axis respectively.","26e64b27":"here we can see how OverallQual is corelated","657fa29c":"* Having a large number of samples (n) with respect to the number of dimensionality (d) increases the quality of our model.\n* One way to reduce the e\ufb00ective number of dimensions is to use those that most contribute to the signal and ignore those that mostly act as noise.\n* L1 regularization achieves this by adding a penalty that results in the weight for the dimensions that act as noise becoming 0. \n* L1 regularisation encourages a sparse vector of weights in which few are non-zero and many are zero. \n","e90bdd75":"most of the features are correlated with each other like Garage Cars and Garage Area\n\nOverallQual is highly correlated with target feature SalePrice 0.79 can you see. ","3d535c14":"# Practice Skills\n* Creative feature engineering \n* Advanced regression techniques like random forest and gradient boosting","3775532b":"There are 1460 instances of training data and 1460 of test data. Total number of attributes equals 81, of which 36 is quantitative, 43 categorical + Id and SalePrice.","49670d87":"*MSE(Mean Squared Error) *\n# >** MSE=1n\u2211i=1n(yi^\u2212yi)2**\n*MAE (Mean Absolute Error) *\n# >  MAE=\u2211ni=1|y\u00af\u2212yi|n","1c5c21bb":"# Train_test split\nWe have separated dependent and independent features; We have separated train and test data. So, why do we still have to split our training data? If you are curious about that, I have the answer. For this competition, when we train the machine learning algorithms, we use part of the training set, usually two-thirds of the train data. Once we train our algorithm using 2\/3 of the train data, we start to test our algorithms using the remaining data. If the model performs well, we dump our test data in the algorithms to predict and submit the competition. The code below, basically splits the train data into 4 parts,** X_train, X_test, y_train, y_test.**\n\n * X_train, y_train first used to train the algorithm.\n * then, X_test is used in that trained algorithms to predict outcomes.\n * Once we get the outcomes, we compare it with y_test\n \n * By comparing the outcome of the model with test_y, we can determine whether our algorithms are performing well or not.","0339c781":"# Regularization Models\nWhat makes regression model more effective is its ability of regularizing. The term \"regularizing\" stands for models ability to structurally prevent overfitting by imposing a penalty on the coefficients.\n\nThere are three types of regularizations.\n\n* Ridge\n* Lasso\n* Elastic Net\nThese regularization methods work by penalizing the magnitude of the coefficients of features and at the same time minimizing the error between the predicted value and actual observed values. This minimization becomes a balance between the error (the difference between the predicted value and observed value) and the size of the coefficients. The only difference between Ridge and Lasso is the way they penalize the coefficients. Elastic Net is the combination of these two. Elastic Net adds both the sum of the squares errors and the absolute value of the squared error. To get more in-depth of it, let us review the least squared loss function.\n\nOrdinary least squared loss function minimizes the residual sum of the square(RSS) to fit the data:\n\n# minimize:RSS=\u2211i=1n(yi\u2212y^i)2=\u2211i=1n(yi\u2212(\u03b20+\u2211j=1p\u03b2jxj))2\n \nLet's review this equation once again, Here:\n\nyi  is the observed value.\ny^i  is the predicted value.\nThe error =  yi  -  y^i \nThe square of the error = ** (yi\u2212y^i)2 **\nThe sum of the square of the error =  \u2211ni=1(yi\u2212y^i)2 , that's the equation on the left.\nThe only difference between left sides equation vs. the right sides one above is the replacement of  y^i , it is replaced by  (\u03b20+\u2211pj=1\u03b2jxj) , which simply follow's the slope equation, y = mx+b, where,\n\u03b20  is the intercept.\n\u03b2j  is the coefficient of the feature( xj ).\nLet's describe the effect of regularization and then we will learn how we can use loss function in Ridge.\n\nOne of the benefits of regularization is that it deals with multicollinearity(high correlation between predictor variables) well, especially Ridge method. Lasso deals with multicollinearity more brutally by penalizing related coefficients and force them to become zero, hence removing them. However, Lasso is well suited for redundant variables.\nRidge:\nRidge regression adds penalty equivalent to the square of the magnitude of the coefficients. This penalty is added to the least square loss function above and looks like this...\n\n# minimize:RSS+Ridge=\u2211i=1n(yi\u2212(\u03b20+\u2211j=1p\u03b2jxj))2+\u03bb2\u2211j=1p\u03b22j\nHere,\n\n\u03bb2  is constant; a regularization parameter. It is also known as  \u03b1 . The higher the value of this constant the more the impact in the loss function.\nWhen  \u03bb2  is 0, the loss funciton becomes same as simple linear regression.\nWhen  \u03bb2  is  \u221e , the coefficients become 0\nWhen  \u03bb2  is between 0 and  \u221e (0< \u03bb2 < \u221e ), The  \u03bb2  parameter will decide the miagnitude given to the coefficients. The coefficients will be somewhere between 0 and ones for simple linear regression.\n\u2211pj=1\u03b22j , is the squared sum of all coefficients.\nNow that we know every nitty-gritty details about this equation, let's use it for science, but before that a couple of things to remember.\n\nIt is essential to standardize the predictor variables before constructing the models.\nIt is important to check for multicollinearity,","1677162d":"# Data Preprocessing and Data Cleaning"}}