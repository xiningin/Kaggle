{"cell_type":{"33eed1ac":"code","dd7ea3ee":"code","9e21cf43":"code","f1202eb9":"code","21aa2904":"code","04ab6dcf":"code","48c01e0a":"code","f74105d3":"code","5f7d9f11":"code","c67243fa":"code","bdc16e2e":"code","5c4f5920":"code","e11e5e35":"code","6a56e575":"code","e2f233e2":"code","e1855283":"code","a135f6dc":"code","880ee11a":"code","e4b20544":"code","03200d49":"code","8850f9a9":"code","9d01d76f":"code","27ee569e":"code","c03db051":"code","1eb17251":"code","7e21381f":"code","4d68d2ba":"code","9fa93da3":"code","10200da6":"code","dc3d8662":"code","80112abb":"code","ba7a743e":"code","33220431":"code","cb9f01ac":"code","75c8ec4f":"code","8b8c30b6":"code","dc26d2db":"code","1b3c4a11":"code","dc69997f":"code","fd5e23de":"code","8d4cdd95":"code","abd6e3a5":"code","ff9dad58":"code","8d9a813d":"code","b1db5a05":"code","76bb0d37":"code","e078d043":"code","1ca8bf7f":"code","6457dffb":"code","85d54538":"code","3e41c8bf":"code","16b30308":"code","051c429b":"code","9f67650d":"code","dc2e04d9":"markdown","d7dbee23":"markdown","b8e8f275":"markdown","05d77887":"markdown","fb3a9608":"markdown","3dde1bb7":"markdown","916dd4ec":"markdown","b1e88acb":"markdown","fd7efa8d":"markdown","66448abc":"markdown","0a3933a5":"markdown","d45813a0":"markdown","fe0f394f":"markdown","5d3ff801":"markdown","46d36697":"markdown","47317384":"markdown","541bb168":"markdown","1cccb683":"markdown","29180771":"markdown","1214d7a5":"markdown","939f2c1d":"markdown","57579999":"markdown","30bf0cf8":"markdown","ebf3d498":"markdown","3bae2d8d":"markdown","57ca6c2b":"markdown","5650da8f":"markdown","5ab493f7":"markdown","5a1242be":"markdown","2ba30330":"markdown","79b81676":"markdown","60d8bae4":"markdown","b5240944":"markdown","276a92b4":"markdown","640ddad5":"markdown","0b6e69f8":"markdown","91b43241":"markdown","5c72b962":"markdown","a27f617b":"markdown","80822b7b":"markdown","e0afb7ba":"markdown","fcf586d1":"markdown","b3101ce8":"markdown","84ce26f9":"markdown","766bfbe6":"markdown","848df8cb":"markdown","7bf4a95e":"markdown","ad1422c7":"markdown","a477a7ce":"markdown","65dd26fb":"markdown","fcde87e4":"markdown","f67f6b47":"markdown","66ed7975":"markdown","d89c0755":"markdown","7d0108c6":"markdown","5107b18a":"markdown"},"source":{"33eed1ac":"%%html\n<style>\n@import url('https:\/\/fonts.googleapis.com\/css?family=Ewert|Roboto&effect=3d|ice|');\nbody {background-color: gainsboro;} \na {color: #37c9e1; font-family: 'Roboto';} \nh1 {color: #37c9e1; font-family: 'Orbitron'; text-shadow: 4px 4px 4px #aaa;} \nh2, h3 {color: slategray; font-family: 'Orbitron'; text-shadow: 4px 4px 4px #aaa;}\nh4 {color: #818286; font-family: 'Roboto';}\nspan {font-family:'Roboto'; color:black; text-shadow: 5px 5px 5px #aaa;}  \ndiv.output_area pre{font-family:'Roboto'; font-size:110%; color:lightblue;}      \n<\/style>","dd7ea3ee":"from IPython.display import YouTubeVideo\nYouTubeVideo('9xoqXVjBEF8')","9e21cf43":"# linear algebra\nimport numpy as np \n\n# data processing\nimport pandas as pd \n\n# data visualization\nimport seaborn as sns\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style\n\n# Algorithms\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nimport itertools\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB","f1202eb9":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\nprint(\"Load and Read Datasets\")","21aa2904":"test['Survived'] = 0\ntest.head()","04ab6dcf":"complete_data = train.append(test, ignore_index=True,sort=False)\ncomplete_data.head()","48c01e0a":"print(\"No. of Training Data samples: \" + str(train.shape[0]))\nprint(\"No. of Test Data samples: \" + str(test.shape[0]))\nprint(\"Complete Data samples: \" + str(complete_data.shape[0]))","f74105d3":"train.isnull().sum()","5f7d9f11":"train['Age'] = train['Age'].fillna(train['Age'].median())\ntest['Age'] = test['Age'].fillna(test['Age'].median())","c67243fa":"train.Embarked.value_counts()","bdc16e2e":"train ['Embarked'] = train['Embarked'].fillna('S')\ntrain.Embarked.unique()","5c4f5920":"test ['Embarked'] = test['Embarked'].fillna('S')\ntest.Embarked.unique()","e11e5e35":"from sklearn.preprocessing import LabelEncoder","6a56e575":"def encode_features(data_set, feature_names):\n    for feature_name in feature_names:\n        le = LabelEncoder()\n        le.fit(data_set[feature_name])\n        encoded_column = le.transform(data_set[feature_name])\n        data_set[feature_name] = encoded_column\n    return data_set","e2f233e2":"features_to_encode = ['Sex', 'Embarked']\ntrain_data = encode_features(train, features_to_encode)\ntrain_data.head(10)","e1855283":"features_to_encode = ['Sex', 'Embarked']\ntest_data = encode_features(test, features_to_encode)\ntest_data.head(10)","a135f6dc":"import re\ndeck = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"U\": 8}\ndata = [train,test]\n\nfor dataset in data:\n    dataset['Cabin'] = dataset['Cabin'].fillna(\"U0\")\n    dataset['Deck'] = dataset['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n    dataset['Deck'] = dataset['Deck'].map(deck)\n    dataset['Deck'] = dataset['Deck'].fillna(0)\n    dataset['Deck'] = dataset['Deck'].astype(int)\n# we can now drop the cabin feature\ntrain = train.drop(['Cabin'], axis=1)\ntest = test.drop(['Cabin'], axis=1)","880ee11a":"data = [train,test]\n\nfor dataset in data:\n    dataset['Fare'] = dataset['Fare'].fillna(0)\n    dataset['Fare'] = dataset['Fare'].astype(int)","e4b20544":"data = [train,test]\ntitles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n\nfor dataset in data:\n    # extract titles\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n    # replace titles with a more common title or as Rare\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr',\\\n                                            'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    # convert titles into numbers\n    dataset['Title'] = dataset['Title'].map(titles)\n    # filling NaN with 0, to get safe\n    dataset['Title'] = dataset['Title'].fillna(0)\ntrain = train.drop(['Name'], axis=1)\ntest = test.drop(['Name'],axis=1)","03200d49":"train['Ticket'].describe()","8850f9a9":"train = train.drop(['Ticket'], axis=1)\ntest = test.drop(['Ticket'],axis=1)","9d01d76f":"train.info()","27ee569e":"test.info()","c03db051":"train.describe()","1eb17251":"test.describe()","7e21381f":"train.head(8)","4d68d2ba":"test.head(8)","9fa93da3":"total = train.isnull().sum().sort_values(ascending=False)\npercent1 = train.isnull().sum()\/train.isnull().count()*100\npercent2 = (round(percent1, 1)).sort_values(ascending=False)\nmissingdata = pd.concat([total, percent2], axis=1, keys=['Total', '%'])\nmissingdata.head(5)","10200da6":"total = test.isnull().sum().sort_values(ascending=False)\npercent1 = test.isnull().sum()\/test.isnull().count()*100\npercent2 = (round(percent1, 1)).sort_values(ascending=False)\nmissingdata = pd.concat([total, percent2], axis=1, keys=['Total', '%'])\nmissingdata.head(5)","dc3d8662":"train.columns.values","80112abb":"survived = 'survived'\nnot_survived = 'not survived'\nfig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10, 4))\nwomen = train[train['Sex']==0]\nmen = train[train['Sex']==1]\nax = sns.distplot(women[women['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[0], kde =False)\nax = sns.distplot(women[women['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[0], kde =False)\nax.legend()\nax.set_title('Female')\nax = sns.distplot(men[men['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[1], kde = False)\nax = sns.distplot(men[men['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[1], kde = False)\nax.legend()\nplt.xkcd()\n_ = ax.set_title('Male')","ba7a743e":"FacetGrid = sns.FacetGrid(train, row='Embarked', size=4.5, aspect=1.6)\nFacetGrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette=None,  order=None, hue_order=None )\nFacetGrid.add_legend()\nplt.xkcd()","33220431":"sns.barplot(x='Pclass', y='Survived', data=train)\nplt.xkcd()","cb9f01ac":"grid = sns.FacetGrid(train, col='Survived', row='Pclass', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend();\nplt.xkcd()","75c8ec4f":"data = [train,test]\nfor dataset in data:\n    dataset['relatives'] = dataset['SibSp'] + dataset['Parch']\n    dataset.loc[dataset['relatives'] > 0, 'not_alone'] = 0\n    dataset.loc[dataset['relatives'] == 0, 'not_alone'] = 1\n    dataset['not_alone'] = dataset['not_alone'].astype(int)\ntrain['not_alone'].value_counts()","8b8c30b6":"axes = sns.factorplot('relatives','Survived', \n                      data=train, aspect = 2.5, )\nplt.xkcd()","dc26d2db":"features_drop = ['SibSp', 'Parch','relatives','Deck']\ntrain = train.drop(features_drop, axis=1)\ntest = test.drop(features_drop, axis=1)\ntrain = train.drop(['PassengerId'], axis=1)\ntest = test.drop(['Survived'],axis=1)","1b3c4a11":"train.head()","dc69997f":"test.head()","fd5e23de":"X_train = train.drop(\"Survived\", axis=1)\nY_train = train[\"Survived\"]\nX_test  = test.drop(\"PassengerId\", axis=1).copy()","8d4cdd95":"clf = LogisticRegression()\nclf.fit(X_train, Y_train)\ny_pred_log_reg = clf.predict(X_test)\nacc_log_reg = round( clf.score(X_train, Y_train) * 100, 2)\nprint (str(acc_log_reg) + ' %')","abd6e3a5":"clf = SVC()\nclf.fit(X_train, Y_train)\ny_pred_svc = clf.predict(X_test)\nacc_svc = round(clf.score(X_train, Y_train) * 100, 2)\nprint (str(acc_svc) + '%')","ff9dad58":"clf = LinearSVC()\nclf.fit(X_train, Y_train)\ny_pred_linear_svc = clf.predict(X_test)\nacc_linear_svc = round(clf.score(X_train, Y_train) * 100, 2)\nprint (str(acc_linear_svc) + '%')","8d9a813d":"sgd = linear_model.SGDClassifier()\nsgd.fit(X_train, Y_train)\nY_pred = sgd.predict(X_test)\n\nsgd.score(X_train, Y_train)\n\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\nprint(str(acc_sgd)+'%')","b1db5a05":"clf = RandomForestClassifier()\nclf.fit(X_train, Y_train)\n\nY_prediction_randomforest = clf.predict(X_test)\n\nclf.score(X_train, Y_train)\nacc_random_forest = round(clf.score(X_train, Y_train) * 100, 2)\nprint(str(acc_random_forest) + '%')","76bb0d37":"clf = KNeighborsClassifier()\nclf.fit(X_train, Y_train)\ny_pred_knn = clf.predict(X_test)\nacc_knn = round(clf.score(X_train, Y_train) * 100, 2)\nprint (str(acc_knn)+'%')","e078d043":"clf = DecisionTreeClassifier()\nclf.fit(X_train, Y_train)\ny_pred_decision_tree = clf.predict(X_test)\nacc_decision_tree = round(clf.score(X_train, Y_train) * 100, 2)\nprint (str(acc_decision_tree) + '%')","1ca8bf7f":"clf = GaussianNB()\nclf.fit(X_train, Y_train)\ny_pred_gnb = clf.predict(X_test)\nacc_gnb = round(clf.score(X_train, Y_train) * 100, 2)\nprint (str(acc_gnb) + '%')","6457dffb":"clf = DecisionTreeClassifier()\nclf.fit(X_train, Y_train)\ny_pred_decision_tree_training_set = clf.predict(X_train)\nacc_decision_tree = round(clf.score(X_train, Y_train) * 100, 2)\nprint (\"Accuracy: %i %% \\n\"%acc_decision_tree)\n\nclass_names = ['Survived', 'Not Survived']\n\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(Y_train, y_pred_decision_tree_training_set)\nnp.set_printoptions(precision=2)\n\nprint ('Confusion Matrix in Numbers')\nprint (cnf_matrix)\nprint ('')\n\ncnf_matrix_percent = cnf_matrix.astype('float') \/ cnf_matrix.sum(axis=1)[:, np.newaxis]\n\nprint ('Confusion Matrix in Percentage')\nprint (cnf_matrix_percent)\nprint ('')\n\ntrue_class_names = ['True Survived', 'True Not Survived']\npredicted_class_names = ['Predicted Survived', 'Predicted Not Survived']\n\ndf_cnf_matrix = pd.DataFrame(cnf_matrix, \n                             index = true_class_names,\n                             columns = predicted_class_names)\n\ndf_cnf_matrix_percent = pd.DataFrame(cnf_matrix_percent, \n                                     index = true_class_names,\n                                     columns = predicted_class_names)\n\nplt.figure(figsize = (15,5))\n\nplt.subplot(121)\nsns.heatmap(df_cnf_matrix, annot=True, fmt='d')\n\nplt.subplot(122)\nsns.heatmap(df_cnf_matrix_percent, annot=True)\nplt.xkcd()","85d54538":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout","3e41c8bf":"# Initialising the ANN\nclassifier = Sequential()","16b30308":"models = pd.DataFrame({\n    'Model': ['Logistic Regression', 'Support Vector Machines', 'Linear SVC', \n              'KNN', 'Decision Tree', 'Random Forest', 'Naive Bayes', 'Stochastic Gradient Decent'],\n    \n    'Score': [acc_log_reg, acc_svc, acc_linear_svc, \n              acc_knn,  acc_decision_tree, acc_random_forest, acc_gnb, acc_sgd]\n    })\n\nmodels.sort_values(by='Score', ascending=False)","051c429b":"test.head()","9f67650d":"submission = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": Y_prediction_randomforest \n    })\nsubmission.to_csv('submission.csv', index=False)","dc2e04d9":"# &#128187; Load and Read DataSets\n","d7dbee23":"# &#128225; Motivation \n## \"The level of technical ability you need to show is not lowered, it\u2019s even higher when you don\u2019t have the educational background, but it\u2019s totally possible.\"\u2014 **Dario Amodei, PhD, Researcher at OpenAI**","b8e8f275":"# 4. Feature Selection\n drop unnecessary columns\/features and keep only the useful ones for our experiment. Column PassengerId is only dropped from Train set because we need PassengerId in Test set while creating Submission file to Kaggle.","05d77887":"Check for missing values in the columns","fb3a9608":"Above you can see the 11 features + the target variable (survived). What features could contribute to a high survival rate ?\nTo me it would make sense if everything except \u2018PassengerId\u2019, \u2018Ticket\u2019 and \u2018Name\u2019 would be correlated with a high survival rate.","3dde1bb7":"# &#128220; About RMS Titanic:\n![Imgur](https:\/\/i.imgur.com\/HyPUqwF.png)\nThe RMS(Royal Mail Ship) Titanic was a British passenger liner that sank in the North Atlantic Ocean in the early morning hours of 15 April 1912, after it collided with an iceberg during its maiden voyage from Southampton to New York City. There were an estimated 2,224 passengers and crew aboard the ship, and more than 1,500 died, making it one of the deadliest commercial peacetime maritime disasters in modern history. The RMS Titanic was the largest ship afloat at the time it entered service and was the second of three Olympic-class ocean liners operated by the White Star Line. The Titanic was built by the Harland and Wolff shipyard in Belfast. Thomas Andrews, her architect, died in the disaster.\n![Imgur](https:\/\/i.imgur.com\/l8lxGPm.jpg)\n","916dd4ec":"## 1. Age and Sex:","b1e88acb":"First I thought, we have to delete the \u2018Cabin\u2019 variable but then I found something interesting. A cabin number looks like \u2018C123\u2019 and the letter refers to the deck. Therefore we\u2019re going to extract these and create a new feature, that contains a persons deck. Afterwords we will convert the feature into a numeric variable. The missing values will be converted to zero. In the picture below you can see the actual decks of the titanic, ranging from A to G.","fd7efa8d":"# 5. Random Forest\n![Imgur](https:\/\/i.imgur.com\/HyPUqwF.png)\n![Imgur](https:\/\/i.imgur.com\/lREy3CV.jpg)\n![Imgur](https:\/\/i.imgur.com\/lEuwiKK.jpg)","66448abc":"# 2.Support Vector Machine (SVM)\n![Imgur](https:\/\/i.imgur.com\/HyPUqwF.png)\nSupport Vector Machine (SVM) model is a Supervised Learning model used for classification and regression analysis. It is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.\n\nIn addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces. Suppose some given data points each belong to one of two classes, and the goal is to decide which class a new data point will be in. In the case of support vector machines, a data point is viewed as a  p -dimensional vector (a list of  p  numbers), and we want to know whether we can separate such points with a  (p\u22121) -dimensional hyperplane.\n\nWhen data are not labeled, supervised learning is not possible, and an unsupervised learning approach is required, which attempts to find natural clustering of the data to groups, and then map new data to these formed groups. The clustering algorithm which provides an improvement to the support vector machines is called support vector clustering and is often used in industrial applications either when data are not labeled or when only some data are labeled as a preprocessing for a classification pass.\n\nIn the below code, SVC stands for Support Vector Classification.","0a3933a5":"# Create Submission File to Kaggle\n","d45813a0":"output_dim is 1 as we want only 1 output from the final layer.\n\nSigmoid function is used when dealing with classfication problems with 2 types of results.(Submax function is used for 3 or more classification results)\n![Imgur](https:\/\/i.imgur.com\/h3TQHR3.png)","fe0f394f":"Since the Ticket attribute has 929 unique tickets, it will be a bit tricky to convert them into useful categories. So we will drop it from the dataset.","5d3ff801":"Replace missing data for Embarked. Let us use the port where maximum passengers have boarded","46d36697":"The \u2018Cabin\u2019 feature needs further investigation, but it looks like that we might want to drop it from the dataset, since 77 % of it are missing.","47317384":"## &#128223; 1. Handle Missing Data","541bb168":"# &#128220; Overview Of Titanic Datasets:\n\n### Data Dictionary\n\n* Age: Age\n\n* Cabin: Cabin\n\n* Embarked: Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)\n\n* Fare: Passenger Fare\n\n* Name: Name\n\n* Parch: Number of Parents\/Children Aboard\n\n* Pclass: Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)\n\n* Sex: Sex\n\n* Sibsp: Number of Siblings\/Spouses Aboard\n\n* Survived: Survival (0 = No; 1 = Yes)\n\n* Ticket: Ticket Number\n\n### Variable Notes\n* pclass: A proxy for socio-economic status (SES) 1st = Upper 2nd = Middle 3rd = Lower\n\n* age: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\n* sibsp: The dataset defines family relations in this way... Sibling = brother, sister, stepbrother, stepsister Spouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\n* parch: The dataset defines family relations in this way... Parent = mother, father Child = daughter, son, stepdaughter, stepson Some children travelled only with a nanny, therefore parch=0 for them.","1cccb683":"## What is Data Pre-pocessing?\n![Imgur](https:\/\/i.imgur.com\/HyPUqwF.png)\nData preprocessing is a data mining technique that involves transforming raw data into an understandable format. Real-world data is often incomplete, inconsistent, and\/or lacking in certain behaviors or trends, and is likely to contain many errors. Data preprocessing is a proven method of resolving such issues. Data preprocessing prepares raw data for further processing.\n![Imgur](https:\/\/i.imgur.com\/VuYZfho.jpg)","29180771":"## 2.  Embarked, Pclass and Sex:\n\n","1214d7a5":"###  &#128210; Note:\nAround 80% of Cabin's data is missing. So it will not be of much use to train the model.\n\nLet us replace the missing values for age with median. Though not a best approach to replace missing data, we shall use this method for sake of simplicity.","939f2c1d":"Above we can see that 38% out of the training-set survived the Titanic. We can also see that the passenger ages range from 0.4 to 80. On top of that we can already detect some features, that contain missing values, like the \u2018Age\u2019 feature.","57579999":"## 3. Pclass:","30bf0cf8":"Here we see clearly, that Pclass is contributing to a persons chance of survival, especially if this person is in class 1. We will create another pclass plot below.","ebf3d498":"Converting \u201cFare\u201d from float to int64, using the \u201castype()\u201d function pandas provides:","3bae2d8d":"From the above table, we can see that *Decision Tree* and *Support Vector Machines* classfiers have the highest accuracy score.\n\nAmong these two, we choose *Support Vector Machines* classifier as it has the ability to limit overfitting as compared to *Decision Tree* classifier.","57ca6c2b":"# &#127909; Intro Of This Notebook:\nI will go through in this notebook the whole process of creating a machine learning model on the famous Titanic dataset, which is used by many people all over the world.","5650da8f":"# &#128250; Short Video on RMS Titanic","5ab493f7":"# \ud83d\udcc8 \ud83d\udcc9 \ud83d\udcca Data Visualization:","5a1242be":"# &#128295; Building Machine Learning Models\n","2ba30330":"### &#128204; Note:\nNow we will train several Machine Learning models and compare their results. Note that because the dataset does not provide labels for their testing-set, we need to use the predictions on the training set to compare the algorithms with each other.","79b81676":"## 4. SibSp and Parch:\nSibSp and Parch would make more sense as a combined feature, that shows the total number of relatives, a person has on the Titanic. I will create it below and also a feature that sows if someone is not alone.","60d8bae4":"Embarked seems to be correlated with survival, depending on the gender.\n\nWomen on port Q and on port S have a higher chance of survival. The inverse is true, if they are at port C. Men have a high survival probability if they are on port C, but a low probability if they are on port Q or S.\n\nPclass also seems to be correlated with survival. We will generate another plot of it below.","b5240944":"## &#128290; 2. Encode categorical feature columns\nEncode the values of the categorical columns -- Sex, Embarked","276a92b4":"# 3. Linear SVM\n![Imgur](https:\/\/i.imgur.com\/HyPUqwF.png)\nLinear SVM is a SVM model with linear kernel.\n\nIn the below code, LinearSVC stands for Linear Support Vector Classification.","640ddad5":"Optimizer is chosen as adam for gradient descent.\n\nBinary_crossentropy is the loss function used.\n\nCross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label. So predicting a probability of .012 when the actual observation label is 1 would be bad and result in a high loss value. A perfect model would have a log loss of 0. [More about this.](https:\/\/ml-cheatsheet.readthedocs.io\/en\/latest\/loss_functions.html)","0b6e69f8":"We will use the Name feature to extract the Titles from the Name, so that we can build a new feature out of that.","91b43241":"# &#128203; Exploratory Data Analysis ","5c72b962":"# 7.Decision Tree\n![Imgur](https:\/\/i.imgur.com\/HyPUqwF.png)\nA decision tree is a flowchart-like structure in which each internal node represents a \"test\" on an attribute (e.g. whether a coin flip comes up heads or tails), each branch represents the outcome of the test, and each leaf node represents a class label (decision taken after computing all attributes). The paths from root to leaf represent classification rules.","a27f617b":"The training-set has 891 examples and 11 features + the target variable (survived). 2 of the features are floats, 5 are integers and 5 are objects. Below I have listed the features with a short description:","80822b7b":"#  &#128223; Data Pre-processing ","e0afb7ba":"# 4. Stochastic Gradient Descent (SGD)\n![Imgur](https:\/\/i.imgur.com\/HyPUqwF.png)\n\nStochastic gradient descent (often shortened to SGD), also known as incremental gradient descent, is an iterative method for optimizing a differentiable objective function, a stochastic approximation of gradient descent optimization. A recent article implicitly credits Herbert Robbins and Sutton Monro for developing SGD in their 1951 article titled \"A Stochastic Approximation Method\"; see Stochastic approximation for more information. It is called stochastic because samples are selected randomly (or shuffled) instead of as a single group (as in standard gradient descent) or in the order they appear in the training set.\nBoth statistical estimation and machine learning consider the problem of minimizing an objective function that has the form of a sum:\n![Imgur](https:\/\/i.imgur.com\/j8zrTZk.png)\nwhere the parameter  ww which minimizes   Q(w) is to be estimated. Each summand function Qi is typically associated with the i-th observation in the data set (used for training).","fcf586d1":"# 10. ANN(Artificial Neuron Network)\n![Imgur](https:\/\/i.imgur.com\/HyPUqwF.png)\n## [What are Artificial Neural Networks?](https:\/\/github.com\/harunshimanto\/100-Days-Of-ML-Code\/blob\/master\/ANN\/ANN.md)\n![Imgur](https:\/\/i.imgur.com\/sml10wz.png)\nAn artificial neuron network (ANN) is a computational model based on the structure and functions of biological neural networks. Information that flows through the network affects the structure of the ANN because a neural network changes - or learns, in a sense - based on that input and output. ANNs are considered nonlinear statistical data modeling tools where the complex relationships between inputs and outputs are modeled or patterns are found. ANN is also known as a neural network.\n<a href=\"https:\/\/imgur.com\/uoXCg42\"><img src=\"https:\/\/i.imgur.com\/uoXCg42.gif\" title=\"source: imgur.com\" \/><\/a>\nA single neuron is known as a perceptron. It consists of a layer of inputs(corresponds to columns of a dataframe). Each input has a weight which controls the magnitude of an input. The summation of the products of these input values and weights is fed to the activation function. Activation functions are really important for a Artificial Neural Network to learn and make sense of something really complicated and Non-linear complex functional mappings between the inputs and response variable.\n\nThey introduce non-linear properties to our Network.Their main purpose is to convert a input signal of a node in a A-NN to an output signal. That output signal now is used as a input in the next layer in the stack. Specifically in A-NN we do the sum of products of inputs(X) and their corresponding Weights(W) and apply a Activation function f(x) to it to get the output of that layer and feed it as an input to the next layer. [1.Refer to this article for more info](https:\/\/towardsdatascience.com\/activation-functions-and-its-types-which-is-better-a9a5310cc8f)\n[2.Refer to this article for more info](https:\/\/towardsdatascience.com\/visualizing-artificial-neural-networks-anns-with-just-one-line-of-code-b4233607209e).\n![Imgur](https:\/\/i.imgur.com\/RFDDPcL.jpg)\nConcept of backpropagation - Backpropagation, short for \"backward propagation of errors,\" is an algorithm for supervised learning of artificial neural networks using gradient descent. Given an artificial neural network and an error function, the method calculates the gradient of the error function with respect to the neural network's weights. It is a generalization of the delta rule for perceptrons to multilayer feedforward neural networks.","b3101ce8":"# 8. Gaussian Naive Bayes\n![Imgur](https:\/\/i.imgur.com\/HyPUqwF.png)\nNaive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features.\n\nBayes' theorem (alternatively Bayes' law or Bayes' rule) describes the probability of an event, based on prior knowledge of conditions that might be related to the event. For example, if cancer is related to age, then, using Bayes' theorem, a person's age can be used to more accurately assess the probability that they have cancer, compared to the assessment of the probability of cancer made without knowledge of the person's age.\n\nNaive Bayes is a simple technique for constructing classifiers: models that assign class labels to problem instances, represented as vectors of feature values, where the class labels are drawn from some finite set. It is not a single algorithm for training such classifiers, but a family of algorithms based on a common principle: all naive Bayes classifiers assume that the value of a particular feature is independent of the value of any other feature, given the class variable. For example, a fruit may be considered to be an apple if it is red, round, and about 10 cm in diameter. A naive Bayes classifier considers each of these features to contribute independently to the probability that this fruit is an apple, regardless of any possible correlations between the color, roundness, and diameter features.","84ce26f9":"# References\nThis notebook is created by learning from the following notebooks:\n\n1.[Predicting the Survival of Titanic Passengers](https:\/\/towardsdatascience.com\/predicting-the-survival-of-titanic-passengers-30870ccc7e8)\n\n2.[A Journey through Titanic](https:\/\/www.kaggle.com\/omarelgabry\/a-journey-through-titanic)\n\n3.[Introduction to machine learning in Python with scikit-learn](https:\/\/www.dataschool.io\/machine-learning-with-scikit-learn\/)\n","766bfbe6":"# 9.Confusion Matrix \n![Imgur](https:\/\/i.imgur.com\/HyPUqwF.png)\n\n\nA [confusion matrix](https:\/\/en.wikipedia.org\/wiki\/Confusion_matrix), also known as an error matrix, is a specific table layout that allows visualization of the performance of an algorithm. Each row of the matrix represents the instances in a predicted class while each column represents the instances in an actual class (or vice versa). The name stems from the fact that it makes it easy to see if the system is confusing two classes (i.e. commonly mislabelling one as another).\n\nIn predictive analytics, a table of confusion (sometimes also called a confusion matrix), is a table with two rows and two columns that reports the number of false positives, false negatives, true positives, and true negatives. This allows more detailed analysis than mere proportion of correct classifications (accuracy). Accuracy is not a reliable metric for the real performance of a classifier, because it will yield misleading results if the data set is unbalanced (that is, when the numbers of observations in different classes vary greatly). For example, if there were 95 cats and only 5 dogs in the data set, a particular classifier might classify all the observations as cats. The overall accuracy would be 95%, but in more detail the classifier would have a 100% recognition rate for the cat class but a 0% recognition rate for the dog class.\n\nHere's another guide explaining [Confusion Matrix with example](http:\/\/www.dataschool.io\/simple-guide-to-confusion-matrix-terminology\/).\n\n$\\begin{matrix} & Predicted Positive & Predicted Negative \\\\ Actual Positive & TP & FN \\\\ Actual Negative & FP & TN \\end{matrix}$\n\nIn our (Titanic problem) case: \n\n>**True Positive:** The classifier predicted *Survived* **and** the passenger actually *Survived*.\n>\n>**True Negative:** The classifier predicted *Not Survived* **and** the passenger actually *Not Survived*.\n>\n>**False Postiive:** The classifier predicted *Survived* **but** the passenger actually *Not Survived*.\n>\n>**False Negative:** The classifier predicted *Not Survived* **but** the passenger actually *Survived*.\n","848df8cb":"## &#128505; What is Exploratory data analysis?\n![Imgur](https:\/\/i.imgur.com\/HyPUqwF.png)\nIn statistics, exploratory data analysis (EDA) is an approach to analyzing data sets to summarize their main characteristics, often with visual methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling or hypothesis testing task.\n\nYou can say that EDA is statisticians way of story telling where you explore data, find patterns and tells insights. Often you have some questions in hand you try to validate those questions by performing EDA. <b>I have one article on [EDA](https:\/\/hackernoon.com\/overview-of-exploratory-data-analysis-with-python-6213e105b00b)","7bf4a95e":"### &#128210; Note:\nYou can see that men have a high probability of survival when they are between 18 and 30 years old, which is also a little bit true for women but not fully. For women the survival chances are higher between 14 and 40.\n\nFor men the probability of survival is very low between the age of 5 and 18, but that isn't true for women. Another thing to note is that infants also have a little bit higher probability of survival.\n\nSince there seem to be certain ages, which have increased odds of survival and because I want every feature to be roughly on the same scale, I will create age groups later on.","ad1422c7":"# &#128229; Download Titanic Datasets:\nYou can Download this Datasets from Our [Machine Learning Home Kaggle.](https:\/\/www.kaggle.com) \n\nHere is Dataset Download link: &#128071;\n* Train => \"https:\/\/www.kaggle.com\/c\/titanic\/download\/train.csv\"\n* Test => \"https:\/\/www.kaggle.com\/c\/titanic\/download\/test.csv\"","a477a7ce":"Batch size defines number of samples that going to be propagated through the network.\n\nAn Epoch is a complete pass through all the training data.","65dd26fb":"# 6.*k*-Nearest Neighbors\n![Imgur](https:\/\/i.imgur.com\/HyPUqwF.png)\n*k* -nearest neighbors algorithm (k-NN) is one of the simplest machine learning algorithms and is used for classification and regression. In both cases, the input consists of the  k  closest training examples in the feature space. The output depends on whether  k -NN is used for classification or regression:\n\n* In  k -NN classification, the output is a class membership. An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its  k  nearest neighbors ( k  is a positive integer, typically small). If  k=1 , then the object is simply assigned to the class of that single nearest neighbor.\n\n* In  k -NN regression, the output is the property value for the object. This value is the average of the values of its  k nearest neighbors.","fcde87e4":"# &#128233; Importing the Libraries","f67f6b47":"From the table above, we can note a few things. First of all, that we need to convert a lot of features into numeric ones later on, so that the machine learning algorithms can process them. Furthermore, we can see that the features have widely different ranges, that we will need to convert into roughly the same scale. We can also spot some more features, that contain missing values (NaN = not a number), that wee need to deal with.Let\u2019s take a more detailed look at what data is actually missing:","66ed7975":"## &#128257; 3. Converting Features\n","d89c0755":"## Comparing Models\n\nLet's compare the accuracy score of all the classifier models used above.","7d0108c6":"# 1. Logistic Regression\n![Imgur](https:\/\/i.imgur.com\/HyPUqwF.png)\nLogistic regression, or logit regression, or logit model is a regression model where the dependent variable (DV) is categorical. This article covers the case of a binary dependent variable\u2014that is, where it can take only two values, \"0\" and \"1\", which represent outcomes such as pass\/fail, win\/lose, alive\/dead or healthy\/sick. Cases where the dependent variable has more than two outcome categories may be analysed in multinomial logistic regression, or, if the multiple categories are ordered, in ordinal logistic regression.","5107b18a":"# &#128209; About the Titanic Problem:\nUsing the machine learning tools, we need to analyze the information about the passensgers of RMS Titanic and predict which passenger has survived. This problem has been published by Kaggle and is widely used for learning basic concepts of Machine Learning"}}