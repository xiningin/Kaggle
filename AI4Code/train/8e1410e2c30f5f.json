{"cell_type":{"324259a2":"code","9bf72045":"code","e887d1fb":"code","fb689d59":"code","756aeb42":"code","df44c294":"code","dd71a2aa":"code","4621ebcc":"code","47fd8272":"code","1f670e42":"code","83a6705b":"code","e061c5d2":"code","af9d29f8":"code","950aabac":"code","6a04b87a":"code","a868cb9e":"code","f858d8aa":"code","efa6eae9":"markdown","8b5d708f":"markdown","a7574b3e":"markdown","79e9b893":"markdown","84ea16a4":"markdown","84670d3d":"markdown"},"source":{"324259a2":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","9bf72045":"import pandas as pd\nimport numpy as np\nfrom scipy.stats import chi2_contingency\n \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline ","e887d1fb":"train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/test.csv')","fb689d59":"train.corr(), train.describe()","756aeb42":"X1 = train.drop(columns = ['claim'])\nX2 = train.drop(columns = ['claim'])\ny = train.claim","df44c294":"X1.isna().sum(), test.isna().sum()","dd71a2aa":"X1[~X1.isnull()] = 0\nX1[X1.isnull()] = 1\nX2[~X1.isnull()] = 0\nX2[X1.isnull()] = 1\n\nX1 = X1.sum(axis=1).astype(int)","4621ebcc":"contigency= pd.crosstab(X1, y)\ncontigency","47fd8272":"plt.figure(figsize=(12,8))\nsns.heatmap(contigency, annot=True, cmap=\"YlGnBu\")","1f670e42":"chi2_contingency(contigency)","83a6705b":"total = contigency[0] + contigency[1]\ncont_rel_0 = contigency[0] \/ total\ncont_rel_1 = contigency[1]  \/ total\npd.DataFrame(np.array([cont_rel_0,cont_rel_1]))","e061c5d2":"n = 15\nn*(n-1)\/2","af9d29f8":"0.05 \/105","950aabac":"pair_comparisons = np.zeros((15,15))\nfor i in range(15):\n    for e in range(i+1,15):\n        temp = X1[(X1 == i) | (X1 == e)]\n        y_temp = y[temp.index]\n        temp_contingency = pd.crosstab(temp, y_temp)\n        c, p, dot, expected = chi2_contingency(temp_contingency)\n        pair_comparisons[e,i] = p","6a04b87a":"pair_comparisons = pd.DataFrame(pair_comparisons)","a868cb9e":"pair_comparisons","f858d8aa":"sns.heatmap(pair_comparisons < (0.05 \/105))\nplt.title('Pair Comparisons Test Results < Adjusted P-Value')\nplt.show()","efa6eae9":"<a id = 'imp'><\/a>","8b5d708f":"<a id = 'chi'><\/a>\n# Chi-Square Test of Independence \n\nDuring a test of independence, our intention is to determine through statistical analysis the **approvation or refection** of a Null Hipothesis.\n\nDepending on the nature of the explanatory and response variables, if they are Categorical or Numerical, we will use a different Independence Test.\n\nIn this case, I'll use a Chi-Squared test of Independence.\n\n## Understanding our Relation\n\nI want to know if the **target variable**, if there was or not a claim, **depends** on the number of missing values in each instance(each row). Under this context, let's define our H0(Null Hipothesis) and H1(Alternative Hipothesis)\n\n**H0**: There is **NO** relation between the number of missing values in each instance and the target variable.\n\n**H1**: There is a relation between the number of missing values in each instance and the target variable.\n\n","a7574b3e":"# Introduction\n\nFor this Kernel, I'm not going to do any predictive model. The intention of it, instead, is to do an in-depth analysis of the Missing values in the tabular data. I'll try to find useful insights in the data, and spend some time exploring the data, which is an often neglected step.\n\n## Table of Contents\n\n1. [Importing Libraries and Dataset](#imp)\n\n2. [Exploratopry Data Analysis](#eda)\n\n    2.1 [Describing the Dataset and Missing Values](#desc)\n        \n3. [Chi-Square Test of Independence](#chi)\n    \n    3.1 [Analyzing the Results](#p)\n        \n    3.2 [Post-Hoc Testing](#posthoc)\n    \n    3.3 [Adjusted P-Value](#adj)\n    \n    3.4[Observations](#obs)\n \n4. [Conclusions](#concl)","79e9b893":"<a id = 'obs'><\/a>\n# Observations\n\nAs you can see from the above DataFrame and it's heatmap representation, **there are many pair relations where we should accept the Null Hipothesis**, which is the opposite to the results obtained at the first approximation.\n\nIt's clear that, as the number of missing values increase, the adjusted p-Value stops being signifficant.\n<a id = 'concl'><\/a>\n# Conclusions\n\nThe Chi-Square Test of Independence allows you to compare significance between two categorical data. From here, you did some Post Hoc Tests and dag deeper into the statistical relations between your explanatory features.\n\n\nWhat do you think? What information do you think this evaluation is giving?\n\ndrK~","84ea16a4":"<a id = 'p'><\/a>\n# Analyzing the Results\n\nIn the Above Table, you can see the **Percentages of claim depending on the number of null values**.  \n\nAs you can see from the Chi-Square Contingency Table, our P-Value is near 0. This number is less than the common standard 0.05 to which we use to compare the P-Value. But this kind of analysis can be prone to many **Type 1 Errors**, \n<a id = 'posthoc'><\/a>\n# Post-Hoc Tests\n\nTo know if this dependency applay to different pair groups of missing values, I'll conduct a Post-Hoc Test, studying the Chi-Square Test for each pair comparison possible.\n\nThe number of possible compaissons is: $$n(n-1)\/2$$ where n is the number of explanatory types(no errors, til 14).\n<a id = 'adj'><\/a>\n# Adjusted P-Value\n\nFor this Pair Comparisons, we will use the p-Value with the **Bonferroni Adjustment**, $$0.05\/c$$ where c is the number of pair comparisons run in the test.","84670d3d":"<a id = 'desc'><\/a>"}}