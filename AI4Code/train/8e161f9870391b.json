{"cell_type":{"e2ac7357":"code","478278e7":"code","40ab1875":"code","d9906246":"code","f9bcb2ca":"code","769ca00c":"code","1488118e":"code","d31466d6":"code","49b38b8c":"code","053e7e77":"code","6d4a4175":"code","e7351f99":"code","b607a763":"code","371691ef":"code","e1537e2c":"code","e57fb000":"code","71aa97d7":"code","2ce172bc":"code","874f0015":"code","14581b89":"code","ce372530":"code","c890b463":"code","448ee215":"code","38442b5d":"code","ccdf7d23":"code","60f8bce9":"code","6ee95f9d":"code","4c5801b6":"code","e6f50274":"code","c3c36971":"code","b7a501aa":"code","7dae6159":"code","8909252a":"code","2c510a09":"code","3a716a29":"code","9dcab27f":"code","d7ecb41f":"code","b2463dc9":"code","3dace225":"code","afbbc6da":"code","44c8be9d":"code","971f6823":"code","23a35765":"code","bc444875":"code","af7c680c":"code","66b9d847":"code","3f3209ca":"code","9159c3e3":"code","4d75ef3a":"code","72e70abd":"code","26525f72":"code","48f8a7cb":"code","faf757c4":"markdown","682b0631":"markdown","e735cc8b":"markdown","ca9603a1":"markdown","edf5639b":"markdown","8f1cea6e":"markdown","552ac578":"markdown","23dcfcc8":"markdown","68c6dc79":"markdown","7edad205":"markdown","8697aead":"markdown","e45bc790":"markdown","0ff01ff1":"markdown","00e29ac9":"markdown","cdb3aa60":"markdown","8138cb9a":"markdown","78d53f77":"markdown","78005851":"markdown","c6d6a92e":"markdown","21f7431b":"markdown","9a0cb0da":"markdown","5e9451ec":"markdown","2f6a55b3":"markdown","f7b29176":"markdown","a84647c9":"markdown","3c05f73e":"markdown","f2aff9c5":"markdown","4adeed66":"markdown","333c9d47":"markdown","7563188d":"markdown","c37c9568":"markdown","9917bce0":"markdown","504ad85f":"markdown","cc49498b":"markdown","eb751bc3":"markdown","295d3edc":"markdown","7ba30c55":"markdown","693c2668":"markdown"},"source":{"e2ac7357":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker imAge: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packAges to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.style.use('fivethirtyeight')\n%matplotlib inline\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","478278e7":"train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain.head()","40ab1875":"test=pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest.head()","d9906246":"sample=pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")\nsample.head()","f9bcb2ca":"train.describe()","769ca00c":"train.info()","1488118e":"import missingno as msno\nmsno.bar(train)\nplt.show()","d31466d6":"plt.figure(figsize = (15, 8))\n\nsns.heatmap(train.corr(), annot = True, linewidths = 1)\nplt.show()","49b38b8c":"plt.figure(figsize = (16, 7))\nsns.distplot(train['Age'])\nplt.title('Distribution Plot of Ages\\n', fontsize =  20)\nplt.show()","053e7e77":"px.scatter(data_frame = train, x = 'Fare', y = 'Pclass', color = 'Fare', template = 'plotly_dark')","6d4a4175":"Age_18_25 = train.Age[(train.Age >= 18) & (train.Age <= 25)]\nAge_26_35 = train.Age[(train.Age >= 26) & (train.Age <= 35)]\nAge_36_45 = train.Age[(train.Age >= 36) & (train.Age <= 45)]\nAge_46_55 = train.Age[(train.Age >= 46) & (train.Age <= 55)]\nAge_55above = train.Age[train.Age >= 55]","e7351f99":"x_Age = ['18-25', '26-35', '36-45', '46-55', '55+']\ny_Age = [len(Age_18_25.values), len(Age_26_35.values), len(Age_36_45.values), len(Age_46_55.values),\n     len(Age_55above.values)]\n\npx.bar(data_frame = train, x = x_Age, y = y_Age, color = x_Age, template = 'plotly_dark',\n       title = 'Number of passengers per Age group')","b607a763":"train.columns","371691ef":"px.histogram(data_frame = train, x = 'Fare', nbins = 10, color = 'Sex', marginal = 'box',\n             template = 'plotly_dark')","e1537e2c":"px.box(x = 'Sex', y = 'Fare', data_frame = train, template = 'plotly_dark')","e57fb000":"sns.catplot(x = 'Sex', y = 'Pclass', hue = 'Survived', data = train, kind = 'box',\n            height = 5, aspect = 2)\nplt.show()\n","71aa97d7":"train.columns","2ce172bc":"# Creating plot\nplt.boxplot(train.Fare)\nplt.legend()","874f0015":"train = train[train['Fare'] <= 300.0]\ntrain.head()","14581b89":"# Creating plot\nplt.boxplot(train.Fare)\nplt.legend()","ce372530":"sns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')","c890b463":"print(\"total number of null nalues in Age\",len(train['Age'])-train['Age'][train['Age'].isnull()==False].count())\nprint(\"total number of null nalues in Cabin\",len(train['Cabin'])-train['Cabin'][train['Cabin'].isnull()==False].count())\nprint(\"total number of null nalues in Embarked\",len(train['Embarked'])-train['Embarked'][train['Embarked'].isnull()==False].count())","448ee215":"sns.heatmap(test.isnull(),yticklabels=False,cbar=False,cmap='viridis')","38442b5d":"print(\"total number of null nalues in Age\",len(test['Age'])-test['Age'][test['Age'].isnull()==False].count())\nprint(\"total number of null nalues in Cabin\",len(test['Cabin'])-test['Cabin'][test['Cabin'].isnull()==False].count())\nprint(\"total number of null nalues in Embarked\",len(test['Fare'])-test['Fare'][test['Fare'].isnull()==False].count())","ccdf7d23":"def impute_nan(df,variable):\n    df[variable+\"_random\"]=df[variable]\n    ##It will have the random sample to fill the na\n    random_sample=df[variable].dropna().sample(df[variable].isnull().sum(),random_state=0)\n    ##pandas need to have same index in order to merge the dataset\n    random_sample.index=df[df[variable].isnull()].index\n    df.loc[df[variable].isnull(),variable+'_random']=random_sample","60f8bce9":"impute_nan(train,\"Age\")","6ee95f9d":"fig = plt.figure()\nax = fig.add_subplot(111)\ntrain['Age'].plot(kind='kde', ax=ax)\ntrain.Age_random.plot(kind='kde', ax=ax, color='red')\nlines, labels = ax.get_legend_handles_labels()\nax.legend(lines, labels, loc='best')\nplt.show()","4c5801b6":"impute_nan(test,\"Age\")","e6f50274":"fig = plt.figure()\nax = fig.add_subplot(111)\ntest['Age'].plot(kind='kde', ax=ax)\ntest.Age_random.plot(kind='kde', ax=ax, color='red')\nlines, labels = ax.get_legend_handles_labels()\nax.legend(lines, labels, loc='best')\nplt.show()","c3c36971":"impute_nan(test,\"Fare\")","b7a501aa":"fig = plt.figure()\nax = fig.add_subplot(111)\ntest['Fare'].plot(kind='kde', ax=ax)\ntest.Fare_random.plot(kind='kde', ax=ax, color='red')\nlines, labels = ax.get_legend_handles_labels()\nax.legend(lines, labels, loc='best')\nplt.show()","7dae6159":"train.Embarked.value_counts()","8909252a":"train.Embarked.fillna('S')","2c510a09":"px.histogram(data_frame = train.Cabin, x = train.Cabin.isnull(), nbins = 10,labels={\n                     \"count\": \"Count\",\n                     \"x\": \"Null values in cabin column\",\n                     },title=\"Training dataset Null values on Cabin column\",template = 'plotly_dark')","3a716a29":"px.histogram(data_frame = test.Cabin, x = test.Cabin.isnull(), nbins = 10,labels={\n                     \"count\": \"Count\",\n                     \"x\": \"Null values in cabin column\",\n                     },title=\"Testing dataset Null values on Cabin column\",template = 'plotly_dark')","9dcab27f":"train=train.drop([\"Age\",\"Cabin\",\"Name\",\"Ticket\",\"PassengerId\"],axis=1)\ntrain.head()","d7ecb41f":"test=test.drop([\"Age\",\"Cabin\",\"Fare\",\"Name\",\"Ticket\"],axis=1)\ntest.head()","b2463dc9":"from sklearn.preprocessing import  LabelEncoder\n# Encode each object type with a label using one hot encoding\n\n# loop over each column\nfor f in train.columns: \n    # check for object type\n    if train[f].dtype=='object':\n      #label encoder \n        lbl = LabelEncoder() \n        lbl.fit(list(train[f].values)) \n        train[f] = lbl.transform(list(train[f].values))\n\n# check the dataframe after label encoding\ntrain.head()","3dace225":"from sklearn.preprocessing import  LabelEncoder\n# Encode each object type with a label using one hot encoding\n\n# loop over each column\nfor f in test.columns: \n    # check for object type\n    if test[f].dtype=='object':\n      #label encoder \n        lbl = LabelEncoder() \n        lbl.fit(list(test[f].values)) \n        test[f] = lbl.transform(list(test[f].values))\n\n# check the dataframe after label encoding\ntest.head()","afbbc6da":"X=train.drop(['Survived'],axis=1)\nY=train[\"Survived\"]","44c8be9d":"from sklearn.model_selection import train_test_split\n# split the data to train and test set\nx_train,x_test,y_train,y_test = train_test_split(X,Y,train_size=0.85,random_state=42)\n\n\nprint(\"training data shape:-{} labels{} \".format(x_train.shape,y_train.shape))\nprint(\"testing data shape:-{} labels{} \".format(x_test.shape,y_test.shape))","971f6823":"from sklearn.tree import DecisionTreeClassifier\n# build a decision tree classifier\nclf = DecisionTreeClassifier().fit(X, Y)\nprint('Accuracy of Decision Tree classifier on training set: {:.2f}'\n     .format(clf.score(x_train, y_train)))\nprint('Accuracy of Decision Tree classifier on test set: {:.2f}'\n     .format(clf.score(x_test, y_test)))","23a35765":"from sklearn.ensemble import RandomForestClassifier\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nfrom sklearn.metrics import r2_score\n","bc444875":"# train Random forest regressor\n\n# create the model\nmodel_rf = RandomForestClassifier(n_estimators=100, max_depth=3, random_state=2)\n\nprint(\"Training..........\")\n# fitting the model\nmodel_rf.fit(X, Y) \n\n# get original predictions\npred_train_rf= model_rf.predict(x_train)\n\nprint(\"Training Evaluation\")\nprint(\"Mean Square Error\",np.sqrt(mean_squared_error(y_train,pred_train_rf)))\nprint(\"R2 Score\",r2_score(y_train, pred_train_rf))\n\nprint(\"Testing Evaluation\")\n# testing predictions\npred_test_rf = model_rf.predict(x_test)\nprint(\"Mean Square Error\",np.sqrt(mean_squared_error(y_test,pred_test_rf)))\nprint(\"R2 Score\",r2_score(y_test, pred_test_rf))","af7c680c":"from xgboost import XGBClassifier\nfrom sklearn.metrics import r2_score\n\nxgb = XGBClassifier(colsample_bylevel= 0.9,\n                    colsample_bytree = 0.8, \n                    gamma=0.99,\n                    max_depth= 5,\n                    min_child_weight= 1,\n                    n_estimators= 8,\n                    nthread= 5,\n                    random_state= 0,\n                    )\nxgb.fit(X,Y)","66b9d847":"from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\nmodel = gnb.fit(X, Y)","3f3209ca":"from sklearn import linear_model\nfrom sklearn import metrics\ndigreg = linear_model.LogisticRegression()\ndigreg.fit(X,Y)\ny_pred = digreg.predict(x_test)\nprint(\"Accuracy of Logistic Regression model is:\",\nmetrics.accuracy_score(y_test, y_pred)*100)","9159c3e3":"from sklearn.svm import SVC # \"Support vector classifier\"\nmodel_svc = SVC(kernel='sigmoid', C=1E10)\nmodel_svc.fit(X, Y)","4d75ef3a":"df_test=test.drop('PassengerId',axis=1)","72e70abd":"predictions_rf = model_rf.predict(df_test)\npredictions_xgb = xgb.predict(df_test)\npredictions_dt = clf.predict(df_test)\nprediction_svc=model_svc.predict(df_test)\nprediction_log=digreg.predict(df_test)\npredctions_gnb = gnb.predict(df_test)","26525f72":"from collections import Counter\n\n\ndef most_frequent(List):\n    occurence_count = Counter(List)\n    return occurence_count.most_common(1)[0][0]\nl=[]\nfor i in range(test.shape[0]):\n    c=list([predictions_rf[i],predictions_xgb[i],predictions_dt[i],prediction_svc[i],prediction_log[i],predctions_gnb[i]])\n    #print(most_frequent(c))\n    l.append(most_frequent(c))","48f8a7cb":"output = pd.DataFrame({'PassengerId': test.PassengerId, 'Survived': l})\noutput.to_csv('submission.csv', index=False)","faf757c4":"## Random Forest","682b0631":"# XG Boost","e735cc8b":"## Let's handle NaN values for categorial data now","ca9603a1":"As we saw earlier, most of the people were from 18-40 so we can use random sampling technique to fill the Age values.","edf5639b":"Random Sampling does it's magic again","8f1cea6e":"## from the above plot it is clear that the data has NaN values","552ac578":"### NaN values in train data","23dcfcc8":"# Logistic Regression","68c6dc79":"## Filling NaN values for age","7edad205":"# Encoding each object type data to an integer","8697aead":"### Encoding the object type","e45bc790":"### Removing unwanted Rows","0ff01ff1":"### For missing values in Cabin dataset, In training data we have 686 values missing out of 888 values in total. Similarly for testing data as well we have 327 missing out of 418 values. Thus predicting missing values here will be of not much use as lot of values are missing from the dataset.","00e29ac9":"## Decision Tree","cdb3aa60":"# Exploratory Data Analysis","8138cb9a":"### NaN values in test data","78d53f77":"## Our aim here is to find out if there is any relation with the Passenger class and Gender to survived ","78005851":"### We can see that Gender has an effect over the surviving rate and class of passenger has some effect on the survived percentage in case of males as well","c6d6a92e":"# Data Preprocessing","21f7431b":"We can see we have class S in maximum number so we can simply fill with the maximum count as we have few values that are missing in the embarked column","9a0cb0da":"Let's do the same for test data as well","5e9451ec":"## Similarly we can use Random samling to fill NaN values in Fare column","2f6a55b3":"## From the above graph it is clear that maxmimum passangers belonged to 20-30 Age group ","f7b29176":"The technique gave similar results in test data as well.","a84647c9":"## Handling NaN values for Embarked column","3c05f73e":"### Reading the data ","f2aff9c5":"## From the above plot it is clear that averAge price range of the tickets has no symmetry, they were sold at uneven prices. ","4adeed66":"We can see that, through Random Sampling technique the varience in the curve is minimal.","333c9d47":"# SVM","7563188d":"# This notebook deals with handling Titanic dataset\n### It is not solution wih 100% overfitted accuracy. There can many other techniques of doing things than that which I used, but this notebook will really help to give you a head start for the data\n#### I have viewed many notebooks on kaggle to come up with this work \npls upvote if like my work","c37c9568":"## Filling NaN values for Cabin column","9917bce0":"# Naive Bias","504ad85f":"# Handeling NaN values","cc49498b":"# Get Prediction","eb751bc3":"# Modeling Phase","295d3edc":"## Looking for outliers","7ba30c55":"## Preparing the data","693c2668":"### We have a single outlier value in Fare which we need to care about"}}