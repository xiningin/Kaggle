{"cell_type":{"4a8c4a69":"code","82e094cd":"code","59f2c3b3":"code","fa1bad17":"code","12a7d0ab":"code","19085b99":"code","e2db69bf":"code","5ba17475":"code","54a0244b":"code","723547c2":"markdown","66f253cd":"markdown","ad033edc":"markdown","ae15cd45":"markdown","a2068fa9":"markdown","20dce93f":"markdown","ca91ac03":"markdown","efefd32a":"markdown","77723d9a":"markdown","c684c062":"markdown"},"source":{"4a8c4a69":"import os\nimport cv2\nimport numpy as np\nimport random\nfrom matplotlib import pyplot\n\nRootDir = \"..\/input\/face-recognition-dataset\/Face Dataset\/\"\n\n# Go over the data, read file names and split them into train and test\ndef index_and_split_data(percentage):\n    # Get data\n    people_count = len(os.listdir(RootDir))\n    print(\"Found images of\", people_count, \"people.\")\n    train_index, test_index = {}, {}\n    n_train, n_test = 0, 0\n    total = 0\n    for name in os.listdir(RootDir):\n        path = RootDir + \"\/\" + name\n        n = len(os.listdir(path))\n        total += n\n        if np.random.rand() < percentage:\n            train_index[name] = n\n            n_train += 1\n        else:\n            test_index[name] = n\n            n_test += 1\n    print(\"Found a total of\", total, \"images.\")\n    return n_train, train_index, n_test, test_index\n\n\n# Run the previous function\nn_train, train_list, n_test, test_list = index_and_split_data(0.9)\nprint(\"Split the people into\", n_train, \"for training and\", n_test, \"for testing.\")\n","82e094cd":"# Use the file names to generate triplets\ndef get_triplets(index):\n    data = []\n    for person in index:\n        d = index[person]\n        anc_no, pos_no = np.random.choice(d, 2, replace=False)\n        anchor = RootDir + \"\/\" + person + \"\/\" + str(anc_no) + \".jpg\"\n        positive = RootDir + \"\/\" + person + \"\/\" + str(pos_no) + \".jpg\"\n\n        # Ensure we select \"other_person\" to be different from \"person\"\n        other_person = int(person)\n        while other_person == int(person):\n            other_person = int(random.choice(list(index.items()))[0])\n\n        negative = RootDir + \"\/\" + str(other_person) + \"\/\"\n        negative += str(np.random.randint(index[str(other_person)])) + \".jpg\"\n        data.append([anchor, positive, negative])\n    random.shuffle(data)\n    return data\n\n\n# Generate triplets for all people, and print the names of the first few files\ntriplets = get_triplets(train_list)\nfor i, triplet in enumerate(triplets):\n    if i == 5:\n        break\n    print(triplet)\n","59f2c3b3":"# Parameters to do with images\nWidth = 299\nHeight = 299\nChannels = 3\nshape = (Width, Height, Channels)\nBatch_Size = 32\n\n# Read a single image, resize and convert it to RGB\ndef read_image(filename):\n    image = cv2.imread(filename)\n    image = cv2.resize(image, (Width, Height))\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    return image\n\n# Read all triplets of images\ndef get_images(data):\n    n = len(data)\n    anchor_data = np.zeros((n, Width, Height, Channels))\n    pos_data = np.zeros((n, Width, Height, Channels))\n    neg_data = np.zeros((n, Width, Height, Channels))\n    for i, triplet in enumerate(data):\n        anchor_name, pos_name, neg_name = triplet\n        anchor_data[i] = read_image(anchor_name)\n        pos_data[i] = read_image(pos_name)\n        neg_data[i] = read_image(neg_name)\n    return anchor_data, pos_data, neg_data\n\n\n# Print a few triplets to see what we have\ndef print_images(a, p, n, flag):\n    # display a  sample of images\n    pyplot.figure(figsize=(15,15))\n    for i in range(a.shape[0]):\n        pyplot.subplot(a.shape[0], 3, 1 + i*3)\n        pyplot.imshow(a[i].astype(int))\n        pyplot.subplot(a.shape[0], 3, 2 + i*3)\n        pyplot.imshow(p[i].astype(int))\n        pyplot.subplot(a.shape[0], 3, 3 + i*3)\n        pyplot.imshow(n[i].astype(int))\n    if flag == 0:  # briefly display images and continue\n        pyplot.show(block=False)\n        pyplot.pause(3)\n        pyplot.close()\n    else:  # wait for user to close window\n        pyplot.show()\n\n\nanchor, pos, neg = get_images(triplets[0:5])\nprint_images(anchor, pos, neg, 0)\n","fa1bad17":"import tensorflow as tf\nfrom tensorflow.keras.applications import inception_v3, resnet, Xception\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import metrics\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras import backend\n\n# model = inception_v3.InceptionV3(include_top=False, weights=\"imagenet\", input_shape=shape)\n# prep = inception_v3.preprocess_input\n\nmodel = resnet.ResNet50(include_top=False, weights=\"imagenet\", input_shape=shape)\nprep = resnet.preprocess_input\n\n# model = Xception(include_top=False, weights='imagenet', input_shape=shape, pooling='max')\n# prep = inception_v3.preprocess_input\n\ndef get_base_model():\n    flatten = layers.Flatten()(model.output)\n    dense1 = layers.Dense(512, activation=\"relu\", name=\"cutoff_dense\")(flatten)\n    dense1 = layers.BatchNormalization()(dense1)\n    dense2 = layers.Dense(256, activation=\"relu\")(dense1)\n    dense2 = layers.BatchNormalization()(dense2)\n    output = layers.Dense(256)(dense2)\n\n    base_model = Model(model.input, output, name=\"Base-Model\")\n\n    trainable = False\n    for layer in base_model.layers:\n        if layer.name == \"cutoff_dense\":\n            trainable = True\n        layer.trainable = trainable\n    # base_model.summary()\n    return base_model\n\n\nencoder = get_base_model()\nprint(\"Got basic model.\")\n","12a7d0ab":"class DistanceLayer(layers.Layer):\n    # A layer to compute \u2016f(A) - f(P)\u2016\u00b2 and \u2016f(A) - f(N)\u2016\u00b2\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n    def call(self, anchor, positive, negative):\n        ap_distance = tf.reduce_sum(tf.square(anchor - positive), -1)\n        an_distance = tf.reduce_sum(tf.square(anchor - negative), -1)\n        return (ap_distance, an_distance)\n\n\nclass SiameseModel(Model):\n    # Builds a Siamese model based on a base-model\n    def __init__(self, siamese_network, margin=1.0):\n        super(SiameseModel, self).__init__()\n        self.siamese_network = siamese_network\n        self.margin = margin\n        self.loss_tracker = metrics.Mean(name=\"loss\")\n\n    def call(self, inputs):\n        return self.siamese_network(inputs)\n\n    def train_step(self, data):\n        # Use GradientTape so we can get the gradients once we compute the loss,\n        # and use them to update the weights\n        with tf.GradientTape() as tape:\n            loss = self._compute_loss(data)\n        gradients = tape.gradient(loss, self.siamese_network.trainable_weights)\n        self.optimizer.apply_gradients(zip(gradients, \n                                           self.siamese_network.trainable_weights))\n        self.loss_tracker.update_state(loss)\n        return {\"loss\": self.loss_tracker.result()}\n\n    def test_step(self, data):\n        loss = self._compute_loss(data)\n        self.loss_tracker.update_state(loss)\n        return {\"loss\": self.loss_tracker.result()}\n\n    def _compute_loss(self, data):\n        # Get the two distances from the network, then compute the triplet loss\n        ap_distance, an_distance = self.siamese_network(data)\n        loss = tf.maximum(ap_distance - an_distance + self.margin, 0.0)\n        return loss\n\n    @property\n    def metrics(self):\n        # We need to list our metrics so the reset_states() can be called automatically.\n        return [self.loss_tracker]\n\n\nanchor_input = layers.Input(name=\"anchor\", shape=shape)\npositive_input = layers.Input(name=\"positive\", shape=shape)\nnegative_input = layers.Input(name=\"negative\", shape=shape)\n\ndistances = DistanceLayer()(\n    encoder(prep(anchor_input)),\n    encoder(prep(positive_input)),\n    encoder(prep(negative_input)),\n)\n\nsiamese_network = Model(inputs=[anchor_input, positive_input, negative_input], \n                        outputs=distances)\nsiamese_model = SiameseModel(siamese_network)\nopt = optimizers.Adam(learning_rate=1e-4)\nsiamese_model.compile(optimizer=opt)\n# plot_model(siamese_model, to_file='siamese model.png', show_shapes=True,\n#            show_layer_names=True)\nprint(\"Got Siamese model.\")","19085b99":"def check_test_data(test_list, n_test):\n    # Get metrics on test data\n    test_triplets = get_triplets(test_list)\n    pos_scores, neg_scores = [], []\n\n    for batch_no in range(int(n_test \/ Batch_Size)):\n        start, end = batch_no * Batch_Size, (batch_no + 1) * Batch_Size\n        anchor, pos, neg = get_images(test_triplets[start:end])\n        pred = siamese_model([anchor, pos, neg])\n        pos_scores += list(pred[0].numpy())\n        neg_scores += list(pred[1].numpy())\n    acc = np.sum(np.array(pos_scores) < np.array(neg_scores)) \/ len(pos_scores)\n    means = (np.mean(pos_scores), np.mean(neg_scores))\n    stds = (np.std(pos_scores), np.std(neg_scores))\n    mins = (np.min(pos_scores), np.min(neg_scores))\n    maxs = (np.max(pos_scores), np.max(neg_scores))\n    print(\"Acc:\", acc, \"Means:\", means, \"stds:\", stds)\n    metrics = [acc, means, stds, mins, maxs]\n    return metrics\n\n\n# Train model and save the best weights\ndef train_and_save_weights():\n    Epochs = 500\n    metrics = []\n    training_loss = []\n\n    for epoch in range(Epochs):\n        # Select triplets for this epoch\n        triplets = get_triplets(train_list)\n        losses = []\n\n        # Run epoch training\n        for batch_no in range(int(n_train \/ Batch_Size)):\n            start, end = batch_no * Batch_Size, (batch_no + 1) * Batch_Size\n            anchor, pos, neg = get_images(triplets[start:end])\n            loss = siamese_model.train_on_batch([anchor, pos, neg]) \/ Batch_Size\n            losses.append(loss)\n        epoch_loss = np.mean(losses)\n        print(\"Epoch:\", epoch, \"Loss:\", epoch_loss)\n        training_loss.append(epoch_loss)\n\n        # Get metrics on test data\n        metrics.append(check_test_data(test_list, n_test))\n\n        # Update learning rate\n        if epoch == 200:\n            print(\"LR now 1e-5.\")\n            backend.set_value(opt.lr, 1e-5)\n        if epoch == 400:\n            print(\"LR now 1e-6.\")\n            backend.set_value(opt.lr, 1e-6)\n\n    # Save weights\n    siamese_model.save_weights(\"final_weights\")\n    return training_loss, metrics\n\nloss, metrics = train_and_save_weights()","e2db69bf":"def plot_metrics(loss, metrics):\n    n = len(metrics)\n    acc = np.zeros(n)\n    pmean, pstd, pmin, pmax = np.zeros(n), np.zeros(n), np.zeros(n), np.zeros(n)\n    nmean, nstd, nmin, nmax = np.zeros(n), np.zeros(n), np.zeros(n), np.zeros(n)\n\n    for i, data in enumerate(metrics):\n        acc[i] = data[0]\n        pmean[i] = data[1][0]\n        nmean[i] = data[1][1]\n        pstd[i] = data[2][0]\n        nstd[i] = data[2][1]\n        pmin[i] = data[3][0]\n        nmin[i] = data[3][1]\n        pmax[i] = data[4][0]\n        nmax[i] = data[4][1]\n\n    pyplot.figure(figsize=(15,5))\n    pyplot.subplot(1, 2, 1)\n    pyplot.plot(loss, 'r', label='Loss')\n    pyplot.legend()\n    pyplot.subplot(1, 2, 2)\n    pyplot.plot(acc, 'g', label='Accuracy')\n    pyplot.legend()\n    pyplot.show()\n\n    pyplot.figure(figsize=(15,5))\n    pyplot.title(\"Separation\")\n    pyplot.xlabel(\"Epochs\")\n    pyplot.ylabel(\"Distance\")\n    pyplot.grid()\n    epochs = np.arange(n)\n    pyplot.fill_between(epochs, pmin, pmax, alpha=0.1, color=\"g\")\n    pyplot.fill_between(epochs, nmin, nmax, alpha=0.1, color=\"r\")\n    pyplot.fill_between(epochs, pmean - pstd, pmean + pstd, alpha=0.9, color=\"g\")\n    pyplot.fill_between(epochs, nmean - nstd, nmean + nstd, alpha=0.9, color=\"r\")\n    pyplot.show()\n\n\n\nplot_metrics(loss, metrics)","5ba17475":"# Copy learned weights from Siamese to encoder model\nfor layer_no in range(len(siamese_model.layers[0].layers[9].layers)):\n    weights = siamese_model.layers[0].layers[9].layers[layer_no].get_weights()\n    encoder.layers[layer_no].set_weights(weights)\n\n\n# Use the trained encoder to encode all images in the dataset\ndef calc_encoding(index):\n    print(\"Calculating database encoding.\")\n    encoding = {}\n    for person in index:\n        d = index[person]\n        batch = np.zeros((d, Width, Height, Channels))\n        for i in range(d):\n            filename = RootDir + \"\/\" + person + \"\/\" + str(i) + \".jpg\"\n            batch[i] = read_image(filename)\n        batch_vec = encoder(prep(batch))\n        encoding[person] = batch_vec.numpy()\n    return encoding\n    print (\"Done.\")\n\n\n# Get all image names together\nn_data, data_list, n_test, test_list = index_and_split_data(1.0)\nencoding = calc_encoding(data_list)","54a0244b":"def get_distance(encoding, first_person, second_person):\n    first_vec = encoding[str(first_person)]\n    second_vec = encoding[str(second_person)]\n    first_norm = 1\/np.sqrt(np.sum(first_vec**2, 1))\n    second_norm = 1\/np.sqrt(np.sum(second_vec**2, 1))\n    dist_mat = np.sum(first_vec[np.newaxis,:,:] * second_vec[:,np.newaxis,:], -1)\n    dist_mat *= first_norm[np.newaxis, :]*second_norm[:, np.newaxis]\n    res = dist_mat.mean()\n    # special case if we are comparing a person to themself: \n    # disregard the diagonal which is all 1s\n    if first_person == second_person:\n        n = first_vec.shape[0]\n        res = (dist_mat.sum() - n) \/ (n*n-n)\n    return res\n\n\nmin_size = 10\n\nimage_no = np.zeros(n_data)\nfor person in data_list:\n    image_no[int(person)] = data_list[person]\nenough_images = (image_no > min_size).sum()\nprint(\"There are\", enough_images, \"people with more than\", min_size, \"images.\")\n\nself = 0\npairs = []\nfor first_person in range(n_data):\n    if image_no[first_person] <= min_size: continue\n    sim = np.zeros(n_data)\n    for second_person in range(n_data):\n        if image_no[second_person] <= min_size: continue\n        sim[second_person] = get_distance(encoding, first_person, second_person)\n    winner = np.argsort(sim)[-1]\n    if winner == first_person:\n        self += 1\n    else:\n        pairs.append((first_person, winner, sim[winner]))\n\nformat_float = \"{:.2f}\".format(self*100\/enough_images)\nprint(\"Of these,\", self, \"(\", format_float, \"%) are closest to themselves.\")\n\npair_no = len(pairs)\nscore = np.zeros(pair_no)\nfirst = np.zeros(pair_no, dtype=int)\nsecond = np.zeros(pair_no, dtype=int)\nfor i, pair in enumerate(pairs):\n    first[i], second[i], score[i] = pair\nbest_pairs = np.argsort(score)[::-1]\n\nprint(\"Best doppelgangers:\")\nfor i in range(10):\n    pyplot.figure(figsize=(15,5))\n    pyplot.subplot(1, 2, 1)\n    filename = RootDir + \"\/\" + str(first[best_pairs[i]]) + \"\/0.jpg\"\n    image1 = read_image(filename)\n    pyplot.imshow(image1.astype(int))\n    pyplot.title(str(first[best_pairs[i]]))\n\n    pyplot.subplot(1, 2, 2)\n    filename = RootDir + \"\/\" + str(second[best_pairs[i]]) + \"\/0.jpg\"\n    image2 = read_image(filename)\n    pyplot.imshow(image2.astype(int))\n    pyplot.title(str(second[best_pairs[i]]))\n    pyplot.show()","723547c2":"Let's try to use our encoding to find out which people look alike. For this we use the cosine similarity metric u\u00b7v \/ (\u2016u\u2016\u00b7\u2016v\u2016)","66f253cd":"# The data\nThis dataset contains images of 1680 celebrities. For each person, there are 2-50 different images named 0.jpg, 1.jpg,... and all are in a single directory whose name is a running index from 0 to 1679. We begin with a few general imports and a function to get the names of all the images and split them into a training and test set.","ad033edc":"# Base model\nNow that the data loader is ready, we turn to the models. Several different base-models can be tried by uncommenting a pair of lines defining \"model\" and \"perp\". Below is code for 3 different models (InceptionV3, Restet50 and Xception). Having tried them it seems Resnet50 trains most quickly so we'll stick to it. We fix the weights in all the convolutional layers on top (as were trained on imagenet) and add a few fully connected layers at the bottom of the model (to be trained below):","ae15cd45":"Now that the model is ready, Let's train it. For this we first need a function to check the test data, which we shall use after each epoch. As there are many more possible triplets than people in the database, the training process is expected to be very noisy and we expect to require many epochs. For each epoch we of course calculate and print the loss (which is used for optimization) but also a few metrics to do with the validation. The most important of these is the accuracy, defined as the number of test triplets in which the anchor-positive distance is smaller than the anchor-negative distance. Following that we calculate and print for each of these the mean, std, minimal and maximal value. Ideally, once trained, the network will provide good separation: the distributions of the anchor-positive and anchor-negative distances will be non-overlapping. Let's see if this is even possible given this dataset, and Kaggle's constraint on session times.","a2068fa9":"Now we need to read the images themselves","20dce93f":"# Using the model\nTraining of the Siamese model is over. Let's extract the weights into the encoder model, and run it on all images to get their encoding vectors:","ca91ac03":"# Siamese model\nUsing the base-model above, we now build a Siamese model. We follow https:\/\/keras.io\/examples\/vision\/siamese_network\/ to build a Siamese network around the base model.\nOur triplet loss is defined as: L(A, P, N) = max(\u2016f(A) - f(P)\u2016\u00b2 - \u2016f(A) - f(N)\u2016\u00b2 + margin, 0)","efefd32a":"# Conclusion\nIn summary, the model can be better trained (pending GPU resources) and the dataset is limited, but we have demonstrated the ability to group images of the same person together and to find similar people.","77723d9a":"# Training behavior\nNow that the training is over, let's graph the loss and metrics:","c684c062":"We now use the file names to generate triplets of the form (anchor, positive, negative), where anchor is an image of each person, positive is another image of the same person, and negative is an image of a different person."}}