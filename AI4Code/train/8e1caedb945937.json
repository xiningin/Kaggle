{"cell_type":{"6ec9380d":"code","d1690349":"code","6663bb74":"code","a3e662c0":"code","562f7e85":"code","28bb997e":"code","36885453":"code","8905812c":"code","d9ddc2c7":"code","5db3692f":"code","8496501d":"code","f51a69de":"code","1e6d488e":"code","6b7c8a97":"code","a5933729":"code","ad102e78":"code","5e9fd3cd":"code","df861c49":"code","4cef7da4":"code","100d3d67":"code","3b129ea6":"code","cbade9ae":"code","aaf0a192":"code","90d9e557":"code","df1a2382":"code","f96905a8":"code","6539671d":"code","0520ed4a":"code","49341e43":"code","1cd4962c":"code","b1753d90":"code","9f5724c6":"code","34f07088":"code","b48c9624":"code","8bbe461a":"code","b83e392b":"code","040c05cd":"code","e4b4af47":"code","3e55b4ca":"code","c355d5d2":"code","2c5dedf0":"code","7049068e":"code","145da67b":"code","a3f227df":"code","2383f137":"code","0610fb9d":"code","02e85ca0":"code","9306413d":"code","cee8a15d":"code","7828f03a":"markdown","026b68ea":"markdown","4f80a8b6":"markdown","ea90e555":"markdown","2abb7f71":"markdown","5eb64084":"markdown","d8d36b14":"markdown","20b2f071":"markdown","63179227":"markdown","48538d33":"markdown","8d8d8bb8":"markdown","457ffec8":"markdown","b4afaecb":"markdown","7534038e":"markdown","3a012b00":"markdown","69433b0d":"markdown","c6090a0f":"markdown","5a18606f":"markdown","a7faf5bc":"markdown","5e9f739b":"markdown","00801909":"markdown","14dccd39":"markdown","08412d60":"markdown","e9342a53":"markdown","f9c9f413":"markdown","1b267c4a":"markdown","eee793d8":"markdown","a0ac29c9":"markdown","a5cb4a3f":"markdown","94743ae3":"markdown","7fa94411":"markdown","e7e037d2":"markdown"},"source":{"6ec9380d":"# importing libraries\nimport pandas as pd # data wrangling and manipulation\nimport numpy as np # linear algebra\nimport matplotlib.pyplot as plt # data visualization\nimport seaborn as sns # data visualization\nimport scipy.stats as stats # scientific python\n\n%matplotlib inline","d1690349":"# importing sklearn stuff\nfrom sklearn.experimental import enable_iterative_imputer # enabling iterative imputer\nfrom sklearn.impute import IterativeImputer # impute missing values after predicting on the model\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nimport xgboost as xgb # boosting method\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.svm import SVC","6663bb74":"# importing the train dataset\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntrain.sample(n = 5) # random records ","a3e662c0":"# importing the test dataset\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntest.sample(n = 5) # random records","562f7e85":"rows, cols = train.shape[0], train.shape[1]\nprint(f'Train Dataset-> Rows: {rows} Columns: {cols}\\n')\n\nprint(\"*\"*37, '\\n')\n\nrows, cols = test.shape[0], test.shape[1]\nprint(f'Test Dataset-> Rows: {rows} Columns: {cols}')","28bb997e":"train_cols = train.columns\nprint(\"Training Dataset Columns: \",train_cols)\n\ntest_cols = test.columns\nprint(\"Test Dataset Columns: \",test_cols)","36885453":"train.describe(include = \"all\")","8905812c":"train.info()","d9ddc2c7":"# (Training Dataset) Variables contain null values\nprint(train.isnull().sum())\n\nprint(\"\\n\",\"*\"*15, \"\\n\")\n\n# (Testing Dataset) Variables contain null values\nprint(test.isnull().sum())","5db3692f":"def drop_cabin(df):\n    df.drop(\"Cabin\", axis = 1, inplace = True)\n    \ndrop_cabin(train)\ndrop_cabin(test)","8496501d":"# continuous numeric variables (Age, Fare) for both train and test\ndef imputing_vals(df, feature):\n    \n    itr_imputer = IterativeImputer()\n    df[feature] = itr_imputer.fit_transform(np.array(df[feature]).reshape(-1, 1))\n\nimputing_vals(train, \"Age\")\nimputing_vals(test, \"Age\")\nimputing_vals(test, \"Fare\")","f51a69de":"# replacing NaN values in \"Embarked\" with most frequent \"S\"\nmost_freq_embarked = train[\"Embarked\"].value_counts().index[0]\ntrain[\"Embarked\"].fillna(most_freq_embarked, inplace = True)","1e6d488e":"# distribution\n\ndef hist_qq_plot(df, feature):\n    sns.set_style(\"darkgrid\")\n    plt.figure(figsize = (10, 5))\n    \n    plt.subplot(1, 2, 1)\n    sns.histplot(df[feature])\n\n    plt.subplot(1, 2, 2)\n    stats.probplot(df[feature], plot = plt)","6b7c8a97":"hist_qq_plot(train, \"Age\") # train's feature \"Age\"","a5933729":"hist_qq_plot(train, \"Fare\") # train's feature \"Fare\"","ad102e78":"# log transformation (will also remove outliers)\ntrain[\"Fare_log\"] = np.log(train[\"Fare\"]+0.1)\nhist_qq_plot(train, \"Fare_log\")","5e9fd3cd":"# log transformation\ntest[\"Fare_log\"] = np.log(test[\"Fare\"]+0.1)","df861c49":"def transform(df, variable1):\n    \n    df[\"Fare\"] = df[variable1]\n    df.drop([variable1], axis = 1, inplace = True)\n    \ntransform(train, \"Fare_log\")\ntransform(test, \"Fare_log\")    ","4cef7da4":"sns.set_style(\"darkgrid\")","100d3d67":"fg, ax = plt.subplots(1, 2, figsize = (10, 5))\n\nsns.countplot(data = train, x = \"Sex\", ax = ax[0], palette = sns.color_palette(\"pastel\"));\nax[0].set_title(\"Count Sex\")\n\nsns.countplot(data = train, x = \"Embarked\", ax = ax[1], palette = sns.color_palette(\"pastel\"));\nax[1].set_title(\"Count Embarked\");","3b129ea6":"cat1 = sns.catplot(data = train, x = \"Sex\", y = \"Survived\", col = \"Embarked\", \n                              kind = \"bar\", ci = None, saturation = 0.5, palette = sns.color_palette([\"r\", \"g\"]))\ncat1.set_axis_labels(\"Sex\", \"Survival Rate\")\ncat1.set_xticklabels([\"Male\", \"Female\"]);","cbade9ae":"cat1 = sns.catplot(data = train, x = \"Sex\", y = \"Survived\", col = \"Pclass\", \n                              kind = \"bar\", ci = None, saturation = 0.5, palette = sns.color_palette([\"r\", \"g\"]))\ncat1.set_axis_labels(\"Sex\", \"Survival Rate\")\ncat1.set_xticklabels([\"Male\", \"Female\"]);","aaf0a192":"# dummy variables of Sex and Embarked\ntrain = train.join(pd.get_dummies(train[[\"Sex\", \"Embarked\"]]))\ntrain.drop([\"Sex\", \"Sex_male\", \"Embarked_Q\", \"Embarked\"], axis = 1, inplace = True)\n\ntest = test.join(pd.get_dummies(test[[\"Sex\", \"Embarked\"]]))\ntest.drop([\"Sex\", \"Sex_male\", \"Embarked_Q\", \"Embarked\"], axis = 1, inplace = True)","90d9e557":"# label encoding  of Pclass because it is ordinal categorical feature\nlabel = LabelEncoder()\ntrain[\"Pclass\"] = label.fit_transform(train[\"Pclass\"])\ntest[\"Pclass\"] = label.fit_transform(test[\"Pclass\"])","df1a2382":"train.head(n = 5)","f96905a8":"# extracting the title name from Name and then droping Name\ndef nameTitle(df):\n    df[\"nameTitle\"] = df[\"Name\"].str.extract('([A-Za-z]+)\\.')\n    df.drop(\"Name\", axis = 1, inplace = True)\n    \nnameTitle(train)\nnameTitle(test)","6539671d":"train[\"nameTitle\"].value_counts()","0520ed4a":"test[\"nameTitle\"].value_counts()","49341e43":"def commonTitle(dataset):\n    dataset[\"nameTitle\"] = dataset[\"nameTitle\"].replace([\"Capt\", \"Col\", \"Countess\", \"Don\", \"Dona\", \"Jonkheer\", \"Lady\", \"Major\", \"Dr\", \"Rev\", \"Sir\"], \"Rare\")\n    dataset[\"nameTitle\"] = dataset[\"nameTitle\"].replace([\"Mlle\", \"Ms\"], \"Miss\")\n    dataset[\"nameTitle\"] = dataset[\"nameTitle\"].replace(\"Mme\", \"Mrs\")\n    \ncommonTitle(train)\ncommonTitle(test)","1cd4962c":"fig, ax = plt.subplots(1, 2, figsize = (10, 5))\n\nsns.countplot(data = train, x = \"nameTitle\", palette = sns.color_palette(\"pastel\"), ax = ax[0]);\nax[0].set_title(\"Count\");\n\nsns.barplot(data = train, x = \"nameTitle\", y = \"Survived\",ci = None, palette = sns.color_palette(\"pastel\"), ax = ax[1]);\nax[1].set_title(\"Survived\");\nax[1].set_ylabel(\"Survival Rate\")","b1753d90":"# use OneHotEnconding on nameTitle as they are nominal categories\ndummies_train_nameTitle = pd.get_dummies(train[\"nameTitle\"], prefix = \"Title_\")\ntrain = train.join(dummies_train_nameTitle)\ntrain.drop([\"nameTitle\", \"Title__Rare\"], axis = 1, inplace = True)\n\ndummies_test_nameTitle = pd.get_dummies(test[\"nameTitle\"], prefix = \"Title_\")\ntest = test.join(dummies_test_nameTitle)\ntest.drop([\"nameTitle\", \"Title__Rare\"], axis = 1, inplace = True)","9f5724c6":"def drop_ticket(df):\n    df.drop(\"Ticket\", axis = 1, inplace = True)\n    \ndrop_ticket(train)\ndrop_ticket(test)","34f07088":"# adding column \"familyAboard\" inluding the person himself and deleting cols \"SibSp\" and \"Parch\"\ndef familyAboard(df, feature1, feature2):\n    df[\"familyAboard\"] = df[feature1] + df[feature2] + 1 # including the person himself\n    df.drop([feature1, feature2], axis = 1, inplace = True)\n\nfamilyAboard(train, \"SibSp\", \"Parch\")\nfamilyAboard(test, \"SibSp\", \"Parch\")","b48c9624":"# droping \"PassengerId\" as it is unique for every passenger and would not contribute anything\ndef drop_passengerId(df):\n    df.drop(\"PassengerId\", axis = 1, inplace = True)\n    \ndrop_passengerId(train)\ndrop_passengerId(test)","8bbe461a":"# correlation \ntrain_correlation  = train.corr()\nplt.figure(figsize = (16, 8));\nsns.heatmap(train_correlation, annot = True, linewidths = 0.7, center = 2, vmin = -1, vmax = 1);","b83e392b":"# separating the target and predictors\nX_train = train.drop(\"Survived\", axis = 1)\ny_train = train[\"Survived\"].ravel()","040c05cd":"# scaling the features\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\ntest = scaler.fit_transform(test)","e4b4af47":"def model_score(model):\n    return cross_val_score(model, X_train, y_train, cv = 5, scoring = \"accuracy\", n_jobs = -1).mean()","3e55b4ca":"model_score(LogisticRegression(random_state = 2))","c355d5d2":"model_score(DecisionTreeClassifier(random_state = 2))","2c5dedf0":"model_score(RandomForestClassifier(random_state = 2))","7049068e":"model_score(KNeighborsClassifier())","145da67b":"model_score(GaussianNB())","a3f227df":"model_score(BaggingClassifier(DecisionTreeClassifier(), n_estimators = 200, random_state = 2))","2383f137":"model_score(xgb.XGBClassifier(random_state = 2))","0610fb9d":"model_score(SVC(random_state = 2))","02e85ca0":"params = {\"C\": [0.1, 0.3, 0.5, 0.7, 1], \"kernel\": [\"rbf\", \"linear\", \"poly\", \"sigmoid\"], \n          \"gamma\": [\"scale\", \"auto\"], \"random_state\" : [1, 2, 3, 4]}\ngrid = GridSearchCV(SVC(), param_grid = params, cv = 5, scoring = None, n_jobs = -1)\ngrid.fit(X_train, y_train)","9306413d":"grid.best_estimator_","cee8a15d":"model_score(grid.best_estimator_)","7828f03a":"The feature Name has name of each passenger which is quite unique but the thing which can be important for our analysis is that each passenger has also his\/her name title.","026b68ea":"Now it looks pretty good so we will use that.\n\n###### I actually tried other transformation like boxcox, exponential and reciprocal but I found log transformation better as compared to them.","4f80a8b6":"What is the brief information about train dataset?","ea90e555":"First of all, lets see the shape of both train and test datasets.","2abb7f71":"Target Variable -> Survived(1 for Yes, 0 for No) \n\nPredictors -> PassengerId, Pclass, Name, Sex, AgeSp, Parch, Ticket, Fare, Cabin and Embarked.\n\n##### Categorical Variables\n- Pclass, Sex and Embarked\n\n- Nominal categorical variables: Sex and Embarked\n- Ordinal categorical variables: Pclass\n\n##### Numeric Variables\n- PassengerId, Age, SibSp, Parch and Fare\n\n- Continuous numeric variables: Age and Fare\n- Discrete numeric variables: PassengerId, SibSp and Parch\n\n##### Mixed Datatypes Variables\n- Ticket and Cabin\n\n##### Object(string) Datatypes Variables\n- Name","5eb64084":"Survival rate for people in Pclass = 1 is highest most of the females survived in all the Pclasses.","d8d36b14":"## Identify the variables","20b2f071":"## Categorical Variables","63179227":"## Hyperparamter tuning for SVC","48538d33":"As some of the title very common like Mr, Miss, Mrs and Master and others are rare so we can make another title from rare ones.","8d8d8bb8":"The Fare feature is testing dataset is also not distributed normally so lets fix it with the log transformation.","457ffec8":"As the feature Age in testing dataset is almost normally distributed so no need to change it.","b4afaecb":"Males were the highest among passengers and most of the passengers embarked from Southampton(\"S\").","7534038e":"##### Conclusion: Survival rate depends on the Sex, Pclass and Embarked.","3a012b00":"Until now we have done the transformations so lets change the name of feature back to Fare from \"Fare_log\".","69433b0d":"As the feature Age in training dataset is almost normally distributed so no need to change it.","c6090a0f":"## Mixed and Object Datatype Variables","5a18606f":"Mrs and Miss nameTitles are fortunate.","a7faf5bc":"## Discrete Numeric Features","5e9f739b":"Oops! the Fare feature in training dataset is not Gaussian distributed. Lets change it.","00801909":"## Missing Values","14dccd39":"Embarked is a nominal categorical feature and only has two missing values. We can simply replace those missing values with most frequent value which is \"S\".","08412d60":"Every passenger can have a ticket with unique number but what if a person is travelling with his family members? So in this case ticket number would be same for some passengers. One way is to get unique ticket numbers and then proceed but this will have a number of unique tickets so it is not a good idea. Instead we can extract information like familyAboard from SibSp and Parch which can be useful as the ticket number all for family members aboard will be same. So we can drop \"Ticket\".","e9342a53":"What are the columns in both train and test datasets?","f9c9f413":"\"nameTile\" is now a nominal categorical feature.","1b267c4a":"## Continuous Numeric Variables","eee793d8":"We can see that emabarked \"C\" provides most survival rate as compare to \"S\" and \"Q\".\n\nAlso the survival rate for females is higher as compare to males.","a0ac29c9":"Cabin feature for both training and testing datasets has a huge number of missing values, filling values for it can lead to overfitting so simply drop it.","a5cb4a3f":"The evaluation score for support vector classifier is highest as compare to other models so we will choose that.","94743ae3":"**Suggestions and Questions are most welcome :)**","7fa94411":"## Feature Scaling","e7e037d2":"## Models Evaluation"}}