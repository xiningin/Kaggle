{"cell_type":{"e683c7a3":"code","1d06d113":"code","2370e3a7":"code","eafe4648":"code","80c976e4":"markdown","0e263698":"markdown","c03e1500":"markdown","d785e8b1":"markdown"},"source":{"e683c7a3":"class LinearRegression:\n    def __init__(self, epochs=500, learning_rate=0.005, threshold=50):\n        self.w = 0\n        self.b = 0\n        self.epochs = epochs\n        self.learning_rate = learning_rate\n        self.threshold = threshold\n        self.__costs = []\n        self.__iterations = []\n        self.__r2 = []\n\n    def normalize(self, X):\n        # first we should normalize (based of Standard normal distribution) features.\n        # By definition, the mean of standard distribution is 0 and the standard deviation is 1.\n        # It will speed up convergence of gradient\n        return X - np.mean(X) \/ np.std(X)\n\n    def predict(self, X):\n        # let we have n examples and m features. Then X is a matrix of shape [n*m] and y is a matrix of shape [n*1]. \n        # y (and, obviously y_hat) have shape [n*1]\n        # this means that w should have shape [m*1] and b should have shape [n*1] (shape of y)\n        # Put all together:  [n*m]*[m*1] + [n*1] = [n*1]\n        #                      X  *  w   +   b   =   y\n        return np.dot(X, self.w) + self.b\n\n    def compute_error(self, X, y):\n        n = len(X)\n        y_hat = self.predict(X) # y_hat is a predicted target variable (y_hat = WX + b)\n        return 1\/(2*n) * np.sum((y_hat - y) ** 2)  # MSE (mean squared error)\n\n    def optimize(self, X, y):  # gradient descent\n        n = len(X)\n        y_hat = self.predict(X)\n        # finding partial derivatives (remember the chain rule!)\n        # also, note that the 2's are mutually destroyed\n        # dw = 1\/2n * 2 * \u2211(y_hat_i - y_i)* x_i    (for one row (for one example))\n        # dw = 1\/n * X.T * (y_hat - y)  (for all data) \n        # (notice that we transpose X because it has shape [n*m] but we multiply it by (y_hat-y) with shape [n*1]\n        # X.T has shape [m*n]\n        \n        dw = 1 \/ n * np.dot(X.T, (y_hat - y)) \n        db = 1 \/ n * np.sum(y_hat - y)\n        # update parameters\n        self.w = self.w - self.learning_rate * dw\n        self.b = self.b - self.learning_rate * db\n\n    def fit(self, X, y):\n        # init parameters\n        self.w = np.zeros(X.shape[1])    # Shape of w is [m*1]\n        self.b = 0\n        # normalize features\n        X = self.normalize(X)\n\n        # actually the learning process \n        for i in range(self.epochs):\n            cost = self.compute_error(X, y)\n            if cost < self.threshold: \n                break   # overfitting prevention\n            else:\n                self.optimize(X, y)\n                # lists for visualization\n                self.__iterations.append(i)\n                self.__costs.append(cost)    \n                self.__r2.append(self.score(X, y))\n    \n    def score(self, X, y): # coefficient of determination (R2-score)\n        return 1 - (np.sum((y - self.predict(X)) ** 2) \/ np.sum((y - np.mean(y)) ** 2))\n    \n    def plot(self,xlabel, ylabel, figsize=(7,5)):\n        plt.figure(figsize=figsize)\n        if ylabel == 'R2-score':\n            plt.plot(self.__iterations,self.__r2)\n        elif ylabel == 'MSE':\n            plt.plot(self.__iterations,self.__costs)\n        plt.xlabel(f'{xlabel}')\n        plt.ylabel(f'{ylabel}')\n        plt.title(f'{xlabel} during iterations')\n        plt.show()","1d06d113":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import datasets\n\nX, y = datasets.make_regression(n_samples=400, n_features=10, noise=25, random_state=9)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=9)\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\npredict = model.predict(X_test)\nprint(f'Score: {model.score(X_test, y_test)}')\nprint(f'MSE: {model.compute_error(X_test, y_test)}')\nmodel.plot('Iterations', 'R2-score')\nmodel.plot('Iterations', 'MSE')","2370e3a7":"class LinearRegression:\n    def __init__(self, epochs=500, learning_rate=0.005, threshold=50):\n        self.w = 0\n        self.b = 0\n        self.epochs = epochs\n        self.learning_rate = learning_rate\n        self.threshold = threshold\n\n    def fit(self, X, y):\n        # normalize features\n        X -= np.mean(X) \n        X \/= np.std(X)\n        \n        # init parameters\n        self.w = np.zeros(X.shape[1])    # Shape of w is [m*1]\n        self.b = 0        \n        \n        n = len(X)   \n        # actually the learning process \n        for i in range(self.epochs):            \n            y_hat = self.predict(X) # y_hat is a predicted target variable (y_hat = WX + b)\n            cost = 1\/(2*n) * np.sum((y_hat - y) ** 2)\n            if cost < self.threshold:\n                break   \n            else: # optimize parameters\n                y_hat = self.predict(X)\n                dw = 1 \/ n * np.dot(X.T, (y_hat - y))\n                db = 1 \/ n * np.sum(y_hat - y)\n                self.w -= self.learning_rate * dw\n                self.b -= self.learning_rate * db   \n    \n    def predict(self, X):\n        return np.dot(X, self.w) + self.b\n    \n    def score(self, X, y): # coefficient of determination (R2-score)\n        return 1 - (np.sum((y - self.predict(X)) ** 2) \/ np.sum((y - np.mean(y)) ** 2))","eafe4648":"import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import datasets\n\nX, y = datasets.make_regression(n_samples=400, n_features=10, noise=25, random_state=9)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=9)\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\npredict = model.predict(X_test)\nprint(f'Score: {model.score(X_test, y_test)}')","80c976e4":"### Hi everybody! I wrote this article for beginners to give the most complete description of how linear regression works. Here is 2 versions of Linear regression. In the 1st one I tried to explain it in as much detail as possible. The 2nd version is the same but more compact. If you like it please upvote it. Let's go!","0e263698":"# 2nd version","c03e1500":"## The process of training and evaluating the model can be divided into the following steps:\n### 1) Normalize features to speed up convergence\n![image.png](attachment:eb4c8c21-4ac9-4bd6-be91-9538166a1abc.png)\n### 2) Predict value (y_hat = wX + b)    (all this variables are vectors)\n### 3) Evaluate cost function (J)\n![image.png](attachment:148f061b-f514-4796-bfb8-6b8d3fd343f2.png)\n#### We devide MSE by 2 for numerical stability to reduce 2 which we got after differentiating (power rule) \n\n### 4) Reduce cost function via gradient descent\n####  a) We calculate partial derivatives (dw, db) with respect w and with respect b\n####  b) multiply this value by learning rate (lr is value that controls the speed of convergence (If speed is too high -- we can never reach minimum. If speed is to slow -- it takes too many steps)\n####  c) We substruct gradient of our cost multiplied by learning rate and became new values are old values to new ones\n####         w_new = w_old - learning_rate * dw \n####         b_new = b_old - learning_rate * db \n### 5) Repeat steps 2-4\n### 6) Evaluate the model with R2-score \n\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#r2-scorehttps:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#r2-scorehttps:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#r2-scorehttps:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#r2-score\n\n![image.png](attachment:dc5b7832-1251-41cd-8732-a9c211f2a209.png)","d785e8b1":"# 1st version"}}