{"cell_type":{"32066ad0":"code","e1e6ccc5":"code","64ba86ec":"code","ea73962c":"code","0ef42d91":"code","d94a7ab5":"code","e839e37a":"code","1b0aa99c":"code","c119e9b0":"code","162f0ee8":"code","42dac6ee":"code","4139180d":"code","7946bbde":"code","27c27ac0":"code","1517bcd9":"code","eea71e1e":"code","ead84b97":"code","98f74b4f":"code","279db636":"code","be647080":"code","11f83adc":"code","15dcb4f9":"code","cbf1b446":"code","76ca772e":"code","3318d5dd":"code","60e14a52":"code","3b9f4751":"code","424ce0ba":"code","a7d2ea39":"code","92280550":"code","6ca38530":"code","e9c16af6":"code","2d6c21d5":"code","c994efa6":"code","46698a21":"code","ca414207":"code","07eaff67":"code","841ea3a3":"code","9e1554f7":"code","16b68973":"code","73068b07":"code","49db28ca":"code","1c271a53":"code","82b75d2e":"code","f2a9dfd4":"code","a10f17de":"code","d8f8598e":"code","8e5cbe1c":"code","4064a93f":"code","4e798265":"code","0ebc7029":"markdown","375e6bcf":"markdown","edfc3434":"markdown","374d5eed":"markdown","89700f5f":"markdown","a59738df":"markdown","5c673826":"markdown","13b7337a":"markdown","1a70981e":"markdown","3852d700":"markdown","1b954b91":"markdown","8f873715":"markdown","7995c701":"markdown","5608e608":"markdown","2cbc28f0":"markdown","8adb393a":"markdown","4ae7b8b0":"markdown"},"source":{"32066ad0":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","e1e6ccc5":"df = pd.read_csv('..\/input\/tvradionewspaperadvertising\/Advertising.csv')","64ba86ec":"df.head()","ea73962c":"df.info()","0ef42d91":"df.describe()","d94a7ab5":"df.columns","e839e37a":"for i in ['TV', 'Radio', 'Newspaper']:\n    sns.scatterplot(x=i,y='Sales',data=df)\n    plt.show()\n          ","1b0aa99c":"sns.pairplot(df,diag_kind='kde')","c119e9b0":"plt.figure(dpi=150)\nsns.heatmap(df.corr(),annot=True, cmap='viridis',lw=1)","162f0ee8":"x = df.drop('Sales',axis=1)\ny = df['Sales']","42dac6ee":"from sklearn.preprocessing import PolynomialFeatures","4139180d":"polynomial_conevrter = PolynomialFeatures(degree=2, include_bias=False)","7946bbde":"polynomial_features = polynomial_conevrter.fit_transform(x)","27c27ac0":"print(x.shape)\nprint(polynomial_features.shape)","1517bcd9":"data = pd.DataFrame(polynomial_features,columns='tv rad news tv**2 tv*rad tv*news rad**2 rad*news news**2'.split())","eea71e1e":"data.head()","ead84b97":"from sklearn.model_selection import train_test_split","98f74b4f":"x_train, x_test, y_train, y_test = train_test_split(polynomial_features, y, test_size=0.3, random_state=42)","279db636":"from sklearn.preprocessing import StandardScaler","be647080":"scaler = StandardScaler()\nx_train_scaled = scaler.fit_transform(x_train)\nx_test_scaled = scaler.transform(x_test)","11f83adc":"from sklearn.linear_model import LinearRegression","15dcb4f9":"model = LinearRegression()","cbf1b446":"model.fit(x_train_scaled,y_train)","76ca772e":"model.coef_","3318d5dd":"test_predictions = model.predict(x_test_scaled)","60e14a52":"from sklearn.metrics import mean_absolute_error,mean_squared_error","3b9f4751":"mae = mean_absolute_error(y_test,test_predictions)\nrmse = np.sqrt(mean_squared_error(y_test,test_predictions))\n","424ce0ba":"mae","a7d2ea39":"rmse","92280550":"sns.displot(y_test-test_predictions, bins=30, kde=True)","6ca38530":"sns.scatterplot(x=y_test,y=test_predictions-y_test)\nplt.axhline(y=0,c='r',ls='--')","e9c16af6":"sns.scatterplot(x=y_test,y=test_predictions)","2d6c21d5":"# TRAINING ERROR PER DEGREE\ntrain_rmse_errors = []\n# TEST ERROR PER DEGREE\ntest_rmse_errors = []\n\nfor d in range(1,10):\n    \n    # CREATE POLY DATA SET FOR DEGREE \"d\"\n    polynomial_converter = PolynomialFeatures(degree=d,include_bias=False)\n    poly_features = polynomial_converter.fit_transform(x)\n    \n    # SPLIT THIS NEW POLY DATA SET\n    X_train, X_test, y_train, y_test = train_test_split(poly_features, y, test_size=0.3, random_state=42)\n    \n    # SCALING\n    x_train_scaled = scaler.fit_transform(X_train)\n    x_test_scaled = scaler.transform(X_test)\n    \n    # TRAIN ON THIS NEW POLY SET\n    model = LinearRegression(fit_intercept=True)\n    model.fit(x_train_scaled,y_train)\n    \n    # PREDICT ON BOTH TRAIN AND TEST\n    train_pred = model.predict(x_train_scaled)\n    test_pred = model.predict(x_test_scaled)\n    \n    # Calculate Errors\n    \n    # Errors on Train Set\n    train_RMSE = np.sqrt(mean_squared_error(y_train,train_pred))\n    \n    # Errors on Test Set\n    test_RMSE = np.sqrt(mean_squared_error(y_test,test_pred))\n\n    # Append errors to lists for plotting later\n   \n    train_rmse_errors.append(train_RMSE)\n    test_rmse_errors.append(test_RMSE)","c994efa6":"poly = pd.DataFrame()\npoly['Train error'] = train_rmse_errors\npoly['Test error'] = test_rmse_errors\nnew_index = \"1 2 3 4 5 6 7 8 9\".split()\npoly['degree'] = new_index\npoly.set_index('degree')","46698a21":"poly['Test error'].min()","ca414207":"plt.figure(dpi=150)\nplt.plot(range(1,6),train_rmse_errors[:5],label='TRAIN')\nplt.plot(range(1,6),test_rmse_errors[:5],label='TEST')\nplt.xlabel(\"Polynomial Degree\")\nplt.ylabel(\"RMSE\")\nplt.legend()","07eaff67":"plt.figure(dpi=150)\nplt.plot(range(1,10),train_rmse_errors,label='TRAIN')\nplt.plot(range(1,10),test_rmse_errors,label='TEST')\nplt.xlabel(\"Polynomial Degree\")\nplt.ylabel(\"RMSE\")\nplt.legend()","841ea3a3":"plt.figure(dpi=150)\nplt.plot(range(1,10),train_rmse_errors,label='TRAIN')\nplt.plot(range(1,10),test_rmse_errors,label='TEST')\nplt.xlabel(\"Polynomial Degree\")\nplt.ylabel(\"RMSE\")\nplt.ylim(0,100)\nplt.legend()","9e1554f7":"final_poly_converter = PolynomialFeatures(degree=2,include_bias=False)","16b68973":"polynomial_features = final_poly_converter.fit_transform(x)","73068b07":"scaled_polynomial_features = scaler.fit_transform(polynomial_features)","49db28ca":"scaled_polynomial_features","1c271a53":"final_model = LinearRegression()","82b75d2e":"final_model.fit(scaled_polynomial_features,y)","f2a9dfd4":"new_data = [[150,20,100]]","a10f17de":"new_data_polynomial = polynomial_conevrter.fit_transform(new_data)","d8f8598e":"new_data_polynomial","8e5cbe1c":"new_data_scaled = scaler.transform(new_data_polynomial)","4064a93f":"new_data_scaled","4e798265":"final_model.predict(new_data_scaled)","0ebc7029":"# Plots","375e6bcf":"# Imports","edfc3434":"## Train | Test Split","374d5eed":"#### Minimum test error is found at degree 2, so it will be a good choice for the degree, can be seen from the plots below too","89700f5f":"# Reading data","a59738df":"# Polynomial Regression","5c673826":"**From Preprocessing, import PolynomialFeatures, which will help us transform our original data set by adding polynomial features**\n\nWe will go from the equation in the form (shown here as if we only had one x feature):\n\n$$\\hat{y} = \\beta_0 + \\beta_1x_1 + \\epsilon $$\n\nand create more features from the original x feature for some *d* degree of polynomial.\n\n$$\\hat{y} = \\beta_0 + \\beta_1x_1 + \\beta_1x^2_1 +  ... + \\beta_dx^d_1 + \\epsilon$$\n\nThen we can call the linear regression model on it, since in reality, we're just treating these new polynomial features x^2, x^3, ... x^d as new features. Obviously we need to be careful about choosing the correct value of *d* , the degree of the model. Our metric results on the test set will help us with this!\n\n**The other thing to note here is we have multiple X features, not just a single one as in the formula above, so in reality, the PolynomialFeatures will also take *interaction* terms into account for example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].**","13b7337a":"---\n\n## Choosing best degress for polynomial features\n\n### Adjusting Parameters\n\nAre we satisfied with this performance? Perhaps a higher order would improve performance even more! But how high is too high? It is now up to us to possibly go back and adjust our model and parameters, let's explore higher order Polynomials in a loop and plot out their error. This will nicely lead us into a discussion on Overfitting.\n\nLet's use a for loop to do the following:\n\n1. Create different order polynomial X data\n2. Split that polynomial data for train\/test\n3. Fit on the training data\n4. Report back the metrics on *both* the train and test results\n5. Plot these results and explore overfitting","1a70981e":"## Model for fitting on Polynomial Data\n\n### Create an instance of the model with parameters","3852d700":"## Predicting on new data","1b954b91":"## Plotting residuals","8f873715":"## Fit\/Train the Model on the training data","7995c701":"## Scaling the data","5608e608":"## These results are better than the plain linear regression model without polunomial features\n### see results at https:\/\/www.kaggle.com\/tharunnayak14\/linear-regression-scikit-learn-and-normal-equation","2cbc28f0":"## Finalizing Model Choice","8adb393a":"### We converted the previous data frame into a degree 2 data frame where new columns are added","4ae7b8b0":"\n-----\n\n## Evaluation on the Test Set"}}