{"cell_type":{"7242a871":"code","4b08be57":"code","cfdf1ee3":"code","df246ce0":"code","52af2fa5":"code","f3a47ea4":"code","34a68801":"code","86316aff":"code","0e81c097":"code","1eba06c7":"code","deb761ff":"code","ae3c1f8b":"code","3f3f7700":"code","737002a5":"code","0988473c":"code","39df72a8":"code","aac0f5ee":"code","f2a50d14":"code","9da2ceaf":"code","b35d5c9b":"code","d77e6f06":"code","5d74a5f3":"code","b75a5c36":"code","b8950226":"code","15ad95c2":"code","89cf7b8b":"code","fb69b88d":"code","5d553748":"code","32c07fb6":"code","77cb2b20":"code","c60e5137":"code","225d9bb9":"code","6037ac02":"code","7ad4fa1d":"code","03d42179":"code","55c4a41b":"code","0e94a941":"code","57ede0fd":"code","b64e86be":"code","0ca5c8b2":"code","86420b74":"code","ccf3216c":"code","a40b3110":"code","f43f3b69":"code","cb243997":"code","802fbe8b":"code","f82f51e9":"code","54d21b8c":"code","a00d4c09":"code","aa540ffc":"code","4ddb71f2":"code","a34a6269":"code","4c357bb7":"code","3ea8def6":"code","74e50447":"code","3b6ad6f5":"code","09a78737":"code","318bdfc9":"code","601b4e1a":"code","d4ba44bf":"code","b70e40ce":"code","680da8e8":"code","56c75151":"code","aa88fa13":"code","d6870047":"code","57022de2":"code","2cd4b37d":"code","45e7ad63":"code","a3d31866":"code","c0403a6c":"code","7f1ce22f":"markdown","e66c7dae":"markdown","2a63a206":"markdown","d1ea5b2f":"markdown","a7f3fe18":"markdown","cf107f29":"markdown","212f1bb8":"markdown","e700c253":"markdown","d5574399":"markdown","58af8ef5":"markdown","b7cb401b":"markdown","7f442962":"markdown","eebb17ba":"markdown","12e4e0ae":"markdown","32a4a630":"markdown","7e85f391":"markdown","ec88fb77":"markdown","da535e99":"markdown","66306208":"markdown","6dbfc412":"markdown","fe89d5fc":"markdown","98cbe934":"markdown","46bf11b1":"markdown","a8be5623":"markdown","0ff21e4a":"markdown","b94767f3":"markdown","e47d1d0f":"markdown","42d4ea38":"markdown","51306621":"markdown","a4676a1f":"markdown","a39581eb":"markdown","24132a37":"markdown","1a78e948":"markdown","c4801a76":"markdown","7a966425":"markdown","2d33a0a9":"markdown","65359819":"markdown","5a0221c1":"markdown","e1a1aad6":"markdown","053c9e54":"markdown","fc8b758b":"markdown","43917ca2":"markdown","9abc0a3e":"markdown","8f020bf5":"markdown","b7d51651":"markdown","387e80c5":"markdown","94c47119":"markdown","4c35b181":"markdown","bf4a1645":"markdown","8e5796c6":"markdown","e0444872":"markdown","c3a15fbb":"markdown","937c0b8b":"markdown","56450a64":"markdown","b2b4978f":"markdown","648fecc0":"markdown","e9924636":"markdown","09c0c325":"markdown","7e5f1cea":"markdown","179c95ef":"markdown"},"source":{"7242a871":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom scipy.stats import spearmanr\nfrom statsmodels.graphics.gofplots import qqplot\nplt.style.use('bmh')\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nfrom sklearn.decomposition import PCA\nimport statsmodels.formula.api as sm\nfrom statsmodels.regression.quantile_regression import QuantReg\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.ensemble import GradientBoostingRegressor\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points","4b08be57":"df_train = pd.read_csv('..\/input\/train.csv')","cfdf1ee3":"df_train.head()","df246ce0":"df_test = pd.read_csv('..\/input\/test.csv')\nset(df_train.columns).difference(df_test.columns)","52af2fa5":"df_train['IsTrain'] = 1\ndf_test['IsTrain']=0\ndf = pd.concat((df_train, df_test), sort=False).reset_index(drop=True)","f3a47ea4":"df.groupby('IsTrain')['IsTrain'].count()","34a68801":"df_IDs = df[['Id','IsTrain']]\ndel df['Id']","86316aff":"VarCatCnt = df[:].nunique()\ndf_uniq = pd.DataFrame({'VarName': VarCatCnt.index,'cnt': VarCatCnt.values})\ndf_uniq.head()","0e81c097":"df_contVars = df_uniq[(df_uniq.cnt > 30) | (df_uniq.VarName == 'SalePrice') | (df_uniq.VarName == 'IsTrain')]\ndf_num = df[df_contVars.VarName]\ndf_num.shape","1eba06c7":"df_categVars = df_uniq[(df_uniq.cnt <= 30) | (df_uniq.VarName == 'SalePrice') | (df_uniq.VarName == 'IsTrain')]\ndf_categ = df[df_categVars.VarName]\ndf_categ.shape","deb761ff":"for i in range(0, len(df_num.columns)-1):\n    if (df_num[df_num.columns[i]].isnull().sum() != 0) & (df_num.columns[i] != 'SalePrice'):\n        print(df_num.columns[i] + \" Null Count:\" + str(df_num[df_num.columns[i]].isnull().sum()))\n        df_num[df_num.columns[i]] = df_num[df_num.columns[i]].fillna(0)","ae3c1f8b":"for i in range(0, len(df_categ.columns)-1):\n    if (df_categ[df_categ.columns[i]].dtype == object) & (df_categ[df_categ.columns[i]].isnull().sum() != 0):\n            #print(df_categ.columns[i] + \" Null Count:\" + str(df_categ[df_categ.columns[i]].isnull().sum()))\n            df_categ[df_categ.columns[i]].replace(np.nan, 'NA', inplace= True)","3f3f7700":"df_categ_N = df_categ.select_dtypes(include = ['float64', 'int64'])\ndf_categ_N.info()","737002a5":"df_categ['BsmtFullBath'].replace(np.nan, 0, inplace= True)\ndf_categ['BsmtHalfBath'].replace(np.nan, 0, inplace= True)","0988473c":"c = 0\nlen_c = 3 # (len(df_categ.columns)-2)\nfig, axes = plt.subplots(len_c, 2, figsize=(10, 13))     # fig height = 70 -> in figsize(width,height)\nfor i, ax in enumerate(fig.axes):\n    if (c < len_c) & (i % 2 == 0):\n        ax.set_xticklabels(ax.xaxis.get_majorticklabels(), rotation=45)\n        sns.countplot(x=df_categ.columns[c], alpha=0.7, data=df_categ, ax=ax)\n\n    if (c < len_c) & (i % 2 != 0):\n        ax.set_xticklabels(ax.xaxis.get_majorticklabels(), rotation=45)\n        sns.boxplot(data = df_categ, x=df_categ.columns[c], y='SalePrice', ax=ax)\n        c = c + 1\nfig.tight_layout()","39df72a8":"df_num.insert(loc=20, column='OverallQual', value=df_categ[['OverallQual']])\ndf_num.insert(loc=20, column='OverallCond', value=df_categ[['OverallCond']])\ndf_num.insert(loc=20, column='YrSold', value=df_categ[['YrSold']])\ndf_num.insert(loc=20, column='TotRmsAbvGrd', value=df_categ[['TotRmsAbvGrd']])\ndf_num.insert(loc=20, column='Fireplaces', value=df_categ[['Fireplaces']])\ndf_num.insert(loc=20, column='GarageCars', value=df_categ[['GarageCars']])","aac0f5ee":"df_num['GarageCars'].fillna(0, inplace=True)","f2a50d14":"df_categ.drop(['GarageCars'\n                ,'Fireplaces'\n                ,'TotRmsAbvGrd'\n                ,'YrSold'\n                ,'OverallQual'\n                ,'OverallCond']\n            , axis=1, inplace = True)","9da2ceaf":"CatVarQual = ['ExterQual','BsmtQual','HeatingQC','KitchenQual','FireplaceQu','GarageQual']\nmap_dict = {'Ex': 5,\n            'Gd': 4,\n            'TA': 3,\n            'Fa': 2,\n            'Po': 1,\n            'NA': 3}\n\ndf_categQ = pd.DataFrame()\nfor i in range(0, len(CatVarQual)):\n    df_categQ[CatVarQual[i]+'_N'] = df_categ[CatVarQual[i]].map(map_dict)","b35d5c9b":"CatVarList = [column for column in df_categ if (column not in set(CatVarQual))]","d77e6f06":"df_categN = df_categ\nfor i in range(0, len(CatVarList)-2):\n    catVar = CatVarList[i]  #catVar = 'MSSubClass'\n    cl = df_categ.groupby(catVar)['SalePrice'].median().sort_values()\n    df_cl = pd.DataFrame({'Category': cl.index,'SortVal': cl.values})\n    df_cl.replace(np.nan, df_categ['SalePrice'].median(), inplace= True)\n    df_cl[catVar+'_N']=df_cl['SortVal']\/10000\n    #df_cl[catVar+'_N']=df_cl['SortVal'].rank()\n    #print(df_cl) #if want to see how the categories got ranked\n    df_categN = pd.merge(df_categN,\n                        df_cl[['Category', catVar+'_N','SortVal']],\n                        left_on=catVar,\n                        right_on='Category',\n                        how = 'left')\n    df_categN.drop(['Category','SortVal',catVar], axis=1, inplace = True)\ndf_categN.drop(CatVarQual, axis=1, inplace = True)\n#df_categN.columns","5d74a5f3":"sns.pairplot(data=df_categ,\n            x_vars='Neighborhood',\n            y_vars=['SalePrice'],\n            size = 6)\nplt.xticks(rotation=45);","b75a5c36":"sns.pairplot(data=df_categN,\n            x_vars='Neighborhood_N',\n            y_vars=['SalePrice'],\n            size = 6);","b8950226":"print(\"Before category encoding:\")\ndf_n = df_categ[df_categ['IsTrain'] == 1]\nprint(spearmanr(df_n['SalePrice'],df_n['Neighborhood']))\n\nprint(\"After category encoding:\")\ndf_n = df_categN[df_categN['IsTrain'] == 1]\nprint(spearmanr(df_n['SalePrice'],df_n['Neighborhood_N']))\nprint(\"Pearson corr = \"+ str(df_n.corr(method='pearson')['SalePrice']['Neighborhood_N']))","15ad95c2":"df_categN = pd.merge(df_categN, df_categQ, left_index=True, right_index=True, sort=False)\n#df_categN.columns","89cf7b8b":"df = pd.merge(df_categN[df_categN.columns[2:]], df_num, left_index=True, right_index=True, sort=False)","fb69b88d":"df_num['SalePrice'].describe()","5d553748":"#skewness and kurtosis\nprint(\"Skewness: %f\" % df['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % df['SalePrice'].kurt())","32c07fb6":"sns.distplot(df[df['SalePrice'].isnull() == False]['SalePrice'],fit=norm);","77cb2b20":"qqplot(df[df['SalePrice'].isnull() == False]['SalePrice'], line='s');","c60e5137":"len_c = 4   #(len(df_num.columns)-2)\nfig, axes = plt.subplots(round(len_c \/ 2), 2, figsize=(12, 10))     # fig height = 70 -> in figsize(width,height)\nfor i, ax in enumerate(fig.axes):\n    if (i < len_c):\n        sns.distplot(df_num[df_num['IsTrain']==1][df_num.columns[i]], label=\"IsTrain = 1\", fit=norm, ax=ax)\n\nfig.tight_layout()","225d9bb9":"# Scatterplots SalePrice vs. numeric Vars\nsns.pairplot(data=df_num,\n            x_vars=df_num.columns[:4],\n            y_vars=['SalePrice']);","6037ac02":"df_corr = df.corr(method='pearson')['SalePrice'][:-2]   \ngolden_features_list = df_corr[abs(df_corr) > 0.5].sort_values(ascending=False)\nprint(\"There is {} correlated values with SalePrice:\\n{}\".format(len(golden_features_list), golden_features_list))\n#df[golden_features_list.index].head()","7ad4fa1d":"df.insert(loc=0, column='TotArea', value=(df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']) )","03d42179":"df.drop(['TotalBsmtSF','1stFlrSF','2ndFlrSF','GrLivArea'], axis=1, inplace = True)","55c4a41b":"df_corr = df.corr(method='pearson')['SalePrice'][:-2]  \ngolden_features_list = df_corr[abs(df_corr) > 0.3].sort_values(ascending=False)\nTop_features_list = df_corr[abs(df_corr) > 0.5].sort_values(ascending=False)\n#print(\"There is {} correlated values with SalePrice:\\n{}\".format(len(golden_features_list), golden_features_list))","0e94a941":"#correlation matrix heatmap\ncorrmat = df[Top_features_list.index].corr()\nf, ax = plt.subplots(figsize=(10, 7))\nsns.heatmap(corrmat, cmap=\"RdBu\", vmin=-1, vmax=1, square=True, annot=True, fmt=\".1f\");","57ede0fd":"# Separating out the features (and Training sample from Testing)\nX = df.loc[:1459, golden_features_list.index].values\n\n# Separating out the target\ny = df.iloc[:1460,-2].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)","b64e86be":"sc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\nsc_Y = StandardScaler()\ny_trainS = sc_Y.fit_transform(y_train.reshape(-1,1))\ny_testS = sc_Y.transform(y_test.reshape(-1,1))","0ca5c8b2":"print(\"mean = \" + str(np.mean(X_train[:,4])))\nprint(\"std = \" + str(np.std(X_train[:,4])))","86420b74":"pca = PCA(n_components = 5)\nprincipalComponents = pca.fit_transform(X_train)\nprincipalComponentsTest = pca.transform(X_test)\nprincipalDf = pd.DataFrame(data = principalComponents\n             , columns = ['PrincComp_1', 'PrincComp_2','PrincComp_3','PrincComp_4','PrincComp_5'])\nprincipalDftest = pd.DataFrame(data = principalComponentsTest\n             , columns = ['PrincComp_1', 'PrincComp_2','PrincComp_3','PrincComp_4','PrincComp_5'])","ccf3216c":"print('Variance explained by all components: ' + str(pca.explained_variance_ratio_.sum()))\npca.explained_variance_ratio_","a40b3110":"compareDf = pd.concat((principalDf, pd.DataFrame(X_train, columns = golden_features_list.index)), axis=1)","f43f3b69":"corrmat = compareDf.corr()['PrincComp_1':'PrincComp_5']\nf, ax = plt.subplots(figsize=(20, 5))\nsns.heatmap(corrmat, cmap=\"RdBu\", vmin=-1, vmax=1, square=False, annot=True, fmt=\".1f\");","cb243997":"principalDf['SalePrice'] = y_trainS","802fbe8b":"mod = sm.quantreg('SalePrice ~ PrincComp_1 + PrincComp_2 + PrincComp_3 + PrincComp_4 + PrincComp_5', principalDf)\nres = mod.fit(q=.5)\nprint(res.summary())","f82f51e9":"pred = res.predict(principalDftest) # make the predictions by the model\ny_pred = sc_Y.inverse_transform(pred)","54d21b8c":"# Plot the y_test and the prediction (y_pred)\nfig = plt.figure(figsize=(15, 5))\nplt.plot(np.arange(0,len(y_test),1), y_test, 'b.', markersize=10, label='Actual')\nplt.plot(np.arange(0,len(y_test),1), y_pred, 'r-', label='Prediction', alpha =0.5)\nplt.xlabel('Obs')\nplt.ylabel('SalePrice')\n#plt.ylim(-10, 20)\nplt.legend(loc='upper right');","a00d4c09":"DFyy = pd.DataFrame({'y_test':y_test,'y_pred': y_pred})\nDFyy.sort_values(by=['y_test'],inplace=True)\nplt.plot(np.arange(0,len(DFyy),1), DFyy['y_pred'])\nplt.plot(np.arange(0,len(DFyy),1), DFyy['y_test'], alpha=0.5)\n#plt.ylim(0,500000)\nplt.ylabel('Red= y_test,  Blue = y_pred')\nplt.xlabel('Index ')\nplt.title('Predicted vs. Real');\nprint('Observations sorted by y_test values, i.e. higher index => higher SalePrice value');","aa540ffc":"plt.plot(np.arange(0,len(DFyy),1), DFyy['y_pred']\/DFyy['y_test'])\nplt.ylabel('Ratio = pred\/real')\nplt.xlabel('Index')\nplt.title('Ratio of Predicted vs. Real (1=excellent Prediction)');\nprint('Observations sorted by y_test values, i.e. higher index => higher SalePrice value');","4ddb71f2":"plt.scatter(y_test, y_pred)\nplt.ylim(-1, 500000)\nplt.xlim(-1, 500000)\nplt.plot(y_test, y_test, \"r\")\nplt.xlabel('y_actual')\nplt.ylabel('y_predicted');","a34a6269":"plt.scatter(np.arange(0,len(DFyy),1), (DFyy['y_test'] - DFyy['y_pred'])\/DFyy['y_test'] )\nplt.ylim(-0.75,0.75)\nplt.ylabel('Relative Error = (real - pred)\/real')\nplt.xlabel('Index')\nplt.title('Relative Error in Testing sample');\nprint('Observations sorted by y_test values, i.e. higher index => higher SalePrice value');","4c357bb7":"# A function to calculate Root Mean Squared Logarithmic Error (RMSLE)\ndef rmsle(real, predicted):\n    sum=0.0\n    for x in range(len(predicted)):\n        if predicted[x]<0 or real[x]<0: #check for negative\n            print('Warning:1 negative value skipped')\n            continue\n        p = np.log(predicted[x]+1)\n        r = np.log(real[x]+1)\n        sum = sum + (p - r)**2\n    return (sum\/len(predicted))**0.5","3ea8def6":"print('Prediction accuracy on Testing Sample:')\nprint('RMSLE       = %f' % (rmsle(y_test, y_pred)))\nprint('r-squared   = %f' % (r2_score(y_test, y_pred)))","74e50447":"pred_train = res.predict(principalDf)\ny_pred_train = sc_Y.inverse_transform(pred_train)","3b6ad6f5":"print('Model accuracy on Training Sample:')\nprint('RMSLE       = %f' % (rmsle(y_train, y_pred_train)))\nprint('r-squared   = %f' % (r2_score(y_train, y_pred_train)))","09a78737":"skf = KFold(n_splits=3, shuffle=True, random_state=123)\n\ni = 1\nIntercept_CV   = list()\nPrincComp_1_CV = list()\nPrincComp_2_CV = list()\nPrincComp_3_CV = list()\nPrincComp_4_CV = list()\nPrincComp_5_CV = list()\n\nfor train_index, test_index in skf.split(X,y):\n    #------ Standardize -------\n    sc_X = StandardScaler()\n    X_train = sc_X.fit_transform(X[train_index])\n    X_test = sc_X.transform(X[test_index])\n    sc_Y = StandardScaler()\n    y_trainS = sc_Y.fit_transform(y[train_index].reshape(-1,1))\n    y_test = y[test_index]\n    #----- PCA ----------------\n    pca = PCA(n_components = 5)\n    principalComponents = pca.fit_transform(X_train)\n    principalComponentsTest = pca.transform(X_test)\n    \n    #---- Applying the Standard scaler so the parameters are easier to compare\n    ScPCA = StandardScaler()\n    principalComponents = ScPCA.fit_transform(principalComponents)\n    principalComponentsTest = ScPCA.transform(principalComponentsTest)\n    principalDf = pd.DataFrame(data = principalComponents\n             , columns = ['PrincComp_1', 'PrincComp_2','PrincComp_3','PrincComp_4','PrincComp_5'])\n    principalDftest = pd.DataFrame(data = principalComponentsTest\n             , columns = ['PrincComp_1', 'PrincComp_2','PrincComp_3','PrincComp_4','PrincComp_5'])\n    principalDf['SalePrice'] = y_trainS\n    \n    #principalDf = StandardScaler().fit_transform(principalDf)\n    \n    #----- QUANTILE REGRESSION ----------\n    mod = sm.quantreg('SalePrice ~ PrincComp_1 + PrincComp_2 + PrincComp_3 + PrincComp_4 + PrincComp_5', principalDf)\n    res = mod.fit(q=.5)\n    pred = res.predict(principalDftest) # make the predictions by the model\n    y_pred = sc_Y.inverse_transform(pred)\n    print('Split No.: ' + str(i))\n    print('RMSLE       = %f' % (rmsle(y_test, y_pred)))\n    print('r-squared   = %f' % (r2_score(y_test, y_pred)))\n    \n    Intercept_CV.append(res.params['Intercept'])\n    PrincComp_1_CV.append(res.params['PrincComp_1'])\n    PrincComp_2_CV.append(res.params['PrincComp_2'])\n    PrincComp_3_CV.append(res.params['PrincComp_3'])\n    PrincComp_4_CV.append(res.params['PrincComp_4'])\n    PrincComp_5_CV.append(res.params['PrincComp_5'])\n    i=i+1\n\n#--- Model Stability Check: Mean and standard deviation of each regression parameter\nprint('Intercept mean = ' + str(np.mean(Intercept_CV)) + ' std = ' + str(np.std(Intercept_CV)))\nprint('PrincComp_1 mean = ' + str(np.mean(PrincComp_1_CV)) + ' std = ' + str(np.std(PrincComp_1_CV)))\nprint('PrincComp_2 mean = ' + str(np.mean(PrincComp_2_CV)) + ' std = ' + str(np.std(PrincComp_2_CV)))\nprint('PrincComp_3 mean = ' + str(np.mean(PrincComp_3_CV)) + ' std = ' + str(np.std(PrincComp_3_CV)))\nprint('PrincComp_4 mean = ' + str(np.mean(PrincComp_4_CV)) + ' std = ' + str(np.std(PrincComp_4_CV)))\nprint('PrincComp_5 mean = ' + str(np.mean(PrincComp_5_CV)) + ' std = ' + str(np.std(PrincComp_5_CV)))","318bdfc9":"# Separating out the features (and Training sample from Testing)\nX = df.iloc[:1460, :-2].values\n\n# Separating out the target\ny = df.iloc[:1460,-2].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)","601b4e1a":"sc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)","d4ba44bf":"GBRmedian = GradientBoostingRegressor(loss='quantile', alpha=0.5,\n                                n_estimators=250, max_depth=5,\n                                learning_rate=.1, min_samples_leaf=10,\n                                min_samples_split=20)\nGBRmedian.fit(X_train, y_train);","b70e40ce":"# Make the prediction on the Testing sample\ny_pred = GBRmedian.predict(X_test)\ny_pred_train = GBRmedian.predict(X_train)","680da8e8":"print('Model Accuracy on Training sample:')\nprint('RMSLE       = %f' % (rmsle(y_train, y_pred_train)))\nprint('r-squared   = %f' % (r2_score(y_train, y_pred_train)))","56c75151":"print('Accuracy of prediction on Testing sample:')\nprint('RMSLE       = %f' % (rmsle(y_test, y_pred)))\nprint('r-squared   = %f' % (r2_score(y_test, y_pred)))","aa88fa13":"# Plot feature importance\nfeature_importance = GBRmedian.feature_importances_\n# make importances relative to max importance\nfeature_importance = 100.0 * (feature_importance \/ feature_importance.max())\nsorted_idx = np.argsort(feature_importance)\n# Let's plot to top10 most important variables\nsorted_idx = sorted_idx[-10:]\npos = np.arange(sorted_idx.shape[0]) + .5\nplt.barh(pos, feature_importance[sorted_idx], align='center')\nplt.yticks(pos, df.columns[sorted_idx])\nplt.xlabel('Relative Importance')\nplt.title('Variable Importance');","d6870047":"# Plot the y_test and the prediction (y_pred)\nfig = plt.figure(figsize=(15, 5))\nplt.plot(np.arange(0,len(y_test),1), y_test, 'b.', markersize=10, label='Actual')\nplt.plot(np.arange(0,len(y_test),1), y_pred, 'r-', label='Prediction', alpha = 0.5)\nplt.xlabel('Obs')\nplt.ylabel('SalePrice')\n#plt.ylim(-10, 20)\nplt.legend(loc='upper right');","57022de2":"DFyy = pd.DataFrame({'y_test':y_test,'y_pred': y_pred})\nDFyy.sort_values(by=['y_test'],inplace=True)\nplt.plot(np.arange(0,len(DFyy),1), DFyy['y_pred'])\nplt.plot(np.arange(0,len(DFyy),1), DFyy['y_test'], alpha=0.5)\n#plt.ylim(0,500000)\nplt.ylabel('Red= y_test,  Blue = y_pred')\nplt.xlabel('Index ')\nplt.title('Predicted vs. Real');\nprint('Observations sorted by y_test values, i.e. higher index => higher SalePrice value');","2cd4b37d":"plt.plot(np.arange(0,len(DFyy),1), DFyy['y_pred']\/DFyy['y_test'])\nplt.ylabel('Ratio = pred\/real')\nplt.xlabel('Index')\nplt.ylim(0,2)\nplt.title('Ratio Predicted vs. Real (1=excellent Prediction)');\nprint('Observations sorted by y_test values, i.e. higher index => higher SalePrice value');","45e7ad63":"plt.scatter(y_test, y_pred)\nplt.ylim(-1, 500000)\nplt.xlim(-1, 500000)\nplt.plot(y_test, y_test, \"r\")\nplt.xlabel('y_actual')\nplt.ylabel('y_predicted');","a3d31866":"plt.scatter(np.arange(0,len(DFyy),1), (DFyy['y_test'] - DFyy['y_pred'])\/DFyy['y_test'] )\nplt.ylim(-0.75,0.75)\nplt.ylabel('Relative Error = (real - pred)\/real')\nplt.xlabel('Index')\nplt.title('Ratio Predicted vs. Real (1=perfectPrediction)');\nprint('Observations sorted by y_test values, i.e. higher index => higher SalePrice value');","c0403a6c":"skf = KFold(n_splits=3, shuffle=True, random_state=123)\n\nfor train_index, test_index in skf.split(X,y):\n    #------ Standardize -------\n    sc_X = StandardScaler()\n    X_train = sc_X.fit_transform(X[train_index])\n    X_test = sc_X.transform(X[test_index])\n    y_train = y[train_index]\n    y_test = y[test_index]\n    \n    #----- Gradient Boosted Regression ----------\n    GBRmedian = GradientBoostingRegressor(loss='quantile', alpha=0.5,\n                                    n_estimators=250, max_depth=5,\n                                    learning_rate=.1, min_samples_leaf=10,\n                                    min_samples_split=20)\n    GBRmedian.fit(X_train, y_train)\n    # Make the prediction on the Testing sample\n    y_pred = GBRmedian.predict(X_test)\n    y_pred_train = GBRmedian.predict(X_train)\n\n    print('Accuracy of prediction on Testing sample:')\n    print('RMSLE       = %f' % (rmsle(y_test, y_pred)))\n    print('r-squared   = %f' % (r2_score(y_test, y_pred)))\n    \n    #---------------------------\n    # Plot feature importance\n    feature_importance = GBRmedian.feature_importances_\n    # make importances relative to max importance\n    feature_importance = 100.0 * (feature_importance \/ feature_importance.max())\n    sorted_idx = np.argsort(feature_importance)\n    # Let's plot to top10 most important variables\n    sorted_idx = sorted_idx[-10:]\n    pos = np.arange(sorted_idx.shape[0]) + .5\n    plt.barh(pos, feature_importance[sorted_idx], align='center')\n    plt.yticks(pos, df.columns[sorted_idx])\n    plt.xlabel('Relative Importance')\n    plt.title('Variable Importance')\n    plt.show();\n","7f1ce22f":"Percentage of variace explained by the components together and seperately","e66c7dae":"## 2. Gradient Boosted Regression\nCompare the outputs of quantile regression accuracy to Gradient boosted regression outputs","2a63a206":"Checking if there are missing values in features with numeric-categories","d1ea5b2f":"Will impute 0 for missings in **numeric** features, since there should be no confusion around the meaning of it","a7f3fe18":"> Can see that the dependent variable will need a transformation to fulfill normality assumption, if linear regression is applied. Unlike the quantile regression, which is insensitive to monotone transformations of target variable. ","cf107f29":"## 1. Quantile Regression","212f1bb8":"Create a subset of categorical features (i.e. with less than 30 categories)","e700c253":"Define RMSLE metrics for model evaluation and comparison","d5574399":"Let's move the numeric features that don't need to be encoded to *df_num* dataframe  \n(they fell-in the categoric feature pot,  because of small number of values)","58af8ef5":"Boxplot gives a good idea about how SalePrice is distributed across the categories.  \nDistribution charts show how well each category is populated.","b7cb401b":"#### Identifying categorical features by number of unique values","7f442962":"Let's join the 2 datasets of encoded variables","eebb17ba":"### Imputing Missing Values","12e4e0ae":"### Categoric Feature Analysis","32a4a630":"# Quantile Regression vs. Gradient Boosting Regression","7e85f391":"Feature standardization is required for Principal component analysis (PCA)","ec88fb77":"Transforming **categorical features** into numeric, based on the order of median *SalePrice* per category (and actual median value, giving the score to each category)","da535e99":"### Conclusion\nGradient Boosted Regression gives better accuracy compared to quantile regression, though the difference is not that big between RMSLEs (0.13 vs. 0.17). In the evaluation charts can see and compare how well the models predict across the testing sample.\nIf we compare model stability outputs - predictor variable importance order is changing in every fold, where in quantile regression equation slope coefficients (of principal components) doesn't change much, i.e. the importance of predictors (and contribution to the model) is stable. This is one of the reasons why some business sectors still choose to use traditional methods over advanced machine learning.","66306208":"What's the shape of our target?    \nLet's check out some key stats and histogram of *SalePrice*","6dbfc412":"#### Histogram of other Numeric Features","fe89d5fc":"Let's put the potential predictors through PCA to create few independent components that could go into the model instead","98cbe934":"The Least Absolute Deviation model is a special case of quantile regression where q=0.5 (median)","46bf11b1":"### 1.1. Prep: Initial train\/test split for Visualised Model Evaluation","a8be5623":"### 1.5. Model Evaluation","0ff21e4a":"*Will leave the testing sample out of analysis for now (we're not submitting anything here)*","b94767f3":"Let's check the strength (and direction) of linear relationship between key features","e47d1d0f":"See how the residuals are distributed across testing sample","42d4ea38":"### *Numeric Feature Analysis*","51306621":"Create a subset of continuous numeric features (i.e. that have more than 30 categories)","a4676a1f":"**The QQ-plot**\ngives a nice picture of how close the sample quantiles are to the theoretical normal distribution quantiles","a39581eb":"    \n#### Will join the numeric and encoded categoric features","24132a37":"> There is strong correlation between some features. Will use PCA to gather the correlated features into components.","1a78e948":"### 1.3. PCA for feature reduction and independence","c4801a76":"### 1.2. Standardize Data","7a966425":"We assign to create as many components as would explain 95% of the variance","2d33a0a9":"### Correlation\nLet's check which features have strongest linear relationship with *SalePrice*","65359819":"See if the errors increase when SalePrice increases (e.g. maybe the model is performing better on lower-value houses rather than the expensive)","5a0221c1":"Spearman correlation to see the affect of category ranking by median *SalePrice*","e1a1aad6":"Let's see which variables mostly contribute to which component","053c9e54":"Create ***TotArea*** feature to cover all the area inside house, since all of these are highly corelated and sensibly important variables when predicting *SalePrice*","fc8b758b":"### GBR Model Stability","43917ca2":"### Categoric Feature Encoding","9abc0a3e":"Will impute 'NA' for missing values in **categoric** features (object type)","8f020bf5":"### 1.4. Quantile Regression - model fitting","b7d51651":"Encoding the Quality type categoric features","387e80c5":"> When we compare prediction error in the tails, gradient boosting seems to give better accuracy for lowest and highest value houses.","94c47119":"Let's fit the Gradient boosted regression tree based on quantile =0.5 (LAD) loss function (i.e. estimating the median value)","4c35b181":"Advanced machine learning algorithms are taking over traditional methods a lot lately.  \nBut how close traditional model prediction can get to advance ML prediction?  \n\nThis notebook gives a chance to compare predictions of Gradient Boosting regression vs. traditional regression.\nTo keep this comparison adequate we will build:\n1. Quantile Regression (median)\n2. Gradient boosted regression with least absolute deviation loss function (quantile=0.5)\n\nSo in both cases we are predicting median house price, hense the loss functions used in both approaches is the same (LAD).\n\n#### Data and the Challenge\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this Kaggle competition challenges you to predict the final price of each home. More details about the competition & data can be found [here](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/data)\n\n","bf4a1645":"Let's drop ID into a separate dataframe for later","8e5796c6":"## Exploratory Data Analysis","e0444872":"#### Read in Training Data","c3a15fbb":"Inverse the transformation (since we applied the transformation prior to analysis)","937c0b8b":">There are features that have low coverage, keep in mind when evaluating its performance in the model outputs ","56450a64":"#### Importing the Libraries","b2b4978f":"Will split the original training sample into train\/test for model evaluation (70% training\/ 30% testing)","648fecc0":"### References\n\n- Great tips on Exploratory Data Analysis by [Pedro Marcelino](https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python) and [Tuatini Godard](https:\/\/www.kaggle.com\/ekami66\/detailed-exploratory-data-analysis-with-python), Categorical Data Analysis by [Samarth Agrawal](https:\/\/www.kaggle.com\/nextbigwhat\/eda-for-categorical-variables-a-beginner-s-way) \n- Principal Component Analysis https:\/\/towardsdatascience.com\/pca-using-python-scikit-learn-e653f8989e60  \n- Quantile Regression http:\/\/www.statsmodels.org\/dev\/generated\/statsmodels.regression.quantile_regression.QuantReg.html  ","e9924636":"See how the relative residuals are distributed across testing sample","09c0c325":"List the other categoric features that need to be encoded","7e5f1cea":"> Will do part of encoding using 'ranking' based on median Sale price at later step","179c95ef":"### 1.6. Model Stability check - using kFolds"}}