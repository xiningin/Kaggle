{"cell_type":{"732e6d06":"code","8e718df0":"code","478418ff":"code","6e2bfd1f":"code","8add162c":"code","bee8871b":"code","324e567c":"code","30c94b9e":"code","19870b7c":"code","dcf9da7a":"code","104ba628":"code","aa17f822":"code","e2506353":"code","22c80de4":"code","67877071":"code","cb997710":"code","c66b9ac6":"code","758278b9":"code","f5bdccfb":"code","d7f5ffed":"code","79039b8e":"code","f2246463":"code","f5d62209":"markdown","be1085c3":"markdown","e7a47768":"markdown","7df76fa8":"markdown","03c98bb4":"markdown","f8a38b5b":"markdown","2a09498b":"markdown","8b21cd73":"markdown","2ae08573":"markdown","080ece2a":"markdown","9fa0796c":"markdown","cebf867e":"markdown","0c7f5a2b":"markdown","26c428c8":"markdown","596f1fb6":"markdown","6101d9bc":"markdown","84cfbc70":"markdown"},"source":{"732e6d06":"\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport re\nimport sklearn\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.experimental import enable_hist_gradient_boosting\n# Going to use these  base models for the stacking\nfrom sklearn.ensemble import (  BaggingClassifier, \n                              ExtraTreesClassifier,  HistGradientBoostingClassifier)\n\nfrom tqdm import tqdm\nfrom mlxtend.classifier import StackingCVClassifier\nfrom lightgbm import LGBMClassifier\nfrom itertools import combinations, chain\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import make_scorer, accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","8e718df0":"train = pd.read_csv('..\/input\/learn-together\/train.csv')\ntest = pd.read_csv('..\/input\/learn-together\/test.csv')\nID = test['Id'] # save this for later ","478418ff":"train.head()","6e2bfd1f":"train.columns","8add162c":"train.dtypes.unique()","bee8871b":"#equal amounts of each variable in the train set\nsns.distplot(train.Cover_Type, kde = False)","324e567c":"fig, axs = plt.subplots(ncols = 2,figsize=(15,5))\n\nsns.boxplot(train.Cover_Type,train.Elevation, whis = 4, hue = train.Cover_Type, ax=axs[0])\nsns.boxplot(train.Cover_Type,train.Slope, whis = 4, hue = train.Cover_Type, ax=axs[1])\n","30c94b9e":"colormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=False, cmap=colormap, linecolor='white', annot=False)","19870b7c":"colormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(test.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=False, cmap=colormap, linecolor='white', annot=False)","dcf9da7a":"colormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=5)\nsns.heatmap(train.corr()[['Cover_Type']].sort_values(by =['Cover_Type'], ascending=False),vmin=-1, \n            square=False, cmap=colormap, linecolor='white', annot=False)","104ba628":"target = train.Cover_Type\ntrain = train.drop(['Cover_Type'], axis = 1)","aa17f822":"\n\ntrain = train.drop(['Soil_Type7','Soil_Type15','Id',], axis =1)\ntest = test.drop(['Soil_Type7','Soil_Type15','Id'], axis =1)\n","e2506353":"train['EV_DTH'] = (train.Elevation - train.Vertical_Distance_To_Hydrology)\ntest['EV_DTH'] = (test.Elevation - test.Vertical_Distance_To_Hydrology)\n\n\ntrain['EH_DTH'] = (train.Elevation -  (train.Horizontal_Distance_To_Hydrology *0.2))\ntest['EH_DTH'] = (test.Elevation -  (test.Horizontal_Distance_To_Hydrology *0.2))\n\ntrain['Dis_To_Hy'] = (((train.Horizontal_Distance_To_Hydrology **2) + (train.Vertical_Distance_To_Hydrology **2))**0.5)\ntest['Dis_To_Hy'] = (((test.Horizontal_Distance_To_Hydrology **2) + (test.Vertical_Distance_To_Hydrology **2))**0.5)\n\ntrain['HyF_1'] = (train.Horizontal_Distance_To_Hydrology + train.Horizontal_Distance_To_Fire_Points)\ntest['HyF_1'] = (test.Horizontal_Distance_To_Hydrology + test.Horizontal_Distance_To_Fire_Points)\n\ntrain['HyF_2'] = (train.Horizontal_Distance_To_Hydrology - train.Horizontal_Distance_To_Fire_Points)\ntest['HyF_2'] = (test.Horizontal_Distance_To_Hydrology - test.Horizontal_Distance_To_Fire_Points)\n\ntrain['HyR_1'] = (train.Horizontal_Distance_To_Hydrology + train.Horizontal_Distance_To_Roadways)\ntest['HyR_1'] = (test.Horizontal_Distance_To_Hydrology + test.Horizontal_Distance_To_Roadways)\n\ntrain['HyR_2'] = (train.Horizontal_Distance_To_Hydrology - train.Horizontal_Distance_To_Roadways)\ntest['HyR_2'] = (test.Horizontal_Distance_To_Hydrology - test.Horizontal_Distance_To_Roadways)\n\n\ntrain['FiR_1'] = (train.Horizontal_Distance_To_Fire_Points + train.Horizontal_Distance_To_Roadways)\ntest['FiR_1'] = (test.Horizontal_Distance_To_Fire_Points + test.Horizontal_Distance_To_Roadways)\n\ntrain['FiR_1'] = (train.Horizontal_Distance_To_Fire_Points - train.Horizontal_Distance_To_Roadways)\ntest['FiR_1'] = (test.Horizontal_Distance_To_Fire_Points - test.Horizontal_Distance_To_Roadways)\n\ntrain['Avg_shade'] = ((train.Hillshade_9am + train.Hillshade_Noon + train.Hillshade_3pm) \/3)\ntest['Avg_shade'] = ((test.Hillshade_9am + test.Hillshade_Noon + test.Hillshade_3pm) \/3)\n\ntrain['Morn_noon_int'] = ((train.Hillshade_9am + train.Hillshade_Noon) \/ 2)\ntest['Morn_noon_int'] = ((test.Hillshade_9am + test.Hillshade_Noon) \/ 2)\n\ntrain['noon_eve_int'] = ((train.Hillshade_3pm + train.Hillshade_Noon) \/ 2)\ntest['noon_eve_int'] = ((test.Hillshade_3pm + test.Hillshade_Noon) \/ 2)\n\ntrain['Slope2'] = np.sqrt(train.Horizontal_Distance_To_Hydrology**2 + train.Vertical_Distance_To_Hydrology**2)\ntest['Slope2'] = np.sqrt(test.Horizontal_Distance_To_Hydrology**2 + test.Vertical_Distance_To_Hydrology**2)\n","22c80de4":"\n#Specify the model and parameters and then use the training data to fit the model\ngm = GaussianMixture(n_components  = 15)\ngm.fit(train)","67877071":"train['g_mixture'] = gm.predict(train)\ntest['g_mixture'] = gm.predict(test)","cb997710":"\ny = target\nX = train\nx_train, x_test, y_train, y_test = train_test_split(X,y, random_state=1)","c66b9ac6":"clf_lgbm = LGBMClassifier(n_estimators=400,num_leaves=100,verbosity=0)\nclf_knc = KNeighborsClassifier(n_jobs = -1, n_neighbors =1)\nclf_etc = ExtraTreesClassifier(random_state = 1, n_estimators = 900, max_depth =50,max_features = 30)\nclf_hbc = HistGradientBoostingClassifier(random_state = 1, max_iter = 500, max_depth =25)","758278b9":"clf_knc.fit(x_train,y_train)\ntesting_predictions = clf_knc.predict(x_test)\n\nprint(round(accuracy_score(y_test, testing_predictions),4))","f5bdccfb":"ensemble = [\n            ('clf_knc', clf_knc),\n            ('clf_hbc', clf_hbc),\n            ('clf_etc', clf_etc),\n            ('clf_lgbm', clf_lgbm)\n           \n           ]\n\nstack = StackingCVClassifier(classifiers=[clf for label, clf in ensemble],\n                             meta_classifier=clf_lgbm,\n                             cv=3,\n                             use_probas=True, \n                             use_features_in_secondary=True,\n                             verbose=-1,\n                             n_jobs=-1)\n\nstack = stack.fit(X,y)\npredictions = stack.predict(test)","d7f5ffed":"stack.predict_proba(test)","79039b8e":"pd.Series(predictions).value_counts()","f2246463":"submission = pd.DataFrame({ 'Id': ID,\n                            'Cover_Type': predictions })\nsubmission.to_csv(\"submission_example.csv\", index=False)","f5d62209":"Looking at the bar charts for elevation, I suspect this will be one of the critical features. Slope does not seem to be as informative.","be1085c3":"<a id=\"section-three\"><\/a>\n### Feature engineering \n\nI did a quick google search for this dataset and found that there have been several competitions using it in the past. I found this paper (reference below) and used some of the same faetures.\n\nPruthvi H.R, Nisha K.K, Chandana T.L, Navami K and Biju R M (2015) \u2018Feature engineering on forest cover type data with ensemble of decision trees\u2019, 2015 IEEE International Advance Computing Conference (IACC). Banglore, India: IEEE, pp. 1093\u20131098. Available at: 10.1109\/IADCC.2015.7154873 (Accessed: 2 September 2019).\n\n\nAfter implementing these features my score improved by ~0.1","e7a47768":"#### Please upvote if this helped you :)\n\nI also have a kernel where I use a neural network for this classification task and a kernel where I use every single classifier I can find.\n - https:\/\/www.kaggle.com\/jakelj\/forest-cover-type-keras-ann\n - https:\/\/www.kaggle.com\/jakelj\/every-classifier-i-could-find-forest-cover-type","7df76fa8":"<a id=\"section-four\"><\/a>\n## Making the model\n\nWe are going to make a stacked model, using the models and parameters that I have found to work well offline. From my research it seems as if you want to use models that predict in different ways. \n","03c98bb4":"All the data is encoded and tranformed already! How nice of Kaggle.","f8a38b5b":"Are the results similar to the known distribution?","2a09498b":"### meta or ensemble model\n\nHere we are using three models for the 1st set of predictions and then a final model to make the meta decision. With using 'use_probas=True' the probabilities of each cover type from the previous model predictions will be used in the final model.  ","8b21cd73":"<a id=\"section-two\"><\/a>\n### Exploring the data","2ae08573":"An even distribution of each cover type in the training data, how interesting.\nNow lets have a look at some of the variables","080ece2a":"#### Define the models we are going to be using","9fa0796c":"looks like soil type 7 and soil type 15 are completely missing, so we wil have to remove them from the test data (if they are in there). I wonder why they are missing, is this a test from kaggle?","cebf867e":" - [Introduction](#section-one)\n - [Exploratory data analysis](#section-two)\n - [Feature Engineering](#section-three)\n - [Making the model](#section-four)\n \n\n\n<a id=\"section-one\"><\/a>\nThis is a work in progress and I accept all critique.\n\nWith this I intend on learning from others and sharpening my skills. Seeing as this is a dataset that is freely available, I suspect the answers are to. Therefore, I do not think that the final position is that relevant, only the methods and principles that one learns through taking part.\n\n\n \n","0c7f5a2b":"all the data is numeric and therefore there does not seem to be a need for it to be encoded. ? If we were having trouble with memory we could down cast these to lower ints.","26c428c8":"Let's have a look to see how the variables are correlated with the Cover Type","596f1fb6":"Add a new feature to the datasets from the guassian mixture. This improved my score from 0.8 t 0.82 so it is worth adding if you haven't already.","6101d9bc":"Now I will remove the missing data and the Id column as that should not give us any information. ","84cfbc70":"Very low correlations across all variables. However, Id has a high correlation, I wonder if there is some data leakage there. It shouldn't give us an information so we shall remove it."}}