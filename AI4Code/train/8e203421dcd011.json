{"cell_type":{"641a3291":"code","f96af197":"code","f24e1414":"code","7d403a42":"code","9da92e1a":"code","833da0a0":"code","7ddabe3a":"code","0a1789bd":"code","9219f58a":"code","b50342e6":"code","e5245a95":"code","eea7edc5":"code","8051c17a":"code","25f14dbb":"code","0efd58ed":"code","79b74f33":"code","c001da53":"code","72088d2e":"code","878af8dd":"code","a60761ba":"code","ca6cdd70":"code","44b5b229":"code","d44cc5fb":"code","2614cd3c":"code","bd3320dc":"code","32f6cfda":"code","e50c001a":"code","62eebdc8":"markdown","90eb66a8":"markdown"},"source":{"641a3291":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"..\/input\/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f96af197":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow_addons as tfa\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n    \nprint(tf.__version__)","f24e1414":"from kaggle_datasets import KaggleDatasets","7d403a42":"GCS_PATH1 = KaggleDatasets().get_gcs_path('comic-faces-paired-synthetic-v2')","9da92e1a":"GCS_PATH3 = KaggleDatasets().get_gcs_path('human-faces')","833da0a0":"\nGCS_PATH2 = KaggleDatasets().get_gcs_path('another-anime-face-dataset')\n","7ddabe3a":"GSC_PATH4 = KaggleDatasets().get_gcs_path('selfie2anime')","0a1789bd":"print(GCS_PATH1)\nprint(GCS_PATH3)","9219f58a":"FACE_DATA1 = tf.io.gfile.glob(str(GCS_PATH1 + '\/face2comics_v2.0.0_by_Sxela\/face2comics_v2.0.0_by_Sxela\/faces\/*.jpg'))\nprint(len(FACE_DATA1))\n# FACE_DATA2 = tf.io.gfile.glob(str(GCS_PATH3 + '\/Humans\/*.jpg'))\n# print(len(FACE_DATA2))\n# ANIME_DATA = tf.io.gfile.glob(str(GCS_PATH2 + '\/animefaces256cleaner\/*.jpg'))\n# print(len(ANIME_DATA))","b50342e6":"# ALL_FACE = FACE_DATA1 + FACE_DATA2\n# print(len(ALL_FACE))","e5245a95":"# import random\n# SAMPLE_ANIME = random.sample(ANIME_DATA,(len(ALL_FACE)))\n# print(len(SAMPLE_ANIME))","eea7edc5":"FACE_DATA_TRAIN = tf.io.gfile.glob(str(GSC_PATH4 + '\/trainA\/*.jpg'))\nprint(len(FACE_DATA_TRAIN))\nANIME_DATA_TRAIN = tf.io.gfile.glob(str(GSC_PATH4 + '\/trainB\/*.jpg'))\nprint(len(ANIME_DATA_TRAIN))\nFACE_DATA_TEST = tf.io.gfile.glob(str(GSC_PATH4 + '\/testA\/*.jpg'))\nprint(len(FACE_DATA_TEST))\nANIME_DATA_TEST = tf.io.gfile.glob(str(GSC_PATH4 + '\/testB\/*.jpg'))\nprint(len(ANIME_DATA_TEST))","8051c17a":"BATCH_SIZE = 32\nBUFFER_SIZE = 100\nIMG_HEIGHT, IMG_WIDTH = 286, 286\nCROP_SIZE = 256\nTEST_SIZE = 0.01\nCHANNELS = 3\nHEIGHT = 256\nWIDTH = 256\n\nTRANSFORMER_BLOCKS = 6\nGENERATOR_LR = 2e-4\nDISCRIMINATOR_LR = 2e-4","25f14dbb":"# from sklearn.model_selection import train_test_split\n# train_anime_path, test_anime_paths, train_faces_paths, test_faces_paths = train_test_split(SAMPLE_ANIME, ALL_FACE, test_size=TEST_SIZE, random_state=420)","0efd58ed":"def load(comic_path,face_path):\n    comic = tf.io.read_file(comic_path)\n    comic = tf.image.decode_jpeg(comic, channels=3)\n    \n\n\n    face = tf.io.read_file(face_path)\n    face = tf.image.decode_jpeg(face, channels=3)\n    \n\n    return comic ,face\n\ndef normalize(comic, face):\n    \"\"\" Normalize the images to [-1, 1]\n    \"\"\"\n    # YOUR CODE HERE\n    comic = (tf.cast(comic, tf.float32) \/255.0 *2) -1\n    face = (tf.cast(face, tf.float32) \/255.0 *2) -1\n    return comic, face\n\ndef random_crop(comic, face, height, width):\n    \"\"\" Stack the two images on top of each other\n        Then crop together.\n    \"\"\"\n    stacked_image = tf.stack([comic, face], axis=0)\n    cropped_image = tf.image.random_crop(stacked_image, \n                                         size=[2, height, width, 3])\n\n    return cropped_image[0], cropped_image[1]\n\ndef resize(comic, face, height, width):\n    \"\"\" Resize the two image to heigh, width \n    \"\"\"\n    comic = tf.image.resize(comic, \n                           [height, width],\n                           method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n    face = tf.image.resize(face, \n                               [height, width],\n                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n\n    return comic, face\n\n@tf.function()\ndef random_jitter(comic, face):\n    comic, face = resize(comic, face, IMG_HEIGHT, IMG_WIDTH)\n    comic, face = random_crop(comic, face, CROP_SIZE, CROP_SIZE)\n\n    # Augmentation to random flip\n    if tf.random.uniform(()) > 0.5: \n        comic = tf.image.flip_left_right(comic)\n        face = tf.image.flip_left_right(face)\n\n    return comic, face\ndef preprocess(comic_path, face_path):\n    comic, face = load(comic_path, face_path)\n    comic, face = normalize(comic, face)\n\n    return comic, face\ndef imgaug(comic,face):\n    comic,face = random_jitter(comic,face)\n    \n    return comic, face","79b74f33":"AUTOTUNE = tf.data.experimental.AUTOTUNE\n\ntrain_ds = tf.data.Dataset.from_tensor_slices((ANIME_DATA_TRAIN,FACE_DATA_TRAIN))\ntrain_ds = train_ds.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE).cache()\ntrain_ds = train_ds.repeat().shuffle(buffer_size=400,reshuffle_each_iteration=True).map(imgaug,num_parallel_calls=tf.data.experimental.AUTOTUNE)\ntrain_ds = train_ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n\ntest_ds = tf.data.Dataset.from_tensor_slices((ANIME_DATA_TEST, FACE_DATA_TEST))\ntest_ds = test_ds.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE).cache()\ntest_ds = test_ds.repeat().shuffle(buffer_size=50)\ntest_ds = test_ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n","c001da53":"###DownSample Block\n\nOUTPUT_CHANNELS = 3\n\ndef downsample(filters, size, apply_instancenorm=True):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n\n    result = keras.Sequential()\n    result.add(layers.Conv2D(filters, size, strides=2, padding='same',\n                             kernel_initializer=initializer, use_bias=False))\n\n    if apply_instancenorm:\n        result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))\n\n    result.add(layers.LeakyReLU())\n\n    return result\n\n###Upsample Block\n\ndef upsample(filters, size, apply_dropout=False):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n\n    result = keras.Sequential()\n    result.add(layers.Conv2DTranspose(filters, size, strides=2,\n                                      padding='same',\n                                      kernel_initializer=initializer,\n                                      use_bias=False))\n\n    result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))\n\n    if apply_dropout:\n        result.add(layers.Dropout(0.5))\n\n    result.add(layers.ReLU())\n\n    return result","72088d2e":"###PatchGAN Discriminator\n\ndef Discriminator():\n    initializer = tf.random_normal_initializer(0., 0.02)\n    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n\n    inp = layers.Input(shape=[256, 256, 3], name='input_image')\n\n    x = inp\n\n    down1 = downsample(64, 4, False)(x) # (bs, 128, 128, 64)\n    down2 = downsample(128, 4)(down1) # (bs, 64, 64, 128)\n    down3 = downsample(256, 4)(down2) # (bs, 32, 32, 256)\n\n    zero_pad1 = layers.ZeroPadding2D()(down3) # (bs, 34, 34, 256)\n    conv = layers.Conv2D(512, 4, strides=1,\n                         kernel_initializer=initializer,\n                         use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n\n    norm1 = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(conv)\n\n    leaky_relu = layers.LeakyReLU()(norm1)\n\n    zero_pad2 = layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)\n\n    last = layers.Conv2D(1, 4, strides=1,\n                         kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\n\n    return tf.keras.Model(inputs=inp, outputs=last)\ntf.keras.utils.plot_model(Discriminator(), show_shapes=True, dpi=64)","878af8dd":"# class ClassActivationMapping(mx.gluon.nn.Block):\n#     def __init__(self, units, activation, **kwargs):\n#         super(ClassActivationMapping, self).__init__(**kwargs)\n#         self._act = activation\n#         with self.name_scope():\n#             self._gap = layers.GlobalAvgPool2D()\n#             self._gap_linear = mx.gluon.nn.Conv2D(units, 1, use_bias=False)\n#             self._gmp = mx.gluon.nn.GlobalMaxPool2D()\n#             self._gmp_linear = mx.gluon.nn.Conv2D(units, 1, use_bias=False)\n#             self._out = mx.gluon.nn.Conv2D(units, 1)\n\n#     def forward(self, x):\n#         gap_y = self._gap_linear(self._gap(x))\n#         gap_m = self._gap_linear(x)\n#         gmp_y = self._gmp_linear(self._gmp(x))\n#         gmp_m = self._gmp_linear(x)\n#         return self._act(self._out(mx.nd.concat(gap_m, gmp_m, dim=1))), mx.nd.concat(gap_y, gmp_y, dim=1)\n    \n    \n    \n # Class Activation Map\ndef CAM (input_layer):\n    filters = input_layer.shape[-1]\n    x = input_layer\n    cam_x = layers.global_avg_pooling(x)\n    cam_gap_logit, cam_x_weight = layers.fully_connected_with_w(cam_x, scope='CAM_logit')\n    x_gap = tf.multiply(x, cam_x_weight)\n\n    cam_x = layers.global_max_pooling(x)\n    cam_gmp_logit, cam_x_weight = layers.fully_connected_with_w(cam_x, reuse=True, scope='CAM_logit')\n    x_gmp = tf.multiply(x, cam_x_weight)\n\n\n    cam_logit = tf.concat([cam_gap_logit, cam_gmp_logit], axis=-1)\n    x = tf.concat([x_gap, x_gmp], axis=-1)\n\n    x = layers.Conv2D(filters, size, \n             strides=strides, \n             padding='same', \n             use_bias=False, \n             kernel_initializer=conv_initializer)(x)\n\n    x = layers.ReLU(x)\n\n    heatmap = tf.squeeze(tf.reduce_sum(x, axis=-1))\n    \n    return x,cam_logit,heatmap","a60761ba":"\nconv_initializer = tf.random_normal_initializer(mean=0.0, stddev=0.02)\ngamma_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n    \ndef encoder_block(input_layer, filters, size=3, strides=2, apply_instancenorm=True, activation=layers.ReLU(), name='block_x'):\n    block = layers.Conv2D(filters, size, \n                     strides=strides, \n                     padding='same', \n                     use_bias=False, \n                     kernel_initializer=conv_initializer, \n                     name=f'encoder_{name}')(input_layer)\n\n    if apply_instancenorm:\n        block = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(block)\n        \n    block = activation(block)\n\n    return block\n\ndef transformer_block(input_layer, size=3, strides=1, name='block_x'):\n    filters = input_layer.shape[-1]\n    \n    block = layers.Conv2D(filters, size, strides=strides, padding='same', use_bias=False, \n                     kernel_initializer=conv_initializer, name=f'transformer_{name}_1')(input_layer)\n#     block = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(block)\n    block = layers.ReLU()(block)\n    \n    block = layers.Conv2D(filters, size, strides=strides, padding='same', use_bias=False, \n                     kernel_initializer=conv_initializer, name=f'transformer_{name}_2')(block)\n#     block = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(block)\n    \n    block = layers.Add()([block, input_layer])\n\n    return block\n\n\ndef decoder_block(input_layer, filters, size=3, strides=2, apply_instancenorm=True, name='block_x'):\n    block = layers.Conv2DTranspose(filters, size, \n                              strides=strides, \n                              padding='same', \n                              use_bias=False, \n                              kernel_initializer=conv_initializer, \n                              name=f'decoder_{name}')(input_layer)\n\n    if apply_instancenorm:\n        block = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(block)\n\n    block = layers.ReLU()(block)\n    \n    return block\n\n\ndef generator_fn(height=HEIGHT, width=WIDTH, channels=CHANNELS, transformer_blocks=TRANSFORMER_BLOCKS):\n    OUTPUT_CHANNELS = 3\n    inputs = layers.Input(shape=[height, width, channels], name='input_image')\n\n    # Encoder\n    enc_1 = encoder_block(inputs, 64,  7, 1, apply_instancenorm=False, activation=layers.ReLU(), name='block_1') # (bs, 256, 256, 64)\n    enc_2 = encoder_block(enc_1, 128, 3, 2, apply_instancenorm=True, activation=layers.ReLU(), name='block_2')   # (bs, 128, 128, 128)\n    enc_3 = encoder_block(enc_2, 256, 3, 2, apply_instancenorm=True, activation=layers.ReLU(), name='block_3')   # (bs, 64, 64, 256)\n    \n    # Transformer\n    x = enc_3\n    for n in range(transformer_blocks):\n        x = transformer_block(x, 3, 1, name=f'block_{n+1}') # (bs, 64, 64, 256)\n\n    # Decoder\n    x_skip = layers.Concatenate(name='enc_dec_skip_1')([x, enc_3]) # encoder - decoder skip connection\n    \n    dec_1 = decoder_block(x_skip, 128, 3, 2, apply_instancenorm=True, name='block_1') # (bs, 128, 128, 128)\n    x_skip = layers.Concatenate(name='enc_dec_skip_2')([dec_1, enc_2]) # encoder - decoder skip connection\n    \n    dec_2 = decoder_block(x_skip, 64,  3, 2, apply_instancenorm=True, name='block_2') # (bs, 256, 256, 64)\n    x_skip = layers.Concatenate(name='enc_dec_skip_3')([dec_2, enc_1]) # encoder - decoder skip connection\n\n    outputs = last = layers.Conv2D(OUTPUT_CHANNELS, 7, \n                              strides=1, padding='same', \n                              kernel_initializer=conv_initializer, \n                              use_bias=False, \n                              activation='tanh', \n                              name='decoder_output_block')(x_skip) # (bs, 256, 256, 3)\n\n    generator = keras.Model(inputs, outputs)\n    \n    return generator\n\nsample_generator = generator_fn()\n\ntf.keras.utils.plot_model(sample_generator, show_shapes=True, dpi=64)\n\n","ca6cdd70":"with strategy.scope():\n    comics_generator = generator_fn(height=None, width=None, transformer_blocks=TRANSFORMER_BLOCKS) \n    faces_generator = generator_fn(height=None, width=None, transformer_blocks=TRANSFORMER_BLOCKS)\n\n    comics_discriminator = Discriminator() # differentiates real Comics and generated Comic\n    faces_discriminator = Discriminator() # differentiates real photos and generated photos","44b5b229":"##Model Creation\n\n###Model Subclassing\n\nclass CycleGan(keras.Model):\n    def __init__(\n        self,\n        comics_generator,\n        faces_generator,\n        comics_discriminator,\n        faces_discriminator,\n        lambda_cycle=10,\n    ):\n        super(CycleGan, self).__init__()\n        self.m_gen = comics_generator\n        self.p_gen = faces_generator\n        self.m_disc = comics_discriminator\n        self.p_disc = faces_discriminator\n        self.lambda_cycle = lambda_cycle\n        \n    def compile(\n        self,\n        m_gen_optimizer,\n        p_gen_optimizer,\n        m_disc_optimizer,\n        p_disc_optimizer,\n        gen_loss_fn,\n        disc_loss_fn,\n        cycle_loss_fn,\n        identity_loss_fn\n    ):\n        super(CycleGan, self).compile()\n        self.m_gen_optimizer = m_gen_optimizer\n        self.p_gen_optimizer = p_gen_optimizer\n        self.m_disc_optimizer = m_disc_optimizer\n        self.p_disc_optimizer = p_disc_optimizer\n        self.gen_loss_fn = gen_loss_fn\n        self.disc_loss_fn = disc_loss_fn\n        self.cycle_loss_fn = cycle_loss_fn\n        self.identity_loss_fn = identity_loss_fn\n        \n    def train_step(self, batch_data):\n        real_comic, real_face = batch_data\n        \n        with tf.GradientTape(persistent=True) as tape:\n            # Face to Comic back to Face\n            fake_comic = self.m_gen(real_face, training=True)\n            cycled_face = self.p_gen(fake_comic, training=True)\n\n            # Comic to Face back to Comic\n            fake_face = self.p_gen(real_comic, training=True)\n            cycled_comic = self.m_gen(fake_face, training=True)\n\n            # generating itself\n            same_comic = self.m_gen(real_comic, training=True)\n            same_face = self.p_gen(real_face, training=True)\n\n            # discriminator used to check, inputing real images\n            disc_real_comic = self.m_disc(real_comic, training=True)\n            disc_real_face = self.p_disc(real_face, training=True)\n\n            # discriminator used to check, inputing fake images\n            disc_fake_comic = self.m_disc(fake_comic, training=True)\n            disc_fake_face = self.p_disc(fake_face, training=True)\n\n            # evaluates generator loss\n            comic_gen_loss = self.gen_loss_fn(disc_fake_comic)\n            face_gen_loss = self.gen_loss_fn(disc_fake_face)\n\n            # evaluates total cycle consistency loss\n            total_cycle_loss = self.cycle_loss_fn(real_comic, cycled_comic, self.lambda_cycle) + self.cycle_loss_fn(real_face, cycled_face, self.lambda_cycle)\n\n            # evaluates total generator loss\n            total_comic_gen_loss = comic_gen_loss + total_cycle_loss + self.identity_loss_fn(real_comic, same_comic, self.lambda_cycle)\n            total_face_gen_loss = face_gen_loss + total_cycle_loss + self.identity_loss_fn(real_face, same_face, self.lambda_cycle)\n\n            # evaluates discriminator loss\n            comic_disc_loss = self.disc_loss_fn(disc_real_comic, disc_fake_comic)\n            face_disc_loss = self.disc_loss_fn(disc_real_face, disc_fake_face)\n\n        # Calculate the gradients for generator and discriminator\n        comic_generator_gradients = tape.gradient(total_comic_gen_loss,\n                                                  self.m_gen.trainable_variables)\n        face_generator_gradients = tape.gradient(total_face_gen_loss,\n                                                  self.p_gen.trainable_variables)\n\n        comic_discriminator_gradients = tape.gradient(comic_disc_loss,\n                                                      self.m_disc.trainable_variables)\n        face_discriminator_gradients = tape.gradient(face_disc_loss,\n                                                      self.p_disc.trainable_variables)\n\n        # Apply the gradients to the optimizer\n        self.m_gen_optimizer.apply_gradients(zip(comic_generator_gradients,\n                                                 self.m_gen.trainable_variables))\n\n        self.p_gen_optimizer.apply_gradients(zip(face_generator_gradients,\n                                                 self.p_gen.trainable_variables))\n\n        self.m_disc_optimizer.apply_gradients(zip(comic_discriminator_gradients,\n                                                  self.m_disc.trainable_variables))\n\n        self.p_disc_optimizer.apply_gradients(zip(face_discriminator_gradients,\n                                                  self.p_disc.trainable_variables))\n        \n        return {\n            \"comic_gen_loss\": total_comic_gen_loss,\n            \"face_gen_loss\": total_face_gen_loss,\n            \"comic_disc_loss\": comic_disc_loss,\n            \"face_disc_loss\": face_disc_loss\n        }\n\n####Discriminator Loss\n\nwith strategy.scope():\n    def discriminator_loss(real, generated):\n        real_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(real), real)\n\n        generated_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.zeros_like(generated), generated)\n\n        total_disc_loss = real_loss + generated_loss\n\n        return total_disc_loss * 0.5\n\n####Generator Loss\n\nwith strategy.scope():\n    def generator_loss(generated):\n        return tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(generated), generated)\n\n####Cycle Loss\n\nwith strategy.scope():\n    def calc_cycle_loss(real_image, cycled_image, LAMBDA):\n        loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n\n        return LAMBDA * loss1\n\n#### Identity Loss\n\nwith strategy.scope():\n    def identity_loss(real_image, same_image, LAMBDA):\n        loss = tf.reduce_mean(tf.abs(real_image - same_image))\n        return LAMBDA * 0.5 * loss\n\n#### Optimizer\n\nwith strategy.scope():\n    comic_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n    face_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n\n    comic_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n    face_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)","d44cc5fb":"with strategy.scope():\n    cycle_gan_model = CycleGan(\n        comics_generator, faces_generator, comics_discriminator, faces_discriminator\n    )\n\n    cycle_gan_model.compile(\n        m_gen_optimizer = comic_generator_optimizer,\n        p_gen_optimizer = face_generator_optimizer,\n        m_disc_optimizer = comic_discriminator_optimizer,\n        p_disc_optimizer = face_discriminator_optimizer,\n        gen_loss_fn = generator_loss,\n        disc_loss_fn = discriminator_loss,\n        cycle_loss_fn = calc_cycle_loss,\n        identity_loss_fn = identity_loss\n    )\n","2614cd3c":"STEP=len(ANIME_DATA_TRAIN)\/\/BATCH_SIZE\nprint(STEP)","bd3320dc":"def generate_images(model, test_input):\n  prediction = model(test_input, training=False)\n  plt.figure(figsize=(20,20))\n\n  display_list = [test_input[0], prediction[0]]\n\n  title = ['Input Image', 'Predicted Image']\n  for i in range(2):\n    plt.subplot(1, 2, i+1)\n    plt.title(title[i])\n    # getting the pixel values between [0, 1] to plot it.\n    plt.imshow(display_list[i] * 0.5 + 0.5)\n    plt.axis('off')\n\n  plt.show()","32f6cfda":"from tensorflow.keras.callbacks import Callback\nclass GANMonitor(Callback):\n    def __init__(self, num_img=1,save_every_epoch=4,watch_every_epoch=2,save_dir='.\/'):\n        self.num_img = num_img\n        self.save_every_epoch = save_every_epoch\n        self.watch_every_epoch = watch_every_epoch\n        self.save_dir = save_dir\n\n    def on_epoch_end(self, epoch, logs=None):\n        # Generate Image every second epoch\n        if epoch % self.watch_every_epoch == 0:\n            for example_input, example_target in test_ds.take(self.num_img):\n                generate_images(comics_generator, example_target)\n            for example_input, example_target in test_ds.take(self.num_img):\n                generate_images(faces_generator, example_input)\n        if (epoch > 0) & (epoch % self.save_every_epoch == 0):\n            #Save Generator\n            comics_generator.save(f\"{self.save_dir}\/ComicGenTrain_{epoch}.h5\",save_format='h5',overwrite=True)\n            faces_generator.save(f\"{self.save_dir}\/FaceGenTrain_{epoch}.h5\",save_format='h5',overwrite=True)\n    \n            ","e50c001a":"history = cycle_gan_model.fit(\n        train_ds,\n        epochs=101,verbose=True,callbacks=[GANMonitor(save_every_epoch=10,watch_every_epoch=5)],steps_per_epoch=STEP,initial_epoch=0\n    )\n    ","62eebdc8":"# Start Preparing Data","90eb66a8":"# Model Building"}}