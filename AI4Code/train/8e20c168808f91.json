{"cell_type":{"3224c490":"code","7c2a3609":"code","76d6b72e":"code","c313452b":"markdown","1fa15a0a":"markdown","88b9b961":"markdown"},"source":{"3224c490":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom transformers import Trainer, TrainingArguments, AutoTokenizer,\\\n                         AutoModelForSequenceClassification","7c2a3609":"# This is simply two pandas.read_csv calls\ndef load_dfs():\n    \"\"\"Loads train and test CSVs and keeps only relevant columns\n    \n    Returns two DataFrames\n        Train with columns ['text', 'label'] \n        Test with columns ['id, 'text']\n    \"\"\"\n    train_csv = '..\/input\/commonlitreadabilityprize\/train.csv'\n    test_csv = '..\/input\/commonlitreadabilityprize\/test.csv'\n    df_train = pd.read_csv(train_csv)[[\"excerpt\", \"target\"]]\\\n                 .rename(columns={\"target\": \"label\", \"excerpt\": \"text\"})\n    df_test = pd.read_csv(test_csv)[[\"id\", \"excerpt\"]]\\\n                .rename(columns={ \"excerpt\": \"text\"})\n    return df_train, df_test\n\n# Simple RMSE calculation\ndef rmse(y_true, y_pred):\n    return np.sqrt(((y_true - y_pred) ** 2).mean().item())\n    \n    \n# Compute metrics is an argument of the Trainer\ndef compute_metrics(pred_results):\n    \"\"\"For computing RMSE inside the training loop\"\"\"\n    y_pred = pred_results.predictions.squeeze()\n    y_true = pred_results.label_ids\n    return {\"rmse\": rmse(y_true, y_pred)}\n\n\ndef submit(trainer, ds_test):\n    \"\"\"Generates predictions with the Trainer for the test dataset and writes them to disk\"\"\"\n    sample_sub_csv = '..\/input\/commonlitreadabilityprize\/sample_submission.csv'\n    pred_csv = '\/kaggle\/working\/submission.csv'\n\n    pred_results = trainer.predict(ds_test)\n    y_pred = pred_results.predictions.squeeze()\n    df_res = pd.read_csv(sample_sub_csv)\n    df_res['target'] = y_pred.tolist()\n    df_res.to_csv(pred_csv, index=False)\n\n\ndef tokenize(tokenizer, df_train, df_val, df_test):\n    \"\"\" Applies the tokenizer to the text columns of the 3 dataframes \"\"\"\n    # We apply padding=\"max_length\" and truncation=True because \n    # the quick guide showed it this way\n    # The max_length=512 was more difficult: the pretrained models \n    # from the modelhub (\"bert-base-cased\") don't require this parameter\n    # We had to find it by hand for BERT and set it here\n    train_tokenized = tokenizer(df_train['text'].tolist(), padding=\"max_length\", truncation=True, max_length=512)\n    val_tokenized = tokenizer(df_val['text'].tolist(), padding=\"max_length\", truncation=True, max_length=512)\n    test_tokenized = tokenizer(df_test['text'].tolist(), padding=\"max_length\", truncation=True, max_length=512)\n    \n    train_tokenized['label'] = df_train['label'].tolist()\n    val_tokenized['label'] = df_val['label'].tolist()\n    \n    # This is currently the only way I found to reverse a dictionary of lists \n    # into a list of dictionaries\n    # Probably something more informed will achieve the same with less code and goes and forths\n    ds_train = [dict(zip(train_tokenized,t)) for t in zip(*train_tokenized.values())]\n    ds_val = [dict(zip(val_tokenized,t)) for t in zip(*val_tokenized.values())]\n    ds_test = [dict(zip(test_tokenized,t)) for t in zip(*test_tokenized.values())]\n    return ds_train, ds_val, ds_test","76d6b72e":"# Load the tokenizer and the model\n\nMODEL_NAME = \"..\/input\/huggingface-bert\/bert-base-cased\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=1)\n\n\n# Create tokenized train, validation and test (aka prediction) datasets\ndf_base, df_test = load_dfs()\ndf_train, df_val = train_test_split(df_base, test_size=0.066)\nds_train, ds_val, ds_test = tokenize(tokenizer, df_train, df_val, df_test)\n\n# Prepare the trainer\n# The report_to=\"none\" was required to disable something called \"wandb\"\n# Later we will research what is wandb and set it back\n# Set epochs to 3\nargs = TrainingArguments(\"\/kaggle\/working\/model\/\", num_train_epochs=3, \n                         evaluation_strategy=\"steps\", eval_steps=100, report_to=\"none\")\n\ntrainer = Trainer(model=model, args=args, train_dataset=ds_train, eval_dataset=ds_val, \n                  compute_metrics=compute_metrics)\n\n# Train the model\ntrainer.train()\n\n# Create the submission\nsubmit(trainer, ds_test)","c313452b":"\n\n\n## Note about pretrained models\n\nThis competition has two requirements:\n* Submissions should be done from Kaggle notebooks\n* Internet should be turned off\n\nSince the transformers library uses Internet to download the pretrained models from its model hub, we must do some changes to the simplest work flow proposed in the \"quick guide\". The Kaggle community has overcome this situation adding pretrained models as \"Datasets\".\n\nTo use pretrained models, you should search for them in the dataset tab and find the one you want to use.\n\nYou can check the details of the \"dataset\" we are using [here](https:\/\/www.kaggle.com\/xhlulu\/huggingface-bert):\n\nTo add any of these pretrained models, go to the \"Add Data\" button in the top right corner of the kernel editor:\n\n![image.png](attachment:09fabbc0-5a6f-4f7b-b779-e8c0cee80e17.png)\n\nGo to the search:\n\n![image.png](attachment:59a3a0b9-481f-48c7-97c8-b73a01898c88.png)\n\nAnd press the \"Add\" button on the one you want:\n\n![image.png](attachment:57531fd1-b4de-41cd-b9d4-3456d2e9691f.png)\n\nThe files will be located in the path `..\/input\/` in remote file system.\n\nIn this case, the reference is:\n```python\nMODEL_NAME = \"..\/input\/huggingface-bert\/bert-base-cased\"\n```\n\n\n## The code\n\nWe have split the code in various simple functions to separates the wheat from the chaff and focus on the parts that are new to us.\n\nDocumentation about the `Trainer` can be found [here](https:\/\/huggingface.co\/transformers\/main_classes\/trainer.html) and about the `TrainerArguments` [here](https:\/\/huggingface.co\/transformers\/main_classes\/trainer.html#transformers.TrainingArguments).","1fa15a0a":"# 1- Learning \ud83e\udd17  - Out-of-the-box BERT [LB: 0.577]\n\nHi, and welcome! This is the first kernel of the series `Learning \ud83e\udd17`, a personal project I'm currently working on. I am an experienced data scientist diving into the hugging face transformers library and this series or kernels is a \"working diary\", as I do it. The approach I'm taking is the following: \n1. Explore various out-of-the-box models, without digging into their technical details. \n2. After that, I'll start going over the best ranked public kernels, understand their ideas, and reproduce them by myself. \n\nYou are invited to follow me in this journey. In this short kernel (~80 lines) we fine-tune an out-of-the-box cased BERT, with just the minimal set up required for it to run in this competition, obtaining a leaderboard score of `0.577`. \n\nThis is an ongoing project, so expect more notebooks to be added to the series soon. Actually, we are currently working on the following ones:\n\n1. [Learning \ud83e\udd17  - Out-of-the-box BERT [LB: 0.577]](https:\/\/www.kaggle.com\/julian3833\/1-learning-out-of-the-box-bert-lb-0-577) (this notebook)\n2. [Learning \ud83e\udd17 - Out-of-the-box RoBERTa [LB: 0.53]](https:\/\/www.kaggle.com\/julian3833\/2-learning-out-of-the-box-roberta-lb-0-53)\n3. [Learning \ud83e\udd17 - Out-of-the-box Electra [LB: 0.58]](https:\/\/www.kaggle.com\/julian3833\/3-learning-out-of-the-box-electra-lb\/) \n4. _Learning \ud83e\udd17 - Minimal fine tuning (WIP)_\n5. _Learning \ud83e\udd17 - Preprocessing (WIP)_\n6. _Learning \ud83e\udd17 - Reviewing public kernels (WIP)_\n7. _Learning \ud83e\udd17 - Intra-domain pre training RoBERTa (WIP)_\n\n\n\n## Using the [`transformers`](https:\/\/huggingface.co\/transformers\/) library\n\nWe are using a very high-level API of the library after following this quick guide article, which we recommend to read:\n[Fine-tuning a pretrained model](https:\/\/huggingface.co\/transformers\/training.html)\n\nWe use only 4 objects from the library: `Trainer`, `TrainingArguments`, `AutoModelForSequenceClassification`, `AutoTokenizer`. And, actually, the full list of imports is quite small as you can see below:","88b9b961":"### \ud83e\udd17\ud83e\udd17 Thanks for reading this notebook! Remember to upvote if you found it useful, and stay tuned for the next deliveries! \ud83e\udd17\ud83e\udd17"}}