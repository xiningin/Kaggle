{"cell_type":{"043edfd5":"code","61b628e3":"code","a1dd87b5":"code","c5e93970":"code","610b7ee8":"code","d8791d62":"code","659133e5":"code","1f005d34":"code","6ee71631":"code","5d3b9e1d":"code","4a89a5dc":"code","45690572":"code","ba4d8946":"code","fb914a56":"code","8784be60":"code","9c51488d":"code","396ee5ed":"code","72fff41a":"code","126dcf58":"code","a83b5303":"code","fe71ca67":"code","6e6acbc7":"code","64436649":"code","ebbd9ebc":"markdown","8d2c18eb":"markdown","25063c34":"markdown","e39ec8a2":"markdown","5c1c8780":"markdown","95296c2f":"markdown","835afbca":"markdown","695bc1e9":"markdown","60ec626e":"markdown","d3c0b7a1":"markdown"},"source":{"043edfd5":"!ls \/kaggle\/input\/nfl-health-and-safety-helmet-assignment","61b628e3":"import os \nimport cv2 \nimport glob\nimport random\nimport subprocess\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt \nfrom matplotlib import animation, rc\nfrom IPython.display import Video, display","a1dd87b5":"root_path = \"..\/input\/nfl-health-and-safety-helmet-assignment\"\nimage_labels_file = \"image_labels.csv\"\ntrain_labels_file = \"train_labels.csv\"\nbaseline_helmets_file = \"train_baseline_helmets.csv\"\nimage_dir = \"images\"\ntrain_videos = \"train\"\ntest_videos = \"test\"","c5e93970":"label_to_idx = {\n    'Helmet': 0,\n    'Helmet-Blurred': 1,\n    'Helmet-Difficult': 2,\n    'Helmet-Sideline': 3,\n    'Helmet-Partial': 4\n}\n\nlabel_to_color = {\n    'Helmet': (255, 255, 255),\n    'Helmet-Blurred': (255, 0, 0),\n    'Helmet-Difficult': (0, 255, 0),\n    'Helmet-Sideline': (0, 0, 255),\n    'Helmet-Partial': (0, 255, 255)\n}\n\ndef draw_bboxes(image, bboxes, labels):\n    # bbox in format [left, width, top, height]\n    for i, bbox in enumerate(bboxes):\n        label = labels[i]\n        (x, y) = (bbox[0], bbox[2])\n        (w, h) = (bbox[1], bbox[3])\n\n        image = cv2.rectangle(\n                    image,\n                    (x, y),\n                    (x+w, y+h),\n                    label_to_color[label],\n                    thickness=2\n                )\n    return image\n\ndef read_image(image_path):\n    try:\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    except:\n        print(f\"Couldn't load the image!\")","610b7ee8":"image_labels = pd.read_csv(f\"{root_path}\/{image_labels_file}\")\nimage_labels","d8791d62":"# Let's find the unique labels \nlabels = image_labels['label'].unique().tolist()\nprint(f\"{len(labels)} labels.\\nThey are: {labels}\")","659133e5":"image_files = glob.glob(f\"{root_path}\/{image_dir}\/*\")\n\nprint(f\"Total number of images: {len(image_files)}\\n\")\nprint(f\"Some examples:\")\nfor i, file in enumerate(image_files[:5]):\n    print(f\"{i} : {file}\")","1f005d34":"# see labels of a random image\nrandom_image = os.path.basename(\n    random.choice(image_files)\n)\n\nimage_labels[image_labels['image'] == random_image]","6ee71631":"# Visualize a random image\ndef get_random_bboxed_image():\n    image_selected = random.choice(image_files)\n    image_name = os.path.basename(image_selected)\n\n    image = cv2.imread(image_selected)\n    selected_labels = image_labels[image_labels['image'] == image_name]\n    bboxes = selected_labels.iloc[:, 2:].to_numpy()\n    labels = selected_labels.iloc[:, 1].to_numpy()\n\n    image_bboxed = draw_bboxes(image.copy(), bboxes, labels)\n    \n    return image_bboxed\n\n# Visualize a particular image\ndef get_bboxed_image(image_path):\n    image_name = os.path.basename(image_path)\n\n    image = cv2.imread(image_path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    \n    selected_labels = image_labels[image_labels['image'] == image_name]\n    bboxes = selected_labels.iloc[:, 2:].to_numpy()\n    labels = selected_labels.iloc[:, 1].to_numpy()\n\n    image_bboxed = draw_bboxes(image.copy(), bboxes, labels)\n    \n    return image_bboxed","5d3b9e1d":"image_bboxed = get_random_bboxed_image()\n\nplt.figure(figsize=(10, 10))\nplt.imshow(image_bboxed)","4a89a5dc":"# Create a slideshow of the images\ndef create_animation(num_frames=10, interval=1000):\n    fig = plt.figure(figsize=(12, 6))\n    plt.axis('off')\n    \n    image_bboxed = get_random_bboxed_image()\n    image = plt.imshow(image_bboxed)\n    \n    def animate_func(i):\n        image_bboxed = get_random_bboxed_image()\n        image.set_array(image_bboxed)\n        return [image]\n    \n    return animation.FuncAnimation(fig,\n                                   animate_func,\n                                   frames=num_frames,\n                                   interval=interval # in ms\n                                  )\nrc('animation', html='jshtml')","45690572":"create_animation()","ba4d8946":"image_labels","fb914a56":"def get_random_video_name():\n#     \"_\".join(image_labels.iloc[1, 0].split('_')[:-1])\n    video_files = os.listdir(f\"{root_path}\/{train_videos}\")\n    return random.choice(video_files)\n\ndef display_video(video_path, ratio=0.5):\n    return Video(f\"{root_path}\/train\/{video_path}\",\n                  embed=True,\n                  height = int(720 * ratio),\n                  width = int(1280 * ratio))\n\nrandom_video = \"_\".join(image_labels.iloc[1, 0].split('_')[:-1])\ndisplay_video( get_random_video_name() )","8784be60":"def video_with_baseline_boxes(video_path, baseline_boxes, gt_labels, verbose=True):\n    \"\"\"\n    Annotates a video with both the baseline model boxes and ground truth boxes.\n    Baseline model prediction confidence is also displayed.\n    \"\"\"\n    VIDEO_CODEC = \"MP4V\"\n    HELMET_COLOR = (255, 0, 0) # Blue \n    BASELINE_COLOR = (0, 255, 0) # Green\n    IMPACT_COLOR = (0, 0, 255) # Red\n    video_name = os.path.basename(video_path).replace(\".mp4\", \"\")\n    \n    if verbose:\n        print(f\"Running for {video_name}\")\n    baseline_boxes = baseline_boxes.copy()\n    gt_labels = gt_labels.copy()\n\n    baseline_boxes[\"video\"] = baseline_boxes[\"video_frame\"].str.split(\"_\").str[:3].str.join(\"_\")\n    gt_labels[\"video\"] = gt_labels[\"video_frame\"].str.split(\"_\").str[:3].str.join(\"_\")\n\n    baseline_boxes[\"frame\"] = baseline_boxes[\"video_frame\"].str.split(\"_\").str[-1].astype(\"int\")\n    gt_labels[\"frame\"] = gt_labels[\"video_frame\"].str.split(\"_\").str[-1].astype(\"int\")\n\n    videocap = cv2.VideoCapture(video_path)\n    fps = videocap.get(cv2.CAP_PROP_FPS)\n    width = int(videocap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(videocap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    output_path = f\"labelled_{video_name}.mp4\"\n    tmp_output_path = f\"tmp_\" + output_path\n    output_video = cv2.VideoWriter(\n                        tmp_output_path,\n                        cv2.VideoWriter_fourcc(*VIDEO_CODEC),\n                        fps,\n                        (width, height)\n                    )\n    frame = 0\n    while True:\n        it_worked, img = videocap.read()\n        if not it_worked:\n            break\n        # We need to add 1 to the frame count to match the label frame index\n        # that starts at 1\n        frame += 1\n\n        # Let's add a frame index to the video so we can track where we are\n        img_name = f\"{video_name}_frame{frame}\"\n        cv2.putText(img, img_name, (0, 50), cv2.FONT_HERSHEY_SIMPLEX,\n                    1.0, HELMET_COLOR, thickness=2)\n\n        # Now, add the boxes\n        boxes = baseline_boxes.query(\"video == @video_name and frame == @frame\")\n        if len(boxes) == 0:\n            print(\"Boxes incorrect\")\n            return \n        for box in boxes.itertuples(index=False):\n            cv2.rectangle(img, (box.left, box.top), (box.left+box.width, box.top+box.height),\n                          BASELINE_COLOR, thickness=1)\n            cv2.putText(img, f\"{box.conf:.2f}\", (box.left, max(0, box.top-5)),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, BASELINE_COLOR, 1)\n\n        boxes = gt_labels.query(\"video == @video_name and frame == @frame\")\n        if len(boxes) == 0:\n            print(\"Boxes incorrect\")\n            return \n        for box in boxes.itertuples(index=False):\n            # Filter for definitive head impacts and turn red\n            if box.isDefinitiveImpact == True:\n                color, thickness = IMPACT_COLOR, 3\n            else:\n                color, thickness = HELMET_COLOR, 1\n            cv2.rectangle(img, (box.left, box.top), (box.left+box.width, box.top+box.height),\n                          color, thickness=thickness)\n            cv2.putText(img, box.label, (box.left+1, max(0, box.top-20)),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, thickness=1)\n\n        output_video.write(img)\n    output_video.release()\n    # Not all browsers support the codec, we will re-load the file at tmp_output_path\n    # and convert to a codec that is more readable using ffmpeg\n    if os.path.exists(output_path):\n        os.remove(output_path)\n    subprocess.run([\n        \"ffmpeg\",\n        \"-i\",\n        tmp_output_path,\n        \"-crf\",\n        \"18\",\n        \"-preset\",\n        \"veryfast\",\n        \"-vcodec\",\n        \"libx264\",\n        output_path\n    ])\n    os.remove(tmp_output_path)\n\n    return output_path","9c51488d":"example_video = '..\/input\/nfl-health-and-safety-helmet-assignment\/train\/57584_000336_Sideline.mp4'\ntrain_df = pd.read_csv(f\"{root_path}\/{train_labels_file}\")\ntrain_predict_df = pd.read_csv(f\"{root_path}\/{baseline_helmets_file}\")\n\noutput_video = video_with_baseline_boxes(example_video,\n                          train_predict_df, train_df)\n\nfrac = 1.0 # scaling factor for display\ndisplay(Video(data=output_video,\n              embed=True,\n              height=int(720*frac),\n              width=int(1280*frac))\n       )","396ee5ed":"train_df","72fff41a":"img_bboxes = image_labels.pivot_table(index=[\"image\"], aggfunc='size')\n\nprint(f\"BBox Statistics\")\nprint(f\"Minimum: {img_bboxes.min()}\")\nprint(f\"Maximum: {img_bboxes.max()}\")\nprint(f\"Mean: {img_bboxes.mean():0.2f}\")\nprint(f\"Median: {img_bboxes.median()}\")\nprint(f\"Mode: {img_bboxes.mode()}\")","126dcf58":"img_count = [img_bboxes[i] for i in range(0, img_bboxes.shape[0])]\nsns.displot(data=img_count, kde=True)","a83b5303":"img_bboxes.sort_values(ascending=False).iloc[:5]","fe71ca67":"image_bboxed = get_bboxed_image(f\"{root_path}\/{image_dir}\/57515_000677_Sideline_frame0892.jpg\")\n\nplt.figure(figsize=(20,20))\nplt.imshow(image_bboxed)","6e6acbc7":"# Only looking at the first few thousand images. Should give an idea\n# Can be increased for more accurate results\n\nmin_h = 1\nmin_w = 1\nmax_h = 0\nmax_w = 0\ncur_video = \"\"\n\nimg_h = 1e-6\nimg_w = 1e-6\n\nfor i, (image, label, left, width, top, height) in image_labels.iterrows():    \n    # load the image\n    this_video = \"_\".join(image.split('_')[:-1])\n    if this_video != cur_video:\n        img = cv2.imread(f\"{root_path}\/{image_dir}\/{image}\")\n        img_h, img_w, _ = img.shape\n        cur_video = this_video\n\n\n    normed_h = height \/ img_h\n    normed_w = width \/ img_w\n    \n    if normed_h < min_h:\n        min_h = normed_h\n    if normed_w < min_w:\n        min_w = normed_w\n    if normed_h > max_h:\n        max_h = normed_h\n    if normed_w > max_w:\n        max_w = normed_w\n    \n    if (i+1)% 100_000 == 0:\n        break","64436649":"print(f\"Min -> H: {min_h:0.5f}, W: {min_w:0.5f}\\nMax -> H: {max_h:0.5f}, W: {max_w:0.5f}\")","ebbd9ebc":"That means the smallest bounding box(in the subset of images we selected) is atleast **0.2%** width and **0.4%** height compared to the image width and height respectively. The maximum values for height and width are **14.8%** and **10.2%** respectively. \nSo, for a 1000x1000 image the bounding box dimensions can be as low as **20px x 40px** and as high as **148px x 102px**.\n\nThese are some considerations to be taken care when selecting the model ","8d2c18eb":"## Look at the video","25063c34":"So, it seems there are people outside the playing area, wearning helmets. So, the dataset isn't mislabelled. :Phew:","e39ec8a2":"## Curious case of increased helmet count\n\nMost of the frames have helmets within the ranges only. Before proceeding further, let's find visualize few images with large number of helmets.","5c1c8780":"# Explorig the dataset","95296c2f":"## Gathering Insights on the Data","835afbca":"Sources:\n1. [NFL Helmet Assignment image_labels view by STPETE_ISHII](https:\/\/www.kaggle.com\/stpeteishii\/nfl-helmet-assignment-image-labels-view)\n2. [(NFL EDA Yukkuri by PIXYZ0130)](https:\/\/www.kaggle.com\/pixyz0130\/nfl-eda-yukkuri)","695bc1e9":"So, the images have a minumum of **1** and a maximum of **74** bboxes. On an average there will be **19** bboxes in each image.\n\nBut, there seems to be something fishy with th maximum value. For american football, there is a maximum of 11 players per team. So, the maximum number of players in each frame(wearing helmets) should be 22. So how come **74** bounding boxes come in a single frame. We will need to analyze that. \n\nFirst, lets draw graph of the number of bounding boxes in each image for a better understanding.","60ec626e":"# The sizes of the bounding boxes compared","d3c0b7a1":"#### Number of bounding boxes per image"}}