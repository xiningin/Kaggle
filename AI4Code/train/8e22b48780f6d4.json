{"cell_type":{"26aeda9f":"code","a759a53c":"code","198b2c25":"code","225878f9":"code","4258fd71":"code","d452a609":"code","11d6ecf5":"code","6cfc62e3":"code","46de34d5":"code","94875f65":"code","85c4849b":"code","2c1beb3f":"code","f16c7769":"code","42bc2b9e":"code","be04bafe":"code","48f0288a":"markdown","c76fef78":"markdown","dd782e5a":"markdown","e3185c89":"markdown","1e9847fb":"markdown","65e6adb4":"markdown","5f779e72":"markdown","50c0b3e7":"markdown","3fbe743c":"markdown","293952f2":"markdown","79bf1521":"markdown"},"source":{"26aeda9f":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import MinMaxScaler\n\ndf_submission = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-aug-2021\/sample_submission.csv\")\\\n# [:10000]# <<< Development mode\ndf_train = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-aug-2021\/train.csv\", index_col=\"id\").fillna(0)\\\n# [:10000]# <<< Development mode\ndf_test = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-aug-2021\/test.csv\", index_col=\"id\").fillna(0)\\\n# [:10000]# <<< Development mode\n\ndisplay(df_submission.head())\ndisplay(df_train.head())\ndisplay(df_test.head())","a759a53c":"# display(\n#     df_train.describe().T.style.background_gradient(\n#         subset=[\"mean\"], cmap=\"coolwarm\"\n#     ).background_gradient(\n#         subset=[\"std\"], cmap=\"inferno\"\n#     )\n# )\n\n# df_train.info()","198b2c25":"corr_mat = df_train.corr()\n\n# loss -> target\ntarget = df_train[\"loss\"]\ndf_train = df_train.drop(['loss'], axis=1)\n\ndf_all = pd.concat([df_train,df_test], axis=0, copy=False)","225878f9":"target_cnt = target.value_counts()\n\nplt.figure(figsize=(14, 5))\nsns.barplot(x=target_cnt.index, y=target_cnt.values, palette=\"coolwarm\")\nplt.title(\"Target unique values\", fontdict={\"fontsize\":20})","4258fd71":"plt.figure(figsize=(25, 6))\ncorr_mat[\"loss\"][:-1].plot(kind=\"bar\", grid=True)\nplt.title(\"Features correlation to target label\", fontdict={\"fontsize\": 20})","d452a609":"print(f'target unique values : {target.nunique()}')\nfor columns in df_train.columns.values.reshape((10, -1)):\n    txt = \"\"\n    for col in columns:\n        txt += f'{col}: {df_train[col].nunique()},\\t'\n    print(txt)","11d6ecf5":"df_train[df_train.duplicated()]","6cfc62e3":"# normalize data [0, 1]\ndef normalize(df_1, df_2):\n    df = pd.concat([df_1, df_2])\n    minmax = MinMaxScaler()\n    \n    for column in df.columns:\n        data = df[column].values.reshape((-1, 1))\n        scaled = minmax.fit_transform(data).reshape((-1))\n        df.update(pd.Series(scaled, name=column))\n\n    split = len(df_1)\n    return df.iloc[:split, :], df.iloc[split:, :]\n\n# plot dataframe\ndef plot_features(df_train, df_test, file_name=None, n_loss=3, n_rows=10):\n    # save file\n    if file_name is not None:\n        df_train.to_pickle(f\"{file_name}_train.pkl\")\n        df_test.to_pickle(f\"{file_name}_test.pkl\")\n    \n    train, test = df_train.copy(), df_test.copy()\n    qdf_train, qdf_test = normalize(train, test)\n\n    for loss in range(1, len(target))[:n_loss]:\n        matches = target.loc[target.values == loss].index[:n_rows]\n        rows = qdf_train.iloc[matches]\n\n        # display\n        fig, ax = plt.subplots(1, 1, figsize=(20, 6))\n        for index in range(len(rows)):\n            row = rows.iloc[index]\n            ax.bar(range(len(row)), row, alpha=0.5)    \n        ax.set_yticks(range(0, 20, 3))\n        ax.margins(0)\n        ax.set_title(f'all Rows of {loss}', loc='left', fontweight='bold')\n        ax.legend()\n        plt.show()","46de34d5":"plot_features(df_train, df_test)","94875f65":"# Quantile Normalization\ndf_all_median = pd.DataFrame.median(df_all, 0)\ndf_all_25quan = df_all.quantile(0.25, 0)\ndf_all_75quan = df_all.quantile(0.75, 0)\ndf_all = (df_all - df_all_median) \/ (df_all_75quan - df_all_25quan)\n\ndef normalize_quantile(df):\n    df_sorted = pd.DataFrame(\n        np.sort(df.values, axis=0), \n        index=df.index, \n        columns=df.columns\n    )\n    \n    df_sorted_mean = df_sorted.mean(axis=1)\n    df_sorted_mean.index = np.arange(1, len(df_sorted_mean) + 1)\n    \n    qdf = df.rank(axis=0, method=\"min\").stack().astype(int).map(df_sorted_mean).unstack()\n    return qdf\n    \nqdf = normalize_quantile(df_all)\nqdf_train = qdf.iloc[:len(df_train), :]\nqdf_test = qdf.iloc[len(df_train):, :]\n\nplot_features(qdf_train, qdf_test, file_name=\"quantile_df\")","85c4849b":"# Quantile Binning\ndef zeros_like(df):\n    new_df = df.copy()\n    for col in new_df.columns:\n        new_df[col].values[:] = 0\n        \n    return new_df\n\ndef quantile_binning(df):\n    df_bin = zeros_like(df)\n    for i in range(df_bin.shape[1]):\n        binning = pd.qcut(df.iloc[:, i], 50, labels=False, duplicates=\"drop\")\n        df_bin.iloc[:, i] = binning\n        \n    return df_bin\n\nbdf = quantile_binning(qdf)    \nbdf_train = bdf.iloc[:qdf_train.shape[0], :]\nbdf_test = bdf.iloc[qdf_train.shape[0]:, :]\n\nplot_features(bdf_train, bdf_test, file_name=\"binning_df\")","2c1beb3f":"import tensorflow as tf\nimport tensorflow.keras.backend as K\n\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import Adam\n\ndef custom_loss(y_true, y_pred):\n    loss = K.mean(K.square(y_pred - y_true))\n    return loss\n\ndef dae_network():\n    ae_input = layers.Input(shape = (100))\n    ae_encoded = layers.Dense(\n        units = 100,\n        activation='elu')(ae_input)\n    ae_encoded = layers.Dense(\n        units = 300,\n        activation='elu')(ae_encoded)\n    ae_decoded = layers.Dense(\n        units = 100,\n        activation='elu')(ae_encoded)\n    \n    return models.Model(ae_input,ae_encoded), models.Model(ae_input, ae_decoded)\n\n# create training data\ndf_noisy = qdf + np.random.normal(0, .1, df_all.shape)\n\nsplit = int(0.8 * len(df_noisy))# split on 80%\nxtrain, ytrain = df_noisy.iloc[:split], qdf.iloc[:split]\nxvalid, yvalid = df_noisy.iloc[split:], qdf.iloc[split:]\n\n# define callbacks\nearly_stop = EarlyStopping(\n    monitor=\"val_loss\", patience=20, verbose=0, mode=\"min\",\n    min_delta=1e-9,\n    baseline=None,\n    restore_best_weights=True\n)\nreduce_lr = ReduceLROnPlateau(\n    monitor=\"val_loss\", patience=4, verbose=0, mode=\"min\",\n    factor=0.8\n)\n\n# define network\ndecoder, autoencoder = dae_network()\nautoencoder.compile(loss=custom_loss, optimizer=Adam(lr=5e-3))\n\n# train network\nhistory = autoencoder.fit(\n    xtrain, \n    ytrain,\n    epochs=200, \n    batch_size=512,\n    verbose=0,# 1 = logs network\n    validation_data=(xvalid, yvalid),\n    callbacks=[early_stop, reduce_lr]\n)","f16c7769":"# Denoiser AutoEncoder\nenp = decoder.predict(qdf)\nprint(f\"max encoded value : {np.max(enp)}\")\n\n# Output of Encoder has 300 features, \n#  we take most accurate values that we end up with 100 Features.\nenp_var = np.var(enp, axis=0, ddof=1)\nenp_var1 = np.where(enp_var > 0.8)[0]# 0.8=RELEASE, 0.108=DEVELOPMENT\n\nassert(len(enp_var1) >= 95 & len(enp_var1) <= 105)\nprint(\"number of selected columns\", len(enp_var1))\ncolumns = [f\"col_{i}\" for i in range(len(enp_var1))]\n\nedf = pd.DataFrame(enp[:, enp_var1], columns=columns)\nedf_train = edf.iloc[:len(df_train),:]\nedf_test = edf.iloc[len(df_train):,:]\n\nplot_features(edf_train, edf_test) # exlude PCA","42bc2b9e":"# (Add) PCA to dataframe\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\npca = PCA(n_components=10)\npnp_all = pca.fit_transform(enp)\n\nscaler = StandardScaler()\npnp_all = scaler.fit_transform(pnp_all)\n\npnp_train = pnp_all[:len(df_train)]\npnp_test = pnp_all[len(df_train):]\n\n# add pca\nfor i in range(pnp_train.shape[1]):\n    index = len(edf_train) + i\n    edf_train[f\"col_{index}\"] = pd.Series(pnp_train[:, i])\n\nfor i in range(pnp_test.shape[1]):\n    index = len(edf_test) + i\n    edf_test[f\"col_{index}\"] = pd.Series(pnp_test[:, i])\n\n# PCA is the last 10 bars\nplot_features(edf_train, edf_test, file_name=\"dae_pca_df\")","be04bafe":"# More ideas will than been added....\n\n# ??? Random Trees Embeddings","48f0288a":"Comparing the the `first plotted graps` with `above plotted graps`:\n- the color where all feature values reached, did increase\n- the high feature values did decrease\n\n## Quantile Binning","c76fef78":"# Investigate features [f0 - f99]\nThe dataset contains a lot of features, let we investigate.  \nAnd create some function to visualize dataframes ","dd782e5a":"In simple words so far i understand.  \nWe are deviding the data, large values will decrease faster than small values.  \n- There is less white empty space, this means that all features get similar output.","e3185c89":"## Denoiser AutoEncoder + PCA\n[`Simple Denoise Autoencoder sample.`](https:\/\/www.kaggle.com\/arenddejong\/denoising-autoencoder-dae)  \nTo learn a Denoiser what noise is we will add noise on the original data and learn the network.  \nThe network contains of 2 parts the `Encoder` and `Decoder`.  \n\nWe need them both to train the network, as input we use the Quantile Normalized data\n\n\n\n","1e9847fb":"The result are Nice as the different colors get closer to each other.  \nWe add the PCA of this dataframe to the dataframe, the last 10 bars are the PCA in the plotted graphs.","65e6adb4":"Shalom,  \n\nIn this Notebook we will do Exploratory Data Analysis (EDA).  \nThis is the first thing we need to do when they give us data and a particular problem.  \nData is the tool to solve problems, so let's dive into the data. \n\nThis database contains:\n- \/kaggle\/input\/tabular-playground-series-aug-2021\/sample_submission.csv\n- \/kaggle\/input\/tabular-playground-series-aug-2021\/train.csv\n- \/kaggle\/input\/tabular-playground-series-aug-2021\/test.csv\n\nLet's get the info about the files(global information), and row index we set to `id`.\n\n## Import Data & Libraries","5f779e72":"# How to use this data?\nAs we can see now this problem is a `Regression Problem` as the data is a continuous\/sparse data.  \nEach Algorithm that can represent any better results than current data, we will use it as input data.  \n\nThe saved data are the following files in this kernel:\n- `quantile_df_train.pkl` & `quantile_df_test.pkl`\n- `binning_df_train.pkl` & `binning_df_test.pkl`\n- `dae_pca_df_train.pkl` & `dae_pca_df_test.pkl`\n\nWe can use this data as input data on a MLP Network, and hope we will beat some places.\n<!-- start on model: https:\/\/www.kaggle.com\/arenddejong\/tps-aug-nn-torch -->\n\nSuggestions, Irritations or Improvements?   \nleave a comment, thanks  \n\nNiek Tuytel","50c0b3e7":"## Quantile Normalization","3fbe743c":"- large amount of features\n- `loss` is the submission(`target`) variable \n\nWe move `loss` to a unique variable called `target`\n\n## Investigate the loss (target)","293952f2":"The X-axis is all features that are present in a single row.  \nThe Y-axis represents the value of eacht feaeture, each color is a unique row.  \nIn one plotted graph there is 1 label value used.  \n\nThis way we can see if there is any pattern in the data that represents the target(loss) value.  \nAs we can see there is a lot of noise present in the data.\n\nTo reduce the noise and dimensions by using:\n- Quantile Normalization\n- Quantile Binning\n- Denoised Auto Encoder + PCA","79bf1521":"The dataset has __250000__ rows, most of the features get above __200000__ unique values.  \nThere is less change that a prediction can been possible on 1 single feature if nearly all rows has a unique value."}}