{"cell_type":{"b17b49d6":"code","6400c488":"code","1ae57918":"code","b9f8f746":"code","9aaec8cf":"code","c83e5089":"code","afac7592":"code","23ac9947":"code","42b65608":"code","9aaebcfd":"code","8a0fb1a0":"code","ff9fb871":"code","e92e3683":"code","3614a83d":"markdown","11c70c39":"markdown","589232c0":"markdown","ae3b643f":"markdown","9ee7b293":"markdown","d01892ce":"markdown","05239b67":"markdown","96212015":"markdown","f71a459f":"markdown","40d31d96":"markdown","554acc99":"markdown","565fd425":"markdown","180a6f30":"markdown","cfa5b24d":"markdown","a445c322":"markdown","86f563cb":"markdown","b6bed953":"markdown","8555aad6":"markdown"},"source":{"b17b49d6":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport datetime as dt\nimport sklearn as sklearn\nimport squarify\n\nfrom sklearn.datasets import load_digits\nfrom sklearn.decomposition import PCA","6400c488":"asset_prices = pd.read_csv('..\/input\/spx-holdings-and-spx-closeprice\/spx_holdings_and_spx_closeprice.csv',\n                           date_parser = lambda dt: pd.to_datetime(dt, format = \"%Y-%m-%d\",),\n                           index_col = 0)","1ae57918":"asset_returns = pd.DataFrame( data = np.zeros (shape = (len(asset_prices.index), asset_prices.shape[1])),\n                            columns = asset_prices.columns.values,\n                            index = asset_prices.index)\n\nprev_row = 0\nfor index, row in asset_prices.iterrows():\n  if index == 0:\n    prev_row = row\n  else:\n    dif = row - prev_row\n    returns = dif \/ prev_row\n    asset_returns.loc[index] = returns\n    prev_row = row\n\nasset_returns = asset_returns.drop(asset_returns.index[0], axis = 0)\n\nnormed_returns = (asset_returns - asset_returns.mean())\/asset_returns.std()\nnormed_prices = (asset_prices - asset_prices.mean())\/asset_prices.std()\n\nsquari = asset_prices.iloc[:, 0:418].mean().sort_values(ascending = False)[0:20]\n\nplt.figure(figsize=(20,10))\nsquarify.plot(sizes=squari.values.tolist(), label=squari.index.tolist(),alpha=0.5)\nplt.axis('off')\nplt.title('Figure 1: Prices at different scales', size=15)\nplt.show()","b9f8f746":"train_end = dt.datetime(2012,3,26)\n\ndf_train = None\ndf_test = None\n\ndf_train = normed_returns[normed_returns.index <= train_end].copy()\ndf_test = normed_returns[normed_returns.index > train_end].copy()\n\ndf_raw_test = asset_returns[asset_returns.index > train_end].copy()\n\nstock_tickers = normed_returns.columns.values[:-1]\n\nplt.figure(num = 3, figsize=(20,10))\nplt.axvline(x = dt.datetime(2012,3,26), color = 'red')\nsns.lineplot(data = asset_prices[asset_prices.index <= train_end], \n             x = asset_prices[asset_prices.index <= train_end].index, y = 'SPX')\nsns.lineplot(data = asset_prices[asset_prices.index > train_end], \n             x = asset_prices[asset_prices.index > train_end].index, y = 'SPX')\nplt.title('Figure 2: Market performance during train and test periods', size=15)\nplt.show()\n","9aaec8cf":"PCA = PCA()\npca = PCA.fit(df_train.dropna()[stock_tickers])\nsummary30 = pd.DataFrame({'Variance' : pca.explained_variance_ratio_[0:30],\n                          'VarianceCum' : pca.explained_variance_ratio_[0:30].cumsum(),\n                          'Eigenvalue' : pca.explained_variance_[0:30],\n                          'PC' : np.arange(1, len(pca.explained_variance_ratio_[0:30])+1)})\n\nplt.figure(num = 3, figsize=(20,20))\n\nplt.subplot(211)\nplt.bar(data = summary30,\n         height = 'Variance', x = 'PC', color=\"steelblue\")\nsns.lineplot(data = summary30,\n         y = 'VarianceCum', x = 'PC', color=\"red\")\nfor i, v in enumerate(round(summary30.VarianceCum,3)):\n    plt.text(summary30.PC.tolist()[i], v+0.01, str(v), ha='right', \n             fontsize=12, color = 'red')\nplt.title('Figure 3: Variance and Cumulative Variance of each PC', size=15)\n\nplt.subplot(212)\nplt.figure(num = 3, figsize=(20,10))\nplt.bar(data = summary30,\n         height = 'Eigenvalue', x = 'PC', color=\"steelblue\")\nfor i, v in enumerate(round(summary30.Eigenvalue,1)):\n    plt.text(summary30.PC.tolist()[i], v+2, str(v), ha='center', \n             fontsize=12, color = 'black')\nplt.title('Figure 4: Eigenvalues of each PC', size=15)\nplt.show()","c83e5089":"pcs = pca.components_\nnormalized_pcs = list()\n\nfor eigen_vector in pcs:\n  normalized_values = eigen_vector\/eigen_vector.sum()\n  normalized_pcs.append(normalized_values)\n\npc_w = normalized_pcs[0]\neigen_prtf1 = pd.DataFrame({'weights':pc_w.squeeze()} , \n                           index = asset_prices.columns.values.tolist()[0:418])\neigen_prtf1.sort_values(by = ['weights'], ascending = False, inplace = True)\n\nxaxis =['']* len(eigen_prtf1)\nfor i in range(len(eigen_prtf1)):\n  if i in [0, 42 ,84, 126, 168, 210, 251, 294, 336, 379, 417]:\n    xaxis[i]=eigen_prtf1.index.tolist()[i]\n\nplt.figure(num = 3, figsize=(20,10))\nplt.xticks(np.arange(418), xaxis)\nplt.tick_params(axis='x', which='both', bottom=False, top=False) \nplt.plot(eigen_prtf1, linestyle='dashed', color = 'steelblue')\nplt.title('Figure 5: Orderly weighting of assets in the market portfolio ', size=15)\nplt.show()","afac7592":"comp = asset_prices[asset_prices.index <= train_end]\ncompA = comp.loc[:, comp.columns != 'SPX']\ncompA = [(compA[x]*y) for x, y in zip(compA.columns.tolist(), normalized_pcs[0])]\n\ndf = pd.concat(compA, axis=1, keys=[s.name for s in compA])\ndf['PC1']= df.iloc[:,:].sum(axis=1)","23ac9947":"plt.figure(num = 3, figsize=(20,15))\nplt.subplot(211)\nsns.lineplot(data = comp, x = comp.index, y = 'SPX')\nplt.title('Figure 6: Market performance', size=15)\n\nplt.subplot(212)\nsns.lineplot(data = df.dropna(), x = df.dropna().index, y = 'PC1')\nplt.title('Figure 7: Market portfolio performance (PC1)', size=15)\nplt.show()","42b65608":"Mark = pd.DataFrame({'Profitability_PC': [0]*418, 'Variance' : [0]*418}, dtype=np.str)\nC = np.asmatrix(asset_returns.iloc[:, 0:418].cov())\n\nfor i in range(418):\n  Mark.Profitability_PC[i] = np.sum((asset_returns.iloc[:, 0:418].mean().values.tolist()*normalized_pcs[i]).tolist())*len(df_train)\n  Mark.Variance[i] = np.sqrt(normalized_pcs[i] * C * np.asmatrix(normalized_pcs[i]).T).tolist()[0][0]","9aaebcfd":"def rand_weights(n):\n    k = np.random.randn(n)\n    return k \/ sum(k)\n\nartificial = [0]*5000\nfor i in range(len(artificial)):\n  artificial[i] = rand_weights(418)","8a0fb1a0":"Mark2 = pd.DataFrame({'Profitability_PC': [0]*len(artificial), 'Variance' : [0]*len(artificial)}, dtype=np.str)\n\nfor i in range(len(artificial)):\n  Mark2.Profitability_PC[i] = np.sum((asset_returns.iloc[:, 0:418].mean().values.tolist()*artificial[i]).tolist())*len(df_train)\n  Mark2.Variance[i] = np.sqrt(artificial[i] * C * np.asmatrix(artificial[i]).T).tolist()[0][0]","ff9fb871":"Mark = Mark[(Mark['Variance']<5) & (Mark['Profitability_PC']<150) & (Mark['Profitability_PC']>-150)]\nMark2 = Mark2[(Mark2['Variance']<5) & (Mark2['Profitability_PC']<150) & (Mark2['Profitability_PC']>-150)]\nplt.figure(num = 2, figsize=(20,10))\nMark2['Sharpe'] = Mark2.Profitability_PC\/Mark2.Variance\nMark['Sharpe'] = Mark.Profitability_PC\/Mark.Variance\n\nsns.regplot(x = Mark2.Variance, y = Mark2.Profitability_PC, fit_reg = False, \n            color='crimson', scatter_kws={'s':1.8}, label='Random portfolios')\nsns.regplot(x = Mark.Variance, y = Mark.Profitability_PC, fit_reg = False, \n            color=\"steelblue\", scatter_kws={'s':1.8}, label='Portfolios from PCA')\nplt.scatter(x = Mark.iloc[0,:].Variance,y = Mark.iloc[0,:].Profitability_PC,\n            marker='*',color='black',s=100, label='Market Profolio (PC1)')\nplt.title('Figure 8: Portfolios according to profitability and risk', size=15)\nplt.legend(labelspacing=1)\nplt.show()","e92e3683":"pce = PCA.fit(df_test.dropna()[stock_tickers])\nsummary30_test = pd.DataFrame({'Variance' : pce.explained_variance_ratio_[0:30],\n                          'VarianceCum' : pce.explained_variance_ratio_[0:30].cumsum(),\n                          'Eigenvalue' : pce.explained_variance_[0:30],\n                          'PC' : np.arange(1, len(pce.explained_variance_ratio_[0:30])+1)})\n\nplt.figure(num = 3, figsize=(20,20))\n\nplt.subplot(211)\nplt.figure(num = 3, figsize=(20,10))\nplt.bar(data = summary30,\n         height = 'Variance', x = 'PC', color=\"steelblue\")\nfor i, v in enumerate(round(summary30.Variance,3)):\n    plt.text(summary30.PC.tolist()[i], v+0.008, str(v), ha='center', \n             fontsize=12, color = 'black', rotation=15)\nplt.title('Figure 9: Variance of each PC with Train sample', size=15)\n\nplt.subplot(212)\nplt.figure(num = 3, figsize=(20,10))\nplt.bar(data = summary30_test,\n         height = 'Variance', x = 'PC', color=\"steelblue\")\nfor i, v in enumerate(round(summary30_test.Variance,3)):\n    plt.text(summary30_test.PC.tolist()[i], v+0.008, str(v), ha='center', \n             fontsize=12, color = 'black', rotation=15)\nplt.title('Figure 10: Variance of each PC with Test sample', size=15)\nplt.show()","3614a83d":"---","11c70c39":"According to Avellaneda & Lee, **Principal Component Analysis (PCA) can be used to extract factors from historical data on asset returns.**\n\nBecause taking into account all the assets available in the market would be unmanageable, the analysis is performed with the assets present in the S&P 500 between specific dates (From January 2000 to December 2013). \n\nThe first thing I do is to generate the asset returns by subtracting the previous day's price from the daily market price and dividing it by the previous day's price. To perform a Principal Component Analysis, we need all the returns to be in the same dimension. Actually, we might think that this is already the case (they are returns after all) but just in case we normalise them by subtracting the mean and dividing by the standard deviation.\n\n---\n\nDe acuerdo con Avellaneda & Lee, **el An\u00e1lisis de Componentes Principales (PCA) puede ser utilizado con el objetivo de extraer factores de los datos hist\u00f3ricos de los retornos de los activos.**\n\nDebido a que tener en cuenta todos los activos disponibles en el mercado ser\u00eda inabarcable, el an\u00e1lisis se realiza con los activos presentes en el S&P 500 entre unas fechas concretas (desde enero del a\u00f1o 2000 hasta diciembre del 2013).\n\nLo primero que hago es generar los retornos de los activos, para ello, resto al precio de mercado diario el precio del dia anterior y lo divido entre este. Para realizar un An\u00e1lisis de Componentes Principales, necesitamos que todos los retornos est\u00e9n en una misma dimensi\u00f3n. Realmente, podr\u00edamos pensar que esto ya es as\u00ed (al fin y al cabo son retornos) pero por si acaso los normalizamos restandoles la media y dividiendolos entre la desviaci\u00f3n t\u00edpica","589232c0":"The next thing I do is to **apply the Markowitz Theory to study the distribution of each portfolio (Principal Component) in a space of returns and Standard Deviations (representing risk) to obtain a series of efficient portfolios.**\n\nThese portfolios have been constructed as a PC, i.e. as the weighted sum of the weight of each asset and its return. \n\n* **For the return**, I have taken into account that the participation of the portfolio in the asset has not varied over time, thus, the participation has been maintained from day 1 to the last day, therefore, the profitability will be equal to the daily average of the period by the number of days that the shares have traded.\n \n*  **Regarding the risk of the portfolio**, I have taken into account the standard deviation of the returns that compose it, weighted by the weights of the assets. This measure represents the risk to be assumed by the portfolio holder.\n\n---\n\nLo siguiente que hago es **aplicar la Teor\u00eda de Markowitz para estudiar la distribuci\u00f3n de casa cartera (Componente Principal) en un espacio de rentabilidades y Desviaciones Tipicas (que representan el riesgo) para obtener una serie de carteras eficientes.**\n\nEstos porfolios o carteras se han construido como una CP, es decir, como la suma ponderada del peso de cada activo y su rentabilidad. \n\n*  **Para la rentabilidad**, he tenido en cuenta que la participaci\u00f3n del porfolio en el activo no ha variado con el tiempo, de esta manera, se ha mantenido la participaci\u00f3n desde el d\u00eda 1 hasta el ultimo dia, por lo tanto, la rentabilidad ser\u00e1 igual a la media diaria del periodo por el numero de dias que han cotizado las acciones\n\n*  **Respecto al riesgo del porfolio**, he tenido en cuenta la desviaci\u00f3n t\u00edpica de las rentabilidades que lo componen ponderadas por los pesos de los activos. Esta medida, representa el riesgo a asumir por el tomador de la cartera.\n","ae3b643f":"---","9ee7b293":"To check if the Principal Component Analysis is robust, let's check if the results that it would give by modeling it on the test sample are the same as those it has generated with the training sample.\n\n---\n\nPara comprobar si el An\u00e1lisis de Componentes Principales es s\u00f3lido, vamos a comprobar si los resultados que dar\u00eda modelizarlo sobre la muestra de test, son iguales a los que ha generado con la muestra de entrenamiento.","d01892ce":"As we can see, **the performance of the market over the time and that of the portfolio generated by the weights of each asset in the first Principal Component are practically identical.** Therefore, we can affirm that the portfolio generated with the weights of ***the first PC behaves practically the same as the market portfolio, so we can accept the hypothesis proposed by Avellanea & Lee.***\n\n---\n\nComo podemos ver, **el rendimiento del mercado en el tiempo y el del porfolio generado por los pesos de cada activo en la primera Componente Principal son pr\u00e1cticamente identicos.** Por todo ello, podemos afirmar que ***el porfolio generado con los pesos de la primera CP se comporta practicamente igual que el porfolio de mercado, por lo que podemos aceptar la hipotesis propuesta por Avellanea & Lee***","05239b67":"# <font size=\"6\">Application of Markowitz Theory <\/font>","96212015":"# <font size=\"6\"> Avellaneda & Lee: Market portfolio with PCA <\/font>","f71a459f":"# <font size=\"6\"> PCA check <\/font>","40d31d96":"I have made **a Data Frame with the real data of the portfolios according to the weights coming from the Principal Component Analysis** (in blue in the graph) and **another one with data from 5,000 randomly generated portfolios** with random weights (in red in the graph). \n\nAs can be seen, **the use of PCA to generate portfolios in a market is completely valid,** since they have completely homogeneous risks and returns with those of the randomly generated portfolios. In the graph, the efficient frontier can be seen as the set of dominant portfolios in the ensemble. \n\n---\n\nHe realizado **un Data Frame con los datos reales de las carteras seg\u00fan los pesos procedentes del An\u00e1lisis de Compinentes Principales** (en azul en la gr\u00e1fica) y **otro con datos de 5.000 carteras generadas aleatoriamente** con pesos aleatorios (en rojo en la gr\u00e1fica). \n\nComo se puede ver, **el uso del ACP para generar carteras en un mercado es completamente v\u00e1lido**, ya que tienen riesgos y retornos completamente homogeneos con los de las carteras generadas de forma aleatoria. En la gr\u00e1fica, se puede observar la frontera eficiente como el conjunto de carteras dominantes del conjunto. ","554acc99":"# <font size=\"6\"> Summary <\/font>","565fd425":"# <font size=\"6\"> Introduction <\/font>\n\nSince I started studying finance, one of the things that caught my attention was Harry Markowitz's (Harry Markowitz, 1952) portfolio theory, since, in my opinion, it allows me to see in a very simple and quick way such complex aspects as the efficiency of a portfolio, which is composed as a combination of all the assets available in a market. The other day in class at the Master of Data Science at CUNEF, I saw how Avellaneda & Lee's (Avellaneda & Lee, 2008) approach to this theory could be applied, from the point of view of Principal Component Analysis, and I decided to do some research.\n\n**In this code, I intend to apply *Harry Markowitz's* modern portfolio theory to real data on the performance of the S&P 500 index assets over the last few years.**\n\nTo do so, I will use the approach of **Avellaneda & Lee** which proposes to **use Principal Component Analysis (PCA) to obtain the market portfolio and the weights of each of the assets in a series of portfolios.** These portfolios can then be used in modern portfolio theory.\n\n---\nDesde que empec\u00e9 a estudiar finanzas, una de las cosas que mas me llam\u00f3 la atenci\u00f3n fue la teor\u00eda de porfolios de Harry Markowitz (Harry Markowitz, 1952), ya que, a mi modo de ver, permite ver de una manera muy simple y r\u00e1pida aspectos tan complejos como la eficiencia de un porfolio, el cual est\u00e1 compuesto como una combinaci\u00f3n de todos los activos disponibles en un mercado. El otro d\u00eda en clase del Master de Data Science de CUNEF, vi como se pod\u00eda aplicar el acercamiento que realizan Avellaneda & Lee (Avellaneda & Lee, 2008) a esta teor\u00eda, desde el punto de vista del An\u00e1lisis de Componentes Principales y he decidido investigar un poco.\n\n**En este c\u00f3digo, pretendo aplicar la teor\u00eda moderna del porfolio de *Harry Markowitz* a datos reales del rendimiento de los activos del indice S&P 500 en los \u00faltimos a\u00f1os.**\n\nPara ello, voy a utilizar el acercamiento de **Avellaneda & Lee** que propone **utilizar el An\u00e1lisis de Componentes Pincipales (ACP) para obtener el porfolio de mercado y los pesos de cada uno de los activos en una serie de porfolios**. Estos porfolios, podr\u00e1n ser utilizados en la teor\u00eda moderna del porfolio.\n","180a6f30":"As can be seen in the graph, **Principal Component Analysis is a robust model for the analysis, since, regardless of the sample, it generates similar Principal Components in terms of explained Variance.**\n\n---\n\nComo se puede ver en el gr\u00e1fico, **el An\u00e1lisis de Componentes Principales es un modelo robusto para el an\u00e1lisis, ya que, independientemente de la muestra, genera Componentes Principales similiares en t\u00e9rminos de Varianza explicada**","cfa5b24d":"With this research, Avellaneda & Lee's theory that the application of Principal Component Analysis to portfolio research allows obtaining the market portfolio, as well as many other valid portfolios for efficient frontier analysis, is proven. The market portfolio coincides with the first Principal Component of PCA and its behavior over time is practically identical to that of the market. The similarity in behavior is due to the fact that the composition is very similar: it is a portfolio generated with positive weights of all assets (the only one in the echo PCA) and whose contribution to the performance of the whole is equal: those assets with high market capitalization (and therefore higher weight in the market portfolio) have a lower volatility than those with a lower market capitalization (and lower weight in the portfolio).\n\nOnce 400 feasible portfolios have been generated through PCA, another 5,000 have been generated with random weights. This set has been applied to approach Harry Markowitz's modern portfolio theory, in which the feasible investment space created by the set of generated portfolios can be perfectly seen, as well as the efficient investment frontier, which includes those portfolios that dominate over the rest of the sample.\n \nTo make the analysis more robust, the result obtained has been tested with the training sample to ensure that it is strong.\n \n## Research limitations:\nDue to the idiosyncrasies of Principal Component Analysis, the portfolios generated may have negative weights of some assets. Although this cannot occur directly in reality, it is true that there are some forms of investment that would allow a very similar approximation in practice. A possible solution could be to generate the weights of the assets taking into account the eigenvalues squared, in order to avoid the sign problem. \n\n## Bibliography:\n* *Avellaneda & Lee (2008):* Marco Avellaneda & Jeong-Hyun Lee. (2008, Jul). Statistical Arbitrage in the U.S. Equities Market.\n    \n    https:\/\/papers.ssrn.com\/sol3\/papers.cfm?abstract_id=1153505\n* *Harry Markowitz (1952):* Harry Markowitz. (1952, Mar). Portfolio Selection. The Journal of Finance.\n    \n    https:\/\/www.math.hkust.edu.hk\/~maykwok\/courses\/ma362\/07F\/markowitz_JF.pdf\n* *Dr. Thomas Starke, et al. (2021):* Dr. Thomas Starke, et al. (2021). Markowitz Portfolio Optimization in Python\/v3.\n    \n    https:\/\/plotly.com\/python\/v3\/ipython-notebooks\/markowitz-portfolio-optimization\/","a445c322":"As shown in *Figure 1*, the asset prices cannot be used directly to apply them to the PCA as they assume really different dimensions, however, the normalized returns are already one and the same comparable dimension which will not alter our analysis.\n\nAs we want to generate a robust model, one of the things we are going to do is to **create a training list** (with which we will generate the model) **and a test list** (which will tell us whether we have actually generated a model that fits the reality of the analysis, or whether it is overfitting the data). At the end of this publication, the robustness of the model will be tested by analyzing whether applying it to the test sample yields results similar to those of the training sample.\n\n---\n\nTal y como se ve en el *gr\u00e1fico 1*, los precios de los activos no pueden ser utilizados directamente para aplicarlos al PCA al suponer dimensiones realmente distintas, sin embargo, los retornos normalizados ya est\u00e1n una misma dimensi\u00f3n comparable que no alterar\u00e1 nuestro an\u00e1lisis.\n\nComo queremos generar un modelo robusto, una de las cosas que vamos a hacer es **crear una lista de entrenamiento** (con la que generaremos el modelo) **y una de prueba** (que nos indicar\u00e1 si realmente hemos generado un modelo que se ajusta a la realidad del an\u00e1lisis, o si en cambio tiene un sobreajuste a los datos). Al final de esta publicaci\u00f3n, se comprobar\u00e1 la robustez del modelo al analizar si al aplicarlo a la muestra de prueba se arrojan unos resultado similares a los de la muestra de entrenamiento","86f563cb":"In this way, we now have the weights of each asset for each portfolio and we can test the hypothesis that the first principal component assumes the market portfolio to behave as such.\n\n---\n\nDe esta manera, ya tenemos los pesos de cada activo para cada porfolio y podemos contrastar la hipotesis de que la primera componente principal supone el porfolio de mercado al comportarse como tal.","b6bed953":"As can be seen in *Figure 3,* **the first PC explains 34% of the total variance explained by the model, while the other components make a much smaller contribution.**\n\nIf we look at *figure 4*, we can see **the relationship between the variance explained in percentage terms by each Principal Component and its Eigenvalue** (after all, it is the same measure but in percentage terms).\n\nTherefore, we state that the first Principal Component (or first portfolio), which represents the dominant eigenvector, has a much higher eigenvalue than the rest. According to *Avellaneda & Lee*, **the dominant eigenvector is associated with the \"market portfolio\", in the sense that all coefficients are positive** (as we see if we run `pca.components_[0]`). Therefore, this portfolio will have positive weights of all assets (as these are determined by the coefficients).\n\n**If we follow *Avellaneda & Lee's research*, we observe that the weights of each asset to the first PC are inversely proportional to the volatility of the stock. This weighting is consistent with the capitalisation weighting, since larger capitalisation companies tend to have smaller volatilities (larger companies have lower volatility and therefore a higher weight in this CP, as is the case in the actual composition of the S&P 500, so it is consistent to determine the first CP as the market portfolio).**\n\nTo see that indeed this first CP behaves like the market, we compare the performance of a portfolio generated by the weights of the first Component with the actual market performance.\n\nTherefore, the next thing to do is to **obtain the weights of each asset on each Principal Component** (or portfolio). To do this, we simply divide the coefficient of each asset in a given PC by the sum of all the coefficients of the PC (i.e. divide each coefficient of the eigenvector by the sum of all of them).\n\n---\n\nComo se puede ver en el *gr\u00e1fico 3*, l**a primera CP explica el 34% de la varianza total explicada por el modelo, mientras que el resto de componentes tienen una aportaci\u00f3n mucho menor.** \n\nSi nos fijamos en el *gr\u00e1fico 4*, vemos la **relaci\u00f3n entre la varianza explicada porcentual por cada Componente Principal y su Autovalor** (al fin y al cabo, no deja de ser la misma medida pero en porcentaje).\n\nPor lo tanto, afirmamos que la primer Componente Principal (o primer cartera), que representa el autovector dominante, tiene un autovalor mucho mayor que el resto. Seg\u00fan *Avellaneda & Lee*, **el autovector dominante est\u00e1 asociado a la \"cartera de mercado\", en el sentido de que que todos los coeficientes son positivos** (tal y como vemos si ejecutamos `pca.components_[0]`). Por lo tanto, este porfolio tendr\u00e1 pesos positivos de todos los activos (ya que estos se determinan en funci\u00f3n de los coeficientes)\n\n**Si seguimos la investigaci\u00f3n de *Avellaneda & Lee*, observamos que los pesos de cada activo a la primera CP son inversamente proporcionales a la volatilidad de la acci\u00f3n. Esta ponderaci\u00f3n es coherente con la ponderaci\u00f3n por capitalizaci\u00f3n, ya que las empresas de mayor capitalizaci\u00f3n tienden a tener volatilidades m\u00e1s peque\u00f1as (las empresas de mayor tama\u00f1o, tienen una volatilidad m\u00e1s baja y por tanto un mayor peso en esta CP, tal y como sucede en la composici\u00f3n real del S&P 500, por lo que es coherente determinar esta primera CP como la cartera de mercado).**\n\nPara ver que efectivamente esta primera CP se comporta como el mercado, comparamos el desempe\u00f1o de un porfolio generado por los pesos de la primera Componente con la evoluci\u00f3n real del mercado.\n\nPor lo tanto, lo siguiente que debemos hacer es **obtener los pesos de cada activo sobre cada Componente Principal** (o porfolio). Para hacer esto, simplemente debemos dividir el coeficiente de cada activo en una CP determinada, entre la suma de todos los coeficientes de la CP (es decir, dividir cada coeficiente del autovector entre la suma de todos ellos)","8555aad6":"As seen in *Figure 2*, of the total 14 years collected in the database (from January 2000 to December 2013), data up to March 26, 2013 were included in the training sample and the remainder in the test sample.\n\nDue to the size of the DF (more than 400 assets), a correlation chart would not be very useful, so I will now perform the Principal Component Analysis directly. For this purpose, NAs cannot be taken into account, so they must be removed.\n\n---\n\nComo se ve en el gr\u00e1fico 2, del total de 14 a\u00f1os que recoge la base de datos (desde enero del a\u00f1o 2000 hasta diciembre del 2013), los datos hasta el 26 de marzo de 2013 se han incluido en la muestra de entrenamiento y el resto en la muestra de prueba.\n\nDebido al tama\u00f1o del DF (m\u00e1s de 400 activos) realizar un gr\u00e1fico de correlaciones no tendr\u00eda mucha utilidad, por lo que paso a realizar el An\u00e1lisis de Componentes Principales directamente. Para ello, no se pueden tener en cuenta los NA, por lo que deben ser eliminados"}}