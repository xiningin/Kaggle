{"cell_type":{"91c5ea34":"code","a6eadc58":"code","f85d94e1":"code","863febc0":"code","b4856a3a":"code","1ee139ed":"code","47053bc7":"code","c34e8e48":"code","51316c60":"code","b6e3922f":"code","9fc430d7":"code","9aeee396":"code","bbe7146d":"code","124782ed":"code","f4837e98":"code","3060ca25":"code","bee4ca2a":"code","5db50cf3":"code","bfdb2684":"code","c5590dce":"code","68808db0":"code","7402c3d0":"code","ac5ff9e1":"code","f9a4a6c8":"code","16389c54":"code","c345ceab":"code","989edbf3":"code","02698aaa":"code","a73b3915":"code","2fee7f1b":"code","88cf16b3":"code","1aaa5ba6":"code","976dc530":"code","1495342e":"code","0bb1ec58":"code","67b5804e":"code","1c574408":"code","d11bf064":"code","4da2d58c":"code","3946b769":"code","01f680d3":"code","f1ce15f1":"code","cd87be28":"code","955afe6c":"code","bdaed491":"code","5cb3becb":"markdown","6187595e":"markdown","a6cb2056":"markdown","c8349c89":"markdown","f21ac6a8":"markdown","1a03d39f":"markdown","4c9a8ea0":"markdown","2b06422b":"markdown","7834ed25":"markdown","6dd14044":"markdown","1c55eb85":"markdown","6036e232":"markdown","c7057d95":"markdown","eba8130e":"markdown"},"source":{"91c5ea34":"# For data analysis\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# For ordinal encoding categorical variables, splitting data\nfrom sklearn.model_selection import train_test_split,KFold\nfrom sklearn.preprocessing import OrdinalEncoder\n\n# For feature engineering\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom category_encoders import MEstimateEncoder\n\n# For pipelines\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\n# For modeling\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\n\n# For validation\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nimport xgboost as xgb","a6eadc58":"# Load the training and test data\ntrain = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\", index_col=0)\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\", index_col=0)\n\n# Preview the data\ntrain.head()","f85d94e1":"# Summary of dataset\ntrain.describe()","863febc0":"train.info()","b4856a3a":"# Separate target from features\ny = train['target']\nfeatures = train.drop(['target'], axis=1)\n\n# Preview features\nfeatures.head()","1ee139ed":"# Split data into numerical and categorical data\nnumerical_data=train.select_dtypes(exclude=['object'])\ncategorical_data=train.select_dtypes(include=['object'])","47053bc7":"fig = plt.figure(figsize = (20, 20))\nj = 0\nfor i in numerical_data.columns:\n    plt.subplot(4, 4, j+1)\n    j += 1\n    sns.histplot(numerical_data[i],bins=10,color='dodgerblue',stat=\"density\")\n    sns.kdeplot(numerical_data[i], color=\"r\")\nfig.suptitle('Numerical Data Analysis',fontsize='25')\nfig.tight_layout()\nfig.subplots_adjust(top=0.95)\nplt.show()","c34e8e48":"fig = plt.figure(figsize = (20, 15))\nj = 0\nfor i in categorical_data.columns:\n    plt.subplot(3, 4, j+1)\n    j += 1\n    categorical_data_copy=categorical_data.copy()\n    categorical_data_sorted=categorical_data_copy.sort_values(i) \n    sns.histplot(categorical_data_sorted[i],discrete=True,shrink=0.8,color='deeppink')\nfig.suptitle('Categorical Data Analysis',fontsize='25')\nfig.tight_layout()\nfig.subplots_adjust(top=0.95)\nplt.show()","51316c60":"%matplotlib inline\ncorr = numerical_data.corr()\nplt.figure(figsize=(12,10))\nsns.heatmap(corr,annot=True)","b6e3922f":"# List of categorical columns\nobject_cols = [col for col in features.columns if 'cat' in col]\nprint(object_cols)","9fc430d7":"# encode categorical columns\nX = features.copy()\nX_test = test.copy()\nfor name in object_cols:\n    X[name] = X[name].astype(\"category\")\n    # Add a None category for missing values\n    if \"None\" not in X[name].cat.categories:\n        X[name].cat.add_categories(\"None\",inplace=True)\n        \nfor name in object_cols:\n    X_test[name] = X_test[name].astype(\"category\")\n    # Add a None category for missing values\n    if \"None\" not in X_test[name].cat.categories:\n        X_test[name].cat.add_categories(\"None\",inplace=True)\n\n# Preview the encoded features\nX.head()","9aeee396":"X_train, X_val, y_train, y_val = train_test_split(X, y,train_size=0.8,test_size=0.2,random_state=0)","bbe7146d":"# Set Matplotlub defaults\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\",autolayout=True)\nplt.rc(\"axes\",labelweight=\"bold\",labelsize=\"large\",titleweight=\"bold\",titlesize=14,titlepad=10)","124782ed":"def score_dataset(X,y,model=XGBRegressor()):\n    for colname in X.select_dtypes([\"category\"]):\n        X[colname]=X[colname].cat.codes\n    score=cross_val_score(model,X,y,cv=5,scoring=\"neg_mean_squared_error\")\n    score=-1*score.mean()\n    score=np.sqrt(score)\n    return score","f4837e98":"# Baseline score\nX_copy=X.copy()\ny_copy=y.copy()\n#baseline_score=score_dataset(X_copy,y_copy)\n#print(f\"Baseline score: {baseline_score:.5f} RMSE\")","3060ca25":"def make_mi_scores(X,y):\n    X=X.copy()\n    for colname in X.select_dtypes([\"object\",\"category\"]):\n        X[colname],_=X[colname].factorize()\n    discrete_features=[pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n    mi_scores=mutual_info_regression(X,y,discrete_features=discrete_features,random_state=0)\n    mi_scores=pd.Series(mi_scores,name=\"MI Scores\",index=X.columns)\n    mi_scores=mi_scores.sort_values(ascending=False)\n    return mi_scores\n\ndef plot_mi_scores(scores):\n    scores=scores.sort_values(ascending=True)\n    width=np.arange(len(scores))\n    ticks=list(scores.index)\n    plt.barh(width,scores)\n    plt.yticks(width,ticks)\n    plt.title(\"Mutual Information Scores\")","bee4ca2a":"X_copy=X.copy()\ny_copy=y.copy()\nmi_scores=make_mi_scores(X_copy,y_copy)\nmi_scores","5db50cf3":"plt.figure(dpi=100,figsize=(8,5))\nplot_mi_scores(mi_scores)","bfdb2684":"feature_list=[\"cat2\",\"cat6\"]\nX_copy=X.copy()\nX_dropped=X_copy.drop(feature_list,axis=1)\n#score_dataset(X_dropped,y)","c5590dce":"X_test_copy=X_test.copy()\nX_test_dropped=X_test_copy.drop(feature_list,axis=1)","68808db0":"sns.relplot(x=\"value\",y=\"target\",col=\"variable\",data=train.melt(id_vars=\"target\",value_vars=[\"cat2\",\"cat6\"]),facet_kws=dict(sharex=False),);","7402c3d0":"sns.catplot(x=\"cat2\",y=\"target\",data=train,kind=\"boxen\")\nsns.catplot(x=\"cat6\",y=\"target\",data=train,kind=\"boxen\")","ac5ff9e1":"X_dropped[\"cont4\"].skew()","f9a4a6c8":"X_created1=X_dropped.copy()\nX_created1[\"Logcont4\"] = X_created1.cont4.apply(np.cbrt)\n#Plot a comparison\nfig, axs = plt.subplots(1, 2, figsize=(8, 4))\nsns.kdeplot(X_dropped.cont4, shade=True, ax=axs[0])\nsns.kdeplot(X_created1.Logcont4, shade=True, ax=axs[1])","16389c54":"X_created1[\"Logcont4\"].skew()","c345ceab":"X_created1=X_created1.drop([\"cont4\"],axis=1)\nX_created1.head()","989edbf3":"#score_dataset(X_created1,y)","02698aaa":"X_test_created1=X_test_dropped.copy()\nX_test_created1[\"Logcont4\"] = X_test_created1.cont4.apply(np.cbrt)\nX_test_created1=X_test_created1.drop([\"cont4\"],axis=1)\nX_test_created1.head()","a73b3915":"feature_list=[\"cont0\",\"cont1\",\"cont2\",\"cont6\",\"cont12\"]\nX_created2=X_created1.copy()\nX_created2[\"Components\"]=X_created2[feature_list].gt(0).sum(axis=1)\nX_created2[feature_list+[\"Components\"]].head()","2fee7f1b":"X_created2.head()","88cf16b3":"#score_dataset(X_created2,y)","1aaa5ba6":"feature_list=[\"cont0\",\"cont1\",\"cont2\",\"cont6\",\"cont12\"]\nX_test_created2=X_test_created1.copy()\nX_test_created2[\"Components\"]=X_test_created2[feature_list].gt(0).sum(axis=1)\nX_test_created2[feature_list+[\"Components\"]].head()","976dc530":"X_test_created2.head()","1495342e":"categorical_cols=[col for col in X_created2.columns if \"cat\" in col]\nprint(categorical_cols)","0bb1ec58":"X_created2.select_dtypes([\"category\"]).nunique()","67b5804e":"X_label = X_created2.copy()\nfor colname in [\"cat0\",\"cat1\"]:\n    X_label[colname] = X_label[colname].cat.codes\nX_label.head()","1c574408":"X_test_label = X_test_created2.copy()\nfor colname in [\"cat0\",\"cat1\"]:\n    X_test_label[colname] = X_test_label[colname].cat.codes\nX_test_label.head()","d11bf064":"class CrossFoldEncoder:\n    def __init__(self, encoder, **kwargs):\n        self.encoder_ = encoder\n        self.kwargs_ = kwargs  # keyword arguments for the encoder\n        self.cv_ = KFold(n_splits=5)\n\n    # Fit an encoder on one split and transform the feature on the\n    # other. Iterating over the splits in all folds gives a complete\n    # transformation. We also now have one trained encoder on each\n    # fold.\n    def fit_transform(self, X, y, cols):\n        self.fitted_encoders_ = []\n        self.cols_ = cols\n        X_encoded = []\n        for idx_encode, idx_train in self.cv_.split(X):\n            fitted_encoder = self.encoder_(cols=cols, **self.kwargs_)\n            fitted_encoder.fit(\n                X.iloc[idx_encode, :], y.iloc[idx_encode],\n            )\n            X_encoded.append(fitted_encoder.transform(X.iloc[idx_train, :])[cols])\n            self.fitted_encoders_.append(fitted_encoder)\n        X_encoded = pd.concat(X_encoded)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded\n\n    # To transform the test data, average the encodings learned from\n    # each fold.\n    def transform(self, X):\n        from functools import reduce\n\n        X_encoded_list = []\n        for fitted_encoder in self.fitted_encoders_:\n            X_encoded = fitted_encoder.transform(X)\n            X_encoded_list.append(X_encoded[self.cols_])\n        X_encoded = reduce(\n            lambda x, y: x.add(y, fill_value=0), X_encoded_list\n        ) \/ len(X_encoded_list)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded","4da2d58c":"categorical_cols.remove(\"cat0\")\ncategorical_cols.remove(\"cat1\")\nencoder = CrossFoldEncoder(MEstimateEncoder, m=1)\nX_encoded = encoder.fit_transform(X_label, y, cols=categorical_cols)\nX_encoded.columns= categorical_cols\nX_final=X_label.copy()\nX_final=X_final.join(X_encoded, how='left', lsuffix='_left', rsuffix='_encoded')\nX_final.head()","3946b769":"unwanted_columns=[col for col in X_final if \"left\" in col]\nX_final=X_final.drop(unwanted_columns,axis=1)\nX_final.head()","01f680d3":"X_test_final=X_test_label.copy()\nX_test_final=X_test_final.join(encoder.transform(X_test_final))\nX_test_final=X_test_final.drop([\"cat3\",\"cat4\",\"cat5\",\"cat7\",\"cat8\",\"cat9\"],axis=1)\nX_test_final.head()","f1ce15f1":"#score_dataset(X_final,y)","cd87be28":"xgb_params = dict(\n    max_depth=2,           \n    learning_rate=0.06840493160210372,    \n    n_estimators=7930,     \n    min_child_weight=10,    \n    colsample_bytree=0.8577205532186647,  \n    subsample=0.9847699840023377,         \n    reg_alpha=10.76107,        \n    reg_lambda=0.00010135348959308447,       \n    num_parallel_tree=1,   \n)\n\nxgb = XGBRegressor(**xgb_params)\n#score_dataset(X_final, y, xgb)","955afe6c":"\"\"\"import optuna\n\ndef objective(trial):\n    xgb_params = dict(\n        max_depth=trial.suggest_int(\"max_depth\", 2, 10),\n        learning_rate=trial.suggest_float(\"learning_rate\", 1e-4, 1e-1, log=True),\n        n_estimators=trial.suggest_int(\"n_estimators\", 1000, 8000),\n        min_child_weight=trial.suggest_int(\"min_child_weight\", 1, 10),\n        colsample_bytree=trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n        subsample=trial.suggest_float(\"subsample\", 0.2, 1.0),\n        reg_alpha=trial.suggest_float(\"reg_alpha\", 1e-4, 1e2, log=True),\n        reg_lambda=trial.suggest_float(\"reg_lambda\", 1e-4, 1e2, log=True),\n    )\n    xgb = XGBRegressor(**xgb_params)\n    return score_dataset(X_final, y, xgb)\n\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(objective, n_trials=20)\nxgb_params = study.best_params\"\"\"","bdaed491":"xgb.fit(X_final,y)\npreds=xgb.predict(X_test_final)\noutput = pd.DataFrame({'Id': X_test_final.index,\n                       'target': preds})\noutput.to_csv('submission.csv', index=False)","5cb3becb":"The correlation between features are realtively low, so none of the features are redundant, thus none of them will be eliminated from the dataset.\n## Step 4: Prepare the data\nNext, we'll need to handle the categorical columns (cat0, cat1, ... cat9).","6187595e":"Since the MI scores for cat2 and cat6 are relatively low compared to other features, we will drop these features from the dataset.","a6cb2056":"# 30 Days of ML Competition","c8349c89":"We will count the total number of inputs greater than 0 for the features cont0, cont1, cont2, cont6 and cont12.","f21ac6a8":"From the information above, there are no missing values in this dataset. Thus, we can skip the part on imputing missing values. <br>\nThe next code cell separates the target (which we assign to y) from the training features (which we assign to features).","1a03d39f":"## Step 2: Load the data\nNext, we'll load the training and test data.\n\nWe set index_col=0 in the code cell below to use the id column to index the DataFrame.","4c9a8ea0":"## Create Features","2b06422b":"## Step 3: Data Visualisation\n## Histogram","7834ed25":"## Correlation matrix plot","6dd14044":"## Target Encoding","1c55eb85":"## Step 1: Import important libraries","6036e232":"Since we can see that cont4 is skewed compare to the other features from the numerical data anlalysis plot above, we will use cube root to normalise cont4.","c7057d95":"## Step 6: Hyperparameter Tuning","eba8130e":"## Step 5: Feature Engineering\n## Feature Utility Scores"}}