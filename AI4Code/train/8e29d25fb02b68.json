{"cell_type":{"7204aee0":"code","2cdb248d":"code","6589ad29":"code","1c6564a1":"code","d793fef7":"code","4138847b":"code","793b8c02":"code","4a717e7b":"code","5e097b16":"code","e16d74c7":"code","674d4cb6":"code","45cbb208":"code","70e3db86":"code","de442aa2":"code","6202beb8":"markdown","38aa3a27":"markdown","fa8b161a":"markdown","02340116":"markdown","5736e31a":"markdown","d726043a":"markdown","62bdcf8f":"markdown","a3fd98d0":"markdown","44d81b65":"markdown","d7a70149":"markdown","18a192ca":"markdown","3d198f8d":"markdown"},"source":{"7204aee0":"import numpy as np # linear algebra\nimport scipy.sparse","2cdb248d":"NrRows=10\nNrCols=12\ndensity=0.2\nMaxRating=6","6589ad29":"R_sparse=np.around(MaxRating*scipy.sparse.rand(NrRows,NrCols,density=density,format='csr'))\nR_sparse.todense().astype('int')","1c6564a1":"np.linalg.svd?","d793fef7":"U,s,V = np.linalg.svd(R_sparse.todense())\nU.shape,s.shape,V.shape","4138847b":"m=np.min(R_sparse.shape)\nn=np.max(R_sparse.shape)\nD = np.zeros((n,n), dtype='float')\nD[:len(s), :len(s)] = np.diag(s)\n\nU = np.hstack((U,np.zeros((U.shape[0],n-m))))\nU=np.matrix(U).astype('float')\nD=np.matrix(D).astype('float')\nV=np.matrix(V).astype('float')","793b8c02":"U.shape,D.shape,V.shape","4a717e7b":"display(np.around(U*D*V,5))\nR_sparse.todense()","5e097b16":"P = U * np.sqrt(D)\nQ = np.sqrt(D)*V\nRhat = P*Q\nnp.around(Rhat)","e16d74c7":"k=10\nassert k<=m, f'Die Matrix R_sparse hat einen zu kleinen Rang. k kann h\u00f6chstens {min(R_sparse.shape)} sein.'","674d4cb6":"Rhat = P[:,:k]*Q[:k,:]\nnp.around(Rhat,1)","45cbb208":"MSE = np.sum(np.array(R_sparse - Rhat)**2)\nMSE","70e3db86":"mse_list=[]\nfor k in range(1,m+1):\n    Rhat = P[:,:k]*Q[:k,:]\n    MSE = np.sum(np.array(R_sparse - Rhat)**2)\n    mse_list.append(MSE)","de442aa2":"import matplotlib.pyplot as plt\nplt.plot(mse_list);\nplt.title('Mean Squared Error der SVD-Approximation an $R$');\nplt.ylabel('MSE');\nplt.xlabel('Rang $k$');","6202beb8":"Nun rekonstruieren wir die urspr\u00fcngliche Matrix:","38aa3a27":"Die Numpy SVD-Version spart sich m\u00f6glichst viele Null-Zeilen. Dies hat den Nachteil, dass die Matrixmultiplikation nicht mehr direkt funktioniert. Die folgenden Zeilen f\u00fcgen einige Null-Spalten und -Zeilen hinzu. Dies w\u00e4re in einem realistischen Setting nat\u00fcrlich anders gel\u00f6st.","fa8b161a":"F\u00fcr kleinere $k$-Werte ergibt sich ein kleineres Overfitting:","02340116":"**Aufgabe**: Ver\u00e4ndern Sie den Wert von $k$ und beobachten Sie, wie sich die Rating Matrix $R$ ver\u00e4ndert. Je kleiner $k$, desto kleiner der Rang von $\\hat{R}$, und desto schlechter die Approximation an $R$.","5736e31a":"# Recommendersysteme und SVD\nRecommendersysteme (RS) haben einen betr\u00e4chtlichen kommerziellen Wert. Es gibt einfache RS wie etwa: \n\n- den popularity recommender, welcher allen Usern die Top k Items vorschl\u00e4gt (z.B. Music Charts)\n- Similarity Recommender, welche \u00c4hnlichkeitsmasse benutzen, um entweder \u00e4hnlichen Usern gegenseitig die \"gelikten\" items vorzuschlagen, oder einem User \u00e4hnliche Items vorzuschlagen wie jene, die er gelikt hat.\n\nEin bekanntes Problem dabei ist der sogenannte \"cold start\": Benutzer, welche noch wenig mit dem System interagiert haben, erhalten eher schlechte Empfehlungen.\n\nIn diesem Notebook schauen wir uns einen dritten Recommendertyp an, den sog. Matrix Factorisation Recommender. Eine Version davon hatte hatte mal den ber\u00fchmten Netflix-Preis gewonnen. In den Grundz\u00fcgen ist dieser Algorithmus eng mit der Singul\u00e4rwertzerlegung (SVD-Zerlegung) verwandt. Wir zeigen hier, wie dies geht.\n\nDaf\u00fcr gehen wir erst mal von einer fiktivien Rating-Matrix $R_{sparse}$ aus, welche wir synthetisch generieren:","d726043a":"* Wir basteln eine Matrix mit vielen Nullen und Zahlen zwischen 0 und `MaxRating`:","62bdcf8f":"W\u00e4hlen wir nun den *Rang* $k$ der Matrix `R`! Dies ist zugleich die kleinere der Dimensionen der Matrizen P und Q (ein Theorem der linearen Algebra). `Rhat` soll die Rankingmatrix R m\u00f6glichst gut approximieren, aber nur Rang $k$ haben:","a3fd98d0":"Die Zeile `Rhat = P[:,:k]*Q[:k,:]` bedeutet folgendes: Jeder User und jedes Item wird durch $k$ Zahlen, den sogenannten latenten Faktoren beschrieben. Das Rating $\\hat{R}_{ui}$ des Users $u$ f\u00fcr das Item $i$ berechnet sich dann aus dem Skalarprodukt zwischen den Vektoren der latenten Faktoren des Users und des Items:\n$$\\hat{r}_{ui}=p_u^T\\cdot q_i$$","44d81b65":"F\u00fcr k gleich dem Minimum aus `NrRows` und `NrCols` ergibt sich eine fast exakte Reproduktion:","d7a70149":"Nun wenden wir eine SVD auf die (Trainings-)Daten an:","18a192ca":"## Precision@k\nMit Precision@k bezeichnet man die Precision, die ein Recommender erreicht, wenn er eine Liste von $k$ Elementen vorschl\u00e4gt. Berechnet wird sie als $x\/k$ wobei $x$ die Anzahl der \"gelikten\" Items in der Empfehlungsliste angibt. Je mehr Items in der Datenbank, desto kleiner wird typischerweise die Precision@k.\n\n**Aufgabe:** Ein Recommender habe 10 Items zur Empfehlung zur Verf\u00fcgung. Wie gross ist wohl seine Precision@10? Wie gross ist die Precision@2, wenn der Recommender v\u00f6llig zuf\u00e4llig empfiehlt und der Benutzer zwei der 10 Items mag?","3d198f8d":"Am besten schreibt man die Matrix-Faktorisierung nicht als drei Faktoren U,D und V, sondern nur als P und Q, die sogenannten latenten Faktoren der User und Items. Mit `Rhat` (eigentlich $\\hat{R}$) bezeichnen wir die gesch\u00e4tzten Ratings. Hier haben alle noch nicht gerateten Items das Rating \"0\"."}}