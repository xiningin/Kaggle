{"cell_type":{"49046b08":"code","f3ca2158":"code","0849bcbd":"code","8d0ee817":"code","a2734b92":"code","bd76c8c2":"code","41788577":"code","b27e3a8e":"code","b3c295bd":"code","4c552c36":"code","0980fe39":"code","8c881cd5":"code","9502a2fa":"code","4d505b9b":"code","29992242":"code","d445c30d":"code","a0e5f774":"code","1e9ff0d8":"code","861f945e":"code","9fa805aa":"code","f8760065":"code","30412e58":"code","f53146cb":"code","94381ec3":"code","a85b0046":"code","0037954a":"code","c224f85c":"code","78c4fd16":"code","30077476":"code","d5789350":"code","31e61fc7":"code","91502fbb":"code","15bb32d0":"markdown","95a7e21e":"markdown","fa4db86d":"markdown","836afe6e":"markdown","0a6d371a":"markdown","7d0b0e73":"markdown","1b1afbca":"markdown","133cb940":"markdown","fda811d5":"markdown","a57cca61":"markdown","a2ae1414":"markdown","bcdda36f":"markdown","98f4b6ed":"markdown","ffe9028c":"markdown","5aee5341":"markdown","f78cd2b7":"markdown","dd4ab5b3":"markdown","3e1a3f7b":"markdown","f912ba40":"markdown","2e7fe5ef":"markdown"},"source":{"49046b08":"from collections import defaultdict\nimport pandas as pd\nimport numpy as np\nimport scipy\nfrom scipy.sparse.linalg import svds\nimport matplotlib.pyplot as plt\nimport surprise as sp\nimport time","f3ca2158":"#Importing the CSVs to Dataframe format\nUsersDF = pd.read_csv('..\/input\/users_cleaned.csv')\nAnimesDF = pd.read_csv('..\/input\/anime_cleaned.csv')\nScoresDF = pd.read_csv('..\/input\/animelists_cleaned.csv')\n\nAnimesDF.head()","0849bcbd":"UsersDF.head()","8d0ee817":"ScoresDF.head()","a2734b92":"#Since ScoresDF is a huge DF (2GB of data) I`ll only take the columns that are important for the recommendation system\nScoresDF = ScoresDF[['username', 'anime_id', 'my_score', 'my_status']]","bd76c8c2":"ScoresDF['my_score'].describe().apply(lambda x: format(x, '.2f')).reset_index()","41788577":"#Analysing all the possible values for the score, this will be used as a parameter later on\nlower_rating = ScoresDF['my_score'].min()\nupper_rating = ScoresDF['my_score'].max()\nprint('Range of ratings vary between: {0} to {1}'.format(lower_rating, upper_rating))","b27e3a8e":"#Only filtering animes in which people actually watched, are watching or are on hold as they are the most revelant for the rec sys\n#RelevantScoresDF = ScoresDF[(ScoresDF['my_status'] == 1) | (ScoresDF['my_status'] == 2) | (ScoresDF['my_status'] == 3)]","b3c295bd":"#Counting how many relevant scores each user have done, resetting the index (so the series could become a DF again) and changing the column names\nUsersAndScores = ScoresDF['username'].value_counts().reset_index().rename(columns={\"username\": \"animes_rated\", \"index\": \"username\"})","4c552c36":"UsersSampled = UsersDF.sample(frac = .01, random_state = 2)\nUsersSampled.head()","0980fe39":"UsersAndScoresSampled = pd.merge(UsersAndScores, UsersSampled, left_on = 'username', right_on = 'username', how = 'inner')","8c881cd5":"#Grouping users whom had the same amount of animes rated\nUserRatedsAggregated = UsersAndScoresSampled['animes_rated'].value_counts().reset_index().rename(columns={\"animes_rated\": \"group_size\", \"index\": \"animes_rated\"}).sort_values(by=['animes_rated'])","9502a2fa":"#Counting how many relevant scores each anime has, resetting the index (so the series could become a DF again) and changing the column names\nRatedsPerAnime = ScoresDF['anime_id'].value_counts().reset_index().rename(columns={\"anime_id\": \"number_of_users\", \"index\": \"anime_id\"})\nRatedsPerAnime.head()","4d505b9b":"#Grouping users whom had the same amount of animes rated\nAnimeRatedsAggregated = RatedsPerAnime['number_of_users'].value_counts().reset_index().rename(columns={\"number_of_users\": \"group_size\", \"index\": \"number_of_users\"}).sort_values(by=['number_of_users'])\nAnimeRatedsAggregated.head(n = 30)","29992242":"#Creating the plots so we can gather information about the distribution of ratings in the sample\nplt.suptitle(\"Distribution of users\", fontsize=13, fontweight=0, color='black', style='italic', y=1.02)\nplt.plot('animes_rated', 'group_size', data = UserRatedsAggregated, color = 'blue')\nplt.xlabel('Number of animes rated')\nplt.ylabel('Number of people in that group')\nplt.xlim(left = 0, right = 2000)\nplt.show()","d445c30d":"#Creating the plots so we can gather information about the distribution of ratings in the sample\nplt.suptitle(\"Distribution of animes\", fontsize=13, fontweight=0, color='black', style='italic', y=1.02)\nplt.plot('number_of_users', 'group_size', data = AnimeRatedsAggregated, color = 'olive')\nplt.xlabel('Number of users rated')\nplt.ylabel('Number of animes in that group')\nplt.xlim(left = 0, right = 2000)\nplt.show()","a0e5f774":"#Creating a dataframe of users  and animes with more than 10 interactions\nUserRatedsCutten = UsersAndScoresSampled[UsersAndScoresSampled['animes_rated'] >= 10]\nAnimeRatedsCutten = RatedsPerAnime[RatedsPerAnime['number_of_users'] >= 10]\n#Joining (merging) our new dataframes with the interactions one (this will already deal with the sample problem,\n#as it is an inner join). The \"HotStart\" name comes from a pun about solving the \"Cold Start\" issue\nScoresDFHotStart = pd.merge(ScoresDF, UserRatedsCutten, left_on = 'username', right_on = 'username', how = 'inner')\nScoresDFHotStart = pd.merge(ScoresDFHotStart, AnimeRatedsCutten, left_on = 'anime_id', right_on = 'anime_id', how = 'inner')","1e9ff0d8":"#Grouping the different scores and resetting the index (so the series could become a DF again) \nAnimeRates = ScoresDF['my_score'].value_counts().reset_index().sort_values('index')\nplt.plot('index', 'my_score', data = AnimeRates, color = 'red')\nplt.xticks(np.arange(11))\nplt.ticklabel_format(axis = 'y', style = 'plain')\nplt.xlabel('Score')\nplt.ylabel('Frequency of that score')\nplt.show()","861f945e":"#Just for the record, lets see the difference in numbers between our initial DF and the sampled and cleaned one\n\nprint('The initial dataframe has {0} registers and the sampled one has {1} rows.'.format(ScoresDF['username'].count(), ScoresDFHotStart['username'].count()))","9fa805aa":"def precision_recall_at_k(predictions, k=10, threshold= 7):\n    '''Return precision and recall at k metrics for each user.'''\n\n    # First map the predictions to each user.\n    # Predictions: Traz uma lista de 5 campos dentro de uma tupla com as seguintes infos: User_ID, Item_ID, True_ID, Est_ID, Details\n    user_est_true = defaultdict(list)\n    for uid, _, true_r, est, _ in predictions:\n        user_est_true[uid].append((est, true_r))\n    # Creates a dict with the key being a user and the value bringing the estimated rating and the true rating.\n\n    precisions = dict()\n    recalls = dict()\n    for uid, user_ratings in user_est_true.items():\n\n        # Sort user ratings by estimated value\n        user_ratings.sort(key=lambda x: x[0], reverse=True)\n\n        # Number of relevant items\n        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n\n        # Number of recommended items in top k\n        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n\n        # Number of relevant and recommended items in top k\n        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold))\n                              for (est, true_r) in user_ratings[:k])\n\n        # Precision@K: Proportion of recommended items that are relevant\n        precisions[uid] = n_rel_and_rec_k \/ n_rec_k if n_rec_k != 0 else 1\n\n        # Recall@K: Proportion of relevant items that are recommended\n        recalls[uid] = n_rel_and_rec_k \/  n_rel if n_rel != 0 else 1\n\n    return precisions, recalls","f8760065":"random_state = 42\nreader = sp.Reader(rating_scale=(0, 10))\ndata = sp.Dataset.load_from_df(ScoresDFHotStart[['username', 'anime_id', 'my_score']], reader)\ntrainset, testset = sp.model_selection.train_test_split(data, test_size=.25, random_state = random_state)\nanalysis = defaultdict(list)\n\ntest_dict = {'SVD' : sp.SVD(random_state=random_state), 'SlopeOne' : sp.SlopeOne(), 'NMF' : sp.NMF(random_state=random_state), 'NormalPredictor' : sp.NormalPredictor(), 'KNNBaseline' : sp.KNNBaseline(random_state=random_state), 'KNNBasic' : sp.KNNBasic(random_state=random_state), 'KNNWithMeans' : sp.KNNWithMeans(random_state=random_state), 'KNNWithZScore' : sp.KNNWithZScore(random_state=random_state), 'BaselineOnly' : sp.BaselineOnly(), 'CoClustering': sp.CoClustering(random_state=random_state)}\n\nfor key, value in test_dict.items():\n    start = time.time()    \n    value.fit(trainset)\n    predictions = value.test(testset)\n\n    rmse = sp.accuracy.rmse(predictions)\n    precisions, recalls = precision_recall_at_k(predictions, k=10, threshold=7)\n    precision_avg = sum(prec for prec in precisions.values()) \/ len(precisions)\n\n    analysis[value] = (key, rmse, precision_avg, time.time() - start)\n\nprint(analysis)","30412e58":"analysis_df = pd.DataFrame.from_dict(analysis, orient = 'index', columns = ['Algorithm', 'RMSE', 'Precision@10', 'Time to run (in seconds)']).reset_index()\n\n#analysis_df['Algorithm'] = ['SVD', 'SlopeOne', 'NMF', 'NormalPredictor', 'KNNBaseline', 'KNNBasic', 'KNNWithMeans', 'KNNWithZScore', 'BaselineOnly', 'CoClustering']\nanalysis_df = analysis_df[['Algorithm', 'RMSE', 'Precision@10', 'Time to run (in seconds)']]\nanalysis_df = analysis_df.sort_values(by=['Precision@10'], ascending = False)\nanalysis_df['RMSE^-1'] = analysis_df['RMSE'] ** -1\nanalysis_df.head(n = 15)","f53146cb":"ax = analysis_df.set_index('RMSE^-1')['Precision@10'].plot(style='o', c = 'DarkBlue', figsize = (15, 20))\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \ndef label_point(x, y, val, ax):\n    a = pd.concat({'x': x, 'y': y, 'val': val}, axis=1)\n    for i, point in a.iterrows():\n        ax.text(point['x'], point['y'], str(point['val']))\n\nlabel_point(analysis_df['RMSE^-1'], analysis_df['Precision@10'], analysis_df['Algorithm'], ax)","94381ec3":"als_param_grid = {'bsl_options': {'method': ['als'],\n                              'reg_i': [5, 10, 15],\n                              'reg_u': [10, 15, 20],\n                              'n_epochs': [5, 10, 15, 20]\n                              }\n              }\n\nsgd_param_grid = {'bsl_options': {'method': ['sgd'],\n                              'reg': [0.01, 0.02, 0.03],\n                              'n_epochs': [5, 10, 15, 20],\n                              'learning_rate' : [0.001, 0.005, 0.01]\n                              }\n              }\n\nals_gs = sp.model_selection.GridSearchCV(sp.BaselineOnly, als_param_grid, measures=['rmse'], cv = 3, joblib_verbose = 0)\n\nsgd_gs = sp.model_selection.GridSearchCV(sp.BaselineOnly, sgd_param_grid, measures=['rmse'], cv = 3, joblib_verbose = 0)","a85b0046":"als_gs.fit(data)\n\n# best RMSE score\nprint(als_gs.best_score['rmse'])\n\n# combination of parameters that gave the best RMSE score\nprint(als_gs.best_params['rmse'])","0037954a":"sgd_gs.fit(data)\n\n# best RMSE score\nprint(sgd_gs.best_score['rmse'])\n\n# combination of parameters that gave the best RMSE score\nprint(sgd_gs.best_params['rmse'])","c224f85c":"trainset = data.build_full_trainset()\nalgo = sp.BaselineOnly()\nalgo.fit(trainset)\ntestset = trainset.build_anti_testset()\npredictions = algo.test(testset)\n    \nlast_predictions = pd.DataFrame(predictions, columns=['uid', 'iid', 'rui', 'est', 'details'])\nlast_predictions.drop('rui', inplace = True, axis = 1)","78c4fd16":"def bringing_first_n_values(df, uid, n=10):\n    df = df[df['uid'] == uid].nlargest(n, 'est')[['uid', 'iid', 'est']]\n    df = pd.merge(df, AnimesDF, left_on = 'iid', right_on = 'anime_id', how = 'left')\n    return df[['uid', 'est', 'title', 'genre']]","30077476":"bringing_first_n_values(last_predictions, 'Tomoki-sama')","d5789350":"sim_options = {'name': 'pearson_baseline', 'user_based': False}\nalgo_items = sp.KNNBaseline(sim_options=sim_options)\nalgo_items.fit(trainset)","31e61fc7":"def get_item_recommendations(anime_title, anime_id=100000, k=10):\n    if anime_id == 100000:     \n        anime_id = AnimesDF[AnimesDF['title'] == anime_title]['anime_id'].iloc[0]\n        \n    iid = algo_items.trainset.to_inner_iid(anime_id)\n    neighbors = algo_items.get_neighbors(iid, k=k)\n    raw_neighbors = (algo.trainset.to_raw_iid(inner_id) for inner_id in neighbors)\n    df = pd.DataFrame(raw_neighbors, columns = ['Anime_ID'])\n    df = pd.merge(df, AnimesDF, left_on = 'Anime_ID', right_on = 'anime_id', how = 'left')\n    return df[['Anime_ID', 'title', 'genre']]","91502fbb":"get_item_recommendations('Dragon Ball Z', k=30)","15bb32d0":"# The ultimate otaku recommender system","95a7e21e":"Here we can imply being that the data has a peak at the score of number '7', probably when someone really likes a show it rates them at that minimum. Interesting to see too that we have a lot of rated '0', Otakus are really demanding apparently.","fa4db86d":"# Training, testing and results structure\n\nMy initial idea was to create a user-based algorithm, where you could input a specific username and it would return to you the top@K recommendations, but I also thought of developing an item-based system, as it would be a lot better to have an empiral understanding if all of it is working or not.\n\nSo, how can I know if my algorithm is performing well or not? For this problem we split the data in a **Training** and a **Test** dataset (with a 75\/25 proportion). The idea behind it is to try to guess the score for animes where we actually can do a comparison. For instance, lets say you gave an score of 10, 9, 8 and 3 to the following shows: Dragon Ball Z, Pokemon, Naruto and One Piece. We then split the first three animes, learn by it and try to estimate what would be your One Piece score, and the difference of the estimated score and the true score would tell us the performance of the recommender.\n\nAfter spliting the data, I've constructed a function where we can compare pretty much any algo from the awesome [Surprise](http:\/\/surpriselib.com\/) lib (the only one I left out was the SVD++ for performance reasons), and then, comparing all the metrics (which I'll explain in the next cell) I chose the best recommender for this specific case.","836afe6e":"After generating the list with all possible recommendations, I created another function that shows only the k most valuable recommendations for each customer. That way we can simulate how an actually recommender system works.","0a6d371a":"As I already trained the model and created a function that brings a show based on it's name (if you want to know the exactly name copy how it is written in www.myanimelist.net) let's test it with one of the most famous animes of all time. Dragon Ball! ","7d0b0e73":"# Datasets\n\nFor the dataset, we will use the [awesome data](https:\/\/www.kaggle.com\/azathoth42\/myanimelist\/version\/9#_=_) from azazoth42 (huge thanks!). To make the cleaning data process easy, we will use only the CSVs that do not contain any null values (which have the cleaned suffix on them). \n\nThe datasets are mainly divided in three. One with info about the MyAnimeList customers (UsersDF) such as location, gender, birth date and so on. Another one for info about the anime, which brings data like the title of the show, what genre is it inside and even the opening themes. The last one (and also the biggest one) is all the scores that a certain user had with a show (that have 31M rows of interactions, wow).","1b1afbca":"This notebook presents a beginner friendly solution for a [Recommender System](https:\/\/en.wikipedia.org\/wiki\/Recommender_system) problem at the MyAnimeList portal. The main idea here is to understand the data presented to us, do all the cleaning and exploration stuff and then test and apply different algorithms so we have available either a User-based and an Item-based Collaborative Filtering system.","133cb940":"Now, as I mentioned early on, I've built an item-based recommender just so we can test if our recommendations make any sense. Not all algorithms can do the item-item recommendation, so the most used ones are the unsupervised-learning-based (as this KNN that I actually use). ","fda811d5":"Another info that would be interesting to know is how are the user scores distributed. That could explain us what number represents something that the user liked or not (and will be used later on as a treshold too).","a57cca61":"Ironically enough, the default options for the parameters actually brought a better RMSE than the tweak of both grids, so we we won't change anything.","a2ae1414":"The reason I sampled the UsersDF and then joined with the UsersAndScores and not just simply sampled the UsersAndScores directly is that I don't know why I can't reproduce the same sampled DF everytime (with the random_seed). If any of you know why, please give me some feedback!","bcdda36f":"Before moving on to later analyses, there is something worth mentioning. The column 'my_status' that we maintaned in the Scores Dataframe represents the following data about the relation of the customer about the show:\n\n1: watching\n2: completed\n3: on hold\n4: dropped\n6: plan to watch\n\nOne optiong would be just considering interactions coming from completed animes, but I came to the conclusion that the info about the other status are relevant too and shouldn't be recommended for the specific customer again, so they were kept in to the dataframe (even though I have an example of treating this data in the next annotations).","98f4b6ed":"Here we create another dataframe which contains only users and animes that have at least 10 interactions. We then combine this data by merging our initial frame with the RatedsPerUser and the RatedsPerAnime. \n\nThe reason for this is trying to solve a problem that Recommender Systems commonly runs in to, that is the 'Cold Start Problem'. It is really hard to recommend something to a customer that you do not have data about, but there are several ways to deal with it, such as a Popularity Recommender (where I'll recommend to my new users the most liked products) and other stuff, but I'll just exclude them from my analysis since I already have a lot of data.","ffe9028c":"# Conclusion\n\nThanks for reading this up until here. This is my first Kaggle and I hope you guys enjoyed my didatic! Any feedback is welcome, please feel free to comment here or send me a message, here is my [linkedin profile](https:\/\/www.linkedin.com\/in\/gabriel-martelloti-a3ab87b6\/). \n\nSources:\n[surprise lib documentation](https:\/\/surprise.readthedocs.io\/en\/stable\/)\n\n[Collaborative filtering with surprise application](https:\/\/towardsdatascience.com\/building-and-testing-recommender-systems-with-surprise-step-by-step-d4ba702ef80b)\n\n[This awesome Kaggle which inspired me](https:\/\/www.kaggle.com\/gspmoreira\/recommender-systems-in-python-101)\n\n[The paper for the BaselineOnly method](http:\/\/courses.ischool.berkeley.edu\/i290-dm\/s11\/SECURE\/a1-koren.pdf)","5aee5341":"# Metrics\n\nIn the following links there are explanations about how the most important metrics work:\n\n[RMSE and MAE](https:\/\/medium.com\/human-in-a-machine-world\/mae-and-rmse-which-metric-is-better-e60ac3bde13d)\n\n[Precision@k and Recall@k](https:\/\/medium.com\/@m_n_malaeb\/recall-and-precision-at-k-for-recommender-systems-618483226c54)\n\nFor this specific model, I chose to base my choice on the Precision@k metric (specifically the Precision@10). The advantage of this metric above all else is the importance that it gives to the items that you actually want to recommend. The problem I see for RMSE and MAE right here is that they do their calculation for all items in the model and I'm not interested, let's say, in the difference of the true rating and the estimated rating for the 376th (sorted by descending rating) item of my customer. And the problem with the Recall@k is that I won't be able to recommend all the relevant items since some of my users have 1000 animes scored.\n","f78cd2b7":"Now I cleaned all the results data and inverted the RMSE (just because having a greater RMSE\u02c6-1 is now good and it looks better in a chart) and wil scatter plot them to see their difference in performance, but we can see right now that the best performing algorithm was the BaselineOnly.","dd4ab5b3":"# Context and motivation\n\nIf any of you have already searched for info and rating about a Anime on the internet, you probably were sent to the [MyAnimeList](www.myanimelist.net) website. It is like the IMDB of japanese content, and has a lot of different ways for you to find something suitable to you as \"Top Airing\", \"Top Upcoming\" and so on. The score of a certain anime is actually the global mean of all the ratings given by the users (I'm not quite sure about this info, but I imagine this is the case). Rates go from 0 to 10, and whenever you create your account you can score pretty much anything you want.\n\nEver since I've learned how [Spotify created and developed their awesome recommendation system](https:\/\/medium.com\/s\/story\/spotifys-discover-weekly-how-machine-learning-finds-your-new-music-19a41ab76efe) I got more and more interested about how all of this stuff works in practice and started to be more curious about it. \n\nNow, how I connected these two stories. A while ago, I was looking for something to watch and thought \"Hey, maybe I can go to MyAnimeList, give my scores of previous watched animes and maybe find something good recommended to me, kind of like Netflix and Spotify do\". But since you are reading up until here, you can imagine that this is not a feature they had available in their website, which coincidentally happenned in the same time I had to think of my final project for the [Machine Learning Engineer course](https:\/\/www.udacity.com\/course\/machine-learning-engineer-nanodegree--nd009t) from Udacity (that I highly recommend if you have the money to spend). So hey, why not do it?","3e1a3f7b":"Cool, the first two ones are Dragon Ball Z and GT! But wait, how on earth is Pokemon similar to Dragon Ball? After questioning it to myself I realized that, as a child, every morning I used to watch Dragon Ball, Pokemon and Yu-Gi-Oh and talk about it with my friends at school... So hey, we already saw a pattern! Maybe if someone likes some anime, they have a higher chance of liking a show that launched in the same decade, at least, than a show with the same genres. That's the magic of collaborative filtering! You don't have to tell the patterns to the model, you have to try to understand them after.","f912ba40":"Since the DF will be too big to run some of the later algorithms, I sampled 1% of the users. The reason behind doing this right now in this specific DF is to sample people without losing info about their ratings. If I sampled them in the ScoresDF, an example that I tried to avoid is sampling a customer that had 100 interactions with just 1 interaction now, and that would hurt the accuracy of the model later on.","2e7fe5ef":"Funny enough, the BaselineOnly is one of the simplest and least costing collaborative filtering algorithms. There is a definition on its paper that explains it better than I can:\n\n\" For example, suppose that we want a baseline\nestimate for the rating of the movie Titanic by user Joe. Now, say that the\naverage rating over all movies, \u03bc, is 3.7 stars. Furthermore, Titanic is better\nthan an average movie, so it tends to be rated 0.5 stars above the average. On\nthe other hand, Joe is a critical user, who tends to rate 0.3 stars lower than the\naverage. Thus, the baseline estimate for Titanic\u2019s rating by Joe would be 3.9\nstars by calculating 3.7 \u2212 0.3 + 0.5. \"\n\nThat is how Machine Learning works, sometimes a linear regression works better than a neural network.\n\nNow that I chose my best algorithm I need to generate new recommendations for my users. For that reason, I create a  new training set, now containing all available scores and create a testing set with all animes the user didn't score but are on the training set for other users (that's why we need to take care on the site of the sample, since this part can be very costly)."}}