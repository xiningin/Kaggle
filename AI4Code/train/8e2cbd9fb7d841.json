{"cell_type":{"3b3cdc3f":"code","4cc825d1":"code","8d2a21e0":"code","af82810c":"code","d06c415e":"code","765f18ad":"code","42a5ca34":"code","0303769c":"code","cdf69753":"code","7c75d0dc":"code","d371f96f":"code","ebf7f6e5":"code","77d849f8":"code","1175ac57":"code","80e3dcd2":"code","c3a640e8":"code","81fa5abf":"code","1d880c2f":"code","575c91f8":"code","b1f84b71":"code","62a88501":"code","7be4047f":"code","69fa1982":"code","19ed977b":"code","5e9726e7":"code","93a985ac":"code","39419962":"code","f8b9231f":"code","fd7b7d36":"code","db90b230":"code","8cffb2aa":"code","6c136ee3":"code","e9a9fcc1":"code","107a2b92":"code","1eeb6acd":"code","8a4223d1":"code","9e90e5f3":"code","b1699761":"code","266acb10":"code","9402fbe7":"code","b2a473b7":"code","90dc0b6c":"code","15bc8290":"code","16340897":"code","425fc2ed":"code","b8455935":"code","d3989619":"code","dd020e2d":"code","347f19e4":"code","d7773f4d":"code","fe1281ba":"code","da7aa90f":"code","e98394ac":"code","6d068d41":"code","32599117":"code","657a9e51":"code","5565b60c":"code","b6321759":"code","997af061":"code","f5d6a1a0":"code","ea70d84f":"code","239a819f":"code","491b0ba2":"code","4a43bb59":"code","6484bd6b":"code","6aeb4ed9":"code","b5681ef8":"code","a7ad232c":"code","63afc26c":"code","bdea095d":"code","52f9aec7":"code","cdac4239":"markdown","5a96ef2f":"markdown","ee44f87e":"markdown","678dc483":"markdown","ee5ea2a3":"markdown","41179753":"markdown","80bd9b03":"markdown","6acf8308":"markdown","55323949":"markdown","dbd06eaa":"markdown","e23127e4":"markdown","167a6d59":"markdown"},"source":{"3b3cdc3f":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline","4cc825d1":"df = pd.read_csv(\"..\/input\/train.csv\")","8d2a21e0":"df.head()","af82810c":"df.info()","d06c415e":"df[df.Age.isnull()].head()","765f18ad":"for i in df.columns:\n    print (df[i].value_counts().head(10))","42a5ca34":"df[df[\"Fare\"] < 0.001].sort_values(\"Ticket\")\n# reasearching several of these individuals shows they were employees\n# and much more likely to have died\n# add a feature that is 1 for free_fare","0303769c":"print( df[df[\"Embarked\"].isnull()])\n# https:\/\/www.encyclopedia-titanica.org\/titanic-survivor\/amelia-icard.html\n# research shows these should both be set to S for Southampton","cdf69753":"master = df[[\"Master.\" in x for x in df[\"Name\"]]][\"Age\"].dropna()\nrev = df[[\"Rev.\" in x for x in df[\"Name\"]]][\"Age\"].dropna()\nmr = df[[\"Mr.\" in x for x in df[\"Name\"]]][\"Age\"].dropna()\nmiss = df[[\"Miss.\" in x for x in df[\"Name\"]]][\"Age\"].dropna()\nmrs = df[[\"Mrs.\" in x for x in df[\"Name\"]]][\"Age\"].dropna()\nquot = df[['\"' in x for x in df[\"Name\"]]][\"Age\"].dropna()\nparen = df[[\"(\" in x for x in df[\"Name\"]]][\"Age\"].dropna()\nboth = df[[\"(\" in x and '\"' in x for x in df[\"Name\"]]][\"Age\"].dropna()\nplt.boxplot([master.values,mr.values,miss.values, \\\n             mrs.values,quot.values,paren.values, \\\n             both.values])\nplt.title(\"Ages vs name keywords\")\nplt.show()","7c75d0dc":"master = df[[\"Master.\" in x for x in df[\"Name\"]]][\"Fare\"].dropna()\nrev = df[[\"Rev.\" in x for x in df[\"Name\"]]][\"Fare\"].dropna()\nmr = df[[\"Mr.\" in x for x in df[\"Name\"]]][\"Fare\"].dropna()\nmiss = df[[\"Miss.\" in x for x in df[\"Name\"]]][\"Fare\"].dropna()\nmrs = df[[\"Mrs.\" in x for x in df[\"Name\"]]][\"Fare\"].dropna()\nquot = df[['\"' in x for x in df[\"Name\"]]][\"Fare\"].dropna()\nparen = df[[\"(\" in x for x in df[\"Name\"]]][\"Fare\"].dropna()\nboth = df[[\"(\" in x and '\"' in x for x in df[\"Name\"]]][\"Fare\"].dropna()\nplt.boxplot([master.values,mr.values,miss.values, \\\n             mrs.values,quot.values,paren.values,both.values])\nplt.title(\"Fares vs name keywords\")\nplt.ylim(0,150)\nplt.show()\n# master","d371f96f":"X = df.iloc[:,2:]\ny = df.iloc[:,0]","ebf7f6e5":"from sklearn.preprocessing import StandardScaler, MaxAbsScaler, MinMaxScaler, Imputer\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, LabelBinarizer\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV\nfrom sklearn.pipeline import make_pipeline, Pipeline, FeatureUnion\nfrom sklearn.linear_model import LogisticRegressionCV, ElasticNetCV\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn import tree","77d849f8":"X.head()","1175ac57":"cols = [x for x in X.columns if x !='last_name' and x != 'Age']","80e3dcd2":"# u = StandardScaler()\n# v = ElasticNetCV()\n# g = GridSearchCV(Pipeline([('scale',u),('fit',v)]),{'fit__l1_ratio':(0.00001,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1)}, n_jobs=-1)\n# g.fit(X[X.Age.notnull()].ix[:,cols],X.Age[X.Age.notnull()])","c3a640e8":"# g.best_estimator_","81fa5abf":"# pd.DataFrame(g.cv_results_).sort_values(\"rank_test_score\")","1d880c2f":"# g.predict(X[X.Age.notnull()].ix[:,cols]).shape","575c91f8":"# X.Age[X.Age.notnull()].shape","b1f84b71":"# g.score(X[X.Age.notnull()].ix[:,cols],X.Age[X.Age.notnull()])","62a88501":"# plt.scatter(X.Age[X.Age.notnull()],g.predict(X[X.Age.notnull()].ix[:,cols]))\n# plt.plot(X.Age[X.Age.notnull()],X.Age[X.Age.notnull()])","7be4047f":"# g.predict(X[X.Age.isnull()].ix[:,cols])","69fa1982":"median_fare = df.Fare.median()","19ed977b":"# generate some columns\ndef feature_cleaning(df:pd.DataFrame)->pd.DataFrame:\n    df.drop([\"Cabin\", \"Ticket\", \"PassengerId\"], axis=1, inplace=True)   # I will ignore these columns\n    df.Embarked.fillna(\"S\",inplace=True)  # fill using the info above\n    df = pd.get_dummies(df,columns=[\"Pclass\",\"Sex\",\"Embarked\"],drop_first=True)  # tokenize these columns\n\n    # feature engineering\n    \n    df['age_estimated'] = df.Age.map(lambda x: 1 if x%1 > 0.2 else 0)\n    df['chaperoned_child'] = ((df.Age < 18) & (df.Parch == 0)).astype(int)\n    df['family_size'] = df.SibSp + df.Parch\n    \n    df['free_fare'] = df.Fare.map(lambda x: 1 if x < 0.001 else 0)\n    df[\"has_master\"] = df.Name.map(lambda x: 1 if 'Master.' in x else 0)\n    df[\"has_rev\"] = df.Name.map(lambda x: 1 if 'Rev.' in x else 0)\n    df[\"has_mr\"] = df.Name.map(lambda x: 1 if 'Mr.' in x else 0)\n    df[\"has_miss\"] = df.Name.map(lambda x: 1 if 'Miss.' in x else 0)\n    df[\"has_mrs\"] = df.Name.map(lambda x: 1 if 'Mrs.' in x else 0)\n    df[\"has_quote\"] = df.Name.map(lambda x: 1 if '\"' in x else 0)\n    df[\"has_parens\"] = df.Name.map(lambda x: 1 if '(' in x else 0)\n    df[\"last_name\"] = df.Name.map(lambda x: x.replace(\",\",\"\").split()[0] )\n\n    df.drop([\"Age\",\"Name\"], axis=1, inplace=True)\n    df.Fare.fillna(median_fare, inplace=True)\n    return df\n\ndf = feature_cleaning(df)","5e9726e7":"df.head()","93a985ac":"X = df.iloc[:,1:]\ny = df.iloc[:,0]","39419962":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test, = train_test_split(X, y, test_size=0.2, random_state=42)","f8b9231f":"class ModelTransformer(BaseEstimator,TransformerMixin):\n\n    def __init__(self, model=None):\n        self.model = model\n\n    def fit(self, *args, **kwargs):\n        self.model.fit(*args, **kwargs)\n        return self\n\n    def transform(self, X, **transform_params):\n        return self.model.transform(X)\n    \nclass SampleExtractor(BaseEstimator, TransformerMixin):\n    \"\"\"Takes in varaible names as a **list**\"\"\"\n\n    def __init__(self, vars):\n        self.vars = vars  # e.g. pass in a column names to extract\n\n    def transform(self, X, y=None):\n        if len(self.vars) > 1:\n            return pd.DataFrame(X[self.vars]) # where the actual feature extraction happens\n        else:\n            return pd.Series(X[self.vars[0]])\n\n    def fit(self, X, y=None):\n        return self  # generally does nothing\n    \n    \nclass DenseTransformer(BaseEstimator,TransformerMixin):\n\n    def transform(self, X, y=None, **fit_params):\n#         print (X.todense())\n        return X.todense()\n\n    def fit_transform(self, X, y=None, **fit_params):\n        self.fit(X, y, **fit_params)\n        return self.transform(X)\n\n    def fit(self, X, y=None, **fit_params):\n        return self","fd7b7d36":"kf_shuffle = StratifiedKFold(n_splits=5,shuffle=True,random_state=777)\n\ncols = [x for x in X.columns if x !='last_name']\n\npipeline = Pipeline([\n    ('features', FeatureUnion([\n        ('names', Pipeline([\n                      ('text',SampleExtractor(['last_name'])),\n                      ('dummify', CountVectorizer(binary=True)),\n                      ('densify', DenseTransformer()),\n                     ])),\n        ('cont_features', Pipeline([\n                      ('continuous', SampleExtractor(cols)),\n                      ])),\n        ])),\n        ('scale', ModelTransformer()),\n        ('fit', LogisticRegressionCV(solver='liblinear')),\n])\n\n\nparameters = {\n    'scale__model': (StandardScaler(),MinMaxScaler()),\n    'fit__penalty': ('l1','l2'),\n    'fit__class_weight':('balanced',None),\n    'fit__Cs': (20,),\n}\n\nlogreg_gs = GridSearchCV(pipeline, parameters, verbose=False, cv=kf_shuffle, n_jobs=-1)\n","db90b230":"%%time\nprint(\"Performing grid search...\")\nprint(\"pipeline:\", [name for name, _ in pipeline.steps])\nprint(\"parameters:\")\nprint(parameters)\n\n\nlogreg_gs.fit(X_train, y_train)\n\nprint(\"Best score: %0.3f\" % logreg_gs.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = logreg_gs.best_estimator_.get_params()\nfor param_name in sorted(parameters.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))","8cffb2aa":"cv_pred = pd.Series(logreg_gs.predict(X_test))","6c136ee3":"pd.DataFrame(list(zip(logreg_gs.cv_results_['mean_test_score'],\\\n                 logreg_gs.cv_results_['std_test_score'])\\\n                )).sort_values(0,ascending=False).head(10)\n","e9a9fcc1":"# logreg_gs.best_estimator_","107a2b92":"confusion_matrix(y_test,cv_pred)","1eeb6acd":"print (classification_report(y_test,cv_pred))","8a4223d1":"from sklearn.metrics import roc_curve, auc, precision_recall_curve\nplt.style.use('seaborn-white')\n\n# Y_score = logreg_gs.best_estimator_.decision_function(X_test)\nY_score = logreg_gs.best_estimator_.predict_proba(X_test)[:,1]\n\n# For class 1, find the area under the curve\nFPR, TPR, _ = roc_curve(y_test, Y_score)\nROC_AUC = auc(FPR, TPR)\nPREC, REC, _ = precision_recall_curve(y_test, Y_score)\nPR_AUC = auc(REC, PREC)\n\n# Plot of a ROC curve for class 1 (has_cancer)\nplt.figure(figsize=[11,9])\nplt.plot(FPR, TPR, label='ROC curve (area = %0.2f)' % ROC_AUC, linewidth=4)\nplt.plot(REC, PREC, label='PR curve (area = %0.2f)' % PR_AUC, linewidth=4)\nplt.plot([0, 1], [0, 1], 'k--', linewidth=4)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate', fontsize=18)\nplt.ylabel('True Positive Rate', fontsize=18)\nplt.title('Logistic Regression for Titanic Survivors', fontsize=18)\nplt.legend(loc=\"lower right\")\nplt.show()","9e90e5f3":"plt.scatter(y_test,cv_pred,color='r')\nplt.plot(y_test,y_test,color='k')\nplt.xlabel(\"True value\")\nplt.ylabel(\"Predicted Value\")\nplt.show()","b1699761":"kf_shuffle = StratifiedKFold(n_splits=5,shuffle=True,random_state=777)\n\ncols = [x for x in X.columns if x !='last_name']\n\npipeline = Pipeline([\n    ('features', FeatureUnion([\n        ('names', Pipeline([\n                      ('text',SampleExtractor(['last_name'])),\n                      ('dummify', CountVectorizer(binary=True)),\n                      ('densify', DenseTransformer()),\n                     ])),\n        ('cont_features', Pipeline([\n                      ('continuous', SampleExtractor(cols)),\n                      ])),\n        ])),\n        ('scale', ModelTransformer()),\n        ('fit', KNeighborsClassifier()),\n])\n\n\nparameters = {\n    'scale__model': (StandardScaler(),MinMaxScaler()),\n    'fit__n_neighbors': (2,3,5,7,9,11,16,20),\n    'fit__weights': ('uniform','distance'),\n}\n\nknn_gs = GridSearchCV(pipeline, parameters, verbose=False, cv=kf_shuffle, n_jobs=-1)\n","266acb10":"%%time\nprint(\"Performing grid search...\")\nprint(\"pipeline:\", [name for name, _ in pipeline.steps])\nprint(\"parameters:\")\nprint(parameters)\n\n\nknn_gs.fit(X_train, y_train)\n\nprint(\"Best score: %0.3f\" % knn_gs.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = knn_gs.best_estimator_.get_params()\nfor param_name in sorted(parameters.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))","9402fbe7":"cv_pred = pd.Series(knn_gs.predict(X_test))","b2a473b7":"pd.DataFrame(list(zip(knn_gs.cv_results_['mean_test_score'],\\\n                 knn_gs.cv_results_['std_test_score'])\\\n                )).sort_values(0,ascending=False).head(10)\n","90dc0b6c":"# knn_gs.best_estimator_","15bc8290":"confusion_matrix(y_test,cv_pred)","16340897":"print( classification_report(y_test,cv_pred))","425fc2ed":"from sklearn.metrics import roc_curve, auc\nplt.style.use('seaborn-white')\n\n# Y_score = knn_gs.best_estimator_.decision_function(X_test)\nY_score = knn_gs.best_estimator_.predict_proba(X_test)[:,1]\n\n\n# For class 1, find the area under the curve\nFPR, TPR, _ = roc_curve(y_test, Y_score)\nROC_AUC = auc(FPR, TPR)\n\nPREC, REC, _ = precision_recall_curve(y_test, Y_score)\nPR_AUC = auc(REC, PREC)\n\n# Plot of a ROC curve for class 1 (has_cancer)\nplt.figure(figsize=[11,9])\nplt.plot(FPR, TPR, label='ROC curve (area = %0.2f)' % ROC_AUC, linewidth=4)\nplt.plot(REC, PREC, label='PR curve (area = %0.2f)' % PR_AUC, linewidth=4)\nplt.plot([0, 1], [0, 1], 'k--', linewidth=4)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate', fontsize=18)\nplt.ylabel('True Positive Rate', fontsize=18)\nplt.title('kNN for Titanic Survivors', fontsize=18)\nplt.legend(loc=\"lower right\")\nplt.show()","b8455935":"kf_shuffle = StratifiedKFold(n_splits=5,shuffle=True,random_state=777)\n\ncols = [x for x in X.columns if x !='last_name']\n\npipeline = Pipeline([\n    ('features', FeatureUnion([\n        ('names', Pipeline([\n                      ('text',SampleExtractor(['last_name'])),\n                      ('dummify', CountVectorizer(binary=True)),\n                      ('densify', DenseTransformer()),\n                     ])),\n        ('cont_features', Pipeline([\n                      ('continuous', SampleExtractor(cols)),\n                      ])),\n        ])),\n#         ('scale', ModelTransformer()),\n        ('fit', tree.DecisionTreeClassifier()),\n])\n\n\nparameters = {\n#     'scale__model': (StandardScaler(),MinMaxScaler()),\n    'fit__max_depth': (2,3,4,None),\n    'fit__min_samples_split': (2,3,4,5),\n    'fit__class_weight':('balanced',None),\n}\n\ndt_gs = GridSearchCV(pipeline, parameters, verbose=False, cv=kf_shuffle, n_jobs=-1)\n","d3989619":"%%time\nprint(\"Performing grid search...\")\nprint(\"pipeline:\", [name for name, _ in pipeline.steps])\nprint(\"parameters:\")\nprint(parameters)\n\n\ndt_gs.fit(X_train, y_train)\n\nprint(\"Best score: %0.3f\" % dt_gs.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = dt_gs.best_estimator_.get_params()\nfor param_name in sorted(parameters.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))","dd020e2d":"cv_pred = pd.Series(dt_gs.predict(X_test))","347f19e4":"pd.DataFrame(list(zip(dt_gs.cv_results_['mean_test_score'],\\\n                 dt_gs.cv_results_['std_test_score'])\\\n                )).sort_values(0,ascending=False).head(10)\n","d7773f4d":"# dt_gs.best_estimator_","fe1281ba":"confusion_matrix(y_test,cv_pred)","da7aa90f":"print (classification_report(y_test,cv_pred))","e98394ac":"from sklearn.metrics import roc_curve, auc\nplt.style.use('seaborn-white')\n\n# Y_score = dt_gs.best_estimator_.decision_function(X_test)\nY_score = dt_gs.best_estimator_.predict_proba(X_test)[:,1]\n\n\n# For class 1, find the area under the curve\nFPR, TPR, _ = roc_curve(y_test, Y_score)\nROC_AUC = auc(FPR, TPR)\n\nPREC, REC, _ = precision_recall_curve(y_test, Y_score)\nPR_AUC = auc(REC, PREC)\n\n# Plot of a ROC curve for class 1 (has_cancer)\nplt.figure(figsize=[11,9])\nplt.plot(FPR, TPR, label='ROC curve (area = %0.2f)' % ROC_AUC, linewidth=4)\nplt.plot(REC, PREC, label='PR curve (area = %0.2f)' % PR_AUC, linewidth=4)\nplt.plot([0, 1], [0, 1], 'k--', linewidth=4)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate', fontsize=18)\nplt.ylabel('True Positive Rate', fontsize=18)\nplt.title('Decision Tree for Titanic Survivors', fontsize=18)\nplt.legend(loc=\"lower right\")\nplt.show()","6d068d41":"from sklearn.ensemble import RandomForestClassifier","32599117":"kf_shuffle = StratifiedKFold(n_splits=5,shuffle=True,random_state=777)\n\ncols = [x for x in X.columns if x !='last_name']\n\npipeline = Pipeline([\n    ('features', FeatureUnion([\n        ('names', Pipeline([\n                      ('text',SampleExtractor(['last_name'])),\n                      ('dummify', CountVectorizer(binary=True)),\n                      ('densify', DenseTransformer()),\n                     ])),\n        ('cont_features', Pipeline([\n                      ('continuous', SampleExtractor(cols)),\n                      ])),\n        ])),\n#         ('scale', ModelTransformer()),\n        ('fit', RandomForestClassifier()),\n])\n\n\nparameters = {\n#     'scale__model': (StandardScaler(),MinMaxScaler()),\n    'fit__max_depth': (4,7,10),\n    'fit__n_estimators': (25,100,200,300),\n    'fit__class_weight':('balanced',None),\n    'fit__max_features': ('auto',0.3,0.5),\n}\n\nrf_gs = GridSearchCV(pipeline, parameters, verbose=False, cv=kf_shuffle, n_jobs=-1)\n","657a9e51":"%%time\nprint(\"Performing grid search...\")\nprint(\"pipeline:\", [name for name, _ in pipeline.steps])\nprint(\"parameters:\")\nprint(parameters)\n\n\nrf_gs.fit(X_train, y_train)\n\nprint(\"Best score: %0.3f\" % rf_gs.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = rf_gs.best_estimator_.get_params()\nfor param_name in sorted(parameters.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))","5565b60c":"cv_pred = pd.Series(rf_gs.predict(X_test))","b6321759":"pd.DataFrame(list(zip(rf_gs.cv_results_['mean_test_score'],\\\n                 rf_gs.cv_results_['std_test_score'])\\\n                )).sort_values(0,ascending=False).head(10)\n","997af061":"# rf_gs.best_estimator_","f5d6a1a0":"confusion_matrix(y_test,cv_pred)","ea70d84f":"print (classification_report(y_test,cv_pred))","239a819f":"from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\nplt.style.use('seaborn-white')\n\nY_score = rf_gs.best_estimator_.predict_proba(X_test)[:,1]\n\n\n# For class 1, find the area under the curve\nFPR, TPR, _ = roc_curve(y_test, Y_score)\nROC_AUC = auc(FPR, TPR)\n\nPREC, REC, _ = precision_recall_curve(y_test, Y_score)\nPR_AUC = auc(REC, PREC)\n\n# Plot of a ROC curve for class 1 (has_cancer)\nplt.figure(figsize=[11,9])\nplt.plot(FPR, TPR, label='ROC curve (area = %0.2f)' % ROC_AUC, linewidth=4)\nplt.plot(REC, PREC, label='PR curve (area = %0.2f)' % PR_AUC, linewidth=4)\nplt.plot([0, 1], [0, 1], 'k--', linewidth=4)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate or Recall', fontsize=18)\nplt.ylabel('True Positive Rate or Precision', fontsize=18)\nplt.title('Random Forest for Titanic Survivors', fontsize=18)\nplt.legend(loc=\"lower right\")\nplt.show()","491b0ba2":"rf_gs.best_estimator_.steps[1][1].feature_importances_[:15]","4a43bb59":"X_pred = pd.read_csv(\"..\/input\/test.csv\")\npred_ids = X_pred.PassengerId\nX_pred = feature_cleaning(X_pred)","6484bd6b":"X_pred.head()","6aeb4ed9":"X_pred.shape","b5681ef8":"X_test.shape","a7ad232c":"X_pred.isnull().sum().sum()  # check there are no nulls","63afc26c":"predictions = rf_gs.predict(X_pred)","bdea095d":"predictions[:5]","52f9aec7":"pd.DataFrame(list(zip(pred_ids, predictions)), columns=[\"PassengerId\",\"Survived\"]).to_csv(\"RF_pred.csv\", index=None)","cdac4239":"Note that those name columns also have a bearing on the fare.","5a96ef2f":"## Aborted imputation\nDrop rows not required","ee44f87e":"## Make a Train\/test split","678dc483":"# Decision Tree","ee5ea2a3":"# kNN","41179753":"The Age of a person is related to some key words in the Name column","80bd9b03":"-------\n-------\n# Basic Analysis for Titanic data set\n-----\n-----\n\nIn this notebook:\n- Basic EDA, \n- look at some imputation techniques (not used in model), \n- Generate a few features (mostly from the name column), \n- then run Grid Search on four models\n    + Logistic Regression\n    + K Nearest Neighbors\n    + Decision Tree\n    + Random Forest","6acf8308":"## Impute the age.... \ndropping for now, will finish when I have more time","55323949":"# Logistic Regression","dbd06eaa":"## Null value analysis\n\nIt seems most columns have no null values.  The `df.info()` summary shows several columns already hold numerical data (for which the non-null value read out is accurate).  Furthermore, the `.value_counts()` method on each series shows none of the typical null value entries (i.e. `*,-,None,NA,999,-1,\"\"`).  The upshot is only three columns are missing any data: Age, Cabin, and Embarked.\n\nThe Cabin column has relatively few values, and seems overly specific to be much help.  I chose to drop it. The Embarked column is only missing 2 values, so there is not much pay off for the effort to re-integrate them. The Age coumn is missing 177 values which need to be imputed, and 15 more that are 0.0000 (though after research I see these VIPs like the ship designer).  I will use a linear regression model on `Pclass, Sex, Fare, Embarked`, etc. to impute Age values that are missing.","e23127e4":"## Define functions for grid search","167a6d59":"# Random Forest"}}