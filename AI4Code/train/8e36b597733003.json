{"cell_type":{"307effe4":"code","1c8fd23c":"code","77b43ada":"code","48b0b3c9":"code","de6034d9":"code","af546824":"code","8d32e77d":"code","74566913":"code","06903626":"code","320785eb":"code","e5f7f304":"code","963471b8":"code","2a489326":"code","cb15b791":"code","8158c0ac":"code","4493c216":"code","13fcb8d7":"code","c8f8cf5b":"code","b6be0439":"code","1d6275ae":"code","55789ffc":"code","d83f6644":"markdown","f0e448f0":"markdown","a8c7f81d":"markdown"},"source":{"307effe4":"import gc\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n%matplotlib inline\n\nfrom tqdm import tqdm_notebook\nfrom itertools import product\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom catboost import CatBoostRegressor, Pool\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_validate\n\nfrom statsmodels.tsa.stattools import pacf\nfrom category_encoders import TargetEncoder\nfrom sklearn.ensemble import StackingRegressor,RandomForestRegressor\nimport xgboost as xgb\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","1c8fd23c":"train = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\nshops = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/shops.csv')\nitems = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/items.csv')\ncats = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\ntest = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/test.csv')\ntrain.head()","77b43ada":"#Save memory\ndef downcast_dtypes(df):\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols = [c for c in df if df[c].dtype in [\"int64\", \"int32\"]]\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols] = df[int_cols].astype(np.int16)\n    return df","48b0b3c9":"#Remove outliers\ntrain = train[(train.item_price < 100000 )& (train.item_cnt_day < 1000)]\n\n#Remove negative values\ntrain = train[train.item_price > 0].reset_index(drop = True)\ntrain.loc[train.item_cnt_day < 1, \"item_cnt_day\"] = 0\n\n#Remove equal shops\ntrain.loc[train.shop_id == 0, \"shop_id\"] = 57\ntest.loc[test.shop_id == 0 , \"shop_id\"] = 57\ntrain.loc[train.shop_id == 1, \"shop_id\"] = 58\ntest.loc[test.shop_id == 1 , \"shop_id\"] = 58\ntrain.loc[train.shop_id == 11, \"shop_id\"] = 10\ntest.loc[test.shop_id == 11, \"shop_id\"] = 10\ntrain.loc[train.shop_id == 40, \"shop_id\"] = 39\ntest.loc[test.shop_id == 40, \"shop_id\"] = 39\n\nX= train.copy(deep=True)\nid_column = test['ID']\nX_test = test.drop('ID',axis=1).copy(deep=True)\nX_test[\"date_block_num\"] = 34\n\ndel train, test\ngc.collect()","de6034d9":"#Create a combined dataframe\n\ndf = pd.concat([X, X_test], sort=False)\ndf.fillna(0, inplace = True )\n\n#Delete the unused data\ndel X\ndel X_test\ngc.collect()","af546824":"# Create \"grid\" with columns\nindex_cols = ['shop_id', 'item_id', 'date_block_num']\n\n# For every month we create a grid from all shops\/items combinations from that month\ngrid = [] \nfor block_num in df['date_block_num'].unique():\n    cur_shops = df.loc[df['date_block_num'] == block_num, 'shop_id'].unique()\n    cur_items = df.loc[df['date_block_num'] == block_num, 'item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n\n# Turn the grid into a dataframe\ngrid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n\n# Groupby data to get shop-item-month aggregates\ngb = df.groupby(index_cols,as_index=False).agg({'item_cnt_day':{'target':'sum'}})\n# Fix column names\ngb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values] \n# Join it to the grid\nall_data = pd.merge(grid, gb, how='left', on=index_cols).fillna(0)\n\n# Same as above but with shop-month aggregates\ngb = df.groupby(['shop_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'target_shop':'sum'}})\ngb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values]\nall_data = pd.merge(all_data, gb, how='left', on=['shop_id', 'date_block_num']).fillna(0)\n\n# Same as above but with item-month aggregates\ngb = df.groupby(['item_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'target_item':'sum'}})\ngb.columns = [col[0] if col[-1] == '' else col[-1] for col in gb.columns.values]\nall_data = pd.merge(all_data, gb, how='left', on=['item_id', 'date_block_num']).fillna(0)\n\n# Downcast dtypes from 64 to 32 bit to save memory\nall_data = downcast_dtypes(all_data)\n\ndel grid, gb \ngc.collect()","8d32e77d":"all_data","74566913":"#Calculate partial autocorrelations\n\nx_pacf = pacf(all_data['target_item'], nlags =5, method= 'ols' )\nx_pacf1 = pacf(all_data['target'], nlags =5, method= 'ols' )\nprint(x_pacf)\nprint(x_pacf1)","06903626":"plt.plot(x_pacf1)","320785eb":"# List of columns that we will use to create lags\ncols_to_rename = list(all_data.columns.difference(index_cols)) \n\nshift_range = [1, 2, 3, 4]\n\nfor month_shift in tqdm_notebook(shift_range):\n    train_shift = all_data[index_cols + cols_to_rename].copy()\n    \n    train_shift['date_block_num'] = train_shift['date_block_num'] + month_shift\n    \n    foo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in cols_to_rename else x\n    train_shift = train_shift.rename(columns=foo)\n\n    all_data = pd.merge(all_data, train_shift, on=index_cols, how='left').fillna(0)\n\ndel train_shift\n\n# Don't use old data from year 2013\nall_data = all_data[all_data['date_block_num'] >= 12] \n\n# List of all lagged features\nfit_cols = [col for col in all_data.columns if col[-1] in [str(item) for item in shift_range]] \n# We will drop these at fitting stage\nto_drop_cols = list(set(list(all_data.columns)) - (set(fit_cols)|set(index_cols))) + ['date_block_num'] \n\n# Category for each item\nitem_category_mapping = items[['item_id','item_category_id']].drop_duplicates()\n\nall_data = pd.merge(all_data, item_category_mapping, how='left', on='item_id')\ndf = downcast_dtypes(all_data)\n\ndel all_data\ngc.collect();","e5f7f304":"#Returning the data\nX = df[df['date_block_num'] <= 33].drop('date_block_num', axis=1)\nY = X['target']\nX = X.drop(['target'], axis=1).clip(0,20)\nX_test = df[df['date_block_num'] == 34 ].drop(['date_block_num','target'], axis=1).clip(0,20)\n\ndel df\ngc.collect();","963471b8":"#Mean encoding\ncolumns = ['shop_id','item_id', 'item_category_id', 'target_shop_lag_1','target_shop_lag_2','target_shop_lag_3',\n           'target_shop_lag_4', 'target_item_lag_1' ,'target_item_lag_2' ,'target_item_lag_3', 'target_item_lag_4']\n\ntarg_enc = TargetEncoder(cols=columns).fit(X, Y)\n\nX = targ_enc.transform(X.reset_index(drop=True))\nX_test = targ_enc.transform(X_test.reset_index(drop=True))","2a489326":"ss = MinMaxScaler()\nX = ss.fit_transform(X)\nss2 = MinMaxScaler()\nX_test = ss2.fit_transform(X_test)\n\nss1 = MinMaxScaler()\nY = ss1.fit_transform(Y.values.reshape(-1, 1))\nY = Y.reshape(Y.shape[0])\n\ndel shops\ndel items\ndel cats\ngc.collect()","cb15b791":"model1 = CatBoostRegressor(iterations=5, depth=10,# learning_rate=0.01, \n                           loss_function='RMSE', verbose=0, thread_count=-1)# task_type=\"GPU\"\n\nmodel2 = RandomForestRegressor(n_estimators = 15, max_depth = 10, n_jobs=-1)\n\nmodel3 = xgb.XGBRegressor(objective ='reg:squarederror', learning_rate = 0.01, eval_metric='rmse',\n                max_depth = 10, n_estimators = 10, tree_method = 'exact')","8158c0ac":"training, valid, ytraining, yvalid = train_test_split(X,Y, test_size=0.3)\n\ndel X\ndel Y\ngc.collect()","4493c216":"%%time\n\nmodel = StackingRegressor(estimators=[('cb',model1), ('xg', model3)], final_estimator=model2, cv=3)\nmodel.fit(training,ytraining)","13fcb8d7":"preds = model.predict(valid)\nprint(np.sqrt(mean_squared_error(preds, yvalid)))\nXX=X_test.copy()\npredictions = model.predict(XX)","c8f8cf5b":"predictions = ss1.inverse_transform(predictions.reshape(-1,1)).clip(0,20)\npredictions = predictions.reshape(predictions.shape[0])\ndf = pd.DataFrame({'ID': id_column, 'item_cnt_month': predictions})\ndf.to_csv('coursera_new.csv', index=False)","b6be0439":"df","1d6275ae":"df.sort_values(by=\"item_cnt_month\",ascending=False).head(20)","55789ffc":"from joblib import dump, load\ndump(model, 'coursera_model.joblib')","d83f6644":"<h1>Part 1: Pre processing<\/h1>\n\nLoad the libraries and data","f0e448f0":"# Part 2: Modelling and predicting","a8c7f81d":"# 3 Predict and save"}}