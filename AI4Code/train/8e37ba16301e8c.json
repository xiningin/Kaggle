{"cell_type":{"4f901a77":"code","801b2924":"code","ca6002ad":"code","2270aa8d":"code","5e66a0d8":"code","e1446d02":"code","443c37b5":"code","f3f5ad22":"code","5373344b":"code","1dcef20f":"code","c08741a5":"code","21aa8051":"code","d9e61c5e":"code","ac48efea":"code","c68fb5d7":"code","475cb578":"code","1991cb19":"code","25ec97a4":"code","1e5a2fe8":"code","75854720":"code","8d2d27fe":"code","f721b6cb":"code","36e27ebd":"code","15986998":"code","6935c8e7":"code","5398d39d":"code","4d45312e":"code","329075b5":"code","32458fa0":"code","bf7b96c6":"code","8f0eb6a4":"code","a005c98d":"code","7f8dcb0c":"code","ac899c5e":"code","4882f57e":"markdown","bf6321da":"markdown","be674712":"markdown","8bf49818":"markdown","574de446":"markdown","ba3723e5":"markdown","d7ca78a9":"markdown","8e3bdf49":"markdown","a6a30eb2":"markdown","bc2363fa":"markdown","744e4c72":"markdown","a7a6b846":"markdown","6fc2c98d":"markdown","79e63f4b":"markdown","92e7bb53":"markdown","29127bf0":"markdown","b2fcb01c":"markdown","4b9735d6":"markdown","8e339180":"markdown","4adde201":"markdown"},"source":{"4f901a77":"# Importing the necessary libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n\nimport lightgbm as lgb\nfrom xgboost import XGBRegressor\nfrom mlxtend.regressor import StackingCVRegressor\nfrom catboost import CatBoostRegressor\n\n\n#some stats stuff\nfrom scipy import stats\nfrom scipy.stats import norm, skew\n\n# Setting pandas display property\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\nimport warnings\nwarnings.filterwarnings('ignore')","801b2924":"TRAIN_PATH = '..\/input\/vishwatatva-datacon-2020\/train.csv'\nTEST_PATH = '..\/input\/vishwatatva-datacon-2020\/test.csv'\n\ntrain = pd.read_csv(TRAIN_PATH, low_memory=False)\ntest = pd.read_csv(TEST_PATH, low_memory=False)","ca6002ad":"# Converting numerical variables which are in object to float (in train data)\ncols = ['width', 'height', 'framerate', 'i_frames', 'frames', 'i_size', 'p_size', 'b_size', 'size', 'bitrate_output',\n        'framerate_output', 'output_width', 'output_height', 'total_processing_time']\n\ntrain[cols] = train[cols].apply(pd.to_numeric, errors='coerce', axis=1)","2270aa8d":"# Removing the Ids from train and test, as they are unique for each row and hence not useful for the model\ntrain_ID = train['id']\ntest_ID = test['id']\ntrain.drop(['id'], axis=1, inplace=True)\ntest.drop(['id'], axis=1, inplace=True)\ntrain.shape, test.shape","5e66a0d8":"# Replacing all the '-'s with NaN \ntrain.replace('-', np.NaN, inplace=True)\ntest.replace('-', np.NaN, inplace=True)","e1446d02":"# Imputing missing values of target variables by mean\ntrain['total_processing_time'].isnull().sum()\ntrain['total_processing_time'] = train['total_processing_time'].fillna(train['total_processing_time'].mean())","443c37b5":"train_labels = train['total_processing_time']\ntrain_features = train.drop(['total_processing_time'], axis=1)\ntest_features = test\n\ncombined = pd.concat([train_features, test_features]).reset_index(drop=True)\ncombined.shape","f3f5ad22":"# Imputing categorical features\ncombined['coding_standard_output'] = combined['coding_standard_output'].fillna(\"mpeg4\")\ncombined['coding_standard'] = combined['coding_standard'].fillna('h264')","5373344b":"# label encoding categorical features\nenc = LabelEncoder()\ncombined['coding_standard_output'] = enc.fit_transform(combined['coding_standard_output'])\ncombined['coding_standard'] = enc.fit_transform(combined['coding_standard'])","1dcef20f":"# imputing numerical features\nnum_imputer = KNNImputer(n_neighbors=5)\ncombined = pd.DataFrame(num_imputer.fit_transform(combined),columns = combined.columns)","c08741a5":"# checking if any missing values are still there\ncombined.isnull().sum()","21aa8051":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ncombined_std = scaler.fit_transform(combined)","d9e61c5e":"combined_scaled = pd.DataFrame(scaler.fit_transform(combined),columns = combined.columns)","ac48efea":"combined_scaled.head()","c68fb5d7":"scaler_labels = StandardScaler()\ntrain_labels = np.array(train_labels).reshape(-1,1)\ntrain_labels_sc = scaler_labels.fit_transform(train_labels)\ntrain_labels_sc = train_labels_sc.flatten()","475cb578":"train_features_scaled = combined_scaled.iloc[:len(train_labels), :]\ntest_features_scaled = combined_scaled.iloc[len(train_labels):, :]\ntrain_features_scaled.shape, train_labels_sc.shape, test_features_scaled.shape","1991cb19":"rf = RandomForestRegressor()\nrf.fit(train_features_scaled, train_labels_sc)","25ec97a4":"## Checking cross-validation scores\n\n# cv_scores = cross_val_score(rf, train_features_scaled, train_labels_sc, cv=5)\n# print(cv_scores)\n# cv_scores.mean()","1e5a2fe8":"# predicting values\nrf_pred = rf.predict(test_features_scaled)\nrf_pred = scaler_labels.inverse_transform(rf_pred)","75854720":"lgbm = lgb.LGBMRegressor()\nlgbm.fit(train_features_scaled, train_labels_sc)\n# cv_scores = cross_val_score(lgbm, train_features_scaled, train_labels_sc, cv=5)\n# print(cv_scores)\n# cv_scores.mean()","8d2d27fe":"## Checking cross-validation scores\n# cv_scores = cross_val_score(lgbm, train_features_scaled, train_labels_sc, cv=5)\n# print(cv_scores)\n# cv_scores.mean()","f721b6cb":"#predicting values\nlgbm_pred = lgbm.predict(test_features_scaled)\nlgbm_pred = scaler_labels.inverse_transform(lgbm_pred)","36e27ebd":"xgboost = XGBRegressor()\nxgboost.fit(train_features_scaled, train_labels_sc)\n","15986998":"## Checking cross-validation scores\n\n# cv_scores = cross_val_score(xgboost, train_features_scaled, train_labels_sc, cv=5)\n# print(cv_scores)\n# cv_scores.mean()","6935c8e7":"# predicting values\nxgb_pred = xgboost.predict(test_features_scaled)\nxgb_pred = scaler_labels.inverse_transform(xgb_pred)","5398d39d":"catboost = CatBoostRegressor()\ncatboost.fit(train_features_scaled, train_labels_sc)\n","4d45312e":"## Checking cross-validation scores\n# cv_scores = cross_val_score(catboost, train_features_scaled, train_labels_sc, cv=5)\n# print(cv_scores)\n# cv_scores.mean()","329075b5":"# predicting values\ncat_pred = catboost.predict(test_features_scaled)\ncat_pred = scaler_labels.inverse_transform(cat_pred)","32458fa0":"from mlxtend.regressor import StackingCVRegressor\n\nstack_gen = StackingCVRegressor(regressors=(xgboost, lgbm,rf, catboost),\n                                meta_regressor=catboost,\n                                use_features_in_secondary=True)\n\n\nstack_gen_model = stack_gen.fit(np.array(train_features_scaled), np.array(train_labels_sc))\ntest_features_scaled.columns = ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19']\n\nstack_pred = stack_gen_model.predict(test_features_scaled)\nstack_pred = scaler_labels.inverse_transform(stack_pred)","bf7b96c6":"pred_df = pd.DataFrame({'rf_pred':rf_pred, 'xgb_pred':xgb_pred, 'lgbm_pred':lgbm_pred, 'stack_pred':stack_pred, 'cat_pred':cat_pred})\npred_df.head()","8f0eb6a4":"final_pred = stack_pred*0.6 + cat_pred*0.2 + rf_pred*0.1 + lgbm_pred*0.1","a005c98d":"final_pred[:5]","7f8dcb0c":"sub = pd.DataFrame()\nsub['id'] = test_ID\nsub['total_processing_time'] = final_pred\nsub.to_csv('submission.csv', index=False)","ac899c5e":"submit_path = '.\/submission.csv'\nsubmit = pd.read_csv(submit_path)\nsubmit.shape","4882f57e":"### XGBoost ","bf6321da":"### Light Gradient Boosting Machine (LGBM)","be674712":"### Removing id columns from train and test","8bf49818":"### Random Forest","574de446":"### Imputing categorical features by most frequent value","ba3723e5":"### CatBoost","d7ca78a9":"## Blending","8e3bdf49":"### Imputing numerical features","a6a30eb2":"### Importing libraries","bc2363fa":"### Weighted average of all significant models","744e4c72":"## Stacking ","a7a6b846":"#### Getting data","6fc2c98d":"### Imputing target variable","79e63f4b":"## Scaling data","92e7bb53":"## Handling missing values","29127bf0":"## Splitting data back into train and test set","b2fcb01c":"### Encoding categorical features","4b9735d6":"## Combining train and test data (for imputing)","8e339180":"Stacking all the models and passing the stacked prediction to meta_regressor (catboost) for final prediction","4adde201":"## Modelling"}}