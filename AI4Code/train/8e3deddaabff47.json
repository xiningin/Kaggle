{"cell_type":{"2dbf4e78":"code","7bd4bb53":"code","9afb741c":"code","4c13541d":"code","28f12e53":"code","d35fa3da":"code","04c41f5d":"code","dee0d2df":"code","582ff779":"code","adc96ef6":"code","d1ba85ce":"code","b980021a":"code","4b1cf45e":"code","f5224db1":"markdown","afc42397":"markdown","5f232d91":"markdown","54d1157f":"markdown","35ad3c8c":"markdown","3428827d":"markdown"},"source":{"2dbf4e78":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n!pip install tensorflow==2.0.0-rc1\nimport tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","7bd4bb53":"imdb = keras.datasets.imdb\n\n(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\nword_index = imdb.get_word_index()","9afb741c":"\n# The first indices are reserved\nword_index = {k:(v+3) for k,v in word_index.items()}\nword_index[\"<PAD>\"] = 0\nword_index[\"<START>\"] = 1\nword_index[\"<UNK>\"] = 2  # unknown\nword_index[\"<UNUSED>\"] = 3\n\nreverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n\ndef decode_review(text):\n    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n\ntrain_data = keras.preprocessing.sequence.pad_sequences(train_data,\n                                                        value=word_index[\"<PAD>\"],\n                                                        padding='post',\n                                                        maxlen=256)\n\ntest_data = keras.preprocessing.sequence.pad_sequences(test_data,\n                                                       value=word_index[\"<PAD>\"],\n                                                       padding='post',\n                                                       maxlen=256)","4c13541d":"print(train_data[0])","28f12e53":"x_val = train_data[:10000]\npartial_x_train = train_data[10000:]\n\ny_val = train_labels[:10000]\npartial_y_train = train_labels[10000:]","d35fa3da":"# input shape is the vocabulary count used for the movie reviews (10,000 words)\nvocab_size = 10000\n\nmodel = keras.Sequential()\nmodel.add(keras.layers.Embedding(vocab_size, 128))\nmodel.add(keras.layers.GlobalAveragePooling1D())\nmodel.add(keras.layers.Dense(128, activation=tf.nn.relu))\nmodel.add(keras.layers.Dense(128, activation=tf.nn.relu))\nmodel.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n\nmodel.summary()\n\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['acc','binary_crossentropy'])\n\ncallback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\nhistory = model.fit(partial_x_train,\n                    partial_y_train,\n                    epochs=40,\n                    batch_size=512,callbacks=[callback],\n                    validation_data=(x_val, y_val),\n                    verbose=2)","04c41f5d":"results = model.evaluate(test_data, test_labels)\n\nprint(results)","dee0d2df":"modelL2 = keras.Sequential()\nmodelL2.add(keras.layers.Embedding(vocab_size, 128))\nmodelL2.add(keras.layers.GlobalAveragePooling1D())\nmodelL2.add(keras.layers.Dense(128, kernel_regularizer=keras.regularizers.l2(0.001), activation=tf.nn.relu))\nmodelL2.add(keras.layers.Dense(128, kernel_regularizer=keras.regularizers.l2(0.001), activation=tf.nn.relu))\nmodelL2.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n\nmodelL2.summary()\n\nmodelL2.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['acc','binary_crossentropy'])\n\ncallback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\nL2_history= modelL2.fit(partial_x_train,\n                    partial_y_train,\n                    epochs=40,\n                    batch_size=512,callbacks=[callback],\n                    validation_data=(x_val, y_val),\n                    verbose=2)","582ff779":"L2_result = modelL2.evaluate(test_data,test_labels)\nprint(L2_result)","adc96ef6":"modeldrop = keras.Sequential()\nmodeldrop.add(keras.layers.Embedding(vocab_size, 128))\nmodeldrop.add(keras.layers.GlobalAveragePooling1D())\nmodeldrop.add(keras.layers.Dense(128, activation=tf.nn.relu))\nkeras.layers.Dropout(0.5),\nmodeldrop.add(keras.layers.Dense(128, activation=tf.nn.relu))\nkeras.layers.Dropout(0.5),\nmodeldrop.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n\nmodeldrop.summary()\n\nmodeldrop.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['acc','binary_crossentropy'])\n\ncallback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\nhistory_drop = modeldrop.fit(partial_x_train,\n                    partial_y_train,\n                    epochs=40,\n                    batch_size=512,callbacks=[callback],\n                    validation_data=(x_val, y_val),\n                    verbose=2)","d1ba85ce":"drop_result = modeldrop.evaluate(test_data,test_labels)\nprint(drop_result)","b980021a":"import matplotlib.pyplot as plt\ndef plot_history(histories, key='binary_crossentropy'):\n  plt.figure(figsize=(16,10))\n\n  for name, history in histories:\n    val = plt.plot(history.epoch, history.history['val_'+key],\n                   '--', label=name.title()+' Val')\n    plt.plot(history.epoch, history.history[key], color=val[0].get_color(),\n             label=name.title()+' Train')\n\n  plt.xlabel('Epochs')\n  plt.ylabel(key.replace('_',' ').title())\n  plt.legend()\n\n  plt.xlim([0,max(history.epoch)])","4b1cf45e":"plot_history([('baseline', history),\n              ('dropout', history_drop),('regularization',L2_history)])","f5224db1":"# Plot Results ","afc42397":"# Overfitting and underfitting \n## in previus notebook we learn early stopping with basic text classification to avoid overfitting :\nhttps:\/\/www.kaggle.com\/salahuddinemr\/basic-text-classification\n## in this notebook we will use two methods to avoid overfitting: \n## 1- Drop out layer\n## 2- Regularization ","5f232d91":"# Drop out Model","54d1157f":"# Basic Model","35ad3c8c":"### from plot we can see the regularization method has the best result then drop out ","3428827d":"# Regularizer Model"}}