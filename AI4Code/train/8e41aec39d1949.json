{"cell_type":{"8b6a9edb":"code","ae55eba6":"code","1e7e5c55":"code","a7917569":"code","414d3a56":"code","a8ebdef9":"code","8c499507":"code","68ab9084":"code","4ed496d2":"code","1b746ed8":"code","c0ebc3b6":"code","5a9161dc":"code","cc9337ac":"code","ffed32ad":"code","4835dc46":"code","1ce5320e":"code","589d47aa":"code","5acfc572":"code","e3ab22f5":"code","19f263c5":"code","e277d469":"code","a391778b":"code","e9fba461":"code","b6f0ade8":"code","6cb01cfa":"code","8ef7f8a6":"code","7bade544":"code","54e8ce8f":"code","282895cc":"code","d4802ba2":"code","faaca83d":"code","1338966c":"code","34bba0ac":"code","ade182c4":"code","f6d375e1":"code","f5e2d796":"code","9653a984":"code","4cecb96f":"code","b0962bdf":"code","12ed70b3":"code","8b079d64":"code","ba464af2":"code","066c57af":"markdown","8f2062a8":"markdown","294ab19d":"markdown","109d82cd":"markdown","fd7b2de6":"markdown","3c472b56":"markdown","8f423fc8":"markdown","781ab9df":"markdown","7e1a6a2b":"markdown","e22519f9":"markdown","0fa7cdaa":"markdown","64e15bc8":"markdown","05cae337":"markdown","a1321a84":"markdown","1b6c9975":"markdown","2070d4ff":"markdown","c85b15f2":"markdown","078a3cce":"markdown","53cc7a4d":"markdown","392278f4":"markdown","120f7cd2":"markdown","78d96384":"markdown","329ed4c9":"markdown","0694ee6c":"markdown"},"source":{"8b6a9edb":"import numpy as np\nimport pandas as pd\nimport pandas_profiling as pp\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nfrom wordcloud import WordCloud\n\nimport missingno as ms\n\nimport string\n\n# NLTK\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\n# spaCy\nimport spacy\nfrom spacy import displacy\nfrom spacy.lang.en.stop_words import STOP_WORDS","ae55eba6":"# Set Plot style\nplt.style.use('fivethirtyeight')","1e7e5c55":"# Load data\ndf = pd.read_csv('..\/input\/real-or-fake-fake-jobposting-prediction\/fake_job_postings.csv', index_col=0)","a7917569":"# print head of data\ndf.head()","414d3a56":"# Info of DataFrame\ndf.info()","a8ebdef9":"df.dtypes.value_counts()","8c499507":"# Statistical Description\ndf.describe()","68ab9084":"ms.matrix(df)\nplt.show()","4ed496d2":"ms.bar(df)\nplt.show()","1b746ed8":"# counts of missing value for each feature and target\ndf.isnull().sum()","c0ebc3b6":"# Drop salary_range\ndel df['salary_range']","5a9161dc":"# Fill null value\ndf.fillna(\"\", inplace=True)","cc9337ac":"fig, ax = plt.subplots(1, 2)\n\nsns.countplot(x='fraudulent', data=df, ax=ax[0])\nax[1].pie(df['fraudulent'].value_counts(), labels=['Real Post', 'Fake Post'], autopct='%1.1f%%')\n\nfig.suptitle('Bar & Pie charts of Fraudulent value count', fontsize=16)\nplt.show()","ffed32ad":"fig, ax = plt.subplots(1, 2)\n\nchart = sns.countplot(x = 'required_experience', data=df[df['fraudulent']==0], ax=ax[0])\nchart.set_xticklabels(chart.get_xticklabels(), rotation=90)\nax[0].set_title('Real required experience')\n\nchart = sns.countplot(x = 'required_experience', data=df[df['fraudulent']==1], ax=ax[1])\nchart.set_xticklabels(chart.get_xticklabels(), rotation=90)\nax[1].set_title('Fake required experience')\nplt.show()","4835dc46":"# Create features from text columns\ntext_features = df[[\"title\", \"company_profile\", \"description\", \"requirements\", \"benefits\",\"fraudulent\"]]","1ce5320e":"# print samples of the text_features\ntext_features.sample(5)","589d47aa":"columns = ['title', 'company_profile', 'description', 'requirements', 'benefits']","5acfc572":"for col in columns:\n    text_features[col+'_len'] = text_features[col].apply(len)","e3ab22f5":"real = text_features[text_features['fraudulent']==0]\nfake = text_features[text_features['fraudulent']==1]\n\nfig, ax = plt.subplots(5, 2, figsize=(15, 15))\nax[0, 0].set_title('Character Count of Real Post ')\nax[0, 1].set_title('Character Count of Fake Post ')\n\nfor i in range(5):\n    for j in range(2):\n        if j==0:\n            ax[i, j].hist(real[columns[i]+'_len'], color='g', bins=15);\n            ax[i, j].set_ylabel( columns[i] )\n        else:\n            ax[i, j].hist(fake[columns[i]+'_len'], color='r', bins=15);\n\nplt.show()","19f263c5":"for col in columns:\n    text_features[col+'_len_word'] = text_features[col].apply(lambda x: len(x.split()))","e277d469":"real = text_features[text_features['fraudulent']==0]\nfake = text_features[text_features['fraudulent']==1]\n\nfig, ax = plt.subplots(5, 2, figsize=(15, 15))\nax[0, 0].set_title('Word Count of Real Post ')\nax[0, 1].set_title('Word Count of Fake Post ')\n\nfor i in range(5):\n    for j in range(2):\n        if j==0:\n            ax[i, j].hist(real[columns[i]+'_len_word'], color='g', bins=15);\n            ax[i, j].set_ylabel( columns[i] )\n        else:\n            ax[i, j].hist(fake[columns[i]+'_len_word'], color='r', bins=15);\n\nplt.show()","a391778b":"def avg_word_ln(string):\n    words = string.split()\n    word_len = [len(word) for word in words]\n    try:\n        return sum(word_len)\/len(words)\n    except:\n        return 0\n\nfor col in columns:\n    text_features[col+'_avg_word_ln'] = text_features[col].apply(avg_word_ln)","e9fba461":"real = text_features[text_features['fraudulent']==0]\nfake = text_features[text_features['fraudulent']==1]\n\nfig, ax = plt.subplots(5, 2, figsize=(15, 15))\nax[0, 0].set_title('Average Word Count of Real Post ')\nax[0, 1].set_title('Average Word Count of Fake Post ')\n\nfor i in range(5):\n    for j in range(2):\n        if j==0:\n            ax[i, j].hist(real[columns[i]+'_avg_word_ln'], color='g', bins=15);\n            ax[i, j].set_ylabel( columns[i] )\n        else:\n            ax[i, j].hist(fake[columns[i]+'_avg_word_ln'], color='r', bins=15);\n\nplt.show()","b6f0ade8":"# delete text_features\ndel text_features","6cb01cfa":"# Create new feature jd (job description)\ndf['jd'] = df['title'] + ' ' + df['location'] + ' ' + df['department'] + ' ' + df['company_profile'] + ' ' + df['description'] + ' ' + df['requirements'] + ' ' + df['benefits'] + ' ' + df['employment_type'] + ' ' + df['required_education'] + ' ' + df['industry'] + ' ' + df['function'] ","8ef7f8a6":"# drop features\ndel df['title']\ndel df['location']\ndel df['department']\ndel df['company_profile']\ndel df['description']\ndel df['requirements']\ndel df['benefits']\ndel df['employment_type']\ndel df['required_experience']\ndel df['required_education']\ndel df['industry']\ndel df['function']","7bade544":"df.head()","54e8ce8f":"# Load spacy large model\nnlp = spacy.load('en_core_web_lg')","282895cc":"df['jd'] = df['jd'].apply(str.lower)","d4802ba2":"df['jd'].iloc[0]","faaca83d":"def remove_punctuation_and_stop_words(s):\n    punctuations = list(string.punctuation)\n    \n    strings = \" \".join([token for token in word_tokenize(s) if not token in punctuations+list(STOP_WORDS)])\n    return strings\n    ","1338966c":"# Apply above function to the jd feature\ndf['jd'] = df['jd'].apply(remove_punctuation_and_stop_words)","34bba0ac":"# After removing puctuations and stopwords\ndf['jd'].iloc[0]","ade182c4":"doc = nlp(df['jd'].iloc[0])","f6d375e1":"def lemmatization(s):\n    doc = nlp(s)\n    return \" \".join([token.lemma_ for token in doc])","f5e2d796":"# Apply above function to the jd feature\ndf['jd'] = df['jd'].apply(lemmatization)","9653a984":"df['jd'].iloc[0]","4cecb96f":"# take first record and visualize NER\ndoc = nlp(df['jd'].iloc[0])\n\ndisplacy.render(doc, style=\"ent\")","b0962bdf":"displacy.render(doc, style=\"dep\")","12ed70b3":"# WordCloud Real\/Fake post\n\nreal = df[df['fraudulent']==0]['jd']\nfake = df[df['fraudulent']==1]['jd']","8b079d64":"# Real WordCloud\n\nplt.figure(figsize = (20,20))\nwc = WordCloud(width = 1600 , height = 800 , max_words = 3000).generate(\" \".join(real))\nplt.imshow(wc , interpolation = 'bilinear')\nplt.show()","ba464af2":"# Fake WordCloud\n\nplt.figure(figsize = (20,20))\nwc = WordCloud(width = 1600 , height = 800 , max_words = 3000).generate(\" \".join(fake))\nplt.imshow(wc , interpolation = 'bilinear')\nplt.show()","066c57af":"3. Lemmatization\n\n    * It is the process of converting a word lowercase base form or lemma.\n    * This is extremely powerful standarization\n    * Examples:\n            am, are, is --->  be\n            n't        ---->  not\n            've        ---->  have","8f2062a8":"# Basic Feature Extraction\n* Number of characters\n* Number of words\n* Average Word Length\n","294ab19d":"**WordCloud**","109d82cd":"The above snippet shows that 13 object dtypes and 4 int64 dtypes","fd7b2de6":"**Salary_range feature containt lot's of missing value.**","3c472b56":"2. Removing extra white spaces, removing Punctuation & Stop words\n\n**Tokenization**\n* It is a process of splitting a string into the consituent tokens.\n* These tokens may be sentence, words or punctutions and is specific to a percular language(In our case: English)\n\n  I'm using word_tokenize from **nltk libarary**","8f423fc8":"**Missingno** is a visualization tools that highlights the missing value in the entire dataset.","781ab9df":"# Data Preprocessing\n1. Converting words into lowercase\n2. Removing leading white spaces \n3. Removing punctuations & stop words\n4. Lemamtization","7e1a6a2b":"* White line shows the missing record in the dataset.\n* It looks lost of missing value in the dataset.\n\n**Let's check out the missing value count**\n","e22519f9":"1. Converting word into lowercase","0fa7cdaa":"# Getting the data","64e15bc8":"2. Number of words","05cae337":"## Missing value","a1321a84":"**Required Experience Real\/Fake**","1b6c9975":"**Count value of flaudulent(target)**","2070d4ff":"1. Number of characters","c85b15f2":"# Imports","078a3cce":"**The value above bar shows the total count of non-missing value**","53cc7a4d":"3. Average Word Length","392278f4":"**Visualize named entities**","120f7cd2":"**After compare all the basic feature such as word length, character length and avg word length, fake post has less count than the real post.**","78d96384":"**NER(Named-entity recognition)**\n\n1. An NER is anything that can be denoted with the proper name or a pronoun.\n2. Indentifying and classifying named entity into predefined category.\n3. Categorization include **Person**, **Organization**, **Country**, etc.","329ed4c9":"**All characters converted into lower case.**","0694ee6c":"SpaCy is great library for NER and NER visualization\nRepresents: \n1. **new york** is GPE(Geopolitical entity, i.e. countries, cities, states.) \n2. **mario batali** is Person \n3. **twitter** ORG(Organizatio)\n\n\nVisualize Dependencies"}}