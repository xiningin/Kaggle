{"cell_type":{"4ff55b4e":"code","d1980f06":"code","7fabd4b3":"code","64147185":"code","f9068235":"code","a9fa656d":"code","f572c28d":"code","4dbb6484":"code","591451c9":"code","3f3a0590":"code","6e5fb659":"code","39e18840":"code","2f7ad268":"code","bfbed65c":"code","d6e10d8d":"code","03fc2a05":"code","f4208200":"code","99c79977":"code","e99040a4":"code","f74904ea":"markdown","c3a5e5d7":"markdown","90af4b42":"markdown","9e803260":"markdown","f9446a68":"markdown","6e699475":"markdown","a094b1b5":"markdown","9df400e9":"markdown","9ebe1a72":"markdown"},"source":{"4ff55b4e":"#remove hastag if having problem with gensim\nimport sys\n\n#!$sys.executable -m pip install keras\n#!$sys.executable -m pip install nltk\nimport nltk\n\n#!$sys.executable -m pip install gensim\n\n#nltk.download('punkt')\n\n#nltk.download('stopwords')\n\n#!$sys.executable -m pip install tensorflow","d1980f06":"import pandas as pd\nimport numpy as np\nimport os","7fabd4b3":"DATASET_DIR=\"Dataset and discription\/\"","64147185":"X = pd.read_csv(os.path.join(DATASET_DIR, 'training_set_rel3.tsv'), sep='\\t', encoding='ISO-8859-1')","f9068235":"X.head()","a9fa656d":"X=X.dropna(axis=1)","f572c28d":"X=X.drop(columns=['rater1_domain1','rater2_domain1'])\n","4dbb6484":"X","591451c9":"Y=X['domain1_score']","3f3a0590":"minimum_scores = np.array([-1, 2, 1, 0, 0, 0, 0, 0, 0])\nmaximum_scores = np.array([-1, 12, 6, 3, 3, 4, 4, 30, 60])","6e5fb659":"old_min = minimum_scores[X['essay_set']]\nold_max = maximum_scores[X['essay_set']]\nold_range = old_max - old_min \nnew_range = (10 - 0)  \nX['score'] = np.around((((X['domain1_score'] - old_min) * new_range) \/ old_range) )\n\nX\n\n","39e18840":"from nltk.corpus import stopwords\nimport nltk\n\nimport re\ndef wordlist(essay, remove_stopwords):\n    \n    essay = re.sub(\"[^a-zA-Z]\", \" \", essay)\n    words = essay.lower().split()\n    if remove_stopwords:\n        stops = set(stopwords.words(\"english\"))\n        words = [w for w in words if not w in stops]\n    return (words)","2f7ad268":"def Make_sentences(essay, remove_stopwords):\n    tokenizer = nltk.data.load('tokenizers\/punkt\/english.pickle')\n    raw_sentences = tokenizer.tokenize(essay.strip())\n    sentences = []\n    for raw_sentence in raw_sentences:\n        if len(raw_sentence) > 0:\n            sentences.append(wordlist(raw_sentence, remove_stopwords))\n    return sentences","bfbed65c":"def makeFeatureVec(words, model, num_features):\n    \n    featureVec = np.zeros((num_features),dtype=\"float32\")\n    num_words = 0.\n    index2word_set = set(model.wv.index_to_key)\n    for word in words:\n        if word in index2word_set:\n            num_words += 1\n            featureVec = np.add(featureVec,model.wv[word])        \n    featureVec = np.divide(featureVec,num_words)\n    return featureVec\n\n    ","d6e10d8d":"def getAvgFeatureVecs(essays, model, num_features):\n\n    counter = 0\n    essayFeatureVecs = np.zeros((len(essays),num_features),dtype=\"float32\")\n    for essay in essays:\n        essayFeatureVecs[counter] = makeFeatureVec(essay, model, num_features)\n        counter = counter + 1\n    return essayFeatureVecs","03fc2a05":"from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Lambda, Flatten\nfrom tensorflow.keras.models import Sequential, load_model, model_from_config\nimport tensorflow.keras.backend as K\n\ndef get_model():\n    \"\"\"Define the model.\"\"\"\n    model = Sequential()\n    model.add(LSTM(300, dropout=0.2, recurrent_dropout=0.4, input_shape=[1, 300], return_sequences=True))\n    model.add(LSTM(64, recurrent_dropout=0.4))\n    model.add(Dropout(0.5))\n    model.add(Dense(1, activation='relu'))\n\n    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n    model.summary()\n\n    return model","f4208200":"from gensim.models import Word2Vec\n# define training data\nexample2='hi I am an example1 sentence here to explain thw working of above function ,DONT MIND THE ENGLISH i AM REALLT BAD AT ENGLISH'\nexample1='hi I am an example22 sentence here to explain thw working of above function ,I WOULD REALLY APPRECIATE FEEDBACKS '\nexample3='hi I am an example33 sentence here to explain thw working of above function ,I am really thanksful for the helps online'\ns=[]\ns +=Make_sentences(example1,remove_stopwords=True)\ns +=Make_sentences(example2,remove_stopwords=True)\ns +=Make_sentences(example3,remove_stopwords=True)\nprint(s)\n","99c79977":"model = Word2Vec(s, min_count=1)\n# summarize the loaded model\nprint(model)\n# summarize vocabulary\nwords = list(model.wv.index_to_key )\nprint(words)\n# access vector for one word\nprint(model.wv['hi'])\n# save model\nmodel.save('model.bin')\n# load model\nnew_model = Word2Vec.load('model.bin')\nprint(new_model)","e99040a4":"from sklearn.model_selection import StratifiedKFold\nfrom keras.callbacks import ModelCheckpoint\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import cohen_kappa_score\nimport nltk\nfrom gensim.models import Word2Vec\nskf = StratifiedKFold(n_splits=5,shuffle=True)\ncount=1\nresults=[]\nfor train,test in skf.split(X,Y):\n        print(\"\\n--------Fold {}--------\\n\".format(count))\n        X_test, X_train, y_test, y_train = X.iloc[test], X.iloc[train], Y.iloc[test], Y.iloc[train]\n        trainE = X_train['essay']\n        testE= X_test['essay']\n        sentences=[]\n        for essay in trainE:\n        # Obtaining all sentences from the training essays.\n        \n            sentences +=Make_sentences(essay, remove_stopwords = True)\n        \n        #Initializing different parameters for the word2vec model to be used\n    \n        num_features = 300\n        min_word_count = 40\n        num_workers = 4\n        context = 10\n        downsampling = 1e-3\n\n        print(\"Training Word2Vec Model...\")\n        \n        #Initializing model fro vectorization\n        \n    \n        #initializing model and loading parameters\n    \n        model = Word2Vec(sentences,vector_size=300, workers=num_workers, min_count = min_word_count, window = context)\n        #avoiding normalization to not reduce the essence of some words used in context\n    \n        model.init_sims(replace=False)\n    \n        #saving model\n    \n        model.wv.save_word2vec_format('word2vecmodel.bin', binary=True)\n        \n        print(\"--------------- Traning complete-------------------\")\n        \n        print(\"--------------- Initializing vectorization on train set -------------------\")\n        \n        \n        #generating vectors train\n    \n        clean_train = []\n        for essay in trainE:\n            clean_train.append(wordlist(essay, remove_stopwords=True))\n        trainDataVecs = getAvgFeatureVecs(clean_train, model, num_features)\n    \n        print(\"--------------completed-----------------\")\n    \n        print(\"--------------- Initializing vectorization on test set -------------------\")\n        \n        #generating vectors for test\n    \n        clean_test = []\n        for essay in testE:\n            clean_test.append(wordlist(essay, remove_stopwords=True))\n        testDataVecs = getAvgFeatureVecs(clean_test, model, num_features)\n        \n        trainDataVecs = np.array(trainDataVecs)\n        testDataVecs = np.array(testDataVecs)\n        \n        print(\"------------Complete----------------------\") \n        \n        print(\"------------Adding extra dimension to essay----------------------\") \n        \n        #Adding extra dimension to essay\n        \n        trainDataVecs = np.reshape(trainDataVecs, (trainDataVecs.shape[0], 1, trainDataVecs.shape[1]))\n        \n        testDataVecs = np.reshape(testDataVecs, (testDataVecs.shape[0], 1, testDataVecs.shape[1]))\n        \n        print(\"------------Complete----------------------\") \n        \n        print(\"------------Traning the Preprocessed Data----------------------\") \n        \n        #Traning the Preprocessed Data\n        \n        lstm_model = get_model()\n        \n        print('-----------Complete--------------')\n        \n        print('------------------fitting data----------------------')\n        \n        #fitting dataset\n        checkpoint = ModelCheckpoint('model-{epoch:03d}.model',monitor='val_loss',verbose=0,save_best_only=True,mode='auto')\n        lstm_model.fit(trainDataVecs, y_train, batch_size=64, epochs=50,callbacks=[checkpoint],validation_split=0.1)\n        \n        print('-----------Complete--------------')\n        \n        print('---------------Predicting test set------------------------')\n        \n        #predicting test\n        \n        y_pred = lstm_model.predict(testDataVecs)\n    \n        # Save any one of the 8 models.\n        \n        if count == 5:\n             lstm_model.save('.\/final_lstm.h5')\n            \n        # Round y_pred to the nearest integer.\n        y_pred = np.around(y_pred)\n    \n            # Evaluate the model on the evaluation metric. \"Quadratic mean averaged Kappa\"\n        result = cohen_kappa_score(y_test.values,y_pred,weights='quadratic')\n        print(\"Kappa Score: {}\".format(result))\n        results.append(result)\n\n        count += 1\n","f74904ea":"# Example of vectorization","c3a5e5d7":"# Tokanization and vectorization functions","90af4b42":"# Data loading and cleaning","9e803260":"# Example of tokanization","f9446a68":"**The traning of the model is complete ****","6e699475":"# Example of pre processing , tokanization then vectorivation","a094b1b5":"# Onto the real model","9df400e9":"# Defining production model","9ebe1a72":"# Run these after removing hastag if any on of the given module is missing "}}