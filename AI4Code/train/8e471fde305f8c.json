{"cell_type":{"2454b867":"code","995eef30":"code","a4766e76":"code","dac5cbc8":"code","0fabdb15":"code","3ddd7a32":"code","94d97159":"code","2ee98d8b":"code","6d9ef75f":"code","58958774":"code","6bba3747":"code","b2db8f75":"code","606f801e":"code","6801fb57":"code","deeeeb89":"code","816e66dd":"code","ba1fab6f":"code","b6d2251d":"code","989d901e":"code","d8d17321":"code","5aef86f6":"code","6345db91":"code","e8834b28":"code","2792950d":"code","ea45a4f9":"code","4a725884":"code","656046e4":"code","9c96a107":"code","8e7843b1":"code","51c5af5f":"code","0900098b":"code","b2e9d313":"code","3206c656":"code","ad63b56d":"code","76c0e9a0":"code","17e19312":"code","a71ce64b":"code","cc75e248":"code","31948a9b":"code","469bf980":"code","4462f5ff":"code","37db725c":"code","a2012188":"code","b5a8c7b3":"code","f2c5127c":"code","a1bd2f7f":"code","6d78d7af":"code","f121e29d":"code","a3ad91b3":"code","a31c0ef5":"code","279bcfc9":"code","f87b888c":"code","bb04e653":"code","db42e8fe":"code","6f1434fc":"code","da25b6c9":"code","554c2981":"code","18c61c37":"code","ec33aaac":"code","13fbcb75":"code","8c158a54":"code","c5a5fd2e":"code","14c72f6d":"code","83a60dfc":"code","abfad735":"code","d7befb06":"code","6525f50e":"code","c800ce43":"code","1d2caf6a":"code","188b0d4f":"code","fdc0da0c":"code","790f9a61":"code","18766b1d":"markdown","0047eed9":"markdown","3ad1a5c2":"markdown","f5170c08":"markdown","abffa596":"markdown","fadd4560":"markdown","589addf6":"markdown","2ea552c8":"markdown","d45b1930":"markdown","0e854752":"markdown","e1b7edd6":"markdown","dadc4118":"markdown","044ac8c5":"markdown","d6ad1d3c":"markdown","33e5578f":"markdown","b8444c94":"markdown","543fa349":"markdown","4f9956bb":"markdown","782ae480":"markdown","7f926c2e":"markdown","eec16f92":"markdown","70a3b977":"markdown","18d153aa":"markdown","baf56015":"markdown","a22ff3d6":"markdown","9315eb17":"markdown","fab74fd4":"markdown","d8b2f881":"markdown","862c6f9a":"markdown","c06c6574":"markdown","667af2ce":"markdown","771aa0ed":"markdown","a8607f89":"markdown","e599603d":"markdown","0a155391":"markdown","f0a25f60":"markdown","07bed599":"markdown","2f2e365d":"markdown","732a2b1f":"markdown","8fa98085":"markdown","c0b12d42":"markdown","2c89cfbb":"markdown","86af6e63":"markdown","a82d2a7a":"markdown","3fe69ea7":"markdown","60aaba7a":"markdown","27aa1ed7":"markdown","1313c054":"markdown","7d832b22":"markdown","250bb485":"markdown","3d4a01ef":"markdown","2d7ac5cf":"markdown","80677b5e":"markdown","0ecb594a":"markdown","bb93fb38":"markdown","4cc58009":"markdown","b68b63bb":"markdown","365d1245":"markdown","302f618c":"markdown","481a0cc3":"markdown","379952f4":"markdown","aaddafb9":"markdown","f960feb3":"markdown","d7010c04":"markdown","1bc3db2e":"markdown","2060fa01":"markdown","877cd9f9":"markdown","b2a31ae4":"markdown","ff117b6f":"markdown","6e68bfe8":"markdown","f1e0f569":"markdown","e9f050fe":"markdown","dbac93e5":"markdown","cde91341":"markdown","071e9a94":"markdown","4fe7dbe7":"markdown","f2b544fe":"markdown","0ba1271b":"markdown","9c39cc2f":"markdown","dc9f0dda":"markdown","761a09a8":"markdown","a26f56ac":"markdown","d4b77059":"markdown","6ca28f10":"markdown","9395f082":"markdown","e5481f0b":"markdown","6c541dbb":"markdown","c6f78ad3":"markdown","3a675767":"markdown"},"source":{"2454b867":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","995eef30":"import csv, json\nimport pandas as pd\nimport numpy as np\ntrain_file = {}\nwith open('..\/input\/house-prices-advanced-regression-techniques\/train.csv') as f:\n  csvReader = csv.DictReader(f)\n  for rows in csvReader:\n    id = rows['Id']\n    train_file[id] = rows\n\ntest_file = {}\nwith open('..\/input\/house-prices-advanced-regression-techniques\/test.csv') as f:\n  csvReader = csv.DictReader(f)\n  for rows in csvReader:\n    id = rows['Id']\n    test_file[id] = rows","a4766e76":"with open('train.json', 'w') as f:\n  f.write(json.dumps(train_file, indent=4))\nwith open('test.json', 'w') as f:\n  f.write(json.dumps(test_file, indent=4))","dac5cbc8":"with open(\"train.json\", \"rb\") as f:\n    train_file = json.load(f)\nwith open('train.json', 'r') as f:\n  for i in range(1*84):\n    print(i, \"\\t\", repr(f.readline()))","0fabdb15":"with open(\"test.json\", \"rb\") as f:\n    test_file = json.load(f)\nwith open('test.json', 'r') as f:\n  for i in range(1*83):\n    print(i, \"\\t\", repr(f.readline()))","3ddd7a32":"type(train_file)","94d97159":"type(test_file)","2ee98d8b":"train_file.keys()","6d9ef75f":"test_file.keys()","58958774":"train_file['1'].keys()","6bba3747":"test_file['1461'].keys()","b2db8f75":"print(\"The training dataset is\", os.path.getsize('..\/input\/house-prices-advanced-regression-techniques\/train.csv') \/ 1e6, \"MB\")","606f801e":"print(\"The test dataset is\", os.path.getsize('..\/input\/house-prices-advanced-regression-techniques\/test.csv') \/ 1e6, \"MB\")","6801fb57":"with open('..\/input\/house-prices-advanced-regression-techniques\/train.csv') as f:\n    print(f)","deeeeb89":"train_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv', na_filter=True, encoding='UTF-8')\nnumeric_cols = train_data.select_dtypes(include=[np.number]).columns.tolist()\nfor col in numeric_cols:\n  train_data[col].fillna(0, inplace=True)\n#cat_cols = train_data.columns.tolist()\n#for col in numeric_cols:\n#  cat_cols.remove(col)\n#for col in cat_cols:\n#    train_data[col].fillna('None', inplace=True)\ntrain_data.head()","816e66dd":"test_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv', na_filter=True, encoding='UTF-8')\nnumeric_cols = test_data.select_dtypes(include=[np.number]).columns.tolist()\nfor col in numeric_cols:\n  test_data[col].fillna(0, inplace=True)\n#cat_cols = test_data.columns.tolist()\n#for col in numeric_cols:\n#  cat_cols.remove(col)\n#for col in cat_cols:\n#    test_data[col].fillna('None', inplace=True)\ntest_data.head()","ba1fab6f":"train_data['SalePrice'].describe()","b6d2251d":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm, skew, spearmanr\nsns.distplot(train_data['SalePrice'])","989d901e":"print(\"Skewness of train data: %f\" % train_data['SalePrice'].skew())\nsns.distplot(np.log1p(train_data['SalePrice']), fit=norm)","d8d17321":"train_data.corr()","5aef86f6":"f, ax = plt.subplots(figsize=(12, 9))\nmask = np.triu(np.ones_like(train_data.corr(), dtype=bool))\nmap = sns.heatmap(train_data.corr(), vmin=-1, vmax=1, center=0, mask=mask, cmap=sns.dark_palette((260, 75, 60), input=\"husl\"), square=True)\nmap.set_xticklabels(map.get_xticklabels(), rotation=45, horizontalalignment='right')","6345db91":"corr = train_data.corr()\nhighest_correlations = corr.index[abs(corr['SalePrice']) > 0.5]\nmask = np.triu(np.ones_like(train_data[highest_correlations].corr(), dtype=bool))\nplt.figure(figsize=(10, 10))\ng = sns.heatmap(train_data[highest_correlations].corr(), annot=True, mask=mask, cmap=\"RdYlGn\")","e8834b28":"corr['SalePrice'].sort_values(ascending=False)","2792950d":"cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(train_data[cols])","ea45a4f9":"train_data['YearBuilt'].describe()","4a725884":"train_data['YearRemodAdd'].describe()","656046e4":"features = ['MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'MasVnrArea','BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', \n            'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', \n            'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal']\ndef detect_outliers():\n  outliers = []\n  for feature in features:\n    subset = []\n    index = 0\n    threshold = 3\n    array = np.array([x for x in train_data[feature].tolist() if isinstance(x, int)])\n    mean = np.mean(array)\n    std = np.std(array)\n\n    for x in array:\n      z_score = (x - mean)\/std\n      feature_pair = {'Feature': feature, 'Home_ID': index, 'Value': x}\n      if np.abs(z_score) > threshold:\n        subset.append(feature_pair)\n      index = index+1\n    outliers.append(subset)\n  return outliers\n\noutliers = detect_outliers()\nfor subset in outliers:\n  index = 0\n  if len(subset) > 0: \n    print(\"Feature: \" + str(subset[index]['Feature']) + \", Proportion of outliers: \" + str(len(subset)\/1460 * 100) + \"%\")\n  index = index+1","9c96a107":"fig, axes = plt.subplots(ncols=5, nrows=2, figsize=(16, 4))\naxes = np.ravel(axes)\n#col_name = ['GrLivArea','TotalBsmtSF','1stFlrSF','BsmtFinSF1','LotArea']\ncol_name = ['GrLivArea', 'TotalBsmtSF', '1stFlrSF', 'BsmtFinSF1', 'LotArea']\nfor i, c in zip(range(5), col_name):\n    train_data.plot.scatter(ax=axes[i], x=c, y='SalePrice', sharey=True, colorbar=False, c='r')\n\n# delete outliers\nprint(train_data.shape)\ntrain = train_data[train_data['GrLivArea'] < 4500]\ntrain = train_data[train_data['LotArea'] < 100000]\ntrain = train_data[train_data['TotalBsmtSF'] < 3000]\ntrain = train_data[train_data['1stFlrSF'] < 2500]\ntrain = train_data[train_data['BsmtFinSF1'] < 2000]\n\nprint(train.shape)\n\nfor i, c in zip(range(5,10), col_name):\n    train.plot.scatter(ax=axes[i], x=c, y='SalePrice', sharey=True, colorbar=False, c='b')","8e7843b1":"train_data.drop('Id', inplace=True, axis=1)\ntrain_data.head()","51c5af5f":"test_ID = test_data['Id']\ntest_data.drop('Id', inplace=True, axis=1)\ntest_data.head()","0900098b":"corr = train_data.corr()\nhighest_correlations = corr.index[abs(corr['SalePrice']) > 0.5]\ntrain_data[highest_correlations]","b2e9d313":"quality_pivot = train.pivot_table(index='OverallQual',\n                                  values='SalePrice', aggfunc=np.median)\nquality_pivot.plot(kind='bar', color='blue')\nplt.xlabel('Overall Quality')\nplt.ylabel('Median Sale Price')\nplt.xticks(rotation=0)\nplt.show()","3206c656":"# Histogram and normal probability plot\nfrom scipy import stats\nsns.distplot(train_data['SalePrice'], fit = norm)\nfig = plt.figure()\nres = stats.probplot(train_data['SalePrice'],plot = plt)","ad63b56d":"categoricals = train.select_dtypes(exclude=[np.number])\ncategoricals.describe()","76c0e9a0":"n = categoricals\nfor c in n.columns:\n    print('{:<14}'.format(c), train[c].unique())","17e19312":"from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\ncols = ('MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities',\n       'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2',\n       'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st',\n       'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation',\n       'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n       'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual',\n       'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual',\n       'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature',\n       'SaleType', 'SaleCondition')\n\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    if train_data[c].notnull().all():\n      lbl = LabelEncoder() \n      lbl.fit(list(train_data[c].values)) \n      train_data[c] = lbl.transform(list(train_data[c].values))\n      lbl.fit(list(test_data[c].values)) \n      test_data[c] = lbl.transform(list(test_data[c].values))","a71ce64b":"train_data","cc75e248":"test_data","31948a9b":"features # Numeric features","469bf980":"def detect_and_remove_outliers(inline_delete= True):\n    global train_data\n    outliers = []\n    cnt = 0\n    min_percentile = 0.001\n    max_percentile = 0.999\n    rows = int(np.ceil(len(features)\/2))\n    for row in range (0, rows):\n      for col in range (0, 2):\n          # df_outliers = outlier_detection_using_percentile(features[cnt])\n          # Outlier detection using percentile\n          min_thresold, max_thresold = train_data[features[cnt]].quantile([min_percentile, max_percentile])\n          df_outliers = train_data[(train_data[features[cnt]] < min_thresold) | (train_data[features[cnt]] > max_thresold)]\n\n          # Updaing list of outliers\n          outliers = outliers + df_outliers.index.tolist()\n\n          if inline_delete: \n              # Drop the outliers inline\n              train_data = train_data.drop(df_outliers.index.tolist())\n              train_data.reset_index(drop = True, inplace = True)\n          cnt = cnt + 1\n          if cnt >= len(features):\n              break\n    unique_outliers= list(set(outliers))\n    \n    if inline_delete == False: \n        # Drop the unique outliers from final list\n        train_data = train_data.drop(unique_outliers)\n        train_data.reset_index(drop = True, inplace = True)\n              \ndetect_and_remove_outliers(inline_delete= False)\ntrain_data","4462f5ff":"total = train_data.isnull().sum().sort_values(ascending=False)\npercent = (train_data.isnull().sum()\/train.isnull().count()).sort_values(ascending=False)\nnull_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nnull_data.head(20)","37db725c":"train_data = train_data.dropna(thresh=0.7*len(train_data), axis=1)\ntest_data = test_data.dropna(thresh=0.7*len(test_data), axis=1)\n\ntrain_data = train_data.fillna(train_data.mean())\ntest_data = test_data.fillna(test_data.mean())","a2012188":"train_data.head()","b5a8c7b3":"test_data.head()","f2c5127c":"remaining = [c for c in train_data.columns if train_data[c].isnull().any()]\nfor c in remaining:\n      lbl = LabelEncoder() \n      lbl.fit(list(train_data[c].values)) \n      train_data[c] = lbl.transform(list(train_data[c].values))\n      lbl.fit(list(test_data[c].values)) \n      test_data[c] = lbl.transform(list(test_data[c].values))","a1bd2f7f":"train_data.head()","6d78d7af":"test_data.head()","f121e29d":"from statsmodels.stats.outliers_influence import variance_inflation_factor \nfrom statsmodels.tools.tools import add_constant\n\ndef get_highest_vif_feature(df, thresh=5):\n    '''\n      Calculates VIF each feature in a pandas dataframe\n    A constant must be added to variance_inflation_factor or the results will be incorrect\n\n    :param df: the pandas dataframe containing only the predictor features, not the response variable\n    :param thresh: the max VIF value before the feature is removed from the dataframe\n    :return: dataframe with features removed\n    '''\n   \n    const = add_constant(df)\n    print(f'Shape of data after adding const column: {const.shape}')\n    cols = const.columns\n    \n    # Calculating VIF for each feature\n    vif_df = pd.Series([ (variance_inflation_factor(const.values, i)) for i in range(const.shape[1]) ], index= const.columns).to_frame()\n    \n    vif_df = vif_df.sort_values(by=0, ascending=False).rename(columns={0: 'VIF'})\n    vif_df = vif_df.drop('const')\n    vif_df = vif_df[vif_df['VIF'] > thresh]\n\n    if vif_df.empty:\n        print('DataFrame is empty!')\n        return None\n    else:\n        print(f'\\nFeatures above VIF threshold: {vif_df.to_dict()}')       \n        # Feature with highest VIF value\n        return list(vif_df.index)[0]\n        print(f'Lets delete the feature with highest VIF value: {list(vif_df.index)[0]}')\n\n\n# Selecting only numeric features\nprint(f'Shape of input data: {train_data.shape}')\nnumeric_feats = train_data.dtypes[train_data.dtypes != \"object\"].index\nprint(f\"Calculating VIF for {len(numeric_feats)} numerical features\")\n\ndf_numeric = train_data[numeric_feats]\nprint(f'Shape of df_numeric: {df_numeric.shape}')\n    \nfeature_to_drop = None\nfeature_to_drop_list = []\nwhile True:\n    feature_to_drop = get_highest_vif_feature(df_numeric, thresh=5)\n    print(f'feature_to_drop: {feature_to_drop}')\n    if feature_to_drop is None:\n        print('No more features to drop!')\n        break\n    else:\n        feature_to_drop_list.append(feature_to_drop)\n        df_numeric = df_numeric.drop(feature_to_drop, axis=1)\n        print(f'Feature {feature_to_drop} droped from df_numeric')\n\nprint(f'\\nfeature_to_drop_list: {feature_to_drop_list}')","a3ad91b3":"print(f'Shape of training data= {train_data.shape}')\ntrain_data = train_data.drop(['LowQualFinSF'], axis= 1) # Default drop axis is 0 i.e. rows \ntrain_data.reset_index(drop = True, inplace = True)\nprint(f'Shape of training data= {train_data.shape}')","a31c0ef5":"#Lets check the count of numerical and categorical features\ncat_feats = train_data.dtypes[train_data.dtypes == \"object\"].index\nnumeric_feats = train_data.dtypes[train_data.dtypes != \"object\"].index\nprint(f\"Number of categorical features: {len(cat_feats)}, Numerical features: {len(numeric_feats)}\")\n\nskew_features = train_data[numeric_feats].apply(lambda x: stats.skew(x)).sort_values(ascending=False)\nskewness = pd.DataFrame({'Skew': skew_features})\n\nprint(f'Skew in numerical features. Shape of skewness: {skewness.shape}')\nskewness.head(10)\n\n# todo add histo and probability plot of skewed features","279bcfc9":"train_data['TotalSF'] = train_data['TotalBsmtSF'] + train_data['1stFlrSF'] + train_data['2ndFlrSF']\ntrain_data['TotalSF1'] = train_data['BsmtFinSF1'] + train_data['BsmtFinSF2'] + train_data['1stFlrSF'] +train_data['2ndFlrSF']\n\ntrain_data['YrBltAndRemod']= train_data['YearBuilt'] + train_data['YearRemodAdd']\n\ntrain_data['TotalBathrooms'] = (train_data['FullBath'] + (0.5 * train_data['HalfBath']) +\n                               train_data['BsmtFullBath'] + (0.5 * train_data['BsmtHalfBath']))\n\ntrain_data['TotalPorchSF'] = (train_data['OpenPorchSF'] + train_data['3SsnPorch'] +\n                              train_data['EnclosedPorch'] +train_data['ScreenPorch'] +\n                              train_data['WoodDeckSF'])\n\nprint(f'Shape train_data: {train_data.shape}')","f87b888c":"test_data['TotalSF'] = test_data['TotalBsmtSF'] + test_data['1stFlrSF'] + test_data['2ndFlrSF']\ntest_data['TotalSF1'] = test_data['BsmtFinSF1'] +test_data['BsmtFinSF2'] + test_data['1stFlrSF'] +test_data['2ndFlrSF']\n\ntest_data['YrBltAndRemod']= test_data['YearBuilt'] + test_data['YearRemodAdd']\n\ntest_data['TotalBathrooms'] = (test_data['FullBath'] + (0.5 * test_data['HalfBath']) +\n                              test_data['BsmtFullBath'] + (0.5 * test_data['BsmtHalfBath']))\n\ntest_data['TotalPorchSF'] = (test_data['OpenPorchSF'] + test_data['3SsnPorch'] +\n                             test_data['EnclosedPorch'] +test_data['ScreenPorch'] +\n                            test_data['WoodDeckSF'])\n\nprint(f'Shape test_data: {test_data.shape}')","bb04e653":"from sklearn.model_selection import train_test_split\nY_train = train_data['SalePrice']\ntrain_data = train_data.drop(['SalePrice'], axis=1)\nx_train, x_test, y_train, y_test = train_test_split(train_data, Y_train, test_size=0.2, random_state=42)","db42e8fe":"x_train","6f1434fc":"x_test","da25b6c9":"y_train","554c2981":"y_test","18c61c37":"from sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler","ec33aaac":"scaler = RobustScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.fit_transform(x_test)\ntest_data = test_data.drop(['LowQualFinSF'], axis=1)\ntest = scaler.fit_transform(test_data)","13fbcb75":"x_train","8c158a54":"x_test","c5a5fd2e":"from sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import make_scorer, r2_score\nfrom sklearn.compose import TransformedTargetRegressor\nimport statsmodels.api as sm\n\n# define models to test:\nbase_models = [(\"LinearRegression\",      LinearRegression()),                     #LinearRegression\n               (\"Poly\",      PolynomialFeatures(degree=2, include_bias=False)),]","14c72f6d":"models_data = {'intercept':{}, 'r_sq':{}}","83a60dfc":"model_name = 'LinearRegression'\nmodel = TransformedTargetRegressor(\n        regressor=LinearRegression(), \n        func=np.log, inverse_func=np.exp)\n\nmodel = LinearRegression()\nmodel.fit(x_train, y_train)\ny_train_pred = model.predict(x_train)\nr_sq = r2_score(y_train, y_train_pred)\n\n#models_data['coefficients'][model_name] = model.coef_\n#models_data['intercept'][model_name] = model.intercept_\n#models_data['r_sq'][model_name] = r_sq\nprint(\"Train Accuracy of Linear Regression:\", r_sq * 100, \"%\")\n\ny_test_pred = model.predict(x_test)\npred_linear = y_test_pred\nr_sq = r2_score(y_test, y_test_pred)\nprint(\"Test Accuracy of Linear Regression:\", r_sq * 100, \"%\")","abfad735":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import GridSearchCV, ShuffleSplit\n\nregressor = DecisionTreeRegressor()\nparams = {'max_depth': [1,2,4,6,8,10,12,14,16,18], 'max_features': [10, 20, 30, 40, 50, 60, 70, 75], 'splitter': ['best', 'random'], 'criterion': ['mse', 'friedman_mse', 'mae']}\n\ncv_sets = ShuffleSplit(n_splits = 10, test_size = 0.2, random_state = 0)\n\ndef func_metric(y, theta):\n  return r2_score(y, theta)\n\n# Estimate the parameter(s) with Grid Search and obtain the best model\nscoring_fnc = make_scorer(func_metric)\ngrid = GridSearchCV(estimator=regressor, param_grid=params, scoring=scoring_fnc, cv=cv_sets)\ngrid = grid.fit(x_train, y_train)\nbest_model = grid.best_estimator_","d7befb06":"best_model =  TransformedTargetRegressor(\n              regressor=best_model, \n              func=np.log, inverse_func=np.exp)\nbest_model","6525f50e":"best_model.fit(x_train, y_train)\ny_train_pred = best_model.predict(x_train)\nscore = r2_score(y_train, y_train_pred)\nprint(\"Train Accuracy of Decision Tree Regressor: \" + str(score * 100) + \"%\")\n\ny_test_pred = best_model.predict(x_test)\npred_decision = y_test_pred\nscore = r2_score(y_test, y_test_pred)\nprint(\"Test Accuracy of Decision Tree Regressor: \" + str(score * 100) + \"%\")","c800ce43":"submission = pd.DataFrame()\nsubmission['Id'] = test_ID\nsubmission['SalePrice'] = model.predict(test)\nsubmission.to_csv('submission1.csv', index=False)\nsubmission","1d2caf6a":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error, r2_score\n\ndef compute_CV_scores(modelA, modelB, X_train, Y_train):\n    '''\n    Split the training data into 5 subsets.\n    For each subset, \n        fit models holding out that subset\n        compute the MSE on that subset (the validation set)\n    You should be fitting 5 models total.\n    Return MSEs and average MSE of modelA and modelB\n\n    Args:\n        modelA and modelB: sklearn models with fit and predict functions \n        X_train (data_frame): Data\n        Y_train (data_frame): Label \n\n    Return:\n        MSE vector containing 5 errors for modelA\n        MSE vector containing 5 errors for modelB\n        the average MSE for the 5 splits of modelA\n        the average MSE for the 5 splits of modelB\n    '''\n    kf = KFold(n_splits=5)\n    validation_accuracies_A = []\n    validation_accuracies_B = []\n    for train_idx, valid_idx in kf.split(X_train):\n        # split the data\n        split_X_train, split_X_valid = X_train[train_idx], X_train[valid_idx]\n        split_Y_train, split_Y_valid = Y_train[train_idx], Y_train[valid_idx]\n\n        # Fit the modelA on the training split\n        modelA.fit(X_train[train_idx], Y_train[train_idx])\n        \n        # Compute the RMSE on the validation split\n        Y_valid_pred = modelA.predict(X_train[valid_idx])\n        accuracyA = r2_score(Y_train[valid_idx], Y_valid_pred)\n        validation_accuracies_A.append(accuracyA)\n\n        # Fit the modelB on the training split\n        modelB.fit(X_train[train_idx], Y_train[train_idx])\n        \n        # Compute the RMSE on the validation split\n        Y_valid_pred = modelB.predict(X_train[valid_idx])\n        accuracyB = r2_score(Y_train[valid_idx], Y_valid_pred)\n\n        validation_accuracies_B.append(accuracyB)\n        \n    return validation_accuracies_A, np.mean(validation_accuracies_A), validation_accuracies_B, np.mean(validation_accuracies_B)","188b0d4f":"X_train = scaler.fit_transform(train_data)\nlinear_accuracies, linear_accuracy, decision_accuracies, decision_accuracy = compute_CV_scores(model, best_model, X_train, Y_train)\nprint(\"Linear Regression: \" + str(linear_accuracy * 100) + \"% \" \"Decision Tree Regressor: \" + str(decision_accuracy*100) + \"%\")","fdc0da0c":"from scipy.stats import ttest_ind\nttest_ind(linear_accuracies, decision_accuracies, equal_var=False)","790f9a61":"submission = pd.DataFrame()\nsubmission['Id'] = test_ID\nsubmission['SalePrice'] = model.predict(test)\nsubmission.to_csv('submission_final.csv', index=False)\nsubmission","18766b1d":"We use this method to compare the two models' performance metrics to eachother to identify the model that should be used to predict the target values.","0047eed9":"**Structure**\n\nThe structure of data we used is the rectangular data.","3ad1a5c2":"## Requirements","f5170c08":"We first observe the faithfulness of the dataset by filtering out all of the outliers within the dataset. Focusing on outliers, defined by Gladwell as people who do not fit into our normal understanding of achievement. Outliers deals with exceptional people, especially those who are smart, rich, and successful, and those who operate at the extreme outer edge of what is statistically plausible. An outlier is a data point that is distant from other similar points. They may be due to variability in the measurement or may indicate experimental errors. If possible, outliers should be excluded from the data set. We'll do a quick analysis through the standard deviation of 'SalePrice' and a set of scatter plots.","abffa596":"#### 1. Split the Data\n\nWe will split the train data into a train subset and test subset, so we can evaluate and test our models.","fadd4560":"### *2. Inference*\n\nWe will compare our models i.e LinearRegression and DecisionTreeRegressor. First, we will split the train dataset into numerous consecutive folds using KFold method from the sklearn.model_selection API.","589addf6":"### *2. Data Wrangling*","2ea552c8":"#### Numeric Feature Scaling","d45b1930":"###### **Temporality:**\n\nEach home was sold between 2006 to 2010. The earliest home was built in 1872 with the newest home being built in 2010. The earliest rennovation was made in 1950 with the latest being 2010. As such, some homes within the dataset represent the value of a modern home within Ames, Iowa whereas some do not as some homes were built early and were either not rennovated, but rennovated many years ago. Naturally homes that were built\/rennovated very recently would be homes we would naturally consider above other homes.","0e854752":"**References:**\n\nhttps:\/\/www.kaggle.com\/satishgunjal\/advanced-reg-techniques-linear-models-top-6\nhttps:\/\/www.kaggle.com\/nikkisharma536\/house-prediction-dealing-with-outlier\nhttps:\/\/machinelearningmastery.com\/standardscaler-and-minmaxscaler-transforms-in-python\/\nhttps:\/\/towardsdatascience.com\/machine-learning-project-predicting-boston-house-prices-with-regression-b4e47493633d","e1b7edd6":"We analyze the distribution in the sale price of all homes in the train dataset.","dadc4118":"##### *A. Exploring the data*","044ac8c5":"##### **Model Evaluation**","d6ad1d3c":"## 1.1 Stage-1","33e5578f":"# Project 2-Stage 1\n\nProject 2 was selected which deals with analyzing house prices using advanced regression techniques. We will be using techniques learned from Assignments 1 and 2 such as working with pandas DataFrame API in order to complete stage 1 of the project.\n\n**Objective:** Transform the data into something that can be thoroughly analyzed and extract relevant results from i.e Data Wrangling.","b8444c94":"We import the train dataset from train.csv to create our train dataframe.","543fa349":"### *1.4 Submission*","4f9956bb":"### B. Features \n\nWe will extract necessary features within the train data and eliminate any redundant data. This ensures that our model will perform faster and will only extrapolate the saleprice based on relevant data.","782ae480":"To analyze the data in greater detail, we must visualize the data. We do so by creating a data frame that encapsulates data from the train dataset.","7f926c2e":"**References:** \n\nhttps:\/\/www.kaggle.com\/satishgunjal\/advanced-reg-techniques-linear-models-top-6\nhttps:\/\/www.kaggle.com\/akuei0419\/housepricepredict02\nhttps:\/\/www.kaggle.com\/nikkisharma536\/house-prediction-dealing-with-outlier","eec16f92":"Throughout this project, we understood how to compile our data, analyze our data thoroughly, learned how to transform our data for data preprocessing, and select features that are most relevant for predicting our target values. We learned how to select, evaluate, and compare two models together so that we can select the model that is most suitable for our problem. Our findings conclude that Linear Regression is the superior model compared to Decision Tree Regression by making a statistical inference using student t-test. \n\n","70a3b977":"We want to delete the outliers in the data so our dataset will be more representative of the problem. Outliers aren't appropriate for predicting the saleprice because they fall outside of the norm.","18d153aa":"## 2.2 Problem Formulation","baf56015":"### *1.3 Conclusions and Learnings*","a22ff3d6":"The granularity of the data is observed by analyzing the correlation between every feature in relation to the sale price. This way, we extract the most impactful features for our data visualizations.","9315eb17":"Because the ID attribute does not contain any useful information that can be used to predict the saleprice, we will drop the attribute.","fab74fd4":"The significance level is below 0.05. Thus, we conclude that the average accuracy computed by the Linear Regression Model is significantly better than the average accuracy of our Decision Tree Regressor at a confidence level of 95%.","d8b2f881":"We import the test dataset from test.csv to create our test dataframe.","862c6f9a":"###### **Granularity:**\n\nWe can analyze the granularity of our data by creating data visualizations that illustrate the correlation between specific features in relation to the sale price. This way, we can create data visualizations that are most relevant to the problem.\n\nFirst, we take a look at the statistics of the sales prices of homes because the sale price is the target value we want to predict.","c06c6574":"The train and test files both represent dictionary containing the data of each house encoded in the datasets.","667af2ce":"In order to give every feature the same importance we perform feature scaling. There are many techniques like Min-Max Scalar, Robust Scalar etc. to do feature scaling.\nBefore we can finalize any scaling technique lets check the skewness of our numeric features. Skewness is the measure of degree of asymmetry of a distribution.\n* skewness = 0 : normally distributed.\n* skewness > 0 : more weight in the left tail of the distribution.\n* skewness < 0 : more weight in the right tail of the distribution.","771aa0ed":"###### **Structure:** \n\nThe aforementioned analyses above illustrate the structure of our data. Both the train and test datasets yield a recursive format in which the keys of the top level json object represents the unique id of each home. The next level represents the final level of the recursive structure in which the keys of the next level denote the explanatory features that influence the value of the home.","a8607f89":"First, we analyze the structure of our data. We do this by delving deep into the recursive structure of each file and analyze the keys of the json objects at the top level.\n\n**Note**: This represents the the unique identifier for each home.","e599603d":"###### **Scope:**\n\nThere are 2390 different records with each record in the dataset representing the sale of an individual residential property in Ames, Iowa. There were originally a total of 3970 different records within the given timeframe, but some were filtered to only include residential sales, thus only leaving us with 2390 different records to work with.","0a155391":"And these are the transformed datasets.","f0a25f60":"# Project 2-Stage 2","07bed599":"### C. Modeling","2f2e365d":"### *3. Observations*\n\nThe data we obtained is representative of the problem because we can infer which features are most relevant to analyze and only a small proportion of the data contains outliers. According to our visualizations, we can observe the linear relationship between two different features and how they relate to the target value i.e the sale price of homes that we are trying to predict.\n\nTo accurately predict the value of a home using this dataset, we would assume that the variables that influence the final sale price remain true across different regions. If that assumption holds true, we can extrapolate the data and results collected and apply it to different datasets involving sales of residential property.","732a2b1f":"Transforming and engineering features Here we are using label encoding. Label encoding refers to transforming the word labels into numerical form so that the algorithms can understand how to operate on them.","8fa98085":"## 2.3 Data ","c0b12d42":"We will remove the columns that have more than 70% of missing data.","2c89cfbb":"## 1.2 Data Transformation and EDA","86af6e63":"Now we need to select a model that will best evaluate our data. Two of the models we have selected is LinearRegression and Decision Tree Regressor.\n\n**Linear Regression:** Linear Regression can serve as an effective model because it models the relationship between two variables by assuming there is a strong linear relationship between the two variables. In the case of SalePrice, we deduced earlier that there were many features in our that had a strong linear relationship with SalePrice. As such, we believe that Linear Regression can be very useful for modeling this problem.\n\n**Decision Tree Regressor:** Decision Tree Regressor was also considered for this problem because it evaluates the data by asking a series of questions related to the features within the dataset to predict the target value by splitting the data into smaller subsets. Because there are numerous features of a home and we have eliminated the outliers within the data, we believe that this model can perform accurate decisions that can predict the SalePrice of our data.","a82d2a7a":"First, we want to read and interpret the file to analyze the file encoding.\n\n**Note:** Not understanding the encoding of the file will perturb some of the column's values and cause it to return NaN instead. As such, pandas.read_csv takes in the parameter, encoding, to ensure that file encoding is taken into account when retrieving the data and return the correct results.","3fe69ea7":"**Data Types** \n\nThe data is comprised of a combination of null, categorical, and numerical data types. The null and categorical data types had to be filtered out or transformed when processing the data through our models.\n\n**Representation**\n\nMaking a slight correction, the data sample ranges from 2006 to 2010 and that made our data quite limited in scope and not necessarily representative of the data population.","60aaba7a":"#### 2. Data Normalization","27aa1ed7":"Before we even begin to load the data, it often helps to analyze the high-level structure:\n\n*How much data do I have?*\n\n*How is it formatted?*","1313c054":"The contents of the json files are shown below:","7d832b22":"And then we can visualize some of the outliers.","250bb485":"### *Introduction*\nHousing prices reflect important information of the state of our economy. When housing prices rise, it corresponds to an increase in spending and borrowing from both individuals and businesses which will help improve the economy. Forming an accurate predicion of the value of a house is crucial when, for example, rennovating a home can significantly improve the value of the home relative to the cost of said reconnvation. This corresponds to a lucrative investment for the homeowner which leads to more spending.\n\nAs such, the problem we seek to address is one that deals with predicting the value of a house. We intend to use advanced regression techniques combined with extensive feature engineering to address this problem. By devising techniques that lead to accurate predictions, we can identify what qualities drastically impact the value of a home.","3d4a01ef":"The train and test files are then converted to a json file such that the data can be thoroughly analyzed in the *Exploring the data* section","2d7ac5cf":"We will check for non-numeric features to encode the data.","80677b5e":"**Linear Regression:** This fits a linear model with specific coefficients to minimize the residual sum of squares between the estimated targets and the observed targets within the dataset. This is called Ordinary Least Squares. It is a statistical technique used to measure the variance within the dataset.\n\n**Decision Tree Regressor:** This model normally uses mean squared error whenever it decides to split a node into two or more child nodes. This means that a node is evaluated as the average of the squared difference between the estimated values and the observed values. However, as we observe later on, the model in question sometimes uses friedman mean squared error which measures the impurity of a node before it is removed or mean absolute error.","0ecb594a":"### *A. Transformations*\n\nWe must prepare and transform the data so that it can be trained using a regression model. Not only would this eliminate redundant data, but it will also transform the data into a set that only contains numerical data. Otherwise, it cannot be processed by the model.","bb93fb38":"We will encode the remaining columns that still retain missing data.","4cc58009":"Every property in the dataset could be thoroughly observed using its correlation matrix. The data was also imported properly to ensure that no data is missing. \n\n**Note:** However as we observed, there is a very small proportion of outliers in the dataset when observing features with numerical data. Thus, there is only a very small proportion of data that is unreliable.","b68b63bb":"### *Data Population*\n\nThe dataset provided by the Kaggle competition via the data_description.txt file contains a list of homes found in Ames, Iowes which was created by Dean De Cock. The dataset is comprised of 2390 different sales of residential properties from 2006 to 2010. The sale price of the home is the target value which is influenced by 79 explanatory features. Each record denotes the details of a particular house involved in a sale of residential property. ","365d1245":"After extracting the most relevant features, we can plot all of the pairs of features that have the highest correlation between eachother.","302f618c":"**Collaborators**: Nazim Zerrouki and Shrustishree Sumanth","481a0cc3":"**Accounts:**  nazimz@uw.edu and ssumanth@uw.edu","379952f4":"# Project 2- Final\n","aaddafb9":"\n\nNow that we have obtained the data we want to understand its:\n\n**Structure** -- the \"shape\" of a data file\n\n**Granularity** -- how fine\/coarse is each datum\n\n**Scope** -- how (in)complete is the data\n\n**Temporality** -- how is the data situated in time\n\n**Faithfulness** -- how well does the data capture \"reality\"","f960feb3":"##### **Decision Tree Regressor**\n\nWe implement the Decision Tree Regressor and train it on our train subset. ","d7010c04":"Judging by our model, we can determine that the linear regression model performs better on average considering how the average performance metric evaluated i.e average regression score is greater than it is for the decision tree regressor. However, we need to use a more accurate comparison metric. \n\nWe need to make a statistical inference to conclude which model is better. We will use student t-test to accomplish this.","1bc3db2e":"#### 1. Missing Data\n\nWe first analyze the missing data within our dataset, so that we can filter the redundancy out.","2060fa01":"**Representation**\n\nWe did initially overlook the limitations of the data. Because the data only observes homes from 2006 to 2008 in only one specific area, it is quite limited in scope and is not exactly representative of the Data Population.","877cd9f9":"The graph is skewed, so we must address it by transforming the data using logarithm.","b2a31ae4":"* Since area related features are very important to determine the house price, we will create a new feature by the name 'TotalSF' by adding 'TotalBsmtSF', '1stFlrSF' and '2ndFlrSF'.\n* Similarly we will create one more new feature by name 'TotalSF1' by adding 'BsmtFinSF1', 'BsmtFinSF2', 'TotalBsmtSF', '1stFlrSF' and '2ndFlrSF'. Here 'BsmtFinSF1' and 'BsmtFinSF2' represent finished square feet of all area, that's why we are creating a separate feature using it.\n* Create new feature 'YrBltAndRemod' by adding 'YearBuilt' and 'YearRemodAdd'\n* Create new feature 'TotalBathrooms' by adding all the bathrooms in the house.\n* Create new feature 'TotalPorchSF' by adding all porch area.","ff117b6f":"#### 3. Model Selection\n\nWe need to select models that we believe would be useful in predicting our data.","6e68bfe8":"#### Getting the Data\n\nFirst, we upload the data as done previously in Assignment 2 and access the directory in which the data was uploaded so it can be retrieved and compiled. This was done by mounting our shared google drive and uploading the specific folder containing the train and test datasets to the google drive.","f1e0f569":"### *4. Sampling*\n\nCompared to the data population, we believe that was used was cluster sampling collected through administrative data in order to retrieve the data. The data was derived from an administrative source with some variables stripped away for usability. ","e9f050fe":"We often like to start the analysis by getting a rough estimate of the size of the data. This will help inform the tools we use and how we view the data. If it is relatively small, we might use a text editor or a spreadsheet to look at the data. If it is larger, we might jump to more programmatic exploration or even use distributed computing tools.\n\nHowever here we will use python tools to probe the file.","dbac93e5":"Normalization plays a significant role in data preprocessing. Normalization will transform the data into a matrix of values ranging from 0 to 1 without distorting any values or losing information. Some models can't operate effectively without applying some sort of normalization on the data.\n\nRobustScaler will be applied because it transforms the data by scaling features using statistics relative to outliers.","cde91341":"**Data Population**\n\nThe Kaggle regression problem represents a sample of the Data Population. The Data Population represents all homes throughout the world.","071e9a94":"**Description:** The train and test files contains rows in which each row represents all of the features for each home. Each row is represented as a dictionary so we can conclude that the train and test files are a list of dictionaries. The key for each dictionary is the id of the home and the value represents all of the features that are observed for each home.","4fe7dbe7":"##### *D. Data Visualization*\n","f2b544fe":"##### *Downloading the Data*\n\nWe want to retrieve the data so we can thoroughly analyze the architecture of the data.\n\nEach file is represented as an empty directory and stores each row in the csv file into each directory respectively. ","0ba1271b":"###### **Faithfulness:**\n","9c39cc2f":"#### 2. Multicollinear Features\n\nNext, we will remove multicollinear features i.e features that have the highest correlations between eachother by analyzing the VIF score. \n\n**Note:** The VIF score is a metric that is used to measure the strength of correlation between 2 independent variables.","dc9f0dda":"##### **Loss Function**","761a09a8":"##### C. *How big is the data?*\n","a26f56ac":"#### Adding New Features","d4b77059":"We need the features that have the highest correlation with the sale price.","6ca28f10":"#### JSON","9395f082":"##### **Linear Regression Model**\n\nWe implement the linear regression model and train it on our train subset.","e5481f0b":"Then we do the same for the keys at the next level.","6c541dbb":"### *1. Stage-2*\n","c6f78ad3":"### *1. Compiling the data*","3a675767":"##### *B. Structure*\n"}}