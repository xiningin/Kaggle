{"cell_type":{"f851de44":"code","fa7f45ab":"code","2662bc8c":"code","1c705ad2":"code","c3374467":"code","c3c1a7f7":"code","ca269c02":"code","93b73377":"code","c2c3876e":"code","b09c9843":"code","347e7e3d":"code","fef32e03":"code","0e1c0d5f":"code","0c5121d6":"code","d70b7c5c":"code","9a2377a2":"code","640bed60":"code","4d8338cd":"code","2e934f2b":"code","213e4d5a":"code","b2057685":"code","a2ec80f9":"code","8567a112":"code","4d90620e":"code","e3914512":"code","581d9954":"code","e5465a8f":"code","bf206187":"code","06a2107b":"code","26de9c9d":"code","e69f8496":"code","17556847":"code","66e40d15":"code","199d31fe":"code","d07028f6":"code","bdd42e4d":"code","94f3095a":"code","540704ab":"code","fdbde6a8":"code","d5c56d87":"markdown","6d312543":"markdown","59baee10":"markdown","dcd456f3":"markdown","49e1a09a":"markdown","9e0c949f":"markdown","abe5c64b":"markdown","dff0a1e0":"markdown","ce46f918":"markdown","287a9661":"markdown","e00b1084":"markdown","54651435":"markdown","36802068":"markdown","1e125af0":"markdown","c9e1d39a":"markdown"},"source":{"f851de44":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fa7f45ab":"import matplotlib.pyplot as plt              # Data Visualization\nimport seaborn as sns","2662bc8c":"from sklearn.model_selection import train_test_split     # For Train\/Test Split","1c705ad2":"Dataset = pd.read_csv(\"\/kaggle\/input\/mobile-price-classification\/train.csv\")","c3374467":"Dataset.head()","c3c1a7f7":"Dataset.columns","ca269c02":"Dataset.shape","93b73377":"Dataset.info()                             # Checking Data types","c2c3876e":"Dataset.isnull().sum()                      # Checking Null-values","b09c9843":"Dataset.describe()","347e7e3d":"# Displaying number of samples for each Disease\nfig, ax = plt.subplots(figsize = (10, 4))\nsns.countplot(x ='price_range', data=Dataset)\nplt.xlabel(\"Class Label\")\nplt.ylabel(\"Number of Samples\")\nplt.show()","fef32e03":"# Calculating Correlation between features\ncorrmat = Dataset.corr()                ","0e1c0d5f":"# Visualizing Correlation between every feature\nf, ax = plt.subplots(figsize =(9, 8)) \nsns.heatmap(corrmat, ax = ax, cmap =\"YlGnBu\", linewidths = 0.1) ","0c5121d6":"corrmat['price_range']","d70b7c5c":"corrmat['price_range'] = abs(corrmat['price_range'])          # Converting all values to positives","9a2377a2":"corrmat['price_range']                                        # All positive values","640bed60":"new = corrmat.sort_values(by=['price_range'])                 # Sorting correlation values","4d8338cd":"new['price_range']                                            # Final list","2e934f2b":"# Selecting features with correlation more than 0.022\nfeatures = ['ram', 'battery_power', 'px_width', 'px_height', 'int_memory', 'sc_w', 'pc', 'touch_screen',\n            'mobile_wt', 'three_g', 'sc_h', 'price_range']","213e4d5a":"Dataset = Dataset[features]","b2057685":"Dataset.head()","a2ec80f9":"Target = np.array(Dataset.pop('price_range'))             # Target\nData   = np.array(Dataset)","8567a112":"print (Data)\nprint (Target)\nprint (\"Shape of input Data is: \", Data.shape)\nprint (\"Shape of input Data is: \", Target.shape)","4d90620e":"X_train, X_test, Y_train, Y_test = train_test_split(Data, Target, test_size=0.2, random_state=100)","e3914512":"print (\"Shape of Train Data is:  \", X_train.shape)\nprint (\"Shape of Test  Data is:  \", X_test.shape)\nprint (\"Shape of Train Label is: \", Y_train.shape)\nprint (\"Shape of Test  Label is: \", Y_test.shape)","581d9954":"from sklearn.tree import DecisionTreeClassifier","e5465a8f":"from sklearn.metrics import accuracy_score ","bf206187":"acc = []\nx_axis_DT = range(3,12)\nfor i in range(3,12):\n    DT = DecisionTreeClassifier(criterion = \"gini\",\n                                random_state = 100,\n                                max_depth=i, \n                                min_samples_leaf=5)\n    DT.fit(X_train, Y_train)\n    y_pred = DT.predict(X_test)\n    accuracy = accuracy_score(Y_test,y_pred)*100\n    acc.append(accuracy)\n    print (\"Accuracy for Decision for max depth \",i,\" is: \", accuracy) ","06a2107b":"plt.subplots(figsize = (10, 4))\nplt.plot(x_axis_DT,acc)\nplt.xlabel('Maximum Depth')\nplt.ylabel('Accuracy')\nplt.show()","26de9c9d":"from sklearn.ensemble import RandomForestClassifier","e69f8496":"acc_RF = []\nx_axis_RF = range(5,31)\nfor i in range(5,31):\n    RF = RandomForestClassifier(n_estimators = i, random_state = 0)\n    RF.fit(X_train, Y_train)\n    y_pred = RF.predict(X_test)\n    accuracy = accuracy_score(Y_test,y_pred)*100\n    acc_RF.append(accuracy)\n    print (\"Accuracy for Random Forest for max depth \",i,\" is: \", accuracy) ","17556847":"plt.subplots(figsize = (10, 4))\nplt.plot(x_axis_RF,acc_RF)\nplt.xlabel('Number of Estimators')\nplt.ylabel('Accuracy')\nplt.show()","66e40d15":"from sklearn.neighbors import KNeighborsClassifier ","199d31fe":"acc_KNN = []\nx_axis_KNN = range(5,16)\nfor i in range(5,16):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train, Y_train)\n    y_pred = knn.predict(X_test)\n    accuracy = accuracy_score(Y_test,y_pred)*100\n    acc_KNN.append(accuracy)\n    print (\"Accuracy for KNN for k = \",i,\" is: \", accuracy) ","d07028f6":"plt.subplots(figsize = (10, 4))\nplt.plot(x_axis_KNN,acc_KNN)\nplt.xlabel('Number of Estimators')\nplt.ylabel('Accuracy')\nplt.show()","bdd42e4d":"from sklearn.naive_bayes import MultinomialNB","94f3095a":"NB = MultinomialNB()\nNB.fit(Data, Target)\ny_pred = NB.predict(X_test)\naccuracy = accuracy_score(Y_test,y_pred)*100\nprint (\"Accuracy of Naive Bayes Classifier is: \", accuracy) ","540704ab":"from sklearn.svm import SVC   ","fdbde6a8":"svc = SVC(kernel='linear')\nsvc.fit(X_train, Y_train)\ny_pred = svc.predict(X_test)\naccuracy = accuracy_score(Y_test,y_pred)*100\nprint (\"Accuracy of Naive Bayes Classifier is: \", accuracy) ","d5c56d87":"## Implementing Decision Tree","6d312543":"## Splitting Data and Targets","59baee10":"Choosing k = 13","dcd456f3":"## Analysing the Data","49e1a09a":"## Implementing KNN Classifier","9e0c949f":"## Implementing Naive Bayes","abe5c64b":"## Implementing Random Forest","dff0a1e0":"Choosing maximum depth = 6","ce46f918":"## Importing Libraries","287a9661":"So, we have perfectly balanced Data","e00b1084":"## Creating Training and Testing Dataset","54651435":"## Loading Data","36802068":"## Data Visualization","1e125af0":"Choosing n_estimators = 26","c9e1d39a":"## Implementing Naive bayes"}}