{"cell_type":{"86e225d5":"code","7797a18f":"code","cbbfa24f":"code","98f21347":"code","0efc2993":"code","eb2b0507":"code","d1490e11":"code","9dbdbdcd":"code","196d31f7":"code","638345f2":"code","270cd990":"code","7982dbd9":"code","cba4b8c7":"code","f0da3076":"code","43d08f57":"code","7125048f":"code","bf367877":"code","31606585":"code","1515636a":"code","0ef8e139":"code","b5ec9e29":"code","86dd4a1e":"code","d9fe1b97":"code","82555bb5":"code","2eed71cb":"code","09a1145d":"code","30db6fd1":"code","23af31c5":"code","f278c659":"code","e196c62b":"code","2a468e35":"code","f166a743":"code","06df4f7d":"code","36e9383f":"code","2f6b9c1d":"code","823d7178":"code","4a6d3ad5":"code","62bd99c2":"code","5f2457e1":"code","3d49aa63":"code","e456dbe9":"code","9cd1533a":"code","caa30926":"code","e6382642":"code","b3d0daf0":"code","9874fd9d":"code","2c1740ac":"code","73263ee7":"code","4a6897f9":"code","e0475f9a":"code","beab3c8b":"code","11aec708":"code","e26b48b0":"code","b16093ea":"code","49124940":"code","de300ccf":"code","5ba2fe41":"code","336b1e39":"markdown","f271d908":"markdown","de66e5e1":"markdown","746a79bd":"markdown","fa11f752":"markdown","c671d79f":"markdown","096bfb38":"markdown","5dcbb27b":"markdown","c0a1f62b":"markdown","c1e497ff":"markdown","0d8b9636":"markdown"},"source":{"86e225d5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7797a18f":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","cbbfa24f":"#importing the training dataset\ntrain = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain.head()","98f21347":"#importing the testing dataset\ntest= pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest.head()","0efc2993":"#Let's control that the shape of the datasets are correct\nprint(train.shape)\nprint(test.shape)","eb2b0507":"#as we can see in the second dataset it miss the target variable \"Survived\"\nprint(train.columns)\nprint(test.columns)","d1490e11":"#there are some missing in the data in some variable. Maybe we could impute it\ntrain.isnull().mean()","9dbdbdcd":"train.isnull().sum()","196d31f7":"train[train.Age.isnull()]","638345f2":"#the same about test data\ntest.isnull().mean()","270cd990":"#let's take a look at the type of variables\ntrain.info()","7982dbd9":"train.columns","cba4b8c7":"train.corr()","f0da3076":"# to have an idea about correlation between variables we can take a look at the heatmap graph for correlations.\nsns.heatmap(train.corr())\nplt.show()","43d08f57":"sns.boxplot(x=\"Embarked\", y=\"Pclass\",hue=\"Survived\", data=train)\n   ","7125048f":"train.nunique()","bf367877":"sns.boxplot(x=\"Sex\", y=\"Age\",hue=\"Survived\", data=train)","31606585":"sns.boxplot(x=\"Survived\", y=\"Age\",hue=\"Sex\", data=train)","1515636a":"train.Cabin=train.Cabin.fillna(\"Missing\")#We operate on Cabin variable to deal with missings. \ntest.Cabin=test.Cabin.fillna(\"Missing\")\ntrain.Cabin.head(),test.Cabin.head()","0ef8e139":"def desk (string):\n    prima=string[0]\n    return prima","b5ec9e29":"train[\"deck\"]=train[\"Cabin\"].apply(desk)\ntest[\"deck\"]=test[\"Cabin\"].apply(desk)\n","86dd4a1e":"train[\"Age\"]=train[\"Age\"].fillna(train[\"Age\"].median()) # imputation for the median of Age in test set \nprint(train.isnull().mean())\ntest[\"Age\"]=test[\"Age\"].fillna(test[\"Age\"].median()) # imputation for the median of Age in test set \ntest.isnull().mean()","d9fe1b97":"test[\"Fare\"]=test[\"Fare\"].fillna(test[\"Fare\"].median())","82555bb5":"test.isnull().mean()","2eed71cb":"train['cabin_multiple'] = train.Cabin.apply(lambda x: 0 if x==\"Missing\" else len(x.split(' ')))\n# after looking at this, we may want to look at cabin by letter or by number. Let's create some categories for this \n# letters \n# multiple letters \ntrain['cabin_multiple'].value_counts()","09a1145d":"pd.pivot_table(train, index = 'Survived', columns = 'cabin_multiple', values = 'Ticket' ,aggfunc ='count')","30db6fd1":"train.head()","23af31c5":"train['cabin_adv'] = train.Cabin.apply(lambda x: str(x)[0])","f278c659":"#comparing surivial rate by cabin\nprint(train.cabin_adv.value_counts())\npd.pivot_table(train,index='Survived',columns='cabin_adv', values = 'Name', aggfunc='count')","e196c62b":"#understand ticket values better \n#numeric vs non numeric \ntrain['numeric_ticket'] = train.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\ntrain['ticket_letters'] = train.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.','').replace('\/','').lower() if len(x.split(' ')[:-1]) >0 else 0)","2a468e35":"#feature engineering on person's title \ntrain.Name.head(50)\ntrain['name_title'] = train.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())\n#mr., ms., master. etc\n#feature engineering on person's title \ntest.Name.head(50)\ntest['name_title'] = train.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())\n#mr., ms., master. etc","f166a743":"sns.relplot(x=\"Age\",y=\"Fare\",hue= \"Survived\",data=train)\nplt.show()","06df4f7d":"sns.relplot(x=\"Age\",y=\"Fare\",hue= \"Survived\",row= \"Pclass\",data=train)\nplt.show()","36e9383f":"sns.relplot(x=\"Age\",y=\"Fare\",col= \"Sex\",hue= \"Survived\",data=train)\nplt.show()","2f6b9c1d":"sns.relplot(x=\"Pclass\",y=\"Survived\",hue= \"Sex\",kind = \"line\",data=train)\nplt.show()","823d7178":"#Introducing the new feature\ntrain['decade'] = [age\/\/10*10 for age in train.Age]\n\ntrain.head(3)","4a6d3ad5":"sns.relplot(x=\"decade\",y=\"Survived\",\n            hue= \"Sex\",\n            kind= \"line\",\n            ci=None,\n            data= train[train.Age<60])\n\nplt.show()","62bd99c2":"train=train.drop(['cabin_multiple', 'cabin_adv', 'numeric_ticket', 'ticket_letters'],axis=1)","5f2457e1":"train['train_test'] = 1\ntest['train_test'] = 0\ntest['Survived'] = np.NaN\nall_data = pd.concat([train,test])\n","3d49aa63":"\"\"\"\n#create all categorical variables that we did above for both training and test sets \nall_data['cabin_multiple'] = all_data.Cabin.apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))\nall_data['cabin_adv'] = all_data.Cabin.apply(lambda x: str(x)[0])\nall_data['numeric_ticket'] = all_data.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\nall_data['ticket_letters'] = all_data.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.','').replace('\/','').lower() if len(x.split(' ')[:-1]) >0 else 0)\nall_data['name_title'] = all_data.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())\n\n#impute nulls for continuous data \n#all_data.Age = all_data.Age.fillna(training.Age.mean())\nall_data.Age = all_data.Age.fillna(training.Age.median())\n#all_data.Fare = all_data.Fare.fillna(training.Fare.mean())\nall_data.Fare = all_data.Fare.fillna(training.Fare.median())\n\"\"\"\n#drop null 'embarked' rows. Only 2 instances of this in training and 0 in test \nall_data.dropna(subset=['Embarked'],inplace = True)\n\n#tried log norm of sibsp (not used)\nall_data['norm_sibsp'] = np.log(all_data.SibSp+1)\nall_data['norm_sibsp'].hist()\n\n# log norm of fare (used)\nall_data['norm_fare'] = np.log(all_data.Fare+1)\nall_data['norm_fare'].hist()\n\n# converted fare to category for pd.get_dummies()\nall_data.Pclass = all_data.Pclass.astype(str)\n\n#created dummy variables from categories (also can use OneHotEncoder)\nall_dummies = pd.get_dummies(all_data[['Pclass','Sex','Age','SibSp','Parch','norm_fare','Embarked','train_test']])\n\n#Split to train test again\nX_train = all_dummies[all_dummies.train_test == 1].drop(['train_test'], axis =1)\nX_test = all_dummies[all_dummies.train_test == 0].drop(['train_test'], axis =1)\n\n\ny_train = all_data[all_data.train_test==1].Survived\ny_train.shape","e456dbe9":"# Scale data \nfrom sklearn.preprocessing import StandardScaler\nscale = StandardScaler()\nall_dummies_scaled = all_dummies.copy()\nall_dummies_scaled[['Age','SibSp','Parch','norm_fare']]= scale.fit_transform(all_dummies_scaled[['Age','SibSp','Parch','norm_fare']])\nall_dummies_scaled\n\nX_train_scaled = all_dummies_scaled[all_dummies_scaled.train_test == 1].drop(['train_test'], axis =1)\nX_test_scaled = all_dummies_scaled[all_dummies_scaled.train_test == 0].drop(['train_test'], axis =1)\n\ny_train = all_data[all_data.train_test==1].Survived","9cd1533a":"from sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC","caa30926":"#I usually use Naive Bayes as a baseline for my classification tasks \ngnb = GaussianNB()\ncv = cross_val_score(gnb,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","e6382642":"lr = LogisticRegression(max_iter = 2000)\ncv = cross_val_score(lr,X_train,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","b3d0daf0":"dt = tree.DecisionTreeClassifier(random_state = 1)\ncv = cross_val_score(dt,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","9874fd9d":"knn = KNeighborsClassifier()\ncv = cross_val_score(knn,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","2c1740ac":"rf = RandomForestClassifier(random_state = 1)\ncv = cross_val_score(rf,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","73263ee7":"svc = SVC(probability = True)\ncv = cross_val_score(svc,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","4a6897f9":"from xgboost import XGBClassifier\nxgb = XGBClassifier(random_state =1)\ncv = cross_val_score(xgb,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","e0475f9a":"#Voting classifier takes all of the inputs and averages the results. For a \"hard\" voting classifier each classifier gets 1 vote \"yes\" or \"no\" and the result is just a popular vote. For this, you generally want odd numbers\n#A \"soft\" classifier averages the confidence of each of the models. If a the average confidence is > 50% that it is a 1 it will be counted as such\nfrom sklearn.ensemble import VotingClassifier\nvoting_clf = VotingClassifier(estimators = [('lr',lr),('knn',knn),('rf',rf),('gnb',gnb),('svc',svc),('xgb',xgb)], voting = 'soft') ","beab3c8b":"cv = cross_val_score(voting_clf,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","11aec708":"cv","e26b48b0":"voting_clf.fit(X_train_scaled,y_train)\ny_hat_base_vc = voting_clf.predict(X_test_scaled).astype(int)\nbasic_submission = {'PassengerId': test.PassengerId, 'Survived': y_hat_base_vc}\nbase_submission = pd.DataFrame(data=basic_submission)\nbase_submission.to_csv('base_submission.csv', index=False)","b16093ea":"\n\n#output = pd.DataFrame({'PassengerId': test.PassengerId, 'Survived': predictions})\n#output.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n","49124940":"train.isnull().mean()","de300ccf":"train[\"Age\"]=train[\"Age\"].fillna(train[\"Age\"].median()) # imputation for the median of Age in test set \nprint(train.isnull().mean())\ntest[\"Age\"]=test[\"Age\"].fillna(test[\"Age\"].median()) # imputation for the median of Age in test set \ntest.isnull().mean()","5ba2fe41":"from sklearn.ensemble import RandomForestClassifier\n\ny = train[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"Embarked\",\"SibSp\", \"Fare\",\"Parch\"]\nX = pd.get_dummies(train[features])\nX_test = pd.get_dummies(test[features])\nX_test=X_test.fillna(X_test.Fare.median())    \n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\npredictions = model.predict(X_test)","336b1e39":"If you want to know more about the relplot and the other functionalities you can watch this video of Kimberly Fessel:\n\nhttps:\/\/www.youtube.com\/watch?v=oinUUsTX7dc","f271d908":"Building the models\n===","de66e5e1":"Scaling\n===","746a79bd":"<center style=\"font-family:verdana;\"><h1 style=\"font-size:200%; padding: 10px; background: #00BFFF;\"><b style=\"color:#0000CD;\">Thank you for Watching<\/b><\/h1><\/center>","fa11f752":"Pre-processing of data\n=========\n\nWhat we want to do:\n\n1) Drop null values from Embarked (only 2)\n\n2) Include only relevant variables (Since we have limited data, I wanted to exclude things like name and passanger ID so that we could have a reasonable number of features for our models to deal with) Variables: 'Pclass', 'Sex','Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'cabin_adv', 'cabin_multiple', 'numeric_ticket', 'name_title'\n\n3) Do categorical transforms on all data. Usually we would use a transformer, but with this approach we can ensure that our traning and test data have the same colums. We also may be able to infer something about the shape of the test data through this method. I will stress, this is generally not recommend outside of a competition (use onehot encoder).\n\n4) Impute data with mean for fare and age (Should also experiment with median)\n\n5) Normalize the variable \"fare\" using logarithm to give more semblance of a normal distribution\n\n6) Scaleing of the data in range(0,1) with standard scaler\u00b6","c671d79f":"We can see the same thing for the difference of Gender","096bfb38":"We want to add more detail introducing the decade features for representing the different group of Age","5dcbb27b":"Some Visualization Tools\n===","c0a1f62b":"Model Building (Baseline Validation Performance)\n===\nBefore going further, I like to see how various different models perform with default parameters. I tried the following models using 5 fold cross validation to get a baseline. With a validation set basline, we can see how much tuning improves each of the models. Just because a model has a high basline on this validation set doesn't mean that it will actually do better on the eventual test set.\n\nNaive Bayes (72.6%)\nLogistic Regression (82.1%)\nDecision Tree (77.6%)\nK Nearest Neighbor (80.5%)\nRandom Forest (80.6%)\nSupport Vector Classifier (83.2%)\nXtreme Gradient Boosting (81.8%)\nSoft Voting Classifier - All Models (82.8%)","c1e497ff":"![Upvote!](https:\/\/img.shields.io\/badge\/Upvote-If%20you%20like%20my%20work-07b3c8?style=for-the-badge&logo=kaggle)","0d8b9636":"As we can see young people has more chance to survive indipendently of the Fare but for adult people( from 20 to 60) The Fare feature seems to be relevant. \n\nWe can see the influence for the different Classes as well in the same graphic"}}