{"cell_type":{"72c0b321":"code","22349faa":"code","ee136ae0":"code","10740ca3":"code","28c3e8c4":"code","acaf6973":"code","9fd84ac7":"code","eb5ce8e7":"code","c2faa06a":"code","19f641e6":"code","18072d40":"code","3e144b94":"code","316f8cac":"code","ee60a38e":"code","a3234621":"code","54f5cab1":"code","77c90cd8":"code","94707c13":"code","ae650eb2":"code","cee25adb":"code","38bd2bcb":"code","a25beef7":"code","31ae71d4":"code","725e66c5":"code","2481e5ac":"code","25e251dd":"code","4b0a741f":"markdown","1f2bdc7e":"markdown","ac303e51":"markdown","dceaba94":"markdown","cfe95df7":"markdown","7745dc98":"markdown","0ec82b2b":"markdown","302295d9":"markdown","e539462a":"markdown","a8b4bd71":"markdown","e071ff12":"markdown","4c019b82":"markdown","09afda01":"markdown","b4e6ffff":"markdown","8e7fc00b":"markdown"},"source":{"72c0b321":"!pip install deep_autoviml","22349faa":"from deep_autoviml import deep_autoviml as deepauto","ee136ae0":"!pip install git+https:\/\/github.com\/keras-team\/keras-tuner.git","10740ca3":"!pip install autokeras","28c3e8c4":"import torch\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport autokeras as ak","acaf6973":"import tensorflow as tf\ntf.__version__","9fd84ac7":"TRAIN_DATA_URL = '\/kaggle\/input\/cusersmarildownloadsgermancsv\/german.csv'","eb5ce8e7":"df=pd.read_csv(TRAIN_DATA_URL,encoding ='ISO-8859-1',sep=\";\")\nprint(df.shape)\ndf.head()","c2faa06a":"# Initialize the structured data classifier.\nclf = ak.StructuredDataClassifier(\n    overwrite=True, max_trials=5)","19f641e6":"y=df[['Creditability']].to_numpy()\ny[:5]","18072d40":"x=df.loc[:,'Duration_of_Credit_monthly':].to_numpy()\nx[:2]","3e144b94":"x_train,x_test,y_train,y_test = train_test_split(x,y,test_size= 0.20, random_state= True,stratify=y) \nx_train.shape,x_test.shape,y_train.shape,y_test.shape","316f8cac":"y_train=y_train.reshape((-1))\ny_train.shape","ee60a38e":"y_test=y_test.reshape((-1))\ny_test.shape","a3234621":"clf","54f5cab1":"#clf.fit(x_train, y_train, epochs=5)","77c90cd8":"y_test[:4]","94707c13":"deepauto","ae650eb2":"################################################################################\nkeras_model_type =  \"fast\" ## always try \"fast\", then \"fast1\", \"fast2\" and \"auto\"\n### always set early_stopping to True first and then change it to False\n#### You always need 15 max_trials to get something decent #####\nkeras_options = {\"early_stopping\": True, 'lr_scheduler': ''}  \n#### always set tuner to \"storm\" and then \"optuna\". \n# NLP char limit kicks off NLP processing. Feature Cross later.\nmodel_options = {'tuner':\"storm\", \"max_trials\": 5, 'nlp_char_limit':10,\n                 'cat_feat_cross_flag':False, }\nproject_name = 'German_Credit' ### this is the folder where the model will be saved\n################################################################################","cee25adb":"preds = df.columns[2:].tolist()\ntargets = df.columns[:1].tolist()\ntarget = targets[0]\ntarget","38bd2bcb":"train = pd.DataFrame(np.c_[x_train,y_train], index=range(len(x_train)), columns = preds+targets)\ntest = pd.DataFrame(np.c_[x_test,y_test], index=range(len(x_test)), columns = preds+targets)\nprint(train.shape, test.shape)\ntrain.head(2)","a25beef7":"model, cat_vocab_dict = deepauto.fit(train, target, keras_model_type=keras_model_type,\n\t\tproject_name=project_name, keras_options=keras_options,  \n\t\tmodel_options=model_options, save_model_flag=True, use_my_model='',\n\t\tmodel_use_case='', verbose=0)","31ae71d4":"predictions = deepauto.predict(model, project_name, test_dataset=test,\n                                 keras_model_type=keras_model_type, \n                                 cat_vocab_dict=cat_vocab_dict)","725e66c5":"y_test = test[target].values\ny_test[:4]","2481e5ac":"y_preds = predictions[-1]\ny_preds[:4]","25e251dd":"from deep_autoviml import print_classification_model_stats\nfrom sklearn.metrics import accuracy_score\nprint('Accuracy = %0.0f%%' %(100*accuracy_score(y_test, y_preds)))\nprint_classification_model_stats(y_test, y_preds)","4b0a741f":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nprint('Accuracy = %0.0f%%' %(100*accuracy_score(y_test, predicted_y)))\nprint('Confusion Matrix: \\n%s' %confusion_matrix(y_test, predicted_y))\nprint('Classification Report: \\n%s' %classification_report(y_test, predicted_y))","1f2bdc7e":"# So deep_autoviml has the same or better performance in less than 1 minute for German Credit Data. But the biggest advantages of deep_autoviml are the following:\n1. The important advantage of using deep_autoviml is that you don't have to clean your data. You can feed it as it is.\n2. You don't have to preprocess text or categorical or string \/ NLP columns. It will handle preprocessing automatically.\n3. The best part is, your model comes with the preprocessing steps as keras layers. So you can immediately deploy the model and predict on raw test data without any preprocessing steps since the model will do that automatically","ac303e51":"## The results on test set are similarly good  - 70% accuracy\n![image.png](attachment:1c3e1713-208a-492c-801e-28451600d3bf.png)","dceaba94":"# Let us now compare the results to Deep_AutoViML which we will install now\n1. The important advantage of using deep_autoviml is that you don't have to clean your data. You can feed it as it is.\n2. You don't have to preprocess text or categorical or string \/ NLP columns. It will handle preprocessing automatically.\n3. The best part is, your model comes with the preprocessing steps as keras layers. So you can immediately deploy the model and predict on raw test data without any preprocessing steps since the model will do that automatically","cfe95df7":"# In autokeras, you can set number of epochs to run and max-trials. We set them quite low but got pretty good results on validation. The validation accuracy is 70%\n![image.png](attachment:7315612c-e9ac-46b6-91f7-805298b3406b.png)","7745dc98":"# Hope this notebook was helpful. If you liked it, pelase upvote it","0ec82b2b":"Please see more notebooks by my friend Marilia Prata for German Credit Data\n\nXBNet Classifier on German Credit Data:\nhttps:\/\/www.kaggle.com\/mpwolke\/xbnet-creditability\n\nDeep_AutoViML on German Credit Data:\nhttps:\/\/www.kaggle.com\/mpwolke\/creditability-deep-autoviml\n\n","302295d9":"## We can see that autokeras provides good results. However it has some major limitations:\n1. You need to convert all your data to numeric values. If you use string or text or NLP columns, it will fail. This is a laborious step that you must do yourself.\n2. You must clean your data before feeding it to autokeras.","e539462a":"# You can see that Precision is 76% while Recall is 97% on the Validation dataset and overall accuracy is 75%. This beats autokeras accuracy of 70%. \n![image.png](attachment:999013f1-aa2c-47c8-96ee-2378e6db8670.png)","a8b4bd71":"## We will now test the model on the heldout test dataset. \nWe can see that accuracy drops a bit since the dataset is too small and model probably overfit on such a small dataset. However, we can try other keras_model_type=\"fast1\", \"fast2\" etc and see whether we can get better results","e071ff12":"# Predict with the best model.\npredicted_y = clf.predict(x_test).ravel()\npredicted_y[:4]","4c019b82":"# Please make sure you install deep_autoviml first and then auto-keras.\nOtherwise the next few steps will give errors","09afda01":"## The results on test set are pretty good. with 68% accuracy\n![image.png](attachment:250524f5-83f9-4e8f-9b7d-8768e44a95f8.png)","b4e6ffff":"# We are going to compare two Deep Learning AutoML libraries on a very difficult classification dataset: German Credit Data. \n## The first AutoML library we will try is Autokeras. You can see their web site here:\nhttps:\/\/autokeras.com\/\n## Autokeras was developed by DATA Lab at Texas A&M University. It has 1000's of stars on Github and maintained by an army of programmers.\n\n## The next AutoML library we will try is: Deep AutoViML.\n<img src=\"https:\/\/github.com\/AutoViML\/deep_autoviml\/raw\/master\/logo.jpg\" alt=\"banner\"\/>\n\n## Deep AV is a brand new library and is built from the ground-up using the latest in Tensorflow and Keras technology. It uses keras preprocessing layers which just came out and is based on Tensorflow 2.5.\n\nWe will use the same test-train split in both using the same random_states and everything. Only thing is we will test on the final heldout test.\n\n# If you want to see more on German Credit, you can see another great notebook by Marilia here:\n\nhttps:\/\/www.kaggle.com\/mpwolke\/creditability-deep-autoviml","8e7fc00b":"# Here is the Deep Learning model that deep_autoviml built:\n![image.png](attachment:b5c3b47a-e900-4570-b419-05ec068b7755.png)"}}