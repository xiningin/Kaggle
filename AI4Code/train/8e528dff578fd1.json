{"cell_type":{"0bce14a9":"code","d3a81982":"code","25d786af":"code","09c61e39":"code","52b0067b":"code","181b0762":"code","684a67be":"code","2f94efc3":"code","304f5ef4":"code","6093c5bc":"code","9175ee3c":"code","d013295b":"code","a17c5dfc":"code","cafb08ce":"code","7355e7ef":"code","4105b8ab":"code","95854244":"code","eb5bfec8":"code","030e97b5":"code","cf999d3b":"code","ea14d79f":"markdown","3e63177f":"markdown","21b3a217":"markdown","ffcf9602":"markdown","ee75e7c9":"markdown","da99d54e":"markdown","3bd76212":"markdown","927a951b":"markdown","2de07d16":"markdown","b71b7446":"markdown","fc694a39":"markdown","fcb84f7f":"markdown","463b53e8":"markdown","05a097ac":"markdown","e5964669":"markdown","70cf57e8":"markdown","201db173":"markdown","d20ebfc9":"markdown","fcdfecb4":"markdown","b5d4cb89":"markdown","77e9c64c":"markdown","aed3df85":"markdown","dfe0bcda":"markdown"},"source":{"0bce14a9":"import numpy as np\nx = np.arange(0,100).reshape(100,1)\ny = (3 + (4 * x))+np.random.randint(0, 100,(100,1))","d3a81982":"import seaborn as sns\nimport matplotlib.pyplot as plt\nax = sns.scatterplot(x.flatten(),y.flatten())\nax.set(xlabel='X', ylabel='Y')\nplt.show()\n","25d786af":"x_final = np.c_[np.ones((100, 1)), x]","09c61e39":"theta = np.linalg.inv(x_final.T.dot(x_final)).dot(x_final.T).dot(y)","52b0067b":"print(\"the equation of the line derrived is y = {}x + {}\".format(theta[1][0],theta[0][0]))","181b0762":"y_pred = theta[0] + theta[1]*x","684a67be":"sns.lineplot(x.flatten(),y_pred.flatten(),color='red')\nsns.scatterplot(x.flatten(),y.flatten())\nax.set(xlabel='X', ylabel='Y')\nplt.show()","2f94efc3":"m = 100\nlam = 0.0001","304f5ef4":"theta = np.random.rand(2,1)\nfor i in range(100):\n    loss_gradient = (2\/m)*(x_final.T.dot(x_final.dot(theta) - y))\n    theta = theta - (lam*loss_gradient)\n    \nprint(\"the equation of the line derrived is y = {}x + {}\".format(theta[1][0],theta[0][0]))","6093c5bc":"y_pred = theta[0] + theta[1]*x","9175ee3c":"sns.lineplot(x.flatten(),y_pred.flatten(),color='red')\nsns.scatterplot(x.flatten(),y.flatten())\nax.set(xlabel='X', ylabel='Y')\nplt.show()","d013295b":"import tensorflow as tf","a17c5dfc":"x_final_tf = tf.constant(x_final)\ny_tf = tf.cast(tf.constant(y),'double')","cafb08ce":"temp_x = tf.matmul(tf.transpose(x_final_tf), x_final_tf)\ntemp_x = tf.linalg.inv(temp_x)\ntemp_x = tf.matmul(temp_x, tf.transpose(x_final_tf))\ntheta_tf = tf.matmul(temp_x, y_tf)","7355e7ef":"y_pred_tf= theta_tf[1]*x + theta_tf[0]","4105b8ab":"sns.lineplot(x.flatten(),y_pred_tf.numpy().flatten(),color='red')\nsns.scatterplot(x.flatten(),y_tf.numpy().flatten())\nax.set(xlabel='X', ylabel='Y')\nplt.show()","95854244":"m = 100\nlam = 0.0001","eb5bfec8":"x_final_tf = tf.constant(x_final)\ny_tf = tf.cast(tf.constant(y),'double')","030e97b5":"theta_tf = tf.cast(tf.random.uniform(\n   (2,1)\n),'double')\nfor i in range(100):\n    ele_1 = 2\/m* tf.transpose(x_final_tf)\n    ele_2 = tf.matmul(x_final_tf,theta_tf) - y_tf\n    loss_grad_tf = tf.matmul(ele_1,ele_2)\n    theta_tf = theta_tf - lam * loss_grad_tf\n    \n","cf999d3b":"y_pred_tf= theta_tf[1]*x + theta_tf[0]\nsns.lineplot(x.flatten(),y_pred_tf.numpy().flatten(),color='red')\nsns.scatterplot(x.flatten(),y_tf.numpy().flatten())\nax.set(xlabel='X', ylabel='Y')\nplt.show()","ea14d79f":"## Numpy implementation of normal equation\n___\n\nHere we will show a numpy implementation of the above equations . We define two variables $\\vec{X}$ and $\\vec{Y}$ , such that $\\vec{Y} = 4\\vec{X}+3$ added with some gaussian noise. We will try to generate the data and then perform linear regression using the above equations to determine the best fitting line. Due to constraints in representation we will only show simple linear regression in numpy. However multiple linear regression can also be done the same way.\n\nThe equation of the line used for dummy data is\n\n## $y = 4x + 3 + Gaussian Noise$","3e63177f":"We gave the batch size as 100 and learning rate as 0.0001","21b3a217":"#### Normal Equation","ffcf9602":"The equation for the loss in first iteration is given as \n\n## $\\lambda(\\theta_0(x),y) = \\frac{1}{m}\\displaystyle\\sum\\limits_{i=0}^m (\\theta_0(x_i) - y_i)^2$\n\nSo we find the partial derivative of this equation and subtract it from the old theta values after multiplying it with the learning rate.\n\nhence the new equation for $\\theta$ is\n\n## $\\theta_1 = \\theta_0 - \\Lambda * \\frac{\\partial }{\\partial \\theta} \\lambda(\\theta_0(x),y)$\nor\n## $\\theta_1 = \\theta_0 - \\frac{\\Lambda}{m}*\\frac{\\partial }{\\partial \\theta} \\displaystyle\\sum\\limits_{i=0}^m (\\theta_0(x_i) - y_i)^2$\n\n\n## $\\theta_0(x_i) = \\vec{\\theta_0^{T}}\\cdot\\vec{X}$\n\n\n#### Batch Gradient Descent\n\nSince we calculate the partial derivatives of the loss function w.r.t each of the $\\theta$ , the individual gradients can be calculated in parallel and we get a vector of gradients called $\\bigtriangledown_\\theta LOSS$.\n\nWe multiply $\\bigtriangledown_\\theta LOSS$ with learning rate $\\Lambda$ and substract it from $\\theta_0$ to get the new $\\theta_1$.\n\nThis new $\\theta_1$ is again used to calculate the loss and the same process is repeated iteratively till the desired loss is achieved or the loop completes.\n\nA detailed derivation of these equations can be found in [mccormickml.com](https:\/\/mccormickml.com\/2014\/03\/04\/gradient-descent-derivation\/#:~:text=When%20there%20are%20multiple%20variables,update%20rule%20for%20each%20variable.&text=A%20partial%20derivative%20just%20means,%CE%B82%20as%20a%20constant.)\n\nThe final vector represented equation for calculating the gradients after batch partial derivative is \n\n## $\\bigtriangledown_\\theta LOSS = \\frac {2}{m} * X^{T} * (X*\\theta - Y)$\n\nHere $m$ is the batch size","ee75e7c9":"Simple linear regression is the method by which we find the best fitting line which can accurately describe the dataset. The best fitting line is the line which minimises the residual sum of squares. \n\nThe reason that the method is called linear regression is because we find the best fitting straight line or a linear equation which describes the dataset. Also The word simple denotes that we have only one independent variable and one dependent variable.","da99d54e":"### Simple Linear Regression Representation\n---\n\nBuilding on the above intuition , we can say that a function which represents a straight line can be used as a simple linear regression model. Here simple means that there is only one independent variable and one dependent variable.\n\nLet us modify the vector representation of the line to a slightly different form:- \n\n## $\\vec{y} = \\beta_0 + \\beta_1 \\cdot \\vec{x}$\n\nHere $\\beta_0$ = Y Intercept and $\\beta_1$ = Slope\n\n### Multiple Linear Regression Representation\n---\n\nMultiple linear regression in a model where we have many independent variables and the dependent variable is a function of the independent variables.\n\nThe vector representation of this equation is :- \n\n## $y = \\beta_0 + \\vec{\\beta^{T}} \\cdot \\vec{x}$\n\n$\\beta_0$ can be combined with $\\vec{\\beta}$ to from another component called \n## $\\vec{\\theta} = $ $\\begin{bmatrix} \\beta_0\\\\\\beta_1\\\\\\beta_2 \\\\\\vdots\\\\\\beta_n\\end{bmatrix}$\n\nwhere $\\theta$ represents the vector of coefficients of $\\vec{x}$\n\nalso $\\vec{x}$ can be rewritten as \n\n## $\\vec{x}$ = $\\begin{bmatrix} 1\\\\x_1\\\\x_2 \\\\\\vdots\\\\x_n\\end{bmatrix}$\n     \nHence the final equation for multiple linear regression in simplified form is \n\n## $y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\dots + \\beta_nx_n$\n\nThis equation in vector form can be represented as \n\n## $\\begin{bmatrix}y_0 \\\\ y_1 \\\\ y_2 \\\\\\vdots\\\\ y_n \\end{bmatrix} = \\begin{bmatrix}\\beta_0 + \\beta_1 + \\beta_2 + \\dots + \\beta_n\\end{bmatrix}\\cdot \\begin{bmatrix}1 \\\\ x_1 \\\\ x_2 \\\\\\vdots\\\\ x_n \\end{bmatrix} $\n\nThis can be represented as :- \n\n## $\\vec{y} = \\vec{\\theta^{T}}\\cdot\\vec{X}$","3bd76212":"The scatter plots shows us the values of x and y and their corresponding y value ","927a951b":"## Summary\n___\n\nSo in this notebook we explored a bit of the math behind linear regression and also explored the underlying workings of different libraries on how they perform linear regression. We also explored the different types of solutions to linear regression problems.\n\nThe insights that we gained in this problem will be carry forwarded and we will use then to explore other concepts like logistic regression and perceptrons","2de07d16":"### The equation of a staright Line\n---\n\nA straight is represented by the below equation.\n\n## **$y = mx + c$**\n\nHere \n\n     y = Dependent Variable\n\n     x = Independent Variable\n     \n     m = Slope of the line\n     \n     c = Y Intercept\n     \n**y** can be represented as a vector of dependent variables and **x** is a vector of independent variables. Hence given we have found the actual values of **m** and **c**, every value of **m , x , and c** will give a value of y and hence the equation will be satisfied.\n\nHence we can also say that :-\n\n$y_0 = m \\cdot x_0 + c\\\\\ny_1 = m \\cdot x_1 + c\\\\\ny_2 = m \\cdot x_2 + c\\\\\n\\vdots\\\\\ny_n = m \\cdot x_n + c$\n\nThe above equations can be represented in matrix format as :- \n\n$\\begin{bmatrix}y_0 \\\\ y_1 \\\\ y_2 \\\\\\vdots\\\\ y_n \\end{bmatrix} = m \\cdot \\begin{bmatrix}x_0 \\\\ x_1 \\\\ x_2 \\\\\\vdots \\\\x_n \\end{bmatrix} + c$\n\nIn other words we define the matrix representation of a straight line as the below formula :- \n\n## $\\vec{y} = m \\cdot \\vec{x} + c$\n\nSo here we get the vector represented equation for the staright line","b71b7446":"The gradient descent algorithm generated the values of theta","fc694a39":"# Linear regression","fcb84f7f":"### Normal Equation solution to linear regression\n___\n\nLinear regressions can be solved using an optimisation algorithm like gradient descent or by directly using the normal equation. Lets begin by exploring the normal equation.\n\n## $\\theta = (X^{T} \\cdot X)^{-1} \\cdot X^{T} \\cdot Y$\n\nThe above equation is derrived by finding the cost function , finding the derivative of the function and equating it to 0.\n\nA detailed derivation of the equation is provided in [ayearofai.com](https:\/\/ayearofai.com\/rohan-3-deriving-the-normal-equation-using-matrix-calculus-1a1b16f65dda).\n\n#### Intuition\n\nFrom our concepts of differential calculus and Maxima\/Minima we know that the maximum or minimum value of an equation lies at a point where its derivative is 0.\n\nLet us consider our loss function or cost function as :-\n\n## $\\lambda(\\theta(x),y) = \\frac{1}{m}\\displaystyle\\sum\\limits_{i=0}^m (\\theta(x_i) - y_i)^2$\n\nhere \n\n## $\\theta(x) = \\vec{\\theta^{T}}\\cdot\\vec{X}$\n\nTo find the parameters of the model for which the loss is minimum , we need to find the derivative of the above equation w.r.t $\\theta$ and equate the result to 0. Then on further simplification we will get the values of $\\theta$ in terms of $X$ and $Y$\n\nSo the final equation will be\n\n## $\\frac{\\partial  }{\\partial \\theta}\\lambda(\\theta(x),y) = 0$\n\nSolving the above equation we get \n## $\\theta = (X^{T} \\cdot X)^{-1} \\cdot X^{T} \\cdot Y$\n","463b53e8":"This code uses the values in theta vector to generate our best fitting line or our linear regression model","05a097ac":"Here we plot the regression line as obtained from the gradient descent algorithm.","e5964669":"## Tensorflow implementations of Normal Equation and Gradient Descent\n___\n\nTensorflow is a very powerful library for mathematical calculation, training neural networks and a lot more. Here we will show the equivalent tensorflow implementations of the above algorithms. Since its already explained using numpy so I will not provide detailed documentation.","70cf57e8":"## Gradient Descent as an optimisation algorithm\n___\n\nAlthough the normal equation is quite well suited to find the parameters of the model, it has certain drawbacks. Since there are multiple matrix multiplications and inverse calculation occuring the normal equation approach consumes a lot of memory and it scales very badly for large datasets. Secondly if $\\vec{x}$ is non invertible then the whole equation fails. So to overcome these problems we go by the gradient descent approach which is an optimisation technique. It is computationally less expensive and also scales well for large datasets.\n\n#### Intuition\n\nIn the gradient descent approach we donot directly find the best theta value , instead  we try to find the change in loss function with the change in theta. This means that we calculate the partial derivative of the loss for each change in theta w.r.t each parameter of theta. This is called the gradient.\n\nThus once we find the gradient , we multiply it with a factor $\\Lambda$ or learning rate and substract it from the gradients. The $\\Lambda$ controls how much we want to change the gradient. It is a hyper parameter and it is controlled by the programmer. Lower learning rate means we change the gradients by a smaller bit and hence the model takes longer to converge. Grater learning rates mean we change the gradients rapidly and hence the learning rate is changed at a larger pace. ","201db173":"## Numpy Implementation of batch gradient descent\n___\n\nHere we will demonstrate the numpy implementation of batch gradient descent . We will use the previous datasets for x and y. Our batch size will be 100. As a pre processing step we will initialise the $\\theta$ with a random value and let the algorithm approach the final $\\theta$.","d20ebfc9":"#### Gradient descent","fcdfecb4":"In the above code we defined our x and y variables","b5d4cb89":"The value of $\\vec{y}$  w.r.t   $\\vec{x}$ and  $\\vec{\\theta}$ is given by\n## $\\vec{y} = \\vec{\\theta^{T}}\\cdot\\vec{X}$\n\nIn this equation we add a constant value of 1 with the x term as explained above. This is the bias term or the y intercept term used for our equation and it is combined with the x vector. So the above snippet adds a constant 1 with each value of x and combines the two into a numpy array.","77e9c64c":"Final plot which plots the best fitting line and compares it to the actual data.","aed3df85":"### Cost Function\n---\n\nCost function or loss function is defined as the difference between the expected output and actual output.\n\nHere we consider the cost function as MSE or Mean Squared Error. The formula is given as :- \n\n## $MSE = \\frac{1}{m}\\displaystyle\\sum\\limits_{i=0}^m (\\theta(x_i) - y_i)^2$\n\nIn other words the term **$\\theta(x_i) - y_i$** is the residual for any given point i. We do a square of the residuals because we want to remove the -ve sign from the equation. \n\nWe can also remove the -ve sign by taking the absolute value of the residual and the equation becomes the MAE or Mean Absolute Error\n\n## $MAE = \\frac{1}{m}\\displaystyle\\sum\\limits_{i=0}^m|(\\theta(x_i) - y_i)|$\n\nBoth these equations serve as good cost functions but when we want the loss to be exagerated i.e small losses become lesser and large losses become greater, then we use the MSE.\n\nThere is an another cost function that we use called RMSE or Root Mean Square Error\n\n## $RMSE = \\sqrt{\\frac{1}{m}\\displaystyle\\sum\\limits_{i=0}^m(\\theta(x_i) - y_i)^{2}}$\n","dfe0bcda":"The above code implements the normal equation in numpy and generates the theta vector. We can compare it with the "}}