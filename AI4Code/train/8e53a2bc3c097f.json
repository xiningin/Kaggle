{"cell_type":{"e7bc4126":"code","e9bbf9ec":"code","9174ffb2":"code","dc600eaf":"code","0c85d5c4":"code","0c903592":"code","c5f0c3c5":"code","7f3a19ba":"code","526b55ba":"code","f36503ca":"code","c549709f":"code","6f07f571":"code","8e1f7653":"code","a908ad04":"code","55154a45":"code","7de8cad7":"code","6deaa7f1":"code","be331fb0":"code","af46bf18":"code","36a661b8":"code","3b3eb832":"code","6c9092cd":"code","0e3c5625":"code","f57119eb":"code","894ace2d":"code","f61e0e88":"code","df43a2d2":"code","00890d57":"code","5cc774bd":"code","881fb42f":"markdown","1718c57f":"markdown","8ab4c61c":"markdown","ef9e4017":"markdown","bf895557":"markdown","bc9aaee8":"markdown","e25a1e77":"markdown","9d563dc5":"markdown","9d64436f":"markdown","3b994385":"markdown","6fc7d38d":"markdown","6fcf493d":"markdown"},"source":{"e7bc4126":"import os\nimport datetime\nimport warnings\nimport pandas as pd\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import StratifiedKFold\n\nwarnings.filterwarnings('ignore')\nmpl.style.use('seaborn-notebook')\nsns.set_style('whitegrid')","e9bbf9ec":"print(os.listdir('..\/input'))","9174ffb2":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\ntrain.shape, test.shape","dc600eaf":"train.head()","0c85d5c4":"test.head()","0c903592":"def missing(data):\n    total = data.isnull().sum()\n    percent = (data.isnull().sum()\/data.isnull().count()*100)\n    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    tt['Types'] = types\n    return(np.transpose(tt))","c5f0c3c5":"missing(train)","7f3a19ba":"missing(test)","526b55ba":"train.describe()","f36503ca":"test.describe()","c549709f":"def scatter_plt(data1, data2, feat):\n    i=0\n    plt.figure()\n    fig, ax = plt.subplots(4,4,figsize=(13,13))\n    \n    for feature in feat:\n        i +=1\n        plt.subplot(4,4,i)\n        plt.scatter(data1[feature], data2[feature], marker='*')\n        plt.xlabel(feature, fontsize=9)\n    plt.show()","6f07f571":"feat = ['var_0', 'var_1','var_2','var_3', 'var_4', 'var_5', 'var_6', 'var_7', 'var_8', 'var_9', 'var_10','var_11','var_12', \n        'var_13', 'var_14', 'var_15']\nscatter_plt(train[::15], test[::15], feat)","8e1f7653":"sns.countplot(train['target'])","a908ad04":"print(' {} % of 1 are there in Training \\n {} % of 0 are there in Training'.format(((train['target'].value_counts()[1]\/train.shape[0])*100),((train['target'].value_counts()[0]\/train.shape[0])*100)))","55154a45":"def distribution_plt(data1, data2, label1, label2, feat):\n    i = 0\n    plt.figure()\n    fig, ax = plt.subplots(10,10,figsize=(18,22))\n\n    for feature in feat:\n        i += 1\n        plt.subplot(10,10,i)\n        sns.kdeplot(data1[feature], bw=0.5,label=label1)\n        sns.kdeplot(data2[feature], bw=0.5,label=label2)\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=6, pad=-6)\n        plt.tick_params(axis='y', which='major', labelsize=6)\n    plt.show();","7de8cad7":"feat = train.columns.values[2:102]\ndistribution_plt(train, test, 'train', 'test', feat)\nfeat = train.columns.values[102:202]\ndistribution_plt(train, test, 'train', 'test', feat)","6deaa7f1":"features = train.columns.values[2:202]\ncorrelations = train[features].corr().abs().unstack().sort_values(kind=\"quicksort\").reset_index()\ncorrelations = correlations[correlations['level_0'] != correlations['level_1']]","be331fb0":"correlations.head(10)","af46bf18":"correlations.tail(10)","36a661b8":"features = train.columns.values[2:202]\nunique_max_train = []\nunique_max_test = []\nfor feature in features:\n    values = train[feature].value_counts()\n    unique_max_train.append([feature, values.max(), values.idxmax()])\n    values = test[feature].value_counts()\n    unique_max_test.append([feature, values.max(), values.idxmax()])","3b3eb832":"np.transpose((pd.DataFrame(unique_max_train, columns=['Feature', 'Max duplicates', 'Value'])).sort_values(by = 'Max duplicates', ascending=False).head(15))","6c9092cd":"np.transpose((pd.DataFrame(unique_max_test, columns=['Feature', 'Max duplicates', 'Value'])).sort_values(by = 'Max duplicates', ascending=False).head(15))","0e3c5625":"idx = features = train.columns.values[2:202]\nfor df in [test, train]:\n    df['sum'] = df[idx].sum(axis=1)  \n    df['min'] = df[idx].min(axis=1)\n    df['max'] = df[idx].max(axis=1)\n    df['mean'] = df[idx].mean(axis=1)\n    df['std'] = df[idx].std(axis=1)\n    df['skew'] = df[idx].skew(axis=1)\n    df['kurt'] = df[idx].kurtosis(axis=1)\n    df['med'] = df[idx].median(axis=1)","f57119eb":"train[train.columns[202:]].head()","894ace2d":"test[test.columns[201:]].head()","f61e0e88":"features = [c for c in train.columns if c not in ['ID_code', 'target']]\nfor feature in features:\n    train['r2_'+feature] = np.round(train[feature], 2)\n    test['r2_'+feature] = np.round(test[feature], 2)\n    train['r1_'+feature] = np.round(train[feature], 1)\n    test['r1_'+feature] = np.round(test[feature], 1)","df43a2d2":"features = [c for c in train.columns if c not in ['ID_code', 'target']]\ntarget = train['target']\nparam = {\n    'bagging_freq': 5,\n    'bagging_fraction': 0.4,\n    'boost_from_average':'false',\n    'boost': 'gbdt',\n    'feature_fraction': 0.05,\n    'learning_rate': 0.01,\n    'max_depth': -1,  \n    'metric':'auc',\n    'min_data_in_leaf': 80,\n    'min_sum_hessian_in_leaf': 10.0,\n    'num_leaves': 13,\n    'num_threads': 8,\n    'tree_learner': 'serial',\n    'objective': 'binary', \n    'verbosity': 1\n}","00890d57":"folds = StratifiedKFold(n_splits=10, shuffle=False, random_state=44000)\noof = np.zeros(len(train))\npredictions = np.zeros(len(test))\nfeature_importance_df = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):\n    print(\"Fold {}\".format(fold_))\n    trn_data = lgb.Dataset(train.iloc[trn_idx][features], label=target.iloc[trn_idx])\n    val_data = lgb.Dataset(train.iloc[val_idx][features], label=target.iloc[val_idx])\n\n    num_round = 1000000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 3000)\n    oof[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    predictions += clf.predict(test[features], num_iteration=clf.best_iteration) \/ folds.n_splits\n\nprint(\"CV score: {:<8.5f}\".format(roc_auc_score(target, oof)))","5cc774bd":"sub = pd.DataFrame({\"ID_code\":test[\"ID_code\"].values})\nsub[\"target\"] = predictions\nsub.to_csv(\"submission.csv\", index=False)","881fb42f":"   <h1><b><i><u>Santander Exploratory Data Analysis<\/u><\/i><\/b><\/h1>\n   <hr>\n   ![Santander](https:\/\/imagesvc.meredithcorp.io\/v3\/mm\/image?url=https%3A%2F%2Fmoneydotcomvip.files.wordpress.com%2F2016%2F07%2Fgettyimages-96378039.jpg&w=800&c=sc&poi=face&q=85)\n   <p>*Banco Santander, S.A., doing business as Santander Group, is a Spanish multinational commercial bank and financial services company founded and based in Santander, Spain. In addition to hubs in Madrid and Barcelona, Santander maintains a presence in all global financial centres as the largest Spanish banking institution in the world. Although known for its European banking operations, it has extended operations across North and South America, and more recently in continental Asia.*<\/p>\n   <p>*Many subsidiaries, such as Abbey National, have been rebranded under the Santander name. The company is a component of the Euro Stoxx 50 stock market index. In May 2016, Santander was ranked as 37th in the Forbes Global 2000 list of the world's biggest public companies. Santander is Spain\u2019s largest bank.*<\/p>\n\n   <p>*As of 2017, Santander is the 5th largest bank in Europe with approximately US D 1.4 trillion in total assets-under-management (AUM).Traded on the Euro Stoxx 50 stock market index, the bank has a total market capitalization of $69.9 billion.*<\/p>","1718c57f":"Train and Test data is distributed in balance to numeric values. ","8ab4c61c":"***Load Packages***","ef9e4017":"Data is highly Unbalanced w.r.t. target variable","bf895557":"Also, NO missing values in Tests Data, So that's good.\n<hr>","bc9aaee8":"Numerical Analysis for Train and Test Data","e25a1e77":"Few points to observe:-\n*     Mean, Std, Min, Max are almost close to each other for train and test data\n*     Std is quite large for both datasets\n*     Range for features is distributed largely\n\nLet's make scaterr plot function to plot scatter plots for datasets","9d563dc5":"NO missing values in Training Data, So that's good.","9d64436f":"Train Contains:-\n*     **ID_code** as id variable (*String*)\n*     **target** as target variable (*Numeric*)\n*     **var_i** where i = (0,199) feature variables (*Numeric*)\n\nTest Contains:-\n*     **ID_code** as id variable (*String*)\n*     **var_i** where i = (0,199) feature variables (*Numeric*)\n\nLet's Check for Missing data","3b994385":"***Feature Engineering***","6fc7d38d":"Training data has one extra column than Test data which indicates the target variable of dataset.\n\nLet's take a look at the train and test data","6fcf493d":"> Refernce:-\n*     [Santander EDA and Predictions](https:\/\/www.kaggle.com\/gpreda\/santander-eda-and-prediction\/notebook)"}}