{"cell_type":{"bd88ce67":"code","dd39c9fb":"code","1057d0a3":"code","156570bc":"code","e7f0b531":"code","ae4df33e":"code","a6c1a9c6":"code","da13babe":"code","657f4130":"code","398fe27b":"code","f7e47f8a":"code","179bf288":"markdown","5acb1e06":"markdown","65ddff60":"markdown","a81bfab5":"markdown","4a14685f":"markdown","31b54b91":"markdown","fc21261c":"markdown","abb91f44":"markdown","167488ea":"markdown","c949a4a3":"markdown","04836e1d":"markdown","b7ca3f8e":"markdown","a965cc64":"markdown","57fc3981":"markdown"},"source":{"bd88ce67":"import numpy as np\nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","dd39c9fb":"#importing library for regular expressions\nimport re\n\n# get the hashtags set with the hashtags that appear in the text more then \"level\" times\ndef find_hashtags(train, level=0): \n    train[\"text\"] = train[\"text\"].apply(lambda x: x.lower()) #we map all the text to lowercase\n    train[\"tag\"] = train[\"text\"].apply(lambda x: re.findall(r\"#(\\w+)\", x)) # we find all of the hashtags\n    tags=set([]) \n    for loc_tags in train[\"tag\"]: \n        for tag in loc_tags:\n            if not (tag is None):\n                train[tag]=train[\"text\"].apply(lambda x: int(\"#\"+tag in x))\n                if train[tag].sum()>level:\n                    tags.add(tag)\n                del train[tag]\n    del train[\"tag\"]\n    return tags\n\n#we get extra features for the dataset, that illustrated does the specific hashtag belong to the text\ndef to_hashtags(train, tags):\n    for tag in tags:\n        train[tag]=train[\"text\"].apply(lambda x: int(\"#\"+tag in x))\n    train[\"text\"]=train[\"text\"].apply(lambda x: x.replace(\"#\", \"\"))\n    del train[\"text\"]\n    del train[\"location\"]\n    del train[\"keyword\"]\n    del train[\"id\"]\n    return train\n\n#now just load data get the hashtags and try to figure can the hashtags alone give the information about markup\ntrain = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\ntags = find_hashtags(train)\ntrain_df = to_hashtags(train, tags) \n#right here we get the embedding of 0 and 1 where the text either contains the hashtag or not\ntest_df = to_hashtags(test, tags)\n#N.B. note, that we use the same \"tags\" set for the test sample, since we got to get the embedding\n#into the same space. We do not find the tags for the test data.\n\n\n#by the standard approach we can validate the models by spliting marked data into train and test.\nfrom sklearn.model_selection import train_test_split\n\ntrain_target = train_df[\"target\"]\ntrain_features = train_df.copy() # here we use copy method not to change the \"train_df\" itself during processing\ndel train_features[\"target\"]\n\nX_train, X_test, y_train, y_test = train_test_split(train_features, train_target, test_size=0.2)\n\nfrom sklearn.metrics import precision_recall_fscore_support \n#it is also usefull not only to score the model, but to find the precision and recall of the model.\n\nfrom sklearn.svm import SVC #Support Vector Classifier\nmodel = SVC()\nmodel.fit(X_train, y_train)\nprint(model.score(X_test, y_test))\nprint(precision_recall_fscore_support(model.predict(X_test), y_test))\n\nfrom sklearn.linear_model import SGDClassifier #Support Vector Classifier trained with Stochastic Gradient Descent\nmodel = SGDClassifier()\nmodel.fit(X_train, y_train)\nprint(model.score(X_test, y_test))\nprint(precision_recall_fscore_support(model.predict(X_test), y_test))\n\nfrom sklearn.naive_bayes import MultinomialNB #Naive Bayes\nmodel = MultinomialNB()\nmodel.fit(X_train, y_train)\nprint(model.score(X_test, y_test))\nprint(precision_recall_fscore_support(model.predict(X_test), y_test))","1057d0a3":"import re\n\ndef find_keywords(train, level=0):\n    train[\"keyword\"] = train[\"keyword\"].apply(lambda x: str(x).lower().split(\",\"))\n    keywords=set([])\n    for loc_keywords in train[\"keyword\"]:\n        for keyword in loc_keywords:\n            if not (keyword == \"nan\"):\n                train[keyword]=train[\"keyword\"].apply(lambda x: int(keyword in x))\n                if train[keyword].sum()>level:\n                    keywords.add(keyword)\n                del train[keyword]\n    return keywords\n\ndef to_keywords(train, keywords):\n    train[\"keyword\"] = train[\"keyword\"].apply(lambda x: str(x).lower().split(\",\"))\n    for keyword in keywords:\n        train[keyword]=train[\"keyword\"].apply(lambda x: int(keyword in x))\n    del train[\"text\"]\n    del train[\"location\"]\n    del train[\"id\"]\n    del train[\"keyword\"]\n    return train\n\ntrain = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nkeywords = find_keywords(train)\ntrain_df = to_keywords(train, keywords)\ntest_df = to_keywords(test, keywords)\n\nfrom sklearn.model_selection import train_test_split\n\ntrain_target = train_df[\"target\"]\ntrain_features = train_df.copy()\ndel train_features[\"target\"]\n\nX_train, X_test, y_train, y_test = train_test_split(train_features, train_target, test_size=0.2)\n\nfrom sklearn.metrics import precision_recall_fscore_support\n\nfrom sklearn.svm import SVC\nmodel = SVC()\nmodel.fit(X_train, y_train)\nprint(model.score(X_test, y_test))\nprint(precision_recall_fscore_support(model.predict(X_test), y_test))\n\nfrom sklearn.linear_model import SGDClassifier\nmodel = SGDClassifier()\nmodel.fit(X_train, y_train)\nprint(model.score(X_test, y_test))\nprint(precision_recall_fscore_support(model.predict(X_test), y_test))\n\nfrom sklearn.naive_bayes import MultinomialNB\nmodel = MultinomialNB()\nmodel.fit(X_train, y_train)\nprint(model.score(X_test, y_test))\nprint(precision_recall_fscore_support(model.predict(X_test), y_test))","156570bc":"from nltk.corpus import stopwords #we must reduce the stopwords from the text\n\ntrain = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntrain[\"text\"] = train[\"text\"].apply(lambda x: x.lower())\n\nstopWords = set(stopwords.words('english')) \n#so we just take the stopwords of English language, since the text is in English.\n#I am not that sophisticated in languages, but I guess that is would be far more\n#fun to clean the stopwords out of agglutinative languages such as German, or\n#if you are processing hieroglyphic text.\n\nfor w in stopWords:\n    train[\"text\"] = train[\"text\"].apply(lambda x: x.replace(\" \"+w+\" \", \" \"))\n\n    \n#Here we use the TF-IDF model for text embedding\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\n\ntrain[\"text\"] =train[\"text\"].apply(lambda x: x.replace(\"#\",\"\")) #for sure just make text \"plain\".\n#print(train[\"text\"])\n\n#here also, it is usefull to replace urls with \"site\" word,\n#especially if you already extracted the urls.\n\ntfidf_train = vectorizer.fit_transform(train[\"text\"])\n#print(vectorizer.get_feature_names())\n\n#The problem is that the dimention of the text embedding in TF-IDF model or BOW model\n#is too large, so we got to reduce the dimentionality, thus, we try to make some\n#feature as the Principal Component Analysis.\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=500)\nlower_dim_tfidf_train = pca.fit_transform(tfidf_train.todense())\n","e7f0b531":"train_df = lower_dim_tfidf_train\n\n#by standard make the validation\nfrom sklearn.model_selection import train_test_split\n\ntrain_target = train[\"target\"]\ntrain_features = train_df.copy()\n\nX_train, X_test, y_train, y_test = train_test_split(train_features, train_target, test_size=0.2)\n\n#and find precision and recall, not only the accuracy.\nfrom sklearn.metrics import precision_recall_fscore_support\n\nfrom sklearn.svm import SVC\nmodel = SVC()\nmodel.fit(X_train, y_train)\nprint(model.score(X_test, y_test))\nprint(precision_recall_fscore_support(model.predict(X_test), y_test))\n\nfrom sklearn.linear_model import SGDClassifier\nmodel = SGDClassifier()\nmodel.fit(X_train, y_train)\nprint(model.score(X_test, y_test))\nprint(precision_recall_fscore_support(model.predict(X_test), y_test))\n\nfrom sklearn.tree import DecisionTreeClassifier #here we add Decision Tree Classifier \nmodel = DecisionTreeClassifier()\nmodel.fit(X_train, y_train)\nprint(model.score(X_test, y_test))\nprint(precision_recall_fscore_support(model.predict(X_test), y_test))\n\nfrom sklearn.ensemble import RandomForestClassifier  #here we add Random Forest Classifier\nmodel = RandomForestClassifier()\nmodel.fit(X_train, y_train)\nprint(model.score(X_test, y_test))\nprint(precision_recall_fscore_support(model.predict(X_test), y_test))","ae4df33e":"some_texts = train[\"text\"]\n\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\ndocuments = [TaggedDocument(doc, [i]) for i, doc in enumerate(some_texts)]\n\nmodel = Doc2Vec(documents, vector_size=1000, workers=4, epochs=1)\ntextdf = model.docvecs\n\nfrom sklearn.model_selection import train_test_split\n\ntrain_target = train[\"target\"]\ntrain_features = textdf\n\nX_train, X_test, y_train, y_test = train_test_split(train_features, train_target, test_size=0.2)\n\nfrom sklearn.metrics import precision_recall_fscore_support\n\nfrom sklearn.svm import SVC\nmodel = SVC()\nmodel.fit(X_train, y_train)\nprint(model.score(X_test, y_test))\nprint(precision_recall_fscore_support(model.predict(X_test), y_test))\n\nfrom sklearn.linear_model import SGDClassifier\nmodel = SGDClassifier()\nmodel.fit(X_train, y_train)\nprint(model.score(X_test, y_test))\nprint(precision_recall_fscore_support(model.predict(X_test), y_test))\n\nfrom sklearn.tree import DecisionTreeClassifier\nmodel = DecisionTreeClassifier()\nmodel.fit(X_train, y_train)\nprint(model.score(X_test, y_test))\nprint(precision_recall_fscore_support(model.predict(X_test), y_test))\n\nfrom sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier()\nmodel.fit(X_train, y_train)\nprint(model.score(X_test, y_test))\nprint(precision_recall_fscore_support(model.predict(X_test), y_test))","a6c1a9c6":"# \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442'\nsome_texts = train[\"text\"]\n\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\ndocuments = [TaggedDocument(doc, [i]) for i, doc in enumerate(some_texts)]\n\nmodel = Doc2Vec(documents, vector_size=1000, workers=4, epochs=10)\ntextdf = model.docvecs\n\nfrom sklearn.model_selection import train_test_split\n\ntrain_target = train[\"target\"]\ntrain_features = textdf\n\nX_train, X_test, y_train, y_test = train_test_split(train_features, train_target, test_size=0.2)\n\nfrom sklearn.metrics import precision_recall_fscore_support\n\nfrom sklearn.svm import SVC\nmodel = SVC()\nmodel.fit(X_train, y_train)\nprint(model.score(X_test, y_test))\nprint(precision_recall_fscore_support(model.predict(X_test), y_test))\n\nfrom sklearn.linear_model import SGDClassifier\nmodel = SGDClassifier()\nmodel.fit(X_train, y_train)\nprint(model.score(X_test, y_test))\nprint(precision_recall_fscore_support(model.predict(X_test), y_test))\n\nfrom sklearn.tree import DecisionTreeClassifier\nmodel = DecisionTreeClassifier()\nmodel.fit(X_train, y_train)\nprint(model.score(X_test, y_test))\nprint(precision_recall_fscore_support(model.predict(X_test), y_test))\n\nfrom sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier()\nmodel.fit(X_train, y_train)\nprint(model.score(X_test, y_test))\nprint(precision_recall_fscore_support(model.predict(X_test), y_test))","da13babe":"# \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442'\ntrain = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\nsome_texts = train[\"text\"].apply(lambda x: x.lower()).apply(lambda x: x.replace(\"#\",\"\"))\n\nfrom nltk.corpus import stopwords\n\nstopWords = set(stopwords.words('english'))\n\nfor w in stopWords:\n    some_texts = some_texts.apply(lambda x: x.replace(\" \"+w+\" \", \" \"))\n\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\ndocuments = [TaggedDocument(doc, [i]) for i, doc in enumerate(some_texts)]\n\nmodel = Doc2Vec(documents, vector_size=1000, workers=4, epochs=50)\ntextdf = model.docvecs\n\nfrom sklearn.model_selection import train_test_split\n\ntrain_target = train[\"target\"]\ntrain_features = textdf\n\nX_train, X_test, y_train, y_test = train_test_split(train_features, train_target, test_size=0.2)\n\nfrom sklearn.metrics import precision_recall_fscore_support\n\nfrom sklearn.svm import SVC\nmodel = SVC()\nmodel.fit(X_train, y_train)\nprint(model.score(X_test, y_test))\nprint(precision_recall_fscore_support(model.predict(X_test), y_test))\n\nfrom sklearn.linear_model import SGDClassifier\nmodel = SGDClassifier()\nmodel.fit(X_train, y_train)\nprint(model.score(X_test, y_test))\nprint(precision_recall_fscore_support(model.predict(X_test), y_test))\n\nfrom sklearn.tree import DecisionTreeClassifier\nmodel = DecisionTreeClassifier()\nmodel.fit(X_train, y_train)\nprint(model.score(X_test, y_test))\nprint(precision_recall_fscore_support(model.predict(X_test), y_test))\n\nfrom sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier()\nmodel.fit(X_train, y_train)\nprint(model.score(X_test, y_test))\nprint(precision_recall_fscore_support(model.predict(X_test), y_test))","657f4130":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer()\ntrain = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntfidf_train = vectorizer.fit_transform(train[\"text\"].apply(lambda x: x.lower()).apply(lambda x: x.replace(\"#\",\"\")))\n#print(vectorizer.get_feature_names())\n\ntrain_df = tfidf_train.todense()\n\nfrom sklearn.model_selection import train_test_split\n\ntrain_target = train[\"target\"]\ntrain_features = train_df.copy()\n\nX_train, X_test, y_train, y_test = train_test_split(train_features, train_target, test_size=0.2)\n\nfrom sklearn.metrics import precision_recall_fscore_support\n\nfrom sklearn.svm import SVC\nmodel = SVC()\nmodel.fit(X_train, y_train)\nprint(model.score(X_test, y_test))\nprint(precision_recall_fscore_support(model.predict(X_test), y_test))\n\nfrom sklearn.linear_model import SGDClassifier\nmodel = SGDClassifier()\nmodel.fit(X_train, y_train)\nprint(model.score(X_test, y_test))\nprint(precision_recall_fscore_support(model.predict(X_test), y_test))\n\nfrom sklearn.naive_bayes import MultinomialNB\nmodel = MultinomialNB()\nmodel.fit(X_train, y_train)\nprint(model.score(X_test, y_test))\nprint(precision_recall_fscore_support(model.predict(X_test), y_test))","398fe27b":"#!apt install default-libmysqlclient-dev -y\n#!pip install pattern","f7e47f8a":"\"\"\"import pattern.en as ptrn\n\nfrom nltk import pos_tag\n\nfor i in range(5):\n    print(train[\"text\"].iloc[i].split())\n    words = word_tokenize(train[\"text\"].iloc[i])\n    wordsFiltered = []\n\n    for w in words:\n        if w not in stopWords:\n            wordsFiltered.append(w)\n    print(wordsFiltered)\n    for w in wordsFiltered:\n        print(ptrn.suggest(w))\n    print()\n\"\"\"","179bf288":"So, we got it, about 0.6. Surely, you can try to concatenate the embeddings of the keywords and hashtags, and even add it to the text embedding, they can get the more accurate classification, but still do not work alone.\n\n# **Exercise: **\ntry to make the same feature extraction finding urls.","5acb1e06":"OK. Now let us pass to the text processing.","65ddff60":"OK 50 epochs. And we still didn't get the benchmark. Maybe it works on some other cases,\nso we must concatinate the embeddings. But, maybe not. \n\n# Exercise:\nYou could try this. Just concatinate the results of the Doc2Vec model along with TF-IDF\nand use the same classifiers. Make a comment if it gains some quality.","a81bfab5":"Just the same operation to get the keywords embedding. Does the keywords effect the markup values?","4a14685f":"First of all we get the extra features from the text. We decompose the\ninfromation written in the text to \"language coded\" and some additional\ninformation like #hashtags. Really you could apply the same approach\nto keywords, urls, etc.","31b54b91":"Note that the dataset is unballanced, so\nif you make the random classifier or constant classifier it must be (in this case) not less then 0.6 accuracy.","fc21261c":"Here are few tips on the generic preprocessing of texts\nand the illustration that the simple models sometimes\nare more usefull to get the benchmark for the score.","abb91f44":"So. We see that TF-IDF model is just fine to start with. What about the\nsimplies Bag of Words vectorization?","167488ea":"I got it even worse, then random or constant clasiffier, under 0.6.\n\nMaybe the number of epochs is to small. Try more epochs.","c949a4a3":"I wanted to make some more text cleaning\nvia the \"pattern\" library. Guess, will do it in the future If you like this Notebook.","04836e1d":"We gain the benchmark about 0.8, which is already better, with the same SVC.","b7ca3f8e":"OK. It is better, but less then the benchmark of TF-IDF model. Maybe the number of epochs is still not enough.","a965cc64":"So let us try Word2Vec embedding now.","57fc3981":"BOW works approximately the same as the TF-IDF model here."}}