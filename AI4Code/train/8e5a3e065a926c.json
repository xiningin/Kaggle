{"cell_type":{"3fb992ec":"code","fbff5b2c":"code","209d75c4":"code","32a10c37":"code","96c7744c":"code","332220b8":"code","a073d175":"code","05df384c":"code","0d515612":"code","2edfebe6":"code","105cb03d":"code","e11f20c0":"code","77f7540f":"code","22ae505a":"code","1b02f615":"code","c2628a75":"code","0378d5cb":"code","f2222b57":"code","4ec53d72":"code","6d8951d3":"code","4715270a":"code","6a5bfb40":"code","9df0373e":"code","eaa6fe99":"code","727a541f":"code","848f9cf6":"code","af1c30ee":"code","f1b1dacc":"code","29adb650":"code","ada09a3d":"code","bbdd839f":"code","7095e36f":"code","f939ce91":"code","8ef67034":"code","264d47eb":"code","3843e207":"code","05489016":"code","0182d285":"code","a6746674":"code","83ca2c0a":"code","29330ba9":"code","fd66fac8":"code","6ca98cbd":"code","9e4f80ef":"code","b2f091cb":"code","d8faca40":"code","0675bf11":"code","2acddb1d":"code","44d505f6":"code","3258aa3e":"code","527573a1":"code","618a1825":"code","6cdbe43b":"code","3f8cbdfd":"code","9002fe6d":"code","56574413":"code","bd7c070a":"code","392fef08":"code","448d2df4":"code","bb12f0bf":"code","56f7fbca":"code","80699062":"code","4afc8f7d":"code","00a68e63":"code","1ea49145":"code","f37d10a9":"code","47e8080c":"code","b99ecaf1":"markdown","4016b167":"markdown","aa1b6fda":"markdown","294c4050":"markdown","c3b697a6":"markdown","5de90f49":"markdown","ba43f033":"markdown","720d4e7c":"markdown","20b54a10":"markdown","5fd45537":"markdown","46d1e711":"markdown","a3c72da1":"markdown","fe5e6744":"markdown","14ddc834":"markdown","5da31505":"markdown","0330f362":"markdown","eabec78f":"markdown","0e9d4d85":"markdown","b549439f":"markdown","2a3c106f":"markdown","89a15298":"markdown","2c5ec880":"markdown","cf7a6c38":"markdown","04409cb3":"markdown","d1a129b2":"markdown","9bc85b04":"markdown","0a1fa71c":"markdown","3b4ebdce":"markdown","11e768b9":"markdown"},"source":{"3fb992ec":"# Data analysis tools\nimport pandas as pd\nimport numpy as np\n\n# Data Visualization Tools\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Data Pre-Processing Libraries\nfrom sklearn.preprocessing import LabelEncoder,StandardScaler\n\n# For Train-Test Split\nfrom sklearn.model_selection import train_test_split\n\n# Libraries for various Algorithms\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Metrics Tools\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import accuracy_score, f1_score\n\n#For Receiver Operating Characteristic (ROC)\nfrom sklearn.metrics import roc_curve ,roc_auc_score, auc","fbff5b2c":"train = pd.read_csv('..\/input\/mobile-price-classification\/train.csv')\ntrain.head()","209d75c4":"train.info()","32a10c37":"train.isnull().sum()","96c7744c":"train.columns","332220b8":"sns.countplot(train[\"price_range\"])\nplt.xlabel(\"Class\")\nplt.ylabel(\"frequency\")\nplt.title(\"Checking imbalance\")","a073d175":"sns.distplot(train.skew(),hist=False)\nplt.show()","05df384c":"# Printing interquartile range (IQR) for each column\nQ1 = train.quantile(0.25)\nQ3 = train.quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)","0d515612":"# Boxplot visualization for columns with high IQR\n\nplt.boxplot([train[\"battery_power\"]])\nplt.xticks([1],[\"battery_power\"])\nplt.show()\nplt.boxplot([train[\"px_height\"]])\nplt.xticks([1],[\"px_height\"])\nplt.show()\nplt.boxplot([train[\"px_width\"]])\nplt.xticks([1],[\"px_width\"])\nplt.show()\nplt.boxplot([train[\"ram\"]])\nplt.xticks([1],[\"ram\"])\nplt.show()","2edfebe6":"train.corr()","105cb03d":"fig = plt.figure(figsize=(15,12))\nsns.heatmap(train.corr(),cmap = \"RdYlGn\", annot = True)","e11f20c0":"train['price_range'].unique()","77f7540f":"import seaborn as sns\nsns.pairplot(data=train,y_vars=['price_range'],x_vars=['battery_power', 'blue', 'clock_speed', 'dual_sim', 'fc', 'four_g',\n       'int_memory', 'm_dep', 'mobile_wt', 'n_cores', 'pc', 'px_height',\n       'px_width', 'ram', 'sc_h', 'sc_w', 'talk_time', 'three_g',\n       'touch_screen', 'wifi'])","22ae505a":"corr_matrix=train.corr()\ncorr_matrix['price_range']","1b02f615":"sns.countplot(train['price_range'])\nplt.show()","c2628a75":"sns.boxplot(train['price_range'],train['talk_time'])","0378d5cb":"sns.boxplot(x='ram',y='price_range',data=train,color='red')","f2222b57":"sns.pointplot(y=\"int_memory\", x=\"price_range\", data=train)","4ec53d72":"labels = [\"3G-supported\",'Not supported']\nvalues=train['three_g'].value_counts().values\ncolors = ['green', 'red']\n_, ax = plt.subplots()\nax.pie(values, labels=labels,colors=colors, autopct='%1.1f%%',shadow=True,startangle=90)\nplt.show()","6d8951d3":"labels = [\"4G-supported\",'Not supported']\nvalues=train['four_g'].value_counts().values\ncolors = ['green', 'red']\n_, ax = plt.subplots()\nax.pie(values, labels=labels,colors=colors, autopct='%1.1f%%',shadow=True,startangle=90)\nplt.show()","4715270a":"plt.figure(figsize=(10,6))\ntrain['fc'].hist(alpha=0.5,color='yellow',label='Front camera')\ntrain['pc'].hist(alpha=0.5,color='red',label='Primary camera')\nplt.legend()\nplt.xlabel('MegaPixels')","6a5bfb40":"sns.pointplot(y=\"talk_time\", x=\"price_range\", data=train)","9df0373e":"scaler = StandardScaler()","eaa6fe99":"X=train.drop('price_range',axis=1)\ny=train['price_range']\n\nscaler.fit(X)\nx = scaler.transform(X)\n\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.30, random_state=0)","727a541f":"#Fitting the model\n\nknn = KNeighborsClassifier(n_neighbors=35)\nknn.fit(x_train,y_train)","848f9cf6":"# Applying the model to the x_test\n\npred_knn = knn.predict(x_test)","af1c30ee":"# Finding Accuracy\n\nKNN = accuracy_score(pred_knn,y_test)*100","f1b1dacc":"# Confusion Matrix\n\ncm_knn=confusion_matrix(pred_knn,y_test)\nprint(cm_knn)","29adb650":"# Classification Report that computes various\n# metrics like Precision, Recall and F1 Score\n\nprint(classification_report(pred_knn,y_test))","ada09a3d":"#Fitting the model\n\ngnb=GaussianNB()\ngnb.fit(x_train,y_train)","bbdd839f":"# Applying the model to the x_test\n\npred_gnb = gnb.predict(x_test)","7095e36f":"# Finding Accuracy\n\nGNB = accuracy_score(pred_gnb,y_test)*100","f939ce91":"# Confusion Matrix\n\ncm_gnb=confusion_matrix(pred_gnb,y_test)\nprint(cm_gnb)","8ef67034":"# Classification Report that computes various \n# metrics like Precision, Recall and F1 Score\n\nprint(classification_report(pred_gnb,y_test))","264d47eb":"#Fitting the model\n\nsvc = SVC(probability=True)\nsvc.fit(x_train,y_train)\n\n# Applying the model to the x_test\npred_svc = svc.predict(x_test)","3843e207":"# Finding Accuracy\n\nSVC = accuracy_score(pred_svc,y_test)*100","05489016":"# Confusion Matrix\n\ncm_svc=confusion_matrix(pred_svc,y_test)\nprint(cm_svc)","0182d285":"# Classification Report that computes various \n#metrics like Precision, Recall and F1 Score\n\nprint(classification_report(pred_svc,y_test))","a6746674":"#Fitting the model\n\ndtree_en = DecisionTreeClassifier()\nclf = dtree_en.fit(x_train,y_train)","83ca2c0a":"# Applying the model to the x_test\n\npred_dt = clf.predict(x_test)","29330ba9":"# Finding Accuracy\n\nDTREE = accuracy_score(pred_dt,y_test)*100","fd66fac8":"# Confusion Matrix\n\ncm_dt=confusion_matrix(y_test,pred_dt)\nprint(cm_dt)\n\n# Classification Report that computes various \n# metrics like Precision, Recall and F1 Score\n\nprint(classification_report(y_test,pred_dt))","6ca98cbd":"#Fitting the model\n\nGBC=GradientBoostingClassifier(n_estimators=150)\nGBC.fit(x_train,y_train)","9e4f80ef":"# Applying the model to the x_test\n\nY_predict=GBC.predict(x_test)","b2f091cb":"# Finding Accuracy\n\ngbc = accuracy_score(y_test,Y_predict)*100","d8faca40":"# Confusion Matrix\n\ncm_gbc=confusion_matrix(y_test,Y_predict)\nprint(cm_gbc)\n\n# Classification Report that computes various \n# metrics like Precision, Recall and F1 Score\n\nprint(classification_report(y_test,Y_predict))","0675bf11":"#Fitting the model\n\nrfc = RandomForestClassifier(n_estimators=30,criterion='gini',random_state=1,max_depth=10)\nrfc.fit(x_train, y_train)","2acddb1d":"# Applying the model to the x_test\n\npred_rf= rfc.predict(x_test)","44d505f6":"# Finding Accuracy\n\nRFC = accuracy_score(y_test,pred_rf)*100","3258aa3e":"# Confusion Matrix\n\ncm_rf=confusion_matrix(pred_rf,y_test)\nprint(cm_rf)","527573a1":"# Classification Report that computes various \n# metrics like Precision, Recall and F1 Score\n\nprint(classification_report(pred_rf,y_test))","618a1825":"#Fitting the model. Base model is chosen to be Decision Tree\n\nmodel = DecisionTreeClassifier(criterion='entropy',max_depth=1,random_state=0)\nadaboost = AdaBoostClassifier(n_estimators=80, base_estimator=model,random_state=0)\nadaboost.fit(x_train,y_train)","6cdbe43b":"# Applying the model to the x_test\n\npred = adaboost.predict(x_test)","3f8cbdfd":"# Finding Accuracy\n\nada = accuracy_score(y_test,pred)*100","9002fe6d":"# Confusion Matrix\n\ncm_ada=confusion_matrix(pred,y_test)\nprint(cm_ada)\n\n# Classification Report that computes various \n# metrics like Precision, Recall and F1 Score\n\nprint(classification_report(pred,y_test))","56574413":"#Fitting the model\n\nxgb =  XGBClassifier(learning_rate =0.000001,n_estimators=1000,max_depth=5,min_child_weight=1,\n                     subsample=0.8,colsample_bytree=0.8,nthread=4,scale_pos_weight=1,seed=27)\nxgb.fit(x_train, y_train)","bd7c070a":"# Applying the model to the x_test\n\n\npredxg = xgb.predict(x_test)\n\n# Finding Accuracy\nxg = accuracy_score(y_test,predxg)*100\n","392fef08":"# Confusion Matrix\n\ncm_xg=confusion_matrix(predxg,y_test)\nprint(cm_xg)\n\n# Classification Report that computes various \n# metrics like Precision, Recall and F1 Score\n\nprint(classification_report(predxg,y_test))","448d2df4":"# Accuracy values for all the models\nprint(\"1)  KNN                    :\",round(KNN, 2))\nprint(\"2)  Naive-Bayes            :\",round(GNB, 2))\nprint(\"3)  SVM                    :\",round(SVC, 2))\nprint(\"4)  Decision Tree          :\",round(DTREE, 2))\nprint(\"5)  Gradient Boosting      :\",round(gbc, 2))\nprint(\"6)  Random Forest          :\",round(RFC, 2))\nprint(\"7)  AdaBoost               :\",round(ada, 2))\nprint(\"8)  XGBoost                :\",round(xg, 2))","bb12f0bf":"test = pd.read_csv('..\/input\/mobile-price-classification\/test.csv')\ntest.head()","56f7fbca":"test = test.drop('id',axis=1) \nscaler.fit(test)\nx = scaler.transform(test)","80699062":"price = GBC.predict(x)","4afc8f7d":"print(price)","00a68e63":"test[\"price_range\"]=price","1ea49145":"test.to_csv('Submission_Price_Detection.csv')","f37d10a9":"submission = pd.read_csv('.\/Submission_Price_Detection.csv')","47e8080c":"submission.head()","b99ecaf1":"# Step 1: Importing Libraries","4016b167":"# Step 3: Understanding the Structure of the Dataset","aa1b6fda":"# Accuracy Reports","294c4050":"# 5) Gradient Boosting","c3b697a6":"# Step 8: Applying the Best fit model to the test.csv file","5de90f49":"<b> Selecting the Dependent and Independent variables and Scaling all the values <\/b>","ba43f033":"<b> No Missing Values in the dataset. Hence no treatment for missing values required<\/b>","720d4e7c":"<b> No of Phones vs Camera megapixels of front and primary camera<\/b>","20b54a10":"# 6) Random Forest","5fd45537":"1)  Gradient Boosting      : 91.50 %\n\n2)  SVM                    : 88.83 %\n\n3)  Random Forest          : 86.83 %\n\n4)  XGBoost                : 84.83 %\n\n5)  Decision Tree          : 82.67 %\n\n6)  Naive-Bayes            : 82.67 %\n\n7)  AdaBoost               : 64.17 %\n\n8)  KNN                    : 61.17 %\n\nHere, <b>Gradient Boosting has the highest accuracy rate.\n\nHence the best fit Model is Gradient Boosting<\/b> ","46d1e711":"# 7) AdaBoost (Entropy-Decision Tree)","a3c72da1":"# Step 9: View Predictions","fe5e6744":"# Step 6: Defining the Tatget and Predictor Variables and Standard Scaling","14ddc834":"# 3) SVM","5da31505":"# Step 2: Loading the Dataset","0330f362":"# 2) Naive-Bayes","eabec78f":"# Step 7: Fitting the dataset to various models\n\n<b>We will fit the dataset to various models and find out the best fit model among these.\n\nVarious models used in this notebook are:\n\n1)  KNN                \n\n2)  Naive-Bayes       \n\n3)  SVM                   \n\n4)  Decision Tree         \n\n5)  Gradient Boosting     \n\n6)  Random Forest         \n\n7)  AdaBoost             \n\n8)  XGBoost    \n\n<\/b>           ","0e9d4d85":"<b>How does ram is affected by price<\/b>","b549439f":"# b) Checking Imbalance","2a3c106f":"# 8) XGBoost ","89a15298":"# 4) Decision Tree","2c5ec880":"Internal Memory vs Price Range","cf7a6c38":"# 1) KNN","04409cb3":"# Step 4: Data Pre-Processing\n\n   # a) Treating Missing Values","d1a129b2":"<b>3G supporting Phones vs Non supporting Phones<\/b>","9bc85b04":"<b> Talk time vs Price range<\/b>","0a1fa71c":"The outliers are very low. Hence, we dont need to treat any outliers.","3b4ebdce":"<b>4G supporting Phones vs Non supporting Phones<\/b>","11e768b9":"# Step 5: Data Visualization"}}