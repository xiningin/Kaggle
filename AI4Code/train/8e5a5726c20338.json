{"cell_type":{"2c50b075":"code","8fbc8b81":"code","185bcc7a":"code","c310bb29":"code","a56bd5d3":"code","02427bce":"code","b8c8b714":"code","17e463fd":"code","5d5dc04c":"code","3f3d3b65":"code","683bbe68":"code","d5706234":"code","f3564f2f":"code","3510dc9a":"code","d7b9911c":"code","7b3d32da":"code","9cdfaca6":"code","b32d4aa6":"code","e0e1e2a3":"code","452dddb4":"code","a2d25d83":"code","e3f6aecb":"code","90434cc9":"code","f09c8815":"code","c21ad7d2":"code","19ee8b90":"code","0c8ec3dd":"code","a306e9c7":"code","9063cd57":"code","a4d31532":"code","2f988f69":"code","0d73c1f6":"code","96ab16c2":"code","52d8e5d8":"code","8ad5cbcf":"code","3fc6e4ba":"code","250460dd":"code","7922d898":"code","78b4dd82":"code","8d04608d":"code","532f8cb4":"code","03e065cd":"code","45edff93":"code","35dd3cda":"code","bb0c4285":"code","053031ba":"code","4655cedf":"code","1b2f37a3":"code","ebe6d835":"code","d7e19c59":"code","a1b983c1":"code","bb012248":"code","599f52f2":"code","c1962898":"code","de40305e":"code","84812d4b":"code","6245c21f":"code","1b0d8bc7":"code","321e80e7":"code","2b7ea744":"code","e48a100d":"code","3584d6d4":"code","9112bcad":"code","bc4c2c0d":"code","ff72574f":"markdown","9b90aad0":"markdown","d93d4a8c":"markdown","d2e31175":"markdown","5a0ed924":"markdown","1c949019":"markdown","c2ec7533":"markdown","125da0ce":"markdown","21e0df81":"markdown","5f5e43f4":"markdown","87e67765":"markdown","7c8c2951":"markdown","00db7937":"markdown","797c2f69":"markdown","9afd97ce":"markdown","0e90616c":"markdown","15740b22":"markdown","c3a0d3f0":"markdown","e16f1e06":"markdown","c3978764":"markdown","451835fc":"markdown","50ab26cb":"markdown","395ef262":"markdown","b07c34c6":"markdown","2a9803fc":"markdown","511f6649":"markdown","3655499a":"markdown","13ec9d34":"markdown","12f6b582":"markdown","a92f2d01":"markdown","d7eec109":"markdown","044970f5":"markdown","44bfd047":"markdown","63c0104a":"markdown","d315c9cb":"markdown","8b3caaec":"markdown","7bdd6e6f":"markdown","76132fa6":"markdown","9bba2d51":"markdown","968cea29":"markdown","b34dc9f6":"markdown","38ff9e35":"markdown","39f0f0fe":"markdown","99d1d793":"markdown","ab96426d":"markdown","7ea079c3":"markdown","53ed838c":"markdown","b8ca9e43":"markdown","181053a6":"markdown","127a2f8d":"markdown","5ef35d6d":"markdown","c1b5bd2e":"markdown","e5361812":"markdown","0cd93efd":"markdown","90366930":"markdown","196d67c5":"markdown","8ec76a73":"markdown","4bd4c973":"markdown","6572b88e":"markdown","6959bfc0":"markdown","fc70bc6b":"markdown","8a0e5f65":"markdown","df42e73b":"markdown","2e83e4af":"markdown"},"source":{"2c50b075":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n%matplotlib inline\nplt.style.use('seaborn')","8fbc8b81":"from subprocess import check_output","185bcc7a":"!pip install scikit-learn==0.23.0\n\nfrom numpy.ma import MaskedArray\nimport sklearn.utils.fixes\n\nsklearn.utils.fixes.MaskedArray = MaskedArray","c310bb29":"df_treino= pd.read_csv(\"..\/input\/adult-pmr3508\/train_data.csv\", index_col=['Id'], na_values=\"?\")","a56bd5d3":"total = df_treino.isnull().sum().sort_values(ascending = False)\npercentual = ((df_treino.isnull().sum()\/df_treino.isnull().count())*100).sort_values(ascending = False)\nmissing_data = pd.concat([total, percentual], axis = 1, keys = ['Total', '%'])\nmissing_data.head()","02427bce":"print('occupation:\\n')\nprint(df_treino['occupation'].describe())\n\nprint('\\n\\nworkclass:\\n')\nprint(df_treino['workclass'].describe())\n\nprint('\\n\\nnative.country:\\n')\nprint(df_treino['native.country'].describe())","b8c8b714":"value = df_treino['workclass'].describe().top\ndf_treino['workclass'] = df_treino['workclass'].fillna(value)\n\nvalue = df_treino['native.country'].describe().top\ndf_treino['native.country'] = df_treino['native.country'].fillna(value)\n\nvalue = df_treino['occupation'].describe().top\ndf_treino['occupation'] = df_treino['occupation'].fillna(value)\n","17e463fd":"df_treino.head()","5d5dc04c":"df_treino.shape","3f3d3b65":"df_treino.describe()","683bbe68":"df_treino.info()","d5706234":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()","f3564f2f":"df = df_treino.copy()\ndf['income'] = le.fit_transform(df['income'])\ndf['income']","3510dc9a":"mask = np.triu(np.ones_like(df.corr(), dtype=np.bool))\nplt.figure(figsize=(10,10))\nsns.heatmap(df.corr(), mask=mask, square = True, annot=True, vmin=-1, vmax=1, cmap='icefire')\nplt.show()","d7b9911c":"plt.figure(figsize=(5, 7))\ndf_treino['income'].hist(color = 'mediumaquamarine')\nplt.xlabel('income')\nplt.ylabel('quantity')\nplt.title('Histograma de Renda')","7b3d32da":"sns.catplot(x=\"income\", y=\"hours.per.week\", kind=\"box\", data=df, palette=\"Accent\");\nsns.catplot(x=\"income\", y=\"education.num\", kind=\"box\", data=df, palette=\"PuOr\");\nsns.catplot(x=\"income\", y=\"age\", kind=\"box\", data=df, palette=\"PuOr\");\nsns.catplot(x=\"income\", y=\"capital.gain\", data=df, palette=\"Accent\");\nsns.catplot(x=\"income\", y=\"capital.loss\", data=df, palette =\"PuOr\");","9cdfaca6":"sns.countplot(x=\"income\", hue=\"sex\", data=df, color=\"crimson\")\nplt.title('Rela\u00e7\u00e3o entre renda e sexo')\nplt.figure(figsize=(10, 7))\ndf['sex'].value_counts().plot(kind = 'pie')","b32d4aa6":"male = df[df['sex'] == 'Male'].count()[0]\nfemale = df.shape[0] - male\n\nprint(\"Podemos ver que a quantidade de mulheres que recebem >50k por ano \u00e9 bem mais baixa que a de homens, em torno de \" ,female*100\/\/(female+male),\"%\")","e0e1e2a3":"plt.figure(figsize=(16, 6))\nsns.barplot(x=\"income\", y=\"education\", hue=\"sex\", data=df, color=\"crimson\")\nplt.title('Rela\u00e7\u00e3o entre Renda, Educa\u00e7\u00e3o e Sexo')","452dddb4":"sns.catplot(y=\"workclass\", x=\"income\", kind=\"bar\", data=df);","a2d25d83":"sns.countplot(x=\"income\", hue=\"workclass\", data=df)","e3f6aecb":"sns.barplot(x=\"income\", y=\"workclass\", hue=\"sex\", data= df, color=\"crimson\")","90434cc9":"sns.catplot(y=\"race\", x=\"income\", kind=\"bar\", data=df, palette=\"Accent\");","f09c8815":"sns.countplot(x=\"income\", hue=\"race\", data=df)","c21ad7d2":"sns.barplot(x=\"income\", y=\"race\", hue=\"sex\", data=df, color=\"crimson\")","19ee8b90":"sns.catplot(y=\"marital.status\", x=\"income\", kind=\"bar\", data=df);","0c8ec3dd":"sns.countplot(x=\"income\", hue=\"marital.status\", data=df);","a306e9c7":"sns.catplot(y=\"occupation\", x=\"income\", kind=\"bar\", height=5, aspect=2, data=df);","9063cd57":"sns.countplot(x=\"income\", hue=\"occupation\", data=df)","a4d31532":"sns.barplot(x=\"income\", y=\"occupation\", hue=\"sex\", data=df, color=\"crimson\")","2f988f69":"sns.catplot(y=\"native.country\", x=\"income\", kind=\"bar\", height=10, aspect=1, data=df)","0d73c1f6":"df[\"native.country\"].value_counts()","96ab16c2":"df_treino.drop_duplicates(keep='first', inplace=True)","52d8e5d8":"df_treino = df_treino.drop(['fnlwgt', 'native.country'], axis=1)","8ad5cbcf":"Y= df_treino.pop('income') #vari\u00e1vel target\n\nX= df_treino","3fc6e4ba":"from sklearn.model_selection import train_test_split\nX_treino, X_valid, Y_treino, Y_valid = train_test_split(X, Y, test_size=0.25, random_state=42)","250460dd":"#Vari\u00e1veis Esparsas\nspa_cols = ['capital.gain', 'capital.loss']\n\n#Vari\u00e1veis Num\u00e9ricas\nnum_cols = list(X_treino.select_dtypes(include=[np.number]).columns.values) \nnum_cols.remove('capital.gain')\nnum_cols.remove('capital.loss')\n\n#Vari\u00e1veis Qualitativas\nquali_cols = list(X_treino.select_dtypes(exclude=[np.number]).columns.values)","7922d898":"from sklearn.impute import SimpleImputer #Usamos um transformador para preencher missing data usando a estrat\u00e9gia de substitui-lo pela **moda**.\nfrom sklearn.preprocessing import OneHotEncoder #para transformar os dados para quantitativos\nfrom sklearn.pipeline import Pipeline\n\nquali_pipeline = Pipeline(steps = [('imputer', SimpleImputer(strategy='most_frequent')),('onehot', OneHotEncoder(drop='if_binary'))])","78b4dd82":"from sklearn.preprocessing import StandardScaler \nfrom sklearn.preprocessing import RobustScaler\n\n#transformar todas as vari\u00e1veis para uma mesma escala\nnum_pipeline = Pipeline(steps=[('scaler', StandardScaler())])\nspa_pipeline = Pipeline(steps=[('scaler', RobustScaler())])","8d04608d":"from sklearn.compose import ColumnTransformer\n\npreprocessador = ColumnTransformer(transformers=[('num', num_pipeline, num_cols),('spr', spa_pipeline, spa_cols),('quali', quali_pipeline, quali_cols)])","532f8cb4":"X_treino = preprocessador.fit_transform(X_treino)","03e065cd":"from sklearn.svm import SVC\n\n# Instancia nosso classificador\nsvc = SVC(random_state=42, probability=True)\n\nfrom sklearn.model_selection import cross_val_score\n\nscore = cross_val_score(svc, X_treino, Y_treino, cv = 4, scoring=\"accuracy\")\nprint(\"Acur\u00e1cia com cross validation:\", score.mean())","45edff93":"# Importa o Bayes Search:\nfrom skopt import BayesSearchCV\n\n# Importa o espa\u00e7o de busca inteiro\nfrom skopt.space import Integer, Real\n\n# Cria o Bayes Search:\nsvc_search_cv = BayesSearchCV(estimator = svc,\n                              search_spaces = {'C': Real(1e-2, 20),\n                                               'gamma': ['scale', 'auto'],},\n                              cv = 2,\n                              n_iter = 15, n_jobs=-1, random_state=42)\n\n# Realizando a otimiza\u00e7\u00e3o por BayesSearch:\n%timeit -n 1 -r 1 svc_search_cv.fit(X_treino, Y_treino)\n\nprint('Melhores hiperpar\u00e2metros: {}'.format(svc_search_cv.best_params_))\nprint('Desempenho do melhor modelo: {}'.format(round(svc_search_cv.best_score_,5)))","35dd3cda":"from sklearn.ensemble import RandomForestClassifier\n\n# Instancia nosso classificador\nrfc = RandomForestClassifier(random_state=42)","bb0c4285":"# Cria o Bayes Search:\nrfc_search_cv = BayesSearchCV(estimator = rfc,\n                              search_spaces = {'n_estimators': Integer(100, 500),\n                                               'criterion': ['gini', 'entropy'],\n                                               'max_depth': Integer(1, 50),},\n                              cv = 5,\n                              n_iter = 20, n_jobs=-1, random_state=42)\n\n# Realizando a otimiza\u00e7\u00e3o por BayesSearch:\n%timeit -n 1 -r 1 rfc_search_cv.fit(X_treino, Y_treino)\n\nprint('Melhores hiperpar\u00e2metros: {}'.format(rfc_search_cv.best_params_))\nprint('Desempenho do melhor modelo: {}'.format(round(rfc_search_cv.best_score_,5)))","053031ba":"from xgboost import XGBClassifier\n\n# Instancia nosso classificador\nxgb = XGBClassifier(random_state=42)\n","4655cedf":"xgb_search_cv = BayesSearchCV(estimator = xgb,\n                              search_spaces = {'n_estimators': Integer(10, 500),\n                                               'learning_rate': Real(1e-3, 1),\n                                               'max_depth': Integer(1, 20),\n                                               'reg_alpha': Real(1e-14, 1e1, prior = 'log-uniform'),\n                                               'reg_lambda': Real(1e-14, 1e1, prior = 'log-uniform'),},\n                              cv = 5,\n                              n_iter = 75, n_jobs=-1, random_state=42)\n\n# Realizando a otimiza\u00e7\u00e3o por BayesSearch:\n%timeit -n 1 -r 1 xgb_search_cv.fit(X_treino, Y_treino)\n\nprint('Melhores hiperpar\u00e2metros: {}'.format(xgb_search_cv.best_params_))\nprint('Desempenho do melhor modelo: {}'.format(round(xgb_search_cv.best_score_,5)))","1b2f37a3":"from sklearn.neural_network import MLPClassifier\n\n# Instancia nosso classificador\nmlp = MLPClassifier(random_state=42, early_stopping=True)","ebe6d835":"from scipy.stats import loguniform as sp_loguniform\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Hiperpar\u00e2metros a serem otimizados\nhyperparams = {'hidden_layer_sizes': [(2 ** i, 2 ** j) for j in np.arange(5, 8) for i in np.arange(4, 7)],\n               'alpha': sp_loguniform(1e-10, 1e-1),\n               'learning_rate': ['constant','adaptive']}\n\n# Busca de Hiperpar\u00e2metros\nmlp_search_cv = RandomizedSearchCV(mlp, hyperparams, scoring='accuracy', n_iter=25, cv=3, n_jobs=-1, random_state=42)\n%timeit -n 1 -r 1 mlp_search_cv.fit(X_treino, Y_treino)\n\nprint('Melhores hiperpar\u00e2metros: {}'.format(mlp_search_cv.best_params_))\nprint('Desempenho do melhor modelo: {}'.format(round(mlp_search_cv.best_score_,5)))","d7e19c59":"X_valid = preprocessador.transform(X_valid)#pre-processamento dos dados de valida\u00e7\u00e3o","a1b983c1":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder() #para o c\u00e1lculo da AUC","bb012248":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\n\n# Calculando a AUC do SVC\nsvc_roc_auc = roc_auc_score(le.fit_transform(Y_valid), svc_search_cv.predict_proba(X_valid)[:,1])\n\n# Calculando a acur\u00e1cia do SVC\nsvc_acc = accuracy_score(Y_valid, svc_search_cv.predict(X_valid))\n\nprint('AUC -------- SVC: {:.4f}'.format(svc_roc_auc))\nprint('Acur\u00e1cia --- SVC: {:.4f}'.format(svc_acc))","599f52f2":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\n\n# Calculando a AUC da Floresta Aleat\u00f3ria\nrfc_roc_auc = roc_auc_score(le.fit_transform(Y_valid), rfc_search_cv.predict_proba(X_valid)[:,1])\n\n# Calculando a acur\u00e1cia da Floresta Aleat\u00f3ria\nrfc_acc = accuracy_score(Y_valid, rfc_search_cv.predict(X_valid))\n\nprint('AUC -------- Random Forest: {:.4f}'.format(rfc_roc_auc))\nprint('Acur\u00e1cia --- Random Forest: {:.4f}'.format(rfc_acc))","c1962898":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\n\n# Calculando a AUC do XGBoost\nxgb_roc_auc = roc_auc_score(le.fit_transform(Y_valid), xgb_search_cv.predict_proba(X_valid)[:,1])\n\n# Calculando a acur\u00e1cia do XGBoost\nxgb_acc = accuracy_score(Y_valid, xgb_search_cv.predict(X_valid))\n\nprint('AUC -------- XGBoost: {:.4f}'.format(xgb_roc_auc))\nprint('Acur\u00e1cia --- XGBoost: {:.4f}'.format(xgb_acc))","de40305e":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\n\n# Calculando a AUC da Rede Neural\nmlp_roc_auc = roc_auc_score(le.fit_transform(Y_valid), mlp_search_cv.predict_proba(X_valid)[:,1])\n\n# Calculando a acur\u00e1cia da Rede Neural\nmlp_acc = accuracy_score(Y_valid, mlp_search_cv.predict(X_valid))\n\nprint('AUC -------- Rede Neural: {:.4f}'.format(mlp_roc_auc))\nprint('Acur\u00e1cia --- Rede Neural: {:.4f}'.format(mlp_acc))","84812d4b":"scores = [[\"Support Vector Classifier\",svc_roc_auc, svc_acc], [\"Random Forest\", rfc_roc_auc, rfc_acc], [\"XGBoost\",xgb_roc_auc, xgb_acc] , [\"Rede Neural\", mlp_roc_auc, mlp_acc]]\n\nscores_df = pd.DataFrame(scores, columns=[\"Modelo\", \"AUC\", \"Acur\u00e1cia\"])\nscores_df","6245c21f":"test_data = pd.read_csv(\"..\/input\/adult-pmr3508\/test_data.csv\", index_col=['Id'], na_values=\"?\")","1b0d8bc7":"X_test = test_data.drop(['fnlwgt', 'native.country'], axis=1) #tirar colunas n\u00e3o necess\u00e1rias\n\nX_test = preprocessador.transform(X_test) #pr\u00e9-processamento","321e80e7":"predictions = xgb_search_cv.predict(X_test)","2b7ea744":"predictions","e48a100d":"submission = pd.DataFrame()","3584d6d4":"submission[0] = test_data.index\nsubmission[1] = predictions\nsubmission.columns = ['Id','income']","9112bcad":"submission.head()","bc4c2c0d":"submission.to_csv('submission.csv',index = False)","ff72574f":"10. Submiss\u00e3o ","9b90aad0":"* Pr\u00e9 processamento dos valores n\u00famericos e os valores de capital gain capital loss","d93d4a8c":"2. Importando Dados","d2e31175":"# PMR3508 - Aprendizado de M\u00e1quina e Reconhecimento de Padr\u00f5es","5a0ed924":"Anteriormente, obtivemos o valor da AUC de cada classificador somente com a base de treino, agora iremos avali\u00e1-los com a dataset de valida\u00e7\u00e3o para termos resultados n\u00e3o enviesados. A AUC ajuda na compara\u00e7\u00e3o de diferentes classificadores, sendo poss\u00edvel resumir o desempenho de cada classificador numa \u00fanica medida.","1c949019":"Usamos o **Bayes Search** para otimiza\u00e7\u00e3o de hiperpar\u00e2metros:\n\nBayesSearchCV implementa um m\u00e9todo de \u201cajuste\u201d e um m\u00e9todo de \u201cpontua\u00e7\u00e3o\u201d.","c2ec7533":"Podemos observar agora alguns fen\u00f4menos, como que a maiora dos indiv\u00edduos do conjunto trabalha no setor privado e a chance de receber <50k \u00e9 similiar entre todas as workclass, menos na self-emp-inc e a federal government. Podemos ver que a presen\u00e7a feminina na categoria self-emp-inc \u00e9 de apenas metade, e na federal-gov representa consideravelmente menos que a metade, justificando a baixa quantidade de mulheres que recebem >50k.","125da0ce":"Vamos exportar nosso DataFrame:","21e0df81":"Podemos ver que temos 32560 perfis e 15 informa\u00e7\u00f5es sobre cada perfil, sendo uma delas o **income**, que \u00e9 a renda da pessao correspondente.","5f5e43f4":"* Dividimos os dados em tr\u00eas partes: os dados qualitativos (categ\u00f3ricos), os dados esparsos e os dados num\u00e9ricos.","87e67765":"Vamos analisar a rela\u00e7\u00e3o entre a v\u00e1rivel income com as outras, caso a caso.","7c8c2951":"6. An\u00e1lise Gr\u00e1fica - 1","00db7937":"Podemos ver a distribui\u00e7\u00e3o da v\u00e1riavel income, entre <50k e >50k.","797c2f69":"3. Tratando nossos dados faltantes","9afd97ce":"Enxergamos nossas 15 vari\u00e1veis diferentes, sendo 14 independentes. Essas vari\u00e1veis est\u00e3o divididas entre vari\u00e1veis num\u00e9ricas e vari\u00e1veis categ\u00f3ricas. \n\nAs num\u00e9ricas (int64) s\u00e3o representadas por n\u00fameros, e as categ\u00f3ricas (objects) est\u00e3o divididas em categorias.","0e90616c":"Come\u00e7o agora uma an\u00e1lise da rela\u00e7\u00e3o entre a renda e var\u00edaveis categ\u00f3ricas, que nos permitem ver certas desigualdades entre os grupos, principalmente a discrepancia entre g\u00eaneros. ","15740b22":"Podemos ver que os grupos que recebem as maiores renda em m\u00e9dia, s\u00e3o os n\u00edveis de educa\u00e7\u00e3o Master, Prof-school e Doctorate. \u00c9 poss\u00edvel ver novamente o nivel de desigualdade de g\u00eanero na educa\u00e7\u00e3o.","c3a0d3f0":"Analisando **race** podemos ver que a maior parte do conjunto se idenfica como white e as maiores m\u00e9dias de renda s\u00e3o dos grupos white e Asian-Pac-Islander. O \u00fanico grupo racial onde as mulheres apresentam participa\u00e7\u00e3o de ordem de grandeza aproximada dos homens \u00e9 o Amer-indian-Eskimo, que \u00e9 justamente um dos grupos com menor participa\u00e7\u00e3o na composi\u00e7\u00e3o do grupo de pessoas que recebem >50k, enquanto nos grupos onde h\u00e1 maior discrep\u00e2ncia de participa\u00e7\u00e3o feminina s\u00e3o justamente os com as maiores rendas.","e16f1e06":"O **gradient boosting** \u00e9 uma t\u00e9cnica de aprendizado de m\u00e1quina para problemas de classifica\u00e7\u00e3o, que produz um modelo de previs\u00e3o na forma de um ensemble de modelos de previs\u00e3o fracos, geralmente \u00e1rvores de decis\u00e3o . Ela constr\u00f3i o modelo em etapas, como outros m\u00e9todos de boosting, e os generaliza, permitindo a otimiza\u00e7\u00e3o de uma fun\u00e7\u00e3o de perda diferenci\u00e1vel arbitr\u00e1ria. \n\nO XGBoost \u00e9 uma das implementa\u00e7\u00f5es do conceito Gradient Boosting, mas o que torna o XGBoost \u00fanico \u00e9 que ele usa uma formaliza\u00e7\u00e3o de modelo mais regularizada para controlar o sobreajuste, o que lhe d\u00e1 melhor desempenho.","c3978764":"4. Conhecendo nossos dados","451835fc":"* Agora removo as vari\u00e1veis considerados por mim sem tanta relev\u00e2ncia para o modelo.","50ab26cb":"$Random$ $Forest$","395ef262":"**Resultados**","b07c34c6":"* Vamos primeiro eliminar alguns dados duplicados","2a9803fc":"$Random$ $Forest$\n","511f6649":"As \u00e1rvores de decis\u00e3o representam uma das formas mais simplificadas de um sistema de suporte \u00e0 decis\u00e3o. \u00c9 um m\u00e9todo estat\u00edstico, de aprendizagem supervisionada, podendo ser utilizado em problemas de classifica\u00e7\u00e3o e na realiza\u00e7\u00e3o de previs\u00f5es.\n\nA partir de um conjunto de dados existente, o m\u00e9todo cria uma representa\u00e7\u00e3o do conhecimento ali embutido, em formato de \u00e1rvore. Os n\u00f3s da \u00e1rvore s\u00e3o criados a partir das caracter\u00edsticas (features) do conjunto de dados. O random significa aleat\u00f3rio, e denota o comportamento do algoritmo ao selecionar subconjuntos de features e montar mini \u00e1rvores de decis\u00e3o","3655499a":"$Rede$ $Neural$","13ec9d34":"Por este gr\u00e1fico, vemos que as maiores m\u00e9dias de renda se concentram em alguns pa\u00edses,como Iran, Fran\u00e7a,\u00cdndia, por\u00e9m estes tamb\u00e9m possuem grandes linhas pretas. Essas linhas pretas indicam uma varri\u00e2ncia muito grande estes esses valores que estamos utilizando, o que indica que a maioria dos pa\u00edses tem uma diferen\u00e7a muito grande de renda entre os grupos, o que representa uma provavel desigualdade de renda alta.\n\nPara vermos melhor a distribui\u00e7\u00e3o das pessoas pelo pa\u00edses para entender melhor o que ocorre:","12f6b582":"5. Trabalhando com a vari\u00e1vel \"income\"","a92f2d01":"Vamos ver nossos resultados obtidos:","d7eec109":"6. Pr\u00e9 Processamento e Limpeza","044970f5":"Agora pegamos um resumo estat\u00edstico do nosso DataFrame, que fornece a contagem, a m\u00e9dia, desvio padr\u00e3o (STD), m\u00ednimo, quartis e m\u00e1ximo, que s\u00e3o informa\u00e7\u00f5es podem nos auxiliar a identificar frequ\u00eancia dos dados e outliers.","44bfd047":"$Support$ $Vector$ $Classifier$","63c0104a":"* Agora juntamos as pipelines (sequ\u00eancia de transformadores)","d315c9cb":"Uma rede neural \u00e9 um modelo computacional inspirado pelo sistema nervoso central e \u00e9 capaz de realizar o aprendizado de m\u00e1quina bem como o reconhecimento de padr\u00f5es.\n\n\"Um neur\u00f4nio de uma rede neural \u00e9 um componente que calcula a soma ponderada de v\u00e1rios inputs, aplica uma fun\u00e7\u00e3o e passa o resultado adiante. Quando utilizamos v\u00e1rios neur\u00f4nios em paralelo temos uma rede neural. N\u00f3s podemos pensar em cada neur\u00f4nio como recebendo sinais das vari\u00e1veis dos inputs e passando adiante uma vers\u00e3o ponderada e tratada desse sinal. Esses neur\u00f4nios em paralelo formam uma camada oculta da rede neural. N\u00f3s podemos tratar o output de cada neur\u00f4nio como uma vari\u00e1vel do input de uma outra camada oculta. Assim, podemos empilhar camadas ocultas e produzir uma rede neural profunda.\"\n\n[Introdu\u00e7\u00e3o \u00e0s Redes Neurais Artificiais](http:\/\/matheusfacure.github.io\/2017\/03\/05\/ann-intro\/)","8b3caaec":"Podemos ver o resultado obtido:","7bdd6e6f":"Para conseguirmos relacionar as vari\u00e1veis com a v\u00e1riavel target income (renda), transformamos os valores de renda em v\u00e1riaveis discretas 0 (<50k) e 1(>50k).","76132fa6":"A v\u00e1riavel income indica a renda da pessoa e \u00e9 o Target do nosso dataset.","9bba2d51":"* Separamos a vari\u00e1vel target renda income do resto","968cea29":"Agora vamos a nossa submiss\u00e3o:","b34dc9f6":"Durante a analise superficial dos dados, alguns valores dessas classes ja chamavam aten\u00e7\u00e3o devido sua enorme vari\u00e2ncia e possuindo at\u00e9 alguns outliers. Podemos ver atrav\u00e9s dos graf\u00edcos a confiram\u00e7\u00e3o das dedu\u00e7\u00f5es anteriores sobre a dispers\u00e3o dos dados e a presen\u00e7a desses dados \u00e1tipicos muito afastados dos demais, principalmente na vari\u00e1vel capital gain.","38ff9e35":"Pelo AUC do XGBoost ser o maior e a acur\u00e1cia muito perto da melhor, ent\u00e3o escolhi est\u00e1 para usar para o dataset de teste.","39f0f0fe":"No geral, pessoas casadas atualmente, s\u00e3o as que possuem maior m\u00e9dia de renda. Sendo Married-civ-spouse bem distribuido entre 0 (<50k) e 1(>50k). Mas esses dados n\u00e3o parecem revelar tanto, j\u00e1 que temos Married-AF-spouse com uma m\u00e9dia alta mas uma vari\u00e2nica alta tamb\u00e9m.","99d1d793":"Analisaremos agora um gr\u00e1fico de correla\u00e7\u00e3o entre as vari\u00e1veis quantitativas e o income","ab96426d":"$Extreme$ $Gradient$ $Boosting$","7ea079c3":"Na vers\u00e3o 1 hav\u00edamos utilizado um classificador KNN, enquanto agora deveremos comparar outros quatro classificadores diferentes e compara-los.","53ed838c":"$Support$ $Vector$ $Classifier$","b8ca9e43":"Aluno: J\u00falia Mello de Almeida","181053a6":"8.Predi\u00e7\u00e3o","127a2f8d":"\u00c9 interessante tamb\u00e9m saber o tipo de vari\u00e1vel de cada uma de nossas vari\u00e1veis.","5ef35d6d":"Vamos escolher o melhor classificador, no caso o escolhido vai ser o \ud835\udc38\ud835\udc65\ud835\udc61\ud835\udc5f\ud835\udc52\ud835\udc5a\ud835\udc52   \ud835\udc3a\ud835\udc5f\ud835\udc4e\ud835\udc51\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc61   \ud835\udc35\ud835\udc5c\ud835\udc5c\ud835\udc60\ud835\udc61\ud835\udc56\ud835\udc5b\ud835\udc54. Agora vamos usar o dataset de teste para ver a acur\u00e1cia da nossa escolha.","c1b5bd2e":"Agora podemos seguir adiante e analisar nossos dados.","e5361812":"$Rede$ $Neural$","0cd93efd":"* Pr\u00e9 processamento das dos valores qualitativos","90366930":"7. Analise Gr\u00e1fica - 2","196d67c5":"Um classificador de vetores de suporte \u00e9 um conceito para um conjunto de m\u00e9todos de aprendizado supervisionado que analisam os dados e reconhecem padr\u00f5es, usado para classifica\u00e7\u00e3o e an\u00e1lise de regress\u00e3o. ","8ec76a73":"Pela primeira vez, o n\u00famero de mulheres de um grupo supera o de homens, e \u00e9 justamente em priv-houve-serv, que possue a menor m\u00e9dia de renda entre todas as ocupa\u00e7\u00f5es, e ainda pelo segunda gr\u00e1fico n\u00e3o vemos amostras de pessoas dessa classe que recebem >50k.\nPodemos ver que as for\u00e7as armadas tem um flutua\u00e7\u00e3o muito grande de renda recebida, enquanto exec-managerial possui a maior renda m\u00e9dia disparada e parece ter n\u00fameros consistentes. ","4bd4c973":"Primeiro importamos as bibliotecas do phyton que ser\u00e3o utilizadas para a anal\u00edse de dados, forma\u00e7\u00e3o de gr\u00e1ficos e para o nosso treino.","6572b88e":"1. Importando Bibliotecas","6959bfc0":"9. Compara\u00e7\u00e3o ","fc70bc6b":"Para os tr\u00eas casos, devido a alta frequ\u00eancia da moda, escolho usar a moda como substituinte desses dados","8a0e5f65":"$Extreme$ $Gradient$ $Boosting$","df42e73b":"Aqui \u00e9 poss\u00edvel ver melhor que, enquanto o Estados Unidos tem 29752 var\u00edaveis, o M\u00e9xico, o segundo maior em quantidade, tem apenas 643 disso, que \u00e9 2% do dataframe inteiro. E ainda existem m\u00e1ises como Holand-Netherlands que tem apenas 1, o que mostra que a an\u00e1lise por meio dessa categoria acaba sendo tendenciosa e deixando os dados sem muita utilidade.","2e83e4af":"A matriz de correla\u00e7\u00e3o mostra os valores de correla\u00e7\u00e3o de Pearson, que medem o grau de rela\u00e7\u00e3o entre cada par de vari\u00e1veis. Os valores de correla\u00e7\u00e3o podem cair entre -1 e +1. Se as duas vari\u00e1veis tendem a aumentar e diminuir juntas, o valor de correla\u00e7\u00e3o \u00e9 positivo. Se uma vari\u00e1vel aumenta enquanto a outra vari\u00e1vel diminui, o valor de correla\u00e7\u00e3o \u00e9 negativo.\n\nUm valor de correla\u00e7\u00e3o alto e positivo indica que as vari\u00e1veis medem a mesma caracter\u00edstica. Se os itens n\u00e3o est\u00e3o altamente correlacionados, os itens podem medir diferentes caracter\u00edsticas ou podem n\u00e3o estar claramente definidos.\n\n\u00c9 possivel notar como a rela\u00e7\u00e3o entre as vari\u00e1veis e fnlwgt \u00e9 negativa ou muito baixa, mostrando sua baixa rela\u00e7\u00e3o com nosso target, logo, esta var\u00edavel n\u00e3o agrega muito para nosso modelo de predi\u00e7\u00e3o.\n\n"}}