{"cell_type":{"038793f8":"code","1c8a08ec":"code","7062bb89":"code","aa43f29a":"code","47a6fae7":"code","73ba4bcd":"code","3c74022d":"code","1a8f4599":"code","3a2ad139":"code","23bda95a":"code","f3b09c11":"code","1874327a":"code","a4d421fc":"code","0cd3baa8":"code","d875a312":"code","0e4aef9e":"code","ddfedcae":"code","27ead99d":"code","faf90347":"code","9807baee":"code","7d08d2dd":"code","6385325b":"code","c7c98e15":"code","25407168":"code","4a2d0d15":"code","15384835":"code","84a5fa8b":"code","8955274c":"code","8a2464ce":"markdown","f228e99e":"markdown","9a048012":"markdown"},"source":{"038793f8":"!pip install num2words","1c8a08ec":"import tensorflow as tf\nfrom keras import layers\nimport keras\nimport matplotlib.pyplot as plt\n\nimport nltk, re\nimport os\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords \nfrom nltk.stem import PorterStemmer       \nfrom num2words import num2words\n\nfrom tqdm import tqdm\nimport pandas as pd\nimport random\nfrom collections import Counter\nimport tensorflow_hub as hub\n\nimport numpy as np\nfrom sklearn.model_selection import train_test_split   \n\nplt.style.use('fivethirtyeight') \n%matplotlib inline","7062bb89":"def remove_url_and_symbols(comment):\n    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\\/\\\/\\S+)\",\" \",comment).split())\n\ndef lower_comment(comment):\n    return comment.lower()\n\n\ndef tokenize_comment(comment):\n    return comment.split()\n\ndef remove_stop_words(comment_tokenized):\n    stopwords_english = stopwords.words('english')\n    comment_tokenized_cleaned = []\n    for word in comment_tokenized:\n        if(word not in stopwords_english):\n            comment_tokenized_cleaned.append(word)\n    return comment_tokenized_cleaned\n\ndef convert_numbers_to_words(comment_tokenized):\n    comment_tokenized_cleaned = []\n    for word in comment_tokenized:  # Go through every word in your tokens list\n        try:\n            comment_tokenized_cleaned.append(num2words(word))\n        except:\n            comment_tokenized_cleaned.append(word)\n    return comment_tokenized_cleaned\n\ndef remove_ponctuation_from_tokenized(comment_tokenized):\n    comment_tokenized_cleaned = []\n    for word in comment_tokenized:\n        comment_tokenized_cleaned.append(remove_url_and_symbols(word))\n    \n    return comment_tokenized_cleaned\n\ndef stem_words(comment_tokenized):\n    stemmer = PorterStemmer()\n    comment_tokenized_cleaned = []\n    for word in comment_tokenized:  # Go through every word in your tokens list\n        comment_tokenized_cleaned.append(stemmer.stem(word))   # stemming word\n    return comment_tokenized_cleaned","aa43f29a":"def get_num_words_per_sample(sample_texts):\n    \"\"\"Gets the median number of words per sample given corpus.\n    # Arguments\n        sample_texts: list, sample texts.\n    # Returns\n        int, median number of words per sample.\n    \"\"\"\n    num_words = [len(s) for s in sample_texts]\n    return np.median(num_words)","47a6fae7":"def get_optimizer(batch_size_var, X_train):\n    # Many models train better if you gradually reduce the learning rate during training. \n    # Use optimizers.schedules to reduce the learning rate over time\n    N_TRAIN = X_train.shape[0]\n    STEPS_PER_EPOCH = N_TRAIN\/\/batch_size_var\n\n    lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(0.001, \n                                                                 decay_steps=STEPS_PER_EPOCH*1000, \n                                                                 decay_rate=1, \n                                                                 staircase=False)\n    \n    return tf.keras.optimizers.Adam(lr_schedule)\n\ndef compile_model(model, loss_func, batch_size_var, epochs_var, X_train, Y_train, X_test, Y_test):\n  model.compile(optimizer=get_optimizer(batch_size_var, X_train), \n                loss=loss_func,\n                metrics=['accuracy'])\n\n  history = model.fit(\n      X_train,\n      Y_train,\n      batch_size=batch_size_var,\n      epochs=epochs_var,\n      # We pass some validation for\n      # monitoring validation loss and metrics\n      # at the end of each epoch\n      validation_data=(X_test, Y_test),\n  )\n\n\n  history_dict = history.history\n  history_dict.keys()\n\n  acc = history_dict['accuracy']\n  val_acc = history_dict['val_accuracy']\n  loss = history_dict['loss']\n  val_loss = history_dict['val_loss']\n\n  epochs = range(1, len(acc) + 1)\n\n  # \"bo\" is for \"blue dot\"\n  plt.plot(epochs, loss, 'bo', label='Training loss')\n  # b is for \"solid blue line\"\n  plt.plot(epochs, val_loss, 'b', label='Validation loss')\n  plt.title('Training and validation loss')\n  plt.xlabel('Epochs')\n  plt.ylabel('Loss')\n  plt.legend()\n\n  plt.show()\n\n  plt.plot(epochs, acc, 'bo', label='Training acc')\n  plt.plot(epochs, val_acc, 'b', label='Validation acc')\n  plt.title('Training and validation accuracy')\n  plt.xlabel('Epochs')\n  plt.ylabel('Accuracy')\n  plt.legend(loc='lower right')\n\n  plt.show()\n  return model","73ba4bcd":"read_data = pd.read_csv(\"..\/input\/twitter-user-gender-classification\/gender-classifier-DFE-791531.csv\", encoding = \"ISO-8859-1\")\ndata = pd.DataFrame()\ndata['name'] = list(read_data['name'])\ndata['text'] = list(read_data['text'])\ndata['gender'] = list(read_data['gender'])","3c74022d":"data","1a8f4599":"list(data['text'])[0]","3a2ad139":"# Clean the data.\ncleaned_comments = []\nfor i in tqdm(range(len(list(data['text'])))):\n    text = remove_url_and_symbols(list(data['text'])[i])\n    text = lower_comment(text)\n    text_tokenized = tokenize_comment(text)\n    text_tokenized = convert_numbers_to_words(text_tokenized)\n    text_tokenized = remove_stop_words(text_tokenized)\n    text_tokenized = remove_ponctuation_from_tokenized(text_tokenized)\n    text_tokenized = stem_words(text_tokenized)\n    \n    cleaned_comments.append(text_tokenized)","23bda95a":"# Clean the data.\ncleaned_names = []\nfor i in tqdm(range(len(list(data['name'])))):\n    name = lower_comment(list(data['name'])[i])\n    cleaned_names.append(name)","f3b09c11":"# Clean the data.\ncleaned_gender = []\nfor i in tqdm(range(len(list(data['gender'])))):\n    if(list(data['gender'])[i]=='male'):\n      cleaned_gender.append(0)\n    else:\n      cleaned_gender.append(1)","1874327a":"get_num_words_per_sample(cleaned_comments)   # Median number of words per sample.","a4d421fc":"vocabulary_size = 20000\ntext_length = 8","0cd3baa8":"len(list(data['name'])) \/ get_num_words_per_sample(cleaned_comments)","d875a312":"cleaned_comments_sentance = []\nfor sentance in cleaned_comments:\n    ch = \"\"\n    for word in sentance:\n        ch = ch + word + \" \"\n    cleaned_comments_sentance.append(ch)","0e4aef9e":"# num_words: The maximum number of words to keep, based on word frequency. Only the most common num_words-1 words will be kept.\ntokenizer_text = tf.keras.preprocessing.text.Tokenizer(\n    num_words=vocabulary_size, \n    filters='!\"#$%&()*+,-.\/:;<=>?@[\\\\]^_`{|}~\\t\\n0123456789', \n    lower=True, \n    split=\" \", \n    char_level=False\n)\ntokenizer_text.fit_on_texts(cleaned_comments_sentance)\nsequences_tokenizer_text = tokenizer_text.texts_to_sequences(cleaned_comments_sentance)","ddfedcae":"# num_words: The maximum number of words to keep, based on word frequency. Only the most common num_words-1 words will be kept.\ntokenizer_name = tf.keras.preprocessing.text.Tokenizer(\n    num_words=vocabulary_size, \n    filters='!\"#$%&()*+,-.\/:;<=>?@[\\\\]^_`{|}~\\t\\n0123456789', \n    lower=True, \n    split=\" \", \n    char_level=False\n)\ntokenizer_name.fit_on_texts(cleaned_names)\nsequences_tokenizer_name = tokenizer_name.texts_to_sequences(cleaned_names)","27ead99d":"for i in tqdm(range(len(sequences_tokenizer_text))):\n  if(len(sequences_tokenizer_text[i])>8):\n    sequences_tokenizer_text[i] = sequences_tokenizer_text[i][:8]\n\n  if(len(sequences_tokenizer_text[i])<8):\n    for j in range(8-len(sequences_tokenizer_text[i])):\n      sequences_tokenizer_text[i].append(0)\n  ","faf90347":"for i in tqdm(range(len(sequences_tokenizer_name))):\n  if(len(sequences_tokenizer_name[i])>8):\n    sequences_tokenizer_name[i] = sequences_tokenizer_name[i][:8]\n\n  if(len(sequences_tokenizer_name[i])<8):\n    for j in range(8-len(sequences_tokenizer_name[i])):\n      sequences_tokenizer_name[i].append(0)","9807baee":"input_layer_text = keras.Input(shape=(None,), name=\"Input_text\", dtype=tf.int64)  \nembedding_layer_text = layers.Embedding(vocabulary_size, 64)(input_layer_text)\nx_text = layers.LSTM(64, activation=\"relu\", dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(embedding_layer_text)\nx_text = layers.LSTM(64, activation=\"relu\", dropout=0.2, recurrent_dropout=0.2)(x_text)\n\ninput_layer_name = keras.Input(shape=(None,), name=\"Input_name\", dtype=tf.int64)  \nembedding_layer_name = layers.Embedding(vocabulary_size, 64)(input_layer_name)\nx_name = layers.LSTM(64, activation=\"relu\", dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(embedding_layer_name)\nx_name = layers.LSTM(64, activation=\"relu\", dropout=0.2, recurrent_dropout=0.2)(x_name)\n\nx = layers.concatenate([x_text, x_name])\n\noutput_layer = tf.keras.layers.Dense(2, activation='softmax')(x)\n\nmodel = keras.Model(inputs=[input_layer_text, input_layer_name], outputs=output_layer, name=\"model\")\n\nmodel.summary()\nkeras.utils.plot_model(model, \"model.png\", show_shapes=True)","7d08d2dd":"X_text = pd.DataFrame(sequences_tokenizer_text)\nX_name = pd.DataFrame(sequences_tokenizer_name)\nY = pd.DataFrame(cleaned_gender)\n\nX_train_text = X_text[:15000]\nX_test_text = X_text[15000:]\nX_train_name = X_name[:15000]\nX_test_name = X_name[15000:]\nY_train = Y[:15000]\nY_test = Y[15000:]","6385325b":"Y_train = tf.keras.utils.to_categorical(Y_train, num_classes=2)\nY_test = tf.keras.utils.to_categorical(Y_test, num_classes=2)","c7c98e15":"def get_optimizer(batch_size_var, X_train):\n    # Many models train better if you gradually reduce the learning rate during training. \n    # Use optimizers.schedules to reduce the learning rate over time\n    N_TRAIN = X_train.shape[0]\n    STEPS_PER_EPOCH = N_TRAIN\/\/batch_size_var\n\n    lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(0.001, \n                                                                 decay_steps=STEPS_PER_EPOCH*1000, \n                                                                 decay_rate=1, \n                                                                 staircase=False)\n    \n    return tf.keras.optimizers.Adam(lr_schedule)\n\ndef compile_model(model, loss_func, batch_size_var, epochs_var, X_train1, X_train2, Y_train, X_test1, X_test2, Y_test):\n  model.compile(optimizer=get_optimizer(batch_size_var, X_train1), \n                loss=loss_func,\n                metrics=['accuracy'])\n\n\n  history = model.fit(\n      x=[X_train1, X_train2], y=Y_train,\n      validation_data=([X_test1, X_test2], Y_test),\n      batch_size=batch_size_var,\n      epochs=epochs_var\n    )\n\n\n  history_dict = history.history\n  history_dict.keys()\n\n  acc = history_dict['accuracy']\n  val_acc = history_dict['val_accuracy']\n  loss = history_dict['loss']\n  val_loss = history_dict['val_loss']\n\n  epochs = range(1, len(acc) + 1)\n\n  # \"bo\" is for \"blue dot\"\n  plt.plot(epochs, loss, 'bo', label='Training loss')\n  # b is for \"solid blue line\"\n  plt.plot(epochs, val_loss, 'b', label='Validation loss')\n  plt.title('Training and validation loss')\n  plt.xlabel('Epochs')\n  plt.ylabel('Loss')\n  plt.legend()\n\n  plt.show()\n\n  plt.plot(epochs, acc, 'bo', label='Training acc')\n  plt.plot(epochs, val_acc, 'b', label='Validation acc')\n  plt.title('Training and validation accuracy')\n  plt.xlabel('Epochs')\n  plt.ylabel('Accuracy')\n  plt.legend(loc='lower right')\n\n  plt.show()\n  return model","25407168":"model = compile_model(model, 'categorical_crossentropy', 40, 10, X_train_text, X_train_name, Y_train, X_test_text, X_test_name, Y_test)","4a2d0d15":"X_text2 = pd.DataFrame(cleaned_comments_sentance)\nX_name2 = pd.DataFrame(cleaned_names)\nY2 = pd.DataFrame(cleaned_gender)\n\nX_train_text2 = X_text2[:15000]\nX_test_text2 = X_text2[15000:]\nX_train_name2 = X_name2[:15000]\nX_test_name2 = X_name2[15000:]\nY_train2 = Y2[:15000]\nY_test2 = Y2[15000:]","15384835":"Y_train2 = tf.keras.utils.to_categorical(Y_train2, num_classes=2)\nY_test2 = tf.keras.utils.to_categorical(Y_test2, num_classes=2)","84a5fa8b":"embedding = \"https:\/\/tfhub.dev\/google\/tf2-preview\/gnews-swivel-20dim\/1\"\ninput_layer_text = keras.Input(shape=(), name=\"Input_text\", dtype=tf.string)  \n\nhub_layer_text = hub.KerasLayer(embedding, trainable=True, name='embedding_text')(input_layer_text)\nx_text = tf.expand_dims(hub_layer_text, 1)\nx_text = layers.LSTM(64, activation=\"relu\", dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(x_text)\nx_text = layers.LSTM(64, activation=\"relu\", dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(x_text)\nx_text = layers.LSTM(64, activation=\"relu\", dropout=0.2, recurrent_dropout=0.2)(x_text)\n\ninput_layer_name = keras.Input(shape=(), name=\"Input_name\", dtype=tf.string)  \nhub_layer_name = hub.KerasLayer(embedding, trainable=True, name='embedding_name')(input_layer_name)\nx_name = tf.expand_dims(hub_layer_name, 1)\nx_name = layers.LSTM(64, activation=\"relu\", dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(x_name)\nx_name = layers.LSTM(64, activation=\"relu\", dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(x_name)\nx_name = layers.LSTM(64, activation=\"relu\", dropout=0.2, recurrent_dropout=0.2)(x_name)\n\nx = layers.concatenate([x_text, x_name])\n\noutput_layer = tf.keras.layers.Dense(2, activation='softmax')(x)\n\nsecond_model = keras.Model(inputs=[input_layer_text, input_layer_name], outputs=output_layer, name=\"model\")\n\nsecond_model.summary()\nkeras.utils.plot_model(second_model, \"model.png\", show_shapes=True)","8955274c":"second_model = compile_model(second_model, 'categorical_crossentropy', 40, 10, X_train_text2, X_train_name2, Y_train2, X_test_text2, X_test_name2, Y_test2)","8a2464ce":"## Second model","f228e99e":"## First model","9a048012":"**In this notebook I will predict the gender based on the tweet and the username.**"}}