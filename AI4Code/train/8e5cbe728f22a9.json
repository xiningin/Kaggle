{"cell_type":{"588a874e":"code","39ec2aa3":"code","2eea78a2":"code","7aef5468":"code","c979d643":"code","7a663d83":"code","fcb6ccc9":"code","b1a07ba5":"code","9d724978":"code","ce999f8e":"code","be43486c":"code","ca3df37b":"code","860d199e":"code","94e2fcac":"code","6e0715a5":"code","b6fa8386":"code","244001d8":"code","b5ca199f":"code","359289d8":"code","c90f9bc0":"code","28e950a7":"code","d615a929":"code","92a3dd4e":"code","5f6192bf":"code","64324b3c":"code","f910e30c":"code","82179c51":"code","e7e1c6a4":"code","eeefc9f3":"code","2961386b":"code","dc8efcd1":"code","ecfe1189":"code","d02b7224":"code","d61e1260":"code","b87a721b":"code","b5595043":"code","84da2d49":"code","b68bf501":"markdown","471533fa":"markdown","7a30f9fd":"markdown","8d1575c0":"markdown","95c10162":"markdown","46488b98":"markdown","5eede6e4":"markdown","f7bfc6e0":"markdown","4cc1d0b1":"markdown","c87f078c":"markdown","7ed6d2f4":"markdown","b219e8b2":"markdown","a3e5f435":"markdown","cda102dd":"markdown","97e7b231":"markdown","2f684219":"markdown","bf9cd667":"markdown","4f4fee18":"markdown","c59f461a":"markdown","4a43a9cc":"markdown"},"source":{"588a874e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport regex\nimport nltk\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.stem import WordNetLemmatizer,PorterStemmer\nfrom nltk.corpus import stopwords\nimport re\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score\nfrom PIL import Image\nimport plotly.express as px\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","39ec2aa3":"train_data=pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/train.csv')\ntest_data=pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/test.csv')\nsample_submission=pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/sample_submission.csv')","2eea78a2":"train_data.head()","7aef5468":"print('The train set contains {0} rows and {1} columns '.format(train_data.shape[0],train_data.shape[1]))","c979d643":"def count_target_plot(data,target):\n    plt.figure(figsize=(8,8))\n    ax=sns.countplot(data=data,x=data[target],order=data[target].value_counts().index)\n    plt.xlabel('Target Variable- Sentiment')\n    plt.ylabel('Count of tweets')\n    plt.title('Count of Sentiment tweets')\n    total = len(data)\n    for p in ax.patches:\n            ax.annotate('{:.1f}%'.format(100*p.get_height()\/total), (p.get_x()+0.1, p.get_height()+5))\n","7a663d83":"count_target_plot(train_data,'sentiment')","fcb6ccc9":"count_target_plot(test_data,'sentiment')","b1a07ba5":"train_data.tail()","9d724978":"lemmatizer = WordNetLemmatizer()\nstemmer = PorterStemmer() \ndef preprocess(sentence):\n    sentence=str(sentence)\n    sentence = sentence.lower()\n    sentence=sentence.replace('{html}',\"\") \n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr, '', sentence)\n    rem_url=re.sub(r'http\\S+', '',cleantext)\n    rem_num = re.sub('[0-9]+', '', rem_url)\n    tokenizer = RegexpTokenizer(r'\\w+')\n    tokens = tokenizer.tokenize(rem_num)  \n    filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')]\n    stem_words=[stemmer.stem(w) for w in filtered_words]\n    lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\n    return \" \".join(filtered_words)","ce999f8e":"train_data['text']=train_data['text'].map(lambda s:preprocess(s))\ntrain_data['selected_text']=train_data['selected_text'].map(lambda s:preprocess(s))","be43486c":"test_data.head()","ca3df37b":"test_data['text']=test_data['text'].map(lambda s:preprocess(s))","860d199e":"train_data.head()","94e2fcac":"print('Checking null values for train data')\nprint(train_data.isnull().sum())\nprint('Checking null values for train data')\nprint(test_data.isnull().sum())","6e0715a5":"# https:\/\/medium.com\/@cristhianboujon\/how-to-list-the-most-common-words-from-text-corpus-using-scikit-learn-dad4d0cab41d\n# Interesting article\ndef get_top_n_words(corpus, n=None):\n    \"\"\"\n    List the top n words in a vocabulary according to occurrence in a text corpus.\n    \n    get_top_n_words([\"I love Python\", \"Python is a language programming\", \"Hello world\", \"I love the world\"]) -> \n    [('python', 2),\n     ('world', 2),\n     ('love', 2),\n     ('hello', 1),\n     ('is', 1),\n     ('programming', 1),\n     ('the', 1),\n     ('language', 1)]\n    \"\"\"\n    vec = CountVectorizer().fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in     vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","b6fa8386":"top_40_words=pd.DataFrame(get_top_n_words(train_data['text'],40),columns=['words','count'])","244001d8":"def count_words_barplot(data_set):\n    plt.figure(figsize=(20,20))\n    ax = sns.barplot(y=\"count\", x=\"words\", data=data_set)\n    plt.xlabel('Count of words')\n    plt.ylabel('Words')\n    plt.title('Count of Top 30 words used in the tweet')","b5ca199f":"count_words_barplot(top_40_words)","359289d8":"##Dividing on the basis of tweets\nneutral_set=train_data[train_data['sentiment']=='neutral'].reset_index()\npositive_set=train_data[train_data['sentiment']=='positive'].reset_index()\nnegative_set=train_data[train_data['sentiment']=='negative'].reset_index()","c90f9bc0":"top_30_words_neutral=pd.DataFrame(get_top_n_words(neutral_set['text'],30),columns=['words','count'])\ntop_30_words_positive=pd.DataFrame(get_top_n_words(positive_set['text'],30),columns=['words','count'])\ntop_30_words_negative=pd.DataFrame(get_top_n_words(negative_set['text'],30),columns=['words','count'])","28e950a7":"count_words_barplot(top_30_words_neutral)","d615a929":"fig = px.treemap(top_30_words_positive,path=['words'], values='count',title='List of top 30 words that affect positive tweets')\nfig.show()","92a3dd4e":"top_300_words_negative=pd.DataFrame(get_top_n_words(negative_set['text'],300),columns=['words','count'])","5f6192bf":"fig = px.sunburst(top_300_words_negative,path=['words'], values='count',color='words',title='Top negative words that are present in the data')\nfig.show()","64324b3c":"bigram_pos=pd.DataFrame()\nbigram_neg=pd.DataFrame()\nbigram_neu=pd.DataFrame()","f910e30c":"#train_data_len=len(train_data)\n\nneutral_set_len=len(neutral_set)\npositive_set_len=len(positive_set)\nnegative_set_len=len(negative_set)","82179c51":"for index in range(0,neutral_set_len):\n    bigrm = pd.DataFrame(nltk.bigrams(neutral_set['text'][index].split()))\n    bigram_neu=pd.concat([bigram_neu,bigrm])\n\n#bigram_neu.head()","e7e1c6a4":"for index in range(0,positive_set_len):\n    bigrm = pd.DataFrame(nltk.bigrams(positive_set['text'][index].split()))\n    bigram_pos=pd.concat([bigram_pos,bigrm])\n\n#bigram_pos.head()","eeefc9f3":"for index in range(0,negative_set_len):\n    bigrm = pd.DataFrame(nltk.bigrams(negative_set['text'][index].split()))\n    bigram_neg=pd.concat([bigram_neg,bigrm])\n\n#bigram_neg.head()","2961386b":"#bigram_neu['bigram']=bigram_neu[['0','1']].apply(lambda x:' '.join(x),axis=1)\n#bigram_pos=bigram_pos.reset_index()\n#bigram_neg=bigram_neg.reset_index()\nbigram_neu=bigram_neu.rename(columns={0:'first',1:'second'}).reset_index()\nbigram_pos=bigram_pos.rename(columns={0:'first',1:'second'}).reset_index()\nbigram_neg=bigram_neg.rename(columns={0:'first',1:'second'}).reset_index()","dc8efcd1":"bigram_neu['combined'] = bigram_neu[['first', 'second']].apply(lambda x: ' '.join(x), axis = 1) \nbigram_pos['combined'] = bigram_pos[['first', 'second']].apply(lambda x: ' '.join(x), axis = 1) \nbigram_neg['combined'] = bigram_neg[['first', 'second']].apply(lambda x: ' '.join(x), axis = 1) \n","ecfe1189":"bigram_neu_count=pd.DataFrame(bigram_neu['combined'].value_counts()).reset_index()\nbigram_pos_count=pd.DataFrame(bigram_pos['combined'].value_counts()).reset_index()\nbigram_neg_count=pd.DataFrame(bigram_neg['combined'].value_counts()).reset_index()","d02b7224":"fig = px.scatter(bigram_neu_count[:30], x=\"index\", y=\"combined\",color='combined',\n\t         size=\"combined\", size_max=20,title='Neutral count of top bigram words')\nfig.show()","d61e1260":"fig = px.bar(bigram_pos_count[:30], x=\"combined\", y=\"index\", color='combined',orientation='h',title='Positive count words top 30')\nfig.show()","b87a721b":"fig = px.line(bigram_neg_count[:30], x=\"index\", y=\"combined\",title='Negative count of top 30 bigram words')\nfig.show()","b5595043":"wordcloud = WordCloud(\n                          background_color='white',\n                          max_words=100,\n                          max_font_size=80, \n                          random_state=42,\n    collocations=False,\n    colormap=\"Oranges_r\"\n                         ).generate(' '.join(top_40_words['words']))\n#.join(text2['Crime Type']))\n\nplt.figure(figsize=(10,10))\nplt.title('Major keywords for tweets', fontsize=10)\nplt.imshow(wordcloud)\n\nplt.axis('off')\nplt.show()","84da2d49":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))\n\nA= \"Going lucky or going hard is the only thing you could do\"\nB=\"going is important or only thing is needed\"\nC=\"It is going lucky or going hard is only needed thing you\"\n\nprint(jaccard(A,B))\nprint(jaccard(A,C))","b68bf501":"## Building Unigrams","471533fa":"Above is a function that I had found in one of the stack overflow questions.The above function takes in a sentence and processes those functions in the following way :\n    \n    1.Converts sentence into strings\n    2.Converts them into lower case(Since in here language is just data we don't need upper case or lower case)\n    3.Removes any html related tags\n    4.Removes any expressions like . * ?\n    5.Removes http related text as these are web related links \n    6.Removes numericals\n    7.Converts sentences into tokens and removes stop words\n    8.Stemmer means stems the filter words\n    9.Lemmatization means getting the originality of those words","7a30f9fd":"**Sentiment analysis** is the interpretation and classification of emotions (positive, negative and neutral) within text data using text analysis techniques. Sentiment analysis allows businesses to identify customer sentiment toward products, brands or services in online conversations and feedback.In this, I have currently done basic EDA on the data that is present.","8d1575c0":"![](https:\/\/miro.medium.com\/max\/1280\/1*lOht9o73PICksasDplo0Pg.jpeg)\n","95c10162":"Seeing from the above data, It seems pretty clear that text and selected text are not that much different and only is extract of the text.Based on the texts, sentiments are decided.","46488b98":"\nThanking the below people's kernels, helped in making the kernels a little better. \n\n    https:\/\/www.kaggle.com\/aashita\/word-clouds-of-various-shapes\n\n    https:\/\/www.kaggle.com\/tanulsingh077\/twitter-sentiment-extaction-analysis-eda-and-model\n\n    https:\/\/towardsdatascience.com\/apply-and-lambda-usage-in-pandas-b13a1ea037f7\n    \n    https:\/\/monkeylearn.com\/sentiment-analysis\/\n    \n    https:\/\/www.kaggle.com\/parulpandey\/eda-and-preprocessing-for-bert\n    \n","5eede6e4":"In terms of bigrams, following bi-grams are in top.\n\n    - last night\n    - next week\n    - mothers day\n    -getting ready\n    - star trek\n    - looks like \n    - wish could\n    - mother day","f7bfc6e0":"## Building Bigrams\n\n\nA bigram or digram is a sequence of two adjacent elements from a string of tokens, which are typically letters, syllables, or words. A bigram is an n-gram for n=2.\n\nExample : Quick dog blows away. \n\nBigram combinations:\n    \n        Quick dog\n        dog blows\n        blows away\n        \nUsually combinations help in determining the sentiment.","4cc1d0b1":"Train and test data have equal share of the number of sentiment tweets.So if we do an analysis it would be similar to both the sets.Neutral tweets have share of around **40.5%** , positive tweets have **31.2%** and negative tweets have **28.3%**.","c87f078c":"The above set shows the count from the total set, and it seems that words like **day,good,get,like,work** are some of the top words.But what we want is which specific word would help in defining which sentiment","7ed6d2f4":"Checking for null values is always a good thing.","b219e8b2":"List of negative counts -\n\n    - feel like \n    - last night\n    - sorry hear\n    - looks like\n    - wish could\n    - gonna miss\n  \nThese are some of the negative bi-grams ","a3e5f435":"Top words for positive tweets:\n\n    - mother day\n    - mothers day\n    - good morning\n    -star wars\n    - wars day\n    - happy birthday\n    - good night","cda102dd":"## Model Building","97e7b231":"## Pre-processing data","2f684219":"## Jaccard Score\n\nJaccard Score refers to check the similarity between two sets against union of the two sets.\n\n            Jaccard Score -> A intersection B\n                            -------------------\n                                A union B\n    \nThe Jaccard similarity index (sometimes called the Jaccard similarity coefficient) compares members for two sets to see which members are shared and which are distinct. It\u2019s a measure of similarity for the two sets of data, with a range from 0% to 100%. The higher the percentage, the more similar the two populations. Although it\u2019s easy to interpret, **it is extremely sensitive to small samples sizes and may give erroneous results, especially with very small samples or data sets with missing observations** .\n\n","bf9cd667":"In terms of neutral words, following are the words which may hlp in determining a sentiment.\n\n    - get\n    - day\n    - lol\n    - work\n    - going\n    - got\n    - like\n    - today\n    ","4f4fee18":"Above Visualisation helps to understand the words which help in sentiments :\n\n    - like\n    - get\n    - miss\n    - work\n    - sad\n    - sorry\n    - really\n    - day\n    \n\nWhat interesting that I find here is in the positive tweets and neutral tweets , *like* word count and *day* word count is lesser while in negative tweets it is just opposite.","c59f461a":"I built a tree map to understand which words would help for positive tweets.\n    \n    - day\n    - good\n    - love\n    - happy\n    - thanks\n    - great\n    - like\n    - hope\n    \nWords like **day** and **like** are both in top words for positive and neutral","4a43a9cc":"The above function helps me select the top words based on corpus and selects those words and the count"}}