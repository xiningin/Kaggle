{"cell_type":{"9028e290":"code","c98269b5":"code","d5cf9822":"code","29ad028b":"code","c9655df3":"code","a374d2ce":"code","eca5d8d0":"code","742ba824":"code","16fdf53a":"code","25940b6d":"code","ed40e906":"code","4859f5d3":"code","b3ea2ba2":"code","b9ad737e":"code","405ff706":"code","1616aa1b":"code","35ab8be2":"code","1ca680c0":"code","4ee2bd54":"code","5677a6bb":"code","c027fb87":"code","afac92be":"code","0321dfb1":"code","050394b8":"code","ba40a67e":"code","bddb0a98":"code","45b84c1d":"code","91e0510d":"code","29584262":"code","025468f2":"code","d6a6c26e":"code","1965938a":"code","b991cc15":"code","b313dc96":"code","26af1fae":"code","1592dfb8":"code","9f8e0e1e":"code","15f36a12":"code","98650bc6":"code","4c03ce20":"code","3a58fb2b":"code","aa1dd1fa":"code","d63eb742":"code","046e1945":"code","0e1dac9e":"code","3e9478dd":"code","0028199a":"code","e061fbd6":"code","ff6ab6d1":"code","490a3b25":"code","961722be":"code","9f1d0fb0":"code","97829de3":"code","94b133be":"code","0c887166":"code","b0d196a9":"code","3b57d61a":"code","ef1b2f0a":"code","ab4cdee4":"code","b588f488":"code","58abfae1":"code","b2476bff":"code","b4ce2c80":"code","d76465cc":"code","11438e06":"code","ae9c119a":"code","517d897b":"code","e77d2044":"code","7de5349e":"code","856f3f49":"code","d2c068ce":"code","fec97cc4":"code","52b0322d":"code","6a36bea3":"code","628e8dda":"code","d1d44ef2":"code","7c8ff13e":"code","934a0385":"code","ca03e5e9":"code","70d4f484":"code","e358e52d":"code","cc45cea2":"code","4e7feae8":"code","58d705e1":"code","e7372cce":"code","3496a269":"code","ac9df715":"code","7b75040e":"code","a75da297":"code","87338c2f":"code","ee54ea2b":"code","f678f6fb":"code","05409512":"code","e0940f1b":"markdown","d6378308":"markdown","7cb32aba":"markdown","a7ef6cd5":"markdown","964943d0":"markdown","6dcd28b3":"markdown","85d28878":"markdown","29e5063e":"markdown","dbac786c":"markdown","c7c36026":"markdown","a5e8a9dc":"markdown","7542a30f":"markdown","7be0ba49":"markdown","e63b2b6f":"markdown","a6a67853":"markdown","a9e7aded":"markdown","96d07c1c":"markdown","82b26926":"markdown","a72970a6":"markdown","1c60567f":"markdown","87f30c1c":"markdown","87956562":"markdown","b68ff0ff":"markdown","ec8783d0":"markdown","b385d2a5":"markdown","38b9067c":"markdown","794c1e6d":"markdown","658125c0":"markdown","07221ae3":"markdown","1626ce0a":"markdown","927bc5d5":"markdown","c41ef8b7":"markdown","4a5ce3e6":"markdown","5b142814":"markdown","c70916d9":"markdown","1e99e61b":"markdown","2186c7a7":"markdown","b454caca":"markdown","199ee5c0":"markdown","bc713130":"markdown"},"source":{"9028e290":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c98269b5":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\nplt.style.use('seaborn-deep')\nplt.style.use('fivethirtyeight')\nplt.rcParams['font.family'] = 'sans-serif'\nplt.rcParams['font.monospace'] = 'Ubunto Mono'\nplt.rcParams['font.size'] = 10\nplt.rcParams['axes.labelsize'] = 12\nplt.rcParams['axes.titlesize'] = 12\nplt.rcParams['xtick.labelsize'] = 12\nplt.rcParams['ytick.labelsize'] = 12\nplt.rcParams['legend.fontsize'] = 12\nplt.rcParams['figure.titlesize'] = 14\nplt.rcParams['figure.figsize'] = (16,10)\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.options.mode.chained_assignment = None\npd.options.display.float_format = '{:.2f}'.format\npd.set_option('display.max_columns', 200)\npd.set_option('display.width', 400)","d5cf9822":"df = pd.read_csv('..\/input\/fast-furious-and-insured\/Fast_Furious_Insured\/train.csv')\ndf","29ad028b":"df_test = pd.read_csv('..\/input\/fast-furious-and-insured\/Fast_Furious_Insured\/test.csv')\ndf_test","c9655df3":"## Let's have a look at some of the damaged vehicles\n\npath = '..\/input\/fast-furious-and-insured\/Fast_Furious_Insured\/trainImages\/'\nfor i,im in enumerate(df[df['Condition'] == 1]['Image_path']):\n    plt.subplot(2,5,i+1)\n    img = plt.imread(path+str(im))\n    plt.imshow(img)\n    plt.xticks([])\n    plt.yticks([])\n    i+=1\n    if i == 10:\n        break\nplt.show()","a374d2ce":"## Let's have a look at some of the Non-damaged vehicles\n\npath = '..\/input\/fast-furious-and-insured\/Fast_Furious_Insured\/trainImages\/'\nfor i,im in enumerate(df[df['Condition'] == 0]['Image_path']):\n    plt.subplot(2,5,i+1)\n    img = plt.imread(path+str(im))\n    plt.imshow(img)\n    plt.xticks([])\n    plt.yticks([])\n    i+=1\n    if i == 10:\n        break\nplt.show()","eca5d8d0":"df['Condition'].value_counts()","742ba824":"os.makedirs('.\/data\/augmented_class_0') ## Directory that will contain the augmented images of label 0\nos.makedirs('.\/data\/class_1')           ## Directory that will contain the original images of label 1\nos.makedirs('.\/test\/test_data')         ## Directory that will contain the given test images","16fdf53a":"## Saving the images labelled 1 in new directory as mentioned above\n\nimport cv2\nfor i in df[df['Condition'] == 1]['Image_path']:\n    img = cv2.imread(path + str(i))\n    cv2.imwrite('.\/data\/class_1\/'+str(i),img)","25940b6d":"## Saving the test images in new directory as mentioned above\n\ntest = '..\/input\/fast-furious-and-insured\/Fast_Furious_Insured\/testImages\/'\nfor i in df_test['Image_path']:\n    img = cv2.imread(test + str(i))\n    cv2.imwrite('.\/test\/test_data\/'+str(i),img)","ed40e906":"## Preparing to equalize the images between class 0 and class 1 by using data augmentation\n\nimport keras\nfrom keras.preprocessing.image import ImageDataGenerator,img_to_array,load_img\n\ndatagen = ImageDataGenerator(\n    rotation_range = 20,\n    shear_range = 0.15,\n    width_shift_range = 0.1,\n    height_shift_range = 0.1,\n    zoom_range = 0.2,\n    featurewise_std_normalization=0.3,\n    channel_shift_range = 0.3,\n    fill_mode = 'nearest',\n    horizontal_flip = True,\n    vertical_flip = True\n)\nfor j,im in enumerate(df[df['Condition'] == 0]['Image_path']):\n    img = load_img(path + str(im))\n    if img == None:\n        continue\n    x = img_to_array(img)\n    x = x.reshape((1,)+x.shape)\n    i = 0\n    for batch in datagen.flow(x,batch_size=1,shuffle = True,save_to_dir ='.\/data\/augmented_class_0\/',save_prefix=j,save_format='jpg'):\n        i = i + 1\n        if i >= 14:\n            break\n","4859f5d3":"train_path = '.\/data\/'\ntest_path = '.\/test\/'\nimport keras\nfrom keras.preprocessing.image import ImageDataGenerator,img_to_array,load_img\n\ndatagentrain = ImageDataGenerator(\n        rescale = 1\/255.0,\n        rotation_range=45,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        shear_range=0.2,\n        zoom_range=0.15,\n        horizontal_flip=True,\n        vertical_flip = True,\n        fill_mode='nearest',\n        validation_split = 0.25\n)\ndatagentest = ImageDataGenerator(rescale=1\/255.0)\n\n\n\ntrain_set = datagentrain.flow_from_directory(directory=train_path,\n                                                 target_size = (224,224),\n                                                 batch_size = 96,\n                                                 class_mode = 'binary',\n                                                 #color_mode='grayscale',\n                                                 subset=\"training\",shuffle=True)\n\nvalidation_set = datagentrain.flow_from_directory(directory=train_path,\n                                                 target_size = (224,224),\n                                                 batch_size = 32,\n                                                 class_mode = 'binary',\n                                                  #color_mode='grayscale',\n                                                 subset=\"validation\",shuffle=True)\n\ntest_set = datagentest.flow_from_directory(directory=test_path,target_size=(224,224),batch_size=1,class_mode=None,#color_mode='grayscale',\n                                           shuffle=False)","b3ea2ba2":"from keras import Sequential\nfrom keras.layers import Input,Flatten,Dense,Activation,Dropout,BatchNormalization,GlobalAveragePooling2D\nimport tensorflow as tf\nfrom keras.models import Model\nimport tensorflow_hub as hub","b9ad737e":"#classifier = tf.keras.Sequential([\n#    hub.KerasLayer(\"https:\/\/tfhub.dev\/google\/tf2-preview\/mobilenet_v2\/classification\/4\", input_shape=(224,224,3))\n#])\nclassifier = tf.keras.Sequential([\n    hub.KerasLayer(\"https:\/\/tfhub.dev\/tensorflow\/resnet_50\/classification\/1\",input_shape=(224,224,3))\n])","405ff706":"feature_extractor_model = \"https:\/\/tfhub.dev\/tensorflow\/resnet_50\/feature_vector\/1\"\n#feature_extractor_model = \"https:\/\/tfhub.dev\/google\/imagenet\/inception_v3\/feature_vector\/5\"\npretrained_model_without_top_layer = hub.KerasLayer(\n    feature_extractor_model, input_shape=(224, 224, 3), trainable=False)\nnum_class = 2\n\nclassification_model = tf.keras.Sequential([\n  pretrained_model_without_top_layer,\n    Dropout(0.2),\n    Dense(8,activation='relu'),\n    Dense(1,activation='sigmoid')\n])\n\nclassification_model.summary()","1616aa1b":"classification_model.compile(\n  optimizer=\"adam\",\n  loss='binary_crossentropy',\n  metrics=['AUC'])\n\nh = classification_model.fit_generator(\n      train_set,validation_data = validation_set,\n                              epochs=80,\n                              callbacks = [\n                              keras.callbacks.EarlyStopping(monitor='val_loss',patience=10,mode='auto'),\n                              keras.callbacks.ModelCheckpoint('.\/classification_model_{val_loss:.3f}.h5',\n                              save_best_only = True,save_weights_only=False,\n                              monitor='val_loss')\n                              ]\n\n\n)\nclassification_model.save('model.h5')\n## If you are getting error of failed to file path 'val_loss', just re-run the cell ","35ab8be2":"## Model auc score and loss visualization\nacc= h.history['auc']\nval_acc=h.history['val_auc']\nloss=h.history['loss']\nval_loss=h.history['val_loss']\n\nepochs=range(len(acc)) #No. of epochs\n\nplt.figure(figsize=(8,5))\nplt.plot(epochs,acc,'r',label='Training AUC score')\nplt.plot(epochs,val_acc,'g',label='Testing AUC score')\nplt.legend()\nplt.xlabel('No. of epochs')\nplt.ylabel('Accuracy score')\n\nplt.figure(figsize=(8,5))\nplt.plot(epochs,loss,'r',label='Training Loss')\nplt.plot(epochs,val_loss,'g',label='Testing Loss')\nplt.xlabel('No. of epochs')\nplt.ylabel('Loss score')\nplt.legend()\nplt.show()","1ca680c0":"classification_model.evaluate(validation_set)","4ee2bd54":"df_test = pd.read_csv('..\/input\/fast-furious-and-insured\/Fast_Furious_Insured\/test.csv')\ndf = pd.read_csv('..\/input\/fast-furious-and-insured\/Fast_Furious_Insured\/train.csv')","5677a6bb":"corr = df.corr()\nplt.figure(figsize=(10,10))\nmask = np.zeros_like(corr,dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corr,mask=mask,annot=True)\nplt.show()","c027fb87":"## Showing which feature got missing values\nsns.heatmap(df.isnull(),cbar=False,yticklabels=False,cmap = 'viridis')","afac92be":"df.info()","0321dfb1":"df.isnull().sum()","050394b8":"df.nunique()","ba40a67e":"df['Insurance_company'].value_counts()","bddb0a98":"x = list(df['Insurance_company'].value_counts().keys())\ny = df['Insurance_company'].value_counts().values\nsns.barplot(x= x,y=y)\n","45b84c1d":"df['Cost_of_vehicle'].plot(kind='kde')","91e0510d":"df['Min_coverage'].plot(kind='kde')","29584262":"df['Max_coverage'].plot(kind='kde')","025468f2":"## scatterplot between feature 'Max_coverage' ,and feature 'Amount' with label of feature 'Condition'\nsns.scatterplot(x ='Max_coverage' ,y='Amount',hue='Condition',data=df)","d6a6c26e":"df['Condition'].value_counts()","1965938a":"sns.boxplot(y='Amount',x='Condition',data=df)","b991cc15":"df['Amount'].plot(kind='box')","b313dc96":"## Removing Outliers\n\nout = df['Amount'] > 12000\nout = np.where(out)\nprint(np.shape(out))\ndf.drop(out[0],inplace=True)\ndf.index = range(df.shape[0])","26af1fae":"df['Amount'].plot(kind='box')","1592dfb8":"df[df['Amount']<0]","9f8e0e1e":"df.drop(index = 641,inplace=True)","15f36a12":"sns.pairplot(df,diag_kind='kde',hue='Condition')","98650bc6":"df.info()","4c03ce20":"df.describe()","3a58fb2b":"df[df['Amount'] != 0].isnull().sum()","aa1dd1fa":"## Separating damaged vehicles data\n\n\n\ntarget_df = df[df['Condition'] == 1]\ntarget_df.index = range(target_df.shape[0])\ntarget_df","d63eb742":"target_df.info()","046e1945":"target_df.describe()","0e1dac9e":"## Visulaizing the missing values\n\nsns.heatmap(target_df.isnull(),cbar=False,yticklabels=False,cmap = 'viridis')","3e9478dd":"sns.pairplot(target_df)","0028199a":"corr = target_df.corr()\nplt.figure(figsize=(10,10))\nmask = np.zeros_like(corr,dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corr,mask=mask,annot=True)\nplt.show()","e061fbd6":"plt.figure()\nax0 = plt.subplot(1,3,1)\nax1 = plt.subplot(1,3,2)\nax2 = plt.subplot(1,3,3)\ntarget_df['Cost_of_vehicle'].plot(kind='kde',ax=ax0)\nax0.set_xlabel('Cost_of_vehicle')\ntarget_df['Min_coverage'].plot(kind='kde',ax=ax1)\nax1.set_xlabel('Min_coverage')\ntarget_df['Max_coverage'].plot(kind='kde',ax=ax2)\nax2.set_xlabel('Max_coverage')\nplt.show()","ff6ab6d1":"x = list(target_df['Insurance_company'].value_counts().keys())\ny = target_df['Insurance_company'].value_counts().values\nsns.barplot(x= x,y=y)\n","490a3b25":"## Filling the missing values using mean\ntarget_df['Amount'].fillna(target_df['Amount'].mean(),inplace=True)\ntarget_df['Cost_of_vehicle'].fillna(target_df['Cost_of_vehicle'].mean(),inplace=True)\ntarget_df['Min_coverage'].fillna(target_df['Min_coverage'].mean(),inplace=True)\ntarget_df['Max_coverage'].fillna(target_df['Max_coverage'].mean(),inplace=True)","961722be":"## Seleting the required features only\n\ncols = ['Min_coverage','Amount','Insurance_company']\ntarget_df = target_df[cols]","9f1d0fb0":"target_df","97829de3":"target_df.info()","94b133be":"df_dum = pd.get_dummies(target_df['Insurance_company'])\ntarget_df = pd.concat([target_df,df_dum],axis=1)\ntarget_df","0c887166":"corr = target_df.corr()\nplt.figure(figsize=(16,10))\nmask = np.zeros_like(corr,dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corr,mask=mask,annot=True)\nplt.show()","b0d196a9":"target_df.info()","3b57d61a":"plt.figure()\nax0 = plt.subplot(1,2,1)\nax1 = plt.subplot(1,2,2)\ntarget_df['Min_coverage'].plot(kind='box',ax=ax0)\ntarget_df['Amount'].plot(kind='box',ax=ax1)\n","ef1b2f0a":"target_df.isnull().sum()","ab4cdee4":"target_df.describe()","b588f488":"target_df.drop(['Insurance_company'],inplace=True,axis=1)","58abfae1":"X = target_df.drop(['Amount'],axis=1)\nY = target_df[['Amount']]","b2476bff":"## Scaling the data using StandardScaler\nfrom sklearn.preprocessing import StandardScaler\nscaler_x = StandardScaler()\nscaler_y = StandardScaler()\nX_scaled = scaler_x.fit_transform(X)\nY_scaled = scaler_y.fit_transform(Y)\n","b4ce2c80":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(X_scaled,Y_scaled,train_size=0.8,random_state=42)\nprint(x_train.shape,y_train.shape)\nprint(x_test.shape,y_test.shape)","d76465cc":"from sklearn.linear_model import LinearRegression, Lasso\nfrom sklearn.metrics import r2_score\nlr = LinearRegression()\nlr.fit(x_train,y_train)\ny_train_pred = lr.predict(x_train)\ny_test_pred = lr.predict(x_test)\nprint(r2_score(y_true=y_train,y_pred=y_train_pred))\nprint(r2_score(y_true=y_test,y_pred=y_test_pred))","11438e06":"from sklearn.linear_model import LassoCV\nlasso_model = LassoCV(alphas=[0.0001,0.0005,0.001,0.005,0.01,0.1,1.0,10],cv=5)\nlasso_model.fit(x_train,y_train)\ny_train_pred = lasso_model.predict(x_train)\ny_test_pred = lasso_model.predict(x_test)\nprint(r2_score(y_true=y_train,y_pred=y_train_pred))\nprint(r2_score(y_true=y_test,y_pred=y_test_pred))","ae9c119a":"from sklearn.ensemble import ExtraTreesRegressor\nextra_model = ExtraTreesRegressor(criterion='mse', random_state=0, n_jobs=-1, \n                                min_samples_leaf=1, max_depth=8, \n                                min_samples_split=3, n_estimators=1000\n                               )\n\nextra_model.fit(x_train, y_train)\n\n# predict\ny_train_pred = extra_model.predict(x_train)\ny_test_pred = extra_model.predict(x_test)\nprint(r2_score(y_train,y_train_pred))\nprint(r2_score(y_test,y_test_pred))","517d897b":"from sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor()\nrf.fit(x_train,y_train)\ny_train_pred = rf.predict(x_train)\ny_test_pred = rf.predict(x_test)\nprint(r2_score(y_train,y_train_pred))\nprint(r2_score(y_test,y_test_pred))","e77d2044":"from sklearn.ensemble import GradientBoostingRegressor\ngb_model = GradientBoostingRegressor(criterion='mse',random_state=0,max_depth=5,\n                                     n_estimators=500,min_samples_split=2,min_samples_leaf=2)\ngb_model.fit(x_train,y_train)\ny_train_pred = gb_model.predict(x_train)\ny_test_pred = gb_model.predict(x_test)\nprint(r2_score(y_train,y_train_pred))\nprint(r2_score(y_test,y_test_pred))","7de5349e":"from xgboost import XGBRegressor\nxgb = XGBRegressor(n_estimators=500,max_depth=5,booster='gbtree',n_jobs=-1,learning_rate=0.1,reg_lambda=0.01,reg_alpha=0.3)\nxgb.fit(x_train,y_train)\ny_train_pred = xgb.predict(x_train)\ny_test_pred = xgb.predict(x_test)\nprint(r2_score(y_train,y_train_pred))\nprint(r2_score(y_test,y_test_pred))","856f3f49":"from sklearn.svm import SVR\nregressor = SVR(kernel = 'rbf')\nregressor.fit(x_train, y_train)\ny_train_pred = regressor.predict(x_train)\ny_test_pred = regressor.predict(x_test)\nprint(r2_score(y_train,y_train_pred))\nprint(r2_score(y_test,y_test_pred))","d2c068ce":"plt.plot(X['Min_coverage'],Y,'o')\nplt.xlabel('Min_coverage')\nplt.ylabel('Amount')\nplt.show()","fec97cc4":"def plotHistory(history):\n    print(\"Min. Validation MSE\",min(history.history[\"val_mse\"]))\n    pd.DataFrame(history.history).plot(figsize=(12,6))\n    plt.show()\ncallbacks_list = [keras.callbacks.EarlyStopping(monitor='val_loss',patience=10,mode='auto'),\n                              keras.callbacks.ModelCheckpoint('.\/reg_model_{val_loss:.3f}.h5',\n                              save_best_only = True,save_weights_only=False,\n                              monitor='val_loss')]","52b0322d":"epochs = 50\nreg_model = Sequential([\n    Dense(1024, activation='relu', input_shape=(x_train.shape[1],)),\n    BatchNormalization(),\n    Dropout(0.3),\n    \n    Dense(512, activation='relu'),\n    BatchNormalization(),\n    Dropout(0.2),\n\n    Dense(256, activation='relu'),\n    BatchNormalization(),\n    Dropout(0.2),\n\n    Dense(128, activation='relu'),\n#     k.layers.BatchNormalization(),\n    Dropout(0.2),\n\n    Dense(1),\n])\nprint(reg_model.summary())\n\nreg_model.compile(optimizer='RMSProp',\n              loss='mae',\n              metrics='mse'\n)\nhistory = reg_model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=epochs,\n                 callbacks=[callbacks_list], batch_size=512)","6a36bea3":"plotHistory(history)","628e8dda":"scaler_y.inverse_transform(xgb.predict(x_test))","d1d44ef2":"df_test = pd.read_csv('..\/input\/fast-furious-and-insured\/Fast_Furious_Insured\/test.csv')\ndf_test","7c8ff13e":"corr = df_test.corr()\nplt.figure(figsize=(10,10))\nmask = np.zeros_like(corr,dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corr,mask=mask,annot=True)\nplt.show()","934a0385":"## Predicting the Condition label for test data using our classification_model\n\narr = classification_model.predict(test_set)\npred = []\nfor i in arr:\n    if i >= 0.5:\n        pred.append(1)\n    else:\n        pred.append(0)\n            ","ca03e5e9":"np.shape(pred)","70d4f484":"df_test['Condition'] = pred","e358e52d":"df_test","cc45cea2":"df_test['Condition'].value_counts()","4e7feae8":"df_test = df_test[['Image_path','Insurance_company','Min_coverage','Condition']]\ndf_test","58d705e1":"df_dum = pd.get_dummies(df_test['Insurance_company'])\ndf_test = pd.concat([df_test,df_dum],axis=1)\ndf_test","e7372cce":"X_test = df_test.drop(['Image_path','Insurance_company','Condition'],axis=1)\nX_test","3496a269":"X_test = scaler_x.transform(X_test)","ac9df715":"predicted = scaler_y.inverse_transform(reg_model.predict(X_test))","7b75040e":"df_test['Amount'] = predicted","a75da297":"df_test","87338c2f":"## We know that the Amount value is zero for Non damaged vehicles and thus doing so.\nfor i in range(df_test.shape[0]):\n    if df_test['Condition'][i] == 0:\n        df_test['Amount'][i] = 0\n        ","ee54ea2b":"df_test","f678f6fb":"sub = df_test[['Image_path','Condition','Amount']]\nsub","05409512":"sub.to_csv('.\/Submission.csv',header=True,index=False)","e0940f1b":"### Feature \"Min_coverage\"","d6378308":"### Linear Regression","7cb32aba":"You might be wondering why all these models are having negative or low R2 score, so here is, why it is happening:","a7ef6cd5":"### Extra Tree Regressor","964943d0":"# Task 5 - Model evaluation and submission","6dcd28b3":"### Feature \"Max_coverage\"","85d28878":"## Using transfer learning to classify the images","29e5063e":"Now we are done with our classification task, we will be using this classification_model to predict the Condition of the test set given and then using regression_model, Amount will be predicted.","dbac786c":"## Dealing with regression task","c7c36026":"### Model definition and summary","a5e8a9dc":"Now we have features (Cost_of_vehicle, Min_coverage, Max_coverage) higly correlated(correlation of 1). So we will be chosing only one of them in end.","7542a30f":"# Task 1 - Importing libraries and dataset ","7be0ba49":"# Task 2 - Exploratory Data Analysis (EDA) ","e63b2b6f":"### SVR ","a6a67853":"### Preparing the test data for regression_model","a9e7aded":"So, here is the deal to do,\nSince we know that the Amount for all vehicle images is Zero whose Condition is labelled Zero i.e., Not damaged vehicle,so we will be fitting our regression_model ONLY for the data of positively labelled class and will be predicting the Amount for those only. The Amount for the data of negatively labeled class will simply Zero. So we will be separating data of labelled One from the combined data. Before moving to that part, let's explore the whole data.","96d07c1c":"### Lasso Regression","82b26926":"## Dealing with classification subtask","a72970a6":"# Task 3 - Data Preparation for model evaluation","1c60567f":"Now we will deal only with the data of damaged vehicles and will fit a regression_model to predict Amount for them.","87f30c1c":"### Random Forest Regressor","87956562":"Since we don't know the data of Insurance_company is ordinal or not. So we will be considering it as nominal data and using it's One hot encoding representation by dummy variables.","b68ff0ff":"As mentioned earlier, Amount for data labelled Condition Zero is also zero and thus shown in scatterplot also. The amount for damaged vehicle is having some points clearly out of these clustered values so they will be considered as outliers. We will separately analyse the data of damaged vehicles.","ec8783d0":"# Task 4 - Data modelling ","b385d2a5":"From the above correlation plot, we can see that features like (Cost_of_vehicle, Min_coverage) and (Max_coverage, Condition) are highly correlated, so in data preparation for regression_model, we will be selecting only one of them.","38b9067c":"Rougly normal distribution to some extent.","794c1e6d":"## Last Notes","658125c0":"From the above cell, we can see that there is imbalance between the instances of label 0 and label 1. To counter this problem we will firstly augment the image labelled 0 in separate directory to equalize the number of examples in both of classes.","07221ae3":"The data was higly imbalanced and the image of damaged vehicles wer not also good, howsoever transfer learning works well. Since the data was imbalanced, I firstly augument the data of minority class to equalize the instances between the classes.This is done because the data augumentation generator at the compile time, don't treat imbalance problem, it just replicates the data in same ratio as there was when classes were imabalanced.\nSo we need to equalize the instances before using the data augmentation generator at compile time.\nThen used data augumentation for whole data while feeding data to the model as classes are now balanced with equal number of examples. Accuracy metric for classification_model is not preferable, since it fails to tell us the exact performance of the model when data is imbalanced, so metric AUC has been used which is good in case of imbalance between two classes or simply we can use accuracy metric as well ONLY when the instances are approximately equal between the classes.\nFor the regression part, I firstly separated the data of condition label 0 and 1. And then fitted the regression_model over the data of damaged vehicles after excluding some highly correlated features. The R2 Score was low as mentioned above why it is happening. If anyone is having good R2 score with other model or with some transformed form of data, please let me know in the comment section.\n\nThis is best intution I have. If anyone is having better approaches or better analysis, please let me know.\nAny kind of queries, improvements or feedbacks are most welcome.\n\nIf you like my work, please show your appreciation by upvoting the notebook. Thank you...............!!!!!!!","1626ce0a":"### Loading resnet_50 pretrained model based on Resnet V1 50 architecture published by tensorflow","927bc5d5":"Now the data has been setuped as you can see from the above cell.","c41ef8b7":"### Artificial Neural Network (ANN)","4a5ce3e6":"### Gradient Boosting Regressor","5b142814":"Fairly normal distributed.","c70916d9":"From the above density plot, we can see that all these three features are having correlation of 1.","1e99e61b":"### XGB Regressor","2186c7a7":"### Feature \"Cost_of_vehicle\"","b454caca":"I believe the negative value or low value of R2 score is justified and i guess no model can fit over this noised data, and similary for other features as well, feature Amount is not showing any kind of correlation. If anyone is having good R2 score with other model or with some transformed form of data, please let me know in the comment section.","199ee5c0":"#### Feature \"Insurance_company\"","bc713130":"From the correlation plot, we concluded that feature cost_of_vehicle and Min_coverage are having correlation of 1 and that is what conveyed by above kde plots."}}