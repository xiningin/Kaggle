{"cell_type":{"385cf737":"code","5e396591":"code","5138e889":"code","5cd6c821":"code","2df63244":"code","c6ca1b7b":"code","28466907":"code","d8f6f41a":"code","0a14ea47":"code","01de7b7c":"code","f3bb94c9":"code","444b2234":"code","093cbcc2":"code","d671994f":"code","85ab4743":"code","f40c33b7":"code","5e585dae":"code","5c6950cc":"code","fcc1f5a5":"code","df40553c":"code","9ff1f0df":"code","14ffa601":"code","a8ea2e9a":"code","49349965":"code","b2a045a8":"code","a079ecef":"markdown","14ca5b2e":"markdown","579daf2c":"markdown","b449335c":"markdown","945ef48d":"markdown","dd2512ed":"markdown","2e0fa434":"markdown","9de6e9ea":"markdown","068ff108":"markdown","7a3f717f":"markdown","24220d99":"markdown","3089ff08":"markdown","c32e04a9":"markdown"},"source":{"385cf737":"!pip install tensorflow_datasets\n","5e396591":"\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport os","5138e889":"mbti_dataset_line = tf.data.TextLineDataset(\"..\/input\/mbti-clean-with-categories\/mbti_clean.csv\")","5cd6c821":"for ex in mbti_dataset_line.take(5):\n  print(ex)","2df63244":"def label(line):\n  label =  tf.strings.substr([line],[-10],[1])\n  if label[0]==',':\n    label = tf.strings.substr([line],[-9],[1])\n  else:\n    label = tf.strings.substr([line],[-10],[2])\n  labelnum=tf.strings.to_number(label,tf.int64)\n  line= tf.strings.substr([line],[6],(tf.strings.length([line])-17))\n  return line[0], labelnum[0]\n","c6ca1b7b":"\nmbti_dataset_line = mbti_dataset_line.skip(1).map(lambda line: label(line))\n","28466907":"for ex in mbti_dataset_line.take(5):\n  print(ex)","d8f6f41a":"BUFFER_SIZE = 100000\nBATCH_SIZE = 64\nTAKE_SIZE = 5000","0a14ea47":"mbti_dataset_line = mbti_dataset_line.shuffle(\n    BUFFER_SIZE, reshuffle_each_iteration=False)","01de7b7c":"tokenizer = tfds.features.text.Tokenizer()\n\nvocabulary_set = set()\nfor text_tensor, _ in mbti_dataset_line:\n  some_tokens = tokenizer.tokenize(text_tensor.numpy())\n  vocabulary_set.update(some_tokens)\n\nvocab_size = len(vocabulary_set)\nvocab_size","f3bb94c9":"encoder = tfds.features.text.TokenTextEncoder(vocabulary_set)","444b2234":"example_text = next(iter(mbti_dataset_line))[0].numpy()\nprint(example_text)","093cbcc2":"encoded_example = encoder.encode(example_text)\nprint(encoded_example)","d671994f":"def encode(text_tensor, label):\n  encoded_text = encoder.encode(text_tensor.numpy())\n  return encoded_text, label\n\ndef encode_map_fn(text, label):\n  return tf.py_function(encode, inp=[text, label], Tout=(tf.int64, tf.int64))\n\nall_encoded_data = mbti_dataset_line.map(encode_map_fn)","85ab4743":"train_data = all_encoded_data.skip(TAKE_SIZE).shuffle(BUFFER_SIZE)\ntrain_data = train_data.padded_batch(BATCH_SIZE, padded_shapes=([-1],[]))\n\ntest_data = all_encoded_data.take(TAKE_SIZE)\ntest_data = test_data.padded_batch(BATCH_SIZE, padded_shapes=([-1],[]))","f40c33b7":"sample_text, sample_labels = next(iter(test_data))\n\nsample_text[0], sample_labels[0]","5e585dae":"vocab_size += 1","5c6950cc":"model = tf.keras.Sequential()","fcc1f5a5":"model.add(tf.keras.layers.Embedding(vocab_size+1, 64))","df40553c":"model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)))","9ff1f0df":"# One or more dense layers.\n# Edit the list in the `for` line to experiment with layer sizes.\nfor units in [64, 64]:\n  model.add(tf.keras.layers.Dense(units, activation='relu'))\n\n# Output layer. The first argument is the number of labels.\nmodel.add(tf.keras.layers.Dense(16, activation='softmax'))","14ffa601":"model.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])","a8ea2e9a":"model.fit(train_data, epochs=1, validation_data=test_data)","49349965":"eval_loss, eval_acc = model.evaluate(test_data)\n\nprint('\\nEval loss: {:.3f}, Eval accuracy: {:.3f}'.format(eval_loss, eval_acc))","b2a045a8":"exit()","a079ecef":"Split the dataset into test and train batches","14ca5b2e":"Reference Code\nhttps:\/\/www.tensorflow.org\/tutorials\/load_data\/text","579daf2c":"Finally we'll have a series of one or more densely connected layers, with the last one being the output layer. The output layer produces a probability for all the labels. The one with the highest probability is the models prediction of an example's label.","b449335c":"The dataset used for analysis is the clean version as preperared using the following kernel \nhttps:\/\/www.kaggle.com\/brunnoricci\/personalitytypeclassification with the numerical categories added for the MBTI \nType . Can be downloaded from www.kaggle.com\/uplytics\/mbti-clean-with-categories","945ef48d":"Start Building the Tensorflow2.0 Model","dd2512ed":"Install Tensorflow datasets Tensorflow2.0","2e0fa434":"Add the padding (0) token to vocabulary","9de6e9ea":"The first layer converts integer representations to dense vector embeddings. ","068ff108":"Create an encoder by passing the vocabulary_set to tfds.features.text.TokenTextEncoder. The encoder's encode method takes in a string of text and returns a list of integers.","7a3f717f":"Now run the encoder on the dataset by wrapping it in tf.py_function and passing that to the dataset's map method.","24220d99":"The next layer is a Long Short-Term Memory layer, which lets the model understand words in their context with other words. A bidirectional wrapper on the LSTM helps it to learn about the datapoints in relationship to the datapoints that came before it and after it.","3089ff08":"Finally, compile the model. For a softmax categorization model, use sparse_categorical_crossentropy as the loss function. You can try other optimizers, but adam is very common.","c32e04a9":"##Encode text lines as numbers\nFirst, build a vocabulary by tokenizing the text into a collection of individual unique words. \n1.Iterate over each example's numpy value.\n2. Use tfds.features.text.Tokenizer to split it into tokens.\n3.Collect these tokens into a Python set, to remove duplicates.\n4.Get the size of the vocabulary for later use."}}