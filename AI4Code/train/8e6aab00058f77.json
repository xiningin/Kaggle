{"cell_type":{"19b3c1e6":"code","5390668f":"code","e4dfb217":"code","2f2d9ff0":"code","15fda9aa":"code","b62fc983":"code","aea2c75e":"code","0c561472":"code","3daff411":"code","bd9ca803":"code","6b608afc":"code","15069cb9":"code","cd6bb33a":"code","dd5c2db9":"code","691e9498":"code","175b562d":"code","5b91e186":"code","31a8aad1":"code","c721afa5":"code","104c9c8d":"code","bb67ec46":"code","d2e9f35a":"code","4d79c428":"code","7537b3a2":"code","6c2631a8":"code","93a501af":"code","d22c905e":"code","73c661bf":"code","5a199ae0":"code","f4212543":"code","207fe45e":"code","65c7164c":"code","c8eaace3":"code","e0bbca26":"code","667582e2":"code","28f26dd3":"code","b7ed3e17":"code","5258181e":"code","7b840949":"code","ee524969":"code","dfa48bb3":"code","1c698c0d":"code","0a159491":"code","7bf2a272":"code","4d92521d":"code","d37012d9":"code","a1aa32c1":"code","cd2bd593":"code","58717902":"code","4dc5c267":"code","ccdbdc00":"code","3eaca239":"code","c6cea9db":"code","3fdda32b":"code","6fc2aa12":"code","f51dd65b":"code","cfe820e9":"code","17f3d810":"code","49d9f43a":"code","85793c24":"code","0a765a01":"code","8687ef35":"code","3535ddae":"code","9e37935a":"code","f947c2c1":"code","d6b2dd56":"code","991f6cbd":"code","9740b4af":"code","a73b2aa5":"code","e208b6e4":"code","60181d23":"code","4ae86d7f":"code","ba1ce146":"code","81693f79":"code","0bd6aec0":"code","516855b4":"code","0e2f5acd":"code","a5265b5b":"code","1d30fee1":"code","f97332ae":"code","0c8a735f":"code","174ea498":"code","b42bbd88":"code","62833cc6":"code","44970e97":"markdown","e53f13f4":"markdown","fc99e842":"markdown","2aa3f7c7":"markdown","fefe31f7":"markdown","67690a61":"markdown","50e32301":"markdown","10e32ddb":"markdown","dc165aa0":"markdown","e9f35dd4":"markdown","0469a42e":"markdown","744d7986":"markdown","ed55bece":"markdown","575c7875":"markdown","4fc29000":"markdown","3a44b890":"markdown","98ab4930":"markdown","67424376":"markdown","03764241":"markdown","4b3722bc":"markdown","eb3444a3":"markdown","0f081cee":"markdown","4ffb06d2":"markdown","c1ecf861":"markdown","49598be4":"markdown","f81f740d":"markdown","5d83aac3":"markdown","3e763590":"markdown","b6c1934a":"markdown","68e69111":"markdown","337f75c7":"markdown","61452fe3":"markdown","c4e177c2":"markdown","e3b4084b":"markdown","6930d1f4":"markdown","766e71e5":"markdown","9bb4af46":"markdown","31f7d2ee":"markdown","c97ebf61":"markdown","ff9634e3":"markdown","9fe0f294":"markdown","61738ee8":"markdown","3693f27b":"markdown","f231c40b":"markdown","8b24a71c":"markdown","f861b64a":"markdown","ba732f5e":"markdown","3927c81f":"markdown","233c9f4d":"markdown","accb1ecf":"markdown","6e037968":"markdown","3537a9d8":"markdown","97ec52fb":"markdown","6b74963d":"markdown","9a58f296":"markdown","9706fcec":"markdown","d0bb4fc3":"markdown","f8d61bb7":"markdown","c09858f3":"markdown","a82c68fc":"markdown","36d57c66":"markdown","700e6510":"markdown","288bf3a4":"markdown","e61e7561":"markdown","86f94183":"markdown","9b21cd9b":"markdown","c89bc96e":"markdown","21dfb7a6":"markdown","55126f99":"markdown","b5528c22":"markdown","acd19e6c":"markdown","060e1737":"markdown","7bf13796":"markdown","f1578da4":"markdown","4b4bd16c":"markdown","2cd62c66":"markdown","5bfbd5e2":"markdown","d957b665":"markdown","44aaac12":"markdown","43ef0133":"markdown","7e8c5e51":"markdown","f7c3e834":"markdown"},"source":{"19b3c1e6":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport re, string, unicodedata\nfrom bs4 import BeautifulSoup\n\n!pip install contractions\nimport nltk\nimport contractions\n\nnltk.download('wordnet')\nnltk.download('punkt')\n\nfrom nltk.tokenize import word_tokenize \nfrom nltk.stem import LancasterStemmer, WordNetLemmatizer\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn import metrics\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report\nfrom mlxtend.plotting import plot_confusion_matrix\n\nfrom imblearn.over_sampling import SMOTE\n","5390668f":"#from google.colab import drive\n#drive.mount('\/content\/drive\/')\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e4dfb217":"#Tweet= pandas.read_csv(\"..\/input\/Tweets.csv\")\n#Tweet.head()\n\nproject_path =  '\/kaggle\/input\/twitter-airline-sentiment\/'\n\n# Load the dataset\ntweet_data = pd.read_csv(project_path + 'Tweets.csv',header=0)","2f2d9ff0":"tweet_data.head()","15fda9aa":"# There are 14640 rows and 15 columns in the tweet data\nprint(tweet_data.shape)","b62fc983":"tweet_data.info()","aea2c75e":"tweet_data.dtypes","0c561472":"# Check for duplicate rows\nduplicateRowsDF = tweet_data[tweet_data.duplicated()]\nprint(duplicateRowsDF.head())\nprint(duplicateRowsDF.shape)  \n# There are 36 duplicate rows","3daff411":"# Drop the duplicate rows\ntweet_data.drop_duplicates(keep=False,inplace=True) ","bd9ca803":"# There are 14568 rows and 15 columns after dropping 36 duplicate rows from the initial load dataset\nprint(tweet_data.shape)","6b608afc":"tweet_data.describe()","15069cb9":"g = sns.FacetGrid(tweet_data, col=\"airline\", col_wrap=3, height=5, aspect =0.7)\ng = g.map(sns.countplot, \"airline_sentiment\",order =tweet_data.airline_sentiment.value_counts().index, palette='plasma')\nplt.show()\n# Here we can see that United Airlines, US Airways, American Airlines has the most number of negative review\n# Virgin America has the least number of negative reviews","cd6bb33a":"# Check the most common negative reason \ny = tweet_data['negativereason']\nprint(y.value_counts())\nplt.figure(figsize=(25,5)) \ng = sns.countplot(y)\n# Customer service and Late flight seems to be the main reason why customers are giving bad feedback","dd5c2db9":"tweet_data.columns","691e9498":"# Let us now remove irrelevant columns\ntweet_data_relevant = tweet_data.drop(['tweet_id', 'airline_sentiment_confidence',\n       'negativereason', 'negativereason_confidence', 'airline',\n       'airline_sentiment_gold', 'name', 'negativereason_gold',\n       'retweet_count','tweet_coord', 'tweet_created',\n       'tweet_location', 'user_timezone'], axis =1)","175b562d":"# There are 14568 rows and 2 columns (This is result of keeping relevant rows and duplicate data cleanup)\ntweet_data_relevant.shape","5b91e186":"tweet_data_relevant.head(5)","31a8aad1":"print(tweet_data_relevant.airline_sentiment.value_counts())","c721afa5":"y = tweet_data_relevant['airline_sentiment']\nprint(y.value_counts())\nplt.figure(figsize=(20,5)) \ng = sns.countplot(y)\n# No, Here we can see that the data is not balanced, There are lot of negative sentiments","104c9c8d":"def perform_html_cleanup( raw_review ):\n  # 1. Remove HTML\n  review_text = BeautifulSoup(raw_review).get_text()\n  return review_text","bb67ec46":"def replace_contractions(raw_review):\n    #Replace contractions in raw_review\n    return contractions.fix(raw_review)","d2e9f35a":"def perform_tokenization( raw_review ):\n  # 2. Perform Tokenization\n  word_tokens = word_tokenize(raw_review)  # Tokenization\n  return word_tokens","4d79c428":"def remove_numbers(list_of_words): \n    pattern = '[0-9]'\n    list = [re.sub(pattern, '', i) for i in list_of_words] \n    return list","7537b3a2":"def remove_special_character_punctuation(list_of_words): \n    pattern = '[^A-Za-z0-9]+'\n    list = [re.sub(pattern, '', i) for i in list_of_words] \n    return list","6c2631a8":"def remove_punctuation(words):\n    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n    new_words = []                        # Create empty list to store pre-processed words.\n    for word in words:\n        new_word = re.sub(r'[^\\w\\s]', '', word)\n        if new_word != '':\n            new_words.append(new_word)    # Append processed words to new list.\n    return new_words","93a501af":"def to_lowercase(words):\n    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n    new_words = []                        # Create empty list to store pre-processed words.\n    for word in words:\n        new_word = word.lower()           # Converting to lowercase\n        new_words.append(new_word)        # Append processed words to new list.\n    return new_words","d22c905e":"def remove_empty_string(words):\n  return list(filter(None, words))","73c661bf":"def stem_words(words):\n    \"\"\"Stem words in list of tokenized words\"\"\"\n    stemmer = LancasterStemmer()\n    stems = []                            # Create empty list to store pre-processed words.\n    for word in words:\n        stem = stemmer.stem(word)\n        stems.append(stem)                # Append processed words to new list.\n    return stems","5a199ae0":"def lemmatize_verbs(words):\n    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n    lemmatizer = WordNetLemmatizer()\n    lemmas = []                           # Create empty list to store pre-processed words.\n    for word in words:\n        lemma = lemmatizer.lemmatize(word, pos='v')\n        lemmas.append(lemma)              # Append processed words to new list.\n    return lemmas","f4212543":"def perform_cleanup(raw_review):\n    clean_review = perform_html_cleanup(raw_review)\n    clean_review = replace_contractions(clean_review)\n    clean_review = perform_tokenization(clean_review)\n    clean_review = remove_numbers(clean_review)\n    clean_review = remove_special_character_punctuation(clean_review)\n    clean_review = remove_punctuation(clean_review)\n    clean_review  = to_lowercase(clean_review)\n    clean_review = remove_empty_string(clean_review)\n    #clean_review = stem_words(clean_review)\n    clean_review = lemmatize_verbs(clean_review)\n    return clean_review","207fe45e":"print(tweet_data_relevant.head())","65c7164c":"clean_reviews = []\n\nfor i, row in tweet_data_relevant.iterrows():\n    words = tweet_data_relevant.at[i, 'text']\n    words = perform_cleanup(words)\n    tweet_data_relevant.at[i,'text'] = \" \".join( words )\n    clean_reviews.append( tweet_data_relevant.at[i, 'text'] )\ntweet_data_relevant.head()","c8eaace3":"tweet_data_relevant.head(5)","e0bbca26":"print (\"Creating the bag of words...\\n\")\n# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n# bag of words tool.  \ncount_vectorizer = CountVectorizer(analyzer = \"word\",   \\\n                             tokenizer = None,    \\\n                             preprocessor = None, \\\n                             stop_words = None,   \\\n                             max_features = 5000) \n\n# fit_transform() does two functions: First, it fits the model\n# and learns the vocabulary; second, it transforms our training data\n# into feature vectors. The input to fit_transform should be a list of \n# strings.\ncount_vectorizer_data_features = count_vectorizer.fit_transform(clean_reviews)\n\n# Numpy arrays are easy to work with, so convert the result to an \n# array\ncount_vectorizer_data_features = count_vectorizer_data_features.toarray()","667582e2":"print (count_vectorizer_data_features.shape)\nprint(count_vectorizer_data_features)","28f26dd3":"# Take a look at the words in the vocabulary\ncount_vectorizer_vocab = count_vectorizer.get_feature_names()\nprint (count_vectorizer_vocab)","b7ed3e17":"count_vectorizer_stop_words = count_vectorizer.get_stop_words()\nprint (count_vectorizer_stop_words)\n# There are no stop words since we are doing sentiment analysis","5258181e":"# Sum up the counts of each vocabulary word\ndist = np.sum(count_vectorizer_data_features, axis=0)\n\n# For each, print the vocabulary word and the number of times it \n# appears in the training set\nfor tag, count in zip(count_vectorizer_vocab, dist):\n    print (count, tag)","7b840949":"# Initialize the \"TfidfVectorizer\" object\n# Convert a collection of raw documents to a matrix of TF-IDF features.\ntfidf_vectorizer = TfidfVectorizer(analyzer = \"word\",   \\\n                             tokenizer = None,    \\\n                             preprocessor = None, \\\n                             stop_words = None,   \\\n                             max_features = 5000,\n                             min_df=5, \n                             max_df=0.7,\n                             ngram_range=(1,2)) \n\n# fit_transform() does two functions: First, it fits the model\n# and learns the vocabulary; second, it transforms our training data\n# into feature vectors. The input to fit_transform should be a list of \n# strings.\ntfidf_vectorizer_data_features = tfidf_vectorizer.fit_transform(clean_reviews)\n\n# Numpy arrays are easy to work with, so convert the result to an \n# array\ntfidf_vectorizer_data_features = tfidf_vectorizer_data_features.toarray()","ee524969":"print (tfidf_vectorizer_data_features.shape)\nprint(tfidf_vectorizer_data_features)","dfa48bb3":"# Take a look at the words in the vocabulary\ntfidf_vectorizer_vocab = tfidf_vectorizer.get_feature_names()\nprint (tfidf_vectorizer_vocab)","1c698c0d":"tfidf_vectorizer_stop_words = tfidf_vectorizer.get_stop_words()\nprint (tfidf_vectorizer_stop_words)","0a159491":"# Sum up the counts of each vocabulary word\ntf_df_dist = np.sum(tfidf_vectorizer_data_features, axis=0)\n\n# For each, print the vocabulary word and the number of times it \n# appears in the training set\nfor tag, count in zip(tfidf_vectorizer_vocab, tf_df_dist):\n    print (count, tag)","7bf2a272":"tweet_data_relevant.head()","4d92521d":"y = tweet_data_relevant['airline_sentiment']\nprint(y.value_counts())\nplt.figure(figsize=(20,5)) \ng = sns.countplot(y)\n# No, Here we can see that the data is not balanced, There are lot of negative sentiments","d37012d9":"x = count_vectorizer_data_features     # Predictor feature columns\ny = tweet_data_relevant['airline_sentiment']   # Predicted class\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1)  # 1 is just any random seed number","a1aa32c1":"print(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)\nprint(y_test.shape)","cd2bd593":"smt = SMOTE(random_state=0)\nX_train_SMOTE, y_train_SMOTE = smt.fit_sample(x_train, y_train)","58717902":"print(X_train_SMOTE.shape)\nprint(y_train_SMOTE.shape)","4dc5c267":"y_train_SMOTE","ccdbdc00":"after_smote_airline_sentiment=pd.DataFrame(y_train_SMOTE, columns=['airline_sentiment']) \ny = after_smote_airline_sentiment['airline_sentiment']\nprint(y.value_counts())\nplt.figure(figsize=(20,5)) \ng = sns.countplot(y)\n# Here we can see that after applying smote, the data is balanced","3eaca239":"# Dividing the test data into test and validation set in 50-50 ratio\nx_validation, x_test_main, y_validation, y_test_main = train_test_split(x_test, y_test, test_size=0.50, random_state=1)\n\nprint(x_validation.shape)\nprint(x_test_main.shape)\n\n\nprint(y_validation.shape)\nprint(y_test_main.shape)\n\n# There are 2185 samples for validation and 2186 samples for testing","c6cea9db":"# Initialize a Random Forest classifier with 100 trees\nrandomforestclassifier = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n                       criterion='gini', max_depth=None, max_features='auto',\n                       max_leaf_nodes=None, max_samples=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, n_estimators=100,\n                       n_jobs=None, oob_score=False, random_state=1, verbose=0,\n                       warm_start=False) \n# Fit the forest to the training set, using the bag of words as \n# features and the sentiment labels as the response variable\n\nprint (\"Training the random forest...\")\nrandomforestclassifier = randomforestclassifier.fit( X_train_SMOTE, y_train_SMOTE)","3fdda32b":"randomforestclassifier.score(X_train_SMOTE, y_train_SMOTE)","6fc2aa12":"print (np.mean(cross_val_score(randomforestclassifier,X_train_SMOTE, y_train_SMOTE,cv=10)))","f51dd65b":"# Make class predictions for the Validation set\ny_validation_predict= randomforestclassifier.predict(x_validation)","cfe820e9":"print(\"Trainig accuracy\",randomforestclassifier.score(X_train_SMOTE,y_train_SMOTE))  \nprint()\nprint(\"Validation accuracy\",randomforestclassifier.score(x_validation, y_validation))\nprint()","17f3d810":"print(metrics.classification_report(y_validation,y_validation_predict))","49d9f43a":"cm=confusion_matrix(y_validation_predict , y_validation)\nplt.figure()\nplot_confusion_matrix(cm,figsize=(12,8), hide_ticks=True,cmap=plt.cm.Reds)\nplt.xticks(range(2), ['Negative', 'Neutral', 'Positive'], fontsize=16,color='black')\nplt.yticks(range(2), ['Negative', 'Neutral', 'Positive'], fontsize=16)\nplt.show()","85793c24":"print(\"Testing accuracy\",randomforestclassifier.score(x_test_main, y_test_main))\nprint()","0a765a01":"# Make class predictions for the Validation set\ny_test_predict= randomforestclassifier.predict(x_test_main)\n\nprint(metrics.classification_report(y_test_main,y_test_predict))","8687ef35":"cm=confusion_matrix(y_test_predict , y_test_main)\nplt.figure()\nplot_confusion_matrix(cm,figsize=(12,8), hide_ticks=True,cmap=plt.cm.Reds)\nplt.xticks(range(2), ['Negative', 'Neutral', 'Positive'], fontsize=16,color='black')\nplt.yticks(range(2), ['Negative', 'Neutral', 'Positive'], fontsize=16)\nplt.show()","3535ddae":"x_tf_idf = tfidf_vectorizer_data_features     # Predictor feature columns\ny_tf_idf = tweet_data_relevant['airline_sentiment']   # Predicted class\n\nx_train_tf_idf, x_test_tf_idf, y_train_tf_idf, y_test_tf_idf = train_test_split(x_tf_idf, y_tf_idf, test_size=0.3, random_state=1)  # 1 is just any random seed number","9e37935a":"print(x_train_tf_idf.shape)\nprint(y_train_tf_idf.shape)\nprint(x_test_tf_idf.shape)\nprint(y_test_tf_idf.shape)","f947c2c1":"y = tweet_data_relevant['airline_sentiment']\nprint(y.value_counts())\nplt.figure(figsize=(20,5)) \ng = sns.countplot(y)\n# No, Here we can see that the data is not balanced, There are lot of negative sentiments","d6b2dd56":"tf_idf_smt = SMOTE(random_state=0)\nX_train_tf_idf_SMOTE, y_train_tf_idf_SMOTE = smt.fit_sample(x_train_tf_idf, y_train_tf_idf)","991f6cbd":"print(X_train_tf_idf_SMOTE.shape)\nprint(y_train_tf_idf_SMOTE.shape)","9740b4af":"after_smote_airline_sentiment_tf_idf=pd.DataFrame(y_train_tf_idf_SMOTE, columns=['airline_sentiment']) \ny = after_smote_airline_sentiment_tf_idf['airline_sentiment']\nprint(y.value_counts())\nplt.figure(figsize=(20,5))\ng = sns.countplot(y)\n# Here we can see that after smote , the data is balanced","a73b2aa5":"# Dividing the test data into test and validation set in 50-50 ratio\nx_validation_tf_idf, x_test_main_tf_idf, y_validation_tf_idf, y_test_main_tf_idf = train_test_split(x_test_tf_idf, y_test_tf_idf, test_size=0.50, random_state=1)\n\nprint(x_validation_tf_idf.shape)\nprint(x_test_main_tf_idf.shape)\n\n\nprint(y_validation_tf_idf.shape)\nprint(y_test_main_tf_idf.shape)\n\n# There are 2185 samples for validation and 2186 samples for testing","e208b6e4":"# Initialize a Random Forest classifier with 100 trees\nrandomforestclassifier_tf_idf = RandomForestClassifier(verbose=1,n_jobs=-1,n_estimators = 100) \n# Fit the forest to the training set\n\nprint (\"Training the random forest...\")\nrandomforestclassifier_tf_idf = randomforestclassifier_tf_idf.fit( X_train_tf_idf_SMOTE, y_train_tf_idf_SMOTE)","60181d23":"randomforestclassifier_tf_idf.score(X_train_tf_idf_SMOTE, y_train_tf_idf_SMOTE)","4ae86d7f":"print (np.mean(cross_val_score(randomforestclassifier_tf_idf,X_train_tf_idf_SMOTE, y_train_tf_idf_SMOTE,cv=10)))","ba1ce146":"# Make class predictions for the Validation set\ny_validation_predict_tf_idf= randomforestclassifier_tf_idf.predict(x_validation_tf_idf)","81693f79":"print(\"Trainig accuracy\",randomforestclassifier_tf_idf.score(X_train_tf_idf_SMOTE,y_train_tf_idf_SMOTE))  \nprint()\nprint(\"Validation accuracy\",randomforestclassifier_tf_idf.score(x_validation_tf_idf, y_validation_tf_idf))\nprint()","0bd6aec0":"print(metrics.classification_report(y_validation_tf_idf,y_validation_predict_tf_idf))","516855b4":"cm=confusion_matrix(y_validation_predict_tf_idf , y_validation_tf_idf)\nplt.figure()\nplot_confusion_matrix(cm,figsize=(12,8), hide_ticks=True,cmap=plt.cm.Reds)\nplt.xticks(range(2), ['Negative', 'Neutral', 'Positive'], fontsize=16,color='black')\nplt.yticks(range(2), ['Negative', 'Neutral', 'Positive'], fontsize=16)\nplt.show()","0e2f5acd":"print(\"Test accuracy\",randomforestclassifier_tf_idf.score(x_test_main_tf_idf, y_test_main_tf_idf))\nprint()","a5265b5b":"# Make class predictions for the test set\ny_test_predict_tf_idf= randomforestclassifier_tf_idf.predict(x_test_main_tf_idf)\nprint(metrics.classification_report(y_test_main_tf_idf,y_test_predict_tf_idf))","1d30fee1":"cm=confusion_matrix(y_test_predict_tf_idf , y_test_main_tf_idf)\nplt.figure()\nplot_confusion_matrix(cm,figsize=(12,8), hide_ticks=True,cmap=plt.cm.Reds)\nplt.xticks(range(2), ['Negative', 'Neutral', 'Positive'], fontsize=16,color='black')\nplt.yticks(range(2), ['Negative', 'Neutral', 'Positive'], fontsize=16)\nplt.show()","f97332ae":"# Pre-processing steps in NLP\n#      1. HTML tag cleanup\n#                          -  It returns all the text in a document or beneath a tag, as a single Unicode string:\n#      2. Contraction\n#                          - Contractions are shortened version of words or syllables. \n#                          - In case of English contractions are often created by removing one of the vowels from the word. \n#                          - Examples would be, do not to don\u2019t and I would to I\u2019d. Converting each contraction to its expanded, original form helps with text standardization.\n#      3. Tokenization\n#                          - Tokenization is a step which splits longer strings of text into smaller pieces, or tokens. \n#                          - Larger chunks of text can be tokenized into sentences, sentences can be tokenized into words, etc. \n#                          - Further processing is generally performed after a piece of text has been appropriately tokenized. \n#                          - Tokenization is also referred to as text segmentation or lexical analysis.\n#                          - Sometimes segmentation is used to refer to the breakdown of a large chunk of text into pieces larger than words (e.g. paragraphs or sentences), \n#                                        while tokenization is reserved for the breakdown process which results exclusively in words.\n#      4. Removing numbers\n#                          - Remove numbers from list of tokenized words\n#      5. Remove special characters\n#                          - Remove special characters from list of tokenized words\n#      6. Remove punctuation\n#                          - Remove punctuation from list of tokenized words\n#      7. Convert text to lower case\n#                          - converting all text to the same case \n#      8. Remove empty strings\n#                          - Remove empty string from list of tokenized words\n#      9. Stemming\n#                          - Converting the words into their base word or stem word ( Ex - tastefully, tasty, these words are converted to stem word called 'tasti'). \n#                            This reduces the vector dimension because we dont consider all similar words\n#      10.Lemmatization\n#                          - Lemmatization, unlike Stemming, reduces the inflected words properly ensuring that the root word belongs to the language. \n#                            In Lemmatization root word is called Lemma. A lemma (plural lemmas or lemmata) is the canonical form, dictionary form, or citation form of a set of words.","0c8a735f":"# Steps to do after text pre-processing:\n#Techniques for Encoding - These are the popular techniques that are used for encoding:\n\n#       o Bag of words         (CountVectorization)\n#                              In BoW we construct a dictionary that contains set of all unique words from our text review dataset. \n#                              The frequency of the word is counted here. If there are d unique words in our dictionary then for every sentence or review the vector will be of length d \n#                              and count of word from review is stored at its particular location in vector. The vector will be highly sparse in such case.\n#       o Tf-idf               (TfIdfVectorization)  (Term Frequency - Inverse Document Frequency)\n#                               Term Frequency - Inverse Document Frequency it makes sure that less importance is given to most frequent words and also considers less frequent words.\n#                               Term Frequency is number of times a particular word(W) occurs in a review divided by totall number of words (Wr) in review. The term frequency value ranges from 0 to 1.\n#                               Inverse Document Frequency is calculated as log(Total Number of Docs(N) \/ Number of Docs which contains particular word(n)). Here Docs referred as Reviews.\n#                               TF-IDF is TF * IDF that is (W\/Wr)*LOG(N\/n)","174ea498":"print(\"Trainig accuracy\",randomforestclassifier.score(X_train_SMOTE,y_train_SMOTE))  \nprint()\nprint(\"Testing accuracy\",randomforestclassifier.score(x_test_main, y_test_main))\nprint()\n\ny_test_predict= randomforestclassifier.predict(x_test_main)\nprint(metrics.classification_report(y_test_main,y_test_predict))\n\ncm=confusion_matrix(y_test_predict , y_test_main)\nplt.figure()\nplot_confusion_matrix(cm,figsize=(12,8), hide_ticks=True,cmap=plt.cm.Reds)\nplt.xticks(range(2), ['Negative', 'Neutral', 'Positive'], fontsize=16,color='black')\nplt.yticks(range(2), ['Negative', 'Neutral', 'Positive'], fontsize=16)\nplt.show()","b42bbd88":"print(\"Trainig accuracy\",randomforestclassifier_tf_idf.score(X_train_tf_idf_SMOTE,y_train_tf_idf_SMOTE))  \nprint()\nprint(\"Test accuracy\",randomforestclassifier_tf_idf.score(x_test_main_tf_idf, y_test_main_tf_idf))\nprint()\n\n# Make class predictions for the test set\ny_test_predict_tf_idf= randomforestclassifier_tf_idf.predict(x_test_main_tf_idf)\nprint(metrics.classification_report(y_test_main_tf_idf,y_test_predict_tf_idf))\n\ncm=confusion_matrix(y_test_predict_tf_idf , y_test_main_tf_idf)\nplt.figure()\nplot_confusion_matrix(cm,figsize=(12,8), hide_ticks=True,cmap=plt.cm.Reds)\nplt.xticks(range(2), ['Negative', 'Neutral', 'Positive'], fontsize=16,color='black')\nplt.yticks(range(2), ['Negative', 'Neutral', 'Positive'], fontsize=16)\nplt.show()","62833cc6":"Accuracy=[]\nModel=[]\nAccuracy.append(randomforestclassifier.score(x_test_main, y_test_main))\nAccuracy.append(randomforestclassifier_tf_idf.score(x_test_main_tf_idf, y_test_main_tf_idf))\nModel.append(\"RandomForestClassifier on CountVectorizer\")\nModel.append(\"RandomForestClassifier on TfidfVectorizer\")\n\n\nindex=[0,1]\nplt.bar(index,Accuracy,color='rgbyk')\nplt.xticks(index,Model,rotation=45)\nplt.ylabel('Accuracy')\nplt.xlabel('Model')\nplt.title('Classifier Accuracies')\n\nxlocs, xlabs = plt.xticks()\n\nfor i, v in enumerate(Accuracy):\n    plt.text(xlocs[i] - 0.25, v + 0.01, str(v))\n\n# The RandomForestClassfier on TfidfVectorizer is having better accuracy","44970e97":"**Dividing Data into train and Test**","e53f13f4":"**Evaluate score by cross-validation**","fc99e842":"**Classification Report**  (Validation Set)","2aa3f7c7":"**RandomForestClassifier on TfidfVectorizer**","fefe31f7":"**Confusion Matrix** (Test Set)","67690a61":"**Checking if data is balanced after applying SMOTE**","50e32301":"**Remove empty String**","10e32ddb":"**Initialize RandomForestClassifier**","dc165aa0":"**Vectorization: )**\n\n\na. Use CountVectorizer.\n\nb. Use TfidfVectorizer","e9f35dd4":"**Testing Data Accuracy**","0469a42e":"**Test Accuracy**","744d7986":"**Explorotary data Analysis   EDA**","ed55bece":"**Summary Part 2**\n\n*   There data is not balanced, The percentage of negative sentiment is high\n*   There are 14568 rows in the dataset after duplicate data cleanup","575c7875":"**Data Description:** \n\n\nA sentiment analysis job about the problems of each major U.S. airline. Twitter data was scraped from February of 2015 and contributors were asked to first classify positive, negative, and neutral tweets, followed by categorizing negative reasons (such as \"late flight\" or \"rude service\"). ","4fc29000":"**Stop words**","3a44b890":"**Most Common negative review reasons**","98ab4930":"**Is the data balanced**","67424376":"**Dividing Test data to Test and Validation Data**","03764241":"**Classification Report**  (Test Set)","4b3722bc":"**Dividing Data to Train and Test**","eb3444a3":"**Printing first 5 rows of data**","0f081cee":"**Shape of data**","4ffb06d2":"**Fit and evaluate model using both type of vectorization. **","c1ecf861":"**CountVectorizer**","49598be4":"**Conversion to Lower case**","f81f740d":"**Drop Irrelevant columns**","5d83aac3":"**Text pre-processing: Data preparation. **\n\n*\ta. Html tag removal. \n*\tb. Tokenization. \n*\tc. Remove the numbers. \n*\td. Removal of Special Characters and Punctuations. \n*\te. Conversion to lowercase. \n*\tf. Lemmatize or stemming. \n*\tg. Join the words in the list to convert back to text string in the dataframe. (So that each row contains the data in text format.) \n*\th. Print first 5 rows of data after pre-processing.","3e763590":"**Checking if data is duplicated and removing duplicates**","b6c1934a":"**Applying SMOTE since the data is not balanced**","68e69111":"**Print first 5 rows of data after pre-processing**","337f75c7":"**RandomForest Classifier on CountVectorizer**","61452fe3":"**Load dataset**","c4e177c2":"**Remove special characters and punctuations**","e3b4084b":"**Tokenization**","6930d1f4":"**Description of Data**","766e71e5":"**Html tag removal**","9bb4af46":"**Summarize your understanding of the application of Various Pre-processing and Vectorization and\nperformance of your model on this dataset.**\n","31f7d2ee":"**Sum up the counts of each vocabulary word**","c97ebf61":"# <font color='blue'>Project - NLP: Sentiment Analysis : Twitter US Airline Sentiment<\/font>\n\n\n*Prasad Menon*","ff9634e3":"**Summary**\n\n---\n\n","9fe0f294":"**Understand of data-columns:**\n\na. Drop all other columns except \u201ctext\u201d and \u201cairline_sentiment\u201d. \n\nb. Check the shape of data. \n\nc. Print first 5 rows of data. ","61738ee8":"**Training and Validation Accuracy**","3693f27b":"**Stop Words**","f231c40b":"**Objective:** \n\n\nTo implement the techniques learnt as a part of the course. ","8b24a71c":"**Confusion Matrix** (Test Set)","f861b64a":"**Join the words in the list to convert back to text string in the dataframe**\n\n\n(So that each row contains the data in text format.)","ba732f5e":"**RandomForestClassifier on CountVectorizer**","3927c81f":"# <font color='blue'>Step 1<\/font>","233c9f4d":"**Dataset** \n\n\nThe project is from a dataset from Kaggle.  \nLink to the Kaggle project site: https:\/\/www.kaggle.com\/crowdflower\/twitter-airline-sentiment  \nThe dataset has to be downloaded from the above Kaggle website. \n\n\nThe dataset has the following columns: \n\n*\ttweet_id\n*\tairline_sentiment\n*\tairline_sentiment_confidence\n*\tnegativereason \n*\tnegativereason_confidence \n*\tairline \n*\tairline_sentiment_gold \n*\tname \n*\tnegativereason_gold \n*\tretweet_count \n*\ttext \n*\ttweet_coord \n*\ttweet_created \n*\ttweet_location \n*\tuser_timezone","accb1ecf":"**Stemming**","6e037968":"**Training and Validation Accuracy**","3537a9d8":"**Sum up the counts of each vocabulary word**","97ec52fb":"**Import the libraries**","6b74963d":"**Shape of Data**","9a58f296":"**Sentiment Analysis for each Airline**","9706fcec":"**Confusion Matrix** (Validation Set)","d0bb4fc3":"**Vocabulary**","f8d61bb7":"**TfidfVectorizer**","c09858f3":"**Vocabulary**","a82c68fc":"# <font color='blue'>Step 3<\/font>","36d57c66":"**Complete Pre-preocessing**","700e6510":"**Import the libraries, load dataset, print shape of data, data description**","288bf3a4":"**Classification Report**  (Validation Set)","e61e7561":"**Lemmatization**","86f94183":"**Summary Part 1**\n\n---\n\n\n\n*   There are 14640 rows and 15 columns in the tweet data\n*   There are 36 duplicate records in the data set\n*   Here we can see that United Airlines, US Airways, American Airlines has the most number of negative review\n*   Virgin America has the least number of negative reviews\n*   Customer service and Late flight seems to be the main reason why customers are giving bad feedback\n\n","9b21cd9b":"**Classification Report**  (Test Set)","c89bc96e":"**Learning Outcomes:**\n\n\n*\tBasic understanding of text pre-processing. \n*\tWhat to do after text pre-processing:  \n*\t\tBag of words \n*\t\tTf-idf \n*\tBuild the classification model. \n*\tEvaluate the Model.","21dfb7a6":"**Replace Contraction**","55126f99":"# <font color='blue'>Step 4<\/font>","b5528c22":"**Summary**\n\n---\n\n\n\n\n\n*   RandomForestClassifier on TfidfVectorizer has an accuracy of 77% whereas \nRandomForestClassifier on CountVectorizer has an accuracy of 75%. This shows that the model using TfidfVectorizer is better.\n*   Model is able to predict airline sentiment correctly 77% of the time\n*   United Airlines, US Airways, American Airlines has the most number of negative review\n*   Virgin America has the least number of negative reviews\n*   Customer service and Late flight seems to be the main reason why customers are giving bad feedback\n\n\n","acd19e6c":"# <font color='blue'>Step 5<\/font>","060e1737":"**Techniques for Encoding**","7bf13796":"**Evaluate score by cross-validation**","f1578da4":"# <font color='blue'>Step 6<\/font>","4b4bd16c":"**Applying SMOTE since the data is not balanced**","2cd62c66":"**Removal of Numbers**","5bfbd5e2":"**Pre-processing steps in NLP**","d957b665":"**Confusion Matrix** (Validation Set)","44aaac12":"**Initialize RandomForestClassifier**","43ef0133":"**Random Forest Classifier on TfidfVectorizer**","7e8c5e51":"**Performance of Classification Model**","f7c3e834":"# <font color='blue'>Step 2<\/font>"}}