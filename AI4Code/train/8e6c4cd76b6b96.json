{"cell_type":{"00f0680d":"code","03412489":"code","a33ec0c1":"code","7bfc0ca1":"code","eac0d944":"code","31c5af6f":"code","3aee28b2":"code","6df05700":"code","32d31343":"code","99e050f5":"code","b86a7a71":"code","b7518b59":"code","1fe53660":"code","e117dbaa":"code","ce250c7c":"code","a87d1b44":"code","a7e54da1":"code","dfc5f07b":"code","46d963ae":"markdown","c1d0ff21":"markdown","e7ea0cbc":"markdown","508bd74a":"markdown","6d9e9a0e":"markdown","20badc9c":"markdown","158ce9b5":"markdown","15f0f7b6":"markdown","9e12d8c5":"markdown","e6940828":"markdown","d07b75fa":"markdown","34c15721":"markdown","cde820d1":"markdown","3c92aa69":"markdown","71fa11a7":"markdown","95158dd9":"markdown","a7f3411b":"markdown","62ed1af3":"markdown","6ac3eb78":"markdown","8cf72cf5":"markdown","78781b23":"markdown","d6593b64":"markdown","7e06920d":"markdown","c3931f5c":"markdown","affb46fd":"markdown","b410d393":"markdown"},"source":{"00f0680d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')","03412489":"path = '\/kaggle\/input\/credit-card-customers\/'\n\ndata = pd.read_csv(path + 'BankChurners.csv')\n\n# drop last two columns\ndata = data.iloc[:, :-2]\ndata.head()","a33ec0c1":"print(f\"\"\"\n    No. of samples  : {data.shape[0]}\n    No. of features : {data.shape[1]}\n    Missing values  : {data.isnull().sum().sum()}\n\"\"\")","7bfc0ca1":"data.select_dtypes(include='object').columns.to_list()","eac0d944":"data['Attrition_Flag'].replace({'Existing Customer': 0, 'Attrited Customer': 1}, inplace=True)\n\ndata['Gender'].replace({'M': 1, 'F': 0})\n\neducation_mapping = {\n    'Unknown': 0, 'Uneducated': 1, 'High School': 2, 'College': 3, \n    'Graduate': 4, 'Post-Graduate': 5, 'Doctorate': 6\n}\ndata['Education_Level'].replace(education_mapping, inplace=True)\n\nincome_mapping= {\n    'Less than $40K': 'less_than_40k', '$40K - $60K': '40k_60k', '$60K - $80K': '60k_80k', \n    '$80K - $120K': '80k_120k', '$120K +': 'greater_than_120k',\n}\ndata['Income_Category'].replace(income_mapping, inplace=True)","31c5af6f":"data_dummies = pd.get_dummies(data)","3aee28b2":"data.drop('CLIENTNUM', axis=1, inplace=True)\n\ndata_dummies.drop(['Gender_F', 'Marital_Status_Unknown', 'Income_Category_Unknown'], axis=1, inplace=True)","6df05700":"print(f\"\"\"\n    No. of samples  : {data.shape[0]}\n    No. of features : {data.shape[1]}\n    Missing values  : {data.isnull().sum().sum()}\n\"\"\")","32d31343":"df_skew = pd.DataFrame(data_dummies.skew(), columns=['Skewness']).sort_values(by='Skewness')\ndf_skew.head()","99e050f5":"## Get all the numeric features in out dataset\nnumeric_features = data_dummies.skew().index\n\n## We do not want to touch our target feature\nif 'Attrition_Flag' in numeric_features:\n    numeric_features = numeric_features.drop('Attrition_Flag')\n    \n## Getting all the skewed features (skew > 0.5 or skew < -0.5)\nskewed_features = data_dummies[numeric_features].skew()[np.abs(data_dummies[numeric_features].skew()) > 0.5].index\n\n## Performing log(1+x) transformation\ndata_dummies[skewed_features] = np.log1p(data_dummies[skewed_features])","b86a7a71":"# Get the correlation dataframe\ndf_corr = data.corr()\n\n# Plot the heatmap\nfig, ax = plt.subplots(figsize=(10, 8))\nmask    = np.triu(np.ones_like(df_corr, dtype=np.bool))\nsns.heatmap(\n    df_corr, mask=mask, annot=True, fmt=\".2f\", vmin = -1, vmax = 1,\n    cmap=sns.diverging_palette(150, 275, s=80, l=55, n=9)\n)\nplt.show()","b7518b59":"# Separate the independent and dependent variable\nX = data_dummies.drop(\"Attrition_Flag\", axis = 1)\ny = data_dummies[\"Attrition_Flag\"]\n\n# Get the training and testing pairs\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","1fe53660":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\nfs = SelectKBest(score_func=chi2, k='all')\nfs.fit(X_train, y_train)\nX_train_fs = fs.transform(X_train)\nX_test_fs = fs.transform(X_test)\n\nfor i in range(len(fs.scores_)):\n    print('Feature %d: %f' % (i, fs.scores_[i]))\n# plot the scores\nplt.bar([i for i in range(len(fs.scores_))], fs.scores_)\nplt.show()","e117dbaa":"X_train = X_train.iloc[:, [5, 6, 7, 9, 12, 13, 14, 15]]\nX_test = X_test.iloc[:, [5, 6, 7, 9, 12, 13, 14, 15]]","ce250c7c":"from sklearn.ensemble import RandomForestClassifier\n\nregressor = RandomForestClassifier(n_estimators=20, random_state=0)\nregressor.fit(X_train, y_train)\ny_pred = regressor.predict(X_test)","a87d1b44":"from sklearn.metrics import confusion_matrix\n\narr_cm = confusion_matrix(y_test, y_pred)\ndf_cm = pd.DataFrame(arr_cm, index=[0, 1], columns=[0, 1])\nfig = plt.figure(figsize=(4,3), dpi=120)\nsns.heatmap(df_cm, annot=True, fmt=\"d\")\nplt.show()","a7e54da1":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\nprint(classification_report(y_test, y_pred))","dfc5f07b":"print('Model Accuracy:', round(accuracy_score(y_test, y_pred)*100, 3), '%')","46d963ae":"**Dataset description after data preparation**","c1d0ff21":"# Data Understanding","e7ea0cbc":"# Training","508bd74a":"**Categorical features present in the dataset**","6d9e9a0e":"The above results give us the feature index value and their corresponding importance. Higher the value, more important the feature is.","20badc9c":"# Evaluation","158ce9b5":"**Applying `log transformation` to the highly skewed features**","15f0f7b6":"**Check the correlation among features using the heatmap**","9e12d8c5":"**Classification Report**","e6940828":"**Using strong ensemble model classifier - Random Forest**","d07b75fa":"### Do not forget to upvote : ) if you liked this notebook.!!","34c15721":"# Feature Engineering","cde820d1":"**Label Encoding the ordinal features**","3c92aa69":"**Chi Square test for feature extraction**\n\n[Reference link](https:\/\/machinelearningmastery.com\/feature-selection-with-categorical-data\/)","71fa11a7":"#### Step by step guide to create a Random Forest Classifier to predict the Customer Churning\n\n1. Data Understanding\n2. Data Preparation\n3. Feature Engineering\n4. Feature Extraction\n5. Training\n6. Evaluation","95158dd9":"**Accuracy Score**","a7f3411b":"**Importing the necessary libraries**","62ed1af3":"**One-Hot encoding the nominal features**","6ac3eb78":"# Data Preparation","8cf72cf5":"**Checking `skewness` of the numeric features**","78781b23":"# Credit Cards - Predicition of churning customers","d6593b64":"**Confusion Matrix**","7e06920d":"**Loading the dataset**","c3931f5c":"**Drop the below features**\n\n1. `CLIENTNUM` - Since, it is an ID parameter and not relevant for our analysis\n2. We can drop one feature from each of the `one-hot encoded` features. In our case, we are dropping -\n    * `Gender_F`\n    * `Marital_Status_Unknown`\n    * `Income_Category_Unknown`","affb46fd":"**Preliminary Data Description**","b410d393":"# Feature Extraction"}}