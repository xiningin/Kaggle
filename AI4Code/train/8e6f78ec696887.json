{"cell_type":{"2c0119da":"code","ec5abf97":"code","4cb1958a":"code","0aad8394":"code","b2b0b53e":"code","2e22eb59":"code","2deebe8f":"code","753425f6":"code","a3bc7b71":"code","a2bbdcb7":"code","a137cf10":"code","04236e7d":"code","bc84eba3":"code","dfe25487":"code","487d95f3":"code","f9024de2":"code","8925a0ca":"code","36c949c1":"code","c947f81e":"code","ed8381e0":"code","ccaaef8f":"code","a654eee2":"code","959bf056":"code","ea477742":"code","6aa8b640":"code","dbd0ada4":"code","f90fd9c4":"code","3fd39191":"code","7fd30d89":"code","7638c235":"code","d64eb8fa":"code","18b8c1b9":"code","bf65946e":"code","f847eda7":"code","768dc5c9":"code","0e708957":"code","92fe7e12":"code","4f699638":"code","f27823a8":"code","1d336b45":"code","04b57c5a":"code","8fc20c77":"code","f11fd53b":"code","1ed8e0e0":"code","4f641971":"code","c0a00672":"code","3a530cea":"code","721a8f1f":"code","949ec427":"code","c3def64c":"code","58f85146":"code","5c957e38":"code","bae84b56":"code","3cd501c8":"code","88101a8e":"code","6ec46405":"code","5d7734c8":"code","ffe0fda6":"code","c64dfcc8":"code","f87a6aa3":"code","860f7569":"code","33861e6a":"code","bc28dc85":"code","85329e15":"code","5305c588":"code","49dd2673":"code","f151e990":"code","e8899258":"code","6e28d129":"code","2fa21226":"code","1f2fb9f0":"code","69af0917":"code","baaaa4d3":"code","3bdb4eb5":"code","b1f94bdb":"code","f6b3361e":"code","b3e10be7":"code","c801cb92":"code","eadad96e":"code","afcc6bec":"code","c3dab281":"code","999ae940":"code","22eabd8a":"code","ee77f7ad":"code","537eed0b":"code","2caefbec":"code","cb1b7af8":"code","a7c07f9d":"code","99f2efd0":"code","0dab2560":"code","929a61d7":"code","bd06903c":"code","6a55fa5f":"code","4a620eae":"code","e275396a":"code","f82460ce":"code","99049850":"markdown","89ebc52e":"markdown","32a6fba4":"markdown","2959638e":"markdown","1f19ca20":"markdown","b66381d0":"markdown","d9bde943":"markdown","f7f60b21":"markdown","006c4e60":"markdown","0aee5a34":"markdown","9c3e3ecf":"markdown","39905a09":"markdown","d0cee0d5":"markdown","fd8649a2":"markdown","760a0b00":"markdown","f58864dc":"markdown","48823e2b":"markdown","777abe2d":"markdown","9a1c3eb6":"markdown","062f53ab":"markdown","2f6e54a9":"markdown","0bc1a0ba":"markdown","3251ca5f":"markdown","32500038":"markdown"},"source":{"2c0119da":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","ec5abf97":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","4cb1958a":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv', index_col='PassengerId')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv', index_col='PassengerId')","0aad8394":"# Show train dataframe\ntrain.head()","b2b0b53e":"# Show test dataframe\ntest.head()","2e22eb59":"# dfs info.\ntrain.info()","2deebe8f":"test.info()","753425f6":"# Check NANs in Train Dataset\ntrain.isnull().sum()","a3bc7b71":"# firstly, we can drop the 2 records with NaN Embarked value\ntrain = train[train['Embarked'].notnull()]","a2bbdcb7":"train.isnull().sum()","a137cf10":"# Check NANs in Test Dataset\ntest.isnull().sum()","04236e7d":"# Check Duplicates\ntrain.duplicated().sum()","bc84eba3":"test.duplicated().sum()","dfe25487":"# Survived vs Not\ntrain['Survived'].value_counts()\/train.shape[0]","487d95f3":"# Survived vs Not\nplt.figure(figsize=(7,7))\nplt.pie(train['Survived'].value_counts(), labels=['Not Survived', 'Survived']);","f9024de2":"train['Pclass'].value_counts()","8925a0ca":"train['Pclass'].value_counts().plot(kind='barh');\nplt.title('Number of Passengers in Each Class');","36c949c1":"train.groupby(['Pclass'])['Survived'].value_counts(sort=False)","c947f81e":"train.groupby(['Pclass'])['Survived'].value_counts(sort=False).plot(kind='barh');\nplt.title('Survived vs Not Survived in Each Class');","ed8381e0":"train.groupby(['Sex'])['Survived'].value_counts(sort=False)","ccaaef8f":"train.groupby(['Sex'])['Survived'].value_counts(sort=False).plot(kind='barh');\nplt.title('Survived vs Not Survived for Each Sex');","a654eee2":"train.groupby(['SibSp'])['Survived'].value_counts(sort=False)","959bf056":"train.groupby(['SibSp'])['Survived'].value_counts(sort=False).plot(kind='barh');\nplt.title('Survived vs Not Survived for Number of Siblings or Spouses');","ea477742":"train.groupby(['Parch'])['Survived'].value_counts(sort=False)","6aa8b640":"train.groupby(['Parch'])['Survived'].value_counts(sort=False).plot(kind='barh');\nplt.title('Survived vs Not Survived for Number of Parents or Children');","dbd0ada4":"train['Fare'].astype(int).value_counts(sort=False)","f90fd9c4":"train['Fare'].astype(int).describe()","3fd39191":"train['Embarked'].value_counts()\/train.shape[0]","7fd30d89":"train.groupby(['Embarked'])['Survived'].value_counts(sort=False)","7638c235":"train.groupby(['Embarked'])['Survived'].value_counts(sort=False).plot(kind='barh');\nplt.title('Survived vs Not Survived for Each Port of Embarkation');","d64eb8fa":"train.groupby(['Pclass'])['Age'].describe()","18b8c1b9":"train.groupby(['Pclass'])['Age'].describe()","bf65946e":"# Taking copies\ntrain_copy = train.copy()\ntest_copy = test.copy()","f847eda7":"# Age imputed using median age of each Pclass\nfor i in range(1,4):\n    train_copy.loc[(train_copy['Pclass'] == i) & (train_copy['Age'].isnull()), 'Age'] = train_copy.loc[train_copy['Pclass'] == i, 'Age'].median()\n    test_copy.loc[(test_copy['Pclass'] == i) & (test_copy['Age'].isnull()), 'Age'] = test_copy.loc[test_copy['Pclass'] == i, 'Age'].median()","768dc5c9":"train_copy.isnull().sum()","0e708957":"test_copy.isnull().sum()","92fe7e12":"# Test Fare imputed using mean fare\ntest_copy['Fare'].fillna(test_copy['Fare'].mean(), inplace=True)","4f699638":"test_copy.isnull().sum()","f27823a8":"# Dropping Cabin feature\ntrain_copy.drop('Cabin', axis=1, inplace=True)\ntest_copy.drop('Cabin', axis=1, inplace=True)","1d336b45":"train_copy.isnull().sum()","04b57c5a":"test_copy.isnull().sum()","8fc20c77":"# Tickets differ, each ticket gives an indication for the class, place and cabin by some-how\n# So, we can convert it's numbers length to a feature\ntrain_copy['Ticket_Type'] = train_copy['Ticket'].str.extract(r'(\\d{3,8})')\ntest_copy['Ticket_Type'] = test_copy['Ticket'].str.extract(r'(\\d{3,8})')","f11fd53b":"train_copy['Ticket_Type'] = train_copy['Ticket_Type'].apply(lambda x : len(str(x)))\ntest_copy['Ticket_Type'] = test_copy['Ticket_Type'].apply(lambda x : len(str(x)))","1ed8e0e0":"train_copy.info()","4f641971":"test_copy.isnull().sum()","c0a00672":"# Drop Ticket\ntrain_copy.drop('Ticket', axis=1, inplace=True)\ntest_copy.drop('Ticket', axis=1, inplace=True)","3a530cea":"# We can Extract Titles of Name\ntrain_copy['Title'] = train_copy['Name'].str.extract(r',\\s(\\w+)')\ntest_copy['Title'] = test_copy['Name'].str.extract(r',\\s(\\w+)')","721a8f1f":"train_copy['Title'].value_counts()","949ec427":"train_copy['Title'] = train_copy['Title'].replace(['Dr', 'Rev', 'Major', 'Col', 'Lady', 'Sir', 'Jonkheer', 'the', 'Don', 'Capt', 'Ms'], 'Others')\ntrain_copy['Title'] = train_copy['Title'].replace(['Mlle', 'Mme'], 'Mrs')","c3def64c":"train_copy['Title'].value_counts()","58f85146":"test_copy['Title'].value_counts()","5c957e38":"test_copy['Title'] = test_copy['Title'].replace(['Dr', 'Rev', 'Col', 'Dona', 'Ms'], 'Others')","bae84b56":"test_copy['Title'].value_counts()","3cd501c8":"# Drop Name Feature\ntrain_copy.drop('Name', axis=1, inplace=True)\ntest_copy.drop('Name', axis=1, inplace=True)","88101a8e":"# we can get a Married_Ladies with children out of Title and Parch Features as they have the priority in Life saving\ntrain_copy['Married_Lady'] = train_copy['Title'].apply(lambda x : 1 if x == 'Mrs' else 0)\ntest_copy['Married_Lady'] = test_copy['Title'].apply(lambda x : 1 if x == 'Mrs' else 0)\n\ntrain_copy['With_Children'] = train_copy['Parch'].apply(lambda x : 1 if x > 0 else 0)\ntest_copy['With_Children'] = test_copy['Parch'].apply(lambda x : 1 if x > 0 else 0)\n\ntrain_copy['Married_With_Childern'] = train_copy['Married_Lady'] + train_copy['With_Children']\ntest_copy['Married_With_Childern'] = test_copy['Married_Lady'] + test_copy['With_Children']\n\ntrain_copy['Married_With_Childern'] = train_copy['Married_With_Childern'].apply(lambda x : 1 if x == 2 else 0)\ntest_copy['Married_With_Childern'] = test_copy['Married_With_Childern'].apply(lambda x : 1 if x == 2 else 0)","6ec46405":"# Drop With_Children feature\ntrain_copy.drop('With_Children', axis=1, inplace=True)\ntest_copy.drop('With_Children', axis=1, inplace=True)","5d7734c8":"train_copy.info()","ffe0fda6":"# We can get number of one family on Board\ntrain_copy['Family_Members'] = train_copy['SibSp'] + train_copy['Parch'] + 1\ntest_copy['Family_Members'] = test_copy['SibSp'] + test_copy['Parch'] + 1","c64dfcc8":"train_copy['Family_Members'].value_counts()","f87a6aa3":"# We can get if its single or not \ntrain_copy['Single'] = train_copy['Family_Members'].apply(lambda x: 1 if x == 1 else 0)\ntest_copy['Single'] = test_copy['Family_Members'].apply(lambda x: 1 if x == 1 else 0)","860f7569":"# Drop SibSp and Parch column\ntrain_copy.drop(['SibSp', 'Parch'], axis=1, inplace=True)\ntest_copy.drop(['SibSp', 'Parch'], axis=1, inplace=True)","33861e6a":"train_copy.info()","bc28dc85":"# We can get if Passenger is Old or Child or not, they have a priority in saving\ntrain_copy['Old'] = train_copy['Age'].apply(lambda x: 1 if x >= 55 else 0)\ntest_copy['Old'] = test_copy['Age'].apply(lambda x: 1 if x >= 55 else 0)\n\ntrain_copy['Child'] = train_copy['Age'].apply(lambda x: 1 if x <= 10 else 0)\ntest_copy['Child'] = test_copy['Age'].apply(lambda x: 1 if x <= 10 else 0)","85329e15":"# We can get if Passenger is rich or not, they have better chance to be in open air cabins, so better chance to save\ntrain_copy['Rich'] = train_copy['Fare'].apply(lambda x: 1 if x >= 200 else 0)\ntest_copy['Rich'] = test_copy['Fare'].apply(lambda x: 1 if x >= 200 else 0)","5305c588":"Catagorical_features = list(train_copy.select_dtypes(include=object).columns)","49dd2673":"Catagorical_features","f151e990":"#OneHot Encoding\nOneHot_Encoded_train = pd.get_dummies(train_copy)\nOneHot_Encoded_test = pd.get_dummies(test_copy)","e8899258":"OneHot_Encoded_train.info()","6e28d129":"OneHot_Encoded_test.info()","2fa21226":"#Labels Encoding\nLabels_Encoded_train = train_copy.copy()\nLabels_Encoded_test = test_copy.copy()\n\nfor column in Catagorical_features:\n    Labels_Encoded_train[column] = pd.factorize(Labels_Encoded_train[column])[0].reshape(-1, 1)\n    Labels_Encoded_test[column] = pd.factorize(Labels_Encoded_test[column])[0].reshape(-1, 1)","1f2fb9f0":"Labels_Encoded_train.info()","69af0917":"Labels_Encoded_test.info()","baaaa4d3":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score","3bdb4eb5":"x = OneHot_Encoded_train.drop('Survived', axis=1)\ny = OneHot_Encoded_train['Survived']","b1f94bdb":"x_train, x_valid, y_train, y_valid = train_test_split(x, y , test_size = 0.2, stratify = y, random_state = 42)","f6b3361e":"# Model Using different Solvers\nLogisticRegressionSolvers = ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']\n\nfor solver in LogisticRegressionSolvers:\n    model = LogisticRegression(solver=solver, max_iter=10000)\n    model.fit(x_train, y_train)\n\n    y_predict = model.predict(x_valid)\n    print(f'Results of Logistic Regression Model 1 USing \"{solver}\" Solver')\n    print(\"Model Accuracy = {}\".format(accuracy_score(y_valid, y_predict)))\n    print(\"Model Precision = {}\".format(precision_score(y_valid, y_predict)))\n    print(\"Model Recall = {}\".format(recall_score(y_valid, y_predict)))\n    print('')","b3e10be7":"# Using GridSearchCV instead of Iteration like before\nfrom sklearn.model_selection import GridSearchCV","c801cb92":"LogisticRegressionSolvers = ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']\niteration = [100, 250, 500, 1000, 2000, 5000, 10000]\n\nparameters_grid = {'solver' : LogisticRegressionSolvers, 'max_iter' : iteration }\nGridSearchResult = GridSearchCV(LogisticRegression(), parameters_grid, scoring=['accuracy','precision', 'recall'],\\\n                                refit='accuracy', return_train_score=True, cv=4) ","eadad96e":"GridSearchResult.fit(x, y)","afcc6bec":"print(GridSearchResult.best_params_)\nprint(GridSearchResult.best_score_)\nprint(GridSearchResult.best_estimator_)","c3dab281":"x = Labels_Encoded_train.drop('Survived', axis=1)\ny = Labels_Encoded_train['Survived']","999ae940":"x_train, x_valid, y_train, y_valid = train_test_split(x, y , test_size = 0.2, stratify = y)","22eabd8a":"# Model Using different Solvers\nLogisticRegressionSolvers = ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']\n\nfor solver in LogisticRegressionSolvers:\n    model = LogisticRegression(solver=solver, max_iter=10000)\n    model.fit(x_train, y_train)\n\n    y_predict = model.predict(x_valid)\n    print(f'Results of Logistic Regression Model 2 USing \"{solver}\" Solver')\n    print(\"Model Accuracy = {}\".format(accuracy_score(y_valid, y_predict)))\n    print(\"Model Precision = {}\".format(precision_score(y_valid, y_predict)))\n    print(\"Model Recall = {}\".format(recall_score(y_valid, y_predict)))\n    print('')","ee77f7ad":"# Using GridSearchCV instead of Iteration like before\nGridSearchResult.fit(x, y)","537eed0b":"print(GridSearchResult.best_params_)\nprint(GridSearchResult.best_score_)\nprint(GridSearchResult.best_estimator_)","2caefbec":"OneHot_Encoded_train.columns","cb1b7af8":"selected_features = ['Pclass', 'Age', 'Married_With_Childern',\n                     'Family_Members', 'Single', 'Child', 'Sex_female', 'Sex_male',\n                     'Title_Master', 'Title_Miss', 'Title_Mr', 'Title_Mrs', 'Title_Others']","a7c07f9d":"# Selected Datasets\nselected_train = OneHot_Encoded_train[selected_features + ['Survived']]\nselected_test = OneHot_Encoded_test[selected_features]","99f2efd0":"x = selected_train.drop('Survived', axis=1)\ny = selected_train['Survived']","0dab2560":"x_train, x_valid, y_train, y_valid = train_test_split(x, y , test_size = 0.2, stratify = y)","929a61d7":"# Model Using different Solvers\nLogisticRegressionSolvers = ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']\n\nfor solver in LogisticRegressionSolvers:\n    model = LogisticRegression(solver=solver, max_iter=10000)\n    model.fit(x_train, y_train)\n\n    y_predict = model.predict(x_valid)\n    print(f'Results of Logistic Regression Model 2 USing \"{solver}\" Solver')\n    print(\"Model Accuracy = {}\".format(accuracy_score(y_valid, y_predict)))\n    print(\"Model Precision = {}\".format(precision_score(y_valid, y_predict)))\n    print(\"Model Recall = {}\".format(recall_score(y_valid, y_predict)))\n    print('')","bd06903c":"# Using GridSearchCV instead of Iteration like before\nGridSearchResult.fit(x, y)","6a55fa5f":"print(GridSearchResult.best_params_)\nprint(GridSearchResult.best_score_)\nprint(GridSearchResult.best_estimator_)","4a620eae":"best_model = LogisticRegression(solver='liblinear', max_iter=100)\nbest_model.fit(x, y)","e275396a":"test_predict = best_model.predict(selected_test)","f82460ce":"# Saving test predictions to file\noutput = pd.DataFrame({\"PassengerId\": selected_test.index, \"Survived\" : test_predict})\noutput.to_csv('submission.csv', index=False)","99049850":"- We will have to deal with `Age`.\n- `Cabin` feature is filled with NaNs and useless feature.","89ebc52e":"- 61.8% Survived and 38.2% Not Survived ","32a6fba4":"- Number of Siblings or Spouses & Parents or Children can be `mixed` and give a valuable feature of `number of family`.","2959638e":"### Cleaning Datasets of NaNs & Useless Feature","1f19ca20":"### Imports & Loading Data","b66381d0":"- We have `no duplicated` records","d9bde943":"- Almost `all passengers` with `high` number of `Siblings or Spouses died`.\n- Most of passengers with `zero Siblings or Spouses` also `died`.\n- In between the numbers are close.","f7f60b21":"##### It seems that `Label Encoding` is almost the same as `OneHot Encoding` in this case.","006c4e60":"### Exploring Data and EDA","0aee5a34":"- We will have to deal with `Age`, `Fare` and `Cabin` NaNs","9c3e3ecf":"- Although it's `not logical` that `15 passengers` travelled with `0 fare`, we can `change` the fare into catagories `describe the affordability status` of each passenger.","39905a09":"- Number of survivore and deads for each Port of Embarkation are `close`, but `most of Southampton Passengers died`.","d0cee0d5":"### Data Pre-Processing","fd8649a2":"### Model 2\n##### Same as Model 1 except using Labeled Encoded data to see if  the different Encoding can change the performance","760a0b00":"- Ages has outliers, so it can be `imputed using median` not mean.","f58864dc":"- It seems that `most females Survived`, but `most males died`.\n- This feature will be `so important` in prediction.","48823e2b":"### Result\n- As shown below, the best model we have got is the `third model`.","777abe2d":"### Catagorized Columns Encoding","9a1c3eb6":"- Most Passengers were in class 3","062f53ab":"### Model 1","2f6e54a9":"- Most of passengers number of Parents or Children above 3 `died`.\n- Most of passengers with `zero Parents or Children` also `died`.\n- In between the numbers are close.","0bc1a0ba":"##### Model using  `\"liblinear\", \"newton-cg\" and \"lbfgs\" Solvers` are so close.","3251ca5f":"- It seems that tha `most` of `class 3` `died`, and `high number` of `class 1 survived`.\n- This feature will be `so important` in prediction.","32500038":"### Model 3\n##### Trying to use valuable features that may enhance the model"}}