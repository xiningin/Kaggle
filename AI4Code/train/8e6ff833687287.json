{"cell_type":{"0c49dd2b":"code","41620cc9":"code","224b00d3":"code","379f7b49":"code","6b68a699":"code","4ebf0907":"code","ff8ab9b5":"code","15ed0c74":"markdown","7ddf8423":"markdown","da9da62a":"markdown","723bf2e5":"markdown"},"source":{"0c49dd2b":"from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Concatenate, Lambda, GaussianNoise, Activation\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers.experimental.preprocessing import Normalization\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom random import choices\n\n\nSEED = 1111\n\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)\n\n# train = pd.read_csv('..\/input\/jane-street-market-prediction\/train.csv')\ntrain = pd.read_feather('..\/input\/janestreet-save-as-feather\/train.feather')\ntrain = train.query('date > 85').reset_index(drop = True) \ntrain = train[train['weight'] != 0]\n\ntrain.fillna(train.mean(),inplace=True)\n\ntrain['action'] = ((train['resp'].values) > 0).astype(int)\n\n\nfeatures = [c for c in train.columns if \"feature\" in c]\n\nf_mean = np.mean(train[features[1:]].values,axis=0)\n\nresp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']\n\ny_train = np.stack([(train[c] > 0).astype('int') for c in resp_cols]).T","41620cc9":"# split features for a ResNet\nfeatures1 = [f'feature_{i}' for i in range(71)]\nfeatures2 = [f'feature_{i}' for i in range(71, 130)]","224b00d3":"X_train = [train.loc[:, features1].values, \n          train.loc[:, features2].values]","379f7b49":"assert len(features) == len(features1) + len(features2)","6b68a699":"def create_resnet(n_features, n_features_2, n_labels, learning_rate=1e-03, label_smoothing=1e-02):    \n    input_1 = tf.keras.layers.Input(shape = (n_features,), name = 'Input1')\n    input_2 = tf.keras.layers.Input(shape = (n_features_2,), name = 'Input2')\n\n    head_1 = tf.keras.Sequential([\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.4),\n        tf.keras.layers.Dense(256, activation=\"elu\"), \n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.4),\n        tf.keras.layers.Dense(256, activation = \"elu\")\n        ],name='Head1') \n\n    input_3 = head_1(input_1)\n    input_3_concat = tf.keras.layers.Concatenate()([input_2, input_3])\n\n    head_2 = tf.keras.Sequential([\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.3),\n        tf.keras.layers.Dense(256, \"elu\"),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.3),\n        tf.keras.layers.Dense(256, \"relu\"),\n        ],name='Head2')\n\n    input_4 = head_2(input_3_concat)\n    input_4_avg = tf.keras.layers.Average()([input_3, input_4]) \n\n    head_3 = tf.keras.Sequential([\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(128, kernel_initializer='lecun_normal', activation='selu'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Dense(n_labels, kernel_initializer='lecun_normal', activation='selu'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Dense(n_labels, activation=\"sigmoid\")\n        ],name='Head3')\n\n    output = head_3(input_4_avg)\n\n    model = tf.keras.models.Model(inputs = [input_1, input_2], outputs = output)\n    opt = tfa.optimizers.RectifiedAdam(learning_rate=learning_rate)\n    opt = tfa.optimizers.SWA(opt)\n    model.compile(optimizer=opt, \n                  loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing), \n                  metrics=['AUC'])\n    \n    return model\n\ntf.keras.backend.clear_session()\nclf = create_resnet(len(features1), len(features2), len(resp_cols), 1e-03, 1e-02)\n\nclf.summary()","4ebf0907":"tf.keras.utils.plot_model(clf)","ff8ab9b5":"\nclf.fit(X_train, y_train, epochs=200, batch_size=4096 * 16)\n\n# save model\nclf.save('resnet.h5')\n\nmodels = [clf]\n\nth = 0.501\n\nf = np.median\nmodels = models[-3:]\nimport janestreet\nenv = janestreet.make_env()\nfor (test_df, pred_df) in tqdm(env.iter_test()):\n    if test_df['weight'].item() > 0:\n        x_tt = test_df.loc[:, features].values\n        if np.isnan(x_tt[:, 1:].sum()):\n            x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * f_mean\n        pred = np.mean([model([x_tt[:, :len(features1)], x_tt[:, len(features1):]], training = False).numpy() for model in models],axis=0)\n        pred = f(pred)\n        pred_df.action = np.where(pred >= th, 1, 0).astype(int)\n    else:\n        pred_df.action = 0\n    env.predict(pred_df)","15ed0c74":"# Fit & Predict","7ddf8423":"That's it!","da9da62a":"# ResNet\nderived from [[janestreet] ResNet with AutoEncoder (infer)](https:\/\/www.kaggle.com\/code1110\/janestreet-resnet-with-autoencoder-infer).","723bf2e5":"# ResNet\n\n![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/c\/c1\/Piramidal_cell.svg\/440px-Piramidal_cell.svg.png)\n\n>ResNet consists of residual blocks that transfer the knowledge from one layer to  further layers by skipping some layers in between. These kinds of connections of  layers are known as skip-connections since we are skipping one or more layers.  Skip-connections help with the vanishing gradient issue by propagating the  gradients to further layers. This allows us to train very large convolutional neural  networks without loss of performance.\n>Thakur, Abhishek. Approaching (Almost) Any Machine Learning Problem (p.205). Abhishek Thakur. \n\nThe model pipeline is derived from this amazing notebook, so please upvote it too!\n\n[OWN Jane Street with Keras NN](https:\/\/www.kaggle.com\/tarlannazarov\/own-jane-street-with-keras-nn)"}}