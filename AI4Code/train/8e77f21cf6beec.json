{"cell_type":{"76a3218a":"code","7297e979":"code","70bbf4aa":"code","df6c1f93":"code","d7505e0b":"code","fdecb216":"code","2002e88f":"code","920df036":"code","67b51b31":"code","40bcb32e":"code","67c7b355":"code","05d30b3d":"code","b3e70ef0":"markdown","5827030f":"markdown","64085c63":"markdown","aecdf621":"markdown","829925eb":"markdown","7a305e8f":"markdown","4d7988d7":"markdown","75935a5d":"markdown","daf72379":"markdown","45af809b":"markdown","af5c16bc":"markdown","a5847ee2":"markdown","de6cdf30":"markdown","8d6253b5":"markdown","be638159":"markdown","4ac76048":"markdown","67570a82":"markdown","de8ff576":"markdown"},"source":{"76a3218a":"import gc\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score","7297e979":"train = pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/test.csv')\n\ntrain","70bbf4aa":"print(train.isna().sum(), test.isna().sum())","df6c1f93":"train['std'] = train.std(axis=1)\ntrain['min'] = train.min(axis=1)\ntrain['max'] = train.max(axis=1)\n\ntest['std'] = test.std(axis=1)\ntest['min'] = test.min(axis=1)\ntest['max'] = test.max(axis=1)","d7505e0b":"y = train['target']\ntrain.drop(columns = ['id', 'target'], inplace = True)\ntest.drop(columns = 'id', inplace = True)","fdecb216":"def Stacking_Data_Loader(model, model_name, train, y, test, fold):\n    '''\n    Put your train, test datasets and fold value!\n    This function returns train, test datasets for stacking ensemble :)\n    '''\n\n    stk = StratifiedKFold(n_splits = fold, random_state = 42, shuffle = True)\n    \n    # Declaration Pred Datasets\n    train_fold_pred = np.zeros((train.shape[0], 1))\n    test_pred = np.zeros((test.shape[0], fold))\n    \n    for counter, (train_index, valid_index) in enumerate(stk.split(train, y)):\n        x_train, y_train = train.iloc[train_index], y[train_index]\n        x_valid, y_valid = train.iloc[valid_index], y[valid_index]\n\n        print('------------ Fold', counter+1, 'Start! ------------')\n        if model_name == 'cat':\n            model.fit(x_train, y_train, eval_set=[(x_valid, y_valid)])\n        elif model_name == 'xgb':\n            model.fit(x_train, y_train, eval_set=[(x_valid, y_valid)], eval_metric = 'auc', verbose = 500, early_stopping_rounds = 200)\n        else:\n            model.fit(x_train, y_train, eval_set=[(x_valid, y_valid)], eval_metric = 'auc', verbose = 500, early_stopping_rounds = 200)\n            \n        print('------------ Fold', counter+1, 'Done! ------------')\n        \n        train_fold_pred[valid_index, :] = model.predict_proba(x_valid)[:, 1].reshape(-1, 1)\n        test_pred[:, counter] = model.predict_proba(test)[:, 1]\n        \n        del x_train, y_train, x_valid, y_valid\n        gc.collect()\n        \n    test_pred_mean = np.mean(test_pred, axis = 1).reshape(-1, 1)\n    \n    del test_pred\n    gc.collect()\n    \n    print('Done!')\n    \n    return train_fold_pred, test_pred_mean","2002e88f":"lgb_params = {\n    'objective': 'binary',\n    'n_estimators': 20000,\n    'random_state': 42,\n    'learning_rate': 8e-3,\n    'subsample': 0.6,\n    'subsample_freq': 1,\n    'colsample_bytree': 0.4,\n    'reg_alpha': 10.0,\n    'reg_lambda': 1e-1,\n    'min_child_weight': 256,\n    'min_child_samples': 20,\n    'device': 'gpu',\n}\n\n\nxgb_params = {'n_estimators': 10000,\n               'learning_rate': 0.03689407512484644,\n               'max_depth': 8,\n               'colsample_bytree': 0.3723914688159835,\n               'subsample': 0.780714581166012,\n               'eval_metric': 'auc',\n               'use_label_encoder': False,\n               'gamma': 0,\n               'reg_lambda': 50.0,\n               'tree_method': 'gpu_hist',\n               'gpu_id': 0,\n               'predictor': 'gpu_predictor',\n               'random_state': 42}\n\ncat_params = {'iterations': 17298,\n               'learning_rate': 0.03429054860458741,\n               'reg_lambda': 0.3242286463210283,\n               'subsample': 0.9433911589913944,\n               'random_strength': 22.4849972385133,\n               'depth': 8,\n               'min_data_in_leaf': 4,\n               'leaf_estimation_iterations': 8,\n               'task_type':\"GPU\",\n               'bootstrap_type':'Poisson',\n               'verbose' : 500,\n               'early_stopping_rounds' : 200,\n               'eval_metric' : 'AUC'}","920df036":"lgbm = LGBMClassifier(**lgb_params)\n\nxgb = XGBClassifier(**xgb_params)\n\ncat = CatBoostClassifier(**cat_params)","67b51b31":"cat_train, cat_test = Stacking_Data_Loader(cat, 'cat', train, y, test, 5)\ndel cat\ngc.collect()\n\nlgbm_train, lgbm_test = Stacking_Data_Loader(lgbm, 'lgbm', train, y, test, 5)\ndel lgbm\ngc.collect()\n\nxgb_train, xgb_test = Stacking_Data_Loader(xgb, 'xgb', train, y, test, 5)\ndel xgb\ngc.collect()","40bcb32e":"stack_x_train = np.concatenate((cat_train, lgbm_train, xgb_train), axis = 1)\nstack_x_test = np.concatenate((cat_test, lgbm_test, xgb_test), axis = 1)\n\ndel cat_train, lgbm_train, xgb_train, cat_test, lgbm_test, xgb_test\ngc.collect()\n\nstack_x_train","67c7b355":"stk = StratifiedKFold(n_splits = 5)\n\ntest_pred_lo = 0\nfold = 1\ntotal_auc = 0\n\nfor train_index, valid_index in stk.split(stack_x_train, y):\n    x_train, y_train = stack_x_train[train_index], y[train_index]\n    x_valid, y_valid = stack_x_train[valid_index], y[valid_index]\n    \n    lr = LogisticRegression(n_jobs = -1, random_state = 42, C = 5, max_iter = 2000)\n    lr.fit(x_train, y_train)\n    \n    valid_pred_lo = lr.predict_proba(x_valid)[:, 1]\n    test_pred_lo += lr.predict_proba(stack_x_test)[:, 1]\n    auc = roc_auc_score(y_valid, valid_pred_lo)\n    total_auc += auc \/ 5\n    print('Fold', fold, 'AUC :', auc)\n    fold += 1\n    \nprint('Total AUC score :', total_auc)","05d30b3d":"sub = pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/sample_submission.csv')\nsub['target'] = test_pred_lo\nsub.to_csv('sub.csv', index = 0)\nsub","b3e70ef0":"### **Stacking Data Loader**","5827030f":"# Feature Generation","64085c63":"# **Submission!**\n\n## Blending Level 2 results","aecdf621":"# Done!\n\n\n## If you think this notebook is helpful for you, Please do not forget upvote!","829925eb":"#### Model's HyperParameters\n* LGBM Param : https:\/\/www.kaggle.com\/hiro5299834\/tps-oct-2021-single-lightgbm\n* Cat Param : https:\/\/www.kaggle.com\/ranjeetshrivastav\/tps-oct-21-catboost\n* xgb Param : https:\/\/www.kaggle.com\/rahulchauhan3j\/tps-oct-2021-xgboost-pipeline-with-optuna#Model-Fit-and-Submission\n\nThanks for Sharing!","7a305e8f":"### Thank you for visiting my notebook :)\n### This notebook is for beginner like me **who wants to study stacking ensemble!**","4d7988d7":"## Level 2 Training","75935a5d":"### **Modeling**","daf72379":"## **Checking Missing Values**","45af809b":"# **Load Data**","af5c16bc":"# **Import Library**","a5847ee2":"![Stacking Ensemble](http:\/\/rasbt.github.io\/mlxtend\/user_guide\/classifier\/StackingClassifier_files\/stackingclassification_overview.png)","de6cdf30":"### **Stacking**\n\n* Making train, test prediction array!\n* Concat 3 arrays in 1 dataset","8d6253b5":"# **TPS - Oct 2021**\n\n## **XGBoost & LightGBM & CatBoost Stacking**","be638159":"#### Here's what you need to do.\n**Step1. Make your train, test data for training & prediction (Preprocessing)**\n\n**Step2. Select some models for making stacking datasets!! (Train models and Making Datasets)**\n\n**Step3. Select final model for meta-model!**\n\n**Step4. With your meta-model, Train & Predict with stacking datasets ;)**","4ac76048":"# **Modeling**","67570a82":"#### **Stacking Ensemble** is a nice technique for forwarding you score.\n#### As you can see below image, Stacking Ensemble needs some models for classification and meta-model for final prediction!","de8ff576":"### **Final Stacking Datasets!**"}}