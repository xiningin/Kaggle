{"cell_type":{"b9cfea65":"code","abb39e91":"code","78b727d4":"code","30ac2074":"code","d3109793":"code","2ccf800a":"code","7195a024":"code","1e4ec9de":"code","6004ceb9":"code","3b72011c":"code","4e962e85":"code","5d21dd94":"code","8ca3c619":"code","1a5ddc6c":"code","b129b359":"code","25a7948d":"code","082d8868":"code","ee5ce7fe":"code","02de1611":"code","3b3b3804":"code","19fd7a36":"code","2019f2fb":"code","99fefad8":"code","cb450a01":"code","3690c414":"markdown","32a541d0":"markdown","8d3da5a4":"markdown","ec39798e":"markdown","2bcd4c95":"markdown","9a174d28":"markdown","f22a0936":"markdown","f9fa4e14":"markdown","c52e3a7a":"markdown","7835e91b":"markdown","efc7d290":"markdown","543e3eff":"markdown","d615e7f5":"markdown","70fbae62":"markdown","2a8bcc68":"markdown","8cca38ab":"markdown","28364980":"markdown","6347780b":"markdown","d0020906":"markdown","1ae1a35b":"markdown","8d204210":"markdown","a72b8233":"markdown","a9aafcc1":"markdown"},"source":{"b9cfea65":"from datetime import date\nfrom lightgbm import LGBMRegressor\nfrom sklearn.ensemble import VotingRegressor\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport seaborn as sns\nsns.set()\n\nseed = 42","abb39e91":"PATH_TO_DATA = \"\/kaggle\/input\/ieor242hw4\"\ntrain = pd.read_csv(PATH_TO_DATA + \"\/train.csv\", parse_dates=[\"pickup_datetime\"])\ntest = pd.read_csv(PATH_TO_DATA + \"\/test.csv\", parse_dates=[\"pickup_datetime\"])\nsub = pd.read_csv(PATH_TO_DATA + \"\/submission.csv\")\n\nprint(\"Train sample:\")\ndisplay(train.sample(5))\nprint(\"Test sample:\")\ndisplay(test.sample(5))","78b727d4":"PATH_TO_LOOKUP = \"\/kaggle\/input\/nyc-yellow-taxi-zone-lookup-table\"\nlookup = pd.read_csv(PATH_TO_LOOKUP + \"\/taxi_zone_lookup.csv\")\n\nloc_to_borough = dict(zip(lookup[\"LocationID\"], lookup[\"Borough\"].apply(lambda x: str(x).lower())))\nloc_to_zone = dict(zip(lookup[\"LocationID\"], lookup[\"Zone\"].apply(lambda x: str(x).lower())))\n\nzone_to_loc = {value: key for key, value in loc_to_zone.items()}\nzone_to_loc[\"unknown\"] = 265\n\nborough_to_label = {'Bronx': 1, 'Brooklyn': 2, 'EWR': 3, 'Manhattan': 4, 'Queens': 5, 'Staten Island': 6, 'Unknown': 7}\nborough_to_label = {key.lower(): value for key, value in borough_to_label.items()}","30ac2074":"train.isnull().mean()","d3109793":"train[\n    train[\"VendorID\"].isnull() |\n    train[\"passenger_count\"].isnull()\n][[\"VendorID\", \"passenger_count\"]].isnull().mean()","2ccf800a":"test.isnull().mean()","7195a024":"out_val = 999\n\ndef preprocess_data(data):\n    # VendorID and passenger count\n    data[\"VendorID\"] = data[\"VendorID\"].replace({np.nan: 0}).apply(int)\n    data[\"passenger_count\"] = data[\"passenger_count\"].replace({np.nan: 0}).apply(int)\n\n    # Pickup and dropoff boroughs and zones\n    data[\"pickup_borough\"] = data[\"pickup_borough\"].apply(lambda x: borough_to_label[x.lower()])\n    data[\"dropoff_borough\"] = data[\"dropoff_borough\"].apply(lambda x: borough_to_label[x.lower()])\n    data[\"pickup_zone\"] = data[\"pickup_zone\"].replace({np.nan: \"Unknown\"}).apply(lambda x: zone_to_loc[x.lower()])\n    data[\"dropoff_zone\"] = data[\"dropoff_zone\"].replace({np.nan: \"Unknown\"}).apply(lambda x: zone_to_loc[x.lower()])\n\n    # Pickup datetime\n    data[\"pickup_datetime\"] = data[\"pickup_datetime\"].replace({np.nan: out_val})\n    data[\"pickup_year\"] = data[\"pickup_datetime\"].apply(lambda x: int(x.year) if x != out_val else out_val)\n    data[\"pickup_month\"] = data[\"pickup_datetime\"].apply(lambda x: int(x.month) if x != out_val else out_val)\n    data[\"pickup_day\"] = data[\"pickup_datetime\"].apply(lambda x: int(x.day) if x != out_val else out_val)\n    data[\"pickup_hour\"] = data[\"pickup_datetime\"].apply(lambda x: x.hour if x != out_val else out_val)\n    data[\"pickup_minute\"] = data[\"pickup_datetime\"].apply(lambda x: x.minute if x != out_val else out_val)\n   \n    # Further extraction: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.Timestamp.html\n    data[\"pickup_dayofweek\"] = data[\"pickup_datetime\"].apply(lambda x: int(x.dayofweek) if x != out_val else out_val)\n    data[\"pickup_dayofyear\"] = data[\"pickup_datetime\"].apply(lambda x: int(x.dayofyear) if x != out_val else out_val)\n    # data[\"pickup_weekofyear\"] = data[\"pickup_datetime\"].apply(lambda x: int(x.weekofyear) if x != out_val else out_val)\n    \n    # Drop useless columns\n    data = data.drop(columns=[\"row_id\"])\n    \n    return data.reset_index(drop=True)\n\n\ntrain = preprocess_data(train)\ntest = preprocess_data(test)","1e4ec9de":"categorical_columns = [\n    \"VendorID\", \"passenger_count\", \"pickup_year\", \"pickup_month\", \"pickup_day\", \"pickup_hour\", \"pickup_minute\",\n    \"pickup_dayofweek\", \"pickup_borough\", \"dropoff_borough\"\n]\nnumerical_columns = [\"trip_distance\", \"pickup_dayofyear\"]\n\n\ndef display_data(data):\n    nb_rows, nb_cols = max((len(categorical_columns + numerical_columns) - 3) \/\/ 4 + 1, 2), 4\n    fig, ax = plt.subplots(figsize=(14.5, 4 * nb_rows), nrows=nb_rows, ncols=nb_cols)\n    for ind, column in enumerate(categorical_columns + numerical_columns):\n        if column in categorical_columns:\n            sns.countplot(data[data[column] != out_val][column], ax=ax[ind \/\/ nb_cols, ind % nb_cols])\n        elif column in numerical_columns:\n            sns.distplot(data[data[column] != out_val][column], ax=ax[ind \/\/ nb_cols, ind % nb_cols])\n            ax[ind \/\/ nb_cols, ind % nb_cols].set_ylim((0, 0.05))\n\n    plt.show()\n\n\nprint(\"Train EDA:\")\ndisplay_data(train)\nprint(\"Test EDA:\")\ndisplay_data(test)","6004ceb9":"fig, ax = plt.subplots(figsize=(15, 4 * 2), nrows=2, ncols=2)\n\nax[0, 0].scatter(train[\"trip_distance\"], train[\"duration\"], s=1)\nax[0, 0].set_title(\"Trip duration vs. distance\")\nmax_duration = 3600 * 3\n# Some outlier removal performed here, be careful\ntrain = train[(train[\"duration\"] <= max_duration) & (train[\"trip_distance\"] <= test[\"trip_distance\"].max())]\nax[0, 1].scatter(train[\"trip_distance\"], train[\"duration\"], s=1)\nax[0, 1].set_title(\"Trip duration vs. distance (duration < 3 hrs)\")\n\nnegative_dist = train[(train[\"trip_distance\"] < 0)]\nax[1, 0].scatter(negative_dist[\"trip_distance\"], negative_dist[\"duration\"], s=1)\nax[1, 0].set_title(\"Trip duration vs. distance (distance < 0)\")\n\npositive_dist = train[(train[\"trip_distance\"] >= 0)]\nax[1, 1].scatter(positive_dist[\"trip_distance\"], positive_dist[\"duration\"], s=1)\nax[1, 1].scatter(negative_dist[\"trip_distance\"].apply(abs), negative_dist[\"duration\"], s=1)\nax[1, 1].set_xlim(0, 40)\nax[1, 1].set_ylim(0, max_duration)\nax[1, 1].set_title(\"Trip duration vs. distance (absolute distance)\")\n\nplt.show()","3b72011c":"fig = px.scatter(\n    x=train[\"trip_distance\"], y=train[\"duration\"], range_x=[0, 40], range_y=[0, 3600 * 2]\n)\nfig.update_traces(marker=dict(size=3))\nfig.show()","4e962e85":"fig, ax = plt.subplots(figsize=(15, 4), nrows=1, ncols=2)\nax[0].scatter(train[\"trip_distance\"], train[\"duration\"], s=1)\nslow_distance, slow_duration = 2.61, 29828\nfast_distance, fast_duration = 35.7, 2124\nax[0].plot(\n    [0.5, 0.5, fast_distance, 125],\n    [0, 0.5 * fast_duration \/ fast_distance, fast_duration, 125 * fast_duration \/ fast_distance],\n    color=\"green\"\n)\nax[0].plot(\n    [0, slow_distance * 3000 \/ slow_duration, slow_distance * 2, slow_distance * 86400 \/ slow_duration],\n    [3000, 3000, slow_duration * 2, 86400],\n    color=\"green\"\n)\nax[0].set_xlim(0, 40)\nax[0].set_ylim(0, 3600 * 2)\nax[0].set_title(\"Trip duration vs. distance (with outliers)\")\n\noutliers = (\n    (train[\"trip_distance\"] == 0) |\n    (train[\"duration\"] == 0) |\n    (train[\"trip_distance\"] \/ train[\"duration\"] >= fast_distance \/ fast_duration) & (train[\"trip_distance\"] >= 0.5) |\n    (train[\"trip_distance\"] \/ train[\"duration\"] <= slow_distance \/ slow_duration) & (train[\"duration\"] >= 3000)\n)\nax[1].scatter(train[~outliers][\"trip_distance\"], train[~outliers][\"duration\"], s=1)\nax[1].set_xlim(0, 40)\nax[1].set_ylim(0, 3600 * 2)\nax[1].set_title(\"Trip duration vs. distance (without outliers)\")\n\nplt.show()","5d21dd94":"def correct_outliers(data):\n    data[\"trip_distance\"] = data[\"trip_distance\"].apply(abs)\n    \n    return data.reset_index(drop=True)\n\n\ntrain = correct_outliers(train)\ntest = correct_outliers(test)","8ca3c619":"def remove_outliers(data):\n    data = data[(data[\"duration\"] <= max_duration)]  # Already removed before, be careful\n    outliers = (\n        (data[\"trip_distance\"] == 0) |\n        (data[\"duration\"] == 0) |\n        (data[\"trip_distance\"] \/ data[\"duration\"] >= fast_distance \/ fast_duration) & (data[\"trip_distance\"] >= 0.5) |\n        (data[\"trip_distance\"] \/ data[\"duration\"] <= slow_distance \/ slow_duration) & (data[\"duration\"] >= 3000)\n    )\n    data = data[~outliers]\n\n    return data.reset_index(drop=True)\n\n\ntrain = remove_outliers(train)","1a5ddc6c":"plt.figure(figsize=(15, 4))\ntrain[\"duration\"].apply(np.log).hist(bins=160)\nplt.xlim(4, 9)\nplt.show()","b129b359":"def feature_engineering(data):\n    data[\"pickup_quarterhour\"] = data[\"pickup_datetime\"].apply(\n        lambda x: (x - pd.Timestamp(int(x.year), 1, 1)).seconds \/\/ (60 * 15) if x != out_val else out_val\n    )\n    data = data.drop(columns=[\n        \"pickup_datetime\", \"pickup_year\", \"pickup_month\", \"pickup_day\", \"pickup_hour\", \"pickup_minute\"\n    ])\n    \n    return data.reset_index(drop=True)\n\n\ntrain = feature_engineering(train)\ntest = feature_engineering(test)","25a7948d":"def one_hot_encoding(data):\n    for i in range(1, 8):\n        data[\"pickup_borough_{}\".format(i)] = data[\"pickup_zone\"].apply(\n            lambda x: x if borough_to_label[loc_to_borough[x]] == i else 0\n        )\n        data[\"dropoff_borough_{}\".format(i)] = data[\"dropoff_zone\"].apply(\n            lambda x: x if borough_to_label[loc_to_borough[x]] == i else 0\n        )\n\n    data = data.drop(columns=[\"pickup_zone\", \"dropoff_zone\", \"pickup_borough\", \"dropoff_borough\"])\n    \n    return data\n\n\ntrain = one_hot_encoding(train)\ntest = one_hot_encoding(test)\n\ntrain.shape","082d8868":"(train >= 0).mean()","ee5ce7fe":"def downcast_data(data):\n    data[\"trip_distance\"] = (100 * data[\"trip_distance\"]).astype(int)\n\n    for column in data.columns:\n        data[column] = pd.to_numeric(data[column], downcast='unsigned')\n    \n    return data.reset_index(drop=True)\n\n\ntrain = downcast_data(train)\ntest = downcast_data(test)\n\ntrain.dtypes","02de1611":"plt.figure(figsize=(15, 15))\nsns.heatmap(np.abs(np.round(train.corr(), 2)), square=True, annot=True, cmap=plt.cm.Blues)\nplt.show()","3b3b3804":"X, y = train.drop(columns=[\"duration\"]), train[\"duration\"]\nX_test = test\n\nvalidate = True\n\nif validate:\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=seed)","19fd7a36":"commargs = {\"learning_rate\": 0.03, \"colsample_bytree\": 0.9, \"reg_lambda\": 0.2, \"random_state\": seed, \"n_jobs\": -1}\n\nlgbm_63 = LGBMRegressor(n_estimators=12000, num_leaves=63, **commargs)\nlgbm_127 = LGBMRegressor(n_estimators=6000, num_leaves=127, **commargs)\nlgbm_255 = LGBMRegressor(n_estimators=4000, num_leaves=255, **commargs)\nlgbm_511 = LGBMRegressor(n_estimators=2000, num_leaves=511, **commargs)\nlgbm_1023 = LGBMRegressor(n_estimators=1000, num_leaves=1023, **commargs)\n\nlgbm_estimators = [\n    (\"LGBM_63\", lgbm_63), (\"LGBM_127\", lgbm_127), (\"LGBM_255\", lgbm_255),\n    (\"LGBM_511\", lgbm_511), (\"LGBM_1023\", lgbm_1023),\n]\n\nlgbm_voting = VotingRegressor(lgbm_estimators)","2019f2fb":"def plot_classifiers_validation(regs, X_train, y_train, X_val, y_val):\n    fitted_regs = []\n    nb_rows, nb_cols = 2, 3\n    fig, ax = plt.subplots(figsize=(15, 10), nrows=nb_rows, ncols=nb_cols)\n    for ind, reg in enumerate(regs):\n        reg.fit(X_train, np.log(y_train))\n        fitted_regs.append(reg)\n        y_pred = reg.predict(X_val)\n        ax[ind \/\/ nb_cols, ind % nb_cols].scatter(y_val, np.exp(y_pred), s=1)\n        max_plot_value = max(y_val.max(), np.exp(y_pred).max())\n        ax[ind \/\/ nb_cols, ind % nb_cols].plot([0, max_plot_value], [0, max_plot_value], color=\"orange\")\n        ax[ind \/\/ nb_cols, ind % nb_cols].set_title(\"IN-RMSE = {0:.2f}, IN-RMSLE = {1:.5f}\".format(\n            np.sqrt(mean_squared_error(y_val[y_val <= 7000], np.exp(y_pred)[y_val <= 7000])),\n            np.sqrt(mean_squared_log_error(y_val[y_val <= 7000], np.exp(y_pred)[y_val <= 7000]))\n        ))\n        ax[ind \/\/ nb_cols, ind % nb_cols].set_xlim(0, max_plot_value)\n        ax[ind \/\/ nb_cols, ind % nb_cols].set_ylim(0, max_plot_value)\n    plt.show()\n    return fitted_regs\n\n\nif validate:\n    lgbm_regs = plot_classifiers_validation(\n        [lgbm_63, lgbm_127, lgbm_255, lgbm_511, lgbm_1023, lgbm_voting], X_train, y_train, X_val, y_val\n    )","99fefad8":"categorical_columns = [\"VendorID\", \"passenger_count\", \"pickup_dayofweek\", \"pickup_borough\", \"dropoff_borough\"]\nnumerical_columns = [\"trip_distance\", \"pickup_dayofyear\", \"pickup_quarterhour\"]\n\n\ndef plot_lgbm_feature_importance(clf, ax):\n    ft_imp_dummies = dict(zip(X_train.columns, clf.feature_importances_))\n    ft_imp = {\n        column: sum([value for key, value in ft_imp_dummies.items() if column in key])\n        if column in categorical_columns else ft_imp_dummies[column]\n        for column in categorical_columns + numerical_columns\n    }\n    ft_imp = {key: value for key, value in sorted(ft_imp.items(), key=lambda item: item[1])}\n\n    labels, values = list(ft_imp.keys()), list(ft_imp.values())\n    ylocs = np.arange(len(values))\n    ax.barh(ylocs, values, align='center', height=0.2)\n    for x, y in zip(values, ylocs):\n        ax.text(x + 1, y, x, va='center')\n    ax.set_yticks(ylocs)\n    ax.set_yticklabels(labels)\n    ax.set_title(\"Feature importance for LGBM\")\n\n\nif validate:\n    fig, ax = plt.subplots(figsize=(13, 8), nrows=3, ncols=2)\n    for ind, reg in enumerate(lgbm_regs[:-1]):\n        plot_lgbm_feature_importance(reg, ax=ax[ind \/\/ 2, ind % 2])\n    plt.subplots_adjust(wspace=0.5, hspace=0.4)\n    plt.show()","cb450a01":"fit_predict = True\npredict_on_train = True\n\nif fit_predict:\n    plt.figure(figsize=(15, 4))\n    lgbm_voting.fit(X, np.log(y))\n\n    if predict_on_train:\n        y_pred = np.exp(lgbm_voting.predict(X))\n        print(\"Train RMSE: \", np.sqrt(mean_squared_error(y, y_pred)))\n        plt.hist(y, density=True, bins=[50 * i for i in range(160)])\n\n    y_sub = np.exp(lgbm_voting.predict(X_test))\n    plt.hist(y_sub, density=True, bins=[50 * i for i in range(160)], alpha=0.5)\n    plt.xlim((0, 3500))\n    plt.show()\n\n    sub[\"duration\"] = y_sub\n    display(sub)\n    sub.to_csv(\"lgbm-voting-final.csv\", index=False)","3690c414":"## 1.3. Data Preprocessing\n\n![processing.jpg](attachment:processing.jpg) <br>\n\nLet's perform some preprocessing of the data in order to perform the EDA. It includes filling missing values with a special value, label encoding of boroughs and zones and extracting valuable information from the pickup date.","32a541d0":"On the upper left graph, we plot trip duration against distance without any transformation performed on the train data. We observe again the presence of negative trip distances, as well as a cluster of dots corresponding to trips of short distance that would have been performed in around 24 hours. These trips are surely outliers, probably as well as most of the trips lasting several hours for a short distance.\n\nWe remove trips lasting more than 3 hours on the upper right graph. We can better see the distribution of the majority of the data, and note that there are not only negative trip distances but also distances and durations being equal to 0. Trips with a distance or duration equal to 0 will are of course outliers.\n\nOn the bottom left, we observe the distribution of trips with negative distance. We notice a symmetry with trips having a positive distance.\n\nAfter taking the absolute value of the trip distance as the real trip distance, we can see with the orange points that trips with a negative distance fit perfectly with the data distribution when this distance to be considered positive. We may thus *correct* trip distance values by taking the absolute value.\n\nFinally, we observe very few points corresponding to trips with either a very high or a very low speed. We may also want to remove these trips from the training set with some manual removal as shown below.","8d3da5a4":"Of course, we note a high correlation between `trip_distance` and `duration`.\n\n# 4. Modeling\n\n## 4.1. Data Splitting\n\nIt's time to build some models ! Here we simply create the train, validation and test sets. ","ec39798e":"## 1.4. Exploratory Data Analysis (EDA)\n\n![eda.jpg](attachment:eda.jpg) <br>\n\nHere, we display the distribution of the data for each column of train and test sets.","2bcd4c95":"# NYC Taxi Trip Duration Prediction\n\n*Author: [Michael Karpe](https:\/\/www.kaggle.com\/mika30)* <br>\n\n[Kaggle InClass Competition](https:\/\/www.kaggle.com\/c\/ieor242hw4) for UC Berkeley *IEOR 242 Applications in Data Analysis* course. Given NYC taxi trips information, we want to predict their travel duration in seconds. More information about the data on the [NYC TLC website](https:\/\/www1.nyc.gov\/site\/tlc\/about\/tlc-trip-record-data.page). Root Mean Square Error (RMSE) is used for evaluation. <br>\n\n*Public Score: 258.47466 (#1). Private Score: 253.84098 (#1).*\n\n**If you find this kernel useful, show your interest in the work done by upvoting the kernel!**\n\n# Contents\n\n1. [Preliminary Study](#PreliminaryStudy) <br>\n    1.1. [Data Loading](#DataLoading) <br>\n    1.2. [Missing Values](#MissingValues)<br>\n    1.3. [Data Preprocessing](#DataPreprocessing)<br>\n    1.4. [Exploratory Data Analysis (EDA)](#ExploratoryDataAnalysisEDA)<br>\n2. [Outlier Analysis](#OutlierAnalysis) <br>\n    2.1. [Outlier Detection](#OutlierDetection) <br>\n    2.2. [Outlier Correction](#OutlierCorrection)<br>\n    2.3. [Outlier Removal](#OutlierRemoval)<br>\n3. [Further Processing](#FurtherProcessing) <br>\n    3.1. [Target Transformation](#TargetTransformation) <br>\n    3.2. [Feature Engineering](#FeatureEngineering)<br>\n    3.3. [Zone Encoding](#ZoneEncoding)<br>\n    3.4. [Data Downcasting](#DataDowncasting)<br>\n    3.5. [Feature Correlations](#FeatureCorrelations)<br>\n4. [Modeling](#Modeling) <br>\n    4.1. [Data Splitting](#DataSplitting) <br>\n    4.2. [Regressors](#Regressors)<br>\n    4.3. [Validation](#Validation)<br>\n    4.4. [Feature Importances](#FeatureImportances)<br>\n5. [Prediction](#Prediction)<br>","9a174d28":"## 4.4. Feature Importances\n\n![ftimp.png](attachment:ftimp.png) <br>\n\nWe mainly use feature importances for feature selection and building the model. On this final model with only 8 main features, we note that `trip_distance`, `pickup_dayofyear` and `pickup_quarterhour` are the most important features, with an importance of around 20% for each. We note that the importance of the least important feature, `VendorID`, is around 2%, which is only 10 times less than the most important one, meaning that we do not have a feature very insignificant compared to others.","f22a0936":"For sake of simplicity, we use lookup tables on the [NYC TLC website](https:\/\/www1.nyc.gov\/site\/tlc\/about\/tlc-trip-record-data.page) for label encoding of pickup and dropoff boroughs and zones. A simple label encoder can be used instead if lookup tables are not available.","f9fa4e14":"## 5. Prediction\n\n![predict.png](attachment:predict.png) <br>\n\nWe are already at this end of this kernel! We just have to train our voting classifier model on all the training data to have our predictions!","c52e3a7a":"We can notice first that distributions of train and test sets look to be the same. We can also take note of the following observations:\n- `VendorID` and `passenger_count` values are mainly 1 or 2.\n- After further observation of `pickup_year` and `pickup_month`, we can see that the data is divided into 3 sets of approximately equal size: trips between 08\/2016 and 11\/2016, between 01\/2019 and 04\/2019 and between 08\/2019 and 11\/2019.\n- Distribution of `pickup_day`, `pickup_minute` and is roughly uniform. Given the previous observation, `pickup_dayofyear` is also uniform.\n- There are less trips on Mondays (0) and Sundays (6) than on other days. There is a clear relation between the day of the week and the number of trips.\n- There is an even clearer relation between the number of trips performed and the hour of the day. The hours with the most trips are 6pm and 7pm, the hours with the fewest trips are 4am and 5am. *This feature will probably be one of the most important features, as we may consider that trips are longer if there is more traffic.*\n- Pickup and dropoff boroughs are mostly 4 (Manhattan), followed by 5 (Queens) and 2 (Brooklyn).\n- Some `trip_distance` are negative (and will be studied in the *Outlier Analysis* section). Otherwise, after further observation, the distribution looks to be lognormal.\n\nCategorical features with only few unique values are going to be handled well by gradient boosted trees regressors. Our goal being to predict the duration of a trip, the most promising features (and which also have a high variance) are `trip_distance`, `pickup_hour` and `pickup_dayofweek`.\n\n\n# 2. Outlier Analysis\n\n## 2.1. Outlier Detection\n\n![outliers.jpeg](attachment:outliers.jpeg) <br>\n\nHere, we try to understand the presence of negative values for `trip_distance`. As this feature is probably the one that most explains the `duration` we want to predict, we will plot the `duration` of a trip against `trip_distance`.","7835e91b":"## 3.4. Data Downcasting\n\nWhen some values are not `float` values requiring the maximum precision, we may want to downcast the data, mainly for memory issues. For example, `VendorID` may be considered as an `int` column and not as a `float` column, as it is done initially when loading the data because there were some missing values.\n\nIn our data, the only \"real\" `float` data is `trip_distance`, however we observe that the precision of `trip_distance` is to the hundredth. Therefore, we can multiply `trip_distance` by 100 and also consider `trip_distance` as an `int`.","efc7d290":"## 3.2. Feature Engineering\n\n![ft.jpeg](attachment:ft.jpeg) <br>\n\nAs explained in the EDA, we observed a clear relationship between the hour of the day and the number of trips. We also explained that probably the more trips on a given hour, the higher the duration of a trip starting at this hour. However, we may think that this correlation can also be observed on a time period shorter than 1 hour; for example, 30 minutes, 20 minutes, 15 minutes or 10 minutes.\n\nTherefore, we may create a feature similar to the `pickup_hour` feature for such a smaller time range. After trying the time ranges mentioned before, we found that the one giving the best result is 15 minutes. We thus create a feature corresponding to the quarter of an hour when the trip started.\n\nAlso, we note that after having created such a feature, the year, month, day, hour and minute features are no longer really useful for our future models. As a result, we also remove these features in the feature engineering function below.","543e3eff":"Very few values are missing (at most 0.27% of values for `dropoff_zone`). We notice that the ratio of missing values is the same for `VendorID` and `passenger_count`. Let's check that `VendorID` is missing if and only if `passenger_count` is missing.","d615e7f5":"## 4.2. Regressors\n\n![lightgbm.jpg](attachment:lightgbm.jpg) <br>\n\nOf course, we use gradient boosted trees! From the experiments I did, `lightgbm` seems to give better results than `xgboost` on this dataset. We first tune `n_estimators` and `num_leaves` on a single LightGBM regressor to get the best possible result on the validation set, and we then use ensembling of LightGBM regressors through a voting classifier, building a smart mix of many shallow trees and few deep trees.\n\nWe then perform some fine-tuning of other parameters to improve the performance. We first decrease the `learning_rate` and increase `n_estimators`, and finally tune `reg_lambda` and `colsample_bytree`.","70fbae62":"## 2.2. Outlier Correction\n\nGiven the previous outlier analysis, we apply here the absolute value to the trip distance column on train and test sets.","2a8bcc68":"## 3.3. Zone Encoding\n\n![dimensionality.png](attachment:dimensionality.png) <br>\n\nFor dealing with categorical features in such problems, we usually perform label or one-hot encoding. If it is reasonable to perform one-hot encoding with boroughs (as we only have 7 unique boroughs), it is not for zones as we have around 265 unique zones. However, label encoding for zones would not be a solution either as labels would not be ordered and our gradient boosted trees methods would have to perform almost as many splits as values for this feature.\n\nWe thus perform a mix between label and one-hot encoding, performing the one-hot encoding of boroughs, and then assigning the zone value in the corresponding column.\n\nFor example, if a trip started in the borough 3 and zone 54, `pickup_borough_3` will be assigned the value 54 (instead of 1 for classical one-hot encoding) and other `pickup_borough_i` columns will be assigned the value 0 for $i$ not equal to 3.\n\nSuch a transformation has the benefit for the training data to have both few features and few values for each feature.","8cca38ab":"Let's check the ratio of missing values for each column of our test set is approximately the same as in the train set.","28364980":"## 4.3. Validation\n\n![cv.jpeg](attachment:cv.jpeg) <br>\n\nWe display the performance of our estimators on the validation set *only on trips lasting less than approximately 2 hours*, as very few outliers are remaining in our training data, and taking the risk to have a bad performance on outliers of the test set. We also use Root Mean Squared Logarithmic Error (RMSLE) in addition to RMSE to evaluate the performance.\n\nFortunately, we note that the performance of the voting classifier based on our 5 LightGBM regressors is better than all of the regressors taken individually.","6347780b":"# 1. Preliminary Study\n\n## 1.1. Data Loading\n\n![loading.jpeg](attachment:loading.jpeg) <br>\n\nLet's load data and display a sample of train and test sets.","d0020906":"## 1.2. Missing Values\n\nLet's evaluate the ratio of missing values for each column of our train set.","1ae1a35b":"## 2.3. Outlier Removal\n\nAgain, given the previous outlier analysis, we remove here trips lasting more than 3 hours, as well as trips having a distance equal to 0, or a duration equal to 0, or a very high speed, or a very low speed.\n\nOf course, we remove outliers only on the train set as we still need to predict duration of outliers on the test set.","8d204210":"We check that the train RMSE is not too far from the validation RMSE to avoid overfitting. Moreover, the distribution of predictions on the test data is the same as the distribution for predictions on train data, confirming there are few chances of overfitting.\n\nPublic score for this model is 258.47466, which is almost the validation RMSE for the voting classifier, showing again that the model clearly does not overfit and learned a bit more by training on 25% more data. The private score is 253.84098, confirming again the absence of overfitting.\n\n**Many thanks to the UC Berkeley IEOR 242 Spring 2020 team (Prof. George Ng and GSIs Peijie Li & Yunduan Lin) for the organization of this competition!**","a72b8233":"## 3.5. Feature Correlations\n\nData downcasting is also useful for accurate computations of correlations. We display the correlation matrix of our train data after all the transformations performed before.","a9aafcc1":"# 3. Further Processing\n\n## 3.1. Target Transformation\n\n![image.jpg](attachment:image.jpg) <br>\n\nWhen plotting the distribution of trips `duration`, we can see that the distribution is also lognormal. Therefore, as we removed trips with a duration equal to 0 in the previous section, we may want to fit our regressors of the log transformation of the target, in order the reduce a bias of the classifier that could be due to very high trip durations.\n\n*N.B.:* We perform target transformation only in the `fit` of the regressors in order not to forget to perform the inverse transformation when doing the `predict`."}}