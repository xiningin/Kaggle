{"cell_type":{"09b41c08":"code","eeb53c69":"code","e44f8d35":"code","861f4cea":"code","98e0a275":"code","878149e9":"code","13264b71":"code","6e71db8f":"code","4ae7cb4b":"code","24fdaff0":"code","d7a91093":"code","8cf4ea43":"code","97e77c24":"code","f637d1a1":"code","c885df2f":"code","2c35ad47":"code","3e03b1b0":"code","9a573c88":"code","478a87c3":"markdown","50f54b3c":"markdown","b2d905f9":"markdown","3eb35b0c":"markdown","8ca378a8":"markdown","1fdfa667":"markdown","d5e988a5":"markdown","efb5cb97":"markdown","2ba8915d":"markdown","858b7eab":"markdown","c329530d":"markdown"},"source":{"09b41c08":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.ticker as mtick\nimport matplotlib.dates as mdates\nimport altair as alt","eeb53c69":"gdpr_violations = pd.read_table('https:\/\/raw.githubusercontent.com\/rfordatascience\/tidytuesday\/master\/data\/2020\/2020-04-21\/gdpr_violations.tsv', parse_dates = ['date'])\ngdpr_text = pd.read_table('https:\/\/raw.githubusercontent.com\/rfordatascience\/tidytuesday\/master\/data\/2020\/2020-04-21\/gdpr_text.tsv')","e44f8d35":"gdpr_violations.rename({'name': 'country'}, inplace = True, axis = 1)\nmask = gdpr_violations['date'] == pd.to_datetime('1970-01-01 00:00:00')\ngdpr_violations['date'] = np.where(mask, np.datetime64(\"NaT\") , gdpr_violations['date'])\ngdpr_violations.head()","861f4cea":"total_fine = gdpr_violations.price.sum()\n\nprint(f'Total amount fined has been: {total_fine:,}')","98e0a275":"fine_per_country = gdpr_violations.groupby('country')['price'].agg(np.nansum)\nlist_largest_countries = fine_per_country.nlargest(8).index\nfine_per_country = fine_per_country.reset_index()\nfine_per_country['country'] = np.where(fine_per_country.country.isin(list_largest_countries), fine_per_country['country'], 'other')\nfine_per_country = fine_per_country.groupby('country')['price'].agg(np.nansum)","878149e9":"ax = fine_per_country.sort_values().plot(kind = 'barh')\nsns.despine()\nfmt = '${x:,}'\ntick = mtick.StrMethodFormatter(fmt)\nax.xaxis.set_major_formatter(tick)\nplt.xticks(rotation = 25)\nplt.show()","13264b71":"fig, ax = plt.subplots(figsize=(10, 6))\nax.yaxis.set_major_formatter(tick)\ngdpr_violations.set_index('date')['price'].resample('M', label = 'left').sum().plot(kind = 'bar', ax = ax)  \nsns.despine()\nplt.show()","6e71db8f":"by_year_by_country = gdpr_violations.set_index('date').groupby([pd.Grouper(freq='M', label = 'left'), 'country']).sum().reset_index()\n\nlist_largest_countries = by_year_by_country.nlargest(6, 'price').country\nby_year_by_country = by_year_by_country.reset_index()\nby_year_by_country['country'] = np.where(by_year_by_country.country.isin(list_largest_countries), by_year_by_country['country'], 'other')\n\nalt.Chart(by_year_by_country, \n         width = 1000, height = 500).mark_bar().encode(\n    x='date',\n    y=alt.Y('price', title = 'Total fines'),\n    color=alt.Color('country', sort = ['France', 'Italy', 'Germany'])\n)","4ae7cb4b":"gdpr_violations[['controller', 'date', 'article_violated', 'type', 'summary', 'price']].sort_values('price', ascending = False)","24fdaff0":"gdpr_violations['article_violated'].str.split('\\\\|', expand = True).melt()['value'].value_counts() ","d7a91093":"gdpr_violations['article_violated'].str.split('\\\\|', expand = True, ).melt()['value'].str.extract('Art\\\\. ?(\\\\d+)')[0].value_counts()","8cf4ea43":"articles_wide = pd.concat([gdpr_violations, gdpr_violations['article_violated'].str.split('\\\\|', expand = True)], axis = 'columns')\nid_vars = articles_wide.columns.difference([0, 1, 2, 3, 4])\nseparated_articles = articles_wide.melt(id_vars).sort_values('id').dropna(subset = ['value'], axis = 0).rename({'value': 'article_sep'}, axis = 1)\nseparated_articles['article_number'] = separated_articles['article_sep'].str.extract(\"Art\\\\. ?(\\\\d+)\")\nseparated_articles","97e77c24":"separated_articles['price_per_article'] = separated_articles.groupby('id')['price'].transform(lambda x: x\/x.shape[0])\nmost_common_articles = separated_articles.groupby('article_number')['price'].agg('sum').sort_values(ascending = False)[:8].index.tolist()\nmost_common_articles","f637d1a1":"separated_articles['article_number'] = np.where(separated_articles['article_number'].isin(most_common_articles), separated_articles['article_number'], 'other')\nseparated_articles.groupby('article_number')['price_per_article'].agg([np.nansum, np.size]).sort_values('nansum', ascending = False)","c885df2f":"# get 8 most profitable type of violations \nmost_common_types = gdpr_violations.groupby('type')['price'].agg('sum').sort_values(ascending = False)[:8].index.tolist()\nmost_common_types","2c35ad47":"# lump violations that accrued less money into a category of their own\ngdpr_violations['type'] = np.where(gdpr_violations['type'].isin(most_common_types), gdpr_violations['type'], 'other')\n# order categories by their mean\norder = gdpr_violations.query('price > 0').groupby('type')['price'].agg('median').sort_values(ascending = False).index.to_list()\n\n# box plot\nbox_plot = alt.Chart(gdpr_violations.query('price>0'), width = 600, height = 500).mark_boxplot(color = \"gray\").encode(\n    x=alt.X('price', scale=alt.Scale(type = 'log', base = 10)),\n    y=alt.Y('type', sort = order))\n# points\npoints = alt.Chart(gdpr_violations.query('price>0')).mark_circle(opacity = 0.3,\n                                                                color = \"red\").encode(\n    x=alt.X('price', scale=alt.Scale(type = 'log', base = 10)),\n    y=alt.Y('type', sort = order))\n\nbox_plot + points","3e03b1b0":"# get article titles\narticle_titles = gdpr_text[['article_title', 'article']].drop_duplicates()\n# data munging to be able to merge\nseparated_articles['article_number'] = pd.to_numeric(separated_articles['article_number'], errors = \"coerce\")\n# merge and get total fine per article\nax = (pd.merge(separated_articles, article_titles, left_on = 'article_number', right_on = 'article').rename({0: 'counts'}, axis = 1).\n    groupby('article_title')['price_per_article'].agg('sum').sort_values().plot(kind = 'barh'))\n# plot styling\nplt.title('What articles got the most fines?')\nax.xaxis.set_major_formatter(tick)\nplt.xticks(rotation = 25)\nsns.despine()\nplt.xlabel('Total Fine')\nplt.ylabel('');","9a573c88":"mask = gdpr_violations['controller'].str.contains('Vodafone')\n\nalt.Chart(gdpr_violations[mask].groupby(['date', 'country'])['price'].agg([np.size, np.sum]).reset_index()).mark_circle().encode(\n    x = 'date',\n    y = alt.Y('sum', title = 'Total fines on this day'),\n    color = 'country',\n    size = 'size'\n).properties(title = \"Vodafone's GDPR violations\")","478a87c3":"### Again we turn at articles\n\nBut this time connecting it with the text data. ","50f54b3c":"### Which article was violated?","b2d905f9":"The several category, where Google was included, is a huge outlier. With only to fines. Whereas the failure to implement.. and non-compliance seem to have many different observations. ","3eb35b0c":"There seems to be a lot of variations. Let's include the countries in this analysis. However, stacked plots are notoriously difficult to plot in `matplotlib`. Thus, I instead switch to plot with `altair`.","8ca378a8":"*What's the total amount that has been fined?*","1fdfa667":"## Reading the Data\n\nNotice that `pandas` support parsing the date from the moment we read the data. ","d5e988a5":"So it seems that article 5 was not only the most common but also the one that accrued more money in fines. The 58 is the huge fine that Google received in France. \n\n## Distribution of fines per Type","efb5cb97":"This time, it took some sidesteps to arrive at what Dave did: ","2ba8915d":"Notice the lengths I had to go to in order to lump the countries whom had been least fined into a group of their own. ","858b7eab":"The greatest culprits tend to be France, Italy and Germany.\n\nNext, let's plot the time series of the fines by upsampling by month","c329530d":"# Copying Dave Robinson to understand Python\n\nIn language learning, there is this concept of triangulation: use your second language to learn your third. For example, use your French knoweldge to learn German. Thus, one can refresh earlier concepts **AND** learn a new language simultaneously. \n\nThis notebook is my attempt at triangulation in programming. Having read and wrote some R, I attempt a TidyTuesday submission in Python following step by step what Dave Robinson did in his [screencast](https:\/\/www.youtube.com\/watch?v=vT-DElIaKtE) with R. That is, I attempt to replicate almost every step he did in his analysis using Python. Thus, I am able to keep up with the growing tidyverse in R and at the same time I get to learn how to do advanced data analysis in Python. \n\n## What is TidyTuesday?\n\n> A weekly [data project](https:\/\/github.com\/rfordatascience\/tidytuesday) aimed at the R ecosystem. As this project was borne out of the R4DS Online Learning Community and the R for Data Science textbook, an emphasis was placed on understanding how to summarize and arrange data to make meaningful charts with ggplot2, tidyr, dplyr, and other tools in the tidyverse ecosystem. However, any code-based methodology is welcome - just please remember to share the code used to generate the results.\n\n## This week's TidyTuesday"}}