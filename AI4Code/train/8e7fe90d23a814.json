{"cell_type":{"36a9ce82":"code","0ce7159e":"code","fd80d0a9":"code","9995c062":"code","e5cdb0fe":"code","434ab497":"code","e1bd6712":"code","cd99628f":"code","ef6b26e0":"code","ab99af36":"code","4801285e":"code","7f2b79ee":"code","784fd3f7":"code","9cec263d":"code","2c2fca0a":"code","bb053b19":"code","b3ac5325":"code","f073d118":"code","3e586769":"code","9060a793":"code","a6ab15c9":"code","772ff7c1":"code","d6ff2548":"code","2d3c5217":"code","5b50a938":"code","b99a798e":"code","3f6b2d65":"code","566599a1":"code","ed363fff":"code","f2af0a78":"code","0b9ca51a":"code","c799d70b":"code","46716bd7":"markdown","601cd025":"markdown","a43415c3":"markdown","193d5f1c":"markdown","6d28dac9":"markdown","d682e560":"markdown","dc96fa91":"markdown","aad3b256":"markdown","72f7e056":"markdown","68e57ab1":"markdown","501eaf56":"markdown"},"source":{"36a9ce82":"import os\nimport numpy as np\nimport pandas as pd\nimport random\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sns\n\nfrom contextlib import contextmanager\nfrom time import time\nfrom tqdm import tqdm\nimport lightgbm as lgbm\nimport category_encoders as ce\n\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.metrics import classification_report, log_loss, accuracy_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold","0ce7159e":"data0 = pd.read_csv(\"..\/input\/mercedes-used-car-listing\/merc.csv\")\ndata0","fd80d0a9":"print(data0.columns.tolist())","9995c062":"df=data0.copy()\nfrom sklearn.preprocessing import LabelEncoder\nfor c in df.columns:\n    if df[c].dtype=='object': \n        df[c] = df[c].fillna('N')\n        lbl = LabelEncoder()\n        lbl.fit(list(df[c].values))\n        df[c] = lbl.transform(df[c].values)","e5cdb0fe":"data1=df\ndata1","434ab497":"data1.columns","e1bd6712":"data1.info()","cd99628f":"target=['price']\ndataY=data1[target[0]]\ndataX=data1.drop(target,axis=1)","ef6b26e0":"print(dataY[0:5].T)\nprint()\nprint(dataX[0:5].T)","ab99af36":"n=len(dataX)\nprint(n)\nN=list(range(n))\nrandom.seed(2021)\nrandom.shuffle(N)","4801285e":"trainX=dataX.iloc[N[0:(n\/\/4)*3]]\ntrainY=dataY.iloc[N[0:(n\/\/4)*3]]\ntestX=dataX.iloc[N[(n\/\/4)*3:]]\ntestY=dataY.iloc[N[(n\/\/4)*3:]]","7f2b79ee":"df_columns = list(dataX.columns)\nprint(df_columns)","784fd3f7":"def create_numeric_feature(input_df):\n    use_columns = df_columns \n    return input_df[use_columns].copy()","9cec263d":"from contextlib import contextmanager\nfrom time import time\n\nclass Timer:\n    def __init__(self, logger=None, format_str='{:.3f}[s]', prefix=None, suffix=None, sep=' '):\n\n        if prefix: format_str = str(prefix) + sep + format_str\n        if suffix: format_str = format_str + sep + str(suffix)\n        self.format_str = format_str\n        self.logger = logger\n        self.start = None\n        self.end = None\n\n    @property\n    def duration(self):\n        if self.end is None:\n            return 0\n        return self.end - self.start\n\n    def __enter__(self):\n        self.start = time()\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.end = time()\n        out_str = self.format_str.format(self.duration)\n        if self.logger:\n            self.logger.info(out_str)\n        else:\n            print(out_str)","2c2fca0a":"from tqdm import tqdm\n\ndef to_feature(input_df):\n\n    processors = [\n        create_numeric_feature,\n    ]\n    \n    out_df = pd.DataFrame()\n    \n    for func in tqdm(processors, total=len(processors)):\n        with Timer(prefix='create' + func.__name__ + ' '):\n            _df = func(input_df)\n\n        assert len(_df) == len(input_df), func.__name__\n        out_df = pd.concat([out_df, _df], axis=1)\n        \n    return out_df","bb053b19":"train_feat_df = to_feature(trainX)\ntest_feat_df = to_feature(testX)","b3ac5325":"import lightgbm as lgbm\nfrom sklearn.metrics import mean_squared_error\n\ndef fit_lgbm(X, y, cv, \n             params: dict=None, \n             verbose: int=50):\n\n    if params is None:\n        params = {}\n\n    models = []\n    oof_pred = np.zeros_like(y, dtype=np.float)\n\n    for i, (idx_train, idx_valid) in enumerate(cv): \n        x_train, y_train = X[idx_train], y[idx_train]\n        x_valid, y_valid = X[idx_valid], y[idx_valid]\n\n        clf = lgbm.LGBMRegressor(**params)\n        \n        with Timer(prefix='fit fold={} '.format(i)):\n            clf.fit(x_train, y_train, \n                    eval_set=[(x_valid, y_valid)],  \n                    early_stopping_rounds=100,\n                    verbose=verbose)\n\n        pred_i = clf.predict(x_valid)\n        oof_pred[idx_valid] = pred_i\n        models.append(clf)\n        print(f'Fold {i} RMSLE: {mean_squared_error(y_valid, pred_i) ** .5:.4f}')\n        print()\n\n    score = mean_squared_error(y, oof_pred) ** .5\n    print('-' * 50)\n    print('FINISHED | Whole RMSLE: {:.4f}'.format(score))\n    return oof_pred, models","f073d118":"params = {\n    'objective': 'rmse', \n    'learning_rate': .1,\n    'reg_lambda': 1.,\n    'reg_alpha': .1,\n    'max_depth': 5, \n    'n_estimators': 10000, \n    'colsample_bytree': .5, \n    'min_child_samples': 10,\n    'subsample_freq': 3,\n    'subsample': .9,\n    'importance_type': 'gain', \n    'random_state': 71,\n    'num_leaves': 62\n}","3e586769":"y = trainY\nprint(y.shape)","9060a793":"ydf=pd.DataFrame(y)\nydf","a6ab15c9":"from sklearn.model_selection import KFold\n\nfor i in range(1):\n    fold = KFold(n_splits=5, shuffle=True, random_state=71)\n    ydfi=ydf.iloc[:,i]\n    y=np.array(ydfi)\n    cv = list(fold.split(train_feat_df, y))\n    oof, models = fit_lgbm(train_feat_df.values, y, cv, params=params, verbose=500)\n    \n    fig,ax = plt.subplots(figsize=(6,6))\n    ax.set_title(target[i],fontsize=20)\n    ax.set_ylabel('Predicted Train '+target[i],fontsize=12)\n    ax.set_xlabel('Actual Train '+target[i],fontsize=12)\n    ax.scatter(y,oof)","772ff7c1":"ydf['oof']=oof\nydf['rate']=(ydf['oof']-ydf['price'])\/ydf['price']\nydf.sort_values('rate',ascending=False)","d6ff2548":"data0.loc[11378:11378]","2d3c5217":"data0.loc[11430:11430]","5b50a938":"def visualize_importance(models, feat_train_df):\n\n    feature_importance_df = pd.DataFrame()\n    for i, model in enumerate(models):\n        _df = pd.DataFrame()\n        _df['feature_importance'] = model.feature_importances_\n        _df['column'] = feat_train_df.columns\n        _df['fold'] = i + 1\n        feature_importance_df = pd.concat([feature_importance_df, _df], \n                                          axis=0, ignore_index=True)\n\n    order = feature_importance_df.groupby('column')\\\n        .sum()[['feature_importance']]\\\n        .sort_values('feature_importance', ascending=False).index[:50]\n\n    fig, ax = plt.subplots(figsize=(8, max(6, len(order) * .25)))\n    sns.boxenplot(data=feature_importance_df, \n                  x='feature_importance', \n                  y='column', \n                  order=order, \n                  ax=ax, \n                  palette='viridis', \n                  orient='h')\n    \n    ax.tick_params(axis='x', rotation=0)\n    #ax.set_title('Importance')\n    ax.grid()\n    fig.tight_layout()\n    \n    return fig,ax\n\n#fig, ax = visualize_importance(models, train_feat_df)","b99a798e":"for i in range(1):\n    fold = KFold(n_splits=5, shuffle=True, random_state=71)\n    ydfi=ydf.iloc[:,i]\n    y=np.array(ydfi)\n    cv = list(fold.split(train_feat_df, y))\n    oof, models = fit_lgbm(train_feat_df.values, y, cv, params=params, verbose=500)\n    fig, ax = visualize_importance(models, train_feat_df)\n    ax.set_title(target[i]+' Imortance',fontsize=20)\n","3f6b2d65":"fig,ax = plt.subplots(figsize=(6,6))\nax.set_title('Price vs Mileage',fontsize=20)\nax.set_xlabel('Train Mileage',fontsize=12)\nax.set_ylabel('Train Price',fontsize=12)\nax.scatter(trainX['mileage'],trainY)","566599a1":"fig,ax = plt.subplots(figsize=(6,6))\nax.set_title('Price vs EngineSize',fontsize=20)\nax.set_xlabel('Train EngineSize',fontsize=12)\nax.set_ylabel('Train Price',fontsize=12)\nax.scatter(trainX['engineSize'],trainY)","ed363fff":"pred0 = np.array([model.predict(test_feat_df.values) for model in models])\nPRED = pred0[4]\nPRED[0:5]","f2af0a78":"ANS=np.array(testY)\nANS[0:5]","0b9ca51a":"fig, ax = plt.subplots(figsize=(8,8))\nsns.histplot(PRED, label='Predicted Test '+target[0], ax=ax, color='black',bins=30)\nsns.histplot(oof, label='Predicted Train '+target[0], ax=ax, color='C1',bins=30)\nax.legend()\nax.grid()","c799d70b":"fig,ax = plt.subplots(figsize=(6,6))\nax.set_title(target[0],fontsize=20)\nax.set_xlabel('Actual Test '+target[0],fontsize=12)\nax.set_ylabel('Predicted Test '+target[0],fontsize=12)\nax.scatter(ANS,PRED)","46716bd7":"# Import Libraries","601cd025":"# Cost Performance is defined as ...\n### (Predicted price - Actual price) \/ (Actual price)","a43415c3":"## **2006 Mercedes-Benz C Class**\n<img src=\"https:\/\/file.kelleybluebookimages.com\/kbb\/base\/house\/2006\/2006-Mercedes-Benz-C-Class-FrontSide_MBC230061_505x375.jpg\" width=\"100%\">","193d5f1c":"## the Highest on cost performance","6d28dac9":"<h1><center>Which Mercedes is the best?<\/center><\/h1>","d682e560":"## the Lowest on cost performance","dc96fa91":"# Model","aad3b256":"# Data preparation","72f7e056":"# Visualize Importance","68e57ab1":"## **2004 Mercedes-Benz M Class**\n<img src=\"https:\/\/images.hgmsites.net\/lrg\/2015-mercedes-benz-m-class_100480899_l.jpg\" width=\"100%\">","501eaf56":"# Target setting"}}