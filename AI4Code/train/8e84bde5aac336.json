{"cell_type":{"8e4b4bdf":"code","b38432de":"code","b537af06":"code","2c9f9a3f":"code","d665758c":"code","21c17011":"code","a2215bdd":"code","a2c35553":"code","bc141dca":"code","e8ad0123":"code","19c2faae":"code","df2dd52b":"code","181e3ae8":"code","b5d68057":"code","b577e2fb":"code","da3dc452":"code","c2493eb0":"code","fbc2c84b":"code","f8b17828":"code","4c76d4e9":"code","0e6dda87":"code","947ce736":"code","e7170cb5":"code","20b46d30":"code","a6b58e86":"code","367ec8e5":"code","968aa93f":"code","a9fc4124":"code","e66e918f":"code","60d54a31":"code","8f9f7253":"code","ab1e058f":"code","6d4bd2f0":"code","b92b8e18":"code","8de43bb4":"code","f6c46c19":"code","93458998":"code","d7484de4":"code","33e955c5":"code","35af678d":"code","4922a9fa":"code","668e028e":"code","03729ac9":"code","8113bc6f":"code","7a9f31ff":"code","01cce626":"code","35859f66":"code","8827f101":"code","4fa75af2":"code","7467568f":"code","8d206c1a":"code","e243c873":"code","f0c5fe50":"code","76515aa2":"code","967993e5":"code","2fff36c8":"code","5e6507a3":"code","191e3e94":"code","bb7131f7":"code","41123e67":"code","a8ebcc96":"code","efb8fb81":"code","c4d93187":"code","83b70bc2":"code","6aa7ac13":"code","4b3893f2":"markdown","137dc03b":"markdown","86356aac":"markdown","0ddde38f":"markdown","a97af356":"markdown","e3e55b08":"markdown"},"source":{"8e4b4bdf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nfrom sklearn.decomposition import PCA\n\nKNNclassifier=KNeighborsClassifier(n_neighbors=5)\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.naive_bayes import GaussianNB\n\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\nimport keras\nfrom keras.optimizers import SGD\nimport graphviz\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","b38432de":"#Importing the training and testing data\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')","b537af06":"#Checking the first five row in training data\ntrain.head()","2c9f9a3f":"#stats of the training data\ntrain.describe()","d665758c":"#Familiarising with the Column name \ntrain.columns","21c17011":"#Checking the data type for better understanding\ntrain.dtypes","a2215bdd":"#countplot of survived by sex\nsns.set_style('whitegrid')\nsns.countplot(x='Survived',hue='Sex',data=train)","a2c35553":"#Heatmap plot of the missing data or null present in the training data\nsns.heatmap(train.isnull(),yticklabels=False,cmap='viridis',cbar=False)","bc141dca":"#Boxplot of the Pclass,Age and Survived \nplt.figure(figsize=(19, 8))\nsns.boxplot(x='Pclass',y='Age',data=train,hue='Survived',color=\"cyan\")","e8ad0123":"# total null presnets in the training data\ntrain.isna().sum().max()","19c2faae":"#Function fortotal null  present\ndef missing_total(data):\n    missing_total= data.isna().sum().sort_values(ascending=False)\n    return missing_total","df2dd52b":"#Function for null percentage present\ndef missing_percent(data):\n    missing_percent = ((data.isna().sum()\/data.isna().count())*100).sort_values(ascending=False)\n    return missing_percent","181e3ae8":"#Table for null percentage and total null present\ntrain_missing = pd.concat([missing_total(train), missing_percent(train)], axis=1, keys=['missing_total', 'missing_percent'])\ntrain_missing.head(50)","b5d68057":"#further anaylsis on missing train data\n#Removing any columns that contains nan greater than 20% which is only cabin\ntrain = train.drop((train_missing[train_missing['missing_percent'] > 20.0]).index,1)","b577e2fb":"#Fill the nan with median of the columns\n#train.fillna(train.median(), inplace=True)","da3dc452":"#complete embarked with mode\ntrain['Embarked'].fillna(train['Embarked'].mode()[0], inplace = True)","c2493eb0":"#Function to create more feature as Age Category\ndef age_group(val):\n    if val<2:\n        return 'Infant'\n    elif val>2 and val < 10:\n        return 'Child'\n    elif val>10 and val < 17:\n        return 'Adolescence'\n    elif val>17 and val < 24:\n        return 'Teen'\n    elif val>24 and val < 65:\n        return 'Adult'\n    else:\n        return 'Elderly'\ntrain['Age_category']=train['Age'].apply(age_group)","fbc2c84b":"#Splitting the data into categorical data, float and Varaible\ntrain_Var = train[train.dtypes[train.dtypes == \"int64\"].index]\n\ntrain_Cat = train[train.dtypes[train.dtypes == \"object\"].index]\n\ntrain_float = train[train.dtypes[train.dtypes == \"float\"].index]","f8b17828":"#Creating more features from Name\ntrain_Cat['Title'] = train_Cat['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\nstat_min = 10\ntitle_names = (train_Cat['Title'].value_counts() < stat_min)\ntrain_Cat['Title'] = train_Cat['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)\nprint(train_Cat['Title'].value_counts())\nprint(\"-\"*10)","4c76d4e9":"# dropping Name and Ticket columns name because they are not important for the analysis\ndrop_column = ['Name', 'Ticket']\ntrain_Cat.drop(drop_column, axis=1, inplace = True)","0e6dda87":"#Converting the categorical data into dummy variable for easy analysis\ntrain_Cat = pd.get_dummies(train_Cat)","947ce736":"train_Cat.head(20)","e7170cb5":"train_Var","20b46d30":"#Fill the nan with median of the columns\ntrain_float['Age'].fillna(train_float['Age'].median(), inplace = True)","a6b58e86":"#dropping the PassengerId becuase it is not important for the prediction\ntrain_Var.drop('PassengerId', axis=1, inplace = True)","367ec8e5":"#concatinating the three data type together\ninputData=train_Var.join([train_Cat, train_float])","968aa93f":"#Checking for the last time the total null value available\ninputData.isna().sum().max()","a9fc4124":"inputData.head()","e66e918f":"inputData.shape","60d54a31":"#Checking the first five row in testing data\ntest.head()","8f9f7253":"#stats of the testing data\ntest.describe()","ab1e058f":"#Familiarising with the Column name \ntest.columns","6d4bd2f0":"#Checking the data type for better understanding\ntest.dtypes","b92b8e18":"# total null presnets in the testing data\ntest.isna().sum().max()","8de43bb4":"#Table for null percentage and total null present\ntest_missing = pd.concat([missing_total(test), missing_percent(test)], axis=1, keys=['missing_total', 'missing_percent'])\ntest_missing.head(50)","f6c46c19":"#Removing any columns that contains nan greater than 20% which is only cabin\ntest = test.drop((test_missing[test_missing['missing_percent'] > 50.0]).index,1)","93458998":"test.head(5)","d7484de4":"drop_column = ['PassengerId', 'Ticket']\ntest.drop(drop_column, axis=1, inplace = True)","33e955c5":"#Fill the nan with median of the columns\ntest['Age'].fillna(test['Age'].median(), inplace = True)\ntest['Fare'].fillna(test['Fare'].median(), inplace = True)","35af678d":"def age_cat(val):\n    if val<2:\n        return 'Infant'\n    elif val>2 and val < 10:\n        return 'Child'\n    elif val>10 and val < 17:\n        return 'Adolescence'\n    elif val>17 and val < 24:\n        return 'Teen'\n    elif val>24 and val < 65:\n        return 'Adult'\n    else:\n        return 'Elderly'\ntest['Age_category']=test['Age'].apply(age_cat)","4922a9fa":"#Splitting the data into categorical data and Varaible\ntest_Var = test[test.dtypes[test.dtypes == \"int64\"].index]\n\ntest_Cat = test[test.dtypes[test.dtypes == \"object\"].index]\n\ntest_float = test[test.dtypes[test.dtypes == \"float\"].index]","668e028e":"#Feature Engineering\ntest_Cat['Title'] = test_Cat['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\nstat_min = 10\ntitle_names = (test_Cat['Title'].value_counts() < stat_min)\ntest_Cat['Title'] = test_Cat['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)\nprint(test_Cat['Title'].value_counts())\nprint(\"-\"*10)","03729ac9":"test_Cat.drop('Name', axis=1, inplace = True)","8113bc6f":"#Converting the categorical data into dummy variable for easy analysis\ntest_Cat = pd.get_dummies(test_Cat)","7a9f31ff":"test_Cat.head(5)","01cce626":"test_Cat.isna().sum().max()","35859f66":"outputData=test_Var.join([test_Cat, test_float])","8827f101":"outputData.head(5)","4fa75af2":"outputData.isna().sum().max()","7467568f":"X_train = inputData.drop(['Survived'],axis=1).values\nY_train = inputData['Survived'].values\nX_test =outputData.values","8d206c1a":"#Data Processing\n#random seed------ Meaning of random seed is explained in the documentation\nseed = 0\nnp.random.seed(seed)\nX_training, X_testing, Y_training, Y_testing= train_test_split(X_train, Y_train.reshape(-1,1), test_size=0.1, random_state=seed)\nX_scaler = MinMaxScaler()\nY_scaler = MinMaxScaler()\n\nX_scaled_training = X_scaler.fit_transform(X_training)\nY_scaled_training = Y_scaler.fit_transform(Y_training)\nX_scaled_testing = X_scaler.transform(X_testing)\nY_scaled_testing = Y_scaler.transform(Y_testing)\nX_scaled_test = X_scaler.transform(X_test)","e243c873":"features_selection = LassoCV(0.5, cv=5)\nreg = SelectFromModel(features_selection)\nreg.fit(X_scaled_training, Y_scaled_training)\n\nX_scaled_training = reg.transform(X_scaled_training)\n\n#Printing the features selected\nfor feature_list_index in reg.get_support(indices=True):\n    print([feature_list_index])","f0c5fe50":"#Transforming the testing data to the actual number of features selected for the training data\nX_scaled_testing = reg.transform(X_scaled_testing)\nX_scaled_test = reg.transform(X_scaled_test)","76515aa2":"KNNclassifier.fit(X_scaled_training ,Y_scaled_training)\ny_pred = KNNclassifier.predict(X_scaled_testing)\nprint(\"Accuracy :\",accuracy_score(Y_scaled_testing,y_pred)*100)\ncm = confusion_matrix(Y_scaled_testing, y_pred)\nprint(\"Confusion Matrix:\\n\", cm)","967993e5":"SVMlinear=SVC(kernel='linear')\nSVMlinear.fit(X_scaled_training ,Y_scaled_training)\ny_pred = SVMlinear.predict(X_scaled_testing)\nprint(\"Accuracy :\",accuracy_score(Y_scaled_testing,y_pred)*100)\ncm = confusion_matrix(Y_scaled_testing,y_pred)\nprint(\"Confusion Matrix:\\n\", cm)","2fff36c8":"SVMrbf=SVC(kernel='rbf')\nSVMrbf.fit(X_scaled_training ,Y_scaled_training)\ny_pred = SVMrbf.predict(X_scaled_testing)\nprint(\"Accuracy :\",accuracy_score(Y_scaled_testing,y_pred)*100)\ncm = confusion_matrix(Y_scaled_testing,y_pred)\nprint(\"Confusion Matrix:\\n\", cm)","5e6507a3":"NB=GaussianNB()\nNB.fit(X_scaled_training ,Y_scaled_training)\ny_pred = NB.predict(X_scaled_testing)\nprint(\"Accuracy :\",accuracy_score(Y_scaled_testing,y_pred)*100)\ncm = confusion_matrix(Y_scaled_testing,y_pred)\nprint(\"Confusion Matrix:\\n\", cm)","191e3e94":"DecisionTree=DecisionTreeClassifier(criterion='entropy',random_state=23)\nDecisionTree.fit(X_scaled_training ,Y_scaled_training)\ny_pred = DecisionTree.predict(X_scaled_testing)\nprint(\"Accuracy :\",accuracy_score(Y_scaled_testing,y_pred)*100)\ncm = confusion_matrix(Y_scaled_testing,y_pred)\nprint(\"Confusion Matrix:\\n\", cm)","bb7131f7":"RFC=RandomForestClassifier(n_estimators=17,criterion='entropy',random_state=0)\nRFC.fit(X_scaled_training ,Y_scaled_training)\ny_pred = RFC.predict(X_scaled_testing)\nprint(\"Accuracy :\",accuracy_score(Y_scaled_testing,y_pred)*100)\ncm = confusion_matrix(Y_scaled_testing,y_pred)\nprint(\"Confusion Matrix:\\n\", cm)","41123e67":"model = Sequential()\n\n# Inputing the first layer with input dimensions\nmodel.add(Dense(100, \n                activation='relu',  \n                input_dim=X_scaled_training.shape[1],\n                kernel_initializer='uniform'))\n\nmodel.add(Dense(20,\n                kernel_initializer='uniform',\n                activation='relu'))\n\n#adding second hidden layer \nmodel.add(Dense(1,\n                kernel_initializer='uniform',\n                activation='sigmoid'))\n#With such a scalar sigmoid output on a binary classification problem, the loss\n#function you should use is binary_crossentropy\n\n#Visualizing the model\nmodel.summary()","a8ebcc96":"#Creating an Stochastic Gradient Descent\nsgd = SGD(lr = 0.01, momentum = 0.9)\n\n# Compiling our model\nmodel.compile(optimizer = sgd, \n                   loss = 'binary_crossentropy', \n                   metrics = ['accuracy'])\n# Fitting the ANN to the Training set\nmodel.fit(X_scaled_training ,Y_scaled_training, \n               batch_size = 70, \n               epochs = 120, verbose=2)","efb8fb81":"#Using KNNclassifier for final prediction \nyhat =SVMrbf.predict(X_scaled_test).astype(int)","c4d93187":"#Transforming the prediction back to normal data before scaling\nprediction = Y_scaler.inverse_transform(yhat.reshape(-1,1)).astype(int)","83b70bc2":"Id = pd.read_csv('..\/input\/titanic\/test.csv')\nPrediction = pd.concat([pd.DataFrame(Id['PassengerId'], columns=['PassengerId']),pd.DataFrame(yhat, columns=['Survived'])],axis=1)","6aa7ac13":"#saving the prediction\nPrediction.to_csv('Prediction.csv', index=None)","4b3893f2":"The same analysis will be performed on the testing data","137dc03b":"*  Feature Engineering","86356aac":"1. Training data Pre-preocessing","0ddde38f":"2.  Test Data data preprocessing","a97af356":"Data Splitting","e3e55b08":"                                                                                                Titanic Model"}}