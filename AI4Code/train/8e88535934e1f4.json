{"cell_type":{"a61c691e":"code","1a7b9ea0":"code","52021600":"code","8e8c16b3":"code","ee3e116c":"code","3c6c745a":"code","e1f9c654":"code","bb0e8083":"code","8e011fc5":"code","38e679b2":"code","9d99ec97":"code","acf965bb":"code","19c8739c":"code","6373aacc":"code","b5c1f8de":"code","3f972008":"code","45865a68":"code","83f6fbba":"code","2c62a977":"code","c05a399b":"code","6d011213":"code","1b9f22df":"code","184b1741":"code","4f44097a":"code","82a3ac32":"code","dbc62200":"code","56c91ab6":"code","cb6505dd":"code","a5e433a0":"code","2a9d7fa1":"code","5f6befcd":"code","6b5c5622":"code","798982dd":"code","1ef2f83e":"code","433b028e":"markdown","243f683f":"markdown","9b0d7c1d":"markdown","3ab9f3db":"markdown","44fd815f":"markdown","eb713cf4":"markdown","418e925d":"markdown","30d540bc":"markdown","b3025d5a":"markdown","cb7d95df":"markdown","23dd89be":"markdown","72a38f2c":"markdown","16591ce4":"markdown","f47631e8":"markdown","303f0a04":"markdown","ceea5e11":"markdown","b00866ff":"markdown","aeaebe21":"markdown","d7897403":"markdown","e2c54a87":"markdown","48582c1a":"markdown","27bcf1ea":"markdown","90ac71ee":"markdown","4358997d":"markdown","9e5dec6b":"markdown","7366e260":"markdown","e0ed7298":"markdown","75d69eac":"markdown","9b3187e2":"markdown","95e003cb":"markdown","df927e30":"markdown","c8bf1825":"markdown","096f19c4":"markdown"},"source":{"a61c691e":"import pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport re\nfrom IPython.display import Markdown, display\nfrom matplotlib.patches import Rectangle\n\nfont = {'family' : 'normal',\n        'weight' : 'normal',\n        'size'   : 10}\n\nmatplotlib.rc('font', **font)\n\ndef printmd(string):\n    display(Markdown(string))\n\ndata = pd.read_csv('\/kaggle\/input\/kaggle-survey-2020\/kaggle_survey_2020_responses.csv')\nheaders = data.loc[:1]\ndata= data.drop(index=0,axis=0)\ndata['Time from Start to Finish (seconds)'] = pd.to_numeric(data['Time from Start to Finish (seconds)'])\n","1a7b9ea0":"'''This gets a list of columns that follow the assumed\/observed \nnaming convention of the dataset based on a simple input Q<number>. \ni.e. Q7 can be provided and the columns and labels for all Q7 columns will be returned. '''\ndef extractCols(question):\n    pattern = re.compile(\"^\" + question + \"(_|$)\")\n    neededCols = []\n    xLabels = []\n    for col in headers.columns:\n        if pattern.match(col):\n            neededCols.append(col)\n            choice = headers[col][0].split('Selected Choice - ')[-1]\n            trimmedChoice = choice.split('(')[0]\n            xLabels.append(trimmedChoice)\n    #watch out for questions with A and B parts\n    foundAPart = False\n    foundBPart = False\n    for col in neededCols:\n        if \"_A_\" in col:\n            foundAPart = True\n        if \"_B_\" in col:\n            foundBPart = True\n    if foundAPart and foundBPart:\n        raise Exception(\"Question has an A and B part. Specify the needed part.\")\n\n    return neededCols,xLabels","52021600":"'''Plot a bar chart with one or more series\n   All must use the same X labels\n\n'''\ndef sideBySidePlot(x1,ys,title,y2s=[]):\n    colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan']\n    center = 0\n    width = 0.8\/len(ys)\n    ind = np.arange(len(x1))\n    fig = plt.figure()\n    ax1 = fig.add_subplot(111)\n    \n    fig.suptitle(title)\n    \n    for y in range(len(ys)):\n        ax1.bar(ind + (((y+0.5) *width) - (len(ys)\/2) * width), ys[y],width,color=colors[y])\n        ax1.set_ylabel('Total')\n    \n    if len(y2s) > 0:\n        ax2 = ax1.twinx()\n        for y in range(len(y2s)):\n            ax2.scatter(ind + (((y+0.5) *width) - (len(ys)\/2) * width), y2s[y],width,color=colors[y])\n            ax2.set_ylabel('Percent %')\n            miny, maxy = ax2.get_ylim()\n            ax2.set_ylim(0, maxy)\n    \n    \n    ax1.set(xticks=ind + center, xticklabels=x1)#, xlim=[2*width - 1, len(x1)])\n    ax1.set_xticklabels(labels=x1, rotation = 45, ha=\"right\")","8e8c16b3":"'''\n    Create a plot based on a single column with many different categorical values\n    or many columns with a single unique value in each. \n'''\ndef comparePlot(question,df,usePercent):\n    #get the columns from the clean dataset\n    neededCols,xLabels = extractCols(question)\n    groupItem = df[neededCols]\n    #get the assumed header for the graph based on the question\n    title = headers[neededCols[0]][0].split('?')[0] + '?'\n    title = title.split(\":\")[0] + \":\"\n    ys = []\n    y2s = []\n    if(len(neededCols) == 1): # single col\n        \n        graphDf = pd.DataFrame(index=df[neededCols[0]].value_counts().index)\n        \n        graphDf[groupItem[neededCols[0]].iloc[0]] = groupItem[neededCols[0]].value_counts()\n        graphDf =graphDf.fillna(0)\n        yn = graphDf[groupItem[neededCols[0]].iloc[0]].values\n        ys.append(yn)\n        if usePercent:\n           y2s.append((100 * yn \/ len(df)))\n        x1 = graphDf.index.values\n           \n        sideBySidePlot(x1,ys,title,y2s) #graph it\n    else:  #multi col\n        graphSet = []\n        desc = groupItem.loc[:,neededCols].describe()\n        #make sure data is 1 unique per col\n        if (desc.loc['unique'] > 1).sum() > 0:\n            raise Exception(\"A multi column graph was needed but one or more columns have more than one unique value.\" + \n                    \"Columns with multiple values must be graphed individually\")\n        graphSet.append(desc)\n\n        graphDf = pd.DataFrame(index=xLabels)\n        \n        x1 = xLabels\n        for lot in graphSet:\n            yn = lot.loc['count'].values\n            ys.append(yn)\n            if usePercent:\n                y2s.append((100 * yn \/ len(df)))\n                 \n\n        sideBySidePlot(x1,ys,title,y2s) #graph it","ee3e116c":"#only pick rows with a response to Q5\nq5Cols = extractCols('Q5')[0]\ndata.dropna(how='all',subset=q5Cols,inplace=True)\n# no students\ndata = data[data['Q5'] != 'Student']\n#only people with 2+ years working with ML from question 15\nexperienceCols = ['2-3 years','3-4 years','4-5 years','5-10 years','10-20 years','20 or more years']\ndata = data[data['Q15'].isin(experienceCols)]\n\n\n# get the column names for Q17\nq17Info = extractCols('Q17')\nq17Cols = q17Info[0]\nq17Labels = q17Info[1]\n#part 11 was the None response so exclude that\nq17Cols.remove('Q17_Part_11')\nq17Labels.remove('None')\n#anything that contains something other than nan in these columns counts as one response\nq17Data = data.dropna(how='all',subset=q17Cols)\n#print(len(q17Data))\n\n#get the Q18 cols\nq18Info = extractCols('Q18')\nq18Cols = q18Info[0]\nq18Labels = q18Info[1]\nq18Cols.remove('Q18_Part_6')\nq18Labels.remove('None')\nq18Data = data.dropna(how='all',subset=q18Cols)\n#print(len(q18Data))\n\n#get the Q19 cols\nq19Info = extractCols('Q19')\nq19Cols = q19Info[0]\nq19Labels = q19Info[1]\nq19Cols.remove('Q19_Part_5')\nq19Labels.remove('None')\nq19Data = data.dropna(how='all',subset=q19Cols)\n#print(len(q19Data))\n\n#get the Q16 cols\nq16Info = extractCols('Q16')\nq16Cols = q16Info[0]\nq16Labels = q16Info[1]\nq16Data = data.dropna(how='all',subset=q16Cols)\n#print(len(q16Data))\n\n#Q12 data\nq12Info = extractCols('Q12')\nq12Cols = q12Info[0]\nq12Labels = q12Info[1]","3c6c745a":"comparePlot('Q5',q17Data,True)","e1f9c654":"comparePlot('Q17',q17Data,True)","bb0e8083":"'''\nThis function iterates over a subset of columns to count every instance where \nany subject pair of columns are both selected ( i.e. neither is NaN or None). \nIt turns this data into a DataFrame where both the index and columns have the provided labels \nand the count for that pair in the cell. \ni.e If A and B are both selected by a respondent we count that as 1.\nIf A is selected and B is not that is 0\nIf neither A or B are selected that is 0\n'''\ndef combinationMap(df,cols,labels):\n    if(len(cols) != len(labels)):\n        raise Exception(\"Length of cols and labels must be the same.\")\n    #create the heatmap dataframe\n    combDf = pd.DataFrame(columns=labels,index=labels)\n    for col in range(len(cols)): # for every column\n        for myIndex in range(len(cols)): #compare with every other column and itself\n            #the next line is a lot of pandas code that:\n            #selects that pair of columns\n            #sets the NaN\/None vaules to True\n            #sum each row (True =1 False =0)\n            #select the subset of rows that are 0 i.e. Neither were None\n            #count the number of rows that remain\n            #set that count as the value for the corrisponding cell in the new table\n            combDf.at[labels[myIndex],labels[col]] = len(df[df[[cols[col],cols[myIndex]]].isna().sum(axis=1) == 0])\n    # the result is a frame with counts of each time that pair is selected\n    return combDf","8e011fc5":"'''Setup a seaborn heatmap'''\ndef showHeatMap(df,format=None,title=None,dims=(10,10),ax=None,xticks=True):\n    if ax is None:\n        fig, ax = plt.subplots(figsize=dims)\n    if format:\n        heatmap = sns.heatmap(ax=ax, data=df,annot=True, fmt=format,xticklabels=xticks)\n    else:\n        heatmap = sns.heatmap(ax=ax, data=df,xticklabels=xticks)\n    heatmap.set_xticklabels(heatmap.get_xticklabels(),rotation=45,ha=\"right\")\n    if title:\n        heatmap.set_title(title)\n    for i in range(len(df)):\n        ax.add_patch(Rectangle((i, i), 1, 1, fill=False, edgecolor='white', lw=4))\n        ax.add_patch(Rectangle((0, i), len(df.iloc[0]), 1, fill=False, edgecolor='white', lw=1))\n        \n    return heatmap","38e679b2":"#get the Q17 pair-wise counts as both raw values and as percentage\nalsoDf = combinationMap(q17Data,q17Cols,q17Labels)\npercentDf = alsoDf\/len(q17Data)","9d99ec97":"''' Create a heatmap with the raw values and percentages displayed on each cell'''\ndef sideBySideHeatMap(df,totalNum,superTitle):\n    fig, ax = plt.subplots(1,1,figsize=(12,10))\n    fig.suptitle(superTitle)\n    fig.subplots_adjust(hspace=0.01)\n    showHeatMap(df.astype(int),\"d\",ax=ax)\n\n    percentDf = (df\/totalNum)*100\n    reannotatePercent(percentDf,ax)\n\n    return ax\n''' Add additional text to all cells in a heatmap'''\ndef reannotatePercent(df,ax):\n    cellCol = 0\n    cellRow = 0\n    if(len(ax.texts) != (len(df.columns) * len(df))):\n        raise Exception(\"The provided dataframe is not the right size for the axis texts. Wrong input?\")\n    for i in range(len(ax.texts)):\n        cellRow = int(i\/len(df.columns))\n        cellCol = int(i % len(df.columns))\n        ax.texts[i].set_text(ax.texts[i].get_text() + \"\\n(\" + \"%2d\" % round(df.iloc[cellRow][cellCol]) + \"%)\")\n","acf965bb":"#heatmap of common pairs in the Q17 data\nax = sideBySideHeatMap(alsoDf,len(q17Data),superTitle=\"ML Algorithm combinations regularly used by experienced professionals\")\n#help people to read the data with a few annotations\nax.annotate('3235 (79%) total\\nRegression users',\n            xy=(0.5, 0.3), xycoords='data',\n            xytext=(0, 1.07), textcoords='axes fraction',\n            arrowprops=dict(facecolor='blue', shrink=0.05),\n            horizontalalignment='right', verticalalignment='top',fontsize=12, color='tab:blue')\n\nax.annotate('1897 (46%) are using Regressions and Gradient Boosting',\n            xy=(2.5, 0.3), xycoords='data',\n            xytext=(1, 1.07), textcoords='axes fraction',\n            arrowprops=dict(facecolor='blue', shrink=0.05),\n            horizontalalignment='right', verticalalignment='top',fontsize=12, color='tab:blue');\n","19c8739c":"#calculate some stats for the text\nevoTotal = alsoDf.loc['Evolutionary Approaches']['Evolutionary Approaches']\nevoTotalPerc = round(percentDf.loc['Evolutionary Approaches']['Evolutionary Approaches'] * 100)\nevoAndLin = alsoDf.loc['Evolutionary Approaches']['Linear or Logistic Regression']\nevoAndLinPerc = round(percentDf.loc['Evolutionary Approaches']['Linear or Logistic Regression'] * 100)\nevoRelPerc = round(alsoDf.loc['Evolutionary Approaches']['Linear or Logistic Regression']\/alsoDf.loc['Evolutionary Approaches']['Evolutionary Approaches'] * 100)\ngenRelPerc = round(alsoDf.loc['Generative Adversarial Networks']['Linear or Logistic Regression']\/alsoDf.loc['Generative Adversarial Networks']['Generative Adversarial Networks'] * 100)\ntransformerRelPerc = round(alsoDf.loc['Transformer Networks ']['Linear or Logistic Regression']\/alsoDf.loc['Transformer Networks ']['Transformer Networks '] * 100)\n\nprintmd(\"Interestingly, convolutional neural networks seem to be used in concert with a wider range of other algorithms. Convolutional is certainly the most prevalent neural network, however, professionals seems to be working with several algorithms by the time they need to move into neural networks. Relatively few of the professionals surveyed use evolutionary approaches, generative adversarial networks or transformer networks on a regular basis. But of those that do, the vast majority (%2d%%, %2d%% and %2d%% respectively) also use some form of regression as well. Clearly, professionals recognise the benefits of keeping things simple wherever possible.\" %(evoRelPerc,genRelPerc,transformerRelPerc))\nprintmd(\"If you're confused about how those large percentages come from the above heatmap, keep in mind it's all relative. For instance, only %d (%2d%%) of professional survey respondents use evolutionary methods, while %d (%2d%%) of all professional survey respondents use both evolutionary and regression methods. That gives you: %d\/%d*100 = %2d%%. So therefore 79%% of those using evolutionary approaches are also using regression methods.\" %(evoTotal,evoTotalPerc,evoAndLin,evoAndLinPerc,evoAndLin,evoTotal,round(evoAndLin\/evoTotal*100)) )\n","6373aacc":"''' Use pandas to group all rows with the exact same selections across the columns\n    then print the top N combinations for the notebook\n'''\ndef showTopXCombos(df,cols,topN):\n    #group identical selections across the subset of columns\n    combinations = df.astype(str).groupby(by=cols).size().reset_index()\n    #calculate the percentage of rows with each selection\n    combinations[\"Percentage\"] = combinations[0]\/len(df) * 100\n    #sort with most frequent at the top\n    combinations = combinations.sort_values(\"Percentage\", ascending=False)\n    combinations.reset_index()\n    prettyStr = \"\"\n    #print the top N values from the table\n    for i in range(topN):\n        prettyStr = str(i+1) + \": \"\n        first = True\n        for col in cols:\n            if(combinations.iloc[i][col] != 'nan'):\n                if first != True:\n                    prettyStr += \" - \"\n                prettyStr += \"\" + combinations.iloc[i][col].split(\" (\")[0]\n                first = False\n                \n        \n        printmd(prettyStr + \" (\" + str(combinations.iloc[i][0]) + \" identical responses)\")\n#show the most common selections in Q17\nshowTopXCombos(q17Data,q17Cols,5)","b5c1f8de":"#which frameworks do our experts use\nframeworkDf = combinationMap(q17Data,q16Cols,q16Labels)\nax = sideBySideHeatMap(frameworkDf,len(q17Data),superTitle=\"Framework combinations regularly used by experienced professionals\")","3f972008":"#Look at the selections on the Computer Vision question\nvizDf = combinationMap(q18Data,q18Cols,q18Labels)\nsideBySideHeatMap(vizDf,len(q18Data),superTitle=\"Computer Vision Methods regularly used by experienced professionals\");\n","45865a68":"#top selections for Q17 by people who answered Q18\nshowTopXCombos(q18Data,q17Cols,5);","83f6fbba":"#CV framework selections\nframeworkDf = combinationMap(q18Data,q16Cols,q16Labels)\nax = sideBySideHeatMap(frameworkDf,len(q18Data),superTitle=\"Framework combinations regularly used by Computer Vision professionals\")","2c62a977":"#NLP Alg selections\nnlpDf = combinationMap(q19Data,q19Cols,q19Labels)\nsideBySideHeatMap(nlpDf,len(q19Data),superTitle=\"NLP Methods regularly used by experienced professionals\");\n","c05a399b":"#top seletions for Q17 by people who answered Q19\nshowTopXCombos(q19Data,q17Cols,5)","6d011213":"#NLP frameworks\nframeworkDf = combinationMap(q19Data,q16Cols,q16Labels)\nax = sideBySideHeatMap(frameworkDf,len(q19Data),superTitle=\"Framework combinations regularly used by NLP professionals\")","1b9f22df":"'''\n Make a smaller heatmap for visual comparison\n'''\ndef showMiniHeatMap(df,format=None,title=None,dims=(10,10),ax=None,cbar=False,xticks=True,yticks=True,vmin=0, vmax=0.92):\n    if ax is None:\n        fig, ax = plt.subplots(figsize=dims)\n    if format:\n        heatmap = sns.heatmap(ax=ax, data=df,annot=False, fmt=format,cbar=cbar,xticklabels=xticks,yticklabels=yticks,vmin=vmin, vmax=vmax)\n    else:\n        heatmap = sns.heatmap(ax=ax, data=df,cbar=cbar,xticklabels=xticks,yticklabels=yticks,vmin=vmin, vmax=vmax)\n    heatmap.set_xticklabels(heatmap.get_xticklabels(),rotation=45,ha=\"right\")\n    if title:\n        heatmap.set_title(title)\n    for i in range(len(df)):\n        ax.add_patch(Rectangle((i, i), 1, 1, fill=False, edgecolor='white', lw=0.4))\n        ax.add_patch(Rectangle((0, i), len(df.iloc[0]), 1, fill=False, edgecolor='white', lw=0.1))\n\n'''\n Supply a job title and get a Q17 heatmap for that job title\n'''        \ndef jobTitleHeatMap(jobTitle,nAx,xticks,yticks):\n    jobData = q17Data[q17Data['Q5']== jobTitle]\n    jobDf = combinationMap(jobData,q17Cols,q17Labels)\/len(jobData)\n    showMiniHeatMap(jobDf.astype(float),format=\"0.2f\",title=jobTitle,dims=(5,5),ax=nAx,cbar=False,xticks=xticks,yticks=yticks)\n\n# create a set of small heatmaps for visual comparison\norderedJobs = ['Statistician','Data Scientist','Data Analyst', 'Machine Learning Engineer','Business Analyst','Software Engineer','Product\/Project Manager','Research Scientist','DBA\/Database Engineer','Data Engineer','Other','Currently not employed']\nnCols = 2\nnRows = int(len(orderedJobs)\/nCols)\nif (len(orderedJobs) % nCols) != 0:\n     nRows += 1\nfig, ax = plt.subplots(nRows,nCols,figsize=(15,15))\nfig.subplots_adjust(wspace=0.01)\nrowTracker = 0\ncolTracker = 0\nfor indx in range(len(orderedJobs)):\n    rowTracker = int(indx\/nCols)\n    colTracker = int(indx % nCols)\n    jobTitleHeatMap(orderedJobs[indx],ax[rowTracker][colTracker],xticks=(rowTracker==nRows-1),yticks=(colTracker==0))\n\n\n\n\n        ","184b1741":"''' \n  Create a detailed heatmap for a supplied job role\n'''\ndef roleGraph(role):\n    roleData = q17Data[q17Data['Q5']==role]\n    headingStr = \"**Top machine-learning-algorithm toolkits for \" + role\n    if role.endswith(\"ed\"):\n        headingStr += \"**\"\n    else:\n        headingStr += \"s**\"\n    printmd(headingStr)\n    roleMap = combinationMap(roleData,q17Cols,q17Labels)\n    ax = sideBySideHeatMap(roleMap,len(roleData),superTitle=role)\n    showTopXCombos(roleData,q17Cols,2)\nrole = 'Statistician'\nroleGraph(role)","4f44097a":"role = 'Data Scientist'\nroleGraph(role)","82a3ac32":"role = 'Data Analyst'\nroleGraph(role)","dbc62200":"role = 'Machine Learning Engineer'\nroleGraph(role)","56c91ab6":"role = 'Business Analyst'\nroleGraph(role)","cb6505dd":"role = 'Software Engineer'\nroleGraph(role)","a5e433a0":"role = 'Product\/Project Manager'\nroleGraph(role)","2a9d7fa1":"role = 'Research Scientist'\nroleGraph(role)","5f6befcd":"role = 'DBA\/Database Engineer'\nroleGraph(role)","6b5c5622":"role = 'Data Engineer'\nroleGraph(role)","798982dd":"role = 'Currently not employed'\nroleGraph(role)","1ef2f83e":"#how do those that answered Q17 respond to Q12\nframeworkDf = combinationMap(q17Data,q12Cols,q12Labels)\/len(q17Data)\nshowHeatMap(frameworkDf.astype(float),\".2f\",dims=(5,5),title=\"All\");\n#how do those that answered Q18 respond to Q12\nframeworkDf = combinationMap(q18Data,q12Cols,q12Labels)\/len(q18Data)\nshowHeatMap(frameworkDf.astype(float),\".2f\",dims=(5,5),title=\"Computer Vision\");\n#how do those that answered Q19 respond to Q12\nframeworkDf = combinationMap(q19Data,q12Cols,q12Labels)\/len(q19Data)\nshowHeatMap(frameworkDf.astype(float),\".2f\",dims=(5,5),title=\"NLP\");\n","433b028e":"![austin-distel-4r72LPFh4Ik-unsplash.jpg](attachment:austin-distel-4r72LPFh4Ik-unsplash.jpg)\n<span>Photo by <a href=\"https:\/\/unsplash.com\/@austindistel?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Austin Distel<\/a> on <a href=\"https:\/\/unsplash.com\/s\/photos\/expert?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Unsplash<\/a><\/span>","243f683f":"The machine learning algorithm toolkits used by professionals engaged in natural language processing activities were once again decidedly different from machine learning and data science professionals as a whole \u2014 and from computer vision professionals as well. As you can see from the below list of the top five combinations of machine learning algorithms, professionals performing natural language processing were far more likely to use neural networks on a regular basis. \nIn fact, these professionals appear to use the broadest range of algorithms so far, including some of the rarer ones. So, if you're interested in a career that involves natural language processing, you may have a lot of study ahead of you! ","9b0d7c1d":"###1. The top toolkit used by statisticians\nAccording to the results of Kaggle's 2020 Machine Learning and Data Science survey, the most commonly used toolkit for professional statisticians comprises linear or logistic regression, and decision trees or random forests, with or without Bayesian approaches. However, there weren't a huge number of respondents in this data subset, so you'll have to take this analysis with a grain of salt. Having said that, if your ideal employment is as a statistician, it's probably safe to say that you'll have a better chance of landing your ideal job if you have a good grasp of basic regression- and tree-based algorithms.","3ab9f3db":"###4. The top toolkit used by machine learning engineers\nThe top toolkits used by machine learning engineers are perhaps the most interesting thus far as the majority of professionals in this category either just use convolutional neural networks, or they use a combination of regression-based, tree-based, and gradient boosting machine algorithms along with Bayesian approaches.  \nIf you're keen to land a job as a machine learning engineer, you might have to do some research into your preferred employers to learn which toolkit is going to stand you in good stead.","44fd815f":"###Common pairs of machine learning algorithms\nAs you might guess, well over half of the professionals surveyed (63%) use linear or logistic regression along with decision trees or random forests. Many of them also include gradient boosting machines and\/or convolutional neural networks in their toolkits. In fact, the most common set of tools across the professionals surveyed were linear or logistic regression, decision trees or random forests and gradient boosting machines. This may reflect a problem solving framework that many professionals follow.  \nFor example, it\u2019s probably very common for professionals to start out with a basic regression and then move onto a tree-based method if the regression didn\u2019t work or they need further information. Gradient boosting machines then become useful when \u2018weak learners\u2019 are identified with the tree-based method.","eb713cf4":"Interestingly, there is a big difference in the types of machine learning algorithms used by the professionals who answer the question about visual data processing tools as compared to the broader pool of professional respondents. Below you'll find the top five combinations of machine learning algorithms used by professionals engaged in computer vision work. If you're interested in pursuing a career that includes computer vision techniques, it seems you might be well served by learning to use convolutional neural networks as well as the other algorithms previously highlighted. If you have some spare time up your sleeve, you might improve your resume if you can add dense neural networks and recurrent neural networks.","418e925d":"###9. The top toolkit used by DBA\/database engineers\nDBA\/database engineers reported using a limited toolkit comprising linear or logistic regression, and decision trees or random forests, with or without gradient boosting machines. However, very few DBA\/database engineers responded to the 2020 survey, so we strongly recommend you further investigate the best toolkit for this role before deciding to limit yourself to the most basic machine learning algorithms.","30d540bc":"TensorFlow, Keras and PyTorch are also frequently used together, in isolation and with scikit-learn. Interestingly, the newer PyTorch is the least popular of this set, perhaps because professionals haven\u2019t needed to adopt a competitor to TensorFlow \u2014 or because PyTorch\u2019s marketing has been less persuasive.  \nGiven the high prevalence of surveyed professionals who use gradient boosting algorithms, the relatively high incidence of Xgboost users is also logical. However, it is interesting to note that Xgboost dominates over LightGBM and CatBoost. If you're planning on adding one or more gradient boosting algorithms to your toolset, it appears you may be better off learning to use Xgboost as opposed to one of the alternative frameworks.","b3025d5a":"###3. The top toolkit used by data analysts\nThe majority of data analysts surveyed use linear or logistic regression, decision trees or random forests, and gradient boosting machines. However, many of them don't bother with gradient boosting machines. So, noting the relatively low number of respondents in this data set, if you want to work in a specialist data analyst role, it's safe to say you'll be well served by learning to use regression- and tree-based machine learning algorithms, and ideally gradient boosting machines as well.","cb7d95df":"##Commonly used processing units\nThe majority of professionals reported using GPUs for their machine learning work. Many said they don\u2019t use GPUs or TPUs and are presumably using standard CPUs.  \nVery few respondents indicated that they use TPUs. Given neural networks were not the most popular machine learning algorithms, it\u2019s unsurprising that processing units designed specifically to be used for neural network machine learning processes were not the most popular processing units among professionals. However, a far greater proportion of respondents use one or more neural network-based machine learning algorithm, so is a little surprising that TPUs were so very unpopular.  \nWhen it came to computer visual and natural language professionals, very few were willing to rely solely on CPUs and were instead largely reliant on GPUs. A slightly larger proportion of these professionals use TPUs on a regularly basis, but the numbers are still not very high.","23dd89be":"###6. The top toolkit used by software engineers\nSimilar to machine learning engineers, the majority of software engineers either use a combination of regression-, tree- and gradient boosting machine-based machine learning algorithms, or they just use convolutional neural networks. So again, if you're keen to land a job as a software engineer, you'll probably have to do some research into your preferred employers to learn which toolkit is going to allow you to deliver the most attractive resume.","72a38f2c":"# Commonly used visual data processing methods\nImage classification and other general purpose networks were the visual data processing tools of choice for the professionals who responded to the 2020 Machine Learning and Data Science Survey. While 78% of professionals in total use such methods, roughly 40% use those tools in concert with general purpose image\/video tools, image segmentation methods or other detection methods.","16591ce4":"The numbers of professionals using specific machine learning algorithms clearly demonstrates that the vast majority of professionals use two or more algorithms on a daily basis. For example, 79% of professionals are using regression-based algorithms and 71% are using tree- or forest-based algorithms, so a substantial proportion of the respondents must be regularly using both types of algorithms.  \nBut which algorithms are most commonly used together?   \nOnce you know that, you can start to build a picture of the kinds of toolkits that could help you win your dream job. And that's where the next piece of our analysis comes in.   \nWe calculated the number of respondents who reported using each pair of machine learning algorithms, regardless of whether they also use additional algorithms, and presented them in a heatmap. The diagonal that runs from the top left to the bottom right of each diagram (outlined in white) represents the total number of professionals who reported using each algorithm. This means you can compare the number of professionals using a given algorithm with the numbers of professionals using that algorithm as well as any other algorithm included in the survey. (As you go through the next part of this article, have some fun studying each row of the heatmaps separately to see which combinations are popular and which are rare. These kinds of comparisons form the basis of the rest of this article.)   ","f47631e8":"##Commonly used frameworks\nGiven the heavy reliance on regression and tree-based algorithms, it comes as no surprise that scikit-learn is the most commonly used framework among professionals, with 77% of those surveyed using it on a regular basis.","303f0a04":"###8. The top toolkit used by machine learning research scientists\nMachine learning research scientists reported using a toolkit comprising either regression- and tree-based algorithms or convolutional neural networks. However, dense and recurrent neural networks were also commonly used by those using convolutional neural networks, and gradient boosting was commonly used by those with a more basic toolkit.  \nThis is another area where it would pay to research your ideal employers before deciding on which toolkit to develop.\n","ceea5e11":"###Currently not employed\nUnfortunately, we were unable to elucidate any clear differential between those employed and not employed. This may be because many businesses had to let go of people with appropriate skill sets in 2020, but there's no way to confirm or refute that hypothesis from the data available in the survey.","b00866ff":"Notice that number five in the list is just regression-based algorithms. Later in the article you'll find out why that's the case.  \nNow, having discovered the most commonly used machine learning algorithms, it's also useful to know which frameworks professionals are employing when using said algorithms, as limiting the number of frameworks you have to learn is another way to save time and effort.  \nTo investigate this, we constructed another heatmap.","aeaebe21":"#Top 10 machine learning toolkits to make you an attractive hire\nWhen you're attempting to transition into a machine learning or data science field, the array of tools on offer can be overwhelming. What platforms should you familiarise yourself with? Which algorithms should you learn to use? Do you need a broad range of experience to get a job? Or should you attempt to specialise?  \nSo many questions. So little concrete information.  \nUntil now.  \nThis study provides data-driven recommedations as to the toolkits that will be most useful when you're applying for jobs or even a promotion. Read on to learn how you can start building a robust skill set that'll make you attractive to recruitment teams, without you having to learn to use every platform and algorithm under the sun.","d7897403":"##The most commonly used machine learning algorithms\nAlmost every machine learning student starts by learning to use linear or logistic regression algorithms. And if you\u2019re like many trying to break into machine learning as a profession, you might wonder whether it\u2019s really useful to learn to use such algorithms in the long term, assuming, perhaps, that more experienced professionals quickly move on to more advanced or specialised algorithms.  \nIf so, you\u2019ll be surprised to learn that Kaggle\u2019s 2020 Machine Learning and Data Science Survey indicates that professionals continue to use linear or logistic regression on a regular basis. In fact, 79% of the professionals surveyed use regression-based algorithms regularly.","e2c54a87":"So, don\u2019t slack off when studying that material. You\u2019ll be using it for years to come.\nOther commonly used algorithms into which you might choose to invest time and effort include decision trees or random forests (71% of professionals regularly use those), gradient boosting machines, Bayesian approaches, dense neural networks, convolutional neural networks and recurrent neural networks. But don\u2019t get started learning to use these just yet as this data doesn\u2019t paint the full picture.  \nIf you read on, you'll find more specific (and timesaving) recommendations to boost your employment potential.","48582c1a":"Once again, the key frameworks most commonly used by this subset of respondents remained fairly consistent with the sets of frameworks used by computer vision professionals and the professional survey respondents as a whole.","27bcf1ea":"###10. The top toolkit used by data engineers\nData engineers reported using toolkits that are heavily reliant on linear or logistic regression, and decision trees or random forests, along with gradient boosting machines, with or without Bayesian approaches. However, more than half of the survey respondents also reported using convolutional neural networks. It seems there is a greater variation in the commonly used tools among data engineers, so this is another profession for which it would pay to do extra research into the preferences of your ideal employers.\n","90ac71ee":"###5. The top toolkit used by business analysts\nBusiness analysts reported using a limited toolkit comprising linear or logistic regression, decision trees or random forests, and gradient boosting machines. Though many respondents have left off gradient boosting machines. Roughly a third of respondents also added convolutional neural networks to their toolkit, but we would need more data to confidently recommend that be included in your toolkit.","4358997d":"##Building your machine learning toolkit\nBased on the results from Kaggle\u2019s 2020 Machine Learning and Data Science Survey, there are 10 distinct machine learning toolkits used by machine learning and data scientist professionals across the world. And as an aspiring professional, you could save yourself a lot of time and effort, and increase your chances of landing your dream job, if you focus your studies on the toolkits most commonly employed by professionals working in the most relevant niche.  \nNo matter your aspirations, you\u2019ll benefit from learning to use the basic regression-based machine learning algorithms along with decision trees or random forests. And you\u2019ll get the best results if any algorithms you learn in addition to those, are those most commonly used by professionals in your chosen field.  \nRegardless of your preferred role, you\u2019ll probably be best served by avoiding evolutionary approaches, generative adversarial networks and transformer networks, and learning to use them on-the-job if they\u2019re necessary for the role you secure. Otherwise you could expend a lot of effort for no gain other than appeasing any intellectual interest you might have in those algorithms.  \nAlong with key algorithms, you may want to spend some time learning to use a GPU, especially if you\u2019d like to secure a role that relies on computer vision or natural language processing. In most cases, learning to effectively use a TPU is probably best left to on-the-job training as very few professionals use them on a regular basis.  \nIf your dream role requires computer vision methods, you\u2019ll get the best results if your first port of call is image classification and other general purpose networks. You could then add general purpose image\/video tools, image segmentation methods, and object detection methods. Generative networks are unnecessary unless you know your ideal employer requires knowledge of them.  \nRegardless of your aspirations, you\u2019ll be well-served if you learn to use the scikit-learn framework and perhaps TensorFlow and Keras. Xgboost and PyTorch could also be useful additions to your toolkit. The other frameworks are used only sporadically, so unless you have a specific interest in using another framework, learning to use them before securing a role would likely be unnecessary at best (or a waste of time at worst).  \nArmed with this knowledge, you no longer have to wonder where you should focus your studies. You now have a data-driven basis for choosing a toolkit that will stand you in good stead when you\u2019re applying for jobs. The hardest part my be choosing your ideal role!","9e5dec6b":"Normally, such a skew might make analysing the data more difficult. However, since the skills used by data scientists are fundamental to most machine learning roles, we concluded that the large number of responses from data scientists shouldn't negatively impact on the conclusions we might draw about the toolkits used by professionals across the board.\nGiven there were also plenty of responses from professionals working in a variety of distinct fields (with the exception of DBA\/database engineer roles), we believed there would be sufficient data with which to attempt to draw conclusions about the toolkits of professionals working in specific fields as well.  \nHaving drawn that conclusion, we then analysed the machine learning algorithms used by our subset of survey respondents.","7366e260":"So far in this article, you've gained some insight into the primary tools used by professionals across the board for general machine learning and data science tasks. But you may be wondering whether specific problem sets affect the algorithms used by the average professional.  \nTo answer this question, we reviewed the responses of those professionals that use computer vision (Q18) and natural language processing (Q19) tools and produced another set of heatmaps.   \nIt's important to note that we have assumed those subsets of survey respondents are computer vision and natural language processing experts or specialists. That's a bit of a long bow to draw, however, there is a noticeable and logical difference between the heatmaps produced for each set of professionals. So, in the absence of a survey question that enables us to draw out the responses of true computer vision and natural language processing specialists, we believe the following analysis at least provides a basis upon which you can research the ideal toolkits for each specialty. If you're interested in specialising in computer vision or natural language processing, we hope you'll find the next couple of sections interesting and enlightening.","e0ed7298":"###2. The top toolkit used by data scientists\nThe survey results revealed a similar toolkit is commonly used by data scientists. The difference here is that most data scientists use regression-based, tree-based and gradient boosting machine-based machine learning algorithms, and many also use Bayesian approaches on a regular basis. If you\u2019d love to work as a data scientist, you\u2019d be well served by learning to use at least three, but preferably all four, of those algorithms.   \nYour resume will look even better if you can add in convolutional neural networks, which are regularly used by nearly half of the data scientists surveyed.","75d69eac":"##Commonly used natural language processing methods\nWord embeddings\/vectors are the most popular methods of conducting natural language processing with 80% of the professionals surveyed reporting that they use them on a regular basis. Nearly half of the professionals surveyed used word embeddings\/vectors with encoder-decoder models or transformer language models.","9b3187e2":"Despite the difference in algorithm toolsets, the combinations of frameworks used by computer vision professionals was very similar to those used by the whole pool of professionals that responded to the survey.","95e003cb":"To take this analysis a step further, here are the top five machine learning algorithm toolsets used by professionals across the board.","df927e30":"##The roles of professionals in the field\nBecause you're interested in learning about the toolkits professionals use, for this article, we had to choose a subset of the data that reflects the practices of professionals. So, we eliminated the responses of those respondents who identified as students (Q5 in the survey). We also decided the most useful advice would be born out of responses from professionals with at least two years of experience using machine learning methods (Q15). After all, those who're still learning the ropes of their role may not be using the best tools for their work.  \nWe also removed the data associated with respondents who said they don't use any machine learning algorithms on a regular basis since that information doesn't help you develop a robust machine learning toolkit (Q17).  \nHaving done that, the first information we looked at was job roles.  \nThe vast majority of survey respondents were data scientists. As a result, the raw data is skewed towards algorithms and tools useful for data scientists. ","c8bf1825":"###7. The top toolkit used by product\/project managers\nMost of the product\/project managers surveyed reported using a toolkit heavily reliant on linear and logistic regression, and also comprising decision trees or random forests, and gradient boosting machines, though many don't find they need gradient boosting machines on a regular basis. Almost half of the respondents also reported regularly using convolutional neural networks.","096f19c4":"##Toolkits used by different professions\nThe analysis thus far has focussed on the tools used by professionals as a whole. And while you could go away and spend hundreds of thousands of hours learning the intricacies of all the most popular algorithms, frameworks, computer vision methods and natural language processing methods, you could save a huge amount of time and effort if you focus your studies on the toolkit that\u2019s most commonly employed in a specific professional role. Not only that, but recruitment officers may see your experience and interests as more relevant if they\u2019re narrowly focussed on the tools that are most useful in the job you\u2019re applying for.  \nSo, to help you with that, we analysed the tools used by survey respondents that identified as working in each specific role. And the first thing we want to draw your attention to is the stark difference between the various professions. In particular, the professions on the left of the below figure use much more narrowly focussed toolsets than those professions on the right.   \nIf you're not yet sure which specialisation you'd like to aim for, you might like to choose one of those listed on the left as you would be able to develop a more ideal skill set with a relatively reduced amount of study.\nOnce you've perused that high level overview of the various professions, keep reading as we provide further details and draw out a set of the top 10 machine learning toolkits."}}