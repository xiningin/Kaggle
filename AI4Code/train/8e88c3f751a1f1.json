{"cell_type":{"70f30b02":"code","0ca34e32":"code","1eeb5795":"code","6baf8868":"code","edc0f2f9":"code","85400b3f":"code","a7460f9c":"code","2ba8168d":"code","79a6d454":"code","135f6580":"code","55fdc865":"code","6243bedc":"code","fdf517eb":"code","2f0a67d9":"code","20293459":"code","b8dfa38e":"code","263a539a":"code","a2256249":"code","b2cd2514":"code","c3a429b5":"code","027e208b":"code","a9e2ce07":"markdown","9b91d4b3":"markdown","4ca2841f":"markdown","746ffd24":"markdown","db56fee0":"markdown","c5b46933":"markdown"},"source":{"70f30b02":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0ca34e32":"df = pd.read_csv(\"\/kaggle\/input\/voicegender\/voice.csv\")","1eeb5795":"df.head(10)","6baf8868":"df.columns","edc0f2f9":"from IPython.core.display import HTML # permet d'afficher du code html dans jupyter\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\ndisplay(HTML(df.head(10).to_html()))","85400b3f":"df.shape","a7460f9c":"df.describe()","2ba8168d":"#sns.pairplot(df, hue = \"label\")","79a6d454":"data_train = df.sample(frac=0.8, random_state=1)          # 80% des donn\u00e9es avec frac=0.8\ndata_test = df.drop(data_train.index)     # le reste des donn\u00e9es pour le test","135f6580":"X_train = data_train.drop(['label'], axis=1)\ny_train = data_train['label']\nX_test = data_test.drop(['label'], axis=1)\ny_test = data_test['label']","55fdc865":"plt.figure(figsize=(9,9))\n\nlogistique = lambda x: np.exp(x)\/(1+np.exp(x))   \n\nx_range = np.linspace(-10,10,50)       \ny_values = logistique(x_range)\n\nplt.plot(x_range, y_values, color=\"red\")","6243bedc":"from sklearn.linear_model import LogisticRegression","fdf517eb":"lr = LogisticRegression(solver='liblinear')\nlr.fit(X_train,y_train)","2f0a67d9":"y_lr = lr.predict(X_test)","20293459":"from sklearn.metrics import accuracy_score, confusion_matrix","b8dfa38e":"lr_score = accuracy_score(y_test, y_lr)\nprint(lr_score)","263a539a":"from sklearn import tree\ndtc = tree.DecisionTreeClassifier()\ndtc.fit(X_train,y_train)\ny_dtc = dtc.predict(X_test)\nprint(accuracy_score(y_test, y_dtc))","a2256249":"plt.figure(figsize=(30,30))\ntree.plot_tree(dtc, feature_names=X_train.columns, class_names=['benin','malin'], fontsize=14, filled=True)  ","b2cd2514":"dtc1 = tree.DecisionTreeClassifier(max_depth = 3, min_samples_leaf = 20)\ndtc1.fit(X_train,y_train)","c3a429b5":"plt.figure(figsize=(30,30))\ntree.plot_tree(dtc1, feature_names=X_train.columns, class_names=['benin','malin'], fontsize=14, filled=True) ","027e208b":"y_dtc1 = dtc1.predict(X_test)\nprint(accuracy_score(y_test, y_dtc1))","a9e2ce07":"# Score et matrice de confusion","9b91d4b3":"# Donn\u00e9es","4ca2841f":"# Visualization","746ffd24":"# Arbres de d\u00e9cision","db56fee0":"# Machine learning","c5b46933":"# R\u00e9gression logistique"}}