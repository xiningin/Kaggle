{"cell_type":{"7df899a8":"code","5f8860b6":"code","c5381031":"code","2bc6b428":"code","d971a9e0":"code","76e543dc":"code","f06bddf2":"code","7b8a8012":"code","739e2a90":"code","55792c4e":"code","3084773c":"code","d1bece39":"code","600812e1":"code","4f7eeb3c":"code","e9ddfa68":"code","6859e85d":"code","124966fd":"code","8049371e":"code","29517aeb":"code","f85e416c":"code","d935ccab":"code","be9787a0":"code","5934753e":"code","1c06b410":"code","88e1fcbc":"code","b01f0d2b":"code","3e27eac2":"code","0dd21d54":"code","8565b897":"code","cf95be6a":"code","bf5796ed":"markdown","7dace4a1":"markdown","c0db966a":"markdown","8735bb7c":"markdown","3bba319c":"markdown","8d1214eb":"markdown","39960756":"markdown","e84700ee":"markdown","232d3f7c":"markdown","ec7eb67e":"markdown","757c141b":"markdown","6e757f92":"markdown","910dd7b1":"markdown","b428e64f":"markdown","298b9def":"markdown","b1517f7b":"markdown","37ddc107":"markdown","f1b43492":"markdown","f3404f3d":"markdown","5491a917":"markdown","4b5a59c8":"markdown","995f2859":"markdown","7d329f95":"markdown","fff9b738":"markdown","c30597bf":"markdown","fbabc5e7":"markdown","ee7e0fd7":"markdown","d560f391":"markdown","7f8561a0":"markdown","740592b4":"markdown","924566fd":"markdown"},"source":{"7df899a8":"import numpy as np\nimport imageio\nimport random\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n\nimport tensorflow as tf \nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import models\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.layers import Activation, Input, Conv2D, MaxPooling2D, BatchNormalization, Conv2DTranspose, concatenate\nfrom tensorflow.keras.models import Model, load_model\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')","5f8860b6":"image_path = [\"..\/input\/lyft-udacity-challenge\/\"+\"data\"+i+\"\/\"+\"data\"+i+\"\/CameraRGB\/\" for i in ['A', 'B', 'C', 'D', 'E']]\nmask_path = [\"..\/input\/lyft-udacity-challenge\/\"+\"data\"+i+\"\/\"+\"data\"+i+\"\/CameraSeg\/\" for i in ['A', 'B', 'C', 'D', 'E']]","c5381031":"def list_image_paths(directory_paths):\n    image_paths = []\n    for directory in range(len(directory_paths)):\n        image_filenames = os.listdir(directory_paths[directory])\n        for image_filename in image_filenames:\n            image_paths.append(directory_paths[directory] + image_filename)\n    return image_paths","2bc6b428":"image_paths = list_image_paths(image_path) \nmask_paths = list_image_paths(mask_path)\nnumber_of_images, number_of_masks = len(image_paths), len(mask_paths)\nprint(f\"1. There are {number_of_images} images and {number_of_masks} masks in our dataset\")\nprint(f\"2. An example of an image path is: \\n {image_paths[0]}\")\nprint(f\"3. An example of a mask path is: \\n {mask_paths[0]}\")\n","d971a9e0":"import random\nnumber_of_samples = len(image_paths)\n\nfor i in range(3):\n    N = random.randint(0, number_of_samples - 1)\n\n    img = imageio.imread(image_paths[N])\n    mask = imageio.imread(mask_paths[N])\n    mask = np.array([max(mask[i, j]) for i in range(mask.shape[0]) for j in range(mask.shape[1])]).reshape(img.shape[0], img.shape[1])\n\n    fig, arr = plt.subplots(1, 3, figsize=(20, 8))\n    arr[0].imshow(img)\n    arr[0].set_title('Image')\n    arr[0].axis(\"off\")\n    arr[1].imshow(mask)\n    arr[1].set_title('Segmentation')\n    arr[1].axis(\"off\")    \n    arr[2].imshow(mask, cmap='Paired')\n    arr[2].set_title('Segmentation')\n    arr[2].axis(\"off\")","76e543dc":"# First split the image paths into training and validation sets\ntrain_image_paths, val_image_paths, train_mask_paths, val_mask_paths = train_test_split(image_paths, mask_paths, train_size=0.8, random_state=0)\n\n# Keep part of the validation set as test set\nvalidation_image_paths, test_image_paths, validation_mask_paths, test_mask_paths = train_test_split(val_image_paths, val_mask_paths, train_size = 0.80, random_state=0)\n\nprint(f'There are {len(train_image_paths)} images in the Training Set')\nprint(f'There are {len(validation_image_paths)} images in the Validation Set')\nprint(f'There are {len(test_image_paths)} images in the Test Set')","f06bddf2":"def read_image(image_path, mask_path):\n    \n    image = tf.io.read_file(image_path)\n    image = tf.image.decode_png(image, channels=3)\n    image = tf.image.convert_image_dtype(image, tf.float32)\n    image = tf.image.resize(image, (256, 256), method='nearest')\n\n    mask = tf.io.read_file(mask_path)\n    mask = tf.image.decode_png(mask, channels=3)\n    mask = tf.math.reduce_max(mask, axis=-1, keepdims=True)\n    mask = tf.image.resize(mask, (256, 256), method='nearest')\n    \n    return image, mask","7b8a8012":"def data_generator(image_paths, mask_paths, buffer_size, batch_size):\n    \n    image_list = tf.constant(image_paths) \n    mask_list = tf.constant(mask_paths)\n    dataset = tf.data.Dataset.from_tensor_slices((image_list, mask_list))\n    dataset = dataset.map(read_image, num_parallel_calls=tf.data.AUTOTUNE)\n    dataset = dataset.cache().shuffle(buffer_size).batch(batch_size)\n    \n    return dataset","739e2a90":"batch_size = 32\nbuffer_size = 500\n\ntrain_dataset = data_generator(train_image_paths, train_mask_paths, buffer_size, batch_size)\nvalidation_dataset = data_generator(validation_image_paths, validation_mask_paths, buffer_size, batch_size)\ntest_dataset = data_generator(test_image_paths, test_mask_paths, buffer_size, batch_size)","55792c4e":"# Take a batch (32 images and their labelled segmentations from each category of data)\nfor train_images, train_masks in train_dataset:\n    break\nfor validation_images, validation_masks in validation_dataset:\n    break\nfor test_images, test_masks in test_dataset:\n    break\n    \n\nfor i in range(3):\n    N = random.randint(0, batch_size-1)\n    \n    images = [train_images[N], validation_images[N], test_images[N]]\n    masks = [train_masks[N], validation_masks[N], test_masks[N]]\n    title = ['Train Image', 'Validation Image', 'Test Image', 'Train Mask', 'Validation Mask', 'Test Mask']\n\n    fig, arr = plt.subplots(1, 3, figsize=(20, 8))\n    arr[0].imshow(images[i])\n    arr[0].set_title(title[i])\n    arr[0].axis(\"off\")\n    arr[1].imshow(masks[i])\n    arr[1].set_title(title[i+3])\n    arr[1].axis(\"off\")\n    arr[2].imshow(masks[i], cmap='Paired')\n    arr[2].set_title(title[i+3])\n    arr[2].axis(\"off\")","3084773c":"def convolution_block(block_input, num_filters=256, kernel_size=3, dilation_rate=1, padding=\"same\", use_bias=False):\n    x = layers.Conv2D(\n        num_filters,\n        kernel_size=kernel_size,\n        dilation_rate=dilation_rate,\n        padding=\"same\",\n        use_bias=use_bias,\n        kernel_initializer=keras.initializers.HeNormal(),\n        )(block_input)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    \n    return x","d1bece39":"def DilatedSpatialPyramidPooling(dspp_input):\n    dims = dspp_input.shape\n    x = layers.AveragePooling2D(pool_size=(dims[-3], dims[-2]))(dspp_input)\n    x = convolution_block(x, kernel_size=1, use_bias=True)\n    out_pool = layers.UpSampling2D(\n        size=(dims[-3] \/\/ x.shape[1], dims[-2] \/\/ x.shape[2]), interpolation=\"bilinear\",\n    )(x)\n\n    out_1 = convolution_block(dspp_input, kernel_size=1, dilation_rate=1)\n    out_6 = convolution_block(dspp_input, kernel_size=3, dilation_rate=6)\n    out_12 = convolution_block(dspp_input, kernel_size=3, dilation_rate=12)\n    out_18 = convolution_block(dspp_input, kernel_size=3, dilation_rate=18)\n\n    x = layers.Concatenate(axis=-1)([out_pool, out_1, out_6, out_12, out_18])\n    output = convolution_block(x, kernel_size=1)\n    return output","600812e1":"def DeeplabV3(image_size, num_classes):\n    model_input = keras.Input(shape=(image_size, image_size, 3))\n    resnet50 = keras.applications.ResNet50(\n        weights=\"imagenet\", include_top=False, input_tensor=model_input\n    )\n    x = resnet50.get_layer(\"conv4_block6_2_relu\").output\n    x = DilatedSpatialPyramidPooling(x)\n\n    input_a = layers.UpSampling2D(\n        size=(image_size \/\/ 4 \/\/ x.shape[1], image_size \/\/ 4 \/\/ x.shape[2]),\n        interpolation=\"bilinear\",\n    )(x)\n    input_b = resnet50.get_layer(\"conv2_block3_2_relu\").output\n    input_b = convolution_block(input_b, num_filters=48, kernel_size=1)\n\n    x = layers.Concatenate(axis=-1)([input_a, input_b])\n    x = convolution_block(x)\n    x = convolution_block(x)\n    x = layers.UpSampling2D(\n        size=(image_size \/\/ x.shape[1], image_size \/\/ x.shape[2]),\n        interpolation=\"bilinear\",\n    )(x)\n    model_output = layers.Conv2D(num_classes, kernel_size=(1, 1), padding=\"same\")(x)\n    model = tf.keras.Model(inputs=model_input, outputs=model_output)\n    \n    return model","4f7eeb3c":"img_height = 256\nimg_width = 256\nnum_channels = 3\nfilters = 32\nn_classes = 23\n\nmodel = DeeplabV3(img_height, num_classes=23)\nmodel.summary()","e9ddfa68":"model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\ncallback = EarlyStopping(monitor='val_accuracy', patience=20, restore_best_weights=True)\nreduce_lr = ReduceLROnPlateau(monitor='val_accuracy',factor=1e-1, patience=5, verbose=1, min_lr = 2e-6)\nbatch_size = 32\nepochs = 30","6859e85d":"history = model.fit(train_dataset, \n                    validation_data = validation_dataset, \n                    epochs = epochs, \n                    verbose=1, \n                    callbacks = [callback, reduce_lr], \n                    batch_size = batch_size, \n                    shuffle = True)","124966fd":"acc = [0.] + history.history['accuracy']\nval_acc = [0.] + history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nplt.figure(figsize=(8, 8))\nplt.subplot(2, 1, 1)\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.ylabel('Accuracy')\nplt.ylim([min(plt.ylim()),1])\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.ylabel('Cross Entropy')\nplt.ylim([0,1.0])\nplt.title('Training and Validation Loss')\nplt.xlabel('epoch')\nplt.show()","8049371e":"model.save('carla-image-segmentation-model.h5')","29517aeb":"train_loss, train_accuracy = model.evaluate(train_dataset, batch_size = 32)\nvalidation_loss, validation_accuracy = model.evaluate(validation_dataset, batch_size = 32)\ntest_loss, test_accuracy = model.evaluate(test_dataset, batch_size = 32)","f85e416c":"print(f'Model Accuracy on the Training Dataset: {round(train_accuracy * 100, 2)}%')\nprint(f'Model Accuracy on the Validation Dataset: {round(validation_accuracy * 100, 2)}%')\nprint(f'Model Accuracy on the Test Dataset: {round(test_accuracy * 100, 2)}%')","d935ccab":"def iou_score(dataset):\n    \n    \"\"\"\n    Argument:\n        dataset -- the dataset to calculate IoU on\n    \n    Returns:\n        min_iou -- minimum IoU\n        max_iou -- maximum IoU\n        mean_iou -- mean IoU ()\n        \"\"\"\n    # Create empty lists \n    intersections, unions, max_ious, min_ious = [], [], [], []\n    \n    for images, masks in dataset:\n        pred_mask = model.predict(images)\n        intersection = np.logical_and(masks, pred_mask)\n        union = np.logical_or(masks, pred_mask)\n        intersection_sum = np.array([np.sum(inter) for inter in intersection])\n        union_sum = np.array([np.sum(un) for un in union])\n        batch_iou_score = intersection_sum \/ union_sum\n        batch_min_iou = np.amin(batch_iou_score)\n        batch_max_iou = np.amax(batch_iou_score)\n        \n        intersections.append(np.sum(intersection))\n        unions.append(np.sum(union))\n        min_ious.append(batch_min_iou)\n        max_ious.append(batch_max_iou)\n\n    min_iou = np.amin(min_ious)\n    max_iou = np.amax(max_ious)\n    mean_iou = np.sum(intersections) \/ np.sum(unions)   \n    \n    return min_iou, max_iou, mean_iou","be9787a0":"train_min_iou, train_max_iou, train_mean_iou = iou_score(train_dataset)\nvalidation_min_iou, validation_max_iou, validation_mean_iou = iou_score(validation_dataset)\ntest_min_iou, test_max_iou, test_mean_iou = iou_score(test_dataset)","5934753e":"print(f'IoU on the Training Dataset: \\n Minimum IoU Score: {round(train_min_iou*100, 2)}% \\n Maximum IoU Score: {round(train_max_iou*100, 2)}% \\n Mean IoU Score: {round(train_mean_iou*100, 2)}% \\n')\nprint(f'IoU on the Validation Dataset: \\n Minimum IoU Score: {round(validation_min_iou*100, 2)}% \\n Maximum IoU Score: {round(validation_max_iou*100, 2)}% \\n Mean Iou Score: {round(validation_mean_iou*100, 2)}% \\n')\nprint(f'IoU on the Test Dataset: \\n Minimum IoU Score: {round(test_min_iou*100, 2)}% \\n Maximum IoU Score: {round(test_max_iou*100, 2)}% \\n Mean IoU Score: {round(test_mean_iou*100, 2)}% \\n')","1c06b410":"# Load model\nfrom tensorflow.keras.models import Model, load_model\nmodel = load_model('carla-image-segmentation-model.h5')","88e1fcbc":"def create_mask(pred_mask):\n    pred_mask = tf.argmax(pred_mask, axis=-1)\n    pred_mask = pred_mask[..., tf.newaxis]\n    return pred_mask[0]","b01f0d2b":"def display(display_list):\n    plt.figure(figsize=(15, 15))\n\n    title = ['Input Image', 'True Mask', 'Predicted Mask']\n\n    for i in range(len(display_list)):\n        plt.subplot(1, len(display_list), i+1)\n        plt.title(title[i])\n        plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n        plt.axis('off')\n    plt.show()","3e27eac2":"def show_predictions(dataset, num):\n    \"\"\"\n    Displays the first image of each of the num batches\n    \"\"\"\n    if dataset:\n        for image, mask in dataset.take(num):\n            pred_mask = model.predict(image)\n            display([image[0], mask[0], create_mask(pred_mask)])\n    else:\n        display([sample_image, sample_mask,\n             create_mask(model.predict(sample_image[tf.newaxis, ...]))])","0dd21d54":"show_predictions(train_dataset, 6)","8565b897":"show_predictions(validation_dataset, 6)","cf95be6a":"show_predictions(test_dataset, 6)","bf5796ed":"<a name='5-1'><\/a>\n### **5.1. Create functions to preprocess selected images and display their true state, true mask and predicted mask**\n\nIn this step, we will:\n1. Load our model\n2. Define a function to create new masks using our model\n3. Define a function to display outputs of this process: an input image, its true mask, and its predicted mask.\n4. Define a function to select images from a specified dataset and return the images, their true masks and their predicted masks.","7dace4a1":"#### **2.1.1. Create lists containing the paths of images and masks**\n\nIn this step, we will\n\n* Create a list that contains all the paths to all directories in the main directory (a list that contains the path to dataA, dataB, dataC, dataD, and dataE)\n* Create a function to iterate over all the direcory paths where our data are located (list in 1.) and return the list of the image paths in those directories. \n* Create lists of image and mask paths by initializing the function above\n* Preview some masked and unmasked images by reading them from their paths","c0db966a":"<a name='4-2'><\/a>\n### **4.2. Intersection-over-Union (IoU)**\n\nThe Intersection-over-Union (IoU), also referred to as the Jaccard index is useful in quantifying the percent overlap between the target mask and the mask prediction output from our model. Recall that the task of semantic segmentation is simply to predict the class of each pixel in an image. So, the IoU aims to evaluate the similarities in the pixels of both the ground true mask and the predicted mask.\n\nSince the size of our ground true masks and the predicted masks size is 256 * 256 * 23 (which is equivalent to 59,069,888 pixels). Hence, our Model's IoU Score for any image in our dataset is the proprortion of the 59,069,888 pixels in the predicted mask that matches with the ground true mask's 59,069,888 pixels. \n\n\n[Kindly Read more on IoU and other Image Segmentation Metrics here](https:\/\/www.jeremyjordan.me\/evaluating-image-segmentation-models\/)","8735bb7c":"<a name='4'><\/a>\n## **4. Model Evaluation**\n\nWe will be using Model Accuracy and Mean Intersection-over-Union (mIoU) to evaluate our model performance.","3bba319c":"<a name='2-1'><\/a>\n### **2.1. Load the images and masks from their directories**\n\nIn this data preparation step, we will:\n1. Create 2 lists containing the paths of images and masks\n2. Split the lists into training, validation and test sets","8d1214eb":"#### **2.2.4. Preview sample images and masks from the three dataset categories**","39960756":"**Preview random masked and unmasked images by reading them from their paths**","e84700ee":"#### **2.2.3. Create data pipelines for the training, validation and test sets using both functions**","232d3f7c":"**A. Create a list that contains all the paths to all directories in the main directory (a list that contains the path to dataA, dataB, dataC, dataD, and dataE)**","ec7eb67e":"<a name='5-2'><\/a>\n### **5.2. Predict and compare masks of images in the training set**","757c141b":"<a name='4-1'><\/a>\n### **4.1. Model Accuracy**\n","6e757f92":"##### **5.1.2. Define a function to create new masks using our model** ","910dd7b1":"<a name='3-2'><\/a>\n### **3.2. Model Training**","b428e64f":"<a name='5'><\/a>\n## **5. Predict image segmentations using the trained Model**\n\nIn this section, we will\n\n1. Create a function to preprocess selected images and display their true state, true mask and predicted mask\n2. Predict and compare masks of images in the training set\n3. Predict and compare masks of images in the validation set\n4. Predict and compare masks of images in the test set","298b9def":"#### **2.1.2. Split the image and mask paths into training, validation, and test sets**","b1517f7b":"<a name='2-2'><\/a>\n### **2.2. - Create a data pipeline to read and preprocess our data**\n\nWe will be using the tf.data.Dataset API to load our images and masks for our model to process. The Dataset API allows us to build an asynchronous, highly optimized data pipeline to prevent our GPU from data starvation. It loads data from the disk (images or text), applies optimized transformations, creates batches and sends it to the GPU. Unlike former data pipelines made the GPU, the Dataset API wait for the CPU to load the data, leading to performance issues.\n\nTo do this, we will \n1. Create a function to read image and mask paths and return equivalent arrays\n2. Create a data generator function to read and load images and masks in batches\n3. Create data pipelines for the training, validation and test sets using both functions\n4. Preview sample images and their segmentations from the three dataset categories","37ddc107":"<a name='1'><\/a>\n## **1. Import Required Packages**","f1b43492":"<a name='3-1'><\/a>\n\n### **3.1. DeepLabV3+ Model Design**","f3404f3d":"<a name='5-4'><\/a>\n### **5.4. Predict and compare masks of images in the test set**","5491a917":"#### **2.2.2. Create a data generator function to read and load images and masks in batches**\n\n","4b5a59c8":"**Create a function to iterate over all the direcory paths where our data are located (list in 2.1.1.) and return the list of the image paths in those directories**","995f2859":"<a name='5-3'><\/a>\n### **5.3. Predict and compare masks of images in the validation set**","7d329f95":"##### **5.1.1. Load our model** ","fff9b738":"**Create lists of image and mask paths by initializing the function above**","c30597bf":"## **Project Overview**: \n\n| <br\/><font size=\"3\"><b> Focus <\/b><\/font><br\/><br\/>  | <br\/><font size=\"3\"><b> Description <\/b><\/font><br\/><br\/> |\n| :-- | :-- |\n| <br\/><font size=\"3\"> <b>Project Title<\/b> <\/font>  <br\/><br\/>| <br\/><font size=\"3\"> CARLA Image Semantic Segmentation with DeepLabV3+<\/font> <br\/><br\/>|\n| <br\/><font size=\"3\"> <b>Project Type<\/b> <\/font> <br\/><br\/>| <br\/><font size=\"3\">Image Segmentation (Semantic Segmenetation) <\/font> <br\/><br\/>|\n| <br\/><font size=\"3\"><b>Project Objectives<\/b> <\/font>  <br\/><br\/>| <br\/><font size=\"3\">1. Create a Model to predict semantic segmentations of CARLA images<br\/><br\/> 2. Predict masks using the model and compare with ground-truth masks <\/font> <br\/><br\/>|\n| <br\/><font size=\"3\"> <b>Dataset Overview <\/b> <\/font> <br\/><br\/>| <br\/><font size=\"3\"> This dataset provides data images and labeled semantic segmentations captured via CARLA self-driving car simulator. The data which was generated as part of the 2018 Lyft Udacity Perception Challenge consists of **5000** images and their semantic segmentations. <\/font> <br\/><br\/>|\n| <br\/><font size=\"3\"> <b> Model Evaluation Metrics<\/b> <\/font> <br\/><br\/>| <br\/><font size=\"3\"> Accuracy:- Minimum of 92% <br\/> Intersection over Union (IoU):- Minimum IoU: 50%, Max IoU: 80%, Mean IoU: 65% <\/font> <br\/><br\/>|\n| <font size=\"3\"> <b>Image Segmentation Model Type<\/b> <\/font> | <br\/><font size=\"3\"> <a href=\"https:\/\/arxiv.org\/pdf\/1802.02611.pdf\"> <b> DeepLabV3+ Architecture<\/b><\/a> <\/font> <br\/><br\/>|\n| <br\/><font size=\"3\"> <b>Major Libraries Used<\/b> <\/font> <br\/><br\/>| <br\/><font size=\"3\"> <a href=\"https:\/\/keras.io\"><b>Keras<\/b><\/a>, <a href=\"https:\/\/imageio.readthedocs.io\/\"><b>ImageIO<\/b><\/a>, <a href=\"https:\/\/scikit-learn.org\"><b> Scikit-Learn <\/b><\/a>, <a href=\"https:\/\/numpy.org\"><b>Numpy<\/b><\/a>, <a href=\"https:\/\/matplpotlib.org\"><b>Matplotlib<\/b><\/a><\/font> <br\/><br\/>|\n\n<br\/>","fbabc5e7":"<a name='2'><\/a>\n## **2. Data Preparation**\n\nThe Lyft Udacity Semantic Segmentation for Self-driving Cars Challenge data (images and masks) is splitted across five directories (dataA, dataB, dataC, dataD, and dataE). As part of the data preparation step, we will load images and masks from all the five directories and carry out the a few preprocessing steps to ensure we provide our model with quality dataset.","ee7e0fd7":"## **Table of Contents**: \n<font size=\"3\">\n\n- [**1 - Import Required Packages**](#1)\n\n    \n- [**2 - Data Preparation**](#2)\n    - [2.1. Load the images and masks from their directories](#2-1)\n    - [2.2.  Create a data pipeline to read and preprocess our data](#2-2)\n\n    \n- [**3 - Model Architecture and Training**](#3)\n    - [3.1. - DeepLabV3+ Model design](#3-1)\n    - [3.2. - Model training](#3-2)    \n\n    \n- [**4 - Model Evaluation**](#4)\n    - [4.1. - Model Accuracy](#4-1)\n    - [4.2. - Intersection-over-Union (IoU)](#4-2)\n\n    \n- [**5 - Predict image segmentations using the trained Model**](#5)\n    - [5.1. - Create functions to preprocess selected images and display their true state, true mask and predicted mask](#5-1)\n    - [5.2. - Prediction on the train set](#5-2)\n    - [5.3. - Prediction on the validation set](#5-3)\n    - [5.4. - Prediction on the test set](#5-4)    \n    \n<font>","d560f391":"##### **5.1.4. Define a function to select images from a specified dataset and return the images, their true masks and their predicted masks** ","7f8561a0":"<a name='3'><\/a>\n## **3. Model Architecture and Training**\nWe will using a the **DeepLabv3+ architecture** to train our semantic segmentation model. The DeepLabv3+ is a semantic segmentation architecture that improves upon DeepLabv3 with several improvements, such as adding a simple yet effective decoder module to achieve an encoder-decoder structure. The encoder module processes multiscale contextual information by applying dilated convolution at multiple scales, while the decoder module refines the segmentation results along object boundaries.\n\n<center><img src=\"https:\/\/i.ibb.co\/cXmRSr3\/deeplabv3-plus-diagram.png\" alt=\"deeplabv3-plus-diagram\" border=\"0\"><\/center>\n","740592b4":"#### **2.2.1. Create a function to read image and mask paths and return equivalent arrays**\n\nThe **read_image** function will\n1. Read an image and its mask from their paths\n2. Convert the digital image and its mask to image arrays \n3. Normalize the datasets\n4. Resize the image and its masks to a desired dimension","924566fd":"##### **5.1.3. Define a function to display outputs of this process: an input image, its true mask, and its predicted mask** "}}