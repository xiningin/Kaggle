{"cell_type":{"0460e1af":"code","7117775f":"code","eb0b6d2e":"code","bb3ec644":"code","cd244f5b":"code","65033667":"code","0524648d":"code","c0a9365d":"code","cb74c868":"code","ee242ff8":"code","948bd8f7":"code","184c8afd":"code","20ee17d0":"code","4d1708f2":"code","2c1dff5b":"code","47b0f683":"code","be16172b":"code","681f499b":"code","86c0ba65":"markdown","2d933fe5":"markdown","690fde39":"markdown","caa7ea72":"markdown","5df4079e":"markdown","5a14643d":"markdown","34398203":"markdown","67f67691":"markdown","a4c92044":"markdown","f4eeae37":"markdown","713bbf76":"markdown","796affba":"markdown","41a75e41":"markdown","691d109b":"markdown","dd48ca72":"markdown","22602a88":"markdown","47a627f6":"markdown"},"source":{"0460e1af":"import pip._internal as pip\npip.main(['install', '--upgrade', 'numpy==1.17.2'])\nimport numpy as np\nimport pandas as pd\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom lightgbm import LGBMClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier\nfrom sklearn.ensemble import BaggingClassifier, ExtraTreesClassifier, RandomForestClassifier\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix, make_scorer, f1_score\nfrom sklearn.model_selection import cross_val_predict, cross_val_score, GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.utils.multiclass import unique_labels\nfrom xgboost import XGBClassifier\n\nimport time\nimport pickle\n\nfrom lwoku import RANDOM_STATE, N_JOBS, VERBOSE, get_prediction\nfrom grid_search_utils import plot_grid_search, table_grid_search","7117775f":"VERBOSE=1","eb0b6d2e":"# Read training and test files\nX_train = pd.read_csv('..\/input\/learn-together\/train.csv', index_col='Id', engine='python')\nX_test = pd.read_csv('..\/input\/learn-together\/test.csv', index_col='Id', engine='python')\n\n# Define the dependent variable\ny_train = X_train['Cover_Type'].copy()\n\n# Define a training set\nX_train = X_train.drop(['Cover_Type'], axis='columns')","bb3ec644":"if_clf = IsolationForest(verbose=VERBOSE,\n                         random_state=RANDOM_STATE,\n                         n_jobs=N_JOBS)","cd244f5b":"f1_scorer = make_scorer(f1_score, average='micro')","65033667":"parameters = {\n    'n_estimators': [100, 200, 300, 400, 500, 600, 700, 800]\n}\nclf = GridSearchCV(if_clf, parameters, scoring=f1_scorer, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf, all_ranks=True)","0524648d":"parameters = {\n    'max_samples': ['auto', 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181, 6765, 10946, 17711]\n}\nclf = GridSearchCV(if_clf, parameters, scoring=f1_scorer, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf, all_ranks=True)","c0a9365d":"parameters = {\n    'contamination': [x \/ 20 for x in range(1, 11)]\n}\nclf = GridSearchCV(if_clf, parameters, scoring=f1_scorer, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf, all_ranks=True)","cb74c868":"parameters = {\n    'max_features': [x \/ 10 for x in range(1, 11)]\n}\nclf = GridSearchCV(if_clf, parameters, scoring=f1_scorer, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf, all_ranks=True)","ee242ff8":"parameters = {\n    'bootstrap': [True, False]\n}\nclf = GridSearchCV(if_clf, parameters, scoring=f1_scorer, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf, all_ranks=True)","948bd8f7":"parameters = {\n    'warm_start': [True, False]\n}\nclf = GridSearchCV(if_clf, parameters, scoring=f1_scorer, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf, all_ranks=True)","184c8afd":"parameters = {\n    'n_estimators': [100, 200],\n    'max_samples': ['auto', 10000],\n#     'contamination': [0.05, 0.10],\n    'max_features': [0.6, 0.7, 1.0],\n    'bootstrap': [True, False],\n}\nclf = GridSearchCV(if_clf, parameters, scoring=f1_scorer, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","20ee17d0":"with open('clf.pickle', 'wb') as fp:\n    pickle.dump(clf, fp)","4d1708f2":"if_clf = IsolationForest(verbose=VERBOSE,\n                         random_state=RANDOM_STATE,\n                         n_jobs=N_JOBS)","2c1dff5b":"contamination_set = [0.01, 0.05, 0.1, 0.2]\noutliers = {}\nfor contamination in contamination_set:\n    c = str(contamination)\n    print(contamination)\n    if_clf.contamination = contamination\n    if_clf.fit(X_train, y_train)\n    outliers[c] = pd.Series(if_clf.predict(X_train), index=X_train.index)\n    outliers[c].to_csv('outliers_{}.csv'.format(contamination),\n                                        header=['Cover_Type'],\n                                        index=True,\n                                        index_label='Id')","47b0f683":"with open('..\/input\/tactic-03-hyperparameter-optimization-lightgbm\/clf.pickle', 'rb') as fp:\n    clf = pickle.load(fp)\nlg_clf = clf.best_estimator_\nlg_clf.class_weight = None\nlg_clf","be16172b":"results = pd.DataFrame(columns = ['Model',\n                                  'Accuracy',\n                                  'Fit time',\n                                  'Predict test set time',\n                                  'Predict train set time'])\n\nX_train_cleaned = {}\ny_train_cleaned = {}\nmodel = lg_clf\n\nfor contamination in contamination_set:\n#     print(contamination)\n    c = str(contamination)\n    X_train_cleaned[c] = X_train[outliers[c] == 1]\n    y_train_cleaned[c] = y_train[outliers[c] == 1]    \n\n    # Fit\n    t0 = time.time()\n    model.fit(X_train_cleaned[c], y_train_cleaned[c])\n    t1 = time.time()\n    t_fit = (t1 - t0)\n#     print(t_fit)\n    \n    # Predict test set\n    t0 = time.time()\n    y_test_pred = pd.Series(model.predict(X_test), index=X_test.index)\n    t1 = time.time()\n    t_test_pred = (t1 - t0)\n#     print(t_test_pred)\n    \n    # Predict train set\n    t0 = time.time()\n    y_train_pred = pd.Series(get_prediction(model, X_train_cleaned[c], y_train_cleaned[c]), index=X_train_cleaned[c].index)\n    accuracy = accuracy_score(y_train_cleaned[c], y_train_pred)\n    t1 = time.time()\n    t_train_pred = (t1 - t0)\n#     print(t_train_pred)\n    \n   # Submit\n    y_train_pred.to_csv('train_lg_{}.csv'.format(contamination), header=['Cover_Type'], index=True, index_label='Id')\n    y_test_pred.to_csv('submission_lg_{}.csv'.format(contamination), header=['Cover_Type'], index=True, index_label='Id')\n    \n    results = results.append({\n        'Model': 'lg_cont_{}'.format(contamination),\n        'Accuracy': accuracy,\n        'Fit time': t_fit,\n        'Predict test set time': t_test_pred,\n        'Predict train set time': t_train_pred\n    }, ignore_index = True)","681f499b":"results = results.sort_values('Accuracy', ascending=False).reset_index(drop=True)\nresults.to_csv('results.csv', index=True, index_label='Id')\nresults","86c0ba65":"# contamination\n##### : float in (0., 0.5), optional (default=0.1)\n\nThe amount of contamination of the data set, i.e. the proportion\nof outliers in the data set. Used when fitting to define the threshold\non the decision function. If 'auto', the decision function threshold is\ndetermined as in the original paper.","2d933fe5":"# Introduction\n\nThe aim of this notebook is to test if the outlier detection has an influence on accuracy.\n\nFirst, all [Isolation Forest Algorithm](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.IsolationForest.html) parameters are analysed separately.\n\nThen, a grid search is carried out.\nThis is a search through all the combinations of parameters,\nwhich optimize the internal score in the train set.\n\nThe results are collected at [Tactic 99. Summary](https:\/\/www.kaggle.com\/juanmah\/tactic-99-summary).","690fde39":"# Search over parameters","caa7ea72":"## Export grid search results","5df4079e":"# Fit and predict","5a14643d":"# Remove outliers","34398203":"# max_samples\n##### : int or float, optional (default=\"auto\")\n\nThe number of samples to draw from X to train each base estimator.\n\n    - If int, then draw `max_samples` samples.\n    - If float, then draw `max_samples * X.shape[0]` samples.\n    - If \"auto\", then `max_samples=min(256, n_samples)`.\n\nIf max_samples is larger than the number of samples provided,\nall samples will be used for all trees (no sampling).\n**","67f67691":"# max_features\n##### : int or float, optional (default=1.0)\n\nThe number of features to draw from X to train each base estimator.\n\n    - If int, then draw `max_features` features.\n    - If float, then draw `max_features * X.shape[1]` features.","a4c92044":"## LightGBM\n\nExamine the full hyperparameter optimization details for this model in the following notebook: [Tactic 03. Hyperparameter optimization. LightGBM](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization-lightgbm)","f4eeae37":"Default values seem to be a correct aproximation.","713bbf76":"# Check results","796affba":"# warm_start\n##### : bool, optional (default=False)\n\nWhen set to ``True``, reuse the solution of the previous call to fit\nand add more estimators to the ensemble, otherwise, just fit a whole\nnew forest.","41a75e41":"# Prepare data","691d109b":"# Exhaustive search","dd48ca72":"# Fit and predict","22602a88":"# bootstrap\n##### : boolean, optional (default=False)\n\nIf True, individual trees are fit on random subsets of the training\ndata sampled with replacement. If False, sampling without replacement\nis performed.\n","47a627f6":"# n_estimators\n##### : int, optional (default=100)\n\nThe number of base estimators in the ensemble."}}