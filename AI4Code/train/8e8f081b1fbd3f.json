{"cell_type":{"82c083ab":"code","0034db53":"code","b5a65574":"code","db8ea512":"code","3ebe4547":"code","7ffe2fd6":"code","722b31a1":"code","208864d6":"code","c04ce4fc":"code","36781a8a":"code","2b5dae96":"code","15108c34":"code","0dd05126":"code","c3d3819b":"code","1aed3c76":"code","8f1d4568":"code","ef2e0144":"code","f165825f":"code","1f95ed81":"code","872ae232":"code","17200407":"code","6b97b208":"markdown","79c22aba":"markdown","79c9fbf0":"markdown"},"source":{"82c083ab":"import pandas as pd\nimport numpy as np\nimport cv2\nimport os\nimport re\nimport time\nimport copy\nimport seaborn as sns\nimport itertools\n\nfrom tqdm import tqdm\n\nfrom PIL import Image\nimport shap\n\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nimport torchvision\nfrom torchvision import datasets, models, transforms\n\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.sampler import SequentialSampler\n\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom torch.nn import functional as F","0034db53":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(f'Device: {device}')","b5a65574":"root_path = '\/kaggle\/input\/skincancermalignantvsbenign\/data'\ntrain_path = os.path.join(root_path,'train')\ntest_path= os.path.join(root_path,'test')\n\npaths ={'root':root_path,'train':train_path,'val':test_path,}","db8ea512":"data_transforms = {\n    'train': transforms.Compose([\n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}","3ebe4547":"image_datasets = {x: datasets.ImageFolder(paths[x],transform=data_transforms[x]) for x in ['train', 'val']}\ndataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n                                             shuffle=True, num_workers=4)\n              for x in ['train', 'val']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\nclass_names = image_datasets['train'].classes\n\nprint(f\"Dataset size: {dataset_sizes}\")\nprint(f\"Classes: {class_names}\")","7ffe2fd6":"def imshow(inp, title=None):\n    \"\"\"Imshow for Tensor.\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)  # pause a bit so that plots are updated\n\n\n# Get a batch of training data\ninputs, classes = next(iter(dataloaders['train']))\n# Make a grid from batch\nout = torchvision.utils.make_grid(inputs)\n\nimshow(out, title=[class_names[x] for x in classes])","722b31a1":"seed = 40\ntorch.cuda.empty_cache()\ntorch.manual_seed(seed)\nnp.random.seed(seed)\nnum_classes = len(class_names)","208864d6":"model_ft = torchvision.models.resnet34(pretrained=True)\nnum_ftrs = model_ft.fc.in_features\nmodel_ft.fc = nn.Linear(in_features=num_ftrs, out_features=num_classes)","c04ce4fc":"for name, param in model_ft.named_parameters():\n    if name not in [\"fc.weight\", \"fc.bias\"]:\n        param.requires_grad = False","36781a8a":"def train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs=25,\n                is_inception=False):\n    since = time.time()\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    history = dict()\n    history['train_acc'] = list()\n    history['train_loss'] = list()\n    history['val_acc'] = list()\n    history['val_loss'] = list()\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print('Epoch {}\/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()  # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # Iterate over data.\n            for inputs, labels in tqdm(dataloaders[phase], desc='Batches'):\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    # Get model outputs and calculate loss\n                    # Special case for inception because in training it has an auxiliary output. In train\n                    #   mode we calculate the loss by summing the final output and the auxiliary output\n                    #   but in testing we only consider the final output.\n\n                    outputs = model(inputs)\n                    loss = criterion(outputs, labels)\n\n                    _, preds = torch.max(outputs, 1)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n            if phase == 'train':\n                scheduler.step()\n\n            epoch_loss = running_loss \/ len(dataloaders[phase].dataset)\n            epoch_acc = running_corrects.double() \/ len(dataloaders[phase].dataset)\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n            history[f'{phase}_loss'] += [epoch_loss]\n            history[f'{phase}_acc'] += [epoch_acc]\n\n            # deep copy the model\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n        print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed \/\/ 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model, history","2b5dae96":"criterion = nn.CrossEntropyLoss()\n\n# Observe that all parameters are being optimized\noptimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n\n# Decay LR by a factor of 0.1 every 7 epochs\nfrom torch.optim import lr_scheduler\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n\nmodel_ft.to(device)","15108c34":"epochs = 50\nmodel_ft, history = train_model(model_ft, dataloaders, criterion, optimizer_ft, exp_lr_scheduler,\n                       num_epochs=epochs)","0dd05126":"ta = history['train_acc']\n\nphs = ['train','val']\nmtr = ['acc','loss']\n\nplt.figure(figsize=(20,5))\nj=1\nfor i,mt in enumerate(mtr):\n    plt.subplot(1,2,j)\n    j=j+1\n    for ph in phs:\n        k = f'{ph}_{mt}'\n        v = [float(vt) for vt in history[k]]\n        plt.plot(v,label=k)\n\n    plt.legend()\nplt.show()","c3d3819b":"def get_test_results(model, dataloader):\n    label = np.array([])\n    label_predicted = np.array([])\n\n    for batch_idx, (inputs, labels) in enumerate(dataloader):\n        inputs = inputs.to(device)\n        outputs = model(inputs)\n        _, preds = torch.max(outputs, 1)\n\n        # append labels\n        label = np.append(label, labels.numpy())\n        label_predicted = np.append(label_predicted, preds.cpu().numpy())\n\n    return label, label_predicted\n\nlabel, label_predicted = get_test_results(model_ft, dataloaders[\"val\"])\nlabel_list = label.tolist()\nlabel_predicted_list = label_predicted.tolist()\nlen(label), len(label_predicted)","1aed3c76":"# create and display a confusion matrix\ndef plot_confusion_matrix(cm, classes, title='Confusion matrix', cmap=plt.cm.Blues):\n    plt.figure(figsize=(20,5))\n    j=1\n    for normalize in [False,True]:\n        fig = plt.subplot(1,2,j)\n        j += 1\n        \"\"\"\n        This function prints and plots the confusion matrix.\n        Normalization can be applied by setting `normalize=True`.\n        \"\"\"\n        if normalize:\n            cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n            x_title = title + ' (Normalized)' \n            # print(\"Normalized confusion matrix\")\n        else:\n            x_title = title \n            # print('Confusion matrix, without normalization')\n\n        # print(cm)\n        \n        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n        plt.title(x_title)\n        plt.colorbar()\n        tick_marks = np.arange(len(classes))\n        plt.xticks(tick_marks, classes, rotation=45)\n        plt.yticks(tick_marks, classes)\n\n        fmt = '.2f' if normalize else 'd'\n        thresh = cm.max() \/ 2.\n        for mi, mj in itertools.product(range(2), range(2)):   \n            plt.text(mj, mi, format(cm[mi, mj], fmt), horizontalalignment=\"center\", color=\"white\" if cm[mi, mj] > thresh else \"black\")\n\n        plt.ylabel('True label')\n        plt.xlabel('Predicted label')\n        plt.tight_layout()\n    plt.show()\n\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(label_list, label_predicted_list)\nnp.set_printoptions(precision=2)\n\n# Plot non-normalized confusion matrix\nplot_confusion_matrix(cnf_matrix, classes=class_names)\n# display()","8f1d4568":"dataset = image_datasets['val']\nimg_idx = np.random.choice(range(len(dataset.imgs)))\nprint(f\"IMG: {img_idx} ({len(dataset.imgs)})\")\nimg_path = dataset.imgs[img_idx][0]\nprint(img_path)\nlabel_idx = dataset.imgs[img_idx][1]\nlabel = class_names[label_idx]\n\nimg = Image.open(img_path)\nimg_t = data_transforms['val'](img)\nbatch_t = torch.unsqueeze(img_t, 0)\nbatch_t = batch_t.to(device)\n\noutputs = model_ft(batch_t)\nsm = torch.nn.Softmax(dim=1)\nprobabilities = sm(outputs).tolist()[0]\npred = class_names[np.argmax(probabilities)]\n\nprob_list = [f'{class_names[i]}:  {probabilities[i]:>.3f}' for i in range(len(class_names))]\ntitle_list = [f'True label: {label}',f'Prediction: {pred}']\n\nt = 'Hello!'\nfor i,t in enumerate(title_list + prob_list):\n    extra_space = 0\n    d = i * 30\n    color= 'black'\n    if i >= len(title_list):\n        extra_space = 80\n        color = 'blue'\n    plt.text(230, 12 + extra_space + d, t,size=18, ha='left',color=color, rotation=0, wrap=True)\n    \n    \n_ = plt.imshow(img)","ef2e0144":"model = model_ft\nndevice = device\n_ = model.to(ndevice)","f165825f":"global features_blobs\n\nfeatures_blobs = []\ndef hook_feature(module, input, output):\n    global features_blobs\n    \n    d = output.data.cpu().numpy()\n    features_blobs =[d]\n    print(f'Added - : {d.shape}\\t{np.mean(d)}\\t blobs: {len(features_blobs)}')\n    \n\ndef register_hook(model, finalconv_name):\n    model._modules.get(finalconv_name).register_forward_hook(hook_feature)\n\nregister_hook(model, 'layer4')","1f95ed81":"def return_cam(feature_conv, weight_softmax, class_idx, model_input_size):\n    # generate the class activation maps upsample to 256x256\n    size_upsample = model_input_size\n    bz, nc, h, w = feature_conv.shape\n    output_cam = []\n    for idx in class_idx:\n        cam = weight_softmax[idx].dot(feature_conv.reshape((nc, h*w)))\n        cam = cam.reshape(h, w)\n        cam = cam - np.min(cam)\n        cam_img = cam \/ np.max(cam)\n        cam_img = np.uint8(255 * cam_img)\n        output_cam.append(cv2.resize(cam_img, size_upsample))\n    return output_cam","872ae232":"def plot_images_cam(paths, model, input_size, features_blobs, class_names, columns=4, rows=5):\n    fig = plt.figure(figsize=(15, 15))\n    end = min(columns*rows+1, len(paths)+1)\n    for i in range(1, end):\n        img, pred = calculate_cam(model, input_size, features_blobs, paths[i-1], class_names)\n        ax = fig.add_subplot(rows, columns, i)\n        ax.title.set_text(\"Predicted: {}\".format(pred))\n        ax.set_xticks([])\n        ax.set_yticks([])\n        plt.imshow(img)\n    plt.show()\n\ndef create_path_list(base_path):\n    path_list = []\n    files = os.listdir(base_path)\n    for _, file in enumerate(files):\n        path_list.append(os.path.join(base_path, file))\n    return path_list","17200407":"dataset = image_datasets['val']\nimg_idx = np.random.choice(range(len(dataset.imgs)))\nprint(f\"IMG: {img_idx} ({len(dataset.imgs)})\")\nimg_path = dataset.imgs[img_idx][0]\nprint(img_path)\nlabel_idx = dataset.imgs[img_idx][1]\nlabel = class_names[label_idx]\n\nimg = Image.open(img_path)\nimg_t = data_transforms['val'](img)\nbatch_t = torch.unsqueeze(img_t, 0)\nbatch_t = batch_t.to(device)\n\nlogit = model_ft(batch_t)\nsm = torch.nn.Softmax(dim=1)\nprobabilities = sm(logit).tolist()[0]\npred = class_names[np.argmax(probabilities)]\n\nparams = list(model.parameters())\nweight_softmax = np.squeeze(params[-2].data.clone().cpu().numpy())\n\nh_x = F.softmax(logit, dim=1).data.squeeze()\nprobs, idx = h_x.sort(0, True)\nprobs = probs.clone().cpu().numpy()\nidx = idx.clone().cpu().numpy()\n\n# generate class activation mapping for the top1 prediction\ninput_size = (224, 224)\n\n# generate the class activation maps upsample to 256x256\nprint(f'Len of blobs: {len(features_blobs)}')\nfeature_conv = features_blobs[0]\nclass_idx = [idx[0]]\nsize_upsample = input_size\nbz, nc, h, w = feature_conv.shape\noutput_cam = []\nfor cidx in class_idx:\n    cam = weight_softmax[cidx].dot(feature_conv.reshape((nc, h*w)))\n    cam = cam.reshape(h, w)\n    cam = cam - np.min(cam)\n    cam_img = cam \/ np.max(cam)\n    cam_img = np.uint8(255 * cam_img)\n    output_cam.append(cv2.resize(cam_img, size_upsample))\nCAMs = output_cam\n        \n\nimg = Image.open(img_path)\nimg_t = np.array(img)\nimg_t = img_t.astype(float) \/ 255.0\n\nheatmap = copy.deepcopy(CAMs[0])\nheatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\nheatmap = heatmap.astype(float)\nheatmap \/= 255.0\nheatmap = 1.0 - heatmap\n\nforeg = Image.fromarray((heatmap * 255).astype(np.uint8)).convert(\"RGBA\")\nbackg = Image.fromarray((img_t * 255).astype(np.uint8)).convert(\"RGBA\")\nxt,yt = foreg.size\nforeg.putalpha(100)\n\nfinal2 = Image.new(\"RGBA\", backg.size)\nfinal2 = Image.alpha_composite(final2, backg)\nfinal2 = Image.alpha_composite(final2, foreg)\n# plt.imshow(final2)\n\nfig1, [ax1,ax2] = plt.subplots(1,2,figsize=(20,10))\nax1.imshow(img)\nax1.set_title(f'Ground truth: {label}')\nax2.imshow(final2)\nax2.set_title(f\"Prediction: {pred}\")\nplt.show()","6b97b208":"# Model training","79c22aba":"# Explanabiliy","79c9fbf0":"# Results analysis"}}