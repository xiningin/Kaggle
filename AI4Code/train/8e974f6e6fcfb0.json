{"cell_type":{"241b2837":"code","7b8cb430":"code","50acba73":"code","1e7b0d1e":"code","f27a9d90":"code","c071a233":"code","3baf4b12":"code","27843b76":"code","5cc5a587":"code","b24a0375":"code","10a9ecdb":"code","767aac61":"code","db2100ae":"code","12dd9852":"code","b8ef80a9":"code","662b8e45":"code","de1fb3d8":"code","11e30a1e":"code","561cc16a":"code","4f046928":"code","2ab22af9":"markdown","a1895aae":"markdown","ae18e61e":"markdown","8e6403cd":"markdown","ba2d3d9e":"markdown","a7f28c9f":"markdown","377139f9":"markdown","5dac59fa":"markdown","c8b14baa":"markdown","c2165f5a":"markdown","e30c5ee1":"markdown","baff65de":"markdown","1c3068ab":"markdown","6e42692d":"markdown","c18f000a":"markdown","b7160d93":"markdown","f53d3d44":"markdown","807ac3be":"markdown","c550c444":"markdown","d230cfd9":"markdown","1e03a3a8":"markdown","5622bff4":"markdown","e5e1d137":"markdown","b3358bc8":"markdown","1ff21dd5":"markdown","2a114af3":"markdown","361681aa":"markdown","97ac12d5":"markdown","ce7c54bc":"markdown","c51a0319":"markdown","75ea5d58":"markdown","7b651c00":"markdown","5f0c1b63":"markdown"},"source":{"241b2837":"from IPython.display import Image\nImage(\"..\/input\/optimization-talk-images\/gradient_descent.png\")","7b8cb430":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt","50acba73":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as opt\nfrom torch.autograd import Variable\nimport torchvision\nimport torchvision.transforms as transforms","1e7b0d1e":"path = '.\/data\/'","f27a9d90":"!tar -zxvf ..\/input\/cifar10-python\/cifar-10-python.tar.gz","c071a233":"transform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n)\n\ntrainset = torchvision.datasets.CIFAR10(root='.', train=True, download=False, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.CIFAR10(root='.', train=False, download=False, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)\n\nclasses = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')","3baf4b12":"class CifarNet(nn.Module):\n    def __init__(self):\n        super(CifarNet, self).__init__()\n        self.conv_layers = nn.Sequential(*[\n            *self.conv_block(3, 64),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            *self.conv_block(64, 128),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            *self.conv_block(128, 256),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.AvgPool2d(kernel_size=2, stride=1),\n        ])\n        self.classifier = nn.Linear(2304, 10)\n        \n    def conv_block(self, in_channels, out_channels):\n        return [\n            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        ]\n    \n    def forward(self, x):\n        x = self.conv_layers(x)\n        x = x.view(x.size(0), -1)\n        return self.classifier(x)","27843b76":"def train_model(mod, optimizer, trn_loader, nepochs=1, verbose=True):\n    crit = nn.CrossEntropyLoss()\n    loss_arr = []\n    for epoch in range(nepochs):  # loop over the dataset multiple times\n        \n        running_loss = 0.0\n        for i, data in enumerate(trn_loader, 0):\n            # get the inputs\n            inputs, labels = data\n            inputs, labels = Variable(inputs, requires_grad=False).cuda(), Variable(labels, requires_grad=False).cuda()\n\n            # zero the parameter gradients\n            optimizer.zero_grad()\n\n            # forward + backward + optimize\n            outputs = mod(inputs)\n            loss = crit(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            # print statistics\n            running_loss += loss.data[0]\n            if i % 100 == 99:    # print every 2000 mini-batches\n                if verbose:\n                    print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss \/ 100))\n                loss_arr.append((epoch*len(trn_loader) + i+1, running_loss \/ 100))\n                running_loss = 0.0\n    print('Finished Training')\n    return loss_arr","5cc5a587":"def ploterr(arr):\n    xs = [x[0] for x in arr]\n    ys = [x[1] for x in arr]\n    plt.figure(figsize=(15,10))\n    plt.subplot(1, 1, 1)\n    plt.plot(xs, ys, )","b24a0375":"net = CifarNet().cuda(0)\noptimizer = opt.SGD(net.parameters(), lr=0.001)\nsgd_res = train_model(net, optimizer, trainloader, verbose=False)\nploterr(sgd_res)","10a9ecdb":"Image(\"..\/input\/optimization-talk-images\/momentum.png\")","767aac61":"Image(\"..\/input\/optimization-talk-images\/ravines.gif\")","db2100ae":"Image(\"..\/input\/optimization-talk-images\/with_momentum.gif\")","12dd9852":"net = CifarNet().cuda(0)\noptimizer = opt.SGD(net.parameters(), lr=0.001, momentum=0.9)\nmom_res = train_model(net, optimizer, trainloader, verbose=False)\nploterr(mom_res)","b8ef80a9":"net = CifarNet().cuda(0)\noptimizer = opt.SGD(net.parameters(), lr=0.001, momentum=0.9, nesterov=True)\nnest_res = train_model(net, optimizer, trainloader, verbose=False)\nploterr(nest_res)","662b8e45":"Image(\"..\/input\/optimization-talk-images\/Nestorov.jpeg\")","de1fb3d8":"net = CifarNet().cuda(0)\noptimizer = opt.Adagrad(net.parameters(), lr=0.001)\nadag_res = train_model(net, optimizer, trainloader, verbose=False)\nploterr(adag_res)","11e30a1e":"net = CifarNet().cuda(0)\noptimizer = opt.RMSprop(net.parameters(), lr=0.001, alpha=0.9, momentum=0.9)\nrms_res = train_model(net, optimizer, trainloader, verbose=False)\nploterr(rms_res)","561cc16a":"net = CifarNet().cuda(0)\noptimizer = opt.Adam(net.parameters(), lr=0.001, betas=(0.9,0.999))\nadam_res = train_model(net, optimizer, trainloader, verbose=False)\nploterr(adam_res)","4f046928":"Image(\"..\/input\/optimization-talk-images\/contours_evaluation_optimizers.gif\")","2ab22af9":"### Momentum","a1895aae":"\\begin{equation*}\nMomentum Update\\\\\n w_t = w_{t-1} - \\gamma * v_{t-1} - \\lambda * \\nabla J(w) \\\\\n\\end{equation*}","ae18e61e":"### Adam - Adaptive Moment Estimation","8e6403cd":"\\begin{equation*}\nUpdate\\\\\n eg_t = \\sqrt{\\gamma * eg_{t-1}^2 + (1-\\gamma) * g_t^2} \\\\\n w_t = w_{t-1} - \\lambda \/ eg_t * \\nabla J(w) \\\\\n\\end{equation*}","ba2d3d9e":"**First Order Optimization Algorithms**\n1. Uses first derivative to find the direction of descent\n2. Gradient descent algorithms, Simplex Algorithm Procedures etc\n\n\n\\begin{equation*}\nUpdate Rule : w = w - \\lambda*\\nabla J(w)\n\\end{equation*}","a7f28c9f":"### GD","377139f9":"## Dataset Preparation","5dac59fa":"\\begin{equation*}\nUpdate\\\\\n m_t = \\beta_1 * m_{t-1} + (1-\\beta_1) * g_t \\\\\n v_t = \\beta_2 * v_{t-1} + (1-\\beta_2) * g_t^2 \\\\\n w_t = w_{t-1} - \\lambda\/\\sqrt{v_t+\\epsilon} * m_t\n\\end{equation*}","c8b14baa":"**Purpose of Optimization Algorithms**\n1. Optimization algorithms help us to minimize or maximize an objective\n2. In the ML scenario most of the times we are trying to optimize loss w.r.t to the parameters of the model\n3. Unless the solution is directly computable (Like in case of Linear Regression), some sort of optimization algorithm is used\n","c2165f5a":"\\begin{equation*}\nMomentum Update\\\\\n v_t = \\gamma * v_{t-1} + \\lambda * \\nabla J(w - \\gamma * v_{t-1}) \\\\\n w_t = w_{t-1} - v_t\n\\end{equation*}","e30c5ee1":"**Types of Optimization Algorithms**\n1. Generally optimization algorithms work by picking a random initial point and trying to follow a direction of decreasing loss function\n2. Two main types of optimization algorithms: First order optimization algorithms, Second order optimization algorithms\n3. Mainly differ in the order derivates used to find the direction of descent","baff65de":"1. A continuation of the AdaGrad. \n2. Instead of taking a normal sum of the squared gradients it calculates a decayed sum to handle the exploding denominator","1c3068ab":"**Challenges**\n1. Vanilla MBGD algos doesn't guarentee good convergence in practical\n2. Choosing a proper lr is generally difficult, too small ends up with slow convergence, too large ends up in erratic behaviour\n3. Learning rate schedulers are generally used to anneal the lr's as training progresses. Either pre-defined scheudes or schedules depending on the objective are used\n4. GD methods are susceptable to saddle points and local minima which makes the training harder","6e42692d":"**Second order optimization algorithms**\n1. Uses second order derivative while computing the direction of descent\n2. Faster convergence rates but very hard to compute since the Hessian needs to be computed at every update step\n3. Uses the curvature of the loss surface to figure out the direction. The hession gives the information of how the gradient changes in different direction\n4. Doesn't get struck in the slow convergence paths or saddle point like first order algorithms\n5. Newton, Quasi-Newton methods, L-BFGS etc\n","c18f000a":"### RMSProp","b7160d93":"1. Main problem with the gradient descent is that learning rate is fixed and not adapted to the learning process\n2. Main purpose of Adagrad is that it adapts learning rate to the parameters, performing smaller updates for frequently occurring values and larger updates for the sparse values\n3. Because of this, it will well suited for the sparse data\n4. The accumulated gradient in the denominator is one of the big problems where lr vanishes as learning goes on","f53d3d44":"### Nestorov Accelrated Gradient","807ac3be":"\\begin{equation*}\nUpdate Equations\\\\\n v_t = \\gamma * v_{t-1} + \\lambda * \\nabla J(w) \\\\\n w_t = w_{t-1} - v_t\n\\end{equation*}\n\n1. Accumulates past gradient in the gradient calculation step as momentum.\n2. Dampens oscillations in irrelevant directions and accelrates in the relavant directions\n3. Works as a moving average of gradient, dampening the variance due to mini-batched gd\n4. Helps in moving faster in the ravines which are prevalent near local-minima","c550c444":"## Optimization Algorithms","d230cfd9":"**Mini-Batch gradient descent**\n1. Updates parameters in batches\n2. Tries to incorporate best of both worlds. Reduces variance compared to SGD but allows to use highly optimized matrix operations to make the updates fatser\n3. Widely used variant for most of the Neural Network optimization","1e03a3a8":"**Batch Gradient Descent**\n1. Compute gradient for whole dataset at once\n2. Computationally intractable when data doesn't fit to memory and are generally slow since we are computing gradient for whole dataset at once\n3. Guarenteed to converge to global minima for convex losses, to a local minima for non convex functions","5622bff4":"\\begin{equation*}\nUpdate Rule : w = w - \\lambda*\\nabla J(w)\n\\end{equation*}","e5e1d137":"### AdaGrad","b3358bc8":"## Comparision","1ff21dd5":"## Optimizing Algorithms","2a114af3":"## Imports","361681aa":"**Stochastic Gradient Descent**\n1. In the opposite end to BGD, here update gradient for each example.\n2. This can be viewed as estimating the gradient on whole dataset using single data point\n3. Since we are using single point to estimate gradient, fluctuations in gradient directions w.r.t optimal directions is huge, so we end off-shooting in sub-optimal directions in between\n4. But starting with a small lr and gradually increasing it allows SGD to perform as good as BGD\n5. Sometimes the huge variance of SGD allows it to find better local minima than BGD","97ac12d5":"1. Improves upon momentum. Momentum uses old gradient even at the new step\n2. Nestorov uses the gradient at new point to update the gradient","ce7c54bc":"1. Similar to Adagrad and RMSProp adapts learning rates for parameters\n2. Along with what RMSProp does it also stores the decaying average of past gradients, it also keeps track of the momentum and uses momentum for updation of the parameters\n3. Beta1 is generally set to 0.9, beta 2 to 0.999 and epsilon to 1e-8","c51a0319":"\\begin{equation*}\nMomentum Update\\\\\n gi = \\sqrt{\\sum_{k=1}^t v_k^2 + \\epsilon} \\\\\n w_t = w_{t-1} - \\lambda \/ gi * \\nabla J(w) \\\\\n\\end{equation*}","75ea5d58":"##  Model Definition","7b651c00":"## References","5f0c1b63":"1. http:\/\/ruder.io\/optimizing-gradient-descent\/index.html#batchgradientdescent\n2. https:\/\/towardsdatascience.com\/types-of-optimization-algorithms-used-in-neural-networks-and-ways-to-optimize-gradient-95ae5d39529f\n3. https:\/\/towardsdatascience.com\/neural-network-optimization-algorithms-1a44c282f61d\n4. https:\/\/ipvs.informatik.uni-stuttgart.de\/mlr\/marc\/teaching\/13-Optimization\/04-secondOrderOpt.pdf\n5. http:\/\/cs231n.github.io\/optimization-1\/\n6. https:\/\/medium.com\/data-science-group-iitr\/loss-functions-and-optimization-algorithms-demystified-bb92daff331c"}}