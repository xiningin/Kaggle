{"cell_type":{"19fe4580":"code","3b22d824":"code","046900e5":"code","61428d6e":"code","628781f6":"code","7a83e9a1":"code","b5ff3d95":"code","7a04e348":"code","81af0749":"code","cacf7aed":"code","25e29f38":"code","c05e1775":"code","37f9ee2e":"code","b941b827":"code","0e810c67":"code","3241b5e0":"code","9d926713":"code","9b9e21b8":"code","fa7ebb68":"code","1a0082a8":"code","2e7ee652":"code","48730aaa":"code","3df2ee8b":"code","ff0a3298":"code","993ca693":"code","d232b1f3":"code","dcb2a704":"markdown","1a98a422":"markdown","49781e17":"markdown","4ff28cba":"markdown","4b9c8b57":"markdown","e91d4d8b":"markdown","8f3c5b2b":"markdown","27581621":"markdown","01b0c83d":"markdown","45617d05":"markdown","d878bb99":"markdown","49d40856":"markdown","9e3719b4":"markdown","3d26628c":"markdown"},"source":{"19fe4580":"from mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n","3b22d824":"#importing libaries for linear regression classifier\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport pandas.util.testing as tm\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing,svm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression","046900e5":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","61428d6e":"# Distribution graphs (histogram\/bar graph) of column data\ndef plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n    nunique = df.nunique()\n    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n    nRow, nCol = df.shape\n    columnNames = list(df)\n    nGraphRow = (nCol + nGraphPerRow - 1) \/ nGraphPerRow\n    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n    for i in range(min(nCol, nGraphShown)):\n        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n        columnDf = df.iloc[:, i]\n        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n            valueCounts = columnDf.value_counts()\n            valueCounts.plot.bar()\n        else:\n            columnDf.hist()\n        plt.ylabel('counts')\n        plt.xticks(rotation = 90)\n        plt.title(f'{columnNames[i]} (column {i})')\n    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n    plt.show()\n","628781f6":"# Correlation matrix\ndef plotCorrelationMatrix(df, graphWidth):\n    filename = df.dataframeName\n    df = df.dropna('columns') # drop columns with NaN\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    if df.shape[1] < 2:\n        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n        return\n    corr = df.corr()\n    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n    corrMat = plt.matshow(corr, fignum = 1)\n    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n    plt.yticks(range(len(corr.columns)), corr.columns)\n    plt.gca().xaxis.tick_bottom()\n    plt.colorbar(corrMat)\n    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n    plt.show()\n","7a83e9a1":"# Scatter and density plots\ndef plotScatterMatrix(df, plotSize, textSize):\n    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n    # Remove rows and columns that would lead to df being singular\n    df = df.dropna('columns')\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    columnNames = list(df)\n    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n        columnNames = columnNames[:10]\n    df = df[columnNames]\n    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n    corrs = df.corr().values\n    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n    plt.suptitle('Scatter and Density Plot')\n    plt.show()\n","b5ff3d95":"nRowsRead = 1000 # specify 'None' if want to read whole file\n# salary_data.csv may have more rows in reality, but we are only loading\/previewing the first 1000 rows\ndf = pd.read_csv('\/kaggle\/input\/salary_data.csv', delimiter=',', nrows = nRowsRead)\ndf.dataframeName = 'salary_data.csv'\nnRow, nCol = df.shape\nprint(f'There are {nRow} rows and {nCol} columns')\n#Read the dataset\ndf_set = df[['YearsExperience', 'Salary']]\n# Taking only the selected two attributes from the dataset \ndf_set.columns = ['YearsExperience', 'Salary']","7a04e348":"df.head(5)","81af0749":"plotPerColumnDistribution(df, 10, 5)","cacf7aed":"plotCorrelationMatrix(df, 8)","25e29f38":"plotScatterMatrix(df, 6, 15)","c05e1775":"#models for 50% train and 50% test\n\nX = np.array(df_set['YearsExperience']).reshape(-1, 1) \ny = np.array(df_set['Salary']).reshape(-1, 1) \n# Separating the data into independent and dependent variables \n# Converting each dataframe into a numpy array \n# since each dataframe contains only one column \n#df_set.dropna(inplace = True) \n# Dropping any rows with Nan values \nX_train, X_test, y_train, y_test = train_test_split(X, y,train_size = 0.5,test_size = 0.5,random_state=0) \n# Splitting the data into training and testing data \nregr = LinearRegression() \nregr.fit(X_train, y_train) \nX_train.shape","37f9ee2e":"#predicting the test result and visualizing the test result\ny_pred=regr.predict(X_test)\ny_pred\nplt.scatter(X_test,y_test,color='orange')\nplt.plot(X_test,regr.predict(X_test),color='black')\nplt.title('Salary vs YearsExperience (Test Data 50%)')\nplt.xlabel('YearsExperience')\nplt.ylabel('Salary')\nplt.show()","b941b827":"#predicting the train result and visualizing the results\nX_pred = regr.predict(X_train)\nX_pred\nplt.scatter(X_train,y_train,color='brown')\nplt.plot(X_train,regr.predict(X_train),color='black')\nplt.title('Salary vs YearsExperience(Train data 50%)')\nplt.xlabel('YearsExperience')\nplt.ylabel('Salary')\nplt.show()","0e810c67":"#Calculating the function for Mean squared error and Root mean squared error\n#MSE\nMSE = np.square(np.subtract(y_test,y_pred)).mean()\nprint(\"Mean squared error of dataset is \",MSE)\n\n#RMSE\nRMSE = np.sqrt(MSE)\nprint(\"Root Mean squared error of dataset is \",RMSE)\n","3241b5e0":"#crosschecking the MSE\/RMSE results with sklearn metrics\nfrom sklearn import metrics\nprint('Mean Squared Error of the Model: ',metrics.mean_squared_error(y_test,y_pred))\nprint('Root Mean Squared Error of the Model: ',np.sqrt(metrics.mean_squared_error(y_test,y_pred)))","9d926713":"#models for 70% train and 30% test\nX = np.array(df_set['YearsExperience']).reshape(-1, 1) \ny = np.array(df_set['Salary']).reshape(-1, 1) \n# Converting each dataframe into a numpy array \n# since each dataframe contains only one column \n#df_set.dropna(inplace = True) \nX_train, X_test, y_train, y_test = train_test_split(X, y,train_size = 0.7,test_size = 0.3,random_state=0) \n# Splitting the data into training and testing data \nregr = LinearRegression() \nregr.fit(X_train, y_train) \nX_train.shape\ny_test.shape","9b9e21b8":"#predicting the test result and visualizing the test result on the regression line \ny_pred=regr.predict(X_test)\ny_pred\nplt.scatter(X_test,y_test,color='orange')\nplt.plot(X_test,regr.predict(X_test),color='black')\nplt.title('Salary vs YearsExperience (Test Data 30%)')\nplt.xlabel('YearsExperience')\nplt.ylabel('Salary')\nplt.show()","fa7ebb68":"#predicting the train result and visualizing the results on the regression line\nX_pred = regr.predict(X_train)\nX_pred\nplt.scatter(X_train,y_train,color='brown')\nplt.plot(X_train,regr.predict(X_train),color='black')\nplt.title('Salary vs YearsExperience(Train data 70%)')\nplt.xlabel('YearsExperience')\nplt.ylabel('Salary')\nplt.show()","1a0082a8":"#Calculating the function for Mean squared error and Root mean squared error\n#MSE\nMSE = np.square(np.subtract(y_test,y_pred)).mean()\nprint(\"Mean squared error of dataset is \",MSE)\n\n#RMSE\nRMSE = np.sqrt(MSE)\nprint(\"Root Mean squared error of dataset is \",RMSE)\n","2e7ee652":"#crosschecking the MSE\/RMSE results with sklearn metrics\nfrom sklearn import metrics\nprint('Mean Squared Error of the Model: ',metrics.mean_squared_error(y_test,y_pred))\nprint('Root Mean Squared Error of the Model: ',np.sqrt(metrics.mean_squared_error(y_test,y_pred)))","48730aaa":"#models for 80% train and 20% test\nX = np.array(df_set['YearsExperience']).reshape(-1, 1) \ny = np.array(df_set['Salary']).reshape(-1, 1) \n# Converting each dataframe into a numpy array \n# since each dataframe contains only one column \n#df_set.dropna(inplace = True) \nX_train, X_test, y_train, y_test = train_test_split(X, y,train_size = 0.8,test_size = 0.2,random_state=0) \n# Splitting the data into training and testing data \nregr = LinearRegression() \nregr.fit(X_train, y_train) \nX_train.shape \ny_test.shape","3df2ee8b":"#predicting the test result and visualizing the test result on the regression line\ny_pred=regr.predict(X_test)\ny_pred\nplt.scatter(X_test,y_test,color='orange')\nplt.plot(X_test,regr.predict(X_test),color='black')\nplt.title('Salary vs YearsExperience (Test Data 20%)')\nplt.xlabel('YearsExperience')\nplt.ylabel('Salary')\nplt.show()","ff0a3298":"#predicting the train result and visualizing the results on the regression line\nX_pred = regr.predict(X_train)\nX_pred\nplt.scatter(X_train,y_train,color='brown')\nplt.plot(X_train,regr.predict(X_train),color='black')\nplt.title('Salary vs YearsExperience(Train data 80%)')\nplt.xlabel('YearsExperience')\nplt.ylabel('Salary')\nplt.show()","993ca693":"#Calculating the function for Mean squared error and Root mean squared error\n#MSE\nMSE = np.square(np.subtract(y_test,y_pred)).mean()\nprint(\"Mean squared error of dataset is \",MSE)\n\n#RMSE\nRMSE = np.sqrt(MSE)\nprint(\"Root Mean squared error of dataset is \",RMSE)\n","d232b1f3":"#crosschecking the MSE\/RMSE results with sklearn metrics\nfrom sklearn import metrics\nprint('Mean Squared Error of the Model: ',metrics.mean_squared_error(y_test,y_pred))\nprint('Root Mean Squared Error of the Model: ',np.sqrt(metrics.mean_squared_error(y_test,y_pred)))","dcb2a704":"Scatter and density plots:","1a98a422":"Now you're ready to read in the data and use the plotting functions to visualize the data.","49781e17":"### Let's check 1st file: \/kaggle\/input\/salary_data.csv","4ff28cba":"1)Calculating the datasets for 50% train and 50% test\n#MSE and #RMSE","4b9c8b57":"## Introduction\nGreetings from the Kaggle bot! This is an automatically-generated kernel with starter code demonstrating how to read in the data and begin exploring. If you're inspired to dig deeper, click the blue \"Fork Notebook\" button at the top of this kernel to begin editing.","e91d4d8b":"## Exploratory Analysis\nTo begin this exploratory analysis, first import libraries and define functions for plotting the data using `matplotlib`. Depending on the data, not all plots will be made. (Hey, I'm just a simple kerneling bot, not a Kaggle Competitions Grandmaster!)","8f3c5b2b":"## Conclusion\nThis concludes your starter analysis! To go forward from here, click the blue \"Fork Notebook\" button at the top of this kernel. This will create a copy of the code and environment for you to edit. Delete, modify, and add code as you please. Happy Kaggling!","27581621":"Let's take a quick look at what the data looks like:","01b0c83d":"There is 1 csv file in the current version of the dataset:\n","45617d05":"The next hidden code cells define functions for plotting data. Click on the \"Code\" button in the published kernel to reveal the hidden code.","d878bb99":"2)Calculating the datasets for 70% train and 30% test\n#MSE and #RMSE","49d40856":"Correlation matrix:","9e3719b4":"Calculating the datasets for 80% train and 20% test\n#MSE and #RMSE","3d26628c":"Distribution graphs (histogram\/bar graph) of sampled columns:"}}