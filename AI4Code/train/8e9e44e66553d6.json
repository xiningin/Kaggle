{"cell_type":{"60b11525":"code","7b48f35a":"code","8fc5c9e3":"code","9dc15271":"code","ab88363d":"code","9ad88e23":"code","c02be262":"code","ec6754fd":"code","09e13925":"code","6a4f8983":"code","b4943453":"code","191819bb":"code","91a35e37":"code","f6f145c4":"code","60a3e5ba":"markdown","63586427":"markdown","9c74f3cf":"markdown","5f833395":"markdown","dfbebe8d":"markdown","028bb4fa":"markdown","775c35ea":"markdown","94ddc57e":"markdown","7bc65321":"markdown","071fa45a":"markdown","da5675ad":"markdown","6da2f778":"markdown"},"source":{"60b11525":"import numpy as np\nimport pandas as pd\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport math\n\nfrom keras.models import Model, Sequential\nfrom keras.layers import Input, Dense, Dropout, BatchNormalization\nfrom keras.optimizers import Adadelta\nfrom keras import regularizers\n\nfrom sklearn.preprocessing import PolynomialFeatures","7b48f35a":"train_df = pd.read_csv('..\/input\/train.csv', nrows = 10 ** 6)\ntest_df = pd.read_csv('..\/input\/test.csv')","8fc5c9e3":"datasets = [train_df, test_df]\n\nfor df in datasets:\n    missing_values = df.isnull().sum().to_frame().sort_values(0, ascending = False)\n    display(missing_values.head())","9dc15271":"print(\"Train before cleaning:\")\ndisplay(train_df.describe())\n\ntrain_df = train_df.dropna(how = 'any', axis = 'rows')\ntrain_df = train_df[(train_df.pickup_longitude > -75.0) & (train_df.pickup_longitude < -73.0)]\ntrain_df = train_df[(train_df.pickup_latitude > 40.0) & (train_df.pickup_latitude < 42.0)]\ntrain_df = train_df[(train_df.dropoff_longitude > -75.0) & (train_df.dropoff_longitude < -73.0)]\ntrain_df = train_df[(train_df.dropoff_latitude > 40.0) & (train_df.dropoff_latitude < 42.0)]\ntrain_df = train_df[(train_df.passenger_count > 0.0) & (train_df.passenger_count <= 6.0)]\n\n\nprint(\"Train after cleaning:\")\ndisplay(train_df.describe())\n\nprint(\"Test for comparison:\")\ndisplay(test_df.describe())","ab88363d":"def calc_haversine(df):\n    df['abs_diff_longitude'] = (df.dropoff_longitude - df.pickup_longitude).abs()\n    df['abs_diff_latitude'] = (df.dropoff_latitude - df.pickup_latitude).abs()\n\n    df['dlat'] = np.radians(df.dropoff_latitude - df.pickup_latitude)\n    df['dlon'] = np.radians(df.dropoff_longitude - df.pickup_longitude)\n    df['haversine_a'] = np.sin(df.dlat\/2) * np.sin(df.dlat\/2) + np.cos(np.radians(df.pickup_latitude)) \\\n            * np.cos(np.radians(df.dropoff_latitude)) * np.sin(df.dlon\/2) * np.sin(df.dlon\/2)\n    df['haversine'] = 6371 * 2 * np.arctan2(np.sqrt(df.haversine_a), np.sqrt(1-df.haversine_a))\n\n    return df.drop(columns=['pickup_datetime'])\n\ntrain_df = calc_haversine(train_df)\ntest_df = calc_haversine(test_df)","9ad88e23":"corr = train_df.corr()\nf, ax = plt.subplots(figsize=(10, 10)) \ncmap = sns.diverging_palette(220, 10, as_cmap=True) \nsns.heatmap(corr, cmap=cmap, vmax=1.0, square=True, linewidths=.3, cbar_kws={\"shrink\": .5}, ax=ax) \nplt.show()","c02be262":"# filter interesting columns and label\ntrain_y = np.array(train_df['fare_amount'])\ntrain_X = train_df.drop(columns=['fare_amount','key'])\n\nprint(\"Shape for X:\")\nprint(train_X.shape)\nprint(\"Shape for Y:\")\nprint(train_y.shape)\n\ntest_X = test_df.drop(columns=['key'])\nprint(\"Shape for test X:\")\nprint(test_X.shape)","ec6754fd":"def run_model(X, Y, dnn_layers_size, dropout_value, batch_size, epochs):\n    \n    input_size = X.shape[1]\n    \n    model = Sequential()\n    \n    for idx, l in enumerate(dnn_layers_size):\n        model.add(Dense(l, input_dim=input_size,\n                           kernel_initializer='normal',\n                           activation='selu'))\n        model.add(Dropout(dropout_value))\n        input_size = l\n        \n    model.add(Dense(1, kernel_initializer='normal'))\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    \n    train_history = model.fit([X], Y, epochs=epochs, batch_size=batch_size, validation_split=0.1, shuffle=True)\n    \n    return train_history, model\n\ndef build_layers(layers, n_features):\n    if len(layers) == 0:\n        n_features = int(n_features * 2.5)\n    else:\n        n_features = int(math.sqrt(n_features))\n        \n    if n_features < 3:\n        return layers\n    else:\n        layers.append(n_features)\n        return build_layers(layers, n_features)\n    \ndef plot_build(train_history):    \n    \n    # plotting train_history\n    plt.figure(0)\n    axes = plt.gca()\n    axes.set_ylim([0,90])\n    plt.plot(train_history.history['loss'],'g')\n    plt.plot(train_history.history['val_loss'],'b')\n    plt.rcParams['figure.figsize'] = (8, 6) \n    plt.xlabel(\"Num of Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Training Loss vs Validation Loss\")\n    plt.grid()\n    plt.legend(['train','validation'])\n\n    plt.show()","09e13925":"layers = build_layers([],train_X.shape[1])\nprint('Layers:', layers)\nprint('-' * 15)\ntrain_history, model = run_model(train_X, train_y, layers, 0.2, batch_size = 32, epochs = 100)","6a4f8983":"plot_build(train_history)","b4943453":"# Generating DNN submission\npred_y = model.predict([test_X])\ntest_df['pred'] = pred_y\n\nsubmission = pd.DataFrame(\n    {'key': test_df.key, 'fare_amount': test_df.pred},\n    columns = ['key', 'fare_amount'])\nsubmission.to_csv('submission_dnn.csv', index = False)\n\nprint(os.listdir('.'))","191819bb":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error as msq\nimport xgboost as xgb\n\nstdscaler = StandardScaler()\nxgb_train_X = stdscaler.fit_transform(train_X)\nxgb_test_X  = stdscaler.fit_transform(test_X)\n\nx_train, x_test, y_train, y_test = train_test_split(xgb_train_X, train_y, random_state=70, test_size=0.2)\n\ndef XGBmodel(x_train,x_test,y_train,y_test):\n    matrix_train = xgb.DMatrix(x_train, label=y_train)\n    matrix_test = xgb.DMatrix(x_test, label=y_test)\n    model=xgb.train(params={'objective':'reg:linear','eval_metric':'rmse'},\n                    dtrain=matrix_train,\n                    num_boost_round=100, \n                    early_stopping_rounds=10,\n                    evals=[(matrix_test,'test')])\n    return model\n\nxgb_model = XGBmodel(x_train,x_test,y_train,y_test)\nxgb_pred = xgb_model.predict(xgb.DMatrix(xgb_test_X), ntree_limit = xgb_model.best_ntree_limit)\n","91a35e37":"# Generating XGB submission\ntest_df['pred_xgb'] = xgb_pred\n\nsubmission = pd.DataFrame(\n    {'key': test_df.key, 'fare_amount': test_df.pred_xgb},\n    columns = ['key', 'fare_amount'])\nsubmission.to_csv('submission_xgb.csv', index = False)\n\nprint(os.listdir('.'))","f6f145c4":"test_df['ensemble'] = (test_df.pred + test_df.pred_xgb) \/ 2.0\n\nsubmission = pd.DataFrame(\n    {'key': test_df.key, 'fare_amount': test_df.ensemble},\n    columns = ['key', 'fare_amount'])\nsubmission.to_csv('submission_ensemble.csv', index = False)\n\nprint(os.listdir('.'))\n","60a3e5ba":"## Adventures with XGB","63586427":"## Correlation Visualization","9c74f3cf":"## Show training for X with no poly-transformation","5f833395":"## Combine previous results (DNN and XGB) in a simplistic average ensemble scheme","dfbebe8d":"## Checking for missing values","028bb4fa":"## Loading data","775c35ea":"## Filtering values for train and test datasets have same distribution","94ddc57e":"Special thanks to Dan Becker, Will Cukierski, and Julia Elliot for reviewing this Kernel and providing suggestions!","7bc65321":"## Build DNN model on Keras","071fa45a":"## Generating submissions for both model","da5675ad":"## Prepare X and Y for magic","6da2f778":"## Calculate haversine"}}