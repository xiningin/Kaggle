{"cell_type":{"b10b4525":"code","d7a07e93":"code","c0ecc031":"code","34018375":"code","d6d47468":"code","0504749c":"code","5b9d9ef2":"code","c66640af":"code","0770038b":"code","c49ee9f8":"code","f59a5812":"code","bc95a023":"code","e78dde0a":"code","185ffca8":"code","44174265":"code","a86ec5a5":"code","b39435b2":"code","06024849":"code","4af44628":"code","52abf035":"code","48e65830":"code","aeb940bc":"code","f42e4ae3":"code","e1b64dc0":"code","f2ad4305":"code","369f5ef6":"code","e7b16389":"code","bab611ba":"code","c61b93ae":"code","41c6831c":"code","9bbe851c":"code","c04dc701":"code","df5f1771":"code","c47f4f2a":"code","782bc53b":"code","fb15e0f7":"code","87ea0485":"code","e8d99736":"code","7df8e25d":"code","36d5f02c":"code","70dfc416":"code","3723e730":"code","d41d8e9b":"code","8f660cc4":"code","66ef03e7":"code","3c1f5b9f":"code","1e4ab403":"code","256e8456":"code","37022fbe":"code","b72fe2c3":"code","dc4ec3e0":"code","492356fe":"code","60b034bf":"code","e0aa72ee":"code","a0361e9a":"code","8c41a8f9":"code","3a70da85":"code","a3ee8bd1":"code","07899921":"code","fe17d13c":"code","bd89999b":"code","dd068834":"code","48b65cd4":"code","9e9fcd96":"code","f31112e4":"code","a419dc9b":"code","15ac858e":"code","ce9746bc":"code","bce78452":"code","abae5ae9":"code","d531d990":"code","50e2e2be":"code","016774a4":"code","1a48cc52":"code","ff6e198f":"code","915c0cf9":"code","e3782863":"code","2124c0be":"code","7239fcad":"code","7d7b7b71":"code","976db78c":"code","c74f5ff4":"code","0e0d1b6e":"code","709b34dc":"code","3126f03c":"code","d561d893":"code","0123e653":"code","bcc5f7f0":"code","f3b9ac1f":"code","a64dbc4b":"code","c3de4bd5":"code","14d11a35":"code","92b5bc32":"code","6fbb9d95":"code","d6da6544":"code","ce08e626":"markdown","0a2a81e8":"markdown","374dbea7":"markdown","88c9abdd":"markdown","1872faab":"markdown","aada1fde":"markdown","d9905461":"markdown","3637336d":"markdown","a8560bb9":"markdown","bf0dd9b5":"markdown","630f5547":"markdown","afbce526":"markdown","436613b8":"markdown","0f77bdb7":"markdown","3ad830e9":"markdown","be1c8c59":"markdown","66fc2ba1":"markdown","6fbdb091":"markdown","6307d69a":"markdown","f4d6ad65":"markdown","68670ed8":"markdown","b1bb1e82":"markdown","f9746d38":"markdown","e394b906":"markdown"},"source":{"b10b4525":"# Suppressing Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","d7a07e93":"# Importing Pandas and NumPy\nimport pandas as pd, numpy as np","c0ecc031":"# Importing Pandas and NumPy\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport datetime as dt\n\nimport sklearn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\nfrom scipy.cluster.hierarchy import linkage\nfrom scipy.cluster.hierarchy import dendrogram\nfrom scipy.cluster.hierarchy import cut_tree","34018375":"# Importing all datasets\ndrug_classification = pd.read_csv(\"\/kaggle\/input\/drug-classification\/drug200.csv\")\ndrug_classification.head(2)","d6d47468":"drug_classification.groupby(by = \"Drug\").median()","0504749c":"labels = drug_classification['Drug'].astype('category').cat.categories.tolist()\ncounts = drug_classification['Drug'].value_counts()\nsizes = [counts[var_cat] for var_cat in labels]\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=True) #autopct is show the % on plot\nax1.axis('equal')\nplt.show()","5b9d9ef2":"drug_classification.shape","c66640af":"drug_classification.isnull().sum()","0770038b":"# missing values\nround(100*(drug_classification.isnull().sum())\/len(drug_classification), 2)","c49ee9f8":"drug_classification.dtypes","f59a5812":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","bc95a023":"import seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use(\"dark_background\")","e78dde0a":"## Lead Number Analysis per Country \nplt.figure(figsize= [15,4])\nplt.xticks(rotation = 90)\nsns.histplot(drug_classification[\"Drug\"], stat=\"frequency\",color=\"red\")\nplt.show()","185ffca8":"## TotalVisits Analysis \nplt.figure(figsize= [10,4])\nplt.xticks(rotation = 90)\nsns.histplot(drug_classification[\"Cholesterol\"], stat=\"count\",color=\"red\")\nplt.show()","44174265":"# Page Views Per Visit\nplt.figure(figsize= [15,4])\nsns.boxplot(drug_classification[\"Na_to_K\"])","a86ec5a5":"# Inter Quartile Range\nQ1 = drug_classification[\"Na_to_K\"].quantile(0.05)\nQ3 = drug_classification[\"Na_to_K\"].quantile(0.95)\nIQR = Q3 - Q1\nprint(\"The interQuartile Range :\",IQR)","b39435b2":"drug_classification.columns","06024849":"# Checking for outliers in the continuous variables\nnum_grouped_df = drug_classification.groupby(by = \"Drug\").sum()","4af44628":"# Checking outliers at 25%, 50%, 75%, 90%, 95% and 99%\nnum_grouped_df.describe(percentiles=[.25, .5, .75, .90, .95, .99])","52abf035":"# import preprocessing from sklearn\nfrom sklearn import preprocessing\n\n# 1. INSTANTIATE\n# encode labels with value between 0 and n_classes-1.\nle = preprocessing.LabelEncoder()\n\n\n# 2\/3. FIT AND TRANSFORM\n# use df.apply() to apply le.fit_transform to all columns\ndrug_classification_2 = drug_classification.apply(le.fit_transform)\ndrug_classification_2.head(5)","48e65830":"# Checking for outliers in the continuous variables\nnum_grouped_df = drug_classification_2.groupby(by = \"Drug\").sum()","aeb940bc":"# Checking outliers at 25%, 50%, 75%, 90%, 95% and 99%\nnum_grouped_df.describe(percentiles=[.25, .5, .75, .90, .95, .99])","f42e4ae3":"# removing (statistical) outliers\nQ1 = drug_classification_2[\"Age\"].quantile(0.05)\nQ3 = drug_classification_2[\"Age\"].quantile(0.95)\nIQR = Q3 - Q1\ndrug_classification_2 = drug_classification_2[(drug_classification_2[\"Age\"] >= Q1 - 1.5*IQR) & (drug_classification_2[\"Age\"] <= Q3 + 1.5*IQR)]\n\n# outlier treatment \nQ1 = drug_classification_2[\"Sex\"].quantile(0.05)\nQ3 = drug_classification_2[\"Sex\"].quantile(0.95)\nIQR = Q3 - Q1\ndrug_classification_2 = drug_classification_2[(drug_classification_2[\"Sex\"] >= Q1 - 1.5*IQR) & (drug_classification_2[\"Sex\"] <= Q3 + 1.5*IQR)]\n\n# outlier treatment\nQ1 = drug_classification_2[\"BP\"].quantile(0.05)\nQ3 = drug_classification_2[\"BP\"].quantile(0.95)\nIQR = Q3 - Q1\ndrug_classification_2 = drug_classification_2[(drug_classification_2[\"BP\"] >= Q1 - 1.5*IQR) & (drug_classification_2[\"BP\"] <= Q3 + 1.5*IQR)]\n\n# outlier treatment\nQ1 = drug_classification_2[\"Cholesterol\"].quantile(0.05)\nQ3 = drug_classification_2[\"Cholesterol\"].quantile(0.95)\nIQR = Q3 - Q1\ndrug_classification_2 = drug_classification_2[(drug_classification_2[\"Cholesterol\"] >= Q1 - 1.5*IQR) & (drug_classification_2[\"Cholesterol\"] <= Q3 + 1.5*IQR)]\n\n# outlier treatment\nQ1 = drug_classification_2[\"Na_to_K\"].quantile(0.05)\nQ3 = drug_classification_2[\"Na_to_K\"].quantile(0.95)\nIQR = Q3 - Q1\ndrug_classification_2 = drug_classification_2[(drug_classification_2[\"Na_to_K\"] >= Q1 - 1.5*IQR) & (drug_classification_2[\"Na_to_K\"] <= Q3 + 1.5*IQR)]\n\n","e1b64dc0":"from sklearn.model_selection import train_test_split","f2ad4305":"# Putting feature variable to X\nX = drug_classification_2.drop(['Drug'], axis=1)\n\nX.head()","369f5ef6":"# Putting response variable to y\ny = drug_classification_2['Drug']\n\ny.head()","e7b16389":"# Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)","bab611ba":"X_train.shape","c61b93ae":"y_train.shape","41c6831c":"X_test.shape","9bbe851c":"X_test.head(5)","c04dc701":"id = X_test.index","df5f1771":"X_test.columns","c47f4f2a":"y_test.shape","782bc53b":"y_test.value_counts()","fb15e0f7":"from sklearn.preprocessing import StandardScaler","87ea0485":"X_train.columns","e8d99736":"scaler = StandardScaler()\n\nX_train = scaler.fit_transform(X_train)\n","7df8e25d":"X_test = scaler.transform(X_test)","36d5f02c":"## Pipelines Creation\n## 1. Data Preprocessing by using Standard Scaler\n## 2. Reduce Dimension using PCA\n## 3. Apply  Classifier","70dfc416":"from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier","3723e730":"pipeline_lr=Pipeline([('scalar1',StandardScaler()),\n                     ('pca1',PCA(n_components=2)),\n                     ('lr_classifier',LogisticRegression(random_state=0))])","d41d8e9b":"pipeline_dt=Pipeline([('scalar2',StandardScaler()),\n                     ('pca2',PCA(n_components=2)),\n                     ('dt_classifier',DecisionTreeClassifier())])","8f660cc4":"pipeline_randomforest=Pipeline([('scalar3',StandardScaler()),\n                     ('pca3',PCA(n_components=2)),\n                     ('rf_classifier',RandomForestClassifier())])","66ef03e7":"pipeline_gradientboosting=Pipeline([('scalar4',StandardScaler()),\n                     ('pca4',PCA(n_components=2)),\n                     ('gb_classifier',GradientBoostingClassifier())])","3c1f5b9f":"## LEts make the list of pipelines\npipelines = [pipeline_lr, pipeline_dt, pipeline_randomforest, pipeline_gradientboosting]","1e4ab403":"best_accuracy=0.0\nbest_classifier=0\nbest_pipeline=\"\"","256e8456":"# Dictionary of pipelines and classifier types for ease of reference\npipe_dict = {0: 'Logistic Regression', 1: 'Decision Tree', 2: 'RandomForest', 3:'GradientBoosting'}\n\n# Fit the pipelines\nfor pipe in pipelines:\n\tpipe.fit(X_train, y_train)","37022fbe":"for i,model in enumerate(pipelines):\n    print(\"{} Test Accuracy: {}\".format(pipe_dict[i],model.score(X_test,y_test)))","b72fe2c3":"for i,model in enumerate(pipelines):\n    if model.score(X_test,y_test)>best_accuracy:\n        best_accuracy=model.score(X_test,y_test)\n        best_pipeline=model\n        best_classifier=i\nprint('Classifier with best accuracy:{}'.format(pipe_dict[best_classifier]))","dc4ec3e0":"from sklearn.linear_model import LogisticRegression \nfrom sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.model_selection import train_test_split\nimport warnings","492356fe":"\n# Building a classification model using one vs rest method\n\n# Fitting the model with training data\nimport statsmodels.api as sm\nfrom sklearn.linear_model import LogisticRegression\nLR = LogisticRegression()\noneVsrest = OneVsRestClassifier(LR)\noneVsrest.fit(X_train, y_train)","60b034bf":"# Making a prediction on the test set\nprediction_oneVsRest = oneVsrest.predict(X_test)\n   \n# Evaluating the model\n\nprint(f\"Test Set Accuracy: {accuracy_score(y_test, prediction_oneVsRest) * 100} %\\n\\n\")\nprint(f\"Classification Report: \\n\\n{classification_report(y_test, prediction_oneVsRest)}\")","e0aa72ee":"# Classes for which individual models are created\nprint(oneVsrest.classes_) \n\n#Coefficient matrix for all the models created\nprint(oneVsrest.coef_.shape)\nprint(\"\\n Intercept Values\")\nprint(oneVsrest.intercept_)\nprint(\"\\n Coefficient Values\")\nCoeff_array = oneVsrest.coef_\nprint(Coeff_array)","a0361e9a":"prediction_oneVsRest","8c41a8f9":"print(f\"Classification Report: \\n\\n{classification_report(y_test, prediction_oneVsRest)}\")","3a70da85":"### Manual Hyperparameter Tuning\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nmodel=LogisticRegression(  penalty='l2',\n    dual=False,\n    tol=0.0001,\n    C=1.0,\n    fit_intercept=True,\n    intercept_scaling=1,\n    class_weight=None,\n    random_state=None,\n    solver='lbfgs',\n    max_iter=100,\n    multi_class='auto',\n    verbose=0,\n    warm_start=False,\n    n_jobs=None,\n    l1_ratio=None,).fit(X_train,y_train)\npredictions=model.predict(X_test)\nprint(confusion_matrix(y_test,predictions))\nprint(accuracy_score(y_test,predictions))\nprint(classification_report(y_test,predictions))","a3ee8bd1":"import numpy as np\nfrom sklearn.model_selection import RandomizedSearchCV\nsolver = ['newton-cg', 'lbfgs', 'liblinear']\npenalty = ['none', 'l1', 'l2', 'elasticnet']\nC = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100]\n\nrandom_grid = {'solver': solver,\n               'penalty': penalty,\n               'C': C,\n               }\nprint(random_grid)","07899921":"rf=LogisticRegression()\nrf_randomcv=RandomizedSearchCV(estimator=rf, param_distributions=random_grid, n_iter=500, scoring='accuracy', n_jobs=-1, random_state=1)\n### fit the randomized model\nrf_randomcv.fit(X_train,y_train)","fe17d13c":"rf_randomcv.best_params_","bd89999b":"rf_randomcv","dd068834":"best_random_grid=rf_randomcv.best_estimator_","48b65cd4":"from sklearn.metrics import accuracy_score\ny_pred=best_random_grid.predict(X_test)\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Accuracy Score {}\".format(accuracy_score(y_test,y_pred)))\nprint(\"Classification report: {}\".format(classification_report(y_test,y_pred)))","9e9fcd96":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'solver': [rf_randomcv.best_params_['solver']],\n    'penalty': [rf_randomcv.best_params_['penalty']],\n    'C': [rf_randomcv.best_params_['C'], \n          rf_randomcv.best_params_['C']+2, \n          rf_randomcv.best_params_['C'] + 4]\n}\n\nprint(param_grid)","f31112e4":"#### Fit the grid_search to the data\nrf=LogisticRegression()\ngrid_search=GridSearchCV(estimator=rf,param_grid=param_grid,n_jobs=-1,verbose=2)\ngrid_search.fit(X_train,y_train)","a419dc9b":"grid_search.best_estimator_","15ac858e":"best_grid=grid_search.best_estimator_","ce9746bc":"best_grid","bce78452":"y_pred=best_grid.predict(X_test)\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Accuracy Score {}\".format(accuracy_score(y_test,y_pred)))\n","abae5ae9":"print(\"Classification Report:\\n {} \".format(classification_report(y_test,y_pred)))","d531d990":"print(\"Accuracy Of :\", \"`93.33 %` achieved\")","50e2e2be":"y_pred","016774a4":"X_test = pd.DataFrame(X_test)","1a48cc52":"X_test = X_test.rename(columns = {\n                                 0:\"Age\",\n                                 1:\"Sex\",\n                                 2:\"BP\",\n                                 3:\"Cholesterol\",\n                                 4:\"Na_to_K\",\n                                 5:\"Drug\"\n})","ff6e198f":"X_test.head()","915c0cf9":"drug_classification = drug_classification.reset_index()","e3782863":"drug_classification.head(4)","2124c0be":"id = pd.DataFrame(id)","7239fcad":"drug_classification2 = drug_classification.join(id,how=\"inner\")","7d7b7b71":"drug_classification2.head(3)","976db78c":"drug_classification2 = drug_classification2.drop(columns = 0)","c74f5ff4":"y_pred = pd.DataFrame(y_pred)","0e0d1b6e":"y_pred = y_pred.reset_index()","709b34dc":"y_pred.head(3)","3126f03c":"drug_classification2 = drug_classification2.join(y_pred,lsuffix='xindex',rsuffix='yindex')","d561d893":"drug_classification2.head(3)","0123e653":"drug_classification2 = drug_classification2.drop(columns = [\"indexxindex\",\"indexyindex\"])","bcc5f7f0":"drug_classification2 = drug_classification2.rename(columns ={0: \"Pred\"} )","f3b9ac1f":"drug_classification2.head(6)","a64dbc4b":"labels = drug_classification2['Pred'].astype('category').cat.categories.tolist()\ncounts = drug_classification2['Pred'].value_counts()\nsizes = [counts[var_cat] for var_cat in labels]\nfig1, ax1 = plt.subplots()\nplt.title(\"Categorical Pedictions\")\nax1.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=True) #autopct is show the % on plot\nax1.axis('equal')\nplt.show()\n","c3de4bd5":"# List of variables to map\n\nvarlist =  ['Pred']\n\n# Defining the map function\ndef binary_map(x):\n    return x.map({0: \"Drug Y\", 1: \"drugC\", 2:\"drugX\", 3:\"drugA\", 4:\"drugB\"})\n\n# Applying the function to the housing list\ndrug_classification2[varlist] = drug_classification2[varlist].apply(binary_map)","14d11a35":"drug_classification2.head(5)","92b5bc32":"labels = drug_classification2['Pred'].astype('category').cat.categories.tolist()\ncounts = drug_classification2['Pred'].value_counts()\nsizes = [counts[var_cat] for var_cat in labels]\nfig1, ax1 = plt.subplots()\nplt.title(\"Categorical PedictionsOn Test DataSet\")\nax1.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=True) #autopct is show the % on plot\nax1.axis('equal')\nplt.show()","6fbb9d95":"labels = drug_classification2['Drug'].astype('category').cat.categories.tolist()\ncounts = drug_classification2['Drug'].value_counts()\nsizes = [counts[var_cat] for var_cat in labels]\nfig1, ax1 = plt.subplots()\nplt.title(\"Categorical Given On Test Dataset\")\nax1.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=True) #autopct is show the % on plot\nax1.axis('equal')\nplt.show()","d6da6544":"drug_classification2.to_csv(r'\/kaggle\/working\/submission.csv', index=False)","ce08e626":"### Dimensions Of The Datasets","0a2a81e8":"### Data Analysis - Univariate And Bivariate Analysis","374dbea7":"#### GridSearch CV","88c9abdd":"### Removing the `Outliers` ","1872faab":"<b> Accuracy : <\/b> Accuracy is the most intuitive performance measure and it is simply a ratio of correctly predicted observation to the total observations.The formula is given as: <br>\n<b> *Accuracy = True Positives + True Negatives\/True Positives+False Positives+False Negatives+True Positives* <\/b> <br> <br>\n<b> Precision : <\/b> The quality of being exact and refers to how close two or more measurements are to each other, regardless of whether those measurements are accurate or not. The formula is : <br>\n<b> *Precision = True Positives \/ (True Positives + False Positives)* <\/b> <br> <br>\n<b> Recall : <\/b> It is calculated as the number of true positives divided by the total number of true positives and false negatives. The result is a value between 0.0 for no recall and 1.0 for full or perfect recall. The formula is : <br>\n<b> *Recall = True Positives \/ (True Positives + False Negatives)* <\/b> <br> <br>\n<b> F1 score : <\/b> F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account.The highest possible value of an F-score is 1.0, indicating perfect precision and recall, and the lowest possible value is 0, if either the precision or the recall is zero. The formula is : <br>\n<b> *F1 score = 2\\*((precision\\*recall)\/(precision+recall))* <\/b> <br> <br>","aada1fde":"### Feature Scaling\n\n- Scaling is important for the features to work. The ML algorithm will not accept the values without being scaled.\n  There are `two types of Scaling` used in industry: <br>\n  1) Standard Scaling <br>\n  2) Min - Max Scaling <br>\n- Here, we are using Standard Scaling","d9905461":"#### `PIE-CHART` CATEGORIES","3637336d":"### Box - Plot","a8560bb9":"### `Hyper Parameter` Tuning","bf0dd9b5":"### <b>Analysing the probabilties and classification values <\/b>","630f5547":"#### Pipeline Creation","afbce526":"### <u> Outlier Treatment <\/u>","436613b8":"#### <b> <u> Content: <\/u> <\/b>\n\nThe target feature is\nDrug type<br>\n\nThe feature sets are:\n1. Age<br>\n2. Sex<br>\n3. Blood Pressure Levels (BP)<br>\n4. Cholesterol Levels<br>\n5. Na to Potassium Ration<br>\n6. Inspiration<br>\nThe main problem here in not just the feature sets and target sets but also the approach that is taken in <br>\nsolving these types of problems as a beginner. So best of luck.<br>","0f77bdb7":"### Label Encoding","3ad830e9":"### <u> <b> Univariate Analysis <\/b> <\/u>","be1c8c59":"### <b> Model Prediction <\/b>","66fc2ba1":"#### IQR - Inter Quartile Range","6fbdb091":"#### <b> <u> Context: <\/u> <\/b>\n\nSince as a beginner in machine learning it would be a great opportunity to try some techniques to predict the outcome \nof the drugs that might be accurate for the patient.","6307d69a":"### Test-Train Split","f4d6ad65":"##### `Randomized` Search Cv","68670ed8":"#### Drug Classification","b1bb1e82":"### Inspecting the Null Values ","f9746d38":"This database contains information about certain drug types.","e394b906":"### <b> Model Building"}}