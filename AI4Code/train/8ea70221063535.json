{"cell_type":{"6ba09d78":"code","84e0c607":"code","c4ea3246":"code","60c077c4":"code","3cef7618":"code","44d79501":"code","a6823a6f":"code","8311a7ee":"code","86c76043":"code","899cb546":"code","d71bc2d2":"code","ae653be1":"code","b8525441":"code","fe3c18c2":"code","16abc2d9":"code","9096a9ac":"code","dce8e8a4":"markdown","9ef52931":"markdown","a560d022":"markdown","59a9a9b4":"markdown","51044c21":"markdown"},"source":{"6ba09d78":"import pandas as pd\nimport xgboost as xgb  \n\ntrain=pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\ntest=pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/test.csv')\n\nshops=pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/shops.csv')\nitem=pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/items.csv')\nitem_category=pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\n","84e0c607":"train['revenue'] = train['item_price'] *  train['item_cnt_day']","c4ea3246":"train=train.drop('date',axis=1)\n\nX_train=train.drop('item_cnt_day',axis=1)\nX_train=X_train.drop('date_block_num',axis=1)\nX_train=X_train.drop('item_price',axis=1)\n\ny_train=train['item_cnt_day']\n\ntest=test.drop('ID',axis=1)","60c077c4":"from sklearn.model_selection import train_test_split  \n\nX_train,X_valid,y_train,y_valid = train_test_split(X_train,y_train, test_size=0.2, shuffle=True)  ","3cef7618":"dtrain = xgb.DMatrix(X_train, label=y_train)  \ndvalid = xgb.DMatrix(X_valid, label=y_valid) ","44d79501":"from sklearn.model_selection import GridSearchCV\n\nprint(\"Parameter optimization\")\nxgb_model = xgb.XGBRegressor()\nreg_xgb = GridSearchCV(xgb_model,\n                   {'max_depth': [2,4,6],\n                    'n_estimators': [50,100,200]}, verbose=1)\n\nreg_xgb.fit(X_train, y_train)","a6823a6f":"import xgboost as xgb\n# \u30e2\u30c7\u30eb\u306e\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u4f5c\u6210\nmod = xgb.XGBRegressor()\nmod.fit(X_train, y_train)\ny_train_pred = reg_xgb.predict(X_train)","8311a7ee":"import pandas as pd\n\nsumple=pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv')","86c76043":"data=pd.DataFrame(y_train_pred)\ndata=data[:214200]","899cb546":"sumple['item_cnt_month']=data\nsumple.to_csv('last.csv',index=False)","d71bc2d2":"from optuna.integration import lightgbm as lgb","ae653be1":"dtrain = lgb.Dataset(X_train, label=y_train)\neval_data = lgb.Dataset(X_valid, label=y_valid)\n\nparam = {\n        'objective': 'regression',\n        'metric': 'rmse',\n        'lambda_l1': 8.72896788870908e-06,\n        'lambda_l2': 5.433642964479813e-07,\n        'num_leaves': 2,\n        'feature_fraction': 0.5402652675292832,\n        'bagging_fraction': 0.5999425893986495,\n        'bagging_freq': 4,\n        'min_child_samples': 14,\n    }\n\nbest = lgb.train(param, \n                 dtrain,\n                 valid_sets=eval_data,\n                 early_stopping_rounds=10)\n\n","b8525441":"import lightgbm as lgb\nimport numpy as np\nimport sklearn.datasets\nimport sklearn.metrics\nfrom sklearn.model_selection import train_test_split\n\nimport optuna\n\n\n# FYI: Objective functions can take additional arguments\n# (https:\/\/optuna.readthedocs.io\/en\/stable\/faq.html#objective-func-additional-args).\ndef objective(trial):\n\n    param = {\n        \"objective\": \"regression\",\n        \"metric\": \"rmse\",\n        \"verbosity\": -1,\n        \"boosting_type\": \"gbdt\",\n        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n    }\n\n    gbm = lgb.train(param, dtrain)\n    preds = gbm.predict(X_valid)\n    pred_labels = np.rint(preds)\n    accuracy = sklearn.metrics.accuracy_score(y_valid, pred_labels)\n    return accuracy\n\n\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=100)\n\n    print(\"Number of finished trials: {}\".format(len(study.trials)))\n\n    print(\"Best trial:\")\n    trial = study.best_trial\n\n    print(\"  Value: {}\".format(trial.value))\n\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))","fe3c18c2":"import lightgbm as lgb\nimport numpy as np\nimport sklearn.datasets\nimport sklearn.metrics\nfrom sklearn.model_selection import train_test_split\n\nimport optuna\n\nlgbm_params = {\n    'objective': 'regression'\n    \n    }\n\nauc_list = []\nprecision_list = []\nrecall_list = []\n\ndtrain = lgb.Dataset(X_train, label=y_train)\neval_data = lgb.Dataset(X_valid, label=y_valid)\n\n\ndef objective(trial):\n\n    param = {\n        \"objective\": \"regression\",\n        \"metric\": \"rmse\",\n        \"verbosity\": -1,\n        \"boosting_type\": \"gbdt\",\n        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n    }\n\n    gbm = lgb.train(param, dtrain)\n    preds = gbm.predict(X_valid)\n    pred_labels = np.rint(preds)\n    accuracy = sklearn.metrics.accuracy_score(y_valid, pred_labels)\n    return accuracy\n\n\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=100)\n\nprint('Number of finished trials: {}'.format(len(study.trials)))\n\nprint('Best trial:')\ntrial = study.best_trial\n\nprint('  Value: {}'.format(trial.value))\n\nprint('  Params: ')\nfor key, value in trial.params.items():\n    print('    {}: {}'.format(key, value))    \n\n    # optuna\u3067\u30b5\u30fc\u30c1\u3057\u305f\u30d1\u30e9\u30e1\u30fc\u30bf\ntrial.params['objective'] = 'regression'\nlgbm_params = trial.params\n\n\n    # \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u751f\u6210\u3059\u308b\nlgb_train = lgb.Dataset(X_train, y_train)\n\n    # \u30e2\u30c7\u30eb\u8a55\u4fa1\u7528\nlgb_valid = lgb.Dataset(X_valid, y_valid, reference=lgb_train)\n\nmodel = lgb.train(lgbm_params, \n                    lgb_train,\n                    valid_sets=lgb_valid,\n                    num_boost_round=100000,\n                    early_stopping_rounds=10)\n\npredict = model.predict(test, num_iteration=model.best_iteration)\n","16abc2d9":"data=pd.DataFrame(predict)\ndata=data[:214200]","9096a9ac":"sumple['item_cnt_month']=data\nsumple.to_csv('last.csv',index=False)","dce8e8a4":"# train xgboost","9ef52931":"# data split","a560d022":"# train lightgbm","59a9a9b4":"# data processing","51044c21":"# predict xgboost"}}