{"cell_type":{"4c54db7c":"code","fb7d1518":"code","4f8d322d":"code","60316c6b":"code","d2195d16":"code","a9948f53":"code","7cd69300":"code","b3c67ee9":"code","de060f73":"code","e3663a71":"code","ed6298c3":"code","3f79988f":"code","f9356ae9":"code","aeab585d":"code","84d37260":"code","dfa6d6d6":"code","a10f731b":"code","f617fa7b":"code","0e5507bf":"code","e494cbce":"code","9530fb4e":"code","6ba3c8a8":"code","12986bb4":"code","7c22bc6d":"code","d19d15e8":"code","ea0369b2":"code","b481537e":"code","f37c6a2f":"code","13fde616":"code","28281496":"code","8d6435fd":"code","2e1ae3c4":"code","bfe39851":"code","578ac75c":"code","c83407f7":"code","fa00150c":"code","398fe7ad":"code","e214ddec":"code","beb029a6":"code","827ab0ce":"code","debb03d8":"code","aab6b705":"code","ea5e8f12":"code","0ac2e018":"code","924d804a":"code","4041a80c":"code","00c0217c":"code","1d720ce9":"code","a18ae805":"code","bc7df866":"code","20e47eea":"code","7bd7806d":"code","8d40f58b":"markdown","f34031cf":"markdown","69214a89":"markdown","0365d3c8":"markdown","2d8175db":"markdown","3bb77888":"markdown","cc7f6ce5":"markdown","4c514b0e":"markdown","dfc84cf9":"markdown","747086a4":"markdown","7cf10f6b":"markdown","99af7466":"markdown","8891b4c3":"markdown","dcf35b04":"markdown","38ff9531":"markdown","98a701f1":"markdown","126578ea":"markdown","0b2a0471":"markdown","150826a9":"markdown","33edd3d7":"markdown","386c97f0":"markdown","9059cf21":"markdown","2e845fcc":"markdown","deb07e18":"markdown","64e8c6ca":"markdown","25d9fbc1":"markdown","b3281f20":"markdown","8bb6a913":"markdown","7eb3b3ee":"markdown","6690def8":"markdown","2aa332b7":"markdown","7a61ccd0":"markdown","069d489a":"markdown","a975580a":"markdown","c45babaa":"markdown","0f554b8e":"markdown","69ff0470":"markdown","b244db82":"markdown","b4faa051":"markdown","96b26812":"markdown","4684fe9d":"markdown"},"source":{"4c54db7c":"import pandas as pd\nimport numpy as np\nimport math\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\nimport warnings\nwarnings.filterwarnings(action = \"ignore\")\n\n%matplotlib inline\ndiamonds = pd.read_csv(\"..\/input\/diamonds.csv\")\n","fb7d1518":"diamonds.head()","4f8d322d":"diamonds.info()","60316c6b":"diamonds[\"cut\"].value_counts()","d2195d16":"diamonds[\"color\"].value_counts()","a9948f53":"diamonds[\"clarity\"].value_counts()","7cd69300":"# Price is of different data type and unnecessary column \"Unnamed\"\ndiamonds = diamonds.drop(\"Unnamed: 0\",axis = 1)\ndiamonds[\"price\"] = diamonds[\"price\"].astype(\"float64\")","b3c67ee9":"diamonds.head()","de060f73":"diamonds.describe()","e3663a71":"diamonds.hist(bins = 50, figsize = (20,15))\nplt.show()","ed6298c3":"corr_matrix = diamonds.corr()\n\nplt.subplots(figsize = (10,8))\nsns.heatmap(corr_matrix, annot = True, cmap = \"Blues\")\nplt.show()","3f79988f":"diamonds[\"carat\"].hist(bins = 50)\nplt.show()","f9356ae9":"diamonds[\"carat\"].max()","aeab585d":"diamonds[\"carat\"].min()","84d37260":"# Divide by 0.4 to limit the number of carat strata\n\ndiamonds[\"carat_cat\"] = np.ceil(diamonds[\"carat\"]\/0.4)\n\n# Label those above 5 as 5\ndiamonds[\"carat_cat\"].where(diamonds[\"carat_cat\"] < 5, 5.0, inplace = True)","dfa6d6d6":"diamonds[\"carat_cat\"].value_counts()","a10f731b":"diamonds[\"carat_cat\"].hist()","f617fa7b":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index,test_index in split.split(diamonds,diamonds[\"carat_cat\"]):\n    strat_train_set = diamonds.loc[train_index]\n    strat_test_set = diamonds.loc[test_index]\n    ","0e5507bf":"strat_test_set[\"carat_cat\"].value_counts() \/ len(strat_test_set)","e494cbce":"for x in (strat_test_set, strat_train_set):\n    x.drop(\"carat_cat\", axis=1,inplace = True)","9530fb4e":"strat_test_set.describe()","6ba3c8a8":"strat_train_set.describe()","12986bb4":"diamonds = strat_train_set.copy()","7c22bc6d":"diamonds.plot(kind=\"scatter\", x=\"price\", y=\"carat\",alpha = 0.1)\nplt.show()","d19d15e8":"fig, ax = plt.subplots(3, figsize = (14,18))\nsns.countplot('cut',data = diamonds, ax=ax[0],palette=\"Spectral\")\nsns.countplot('clarity',data = diamonds, ax=ax[1],palette=\"deep\")\nsns.countplot('color',data = diamonds, ax=ax[2],palette=\"colorblind\")\nax[0].set_title(\"Diamond cut\")\nax[1].set_title(\"Diamond Clarity\")\nax[2].set_title(\"Diamond Color\")\nplt.show()","ea0369b2":"sns.pairplot(diamonds[[\"price\",\"carat\",\"cut\"]], markers = [\"o\",\"v\",\"s\",\"p\",\"d\"],hue=\"cut\", height=5)\nplt.show()\n\nf, ax = plt.subplots(2,figsize = (12,10))\nsns.barplot(x=\"cut\",y=\"price\",data = diamonds,ax=ax[0])\nsns.barplot(x=\"cut\",y=\"carat\",data = diamonds, ax=ax[1])\nax[0].set_title(\"Cut vs Price\")\nax[1].set_title(\"Cut vs Carat\")\nplt.show()","b481537e":"sns.pairplot(diamonds[[\"price\",\"carat\",\"color\"]], hue=\"color\", height=5, palette=\"husl\")\nplt.show()\n\nf, ax = plt.subplots(2,figsize = (12,10))\nsns.barplot(x=\"color\",y=\"price\",data = diamonds,ax=ax[0])\nsns.barplot(x=\"color\",y=\"carat\",data = diamonds, ax=ax[1])\nax[0].set_title(\"Color vs Price\")\nax[1].set_title(\"Color vs Carat\")\nplt.show()","f37c6a2f":"sns.pairplot(diamonds[[\"price\",\"carat\",\"clarity\"]],hue=\"clarity\", height=5)\nplt.show()\n\nf, ax = plt.subplots(2,figsize = (12,10))\nsns.barplot(x=\"clarity\",y=\"price\",data = diamonds,ax=ax[0])\nsns.barplot(x=\"clarity\",y=\"carat\",data = diamonds, ax=ax[1])\nax[0].set_title(\"Clarity vs Price\")\nax[1].set_title(\"Clarity vs Carat\")\nplt.show()","13fde616":"fig, ax = plt.subplots(3, figsize = (14,18))\nsns.violinplot(x='cut',y='price',data = diamonds, ax=ax[0],palette=\"Spectral\")\nsns.violinplot(x='clarity',y='price',data = diamonds, ax=ax[1],palette=\"deep\")\nsns.violinplot(x='color',y='price',data = diamonds, ax=ax[2],palette=\"colorblind\")\nax[0].set_title(\"Cut vs Price\")\nax[1].set_title(\"Clarity vs Price\")\nax[2].set_title(\"Color vs Price \")\nplt.show()","28281496":"from pandas.plotting import scatter_matrix\n\nattributes = [\"depth\",\"table\",\"x\",\"y\",\"z\",\"price\"]\nscatter_matrix(diamonds[attributes], figsize=(12, 8))\n","8d6435fd":"sample_incomplete_rows = diamonds[diamonds.isnull().any(axis=1)].head()\nsample_incomplete_rows","2e1ae3c4":"diamonds = strat_train_set.drop(\"price\", axis=1)\ndiamonds_label = strat_train_set[\"price\"].copy()\ndiamonds_only_num = diamonds.drop([\"cut\",\"clarity\",\"color\"],axis=1)\n\ndiamonds_only_num.head()","bfe39851":"from sklearn.preprocessing import StandardScaler\n\nstd_scaler = StandardScaler()\ndiamonds_scaled_num = std_scaler.fit_transform(diamonds_only_num)\n\ndiamonds_scaled_num","578ac75c":"pd.DataFrame(diamonds_scaled_num).head()","c83407f7":"diamonds_cat = diamonds[[\"cut\",\"color\",\"clarity\"]]\ndiamonds_cat.head()","fa00150c":"from sklearn.preprocessing import OneHotEncoder\n\ncat_encoder = OneHotEncoder()\ndiamonds_cat_encoded = cat_encoder.fit_transform(diamonds_cat)\n\ndiamonds_cat_encoded.toarray()","398fe7ad":"cat_encoder.categories_","e214ddec":"from sklearn.compose import ColumnTransformer\n\nnum_attribs = list(diamonds_only_num)\ncat_attribs = [\"cut\",\"color\",\"clarity\"]\npipeline = ColumnTransformer([\n    (\"num\", StandardScaler(),num_attribs),\n    (\"cat\",OneHotEncoder(),cat_attribs),\n])\n\ndiamonds_prepared = pipeline.fit_transform(diamonds)","beb029a6":"diamonds_prepared","827ab0ce":"pd.DataFrame(diamonds_prepared).head()","debb03d8":"diamonds_prepared.shape","aab6b705":"from sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\nfrom random import randint\n\nX_test = strat_test_set.drop(\"price\",axis=1)\ny_test = strat_test_set[\"price\"].copy()\n\nmodel_name = []\nrmse_train_scores = []\ncv_rmse_scores = []\naccuracy_models = []\nrmse_test_scores = []\n\ndef model_performance(modelname,model,diamonds = diamonds_prepared, diamonds_labels = diamonds_label,\n                      X_test = X_test,y_test = y_test,\n                      pipeline=pipeline, cv = True):\n    \n    model_name.append(modelname)\n    \n    model.fit(diamonds,diamonds_labels)\n    \n    predictions = model.predict(diamonds)\n    mse_train_score = mean_squared_error(diamonds_labels, predictions)\n    rmse_train_score = np.sqrt(mse_train_score)\n    cv_rmse = np.sqrt(-cross_val_score(model,diamonds,diamonds_labels,\n                                       scoring = \"neg_mean_squared_error\",cv=10))\n    cv_rmse_mean = cv_rmse.mean()\n    \n    print(\"RMSE_Train: %.4f\" %rmse_train_score)\n    rmse_train_scores.append(rmse_train_score)\n    print(\"CV_RMSE: %.4f\" %cv_rmse_mean)\n    cv_rmse_scores.append(cv_rmse_mean)\n    \n    \n    print(\"---------------------TEST-------------------\")\n    \n    X_test_prepared = pipeline.transform(X_test)\n    \n    test_predictions = model.predict(X_test_prepared)\n    mse_score = mean_squared_error(y_test,test_predictions)\n    rmse_score = np.sqrt(mse_score)\n    \n    print(\"RMSE_Test: %.4f\" %rmse_score)\n    rmse_test_scores.append(rmse_score)\n    \n    accuracy = (model.score(X_test_prepared,y_test)*100)\n    print(\"accuracy: \"+ str(accuracy) + \"%\")\n    accuracy_models.append(accuracy)\n    \n    start = randint(1, len(y_test))\n    some_data = X_test.iloc[start:start + 5]\n    some_labels = y_test.iloc[start:start + 5]\n    some_data_prepared = pipeline.transform(some_data)\n    print(\"Predictions:\", model.predict(some_data_prepared))\n    print(\"Labels:    :\", list(some_labels))\n    \n    \n    plt.scatter(y_test,test_predictions)\n    plt.xlabel(\"Actual\")\n    plt.ylabel(\"Predicted\")\n    x_lim = plt.xlim()\n    y_lim = plt.ylim()\n    plt.plot(x_lim, y_lim, \"go--\")\n    plt.show()\n    \n    ","ea5e8f12":"from sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression(normalize=True)\nmodel_performance(\"Linear Regression\",lin_reg)","0ac2e018":"from sklearn.tree import DecisionTreeRegressor\n\ndec_tree = DecisionTreeRegressor(random_state=42)\nmodel_performance(\"Decision Tree Regression\",dec_tree)","924d804a":"from sklearn.ensemble import RandomForestRegressor\n\nforest_reg = RandomForestRegressor(n_estimators = 10, random_state = 42)\nmodel_performance(\"Random Forest Regression\",forest_reg)","4041a80c":"from sklearn.linear_model import Ridge\n\nridge_reg = Ridge(normalize = True)\nmodel_performance(\"Ridge Regression\",ridge_reg)","00c0217c":"from sklearn.linear_model import Lasso\n\nlasso_reg = Lasso(normalize = True)\nmodel_performance(\"Lasso Regression\",lasso_reg)","1d720ce9":"from sklearn.linear_model import ElasticNet\n\nnet_reg = ElasticNet()\nmodel_performance(\"Elastic Net Regression\",net_reg)","a18ae805":"from sklearn.ensemble import AdaBoostRegressor\n\nada_reg = AdaBoostRegressor(n_estimators = 100)\nmodel_performance(\"Ada Boost Regression\",ada_reg)","bc7df866":"from sklearn.ensemble import GradientBoostingRegressor\n\ngrad_reg = GradientBoostingRegressor(n_estimators = 100, learning_rate = 0.1,\n                                     max_depth = 1, random_state = 42, loss = 'ls')\nmodel_performance(\"Gradient Boosting Regression\",grad_reg)","20e47eea":"compare_models = pd.DataFrame({\"Algorithms\" : model_name, \"Models RMSE\" : rmse_test_scores, \n                               \"CV RMSE Mean\" : cv_rmse_scores, \"Accuracy\" : accuracy_models})\ncompare_models.sort_values(by = \"Accuracy\", ascending=False)","7bd7806d":"sns.pointplot(\"Accuracy\",\"Algorithms\",data=pd.DataFrame({'Algorithms':model_name,\"Accuracy\":accuracy_models}))\n","8d40f58b":"### Comparing the Accuracies of different Regression Models","f34031cf":"### Comparison of carat with price based on diamond color","69214a89":"### Comparison of carat with price based on diamond clarity","0365d3c8":"<a id=\"link6\"><\/a>\n## Applying ML Algorithms on the Dataset\n","2d8175db":"### Conclusions\n - **x , y and z are correlated with the price.** \n - **Price of the diamond and carat weight of the diamond are highly correlated**\n - **Depth and Table are weakly correlated with the price of the diamond.**\n - **Carat is one of the main features to predict the price of a diamond.**","3bb77888":"<a id=\"link5\"><\/a>\n## Preparing data for the ML Algorithms\n","cc7f6ce5":"### Encoding Categorical Attributes\n\nIn this dataset, we have three categorical attributes.ML algorithms work better with numbers.Thus, we will convert them into numbers using OneHotEncoder of scikit learn.","4c514b0e":"**Conclusions**\n- J color diamonds are the most expensive and the heaviest diamonds.\n- The two plots are very similar.\n\nThus, it can be concluded that the heavier diamond is expensive, if only color is considered.","dfc84cf9":"### More plots to understand the realtion between cut,color and clarity with prices","747086a4":"\n### Feature Scaling\n\n\nMachine Learning algorithms don\u2019t perform well when the input numerical attributes have very different scales. Therefore, it is necessary to feature scale all the features of diamond dataset. There are two ways of doing feature scaling -min-max scaling and standardization. I will be using standardization as it is not affected by any outliers.","7cf10f6b":"### Plotting scatterplot between price and carat","99af7466":"**Random Forest Regression**","8891b4c3":"<a id=\"link4\"><\/a>\n## Data Visualisation \n\nWe will be using training set to plot varoius graphs to visualise and draw conclusions from the data.","dcf35b04":"Most of the carat value ranges from 0.3 to 1.2. So, we will divide the carat into 5 categories.","38ff9531":"Size of Train Set = 43152","98a701f1":"**Gradient Boosting Regression**","126578ea":"<a id=\"link2\"><\/a>\n## Exploring Correlation between Features","0b2a0471":"<a id=\"link3\"><\/a>\n### Splitting Data into Test and Train Set\n\nIt is advisable to split the dataset into Test set (80%) and Train set (20%). The test set allows our model to make \npredictions on values which it has never seen before.\n\nBut taking random samples from our dataset can introduce significant **sampling bias**. Therefore, in order to avoid sampling bias, the data will be divide into different homogenous subgroups called strata. This is called **Stratified Sampling**. Since, we know that carat is the most important parameter to predict the price of the diamonds we will use it for Stratified sampling ","150826a9":"**Decision Tree Regression**","33edd3d7":"### Count plots of different categorical features of diamonds","386c97f0":"### Comparison of carat with price based on diamond cut.","9059cf21":"Now, it is time to select a model, train it and evaluate its performance using test set.\nFirst of all we will import mean_squared_error and cross_val_score from sklearn to evaluate the models.\n\nWe will create one function that will run through each algorithm. We'll also have variables that hold results of the algorithms for future comparisons. RMSE and CV_scores are used to check the performance. The function will plot a graph to show how well our algorithm has predicted the data.","2e845fcc":"## TOPICS\n\n1. [**A Quick Look at the Dataset**](#link1)\n2. [**Exploring Correlation between Features**](#link2)\n3. [**Splitting Data into Test and Train Set**](#link3)\n4. [**Data Visualisation**](#link4)\n5. [**Preparing Data for ML algorithm**](#link5)\n6. [**Applying ML Algorithm on the Dataset**](#link6)\n7. [**Conclusion**](#link7)","deb07e18":"Size of Test Set = 10788","64e8c6ca":"### Dropping the unnecessary column Unnamed: 0","25d9fbc1":"Now we will perform the stratified splitting of the dataset using sklearn's StratifiedShuffleSplit class","b3281f20":"**Conclusion**\n- Fair cut diamonds weigh the most but are not the most expensive diamonds.\n- Premium cut diamonds are the most expensive diamonds.\n- Ideal cut diamonds weigh less and are cheapest diamonds.\n\nWe can see that price of diamond is dependent on the cut.","8bb6a913":"<a id=\"link7\"><\/a>\n## Conclusion\n\n**Random Forest Regressor gives us the Highest accuracy.**\n\n**THANK YOU**","7eb3b3ee":"<a id=\"link1\"><\/a>\n## A Quick Look at the Dataset","6690def8":"### Features of the Dataset\n\n- **Carat** weight of the diamond\n- **cut** Describe cut quality of the diamond. Quality in increasing order Fair, Good, Very Good, Premium, Ideal - - - **color** Color of the diamond, with D being the best and J the worst\n- **clarity** How obvious inclusions are within the diamond:(in order from best to worst, FL = flawless, I3= level 3 inclusions) FL,IF, VVS1, VVS2, VS1, VS2, SI1, SI2, I1, I2, I3\n- **depth** The height of a diamond, measured from the culet to the table, divided by its average girdle diameter\n- **table** The width of the diamond's table expressed as a percentage of its average diameter\n- **price** the price of the diamond\n- **x** length mm\n- **y** width mm\n- **z** depth mm","2aa332b7":"**Ridge Regression**","7a61ccd0":"### Plotting Histogram to get an idea about the different features\/attributes of the dataset","069d489a":"We will now drop the carat category columns.","a975580a":"**Elastic Net Regression**","c45babaa":"**Ada Boost Regression**","0f554b8e":"**Lasso Regression**","69ff0470":"# Diamond Price Modelling\n\n **What are diamonds ?**\n\n> Diamond is a solid form of the element carbon with its atoms arranged in a crystal structure called diamond cubic.\nThe most familiar uses of diamonds today are as gemstones used for adornment, and as industrial abrasives for cutting hard materials.\n\n\n\n **In this notebook, we will try to build a model to predict the prices of diamonds based on various features of diamond  like carat weight, cut quality ,etc.**\n \n*Dataset used in this notebook has been taken from [KAGGLE](https:\/\/www.kaggle.com\/shivam2503\/diamonds)*","b244db82":"### Transformation Pipeline\n\nWe have to perform feature scaling and label encoding on dataset before feeding it into ML algorithms. So, to simplify the process we will create a pipeline using ColumnTransformer which successively performs feature scaling and Label encoding.  ","b4faa051":"**Now let's take a look at our diamond dataset.**","96b26812":"### Importing the important libraries required for this project and getting the data from the dataset\n","4684fe9d":"**Linear Regression**"}}