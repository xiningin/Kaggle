{"cell_type":{"62c78727":"code","9571f19a":"code","a7b4463b":"code","f4e6f8e4":"code","0d811dd4":"code","4983d3c1":"code","56de9946":"code","f4ab6f34":"code","3c4e9ca6":"code","b0df5132":"code","44cacf99":"code","007e1977":"code","4302501c":"code","304926de":"code","fdc9930c":"code","52057911":"code","ca0fae2e":"code","25d0a279":"code","57ff4326":"markdown","449945cf":"markdown","cdae95ae":"markdown","95e143e0":"markdown","5196f840":"markdown","04666ae3":"markdown","2430190b":"markdown","7d4d9e92":"markdown","19985d1b":"markdown","32187010":"markdown"},"source":{"62c78727":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9571f19a":"# Step 1 - Importing the required Libraries\nimport tensorflow as tf\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Flatten\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport warnings \nwarnings.filterwarnings('ignore')\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nfrom xgboost import XGBRegressor   # We will compare our NN performance using a XGboost base model","a7b4463b":"# Step 2 - Get the data\ndef get_data():\n    train_data_path ='\/kaggle\/input\/tabular-playground-series-feb-2021\/train.csv'\n    train = pd.read_csv(train_data_path)\n    \n    #get test data\n    test_data_path ='\/kaggle\/input\/tabular-playground-series-feb-2021\/test.csv'\n    test = pd.read_csv(test_data_path)\n    \n    return train , test\n\ntrain, test = get_data()","f4e6f8e4":"train.describe()","0d811dd4":"# Seeing what other columns our data has\ntrain.columns","4983d3c1":"# Dividind the data into categorical and continuous features for better visualization and pre processing.\ncat_features = ['cat{0}'.format(x) for x in range(0,10)]\ncont_features = ['cont{0}'.format(x) for x in range(0,14)]\n\n# Appending the target column name to both the lists\ncat_features.append('target')\ncont_features.append('target')\n\ntrain_cat = train[cat_features]\ntrain_cont = train[cont_features]\n\n# Now removing the 'target' from the list\ncat_features.remove('target')\ncont_features.remove('target')\n\n# Doing the same for test data\ntest_cat = test[cat_features]\ntest_cont = test[cont_features]\n","56de9946":"# Analysis of Categorical Features\nplt.figure(figsize = (12,9))\nj = 1\nfor i in cat_features:\n    plt.subplot(5,2,j)\n    sns.countplot(x = i, data = train_cat)\n    j+=1\n\nplt.show()","f4ab6f34":"train_cat['cat4'].value_counts()","3c4e9ca6":"# Plotting the distribution of each continuous variable\nplt.figure(figsize = (16,10))\nj = 1\nfor con in cont_features:\n    plt.subplot(7,2,j)\n    sns.distplot(train_cont[con])\n    j+=1","b0df5132":"# Label Encoding categorical features\n# Code Reference : https:\/\/www.kaggle.com\/inversion\/get-started-feb-tabular-playground-competition\n\nfor c in train_cat.columns:\n    if train_cat[c].dtype=='object': \n        lbl = LabelEncoder()\n        lbl.fit(list(train_cat[c].values) + list(test_cat[c].values))\n        train_cat[c] = lbl.transform(train_cat[c].values)\n        test_cat[c] = lbl.transform(test_cat[c].values)\n        \ndisplay(train_cat.head())","44cacf99":"# Checking the correlation matrix for continuous features\nplt.figure(figsize = (12,10))\nsns.heatmap(train_cont.corr(), annot = True)\nplt.show()","007e1977":"from sklearn.ensemble import ExtraTreesRegressor\n\nY = train_cont.pop('target')\nX = train_cont\n# feature extraction\nmodel = ExtraTreesRegressor(n_estimators=10)\nmodel.fit(X, Y)\nprint(model.feature_importances_)","4302501c":"# Getting our final train and test data\nX_train = pd.concat([train_cat, train_cont], axis = 1)\nX_train.pop('target')\ny_train = Y\nX_test = pd.concat([test_cat, test_cont], axis = 1)","304926de":"# Our DNN Model! Test 1\n\nNN_model = Sequential()\n\n# The Input Layer :\nNN_model.add(Dense(128, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))\n\n# The Hidden Layers :\nNN_model.add(Dense(512, kernel_initializer='normal',activation='relu'))\nNN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\nNN_model.add(Dense(128, kernel_initializer='normal',activation='relu'))\n\n# The Output Layer :\nNN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n\n# Compile the network :\nNN_model.compile(loss='mse', optimizer='adam', metrics=[tf.keras.metrics.MeanSquaredError()])\nNN_model.summary()","fdc9930c":"# Training the model\nNN_model.fit(X_train, y_train, epochs=50, batch_size=2000, validation_split = 0.2)","52057911":"predictions = NN_model.predict(X_test)","ca0fae2e":"predictions","25d0a279":"submission = pd.read_csv('\/kaggle\/input\/tabular-playground-series-feb-2021\/sample_submission.csv', index_col='id')\nsubmission['target'] = predictions\nsubmission.to_csv('final_submission.csv')","57ff4326":"Now that we have seen the plots, we can move on to the pre processing part. That will incude two main tasks:\n\n1. Label Encoding the categorical features. (Code Reference from the starter notebook)\n2. Checking the correlation of continuous variables to remove or combine correlated features.","449945cf":"# Step 3 - Data Pre Processing","cdae95ae":"Some interesting observations can be made seeing this table:\n\n1. Almost all the continuous variables are standardised (values between 0 and 1). Only some variables have a value greater than one.\n2. The mean value of our target variable is 7.5 and the standard deviation is 0.88.","95e143e0":"We cannot draw any conclusive decision from these values as all the values are almost the same. Hence, for now we will keep all the continuous features and see how our model performs.","5196f840":"# Step 1 : Importing the required Libraries","04666ae3":"Please note that the empty columns just indicate that there are very less values compared to the scale of the graph. For instance, see the values counts for 'cat4' in the next cell ","2430190b":"# Step 2 - Fetching the Data","7d4d9e92":"# Creating our Neural Network and training it","19985d1b":"From the above output, we see that we have 10 categorical features and 14 continuous features. ","32187010":"Hello! In this notebook we will test a simple DNN model for regression. We will use a keras Sequential model with 3 hidden dense layers for now. \n\nThere is room for much improvement in this so please do suggest any changes that can make my model better."}}