{"cell_type":{"f94ed9b4":"code","4d4b6874":"code","4b532a59":"code","55f3ad16":"code","c57e6873":"code","cdf3b1dc":"code","55acd4a5":"code","b4b01a73":"code","035c3719":"code","b2bba1d1":"code","85912788":"code","fda60926":"code","dc4e90a5":"code","24b8f19d":"code","0a07bd1b":"code","ea5049b3":"code","c447df7b":"code","bc9dcef5":"code","d43b152e":"code","d8b0c79f":"code","04f68cdb":"code","9df1533e":"markdown","f33be56b":"markdown","8097ff7d":"markdown","f6c6ae47":"markdown","ea8e9c3b":"markdown","9f24769b":"markdown","688de582":"markdown","a00570ad":"markdown","0e901c14":"markdown","525f23f3":"markdown","f686b03f":"markdown","dfbfb0d3":"markdown","812951d0":"markdown","d1560a7d":"markdown","4af04f72":"markdown","3848ce06":"markdown"},"source":{"f94ed9b4":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n\nimport warnings\nwarnings.simplefilter(action='ignore', category = FutureWarning)\n\nsns.set_style(\"darkgrid\")","4d4b6874":"from sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgb\nimport xgboost as xgb","4b532a59":"df = pd.read_csv(\"..\/input\/heart-attack-analysis-prediction-dataset\/heart.csv\")\ndf","55f3ad16":"df.info()","c57e6873":"df.describe()","cdf3b1dc":"df[\"slp\"] = df[\"slp\"].map({0: \"downsloping\", 1: \"flat\", 2: \"upsloping\"})\ndf[\"thall\"] = df[\"thall\"].map({1: \"fixed_effect\", 2: \"normal\", 3: \"reversable_defect\", 0: \"else\"})","55acd4a5":"fig, ax = plt.subplots(figsize = (8, 8))\nax.pie(df.output.value_counts(), labels=[\"0\", \"1\"], autopct='%1.2f%%', startangle=180)\nax.set_title(\"Target\")","b4b01a73":"cat_cols = [\"sex\", \"cp\", \"fbs\", \"restecg\", \"exng\", \"slp\", \"caa\", \"thall\"]\nnum_cols = [\"age\", \"trtbps\", \"chol\", \"thalachh\", \"oldpeak\"]","035c3719":"def count_percentage(df, col, hue):\n\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(22, 6))\n    order = sorted(df[col].unique())\n    \n    sns.countplot(col, data = df, hue = hue, ax = ax1, order = order).set_title(\"Counts For Feature:\\n\" + col)\n\n    df_temp = df.groupby(col)[hue].value_counts(normalize = True).\\\n    rename(\"percentage\").\\\n    reset_index()\n\n    fig = sns.barplot(x = col, y = \"percentage\", hue = hue, data = df_temp, ax = ax2, order = order)\n    fig.set_ylim(0,1)\n    \n    fontsize = 14 if len(order) <= 10 else 10\n    for p in fig.patches:\n        txt = str(p.get_height().round(2)) + '%'\n        txt_x = p.get_x() \n        txt_y = p.get_height()\n        fig.text(txt_x + 0.125, txt_y + 0.02,txt, fontsize = fontsize)\n\n    ax2.set_title(\"Percentages For Feature: \\n\" + col)","b2bba1d1":"for col in cat_cols:\n    count_percentage(df, col, \"output\")","85912788":"def feature_dist_clas(df, col, hue):\n    fig, axes = plt.subplots(1, 4, figsize = (25, 5))\n    order = sorted(df[hue].unique())\n\n    sns.histplot(x = col, hue = hue, data = df, ax = axes[0])\n    sns.kdeplot(x = col, hue = hue, data = df, fill = True, ax = axes[1])\n    sns.boxplot(y = col, hue = hue, data = df, x = [\"\"] * len(df), ax = axes[2])\n    sns.violinplot(y = col, hue = hue, data = df, x = [\"\"] * len(df), ax = axes[3])\n    \n    fig.suptitle(\"For Feature:  \" + col)\n    axes[0].set_title(\"Histogram For Feature \" + col)\n    axes[1].set_title(\"KDE Plot For Feature \" + col)   \n    axes[2].set_title(\"Boxplot For Feature \" + col)   \n    axes[3].set_title(\"Violinplot For Feature \" + col)   ","fda60926":"for col in num_cols:\n    feature_dist_clas(df, col, \"output\")","dc4e90a5":"def feature_distribution(df, col):\n    \n    skewness = np.round(df[col].skew(), 3)\n    kurtosis = np.round(df[col].kurtosis(), 3)\n\n    fig, axes = plt.subplots(1, 3, figsize = (18, 6))\n\n    sns.kdeplot(data = df, x = col, fill = True, ax = axes[0], color = \"orangered\")\n    sns.boxplot(data = df, y = col, ax = axes[1], color = \"orangered\")\n    stats.probplot(df[col], plot = axes[2])\n\n    axes[0].set_title(\"Distribution \\nSkewness: \" + str(skewness) + \"\\nKurtosis: \" + str(kurtosis))\n    axes[1].set_title(\"Boxplot\")\n    axes[2].set_title(\"Probability Plot\")\n    fig.suptitle(\"For Feature:  \" + col)","24b8f19d":"for col in num_cols:\n    feature_distribution(df, col)","0a07bd1b":"def heatmap(df):\n    \n    fig, ax = plt.subplots(figsize = (15, 15))\n    \n    sns.heatmap(df.corr(), cmap = \"coolwarm\", annot = True, fmt = \".2f\", annot_kws = {\"fontsize\": 9},\n                vmin = -1, vmax = 1, square = True, linewidths = 0.8, cbar = False)\n    \nheatmap(df)","ea5049b3":"encode_cols = [\"slp\", \"thall\"]\n\ndummies = pd.get_dummies(df[encode_cols], drop_first = True)\n\nfin = pd.concat([df, dummies], axis = 1).drop(encode_cols, axis = 1)\nfin","c447df7b":"target = \"output\"\npredictors = [col for col in fin.columns if col != target]\n\nX_train, X_test, y_train, y_test = train_test_split(fin[predictors],\n                                                    fin[target],\n                                                    test_size = 0.25,\n                                                    random_state = 42)\n\ncv3 = KFold(n_splits = 3, shuffle = True, random_state = 42)\ncv5 = KFold(n_splits = 5, shuffle = True, random_state = 42)\ncv10 = KFold(n_splits = 10, shuffle = True, random_state = 42)\n\ndef cv_model(model, X = X_train, y = y_train, cv = cv5):\n    return cross_val_score(model, X, y, scoring = \"accuracy\", cv = cv, n_jobs = -1).mean()","bc9dcef5":"for col in num_cols:   \n    scaler = StandardScaler()\n\n    X_train[col] = scaler.fit_transform(X_train[col].values.reshape(-1, 1))\n    X_test[col] = scaler.transform(X_test[col].values.reshape(-1, 1))","d43b152e":"X_train2 = X_train.drop(\"fbs\", axis = 1)\nX_test2 = X_test.drop(\"fbs\", axis = 1)","d8b0c79f":"logreg = LogisticRegression(random_state = 42)\nsvc = SVC(random_state=42, probability = True)\ngnb = GaussianNB()\nrfc = RandomForestClassifier(random_state = 42)\nknnc = KNeighborsClassifier(n_jobs = -1)\nlgbc = lgb.LGBMClassifier(random_state = 42, n_jobs = 1)\ndtc = DecisionTreeClassifier(random_state = 42)\nxgbc = xgb.XGBClassifier(random_state = 42, n_jobs = -1, use_label_encoder = False, eval_metric = \"logloss\")\n\nvc_logreg_svc_rfc_knn = VotingClassifier([(\"logreg\", logreg), (\"svc\", svc), (\"rfc\", rfc), (\"knn\", knnc)],\n                                         voting = \"soft\")\nvc_logreg_svc_knn = VotingClassifier([(\"logreg\", logreg), (\"svc\", svc), (\"knn\", knnc)],\n                                         voting = \"soft\")\nvc_logreg_svc = VotingClassifier([(\"logreg\", logreg), (\"svc\", svc)],\n                                         voting = \"soft\")\nvc_all = VotingClassifier([(\"logreg\", logreg), (\"svc\", svc), (\"gnb\", gnb), ( \"rfc\", rfc), (\"knn\", knnc),\n                           (\"lgb\", lgbc), (\"dtc\", dtc), (\"xgb\", xgbc)],\n                          voting = \"soft\")\n\ntrain_accuracy = {}\ntest_accuracy = {}\ncv_score3 = {}\ncv_score5 = {}\ncv_score10 = {}\n\nmodels = {\n    \"LogisticRegression\": logreg,\n    \"SupportVectorMachine\": svc,\n    \"GaussianNaiveBayes\": gnb,\n    \"RandomForest\": rfc,\n    \"KNN\": knnc,\n    \"LightGBM\": lgbc,\n    \"DecisionTree\": dtc,\n    \"XGBoost\": xgbc,\n    \"VotingClassifier (All Models)\": vc_all,\n    \"VotingClassifier (Logreg-SVC)\": vc_logreg_svc,\n    \"VotingClassifier (Logreg-SVC-KNN)\": vc_logreg_svc_knn,\n    \"VotingClassifier (Logreg-SVC-RFC-KNN)\": vc_logreg_svc_rfc_knn   \n}\n\nfor name, model in models.items():\n    model.fit(X_train2, y_train)\n    train_preds = model.predict(X_train2)\n    test_preds = model.predict(X_test2)\n    \n    train_accuracy[name] = accuracy_score(train_preds, y_train).round(4)\n    test_accuracy[name] = accuracy_score(test_preds, y_test).round(4)\n    cv_score3[name] = cv_model(model, X_train2, y_train, cv = cv3).round(4)\n    cv_score5[name] = cv_model(model, X_train2, y_train, cv = cv5).round(4)\n    cv_score10[name] = cv_model(model, X_train2, y_train, cv = cv10).round(4)\n    \nscores = pd.DataFrame([train_accuracy, test_accuracy, cv_score3, cv_score5, cv_score10], \n                      index = [\"TrainAccuracy\", \"TestAccuracy\", \"3FoldCVScore\", \"5FoldCVScore\", \"10FoldCVScore\"]).T","04f68cdb":"scores","9df1533e":"### 4.2.1 Takeaways - Numerical Features\n\nAge, thalachh and oldpeak would be useful variables.\n\ntrtbps, col and oldpeak have a few outliers. thalachh has just an outlier.\n\nOur numerical variables generally have normal distribution except oldpeak. Also, outliers at chol feature are problem for this feature's normality.","f33be56b":"Target value's distribution is 54.5% - 46.5%. It is balanced. So, we don't have to use stratification techniques for cross validation and splitting the data, or we don't need to applying sampling to the data.","8097ff7d":"## 4.1 Categorical Features","f6c6ae47":"# 6. Models\n\nI just use basic classification algorithms with their default parameters.\n\nI also use Voting Classifier to construct ensemble models with choosing a couple of them. I won't tune hyperparameters.","ea8e9c3b":"We have 303 observations, it means we have a small data. 25-75 or 30-70 is ideal for train test proportion.","9f24769b":"# 5. Preprocessing\n\nOne hot encoding for two feature,\nSplitting the data,\nDefining cross validations,\nScaling data with using Standard Scaler","688de582":"# 1. Packages","a00570ad":"### 4.1.1 Takeaways - Categorical Features\n\n**Sex**: Really effective feature. Male's target value is 75%, female's target value is 45%\n\n**cp**: If a person doesn't have asymptomatic chest pain (encoded as 0), target value is at least 70%\n\n**fbs**: This feature looks like ineffective. Target's values are 55% and 51% for two option.\n\n**nestecg**: Target values are 46%, 63% and 25%. This variable could be useful\n\n**exang**: Really effective feature. 70% - 23%\n\n**slp**: 43, 35 and 75 percent. It could be also useful.\n\n**thall**: 50-33-78-24. It could be also useful.","0e901c14":"# 2.1 Features\n\n**1. age** - age in years\n\n**2. sex** - sex (1 = male; 0 = female)\n\n**3. cp** - chest pain type (1 = typical angina; 2 = atypical angina; 3 = non-anginal pain; 0 = asymptomatic)\n\n**4. trestbps** - resting blood pressure (in mm Hg on admission to the hospital)\n\n**5. chol** - serum cholestoral in mg\/dl\n\n**6. fbs** - fasting blood sugar > 120 mg\/dl (1 = true; 0 = false)\n\n**7. restecg** - resting electrocardiographic results (1 = normal; 2 = having ST-T wave abnormality; 0 = hypertrophy)\n\n**8. thalach** - maximum heart rate achieved\n\n**9. exang** - exercise induced angina (1 = yes; 0 = no)\n\n**10. oldpeak** - ST depression induced by exercise relative to rest\n\n**11. slope** - the slope of the peak exercise ST segment (2 = upsloping; 1 = flat; 0 = downsloping)\n\n**12. ca** - number of major vessels (0-3) colored by flourosopy\n\n**13. thal** - 2 = normal; 1 = fixed defect; 3 = reversable defect\n\n**14. num** - the predicted attribute - diagnosis of heart disease (angiographic disease status) (Value 0 = < diameter narrowing; Value 1 = > 50% diameter narrowing)\n\nThanks to https:\/\/www.kaggle.com\/rashikrahmanpritom\/heart-attack-analysis-prediction-dataset\/discussion\/234843","525f23f3":"# 2. General Infos About Data","f686b03f":"# 3. Target Value","dfbfb0d3":"I just remove the variable **fbs**. It is a categorical feature that has two possibility(55% - 51%). It has lowest correlation as we can from heatmap. ","812951d0":"# 4.2 Numerical Features\n## 4.2.1 Numerical vs Target","d1560a7d":"Our categorical features had encoded with using label encoder or had been considered as ordinal feature. I will remap them with meanings and I will re-encode them with using one-hot encoder.","4af04f72":"# 4. EDA","3848ce06":"# 6.1. Takeaways - Models\n\nIf we look at above table;\n\nLinear classifiers achieves better results i.e Logistic Regression, SVM\n\nTree based algorithms have overfitting problem since we don't tune hyperparameters.\n\nCross validation scores are unstable. Our cv scores are changing between 0.79 - 0.83 but our scores on testing data are 0.88 - 0.90\n\nFor individual models, Logistic Regression has best test accuracy with nearly 90%\n\nFor ensemble models, Voting Classifier with using Logistic Regression and SVM has best test accuracy with almost 91%"}}