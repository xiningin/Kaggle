{"cell_type":{"40e0b081":"code","a567de81":"code","3e4480cb":"code","911ca08e":"code","e78d6920":"code","87699a36":"code","d6bedf44":"code","521afb82":"code","dc8e7f9f":"code","8526a46c":"code","d3c61c19":"code","34981d79":"code","38fb67cd":"code","ef7c17a3":"code","2d00face":"code","044b4c23":"code","c59b1f60":"code","620d8231":"code","1e0e8fa2":"code","d57ee5c4":"code","3d3cb1ff":"code","d38199ff":"code","71da22e5":"code","7d6e7370":"code","8e40648a":"code","460a0347":"code","bd5cab62":"code","3f86ca07":"code","1bdbf2d2":"code","abb94d92":"code","43a02c3f":"code","07a40a96":"code","1ed1ba1b":"code","b9fe5dc2":"code","236f0163":"code","2d1c839a":"code","d57caec4":"code","51abfa3d":"code","d6606dac":"code","1895b05a":"code","27cdc99f":"code","4e750b78":"code","73b0c2f7":"code","6c3f0714":"code","bed63ed1":"code","51c9570f":"code","124c1ea5":"code","cc9012c4":"code","310a5b4b":"code","8aa73bf8":"code","ec81668a":"code","aae134a3":"code","71978c6f":"code","bf768d24":"code","b589d879":"code","919764b6":"code","14f91fb1":"code","5f0bd298":"code","a1ec8d83":"code","8905399d":"code","56ffafef":"code","b1c6002c":"code","7d94e0aa":"code","31a2a6c2":"markdown","5bd18e4b":"markdown","20034a31":"markdown","a03f93c4":"markdown","378bc571":"markdown","dd61e665":"markdown","915a5d8a":"markdown"},"source":{"40e0b081":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a567de81":"! git clone https:\/\/github.com\/h2oai\/pystacknet\n! cd pystacknet\n! python setup.py install\n","3e4480cb":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport os\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import Model\nfrom tensorflow.keras import regularizers\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.manifold import TSNE\nfrom sklearn.preprocessing import StandardScaler,LabelEncoder,scale\nfrom math import sqrt\nfrom numpy import hstack\nfrom numpy import vstack\nfrom numpy import asarray\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error,accuracy_score\nfrom sklearn.linear_model import LinearRegression,LogisticRegression\nfrom sklearn.linear_model import ElasticNet\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.naive_bayes import GaussianNB\nfrom numpy import hstack\nfrom numpy import vstack\nfrom numpy import asarray\n\n\nfrom catboost import CatBoostClassifier\n\nimport catboost\nimport datetime\nprint(tf.__version__)\n\n\n\n\n\n\n\n","911ca08e":"train=pd.read_csv('\/kaggle\/input\/tabular-playground-series-mar-2021\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/tabular-playground-series-mar-2021\/test.csv')","e78d6920":"train.isnull().sum()","87699a36":"test.isnull().sum()","d6bedf44":"train.head()","521afb82":"def categorical_bar_plot(column_name,mode):\n    if mode==\"train\":\n        df=pd.DataFrame(train[column_name].value_counts()).reset_index()\n        print(df)\n    else:\n        df=pd.DataFrame(test[column_name].value_counts()).reset_index()\n        print(df)\n\n    ax = sns.barplot(x=\"index\",y=column_name, data=df)\n    ax.set_title(mode+\" \"+column_name+\" \"+\"count\")","dc8e7f9f":"categorical_bar_plot(\"cat0\",mode=\"train\")","8526a46c":"categorical_bar_plot(\"cat0\",mode=\"test\")","d3c61c19":"categorical_bar_plot(\"cat1\",mode=\"train\")","34981d79":"categorical_bar_plot(\"cat1\",mode=\"test\")","38fb67cd":"categorical_bar_plot(\"cat2\",mode=\"train\")","ef7c17a3":"categorical_bar_plot(\"cat2\",mode=\"test\")","2d00face":"categorical_bar_plot(\"cat3\",mode=\"train\")","044b4c23":"categorical_bar_plot(\"cat3\",mode=\"test\")","c59b1f60":"categorical_bar_plot(\"cat4\",mode=\"train\")","620d8231":"categorical_bar_plot(\"cat3\",mode=\"test\")","1e0e8fa2":"categorical_bar_plot(\"cat3\",mode=\"train\")\nfig=plt.gcf()\nfig.set_size_inches((30,10))","d57ee5c4":"categorical_bar_plot(\"cat3\",mode=\"test\")\nfig=plt.gcf()\nfig.set_size_inches((30,10))","3d3cb1ff":"categorical_bar_plot(\"cat6\",mode=\"train\")","d38199ff":"categorical_bar_plot(\"cat6\",mode=\"test\")","71da22e5":"categorical_bar_plot(\"cat7\",mode=\"train\")\nfig=plt.gcf()\nfig.set_size_inches((30,10))","7d6e7370":"categorical_bar_plot(\"cat7\",mode=\"test\")\nfig=plt.gcf()\nfig.set_size_inches((30,10))","8e40648a":"categorical_bar_plot(\"cat8\",mode=\"train\")\nfig=plt.gcf()\nfig.set_size_inches((30,10))","460a0347":"categorical_bar_plot(\"cat8\",mode=\"test\")\nfig=plt.gcf()\nfig.set_size_inches((30,10))","bd5cab62":"categorical_bar_plot(\"cat9\",mode=\"train\")\nfig=plt.gcf()\nfig.set_size_inches((30,10))","3f86ca07":"categorical_bar_plot(\"cat9\",mode=\"test\")\nfig=plt.gcf()\nfig.set_size_inches((30,10))","1bdbf2d2":"categorical_bar_plot(\"cat10\",mode=\"train\")\nfig=plt.gcf()\nfig.set_size_inches((80,10))","abb94d92":"categorical_bar_plot(\"cat10\",mode=\"test\")\nfig=plt.gcf()\nfig.set_size_inches((80,10))","43a02c3f":"categorical_bar_plot(\"cat11\",mode=\"train\")","07a40a96":"categorical_bar_plot(\"cat11\",mode=\"test\")","1ed1ba1b":"categorical_bar_plot(\"cat12\",mode=\"train\")","b9fe5dc2":"categorical_bar_plot(\"cat12\",mode=\"test\")","236f0163":"categorical_bar_plot(\"cat13\",mode=\"train\")","2d1c839a":"categorical_bar_plot(\"cat13\",mode=\"test\")","d57caec4":"categorical_bar_plot(\"cat14\",mode=\"train\")","51abfa3d":"categorical_bar_plot(\"cat14\",mode=\"test\")","d6606dac":"categorical_bar_plot(\"cat15\",mode=\"train\")","1895b05a":"categorical_bar_plot(\"cat15\",mode=\"test\")","27cdc99f":"categorical_bar_plot(\"cat16\",mode=\"train\")","4e750b78":"categorical_bar_plot(\"cat16\",mode=\"test\")","73b0c2f7":"categorical_bar_plot(\"cat17\",mode=\"train\")","6c3f0714":"categorical_bar_plot(\"cat17\",mode=\"test\")","bed63ed1":"categorical_bar_plot(\"cat18\",mode=\"train\")","51c9570f":"categorical_bar_plot(\"cat18\",mode=\"test\")","124c1ea5":"\n### Checking whether there is any difference in unique values of category between train and test set\nfor i in range(0,19):\n    a=set(pd.DataFrame(train[\"cat{}\".format(str(i))].value_counts()).reset_index()['index'].tolist())\n    b=set(pd.DataFrame(test[\"cat{}\".format(str(i))].value_counts()).reset_index()['index'].tolist())\n    print((b^a,i))","cc9012c4":"### Cat_10 is a red_flag\ntemp=pd.DataFrame(train[\"cat10\"].value_counts()).reset_index()\nprint(\"Counts of categories present in train but not in test\")\nprint(temp[(temp[\"index\"].isin(['MW', 'LK', 'IL', 'JF', 'BS', 'MO', 'CH', 'FW', 'CX', 'GH', 'AW', 'MK']))][\"cat10\"])\ntemp=pd.DataFrame(test[\"cat10\"].value_counts()).reset_index()\nprint(\"Counts of categories present in test but not in train\")\nprint(temp[(temp[\"index\"].isin(['BU', 'BW', 'CA', 'DG', 'EJ', 'JM', 'KE', 'KM']))][\"cat10\"])\n### The count of the red flag values is really low therefore can be safetly replaced","310a5b4b":"THRESHOLD_COUNT=600 ### A category count must be above 30 in order to be counted as a separate category\nNEW_CAT=\"Z\"\ncolumn_categories_merged=[\"cat1\",\"cat2\",\"cat3\",\"cat4\",\"cat5\",\"cat6\",\"cat7\",\"cat8\",\"cat9\",\"cat10\"]\n\n\ndef merge_categories(cat_name):\n    df=pd.DataFrame(train[cat_name].value_counts()).reset_index()\n    cat_to_be_merged=df[(df[cat_name]<THRESHOLD_COUNT)][\"index\"].tolist()\n    if cat_name==\"cat10\":\n        cat_to_be_merged.extend(['BU', 'BW', 'CA', 'DG', 'EJ', 'JM', 'KE', 'KM'])\n    return (train[cat_name].replace(cat_to_be_merged,NEW_CAT),test[cat_name].replace(cat_to_be_merged,NEW_CAT))\nfor col in column_categories_merged:\n    train_new,test_new=merge_categories(col)\n    train[col]=train_new\n    test[col]=test_new\n","8aa73bf8":"train.shape,test.shape","ec81668a":"def contnious_plot(column_name):\n    f, axes = plt.subplots(1, 2,figsize=(10,5),sharey=True)\n    sns.kdeplot(data=train, x=column_name,ax=axes[0])\n    sns.kdeplot(data=test, x=column_name,ax=axes[1])  \n    axes[0].set_title(\"Train\")\n    axes[1].set_title(\"Test\")\n    plt.tight_layout()","aae134a3":"for i in range(0,11):\n    contnious_plot(\"cont{}\".format(str(i)))","71978c6f":"#### Quick check of Target Variable\ncategorical_bar_plot(\"target\",mode=\"train\")","bf768d24":"target=train[\"target\"]\ntrain=pd.get_dummies(train.drop(\"target\",1))\ntrain[\"target\"]=target\ntest=pd.get_dummies(test)","b589d879":"train.shape,test.shape ## After one hot encoding","919764b6":"# create a list of base-models\ndef get_models():\n    models = list()\n    models.append(DecisionTreeClassifier())\n    models.append(GaussianNB())\n    models.append(AdaBoostClassifier())\n    models.append(BaggingClassifier(n_estimators=10))\n    models.append(RandomForestClassifier(n_estimators=10))\n    models.append(ExtraTreesClassifier(n_estimators=10))\n    models.append(LGBMClassifier())\n    models.append(CatBoostClassifier())\n    return models","14f91fb1":"# collect out of fold predictions form k-fold cross validation\ndef get_out_of_fold_predictions(X, y, models):\n    meta_X, meta_y = list(), list()\n    # define split of data\n    kfold = KFold(n_splits=10, shuffle=True)\n    # enumerate splits\n    for train_ix, test_ix in kfold.split(X):\n        fold_yhats = list()\n        # get data\n        train_X, test_X = X[train_ix], X[test_ix]\n        train_y, test_y = y[train_ix], y[test_ix]\n        meta_y.extend(test_y)\n        # fit and make predictions with each sub-model\n        print(\"Training the Models\")\n        for model in tqdm(models):\n            model.fit(train_X, train_y)\n            yhat = model.predict_proba(test_X)\n            # store columns\n            fold_yhats.append(yhat)\n        # store fold yhats as columns\n        meta_X.append(hstack(fold_yhats))\n    return vstack(meta_X), np.array(meta_y)","5f0bd298":"# fit a meta model\ndef fit_meta_model(X, y):\n    model = LogisticRegression(solver='liblinear')\n    model.fit(X, y)\n    return model","a1ec8d83":"# fit all base models on the training dataset\ndef fit_base_models(X, y, models):\n    for model in models:\n        model.fit(X, y)\n        \n# evaluate a list of models on a dataset\ndef evaluate_models(X, y, models):\n    for model in models:\n        yhat = model.predict(X)\n        acc = accuracy_score(y, yhat)\n        print('%s: %.3f' % (model.__class__.__name__, acc*100))\n        \n# make predictions with stacked model\ndef super_learner_predictions(X, models, meta_model):\n    meta_X = list()\n    for model in tqdm(models):\n        yhat = model.predict_proba(X)\n        meta_X.append(yhat)\n    meta_X = hstack(meta_X)\n    # predict\n    return meta_model.predict(meta_X)","8905399d":"X=train.drop(\"target\",1).values\ny=train[\"target\"].values\nX, X_val, y, y_val = train_test_split(X, y, test_size=0.50,stratify=y)\n\n# get models\nmodels = get_models()\n# get out of fold predictions\nmeta_X, meta_y = get_out_of_fold_predictions(X, y, models)\nprint('Meta ', meta_X.shape, meta_y.shape)\n# fit base models\nfit_base_models(X, y, models)\n","56ffafef":"# fit the meta model\nmeta_model = fit_meta_model(meta_X, meta_y)\n# evaluate base models\nevaluate_models(X_val, y_val, models)\n# evaluate meta model\nyhat = super_learner_predictions(X_val, models, meta_model)\nprint('Super Learner: %.3f' % (accuracy_score(y_val, yhat) * 100))","b1c6002c":"final_prediction = super_learner_predictions(test.values, models, meta_model)","7d94e0aa":"submission = pd.read_csv('\/kaggle\/input\/tabular-playground-series-mar-2021\/sample_submission.csv')\nsubmission[\"target\"]=final_prediction\nsubmission.to_csv('submission.csv', index=False)","31a2a6c2":"# Super Learner\n## I follwed the following implementation:\n[Super Learner](https:\/\/machinelearningmastery.com\/super-learner-ensemble-in-python\/)\n\n![Super Learner Overview](https:\/\/machinelearningmastery.com\/wp-content\/uploads\/2019\/10\/Diagram-Showing-the-Data-Flow-of-the-Super-Learner-Algorithm.png)","5bd18e4b":"## Checking Categorical Columns","20034a31":"## Checking continuous Columns","a03f93c4":"### Simple One-Hot-Encoding","378bc571":"### Following columns are identified which have alot of unique categories therefore following columns categories will be merged according to some threshold. \n1.  cat1\n2.  cat2\n3.  cat3\n4.  cat4\n5.  cat5\n6.  cat6\n7.  cat7\n8.  cat8\n9.  cat9\n10. cat10\n### For example Before and After Value Counts are shown for Cat9:\n#### Before\n![1.JPG](attachment:1.JPG)\n\n #### After\n![2%20%282%29.JPG](attachment:2%20%282%29.JPG)\n","dd61e665":"### Just analysing the distribution of each continuous column","915a5d8a":"### Drawing a Barplot and analysing the value counts for each unique category. If alot of unique values then will merge some columns according to a decided a threshold of counts"}}