{"cell_type":{"fd50d85b":"code","0fa91f5b":"code","9838be1a":"code","135938bc":"code","b579288b":"code","354b3c91":"code","93e6a54b":"code","9aba8d31":"code","6fd4e312":"code","65a89ac4":"code","632951ca":"code","f31302a5":"code","c9a315d9":"code","f10d200c":"markdown","09900280":"markdown","f05475f4":"markdown","008de157":"markdown","8ab8515f":"markdown","29d9e30c":"markdown","71531adc":"markdown","02dd7cd2":"markdown","7e3730d0":"markdown","7586646e":"markdown","3124c5d0":"markdown","0d7be350":"markdown","d6a3c46a":"markdown"},"source":{"fd50d85b":"import cv2\nimport numpy as np\nimport os\n\nimport datetime\nfrom skimage import io\nimport os\nimport random\nimport matplotlib.pyplot as plt\n% matplotlib inline\nimport glob","0fa91f5b":"# # Starts capturing video\n# cap = cv2.VideoCapture(0)\n\n# fps = int(cap.get(cv2.CAP_PROP_FPS))\n\n# print(\"Frames per second using video.get(cv2.CAP_PROP_FPS) : {0}\".format(fps))\n\n# while cap.isOpened():\n#     ret, frame = cap.read()\n#     cv2.imshow('Captured Frame', frame)\n#     if cv2.waitKey(1) == ord('q'):\n#         break\n\n#     keypress = cv2.waitKey(1) & 0xFF\n\n# cap.release()\n# cv2.destroyAllWindows()\n","9838be1a":"#---------------------------------------------\n# To segment the region of hand in the image\n#---------------------------------------------\ndef segment(image, threshold=25):\n    global bg\n    # find the absolute difference between background and current frame\n    diff = cv2.absdiff(bg.astype(\"uint8\"), image)\n\n    # threshold the diff image so that we get the foreground\n    thresholded = cv2.threshold(diff, threshold, 255, cv2.THRESH_BINARY)[1]\n\n    # get the contours in the thresholded image\n    (cnts, _) = cv2.findContours(thresholded.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n    # return None, if no contours detected\n    if len(cnts) == 0:\n        return\n    else:\n        # based on contour area, get the maximum contour which is the hand\n        segmented = max(cnts, key=cv2.contourArea)\n        return (thresholded, segmented)","135938bc":"import keras\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, BatchNormalization\nfrom keras.layers import Activation, Dropout\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras import backend as K\nfrom keras.optimizers import Adam\n\n\n# model\nmodel = Sequential()\n\n# first conv layer\n# input shape = (img_rows, img_cols, 1)\nmodel.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(100,120, 1))) \nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n# second conv layer\nmodel.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n# flatten and put a fully connected layer\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu')) # fully connected\nmodel.add(Dropout(0.5))\n\n# softmax layer\nmodel.add(Dense(6, activation='softmax'))\n\n# model summary\noptimiser = Adam() \nmodel.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nmodel.summary()","b579288b":"# Loading the Dataset\n\nDATASET_PATH = '\/kaggle\/input\/hand-gesture-recog-dataset\/data'\n\ndataset_path = os.path.join(DATASET_PATH, '*')\nimport glob\ndataset_path = glob.glob(dataset_path)\ndataset_path","354b3c91":"import cv2\nimage = cv2.imread('\/kaggle\/input\/hand-gesture-recog-dataset\/data\/five\/hand1(1015).jpg')\nimage = cv2.resize(image,(100, 120))\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))","93e6a54b":"loaded_images = []\n\nlist_of_gestures = ['blank', 'ok', 'thumbsup', 'thumbsdown', 'fist', 'five']\n\nfor path in range(0, len(dataset_path)):\n    dataset_path = \"\/kaggle\/input\/hand-gesture-recog-dataset\/data\/\" + str(list_of_gestures[path])\n    gesture_path = os.path.join(dataset_path, '*')\n    import glob\n    gest_path = glob.glob(gesture_path)\n    k = 0\n    for i in range(0, len(gest_path)):\n        if k < 1600:\n            image = cv2.imread(gest_path[i])\n            gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n            gray_image = cv2.resize(gray_image,(100, 120))\n            loaded_images.append(gray_image)\n        k=k+1\nprint(len(loaded_images))\n\noutputVectors = []\nfor i in range(1, 1601):\n    outputVectors.append([1, 0, 0, 0, 0, 0])\n\nfor i in range(1, 1601):\n    outputVectors.append([0, 1, 0, 0, 0, 0])\n\nfor i in range(1, 1601):\n    outputVectors.append([0, 0, 1, 0, 0, 0])\n    \nfor i in range(1, 1601):\n    outputVectors.append([0, 0, 0, 1, 0, 0])\n    \nfor i in range(1, 1601):\n    outputVectors.append([0, 0, 0, 0, 1, 0])\n\nfor i in range(1, 1601):\n    outputVectors.append([0, 0, 0, 0, 0, 1])\n\nprint(len(outputVectors))","9aba8d31":"X = np.asarray(loaded_images)\ny = np.asarray(outputVectors)\nprint(X.shape)\nprint(y.shape)","6fd4e312":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)\nX_train = X_train.reshape(X_train.shape[0], 100, 120, 1)\nX_test = X_test.reshape(X_test.shape[0], 100, 120, 1)\nprint(X_train.shape)\nprint(X_test.shape)","65a89ac4":"# Training the model with data\nmodel.fit(X_train, y_train,\n          batch_size=128,\n          epochs=10,\n          verbose=1,\n          validation_data=(X_test, y_test))\n\n# model.save(\"hand_gesture_recognition.h5\")","632951ca":"[loss, acc] = model.evaluate(X_test,y_test,verbose=1)\nprint(\"Accuracy: \" + str(acc))","f31302a5":"# # load Model Weights\n\n# def _load_weights():\n#     try:\n#         model = load_model(\"hand_gesture_recog_model.h5\")\n#         print(model.summary())\n#         # print(model.get_weights())\n#         # print(model.optimizer)\n#         return model\n#     except Exception as e:\n#         return None\n\n\n    \n# def getPredictedClass(model):\n\n#     image = cv2.imread('Temp.png')\n#     gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n#     gray_image = cv2.resize(gray_image, (100, 120))\n\n#     gray_image = gray_image.reshape(1, 100, 120, 1)\n\n#     prediction = model.predict_on_batch(gray_image)\n\n#     predicted_class = np.argmax(prediction)\n#     if predicted_class == 0:\n#         return \"Blank\"\n#     elif predicted_class == 1:\n#         return \"OK\"\n#     elif predicted_class == 2:\n#         return \"Thumbs Up\"\n#     elif predicted_class == 3:\n#         return \"Thumbs Down\"\n#     elif predicted_class == 4:\n#         return \"Punch\"\n#     elif predicted_class == 5:\n#         return \"High Five\"\n\n\n# if __name__ == \"__main__\":\n#     # initialize accumulated weight\n#     accumWeight = 0.5\n\n#     # get the reference to the webcam\n#     camera = cv2.VideoCapture(0)\n\n#     fps = int(camera.get(cv2.CAP_PROP_FPS))\n#     # region of interest (ROI) coordinates\n#     top, right, bottom, left = 10, 350, 225, 590\n#     # initialize num of frames\n#     num_frames = 0\n#     # calibration indicator\n#     calibrated = False\n#     model = _load_weights()\n#     k = 0\n#     # keep looping, until interrupted\n#     while (True):\n#         # get the current frame\n#         (grabbed, frame) = camera.read()\n\n#         # resize the frame\n#         frame = cv2.resize(frame, (700,700))\n#         # flip the frame so that it is not the mirror view\n#         frame = cv2.flip(frame, 1)\n\n#         # clone the frame\n#         clone = frame.copy()\n\n#         # get the height and width of the frame\n#         (height, width) = frame.shape[:2]\n\n#         # get the ROI\n#         roi = frame[top:bottom, right:left]\n\n#         # convert the roi to grayscale and blur it\n#         gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n#         gray = cv2.GaussianBlur(gray, (7, 7), 0)\n\n#         # to get the background, keep looking till a threshold is reached\n#         # so that our weighted average model gets calibrated\n#         if num_frames < 30:\n#             run_avg(gray, accumWeight)\n#             if num_frames == 1:\n#                 print(\"[STATUS] please wait! calibrating...\")\n#             elif num_frames == 29:\n#                 print(\"[STATUS] calibration successfull...\")\n#         else:\n#             # segment the hand region\n#             hand = segment(gray)\n\n#             # check whether hand region is segmented\n#             if hand is not None:\n#                 # if yes, unpack the thresholded image and\n#                 # segmented region\n#                 (thresholded, segmented) = hand\n\n#                 # draw the segmented region and display the frame\n#                 cv2.drawContours(clone, [segmented + (right, top)], -1, (0, 0, 255))\n\n#                 # count the number of fingers\n#                 # fingers = count(thresholded, segmented)\n#                 if k % (fps \/ 6) == 0:\n#                     cv2.imwrite('Temp.png', thresholded)\n#                     predictedClass = getPredictedClass(model)\n#                     cv2.putText(clone, str(predictedClass), (70, 45), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n\n#                 # show the thresholded image\n#                 cv2.imshow(\"Thesholded\", thresholded)\n#         k = k + 1\n#         # draw the segmented hand\n#         cv2.rectangle(clone, (left, top), (right, bottom), (0, 255, 0), 2)\n\n#         # increment the number of frames\n#         num_frames += 1\n\n#         # display the frame with segmented hand\n#         cv2.imshow(\"Video Feed\", clone)\n\n#         # observe the keypress by the user\n#         keypress = cv2.waitKey(1) & 0xFF\n\n#         # if the user pressed \"q\", then stop looping\n#         if keypress == ord(\"q\"):\n#             break\n\n#     # free up memory\n#     camera.release()\n#     cv2.destroyAllWindows()\n","c9a315d9":"from IPython.display import Video\n\nVideo('\/kaggle\/input\/hand-gesture-video\/compress_Hand_Gesture_2.mp4', embed=True)","f10d200c":"### Motion Detection and Thresholding\n\nTo detect the hand region from this difference image, we need to threshold the difference image, so that only our hand region \nbecomes visible and all the other unwanted regions are painted as black. This is what Motion Detection is all about.\n\n\nThresholding is the assigment of pixel intensities to 0's and 1's based a particular threshold level so that our \nobject of interest alone is captured from an image.\n\n","09900280":"### 2D CNN Model","f05475f4":"# WebCam Live feed to predict Hand Gestures","008de157":"### Repository containing complete code for above implementation.\nhttps:\/\/github.com\/sarjit07\/Hand-Gestures-Recognition\n\n\n\n- Hoped you liked my notebook (upvote top right, or comment), my way to conribute back to this fantastic Kaggle platform and community.\n- Author - Arjit Sharma","8ab8515f":"### Get the thresholded image from live feed and use model to predict the hand gesture in image\n\n## Run LIVE FEED","29d9e30c":"### Segmenting Hand Region \n* ![image.png](attachment:image.png)\n\n","71531adc":"### Now, Having segmented the hand region from the live video sequence, we do the following:\n1. Create a 2D Convolutional Neural Network Model using keras\n2. Train the CNN model using the different gestures dataset\n3. Get the thresholded image from live feed and use model to predict the hand gesture in image","02dd7cd2":"### Training the model with the dataset","7e3730d0":"### Contour Extraction\n\nAfter thresholding the difference image, we find contours in the resulting image. The contour with the largest area is assumed to be our hand.\n\n#### Note: Contour is the outline or boundary of an object located in an image.\n\n","7586646e":"# Segment the Hand region\n\nThe first step in hand gesture recognition is obviously to find the hand region by eliminating all the other unwanted portions in the video sequence. \n\nIn order to get the segment, we do three things \n\n- Background Subtraction\n- Motion Detection and Thresholding\n- Contour Extraction\n","3124c5d0":"### Background Subtraction\n\nBackground subtraction (BS) is a common and widely used technique for generating a foreground mask \n(namely, a binary image containing the pixels belonging to moving objects in the scene) by using static cameras.\nIt calculates the foreground mask performing a subtraction between the current frame and a background model, \ncontaining the static part of the scene or, more in general, everything that can be considered as background \ngiven the characteristics of the observed scene.\n\n\n![image.png](attachment:image.png)\n\n\nAfter figuring out the background model using running averages, we use the current frame which holds the foreground object (hand in our case) in addition to the background. We calculate the absolute difference between the background model (updated over time) and the current frame (which has our hand) to obtain a difference image that holds the newly added foreground object (which is our hand). This is what Background Subtraction is all about.","0d7be350":"# Overview\n\n\nWe are going to recognize hand gestures from a video sequence. To recognize these gestures from a live video sequence, we need to do three things...\n1.  We, first need to segment the hand region alone removing all the unwanted portions in the video sequence. \n2.  After segmenting the hand region, we then pass that region to the trained 2D CNN model, to predict the hand gesture.\n\n![image.png](attachment:image.png)","d6a3c46a":"# PART 2 - Train and predict hand gesture with 2D CNN Model"}}