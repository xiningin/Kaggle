{"cell_type":{"d9251ab2":"code","fac42884":"code","ef06fbad":"code","c29b70b4":"code","fd983fd4":"code","5efaddd0":"code","16fef78a":"markdown"},"source":{"d9251ab2":"import torch\nimport torch.nn as nn","fac42884":"# Defining input size, hidden layer size, output size and batch size respectively\nn_in, n_h, n_out, batch_size = 10, 5, 1, 10","ef06fbad":"# Create dummy input and target tensors (data)\nx = torch.randn(batch_size, n_in)\ny = torch.tensor([[1.0], [0.0], [0.0], [1.0], [1.0], [1.0], [0.0], [0.0], [1.0], [1.0]])","c29b70b4":"# Create a model\nmodel = nn.Sequential(nn.Linear(n_in, n_h),\n                     nn.ReLU(),\n                     nn.Linear(n_h, n_out),\n                     nn.Sigmoid())","fd983fd4":"# Construct the loss function\ncriterion = torch.nn.MSELoss()\n\n# Construct the optimizer (Stochastic Gradient Descent in this case)\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)","5efaddd0":"# Gradient Descent\nfor epoch in range(50):\n    # Forward pass: Compute predicted y by passing x to the model\n    y_pred = model(x)\n\n    # Compute and print loss\n    loss = criterion(y_pred, y)\n    print('epoch: ', epoch,' loss: ', loss.item())\n\n    # Zero gradients, perform a backward pass, and update the weights.\n    optimizer.zero_grad()\n    \n    # perform a backward pass (backpropagation)\n    loss.backward()\n    \n    # Update the parameters\n    optimizer.step()","16fef78a":"A simple tutorial to create a simple neural network in PyTorch."}}