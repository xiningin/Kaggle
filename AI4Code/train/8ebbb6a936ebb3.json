{"cell_type":{"cc700563":"code","c4c02444":"code","d5e8310f":"code","e08b6647":"code","ae622f94":"code","9b974f0b":"code","7e112fda":"code","2aa4037e":"code","9c5cef1b":"code","9cc5759a":"code","78d0349d":"code","8e26b74e":"code","ad6c5ea8":"code","925aaeb1":"code","200e6564":"code","3fe4c584":"code","d9a77765":"code","d256b5cb":"code","013ee493":"code","f071ab86":"code","c3775b04":"code","be05d7ef":"code","ebb6ced6":"code","97de8045":"code","7d3e1d74":"code","a18b324b":"code","8755be31":"code","306cfdf6":"code","78bbd5d4":"code","2d5ac975":"code","6a06b814":"code","642fa7a1":"code","fffab71b":"code","c26aa9a8":"code","74d87735":"markdown"},"source":{"cc700563":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/spam-or-not-spam-dataset\/spam_or_not_spam.csv'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c4c02444":"import pandas as pd\nimport numpy as np\nimport re\nimport unicodedata\n\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\n\nfrom textblob import TextBlob\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n#model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.metrics import classification_report\n\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.over_sampling import SMOTE \nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.multiclass import OneVsRestClassifier\n\n#plots\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\n\nimport cufflinks as cf\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\n\n\n#loading small corpus\nnlp = spacy.load('en_core_web_sm')\n\n#get data\ndf = pd.read_csv(\"\/kaggle\/input\/spam-or-not-spam-dataset\/spam_or_not_spam.csv\")\n\n\nprint(\"Setup Done!\")","d5e8310f":"df.head()","e08b6647":"#list of contractions and their related expansions (from web)\ncontractions = { \n\"ain't\": \"am not\",\n\"aren't\": \"are not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he will\",\n\"he'll've\": \"he will have\",\n\"he's\": \"he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how does\",\n\"i'd\": \"i would\",\n\"i'd've\": \"i would have\",\n\"i'll\": \"i will\",\n\"i'll've\": \"i will have\",\n\"i'm\": \"i am\",\n\"i've\": \"i have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it would\",\n\"'ll\": \"will\",\n\"'ve\": \"have\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it will\",\n\"it'll've\": \"it will have\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she will\",\n\"she'll've\": \"she will have\",\n\"she's\": \"she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so is\",\n\"that'd\": \"that would\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that is\",\n\"there'd\": \"there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there is\",\n\"they'd\": \"they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they will\",\n\"they'll've\": \"they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\" u \": \" you \",\n\" ur \": \" your \",\n\" n \": \" and \",\n\"tbh\":\"to be honest\" }","ae622f94":"df.isna().sum()","9b974f0b":"df = df.dropna()","7e112fda":"df.isna().sum()","2aa4037e":"df.info()","9c5cef1b":"df[\"label\"].value_counts()","9cc5759a":"df.info()","78d0349d":"def get_avg_word_len(x):\n    \"\"\"Get the average word length from a given sentence\n    param x(str): the sentence of whose word length is to be taken\n    return leng(numeric): the average word length \"\"\"\n\n    words = x.split()\n    word_len = 0\n    for word in words:\n        word_len = word_len + len(word)\n    return word_len\/len(words)\n\ndef feature_extract(df,d):\n    \"\"\"Adds new columns in the given df, from the existing data\n    count: number of words in the document (df[d])\n    char count: number of characters in df[d]\n    avg word_len: the average number of characters in the df[d]\n    stop_words_len: number of stopwords present\n    numeric_count: number of numeric characters present\n    upper_counts: number of words in CAPS LOCK\n    polarity: sentiment of the word, from -1(negative) to 1(positive)\n    \n    param df(dataframe): dataframe on which manipulation is to be done\n    param d(str): column name in which the reuired words are present\"\"\"\n    \n    df['count']=df[d].apply(lambda x: len(str(x).split()))\n    df['char count']=df[d].apply(lambda x: len(x))\n    #df['avg word_len'] = df[d].apply(lambda x:get_avg_word_len(x))\n    df['stop_words_len'] = df[d].apply(lambda x: len([t for t in x.split() if t in STOP_WORDS]))\n    df['numeric_count'] = df[d].apply(lambda x:len([t for t in x.split()if t.isdigit()] ))\n    df['upper_counts'] = df[d].apply(lambda x: len([t for t in x.split() if t.isupper() and len(x)>3]))\n    df['polarity'] = df[d].map(lambda text: TextBlob(text).sentiment.polarity)\n    \n#calling the function\nfeature_extract(df,'email')\nf = df\n\nprint(\"Extraction complete: \")\nprint(\"Number of words, characters, stop words, numeric characters, upper case characters and polarity extracted\")","8e26b74e":"df.head()","ad6c5ea8":"def expand(x):\n    \"\"\"Some of the words like 'i'll', are expanded to 'i will' for better text processing\n    The list of contractions is taken from the internet\n    \n    param x(str): the sentence in which contractions are to be found and expansions are to be done\n    \n    return x(str): the expanded sentence\"\"\"\n    if type(x)== str:\n        for key in contractions:\n            value = contractions[key]\n            x = x.replace(key,value)\n        return x\n    else:\n        return x\n\ndef remove_accented_chars(x):\n    \"\"\"The function changes the accented characters into their equivalent normal form,\n    to do so, normalize function with 'NFKD' is used which replaces the compatibility characters into\n    theri euivalent\n    \n    param x(str): the sentence in which accented characters are to be detected and removes\n    return x(str): sentence with accented characters replaced by their equivalent\"\"\"\n    \n    x = unicodedata.normalize('NFKD', x).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    return x\n\n\ndef make_to_base(x):\n    \"\"\"Converting the words to their base word and dictionary head word i.e to lemmatize\n    param x(str): the sentence in which the words are to be converted (lemmatization)\n    return x(str): the lemmatized sentence\"\"\"\n    \n    x_list = []\n    doc = nlp(x)\n    \n    for token in doc:\n        lemma = str(token.lemma_)\n        \n        #in spacy, words like I, you are lemmatized as -PRON- and are,and etc are lemmatized to be,\n        #since these words are present widely, we keep them as the original. \n        #Anyways the words will be removed as stop words later\n        \n        if lemma == '-PRON-' or lemma == 'be':\n            lemma = token.text\n        x_list.append(lemma)\n    return (\" \".join(x_list))\n\n","925aaeb1":"def preprocess(df,d):\n    \"\"\"Preprocesses the given document by applying the following functionalities\n    lower: lowers all the characters for uniformity\n    expansion: expands words like i'll to i will for better text classification\n    remove special characters: using regex, removes all the punctuations etc\n    remove space: removes trailing spaces and extra spaces between words\n    remove accented characters: change accented characters to its normal equivalent\n    remove stop words: removes the stop words in the sentence\n    lemmatization: changes the words to their base form\"\"\"\n    \n    df[d] = df[d].apply(lambda x: x.lower())\n    df[d] = df[d].apply(expand)\n    df[d] = df[d].apply(lambda x: re.sub('[^A-Z a-z 0-9-]+', '', x))\n    df[d] = df[d].apply(lambda x: \" \".join(x.split()))\n    df[d] = df[d].apply(lambda x: remove_accented_chars(x))\n    df[d] = df[d].apply(lambda x: make_to_base(x))\n    df[d] = df[d].apply(lambda x: \" \".join([t for t in x.split() if t not in STOP_WORDS]))","200e6564":"#calling the function\npreprocess(df,'email')\n\nprint(\"Pre processing done!\")","3fe4c584":"df.head()","d9a77765":"df['polarity'].iplot(\n    kind='hist',\n    bins=50,\n    xTitle='polarity',\n    linecolor='black',\n    yTitle='count',\n    title='Sentiment Polarity Distribution')","d256b5cb":"df[df['label']==0]['polarity'].iplot(\n    kind='hist',\n    bins=50,\n    xTitle='polarity',\n    linecolor='black',\n    yTitle='count',\n    title='Spam Sentiment Polarity Distribution')","013ee493":"df['count'].iplot(\n    kind='hist',\n    bins=100,\n    xTitle='review length',\n    linecolor='black',\n    yTitle='count',\n    title='Email Text Length Distribution')","f071ab86":"df[df['label']==0]['count'].iplot(\n    kind='hist',\n    bins=100,\n    xTitle='review length',\n    linecolor='black',\n    yTitle='count',\n    title='Spam Email Text Length Distribution')","c3775b04":"fig = plt.figure(figsize=(20,8))\n\ntext = ' '.join(f.loc[f['count']<=199,'email'].values)\nwc = WordCloud(width=1000, \n                   height=1000, \n                   random_state=1, \n                   background_color='Black',\n                   colormap='Set2',\n                   collocations=False).generate(text)\n\nplt.imshow(wc)\nplt.axis(\"off\");","be05d7ef":"fig = plt.figure(figsize=(20,8))\n\ntext = ' '.join(f.loc[f['label']==1,'email'].values)\nwc = WordCloud(width=1000, \n                   height=1000, \n                   random_state=1, \n                   background_color='Black',\n                   colormap='Set2',\n                   collocations=False).generate(text)\n\nplt.imshow(wc)\nplt.axis(\"off\");","ebb6ced6":"fig = plt.figure(figsize=(20,8))\n\ntext = ' '.join(f.loc[f['label']==0,'email'].values)\nwc = WordCloud(width=1000, \n                   height=1000, \n                   random_state=1, \n                   background_color='Black',\n                   colormap='Set2',\n                   collocations=False).generate(text)\n\nplt.imshow(wc)\nplt.axis(\"off\");","97de8045":"df['char count'].iplot(\n    kind='hist',\n    bins=1000,\n    xTitle='review length',\n    linecolor='black',\n    yTitle='count',\n    title='Ham Email Character Length Distribution')","7d3e1d74":"df[df['label']==0]['char count'].iplot(\n    kind='hist',\n    bins=100,\n    xTitle='review length',\n    linecolor='black',\n    yTitle='count',\n    title='spam email Character Length Distribution')","a18b324b":"import seaborn as sns","8755be31":"sns.countplot(x ='label', data = df)","306cfdf6":"df['label'].iplot(\n    kind='bar',\n    xTitle='Label',\n    linecolor='black',\n    yTitle='count',\n    title='Spam Ham class')","78bbd5d4":"#splitting the data\nX = df[\"email\"].values\ny = df[\"label\"].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=345, stratify=y)","2d5ac975":"# Define a pipeline combining a text feature extractor with multi label classifier\nNB_pipeline = Pipeline([\n                ('tfidf', TfidfVectorizer(stop_words=STOP_WORDS)),\n                ('smote', SMOTE(random_state=12)),\n                ('clf', BernoulliNB(alpha=10 ** -6)),\n            ])","6a06b814":"NB_pipeline.fit(X_train, y_train)\ny_test_pred = NB_pipeline.predict(X_test)\nprint(classification_report(y_test, y_test_pred))","642fa7a1":"from sklearn.svm import LinearSVC","fffab71b":"# Define a pipeline combining a text feature extractor with multi lable classifier\nSVM_pipeline = Pipeline([\n                ('tfidf', TfidfVectorizer(stop_words=STOP_WORDS)),\n                ('smote', SMOTE(random_state=12)),\n                ('clf', LinearSVC()),\n            ])","c26aa9a8":"SVM_pipeline.fit(X_train, y_train)\ny_test_pred = SVM_pipeline.predict(X_test)\nprint(classification_report(y_test, y_test_pred))","74d87735":"# Visualize"}}