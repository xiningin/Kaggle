{"cell_type":{"e8fa7b06":"code","754e089f":"code","c185c29d":"code","f7f3be1a":"code","e2f9587b":"code","5135b75d":"code","493c92ab":"code","360fc2ea":"code","cb826c2c":"code","e766b835":"code","6f86ceb3":"code","a0216fd3":"code","aed39fa3":"code","d876bb5b":"code","6b83f3df":"code","e5e60869":"code","6e474990":"code","d857a174":"code","f546a6c7":"code","6413120e":"code","d285e8ca":"code","c3e352be":"code","b20ffafb":"markdown","62e0ccf0":"markdown","686718a3":"markdown"},"source":{"e8fa7b06":"import gc\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\nfrom xgboost import XGBClassifier\n\n\ngc.enable()","754e089f":"TestRun = False\n\ntest_config = {\n    \"vocab_size\"    :200,\n    \"embedding_dim\" :20,\n    \"max_length\"    :50,\n    \"max_tfidf_features\" :500,\n    \"max_ngrams\" :1,\n    \"n_epochs\": 3}\n\nmain_config = {\n    \"vocab_size\"    :6000,\n    \"embedding_dim\" :300,\n    \"max_length\"    :6000,\n    \"max_tfidf_features\" :2000,\n    \"max_ngrams\" :1,\n    \"n_epochs\": 12}\n\nif TestRun:\n    config = test_config\nelse:\n    config = main_config\n    ","c185c29d":"train_df = pd.read_csv(\"\/kaggle\/input\/fake-news\/train.csv\")\nprint(train_df.shape)\ntrain_df.head()","f7f3be1a":"test_df = pd.read_csv(\"\/kaggle\/input\/fake-news\/test.csv\")\nprint(test_df.shape)\ntest_df.head()","e2f9587b":"train_df[\"full_text\"] = train_df[\"title\"] + \" \" + train_df[\"author\"] + \" \" + train_df[\"text\"]\ntest_df[\"full_text\"] = test_df[\"title\"] + \" \" + test_df[\"author\"] + \" \" + test_df[\"text\"]\n\ntext_lengths = train_df[\"full_text\"].astype(str).apply(len)\ntext_lengths.plot(kind=\"hist\", bins=200)\nplt.yscale(\"log\")\ntext_lengths.describe()","5135b75d":"# %%time\n\n# transformer = TfidfVectorizer(ngram_range=(1, config[\"max_ngrams\"]), max_features=config[\"max_tfidf_features\"])\n# transformer.fit(train_df['full_text'].astype(str).values)\n# tfidf = transformer.transform(train_df['full_text'].astype(str).values)\n# print(tfidf.shape)\n\n# test_tfidf = transformer.transform(test_df['full_text'].astype(str).values)\n# print(test_tfidf.shape)","493c92ab":"sentences = train_df[\"full_text\"].astype(str).tolist()\ntest_sentences = test_df[\"full_text\"].astype(str).tolist()\nlabels = train_df[\"label\"].values","360fc2ea":"%%time\n\ntokenizer = Tokenizer(oov_token = \"<OOV>\", num_words=config[\"vocab_size\"])\ntokenizer.fit_on_texts(sentences)\nword_index = tokenizer.word_index\nprint(len(word_index))\n\nsequences = tokenizer.texts_to_sequences(sentences)\npadded = pad_sequences(sequences, padding = 'post', maxlen=config[\"max_length\"])\nprint(padded[0])\nprint(padded.shape)\n\ntest_sequences = tokenizer.texts_to_sequences(test_sentences)\ntest_padded = pad_sequences(test_sequences, padding = 'post', maxlen=config[\"max_length\"])\nprint(test_padded[0])\nprint(test_padded.shape)","cb826c2c":"# model with multiple inputs: token sequence, tfidf\ndef get_model():\n\n    keras.backend.clear_session()\n\n    input1 = keras.layers.Input(shape=(config[\"max_length\"],))\n    x1 = keras.layers.Embedding(input_dim=config[\"vocab_size\"], output_dim=config[\"embedding_dim\"], input_length=config[\"max_length\"])(input1)\n    x1 = keras.layers.Conv1D(128, 3, 2)(x1)\n    x1 = keras.layers.MaxPool1D(2)(x1)\n    x1 = keras.layers.Conv1D(64, 3, 2)(x1)\n    x1 = keras.layers.MaxPool1D(2)(x1)\n    x1 = keras.layers.Flatten()(x1)\n   \n    x = keras.layers.Dropout(0.5)(x1)\n    x = keras.layers.Dense(64, activation = 'relu')(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Dense(32, activation = 'relu')(x)\n    x = keras.layers.Dense(2, activation = 'softmax')(x)\n\n    model = keras.Model([input1],[x])\n    return model\n\nmodel = get_model()\nmodel.summary()","e766b835":"np.random.seed(1291295)\nX_train, X_test, y_train, y_test = train_test_split(padded, keras.utils.to_categorical(labels), test_size=0.2)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)","6f86ceb3":"callbacks=[\n    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=15, \n                                  verbose=1, mode=\"min\", restore_best_weights=True),\n    keras.callbacks.ModelCheckpoint(filepath=\"best_model.hdf5\", verbose=1, save_best_only=True),\n    keras.callbacks.ReduceLROnPlateau(factor=0.7, verbose=1, patience=3, min_delta=0.0001)\n]\n\nnp.random.seed(1291295)\nmodel = get_model()\nmodel.compile(loss=tf.keras.losses.categorical_crossentropy,\n                    optimizer=tf.keras.optimizers.Adam(lr=0.01),\n                    metrics=[keras.metrics.AUC(), keras.metrics.Accuracy()])\n\nhistory = model.fit([X_train], y_train, epochs=config[\"n_epochs\"],\n                    batch_size=64, validation_data=([X_val], y_val), callbacks=callbacks)","a0216fd3":"model = keras.models.load_model('best_model.hdf5')","aed39fa3":"metric_toplot = \"loss\"\nplt.plot(history.epoch, history.history[metric_toplot], \".:\", label=\"loss\")\nplt.plot(history.epoch, history.history[\"val_\"+metric_toplot], \".:\", label=\"val_loss\")\nplt.legend()","d876bb5b":"encoder = keras.models.Sequential(\n    model.layers[:-3]\n)\nencoder.summary()","6b83f3df":"X_train_enc = encoder.predict([X_train])\nX_val_enc = encoder.predict([X_val])\nX_test_enc = encoder.predict([X_test])","e5e60869":"%%time\n\nxgb = XGBClassifier(n_estimators=1000, max_depth=4, verbosity=1,\n                   n_jobs=2, colsample_bytree=0.7, random_state=1291,\n                   min_child_weight=5)\nxgb.fit(X_train_enc.reshape(len(X_train_enc),-1), y_train.argmax(1))\n\ntrain_score = xgb.score(X_train_enc.reshape(len(X_train_enc),-1), y_train.argmax(1))\nval_score = xgb.score(X_val_enc.reshape(len(X_val_enc),-1), y_val.argmax(1))\ntest_score = xgb.score(X_test_enc.reshape(len(X_test_enc),-1), y_test.argmax(1))\n\ntrain_score, val_score, test_score ","6e474990":"y_test_pred = xgb.predict(X_test_enc.reshape(len(X_test_enc),-1))","d857a174":"cr = classification_report(y_test[:,1], y_test_pred)\nprint(cr)","f546a6c7":"X_enc = encoder.predict([test_padded])\ny_submit_pred = xgb.predict(X_enc.reshape(len(X_enc),-1))\n","6413120e":"submit_df = pd.DataFrame({\"id\":test_df[\"id\"],\n                         \"label\":y_submit_pred})\nsubmit_df.head()","d285e8ca":"submit_df.to_csv(\"submit.csv\", index=False)","c3e352be":"!head submit.csv","b20ffafb":"# Split","62e0ccf0":"# Define Model","686718a3":"# Tokenize"}}