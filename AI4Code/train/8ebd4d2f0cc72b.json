{"cell_type":{"d9ba9f25":"code","d38ffa14":"code","829d4b94":"code","3ccaef22":"code","dd37a7dc":"code","e98ab093":"code","d30fe60f":"code","6a0d7031":"code","15632c3f":"code","460c59ef":"code","312a8df0":"code","2724c166":"code","a36d2269":"code","f7572c14":"code","74935924":"code","11170458":"code","4f3dede8":"code","eeef8ee5":"code","35608eb8":"code","e2c44811":"code","d5f333fe":"code","e28ec5d9":"code","d9815cd1":"code","a53234f5":"code","ae4c0ee1":"code","8a5f327b":"code","79cd8555":"code","73395a85":"code","c4da59ee":"markdown","b5d4abe4":"markdown","880ee695":"markdown","f8183c5c":"markdown","b5c8a679":"markdown","bee80542":"markdown","dce9de10":"markdown","e92b396f":"markdown","cf520a9f":"markdown","ae470413":"markdown","8b47877b":"markdown","e99c98d9":"markdown"},"source":{"d9ba9f25":"# data manipulation \nimport numpy as np\nimport pandas as pd\n\n# data visualization \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import learning_curve\n%matplotlib inline\n\n# preprocesing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_validate\n\n# algorithms\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nimport xgboost as xgb\n\n# metrics\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# optimization\nfrom functools import partial\nfrom hyperopt import hp, fmin, tpe\n\n# ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","d38ffa14":"# Look for missing values in DataFrame\ndef missing_values(df):\n    for column in df.columns:\n        null_rows = df[column].isnull()\n        if null_rows.any() == True:\n            print('%s: %d nulls' % (column, null_rows.sum()))\n\ndef survived_plot(feature):\n    all = df_train.groupby(('Survived', feature)).size().unstack()\n    all.index = ['Survived', 'Dead']\n    all.plot(kind='bar', stacked=True,\n             title = 'Who survived?', figsize = (12, 6))\n\ndef good_feats(df):\n    feats_from_df = set(df.select_dtypes([np.int, np.float]).columns.values)\n    bad_feats = {'PassengerId', 'Survived', 'SibSp', 'Parch'}\n    return list(feats_from_df - bad_feats)\n\ndef factorize(df, *columns):\n    for column in columns:\n        df[column + '_cat'] = pd.factorize(df[column])[0]\n\ndef plot_learning_curve(model, title, X, y, ylim=None, cv = None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \n    plt.figure(figsize=(12,8))\n    plt.title(title)\n    if ylim is not None:plt.ylim(*ylim)\n\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    \n    train_sizes, train_scores, test_scores = learning_curve(\n        model, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Testing score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\ndef title_feat(*dfs):\n    for df in dfs:\n        title = df['Name'].apply(lambda x:\n                                       x.split(',')[1].split('.')[0].strip())\n        # title mapping\n        title_map = {'Mr':1, 'Miss':2, 'Mrs':3, 'Master': 2, 'Rev': 5,\n                     'Dr':6, 'Col':8, 'Mlle':2, 'Major':8, 'Ms':1,\n                     'the Countess':3, 'Don':1, 'Capt':8, 'Lady':4, 'Mme':4,\n                     'Sir':4, 'Jonkheer':7, 'Dona':3}\n        df['Title_cat'] = title.apply(lambda x: title_map[x])\n    \ndef age_feat(*dfs):\n    for df in dfs:\n        age_title = df.groupby(['Title_cat'])['Age'].median().to_dict()\n        df['Age'] = df.apply(lambda row: age_title[row['Title_cat']]\n                                         if pd.isnull(row['Age'])\n                                         else row['Age'], axis=1)\n        df['Age_norm'] = pd.cut(df['Age'],[0,5,18,35,60,80])\n        factorize(df, 'Age_norm')\n    \ndef family_feat(*dfs):\n    for df in dfs:\n        df['Family'] = df['SibSp'] + df['Parch'] + 1\n        \ndef model_train_predict(model, X, y, success_metric=accuracy_score):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    return success_metric(y_test, y_pred)","829d4b94":"# load data\ndf_train = pd.read_csv('..\/input\/train.csv')\ndf_test = pd.read_csv('..\/input\/test.csv')\ndf_test['Survived'] = np.nan","3ccaef22":"# simple info about train dataset\ndf_train.info()","dd37a7dc":"# get five first rows\ndf_train.head()","e98ab093":"# get five random rows\ndf_train.sample(5)","d30fe60f":"# statistic information about training set\ndf_train.describe()","6a0d7031":"# columns names\ndf_train.columns","15632c3f":"# columns correlation\ndf_train.corr()","460c59ef":"# visualization correlation\nplt.rcParams['figure.figsize']=(15,7)\nsns.heatmap(df_train.corr().abs(), annot=True, linewidths=.5, cmap=\"Blues\");","312a8df0":"# curiosity about function\ndef func():\n    together = df_train.groupby(['Sex', 'Survived']).size()\n    \ndef func2():\n    survived = df_train[df_train.Survived == 1]['Sex'].value_counts()\n    dead = df_train[df_train.Survived == 0]['Sex'].value_counts()\n\nprint('Calculation time for groupby():')\n%time for i in range(10000): func()\nprint('Calculation time for another method:')\n%time for i in range(10000): func2()","2724c166":"# who survived? per sex\nsurvived_plot('Sex')","a36d2269":"# who survived? per pclass\nsurvived_plot('Pclass')","f7572c14":"# survival possibility dependent on age\nsns.set(style=\"darkgrid\")\n\nsns.FacetGrid(df_train, hue = 'Survived', size = 2.5,\n              aspect = 5, palette = 'Blues') \\\n  .map(sns.kdeplot, 'Age', shade=True) \\\n  .set(xlim = (0, df_train['Age'].max())) \\\n  .add_legend();","74935924":"# how survival possibility depend on class \nsns.set(rc = {'figure.figsize':(11.7,8.27)})\nsns.barplot(x = 'Pclass', y = 'Survived', hue = 'Sex', data = df_train);","11170458":"# dependency between Pclass and Sex - who had change of survived?\nsns.pointplot(x = 'Sex', y = 'Survived', hue = 'Pclass', data = df_train);","4f3dede8":"df_train = df_train[pd.notnull(df_train['Embarked'])]\n\nfactorize(df_train, 'Sex', 'Embarked')\nfactorize(df_test, 'Sex', 'Embarked')\ndf_train.head(5)","eeef8ee5":"# we have two new columns with factorized variables: 'Sex_cat', 'Embarked_cat'\ngood_feats(df_train)","35608eb8":"# basic model\nX = df_train[good_feats(df_train)].values\ny = df_train['Survived']\n\nmodel = DummyClassifier()\nscore = model_train_predict(model, X, y)\nprint(\"Score: %.2f\" % score)","e2c44811":"plt = plot_learning_curve(model, \"Learning Curves (Dummy Classifier)\", X, y, ylim=(0.5, 1.0), cv=10, n_jobs=4)\nplt.show()","d5f333fe":"title_feat(df_train, df_test)\nage_feat(df_train, df_test)\nfamily_feat(df_train, df_test)","e28ec5d9":"X = df_train[good_feats(df_train)].values\ny = df_train['Survived']\n\nmodels = [\n    LogisticRegression(),\n    DecisionTreeClassifier(max_depth=10),\n    RandomForestClassifier(max_depth=10),\n    ExtraTreesClassifier(max_depth=20)\n]\n\nfor model in models:\n    print(str(model) + \": \")\n    %time score = model_train_predict(model, X, y)\n    print(str(score) + \"\\n\")\n    plt = plot_learning_curve(model, \"Learning Curves\", X, y, ylim=(0.5, 1.0), n_jobs=4)\n    plt.show()","d9815cd1":"for model in models:\n    print(str(model) + \": \")\n    %time cross_validate(model, X, y, scoring='accuracy', cv=3)\n    print(str(score) + \"\\n\")\n    plt = plot_learning_curve(model, \"Learning Curves\", X, y, ylim=(0.5, 1.0), cv=3, n_jobs=4)\n    plt.show()","a53234f5":"# Feature importances\n# graphs show feature importances\n\nmodels = [\n    DecisionTreeClassifier(max_depth=10),\n    RandomForestClassifier(max_depth=10),\n    ExtraTreesClassifier(max_depth=20)\n]\n\nfor model in models:\n    model.fit(X, y)\n    importances = model.feature_importances_\n    indices = np.argsort(importances)[::-1]\n    plt.figure(figsize=(10, 5))\n    plt.title('Feature importances: ' + str(model).split('(')[0])\n    plt.bar(range(X.shape[1]), model.feature_importances_[indices],\n           color = 'g', align = 'center')\n    plt.xticks(range(X.shape[1]), [ good_feats(df_train)[x] for x in indices])\n    plt.xticks(rotation=90)\n    plt.xlim([-1, X.shape[1]])\n    plt.show()","ae4c0ee1":"model = xgb.XGBClassifier()\nmodel.fit(X, y)\n\ny_pred = model.predict(X)\nscore = accuracy_score(y, y_pred)\nprint(\"Score: %.2f\" % score)","8a5f327b":"X = df_train[good_feats(df_train)].values\ny = df_train['Survived']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n\ndef compute(params):\n    model = xgb.XGBClassifier(**params)\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_test)\n    score = accuracy_score(y_test, y_pred)\n    #print(\"Score: %.2f\" % score)\n    #print(params)\n    return (1 - score)\n\nspace = {\n        'max_depth':  hp.choice('max_depth', range(4,6)),\n        'min_child_weight': hp.uniform('min_child_weight', 0, 10),\n        'subsample': hp.quniform('subsample', 0.5, 1, 0.05),\n        'gamma': hp.quniform('gamma', 0.5, 1, 0.05)\n    }\n\nbest = fmin(compute, space, algo=tpe.suggest, max_evals=250)\nprint(best)","79cd8555":"X_train = df_train[good_feats(df_train)].values\ny_train = df_train['Survived']\nX_test = df_test[good_feats(df_test)].values\n\nmodel = xgb.XGBClassifier(**best)\nmodel.fit(X_train, y_train)","73395a85":"print(classification_report(y, \n                            y_pred, \n                            target_names=['Not Survived', 'Survived']))","c4da59ee":"# xgboost","b5d4abe4":"As you can see, function with groupby() is three times faster than alternative. Why? The second function browses dataset twice, but groupby() in first function browse it only once.","880ee695":"# Collect some important information","f8183c5c":"Red line shows result for train dataset. This line must be better than green line, because we learn on train dataset.\nHowever, we want that green line also have hight score.","b5c8a679":"# How Titanic sank?\n\n![Sinking of the RMS Titanic animation](images\/Sinking_of_the_RMS_Titanic_animation.gif)\n\nhttps:\/\/en.wikipedia.org\/wiki\/April_1912#\/media\/File:Sinking_of_the_RMS_Titanic_animation.gif\n\n# Where was the lifeboats?\n\n![Titanic_Boat_Deck_plan_with_lifeboats.png](images\/Titanic_Boat_Deck_plan_with_lifeboats.png)\n\nhttps:\/\/en.wikipedia.org\/wiki\/Lifeboats_of_the_RMS_Titanic#\/media\/File:Titanic_Boat_Deck_plan_with_lifeboats.png\n\nI think it's very important questions, because depending on where someone had a cabin, they had a different chance of survival. Some cabins were sunk at the very beginning. Other cabins were far to the lifeboats.\n\n# Visualization\nI used groupby() method to group survivors with feature, but I found another method in someone repository, so I compared them:","bee80542":"# Feature engineering","dce9de10":"Two important things: \n    - correlation between two features (should be removed from model)\n    - correlation between feature and target variable (should be added to model)\n    \nIn this case we have correlated two features: Parch and SibSp, so we can deduce, that marrieds had children.\n\nNext correlation, which is worth paying attention is correlation between target variable and feature Fare. I think, that it is main point to study. If someone was payed to much for, a ticket, that had better chance of survival.","e92b396f":"# Advanced model","cf520a9f":"Next I defined required functions to analyze, train and predict model:","ae470413":"# Basic model\n\nFirst I choose features from dataset. I search it with select_dtypes() method. I choose numerical variable \nbecause classification model works only on numerical variables. Frequently objects cointains categorial variable, but we can't use it in this form - we must change it to numerical. We could do this for example by use factorization function.\n\n```feats_from_dataset = df.select_dtypes([np.int, np.float]).columns.values```\n\n\nVariable 'Survived' is our target so we mustn't put it to models's features. 'PassengerId' is id of passenger \non titanic. It isn't necessary for us.\n\n```bad_feats = ['PassengerId', 'Survived']```\n\n```good_feats = [ feats for feats in feats_from_dataset if feats not in bad_feats ]```","8b47877b":"# Titanic: Machine Learning from Disaster\n\"Titanic: Machine Learning from Disaster\" is [Kaggle competition](https:\/\/www.kaggle.com\/c\/titanic),\nwhich is starting my adventure with machine learning.\nThe goal is to predict if a passenger survived the sinking of the Titanic or not.\n\n\nCompetition contain dataset presented in table:\n\nVariable|Definition|Key\n:-------|:--------:|---\nSurvival|Survival|0 = No<br>1 = Yes\nPclass|Ticket class|1 = 1st<br>2 = 2nd<br>3 = 3rd\nSex|Sex|\nAge|Age in years|\nSibSp|# of siblings \/ spouses<br>aboard the Titanic|\nParch|# of parents \/ children<br>aboard the Titanic|\nTicket|Ticket number|\nFare|Passenger fare|\nCabin|Cabin number|\nEmbarked|Port of Embarkation|C = Cherbourg<br>Q = Queenstown<br>S = Southampton\n\n\nAt the beggining, we need import libraries which I used later:","e99c98d9":"Hmmmm... Logistic Regresion is the best. The rest have overfitting."}}