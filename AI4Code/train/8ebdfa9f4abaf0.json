{"cell_type":{"268d912b":"code","0f2846d5":"code","ebca56f5":"code","0e5c7015":"code","e69dcea3":"code","6e3b6fcd":"code","efd70d0e":"code","7ee6389a":"code","41de23b0":"code","6f98f9e9":"code","1e67a3b0":"code","8b02cf11":"code","88549123":"code","41b298c3":"code","9938deb1":"code","1b221827":"code","3fdc0c7f":"code","8914b3d6":"code","04e1e3b4":"code","27e66ea7":"code","9556b47e":"code","cd7368a0":"code","cffb57e7":"code","003d315b":"code","6d58f0ab":"code","17c5252b":"code","6fc61865":"code","7f29683c":"code","39d0da27":"code","2ac27b49":"code","122e45a6":"code","36548dd6":"code","13d68836":"code","c62e1186":"code","75e98229":"code","bddd1443":"code","fcfe1b26":"code","b568a830":"code","119788b7":"code","b8e72480":"code","97bb69e4":"code","633a19d7":"code","0be7cba6":"code","c56b43cb":"code","74f150f4":"code","4910b827":"code","d67b2951":"code","0a8ae304":"code","ccb6576a":"code","4407dfbe":"code","024db297":"code","a1818efa":"code","2fa17f62":"code","8d369a9f":"code","efee1060":"code","57e1adfd":"code","af77e34b":"code","602098ff":"code","f8746e76":"code","f9a638a9":"code","09caf75b":"code","924b7156":"code","55289d15":"code","3028e20d":"code","dc4dca93":"code","6da7a650":"code","110ec77c":"markdown","a000bb0b":"markdown","cac8af4e":"markdown","9cd1bc73":"markdown","04188323":"markdown","8d16cc8a":"markdown","a97e2cc6":"markdown","2ee479ed":"markdown","0bae4b96":"markdown","bc44967a":"markdown","2b1fd406":"markdown","d05f9666":"markdown","69c2cab3":"markdown","4ec49371":"markdown","b8c0c20b":"markdown","8f39adc4":"markdown","497e3421":"markdown","8c759d7d":"markdown","812267c5":"markdown","ef3c6cf5":"markdown","25c4dc91":"markdown","2610215d":"markdown","adcfcdab":"markdown","fcd3dd71":"markdown","8c6552b6":"markdown","ab126b01":"markdown","bfeff073":"markdown","626c129b":"markdown","6e87f3a3":"markdown","eb8fd3b0":"markdown","36c149e5":"markdown","9b7fd07f":"markdown","560aa674":"markdown","ef45a8a2":"markdown","9ef2c97a":"markdown","e97c20d8":"markdown","5b40f6e0":"markdown","09329fe7":"markdown","c9d6eb95":"markdown","39da03e4":"markdown","684ab8f7":"markdown","79cdd399":"markdown","7b062efb":"markdown","4e6112d3":"markdown","90d3ed99":"markdown"},"source":{"268d912b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.cluster import KMeans\nfrom sklearn.utils.testing import ignore_warnings\nfrom sklearn.exceptions import ConvergenceWarning\n\nimport datetime\nimport math\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = [25, 15]\nimport seaborn as sns\nsns.set()\nimport networkx as nx\nimport csv\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0f2846d5":"trips = pd.read_csv(\"\/kaggle\/input\/mrt_trips_sampled.csv\")\nprint(trips.isna().sum())\ntrips","ebca56f5":"trips.applymap(lambda x : x.str.strip() if type(x) == 'str' else x)","0e5c7015":"print(trips.groupby(['destination','destination_tm','origin','origin_tm']).size().reset_index(name='count')['count'].value_counts())\n\ndef remove_repeats(trips):\n    \"\"\"\n    scales df by one third\n    \"\"\"\n    trips = trips.groupby(['destination','destination_tm','origin','origin_tm']).size().reset_index(name='count')\n    trips = pd.concat([trips,trips[trips['count']>3],trips[trips['count']==9]])\n    return trips.reset_index().drop(columns=['count','index'])\ntrips = remove_repeats(trips)\n\nprint(trips.groupby(['destination','destination_tm','origin','origin_tm']).size().reset_index(name='count')['count'].value_counts())","e69dcea3":"print('There are {} and {} unique destination and origin stations respectively.'.format(trips['destination'].nunique(),trips['origin'].nunique()))\nfig, axes = plt.subplots(nrows=2, ncols=2,figsize=[15, 7],tight_layout=True)\ntrips['destination'].value_counts(ascending=True)[-20:].plot(kind='barh',title='Top 20 Destinations by frequency',ax=axes[0,0])\ntrips['destination'].value_counts(ascending=True)[:20].plot(kind='barh',title='Bottom 20 Destinations by frequency',ax=axes[0,1])\ntrips['origin'].value_counts(ascending=True)[-20:].plot(kind='barh',title='Top 20 Origins by frequency',ax=axes[1,0])\ntrips['origin'].value_counts(ascending=True)[:20].plot(kind='barh',title='Bottom 20 Origins by frequency',ax=axes[1,1])\nfig.show()","6e3b6fcd":"print('The top {} origin stations account for {:.2f}% of the trips.'.format(20,trips['origin'].value_counts().values[:20].sum() \/ len(trips)*100))\nprint('The top {} destination stations account for {:.2f}% of the trips.'.format(20,trips['destination'].value_counts().values[:20].sum() \/ len(trips)*100))","efd70d0e":"n_trips = 50\ny1 = trips['destination'].value_counts().values[:n_trips] \ny2 = trips['origin'].value_counts().values[:n_trips]\nx = np.arange(n_trips)\nplt.bar(x,y1,color='r',label='Destination',width=.45)\nplt.bar(x+.5,y2,color='b',label='Origin',width=.45)\nplt.title('Frequency distribution of top {} stations'.format(n_trips))\nplt.legend()\nplt.xlabel('Top nth station by frequency')\nplt.ylabel('Number of trips')\nplt.figure(figsize=(8,5))\nplt.show()","7ee6389a":"stations = trips[[\"origin\", \"destination\"]].values.ravel()\nstations = pd.unique(stations)\nstations.sort()\nprint('There are',len(stations),'unique stations.')\n\norigin_stations = trips['origin'].unique()\ndest_stations = trips['destination'].unique()\no_unique = []\nd_unique = []\nfor station in stations:\n    if station not in origin_stations: o_unique.append(station)\n    if station not in dest_stations: d_unique.append(station)\nprint('Stations with trips only starting from in dataset:',o_unique)\nprint('Stations with trips only ending from in dataset:',d_unique)","41de23b0":"d_time = pd.to_datetime(trips['destination_tm'])\no_time = pd.to_datetime(trips['origin_tm'])\nprint('Destination timestamp ranges from',d_time.min(),'to',d_time.max())\nprint('Origin timestamp ranges from',o_time.min(),'to',o_time.max())","6f98f9e9":"# test = pd.to_datetime(trips['destination_tm'],format='%H%M%S')\ntest =trips['destination_tm'].astype('datetime64[ns]')\ntest","1e67a3b0":"d_time.hist(bins=100,figsize= [10, 7],label='Histogram of {} trips, ranging from {} to {}'.format(len(d_time),d_time.min(),d_time.max()))","8b02cf11":"def add_duration(trips): \n    d_time = pd.to_datetime(trips['destination_tm'])\n    o_time = pd.to_datetime(trips['origin_tm'])\n    diff = d_time - o_time\n    assert len(trips[diff <= datetime.timedelta(0)]) == 0 # Sanity check that all destination times recorded are after origin times\n    diff = diff.astype('timedelta64[s]')\n    trips.loc[:,'duration'] = diff\n    return trips\n\ntrips = add_duration(trips)\ntrips","88549123":"dest_ori_counts = trips.groupby(['destination','origin']).size().reset_index(name='freq')\nmax_d, max_o = dest_ori_counts.iloc[dest_ori_counts.freq.idxmax()].destination, dest_ori_counts.iloc[dest_ori_counts.freq.idxmax()].origin\ntrips[(trips.destination == max_d) & (trips.origin == max_o)].duration.hist(bins=100,figsize= [10, 7],label='{} trips from {} to {}'.format(dest_ori_counts.freq.max(),max_o,max_d))","41b298c3":"def filter_outlier(s):\n    \"\"\" \n    Filters outliers in given series. Outliers defined as 1.5 * interquartile range smaller than Q1 or larger than Q3 (i.e. mild outliers). We also filter for values appearing less than 3 times. \n    Inspired from https:\/\/datascience.stackexchange.com\/questions\/33632\/remove-local-outliers-from-dataframe-using-pandas\n    \n    Parameters:\n    s (pandas series): Input series to identify outliers.\n    \n    Returns:\n    boolean of whether we keep a value\n    \"\"\"\n    if s.size < 2:\n        return s.between(0,0)\n    Q1 = s.quantile(0.25)\n    Q3 = s.quantile(0.75)\n    IQR = Q3 - Q1\n    LOW = Q1 - 1.5 * IQR\n    HIGH = Q3 + 1.5 * IQR\n    return s.between(LOW, HIGH)","9938deb1":"# Filters outliers in given series. Outliers defined as 3 * interquartile range smaller than Q1 or larger than Q3 (i.e. extreme outliers)\nQ1 = trips['duration'].quantile(0.25)\nQ3 = trips['duration'].quantile(0.75)\nIQR = Q3 - Q1\nLOW = Q1 - 3 * IQR\nHIGH = Q3 + 3 * IQR\n\ndf = trips[trips['duration'].apply(lambda x: (LOW <= x <= HIGH))]\nprint('We removed {} trips.'.format(len(trips) - len(df)))","1b221827":"d_time = pd.to_datetime(df['destination_tm'])\no_time = pd.to_datetime(df['origin_tm'])\nprint('Destination timestamp ranges from',d_time.min(),'to',d_time.max())\nprint('Origin timestamp ranges from',o_time.min(),'to',o_time.max())\ndf.loc[d_time.idxmax()]","3fdc0c7f":"df = df[df.groupby(['origin','destination']).duration.apply(filter_outlier)]\nprint('We removed {} trips.'.format(len(trips) - len(df)))\ndf","8914b3d6":"plt.figure(figsize=(8,5))\ndest_ori_counts = trips.groupby(['destination','origin']).size().reset_index(name='freq')\nmax_d, max_o = dest_ori_counts.iloc[dest_ori_counts.freq.idxmax()].destination, dest_ori_counts.iloc[dest_ori_counts.freq.idxmax()].origin\nax = sns.distplot(trips[(trips.destination == max_d) & (trips.origin == max_o)].duration\/60)\nplt.title('Original dataset: {} trips from {} to {}'.format(dest_ori_counts.freq.max(),max_o,max_d))\nplt.xlabel('Duration in minutes')\nplt.ylabel('Proportion of trips')\nplt.show()","04e1e3b4":"plt.figure(figsize=(8,5))\ndest_ori_counts = df.groupby(['destination','origin']).size().reset_index(name='freq')\nmax_d, max_o = dest_ori_counts.iloc[dest_ori_counts.freq.idxmax()].destination, dest_ori_counts.iloc[dest_ori_counts.freq.idxmax()].origin\nax = sns.distplot(df[(df.destination == max_d) & (df.origin == max_o)].duration\/60)\nplt.title('After removing outliers: {} trips from {} to {}'.format(dest_ori_counts.freq.max(),max_o,max_d))\nplt.xlabel('Duration in minutes')\nplt.ylabel('Proportion of trips')\nplt.show()","27e66ea7":"plt.figure(figsize=(8,5))\nax = sns.distplot(trips.duration\/60)\nplt.title('Original dataset of {:,} trips'.format(len(trips)))\nplt.xlabel('Duration in minutes')\nplt.ylabel('Proportion of trips')\nplt.show()","9556b47e":"plt.figure(figsize=(8,5))\nax = sns.distplot(df.duration\/60)\nplt.title('Filtered dataset of {:,} trips'.format(len(df)))\nplt.xlabel('Duration in minutes')\nplt.ylabel('Proportion of trips')\nplt.show()","cd7368a0":"def get_trip_pairs(trips,direction='o2d',quantile=0.5):\n    \"\"\"\n    modes: o2d, d2o, both, default=o2c, else=both\n    undirected trip pairs: combine origin-destination pairs with destination-origin pairs\n    d2o: reverse destination with origin columns\n    quantile: Group by this quantile level\n    \"\"\"\n    trips = trips[trips['origin'] != trips['destination']]\n    if direction == 'o2d':\n        return trips.groupby(['origin', 'destination']).duration.quantile(quantile).reset_index()\n    else:\n        trips_rev = trips.rename(columns={'origin':'destination','destination':'origin'})\n        trips_rev = trips_rev.reindex(columns=['origin','destination','duration'])\n        if direction == 'd2o': return trips_rev\n        trip_pairs_combined = pd.concat([trips.reindex(columns=['origin','destination','duration']),trips_rev], ignore_index=True)\n        trip_pairs_combined = trip_pairs_combined.groupby(['origin', 'destination']).duration.quantile(quantile).reset_index()\n        return trip_pairs_combined","cffb57e7":"def get_nearest_n(trip_pairs,n):\n    \"\"\"\n    input: trip_pairs = df of all connections, n = number of smallest trips\n    output: df of nearest n stations for each unique origin station\n    \"\"\"\n    adjacency = trip_pairs.groupby(['origin']).duration.nsmallest(n).reset_index()\n    adjacency = adjacency.rename(columns={'level_1':'destination'})\n    adj_dest = trip_pairs['destination'][adjacency['destination']].reset_index()\n    return pd.DataFrame({'origin':adjacency['origin'],'destination':adj_dest['destination'],'duration':adjacency['duration']})","003d315b":"def filter_triangle_inequality(adjacency):\n    \"\"\"\n    generates new adjacency df, filtering using triangle inequality rule\n    \"\"\"\n    filtered_adj = {'origin':[],'destination':[],'duration':[]}\n    for o,d,dur in zip(adjacency['origin'],adjacency['destination'],adjacency['duration']):\n        candidate = adjacency[(adjacency['origin'] == o) & (adjacency['duration'] < dur)]\n        if len(candidate) == 1:\n            # directly add the nearest neighbour\n            filtered_adj['origin'].append(o)\n            filtered_adj['destination'].append(d)\n            filtered_adj['duration'].append(dur)\n\n        else:\n            # compare all other pairs, add pair if for all other stations \n            add = True\n            for dest in candidate['destination']:\n                if adjacency[(adjacency['origin'] == dest) & (adjacency['destination'] == d)].empty: \n                    add = False\n                    break\n                a = adjacency[(adjacency['origin'] == o) & (adjacency['destination'] == dest)].duration.values[0]\n                b = adjacency[(adjacency['origin'] == dest) & (adjacency['destination'] == d)].duration.values[0]\n                if a+b < dur + 10:\n                    add = False\n                    break\n            if add: \n                filtered_adj['origin'].append(o)\n                filtered_adj['destination'].append(d)\n                filtered_adj['duration'].append(dur)\n\n    return pd.DataFrame.from_dict(filtered_adj)","6d58f0ab":"def match_bidirectional(o2d,d2o):\n    \"\"\"\n    default: finds matches in o2d and d2o adjacency lists, returns df of smaller durations\n    \"\"\"\n    filtered_adj = {'origin':[],'destination':[],'duration':[]}\n    for o,d,dur in zip(o2d['origin'],o2d['destination'],o2d['duration']):\n        if not d2o[(d2o['origin'] == o) & (d2o['destination'] == d)].empty:\n            filtered_adj['origin'].append(o)\n            filtered_adj['destination'].append(d)\n            d2o_dur = d2o[(d2o['origin'] == o) & (d2o['destination'] == d)].duration.values[0]\n            filtered_adj['duration'].append(min(dur,d2o_dur))\n    return pd.DataFrame.from_dict(filtered_adj)","17c5252b":"def get_adjacency(trips,triangle=False,direction=1, match=False, n=2, quantile=0.5):\n    \"\"\"\n    Choose adjacency list generating algorithm.\n    nn: Selects nearest-2-neighbour as edges. Else, use triangle inequality to generate edges.\n    direction: 1 or 2 to denote if we concat d2o into original o2d table to generate direction agnostic graph.\n    match: If true, generates o2d edges and d2o edges, and returns intersection of both sets.\n    quantile: Select quantile level for groupby operation for trips to trip_pairs\n    \n    Returns adjacency list from trips dataset\n    \"\"\"\n    if match: \n        o2d, d2o = get_trip_pairs(trips, quantile=quantile), get_trip_pairs(trips,direction='d2o',quantile=quantile)\n        o2d, d2o = get_nearest_n(o2d,5), get_nearest_n(d2o,5)\n        if triangle: o2d, d2o = filter_triangle_inequality(o2d), filter_triangle_inequality(d2o)\n        return match_bidirectional(o2d,d2o)\n    else:\n        trip_pairs = get_trip_pairs(trips) if direction == 1 else get_trip_pairs(trips,direction='both')\n        trip_pairs = get_nearest_n(trip_pairs,n)\n        return trip_pairs if triangle else filter_triangle_inequality(trip_pairs)","6fc61865":"# adjacency = get_adjacency(df,triangle=False,n=2,direction=1, match=False)  # nearest 2\nadjacency = get_adjacency(df,triangle=False,n=2,direction=2, match=False)  # bidirectional, unmatched\n# adjacency = get_adjacency(df,triangle=True,n=5,direction=2, match=True)  # bidirectional, matched, triangle ineq. filtered\nadjacency","7f29683c":"derived_network = nx.Graph(name='Derived')\nfor origin, dest, weight in zip(adjacency['origin'], adjacency['destination'], adjacency['duration']):\n    derived_network.add_edge(origin, dest, weight=weight)\nprint(nx.info(derived_network))\nnx.draw_networkx(derived_network,font_size=10,node_size=800)","39d0da27":"node_labels = pd.read_csv('\/kaggle\/input\/mrt_network_nodes.csv')\nnode_labels['Node'] = node_labels['Node'].str.strip()\nnode_labels['Name'] = node_labels['Name'].str.strip()\nnode_labels","2ac27b49":"with open('\/kaggle\/input\/mrt_network_edges.csv', 'r') as edgecsv:\n    edgereader = csv.reader(edgecsv)\n    next(edgereader)\n    edges = []\n    nodes = []\n    for e in edgereader:\n        name = e[0].strip()\n        nodes.append(name)\n        for dest in e[1][2:-1].split(','):\n            edges.append((name,dest.strip()))\n# print(edges)\n# print(nodes)","122e45a6":"actual_graph = nx.Graph(name='Actual')\nactual_graph.add_nodes_from(nodes)\nactual_graph.add_edges_from(edges)\nprint(nx.info(actual_graph))","36548dd6":"def get_colors(G):\n    color_map = {'NS':'#c3301d','EW':'#419246','CG':'#419246','NE':'#8c3aaf','DT':'#1e59b2','CC':'#f2a13e','CE':'#f2a13e'}\n    return [color_map[node[0:2]] if node[0:2] in color_map else '#999999' for node in G]\n\ndef get_pos(nodes):\n    \"\"\"\n    Unused, as this does not guarantee the positions between lines are fixed (unless we order them).\n    \"\"\"\n    pos = {}\n    added = {}\n    y = 0\n    for node in nodes:\n        if node[:2] not in added:\n            added[node[:2]] = y\n            y += 1\n        pos[node] = (int(node[2:]),added[node[:2]]) if node[2:].isdigit() else (1,added[node[:2]])\n    return pos\n\ndef get_fixed_pos():\n    \"\"\"\n    Hard-coded positions for neater visualizations\n    \"\"\"\n    pos = {}\n    j = 0\n    for i in range(28):\n        pos['NS'+str(i+1)] = (i,j+i\/50)\n    j += 1\n    for i in range(29):\n        pos['EW'+str(i+1)] = (i,j+i\/50)\n    j += 1\n    for i in range(2):\n        pos['CG'+str(i+1)] = (i,j+i\/50)\n    j += 1\n    for i in range(17):\n        pos['NE'+str(i+1)] = (i,j+i\/50)\n    j += 1\n    for i in range(29):\n        pos['CC'+str(i+1)] = (i,j+i\/50)\n    j += 1\n    for i in range(2):\n        pos['CE'+str(i+1)] = (i,j+i\/50)\n    j += 1\n    for i in range(19):\n        pos['DT'+str(i+1)] = (i,j+i\/50)\n    j += 1\n    for i in range(14):\n        pos['BP'+str(i+1)] = (i,j+i\/50)\n    j += 1\n    for i in range(5):\n        pos['SE'+str(i+1)] = (i,j+i\/50)\n    pos['STC'] = (6,j)\n    j += 1\n    for i in range(8):\n        pos['SW'+str(i+1)] = (i,j+i\/50)\n    j += 1\n    for i in range(7):\n        pos['PE'+str(i+1)] = (i,j+i\/50)\n    pos['PTC'] = (8,j)\n    pos['PW1'] = (9,j)\n    pos['PW5'] = (10,j)\n    pos['PW6'] = (11,j)\n    pos['PW7'] = (12,j)\n    return pos\n\nfixed_pos = get_fixed_pos()","13d68836":"def name_to_labels(adjacency, node_labels):\n    \"\"\"\n    in-place modification of adjacency df, adds two new columns origin_code and destination_code\n    input: names = pandas df of adjacency to concvert origin & destination names; node_labels = pandas df to map names to mrt code\n    output: original df\n    \"\"\"\n    ans = [[] for _ in range(2)]\n    skipped = [[] for _ in range(2)]\n    # mapping of dirty data, i.e. name of interchanges: City Hall, Jurong East, Promenade, Raffles Place, Sengkang, Punggol, Dhoby Ghaut CCL\n    name_dict = {'Bayfront CCL':'CE1','Bayfront DTL':'DT16','Bishan NSEW': 'NS17', 'Bugis NSEW': 'EW12', 'Bukit Panjang BPLRT': 'BP6', 'Buona Vista NSEW': 'EW21', 'Choa Chu Kang': 'NS4', 'Choa Chu Kang BPLRT': 'BP1', 'Dhoby Ghaut NSEW': 'NS24', 'Expo NSEW': 'CG1','Marina Bay': 'NS27','Marina Bay CCL': 'CE2', 'Newton NSEW': 'NS21', 'Outram Park NSEW': 'EW16', 'Paya Lebar NSEW': 'EW8', 'Tampines NSEW': 'EW2'}\n    for i in range(2):\n        names = adjacency['origin'] if i == 0 else adjacency['destination']\n        for name in names:\n            match = node_labels['Node'][node_labels['Name'] == name]\n            if len(match) == 1:\n                ans[i].append(match.values[0])\n            else:\n                first, end = ' '.join(name.split()[:-1]), name.split()[-1]\n                if end[-1] == 'L':  # CCL, DTL\n                    if first == 'Bayfront' or first == 'Marina Bay': ans[i].append(name_dict[name])\n                    else: \n                        for n in node_labels['Node'][node_labels['Name'] == first]:\n                            if n[:2] == end[:-1]: \n                                ans[i].append(n)\n                                continue\n                else:\n                    if name not in name_dict:\n                        skipped[i].append(len(ans[i]))\n                        ans[i].append(name)\n                    else:\n                        ans[i].append(name_dict[name])\n    for i in range(2):\n        for idx in skipped[i]:\n            name = ans[i][idx]\n            other = ans[abs(i-1)][idx]\n            changed=False\n            if other[:1].isupper():\n                for n in node_labels['Node'][node_labels['Name'] == name]:\n                    if n[:1] == other[:1]:\n                        changed=True\n                        ans[i][idx] = n\n                if not changed: \n                    default = node_labels['Node'][node_labels['Name'] == name].values[0]\n                    ans[i][idx] = default\n                    print(name,other,default)\n    adjacency['origin_code'], adjacency['destination_code'] = ans[0], ans[1]\n    return adjacency","c62e1186":"derived_network = nx.Graph(name='Derived')\nname_to_labels(adjacency, node_labels)\nderived_network.add_nodes_from(adjacency['origin_code'])\nfor origin, dest, weight in zip(adjacency['origin_code'],adjacency['destination_code'],adjacency['duration']):\n    derived_network.add_edge(origin, dest, weight=weight)\n\n# manually add interchanges    \n# for edge in edges:\n#     code0, code1 = code_to_name(edge[0]), code_o_name(edge[1])\n#     if code0 in interchanges and code1 in interchanges and code0 == code1:\n#         print('interchange',code0,code1)\n#         derived_network.add_edge(origin, dest)\n        \npos = get_pos(adjacency['origin_code'])\nprint(nx.info(derived_network))","75e98229":"nx.draw_networkx(actual_graph,node_color=get_colors(actual_graph),font_size=10,node_size=800, pos=fixed_pos)","bddd1443":"nx.draw_networkx(derived_network,node_color=get_colors(derived_network),font_size=10,node_size=800, pos=fixed_pos)","fcfe1b26":"def similarity(g, h, mode='jaccard', nodes=False):\n    \"\"\"\n    measure: jaccard, precision, recall, f1\n    g: ground truth network, h: derived network\n    returns float value of similarity between two graphs range [0,1] \n    \"\"\"\n    tp, tn, fp, fn = [], [], [], []\n    if nodes:\n        g, h, gh = set(g.nodes()), set(h.nodes()), nx.compose(g,h)\n        for node in gh.nodes():\n            if node in g and node in h:\n                tp.append(node)\n            elif node in g and node not in h:\n                fn.append(node)\n            elif node not in g and node in h:\n                fp.append(node)\n            elif node not in g and node not in h:\n                tn.append(node)\n    else:\n        for edge in nx.compose(g,h).edges():\n            if g.has_edge(*edge) and h.has_edge(*edge):\n                tp.append(edge)\n            elif g.has_edge(*edge) and not h.has_edge(*edge):\n                fn.append(edge)\n            elif not g.has_edge(*edge) and h.has_edge(*edge):\n                fp.append(edge)\n            elif not g.has_edge(*edge) and not h.has_edge(*edge):\n                tn.append(edge)\n        g, h = g.edges(), h.edges()\n\n    assert len(tn) == 0, 'True negative set: {}'.format(tn)\n    \n    p = len(tp) \/ (len(tp) + len(fp))\n    r = len(tp) \/ (len(tp) + len(fn))\n    \n    if mode == 'jaccard':\n        return len(tp) \/ (len(g) + len(h) - len(tp))\n    elif mode == 'precision':\n        return p\n    elif mode == 'recall':\n        return r\n    elif mode == 'f1':\n        return 2 * p * r \/ (p + r)\n    else:\n        raise Exception(mode,' mode not supported.')\n    ","b568a830":"def plot_comparison(G,H, pos=None):\n    GH = nx.compose(G,H)\n    \n    # set edge colors\n    edge_colors = dict()\n    for edge in GH.edges():\n        if G.has_edge(*edge):\n            if H.has_edge(*edge):\n                edge_colors[edge] = 'green'\n                continue\n            edge_colors[edge] = 'blue'\n        elif H.has_edge(*edge):\n            edge_colors[edge] = 'red'\n\n    # set node colors\n    G_nodes = set(G.nodes())\n    H_nodes = set(H.nodes())\n    node_colors = []\n    for node in GH.nodes():\n        if node in G_nodes:\n            if node in H_nodes:\n                node_colors.append('green')\n                continue\n            node_colors.append('blue')\n        if node in H_nodes:\n            node_colors.append('red')\n\n    nx.draw_networkx(GH, pos=pos, \n            nodelist=GH.nodes(),\n            node_color=node_colors,\n            edgelist=edge_colors.keys(), \n            edge_color=edge_colors.values(),\n            node_size=800,\n            width=5,alpha=0.6,\n            with_labels=True)","119788b7":"print(nx.info(actual_graph))\nprint(nx.info(derived_network))\nprint('Nodes:')\nprint('Jaccard Similarity: {:3f}'.format(similarity(actual_graph, derived_network, mode='jaccard', nodes=True)))\nprint('Precision: {:3f}'.format(similarity(actual_graph, derived_network, mode='precision', nodes=True)))\nprint('Recall: {:3f}'.format(similarity(actual_graph, derived_network, mode='recall', nodes=True)))\nprint('F1: {:3f}'.format(similarity(actual_graph, derived_network, mode='f1', nodes=True)))\nprint('Edges:')\nprint('Jaccard Similarity: {:3f}'.format(similarity(actual_graph, derived_network, mode='jaccard')))\nprint('Precision: {:3f}'.format(similarity(actual_graph, derived_network, mode='precision')))\nprint('Recall: {:3f}'.format(similarity(actual_graph, derived_network, mode='recall')))\nprint('F1: {:3f}'.format(similarity(actual_graph, derived_network, mode='f1')))\n\nplot_comparison(actual_graph,derived_network,pos=fixed_pos)","b8e72480":"def remove_block_letters(s):\n    \"\"\"\n    removes block letter MRT codes (e.g. CCL) from pandas series. Use as filter function to change row values.\n    \"\"\"\n    ret = []\n    for substr in s.split():\n        if not substr.isupper(): ret.append(substr)\n    return ' '.join(ret)\n\ndef code_to_name(s):\n    return node_labels[node_labels['Node'] == s]['Name'].values[0]","97bb69e4":"from collections import Counter\nname_count = Counter()\nfor name in df['origin'].unique():\n    cleaned = remove_block_letters(name)\n    name_count[cleaned] += 1\ninterchanges = {name for name in name_count if name_count[name]>1}\nprint('Interchange stations:',interchanges)\n\nactual_named_graph = nx.Graph(name='Actual-named')\nactual_named_graph.add_edges_from([(code_to_name(e1), code_to_name(e2)) for (e1, e2) in edges])\n\nderived_named_graph = nx.Graph(name='Derived-named')\nfor origin, dest, weight in zip(adjacency['origin'], adjacency['destination'], adjacency['duration']):\n    o_name, d_name = remove_block_letters(origin), remove_block_letters(dest)\n    if o_name != d_name:\n        derived_named_graph.add_edge(o_name, d_name, weight=weight)\n    elif o_name in interchanges and d_name in interchanges:\n        derived_named_graph.add_edge(o_name, d_name, weight=0)\n\nprint(nx.info(actual_named_graph))\nprint(nx.info(derived_named_graph))\nprint('Nodes:')\nprint('Jaccard Similarity: {:3f}'.format(similarity(actual_named_graph, derived_named_graph, mode='jaccard', nodes=True)))\nprint('Precision: {:3f}'.format(similarity(actual_named_graph, derived_named_graph, mode='precision', nodes=True)))\nprint('Recall: {:3f}'.format(similarity(actual_named_graph, derived_named_graph, mode='recall', nodes=True)))\nprint('F1: {:3f}'.format(similarity(actual_named_graph, derived_named_graph, mode='f1', nodes=True)))\nprint('Edges:')\nprint('Jaccard Similarity: {:3f}'.format(similarity(actual_named_graph, derived_named_graph, mode='jaccard')))\nprint('Precision: {:3f}'.format(similarity(actual_named_graph, derived_named_graph, mode='precision')))\nprint('Recall: {:3f}'.format(similarity(actual_named_graph, derived_named_graph, mode='recall')))\nprint('F1: {:3f}'.format(similarity(actual_named_graph, derived_named_graph, mode='f1')))\n\nplot_comparison(actual_named_graph,derived_named_graph)","633a19d7":"def name_to_code(s):\n    \"\"\"\n    return list of all possible codes for input station names\n    \"\"\"\n    return list(node_labels[node_labels['Name'] == s]['Node'].values)","0be7cba6":"# cache for get_same_line\nnames2codes = {}\n\ndef get_same_line(o,d):\n    \"\"\"\n    takes in pairs of station names, return mrt code if they are on same line, else False\n    \"\"\" \n    o,d = remove_block_letters(o), remove_block_letters(d)\n    if (o,d) in names2codes: return names2codes[(o,d)]\n    ol,dl = name_to_code(o), name_to_code(d)\n    if not ol or not dl: return (False,False)\n    for i in ol:\n        for j in dl:\n            if i[:2] == j[:2]: \n                names2codes[(o,d)] = (i,j)\n                return (i,j)\n    names2codes[(o,d)] = (False,False)\n    return (False,False)\n\ndef get_same_line_row(s):\n    return get_same_line(s['origin'],s['destination'])","c56b43cb":"line_range = {}\nfor n in node_labels['Node']:\n    if n[:2] not in line_range:\n        if n[2:].isnumeric(): line_range[n[:2]] = (int(n[2:]),int(n[2:]))\n    else:\n        if n[2:].isnumeric(): \n            l,h = line_range[n[:2]]\n            line_range[n[:2]] = (min(l,int(n[2:])),max(h,int(n[2:])))\nprint(line_range)","74f150f4":"codes = df.apply(get_same_line_row,axis=1)\ndirect = df[[True if x[0] else False for x in codes]]\ncodes = codes[[True if x[0] else False for x in codes]]\ndirect.loc[:,'origin'] = [int(x[0][2:]) for x in codes]\ndirect.loc[:,'destination'] = [int(x[1][2:]) for x in codes]\ndirect.loc[:,'line'] = [x[0][:2] for x in codes]\ndirect.loc[:,'destination_tm'] = direct.destination_tm.apply(pd.to_datetime)\ndirect.loc[:,'origin_tm'] = direct.origin_tm.apply(pd.to_datetime)\ndirect","4910b827":"def get_trips_A_B(line='EW',ori=23,des=10,time_min='9:00:00',time_max='10:00:00',plot=False):\n    \"\"\"\n    input: line code, origin and destination station numbers, time range\n    output: pd series of destination time x, list of durations y\n    \"\"\"\n    assert line in line_range, '{} not valid'.format(line)\n    assert line_range[line][0] <= ori <=line_range[line][1], 'Station {} not valid'.format(line)\n    assert line_range[line][0] <= des <=line_range[line][1], 'Station {} not valid'.format(line)\n    time_min = pd.to_datetime(time_min)\n    time_max = pd.to_datetime(time_max)\n    prev = des-1 if des > ori else des+1\n    # prev = ori\n\n    to_des = direct[(direct['line']==line) & (direct['origin'] == prev) & (direct['destination'] == des)]\n    x = (to_des['destination_tm'] - time_min).astype('timedelta64[m]')\n    y = to_des['duration']\/60\n\n    # i = ori\n    i = direct[(direct['line']==line) & (direct['destination'] == des)].origin.min() if ori < des else direct[(direct['line']==line) & (direct['destination'] == des)].origin.max() \n    start = i\n\n    while i != prev:\n        to_des = direct[(direct['line']==line) & (direct['origin'] == i) & (direct['destination'] == des)]\n\n        i = i+1 if ori < des else i-1\n\n        time = (to_des['destination_tm']-time_min).astype('timedelta64[m]')\n        dur = to_des['duration']\n\n        x = x.append(time)\n        y = y.append(dur\/60)\n        \n    if plot:\n        plt.figure(figsize=(8,8))\n        # plt.title('Trips from {}{} {} to {}{} {}'.format(line,ori,code_to_name(line+str(ori)),line,des,code_to_name(line+str(des))))\n        plt.title('Trips ending in {}{} {} for all origins along {} line ({}{} {} to {}{} {})'.format(line,des,code_to_name(line+str(des)),line,line,start,code_to_name(line+str(start)),line,prev,code_to_name(line+str(prev))))\n        plt.xlabel('Time elapsed since 9:00 (min)')\n        plt.ylabel('Duration of trips (min)')\n        plt.scatter(x, y, color ='b') \n        plt.show()\n\n    return x, y\n\nx,y = get_trips_A_B(line='EW',ori=23,des=10,plot=True)\nX = np.array(x).reshape(-1, 1)\nY = np.array(y).reshape(-1, 1)","d67b2951":"k_low, k_high = int((max(x)-min(x))\/7), int((max(x)-min(x))\/2) # calculate bounds from assumptions\nK = range(k_low,k_high)\n\n@ignore_warnings(category=ConvergenceWarning)\ndef get_distortions(K,x,plot=False):\n    \"\"\"\n    k-means 1-D array x in range K\n    returns list of distortions\n    \"\"\"\n    X = np.array(x).reshape(-1, 1)\n    distortions = []\n    for k in K:\n        kmeans = KMeans(n_clusters=k)\n        kmeans.fit(X)\n        distortions.append(kmeans.inertia_)\n\n    if plot:\n        plt.figure(figsize=(7, 5))\n        plt.plot(K, distortions,marker='o')\n        plt.grid(True)\n        plt.xlabel('k')\n        plt.ylabel('Loss')\n        plt.title('Elbow curve for deriving k')\n        plt.show()\n    \n    return distortions\n\ndistortions = get_distortions(K,x,plot=True)","0a8ae304":"def get_k(distortions, K, plot=False):\n    \"\"\"\n    Algorithm to find the maximum distance between line from k_low to k_high, and each k in range.\n    input: list of loss for k-means in range K, K is a list from k_low to k_high\n    output: optimum k\n    \"\"\"\n    def shortest_distance(x1, y1, a, b, c):    \n        return abs((a * x1 + b * y1 + c)) \/ (math.sqrt(a * a + b * b))\n\n    a = distortions[0] - distortions[-1]\n    b = K[-1] - K[0]\n    c = K[0] * distortions[-1] - K[-1] * distortions[0]\n\n    dist = [shortest_distance(K[k],distortions[k],a,b,c) for k in range(len(K))]\n    if plot:\n        plt.figure(figsize=(6,6))\n        plt.xlabel('k')\n        plt.ylabel('Distance')\n        plt.title('Finding the peak for deriving k')\n        plt.plot(K,dist)\n    opt_k = K[dist.index(max(dist))]\n    return opt_k\n\nk = get_k(distortions, K)","ccb6576a":"def get_relevant_trains(x,y,k,line='EW',ori=23,des=10,time_min='9:00:00',time_max='10:00:00',plot=False,verbose=False):\n    \"\"\"\n    Finds k clusters of arrival times from ori to des\n    Assigns train arrivals at the 90th quantile of each cluster\n    returns relevant trains\n    \"\"\"\n    assert line in line_range, '{} not valid'.format(line)\n    assert line_range[line][0] <= ori <=line_range[line][1], 'Station {} not valid'.format(line)\n    assert line_range[line][0] <= des <=line_range[line][1], 'Station {} not valid'.format(line)\n    time_min = pd.to_datetime(time_min)\n    time_max = pd.to_datetime(time_max)\n\n    X = np.array(x).reshape(-1, 1)\n    Y = np.array(y).reshape(-1, 1)\n    \n    kmeans = KMeans(n_clusters=k)\n    kmeans.fit(X)\n    \n    if plot:\n        plt.figure(figsize=(10,10))\n        plt.scatter(X[:,0], Y, c=kmeans.labels_, cmap='rainbow')\n        plt.xlabel('Time elapsed since 9:00 (min)')\n        plt.ylabel('Duration of trips (min)')\n        # start = direct[(direct['line']==line) & (direct['destination'] == des)].origin.min() if ori < des else direct[(direct['line']==line) & (direct['destination'] == des)].origin.max()\n        # plt.scatter(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1] ,color='black')\n        # plt.title('Trips from {}{} {} to {}{} {}'.format(line,start,code_to_name(line+str(start)),line,des,code_to_name(line+str(des))))\n        # plt.title('Trips ending in {}{} {} for all origins along {} line ({}{} {} to {}{} {})'.format(line,des,code_to_name(line+str(des)),line,line,start,code_to_name(line+str(start)),line,prev,code_to_name(line+str(prev))))\n\n    means = pd.DataFrame([kmeans.labels_,x,y]).T\n    means = means.rename(columns={0:'label',1:'x',2:'y'})\n    times = []\n    for i in means.groupby('label').x.quantile(.9):\n        i = int(i)\n        if plot: plt.axvline(i)\n        times.append(i)\n    times.sort()\n\n    # idx = []\n    # for i,l in enumerate(kmeans.labels_[1:],1):\n    #     if kmeans.labels_[i-1] != l: idx.append(i-1)\n    # print(idx)\n\n    # [plt.scatter(x,15,color='black') for x in kmeans.cluster_centers_[:,0]]\n\n    timestamps = pd.Series([time_min + datetime.timedelta(minutes=time) for time in times])\n\n    filtered = direct[(direct['line']==line) & (direct['origin'] == ori) & (direct['destination'] == des) & (direct['origin_tm']>= time_min)].reset_index()\n    first_arrival = filtered.iloc[filtered['origin_tm'].idxmin()].destination_tm\n\n    valid_trains = timestamps[timestamps.apply(lambda x : first_arrival <= x <= time_max)]\n    if verbose: \n        print('There are {} trains arriving at {}{} {} between {} and {}'.format(len(timestamps),line,des,code_to_name(line+str(des)),timestamps.min().time(),timestamps.max().time()))\n        print('There are {} trains leaving {}{} {} after {} and arriving at {}{} {} before {}'.format(len(valid_trains),line,ori,code_to_name(line+str(ori)),time_min.time(),line,des,code_to_name(line+str(des)),time_max.time()))\n        print('These trains are:',[str(time.time()) for time in valid_trains])\n\n    if plot: \n        if len(valid_trains) > 1:\n            left = times[valid_trains.index[0]-1] if valid_trains.index[0] != 0 else 0\n            plt.axvspan(left, times[valid_trains.index[-1]], alpha=0.2, color='green')\n        plt.title('{} trains leave {}{} {} after {} and arrive at {}{} {} before {}'.format(len(valid_trains),line,ori,code_to_name(line+str(ori)),time_min.time(),line,des,code_to_name(line+str(des)),time_max.time()))\n        plt.show()\n        \n    return valid_trains\n\nvalid_trains = get_relevant_trains(x,y,k,line='EW',ori=23,des=10,time_min='9:00:00',time_max='10:00:00',plot=True,verbose=True)","4407dfbe":"line, ori, des = 'NS', 16, 26\n# line, ori, des = 'EW', 23, 10\n\ntime_min, time_max ='9:00','10:00'\n\nx,y = get_trips_A_B(line=line,ori=ori,des=des,time_min=time_min,time_max=time_max,plot=False)\n\nK = range(int((max(x)-min(x))\/7), int((max(x)-min(x))\/2)) # calculate bounds from assumptions\n\ndistortions = get_distortions(K,x,plot=False)\nk = get_k(distortions, K,plot=False)\n\ntimestamps = get_relevant_trains(x,y,k,line=line,ori=ori,des=des,time_min=time_min,time_max=time_max,plot=True,verbose=True)","024db297":"combined_adjacency = get_trip_pairs(df,direction='both', quantile=.1)\n\ncombined_adjacency['origin'] = combined_adjacency['origin'].apply(remove_block_letters)\ncombined_adjacency['destination'] = combined_adjacency['destination'].apply(remove_block_letters)\n\n# combined_adjacency = get_nearest_n(combined_trip_pairs,20)\n# name_to_labels(combined_adjacency, node_labels)\n\ncombined_adjacency","a1818efa":"combined_network = nx.Graph(name='Combined')\nmissing_edges = []\nfor i,edge in enumerate(edges):\n    origin,dest = code_to_name(edge[0]), code_to_name(edge[1])\n    weight = combined_adjacency.loc[(combined_adjacency['origin'] == origin) & (combined_adjacency['destination'] == dest)]['duration']\n    if weight.empty: \n        if origin == dest:\n            combined_network.add_edge(edge[0], edge[1], weight=0) # assumption: Changing at interchanges takes 0 mins\n        else:\n            missing_edges.append((edge[0],origin,'->',edge[1],dest))\n        continue\n    combined_network.add_edge(edge[0], edge[1], weight=weight.values[0])\n    \nprint(len(missing_edges),'Missing edges:',[' '.join(edge) for edge in missing_edges])\npos = get_pos([node for node,_ in edges])\nprint(nx.info(combined_network))","2fa17f62":"def get_path(network,o,d,weight='weight',same_line=False):\n    \"\"\"\n    given name of station 1 and 2 in string, return shortest path of weighted network graph\n    shortest path algorithm used: dijkstra's\n    same_line: If True, origin-destination pairs must be from the same line.\n    \"\"\"\n    ol,dl = name_to_code(o), name_to_code(d)\n    if not ol: raise Exception(o+' is not a valid station')\n    if not dl: raise Exception(d+' not found')\n    for i in ol:\n        for j in dl:\n            if i[:2] == j[:2]:\n                return nx.shortest_path(network,i,j,weight=weight)\n    if same_line: raise Exception(o+' and '+d+' are not on the same line')\n    min_path = nx.shortest_path(network,ol[0],dl[0],weight=weight)\n    for i in ol:\n        for j in dl:\n            path = nx.shortest_path(network,i,j,weight=weight)\n            min_path = min(min_path,path)\n    return min_path","8d369a9f":"path = get_path(combined_network,'Clementi','Kallang',weight=None)\ntotal_dur = 0\nfor i,stn in enumerate(path[1:],1): \n    o,d = path[i-1],stn\n    dur = combined_network[o][d]['weight']\n    total_dur +=  dur\n    print(o,code_to_name(o),'to',d,code_to_name(d),'Duration:',dur\/60,'min')\nprint('Total time taken',total_dur\/60,'min')","efee1060":"nx.draw_networkx(combined_network,node_color=get_colors(combined_network),font_size=10,node_size=800, pos=fixed_pos)","57e1adfd":"print(nx.info(actual_graph))\nprint(nx.info(combined_network))\nprint('Jaccard Similarity',similarity(actual_graph, combined_network))\nplot_comparison(actual_graph,combined_network,pos=fixed_pos)","af77e34b":"def get_trips_to_des(line,ori,des):\n    \"\"\"\n    Finds k clusters of arrival times from ori to des\n    Assigns train arrivals at the 90th quantile of each cluster\n    returns all trips to given destination\n    \"\"\"\n    time_min = pd.to_datetime('9:00:00') # get all trips\n    time_max = pd.to_datetime('12:00:00') # get all trips\n    \n    x,y = get_trips_A_B(line=line,ori=ori,des=des,time_min=time_min,time_max=time_max)\n    K = range(int((max(x)-min(x))\/7), int((max(x)-min(x))\/2)) # calculate bounds from assumptions\n    distortions = get_distortions(K,x)\n    k = get_k(distortions, K)\n\n\n    X = np.array(x).reshape(-1, 1)\n    Y = np.array(y).reshape(-1, 1)\n    \n    kmeans = KMeans(n_clusters=k)\n    kmeans.fit(X)\n    \n    means = pd.DataFrame([kmeans.labels_,x,y]).T\n    means = means.rename(columns={0:'label',1:'x',2:'y'})\n    times = []\n    for i in means.groupby('label').x.quantile(.9):\n        times.append(int(i))\n    times.sort()\n    \n    return pd.Series([time_min + datetime.timedelta(minutes=time) for time in times])","602098ff":"timestamps = get_trips_to_des(line='EW',ori=23,des=22)\ntimestamps","f8746e76":"def get_time_diff(timestamps):\n    \"\"\"\n    input: pd date_time series\n    output: time difference\n    \"\"\"\n    td_series = pd.Series([(time - timestamps[i-1]) for i,time in enumerate(timestamps[1:],1)]).astype('timedelta64[m]')\n#     print('Average time between trains: {:.2f} mins with std dev {:.2f} mins.'.format(td_series.mean(),td_series.std()))\n    return td_series\n\nget_time_diff(timestamps)","f9a638a9":"EW_eastbound = {des: get_trips_to_des(line='EW',ori=line_range['EW'][1],des=des) for des in range(line_range['EW'][1]-5,line_range['EW'][0]-1,-1)}","09caf75b":"plt.figure(figsize=(8,5))\nplt.plot([len(timestamps) for timestamps in EW_eastbound.values()])\nplt.title('Number of learned arrival times per station')\nplt.ylabel('Number of arrival times')\nplt.xlabel('Station Number')\nplt.show()","924b7156":"plt.figure(figsize=(15,10))\n[plt.scatter(np.ones(len(timestamps))*stn,timestamps) for stn, timestamps in EW_eastbound.items()]\n    \nplt.title('Eastbound stations on EW line by arrivals')\nplt.xlabel('Station Number')\nplt.ylabel('Arrival times')\nplt.show()","55289d15":"def get_ew_east_trains(num, walking=120):\n    assert num <= len(EW_eastbound[line_range['EW'][1]-5])\n    trains = []\n    for i in range(num):\n        train = {}\n        stn = line_range['EW'][1]-5\n        curr = EW_eastbound[stn][i]\n        train[stn] = curr\n        while stn != line_range['EW'][0]:\n            # print('From {} to {}'.format(stn,stn-1))\n            o2d = direct[(direct['line']=='EW')&(direct['destination']==stn-1)&(direct['origin']==stn)] if stn != 14 else direct[(direct['line']=='NS')&(direct['destination']==25)&(direct['origin']==26)]\n            match = curr + datetime.timedelta(seconds=o2d.duration.min()-walking)\n            curr = EW_eastbound[stn-1].where(EW_eastbound[stn-1]>match).min()\n            \n            if i != 0:\n                if (stn-1) in trains[i-1]:\n                    while curr == trains[i-1][stn-1]:\n                        match = trains[i-1][stn-1]\n                        curr = EW_eastbound[stn-1].where(EW_eastbound[stn-1]>match).min()\n                        if str(curr) =='NaT': break\n\n            if str(curr) =='NaT': break\n            train[stn-1] = curr\n            stn -= 1\n        trains.append(train)\n    return trains\n\ntrains = get_ew_east_trains(10,walking=120)","3028e20d":"plt.figure(figsize=(15,10))\n[plt.scatter(np.ones(len(timestamps))*stn,timestamps) for stn, timestamps in EW_eastbound.items()]\n\n[plt.plot([stn for stn in train.keys()],[time for time in train.values()]) for train in trains]\n    \nplt.title('Eastbound stations on EW line by arrivals')\nplt.xlabel('Station Number')\nplt.ylabel('Arrival times')\nplt.show()","dc4dca93":"def get_train_from_station(ori=23,des=10,time='09:25'):\n    time = pd.to_datetime(time)\n    for i, train in enumerate(trains):\n        if ori in train:\n            if train[ori] > time:\n                return (i,train)\n               # return {key:time for key,time in train.items() if 10 <= key <= ori}","6da7a650":"train_num, curr_train = get_train_from_station(ori=23,des=10,time='09:25')\nprev_train, next_train = trains[train_num-1], trains[train_num+1]\n\ni = 23\ndes = 10\npeople = len(direct[(direct['line']=='EW')&(direct['origin']==i+1)&(direct['destination']<i+1)&(direct['origin_tm'].between(prev_train[i+1],curr_train[i+1]))])\n\nwhile i >= des:\n    enter = len(direct[(direct['line']=='EW')&(direct['origin']==i)&(direct['destination']<i)&(direct['origin_tm'].between(prev_train[i],curr_train[i]))])\n    exit = len(direct[(direct['line']=='EW')&(direct['destination']==i)&(direct['origin']>i)&(direct['destination_tm'].between(curr_train[i],next_train[i]))])\n    print('EW{} {:13}: {:4} commuters on the train. {:2} entered the train, while {:3} exited.'.format(i,code_to_name('EW'+str(i)),people,enter,exit))\n    people += enter-exit\n    i -= 1\n","110ec77c":"### Checking validity of data","a000bb0b":"Checking for repeats, we see that the dataset is comprises mainly triplets, with some data points being multiplied by 6 and 9. This is rather odd, as there are frequencies in {1,2,4,5,...}.\nTo reduce repeat computation, we group the data by the key information, neglecting index on the assumption that we do not need to query trips by their 'trip_id'. This allows us to do the remaining EDA and analysis quicker (dataset shrinks by 3 to 126406 rows).","cac8af4e":"Two-side matching algorithm: an edge must be in both sets of candidate directions.","9cd1bc73":"## Overall algorithm to get relevant trips","04188323":"Visualization to illustrate similarity between derived and actual: green = same edge\/node on both graphs, blue \/ red = edge\/node only on actual \/ derived graphs respectively","8d16cc8a":"Utils for plotting with colors and positions on networkx.","a97e2cc6":"Algorithm choices:\n* 2-Nearest: get_adjacency(df,triangle=False,n=2,direction=1, match=False)\n* Bidirectional, unmatched: get_adjacency(df,triangle=False,n=2,direction=2, match=False)\n* Bidirectional, matched, triangle ineq. filtered: get_adjacency(df,triangle=True,n=5,direction=2, match=True)","2ee479ed":"Turns out, we have 154 total stations recorded in our dataset. We have Sam Kee as the only station where commuters only leave from (in our dataset), and Ten Mile Junction with only arrivals.","0bae4b96":"Strip removes leading and trailing whitespaces, when parsing input adjacency list.","bc44967a":"Here, we see that the derived train schedule per station is inconsistent. We need to figure out a way to match them. ","2b1fd406":"Build adjacency list from nearest two stations per origin station.","d05f9666":"## Get adjacency list to generate network graph","69c2cab3":"### Get unique stations\n\nWe now want to see how many stations there are, for us to plot into a network. First, we see how many unique destination and origin stations.\n\nWe see that:\n* The top destinations are dominating the ","4ec49371":"# MRT Analytics\n","b8c0c20b":"> Data cleaning:\nCheck for edge case: Same destination and origin (commuter tapped in and out without travelling, or travelling in a loop). This reduces the number of rows from 381249 to 379218. (~0.5% of dataset)\nClean input names.","8f39adc4":"Calculate time difference in series to get waiting time","497e3421":"## People counting algorithm\nCalculating the number of people in our selected train for each station in the line:\n1. For each station S along line, calculate the number of people coming in (i.e. origin station == S along the direction). We assume all commuters will board the earliest possible train, and hence the number of people corresponds to the origin times between the previous and current train. This adds to our running counter of the number of people in the train.\n2. For each staton S along line, deduct the outflow of commuters (i.e. destination station == S along the direction). This is found from matching the destination time with the arrival time of the current and next train.\n\nUnfortunately, as we did not account for commuters entering the current train from other lines (e.g. the inflow at Buona Vista from CCL), this causes the calculation to be wrong (reaching into the negatives). To ameliorate this, we need to build the schedule for the entire trips dataset, mapping the different trains each journey with exchanges take.","8c759d7d":"Visualising the train arrivals per station allows us to see that we can learn a pattern, to map arrival times to trains,","812267c5":"Alternate algorithm: Shortlist the shortest 5 pairs. For each pair, if there is an alternate path that allows us to reach it in a shorter time, the connection cannot be a direct link.","ef3c6cf5":"### Get trip durations","25c4dc91":"## Question 1\n\n### Overall Thought Process\nWe are given a dataset of MRT journeys, defined by origin station, tap-in timestamp, destination station and tap-out timestamp. We first begin with EDA on the dataset, identifying 154 total unique stations from this journeys dataset. This means we can form a network up of up to 154 stations from the given dataset.\n\nHow do we connect the stations to form a network? Since we are not given the adjacency list of this network, the only other reference point we have is measuring the time taken between stations to form the network. This means that the network we form is based on proximity measured by time, and not space.","2610215d":"### Import dependencies","adcfcdab":"Assumptions: Queried stations are on the same line denoted by the line prefix, e.g. NS1 to NS28","fcd3dd71":"Combine data from both directions to get median duration between station pairs.","8c6552b6":"## K-means to cluster arrival times to trains","ab126b01":"## Getting all journeys on the same line","bfeff073":"Auto detect k by largest arc (height of convex hull): https:\/\/www.youtube.com\/watch?v=IEBsrUQ4eMc","626c129b":"### Comparison\nWe can use Jaccard similarity to compare similarity between sets. Casting the identification of MRT nodes and edges as a classification problem, we can use precision, recall and F1 metrics to evaluate the efficacies of our algorithms.","6e87f3a3":"From the line tracing algorithm, we can plot the identified trains.","eb8fd3b0":"## Mapping codes to names","36c149e5":"We see that the origin times range from 9am to 10am, while destination times range from just after 9am to 8pm. Considering the latest departure time is 10am, the 8pm arrival looks like an anomaly. To filter these outliers out, we will cluster each trip by origin-destination pairs, and remove outliers based on statistical measures.","9b7fd07f":"Actual MRT network","560aa674":"## Question 3","ef45a8a2":"## Clean data\nWe define outliers to be $1.5\\times Interquartile Range$ away from Q1 or Q3, which is the definition for mild outliers. Statistics calculated for each origin-destination pair. This is reduces our dataset size from 127083 to 121913 (-4%). Further filtering for origin-destination pairs with only 1 data point reduces our dataset size to 121298.","9ef2c97a":"Assume: K ranges from 2 to 7 mins","e97c20d8":"Derived MRT network","5b40f6e0":"We see that there are no missing values for all columns. The size of the dataset is 381249 rows by 5 columns.","09329fe7":"## Question 4","c9d6eb95":"Another example to show algorithm. With more data, the algorithm is better able to cluster.","39da03e4":"Visualising missing connections from dataset","684ab8f7":"## Question 2","79cdd399":"Main algorithm to call other functions","7b062efb":"Unsuccessful attempt: Populating weights in the network using durations of adjacent stations based on trips dataset. The error from adding up weights to generate duration from station A to B for non-adjacent A-B accumulates when A is far from B, making the calculated duration too inaccurate to use.","4e6112d3":"### Exploratory data analysis","90d3ed99":"How many unique stations are there in all?"}}