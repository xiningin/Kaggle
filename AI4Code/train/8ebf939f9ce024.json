{"cell_type":{"29e16eb2":"code","50d6c7de":"code","d25712b8":"code","8e8e6b05":"code","6c464ca1":"code","b68009e0":"code","b1f2d060":"code","8862d776":"code","3368b51d":"code","e6c9fe70":"code","b6f4dc26":"code","60b9d00b":"code","b12d9ee2":"markdown","70105d17":"markdown","0ab5060a":"markdown","f6ec6ef3":"markdown","dbb4d15e":"markdown","fc88c7e5":"markdown","57dfd07a":"markdown","1bba88c7":"markdown","2a947d64":"markdown","f6eaa560":"markdown","7c74bb4c":"markdown","7938d655":"markdown"},"source":{"29e16eb2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\nfrom IPython.display import Image","50d6c7de":"Image(filename='..\/input\/photo123\/maxresdefault.jpg') ","d25712b8":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.decomposition import PCA \nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport seaborn as sns\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.metrics.pairwise import pairwise_distances_argmin\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","8e8e6b05":"df_=pd.read_csv(r'..\/input\/banking-data\/Datae2.csv')\n\ndf=df_.drop(['id','year','message_code','service','atm_status',\n       'atm_manufacturer','atm_location','atm_streetname',\n       'atm_street_number','atm_zipcode','weather_lat',\"atm_id\",\n       'weather_lon','weather_city_id','weather_city_name','weather_description'],axis=1)\n\n\ndf['rain_3h']=df['rain_3h'].fillna(0)\ndf.message_text=df.message_text.fillna(\"None\")\n\ndf.message_text = df.message_text.replace([\"Suspected malfunction, no cash dispensed\",\"Suspected malfunction, card retained\"],\"Suspected malfunction\")\ndf.message_text = df.message_text.replace([\"Timed-out taking card, card retained and no cash dispensed\",\"Timed-out taking money\"],\"Timed-out\")","6c464ca1":"def fuzzyday(x):\n    a=0\n    b=0\n    c=0\n\n    if x<4:\n        a=1\n    elif x<9:\n        Y=x-4\n        a = 1-Y*0.25\n    elif x>8:\n        a=0\n        \n    if x<4:\n        b=0\n    elif x<8:\n        T = x-4\n        b=T*0.25\n    elif x<25:\n        b=1\n    elif x<29:\n        T = x-25\n        b=1-T*0.25\n        T+=1\n        \n    if x<25:\n        c=0\n    elif x<29:\n        L=x-25\n        c=0.25*L\n    elif x>28:\n        c=1\n    \n    \n    return round(a*10),round(b*10),round(c*10)\n\ndef fuzzyhour(x):\n    a=0\n    b=0\n    c=0\n\n    if x<6:\n        a=1\n    elif x<11:\n        Y=x-5\n        a = 1-Y*0.2\n    elif x>10:\n        a=0\n        \n    if x<6:\n        b=0\n    elif x<11:\n        T = x-5\n        b=T*0.2\n    elif x<15:\n        b=1\n    elif x<21:\n        T = x-15\n        b=1-T*0.2\n        \n        \n    if x<15:\n        c=0\n    elif x<20:\n        L=x-15\n        c=0.20*L\n    elif x>20:\n        c=1\n    \n    \n    return round(a*10),round(b*10),round(c*10)","b68009e0":"df[\"1day\"]=1\ndf[\"2day\"]=2\ndf[\"3day\"]=3\ndf[\"1day\"],df[\"2day\"],df[\"3day\"] = zip(*df.day.apply(lambda x:fuzzyday(x)))\ndf.drop(\"day\",axis=1, inplace=True)\n\ndf[\"1hour\"]=1\ndf[\"2hour\"]=2\ndf[\"3hour\"]=3\ndf[\"1hour\"],df[\"2hour\"],df[\"3hour\"] = zip(*df.hour.apply(lambda x:fuzzyhour(x)))\ndf.drop(\"hour\",axis=1, inplace=True)\n\n\ndff = pd.get_dummies(df[[\"month\",\"weekday\",\"currency\",\"card_type\",'weather_main',\"message_text\"]],drop_first=True)\n\ndf = df.drop([\"month\",\"weekday\",\"currency\",\"card_type\",'weather_main',\"message_text\"],axis=1)\n\ndf = pd.merge(df,dff,left_index=True,right_index=True)\n\nMMS = MinMaxScaler()\nMMS.fit(df)\ndf_norm = MMS.transform(df)\ndf_norm = pd.DataFrame(df_norm, columns=df.columns)\n# df_norm.head()\n\npca_model = PCA(n_components=0.98)\npca_model.fit(df_norm)\ndf_pca = pca_model.transform(df_norm)\ndf_pca = pd.DataFrame(df_pca)\n\n\npca_ratio=pca_model.explained_variance_ratio_\npca_ratio","b1f2d060":"p=0\ndf_pca2 = pd.DataFrame()\nfor j in df_pca.columns:\n    df_pca2[j] = df_pca[j]*pca_ratio[p]\n    p+=1\n    \ndf_pca2 = df_pca2*10","8862d776":"distrToTal = []\n\nfor j in df_pca.columns:\n    distr = []\n    arange = []\n    pas = 0\n    sta = -0.95\n    for i in range(2000): \n        lenn =len(df_pca[j][df_pca[j]<sta])\n        \n        distr.append((lenn \/ 1250000) - pas)\n        pas = np.sum(distr)\n        \n        arange.append(round(sta,2))\n        sta +=0.02\n        \n        if sta>1.5:\n            break\n            \n    distrToTal.append(distr)\ndistrToTal[0]","3368b51d":"def fitness(df):\n    summy = []\n    for j in df.columns:\n        j = int(j)\n        sc=0\n        pas=0\n        count=0\n        summ=[]\n        for i in arange:\n            lenn =df[df[j]<i].shape[0]\n            pers = (lenn \/ df.shape[0]) - pas\n            pas += pers\n            summ.append(np.abs(pers - distrToTal[j][count]))\n            count+=1\n        summy.append(np.sum(summ) * pca_ratio[j])\n        \n    return (np.sum(summy)\/2)","e6c9fe70":"df_pca_mi = df_pca2.copy()\nmbkm =MiniBatchKMeans(n_clusters=1700,max_iter=800, batch_size=50000 , verbose=1)\n\nmbkm.fit(df_pca_mi)\ndf_pca_mi[\"lable\"] = mbkm.predict(df_pca_mi)\n\n\n\nlist0 = []\nfor i in range(1900):\n    dff = df_pca_mi[df_pca_mi[\"lable\"] == i]\n    list1 = np.random.choice(dff.index ,size = int(np.round((len(dff)\/10))),replace=False)\n    list0.append(list1)\n    \n    \n    \nhhh = []\nfor i in list0:\n    for j in i:\n        hhh.append(j)\n        \n        \n        \nprint(len(hhh))\n\n\ndf_sample = df_pca[df_pca.index.isin(hhh)]\n\nprint(\"Matching percentage:\",((1-fitness(df_sample))*100) , \"%\")","b6f4dc26":"best=10\nfor i in tqdm(range(20)):\n    ind = []\n\n    for i in range(len(arange)-1):\n        data = df_pca[(df_pca[0]<arange[i+1]) & (df_pca[0]>arange[i])]\n        data = data.iloc[:,1:]\n        le = round(len(data)\/10)\n\n        if le<500:\n            tt = np.random.choice(data.index,le,replace=False)\n            ind.append(tt)\n        else:\n            n_c = round(le\/100)\n            mbkm =MiniBatchKMeans(n_clusters=n_c,max_iter=800, batch_size=round(le\/4) , verbose=0)\n\n            mbkm.fit(data)\n            data[\"lable\"] = mbkm.predict(data)\n\n\n\n            list3 = []\n            for i in range(n_c):\n                dff = data[data[\"lable\"] == i]\n                list1 = np.random.choice(dff.index ,size = int(np.round((len(dff)\/10))),replace=False)\n                list3.append(list1)\n\n            hhh = []\n            for i in list3:\n                for j in i:\n                    hhh.append(j)\n\n            ind.append(hhh)\n    fina = []\n    for i in ind:\n        for j in i:\n            fina.append(j)\n            \n    df_sample = df_pca[df_pca.index.isin(fina)] \n    fit = fitness(df_sample)\n\n    if fit<best:\n        best = fit\n        best_sample=df_sample\n        print(\"Matching percentage:\" ,(1-best)*100,\"%\")","60b9d00b":"print(\"my best sample is:\")\nbest_sample","b12d9ee2":"We multiply each column by its **pca_ratio** to give more **importance** to maintaining the distribution of the PC0 than the other columns.","70105d17":"# Fitness","0ab5060a":"# Data pre-processing & PCA\nWe use pca for continuous distribution that represents the actual distribution! (Our comparison will be easier in this case.)","f6ec6ef3":"# Mini batch K-means","dbb4d15e":"# A strange algorithm I made myself:)))))))","fc88c7e5":"# fuzzy encoding!!","57dfd07a":"**I think if we have two distributions, and the area below the diagram of these two distributions in very small intervals is Equal then this two distributions is equal**\n\nLet's go to implement the idea","1bba88c7":"**Apparently, this algorithm that I made gets a better answer\nIf you enjoyed this notebook, I will be happy to like it**","2a947d64":"# Hi!!!","f6eaa560":"**Hi, I am Sina Tavakoli, today I am trying to get a good example of large scale data. For this we need a fitness function and an algorithm!**\n\nI get my main idea for fitness function from The surface area below the chart.","7c74bb4c":"I have doubts about whether this is useful and defensible, and I would like you to comment on this.:)","7938d655":"**And the area below the relative distribution diagram of each feature is stored in the intervals of 0.02**"}}