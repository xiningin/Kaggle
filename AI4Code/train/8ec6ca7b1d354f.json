{"cell_type":{"552de724":"code","83ad4bde":"code","7584c328":"code","68e71f57":"code","d81b7b66":"code","8e14c8f2":"code","73acb630":"code","8bc96c4c":"code","a66ea7cc":"code","1caacbc5":"code","d757a74e":"code","9d6aba22":"code","3a7feb7d":"code","bca53ce2":"code","6ae36c7c":"code","9e20c35d":"code","32457197":"code","407a8621":"code","116a8aee":"code","0f1fbb0f":"code","435664d7":"code","9ecdd9da":"code","37eec65f":"code","9e64759a":"code","57a51289":"code","1595a714":"code","7e773b6f":"markdown","a1a625e0":"markdown","f290bce6":"markdown","fbe00a9f":"markdown","4b59162b":"markdown","26886b50":"markdown","2741b6e5":"markdown","7de5910a":"markdown","78e96ac0":"markdown","ebf17d96":"markdown","7552dff5":"markdown","32a89ed2":"markdown","8e838d75":"markdown","33b4db9a":"markdown","7d49f84c":"markdown","c1751aca":"markdown","8d164d4f":"markdown","65329cc1":"markdown","dc02eca9":"markdown","1d276ed0":"markdown","ea497a06":"markdown","9e5cb5dc":"markdown","04ccf5f1":"markdown","b6020b9a":"markdown"},"source":{"552de724":"from glob import glob\nimport pandas as pd\nimport numpy as np\nfrom nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\nfrom nltk.stem import PorterStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nimport itertools\nimport re\nimport os\nimport string\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, BatchNormalization, Dropout, Activation\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.models import load_model\nfrom sklearn.model_selection import train_test_split\nfrom IPython.display import Image\nfrom math import ceil","83ad4bde":"def parse_folder(name):\n    data = []\n    for verdict in ('neg', 'pos'):\n        for file in glob(os.path.join(name, verdict, '*.txt')):\n            data.append({\n                'text': open(file, encoding='utf8').read(),\n                'verdict': verdict == 'pos'\n            })\n    return pd.DataFrame(data)","7584c328":"df_train = parse_folder('..\/input\/aclimdb\/aclImdb\/train\/')\ndf_test = parse_folder('..\/input\/aclimdb\/aclImdb\/test\/')","68e71f57":"df_train.iloc[0].text, df_train.iloc[0].verdict","d81b7b66":"reviews = ['This movie is good', 'The movie is bad', 'Bad this movie was']","8e14c8f2":"vocabulary = set()\nfor review in reviews:\n    for word in review.split(' '):\n        vocabulary.add(word)\nprint(vocabulary)","73acb630":"doc_term = []\nfor document in reviews:\n    row = {'!document': document}\n    row.update({word: document.split(' ').count(word) for word in vocabulary})\n    doc_term.append(row)\ndoc_term = pd.DataFrame(doc_term)\ndisplay(doc_term)","8bc96c4c":"re_tok = re.compile(f'([{string.punctuation}\u201c\u201d\u00a8\u00ab\u00bb\u00ae\u00b4\u00b7\u00ba\u00bd\u00be\u00bf\u00a1\u00a7\u00a3\u20a4\u2018\u2019\\'\u201c\u201d\u00a8\u00ab\u00bb\u00ae\u00b4\u00b7\u00ba\u00bd\u00be\u00bf\u00a1\u00a7\u00a3\u20a4\u2018\u2019])')\ndef tokenize_re(text):\n    return re_tok.sub(r' \\1 ', text).split()","a66ea7cc":"def tokenize_nltk(text):\n    return list(itertools.chain.from_iterable(word_tokenize(sentence) for sentence in sent_tokenize(text)))    ","1caacbc5":"stemmer = PorterStemmer()\ndef tokenize(text):\n    return [stemmer.stem(word) for word in tokenize_nltk(text)]","d757a74e":"vectorizer = CountVectorizer(ngram_range=(1, 3), tokenizer=tokenize, max_features=1000000)","9d6aba22":"train_doc_term = vectorizer.fit_transform(df_train.text)\ntest_doc_term = vectorizer.transform(df_test.text)\ntrain_doc_term","3a7feb7d":"Image(url='https:\/\/ibin.co\/4hxqDwhJnxCE.png')","bca53ce2":"classes = np.array([1, 0, 0])\ndoc_term_mat = doc_term.drop('!document', axis='columns').values\ndisplay(doc_term_mat)\np_c = np.array([(classes == 0).mean(), (classes == 1).mean()])\np_dc = np.ones((2, doc_term_mat.shape[1])) # use ones because by default every term can appear once in every class\nfor col in range(doc_term_mat.shape[1]):\n    for row in range(doc_term_mat.shape[0]):\n        p_dc[classes[doc_term_mat[row][col]]][col] += doc_term_mat[row][col]\nfor c in (0, 1):\n    p_dc[c] = p_dc[c] \/ p_dc[c].sum()\ndisplay(p_c, p_dc)","6ae36c7c":"clf = MultinomialNB()\nclf.fit(train_doc_term, df_train.verdict)","9e20c35d":"clf.score(test_doc_term, df_test.verdict)","32457197":"weights = np.log(p_dc[1] \/ p_dc[0])\nbias = np.log(p_c[1] \/ p_c[0])\ndisplay(weights, bias, doc_term_mat @ weights + bias)","407a8621":"train_doc_term_bool = train_doc_term > 0\nr_neg = (train_doc_term_bool[df_train.verdict.values == 0].sum(0) + 1) \/ (sum(df_train.verdict == 0) + 1)\nr_pos = (train_doc_term_bool[df_train.verdict.values == 1].sum(0) + 1) \/ (sum(df_train.verdict == 1) + 1)\ncoef = np.log((r_pos \/ r_neg).A.flatten())","116a8aee":"lreg = LogisticRegression(C=0.2, solver='liblinear', max_iter=500, dual=True) # C comes from regularization\nlreg.fit(train_doc_term, df_train.verdict)\nlreg.score(test_doc_term, df_test.verdict)","0f1fbb0f":"def batch_generator(X, y, batch_size, shuffle=False):\n    number_of_batches = ceil(X.shape[0]\/batch_size)\n    counter = 0\n    sample_index = np.arange(X.shape[0])\n    if shuffle:\n        np.random.shuffle(sample_index)\n    while True:\n        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n        X_batch = X[batch_index,:].toarray()\n        y_batch = y[batch_index]\n        counter += 1\n        yield X_batch, y_batch\n        if (counter == number_of_batches):\n            if shuffle:\n                np.random.shuffle(sample_index)\n            counter = 0","435664d7":"net = Sequential([\n    Dense(1, activation='sigmoid', input_dim=train_doc_term.shape[1], kernel_regularizer=l2(0.1)),\n])\nnet.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\ncallbacks = [\n    ModelCheckpoint('nn_best.h5', monitor='val_acc', verbose=0, save_weights_only=False, save_best_only=True, mode='max'),\n    EarlyStopping(monitor='val_loss', min_delta=0, patience=1, verbose=0, mode='auto')\n]","9ecdd9da":"net.set_weights([coef.reshape(-1, 1)])","37eec65f":"X_train, X_valid, Y_train, Y_valid = train_test_split(train_doc_term, df_train.verdict.values, test_size=0.2, stratify=df_train.verdict.values)","9e64759a":"batch_size = 64\nnet.fit_generator(\n    generator=batch_generator(X_train, Y_train, batch_size, shuffle=True),\n    validation_data=batch_generator(X_valid, Y_valid, batch_size), validation_steps=ceil(len(Y_valid) \/ batch_size),\n    epochs=5, steps_per_epoch=ceil(len(Y_train) \/ batch_size), callbacks=callbacks)","57a51289":"net = load_model('nn_best.h5')","1595a714":"net.evaluate_generator(batch_generator(test_doc_term, df_test.verdict, batch_size=batch_size), steps=ceil(len(df_test.verdict) \/ batch_size))","7e773b6f":"A common way to do tokenization is with regular expressions, but [nltk](https:\/\/www.nltk.org\/api\/nltk.tokenize.html) contains lots of tokenizers to choose from.","a1a625e0":"A lot of columns are going to be 0, because only a small percentage of the vocabulary appears in each document. In order to keep it from exploding in memory, 0s are not stored.","f290bce6":"Learning the coefficients seems to yield better results than the theoretical model.","fbe00a9f":"### Infering probabilities using the term-document matrix\nProbability of class C being 1 (positive review), given a document D:\n$P(C=1|D)=\\frac{P(D \\wedge C=1)}{P(D)}=\\frac{P(D|C=1)*P(C=1)}{P(D)}$  \nBy computing $\\frac{P(C=1|D)}{P(C=0|D)}$ we obtain $\\frac{P(D|C=1)}{P(D|C=0)}*\\frac{P(C=1)}{P(C=0)}$. If this number is greater than 1, the probability of the review being a positive one is greater than the probability of it being negative.  \nFor each term we compute the probabilities of it appearing in a negative and in a positivie review.\n\nLearn more about probabilistic inference from [here](https:\/\/ocw.mit.edu\/courses\/electrical-engineering-and-computer-science\/6-034-artificial-intelligence-fall-2010\/lecture-videos\/) (lectures 21 and 22).","4b59162b":"### Why is it naive?\n\nIt assumes that events are independent. In reality, the probability of the term *awful* appearing in a negative review is not independent of the probability of *bad*.","26886b50":"The vocabulary can be seen with *get_feature_names*.","2741b6e5":"### How similar it is to Logistic Regression?\n\nIn order to convert multiplication into addition we can work with logarithms.  \n  \n$\\log(\\frac{P(D|C=1)}{P(D|C=0)}*\\frac{P(C=1)}{P(C=0)})=\\frac{P(D|C=1)}{P(D|C=0)}+\\log(\\frac{P(C=1)}{P(C=0)})$.  \n  \nY = predicted values  \n  \nX = term-doc matrix  \n  \nW = $\\frac{P(D|C=1)}{P(D|C=0)}$  \n  \nB = $\\log(\\frac{P(C=1)}{P(C=0)})$  \n  \n$Y = X * W + B$\n\nInstead of learning these coefficients, we approximated them using a theoretical model.","7de5910a":"### BOW approach\n\nA Bag Of Words approach is a way to represent text in a manner that makes it usable in machine learning, which is usually a tensor (i.e a vector or an array). It is fairly easy to implement and can be quite effective, especially when dealing with short text messages, such as movie reviews.  \nFor illustration, suppose there are only three reviews in the dataset.","78e96ac0":"### Basic probability rules\n\nProbability of C occuring, given D: $P(C|D)=\\frac{P(C \\wedge D)}{P(D)}$  \nProbability of both C and D occuring: $P(C \\wedge D)=P(C|D)*P(D)$","ebf17d96":"## Logistic Regression\n\nBy doing logistic regression, we try to fit an n-dimensional plane that separates positive reviews from negative reviews.","7552dff5":"### Parsing","32a89ed2":"### Tokenization","8e838d75":"The BOW representation works on the principle that if the word *good* appears a lot in a movie review, it is very probable to be a positive one. This approach unfortunately doesn't take into account the order in which the words appear in a sentence, which is  fundamental to its meaning.  \n**The movie is shit!** and **The movie is the shit!** look very similar when considering their BOW representation, but these two sencences have very different meanings.","33b4db9a":"## Preparing the dataset\nThe aclmdb folder contains two folders: train and test. Each of these folders contains the neg and pos folders with movie reviews.\nA negative review will have be labeled with 0, while a positive review with 1.\n","7d49f84c":"## Combining the two approaches\n\nWe can initialize the coefficients with these obtained from Naive Bayes and start optimizing from there, instead of randomly initializing them.","c1751aca":"Tokenization refers to the process of turning a piece of text into a list of tokens or symbols, dealing with punctuation. For example, the text **You call this a \"movie\"?! It isn't good at all!** could be tokenized like this: **You call this a \" movie \" ?! It is n't good at all !**. The text is first separated into sentences, then each sentence is separated into tokens.","8d164d4f":"# IMDB sentiment classification task using BOW\n\nInspired from fast.ai https:\/\/course18.fast.ai\/ml\n\nA negative review has a score \u2264 4 out of 10, and a positive review has a score \u2265 7 out of 10. Reviews with a score ranging from 5 to 6 are considered netural and thus are not included in the dataset.","65329cc1":"### Stemming\n\nWords such as gaming, gamed, games are replaced with game. Only the stems are kept.","dc02eca9":"In order to compute the probability of a document, given the class, we can multiply the probabilities of all its terms given the same class with the probability of that class occuring. For example, given the negative class, the probability of the first review belonging to it is approximately $0.095*0.095*0.142*0.190 * 0.66$. The last 0.66 comes from the 0.66 probability of the negative class.","1d276ed0":"1. The *document-term matrix* or *term-document matrix* is obtained by viewing every word in the vocabulary as a column, indicating its presence or number of occurences in every document. In this context a document is a movie review. This matrix is the *bag of words* representation.","ea497a06":"The first thing to do is to define the *vocabulary*, which is the set of all words. Then the *document-term matrix* is created, which can be seen as a matrix interpretation of the dataset.","9e5cb5dc":"[CountVectorizer](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.CountVectorizer.html) is used to create the matrix of token counts. Instead of actual words, there is the posibility to use n-grams. An n-gram is a tuple of n words.","04ccf5f1":"The threshold is now 0 instead of 1, because $\\log 1 = 0$.","b6020b9a":"## Naive Bayes\nUsing the term-document matrix it is possbile to infer the probability of a review being positive or negative.\nhttps:\/\/scikit-learn.org\/stable\/modules\/naive_bayes.html#naive-bayes"}}