{"cell_type":{"ac6cb979":"code","5d972177":"code","b9642192":"code","0de301a2":"code","1ca1a8ae":"code","5d45657b":"code","4c8c0abb":"code","98ec6d62":"code","7285566d":"code","65471c47":"code","c82405e3":"code","6a11ff22":"code","6dd0016e":"code","32e1e7cf":"code","c223f931":"code","70f75f14":"code","63a8526e":"code","a0c0e7bf":"code","0dff422d":"code","62dee0d4":"code","34d06f84":"code","f954693b":"code","fe0b532e":"code","08782271":"code","c099abc8":"code","2645ddea":"code","e48c82b2":"code","7de6e0c5":"code","37ab3c0e":"code","7654d1f9":"code","7f1340bb":"code","eebadb4b":"code","eb374b10":"code","dfe4ee28":"code","b6311f26":"code","df3cf7a8":"code","d43dc729":"code","97f0ab23":"code","b385628f":"code","e6b50572":"code","5ede6139":"code","544156e8":"code","16877370":"code","55df7cdc":"code","d9a85635":"code","49dd21b8":"code","11919a40":"code","3482b6a7":"code","fb2d6153":"code","e8694aea":"code","c5442edb":"code","a72cec00":"code","6e5ed99f":"code","5022c2fe":"code","cc17b939":"code","b965c869":"code","7ca9bed3":"code","9cb44cbd":"code","61024cd3":"code","68413eec":"code","af106970":"code","dde52256":"code","ec45f71a":"code","d0be5e3d":"code","3a4c8c9c":"code","d9f2488a":"code","b1b956e5":"code","02f6d82b":"code","caff570a":"code","bf7c5749":"code","d2b262f6":"code","2653cc87":"code","a1e07f9d":"code","5d13dc79":"code","7f88fe56":"code","4e81cd9a":"code","7e97a9d8":"code","1099988f":"code","12b1fe25":"code","9096042d":"code","5a7dbcb1":"code","fd421f90":"markdown","0de692fb":"markdown"},"source":{"ac6cb979":"# Here is NY Watersheds which shows water quality values sampled from many locations. \n# This analysis is to show the healtiness level of surface water bodies in NY \n# Data includes 4-hour-Turbidity values (six times a day), Daily Average Turbidity\n# and Fecal Coliform level of the sample as well as with the date and location info.\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n# I, first start by importing necessary py libs and opening the data \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns  # visualization tool\n\nimport os\nwqdata = pd.read_csv('..\/input\/watershed-water-quality-data.csv')\nwqdata.columns","5d972177":"# Here we look at the descriptive statistics on the data exploring the \n# data range, averages and sample counts of only numeric data (so it excludes date and location)\nwqdata.describe()","b9642192":"# To examine data, we could use .head() or .tail() for instance\nwqdata.head()","0de301a2":"# .info shows data types and names of the columns with data range\n# it is also possible to see the missing data in each data attribute (columns)\nwqdata.info()","1ca1a8ae":"# Here, we produce a correlation matrix of each attribute to see \n# if there is a direct relationship among turbidity and fecal coliform (FC) levels\n\nwqdata.corr()\n\n# We can say that 4-hour-turbidity samples shows high correlation with previous and next samples\n# All 4-hour-turbidity samples also highly correlated with daily turbidity","5d45657b":"# By using seaborn library's correlation map function we build a color - coded correlation matrix \n\nf,ax = plt.subplots(figsize=(18, 18))\nsns.heatmap(wqdata.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)\nplt.show()\n\n# Here we have 4-hour-turbidity (interval) samples and daily turbidity average\n# It is possible to say the 4PM has the least effect on daily turbidity average \n# and other interval samplings since it has the lowest correlation with the others","4c8c0abb":"wqdata.columns","98ec6d62":"# To see that better we will look at the scatter plots of interval \n# samples against daily average of turbidity\n\n# Scatter Plots will be set as x = Daily Turbidity Average and y = interval sampling \n\nwqdata.plot(kind='scatter', marker='+', grid=True, x='Average 24hrTurbidity(NTU)', y='Turbidity(NTU) at 12AM',alpha = 0.5,color = 'orange')\nplt.xlabel('Average 24hrTurbidity(NTU)') \nplt.ylabel('Turbidity(NTU) at 12AM')\nplt.title('12 AM vs Daily Turbidity Scatter Plot')\n\nwqdata.plot(kind='scatter', marker='+', grid=True, x='Average 24hrTurbidity(NTU)', y='Turbidity(NTU) at 4AM',alpha = 0.5,color = 'orange')\nplt.xlabel('Average 24hrTurbidity(NTU)') \nplt.ylabel('Turbidity(NTU) at 4AM')\nplt.title('4 AM vs Daily Turbidity Scatter Plot')\n\nwqdata.plot(kind='scatter', marker='+', grid=True, x='Average 24hrTurbidity(NTU)', y='Turbidity(NTU) at 8AM',alpha = 0.5,color = 'orange')\nplt.xlabel('Average 24hrTurbidity(NTU)') \nplt.ylabel('Turbidity(NTU) at 8AM')\nplt.title('8 AM vs Daily Turbidity Scatter Plot')\n\nwqdata.plot(kind='scatter', marker='+', grid=True, x='Average 24hrTurbidity(NTU)', y='Turbidity(NTU) at 12PM',alpha = 0.5,color = 'orange')\nplt.xlabel('Average 24hrTurbidity(NTU)') \nplt.ylabel('Turbidity(NTU) at 12PM')\nplt.title('12 PM vs Daily Turbidity Scatter Plot')\n\nwqdata.plot(kind='scatter', marker='+', grid=True, x='Average 24hrTurbidity(NTU)', y='Turbidity(NTU) at 4PM',alpha = 0.5,color = 'orange')\nplt.xlabel('Average 24hrTurbidity(NTU)') \nplt.ylabel('Turbidity(NTU) at 4PM')\nplt.title('4 PM vs Daily Turbidity Scatter Plot')\n\nwqdata.plot(kind='scatter', marker='+', grid=True, x='Average 24hrTurbidity(NTU)', y='Turbidity(NTU) at 8PM',alpha = 0.5,color = 'orange')\nplt.xlabel('Average 24hrTurbidity(NTU)') \nplt.ylabel('Turbidity(NTU) at 8PM')\nplt.title('8 PM vs Daily Turbidity Scatter Plot')","7285566d":"# This section covers some plots by using Matplotlib library\n# such as line, scatter and histogram plots\n\n# Line Plot \nwqdata['Average 24hrTurbidity(NTU)'].plot(kind='line', y='Average 24hrTurbidity(NTU)', x='Date', color='r', label='Turbidity Daily Avg.', linewidth=1, alpha=0.7, grid=True, linestyle='-')\nwqdata['Turbidity(NTU) at 12PM'].plot(color='b', y='Turbidity(NTU) at 12PM', x='Date', label='Turbidity at 12PM', linewidth=1, alpha=0.7, grid=True, linestyle='-')\n\nplt.legend(loc='upper right')     # legend = puts label into plot\n#plt.xlabel('Date')              # label = name of label\n#plt.ylabel('Turbidity (NTU)')\nplt.title('Line Plot')            # title = title of plot\nplt.show","65471c47":"wqdata.columns","c82405e3":"wqdata.head()","6a11ff22":"wqdata.plot(kind='scatter', x='Average 24hrTurbidity(NTU)', y='Turbidity(NTU) at 12PM', color='red', alpha=0.2, grid=True)\nplt.show()","6dd0016e":"wqdata['Average 24hrTurbidity(NTU)'].plot(kind='hist', color='blue', label='Turbidity Daily Avg.', bins=50, figsize=(15,15), alpha=0.7, grid=True)\n\nplt.show","32e1e7cf":"# We will create and play with pandas library using our data \nwqdata","c223f931":"wqdata[:6]","70f75f14":"wqdata[2:6]","63a8526e":"import pandas as pd \nwqdata = pd.read_csv('..\/input\/watershed-water-quality-data.csv')\n\n# We'll define two columns as separate data series \n# which are dates and corresponding turbidity avegare values\ndates=wqdata['Date']\nturbidity=wqdata['Average 24hrTurbidity(NTU)']\n\nprint(dates)\nprint(turbidity)","a0c0e7bf":"print(dates>'2018-12-12T00:00:00')","0dff422d":"print(turbidity<0.85)","62dee0d4":"# Now we define a filter to see the data exactly we wanted to see\ndate_filter = wqdata['Date']>'2018-12-27T00:00:00' # Last four days of the dataset\nwqdata[date_filter]","34d06f84":"val_filter = wqdata['Average 24hrTurbidity(NTU)']>0.55\nwqdata[val_filter]\n# Now we got the turbidity values which are greater than 0.95\n# 1440 rows to be exact, as noted in the bottom","f954693b":"# Now we use numpy logical_and operator to combine both filter \nwqdata[np.logical_and(wqdata['Date']>'2018-12-27T00:00:00', wqdata['Average 24hrTurbidity(NTU)']>0.55)]\n# We combined the filters and got data collected after specified date \n# and having values higher than 0.55","fe0b532e":"#gives same output \nwqdata[(wqdata['Date']>'2018-12-27T00:00:00')&(wqdata['Average 24hrTurbidity(NTU)']>0.55)]","08782271":"wqdata.loc[3:10, 'Date']","c099abc8":"wqdata.loc[3:10, ['Date','Turbidity(NTU) at 8AM', 'Turbidity(NTU) at 8PM']","2645ddea":"# Data Cleaning\nwqdata.head(3)","e48c82b2":"wqdata.shape","7de6e0c5":"wqdata.info()","37ab3c0e":"# We'd like to know the frequency of different values for a data column (attribute).\n# This is why we'd use .value_counts() method \n# This method is good to produce data frequency and visualize it by using histogram plot\nprint(wqdata['Average 24hrTurbidity(NTU)'].value_counts(dropna=False)) #dropna property is for considering NaN \/ null values","7654d1f9":"wqdata.describe()","7f1340bb":"wqdata.boxplot()","eebadb4b":"wqdata.boxplot(column='Average 24hrTurbidity(NTU)')","eb374b10":"# Data Melting \n\n# Melting a dataset actually unpivots it. This means, by melting a data, \n# we create a new dataset, insert entries by using the identifier value (ID column).\n# Identifier value is used in a new entry for each of the value column we choose. \n# For instance, we melt wqdata dataframe around 'Date' column and choose \n# 'Turbidity(NTU) at 12AM', 'Turbidity(NTU) at 12PM', and 'Average 24hrTurbidity(NTU)'\n# columns as value columns, we'll get 3x rows for the new dataframe since we'll see\n# a new row of Turbidity values of a date.","dfe4ee28":"wqdata.columns","b6311f26":"melted = pd.melt(frame=wqdata, id_vars='Date', value_vars=['Turbidity(NTU) at 12AM', 'Turbidity(NTU) at 12PM', 'Average 24hrTurbidity(NTU)'])\nmelted","df3cf7a8":"melted.pivot(index='Date', columns='variable', values='value') \n\n# this code should re-pivot selected four columns though it throws the error below\n# the reason is index column in our data has duplicates which should not be. \n# there is no way to build a column with unique values within this dataset right now \n# without changing the values or combining columns etc. ","d43dc729":"# Data Concatenation \n\n# This can bw done in two ways: \n# 1) Vertical which glues rows of two datasets with same number of columns,  pd.concat([dataset1, dataset2], axis=0, ignore_index=True)\n# 2) Horizontal which glues columns of two datasets with same number of rows,  pd.concat([dataset1, dataset2], axis=1)\n\ndataset1 = wqdata.head()\ndataset2 = wqdata.tail()\n\nvconcat = pd.concat([dataset1, dataset2], axis=0, ignore_index=True) # ignore_index=True re-index all data rows\nvconcat","97f0ab23":"dataset1 = wqdata.head()\ndataset2 = wqdata.tail()\n\nhconcat = pd.concat([dataset1.Date, dataset1['Turbidity(NTU) at 12AM'], dataset1['Turbidity(NTU) at 12PM']], axis=1)\nhconcat","b385628f":"# Our data could have columns with wrong data types\n# When we examine it with .dtypes attribute we see that\n# Coliform data is inputted as string\n\nwqdata.dtypes","e6b50572":"wqdata.head()\n# We can transform data types between object - categorical and float - integer\n# False definitions such as string-integer data could be transformed by using astype('integer') as well ","5ede6139":"wqdata['Site'] = wqdata['Site'].astype('category')\nwqdata['NewAvg'] = wqdata['Average 24hrTurbidity(NTU)'].astype('int32')\n#wqdata.dtypes\nwqdata['NewAvg']","544156e8":"# Missing Data Problem \n\n# 1. Leave 2.dropna(), 3.fillna(), 4.fill with statistics\n\nwqdata.info()","16877370":"wqdata['Turbidity(NTU) at 12AM'].value_counts(dropna=False) \n# We check value counts of each unique value in 'Turbidity(NTU) at 12AM' dataset \n# We DO count NaN \/ null values also since dropna=False","55df7cdc":"# Creating a new dataset by eliminating NaN values\nwqdataComplete=wqdata.copy()\nwqdataComplete['Turbidity(NTU) at 12AM'].dropna(inplace=True)\nwqdataComplete['Turbidity(NTU) at 12AM'].value_counts(dropna=False) ","d9a85635":"assert wqdata['Turbidity(NTU) at 12AM'].notnull().all() # Returns error since wqdata has NaN values","49dd21b8":"assert wqdataComplete['Turbidity(NTU) at 12AM'].notnull().all() # Returns nothing (which means it is true) since wqdataComplete has no NaN values","11919a40":"wqdataComplete['Turbidity(NTU) at 12AM'].fillna('empty', inplace=True)\nwqdataComplete['Turbidity(NTU) at 12AM'].value_counts()","3482b6a7":"wqdataCompleteNan = wqdata.copy()\nwqdataCompleteNan['Turbidity(NTU) at 12AM'].fillna('empty', inplace=True)\nwqdataCompleteNan['Turbidity(NTU) at 12AM'].value_counts() \n\n# Now we copied a new dataframe and replace NaN values with 'empty'\n# Value_counts() gives us the count of new 'empty' rows","fb2d6153":"# Now we create to lists called \"Months\" and \"Seasons\"\nMonths = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\nSeasons = [\"Winter\", \"Spring\", \"Summer\", \"Autumn\"]\n\nMonthNum = []\nfor i in range(12):\n    MonthNum.append(i)\nzipMonths = dict(zip(MonthNum, Months))\nzipMonths","e8694aea":"Seasons = [\"Winter\", \"Spring\", \"Summer\", \"Autumn\"]\nSeasons","c5442edb":"Seasons = [\"Winter\", \"Spring\", \"Summer\", \"Autumn\"]\nNewSeasons = []\ni = 0 #Season[i]\nwhile i < len(Seasons):\n    j = 0 #InsertPos\n    while j < 3:\n        NewSeasons.insert(3*i+j, Seasons[i])\n        j += 1\n    i += 1\nNewSeasons\nNewSeasons.append(NewSeasons[0])\nNewSeasons.pop(0)\nNewSeasons","a72cec00":"MonthsSeason = dict(zip(Months, NewSeasons))\nMonthsSeason","6e5ed99f":"# Now we create a new column called \"Sampling Season\" and assign a default value \"Winter\" to the whole column\nwqdata['Sampling Season']=\"Winter\"\nwqdata['Sampling Month']=wqdata.Date[:3]\nwqdata.head()\n","5022c2fe":"# Subplots \n\nwqdata.plot(subplots=True, figsize=(30,30))\nplt.show()","cc17b939":"wqdata.columns","b965c869":"# Renaming column names\n\nwqdata.rename(columns={'Site':'site', 'Date':'date', 'Turbidity(NTU) at 12AM':'turb12am', 'Turbidity(NTU) at 4AM':'turb4am',\n                       'Turbidity(NTU) at 8AM':'turb8am', 'Turbidity(NTU) at 12PM':'turb12pm','Turbidity(NTU) at 4PM':'turb4pm', \n                       'Turbidity(NTU) at 8PM':'turb8pm', 'Average 24hrTurbidity(NTU)':'turbavg', 'Coliform, Fecal(fc\/100mL)':'coliform',\n                       'Sampling Season':'samplingseason', 'Sampling Month':'samplingmonth'}, inplace=True)\nwqdata.info()","7ca9bed3":"# Changing date column's data type to 'datetime'\n\nwqdata.date = pd.to_datetime(wqdata.date)\nwqdata = wqdata.set_index(\"date\")\nwqdata.head()","9cb44cbd":"print(wqdata.loc['2015-05-11':'2015-05-30'])","61024cd3":"wqdata.columns","68413eec":"wqdata.resample('M').mean().head(15) # Shows a monthly mean values (limited with the first 15 months) of each column in the dataframe ","af106970":"wqdata.resample('A').max() # Shows a annual mean values of each column in the dataframe ","dde52256":"df1 = wqdata['turb12am'][2:12]\ndf1 = wqdata.reset_index()\n#df1 = wqdata.set_index(\"date\")\ndf1","ec45f71a":"df2 = wqdata.turb12am[2:12]\ndf2 = wqdata.reset_index()\ndf2","d0be5e3d":"df3 = wqdata.loc['2018-12-31', ['turb12am']]\ndf3","3a4c8c9c":"df4 = wqdata[['turb4am', 'turb4pm']]\ndf4\n","d9f2488a":"df6=wqdata.loc['2018-12-27':,'turb4am':'turb4pm']\ndf6","b1b956e5":"df7=wqdata.loc['2018-12-27'::-1,'turb4am':'turb4pm'] #reverse\ndf7","02f6d82b":"# Filtering Data Frame \n\nflt1 = wqdata.turb4am > 1.20\ndf11 = wqdata[flt1]\ndf11","caff570a":"flt1 = wqdata.turb4am > 1.20\nflt2 = wqdata.turb8am > 1.30\ndf12 = wqdata[flt1 & flt2]\ndf12","bf7c5749":"flt1 = wqdata.turb4am > 1.20\ndf13 = wqdata.turb4am[wqdata.turb4am > 1.20]\ndf13\n# df14 = wqdata.turb4am[flt1]\n# df14 ","d2b262f6":"# Transformation Function \n\n# We define a function that makes a unit conversion here \n# Create a new column and assigned each cell the value calculated with the defined function \n\ndef turbidity_unittransf(x):\n    return x**2+0.33\nwqdata['mew_turb4am'] = wqdata.turb4am.apply(turbidity_unittransf)\nwqdata.head()","2653cc87":"# Lambda function definition is a shorter way to do the same thing\n\nwqdata['mew_turb8am'] = wqdata.turb8am.apply(lambda z : z**2+0.33)\nwqdata.head()","a1e07f9d":"print(wqdata.index.name)","5d13dc79":"wqdata.index.name = 'index_date'\nwqdata.head()","7f88fe56":"wqdata.turb12am.rename = 'turb_12am'\nwqdata.head()","4e81cd9a":"x = range(3, 20, 3)\n         \nfor n in x:\n  print(n)","7e97a9d8":"wqdata['no']=range(1,1459,1)\nwqdata=wqdata.set_index('no')\nwqdata.tail()","1099988f":"# Group By \nwqdata.describe()\n\n# turbavg column has changing values between 0.45 and 1.43\n# lets create three intervals called low (<= 0.8), mid (0.8 - 1.2) and high (=>1.2)\n\ndef classify(x):\n    level=''\n    if (x<=0.8):\n        level='low'\n    elif (x>0.8 and x<1.2):\n        level='mid'\n    else:\n        level='high'\n    return level\n\nwqdata['turbiditylevel'] = wqdata.turbavg.apply(classify)\nwqdata.head(10)","12b1fe25":"# After classified our data by adding turbidity level column, we can use it for\n# aggregating and producing further statistics based on the 'group by'\n\nwqdata.groupby('turbiditylevel').mean() # We found mean of each turbidity level. We cam even find using min, max or std","9096042d":"# We filter the previous output with one column\nwqdata.groupby('turbiditylevel').turb12pm.mean()","5a7dbcb1":"# ...or more columns\nwqdata.groupby('turbiditylevel')[[\"turb12am\",\"turb12pm\"]].mean()","fd421f90":"**Indexing Data Frames**\n\n#1. Indexing using square brackets \/\/ *wqdata['turb12am'][2:12]*\n#2. Using column attribute and row label\n#3. Using loc accessor\n#4. Selecting only some columns\n","0de692fb":"**Bu k\u0131sm\u0131 d\u00fczenleyerek t\u00fcm veri serilerini tek plota \u00e7izdirelim**\n\nindex = [0,1,2,3,4,5]\nmarkers=['.','x','+','-','*','~',]\ncolors=['orange','blue','green','purple','red','yellow']\nintervals = ['Turbidity(NTU) at 12AM', 'Turbidity(NTU) at 4AM','Turbidity(NTU) at 8AM','Turbidity(NTU) at 12PM','Turbidity(NTU) at 4PM', 'Turbidity(NTU) at 8PM']\n\nfor i in index:\n    data.plot(kind='scatter', marker=markers[i], grid=True, x='Average 24hrTurbidity(NTU)', y=intervals[i], alpha = 0.5,color = colors[index]\n              \nplt.xlabel('Average 24hrTurbidity(NTU)') \nplt.ylabel('4-hour-Turbidity(NTU)')\nplt.title('4 PM vs Daily Turbidity Scatter Plot')"}}