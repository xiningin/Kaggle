{"cell_type":{"55daa545":"code","2531f838":"code","416ab1f2":"code","112982f0":"code","cc56450f":"code","af79c79c":"code","5be84a3e":"code","1789a248":"code","b9ad9cc8":"code","57293631":"code","7da36054":"code","71ae916c":"code","36570cb6":"code","82ec80c9":"code","078c424f":"code","e829b0ce":"code","2ec0a6d3":"code","520d0a4a":"code","dcafb99f":"code","b9ed3611":"code","a176aa6a":"code","4621a041":"code","8ef2e363":"code","aae27e4f":"code","0fb40da9":"code","095ac55c":"code","efeb0d09":"code","e0b9ae29":"code","2755dfbe":"code","348f0fb6":"code","3e6f67b4":"code","595b5bf7":"code","9e36036e":"code","ff47e342":"code","0efd573f":"code","da05b5cc":"code","ca3722b3":"code","e2220443":"code","1137e4a3":"code","de33e01f":"code","871b1d0f":"code","b11471ee":"code","d1f77153":"code","a3c782b8":"code","1199ec0d":"code","3da69d0e":"code","07b278bc":"code","3aa709dc":"code","2d00f49c":"code","5680a67f":"code","e7574868":"code","bb6ad7af":"code","c795099d":"code","5397de70":"code","3966d49b":"code","a40a1e5f":"code","e0fefd41":"code","e17a35d5":"code","47359241":"code","a402c89f":"code","8a697ed6":"code","6c8475f5":"code","78c8aec1":"code","52977bd1":"code","4289c7db":"code","2544501d":"code","b327cdbb":"code","074ffffc":"code","4bb833f8":"code","98f13fae":"code","cf3c01d3":"code","8fe60ffd":"code","d54769cf":"code","d32bee19":"code","69351c2a":"code","3f3e36fc":"code","2065e498":"code","1ef88912":"code","52962881":"code","2f917b77":"code","7223855a":"code","f0d58815":"code","9e7528f4":"code","15716271":"code","62527622":"code","017804ee":"code","3ac08c39":"code","c659644a":"code","f855b15e":"code","4f91a84f":"code","f2f63c7b":"code","2a15b18a":"code","5e39ef5a":"code","a37edab7":"code","a957b58e":"code","86486e2f":"code","14aad20d":"code","e78fc622":"code","2c2ee6b3":"code","5c8f7d4b":"code","f6b13566":"code","07c53a9e":"code","4fd62957":"code","6499501b":"code","dd0ef773":"code","12b462a9":"code","d07d8f3e":"code","524ea61c":"code","cdd92c1d":"code","0a9e12e9":"code","a3877561":"code","0aa1a5d0":"code","552f0bd2":"code","cea4ea0c":"markdown","d1f1e48b":"markdown","14be962e":"markdown","c7669111":"markdown","18ca3ed9":"markdown","ee9097f8":"markdown","865dbb9f":"markdown","65f762bd":"markdown","aa0049bd":"markdown","0331f35a":"markdown","cb403bb0":"markdown","b5734d22":"markdown","2fa993ab":"markdown","9ec48e48":"markdown","86a6afad":"markdown","bb51bad9":"markdown","027ad59d":"markdown","aeebd135":"markdown","63b13446":"markdown","a38e90dd":"markdown","cceb1759":"markdown","1ce5fb54":"markdown","4cf68ab0":"markdown","72608ba6":"markdown","d70e1183":"markdown","8e7dbbad":"markdown","24fda3d4":"markdown","59c6c378":"markdown","a0a60f41":"markdown","1c492587":"markdown","1007e4b7":"markdown","d98bf9d0":"markdown","c714df6a":"markdown","3d44fded":"markdown","616c096d":"markdown","0d6e8a2c":"markdown","02a16855":"markdown","f7334cad":"markdown","64f8cd5a":"markdown","97fce029":"markdown","5e538008":"markdown","9fdbb1cd":"markdown","dd1b8b5c":"markdown","31cbfe07":"markdown","011acb40":"markdown","f0b820cb":"markdown","317df4b5":"markdown","43223003":"markdown","423f665b":"markdown","566209a4":"markdown","01f2054c":"markdown","532f8e24":"markdown","7a4af0ee":"markdown","e0d2cb36":"markdown","e8a6e9dd":"markdown","b0b92e1f":"markdown","bd93f022":"markdown","08e5c09c":"markdown","d6498af5":"markdown","6456258f":"markdown","bfddc43f":"markdown"},"source":{"55daa545":"#clone forked IndoNLU Github\r\n!git clone 'https:\/\/github.com\/billiechristian\/indonlu'","2531f838":"!pip install transformers","416ab1f2":"import pandas as pd\nimport numpy as np\nimport re\nimport time\nimport timeit\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report as report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import KFold, cross_val_score, RepeatedKFold, cross_val_predict\nfrom xgboost import XGBClassifier\n\n# Viz\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport matplotlib.gridspec as gridspec\nimport matplotlib.patches as mpatches\nimport plotly.express as px\nimport seaborn as sns\nfrom wordcloud import WordCloud\nfrom PIL import Image\nsns.set(style='ticks', palette='Set2')\n\n# NLP\nimport nltk\nfrom Sastrawi.Stemmer.StemmerFactory import StemmerFactory\nfrom Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.probability import FreqDist\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tag import pos_tag\nimport emoji\n\n#Model IndoBERT\nimport random\nimport torch\nfrom torch import optim\nimport torch.nn.functional as F\nfrom tqdm import tqdm\n\nfrom transformers import BertForSequenceClassification, BertConfig, BertTokenizer\nfrom indonlu.utils.forward_fn import forward_sequence_classification\nfrom indonlu.utils.metrics import document_sentiment_metrics_fn\nfrom indonlu.utils.data_utils import DocumentSentimentDataset, DocumentSentimentDataLoader","112982f0":"# Import data\ntrain = pd.read_csv('train.csv')\ntest = pd.read_csv('test.csv')","cc56450f":"display(train.head(), test.head())","af79c79c":"print(f'Train shape: {train.shape}')\nprint(f'Test shape: {test.shape}')","5be84a3e":"train.info()","1789a248":"test.info()","b9ad9cc8":"nrows = len(train)\nsample_size = 0.01 # 10% of all data","57293631":"# Generate random indexes\nindexes = np.random.randint(0,len(train),size=int(nrows*sample_size))","7da36054":"# Mengubah setting display\npd.set_option('display.max_rows',None) # Display all rows\npd.set_option('display.max_colwidth',None) # Display all text\n\n# Display sample text\ntrain[['review_text','category']].loc[indexes,:]","71ae916c":"# Reset pandas display settings\npd.reset_option('^display.', silent=True)","36570cb6":"train.category.value_counts() # Imbalanced dataset","82ec80c9":"# Function to make a donut chart\ndef donut(sizes, ax, angle=90, labels=None,colors=None, explode=None, shadow=None):\n\n    # Plot\n    ax.pie(sizes, colors = colors, labels=labels, autopct='%.1f%%', \n           startangle = angle, pctdistance=0.8, explode = explode, \n           wedgeprops=dict(width=0.4), shadow=shadow)\n\n    # Formatting\n    plt.axis('equal')  \n    plt.tight_layout()","078c424f":"# Plot arguments\nsizes = train.category.value_counts()\nlabels = ['Review Buruk', 'Review Baik']\ncolors = ['lightskyblue', 'lightcoral']\nexplode = (0,0.1)","e829b0ce":"# plt.style.use('default')","2ec0a6d3":"# Create axes\nf, ax = plt.subplots(figsize=(6,4))\n\n# plot donut\ndonut(sizes, ax, 90, labels, colors=colors, explode=explode, shadow=True)\nax.set_title('Review Category Proportions')\n\nplt.show()","520d0a4a":"sample_emoji1 = train.review_text[4777]\nsample_emoji2 = train.review_text[7159]","dcafb99f":"display(sample_emoji1, sample_emoji2)","b9ed3611":"demojized1, demojized2 = emoji.demojize(sample_emoji1), emoji.demojize(sample_emoji2)\ndisplay(demojized1, demojized2)","a176aa6a":"# Both functions below will assist in retrieving text containing emojis.\n\n# Function to iterate over all review text (will return text if there is emoji)\ndef is_emoji(text):\n    demojized_txt = emoji.demojize(text)\n    res = re.findall(':[a-zA-Z_]+:', demojized_txt)\n    if res:\n        return text # return original text if emoji is present\n    \n# Function to return row indexes of emoji reviews\ndef emoji_indexes(series):\n    filtered = series.map(is_emoji).dropna()\n    return list(filtered.index)\n    ","4621a041":"# Row indexes for reviews that contain emoji\ntrain_emo_ix = emoji_indexes(train.review_text)\ntest_emo_ix = emoji_indexes(test.review_text)","8ef2e363":"# Only reviews that contain emoji\ntrain_emo_only = train.review_text.loc[train_emo_ix]\ntest_emo_only = test.review_text.loc[test_emo_ix]","aae27e4f":"# Change some display settings\n# pd.set_option('display.max_colwidth',None)\n\n# Show train text with emojis\ntrain[['review_text']].loc[train_emo_ix].head()","0fb40da9":"def unique_emojis(series):\n    emojis = [] # list to store all emojis in series\n    demojized = series.map(emoji.demojize)\n    for text in demojized:\n        emojis.extend(re.findall(':[a-z_]+:', text))\n    \n    emojis = set(emojis) # keep uniques\n    emojis = [emoji.emojize(emo) for emo in emojis]\n    \n    return emojis\n    ","095ac55c":"train_emo_set = unique_emojis(train_emo_only)\nprint(f'Count of unique emojis in train: {len(train_emo_set)}')\ntrain_emo_set","efeb0d09":"test_emo_set = unique_emojis(test_emo_only)\nprint(f'Count of unique emojis in test: {len(test_emo_set)}')\ntest_emo_set","e0b9ae29":"# Does all emojis in test exist in train?\nlen([_ for emo in test_emo_set if emo in train_emo_set])","2755dfbe":"# Reuse the is_emoji function from before.\ntrain['emoji'] = train.review_text.map(lambda x:'has emoji' if is_emoji(x) else 'no emoji')\ntrain.head()\n            ","348f0fb6":"# Count of emoji rows\nhas_emoji = (train['emoji']=='has emoji')\nprint(f'Number of reviews that contain emoji: {sum(has_emoji)}')","3e6f67b4":"# Count of categories for reviews that has emoji.\ntr_has_emoji_counts = train.emoji.value_counts()\ntr_has_emoji_counts","595b5bf7":"# Class distribution of emoji reviews\nemoji_classes = train[train.emoji=='has emoji'].category.value_counts()\nemoji_classes","9e36036e":"# Since test data has no labels, we can only see whether each review has an emoji i.e. no class distribution.\ntest['emoji'] = test.review_text.map(lambda x:'has emoji' if is_emoji(x) else 'no emoji')\ntest.head()","ff47e342":"# Count of categories for reviews that has emoji\nts_has_emoji_counts = test.emoji.value_counts()\nts_has_emoji_counts","0efd573f":"# plt.style.use('default')","da05b5cc":"# Alter a few params\nmpl.rcParams['xtick.labelsize'] = 25\nmpl.rcParams['ytick.labelsize'] = 25\nmpl.rcParams['axes.titlesize'] = 25\nmpl.rcParams['font.size'] = 25\n\n# Create axes\nf, (ax,ax2) = plt.subplots(1,2,figsize=(20,10))\n\n# Data to be visualized\nno_emo = [14438, 3623]\nhas_emo = [418, 91]\nix = [0,1]\nlab = ['Train', 'Test']\n\n# Stacked bar\nax.bar(lab, no_emo, width=0.8, label='No Emoji') # no emo\nax.bar(lab, has_emo, width=0.8, label='Has Emoji', bottom=no_emo) # has emo\nax.set_title('Emoji Proportions in Train & Test Reviews', fontsize=25)\nax.set_ylabel('Count of rows', fontsize=25)\nax.legend(prop={'size':20})\n\n# Text on bar\nfor i,p in enumerate(ax.patches):\n    width = p.get_width()\n    height = p.get_height()\n    x, y = p.get_xy() \n    ax.annotate(f'{height} rows', (x+width\/2, height\/2 if i<=1 else y+height*1.05), ha='center', fontsize=20)\n    \n# Donut chart\nsizes, labels = emoji_classes.values, ['Buruk (0)', 'Baik (1)']\ndonut(sizes,ax2,60,labels, colors=colors, explode=None, shadow=True)\nax2.set_title('Emoji Class Distribution - Train Dataset')\n    \n# Show plot    \nplt.tight_layout()\nplt.show()","ca3722b3":"del train['emoji']\ndel test['emoji']","e2220443":"def clean_string(text):\n    cleaned = re.sub('[^a-zA-Z]+',' ',text).lower() # exclude numbers\n    return cleaned","1137e4a3":"def make_corpus(column):\n    \n    corpus_list = []\n    additional_stop = ['yg', 'nya']\n    all_stopwords = stopwords.words('indonesian') + additional_stop\n    \n    for text in column:\n        cleaned = clean_string(text)\n        cleaned_list = cleaned.split(' ')\n        \n        # Stem words to shortest form\n        stemindo = StemmerFactory().create_stemmer()\n        stemmed_list = map(stemindo.stem, cleaned_list)\n        \n        # Remove stopwords\n        stemmed_list = [word for word in cleaned_list if word not in all_stopwords]\n        corpus_list.extend(stemmed_list)\n        \n    # transform list of words into 1 body of text\n    corpus = ' '.join(corpus_list)\n    corpus = re.sub('[ ]+',' ',corpus) # replace double whitespace with one\n        \n    return corpus","de33e01f":"# Takes a while\ntrain_corpus = make_corpus(train.review_text)\ntest_corpus = make_corpus(test.review_text)","871b1d0f":"train_corpus_set = set(train_corpus.split(' '))\ntest_corpus_set = set(test_corpus.split(' '))\n\nprint(f'Count of unique words in train: {len(train_corpus_set)}')\nprint(f'Count of unique words in test: {len(test_corpus_set)}')","b11471ee":"nltk.download('punkt') # for freqdist","d1f77153":"# function for freqdist\ndef word_freq(corpus, top=5):\n    tokenized_word = word_tokenize(corpus)\n    freqdist = FreqDist(tokenized_word)\n    freqdist = freqdist.most_common(top) # list of tuples \n    \n    # decompose into label and frequency\n    label = [tup[0] for tup in freqdist]\n    freq = [tup[1] for tup in freqdist]\n    df = pd.DataFrame({'word':label, 'freq':freq})\n    \n    return df","a3c782b8":"train_freq = word_freq(train_corpus, top=10)\ntest_freq = word_freq(test_corpus, top=10)","1199ec0d":"# Config params\nplt.style.use('default')\nsns.set(style='ticks', palette='Set2')\nmpl.rcParams['axes.titlesize'] = 20\nmpl.rcParams['axes.titlepad'] = 20\n\n# Compare plots of train and test corpus\nf, (ax1, ax2) = plt.subplots(1,2,figsize=(15,5))\n\nsns.barplot(x='word', y='freq', data=train_freq, ax=ax1)\nax1.set_title('Word Frequency in Train Data')\n\nsns.barplot(x='word', y='freq', data=test_freq, ax=ax2)\nax2.set_title('Word Frequency in Test Data')\n\n# sns.despine(ax=ax1)\n# sns.despine(ax=ax2)\nplt.show()","3da69d0e":"# Stopwords\nadditional_stop = ['nya','yg','ga','gk','tp']\nall_stopwords = stopwords.words('indonesian') + additional_stop","07b278bc":"# Cloud for train corpus\nwordcloud = WordCloud(stopwords=all_stopwords,max_font_size=50, max_words=100, background_color=\"white\").generate(train_corpus)\nplt.figure()\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","3aa709dc":"# Cloud for test corpus\nwordcloud = WordCloud(stopwords=all_stopwords,max_font_size=50, max_words=100, background_color=\"white\").generate(test_corpus)\nplt.figure()\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","2d00f49c":"# Clear Non-Alphanumeric and lowercase\r\nedatrain = train.copy()\r\nedatest = test.copy()\r\nedatrain['review_text'] = edatrain['review_text'].apply(lambda x : re.sub('[^a-zA-Z0-9]',' ',x).lower())\r\nedatest['review_text'] = edatest['review_text'].apply(lambda x : re.sub('[^a-zA-Z0-9]',' ',x).lower())","5680a67f":"# Pembuatan list kosong untuk menginkorporasi 'review_id' yang berisikan konten non-alphanumeric.\r\nproblematic = []\r\n\r\n# Pembersihan whitespace dari seluruh record dataset.\r\nantinan = edatrain.copy()\r\nantinan['string'] = antinan['review_text'].apply(lambda x : re.sub('\\s','',x))\r\n\r\n# Pencarian seluruh 'review_id' yang tidak memiliki apapun di dalam recordnya.\r\nerrors = 0\r\nfor i in range (0,len(antinan.index)):\r\n    if antinan['string'][i] == '':\r\n        problematic.append(antinan['review_id'][i])\r\n        errors += 1\r\n\r\nprint('Found ', errors, ' errors!')\r\nproblematic","e7574868":"problematicreviews = train[edatrain['review_id'].apply(lambda x : x in problematic)]\r\nproblematicreviews","bb6ad7af":"# Tokenization\r\nedatrain['tokenized'] = edatrain['review_text'].apply(lambda x : x.split())\r\nedatest['tokenized'] = edatest['review_text'].apply(lambda x : x.split())","c795099d":"# Pencarian panjang dari tiap list yang ada pada kolom 'tokenized'\r\nedatrain['length_of_review'] = edatrain['tokenized'].apply(lambda x : len(x))\r\nedatest['length_of_review'] = edatest['tokenized'].apply(lambda x : len(x))\r\n\r\n# Menunjukkan kolom 'review_id' serta 'length_of_review'\r\nprint(edatrain[['review_id','length_of_review']], '\\n', edatest[['review_id','length_of_review']])","5397de70":"# Metrik untuk rekapitulasi terkait panjang ulasan.\r\nmaxtrain = max(edatrain['length_of_review'])\r\navgtrain = sum(edatrain['length_of_review'])\/len(edatrain.index)\r\nmintrain = min(edatrain['length_of_review'])\r\ndevtrain = np.std(edatrain['length_of_review'])\r\nmaxtest = max(edatest['length_of_review'])\r\navgtest = sum(edatest['length_of_review'])\/len(edatest.index)\r\nmintest = min(edatest['length_of_review'])\r\ndevtest = np.std(edatest['length_of_review'])\r\n\r\nprint('Rekapitulasi dari panjang ulasan:\\n')\r\nprint('- Train -')\r\nprint('Max = ', maxtrain, '\\nMin = ', mintrain, '\\nAverage = ', avgtrain,'\\nStdDev = ', devtrain)\r\nprint('\\n- Test -')\r\nprint('Max = ', maxtest, '\\nMin = ', mintest, '\\nAverage = ', avgtest, '\\nStdDev = ', devtest)","3966d49b":"# Penggunaan library plotly untuk menggambarkan Histogram frekuensi dari panjang ulasan.\r\nfig1 = px.histogram(x = edatrain['length_of_review'], nbins = 200)\r\nfig2 = px.histogram(x = edatest['length_of_review'], nbins = 200)\r\nfig1.show()\r\nfig2.show()","a40a1e5f":"# train val split\ntrain_set, val_set = train_test_split(train, test_size=0.2, stratify=train.category, random_state=1)\ntest_set = test.copy()","e0fefd41":"# Functions used in preprocessing v1.\n\n# 1. Clean \ndef clean_review(text):\n\n    # demojize first\n    cleaned = emoji.demojize(text)\n    \n    # double whitespace to single\n    cleaned = re.sub('[ ]+',' ',cleaned)\n    \n    return cleaned\n\n# 2. Full clean\ndef full_clean(text):\n\n    # demojize first\n    text = emoji.demojize(text)\n    \n    # clean\n    cleaned = re.sub('[^a-zA-Z0-9]+',' ',text).lower() # clean everything (emoji inc.)\n    \n    # remove 2 letter words\n    shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n    cleaned = re.sub(shortword, '', cleaned)\n    \n    # double whitespace to single\n    cleaned = re.sub('[ ]+',' ',cleaned)\n    \n    return cleaned\n\n# 3. Func to find 'en' in set of words\ndef find_en(row, ignore_indexes):\n    result = 'no en'\n    \n    if re.findall(':[a-zA-Z_]+:', row.review_text):\n        result = 'has en'\n    elif row.name in ignore_indexes:\n        result = 'no en'\n    elif detect(row.review_text) == 'en':\n        result = 'has en'\n    else:\n        result = 'no en'\n        \n    return result\n\n# 4. remove punc from emoji\ndef remove_emo_punc(text):\n    text = re.sub('[:_-]+',' ',text)\n    return text\n\n# 5. Google Translate\ntranslator = google_translator() \ndef trans_to_id(text):\n    res = translator.translate(text,lang_src='en',lang_tgt='id')  \n    return res","e17a35d5":"def preprocess_v1(df):\n    df_pp = df.copy()\n    \n    # demojize all reviews (clean_text function)\n    df_pp.review_text = df_pp.review_text.map(clean_review)\n    \n    # find abnormal rows (full_clean function)\n    df_fcleaned = df_pp.review_text.map(full_clean)\n    ## indexes of garbage rows (reviews with no text, only punctuation, etc.)\n    df_empty_after_cleaned_ix = df_fcleaned.loc[(df_fcleaned=='')|(df_fcleaned==' ')].index\n    \n    # locate reviews that has english (find_en function)\n    df_en_res = df_pp.apply(find_en, args=(df_empty_after_cleaned_ix,), axis=1)\n    ## create mask from results\n    df_has_en_mask = (df_en_res == 'has en')\n    ### final english indexes\n    df_has_en_ix = [i for i,j in enumerate(df_has_en_mask) if j]\n    \n    # remove punctuation from emoji (remove_emo_punc function)\n    df_pp.review_text.loc[df_has_en_mask] = df_pp.review_text.loc[df_has_en_mask].map(remove_emo_punc)\n    \n    # translate rows that has english (trans_to_id)\n    df_en_translated = df_pp.review_text.loc[df_has_en_mask].map(trans_to_id)\n    \n    # combine translated rows with initial dataframe\n    df_pp.review_text.loc[df_has_en_ix] = df_en_translated\n    \n    return df_pp\n    ","47359241":"train_v1, val_v1, test_v1,  = preprocess_v1(train_set), preprocess_v1(val_set), preprocess_v1(test_set)","a402c89f":"# export to csv\ntrain_v1.drop('review_id', axis=1).to_csv('train_v1.csv', index=False)\nval_v1.drop('review_id', axis=1).to_csv('val_v1.csv', index=False)\n\n# add empty category column to test_v1 b4 exporting\ntest_v1['category'] = np.zeros(len(test_v1))\ntest_v1.drop('review_id', axis=1).to_csv('test_v1.csv', index=False)\n\n","8a697ed6":"# Functions used in preprocessing v2. (mostly the same but some slightly changed)\n\n# 1. Clean from emoji so lang.detect can determine eng rows\ndef clean_review(text):\n\n    # demojize first\n    cleaned = emoji.demojize(text)\n    cleaned = re.sub(':[A-Za-z_]+:', '', cleaned) # delete emot\n    \n    # double whitespace to single\n    cleaned = re.sub('[ ]+',' ',cleaned)\n    \n    return cleaned\n\n# 2. Same function as full_clean\n\n# 3. func to find 'en' in set of words\ndef find_en(row, ignore_indexes):\n    result = 'no en'\n\n    if row.name in ignore_indexes:\n        result = 'no en'\n    elif detect(row.review_text) == 'en':\n        result = 'has en'\n    else:\n        result = 'no en'\n        \n    return result\n\n# 4. Google Translate\ntranslator = google_translator() \ndef trans_to_id(text):\n    res = translator.translate(text,lang_src='en',lang_tgt='id')  \n    return res","6c8475f5":"def preprocess_v2(df):\n    df_pp = df.copy()\n    \n    # demojize all reviews (clean_text function)\n    df_pp.review_text = df_pp.review_text.map(clean_review)\n    \n    # find abnormal rows (full_clean function)\n    df_fcleaned = df_pp.review_text.map(full_clean)\n    ## indexes of garbage rows (reviews with no text, only punctuation, etc.)\n    df_empty_after_cleaned_ix = df_fcleaned.loc[(df_fcleaned=='')|(df_fcleaned==' ')].index\n    \n    # locate reviews that has english (find_en function)\n    df_en_res = df_pp.apply(find_en, args=(df_empty_after_cleaned_ix,), axis=1)\n    ## create mask from results\n    df_has_en_mask = (df_en_res == 'has en')\n    ### final english indexes\n    df_has_en_ix = [i for i,j in enumerate(df_has_en_mask) if j]\n    \n    # translate rows that has english (trans_to_id)\n    df_en_translated = df_pp.review_text.loc[df_has_en_mask].map(trans_to_id)\n    \n    # combine translated rows with initial dataframe\n    df_pp.review_text.loc[df_has_en_ix] = df_en_translated\n    \n    return df_pp","78c8aec1":"train_v2, val_v2, test_v2  = preprocess_v2(train_set), preprocess_v2(val_set), preprocess_v2(test_set)","52977bd1":"# export to csv\ntrain_v2.drop('review_id', axis=1).to_csv('train_v2.csv', index=False)\nval_v2.drop('review_id', axis=1).to_csv('val_v2.csv', index=False)\n\n# add empty category column to test_v1 b4 exporting\ntest_v2['category'] = np.zeros(len(test_v2))\ntest_v2.drop('review_id', axis=1).to_csv('test_v2.csv', index=False)\n","4289c7db":"###\r\n# common functions\r\n###\r\ndef set_seed(seed):\r\n    random.seed(seed)\r\n    np.random.seed(seed)\r\n    torch.manual_seed(seed)\r\n    torch.cuda.manual_seed(seed)\r\n    \r\ndef count_param(module, trainable=False):\r\n    if trainable:\r\n        return sum(p.numel() for p in module.parameters() if p.requires_grad)\r\n    else:\r\n        return sum(p.numel() for p in module.parameters())\r\n    \r\ndef get_lr(optimizer):\r\n    for param_group in optimizer.param_groups:\r\n        return param_group['lr']\r\n\r\ndef metrics_to_string(metric_dict):\r\n    string_list = []\r\n    for key, value in metric_dict.items():\r\n        string_list.append('{}:{:.5f}'.format(key, value))\r\n    return ' '.join(string_list)","2544501d":"# Set random seed\r\nset_seed(27)","b327cdbb":"# Load Tokenizer and Config\r\ntokenizer = BertTokenizer.from_pretrained('indobenchmark\/indobert-large-p1')\r\nconfig = BertConfig.from_pretrained('indobenchmark\/indobert-large-p1')\r\nconfig.num_labels = DocumentSentimentDataset.NUM_LABELS # 2 labels\r\n\r\n# Instantiate model\r\nmodel = BertForSequenceClassification.from_pretrained('indobenchmark\/indobert-large-p1', config=config)","074ffffc":"# Struktur model\r\nmodel","4bb833f8":"# Hitung jumlah parameter modal\r\ncount_param(model)","98f13fae":"# export to csv\r\ntrain_set.drop('review_id', axis=1).to_csv('train_ori.csv', index=False)\r\nval_set.drop('review_id', axis=1).to_csv('val_ori.csv', index=False)\r\n\r\n# add empty category column to test_v1 b4 exporting\r\ntest_set['category'] = np.zeros(len(test_v1))\r\ntest_set.drop('review_id', axis=1).to_csv('test_ori.csv', index=False)","cf3c01d3":"train_dataset_path = '\/content\/train_ori.csv'\r\nvalid_dataset_path = '\/content\/val_ori.csv'\r\ntest_dataset_path = '\/content\/test_ori.csv'","8fe60ffd":"#fungsi dataset loader dari utils IndoNLU\r\ntrain_dataset = DocumentSentimentDataset(train_dataset_path, tokenizer, lowercase=True)\r\nvalid_dataset = DocumentSentimentDataset(valid_dataset_path, tokenizer, lowercase=True)\r\ntest_dataset = DocumentSentimentDataset(test_dataset_path, tokenizer, lowercase=True)\r\n\r\ntrain_loader = DocumentSentimentDataLoader(dataset=train_dataset, max_seq_len=512, batch_size=16, num_workers=16, shuffle=True)  \r\nvalid_loader = DocumentSentimentDataLoader(dataset=valid_dataset, max_seq_len=512, batch_size=16, num_workers=16, shuffle=False)  \r\ntest_loader = DocumentSentimentDataLoader(dataset=test_dataset, max_seq_len=512, batch_size=16, num_workers=16, shuffle=False)","d54769cf":"w2i, i2w = DocumentSentimentDataset.LABEL2INDEX, DocumentSentimentDataset.INDEX2LABEL\r\nprint(w2i) #word to index\r\nprint(i2w) #index to word","d32bee19":"text = 'Kamarnya nyaman, bersih, pelayanannya bagus.'\r\nsubwords = tokenizer.encode(text)\r\nsubwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\r\n\r\nlogits = model(subwords)[0]\r\nlabel = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\r\n\r\nprint(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')","69351c2a":"# Tentukan optimizer\r\noptimizer = optim.Adam(model.parameters(), lr=3e-6)\r\nmodel = model.cuda()","3f3e36fc":"# Train\r\nn_epochs = 3\r\nfor epoch in range(n_epochs):\r\n    model.train()\r\n    torch.set_grad_enabled(True)\r\n \r\n    total_train_loss = 0\r\n    list_hyp_train, list_label = [], []\r\n\r\n    train_pbar = tqdm(train_loader, leave=True, total=len(train_loader))\r\n    for i, batch_data in enumerate(train_pbar):\r\n        # Forward model\r\n        loss, batch_hyp, batch_label = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\r\n\r\n        # Update model\r\n        optimizer.zero_grad()\r\n        loss.backward()\r\n        optimizer.step()\r\n\r\n        tr_loss = loss.item()\r\n        total_train_loss = total_train_loss + tr_loss\r\n\r\n        # Hitung skor train metrics\r\n        list_hyp_train += batch_hyp\r\n        list_label += batch_label\r\n\r\n        train_pbar.set_description(\"(Epoch {}) TRAIN LOSS:{:.4f} LR:{:.8f}\".format((epoch+1),\r\n            total_train_loss\/(i+1), get_lr(optimizer)))\r\n\r\n    metrics = document_sentiment_metrics_fn(list_hyp_train, list_label)\r\n    print(\"(Epoch {}) TRAIN LOSS:{:.4f} {} LR:{:.8f}\".format((epoch+1),\r\n        total_train_loss\/(i+1), metrics_to_string(metrics), get_lr(optimizer)))\r\n\r\n    # Evaluate di validation set\r\n    model.eval()\r\n    torch.set_grad_enabled(False)\r\n    \r\n    total_loss, total_correct, total_labels = 0, 0, 0\r\n    list_hyp, list_label = [], []\r\n\r\n    pbar = tqdm(valid_loader, leave=True, total=len(valid_loader))\r\n    for i, batch_data in enumerate(pbar):\r\n        batch_seq = batch_data[-1]        \r\n        loss, batch_hyp, batch_label = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\r\n        \r\n        # Hitung total loss\r\n        valid_loss = loss.item()\r\n        total_loss = total_loss + valid_loss\r\n\r\n        # Hitung skor evaluation metrics\r\n        list_hyp += batch_hyp\r\n        list_label += batch_label\r\n        metrics = document_sentiment_metrics_fn(list_hyp, list_label)\r\n\r\n        pbar.set_description(\"VALID LOSS:{:.4f} {}\".format(total_loss\/(i+1), metrics_to_string(metrics)))\r\n        \r\n    metrics = document_sentiment_metrics_fn(list_hyp, list_label)\r\n    print(\"(Epoch {}) VALID LOSS:{:.4f} {}\".format((epoch+1),\r\n        total_loss\/(i+1), metrics_to_string(metrics)))\r\n    ","2065e498":"# Hasil Validasi\r\nval_df = pd.read_csv(valid_dataset_path)\r\nval_df['pred'] = list_hyp\r\nval_df.head()\r\nval_df.to_csv('val result original.csv', index=False)","1ef88912":"# Prediksi test set\r\nmodel.eval()\r\ntorch.set_grad_enabled(False)\r\n\r\ntotal_loss, total_correct, total_labels = 0, 0, 0\r\npred, list_label = [], []\r\n\r\npbar = tqdm(test_loader, leave=True, total=len(test_loader))\r\nfor i, batch_data in enumerate(pbar):\r\n    _, batch_hyp, _ = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\r\n    pred += batch_hyp\r\n\r\n# Simpan prediksi\r\ndf = pd.read_csv('sample_submission.csv')\r\ndf['category'] = pred\r\ndf.to_csv('sub original.csv', index=False)\r\n\r\nprint(df)","52962881":"###\r\n# common functions\r\n###\r\ndef set_seed(seed):\r\n    random.seed(seed)\r\n    np.random.seed(seed)\r\n    torch.manual_seed(seed)\r\n    torch.cuda.manual_seed(seed)\r\n    \r\ndef count_param(module, trainable=False):\r\n    if trainable:\r\n        return sum(p.numel() for p in module.parameters() if p.requires_grad)\r\n    else:\r\n        return sum(p.numel() for p in module.parameters())\r\n    \r\ndef get_lr(optimizer):\r\n    for param_group in optimizer.param_groups:\r\n        return param_group['lr']\r\n\r\ndef metrics_to_string(metric_dict):\r\n    string_list = []\r\n    for key, value in metric_dict.items():\r\n        string_list.append('{}:{:.5f}'.format(key, value))\r\n    return ' '.join(string_list)","2f917b77":"# Set random seed\r\nset_seed(27)","7223855a":"# Load Tokenizer and Config\r\ntokenizer = BertTokenizer.from_pretrained('indobenchmark\/indobert-large-p1')\r\nconfig = BertConfig.from_pretrained('indobenchmark\/indobert-large-p1')\r\nconfig.num_labels = DocumentSentimentDataset.NUM_LABELS # 2 labels\r\n\r\n# Instantiate model\r\nmodel = BertForSequenceClassification.from_pretrained('indobenchmark\/indobert-large-p1', config=config)","f0d58815":"# Struktur model\r\nmodel","9e7528f4":"# Hitung jumlah parameter modal\r\ncount_param(model)","15716271":"train_dataset_path = '\/content\/train_v1.csv'\r\nvalid_dataset_path = '\/content\/val_v1.csv'\r\ntest_dataset_path = '\/content\/test_v1.csv'","62527622":"#fungsi dataset loader dari utils IndoNLU\r\ntrain_dataset = DocumentSentimentDataset(train_dataset_path, tokenizer, lowercase=True)\r\nvalid_dataset = DocumentSentimentDataset(valid_dataset_path, tokenizer, lowercase=True)\r\ntest_dataset = DocumentSentimentDataset(test_dataset_path, tokenizer, lowercase=True)\r\n\r\ntrain_loader = DocumentSentimentDataLoader(dataset=train_dataset, max_seq_len=512, batch_size=16, num_workers=16, shuffle=True)  \r\nvalid_loader = DocumentSentimentDataLoader(dataset=valid_dataset, max_seq_len=512, batch_size=16, num_workers=16, shuffle=False)  \r\ntest_loader = DocumentSentimentDataLoader(dataset=test_dataset, max_seq_len=512, batch_size=16, num_workers=16, shuffle=False)","017804ee":"w2i, i2w = DocumentSentimentDataset.LABEL2INDEX, DocumentSentimentDataset.INDEX2LABEL\r\nprint(w2i) #word to index\r\nprint(i2w) #index to word","3ac08c39":"text = 'Kamarnya nyaman, bersih, pelayanannya bagus.'\r\nsubwords = tokenizer.encode(text)\r\nsubwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\r\n\r\nlogits = model(subwords)[0]\r\nlabel = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\r\n\r\nprint(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')","c659644a":"# Tentukan optimizer\r\noptimizer = optim.Adam(model.parameters(), lr=3e-6)\r\nmodel = model.cuda()","f855b15e":"# Train\r\nn_epochs = 3\r\nfor epoch in range(n_epochs):\r\n    model.train()\r\n    torch.set_grad_enabled(True)\r\n \r\n    total_train_loss = 0\r\n    list_hyp_train, list_label = [], []\r\n\r\n    train_pbar = tqdm(train_loader, leave=True, total=len(train_loader))\r\n    for i, batch_data in enumerate(train_pbar):\r\n        # Forward model\r\n        loss, batch_hyp, batch_label = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\r\n\r\n        # Update model\r\n        optimizer.zero_grad()\r\n        loss.backward()\r\n        optimizer.step()\r\n\r\n        tr_loss = loss.item()\r\n        total_train_loss = total_train_loss + tr_loss\r\n\r\n        # Hitung skor train metrics\r\n        list_hyp_train += batch_hyp\r\n        list_label += batch_label\r\n\r\n        train_pbar.set_description(\"(Epoch {}) TRAIN LOSS:{:.4f} LR:{:.8f}\".format((epoch+1),\r\n            total_train_loss\/(i+1), get_lr(optimizer)))\r\n\r\n    metrics = document_sentiment_metrics_fn(list_hyp_train, list_label)\r\n    print(\"(Epoch {}) TRAIN LOSS:{:.4f} {} LR:{:.8f}\".format((epoch+1),\r\n        total_train_loss\/(i+1), metrics_to_string(metrics), get_lr(optimizer)))\r\n\r\n    # Evaluate di validation set\r\n    model.eval()\r\n    torch.set_grad_enabled(False)\r\n    \r\n    total_loss, total_correct, total_labels = 0, 0, 0\r\n    list_hyp, list_label = [], []\r\n\r\n    pbar = tqdm(valid_loader, leave=True, total=len(valid_loader))\r\n    for i, batch_data in enumerate(pbar):\r\n        batch_seq = batch_data[-1]        \r\n        loss, batch_hyp, batch_label = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\r\n        \r\n        # Hitung total loss\r\n        valid_loss = loss.item()\r\n        total_loss = total_loss + valid_loss\r\n\r\n        # Hitung skor evaluation metrics\r\n        list_hyp += batch_hyp\r\n        list_label += batch_label\r\n        metrics = document_sentiment_metrics_fn(list_hyp, list_label)\r\n\r\n        pbar.set_description(\"VALID LOSS:{:.4f} {}\".format(total_loss\/(i+1), metrics_to_string(metrics)))\r\n        \r\n    metrics = document_sentiment_metrics_fn(list_hyp, list_label)\r\n    print(\"(Epoch {}) VALID LOSS:{:.4f} {}\".format((epoch+1),\r\n        total_loss\/(i+1), metrics_to_string(metrics)))\r\n    ","4f91a84f":"# Hasil Validasi\r\nval_df = pd.read_csv(valid_dataset_path)\r\nval_df['pred'] = list_hyp\r\nval_df.head()\r\nval_df.to_csv('val result v1.csv', index=False)","f2f63c7b":"# Prediksi test set\r\nmodel.eval()\r\ntorch.set_grad_enabled(False)\r\n\r\ntotal_loss, total_correct, total_labels = 0, 0, 0\r\npred, list_label = [], []\r\n\r\npbar = tqdm(test_loader, leave=True, total=len(test_loader))\r\nfor i, batch_data in enumerate(pbar):\r\n    _, batch_hyp, _ = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\r\n    pred += batch_hyp\r\n\r\n# Simpan prediksi\r\ndf = pd.read_csv('sample_submission.csv')\r\ndf['category'] = pred\r\ndf.to_csv('sub v1.csv', index=False)\r\n\r\nprint(df)","2a15b18a":"###\r\n# common functions\r\n###\r\ndef set_seed(seed):\r\n    random.seed(seed)\r\n    np.random.seed(seed)\r\n    torch.manual_seed(seed)\r\n    torch.cuda.manual_seed(seed)\r\n    \r\ndef count_param(module, trainable=False):\r\n    if trainable:\r\n        return sum(p.numel() for p in module.parameters() if p.requires_grad)\r\n    else:\r\n        return sum(p.numel() for p in module.parameters())\r\n    \r\ndef get_lr(optimizer):\r\n    for param_group in optimizer.param_groups:\r\n        return param_group['lr']\r\n\r\ndef metrics_to_string(metric_dict):\r\n    string_list = []\r\n    for key, value in metric_dict.items():\r\n        string_list.append('{}:{:.5f}'.format(key, value))\r\n    return ' '.join(string_list)","5e39ef5a":"# Set random seed\r\nset_seed(27)","a37edab7":"# Load Tokenizer and Config\r\ntokenizer = BertTokenizer.from_pretrained('indobenchmark\/indobert-large-p1')\r\nconfig = BertConfig.from_pretrained('indobenchmark\/indobert-large-p1')\r\nconfig.num_labels = DocumentSentimentDataset.NUM_LABELS # 2 labels\r\n\r\n# Instantiate model\r\nmodel = BertForSequenceClassification.from_pretrained('indobenchmark\/indobert-large-p1', config=config)","a957b58e":"# Struktur model\r\nmodel","86486e2f":"# Hitung jumlah parameter modal\r\ncount_param(model)","14aad20d":"train_dataset_path = '\/content\/train_v2.csv'\r\nvalid_dataset_path = '\/content\/val_v2.csv'\r\ntest_dataset_path = '\/content\/test_v2.csv'","e78fc622":"#fungsi dataset loader dari utils IndoNLU\r\ntrain_dataset = DocumentSentimentDataset(train_dataset_path, tokenizer, lowercase=True)\r\nvalid_dataset = DocumentSentimentDataset(valid_dataset_path, tokenizer, lowercase=True)\r\ntest_dataset = DocumentSentimentDataset(test_dataset_path, tokenizer, lowercase=True)\r\n\r\ntrain_loader = DocumentSentimentDataLoader(dataset=train_dataset, max_seq_len=512, batch_size=16, num_workers=16, shuffle=True)  \r\nvalid_loader = DocumentSentimentDataLoader(dataset=valid_dataset, max_seq_len=512, batch_size=16, num_workers=16, shuffle=False)  \r\ntest_loader = DocumentSentimentDataLoader(dataset=test_dataset, max_seq_len=512, batch_size=16, num_workers=16, shuffle=False)","2c2ee6b3":"w2i, i2w = DocumentSentimentDataset.LABEL2INDEX, DocumentSentimentDataset.INDEX2LABEL\r\nprint(w2i) #word to index\r\nprint(i2w) #index to word","5c8f7d4b":"text = 'Kamarnya nyaman, bersih, pelayanannya bagus.'\r\nsubwords = tokenizer.encode(text)\r\nsubwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\r\n\r\nlogits = model(subwords)[0]\r\nlabel = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\r\n\r\nprint(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')","f6b13566":"# Tentukan optimizer\r\noptimizer = optim.Adam(model.parameters(), lr=3e-6)\r\nmodel = model.cuda()","07c53a9e":"# Train\r\nn_epochs = 3\r\nfor epoch in range(n_epochs):\r\n    model.train()\r\n    torch.set_grad_enabled(True)\r\n \r\n    total_train_loss = 0\r\n    list_hyp_train, list_label = [], []\r\n\r\n    train_pbar = tqdm(train_loader, leave=True, total=len(train_loader))\r\n    for i, batch_data in enumerate(train_pbar):\r\n        # Forward model\r\n        loss, batch_hyp, batch_label = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\r\n\r\n        # Update model\r\n        optimizer.zero_grad()\r\n        loss.backward()\r\n        optimizer.step()\r\n\r\n        tr_loss = loss.item()\r\n        total_train_loss = total_train_loss + tr_loss\r\n\r\n        # Hitung skor train metrics\r\n        list_hyp_train += batch_hyp\r\n        list_label += batch_label\r\n\r\n        train_pbar.set_description(\"(Epoch {}) TRAIN LOSS:{:.4f} LR:{:.8f}\".format((epoch+1),\r\n            total_train_loss\/(i+1), get_lr(optimizer)))\r\n\r\n    metrics = document_sentiment_metrics_fn(list_hyp_train, list_label)\r\n    print(\"(Epoch {}) TRAIN LOSS:{:.4f} {} LR:{:.8f}\".format((epoch+1),\r\n        total_train_loss\/(i+1), metrics_to_string(metrics), get_lr(optimizer)))\r\n\r\n    # Evaluate di validation set\r\n    model.eval()\r\n    torch.set_grad_enabled(False)\r\n    \r\n    total_loss, total_correct, total_labels = 0, 0, 0\r\n    list_hyp, list_label = [], []\r\n\r\n    pbar = tqdm(valid_loader, leave=True, total=len(valid_loader))\r\n    for i, batch_data in enumerate(pbar):\r\n        batch_seq = batch_data[-1]        \r\n        loss, batch_hyp, batch_label = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\r\n        \r\n        # Hitung total loss\r\n        valid_loss = loss.item()\r\n        total_loss = total_loss + valid_loss\r\n\r\n        # Hitung skor evaluation metrics\r\n        list_hyp += batch_hyp\r\n        list_label += batch_label\r\n        metrics = document_sentiment_metrics_fn(list_hyp, list_label)\r\n\r\n        pbar.set_description(\"VALID LOSS:{:.4f} {}\".format(total_loss\/(i+1), metrics_to_string(metrics)))\r\n        \r\n    metrics = document_sentiment_metrics_fn(list_hyp, list_label)\r\n    print(\"(Epoch {}) VALID LOSS:{:.4f} {}\".format((epoch+1),\r\n        total_loss\/(i+1), metrics_to_string(metrics)))\r\n    ","4fd62957":"# Hasil Validasi\r\nval_df = pd.read_csv(valid_dataset_path)\r\nval_df['pred'] = list_hyp\r\nval_df.head()\r\nval_df.to_csv('val result v2.csv', index=False)","6499501b":"# Prediksi test set\r\nmodel.eval()\r\ntorch.set_grad_enabled(False)\r\n\r\ntotal_loss, total_correct, total_labels = 0, 0, 0\r\npred, list_label = [], []\r\n\r\npbar = tqdm(test_loader, leave=True, total=len(test_loader))\r\nfor i, batch_data in enumerate(pbar):\r\n    _, batch_hyp, _ = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\r\n    pred += batch_hyp\r\n\r\n# Simpan prediksi\r\ndf = pd.read_csv('sample_submission.csv')\r\ndf['category'] = pred\r\ndf.to_csv('sub v2.csv', index=False)\r\n\r\nprint(df)","dd0ef773":"val_1 = pd.read_csv('val result original.csv')\r\nval_2 = pd.read_csv('val result v1.csv')\r\nval_3 = pd.read_csv('val result v2.csv')\r\n\r\nsub_1 = pd.read_csv('sub original.csv')\r\nsub_2 = pd.read_csv('sub v1.csv')\r\nsub_3 = pd.read_csv('sub v2.csv')","12b462a9":"# Cek apa label sama semua\r\nany(val_1['category'] != val_2['category'])\r\n# any(val_1['category'] != val_3['category'])\r\n# any(val_2['category'] != val_3['category'])","d07d8f3e":"# Gabungkan data\r\nval_comb = pd.concat([val_1['pred'], val_2['pred'],val_3['pred']], axis=1)\r\nval_comb.columns = ['pred1','pred2', 'pred3']\r\n\r\nsub_comb = pd.concat([sub_1['category'], sub_2['category'], sub_3['category']], axis=1)\r\nsub_comb.columns = ['pred1','pred2', 'pred3']\r\n\r\nsub_comb","524ea61c":"y_val = val_1['category']","cdd92c1d":"blend = XGBClassifier(objective='binary:logistic', eval_metric='error', seed=27,\r\n                      use_label_encoder=False)\r\nblend.fit(val_comb, y_val)","0a9e12e9":"cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=27)\r\n\r\nacc = cross_val_score(blend, val_comb, y_val, scoring='accuracy', cv=cv, n_jobs=-1)\r\nprint('Accuracy: %.5f (%.5f)' % (np.mean(acc), np.std(acc)))\r\n\r\nf1 = cross_val_score(blend, val_comb, y_val, scoring='f1_macro', cv=cv, n_jobs=-1)\r\nprint('F1 macro: %.5f (%.5f)' % (np.mean(f1), np.std(f1)))","a3877561":"cv = KFold(n_splits=5, random_state=27, shuffle=True)\r\nval_pred = cross_val_predict(blend, val_comb, y_val, cv=cv)\r\nconf_mat = confusion_matrix(y_val, val_pred)\r\nplt.figure(figsize=(10,6))\r\nsns.heatmap(conf_mat, annot=True,annot_kws={\"size\": 24})","0aa1a5d0":"blend_pred = model.predict(sub_comb)\r\npd.Series(blend_pred).value_counts()","552f0bd2":"submission = pd.read_csv('sample_submission.csv')\r\nsubmission['category'] = blend_pred\r\nsubmission.to_csv('submission ELSIX.csv', index=False)","cea4ea0c":"Untuk bisa lebih memahami distribusi dari panjang ulasan yang terdapat di dalam kedua dataset, dilakukan plotting dalam bentuk histogram.","d1f1e48b":"### Download Hasil Prediksi Validation Set","14be962e":"### Load Model","c7669111":"### Uji coba pre-trained model","18ca3ed9":"### Finetuning IndoBERT","ee9097f8":"<font size=3>Tahap ini akan meliputi dua versi pemrosesan data, yaitu preprocessing v1 dan preprocessing v2. Namun, pertama data train akan displit menjadi train_set dan val_set dengan rasio 0.8\/0.2 dan test_set. Ketiga dataframe tersebut akan melalui preprocessing v1 dan v2. <\/font>","865dbb9f":"<font size=3>Dari 46 emoji unik pada data test, hanya 41 emoji yang muncul dalam data train.<\/font>","65f762bd":"## Finetuning IndoBERT Dataset Original\r\n","aa0049bd":"### Blending","0331f35a":"<font size=3>Pada bagian ini, akan dibuat sebuah korpus untuk masing-masing ulasan pada train dan test.<\/font>","cb403bb0":"### B. Test Data ","b5734d22":"### Prediksi Test Set","2fa993ab":"### Persiapan Dataset","9ec48e48":"### Sample Ulasan Dengan Emoji","86a6afad":"### Persiapan Dataset","bb51bad9":"## Ensemble Model (Blending)","027ad59d":"### Load Model","aeebd135":"### Proporsi Variabel Target","63b13446":"### Ekstraksi Emoji Set (List of unique emojis)","a38e90dd":"### Persiapan Dataset","cceb1759":"### A. Train Data","1ce5fb54":"## Finetuning IndoBERT Dataset Preprocessing V1\r\n","4cf68ab0":"### Persiapan Dataset","72608ba6":"## Ekplorasi Emoji Dalam Ulasan","d70e1183":"<font size=3> Plot hasil penemuan:","8e7dbbad":"## Cross Validation","24fda3d4":"## Panjang Ulasan","59c6c378":"Di sini terlihat bahwa emoji yang telah dikonversi memiliki pattern sebagai berikut: <font size=\"3\">:[emoji_name]:<\/font>","a0a60f41":"# Validation","1c492587":"## Prediksi Test Set Final","1007e4b7":"### Uji coba pre-trained model","d98bf9d0":"### Uji coba pre-trained model","c714df6a":"## Overview","3d44fded":"### Prediksi Test Set","616c096d":"Setelah menemukan id ulasan yang bermasalah, selanjutnya akan ditampilkan ulasan-ulasan tersebut.","0d6e8a2c":"### Download Hasil Prediksi Validation Set","02a16855":"With this method we, team Elsix succeeded in achieving 0.9 F1 Macro Score. A little  bit of tips, if you want to score higher, don't do the stacking method and skip all the preprocessing step :) <br>\nMiraculously, the model will perform even better.","f7334cad":"## Preprocessing V2","64f8cd5a":"### Prediksi Test Set","97fce029":"## Ulasan non-alphanumeric","5e538008":"# Exploratory Data Analysis","9fdbb1cd":"### Visualize Corpus","dd1b8b5c":"### Finetuning IndoBERT","31cbfe07":"# Modelling\r\nFinetuning dilakukan 3x, yaitu pada dataset original, dataset preprocessing V1, dataset preprocessing V2","011acb40":"### Finetuning IndoBERT","f0b820cb":"### Load Model","317df4b5":"### Download Hasil Prediksi Validation Set","43223003":"<font size=3> Kesimpulan:\n- 3623 ulasan pada data test tidak memiliki emoji\n- 91 ulasan pada data test memiliki emoji","423f665b":"# Data Preprocessing","566209a4":"<font size=\"3\">Pada bagian ini, akan dilihat sekilas komposisi dari data ulasan dengan melakukan men-sample secara random baris-baris pada dataset train.<\/font>","01f2054c":"### Melihat ulasan dengan emoji","532f8e24":"Setelah didapatkan panjang dari tiap ulasan untuk tiap id, maka dapat dilihat karakteristik panjang ulasan untuk kedua dataset yang akan membantu untuk EDA serta pertimbangan konstruksi model.","7a4af0ee":"<font size=\"3\">Berikutnya, akan dicek proporsi ulasan positif dan negatif pada data train.<\/font>","e0d2cb36":"Akan dicari id ulasan yang tidak bersifat alphanumeric.\r\n\r\n","e8a6e9dd":"## Preprocessing V1","b0b92e1f":"<font size=3> Kesimpulan:\n- 14438 ulasan pada data train tidak memiliki emoji\n- 418 ulasan pada data train memiliki emoji","bd93f022":"### Proporsi Ulasan Dengan Emoji & Distribusi Kelas Emoji Pada Train","08e5c09c":"## Corpus","d6498af5":"## Finetuning IndoBERT Dataset Preprocessing V2\r\n","6456258f":"Cari panjang ulasan dari \"train.csv\" dan \"test.csv\" dengan menggunakan tokenization yang akan dilakukan di bawah ini.","bfddc43f":"Dapat digunakan library regex untuk mengidentifikasi pattern tersebut."}}