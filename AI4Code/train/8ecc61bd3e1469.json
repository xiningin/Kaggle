{"cell_type":{"3bc6d26d":"code","2c01f309":"code","d721884e":"code","cc93ad9f":"code","eb17fbc4":"code","5454e021":"code","a061a89f":"code","e2564ffb":"code","177365b5":"code","d72178e5":"code","1433216f":"code","655a1b58":"code","5d072b32":"code","6b699634":"code","3ada51b5":"code","876c1818":"code","2b314713":"code","96edc23e":"code","9c9056b7":"code","c5e11a93":"code","5b919273":"code","1175de1a":"code","0b9a924c":"code","84760451":"code","ef5db240":"code","a86fad65":"code","98dc87b3":"code","aa230729":"code","a4db1e1e":"code","3478f7c0":"code","1d230eda":"code","1a44351f":"code","b59d919a":"code","a9912f7f":"code","efc78a44":"code","bbc15b7f":"code","50eeaa91":"code","daa375e5":"code","44756321":"code","a6561d36":"code","263cff1c":"code","c9a606fa":"code","3ebfa55c":"code","556c75b2":"code","2c2fc40b":"code","f5a096cc":"code","6b4eb1b6":"code","92646471":"code","8cebabce":"code","0bb65e5c":"markdown","96d7454c":"markdown","c3fd0baf":"markdown","3f46163b":"markdown","137129da":"markdown","b20bde79":"markdown","ee1c16af":"markdown","3e6678cd":"markdown","23061b84":"markdown","efb92b03":"markdown","730a3204":"markdown","dba704b8":"markdown","7c3d0b0f":"markdown","abf45ad7":"markdown","6b5f27d7":"markdown","7b798449":"markdown","3aae4a61":"markdown"},"source":{"3bc6d26d":"#Importing required packages.\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score, classification_report\n","2c01f309":"#Loading dataset\nwine = pd.read_csv('..\/input\/winequality-red.csv')","d721884e":"#Let's check how the data is distributed\nwine.head()","cc93ad9f":"#Information about the data columns\nwine.info()","eb17fbc4":"#Here we see that fixed acidity does not give any specification to classify the quality.\nfig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'fixed acidity', data = wine)","5454e021":"#Here we see that its quite a downing trend in the volatile acidity as we go higher the quality \nfig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'volatile acidity', data = wine)","a061a89f":"#Composition of citric acid go higher as we go higher in the quality of the wine\nfig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'citric acid', data = wine)","e2564ffb":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'residual sugar', data = wine)","177365b5":"#Composition of chloride also go down as we go higher in the quality of the wine\nfig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'chlorides', data = wine)","d72178e5":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'free sulfur dioxide', data = wine)","1433216f":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'total sulfur dioxide', data = wine)","655a1b58":"#Sulphates level goes higher with the quality of wine\nfig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'sulphates', data = wine)","5d072b32":"#Alcohol level also goes higher as te quality of wine increases\nfig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'alcohol', data = wine)","6b699634":"wine.quality.describe()","3ada51b5":"#Making binary classificaion for the response variable.\n#Dividing wine as good and bad by giving the limit for the quality\nbins = (2,6,8)\ngroup_names = ['bad', 'good']\nwine['quality'] = pd.cut(wine['quality'], bins = bins, labels = group_names)","876c1818":"#Now lets assign a labels to our quality variable\nlabel_quality = LabelEncoder()","2b314713":"#Bad becomes 0 and good becomes 1 \nwine['quality'] = label_quality.fit_transform(wine['quality'])","96edc23e":"wine['quality'].value_counts()","9c9056b7":"sns.countplot(wine['quality'])","c5e11a93":"#Now seperate the dataset as response variable and feature variabes\nX = wine.drop('quality', axis = 1)\ny = wine.quality","5b919273":"#Train and Test splitting of data \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","1175de1a":"#Applying Standard scaling to get optimized result\nsc = StandardScaler()","0b9a924c":"# X_train = sc.fit_transform(X_train)\n# X_test = sc.fit_transform(X_test)\n#commented on purpose to demonstrate something","84760451":"type(X_train)","ef5db240":"logreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\npred_logreg = logreg.predict(X_test)","a86fad65":"#Testing time\nprint(classification_report(y_test,pred_logreg))","98dc87b3":"#Confusion matrix for the random forest classification\nprint(confusion_matrix(y_test, pred_logreg))","aa230729":"import eli5\neli5.show_weights(logreg)\n#uses permutation importance to compute feature weights","a4db1e1e":"feat_names=wine.columns[:-1].tolist()","3478f7c0":"feat_names","1d230eda":"eli5.show_weights(logreg, feature_names=feat_names)","1a44351f":"import numpy as np\ni = np.random.randint(1,100)","b59d919a":"i","a9912f7f":"X_test.iloc[i]","efc78a44":"y_test.iloc[i]","bbc15b7f":"eli5.show_prediction(logreg, \n                     X_test.iloc[i],\n                     feature_names=feat_names, show_feature_values=True)","50eeaa91":"rfc=RandomForestClassifier(n_estimators=200)\nrfm = rfc.fit(X_train,y_train)\npred_rfc= rfc.predict(X_test)","daa375e5":"#Testing time\nprint(classification_report(y_test,pred_rfc))","44756321":"print(confusion_matrix(y_test, pred_rfc))","a6561d36":"#looking at eli5 first\neli5.show_weights(rfm, \n                  feature_names=feat_names)","263cff1c":"from lime.lime_tabular import LimeTabularExplainer\n","c9a606fa":"X_train.head()","3ebfa55c":"explainer = LimeTabularExplainer(X_train.values,\n                                 mode=\"classification\",\n                                 feature_names=X_train.columns.tolist(),\n                                 categorical_names=None,\n                                 categorical_features=None,\n                                 discretize_continuous=True,\n                                 random_state=42)","556c75b2":"prob=lambda x:rfm.predict_proba(X_test[[i]]).astype(float)\n","2c2fc40b":"X_test.iloc[i]","f5a096cc":"#prediction function\npred_fn = lambda x: rfm.predict_proba(x).astype(float)","6b4eb1b6":"explanation = explainer.explain_instance(X_test.iloc[i], pred_fn)\nexplanation.show_in_notebook(show_table=True, show_all=False,)\nprint(explanation.score)","92646471":"!pip install https:\/\/github.com\/adebayoj\/fairml\/archive\/master.zip\n# Installing another package called fairML","8cebabce":"from fairml import audit_model\nimportances, _ = audit_model(rfm.predict, X_test)\nprint(importances)","0bb65e5c":"That gives us the weights associated to each feature, that can be seen as the contribution of each feature into predicting that the class will be y=1 (the client will subscribe after the campaign).\n\nThe names for each features aren't really helping though, we can pass a list of column names to eli5 but we'll need to do a little gymnastics first to extract names from our preprocessor in the pipeline (since we've generated new features on the fly with the one hot encoder)","96d7454c":"## **Let's do some plotting to know how the data columns are distributed in the dataset**","c3fd0baf":"So this is for the whole model.Let's nitpick a particular observation","3f46163b":"# Hi I'm LIME,I'm the infamous gossip monger. If I dont know you,I'll take your neighbour's word for it!! \n[Kriti is awesome though! Be like KD] <3","137129da":"## Simple Logistic Regression","b20bde79":"## LIME :Local Interpretable Model-Agnostic Explanations\nLime works on the principle of local fidelity ie that a point behaves in the same manner as that of its immediate neighbors. Fort his purpose,Lime is lighter than ELI5 and often faster.","ee1c16af":"A decent accuracy!","3e6678cd":"# Explain to me like I'm Five","23061b84":"> The basic idea behind FairML is to measure a model\u2019s dependence on its inputs by changing them. If a small change to an input feature dramatically changes the output, the model is sensitive to the feature.\n> Think sensitivty analysis that we studied in economics\/Calculus in schools.\nRemember orthogonal vectors?\n\n[Source](https:\/\/blog.fastforwardlabs.com\/2017\/03\/09\/fairml-auditing-black-box-predictive-models.html)","efb92b03":"<h1> Explainable AI: Pydata New Delhi Oct'19: Kriti Doneria <\/h1>","730a3204":"# Moving on,let's talk about the concept of fairness.How do you define fair?","dba704b8":"ELI5 understands text processing utilities from scikit-learn and can highlight text data accordingly. It also allows to debug scikit-learn pipelines which contain HashingVectorizer, by undoing hashing.\n\nXGBoost - show feature importances and explain predictions of XGBClassifier, XGBRegressor and xgboost.Booster.\n\nLightGBM - show feature importances and explain predictions of LGBMClassifier and LGBMRegressor.\n\nCatBoost - show feature importances of CatBoostClassifier and CatBoostRegressor.\n\nlightning - explain weights and predictions of lightning classifiers and regressors.\n\nsklearn-crfsuite. ELI5 allows to check weights of sklearn_crfsuite.CRF models.\n\nKeras - explain predictions of image classifiers via Grad-CAM visualizations.\n\nSource: https:\/\/eli5.readthedocs.io\/en\/latest\/overview.html\n    ","7c3d0b0f":"The explainer is all set up to explain observations!\nLet's use the old i","abf45ad7":"    ### Simple Logistic Regression","6b5f27d7":"##  Model Explainability is a broad concept of analyzing and understanding the results provided by ML models.","7b798449":"The parameters passed to the explainer are:\n\nTraining set sans one hot encoding\nmode: the explainer can be used for classification or regression\nfeature_names: list of labels for our features\ncategorical_features: list of indexes of categorical features\ncategorical_names: dict mapping each index of categorical feature to a list of corresponding labels\ndicretize_continuous: will discretize numerical values into buckets that can be used for explanation. For instance it can tell us that the decision was made because distance is in bucket [5km, 10km] instead of telling us distance is an importante feature.","3aae4a61":"This is simple.sign=direction of relationship and coefficients= weights"}}