{"cell_type":{"37be02b3":"code","617ae3e4":"code","80177172":"code","fb148745":"code","926c452e":"code","7cf9397d":"code","00c2d375":"code","7e3ab2e0":"code","e00c18e6":"code","e7bbb932":"code","f47a770a":"code","c00e585a":"code","537fc41e":"code","f82bbdf7":"code","a17b53fa":"code","b5d9f23a":"code","402924dc":"code","ef0efba2":"code","45d31613":"code","fcbe7882":"code","31e17816":"code","bda2ff3d":"code","d0bf6f23":"code","2046c7e5":"code","fb5cb06f":"code","8f8d26d4":"code","68e3c703":"code","3666d981":"code","3e13541b":"code","ce505e05":"code","3c4c9c6f":"code","ff0662a3":"code","43bf9b74":"code","b87f0bec":"code","96aa50ea":"code","a94b982a":"code","9d5f71a3":"code","bd634e14":"code","f930842e":"code","e66ae938":"code","77042775":"code","5599f238":"code","d9836740":"markdown","e8c0d650":"markdown","a9798c22":"markdown","e9e4a3ac":"markdown","49d82b4d":"markdown","fe0f2d30":"markdown","5984efa0":"markdown","73bff612":"markdown","9c17fb06":"markdown","e1a20592":"markdown","21f4c3fd":"markdown","3ceeecfd":"markdown","f4fea4e0":"markdown","b046e9df":"markdown","252397f3":"markdown","a3abbe8b":"markdown","b531f827":"markdown","2dfebe1d":"markdown","95f66737":"markdown","36effd93":"markdown","75376cc8":"markdown","11629550":"markdown","48a3dfa6":"markdown","36f55b63":"markdown","6515caae":"markdown","89d55d9a":"markdown","f64dd8d2":"markdown","eca72ef8":"markdown","303a4308":"markdown","f1bb014f":"markdown","55058239":"markdown"},"source":{"37be02b3":"\nimport pandas as pd\nimport numpy as np\nimport pprint as pp\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nimport statsmodels.api as sm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","617ae3e4":"loan = pd.read_csv(\"..\/input\/loan_predict_train.csv\")","80177172":"loan.shape","fb148745":"loan.info()","926c452e":"loan.describe()","7cf9397d":"loan.isnull().sum()","00c2d375":"loan.Loan_Status.value_counts()","7e3ab2e0":"fig, ax = plt.subplots(figsize=(8, 8))\nplt.style.use('seaborn-whitegrid')\nsns.set(palette=\"muted\")\nax.set_ylim(0,500)\nax = sns.boxplot(x=\"Loan_Status\", y=\"LoanAmount\", data=loan)","e00c18e6":"fig, ax = plt.subplots(figsize=(8, 10))\nax.set_ylim(0,20000)\nax = sns.boxplot(x=\"Loan_Status\", y=\"ApplicantIncome\", data=loan)","e7bbb932":"fig, ax = plt.subplots(figsize=(8, 10))\nax.set_ylim(0,20000)\nax = sns.boxplot(x=\"Loan_Status\", y=\"CoapplicantIncome\", data=loan)","f47a770a":"plt.style.use('seaborn-whitegrid')\nfig = plt.figure(figsize=(15,20))\nfig.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.5, hspace=0.3)\nrows = 3\ncols = 3\ncategorical_col = ['Gender', 'Married', 'Education', 'Property_Area', 'Dependents', 'Self_Employed']\nfor i, column in enumerate(categorical_col):\n    ax = fig.add_subplot(rows, cols, i + 1)\n    ax.set(xticks=[])\n    ax = pd.crosstab(loan[categorical_col[i]], loan.Loan_Status, normalize='index').plot.bar(ax=ax)","c00e585a":"fig = plt.figure(figsize=(10,10))\nax = fig.add_subplot(111)\npd.crosstab([loan.Married, loan.Gender, loan.Dependents], loan.Loan_Status, normalize='index').plot.barh(ax=ax);","537fc41e":"loan.loc[(loan.Gender == 'Female') & (loan.Married == 'No') & (loan.Dependents == '3+')]","f82bbdf7":"def do_preprocess(data):\n    table = data.pivot_table(values='LoanAmount', index='Self_Employed' ,columns='Education', aggfunc=np.median)\n    def fage(x):\n        return table.loc[x['Self_Employed'],x['Education']]\n    # Replace missing values\n    data['Self_Employed'].fillna('No', inplace=True)\n    data['LoanAmount'].fillna(data[data['LoanAmount'].isnull()].apply(fage, axis=1), inplace=True)\n    data['Gender'].fillna(data['Gender'].mode()[0], inplace=True)\n    data['Married'].fillna(data['Married'].mode()[0], inplace=True)\n    data['Dependents'].fillna(data['Dependents'].mode()[0], inplace=True)\n    data['Loan_Amount_Term'].fillna(data['Loan_Amount_Term'].mode()[0], inplace=True)\n    data['Credit_History'].fillna(data['Credit_History'].mode()[0], inplace=True)\n\n\ndo_preprocess(loan)","a17b53fa":"def add_new_features(data):\n    data['TotalIncome'] = data['ApplicantIncome'] + data['CoapplicantIncome']\n    data['IncomeByLoanAmount'] = data['TotalIncome'] \/ data['LoanAmount']\n    data['AplIncomeByLoanAmount'] = data['ApplicantIncome'] \/ data['LoanAmount']\n   \nadd_new_features(loan)","b5d9f23a":"def doOneHotEncoding(data, cols):\n    for var in cols:\n        one_hot = pd.get_dummies(data[var], prefix = var)\n        # Drop column B as it is now encoded\n        data = data.drop(var,axis = 1)\n        # Join the encoded data\n        data = data.join(one_hot)\n    return data\n\nloan = doOneHotEncoding(loan, ['Gender', 'Married','Dependents','Education','Self_Employed','Property_Area'])\nloan.Loan_Status = loan.Loan_Status.map(dict(Y=1,N=0))","402924dc":"loan.Loan_Status.value_counts()","ef0efba2":"outcome_var = \"Loan_Status\"\n#exclude baseline categorical variables\npredictor_var = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount',\n       'Loan_Amount_Term', 'Credit_History', #'TotalIncome',\n       'IncomeByLoanAmount', 'AplIncomeByLoanAmount', #'EMI',\n       'Gender_Male', 'Married_No', 'Dependents_0', 'Dependents_1', 'Dependents_3+',\n       'Education_Not Graduate', 'Self_Employed_Yes',\n       'Property_Area_Semiurban', 'Property_Area_Urban']\n\nlogit = sm.Logit(loan[outcome_var], loan[predictor_var])\nresult = logit.fit_regularized()","45d31613":"result.summary2()","fcbe7882":"pt = result.pred_table()\npt","31e17816":"print (\"Accuracy : %s\" % \"{0:.3%}\".format((pt[0,0]+pt[1,1])\/pt.sum()))","bda2ff3d":"predictor_var = ['Loan_Amount_Term', 'Credit_History', 'Property_Area_Semiurban',\n                 'Married_No', 'Dependents_1', 'Education_Not Graduate']","d0bf6f23":"logit = sm.Logit(loan[outcome_var], loan[predictor_var])\nresult = logit.fit_regularized()","2046c7e5":"result.summary2()","fb5cb06f":"pt = result.pred_table()\npt","8f8d26d4":"print (\"Accuracy : %s\" % \"{0:.3%}\".format((pt[0,0]+pt[1,1])\/pt.sum()))","68e3c703":"from sklearn.linear_model import LogisticRegression\nmodel_logistic = LogisticRegression(random_state=42)","3666d981":"def classify_and_report_metrics(model, data, predictors, outcome):\n    X_train, X_test, y_train, y_test = train_test_split(data[predictors], data[outcome], random_state=42, stratify=loan[outcome_var])\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    print('Accuracy on test set: %s' % '{0:.3%}'.format(model.score(X_test, y_test)))\n    cm = confusion_matrix(y_test, y_pred)\n    print(cm)\n    print(classification_report(y_test, y_pred))","3e13541b":"##adding all non-base levels of categorical variables that were found significant\npredictor_var += ['Dependents_0', 'Dependents_3+', 'Property_Area_Urban']\npredictor_var_logistic = predictor_var","ce505e05":"classify_and_report_metrics(model_logistic, loan, predictor_var, outcome_var)","3c4c9c6f":"def fit_and_validate(model, data, predictors, outcome):\n    #Fit the model:\n    model.fit(data[predictors],data[outcome])\n    #Make predictions on training set:\n    predictions = model.predict(data[predictors])\n    #Print accuracy\n    accuracy = metrics.accuracy_score(predictions,data[outcome])\n    print (\"Accuracy : %s\" % \"{0:.3%}\".format(accuracy))\n\n    cm = confusion_matrix(data[outcome], predictions)\n    print(cm)\n    print(classification_report(data[outcome], predictions))    \n    \n    #Perform k-fold cross-validation with 5 folds\n    kf = KFold(n_splits=5)\n    error = []\n    for train, test in kf.split(data):\n        # Filter training data\n        train_predictors = (data[predictors].iloc[train,:])\n\n        # The target we're using to train the algorithm.\n        train_target = data[outcome].iloc[train]\n\n        # Training the algorithm using the predictors and target.\n        model.fit(train_predictors, train_target)\n\n        #Record error from each cross-validation run\n        error.append(model.score(data[predictors].iloc[test,:], data[outcome].iloc[test]))\n    print(\"Cross-Validation Score : %s\" % \"{0:.3%}\".format(np.mean(error)))\n    if (isinstance(model, (RandomForestClassifier))):\n            #Create a series with feature importances:\n            featimp = pd.Series(model.feature_importances_, index=predictors).sort_values(ascending=False)\n            print (featimp)\n            if (isinstance(model, RandomForestClassifier)):\n                if model.get_params()['oob_score'] == True:\n                    print('OOB Score %f' % (1 - model.oob_score_))\n                else:\n                    print('OOB Score False')","ff0662a3":"fit_and_validate(model_logistic, loan, predictor_var, outcome_var)","43bf9b74":"predictor_var = [\n 'Credit_History', 'IncomeByLoanAmount', 'AplIncomeByLoanAmount',\n    'LoanAmount', 'Loan_Amount_Term',\n    'Property_Area_Semiurban', 'ApplicantIncome','Married_No','Dependents_1',\n    'Education_Not Graduate'\n]","b87f0bec":"model_rf = RandomForestClassifier(random_state=42, n_estimators=200, bootstrap= True, oob_score=True)","96aa50ea":"classify_and_report_metrics(model_rf, loan, predictor_var, outcome_var)","a94b982a":"fit_and_validate(model_rf, loan, predictor_var, outcome_var)","9d5f71a3":"predictor_var = [\n 'Credit_History', 'IncomeByLoanAmount', 'AplIncomeByLoanAmount',\n    'LoanAmount','Property_Area_Semiurban', 'ApplicantIncome'\n]\npredictor_var_rf = predictor_var","bd634e14":"import pprint as pp\nfrom sklearn.model_selection import RandomizedSearchCV\n# Number of trees in random forest\n# n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 400, num = 2)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\n# max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 5)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\npp.pprint(random_grid)\n# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = RandomForestClassifier()\n# Random search of parameters, using 3 fold cross validation,\n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(loan[predictor_var], loan[outcome_var])\nrf_random.best_params_","f930842e":"model_rf = RandomForestClassifier(random_state=42, n_estimators = rf_random.best_params_['n_estimators'], \n                                  min_samples_split = rf_random.best_params_['min_samples_split'], \n                                  min_samples_leaf = rf_random.best_params_['min_samples_leaf'], \n                                  max_features = rf_random.best_params_['max_features'], \n                                  max_depth = rf_random.best_params_['max_depth'],\n                                  bootstrap = rf_random.best_params_['bootstrap'],\n                                  oob_score = rf_random.best_params_['bootstrap'])","e66ae938":"fit_and_validate(model_rf, loan, predictor_var, outcome_var)","77042775":"print(rf_random.best_params_['n_estimators']), print(rf_random.best_params_['max_features']), \nprint(rf_random.best_params_['max_depth']), print(rf_random.best_params_['min_samples_split']),\nprint(rf_random.best_params_['min_samples_leaf']), print(rf_random.best_params_['bootstrap'])","5599f238":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\n# logit_roc_auc = roc_auc_score(y_test, logistic_model.predict(X_test))\n# fpr, tpr, thresholds = roc_curve(y_test, logistic_model.predict_proba(X_test)[:,1])\nX_train, X_test, y_train, y_test = train_test_split(loan[predictor_var_logistic], loan[outcome_var], random_state=42, stratify=loan[outcome_var])\nmodel_logistic.fit(X_train, y_train)\nlogit_roc_auc = roc_auc_score(y_test, model_logistic.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, model_logistic.predict_proba(X_test)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\n\nX_train, X_test, y_train, y_test = train_test_split(loan[predictor_var_rf], loan[outcome_var], random_state=42, stratify=loan[outcome_var])\nmodel_rf.fit(X_train, y_train)\nlogit_roc_auc = roc_auc_score(y_test, model_rf.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, model_rf.predict_proba(X_test)[:,1])\n# plt.figure()\nplt.plot(fpr, tpr, label='Random Forest (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()","d9836740":"- Dropping insignificant variables slightly improved the accuracy.","e8c0d650":"### Model Evaluation\n#### A single fit is not sufficient and we need to Evaluate the model to ensure that it generalizes well. We need scikit learn api for cross validation.\n\n#### In classification, Accuracy does not tell the whole story particularly in unbalanced datasets - we need to look at the precision and recall of the classifier on each class to understand the how well the classfier is doing its job.","a9798c22":"#### We can see that Random Forest gives the best accuracy on this dataset.","e9e4a3ac":"#### Let's find the best hyperparameters for the RandomForestClassifier","49d82b4d":"### How is approval rate among various categories ?","fe0f2d30":"- ###### We can see that SemiUrban properties have relatively higher rate of approval","5984efa0":"#### Cross validation score is slightly lower than Logistic Regression. Let's use the top 6 important features and fit again","73bff612":"##### Now perform a Five fold cross validation","9c17fb06":"### Feature Engineering\n- Performance of a model will be greatly influenced by carefully crafted features.\n- It is reasonable to assume a loan application is weighed by the ability to repay it. This ability will depend on the applicant's income. Hence we can add a feature of the ratio of there two terms.\n- We can come up with the below features\n    - ApplicantIncomeByLoanAmount\n    - IncomeByLoanAmount - here Income is the total income of applicant and co-applicant","e1a20592":"- Perform 5-fold cross validation","21f4c3fd":"Let's fit Logistic Regression classfier from scikit-learn","3ceeecfd":"- Fit the model using the tuned parameters","f4fea4e0":"- ###### We can see from above plot that mean coapplicant income is higher for approved loans","b046e9df":"## Data Preprocessing","252397f3":"##### That's it ! We have reached the end of this kernel. Thank's for your time !\nIf you find this kernel useful please UP VOTE ! If anything can be improved please share your valuable feedback !","a3abbe8b":"- It can be noted from above plot that Unmarried Females with 3+ dependents have very high rate of rejection - let's dive into the details - from the data it doesn't show any gender bias as there are only three samples out of which only one has good credit history","b531f827":"### Encoding Predictors\n- As the machine learning algorithms internally operate on matrices of numerical data we need to encode our categorical variables into numberic form.\n- The most widely used encoding is the OneHotEncoding and pandas readily provides this functionality","2dfebe1d":"## Exploratory Analysis\n#### Is there any trend in loan status based on LoanAmount or Income of applicant\/coapplicant ?","95f66737":"### Interpretation\n#### - Having good credit history changes the log odds of loan approval by 3.4846\n#### - For every unit change in Loan Amount Term the log odds of loan approval decreases by 0.0057 units\n#### - Buying a property located in SemiUrban area as compared to a Rural area changes the log odds of loan approval by 0.7526 units","36effd93":"## Building Model\n#### We will begin with building a linear model because of its interpretability. Will use statsmodels api to fit the data as it gives detailed statistics on the fitted model. Let's do Logistic Regression on this data.","75376cc8":"### Let's see if we can improve the accuracy. We will use Random Forest classifier as it,\n\n#### - Works well for classification\n#### - Robust to outliers in the data\n#### - Relatively easy to tune\n#### - Provides out of bag error rate indicative of the unbiased nature of the fit\n\n#### For fitting a RandomForest model, we will include,\n- Variables identified as significant by Linear Model\n- Two additonal Features created earlier\n- ApplicantIncome and LoanAmount which has many outliers","11629550":"#### Target Variable is Loan_Status - let's see its distribution","48a3dfa6":"### Is there any gender bias?","36f55b63":"##### Split the data into train and test - fit the model on train dataset and test on the test set","6515caae":"- Split the data, train the model and verify on the test set","89d55d9a":"- Based on p-values we got 6 out of 16 variables as significant\n- Let's fit the model with only those variables","f64dd8d2":"### Missing data\n- Out of the 13 columns we will exclude Loan_Status and Loan_ID and arrive at 11 predictor variblles. Out of these seven variables have varying degrees of missing values. We need to impute these appropriately.","eca72ef8":"### Conclusion","303a4308":"# Loan Prediction\n### In this notebook I will analyze the data set from Loan prediction III problem from Analytics Vidhya and build models to predict the loan status\n\n\n#### About Dataset\nDream Housing Finance company deals in all home loans. They have presence across all urban, semi urban and rural areas. Customer first apply for home loan after that company validates the customer eligibility for loan.\n\n\n#### Problem\nCompany wants to automate the loan eligibility process (real time) based on customer detail provided while filling online application form. These details are Gender, Marital Status, Education, Number of Dependents, Income, Loan Amount, Credit History and others. To automate this process, they have given a problem to identify the customers segments, those are eligible for loan amount so that they can specifically target these customers. Here they have provided a partial data set\n\n\n\n## Load the data and look at summary statistics","f1bb014f":"#### Create the model with tuned parameters","55058239":"#### The data is moderately unbalanced - minority class is about 31%."}}