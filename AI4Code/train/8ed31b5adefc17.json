{"cell_type":{"a4b3c575":"code","da283aa8":"code","91cb7d38":"code","aa162673":"code","c8f363f7":"code","cb71c8db":"code","f762fb83":"code","82d4483a":"code","961cc42a":"code","534a92f2":"code","b2160896":"code","ca25a0ba":"code","83adb82e":"code","a7d95035":"code","081bc568":"code","593bcc51":"code","2daa3ef0":"code","02c02fa9":"code","99ef23ce":"code","296f0e94":"code","baf924f4":"code","07577a1d":"code","230a623b":"code","7bf09b01":"code","5160126a":"code","b223a79d":"code","c92a8484":"code","59fd8c4c":"code","10376204":"code","a457355b":"code","8cab2971":"code","317acd5c":"code","6311fa8d":"code","dc9fe9a4":"code","ca1466d6":"code","e1c68c9b":"code","f7808798":"code","eb9d1019":"code","4ff93f35":"code","9bdca926":"code","e36fccb3":"code","f8e34743":"code","be73aeea":"code","91e4c1bd":"code","c6e5f1c9":"code","33ec49b3":"code","881f06da":"code","fe97a5db":"code","317d58b1":"code","4fbf2c2d":"markdown","f74758ee":"markdown","fccee8ca":"markdown","1d41d377":"markdown","e5d93bf4":"markdown","ee40bca0":"markdown","727d8fe7":"markdown","e53ec6d1":"markdown","0cc95512":"markdown","fadae7a6":"markdown","8257ae63":"markdown","f1055101":"markdown","e19256c5":"markdown","3692282a":"markdown","8ad5caa1":"markdown","3faccb91":"markdown","fed5776e":"markdown","6af4ad5c":"markdown","69981cb8":"markdown","d3e291e5":"markdown","48734e69":"markdown","f481a53e":"markdown","bd8e4e68":"markdown","bdfa32b8":"markdown","cd8f5455":"markdown","0aa5bd0a":"markdown","b6d4d70a":"markdown","c4dab125":"markdown","4a1f8396":"markdown","b5a1328b":"markdown","52fc9b83":"markdown","9b402cc5":"markdown","6321922b":"markdown","e5399082":"markdown","677df233":"markdown","afc7e74c":"markdown","f22bd7d9":"markdown","d0722072":"markdown","d7571be1":"markdown","243de97d":"markdown","5028ecd6":"markdown","67fd1d4d":"markdown","b095748a":"markdown","10614037":"markdown"},"source":{"a4b3c575":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn import metrics\nfrom sklearn import ensemble\nfrom sklearn import model_selection\nfrom sklearn import preprocessing\nfrom sklearn import svm\nfrom sklearn import neighbors\nfrom sklearn import naive_bayes\nfrom sklearn import linear_model\nimport matplotlib.pyplot as plt\n\nnp.random.seed(1234) #For reproducible results","da283aa8":"dataset = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\ny = dataset['Survived'].values\n\ndataset.head()","91cb7d38":"dataset.describe(include='all')","aa162673":"fig,ax = plt.subplots(2,1,figsize=(10,10)) \nax[0].set_title('dataset missing count(%)')\nax[1].set_title('test set missing count(%)')\n\nsns.barplot(x = dataset.columns, y = dataset.isna().sum()\/dataset.shape[0]*100, ax = ax[0])\nsns.barplot(x = test.columns, y = test.isna().sum()\/test.shape[0]*100, ax = ax[1])","c8f363f7":"dataset['Cabin'].fillna('NotKnown',inplace=True)\ndataset['Age'].fillna(dataset['Age'].mean(),inplace=True)\ndataset['Embarked'].fillna('S',inplace=True)\n\ntest['Cabin'] = test['Cabin'].fillna('NotKnown')\ntest['Age'] = test['Age'].fillna(test['Age'].mean())\n#We'll deal with Fare after.","cb71c8db":"dataset.describe(include='all')","f762fb83":"d= pd.DataFrame([dataset.loc[dataset.Sex=='male'].shape[0],dataset.loc[dataset.Sex=='female'].shape[0]],columns=['number'],index=['male','female'])\nsns.barplot(x=dataset['Sex'], y=dataset['Survived'])\nd","82d4483a":"age_survived_males = dataset.loc[dataset['Survived'] == 1].loc[dataset['Sex'] == 'male']['Age']\nage_deceased_males = dataset.loc[dataset['Survived'] == 0].loc[dataset['Sex'] == 'male']['Age']\n\nage_survived_females = dataset.loc[dataset['Survived'] == 1].loc[dataset['Sex'] == 'female']['Age']\nage_deceased_females = dataset.loc[dataset['Survived'] == 0].loc[dataset['Sex'] == 'female']['Age']\n\nfemale_age_dist = dataset.loc[dataset['Sex']=='female']['Age']\nmale_age_dist = dataset.loc[dataset['Sex']=='male']['Age']\n\nfig, ax = plt.subplots(3,2,figsize=(30,15))\nsns.distplot(age_survived_males, kde=True,rug=True, color='green', ax = ax[0][0])\nax[0][0].set_title('Survived Titanic Men')\nsns.distplot(age_deceased_males, kde=True,rug=True, color='red', ax = ax[0][1])\nax[0][1].set_title('Survived Titanic Women')\nsns.distplot(age_survived_females, kde=True,rug=True, color='grey', ax = ax[1][0])\nax[1][0].set_title('Deceased Titanic Men')\nsns.distplot(age_deceased_females, kde=True,rug=True, color='grey', ax = ax[1][1])\nax[1][1].set_title('Deceased Titanic Women')\nsns.distplot(male_age_dist, kde=True,rug=True, color='blue', ax = ax[2][0])\nax[2][0].set_title('Men Age Distribution')\nsns.distplot(female_age_dist, kde=True,rug=True, color='pink', ax = ax[2][1])\nax[2][1].set_title('Women Age Distribution')\n\nfor i in range(2):\n    for j in range(2):\n        ax[i][j].axvspan(0,18, facecolor='orange', label='adolescents([0,18])', alpha=0.1)\n        ax[i][j].axvspan(18,35, facecolor='cyan', label='young adults([19,35])', alpha=0.1)\n        ax[i][j].axvspan(35,60, facecolor='purple', label='adults([35,60])', alpha=0.1)\n        ax[i][j].axvspan(60,100, facecolor= 'brown', label='elders([60+])',alpha=0.1)\n        ax[i][j].legend()\n","961cc42a":"survived_class = dataset.loc[dataset['Survived']==1]['Pclass'] #Distribution of survived per class\ndeceased_class = dataset.loc[dataset['Survived']==0]['Pclass'] #Distribution of deceased per class\n\nfig, ax = plt.subplots(1,2,figsize=(20,5))\nsns.distplot(survived_class,ax = ax[0],label='Survived')\nsns.distplot(deceased_class,ax = ax[1],label='Decesed')\n","534a92f2":"people_per_class = dataset.groupby('Pclass').count()['Survived']\nsurv_per_class = dataset.loc[dataset.Survived == 1].groupby('Pclass').count()['Survived']\nratios = surv_per_class\/people_per_class\nfig, ax = plt.subplots(1,3,figsize=(30,7))\nax[0].set_title('# of passenger for the Class')\nsns.barplot(x = people_per_class.index, y = people_per_class, ax=ax[0])\nax[1].set_title('# of SURVIVED for the Class')\nsns.barplot(x = surv_per_class.index, y = surv_per_class, ax=ax[1])\nax[2].set_title('survived\/total per class')\nsns.barplot(x = ratios.index, y = ratios, ax=ax[2])","b2160896":"print(np.unique([s.split(',')[1].split(' ')[1] for s in dataset['Name']]))\nprint(np.unique([s.split(',')[1].split(' ')[1] for s in test['Name']]))\n\ndataset['Title'] = [s.split(',')[1].split(' ')[1] for s in dataset['Name']]\ntest['Title'] = [s.split(',')[1].split(' ')[1] for s in test['Name']]\ndataset.head()","ca25a0ba":"sns.barplot(data=dataset, x='Title', y='Survived')\nplt.xticks(rotation=45);","83adb82e":"dataset['Title'].replace(['Capt.'],'Board',inplace=True)\ndataset['Title'].replace(['Don.','Dona.','Lady.','Jonkheer.','Rev.','Sir.','the'],'Hon',inplace=True)\ndataset['Title'].replace(['Col.','Dr.','Major.','Master.'],'Professionals',inplace=True)\ndataset['Title'].replace(['Miss.','Mlle.','Mme.','Mr.','Mrs.','Ms.'],'NoHon', inplace=True)\n\ntest['Title'].replace(['Capt.'],'Board',inplace=True)\ntest['Title'].replace(['Don.','Dona.','Lady.','Jonkheer.','Rev.','Sir.','the'],'Hon',inplace=True)\ntest['Title'].replace(['Col.','Dr.','Major.','Master.'],'Professionals',inplace=True)\ntest['Title'].replace(['Miss.','Mlle.','Mme.','Mr.','Mrs.','Ms.'],'NoHon', inplace=True)","a7d95035":"sns.barplot(data=dataset, x='Title',y='Survived')","081bc568":"dataset['Parch-SibSp'] = dataset['Parch'] + dataset['SibSp']\ntest['Parch-SibSp'] = test['Parch'] + test['SibSp']\n\nsns.barplot(data=dataset, x = 'Parch-SibSp', y='Survived')\n\ndataset.drop(columns=['Parch','SibSp'],inplace = True)\ntest.drop(columns=['Parch','SibSp'],inplace=True)","593bcc51":"dataset['Parch-SibSp'] = pd.cut(x = dataset['Parch-SibSp'], bins=[0,1,4,7,20],right=False,labels=['Alone','Small','Medium','Big'])\ntest['Parch-SibSp'] = pd.cut(x = test['Parch-SibSp'], bins=[0,1,4,7,20],right=False,labels=['Alone','Small','Medium','Big'])","2daa3ef0":"sns.barplot(data=dataset,x='Parch-SibSp',y='Survived')","02c02fa9":"print(np.unique([x.split(' ')[0] if not(x.isdigit()) else 'OnlyNumber' for x in dataset['Ticket']]))\ndataset['Ticket'] = [x.split(' ')[0] if not(x.isdigit()) else 'OnlyNumber' for x in dataset['Ticket']]\ntest['Ticket'] = [x.split(' ')[0] if not(x.isdigit()) else 'OnlyNumber' for x in test['Ticket']]","99ef23ce":"sns.boxplot(data = dataset, x = 'Fare')","296f0e94":"\nprint('before(train) -> '+ str(len(dataset.loc[dataset.Fare == 0.0]['Fare'])))\nprint('before(test) -> '+ str(len(test.loc[test.Fare == 0.0]['Fare'])))\n\ndataset['Fare'].replace(0.0,np.nan,inplace=True)\ntest['Fare'].replace(0.0,np.nan,inplace=True)\n\n##Depending on the class we fill the Fare\nmeanFirst = dataset.loc[dataset.Pclass==1]['Fare'].mean()\nprint('First Class mean is ' + str(meanFirst))\nmeanSecond = dataset.loc[dataset.Pclass==2]['Fare'].mean()\nprint('Second Class mean is ' + str(meanSecond))\nmeanThird = dataset.loc[dataset.Pclass==3]['Fare'].mean()\nprint('Third Class mean is ' + str(meanThird))\nmeans=[meanFirst,meanSecond,meanThird]\n\n\nfcd = dataset.loc[dataset.Pclass == 1].index\nscd = dataset.loc[dataset.Pclass == 2].index\ntcd = dataset.loc[dataset.Pclass == 3].index\n\nfct = test.loc[test.Pclass == 1].index\nsct = test.loc[test.Pclass == 2].index\ntct = test.loc[test.Pclass == 3].index\n\ndataset.loc[fcd,'Fare'] = dataset.loc[fcd,'Fare'].fillna(means[0])\ndataset.loc[scd,'Fare'] = dataset.loc[scd,'Fare'].fillna(means[1])\ndataset.loc[tcd,'Fare'] = dataset.loc[tcd,'Fare'].fillna(means[2])\n\ntest.loc[fct,'Fare'] = test.loc[fct,'Fare'].fillna(means[0])\ntest.loc[sct,'Fare'] = test.loc[sct,'Fare'].fillna(means[1])\ntest.loc[tct,'Fare'] = test.loc[tct,'Fare'].fillna(means[2])\n\n# dataset['Fare'].fillna(dataset['Fare'].mean(),inplace=True)\n# test['Fare'].fillna(dataset['Fare'].mean(),inplace=True)\n\nprint('after(train) -> '+ str(len(dataset.loc[dataset.Fare == 0]['Fare'])))\nprint('after(test) -> '+ str(len(test.loc[dataset.Fare == 0]['Fare'])))\nprint('after(train) null -> '+ str(dataset['Fare'].count() -891))\nprint('after(test) null -> '+ str(test['Fare'].count() - 418))\n\nminFare = 0\nqntl25 = dataset['Fare'].quantile(0.25)\nqntl50 = dataset['Fare'].quantile(0.5)\nqntl75 = dataset['Fare'].quantile(0.75)\nqntplus = dataset['Fare'].quantile(0.75) + 1.5*(qntl75-qntl50)\nmaxFare = 1e6\n\ndataset['Fare'] = pd.cut(dataset['Fare'], bins= [minFare,qntl25, qntl75,qntplus,maxFare], labels=['Low','Medium','High','VeryHigh'])\ntest['Fare'] = pd.cut(test['Fare'], bins= [minFare,qntl25, qntl75,qntplus,maxFare], labels=['Low','Medium','High','VeryHigh'])\n\nsns.barplot(data=dataset, x='Fare', y='Survived')","baf924f4":"#Categorize\n\nencSex = preprocessing.OrdinalEncoder().fit(pd.DataFrame(dataset['Sex']))\nencTitle = preprocessing.OrdinalEncoder().fit(pd.DataFrame(dataset['Title']))\nencParchSibSp = preprocessing.OrdinalEncoder().fit(pd.DataFrame(dataset['Parch-SibSp']))\nencFare = preprocessing.OrdinalEncoder().fit(pd.DataFrame(dataset['Fare']))\nencEmbark = preprocessing.OneHotEncoder().fit(pd.DataFrame(dataset['Embarked']))\n\n\ndataset['Sex']= encSex.transform(pd.DataFrame(dataset['Sex']))\n\ndataset['Title']= encTitle.transform(pd.DataFrame(dataset['Title']))\n\ndataset['Parch-SibSp']= encParchSibSp.transform(pd.DataFrame(dataset['Parch-SibSp']))\n\ndataset['Fare']= encFare.transform(pd.DataFrame(dataset['Fare']))\n\ndataset = pd.concat([dataset,pd.DataFrame(encEmbark.transform(pd.DataFrame(dataset['Embarked'])).toarray(),columns=['Embark_C','Embark_Q','Embark_S'])], axis=1)\ndataset.drop(columns=['Embarked'],inplace=True)\n\ntest = pd.concat([test,pd.DataFrame(encEmbark.transform(pd.DataFrame(test['Embarked'])).toarray(),columns=['Embark_C','Embark_Q','Embark_S'])], axis=1)\ntest.drop(columns=['Embarked'],inplace=True)\n\ntest['Sex'] = encSex.transform(pd.DataFrame(test['Sex']))\n\ntest['Title'] = encTitle.transform(pd.DataFrame(test['Title']))\n\ntest['Parch-SibSp'] = encParchSibSp.transform(pd.DataFrame(test['Parch-SibSp']))\n\ntest['Fare']= encFare.transform(pd.DataFrame(test['Fare']))","07577a1d":"dataset.drop(columns=['Cabin','PassengerId','Name','Ticket','Survived'], inplace=True)\ntest.drop(columns=['Cabin','PassengerId','Name','Ticket'], inplace=True)","230a623b":"dataset.describe(include='all')","7bf09b01":"sns.barplot(x=dataset.columns, y=ensemble.RandomForestClassifier().fit(preprocessing.StandardScaler().fit_transform(dataset),y).feature_importances_)\nplt.xticks(rotation=45)\nplt.axhline(y=0.05,c='yellow')\nplt.axhline(y=0.1,c='orange')\nplt.axhline(y=0.15,c='red')\n\nsel_threshold = 0.00;","5160126a":"if sel_threshold >= 0.05 and sel_threshold < 0.1:\n    dataset.drop(columns=['Embark_S','Embark_Q','Embark_C','Title'],inplace=True)\n    test.drop(columns=['Embark_S','Embark_Q','Embark_C','Title'],inplace=True)\nif sel_threshold >= 0.1 and sel_threshold < 0.15:\n    dataset.drop(columns=['Embark_S','Embark_Q','Embark_C','Parch-SibSp','Fare','Title'],inplace=True)\n    test.drop(columns=['Embark_S','Embark_Q','Embark_C','Parch-SibSp','Fare','Title'],inplace=True)\nif sel_threshold == 0.15:\n    dataset.drop(columns=['Embark_S','Embark_Q','Embark_C','Parch-SibSp','Pclass','Title','Fare'],inplace=True)\n    test.drop(columns=['Embark_S','Embark_Q','Embark_C','Parch-SibSp','Pclass','Title','Fare'],inplace=True)","b223a79d":"sns.heatmap(dataset.corr(),cmap='Blues')","c92a8484":"dataset.head()","59fd8c4c":"test.head()","10376204":"scaler = preprocessing.StandardScaler()\nX = scaler.fit_transform(dataset)\ntest_X = scaler.transform(test)","a457355b":"cv_size = 5","8cab2971":"lr_model = linear_model.LogisticRegression()\nscores_lr = model_selection.cross_validate(lr_model,X,y,cv=cv_size, scoring=('accuracy','f1'))\nprint('Trained in ' + '{:.2f}'.format(scores_lr['fit_time'].mean()) + 's , achieved Accuracy of ' +  '{:.3f}'.format(scores_lr['test_accuracy'].mean()) + ' ('+ '{:.3f}'.format(scores_lr['test_accuracy'].std()) +')')","317acd5c":"lr_model = linear_model.LogisticRegression()","6311fa8d":"results_rf = []\nindexes = []\nfor j in range(100,300,100):\n    for i in range(2,10):\n        for k in range(2,10):\n            indexes.append('n'+str(j)+'d'+str(i)+'s'+str(k));\n            rf_model = ensemble.RandomForestClassifier(n_estimators=j,max_depth=i,min_samples_split=k, n_jobs=-1,oob_score = True)\n            #scores_rf = model_selection.cross_validate(rf_model, X, y,cv=cv_size, scoring=('accuracy','f1'))\n            scores_rf = rf_model.fit(X,y).oob_score_\n            #print('Trained in ' + '{:.2f}'.format(scores_rf['fit_time'].mean()) + 's , achieved Accuracy of ' +  '{:.3f}'.format(scores_rf['test_accuracy'].mean()) + ' ('+ '{:.3f}'.format(scores_rf['test_accuracy'].std()) +')')\n            #results_rf.append({'acc_mean': scores_rf['test_accuracy'].mean(), 'acc_std': scores_rf['test_accuracy'].std(), 'f1_mean': scores_rf['test_f1'].mean()})\n            results_rf.append({'acc_mean': scores_rf})\nresults_rf = pd.DataFrame(results_rf, index=indexes)","dc9fe9a4":"# fig, ax = plt.subplots(1,1,figsize=(20,10))\n# for i in range(3):\n#     ax[i].xaxis.set_ticklabels(indexes ,rotation=90)\n#     ax[i].grid()\n\n# ax[0].plot(results_rf['acc_mean'], c = 'red')\n# ax[0].set_ylabel('accuracy')\n# ax[1].plot(results_rf['acc_std'], c = 'blue')\n# ax[1].set_ylabel('standard deviation')\n# ax[2].plot(results_rf['f1_mean'], c= 'purple')\n# ax[2].set_ylabel('f1 score')\n\n# mam = max(results_rf['acc_mean'])\n# mas = min(results_rf['acc_std'])\n# mfm = max(results_rf['f1_mean'])\n# ax[0].axhline(y=mam,ls='--',c='green')\n# ax[1].axhline(y=mas,ls='--',c='green')\n# ax[2].axhline(y=mfm,ls='--',c='green')\nplt.figure(figsize=(20,5))\nplt.plot(results_rf['acc_mean'],c='red')\nplt.grid()\nplt.xticks(indexes,rotation=90)\nplt.ylabel('accuracy')\nplt.axhline(y=max(results_rf['acc_mean']))","ca1466d6":"rf_model = ensemble.RandomForestClassifier(n_estimators=100,max_depth=4,min_samples_split=4, n_jobs=-1)","e1c68c9b":"results_et = []\nindexes = []\nfor j in range(100,300,100):\n    for i in range(2,10):\n        for k in range(2,10):\n            indexes.append('n'+str(j)+'d'+str(i)+'s'+str(k));\n            et_model = ensemble.ExtraTreesClassifier(n_estimators=j,max_depth=i,min_samples_split=k,n_jobs=-1,oob_score=True,bootstrap=True)\n            #scores_et = model_selection.cross_validate(et_model, X, y,cv=cv_size, scoring=('accuracy','f1'))\n            scores_et = et_model.fit(X,y).oob_score_\n            #print('Trained in ' + '{:.2f}'.format(scores_rf['fit_time'].mean()) + 's , achieved Accuracy of ' +  '{:.3f}'.format(scores_rf['test_accuracy'].mean()) + ' ('+ '{:.3f}'.format(scores_rf['test_accuracy'].std()) +')')\n            #results_et.append({'acc_mean': scores_et['test_accuracy'].mean(), 'acc_std': scores_et['test_accuracy'].std(), 'f1_mean': scores_et['test_f1'].mean()})\n            results_et.append({'acc_mean': scores_et})\n\nresults_et = pd.DataFrame(results_et, index=indexes)","f7808798":"# fig, ax = plt.subplots(3,1,figsize=(20,15))\n# for i in range(3):\n#     ax[i].xaxis.set_ticklabels(indexes ,rotation=90)\n#     ax[i].grid()\n\n# ax[0].plot(results_et['acc_mean'], c = 'red')\n# ax[0].set_ylabel('accuracy')\n# ax[1].plot(results_et['acc_std'], c = 'blue')\n# ax[1].set_ylabel('standard deviation')\n# ax[2].plot(results_et['f1_mean'], c= 'purple')\n# ax[2].set_ylabel('f1 score')\n\n# mam = max(results_et['acc_mean'])\n# mas = min(results_et['acc_std'])\n# mfm = max(results_et['f1_mean'])\n# ax[0].axhline(y=mam,ls='--',c='green')\n# ax[1].axhline(y=mas,ls='--',c='green')\n# ax[2].axhline(y=mfm,ls='--',c='green')\n\nplt.figure(figsize=(20,5))\nplt.plot(results_et['acc_mean'],c='red')\nplt.grid()\nplt.xticks(indexes,rotation=90)\nplt.ylabel('accuracy')\nplt.axhline(y=max(results_et['acc_mean']))","eb9d1019":"et_model = ensemble.ExtraTreesClassifier(n_estimators=200,max_depth=6,min_samples_split=4,n_jobs=-1,bootstrap=True)","4ff93f35":"results_svc = []\nfor i in np.arange(0.1,10,0.1):\n    svm_model = svm.SVC(C=i)\n    scores_svc = model_selection.cross_validate(svm_model,X,y,cv=cv_size, scoring=('accuracy','f1'))\n    #print('Trained in ' + '{:.2f}'.format(scores_svc['fit_time'].mean()) + 's , achieved Accuracy of ' +  '{:.3f}'.format(scores_svc['test_accuracy'].mean()) + ' ('+ '{:.3f}'.format(scores_svc['test_accuracy'].std()) +')')\n    results_svc.append({'acc_mean': scores_svc['test_accuracy'].mean(), 'acc_std': scores_svc['test_accuracy'].std(), 'f1_mean': scores_svc['test_f1'].mean()})\nresults_svc = pd.DataFrame(results_svc, index=np.arange(0.1,10,0.1))","9bdca926":"fig, ax = plt.subplots(3,1,figsize=(20,10))\nfor i in range(3):\n    ax[i].xaxis.set_ticks(np.arange(0.1,10,0.1))\n    ax[i].xaxis.set_ticklabels(['{:.1f}'.format(x) for x in np.arange(0.1,10,0.1)] ,rotation=90)\n    ax[i].grid()\n\nax[0].plot(results_svc['acc_mean'], c = 'red')\nax[0].set_ylabel('accuracy')\nax[1].plot(results_svc['acc_std'], c = 'blue')\nax[1].set_ylabel('std deviation')\nax[2].plot(results_svc['f1_mean'], c= 'purple')\nax[2].set_ylabel('f1 score')\n\nmam = max(results_svc['acc_mean'])\nmas = min(results_svc['acc_std'])\nmfm = max(results_svc['f1_mean'])\nax[0].axhline(y=mam,ls='--',c='green')\nax[1].axhline(y=mas,ls='--',c='green')\nax[2].axhline(y=mfm,ls='--',c='green')","e36fccb3":"svm_model = svm.SVC(C=0.5)","f8e34743":"results_knn = []\nfor k in range(5,50):\n    knn_model = neighbors.KNeighborsClassifier(n_neighbors=k, weights='uniform',algorithm='ball_tree')\n    scores_knn = model_selection.cross_validate(knn_model,X,y,cv=cv_size,scoring=('accuracy','f1'))\n    #print('Trained in ' + '{:.2f}'.format(scores_knn['fit_time'].mean()) + 's , achieved Accuracy of ' +  '{:.3f}'.format(scores_knn['test_accuracy'].mean()) + ' ('+ '{:.3f}'.format(scores_knn['test_accuracy'].std()) +')')\n    results_knn.append({'acc_mean': scores_knn['test_accuracy'].mean(), 'acc_std': scores_knn['test_accuracy'].std(), 'f1_mean': scores_knn['test_f1'].mean()})\nresults_knn = pd.DataFrame(results_knn, index=range(5,50))","be73aeea":"fig, ax = plt.subplots(3,1,figsize=(15,10))\nfor i in range(3):\n    ax[i].xaxis.set_ticks(range(1,50))\n    ax[i].grid()\n\nax[0].plot(results_knn['acc_mean'], c = 'red')\nax[0].set_ylabel('accuracy')\nax[1].plot(results_knn['acc_std'], c = 'blue')\nax[1].set_ylabel('std deviation')\nax[2].plot(results_knn['f1_mean'], c= 'purple')\nax[1].set_ylabel('f1 score')\n\nmam = max(results_knn['acc_mean'])\nmas = min(results_knn['acc_std'])\nmfm = max(results_knn['f1_mean'])\nax[0].axhline(y=mam,ls='--',c='green')\nax[1].axhline(y=mas,ls='--',c='green')\nax[2].axhline(y=mfm,ls='--',c='green')","91e4c1bd":"knn_model = neighbors.KNeighborsClassifier(n_neighbors=10, weights='uniform',algorithm='ball_tree')","c6e5f1c9":"nb_model = naive_bayes.GaussianNB()\nscores_nb = model_selection.cross_validate(nb_model,X,y,cv=cv_size, scoring=('accuracy','f1'))\nprint('Trained in ' + '{:.2f}'.format(scores_nb['fit_time'].mean()) + 's , achieved Accuracy of ' +  '{:.3f}'.format(scores_nb['test_accuracy'].mean()) + ' ('+ '{:.3f}'.format(scores_nb['test_accuracy'].std()) +')')\n    ","33ec49b3":"nb_model = naive_bayes.GaussianNB()","881f06da":"#Stacking or Voting?\nstacked_model = ensemble.StackingClassifier(estimators=[('et',et_model),\n                                                        ('rf',rf_model),\n                                                        ('svm',svm_model),\n                                                        ('lr',lr_model)\n                                                       ])\n\nvoting_model = ensemble.VotingClassifier(estimators=[('et',et_model),\n                                                     ('rf',rf_model), \n                                                     ('svm',svm_model),\n                                                     ('lr',lr_model)\n                                                    ])\n                                            \nscores_stack = model_selection.cross_validate(stacked_model,X,y,cv=cv_size,scoring='accuracy')\nscores_voting = model_selection.cross_validate(voting_model,X,y,cv=cv_size,scoring='accuracy')\nprint('Trained in ' + '{:.2f}'.format(scores_stack['fit_time'].mean()) + 's , achieved Accuracy of ' +  '{:.3f}'.format(scores_stack['test_score'].mean()) + ' ('+ '{:.3f}'.format(scores_stack['test_score'].std()) +')')\nprint('Trained in ' + '{:.2f}'.format(scores_voting['fit_time'].mean()) + 's , achieved Accuracy of ' +  '{:.3f}'.format(scores_voting['test_score'].mean()) + ' ('+ '{:.3f}'.format(scores_voting['test_score'].std()) +')')","fe97a5db":"models = {'lr':lr_model,\n          'rf':rf_model,\n          'et':et_model,\n          'svm': svm_model,\n          'knn': knn_model,\n          'nb': nb_model,\n          'stack': stacked_model,\n          'vote':voting_model}","317d58b1":"for i in models.keys():\n    m = models[i].fit(X,y)\n    y_pred = m.predict(test_X)\n    prediction = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\n    prediction['Survived'] = y_pred\n    prediction.set_index('PassengerId')\n    prediction.to_csv('submission-'+i+'.csv',index = False)","4fbf2c2d":"<a id=\"normalization\"><\/a>\n### Data Normalization","f74758ee":"We can however divide these title in four different groups:\n* Titles regarding the crew members\n* Honourific Titles\n* Honourific titles regarding the profession\n* Normal titles","fccee8ca":"<a id=\"knn\"><\/a>\n### K-Nearest Neighbors\n\nIn this case we explore the possible number of neighbors. We'll go from 5 to 50.","1d41d377":"<a id=\"feature-selection\"><\/a>\n### Feature Selection","e5d93bf4":"... then we can delete the columns we don't need: Cabin (due to the many missing values), PassengerId (it is a Primary Key), Name, Ticket and Survived, the latter only on the training set of course.","ee40bca0":"Let's begin by seeing the proportions in men and women among passengers:","727d8fe7":"### Libraries and Data","e53ec6d1":"<a id=\"random-forest\"><\/a>\n### Random Forest\n\nFor Random Forest using the standard Gini Index. For now we  try to explore various values for max_depth and min_samples_split and 100 and 200 n_estimators.\n\n<code>max_depth<\/code> -> **[2, 10]**\n\n<code>min_sample_split<\/code> -> **[2, 10]**\n\nFor encoding the grid search, we use the terminology n**k**d***x***s***y*** where x is the max_dept and y the min_samples and k the estimators number.","0cc95512":"Something I almost immediately noticed is that 'Fare' has some REALLY big values along the dataset. So I tried doing a boxplot in order to check how many outliers are there:","fadae7a6":"<a id=\"Fam-size\"><\/a>\n### Family size","8257ae63":"We can highlight different threshold in order to select the features.\nWe could also check the Correlation among the features:","f1055101":"## Table of Content\n* 0. [Abstract](#0)\n* 1. [*Investigation*](#1): can we understand what happened only from data?\n* 2. [*Feature Selection* and *Engineering*](#2)\n    - 2.1 [People Titles](#title-extraction)\n    - 2.2 [Family Size](#Fam-size)\n    - 2.3 [Ticket Info](#ticket-info)\n    - 2.4 [Fare Analysis](#fare-analysis)\n    - 2.5 [Data Preparation](#data-preparation)\n    - 2.6 [Feature Selection](#feature-selection)\n    - 2.7 [Data Normalization](#normalization)\n* 3. [*Prediction* ](#3): Model selection and Performance evaluation\n    - 3.1 [Logistic Regression with Simple Features](#logistic-regression)\n    - 3.2 [Random Forest](#random-forest)\n    - 3.3 [Extra Trees Classifier](#extra-trees)\n    - 3.4 [Support Vector Machine](#svm)\n    - 3.5 [K-Nearest Neighbors](#knn)\n    - 3.6 [Naive Bayes](#naive-bayes)\n    - 3.7 [Stacking](#stacking)","e19256c5":"<a id=\"data-preparation\"><\/a>\n### Data Preparation","3692282a":"Then we can plot the survival rate according to this new division.","8ad5caa1":"We can have here a first summary for our dataset. We can see the statistics for the numerical values and also for the categorical ones. It can be seen at first glance that some variables have missing values: let's plot however how much and what of them have it...","3faccb91":"<a id=\"extra-trees\"><\/a>\n### Extra Trees Classifier\n","fed5776e":"<a id = \"3\" ><\/a>\n## 3.*Prediction*: Model selection and Performance evaluation\n\nNow that we have the data in a good format, it is time to perform prediction in order to identify the best models for a **Binary Classification** of this kind. The possible candidates are:\n\n* Logistic Regression\n* Random Forest\n* Support Vector Machines\n* K-Nearest Neighbors\n* Boosting with Decision Stumps\n* Stacking of models\n* Naive Bayes\n\nFor each one of these methods I will also use *Cross Validation* in order to find a good set of h-parameters for the model (for example in the RF I'll tune the max depth and minimum samples of split, for KNN the best number of neighbors and so on).\nAt the end of the process, after the search for the best hyper parameters, a final comparison with all the models will be done and the best one is chosen for submission. \n\nHow will we choose the best model? We'll simply look at the best accuracy and minimum standard deviation, then try to find a compromise between the two paramenters, since we'd like a high and \"peaked\" accuracy. ","6af4ad5c":"We now load the dataset and reserve the survived feature onto another variable for later.","69981cb8":"Now we try also to transform and rearrange your data while trying to estrapolate information about the survival rates on various categories.\n\nLet's begin by looking at some 'Titles' (honourific or not) that people contain in their names. We have to estrapolate them both by the training and the test set:","d3e291e5":"<a id=\"svm\"><\/a>\n### Support Vector Machine\n\nFor the SVC we will tune the parameter for the soft margin making it vary from 0.1 to 10 with a step of 0.1.","48734e69":"<a id=\"naive-bayes\"><\/a>\n### Naive Bayes","f481a53e":"<a id=\"fare-analysis\"><\/a>\n### Fare Analysis","bd8e4e68":"Another really important feature that can be created is the total number of family members the person in traveling with. \nWe can sum then the columns regarding \"Parch\" and \"SibSp\" in order to obtain it.","bdfa32b8":"<a id=\"stacking\"><\/a>\n### Stacking","cd8f5455":"... And here we are! It can be clearly seen how first class passengers had more possibilities to escape the distaster. Hypothesis could be two: first class men and women were VIPs, so crew gave them priority OR simply they were on higher levels of the boat, so they had more time and more resources before the total sinking happened. However the ratios are really discouraging: more than 60% of 1st class passengers survived, whereas only about the 25% of the 3rd class was lucky enough. Of course this could be also due to the impressive number of people in third class: more than double with respect to first class.","0aa5bd0a":"# Titanic Distaster: Investigation, Feature Selection and Prediction","b6d4d70a":"<a id=\"logistic-regression\"><\/a>\n### Logistic Regression with Simple Features","c4dab125":"<a id='1'><\/a>\n## 1.*Investigation*: can we understand what happened only from data?\n\nI think a very beautiful and fascinating part about data analysis is investigating about the causes of some results that we see in data. Even if this is a very little dataset, we are able to validate some facts that have been effectively told by survivors about the disaster.\n\nLet's begin by seeing some proportions inside the people that were on board:\n* How many men and how many women?\n* How many died and how many survived?\n* Can we distinguish crew members from simple passengers?\n* If anyone couldn't be saved, what was used in order to decide who to save and who not?","4a1f8396":"Now we can proceed to the encoding of the new features we created.\nWe are going to use the OrdinalEncoder and OneHotEncoder of scikit-learn.\nEncoders will be fitted on training set but applied to both train and test set.","b5a1328b":"<a id=\"ticket-info\"><\/a>\n### Ticket Information","52fc9b83":"The first table tells us how many men and women were on board: 577 men and 314 women. But the impressive fact is that only 20% of men survived, whereas the 75% of women survived the sinking.\nAlready by this strange ratio we can see that the policy was to save women before men. Let's dig deeper: Let's plot the distribution in Age of survived men and women and deceased men and women. We will distinguish three main age periods: Adolescents, Adults and Elders.","9b402cc5":"We can already see something interesting: many person on the third class died whereas a small number of people died for the other two classes. It can be seen also that the distribution for the saved people along the three classes is quite balanced.\nLet's examine the ratio saved\/total for every class in order to see it better:","6321922b":"Also there are columns having a 0.0 Fare: Honestly i don't know if it can be considered as an error in the data, because it could be possible that some of the tickets have been gifted, mainly to relevant people. However, in the doubt, i considered them as null values and assigned them the mean Fare value DEPENDING on the class they're in. ","e5399082":"<a id='0'><\/a>\n## 0.Abstract\n\nIn this simple and short Notebook we will analyse the data that have been given to us and try to use them in order to predict if a person that has specific characteristics would survive in a sinking like the one happened in 1912 of the Titanic.\n\nAlso, a quite interesting thing to do in also to try **Reconstructing** what policies for saving people were adopted on the cruiseafter the situation degenerated into the disaster that we sadly well know.\n\n****WARNING!!!****: The notebook is published now in a very raw format. It will be enhanced later in time, when possible. If you have any suggestion or anything that you don't agree with me feel free to comment: I really look forward to learn more and more! :)","677df233":"We can then plot a barplot that shows the survival rate for each title.","afc7e74c":"We can see that Cabin has a very high percentage of missing values. Could they be replaced with something? Well... yes, but this way we are making an assumption we don't know it's correct (we could assume that if the Cabin is NaN it si some low level class cabin and it has not been tracked). It is more probable that the NaN represents here a value that simply we don't have. So we will delete also this attribute. For now, however, let's mantain the value: it could be useful for our investigation part.\n\nAge, instead, could be imputed in two different ways: the first (and the one we use for simplicity) is to replace the missing value with the mean value of the age. The second one could be the imputation by training a simple linear model and trying to predict the age of the person by some attributes like Sex, Parch and SibSp. In this way we COULD have some more accurate results (we shall also try this later).","f22bd7d9":"We can divide the values by distinguishing among:\n* Alone travelers\n* Small family (1\/3 people)\n* Medium family (4\/6 people)\n* Large family (6+ people)","d0722072":"Can we do something else for the features? It depends: We can use some feature selection methods in order to see if these features can effectively be used or they are just useless for our prediction.\n\nNow I use a simple Random Forest in order to compute the feature importances and try to detect the best set of features to fit.\nIt results in the following barplot:","d7571be1":"Now it is time to normalize our data:","243de97d":"<a id=\"title-extraction\"><\/a>\n### People Titles","5028ecd6":"<a id=\"data-submission\" ><\/a>\n## Data Submission","67fd1d4d":"These last plots highlight that not only on the sexes, but the decision of the crew might have been to save women and children first. In fact you could notice the peaks that appear near to early ages, in particular for young men.\nFor women it's a little different: it can be clearly seen how many young and old women died during the sinking. The elder ones maybe due to previous illnesses or physical inability to sustain the very low temperatures of that night, others probably drowned. Younger women probably died in the same way. Women in the range of age occurring between 19 and 35 instead survived more ofted according to the distributions.\n\nAnother important datum that can help us discovering the policies that were adopted by the crew in order to save people could have been the class of the cabin of the crew. We could plot a similar distribution by conditioning upon the Class of the cabin.","b095748a":"We could also estrapolate information from the ticket name, but honestly i didn't find anything useful in it, so if someone has some suggestion on how to deal with it and what these initial names in ticket represent... well comment or link some other kernel that explains it! It will be very useful :D","10614037":"<a id=\"2\"><\/a>\n## 2.*Feature Selection* and *Engineering*"}}