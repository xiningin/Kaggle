{"cell_type":{"1d655c0a":"code","bc4eeee6":"code","b9f743fe":"code","5e47239a":"code","efb5ff4c":"code","03421df8":"code","fe7c2258":"code","6538dc54":"code","9a633300":"code","bde3a6c6":"code","af5cf305":"code","bffd2b8a":"code","b6a0c09f":"code","c2216bed":"code","77e98dcb":"code","83ad8982":"code","97feab65":"code","d6088bf9":"code","7d7cf73a":"code","83580364":"code","dce324cb":"code","e6452bbf":"code","d6160d38":"code","42398f2e":"code","75d2c431":"code","e1e9c443":"code","87130c90":"markdown","d6ffd1a7":"markdown","4b57b1e1":"markdown","83e0c79b":"markdown","150cccb4":"markdown","2b8486a0":"markdown","e80c8ce3":"markdown","8502a537":"markdown","3fe233a9":"markdown","04a7c606":"markdown","f68afbf7":"markdown","939bb92a":"markdown","9252fb35":"markdown","d9aca7e3":"markdown","4b789595":"markdown","f6f5a84c":"markdown"},"source":{"1d655c0a":"import numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.array([1,2,4,3,5,6,7,8])\ny = np.array([1,3,4,2,5,6,9,6])\n\nprint(x)\nprint(y)","bc4eeee6":"plt.scatter(x,y,s=100)\nplt.grid(color='b', ls = '-.', lw = 0.25)\nplt.title(\"X versus Y\")\nplt.show()","b9f743fe":"def get_slope_and_bias(x,y):\n    mean_x = np.mean(x)\n    mean_y = np.mean(y)\n    \n    numerator = 0\n    denominator = 0\n    \n    for i in range(len(x)):\n        numerator += (x[i] - mean_x) * (y[i] - mean_y)\n        denominator += np.square(x[i] - mean_x)\n        \n    slope = numerator\/ denominator\n    \n    ## bias\n    bias = mean_y - slope * mean_x\n    \n    return slope, bias\n\nslope, bias = get_slope_and_bias(x,y)","5e47239a":"y_pred = x * slope + bias","efb5ff4c":"plt.scatter(x,y,s=100,label='True values')\nplt.grid(color='b', ls = '-.', lw = 0.25)\nplt.plot(x,y_pred,c='g')\nplt.scatter(x,y_pred,c='r',s=100,label=\"prediction\")\nplt.title(\"Simple Linear Regression\")\nplt.legend()\nplt.show()","03421df8":"def get_rmse_error(y,y_pred):\n    n = len(y)\n    return np.sqrt(np.sum([np.square(pi -yi)\/n for pi,yi in zip(y,y_pred)]))\n\nget_rmse_error(y,y_pred)","fe7c2258":"def get_slope_and_bias(x,y,alpha,epochs):\n    b0 = 0.0\n    b1 = 0.0\n    errors = list()\n    \n    for _ in range(epochs):\n        for i in range(len(x)): \n            y_p = b0 + b1*x[i]\n            error = y_p - y[i]\n            errors.append(error)\n            b0 -= alpha * error\n            b1 -= alpha * error * x[i]\n            \n    return b0,b1,errors\n\nb0, b1,errors = get_slope_and_bias(x,y,0.01,4)    ","6538dc54":"y_pred = b0 + b1 * x","9a633300":"plt.scatter(x,y,s=100,label='True values')\nplt.grid(color='b', ls = '-.', lw = 0.25)\nplt.plot(x,y_pred,c='g')\nplt.scatter(x,y_pred,c='r',s=100,label=\"prediction\")\nplt.title(\"Simple Linear Regression\")\nplt.legend()\nplt.show()","bde3a6c6":"plt.plot(errors)\nplt.title(\"error\")\nplt.show()","af5cf305":"data = [[2.7810836,2.550537003,0],\n    [1.465489372,2.362125076,0],\n    [3.396561688,4.400293529,0],\n    [1.38807019,1.850220317,0],\n    [3.06407232,3.005305973,0],\n    [7.627531214,2.759262235,1],\n    [5.332441248,2.088626775,1],\n    [6.922596716,1.77106367,1],\n    [8.675418651,-0.242068655,1],\n    [7.673756466,3.508563011,1]] ","bffd2b8a":"def predict(row,b0,b1,b2):\n    y =  b0 + row[0] * b1 + row[1] * b2 \n    y = 1\/(1+np.exp(-y))\n    return y\n\ndef gradient_descent(data,epochs,lr):\n    #initializing the weights\n    b0 = 0.0\n    b1 = 0.0\n    b2 = 0.0\n    \n    for _ in range(epochs):\n        for row in data:\n            y_pred = predict(row,b0,b1,b2)\n            error =  row[-1] - y_pred\n            \n            b0 += lr * error * y_pred * (1.0 - y_pred)\n            b1 += lr * error * y_pred * (1.0 - y_pred) * row[0]\n            b2 += lr * error * y_pred * (1.0 - y_pred) * row[1]\n    \n    return b0, b1, b2\n\nb0 , b1 , b2 = gradient_descent(data,100,0.01)\nprint([b0,b1,b2])","b6a0c09f":"def predict(row,coef):\n    #b0\n    y = coef[0]\n    # y = b0 + b1 * x1 + b2 *x2 .....\n    for i in range(len(row)-1):\n        y += row[i] * coef[i+1]\n    y = 1\/(1 + np.exp(-y))\n    return y\n\ndef gradient_descent(data,epochs,lr):\n    #initializing the weights\n    coef = [0.0 for _ in range(len(data[0]))]\n    \n    for _ in range(epochs):\n        for row in data:\n            y_pred = predict(row,coef)\n            error =  row[-1] - y_pred\n            #updating b0\n            coef[0] += lr * error * y_pred * (1.0 - y_pred)\n            for i in range(len(row)-1):\n                coef[i + 1] += lr * error * y_pred * (1.0 - y_pred) * row[i]\n                \n    return coef\n\ncoef = gradient_descent(data,100,0.01)\nprint(coef)\n\ndef logistic_regression(train,test,lr,n_epoch):\n    predictions = list()\n    coef = gradient_descent(train,n_epoch,lr)\n    for row in test:\n        y_pred = predict(row,coef)\n        y_pred = np.round(y_pred)\n        predictions.append(y_pred)\n        \n    return predictions\n\npredictions = logistic_regression(data[:-2],data[-2:],0.1,100)\nprint(predictions)\n\nactual = [row[-1] for row in data[-2:]]\n\ndef accuracy(prediction,actual):\n    correct = 0\n    for i in range(len(prediction)):\n        if prediction[i] ==  actual[i]:\n            correct += 1\n    return correct\/ len(actual) * 100 \n\nprint(f\"{accuracy(predictions,actual)}%\")","c2216bed":"import pandas as pd\n\ndef load_data(path):\n    csv = pd.read_csv(path).values\n    data = list()\n    for row in csv:\n        data.append(row)\n    return data\n\n#we need to normalize the data\ndef normalize_data(data):\n    #first calculate min and max values for all columns\n    minmax = list()\n    for i in range(len(data[0])):\n        column = [row[i] for row in data]\n        col_min = np.min(column)\n        col_max = np.max(column)\n        minmax.append([col_min,col_max])\n    \n    #normalize the data\n    for row in data:\n        for i in range(len(row)):\n            row[i] = (row[i] - minmax[i][0])\/ (minmax[i][1] - minmax[i][0])\n    \n    return data\n\n\ndef train_test_split(data,ratio):\n    n = int(len(data) * ratio)\n    return data[:-n],data[-n:]\n\n\ndata = load_data(\"..\/input\/prima-indiansdiabetes\/pima-indians-diabetes.csv\")\ndata = normalize_data(data)\ntrain, test = train_test_split(data,0.2)\n\nprint(len(train))\nprint(len(test))\n\nprediction = logistic_regression(train,test,0.1,100)\nactual = [row[-1] for row in test]\nacc = accuracy(prediction,actual)\n\n\nprint(f\"Accuracy of model is : {acc}%\")","77e98dcb":"data = [[4.667797637  , 0]\n        ,[5.509198779 , 0]\n        ,[4.702791608 , 0]\n        ,[5.956706641 , 0]\n        ,[5.738622413 , 0]\n        ,[5.027283325 , 0]\n        ,[4.805434058 , 0]\n        ,[4.425689143 , 0]\n        ,[5.009368635 , 0]\n        ,[5.116718815 , 0]\n        ,[6.370917709 , 0]\n        ,[2.895041947 , 0]\n        ,[4.666842365 , 0]\n        ,[5.602154638 , 0]\n        ,[4.902797978 , 0]\n        ,[5.032652964 , 0]\n        ,[4.083972925 , 0]\n        ,[4.875524106 , 0]\n        ,[4.732801047 , 0]\n        ,[5.385993407 , 0]\n        ,[20.74393514 , 1]\n        ,[21.41752855 , 1]\n        ,[20.57924186 , 1]\n        ,[20.7386947  , 1]\n        ,[19.44605384 , 1]\n        ,[18.36360265 , 1]\n        ,[19.90363232 , 1]\n        ,[19.10870851 , 1]\n        ,[18.18787593 , 1]\n        ,[19.71767611 , 1]\n        ,[19.09629027 , 1]\n        ,[20.52741312 , 1]\n        ,[20.63205608 , 1]\n        ,[19.86218119 , 1]\n        ,[21.34670569 , 1]\n        ,[20.333906   , 1]\n        ,[21.02714855 , 1]\n        ,[18.27536089 , 1]\n        ,[21.77371156,  1]\n        ,[20.65953546,  1]]\n\ndata = np.array(data)\n\ndata_1 = data[data[:,1] == 0,0]\ndata_0 = data[data[:,1] == 1,0]\n\nplt.scatter(np.arange(len(data_1)),data_1,c='r', label='1')\nplt.scatter(np.arange(len(data_0)),data_0, c ='g', label='0')\nplt.legend()\nplt.title(\"Points of two classes\")\nplt.show()","83ad8982":"def LDA_binary(data,test):\n    \n    predictions = list()\n    data_1 = data[data[:,1] == 1,0]\n    data_0 = data[data[:,1] == 0,0]\n    \n    p_1 = len(data_1)\/len(data)\n    p_0 = len(data_0)\/len(data)\n\n    mean_1 = np.mean(data_1)\n    mean_0 = np.mean(data_0)\n\n    variance = 1\/(len(data) - 2) * (np.sum((data_1 - mean_1)**2) + np.sum((data_0 - mean_0)**2))\n\n    def discriminant_1(x,mean_1,variance,p_1):\n        return x * (mean_1 \/ variance) - (mean_1**2 \/ 2 * variance) + np.log(p_1)\n\n    def discriminant_0(x,mean_0,variance,p_0):\n        return x * (mean_0 \/ variance) - (mean_0**2 \/ 2 * variance) + np.log(p_0)\n    \n    for x in test[:,0]:\n        d1 = discriminant_1(x,mean_1,variance,p_1)\n        d0 = discriminant_0(x,mean_0,variance,p_0)\n        \n        if d0 >= d1:\n            predictions.append(0)\n        else:\n            predictions.append(1)\n    \n    actual = test[:,1]\n    \n    prediction = np.array(predictions)\n    acc = accuracy(prediction,actual)\n    \n    return predictions, acc\n\n## for demonstration purpose we are using data as train and tes\npredictions, acc = LDA_binary(data,data)\n            \nprint(acc)\nprint(predictions)\n        ","97feab65":"def data_split(index,value,dataset):\n    left, right = list(), list()\n    for row in dataset:\n        if row[index] < value:\n            left.append(row)\n        else:\n            right.append(row)\n    return left , right\n\ndef gini_index(groups, classes):\n    \n    n = float(sum(len(group) for group in groups))\n    gini = 0.0\n    for group in groups:\n        size = float(len(group))\n        if size == 0:\n            continue\n        score = 0.0\n        \n        for cls in classes:\n            p = [row[-1] for row in group].count(cls) \/ size\n            score += p**2\n            \n        gini += (1.0 - score) * (size \/ n)\n        \n    return gini\n        \n\ndef get_split(dataset):\n    class_values = list(set(row[-1] for row in dataset))\n    index, value, score, groups = 999,999,999, None\n    \n    for idx in range(len(dataset[0])-1):\n        for row in dataset:\n            grps = data_split(idx,row[idx],dataset)\n            gini = gini_index(grps,class_values)\n            if gini < score:\n                index, value, score, groups = idx, row[idx],gini,grps\n    \n    return {'index':index,'value':value,'score':score,'groups':groups}\n                \n\n    \ndataset = [[2.771244718,1.784783929,0],\n    [1.728571309,1.169761413,0],\n    [3.678319846,2.81281357,0],\n    [3.961043357,2.61995032,0],\n    [2.999208922,2.209014212,0],\n    [7.497545867,3.162953546,1],\n    [9.00220326,3.339047188,1],\n    [7.444542326,0.476683375,1],\n    [10.12493903,3.234550982,1],\n    [6.642287351,3.319983761,1]]\n\nsplit = get_split(dataset)\nprint(split)","d6088bf9":"def to_terminal(group):\n    outcomes = [row[-1] for row in group]\n    return max(set(outcomes), key= outcomes.count)\n\ndef split(node, max_depth,min_size,depth):\n    left, right = node['groups']\n    del(node['groups'])\n    \n    if not left or not right:\n        node['left'] = node['right'] = to_terminal(left + right)\n        return\n    \n    if depth >= max_depth:\n        node['left'] , node['right'] = to_terminal(left), to_terminal(right)\n        return\n    \n    if len(left) <= min_size:\n        node['left'] = to_terminal(left)\n        \n    else:\n        node['left'] = get_split(left)\n        split(node['left'],max_depth,min_size,depth+1)\n    \n    if len(right) <= min_size:\n        node['right'] = to_terminal(right)\n        \n    else:\n        node['right'] = get_split(right)\n        split(node['right'],max_depth,min_size,depth+1)\n\n\ndef build_tree(train,max_depth,min_size):\n    root = get_split(train)\n    split(root,max_depth,min_size,1)\n    return root\n        \n\n# Print a decision tree\ndef print_tree(node, depth=0):\n    if isinstance(node, dict):\n        print('%s[X%d < %.3f]' % ((depth*' ', (node['index']+1), node['value'])))\n        print_tree(node['left'], depth+1)\n        print_tree(node['right'], depth+1)\n    else:\n        print('%s[%s]' % ((depth*' ', node)))\n\ndataset = [[2.771244718,1.784783929,0],\n    [1.728571309,1.169761413,0],\n    [3.678319846,2.81281357,0],\n    [3.961043357,2.61995032,0],\n    [2.999208922,2.209014212,0],\n    [7.497545867,3.162953546,1],\n    [9.00220326,3.339047188,1],\n    [7.444542326,0.476683375,1],\n    [10.12493903,3.234550982,1],\n    [6.642287351,3.319983761,1]]\n\ntree = build_tree(dataset, 2, 1)\nprint_tree(tree)","7d7cf73a":"def predict(node, row):\n    if row[node['index']] < node['value']:\n        if isinstance(node['left'], dict):\n            return predict(node['left'], row)\n        else:\n            return node['left']\n    else:\n        if isinstance(node['right'], dict):\n            return predict(node['right'], row)\n        else:\n            return node['right']\n        \n\nfor row in dataset:\n    prediction = predict(tree, row)\n    print('Expected=%d, Got=%d' % (row[-1], prediction))","83580364":"dataset = [[2.771244718,1.784783929,0],\n    [1.728571309,1.169761413,0],\n    [3.678319846,2.81281357,0],\n    [3.961043357,2.61995032,0],\n    [2.999208922,2.209014212,0],\n    [7.497545867,3.162953546,1],\n    [9.00220326,3.339047188,1],\n    [7.444542326,0.476683375,1],\n    [10.12493903,3.234550982,1],\n    [6.642287351,3.319983761,1]]\n\ndataset = np.array(dataset)\n\n\n#lets create dictionary which contains mean, std and len of columns of each class\n\ndef get_summary(data,classes):\n    summary_by_class = dict()\n    \n    for c in classes:\n        summary_by_class[c] = list()\n        temp = data[data[:,-1] == c][:,:-1]\n        \n        for column in zip(*temp):\n            summary = [np.mean(column),np.std(column),len(column)]\n            summary_by_class[c].append(summary)\n            \n    return summary_by_class\n        \nsummary = get_summary(dataset,[0,1])\n\nprint(\"mean, std and len of each columns of each class\")\nfor label in summary:\n    print(label)\n    for row in summary[label]:\n        print(row)\n\n# now as we have this statistics we could use it to calculate Pdf(probability density function)\ndef PDF(x,mean,std):\n    exponent = np.exp(-((x-mean)**2 \/ (2 * std**2)))\n    return (1\/ (np.sqrt(2 * np.pi) * std)) * exponent\n\ndef make_prediction(summaries,data):\n    predictions = list()\n    #total rows in orignal data from summaries\n    total_rows = sum([summaries[label][0][2] for label in summaries])\n    for row in data:\n        prediction = dict()\n        for class_value, summary in summaries.items():\n            #class probability\n            prediction[class_value] = summaries[class_value][0][2]\/ float(total_rows)\n            #probability of data given class\n            for i in range(len(summary)):\n                mean, std, _ = summary[i]\n                prediction[class_value] *= PDF(row[i],mean,std)\n        predictions.append(max(prediction,key=prediction.get))\n    return predictions\n\npredictions = make_prediction(summary,dataset)\nprint(\"prediction : \",predictions)","dce324cb":"data = pd.read_csv(\"..\/input\/iris\/Iris.csv\")\ndata.drop(\"Id\",axis=1,inplace=True)\n#converting categorical data to number\ndata['Species'] = data['Species'].astype('category').cat.codes","e6452bbf":"train, test = train_test_split(data.values,0.2)\n\nsummary = get_summary(train,train[:,-1])\n\npredictions = make_prediction(summary,test)\n\nactual = test[:,-1]\n\naccuracy(predictions,actual)","d6160d38":"#let's use our same dummy dataset for illustration purpose\ndataset = [[2.771244718,1.784783929,0],\n    [1.728571309,1.169761413,0],\n    [3.678319846,2.81281357,0],\n    [3.961043357,2.61995032,0],\n    [2.999208922,2.209014212,0],\n    [7.497545867,3.162953546,1],\n    [9.00220326,3.339047188,1],\n    [7.444542326,0.476683375,1],\n    [10.12493903,3.234550982,1],\n    [6.642287351,3.319983761,1]]\n\ndataset = np.array(dataset)\n\ndef KNN(train,test,k):\n    predictions = list()\n    classes = list(set(train[:,-1]))\n    \n    for row in test:\n        diff_sqr = (train[:,:-1] - row)**2\n        summation = list(map(sum,diff_sqr))\n        distances = np.sqrt(summation)\n        \n        indexes = np.argsort(distances)[:k]\n        labels = train[indexes,-1]\n\n        values ,counts = np.unique(labels,return_counts=True)\n        prediction = values[np.argmax(counts)]\n        predictions.append(prediction)\n        \n    return predictions\n        \npredictions = KNN(dataset,dataset[:,:-1],3)\nprint(predictions)","42398f2e":"#lets check on iris dataset\n\ndata = pd.read_csv(\"..\/input\/iris\/Iris.csv\")\ndata.drop(\"Id\",axis=1,inplace=True)\n#converting categorical data to number\ndata['Species'] = data['Species'].astype('category').cat.codes\n\ndef train_test_split(data,ratio):\n    n = int(len(data) * ratio)\n    np.random.shuffle(data)\n    return data[:-n],data[-n:]\n\ntrain, test = train_test_split(data.values,0.2)\n\npredictions = KNN(train,test[:,:-1],4)\nactual = test[:,-1]\n\nacc = accuracy(predictions,actual)\n\nprint(f\"Accuracy of the model is: {acc}\")","75d2c431":"#let's use our same dummy dataset for illustration purpose\ndataset = [[2.771244718,1.784783929,0],\n    [1.728571309,1.169761413,0],\n    [3.678319846,2.81281357,0],\n    [3.961043357,2.61995032,0],\n    [2.999208922,2.209014212,0],\n    [7.497545867,3.162953546,1],\n    [9.00220326,3.339047188,1],\n    [7.444542326,0.476683375,1],\n    [10.12493903,3.234550982,1],\n    [6.642287351,3.319983761,1]]\n\ndataset = np.array(dataset)\n\ndef train_learned_vectors(train,n_vectors,n_epochs,alpha):\n    classes = list(set(train[:,-1]))\n    learned_vectors = list()\n    vector_per_class = int(n_vectors\/len(classes))\n    \n    #select learning vectors for each class\n    for c in classes:\n        temp = train[train[:,-1]==c]\n        #get random indexes to select from\n        indexes = np.random.choice(temp.shape[0],vector_per_class)\n        vector = temp[indexes]\n        learned_vectors.extend(vector)\n        \n    learned_vectors = np.array(learned_vectors)\n    \n    #train vectors\n    for epoch in range(n_epochs):\n        learning_rate = alpha * (1 - epoch\/n_epochs) \n        for row in train:\n            diff_sqr = (learned_vectors[:,:-1] - row[:-1])**2\n            summation = list(map(sum,diff_sqr))\n            distances = np.sqrt(summation)\n        \n            #get best matchine unit\n            index = np.argsort(distances)[0]\n            bmu = learned_vectors[index]\n        \n            #update bmu\n            if row[-1] == bmu[-1]:\n                bmu[:-1] += learning_rate * (row[:-1] - bmu[:-1])\n            else:\n                bmu[:-1] -= learning_rate * (row[:-1] - bmu[:-1])\n        learned_vectors[index] = bmu\n        \n    return learned_vectors\n\nlearned_vectors = train_learned_vectors(dataset,4,100,0.3)\n\ndef LVQ(learned_vectors,test,k):\n    predictions = list()\n    classes = list(set(train[:,-1]))\n    \n    for row in test:\n        diff_sqr = (learned_vectors[:,:-1] - row)**2\n        summation = list(map(sum,diff_sqr))\n        distances = np.sqrt(summation)\n        \n        indexes = np.argsort(distances)[:k]\n        labels = learned_vectors[indexes,-1]\n\n        values ,counts = np.unique(labels,return_counts=True)\n        prediction = values[np.argmax(counts)]\n        predictions.append(prediction)\n        \n    return predictions\n\npredictions = LVQ(learned_vectors,dataset[:,:-1],3)\nprint(predictions)","e1e9c443":"#lets check on iris dataset\n\ndata = pd.read_csv(\"..\/input\/iris\/Iris.csv\")\ndata.drop(\"Id\",axis=1,inplace=True)\n#converting categorical data to number\ndata['Species'] = data['Species'].astype('category').cat.codes\n\ndef train_test_split(data,ratio):\n    n = int(len(data) * ratio)\n    np.random.shuffle(data)\n    return data[:-n],data[-n:]\n\ntrain, test = train_test_split(data.values,0.2)\n\n\nn_vectors = int(train.shape[0]*0.3)\n\nlearned_vectors = train_learned_vectors(train,n_vectors,100,0.3)\n\npredictions = LVQ(learned_vectors,test[:,:-1],4)\nactual = test[:,-1]\n\nacc = accuracy(predictions,actual)\n\nprint(f\"Accuracy of the model is: {acc}\")","87130c90":"## Logistic Regression\n\nLogistic Regression Algorithm is the simplest binary-classification algorithm known. It's name is derived from the function used at it's core called Logistic Function \n\n![Image](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/9e26947596d387d045be3baeb72c11270a065665)\n\nThis is the Genreral logistic function.<br\/>\nFunction used in Machine Learning has value of L=1,k=1 and x0 =0.\n\nBasically logistic function has a super-power that it can convert any-number given to it between 0 to 1 we could use this power to make binary-classification.\n\nSo we will take same equation from above linear regression that is y = b0 + b1 * x and put it in the Logistic function.\n\nWe will use Stocastic Gradient Descent to reduce the error and find out values of b0 and b1. So it is clear that we are finding \nsame b0 and b1 but change here is during updating error we need to take derivative of this logistic function(remember deivative of function is it's slope).\n\nNot going into much maths the if logistic function is f(x)\nthen its derivative is f(x) * (1- f(x)) (aha that's simple)\n\nso this is how weight updation will look like.\n\nb1 += lr * error * f(x) * (1- f(x)) * x <br\/>\nb0 += lr * error * f(x) * (1- f(x))\n\nif you are wondering why there is extra x in b1 well that is called chain rule in calculus. but as we are not going into much maths I will leave that out for you to find out.\n\n\n## Algorithm steps\n\n1. Make prediction\n2. calculate error\n3. update wieghts\n4. Do that for all data for n epochs\n","d6ffd1a7":"### Testing on pima indians diabetes","4b57b1e1":"Before applying above gradient_descent to a real dataset let us make it more general for data with more than two columns.\n\nwe will use a list of coeff where first will be b0 and rest will be b1, b2...","83e0c79b":"we get exactly same coef which is good but now we can apply this algorithm to data with multiple columns","150cccb4":"## Naive Bayes Algorithm for classification\n\nNaive Bayes is simple, old yet powerfull algorithm when it comes to classification.\n\nThe algorithm is based on the Bayes theorem which says we can calculate probability of hypothysis given condition a by using some availabel information.\n\nThe formula is like this.\n\n![image](https:\/\/qph.fs.quoracdn.net\/main-qimg-b564cbdb0f1dbaa240e5d94dc172bcf1)\n\nP(A|b) is probability of event A given condition B.\nP(B|A) is probability of event B given condition A.\nP(B) is probability of event B\nP(A) is probability of event A.\n\n\nLet's see this example to understand how bayes theorem works.\n\nWe have two  machines A and B which produces same item. out of 100 items of A machine are 10 defective and out of 100  items of B machine 15 are defective.Machine A works twice as fast as B .So what is probability of item being A if it is found defective.\n\nAccording to bayes theorem.\n\nP(A| defective) = P(defective | A) * P (A) \/ P(defective)\n\nP(defective|A) = out of 100 items from machine A 10 are defective = 10 \/100\n\nP(A) is 2\/3 as it machine A works twice as fast as B so 2\/3 items are from machine A.\n\nP(defective) = probability(defective|A) + probability(defective|B) = 25\/100\n\nso P(A | defective) = 0.1 * (2\/3) \/ 0.25 = 0.267\n\nso that's the probability of item being faulty belonging to A.\n\n\nNow the question is how can we use this theorem for classification.\n\nWe need to find P(c|d) where c is class and d is row of data for every class and output with maximum probability will be the class.\n\nclass = max of P(C|d) = max(P(d|c) * P(c) \/ P(d))\n\nnow as P(d) is constant for every class.<br\/> \nwe need to find max(P(d|c) * P(c))\n\n## How naive bayes works.\n\nFirst we will see example on categorical input variable\n\nsuppose we have data with two categorical input variables X1 and X2 and a binary class 0 or 1.\n\n50% of classes are 0 and 50% are 1.\n\n1. calculate probability of each class\n\nP(0) = 0.5 and P(1) = 0.5\n\n2. calculate prabability of data given class for each class.\n\nP(x1|0) P(X1|1) P(X2|0) P(X2|1)\n\n3. Make prediction for new input d\n\n**Imp**: One most important thing that Naive Bayes assumes is that all the input data are independent of each other so P(d|0) = P(X1|0) * P(X2|0)\n(that is how probability for independent variable works)\n\nfind P(d|0) = P(x1|0) * P(x2|0) * P(0) and P(d|1) = P(x1|1) * P(x2|1) * P(1)\n\none with higher probability will give us the class.\n\nBut here the input data is categorical variable so it could take finite values which are already present in the data so we could calculate P(x1|c) and P(x2|c) easily as that data is already present in the data.\n\nBut how to find the probability of countinous data because continous data could take infinite values in certain range and so data of all the points will not be present in the dataset.\n\nFor continous data we will use something called as PDF (probability density function) which is based on the assumption that the input data follow Gaussian Distribution.\n\npdf for any input value could be calculated by using mean and standard deviation of that input column.\n\nformula of pdf is like this.\n\n![image](https:\/\/sites.nicholas.duke.edu\/statsreview\/files\/2013\/06\/normpdf1.jpg)\n\nAha! did't see that coming.\n\nHere \u03c3 is standard deviation<br\/>\n\u03bc  is mean \ne is the famous euler constant <br\/>\nand well \u03c0 is PI.\n\nso we calculate mean and std for each column from given data and use that in PDF to find P(d|c) for any new input.","2b8486a0":"## Simple Linear Regression\n\nMost simple and understood algorithm in ML is Linear regression.<br\/>\nLinear Regression is an idea from statistic. suppose we have two variable x and y which has linear relationship. \n\nso y  = b0 + b1 * x\n\nso task of linear regression is to find b0 and b1 from the given data. Then use that b0 and b1 to make prediction on unknown x. This is called Simple Linear Regression. If there are more than one input variable x we call is Linear Regression.\n\n\nb0 and b1 for simple regression can be calculated using one formula<br\/>\n\nb1 = \u2211n1-i (xi\u2212mean(x))\u00d7(yi\u2212mean(y)) \/ \u2211n 1-i (xi\u2212mean(x))** 2\n\nhere \u2211n1-i means sigma 1 to i\n\nb0 = mean(y)\u2212 b1\u00d7mean(x)\n\nand we can use this b0 and b1 to make predictions","e80c8ce3":"## All algorithms from scratch\n\nThis is a guide on various machine learning algorithms and how to write them from scratch.<br\/>\nIn this notebook we will learn about various algorithms from scratch like linear-regression , K-NN etc. <br\/>\n\n\n## Format of the notebook\n\nThis notebook will be following this steps while explaining in the algorithm<br\/>\n1. Background of algorithm\n2. Basic understanding of the process\n3. code in python\n4. Testing some of them on real dataset\n\n## Not in the Notebook\n\nThis notebook does note include mathematical derivation<br\/>\nwe are only interested in understanding how the algorithm works <br\/>\nThere will be no explanation on how algorithm is derived.\n\nand we are not looking to make any scalable or production level algorithms<br\/>\nour only goal is to understand algorithms.\n\n\n## Prerequisite.\n\nAlthough this notebook is begineer friendly this notebook requires you to <br\/>\nunderstand basic statistical concepts like, mean , median, standard deviation, probability<br\/>\nand understanding of basic linear-algebra and some simple calculus like basic derivation.\n\n## Topics in this Notebooks.\n\n### 1. Linear Algorithms\n1. Linear Regression\n2. Logistic Regression\n3. Linear Discriminant Analysis (for classification)\n\n### 2. Non-Linear Algorithms\n1. Decision Trees.\n2. Naive Bayes\n3. K- Nearest Neighbour\n4. Learning vector quantization.\n\nsvm, bagging , boosting coming soon.\n\n\n**Note:\nI test amost all of the algorithms for classification problems and<br\/>\nall algorithm assumes that last column is for label of classification.**","8502a537":"## Linear Discriminant Analysis (for classification)\n\nLogistic Regression is an amazing algorithm when we are working with binary-data, but when it comes to multiclass-classification then logistic regression will not work. There are methods where logistic regression is extended for multiclass but it's performance is not that good.\n\nSo for multiclass classification we can use another linear algorithm LDA (linear Discriminant Analysis).\n\nLinear Discriminant Analysis is classification method developed by R.A.Fisher in 1936.\n\nLDA is mostly used as dimension algotithm like PCA (Principle component Analysis) but unlike PCA it is supervised algorithm it means it takes classes into account while performing dimension reduction.\n\nLet us take an example of 2D with 2 classes data which we want to transform to 1D. What LDA does is that it finds a axis in that 2D plane such that it maximizes the distance between mean value of both the classes and minimizes the variation in the data.\n\nAssumption made by LDA.\n1. LDA assumes that each independent value has gaussian distribution.\n\n2. The variance calculated for each input variabbles by class grouping is the same.\n\n3. That the mix of classes in your training set is representative of the problem.\n\n## How does LDA classification works.\n\n1. LDA first calculates the mean of each input value x for and that for each class k.\n\nmean_c(x) = 1\/n * sum(x)<br\/>\nn = number of instances for that class.<br\/>\nc in mean_c stands for class number\n\n2. Now we will calculate the variance of each input x. We will take average of  square of difference of input variable to its mean. (remember we will use mean for given class not the mean of whole input x). so formula is\n\nvariance = 1\/(n - classes) * sigma((xi - mean_c)** 2)\n\nand now after calculating this thing we can do prediction\n\n3. For prediction LDA uses Probability it calculates what is probability what is probability of class c given input x P(Y=k|X=x). To simplify this bayes theorem is used.\n\nP(Y=c|X=x) = P(k) * P(x|c) \/ sigma_1_c(P(ci) * P(x|ci))\n\nhere sigma_1_c means sigma from 1 to c<br\/>\nwe will use above calculated mean and variance to find p(x|c).\n\nNow we will not show much maths but above probability equation could be converted to mean and variance using some complicated methods and it will give us this equation.\n\nDc(x) = x * (mean_c\/(variance)) - (mean_c** 2 \/ 2* variance** 2)+ ln(P(c))\n\nP(c) is just probability of that class in our dataset.\n\nDc(x) is the discriminate function for class c.<br>\n\n4. calculate Dc(x) for each class and highest Dc value gives us the class. \n\n**Note: above algorithm could be used for classification of the data with one independent variable.**","3fe233a9":"we got excellent accuracy with LVQ with one third data","04a7c606":"# Non-Linear Algorithms\n\n## Classification and Regression trees.\n\nClassification and Regression Trees also known as CART for short introduced by Leo Breiman for Decision Tree algorithms.\n\nDecision Tree is has a very simple representation that it could be understood by everyone. It has very intutive way of creating a model.\n\nLet's understand how a Trained Decision Tree for classification works.\n\nBascically what trained Decision Tree does is to divide each data into different different classes.\n\nsuppose we have a dataset of height and weight of 100 person and on basis of that we have to predict wether person is male of female.\n\nTrained Decision Will do something like this<br\/>\n\n![image](https:\/\/miro.medium.com\/max\/1030\/0*8V2wmrSajfg_UEKE.png)\n\n\nSo what we see here is binary decision tree.<br\/>\nAnd this data flows down the tree to reach leaf node and the value in the leaf node is our answer.\n\nDecision Tree could be stored using a tree or using steps like this.\n\nwhile not leaf node:<br\/>\nif height > 180 -> go left<br\/>\nelse height <= 180 -> go right <br\/>\nif weight > 80kg - > go left<br\/>\nelse go-> right<br\/>\n\nHere suppose we input data height==200 and weight = 75 and it will go first to right and then again right and will reach female node.\n\n## How Decision Trees are Trained.\n\nAs we saw main idea of Decision Tree is to divide data into proper classes. Here we will see how a binary DT is trained.\n\nsteps involved in training.\n1. Split the data from a point\n2. Evaluate how good split is \n3. Split again from different point\n4. keep doing for some number of splits\n5. select tree with best evaluation metrics.\n\nInorder to evaluate how good a split of data is we use something called gini index.\n\n## Gini Index Calculation \n\nGini Index calculation could be best explained with binary tree with two classes 0 and 1.\n\nIn binary tree we will have two groups left(l) and right(r).\n\nfirst we will calculate what is the proportion of each classes in each group.\n\nproportion = count(class_n)\/count(rows_in_parent_group)\n\nsuposse we have 5 rows in our data 3 with class 0 and 2 with class 1 and decision splits 2 rows with 0 and 1 with 1 to right and 1 row of 0 and 1 with 1 to left\n\ngroup_left = {\"0\": 1, \"1\":1} <br\/>\ngroup_right = {\"0\":2, \"1\":1}\n\nso proportion is calculated as<br\/>\ngroup_left_class_0 = 1\/5<br\/>\ngroup_left_class_1 = 1\/5<br\/>\ngroup_right_class_0 = 2\/5<br\/>\ngroup_right_class_1 = 1\/5\n\nNow formula of gini index for each group is.<br\/>\n\ngini_index = 1 - sum(proportions** 2)<br\/>\n\nThen it is weighted for total proportion of samples present in it.\n\ngini_index = 1 - sum(porportion** 2) * (group_size\/total_samples_in_parent_node)\n\n\ngini(group_1) = (1 - ( (1\/5)** 2 + (1\/5) ** 2 )) * 2\/5  = 0.368 <br\/>\ngini(group_2) = (1 - ( (1\/5)** 2 + (2\/5)** 2)) * 3\/5 = 0.48\n\n\ngini_node = gini(group_1) + gini(gropu_2) = 0.848\n\nAnd gini scores of various splits are calculated and one with lowes gini score is selected. gini_score of 0.0 is perfect split.","f68afbf7":"We try to classify above dataset into where first two columns are independent-variable(X) and last is our classification variable (y)\n\nAs there are two columns x1 and x2 we will have three weights \nb0 , b1, b2 instead of two.","939bb92a":"## Testing on iris dataset","9252fb35":"## Learning Vector Quantization\n\nThe problem with KNN is that KNN has to carry whole training dataset with it inorder to make predictions.\n\nLearninc Vector Quantization or LVQ solves this problem by decreasing the number of input data required to make prediction by learning from training data\n\nAnd then follows same KNN steps with this learned vectors.\n\n## How LVQ learns ?\n\nLVQ uses ANN (artificial neural network) like approch to learn vectors.\n\nsteps involved in learning\n\n1.  Select some random inputs from the train dataset such that it has similar proportion of classes.\n\n2. Iterate through each training inputs and find the nearest one using any distance from the selected random inputs. which is called best matching unit(BMU)\n\n3. Now compare label of BMU with label of training data.\n\n4. If it matches move it's value closer to that training sample and if it does not match move away.\n\n5. Do this for certain number of epochs.\n\nNow for prediction use this learned vectors as the training data of KNN.\n\n## How to update the learning vector ?\n\nIf the labels of BMU matches with training data.<br\/>\nbmui = bmui + learning_rate * (training - bmui)\n\nIf labels does not match<br\/>\nbmui = bmui - learning_rate * (training - bmui)\n\n### Update Learning rate\n\nWe also need to decrease learning rate for each epoch.\n\nlearning_rate = alpha * (1 - epoch\/maxepoch)\n\nWhere alpha is initial learning_rate","d9aca7e3":"I would consider 74% as a good acc although you should use cross validation to check robustness of your model","4b789595":"# Linear Algorithms","f6f5a84c":"## Linear Regression using Gradient Descent\n\n### What is Gradient Descent ?\n\nGradient Descent is one of the most common algorithm to reduce the error in any machine learning model.\n\nGradient Descent does this by using little calculus.\n\nLet's have a look an example to understand how gradient descent.\n\nThe task of the Gradient Descent is to minimize the error function.\n\nHow high or low the error function is depends on the weights like our coefficient in the linear regression.\n\nSo if we plot the error function with different values of weights. we will get a shape for that graph which will have lowest value at some value of weights and our goal is to reach that lowest point.\n\nNow in order to reduce the error and reach that lowest point we need to find the slope of the error function at current position with arbitary wights. A slope of graph shows in which direction we need to go inorder to reach zero.\n\nSo we calculate that slope and subtract that slope from the weights. We do not subtract whole slope value but we multiply a small value to the slope called learning and subtract that from weights.\n\n## Gradient regression for linear regression.\n\nfor Linear Regression we use error = (predicted - actual) \nand now we update b0 and b1 using below equation.\n\nb0 = b0 - learning_rate * error * slope<br\/>\nb1  = b1 - learning_rate * error * slope * x (extra x due to chain rule in calculus)\n\nhere slope is 1 because we have simple error function (predicted - actual) whose derivation will give 1.\n\n"}}