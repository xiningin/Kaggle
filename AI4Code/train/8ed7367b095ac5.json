{"cell_type":{"446ce716":"code","a13cefc6":"code","2b906b5f":"code","6570c32a":"code","d243c947":"code","476c16a3":"code","6120214d":"code","68c05f9c":"code","0a648420":"code","e951db97":"code","4b6a83ba":"code","287b469a":"code","f90086bb":"code","ceb185e0":"markdown","7b01a1a9":"markdown","794fc514":"markdown","40e4d345":"markdown","bdcd9c91":"markdown","653b1a76":"markdown","07d453a5":"markdown","e1bf54bd":"markdown","cdd23eb6":"markdown","ba5dfff9":"markdown","55184f57":"markdown","c60a42a6":"markdown","633e2983":"markdown","0c63dd66":"markdown","7a47b929":"markdown","65c89220":"markdown","f357628d":"markdown","0334d957":"markdown","76e759c8":"markdown"},"source":{"446ce716":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","a13cefc6":"train = pd.read_csv(\"\/kaggle\/input\/bcu-ratings\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/bcu-ratings\/test.csv\")\n\ntrain.head()","2b906b5f":"features = list(set(train.columns.values) - set(['ID', 'TARGET']))\ncat_features = [feat for feat in features if train[feat].dtype.name in ['category', 'object']]\nnum_features = list(set(features) - set(cat_features))\n\nprint(f\"Categorical features: \\n {cat_features}\")\nprint(f\"Numerical features: \\n {num_features}\")","6570c32a":"for feat in num_features:\n    # cuenta si hay missing en el train o en el test\n    if (np.sum(train[feat].isna() * 1) + np.sum(test[feat].isna() * 1)) > 0:\n        # calcula la mediana\n        mediana = train.loc[np.logical_not(train[feat].isna()), feat].median()\n        \n        # rellena los missing tanto del train como del test con la mediana\n        train.loc[train[feat].isna(), feat] = mediana\n        test.loc[test[feat].isna(), feat] = mediana","d243c947":"# relleno de missing\nfor feat in cat_features:\n    train.loc[train[feat].isna(), feat] = \"NA\"\n    test.loc[test[feat].isna(), feat] = \"NA\"","476c16a3":"# one-hot encoding de categ\u00f3ricas\ntest['TARGET'] = np.nan\ndata = pd.concat([train, test], axis = 0)\n\ndata = pd.get_dummies(data, drop_first = True, columns = cat_features) # el par\u00e1metro drop first elimina uno de los valores, tal y como queremos que suceda\n\n# volvemos a calcular los features, ya que el nombre de algunas columnas ha cambiado\nfeatures = list(set(data.columns.values) - set(['ID', 'TARGET']))\n\n# separamos otra vez el train del test (el test ser\u00e1n aquellas observaciones para las que el target es NA)\ntrain = data.loc[np.logical_not(data['TARGET'].isna())]\ntest = data.loc[data['TARGET'].isna()]\n\ntrain.head()","6120214d":"# partimos la muestra en train y test\nfrom sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(train[features], train['TARGET'], test_size = 0.25,\n                                                  stratify = train['TARGET'], random_state = 1234)\n\n# escalamos los features\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(X_train)\nX_train = pd.DataFrame(scaler.transform(X_train), columns = X_train.columns.values)\nX_val = pd.DataFrame(scaler.transform(X_val), columns = X_val.columns.values)\n\n# entrenamos el modelo y hacemos las prediciones\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\n\nmodel = LogisticRegression(penalty = 'elasticnet', l1_ratio = 0, C = 1, solver = 'saga', max_iter = 10000)\nmodel.fit(X_train, y_train)\npreds = [i[1] for i in model.predict_proba(X_val)]\nprint(f\"Roc auc score del modelo: {roc_auc_score(y_val, preds)}\")","68c05f9c":"from sklearn.model_selection import KFold\n\ndef cross_validation_roc_auc(modelo, X, y, number_folds):\n    # lista auxiliar para guardar el resultado de cada fold\n    metrics = []\n    \n    # reset index de X e y para poder hacer el split (necesario para hacer que los indices\n    # de X e y sean iguales)\n    X = X.reset_index(drop = True); y = y.reset_index(drop = True)\n    \n    # utilizando el objeto KFold de sklearn, creamos los splits, y para cada split\n    # entrenamos un modelo y calculamos su m\u00e9trica en el fold de validaci\u00f3n\n    for train_index, val_index in KFold(n_splits = number_folds).split(X):\n        modelo.fit(X.loc[train_index], y[train_index])\n        val_pred = [i[1] for i in modelo.predict_proba(X.loc[val_index])]\n        metrics.append(roc_auc_score(y[val_index], val_pred))\n    \n    # la funci\u00f3n devuelve la media y la d. est\u00e1ndar de los valores de CV score\n    return np.round(np.mean(metrics), 3), np.round(np.std(metrics), 4)","0a648420":"C_parametros = [0.01, 0.1, 1, 10, 100, 1000]\n\nfor c in C_parametros:\n    model = LogisticRegression(penalty = 'elasticnet', l1_ratio = 0, C = c, solver = 'saga', max_iter = 10000)\n    mean_cv, std_cv = cross_validation_roc_auc(model, X_train, y_train, number_folds = 5)\n    print(f\"C = {c} | Resultado = {mean_cv} +- {std_cv}\")","e951db97":"model = LogisticRegression(penalty = 'elasticnet', l1_ratio = 0, C = 1000, solver = 'saga', max_iter = 10000)\nmodel.fit(X_train, y_train)\npreds = [i[1] for i in model.predict_proba(X_val)]\nprint(f\"Roc auc score del modelo: {roc_auc_score(y_val, preds)}\")","4b6a83ba":"def score_function(c):\n    \n    # esto no es necesario en este caso, pero en caso de utilizar otro modelo, en el que alguno de los par\u00e1metros\n    # solamente pudiera tomar valores enteros, deber\u00edamos asegurar que el par\u00e1metro es un entero\n    c = int(c)\n    \n    # definimos el modelo con el par\u00e1metro c que hemos pasado en la funci\u00f3n\n    model = LogisticRegression(penalty = 'elasticnet', l1_ratio = 0, C = c, solver = 'saga', max_iter = 10000)\n    \n    # utilizamos la funci\u00f3n auxiliar anterior para calcular el cross-validation ROC AUC score anterior\n    score, _ = cross_validation_roc_auc(model, X_train, y_train, number_folds = 5)\n    return score\n    ","287b469a":"from bayes_opt import BayesianOptimization\n\nbounds = {'c': (1, 1000)}\n\noptimizer = BayesianOptimization(f = score_function, pbounds = bounds, random_state = 1234)\noptimizer.maximize(init_points = 10, n_iter = 20)","f90086bb":"model = LogisticRegression(penalty = 'elasticnet', l1_ratio = 0, C = 1, solver = 'saga', max_iter = 10000)\nmodel.fit(X_train, y_train)\n\n# escalado y predicci\u00f3n\ntest_scaled = pd.DataFrame(scaler.transform(test[features]), columns = test[features].columns.values)\ntest_pred = model.predict_proba(test_scaled)\ntest_pred = [i[1] for i in test_pred]\n\nsubmission = pd.DataFrame({'ID': test['ID'], 'Pred': test_pred}).to_csv(\"reg_logistic_regression.csv\", index = False)","ceb185e0":"Con la funci\u00f3n para obtener el ROC AUC mediante *cross-validation*, podemos entrenar distintos modelos y calcular su score para quedarnos con los par\u00e1metros que mejor resultado nos den.","7b01a1a9":"Vale, parece que los modelos regularizados tienden a comportarse mejor que los \"comunes\", pero,  \u00bftiene esto alg\u00fan coste? Pues s\u00ed, ya que hemos introducido un par\u00e1metro \"lambda\", para el que hay que obtener un valor \u00f3ptimo, y cuyo valor \u00f3ptimo varia seg\u00fan los datos que se est\u00e9n modelizando.\n\nEste lambda controla la intensidad de la regularizaci\u00f3n: un lambda muy bajo da m\u00e1s peso a que el modelo se ajuste mejor a la muestra de train a costa de posible overfitting, mientras que un lambda muy alto penaliza mucho tanto a los features con coeficientes m\u00e1s altos como a introducir nuevos features (reduciendo la posibilidad de overfitting), a costa de ajustar peor la muestra de train. Como se ha dicho antes, se debe encontrar el lambda que mejor balancea estos dos efectos y que da una mejor predicci\u00f3n en las muestras de validaci\u00f3n y test.","794fc514":"## Importado de datos\n\nEn primer lugar, importamos los datos tal y como lo hicimos en el ejercicio anterior","40e4d345":"## Submission del modelo con el mejor par\u00e1metro\n\nUna vez tenemos el modelo con el mejor par\u00e1metro, podemos hacer una submission. Tal y como hemos visto, para estos datos el proceso de optimzaci\u00f3n de par\u00e1metros no aporta demasiado (las CV score en cada *fold* var\u00edan de comportamiento seg\u00fan el par\u00e1metro), por lo que podemos quedarnos con los par\u00e1metros iniciales","bdcd9c91":"## Ejercicio (para intentar individualmente)\n\nModificar el c\u00f3digo anterior para optimizar de manera conjunta el par\u00e1metro `C`y el par\u00e1metro `l1_ratio` (que aqu\u00ed hemos dejado constante como 0). De esta forma, se puede jugar con distintas proporciones de penalizaci\u00f3n L1 y L2 y ver si de esta forma se mejora el performance del modelo.","653b1a76":"## Tratamiento de missing y variables categ\u00f3ricas\n\nTal y como se ha hecho en el anterior ejercicio:\n\n1. Rellenamos los missing values de las variables num\u00e9ricas y categ\u00f3ricas\n2. Transformamos las variables categ\u00f3ricas en num\u00e9ricas para poder modelizar","07d453a5":"Tal y como hicimos la vez anterior, rellenaremos los missing de los features num\u00e9ricos con la mediana (la mediana del train, no del dataset completo!!)","e1bf54bd":"Una vez definida la funci\u00f3n, utilizamos el objeto BayesianOptimization del package bayes_opt para obtener el valor que maximiza esta funci\u00f3n. Este objeto necesita como par\u00e1metro `f` la funci\u00f3n a maximizar \/ minimizar, y como par\u00e1metro `pbouds` un diccionario donde la *key* sea el nombre del par\u00e1metro de la funci\u00f3n, y el valor un *tuple* con el valor m\u00ednimo y m\u00e1ximo que puede tomar el par\u00e1metro.\n\nPor \u00faltimo, llamamos al m\u00e9todo maximize o minimize seg\u00fan corresponda, pasando como par\u00e1metros el n\u00famero de puntos aleatorios a explorar al principio del algoritmo, y el n\u00famero total de iteraciones. El resultado es una tabla con el valor del par\u00e1metro y el valor del score para dicho par\u00e1metro.","cdd23eb6":"## Optimizaci\u00f3n del modelo\n\nPara optimizar el modelo, vamos a utilixar dos procedimientos:\n1. **Grid Search**: consiste en establecer una lista de valores para los distintos par\u00e1metros (grid), y entrenar un modelo para cada valor del par\u00e1metro. Se compara el performance de cada modelo utilizando *cross-validation* y se selecciona el par\u00e1metro que da el modelo con mejor performance. Es un procedimiento relativamente manual\n2. **Bayesian Optimization**: procedimiento relativamente autom\u00e1tico, que dado una funci\u00f3n objetivo (como puede ser el score de *corss-validation*) devuelve el par\u00e1metro o par\u00e1metros que maximizan o minimizan dicha funci\u00f3n","ba5dfff9":"Rellenamos los missing de las categ\u00f3ricas con una nueva categor\u00eda \"NA\", y hacemos one-hot encoding de las categ\u00f3ricas para pasarlas a num\u00e9ricas","55184f57":"### Grid Search\n\nEl procedimiento para optimizar par\u00e1metros utilizando Grid Search se puede resumir en:\n1. Establecer un listado de par\u00e1metros que se quieren probar\n2. Entrenar un modelo para cada par\u00e1metro y obtener un valor del performance del modelo (por ejemplo, *cross-validation score*)\n3. Seleccionar el modelo con mejor performance\n\nPara realizar este procedimiento, primero vamos a crear una funci\u00f3n auxiliar que nos devuelva el valor de la m\u00e9trica ROC AUC del modelo con *cross-validation*. En este caso, queremos que nos devuelva tanto la media del ROC AUC por fold como la desviaci\u00f3n est\u00e1ndar","c60a42a6":"Tambi\u00e9n podemos comprobar el performance del modelo con los nuevos par\u00e1metros contra la muestra de validaci\u00f3n anterior. En este caso, comprobar\u00edamos el performance del modelo con un C = 1000, ya que es el mejor resultado de *cross-validation* (mayor media y menor desviaci\u00f3n est\u00e1ndar)","633e2983":"Y guardamos en variables el listado de features num\u00e9ricos y categ\u00f3ricos","0c63dd66":"## Modelizaci\u00f3n\n\nUna vez tratados los datos, podemos empezar a modelizar. El objetivo de este ejercicio es utilizar modelos un poco m\u00e1s complejos, por lo que esta vez no vamos a utilizar una simple Regresi\u00f3n Log\u00edstica, si no que introduciremos el concepto de regularizaci\u00f3n.\n\n\u00bfQu\u00e9 es la regularizaci\u00f3n? La regresi\u00f3n regularizada b\u00e1sicamente soluciona dos problemas de la regresi\u00f3n \"com\u00fan\": que el modelo dependa excesivamente de unos pocos features y que el hecho de a\u00f1adir nuevos features casi siempre mejore el modelo en la muestra de train (lo que puede llevar a overfitting y que el modelo se comporte peor en la muestra de test).\n\n\u00bfC\u00f3mo se consigue esto? Pues introduciendo una penalizaci\u00f3n a la funci\u00f3n de coste que el modelo intenta minimizar. La penalizaci\u00f3n suele ser un factor \"lambda\" multiplicado por la suma del valor absoluto de los coeficientes (en este caso, se suele hablar de penalizaci\u00f3n L1) o la suma del cuadrado del valor de los coeficientes (este caso se conoce como penalizaci\u00f3n L2). As\u00ed, si alguno de los coeficientes es \"demasiado\" elevado o se introducen nuevos features que no aportan demasiado, la penalizaci\u00f3n aumenta y perjudica al modelo.\n\nEste art\u00edculo explica con m\u00e1s detalle los conceptos de regularizaci\u00f3n y el tipo de penalizaciones: https:\/\/towardsdatascience.com\/the-basics-logistic-regression-and-regularization-828b0d2d206c","7a47b929":"\u00bf\u00bfQu\u00e9?? \u00bfEl par\u00e1metro con mejor score de CV empeora la muestra de validaci\u00f3n? Si se analiza en m\u00e1s detalle, y en vez de seguir \u00fanicamente la media y la std de la puntuaci\u00f3n de CV se analiza la puntuaci\u00f3n individual en cada *fold* de CV, se ve que el comportamiento de cada *fold* para cada par\u00e1metro es distinto (mientras que para unos par\u00e1metros el performance en unas *folds* sube, en otras baja y en otros se mantiene m\u00e1s o menos estable). Esto suele significar que, **en este caso**, el performance global del modelo no depende mucho del par\u00e1metro elegido, y que vamos a conseguir poca o nula mejora del modelo optimizando par\u00e1metros.\n\nSin embargo, no todo son malas noticias. Un performance de este tipo puede indicar que hay \"algo\" en los datos: por ejemplo, puede que haya comportamientos distintos en empresas, que haya tipos de empresas distintas en la base de datos, etc. Esto puede suponer una buena oportunidad para explorar en m\u00e1s detalle los datos y crear nuevas variables que mejoren el comportamiento del modelo.","65c89220":"## Objetivos del ejercicio\n\nLos objetivos de este ejercicio son los siguientes:\n\n1. Utilizar modelos m\u00e1s complejos y, en general, con mejor performance\n2. Entender el concepto de hyperpar\u00e1metros de un modelo\n3. Utilizar distintos m\u00e9todos para optimizar los hyperpar\u00e1metros de un modelo","f357628d":"### Bayesian Optimization\n\nEl aproach para optimizar los par\u00e1metros de un modelo utilizando Bayesian Optimization es ligeramente diferente. En vez de definir manualmente el listado de par\u00e1metros que queremos probar, dejaremos que sea el algoritmo el que decida qu\u00e9 par\u00e1metros son los \u00f3ptimos para ir probando.\n\nEl proceso de Bayesian Optimization funciona de la siguiente manera:\n1. Se define una funci\u00f3n que toma como input uno o varios par\u00e1metros de modelo, y devuelve como output un valor. Esta funci\u00f3n es la funci\u00f3n para la que queremos obtener un valor m\u00e1ximo o m\u00ednimo. En nuestro caso, podemos definir una funci\u00f3n que tome como input el par\u00e1metro `C` y devuelva como output el valor de `cross-validation ROC AUC` del modelo para ese par\u00e1metro. En este caso, querr\u00edamos obtener el par\u00e1metro `C` que maximiza esta funci\u00f3n. En la figura siguiente, esta funci\u00f3n ser\u00eda la funci\u00f3n de la l\u00ednea naranja discontinua\n2. El algoritmo de Bayesian Optimization elije algunos puntos a evaluar de manera aleatoria, para tener unos valores iniciales. El n\u00famero de puntos aleatorios a evaluar inicialmente es un par\u00e1metro que podemos controlar. En la figura, los puntos ya observados se representan por los puntos negros\n3. En base a los puntos ya evaluados, el algoritmo de Bayesian Optimization calcula una aproximaci\u00f3n a la funci\u00f3n definida en 1. Esta aproximaci\u00f3n calcula tanto el valor esperado de la funci\u00f3n (representado por la l\u00ednea continua azul) como la incertidumbre que existe en cuanto al valor esperado (representado por la zona sombreada en azul). Este proceso es autom\u00e1tico\n4. Se elije el siguiente punto a evaluar en funci\u00f3n de la aproximaci\u00f3n a la funci\u00f3n calculada en el punto 3. En este caso, los dos mejores candidatos a tener un m\u00e1ximo de la funci\u00f3n son los puntos aprox. 0,2 y 0,6 (los puntos donde la zona sombreada alcanza un m\u00e1ximo global). En este caso, el siguiente punto a evaluar ser\u00eda uno de estos dos (por la figura no se puede apreciar, pero el punto a evaluar ser\u00eda aquel en el que la zona sombreada alcance un valor superior)\n5. Se repiten los pasos 3 y 4 un n\u00famero n de veces\n6. Despu\u00e9s de n iteraciones, se elige como \u00f3ptimo el punto evaluado donde la funci\u00f3n haya alcanzado un m\u00e1ximo (o m\u00ednimo, si se intenta minimizar una funci\u00f3n de error)\n\n![imagen.png](attachment:imagen.png)","0334d957":"### Regresi\u00f3n log\u00edstica regularizada en sklearn\n\nPara realizar la regresi\u00f3n log\u00edstica regularizada utilizaremos el objeto `LogisticRegression` de sklearn, que permite aplicar tanto regularizaci\u00f3n L1 como L2. Es \u00fatil consultar la documentaci\u00f3n del objeto `LogisticRegression` (https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression) para saber qu\u00e9 par\u00e1metros y qu\u00e9 valores introducir al objeto para implementar el tipo de modelo que se quiere.\n\nPor ejemplo, en este caso debemos pasar los par\u00e1metros *C* (C = 1\/lambda) y *l1_ratio*, que controlan la intensidad y el tipo de regularizaci\u00f3n aplicada. En nuestro caso, *l1_ratio* ser\u00e1 igual a 0 (ya que queremos penalizaci\u00f3n L2) y *C* ser\u00e1 el par\u00e1metro a optimizar.\n\n**Nota**: por el tipo de procedimiento num\u00e9rico que se usa para entrenar el modelo, es necesario escalar los features, o el algoritmo num\u00e9rico no llegar\u00e1 a una buena soluci\u00f3n. En este caso, podemos escalar los features entre 0 y 1 con el objeto `MinMaxScaler` (https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.MinMaxScaler.html)","76e759c8":"Ok, vista la teor\u00eda, vamos a implementar el algoritmo. En primer lugar, vamos a implementar la funci\u00f3n que queremos optimizar. En nuestro caso, queremos maximizar el `cross-validation ROC AUC` score."}}