{"cell_type":{"d43be5b2":"code","b1a8ec23":"code","502979a6":"code","8b81f610":"code","ebe15288":"code","df91045e":"code","d84ed107":"code","e92e4958":"code","d02108aa":"code","c66db403":"code","f20ce272":"code","56104a36":"code","6d80ec61":"code","9c0787b8":"code","e5ccbca2":"code","58d5a4a9":"code","900879f6":"code","f8b5dedd":"code","de434206":"code","30d10596":"code","3846d07e":"code","1a1d35ef":"code","7797d6b8":"code","83e4232c":"code","7ae45671":"code","d2b58d78":"code","843713a4":"code","6cdeb929":"code","7e0fcf42":"code","3003e264":"code","3e512786":"code","0e9788b7":"code","7dbd4861":"code","b9562cb6":"code","bd5315e3":"code","cbbc31b2":"code","6038b299":"code","f8b6cb02":"code","7a85c6c2":"code","455f7e56":"code","082608f9":"code","abbb555a":"code","eca1c50c":"code","fdced321":"code","5e86ef2d":"code","614d7e8d":"code","2d58352d":"code","4180594d":"code","71db3067":"code","3d4b8eef":"code","7d7d15ef":"code","15c239c3":"code","b4a9b77c":"code","03342960":"code","1c681648":"code","79d1d9ab":"code","44a2423b":"code","e17eef4a":"code","1eab6d3e":"code","62c55996":"code","9042b8ea":"code","61e8d13c":"code","af9b26b8":"code","1f24980d":"code","307fe620":"code","fd099d19":"code","246c629f":"code","bdbe04f1":"code","2cab831e":"code","03ce9056":"code","8fc11901":"code","24a79609":"code","4908397a":"code","bc69ba2b":"code","4e4f0ab4":"code","b31e1058":"code","f0c20d8c":"code","390a4661":"code","608bedae":"code","cf19eb4b":"code","fbe0a649":"code","377a3ee4":"code","21f280f0":"code","68a059f2":"code","950939ed":"code","44b69b2d":"markdown","64c69dd7":"markdown","7c4c783c":"markdown","c276eede":"markdown","6e04d4e7":"markdown","43c8a37d":"markdown","0fe01a67":"markdown","f60b7592":"markdown","658b3bb0":"markdown","9ff18a01":"markdown","577a7201":"markdown","4d1a5451":"markdown","465148d5":"markdown","7465111c":"markdown","e0d4aa65":"markdown","5a8d0db1":"markdown","2ce0700f":"markdown","f30b936d":"markdown","689fdd5d":"markdown","7dcde00c":"markdown","e2deacf4":"markdown","41284c8b":"markdown","2ed2c6ab":"markdown","380a97eb":"markdown","83a14c66":"markdown","a60b9bd8":"markdown","963de0a1":"markdown","1b5770d6":"markdown","aba018aa":"markdown","d029445c":"markdown","81e4c27a":"markdown","3157a0f8":"markdown","8b8e2096":"markdown","8204d80f":"markdown","64e77697":"markdown","920c2258":"markdown","f8098c9d":"markdown","45c34bdd":"markdown","cc96be0b":"markdown"},"source":{"d43be5b2":"import os\nimport gc\nimport sys\nimport random\nimport logging\nimport datetime\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport lightgbm as lgb\nfrom plotly import tools\nfrom pathlib import Path\nimport plotly.graph_objs as go\nimport matplotlib.pyplot as plt\nimport plotly.figure_factory as ff\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)","b1a8ec23":"IS_LOCAL = False\nif(IS_LOCAL):\n    PATH=\"..\/input\/elo\/\"\nelse:\n    PATH=\"..\/input\/\"\nos.listdir(PATH)","502979a6":"train_df=pd.read_csv(PATH+'train.csv')\ntest_df=pd.read_csv(PATH+'test.csv')\nhistorical_trans_df=pd.read_csv(PATH+'historical_transactions.csv')\nnew_merchant_trans_df=pd.read_csv(PATH+'new_merchant_transactions.csv')\nmerchant_df=pd.read_csv(PATH+'merchants.csv')","8b81f610":"print(\"Train: rows:{} cols:{}\".format(train_df.shape[0], train_df.shape[1]))\nprint(\"Test:  rows:{} cols:{}\".format(test_df.shape[0], test_df.shape[1]))\nprint(\"Historical trans: rows:{} cols:{}\".format(historical_trans_df.shape[0], historical_trans_df.shape[1]))\nprint(\"New merchant trans:  rows:{} cols:{}\".format(new_merchant_trans_df.shape[0], new_merchant_trans_df.shape[1]))\nprint(\"Merchants: rows:{} cols:{}\".format(merchant_df.shape[0], merchant_df.shape[1]))","ebe15288":"train_df.sample(3).head()","df91045e":"test_df.sample(3).head()","d84ed107":"historical_trans_df.sample(3).head()","e92e4958":"new_merchant_trans_df.sample(3).head()","d02108aa":"merchant_df.sample(3).head()","c66db403":"def missing_data(data):\n    total = data.isnull().sum().sort_values(ascending = False)\n    percent = (data.isnull().sum()\/data.isnull().count()*100).sort_values(ascending = False)\n    return pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])","f20ce272":"missing_data(train_df)","56104a36":"missing_data(test_df)","6d80ec61":"missing_data(historical_trans_df)","9c0787b8":"missing_data(new_merchant_trans_df)","e5ccbca2":"missing_data(merchant_df)","58d5a4a9":"def get_categories(data, val):\n    tmp = data[val].value_counts()\n    return pd.DataFrame(data={'Number': tmp.values}, index=tmp.index).reset_index()","900879f6":"def get_target_categories(data, val):\n    tmp = data.groupby('target')[val].value_counts()\n    return pd.DataFrame(data={'Number': tmp.values}, index=tmp.index).reset_index()","f8b5dedd":"def draw_trace_bar(data_df,color='Blue'):\n    trace = go.Bar(\n            x = data_df['index'],\n            y = data_df['Number'],\n            marker=dict(color=color),\n            text=data_df['index']\n        )\n    return trace\n\ndef draw_trace_histogram(data_df,target,color='Blue'):\n    trace = go.Histogram(\n            y = data_df[target],\n            marker=dict(color=color)\n        )\n    return trace","de434206":"def plot_bar(data_df, title, xlab, ylab,color='Blue'):\n    trace = draw_trace_bar(data_df, color)\n    data = [trace]\n    layout = dict(title = title,\n              xaxis = dict(title = xlab, showticklabels=True, tickangle=0,\n                          tickfont=dict(\n                            size=10,\n                            color='black'),), \n              yaxis = dict(title = ylab),\n              hovermode = 'closest'\n             )\n    fig = dict(data = data, layout = layout)\n    iplot(fig, filename='draw_trace')","30d10596":"def plot_two_bar(data_df1, data_df2, title1, title2, xlab, ylab):\n    trace1 = draw_trace_bar(data_df1, color='Blue')\n    trace2 = draw_trace_bar(data_df2, color='Lightblue')\n    \n    fig = tools.make_subplots(rows=1,cols=2, subplot_titles=(title1,title2))\n    fig.append_trace(trace1,1,1)\n    fig.append_trace(trace2,1,2)\n    \n    fig['layout']['xaxis'].update(title = xlab)\n    fig['layout']['xaxis2'].update(title = xlab)\n    fig['layout']['yaxis'].update(title = ylab)\n    fig['layout']['yaxis2'].update(title = ylab)\n    fig['layout'].update(showlegend=False)\n    \n    iplot(fig, filename='draw_trace')","3846d07e":"def plot_target_distribution(var):\n    hist_data = []\n    varall = list(train_df.groupby([var])[var].nunique().index)\n    for i, varcrt in enumerate(varall):\n        classcrt = train_df[train_df[var] == varcrt]['target']\n        hist_data.append(classcrt)\n    fig = ff.create_distplot(hist_data, varall, show_hist=False, show_rug=False)\n    fig['layout'].update(title='Target variable density plot group by {}'.format(var), xaxis=dict(title='Target'))\n    iplot(fig, filename='dist_only')","1a1d35ef":"plot_two_bar(get_categories(train_df,'feature_1'), get_categories(test_df,'feature_1'), \n             'Train data', 'Test data',\n             'Feature 1', 'Number of records')","7797d6b8":"plot_target_distribution('feature_1')","83e4232c":"plot_two_bar(get_categories(train_df,'feature_2'), get_categories(test_df,'feature_2'), \n             'Train data', 'Test data',\n             'Feature 2', 'Number of records')","7ae45671":"plot_target_distribution('feature_2')","d2b58d78":"plot_two_bar(get_categories(train_df,'feature_3'), get_categories(test_df,'feature_3'), \n             'Train data', 'Test data',\n             'Feature 3', 'Number of records')","843713a4":"plot_target_distribution('feature_3')","6cdeb929":"plot_two_bar(get_categories(train_df,'first_active_month'), get_categories(test_df,'first_active_month'), \n             'Train data', 'Test data',\n             'First active month', 'Number of records')","7e0fcf42":"plot_bar(get_categories(historical_trans_df,'category_1'), \n             'Category 1 distribution', 'Category 1', 'Number of records')","3003e264":"plot_bar(get_categories(historical_trans_df,'category_2'), \n             'Category 2 distribution', 'Category 2', 'Number of records','red')","3e512786":"plot_bar(get_categories(historical_trans_df,'category_3'), \n             'Category 3 distribution', 'Category 3', 'Number of records','magenta')","0e9788b7":"plot_bar(get_categories(historical_trans_df,'city_id'), \n             'City ID distribution', 'City ID', 'Number of records','lightblue')","7dbd4861":"plot_bar(get_categories(historical_trans_df,'merchant_category_id'), \n             'Merchant Cateogory ID distribution', 'Merchant Category ID', 'Number of records','lightgreen')","b9562cb6":"plot_bar(get_categories(historical_trans_df,'state_id'), \n             'State ID distribution', 'State ID', 'Number of records','brown')","bd5315e3":"plot_bar(get_categories(historical_trans_df,'subsector_id'), \n             'Subsector ID distribution', 'Subsector ID', 'Number of records','orange')","cbbc31b2":"historical_trans_df['purchase_date'] = pd.to_datetime(historical_trans_df['purchase_date'])\nhistorical_trans_df['month'] = historical_trans_df['purchase_date'].dt.month\nhistorical_trans_df['dayofweek'] = historical_trans_df['purchase_date'].dt.dayofweek\nhistorical_trans_df['weekofyear'] = historical_trans_df['purchase_date'].dt.weekofyear","6038b299":"def plot_scatter_data(data, xtitle, ytitle, title, color='blue'):\n    trace = go.Scatter(\n        x = data.index,\n        y = data.values,\n        name=ytitle,\n        marker=dict(\n            color=color,\n        ),\n        mode='lines+markers'\n    )\n    data = [trace]\n    layout = dict(title = title,\n              xaxis = dict(title = xtitle), yaxis = dict(title = ytitle),\n             )\n    fig = dict(data=data, layout=layout)\n    iplot(fig, filename='lines')","f8b6cb02":"count_all = historical_trans_df.groupby('dayofweek')['purchase_amount'].agg(['sum'])\ncount_all.columns = [\"Total\"]\ncount_all = count_all.sort_index()\nplot_scatter_data(count_all['Total'],'Day of week', 'Total','Total sum of purchase per day of week','green')","7a85c6c2":"count_all = historical_trans_df.groupby('weekofyear')['purchase_amount'].agg(['sum'])\ncount_all.columns = [\"Total\"]\ncount_all = count_all.sort_index()\nplot_scatter_data(count_all['Total'],'Week of year', 'Total','Total sum of purchase per Week of Year','red')","455f7e56":"count_all = historical_trans_df.groupby('month')['purchase_amount'].agg(['sum'])\ncount_all.columns = [\"Total\"]\ncount_all = count_all.sort_index()\nplot_scatter_data(count_all['Total'],'Month', 'Total','Total sum of purchase per month','blue')","082608f9":"plot_bar(get_categories(new_merchant_trans_df,'category_1'), \n             'Category 1 distribution', 'Category 1', 'Number of records','gold')","abbb555a":"plot_bar(get_categories(new_merchant_trans_df,'category_2'), \n             'Category 2 distribution', 'Category 2', 'Number of records','tomato')","eca1c50c":"plot_bar(get_categories(new_merchant_trans_df,'category_3'), \n             'Category 3 distribution', 'Category 3', 'Number of records','magenta')","fdced321":"plot_bar(get_categories(new_merchant_trans_df,'city_id'), \n             'City ID distribution', 'City ID', 'Number of records','brown')","5e86ef2d":"plot_bar(get_categories(new_merchant_trans_df,'merchant_category_id'), \n             'Merchant category ID distribution', 'Merchant category ID', 'Number of records','green')","614d7e8d":"plot_bar(get_categories(new_merchant_trans_df,'state_id'), \n             'State ID distribution', 'State ID', 'Number of records','darkblue')","2d58352d":"plot_bar(get_categories(new_merchant_trans_df,'subsector_id'), \n             'Subsector ID distribution', 'Subsector ID', 'Number of records','darkgreen')","4180594d":"new_merchant_trans_df['purchase_date'] = pd.to_datetime(new_merchant_trans_df['purchase_date'])\nnew_merchant_trans_df['month'] = new_merchant_trans_df['purchase_date'].dt.month\nnew_merchant_trans_df['dayofweek'] = new_merchant_trans_df['purchase_date'].dt.dayofweek\nnew_merchant_trans_df['weekofyear'] = new_merchant_trans_df['purchase_date'].dt.weekofyear","71db3067":"count_all = new_merchant_trans_df.groupby('month')['purchase_amount'].agg(['sum'])\ncount_all.columns = [\"Total\"]\ncount_all = count_all.sort_index()\nplot_scatter_data(count_all['Total'],'Month', 'Total','Total sum of purchase per month','red')","3d4b8eef":"count_all = new_merchant_trans_df.groupby('dayofweek')['purchase_amount'].agg(['sum'])\ncount_all.columns = [\"Total\"]\ncount_all = count_all.sort_index()\nplot_scatter_data(count_all['Total'],'Day of week', 'Total','Total sum of purchase per day of week','magenta')","7d7d15ef":"count_all = new_merchant_trans_df.groupby('weekofyear')['purchase_amount'].agg(['sum'])\ncount_all.columns = [\"Total\"]\ncount_all = count_all.sort_index()\nplot_scatter_data(count_all['Total'],'Week of year', 'Total','Total sum of purchase per week of year','darkblue')","15c239c3":"def plot_purchase_amount_distribution(data_df, var):\n    hist_data = []\n    varall = list(data_df.groupby([var])[var].nunique().index)\n    for i, varcrt in enumerate(varall):\n        classcrt = np.log(data_df[data_df[var] == varcrt]['purchase_amount'] + 1)\n        hist_data.append(classcrt)\n    fig = ff.create_distplot(hist_data, varall, show_hist=False, show_rug=False)\n    fig['layout'].update(title='Purchase amount (log) variable density plot group by {}'.format(var), xaxis=dict(title='log(purchase_amount + 1)'))\n    iplot(fig, filename='dist_only')","b4a9b77c":"plot_purchase_amount_distribution(new_merchant_trans_df,'category_1')","03342960":"plot_purchase_amount_distribution(new_merchant_trans_df,'category_2')","1c681648":"plot_purchase_amount_distribution(new_merchant_trans_df,'category_3')","79d1d9ab":"plot_purchase_amount_distribution(new_merchant_trans_df,'state_id')","44a2423b":"merchant_df.head(3)","e17eef4a":"plot_bar(get_categories(merchant_df,'merchant_category_id'), \n             'Merchant category ID distribution', 'Merchant category ID', 'Number of records','darkblue')","1eab6d3e":"plot_bar(get_categories(merchant_df,'subsector_id'), \n             'Subsector ID distribution', 'Subsector ID', 'Number of records','blue')","62c55996":"plot_bar(get_categories(merchant_df,'category_1'), \n             'Category 1 distribution', 'Category 1', 'Number of records','lightblue')","9042b8ea":"plot_bar(get_categories(merchant_df,'category_2'), \n             'Category 2 distribution', 'Category 2', 'Number of records','lightgreen')","61e8d13c":"plot_bar(get_categories(merchant_df,'category_4'), \n             'Category 4 distribution', 'Category 4', 'Number of records','tomato')","af9b26b8":"plot_bar(get_categories(merchant_df,'most_recent_sales_range'), \n             'Most recent sales range distribution', 'Most recent sales range', 'Number of records','red')","1f24980d":"plot_bar(get_categories(merchant_df,'most_recent_purchases_range'), \n             'Most recent sales purchases distribution', 'Most recent purchases range', 'Number of records','magenta')","307fe620":"plot_bar(get_categories(merchant_df,'city_id'), \n             'City ID distribution', 'City ID', 'Number of records','brown')","fd099d19":"plot_bar(get_categories(merchant_df,'state_id'), \n             'State ID distribution', 'State ID', 'Number of records','orange')","246c629f":"def plot_distribution(df,feature,color):\n    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,6))\n    s = sns.boxplot(ax = ax1, data = df[feature].dropna(),color=color,showfliers=True)\n    s.set_title(\"Distribution of %s (with outliers)\" % feature)\n    s = sns.boxplot(ax = ax2, data = df[feature].dropna(),color=color,showfliers=False)\n    s.set_title(\"Distribution of %s (no outliers)\" % feature)\n    plt.show()   ","bdbe04f1":"plot_distribution(merchant_df, \"numerical_1\", \"blue\")","2cab831e":"plot_distribution(merchant_df, \"numerical_2\", \"green\")","03ce9056":"plot_distribution(merchant_df, \"avg_sales_lag3\", \"blue\")","8fc11901":"plot_distribution(merchant_df, \"avg_sales_lag6\", \"green\")","24a79609":"plot_distribution(merchant_df, \"avg_sales_lag12\", \"green\")","4908397a":"def get_logger():\n    FORMAT = '[%(levelname)s]%(asctime)s:%(name)s:%(message)s'\n    logging.basicConfig(format=FORMAT)\n    logger = logging.getLogger('main')\n    logger.setLevel(logging.DEBUG)\n    return logger","bc69ba2b":"# reduce memory\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n\n    return df","4e4f0ab4":"logger = get_logger()\n#process NAs\nlogger.info('Start processing NAs')\n#process NA2 for transactions\nfor df in [historical_trans_df, new_merchant_trans_df]:\n    df['category_2'].fillna(1.0,inplace=True)\n    df['category_3'].fillna('A',inplace=True)\n    df['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)\n    df['installments'].replace(-1, np.nan,inplace=True)\n    df['installments'].replace(999, np.nan,inplace=True)\n#define function for aggregation\ndef create_new_columns(name,aggs):\n    return [name + '_' + k + '_' + agg for k in aggs.keys() for agg in aggs[k]]","b31e1058":"logger.info('process historical and new merchant datasets')\nfor df in [historical_trans_df, new_merchant_trans_df]:\n    df['purchase_date'] = pd.to_datetime(df['purchase_date'])\n    df['year'] = df['purchase_date'].dt.year\n    df['weekofyear'] = df['purchase_date'].dt.weekofyear\n    df['month'] = df['purchase_date'].dt.month\n    df['dayofweek'] = df['purchase_date'].dt.dayofweek\n    df['weekend'] = (df.purchase_date.dt.weekday >=5).astype(int)\n    df['hour'] = df['purchase_date'].dt.hour\n    df['authorized_flag'] = df['authorized_flag'].map({'Y':1, 'N':0})\n    df['category_1'] = df['category_1'].map({'Y':1, 'N':0}) \n    df['category_3'] = df['category_3'].map({'A':0, 'B':1, 'C':2}) \n    df['month_diff'] = ((datetime.datetime.today() - df['purchase_date']).dt.days)\/\/30\n    df['month_diff'] += df['month_lag']\nlogger.info('new features historical and new merchant datasets')\nfor df in [historical_trans_df, new_merchant_trans_df]:\n    df['price'] = df['purchase_amount'] \/ df['installments']\n    df['Christmas_Day_2017']=(pd.to_datetime('2017-12-25')-df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n    df['Children_day_2017']=(pd.to_datetime('2017-10-12')-df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n    df['Black_Friday_2017']=(pd.to_datetime('2017-11-24') - df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n    df['Mothers_Day_2018']=(pd.to_datetime('2018-05-13')-df['purchase_date']).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n    df['month_diff'] = ((datetime.datetime.today() - df['purchase_date']).dt.days)\/\/30\n    df['month_diff'] += df['month_lag']\n    df['duration'] = df['purchase_amount']*df['month_diff']\n    df['amount_month_ratio'] = df['purchase_amount']\/df['month_diff']\n","f0c20d8c":"logger.info('reduce memory usage for historical trans')\nhistorical_trans_df = reduce_mem_usage(historical_trans_df)\nlogger.info('reduce memory usage for new merchant trans')\nnew_merchant_trans_df = reduce_mem_usage(new_merchant_trans_df)","390a4661":"#define aggregations with historical_trans_df\nlogger.info('Aggregate historical trans')\naggs = {}\n\nfor col in ['subsector_id','merchant_id','merchant_category_id', 'state_id', 'city_id']:\n    aggs[col] = ['nunique']\nfor col in ['month', 'hour', 'weekofyear', 'dayofweek']:\n    aggs[col] = ['nunique', 'mean', 'min', 'max']\n    \naggs['purchase_amount'] = ['sum','max','min','mean','var', 'std']\naggs['installments'] = ['sum','max','min','mean','var', 'std']\naggs['purchase_date'] = ['max','min', 'nunique']\naggs['month_lag'] = ['max','min','mean','var','nunique']\naggs['month_diff'] = ['mean', 'min', 'max', 'var','nunique']\naggs['authorized_flag'] = ['sum', 'mean', 'nunique']\naggs['weekend'] = ['sum', 'mean', 'nunique']\naggs['year'] = ['nunique', 'mean']\naggs['category_1'] = ['sum', 'mean', 'min', 'max', 'nunique', 'std']\naggs['category_2'] = ['sum', 'mean', 'min', 'nunique', 'std']\naggs['category_3'] = ['sum', 'mean', 'min', 'nunique', 'std']\naggs['card_id'] = ['size', 'count']\naggs['Christmas_Day_2017'] = ['mean']\naggs['Children_day_2017'] = ['mean']\naggs['Black_Friday_2017'] = ['mean']\naggs['Mothers_Day_2018'] = ['mean']\n\nfor col in ['category_2','category_3']:\n    historical_trans_df[col+'_mean'] = historical_trans_df.groupby([col])['purchase_amount'].transform('mean')\n    historical_trans_df[col+'_min'] = historical_trans_df.groupby([col])['purchase_amount'].transform('min')\n    historical_trans_df[col+'_max'] = historical_trans_df.groupby([col])['purchase_amount'].transform('max')\n    historical_trans_df[col+'_sum'] = historical_trans_df.groupby([col])['purchase_amount'].transform('sum')\n    historical_trans_df[col+'_std'] = historical_trans_df.groupby([col])['purchase_amount'].transform('std')\n    aggs[col+'_mean'] = ['mean']    \n\nnew_columns = create_new_columns('hist',aggs)\nhistorical_trans_group_df = historical_trans_df.groupby('card_id').agg(aggs)\nhistorical_trans_group_df.columns = new_columns\nhistorical_trans_group_df.reset_index(drop=False,inplace=True)\nhistorical_trans_group_df['hist_purchase_date_diff'] = (historical_trans_group_df['hist_purchase_date_max'] - historical_trans_group_df['hist_purchase_date_min']).dt.days\nhistorical_trans_group_df['hist_purchase_date_average'] = historical_trans_group_df['hist_purchase_date_diff']\/historical_trans_group_df['hist_card_id_size']\nhistorical_trans_group_df['hist_purchase_date_uptonow'] = (datetime.datetime.today() - historical_trans_group_df['hist_purchase_date_max']).dt.days\nhistorical_trans_group_df['hist_purchase_date_uptomin'] = (datetime.datetime.today() - historical_trans_group_df['hist_purchase_date_min']).dt.days\n\nlogger.info('reduce memory usage for historical trans')\nhistorical_trans_df = reduce_mem_usage(historical_trans_df)\n\nlogger.info('Completed aggregate historical trans')","608bedae":"#merge with train, test\ntrain_df = train_df.merge(historical_trans_group_df,on='card_id',how='left')\ntest_df = test_df.merge(historical_trans_group_df,on='card_id',how='left')\n#cleanup memory\ndel historical_trans_group_df; gc.collect()","cf19eb4b":"#define aggregations with new_merchant_trans_df \nlogger.info('Aggregate new merchant trans')\naggs = {}\nfor col in ['subsector_id','merchant_id','merchant_category_id','state_id', 'city_id']:\n    aggs[col] = ['nunique']\nfor col in ['month', 'hour', 'weekofyear', 'dayofweek']:\n    aggs[col] = ['nunique', 'mean', 'min', 'max']\n\n    \naggs['purchase_amount'] = ['sum','max','min','mean','var','std']\naggs['installments'] = ['sum','max','min','mean','var','std']\naggs['purchase_date'] = ['max','min', 'nunique']\naggs['month_lag'] = ['max','min','mean','var', 'nunique']\naggs['month_diff'] = ['mean', 'max', 'min', 'var','nunique']\naggs['weekend'] = ['sum', 'mean', 'nunique']\naggs['year'] = ['nunique', 'mean']\naggs['category_1'] = ['sum', 'mean', 'min', 'nunique']\naggs['category_2'] = ['sum', 'mean', 'min', 'nunique']\naggs['category_3'] = ['sum', 'mean', 'min', 'nunique']\naggs['card_id'] = ['size', 'count']\naggs['Christmas_Day_2017'] = ['mean']\naggs['Children_day_2017'] = ['mean']\naggs['Black_Friday_2017'] = ['mean']\naggs['Mothers_Day_2018'] = ['mean']\n\nfor col in ['category_2','category_3']:\n    new_merchant_trans_df[col+'_mean'] = new_merchant_trans_df.groupby([col])['purchase_amount'].transform('mean')\n    new_merchant_trans_df[col+'_min'] = new_merchant_trans_df.groupby([col])['purchase_amount'].transform('min')\n    new_merchant_trans_df[col+'_max'] = new_merchant_trans_df.groupby([col])['purchase_amount'].transform('max')\n    new_merchant_trans_df[col+'_sum'] = new_merchant_trans_df.groupby([col])['purchase_amount'].transform('sum')\n    new_merchant_trans_df[col+'_std'] = new_merchant_trans_df.groupby([col])['purchase_amount'].transform('std')\n    aggs[col+'_mean'] = ['mean']\n\nnew_columns = create_new_columns('new_hist',aggs)\nnew_merchant_trans_group_df = new_merchant_trans_df.groupby('card_id').agg(aggs)\nnew_merchant_trans_group_df.columns = new_columns\nnew_merchant_trans_group_df.reset_index(drop=False,inplace=True)\nnew_merchant_trans_group_df['new_hist_purchase_date_diff'] = (new_merchant_trans_group_df['new_hist_purchase_date_max'] - new_merchant_trans_group_df['new_hist_purchase_date_min']).dt.days\nnew_merchant_trans_group_df['new_hist_purchase_date_average'] = new_merchant_trans_group_df['new_hist_purchase_date_diff']\/new_merchant_trans_group_df['new_hist_card_id_size']\nnew_merchant_trans_group_df['new_hist_purchase_date_uptonow'] = (datetime.datetime.today() - new_merchant_trans_group_df['new_hist_purchase_date_max']).dt.days\nnew_merchant_trans_group_df['new_hist_purchase_date_uptomin'] = (datetime.datetime.today() - new_merchant_trans_group_df['new_hist_purchase_date_min']).dt.days\n\nlogger.info('reduce memory usage for new merchant trans')\nnew_merchant_trans_df = reduce_mem_usage(new_merchant_trans_df)\n\nlogger.info('Completed aggregate new merchant trans')","fbe0a649":"#merge with train, test\ntrain_df = train_df.merge(new_merchant_trans_group_df,on='card_id',how='left')\ntest_df = test_df.merge(new_merchant_trans_group_df,on='card_id',how='left')\n#clean-up memory\ndel new_merchant_trans_group_df; gc.collect()\ndel historical_trans_df; gc.collect()\ndel new_merchant_trans_df; gc.collect()","377a3ee4":"#process train\nlogger.info('Process train - outliers')\ntrain_df['outliers'] = 0\ntrain_df.loc[train_df['target'] < -30, 'outliers'] = 1\noutls = train_df['outliers'].value_counts()\nprint(\"Outliers: {}\".format(outls))\nlogger.info('Process train and test')\n## process both train and test\nfor df in [train_df, test_df]:\n    df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n    df['dayofweek'] = df['first_active_month'].dt.dayofweek\n    df['weekofyear'] = df['first_active_month'].dt.weekofyear\n    df['dayofyear'] = df['first_active_month'].dt.dayofyear\n    df['quarter'] = df['first_active_month'].dt.quarter\n    df['is_month_start'] = df['first_active_month'].dt.is_month_start\n    df['month'] = df['first_active_month'].dt.month\n    df['elapsed_time'] = (datetime.datetime.today() - df['first_active_month']).dt.days\n    df['hist_first_buy'] = (df['hist_purchase_date_min'] - df['first_active_month']).dt.days\n    df['hist_last_buy'] = (df['hist_purchase_date_max'] - df['first_active_month']).dt.days\n    df['new_hist_first_buy'] = (df['new_hist_purchase_date_min'] - df['first_active_month']).dt.days\n    df['new_hist_last_buy'] = (df['new_hist_purchase_date_max'] - df['first_active_month']).dt.days\n    for f in ['hist_purchase_date_max','hist_purchase_date_min','new_hist_purchase_date_max',\\\n                     'new_hist_purchase_date_min']:\n        df[f] = df[f].astype(np.int64) * 1e-9\n    df['card_id_total'] = df['new_hist_card_id_size']+df['hist_card_id_size']\n    df['card_id_cnt_total'] = df['new_hist_card_id_count']+df['hist_card_id_count']\n    df['purchase_amount_total'] = df['new_hist_purchase_amount_sum']+df['hist_purchase_amount_sum']\n    df['purchase_amount_mean'] = df['new_hist_purchase_amount_mean']+df['hist_purchase_amount_mean']\n    df['purchase_amount_max'] = df['new_hist_purchase_amount_max']+df['hist_purchase_amount_max']\n\n    for f in ['feature_1','feature_2','feature_3']:\n        order_label = train_df.groupby([f])['outliers'].mean()\n        df[f] = df[f].map(order_label)\n\n    df['feature_sum'] = df['feature_1'] + df['feature_2'] + df['feature_3']\n    df['feature_mean'] = df['feature_sum']\/3\n    df['feature_max'] = df[['feature_1', 'feature_2', 'feature_3']].max(axis=1)\n    df['feature_min'] = df[['feature_1', 'feature_2', 'feature_3']].min(axis=1)\n    df['feature_var'] = df[['feature_1', 'feature_2', 'feature_3']].std(axis=1)\n\n    \n##\ntrain_columns = [c for c in train_df.columns if c not in ['card_id', 'first_active_month','target','outliers']]\ntarget = train_df['target']\ndel train_df['target']\nlogger.info('Completed process train')","21f280f0":"#model\n##model params\nlogger.info('Prepare model')\nparam = {'num_leaves': 51,\n         'min_data_in_leaf': 35, \n         'objective':'regression',\n         'max_depth': -1,\n         'learning_rate': 0.008,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.85,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.82,\n         \"bagging_seed\": 42,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.11,\n         \"verbosity\": -1,\n         \"nthread\": 4,\n         \"random_state\": 2019}\n#prepare fit model with cross-validation\nfolds = StratifiedKFold(n_splits=9, shuffle=True, random_state=2019)\noof = np.zeros(len(train_df))\npredictions = np.zeros(len(test_df))\nfeature_importance_df = pd.DataFrame()\n#run model\nlogger.info('Start running model')\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train_df,train_df['outliers'].values)):\n    strLog = \"Fold {}\".format(fold_)\n    print(strLog)\n    trn_data = lgb.Dataset(train_df.iloc[trn_idx][train_columns], label=target.iloc[trn_idx])#, categorical_feature=categorical_feats)\n    val_data = lgb.Dataset(train_df.iloc[val_idx][train_columns], label=target.iloc[val_idx])#, categorical_feature=categorical_feats)\n\n    num_round = 10000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=200, early_stopping_rounds = 150)\n    oof[val_idx] = clf.predict(train_df.iloc[val_idx][train_columns], num_iteration=clf.best_iteration)\n    #feature importance\n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = train_columns\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    #predictions\n    predictions += clf.predict(test_df[train_columns], num_iteration=clf.best_iteration) \/ folds.n_splits\n    logger.info(strLog)\n    \nstrRMSE = \"\".format(np.sqrt(mean_squared_error(oof, target)))\nprint(strRMSE)","68a059f2":"##plot the feature importance\nlogger.info(\"Feature importance plot\")\ncols = (feature_importance_df[[\"Feature\", \"importance\"]]\n        .groupby(\"Feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:1000].index)\nbest_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\nplt.figure(figsize=(14,26))\nsns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\nplt.title('LightGBM Features (averaged over folds)')\nplt.tight_layout()\nplt.savefig('lgbm_importances.png')","950939ed":"##submission\nlogger.info(\"Prepare submission\")\nsub_df = pd.DataFrame({\"card_id\":test_df[\"card_id\"].values})\nsub_df[\"target\"] = predictions\nsub_df.to_csv(\"submission.csv\", index=False)","44b69b2d":"Let's show now the distribution of **feature_3** for  **train** and **test**.","64c69dd7":"<a href=\"#0\"><font size=\"1\">Go to top<\/font><\/a>  \n\n\n## <a id='22'>Load the data<\/a>  \n\nLet's see first what data files do we have in the root directory. ","7c4c783c":"Let's check **most_recent_sales_range** and **most_recent_purchase_range**[](http:\/\/).","c276eede":"# <a id='2'>Prepare the data analysis<\/a>   \n\n\nBefore starting the analysis, we need to make few preparation: load the packages, load and inspect the data.\n","6e04d4e7":"Let's plot now the distribution of **first_active_month** from **train** and **test** datasets.","43c8a37d":"## Feature importance","0fe01a67":"Let's show the purchase amount grouped by purchase date types.","f60b7592":"# <a id='1'>Introduction<\/a>  \n\nThis Kernel will take you through the process of analyzing the data to understand the predictive values of various features and the possible correlation between different features, selection of features with predictive value, features engineering to create features with higher predictive value and creation of a baseline model.","658b3bb0":"Let's see **city_id**, **merchant_category_id**,  **state_id**, **subsector_id**.","9ff18a01":"## <a id='32'>Train and test data<\/a>  \n\nLet's check the distribution of train and test features.\n\nBoth have the same features:\n* card_id;  \n* feature1, feature2, feature3;  \n* first_active_month;  \n\nTrain has also the target value, called **target**.   \n\nLet's define few auxiliary functions.\n","577a7201":"Let's follow with **category_1**, **category_2**, **category_4**.","4d1a5451":"<a href=\"#0\"><font size=\"1\">Go to top<\/font><\/a>  \n\n# <a id='3'>Data exploration<\/a>  \n\nLet's check the dataframes created.","465148d5":"## Process new merchant transaction data","7465111c":"# <a id='5'>Model<\/a>  ","e0d4aa65":"# <a id='4'>Feature engineering<\/a>  \n\n\nBefore creating the model we will prepare the aggregated features.\n","5a8d0db1":"And let's see also the distribuiton of **target** grouped by values of **feature_3**.","2ce0700f":"Let's check missing data for all dataframes.","f30b936d":"# <a id='6'>Submission<\/a>  ","689fdd5d":"<h1><center><font size=\"6\">Elo EDA Prediction<\/font><\/center><\/h1>\n\n<img src=\"https:\/\/www.redhat.com\/cms\/managed-files\/elo-225x158.png\" width=400><\/img>\n\n# <a id='0'>Content<\/a>\n\n- <a href='#1'>Introduction<\/a>  \n- <a href='#2'>Prepare the data analysis<\/a>  \n - <a href='#21'>Load packages<\/a>  \n - <a href='#22'>Load the data<\/a>   \n- <a href='#3'>Data exploration<\/a>   \n - <a href='#31'>Check for missing data<\/a>  \n - <a href='#32'>Train and test data<\/a>  \n - <a href='#33'>Historical transaction data<\/a>  \n  - <a href='#34'>New merchant transaction data<\/a>  \n  - <a href='#35'>Merchant data<\/a>  \n- <a href='#4'>Feature engineering<\/a>\n- <a href='#5'>Model<\/a>\n- <a href='#6'>Submission<\/a>\n- <a href='#7'>References<\/a>    \n    ","7dcde00c":"Let's see the distribution of **feature_2** for **train** and **test** set.","e2deacf4":"## Process historical transaction data","41284c8b":"Let's check the distribution of the purchase amount grouped by various features. We will represent  log(purchase_amount + 1).","2ed2c6ab":"Let's see the distribution of **target** value groped on  **feature_1** values.","380a97eb":"Let's plot distribution of **numerical_1**, **numerical_2**, **avg_sales_lag3**, **avg_sales_lag6**, **avg_sales_lag12**.","83a14c66":"# <a id='7'>References<\/a>  \n\n[1]  https:\/\/www.kaggle.com\/gpreda\/elo-world-high-score-without-blending    \n[2] https:\/\/www.kaggle.com\/mfjwr1\/simple-lightgbm-without-blending   \n","a60b9bd8":"Let's look to the **city_id**, **state_id**.","963de0a1":"Let's show the purchase amount grouped by purchase time types.\n\nBefore this, let's extract the date.","1b5770d6":"Let's plot the amount of purchase per day of week, week of year and month.","aba018aa":"## <a id='33'>Historical transaction data<\/a>  \n\nLet's check the distribution of historical transaction data features.  \n\n**historical_trans_df** is linked with **train_df** and **test_df** by the **card_id** key.\n\nLet's plot **category_1**, **category_2**, **category_3** features distribution.\n","d029445c":"## <a id='21'>Load packages<\/a>\n\nWe load the packages used for the analysis.","81e4c27a":"## <a id='35'>Merchant data<\/a>  \n\nLet's check the distribution of merchant data features.   \n\nLet's start with  **merchant_category_id**, **subsector_id**.\n","3157a0f8":"## Utility functions and data cleaning","8b8e2096":"Let's see city_id, merchant_category_id,  state_id, subsector_id.","8204d80f":"## Process train and test data","64e77697":"Let's load the data files.","920c2258":"## <a id='34'>New merchant transaction data<\/a>  \n\nLet's check the distribution of new merchant transaction data features.   \n\n**new_merchant_trans_df** is linked with **train_df** and **test_df** by the **card_id** key.\n\nLet's plot **category_1**, **category_2**, **category_3** features distribution.","f8098c9d":"Let's show the distribution of **feature_1** for **train** and **test** set.","45c34bdd":"Let's start by checking if there are missing data, unlabeled data or data that is inconsistently labeled. \n\n## <a id='31'>Check for missing data<\/a>  \n\nLet's create a function that check for missing data in the dataframes.","cc96be0b":"Let's see the distribution of **target** grouped on **feature_2** values."}}