{"cell_type":{"d56f31b2":"code","a70a9930":"code","86a4a532":"code","fdce8640":"code","67ddada4":"code","710f484c":"code","064409ba":"code","bc4d5e8e":"code","ff518eac":"code","0ae53f9a":"code","8566e690":"code","77f6c8a7":"code","b551d15f":"code","364b699d":"code","c4125f5e":"markdown","e462a7c9":"markdown","3e19e881":"markdown","7723bf81":"markdown","c60a57f1":"markdown","6122b2ef":"markdown","2ab73de0":"markdown","8fcb6131":"markdown","c120e4b5":"markdown","20c8a7dd":"markdown","6a788ea7":"markdown","b4bd4056":"markdown"},"source":{"d56f31b2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a70a9930":"%%time\ntrain= pd.read_csv('\/kaggle\/input\/riiid-test-answer-prediction\/train.csv',\n       usecols=[1, 2, 3,4,7,8,9], dtype={'timestamp': 'int64', 'user_id': 'int32' ,'content_id': 'int16','content_type_id': 'int8',\n        'answered_correctly':'int8','prior_question_elapsed_time': 'float32','prior_question_had_explanation': 'boolean'}\n              )","86a4a532":"%%time\ntrain.head()","fdce8640":"%%time\nresults_u = train[['user_id','answered_correctly']].groupby(['user_id']).agg(['mean', 'sum'])\nresults_u.columns = [\"answered_correctly_user\", 'sum']","67ddada4":"import dask.dataframe as dd","710f484c":"%%time\ntrain_dask = dd.read_csv(\"\/kaggle\/input\/riiid-test-answer-prediction\/train.csv\",\n       usecols=[1, 2, 3,4,7,8,9], dtype={'timestamp': 'int64', 'user_id': 'int32' ,'content_id': 'int16','content_type_id': 'int8',\n        'answered_correctly':'int8','prior_question_elapsed_time': 'float32','prior_question_had_explanation': 'boolean'})","064409ba":"%%time\ntrain_dask.head(npartitions=-1)","bc4d5e8e":"%%time\ntrain_dask.head()","ff518eac":"train_dask.npartitions","0ae53f9a":"%%time\ntrain_dask","8566e690":"%%time\nresults_u = train_dask[['user_id','answered_correctly']].groupby(['user_id']).agg(['mean', 'sum'])\nresults_u.columns = [\"answered_correctly_user\", 'sum']","77f6c8a7":"import sys\n!cp ..\/input\/rapids\/rapids.0.15.0 \/opt\/conda\/envs\/rapids.tar.gz\n!cd \/opt\/conda\/envs\/ && tar -xzvf rapids.tar.gz > \/dev\/null\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\/site-packages\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\"] + sys.path \n!cp \/opt\/conda\/envs\/rapids\/lib\/libxgboost.so \/opt\/conda\/lib\/\nimport cudf","b551d15f":"from time import time\nfrom contextlib import contextmanager","364b699d":"\n\n%%time\n\ndata = cudf.read_csv('\/kaggle\/input\/riiid-test-answer-prediction\/train.csv')\n\nprint(\"Train size:\", data.shape)\n\n","c4125f5e":"It takes 14.3 seconds to compute the above cell, Lets see how dask is performing on that ","e462a7c9":"To Lean More about Dask [Follow Here](https:\/\/dask.org\/) ","3e19e881":"### That was a huge time difference between pandas and DASK","7723bf81":"### There are toally 92 partitions dask has created on this dataset","c60a57f1":"voila It takes just 19.7 ms to compute the above cell, Which is actually great to see ","6122b2ef":"# Pandas Dataframe","2ab73de0":"## Let's see how many partitions are there in the dask dataframe\n","8fcb6131":"Yes it is faster than previous one, as it is printing from one partition","c120e4b5":"# Dask Dataframe","20c8a7dd":"Printing the entire dataframe takes less than 10 micro seconds","6a788ea7":"![Image](https:\/\/docs.dask.org\/en\/latest\/_static\/images\/dask-horizontal-white.svg)","b4bd4056":"> usually dask read the csv files in a chunk of partition so to take up all the partition we must use the *nparameter=-1* argument ."}}