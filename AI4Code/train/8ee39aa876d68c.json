{"cell_type":{"21f106cb":"code","d5a8e332":"code","d9e197a0":"code","5124c0eb":"code","2fa5143b":"code","fcd99736":"code","618be682":"code","5d505004":"code","0ea8beae":"code","4cd460bd":"code","ebe26ebc":"code","57a9c392":"code","39f0a196":"code","da0dd121":"code","93c3291c":"code","284e550e":"code","2ea93054":"code","ed4177df":"code","f2ffb703":"code","404d97ed":"code","47abd653":"code","70c64066":"code","09b12957":"code","8c3485b3":"code","2a9a4004":"code","6514d564":"code","91e31f87":"code","7360b9aa":"code","972287da":"code","8e64bcbd":"code","f70312c8":"code","94980780":"code","ab5b2843":"code","b686a164":"code","b8e46fca":"code","1a6493f7":"code","6e20eca1":"code","55524569":"code","f2692595":"code","d57ec3cd":"code","f1a3007e":"code","5d791eb3":"code","f9f62408":"code","e761df20":"code","325a8d39":"code","695f90f5":"code","4a92b784":"code","10c9ceec":"code","7441ad58":"code","eb432541":"code","3fc16deb":"code","f103ca3b":"code","e255e508":"code","9458c6a1":"code","e4cff63e":"code","5e87ad6b":"code","19d300d2":"code","4d69d90f":"code","7f9fed6b":"code","2bae996e":"code","643253ab":"code","d8e427df":"code","19bad745":"code","49182276":"code","e1f8caee":"code","38ca8f6f":"code","0a4b3de2":"code","84af93e5":"code","b49eafc6":"code","7495cd83":"code","79c70700":"markdown","5b68b32a":"markdown","66e383dd":"markdown","2732ef74":"markdown","8a58276f":"markdown","dcf2305f":"markdown","8402869d":"markdown","82559007":"markdown","703c5e88":"markdown","309c77c4":"markdown","b47a0e6e":"markdown","6645cfdd":"markdown","26a2372f":"markdown","609c00aa":"markdown","d990b7d2":"markdown","651b28be":"markdown","e04109b7":"markdown","06eb4266":"markdown","89b314f8":"markdown","8c8e1940":"markdown","757788a1":"markdown","b1e4f58c":"markdown","dbadb160":"markdown","ae58fda4":"markdown","32507077":"markdown","b96a44cb":"markdown","def97749":"markdown","bd79d3c9":"markdown","8f3f611e":"markdown","b4e3d5cd":"markdown","9853069b":"markdown","616696d5":"markdown","7ce6d999":"markdown","fb3669ab":"markdown","6deae37b":"markdown","91051612":"markdown","0c4ea813":"markdown","2ec322e5":"markdown","a39282a2":"markdown","d01478e1":"markdown","59d690d0":"markdown","8f0d3028":"markdown","258d5967":"markdown","021c6fa0":"markdown","1341a4e1":"markdown","65ff7803":"markdown","d4a353b5":"markdown","3d172ff9":"markdown","f3baf8c5":"markdown","d2a69b69":"markdown","445f500d":"markdown","25210d35":"markdown","79b1f590":"markdown","848495ae":"markdown","aad3a6ce":"markdown","eb1ac95c":"markdown","aa44de70":"markdown","914417c1":"markdown","fa388329":"markdown"},"source":{"21f106cb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","d5a8e332":"df=pd.read_csv('\/kaggle\/input\/madrid-real-estate-market\/houses_Madrid.csv')\ndf.set_index('id', inplace=True)\ndf.sort_index(axis=0, inplace=True)\ndf.drop(columns=['Unnamed: 0'], inplace=True)\npd.set_option('display.max_columns', 500)\ndf","d9e197a0":"#DF of buy price only if buy price known\ntarget=df[df['is_buy_price_known']==True]\ntarget=target['buy_price']\ntarget.describe()\n","5124c0eb":"#DF of rent price only if rent price known\ntarget2=df[df['is_rent_price_known']==True]\ntarget2=target2['rent_price']\ntarget2.describe()","2fa5143b":"sns.distplot(target);","fcd99736":"print(\"Skewness: %f\" % target.skew())\nprint(\"Kurtosis: %f\" % target.kurt())","618be682":"#Let's see our correlation matrix\ncorrmat = df.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);","5d505004":"from sklearn.model_selection import train_test_split\ntrain_set, test_set=train_test_split(df, test_size=.2,random_state=42)","0ea8beae":"total = train_set.isnull().sum().sort_values(ascending=False)\npercent = (train_set.isnull().sum()\/train_set.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(50)","4cd460bd":"dftrim=train_set.drop(columns=['title','has_public_parking','portal','are_pets_allowed','is_furnished','is_kitchen_equipped','rent_price_by_area','latitude','longitude','has_private_parking','door','street_number','street_name','raw_address','is_exact_address_hidden','rent_price','is_rent_price_known','is_buy_price_known'])\ndftrim","ebe26ebc":"dftrim.dtypes\nfor col in dftrim:\n    print(col)\n    print(dftrim[col].value_counts())\n    print('')\n#Change categorical into categorical","57a9c392":"dftrim.drop(columns='operation',inplace=True)\ntotal = dftrim.isnull().sum().sort_values(ascending=False)\npercent = (dftrim.isnull().sum()\/dftrim.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(40)","39f0a196":"collist=['has_ac','has_fitted_wardrobes','has_garden','has_pool','has_terrace','has_balcony','has_storage_room','is_accessible','has_green_zones']\nfor col in collist:\n    dftrim[col]=dftrim[col].fillna(False)\ndftrim","da0dd121":"dftrim.drop(columns=['sq_mt_allotment','sq_mt_useful','parking_price','is_parking_included_in_price','n_floors',], inplace=True)","93c3291c":"total = dftrim.isnull().sum().sort_values(ascending=False)\npercent = (dftrim.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(10)","284e550e":"dftrim.drop(columns=['is_orientation_east','is_orientation_south','is_orientation_west','is_orientation_north','has_individual_heating','has_central_heating','floor','is_floor_under','buy_price_by_area'], inplace=True)","2ea93054":"dftrim","ed4177df":"#Turn Neighborhood into Neighborhood & District separately\ndftrim['district_id']=dftrim['neighborhood_id'].copy()\ndftrim.district_id=dftrim.district_id.str.extract(r'(District \\d+)')\ndftrim.neighborhood_id=dftrim.neighborhood_id.str.extract(r'(Neighborhood \\d+)')\ndftrim.district_id=dftrim.district_id.str.extract(r'(\\d+)')\ndftrim.neighborhood_id=dftrim.neighborhood_id.str.extract(r'(\\d+)')\ndftrim","f2ffb703":"dftrim=dftrim[dftrim['sq_mt_built'].notna()]\ndftrim=dftrim[dftrim['n_bathrooms'].notna()]","404d97ed":"dftrim['house_type_id']=dftrim['house_type_id'].fillna(\"Misc\")","47abd653":"mask=(dftrim['is_new_development'].isnull()) & (~dftrim['built_year'].isnull())\ndftrim['is_new_development'][mask]=False\ndftrim.drop(columns='built_year',inplace=True)\ndftrim['is_new_development'].value_counts()","70c64066":"#dft1=dftrim[dftrim['is_exterior'].isnull()]\ndft1=dftrim.groupby(by='house_type_id')\ndft1['is_exterior'].describe()\n","09b12957":"dft1['has_lift'].describe()","8c3485b3":"dftrim=pd.get_dummies(dftrim,columns=['house_type_id'])\n#Need to drop one, so why not drop the one I created\ndftrim.drop(columns='house_type_id_Misc', inplace=True)\ndftrim","2a9a4004":"dftrim.drop(columns='subtitle',inplace=True)","6514d564":"total = dftrim.isnull().sum().sort_values(ascending=False)\npercent = (dftrim.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(10)","91e31f87":"print(dftrim['is_new_development'].value_counts())\nprint(dftrim['is_exterior'].value_counts())\nprint(dftrim['has_lift'].value_counts())","7360b9aa":"devmode=dftrim['is_new_development'].mode()[0]\ndftrim['is_new_development']=dftrim['is_new_development'].fillna(devmode)\nprint(dftrim['is_new_development'].value_counts())","972287da":"mask=(dftrim['has_lift'].isnull()) & (dftrim['house_type_id_HouseType 2: Casa o chalet']==0)\nliftmode=dftrim['has_lift'].mode()[0]\ndftrim['has_lift'][mask]=liftmode\nprint(dftrim['has_lift'].value_counts())","8e64bcbd":"mask=(dftrim['is_exterior'].isnull()) & (dftrim['house_type_id_HouseType 2: Casa o chalet']==0)\nextmode=dftrim['is_exterior'].mode()[0]\ndftrim['is_exterior'][mask]=extmode\nprint(dftrim['is_exterior'].value_counts())","f70312c8":"total = dftrim.isnull().sum().sort_values(ascending=False)\npercent = (dftrim.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(5)","94980780":"dffinal=dftrim.drop(columns=[\"neighborhood_id\"])\ndffinal['has_lift']=pd.to_numeric(dffinal['has_lift'],errors='coerce')\ndffinal['is_exterior']=pd.to_numeric(dffinal['is_exterior'],errors='coerce')\ndffinal=pd.get_dummies(dffinal)\ndffinal=dffinal.drop(columns=[\"district_id_21\"])\ndffinal","ab5b2843":"#Let's see our correlation matrix\ncorrmat = dffinal.corr()\nf, ax = plt.subplots(figsize=(20, 20))\nsns.heatmap(corrmat, vmax=.8, square=True);\n","b686a164":"corr_matrix=dffinal.corr()\ncorr_matrix['buy_price'].sort_values(ascending=False)","b8e46fca":"dffinal=dffinal.drop(columns=[\"energy_certificate_A\",'energy_certificate_B','energy_certificate_C','energy_certificate_D','energy_certificate_E','energy_certificate_F','energy_certificate_G','energy_certificate_en tr\u00e1mite','energy_certificate_inmueble exento','energy_certificate_no indicado'])\ncorrmat = dffinal.corr()\nf, ax = plt.subplots(figsize=(15, 15))\nsns.heatmap(corrmat, vmax=.8, square=True);","1a6493f7":"cols=['house_type_id_HouseType 1: Pisos','house_type_id_HouseType 2: Casa o chalet','house_type_id_HouseType 4: D\u00faplex','house_type_id_HouseType 5: \u00c1ticos']\ndffinal[cols][dffinal['has_garden']==True].value_counts()","6e20eca1":"dffinal=dffinal.drop(columns=['has_garden','is_renewal_needed'])\ncorrmat = dffinal.corr()\nf, ax = plt.subplots(figsize=(15, 15))\nsns.heatmap(corrmat, vmax=.8, square=True);","55524569":"def prepare(df):\n    df=df.drop(columns=['title','subtitle','sq_mt_useful','n_floors','sq_mt_allotment','latitude','longitude','raw_address','is_exact_address_hidden','street_name','street_number','portal','floor','is_floor_under','door','operation','rent_price','rent_price_by_area','is_rent_price_known','buy_price_by_area','is_buy_price_known','has_central_heating','has_individual_heating','are_pets_allowed','is_furnished','is_kitchen_equipped','has_garden','is_renewal_needed','energy_certificate','has_private_parking','has_public_parking','is_parking_included_in_price','parking_price','is_orientation_north','is_orientation_west','is_orientation_south','is_orientation_east'])\n    df=df[df['sq_mt_built'].notna()]\n    df=df[df['n_bathrooms'].notna()]\n    collist=['has_ac','has_fitted_wardrobes','has_pool','has_terrace','has_balcony','has_storage_room','is_accessible','has_green_zones']\n    for col in collist:\n        df[col]=df[col].fillna(False)\n    mask=(df['is_new_development'].isnull()) & (~df['built_year'].isnull())\n    df['is_new_development'][mask]=False\n    df.drop(columns='built_year',inplace=True)\n    df['is_new_development']=df['is_new_development'].fillna(devmode)\n    df['house_type_id']=df['house_type_id'].fillna(\"Misc\")\n    df['district_id']=df['neighborhood_id'].copy()\n    df.district_id=df.district_id.str.extract(r'(District \\d+)')\n    df.neighborhood_id=df.neighborhood_id.str.extract(r'(Neighborhood \\d+)')\n    df.district_id=df.district_id.str.extract(r'(\\d+)')\n    df.neighborhood_id=df.neighborhood_id.str.extract(r'(\\d+)')\n    df.drop(columns='neighborhood_id',inplace=True)\n    df['has_lift']=pd.to_numeric(df['has_lift'],errors='coerce')\n    df['is_exterior']=pd.to_numeric(df['is_exterior'],errors='coerce')\n    df=pd.get_dummies(df)\n    mask=(df['has_lift'].isnull()) & (df['house_type_id_HouseType 2: Casa o chalet']==0)\n    df['has_lift'][mask]=liftmode\n    mask=(df['is_exterior'].isnull()) & (df['house_type_id_HouseType 2: Casa o chalet']==0)\n    df['is_exterior'][mask]=extmode\n    df=df.drop(columns=['house_type_id_Misc','district_id_21'])\n    return df\n\ntesting=prepare(test_set)\ntesting","f2692595":"train_y=dffinal[['buy_price']]\ntrain_x=dffinal.drop(columns='buy_price')\ntest_y=testing[['buy_price']]\ntest_x=testing.drop(columns='buy_price')","d57ec3cd":"train_x_nonna=train_x.drop(columns=['has_lift','is_exterior'])\ntest_x_nonna=test_x.drop(columns=['has_lift','is_exterior'])","f1a3007e":"from sklearn import preprocessing\n\ncols=['sq_mt_built','n_rooms','n_bathrooms']\nsca=preprocessing.StandardScaler()\ntrain_x_std=train_x_nonna.copy()\ntest_x_std=test_x_nonna.copy()\nfor col in cols:\n    train_x_std[col]=sca.fit_transform(train_x_std[[col]])\n    test_x_std[col]=sca.transform(test_x_std[[col]])","5d791eb3":"from sklearn.dummy import DummyRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost as xgb\nimport lightgbm as lgb","f9f62408":"from sklearn import metrics\nfrom sklearn import model_selection\nimport math\n#standardized\n#K-fold means we don't need to split train and test sets\ncollist=['is_new_development','has_ac','has_fitted_wardrobes','has_pool','has_terrace','has_balcony','has_storage_room','is_accessible','has_green_zones','has_parking']\n#for col in collist:\n    #x[col]=x[col].astype(int)\nscores=['r2','neg_mean_absolute_error','neg_mean_squared_error']\n#SVR cut out for now\nfor model in [DummyRegressor, LinearRegression,KNeighborsRegressor,SVR]:\n    if model==SVR:\n        cls=model(kernel='linear')\n    else:\n        cls=model()\n    kfold=model_selection.KFold(n_splits=10)\n    s=model_selection.cross_validate(cls,train_x_std,train_y,scoring=scores,cv=kfold)\n    print(\n        f\"{model.__name__:22} Fit Time: \"\n        f\"{s['fit_time'].mean():.3f}s  R2: \"\n        f\"{s['test_r2'].mean():.3f}  MAE: \"\n        f\"{s['test_neg_mean_absolute_error'].mean()*-1:.3f}  RMSE: \"\n        f\"{math.sqrt(s['test_neg_mean_squared_error'].mean()*-1):.3f}\"\n    )","e761df20":"scores=['r2','neg_mean_absolute_error','neg_mean_squared_error']\nfor model in [DecisionTreeRegressor, RandomForestRegressor]:\n    cls=model()\n    kfold=model_selection.KFold(n_splits=10)\n    s=model_selection.cross_validate(cls,train_x_nonna,train_y,scoring=scores,cv=kfold)\n    mse=s['test_neg_mean_squared_error'].mean()*-1\n    rmse=math.sqrt(mse)\n    print(\n        f\"{model.__name__:22} Fit Time: \"\n        f\"{s['fit_time'].mean():.3f}s  R2: \"\n        f\"{s['test_r2'].mean():.3f}  MAE: \"\n        f\"{s['test_neg_mean_absolute_error'].mean()*-1:.3f}  RMSE: \"\n        f\"{rmse:.3f}\"\n    )\n","325a8d39":"import regex as re\ntrain_x = train_x.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\ntrain_y=train_y.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n#missing vals included\nfor model in [xgb.XGBRegressor, lgb.LGBMRegressor]:\n    cls=model()\n    kfold=model_selection.KFold(n_splits=10)\n    s=model_selection.cross_validate(cls,train_x,train_y,scoring=scores,cv=kfold)\n    mse=s['test_neg_mean_squared_error'].mean()*-1\n    rmse=math.sqrt(mse)\n    print(\n        f\"{model.__name__:22} Fit Time: \"\n        f\"{s['fit_time'].mean():.3f}s  R2: \"\n        f\"{s['test_r2'].mean():.3f}  MAE: \"\n        f\"{s['test_neg_mean_absolute_error'].mean()*-1:.3f}  RMSE: \"\n        f\"{rmse:.3f}\"\n    )","695f90f5":"from yellowbrick.features import RFECV\n#rfe=RFECV(RandomForestClassifier(n_estimators=50),cv=5)\nrfe=RFECV(LinearRegression(),cv=5)\nrfe.fit(train_x_std,train_y)\n#rfe.fit(train_x_nonna,train_y)\nprint(train_x_std.columns)\nprint(rfe.rfe_estimator_.ranking_)\nprint(rfe.rfe_estimator_.n_features_)\nprint(rfe.rfe_estimator_.support_)","4a92b784":"from sklearn.model_selection import GridSearchCV\nparams = [{\n        'n_neighbors': [5,10,50,100,1000],\n    }]\nkgrid = GridSearchCV(estimator=KNeighborsRegressor(),           \n                      param_grid=params, \n                      cv=5,\n                      verbose=0) \nkgrid.fit(train_x_std, train_y)\nprint(kgrid.best_score_)\nprint(kgrid.best_estimator_)\nprint(kgrid.best_params_)\nprint(list(zip(kgrid.cv_results_['params'],kgrid.cv_results_['mean_test_score'])))","10c9ceec":"params = [{\n        'n_neighbors': [3,5,7,9,11],\n    }]\nkgrid = GridSearchCV(estimator=KNeighborsRegressor(),           \n                      param_grid=params, \n                      cv=5,\n                      verbose=0) \nkgrid.fit(train_x_std, train_y)\nprint(kgrid.best_score_)\nprint(kgrid.best_estimator_)\nprint(kgrid.best_params_)\nprint(list(zip(kgrid.cv_results_['params'],kgrid.cv_results_['mean_test_score'])))","7441ad58":"params = [{\n        'n_neighbors': [5,6,7,8],\n        'p':[1,2],\n        'weights':['uniform','distance']\n    }]\nkgrid = GridSearchCV(estimator=KNeighborsRegressor(),           \n                      param_grid=params, \n                      cv=10,\n                      verbose=0) \nkgrid.fit(train_x_std, train_y)\nprint(kgrid.best_score_)\nprint(kgrid.best_estimator_)\nprint(kgrid.best_params_)\nprint(list(zip(kgrid.cv_results_['params'],kgrid.cv_results_['mean_test_score'])))","eb432541":"params = [{\n    'n_estimators': [10,100,500,1000,2000]\n    }]\nfgrid = GridSearchCV(estimator=RandomForestRegressor(),           \n                      param_grid=params, \n                      cv=5,\n                      verbose=0) \nfgrid.fit(train_x_nonna, train_y)\nprint(fgrid.best_score_)\nprint(fgrid.best_estimator_)\nprint(fgrid.best_params_)\nprint(list(zip(fgrid.cv_results_['params'],fgrid.cv_results_['mean_test_score'])))","3fc16deb":"#RandomForestRegressor\nparams = [{\n'max_depth': [10,100,1000],\n'min_samples_split': [10,100,1000],\n'n_estimators': [100,300,500],\n    }]\nfgrid = GridSearchCV(estimator=RandomForestRegressor(),           \n                      param_grid=params, \n                      cv=5,\n                      verbose=0) \nfgrid.fit(train_x_nonna, train_y)\nprint(fgrid.best_score_)\nprint(fgrid.best_estimator_)\nprint(fgrid.best_params_)\nprint(list(zip(fgrid.cv_results_['params'],fgrid.cv_results_['mean_test_score'])))","f103ca3b":"#RandomForestRegressor\nparams = [{\n'max_depth': [500,1000,None],\n'min_samples_split': [5,10,30],\n'n_estimators': [500],\n    }]\nfgrid = GridSearchCV(estimator=RandomForestRegressor(),           \n                      param_grid=params, \n                      cv=5,\n                      verbose=0) \nfgrid.fit(train_x_nonna, train_y)\nprint(fgrid.best_score_)\nprint(fgrid.best_estimator_)\nprint(fgrid.best_params_)\nprint(list(zip(fgrid.cv_results_['params'],fgrid.cv_results_['mean_test_score'])))","e255e508":"#RandomForestRegressor\nparams = [{\n'max_depth': [1000,1500],\n'max_features': ['auto','sqrt'],\n'min_samples_split': [5],\n'n_estimators': [500,2000]\n    }]\nfgrid = GridSearchCV(estimator=RandomForestRegressor(),           \n                      param_grid=params, \n                      cv=10,\n                      verbose=0) \nfgrid.fit(train_x_nonna, train_y)\nprint(fgrid.best_score_)\nprint(fgrid.best_estimator_)\nprint(fgrid.best_params_)\nprint(list(zip(fgrid.cv_results_['params'],fgrid.cv_results_['mean_test_score'])))","9458c6a1":"#XGBoost\nparams = {\n    'max_depth': [10,100,1000],\n    'n_estimators': [10,100,1000],\n}\nxgrid = GridSearchCV(estimator=xgb.XGBRegressor(),           \n                      param_grid=params, \n                      cv=5,\n                      verbose=0) \nxgrid.fit(train_x, train_y)\nprint(xgrid.best_score_)\nprint(xgrid.best_estimator_)\nprint(xgrid.best_params_)","e4cff63e":"print(list(zip(xgrid.cv_results_['params'],xgrid.cv_results_['mean_test_score'])))","5e87ad6b":"#XGBoost\nparams = {\n    'max_depth': [5,10,30,50],\n    'n_estimators': [5,10,30,50],\n    'learning_rate': [.5,0.1,0.05,0.01]\n}\nxgrid = GridSearchCV(estimator=xgb.XGBRegressor(),           \n                      param_grid=params, \n                      cv=5,\n                      verbose=0) \nxgrid.fit(train_x, train_y)\nprint(xgrid.best_score_)\nprint(xgrid.best_estimator_)\nprint(xgrid.best_params_)","19d300d2":"#XGBoost\nparams = {\n    'max_depth': [10, 15, 20],\n    'n_estimators': [50, 60, 70, 80],\n    'learning_rate': [0.1]\n}\nxgrid = GridSearchCV(estimator=xgb.XGBRegressor(),           \n                      param_grid=params, \n                      cv=10,\n                      verbose=0) \nxgrid.fit(train_x, train_y)\nprint(xgrid.best_score_)\nprint(xgrid.best_estimator_)\nprint(xgrid.best_params_)","4d69d90f":"#LGBM\nparams = {\n    'num_leaves': [10,100,1000],\n    'min_child_samples': [10,100,1000],\n    'n_estimators':[10,100,1000],\n}\nlgrid = GridSearchCV(estimator=lgb.LGBMRegressor(),           \n                      param_grid=params, \n                      cv=5,\n                      verbose=0) \nlgrid.fit(train_x, train_y)\nprint(lgrid.best_score_)\nprint(lgrid.best_estimator_)\nprint(lgrid.best_params_)","7f9fed6b":"#LGBM\nparams = {\n    'num_leaves': [5,10,30],\n    'min_child_samples': [5,10,30],\n    'n_estimators':[500,1000,2000]\n}\nlgrid = GridSearchCV(estimator=lgb.LGBMRegressor(),           \n                      param_grid=params, \n                      cv=5,\n                      verbose=0) \nlgrid.fit(train_x, train_y)\nprint(lgrid.best_score_)\nprint(lgrid.best_estimator_)\nprint(lgrid.best_params_)","2bae996e":"#LGBM\nparams = {\n    'num_leaves': [10,20],\n    'min_child_samples': [3,4,5],\n    'n_estimators':[300,500]\n}\nlgrid = GridSearchCV(estimator=lgb.LGBMRegressor(),           \n                      param_grid=params, \n                      cv=10,\n                      verbose=0) \nlgrid.fit(train_x, train_y)\nprint(lgrid.best_score_)\nprint(lgrid.best_estimator_)\nprint(lgrid.best_params_)","643253ab":"fmodel=RandomForestRegressor(max_depth=1500, min_samples_split=5, n_estimators=500)\nfmodel.fit(train_x_nonna,train_y)","d8e427df":"from yellowbrick.features import FeatureImportances\nfig,ax=plt.subplots(figsize=(10,10))\nfi_viz=FeatureImportances(fmodel)\nfi_viz.fit(train_x,train_y)\nfi_viz.poof()","19bad745":"train_x_nonna.columns","49182276":"ftrim1=train_x_nonna.drop(columns=['is_new_development', 'has_ac',\n       'has_fitted_wardrobes', 'has_pool', 'has_terrace', 'has_balcony',\n       'has_storage_room', 'is_accessible', 'has_green_zones', \n       'house_type_id_HouseType 1: Pisos',\n       'house_type_id_HouseType 2: Casa o chalet',\n       'house_type_id_HouseType 4: D\u00faplex',\n       'house_type_id_HouseType 5: \u00c1ticos', 'district_id_1', 'district_id_10',\n       'district_id_11', 'district_id_12', 'district_id_14',\n       'district_id_15', 'district_id_17', 'district_id_18', 'district_id_19',\n       'district_id_2', 'district_id_20', 'district_id_4',\n       'district_id_5', 'district_id_6', 'district_id_7', 'district_id_8',\n       'district_id_9'])\nftrim1.columns","e1f8caee":"ftrim2=train_x_nonna.drop(columns=[\n       'house_type_id_HouseType 1: Pisos',\n       'house_type_id_HouseType 4: D\u00faplex',\n       'house_type_id_HouseType 5: \u00c1ticos', 'district_id_10',\n       'district_id_11',  'district_id_14',\n       'district_id_15', 'district_id_17', 'district_id_18', 'district_id_19',\n       'district_id_2', 'district_id_5',  'district_id_8','district_id_9'])\nftrim2.columns","38ca8f6f":"kfold=model_selection.KFold(n_splits=10)\nfmodelshort1=model_selection.cross_validate(RandomForestRegressor(max_depth=1500, min_samples_split=5, n_estimators=500),ftrim1,train_y,scoring=scores,cv=kfold)\nmse=fmodelshort1['test_neg_mean_squared_error'].mean()*-1\nrmse=math.sqrt(mse)\nprint(\n        f\"Random Forest Trim 1 \"\n        f\"{fmodelshort1['fit_time'].mean():.3f}s  R2: \"\n        f\"{fmodelshort1['test_r2'].mean():.3f}  MAE: \"\n        f\"{fmodelshort1['test_neg_mean_absolute_error'].mean()*-1:.3f}  RMSE: \"\n        f\"{rmse:.3f}\"\n    )\nfmodelshort2=model_selection.cross_validate(RandomForestRegressor(max_depth=1500, min_samples_split=5, n_estimators=500),ftrim2,train_y,scoring=scores,cv=kfold)\nmse=fmodelshort2['test_neg_mean_squared_error'].mean()*-1\nrmse=math.sqrt(mse)\nprint(\n        f\"Random Forest Trim 2 \"\n        f\"{fmodelshort2['fit_time'].mean():.3f}s  R2: \"\n        f\"{fmodelshort2['test_r2'].mean():.3f}  MAE: \"\n        f\"{fmodelshort2['test_neg_mean_absolute_error'].mean()*-1:.3f}  MSE: \"\n        f\"{rmse:.3f}\"\n    )\n    ","0a4b3de2":"train_x_nonna = train_x_nonna.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\ntest_x_nonna=test_x_nonna.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\nlmodel=lgb.LGBMRegressor(min_child_samples=4, n_estimators=300, num_leaves=20)\nlmodel.fit(train_x_nonna,train_y)\nlmodel.score(test_x_nonna,test_y)","84af93e5":"xmodel=xgb.XGBRegressor(learning_rate=0.1, max_depth=10, n_estimators=70)\nxmodel.fit(train_x,train_y)\nxmodel.score(test_x,test_y)","b49eafc6":"kmodel=KNeighborsRegressor(n_neighbors=7,p=2,weights='distance')\nkmodel.fit(train_x_std,train_y)\nkmodel.score(test_x_std,test_y)","7495cd83":"fmodel=RandomForestRegressor(max_depth=1200, max_features='auto', min_samples_split= 5, n_estimators=100)\nfmodel.fit(train_x_nonna,train_y)\nfmodel.score(test_x_nonna,test_y)","79c70700":"sq_mt_built & n_bathrooms are two highly correlated values with very few missing values, so we'll drop all the rows that don't contain this value.","5b68b32a":"Before we continue, we need to transform our test set the way we transformed our training set.\n* Drop columns we didn't end up using\n* Drop rows based on nan's in sq_mt_built & n_bathrooms\n* Fill na with false for those columns that used nan instead of false\n* Fill na with mode for the others\n* Fill blank house type with misc\n* Split neighborhood and district (drop neighborhood)\n* Get dummies house type and district","66e383dd":"So our absolute best scoring model is the Random Forest model, coming in with an R2 score of 0.894. Not too bad!","2732ef74":"For variables that are co-related, we'll take the ones with the least missing values:\n* sq_mt_built included, sq_mt_allotment & sq_mt_useful removed\n* has_parking included, parking_price & is_parking_included_in_price removed\n\nAnd since n_floors has so many missing, we'll get rid of that as well.","8a58276f":"And one final run","dcf2305f":"So with that, here are our parameters, ranked in order of highest score:\n* Random Forest: (max_depth=1500, min_samples_split=5, n_estimators=500)\n* XGBoost:  0.874 {'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 70}\n* Light GBM: 0.873 {'min_child_samples': 4, 'n_estimators': 300, 'num_leaves': 20}\n* KNN:  0.834 {'n_neighbors': 7, 'p': 2, 'weights': 'distance'}","8402869d":"I'm going to cut out:\n* Fully empty rows\n    * has_public_parking\n    * portal\n    * are_pets_allowed\n    * is_furnished\n    * is_kitchen_equipped\n    * rent_price_by_area\n    * latitude\n    * longitude\n    * has_private_parking\n    * door\n* Rows that won't help us get an estimate (half are not numbers\/categories, and the is_known will be filtered by us, making this column irrelevant)\n    * title\n    * street_number\n    * street_name\n    * raw_address\n    * is_exact_address_hidden\n    * is_rent_price_known\n    * is_buy_price_known","82559007":"Every house has these values as NaN, and it makes sense because they're apartment features. For now I'm going to just keep this in mind and move on to some easier-to-manage tasks.\n\nLet's one-hot encode our house_type_id values.\nWe have to drop one category so they're not fully interdependent (dummy variable trap); I'll drop the misc category since we created it, anyway.","703c5e88":"One final round to really hone in on it, add some new parameters, and add to the kfolds to make sure.","309c77c4":"The energy columns add a lot of variables, are something I don't overtly care about, and don't appear to have any predictive value, so I'm going to remove them. ","b47a0e6e":"500 and above seems about the same, so I'll try some values 500 and lower to see if they're the same (in hopes of reducing processing time a little). I'll also add in a few more parameters.","6645cfdd":"Can we get a better score if we limit features to most important ones?\n\nLinear Regression does well, and is fast. I'll use it to do a recursive feature elimination on this to see if there are any unimportant variables in our model.","26a2372f":"Finally, even though all of the features were important, I want to graph feature importance and see what dropping some of the lesser values might do for our model.","609c00aa":"Plans:\n* Split neighborhood into neighborhood and district\n* One-hot encode for the following categorical values:\n    * housetype ID\n    * energy certificate\n    * neighborhood\n    * district","d990b7d2":"Which variables have the highest correlation with our target variable?","651b28be":"Now we don't touch our test set until we've finished training and tuning our model.","e04109b7":"Here's the final heatmap to get an idea of what variables are important, as well as any that might be colinear:","06eb4266":"Finally, the most flexible ones that allow for anything (nonstandard and missing values)","89b314f8":"Apparently there is no rent price, so we should focus on buy price only.","8c8e1940":"One last run for further narrowing. And I'll add back in the 2,000 estimators option to see if it can help my final parameters.","757788a1":"Now where are we at in terms of missing values?","b1e4f58c":"Here's our data frame:","dbadb160":"Last bit: get dummies; Neighborhood ID can probably be better approximated with district (far less values), so we'll remove neighborhood ID. And again, for dummies we remove one column to avoid full interdependence, so we remove the last district id.","ae58fda4":"Now we have the range for the other two, so we can look around those values to hone in on them (we'll keep our n_estimators as we found it since we've gotten pretty close by now).","32507077":"I'm going to try one cut where I remove everything below is_new_development, and then one where I remove everything below n_rooms, and see if it helps any.","b96a44cb":"Let's get back to dealing with new_development, is_exterior, and has_lift, which we modified earlier but didn't do a final missing values check\/modification.","def97749":"Ranking our models:\n1. XGB\n2. Random Forest\n3. LGBM\n4. KNN\n5. Linear Regression\n6. Decision Tree\n7. SVM","bd79d3c9":"has_garden appears to only occur in cases where we have a house, so if that's true (or mostly true), we'll get rid of that because it's colinear.\nI'm also going to drop \"is_renewal_needed\" because it doesn't seem to have a huge effect and I'm not quite sure what it means (cannot find info in original data set).","8f3f611e":"The distribution of the target variable is not normal; it has a positive skew and peakendness.","b4e3d5cd":"Now we update params at try again","9853069b":"First, new development fill with mode.","616696d5":"\nTime to do a final run of these models with the test set that we originally made to establish my final model performance.","7ce6d999":"So it's easy to see here that the number of neighbors should be somewhere in the magnitude of 10, not 100 or 1000. We'll use values in this range to test our model and then test a few additional parameters as well.","fb3669ab":"Data from: https:\/\/www.kaggle.com\/mirbektoktogaraev\/madrid-real-estate-market\nHuge thanks to all of your hard work!","6deae37b":"We'll continue with some logical missing value filling:\n* Fill blanks in house type id with a 'misc' category\n* If it has a built year, it can't be a new development, because it's already built\n* Then we can get rid of built year, it has too many missing\n","91051612":"For exterior and lift, fill with mode, except for house, which should be NaN (if our model can't handle missing values, we'll drop these two rows).","0c4ea813":"Next we'll split the neighborhood column into neighborhood and district (separate columns).","2ec322e5":"Buy price is vastly different than rent price. Let's look at them separately.","a39282a2":"Onto the random forest, where we'll do something similar. Here, n_estimators is the big one (increases cause huge changes in processing time) so we'll start with just modifying that","d01478e1":"And the final run:","59d690d0":"Our NaN values for lift & exterior might be NaN when it's a house, which makes sense, as they're mostly apartment features, so let's see if breaking the missing values into categories makes a difference","8f0d3028":"Let's create a version of x that has no missing values, and a version of x that is standardized.\nAgain, we train our standardizer on the training values, and then apply it to the testing values.","258d5967":"Now, models that can be non-standardized, but cannot have missing values","021c6fa0":"All of the features add something, so we'll keep them in for now.\n\nWe can also optimize using gridsearch. But I have no idea what range any of the paramters should be, so first I'll run one to get an idea of the scale we want, and then hone in on values in the second gridsearch iteration. Then I'll perform a final go where I use a higher number of crossfolds and a low number of varying parameters. It would be great if I could do this all at once but I don't have a supercomputer on me so it'll have to do.","1341a4e1":"It appears there are a LOT of columns with missing values. Because there are so many useless columns I'm going to start with dropping the empty\/near empty ones","65ff7803":"Removing these variables actually makes my model perform worse. \n","d4a353b5":"Essentially, we used our training set to establish our preprocessing steps. So we can now apply these to our testing set.\n\n\"We only want to impute on the training set, and use that imputer to fill in for the test set.\"\n\nSo as long as we follow the SAME STEPS, we aren't leaking data. But that means that when we impute from mode, we ahve to use the TRAINING set mode, NOT the testing set mode.","3d172ff9":"Finally, LGBM","f3baf8c5":"Version note: I modified my process a bit to avoid any data leakage and to add in a few new machin learning models. So the final score is a bit lower than it was previously but ensures no leakage or overfitting, therefore resulting in a better model.","d2a69b69":"First, models that require a standardized input.","445f500d":"And we run it again in that range.","25210d35":"Proceeding onward with XGBoost.","79b1f590":"We learned: \n* Operation can be removed because it just has one value and it isn't giving information\n* The following variables only have True, which means NaN is (probably) False and we can replace it with False:\n    * has_ac\n    * has_fitted_wardrobes\n    * has_lift\n    * is_exterior\n    * has_garden\n    * has_pool\n    * has_terrace\n    * has_balcony\n    * has_storage_room\n    * is_accessible\n    * has_green_zones\n    \nSo let's take a look at missing values again and trim it further","848495ae":"Something I'm curious about is if some of these have NaN instead of false, so a describe might help me determine what else needs cutting or just changing.","aad3a6ce":"There are a huge number of missing values for everything above \"floor\", so we'll get rid of most of them.\n\nWe also don't really care about the floor or is_floor_under, and they're also missing a relatively large number of values, so we'll cut them out, too.\n\nAlso buy price by region is going to be affected by the price of the home; it's too correlated with our target output and so we'll also remove this value.","eb1ac95c":"Let's observe the current status of our missing data.","aa44de70":"We're about to start modifying the data, so now we'll do our train\/test split. We set the random state so that we NEVER will use the test set when evaluting our model (ex: if we re-run this set, we might then let the model see data it didn't see last time, causing leakage)","914417c1":"Let's make a correlation matrix and see what variables might be important and\/or correlated.","fa388329":"Subtitle is basically the same as neighborhood ID, so drop that."}}