{"cell_type":{"c19fc257":"code","94e77e82":"code","bbc628ae":"code","9e033ccb":"code","c41a5946":"code","9df5faf8":"code","47a01a75":"code","de2056f2":"code","b61237bc":"code","8377c5c9":"code","530a8ab3":"code","7f302f62":"code","498f5977":"code","fb3af14b":"code","2f09e0c4":"code","b9c5bf4d":"code","69bab169":"code","3e28c112":"code","92f1fce4":"code","92dbf83c":"code","0fe707b3":"code","939ff706":"code","66861fd8":"code","991021a6":"code","ea1e271d":"code","ff771322":"code","e84c5956":"code","91998bd5":"code","1631909c":"code","be1eb6f4":"code","76af7c16":"code","5f27456f":"code","3214c143":"code","c2a86ddf":"code","a4c063b3":"code","f6124aa4":"code","4a832c72":"code","02f175fc":"code","db83d6b3":"code","fd76e49c":"code","0f50e361":"code","ffde1de9":"code","34e861e2":"code","009ded45":"code","964e465f":"code","8af3e62b":"code","93914f8f":"markdown","06eae80c":"markdown","06f07c66":"markdown","aa697282":"markdown","28cab9b3":"markdown","622aae7d":"markdown","3efd8630":"markdown","0f699f1d":"markdown","0bab37f2":"markdown","9f2d0fa3":"markdown","542acd83":"markdown","af5a9940":"markdown","c39c4e6d":"markdown","4768dffb":"markdown","97789aeb":"markdown","98dd8d7d":"markdown","1f60f14b":"markdown","81506cb8":"markdown","e6ef533c":"markdown","f694d0c2":"markdown","bd69f38f":"markdown","1d7d8048":"markdown","3755caac":"markdown","a4d27a12":"markdown","73239bec":"markdown","40736324":"markdown"},"source":{"c19fc257":"import numpy as np \nimport pandas as pd \n\n%matplotlib inline\nimport matplotlib.pyplot as plt # Visualization \nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport warnings      # to ignore warnings\nwarnings.filterwarnings(\"ignore\")\n\n","94e77e82":"\npath=\"\/kaggle\/input\/graduate-admissions\/Admission_Predict_Ver1.1.csv\"\ndf=pd.read_csv(path)\n#df1=pd.read_csv(path1)\ndf.head()  # Top 5 rows\ndf.tail()  # Bottom 5 rows\ndf.sample(5)  # Random 5 rows\ndf.sample(5) # random fractional numbers rows  of total no of rows","bbc628ae":"import pandas_profiling as pp\npp.ProfileReport(df)  # overview and quick data analysis.","9e033ccb":"print(\"Columns of the data are:\",df.columns)","c41a5946":"# Rename the columns name which have to required change.\ndf=df.rename(columns={'Serial No.':'SerialNo', 'GRE Score':'GRE', 'TOEFL Score':'TOEFL',\n                      'University Rating':'UniversityRating','LOR ':'LOR','Chance of Admit ':'ChanceOfAdmit'})\ndf.columns","9df5faf8":"#Drop the column \"Serial No.\" \ndf=df.drop(\"SerialNo\",axis=1)","47a01a75":"df.isnull().sum()","de2056f2":"df.isnull().values.any() # check the null values in whole of the data set if any.\n","b61237bc":"missing_data=df.isnull()\nfor column in missing_data.columns.values.tolist():\n    print(column)\n    print(missing_data[column].value_counts())","8377c5c9":"def missing_percentage(data):\n    total=data.isnull().sum().sort_values(ascending=False)\n    percent=np.round(total\/len(data)*100,2)\n    return pd.concat([total,percent],axis=1,keys=[\"Total\",\"Percent\"])\nmissing_percentage(df)","530a8ab3":"df.duplicated().any()","7f302f62":"df.info()","498f5977":"\nprint(\"Shape of the data\",df.shape)\ndf.describe()","fb3af14b":"# Groupby the data by \"University rating\".\ndf.groupby(\"UniversityRating\").mean()","2f09e0c4":"print(\" Minimum requirements for more than 85% chance to get admission.\\n\",df[(df['ChanceOfAdmit']>0.85)].min())\n","b9c5bf4d":"df.pivot_table(values=['GRE','TOEFL'],index=['UniversityRating'],columns='Research',aggfunc=np.median)\n","69bab169":"plt.figure(figsize=(15,15))\ndf['ChanceOfAdmit'].value_counts().plot.bar()","3e28c112":"#relashionship between the variables of the data in scatter form.\npd.plotting.scatter_matrix(df,figsize=(15,20)) # Scatter matrix for the data.\nplt.show()","92f1fce4":"sns.pairplot(df,hue=\"ChanceOfAdmit\")","92dbf83c":"df.hist(figsize=(10,10),edgecolor=\"y\")\nplt.tight_layout()\nplt.show()","0fe707b3":"plt.figure(figsize=(15,12))\ncol_list=df.columns\n \nfor i in range(len(df.columns)):\n    plt.subplot(3,3,i+1)\n    plt.hist(df[col_list[i]],edgecolor=\"w\")\n    plt.title(col_list[i],color=\"g\",fontsize=15)\n\n\nplt.show()","939ff706":"\n#Boxplot for all variables.\n\"\"\"for col in df.columns:\n    df[[col]].boxplot()\n    plt.show()\"\"\"\ndf.plot(kind='box',subplots=True,layout=(3,3),grid=True,figsize=(8,8))\nplt.tight_layout()\n\nplt.show()\n\n\n","66861fd8":"# Correlation Between the data features.\ndf.corr()","991021a6":"#heatmap of the correlation of the data variables.\nmask = np.zeros_like(df.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nplt.figure(figsize=(12,12))\nsns.heatmap(df.corr(),mask=mask,annot=True,linewidths=1.0)\nplt.show()","ea1e271d":"\nsns.pairplot(df,x_vars=['GRE','TOEFL','UniversityRating','CGPA','SOP','LOR','Research'],\n             y_vars='ChanceOfAdmit')\nplt.tight_layout()","ff771322":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.linear_model import LinearRegression","e84c5956":"X=df.iloc[:,:-1]\nY=df.iloc[:,-1]\nx_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.20,random_state=2)","91998bd5":"sc=StandardScaler()\nX_train=sc.fit_transform(x_train)\nX_test=sc.fit_transform(x_test)\n","1631909c":"lr=LinearRegression()\nlr.fit(X_train,y_train)\n\n#Testdata score\ny_pred=lr.predict(X_test)\nr2_score(y_test,y_pred),mean_squared_error(y_test,y_pred)","be1eb6f4":"#Training data score\nytrain_pred=lr.predict(X_train)\nr2_score(y_train,ytrain_pred),mean_squared_error(y_train,ytrain_pred)\n","76af7c16":"\nprint(\"Intercept of Linear Regression is:\\n,\",lr.intercept_,\"Coefficients of Linear Regression are:\\n,\",lr.coef_)","5f27456f":"from sklearn.ensemble import RandomForestRegressor\nrf_model = RandomForestRegressor(n_estimators = 100,random_state = 42,criterion=\"mse\")\nrf_model.fit(X_train,y_train)\ny_pred_rf=rf_model.predict(X_test)\nr2_score(y_test,y_pred_rf),mean_squared_error(y_test,y_pred_rf)\n","3214c143":"feature_importance = pd.DataFrame(rf_model.feature_importances_, X.columns)\nfeature_importance","c2a86ddf":"from sklearn.tree import DecisionTreeRegressor\ndt = DecisionTreeRegressor(random_state = 4,max_depth=4)\ndt.fit(X_train,y_train)\ny_pred_dt = dt.predict(X_test) \nprint(r2_score(y_test,y_pred_dt),mean_squared_error(y_test,y_pred_dt))\n    ","a4c063b3":"dt","f6124aa4":"from sklearn.tree import DecisionTreeRegressor\ndt = DecisionTreeRegressor(random_state = 1)\ndt.fit(x_train,y_train)\ny_pred_dt = dt.predict(x_test) \nr2_score(y_test,y_pred_dt),mean_squared_error(y_test,y_pred_dt)","4a832c72":"#classification\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC,SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n \n\n#regression\nfrom sklearn.linear_model import LinearRegression,Ridge,Lasso,RidgeCV\nfrom sklearn.ensemble import RandomForestRegressor,BaggingRegressor,GradientBoostingRegressor,AdaBoostRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\n\n#model selection\nfrom sklearn.model_selection import train_test_split,cross_validate\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n#preprocessing\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler,Imputer,LabelEncoder\n\n#evaluation metrics\nfrom sklearn.metrics import mean_squared_log_error,mean_squared_error, r2_score,mean_absolute_error # for regression\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score ","02f175fc":"models=[LinearRegression(),\n        RandomForestRegressor(n_estimators=100,max_depth=4),\n        DecisionTreeRegressor(random_state=4,max_depth=4),GradientBoostingRegressor(),AdaBoostRegressor(),\n        KNeighborsRegressor(n_neighbors=31),\n        BaggingRegressor(),Ridge(alpha=1.0),RidgeCV(),SVR()]\nmodel_names=['LinearRegression','RandomForestClassifier','DecisionTree','GradientBoostingClassifier','AdaBoost','kNN',\n             'BaggingReg','Ridge','RidgeCV',\"SVR\"]\n\nR2_SCORE=[]\nMSE=[]\n      \nfor model in range(len(models)):\n    print(\"*\"*35,\"\\n\",model_names[model])\n    reg=models[model]\n    reg.fit(X_train,y_train)\n    pred=reg.predict(X_test)\n    r=r2_score(y_test,pred)\n    mse=mean_squared_error(y_test,pred)\n    R2_SCORE.append(r)\n    MSE.append(mse)\n    print(\"R2 Score\",r)\n    print(\"MSE\",mse)\n","db83d6b3":"df_model=pd.DataFrame({'Modelling Algorithm':model_names,'R2_score':R2_SCORE,\"MSE\":MSE})\ndf_model=df_model.sort_values(by=\"R2_score\",ascending=False).reset_index()\nprint(df_model)\n\n\nplt.figure(figsize=(10,10))\nsns.barplot(y=\"Modelling Algorithm\",x=\"R2_score\",data=df_model)\n\nplt.xlim(0.35,0.95)\nplt.grid()\nplt.tight_layout()","fd76e49c":"df_model.head(4)","0f50e361":"\nlr=LinearRegression()\nlr.fit(X_train,y_train)\ny_lr_pred=lr.predict(X_test)\nprint(\"MSE:\",mean_squared_error(y_test,y_lr_pred),\"R2 SCORE:\",r2_score(y_test,y_lr_pred))","ffde1de9":"ridge=Ridge()\nridge.fit(X_train,y_train)\ny_ridge=ridge.predict(X_test)\nprint(\"MSE:\",mean_squared_error(y_test,y_ridge),\"R2 SCORE:\",r2_score(y_test,y_ridge))","34e861e2":"r_CV=RidgeCV()\nr_CV.fit(X_train,y_train)\ny_rCV=r_CV.predict(X_test)\nprint(\"MSE:\",mean_squared_error(y_test,y_rCV),\"R2 SCORE:\",r2_score(y_test,y_rCV))\n","009ded45":"tp=pd.DataFrame({\"TEST_value\":y_test,\"LR_predict_value\": y_lr_pred,\"RIDGE_predict_value\": y_ridge,\"RCV_predict_value\": y_rCV,\"DIFF(TEST_value-LR_predict_value)\": (y_test-y_lr_pred)})\ntp.head()","964e465f":"plt.figure(figsize=(10,10),dpi=75)\nx=np.arange(len(tp[\"TEST_value\"]))\ny=tp[\"TEST_value\"]\nz=tp[\"LR_predict_value\"]\nplt.plot(x,y)\nplt.plot(x,z,color='r')\n","8af3e62b":"print(\"Score of Linear Regression:\",r2_score(y_test,y_lr_pred))","93914f8f":"**Load the data.**\n> Load the data and look into  sample of the data.","06eae80c":"**If you find this kernal helpful and useful, Kindly comments your suggestion and upvote the kernal.\n**\n\n## THANKYOU","06f07c66":"\n\n\nAll columns in the data and shape of the data(columns,rows).We will list all the columns for all data by * df.columns *. We will check all columns, are there any spelling mistake?\nIf we found any spelling mistake we will correct it.","aa697282":"Same matrix plot can be plot as below:-","28cab9b3":"**Bar Plot**","622aae7d":"CGPA  is the most important feature to determine Chance of admission.","3efd8630":"**Heatmap of the correlation**","0f699f1d":" Minimum requirements for more than 85% chance of the admission.\n \n\n\tGRE Score\t        320.00\n\tTOEFL Score \t    108.00\n    University Rating\t2.00\n\tSOP\t               3.00\n\tLOR                3.00\n   \tCGPA\t          8.94\n\tResearch        \t0.00\n\tChance of Admit \t0.86","0bab37f2":"**Correlation Between the data features**","9f2d0fa3":" average requirements of all features  to get admission for all universities on the basis of their Ratings.","542acd83":"We will fit the various models with training data ,predict target feature values with test data and will find the which model provide the best R2 Value and MSE value, so that our prediction are more likely to true values.","af5a9940":"**Pairplot**\n\nScatter diagram between the variables of the data and \"Chance of Admit\".","c39c4e6d":" ### Random Forest Regressor","4768dffb":"**Most useful Machine Learning Libraries import**","97789aeb":"**Histogram**\n> Distribution Of the data by visualise the histograms for all features of the data.","98dd8d7d":"![](http:\/\/)Lets found shape of the data and some important statistical summary of the data e.g. Mean,std,minimum , maximum value, 25,50,75 pecentiles of the data.","1f60f14b":"There is no null values . It can be found in the following way.\n                  \n> df.info() --> give the information about the data as Index columns, datatypes of the variables,null values,memory used by data.","81506cb8":"So Best R2 are for the 4 models:-\n1. Linear Regressioin\n2. Ridge Regression\n3. RidgeCV Regression\n4. KNN\n\n      ","e6ef533c":"* Decision tree model without standardised data.","f694d0c2":"### DecisionTreeRegressor","bd69f38f":"**Box Plots for all features**","1d7d8048":"### Data Preprocessing-\n\n* SPLIT the data into train and test data.\n","3755caac":"Import the important libraries for machine learning.","a4d27a12":"** Pivot Table **","73239bec":"\n\nCheck the sum of null values in every column.","40736324":"### Linear Regression Model\n* Fit the model\n* Find the predicted Values with the test data  applied on model.\n* R2 value and Mean Square Error with test data and predicted data."}}