{"cell_type":{"5bb25a94":"code","65f2a66c":"code","a15e4654":"code","8f87d04e":"code","49ef7b5a":"code","b144f6b5":"code","ed1f97d0":"code","c0fde8e5":"code","14b0d178":"code","88103014":"code","5923ca36":"code","a4ea2fe2":"code","0ab004fb":"markdown","3d21eb91":"markdown"},"source":{"5bb25a94":"import numpy as np\nimport pandas as pd\nimport pyarrow.parquet as pq\nimport random\nrandom.seed(1)\n\nimport os\nimport sys\nimport gc\nprint(os.listdir(\"..\/input\"))\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\n\nimport lightgbm as lgb\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport re\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn import metrics","65f2a66c":"%%time\n# 1min in Kernel\nmeta_train = pd.read_csv('..\/input\/metadata_train.csv')\nsubset_train = pq.read_pandas('..\/input\/train.parquet', columns=[str(i) for i in range(8712)]).to_pandas()","a15e4654":"%%time\n# 20s in Kernel\ntrain_length = 8712 #max 8712\npositive_length = len(meta_train[meta_train['target']==1])\ntrain_df = pd.DataFrame()\nrow_index = 0\nfor i in range(train_length):\n    # downsampling\n    if meta_train.loc[i,'target'] == 1 or random.random() < positive_length \/ train_length:\n        subset_train_row = subset_train[str(i)]\n        train_df.loc[row_index, 'signal_min'] = subset_train_row.min()\n        train_df.loc[row_index, 'signal_max'] = subset_train_row.max()\n        train_df.loc[row_index, 'signal_mean'] = subset_train_row.mean()\n        # *** Add your feature here ***\n        train_df.loc[row_index, 'signal_id'] = i\n        row_index += 1\nprint(\"positive length: \" + str(positive_length))\n# positive length 525\nprint(\"train length: \" + str(len(train_df)))\n# train length 1038  example\n# This will be about 1050","8f87d04e":"train_df = pd.merge(train_df, meta_train, on='signal_id')\ntrain_df.to_csv(\"train.csv\", index=False)\ntrain_df.head()","49ef7b5a":"# From https:\/\/www.kaggle.com\/delayedkarma\/lightgbm-cv-matthews-correlation-coeff\n# Thank you!!\n# If you have preprocessed data, input here and delete process method.\n# x_train = pd.read_csv('..\/input\/***\/train.csv')\nx_train = train_df\ntarget = x_train['target']\ninput_target = x_train['target']\nx_train.drop('target', axis=1, inplace=True)\nx_train.drop('signal_id', axis=1, inplace=True)\nfeatures = x_train.columns\nparam = {'num_leaves': 80,\n         'min_data_in_leaf': 60, \n         'objective':'binary',\n         'max_depth': -1,\n         'learning_rate': 0.1,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.8,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.8 ,\n         \"bagging_seed\": 11,\n         \"metric\": 'auc',\n         \"lambda_l1\": 0.1,\n         \"random_state\": 133,\n         \"verbosity\": -1}\nmax_iter=5","b144f6b5":"folds = KFold(n_splits=5, shuffle=True, random_state=15)\noof = np.zeros(len(x_train))\nfeature_importance_df = pd.DataFrame()\nscore = [0 for _ in range(folds.n_splits)]\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(x_train.values, target.values)):\n    print(\"Fold No.{}\".format(fold_+1))\n    trn_data = lgb.Dataset(x_train.iloc[trn_idx][features],\n                           label=target.iloc[trn_idx])\n    val_data = lgb.Dataset(x_train.iloc[val_idx][features],\n                           label=target.iloc[val_idx])\n    num_round = 10000\n    clf = lgb.train(param,\n                    trn_data,\n                    num_round,\n                    valid_sets = [trn_data, val_data],\n                    verbose_eval=100,\n                    early_stopping_rounds = 100)\n    \n    oof[val_idx] = clf.predict(x_train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance(importance_type='gain')\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    score[fold_] = metrics.roc_auc_score(target.iloc[val_idx], oof[val_idx])\n    if fold_ == max_iter - 1: break\nif (folds.n_splits == max_iter):\n    print(\"CV score: {:<8.5f}\".format(metrics.roc_auc_score(target, oof)))\nelse:\n     print(\"CV score: {:<8.5f}\".format(sum(score) \/ max_iter))","ed1f97d0":"cols = (feature_importance_df[[\"feature\", \"importance\"]]\n        .groupby(\"feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:1000].index)\nbest_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n\nplt.figure(figsize=(10,10))\nsns.barplot(x=\"importance\",\n            y=\"feature\",\n            data=best_features.sort_values(by=\"importance\",ascending=False))\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()","c0fde8e5":" gc.collect()","14b0d178":"%%time\n# 25ms in Kernel\nmeta_test = pd.read_csv('..\/input\/metadata_test.csv')","88103014":"%%time\n# About 10min in Kernel\ntest_df = pd.DataFrame()\nrow_index = 0\nfor i in range(10):\n    subset_test = pq.read_pandas('..\/input\/test.parquet', columns=[str(i*2000 + j + 8712) for j in range(2000)]).to_pandas()\n    for j in range(2000):\n        subset_test_row = subset_test[str(i*2000 + j + 8712)]\n        test_df.loc[row_index, 'signal_min'] = subset_test_row.min()\n        test_df.loc[row_index, 'signal_max'] = subset_test_row.max()\n        test_df.loc[row_index, 'signal_mean'] = subset_test_row.mean()\n        # *** Add your feature here ***\n        test_df.loc[row_index, 'signal_id'] = i*2000 + j + 8712\n        row_index += 1\nsubset_test = pq.read_pandas('..\/input\/test.parquet', columns=[str(i + 28712) for i in range(337)]).to_pandas()\nfor i in tqdm(range(337)):\n    subset_test_row = subset_test[str(i + 28712)]\n    test_df.loc[row_index, 'signal_min'] = subset_test_row.min()\n    test_df.loc[row_index, 'signal_max'] = subset_test_row.max()\n    test_df.loc[row_index, 'signal_mean'] = subset_test_row.mean()\n    # *** Add your feature here ***\n    test_df.loc[row_index, 'signal_id'] = i + 28712\n    row_index += 1\ntest_df = pd.merge(test_df, meta_test, on='signal_id')\ntest_df.to_csv(\"test.csv\", index=False)\ntest_df.head()","5923ca36":"# If you have preprocessed data, input here and delete process method.\n# x_test = pd.read_csv('..\/input\/***\/test.csv')\nx_test = test_df\nx_filename = x_test['signal_id']\nx_test = x_test.drop('signal_id', axis=1)\n\npredictions = clf.predict(x_test, num_iteration=clf.best_iteration)\n\nsub_df = pd.DataFrame({\"signal_id\":x_filename.values})\nsub_df[\"target\"] = pd.Series(predictions).round()\nsub_df['signal_id'] = sub_df['signal_id'].astype(np.int64)\nsub_df['target'] = sub_df['target'].astype(np.int64)\nsub_df.to_csv(\"submission.csv\", index=False)\nsub_df","a4ea2fe2":"positive = len(sub_df[sub_df[\"target\"] == 1])\nprint(positive)\nprint(str(positive\/len(sub_df)*100) + \"%\")","0ab004fb":"In this notebook, I made a simple lightgbm code. \nThis code takes **less than 15min** to finish all process. ","3d21eb91":"Check your CV carefully because\n> You may submit a maximum of 2 entries per day."}}