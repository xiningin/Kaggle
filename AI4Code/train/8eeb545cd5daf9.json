{"cell_type":{"e2bdd670":"code","c9824d76":"code","5d6129e2":"code","bb731002":"code","cc145772":"code","86d3dbb5":"code","dd8bf049":"code","ef94192d":"code","1fd9fb4a":"code","9c904379":"code","4cbb9c11":"code","e356a7ac":"code","9ed6e9a6":"code","1eab9d30":"code","c1061c40":"code","eefd5513":"code","204779e8":"code","d40d2e3f":"code","0edc6009":"code","ff37b10a":"code","544b43e1":"code","e7480354":"code","b9122e94":"code","cbde001c":"code","c88d9a17":"code","071a64a3":"code","c4a7c170":"code","8023e3e8":"code","033be1ae":"code","96d75fda":"markdown","1238a757":"markdown","fcbe9854":"markdown","c077d117":"markdown","b75e2645":"markdown","c76b83b0":"markdown","7219cdf0":"markdown","3cdf1352":"markdown","aedec278":"markdown","c36123fd":"markdown","898c6842":"markdown","3328c1ac":"markdown"},"source":{"e2bdd670":"import numpy as np \nimport pandas as pd\nimport os\nimport glob\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport itertools\nimport seaborn as sns\nfrom pandas_profiling import ProfileReport","c9824d76":"os.listdir('\/kaggle\/input\/indoor-location-navigation')","5d6129e2":"train_dir = \"..\/input\/indoor-location-navigation\/train\"\ntest_dir = \"..\/input\/indoor-location-navigation\/test\"\nmeta_dir = \"..\/input\/indoor-location-navigation\/metadata\"\nss = \"..\/input\/indoor-location-navigation\/sample_submission.csv\"","bb731002":"train_names = ['Time', 'Type'] + ['reading_'+str(x)+'_' for x in range(1,9)]\ntest_names = ['Time', 'Type'] + ['reading_'+str(x)+'_' for x in range(1,9)]\n\ntrain_files = glob.glob(os.path.join(train_dir, \"**\/*.txt\"), recursive=True)\ntest_files = glob.glob(os.path.join(test_dir, \"**\/*.txt\"), recursive=True)\ntrain_files = train_files[0:9]\ntest_files = test_files[0:9]\n\ndef find_floor(FloorName):\n    floor_type = FloorName[:1]\n    floor_number = int(FloorName[1:])\n    if floor_type == 'B':\n        floor_level = -floor_number\n    elif floor_type == 'F':\n        floor_level = floor_number-1\n    else:\n        floor_level = -99\n    return floor_level\n\ndef read_files(files, names):\n    full_df = pd.DataFrame(columns= ['WalkID', 'SiteID', 'Floor']+names)\n    for file in files:\n        file_df = pd.read_csv(file, sep='\\t', comment='#', header=None, names=names) \n        file_df['WalkID'] = files.index(file)\n        deets = train_files[0].split(\"\/\")\n        file_df['SiteID'] = deets[4]\n        file_df['Floor'] = find_floor(deets[5])\n        full_df = full_df.append(file_df)\n    full_df.replace(0, np.nan, inplace=True)\n    return full_df\n\nraw_train_df = read_files(train_files, train_names)\nraw_test_df = read_files(test_files, test_names)\nsample_submission = pd.read_csv(ss)\n\n\n# additional data:\n# start time\n# floor name\n# phone details\n# sensor details","cc145772":"#file_df = pd.read_csv(train_files[0], sep='\\t', comment='#', header=None, names=train_names)\n#file_df[file_df['Type']=='TYPE_WAYPOINT']","86d3dbb5":"#with open(train_files[0], \"r\") as fh:\n#    for line in fh.readlines():\n#        print(line)\n#    fh.close()","dd8bf049":"floor_images = glob.glob(os.path.join(meta_dir, \"**\/*.png\"), recursive=True)\nfloor_info = glob.glob(os.path.join(meta_dir, \"**\/floor_info.json\"), recursive=True)\nGeoMaps = glob.glob(os.path.join(meta_dir, \"**\/geojson_map.json\"), recursive=True)\n                                      \nprint(\"Number of Floor Images in Meta Data: \", len(floor_images))\nprint(\"Number of Floor Info(in JSON) in Meta Data: \", len(floor_info))\nprint(\"Number of Geo Map (in JSON) in Meta Data: \", len(GeoMaps))","ef94192d":"for _ in range(5):\n    img = Image.open(floor_images[np.random.randint(0, len(floor_images))])\n    display(img)","1fd9fb4a":"def read_txt(txt_path):\n    # ignore lines starting with # because they contain meta-data sort of thing\n    with open(txt_path, 'r') as fh:\n        unique_keys = []\n        for line in fh.readlines():\n            if line.startswith(\"#\"):\n                dummy = line.split(\"\\n\")[0].split(\"\\t\")\n                unique_keys.extend(list(map(lambda x: '' if x==\"#\" else x, dummy)))\n            else:\n                pass\n        fh.close()\n    return unique_keys\n    pass\n\nread_txt(train_files[0])","9c904379":"raw_train_df.groupby('Type').agg({'Time': 'count'})","4cbb9c11":"null_counts = raw_train_df.isnull().groupby([raw_train_df['Type']]).sum().astype(int)\nprint (null_counts)\n# should be no nulls for 'TYPE_MAGNETIC_FIELD_UNCALIBRATED' ?\n#raw_train_df[raw_train_df['Type']=='TYPE_MAGNETIC_FIELD_UNCALIBRATED']","e356a7ac":"null_counts = raw_test_df.isnull().groupby([raw_test_df['Type']]).sum().astype(int)\nprint (null_counts)","9ed6e9a6":"raw_test_df.head()","1eab9d30":"sample_submission.head()","c1061c40":"# create clean dataset of paths\n\nwaypoint_df = raw_train_df[raw_train_df['Type'] == 'TYPE_WAYPOINT']\nwaypoint_df['x'] =  pd.to_numeric(waypoint_df['reading_1_'])\nwaypoint_df['y'] = pd.to_numeric(waypoint_df['reading_2_'])\nwaypoint_df = waypoint_df[['WalkID', 'SiteID', 'Time', 'Floor', 'x', 'y']]","eefd5513":"# display example path\n\nexample_id = 6\nexample_path = waypoint_df[waypoint_df['WalkID']==example_id][['x', 'y']].to_numpy()\n\nstart_x = example_path[0, 0]\nstart_y = example_path[0, 1]   \nend_x = example_path[len(example_path)-1, 0]\nend_y = example_path[len(example_path)-1, 1]\n\nfig = plt.figure()\nax = fig.add_subplot(111)\nplt.plot(start_x, start_y, 'go', end_x, end_y, 'ro', example_path[:, 0], example_path[:, 1])\nax.annotate('start', (example_path[0, 0]-0.2, example_path[0, 1]-0.2))\nax.annotate('end', (example_path[len(example_path)-1, 0]+0.2, example_path[len(example_path)-1, 1]+0.2))","204779e8":"# create clean dataset of readings\n\n# raw_train_df.dtypes\n# raw_train_df[raw_train_df['Type']=='TYPE_BEACON'].head(5)\n# train_df_clean.iloc[[738,739,740]]\n# many reading seem to contain SiteIDs or other non numeric values\n\ntrain_df_clean = raw_train_df[raw_train_df['Type'] != 'TYPE_WAYPOINT']\n\n# Convert sensor readings to decimal floats\n\n# TYPE_WIFI readings 1 and 2 are strings for some reason\ntrain_df_clean.loc[train_df_clean['Type']=='TYPE_WIFI', 'reading_1_'] = np.NaN\ntrain_df_clean.loc[train_df_clean['Type']=='TYPE_WIFI', 'reading_2_'] = np.NaN\n# TYPE_BEACON readings 1, 2, 3 and 7 are strings for some reason\ntrain_df_clean.loc[train_df_clean['Type']=='TYPE_BEACON', 'reading_1_'] = np.NaN\ntrain_df_clean.loc[train_df_clean['Type']=='TYPE_BEACON', 'reading_2_'] = np.NaN\ntrain_df_clean.loc[train_df_clean['Type']=='TYPE_BEACON', 'reading_3_'] = np.NaN\ntrain_df_clean.loc[train_df_clean['Type']=='TYPE_BEACON', 'reading_7_'] = np.NaN\ntrain_df_clean.loc[train_df_clean['Type']=='TYPE_BEACON', 'reading_8_'] = np.NaN\n\ntrain_df_clean = train_df_clean.astype({'reading_1_': 'float64', 'reading_2_': 'float64', 'reading_3_': 'float64', 'reading_4_': 'float64', 'reading_5_': 'float64', 'reading_6_': 'float64', 'reading_7_': 'float64', 'reading_8_': 'float64'})\ntrain_df_clean.head(5)","d40d2e3f":"train_df_sparce = pd.pivot_table(train_df_clean, \n                          index=['WalkID', 'SiteID', 'Time', 'Floor'], \n                          columns=['Type'], \n                          values=['reading_1_', 'reading_2_', 'reading_3_', 'reading_4_', 'reading_5_', 'reading_6_', 'reading_7_', 'reading_8_'], \n                          aggfunc={'reading_1_': np.sum, 'reading_2_': np.sum, 'reading_3_': np.sum, 'reading_4_' : np.sum, 'reading_5_': np.sum, 'reading_6_': np.sum, 'reading_7_': np.sum, 'reading_8_': np.sum})\ntrain_df_sparce.reset_index(inplace=True)\ntrain_df_sparce.columns = [''.join(col).strip() for col in train_df_sparce.columns.values]","0edc6009":"null_counts = train_df_sparce.isnull().sum().astype(int)\nprint(null_counts)\n#with pd.option_context('display.max_rows', None, 'display.max_columns',  None):  # more options can be specified also\n#    print(null_counts)","ff37b10a":"#TYPE_WIFI and TYPE_BEACON have too many nulls for all readings (why?) - remove these columns\n\nnan_columns = ['reading_1_TYPE_BEACON', 'reading_1_TYPE_WIFI', 'reading_2_TYPE_BEACON', 'reading_2_TYPE_WIFI', 'reading_3_TYPE_BEACON', 'reading_3_TYPE_WIFI', 'reading_4_TYPE_BEACON', 'reading_4_TYPE_WIFI', 'reading_5_TYPE_BEACON', 'reading_5_TYPE_WIFI', 'reading_6_TYPE_BEACON', 'reading_6_TYPE_WIFI', 'reading_7_TYPE_BEACON', 'reading_7_TYPE_WIFI', 'reading_8_TYPE_BEACON', 'reading_8_TYPE_WIFI']\ntrain_df_sparce.drop(nan_columns, axis=1, inplace=True)\ntrain_df_sparce.rename(columns={\"Time\": \"Reading_Time\"}, inplace=True)\ntrain_df_sparce.head()","544b43e1":"interpolation_data = waypoint_df.merge(train_df_sparce, on=['WalkID', 'SiteID', 'Floor'], how='left')\n#interpolation_data['ID'] = interpolation_data['WalkID'].map(str) + '_' + interpolation_data['SiteID'].map(str)  + '_' + interpolation_data['Floor'].map(str) \n\ninterpolation_data_prev = interpolation_data[interpolation_data['Reading_Time'] <= interpolation_data['Time']]\ninterpolation_data_prev_agg = interpolation_data_prev[['WalkID', 'SiteID', 'Floor', 'Time', 'Reading_Time']].groupby(['WalkID', 'SiteID', 'Floor', 'Time']).agg('max')\ninterpolation_data_prev = interpolation_data_prev.merge(interpolation_data_prev_agg, on=['WalkID', 'SiteID', 'Floor', 'Time', 'Reading_Time'], how='inner')\n\ninterpolation_data_after = interpolation_data[interpolation_data['Reading_Time'] >= interpolation_data['Time']]\ninterpolation_data_after_agg = interpolation_data_after[['WalkID', 'SiteID', 'Floor', 'Time', 'Reading_Time']].groupby(['WalkID', 'SiteID', 'Floor', 'Time']).agg('min')\ninterpolation_data_after = interpolation_data_after.merge(interpolation_data_after_agg, on=['WalkID', 'SiteID', 'Floor', 'Time', 'Reading_Time'], how='inner')","e7480354":"interpolation_data = interpolation_data_prev.merge(interpolation_data_after, on=['WalkID', 'SiteID', 'Floor', 'Time', 'x', 'y'], how='outer')\ninterpolation_data['interpolation_fraction'] = [np.NaN if np.isnan(i) or np.isnan(m) or np.isnan(j) else 1 if j==i else (m-i)\/(j-i) for i,m,j in zip(interpolation_data['Reading_Time_x'], interpolation_data['Time'], interpolation_data['Reading_Time_y'])]\nreading_type_names = ['TYPE_ACCELEROMETER', 'TYPE_ACCELEROMETER_UNCALIBRATED', 'TYPE_BEACON', 'TYPE_GYROSCOPE', 'TYPE_GYROSCOPE_UNCALIBRATED', 'TYPE_MAGNETIC_FIELD', 'TYPE_MAGNETIC_FIELD_UNCALIBRATED', 'TYPE_ROTATION_VECTOR', 'TYPE_WIFI']\nreading_numbers = ['reading_1_', 'reading_2_', 'reading_3_']#, 'reading_4_', 'reading_5_', 'reading_6_', 'reading_7_', 'reading_8_']\nreading_names = [i[0]+i[1] for i in list(itertools.product(reading_numbers, reading_type_names))]\nfor nan_column in nan_columns:\n    if nan_column in reading_names:\n        reading_names.remove(nan_column)\nfor reading_name in reading_names:\n    prev_readings = interpolation_data[reading_name+'_x']\n    after_readings = interpolation_data[reading_name+'_y'] \n    interpolation_data[reading_name] = [j if np.isnan(i) else i if np.isnan(j) is None else i+((j-i)*f) for i,j,f in zip(prev_readings, after_readings, interpolation_data['interpolation_fraction'])]\n","b9122e94":"train_df = interpolation_data[['WalkID', 'SiteID', 'Time', 'Floor', 'x', 'y']+reading_names]\ntrain_df.head()","cbde001c":"waypoint_projection_df = waypoint_df\n\ndef projected_coordinate(prev_prev_point, prev_point, prev_prev_time, prev_time, new_time):\n    if prev_point == pd.NaN:\n        new_point = 0\n    elif prev_prev_point == pd.NaN:\n        new_point = prev_point\n    else:\n        speed = (prev_point-prev_prev_point) \/ (prev_time-prev_prev_time)\n        new_point = prev_point + speed*(new_time-prev_time)\n        \n#waypoint_df['projected_x'] = \n#waypoint_df['projected_y'] = ","c88d9a17":"from matplotlib.pyplot import figure\nfigure(figsize=(20, 5), dpi=80)\nboxplot = train_df.boxplot(column=reading_names)\nboxplot = plt.xticks(rotation=45, ha=\"right\", fontsize=8)","071a64a3":"# select the metrics and factors to correlate\nmetrics = ['Floor', 'x', 'y']\nfactors = reading_names\n#train_df.dtypes","c4a7c170":"# calculate the correlation coefficient map \ncorr_matrix = train_df.corr(method ='spearman')\ncorr_matrix = corr_matrix[metrics].filter(factors, axis = 0)\ncm = sns.diverging_palette(20, 133, sep=20, as_cmap=True)\ncorr_matrix.style.background_gradient(cmap=cm)","8023e3e8":"# plot the correlations\nfor factor in factors:\n    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20,3))\n    axs = [ax1, ax2, ax3]\n    x_data = train_df[factor]\n    for i in range(len(metrics)):\n        metric = metrics[i]\n        y_data = train_df[metric]\n        axs[i].set_title(metric+' vs '+factor)\n        axs[i].plot(x_data, y_data, 'o')","033be1ae":"prof = ProfileReport(train_df)\nprof.to_file(output_file='output.html')\ndisplay(prof)","96d75fda":"| ID Variables | Target Variables | Prediction Variables |\n| --- | --- | --- |\n| siteID    | Floor       | TYPE_ACCELEROMETER_1 |\n| walkID    | x           | TYPE_ACCELEROMETER_UNCALIBRATED_1 | \n| timestamp | y           | TYPE_BEACON_1 |\n|           |             | TYPE_GYROSCOPE_1 |\n|           |             | TYPE_GYROSCOPE_UNCALIBRATED_1 |\n|           |             | TYPE_MAGNETIC_FIELD_1 |\n|           |             | TYPE_MAGNETIC_FIELD_UNCALIBRATED_1 |\n|           |             | TYPE_ROTATION_VECTOR_1 |\n|           |             | TYPE_WIFI_1 |\n|           |             | (projected_x) |\n|           |             | (projected_y) |","1238a757":"# **Data Prep**\n#### wrangling the data into a format that we can use to build a model","fcbe9854":"## Readings Interpolation","c077d117":"# **Model Implimentation**\n#### applying the predictive model(s) to the test data","b75e2645":"# **Data Mining**\n#### exploring the data correlations to inform the model build","c76b83b0":"# **Read Data**\n#### reading in the data files","7219cdf0":"**Additional Ideas:**\nproject path,\nlook at site_path as a whole,\nproject distance from sensor\n\nFor projecting path:\nprevious_x,\nprevious_y,\nprevious_direction,\nprevious_speed\n-->\nprojected_x,\nprojected_y","3cdf1352":"# **Model Builds**\n#### building the predictive model(s)","aedec278":"## Data Cleaning","c36123fd":"The waypoints and readings are gathered at different times, so will split these apart as different data sets and interpolate the readings at each waypoint","898c6842":"## Path Projection","3328c1ac":"# **EDA**\n#### exploring the data available as well as it's health and completeness"}}