{"cell_type":{"a56d6268":"code","12a0990e":"code","ca7754ad":"code","fdb95fc0":"code","a8789c37":"code","c300a135":"code","c503b08b":"code","33b55183":"code","aa7a4c55":"code","cb80d7c4":"code","8b7741d6":"code","da28c831":"code","a5de1e77":"code","3ada5d82":"code","a6b921e3":"code","0cfaaeab":"code","ca6da3a8":"code","466296dd":"code","215b15e7":"code","ad5d2b6c":"code","4e531eaa":"code","4ad67bfe":"markdown","cd9e3bdc":"markdown","c149f988":"markdown","5bcba29e":"markdown","d3822e7b":"markdown","31b87b10":"markdown","981ab885":"markdown","dac1250f":"markdown","a1f99cde":"markdown","235685e5":"markdown","7c279276":"markdown","f327e8d8":"markdown"},"source":{"a56d6268":"# Load necessary libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt  # data visualization\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.np_utils import to_categorical # to convert to one-hot-encoding\nfrom keras.models import Sequential\nfrom keras.layers import Flatten, Dense, Dropout\nfrom keras.optimizers import Adam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.regularizers import l2","12a0990e":"# Load datasets\ntrain, test = pd.read_csv(\"..\/input\/train.csv\"), pd.read_csv(\"..\/input\/test.csv\")","ca7754ad":"# Review data\nprint(f'train data shape = {train.shape}', '\/', f'test data shape = {test.shape}')\ntrain.head()","fdb95fc0":"# let's check the count of different labels in the dataset (~balanced)\ntrain['label'].value_counts()","a8789c37":"# Numpy representation of the train and test data:\ntrain_pixels, test_pixels = train.iloc[:,1:].values.astype('float32'), test.values.astype('float32') # all pixel values\ntrain_labels = train.iloc[:,0].values.astype('int32') # only labels i.e targets digits\ntrain_labels = train_labels.reshape(-1, 1) # ensure proper shape of the array\n\nprint(f'train_pixels shape = {train_pixels.shape}')\nprint(f'test_pixels shape = {test_pixels.shape}')\nprint(f'train_labels shape = {train_labels.shape}')","c300a135":"# Reshape input data to fit Keras model (height=28px, width=28px, channels=1):\ntrain_pixels, test_pixels = train_pixels.reshape(-1, 28, 28, 1), test_pixels.reshape(-1, 28, 28, 1)\nprint(f'train_pixels shape = {train_pixels.shape}')\nprint(f'test_pixels shape = {test_pixels.shape}')","c503b08b":"# Visualize some images from the dataset:\nnrows, ncols = 3, 5  # number of rows and colums in subplots\nfig, ax = plt.subplots(nrows, ncols, sharex=True, sharey=True, figsize=(8,5))\nfor row in range(nrows):\n    for col in range(ncols):\n        i = np.random.randint(0, 30000)  # pick up arbitrary examples\n        ax[row, col].imshow(train_pixels[i,:,:,0], cmap='Greys')\n        ax[row, col].set_title(f'<{train.label[i]}>');","33b55183":"# Input data are greyscale pixels of intensity [0:255]. Let's normalize to [0:1]:\ntrain_pixels, test_pixels = train_pixels \/ 255.0, test_pixels \/ 255.0","aa7a4c55":"# Encode labels to one hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0])\ntrain_labels = to_categorical(train_labels, num_classes = 10)\nprint(f'train_labels shape = {train_labels.shape}')\ntrain_labels","cb80d7c4":"# Split training and validation set for the fitting\ntrain_pixels, val_pixels, train_labels, val_labels = train_test_split(train_pixels, train_labels, test_size = 0.1, random_state=2)\n\ntrain_pixels.shape, train_labels.shape, val_pixels.shape, val_labels.shape, test_pixels.shape","8b7741d6":"# let's fix the important numbers for further modeling:\nm_train = train_pixels.shape[0]   # number of examples in the training set\nm_val = val_pixels.shape[0]       # number of examples in the validation set\nm_test = test_pixels.shape[0]     # number of examples in the test set\nn_x = test.shape[1]               # input size, number of pixels in the image\nn_y = train_labels.shape[1]       # output size, number of label classes\nprint(f\" m_train = {m_train} \/ m_val = {m_val} \/ m_test = {m_test} \/ n_x = {n_x} \/ n_y = {n_y}\")","da28c831":"# Decide on the model architecture: [n_x, hidden_layers, n_y]\nlayer_dims = [n_x, 512, n_y]  # the model architecture is adjustable","a5de1e77":"# create an instance of a neural network:\nk_model = Sequential()\n# the first hidden layer must have input dimensions:\nk_model.add(Flatten(input_shape=[28,28,1]))\nk_model.add(Dense(layer_dims[1], activation='relu',\n                  kernel_regularizer=l2(0)))\nk_model.add(Dropout(0.25))\n# additional hidden layers are optional\n# output layer w\/softmax activation:\nk_model.add(Dense(n_y, activation='softmax',\n                  kernel_regularizer=l2(0)))\n\n# Compile the model w\/Adam optimizer:\nk_model.compile(optimizer=Adam(lr=1e-3),\n                loss='categorical_crossentropy',\n                metrics=['accuracy'])\n\n# Define a learning rate decay method:\nlr_decay = ReduceLROnPlateau(monitor='loss', \n                             patience=1, verbose=0, \n                             factor=0.5, min_lr=1e-8)\n# Train the model:\nk_model.fit(train_pixels, train_labels, epochs=20, batch_size=128,\n            callbacks=[lr_decay], verbose=0)\n\n# Evaluate the model:\nk_train_loss, k_train_acc = k_model.evaluate(train_pixels, train_labels)\nk_val_loss, k_val_acc = k_model.evaluate(val_pixels, val_labels)\n\nprint(f'k_model: train accuracy = {round(k_train_acc * 100, 4)}%')\nprint(f'k_model: val accuracy = {round(k_val_acc * 100, 4)}%')\nprint(f'k_model: val error = {round((1 - k_val_acc) * m_val)} examples')\n","3ada5d82":"# Reshape data to fit the Custom Model architecture: (n_pixels, m_examples)\nX_train = train_pixels.reshape(m_train, -1).T\nY_train = train_labels.T\nX_val   = val_pixels.reshape(m_val,-1).T\nY_val   = val_labels.T\nX_test  = test_pixels.reshape(m_test, -1).T\n\nX_train.shape, Y_train.shape, X_val.shape, Y_val.shape, X_test.shape","a6b921e3":"# Define a function to create random mini-batches for the gradient descent:\ndef random_mini_batches(X, Y, batch_size):\n    \"\"\"\n    This funcion creates a list of random minibatches from (X, Y)\n    Arguments:\n        X -- input data, of shape (input size, number of examples)\n        Y -- \"true\" labels vector, of shape (output size, number of examples)\n        batch_size -- size of mini-batches, integer\n    Returns:\n        mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n    \"\"\"\n    m = X.shape[1]              # number of examples\n    mini_batches = []           # initialize a list to contain all minibatches\n        \n    # Shuffle (X, Y)\n    permutation = list(np.random.permutation(m))\n    shuffled_X = X[:, permutation]\n    shuffled_Y = Y[:, permutation]\n\n    # Partition (shuffled_X, shuffled_Y). Minus the end case.\n    num_complete_minibatches = m \/\/ batch_size # number of minibatches of size batch_size\n    for k in range(0, num_complete_minibatches):\n        mini_batch_X = shuffled_X[:, k*batch_size:(k+1)*batch_size]\n        mini_batch_Y = shuffled_Y[:, k*batch_size:(k+1)*batch_size]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    # Handling the end case (last mini-batch < mini_batch_size)\n    if m % batch_size != 0:\n        mini_batch_X = shuffled_X[:, num_complete_minibatches*batch_size:]\n        mini_batch_Y = shuffled_Y[:, num_complete_minibatches*batch_size:]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    return mini_batches","0cfaaeab":"class Custom_model(object):\n    \n    def __init__(self, layer_dims):\n        \"\"\" \n        The model consists of the input layer (pixels), a number of hidden layers and\n        the output layer (categorical classifier). To create an instance of the model, we set\n        dimensions of its layers and initialize parameters for the hidden\/ output layers.\n        Arguments: \n            layer_dims -- list containing the input size and each layer size\n        Returns: \n            parameters -- python dictionary containing parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n                Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n                bl -- bias vector of shape (layer_dims[l], 1) \n        \"\"\"\n        self.layer_dims = layer_dims       # a list with dimensions of all layers\n        self.num_layers = len(layer_dims)  # number of layers (with input layer)\n        self.parameters = {}        # a dictionary with weights and biases of the model\n        # Initializing weights randomly (He initialization) and biases to zeros\n        for l in range(1, len(layer_dims)):\n            self.parameters[f\"W{l}\"] = np.random.randn(layer_dims[l], \n                                                       layer_dims[l-1])*np.sqrt(2.\/layer_dims[l-1])\n            self.parameters[f\"b{l}\"] = np.zeros((layer_dims[l], 1))\n    \n    # define getters and setters for accessing the model class attributes:        \n    def get_layer_dims(self):\n        return self.layer_dims\n    def get_num_layers(self):\n        return self.num_layers\n    def get_params(self, key):\n        return self.parameters.get(key)\n    def set_params(self, key, value):\n        self.parameters[key] = value\n    \n        \n    def forward_propagation(self, X, keep_prob):\n        \"\"\" \n        Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SOFTMAX computation\n        Arguments:\n            X -- data, numpy array of shape (input size, number of examples)\n            keep_prob - probability of keeping a neuron active during drop-out, scalar\n        Returns:\n            AL -- last post-activation value\n            caches -- list of caches containing:\n               every cache of layers w\/ReLU activation (there are L-1 of them, indexed from 0 to L-2)\n               the cache of the output layer with Softmax activation (there is one, indexed L-1)\n        \"\"\"\n        caches = []\n        L = self.get_num_layers()-1    # number of layers with weights (hidden + output)\n        A = X                          # set input as the first hidden layer activation\n        # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n        for l in range(1, L):\n            A_prev = A                # initialize activation of the previous layer\n            W, b = self.get_params(f'W{l}'), self.get_params(f'b{l}') # get weights and biases\n            Z = W.dot(A_prev) + b     # linear activation for the hidden layers\n            A = np.maximum(0,Z)       # ReLU activation for the hidden layers\n            if keep_prob == 1:        # if no dropout\n                cache = (A_prev, Z)   # useful during backpropagation\n            elif keep_prob < 1:       # if dropout is used for regularization\n                D = np.random.rand(A.shape[0], A.shape[1])  # initialize matrix D\n                D = D < keep_prob   # convert entries of D to 0\/1 (using keep_prob as threshold)\n                A *= D              # shut down some neurons of A\n                A \/= keep_prob      # scale the value of neurons that haven't been shutdown\n                cache = (A_prev, Z, D)   # useful during backpropagation\n            caches.append(cache)\n        # Implement LINEAR -> SOFTMAX. Add \"cache\" to the \"caches\" list.\n        W, b = self.get_params(f'W{L}'), self.get_params(f'b{L}')\n        Z = W.dot(A) + b                        # Linear activation of the output layer\n        Z -= np.max(Z, axis=0, keepdims=True)   # Normalize Z to make Softmax stable\n        AL = np.exp(Z)\/np.sum(np.exp(Z),axis=0,\n                              keepdims=True) # Softmax activation of the output layer\n        cache = (A, Z)                          # useful during backpropagation\n        caches.append(cache)\n        return AL, caches\n        \n    \n    def compute_cost(self, AL, Y, lambd):\n        \"\"\"\n        Implement the cost function with L2 regularization.\n        Arguments:\n            AL -- post-activation, output of forward propagation,\n                                    of shape (output size, number of examples)\n            Y -- \"true\" labels vector, of shape (output size, number of examples)\n            lambd -- regularization hyperparameter, scalar\n        Returns:\n            cost - value of the regularized loss function\n        \"\"\"\n        m = AL.shape[1]               # number of training examples\n        L = self.get_num_layers()-1   # number of layers with weights (hidden and output)\n        assert(Y.shape == AL.shape)\n        # Compute the cross-entropy part of the cost for Softmax activation function:\n        cross_entropy_cost = -(1.\/m) * np.sum(np.multiply(Y, np.log(AL)))\n\n        # Compute L2 regularization cost\n        L2_reg_cost = 0\n        if lambd == 0:\n            pass\n        else:\n            for l in range(1, L+1):     # sum of all squared weights\n                L2_reg_cost += (1.\/m)*(lambd\/2)*(np.sum(np.square(self.get_params(f'W{l}'))))\n\n        # Total cost:\n        cost = cross_entropy_cost + L2_reg_cost\n        # To make sure cost's shape is what we expect (e.g. this turns [[17]] into 17).\n        cost = np.squeeze(cost)   \n        return cost\n    \n            \n    def backward_propagation(self, AL, Y, caches, lambd, keep_prob):\n        \"\"\" \n        Implement the backward propagation for the [LINEAR->RELU]*(L-1)->[LINEAR->SOFTMAX]\n        Arguments:\n            AL -- probability vectors for the training examples, output of the forward propagation\n            Y -- true one-hot \"label\" vectors for the training examples\n            caches -- list of caches containing:\n                every cache of forward propagation with \"relu\" \n                                        (there are (L-1) or them, indexes from 0 to L-2)\n                the cache of forward propagation with \"softmax\" (there is one, index L-1)\n            lambd -- lambda, an L2 regularization parameter, scalar\n            keep_prob - probability of keeping a neuron active during drop-out, scalar\n        Returns: grads -- a dictionary with updated gradients\n        \"\"\"\n        L = self.get_num_layers()-1  # number of layers with weights (hidden and output)\n        m = AL.shape[1]              # number of training examples\n        assert(Y.shape == AL.shape)\n        grads = {}                   # a dict for the gradients of the cost function\n\n        # Lth layer (SOFTMAX -> LINEAR) gradients.\n        W = self.get_params(f\"W{L}\") # get weights for the output layer\n        A_prev, Z = caches[L-1]  # get inputs and linear activations for the output layer\n        dZ = AL - Y                  # Gradient of the cost w.r.t. Z (from calculus)\n        dW = (1.\/m) * dZ.dot(A_prev.T) + (lambd\/m) * W  # Gradient of cost w.r.t. W\n        db = (1.\/m) * np.sum(dZ, axis=1, keepdims=True) # Gradient of cost w.r.t. b\n        dA_prev = np.dot(W.T,dZ)                        # Gradient of cost w.r.t. dA_prev\n        # Update the grads dictionary for the output layer\n        grads[f\"dW{L}\"], grads[f\"db{L}\"] = dW, db\n\n        # l-th hidden layer: (RELU -> LINEAR) gradients.\n        for l in reversed(range(1, L)):\n            W = self.get_params(f\"W{l}\") # get weights for the l-th hidden layer\n            dA = dA_prev\n            if keep_prob == 1:           # without dropout\n                A_prev, Z = caches[l-1]  # get ReLU & linear activations for l-th hidden layer\n            elif keep_prob < 1:          # with dropout\n                A_prev, Z, D = caches[l-1]  # get ReLU & linear activations + mask\n                dA *= D                     # apply mask to shut down the same neurons as in f.p.\n                dA \/= keep_prob             # scale the value of the remaining neurons\n            dZ = np.array(dA, copy=True)    # just converting dz to a correct object.\n            dZ[Z <= 0] = 0                  # when z <= 0, set dz to 0 as well. \n            dW = (1.\/m) * dZ.dot(A_prev.T) + (lambd\/m) * W  # Gradient of cost w.r.t. W\n            db = (1.\/m) * np.sum(dZ, axis=1, keepdims=True) # Gradient of cost w.r.t. b\n            dA_prev = np.dot(W.T,dZ)          # Gradient of cost w.r.t. dA_prev\n            # Update the grads dictionary for the hidden layers\n            grads[f\"dA{l}\"] = dA\n            grads[f\"dW{l}\"], grads[f\"db{l}\"] = dW, db\n        return grads\n    \n        \n    def update_parameters_with_gd(self, grads, lr):\n        \"\"\" \n        Update parameters of the model using method gradient descent\n        Arguments:\n            grads -- python dictionary containing gradients, output of backprop\n            lr -- the learning rate, scalar\n        Returns: \n            updated parameters (weithts and biases of the model)\n        \"\"\"\n        L = self.get_num_layers()-1  # get number of layers with weights (hidden + output)\n        for l in range(1, L+1):\n            self.set_params(f\"W{l}\", self.get_params(f\"W{l}\") - lr * grads[f\"dW{l}\"])\n            self.set_params(f\"b{l}\", self.get_params(f\"b{l}\") - lr * grads[f\"db{l}\"])\n               \n            \n    def initialize_adam(self):\n        \"\"\"\n        Initializes v and s for the Adam optimizer as two python dictionaries with:\n            keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n            values: numpy arrays of zeros of the same shape as the corresponding gradients.\n        Returns: \n            v -- python dict that will contain the exponentially weighted average of the gradient.\n            s -- python dict that will contain the exponentially weighted average of the squared gradient.\n        \"\"\"\n        v = {}   \n        s = {}\n        L = self.get_num_layers()-1   # get number of layers with weights (hidden and output)\n        for l in range(1, L+1):\n            v[f\"dW{l}\"] = np.zeros((self.get_params(f\"W{l}\").shape))\n            v[f\"db{l}\"] = np.zeros((self.get_params(f\"b{l}\").shape))\n            s[f\"dW{l}\"] = np.zeros((self.get_params(f\"W{l}\").shape))\n            s[f\"db{l}\"] = np.zeros((self.get_params(f\"b{l}\").shape))    \n        return v,s\n    \n    \n    def update_parameters_with_adam(self, grads, v, s, t, lr, beta1, beta2, epsilon):\n        \"\"\"\n        Update parameters of the model using Adam optimization algorithm\n        Arguments:\n            v -- Adam variable, moving average of the first gradient, python dict\n            s -- Adam variable, moving average of the squared gradient, python dict\n            t -- current timestep (minibatch)\n            lr -- the learning rate, scalar.\n            beta1 -- Exponential decay hyperparameter for the first moment estimates \n            beta2 -- Exponential decay hyperparameter for the second moment estimates \n            epsilon -- hyperparameter preventing division by zero in Adam updates\n        Returns:\n            updated parameters (model attributes)\n            v -- Adam variable, moving average of the first gradient, python dict\n            s -- Adam variable, moving average of the squared gradient, python dict\n        \"\"\"\n        v_corr = {}       # Initializing a bias-corrected first moment\n        s_corr = {}       # Initializing a bias corrected second moment estimate\n        L = self.get_num_layers()-1   # get number of layers with weights (hidden+output)\n        \n        # Perform Adam update on all parameters\n        for l in range(1, L+1):\n            # Moving average of the gradients\n            v[f\"dW{l}\"] = beta1*v[f\"dW{l}\"] + (1-beta1)*grads[f\"dW{l}\"]\n            v[f\"db{l}\"] = beta1*v[f\"db{l}\"] + (1-beta1)*grads[f\"db{l}\"]\n\n            # Compute bias-corrected first moment estimate\n            v_corr[f\"dW{l}\"] = v[f\"dW{l}\"]\/(1-beta1**t)\n            v_corr[f\"db{l}\"] = v[f\"db{l}\"]\/(1-beta1**t)\n\n            # Moving average of the squared gradients\n            s[f\"dW{l}\"] = beta2*s[f\"dW{l}\"] + (1-beta2)*grads[f\"dW{l}\"]**2\n            s[f\"db{l}\"] = beta2*s[f\"db{l}\"] + (1-beta2)*grads[f\"db{l}\"]**2\n\n            # Compute bias-corrected second raw moment estimate\n            s_corr[f\"dW{l}\"] = s[f\"dW{l}\"]\/(1-beta2**t)\n            s_corr[f\"db{l}\"] = s[f\"db{l}\"]\/(1-beta2**t)\n\n            # Update parameters\n            temp_W = v_corr[f\"dW{l}\"]\/(s_corr[f\"dW{l}\"]**0.5 + epsilon)\n            temp_b = v_corr[f\"db{l}\"]\/(s_corr[f\"db{l}\"]**0.5 + epsilon)\n            self.set_params(f\"W{l}\", self.get_params(f\"W{l}\") - lr*temp_W)\n            self.set_params(f\"b{l}\", self.get_params(f\"b{l}\") - lr*temp_b)\n\n        return v, s\n        \n        \n    def predict(self, X):\n        \"\"\" \n        This method is used to predict the results of a L-layer neural network.\n        Arguments:\n            X -- dataset of examples to label, shape (num_pixels, num_examples)\n        Returns:\n            p -- predictions of one-hot labels for X, shape (num_classes, num_examples)\n        \"\"\"\n        m = X.shape[1]          # number of examples\n        # Forward propagation\n        AL, _ = self.forward_propagation(X, keep_prob=1)\n        n = AL.shape[0]         # number of classes\n        p = np.zeros((n, m))    # initialize predictions\n        # convert probabilities AL to one-hot label predictions:\n        for i in range(m):\n            max_i = np.amax(AL[:,i], axis=0)\n            for j in range(n):\n                if AL[j,i] == max_i:\n                    p[j,i] = 1\n                else:\n                    p[j,i] = 0\n        return np.int64(p)\n    \n        \n    def fit(self, X, Y, batch_size=128, num_epochs=20, lr=1e-3, min_lr=1e-8, lambd=0, keep_prob=0.75,\n            beta1=0.9, beta2=0.999, epsilon=1e-8, optimizer='adam', print_cost=False):\n        \"\"\" \n        Implements an L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SOFTMAX.\n        Arguments:\n            X -- data, numpy array of shape (# pixels, # examples)\n            Y -- true \"label\" vector, of shape (# classes, # examples)\n            batch_size -- the size of a mini batch on which parameters are get updated\n            num_epochs -- number of epochs, i.e. passes through the training set\n            lr -- learning rate of the gradient descent update rule\n            min_lr -- the lower threshold of the learning rate decay\n            lambd -- lambda, the L2 regularization hyperparameter\n            optimizer -- optimization metod [\"gd\"=gradient_descent or \"adam\"]\n            beta1 -- exp decay hyperparameter for the past gradients estimates in 'adam'\n            beta2 -- exp decay hyperparameter for the past squared gradients estimates in 'adam'\n            epsilon -- hyperparameter preventing division by zero in 'adam' updates\n            print_cost -- if True, it prints the cost every # steps\n        Returns:\n            parameters -- parameters learnt by the model. They can then be used to predict.\n        \"\"\"\n        costs = []                # to keep track of the cost\n        lr_0 = lr                 # to fix the initial learning rate before decay\n        t = 0                     # initializing the minibatch counter (for Adam update)\n        cost_prev = 1000          # initialize cost to a big number\n        # Initialize the optimizer\n        if optimizer == \"gd\":\n            pass                  # no initialization required for gradient descent\n        elif optimizer == \"adam\":\n            v, s = self.initialize_adam()\n        # Optimization loop\n        for epoch in range(num_epochs):\n            # Define the random minibatches. Reshuffle the dataset after each epoch\n            minibatches = random_mini_batches(X, Y, batch_size)\n            # Initialize a list of minibatch costs for each epoch:\n            minibatch_costs = []\n            # update parameters after each minibatch:\n            for minibatch in minibatches:\n                # Select a minibatch\n                (minibatch_X, minibatch_Y) = minibatch\n                # Forward propagation: [LINEAR->RELU]*(L-1) -> LINEAR->SOFTMAX.\n                AL, caches = self.forward_propagation(minibatch_X, keep_prob)\n                # Compute cost.\n                cost = self.compute_cost(AL, minibatch_Y, lambd)\n                minibatch_costs.append(cost)\n                # Backward propagation.\n                grads = self.backward_propagation(AL, minibatch_Y, caches, lambd, keep_prob)\n                # Update parameters\n                t = t + 1 # minibatch counter\n                if optimizer == \"gd\":\n                    self.update_parameters_with_gd(grads, lr)\n                elif optimizer == \"adam\":\n                    v, s = self.update_parameters_with_adam(grads, v, s, t, lr, \n                                                            beta1, beta2, epsilon)\n            # Find average cost over all minibathes within the epoch: \n            ave_cost = sum(minibatch_costs) \/ len(minibatch_costs)\n            # Define learning rate decay:\n            if ave_cost > cost_prev and lr > min_lr:\n                lr = lr \/ 2   # reduce lr, but not below the min value\n            cost_prev = ave_cost  # save cost value for the next epoch\n                \n            # Print the cost every 2 epoch\n            if print_cost and epoch % 2 == 0:\n                print(f\"Cost after epoch {epoch}: {ave_cost}\")\n            if print_cost and epoch % 1 == 0:    \n                costs.append(ave_cost)\n        # plot the Learning curve\n        if print_cost:\n            plt.plot(np.squeeze(costs))\n            plt.ylabel(\"cost\")\n            plt.xlabel(\"epochs (x 1)\")\n            plt.title(f\"Learning rate = {lr_0} \/ {lr}\")\n            plt.show()","ca6da3a8":"# Create list to save journal records of the current session\nrecords_list = []","466296dd":"# Create an instance of the Custom_model class with 'layer_dims' architecture:\nc_model = Custom_model(layer_dims)\n\n# Set hyperparameters that we want to tune for the custom model:\nlr          = 1e-3   # the learning rate for the gradient descent\nmin_lr      = 1e-8   # the lower threshold of the learning rate decay\noptimizer   = 'adam'\nbatch_size  = 128  \nnum_epochs  = 25          \nlambd       = 0   # lambda - regularization hyperparameter, scalar\nkeep_prob   = 0.75   # keep_prob - probability of keeping a neuron active during dropout\n\n# Train the model at various hyperparameters settings:\nc_model.fit(X_train, Y_train, batch_size=batch_size, num_epochs=num_epochs,\n            lr=lr, min_lr=min_lr, lambd=lambd, keep_prob=keep_prob,\n            optimizer=optimizer, print_cost=True)       \n\n# Evaluation on the train data:\npredict_train = c_model.predict(X_train)\ncorrect_train = np.argmax(predict_train, axis=0) == np.argmax(Y_train, axis=0)\nc_train_acc = round(np.sum(correct_train)\/m_train, 2)\n\n# Evaluation on the validation data:\npredict_val = c_model.predict(X_val)\ncorrect_val = np.argmax(predict_val, axis=0) == np.argmax(Y_val, axis=0)\nc_val_acc = round(np.sum(correct_val)\/m_val, 2)\n\nprint(f\"c_model: train accuracy: {c_train_acc * 100}%\")\nprint(f\"c_model: val accuracy = {c_val_acc * 100}%\")\nprint(f'c_model: val error = {round((1 - c_val_acc) * m_val)} examples')\n\n# Update the journal of hyperparameters tuning records\nrecord = {'layer_dims'   : c_model.get_layer_dims(), \n          'acc_train'    : c_train_acc, \n          'acc_val'      : c_val_acc,\n          'val_error'    : round((1 - c_val_acc) * m_val),\n          'batch_size'   : batch_size,\n          'num_epochs'   : num_epochs,\n          'lr'           : lr,\n          'min_lr'       : min_lr,\n          'lambda'       : lambd,\n          'keep_prob'    : keep_prob}\n\nrecords_list.append(record)\njournal = pd.DataFrame(records_list)   # saves records when repeatedly running the current cell\njournal","215b15e7":"# Save the Keras model to JSON:\nmodel_json = k_model.to_json()\nwith open(\"k_model.json\", \"w\") as f1:\n    f1.write(model_json)\n# Save the weights to HDF5:\nk_model.save_weights(\"k_weights.h5\")","ad5d2b6c":"# Save the Custom model weights:\nimport pickle\nwith open(\"c_weights.pickle\", \"wb\") as f2:\n    pickle.dump(c_model.parameters, f2, pickle.HIGHEST_PROTOCOL)","4e531eaa":"# Predict labels on the test dataset:\npredictions = k_model.predict_classes(test_pixels)\n#predict_test = c_model.predict(X_test)   # one-hot prediction vectors\n#predictions = np.argmax(predict_test, axis=0)  # label predictions\n\nsubmission = pd.DataFrame({\"ImageId\": list(range(1,len(predictions)+1)),\n                           \"Label\"  : predictions})\nsubmission.to_csv(\"submission.csv\", index=False, header=True)\nsubmission.head()","4ad67bfe":"## 4. MODEL TRAINING","cd9e3bdc":"Hi! I concieved of the ___Under the Hood___ series of Kaggle kernels as part of my machine learning training. The idea is to implement popular machine learning models in Python\/Numpy and compare their performance against a benchmark, i.e. a similar model in the machine learning libraries like Scikit-learn or Keras. I agree with Andrej Karpathy who wrote in his [blog](http:\/\/karpathy.github.io\/neuralnets\/) : \"...everything became much clearer when I started writing code.\" \n\nThis kernel is dedicated to the ___Dense Neural Network___ (aka fully connected network) applied to the __MNIST__ dataset. What I'd like to have in the model: \n- an adjustable network architecture,\n- \"He\" initialization, \n- forward propagation with ReLU activation in the hidden layers,\n- optional dropout regularization,\n- Softmax activation in the output layer,\n- cross-entropy cost function with optional L2 regularization, \n- backpropagation and parameters update with stochastic gradient descent,\n- optional Adam optimizer, \n- prediction of classificaton labels after learning the model for various hyperparameters. \n\nIf compared to the __Titanic__ dataset, there are more features (pixels) in the input, and therefore we can try larger hidden layers size (512 in this kernel).\n \nAny feedback or ideas are welcome.","c149f988":"Let's have a look \"under the hood\" at a similar model in Python\/Numpy and appreciate the greatness of Keras :)","5bcba29e":"## 0. DESCRIPTION","d3822e7b":"### 3.2 Define a neural network model with fully connected layers","31b87b10":"### 3.1 Reshape data and define mini-batches","981ab885":"## -1. SUBMIT PREDICTIONS TO KAGGLE","dac1250f":"## 1. MNIST DATA","a1f99cde":"## Conclusion: we have looked \"under the hood\" of a fully connected network with ReLU activation and dropout regularization for the hidden layers, Softmax activation for the output layer, cross-entropy cost with L2 regularization, and parameters update with Adam optimization. \n\n## Great! We know how it works!","235685e5":"## 3. A CUSTOM NEURAL NETWORK IN PYTHON\/NUMPY (classification)","7c279276":"## 2. A DENSE NEURAL NETWORK IN KERAS","f327e8d8":"## 5. SAVING THE TRAINED MODELS "}}