{"cell_type":{"0080229e":"code","8e7f3317":"code","55e90786":"code","3a1e606f":"code","a1ce1d32":"code","90a717f5":"code","f45b64bc":"code","6309176d":"code","d2ed4c54":"code","2551aad1":"code","c179a54f":"code","d428c5af":"code","bfb528a5":"code","0f1b6cea":"code","c221e525":"code","93b9d646":"code","364fd7bb":"code","1834d2bb":"code","f00b3ec8":"code","6b7a5f41":"code","a811cde3":"code","efb68ecf":"code","6b194099":"code","ecc5546c":"code","6cbd6f4b":"code","ace05f6d":"code","310a695b":"code","c5c30ea5":"code","fe80cb1d":"code","07d1ef61":"code","d1339587":"markdown","903269c9":"markdown","22de58b9":"markdown","f03756dd":"markdown","95b92c83":"markdown","1b0c6179":"markdown","ebef15f6":"markdown","fd305fd8":"markdown","7cf09b75":"markdown","abebf915":"markdown","701d2329":"markdown","3c4b2c1a":"markdown","8c826a46":"markdown","d0897786":"markdown","bc5708f1":"markdown","04ec7ed5":"markdown","8f5b1cc7":"markdown","099e2388":"markdown","e1e1ef5a":"markdown","d5975cfa":"markdown","f5d9de91":"markdown","3990354e":"markdown","375ce674":"markdown","1034f013":"markdown","082e4e08":"markdown","3a7823ff":"markdown","6f7ccd89":"markdown"},"source":{"0080229e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns # data visualization library\nimport matplotlib.pyplot as plt # data visualization library\nimport scipy.stats as stats # library of statistical functions\nimport warnings\nwarnings.filterwarnings(\"ignore\") # warnings filter to never print matching warnings\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","8e7f3317":"df_train = pd.read_csv(\"\/kaggle\/input\/liberty-mutual-group-property-inspection-prediction\/train.csv.zip\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/liberty-mutual-group-property-inspection-prediction\/test.csv.zip\")","55e90786":"df_train.info()","3a1e606f":"df_test.info()","a1ce1d32":"for col in df_train.columns: \n    print('{} :{} ' . format(col.upper(),df_train[col].unique()))","90a717f5":"objectCol = list(df_train.select_dtypes(include=['object']).columns)\n\nfor col in objectCol:\n    df_train[col] = df_train[col].astype(\"category\")\n    df_test[col] = df_test[col].astype(\"category\")","f45b64bc":"# Memory saving function credit to https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    #start_mem = df.memory_usage().sum() \/ 1024**2\n    #print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype\n        if col_type != object and str(col_type) != 'category':\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            elif str(col_type)[:5] == 'float':\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    #end_mem = df.memory_usage().sum() \/ 1024**2\n    #print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    #print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n\n    return df","6309176d":"df_train = reduce_mem_usage(df_train)\ndf_test = reduce_mem_usage(df_test)\ndf_train.info()","d2ed4c54":"df_train=df_train.drop(['Id'],axis=1)\ndf_test_Id=df_test['Id']\ndf_test=df_test.drop(['Id'],axis=1)\ndf_numerical_cols = df_train.select_dtypes(exclude='object').select_dtypes(exclude='category').columns.tolist()\n# df_categorical_cols = df.select_dtypes(include='object').columns.tolist()\ndf_categorical_cols = [i for i in df_train.columns if i not in df_numerical_cols]","2551aad1":"df_numerical_cols","c179a54f":"df_categorical_cols","d428c5af":"fig, axs = plt.subplots(len(df_numerical_cols),2)\nfig.set_figwidth(10)\nfig.set_figheight(120)\ni=0\nfor col in df_numerical_cols:\n    sns.histplot(x=df_train[col], ax=axs[i,0],kde=True)\n    stats.probplot(df_train[col], dist=\"norm\", plot=axs[i,1])\n    i=i+1","bfb528a5":"corrmat = df_train.corr()\nf, ax = plt.subplots(figsize=(15, 15))\nmatrix = np.triu(df_train.corr())\nsns.heatmap(corrmat, square=True, annot=True, fmt='.1g',  cbar=False, mask=matrix);","0f1b6cea":"df_train.skew(axis = 0, skipna = True)","c221e525":"col = 'T2_V2'\nfig, axs = plt.subplots(3)\nfig.set_figwidth(8)\nfig.set_figheight(15)\nsns.kdeplot(df_train[col],color='Purple',fill=True, ax=axs[0])\nprint(\"Old skew of %s: %.2f\" % (col,df_train[col].skew(axis = 0, skipna = True)))\n# Removing the skewness using a log function and checking the distribution again\ndf_train[col] = df_train[col].map(lambda i : np.log(i) if i > 0 else 0)\nsns.kdeplot(df_train[col],color='Orange',fill=True, ax=axs[1])\nstats.probplot(df_train[col], dist=\"norm\", plot=axs[2])\nprint(\"New skew of %s: %.2f\" % (col,df_train[col].skew(axis = 0, skipna = True)))\ndf_test[col] = df_test[col].map(lambda i : np.log(i) if i > 0 else 0)","93b9d646":"col = 'T2_V6'\nfig, axs = plt.subplots(3)\nfig.set_figwidth(8)\nfig.set_figheight(15)\nsns.kdeplot(df_train[col],color='Purple',fill=True, ax=axs[0])\nprint(\"Old skew of %s: %.2f\" % (col,df_train[col].skew(axis = 0, skipna = True)))\n# Removing the skewness using a log function and checking the distribution again\ndf_train[col] = df_train[col].map(lambda i : np.log(i) if i > 0 else 0)\nsns.kdeplot(df_train[col],color='Orange',fill=True, ax=axs[1])\nstats.probplot(df_train[col], dist=\"norm\", plot=axs[2])\nprint(\"New skew of %s: %.2f\" % (col,df_train[col].skew(axis = 0, skipna = True)))\ndf_test[col] = df_test[col].map(lambda i : np.log(i) if i > 0 else 0)","364fd7bb":"col = 'T2_V14'\nfig, axs = plt.subplots(3)\nfig.set_figwidth(8)\nfig.set_figheight(15)\nsns.kdeplot(df_train[col],color='Purple',fill=True, ax=axs[0])\nprint(\"Old skew of %s: %.2f\" % (col,df_train[col].skew(axis = 0, skipna = True)))\n# Removing the skewness using a log function and checking the distribution again\ndf_train[col] = df_train[col].map(lambda i : np.log(i) if i > 0 else 0)\nsns.kdeplot(df_train[col],color='Orange',fill=True, ax=axs[1])\nstats.probplot(df_train[col], dist=\"norm\", plot=axs[2])\nprint(\"New skew of %s: %.2f\" % (col,df_train[col].skew(axis = 0, skipna = True)))\ndf_test[col] = df_test[col].map(lambda i : np.log(i) if i > 0 else 0)","1834d2bb":"col = 'T2_V15'\nfig, axs = plt.subplots(3)\nfig.set_figwidth(8)\nfig.set_figheight(15)\nsns.kdeplot(df_train[col],color='Purple',fill=True, ax=axs[0])\nprint(\"Old skew of %s: %.2f\" % (col,df_train[col].skew(axis = 0, skipna = True)))\n# Removing the skewness using a log function and checking the distribution again\ndf_train[col] = df_train[col].map(lambda i : np.log(i) if i > 0 else 0)\nsns.kdeplot(df_train[col],color='Orange',fill=True, ax=axs[1])\nstats.probplot(df_train[col], dist=\"norm\", plot=axs[2])\nprint(\"New skew of %s: %.2f\" % (col,df_train[col].skew(axis = 0, skipna = True)))\ndf_test[col] = df_test[col].map(lambda i : np.log(i) if i > 0 else 0)","f00b3ec8":"# creating a copy of dataframe\ndf1 = df_train\n\n# separating the features and target \nX = df1.drop(['Hazard'],axis=1)\ny = df1[['Hazard']]\nX_df_test = df_test","6b7a5f41":"df_numerical_cols.remove('Hazard')","a811cde3":"import gc\ndel df1\ngc.collect()","efb68ecf":"# In case when few categorical values are not common in Train set and Test set \n# then training the model using Train set and transforming Test set will result in \n# different number of columns\n# Combining training set and test set so make the number of encoded columns same\nX['train']=1\nX_df_test['train']=0\ncombined = pd.concat([X,X_df_test])\ndf = pd.get_dummies(combined[df_categorical_cols])\ndf_dummies=pd.concat([combined[df_numerical_cols],df,combined['train']],axis=1)\n# Splitting training set and test set\nX = df_dummies[df_dummies['train']==1]\nX_df_test = df_dummies[df_dummies['train']==0]\n# Deleting Train column\nX.drop(['train'],axis=1,inplace=True)\nX_df_test.drop(['train'],axis=1,inplace=True)\nX.shape","6b194099":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)","ecc5546c":"from sklearn.preprocessing import StandardScaler \nsc = StandardScaler()\n\nfor col in df_numerical_cols:\n    X_train[col] =  sc.fit_transform(X_train[col].values.reshape(-1,1))\n    X_test[col] =  sc.transform(X_test[col].values.reshape(-1,1))\ny_train['Hazard'] =  sc.fit_transform(y_train['Hazard'].values.reshape(-1,1))\ny_test['Hazard'] =  sc.transform(y_test['Hazard'].values.reshape(-1,1))\n# last object is fitted on y, this will be used to inverse transform y_pred","6cbd6f4b":"# Base Models\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\n\n# Ensembling and Boosting\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n","ace05f6d":"def gini(actual, pred):\n    assert (len(actual) == len(pred))\n    all = np.asarray(np.c_[actual, pred, np.arange(len(actual))], dtype=np.float)\n    all = all[np.lexsort((all[:, 2], -1 * all[:, 1]))]\n    totalLosses = all[:, 0].sum()\n    giniSum = all[:, 0].cumsum().sum() \/ totalLosses\n\n    giniSum -= (len(actual) + 1) \/ 2.\n    return giniSum \/ len(actual)\n\n# The higher the better\ndef gini_normalized(actual, pred):\n    return gini(actual, pred) \/ gini(actual, actual)","310a695b":"models = [\n    ('LinearRegression', LinearRegression()),\n    ('GradientBoostingRegressor', GradientBoostingRegressor()),\n    ('DecisionTreeRegressor',DecisionTreeRegressor(random_state = 0)),\n    ('RandomForestRegressor',RandomForestRegressor(n_estimators = 10, random_state = 0))\n]","c5c30ea5":"print(\"The accuracy scores of the models are :\")\nfor model_name, model in models:\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    y_test['Hazard'] =  sc.inverse_transform(y_test['Hazard'].values)\n    y_pred =  sc.inverse_transform(y_pred)\n    print(model_name, \" after rescale Test score: \", gini_normalized(y_test, y_pred))\n    y_pred = model.predict(X_train)\n    y_train['Hazard'] =  sc.inverse_transform(y_train['Hazard'].values)\n    y_pred =  sc.inverse_transform(y_pred)\n    print(model_name, \" after rescale Train score: \", gini_normalized(y_train, y_pred))","fe80cb1d":"from sklearn.preprocessing import StandardScaler \nsc = StandardScaler()\n\nfor col in df_numerical_cols:\n    X[col] =  sc.fit_transform(X[col].values.reshape(-1,1))\n    X_df_test[col] =  sc.transform(X_df_test[col].values.reshape(-1,1))\ny['Hazard'] =  sc.fit_transform(y['Hazard'].values.reshape(-1,1))\n# last object is fitted on y, this will be used to inverse transform y_pred","07d1ef61":"model = GradientBoostingRegressor()\nmodel.fit(X, y)\ny_pred = model.predict(X_df_test)\ny_pred = sc.inverse_transform(y_pred)\noutput = pd.DataFrame({'Id': df_test_Id, 'Hazard': y_pred})\noutput.to_csv('GradientBoostingRegressor10May.csv', index=False)","d1339587":"Getting Unique values of each column","903269c9":"Histogram","22de58b9":"# Data Preprocessing","f03756dd":"# Understanding the Data","95b92c83":"Deleting unused variables and collecting memory","1b0c6179":"Removing the skewness with |skewness| > 1","ebef15f6":"the Python implementation from the [Gini coefficient discussion with code samples](https:\/\/www.kaggle.com\/c\/ClaimPredictionChallenge\/discussion\/703):","fd305fd8":"Splitting columns into features and dependent columns","7cf09b75":"Reducing data size","abebf915":"Checking the accuracy of models on Train data first and then we will use that model to predict the Test outcome.","701d2329":"# Modeling","3c4b2c1a":"Splitting the dataset into the Training set and Test set","8c826a46":"Splitting Column names based on Data Types","d0897786":"Base Modeling","bc5708f1":"Correlation matrix","04ec7ed5":"Converting Object data type to Category","8f5b1cc7":"Skewness along the index axis","099e2388":"Gini score","e1e1ef5a":"Feature Scaling","d5975cfa":"Encoding categorical data","f5d9de91":"Feature Scaling","3990354e":"Changing the numerical data types to reduce size further","375ce674":"No feature is highly correlated","1034f013":"GradientBoostingRegressor and LinearRegression performed the best, We will use these 2 models. Now we will use whole train data to train and test data to predict.","082e4e08":"Packages","3a7823ff":"Reading Data","6f7ccd89":"**T2_V2, T2_V6, T2_V8, T2_V14, T2_V15** \nnot able to remove skewness of T2_V8"}}