{"cell_type":{"390b50f3":"code","44a85ca2":"code","8eac380f":"code","639483b3":"code","4d877b82":"code","633b7bd9":"code","0a4d1da0":"code","2695ac19":"code","55bc5413":"code","7a0a4436":"code","cfc3a288":"code","ae7bdb8c":"code","ff9ac78a":"code","096023e8":"code","066750e8":"code","752cb994":"code","deb63bfd":"code","c2e7661b":"code","71ebf515":"code","9fcefd71":"code","21da295d":"code","b27f9c67":"code","8c56bcea":"code","07631355":"code","80fb544d":"code","3bd2244d":"code","2fd61b4c":"code","780056ce":"code","b8e350e9":"code","e63f3eb9":"code","07097f70":"code","a69ef91b":"code","f8f2d080":"code","def69a2f":"code","8fa06d65":"code","be64d936":"code","6becdd09":"code","563c8ac5":"code","f6d1ff58":"code","cb676406":"code","2787e9a4":"code","34bd4156":"code","05629f99":"code","57240ee8":"code","59b1963b":"code","6e2ed928":"code","e818ef71":"code","a96428ab":"code","af01a86e":"code","0ed52f67":"code","484345b1":"code","ce09b754":"code","3f352f34":"code","7b601673":"code","e4b7c3ae":"code","08083bdd":"code","aa253a55":"code","47bea5ed":"code","73cefe1a":"code","2e50f243":"code","107a092a":"code","300b1012":"code","43d0e312":"code","9bab22a9":"code","19b6bfc8":"code","b0951671":"code","94d63e9f":"code","30aaa6f8":"code","bf8cd44a":"code","4c76b1a5":"code","4b52266e":"code","cad6c44f":"code","58ba5a4b":"code","264878ed":"code","3b9264ed":"code","d0bcd9a7":"code","0d2a6428":"code","21649672":"code","529ba739":"code","da45bee8":"code","ae56814b":"code","e691d810":"code","ae9febfa":"code","b5cb7bf8":"code","eb76b658":"code","c3ae34b2":"code","c95b0966":"code","0e4209f9":"code","914b8a6b":"markdown","8867dfd3":"markdown","de8ad992":"markdown","ef781670":"markdown","db88939f":"markdown","55cef985":"markdown","9f1cffb7":"markdown","b97a520c":"markdown","fdb3f6d8":"markdown","cf1203cf":"markdown","1df9f15c":"markdown","f9ea1c7a":"markdown","655341a7":"markdown","e072d1d6":"markdown","d96f5b60":"markdown","98d88d59":"markdown","718a0b0e":"markdown","d6a76ef9":"markdown","8cea6907":"markdown","1a9aa4f1":"markdown","338baa50":"markdown","1d2998c9":"markdown","7273b7ca":"markdown","38089448":"markdown","f9b0f57a":"markdown","d3eef2e5":"markdown","6d08fb10":"markdown","beefa9fc":"markdown","8350b83e":"markdown","f2cc656a":"markdown"},"source":{"390b50f3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","44a85ca2":"# read data\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ndisplay(train)\ndisplay(test)\ndisplay(train.dtypes)","8eac380f":"# data description for numbers data\ntrain.describe()","639483b3":"# checking missing value, will be processed later\n# Alley,FireplaceQu,PoolQC,Fence,MiscFeature have so manying missing values,may delete\n# other missing value need to fill with some methods\ncol_name = list(train.columns)\ncount = 0\nfor i in range(len(col_name)):\n    name = col_name[i]\n    missing_value = train[name].isnull().sum()\n    if missing_value != 0:\n        count = count+1\n        percent = (missing_value\/len(train[name]))*100\n        print(col_name[i])\n        print(missing_value,end = \" \")      \n        print(percent,end = '%')\n        print()\n        \n    else:\n        continue\nprint(count)   ","4d877b82":"# checking missing value for the test set\ncol_name = list(test.columns)\ncount = 0\nfor i in range(len(col_name)):\n    name = col_name[i]\n    missing_value = test[name].isnull().sum()\n    if missing_value != 0:\n        count = count+1\n        percent = (missing_value\/len(train[name]))*100\n        print(col_name[i])\n        print(missing_value,end = \" \")      \n        print(percent,end = '%')\n        print()\n        \n    else:\n        continue\nprint(count)   ","633b7bd9":"train.info()","0a4d1da0":"# columns\ncolumns_list = train.columns\ncolumns_list","2695ac19":"print('The cheapest price ${:,.0f} and the most expensive price ${:,.0f}'.format(\n    train.SalePrice.min(), train.SalePrice.max()))\nprint('The average sales price is ${:,.0f}, while median is ${:,.0f}'.format(\n    train.SalePrice.mean(), train.SalePrice.median()))\ntrain.SalePrice.hist(bins=60, rwidth=0.8, figsize=(10,4))\nplt.title('Houses Price')\nplt.xlabel('Prince of House($)')\nplt.ylabel('Number of Houses')\nplt.show()","55bc5413":"train.YearBuilt.hist(bins=20, rwidth=.8, figsize=(10,4))\nplt.xlabel('Years')\nplt.ylabel('Number of Houses')\nplt.show()","7a0a4436":"train.GrLivArea.hist(bins=20, rwidth=.8, figsize=(8,4))\nplt.title('Size of Houses (square feet)')\nplt.show()","cfc3a288":"#print(train.groupby(['YearBuilt']).mean()['SalePrice'])\ntemp1 = train.groupby(['YearBuilt']).mean()['SalePrice']\nplt.plot(temp1)\nprint(temp1)","ae7bdb8c":"numerical_features = [feature for feature in train.columns if train[feature].dtypes != 'O']\nyear_feature = [feature for feature in numerical_features if 'Yr' in feature or 'Year' in feature]\nprint(\"Variables which include year is :-\", year_feature)\n\n# for feature in year_feature:\n#     print(feature, train[feature].unique())\n#     print('\\n')\n    \ntrain.groupby('YrSold')['SalePrice'].median().plot(color = \"tomato\",linestyle = \"--\",linewidth=3)\nplt.xlabel('Year Sold',fontsize = 15)\nplt.ylabel('Median House Price',fontsize =15)\nplt.title(\"House Price Vs YearSold\",fontsize=22)\nplt.figure(figsize=(30,12))\n\nprint(year_feature)","ff9ac78a":"#Plot Bar Chart: Nominal Features & Sale Prices\nimport seaborn as sns\nimport pandas as pd\n\n# Pick up Nominal Features\nnominal_features = ['MSSubClass','MSZoning', 'Street', 'Alley', 'LandContour', 'LotConfig', 'Neighborhood', 'Condition1', \\\n                    'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', \\\n                    'MasVnrType', 'Foundation','Heating','CentralAir', 'GarageType', 'MiscFeature', 'SaleType', \\\n                    'SaleCondition','SalePrice']\n\ntrain_nominal = train[nominal_features] \ni=0\ndata=train_nominal.copy()\n\nfor feature in train_nominal.columns:\n    \n    if (feature=='SalePrice'):\n        continue\n    print('\\n----------------------------------------')\n    \n    data_group = data.groupby(feature)['SalePrice'].median().sort_values().to_frame()\n    ax = sns.barplot(x = data_group.index, y = 'SalePrice', data = data_group, palette = 'Blues',order=data_group.index)\n    ax.set_xticklabels(labels = data_group.index,rotation=90)\n    plt.xlabel(feature, fontsize = 15)\n    plt.ylabel('SalePrice',fontsize = 15)\n    plt.title(feature + \" -- SalePrice \",fontsize = 20)\n    plt.figure(figsize=(30,12))\n    #data_group = data.groupby(feature)['SalePrice'].plot(kind='kde', legend=True, figsize=(10, 5))\n    plt.show()\n    \n    c = data.groupby(feature)['SalePrice'].count().to_frame()\n    c.rename(columns={'SalePrice':'count'},inplace=True)\n    d = data.groupby(feature)['SalePrice'].agg([np.mean, np.median, np.std])\n    print(pd.concat([c,d],axis=1))\n    #sns.heatmap(pd.concat([c,d],axis=1))\n    i+=1\n    \nprint(i)","096023e8":"continuous_features = ['LotFrontage','LotArea','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','1stFlrSF','2ndFlrSF','LowQualFinSF',\\\n                      'GrLivArea','GarageArea','WoodDeckSF','OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MiscVal','SalePrice']\nchosen_feature = pd.DataFrame(train,columns =continuous_features)\ndata = chosen_feature.iloc[:,:] \nf,ax = plt.subplots(figsize=(15, 15)) \nsns.heatmap(data.corr(), annot=True, linewidths=.5, fmt= '.2f',ax=ax,robust = True) \nplt.show()","066750e8":"training_set = train.copy()\ntest_set = test.copy()","752cb994":"discrete_features = ['OverallQual', 'OverallCond', 'LowQualFinSF', 'BsmtFullBath', 'BsmtHalfBath', \\\n                    'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', \\\n                     'GarageCars', '3SsnPorch', 'PoolArea', 'MiscVal', 'MoSold']\n#Locate Missing Discrete Values\ntrain_discrete = training_set[discrete_features] \nfeatures_with_na=[features for features in train_discrete.columns if train_discrete[features].isnull().sum()>1]\n# non missing values in discrete features","deb63bfd":"training_set","c2e7661b":"test_set","71ebf515":"def find_index(obj,l):\n    index = 0\n    for item in l:\n        if obj == item:\n            break\n        else:\n            index = index+1\n            continue\n    return index","9fcefd71":"# by the meaning of LotFrontage, we try to use the neigbourhood mean value to fill the missing value\nneighborhood_name= np.sort(train['Neighborhood'].unique())\nLotFrontage_mean = train['LotFrontage'].groupby(train['Neighborhood']).mean()\n#print('the number of null value now is {}'.format(training_set['LotFrontage'].isnull().sum()))\n\n# training set\nfor i in range(len(train['LotFrontage'])):\n    if np.isnan(train['LotFrontage'][i]):\n        neigbour = train['Neighborhood'][i]\n        index = find_index(neigbour,neighborhood_name)\n        training_set['LotFrontage'][i] = LotFrontage_mean[index]\n# test set\nfor j in range(len(test_set['LotFrontage'])):\n    if np.isnan(test_set['LotFrontage'][j]):\n        neigbour = test_set['Neighborhood'][j]\n        index = find_index(neigbour,neighborhood_name)\n        test_set['LotFrontage'][j] = LotFrontage_mean[index]\n#plt.hist(training_set['LotFrontage'],bins=25,color = \"tomato\",edgecolor = \"black\",linewidth = 1.75)\n#plt.show()\n# print('the number of null value in training set now is {}'.format(training_set['LotFrontage'].isnull().sum()))\n# print('the number of null value in test now is {}'.format(test_set['LotFrontage'].isnull().sum()))","21da295d":"# try to use log to make the data more smooth\n# display(training_set[['LotFrontage','SalePrice']].corr())\n# plt.figure(figsize=(12, 3))\n# plt.subplot(121,title='original')\n# plt.scatter(training_set['LotFrontage'],training_set['SalePrice'],color = 'tomato')\n# plt.subplot(122,title='log')\n# plt.scatter(np.log(training_set['LotFrontage']),training_set['SalePrice'],color = 'orange')\n# plt.legend()\n# display(np.corrcoef(np.log(training_set['LotFrontage']),training_set['SalePrice']))\n\n\n# the log of LotFrontage seems to have more linear with the sale price\n# so we create a new feature LotFrontage, by PCA or feature importance to choose \n# training_set\ntraining_set['logLotFrontage'] = np.log(training_set['LotFrontage'])\n# test_set\ntest_set['logLotFrontage'] = np.log(test_set['LotFrontage'])\n","b27f9c67":"# LotArea\n#print('the number of null value now is {}'.format(training_set['LotFrontage'].isnull().sum()))\n# non need to fill missing value\n# check the relationship between\n# display(training_set[['LotArea','SalePrice']].corr())\n# plt.figure(figsize=(12, 3))\n# plt.subplot(121,title='original')\n# plt.scatter(training_set['LotArea'],training_set['SalePrice'],color = 'tomato')\n# plt.subplot(122,title='log')\n# plt.scatter(np.log(training_set['LotArea']),training_set['SalePrice'],color = 'orange')\n# plt.legend()\n# display(np.corrcoef(np.log(training_set['LotArea']),training_set['SalePrice']))\n\n\n\n\n# the log of LotArea seems to have more linear with the sale price\n# so we create a new feature logLotArea, by PCA or feature importance to choose \n# training_set\ntraining_set['logLotArea'] = np.log(training_set['LotArea'])\n# test_set\ntest_set['logLotArea'] = np.log(test_set['LotArea'])\n# print('the number of null value in training set now is {}'.format(training_set['LotFrontage'].isnull().sum()))\n# print('the number of null value in test now is {}'.format(test_set['LotFrontage'].isnull().sum()))","8c56bcea":"#print('the number of null value of {0} now is {1}'.format('MasVnrArea',training_set['MasVnrArea'].isnull().sum()))\n# display(training_set.loc[training_set['MasVnrType'].isnull(),['MasVnrType','MasVnrArea']])\n# we cannot use the related feature to filling the missing value\n#display(training_set[['MasVnrArea','SalePrice']].corr())\n\n\n# use the mean of MasVnrArea to fill the nan, this will not influent the correlation\nMasVnrArea_mean = train['MasVnrArea'].groupby(train['Neighborhood']).mean()\n# training set\nfor i in range(len(train['MasVnrArea'])):\n    if np.isnan(train['MasVnrArea'][i]):\n        neigbour = train['Neighborhood'][i]\n        index = find_index(neigbour,neighborhood_name)\n        training_set['MasVnrArea'][i] = MasVnrArea_mean[index]\n# test set\nfor j in range(len(test_set['MasVnrArea'])):\n    if np.isnan(test_set['MasVnrArea'][j]):\n        neigbour = test_set['Neighborhood'][j]\n        index = find_index(neigbour,neighborhood_name)\n        test_set['MasVnrArea'][j] = MasVnrArea_mean[index]\n\n# print('the number of null value of {0} in training set now is {1}'.format('MasVnrArea',training_set['MasVnrArea'].isnull().sum()))\n# print('the number of null value of {0} in test set now is {1}'.format('MasVnrArea',test_set['MasVnrArea'].isnull().sum()))\n# display(training_set[['MasVnrArea','SalePrice']].corr())\n# there are too many 0 in 'MasVnrArea', so natural log is not situable ","07631355":"# # BsmtFinSF1\n# print('the number of null value of {0} now is {1}'.format('BsmtFinSF1',training_set['BsmtFinSF1'].isnull().sum()))\n# # non need to fill missing value\n# display(training_set[['BsmtFinSF1','SalePrice']].corr())\n# plt.scatter(training_set['BsmtFinSF1'],training_set['SalePrice'],color = 'tomato')\n# # there are too many 0 in 'BsmtFinSF1', so natural log is not situable \n# display('number of 0 value of BsmtFinSF1:{}'.format(len(training_set.loc[training_set['BsmtFinSF1']==0])))\n# # from the scatter picture, there is likely to have an outlier\n# # might to check with the feature 1stFlrSF","80fb544d":"# # BsmtFinSF2\n# print('the number of null value of {0} now is {1}'.format('BsmtFinSF2',training_set['BsmtFinSF2'].isnull().sum()))\n# # non need to fill missing value\n# display(training_set[['BsmtFinSF2','SalePrice']].corr())\n# plt.scatter(training_set['BsmtFinSF2'],training_set['SalePrice'],color = 'tomato')\n# # there are too many 0 in 'BsmtFinSF2', so natural log is not situable\n# display('number of 0 value of BsmtFinSF2:{}'.format(len(training_set.loc[training_set['BsmtFinSF2']==0])))\n# # the the correlation is quite small, also there are too many 0, this feature might useful\n","3bd2244d":"# #BsmtUnfSF\n# print('the number of null value of {0} now is {1}'.format('BsmtUnfSF',training_set['BsmtUnfSF'].isnull().sum()))\n# # non need to fill missing value\n# display(training_set[['BsmtUnfSF','SalePrice']].corr())\n# plt.scatter(training_set['BsmtUnfSF'],training_set['SalePrice'],color = 'tomato')\n# display('number of 0 value of BsmtUnfSF:{}'.format(len(training_set.loc[training_set['BsmtUnfSF']==0])))\n","2fd61b4c":"# #TotalBsmtSF\n# print('the number of null value of {0} now is {1}'.format('TotalBsmtSF',training_set['TotalBsmtSF'].isnull().sum()))\n# # non need to fill missing value\n# display(training_set[['TotalBsmtSF','SalePrice']].corr())\n# plt.scatter(training_set['TotalBsmtSF'],training_set['SalePrice'],color = 'tomato')\n# display('number of 0 value of TotalBsmtSF:{}'.format(len(training_set.loc[training_set['TotalBsmtSF']==0])))\n# # high correlation with SalePrice","780056ce":"# test set, the record 660 has some missing value\nprint('the number of null value of {0} in test set now is {1}'.format('BsmtFinSF1',test_set['BsmtFinSF1'].isnull().sum()))\n# only one missing value, but it miss all basement relative value\ndisplay(test_set.iloc[660][['BsmtFinType1','BsmtFinSF1','BsmtFinType2','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','1stFlrSF','GrLivArea','2ndFlrSF']])\n# # by the 1stFlrSF,1stFlrSF,2ndFlrSF of this missing record, we assign\ntest_set['BsmtFinSF1'][660] = 896\ntest_set['BsmtFinSF2'][660] = 0\ntest_set['BsmtUnfSF'][660]= 0\ntest_set['TotalBsmtSF'][660] = 896\nprint('the number of null value of {0} in test set now is {1}'.format('BsmtFinSF1',test_set['BsmtFinSF1'].isnull().sum()))","b8e350e9":"# correlation between the basement series data\n#training_set[['TotalBsmtSF','BsmtUnfSF','BsmtFinSF2','BsmtFinSF1']].corr()\n# by the meaning of those data, may need to combine to the finished type","e63f3eb9":"# 1stFlrSF\n# print('the number of null value of {0} now is {1}'.format('1stFlrSF',training_set['1stFlrSF'].isnull().sum()))\n# # non need to fill missing value\n# display(training_set[['1stFlrSF','SalePrice']].corr()) # high linear correlated\n# plt.scatter(training_set['1stFlrSF'],training_set['SalePrice'],color = 'tomato')\n# plt.show()\n# plt.scatter(np.log(training_set['1stFlrSF']),training_set['SalePrice'],color = 'tomato')\n# plt.show()","07097f70":"# training_set\ntraining_set['log1stFlrSF'] = np.log(training_set['1stFlrSF'])\n# test_set\ntest_set['log1stFlrSF'] = np.log(test_set['1stFlrSF'])","a69ef91b":"# may need to check wheather it is logically match with the ground living area\n# plt.scatter(training_set['1stFlrSF'],training_set['GrLivArea'],color = 'orange')\n# plt.xlabel('1stFlrSF')\n# plt.ylabel('GrLivArea')\n# plt.show()\n# logically match\ncount = 0\nfor i in range(len(training_set['1stFlrSF'])):\n    if training_set['1stFlrSF'][i]> training_set['GrLivArea'][i]:\n        count = count+1\n    else:\n        continue\nif count!=0:\n    print('not logically match :',count)\nelse:\n    print('logically match')\n","f8f2d080":"# 2ndFlrSF\n# print('the number of null value of {0} now is {1}'.format('2ndFlrSF',training_set['2ndFlrSF'].isnull().sum()))\n# # non need to fill missing value\n# display(training_set[['2ndFlrSF','SalePrice']].corr()) # high linear correlated\n# plt.scatter(training_set['2ndFlrSF'],training_set['SalePrice'],color = 'tomato')\n# plt.show()\n# display('number of 0 value of 2ndFlrSF:{}'.format(len(training_set.loc[training_set['2ndFlrSF']==0])))\n# though has relative high correlation but too much 0","def69a2f":"# create a new feature that show the house has second floor or not,1 = yes,0=no\n# training set\ntraining_set['2ndFlr_Flag'] = 0\ntraining_set.loc[training_set['2ndFlrSF']!=0,'2ndFlr_Flag']=1\n# training_set['2ndFlr_Flag'].value_counts()\n# test set\ntest_set['2ndFlr_Flag'] = 0\ntest_set.loc[training_set['2ndFlrSF']!=0,'2ndFlr_Flag']=1\n\n# display(training_set[['2ndFlr_Flag','SalePrice']].corr()) \n# plt.scatter(training_set['2ndFlr_Flag'],training_set['SalePrice'],color = 'tomato')","8fa06d65":"# print('the number of null value of {0} now is {1}'.format('LowQualFinSF',training_set['LowQualFinSF'].isnull().sum()))\n# # non need to fill missing value\n# display(training_set[['LowQualFinSF','SalePrice']].corr()) # high linear correlated\n# plt.scatter(training_set['LowQualFinSF'],training_set['GrLivArea'],color = 'tomato')\n# plt.show()\n# display('number of 0 value of LowQualFinSF:{}'.format(len(training_set.loc[training_set['LowQualFinSF']==0])))\n# # very low correlation and 0,may not so helpful","be64d936":"# print('the number of null value of {0} now is {1}'.format('GrLivArea',training_set['GrLivArea'].isnull().sum()))\n# # non need to fill missing value\n# display(training_set[['GrLivArea','SalePrice']].corr()) # high linear correlated\n# plt.scatter(training_set['GrLivArea'],training_set['SalePrice'],color = 'tomato')\n# plt.show()","6becdd09":"# training_set\ntraining_set['logGrLivArea'] = np.log(training_set['GrLivArea'])\n# test_set\ntest_set['logGrLivArea'] = np.log(test_set['GrLivArea'])","563c8ac5":"# print('the number of null value of {0} now is {1}'.format('GarageArea',training_set['GarageArea'].isnull().sum()))\n# # non need to fill missing value\n# display(training_set[['GarageArea','SalePrice']].corr()) # high linear correlated\n# plt.scatter(training_set['GarageArea'],training_set['SalePrice'],color = 'tomato')\n# plt.show()","f6d1ff58":"# test set, record 1161 has missing value in garagearea\n#print('the number of null value of {0} in test set now is {1}'.format('GarageArea',test_set['GarageArea'].isnull().sum()))\n#display(test_set.iloc[1116][['Neighborhood','GarageType','GarageYrBlt','GarageFinish','GarageCars','GarageArea','GarageQual']])\n# from the relative we know this record has garage with type detchd,so fill this value with the mean from the type\ntest_set['GarageArea'][1116]= test_set.loc[(test_set['GarageType']=='Detchd')&(test_set['Neighborhood']=='IDOTRR'),'GarageArea'].mean()\n# display(test_set['GarageArea'][1116])\n#print('the number of null value of {0} in test set now is {1}'.format('GarageArea',test_set['GarageArea'].isnull().sum()))","cb676406":"# print('the number of null value of {0} now is {1}'.format('WoodDeckSF',training_set['WoodDeckSF'].isnull().sum()))\n# # non need to fill missing value\n# display(training_set[['WoodDeckSF','SalePrice']].corr()) \n# plt.scatter(training_set['WoodDeckSF'],training_set['SalePrice'],color = 'tomato')\n# plt.show()","2787e9a4":"# #OpenPorchSF\n# print('the number of null value of {0} now is {1}'.format('OpenPorchSF',training_set['OpenPorchSF'].isnull().sum()))\n# # non need to fill missing value\n# display(training_set[['OpenPorchSF','SalePrice']].corr()) \n# plt.scatter(training_set['OpenPorchSF'],training_set['SalePrice'],color = 'tomato')\n# plt.show()\n# display('number of 0 value of OpenPorchSF:{}'.format(len(training_set.loc[training_set['OpenPorchSF']==0])))","34bd4156":"# #EnclosedPorch\n# print('the number of null value of {0} now is {1}'.format('EnclosedPorch',training_set['EnclosedPorch'].isnull().sum()))\n# # non need to fill missing value\n# display(training_set[['EnclosedPorch','SalePrice']].corr()) \n# plt.scatter(training_set['EnclosedPorch'],training_set['SalePrice'],color = 'tomato')\n# plt.show()","05629f99":"# #3SsnPorch\n# print('the number of null value of {0} now is {1}'.format('3SsnPorch',training_set['3SsnPorch'].isnull().sum()))\n# # non need to fill missing value\n# display(training_set[['3SsnPorch','SalePrice']].corr()) \n# plt.scatter(training_set['3SsnPorch'],training_set['SalePrice'],color = 'tomato')\n# plt.show()","57240ee8":"# #ScreenPorch\n# print('the number of null value of {0} now is {1}'.format('ScreenPorch',training_set['ScreenPorch'].isnull().sum()))\n# # non need to fill missing value\n# display(training_set[['ScreenPorch','SalePrice']].corr()) \n# plt.scatter(training_set['ScreenPorch'],training_set['SalePrice'],color = 'tomato')\n# plt.show()","59b1963b":"# combine above these four\ntraining_set['PorchArea'] = training_set['OpenPorchSF']+training_set['EnclosedPorch']+training_set['3SsnPorch']+training_set['ScreenPorch']\n#display(training_set[['PorchArea','SalePrice']].corr()) \n# whether have porch 0 is no, 1 is yes\ntraining_set['Porch_Flag']  = 0\ntraining_set.loc[training_set['PorchArea']!=0,'Porch_Flag']=1\n#display(training_set[['Porch_Flag','SalePrice']].corr()) \n# test set\ntest_set['PorchArea'] = test_set['OpenPorchSF']+test_set['EnclosedPorch']+test_set['3SsnPorch']+test_set['ScreenPorch']\ntest_set['Porch_Flag']  = 0\ntest_set.loc[test_set['PorchArea']!=0,'Porch_Flag']=1\n# flag has shown more correlation with sale price than most like of porch data\n# display('number of 0 value of Porch_Flag:{}'.format(len(training_set.loc[training_set['Porch_Flag']==0])))","6e2ed928":"# print('the number of null value of {0} now is {1}'.format('PoolArea',training_set['PoolArea'].isnull().sum()))\n# # non need to fill missing value\n# display(training_set[['PoolArea','SalePrice']].corr()) \n# plt.scatter(training_set['PoolArea'],training_set['SalePrice'],color = 'tomato')\n# plt.show()\n# display('number of 0 value of PoolArea:{}'.format(len(training_set.loc[training_set['PoolArea']==0]))) \n# to much 0 value, this feature may be useless","e818ef71":"# print('the number of null value of {0} now is {1}'.format('MiscVal',training_set['MiscVal'].isnull().sum()))\n# # non need to fill missing value\n# display(training_set[['MiscVal','SalePrice']].corr()) \n# plt.scatter(training_set['MiscVal'],training_set['SalePrice'],color = 'tomato')\n# plt.show()\n# display('number of 0 value of MiscVal:{}'.format(len(training_set.loc[training_set['MiscVal']==0]))) \n# to many 0 value, this feature may be useless","a96428ab":"training_set","af01a86e":"test_set","0ed52f67":"train_test = pd.concat([training_set,test_set],axis=0)\n\n#Deal with Alley NA \ntrain_test['Alley'].fillna(\"NoAccess\",inplace=True)\n\n#Deal with other Missing values\ntrain_test.drop(['MiscFeature'],axis=1,inplace=True)\ntrain_test['MSZoning']=train_test['MSZoning'].fillna(train_test['MSZoning'].mode()[0])\ntrain_test['Exterior1st']=train_test['Exterior1st'].fillna(train_test['Exterior1st'].mode()[0])\ntrain_test['Exterior2nd']=train_test['Exterior2nd'].fillna(train_test['Exterior2nd'].mode()[0])\ntrain_test['MasVnrType']=train_test['MasVnrType'].fillna(train_test['MasVnrType'].mode()[0])\ntrain_test['GarageType']=train_test['GarageType'].fillna(train_test['GarageType'].mode()[0])\ntrain_test['SaleType']=train_test['SaleType'].fillna(train_test['SaleType'].mode()[0])\n######\ntrain_test['GarageYrBlt']=train_test['GarageYrBlt'].fillna(train_test['GarageYrBlt'].mode()[0])\ntrain_test['GarageCars']=train_test['GarageCars'].fillna(train_test['GarageCars'].mode()[0])\n#####\n#Finally, do one-hot encode of all nominal variables\nnominal_features_new = ['MSSubClass', 'MSZoning', 'Street', 'Alley', 'LandContour', 'LotConfig', 'Neighborhood', 'Condition1', \\\n                    'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', \\\n                    'MasVnrType', 'Foundation','Heating','CentralAir', 'GarageType', 'SaleType', \\\n                    'SaleCondition']\n\ntrain_test_nominal_dummy = pd.get_dummies(train_test, columns=nominal_features_new, drop_first=True)\n\n#Divide into training & test sets\ntraining_set = train_test_nominal_dummy.iloc[:1460,:]\ntest_set = train_test_nominal_dummy.iloc[1460:,:]\ndel test_set['SalePrice']","484345b1":"training_set","ce09b754":"test_set","3f352f34":"train_cols = training_set.columns\ntest_cols = test_set.columns\nfor item in train_cols:\n    if item not in test_cols:\n        print(item)\n    else:\n        continue","7b601673":"# step1. according to data_description, some ordinal values have common categorized. That is, the value of those data are Excellent, Good, Average\/Typical, Fair, Poor; which can be transform into integers in descending order using pre-defined dictionary:\n# function for step 1:\ndef scale1(df, column_list):\n    for column in column_list:\n        df[column] = df[column].map({\"Ex\":5,\"Gd\":4,\"TA\":3,\"Fa\":2,\"Po\":1,\"NA\":0})\n        # Excellent, Good, Average\/Typical, Fair, Poor, None, in descending order\n        df[column] = pd.to_numeric(df[column])\n        df[column] = df[column].fillna(0)\n\n# the variables that satisfy this naming orders are as follows:       \ncolumn_list = ['BsmtQual','BsmtCond',\"ExterQual\",\"ExterCond\",\"FireplaceQu\",\"GarageQual\",\"GarageCond\",\"HeatingQC\",\"KitchenQual\",\"PoolQC\"]\n# training_set\nscale1(training_set,column_list)\n# test_set\nscale1(test_set,column_list)\n\n        \n# step2. by similar means, we create a general functions that transform the ordinal value of each variables into non-negative integers using dictionary:\ndef scale2(column, reassign_dict, data_set):\n    data_set[column] = pd.to_numeric(data_set[column].map(reassign_dict))\n    data_set[column] = data_set[column].fillna(0)\n    \n# training set\nscale2(\"LotShape\",{'Reg':3, 'IR1':2, 'IR2':1, 'IR3':0},training_set)\nscale2(\"Utilities\",{'AllPub':3, 'NoSewr':2, 'NoSeWa':1, 'ELO':0},training_set)\nscale2(\"LandSlope\",{'Gtl':2, 'Mod':1, 'Sev':0},training_set)\nscale2(\"BsmtExposure\",{'Gd':4, 'Av':3, 'Mn':2, 'No':1, 'NA':0},training_set)\nscale2(\"BsmtFinType1\",{'GLQ':6, 'ALQ':5, 'BLQ':4,'Rec':3, 'LwQ':2, 'Unf':1, 'NA':0},training_set)\nscale2(\"BsmtFinType2\",{'GLQ':6, 'ALQ':5, 'BLQ':4,'Rec':3, 'LwQ':2, 'Unf':1, 'NA':0},training_set)\nscale2(\"Electrical\",{'SBrkr':4,'FuseA':3, 'FuseF':2, 'FuseP':1, 'Mix':0},training_set)\nscale2(\"Functional\",{'Typ':7, 'Min1':6, 'Min2':5, 'Mod':4, 'Maj1':3, 'Maj2':2, 'Sev':1, 'Sal':0},training_set)\nscale2(\"GarageFinish\",{'Fin':2, 'Rfn':1, 'Unf':0},training_set)\nscale2(\"PavedDrive\",{'Y':2, 'P':1, 'N':0},training_set)\nscale2(\"Fence\",{'GdPrv':4, 'MnPrv':3, 'GdWo':2, 'MuWw':1, 'NA':0},training_set)\n# test set\nscale2(\"LotShape\",{'Reg':3, 'IR1':2, 'IR2':1, 'IR3':0},test_set)\nscale2(\"Utilities\",{'AllPub':3, 'NoSewr':2, 'NoSeWa':1, 'ELO':0},test_set)\nscale2(\"LandSlope\",{'Gtl':2, 'Mod':1, 'Sev':0},test_set)\nscale2(\"BsmtExposure\",{'Gd':4, 'Av':3, 'Mn':2, 'No':1, 'NA':0},test_set)\nscale2(\"BsmtFinType1\",{'GLQ':6, 'ALQ':5, 'BLQ':4,'Rec':3, 'LwQ':2, 'Unf':1, 'NA':0},test_set)\nscale2(\"BsmtFinType2\",{'GLQ':6, 'ALQ':5, 'BLQ':4,'Rec':3, 'LwQ':2, 'Unf':1, 'NA':0},test_set)\nscale2(\"Electrical\",{'SBrkr':4,'FuseA':3, 'FuseF':2, 'FuseP':1, 'Mix':0},test_set)\nscale2(\"Functional\",{'Typ':7, 'Min1':6, 'Min2':5, 'Mod':4, 'Maj1':3, 'Maj2':2, 'Sev':1, 'Sal':0},test_set)\nscale2(\"GarageFinish\",{'Fin':2, 'Rfn':1, 'Unf':0},test_set)\nscale2(\"PavedDrive\",{'Y':2, 'P':1, 'N':0},test_set)\nscale2(\"Fence\",{'GdPrv':4, 'MnPrv':3, 'GdWo':2, 'MuWw':1, 'NA':0},test_set)","e4b7c3ae":"training_set","08083bdd":"test_set","aa253a55":"import copy\ntrain_select = copy.deepcopy(training_set)","47bea5ed":"# continous features,use correlation\n\ncontinuous_features = ['LotFrontage','LotArea','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','1stFlrSF','2ndFlrSF','LowQualFinSF',\\\n                      'GrLivArea','GarageArea','WoodDeckSF','OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MiscVal',\\\n                      'logGrLivArea','log1stFlrSF','logLotArea','logLotFrontage','SalePrice']\nchosen_feature = pd.DataFrame(training_set,columns =continuous_features)\ndata = chosen_feature.iloc[:,:] \nf,ax = plt.subplots(figsize=(15, 15)) \nsns.heatmap(data.corr(), annot=True, linewidths=.5, fmt= '.2f',ax=ax,robust = True) \nplt.show()","73cefe1a":"# ordinal features,using ANOVA correlation coefficient     \nordinal_list = ['BsmtQual','BsmtCond',\"ExterQual\",\"ExterCond\",\"FireplaceQu\",\"GarageQual\",\"GarageCond\",\"HeatingQC\",\"KitchenQual\",\"PoolQC\",\\\n              'LotShape','Utilities','LandSlope','BsmtExposure','BsmtFinType1','BsmtFinType2','Electrical','Functional','GarageFinish','PavedDrive','Fence']\nfrom sklearn.feature_selection import f_classif\nf,p = f_classif(training_set[ordinal_list], training_set['SalePrice'])\nprint(f)\n","2e50f243":"cols = list(train_select.columns)\ncols.remove('SalePrice')\nlen(cols)","107a092a":"# feature selection,lightgbm-feature_importance\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfeature_importances = np.zeros(training_set.shape[1]-1)\ntraining = train_select[cols]\ntraining_labels = train_select['SalePrice']\nclt = lgb.LGBMRegressor(boosting_type = 'goss',b_estimators = 10000)\n# avoid overfitting\nfor i in range(10):\n    # create training and validation set from train_f\n    train_set, valid_set, train_y, valid_y = train_test_split(training,training_labels,test_size = 0.25, random_state = i)\n    # training model\n    clt.fit(train_set,train_y,early_stopping_rounds = 200, eval_set = [(valid_set,valid_y)],eval_metric = 'rmse',verbose = 200)\n    # save feature importance\n    feature_importances += clt.feature_importances_\n# use the average of two feature_impotances result\nfeature_importances = feature_importances\/10\nfeature_importances = pd.DataFrame({'Feature': list(training.columns),'importance':feature_importances}).sort_values('importance',ascending = False) ","300b1012":"feature = feature_importances.loc[feature_importances['importance']>10,:]","43d0e312":"col_n = []\nfor item in feature['Feature']:\n    col_n.append(item)\nprint(len(col_n))\n","9bab22a9":"test_f = test_set[col_n]","19b6bfc8":"train_f = training_set[col_n]","b0951671":"train_f","94d63e9f":"col_name = list(train_f.columns)\ncount = 0\nfor i in range(len(col_name)):\n    name = col_name[i]\n    missing_value = train_f[name].isnull().sum()\n    if missing_value != 0:\n        count = count+1\n        percent = (missing_value\/len(train_f[name]))*100\n        print(col_name[i])\n        print(missing_value,end = \" \")      \n        print(percent,end = '%')\n        print()\n        \n    else:\n        continue\nprint(count)   ","30aaa6f8":"col_name = list(test_f.columns)\ncount = 0\nfor i in range(len(col_name)):\n    name = col_name[i]\n    missing_value = test_f[name].isnull().sum()\n    if missing_value != 0:\n        count = count+1\n        percent = (missing_value\/len(test_f[name]))*100\n        print(col_name[i])\n        print(missing_value,end = \" \")      \n        print(percent,end = '%')\n        print()\n        \n    else:\n        continue\nprint(count)   ","bf8cd44a":"test_f","4c76b1a5":"app_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')","4b52266e":"continuous_features = ['LotFrontage','LotArea','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','1stFlrSF','2ndFlrSF','LowQualFinSF',\\\n                      'GrLivArea','GarageArea','WoodDeckSF','OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MiscVal']\n\nX = training_set[continuous_features]","cad6c44f":"# implement PCA  \nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n%pylab inline\npca1 = PCA(n_components='mle',whiten=True) #,svd_solver='arpack'\npca1.fit(X)\nx_pca1 = pca1.transform(X)\n# explained variance or eigenvalues \n# note that they are sorted in an increasing order\nL = pca1.explained_variance_ \n#  Scree plot \nfig, (ax1, ax2) = plt.subplots(2, 1)\nfig.align_ylabels()\nfig.suptitle('Scree Plot For PCA')\nax1.xaxis.set_major_locator(MaxNLocator(integer = True))\nax2.xaxis.set_major_locator(MaxNLocator(integer = True))\n# explained variance \nax1.plot(L,'o-')\nax1.set_ylabel('Eigenvalues')\nax1.grid()\ncmsm = np.cumsum(L)\n# cumulative Variance Ratio\nax2.plot(cmsm\/cmsm[-1],'o-r')\nax2.set_xlabel('Principal Components')\nax2.set_ylabel('Cumulative Variance Ratio');\nax2.grid()\ncmsm\/cmsm[-1]\n#pca1.get_params()\n## here we choose the component equal to 4","58ba5a4b":"from sklearn.decomposition import FactorAnalysis, PCA\nn_comps = 4\n\nmethods = [\n    (\"PCA\", PCA()),\n    (\"Unrotated FA\", FactorAnalysis()),\n    #(\"Varimax FA\", FactorAnalysis(rotation=\"varimax\")),\n]\nfig, axes = plt.subplots(ncols=len(methods), figsize=(10, 8))\n\nfor ax, (method, fa) in zip(axes, methods):\n    fa.set_params(n_components=n_comps)\n    fa.fit(X)\n\n    components = fa.components_.T\n    print(\"\\n\\n %s :\\n\" % method)\n    print(components)\n\n    vmax = np.abs(components).max()\n    ax.imshow(components, cmap=\"RdBu_r\", vmax=vmax, vmin=-vmax)\n    ax.set_yticks(np.arange(len(continuous_features)))\n    if ax.is_first_col():\n        ax.set_yticklabels(continuous_features)\n    else:\n        ax.set_yticklabels([])\n    ax.set_title(str(method))\n    ax.set_xticks([0, 1 , 2, 3])\n    ax.set_xticklabels([\"C 1\", \"C 2\", \"C 3\", \"C 4\"])\nfig.suptitle(\"Factors\")\nplt.tight_layout()\nplt.show()","264878ed":"from sklearn.linear_model import LinearRegression\nmodel = LinearRegression().fit(X, training_set['SalePrice'])\nr_sq = model.score(X, training_set['SalePrice'])\nprint('coefficient of determination for original numerical data:', r_sq)\nfrom sklearn.linear_model import LinearRegression\npca4 = PCA(n_components=4,whiten=True) #,svd_solver='arpack'\npca4.fit(X)\nx_pca4 = pca4.transform(X)\nmodel = LinearRegression().fit(x_pca4, training_set['SalePrice'])\nr_sq = model.score(x_pca4, training_set['SalePrice'])\nprint('coefficient of determination for transformed data:', r_sq)","3b9264ed":"xt_pca4 = pca4.transform(test_set[continuous_features])\ntrain_sel = training_set.copy().drop(continuous_features, axis='columns').drop('Id', axis='columns')\ntest_sel = test_set.copy().drop(continuous_features, axis='columns').drop('Id', axis='columns')\ntrain_f2 = pd.concat([pd.DataFrame(x_pca4, columns = ['Comp A','Comp B','Comp C','Comp D']), train_sel], axis=1)\ntest_f2 = pd.concat([pd.DataFrame(xt_pca4, columns = ['Comp A','Comp B','Comp C','Comp D']), train_sel], axis=1)","d0bcd9a7":"col_name = list(train_f2.columns)\ncount = 0\nfor i in range(len(col_name)):\n    name = col_name[i]\n    missing_value = train_f2[name].isnull().sum()\n    if missing_value != 0:\n        count = count+1\n        percent = (missing_value\/len(train_f2[name]))*100\n        print(col_name[i])\n        print(missing_value,end = \" \")      \n        print(percent,end = '%')\n        print()\n        \n    else:\n        continue\nprint(count)   ","0d2a6428":"col_name = list(test_f2.columns)\ncount = 0\nfor i in range(len(col_name)):\n    name = col_name[i]\n    missing_value = train_f2[name].isnull().sum()\n    if missing_value != 0:\n        count = count+1\n        percent = (missing_value\/len(train_f2[name]))*100\n        print(col_name[i])\n        print(missing_value,end = \" \")      \n        print(percent,end = '%')\n        print()\n        \n    else:\n        continue\nprint(count)   ","21649672":"# pip install git+https:\/\/github.com\/MaxHalford\/Prince\n# import prince\n# famd = prince.FAMD(n_components=2, n_iter=10,copy=True,check_input=True,engine='auto',random_state=42)\n# famd = famd.fit(train)","529ba739":"# import sklearn\n# from sklearn.linear_model import LinearRegression\n# ntrain_f = sklearn.preprocessing.normalize(train_f, norm='l2', axis=1, copy=True, return_norm=False)\n# ntest_f = sklearn.preprocessing.normalize(test_f, norm='l2', axis=1, copy=True, return_norm=False)\n# reg = lgb.LGBMRegressor(boosting_type = 'goss',n_estimators = 10000).fit(ntrain_f,training_set['SalePrice'])\n# y_pred = reg.predict(ntest_f)","da45bee8":"# submit3 = app_test[['Id']]\n# submit3['SalePrice'] = y_pred\n# submit3.to_csv('try3.csv', index=False)","ae56814b":"from sklearn.linear_model import LinearRegression\nreg = lgb.LGBMRegressor(boosting_type = 'goss',n_estimators = 10000).fit(train_f,training_set['SalePrice'])\ny_pred = reg.predict(test_f)\nsubmit0 = app_test[['Id']]\nsubmit0['SalePrice'] = y_pred\nsubmit0.to_csv('try2.csv', index=False)\n","e691d810":"cols = list(training_set.columns)\ncols.remove('SalePrice')\nlen(cols)","ae9febfa":"from sklearn.linear_model import LinearRegression\nreg = lgb.LGBMRegressor(\n    n_estimators=10000,\n    learning_rate=0.02,\n    num_leaves=32,\n    colsample_bytree=0.9497036,\n    subsample=0.8715623,\n    max_depth=8,\n    reg_alpha=0.04,\n    reg_lambda=0.073,\n    min_split_gain=0.0222415,\n    min_child_weight=40,\n    silent=-1,\n    verbose=-1)\nresult = reg.fit(training_set[cols],training_set['SalePrice'])\ny_pred = result.predict(test_set)\nsubmit1 = app_test[['Id']]\nsubmit1['SalePrice'] = y_pred\nsubmit1.to_csv('try3.csv', index=False)","b5cb7bf8":"from xgboost import XGBRegressor\nclf1 = XGBRegressor(\n    learning_rate =0.01,\n    n_estimators=10000,\n    max_depth=4,\n    min_child_weight=4,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    objective= 'reg:linear',\n    scale_pos_weight=2,\n    seed=27)\nresult = clf1.fit(training_set[cols],training_set['SalePrice'])\ny_pred = result.predict(test_set)\nsubmit1 = app_test[['Id']]\nsubmit1['SalePrice'] = y_pred\nsubmit1.to_csv('try4.csv', index=False)","eb76b658":"# # DNN\n# from keras.callbacks import ModelCheckpoint\n# from keras.models import Sequential\n# from keras.layers import Dense, Activation, Flatten\n\n# NN_model = Sequential()\n\n# # The Input Layer :\n# NN_model.add(Dense(128, kernel_initializer='normal',input_dim = train.shape[1], activation='relu'))\n\n# # The Hidden Layers :\n# NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n# NN_model.add(Dense(256, kernel_initializer='normal',activation='sigmoid'))\n# NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n\n# # The Output Layer :\n# NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n\n# # Compile the network :\n# NN_model.compile(loss='root_mean_square_error', optimizer='adam', metrics=['root_mean_square_error'])\n# NN_model.summary()","c3ae34b2":"\n# checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n# checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n# callbacks_list = [checkpoint]\n# NN_model.fit(training_set[cols], training_set['SalePrice'], epochs=500, batch_size=32, validation_split = 0.2, callbacks=callbacks_list)\n","c95b0966":"training_set_f = training_set[cols]\ntraining_labels= training_set['SalePrice']","0e4209f9":"# \u6a21\u578b\u6574\u5408 stacking\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import StackingRegressor\nclf0 = lgb.LGBMRegressor(\n    n_estimators=10000,\n    learning_rate=0.02,\n    num_leaves=32,\n    colsample_bytree=0.9497036,\n    subsample=0.8715623,\n    max_depth=8,\n    reg_alpha=0.04,\n    reg_lambda=0.073,\n    min_split_gain=0.0222415,\n    min_child_weight=40,\n    silent=-1,\n    verbose=-1)\nclf2 = XGBRegressor(\n    learning_rate =0.01,\n    n_estimators=10000,\n    max_depth=4,\n    min_child_weight=4,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    objective= 'reg:linear',\n    scale_pos_weight=2,\n    seed=27)\nclf1 = XGBRegressor(\n    learning_rate =0.01,\n    n_estimators=10000,\n    max_depth=4,\n    min_child_weight=4,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    objective= 'reg:linear',\n    scale_pos_weight=2,\n    seed=27)\nests = [('lgm',clf0),('xgb',clf2)]\nsclf = StackingRegressor(estimators = ests,\n                         final_estimator = clf1,\n                         )\n\nsclf.fit(training_set_f,training_labels)\ny_pred = sclf.predict(test_set)\nsubmit1 = app_test[['Id']]\nsubmit1['SalePrice'] = y_pred\nsubmit1.to_csv('try5.csv', index=False)","914b8a6b":"**LowQualFinSF**","8867dfd3":"**LotArea**","de8ad992":"Apply the same transformation to the test set and combine the transformed continuous data with the other categorical data (which are transformed into intergers).","ef781670":"**MasVnrArea**","db88939f":"### **Ordinal features**","55cef985":"## **Visualization**","9f1cffb7":"### Here we try to use normalization to the first kind of data selection, the result is not great","b97a520c":"**Nominal Features eda**","fdb3f6d8":"# **Prediction**","cf1203cf":"**GarageArea**","1df9f15c":"### **Continuous features eda**","f9ea1c7a":"**Porch**","655341a7":"### **Nominal features**","e072d1d6":"**GrLivArea**","d96f5b60":"# **Model Training** \nUse data with approach 1","98d88d59":"**LotFrontage**","718a0b0e":"### **Approach 2: partial PCA of Dimension Reduction**.\nThere has being a debating about whether it is peasible to use PCA to procee the categorical data. Our original though is to extract the original continuous variables and reduce the dimension of these data using principal component analysis, and then combine the categorical data with them to create a new data set. Here are the steps:\n* Select the component size of PCA according to the eigenvalues and cumulative variance ratio.\n* Apply the PCA function to the training data set obtain the transformed data.\n* Visualize the result, (here we aslo tried Factor Analysis), and test it with simple regression model.\n* Then we can also apply the same transformation to the test set. \n* Combine the transformed continuous data with the other categorical data (which are transformed into intergers).\n\nTo refine our data selection, we also use a package called prince that can process Factor Analyais for mixture data, which could be a better approach to our problem.","d6a76ef9":"> those features have correlation <0.1 are likely useless, need to be removed.(MiscVal,PoolArea,3SsnPorch,LowQualFinSF,BsmtFinSF2)","8cea6907":"**FlrSF series**","1a9aa4f1":"**Bsmt Serise(may combination)**","338baa50":"### **Future Approach: FA of Dimension Reduction**\n### There is a package in python that can process Factor Anslysis of Mixture data","1d2998c9":"# **Features Selection and Model Selection**","7273b7ca":"### **Discrete features**","38089448":"**PoolArea**","f9b0f57a":"# **Data Preprocessing and Features Engineering**","d3eef2e5":"### **Continuous features**","6d08fb10":"**MiscVal**","beefa9fc":"**WoodDeckSF**","8350b83e":"**numerical features eda**","f2cc656a":"# **EDA**"}}