{"cell_type":{"83adb4b4":"code","df7a0719":"code","eb739bee":"code","a368a5c0":"code","456e6f34":"code","f6c44e1a":"code","b90365b7":"code","02a426e8":"code","943c83b3":"code","643898c3":"code","5b01e092":"code","08acaa87":"code","6086feaf":"code","cf27fd45":"code","296e0d4b":"code","84369b65":"code","b59b8d7c":"code","a62b6980":"code","1ea1c0a5":"code","d4102804":"markdown","d60d6f72":"markdown","e7f9fde4":"markdown","310b1fcc":"markdown","01fb568f":"markdown","c183b9dd":"markdown"},"source":{"83adb4b4":"from __future__ import absolute_import, division, print_function, unicode_literals\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, concatenate, Input\nimport pandas as pd\nimport numpy as np\nimport os\nimport time\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import PillowWriter\nimport matplotlib.animation as animation\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler, PowerTransformer, QuantileTransformer\n\n","df7a0719":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","eb739bee":"PATH = \"\/kaggle\/input\"","a368a5c0":"def load(file):\n    df = pd.read_csv(file, sep=' ', index_col=0, \n                 names=['time', 'temp', 'c1', 'c2', 'c3', 'x1', 'x2', 'x3'])\n    \n    input_c = df[['c1', 'c2', 'c3']].values\n    input_c[:,2] = np.abs(input_c[:,2])\n    output_x = df[['x1', 'x2', 'x3']].values\n\n    input_c = tf.cast(input_c, tf.float32)\n    output_x = tf.cast(output_x, tf.float32)\n\n    return input_c, output_x","456e6f34":"def density_plot(inp_trans, out_trans, num_first, list_lim):\n\n    fig, axs = plt.subplots(ncols=4, figsize=(20,4))\n\n    for i in range(3):\n        sns.kdeplot(inp_trans[:num_first,i], out_trans[:num_first,i],  n_levels=10, \n                    cmap=\"Blues\", shade=True, shade_lowest=False, ax=axs[i])\n        sns.regplot(inp_trans[:num_first,i],out_trans[:num_first,i], scatter=False, \n                    color='blue', ax=axs[i])\n        axs[i].set_xlim(list_lim[i])\n        axs[i].set_ylim(list_lim[i])\n\n    sns.kdeplot(np.sum(inp_trans[:num_first]**2, axis=1), np.sum(out_trans[:num_first]**2, axis=1),  \n                n_levels=10, cmap=\"Blues\", shade=True, shade_lowest=False, ax=axs[3])\n    sns.regplot(np.sum(inp_trans[:num_first]**2, axis=1), np.sum(out_trans[:num_first]**2, axis=1), \n                scatter=False, color='blue', ax=axs[3])\n    axs[3].set_xlim(list_lim[3])\n    axs[3].set_ylim(list_lim[3])\n    ","f6c44e1a":"inp, out = load(PATH+'\/He_300_rough_collision.dat')\nscale = QuantileTransformer(n_quantiles=500, output_distribution='normal')\ninp_trans = scale.fit_transform(inp)\nout_trans = scale.transform(out)\n\ninp_trans = tf.cast(inp_trans, tf.float32)\nout_trans = tf.cast(out_trans, tf.float32)\n\ncollision_data = tf.concat([inp, out], axis=1)\n\n","b90365b7":"num_first = 1000\nlist_lim=[[-3,3],[-3,3],[-3,3],[-0.5,7.5]]\ndensity_plot(inp_trans.numpy(), out_trans.numpy(), num_first,list_lim)","02a426e8":"num_first = 1000\nlist_lim=[[-25,25],[-25,25],[0,25],[-25,700]]\ndensity_plot(inp.numpy(), out.numpy(), num_first,list_lim)","943c83b3":"noise_dim = 3\ncondition_dim = 3\ngen_dim = 3\ninitializer = tf.random_normal_initializer(0., 1)\n\ndef Generator():\n    inp_condition = Input(shape=[condition_dim,], name='condition_G')\n    inp_noise = Input(shape=[noise_dim,], name='noise')\n    X = concatenate([inp_condition, inp_noise], axis=1)\n    \n    X = Dense(32, activation='relu')(X)\n    X = Dense(32, activation='relu')(X)\n    X = Dense(16, activation='relu')(X)\n    last = Dense(gen_dim)(X)\n    \n    return tf.keras.Model(inputs=[inp_condition, inp_noise], outputs=last, name='Generator')\n\ndef Discriminator():\n    inp_condition = Input(shape=[condition_dim,], name='condition_D')\n    inp_target = tf.keras.layers.Input(shape=[gen_dim,], name='target')\n    X = concatenate([inp_condition, inp_target], axis=1)\n    \n    X = Dense(32, activation='relu')(X)\n    X = Dense(32, activation='relu')(X)\n    X = Dense(16, activation='relu')(X)\n    last = Dense(1)(X)\n    \n    return tf.keras.Model(inputs=[inp_condition, inp_target], outputs=last, name='Discriminator')","643898c3":"num_test = 1000\ngenerator = Generator()\ndiscriminator = Discriminator()\ngenerator.summary()\ndiscriminator.summary()\n\n\nnoise = tf.random.normal([num_test, noise_dim])\ngenerated_out = generator([noise,noise], training=False)\n\ndecision = discriminator([noise,generated_out])\n\n# print(generated_out)\n# print(decision)\n\n# Initial plot\nlist_lim=[[-3,3],[-3,3],[-3,3],[-0.5,7.5]]\ndensity_plot(noise.numpy(), generated_out.numpy(), num_test, list_lim)","5b01e092":"lambda_reg = .5\n\ndef discriminator_loss(D_real, D_fake, penalty):\n    D_loss = tf.reduce_mean(D_fake - D_real + lambda_reg * penalty)\n    return D_loss\n\ndef generator_loss(D_fake):\n    G_loss = -tf.reduce_mean(D_fake)\n    return G_loss\n","08acaa87":"generator_optimizer = tf.keras.optimizers.Adam(1e-3, beta_1=0.5, beta_2=0.9)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(1e-3, beta_1=0.5, beta_2=0.9)\ncheckpoint_dir = '.\/training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\ncheckpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n                                 discriminator_optimizer=discriminator_optimizer,\n                                 generator=generator,\n                                 discriminator=discriminator)","6086feaf":"def penalty_calculation(X_real, G_fake, condition):\n    # Create the gradient penalty operations.\n    epsilon = tf.random.uniform(shape=tf.shape(X_real), minval=0., maxval=1.)\n    interpolation = epsilon * X_real + (1 - epsilon) * G_fake\n    with tf.GradientTape() as pena_tape:\n        pena_tape.watch(interpolation)\n        penalty = (tf.norm(\n            pena_tape.gradient(discriminator([condition, interpolation]), interpolation),\n            axis=1) - 1) ** 2.0\n    \n    return penalty","cf27fd45":"@tf.function\ndef train_G(data_batch):\n    noise = tf.random.normal([data_batch.shape[0], noise_dim], mean=0.0, stddev=1.0, \n                             dtype=tf.dtypes.float32)\n    condition = data_batch[:, :3]\n    \n    with tf.GradientTape() as gen_tape:        \n        G_fake = generator([condition, noise], training=True)\n        D_fake = discriminator([condition, G_fake], training=True)\n        G_loss = generator_loss(D_fake)\n\n    gradients_of_generator = gen_tape.gradient(G_loss, generator.trainable_variables)\n    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n    \n    return G_loss\n    \n@tf.function\ndef train_D(data_batch):\n    noise = tf.random.normal([data_batch.shape[0], noise_dim],mean=0.0, stddev=1.0, \n                             dtype=tf.dtypes.float32)\n    condition = data_batch[:, :3]\n    target = data_batch[:, 3:]\n    \n    with tf.GradientTape() as disc_tape:\n        G_fake = generator([condition, noise], training=True)\n\n        D_real = discriminator([condition, target], training=True)\n        D_fake = discriminator([condition, G_fake], training=True)\n        penalty = penalty_calculation(target, G_fake, condition)\n        D_loss = discriminator_loss(D_real, D_fake, penalty)\n\n    gradients_of_discriminator = disc_tape.gradient(D_loss, discriminator.trainable_variables)\n    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n    \n    return D_loss\n","296e0d4b":"num_first = 10_000\nBATCH_SIZE = 1024\n\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices(collision_data[:num_first,:]).batch(BATCH_SIZE)","84369b65":"def train(dataset, epochs, D_cycle=1, steps_show=10):\n    list_lim=[[-25,25],[-25,25],[0,25],[-25,700]]\n    start = time.time()\n    figure, ax = plt.subplots(1, 5, figsize=(15, 3))\n    figure.suptitle(\"Generative Adversarial Network Example (WGAN-GP) on Collision Data\")\n    sns.set(color_codes=True, style='white', palette='colorblind')\n    loss_G_train = []\n    loss_D_train = []\n    for epoch in range(epochs):\n        for data_batch in dataset:\n            G_loss = train_G(data_batch)\n            for _ in range(D_cycle):\n                D_loss = train_D(data_batch)\n        \n        loss_G_train.append(G_loss.numpy())\n        loss_D_train.append(D_loss.numpy())\n                \n        if (epoch+1) % steps_show ==0:   \n            num_test = 1000\n            condition = collision_data[:num_test, :3]\n            noise = tf.random.normal([num_test, noise_dim],mean=0.0, stddev=1.0, dtype=tf.dtypes.float32)\n            generated_out = generator([condition, noise], training=True)\n            \n            ### Figure Velocity Density ###\n            for i in range(3):\n                ax[i].clear()\n                ax[i].set_ylim(list_lim[i])\n                ax[i].set_xlim(list_lim[i])\n                \n                plot_data = collision_data.numpy()[:num_test,:]\n                ax[i].plot(plot_data[:num_test,i], plot_data[:num_test,i+3], '.b', alpha=.1)\n                plot_data = np.concatenate([condition.numpy(), generated_out.numpy()], axis=1)\n                ax[i].plot(plot_data[:num_test,i], plot_data[:num_test,i+3], '.r', alpha=.5)\n                \n                \n            ### Figure Energy Density ###\n            i=3\n            ax[i].clear()\n            ax[i].set_ylim(list_lim[i])\n            ax[i].set_xlim(list_lim[i])\n            \n            plot_data = collision_data.numpy()[:num_test,:]\n            ax[i].plot(np.sum(plot_data[:num_test,:3]**2, axis=1), \n                       np.sum(plot_data[:num_test,3:]**2, axis=1), '.b', alpha=.1)\n            plot_data = np.concatenate([condition.numpy(), generated_out.numpy()], axis=1)\n            ax[i].plot(np.sum(plot_data[:num_test,:3]**2, axis=1), \n                       np.sum(plot_data[:num_test,3:]**2, axis=1), '.r', alpha=.5)\n            \n            ### Figure Discriminator Loss ###\n            i=4\n            ax[i].clear()\n            ax[i].plot([-i for i in loss_D_train], '-')\n            ax[i].set_title('Negative critic loss')\n            ax[i].set_xlabel('Epoch')\n#             ax[i].set_xlim([0, epochs])\n            \n#             ### Flush ###\n            figure.canvas.draw()\n            figure.canvas.flush_events()\n            plt.pause(0.00001)\n#             figure.show()\n#         if (epoch+1 )% 5 ==0: \n#             E_number = str(epoch+1)\n#             E_number = '0'*(4-len(E_number)) + E_number\n#             figure.savefig(f'.\/img_ani\/img_{E_number}.png', bbox_inches = 'tight', pad_inches=0.3)\n            \n            \n        if (epoch+1 )% 50 ==0:     \n            print ('Time for epoch {}\/{} is {} sec'.format(epoch+1,epochs, time.time()-start))\n            start = time.time()\n    figure.show()\n    \n    return loss_G_train, loss_D_train\n\n","b59b8d7c":"## About 3 mins\n# %matplotlib qt\nloss_G_train, loss_D_train = train(train_dataset, epochs=100, D_cycle=5, steps_show=2)\n%matplotlib inline","a62b6980":"num_first = 5000\n# list_lim=[[-1,1],[-1,1],[-1,1],[0,1]]\nlist_lim=[[-25,25],[-25,25],[0,25],[-25,700]]\n\nplot_data = collision_data[:num_first, :].numpy()\ndensity_plot(plot_data[:,:3], plot_data[:,3:], num_first, list_lim)\n","1ea1c0a5":"num_test = 5_000\nnoise = tf.random.normal([num_test, noise_dim])\ngenerated_out = generator([collision_data[:num_test, :3], noise], training=False)\nlist_lim=[[-25,25],[-25,25],[0,25],[-25,700]]\n\nplot_data = np.concatenate([collision_data.numpy()[:num_test, :3], generated_out.numpy()], axis=1)\ndensity_plot(plot_data[:,:3], plot_data[:,3:], num_test, list_lim)\n\n","d4102804":"## Real incident-reflection distribution","d60d6f72":"## Define the loss and optimizers\n","e7f9fde4":"## Generator \n  * The input are 3 conditions c and 3 noise values z.\n  * The output are 3 values x\n  \n## Discriminator\n* The input are 3 conditions c and 3 target\/G_fake values x.\n* The output is one scale value.\n","310b1fcc":"### Train discriminator several times in one generator iteration","01fb568f":"## Define the training loop","c183b9dd":"## cGAN incident-reflection distribution"}}