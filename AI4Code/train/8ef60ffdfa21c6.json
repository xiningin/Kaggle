{"cell_type":{"3a0c27d1":"code","168b9cb3":"code","f9d7e521":"code","88e07cc3":"code","5a3c6e9b":"code","c72ee5cd":"code","a88a0c0d":"code","004720c8":"code","ba360336":"code","2a4e6d14":"code","3057e067":"code","6d009309":"code","f250c7bb":"code","34b37c08":"markdown","69b8e59a":"markdown","224adfb6":"markdown","debe7012":"markdown","501a1979":"markdown","51b0676e":"markdown","ebe4bff4":"markdown","cdd235cc":"markdown","b4644de2":"markdown","5d8cbe1a":"markdown","96fdf19a":"markdown","86b7c7f4":"markdown","f123cf30":"markdown","32b9e46e":"markdown"},"source":{"3a0c27d1":"import matplotlib.pyplot as plt\nplt.figure(figsize=(15,8))\nplt.imshow(plt.imread('..\/input\/learn2reg\/concept_learn2reg.png'))\nplt.axis('off')","168b9cb3":"# This cell imports all requires packages and provides namespace aliases\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport scipy.io\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nprint(os.listdir(\"..\/input\/learn2reg\"))\nimport sys\n# insert at 1, 0 is the script path (or '' in REPL)\nsys.path.insert(1, '..\/input\/learn2reg')\n\nfrom learn2reg_discrete import *\n","f9d7e521":"# Here, we import the image data and perform some preprocessing on the grayvalue images.\n# In addition, to every slice, we provide organ segmentations that will be used for registration evaluation\n# purposes (Dice scores) as well as for the weakly supervised training of the feature extraction \n\n# load TCIA data\nimgs = torch.load('..\/input\/learn2reg\/tcia_prereg_2d_img.pth').float()\/255\nsegs = torch.load('..\/input\/learn2reg\/tcia_prereg_2d_seg.pth').long()\n\n# show an example patient slice\nplt_pat = 17\nplt.figure()\nplt.subplot(121)\nplt.imshow(imgs[plt_pat,:,:].cpu(),'gray')\nplt.subplot(122)\nplt.imshow(segs[plt_pat,:,:].cpu())\n\nprint('# labels:', segs.unique().size(0))","88e07cc3":"# Uncomment to use pretrained MIND-net to extract features\nnet = torch.load('..\/input\/learn2reg\/mindnet_cnn_pancreas.pth')\n\ntorch.manual_seed(10)\n\npat_indices = torch.cat((torch.arange(0,17),torch.arange(18,43)),0)\n\nrnd_perm_idc = torch.randperm(pat_indices.size(0))\npat_indices = pat_indices[rnd_perm_idc]\ntrain_set = pat_indices[:35]\ntest_set = torch.cat((pat_indices[35:],torch.LongTensor([17])),0)\n\n# Now, we prepare our train & test dataset. \ntest_set = torch.LongTensor([35,41,0,4,33,38,39,17])\ntrain_set = torch.arange(43)\nfor idx in test_set:\n    train_set = train_set[train_set != idx]\n\nprint('Test_Set:',test_set)\nprint('Train_Set:',train_set)","5a3c6e9b":"p_fix = train_set[9]\np_mov = train_set[15]\n\n\nimg_fixed = imgs[p_fix:p_fix+1,:,:].unsqueeze(1)#.to(crnt_dev)\nimg_moving = imgs[p_mov:p_mov+1,:,:].unsqueeze(1)#.to(crnt_dev)\nseg_fixed = segs[p_fix:p_fix+1,:,:]\nseg_moving = segs[p_mov:p_mov+1,:,:]\nfeat_fixed = net(img_fixed)\nfeat_moving = net(img_moving)\n\nplt.figure(figsize=(12,8))\nplt.subplot(231)\nplt.imshow(img_fixed[0,0,:,:].cpu().data,'gray')\nplt.subplot(232)\nplt.imshow(feat_fixed[0,0,:,:].cpu().data)\nplt.subplot(233)\nplt.imshow(seg_fixed[0,:,:].cpu().data)\nplt.subplot(234)\nplt.imshow(img_moving[0,0,:,:].cpu().data,'gray')\nplt.subplot(235)\nplt.imshow(feat_moving[0,0,:,:].cpu().data)\nplt.subplot(236)\nplt.imshow(seg_moving[0,:,:].cpu().data)\nplt.show()","c72ee5cd":"displace_range = 9\ndisp_hw = (displace_range-1)\/\/2\n\nB,C,H,W = feat_fixed.size()\nprint(feat_fixed.size())\nssd_distance = correlation_layer(displace_range, feat_moving, feat_fixed)\nprint(ssd_distance.size())\nsoft_cost,xi,yi = meanfield(ssd_distance, img_fixed, displace_range, H,W)\nprint(soft_cost.size())\n\n\nwarp_and_evaluate(xi,yi, img_fixed, img_moving, seg_fixed, seg_moving , displace_range, H, W)\n","a88a0c0d":"# The network defined here has the same architecture as the pretrained network above and we will train it from scratch \n# on the given image data.\nnet = torch.nn.Sequential(torch.nn.Conv2d(1,32,kernel_size=5,stride=2,padding=4,dilation=2),\n                          torch.nn.BatchNorm2d(32),\n                          torch.nn.PReLU(),\n                          torch.nn.Conv2d(32,32,kernel_size=3,stride=1,padding=1,dilation=1),\n                          torch.nn.BatchNorm2d(32),\n                          torch.nn.PReLU(),\n                          torch.nn.Conv2d(32,64,kernel_size=3,stride=2,padding=1,dilation=1),\n                          torch.nn.BatchNorm2d(64),\n                          torch.nn.PReLU(),\n                          torch.nn.Conv2d(64,24,kernel_size=1,stride=1,padding=0,dilation=1),\n                          torch.nn.Sigmoid())\n","004720c8":"def my_correlation_layer(displace_range, feat_moving, feat_fixed):\n    # TODO IMPLEMENT THE CORRELATION LAYER (or find the solution in learn2reg_discrete.py)\n    \n    # tensor dimensionalities in comments are for an arbitrary choice of\n    # displace_range = 11 & feat sizes of [1,24,80,78];\n    # they clearly depend on the actual choice and only serve as numerical examples here.\n    \n    disp_hw = (displace_range-1)\/\/2\n    # feat_mov: [1,24,80,78] -> 24 feature channels + spatial HW dims\n    # feat_mov_unfold: [24,121,6240] -> mind chans, 11*11 = 121 displ steps, 6240 = 80*78 spatial positions\n    feat_moving_unfold = F.unfold(feat_moving.transpose(1,0),(displace_range,displace_range),padding=disp_hw)\n    B,C,H,W = feat_fixed.size()\n    \n    # feat_fixed: [24,1,6240] -> compute scalarproduct along feature dimension per broadcast + sum along 0\n    # and reshape to [1,121,80,78]\n    ssd_distance = ((feat_moving_unfold-feat_fixed.view(C,1,-1))**2).sum(0).view(1,displace_range**2,H,W)\n    #reshape the 4D tensor back to spatial dimensions\n    return ssd_distance#.detach()\nssd_distance = my_correlation_layer(displace_range, feat_moving, feat_fixed)\nprint(ssd_distance.size())","ba360336":"#net = torch.load('mindnet_cnn.pth')\n#net.to(crnt_dev)\nnet.train()\n\noptimizer = optim.Adam(list(net.parameters()),lr=0.00025)\n\nnr_train_pairs = 50\ngrad_accum = 4\n\nfor pdx in range(nr_train_pairs):\n    rnd_train_idx = torch.randperm(train_set.size(0))\n    p_fix = train_set[rnd_train_idx[0]]\n    p_mov = train_set[rnd_train_idx[1]]\n\n    img_fixed = imgs[p_fix:p_fix+1,:,:].unsqueeze(1)#.to(crnt_dev)\n    img_moving = imgs[p_mov:p_mov+1,:,:].unsqueeze(1)#.to(crnt_dev)\n    feat_fixed = net(img_fixed)\n    feat_moving = net(img_moving)\n\n    seg_fixed = segs[p_fix:p_fix+1,:,:]\n    seg_moving = segs[p_mov:p_mov+1,:,:]\n    \n    label_moving = F.one_hot(seg_moving,num_classes=9).permute(0,3,1,2).float()\n    _,C1,Hf,Wf = label_moving.size()\n    label_moving = F.interpolate(label_moving,size=(Hf\/\/4,Wf\/\/4),mode='bilinear')\n    label_fixed = F.one_hot(seg_fixed,num_classes=9).permute(0,3,1,2).float()\n    label_fixed = F.interpolate(label_fixed,size=(Hf\/\/4,Wf\/\/4),mode='bilinear')\n    # generate the \"unfolded\" version of the moving encoding that will result in the shifted versions per channel\n    # according to the corresponding discrete displacement pair\n    label_moving_unfold = F.unfold(label_moving,(displace_range,displace_range),padding=disp_hw).view(1,9,displace_range**2,-1)\n    \n\n    #forward path: pass both images through the network so that the weights appear in the computation graph\n    # and will be updated\n    feat_fixed = net(img_fixed)\n    feat_moving = net(img_moving)\n    # compute the cost tensor using the correlation layer\n    ssd_distance = my_correlation_layer(displace_range, feat_moving, feat_fixed)\n    \n    # compute the MIN-convolution & probabilistic output with the given function\n    soft_cost,xi,yi = meanfield(ssd_distance, img_fixed, displace_range, H, W)\n    # loss computation:\n    # compute the weighted sum of the shifted moving label versions \n    label_warped = torch.sum(soft_cost.cpu().t().unsqueeze(0)*label_moving_unfold.squeeze(0),1)\n    # compute the loss as sum of squared differences between the fixed label representation and the \"warped\" labels\n    label_distance1 = torch.sum(torch.pow(label_fixed.view(8,-1)-label_warped.view(8,-1),2),0)\n    loss = label_distance1.mean()\n    # perform the backpropagation and weight updates\n    loss.backward()\n    \n    if (pdx+1)%grad_accum == 0:\n        # every grad_accum iterations : backpropagate the accumulated gradients\n        optimizer.step()\n        optimizer.zero_grad()\n\n    if(pdx%(nr_train_pairs\/4)==((nr_train_pairs\/4)-1)):\n        print(pdx,loss.item())\n\n","2a4e6d14":"# Validate:\nvalid_pat_fix_idx = -1 # patient 17\nvalid_pat_mov_idx = 0\n\np_fix = test_set[valid_pat_fix_idx] # pat17\np_mov = test_set[valid_pat_mov_idx]\n\n\n# 1) compute the feature representations\nnet = net.eval()\nimg_fixed = imgs[p_fix:p_fix+1,:,:].unsqueeze(1)#.to(crnt_dev)\nimg_moving = imgs[p_mov:p_mov+1,:,:].unsqueeze(1)#.to(crnt_dev)\nfeat_fixed = net(img_fixed)\nfeat_moving = net(img_moving)\n\nseg_fixed = segs[p_fix:p_fix+1,:,:]\nseg_moving = segs[p_mov:p_mov+1,:,:]\n\n# 2) perform the SSD cost calculation based on the correlation layer \nssd_distance = correlation_layer(displace_range, feat_moving, feat_fixed)\n\n\nsoft_cost,xi,yi = meanfield(ssd_distance, img_fixed, displace_range, H, W)\n\nwarp_and_evaluate(xi,yi, img_fixed, img_moving, seg_fixed, seg_moving , displace_range, H, W)","3057e067":"plt.figure(figsize=(20,8))\nplt.imshow(plt.imread('..\/input\/learn2reg\/cnn_matmul.png'))\nplt.axis('off')","6d009309":"plt.figure(figsize=(20,8))\nplt.imshow(plt.imread('..\/input\/learn2reg\/unfold_tensors.png'))\nplt.axis('off')","f250c7bb":"plt.figure(figsize=(20,8))\nplt.imshow(plt.imread('..\/input\/learn2reg\/discrete_weights.png'))\nplt.axis('off')","34b37c08":"In the next part, for this image pair, we demonstrate the basic registration workflow, after having defined the discrete search region size:\n1. compute the cost tensor using the correlation layer\n2. compute the displacements using the meanfield function\n3. applying the displacement field on the moving image with the warp_and_evaluate function","69b8e59a":"#### Correlation layer\nThis *unfolding* operation comes in very handy to implement the correlation layer, where we need to perform a similar operation. While during a convolution, we use the unfold operation to *extract image patches (__regions__)* to be multiplied with a kernel, for the correlation layer, we also need to *extract regions for our __similarity search__* on a predefined discrete neighbourhood grid. \n\nAfter having processed our registration image pair by a __CNN, whose weights will be adjusted,__ we try to find corresponding image contents (cf. the red T) inside the discretized neighbourhood (yellow rectangle). In the example below, the 3x3 search region is defined by discrete voxel displacement pairs in [-1,0,1]x[-1,0,1] on the current resolution and (-1,-1) ideally yields the position with the highest correlation. ","224adfb6":"### 4) Spatial Transform\nPerforming 1 step of the discrete registration approach to check whether so far the implementation is working.\nThe refinement of the learnable feature CNN weights will be performed further down.\n\nHere, the idea is to transfer the regularized cost tensor from above into a displacement field.\n\nTo achieve this, per spatial position, we combine weighted contributions of every possible displacement based on its cost determined by the correlation layer. In essence, applying the softmax function along the (negatively weighted) displacement dimension provides the weighting scheme for each displacement and their summation results in a smooth and continuous displacement per position.","debe7012":"## Registration using pretrained CNNs\nIn the next cell, we load  pretrained network that computes features based on the input images (fixed and moving).","501a1979":"## Introduction\nIn this tutorial on discrete registration we will explore the implementation of the correlation layer, used e.g. in FlowNet to compute a dissimilarity map for a range of discrete displacement. The method is demonstrated on slices of abomdinal CT scans from the TCIA pancreas dataset. ","51b0676e":"## Importing provided helper functions\nuse \"Add Data\" > \"Search by URL\" > and type: kaggle.com\/mattiaspaul\/learn2reg to load the provided dataset, aside images and segmentations it contains the functions warp_and_evaluate, which is used to visualise your results and meanfield, which is required to regularise the probabilistic displacements","ebe4bff4":"## Explanatory Details","cdd235cc":"Next, we want to evaluate our network, that we trained from scratch on the held out test data","b4644de2":"### 3) Min-Convolution\nThe finding of the displacement pair yielding the smallest SSD value corresponds to the idea of finding the feature representations that correlate the most. Since this process tends to be unstable due to noise or ambiguities, in the following, we introduce our two-fold regularisation strategy.  \n\nIn order to compute a reasonable displacement field, a two fold regularisation strategy is employed. Firstly, the subvoxel accuracy determination of the minimum SSD position inside the search region will be computed by a weighted averaging. Secondly, these values are subsequently also averaged along the spatial image dimensions to result in a smooth displacement field. \n\nFor more theoretical insights, we refer you again to https:\/\/arxiv.org\/pdf\/1907.10931 and focus on the practical implementation details below.\n\nFor the subvoxel displacement parameter estimation per spatial position, we pursue the following strategy:\n- Starting with the SSD tensor of size [r_sz x r_sz, H x W] computed in the last step above, we first perform the difussion regularisation using min-convolutions with a lower envelope of parabolas rooted at the displacement offsets with heights equalling the SSD values. This lower envelope is not directly differentiable, but we can obtain a very accurate approximation using first, a min-pooling (with stride=1) that finds local minima in the SSD tensor followed by two average pooling operations (with stride=1) that provide a quadratic smoothing.\n- In order to obtain a reasonable displacement field, this next step is the mean-field inference. Here, we perform an average filtering along the spatial dimensions to get the final cost tensor. ","5d8cbe1a":"In order to compute the correlation values *for every voxel* in the moving image for *all discrete displacement steps in its corresponding search region* in the fixed image, we perform the following steps for an efficient implementation:\n- We use the .view() operation to swap the batch and channel dimensions. For the sake of clarity, we process only one pair at a time. Pushing the channel into the batch dimension allows to leave the feature channels untouched, while \"constructing\" the search region patches along the first dimension with the *unfold* operation functionality. In essence, we change tensors [1, C, H, W] to [C, 1, H, W]. \n- Specifying the search region by (r_sz, r_sz) with r_sz being the size of the region, we use the unfold operation to obtain the \"unfolded\" tensor of dimension [C,r_sz x r_sz, H x W].\n- Now, we are able to compute the sum of squared differences (SSD) values for every spatial position between the moving feature representation *m* and its corresponding search region feature representations in the fixed image *f*. Again, we use the .view() operation to transform the moving tensor [1,C,H,W] to [C,1,H x W]. Relying on the implicit broadcasting along the first dimension when computing (m-f)^2, we sum along the 0th dimension to obtain the SSD values for every spatial position in a tensor of dimension [r_sz x r_sz, H x W]. \n\nPyTorch documentation for the *unfold* operation: https:\/\/pytorch.org\/docs\/stable\/nn.functional.html?highlight=unfold#torch.nn.functional.unfold\n","96fdf19a":"### 2) Correlation layer implementation: Basic steps\n\n#### CNN: Convolution operation\nIn order to \"move\" a filter kernel across an image, behind the scenes an __*unfold*__ operation is used to transfer this operation, i.e. the extraction of image patch sequences, into a matrix multiplication. Based on the kernel size, the patches are arranged into the \"unfolded\" matrix according to their position during the sweeping process of the convolution.\nThe image below illustrates this for an image that is convolved with a 2x2 kernel: consecutively, the red, green, yellow and blue patch are multiplied by the kernel to gain convolution output. Here, a [1,1,3,3] Image-Tensor (batch_sz, #channels, heigth, width) is convolved with a 2x2 kernel (without padding etc.), resulting in a [1,1,2,2] output. Without padding, we have 4 valid patches to apply the 2x2 kernel, therefore the \"unfolded\" matrix is of dimensionality [4x4].     \n\n","86b7c7f4":"## The whole picture\nExtracted from the MICCAI submission (https:\/\/arxiv.org\/pdf\/1907.10931), the figure above illustrates the registration approach that will be implemented below.\n\nThe main idea is to incorporate learnable CNNs as feature extractors into an optimization pipeline that is inspired by a well studied discrete registration approach.","f123cf30":"## Training a CNN for registration\nNext we will start implementing our own correlation layer and start training a network for discrete image registration","32b9e46e":"Now, we visualize a pair of images (fixed & moving), showing their grayvalue images, annotations and  feature representations."}}