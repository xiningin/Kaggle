{"cell_type":{"6fe39aa7":"code","39686f0f":"code","0f70ebf3":"code","7cc484ef":"code","8a94df7e":"code","548ff0bf":"code","d03f7bbb":"code","51761558":"code","d1fa0c90":"code","58673e9c":"code","63ecb358":"code","338e1e84":"code","a1879360":"code","33f74f39":"code","76720712":"markdown","20e133fc":"markdown","ec87551d":"markdown","eca95a47":"markdown","9fc832c8":"markdown","80770d03":"markdown","accaebd0":"markdown","11850b59":"markdown","7a1a4798":"markdown","66f2759d":"markdown","7e97557d":"markdown","20fabd8f":"markdown","9de29375":"markdown"},"source":{"6fe39aa7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","39686f0f":"import nltk\nimport collections\nfrom glob import glob\n\n\n# convert lowercase and delete whitespace\ndef clean(text):\n    text = text.lower()\n    text = \" \".join(text.split())\n    return text\n\n\n# dictionary returning structure to keep frequency values\ndef dict_freq(liste):\n    dict1 = {}\n    for i in liste:\n        if i in dict1:\n            dict1[i] += 1\n        else:\n            dict1[i] = 1\n    return dict1\n\n\n\ndef sorted_dict(sozluk):\n    return sorted(sozluk.items(), key=lambda x: x[1], reverse=True)\n\n\n# Reading files\nraw_text = ''\n\n#Train files\nfilenames = glob('\/kaggle\/input\/brown-corpus\/brown\/brown\/ca*') + glob('\/kaggle\/input\/brown-corpus\/brown\/brown\/cb*') + glob('\/kaggle\/input\/brown-corpus\/brown\/brown\/cc*')\n\ntest_files = ['\/kaggle\/input\/brown-corpus\/brown\/brown\/ca41', \n              '\/kaggle\/input\/brown-corpus\/brown\/brown\/ca42',\n              '\/kaggle\/input\/brown-corpus\/brown\/brown\/ca43', \n              '\/kaggle\/input\/brown-corpus\/brown\/brown\/ca44',\n              '\/kaggle\/input\/brown-corpus\/brown\/brown\/cb26',\n              '\/kaggle\/input\/brown-corpus\/brown\/brown\/cb27',\n              '\/kaggle\/input\/brown-corpus\/brown\/brown\/cc16', \n              '\/kaggle\/input\/brown-corpus\/brown\/brown\/cc17']  # test files\n","0f70ebf3":"# Removing test files from the train list.\nfor i in test_files:\n    if i in filenames:\n        filenames.remove(i)","7cc484ef":"filenames.remove('\/kaggle\/input\/brown-corpus\/brown\/brown\/cats.txt') #cats ??","8a94df7e":"for i in filenames:\n    ff = open(i)\n    raw_text = raw_text + ff.read()\n\n\ntagged_text = clean(raw_text)  # cleaning process","548ff0bf":"# scrapping words and tags\nword_and_tag = [nltk.tag.str2tuple(t) for t in tagged_text.split()]\nkelime_sayisi = len(word_and_tag)","d03f7bbb":"pos_counts = collections.Counter((subl[1] for subl in word_and_tag))\n\nd1 = dict()  # Dictionary structure for finding tag frequencies.\n\n#f = open(\"PosTags.txt\", \"w\")\n#f.write(\"tag - tag_frequency\\n\")\nprint(\"tag - tag_frequency\\n\")\nfor i, j in pos_counts.most_common():\n    d1[i] = j\n    #f.write(str(i) + \"\\t\" + str(j) + \"\\n\")  # Converted to str structure.\n    print(i, \"-\", j)\n\n#f.close()","51761558":"l1 = list()  # tag list to hold bigrams\nfor a, b in nltk.ngrams(word_and_tag, 2):\n    l1.append((a[1], b[1]))\n\nd2 = dict_freq(l1)\nsorted_bigram = sorted_dict(d2)  # Sorting bigrams.\n\nd3 = d2.copy()  # Dictionary structure for holding transition probabilities\n\nfor i, j in sorted_bigram:\n    d3[i] = j \/ d1[i[0]]\n","d1fa0c90":"TransitionProbs = sorted(d3.items(), key=lambda x: (x[0], x[1]),\n                         reverse=False)  # sorted d3 prob. dictionary\n\n#f = open(\"TransitionProbs.txt\", \"w\")\n#f.write(\"taga - tagb - P(tagb|taga)\\n\")\nprint(\"taga - tagb - P(tagb|taga)\\n\")\nfor i, j in TransitionProbs:\n    #f.write(str(i[0]) + \"\\t\" + str(i[1]) + \"\\t\" + str(j) + \"\\n\")\n    print(i[0], '-', i[1], '-', j)\n\n#f.close()","58673e9c":"# Find word frequencies - to find unique word count and frequency before replacing them with unk\npos_counts = collections.Counter((subl[0] for subl in word_and_tag))\n\nd4 = dict()  # Dictionary structure for finding word frequencies.\nfor i, j in pos_counts.most_common():\n    d4[i] = j\nunique_kelime_sayisi = len(d4.keys())  # unique word count before replacing with unk\n\n#################       UNK       ################################## ---- Since the tags are not affected, they were made in the word section.\n\nsiralanmis_kelimeler = sorted_dict(d4)  # Sorting words by frequency\n\nen_az_gecen = list()  # List structure to keep at least 10 last words\nfor i in range(len(siralanmis_kelimeler) - 1, len(siralanmis_kelimeler) - 11, -1):\n    en_az_gecen.append(siralanmis_kelimeler[i][0])\n\n\"\"\"frekansi_1_olan_kelimeler = list()   # List structure to keep words with frequency 1.\nfor i, j in d4.items():\n    if j == 1:\n        frekansi_1_olan_kelimeler.append(i)\"\"\"\n\ndunk = list()  # To create word and tag structure with unk \/ cannot be changed because nltk works with tuple.\nfor i, j in word_and_tag:\n    dunk.extend([[i, j]])\n\nfor i in range(0, len(dunk)):\n    if dunk[i][0] in en_az_gecen:  # Adding unk to the 'word \/ tag' list for at least 10 last words.\n        dunk[i][0] = 'unk'\n\nword_and_tag_unk = list()  # for creating unk with word_and_tag\nfor i, j in dunk:\n    word_and_tag_unk.extend([(i, j)])  # for avoid tuple\n\n#################       UNK       ##################################\n\npos_counts = collections.Counter(\n    (subl[0] for subl in word_and_tag_unk))  # post counts word count with variable word_and_tag_unk\n\nd5 = dict()  # Dictionary structure for holding word frequencies after unk change.\nfor i, j in pos_counts.most_common():\n    d5[i] = j\n\nmostliketag = nltk.FreqDist(word_and_tag_unk)  # most likely tag function.\n\nd6 = dict()  # Dictionary structure for holding the mostlike frequencies of words.\n\n#f = open(\"Vocabulary.txt\", \"w\")\n#f.write(\"Total number of words\" + \" \" + str(kelime_sayisi) + \" \" + \"Vocubulary Size\" + \" \" + str(unique_kelime_sayisi) + \"\\n\")\n#f.write(\"word - Frequency of the word - MostLikelyTag\\n\")\nprint(\"Total number of words\" + \" \" + str(kelime_sayisi) + \" \" + \"Vocubulary Size\" + \" \" + str(unique_kelime_sayisi) + \"\\n\")","63ecb358":"print(\"word - Frequency of the word - MostLikelyTag\\n\")\nfor i, j in mostliketag.items():   # to move the most like tag of words to dictionary structure.\n    d6[i[0]] = i[1]\n\nfor i, j in d5.items():\n    #f.write(str(i) + \"\\t\" + str(j) + \"\\t\" + str(d6[i]) + \"\\n\")\n     print(i, '-', j , '-', d6[i])\n#f.close()","338e1e84":"d7 = dict_freq(word_and_tag_unk)  # Dictionary structure established to find the number of occurrences of words and tags.\n\n# Sort the lines of the log in alphabetical order first by tag, then by word\nsorted_word_tags = sorted(d7.items(), key=lambda x: (x[0][1], x[0][0]), reverse=False)\n\ndf_t = list()  # List structure holding word and tag probabilities to be used in test section\n#f = open(\"EmissionProbs.txt\", \"w\")\n#f.write(\"tag - kelime - P(kelime|tag)\\n\")\nprint(\"tag - word - P(word|tag)\\n\")\nfor i, j in sorted_word_tags:\n    #f.write(str(i[1]) + \"\\t\" + str(i[0]) + \"\\t\" + str((j \/ d1[i[1]])) + \"\\n\")\n    print(i[1], '-', i[0], '-', j\/d1[i[1]] )\n    df_t.extend([[i[0], i[1], (j \/ d1[i[1]])]])  # filling in the list part for the test.\n\n#f.close()","a1879360":"cumleler = nltk.sent_tokenize(tagged_text)  # division into sentences. kept in the list structure.\ncumle_sayisi = len(cumleler)\n\ninit = dict()  # Dictionary structure defined to find the tag frequency at the beginning of the sentence.\nfor i in range(0, cumle_sayisi):\n    test = [nltk.tag.str2tuple(t) for t in cumleler[i].split()]  # separating the words of each sentence into tags.\n\n    if test[0][1] in init:  # First element of the list is the tag of the first word of the sentence.\n        init[test[0][1]] += 1\n    else:\n        init[test[0][1]] = 1\n\nfor i in init:\n    init[i] = init[i] \/ cumle_sayisi  # To replace with new dictionary values by computing initial probabilities\n\nsorted_init = sorted(init.items(), key=lambda x: x[0], reverse=False)  # Sorting by tag\n\n#f = open(\"InitialProbs.txt\", \"w\")\n#f.write(\"tag - P(tag|<s>)\\n\")\nprint(\"tag - P(tag|<s>)\\n\")\nfor i, j in sorted_init:\n    #f.write(str(i) + \"\\t\" + str(j) + \"\\n\")\n    print(i, '-' ,j)\n#f.close()\n\n#ff.close()  # Closing train files.","33f74f39":"#f = open(\"Sonuc.txt\", \"w\")\nerdem = 1  # Control variable for printing the results for ca41.\nwhile erdem <= 2:\n\n    # Reading file\n    raw_text_test = ''\n\n    if erdem == 1:\n        test_files = ['\/kaggle\/input\/brown-corpus\/brown\/brown\/ca41', \n              '\/kaggle\/input\/brown-corpus\/brown\/brown\/ca42',\n              '\/kaggle\/input\/brown-corpus\/brown\/brown\/ca43', \n              '\/kaggle\/input\/brown-corpus\/brown\/brown\/ca44',\n              '\/kaggle\/input\/brown-corpus\/brown\/brown\/cb26',\n              '\/kaggle\/input\/brown-corpus\/brown\/brown\/cb27',\n              '\/kaggle\/input\/brown-corpus\/brown\/brown\/cc16', \n              '\/kaggle\/input\/brown-corpus\/brown\/brown\/cc17']\n    else:\n        test_files = ['\/kaggle\/input\/brown-corpus\/brown\/brown\/ca41']\n\n    for i in test_files:\n        fft = open(i)\n        raw_text_test = raw_text_test + fft.read()\n\n    tagged_text_test = clean(raw_text_test)  # cleaning\n\n    # reading word and tag\n    test_word_and_tag = [nltk.tag.str2tuple(t) for t in tagged_text_test.split()]\n    test_kelime_sayisi = len(test_word_and_tag)\n\n    if erdem == 1:\n        #f.write(\"Total number of words in the test set (all 8 logs)\\t\" + str(test_kelime_sayisi) + \"\\n\")\n        print(\"Total number of words in the test set (all 8 logs)\\t\" + str(test_kelime_sayisi) + \"\\n\")\n    else:\n        #f.write(\"The total number of words in the ca41 log\\t\" + str(test_kelime_sayisi) + \"\\n\")\n        print(\"The total number of words in the ca41 log\\t\" + str(test_kelime_sayisi) + \"\\n\")\n\n    # reading sen.\n    test_cumleler = nltk.sent_tokenize(tagged_text_test)\n\n    kelime_tag_olasilik = list()  # List for keeping tags found according to HMM model.\n\n# VITERBI ALGORITHM\n    for i in range(0, len(test_cumleler)):\n        test = [nltk.tag.str2tuple(t) for t in\n                test_cumleler[i].split()]  # separating the words of each sentence into tags\n\n        for k in range(0, len(test)):  # list to scan words within returned sentences.\n            if test[k][0] in d5:  # If this word is not present in the test set, it is replaced with 'unk'.\n                kelime = test[k][0]\n            else:\n                kelime = 'unk'\n\n            if k == 0:  # The first index of this list is the first word and the initial probe must be calculated.\n                maks = 0\n                tag = 'bo\u015f'\n\n                for z in range(0,\n                               len(df_t)):  # Finding probability and tag of this first word from emission prob.\n                    if kelime == df_t[z][0]:\n\n                        if df_t[z][1] in init:  # Is the given tag included in the inital prob.?\n                            init_prob = init[df_t[z][1]]  # Finding the possibility to tag through the init probe.\n                            emis_prob = df_t[z][2]\n                            olasilik = (emis_prob * init_prob)\n                            if olasilik > maks:  # to find the maximum probability value\n                                maks = olasilik\n                                tag = df_t[z][1]\n\n                        else:\n                            init_prob = 0.00000000001  # If it is not included in inital prob, very small probability value is assigned.\n                            emis_prob = df_t[z][2]\n                            olasilik = (emis_prob * init_prob)\n                            if olasilik > maks:\n                                maks = olasilik\n                                tag = df_t[z][1]\n\n            else:\n                onceki_olasilik = maks  # the previously found probability value.\n                onceki_tag = tag  # the previous tag value.\n                flag = 0\n                maks = 0\n                yeni_tag = 'bo\u015f'\n                for z in range(0, len(df_t)):\n                    if kelime == df_t[z][0]:\n                        if (\n                                onceki_tag,\n                                df_t[z][1]) in d3:  # Checking if the bigram tag is included in the transition probe.\n                            trans_prob = d3[(onceki_tag, df_t[z][1])]\n                            emis_prob = df_t[z][2]\n                            olasilik = (emis_prob * trans_prob * onceki_olasilik)\n                            if olasilik > maks:\n                                maks = olasilik\n                                tag = df_t[z][1]\n                        else:\n                            trans_prob = 0.00000000001  # If it is not included, a very small value is assigned.\n                            emis_prob = df_t[z][2]\n                            olasilik = (emis_prob * trans_prob * onceki_olasilik)\n                            if olasilik > maks:\n                                maks = olasilik\n                                tag = df_t[z][1]\n\n            kelime_tag_olasilik.extend([(kelime, tag)])  # Word and probability values found with viterbi.\n            onceki_tag = tag  # The previous tag becomes the new value found.\n            onceki_olasilik = maks  # the previous probability value becomes the new value found.\n\n    dogru_bulunan_tag_sayisi = 0\n    for i in range(0, len(test_word_and_tag)):\n        if kelime_tag_olasilik[i][1] == test_word_and_tag[i][1]:\n            dogru_bulunan_tag_sayisi += 1\n\n    if erdem == 1:\n        #f.write(\"Number of words with correct POSTags in the test set (all 8 logs)\\t\" + str(dogru_bulunan_tag_sayisi) + \"\\n\")\n        print(\"Number of words with correct POSTags in the test set (all 8 logs)\\t\" + str(dogru_bulunan_tag_sayisi) + \"\\n\")\n    else:\n        #f.write(\"Number of correct POSTag words found in ca41 file\\t\" + str(dogru_bulunan_tag_sayisi) + \"\\n\")\n        print(\"Number of correct POSTag words found in ca41 file\\t\" + str(dogru_bulunan_tag_sayisi) + \"\\n\")\n    if erdem != 1:  # Printing ca41 according to the words and tags found.\n        for i in range(0, len(kelime_tag_olasilik)):\n            #f.write(str(kelime_tag_olasilik[i][0]) + \"\/\" + str(kelime_tag_olasilik[i][1]) + \" \")\n            print(kelime_tag_olasilik[i][0] + \"\/\" + kelime_tag_olasilik[i][1] ,end = \" \")\n    erdem += 1","76720712":"# Emission Probabilities","20e133fc":"Hello everyone. This is my Python code for homework in natural language processing. For this assignment, the professor shared with us a Brown Corpus with separated test and train sets. So that's why I only worked with certain files. The prerequisites for the assignment were as follows.","ec87551d":"# Initial Probabilities","eca95a47":"In this assignment, you will create an HMM PartofSpeechTagger using the scaled down BrownCorpus.\n\nDataSet (Brown_hw.rar) DataSet is divided into two directories as Train and Test. There are 80 files in the Train directory and 8 files in the Test directory. Each row of the log holds a sentence (there are blank lines; you will not consider blank lines). An example sentence is as follows.\n\nAttorneys\/nns for\/in the\/at mayor\/nn said\/vbd that\/cs an\/at amicable\/jj property\/nn settlement\/nn has\/hvz been\/ben agreed\/vbn upon\/rb .\/.\n\nIt is separated from the Part-Of-Speech, which is marked with each word \/ character.\n\nYour program should do the following:\n\nCreation of First-Order-HMM for POS-Tagging:\n\n* Your program will create the First-Order-HMM by reading all the files (80) in the Train directory. This means that all probability data required for the HMM will be collected from the Train set. In addition to the information required for HMM, you may need to collect other information (word frequencies, total word count,..).\n\n* First convert all words to lowercase letters. So all words in your model will only consist of lowercase letters (and other characters).\n \n* While creating the HMM model, assume that lowest 10 frequency words from the Train set as UNK (unknown) words. So you will pretend that the word UNK is used instead of these words. While finding the POS of the test set with the created HMM, if the word is not in the Train set, you will use the UNK word for that word.\n\nWriting HMM Data Collected From Train Set to Logs:\n\n* PosTags.txt: You will write the POStags in the train cluster together with their frequencies in the PosTags.txt file in descending order according to their frequencies (how many times they passed in the test set). Each row of this log must contain a POS tag and its frequency. The first line should keep the most frequently passing POStag and its frequency, and the last line the least passing POStag and its frequency. Each row of this log should look like the following. tag tag_frequency\n\n* TransitionProbs.txt: You will write the TagTransitionProbability values you created for HMM into this file. Each row of this file should hold the following data. taga tagb P(tagb | taga) The lines of this file should be sorted first by taga and then alphabetically by tagb.\n\n* Vocabulary.txt: Write all the words in the train set in descending order according to their frequencies, along with their frequencies and the most probable POSTag (Most Likely Tag). Each row of this log should look like the following. Word - Frequency of the word - MostLikelyTag In the first line of this file, print the total number of words and the number of individual words (Vocubulary Size) in the Train set.\n\n* EmissionProbs.txt: You will write the EmissionProbability values of POSTags to this file. Each row of this log should look like the following. tag - word - P(word | tag) These log lines should be sorted first by tag and then alphabetically by word.\n\n* InitialProbs.txt: Write the probability that POSTags appear at the beginning of the sentence in this file. Each row of this log should look like the following. tag - P(tag | s ) The lines of this file should be sorted alphabetically according to tag.\n\n* Finding POSTags of the Test Set using HMM and writing the obtained results to a file.\n\n* You will find the POSTags of the words in the file in the test set using the HMM you have created and calculate the success rates and write them in the result file. You must first convert the words to lowercase.\n\n* You will find the most probable POSTag order of each sentence (lines of file) in the test set with the help of the HMM and Viterbi algorithm you created. So you will find the POSTag of every word in the sentence. You can find the success value of your model because it is in the correct PosTags in the test set.\n\n* Result.txt: You will write your success results for the test set in this file. This file should contain the following information in order.\n\n* Total number of words in the test set (all 8 logs)\n\n* Number of words with correct POSTags in the test set (all 8 logs)\n\n* Ca41 log results in test directory:\n\n* The total number of words in the ca41 log\n\n* Number of correct POSTag words found in ca41 file\n\n* The sentences of the ca41 file must be written to the Result.txt file together with the found POSTags.","9fc832c8":"**In the assignment, the professor required to print the results into a file. But for Kaggle, I'll close these lines of code and print the results directly.I will do the same for the other stages.**","80770d03":"> The lines of this file should be sorted first by taga and then alphabetically by tagb.\n\nThat's why the defined function is not used when sorting.","accaebd0":"# Vocabulary Section","11850b59":"# Transition Probabilities","7a1a4798":"# The prerequisites for the assignment","66f2759d":"# PosTaging Section","7e97557d":"# Part of Speech Tagging with Viterbi Algorithm","20fabd8f":"# RESULT PART","9de29375":"According to these results, my model works with **83%** accuracy and this will be my first submit for Kaggle I hope you like it."}}