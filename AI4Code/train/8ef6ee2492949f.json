{"cell_type":{"ea082566":"code","0545b0e5":"code","b5cd2759":"code","c30e9fee":"code","44499a0a":"code","b1aa9546":"code","3c91392e":"code","8270e1aa":"code","c2550348":"markdown","fd9416c2":"markdown","8bbc6ae4":"markdown","ea7ddcc3":"markdown","4791befd":"markdown","0a44ceaa":"markdown","4fc3d4fc":"markdown","92b11395":"markdown"},"source":{"ea082566":"# To store data\nimport pandas as pd\n\n# To do linear algebra\nimport numpy as np\n\n# To create plots\nimport matplotlib.pyplot as plt\n\n# To create nicer plots\nimport seaborn as sns\n\n# To search in directories\nimport os\n\n# To measure time\nimport time\n\n# To flow tensors\nfrom tensorflow.python import keras\nfrom tensorflow.python.keras.models import Sequential, save_model, load_model\nfrom tensorflow.python.keras.layers import Dense, Flatten, Conv2D, Dropout, BatchNormalization\n\n# To split the data\nfrom sklearn.model_selection import train_test_split\n\n# To augment the data\nfrom keras.preprocessing.image import ImageDataGenerator\n\n# To interpret strings\nfrom ast import literal_eval","0545b0e5":"# Load the data\nX_train = pd.read_csv('..\/input\/mnist_train.csv')\nX_test = pd.read_csv('..\/input\/mnist_test.csv')\ny_train = X_train.pop('label')\n\n\n# Settings for the images\nimg_rows, img_cols = 28, 28\nnum_classes = 10\n\n\n# Functions to modify the data\ndef prepareData(X_data):\n    # Reshape and normalize the images\n    num_images = X_data.shape[0]\n    out_x = X_data.reshape(num_images, img_rows, img_cols, 1)\n    out_x = out_x \/ 255\n    return out_x\n\ndef prepareLabel(y_data):\n    # Transform labels to vectors\n    out_y = keras.utils.to_categorical(y_data, num_classes)\n    return out_y\n\n\n# Prepare the images and labels for the network\nX = prepareData(X_train.values)\ny = prepareLabel(y_train.values)\n\n\n# Check shapes\nprint('Image-Shape: {}'.format(X.shape))\nprint('Label-Shape: {}'.format(y.shape))","b5cd2759":"# Split the images into training and validation dataset\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=1)\n\n\n#Data augmentation\ndatagen = ImageDataGenerator(rotation_range = 20,\n                             zoom_range = 0.2,\n                             width_shift_range = 0.2,\n                             height_shift_range = 0.2,\n                             shear_range = 20)\ndatagen.fit(X_train)","c30e9fee":"# Path to store the results of all individuals\nevolution_path = 'evolution.csv'\n\n\n# Check if there is an existing evolution to continue\nif evolution_path in os.listdir():\n    # Load training status of an existing evolution\n    df = pd.read_csv(evolution_path)\n    \nelse:\n    # Create a new training status to start a new evolution\n    df = pd.DataFrame(columns=['Epoch','Individual', 'Path', 'Duration', 'Layers', 'Settings', 'T_Loss', 'T_Score', 'V_Loss', 'V_Score', 'Error'])\n    df.to_csv(evolution_path, index=False)\n\n\n# Get current count values to continue the evolution with the correct indices\nif df.empty:\n    current_count = 0\n    epoch_count = 0\n    \nelse:\n    current_count = df['Individual'].max()\n    epoch_count = df['Epoch'].max()","44499a0a":"# Functions to create layers with random parameters\n# You can alter the settings for the possible parameters here\n\ndef createConv2D(new=True, settings=None, first=False):\n    # (Re-)create a Conv2D-Layer\n    if new:\n        kernel = np.random.choice([1,3,5])\n        filters = np.random.randint(1, 50)\n    else:\n        filters, kernel = settings\n    if first:\n        return Conv2D(filters, kernel_size=(kernel, kernel), activation='relu', input_shape=(img_rows, img_cols, 1))\n    else:\n        return Conv2D(filters, kernel_size=(kernel, kernel), activation='relu')\n\n    \ndef createBatchNormalization():\n    # (Re-)create a BatchNormalization-Layer\n    return BatchNormalization(axis=1)\n\n\ndef createDropout(new=True, rate=None):\n    # (Re-)create a Dropout-Layer\n    if new:\n        rate = np.random.random()\n    return Dropout(rate=rate)\n\n\ndef createFlatten():\n    # (Re-)create a Flatten-Layer\n    return Flatten()\n\n\ndef createDense(new=True, dense=None, last=False):\n    # (Re-)create a Dense-Layer\n    if new:\n        dense = np.random.randint(1, 128)\n    if last:\n        return Dense(dense, activation='softmax')\n    else:\n        return Dense(dense, activation='relu')\n\n\n# List of layers to choose from for the evolution\n# You can add new functions to create different layers here\nlayers = [createConv2D, createDense, createBatchNormalization, createDropout]","b1aa9546":"def createNewIndividual():\n    # If a new evolution starts this function stocks up the first evolution-epoch with individual networks\n    # Create model\n    model = Sequential()\n    \n    # Start with a Convolution-Layer\n    kernel_size = np.random.choice([1,3,5])\n    model.add(Conv2D(np.random.choice([1,3,5]), kernel_size=(kernel_size, kernel_size), activation='relu', input_shape=(img_rows, img_cols, 1)))\n    \n    # Add flatten layer\n    model.add(Flatten())\n    \n    # Add last Dense-Layer\n    model.add(Dense(num_classes, activation='softmax'))\n    \n    # Compile the network\n    model.compile(loss=keras.losses.categorical_crossentropy, optimizer='adam', metrics=['accuracy'])\n    return model\n\n\ndef recreateEvolveIndividual(individual):\n    # If an evolution status exists, this function recreates the choosen network and alters some settings randomly\n    # Separate network-layers and layer-settings to recreate the network\n    layers_string, settings = individual.values[0]\n    layers_string = layers_string.split('_')\n    settings = literal_eval(settings)\n    \n    # Random number to choose an action: delete\/switch\/insert layer\n    random = np.random.random()\n    # Random number to choose index for the action\n    layer_index = np.random.randint(len(layers_string)-1)+1\n    \n    \n    # Create model\n    model = Sequential()\n    \n    # Iterate over all layers of the network and add them\n    # Check if an alternation has to be implemented\n    for i, (layer, setting) in enumerate(zip(layers_string, settings)):\n        \n        # Evolve randomly choosen layer\n        if i==layer_index:\n            \n            # Delete layer (by not adding it)\n            if random<0.25:\n                # Pass the adding and continue with next layer\n                pass\n            \n            # Switch layer\n            elif random<0.75:\n                # Instead of adding the layer, add a different layer\n                model.add(np.random.choice(layers)())\n                \n            # Insert layer\n            else:\n                # Add a randomly choosen layer and add the normal layer afterwards\n                model.add(np.random.choice(layers)())\n                # Check for first\/last layer since they have to be implemented differently (input-shape\/activation-function)\n                if i==0:\n                    model.add(createConv2D(new=False, settings=setting, first=True))\n                elif i+1==len(settings):\n                    model.add(createDense(new=False, dense=setting, last=True))\n                elif layer=='conv2d':\n                    model.add(createConv2D(new=False, settings=setting))\n                elif layer=='flatten':\n                    model.add(createFlatten())\n                elif layer=='dense':\n                    model.add(createDense(new=False, dense=setting))\n                elif layer=='batch':\n                    model.add(createBatchNormalization())\n                elif layer=='dropout':\n                    model.add(createDropout(new=False, rate=setting))\n                    \n        # Add layer without evolution\n        else:\n            # Check for first\/last layer since they have to be implemented differently (input-shape\/activation-function)\n            if i==0:\n                model.add(createConv2D(new=False, settings=setting, first=True))\n            elif i+1==len(settings):\n                model.add(createDense(new=False, dense=setting, last=True))\n            elif layer=='conv2d':\n                model.add(createConv2D(new=False, settings=setting))\n            elif layer=='flatten':\n                model.add(createFlatten())\n            elif layer=='dense':\n                model.add(createDense(new=False, dense=setting))\n            elif layer=='batch':\n                model.add(createBatchNormalization())\n            elif layer=='dropout':\n                model.add(createDropout(new=False, rate=setting))\n        \n    # Compile the network\n    model.compile(loss=keras.losses.categorical_crossentropy, optimizer='adam', metrics=['accuracy'])\n    return model\n\n\ndef storeModel(df, model, epoch, j, t_loss, t_score, v_loss, v_score, end_time, error):\n    # After training the network the results have to be saved to disc\n    # Initialze some variables to store the results\n    network = []\n    network_settings = []\n    \n    # Iterate over all layers\n    for layer in model.layers:\n        # Add the name of the layer to recreate it\n        name = layer.name.split('_')[0]\n        network.append(name)\n\n        # Add the settings of the layer to recreate the hyperparameters\n        if name=='dense':\n            settings = layer.units\n        elif name=='conv2d':\n            settings = [layer.filters, layer.kernel_size[0]]\n        elif name=='dropout':\n            settings = [layer.rate]\n        else:\n            settings = None\n        network_settings.append(settings)\n        \n    # Combine the names of al layers\n    network = '_'.join(network)\n    \n    # To save the model to disc\n    # Check if training has been successfull\n    # Save the network to disc to have the ability to reload it\n    network_file = 'Saves\/{}_{}.model'.format(epoch_count+epoch, current_count+j)\n    #if not error:\n        #save_model(model, network_file)\n\n    # Append the training status and save it to disc\n    df = df.append(pd.DataFrame([[epoch_count+epoch, current_count+j, network_file, end_time, network, network_settings, t_loss, t_score, v_loss, v_score, error]], columns=df.columns))\n    df.to_csv(evolution_path, index=False)","3c91392e":"# Number of individuals in the population\nn = 10\n\n# Number of epochs for the evolution\n# Since tensorflow allocates some memory for each model trained it slows down dramatically after many networks\n# Therefore I recommend to run the notebook several times locally\nepochs = 5\n\n# Iterate over all epochs\nfor epoch in range(1, epochs+1):\n    # Iteration of individuals\n    j = 0\n    \n    # Iterate over all individuals:\n    for i in range(n):\n        \n        # Iterate until a valid individual has been (re-)created and trained\n        iterate = True\n        while iterate:\n\n            # Load training history\n            df = pd.read_csv(evolution_path)\n\n            # Check if n individuals have been initialised for first generation\n            if df[df['Error']==False].shape[0] < n:\n                model = createNewIndividual()\n\n            # First generation has been computed\n            else:\n                # Number of the current epoch\n                current_epoch = df['Epoch'].max()\n\n                # Choose next individual from weighted individuals\n                evolvable_individual = df.sample(1, weights=df['V_Score']*df['Epoch'])[['Layers', 'Settings']]\n\n                try:\n                    # Evolve individual\n                    model = recreateEvolveIndividual(evolvable_individual)\n                except:\n                    # Evolved model is not legit\n                    continue\n\n            \n            # Start training process of the network\n            try:\n                batch_size = 100\n                start_time = time.time()\n                model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),\n                                    epochs = 3, \n                                    validation_data = (X_val, y_val),\n                                    verbose = 0, \n                                    steps_per_epoch = X_train.shape[0] \/\/ batch_size)\n                end_time = time.time() - start_time\n\n                # Retrieve training results\n                t_loss = model.history.history['loss'][-1]\n                t_score = model.history.history['acc'][-1]\n                v_loss = model.history.history['val_loss'][-1]\n                v_score = model.history.history['val_acc'][-1]\n\n                print()\n                iterate = False\n                error = False\n            \n            # Set correct variables for untrainable network\n            except:\n                error = True\n                t_loss = 0\n                t_score = 0\n                v_loss = 0\n                v_score = 0\n                end_time = 0\n                pass\n            j += 1\n            \n            # Store model and training results to disc\n            try:\n                storeModel(df, model, epoch, j, t_loss, t_score, v_loss, v_score, end_time, error)\n            except:\n                pass\n            \n            \n\n# Since tensorflow slows the process down after several models you can use this function to restart and rerun your notebook locally\n# An automatic stop for the rerunning has not been implemented\n# Restarts the kernel and runs all cells\n#from IPython.display import display_html\n\n#def rerunKernel():\n#    display_html(\"\"\"<script>Jupyter.notebook.kernel.restart(); setTimeout( function(){ IPython.notebook.execute_all_cells(); }, 1000);<\/script>\"\"\",raw=True)\n#rerunKernel()","8270e1aa":"# Reload the training results\ndf = pd.read_csv(evolution_path)\n\n# Filter for the trained networks\ndf2 = df[df['Error']==False]\n\nprint('{} Networks have been tried'.format(df.shape[0]))\nprint('{} Networks have been trained'.format(df2.shape[0]))\n\n\n\n# Plot the training duration for all networks\n# You should see the increasing time for the training\n# This occurs since tensorflo allocates memory for old models\ndf2['Duration'].plot(figsize=(10, 6), title='Training Duration For All Networks')\nplt.xlabel('Network Individual [#]')\nplt.ylabel('Training Duration [s]')\nplt.show()\n\n\n\n# Plot top n network architectures\nn = 10\n\n# Create order within the networks architectures\norder = df2.groupby('Layers').max().sort_values('V_Score', ascending=False).index\n\nplt.figure(figsize=(10,10))\nsns.swarmplot(data=df2, x='V_Score', y='Layers', order=order[:n])\nplt.title('Top {} Network Architectures - Best Score: {:.4f}'.format(n, df2['V_Score'].max()))\nplt.grid()\nplt.xlim(0.0, 1)\nplt.show()\n\n\n\n# Plot validation accuracy for the evolution proccess\nn = 3\n\nplt.figure(figsize=(12,6))\ndf3 = df2.groupby('Epoch').apply(lambda x: x.sort_values('V_Score', ascending=False).head(n))\nsns.lineplot(data=df3, x='Epoch', y='V_Score')\nplt.title('Validation Accuracy Of The Top {} Networks Of Each Epoch'.format(n))\nplt.show()","c2550348":"## 7 Train The Networks With Evolution\n\nThe evolution process creates n networks for each epoch. Between the epochs the networks are being weighted by their validation accuracy to choose the best ones for reproduction. To compensate for the few individuals trained the networks will be weighted by their epoch as well. That is why new individuals will be prefered but old ones will not be forgotten.\n\nSince Tensorflow allocates some memory for each model built the training process slows down after a while. This can be avoided by restarting the Tensorflow-process. Locally you can implement this by restarting the notebook or by using subprocesses for each network.","fd9416c2":"# Evolutionary Deep Networks\n\nOne of the hardest parts in deep learning is to optimize the network's architecture and the layer's hyperparameters.\nBy using evolutionary algorithms this optimization can be automated.\n\nIn this notebook you can find an implementation which will optimize the network architecture and the hyperparameters for a given problem. \nIt has to be noted, that the process is computation-intensive. But since computation resources keep getting cheaper, this approach should keep getting more valuable by the day.\n\n## 1 Import Libraries\n\nTensorflow and keras are the main libraries needed in this notebook.","8bbc6ae4":"## 3 Split And Augment The Images\n\nThe dataset will be split into a fixed training- and  validation-dataset. A final testing-dataset has not been implemented. Furthermore to get more training data the images will be augmented to make the network more robust.","ea7ddcc3":"## 6 Functions To (Re-)Create Networks\n\nThe evolution needs to have the ability to create new networks on its own, to recreate old networks and to alter existing ones.","4791befd":"## 2 Load The Data\n\nThe images of the MNIST-dataset have to be reshaped and normalized.","0a44ceaa":"## 4 Load Training Status\n\nTo track the training process each individual network of the evolution will be saved with it's necessary settings and results. The training status of each network can be used to recreate the network afterwards.","4fc3d4fc":"## 5 Functions To Define Layers\n\nTo add layers to the model in an automated fashion the process needs to be able to choose layers and recreate networks.","92b11395":"## 8 Inspect Training Process\n\nAfter the training and by inspecting the training process you can see the increasing training duration (restarting the notebook locally is recommended).\nFurthermore the evolution produces reliably high models. By using more computation resources this should even get better.\n\nTherefore the evolution of network architectures can be an approach to find good archiectures for your problems. \nSince this can be automated the success of the process is just limited by its resources.\n\nHave a good day!"}}