{"cell_type":{"ecd3839f":"code","163d897d":"code","53cc1783":"code","942643f2":"code","d862ca02":"code","643fa69b":"code","86573458":"code","219d81b8":"code","0c4d150a":"code","0a6d34c4":"code","db3acdd4":"code","eb66fb80":"code","7070a565":"code","103ec913":"code","7b6bdfe3":"code","913f31ad":"code","e0f76cb6":"code","a5b03dca":"code","9813951d":"code","608320bd":"code","e356c2b9":"code","148c34ac":"code","2780242e":"code","119c773d":"code","83def128":"code","6bbccbc3":"code","c2785957":"code","63a7fe83":"code","e5b81372":"code","cef72284":"code","b3a4f19b":"code","54f91751":"code","f2b0e8bb":"code","be4e7c35":"code","cb26b7b3":"code","72dfcea5":"code","50764a57":"code","b62e8541":"code","46123d6e":"code","477d1f0b":"code","55626775":"code","4610b95e":"code","7d76cdeb":"code","d8cba8c8":"code","0388b817":"code","ebd9449b":"code","cbdc5717":"code","19a14ea6":"code","ca1a8a5b":"code","aab6aaf6":"code","c0006802":"code","b09ca271":"code","bc11d086":"code","b4ce2f95":"code","20a913c5":"code","4f949c80":"code","ab671372":"code","a45e4d9f":"code","082640a0":"code","e839bfd2":"code","bf0711c0":"code","2c476d5c":"code","d968a710":"code","c8784395":"code","1dd309a8":"code","d9d3f56f":"code","ba11bf16":"markdown","9d1a2672":"markdown","0647d22d":"markdown","0804747b":"markdown","20d354ac":"markdown","6f0b76d2":"markdown","7c8569c8":"markdown","7cab06d9":"markdown","b72931d8":"markdown","8c6a3192":"markdown","3f2d05cf":"markdown","13a66124":"markdown","cbe301c5":"markdown","1b369cf0":"markdown","19736992":"markdown","52dcdfc3":"markdown","dce1b441":"markdown","faba29d8":"markdown"},"source":{"ecd3839f":"import numpy as np # linear algebra\nimport pandas as pd # data processing\nimport seaborn as sns\nimport matplotlib.pylab as plt\nimport tensorflow as tf\nfrom sklearn import preprocessing","163d897d":"items = pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/items.csv\")\nitems.tail()","53cc1783":"categories = pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/item_categories.csv\")\ncategories.tail()","942643f2":"shops = pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/shops.csv\")\nshops","d862ca02":"sales = pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/sales_train.csv\")\nsales.tail()","643fa69b":"sales_test = pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/test.csv\")\nsales_test.tail()","86573458":"print(sum(items.duplicated(['item_name'])))\nprint(sum(categories.duplicated(['item_category_name'])))\nprint(sum(shops.duplicated(['shop_name'])))\n# We can see that the names of shops 10 and 11 differ only by one letter. It is probably the same shop.  \n# Also 0 and 57, 1 and 58, ?39 and 40?","219d81b8":"# Let's find out if shops 10,11,0,57,1,58 present in the dataframe for forecasting.\nuniq_shops = sales_test['shop_id'].unique()\nfor shop in list([10,11,0,57,1,58]):\n    print(shop, shop in uniq_shops)","0c4d150a":"new_shop_id = {11: 10, 0: 57, 1: 58}\nshops['shop_id'] = shops['shop_id'].apply(lambda x: new_shop_id[x] if x in new_shop_id.keys() else x)\nsales['shop_id'] = sales['shop_id'].apply(lambda x: new_shop_id[x] if x in new_shop_id.keys() else x)","0a6d34c4":"sales = pd.merge(sales_test, sales, on = ('shop_id', 'item_id'), how = 'left')","db3acdd4":"print(sum(sales.duplicated()))","eb66fb80":"# drop the duplicate rows from sales\nsales = sales.drop_duplicates()\nsales.shape","7070a565":"print(sum(sales.duplicated(['ID','date','date_block_num','item_price'])))\nprint(sum(sales.duplicated(['ID','date','date_block_num','item_cnt_day'])))","103ec913":"# We should think carefully which row should be dropped. Price will help us. But now we will just keep first duplicate and drop later.\nsales = sales.drop_duplicates(['date','date_block_num','shop_id','item_id','item_cnt_day'])\nsales.shape","7b6bdfe3":"sales = sales.drop_duplicates(['ID','date','date_block_num'], keep = 'last')\nsales.shape","913f31ad":"print(items.isnull().sum().sum())\nprint(categories.isnull().sum().sum())\nprint(shops.isnull().sum().sum())\nprint(sales.isnull().sum().sum())\n# There are missing values in the data. Most of them corresponds to IDs from the forcast set that doesn't represent in training set.","e0f76cb6":"sales.describe()\n# It is possible that item_price and item_cnt_day has outliers (max >> 0.75-quantile), and item_cnt_day has wrong values (min < 0)","a5b03dca":"# change a sign of negative values\nsales.loc[sales.item_cnt_day < 0, 'item_cnt_day'] = -1. * sales.loc[sales.item_cnt_day < 0, 'item_cnt_day']","9813951d":"#sales_month = sales.sort_values('date_block_num').groupby(['ID', 'date_block_num'], as_index = False).agg({'item_cnt_day': ['sum'], 'item_price': ['mean']})\n#sales_month.columns = ['ID', 'date_block_num', 'item_cnt_month', 'item_price']\nsales_month = sales.sort_values('date_block_num').groupby(['ID', 'date_block_num'], as_index = False).agg({'item_cnt_day': ['sum']})\nsales_month.columns = ['ID', 'date_block_num', 'item_cnt_month']\nsales_month.sample(10)\n# after we grouped and aggregate data we delete all rows corresponding to IDs that don't present in train data set (and preset just in forcasting set)","608320bd":"sales_month.describe()","e356c2b9":"def to_IDs(np_data, col_ID):\n    # np_data - sales converted to numpy array\n    # col_ID - name of ID column\n    sales_by_ID = list()\n    IDs = np.unique(np_data[:,col_ID]).astype(int)\n    for i in IDs:\n        positions = np_data[:,col_ID] == i\n        sales_ID = np_data[positions,1:]\n        sales_by_ID.append(sales_ID)\n    return sales_by_ID, IDs","148c34ac":"sales_by_ID, list_IDs = to_IDs(sales_month.values,0)\nprint(len(sales_by_ID))","2780242e":"# to decrease calculation time during a code debugging we remove IDs that don't have observtions for last months\ndef remove_ID_nan_last_year(np_data):\n    N_IDs = len(np_data)\n    col_date = 0\n    clear_data = list()\n    cut_month = 33 - 2\n    for i in range(N_IDs):\n        ID_data = np_data[i]\n        if len(ID_data[ID_data[:,col_date] >= cut_month,1]) != 0:\n            clear_data.append(ID_data)\n    return clear_data","119c773d":"#sales_by_ID = remove_ID_nan_last_year(sales_by_ID)\n#len(sales_by_ID)","83def128":"#val_month = 28\n#train, test_actual = split_train_test(sales_by_ID, last_month = val_month)","6bbccbc3":"#test_actual = np.nan_to_num(test_actual, nan = 0)","c2785957":"# Let's fill the missing date_block_num by NaN for paticular ID\ndef missing_months(np_data, col_date, col_TS, N_months = 34):\n    # col_date - index of date_block_num column\n    # col_TS - index of item_price column and item_cnt_month column\n    # at first fill time series by NaN for all months\n    series = [np.nan for _ in range(N_months)]\n    for i in range(len(np_data)):\n        position = int(np_data[i, col_date] - 1)\n        # fill positions that present in data\n        series[position] = np_data[i, col_TS]\n    return series","63a7fe83":"# Let's fill the missing item_cnt_month and item_price for particular ID\ndef to_fill_missing(np_data, N_months = 34):\n    col = ['date_block_num','item_cnt_month']\n    sales_ID = pd.DataFrame(np_data, columns = col)\n    if sales_ID.shape[0] < N_months:\n        date_month = pd.DataFrame(range(N_months),columns = ['date_block_num'])\n        sales_ID = pd.merge(date_month, sales_ID, on = ('date_block_num'), how = 'left')\n        sales_ID = sales_ID.reindex(columns = col)\n        sales_ID['item_cnt_month'] = sales_ID['item_cnt_month'].fillna(0.0)\n    return sales_ID['item_cnt_month'].to_numpy()","e5b81372":"# Plot time series for particular ID to find out missing months\ndef plot_TS(np_data, n_vars = 1, N_months = 34, flag = 0):\n    # n_vars = 1 or 2 (plot item_cnt OR item_cnt and item_price)\n    plt.figure()\n    if flag == 1:\n        TSs = to_fill_missing(np_data, N_months)\n    for i in range(n_vars):\n        col_plot = i + 1 # index of column to plot\n        if flag == 1:\n            series = TSs#[:,col_plot]\n        else:\n            series = missing_months(np_data, 0, col_plot, N_months)\n        ax = plt.subplot(n_vars, 1, i+1)\n        plt.plot(series, 'o')\n        plt.plot(series)\n    plt.show()","cef72284":"for i in np.random.randint(0, len(sales_by_ID), 5):\n    plot_TS(sales_by_ID[i], flag = 1)","b3a4f19b":"# Let's create 2D-array and each column is counts of particular ID where missing months is filled\ndef full_data(data, N_months = 34):\n    N_IDs = len(data)\n    TS = np.empty((N_months, N_IDs))\n    for i in range(N_IDs):\n        TS[:, i] = to_fill_missing(data[i], N_months)\n    return TS","54f91751":"TS = full_data(sales_by_ID)\nTS.shape","f2b0e8bb":"val_month = 29\nvalid_TS = TS[val_month:,:]\nprint(valid_TS.shape)\n\ntrain_TS = TS[:val_month,:]","be4e7c35":"scaler = preprocessing.MinMaxScaler()\nscaler.fit(TS)\ntrain_scaled = scaler.transform(train_TS)\nvalid_scaled = scaler.transform(valid_TS)","cb26b7b3":"def to_make_features(TS, n_lag, batch_size):\n    ds = tf.data.Dataset.from_tensor_slices(TS) # each element of dataset is one value of TS \n    ds = ds.window(n_lag+1, shift = 1, drop_remainder = True) # (n_lag+1)-elements of dataset is combined to window\n    ds = ds.flat_map(lambda row: row.batch(n_lag + 1)) # to batch elements in window to tensor (one element) and to flat (now there are no windows)\n  # Let's shuffle befor we combine batches for epoch\n    ds = ds.shuffle(300)\n  # make the tuple: first element is features, second element is labels\n  # features-(1,2,3) and labels-(2,3,4). 2 goes after 1, 3 goes after 2, 4 goes after 3.\n    ds = ds.map(lambda row: (row[:-1,:], row[1:,:]))\n  # combine tuples to banch for gradient descent\n  # instead of a row we will have a matrix in every tuple\n    ds = ds.batch(batch_size).prefetch(1)\n    return ds","72dfcea5":"import random\ntf.random.set_seed(53)\nrandom.seed(53)","50764a57":"n_lag = 6\nbatch_size = 8\nfeatures = to_make_features(train_scaled, n_lag, batch_size)\nval_features = to_make_features(valid_scaled, n_lag, batch_size)\nConv_filters = 64\nConv_kernel_size = 4\nLSTM_filters = 64\nn_outputs = train_scaled.shape[1]","b62e8541":"model = tf.keras.models.Sequential([\n  tf.keras.layers.Conv1D(filters = Conv_filters, kernel_size = Conv_kernel_size,\n                      strides=1, padding=\"causal\", activation=\"relu\", input_shape=[None, n_outputs]),\n  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(LSTM_filters, return_sequences=True)),\n  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(LSTM_filters, return_sequences=True)),\n  tf.keras.layers.Dense(n_outputs)\n])\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-5 * 10**(epoch \/ 20))\n\noptimizer = tf.keras.optimizers.Adam(learning_rate = 1e-5)\n\nmodel.compile(loss=tf.keras.losses.Huber(), optimizer=optimizer, metrics=[\"mae\"])\n\nmodel.summary()","46123d6e":"fitting = model.fit(features, epochs=80, callbacks=[lr_schedule])","477d1f0b":"plt.semilogx(fitting.history[\"lr\"], fitting.history[\"loss\"])","55626775":"optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-2)\nmodel.compile(loss=tf.keras.losses.Huber(), optimizer=optimizer, metrics=[\"mae\"])\nfitting = model.fit(features, epochs=300, verbose = 1, validation_data = val_features)","4610b95e":"mae = fitting.history['mae']\nloss = fitting.history['loss']\nepochs=range(len(loss))\nplt.plot(epochs, mae, 'r')\nplt.plot(epochs, fitting.history['val_mae'], 'r--')\nplt.plot(epochs, loss, 'b')\nplt.plot(epochs, fitting.history['val_loss'], 'b--')\nplt.title('MAE and Loss')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend([\"MAE\", \"val_MAE\", \"Loss\", \"val_Loss\"])","7d76cdeb":"plt.plot(epochs, fitting.history['val_mae'], 'r--')","d8cba8c8":"def model_forecast(model, TS, n_lag, batch_size):\n    ds = tf.data.Dataset.from_tensor_slices(TS)\n    ds = ds.window(n_lag, shift=1, drop_remainder=True)\n    ds = ds.flat_map(lambda row: row.batch(n_lag))\n    ds = ds.batch(batch_size)\n    forecast = model.predict(ds)\n    return forecast","0388b817":"forecast = model_forecast(model, train_scaled, n_lag, batch_size)\nforecast = forecast[:,-1,:]\nforecast = scaler.inverse_transform(forecast)","ebd9449b":"iplot = 0\nfor i in np.random.randint(0, n_outputs, 4):\n    iplot += 1\n    plt.subplot(4,1,iplot)\n    plt.plot(range(n_lag, val_month+1), np.append(train_TS[n_lag:,i],valid_TS[0,i]), 'r')\n    plt.plot(range(n_lag, val_month+1), forecast[:,i], 'b')\n    plt.legend([\"actual\", \"predicted\"])","cbdc5717":"lag_set = range(2,34-val_month,2)\nbatch_size_set = np.array([4,8,16])","19a14ea6":"mae_val = np.zeros((len(lag_set), len(batch_size_set)))\nmae_train = np.zeros((len(lag_set), len(batch_size_set)))\ni, j = 0, 0\nfor batch_size in batch_size_set:\n    for lag in lag_set:\n        features = to_make_features(train_scaled, lag, batch_size)\n        val_features = to_make_features(valid_scaled, lag, batch_size)\n        model = tf.keras.models.Sequential([\n              tf.keras.layers.Conv1D(filters = Conv_filters, kernel_size = Conv_kernel_size,\n                                  strides=1, padding=\"causal\", activation=\"relu\", input_shape=[None, n_outputs]),\n              tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(LSTM_filters, return_sequences=True)),\n              tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(LSTM_filters, return_sequences=True)),\n              tf.keras.layers.Dense(n_outputs)\n        ])\n        optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-2)\n        model.compile(loss=tf.keras.losses.Huber(), optimizer=optimizer, metrics=[\"mae\"])\n        \n        fitting = model.fit(features, epochs = 100, verbose = 1, validation_data = val_features)\n        mae_val_i = np.array(fitting.history['val_mae'])\n        mae_val[i,j] = np.min(mae_val_i[np.nonzero(mae_val_i)])\n        min_position = fitting.history['val_mae'].index(mae_val[i,j])\n        mae_train[i,j] = fitting.history['mae'][min_position]\n        i += 1\n    j += 1\n    i = 0","ca1a8a5b":"plt.subplot(2,1,1)\nfor j in range(len(batch_size_set)):\n    plt.plot(lag_set, mae_train[:,j])\nplt.legend([\"batch 1\", \"batch 2\", \"batch 3\"])\nplt.title(\"MAE\")\nplt.subplot(2,1,2)\nfor j in range(len(batch_size_set)):\n    plt.plot(lag_set, mae_val[:,j])\nplt.legend([\"batch 1\", \"batch 2\", \"batch 3\"])\nplt.title(\"val_MAE\")","aab6aaf6":"# 4 lags and 16 batch size are the best\nn_lag = 4\nbatch_size = 16\nfeatures = to_make_features(train_scaled, n_lag, batch_size)\nval_features = to_make_features(valid_scaled, n_lag, batch_size)","c0006802":"epoch_set = np.array([50, 100, 300, 500, 1000])\nmae_val_ep = np.zeros(len(epoch_set))\nmae_train_ep = np.zeros(len(epoch_set))\ni = 0\nfor epoch in epoch_set:\n    fitting = model.fit(features, epochs = epoch)\n    forecast_val = model_forecast(model, train_scaled[33-n_lag:,:], n_lag, batch_size)\n    forecast_val = forecast_val[:,-1,:]\n    forecast_val = scaler.inverse_transform(forecast_val)\n    mae_val_ep[i] = np.mean(np.abs(forecast_val - test_actual))\n    mae_train_ep[i] = fitting.history['mae'][-1]\n    i += 1\nplt.subplot(2,1,1)\nplt.plot(epoch_set, mae_train_ep)\nplt.subplot(2,1,2)\nplt.plot(epoch_set, mae_val_ep)","b09ca271":"# 500 epochs is the best\nn_epoch = 100","bc11d086":"Conv_filters_set = [32, 64, 256]\nConv_kernel_size_set = [2, 4, 6]\nLSTM_filters_set = [32, 64, 256, 512]","b4ce2f95":"mae_val = np.zeros((len(Conv_filters_set), len(Conv_kernel_size_set)))\nmae_train = np.zeros((len(Conv_filters_set), len(Conv_kernel_size_set)))\ni, j = 0, 0\nfor Conv_filters in Conv_filters_set:\n    for Conv_kernel_size in Conv_kernel_size_set:\n        model = tf.keras.models.Sequential([\n          tf.keras.layers.Conv1D(filters = Conv_filters, kernel_size = Conv_kernel_size,\n                      strides=1, padding=\"causal\", activation=\"relu\", input_shape=[None, n_outputs]),\n          tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(LSTM_filters, return_sequences=True)),\n          tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(LSTM_filters, return_sequences=True)),\n          tf.keras.layers.Dense(n_outputs)\n        ])\n        optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-2)\n        model.compile(loss=tf.keras.losses.Huber(), optimizer=optimizer, metrics=[\"mae\"])\n        fitting = model.fit(features, epochs = n_epoch, verbose = 1, validation_data = val_features)\n\n        mae_val_i = np.array(fitting.history['val_mae'])\n        mae_val[i,j] = np.min(mae_val_i[np.nonzero(mae_val_i)])\n        min_position = fitting.history['val_mae'].index(mae_val[i,j])\n        mae_train[i,j] = fitting.history['mae'][min_position]\n        i += 1\n    j += 1\n    i = 0\nplt.subplot(2,1,1)\nfor j in range(len(Conv_filters_set)):\n    plt.plot(Conv_kernel_size_set, mae_train[:,j])\nplt.legend([\"Filter 1\", \"Filter 2\", \"Filter 3\"])\nplt.subplot(2,1,2)\nfor j in range(len(Conv_filters_set)):\n    plt.plot(Conv_kernel_size_set, mae_val[:,j])\nplt.legend([\"Filter 1\", \"Filter 2\", \"Filter 3\"])","20a913c5":"mae_train","4f949c80":"mae_val","ab671372":"Conv_filters = 32\nConv_kernel_size = 4\n\nmae_val_ep = np.zeros(len(LSTM_filters_set))\nmae_train_ep = np.zeros(len(LSTM_filters_set))\ni = 0\nfor LSTM_filters in LSTM_filters_set:\n    model = tf.keras.models.Sequential([\n          tf.keras.layers.Conv1D(filters = Conv_filters, kernel_size = Conv_kernel_size,\n                      strides=1, padding=\"causal\", activation=\"relu\", input_shape=[None, n_outputs]),\n          tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(LSTM_filters, return_sequences=True)),\n          tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(LSTM_filters, return_sequences=True)),\n          tf.keras.layers.Dense(n_outputs)\n        ])\n    optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-2)\n    model.compile(loss=tf.keras.losses.Huber(), optimizer=optimizer, metrics=[\"mae\"])\n    fitting = model.fit(features, epochs = n_epoch, verbose = 1, validation_data = val_features)\n    \n    mae_val_i = np.array(fitting.history['val_mae'])\n    mae_val_ep[i] = np.min(mae_val_i[np.nonzero(mae_val_i)])\n    min_position = fitting.history['val_mae'].index(mae_val_ep[i])\n    mae_train_ep[i] = fitting.history['mae'][min_position]\n    i += 1\nplt.subplot(2,1,1)\nplt.plot(LSTM_filters_set, mae_train_ep)\nplt.subplot(2,1,2)\nplt.plot(LSTM_filters_set, mae_val_ep)","a45e4d9f":"LSTM_filters = 32\n\nmodel = tf.keras.models.Sequential([\n          tf.keras.layers.Conv1D(filters = Conv_filters, kernel_size = Conv_kernel_size,\n                      strides=1, padding=\"causal\", activation=\"relu\", input_shape=[None, n_outputs]),\n          tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(LSTM_filters, return_sequences=True)),\n          tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(LSTM_filters, return_sequences=True)),\n          tf.keras.layers.Dense(n_outputs)\n        ])\noptimizer = tf.keras.optimizers.Adam(learning_rate = 1e-2)\nmodel.compile(loss=tf.keras.losses.Huber(), optimizer=optimizer, metrics=[\"mae\"])\nfitting = model.fit(features, epochs = 50, verbose = 1, validation_data = val_features)","082640a0":"mae = fitting.history['mae']\nloss = fitting.history['loss']\nepochs=range(len(loss))\nplt.plot(epochs, mae, 'r')\nplt.plot(epochs, fitting.history['val_mae'], 'r--')\nplt.plot(epochs, loss, 'b')\nplt.plot(epochs, fitting.history['val_loss'], 'b--')\nplt.title('MAE and Loss')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend([\"MAE\", \"val_MAE\", \"Loss\", \"val_Loss\"])","e839bfd2":"last_month_forecast = model_forecast(model, valid_scaled, n_lag, batch_size)\nlast_month_forecast = forecast[-1,-1,:]","bf0711c0":"last_month_forecast = scaler.inverse_transform(np.expand_dims(last_month_forecast, axis = 0))","2c476d5c":"submission = pd.DataFrame({\n        'ID': list_IDs,\n        'item_cnt_month': np.squeeze(last_month_forecast)\n    })\nsubmission.head()","d968a710":"submission.loc[submission.item_cnt_month < 0, 'item_cnt_month'] = 0","c8784395":"submission = pd.merge(sales_test.ID, submission, on = ('ID'), how = 'left')","1dd309a8":"submission = submission.fillna(0)\nsubmission.tail()","d9d3f56f":"submission.to_csv('submission.csv', index=False)","ba11bf16":"In order to execute supervised algorithms we have to modify the data this way:\nseveral lags like an input and next lags like an output.\n\nLet's assume n_lag = 3 than modified time series of item_cnt_month will be like the following:\n\n---------------------input--------------- ||         output \n\ndata_month1 data_month2 data_month3 ||      data_month2 data_month3  data_month4 \n\ndata_month2 data_month3 data_month4 ||      data_month3 data_month4  data_month5 \n\ndata_month3 data_month4 data_month5 ||      data_month4 data_month5  data_month6 \n\n.....","9d1a2672":"# Delete the duplicates","0647d22d":"# Outliers and negative values","0804747b":"# Model evaluation","20d354ac":"# Hyperparameters tuning","6f0b76d2":"# Tuning of lags' number and batch size","7c8569c8":"Let's take a look on graphs of data with missing values and data with filled missing values","7cab06d9":"We have totaly different numbers and positions of missing months for different IDs. Let's fill item_cnt_month by 0 and add column of missing flag.","b72931d8":"Let's take a look on the plots of several IDs","8c6a3192":"Let's split the data by ID. We will store ID and corresponding data in a list.","3f2d05cf":"def split_train_test(np_data, col_date = 0, last_month = 33):\n    col_TS = 1 # order of item_cnt_month column\n    N_IDs = len(np_data)\n    train = list()\n    test = list()\n    for i in range(N_IDs):\n        ID_data = np.array(np_data[i])\n        train_rows = ID_data[ID_data[:,col_date] < last_month, :]\n        test_response = ID_data[ID_data[:,col_date] >= last_month, :]\n        #if len(train_rows) == 0:\n            #continue\n        if len(test_response) == 0:\n            test.append(np.array([np.array(range(last_month,34,1)), np.zeros(34-last_month)]).T)\n        else:\n            test.append(test_response)\n        train.append(train_rows)\n    return train, np.array(test)","13a66124":"# Convert to month data\nLet's group the sales by ID and calculate month number of sold items and average price.","cbe301c5":"Let's drop the pairs (shop_id, item_id) that are not represented in the dataframe for forecasting. And merge two dataframes.","1b369cf0":"# Fill data for missing months","19736992":"# Scaling","52dcdfc3":"# Tuning of number of epochs","dce1b441":"We have 34 months of observations. Let's split the data into train (33 months) and test (last month) samples.","faba29d8":"# Feature engineering"}}