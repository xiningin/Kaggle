{"cell_type":{"a953eddc":"code","32a9b149":"code","55f76bc2":"code","f5338c52":"code","a942a6a2":"code","28c72e0d":"code","d42aa52f":"code","c6d9507a":"code","d13fb2e9":"code","c3d1e978":"code","e6b47aac":"code","9641994a":"code","5a5c5d5c":"markdown","f4323b20":"markdown","cd6115c2":"markdown","14b24126":"markdown","e8a6424b":"markdown","4bed2684":"markdown","4a4b6599":"markdown","e28e25bc":"markdown","3d979263":"markdown","2c50423a":"markdown","c23b95b4":"markdown","c4a556d0":"markdown"},"source":{"a953eddc":"import random, re, math\nimport numpy as np, pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport tensorflow as tf\nfrom kaggle_datasets import KaggleDatasets\nprint('Tensorflow version ' + tf.__version__)\n","32a9b149":"GCS_DS_PATH = KaggleDatasets().get_gcs_path('flower-classification-with-tpus')\nGCS_PATH = os.path.join(GCS_DS_PATH, 'tfrecords-jpeg-512x512')\nTEST_FNS = tf.io.gfile.glob(os.path.join(GCS_PATH, 'test\/*.tfrec'))\nAUG_BATCH = 48\nBATCH_SIZE = 48\nIMAGE_SIZE = [512, 512]\nAUTO = tf.data.experimental.AUTOTUNE\nTRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH + '\/train\/*.tfrec')\nrow = 6; col = 4;","55f76bc2":"def decode_image(image_data):\n    image = tf.image.decode_jpeg(image_data, channels=3)\n    image = tf.cast(image, tf.float32) \/ 255.0  # convert image to floats in [0, 1] range\n    image = tf.reshape(image, [*IMAGE_SIZE, 3]) # explicit size needed for TPU\n    return image\n\ndef read_labeled_tfrecord(example):\n    LABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n        \"class\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means single element\n    }\n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    label = tf.cast(example['class'], tf.int32)\n    return image, label # returns a dataset of (image, label) pairs\n\ndef read_unlabeled_tfrecord(example):\n    UNLABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n        \"id\": tf.io.FixedLenFeature([], tf.string),  # shape [] means single element\n        # class is missing, this competitions's challenge is to predict flower classes for the test dataset\n    }\n    example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    idnum = example['id']\n    return image, idnum # returns a dataset of image(s)\n\ndef load_dataset(filenames, labeled = True, ordered = False):\n    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n    # Diregarding data order. Order does not matter since we will be shuffling the data anyway\n    \n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False # disable order, increase speed\n        \n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads = AUTO) # automatically interleaves reads from multiple files\n    dataset = dataset.with_options(ignore_order) # use data as soon as it streams in, rather than in its original order\n    dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls = AUTO) # returns a dataset of (image, label) pairs if labeled = True or (image, id) pair if labeld = False\n    return dataset\n\ndef data_augment(image, label):\n    # data augmentation. Thanks to the dataset.prefetch(AUTO) statement in the next function (below),\n    # this happens essentially for free on TPU. Data pipeline code is executed on the \"CPU\" part\n    # of the TPU while the TPU itself is computing gradients.\n    image = tf.image.random_flip_left_right(image)\n    return image, label   \n\ndef get_training_dataset(dataset, do_aug=True):\n    dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n    dataset = dataset.repeat() # the training dataset must repeat for several epochs\n    dataset = dataset.batch(AUG_BATCH)\n    if do_aug: dataset = dataset.map(transform, num_parallel_calls=AUTO) # note we put AFTER batching\n    dataset = dataset.unbatch()\n    dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n","f5338c52":"def transform(image, inv_mat, image_shape):\n    h, w, c = image_shape\n    cx, cy = w\/\/2, h\/\/2\n    new_xs = tf.repeat( tf.range(-cx, cx, 1), h)\n    new_ys = tf.tile( tf.range(-cy, cy, 1), [w])\n    new_zs = tf.ones([h*w], dtype=tf.int32)\n    old_coords = tf.matmul(inv_mat, tf.cast(tf.stack([new_xs, new_ys, new_zs]), tf.float32))\n    old_coords_x, old_coords_y = tf.round(old_coords[0, :] + w\/\/2), tf.round(old_coords[1, :] + h\/\/2)\n    clip_mask_x = tf.logical_or(old_coords_x<0, old_coords_x>w-1)\n    clip_mask_y = tf.logical_or(old_coords_y<0, old_coords_y>h-1)\n    clip_mask = tf.logical_or(clip_mask_x, clip_mask_y)\n    old_coords_x = tf.boolean_mask(old_coords_x, tf.logical_not(clip_mask))\n    old_coords_y = tf.boolean_mask(old_coords_y, tf.logical_not(clip_mask))\n    new_coords_x = tf.boolean_mask(new_xs+cx, tf.logical_not(clip_mask))\n    new_coords_y = tf.boolean_mask(new_ys+cy, tf.logical_not(clip_mask))\n    old_coords = tf.cast(tf.stack([old_coords_y, old_coords_x]), tf.int32)\n    new_coords = tf.cast(tf.stack([new_coords_y, new_coords_x]), tf.int64)\n    rotated_image_values = tf.gather_nd(image, tf.transpose(old_coords))\n    rotated_image_channel = list()\n    for i in range(c):\n        vals = rotated_image_values[:,i]\n        sparse_channel = tf.SparseTensor(tf.transpose(new_coords), vals, [h, w])\n        rotated_image_channel.append(tf.sparse.to_dense(sparse_channel, default_value=0, validate_indices=False))\n    return tf.transpose(tf.stack(rotated_image_channel), [1,2,0])\n\ndef random_rotate(image, angle, image_shape):\n    def get_rotation_mat_inv(angle):\n        # transform to radian\n        angle = math.pi * angle \/ 180\n        cos_val = tf.math.cos(angle)\n        sin_val = tf.math.sin(angle)\n        one = tf.constant([1], tf.float32)\n        zero = tf.constant([0], tf.float32)\n        rot_mat_inv = tf.concat([cos_val, sin_val, zero, -sin_val, cos_val, zero, zero, zero, one], axis=0)\n        rot_mat_inv = tf.reshape(rot_mat_inv, [3,3])\n        return rot_mat_inv\n    angle = float(angle) * tf.random.normal([1],dtype='float32')\n    rot_mat_inv = get_rotation_mat_inv(angle)\n    return transform(image, rot_mat_inv, image_shape)\n\n\ndef GridMask(image_height, image_width, d1, d2, rotate_angle=1, ratio=0.5):\n    h, w = image_height, image_width\n    hh = int(np.ceil(np.sqrt(h*h+w*w)))\n    hh = hh+1 if hh%2==1 else hh\n    d = tf.random.uniform(shape=[], minval=d1, maxval=d2, dtype=tf.int32)\n    l = tf.cast(tf.cast(d,tf.float32)*ratio+0.5, tf.int32)\n\n    st_h = tf.random.uniform(shape=[], minval=0, maxval=d, dtype=tf.int32)\n    st_w = tf.random.uniform(shape=[], minval=0, maxval=d, dtype=tf.int32)\n\n    y_ranges = tf.range(-1 * d + st_h, -1 * d + st_h + l)\n    x_ranges = tf.range(-1 * d + st_w, -1 * d + st_w + l)\n\n    for i in range(0, hh\/\/d+1):\n        s1 = i * d + st_h\n        s2 = i * d + st_w\n        y_ranges = tf.concat([y_ranges, tf.range(s1,s1+l)], axis=0)\n        x_ranges = tf.concat([x_ranges, tf.range(s2,s2+l)], axis=0)\n\n    x_clip_mask = tf.logical_or(x_ranges < 0 , x_ranges > hh-1)\n    y_clip_mask = tf.logical_or(y_ranges < 0 , y_ranges > hh-1)\n    clip_mask = tf.logical_or(x_clip_mask, y_clip_mask)\n\n    x_ranges = tf.boolean_mask(x_ranges, tf.logical_not(clip_mask))\n    y_ranges = tf.boolean_mask(y_ranges, tf.logical_not(clip_mask))\n\n    hh_ranges = tf.tile(tf.range(0,hh), [tf.cast(tf.reduce_sum(tf.ones_like(x_ranges)), tf.int32)])\n    x_ranges = tf.repeat(x_ranges, hh)\n    y_ranges = tf.repeat(y_ranges, hh)\n\n    y_hh_indices = tf.transpose(tf.stack([y_ranges, hh_ranges]))\n    x_hh_indices = tf.transpose(tf.stack([hh_ranges, x_ranges]))\n\n    y_mask_sparse = tf.SparseTensor(tf.cast(y_hh_indices, tf.int64),  tf.zeros_like(y_ranges), [hh, hh])\n    y_mask = tf.sparse.to_dense(y_mask_sparse, 1, False)\n\n    x_mask_sparse = tf.SparseTensor(tf.cast(x_hh_indices, tf.int64), tf.zeros_like(x_ranges), [hh, hh])\n    x_mask = tf.sparse.to_dense(x_mask_sparse, 1, False)\n\n    mask = tf.expand_dims( tf.clip_by_value(x_mask + y_mask, 0, 1), axis=-1)\n\n    mask = random_rotate(mask, rotate_angle, [hh, hh, 1])\n    mask = tf.image.crop_to_bounding_box(mask, (hh-h)\/\/2, (hh-w)\/\/2, image_height, image_width)\n\n    return mask\n\ndef apply_grid_mask(image, image_shape):\n    AugParams = {\n        'd1' : 100,\n        'd2': 160,\n        'rotate' : 45,\n        'ratio' : 0.3\n    }\n    mask = GridMask(image_shape[0], image_shape[1], AugParams['d1'], AugParams['d2'], AugParams['rotate'], AugParams['ratio'])\n    if image_shape[-1] == 3:\n        mask = tf.concat([mask, mask, mask], axis=-1)\n    return image * tf.cast(mask,tf.float32)\n\ndef gridmask(img_batch, label_batch):\n    return apply_grid_mask(img_batch, (512,512, 3)), label_batch","a942a6a2":"\nrow = min(row,AUG_BATCH\/\/col)\nall_elements = get_training_dataset(load_dataset(TRAINING_FILENAMES),do_aug=False).unbatch()\naugmented_element = all_elements.repeat().batch(AUG_BATCH).map(gridmask)\n\nfor (img,label) in augmented_element:\n    plt.figure(figsize=(15,int(15*row\/col)))\n    for j in range(row*col):\n        plt.subplot(row,col,j+1)\n        plt.axis('off')\n        plt.imshow(img[j,])\n    plt.show()\n    break","28c72e0d":"def onehot(image,label):\n    CLASSES = 104\n    return image,tf.one_hot(label,CLASSES)\n\ndef cutmix(image, label): #, PROBABILITY = 1.0\n    # input image - is a batch of images of size [n,dim,dim,3] not a single image of [dim,dim,3]\n    # output - a batch of images with cutmix applied\n    DIM = IMAGE_SIZE[0]\n    CLASSES = 104\n    \n    imgs = []; labs = []\n    for j in range(AUG_BATCH):\n        # CHOOSE RANDOM IMAGE TO CUTMIX WITH\n        k = tf.cast( tf.random.uniform([],0,AUG_BATCH),tf.int32)\n        # CHOOSE RANDOM LOCATION\n        x = tf.cast( tf.random.uniform([],0,DIM),tf.int32)\n        y = tf.cast( tf.random.uniform([],0,DIM),tf.int32)\n        b = tf.random.uniform([],0,1) # this is beta dist with alpha=1.0\n        WIDTH = tf.cast( DIM * tf.math.sqrt(1-b),tf.int32)\n        ya = tf.math.maximum(0,y-WIDTH\/\/2)\n        yb = tf.math.minimum(DIM,y+WIDTH\/\/2)\n        xa = tf.math.maximum(0,x-WIDTH\/\/2)\n        xb = tf.math.minimum(DIM,x+WIDTH\/\/2)\n        # MAKE CUTMIX IMAGE\n        one = image[j,ya:yb,0:xa,:]\n        two = image[k,ya:yb,xa:xb,:]\n        three = image[j,ya:yb,xb:DIM,:]\n        middle = tf.concat([one,two,three],axis=1)\n        img = tf.concat([image[j,0:ya,:,:],middle,image[j,yb:DIM,:,:]],axis=0)\n        imgs.append(img)\n        # MAKE CUTMIX LABEL\n        a = tf.cast(WIDTH*WIDTH\/DIM\/DIM,tf.float32)\n        if len(label.shape)==1:\n            lab1 = tf.one_hot(label[j],CLASSES)\n            lab2 = tf.one_hot(label[k],CLASSES)\n        else:\n            lab1 = label[j,]\n            lab2 = label[k,]\n        labs.append((1-a)*lab1 + a*lab2)\n            \n    # RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR (maybe use Python typing instead?)\n    image2 = tf.reshape(tf.stack(imgs),(AUG_BATCH,DIM,DIM,3))\n    label2 = tf.reshape(tf.stack(labs),(AUG_BATCH,CLASSES))\n    return image2,label2","d42aa52f":"\nrow = min(row,AUG_BATCH\/\/col)\nall_elements = get_training_dataset(load_dataset(TRAINING_FILENAMES),do_aug=False).unbatch()\naugmented_element = all_elements.repeat().batch(AUG_BATCH).map(cutmix)\n\nfor (img,label) in augmented_element:\n    plt.figure(figsize=(15,int(15*row\/col)))\n    for j in range(row*col):\n        plt.subplot(row,col,j+1)\n        plt.axis('off')\n        plt.imshow(img[j,])\n    plt.show()\n    break","c6d9507a":"def mixup(image, label):\n    # input image - is a batch of images of size [n,dim,dim,3] not a single image of [dim,dim,3]\n    # output - a batch of images with mixup applied\n    DIM = IMAGE_SIZE[0]\n    CLASSES = 104\n    \n    imgs = []; labs = []\n    for j in range(AUG_BATCH):\n        # CHOOSE RANDOM\n        k = tf.cast( tf.random.uniform([],0,AUG_BATCH),tf.int32)\n        a = tf.random.uniform([],0,1) # this is beta dist with alpha=1.0\n        # MAKE MIXUP IMAGE\n        img1 = image[j,]\n        img2 = image[k,]\n        imgs.append((1-a)*img1 + a*img2)\n        # MAKE CUTMIX LABEL\n        if len(label.shape)==1:\n            lab1 = tf.one_hot(label[j],CLASSES)\n            lab2 = tf.one_hot(label[k],CLASSES)\n        else:\n            lab1 = label[j,]\n            lab2 = label[k,]\n        labs.append((1-a)*lab1 + a*lab2)\n            \n    # RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR (maybe use Python typing instead?)\n    image2 = tf.reshape(tf.stack(imgs),(AUG_BATCH,DIM,DIM,3))\n    label2 = tf.reshape(tf.stack(labs),(AUG_BATCH,CLASSES))\n    return image2,label2","d13fb2e9":"row = 6; col = 4;\nrow = min(row,AUG_BATCH\/\/col)\nall_elements = get_training_dataset(load_dataset(TRAINING_FILENAMES),do_aug=False).unbatch()\naugmented_element = all_elements.repeat().batch(AUG_BATCH).map(mixup)\n\nfor (img,label) in augmented_element:\n    plt.figure(figsize=(15,int(15*row\/col)))\n    for j in range(row*col):\n        plt.subplot(row,col,j+1)\n        plt.axis('off')\n        plt.imshow(img[j,])\n    plt.show()\n    break","c3d1e978":"def get_random_eraser(input_img,p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1\/0.3, v_l=0, v_h=255, pixel_level=False):\n   # def eraser(input_img):\n    img_h, img_w, img_c = input_img.shape\n#     p_1 = np.random.rand()\n#     if p_1 > p:\n#         return input_img\n\n    while True:\n        s = np.random.uniform(s_l, s_h) * img_h * img_w\n        r = np.random.uniform(r_1, r_2)\n        w = int(np.sqrt(s \/ r))\n        h = int(np.sqrt(s * r))\n        left = np.random.randint(0, img_w)\n        top = np.random.randint(0, img_h)\n\n        if left + w <= img_w and top + h <= img_h:\n            break\n\n    if pixel_level:\n        c = np.random.uniform(v_l, v_h, (h, w, img_c))\n    else:\n        c = np.random.uniform(v_l, v_h)\n\n    input_img[top:top + h, left:left + w, :] = c\n\n    return input_img","e6b47aac":"TRAIN = '..\/input\/flower-image\/'\nIMAGE_SIZE = 512\nimport cv2\nn_imgs = 12\nimg_filenames = os.listdir(TRAIN)[:n_imgs]\nimg_filenames[:3]\ncols, rows = 4, 3\nimage=[]\ndef grid_display(list_of_images, no_of_columns=2, figsize=(15,15)):\n\n    fig = plt.figure(figsize=figsize)\n    column = 0\n    for i in range(len(list_of_images)):\n        column += 1\n        #  check for end of column and create a new figure\n        if column == no_of_columns+1:\n            fig = plt.figure(figsize=figsize)\n            column = 1\n        fig.add_subplot(1, no_of_columns, column)\n        plt.imshow(list_of_images[i])\n        plt.axis('off')\n\nfor file_name in img_filenames:\n    img = cv2.imread(TRAIN +file_name)\n    img = get_random_eraser(img)\n    image.append(img)\ngrid_display(image, 4, (15,15))","9641994a":"TRAIN = '\/kaggle\/input\/siim-isic-melanoma-classification\/jpeg\/train\/'\nIMAGE_SIZE = 1024\nimport cv2\nn_imgs = 12\nimg_filenames = os.listdir(TRAIN)[:n_imgs]\nimg_filenames[:3]\ncols, rows = 4, 3\nimage=[]\ndef grid_display(list_of_images, no_of_columns=2, figsize=(15,15)):\n\n    fig = plt.figure(figsize=figsize)\n    column = 0\n    for i in range(len(list_of_images)):\n        column += 1\n        #  check for end of column and create a new figure\n        if column == no_of_columns+1:\n            fig = plt.figure(figsize=figsize)\n            column = 1\n        fig.add_subplot(1, no_of_columns, column)\n        plt.imshow(list_of_images[i])\n        plt.axis('off')\n\nfor file_name in img_filenames:\n    img = cv2.imread(TRAIN +file_name)\n    img = get_random_eraser(img)\n    image.append(img)\ngrid_display(image, 4, (15,15))","5a5c5d5c":"thnaks for starter [kernal](https:\/\/www.kaggle.com\/mgornergoogle\/getting-started-with-100-flowers-on-tpu)","f4323b20":"# CutMix data augmentation\n\nCutMix augmentation strategy: patches are cut and pasted among training images where the ground truth labels are also mixed proportionally to the area of the patches. By making efficient use of training pixels and retaining the regularization effect of regional dropout, CutMix consistently outperforms the state-of-the-art augmentation strategies on CIFAR and ImageNet classification tasks, as well as on the ImageNet weakly-supervised localization task. \nCutMix improves the model robustness against input corruptions and its out-of-distribution detection performances.\n\n![cutmix.png](attachment:cutmix.png)\n\nMore details at [here](https:\/\/arxiv.org\/abs\/1905.04899)","cd6115c2":"## There are lot other in image Augmentation...\n\n### Data augmentation strategies that enhance localization and generalization performance have been suggested to ameliorate the performance of convolutional neural network classifiers.\n### You can apply it to the various classification problem...\n### like on kaggle...\n* Flower Classification\n* ISIC Melanoma Classification\n* Global wheet Detection..","14b24126":"# About this notebook....\nThis notebook give the overview of how to perform cutmix, mixup, Gridmask and Cutout data augmentation. And analyse the performance of AUgmentation.\n\nAugmentation help to improve model performance, but it depend on various things, like what is the problem statement, Which Augmentation help to improve the performance, Ratio of Augmentation etc...\n\n\n## <font color='red'>If you find it helpful please upvote it. Thanks!<font>\n","e8a6424b":"## Why Data Augmentation is Needed?\n\nGiven the limited set of data, one can train the model by applying variations to the existing image. Such modification to the image increases the overall size of the dataset, thereby contributing to the model\u2019s robustness. Thus, data augmentation strategies that enhance localization and generalization performance have been suggested to ameliorate the performance of convolutional neural network classifiers.\n## CutMix vs MixUp vs Gridmask vs CutOut Augmentation\n![accur.png](attachment:accur.png)","4bed2684":"# Cutout data augmentation\n\nCutout is a simple regularization technique for convolu- tional neural networks that involves removing contiguous sections of input images, effectively augmenting the dataset with partially occluded versions of existing samples.\n\n![cutout.png](attachment:cutout.png)\n\ncutout explain [here](https:\/\/arxiv.org\/pdf\/1708.04552.pdf)","4a4b6599":"# MixUp data augmentation\n\nMixup augmentation is a type of augmentation where in we form a new image through weighted linear interpolation of two existing images. We take two images and do a linear combination of them in terms of tensors of those images.\nMixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.\n\n![cutmix.png](attachment:cutmix.png)\n\nMixup are well explain [here](https:\/\/arxiv.org\/abs\/1710.09412)","e28e25bc":"Applying Augmentation On Melanoma Classification","3d979263":"## Refrences...\n* Thanks Chris Deotte for your kernal help.\n* Starter kernal help.[click here](https:\/\/www.kaggle.com\/mgornergoogle\/getting-started-with-100-flowers-on-tpu)\n\n## My other work on kaggle...\n* Melanoma Image classification.[click here](https:\/\/www.kaggle.com\/saife245\/melanoma-detail-analysis-eda-ip-augmentation-model\/notebook?scriptVersionId=35184465)\n* Reticultae- journey of python in r.[click here](https:\/\/www.kaggle.com\/saife245\/reticulate-a-journey-of-python-in-r)\n* Squeezenet- model of size 0.5mb.[click here](https:\/\/www.kaggle.com\/saife245\/squeezenet-model-size-of-0-5mb-is-it-true)\n* NeuroImaging Depth Analysis.[click here](https:\/\/www.kaggle.com\/saife245\/neuroimaging-in-depth-understanding-eda-model)","2c50423a":"## You can also apply different augmentation.You can also check out my work on this.[Click here](https:\/\/www.kaggle.com\/saife245\/melanoma-detail-analysis-eda-ip-augmentation-model\/notebook?scriptVersionId=35184465)\n\n### Thanks for reading....","c23b95b4":"# GridMask data augmentation\n\nGridMask is belong to Information dropping method. Information dropping might help improving model generalization through enforcing model to learn on remain information. But keeping the deletion and reservation region in a balance relation is needed. Too much information dropping might result in under-fitting, but too few might result in over-fitting.\n\n![grid.png](attachment:grid.png)\n\nGet more Details from [here](https:\/\/arxiv.org\/abs\/2001.04086)","c4a556d0":"## <font color='blue'>If you think this kernal will help you, please upvote it.;)<font>"}}