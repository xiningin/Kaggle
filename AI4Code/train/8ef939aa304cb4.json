{"cell_type":{"8241903b":"code","4caf98bd":"code","87c3e1fb":"code","9aef2221":"code","b468c2eb":"code","8a62db32":"code","5f564a24":"code","466bc558":"code","123f0b25":"code","21599358":"code","9004f708":"code","95ccfc54":"code","2a50d371":"code","6309a0e2":"code","cf77c1d2":"code","ff49c27f":"code","8db78707":"code","14dbbb60":"code","1c125e76":"code","ab912311":"code","a2ff2591":"code","4413667a":"code","bb6229b0":"code","33fba16c":"code","254a925c":"code","6618115b":"code","62912b12":"code","7045828c":"code","706e3d3c":"code","3325fcc5":"code","1efcc3fc":"code","41c22368":"code","22d95c41":"code","8294c2b8":"code","f472da34":"code","bddc445d":"code","7e9b162f":"code","08e59da1":"code","00d1ac0b":"code","36af0789":"code","ea0f2acb":"code","a5253f24":"code","0eedf6f3":"code","a80a8ca2":"code","4ef84e32":"code","2ac9b696":"code","0095fe0d":"code","6aa1f747":"code","5055340f":"code","d46f642e":"code","03504e5a":"code","e765df9f":"code","4d55ceda":"code","4f822971":"code","98e38908":"code","4665b8b2":"code","968dda5d":"code","0abf2d90":"markdown","dafc12d0":"markdown","c77a2117":"markdown","74923e2a":"markdown","600fdf03":"markdown","ef5941e3":"markdown","00df9f58":"markdown","3ddf505a":"markdown","2b87f81c":"markdown","80facff2":"markdown","eaa5f1e9":"markdown","64794ede":"markdown","21e975e1":"markdown","0e84c2be":"markdown","6b287fa0":"markdown","2f57f1ec":"markdown","fd31af4f":"markdown","5a76236e":"markdown","ce8b37de":"markdown","5016487e":"markdown","9cca96a3":"markdown","37595cfb":"markdown","9f572dd6":"markdown","1d60b72b":"markdown","d58cb7e6":"markdown","d66f7521":"markdown","5a57e9d6":"markdown","0e82d91c":"markdown","bf097b53":"markdown","352b31a5":"markdown","85fedef1":"markdown","96e4e374":"markdown","098b278f":"markdown","fc3cef6d":"markdown"},"source":{"8241903b":"#Importing Libraries to use for exploration of the data\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","4caf98bd":"data=pd.read_csv(\"..\/input\/pima-indians-diabetes-database\/diabetes.csv\")","87c3e1fb":"data.head(5) #Getting a glimpse at the data","9aef2221":"#Renaming the columns to an easy-to-understand names\ndata=data.rename(columns={\"BloodPressure\":\"Blood Pressure\",\"SkinThickness\":\"Skin Thickness\",\n                          \"DiabetesPedigreeFunction\":\"Pedigree Function\"})\ndata.head(3)","b468c2eb":"#Checking the shape of the data\nprint(f\"The dataset contains: {data.shape[0]} rows and {data.shape[1]} columns\")","8a62db32":"data.info()","5f564a24":"#Checking for duplicate values\ndata.duplicated().sum()","466bc558":"#Checking for null values\ndata.isnull().sum()","123f0b25":"# Checking the measure of central tendency and measure of dispersion\ndata.describe().T","21599358":"c=[\"Glucose\",\"Blood Pressure\",\"Skin Thickness\",\"Insulin\",\"BMI\",\"Pedigree Function\",\"Age\"]\ncontinous=[features for features in data[c]]","9004f708":"fig = plt.figure(1, (15, 15))\nsns.set_palette(sns.color_palette(\"Greens_r\"))\nfor i,cont in enumerate(continous):\n    ax = plt.subplot(3,3,i+1)\n    sns.histplot(data[cont],kde=True)\n    ax.set_title(f\"Distribution of {cont}\")\n    plt.tight_layout()\n    \nplt.show()","95ccfc54":"fig = plt.figure(1, (10, 5))\nsns.set_palette(sns.color_palette(\"Accent\"))\nfor i,cont in enumerate(continous):\n    ax = plt.subplot(3,3,i+1)\n    sns.boxplot(data=data,x=data[cont],showmeans=True)\n    ax.set_title(f\"Distribution of {cont}\")\n    ax.set_xlabel(\"\")\nplt.tight_layout()","2a50d371":"# Getting the count of the number of females in the outcome and pregnancy categories\nplt.figure(figsize = (12,12))\nplt.subplot(2,1,1)\nchart1=sns.countplot(x=\"Outcome\",data=data)\nfor k in chart1.patches:\n        percentage = '{:.1f}%'.format(100 * k.get_height()\/len(data[\"Outcome\"]))\n        x = k.get_x() + k.get_width() \/ 2\n        y = k.get_y() + k.get_height()\n        plt.annotate(percentage, (x, y),ha='center')\nplt.subplot(2,1,2)\nchart2=sns.countplot(x=\"Pregnancies\",data=data,palette=\"Greens_r\")\nfor l in chart2.patches:\n    chart2.annotate(format(l.get_height()),\n                   (l.get_x() + l.get_width() \/ 2,\n                    l.get_height()), ha='center', va='center',\n                   size=12, xytext=(0, 4),\n                   textcoords='offset points')","6309a0e2":"for i,col in enumerate(data.columns[:-1]):\n    sns.catplot(x='Outcome', y=col, data=data,kind=\"box\")\nplt.show()","cf77c1d2":"#Making a copy of the data so that it doesn't get affected during feature engineering and can be used for further analysis\ndata2=data.copy()","ff49c27f":"#Checking the values that has zero(s) in their column\nfor i in data2.columns[:-1]:\n    print(f\" {i} has {len(data2[data2[i]==0])} zero values\")","8db78707":"'''Replacing all the zero values with their median due to outliers \nexcept pregnancy because 0 could stand for women that aren't pregnant'''\n\ncol=[\"Glucose\",\"Blood Pressure\",\"Skin Thickness\",\"Insulin\",\"BMI\"]\nfor i in col:\n    data2[i]=data2[i].replace(0,np.median(sorted(data2[i])))","14dbbb60":"# Confirming the replacement in the above cell\nfor i in data2.columns[:-1]:\n    print(f\" {i} has {len(data2[data2[i]==0])} zero values\")","1c125e76":"#Checking for correlation with the target variable only\ndata2.corr()[\"Outcome\"].sort_values(ascending=False)","ab912311":"# dividing the features and target variables into individual dataframe\/series\nX=data2.iloc[:,:-1]\ny=data2.iloc[:,-1]\n","a2ff2591":"from sklearn.model_selection import train_test_split #module for splitting the data\nX_train,X_test,y_train,y_test= train_test_split(X,y,random_state=3,test_size=0.3)#splitting the data into train,test split","4413667a":"# Libraries needed for building and analysing the model\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import  classification_report, accuracy_score, precision_score, recall_score,f1_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay,plot_confusion_matrix ","bb6229b0":"def create_confusion_matrix(title,y_test,y_pred):\n    '''A function for plotting a Confusion Matrix'''\n    fig, ax = plt.subplots(1, 1)\n    matrix = confusion_matrix(y_test, y_pred, labels=[0,1])\n    matrix_display= ConfusionMatrixDisplay(confusion_matrix=matrix,\n                               display_labels=[\"No\",\"Yes\"])\n    matrix_display.plot(cmap='Greens',ax=ax)\n    ax.set_title(title)\n    plt.show()","33fba16c":"def metrics(model,X_train,X_test,y_train,y_test):\n    '''A function for getting the Accuracy,Recall and Precision score for both Training and test data '''\n    metrics_score=[]\n    train_pred=model.predict(X_train)\n    test_pred=model.predict(X_test)\n    train_accuracy=accuracy_score(y_train,train_pred)\n    test_accuracy=accuracy_score(y_test,test_pred)\n    train_recall=recall_score(y_train,train_pred)\n    test_recall=recall_score(y_test,test_pred)\n    train_precision=precision_score(y_train,train_pred)\n    test_precision=precision_score(y_test,test_pred)\n    train_f1 = f1_score(y_train,train_pred)\n    test_f1 = f1_score(y_test,test_pred)\n    metrics_score.extend((train_accuracy,test_accuracy,train_recall,test_recall,train_precision,test_precision,train_f1,test_f1))\n    print(\"PERFORMANCE OF THE MODEL FOR BOTH TRAIN AND TEST DATA\")\n    print(\"Accuracy   : Train:\",round(train_accuracy,2),\": \" \"Test:\",round(test_accuracy,2))\n    print(\"Recall Score   : Train:\",round(train_recall,2),\": \" \"Test:\",round(test_recall,2))\n    print(\"Precision Score   : Train:\",round(train_precision,2),\": \" \"Test:\",round(test_precision,2))\n    print(\"F1 score   : Train:\",round(train_f1,2),\": \" \"Test:\",round(test_f1,2))\n    create_confusion_matrix(\"Confusion Matrix for Train\",y_train,train_pred)     \n    create_confusion_matrix(\"Confusion Matrix for Test\",y_test,test_pred) \n    return metrics_score","254a925c":"# Empty lists for appending individual metric scores for each model to create a dataframe object later\ntrain_accuracy_score =[]\ntest_accuracy_score = []\ntrain_recall_score =  []\ntest_recall_score =  []\ntrain_precision_score= []\ntest_precision_score = []\ntrain_f1_score =       []\ntest_f1_score =        []\n\ndef model_scores(score):\n    '''A function for appending the metric scores into individual lists'''  \n    train_accuracy_score.append(score[0])\n    test_accuracy_score.append(score[1])\n    train_recall_score.append(score[2])\n    test_recall_score.append(score[3])\n    train_precision_score.append(score[4])\n    test_precision_score.append(score[5])\n    train_f1_score.append(score[6])\n    test_f1_score.append(score[7])","6618115b":"DT_model=DecisionTreeClassifier(random_state=1,class_weight=\"balanced\")\nDT_model.fit(X_train,y_train)","62912b12":"# Calling the metrics function\nDT_scores=metrics(DT_model,X_train,X_test,y_train,y_test)","7045828c":"model_scores(DT_scores) #storing the scores","706e3d3c":"RF_model=RandomForestClassifier(random_state=10,class_weight=\"balanced_subsample\")\nRF_model.fit(X_train,y_train)","3325fcc5":"RF_scores=metrics(RF_model,X_train,X_test,y_train,y_test)","1efcc3fc":"model_scores(RF_scores)","41c22368":"GB_model=GradientBoostingClassifier(random_state=2)\nGB_model.fit(X_train,y_train)","22d95c41":"GB_score=metrics(GB_model,X_train,X_test,y_train,y_test)","8294c2b8":"model_scores(GB_score)","f472da34":"path=DT_model.cost_complexity_pruning_path(X_train,y_train)\nccp_alphas,impurities=path.ccp_alphas,path.impurities","bddc445d":"alpha_val=[] # creating an empty list to append the ccp alphas and list of scores\nfor ccp_alpha in ccp_alphas:\n    pruned_model=DecisionTreeClassifier(random_state=1,ccp_alpha=ccp_alpha)\n    scores=cross_val_score(pruned_model,X_train,y_train,cv=5) # doing a cross validation on the ccp alphas\n    alpha_val.append([ccp_alpha,np.mean(scores)]) # Taking the mean of the cross validated ccp alphas","7e9b162f":"# Creating a dataframe to append the ccp alpha and the mean accuracy scores\nresults=pd.DataFrame(alpha_val,columns=[\"ccp alpha\",\"mean accuracy\"])\nresults.head(3)","08e59da1":"# Visualizing the plot of the mean accuracy against the ccp alpha so as to choose the ccp alpha with highest accuracy\nplt.figure(figsize=(15,10))\nresults.plot(x=\"ccp alpha\",y=\"mean accuracy\",marker=\"o\",drawstyle=\"steps-post\")\nplt.show()","00d1ac0b":"#To get the exact ccp alpha that gave the best mean accuracy,I will filter the dataframe to get the ccp alpha with mean accuracy > 0.76\nresults[results[\"mean accuracy\"]>0.76]","36af0789":"## The highest mean accuracy filtered out is 0.767117 and it's ccp alpha is 0.010332\n## Refitting the model with the parameters above\nDT_pruned_model=DecisionTreeClassifier(random_state=1,ccp_alpha=0.010332,class_weight=\"balanced\")\nDT_pruned_model.fit(X_train,y_train)","ea0f2acb":" # calling the metric function and passing the arguments into it\nDT_pruned_score=metrics(DT_pruned_model,X_train,X_test,y_train,y_test)","a5253f24":"model_scores(DT_pruned_score)","0eedf6f3":"DT_optimized=DecisionTreeClassifier(random_state=1,class_weight=\"balanced\")\n\nparameters={\"max_features\":[\"auto\",\"sqrt\",\"log2\"],\n            'min_samples_leaf': [10, 7, 5],\n            \"max_depth\":[None,3,5,9,10],\n            'max_leaf_nodes' : [ 5,7, 10],\n            \"min_samples_split\":[2,3,5,7,10],\n           \"min_impurity_decrease\": [0.0001,0.001,0.01,0.1]}\n\nfrom sklearn.metrics import make_scorer\nscorer = make_scorer(recall_score) # using recall score as the metric to evaluate by\n\nTuned= GridSearchCV(DT_optimized,param_grid=parameters, scoring=scorer,n_jobs=-1,cv=5) # running the Grid search\nTuned = Tuned.fit(X_train, y_train)\n\nDT_optimized = Tuned.best_estimator_  #Selecting the best estimator from the Grid Search\n \nDT_optimized.fit(X_train, y_train) #fitting the best estimator to the model","a80a8ca2":"\nDT_optimized_score=metrics(DT_optimized,X_train,X_test,y_train,y_test)","4ef84e32":"model_scores(DT_optimized_score)","2ac9b696":"RF_pre_tuned = RandomForestClassifier(random_state=10,class_weight=\"balanced_subsample\")\n\nparameters= {\"n_estimators\": [100,200,250,500],\n            \"max_depth\":[None],\n            \"min_samples_leaf\": [3,5,7,10],\n            \"max_features\": ['auto']\n            }\nscorer = make_scorer(recall_score)\n\nRF_pre_tuned=GridSearchCV(RF_pre_tuned,param_grid=parameters,n_jobs=-1,scoring=scorer,cv=5)\n\nRF_pre_tuned.fit(X_train,y_train)\nRF_tuned = RF_pre_tuned.best_estimator_  \n \nRF_tuned.fit(X_train, y_train)","0095fe0d":"RF_tuned_score=metrics(RF_tuned,X_train,X_test,y_train,y_test)","6aa1f747":"model_scores(RF_tuned_score)","5055340f":"gbc = GradientBoostingClassifier(random_state=5)\nparameters = {\n    \"n_estimators\":[10,20,30],\n    \"max_depth\":[1,3,5,7,9],\n    \"learning_rate\":[0.001,0.01,0.1,0.2,0.03],\n    \"min_samples_leaf\":[1,3,4]\n}\ngbc1= GridSearchCV(gbc,parameters,cv=5,scoring = make_scorer(recall_score))\ngbc1.fit(X_train,y_train)\n\nGB_tuned= gbc1.best_estimator_   \nGB_tuned.fit(X_train, y_train)","d46f642e":"GB_tuned_score=metrics(GB_tuned,X_train,X_test,y_train,y_test)","03504e5a":"model_scores(GB_tuned_score)","e765df9f":"test_recall_score","4d55ceda":"model_metrics = pd.DataFrame({'Model Names':['Decision Tree','Random Forest','Gradient Boosting','Pruned Decision Tree',\n                            'Optimized Decision Tree','Tuned Random Forest','Tuned Gradient Boosting'], \n                            'Train_Accuracy_Score':np.round(train_accuracy_score,2),'Test_Accuracy_Score': np.round(test_accuracy_score,2),\n                            'Train_Recall_Score':np.round(train_recall_score,2),'Test_Recall_Score':np.round(test_recall_score,2),\n                            'Train_Precision_Score':np.round(train_precision_score,2),'Test_Precision_Score':np.round(test_precision_score,2),\n                            'Train_F1 Score':np.round(train_f1_score,2),\"Test_F1 Score\":np.round(test_f1_score,2)}) ","4f822971":"model_metrics","98e38908":"model_metrics.sort_values(by='Test_Recall_Score',ascending=False)#Sort values in descending order using the Test Recall score","4665b8b2":"##Visualizing the decision tree\nfrom sklearn import tree\nplt.figure(figsize=(20,15))\ntree.plot_tree(DT_optimized,filled=True,rounded=True,feature_names=X.columns,fontsize=12,node_ids=True,class_names=[\"NO\",\"YES\"])\nplt.show()","968dda5d":"feature_names = X_train.columns\nimportant_features = DT_optimized.feature_importances_\nindices = np.argsort(important_features)\n\nplt.figure(figsize=(12,8))\nplt.title('Most Important Features')\nplt.barh(range(len(indices)), important_features[indices], color='Green', align='center')\nplt.yticks(range(len(indices)), [feature_names[i] for i in indices])\nplt.xlabel('Importance')\nplt.show()","0abf2d90":"#### Observations\n1) There are 769 rows and 9 columns of data <br \/>\n2) There is no duplicate value<br \/>\n3) The dataset contains no null values<br \/>\n4) The dataset comprises of float and integer data types<br \/>\n5) The minimum pregnancy is zero which can mean that the woman has never been pregnant while the maximum pregnancy is 17<br \/>\n6) Glucose has a minimum value of 0 which isn't possible in an oral glucose tolerance test so this should further be looked into<br \/>\n7) Insulin has a minimum value of 0 which could mean a type 1 diabetes but should be further looked into<br \/>\n8) Skinthickness and BMI both has a minimum value of 0 which isn't possible and may be dropped<br \/>\n9) The dialostic blood pressure has a minimum of 0 which is almost impossible since it means there is no blood flow in the body though it has rare cases in hypotension <br \/>\n9) The age range is from 21-81<br \/>\n10) Blood pressure,skin thickness,insulin and age seems to have outliers due to the distance between the 3rd quartile and maximum value. Should be looked into\n","dafc12d0":"#### Observations\n1) Glucose is normally distributed and 0 is the only outlier value.<br \/>\n2) Blood Pressure is normally distributed and have a few outliers...most of which falls in 0. Based on domain knowledge, it's known that diastolitic blood pressure is almost never 0 because blood flows through the body except for rare cases of hypotension where we can have very low blood pressure and rarely 0 as blood pressure. Most of the outliers fall on 0 which means that it's not a rare case of hypotension and 0 can be treated as a missing value.<br \/>\n3) Skin thickness is right skewed and has an outlier value of 99.<br \/>\n4) Insulin is right skewed and has outliers ranging from 300-846.<br \/>\n5) BMI is normally distributed and has 0 as an outlier as well as values between 50-70. We can't have a body mass index of 0 even for an underweight person so we treat 0 as a missing value. The other outliers can be for rare cases of extreme obesity as obesity starts from values greater than 30 BMI <br \/>\n6) Pedigree function is rightskewed <br \/>\n7) Age is right skewed and the outliers falls between 68-81. Age is supposed to be normally distributed by convention but might have been right skewed due to the constraints put on it. 21-81","c77a2117":"#### Analysing the discrete numeric variable and the target variable which is Pregnancies and Outcome respectively","74923e2a":"### Hypertuning the Decision Tree model\n\nThere are two methods of hypertuning a decision tree\n\n1) Cost complexity pruning method<br \/>\n2) Optimizing its parameters and this can be done using RandomSearchCV or GridSearcCV","600fdf03":"# CONCLUSIONS\n\n1) The model has been successfully optimized and it has the lowest False negative value and is the best model among all the algorithms used based on this<br \/>\n2) The model incorrectly classified 11 out of 98 diabetic patient as non-diabetic. More optimization could be done in the future to get better metric values<br \/>\n3) Women older than 28 years should watch their glucose level and their weight well so as not to run risk of having diabetes.<br \/>\n4) According to the medical BMI,obesity starts from 30 and is same as what the decision tree optimized. That being said, it is advisable that women should watch their weight and stay fit.<br \/>\n5) The initial Data Exploratory Data analysis hit most of the mark the models hit with only minimal deviation. This shows the importance of doing Exploratory data analysis even before building a model.","ef5941e3":"#### observation\n1) The data is imbalanced and the ratio is 65:35.<br \/>\n2) The women with only 1 pregnancy are more prominent in the dataset followed by women that hasn't gotten pregnant. Women with 11-17 pregnancies are low-rare in the dataset","00df9f58":"#### Factors Evidently  Affecting Diabetes\n1) Diabetes is common in patients with glucose value above 125<br \/>\n2) Obesity has an influence on Diabetes. Obese women has higher rate of diabetes. Obese women have BMI greater than 30<br \/>\n3) Diabetic patients has insulin level above 150<br \/>\n4) Female Patients above the age of 30 are at more risk of diabetes<br \/>","3ddf505a":"#### Observation\n1) The models are all overfitted by the training data with the training scores being 1.0 except for Gradient Boosting Classifier with 0.94 <br \/>\n2) Decision tree has the lowest recall score of 0.5 while Gradient boosting has the highest recall score of 0.64","2b87f81c":"**The best model with the highest Test Recall score is the Decision tree that was hypertuned using GridSearchCV with a recall score of 89% and false negatives of 11. It is also the most generalized model.**\n\nOut of the 98 patients that actually had diabetes,the model correctly predicted that 87 had diabetes and made wrongly classified 11 people as not having diabetes.**","80facff2":"#### Visualizing the distribution of the Numerical values with emphasis on the outliers","eaa5f1e9":"#### Correlation with the target variable","64794ede":"### Tuning the Random Forest Clasifier","21e975e1":"# Data Preprocessing","0e84c2be":"## Decision Tree Algorithm\n\nClass_weight is a parameter and can be None,balanced or a dict of weights for each classes.If None, all classes are supposed to have weight one and if \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data. **This means that the balanced mode is best for imbalanced datasets as it automatically gives the minority class higher weights and this accounts for the imbalance. None means that the classes has equal weights and works best for a balanced dataset**","6b287fa0":"**Using cost complexity pruning path method**","2f57f1ec":"# Exploratory Data Analysis\nAnalysing and creating a visual representation of the relationship between the features","fd31af4f":"# Building the models using some Tree ML algorithms\n\nThe dataset being analysed for prediction has to do with accurately predicting whether a person has or do not have diabetes. There are particular metrics to be considered depending on the scenario that occurs. The scenarios includes a case of:\n1) True positives:  This is when the model correctly predicts that the person has diabetes and the person actually has it. <br \/>  <br \/>\n2) True negatives: This is when the model predicts that the person doesn't have diabetes and in reality,the person doesn't.  <br \/> <br \/>\n3) False Positives: This is when the person doesn't have diabetes but the model predicts that the person has diabetes. This is not a bad case scenario. Atleast, the person can get tested at the hospital then  <br \/> <br \/>\n4) False Negatives: This is when a person has diabetes but the model predicts that the person doesn't have diabetes. This can lead to the condition of the person worsening without being treated  <br \/>\n\n**The scenario to avoid in this case is the False Negative scenario. It is the worst thing that can happen with this dataset and thus, the aim of the model should be to reduce the number of False Negatives. \nThere are different metrics that can be used to evaluate the models. The metric to watch out for in this case is the RECALL score. The higher the recall score, the lower the false negatives in the model.**","5a76236e":"**Having known the best model(according to the metric..Recall score),it's time to know which variables were most responsible in the patient having or not having diabetes**","ce8b37de":"#### Analyzing the continous numerical variables which are Glucose,Blood Pressure,Skin Thickness,Insulin,BMI,Pedigree Function and Age","5016487e":"The most important and deciding factors according to the model that predicted the lowest false negatives are\n1) Age<br \/>\n2) Glucose<br \/>\n3) Insulin<br \/>\n4) BMI<br \/><br \/>\n**The people who had diabetes had the following values in their variables:**\n1) Age >28.5<br \/>\n2) Glucose>127.5mg\/dL<br \/>\n3) BMI>29.95<br \/>\n4)Insulin <=123.5 or>123.5","9cca96a3":"## Random Forest Algorithm\n\nClass_weight can be balanced,balanced subsample,None or a dictionary of weights.\n. Balanced here means the same as that of decision tree\n. balanced subsample: Random forest is a bagging(bootstrap)ensembling method. Random samples with replacement are taken to build each decision tree in it. If the class weight was balanced, it will be balanced for the entire dataset but not for each bootstrapped data. Balanced subsample accounts for balancing in each of the bootstrapped dataset","37595cfb":"**Hypertuning using GridSearchCV(optimization)**","9f572dd6":"**Hypertuning all the models to choose the model that best increase the Recall score and reduce the number of false negative**","1d60b72b":"## Gradient Boosting","d58cb7e6":"#### Observation\n\nAfter hypertuning the Decision Tree algorithm with 2 methods( Cost complexity pruning and GridCv),i deduced that:\n1) The Cost complexity pruning gave a Test recall score of 0.69 and false negatives of 30. This is a higher recall score than the pre-pruned decision tree of 0.5 <br \/>\n2) The GridCV gave the highest test recall value of 0.89 and False negatives of 11 which is the best among the decision tree model so far","d66f7521":"#                           DIABETES PREDICTION IN FEMALE\n\n\n### Problem Statement\nDiabetes is one of the ailments that takes lives if not detected early and treated. Knowing and confirming the variables that contribute to this disease is one of the aims of this project. I want to  find out the variable(s) that are indicators of diabetes as this would be variables that other women would try to work on so that they don't get diagnosed with diabetes. Like a quote from one of my favourite movie says,\"If you know yourself and know your enemy,then there would be no reason to fight\". Likewise,if we know what to watch out for, we stand a better chance of being safe from it or detecting it early.\n\n### Information about the dataset\nThis dataset was recorded with information from female patients that has been diagnosed or not diagnosed with Diabetes. With the aid this machine learning project, females can know what to look out for in order not to have the ailment and they will also be able to get a preliminary diagnosis on their status. This data was gotten from females of Pima Indian heritage who were 21 years and above.\n\nThe dataset contains the following variables:\n\n1) Pregnancies: Number of times pregnant<br \/>\n2) Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test<br \/>\n3) BloodPressure: Diastolic blood pressure (mm Hg)<br \/>\n4) SkinThickness: Triceps skin fold thickness (mm)<br \/>\n5) Insulin: 2-Hour serum insulin (mu U\/ml)<br \/>\n6) BMI: Body mass index (weight in kg\/(height in m)^2)<br \/>\n7) DiabetesPedigreeFunction: is it inheditary? Depending on which part of the family it is, the values are different. The pedigree function of parents is different from that of uncles of grandparents,etc<br \/>\n8) Age: Age (years)<br \/>\n9) Outcome: Class variable (0 or 1)\n\nIn order to further understand some of the variables, you can click on the below links\n\n. [BMI](https:\/\/www.cdc.gov\/healthyweight\/assessing\/bmi\/adult_bmi\/english_bmi_calculator\/bmi_calculator.html)<br \/>\n. [GLUCOSE](https:\/\/www.mountsinai.org\/health-library\/tests\/glucose-tolerance-test-non-pregnant)<br \/>\n. [INSULIN](https:\/\/www.endocrineweb.com\/conditions\/diabetes\/diagnosing-diabetes)<br \/>\n\n\n\n**By the end of this project,i hope to have answers to the problem statement and to be able to build a model that can diagnose the patients correctly**\n","5a57e9d6":"# Feature Engineering","0e82d91c":"**Bivariate Analysis**\n\nAnalysing the relationship between each features and the Outcome variable","bf097b53":"### Observation\nThe top 4 features that mostly affects the possibility of a female having diabetes are:\n1) The Glucose Level<br \/>\n2) The BMI<br \/>\n3) Age<br \/>\n4) Pregnancies","352b31a5":"#### Splitting the data","85fedef1":"### Tuning the Gradient Boosting Clasifier","96e4e374":"# Hypertuning the Models","098b278f":"#### Importing Libraries and defining Functions for the models","fc3cef6d":"**UNIVARIATE ANALYSIS**"}}