{"cell_type":{"438bf912":"code","39ac5a28":"code","386a026f":"code","9186ca9a":"code","65988564":"code","aa5b2454":"code","6ea3481c":"code","033ac784":"code","b51ee0e6":"code","a78aed7d":"markdown"},"source":{"438bf912":"import pandas as pd\nimport numpy as np\nimport re\nimport glob\nimport json\nfrom tqdm import tqdm","39ac5a28":"# Code for loading JSON into a dataframe adopted from https:\/\/www.kaggle.com\/maksimeren\/covid-19-literature-clustering\n# Load all Json \nroot_path = '\/kaggle\/input\/CORD-19-research-challenge\/'\nall_json = glob.glob(f'{root_path}\/**\/*.json', recursive=True)\nprint(len(all_json))\n    \n#A File Reader Class which loads the json and make data available\nclass FileReader:\n    def __init__(self, file_path):\n        with open(file_path) as file:\n            content = json.load(file)\n            self.paper_id = content['paper_id']\n            self.abstract = []\n            self.body_text = []\n            # Abstract\n            try:\n                if content['abstract']:\n                    for entry in content['abstract']:\n                        self.abstract.append(entry['text'])  \n            except KeyError:\n                #do nothing\n                pass \n            # Body text\n            for entry in content['body_text']:\n                self.body_text.append(entry['text'])\n            self.abstract = '\\n'.join(self.abstract)\n            self.body_text = '\\n'.join(self.body_text)\n    def __repr__(self):\n        return f'{self.paper_id}: {self.abstract[:200]}... {self.body_text[:200]}...'\n        \ndict_ = None\ndict_ = {'paper_id': [], 'abstract': [], 'body_text': []}\nfor idx, entry in enumerate(all_json):\n    if idx % (len(all_json) \/\/ 10) == 0:\n         print(f'Processing index: {idx} of {len(all_json)}')\n    content = FileReader(entry)\n    dict_['paper_id'].append(content.paper_id)\n    dict_['abstract'].append(content.abstract)\n    dict_['body_text'].append(content.body_text)\n        \ndf_covid = pd.DataFrame(dict_, columns=['paper_id', 'abstract', 'body_text'])\n    ","386a026f":"df_covid.head()","9186ca9a":"import gensim\nimport datetime\nfrom nltk.tokenize import sent_tokenize\nfrom gensim.utils import simple_preprocess\nfrom gensim.sklearn_api.phrases import PhrasesTransformer\nfrom gensim.models import Word2Vec\n# Commented out as the EpocLogger creates issues when the model is loaded into a new kernel as it is not supported\n#class EpochLogger(gensim.models.callbacks.CallbackAny2Vec):\n#    \"\"\"Callback to log information about training.\"\"\"\n#    def __init__(self):\n#       self.epoch = 0\n\n#def on_epoch_begin(self, model):\n#        print(\"{} Starting epoch #{}\".format(\n#            datetime.datetime.now(), self.epoch))\n    \n#    def on_epoch_end(self, model):\n#        print(\"{} Finished epoch #{}\".format(\n#            datetime.datetime.now(), self.epoch))\n#        self.epoch += 1\n\n\nsentences = []\nfor text in tqdm(df_covid['body_text'].iloc[0:9999]):\n    sentences += [simple_preprocess(sentence) for sentence in sent_tokenize(gensim.parsing.preprocessing.remove_stopwords(text))]\nbigrams = PhrasesTransformer(min_count=20, threshold=50)\nmodel = Word2Vec(bigrams.fit_transform(sentences), \n                 size=200, \n                 window=8, \n                 min_count=10,\n                 sg=True,\n                 hs=False,\n                 alpha=0.01,\n                 sample=0.0001,\n                 negative=15,\n                 workers=4, \n                 #callbacks=[EpochLogger()],\n                 iter=4)\n    \nsentences = []\nfor text in tqdm(df_covid['body_text'].iloc[10000:19999]):\n    sentences += [simple_preprocess(sentence) for sentence in sent_tokenize(gensim.parsing.preprocessing.remove_stopwords(text))]\nmodel.train(sentences, total_words=len(sentences),  epochs=model.epochs)\n\nsentences = []\nfor text in tqdm(df_covid['body_text'].iloc[20000:]):\n    sentences += [simple_preprocess(sentence) for sentence in sent_tokenize(gensim.parsing.preprocessing.remove_stopwords(text))]\nmodel.train(sentences, total_words=len(sentences),  epochs=model.epochs)\n\n    \nmodel.save('covid19word2vec.model')","65988564":"# Enter words in lower case\nprint(model.wv.most_similar('covid', topn=10))\n","aa5b2454":"print(model.wv.most_similar('death', topn=10))","6ea3481c":"Code to load the word2vec model after adding the output of this kernel from Add Data -> ","033ac784":"import gensim\nword2vec_root_path = '\/kaggle\/input\/covid-19-word2vec-model-and-vectors\/'\nword2vec_filename = 'covid19word2vec.model'\nword2vecfile =  word2vec_root_path + word2vec_filename\nmodel = gensim.models.Word2Vec.load(word2vecfile)","b51ee0e6":"print(model.wv.most_similar('diabetes', topn=10))","a78aed7d":"# ## COVID-19 Word2Vec Model and Vectors\n\nThe kernel reads CORD-19 data and trains a Word2Vec model on the body text of the various papers. This kernel has been implemented to support [this kernel ](https:\/\/www.kaggle.com\/uplytics\/thematic-analysis-of-risks-with-evidence-gap-maps). The model and embeddings are available as part of the kernel output to be included in other kernels. \n\nThe approach to train this separately has been taken to avoid RAM overflow issues which has been expereincd while training Word2Vec model, integrated into a larger kernel. Options to decrease vector size, vocabulary size or increasing the min count parameters of the Gensim Word2Vec model to reduce RAM requirements. However in this kernel , iteratively the Word2Vec model is trained to ensure RAM usage stays below the 16GB Kaggle threshold.\n\nThis kernel only removed stopwords and does not use any stemming & lematization as the purpose is to support regex based keyword extraction !!\n\nThe configuration option used for tuning the Word2vec are obtained from the kernel given [here](https:\/\/www.kaggle.com\/vahetshitoyan\/word-and-phrase-associations-in-cord-19-corpus) \n"}}