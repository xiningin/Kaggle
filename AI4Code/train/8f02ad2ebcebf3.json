{"cell_type":{"2c1a35f6":"code","a72a5d81":"code","5dae0a89":"code","508c364a":"code","0e697e84":"code","6a157595":"code","0908e854":"code","465a38f7":"code","c4ea7f56":"code","1c9946a7":"code","b6908f48":"code","a8478804":"code","ec6d3059":"code","3a53021d":"code","3c0ce2ae":"code","081ad1bd":"code","f9c7a480":"code","63db3cfa":"code","1b00d33a":"code","e47b9858":"code","1df6ced5":"code","a7af9e05":"code","c29fd9f7":"code","f756908f":"code","eb44492b":"markdown","2fae2fb4":"markdown","8398445b":"markdown","559ee219":"markdown","dd8ef405":"markdown","eddcbdc9":"markdown","fad4510d":"markdown","b0e9a946":"markdown","f1590953":"markdown","bee7d3f4":"markdown","29bde0de":"markdown","e3597333":"markdown"},"source":{"2c1a35f6":"!nvidia-smi","a72a5d81":"# !pip uninstall transformers -y &> \/dev\/null\n!pip install transformers==3.5.1 &> \/dev\/null\n# !pip uninstall torch &> \/dev\/null\n!pip install torch==1.4.0 &> \/dev\/null\nprint(\"finished installing requirements\")","5dae0a89":"import transformers\nprint(transformers.__version__)","508c364a":"import numpy as np\nimport pandas as pd \nimport os\nimport re\nimport time\nimport string\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom plotly import graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\nfrom collections import Counter\nimport nltk\nfrom tqdm import tqdm\nimport os\nimport spacy\nfrom spacy.util import compounding\nfrom spacy.util import minibatch\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom torch import optim\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline\ntorch.cuda.empty_cache()","0e697e84":"torch.cuda.empty_cache()","6a157595":"data = pd.read_csv(\"..\/input\/indonesiahatespeechpreprocessed\/notsoclean\/data_preprocessed.csv\")\ndata.sample(5)","0908e854":"import random\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch import optim\nimport torch.nn.functional as F\nfrom tqdm import tqdm\n\nfrom transformers import BertForSequenceClassification, BertConfig, BertTokenizer\nfrom nltk.tokenize import TweetTokenizer","465a38f7":"import numpy as np\nimport pandas as pd\nimport string\nimport torch\nimport re\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer\n\nclass DocumentSentimentDataset(Dataset):\n    # Static constant variable\n    LABEL2INDEX = {'positive': 1, 'negative': 0}\n    INDEX2LABEL = {1: 'positive', 0: 'negative'}\n    NUM_LABELS = 2\n    \n    def load_dataset(self, path): \n        df = pd.read_csv(path)\n        # df.columns = ['sentiment','text']\n        df['hs_class'] = df['hs_class'].apply(lambda lab: self.LABEL2INDEX[lab])\n        return df\n    \n    def __init__(self, dataset_path, tokenizer, no_special_token=False, *args, **kwargs):\n        self.data = self.load_dataset(dataset_path)\n        self.tokenizer = tokenizer\n        self.no_special_token = no_special_token\n    \n    def __getitem__(self, index):\n        data = self.data.loc[index,:]\n        text, hs_class = data['text'], data['hs_class']\n        subwords = self.tokenizer.encode(text, add_special_tokens=not self.no_special_token)\n        return np.array(subwords), np.array(hs_class), data['text']\n    \n    def __len__(self):\n        return len(self.data)    \n        \nclass DocumentSentimentDataLoader(DataLoader):\n    def __init__(self, max_seq_len=512, *args, **kwargs):\n        super(DocumentSentimentDataLoader, self).__init__(*args, **kwargs)\n        self.collate_fn = self._collate_fn\n        self.max_seq_len = max_seq_len\n        \n    def _collate_fn(self, batch):\n        batch_size = len(batch)\n        max_seq_len = max(map(lambda x: len(x[0]), batch))\n        max_seq_len = min(self.max_seq_len, max_seq_len)\n        \n        subword_batch = np.zeros((batch_size, max_seq_len), dtype=np.int64)\n        mask_batch = np.zeros((batch_size, max_seq_len), dtype=np.float32)\n        sentiment_batch = np.zeros((batch_size, 1), dtype=np.int64)\n        \n        seq_list = []\n        for i, (subwords, sentiment, raw_seq) in enumerate(batch):\n            subwords = subwords[:max_seq_len]\n            subword_batch[i,:len(subwords)] = subwords\n            mask_batch[i,:len(subwords)] = 1\n            sentiment_batch[i,0] = sentiment\n            \n            seq_list.append(raw_seq)\n            \n        return subword_batch, mask_batch, sentiment_batch, seq_list","c4ea7f56":"def forward_sequence_classification(model, batch_data, i2w, is_test=False, device='cpu', **kwargs):\n    # Unpack batch data\n    if len(batch_data) == 3:\n        (subword_batch, mask_batch, label_batch) = batch_data\n        token_type_batch = None\n    elif len(batch_data) == 4:\n        (subword_batch, mask_batch, token_type_batch, label_batch) = batch_data\n    \n    # Prepare input & label\n    subword_batch = torch.LongTensor(subword_batch)\n    mask_batch = torch.FloatTensor(mask_batch)\n    token_type_batch = torch.LongTensor(token_type_batch) if token_type_batch is not None else None\n    label_batch = torch.LongTensor(label_batch)\n    if device == \"cuda\":\n        subword_batch = subword_batch.cuda()\n        mask_batch = mask_batch.cuda()\n        token_type_batch = token_type_batch.cuda() if token_type_batch is not None else None\n        label_batch = label_batch.cuda()\n\n    # Forward model\n    outputs = model(subword_batch, attention_mask=mask_batch, token_type_ids=token_type_batch, labels=label_batch)\n    loss, logits = outputs[:2]\n    # generate prediction & label list\n    list_hyp = []\n    list_label = []\n    hyp = torch.topk(logits, 1)[1]\n    for j in range(len(hyp)):\n        list_hyp.append(i2w[hyp[j].item()])\n        list_label.append(i2w[label_batch[j][0].item()])\n        \n    return loss, list_hyp, list_label","1c9946a7":"import itertools\nfrom sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n\ndef document_sentiment_metrics_fn(list_hyp, list_label):\n    metrics = {}\n    metrics[\"ACC\"] = accuracy_score(list_label, list_hyp)\n    metrics[\"F1\"] = f1_score(list_label, list_hyp, average='macro')\n    metrics[\"REC\"] = recall_score(list_label, list_hyp, average='macro')\n    metrics[\"PRE\"] = precision_score(list_label, list_hyp, average='macro')\n    return metrics","b6908f48":"import itertools\nfrom sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n\ndef get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n    \ndef count_param(module, trainable=False):\n    if trainable:\n        return sum(p.numel() for p in module.parameters() if p.requires_grad)\n    else:\n        return sum(p.numel() for p in module.parameters())\n\ndef metrics_to_string(metric_dict):\n    string_list = []\n    for key, value in metric_dict.items():\n        string_list.append('{}:{:.2f}'.format(key, value))\n    return ' '.join(string_list)\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    \nset_seed(26092020)\n","a8478804":"# Load Tokenizer and Config\ntokenizer = BertTokenizer.from_pretrained('indobenchmark\/indobert-base-p1')\nconfig = BertConfig.from_pretrained('indobenchmark\/indobert-base-p1')\nconfig.num_labels = DocumentSentimentDataset.NUM_LABELS\n\n# Instantiate model\nmodel = BertForSequenceClassification.from_pretrained('indobenchmark\/indobert-base-p1', config=config)\n\n# tokenizer = BertTokenizer.from_pretrained(\"indobenchmark\/indobert-base-p1\")\n# model = BertForMultiLabelClassification.from_pretrained('indobenchmark\/indobert-base-p1', config=config)\n\n'''\nif you are loading saved model\nloaded_model = torch.load(\".\/model.pkl\")\nmodel = loaded_model\n'''\n# model","ec6d3059":"count_param(model)","3a53021d":"train_dataset_path = \"..\/input\/indonesiahatespeechpreprocessed\/notsoclean\/train_split.csv\"\ntest_dataset_path = \"..\/input\/indonesiahatespeechpreprocessed\/notsoclean\/test_split.csv\"\nvalid_dataset_path = \"..\/input\/indonesiahatespeechpreprocessed\/notsoclean\/validate_split.csv\"\n\ntrain_na = pd.read_csv(train_dataset_path).dropna().to_csv(\"train_new.csv\", index=False)\ntest_na = pd.read_csv(test_dataset_path).dropna().to_csv(\"test_new.csv\", index=False)\nvalid_na = pd.read_csv(valid_dataset_path).dropna().to_csv(\"validate_new.csv\", index=False)\n\ntrain_dataset_path = '.\/train_new.csv'\ntest_dataset_path = '.\/test_new.csv'\nvalid_dataset_path = '.\/validate_new.csv'\n\ntrain_dataset = pd.read_csv(train_dataset_path)\ntest_dataset = pd.read_csv(test_dataset_path)\nvalid_dataset = pd.read_csv(valid_dataset_path)","3c0ce2ae":"df = pd.read_csv(train_dataset_path, header=None)\ndf.columns = ['sentiment', 'text']\ndf.head()","081ad1bd":"train_dataset = DocumentSentimentDataset(train_dataset_path, tokenizer, lowercase=True)\ntest_dataset = DocumentSentimentDataset(test_dataset_path, tokenizer, lowercase=True)\nvalid_dataset = DocumentSentimentDataset(valid_dataset_path, tokenizer, lowercase=True)\n\ntrain_loader = DocumentSentimentDataLoader(dataset=train_dataset, max_seq_len=512, batch_size=16, num_workers=16, shuffle=True)  \ntest_loader = DocumentSentimentDataLoader(dataset=test_dataset, max_seq_len=512, batch_size=16, num_workers=16, shuffle=False)\nvalid_loader = DocumentSentimentDataLoader(dataset=valid_dataset, max_seq_len=512, batch_size=16, num_workers=16, shuffle=False)  \n\n\nw2i, i2w = DocumentSentimentDataset.LABEL2INDEX, DocumentSentimentDataset.INDEX2LABEL\nprint(w2i)\nprint(i2w)","f9c7a480":"text = 'Woi dasar kau anjing babi antek komunis'\nsubwords = tokenizer.encode(text)\nsubwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n\nlogits = model(subwords)[0]\nlabel = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n\nprint(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')","63db3cfa":"text = 'Budi pergi ke pondok indah mall membeli cakwe'\nsubwords = tokenizer.encode(text)\nsubwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n\nlogits = model(subwords)[0]\nlabel = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n\nprint(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')","1b00d33a":"optimizer = optim.Adam(model.parameters(), lr=3e-6)\nmodel = model.cuda()","e47b9858":"# Train\nn_epochs = 10\nfor epoch in range(n_epochs):\n    model.train()\n    torch.set_grad_enabled(True)\n \n    total_train_loss = 0\n    list_hyp, list_label = [], []\n\n    train_pbar = tqdm(train_loader, leave=True, total=len(train_loader))\n    for i, batch_data in enumerate(train_pbar):\n        # Forward model\n        loss, batch_hyp, batch_label = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n\n        # Update model\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        tr_loss = loss.item()\n        total_train_loss = total_train_loss + tr_loss\n\n        # Calculate metrics\n        list_hyp += batch_hyp\n        list_label += batch_label\n\n        train_pbar.set_description(\"(Epoch {}) TRAIN LOSS:{:.4f} LR:{:.8f}\".format((epoch+1),\n            total_train_loss\/(i+1), get_lr(optimizer)))\n\n    # Calculate train metric\n    metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n    print(\"(Epoch {}) TRAIN LOSS:{:.4f} {} LR:{:.8f}\".format((epoch+1),\n        total_train_loss\/(i+1), metrics_to_string(metrics), get_lr(optimizer)))\n\n    # Evaluate on validation\n    model.eval()\n    torch.set_grad_enabled(False)\n    \n    total_loss, total_correct, total_labels = 0, 0, 0\n    list_hyp, list_label = [], []\n\n    pbar = tqdm(valid_loader, leave=True, total=len(valid_loader))\n    for i, batch_data in enumerate(pbar):\n        batch_seq = batch_data[-1]        \n        loss, batch_hyp, batch_label = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n        \n        # Calculate total loss\n        valid_loss = loss.item()\n        total_loss = total_loss + valid_loss\n\n        # Calculate evaluation metrics\n        list_hyp += batch_hyp\n        list_label += batch_label\n        metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n\n        pbar.set_description(\"VALID LOSS:{:.4f} {}\".format(total_loss\/(i+1), metrics_to_string(metrics)))\n        \n    metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n    print(\"(Epoch {}) VALID LOSS:{:.4f} {}\".format((epoch+1),\n        total_loss\/(i+1), metrics_to_string(metrics)))","1df6ced5":"correct, incorrect = 0, 0\nfor i in range(10):\n    print('='*40)\n#     print()\n    single_row = data.sample()\n    test_text = single_row['text'].values[0]\n    test_res = single_row['hs_class'].values[0]\n    print(f'test sentence:\\n{test_text}\\n')\n    print(f'actual class : {test_res}')\n\n    text = test_text\n\n    subwords = tokenizer.encode(text)\n    subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n\n    logits = model(subwords)[0]\n    labels = [torch.topk(logit, k=1, dim=-1)[1].squeeze().item() for logit in logits]\n    \n    for i, label in enumerate(labels):\n        print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')\n        if(i2w[label]==test_res):\n            print(\"> CORRECT PREDICTION!\\n\")\n            correct+=1\n        else:\n            print(\"> INCORRECT PREDICTION!\\n\")\n            incorrect+=1\nprint(f'\\n\\ncorrect result: {correct}\/10')\nprint(f'incorect result: {incorrect}\/10')","a7af9e05":"def return_prediction_result(text):\n    subwords = tokenizer.encode(text)\n    subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n\n    logits = model(subwords)[0]\n    labels = [torch.topk(logit, k=1, dim=-1)[1].squeeze().item() for logit in logits]\n\n    for i, label in enumerate(labels):\n        return i2w[label]","c29fd9f7":"test_data = pd.read_csv(test_dataset_path)\ntotal_len, correct = len(test_data.values), 0\nfor row in test_data.values:\n    actual = row[0]\n    text = row[1]\n    pred = (return_prediction_result(text))\n    if actual == pred:\n        correct+=1\nprint(f'Score based on test data: {correct}\/{total_len}  Percentage:{round(correct\/total_len * 100,2)}%')","f756908f":"torch.save(model, \"model.pkl\")\nloaded_model = torch.load(\".\/model.pkl\")\nsingle_row = data.sample()\ntest_text = single_row['text'].values[0]\ntest_res = single_row['hs_class'].values[0]\nprint(f'test sentence:\\n{test_text}\\n')\nprint(f'actual class : {test_res}')\n\ntext = test_text\n\nsubwords = tokenizer.encode(text)\nsubwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n\nlogits = loaded_model(subwords)[0]\nlabels = [torch.topk(logit, k=1, dim=-1)[1].squeeze().item() for logit in logits]\n\nfor i, label in enumerate(labels):\n    print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')","eb44492b":"# Model class section (Using IndoBert SmSA - Sentence-level Sentiment Analysis)\n---\n1. Requirement Imports","2fae2fb4":"5. Methods Include:\n    - get_lr\n    - count_param\n    - metrics_to_string\n    - set_seed","8398445b":"# Fine Tuning\n---","559ee219":"## Test fine-tuned model on sample sentences","dd8ef405":"# Lib import\n---","eddcbdc9":"2. Include:\n    - DocumentSentimentDataset\n    - DocumentSentimentDataLoader","fad4510d":"# Pretrained Model Load\n---","b0e9a946":"3. Include:\n    - forward_sequence_classification","f1590953":"# Saving Model to PKL \n---","bee7d3f4":"4. Metrics Include :\n    - document_sentiment_metrics_fn","29bde0de":"# Data Load\n---","e3597333":"# Testing single row data without training\n---"}}