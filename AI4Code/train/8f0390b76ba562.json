{"cell_type":{"5762e276":"code","8140d82d":"code","c2859e4a":"code","3919e641":"code","2da1d5d0":"code","93a76999":"code","3f4c9e05":"code","7d2ca710":"code","7609653c":"code","0dc7e402":"code","24a3445f":"code","3ad42a6e":"code","b03571de":"markdown","a6373a26":"markdown","1dfa2ef8":"markdown","80b53940":"markdown","3cf035cd":"markdown","be3411c3":"markdown","0279effd":"markdown","80e29468":"markdown","2b67fe26":"markdown","2083d3a4":"markdown","a1ac3f98":"markdown","8a9f5fc3":"markdown"},"source":{"5762e276":"# !pip install umap-learn\n## requires internet connection\n\n## UMAP is typically faster and can be better than PCA or even tsne for dim reduciton ;  https:\/\/umap-learn.readthedocs.io\/en\/latest\/","8140d82d":"import pandas as pd\nimport numpy as np\nimport gensim\nfrom matplotlib import pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\n%matplotlib inline","c2859e4a":"orders = pd.concat([pd.read_csv(\"..\/input\/order_products__train.csv\"),pd.read_csv(\"..\/input\/order_products__prior.csv\")])\nprint(\"orders\",orders.shape)\nproducts = pd.read_csv(\"..\/input\/products.csv\").set_index('product_id')\nprint(\"products\",products.shape)","3919e641":"orders[\"product_id\"] = orders[\"product_id\"].astype(str)\norders.head()","2da1d5d0":"# train_products = train_orders.groupby(\"order_id\").apply(lambda order: order['product_id'].tolist())\n# prior_products = prior_orders.groupby(\"order_id\").apply(lambda order: order['product_id'].tolist())\n\n# new \nsentences = orders.groupby(\"order_id\").apply(lambda order: order['product_id'].tolist())\n# print(sentences.shape)\n# sentences.head()","93a76999":"# sentences = prior_products.append(train_products)\nlongest = np.max(sentences.apply(len))\nprint(\"longest len\",longest)\nprint(\"mean length\",np.mean(sentences.apply(len)))\nsentences = sentences.values","3f4c9e05":"model = gensim.models.Word2Vec(sentences, size=60, window=longest, min_count=4, workers=4)","7d2ca710":"vocab = list(model.wv.vocab.keys())","7609653c":"pca = PCA(n_components=2) # ORIG\n# pca = PCA(n_components=50)\n\n# pca = TSNE(n_components=2)\npca.fit(model.wv.syn0)","0dc7e402":"def get_batch(vocab, model, n_batches=4):\n    output = list()\n    for i in range(0, n_batches):\n        rand_int = np.random.randint(len(vocab), size=1)[0]\n        suggestions = model.most_similar(positive=[vocab[rand_int]], topn=5)\n        suggest = list()\n        for i in suggestions:\n            suggest.append(i[0])\n        output += suggest\n        output.append(vocab[rand_int])\n    return output\n\ndef plot_with_labels(low_dim_embs, labels, filename='tsne.png'):\n    \"\"\"From Tensorflow's tutorial.\"\"\"\n    assert low_dim_embs.shape[0] >= len(labels), \"More labels than embeddings\"\n    plt.figure(figsize=(18, 18))  #in inches\n    for i, label in enumerate(labels):\n        x, y = low_dim_embs[i,:]\n        plt.scatter(x, y)\n        plt.annotate(label,\n                     xy=(x, y),\n                     xytext=(5, 2),\n                     textcoords='offset points',\n                     ha='right',\n                     va='bottom')\n#     plt.savefig(filename)\n    plt.show()","24a3445f":"embeds = []\nlabels = []\nfor item in get_batch(vocab, model, n_batches=4):\n    embeds.append(model[item])\n    labels.append(products.loc[int(item)]['product_name'])\nembeds = np.array(embeds)\nembeds = pca.fit_transform(embeds)\nplot_with_labels(embeds, labels)\n","3ad42a6e":"model.save(\"product2vec.model\")","b03571de":"### Organize data for visualization","a6373a26":"### Extract the ordered products in each order","1dfa2ef8":"### Turn the product ID to a string\n#### This is necessary because Gensim's Word2Vec expects sentences, so we have to resort to this dirty workaround","80b53940":"### Load the Data","3cf035cd":"### Some helpers for visualization","be3411c3":"### Train Word2Vec model\n#### I have modified the window size to be equal to the longest order in our dataset. I've explained why in a blog post that is further explaining this kernel in details\nhttp:\/\/omarito.me\/word2vec-product-recommendations\/\n\n* We could\/should also consider setting sentenes not just aacording to ORDERS (i.e baskets), but also by USERS","0279effd":"### PCA\/lower dimensional embed transform the vectors into 2d\n\n* Could use with multiple steps, e.g. pca and tsne (however, this is wasteful since tsne will refit anyway). \n* Could try (also) with umap. \n* another clustering pipeline example https:\/\/gist.github.com\/stes\/92db6023aa3dab5d13e49ece198102c7\n\n* https:\/\/github.com\/lmcinnes\/umap","80e29468":"### Create the final sentences","2b67fe26":"### Load the needed libraries","2083d3a4":"# Word2Vec on Instacart products\n### The goal of this kernel is to try a Word2Vec model on the data of product orders\n### The orders can act as sentences and product ids can act as words, in this kernel we will see if the model will learn any useful information about the products from the order history of all users, maybe in the future this can be used as input to a classifier that recommends products.\n\n* Original author's kernel's blog post: http:\/\/omarito.me\/word2vec-product-recommendations\/","a1ac3f98":"### Visualize a random sample","8a9f5fc3":"### Save the model"}}