{"cell_type":{"6f3594ce":"code","09ea7ded":"code","49ef7510":"code","25cf3519":"code","e7e6df5b":"code","6b116420":"code","69105f18":"code","02705961":"code","6eaeb3a3":"code","9d6abde8":"code","716fc5c5":"markdown","3f809591":"markdown","f8248fac":"markdown","11f34818":"markdown","e29f3795":"markdown","aed7c8b1":"markdown","5224e979":"markdown","be538849":"markdown","544c22e4":"markdown","93d6d1d1":"markdown","fe29e45a":"markdown","f8546631":"markdown","3c0b7cbc":"markdown","2c22d59d":"markdown","5a6939b0":"markdown","c9b41f53":"markdown","2e03e3dd":"markdown"},"source":{"6f3594ce":"# This codes are taken from datai team\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n# import warnings\nimport warnings\n# filter warnings\nwarnings.filterwarnings('ignore')\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n# Any results you write to the current directory are saved as output.\n# load data set\nx_l = np.load('..\/input\/sign-language-digits-dataset\/X.npy')\nY_l = np.load('..\/input\/sign-language-digits-dataset\/Y.npy')\nimg_size = 64\nplt.subplot(1, 2, 1)\nplt.imshow(x_l[260].reshape(img_size, img_size))\nplt.axis('off')\nplt.subplot(1, 2, 2)\nplt.imshow(x_l[900].reshape(img_size, img_size))\nplt.axis('off')\n# Then lets create x_train, y_train, x_test, y_test arrays\n# Join a sequence of arrays along an row axis.\nX = np.concatenate((x_l[204:409], x_l[822:1027] ), axis=0) # from 0 to 204 is zero sign and from 205 to 410 is one sign \nz = np.zeros(205)\no = np.ones(205)\nY = np.concatenate((z, o), axis=0).reshape(X.shape[0],1)\nprint(\"X shape: \" , X.shape)\nprint(\"Y shape: \" , Y.shape)\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15, random_state=42)\nnumber_of_train = X_train.shape[0]\nnumber_of_test = X_test.shape[0]\nX_train_flatten = X_train.reshape(number_of_train,X_train.shape[1]*X_train.shape[2])\nX_test_flatten = X_test .reshape(number_of_test,X_test.shape[1]*X_test.shape[2])\nprint(\"X train flatten\",X_train_flatten.shape)\nprint(\"X test flatten\",X_test_flatten.shape)\nx_train = X_train_flatten.T\nx_test = X_test_flatten.T\ny_train = Y_train.T\ny_test = Y_test.T\nprint(\"x train: \",x_train.shape)\nprint(\"x test: \",x_test.shape)\nprint(\"y train: \",y_train.shape)\nprint(\"y test: \",y_test.shape)","09ea7ded":"def function_name(first_parameter, second_parameter):\n    sumation = first_parameter + second_parameter\n    return sumation\nsumation_result = function_name(4,5) # 4 + 5 = 9\nprint(\"sumation result is \" + str(sumation_result))","49ef7510":"def parameter_initialization(size):\n    w = np.full((size,1),0.02)\n    print(w.shape)\n    b = 0.0\n    return w,b\nw,b = parameter_initialization(4096)","25cf3519":"\ndef sigmoid(z):\n    y_head = 1\/(np.exp(-z)+1)\n    return y_head\n#y_head = sigmoid(3)\n#print(y_head) # for example as z = 3 result is ~0.952574; as z = 0 result is 0.5","e7e6df5b":"def forward_propagation(w,b,x_train,y_train):\n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))\/ x_train.shape[1]\n    return cost, y_head","6b116420":"def backward_propagation(w,b,x_train,y_train):\n    #forward propagation and return result cost and y_head\n    #we use y_head in derivative of bias and weights.\n    #we return cost and gradients, actually it is dictionary as data type, to update weight and bias\n\n    cost, y_head = forward_propagation(w,b,x_train,y_train)\n    sample_size = x_train.shape[1]\n    weights_derivative = (np.dot(x_train,((y_head-y_train).T)))\/sample_size\n    bias_derivative = np.sum(y_head-y_train)\/sample_size\n    gradients = {\"weights_derivative\": weights_derivative, \"bias_derivative\":bias_derivative}\n    return cost, gradients","69105f18":"def update_weights_and_bias(w,b,x_train,y_train,learning_rate,number_of_iteration):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    for i in range(number_of_iteration):\n        #backward and forward propagation\n        cost,gradients = backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        w = w - learning_rate*gradients[\"weights_derivative\"]\n        b = b - learning_rate*gradients[\"bias_derivative\"]\n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print(\"Cost after iteration %i: %f\" %(i,cost))\n        #update weights and bias\n    parameters_dictionary = {\"weights\":w, \"bias\" : b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation = 'vertical')\n    plt.xlabel(\"Number of Iteration\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters_dictionary, gradients, cost_list","02705961":"def prediction(w,b,x_test):\n    z = np.dot(w.T,x_test) + b\n    y_head = sigmoid(z)\n    #for allacotion to make code execution faster. \n    #Time complexity is constant 0.\n    #If we implement list, time complexity become n, not constant. \n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    for i in range(y_head.shape[1]):\n        if y_head[0,i] <= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n    return Y_prediction","6eaeb3a3":"def logistic_regression(x_train,y_train,x_test,y_test,learning_rate,number_of_iteration):\n    size = x_train.shape[0]\n    w,b = parameter_initialization(size)\n    parameters,gradients,cost_list = update_weights_and_bias(w,b,x_train,y_train,learning_rate,number_of_iteration)\n    y_prediction_test = prediction(parameters[\"weights\"],parameters[\"bias\"],x_test)\n    y_prediction_train = prediction(parameters[\"weights\"],parameters[\"bias\"],x_train)\n    \n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\nlogistic_regression(x_train, y_train, x_test, y_test,learning_rate = 0.02,number_of_iteration = 150)","9d6abde8":"from sklearn import linear_model as lm\nlogistic_regression = lm.LogisticRegression(random_state = 42, max_iter = 150)\nprint(\"train accuracy: {} %\".format(logistic_regression.fit(x_train.T,y_train.T).score(x_test.T,y_test.T)))\nprint(\"test accuracy: {} %\".format(logistic_regression.fit(x_train.T,y_train.T).score(x_train.T,y_train.T)))\n\n","716fc5c5":"# Weights and Bias Initialization","3f809591":"> Note: We can also use Sklearn Library to do what we did above","f8248fac":"# Update Weights and bias\n* To update weights, we use w:= w - learning_rate * weights_derivative\n* To update bias, we use b:= b - learning_rate * bias_derivative","11f34818":"# **Sign Language Dataset Train and Test**\n* Explanation of first code snippet is : [https:\/\/www.kaggle.com\/muhammeddalkran\/take-sign-dataset](https:\/\/www.kaggle.com\/muhammeddalkran\/take-sign-dataset)","e29f3795":"# **Parameter Initialization (Weight and Bais)**","aed7c8b1":"# How to Write Functions in Python\n* We first use defined word for defining function is \"def\"\n* We write function name, it can be anyting; however when calling it, we have to use same name.\n* We write our parameters,unlike java, python does not include type defining(Ex.: in java :  string name, int age; in python : name, age   )\n* Then, we write what method\/function should do or fulfill.\n* Lastly, If we want to use result or return some thing we use return word and write what variable is returened.  \n* Otherwise, our method is void method and it cannot return any value or variable. ****","5224e979":"# Start Testing\n> We implemented everything that we need. Now, we are ready to train our data. ","be538849":"# Backward Propagation Initialization","544c22e4":"# Forward Propagation Initialization","93d6d1d1":"# Prediction","fe29e45a":"> Note :Assumption We have 348 images and images' size is 64x64 \n\n* First we have to know how to write functions or methods in Python","f8546631":"* Firstly, we find our z result by using z = (w.T)x + b equation, namely z = np.dot(w.T,x_train) + b.\n* Then, we evaluate our z result by using sigmoid funtion which is  sigmaod(z) = 1\/(1+ e^-z).\n* Lastly, we calculate loss function which is -(1-y)log(1-y_head) + ylog(y_head) and cost function which is summation of loss function results.","3c0b7cbc":"# **Sklearn Library**\n* Library provides coders with opportunity to implement Logistic Regression\n* To use this library, you can check : [https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html)\n","2c22d59d":"* \"size\" is dimension of your image or data. Our image size is 64*64 = 4096\n* Shape of our weight array is (4096, 1) and we full it with 0.02 as default.\n* b is our bias and it is just variable and its value is zero as default.","5a6939b0":"# Forward Propagation function","c9b41f53":"* In Backward Propagation, we update weights and bias with loss derivative according to weights and bias. \n* For weights, we use (1\/sample_size)* x(y_head - y_train)T\n* For bais, we use (1\/sample_size)* (sum from 1 to sample_size (y_head - y_train))","2e03e3dd":"# Sigmoid Function"}}