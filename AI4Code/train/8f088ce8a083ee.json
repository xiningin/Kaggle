{"cell_type":{"b2d91021":"code","a9a30545":"code","ded8499a":"code","2eedf1f2":"code","4060a9a4":"code","b5a3583b":"code","4126b9b4":"code","5abe45f5":"code","a0a0f379":"code","90cce11d":"code","96320c9c":"code","66e5cc53":"code","da12797a":"code","4164464e":"code","a2277045":"code","0943b95a":"code","603999a5":"code","e8db569e":"code","5ca3bbe6":"code","edcdb06c":"code","b9acc2f7":"code","c3254d4e":"code","06405ecf":"code","84ee9c97":"code","6810ac33":"code","91f1585d":"code","15d2a9e0":"code","66f28f07":"code","87b28e30":"code","9fe67190":"markdown","45d4043f":"markdown","c049c52f":"markdown","8363edf9":"markdown","abeae043":"markdown","ad4ae780":"markdown","18a87953":"markdown","490c434c":"markdown","76f6e4b6":"markdown","b0f7f6b9":"markdown","f2577c59":"markdown","7ad02591":"markdown","7c1a8466":"markdown","765ab047":"markdown","4d36948c":"markdown","f45b364a":"markdown","c28c14fd":"markdown","3d98adaf":"markdown","75bd3b68":"markdown","a0363c0d":"markdown","9a594035":"markdown","6cd5831b":"markdown","01e8fc6d":"markdown","f869c2b4":"markdown","d5880c2e":"markdown"},"source":{"b2d91021":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score, f1_score, accuracy_score, roc_curve, auc, confusion_matrix, recall_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import *\nfrom sklearn.tree import DecisionTreeClassifier\nimport xgboost as xgb\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","a9a30545":"df = pd.read_csv('..\/input\/heart.csv')\ndf.shape","ded8499a":"df.head()","2eedf1f2":"df['target'].value_counts()","4060a9a4":"def histogram(col_name, title, xlabel):\n    fig, ax = plt.subplots()\n    df[col_name].hist(color='#A9C5D3', edgecolor='black', grid=False)\n    ax.set_title(title, fontsize=12)\n    ax.set_xlabel(xlabel, fontsize=12)\n    ax.set_ylabel('Frequency', fontsize=12)","b5a3583b":"histogram('age', 'Patient Age Histogram', 'Age')","4126b9b4":"def cut_quantile(quant_col_name, col_to_cut):\n    quantile_list = [0, 0.25, 0.5, 0.75, 1.0]\n    quantile_labels = ['0-25Q', '25-50Q', '50-75Q', '75-100Q']\n    df[quant_col_name] = pd.qcut(df[col_to_cut], q=quantile_list, labels=quantile_labels)","5abe45f5":"cut_quantile('age_quantile_label', 'age')\ndf.head(5)","a0a0f379":"sns.countplot('age_quantile_label', data=df);\nsns.catplot(x=\"age_quantile_label\", col=\"target\", data=df, kind=\"count\", height=4, aspect=.9);","90cce11d":"histogram('trestbps', 'Patient trestbps', 'trestbps')","96320c9c":"cut_quantile('trestbps_quantile_label', 'trestbps')\ndf.head(5)","66e5cc53":"sns.countplot('trestbps_quantile_label', data=df);\nsns.catplot(x=\"trestbps_quantile_label\", col=\"target\", data=df, kind=\"count\", height=4, aspect=.9);","da12797a":"histogram('chol', 'Patient Cholestrol', 'Cholestrol')","4164464e":"cut_quantile('chol_quantile_label', 'chol')\nsns.countplot('chol_quantile_label', data=df);\nsns.catplot(x=\"chol_quantile_label\", col=\"target\", data=df, kind=\"count\", height=4, aspect=.9);","a2277045":"refined_df = df\ntarget = refined_df['target']\n\nrefined_df = refined_df.drop(['age','chol', 'trestbps', 'target'], axis=1)\nrefined_df = pd.get_dummies(refined_df)\nrefined_df.head(5)","0943b95a":"X_train, X_test, y_train, y_test = train_test_split(refined_df, target, test_size=0.20, random_state=42)","603999a5":"def plot_roc_(false_positive_rate,true_positive_rate,roc_auc):\n    plt.figure(figsize=(5,5))\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(false_positive_rate, true_positive_rate, color='red',label = 'AUC = %0.2f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],linestyle='--')\n    plt.axis('tight')\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()","e8db569e":"models_data = dict()\ndef analyse_model(model, name):\n    y_preds = model.predict(X_test)\n    y_proba = model.predict_proba(X_test)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_proba[:,1])\n    roc_auc = round(auc(false_positive_rate, true_positive_rate), 3)\n    accuracy = round(accuracy_score(y_test, y_preds), 3)\n    f1 = round(f1_score(y_test, y_preds), 3)\n    recall = round(recall_score(y_test, y_preds), 3)\n    plot_roc_(false_positive_rate, true_positive_rate, roc_auc)\n    \n    print(\"Accuracy {}\".format(accuracy))\n    print(\"F1 Score {}\".format(f1))\n    print(\"AUC Score {}\".format(roc_auc))\n    print(\"Recall {}\".format(recall))\n    models_data[name] = [roc_auc, accuracy, f1, recall] ","5ca3bbe6":"logs_regr_model = LogisticRegression(random_state=0, solver='lbfgs', max_iter=1000).fit(X_train, y_train)\nanalyse_model(logs_regr_model, \"Logistic Regression\")","edcdb06c":"parameters = {'C': [0.01, 0.05, 0.1, 0.5, 1]}\nlogs_grid_model = GridSearchCV(logs_regr_model, parameters, scoring='roc_auc')\nlogs_grid_model.fit(X_train, y_train)\nprint(logs_grid_model.best_params_)\nanalyse_model(logs_grid_model, \"Logistic Regression GS\")","b9acc2f7":"dec_tree_model = DecisionTreeClassifier(random_state=0).fit(X_train, y_train)\nanalyse_model(dec_tree_model, \"Decision Tree\")","c3254d4e":"parameters = {'max_depth': [2, 3, 4, 5, 6, 7], 'criterion': ('gini', 'entropy'), \n              'min_samples_split': [2, 3, 4, 5], 'min_samples_leaf': [1, 2, 3, 4]}\ndec_grid_model = GridSearchCV(dec_tree_model, parameters, scoring='roc_auc')\ndec_grid_model.fit(X_train, y_train)\n\nprint(dec_grid_model.best_params_)\nanalyse_model(dec_grid_model, \"Decision Tree GS\")","06405ecf":"rf_model = RandomForestClassifier(random_state=0).fit(X_train, y_train)\nanalyse_model(rf_model, \"Random Forest\")","84ee9c97":"parameters = {'n_estimators': [10, 50, 100], 'max_depth': [2, 3, 4, 5, 6, 7]}\nrf_grid_model = GridSearchCV(rf_model, parameters, scoring='roc_auc').fit(X_train, y_train)\nprint(rf_grid_model.best_params_)\nanalyse_model(rf_grid_model, \"Random Forest GS\")","6810ac33":"gbm_model = GradientBoostingClassifier(random_state=0).fit(X_train, y_train)\nanalyse_model(gbm_model, \"Gradient Boost\")","91f1585d":"parameters = {'n_estimators': [10, 100], 'max_depth': [2, 3, 4, 5, 6, 7]}\ngbm_grid_model = GridSearchCV(gbm_model, parameters, scoring='roc_auc').fit(X_train, y_train)\nprint(gbm_grid_model.best_params_)\nanalyse_model(gbm_grid_model, \"Gradient Boost GS\")","15d2a9e0":"xgb_model = xgb.XGBClassifier(random_state=0).fit(X_train, y_train)\nanalyse_model(xgb_model, \"XGBoost\")","66f28f07":"parameters = {'n_estimators': [10, 100], 'max_depth': [2, 3, 4, 5, 6, 7]}\nxgb_grid_model = GridSearchCV(xgb_model, parameters, scoring='roc_auc').fit(X_train, y_train)\nprint(xgb_grid_model.best_params_)\nprint(xgb_grid_model.best_score_)\nanalyse_model(xgb_grid_model, \"XGBoost GS\")","87b28e30":"score_df = pd.DataFrame.from_dict(models_data, orient='index', columns=['AUC Score', 'Accuracy', 'F1 Score', 'Recall'])\nscore_df.sort_values(by=['AUC Score'], ascending=False)","9fe67190":"### Random Forest Model","45d4043f":"* XGBoost is advanced form of gradient boosting which penalizes more complex models through both LASSO (L1) and Ridge (L2) regularization to prevent overfitting.\n* The algorithm comes with built-in cross-validation method at each iteration, taking away the need to explicitly program this search and to specify the exact number of boosting iterations required in a single run.","c049c52f":"### Logistic Regression Model","8363edf9":"* Here, we choose the subsets of data sequentially not parallely.\n* This technique employs the logic in which the subsequent predictors learn from the mistakes of the previous predictors. \n* Because new predictors are learning from mistakes committed by previous predictors, it takes less time\/iterations to reach close to actual predictions. But we have to choose the stopping criteria carefully or it could lead to overfitting on training data.\n* We first model data with simple models and analyze data for errors. \n* These errors signify data points that are difficult to fit by a simple model. \n* Then for later models, we particularly focus on those hard to fit data to get them right. \n* In the end, we combine all the predictors by giving some weights to each predictor.\n\n**The idea is to use the weak learning method several times to get a succession of hypotheses, each one refocused on the examples that the previous ones found difficult and misclassified.**","abeae043":"### Random Forest Model with Grid Search","ad4ae780":"* Logistic Regression assigns probabilities to the regression score which is calculated as product of coefficients of the variables and the variable value itself\n$$Score = \\hat(w) h(x)$$\n$$ P(y=+1 | w) = \\frac{1}{1 + \\mathrm{e}^{-\\hat(w) h(x)}}$$\n\n**This means for a score with a value of $\\infty$ the probability assigned is 1 and for a score with value of -$\\infty$ the probability assigned is 0**","18a87953":"### See if the target class is unbalanced","490c434c":"### Decision Tree Model","76f6e4b6":"Random Forest Classifier works in following way -\n* It uses number of decision trees called estimators\n* It divides the training data into subsets. These subsets are created using pick-one-replace-one algorithm like below <br>\n> [X1, X2, X3]<br>\n> [X1, X4, X3]<br>\n> [X4, X2, X1]<br>\n* This votes from different trees or estimators are aggregated to produce the final output.\n* Random Forest Classifier has the tendency to cancel overfitting from individual trees","b0f7f6b9":"We will use ROC-AUC plot here because there is not much class imbalance because -\n* There is almost negligible class imbalance (165 vs 138)\n* Precision-Recall curves are generally suited when there is moderate to high class imbalance\n\n> ROC-AUC plots draw a curve between True Positive Rate and False Positive Rate <br>\n**True Positive Rate** = TP\/(TP+FP)<br>\n**False Positive Rate** = FP\/(FP+TN)\n\nWe will use roc_curve from sklearn to derive roc-auc curve. This function in sklearn returns a list of False Positive rates and True Positive Rates against a list of threshold values.","f2577c59":"### ROC-AUC Metric\n**Create a function to draw ROC-AUC plot for the model**","7ad02591":"### Create histogram function to see distribution of various numerical variables","7c1a8466":"### Decision Tree Model with Grid Search","765ab047":"### Ready Train and Test Data\nRemove the variables age, trestbps, chol and target and use get_dummies to transform categorical features to numerical binary features","4d36948c":"### XGBoost Model with Grid Search","f45b364a":"### Load Data","c28c14fd":"### Binning of Continuous Variables\nAs we can see from above histogram, the age is skewed towards right. \n* We can do binning of such variables. \n* Binning, also known as quantization is used for transforming continuous numeric features into discrete ones (categories). These discrete values or numbers can be thought of as categories or bins into which the raw, continuous numeric values are binned or grouped into. \n* We can either do a fixed-width binning which means we decide the bins ourselves for a variable such as age like 0-20, 21-50, 51-70, 70+, or we do adaptive binning.\n* The drawback in using fixed-width binning is that due to us manually deciding the bin ranges, we can end up with irregular bins which are not uniform based on the number of data points or values which fall in each bin. Some of the bins might be densely populated and some of them might be sparsely populated or even empty! Adaptive binning is a safer strategy in these scenarios where we let the data speak for itself! That\u2019s right, we use the data distribution itself to decide our bin ranges.\n* Quantile based binning is a good strategy to use for adaptive binning. \n* Quantiles are specific values or cut-points which help in partitioning the continuous valued distribution of a specific numeric field into discrete contiguous bins or intervals.","3d98adaf":"### Logistic Regression Model with Grid Search","75bd3b68":"### Gradient Boosting Model with Grid Search","a0363c0d":"Similar exercise is done for variables like **'trestbps'** and **'patient cholestrol'**","9a594035":"### AUC Score Comparison of different classifiers","6cd5831b":"### XGBoost Model","01e8fc6d":"**Create a function to print accuracy, roc_auc score, F1 score and recall of the classification model**","f869c2b4":"### Gradient Boosting Model","d5880c2e":"A decision tree is a flowchart-like tree structure where an internal node represents feature(or attribute), the branch represents a decision rule, and each leaf node represents the outcome. The topmost node in a decision tree is known as the root node. It learns to partition on the basis of the attribute value. It partitions the tree in recursively manner call recursive partitioning. This flowchart-like structure helps you in decision making. It's visualization like a flowchart diagram which easily mimics the human level thinking. That is why decision trees are easy to understand and interpret.\n\n![](https:\/\/www.xoriant.com\/blog\/wp-content\/uploads\/2017\/08\/Decision-Trees-modified-1.png)"}}