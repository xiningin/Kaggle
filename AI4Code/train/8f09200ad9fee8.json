{"cell_type":{"35419498":"code","3da653b7":"code","6ab92434":"code","28a235a5":"code","6249e9bc":"code","6d1b6cb7":"code","b623da1f":"code","864a806a":"code","750665b1":"code","1ea4c998":"markdown","84349d95":"markdown","662845d9":"markdown"},"source":{"35419498":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","3da653b7":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.datasets import make_circles\nfrom sklearn import linear_model\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score","6ab92434":"X,Y=make_circles(noise=0.1, factor=0.2, random_state=1)\nX1 = X[:, 0].flatten()\nX2 = X[:, 1].flatten()","28a235a5":"#Standard Scaler\nscaler = StandardScaler()\nXS = scaler.fit_transform(X)\n\n#Polynomial Features\nfrom sklearn.preprocessing import PolynomialFeatures\nPolyFeatures = PolynomialFeatures(2)\n\nX_NEW = PolyFeatures.fit_transform(XS)","6249e9bc":"\nclf = linear_model.SGDClassifier(loss='log', random_state=123)\nclf.fit(X_NEW, Y)\npredict_Y = clf.predict(X_NEW)\n#prob_Y  = clf.predict_proba(X_NEW=X_NEW) \n\nsuccess = 1-sum(abs(predict_Y - Y))\/len(Y)\nprint(\"Hypothesis prediction success rate is %.2f\" %success)\nprint(\"Classifier Score\",clf.score(X_NEW,Y))\nprint(\"Accuracy Score\", accuracy_score(Y,predict_Y))","6d1b6cb7":"#Plotting\n\ncmap = ListedColormap(['blue', 'red'])                    \nplt.scatter(X1,X2, c=predict_Y,marker='.', cmap=cmap)\n#plt.scatter(errors,errors, c=errors, marker='*',s=100,edgecolors=\"black\",facecolors=\"none\")\nplt.show()","b623da1f":"def do_fit_logreg(classifier, X, Y):\n    classifier.fit(X, Y)\n    predict_y1 = classifier.predict(X=X)\n    success = 1-sum(abs(predict_y1 - Y))\/len(Y)\n    print(\"Hypothesis prediction success rate is %.2f\" %success)\n    print(classifier.score(X,Y))\n    print(accuracy_score(Y,predict_y1))\n    \n    cmap = ListedColormap(['blue', 'red'])                    \n    plt.scatter(X1,X2, c=predict_y1,marker='.', cmap=cmap)\n    plt.show()","864a806a":"from sklearn.linear_model import LogisticRegression\nlogregressor = LogisticRegression(solver=\"liblinear\")\ndo_fit_logreg(logregressor, X_NEW, Y) ","750665b1":"from matplotlib.colors import ListedColormap\nmyColorMap = ListedColormap(['blue', 'red'])                    \n\nplt.figure(figsize=(8,6))\n# Set min and max values and give it some padding\nx1_min, x1_max = X[:,0].min() - 1, X[ :,0].max() + 1\nx2_min, x2_max = X[:,1].min() - 1, X[ :,1].max() + 1\n\nxx1 =np.linspace(x1_min, x1_max, 50)\nxx2 =np.linspace(x2_min, x2_max, 50)\n#Plot Prediction Data\nfor i in range(len(xx1)):\n    for j in range(len(xx2)):\n        newX=(np.column_stack((xx1[i],xx2[j])))\n        #newY=predict(newX, weights) \n        \n        \n        newY = clf.predict(PolyFeatures.fit_transform(scaler.transform(newX)))\n        yColor=myColorMap(int(newY))\n        plt.scatter(xx1[i].flatten(),xx2[j].flatten(),color=yColor,alpha=0.4);\n\n#Plot Training data\nplt.scatter(X[:,0].flatten(),X[:,1].flatten(), c=Y.flatten(),  cmap=myColorMap,edgecolor='k');\nplt.show()","1ea4c998":"Plotting Desicion Boundry\n","84349d95":"# SGDClassifier Logistic Classification","662845d9":"# Using Logistic Regression - Classification"}}