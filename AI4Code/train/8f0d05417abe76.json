{"cell_type":{"21020ac9":"code","e9033066":"code","6f6dcb23":"code","4c02267d":"code","93cd7ee9":"code","112cff2b":"code","9def8b24":"code","d324d875":"code","f33112fb":"code","110ecf92":"code","2d1387a5":"code","4dc02544":"code","ce27c24f":"code","2aca77b3":"code","9abd6af1":"code","062d8d2b":"code","1c96aa34":"code","d11904f3":"code","31b6e5b2":"code","4a57c6e4":"code","7b32ac4e":"code","c5c144c9":"code","802518a9":"code","0cf34df1":"code","3806cb7b":"code","67baf827":"code","bdcdb9a0":"code","78d832a5":"code","39c62913":"code","2e6ee677":"code","978e6958":"code","db48b0c4":"code","2b2002ac":"code","62afd1a2":"code","f54ff81d":"code","ba7ccc29":"code","c2171d59":"code","519d99e5":"code","a1f882b3":"code","7f723b29":"code","14aaf827":"code","c2edeaed":"code","0bd30d5e":"code","e34f8728":"code","c47de455":"code","312a1b19":"code","dfe6fc5a":"code","f444e544":"code","2e729cf2":"code","12744bcc":"code","bbf3e479":"code","7652165b":"markdown","6c5e2011":"markdown","ca5bc4fb":"markdown","9feaec03":"markdown","673212e3":"markdown","4d77f468":"markdown","0891731b":"markdown","768d99d0":"markdown","5a99036d":"markdown","e2a76988":"markdown","06ea98b4":"markdown","9ee188ce":"markdown","1249c79d":"markdown","fa66a70f":"markdown","f4a0107e":"markdown","97f4054e":"markdown","6bce978a":"markdown","9ccfb86c":"markdown","7444136f":"markdown","087f711b":"markdown","1fd9a6b6":"markdown","cf9b1e99":"markdown","a123ce7c":"markdown"},"source":{"21020ac9":"# example text for model training\nsimple_train = ['call you tonight', 'Call me a cab', 'please call me.. please']","e9033066":"\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n#  instantiate CountVectorizer (vectorizer)\nvect = CountVectorizer()","6f6dcb23":"# 3. fit\n# learn the 'vocabulary' of the training data (occurs in-place)\nvect.fit(simple_train)","4c02267d":"# examine the fitted vocabulary\nvect.get_feature_names()","93cd7ee9":"# 4. transform training data into a 'document-term matrix','sparse matrix'\nsimple_train_dtm = vect.transform(simple_train)\nprint(simple_train_dtm)","112cff2b":"simple_train_dtm.toarray()","9def8b24":"print('sparse matrix')\nprint(simple_train_dtm)\n\nprint('dense matrix')\nprint(simple_train_dtm.toarray())","d324d875":"import pandas as pd\n# examine the vocabulary and document-term matrix together\n# pd.DataFrame(matrix, columns=columns)\npd.DataFrame(simple_train_dtm.toarray(), columns=vect.get_feature_names())","f33112fb":"#reading data\nimport warnings as wr\nwr.filterwarnings(\"ignore\")\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsms=pd.read_csv(\"..\/input\/sms-spam-collection-dataset\/spam.csv\")\nsms.head()","110ecf92":"sms.shape","2d1387a5":"plt.figure(figsize=(10,6))\nsns.heatmap(sms.isnull(),yticklabels=False,cbar=True,cmap='mako')","4dc02544":"sms.isnull().sum()","ce27c24f":"sms=sms.drop([\"Unnamed: 2\",\"Unnamed: 3\",\"Unnamed: 4\"],axis=1)\nsms.head()","2aca77b3":"# examine the class distribution\nsms.v1.value_counts()","9abd6af1":"\nsns.countplot(sms[\"v1\"])","062d8d2b":"# convert label to a numerical variable\n\nsms['v1'] = sms.v1.map({'ham':0, 'spam':1})","1c96aa34":"sms.head()","d11904f3":"ham=sms[sms[\"v1\"]==0]\nspam=sms[sms[\"v1\"]==1]","31b6e5b2":"ham.shape,spam.shape","4a57c6e4":"ham=ham.sample(spam.shape[0])\nham.shape","7b32ac4e":"data=spam.append(ham,ignore_index=True)\nprint(\"Shape :\",data.shape)\ndata.head()","c5c144c9":"sns.countplot(data[\"v1\"])","802518a9":"X = data.v2\ny = data.v1","0cf34df1":"# split X and y into training and testing sets\n# by default, it splits 75% training and 25% test\n# random_state=1 for reproducibility\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=5)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","3806cb7b":"vect = CountVectorizer()","67baf827":"X_train_dtm = vect.fit_transform(X_train)","bdcdb9a0":"X_train_dtm","78d832a5":"# 4. transform testing data (using fitted vocabulary) into a document-term matrix\nX_test_dtm = vect.transform(X_test)\nX_test_dtm\n\n# you can see that the number of columns, 7456, is the same as what we have learned above in X_train_dtm","39c62913":"# 1. import\nfrom sklearn.naive_bayes import MultinomialNB\n\n# 2. instantiate a Multinomial Naive Bayes model\nnb = MultinomialNB()","2e6ee677":"# 3. train the model \n\nnb.fit(X_train_dtm, y_train)","978e6958":"# 4. make class predictions for X_test_dtm\ny_pred_class = nb.predict(X_test_dtm)","db48b0c4":"# calculate accuracy of class predictions\nfrom sklearn import metrics\nmetrics.accuracy_score(y_test, y_pred_class)","2b2002ac":"\ncf_matrix=metrics.confusion_matrix(y_test, y_pred_class)\ncf_matrix","62afd1a2":"import matplotlib.pyplot as plt\n#Ploting confusion matrix\nplt.figure(figsize=(8,5))\nsns.heatmap(cf_matrix, annot=True, fmt='d')","f54ff81d":"# print message text for the false positives (ham incorrectly classified as spam)\n\nX_test[(y_pred_class==1) & (y_test==0)]","ba7ccc29":"# print message text for the false negatives (spam incorrectly classified as ham)\nX_test[(y_pred_class==0) & (y_test==1)]","c2171d59":"# calculate AUC\ny_pred_prob = nb.predict_proba(X_test_dtm)[:, 1]\nmetrics.roc_auc_score(y_test, y_pred_prob)","519d99e5":"# 1. import\nfrom sklearn.linear_model import LogisticRegression\n\n# 2. instantiate a logistic regression model\nlogreg = LogisticRegression()","a1f882b3":"# 3. train the model using X_train_dtm\nlogreg.fit(X_train_dtm, y_train)","7f723b29":"# 4. make class predictions for X_test_dtm\ny_pred_class = logreg.predict(X_test_dtm)","14aaf827":"# calculate predicted probabilities for X_test_dtm (well calibrated)\ny_pred_prob = logreg.predict_proba(X_test_dtm)[:, 1]\ny_pred_prob","c2edeaed":"# calculate accuracy\nmetrics.accuracy_score(y_test, y_pred_class)","0bd30d5e":"# calculate AUC\nmetrics.roc_auc_score(y_test, y_pred_prob)","e34f8728":"# remove English stop words\nvect1 = CountVectorizer(stop_words='english')\n\nX_train_1 = vect1.fit_transform(X_train)\n\nX_train_1\n","c47de455":"# include 1-grams and 2-grams\n\n# how to differentiate between \"Happy\", \"Not Happy\", \"Very Happy\"\nvect2 = CountVectorizer(ngram_range=(1, 2))\n\nX_train_2 = vect2.fit_transform(X_train)\n\nX_train_2","312a1b19":"# ignore terms that appear in more than 50% of the documents\nvect3 = CountVectorizer(max_df=0.5)\n\nX_train_3 = vect3.fit_transform(X_train)\n\nX_train_3","dfe6fc5a":"# only keep terms that appear in at least 2 documents\nvect4 = CountVectorizer(min_df=2)\n\nX_train_4 = vect4.fit_transform(X_train)\n\nX_train_4","f444e544":"vect_combined= CountVectorizer(stop_words='english',ngram_range=(1, 2),min_df=2,max_df=0.5)","2e729cf2":"X_train_c = vect_combined.fit_transform(X_train)\nX_test_c = vect_combined.transform(X_test)\n\nX_train_c","12744bcc":"# 1. import\nfrom sklearn.naive_bayes import MultinomialNB\n\n# 2. instantiate a Multinomial Naive Bayes model\nnb = MultinomialNB()\n\nnb.fit(X_train_c, y_train)\n\ny_pred_class = nb.predict(X_test_c)\n\nnb_cf_matrix=metrics.confusion_matrix(y_test, y_pred_class)","bbf3e479":"plt.figure(figsize=(8,5))\nsns.heatmap(nb_cf_matrix, annot=True, fmt='d')","7652165b":"Feature description\n* v1=label(ham\/spam) outcome to predict\n* v2=sms","6c5e2011":"**printing the confusion matrix**\nto compare actual and prediction","ca5bc4fb":"**know we are going to use transform function after fitiong the data**\n\n**It will convert vocabulary to number(sparse matrix)**","9feaec03":"**By the above data frame you essaly understand**\n\n**How we convert text data in to number**\n**you can see we have three row in the data frame \nbecouse there are only three sentances in over simple data**\n\n**here you can see all the vocobulary we get from the data, now become a featue**\n","673212e3":"**When we fit CountVectorizer it will a vocabulary of word in a dict**","4d77f468":"# Please do a up vote if you find usefull\n**write your suggestion in the comment**","0891731b":"# MultinomialNB\n**MultinomialNB is the one of the best algo for Text data**","768d99d0":"Here we can say there are many NaN value in numamed so we going to drop those frature","5a99036d":"**Here we going to make a sample data**","e2a76988":"## 1st we know how countvectorizer work\n","06ea98b4":"**import and instantiate CountVectorizer (with the default parameters)**","9ee188ce":"* Now we can see data is balanced","1249c79d":"**dividing data in X & Y**","fa66a70f":"**Know we are going to convert sparse matrix to a dense matrix**","f4a0107e":"# *sms ham & spam classification*","97f4054e":"**Here we got .96 score**","6bce978a":"examine the sparse matrix contents\n* left: coordinates of non-zero values\n* right: values at that point\n\nCountVectorizer() will output a sparse matrix","9ccfb86c":"**calling CountVectorizer**","7444136f":"# LogisticRegression","087f711b":"# Now be going to work on original data ","1fd9a6b6":"### Thanks","cf9b1e99":"**Balance Data**","a123ce7c":"# Model building"}}