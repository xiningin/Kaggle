{"cell_type":{"8b0ee405":"code","c5a14723":"code","bfe41c27":"code","cfce8d8d":"code","6801978c":"code","f970c43e":"code","8bfb7177":"code","1f96da23":"code","2cffd93f":"code","7a1fe195":"code","1e5554e6":"code","ca615fba":"code","a1aa61fe":"code","f0f36986":"code","f7db1127":"code","3d99e69a":"markdown","e39b4cf7":"markdown","116fe63c":"markdown","609a6940":"markdown"},"source":{"8b0ee405":"\nimport gc\nimport pandas as pd\nimport numpy as np\nimport os\nimport torch\nimport torch.nn as nn\nimport transformers\n\nfrom transformers import AdamW, AutoTokenizer, AutoModel\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import DataLoader\nimport time\nfrom tqdm import tqdm\n\nimport nltk\nfrom nltk.stem import SnowballStemmer, WordNetLemmatizer\nimport re\nfrom nltk.corpus import stopwords","c5a14723":"class Config:\n    model_name = '..\/input\/roberta-base'\n    batch_size = 64\n    lr = 1e-4\n    weight_decay = 0.01\n    scheduler = 'CosineAnnealingLR'\n    early_stopping_epochs = 1\n    epochs = 20\n    max_length = 128","bfe41c27":"class ToxicDataset:\n    def __init__(self, comments, tokenizer, max_len=196):\n        self.comments = comments\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        \n    def __len__(self):\n        return len(self.comments)\n    \n    def __getitem__(self, idx):\n        \n        tokenized = self.tokenizer.encode_plus(\n            self.comments[idx],\n            truncation=True,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length'\n        )\n        \n        input_ids = tokenized['input_ids']\n        attention_mask = tokenized['attention_mask']\n        \n\n        return {\n            'input_ids' : torch.tensor(input_ids, dtype=torch.long),\n            'attention_mask' : torch.tensor(attention_mask, dtype=torch.long)\n        }","cfce8d8d":"class ToxicModel(nn.Module):\n    def __init__(self, args):\n        super(ToxicModel, self).__init__()\n        self.args = args\n        self.model = AutoModel.from_pretrained(self.args.model_name)\n        self.dropout = nn.Dropout(p=0.2)\n        self.toxic = nn.Linear(768, 1)\n        self.stoxic = nn.Linear(768, 1)\n        self.obs = nn.Linear(768, 1)\n        self.threat = nn.Linear(768, 1)\n        self.insult = nn.Linear(768, 1)\n        self.id_hate = nn.Linear(768, 1)\n    \n        \n    def forward(self, input_ids, attention_mask):\n        \n        out = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            output_hidden_states=False\n        )\n        \n        out = self.dropout(out[1])\n        \n        toxic = self.toxic(out)\n        stoxic = self.stoxic(out)\n        obs = self.obs(out)\n        threat = self.threat(out)\n        insult = self.insult(out)\n        id_hate = self.id_hate(out)\n\n        return torch.cat([toxic, stoxic, obs, threat, insult, id_hate], dim=-1)\n        ","6801978c":"def get_predictions(args, dataloader, model):\n    model.eval()\n    all_outputs=[]\n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    with torch.no_grad():\n        for step, data in bar:\n            batch_size = args.batch_size\n\n            input_ids = data['input_ids'].cuda()\n            attention_mask = data['attention_mask'].cuda()\n            outputs = model(input_ids, attention_mask)\n            outputs = outputs.cpu().detach().numpy()\n            outputs = [sum(output) for output in outputs]\n            all_outputs.append(outputs)\n\n            bar.set_postfix(Stage='Inference') \n    return np.hstack(all_outputs)","f970c43e":"def washing_machine(comments):\n    corpus=[]\n    for i in tqdm(range(len(comments))):\n        comment = re.sub('[^a-zA-Z]', ' ', comments[i])\n        comment = comment.lower()\n        comment = comment.split()\n        stemmer = SnowballStemmer('english')\n        lemmatizer = WordNetLemmatizer()\n        all_stopwords = stopwords.words('english')\n        comment = [stemmer.stem(word) for word in comment if not word in set(all_stopwords)]\n        comment = [lemmatizer.lemmatize(word) for word in comment]\n        comment = ' '.join(comment)\n        corpus.append(comment)\n\n    return corpus","8bfb7177":"def inference(data):\n    args=Config()\n    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n    base_path= ['..\/input\/2-folds-test\/', '..\/input\/robbaseuc\/']\n    \n    dataset = ToxicDataset(data, tokenizer)\n    dataloader = DataLoader(dataset, batch_size=16*args.batch_size)\n    \n    final_preds = []\n    \n    \n    ### 2 folds test ###\n    num_folds = 5\n    \n    for fold in range(num_folds):\n        model = ToxicModel(args)\n        model = model.cuda()\n        path = base_path[0] + f'model_fold_{fold}.bin'\n        model.load_state_dict(torch.load(path))\n        \n        print(f\"Getting predictions for model {fold+1}\")\n        preds = get_predictions(args, dataloader, model)\n        final_preds.append(np.vstack(preds))\n        del model\n        gc.collect()\n        \n    ### roberta base un cleaned ###\n    num_folds = 2\n    for fold in range(num_folds):\n        model = ToxicModel(args)\n        model = model.cuda()\n        path = base_path[1] + f'model_fold_{fold}.bin'\n        model.load_state_dict(torch.load(path))\n        \n        print(f\"Getting predictions for model {fold+1}\")\n        preds = get_predictions(args, dataloader, model)\n        final_preds.append(np.vstack(preds))\n        del model\n        gc.collect()\n        \n    return np.hstack(sum(final_preds)\/7)","1f96da23":"def show_validation():\n\n    df = pd.read_csv('..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv')\n#     com1 = washing_machine(df['less_toxic'].values)\n#     com2 = washing_machine(df['more_toxic'].values)\n    com1 = df['less_toxic'].values\n    com2 = df['more_toxic'].values\n    pred1 = inference(com1)\n    pred2 = inference(com2)\n    score=[]\n    for o1,o2 in zip(pred1, pred2):\n        if o1<o2:\n            score.append(1)\n        else:\n            score.append(0)\n\n    mean_score = np.mean(score)\n    print('-'*50)\n    print('Validation Score :',mean_score)","2cffd93f":"# show_validation()","7a1fe195":"df = pd.read_csv('..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv')","1e5554e6":"df.head()","ca615fba":"comments = washing_machine(df['text'].values)","a1aa61fe":"pred = inference(comments)","f0f36986":"df['score'] = pred","f7db1127":"df[['comment_id', 'score']].to_csv('submission.csv', index=False)","3d99e69a":"# Validation","e39b4cf7":"# Inference","116fe63c":"# Inference kernel\n\n### This is the Inference kernel to : [Toxic Trainer | FIT | Multi Label :)](https:\/\/www.kaggle.com\/kishalmandal\/toxic-trainer-fit-multi-label\/edit\/run\/79270347)","609a6940":"# Prediction and submission"}}