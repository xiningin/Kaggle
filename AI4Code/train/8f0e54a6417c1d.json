{"cell_type":{"47c40a14":"code","d5e39c7a":"code","b1407e56":"code","e7d5ca65":"code","31dc6b65":"code","fca37fb3":"code","f46f5bd6":"code","0400c007":"markdown","856ed620":"markdown"},"source":{"47c40a14":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tqdm.notebook as tqdm\n\n%matplotlib inline","d5e39c7a":"df_train = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-may-2021\/train.csv\").drop(columns='id')\ndf_test = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-may-2021\/test.csv\").drop(columns='id')","b1407e56":"from xgboost import XGBClassifier, plot_importance\nfrom sklearn.metrics import classification_report as cr, confusion_matrix as cm\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn import preprocessing","e7d5ca65":"x = df_train.drop(columns='target')\nle = preprocessing.LabelEncoder().fit(df_train.target)\ny = le.transform(df_train.target)\n\nx_train, x_val, y_train, y_val = train_test_split(x, y, stratify=y, shuffle=True, random_state=0)\n\nx_train, y_train = SMOTE().fit_resample(x_train, y_train)\nx_val, y_val = SMOTE().fit_resample(x_val, y_val)\n\nmodel = XGBClassifier(eval_metric='mlogloss', use_label_encoder=False, random_state=0).fit(x_train, y_train) #, sample_weight=sample_weight)\n\nprint(\"Train:\")\ny_pred = model.predict_proba(x_train)\nprint(cm(y_true=y_train, y_pred=y_pred.argmax(axis=1)))\nprint(cr(y_true=y_train, y_pred=y_pred.argmax(axis=1)))\nprint(log_loss(y_pred=y_pred, y_true=y_train, labels=np.unique(y_train)))\n\nprint(\"Val:\")\ny_pred = model.predict_proba(x_val)\nprint(cm(y_true=y_val, y_pred=y_pred.argmax(axis=1)))\nprint(cr(y_true=y_val, y_pred=y_pred.argmax(axis=1)))\nprint(log_loss(y_pred=y_pred, y_true=y_val, labels=np.unique(y_val)))\n\n_, ax = plt.subplots(1, 1, figsize=(18, 18))\nplot_importance(model, ax=ax)\nplt.title('Feature Importance')\nplt.show()","31dc6b65":"N = 100\na_min = np.linspace(0.0, 0.5, num=N)\na_max = np.linspace(0.5, 1.0, num=N)\n\nz = np.array([[log_loss(y_pred=np.clip(y_pred, a_min=i, a_max=j), y_true=y_val, labels=np.unique(y_val)) for i in a_min] for j in a_max])\n\nprint(z.min())\ni, j = np.unravel_index(z.argmin(), z.shape)\na_min, a_max = a_min[i], a_max[j]\nprint(a_min, a_max)\n\n# The a_max cutoff is pretty low here.\na_max = 1 - a_min","fca37fb3":"# Retrain on all of the data\nx, y = SMOTE().fit_resample(x, y)\nmodel = XGBClassifier(eval_metric='mlogloss', use_label_encoder=False, random_state=0).fit(x, y)","f46f5bd6":"df_test = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-may-2021\/test.csv\")\ny_pred = model.predict_proba(df_test.drop(columns='id'))\n# y_pred = np.clip(y_pred, a_min, a_max)\n\nsubmission = pd.DataFrame(y_pred, columns=le.classes_)\nsubmission = submission[['Class_1','Class_2','Class_3','Class_4']]\n\nsubmission['id'] = df_test['id']\nsubmission.to_csv('.\/submission.csv', index=False)\nassert len(submission) == 50000","0400c007":"# XGBoost\n\nI'm just going to assume that the test data distribution is balanced","856ed620":"# Load Data"}}