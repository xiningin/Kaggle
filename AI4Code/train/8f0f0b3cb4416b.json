{"cell_type":{"e7164677":"code","0ea04baf":"code","24b17a15":"code","2741c25e":"code","823ddcd8":"code","e5f067d5":"code","9d2f01c3":"code","28ed0f55":"code","6933c4f8":"code","56869041":"code","1d03c2c2":"code","a8774a33":"code","df3768d9":"code","4ca9844b":"code","b43ae78e":"code","3cc27863":"markdown","360d1306":"markdown","dfce2063":"markdown","8c2e004d":"markdown","4435971d":"markdown"},"source":{"e7164677":"import numpy as np\nimport pandas as pd","0ea04baf":"from google.colab import drive\ndrive.mount('\/content\/drive')","24b17a15":"data_set = pd.read_excel(\"drive\/MyDrive\/data sets\/covid.xlsx\")\ndata_set.drop(labels = [\"#\" ,\"age\"] ,axis = 1,inplace = True)\ndata_set.head()","2741c25e":"def encode (x):\n  if (x == \"yes\"):\n    return 1\n\n  return 0\n","823ddcd8":"columns = data_set.columns\nfor col in columns:\n  data_set[col] = data_set[col].apply(encode)\n\ndata_set.head()","e5f067d5":"data_set.info()","9d2f01c3":"data_set.drop_duplicates(inplace=True)\n\nnp.random.seed(1)\nrandom_numbers = np.random.randint(0, 2, size = (500,21))\n\ntemp_df = pd.DataFrame(random_numbers , columns = columns)\ndata_set [\"covid\"] = 1\ntemp_df [\"covid\"] = 0\ndf = pd.concat([data_set,temp_df] , axis=0)\ndf.drop_duplicates(subset=columns , keep=\"first\",inplace=True)\n","28ed0f55":"from sklearn.model_selection import train_test_split\nX = df[columns]\ny = df[\"covid\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1)\n","6933c4f8":"from sklearn.tree import DecisionTreeClassifier\n\n\nd_tree = DecisionTreeClassifier(random_state=1,criterion = 'gini' , splitter = \"random\")\nd_tree.fit(X_train,y_train)\n\nd_tree_score = d_tree.score(X_test,y_test)\n\nx = zip(columns , d_tree.feature_importances_)\nx = list(x)\nx.sort(key=lambda tup: tup[1])\n\nprint(\"deghate model random DecisionTreeClassifier : \",d_tree_score)\nprint(\"5 feature ba kamtarin ahamiat:\")\nx[0:6]","56869041":"from sklearn.tree import DecisionTreeClassifier\n\n\nd_tree2 = DecisionTreeClassifier(random_state=1,criterion = 'entropy' , splitter = \"best\")\nd_tree2.fit(X_train,y_train)\n\nd_tree2_score = d_tree2.score(X_test,y_test)\n\nx = zip(columns , d_tree2.feature_importances_)\nx = list(x)\n\nprint(\"deghate model random DecisionTreeClassifier ba id3: \",d_tree2_score)\n","1d03c2c2":"d = np.array(X_train,dtype=str)[:,:-1]\ntarget = np.array(y_train,dtype=str)\n\n# traing function to implement Find-s algorithm\ndef find_s(c,t):\n    for i, val in enumerate(t):\n        if val == \"1\":\n            specific_hypothesis = c[i].copy()\n            break\n             \n    for i, val in enumerate(c):\n        if t[i] == \"1\":\n            for x in range(len(specific_hypothesis)):\n                if val[x] != specific_hypothesis[x]:\n                    specific_hypothesis[x] = \"?\"\n                else:\n                    pass\n                 \n    return specific_hypothesis\n\n\nprint(\"The final hypothesis is:\",find_s(d,target))","a8774a33":"d = np.array(X_train)[:,:-1]\ntarget = np.array(y_train)\n\n#Candidate Elimination algorithm\ndef candidate_elimination(concepts, target):\n    specific_h = concepts[0].copy()\n    print(\"Initialization of specific_h and general_h\")\n    print(\"specific_h: \",specific_h)\n    general_h = [[-1 for i in range(len(specific_h))] for i in range(len(specific_h))]\n    print(\"general_h: \",general_h)\n    print(\"concepts: \",concepts)\n    for i, h in enumerate(concepts):\n        if target[i] == 1:\n            for x in range(len(specific_h)):\n                if h[x] != specific_h[x]:\n                    specific_h[x] = -1\n                    general_h[x][x] = -1\n        if target[i] == 0:\n            for x in range(len(specific_h)):\n                if h[x] != specific_h[x]:\n                    general_h[x][x] = specific_h[x]\n                else:\n                    general_h[x][x] = -1\n    print(\"\\nSteps of Candidate Elimination Algorithm: \",i+1)\n    print(\"Specific_h: \",i+1)\n    print(specific_h,\"\\n\")\n    print(\"general_h :\", i+1)\n    print(general_h)\n    indices = [i for i, val in enumerate(general_h) if val == [-1, -1, -1, -1, -1, -1]]\n    print(\"\\nIndices\",indices)\n    for i in indices:\n        general_h.remove([-1, -1, -1, -1, -1, -1])\n    return specific_h, general_h\n\ns_final,g_final = candidate_elimination(d, target)\nprint(\"\\nFinal Specific_h:\", s_final, sep=\"\\n\")\nprint(\"Final General_h:\", g_final, sep=\"\\n\")","df3768d9":"from sklearn.naive_bayes import CategoricalNB\n\nnb = CategoricalNB()\nnb.fit(X_train,y_train)\n\nx = nb.score(X_test,y_test)\nprint(\"deghate model  Categorical naive bayes : \",x)","4ca9844b":"from sklearn.neighbors import KNeighborsClassifier\n\nneigh = KNeighborsClassifier(n_neighbors=3)\nneigh.fit(X_train,y_train)\nx = neigh.score(X_test,y_test)\nprint(\"deghate model KNeighborsClassifiers ba k=3: \",x)","b43ae78e":"from sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=2, random_state=0).fit(X_train)\n\n\ny_true = y_test.to_numpy()\n\nx = kmeans.predict(X_test)\nz = 0\nfor i in range(0,len(X_test)):\n  if x[i] == y_true[i]:\n    z = z+1\nz = z\/len(X_test)\nprint(\"score model KMeans ba k=2: \",z)","3cc27863":"KNeighborsClassifier","360d1306":"bayse","dfce2063":"find _ s","8c2e004d":"ce","4435971d":"k means"}}