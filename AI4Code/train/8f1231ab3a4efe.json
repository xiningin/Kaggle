{"cell_type":{"d4aa6915":"code","dc265008":"code","277eb89f":"code","5805295e":"code","18ed6ff3":"code","90e372a3":"code","6aceb603":"code","a0dc27ee":"code","6c60a3f5":"code","7ebf0035":"code","3dc2a148":"code","a5193b6f":"code","249cdb44":"code","a1da9140":"code","bae709b4":"code","1df4737f":"markdown"},"source":{"d4aa6915":"import numpy as np\nimport tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras.applications import vgg19\n","dc265008":"base_image_path = keras.utils.get_file(\"paris.jpg\", \"https:\/\/i.imgur.com\/F28w3Ac.jpg\")\nstyle_reference_image_path = keras.utils.get_file(\n    \"starry_night.jpg\", \"https:\/\/i.imgur.com\/9ooB60I.jpg\"\n)\nresult_prefix = \"paris_generated\"\n\n# Weights of the different loss components\ntotal_variation_weight = 1e-6\nstyle_weight = 1e-6\ncontent_weight = 2.5e-8\n\n# Dimensions of the generated picture.\nwidth, height = keras.preprocessing.image.load_img(base_image_path).size\nimg_nrows = 400\nimg_ncols = int(width * img_nrows \/ height)","277eb89f":"\nfrom IPython.display import Image, display\n\ndisplay(Image(base_image_path))\ndisplay(Image(style_reference_image_path))","5805295e":"def preprocess_image(image_path):\n    # Util function to open, resize and format pictures into appropriate tensors\n    img = keras.preprocessing.image.load_img(\n        image_path, target_size=(img_nrows, img_ncols)\n    )\n    img = keras.preprocessing.image.img_to_array(img)\n    img = np.expand_dims(img, axis=0)\n    img = vgg19.preprocess_input(img)\n    return tf.convert_to_tensor(img)","18ed6ff3":"def deprocess_image(x):\n    # Util function to convert a tensor into a valid image\n    x = x.reshape((img_nrows, img_ncols, 3))\n    # Remove zero-center by mean pixel\n    x[:, :, 0] += 103.939\n    x[:, :, 1] += 116.779\n    x[:, :, 2] += 123.68\n    # 'BGR'->'RGB'\n    x = x[:, :, ::-1]\n    x = np.clip(x, 0, 255).astype(\"uint8\")\n    return x","90e372a3":"def gram_matrix(x):\n    x = tf.transpose(x, (2, 0, 1))\n    features = tf.reshape(x, (tf.shape(x)[0], -1))\n    gram = tf.matmul(features, tf.transpose(features))\n    return gram","6aceb603":"def style_loss(style, combination):\n    S = gram_matrix(style)\n    C = gram_matrix(combination)\n    channels = 3\n    size = img_nrows * img_ncols\n    return tf.reduce_sum(tf.square(S - C)) \/ (4.0 * (channels ** 2) * (size ** 2))","a0dc27ee":"def content_loss(base, combination):\n    return tf.reduce_sum(tf.square(combination - base))","6c60a3f5":"def total_variation_loss(x):\n    a = tf.square(\n        x[:, : img_nrows - 1, : img_ncols - 1, :] - x[:, 1:, : img_ncols - 1, :]\n    )\n    b = tf.square(\n        x[:, : img_nrows - 1, : img_ncols - 1, :] - x[:, : img_nrows - 1, 1:, :]\n    )\n    return tf.reduce_sum(tf.pow(a + b, 1.25))","7ebf0035":"# Build a VGG19 model loaded with pre-trained ImageNet weights\nmodel = vgg19.VGG19(weights=\"imagenet\", include_top=False)\n\n# Get the symbolic outputs of each \"key\" layer (we gave them unique names).\noutputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n\n# Set up a model that returns the activation values for every layer in\n# VGG19 (as a dict).\nfeature_extractor = keras.Model(inputs=model.inputs, outputs=outputs_dict)","3dc2a148":"# List of layers to use for the style loss.\nstyle_layer_names = [\n    \"block1_conv1\",\n    \"block2_conv1\",\n    \"block3_conv1\",\n    \"block4_conv1\",\n    \"block5_conv1\",\n]\n# The layer to use for the content loss.\ncontent_layer_name = \"block5_conv2\"","a5193b6f":"def compute_loss(combination_image, base_image, style_reference_image):\n    input_tensor = tf.concat(\n        [base_image, style_reference_image, combination_image], axis=0\n    )\n    features = feature_extractor(input_tensor)\n\n    # Initialize the loss\n    loss = tf.zeros(shape=())\n\n    # Add content loss\n    layer_features = features[content_layer_name]\n    base_image_features = layer_features[0, :, :, :]\n    combination_features = layer_features[2, :, :, :]\n    loss = loss + content_weight * content_loss(\n        base_image_features, combination_features\n    )\n    # Add style loss\n    for layer_name in style_layer_names:\n        layer_features = features[layer_name]\n        style_reference_features = layer_features[1, :, :, :]\n        combination_features = layer_features[2, :, :, :]\n        sl = style_loss(style_reference_features, combination_features)\n        loss += (style_weight \/ len(style_layer_names)) * sl\n\n    # Add total variation loss\n    loss += total_variation_weight * total_variation_loss(combination_image)\n    return loss","249cdb44":"@tf.function\ndef compute_loss_and_grads(combination_image, base_image, style_reference_image):\n    with tf.GradientTape() as tape:\n        loss = compute_loss(combination_image, base_image, style_reference_image)\n    grads = tape.gradient(loss, combination_image)\n    return loss, grads","a1da9140":"optimizer = keras.optimizers.SGD(\n    keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate=100.0, decay_steps=100, decay_rate=0.96\n    )\n)","bae709b4":"base_image = preprocess_image(base_image_path)\nstyle_reference_image = preprocess_image(style_reference_image_path)\ncombination_image = tf.Variable(preprocess_image(base_image_path))\n\niterations = 100\nfor i in range(1, iterations + 1):\n    loss, grads = compute_loss_and_grads(\n        combination_image, base_image, style_reference_image\n    )\n    optimizer.apply_gradients([(grads, combination_image)])\n    if i % 100 == 0:\n        print(\"Iteration %d: loss=%.2f\" % (i, loss))\n        img = deprocess_image(combination_image.numpy())\n        fname = result_prefix + \"_at_iteration_%d.png\" % i\n        keras.preprocessing.image.save_img(fname, img)\n\n\"\"\"\nAfter 100 iterations, you get the following result:\n\"\"\"\n\ndisplay(Image(result_prefix + \"_at_iteration_100.png\"))\n","1df4737f":"# Style-Transfer-Using-VGG19\n![styletransfer.png](attachment:86790a13-7af0-4673-a2fd-ac501d6925fc.png)\n# Please Upvote if you like this notebook \ud83d\udc4d\ud83d\udc4d"}}