{"cell_type":{"69ff73e5":"code","ceb7e3e6":"code","db6fe721":"code","7f70c5be":"code","2b5eaff4":"code","18487d87":"code","736b6fb1":"code","44a56e57":"code","d9338614":"code","3e8c06b2":"code","5249825e":"code","a16f56f7":"code","2c35b7f0":"code","2957ecec":"code","939636b3":"code","5c0bf88f":"code","2571b094":"code","054ae6f4":"code","9b800ba4":"code","ba482dac":"code","109bc69a":"markdown","6b0d83d6":"markdown","38c77691":"markdown"},"source":{"69ff73e5":"!pip install geffnet\n!pip install timm\n!pip install ttach","ceb7e3e6":"class CFG:\n    debug=False\n    n_classes = 2\n    lr=1e-4\n    batch_size=16\n    epochs=10# you can train more epochs\n    seed=777\n    n_fold=4\n    warmup=-1\n    device=1\n    amp = True\n    amp_inf = False\n    smooth = False\n    smooth_alpha = 0.1\n    efnet_num = 10##:0,1,2\n    drop_rate = 0.25\n    crop = True##bool\n    psuedo_label = False\n    pseudo_predict = \"2020-11-06_14:50:43.385109_predict.csv\"\n    TTA=True\n    Attention=True\n    white = False#\u3054\u307e\u3057\u304a\u306b\u306a\u308b\n    model_name = \"tf_efficientnet_b0_ns\"\n    zoom = True\nSIZE = 512","db6fe721":"import pandas as pd \nimport glob\n\nopen_nega = glob.glob(\"..\/input\/glaucomadataset\/Non Glaucoma\/*png\")\nopen_pos = glob.glob(\"..\/input\/glaucomadataset\/Glaucoma\/*tif\")\n\n\n\nopen_p = pd.DataFrame()\nopen_p[\"file\"] = open_pos\nopen_p[\"label\"] = 1\n\nopen_n = pd.DataFrame()\nopen_n[\"file\"] = open_nega\nopen_n[\"label\"] = 1\n\nopen_ = pd.concat([open_n,open_p])\nprint(open_.head())\n\nopen_[\"from_china\"] = 0\n\n\n\nchina = pd.read_csv(\"..\/input\/panda-efnetb2-180-weight\/china_gla.csv\")\nchina[\"file\"] = [\"..\/input\/ocular-disease-recognition-odir5k\/ODIR-5K\/ODIR-5K\/Training Images\/{}\".format(china[\"filename\"].values[i]) for i in range(len(china))]\nchina[\"label\"] = china[\"Gla\"]\nchina_ = china.drop([\"Unnamed: 0\",\"Patient Age\",\"ID\",\"Patient Sex\"],axis=1)\nchina_1 = china_.head(300)\nchina_0 = china_.tail(300)\ncat_df = pd.concat([china_1,china_0])\ncat_df = cat_df.reset_index(drop=True)\n\ncat_df[\"from_china\"] = 1\n\ndf = pd.concat([cat_df,open_])\ndf.to_csv(\"concat_df.csv\",index=False)","7f70c5be":"import geffnet\nimport sys\nimport gc\nimport os\nimport random\nimport time\nfrom contextlib import contextmanager\nfrom pathlib import Path\nfrom collections import defaultdict, Counter\n\n#from  torch.cuda.amp import autocast, GradScaler \nimport cv2\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nimport scipy as sp\nimport pydicom\nimport sklearn.metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss\nfrom functools import partial\nfrom tqdm import tqdm\nimport ttach as tta##TTA\nimport timm\n\nimport matplotlib as mpl\nmpl.use ( 'Agg') # must be written both in import intermediate\nimport matplotlib.pyplot as plt\nimport sklearn.metrics as metric\nimport torch\nimport torch.nn as nn\nfrom torch.optim import Adam, SGD\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.models as models\nfrom albumentations import Compose, Normalize, HorizontalFlip, VerticalFlip,RandomGamma, RandomRotate90,GaussNoise,Cutout\nfrom albumentations.pytorch import ToTensorV2\n\n\n# Utils\n# ====================================================","2b5eaff4":"def seed_torch(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nseed_torch(seed=42)\n","18487d87":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(torch.cuda.current_device())","736b6fb1":"class efenet_Model_attention(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        if CFG.efnet_num ==0:\n            m = geffnet.efficientnet_b0(pretrained=True, drop_rate=CFG.drop_rate)\n        elif CFG.efnet_num ==1:\n            m = geffnet.efficientnet_b1(pretrained=True, drop_rate=CFG.drop_rate)\n        elif CFG.efnet_num ==2:\n            m = geffnet.efficientnet_b2(pretrained=True, drop_rate=CFG.drop_rate)\n        elif CFG.efnet_num ==10:\n            m = timm.create_model(CFG.model_name, pretrained=True, num_classes=CFG.n_classes)\n        self.enc = nn.Sequential(*list(m.children())[:-3]) \n        nc = list(m.children())[-1].in_features\n        self.atten = nn.Sequential(\n                nn.Conv2d(nc, 1, 1),\n                nn.Sigmoid())\n        self.head = nn.Sequential(nn.AdaptiveAvgPool2d(1),nn.Flatten(),nn.Linear(nc,512),\n                            nn.ReLU(),nn.BatchNorm1d(512), nn.Dropout(0.5),nn.Linear(512,2))\n    def forward(self, x):\n        x = self.enc(x)#\u30d9\u30fc\u30b9\u306e\u30e2\u30c7\u30eb\u306e\u6d41\u308c\u306b\u540c\u3058\n        atten = self.atten(x)\n        #print(x.size(),atten.size())\n        y = x*atten\n        x = self.head(y)\n        return x,atten\n\nclass baseline_model(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        if CFG.efnet_num ==0:\n            self.model = geffnet.efficientnet_b0(pretrained=True, drop_rate=CFG.drop_rate)\n        elif CFG.efnet_num ==1:\n            self.model = geffnet.efficientnet_b1(pretrained=True, drop_rate=CFG.drop_rate)\n        elif CFG.efnet_num ==2:\n            self.model = geffnet.efficientnet_b2(pretrained=True, drop_rate=CFG.drop_rate)\n        elif CFG.efnet_num ==10:\n            self.model = timm.create_model(CFG.model_name, pretrained=False, num_classes=CFG.n_classes)\n        elif CFG.efnet_num<3:\n            self.model.classifier = nn.Linear(self.model.classifier.in_features, 2)\n\n    def forward(self, x):\n        \n        if CFG.efnet_num ==10:\n            x = self.model(x)\n        else:\n            x = self.model(x)#\u30d9\u30fc\u30b9\u306e\u30e2\u30c7\u30eb\u306e\u6d41\u308c\u306b\u540c\u3058\n        return x\n\nclass baseline_Model_attention(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        if CFG.efnet_num ==0:\n            m = geffnet.efficientnet_b0(pretrained=False, drop_rate=CFG.drop_rate)\n        elif CFG.efnet_num ==1:\n            m = geffnet.efficientnet_b1(pretrained=False, drop_rate=CFG.drop_rate)\n        elif CFG.efnet_num ==2:\n            m = geffnet.efficientnet_b2(pretrained=False, drop_rate=CFG.drop_rate)\n        elif CFG.efnet_num ==10:\n            m = timm.create_model(CFG.model_name, pretrained=True, num_classes=CFG.n_classes)\n        self.enc = nn.Sequential(*list(m.children())[:-3]) \n        nc = list(m.children())[-1].in_features\n        self.atten = nn.Sequential(\n                nn.Conv2d(nc, 1, 1),\n                nn.Sigmoid())\n        self.head = nn.Sequential(nn.AdaptiveAvgPool2d(1),nn.Flatten(),nn.Linear(nc,512),\n                            nn.ReLU(),nn.BatchNorm1d(512), nn.Dropout(0.5),nn.Linear(512,2))\n    def forward(self, x):\n        x = self.enc(x)#\u30d9\u30fc\u30b9\u306e\u30e2\u30c7\u30eb\u306e\u6d41\u308c\u306b\u540c\u3058\n        atten = self.atten(x)\n        #print(x.size(),atten.size())\n        y = x*atten\n        x = self.head(y)\n        return x,atten\n","44a56e57":"import math\ndef get_pad_width(im, new_shape, is_rgb=True):\n    pad_diff = new_shape - im.shape[0], new_shape - im.shape[1]\n    t, b = math.floor(pad_diff[0]\/2), math.ceil(pad_diff[0]\/2)\n    l, r = math.floor(pad_diff[1]\/2), math.ceil(pad_diff[1]\/2)\n    if is_rgb:\n        pad_width = ((t,b), (l,r), (0, 0))\n    else:\n        pad_width = ((t,b), (l,r))\n    return pad_width\n\ndef crop_object(img, thresh=10, maxval=200, square=False):\n    \"\"\"\n    Source: https:\/\/stackoverflow.com\/questions\/49577973\/how-to-crop-the-biggest-object-in-image-with-python-opencv\n    \"\"\"\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)# convert to grayscale\n    #plt.imshow(gray,cmap=\"gray\")\n    #plt.show()#\u666e\u901a\u306b\u767d\u9ed2\u306e\u304c\u307f\u3048\u308b\n    # threshold to get just the signature (INVERTED)\n    retval, thresh_gray = cv2.threshold(gray, thresh=thresh, maxval=maxval, type=cv2.THRESH_BINARY)\n    #plt.imshow(thresh_gray,cmap=\"gray\")\n    #plt.show()\n    contours, hierarchy = cv2.findContours(thresh_gray,cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n    #https:\/\/qiita.com\/anyamaru\/items\/fd3d894966a98098376c\n    # Find object with the biggest bounding box\n    mx = (0,0,0,0)      # biggest bounding box so far\n    mx_area = 0\n    for cont in contours:\n        x,y,w,h = cv2.boundingRect(cont)\n        area = w*h\n        if area > mx_area:\n            mx = x,y,w,h\n            mx_area = area\n    x,y,w,h = mx#(0,0,0,0)\u306a\u306e\u306fcontours\u306b\u4f55\u3082\u5165\u3063\u3066\u306a\u3044\u304b\u3089\n    crop = img[y:y+h, x:x+w]\n    if square:\n        pad_width = get_pad_width(crop, max(crop.shape))\n        crop = np.pad(crop, pad_width=pad_width, mode='constant', constant_values=255)\n    if CFG.white:\n        black = [0, 0, 0]\n        white = [255, 255, 255]\n        crop[np.where((crop == black).all(axis=2))] = white\n    if CFG.zoom:\n        h = crop.shape[0]\n        w = crop.shape[1]\n        h_ = int(h*0.15)\n        w_ = int(w*0.15)\n        crop = crop[h_:h-h_,w_:w-w_]\n    return crop\n","d9338614":"##train_test_split\nfrom sklearn.model_selection import train_test_split\ntrain, test = train_test_split(cat_df, test_size=0.3,stratify = cat_df[\"label\"], random_state=2020)\ntrain = train.reset_index(drop=True)\ntest = test.reset_index(drop=True)","3e8c06b2":"##train_valid_split\nif CFG.debug:\n    folds = train.sample(n=200, random_state=CFG.seed).reset_index(drop=True).copy()\nelse:\n    folds = train.copy()\ntrain_labels = folds[\"label\"].values\nkf = StratifiedKFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)\nfor fold, (train_index, val_index) in enumerate(kf.split(folds.values, train_labels)):\n    print(\"num_train,val\",len(train_index),len(val_index),len(val_index)+len(train_index))\n    folds.loc[val_index, 'fold'] = int(fold)\n\nfolds['fold'] = folds['fold'].astype(int)\nfolds.to_csv('folds.csv', index=None)\nfolds.head()","5249825e":"class TrainDataset(Dataset):\n    def __init__(self, df,transform1=None, transform2=None):\n        self.df = df\n        self.transform = transform1\n        self.transform_ = transform2\n        \n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        file_path = self.df['file'].values[idx]\n        image = cv2.imread(file_path)\n        if CFG.crop:\n            image = crop_object(image)\n        try:\n            image = cv2.resize(image,(SIZE,SIZE))\n        except Exception as e:\n            print(file_path)\n        label_ = self.df[\"label\"].values[idx]\n        if self.transform:\n            image = self.transform(image=image)['image']\n        if self.transform_:\n            image = self.transform_(image=image)['image']\n\n        \n            \n        label = torch.tensor(label_)\n        #print(label_,type(label_),label,label.size())\n        \n        return image, label","a16f56f7":"import sklearn.metrics as metric\n\ndef auc(y,y_hat):\n    return metric.roc_auc_score(y,y_hat)\n","2c35b7f0":"def get_transforms1(*, data):\n\n    #train,valid\u4ee5\u5916\u3060\u3063\u305f\u3089\u6012\u308b\n    \n    if data == 'train':\n        return Compose([\n            HorizontalFlip(p=0.5),\n            VerticalFlip(p=0.5),\n            #GaussNoise(p=0.5),\n            #RandomRotate90(p=0.5),\n            #RandomGamma(p=0.5),\n            #RandomAugMix(severity=3, width=3, alpha=1., p=0.5),\n            #GaussianBlur(p=0.5),\n            #GridMask(num_grid=3, p=0.3),\n            #Cutout(p=0.5),\n            Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n        ])\n    \n    elif data == 'valid':\n        return Compose([\n            Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225],\n            )\n        ])\n\ndef to_tensor(*args):\n\n        return Compose([\n            ToTensorV2()\n        ])","2957ecec":"!pip install --upgrade pip\n!pip install -U torch","939636b3":"@contextmanager\ndef timer(name):\n    t0 = time.time()\n    LOGGER.info(f'[{name}] start')\n    yield\n    LOGGER.info(f'[{name}] done in {time.time() - t0:.0f} s.')\n\n    \ndef init_logger(log_file='train.log'):\n    from logging import getLogger, DEBUG, FileHandler,  Formatter,  StreamHandler\n    \n    log_format = '%(asctime)s %(levelname)s %(message)s'\n    \n    stream_handler = StreamHandler()\n    stream_handler.setLevel(DEBUG)\n    stream_handler.setFormatter(Formatter(log_format))\n    \n    file_handler = FileHandler(log_file)\n    file_handler.setFormatter(Formatter(log_format))\n    \n    logger = getLogger('RSNA2020')\n    logger.setLevel(DEBUG)\n    logger.addHandler(stream_handler)\n    logger.addHandler(file_handler)\n    \n    return logger\nimport datetime\ndt_now = datetime.datetime.now()\ndt_now_ = str(dt_now).replace(\" \",\"_\")\nprint(\"\u5b9f\u9a13\u958b\u59cb\",dt_now_)\nLOG_FILE = 'train{}.log'.format(dt_now_)\nLOGGER = init_logger(LOG_FILE)","5c0bf88f":"def onehot_encoding(label, n_classes):\n    \"\"\"\n  \u3000torch,tensor(k_0,k_1,k_2,)k_i\u306f0~n_classes-1\u306e\u3069\u308c\u304b\u2192one_hot\n  \u3000\"\"\"\n    return torch.eye(n_classes)[label.type(torch.long)]\ndef label_smoothing(label,epsilon,n_classes):\n    onehot = onehot_encoding(label, n_classes).float()\n    label = onehot * (1 - epsilon) + torch.ones_like(onehot)* epsilon\/n_classes\n    return label\n\ndef cross_entropy_loss(input, target, reduction):\n    logp = nn.functional.log_softmax(input, dim=1)\n    loss = torch.sum(-logp * target, dim=1)\n    if reduction == 'none':\n        return loss\n    elif reduction == 'mean':\n        return loss.mean()\n    elif reduction == 'sum':\n        return loss.sum()\n    else:\n        raise ValueError(\n            '`reduction` must be one of \\'none\\', \\'mean\\', or \\'sum\\'.')\n\n\ndef train_fn(fold):\n    print(f\"### fold: {fold} ###\")\n\n        \n    trn_idx = folds[folds['fold'] != fold].index\n    val_idx = folds[folds['fold'] == fold].index\n    train_dataset = TrainDataset(folds.loc[trn_idx].reset_index(drop=True), \n                                 transform1=get_transforms1(data='train'),transform2=to_tensor())#\n    valid_dataset = TrainDataset(folds.loc[val_idx].reset_index(drop=True), \n                                 transform1=get_transforms1(data='valid'),transform2=to_tensor())#\n\n    train_loader = DataLoader(train_dataset, batch_size=CFG.batch_size, shuffle=True, num_workers=4)\n    valid_loader = DataLoader(valid_dataset, batch_size=CFG.batch_size, shuffle=False, num_workers=4)\n    \n    if CFG.Attention:\n        model = efenet_Model_attention()\n    else:\n        model = efnet_model()\n    model.to(device)\n    \n    optimizer = Adam(model.parameters(), lr=CFG.lr, amsgrad=False)\n    #scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2, verbose=True, eps=1e-6)\n    #scheduler = CosineAnnealingLR(optimizer, T_max=20, eta_min=0.001)\n    \n    criterion = nn.CrossEntropyLoss()#weight = class_weight\n    softmax = nn.Softmax(dim = 1)\n    scaler = torch.cuda.amp.GradScaler()\n    #criterion = FocalLoss_BCE()#doesn't work\n    best_score = 0\n    best_loss = np.inf\n    best_preds = None\n    \n    \n    for epoch in range(CFG.epochs):\n        \n        start_time = time.time()\n\n        model.train()\n\n        if CFG.warmup>epoch:\n            print(\"freeze\")\n            for param in model.model.parameters():\n                param.requires_grad = False\n            for param in model.model.classifier.parameters():\n                param.requires_grad = True\n        \n        if CFG.warmup<=epoch:\n            print(\"unfreeze\")\n            for param in model.parameters():\n                param.requires_grad = True\n        avg_loss = 0.\n\n        tk0 = tqdm(enumerate(train_loader), total=len(train_loader))\n\n        for i, (images, labels) in tk0:\n            optimizer.zero_grad()\n\n            images = images.to(device)\n            smooth_labels = label_smoothing(labels,CFG.smooth_alpha,2).to(device)\n            #print(\"smoooth\",smooth_labels.size())#torch.Size([64, 3, 512, 512, 3])\n            labels = labels.to(device)\n            if CFG.Attention:\n                y_preds,attention = model(images.float())\n            else:\n                y_preds = model(images.float())\n            if CFG.smooth:\n                loss = cross_entropy_loss(y_preds, smooth_labels,\"mean\")\n            else:\n                loss = criterion(y_preds, labels.long())\n            loss.backward()\n            optimizer.step()\n                #optimizer.zero_grad()\n\n            avg_loss += loss.item() \/ len(train_loader)\n        model.eval()\n        avg_val_loss = 0.\n        preds = []\n        valid_labels = []\n        masks = []\n        tk1 = tqdm(enumerate(valid_loader), total=len(valid_loader))\n\n        for i, (images, labels) in tk1:\n\n            images = images.to(device)\n            smooth_labels = label_smoothing(labels,CFG.smooth_alpha,2).to(device)\n            labels = labels.to(device)\n\n            with torch.no_grad():\n                if CFG.Attention:\n                    y_preds,attention = model(images.float())\n                    masks.append(attention.to(\"cpu\").detach().numpy())\n                else:\n                    y_preds = model(images.float())\n                if CFG.smooth:\n                    loss = cross_entropy_loss(y_preds, smooth_labels,\"mean\")\n                else:\n                    loss = criterion(y_preds,labels.long())\n\n\n\n            valid_labels.append(labels.to('cpu').numpy())\n            softmax = nn.Softmax(dim = 1)\n            y_preds = softmax(y_preds)\n            #if i ==0:\n                #print(y_preds.shape,y_preds.dtype,y_preds)#torch.Size([16, 5]) torch.float32\n\n            preds.append(y_preds.to('cpu').numpy())\n            avg_val_loss += loss.item() \/ len(valid_loader)\n            \n        #scheduler.step(avg_val_loss)\n            \n        preds = np.concatenate(preds)\n        if CFG.Attention:\n            masks = np.concatenate(masks)\n        #print(preds.shape)\n        valid_labels = np.concatenate(valid_labels)\n        #valid_labels = np.identity(5)[valid_labels]\n\n        print(preds.shape,valid_labels.shape)\n\n        score = auc(valid_labels,preds[:,1])\n\n        elapsed = time.time() - start_time\n        LOGGER.debug(f'  Epoch {epoch+1} - avg_train_loss: {avg_loss:.6f}  avg_val_loss: {avg_val_loss:.6f}  time: {elapsed:.0f}s')\n        \n        if score>best_score:#logloss\u306e\u30b9\u30b3\u30a2\u304c\u826f\u304b\u3063\u305f\u3089\u4e88\u6e2c\u5024\u3092\u66f4\u65b0...best_epoch\u3092\u304d\u3081\u308b\u305f\u3081\n            best_score = score\n            best_preds = preds\n            LOGGER.debug(f'  Epoch {epoch+1} - Save Best Score: {best_score:.4f}')\n            torch.save(model.state_dict(), f'fold{fold}_{dt_now_}_baseline.pth')#\u5404epoch\u306e\u30e2\u30c7\u30eb\u3092\u4fdd\u5b58\u3002\u3002\u3002best_epoch\u7d42\u4e86\u6642\u306e\u30e2\u30c7\u30eb\u3092\u63a8\u8ad6\u306b\u4f7f\u7528\u3059\u308b\uff1f\n    \n    return best_preds, valid_labels,masks\n","2571b094":"\npreds = []\nvalid_labels = []\nmodels =[]\nfor fold in range(CFG.n_fold):\n    _preds, _valid_labels,_model = train_fn(fold)\n    preds.append(_preds)\n    valid_labels.append(_valid_labels)\n    models.append(_model)\n    \n##\npreds_ = np.concatenate(preds)\nvalid_labels_ = np.concatenate(valid_labels)\n\nscore = auc(valid_labels_,preds_)\nimport datetime\n\ndt_now = datetime.datetime.now()\nprint(\"\u73fe\u5728\u6642\u523b\",dt_now)\nprint(\"=====AUC(CV)======\",score)\n    ","054ae6f4":"class TestDataset(Dataset):\n    def __init__(self, df, transform1=None, transform2=None):\n        self.df = df\n        self.transform = transform1\n        self.transform_ = transform2\n        \n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        file_path = self.df['file'].values[idx]\n        image = cv2.imread(file_path)\n        if CFG.crop:\n            image = crop_object(image)\n        image = cv2.resize(image,(SIZE,SIZE))\n        label = self.df[\"label\"].values[idx]\n        if self.transform:\n            image = self.transform(image=image)['image']\n        if self.transform_:\n            image = self.transform_(image=image)['image']\n\n        \n        return image\n\n\ndef inference(model, test_loader, device):\n\n    transforms = tta.Compose([\n        tta.HorizontalFlip(),\n        tta.VerticalFlip(),\n        tta.Rotate90(angles=[0, 180]),\n        tta.Multiply(factors=[0.9, 1, 1.1]),        \n    ])\n    model.to(device) \n    if CFG.TTA and CFG.Attention:\n        model = model\n    elif CFG.TTA:\n        model = tta.ClassificationTTAWrapper(model, transforms)\n    \n    model.to(device) \n    model.eval()\n    probs = []\n    masks = []\n\n    for i, images in tqdm(enumerate(test_loader), total=len(test_loader)):\n            \n        images = images.to(device)\n        with torch.no_grad():\n            if CFG.Attention and CFG.Attention:\n                attention_ = []\n                for transformer in transforms:\n                    augmented_image = transformer.augment_image(images.float())\n                    y_preds,attention = model(augmented_image)\n                    attention = transformer.deaugment_mask(attention)\n                    attention_.append(attention)\n                attention = torch.stack(attention_)\n                attention = torch.mean(attention,axis=0)\n            elif CFG.Attention:\n                y_preds,attention = model(image.float())\n\n            else:\n                y_preds = model(images)\n            softmax = nn.Softmax(dim = 1)\n            y_preds = softmax(y_preds)\n            \n            \n        probs.append(y_preds.to('cpu').numpy())\n        if CFG.Attention:\n            masks.append(attention.to(\"cpu\").detach().numpy())\n\n    probs = np.concatenate(probs)\n    if CFG.Attention:\n        masks = np.concatenate(masks)\n        print(masks.shape)\n\n\n    \n    return probs,masks","9b800ba4":"def submit():\n        print('run inference')\n        test_dataset = TestDataset(test, transform1=get_transforms1(data='valid'),transform2=to_tensor())\n        test_loader = DataLoader(test_dataset, batch_size=CFG.batch_size, shuffle=False)\n        probs = []\n        masks = []\n        for fold in range(CFG.n_fold):\n            weights_path = f\"..\/input\/panda-efnetb2-180-weight\/fold{fold}_2020-11-24_022939.936386_baseline.pth\"\n            if CFG.Attention:\n                model = baseline_Model_attention()\n            else:\n                model = baseline_model()\n            state_dict = torch.load(weights_path,map_location=device)\n            model.load_state_dict(state_dict)\n            _probs,_mask = inference(model, test_loader, device)\n            probs.append(_probs)\n            masks.append(_mask)\n\n        probs = np.mean(probs, axis=0)\n        if CFG.Attention:\n            masks = np.mean(masks, axis=0)\n\n        return probs,masks\n\npredict,masks= submit()\nif CFG.Attention:\n    np.save(\"attention_masks.npy\",masks)\nprint(test.head())\nprint(test[\"label\"].values.shape,predict.shape)\nscore = auc(test[\"label\"].values,predict[:,1])\nprint(\"=====AUC(inner_test)======\",score)\ntest[\"predict\"] = predict[:,1]\ntest.to_csv(\"predict.csv\",index = False)","ba482dac":"masks = np.load(\"attention_masks.npy\")\n%matplotlib  inline\nfor idx in tqdm(range(len(test))):\n    file_path = test['file'].values[idx]\n    label  = test['label'].values[idx]\n    pred = test['predict'].values[idx]\n    mask = masks[idx,:,:,:].transpose((1,2,0))[:,:,0]\n    image = cv2.imread(file_path)\n    image = crop_object(image)\n    image = cv2.resize(image,(SIZE,SIZE))\n    plt.title(f\"{file_path}__label{label}_pred{pred}\")\n    plt.imshow(image)\n    plt.show()\n    plt.imshow(mask,cmap='jet')\n    plt.show()\n    \n    \n    ","109bc69a":"OScular\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\uff08\u4ee5\u4e0b\u4e2d\u56fd\u7523\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3068\u547c\u3076\uff09\u306f\u5206\u96e2\u304c\u96e3\u3057\u305d\u3046\u3002","6b0d83d6":"\u53ef\u8996\u5316","38c77691":"glaucomadataset\u7531\u6765\u306e\u30c7\u30fc\u30bf\u306f\u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u306b\u901a\u3057\u305f\u30d9\u30af\u30c8\u30eb\u3092\u6b21\u5143\u5727\u7e2e\u3059\u308b\u3068\u5272\u3068\u306f\u3063\u304d\u308a\u75be\u60a3\u304b\u5426\u304b\u304c\u5225\u308c\u3066\u3044\u308b\u3053\u3068\u304c\u308f\u304b\u308b\u3002"}}