{"cell_type":{"f9f54e96":"code","86e9a16b":"code","771b88a0":"code","d53f5a6e":"code","953a6898":"code","33631eae":"code","557d1c4f":"code","498dbdfe":"code","25945cad":"code","c8bf1a9e":"code","f7e4625c":"code","9376f038":"code","dad4fcac":"code","ff863ccc":"code","6bd18cf8":"code","82ccb959":"code","33f0f893":"markdown","06407f89":"markdown","0fa5f49e":"markdown","2a6c8f6f":"markdown","1d8e63d7":"markdown"},"source":{"f9f54e96":"#augment image data for dataset\n#create models\n#train models\n#generate examples (val)\n#walk latent space as in DCGAN","86e9a16b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport random\nfrom matplotlib import pyplot as plt\nimport matplotlib.animation as animation\nfrom matplotlib.lines import Line2D\nfrom IPython.display import HTML\nimport os\n\nimport skimage.io as io\nfrom skimage.transform import rotate, AffineTransform, warp\nfrom skimage.util import random_noise\nfrom skimage.filters import gaussian\n\nimport torch\nfrom torch import nn\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import DataLoader\nimport torchvision.utils as vutils\nfrom torch.optim import Adam\n\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","771b88a0":"img = io.imread('..\/input\/monet-jpg-improved\/monet_jpg_improved\/05144e306fa.jpg')\nprint(img.shape)\nio.imshow(img)","d53f5a6e":"data_path = '..\/input\/monet-jpg-improved'\n#check dimensions of all images\nsizes = []\nfor directory in os.listdir(data_path):\n    for file in os.listdir(data_path+'\/\/'+directory):\n        image = io.imread(data_path+'\/\/'+directory+'\/\/'+file)\n        sizes.append(image.shape)\n\nprint('Num Images: ' + str(len(sizes)))\nuniq_sizes = list(set(sizes))\nprint('Num Shapes: ' + str(len(uniq_sizes)))\nprint(uniq_sizes)","953a6898":"#create dataset\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nbatch_size = 32\n\ndata = datasets.ImageFolder(root=data_path,\n                           transform=transforms.Compose([\n                               transforms.ToTensor(),\n                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n                           ])\n                           )\n\ndataloader = DataLoader(data, batch_size=batch_size, shuffle=True)\nreal_batch = next(iter(dataloader))\nplt.figure(figsize=(8,8))\nplt.axis(\"off\")\nplt.title(\"Training Images\")\nplt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:batch_size], padding=2, normalize=True).cpu(),(1,2,0)))","33631eae":"#info returned by dataloader in each batch: first element is batch of images, second is (useless) batch of labels\nprint(len(real_batch))\nprint(real_batch[0].size())\nprint(real_batch[1].size())","557d1c4f":"#adapted from: https:\/\/pytorch.org\/tutorials\/beginner\/dcgan_faces_tutorial.html\n#set bias=False in conv layers as BatchNorm2d implicitly adds a bias paramter\nclass Discriminator(nn.Module):\n    def __init__(self, input_channels=3, n_filt=64):\n        super(Discriminator, self).__init__()\n        #defualt input size is 3 x 256 x 256\n        self.network = []\n        self.network.append(nn.Sequential(\n            nn.Conv2d(input_channels, n_filt, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True)\n        ))\n        \n        #iterate, downsampling by 2 at each step but doubling the number of channels\n        #by default depth=6 (5 hidden + output) results in a final size of n_filt*(2^5) x 4 x 4 prior to the final logit calculation\n        for i in range(5):\n            if i == 0:\n                prev_filt = n_filt\n            else:\n                prev_filt = n_filt*(2**i)\n                \n            self.network.append(nn.Sequential(\n            nn.Conv2d(prev_filt, n_filt*(2**(i+1)), kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(n_filt*(2**(i+1))),\n            nn.LeakyReLU(0.2, inplace=True)\n        ))\n            \n        #output layer with sigmoid\n        self.network.append(nn.Sequential(\n            nn.Conv2d(n_filt*(2**(5)), 1, kernel_size=4, stride=1, padding=0, bias=False),\n            nn.Sigmoid()\n        ))\n        \n        self.network = nn.ModuleList(self.network)\n        \n    def forward(self, x):\n        o = x\n        for index, layer in enumerate(self.network):\n            o = layer(o)\n            #print(\"D \" + str(index))\n            #print(o.size())\n        \n        return o\n    \nclass Generator(nn.Module):\n    def __init__(self, z_dim=100, n_filt=64, num_channels=3):\n        super(Generator, self).__init__()\n        \n        self.network = []\n        depth=6\n        \n        self.network.append(nn.Sequential(\n            nn.ConvTranspose2d( z_dim, n_filt*(2**(depth-1)), kernel_size=4, stride=1, padding=0, bias=False),\n            nn.BatchNorm2d(n_filt*(2**(depth-1))),\n            nn.ReLU(inplace=True)\n        ))\n        \n        for i in range(depth-1):\n            self.network.append(nn.Sequential(\n                nn.ConvTranspose2d( n_filt*(2**(depth-i-1)), n_filt*(2**(depth-i-2)), kernel_size=4, stride=2, padding=1, bias=False),\n                nn.BatchNorm2d(n_filt*(2**(depth-i-2))),\n                nn.ReLU(inplace=True)\n            ))\n            \n        self.network.append(nn.Sequential(\n            nn.ConvTranspose2d(n_filt, num_channels, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.Tanh()\n        ))\n        \n        self.network = nn.ModuleList(self.network)\n        \n    def forward(self, x):\n        o = x\n        for index, layer in enumerate(self.network):\n            o = layer(o)\n            #print(\"G \" + str(index))\n            #print(o.size())\n        \n        return o\n    \n#copied directly from DCGAN tutorial above\n# custom weights initialization called on netG and netD\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)\n        \n#https:\/\/discuss.pytorch.org\/t\/check-gradient-flow-in-network\/15063  \ndef plot_grad_flow(named_parameters):\n    '''Plots the gradients flowing through different layers in the net during training.\n    Can be used for checking for possible gradient vanishing \/ exploding problems.\n    \n    Usage: Plug this function in Trainer class after loss.backwards() as \n    \"plot_grad_flow(self.model.named_parameters())\" to visualize the gradient flow'''\n    ave_grads = []\n    max_grads= []\n    layers = []\n    for n, p in named_parameters:\n        if(p.requires_grad) and (\"bias\" not in n):\n            layers.append(n)\n            ave_grads.append(p.grad.abs().mean())\n            max_grads.append(p.grad.abs().max())\n    plt.figure()\n    plt.bar(np.arange(len(max_grads)), max_grads, alpha=0.1, lw=1, color=\"c\")\n    plt.bar(np.arange(len(max_grads)), ave_grads, alpha=0.1, lw=1, color=\"b\")\n    plt.hlines(0, 0, len(ave_grads)+1, lw=2, color=\"k\" )\n    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n    plt.xlim(left=0, right=len(ave_grads))\n    plt.ylim(bottom = -0.001, top=0.02) # zoom in on the lower gradient regions\n    plt.xlabel(\"Layers\")\n    plt.ylabel(\"average gradient\")\n    plt.title(\"Gradient flow\")\n    plt.grid(True)\n    plt.legend([Line2D([0], [0], color=\"c\", lw=4),\n                Line2D([0], [0], color=\"b\", lw=4),\n                Line2D([0], [0], color=\"k\", lw=4)], ['max-gradient', 'mean-gradient', 'zero-gradient'])\n    plt.show()","498dbdfe":"#hyper params from DCGAN Paper\nz_dim = 100\nlr = 0.00005\nbetas = (.5, .999)\n\nnetG = Generator(z_dim=z_dim, n_filt=64, num_channels=3)\nnetD = Discriminator(input_channels=3, n_filt=64)\n\nnetG.apply(weights_init)\nnetD.apply(weights_init)\n\ncriterion = nn.BCELoss()\noptimizerG = Adam(netG.parameters(), lr=lr, betas=betas)\noptimizerD = Adam(netD.parameters(), lr=lr, betas=betas)\n\nnetG.to(device)\nnetD.to(device)\n\n#fixed noise to predict on, visualize model progress\nfixed_noise = torch.randn(64, z_dim, 1, 1, device=device)","25945cad":"print(netG)\nprint(netD)","c8bf1a9e":"# Training Loop - adapted directly from DCGAN tutorial\n\n# Lists to keep track of progress\nimg_list = []\nG_losses = []\nD_losses = []\niters = 0\n\nnum_epochs = 250\n\nreal_label = 0.9\nfake_label = 0.1\n\n#params for decaying noise to be added to discriminator input\nmu = 0.0\nstd = 10.0\n\ntorch.autograd.set_detect_anomaly(True)\n\nprint(\"Starting Training Loop...\")\nprint(device)\n# For each epoch\nfor epoch in range(num_epochs):\n    # For each batch in the dataloader\n    for i, data in enumerate(dataloader, 0):\n\n        ############################\n        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n        ###########################\n        ## Train with all-real batch\n        netD.zero_grad()\n        # Format batch\n        real_cpu = data[0].to(device)\n        b_size = real_cpu.size(0)\n        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n        # Forward pass real batch through D\n        #add decaying noise to real input\n        real_input = real_cpu + ( torch.randn(real_cpu.size(), device=device) + mu ) * (std\/(epoch+1))\n        output_real = netD(real_input).view(-1)\n        \n        # Calculate gradients for D in backward pass\n        #errD_real.backward()\n        D_x = output_real.mean().item()\n\n        ## Train with all-fake batch\n        # Generate batch of latent vectors\n        noise = torch.randn(b_size, z_dim, 1, 1, device=device)\n        # Generate fake image batch with G\n        fake = netG(noise)\n        f_label = torch.full((b_size,), fake_label, dtype=torch.float, device=device)#.fill_(fake_label)\n        # Classify all fake batch with D\n        #add decaying noise to fake input\n        fake_input = fake + ( torch.randn(fake.size(), device=device) + mu ) * (std\/(epoch+1))\n        output_fake = netD(fake_input.detach()).view(-1)\n        \n        # Calculate the gradients for this batch\n        #errD_fake.backward()\n        \n        # Calculate loss on all-real batch\n        errD_real = criterion(output_real, label)\n        # Calculate D's loss on the all-fake batch\n        errD_fake = criterion(output_fake, f_label)\n        \n        # Add the gradients from the all-real and all-fake batches\n        errD = errD_real + errD_fake\n        #calculate average gradient as per Naruto-Sasuke\n        gradD = errD * 0.5\n        gradD.backward()\n        \n        D_G_z1 = output_fake.mean().item()\n        # Update D\n        optimizerD.step()\n\n        ############################\n        # (2) Update G network: maximize log(D(G(z)))\n        ###########################\n        netG.zero_grad()\n        label.fill_(real_label)  # fake labels are real for generator cost\n        # Since we just updated D, perform another forward pass of all-fake batch through D\n        output = netD(fake_input).view(-1)\n        # Calculate G's loss based on this output\n        errG = criterion(output, label)\n        # Calculate gradients for G\n        errG.backward()\n        D_G_z2 = output.mean().item()\n        # Update G\n        optimizerG.step()\n\n        # Output training stats\n        if i % 50 == 0:\n            print('[%d\/%d][%d\/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f \/ %.4f'\n                  % (epoch, num_epochs, i, len(dataloader),\n                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n\n        # Save Losses for plotting later\n        G_losses.append(errG.item())\n        D_losses.append(errD.item())\n        \n        #plot gradient flow of generator\n        #if iters == 10 or iters == 50 or iters % 500 == 0:\n         #   plot_grad_flow(netG.named_parameters())\n\n        # Check how the generator is doing by saving G's output on fixed_noise\n        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n            with torch.no_grad():\n                fake = netG(fixed_noise).detach().cpu()\n            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n\n        iters += 1","f7e4625c":"#save models\nmodel_path = 'GANExp6.pt'\ntorch.save({\n    'generator_state': netG.state_dict(),\n    'discriminator_state': netD.state_dict(),\n    'generator_optimizer_state': optimizerG.state_dict(),\n    'discriminator_optimizer_state': optimizerD.state_dict()\n}, model_path)\n\nfrom IPython.display import FileLink\nFileLink(model_path)","9376f038":"plt.figure()\nplt.title(\"Generator and Discriminator Loss During Training\")\nplt.plot(G_losses,label=\"G\")\nplt.plot(D_losses,label=\"D\")\nplt.xlabel(\"iterations\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","dad4fcac":"len(img_list)","ff863ccc":"fig = plt.figure(figsize=(8,8))\nplt.axis(\"off\")\nims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\nani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n\nHTML(ani.to_jshtml())","6bd18cf8":"#linear interpolation between two images in latent space\ndef lerp(start, end, num_steps):\n    weights = np.linspace(0,1, num=num_steps)\n    points = [start]\n    for w in weights:\n        points.append(torch.lerp(start, end, w))\n    points.append(end)\n    \n    return points\n\n\nnum_steps = 6\nnum_pairs = 8\npoints = []\nlerp_images = []\n\nfor i in range(num_pairs):\n    a = fixed_noise[random.randint(0,63)]\n    b = fixed_noise[random.randint(0,63)]\n    #get latent points in paths\n    points.append(lerp(a,b,num_steps))\n\nfor item in points:\n    for p in item:\n        lerp_images.append( torch.squeeze( netG(torch.reshape(p, (1,p.size()[0],p.size()[1],p.size()[2])) ).detach() ) )\n\nprint(type(lerp_images[0]))  \nprint(lerp_images[0].size())\nplt.figure(figsize=(8,num_steps*num_pairs))\nplt.axis(\"off\")\nplt.title(\"Lerp Images\")\nplt.imshow(np.transpose(vutils.make_grid(lerp_images, padding=2, normalize=True).cpu(),(1,2,0)))","82ccb959":"#spherical interpolation between two images in latent space\n#https:\/\/github.com\/soumith\/dcgan.torch\/issues\/14\ndef slerp(val, low, high):\n    omega = np.arccos(np.clip(np.dot(low\/np.linalg.norm(low), high\/np.linalg.norm(high)), -1, 1))\n    so = np.sin(omega)\n    if so == 0:\n        return (1.0-val) * low + val * high # L'Hopital's rule\/LERP\n    return np.sin((1.0-val)*omega) \/ so * low + np.sin(val*omega) \/ so * high\n\ndef spherical_interp(start, stop, num_steps):\n    weights = np.linspace(0,1, num=num_steps)\n    points = [start]\n    for w in weights:\n        points.append( slerp(w, start, stop) )\n    points.append(stop)\n    \n    return points\n    \nslerp_images = []\npoints = []\n\nfor i in range(num_pairs):\n    a = fixed_noise[random.randint(0,63)]\n    b = fixed_noise[random.randint(0,63)]\n    points.append(spherical_interp(a.cpu().squeeze(),b.cpu().squeeze(),num_steps))\n\nfor item in points:\n    for p in item:\n        slerp_images.append( torch.squeeze( netG(torch.reshape(p, (1,p.size()[0],1,1)).to(device)).detach().cpu() ) )\n\nprint(type(slerp_images[0]))\nprint(slerp_images[0].size())\nplt.figure(figsize=(8,num_steps*num_pairs))\nplt.axis(\"off\")\nplt.title(\"Slerp Images\")\nplt.imshow(np.transpose(vutils.make_grid(slerp_images, padding=2, normalize=True).cpu(),(1,2,0)))","33f0f893":"# Exploring the Latent Space","06407f89":"# Generator and Discriminator Models","0fa5f49e":"# Explore Data","2a6c8f6f":"Notes on training attempts:\n\n1. Vanilla attempt, D loss goes to 100 G loss goes to 0 around epoch 17, seems like mode collapse. Generator produces noise images which somehow minimize loss\nLoss: BCELoss, Optim: Adam\n\n2. As we have only about 1800 samples the collapse in the previous iteration could be due to a high learning rate, in this experiment lets reduce the learning rate from 0.0002 to 0.00001 the intuition for this change is that the generator may be overfitting for this particular discriminator too quickly (this is supported by the fact that this occurs in an early epoch)\n\nHere the training looked more stable but the discriminator loss went to 0 while the GAN loss stbailized around 45-50. This is a documented issue. It seems that the Discriminator is learning the real distribution faster than the Generator can fool it, the learning rate reduction certainly helped but the generator still produced garbage images.\n\n3. To address the Discriminator's strength relative to the Generator in the last experiment lets impair the Discriminator by implementing a form of label smoothing - adding noise to the labels so that the discriminators confidence decreases. Lets start with a simple technique: change real_label from 1->0.9 and fake_label from 0->0.1\n\nThis improves the GAN training and doesn't cause the discriminator loss to go to 0 as before, lets increase num_epochs from 50->200, still garbage images\n\n4. Based on input from Naturo-Sasuke on PyTorch Forums, changed the Discriminator loss to (fake+real)\/2\nStill garbage, lets check gradient flows in Generator\n\nAs expected the gradient flows decreased greatly in the first few layers of the Generator as we iterated forward\n\n5. To address this problem lets add decaying noise to the inputs of the discriminator\nSuccess! The images look atleast passingly similar to the real monet styles - approx 130 epochs\n\n6. Set num_epochs to 300, allow longer training with the intent of exploring the latent space next\n\nIncrease learning rate from 0.00001 -> 0.00005 results are even better","1d8e63d7":"# Training"}}