{"cell_type":{"86b2c1fb":"code","45225686":"code","c452f1a4":"code","5f81ddc8":"code","ecf22caa":"code","93154135":"code","10c71048":"code","b0fb3845":"code","32dbfde3":"code","8e65cf6a":"code","e99161b2":"code","7ac8c87f":"code","74747667":"code","56e68df9":"code","cf663348":"code","4c2d4e79":"code","dce482ff":"code","45725b7b":"code","801e36cf":"code","4737f684":"code","c8e8a016":"code","52c0f560":"code","44174603":"code","3840dfa6":"code","e34f9442":"markdown","0bb06e58":"markdown","dd88c690":"markdown","44a0594c":"markdown","5c19553a":"markdown","c0dd841d":"markdown","6ee87fbc":"markdown","72ad5403":"markdown","6f1dbcb2":"markdown","4ec25652":"markdown","6d1a1584":"markdown","de2fa71b":"markdown"},"source":{"86b2c1fb":"!pip --quiet install ..\/input\/treelite\/treelite-0.93-py3-none-manylinux2010_x86_64.whl","45225686":"!pip --quiet install ..\/input\/treelite\/treelite_runtime-0.93-py3-none-manylinux2010_x86_64.whl","c452f1a4":"import numpy as np\nimport pandas as pd\n\nimport os, sys\nimport gc\nimport math\nimport random\nimport pathlib\nfrom tqdm import tqdm\nfrom typing import List, NoReturn, Union, Tuple, Optional, Text, Generic, Callable, Dict\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, QuantileTransformer\nfrom sklearn.decomposition import PCA\nfrom sklearn import linear_model\nfrom sklearn import metrics\nimport operator\nimport xgboost as xgb\nimport lightgbm as lgb\nimport optuna\nfrom tqdm import tqdm_notebook as tqdm\n\n# treelite\nimport treelite\nimport treelite_runtime \n\n# visualize\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\nimport seaborn as sns\nfrom matplotlib_venn import venn2\nfrom matplotlib import pyplot\nfrom matplotlib.ticker import ScalarFormatter\nsns.set_context(\"talk\")\nstyle.use('fivethirtyeight')\npd.options.display.max_columns = None\n\nimport warnings\nwarnings.filterwarnings('ignore')","5f81ddc8":"SEED = 20201225 # Merry Christmas!\n# INPUT_DIR = '..\/input\/jane-street-market-prediction\/'\nINPUT_DIR = '..\/input\/janestreet-save-as-feather\/'\nTRADING_THRESHOLD = 0.50 # 0 ~ 1: The smaller, the more aggressive\nDATE_BEGIN = 0 # 0 ~ 499: set 0 for model training using the complete data ","ecf22caa":"os.listdir(INPUT_DIR)","93154135":"%%time\n\n# load data blitz fast!\ndef load_data(input_dir=INPUT_DIR):\n    train = pd.read_feather(pathlib.Path(input_dir + 'train.feather'))\n    features = pd.read_feather(pathlib.Path(input_dir + 'features.feather'))\n    example_test = pd.read_feather(pathlib.Path(input_dir + 'example_test.feather'))\n    ss = pd.read_feather(pathlib.Path(input_dir + 'example_sample_submission.feather'))\n    return train, features, example_test, ss\n\ntrain, features, example_test, ss = load_data(INPUT_DIR)","10c71048":"# delete irrelevant files to save memory\ndel features, example_test, ss\ngc.collect()","b0fb3845":"# remove weight = 0 for saving memory \noriginal_size = train.shape[0]\ntrain = train.query('weight > 0').reset_index(drop=True)\n\n# use data later than DATE_BEGIN\ntrain = train.query(f'date >= {DATE_BEGIN}')\n\nprint('Train size reduced from {:,} to {:,}.'.format(original_size, train.shape[0]))","32dbfde3":"# target\ntrain['action'] = train['resp'] * train['weight']\ntrain['action'] = 1 * (train['action'] > 0)","8e65cf6a":"# features to use\nfeats = [f for f in train.columns.values.tolist() if f.startswith('feature')]\nprint('There are {:,} features.'.format(len(feats)))","e99161b2":"train['date'].unique()","7ac8c87f":"# time series split like\npivot = 460\nx_train = train.query(f'date < {pivot}')[feats]\ny_train = train.query(f'date < {pivot}')['action']\nx_val = train.query(f'date >= {pivot}')[feats]\ny_val = train.query(f'date >= {pivot}')['action']","74747667":"# from https:\/\/www.kaggle.com\/gogo827jz\/jane-street-super-fast-utility-score-function\/notebook\nfrom numba import njit\n\n@njit(fastmath = True)\ndef utility_score_numba(date, weight, resp, action):\n    Pi = np.bincount(date, weight * resp * action)\n    t = np.sum(Pi) \/ np.sqrt(np.sum(Pi ** 2)) * np.sqrt(250 \/ len(Pi))\n    u = min(max(t, 0), 6) * np.sum(Pi)\n    return u","56e68df9":"# Theoretical best score for this validation period\ndate = train.query(f'date >= {pivot}')['date'].values\nweight = train.query(f'date >= {pivot}')['weight'].values\nresp = train.query(f'date >= {pivot}')['resp'].values\naction = 1 * (train.query(f'date >= {pivot}')['action'].values > TRADING_THRESHOLD)\nscore = utility_score_numba(date, weight, resp, action)\nprint(f\"Utility Score = {score}\")","cf663348":"lgb_train = lgb.Dataset(x_train, y_train)\nlgb_eval = lgb.Dataset(x_val, y_val)\n\ndef objective(trial):    \n    params = {\n            'num_leaves': trial.suggest_int('num_leaves', 32, 1024),\n            'boosting_type': 'gbdt',\n            'objective': 'binary',\n            'metric': 'binary_logloss',\n            'max_depth': trial.suggest_int('max_depth', 4, 16),\n            'min_child_weight': trial.suggest_int('min_child_weight', 1, 12),\n            'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n            'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n            'bagging_freq': trial.suggest_int('bagging_freq', 1, 8),\n            'min_child_samples': trial.suggest_int('min_child_samples', 4, 80),\n            'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-6, 1.0),\n            'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-6, 1.0),\n            }\n\n    model = lgb.train(params, lgb_train, valid_sets=[lgb_train, lgb_eval], \n                      early_stopping_rounds=10, verbose_eval=1000)\n    val_pred = model.predict(x_val)\n    \n    # score\n    date = train.query(f'date >= {pivot}')['date'].values\n    weight = train.query(f'date >= {pivot}')['weight'].values\n    resp = train.query(f'date >= {pivot}')['resp'].values\n    action = 1 * (val_pred > TRADING_THRESHOLD)\n    score = utility_score_numba(date, weight, resp, action)\n    print(f\"Utility Score = {score}\")\n    return score","4c2d4e79":"%%time\n\n# Bayesian optimization\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=40)","dce482ff":"print('Number of finished trials: {}'.format(len(study.trials)))\n\nprint('Best trial:')\ntrial = study.best_trial\n\nprint('  Value: {}'.format(trial.value))\n\nprint('  Params: ')\nfor key, value in trial.params.items():\n    print('    {}: {}'.format(key, value))","45725b7b":"# plot history\nfrom optuna.visualization import plot_optimization_history\nplot_optimization_history(study)","801e36cf":"%%time\n\nprint('Starting training...')\nlgb_train = lgb.Dataset(train[feats], train['action'])\nmodel = lgb.train(trial.params,\n                lgb_train,\n                num_boost_round=480,\n                valid_sets=lgb_train,  # eval training data\n                feature_name=feats,\n                categorical_feature=[])\n\nprint('Saving model...')\n# save model to file\nmodel.save_model('my_model.txt')","4737f684":"lgb.plot_importance(model, importance_type=\"gain\", figsize=(7, 40))","c8e8a016":"# load LGB with Treelite\nmodel = treelite.Model.load('my_model.txt', model_format='lightgbm')","52c0f560":"# generate shared library\ntoolchain = 'gcc'\nmodel.export_lib(toolchain=toolchain, libpath='.\/mymodel.so',\n                 params={'parallel_comp': 32}, verbose=True)","44174603":"# predictor from treelite\npredictor = treelite_runtime.Predictor('.\/mymodel.so', verbose=True)","3840dfa6":"import janestreet\nenv = janestreet.make_env() # initialize the environment\niter_test = env.iter_test() # an iterator which loops over the test set\n    \nfor (test_df, pred_df) in tqdm(iter_test):\n    if test_df['weight'].item() > 0:\n        # inference with treelite\n        batch = treelite_runtime.Batch.from_npy2d(test_df[feats].values)\n        pred_df.action = (predictor.predict(batch) > TRADING_THRESHOLD).astype('int')\n    else:\n        pred_df.action = 0\n    env.predict(pred_df)","e34f9442":"# Install Treelite","0bb06e58":"# Load data\nI have already saved the training data in the feather-format in [my another notebook](https:\/\/www.kaggle.com\/code1110\/janestreet-save-as-feather?scriptVersionId=47635784). Loading csv takes time but loading feather is really light:)","dd88c690":"All done!","44a0594c":"# Re-training LGB with the best params","5c19553a":"# Best sets of hyperparameters","c0dd841d":"# Hyperparameter optimization\nI use the last dates as a validation data (Time-series split) to Bayesian-Optimize hyperparameters of my LGB.","6ee87fbc":"# Config\nSome configuration setups.","72ad5403":"<center><h2>Jane Street Market Prediction | LGB Hyperparameter Optimization | katsu1110 <\/h2><\/center><hr>\n\n![](https:\/\/optuna.org\/assets\/img\/optuna-logo@2x.png)\n\nHere I demonstrate how to use [Optuna](https:\/\/optuna.org\/) to get a better set of hyperparameters by the Bayesian Optimization. I need a good LGB model for my ensemble:D\n\nAs a bonus, I save the tuned model in the [Treelite](https:\/\/treelite.readthedocs.io\/en\/latest\/) format to accelerate the inference speed.\n\nThis notebook loads feathered-data from [my another notebook](https:\/\/www.kaggle.com\/code1110\/janestreet-save-as-feather?scriptVersionId=47635784) such that we don't have to spend our time on waiting long for loading csv files.\n\nIn this notebook we treat the task as a binary classification.","6f1dbcb2":"# Treelite\nI believe Treelite is must in this competition, to avoid the sumission error due to the long inference time.","4ec25652":"# Submit\nLet's use Treelite for faster inference.","6d1a1584":"# Feature importance\nLet's see feature importance given by the model.","de2fa71b":"# Model fitting"}}