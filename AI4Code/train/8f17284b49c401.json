{"cell_type":{"1860ace3":"code","0f3972ea":"code","defdc01d":"code","1a48eb5f":"code","6fa7ff55":"code","4a14dc2a":"code","91738c99":"code","776a52f6":"code","b36ebbed":"code","fe807de5":"code","a7bb6aee":"code","0f545c7a":"code","cd04336e":"code","1deba944":"code","0000ed6a":"code","50daff37":"code","3caed64e":"code","9a950499":"code","26adee3c":"code","95072714":"code","60f1069c":"code","3a5514c6":"code","6dc30b92":"code","489c7107":"code","b28ee868":"code","63ab7cb2":"code","8872b4a1":"code","41d06ac8":"code","ad7c4776":"code","b5989908":"code","2832b67e":"code","abee957d":"code","3c0716b2":"code","ce8de977":"code","f4718fbb":"code","d90b4586":"code","1589bf33":"code","eb9f50e6":"code","dd635f7e":"code","9e9f1d25":"code","74ae8151":"code","ff8f46b3":"code","e9e0763c":"code","9bdc1ab9":"code","b55301c3":"code","4da01c2d":"code","8c40ee86":"code","ac0db98d":"code","2ee4064f":"code","8a7053e9":"code","2b35ab87":"code","f8971ae1":"code","2ab01c77":"code","0ca823e7":"code","9bfb1f8c":"code","038f7bba":"code","9b764040":"code","502e93c0":"code","7d8c0b1b":"code","e3a3d8f6":"markdown","bd2b32d8":"markdown","e3779d55":"markdown","ed7e99fa":"markdown","87127d31":"markdown","b8ffffaa":"markdown","8c2e455d":"markdown","713f2e84":"markdown","11e03d90":"markdown","9fb466a2":"markdown","9492f139":"markdown","015f24b9":"markdown","8579805b":"markdown","e2ca47cb":"markdown","d0596c4d":"markdown","79915d74":"markdown","b3dcca7b":"markdown","7b4efa44":"markdown","e0699ca3":"markdown","5b991cb5":"markdown","02ca513b":"markdown","4bfc24d9":"markdown","d9510b37":"markdown","d218068d":"markdown","a2afe942":"markdown","105e7179":"markdown","41b0eb8f":"markdown","845da1ed":"markdown","b7474d0f":"markdown","9e787a3d":"markdown","c8c2f2b4":"markdown","28a0d702":"markdown","e15b0685":"markdown","8fc42c7d":"markdown","461a3e32":"markdown","ed6b17f9":"markdown","a4a76ef5":"markdown"},"source":{"1860ace3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/inpreprocessing ```Age```put\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","0f3972ea":"# make sure to turn on the Interenet switch in the right side menu\n! pip install pyspark","defdc01d":"from pyspark.sql import SparkSession\nfrom pyspark.ml import Pipeline\nimport pyspark.sql.functions as F\nfrom pyspark.sql.types import DoubleType, StringType, StructType, StructField\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler, QuantileDiscretizer\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark import SparkContext\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator","1a48eb5f":"# build spark session\nspark = SparkSession.builder.appName(\"Spark on titanic data \").getOrCreate()","6fa7ff55":"train = spark.read.csv(\"\/kaggle\/input\/titanic\/train.csv\", header=\"true\", inferSchema=\"true\")\ntest = spark.read.csv(\"\/kaggle\/input\/titanic\/train.csv\", header=\"true\", inferSchema=\"true\")","4a14dc2a":"type(train)","91738c99":"train.show(5)","776a52f6":"train.printSchema()","b36ebbed":"# statistical summary\ntrain.describe().show()","fe807de5":"train_count = train.count()","a7bb6aee":"print(train_count)","0f545c7a":"survived_groupped_df = train.groupBy(\"Survived\").count()","cd04336e":"survived_groupped_df.show()","1deba944":"getRatio = F.udf(lambda x: round(x\/train_count,2), DoubleType())\nsurvived_groupped_df = survived_groupped_df.withColumn(\"Ratio\", getRatio('count'))","0000ed6a":"survived_groupped_df.show()","50daff37":"train.groupBy(\"Sex\").count().show()","3caed64e":"train.groupBy(\"Sex\").agg(F.mean('Survived'), F.sum('Survived')).show()","9a950499":"train.createOrReplaceTempView(\"train\")","26adee3c":"spark.sql(\"SELECT Sex, round(SUM(Survived)\/count(1),2) as ratio  FROM train GROUP BY Sex\").show()","95072714":"spark.sql(\"SELECT Pclass, round(SUM(Survived)\/count(1),2) as ratio  FROM train GROUP BY Pclass\").show()","60f1069c":"combined = train.union(test)","3a5514c6":"combined.count()","6dc30b92":"combined.createOrReplaceTempView(\"combined\")","489c7107":"null_columns = []\nfor col_name in combined.columns:\n    null_values = combined.where(F.col(col_name).isNull()).count()\n    if(null_values > 0):\n        null_columns.append((col_name, null_values))\nprint(null_columns)","b28ee868":"spark.createDataFrame(null_columns, ['column', 'missing_value']).show()","63ab7cb2":"spark.sql(\"SELECT Name  FROM combined\").show()","8872b4a1":"combined = combined.withColumn('Title',F.regexp_extract(F.col(\"Name\"),\"([A-Za-z]+)\\.\",1))","41d06ac8":"combined.createOrReplaceTempView('combined')\nspark.sql(\"SELECT Title,count(1)  FROM combined GROUP BY Title\").show()","ad7c4776":"titles_map = {\n 'Capt' : 'Rare',\n 'Col' : 'Rare',\n 'Don': 'Rare',\n 'Dona': 'Rare',\n 'Dr' : 'Rare',\n 'Jonkheer' :'Rare' ,\n 'Lady': 'Rare',\n 'Major': 'Rare',\n 'Master': 'Master',\n 'Miss' : 'Miss',\n 'Mlle' : 'Rare',\n 'Mme': 'Rare',\n 'Mr': 'Mr',\n 'Mrs': 'Mrs',\n 'Ms': 'Rare',\n 'Rev': 'Rare',\n 'Sir': 'Rare',\n 'Countess': 'Rare'\n}\ndef impute_title(title):\n    return titles_map[title]\n\ntitle_map_func = F.udf(lambda x: impute_title(x), StringType())\n\ncombined = combined.withColumn('Title', title_map_func('Title'))","b5989908":"combined.createOrReplaceTempView('combined')\nspark.sql(\"SELECT Title  FROM combined GROUP BY Title\").show()","2832b67e":"round(spark.sql(\"SELECT AVG(Age) FROM combined\").collect()[0][0])","abee957d":"combined = combined.fillna(30, subset=['Age'])","3c0716b2":"groupped_Embarked = spark.sql(\"SELECT Embarked,count(1) as count_it FROM combined GROUP BY Embarked ORDER BY count_it DESC\")","ce8de977":"groupped_Embarked.show()","f4718fbb":"embarked_mode = groupped_Embarked.collect()[0][0]","d90b4586":"print(embarked_mode)","1589bf33":"combined = combined.fillna(embarked_mode, subset=['Embarked'])","eb9f50e6":"combined = combined.withColumn(\"Cabin\", combined.Cabin.substr(0, 1))","dd635f7e":"combined.createOrReplaceTempView('combined')\ngroupped_cabin = spark.sql(\"SELECT Cabin,count(1) as count_it FROM combined GROUP BY Cabin ORDER BY count_it DESC\")\ngroupped_cabin.show()","9e9f1d25":"combined = combined.fillna('U', subset=['Cabin'])","74ae8151":"combined = combined.withColumn('Family_size', F.col('Parch')+ F.col('SibSp'))","ff8f46b3":"indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(combined) for column in [\"Sex\",\"Embarked\",\"Title\", \"Cabin\"]]\n\n\npipeline = Pipeline(stages=indexers)\ncombined = pipeline.fit(combined).transform(combined)\n\ncombined.show()","e9e0763c":"combined = combined.drop('Sex','PassengerId','Name','Title', 'SibSp', 'Parch', 'Ticket', 'Cabin', 'Embarked')","9bdc1ab9":"combined_pandas = combined.toPandas()","b55301c3":"combined_pandas","4da01c2d":"train_pandas = combined_pandas[:train_count]\ntest_pandas = combined_pandas[train_count:]","8c40ee86":"train = spark.createDataFrame(train_pandas)\ntest = spark.createDataFrame(test_pandas)\ntest = test.drop('Survived')","ac0db98d":"assembler = VectorAssembler(inputCols=train.columns[1:],outputCol=\"features\")\ntrain_assembler_vector = assembler.transform(train)\ntrain_assembler_vector.show()\n","2ee4064f":"test_assembler = VectorAssembler(inputCols=test.columns,outputCol=\"features\")\ntest_assembler_vector = test_assembler.transform(test)\ntest_assembler_vector.show()\n","8a7053e9":"X_train, X_test = train_assembler_vector.randomSplit([0.8, 0.2],seed = 11)","2b35ab87":"rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'Survived')\nrfModel = rf.fit(X_train)\npredictions = rfModel.transform(X_test)\npredictions.select(\"prediction\", \"Survived\", \"features\").show()","f8971ae1":"evaluator = MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"accuracy\")\nprint(\"Accuracy : \" + str(evaluator.evaluate(predictions)))","2ab01c77":"submission = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')","0ca823e7":"test = test.drop('Survived')","9bfb1f8c":"\nfinal_predictions = rfModel.transform(test_assembler_vector)\nfinal_predictions = final_predictions.toPandas()","038f7bba":"final_predictions","9b764040":"submission['Survived'] = final_predictions['prediction']","502e93c0":"submission['Survived'] = submission['Survived'].astype(int)","7d8c0b1b":"submission.to_csv(\"RF.csv\",index=False)","e3a3d8f6":"62% was survived, and 38% died.","bd2b32d8":"## 7. Data Cleaning","e3779d55":"### 1.3 Spark Session","ed7e99fa":"### 1.2 Apache Spark","87127d31":"### 7.1 preprocessing ```Name```","b8ffffaa":"74% of the females were survived. 19% of the men were survived.","8c2e455d":"i will replace cabin with first char from the string.","713f2e84":"now i need to remove all sting columns and unnecessary columns","11e03d90":"first of all we need to combie both ```train``` and ```test``` datasets","9fb466a2":"## 2. Problem","9492f139":"now i will put all columns into vectors.\nVectorAssembler is a transformer that combines a given list of columns into a single vector column. It is useful for combining raw features and features generated by different feature transformers into a single feature vector, in order to train ML models","015f24b9":"we can see that  Dr, Rev, Major, Col, Mlle, Capt, Don, Jonkheer, Countess, Ms, Sir, Lady, Mme    \nis really rare titles, so i will map them as Rare","8579805b":"now i will split the data into train and test splits.\n","e2ca47cb":"now i will transform string columns with int values","d0596c4d":"i will fill missing age values based on age mean.","79915d74":"we can see that ```S``` is the mode(e.g. the most frequesnt value) of ```Embarked``` column.","b3dcca7b":"## 6. EDA -  Exploratory data analysis","7b4efa44":"### 1.1 Titanic","e0699ca3":"create train table","5b991cb5":"### 7.3 preprocessing ```Cabin```","02ca513b":"let take title from the name.","4bfc24d9":"Apache Spark is an open-source distributed general-purpose cluster-computing framework. Spark provides an interface for programming entire clusters with implicit data parallelism and fault tolerance.\nbecause of the powerful cluster computing engine, spark designed for fast computation of big data.","d9510b37":"## 1. Background","d218068d":"## 3. Install & Import Libraries","a2afe942":"The sinking of the [Titanic](https:\/\/www.kaggle.com\/c\/titanic) is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.","105e7179":"### 9. Evaluation","41b0eb8f":"### 7.1 preprocessing ```Age```","845da1ed":"for all null values i will fill with ```U``` as undefined.","b7474d0f":"## 8. Random Forest Classifier Model","9e787a3d":"i will add new column ```Family_size```- the sum of ```Parch``` and ```SibSp```","c8c2f2b4":"we can see that most of women were survived.","28a0d702":"### 7.2 preprocessing ```Embarked```","e15b0685":"# Titanic Survival Prediction With Spark","8fc42c7d":"now i need to return back train and test datasets.","461a3e32":"## 5. Data Loading","ed6b17f9":"Our target is to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).","a4a76ef5":"Spark session is a unified entry point of a spark application. It provides a way to interact with various spark\u2019s functionality."}}