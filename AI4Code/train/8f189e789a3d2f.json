{"cell_type":{"18b6e624":"code","e2fc7b3e":"code","7331bb19":"code","ea7a9c8a":"code","92dbc3ac":"code","b0c52dd8":"code","32fd8b32":"code","21064439":"code","bb23a479":"code","497e6158":"code","d72c9b6f":"code","fd446bc5":"code","777ffc91":"code","0e888022":"code","1ccbc7b2":"code","27c18b7f":"code","c7e2f5c6":"code","ab3ac16f":"code","027f5d57":"code","a1b4f539":"code","68ae9a5f":"code","e48de451":"code","1d7db966":"code","7ec4e0cc":"code","fe77a649":"code","ca6db8a6":"code","74e334f2":"code","5489cc06":"code","e96c08cb":"code","f2854598":"code","f42b8215":"code","5a8ff846":"code","2f030948":"code","1e34c33d":"code","c11aa552":"code","a8d8d60a":"code","3b9011da":"code","2cdbf4e7":"code","8c870856":"code","6eef95c1":"code","7d75891c":"code","5d90f1c1":"code","9177de37":"code","705fa57f":"code","41b2f12b":"code","ffdfa240":"code","c2d19271":"code","4db6e7a2":"code","a2e2937e":"code","c591b1f3":"code","fdc9df0d":"code","3bbedb1f":"code","b8ed2f02":"code","05ebb435":"code","38575016":"code","66548d16":"code","1cb36d4b":"code","25d0645c":"code","0bc92ac7":"code","560fb522":"code","370ea216":"code","82b1fd20":"code","8f07816d":"code","71f4f876":"code","44fd64d1":"code","ab3ce798":"code","8570f25a":"code","b87a1723":"code","f60de6f6":"markdown","196669de":"markdown","f22d32f4":"markdown","811f3d74":"markdown","27be4e5f":"markdown","ee6e5dd0":"markdown","a2201214":"markdown","47617ebb":"markdown","42ca32dd":"markdown","09b4fc02":"markdown","56ff4d2a":"markdown","e0413d62":"markdown","663aba60":"markdown"},"source":{"18b6e624":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e2fc7b3e":"# libraries to read the data and perform mathematical operations\nimport pandas as pd\nimport numpy as np\n\n# libraries to visualise the data\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","7331bb19":"data = pd.read_csv('\/kaggle\/input\/weather-dataset-rattle-package\/weatherAUS.csv')","ea7a9c8a":"data.head(10)","92dbc3ac":"data.shape","b0c52dd8":"data.info()","32fd8b32":"data.describe().T","21064439":"data.describe(include = object).T","bb23a479":"data.isnull().sum().sort_values(ascending = False).plot(kind = 'bar')","497e6158":"# the target column, RainTomorrow is also having missing values","d72c9b6f":"data.isnull().sum()\/len(data) * 100","fd446bc5":"# the amount of missing values in these columns > 20%\n# Evaporation      43.166506\n# Sunshine         48.009762\n# Cloud9am         38.421559\n# Cloud3pm         40.807095\n\n# Hence, due to data insufficiency,dropping these columns\ndata.drop(columns = ['Evaporation', 'Sunshine', 'Cloud9am', 'Cloud3pm'], inplace = True)","777ffc91":"# extracting the year and month from the date column since not applying times series analysis on this data\ndata['Year'] = data['Date'].apply(lambda x : x.split('-')[0]).astype(int)\ndata['Month'] = data['Date'].apply(lambda x : x.split('-')[1]).astype(int)","0e888022":"data.drop(columns = 'Date', inplace = True)","1ccbc7b2":"data['Year'].value_counts()","27c18b7f":"data['Month'].value_counts()","c7e2f5c6":"data['RainTomorrow'].value_counts().plot(kind = 'bar')","ab3ac16f":"# the target set is imbalanced\n# it would be labelled highly imbalanced if one class is < 10%","027f5d57":"data['RainTomorrow'].value_counts()\/len(data) *100","a1b4f539":"# segregating the numerical and categorical columns\nnum_data = data.select_dtypes(include = np.number)\ncat_data = data.select_dtypes(include = object)","68ae9a5f":"num_data.head(5)","e48de451":"cat_data.head(5)","1d7db966":"num_data.isnull().sum()","7ec4e0cc":"plt.figure(figsize = (20,12))\nnum_data.hist(bins = 100)\nplt.tight_layout()","fe77a649":"# the columns are near-normally distributed, with very less skewness, so not much transformation is required, except for the column Rainfall","ca6db8a6":"plt.figure(figsize = (10,5))\nsns.histplot(num_data['Rainfall'], bins = 100)","74e334f2":"# Standard Scaler is used when the shape of the distribution is near normal. It preserves the shape of the distribution\n# Min-Max scaler is used when we need to preserve the effect of the outliers\n# Standard Scaler is preferred for this case\n# To remove the effect of outliers, need to apply Robust Scaler","5489cc06":"sns.boxplot(data = num_data, orient = 'h')","e96c08cb":"for col in num_data.columns:\n    plt.figure(figsize = (10,5))\n    sns.boxplot(x = col, data = num_data)\n    plt.show()\n    \n","f2854598":"# the classification algorithms which work using Likelihood Estimation, are affected by outliers.\n# Tree-based algorithms are not affected by outliers\n# Outliers make the classification model actually better.","f42b8215":"for col in cat_data:\n    plt.figure(figsize = (20,5))\n    sns.countplot(x = col, data = cat_data, hue = 'RainTomorrow', palette='rainbow')\n    plt.legend(loc = 'best')\n    plt.xticks(rotation  = 90 )\n    plt.show()","5a8ff846":"# when lat-long values are given, multiply them to use as a new column\n# when know from domain expertise that location is neccesary to predict rainfall\n# but the problem is that the number of locations is so long, that it is not possible to label encode these values manually\n# so, we use dummy encoding; so that certain algorithms like ensemble techniques will work well on these encoded locations","2f030948":"data.groupby(['Location']).describe(include = object)","1e34c33d":"# All the categorical columns in this dataset are ordinal but still we could go with dummy variable encoding. \n# It won't affect the performance of a modern ML algorithm.\n# Because, modern day ML algorithms are not entirely affected by the differences in dummy encoding and label encoding, in general.\n# when to choose which, comes from domain expertise","c11aa552":"# segregating the target column\ny = cat_data['RainTomorrow'].values\ncat_data.drop(columns = 'RainTomorrow', inplace = True)","a8d8d60a":"# concatenating the numerical and categorical data to get the feature set\nX = pd.concat([num_data, cat_data], axis = 1)\n\n# splitting into training and test sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, )","3b9011da":"# Segregating the training and test sets into numerical and categorical data so as to apply different operations on each\nX_train_num = X_train.select_dtypes(include = np.number)\nX_train_cat = X_train.select_dtypes(include = object)\n\nX_test_num = X_test.select_dtypes(include = np.number)\nX_test_cat = X_test.select_dtypes(include = object)","2cdbf4e7":"# Saving the names of the numerical and categorical columns to be added later\nnum_cols = X_train_num.columns\ncat_cols = X_train_cat.columns","8c870856":"from sklearn.impute import SimpleImputer","6eef95c1":"# imputing the numerical missing values by the median\nimputer = SimpleImputer(strategy='median')\n\nX_train_num = pd.DataFrame(imputer.fit_transform(X_train_num),columns=num_cols)\nX_test_num = pd.DataFrame(imputer.transform(X_test_num), columns = num_cols)","7d75891c":"# imputing the categorical missing values by the mode\nimputer = SimpleImputer(strategy='most_frequent')\n\nX_train_cat = pd.DataFrame(imputer.fit_transform(X_train_cat),columns=cat_cols)\nX_test_cat  = pd.DataFrame(imputer.transform(X_test_cat), columns = cat_cols)","5d90f1c1":"# Scaling the numerical data\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\nX_train_num = pd.DataFrame(scaler.fit_transform(X_train_num), columns = num_cols)\nX_test_num = pd.DataFrame(scaler.transform(X_test_num), columns = num_cols)","9177de37":"# dummy encoding the training categorical data\nX_train_cat  =pd.get_dummies(X_train_cat, drop_first=True)","705fa57f":"# checking the encoded training categorical data\nX_train_cat.head(3)","41b2f12b":"# dummy encoding the test categorical data\nX_test_cat  =pd.get_dummies(X_test_cat, drop_first=True)","ffdfa240":"# Creating the training dataset\nX_train = pd.concat([X_train_num, X_train_cat], axis = 1)","c2d19271":"# Creating the test dataset\nX_test = pd.concat([X_test_num, X_test_cat], axis = 1)","4db6e7a2":"# Checking the shape of the newly created datasets\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","a2e2937e":"# How to ascertain that the train-test split is dividing the data correctly. Ans: Check the mean and std for both test and train","c591b1f3":"X_train.describe().T[['mean', 'std']]","fdc9df0d":"X_test.describe().T[['mean', 'std']]","3bbedb1f":"y_train = y_train.reshape(-1)","b8ed2f02":"y_train.shape","05ebb435":"# Imputing the missing values in target variable by the mode\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy='most_frequent')\ny_train = imputer.fit_transform(y_train.reshape(-1,1))\ny_test = imputer.transform(y_test.reshape(-1,1))","38575016":"y_train.shape","66548d16":"X_train.shape, y_train.shape","1cb36d4b":"# Label encoding the target variable\nfrom sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\ny_train = encoder.fit_transform(y_train.ravel())\n\ny_test = encoder.transform(y_test.ravel())","25d0645c":"y_train.shape","0bc92ac7":"from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, precision_score","560fb522":"scorecard = pd.DataFrame(columns = ['Estimator', 'f1_score', 'Accuracy', 'Precision', 'ROC_AUC_Score'])","370ea216":"def update_score (estimator):\n    global scorecard \n    name = estimator.__class__.__name__\n    y_pred = estimator.predict(X_test)\n    y_pred_proba = estimator.predict_proba(X_test)\n    f1 = f1_score(y_test, y_pred)\n    acc = accuracy_score(y_test, y_pred)\n    roc = roc_auc_score(y_test, y_pred_proba[:,1])\n    prec = precision_score(y_test, y_pred)\n    scorecard = scorecard.append({'Estimator':name, 'f1_score': f1, 'Accuracy':acc, 'Precision': prec,'ROC_AUC_Score':roc} ,\n                                       ignore_index=True)\n    return(scorecard)","82b1fd20":"def plot_roc_curve(estimator):\n    y_pred_proba = estimator.predict_proba(X_test)\n    tpr, fpr, thres = roc_curve(y_test, y_pred_proba[:,1])\n    \n    plt.figure(figsize = (12,7))\n    \n    plt.xlim([0,1])\n    plt.ylim([0,1])\n    plt.plot([0,1], [0,1], '--')\n    \n    plt.plot(tpr, fpr, label = estimator.__class__.__name__ %roc_auc_score(y_test, y_pred_proba[:,1]) )\n    plt.legend()\n    plt.xlabel('False positive rate (1-Specificity)', fontsize = 12)\n    plt.ylabel('True positive rate (Sensitivity)', fontsize = 12)","8f07816d":"from sklearn.tree import DecisionTreeClassifier\ndtf = DecisionTreeClassifier()\n\ndtf.fit(X_train, y_train)\nscorecard = update_score(dtf)\nplot_roc_curve(dtf)","71f4f876":"from sklearn.model_selection import KFold\nkf = KFold(n_splits=5)","44fd64d1":"X_train_f = X_train.values","ab3ce798":"for train_idx, test_idx in kf.split(X_train_f):\n    X_train_idx, X_test_idx = X_train_f[train_idx], X_train_f[test_idx]\n    y_train_idx, y_test_idx = y_train[train_idx], y_train[test_idx]\n    dtf.fit(X_train_idx, y_train_idx)\n    update_score(dtf)","8570f25a":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\n\nlogreg.fit(X_train, y_train)\n# y_pred_proba = logreg.predict_proba(X_test)\nupdate_score(logreg)\nplot_roc_curve(logreg)","b87a1723":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n","f60de6f6":"# creating the DecisionTree model","196669de":"# Concatenating the categorical and numerical data to get the final train and test sets","f22d32f4":"# **Importing the Data**","811f3d74":"# Creating a scorecard to compare different models","27be4e5f":"# Missing Value imputation","ee6e5dd0":"# Running different models","a2201214":"# Segregating the numerical and categorical columns","47617ebb":"# Missing Value Imputation","42ca32dd":"# Feature Extraction","09b4fc02":"# Scaling the numerical columns","56ff4d2a":"# Checking the data characteristics","e0413d62":"# Checking the target class","663aba60":"# Missing Value Analysis"}}