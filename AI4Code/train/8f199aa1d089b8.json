{"cell_type":{"62809af1":"code","9dafe9c5":"code","1b9d513c":"code","4debe928":"code","f3896fb5":"code","2df518a0":"code","2cdb5d10":"code","b90eccb1":"code","9d603949":"code","0115105a":"code","95353514":"code","9339f3c1":"code","8df3e772":"code","6095dc17":"code","93f4af53":"code","865491a6":"code","f660928b":"code","60aef1e1":"code","f393e485":"code","4cb001a8":"code","754ea686":"code","14731c8d":"code","de0fcfaa":"code","5d43e194":"markdown","e25e5789":"markdown"},"source":{"62809af1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy\nfrom PIL import Image\nfrom scipy import ndimage\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\n#from cnn_utils import *\nimport IPython.display\nimport pandas as pd\n\n%matplotlib inline\nnp.random.seed(1)\n\n# Any results you write to the current directory are saved as output.","9dafe9c5":"cancer_data = pd.read_csv('..\/input\/hmnist_28_28_RGB.csv')","1b9d513c":"cancer_data.shape","4debe928":"cancer_labels = cancer_data.iloc[:,-1].values","f3896fb5":"cancer_labels.shape","2df518a0":"cancer_labels","2cdb5d10":"cancer_final_data = cancer_data.drop(labels='label',axis= 1)","b90eccb1":"cancer_final_data.shape","9d603949":"sample.shape","0115105a":"#sample = (cancer_final_data.iloc[:,0:].values).astype('float32')\ncancer_final_images = sample.reshape(cancer_final_data.shape[0],28,28,3)","95353514":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder","9339f3c1":"onehot_encoder = OneHotEncoder(sparse=False)\ninteger_encoded = cancer_labels.reshape(len(cancer_labels), 1)\ncancer_labels_encoded = onehot_encoder.fit_transform(integer_encoded)","8df3e772":"cancer_labels_encoded","6095dc17":"cancer_final_images.shape","93f4af53":"from sklearn.model_selection import train_test_split","865491a6":"X_train, X_test, y_train, y_test = train_test_split(cancer_final_images, cancer_labels_encoded, test_size=0.2)\nprint(X_train.shape, y_train.shape)\nprint (X_test.shape, y_test.shape)","f660928b":"# GRADED FUNCTION: create_placeholders\ndef create_placeholders(n_H0, n_W0, n_C0, n_y):\n    \"\"\"\n    Creates the placeholders for the tensorflow session.\n    \n    Arguments:\n    n_H0 -- scalar, height of an input image\n    n_W0 -- scalar, width of an input image\n    n_C0 -- scalar, number of channels of the input\n    n_y -- scalar, number of classes\n        \n    Returns:\n    X -- placeholder for the data input, of shape [None, n_H0, n_W0, n_C0] and dtype \"float\"\n    Y -- placeholder for the input labels, of shape [None, n_y] and dtype \"float\"\n    \"\"\"\n\n    ### START CODE HERE ### (\u22482 lines)\n    X = tf.placeholder(tf.float32,[None,n_H0,n_W0,n_C0])\n    Y =  tf.placeholder(tf.float32,[None,n_y])\n    ### END CODE HERE ###\n    \n    return X, Y","60aef1e1":"def initialize_parameters():\n    \"\"\"\n    Initializes weight parameters to build a neural network with tensorflow. The shapes are:\n                        W1 : [4, 4, 3, 8]\n                        W2 : [2, 2, 8, 16]\n    Returns:\n    parameters -- a dictionary of tensors containing W1, W2\n    \"\"\"\n    tf.set_random_seed(1)                              # so that your \"random\" numbers match ours\n        \n    ### START CODE HERE ### (approx. 2 lines of code)\n    W1 = tf.get_variable(\"W1\", [4, 4, 3, 8], initializer=tf.contrib.layers.xavier_initializer(seed=0))\n    W2 = tf.get_variable(\"W2\", [2, 2, 8, 16], initializer=tf.contrib.layers.xavier_initializer(seed=0))\n    ### END CODE HERE ###\n\n    parameters = {\"W1\": W1,\n                  \"W2\": W2}\n    \n    return parameters","f393e485":"def forward_propagation(X, parameters):\n    \"\"\"\n    Implements the forward propagation for the model:\n    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED\n    \n    Arguments:\n    X -- input dataset placeholder, of shape (input size, number of examples)\n    parameters -- python dictionary containing your parameters \"W1\", \"W2\"\n                  the shapes are given in initialize_parameters\n\n    Returns:\n    Z3 -- the output of the last LINEAR unit\n    \"\"\"\n    \n    # Retrieve the parameters from the dictionary \"parameters\" \n    W1 = parameters['W1']\n    W2 = parameters['W2']\n    \n    ### START CODE HERE ###\n    # CONV2D: stride of 1, padding 'SAME'\n    Z1 = tf.nn.conv2d(X, W1, strides=[1, 1, 1, 1], padding='SAME')\n    # RELU\n    A1 = tf.nn.relu(Z1)\n    # MAXPOOL: window 8x8, stride 8, padding 'SAME'\n    P1 = tf.nn.max_pool(A1, ksize = [1, 8, 8, 1], strides = [1, 8, 8, 1], padding='SAME')\n    # CONV2D: filters W2, stride 1, padding 'SAME'\n    Z2 = tf.nn.conv2d(P1, W2, strides=[1, 1, 1, 1], padding='SAME')\n    # RELU\n    A2 = tf.nn.relu(Z2)\n    # MAXPOOL: window 4x4, stride 4, padding 'SAME'\n    P2 = tf.nn.max_pool(A2, ksize = [1, 4, 4, 1], strides = [1, 4, 4, 1], padding='SAME')\n    # FLATTEN\n    P = tf.contrib.layers.flatten(P2)\n    # FULLY-CONNECTED without non-linear activation function (not not call softmax).\n    # 6 neurons in output layer. Hint: one of the arguments should be \"activation_fn=None\" \n    Z3 = tf.contrib.layers.fully_connected(P, 7, activation_fn=None)\n    ### END CODE HERE ###\n\n    return Z3","4cb001a8":"def compute_cost(Z3, Y):\n    \"\"\"\n    Computes the cost\n    \n    Arguments:\n    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n    Y -- \"true\" labels vector placeholder, same shape as Z3\n    \n    Returns:\n    cost - Tensor of the cost function\n    \"\"\"\n    \n    ### START CODE HERE ### (1 line of code)\n    cost =  tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Z3, labels=Y))\n    ### END CODE HERE ###\n    \n    return cost","754ea686":"def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n    \"\"\"\n    Creates a list of random minibatches from (X, Y)\n    \n    Arguments:\n    X -- input data, of shape (input size, number of examples) (m, Hi, Wi, Ci)\n    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples) (m, n_y)\n    mini_batch_size - size of the mini-batches, integer\n    seed -- this is only for the purpose of grading, so that you're \"random minibatches are the same as ours.\n    \n    Returns:\n    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n    \"\"\"\n    \n    m = X.shape[0]                  # number of training examples\n    mini_batches = []\n    np.random.seed(seed)\n    \n    # Step 1: Shuffle (X, Y)\n    permutation = list(np.random.permutation(m))\n    shuffled_X = X[permutation,:,:,:]\n    shuffled_Y = Y[permutation,:]\n\n    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n    num_complete_minibatches = math.floor(m\/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n    for k in range(0, num_complete_minibatches):\n        mini_batch_X = shuffled_X[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:,:,:]\n        mini_batch_Y = shuffled_Y[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    # Handling the end case (last mini-batch < mini_batch_size)\n    if m % mini_batch_size != 0:\n        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size : m,:,:,:]\n        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size : m,:]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    return mini_batches","14731c8d":"def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.001,\n          num_epochs = 1000, minibatch_size = 128, print_cost = True):\n    \"\"\"\n    Implements a three-layer ConvNet in Tensorflow:\n    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED\n    \n    Arguments:\n    X_train -- training set, of shape (None, 64, 64, 3)\n    Y_train -- test set, of shape (None, n_y = 6)\n    X_test -- training set, of shape (None, 64, 64, 3)\n    Y_test -- test set, of shape (None, n_y = 6)\n    learning_rate -- learning rate of the optimization\n    num_epochs -- number of epochs of the optimization loop\n    minibatch_size -- size of a minibatch\n    print_cost -- True to print the cost every 100 epochs\n    \n    Returns:\n    train_accuracy -- real number, accuracy on the train set (X_train)\n    test_accuracy -- real number, testing accuracy on the test set (X_test)\n    parameters -- parameters learnt by the model. They can then be used to predict.\n    \"\"\"\n    \n    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n    tf.set_random_seed(1)                             # to keep results consistent (tensorflow seed)\n    seed = 3                                          # to keep results consistent (numpy seed)\n    (m, n_H0, n_W0, n_C0) = X_train.shape             \n    n_y = Y_train.shape[1]                            \n    costs = []                                        # To keep track of the cost\n    \n    # Create Placeholders of the correct shape\n    ### START CODE HERE ### (1 line)\n    X, Y = create_placeholders(n_H0, n_W0, n_C0, n_y)\n    ### END CODE HERE ###\n\n    # Initialize parameters\n    ### START CODE HERE ### (1 line)\n    parameters = initialize_parameters()\n    ### END CODE HERE ###\n    \n    # Forward propagation: Build the forward propagation in the tensorflow graph\n    ### START CODE HERE ### (1 line)\n    Z3 = forward_propagation(X, parameters)\n    ### END CODE HERE ###\n    \n    # Cost function: Add cost function to tensorflow graph\n    ### START CODE HERE ### (1 line)\n    cost = compute_cost(Z3,Y)\n    ### END CODE HERE ###\n    \n    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer that minimizes the cost.\n    ### START CODE HERE ### (1 line)\n    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n    ### END CODE HERE ###\n    \n    # Initialize all the variables globally\n    init = tf.global_variables_initializer()\n     \n    # Start the session to compute the tensorflow graph\n    with tf.Session() as sess:\n        \n        # Run the initialization\n        sess.run(init)\n        \n        # Do the training loop\n        for epoch in range(num_epochs):\n\n            minibatch_cost = 0.\n            num_minibatches = int(m \/ minibatch_size) # number of minibatches of size minibatch_size in the train set\n            seed = seed + 1\n            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n\n            for minibatch in minibatches:\n\n                # Select a minibatch\n                (minibatch_X, minibatch_Y) = minibatch\n                # IMPORTANT: The line that runs the graph on a minibatch.\n                # Run the session to execute the optimizer and the cost, the feedict should contain a minibatch for (X,Y).\n                ### START CODE HERE ### (1 line)\n                _ , temp_cost = sess.run([optimizer, cost], feed_dict={X:minibatch_X, Y:minibatch_Y})\n                ### END CODE HERE ###\n                \n                minibatch_cost += temp_cost \/ num_minibatches\n                \n\n            # Print the cost every epoch\n            if print_cost == True and epoch % 100 == 0:\n                print (\"Cost after epoch %i: %f\" % (epoch, minibatch_cost))\n            if print_cost == True and epoch % 1 == 0:\n                costs.append(minibatch_cost)\n        \n        \n        # plot the cost\n        plt.plot(np.squeeze(costs))\n        plt.ylabel('cost')\n        plt.xlabel('iterations (per tens)')\n        plt.title(\"Learning rate =\" + str(learning_rate))\n        plt.show()\n\n        # Calculate the correct predictions\n        predict_op = tf.argmax(Z3, 1)\n        correct_prediction = tf.equal(predict_op, tf.argmax(Y, 1))\n        \n        # Calculate accuracy on the test set\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n        print(accuracy)\n        train_accuracy = accuracy.eval({X: X_train, Y: Y_train})\n        test_accuracy = accuracy.eval({X: X_test, Y: Y_test})\n        print(\"Train Accuracy:\", train_accuracy)\n        print(\"Test Accuracy:\", test_accuracy)\n                \n        return train_accuracy, test_accuracy, parameters","de0fcfaa":"_, _, parameters = model(X_train, y_train, X_test, y_test)","5d43e194":"**ENCODING LABELS**","e25e5789":"Splitting the data"}}