{"cell_type":{"d31e29f0":"code","b859543e":"code","8509ecd1":"code","db6aff94":"code","96aba1ab":"code","a889296e":"code","dc9a83cb":"code","aff89d84":"code","31947200":"code","d978b7a3":"code","2bb88828":"code","7c6c3108":"code","25eaca83":"code","fbc210ed":"code","7beca03c":"code","afdf8223":"code","ffba695b":"code","39e887b3":"code","7d1b5216":"code","fb2d31b5":"markdown","67d2391d":"markdown","c4cc9726":"markdown","b0056090":"markdown","6004b9f0":"markdown","6726678d":"markdown","fea215b2":"markdown","c968c5dc":"markdown","968cd7dc":"markdown","e464a14a":"markdown","36b39223":"markdown","01ebaabf":"markdown","03c4a42b":"markdown"},"source":{"d31e29f0":"# import necessary libraries\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nplt.rcParams['font.size']= 14\nplt.rcParams['legend.fontsize']=11\nplt.rcParams['lines.linewidth']=2\nplt.rcParams['font.serif'] = 'Time New Roman'","b859543e":"# Load the dataset\nadv_df = pd.read_csv('..\/input\/advertising\/Advertising.csv')\nadv_df.info()","8509ecd1":"adv_df.head()","db6aff94":"adv_df.shape","96aba1ab":"# No missing values\n# Cehck for outliers\n# Explorarity data analysis\nplt.figure()\nsns.pairplot(data=adv_df)","a889296e":"adv_df.corr()['sales']","dc9a83cb":"plt.figure()\nsns.heatmap(adv_df.corr() , annot=True, cmap= \"Blues\")","aff89d84":"X= adv_df.drop('sales', axis=1)\ny= adv_df['sales']","31947200":"X.shape","d978b7a3":"# apply polynomial regression on this dataset\nfrom sklearn.preprocessing import PolynomialFeatures\npoly_converter = PolynomialFeatures(degree=2, include_bias=False)\npoly_features = poly_converter.fit_transform(X)","2bb88828":"poly_features.shape","7c6c3108":"from sklearn.model_selection import train_test_split\nX_train, X_test,y_train, y_test = train_test_split(poly_features, y, test_size=0.3)","25eaca83":"from sklearn.linear_model import LinearRegression\npoly_model= LinearRegression()\npoly_model.fit(X_train, y_train)","fbc210ed":"y_pred_poly = poly_model.predict(X_test)\nresiduals_poly = y_test-y_pred_poly\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nMAE_poly = mean_absolute_error(y_test, y_pred_poly)\nMSE_poly = mean_squared_error(y_test, y_pred_poly)\nRMSE_poly = np.sqrt(MSE_poly)","7beca03c":"X_train_s , X_test_s ,y_train_s , y_test_s  = train_test_split(X, y, test_size=0.3)\ns_model = LinearRegression()\ns_model.fit(X_train_s, y_train_s)\ny_pred_simple = s_model.predict(X_test_s)\nresiduals_simple = y_test_s-y_pred_simple\nMAE_s = mean_absolute_error(y_test_s , y_pred_simple)\nMSE_s = mean_squared_error(y_test_s , y_pred_simple)\nRMSE_s = np.sqrt(MSE_s)","afdf8223":"pd.DataFrame({'simple model': [MAE_s , MSE_s, RMSE_s] , 'polynomial model':[MAE_poly, MSE_poly, RMSE_poly]} , index=['MAE', 'MSE',' RootMSE'])\n","ffba695b":"# CHECK THE RESIDUALS\nf , axes = plt.subplots(2,2, figsize= (10,10))\nsns.distplot(residuals_simple, ax=axes[0,0])\nsns.distplot(residuals_poly, ax=axes[0,1])\naxes[1,0].axhline(y=0, color='r')\nsns.scatterplot(x= y_test_s , y=residuals_simple, ax=axes[1,0])\nplt.axhline(y=0, color='r')\nsns.scatterplot(x= y_test , y=residuals_poly, ax=axes[1,1])","39e887b3":"# finding the optimal degree of the model based on the root squared error of both training dataset and test dataset\ntrain_RMSE=[]\ntest_RMSE=[]\nfor i in range(1,10):\n    poly_converter_new = PolynomialFeatures(degree=i, include_bias=False)\n    poly_features_new = poly_converter_new.fit_transform(X)\n    \n    # train test split\n    X_train_new , X_test_new ,y_train_new , y_test_new = train_test_split(poly_features_new, y, test_size=0.3)\n    \n    # train the model\n    new_model= LinearRegression()\n    new_model.fit(X_train_new, y_train_new)\n    \n    y_pred_train = new_model.predict(X_train_new)\n    y_pred_test = new_model.predict(X_test_new)\n    \n    train_RMSE.append(np.sqrt(mean_squared_error(y_train_new,y_pred_train)))\n    test_RMSE.append(np.sqrt(mean_squared_error(y_test_new, y_pred_test)))\n    \n","7d1b5216":"\n# plot the two errors wth the rise of the degree\ndegree = list(range(1,6))\nplt.figure()\nplt.plot(degree, train_RMSE[:5], label='train RMSE')\nplt.plot(degree, test_RMSE[:5], label='test RMSE')\nplt.xlabel('Polynomial degree')\nplt.ylabel('RMSE')\nplt.legend(loc='upper right')","fb2d31b5":"# train the model","67d2391d":"**The errors in polynomial regression model are less then the errors in simple regression model**","c4cc9726":"# Loading the dataset","b0056090":"It is demonstrated by the correlations that all the features are correlated with sales. the most correlated one is TV advertising.","6004b9f0":"# EDA","6726678d":"# Preprocessing","fea215b2":"**features: TV, radio, newspaper**\n\n**target: sales**","c968c5dc":"# Adjusting model parameters","968cd7dc":"**optimal model is with degree= 2, so the above poly_model is our optimal model **","e464a14a":"# evaluating the model (residuals and metrics) and compare it with simple regression","36b39223":"# Determining the features and target variables","01ebaabf":"**** the residuals are nearly normal\n and they are rendom","03c4a42b":"# split the data to train and test"}}