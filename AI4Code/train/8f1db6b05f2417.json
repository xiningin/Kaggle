{"cell_type":{"41fb887b":"code","9a0678ed":"code","cd2debb1":"code","384f3a2c":"code","9f54e85b":"code","8d9722c8":"code","8cb5cf78":"code","165c7688":"code","7b182470":"code","616e1110":"code","5a124194":"code","d79fbc7e":"code","74808d38":"code","09d6df9c":"code","df5cf8d5":"code","3255c412":"code","1d914ac8":"code","a9fdbe5a":"code","84405c7f":"code","5f556f11":"markdown","af217ee8":"markdown","aa079239":"markdown","5bb5aec7":"markdown","d5b54ed8":"markdown","83b22dae":"markdown","e0c652c6":"markdown","888b4864":"markdown","7fd582d0":"markdown","a003082d":"markdown","d0cc9fa9":"markdown","2626b4d0":"markdown","8258842a":"markdown","61ed775c":"markdown","d0489fee":"markdown","13f6ba75":"markdown","6c681d1f":"markdown","1f130882":"markdown","e163224c":"markdown"},"source":{"41fb887b":"import pandas as pd\nsample_submission = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")\ntest = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")","9a0678ed":"import re\n\n\ndef clean_keyword(keyword):\n    return str(keyword).replace('%20', ' ')\n\n\ndef clean_location(location):\n    return location\n\n\ndef clean_text(text):\n    text = re.sub('http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', text)\n    clean_text = text\n    for character in text:\n        if ord(character) > 126 or ord(character) < 32:\n            clean_text = clean_text.replace(character, '')\n\n    return clean_text","cd2debb1":"test.fillna(value='')\ntrain.fillna(value='')\n\ntest_arr = []\nfor _, row  in test.iterrows():\n    test_arr.append([row['id'], clean_keyword(row['keyword']), clean_location(row['location']), clean_text(row['text'])])\n\nnlp_test_df = pd.DataFrame(test_arr, columns=['id', 'keyword', 'location', 'text'])\n\ntrain_arr = []\nfor _, row in train.iterrows():\n    train_arr.append([row['id'], clean_keyword(row['keyword']), clean_location(row['location']), clean_text(row['text']), row['target']])\n    \nnlp_train_df = pd.DataFrame(train_arr, columns=['id', 'keyword', 'location', 'text', 'target'])","384f3a2c":"import numpy as np\nimport time\nfrom datetime import datetime\n\nfrom sklearn.model_selection import train_test_split\n\nfrom google.cloud import storage\nfrom google.cloud import automl_v1beta1 as automl\n\nfrom google.api_core.gapic_v1.client_info import ClientInfo","9f54e85b":"PROJECT_ID = 'nlp-disaster-tweets-267516'\nBUCKET_NAME = 'nlp-disaster-tweets'\nBUCKET_REGION = 'us-central1'\n\ndataset_display_name = 'kaggle_tweets'\nmodel_display_name = 'kaggle_starter_model1'\n\nstorage_client = storage.Client(project=PROJECT_ID)\n\n# adding ClientInfo here to get the gapic_v1 call in place\nclient = automl.AutoMlClient(client_info=ClientInfo())\n\nprint(f'Starting AutoML notebook at {datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d, %H:%M:%S UTC\")}')","8d9722c8":"def gcs_upload(bucket_name, source_file_name, destination_blob_name):\n    bucket = storage_client.get_bucket(bucket_name)\n    blob = bucket.blob(destination_blob_name)\n    blob.upload_from_filename(source_file_name)\n    print('File {} uploaded to {}'.format(\n        source_file_name,\n        'gs:\/\/' + bucket_name + '\/' + destination_blob_name))\n    \ndef gcs_download(bucket_name, destination_directory, file_name,prefix=None):\n    os.makedirs(destination_directory, exist_ok = True)\n    full_file_path = os.path.join(destination_directory, file_name)\n    blobs = storage_client.list_blobs(bucket_name,prefix=prefix)\n    for blob in blobs:\n        blob.download_to_filename(full_file_path)","8cb5cf78":"bucket = storage.Bucket(storage_client, name=BUCKET_NAME)\nif not bucket.exists():\n    bucket.create(location=BUCKET_REGION)","165c7688":"nlp_train_df[['text', 'target']].to_csv('train.csv', index=False, header=False)","7b182470":"training_gcs_path = 'data\/train.csv'\ngcs_upload(BUCKET_NAME, 'train.csv', training_gcs_path)","616e1110":"dataset_name = 'disaster_tweets'\ndataset_filter = 'display_name=' + dataset_name\nlocation_path = client.location_path(PROJECT_ID, BUCKET_REGION)","5a124194":"print(f'Getting dataset ready at {datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d, %H:%M:%S UTC\")}')\n\nlist_datasets_response = client.list_datasets(location_path, dataset_filter)\n\ndataset_results = []\nfor dataset_response in list_datasets_response:\n    dataset_results.append(dataset_response)\n\ndataset = dataset_results[0]\n    \nif not dataset_results:\n    print('No matching datasets found, creating one...')\n    dataset_metadata = {'classification_type': 'MULTICLASS'}\n    dataset_info = {'display_name': dataset_name,\n                    'text_classification_dataset_metadata': dataset_metadata}\n    print('Creating new dataset...')\n    dataset = client.create_dataset(location_path, dataset_info)\n    data_config = {'gcs_source': {'input_uris': [f'gs:\/\/{BUCKET_NAME}\/{training_gcs_path}']}}\n\n    print('Importing CSV data. This may take a while...')\n    operation = client.import_data(name=dataset.name, input_config=data_config)\n    print(operation)\n\n    result = operation.result()\n    print(result)\n\nprint(dataset)\ndataset_id = dataset.name.split('\/')[-1]\n    ","d79fbc7e":"print(f'Getting model trained at {datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d, %H:%M:%S UTC\")}')\n\nlist_models_response = client.list_models(location_path, dataset_filter)\n\nmodels_results = []\nfor models_response in list_models_response:\n    models_results.append(models_response)\n\nif not models_results:\n    print('No matching models found, creating one...')\n    \n    model_info = {\n        \"display_name\": dataset_name,\n        \"dataset_id\": dataset_id,\n        \"text_classification_model_metadata\": {}\n    }\n    \n    print('Creating and training model...')\n    create_model_response = client.create_model(location_path, model_info)\n    print(create_model_response)\n    result = create_model_response.result()\n    print(result)\n    \n    list_models_response = client.list_models(location_path, dataset_filter)\n\n    models_results = []\n    for model_response in list_models_response:\n        models_results.append(models_reponse)\n        \nif models_results:\n    print('Found model.')\n    model = models_results[0]\nelse:\n    print('Still no models found.')\n    \nprint(model)","74808d38":"if model.deployment_state == model.UNDEPLOYED:\n    print(f'Deploying model: {dataset_name} at {datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d, %H:%M:%S UTC\")}')\n    response = client.deploy_model(name=model.name)\n    while model.deployment_state == model.UNDEPLOYED:\n        time.sleep(120)\n        list_models_response = client.list_models(location_path, dataset_filter)\n        models_results = []\n        for models_response in list_models_response:\n            models_results.append(models_response)\n        if not models_response:\n            print('Model Not Found')\n            quit()\n        model = models_results[0]\n    print(f'Finished deploying model: {dataset_name} at {datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d, %H:%M:%S UTC\")}')\nelse:\n    print(f'Model {dataset_name} is already deployed.')","09d6df9c":"input_col_name = 'text'\nthreshold = 0.5","df5cf8d5":"print(f'Beginning Predictions at {datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d, %H:%M:%S UTC\")}')\n\nprediction_client = automl.PredictionServiceClient()\n\npredictions_list = []\ncorrect = 0\ntotal_test_size = len(nlp_test_df)\n\nfor i in range(total_test_size):\n    row = nlp_test_df.iloc[i]\n    snippet = row[input_col_name]\n    \n    payload = {\"text_snippet\": {\"content\": snippet,\n                            \"mime_type\": \"text\/plain\"}}\n    params = {}\n    response = prediction_client.predict(model.name, payload, params)\n\n    for result in response.payload:\n        if result.classification.score >= threshold:\n            prediction = {'score': result.classification.score,\n                          'class': result.display_name,\n                          'text': snippet}\n            predictions_list.append(prediction)\n    time.sleep(0.3)\n\npredictions_df = pd.DataFrame(predictions_list)\n\nprint(f'Finished getting predictions at {datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d, %H:%M:%S UTC\")}')","3255c412":"print(predictions_df.head())","1d914ac8":"if model.deployment_state == model.DEPLOYED:\n    print(f'Undepploying Model: {dataset_name} at {datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d, %H:%M:%S UTC\")}')\n    response = client.undeploy_model(name=model.name)\n    while model.deployment_state == model.DEPLOYED:\n        time.sleep(120)\n        list_models_response = client.list_models(location_path, dataset_filter)\n        models_results = []\n        for models_response in list_models_response:\n            models_results.append(models_response)\n        if not models_response:\n            print('Model Not Found')\n            quit()\n        model = models_results[0]\n    print(f'Finished Undeploying Model: {dataset_name} at {datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d, %H:%M:%S UTC\")}')\nelse:\n    print(f'Model {dataset_name} is already undeployed.')","a9fdbe5a":"submission_df = pd.concat([nlp_test_df['id'], predictions_df['class']], axis=1)\nsubmission_df = submission_df.rename(columns={'class': 'target'})\nsubmission_df.to_csv(\"submission.csv\", index=False, header=True)","84405c7f":"! ls -l submission.csv","5f556f11":"# Natural Language Processing with Disaster Tweets","af217ee8":"### Retrieving the Dataset\nCheck to see if the dataset exists and if it does retrieve it, else create it.","aa079239":"### Uploading to Google Cloud Services","5bb5aec7":"The test and training data are cleaned and put back into Pandas DataFrames for re-use.","d5b54ed8":"### Deploying the Model\nThis will incur fees, so make sure to only deploy when ready.","83b22dae":"### Retrieving Storage Buckets","e0c652c6":"## Importing Relevant Services and Google Cloud","888b4864":"### General Information for Accessing AutoML\nThe dataset name is uesd as a display name filter for retrieving various things from Google Cloud. The dataset filter is the filter that is used for doing so. The location path is the location to this particular project location.","7fd582d0":"## Using the AutoML Model\nDeploying, making predictions, and undeploying.","a003082d":"### Making Predictions\nThe model must be deployed before predictions can be made, but once the model is trained it doesn't need to be re-trained.","d0cc9fa9":"## Setting Up Clients","2626b4d0":"### Exporting to CSV","8258842a":"## Preparing for Submission","61ed775c":"## Creating the AutoML Model","d0489fee":"## Google Cloud Set-Up\nAutoML requires testing and training data to be hosted in Google Cloud storage. For this, we'll create some functions to handle uploading and downloading for us.","13f6ba75":"## Cleaning the data\nUnclean values are present in keyword, location, and text.\n\nIn the keyword column there are two primary dirty values we need to fix. There are missing values which are converted to nan. There are also multi-word keywords that are joined with '%20' which we want to replace with a space.\n\nIn the location column there are also two primary dirty values. There are missing values which are also converted to nan. There are also invalid locations, which we currently do not check for and let slide.\n\nIn the text column there are a few options for dirty values. There are no missing values, as all tweets have text. There are hyperlinks within tweets which we remove using a regular expression. There are also invalid characters, characters with accents, and characters that complicate analysis without adding value to the tweet; these we simply remove, as a majority of the time they are superfluous and not necessary for the text of the tweet.","6c681d1f":"## Reading the Data","1f130882":"### Retrieving the Training Model\nRetrieve the training model if it exists, else create it.","e163224c":"### Undeploying the Model\nDo this to stop GCS charges."}}