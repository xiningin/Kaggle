{"cell_type":{"c6c9341c":"code","ef07e153":"code","9253133c":"code","40a35769":"code","a20c93bd":"code","ad95ad8b":"code","c91d1a03":"code","cbc48611":"code","f5ef4f1e":"code","869b121a":"code","30dfe06e":"code","5f469679":"code","41b6c855":"code","e94ba51d":"code","017a34dd":"code","5ee28a18":"code","2df6114e":"code","a7b0d35d":"code","33418233":"code","051ceae7":"code","69a18455":"code","bfe535c9":"code","931d9d9d":"code","fd292c12":"code","63e48cc5":"code","7f0f839c":"code","52085fd4":"code","40eea78d":"code","ba21ff8d":"code","883ba49a":"code","00075ae5":"code","dc4490d8":"code","d746d536":"code","c17e6690":"code","e7fdf2cc":"code","5d3cccd9":"code","04f48fcc":"code","2953fbba":"code","aca84078":"code","827f075b":"code","bdb5a2ee":"code","ec49f128":"code","174a36cd":"code","45a1e526":"code","138bbc7a":"code","974dcfe9":"code","6f8d03ea":"code","2cc6815b":"code","3d7d426a":"code","81151505":"code","b590402e":"code","c05fa05b":"code","8827db2f":"code","45503eeb":"code","bb600755":"code","d5cbf568":"code","319d7002":"code","9b8b3792":"code","4c40dd4d":"code","64fbaaba":"code","4be4f6fd":"code","123c04ee":"code","6e7acd45":"code","16381404":"code","a5672d3e":"code","24a1ecc9":"code","bc545418":"code","602ceaee":"code","7050e753":"code","f48ca785":"code","1830e100":"code","2a89dcaf":"code","138e1fcc":"markdown","6853882d":"markdown","d4dbf6c6":"markdown","565a2c2e":"markdown","fceca559":"markdown","2303ad86":"markdown","c3ec41b3":"markdown","86bb0489":"markdown","d2ae65df":"markdown","4544fcf0":"markdown","c865ea6c":"markdown","066c0a0a":"markdown","c549e9e6":"markdown","d43565aa":"markdown","1df2fc14":"markdown","06b4ae7d":"markdown","ed1de430":"markdown","aef630e6":"markdown","9be75f48":"markdown","d6f72425":"markdown","59eb283b":"markdown","ed0a29de":"markdown","3a2c3f84":"markdown","7ab59a36":"markdown","e26566ee":"markdown","e813ac32":"markdown","38adffff":"markdown","1c8faa79":"markdown","531706f0":"markdown","b2e71b93":"markdown","2e82c4d5":"markdown","fa3c60e2":"markdown","137b253b":"markdown","c888ebfe":"markdown","45b6332f":"markdown","633c67e8":"markdown","8e07c75f":"markdown","becb1c4a":"markdown","673ed4fc":"markdown","ace4ba48":"markdown","53a43516":"markdown"},"source":{"c6c9341c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ef07e153":"import warnings\nwarnings.filterwarnings('ignore')","9253133c":"fname = \"..\/input\/powerlifting-database\/openpowerlifting.csv\"\ndf = pd.read_csv(fname, parse_dates=True)\ndf.head()","40a35769":"df.shape","a20c93bd":"df.info()","ad95ad8b":"df.describe()","c91d1a03":"# What percent of each feature is missing?\n\nlst = []\nfor cname in df.columns.tolist():\n    percentage = round((100*df[cname].isna().sum()\/df[cname].isna().count()), 2)\n    lst.append((cname, percentage))\n\nlst.sort(reverse=True, key = lambda x: x[1])\n\nfor element in lst:\n    print(element[0] + \": \" + str(element[1]) + \"%\")","cbc48611":"# Removing Deadlift1Kg, Deadlift2Kg, Deadlift3Kg, TotalKg and Place because target leakage\n\ndf = df.drop([\"Deadlift1Kg\", \"Deadlift2Kg\", \"Deadlift3Kg\", \"TotalKg\", \"Place\"], axis=1)","f5ef4f1e":"# Removing the lifter metrics because they use Total in their calculation\n\ndf = df.drop([\"Wilks\", \"Glossbrenner\", \"IPFPoints\", \"McCulloch\"], axis=1)","869b121a":"# Remove Squat4Kg, Deadlift4Kg, and Bench4Kg because they're rarely used\n\ndf = df.drop([\"Squat4Kg\", \"Deadlift4Kg\", \"Bench4Kg\"], axis=1)","30dfe06e":"# Remove AgeClass and WeightClass because they can be substituted by Age and BodyWeightKg\n\ndf = df.drop([\"AgeClass\", \"WeightClassKg\"], axis=1)","5f469679":"# Remove MeetName because I don't want it to affect predictions\n# Remove Event because it'll all be SBD anyways\n\ndf = df.drop([\"MeetName\", \"Event\"], axis=1)","41b6c855":"# Target is Best3DeadliftKg, so remove any rows where it is NaN or negative\n# Do the same for these other rows\n# I can be choosy when dropping rows with NaNs because there's so much data\n\nremoveNANCols = [\"BodyweightKg\", \"Age\", \"Best3DeadliftKg\", \"Best3SquatKg\", \"Best3BenchKg\"]\n\nfor col in removeNANCols:\n    df = df.loc[(df[col].isna() == False) & (df[col] > 0)]","e94ba51d":"# Drop Name, even though there are name repeats\n# I'm going to consider each meet entrant a new person\n# Yes, this will skew results towards those who were in many meets, but I hope it works out fine\n\nprint(df[\"Name\"].value_counts())\n\ndf = df.drop(\"Name\", axis=1)","017a34dd":"# Make the values in these columns easier to work with\n# Removes NaNs in the process\n# Categorizing these columns removes the possible target leakage from the deadlift columns\n# Creates a separate feature for if the lift attempt was failed\/successful\/unknown outcome\n\nattemptCols = [\"Squat1Kg\", \"Squat2Kg\", \"Squat3Kg\", \"Bench1Kg\", \"Bench2Kg\", \"Bench3Kg\"]\n\nimport math\n\ndef attemptTransformer(datapoint):\n    if math.isnan(datapoint):\n        return \"Unknown\"\n    elif datapoint <= 0:\n        return \"Fail\"\n    else:\n        return \"Success\"\n    \nfor col in attemptCols:\n    df[col] = df[col].apply(lambda x: attemptTransformer(x))","5ee28a18":"# Replace NaN with No\n\ndf[\"Tested\"] = df[\"Tested\"].fillna(\"No\")","2df6114e":"# Use simple imputer to impute these categorical features\nfrom sklearn.impute import SimpleImputer\n\nsimpImputeCols = [\"Country\", \"MeetState\", \"Division\"]\n\nsimpImputer = SimpleImputer(strategy=\"constant\")\ndf[simpImputeCols] = simpImputer.fit_transform(df[simpImputeCols])","a7b0d35d":"# Check if there are any NaNs left\n\ndf.isna().any()","33418233":"# See how many training examples and columns we're left with\n\ndf.shape","051ceae7":"# Reindex\n\ndf.index = np.arange(df.shape[0])","69a18455":"# Get X and y\n\nX = df.drop(\"Best3DeadliftKg\", axis=1)\ny = df[\"Best3DeadliftKg\"]","bfe535c9":"# Make lists of the numerical and categorical columns\ncategorical = [cname for cname in X.columns.tolist() if X[cname].dtype == \"object\"]\nnumerical = [cname for cname in X.columns.tolist() if X[cname].dtype == \"float64\"]","931d9d9d":"# Create a correlation heatmap between the numerical values\n# Source: https:\/\/seaborn.pydata.org\/examples\/many_pairwise_correlations.html\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Compute the correlation matrix\ncorr = X[numerical].corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(10,10))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, annot=True, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","fd292c12":"# Create histograms for each numerical feature to see their distributions\n\nfor cname in numerical:\n    sns.distplot(a=X[cname], kde=False)\n    plt.show()","63e48cc5":"# See distributions based on Sex\nfor cname in numerical:\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n    \n    male_cname = X.loc[X[\"Sex\"] == \"M\"].index\n    female_cname = X.loc[X[\"Sex\"] == \"F\"].index\n    \n    sns.distplot(a=X[cname].iloc[male_cname], kde=False, ax=ax1)\n    ax1.title.set_text('Male')\n    sns.distplot(a=X[cname].iloc[female_cname], kde=False, ax=ax2)\n    ax2.title.set_text('Female')\n    \n    plt.show()","7f0f839c":"# Create scatter plots for each numerical value against the target\n\nfor cname in numerical:\n    \n    sns.regplot(x=X[cname], y=y, order=3, scatter_kws={'s': .1}, line_kws={'color': 'black'})\n    \n    plt.show()","52085fd4":"# How many unique values does each categorical feature have?\n\nfor cname in categorical:\n    print(cname + \": \" + str(df[cname].nunique()))","40eea78d":"pd.set_option('display.max_columns', 50)\nX.head()","ba21ff8d":"# Create barplots for each categorical feature to see their distributions\n\nfor cname in categorical:\n    valueCounts = X[cname].value_counts()\n    sns.barplot(valueCounts.index, valueCounts).set_title(cname)\n    plt.show()","883ba49a":"X[\"Country\"].value_counts().head()","00075ae5":"X[\"MeetCountry\"].value_counts().head()","dc4490d8":"X[\"MeetState\"].value_counts().head()","d746d536":"X[\"Division\"].value_counts().head()","c17e6690":"X[\"Federation\"].value_counts().head()","e7fdf2cc":"# Look at powerlifter's mean deadlifts over time\n\nsns.lineplot(x=X[\"Date\"], y=y, linewidth=.1, estimator='mean')","5d3cccd9":"# I don't want Date to be a factor in predicting deadlifts\n# Also it doesn't seem to have any effect on Best Deadlift\n# Drop Date\nX = X.drop(\"Date\", axis=1)\ncategorical.remove(\"Date\")","04f48fcc":"# See the boxplots for categorical features against Best3DeadliftKg\n# Don't make boxplots for high cardinality features because their plots are hard to interpret\n\nfor cname in categorical:\n    if X[cname].nunique() < 15:\n        sns.boxplot(x=X[cname], y=y)\n        plt.show()","2953fbba":"# Look at high cardinality categorical features\n# See the top 5 and bottom 5 categorical values by median\n\nfor cname in categorical:\n    if X[cname].nunique() >= 15:\n        print(df.groupby([cname])[\"Best3DeadliftKg\"].agg(func='median').sort_values(ascending=False))\n        print(\"\\n\") ","aca84078":"X[categorical].head()","827f075b":"# One-Hot Encode columns with low cardinality\n\ntoOHEnc = [\"Sex\", \"Equipment\", \"Tested\", \"Squat1Kg\", \"Squat2Kg\", \"Squat3Kg\", \"Bench1Kg\", \"Bench2Kg\", \"Bench3Kg\"]\n\nfor cname in toOHEnc:\n    X_one_hot = pd.get_dummies(X[cname], prefix=cname)\n    X = pd.concat([X, X_one_hot], axis=1)\n    X = X.drop(cname, axis=1)","bdb5a2ee":"# Target Encode columns with high cardinality\n\ntoTargetEnc = [\"Division\", \"Country\", \"Federation\", \"MeetCountry\", \"MeetState\"]\nfrom category_encoders import TargetEncoder\n\n# NOTE: Have to do this on a specific training split to avoid target leakage","ec49f128":"X[\"Squat_Fail_Total\"] = X[\"Squat1Kg_Fail\"] + X[\"Squat2Kg_Fail\"] + X[\"Squat3Kg_Fail\"]\nX[\"Squat_Success_Total\"] = X[\"Squat1Kg_Success\"] + X[\"Squat2Kg_Success\"] + X[\"Squat3Kg_Success\"]\nX[\"Squat_Unknown_Total\"] = X[\"Squat1Kg_Unknown\"] + X[\"Squat2Kg_Unknown\"] + X[\"Squat3Kg_Unknown\"]\n\nX[\"Bench_Fail_Total\"] = X[\"Bench1Kg_Fail\"] + X[\"Bench2Kg_Fail\"] + X[\"Bench3Kg_Fail\"]\nX[\"Bench_Success_Total\"] = X[\"Bench1Kg_Success\"] + X[\"Bench2Kg_Success\"] + X[\"Bench3Kg_Success\"]\nX[\"Bench_Unknown_Total\"] = X[\"Bench1Kg_Unknown\"] + X[\"Bench2Kg_Unknown\"] + X[\"Bench3Kg_Unknown\"]","174a36cd":"X[\"Fail_Total\"] = X[\"Squat_Fail_Total\"]+X[\"Bench_Fail_Total\"]\nX[\"Success_Total\"] = X[\"Squat_Success_Total\"]+X[\"Bench_Success_Total\"]\nX[\"Unknown_Total\"] = X[\"Squat_Unknown_Total\"]+X[\"Bench_Unknown_Total\"]","45a1e526":"X[\"AgePerWeight\"] = X[\"Age\"]\/X[\"BodyweightKg\"]\nX[\"BestSquatBench\"] = X[\"Best3SquatKg\"]*X[\"Best3BenchKg\"]\nX[\"AgeWeightBenchSquat\"] = X[\"AgePerWeight\"]*X[\"BestSquatBench\"]","138bbc7a":"X.columns","974dcfe9":"# Scale the numerical data for the models that require it (like KNearestNeighbors)\nfrom sklearn.preprocessing import StandardScaler\nscale = StandardScaler()\n\nnumerical.extend([\"BestSquatBench\", \"AgeWeightBenchSquat\"]) # Not extending AgePerWeight bc it'll always be <1\n\nX[numerical] = scale.fit_transform(X[numerical])","6f8d03ea":"def get_preprocessed_cv_data(train_X, val_X, train_y, val_y):\n    ###############################################################################################################\n    # Data Cleaning\n    # Filter rows based on these conditions\n    for col in removeNANCols:\n        train_X = train_X.loc[(train_X[col].isna() == False) & (train_X[col] > 0)]\n        val_X = val_X.loc[(val_X[col].isna() == False) & (val_X[col] > 0)]\n\n    # Filter the same rows out of y's\n    \n    train_y_toRemove = train_y.drop(train_X.index, axis=0)\n    train_y = train_y.drop(train_y_toRemove.index, axis=0)\n    \n    val_y_toRemove = val_y.drop(val_X.index, axis=0)\n    val_y = val_y.drop(val_y_toRemove.index, axis=0)\n    \n    # Reindex\n    train_X.index = np.arange(train_X.shape[0])\n    val_X.index = np.arange(train_X.shape[0], train_X.shape[0]+val_X.shape[0])\n    \n    train_y.index = train_X.index\n    val_y.index = val_X.index\n    \n    # Make these rows easier to work with\n    for col in attemptCols:\n        train_X[col] = train_X[col].apply(lambda x: attemptTransformer(x))\n        val_X[col] = val_X[col].apply(lambda x: attemptTransformer(x))\n    \n    # Imputation\n    train_X[\"Tested\"] = train_X[\"Tested\"].fillna(\"No\")\n    val_X[\"Tested\"] = val_X[\"Tested\"].fillna(\"No\")\n    \n    simpImputer = SimpleImputer(strategy=\"constant\")\n    train_X[simpImputeCols] = simpImputer.fit_transform(train_X[simpImputeCols])\n    val_X[simpImputeCols] = simpImputer.transform(val_X[simpImputeCols])\n        \n    ###############################################################################################################\n    # One-Hot Encode the columns with low cardinality\n    for cname in toOHEnc:\n        train_X_one_hot = pd.get_dummies(train_X[cname], prefix=cname)\n        train_X = pd.concat([train_X, train_X_one_hot], axis=1)\n        train_X = train_X.drop(cname, axis=1)\n        \n        val_X_one_hot = pd.get_dummies(val_X[cname], prefix=cname)\n        val_X = pd.concat([val_X, val_X_one_hot], axis=1)\n        val_X = val_X.drop(cname, axis=1)\n    \n    # Sometimes there isn't a success or fail or unknown value present in the split, so the column is never made\n    # These columns are necessary for the later feature engineering\n    for col in [\"Squat1Kg_Fail\", \"Squat1Kg_Success\", \"Squat1Kg_Unknown\", \"Bench1Kg_Fail\", \"Bench1Kg_Success\", \"Bench1Kg_Unknown\",\n               \"Squat2Kg_Fail\", \"Squat2Kg_Success\", \"Squat2Kg_Unknown\", \"Bench2Kg_Fail\", \"Bench2Kg_Success\", \"Bench2Kg_Unknown\",\n               \"Squat3Kg_Fail\", \"Squat3Kg_Success\", \"Squat3Kg_Unknown\", \"Bench3Kg_Fail\", \"Bench3Kg_Success\", \"Bench3Kg_Unknown\"]:\n        \n        if col not in train_X.columns.tolist():\n            train_X[col] = 0\n        if col not in val_X.columns.tolist():\n            val_X[col] = 0\n            \n    ###############################################################################################################\n    # Target Encode the columns with high cardinality\n        \n    enc = TargetEncoder(cols=toTargetEnc, handle_missing=\"value\", handle_unknown=\"value\")\n    train_X = enc.fit_transform(train_X, train_y)\n    val_X = enc.transform(val_X)\n    \n    ###############################################################################################################\n    # Feature Engineering\n    \n    train_X[\"Squat_Fail_Total\"] = train_X[\"Squat1Kg_Fail\"] + train_X[\"Squat2Kg_Fail\"] + train_X[\"Squat3Kg_Fail\"]\n    train_X[\"Squat_Success_Total\"] = train_X[\"Squat1Kg_Success\"] + train_X[\"Squat2Kg_Success\"] + train_X[\"Squat3Kg_Success\"]\n    train_X[\"Squat_Unknown_Total\"] = train_X[\"Squat1Kg_Unknown\"] + train_X[\"Squat2Kg_Unknown\"] + train_X[\"Squat3Kg_Unknown\"]\n\n    train_X[\"Bench_Fail_Total\"] = train_X[\"Bench1Kg_Fail\"] + train_X[\"Bench2Kg_Fail\"] + train_X[\"Bench3Kg_Fail\"]\n    train_X[\"Bench_Success_Total\"] = train_X[\"Bench1Kg_Success\"] + train_X[\"Bench2Kg_Success\"] + train_X[\"Bench3Kg_Success\"]\n    train_X[\"Bench_Unknown_Total\"] = train_X[\"Bench1Kg_Unknown\"] + train_X[\"Bench2Kg_Unknown\"] + train_X[\"Bench3Kg_Unknown\"]\n\n    train_X[\"Fail_Total\"] = train_X[\"Squat_Fail_Total\"]+train_X[\"Bench_Fail_Total\"]\n    train_X[\"Success_Total\"] = train_X[\"Squat_Success_Total\"]+train_X[\"Bench_Success_Total\"]\n    train_X[\"Unknown_Total\"] = train_X[\"Squat_Unknown_Total\"]+train_X[\"Bench_Unknown_Total\"]\n\n    train_X[\"AgePerWeight\"] = train_X[\"Age\"]\/train_X[\"BodyweightKg\"]\n    train_X[\"BestSquatBench\"] = train_X[\"Best3SquatKg\"]*train_X[\"Best3BenchKg\"]\n    train_X[\"AgeWeightBenchSquat\"] = train_X[\"AgePerWeight\"]*train_X[\"BestSquatBench\"]\n    \n    \n    val_X[\"Squat_Fail_Total\"] = val_X[\"Squat1Kg_Fail\"] + val_X[\"Squat2Kg_Fail\"] + val_X[\"Squat3Kg_Fail\"]\n    val_X[\"Squat_Success_Total\"] = val_X[\"Squat1Kg_Success\"] + val_X[\"Squat2Kg_Success\"] + val_X[\"Squat3Kg_Success\"]\n    val_X[\"Squat_Unknown_Total\"] = val_X[\"Squat1Kg_Unknown\"] + val_X[\"Squat2Kg_Unknown\"] + val_X[\"Squat3Kg_Unknown\"]\n\n    val_X[\"Bench_Fail_Total\"] = val_X[\"Bench1Kg_Fail\"] + val_X[\"Bench2Kg_Fail\"] + val_X[\"Bench3Kg_Fail\"]\n    val_X[\"Bench_Success_Total\"] = val_X[\"Bench1Kg_Success\"] + val_X[\"Bench2Kg_Success\"] + val_X[\"Bench3Kg_Success\"]\n    val_X[\"Bench_Unknown_Total\"] = val_X[\"Bench1Kg_Unknown\"] + val_X[\"Bench2Kg_Unknown\"] + val_X[\"Bench3Kg_Unknown\"]\n\n    val_X[\"Fail_Total\"] = val_X[\"Squat_Fail_Total\"]+val_X[\"Bench_Fail_Total\"]\n    val_X[\"Success_Total\"] = val_X[\"Squat_Success_Total\"]+val_X[\"Bench_Success_Total\"]\n    val_X[\"Unknown_Total\"] = val_X[\"Squat_Unknown_Total\"]+val_X[\"Bench_Unknown_Total\"]\n\n    val_X[\"AgePerWeight\"] = val_X[\"Age\"]\/val_X[\"BodyweightKg\"]\n    val_X[\"BestSquatBench\"] = val_X[\"Best3SquatKg\"]*val_X[\"Best3BenchKg\"]\n    val_X[\"AgeWeightBenchSquat\"] = val_X[\"AgePerWeight\"]*val_X[\"BestSquatBench\"]\n    \n    return train_X, val_X, train_y, val_y","2cc6815b":"# Create 5 Cross Validation folds and keep track of the preprocessed train\/test sets they create\n# Can speed up each following cell because I don't have to run the folds every time\n# Just have to fit to each of the 5 sets of data and get the average score at the end\n# Can make a function that takes a model and a bunch of training\/validation sets and returns the average score\n\n# This multiplies the RAM used x5 but I shouldn't go over 16GB\n\nfrom sklearn.model_selection import KFold\n\n# I'm choosing to score with MAE because it's more interpretable\nfrom sklearn.metrics import mean_absolute_error\n\ndef get_folds(X, y):\n\n    folds = []\n    kf = KFold(n_splits=5)\n    for train_index, val_index in kf.split(X):\n        train_X, val_X, train_y, val_y = X.iloc[train_index], X.iloc[val_index], y.iloc[train_index], y.iloc[val_index]\n        train_X, val_X, train_y, val_y = get_preprocessed_cv_data(train_X, val_X, train_y, val_y)\n        folds.append((train_X, val_X, train_y, val_y))\n        \n    for fold in folds:\n        train_X, val_X, train_y, val_y = fold[0], fold[1], fold[2], fold[3]\n        print(\"Shapes:\", train_X.shape, train_y.shape, val_X.shape, val_y.shape)\n        \n    return folds\n\n\ndef get_fold_scores(folds, model):\n    \n    totalError = 0\n    for fold in folds:\n        train_X, val_X, train_y, val_y = fold[0], fold[1], fold[2], fold[3]\n        model.fit(train_X, train_y)\n        predictions = model.predict(val_X)\n        totalError += mean_absolute_error(val_y, predictions)\n    \n    return totalError\/5","3d7d426a":"# Get a fresh X and y\ndf = pd.read_csv(fname)\n\n# Drop Useless Columns\ndf = df.drop([\"TotalKg\", \"Place\", \"Wilks\", \"Glossbrenner\", \"IPFPoints\", \"McCulloch\", \"Squat4Kg\",\n            \"Bench4Kg\", \"Deadlift1Kg\", \"Deadlift2Kg\", \"Deadlift3Kg\", \"Deadlift4Kg\", \"AgeClass\",\n              \"WeightClassKg\", \"MeetName\", \"Event\", \"Name\", \"Date\"], axis=1)\n\n# Set up Best3DeadliftKg to have real positive values\ndf = df.loc[(df[\"Best3DeadliftKg\"].isna() == False) & (df[\"Best3DeadliftKg\"] > 0)]\n\n# Reindex\ndf.index = np.arange(df.shape[0])\n\n# Remove Best3DeadliftKg from removeNANCols because I just removed its NANs\nif \"Best3DeadliftKg\" in removeNANCols:\n        removeNANCols.remove(\"Best3DeadliftKg\")\n\nX = df.drop(\"Best3DeadliftKg\", axis=1)\ny = df[\"Best3DeadliftKg\"]\n\n# Shuffle the data because I think it's in some kind of order\n# Also makes my KFold basically a StratifiedKFold\nfrom sklearn.utils import shuffle\nX, y = shuffle(X, y)","81151505":"# How much data do I start off with before preprocessing?\n\nprint(X.shape, y.shape)","b590402e":"# Get the 5 CV folds to test on each model\n\nfolds = get_folds(X, y)","c05fa05b":"# Support Vector Machines, K Nearest Neighbors, and Ridge Regression rely on scaled data\n\nscaledFolds = folds.copy()\n\nfor fold in scaledFolds:\n    train_X, val_X, train_y, val_y = fold[0], fold[1], fold[2], fold[3]\n    scale = StandardScaler()\n    train_X[numerical] = scale.fit_transform(train_X[numerical])\n    val_X[numerical] = scale.transform(val_X[numerical])","8827db2f":"# Model: Linear Regression\nfrom sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression()\n\n# print(get_fold_scores(folds, model))\n\n# Previous Output:\n# 15.77874015086079","45503eeb":"# Model: Ridge Regression\nfrom sklearn.linear_model import Ridge\n\nmodel = Ridge()\n\n# print(get_fold_scores(scaledFolds, model))\n\n# Previous Output:\n# 15.778774089924635","bb600755":"# Model: Decision Tree Regressor\nfrom sklearn.tree import DecisionTreeRegressor\n\nmodel = DecisionTreeRegressor()\n\n# print(get_fold_scores(folds, model))\n\n# Previous Output:\n# 19.417925633468293","d5cbf568":"# Model: K Nearest Neighbors Regressor\nfrom sklearn.neighbors import KNeighborsRegressor\n\nmodel = KNeighborsRegressor()\n\n# print(get_fold_scores(scaledFolds, model))\n\n# Previous Output:\n# 19.946031144268616","319d7002":"# Model: XGBoost Regressor\nfrom xgboost import XGBRegressor\n\nmodel = XGBRegressor()\n\n# print(get_fold_scores(folds, model))\n\n# Previous Output:\n# 14.679437128510765","9b8b3792":"# Model: Random Forest Regressor\nfrom sklearn.ensemble import RandomForestRegressor\n\nmodel = RandomForestRegressor()\n\n# print(get_fold_scores(folds, model))\n\n# Previous Output:\n# 14.05798364898988","4c40dd4d":"# Model: Support Vector Regressor\nfrom sklearn.svm import LinearSVR\n\n# Not using base SVR() because of the quadratic fit time with respect to observations\n\nmodel = LinearSVR()\n\n# print(get_fold_scores(scaledFolds, model))\n\n# Previous Output:\n# 21.95390","64fbaaba":"# Find the best hyperparamters for the chosen model using RandomizedSearchCV\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import PredefinedSplit\n\n# Using RandomizedSearchCV instead of GridSearchCV because it does fewer iterations through parameter space\n# The cost is it might not find the absolute best hyperparameters\nfrom sklearn.model_selection import RandomizedSearchCV\n\ndef find_best_hyperparameters():\n\n    # Define the chosen model\n    model = XGBRegressor()\n\n    # Value ideas from: https:\/\/machinelearningmastery.com\/tune-number-size-decision-trees-xgboost-python\/\n    # and https:\/\/machinelearningmastery.com\/tune-learning-rate-for-gradient-boosting-with-xgboost-in-python\/\n    paramfield = {'learning_rate': [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3],\n                  'n_estimators': range(50, 400, 50),\n                  \"max_depth\": range(1, 11, 2),\n                 }\n\n    totalScore = 0\n    bestScore = 0\n    bestParams = {}\n\n    # Use 5 random train\/test splits instead of KFolds\n    for _ in range(5):\n        # Create train\/test splits and preprocess using previously defined function\n        train_X, val_X, train_y, val_y = train_test_split(X, y, train_size=.8, test_size=.2)\n        train_X, val_X, train_y, val_y = get_preprocessed_cv_data(train_X, val_X, train_y, val_y)\n\n        train_length = train_X.shape[0]\n        val_length = val_X.shape[0]\n\n        new_X = pd.concat([train_X, val_X], axis=0)\n        new_y = pd.concat([train_y, val_y], axis=0)\n\n\n        # Want to find the best hyperparameters for this specific train\/test split\n        # Create a list where train data indices are -1 and validation data indices are 0\n        # Source: https:\/\/stackoverflow.com\/questions\/31948879\/using-explicit-predefined-validation-set-for-grid-search-with-sklearn\n        split_index = [-1 if i<=train_length else 0 for i in range(train_length+val_length)]\n\n        ps = PredefinedSplit(test_fold = split_index)\n\n        # Scoring metric is from the dataset description\n        # Tries out all permutations of parameters in paramField on this split\n        clf = RandomizedSearchCV(model, paramfield, scoring=\"neg_mean_absolute_error\", cv=ps, n_iter=10)\n\n        # Fit the classifier using the preprocessed X and y\n        # It will know what the validation set is bc the PredefinedSplit\n        clf.fit(new_X, new_y)\n\n        # Get the best score from this split coming from the best params\n        score = clf.best_score_\n\n        # Add to the totalScore to later return the average score\n        totalScore += score\n\n        # If this is the best split so far, keep track of the found hyperparameters\n        if score >= bestScore:\n            bestParams = clf.best_params_\n\n        print(score, clf.best_params_)\n\n# find_best_hyperparameters()\n\n# Previous Output:\n# -14.647675587224361 {'n_estimators': 100, 'max_depth': 7, 'learning_rate': 0.2}\n# -14.32331298119558 {'n_estimators': 300, 'max_depth': 9, 'learning_rate': 0.2}\n# -14.34693246644678 {'n_estimators': 250, 'max_depth': 9, 'learning_rate': 0.2}\n# -14.273737167689513 {'n_estimators': 300, 'max_depth': 9, 'learning_rate': 0.1}\n# -14.278972928620703 {'n_estimators': 350, 'max_depth': 9, 'learning_rate': 0.2}","4be4f6fd":"# Fit the model on another random train\/test split using tuned hyperparameters\n\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, train_size=.8, test_size=.2)\ntrain_X, val_X, train_y, val_y = get_preprocessed_cv_data(train_X, val_X, train_y, val_y)\n\n# From RandomizedSearchCV: {'n_estimators': 300, 'max_depth': 9, 'learning_rate': 0.1}\nmodel = XGBRegressor(n_estimators = 300, max_depth = 9, learning_rate = 0.1)\nmodel.fit(train_X, train_y)\npredictions = model.predict(val_X)\n        \nprint(mean_absolute_error(val_y, predictions))","123c04ee":"# Use L1 Regularization to see the most important features\n# The less important coefficients will be regularized to 0\n# Make alpha higher to remove more features\n\nfrom sklearn.linear_model import Lasso\nfrom sklearn.feature_selection import SelectFromModel\n\n# Lasso Relies on scaled data\nscaled_train_X = train_X.copy()\nscaled_train_X[numerical] = StandardScaler().fit_transform(train_X[numerical])\n\nlasso = Lasso(alpha=1).fit(scaled_train_X, train_y)\n\n# Select the nonzero coefficients\nselector = SelectFromModel(lasso, prefit=True)\n\ntrain_X_new = selector.transform(scaled_train_X)","6e7acd45":"# Create a list of which features were selected\n\nselected_df = pd.DataFrame(selector.inverse_transform(train_X_new),\n                          index=np.arange(train_X_new.shape[0]),\n                          columns=train_X.columns.tolist())\n\nselected_features = selected_df.columns[selected_df.sum() != 0]\n\nselected_features","16381404":"# See which features should be removed according to this method\n\nfor col in train_X.columns.tolist():\n    if col not in selected_features:\n        print(col)","a5672d3e":"# Different aproach to see feature importance\n# Permutation importance\n\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(model).fit(val_X, val_y)\neli5.show_weights(perm, top=100, feature_names = val_X.columns.tolist())","24a1ecc9":"# Test the model's MAE score on only the selected features from L1 Regularization\n\nmodel_selected = XGBRegressor(n_estimators = 300, max_depth = 9, learning_rate = 0.1)\n\ntrain_X_selected = train_X[selected_features]\nval_X_selected = val_X[selected_features]\n\nmodel_selected.fit(train_X_selected, train_y)\npredictions = model_selected.predict(val_X_selected)\n        \nprint(mean_absolute_error(val_y, predictions))","bc545418":"# See how exactly Squat and Bench affect predictions\n\nfrom matplotlib import pyplot as plt\nfrom pdpbox import pdp, get_dataset, info_plots\n\nfeatures_to_plot = ['Best3SquatKg', 'Best3BenchKg']\ninter1  =  pdp.pdp_interact(model=model, dataset=val_X, model_features=val_X.columns.tolist(), features=features_to_plot)\n\npdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=features_to_plot, plot_type='contour')\nplt.show()","602ceaee":"# See how exactly Age and Annual Premiumn affect predictions\n\nfrom matplotlib import pyplot as plt\nfrom pdpbox import pdp, get_dataset, info_plots\n\nfeatures_to_plot = ['Age', 'BodyweightKg']\ninter1  =  pdp.pdp_interact(model=model, dataset=val_X, model_features=val_X.columns.tolist(), features=features_to_plot)\n\npdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=features_to_plot, plot_type='contour')\nplt.show()","7050e753":"# Another way to see relationships between variables\n# SHAP values and SHAP summary plots\n\nimport shap\n\nexplainer = shap.TreeExplainer(model)\n\nshap_values = explainer.shap_values(val_X)\n\nshap.summary_plot(shap_values, val_X)","f48ca785":"# Partial Dependence Plot, but enhanced with SHAP values\n# Check out Age and Bench\n\nshap.dependence_plot(\"Best3BenchKg\", shap_values, val_X, interaction_index=\"Age\")","1830e100":"# Partial Dependence Plot, but enhanced with SHAP values\n# Check out the interaction between Squat and Sex_F\n\nshap.dependence_plot(\"BodyweightKg\", shap_values, val_X, interaction_index=\"Sex_F\")","2a89dcaf":"# Partial Dependence Plot, but enhanced with SHAP values\n# Check out the interaction between Squat and Sex_F\n\nshap.dependence_plot(\"Best3SquatKg\", shap_values, val_X, interaction_index=\"BodyweightKg\")","138e1fcc":"## SHAP Value Plots","6853882d":"# *Exploratory Data Analysis*","d4dbf6c6":"## Dealing with Categorical Values","565a2c2e":"## Categorical Features","fceca559":"## Normalization and Outliers","2303ad86":"# *Data Cleaning*","c3ec41b3":"The numerical columns all looked relatively normal so I won't mess with log transforms or boxcox transforms.\\\nI didn't notice any huge outliers in numerical columns.","86bb0489":"Really only the heaviest people are able to squat 400kg+. And this correlates heavily with a higher deadlift because they're both lower body movements.","d2ae65df":"## Creating New Features","4544fcf0":"Division: Highest are Super Heavyweight and Elite Pro Open. Lowest are Ages 7-8 and Paralympics.\\\nCountry: Highest lifters are from Syria, Yugoslavia, and Bulgaria. Lowest are from Hong Kong, Nepal, and Djibouti.\\\nFederation: Highest are SCT and SPSS. Lowest are RAWU and THSWPA. (no clue what any of those are).\\\nMeetCountry: Highest lifts are from Egypt. Lowest are from Kazakhstan and Costa Rica.\\\nMeetState: Highest lifts are from WB and GR. Lowest are from HG and DL. (no clue what any of those are).","c865ea6c":"There is not a noticeable trend. I would think people's deadlifts increase over time because of advances in exercise and nutrition science. Maybe that's too naive because there are always experienced and inexperienced people going to these events.","066c0a0a":"IMPORTANT: Because I have so much data, it takes absolutely forever to train the more complicated models with KFold and GridSearch running simultaneously. I'm going to forgo using GridSearch or RandomSearch to tune hyperparameters now. I'll see which model works best from its default hyperparameters based off of KFold, then tune hyperparameters later.","c549e9e6":"# *Feature Selection*\nWhy?\n- Prevents overfitting on training and validation data\n- Train and do inference faster with fewer features","d43565aa":"## Partial Dependence Plots","1df2fc14":"There were so many missing values for these features that the imputed values stick out like a sore thumb. I don't know if there was much I could do about that.\n\nMost of these numerical features look normally distributed. Maybe a little right skew.","06b4ae7d":"## Numerical Features","ed1de430":"It looks like there's also a sweet spot around age 30 and a bodyweight of around 125kg.","aef630e6":"Male deadlift median higher than female.\\\nMulti-ply deadlift median higher than other equipment.\\\nNon-tested deadlift higher than tested.\\\nThe attempt features don't really mean much apparently.\\\nAll makes sense.","9be75f48":"# *Machine Learning Explainability*\n#### Making sense of the model's predictions","d6f72425":"# *Data Cleaning Part 2*","59eb283b":"The barplots for Division, Country, Federation, MeetCountry, and MeetState look awful.\\\nCountry, MeetState, and Division were imputed, so the highest bars are probably \"missing_value\".","ed0a29de":"## Hyperparameter Optimization","3a2c3f84":"Very high correlation between best squat and deadlitft.  Bodyweight has some correlation with best squat and deadlift.","7ab59a36":"# *Model Selection*","e26566ee":"# Predict a Powerlifter's Best Deadlift (by Andrew Dettor)\n\n### Dataset: [Powerlifting Database](https:\/\/www.kaggle.com\/open-powerlifting\/powerlifting-database)\n### Goals:\n* Deal with missing values present in dataset\n* Explore distributions of features and their relationships with the target feature, Best3DeadliftKg (best deadlift from 3 attempts)\n* Preprocess data in order to model it (look at outliers\/skewed distributions\/standardization)\n* Test out different Regression models and compare their performance\n* Tune hyperparameters of the best model\n* Explore which features were the most impactful\n* Explore interactions between features\n\n\n### FYI - Confusing Names for Features:\n* Squat3Kg - Lifter's 3rd squat attempt weight. Negative means the attempt was failed. \"Ignore NaN\" according to the dataset creator, which doesn't make sense to me. How do I just 'ignore' NaN?\n* Bench1Kg - Lifter's 1st bench attempt weight.\n* Best3BenchKg - Lifter's highest weight benched out of 3 attempts.\n* Wilks\/McCulloch\/Glossbrenner\/IPFPoints - Metrics used to measure a lifter's performance against others. Takes into account bodyweight and powerlifting total (which is equal to Best3BenchKg + Best3SquatKg + Best3DeadliftKg) among other things.","e813ac32":"It looks like there's a sweet spot around 300kg on Squat and 200kg Bench where Deadlift Kg is the highest. Maybe those with really high Bench or really high Squat are specialists.","38adffff":"Okay, slightly worse score, but like 40 features were removed, so I think that's great.\\\nI'm going to continue further analysis with all the features, just to see the whole picture.","1c8faa79":"Permutation importance tells a different story. Here, the interaction feature of BestSquatBench was a very powerful predictor.\\\nHowever, its inclusion is probably pointless if I already have BestSquat and BestBench in the feature set.\\\nAge was also important, along with its interaction features.\\\nMaybe I should reduce alpha in the L1 regularization to see how many more features it says are important.\\\nMaybe different features become important in a linear model like Lasso than in XG Boost?","531706f0":"## Model Training","b2e71b93":"## Feature Selection Conclusion\n\nIf I were to use this model in the real world, not needing all those features regarding fails and successes on 1 rep max Bench and Squat attempts would be amazing. Those features and the ones I engineered from them really increase the number of features I had, while not being useful at all. Also some features are redundant, like Sex_M, so that wouldn't even be needed.\n\nFurthermore, I think the scaling of Age hurt my ability to interpret its importance with L1 Regularization in Lasso Regression. I don't think there's a way around that because Lasso Regression needs scaled data.","2e82c4d5":"Clearly, there are more males than females in this dataset. Even though most women are of lower body weight, there are some around 75kg bodyweight who have a higher deadlift compared to men of the same bodyweight. Women above 100kg bodyweight have comparatively lower deadlifts than men of the same bodyweight.","fa3c60e2":"Distributions didn't seem to change much, except that the male Kgs on everything are shifted up by a lot.","137b253b":"## Scaling","c888ebfe":"I don't see a clear pattern here between Bench and Age with the SHAP values from Best3DeadliftKg. It seems that people in the middle range of ages have a higher bench, but that doesn't correlate very much with the SHAP values. This is surprising because of the graph I made earlier that showed quite a large correlation between highest bench and highest deadlift.","45b6332f":"Clear indicators of a high 1 Rep Max Deadlift:\n* High 1RM Squat\n* Higher bodyweight\n* Male\n* In a Federation\/Division that historically has high deadlifts\n* Born in a country whose competitors historically have high deadlifts\n\nNot-so-clear indicators of a high 1RM Deadlift:\n* For some reason, lifting raw (without equipment) leads to a higher deadlift. I would think using wraps would help more. Maybe the original data is biased in some way.\n* Bench doesn't help as much as I previously thought. For some people, they had a high bench but very low squat. I mean it makes sense because it's not a lower body movement, but it's still a staple strength movement.\n* I would think a lower age would help, but maybe not.","633c67e8":"I guess all that feature engineering with the fails and successes of lifts were useless.\\\nAge not being useful very much surprises me. It's probably because I normalized it with BoxCox and then scaled it.\\\nAlso, how is being tested not useful? People taking PEDs should be able to lift more.\\\nNone of the interaction features I made were useful.","8e07c75f":"# *Feature Engineering*","becb1c4a":"Most of these have very very high cardinality.\\\nOne-Hot encode the ones with low cardinality.\\\nTarget encode the ones with high cardinality.","673ed4fc":"# *Read in the Data*","ace4ba48":"## Results\n\nWhich model to choose? (based on Mean Absolute Error and default hyperparameters)\n* **Linear Regression**: 15.77874\n* **Ridge Regression**: 15.77877\n* **Decision Tree**: 19.41793\n* **K Nearest Neighbors**: 19.94603\n* **XGBoost**: 14.67944\n* **Random Forest**: *14.05798*\n* **Linear Support Vector Machine**: 21.95390\n\nEven though the Random Forest Classifier performed the best, it took like an hour to run its cell. The next best options are XGBoost and Ridge\/Linear Regression. The latter trained lightning quick but the former has just a bit better score, so I'm going to go with it. Surprisingly, XGBoost didn't take nearly as long to train as the RandomForestClassifier.\n\nAlso, the differences in MAE across all models is miniscule. It's a difference of a few kilograms of weight.","53a43516":"## Preprocessing and cross validation functions\nNOTE: I could do things a lot simpler if I didn't have to worry about target leakage with the TargetEncoder. Right now, each validation split needs to be tailored to the values in the corresponding training split. I can't just pass in a preprocessed big X into a cross validation function which randomly splits the X, because it would contain target-leaked data. "}}