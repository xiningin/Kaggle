{"cell_type":{"0d23c057":"code","4e816eef":"code","a46701e3":"code","9078bb8e":"code","2242bc62":"code","69c59bd3":"code","d2b5e1f1":"code","b5dd3dcd":"code","430a9223":"code","0f6b57fe":"code","e7327f18":"code","fd69a4c5":"code","34bd8416":"code","fb947165":"code","01ec49d8":"code","f155e477":"code","15289d3e":"code","ffa0cc7e":"code","423772fa":"code","f00dcbeb":"code","1c024044":"code","6b1a9f68":"markdown","3a26de72":"markdown","8140ed2a":"markdown","4a72ff92":"markdown","fad64765":"markdown","96bf6b02":"markdown","f7fe1d90":"markdown","e918f20f":"markdown","a0af38de":"markdown","2631f983":"markdown","0dc4d4e6":"markdown","1c77af70":"markdown","46e0b3b6":"markdown","a3ddfe94":"markdown","baa39ff0":"markdown","7187bd25":"markdown","987baac6":"markdown","207698f9":"markdown","c9b08169":"markdown","db4b5109":"markdown","2b6edc88":"markdown"},"source":{"0d23c057":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4e816eef":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.preprocessing import MinMaxScaler","a46701e3":"df = pd.read_csv('..\/input\/water-potability\/water_potability.csv')\ndf.head(5)","9078bb8e":"df.describe()","2242bc62":"colors = ['#ff0000','#fff000','#18fff9','#8f139f']\nfig, axes = plt.subplots(3, 3,figsize=(20,12))\ncolumn = df.columns\nfig.suptitle('Boxplots of each variable')\nsns.boxplot(ax=axes[0,0],x=column[0],data=df,color=colors[0])\nsns.boxplot(ax=axes[0,1],x=column[1],data=df,color=colors[1])\nsns.boxplot(ax=axes[0,2],x=column[2],data=df,color=colors[2])\nsns.boxplot(ax=axes[1,0],x=column[3],data=df,color=colors[3])\nsns.boxplot(ax=axes[1,1],x=column[4],data=df,color=colors[0])\nsns.boxplot(ax=axes[1,2],x=column[5],data=df,color=colors[1])\nsns.boxplot(ax=axes[2,0],x=column[6],data=df,color=colors[2])\nsns.boxplot(ax=axes[2,1],x=column[7],data=df,color=colors[3])\nsns.boxplot(ax=axes[2,2],x=column[8],data=df,color=colors[0])\nplt.show()","69c59bd3":"df.isnull().sum()","d2b5e1f1":"#ph values are evenly distributed so we can use mean\ndf['ph'].fillna(df['ph'].mean(),inplace=True)\n#sulphate values are slightly on the right side and it has outliers which may affect mean hence we will use median here\ndf['Sulfate'].fillna(df['Sulfate'].median(),inplace=True)\n#Trihalomethanes values are evenly distributed so we will use mean\ndf['Trihalomethanes'].fillna(df['Trihalomethanes'].mean(),inplace=True)","b5dd3dcd":"df.isnull().sum()","430a9223":"sns.pairplot(data=df,hue='Potability')","0f6b57fe":"df['Potability'].value_counts()","e7327f18":"X = df.drop(['Potability'],axis=1)\ny = df['Potability']","fd69a4c5":"X_train , X_test , y_train , y_test = train_test_split(X,y,test_size=0.2,random_state=17)","34bd8416":"sns.countplot(x=y_train)","fb947165":"rus = RandomUnderSampler(sampling_strategy=0.75)\nX_train,y_train = rus.fit_resample(X_train,y_train)","01ec49d8":"sns.countplot(x=y_train)","f155e477":"smote = SMOTE(sampling_strategy='minority')\nX_train,y_train = smote.fit_resample(X_train,y_train)","15289d3e":"sns.countplot(x=y_train)","ffa0cc7e":"scaler = MinMaxScaler()\nX_scaled = scaler.fit_transform(X_train)\nX_scaled","423772fa":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import classification_report","f00dcbeb":"models = {'Logistic Regression':LogisticRegression,'Random Forest':RandomForestClassifier,'KNN':KNeighborsClassifier,'Support Vector':SVC,'Naive bayes gaussian':GaussianNB}\nX_test_scaled = scaler.transform(X_test)\nfor i in models:\n    clf = models[i]()\n    clf.fit(X_scaled,y_train)\n    print(i)\n    print(classification_report(y_test,clf.predict(X_test_scaled)))","1c024044":"from sklearn.model_selection import GridSearchCV\nparams=[{'n_estimators':[150,200,250,300],'criterion':['gini', 'entropy'],'max_features':['auto','sqrt','log2']}]\nrf_clf = RandomForestClassifier(random_state=17)\nfinal_clf = GridSearchCV(rf_clf,params)\nfinal_clf.fit(X_scaled,y_train)\nprint(classification_report(y_test,final_clf.predict(X_test_scaled)))","6b1a9f68":"# Model Selection","3a26de72":"- ### After Sampling is completed","8140ed2a":"# Hyperparameter Tuning","4a72ff92":"# Data Preparation","fad64765":"- ### Function to evaluate base models","96bf6b02":"- ### We can see the imbalance in classes","f7fe1d90":"# The End\n`If you liked the notebook then don't forget to upvote and suggestions are always welcomed.`\n`Follow me on Linkedin :` __[Atharva_Dumbre](https:\/\/www.linkedin.com\/in\/atharva-dumbre-208b5716b)__","e918f20f":"## Checking for Null values","a0af38de":"- ### We will use MinMaxScaler from sklearn library to scale the data in the range of 0 to 1","2631f983":"- ## Generating Train and Test sets","0dc4d4e6":"## Importing the dataset","1c77af70":"### Our dataset has less samples of class 1","46e0b3b6":"#### Random Forest seems to perform rather good than other models on both the classes","a3ddfe94":"- ## Splitting the Dataframe","baa39ff0":"# Importing the required libraries","7187bd25":"### We managed to get a little improvement in f1 score for '1' class","987baac6":"## Imputing the missing values","207698f9":"- ### We will under-sample the majority class and oversample the minority , this gives us the best results","c9b08169":"- ### Importing the models from sklearn","db4b5109":"## Pairplot gives a fair understanding about data distribution","2b6edc88":"# Data Analysis and Visualization"}}