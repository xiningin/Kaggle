{"cell_type":{"46628991":"code","36e79576":"code","b8fecb3c":"code","6bbf0f87":"code","ff456800":"code","e708db05":"code","84504830":"code","5f4de504":"code","fec761d7":"code","3340c176":"code","2bedfd8d":"code","23c2bc74":"code","26273ff8":"code","4a87d4eb":"code","aa6b9095":"markdown","d8308606":"markdown","da623aba":"markdown","610badf0":"markdown","30c6a317":"markdown","90fd238b":"markdown","05808bc2":"markdown","8b46b6fe":"markdown","09a1dced":"markdown","da63f6f6":"markdown","4b3b5950":"markdown","1866e0a7":"markdown","e7ba1eda":"markdown","260ca3e6":"markdown","2f4c1880":"markdown"},"source":{"46628991":"import pandas as pd\nimport numpy as np\n\nfrom scipy.stats import kurtosis,skew\nimport time\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set()\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import svm,tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n\nimport xgboost as xgb\nimport lightgbm as lgb","36e79576":"data = pd.read_csv('..\/input\/gld-resnet-50-features\/resnet_50_features.csv')\ndata.head()","b8fecb3c":"plt.figure(figsize=(10,5))\nax = sns.barplot(x = data['label'].describe().index, y = data['label'].describe().values.astype(str))\nfor p in ax.patches:\n    ht = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/\/2,ht,'{:.1f}'.format(ht))\nplt.title('Label Summary')\nplt.show()","6bbf0f87":"data['mapped_label'] = data['label'].astype('category')\ndata['mapped_label'] = data['mapped_label'].cat.codes\nplt.figure(figsize=(20,5))\nplt.subplot(121)\nsns.distplot(data.label.astype(int), color='#2CDAE5')\nplt.title('Original Label')\n\nplt.subplot(122)\nsns.distplot(data.mapped_label.astype(int),color='#F3322C')\nplt.title('Mapped Label')\nplt.show()","ff456800":"X = data.iloc[:,:-4]\nY = data['mapped_label']\n\nprint('X shape: {} \\nY shape: {}'.format(X.shape, Y.shape))\nx_train,x_valid,y_train,y_valid = train_test_split(X,Y, stratify=Y, test_size=0.2, random_state=2020)\nprint('x_train shape: {}\\ny_train shape: {}\\nx_valid shape: {}\\ny_valid shape: {}'.format(x_train.shape,y_train.shape,x_valid.shape,y_valid.shape))\ndel data,X,Y","e708db05":"st = time.time()\nnb = GaussianNB()\nnb.fit(x_train,y_train)\n\ny_pred = nb.predict(x_valid)\n\nnb_as = round(accuracy_score(y_valid,y_pred),4)\nnb_ps = round(precision_score(y_valid, y_pred,average='weighted'),4)\nnb_rs = round(recall_score(y_valid,y_pred,average='weighted'),4)\nnb_f1 = round(f1_score(y_valid,y_pred,average='weighted'),4)\n\nprint('Time Taken in seconds: {}\\nAccuracy Score: {}\\nPrecision Score: {}\\nRecall Score: {}\\nF1-Score: {}'.format(time.time()-st,nb_as,nb_ps,nb_rs,nb_f1))\n","84504830":"st = time.time()\nsvc = svm.SVC()\nsvc.fit(x_train,y_train)\n\ny_pred = svc.predict(x_valid)\n\nsvc_as = round(accuracy_score(y_valid,y_pred),4)\nsvc_ps = round(precision_score(y_valid, y_pred,average='weighted'),4)\nsvc_rs = round(recall_score(y_valid,y_pred,average='weighted'),4)\nsvc_f1 = round(f1_score(y_valid,y_pred,average='weighted'),4)\n\nprint('Time Taken in seconds: {}\\nAccuracy Score: {}\\nPrecision Score: {}\\nRecall Score: {}\\nF1-Score: {}'.format(time.time()-st,svc_as,svc_ps,svc_rs,svc_f1))","5f4de504":"st = time.time()\ndt = tree.DecisionTreeClassifier()\ndt.fit(x_train,y_train)\n\ny_pred = dt.predict(x_valid)\n\ndt_as = round(accuracy_score(y_valid,y_pred),4)\ndt_ps = round(precision_score(y_valid, y_pred,average='weighted'),4)\ndt_rs = round(recall_score(y_valid,y_pred,average='weighted'),4)\ndt_f1 = round(f1_score(y_valid,y_pred,average='weighted'),4)\n\nprint('Time Taken in seconds: {}\\nAccuracy Score: {}\\nPrecision Score: {}\\nRecall Score: {}\\nF1-Score: {}'.format(time.time()-st,dt_as,dt_ps,dt_rs,dt_f1))","fec761d7":"st = time.time()\nknn = KNeighborsClassifier()\nknn.fit(x_train,y_train)\n\ny_pred = knn.predict(x_valid)\n\nknn_as = round(accuracy_score(y_valid,y_pred),4)\nknn_ps = round(precision_score(y_valid, y_pred,average='weighted'),4)\nknn_rs = round(recall_score(y_valid,y_pred,average='weighted'),4)\nknn_f1 = round(f1_score(y_valid,y_pred,average='weighted'),4)\n\nprint('Time Taken in seconds: {}\\nAccuracy Score: {}\\nPrecision Score: {}\\nRecall Score: {}\\nF1-Score: {}'.format(time.time()-st,knn_as,knn_ps,knn_rs,knn_f1))","3340c176":"st = time.time()\nrf = RandomForestClassifier()\nrf.fit(x_train,y_train)\n\ny_pred = rf.predict(x_valid)\n\nrf_as = round(accuracy_score(y_valid,y_pred),4)\nrf_ps = round(precision_score(y_valid, y_pred,average='weighted'),4)\nrf_rs = round(recall_score(y_valid,y_pred,average='weighted'),4)\nrf_f1 = round(f1_score(y_valid,y_pred,average='weighted'),4)\n\nprint('Time Taken in seconds: {}\\nAccuracy Score: {}\\nPrecision Score: {}\\nRecall Score: {}\\nF1-Score: {}'.format(time.time()-st,rf_as,rf_ps,rf_rs,rf_f1))","2bedfd8d":"st = time.time()\nab = AdaBoostClassifier()\nab.fit(x_train,y_train)\n\ny_pred = ab.predict(x_valid)\n\nab_as = round(accuracy_score(y_valid,y_pred),4)\nab_ps = round(precision_score(y_valid, y_pred,average='weighted'),4)\nab_rs = round(recall_score(y_valid,y_pred,average='weighted'),4)\nab_f1 = round(f1_score(y_valid,y_pred,average='weighted'),4)\n\nprint('Time Taken in seconds: {}\\nAccuracy Score: {}\\nPrecision Score: {}\\nRecall Score: {}\\nF1-Score: {}'.format(time.time()-st,ab_as,ab_ps,ab_rs,ab_f1))","23c2bc74":"st = time.time()\nxg = xgb.XGBClassifier()\nxg.fit(x_train,y_train)\n\ny_pred = xg.predict(x_valid)\n\nxg_as = round(accuracy_score(y_valid,y_pred),4)\nxg_ps = round(precision_score(y_valid, y_pred,average='weighted'),4)\nxg_rs = round(recall_score(y_valid,y_pred,average='weighted'),4)\nxg_f1 = round(f1_score(y_valid,y_pred,average='weighted'),4)\n\nprint('Time Taken in seconds: {}\\nAccuracy Score: {}\\nPrecision Score: {}\\nRecall Score: {}\\nF1-Score: {}'.format(time.time()-st,xg_as,xg_ps,xg_rs,xg_f1))","26273ff8":"st = time.time()\nlg = lgb.LGBMClassifier()\nlg.fit(x_train,y_train)\n\ny_pred = lg.predict(x_valid)\n\nlg_as = round(accuracy_score(y_valid,y_pred),4)\nlg_ps = round(precision_score(y_valid, y_pred,average='weighted'),4)\nlg_rs = round(recall_score(y_valid,y_pred,average='weighted'),4)\nlg_f1 = round(f1_score(y_valid,y_pred,average='weighted'),4)\n\nprint('Time Taken in seconds: {}\\nAccuracy Score: {}\\nPrecision Score: {}\\nRecall Score: {}\\nF1-Score: {}'.format(time.time()-st,lg_as,lg_ps,lg_rs,lg_f1))","4a87d4eb":"plt.figure(figsize=(20,8))\nplt.subplot(221)\nsns.lineplot(['Naive_Bayes', 'SVC', 'DT', 'KNN', 'RF', 'AB', 'XGB', 'LGBM'], [nb_as,svc_as,dt_as,knn_as,rf_as,ab_as,xg_as,lg_as],\n        marker='*',label='Accuracy')\nsns.lineplot(['Naive_Bayes', 'SVC', 'DT', 'KNN', 'RF', 'AB', 'XGB', 'LGBM'], [nb_ps,svc_ps,dt_ps,knn_ps,rf_ps,ab_ps,xg_ps,lg_ps],\n        marker='*',label='Precision')\nsns.lineplot(['Naive_Bayes', 'SVC', 'DT', 'KNN', 'RF', 'AB', 'XGB', 'LGBM'], [nb_rs,svc_rs,dt_rs,knn_rs,rf_rs,ab_rs,xg_rs,lg_rs],\n        marker='*',label='Recall')\nsns.lineplot(['Naive_Bayes', 'SVC', 'DT', 'KNN', 'RF', 'AB', 'XGB', 'LGBM'], [nb_f1,svc_f1,dt_f1,knn_f1,rf_f1,ab_f1,xg_f1,lg_f1],\n        marker='*',label='F1-Score')\nplt.legend()\nplt.show()","aa6b9095":"## AdaBoost Classifier","d8308606":"## Decision Tree","da623aba":"## XGBoost Classifier","610badf0":"In my previous kernel, I have done a basic EDA on Google Landmark Recognition Dataset by extracting the features for 51 classes out of ~81K classes. Selected 51 Classes had more than 500 images each. \n\n* For More Information - https:\/\/www.kaggle.com\/ramjib\/landmark-recognition-eda (Do upvote if you find this is interesting)\n\nIn this kernel I am planning to experiment on various ML algorithms like SVC, Boosting Classifiers for the extracted features and few EDA for the same.\n\n- I hope this turns out to be someting interesting!!!","30c6a317":"When i started my kaggle competitons few years back, I saw most of the notebooks(related to images) uses CNN for classification tasks. So I was also doing most of my stuffs using CNN and completely away from ML algorithms but ML algorithms plays a major role to improve the score in decimals. Thats were I came across the link mentioned below, which explains all the basic algorithms in layman terms. From here we are going to observe how different algorithms reflect to the classification task.\n- https:\/\/medium.com\/machine-learning-101\n- https:\/\/towardsdatascience.com\/accuracy-precision-recall-or-f1-331fb37c5cb9","90fd238b":"# ML Algorithms- Naive Bayes, SVM, Decision Tree, K Neares Classifier, Random Forest Classifier, Adaboost Classifier","05808bc2":"## LightGBM Classification","8b46b6fe":"# Mapping class labels","09a1dced":"## SVC","da63f6f6":"## Naive Bayes","4b3b5950":"## Random Forest Classifier","1866e0a7":"# Data Preparation","e7ba1eda":"## K-Nearest Nieghbors","260ca3e6":"* Total Number of images - 45579\n* Class Labels vary from 27 to 194914 for 51 classes\n\nSo will map the the class labels from **0-51**","2f4c1880":"# ML Algorithms on Extracted Features"}}