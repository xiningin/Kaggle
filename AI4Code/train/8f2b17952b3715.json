{"cell_type":{"27dd2016":"code","5aff3c16":"code","d09bb356":"code","bbef05eb":"code","aaad7237":"code","881723ca":"markdown","63267632":"markdown","b3ccf472":"markdown","0632ce1e":"markdown","20b92f87":"markdown","60f9511c":"markdown","7ead5e7b":"markdown","928474b0":"markdown","ebf73de5":"markdown","06ac0262":"markdown","53ac221f":"markdown","c08fbcdc":"markdown","27d834bf":"markdown","0dfdd79b":"markdown","5ff4b2cc":"markdown","6e66cf6a":"markdown"},"source":{"27dd2016":"# Packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport glob\nimport os, sys\nimport cv2\nfrom IPython.display import display\nplt.rcParams['figure.figsize'] = [7,5]\n\n# Metadata \ndata_df = pd.read_csv('\/kaggle\/input\/global-wheat-detection\/train.csv')\ndata_df.head()\n\n# Data path\ndata_path = '\/kaggle\/input\/global-wheat-detection\/train\/'","5aff3c16":"# Read a sample image\nI = cv2.cvtColor(cv2.imread(data_path + data_df.image_id[0] + '.jpg'), cv2.COLOR_BGR2RGB)\n# Show the image\nplt.imshow(I);\nplt.axis('off')\nplt.title('Wheat image')\nplt.show()","d09bb356":"bboxes = data_df[data_df.image_id==data_df.image_id[0]].bbox.tolist()\nJ = I.copy()\nfor i in range(len(bboxes)):\n    x = int(str(bboxes[i][1:-1]).split(',')[0][:-2])\n    y = int(str(bboxes[i][1:-1]).split(',')[1][1:-2])\n    xw = x + int(str(bboxes[i][1:-1]).split(',')[2][1:-2])\n    yh = y + int(str(bboxes[i][1:-1]).split(',')[3][1:-2])\n    cv2.rectangle(J,(x,y),(xw,yh),(180,190,0),4)\nplt.imshow(J);\nplt.axis('off')\nplt.title('Wheat image bounding boxes')\nplt.show()","bbef05eb":"import torchvision\nbase_model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)","aaad7237":"from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\n# replace the classifier with a new one, that has num_classes which is user-defined\nnum_classes = 2  # 1 class (wheat) + background\n\n# get number of input features for the classifier\nin_features = base_model.roi_heads.box_predictor.cls_score.in_features\n\n# replace the pre-trained head with a new one\nbase_model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)","881723ca":"### <span style=\"color:green\">  Output:\n---\n> ### ```bbox```: Bounding boxes around wheat heads present in the image ```I```. \n> The format is ```[min_x_coordinate, min_y_coordinate, width, height]```\n> \n> ### ```confidence```: A score that tells how confident the model is for a particular bounding box ```bbox```. \n> The value lies in ```[0,1]```.\n\nA particular wheat image can have multiple wheat head, therefore we can have multiplte ```bbox```es. \n\nFor example, lets draw all the ```bbox```es on the previous image.","63267632":"## <span style=\"color:green\">  So what is this type of problem called?\n    \nOur problem can be classfied as:\n\n> ### Binary classification and object localization problem\n\nIn particular, \n\n- **Classification**: We have only two classes: wheat containing images and no wheat containing images. We expect most of the images contain wheat (unless there are few exceptions).\n\n- **Localization**: For localization, just saying it contains wheat is NOT ENOUGH. We also have to tell where those wheat heads are located in the image, i.e., localization.\n\n \nHere is a screenshot from Stanford University School of Engineering CS231 course video (https:\/\/www.youtube.com\/watch?v=nDPWywWRIRo) that shows different computer vision tasks realted to ours. Can you tell which one is most related to ours?\n\n![image.png](attachment:image.png)\n\n\n### Therefore, we need to find a technology that can:\n1. Tell if our input image contains wheat heads,\n2. If there are wheat heads, where are they located?\n3. It has to draw bounding boxes around the located objects (i.e., wheat heads), and \n4. Tell us how confident the algorithm is that it is wheat head?","b3ccf472":"## <span style=\"color:green\"> Background","0632ce1e":"# Training\n\nFor more on how to train, I suggest look at the kernel by @pestipeti https:\/\/www.kaggle.com\/pestipeti\/pytorch-starter-fasterrcnn-train\n\nThank you! I hope this discussion was useful.","20b92f87":"## <span style=\"font-family:Papyrus;\"> How can we use Faster RCNN in this competition? <\/span>\n\nWe can perform transfer learning with a pretrained faster RCNN model. You can load the pretrained (pre-trained on COCO train2017) model using ```torchvision.model```.","60f9511c":"Faster RCNN belongs to region-based object detection methods. \n\nThe journey to the development of Faster RCNN followed this path:\n\n1. **2014**: In the original [paper](https:\/\/arxiv.org\/pdf\/1311.2524.pdf), Ross Girshick et al. proposed Regions with CNN features (RCNN).\n\n2. **2015**: Girshick improved RCNN, and proposed Fast RCNN in this original [paper](https:\/\/arxiv.org\/pdf\/1504.08083.pdf).\n\n3. **2015**: Finally, this [paper](http:\/\/arxiv.org\/abs\/1506.01497) improved upon Fast RCNN and proposed Faster RCNN!\n\nLet's quickly overview each of them!\n\n### Model 1: RCNN\n\nThis is the architecture of RCNN:\n\n\n![](https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets\/638003\/1133328\/rcnn1.png?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1588991839&Signature=EwsVfQaUPuC%2FxHBlbwPFDfSBbmOumh6L9xRp8V%2B8A7m60%2BtWw4%2BIZQi6UXuAjzVCrVSPrn5jKfQ7aAWjqOd9jFShfNG%2Bp4EczhB64aStAGYedCisCii2a2gZjwv4kKTFMkExUXJzxizLwwce2ICr%2B7I2tIKImvzctrx4FlGqonwUxLRFyr97hcstVdgD9V5kXdfd513xmrcTLqeuh5zt7VWic28qkk%2F3tJSlZqrdE6B4K8Z14Rmn6W%2BrXFLx9Mol%2FNN6h8NuPsIRCxwM1eb4yjq3reCqf7YNFOFZM3wfTTWLLEkRbhRI8UlLnW1S3BvrFEziqOnA3bUzGBFAl29CZA%3D%3D)\n\n![](https:\/\/www.learnopencv.com\/wp-content\/uploads\/2019\/06\/rcnn.png)\n\n#### <center> *(Images from Stanford and learnopencv.com)*\n\n\n\n**Step 1**: They used an algorithm called Selective Search to detect 2000 region proposals. This proposes the regions that could be potential objects.\n\n**Step 2**: They ran a CNN + SVM based image classifier on these 2000 bounding boxes.\n\n####  <span style=\"color:orange\"> Verdict:\nThe accuracy of R-CNN at that time was the state of the art, but the speed was still very slow. \n- Inference takes 18-20 seconds per image on a GPU\n- Training was slow (84 hour) and takes lot of disk space\n- Adhoc training objectives for region proposal and SVM\n\n\n### Model 2: Fast RCNN\n\nThis is the architecture of Fast RCNN:\n\n\n![](https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets\/638003\/1133365\/fast-rcnn.png?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1588992630&Signature=GOSw5nVWxWEDijlYuDjWJaLC4TQ2t%2F9fjcWCyxhDMnj6zwBnxoQzk%2B%2BTHLcRiw%2Ba%2BmRJugVGSuekV2JvGFU2Mker9kTWYs4KZzNCj%2F09O55zX2hk8SXCnFCqXnb2B0jnIZnprkS66zs80jYqkvXo2XQ8qf28kUqAByY9GnxqZ9AlbtYyiMn5x5u%2BP2kfYovVxL%2BeSN8kBa7QIIRzLn4QVWhvSHOlaiwvdLrlouoKW%2BslgluA2Z3riYYYdUoOoVxuJhT7no4Lxg3tmv5bWLWcX9LvsYe%2BJDCW5nOwzygQpHbdmezaLHXayUNSKQyAZ9fKjhhkqYVhS9UhP08qvi2%2B2w%3D%3D)\n\n![](https:\/\/www.learnopencv.com\/wp-content\/uploads\/2019\/06\/frcnn.png)\n\n#### <center> *(Images from Stanford and learnopencv.com)*\n\nIn Fast RCNN, instead of feeding 2000 region proposals separately to different convlution neural networks, now estimated region proposals (not necessarily 2000) are combined to a feature map and fed to a single neural network.\n\n####  <span style=\"color:orange\"> Verdict:\nFast RCNN achieved a great performance boost.\n\n![](https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets\/638003\/1133386\/fastrcnn-performance.png?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1588993106&Signature=gpSbM560XcmkRoyRqx0p8%2FrTV8Y0m2%2FtJ96yZy%2Bjcw1%2F%2FdzInxcrdAlJahnAeahZbzou9B7Mx76fGsYkcsET490DKxChG%2BJbTjJvy11sLlhWmgwZHEHJftwYnSQJqbW3%2B%2FIDn4980NFw%2BNHF59dHMePSjJ%2Bmmtkd3HTzv54qKkBccHCm83wUeO2z9eqWFKTcmYNH71JHAJnwVUDioh73McusN6ZaF71l%2FeP5iRxWweknPjGLaTZwmrO4IhW4U04kxIohTPh7UWjuDgLmr5C%2BNvN2gDI90J16M0GFvdYLfBdf9PTBYC6SzQnguL3pY%2BjpJQs7kqLY9qhnN%2FqtiRmDQQ%3D%3D)\n#### <center> *(Images from Stanford)*\n\n### Model 3: Faster RCNN\n\nThis is the architecture of Faster RCNN:\n\n![](https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets\/638003\/1133437\/fasterrcnn.png?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1588995574&Signature=iMvUbG3g7Ck2fNErU8MgqCXGqzxMnYUO%2B%2F%2Bj5xyy1%2FLW3Brmkh6gfFMJEuDH%2FcPkOf9aIhYHs4tFes9ZX4BDaZXF2fkp%2Frb%2BxXET%2FS2CljZd9v%2BPVkcDOPUMm41jLr8okNDDu1yDxU%2F1vgCrAddup01gwW8%2FxjiyQIG3AYGG3b%2Fjm4Xr019EAKLfD%2B4vd%2FjOQ13c7o%2FI0geKwE916P0GDaIjuLO3TraqkDaVP%2BDpb%2Fy56jw6S%2FFN82%2BonaqPu%2FSh1f88c%2F%2BwPnYD22SIJ7tONIm2CFI2URrnVI5N%2FKbQltSmrLWs%2BgAa1Dp7Fcq5xXhV1i3YIiUUaeLBd39BRzVBbQ%3D%3D)\n\n\nA Convolutional Neural Network was used to produce a feature map of the image which was simultaneously used for training a region proposal network and an image classifier. Because of this shared computation, there was a significant improvement in the speed of object detection.\n\n####  <span style=\"color:orange\"> Verdict:\n    \nFaster RCNN revolutionized object detection. Now inference can be done in less than a second, although training time is higher than Fast RCNN.\n\n![](https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets\/638003\/1133441\/faster-rcnn-perf.png?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1588995836&Signature=pfd5oAjlM6KGlKvA4W4ZJrYAtP09Eg7PjYYNOCjUO6OBFIL1kVBcOBvgedULinuSTt0kY%2FB7d%2B8ALWDkvV%2FbVeaUU5QL5LhzMWzhyWcdHu9M%2BNFqivktRCg5RoYFbuNzWlDUCuogYc41KYQ4Zil2sQGXQZe52OZwdRRlTUXJ0NGa5Ji86tKxh8MbcjVAnztXqPaJzPbEAHro%2BR9eJND80afr3wEfRimaxsHfv%2BWY6m%2BBBzWpC4DgOGogAMXXBADnHOSQeD0%2Fgh%2BkHjknmPiV7X4ruI8oiEom67yX6AMT8v8V31COYNdQfB5mtN0KIkPXHfXQ1wZ1vxUotMj5l8m5HA%3D%3D)\n\n### Overall:\n\n![](https:\/\/dzone.com\/storage\/temp\/9814919-screen-shot-2018-07-23-at-114334-am.png)\n","7ead5e7b":"# <center> <span style=\"font-family:Papyrus;\"> Faster RCNN: A Gentle Introduction<\/span> <\/center>\n\n### <center> In the persepctive of <span style=\"color:#b08f26\">  Global Wheat Detection Competition\n    \n\n#    \n![](https:\/\/images.wsj.net\/im-168083?width=620&size=1.5)    \n\n\nIn this notebook, I will attempt to provide an introduction to Faster RNN that will be help a beginner to object detection to start applying this technology in this competition. \n\n### <span style=\"color:green\"> If you find this notebook useful, please upvote. Thank you! <\/span>\n\n---","928474b0":"### <span style=\"color:green\"> Input:\n---\n\n> ### ```I```: Outdoor image of wheat plant ","ebf73de5":"Faster RCNN is a technology that can provide answers the questions we asked above. How? \n\nBefore answering it, I will provide a brief background to its predecessors. This will help you to understand why it is so popular.","06ac0262":"###  <span style=\"color:#ddd\"> Import Packages and Data","53ac221f":"**Note**: The training data also includes: \n- **Size of the image**: it can also be computed by ```np.shape()``` function\n- **Location**: the locations from which the images were captured.","c08fbcdc":"## <span style=\"font-family:Papyrus;\"> What is Faster RCNN?  <\/span>","27d834bf":"## References","0dfdd79b":"We can then replace the pre-trained head.","5ff4b2cc":"- https:\/\/github.com\/rbgirshick\/py-faster-rcnn\n- http:\/\/arxiv.org\/abs\/1506.01497\n- https:\/\/papers.nips.cc\/paper\/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks","6e66cf6a":"## <span style=\"color:#b08f26\">  <span style=\"font-family:Papyrus;\"> Problem Definition <\/span>\n\nThe goal of this competition is **to detect wheat heads from outdoor images of wheat plants**, including wheat datasets from around the globe. \n\nLet's look at our inputs and outputs."}}