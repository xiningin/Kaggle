{"cell_type":{"26cac7a1":"code","e07e6b4d":"code","d462b903":"code","0449ebea":"code","12f2661a":"code","33e9ba05":"code","413d59a0":"code","ce665842":"code","c70e0a3f":"code","7420fa06":"code","4f732b58":"markdown","8bd741d7":"markdown","594c5525":"markdown","06c9d0e9":"markdown","0c809153":"markdown","1732c52d":"markdown","614d21e7":"markdown"},"source":{"26cac7a1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e07e6b4d":"# Load packages\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Load all data:\nall_data = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/vw.csv')\n# Change order of data\ncol_index_new = [0,1,3,4,5,6,7,8,2] # Third column should become last one \nall_data = all_data.reindex(columns = all_data.columns.values[col_index_new])\nall_data.head(5), all_data.shape","d462b903":"# Plot of distribution as well as density\nplt.figure(figsize=(10,10))\nh2 = sns.histplot(data= all_data, x = 'price', hue ='model',kde=True, linewidth=0)\nprint('Skewness = %f and Kurtosis = %f' % (all_data['price'].skew(),all_data['price'].kurt()))","0449ebea":"# EDA: Correlations, scatter plots etc.\ncorr = all_data.corr()\nmask = np.triu(np.ones_like(corr))\ncmap=sns.diverging_palette(20, 220, n=200),\nh1 = sns.heatmap(corr,mask=mask,cmap=sns.diverging_palette(20, 220, n=200), square=True, annot = True)","12f2661a":"f,ax = plt.subplots(1,2,figsize=(12,5))\nsns.scatterplot(data=all_data,x='tax',y='engineSize',hue='fuelType',ax=ax[0])\nsns.scatterplot(data=all_data,x='mpg',y='price',hue='fuelType',ax=ax[1])\n","33e9ba05":"plt.figure(figsize=(12,12))\np1 = sns.pairplot(data=all_data,hue='model')","413d59a0":"# Check for missing data:\nall_data.isna().sum()>0\n# Dummy variables\nall_dataWithDummy = pd.get_dummies(all_data)\n# Predictor and target variables\nX_data = all_dataWithDummy.drop(['price'],axis=1)\ny_data = all_dataWithDummy['price']","ce665842":"# Import modeling packages\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\n","c70e0a3f":"# Split train and test set\nX_train, X_test, y_train, y_test = train_test_split(X_data,y_data,test_size = 0.2, random_state = 8)\n# Create linear regression model\nLR = LinearRegression()\n# Fit linear regression model\nLR.fit(X_train,y_train)\n# Use linear regression to predict test set\ny_pred = LR.predict(X_test)\n# Quantify prediction\nRMSE = np.sqrt(mean_squared_error(y_test,y_pred))\nprint('First 5 real values to predict: ' +  str(np.round(y_test[0:5].values,1)))\nprint('First 5 predicted values: ' +  str(np.round(y_pred[0:5],1)))\nprint('Average error in price units: ' + str(np.round(RMSE,1)))","7420fa06":"RFR = RandomForestRegressor()\nDTR = DecisionTreeRegressor()\nLR = LinearRegression()\n\nmodels = [RFR,DTR,LR]\n\nfor model in models:\n    model.fit(X_train,y_train)\n    y_pred = model.predict(X_test)\n    RMSE = np.sqrt(mean_squared_error(y_test,y_pred))\n    print(str(model),': ',np.round(RMSE,1))","4f732b58":"***Car sale price regression for Volkswagen***\n\nThis was my first notebook that I made myself from scratch. Very interesting dataset.\n\nPlease comment on what you would improve or try out further? Suggestions or improvements would be interesting and appreciated!","8bd741d7":"Seems like that last thought was indeed correct! Furthermore, the hybrids further strengthen this negative correlation!","594c5525":"**Take-aways:**\n* The overall distribution seems to consist of different peaks. This could be possible explained by the different types of cars (in the industry different classes\/segments). \n* Most of the cars on sale are of type: Polo, closely followed by Golf.\n* Golf has a second peak? GTI spec?  \n\nLet's now look for correlations:","06c9d0e9":"***Take-aways:***\n* Random forest outperforms other models with default parameters\n\nNote: This is definitely NOT the most optimal model to use. There are more advanced\/other models to try!\n","0c809153":"***Take-aways***\n* None of the data is missing! \n\nLets get modeling!","1732c52d":"***Take-aways***\n* Clearly not all variables have a linear relationship with the target (price) variable. Linear models might struggle with this dataset because of this reason. Non-linear models required? ","614d21e7":"***Take-aways:***\n* The year the car is made is most strongly correlated with the sale price. Newer = More expensive\n* More milage = cheaper car evidently.\n* Tax and engine size is not as correlated as I expected. Tax calculation might be based on factors such as weight as well, which is not included in the data.\n* Interestingly mpg is also quite strongly negatively correlated. There might be a reason for that gasoline = lower mpg but cheaper!\n\nLets dive (even) deeper in these last points! "}}