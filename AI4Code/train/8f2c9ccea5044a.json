{"cell_type":{"bdeb0950":"code","c9090785":"code","8f83b768":"code","adf70dfc":"code","2e3b3d38":"markdown","4ab61fe4":"markdown","168ed9fb":"markdown","7f0b4311":"markdown","33eb4f3d":"markdown"},"source":{"bdeb0950":"import numpy as np\nimport matplotlib.pyplot as plt\n\ndef sigmoid(x):\n    return 1 \/ (1 + np.exp(-x))\n\ndef Relu(x):\n    return np.maximum(0, x)\n\ndef tanh(x):\n    return np.tanh(x)\n\ndef Weight_Initialization(Initialization, activate = 'sigmoid', deviation = 1):\n    input_data = np.random.randn(1000, 100)\n    node_num = 100\n    hidden_layer = 5\n    activations = {}\n    \n    x = input_data\n    \n    for i in range(hidden_layer):\n        if i != 0:\n            x = activations[i-1]\n        \n        if Initialization == 'Random':\n            w = np.random.randn(node_num, node_num) * deviation\n        elif Initialization == 'Xavier':\n            w = np.random.randn(node_num, node_num) * np.sqrt(1\/node_num)\n        elif Initialization == 'He':\n            w = np.random.randn(node_num, node_num) * np.sqrt(2\/node_num)\n        \n        a = np.dot(x, w)\n        \n        if activate == 'sigmoid':\n            z = sigmoid(a)\n        elif activate == 'Relu':\n            z = Relu(a)\n        elif activate == 'tanh':\n            z = tanh(a)\n        \n        activations[i] = z\n            \n    \n    for i, a in activations.items():\n        plt.subplot(1, len(activations), i+1)\n        plt.title(str(i + 1) + '-layer')\n        if i != 0:\n            plt.yticks([], [])\n        if Initialization == 'He':\n            plt.ylim(0, 7000)\n        plt.xlim(0.1, 1.0)\n        plt.hist(a.flatten(), 30, range=(0, 1))\n    \n    plt.show()\n","c9090785":"# Random Initialization\nWeight_Initialization(Initialization='Random', activate='sigmoid')\n# Deviation = 0.01\nWeight_Initialization(Initialization='Random', activate='sigmoid', deviation=0.01)","8f83b768":"# Xavier Initialization\nWeight_Initialization(Initialization='Xavier', activate='sigmoid')\n# activation Relu\nWeight_Initialization(Initialization='Xavier', activate='Relu')","adf70dfc":"# He Initialization\nWeight_Initialization(Initialization='He', activate='Relu')","2e3b3d38":"# Xavier Initialization(\uc0ac\ube44\uc5d0\ub974 \ucd08\uae30\ud654)\n<img src = 'https:\/\/blog.kakaocdn.net\/dn\/bQn1My\/btqB1cEniE0\/to4hTdl9SzGF6zuv9MsIO1\/img.png'><\/src>\n##### ENG\n- It's called Xavier Initilization or Glorot Initilization\n- Initialize weights considering node_input and node_output\n- It's often used tanh activation function\n- it's preserve the backpropagated signal as well\n\n<b>Disadvantage<\/b>\n   - It doesn't working well in Relu activation function.\n\n\n##### KOR\n- Xavier Initilization \ub610\ub294 Glorot Initilization\ub77c\uace0 \ubd88\ub9bd\ub2c8\ub2e4.\n- \uc785\ub825 \ub178\ub4dc\uc758 \uac1c\uc218 \uc640 \ucd9c\ub825 \ub178\ub4dc\uc758 \uac1c\uc218\ub97c \uace0\ub824\ud558\uc5ec, \uac00\uc911\uce58\ub97c \ucd08\uae30\ud654\ud569\ub2c8\ub2e4.\n- \ud65c\uc131\ud654 \ud568\uc218\ub85c Tanh\ub97c \uc8fc\ub85c \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\n- \uc624\ucc28\uc5ed\uc804\ud30c\ub97c \ubcf4\uc804\ud569\ub2c8\ub2e4.\n\n<b>\ub2e8\uc810<\/b>\n- Relu \ud65c\uc131\ud654 \ud568\uc218\uc5d0\uc11c \uc798 \uc791\ub3d9\ud558\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.\n\nreference link : https:\/\/prateekvishnu.medium.com\/xavier-and-he-normal-he-et-al-initialization-8e3d7a087528","4ab61fe4":"# He Initialization(He \ucd08\uae30\ud654)\n<img src = 'https:\/\/mblogthumb-phinf.pstatic.net\/MjAxOTA3MzBfNjQg\/MDAxNTY0NDcwNTU1MDg2.5-fh3tQMhy_9dBHH2URK2w1IUxoemhGGvi2LB5DQJ5kg.qSi4AGif9AqyjjQudKYs7DGzTKVxUuvSxF_AcStEDo0g.PNG.sohyunst\/SE-b8654ce8-25af-4ff8-9c47-3f2fd5b2a884.png?type=w800'><\/src>\n##### ENG\n- It's used Relu activation function\n- He Initialization is one of the methods you can choose to bring the variance of those outputs to approximately one\n- Bias disappears in Relu activation function\n\n##### KOR\n- \uc8fc\ub85c Relu activation function\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\n- He Intialization\uc740 \ucd9c\ub825\uc758 \ubd84\uc0b0\uac12\uc744 \ub300\ub7b5 1\ub85c \ub9cc\ub4ed\ub2c8\ub2e4.\n- Relu activation function \uc5d0\uc11c \ud3b8\ud5a5\uc774 \uc0ac\ub77c\uc9d1\ub2c8\ub2e4.","168ed9fb":"# Weight Initialization(\uac00\uc911\uce58 \ucd08\uae30\ud654)\n<img src = 'https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2021\/05\/291611_dmRbfOye2PcDMl2-bQazVg.jpeg'><\/src>\n\n\n## - In Neural Network learning, neural network differ depending on intial weights\n    - \uc2e0\uacbd\ub9dd \ud559\uc2b5\uc5d0\uc11c \uac00\uc911\uce58 \ucd08\uae43\uac12\uc744 \uc5b4\ub5bb\uac8c \uc815\ud558\ub290\ub0d0\uc5d0 \ub530\ub77c\uc11c \uc2e0\uacbd\ub9dd \ud559\uc2b5\uc774 \ub2ec\ub77c\uc9d1\ub2c8\ub2e4.\n\n# Content\n\n## - Random Initialization(\ub79c\ub364 \ucd08\uae30\ud654)\n\n## - Xavier Initialization(\uc0ac\ube44\uc5d0\ub974 \ucd08\uae30\ud654)\n\n## - He Initialization(He \ucd08\uae30\ud654)\n","7f0b4311":"# If helpful or useful, plz upvote! It's Free","33eb4f3d":"# Random Initialization(\ub79c\ub364 \ucd08\uae30\ud654)\n<img src = 'https:\/\/www.researchgate.net\/profile\/Aleksandra-Vuckovic\/publication\/3978633\/figure\/fig2\/AS:394699887136770@1471115194381\/Feed-forward-neural-network-with-sigmoid-activation-function-X-i-i-1P-input.png'><\/src>\n\n##### ENG\n- Best practices recommend using a random set, with initial bias of zero.\n\n- we need to break the symmetry, It makes each neuron perform a different computation.\n\n- if symmetry condition, training can be severely penalized or even impossible.\n\n* Disadvantage\n    - if deviation == 1, Due to Vanishing Gradient Problem, Neural Network doesn't learning.\n    - if deviation == 0.01, The expression of the Activation value distribution is limited.\n\n\n##### KOR\n- \ucd5c\uace0\uc758 \ud559\uc2b5\ubc29\ubc95\uc740 \ucd08\uae30 \ud3b8\ud5a5\uc778 0\uc778 random set\uc744 \ud65c\uc6a9\ud558\ub294 \uac83\uc785\ub2c8\ub2e4,\n- \uac01 \ub274\ub828\uc774 \ub2e4\ub974\uac8c \uacc4\uc0b0\uc744 \ud574\uc57c \ud558\uae30 \ub54c\ubb38\uc5d0, \ub300\uce6d\uc131\uc744 \uae68\uc57c \ud569\ub2c8\ub2e4.\n- \ub9cc\uc57d \ub300\uce6d\uc131\uc744 \uac00\uc9c0\uac8c \ub418\ub294 \uacbd\uc6b0, \ud559\uc2b5\uc774 \ubd88\uac00\ub2a5\ud558\uac70\ub098 \uc774\uc0c1\ud560 \uac83\uc785\ub2c8\ub2e4.\n\n* \ub2e8\uc810\n    - \ud45c\uc900\ud3b8\ucc28\uac00 1\uc778 \uacbd\uc6b0, \uae30\uc6b8\uae30 \uc18c\uc2e4 \ubb38\uc81c\uac00 \ubc1c\uc0dd\ud574, \ud559\uc2b5\uc774 \uc81c\ub300\ub85c \uc2e4\ud589\ub418\uc9c0 \uc54a\ub294\ub2e4.\n    - \ud45c\uc900\ud3b8\ucc28\uac00 0.01\uc778 \uacbd\uc6b0, \ud45c\ud604\ub825\uc774 \uc81c\ud55c\ub429\ub2c8\ub2e4.\n\nreference link : https:\/\/www.coursera.org\/lecture\/machine-learning\/random-initialization-drcBh"}}