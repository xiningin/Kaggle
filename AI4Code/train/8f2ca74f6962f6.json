{"cell_type":{"a567bab0":"code","42957f25":"code","cd2d1bf5":"code","a826a017":"code","1b9bf652":"code","0e40f208":"code","d8c8992b":"code","fd25096c":"code","9394c986":"code","78f1d9cb":"code","9c58803b":"code","d5fdb981":"code","2f9286ce":"code","ee42a5c3":"code","06f4949e":"code","37d00dc0":"code","b537650a":"code","881fb3d6":"code","2c01ebf4":"code","1170d6f1":"code","fb1f5df4":"code","de9e2c10":"code","283dfd2b":"code","ff3eee7e":"code","379d001d":"code","4132eb93":"code","10651cea":"code","2c387cee":"code","690e6aa1":"code","cf406bad":"code","7f49ac85":"code","342ce8dc":"code","7e5b45e3":"markdown","a69cb01f":"markdown","b3237e6e":"markdown","25e68ae2":"markdown","1b0b1074":"markdown","e41a5f78":"markdown"},"source":{"a567bab0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib\nimport tensorflow as tf\nfrom matplotlib import colors as mcolors\nimport matplotlib.pyplot as plt\ncolor_array = list(mcolors.CSS4_COLORS.keys())\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nnames_list = []\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        #print(os.path.join(dirname, filename))\n        \n        names_list.append(filename)\nnames_list = np.array(names_list)\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","42957f25":"# TPU session :)\n# detect and init the TPU\nif False:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n\n    # instantiate a distribution strategy\n    tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n","cd2d1bf5":"root_path = '\/kaggle\/input\/price-volume-data-for-all-us-stocks-etfs\/Stocks\/'\n#root_path = \"\/kaggle\/input\/price-volume-data-for-all-us-stocks-etfs\/ETFs\/\"\n\ndf = pd.read_csv(root_path + \"googl.us.txt\")","a826a017":"## Here I will predict the close price only, but you can easily add more dimensions\ndf.head()","1b9bf652":"import matplotlib\nfrom matplotlib.pylab import rc\nfont = {'family': 'normal','weight': 'bold',\n        'size': 25}\n\nmatplotlib.rc('font', **font)\nrc('axes', linewidth=3)\n\n\nplt.subplot(1,1,1)\nplt.plot(df[\"Open\"],\"k\",label=\"Open\")\nplt.plot(df[\"Close\"],\"r\",label=\"Close\")\n\n\nplt.xlabel(\"Day\")\nplt.ylabel(r\"${\\rm value}$\")\nplt.suptitle(\"Stock price vs Day Google\")\n\nfig = matplotlib.pyplot.gcf()\n\nfig.set_size_inches(20,10)\nplt.legend(fontsize=25,handlelength=5,ncol=3)\nplt.show()","0e40f208":"# Let's check whether there is a peroid in week\/month?\nimport datetime\ndf['Date'] = pd.to_datetime(df['Date'], errors='coerce')\ndf['weekday'] = df['Date'].dt.weekday\ndf['monthday'] = df['Date'].dt.day\ndf['month'] = df['Date'].dt.month","d8c8992b":"\nfrom matplotlib.pylab import rc\nfont = {'family': 'normal','weight': 'bold',\n        'size': 25}\n\nmatplotlib.rc('font', **font)\nrc('axes', linewidth=3)\n\ncolor_array = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n\nplt.subplot(1,1,1)\n\n\nnames_array = list(df.keys()[1:5])\nfor i in range(len(names_array)):\n    #print(\"Doing %d\"%i)\n    plt.plot(df.groupby('weekday').mean().index,df.groupby('weekday').mean()[names_array[i]],color_array[i],label=names_array[i],linewidth=2,markersize=5)\n    \n\n\n\n\nplt.xlabel(\"Day\")\nplt.ylabel(r\"${\\rm value}$\")\nplt.suptitle(\"Mean Usage grouped by weekday\")\n\nfig = matplotlib.pyplot.gcf()\n\nfig.set_size_inches(20,10)\nplt.legend()\nplt.show()","fd25096c":"\nfrom matplotlib.pylab import rc\nfont = {'family': 'normal','weight': 'bold',\n        'size': 25}\n\nmatplotlib.rc('font', **font)\nrc('axes', linewidth=3)\n\ncolor_array = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n\nplt.subplot(1,1,1)\n\n\nnames_array = list(df.keys()[1:5])\nfor i in range(len(names_array)):\n    #print(\"Doing %d\"%i)\n    plt.plot(df.groupby('monthday').mean().index,df.groupby('monthday').mean()[names_array[i]],color_array[i],label=names_array[i],linewidth=2,markersize=5)\n    \n\n\n\n\nplt.xlabel(\"Day\")\nplt.ylabel(r\"${\\rm value}$\")\nplt.suptitle(\"Mean Usage grouped by monthday\")\n\nfig = matplotlib.pyplot.gcf()\n\nfig.set_size_inches(20,10)\nplt.legend()\nplt.show()","9394c986":"# By month?\n\n\nfrom matplotlib.pylab import rc\nfont = {'family': 'normal','weight': 'bold',\n        'size': 25}\n\nmatplotlib.rc('font', **font)\nrc('axes', linewidth=3)\n\ncolor_array = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n\nplt.subplot(1,1,1)\n\n\nnames_array = list(df.keys()[1:5])\nfor i in range(len(names_array)):\n    #print(\"Doing %d\"%i)\n    plt.plot(df.groupby('month').mean().index,df.groupby('month').mean()[names_array[i]],color_array[i],label=names_array[i],linewidth=2,markersize=5)\n    \n\n\n\n\nplt.xlabel(\"month\")\nplt.ylabel(r\"${\\rm value}$\")\nplt.suptitle(\"Mean Usage grouped by month\")\n\nfig = matplotlib.pyplot.gcf()\n\nfig.set_size_inches(20,10)\nplt.legend()\nplt.show()","78f1d9cb":"from sklearn import preprocessing\nfrom keras.models import Model\nfrom sklearn.preprocessing import LabelEncoder \nimport tensorflow as tf\nfrom keras.layers import Input,Masking,Dense,GRU\nfrom sklearn.model_selection import train_test_split \nfrom keras.layers import Dropout","9c58803b":"# some hyper parameters\n# Here we use 30 day data to predict future stock price. May need to modify later\ndelta_t = 30\nn_cell = 50\ndropout_rate= 0.1\nn_epoch=30\nbatch=256","d5fdb981":"checkpoint_path = \"model\/cp.ckpt\"\ncheckpoint_dir = os.path.dirname(checkpoint_path)\n\nmin_max_scaler = preprocessing.StandardScaler()\n\n# min-max scaler\nnp_scaled = min_max_scaler.fit_transform(df[names_array])\n\ndf_scaled = pd.DataFrame(np_scaled,columns=names_array)\n\n\nX = np.zeros((df_scaled.shape[0]-delta_t,delta_t,len(names_array)),dtype=float)\ny = df_scaled[\"Close\"][delta_t:]\n\n  \nfor i in range(len(y)):\n    if i%800==0:\n        print(\"Prepare data %.2f percent\"%(100*i\/len(y)))\n    X[i,:,:] = df_scaled[i:i+delta_t][names_array].values\n\n# split train test:\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True)\n\n","2f9286ce":"\ndef generate_model(MAX_TIMESTEPS,MAX_NB_VARIABLES):\n    ip = Input(shape=(MAX_TIMESTEPS,MAX_NB_VARIABLES))\n    # split into x and y two channels\n\n    x = Masking()(ip)\n    x = GRU(n_cell)(x)\n\n    out = Dense(1)(x)\n    #out = Dropout(dropout_rate)(x)\n    #loc, scale = tf.keras.layers.GaussianNoise(stddev=0.01)(out)\n    \n    # now we only output loc\n    #out=loc\n\n    model = Model(ip, out)\n    model.summary()\n\n    # add load model code here to fine-tune\n\n    return model\n\nmodel = generate_model(delta_t,X.shape[2])\nmodel.compile(loss='mae', optimizer='adam')\n#model.summary()\n\ncallback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n                                                 save_weights_only=True,\n                                                 verbose=1)\n\n\n","ee42a5c3":"\nhistory = model.fit(X_train, y_train, epochs=n_epoch, batch_size=batch, validation_data=(X_test, y_test),callbacks=[callback], verbose=2, shuffle=False)\n\n\n","06f4949e":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=False)\ny_pre = model.predict(X_test)","37d00dc0":"from matplotlib.pylab import rc\nfont = {'family': 'normal','weight': 'bold',\n        'size': 25}\n\nmatplotlib.rc('font', **font)\nrc('axes', linewidth=3)\n\ncolor_array = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n\nplt.subplot(1,1,1)\n\nmin_val = np.nanmin(df[\"Close\"])\nmax_val = np.nanmax(df[\"Close\"])\n\n\n\ndate_array = np.arange(0,len(y_test),1)\n\ny1 = min_val+(max_val-min_val)*y_test\ny2 = min_val+(max_val-min_val)*y_pre[:,0]\nplt.plot(date_array,y1,\"k\",label = \"Data\")\nplt.plot(date_array,y2,\"r\",label = \"Prediction\")\nplt.xlabel(\"Day\")\nplt.ylabel(r\"${\\rm value}$\")\nplt.suptitle(\"Stock price MAE=%.4f\"%np.nanmean(abs(y1-y2)))\n\nfig = matplotlib.pyplot.gcf()\n\nfig.set_size_inches(20,10)\nplt.legend()\nplt.show()","b537650a":"# scale dot attention:\n\nimport tensorflow as tf\nimport os\n\nfrom sklearn import preprocessing\n\nfrom sklearn.model_selection import train_test_split \n\n\n\n\ndef scaled_dot_product_attention(q, k, v, mask):\n    matmul_qk = tf.matmul(q, k, transpose_b=True)\n    # Dimension of k\n    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n    scaled_attention_logits = matmul_qk \/ tf.math.sqrt(dk)\n    if mask is not None:\n        scaled_attention_logits += (mask * -1e9)  \n    # calculate attention weight:\n    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n    output = tf.matmul(attention_weights, v)\n    return output, attention_weights\n\n# Multi-head Attention:\n# This is what we use\nclass MultiHeadAttention(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads):\n        \n        # Always use Super to inheriatte and avoid extra code.\n        assert d_model%num_heads==0\n        super(MultiHeadAttention, self).__init__()\n        self.num_heads = num_heads\n        self.d_model = d_model\n        # sanity check:\n        assert d_model % self.num_heads == 0\n        self.depth = d_model \/\/ self.num_heads\n        # Q K W:\n        self.wq = tf.keras.layers.Dense(d_model)\n        self.wk = tf.keras.layers.Dense(d_model)\n        self.wv = tf.keras.layers.Dense(d_model)\n        \n        self.dense = tf.keras.layers.Dense(d_model)\n    def split_heads(self, x, batch_size):\n        # Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n    def call(self, v, k, q, mask):\n        batch_size = tf.shape(q)[0]\n        q = self.wq(q)  # (batch_size, seq_len, d_model)\n        k = self.wk(k)  # (batch_size, seq_len, d_model)\n        v = self.wv(v)  # (batch_size, seq_len, d_model)\n        \n        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n        \n        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n        \n        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n        # https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/transpose : perm\n        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n        concat_attention = tf.reshape(scaled_attention, \n                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n        return output, attention_weights\n    \n    \n        \n        \n\n\n\n","881fb3d6":"## Encoder decoder for Time series:\n\n# pointwise feed forward network\ndef point_wise_feed_forward_network(d_model, dff):\n    # Two FC layers:\n    return tf.keras.Sequential([\n      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n  ])\n\n# Change embedding since it's not int anymore:\nclass EmbeddingLayer(tf.keras.layers.Layer):\n    def __init__(self,embedding_size):\n        super(EmbeddingLayer,self).__init__()\n        self.embedding_size=embedding_size\n\n    def build(self,input_shape):\n        with tf.name_scope('embedding'):\n            self.shared_weights=self.add_weight(name='weights',\n                                                shape=[input_shape[-1],self.embedding_size],\n                                                initializer=tf.random_normal_initializer(mean=0.,\n                                                                                         stddev=self.embedding_size ** -0.5))\n        super(EmbeddingLayer,self).build(input_shape)\n\n\n    def call(self,x):\n        y=tf.einsum('bsf,fk->bsk',x,self.shared_weights)\n        return y\n    \n\nclass EncoderLayer(tf.keras.layers.Layer):\n    # Here we use a 0.1 dropout rate as default\n    def __init__(self, d_model, num_heads, dff, rate=0.1):\n        super(EncoderLayer, self).__init__()\n        self.mha = MultiHeadAttention(d_model, num_heads)\n        self.ffn = point_wise_feed_forward_network(d_model, dff)\n\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n        \n    def call(self, x, training, mask):\n        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n        attn_output = self.dropout1(attn_output, training=training)\n        \n        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        \n        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n    \n        return out2\nsample_encoder_layer = EncoderLayer(512, 8, 2048)\n\nsample_encoder_layer_output = sample_encoder_layer(tf.random.uniform((64, 43, 512)), False, None)\n\nprint(sample_encoder_layer_output.shape)  # (batch_size, input_seq_len, d_model)\n\nclass DecoderLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, dff, rate=0.1):\n        super(DecoderLayer, self).__init__()\n\n        self.mha1 = MultiHeadAttention(d_model, num_heads)\n        self.mha2 = MultiHeadAttention(d_model, num_heads)\n\n        self.ffn = point_wise_feed_forward_network(d_model, dff)\n\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n        self.dropout3 = tf.keras.layers.Dropout(rate)\n        \n    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n        # enc_output.shape == (batch_size, input_seq_len, d_model)\n\n        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n        attn1 = self.dropout1(attn1, training=training)\n        out1 = self.layernorm1(attn1 + x)\n\n        attn2, attn_weights_block2 = self.mha2(\n            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n        attn2 = self.dropout2(attn2, training=training)\n        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n\n        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n        ffn_output = self.dropout3(ffn_output, training=training)\n        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n\n        return out3, attn_weights_block1, attn_weights_block2\n\n\n\n","2c01ebf4":"def get_angles(pos, i, d_model):\n    angle_rates = 1 \/ np.power(10000, (2 * (i\/\/2)) \/ np.float32(d_model))\n    return pos * angle_rates\n\ndef positional_encoding(position, d_model):\n    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n                              np.arange(d_model)[np.newaxis, :],\n                              d_model)\n\n    # apply sin to even indices in the array; 2i\n    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n\n    # apply cos to odd indices in the array; 2i+1\n    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n    pos_encoding = angle_rads[np.newaxis, ...]\n    return tf.cast(pos_encoding, dtype=tf.float32)\n\nclass Encoder(tf.keras.layers.Layer):\n    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n               maximum_position_encoding, rate=0.1):\n        super(Encoder, self).__init__()\n\n        self.d_model = d_model\n        self.num_layers = num_layers\n\n        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n        self.pos_encoding = positional_encoding(maximum_position_encoding, \n                                                self.d_model)\n\n\n        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n                           for _ in range(num_layers)]\n\n        self.dropout = tf.keras.layers.Dropout(rate)\n        \n    def call(self, x, training, mask):\n        seq_len = tf.shape(x)[1]\n    \n        # adding embedding and position encoding.\n        #print(\"Check\",x.shape)\n        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n        #x = tf.keras.layers.Dense(self.d_model)(x)\n        #print(\"check 2\",x.shape)\n        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n        x += self.pos_encoding[:, :seq_len, :]\n        #print(\"check 3\",x.shape)\n\n        x = self.dropout(x, training=training)\n        #print(\"check 4\",x.shape)\n\n        for i in range(self.num_layers):\n            x = self.enc_layers[i](x, training, mask)\n        return x  # (batch_size, input_seq_len, d_model)\n    \nclass Decoder(tf.keras.layers.Layer):\n    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n               maximum_position_encoding, rate=0.1):\n        super(Decoder, self).__init__()\n        self.d_model = d_model\n        self.num_layers = num_layers\n\n        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n\n        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n                           for _ in range(num_layers)]\n        self.dropout = tf.keras.layers.Dropout(rate)\n        \n    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n        seq_len = tf.shape(x)[1]\n        attention_weights = {}\n\n        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n        #x = tf.keras.layers.Dense(self.d_model)(x)\n        \n        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n        x += self.pos_encoding[:, :seq_len, :]\n\n        x = self.dropout(x, training=training)\n        for i in range(self.num_layers):\n            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n                                                 look_ahead_mask, padding_mask)\n\n            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n    \n        # x.shape == (batch_size, target_seq_len, d_model)\n        return x, attention_weights\n    ","1170d6f1":"class Transformer(tf.keras.Model):\n    def __init__(self, num_layers, d_model, num_heads, dff, input_seq_size, \n               output_seq_size, input_delta_t, output_delta_t, rate=0.1):\n        super(Transformer, self).__init__()\n\n        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n                               input_seq_size, input_delta_t, rate)\n\n        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n                               output_seq_size, output_delta_t, rate)\n        \n        # Now it output one cell: we ignore sigma for now and only miu\n\n        #self.final_layer = tf.keras.layers.Dense(output_seq_size)\n        self.final_layer = tf.keras.layers.Dense(1)\n        \n        # Optional: Add sigma to model\n        #self.final_layer_sigma = tf.keras.layers.Dense(1)\n\n\n    def call(self, inp, tar, training, enc_padding_mask, \n           look_ahead_mask, dec_padding_mask):\n        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n        #print(\"check encoder size\",enc_output.shape)\n\n    \n        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n        dec_output, attention_weights = self.decoder(\n            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n        \n        #print(\"check decoder size\",dec_output.shape)\n\n        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n\n        return final_output, attention_weights\n\n\n\n","fb1f5df4":"# sanity check:\n# We encoder the float32 input to input_seq_size\/output_seq_size integers\n# The output is a sliding time table for different time scale prediction:\n# Eg: you need to make sure your prediction delta_t<output delta_t and input data delta_t < input_delta_t\n# For GTX 1060 we can set batch=16 and use 4X batch size for Tesla P40\n\nbatch = 8\n\nsample_transformer = Transformer(\n    num_layers=2, d_model=512, num_heads=8, dff=2048, \n    input_seq_size=1000, output_seq_size=1000, \n    input_delta_t=1440, output_delta_t=240)\n\n# input: batch+sequence length\n# biggest length for in\/out put is pe_input,  pe_target\ntemp_input = tf.random.uniform((batch, 720), dtype=tf.int64, minval=0, maxval=1000)\ntemp_target = tf.random.uniform((batch, 3), dtype=tf.int64, minval=0, maxval=1000)\n\n#temp_input = tf.cast(temp_input,dtype=tf.float32)\n#temp_target = tf.cast(temp_target,dtype=tf.float32)\n\nfn_out, _ = sample_transformer(temp_input, temp_target, training=False, \n                               enc_padding_mask=None, \n                               look_ahead_mask=None,\n                               dec_padding_mask=None)\n\nprint(\"final output size\",fn_out.shape)  # (batch_size, tar_seq_len, target_vocab_size)\n\n","de9e2c10":"delta_t = 30\ndelta_t_out = 1\n\ntemp = df[names_array].values\n\n# normalize first\n\ntemp = (temp - temp.min(axis=0)) \/ (temp.max(axis=0) - temp.min(axis=0))\n\nlower, upper = 0, 4999\ntemp = lower + (upper - lower) * temp\ntemp = np.array(temp,dtype=int)\n\n\n\nfor j in range(temp.shape[1]):\n    for i in range(delta_t_out):\n        if i==0 and j==0:\n            y = temp[delta_t:-delta_t_out,j]\n        else:\n            y = np.c_[y,temp[delta_t+i:-(delta_t_out-i),j]]\n    \n    \n\nfor j in range(temp.shape[1]):\n    for i in range(delta_t):\n        if i%300==0:\n            print(\"Doing %.2f percent\"%((100*i+100*j*delta_t)\/delta_t\/temp.shape[1]))\n        if i==0 and j==0:\n            X = temp[delta_t_out:-delta_t,j]\n        else:\n            X = np.c_[X,temp[delta_t_out+i:-(delta_t-i),j]]\n    \n    \n\nX = np.atleast_3d(X)\ny = y[:,-1]","283dfd2b":"## Optimizor:\n## We may need to modify this part since it's a very slow learning rate and too much warm-up steps :)\n\nimport matplotlib.pyplot as plt\n\nd_model=128\n\nclass CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(self, d_model, warmup_steps=4000):\n        super(CustomSchedule, self).__init__()\n    \n        self.d_model = d_model\n        self.d_model = tf.cast(self.d_model, tf.float32)\n\n        self.warmup_steps = warmup_steps\n        \n    def __call__(self, step):\n        arg1 = tf.math.rsqrt(step)\n        arg2 = step * (self.warmup_steps ** -1.5)\n\n        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n    \nlearning_rate = CustomSchedule(d_model)\n\n\noptimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n                                     epsilon=1e-9)\n# Learning rate curve:\ntemp_learning_rate_schedule = CustomSchedule(d_model)\n\nplt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\nplt.ylabel(\"Learning Rate\")\nplt.xlabel(\"Train Step\")\nfig = matplotlib.pyplot.gcf()\n\nfig.set_size_inches(16,16)\nplt.show()","ff3eee7e":"# Loss function:\n# loss and metric\n\n# For now I use sparse-cross entropy. But MAE may make more sense here:\n\nloss_object = tf.keras.losses.MeanSquaredError(reduction='none')\n\n\ndef loss_function(real, pred):\n    #mask = tf.math.logical_not(tf.math.equal(real, 0))\n    loss_ = loss_object(real, pred)\n\n    #mask = tf.cast(mask, dtype=loss_.dtype)\n    #loss_ *= mask\n  \n    return tf.reduce_sum(loss_)\/tf.cast(len(loss_),dtype=tf.float32)\n\n\n\n\ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\n\n\ntrain_accuracy = tf.keras.metrics.MeanSquaredError(name='mean_squared_error',dtype=tf.float32)\n\n\n\n# Optional\n#train_accuracy = tf.keras.metrics.MeanSquaredError(name='train_MSE')\n\n\n\n\n\n\n","379d001d":"def create_padding_mask(seq):\n    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n  \n    # add extra dimensions to add the padding\n    # to the attention logits.\n    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n\ndef create_look_ahead_mask(size):\n    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n    return mask  # (seq_len, seq_len)\n\ndef create_masks(inp, tar):\n    # Encoder padding mask\n    enc_padding_mask = create_padding_mask(inp)\n\n    # Used in the 2nd attention block in the decoder.\n    # This padding mask is used to mask the encoder outputs.\n    dec_padding_mask = create_padding_mask(inp)\n\n    # Used in the 1st attention block in the decoder.\n    # It is used to pad and mask future tokens in the input received by \n    # the decoder.\n    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n    dec_target_padding_mask = create_padding_mask(tar)\n    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n\n      \n    return enc_padding_mask, combined_mask, dec_padding_mask","4132eb93":"\n\n\n\nbatch = 8\n\ntransformer = Transformer(\n    num_layers=2, d_model=128, num_heads=8, dff=512, \n    input_seq_size=5000, output_seq_size=5000, \n    input_delta_t=1440, output_delta_t=240)\n\n\n\n# save file: optional\nimport os\n\ncheckpoint_path = \"checkpoints\/TST\"\nos.system(\"mkdir %s\"%checkpoint_path)\n\nckpt = tf.train.Checkpoint(transformer=transformer,\n                           optimizer=optimizer)\n\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n\n# if a checkpoint exists, restore the latest checkpoint.\nif ckpt_manager.latest_checkpoint:\n    ckpt.restore(ckpt_manager.latest_checkpoint)\n    print ('Latest checkpoint restored!!')\n\n\n\ntrain_step_signature = [\n        tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n        tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n    ]\n\n@tf.function(input_signature=train_step_signature)\n\ndef train_step(inp, tar):\n        \n    tar_inp = tar\n    tar_real = tar\n\n    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n    with tf.GradientTape() as tape:\n        # No mask for now : Optional\n        enc_padding_mask, combined_mask, dec_padding_mask = None,None,None\n        predictions, _ = transformer(inp, tar_inp, True, enc_padding_mask, combined_mask, dec_padding_mask)\n        predictions = predictions[:,:,0]\n        loss = loss_function(tar_real, predictions)\n        # Optional: add validation loss\n        ## Optional: Add MSE error term. Since the number in SCCE doesn't make sense. Add MSE to punish far away dots like 0 and 999\n        #predictions_id = tf.argmax(predictions, axis=-1)\n        #loss+=float(tf.reduce_sum(tf.keras.losses.MSE(tar,predictions_id))\/(10000*batch))\n        #value = float(tf.reduce_sum(tf.keras.losses.MSE(tar,predictions_id))\/(1*batch))\n        # Avoid gradient exploding\n        \"\"\"\n        if not loss>0:\n            value=float(100000)\n        loss+=value\n        \n        \"\"\"\n        \n        \n        # Or we can only use MSE loss.\n        \n    gradients = tape.gradient(loss, transformer.trainable_variables)    \n    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n\n    train_loss(loss)\n    train_accuracy(tar_real, predictions)\n\n\n\n\n\n\n\n\n\n\n\n","10651cea":"\n\n\n#Train and save:\n\nimport time\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True)\n\nEPOCHS = 500\ntrain_dataset = tf.data.Dataset.from_tensor_slices((X_train,y_train))\nbatch=128\n\nN = len(y_train)\n\nfor epoch in range(EPOCHS):\n    start = time.time()\n  \n    train_loss.reset_states()\n    train_accuracy.reset_states()\n    for i in range(N\/\/batch):\n        inp, tar=X_train[batch*i:min(batch*i+batch,N),:,0],y_train[batch*i:min(batch*i+batch,N)]\n        tar = np.atleast_2d(tar)\n        lo = train_step(inp, tar)\n        if epoch%10==0 and i==0:\n            # shuffle every 10 epochs\n            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True)\n            print(\"Doing %d (%d) batch in epoch %d \"%(i,N\/\/batch,epoch))\n\n            #print(\"Loss\",train_loss.result(), \"MSE\",train_accuracy.result())\n            print(\"MSE\",train_accuracy.result())\n    \n  ","2c387cee":"# testing:\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=False)\n\nN_test = len(y_test)\n\ny_pred_all = []\nfor i in range(N_test\/\/batch):\n    if i%200==0:\n            print(\"Doing %d (%d)\"%(i,N_test\/\/batch))\n    \n    inp, tar=X_train[batch*i:min(batch*i+batch,N),:,0],y_train[batch*i:min(batch*i+batch,N)]\n    tar = np.atleast_2d(tar)\n\n\n    # enc_padding_mask, combined_mask, dec_padding_mask = None,None,None\n    \n    predictions, attention_weights = transformer(inp, \n                                                 tar,\n                                                 False,\n                                                 None,None,None)\n    predictions = np.array(predictions)\n    y_pred_all.extend(predictions.ravel())\n\n\n    \ny_pred_all = np.array(y_pred_all)\n\nprint(\"Train+Test all set!\")\n\n\ny_test = y_train[:len(y_pred_all)]\n\n\n","690e6aa1":"\n\nimport matplotlib\nfrom matplotlib.pylab import rc\n\n\nfont = {'family': 'normal','weight': 'bold',\n        'size': 25}\n\nmatplotlib.rc('font', **font)\nrc('axes', linewidth=3)\n\n\n\nplt.plot(y_pred_all,\"ro\",label=\"Transformer Prediction\")\nplt.plot(y_test,\"k\",label=\"Data\")\nplt.suptitle(\"Training set\")\n\nplt.legend()\nfig = matplotlib.pyplot.gcf()\n\n\nfig.set_size_inches(35,12)\n\nplt.show()\n\n\n","cf406bad":"# testing:\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=False)\n\nN_test = len(y_test)\n\ny_pred_all = []\nfor i in range(N_test\/\/batch):\n    if i%200==0:\n            print(\"Doing %d (%d)\"%(i,N_test\/\/batch))\n    \n    inp, tar=X_test[batch*i:min(batch*i+batch,N),:,0],y_test[batch*i:min(batch*i+batch,N)]\n    tar = np.atleast_2d(tar)\n\n\n    # enc_padding_mask, combined_mask, dec_padding_mask = None,None,None\n    \n    predictions, attention_weights = transformer(inp, \n                                                 tar,\n                                                 False,\n                                                 None,None,None)\n    predictions = np.array(predictions)\n    y_pred_all.extend(predictions.ravel())\n\n\n    \ny_pred_all = np.array(y_pred_all)\n\nprint(\"Train+Test all set!\")\n\n\ny_test = y_test[:len(y_pred_all)]\n\n\n\n\n","7f49ac85":"\nimport matplotlib\nfrom matplotlib.pylab import rc\n\n\nfont = {'family': 'normal','weight': 'bold',\n        'size': 25}\n\nmatplotlib.rc('font', **font)\nrc('axes', linewidth=3)\n\n\n\nplt.plot(y_pred_all,\"ro\",label=\"Transformer Prediction\")\nplt.plot(y_test,\"k\",label=\"Data\")\nplt.suptitle(\"Testing set\")\n\nplt.legend()\nfig = matplotlib.pyplot.gcf()\n\n\nfig.set_size_inches(35,12)\nplt.show()\n\n\n\n","342ce8dc":"print(\"All set\")","7e5b45e3":"## Well, for now the Transformer doesn't seems to be good There is definitely a overfitting inside since there is a huge difference between training and testing set results.\nI will add validation loss in the total loss function soon.\nAlso, it seems the model doesn't learn the long-term trend in the stock price","a69cb01f":"## Baseline model: GRU+dense layer","b3237e6e":"# Now we have the baseline model. Let's go transformer :)\nIf you are interested, please also check my kernel at: https:\/\/www.kaggle.com\/peraktong\/transformer-ts","25e68ae2":"In conclusion, we saw some trend here, which means we should choose a time-series model for this problem","1b0b1074":"# The prediction seems to have a \"Latency\" inside.","e41a5f78":"# Transformer in Time series data: Stock price verion\n## Jason's Transformer in time series problem episode II :)\nDoesn't seem to work now. Need further thinking :) <br>\nOur goal is to predict close price using our model from previous input <br>\nEpisode one: https:\/\/www.kaggle.com\/peraktong\/transformer-ts <br>\nThanks to this useful link : https:\/\/www.tensorflow.org\/tutorials\/text\/transformer <br>\nPlease let me know if you have any question. <br>\n"}}