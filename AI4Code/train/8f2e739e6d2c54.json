{"cell_type":{"28bcdc9d":"code","fef50551":"code","7a4d1a6f":"code","8b0e0fdf":"code","56e6ec71":"code","e6e2e8d4":"code","b7b17c77":"code","6e8412b2":"code","ba8c1f28":"code","9edc3b28":"code","5f149af3":"code","99e364a2":"code","a88604ed":"code","982e348e":"code","c22086f1":"code","6d6286de":"code","e2792688":"code","b1e58b2f":"code","e94924cc":"code","1951242e":"code","bd060be4":"code","765555fc":"code","9de50e47":"code","375b5fee":"code","323e8f1a":"code","eb4df734":"code","f0d69461":"code","d69e153a":"code","51204ae1":"code","d9ab4041":"code","2615de43":"code","6527df65":"code","c9a42d68":"code","0fd65bbc":"code","31643303":"code","4d5b080d":"code","2983b083":"code","a475eade":"code","037d0678":"markdown","c47e062e":"markdown","11575514":"markdown","df82ccb5":"markdown","c9c51bbb":"markdown","64b5d634":"markdown","c5ee8063":"markdown","e571f1e2":"markdown","655ec3eb":"markdown","8869a4a8":"markdown","629e61e6":"markdown","446768d7":"markdown","ea766f30":"markdown","92356de7":"markdown","813115e1":"markdown","d585ceb1":"markdown","8bca81e6":"markdown","712361f3":"markdown","9279b778":"markdown","bf9a490f":"markdown","ea3bbf1d":"markdown","37b66e4e":"markdown"},"source":{"28bcdc9d":"# feature extractoring and preprocessing data\nimport librosa\nimport librosa, librosa.display, os, csv\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import pyplot\n%matplotlib inline\nimport os\nfrom PIL import Image\nimport pathlib\nimport csv\n\n# Preprocessing\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler,MinMaxScaler,scale\nfrom sklearn.model_selection import GridSearchCV, train_test_split, RepeatedStratifiedKFold, cross_val_score, KFold,StratifiedKFold \nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score,roc_curve,roc_auc_score, auc\nfrom sklearn.decomposition import PCA\nfrom sklearn import metrics\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n\n\n#Keras\nimport keras\nfrom keras import models\nfrom keras import layers\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPool2D, Dropout\nfrom tensorflow.keras.utils import to_categorical \n\n\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\n\nimport xgboost as xgb\nimport joblib\n\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')","fef50551":"#Loading CSV file\ntrain_csv = pd.read_csv(\"..\/input\/coughclassifier-trial\/cough_trial_extended.csv\") #\u0110\u1ecdc file csv\ndataset = \"..\/input\/coughclassifier-trial\/cough_trial_extended.csv\" # \u0110\u01b0\u1eddng d\u1eabn\nanothercsv = pd.read_csv('..\/input\/coughclassifier-trial\/cough_dataset.csv')\nprint (train_csv.head(4)) # L\u1ea5y 4 gi\u00e1 tr\u1ecb \u0111\u1ea7u ti\u00ean\nprint (anothercsv)","7a4d1a6f":"print (train_csv['class'].unique()) #T\u00ecm s\u1ed1 class trong c\u1ed9t class\nprint(anothercsv['status'].unique())\nprint ('\\n', train_csv['class'])","8b0e0fdf":"cmap = plt.get_cmap('inferno')\ntot_rows = train_csv.shape[0]\nprint ('tot_rows',tot_rows, 's\u1ed1 c\u1ed9t: ', train_csv.shape[1])\nfor i in range(1): #Thay range(1) = range(tot_rows)\n    source = train_csv['file_properties'][i]\n    filename = '..\/input\/coughclassifier-trial\/trial_covid\/'+source\n    y,sr = librosa.load(filename, mono=True, duration=5)\n    plt.specgram(y, NFFT=2048, Fs=2, Fc=0, noverlap=128, cmap='magma', sides='default', mode='default', scale='dB');\n    #plt.axis('off');\n    plt.savefig(f'.\/{source[:-3].replace(\".\", \"\")}.png')\n    print (source[:-4]) #B\u1ecf \u0111i 4 k\u00ed t\u1ef1 cu\u1ed1i l\u00e0 \".wav\"\n    #plt.clf() ","56e6ec71":"header = 'filename chroma_stft rmse spectral_centroid spectral_bandwidth rolloff zero_crossing_rate'\nfor i in range(1, 21):\n    header += f' mfcc{i}' #C\u00f3 f th\u00ec h\u00e0m trong ' ' m\u1edbi ch\u1ea1y\nheader += ' label'\nheader = header.split()\nprint (header)","e6e2e8d4":"file = open('data_new_extended.csv', 'w')\nwith file:\n    writer = csv.writer(file)\n    writer.writerow(header)\ndata_new_extended = pd.read_csv('..\/input\/coughclassifier-trial\/data_new_extended.csv')\n#print ('data_new_extended\\n',data_new_extended)\nfor i in range(2,4):\n        source = train_csv['file_properties'][i]\n        print ('source',source)\n        file_name = '..\/input\/coughclassifier-trial\/trial_covid\/'+source\n        label =  train_csv['class'][i]\n        print ('\\nlabel', label)\n        y,sr = librosa.load(file_name, mono=True, duration=5)\n        chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)\n        rmse = librosa.feature.rms(y=y)\n        spec_cent = librosa.feature.spectral_centroid(y=y, sr=sr,hop_length=1024)\n        spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr,hop_length=1024)\n        rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr,hop_length=1024) #N\u00ean c\u00f3 hop-length\n        #print ('spec_cent',spec_cent, 'shape:',spec_cent.shape)\n        #print ('spec_bw',spec_bw, 'shape:',spec_bw.shape)\n        #print ('rolloff',rolloff, 'shape:',rolloff.shape)\n        zcr = librosa.feature.zero_crossing_rate(y)\n        mfcc = librosa.feature.mfcc(y=y, sr=sr)\n        #print ('mfcc',mfcc)\n        to_append = f'{np.mean(chroma_stft)} {np.mean(rmse)} {np.mean(spec_cent)} {np.mean(spec_bw)} {np.mean(rolloff)} {np.mean(zcr)}'    \n        #np.mean t\u00ednh trung b\u00ecnh gi\u00e1 tr\u1ecb\n        librosa.display.specshow(mfcc, x_axis='time') #Show MFCC\n        plt.title('MFCC')\n        #plt.show()\n        \n        for e in mfcc:\n            to_append += f' {np.mean(e)}'\n        to_append += f' {label}'\n        value = [str(source)]\n        value.extend(to_append.split())\n        file = open('data_new_extended.csv', 'a')\n        with file:\n            writer = csv.writer(file)\n            writer.writerow(value)\n            \ndata_new_extended = pd.read_csv('.\/data_new_extended.csv')\nprint ('data_new_extended\\n',data_new_extended)","b7b17c77":"# Drawing mel spectrogram\ny,sr = librosa.load('..\/input\/coughclassifier-trial\/trial_covid\/-ej81N6Aqo4_ 0.000_ 8.000.wav', mono=True)\nfig, ax = plt.subplots(sharex=True, sharey=True)\n#draw sound with time axis \nlibrosa.display.waveplot(y, sr=sr, max_points=50000.0, x_axis='time')\n\nS = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=129,fmax=8000)\n\nprint ('S.shape',S.shape)\nfig, ax = plt.subplots()\nS_dB = librosa.power_to_db(S, ref=np.max)\nimg = librosa.display.specshow(S_dB, x_axis='time',\n                         y_axis='mel', sr=sr,\n                         fmax=8000, ax=ax)\nfig.colorbar(img, ax=ax, format='%+2.0f dB')\nax.set(title='Mel-frequency spectrogram 2')","6e8412b2":"mel_mean_feature = pd.read_csv('..\/input\/covid-19-cough-primary-features\/mel_mean_feature.csv')\nmfcc_mean_feature = pd.read_csv('..\/input\/covid-19-cough-primary-features\/mfcc_mean_feature.csv')\nchroma_mean_feature = pd.read_csv('..\/input\/covid-19-cough-primary-features\/chroma_mean_feature.csv')\ndata2 = pd.concat([mfcc_mean_feature.iloc[:, :-1],mel_mean_feature.iloc[:, :-1],chroma_mean_feature], axis = 1)\nprint (data2.shape)\nprint(data2.head(4))","ba8c1f28":"# This code is for 170-case dataset\n\n#data = pd.read_csv('..\/input\/coughclassifier-trial\/data_new_extended.csv')\n#print (data.iloc[:,-1] )\n#for i in range(0,len(data.iloc[:,-1])):\n#    if data.iloc[i ,-1] == 'not_covid':\n#        data.iloc[i ,-1] = '0'\n#    else:\n#        data.iloc[i ,-1] = '1'\n#data.head(6)    ","9edc3b28":"# This code is for 4068-case dataset \ndata = data2\nprint ('data.shape',data.shape)\ndata.head(3)","5f149af3":"# This code is for 170 - case dataset\n\n# Dropping unneccesary columns\n#data = data.drop(['filename'],axis=1)\n#print (data.shape)","99e364a2":"genre_list = data.iloc[:, -1]\n#print ('genre_list\\n',genre_list)\nencoder = LabelEncoder()\ny = encoder.fit_transform(genre_list) #G\u00e1n nh\u00e3n 0,1 cho class. C\u00f3 th\u1ec3 n\u00f3i l\u00e0 \u0111\u01b0a v\u1ec1 one hot coding\nneg, pos = np.bincount(y)\ntotal = neg + pos\nprint ('positive: {} ({:.2f}% of total) \\nnegative cases: {}'.format(pos, 100 * pos\/total ,neg)) ","a88604ed":"scaler = StandardScaler()\nprint ('X before scaling:\\n',np.array(data.iloc[:, :-1]))\nX = scaler.fit_transform(np.array(data.iloc[:, :-1], dtype = float)) #kh\u00f4ng scale 2 c\u1ed9t file name, label\nprint ('\\nX after scaling:\\n',X,'\\nX.shape', X.shape)","982e348e":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle = True, \n                                                    random_state = None, stratify = y)\n#print (y_test)\nprint (len(y_test))","c22086f1":"print('X_train.shape:',X_train.shape)\nprint('\\nX_train.shape[1]:',X_train.shape[1])\nprint ('\\ny_train.shape:',y_train.shape)","6d6286de":"def get_model():\n    model = models.Sequential()\n    model.add(layers.Dense(512, activation='relu', input_shape=(X_train.shape[1],))) #\u0110\u1ea7u v\u00e0o \u0111\u00e3 \u0111\u01b0\u1ee3c transpose\n\n    model.add(layers.Dense(256, activation='relu'))\n\n    model.add(Dropout(0.2))\n\n    model.add(layers.Dense(128, activation='relu'))\n\n    model.add(layers.Dense(64, activation='relu'))\n\n    model.add(Dropout(0.2))\n\n    model.add(layers.Dense(10, activation='relu'))\n\n    model.add(layers.Dense(2, activation='softmax'))\n    \n    model.compile(optimizer= keras.optimizers.Adam(learning_rate=0.001),\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n    \n    return model\n#Model n\u00e0y c\u00f3 d\u1ea5u hi\u1ec7u over fitting n\u00ean cho drop out\n\n# plot model\nmodel = get_model()\nmodel.summary()","e2792688":"# # loading weights into new model\n# loaded_model.load_weights(\"model.h5\")\n# print(\"Loaded model from disk\")\n \n# # evaluate loaded model on test data\n# loaded_model.compile(optimizer='adam',\n#               loss='sparse_categorical_crossentropy',\n#               metrics=['accuracy'])\n# score = loaded_model.evaluate(X, Y, verbose=0)\n# print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))","b1e58b2f":"batch_size = 16\nearly_stopping_patience = 10\n\n# Add early stopping\n\nmy_callbacks = [\n    tf.keras.callbacks.ModelCheckpoint(filepath='.\/model_{epoch:02d}.h5', \n                                       save_freq='epoch', \n                                       save_best_only=True,\n                                       period = 10),\n    tf.keras.callbacks.EarlyStopping(\n    monitor=\"val_loss\", patience=early_stopping_patience, restore_best_weights=True\n    )\n]\n\n\n\nhistory = model.fit(X_train, y_train,\n                    epochs=100,\n                    batch_size=batch_size,\n                    callbacks = my_callbacks,\n                    validation_split=0.15)","e94924cc":"def history_loss_acc(history,name):\n    # list all data in history\n    print(history.history.keys())\n    \n    # summarize history for accuracy\n    \n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.title('model accuracy_'+name)\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()\n    # summarize history for loss\n    \n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss_'+name)\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()\n\nhistory_loss_acc(history, 'Original data')","1951242e":"# serialize model to JSON\nmodel_json = model.to_json()\nwith open(\"model.json\", \"w\") as json_file:\n    json_file.write(model_json)\n# serialize weights to HDF5\nmodel.save_weights(\"model.h5\")\nprint(\"Saved model to disk\")","bd060be4":"test_loss, test_acc = model.evaluate(X_test,y_test) ","765555fc":"print('test_acc: ',test_acc)","9de50e47":"predictions = model.predict(X_test)\n#print ('so predict:',len(predictions))\n#print('\\npredictions[0].shape',predictions[0].shape)\n#print('\\nnp.sum(predictions)',np.sum(predictions[0]))\n#print('\\npredictions[:4]\\n',predictions[:4])\n#print('\\ny_test',y_test[:4])\ny_predict =[]\nfor i in range(len(predictions)):\n    predict = np.argmax(predictions[i])\n    y_predict.append(predict)\n#predict = np.argmax(predictions[4])\n#print ('predict\\n',y_predict)","375b5fee":"# This code is for 170- case dataset\n\n#!tar -zcvf outputname.tar.zip \/kaggle\/working","323e8f1a":"#print ('y_test',y_test)\n#print('y_predict',y_predict)","eb4df734":"## predictions = np.array([1 if x >= 0.5 else 0 for x in seed_final_test])\ndef evaluate_matrix(y_test, y_predict, name):\n    cm = confusion_matrix(y_test, y_predict)\n    cm_df = pd.DataFrame(cm, index=[\"Negative\", \"Positive\"], columns=[\"Negative\", \"Positive\"])\n\n    plt.figure(figsize=(10, 10))\n\n    sns.set(font_scale=1)\n\n    ax = sns.heatmap(cm_df, annot=True, square=True, fmt='d', linewidths=.2, cbar=0, cmap=plt.cm.Blues)\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n\n    plt.ylabel(\"True labels\")\n    plt.xlabel(\"Predicted labels\")\n    plt.tight_layout()\n    plt.title(name)\n\n    plt.show()\n\n    print(classification_report(y_test, y_predict, target_names=[\"Negative\", \"Positive\"]))\n    \nevaluate_matrix(y_test, y_predict, 'Original model')","f0d69461":"# summarize score\n#print(predictions[:,1], '\\n',predictions[:,1].shape )\ndef ROC_curve(y_test,predictions,name):\n    \n    # calculate roc curves\n    lr_fpr, lr_tpr, _ = roc_curve(y_test, predictions[:,1])\n    print ('model: {} \\nAUC = {}'. format(name, auc(lr_fpr, lr_tpr)))\n    # plot the roc curve for the model\n    lw = 2\n    plt.plot(lr_fpr, lr_tpr, color=\"darkorange\",\n             lw=lw, label=\"ROC curve (area = %0.2f)\" % auc(lr_fpr, lr_tpr))\n    plt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\n    plt.xlim([-0.02, 1.0])\n    plt.ylim([0.0, 1.05])\n    # axis labels\n    pyplot.xlabel('False Positive Rate')\n    pyplot.ylabel('True Positive Rate')\n    plt.title(name)\n    # show the legend\n    pyplot.legend()\n    # show the plot\n    pyplot.show()\n\nROC_curve(y_test,predictions,'Original data')","d69e153a":"print ('original positive cases: {}  and  total cases: {}'.format(pos, total) )\n# transform the dataset\noversample = SMOTE(sampling_strategy=0.5, k_neighbors=5) #pos is equal to 50% neg\nX_os, y_os = oversample.fit_resample(X_train, y_train)\n\norder = np.arange(len(y_os))\nnp.random.shuffle(order)\nX_os = X_os[order]\ny_os = y_os[order]\n\nneg_os, pos_os = np.bincount(y_os)\ntotal_os = neg_os + pos_os\nprint ('\\nAfter oversampling \\nnegative cases: {}  \\npositive cases: {} ({:.2f}% of total)'.format(neg_os, pos_os, 100 * pos_os\/total_os )) ","51204ae1":"history_os = model.fit(X_os, y_os,\n                    epochs=100,\n                    batch_size=batch_size,\n                    callbacks = my_callbacks,\n                    validation_split=0.15)","d9ab4041":"history_loss_acc(history,'Original Data')\nhistory_loss_acc(history_os,'Oversampling Data')","2615de43":"test_loss_os, test_acc_os = model.evaluate(X_test,y_test) \nprint ('test_acc_os',test_acc_os)\npredictions_os = model.predict(X_test)\ny_predict_os =[]\nfor i in range(len(predictions_os)):\n    predict = np.argmax(predictions_os[i])\n    y_predict_os.append(predict)\n#predict = np.argmax(predictions[4])\n#print ('predict_os\\n',y_predict_os)","6527df65":"## predictions = np.array([1 if x >= 0.5 else 0 for x in seed_final_test])\nevaluate_matrix(y_test, y_predict,'Original data')\nevaluate_matrix(y_test, y_predict_os, 'Oversampling Data')","c9a42d68":"# summarize score\nROC_curve(y_test, predictions,'Original data')\nROC_curve(y_test, predictions_os, 'Oversampling Data')","0fd65bbc":"#print('y_test ',y_test)\n#print('y_predict_origin_data ',y_predict)\n#print('y_predict_os ',y_predict_os)","31643303":"accuracy_list = []\nloss_list = []\n\n# K-Fold CV\nkfold = StratifiedKFold(n_splits=5, shuffle=True)\n# We should use Stratified KFold for binary cassification & huge class imbalance\n\n# K-fold Cross Validation model evaluation\nfold_idx = 1\n\nfor train_ids, val_ids in kfold.split(X_os, y_os):\n    \n    model = get_model()\n\n    print(\"\\nB\u1eaft \u0111\u1ea7u train Fold \", fold_idx)\n\n    # Train model\n    model.fit(X_os[train_ids], y_os[train_ids],\n              batch_size=16,\n              epochs=25,\n              callbacks = my_callbacks,\n              verbose=1)\n\n\n    # Test v\u00e0 in k\u1ebft qu\u1ea3\n    scores = model.evaluate(X_os[val_ids], y_os[val_ids], verbose=0)\n    print(\"\u0110\u00e3 train xong Fold \", fold_idx)\n    print(f'> Fold {fold_idx} - Loss: {scores[0]} - Accuracy: {100* scores[1]}%')\n    \n    # Th\u00eam th\u00f4ng tin accuracy v\u00e0 loss v\u00e0o list\n    accuracy_list.append(scores[1] * 100)\n    loss_list.append(scores[0])\n\n    # To the next fold\n    fold_idx = fold_idx + 1","4d5b080d":"# In k\u1ebft qu\u1ea3 t\u1ed5ng th\u1ec3\nprint('* Chi ti\u1ebft c\u00e1c fold')\nfor i in range(0, len(accuracy_list)):\n    print(f'> Fold {i+1} - Loss: {loss_list[i]} - Accuracy: {accuracy_list[i]}%')\n\nprint('* \u0110\u00e1nh gi\u00e1 t\u1ed5ng th\u1ec3 c\u00e1c folds:')\nprint(f'> Accuracy: {np.mean(accuracy_list)} (\u0110\u1ed9 l\u1ec7ch +- {np.std(accuracy_list)})')\nprint(f'> Loss: {np.mean(loss_list)}')\n","2983b083":"# Train model\nhistory_cv = model.fit(X_os, y_os,\n          batch_size=16,\n          epochs=100,\n          callbacks = my_callbacks,\n          verbose=1)\nmodel.save('.\/kfold.h5')\n\n# load model and predict\nloaded_model = get_model()\nloaded_model.load_weights('.\/kfold.h5')\npredictions_cv = loaded_model.predict(X_test)\ny_predict_cv =[]\nfor i in range(len(predictions_cv)):\n    predict = np.argmax(predictions_cv[i])\n    y_predict_cv.append(predict)\n#predict = np.argmax(predictions[4])\n#print ('predict_cv\\n',y_predict_cv)","a475eade":"## predictions = np.array([1 if x >= 0.5 else 0 for x in seed_final_test])\nevaluate_matrix(y_test, y_predict,'Original data')\nevaluate_matrix(y_test, y_predict_os, 'Oversampling Data')\nevaluate_matrix(y_test, y_predict_cv, 'Oversmap & K_fold')\n# summarize score\nROC_curve(y_test, predictions,'Original data')\nROC_curve(y_test, predictions_os, 'Oversampling Data')\nROC_curve(y_test, predictions_cv, 'Oversamp & K_fold')\n\n#print('y_test ',y_test)\n#print('y_predict_origin_data ',y_predict)\n#print('y_predict_os ',y_predict_os)\n#print('y_predict_cv ',y_predict_cv)","037d0678":"## Model finalization\n> If you\u2019re satisfied with the performance of your model, you can finalize it. There are two options for doing so:\n> \n>* Save the best performing model instance (check \u201cHow to save and load a model with Keras?\u201d \u2013 do note that this requires retraining because you haven\u2019t saved models with the code above), and use it for generating predictions.\n\n\n>* Retrain the model, but this time with all the data \u2013 i.e., without making the train\/test split. Save that model, and use it for generating predictions. I do suggest to continue using a validation set, as you want to know when the model is overfitting.\n\nReference: https:\/\/www.machinecurve.com\/index.php\/2020\/02\/18\/how-to-use-k-fold-cross-validation-with-keras\/#model-finalization\n\nFor me, I prefer the latter","c47e062e":"## ROC_curve","11575514":"# COVID-19 Cough classification notebook","df82ccb5":"## Predictions on Test Data","c9c51bbb":"# Evaluation \n\n## Confusion_matrix\nRecall, precision, F1-score, fpr, tpr\n\nReference: https:\/\/machinelearningcoban.com\/2017\/08\/31\/evaluation\/#-receiver-operating-characteristic-curve","64b5d634":"## Encoding the Labels","c5ee8063":"All the audio files get converted into their respective spectrograms .We can know easily extract features from them.","e571f1e2":"I add some drop out layers to avoid overfitting","655ec3eb":"\n\n# Analysing the Data set","8869a4a8":"# Classification with Keras\n\n## Building our Network","629e61e6":"## Extracting features from Spectrogram\n\n\nWe will extract\n\n* Mel-frequency cepstral coefficients (MFCC)(20 in number)\n* Spectral Centroid,\n* Zero Crossing Rate\n* Chroma Frequencies\n* Spectral Roll-off.","446768d7":"# Combining K fold cross validation\n\nHere I use K-fold (Stratified k fold) with the oversampled data\n\nAfter all, I think K-fold is just a method to generally assessed how good or bad the model is. Help us tune the hyperparameters better\n\nReference:\n* https:\/\/viblo.asia\/p\/lam-chu-stacking-ensemble-learning-Az45b0A6ZxY\n* https:\/\/www.machinecurve.com\/index.php\/2020\/02\/18\/how-to-use-k-fold-cross-validation-with-keras\/\n* https:\/\/github.com\/SadmanSakib93\/Stratified-k-fold-cross-validation-Image-classification-keras\/blob\/master\/stratified_K_fold_CV.ipynb\n* https:\/\/miai.vn\/2021\/01\/18\/k-fold-cross-validation-tuyet-chieu-train-khi-it-du-lieu\/","ea766f30":"**Conclusion about SMOTE based on this project and reference here: https:\/\/www.kaggle.com\/theoviel\/dealing-with-class-imbalance-with-smote**\n\nIt appears that SMOTE does not really help improve the results (sometimes it hepls but sometime it doesn't). Yu should run the model several times. However, it makes the network learning faster.\n\nMoreover, there is one big problem, this method is not compatible larger datasets","92356de7":"I'll try on new 4068-case datase. \n\nLabel 0: negative, label 1: positive","813115e1":"# Comparison on Original data, Oversampling data and K fold","d585ceb1":"# Imbalanced data\n\nAs you can see, our project is imbalanced positive: 669 (16.45% of total), negative cases: 3399\nThere are a alot of proposed methods to solve this like: over sampling, undersampling, gain more data,...\n\nReference: https:\/\/phamdinhkhanh.github.io\/2020\/02\/17\/ImbalancedData.html#45-thu-th%E1%BA%ADp-th%C3%AAm-quan-s%C3%A1t\n\nIn this project. I'll try resolving the imbalanced data by oversampling with SMOTE\n\nReference:\nhttps:\/\/machinelearningmastery.com\/smote-oversampling-for-imbalanced-classification\/","8bca81e6":"## Dividing data into training and Testing set","712361f3":"## Extracting the Spectrogram for every Audio File","9279b778":"## Scaling the Feature columns\n\n> Just to give you an example \u2014 if you have multiple independent variables like age, salary, and height; With their range as (18\u2013100 Years), (25,000\u201375,000 Euros), and (1\u20132 Meters) respectively, feature scaling would help them all to be in the same range, for example- centered around 0 or in the range (0,1) depending on the scaling technique.\n\nReference: https:\/\/www.atoti.io\/when-to-perform-a-feature-scaling\/","bf9a490f":"## Importing Libraries","ea3bbf1d":"## Writing data to csv file\n\nWe write the data to a csv file","37b66e4e":"**I will add more data into this** with positive: 669 (16.45% of total) negative cases: 3399. \n\nThis data concludes 3 raw primary features (no normalization).\n1. Mell\n2. Chroma\n3. MFCC\n"}}