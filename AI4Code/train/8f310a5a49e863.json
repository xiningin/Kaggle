{"cell_type":{"b63cfaf1":"code","0262854c":"code","9fd5a059":"code","9edfb8fe":"code","05bfbfe4":"code","8243ef86":"code","b6510d85":"code","1b74128a":"code","4c8ecaf7":"code","0dd978a1":"code","2e84f6ea":"code","506b93f9":"code","3c389df0":"code","2f7925bc":"code","24fc50e6":"code","12455191":"code","06151771":"code","c54af1c2":"code","b311949b":"code","c251a69b":"code","6f47eeec":"code","a7a4362d":"code","2661fdaa":"markdown"},"source":{"b63cfaf1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0262854c":"#read data\ndata = pd.read_csv(\"..\/input\/sales-analysis\/SalesKaggle3.csv\")\ndata.head()","9fd5a059":"#ubah type data string menjadi categorical\nfrom sklearn.preprocessing import StandardScaler\nsc=StandardScaler()\n\ndata=data.drop([\"SKU_number\",\"Order\",\"SoldCount\",\"ReleaseYear\"],axis=1)\n\nscdata=pd.DataFrame(sc.fit_transform(data.drop([\"File_Type\",\"SoldFlag\",\"MarketingType\",\"New_Release_Flag\"],axis=1)),columns=data.drop([\"File_Type\",\"SoldFlag\",\"MarketingType\",\"New_Release_Flag\"],axis=1).columns)\nscdata[[\"File_Type\",\"SoldFlag\",\"MarketingType\",\"New_Release_Flag\"]]=data[[\"File_Type\",\"SoldFlag\",\"MarketingType\",\"New_Release_Flag\"]]\n\nscdata.head()","9edfb8fe":"data=pd.get_dummies(scdata)\ndata.head()","05bfbfe4":"#cek apakah ada null data\nprint('data_shape:',data.shape)\nprint('jumlah field null:',len(data.isnull().any()))\ndata.isnull().any()","8243ef86":"#Ganti data yang berisi null dengan 0\n#replace 14 null data pada kolom SoldFlag \n# data['SoldFlag'].replace(np.nan, 0, inplace=True) \n# #tampilkan 5 row pertama\n# data['SoldFlag'].head(5)","b6510d85":"#replace 14 null data pada kolom SoldCount\n# data['SoldCount'].replace(np.nan, 0, inplace=True) \n# #tampilkan 5 row pertama\n# data['SoldCount'].head(5)","1b74128a":"#cek dulu multicolinearity data\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ncorrelation = data.corr()\nabs_correlation = abs(correlation)\nplt.figure(figsize=(20,8))\nax = sns.heatmap(abs_correlation,annot=True,fmt=\".2f\")\nplt.show()\n\n# correlation = data.corr()\n# abs_correlation = abs(correlation)\n# plt.figure(figsize=(20,8))\n# mask = np.triu(np.ones_like(correlation, dtype=np.bool))\n# ax = sns.heatmap(abs_correlation,mask=mask,annot=True,fmt=\".2f\")\n# plt.show()","4c8ecaf7":"# fig, ax = plt.subplots(figsize=(8,12))\n\nplt.figure(figsize=(8, 12))\nheatmap = sns.heatmap(data.corr()[['SoldFlag']].sort_values(by='SoldFlag', ascending=False),data, cbar=True, ax=ax,\n                      annot=True, fmt='.2f', annot_kws={'rotation': 90})\nheatmap.set_title('Features Correlating with SoldFlag', fontdict={'fontsize':18}, pad=16);\n\n# heatmap = sns.heatmap(data.corr()[['SoldFlag']].sort_values(by='SoldFlag', ascending=False), cbar=True, ax=ax,\n#                       annot=True, fmt='.2f', annot_kws={'rotation': 90})\n# cbar = heatmap.collections[0].colorbar\n# heatmap.set_xticklabels(heatmap.get_xticklabels(), rotation=90)\n# cbar.ax.set_yticklabels(cbar.ax.get_yticklabels(), rotation=90, va='center')\n# plt.tight_layout()\n# plt.show()","0dd978a1":"#bagi data train dan data test\nfrom sklearn.model_selection import train_test_split\n\n# variables = data[['ReleaseNumber','StrengthFactor','PriceReg','ItemCount','LowUserPrice','LowNetPrice','New_Release_Flag']]\n# results = data['SoldFlag']\n# x = variables\n# y = results\n\ntrain=data[data.SoldFlag.notnull()]\nxtrain_=train.drop([\"SoldFlag\"],axis=1)\nytrain_=train.SoldFlag\n\nx = xtrain_\ny = ytrain_\n\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.30,random_state=2)\nx_train.head()","2e84f6ea":"#logistics regression\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n\nlr=LogisticRegression()\n\ny_pred_logreg=lr.fit(x_train,y_train).predict(x_test)\n\nprint(\"_________________Logistic Regression_________________\")\nprint (classification_report(y_test, y_pred_logreg))\nprint (\"Accuracy: {:.2f} %\".format(accuracy_score(y_test, y_pred_logreg) * 100))\n\nsns.heatmap(confusion_matrix(y_test, y_pred_logreg), annot=True, fmt='.2f')\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actal\")\nplt.show()\n","506b93f9":"#decision tree\nfrom sklearn.tree import DecisionTreeClassifier\n\ndt=DecisionTreeClassifier()\ny_pred_decisiontree=dt.fit(x_train,y_train).predict(x_test)\n\nprint(\"_________________Decision Tree_________________\")\nprint(classification_report(y_test, y_pred_decisiontree))\nprint(\"Accuracy: {:.2f} %\".format(accuracy_score(y_test, y_pred_decisiontree) * 100))\n\nsns.heatmap(confusion_matrix(y_test, y_pred_decisiontree), annot=True, fmt='.2f')\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actal\")\nplt.show()\n","3c389df0":"#knn k-nearest neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn=KNeighborsClassifier()\ny_pred_knn=knn.fit(x_train,y_train).predict(x_test)\n\nprint(\"_________________K-Nearest Neighbors_________________\")\nprint (classification_report(y_test, y_pred_knn))\nprint (\"Accuracy: {:.2f} %\".format(accuracy_score(y_test, y_pred_knn) * 100))\n\nsns.heatmap(confusion_matrix(y_test, y_pred_knn), annot=True, fmt='.2f')\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actal\")\nplt.show()","2f7925bc":"#Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf=RandomForestClassifier()\ny_pred_rf=rf.fit(x_train,y_train).predict(x_test)\n\nprint(\"_________________Random Forest_________________\")\nprint (classification_report(y_test, y_pred_rf))\nprint (\"Accuracy: {:.2f} %\".format(accuracy_score(y_test, y_pred_rf) * 100))\n\nsns.heatmap(confusion_matrix(y_test, y_pred_rf), annot=True, fmt='.2f')\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actal\")\nplt.show()\n","24fc50e6":"#SVM\n# from sklearn import svm\n\n# svm_method=svm.SVC(kernel='linear')\n# y_pred_svm=svm_method.fit(x_train,y_train).predict(x_test)\n\n# print(\"_________________SVM_________________\")\n# print (classification_report(y_test, y_pred_svm))\n# print (\"Accuracy: {:.2f} %\".format(accuracy_score(y_test, y_pred_svm) * 100))\n\n# sns.heatmap(confusion_matrix(y_test, y_pred_svm), annot=True, fmt='.2f')\n# plt.xlabel(\"Predicted\")\n# plt.ylabel(\"Actal\")\n# plt.show()","12455191":"#multi-layer perceptron (MLP)\nfrom sklearn.neural_network import MLPClassifier\n\nmlp = MLPClassifier(hidden_layer_sizes=(10, 10, 10), max_iter=1000)\ny_pred_mlp = mlp.fit(x_train, y_train.values.ravel()).predict(x_test)\n\n# svm_method=svm.SVC(kernel='linear')\n# y_pred_svm=svm_method.fit(x_train,y_train).predict(x_test)\n\nprint(\"_________________MLP_________________\")\nprint (classification_report(y_test, y_pred_mlp))\nprint (\"Accuracy: {:.2f} %\".format(accuracy_score(y_test, y_pred_mlp) * 100))\n\nsns.heatmap(confusion_matrix(y_test, y_pred_mlp), annot=True, fmt='.2f')\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actal\")\nplt.show()","06151771":"#Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import cross_val_score\n\nNB = GaussianNB()\naccuraccies = cross_val_score(estimator = NB, X= x_train, y=y_train, cv=35)\n\nprint(\"Average Accuracies: \",np.mean(accuraccies))\nprint(\"Standart Deviation Accuracies: \",np.std(accuraccies))\ny_pred_nb = NB.fit(x_train,y_train).predict(x_test)\n#prediction\nprint(\"Accuracy of NB Score: \", NB.score(x_test,y_test))\n\nNBscore= NB.score(x_test,y_test)\n\nprint(\"_________________Naive Bayes_________________\")\nprint (classification_report(y_test, y_pred_nb))\nprint (\"Accuracy: {:.2f} %\".format(accuracy_score(y_test, y_pred_nb) * 100))\n\nsns.heatmap(confusion_matrix(y_test, y_pred_nb), annot=True, fmt='.2f')\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actal\")\nplt.show()","c54af1c2":"#tabel accuracy setiap methode\n\nprint (\"Accuracy Logistics Regression: {:.2f} %\".format(accuracy_score(y_test, y_pred_logreg) * 100))\nprint (\"Accuracy Decision Tree: {:.2f} %\".format(accuracy_score(y_test, y_pred_decisiontree) * 100))\nprint (\"Accuracy KNN: {:.2f} %\".format(accuracy_score(y_test, y_pred_knn) * 100))\nprint (\"Accuracy Random Forest: {:.2f} %\".format(accuracy_score(y_test, y_pred_rf) * 100))\n# print (\"Accuracy SVM: {:.2f} %\".format(accuracy_score(y_test, y_pred_svm) * 100))\nprint (\"Accuracy MLP(Neural Network dengan Multi Layer Perceptron): {:.2f} %\".format(accuracy_score(y_test, y_pred_mlp) * 100))\nprint (\"Accuracy Naive Bayes: {:.2f} %\".format(accuracy_score(y_test, y_pred_nb) * 100))","b311949b":"from sklearn.model_selection import cross_validate\n\nscoring = ['accuracy', 'precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\n\n#naive bayes\nscores_nb = cross_validate(NB, x_train, y_train, scoring=scoring, cv=20)\nnb_fit_time = scores_nb['fit_time'].mean()\nnb_score_time = scores_nb['score_time'].mean()\nnb_accuracy = scores_nb['test_accuracy'].mean()\nnb_precision = scores_nb['test_precision_macro'].mean()\nnb_recall = scores_nb['test_recall_macro'].mean()\nnb_f1 = scores_nb['test_f1_weighted'].mean()\nnb_roc = scores_nb['test_roc_auc'].mean()\n\n#decision tree\nscores_dt = cross_validate(dt, x_train, y_train, scoring=scoring, cv=20)\ndt_fit_time = scores_dt['fit_time'].mean()\ndt_score_time = scores_dt['score_time'].mean()\ndt_accuracy = scores_dt['test_accuracy'].mean()\ndt_precision = scores_dt['test_precision_macro'].mean()\ndt_recall = scores_dt['test_recall_macro'].mean()\ndt_f1 = scores_dt['test_f1_weighted'].mean()\ndt_roc = scores_dt['test_roc_auc'].mean()\n\n#knn\nscores_knn = cross_validate(knn, x_train, y_train, scoring=scoring, cv=20)\nknn_fit_time = scores_knn['fit_time'].mean()\nknn_score_time = scores_knn['score_time'].mean()\nknn_accuracy = scores_knn['test_accuracy'].mean()\nknn_precision = scores_knn['test_precision_macro'].mean()\nknn_recall = scores_knn['test_recall_macro'].mean()\nknn_f1 = scores_knn['test_f1_weighted'].mean()\nknn_roc = scores_knn['test_roc_auc'].mean()\n\n#logistic regression\nscores_lr = cross_validate(lr, x_train, y_train, scoring=scoring, cv=20)\nlr_fit_time = scores_lr['fit_time'].mean()\nlr_score_time = scores_lr['score_time'].mean()\nlr_accuracy = scores_lr['test_accuracy'].mean()\nlr_precision = scores_lr['test_precision_macro'].mean()\nlr_recall = scores_lr['test_recall_macro'].mean()\nlr_f1 = scores_lr['test_f1_weighted'].mean()\nlr_roc = scores_lr['test_roc_auc'].mean()\n\n# #randon forest\n# scores_rf = cross_validate(rf, x_train, y_train, scoring=scoring, cv=35)\n# rf_fit_time = scores_rf['fit_time'].mean()\n# rf_score_time = scores_rf['score_time'].mean()\n# rf_accuracy = scores_rf['test_accuracy'].mean()\n# rf_precision = scores_rf['test_precision_macro'].mean()\n# rf_recall = scores_rf['test_recall_macro'].mean()\n# rf_f1 = scores_rf['test_f1_weighted'].mean()\n# rf_roc = scores_rf['test_roc_auc'].mean()\n\n# #multi layer perceptron\n# scores_mlp = cross_validate(mlp, x_train, y_train, scoring=scoring, cv=20)\n# mlp_fit_time = scores_mlp['fit_time'].mean()\n# mlp_score_time = scores_mlp['score_time'].mean()\n# mlp_accuracy = scores_mlp['test_accuracy'].mean()\n# mlp_precision = scores_mlp['test_precision_macro'].mean()\n# mlp_recall = scores_mlp['test_recall_macro'].mean()\n# mlp_f1 = scores_mlp['test_f1_weighted'].mean()\n# mlp_roc = scores_mlp['test_roc_auc'].mean()\n\nprint(nb_accuracy)\nprint(dt_accuracy)\nprint(knn_accuracy)\nprint(lr_accuracy)\n# print(mlp_accuracy)\n\ndata_comparison_method = {\n    'Model'       : ['Naive Bayes', 'Decision Tree', 'K-Nearest Neighbors', 'Logistic Regression'],\n    'Fitting time': [nb_fit_time,dt_fit_time,knn_fit_time,lr_fit_time],\n    'Scoring time': [nb_score_time,dt_score_time,knn_score_time,lr_score_time],\n    'Accuracy': [nb_accuracy,dt_accuracy,knn_accuracy,lr_accuracy],\n    'Precision': [nb_precision,dt_precision,knn_precision,lr_precision],\n    'Recall': [nb_recall,dt_recall,knn_recall,lr_recall],\n    'F1_score': [nb_f1,dt_f1,knn_f1,lr_f1],\n    'AUC_ROC': [nb_roc,dt_roc,knn_roc,lr_roc]\n    }\n\nprint(data_comparison_method)\n\nmodels_tree = pd.DataFrame(data_comparison_method, columns = ['Model', 'Fitting time', 'Scoring time', 'Accuracy', 'Precision', 'Recall', 'F1_score', 'AUC_ROC'])\n\nmodels_tree.sort_values(by='Accuracy', ascending=False)","c251a69b":"#randon forest\n# scores_rf = cross_validate(rf, x_train, y_train, scoring=scoring, cv=20)\n# rf_fit_time = scores_rf['fit_time'].mean()\n# rf_score_time = scores_rf['score_time'].mean()\n# rf_accuracy = scores_rf['test_accuracy'].mean()\n# rf_precision = scores_rf['test_precision_macro'].mean()\n# rf_recall = scores_rf['test_recall_macro'].mean()\n# rf_f1 = scores_rf['test_f1_weighted'].mean()\n# rf_roc = scores_rf['test_roc_auc'].mean()\n\n# data_comparison_method_rf = {\n#     'Model'       : ['Random Forest'],\n#     'Fitting time': [rf_fit_time],\n#     'Scoring time': [rf_score_time],\n#     'Accuracy': [rf_accuracy],\n#     'Precision': [rf_precision],\n#     'Recall': [rf_recall],\n#     'F1_score': [rf_f1],\n#     'AUC_ROC': [rf_roc]\n#     }\n\n# data_comparison_method_rf = {\n#     'Model'       : ['Random Forest'],\n#     'Fitting time': [1],\n#     'Scoring time': [1],\n#     'Accuracy': [1],\n#     'Precision': [1],\n#     'Recall': [1],\n#     'F1_score': [1],\n#     'AUC_ROC': [1]}\n\n# print(data_comparison_method_rf)\n\n# models_tree_rt = pd.DataFrame(data_comparison_method_rf, columns = ['Model', 'Fitting time', 'Scoring time', 'Accuracy', 'Precision', 'Recall', 'F1_score', 'AUC_ROC'])\n# models_tree = pd.concat([models_tree_rt, models_tree])\nmodels_tree.sort_values(by='Accuracy', ascending=False)","6f47eeec":"from sklearn.model_selection import cross_validate\n\nscoring = ['accuracy', 'precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\n\n#naive bayes\nscores_nb = cross_validate(NB, x_train, y_train, scoring=scoring, cv=20)\nnb_fit_time = scores_nb['fit_time'].mean()\nnb_score_time = scores_nb['score_time'].mean()\nnb_accuracy = scores_nb['test_accuracy'].mean()\nnb_precision = scores_nb['test_precision_macro'].mean()\nnb_recall = scores_nb['test_recall_macro'].mean()\nnb_f1 = scores_nb['test_f1_weighted'].mean()\nnb_roc = scores_nb['test_roc_auc'].mean()\n\n#decision tree\nscores_dt = cross_validate(dt, x_train, y_train, scoring=scoring, cv=20)\ndt_fit_time = scores_dt['fit_time'].mean()\ndt_score_time = scores_dt['score_time'].mean()\ndt_accuracy = scores_dt['test_accuracy'].mean()\ndt_precision = scores_dt['test_precision_macro'].mean()\ndt_recall = scores_dt['test_recall_macro'].mean()\ndt_f1 = scores_dt['test_f1_weighted'].mean()\ndt_roc = scores_dt['test_roc_auc'].mean()\n\n#knn\nscores_knn = cross_validate(knn, x_train, y_train, scoring=scoring, cv=20)\nknn_fit_time = scores_knn['fit_time'].mean()\nknn_score_time = scores_knn['score_time'].mean()\nknn_accuracy = scores_knn['test_accuracy'].mean()\nknn_precision = scores_knn['test_precision_macro'].mean()\nknn_recall = scores_knn['test_recall_macro'].mean()\nknn_f1 = scores_knn['test_f1_weighted'].mean()\nknn_roc = scores_knn['test_roc_auc'].mean()\n\n#logistic regression\nscores_lr = cross_validate(lr, x_train, y_train, scoring=scoring, cv=20)\nlr_fit_time = scores_lr['fit_time'].mean()\nlr_score_time = scores_lr['score_time'].mean()\nlr_accuracy = scores_lr['test_accuracy'].mean()\nlr_precision = scores_lr['test_precision_macro'].mean()\nlr_recall = scores_lr['test_recall_macro'].mean()\nlr_f1 = scores_lr['test_f1_weighted'].mean()\nlr_roc = scores_lr['test_roc_auc'].mean()\n\n#randon forest\nscores_rf = cross_validate(rf, x_train, y_train, scoring=scoring, cv=20)\nrf_fit_time = scores_rf['fit_time'].mean()\nrf_score_time = scores_rf['score_time'].mean()\nrf_accuracy = scores_rf['test_accuracy'].mean()\nrf_precision = scores_rf['test_precision_macro'].mean()\nrf_recall = scores_rf['test_recall_macro'].mean()\nrf_f1 = scores_rf['test_f1_weighted'].mean()\nrf_roc = scores_rf['test_roc_auc'].mean()\n\n#multi layer perceptron\nscores_mlp = cross_validate(mlp, x_train, y_train, scoring=scoring, cv=20)\nmlp_fit_time = scores_mlp['fit_time'].mean()\nmlp_score_time = scores_mlp['score_time'].mean()\nmlp_accuracy = scores_mlp['test_accuracy'].mean()\nmlp_precision = scores_mlp['test_precision_macro'].mean()\nmlp_recall = scores_mlp['test_recall_macro'].mean()\nmlp_f1 = scores_mlp['test_f1_weighted'].mean()\nmlp_roc = scores_mlp['test_roc_auc'].mean()\n\ndata_comparison_method = {\n    'Model'       : ['Naive Bayes', 'Decision Tree', 'K-Nearest Neighbors', 'Logistic Regression', 'Random Forest', 'Multi Layer Perceptron'],\n    'Fitting time': [nb_fit_time,dt_fit_time,knn_fit_time,lr_fit_time,rf_fit_time,mlp_fit_time],\n    'Scoring time': [nb_score_time,dt_score_time,knn_score_time,lr_score_time,rf_score_time,mlp_score_time],\n    'Accuracy': [nb_accuracy,dt_accuracy,knn_accuracy,lr_accuracy,rf_accuracy,mlp_accuracy],\n    'Precision': [nb_precision,dt_precision,knn_precision,lr_precision,rf_precision,mlp_precision],\n    'Recall': [nb_recall,dt_recall,knn_recall,lr_recall,rf_recall,mlp_recall],\n    'F1_score': [nb_f1,dt_f1,knn_f1,lr_f1,rf_f1,mlp_f1],\n    'AUC_ROC': [nb_roc,dt_roc,knn_roc,lr_roc,rf_roc,mlp_roc]\n    }\n\nprint(data_comparison_method)\n\nmodels_tree = pd.DataFrame(data_comparison_method, columns = ['Model', 'Fitting time', 'Scoring time', 'Accuracy', 'Precision', 'Recall', 'F1_score', 'AUC_ROC'])\n\nmodels_tree.sort_values(by='Accuracy', ascending=False)","a7a4362d":"from plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\n\naccuracy = [round(nb_accuracy,4),round(dt_accuracy,4),round(knn_accuracy,4),round(lr_accuracy,4),round(rf_accuracy,4),round(mlp_accuracy,4)]\nprecision = [round(nb_precision,4),round(dt_precision,4),round(knn_precision,4),round(lr_precision,4),round(rf_precision,4),round(mlp_precision,4)]\nrecall = [round(nb_recall,4),round(dt_recall,4),round(knn_recall,4),round(lr_recall,4),round(rf_recall,4),round(mlp_recall,4)]\nalgorithm = ['Naive Bayes', 'Decision Tree', 'K-Nearest Neighbors', 'Logistic Regression', 'Random Forest', 'Multi Layer Perceptron']\n\ntrace_accuracy = go.Scatter(\n    x = algorithm,\n    y= accuracy,\n    name='Accuracy',\n    marker =dict(color='#05ffa1',line =dict(color='#089f5e',width=2)),text=accuracy,textposition='top right',textfont=dict(color='#E58606'),mode='lines+markers+text')\n\ntrace_precision = go.Scatter(\n    x = algorithm,\n    y= precision,\n    name='Precision',\n    marker =dict(color='#01cdfe',line =dict(color='#0360b6',width=2)),text=precision,textposition='top right',textfont=dict(color='#E58606'),mode='lines+markers+text')\n\ntrace_recall = go.Scatter(\n    x = algorithm,\n    y= recall,\n    name='Recall',\n    marker =dict(color='#b967ff',line =dict(color='#8303b0',width=2)),text=recall,textposition='top right',textfont=dict(color='#E58606'),mode='lines+markers+text')\n\ndata = [trace_accuracy,trace_precision, trace_recall]\n\nlayout = go.Layout(barmode = \"group\",\n                  xaxis= dict(title= 'Machine Learning Algorithms',ticklen= 5,zeroline= False),\n              yaxis= dict(title= 'Scores',ticklen= 5,zeroline= False))\n\nfig = go.Figure(data = data, layout = layout)\n    \niplot(fig)","2661fdaa":"Tujuan utama: menentukan model terbaik untuk mengklasifikasikan barang"}}