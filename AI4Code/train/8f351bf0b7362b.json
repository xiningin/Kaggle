{"cell_type":{"fbb34f52":"code","d694f39b":"code","1d2ae0ae":"code","b576cef5":"code","310d4f23":"code","418e5208":"code","f4dc0d17":"code","235c7647":"code","9f96d69c":"code","2f5f8464":"code","88e09214":"code","f810bbdd":"code","f708b3b1":"code","f9a80857":"code","ae3aa783":"code","cafda05d":"code","c648c257":"code","e7ab4a90":"code","0b7325e6":"code","c49c0c04":"code","31443852":"code","0f8252c6":"code","3abe06c9":"code","8c08ab80":"markdown","7a269656":"markdown","0f17e10c":"markdown","c750a2a7":"markdown","a4c14ca8":"markdown","10799792":"markdown","75042b94":"markdown","2dc25867":"markdown","a130526c":"markdown","7069d4e9":"markdown","21da28d4":"markdown","7685ef8f":"markdown","9d21b56f":"markdown","5134c094":"markdown","8fcf1612":"markdown","0d0ceb67":"markdown","e2a455d4":"markdown","cc077341":"markdown","fcfeda31":"markdown","5541fad9":"markdown","b7a1b2be":"markdown","33d65565":"markdown","051202ce":"markdown","05fdc5d7":"markdown","34b59d2a":"markdown"},"source":{"fbb34f52":"#import libraries for data preprocessing\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport random as rnd\nimport matplotlib.pyplot as plt\n#import machine learning models ,no need to understand the different classifiers now \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n","d694f39b":"#load data \ntrain_df=pd.read_csv('..\/input\/train.csv')\ntest_df=pd.read_csv('..\/input\/test.csv')\n#create a pipeline for sumltaneous processing \ncombine=[train_df,test_df]\nprint(combine[0].shape)\nprint(combine[1].shape)","1d2ae0ae":"#showing columns in the dataset \nprint(train_df.columns.values)","b576cef5":"train_df","310d4f23":"train_df.info()\ntest_df.info()","418e5208":"train_df.describe()","f4dc0d17":"train_df.head()","235c7647":"train_df[['Pclass','Survived']].groupby(['Pclass'],as_index=False).mean().sort_values(by='Survived',ascending=False)","9f96d69c":"train_df[['SibSp','Survived']].groupby(['SibSp'],as_index=False).mean().sort_values(by='Survived',ascending=False)","2f5f8464":"train_df[['Parch','Survived']].groupby(['Parch'],as_index=False).mean().sort_values(by='Survived',ascending=False)","88e09214":"train_df[['Embarked','Survived']].groupby(['Embarked'],as_index=False).mean().sort_values(by='Survived',ascending=False)","f810bbdd":"new_features=train_df\nnew_features['FamilySize']=train_df['Parch']+train_df['SibSp']+1\nnew_features.FamilySize.nunique()\n","f708b3b1":"new_features[['FamilySize','Survived']].groupby(['FamilySize'],as_index=False).mean().sort_values(by='Survived',ascending=False)","f9a80857":"train_df[['Sex','Survived']].groupby(['Sex'],as_index=False).mean().sort_values(by='Survived',ascending=False)","ae3aa783":"g=sns.FacetGrid(train_df,col='Survived')\ng.map(plt.hist, 'Age', bins=20)","cafda05d":"new_features=train_df\nnew_features.Fare=new_features.Fare.astype(int)\ng=sns.FacetGrid(new_features,col='Survived')\ng.map(plt.hist, 'Fare', bins=20)","c648c257":"#Plotting age with survival\n#grid = sns.FacetGrid(train_df, col='Survived', row='Pclass', size=2.2, aspect=1.6)\ngrid=sns.FacetGrid(train_df,col='Survived')\ngrid.map(sns.distplot,'Age',kde=False,bins=20)","e7ab4a90":"train_df.head()","0b7325e6":"#plotting pclass with survival variations\nsns.catplot(x='Pclass',y='Survived',kind='bar',data=train_df,)\n","c49c0c04":"sns.catplot(x='Pclass',y='Fare',kind='bar',data=train_df,)\n","31443852":"sns.catplot(x='Pclass',y='Survived',kind='bar',hue='Sex',data=train_df)","0f8252c6":"sns.catplot(x='Pclass',hue='Sex',kind='count',data=train_df)","3abe06c9":"grid=sns.FacetGrid(train_df,col='Pclass',row='Sex')\ngrid.map(sns.distplot,'Age',kde=False,bins=20)","8c08ab80":"The following is the explanation of various features in the question \n- sibsp\t# of siblings \/ spouses aboard the Titanic\t\n- parch\t# of parents \/ children aboard the Titanic\t\n- ticket\tTicket number\t\n- fare\tPassenger fare\t\n- cabin\tCabin number\t\n- embarked\tPort of Embarkation\n\nTo understand the correlation between different features , lets identify the features whose correlation needs to be calculated w.r.t Survived\n- name cannot be plotted as it is string, we might do later by encoding names with numbers\n- pclass-okay \n- sex-no , it is an string, maybe one hot encoding is required to get the mean values\n- age-no, as it is coninuos stream of values\n- SibSp-okay\n- Parch-Okay\n- Ticket-no as it is string type\n- Fare- no it has continous values\n- Cabin-no, it is of the form string for the values\n- Embarked- 3 possible values, okay to analyze\n\nBasically the concept is as follows. Suppose we have a target label i.e. survived and a features which can be grouped into logical categories. \nFor them we can use pandas groupby function to create appropriate groups, and take mean for each label value to get the correlation between feature and the \ntarget label. So we get:\n\n\n","7a269656":"Next comes the Fare feature which is continous. But it has a seeming fractional part. Lets separate out the integer part and plot the histogram to see the possibility of binning.","0f17e10c":"This means that a larger number of females survived.(as expected, both children and females had a better chance of survival)","c750a2a7":"Yes, so pclass1 is expensive and higher people have survived there. Next, lets try to understand the genderwise survivability in each class.","a4c14ca8":"This offers a brilliant insight\n- the names features are absolutely unique, We cannot derive a model from this features as here is no similarity. Perhaps we can extract the Mr\/Mrs field out of this, along with the first and last name and generate some features out of them .\n- Parch shows the number of parents\/childre. Max value of Parch was 6.The maximum values in the cabin features are 4. Perhaps, the passengers travelled in families anf prefererred to stay in groups of 4 each in various cabins(with the parents living in other cabins).\n- a large no of passengers embarked from the terminal S (644). Could we explore whther the type of ticket purchased had any correlation with the point of embarking? Furthermore, does survivability have any relation with the point of embarkation ?\n","10799792":"3. Numeric-Age,Parch,SibSP,Fares (However the passengerid is not included, because they refer to individual training exmaples)","75042b94":"Higher number of females seem to have survived in each class.Lets check the actual count of people in each class accordinf to gender ","2dc25867":"So we have obtained the following conclusions:\n- Pclass is storngly correlated with survival, use that in model\n- FamilySize feature is better , and would allow us to drop prach and sibsp features\n- Embarked feature also holds a strong correlation with survivability, but it is categorical variable. We need some mechanism to replace its values by some integers\n- Age is the continous feature. which will give a better performance if we 'bin' the values in a hostogram. The optimal value of the bin size needs to be explored. People of younger age had a better chance of survival.\n- Fare was a continous feature. We extracted integer part out of it and saw a direct corelation with survival.But  more number of people are there with a particular lower number of fares, which raises the number of training examples in the model. This skews the results in the favour of lower features. Therefore we can do FARE NORMALIZATION. Lets append the percentage of people who survived for each fare class, to each training example. Hopefully , this will make the model independent of number of training examples for each fare.\n- Sex feature is useful to understand that the higher number of females survived.","a130526c":"consider fare 0 , more persons died who purchased lower fare ticket (300 vs 150 for survival)\nconsider fare 500 , 0 person died for fare 500 , and everyone survived. \nSeeing these two extrema, there is a logical trend that seems to be followd, therefore 'fare' feature should be used in the model.","7069d4e9":"The new feature generated from parch and sibsp has unique values of 9. Treating each value , as a category let's check if the correlation with the survived feature is non zero for all categories or not.","21da28d4":"There is the concept of numric features, which are just numbers. If you can subtract two consecutive numeric features and get the same difference, then they are numeric. For eg 2-1=1 and 3-2=1 , otherwise they are gonna be categorical (since some biasings are there).Lets list some numeric features:","7685ef8f":"** list of the features which contain empty values**\n- cabin\n- embarked \n- age \n- fare\n\n** A question to ponder**\n* If we handle the missing values prior to fitting the model , will it impact the models performance? Lets have a look later on in this tutorial ","9d21b56f":"**Cross Correlating the features**\n\nTill now we  have covered visualizations of the a single feature with survivability(ie the target variable). To understand the correlation between mutiple features with each other, lets use the plots which plot multiple features together. Lets identify the features we might want to plot for relation amongst themselves.(by separating into numerical\/categorical\/features which dont need to be plotted)\n\n***Features which cannot be\/need not be plotted***\n- PassengerID\n- Name\n- Ticket\n\n***Numeric Features***\n- Survived\n- Age\n- Sibsp\n- Parch \n\n***Categorical Features***\n- Pclass\n- Sex\n- Embarked\n\n\n","5134c094":"now comes the time for feature identification.\nVarious types of features are as follows:\n- Categorical- belong to a particular class of categories\n- Ordinal- are categorical, but in the sense that as we move across different categories, the importance changes. For eg the various classes of tickets\n\nNow lets identify which features belong to which ","8fcf1612":"The sibsp features and parch features are zeroes for certain values. IE. there is no correlation between them and 'survived' variable.We can use features with zero correlations to safely generate new features without impacting the models performance negatively.So lets do that , by first checking on a test array ","0d0ceb67":"We want to understand corresponding to each pclass what is the age distribution of the peeople who survived.","e2a455d4":"As expected the probablity of survivability of infants is higher than old people. We will bin ages into infant, adults and old people in the feature engineering process","cc077341":"Lets try to build understanding from this date\n- Survived was a binary feature with values 0\/1 which is what we want to predict\n- PClass was a dicrete variable with 3 values. The 25th percentile lies on value 2, and 3 sustains across 50 and 75th percentile. \nIt might be safe to presume that 1 value of Pclass corresponds to smaller no of people, perhaps some importance is associated with this class and might lead to better survivability. \nLets explore this later on \n- Age throttles at 38 at 75th percentile , while the max is 82. Hence the no oof senior citizes in less","fcfeda31":"Our analysis helps us understand \n- Pclass is storngly correlated with survival, use that in model\n- FamilySize feature is better , and would allow us to drop prach and sibsp features\n- Embarked feature also holds a strong correlation with survivability, but it is categorical variable. We need some mechanism to replace its values by some integers","5541fad9":"We can see that the Pclass,Sex,Embarked,Survived features belong to categories. lets segregate them \n\n1. Categorical-Sex,Embarked,Survived (there is no bias among individual categories)\n2. Ordinal-Pclass (Better class might have better chance of survival, this is biased so an ordinal feature)\n> Note:if you dont understand the relationship between ordinal and survived features for now, dont worry we will cover them through heat correlationmaps later on","b7a1b2be":"Thus it seems that for Pclass 1 ,higher number of people have survived.Lets now check whether Pclass bears any correlatiion to the fares.","33d65565":"Next we have to capture the distribution of these features in the dataset. Lets take the look at numberic features first ","051202ce":"Next, we need to analyze those numberic features which are in continuos range, and cannot be classified into categories. The best way to do that is through histograms.\nThe visualizations might help us to identify the regions of concentrations and replace the actual age values so that they belong to a particular bin (so that the sensitivity of the model might be reduced towards ages ie, age 19 and 20 dont make tht much difference, they both can be replaced by a value meaning \"children\")\n","05fdc5d7":"In Pclass3, a very large number of men died inspite of huge population. While there was a general trend in preserving females across all the pclass levels, but the dip in pclass3 is the largest. Lets explore possible reasons behind this.\n\nlets plot the distribution of age with survivability across differnet pclass and gender.","34b59d2a":"Next, we have to figure out whether any values are missing in the dataset"}}