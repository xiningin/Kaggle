{"cell_type":{"9a667470":"code","2c022877":"code","0c0635ad":"code","42caa1ed":"code","8ffc71ba":"code","637aff3f":"code","6f715d44":"code","6f01e024":"code","328b16f8":"code","04dffe01":"code","aad11db1":"code","f84eeaef":"code","c82a5c21":"code","395f2f61":"code","592ca15d":"code","8b3b2c39":"code","6de04c05":"code","30a2abf6":"code","10dc2b0d":"code","94e1fa0a":"code","b3bc8add":"code","431344bc":"code","e5f0d804":"code","e190dc98":"code","b8da34f5":"code","f5d1e1be":"code","60b9469a":"code","9d86b91a":"code","ba371df0":"code","11a9fb54":"code","c577ff2b":"code","ac4a73a4":"code","db4ba3ce":"code","868aaa36":"markdown","fdc19391":"markdown","f92b4058":"markdown","6d37bf00":"markdown","68530e48":"markdown","21af2b65":"markdown","b757d6b1":"markdown","ee850fad":"markdown","cc1182a9":"markdown"},"source":{"9a667470":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom nltk.corpus import stopwords\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2c022877":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix\nfrom sklearn.preprocessing  import LabelEncoder\nle = LabelEncoder()\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport missingno as miss\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nfrom plotly.offline import iplot\nimport cufflinks as cf\ncf.go_offline()","0c0635ad":"df = pd.read_csv(\"\/kaggle\/input\/sentiment-analysis-for-financial-news\/all-data.csv\", encoding='latin-1', header = None)\ndf.columns = [\"Sentiment\", \"News Headline\"]\ndf.head()","42caa1ed":"df.shape","8ffc71ba":"df.info()","637aff3f":"df.describe(include = \"all\")","6f715d44":"df.isna().sum()","6f01e024":"miss.bar(df)\nplt.show()","328b16f8":"#check for duplicates\n\nlen(df[df.duplicated()])","04dffe01":"df = df.drop_duplicates()\nprint(df.head())\nprint(df.shape)","aad11db1":"df['nr_of_char'] = df['News Headline'].str.len()\ndf['nr_of_char'] = df['nr_of_char'] \/ df['nr_of_char'].max()\ndf[['Sentiment', 'nr_of_char']].pivot(columns = 'Sentiment', values = 'nr_of_char').iplot(kind = 'box')","f84eeaef":"df['nr_of_words'] = df['News Headline'].str.split().str.len()\ndf['nr_of_words'] = df['nr_of_words'] \/ df['nr_of_char'].max()\ndf[['Sentiment', 'nr_of_words']].pivot(columns = 'Sentiment', values = 'nr_of_words').iplot(kind = 'box')","c82a5c21":"df['nr_of_unique_words'] = df['News Headline'].apply(lambda x: len(set(x.split())))\ndf['nr_of_unique_words'] = df['nr_of_unique_words'] \/ df['nr_of_unique_words'].max()\ndf[['Sentiment', 'nr_of_unique_words']].pivot(columns = 'Sentiment', values = 'nr_of_unique_words').iplot(kind='box')","395f2f61":"df['nr_of_punctuation'] = df['News Headline'].str.split(r\"\\?|,|\\.|\\!|\\\"|'\").str.len()\ndf['nr_of_punctuation'] = df['nr_of_punctuation'] \/ df['nr_of_punctuation'].max()\ndf[['Sentiment', 'nr_of_punctuation']].pivot(columns = 'Sentiment', values = 'nr_of_punctuation').iplot(kind = 'box')","592ca15d":"stop_words = set(stopwords.words('english'))\ndf['nr_of_stopwords'] = df['News Headline'].str.split().apply(lambda x: len(set(x) & stop_words))\ndf['nr_of_stopwords'] = df['nr_of_stopwords'] \/ df['nr_of_stopwords'].max()\ndf[['Sentiment', 'nr_of_stopwords']].pivot(columns = 'Sentiment', values = 'nr_of_stopwords').iplot(kind = 'box')","8b3b2c39":"df.corr().iplot(kind='heatmap',colorscale=\"YlGnBu\",title=\"Feature Correlation Matrix\")","6de04c05":"df.insert(0, 'Id', range(1, 1 + len(df))) #defining custom Id column\ndef show_donut_plot(col): #donut plot function\n    \n    rating_data = df.groupby(col)[['Id']].count().head(10)\n    plt.figure(figsize = (12, 8))\n    plt.pie(rating_data[['Id']], autopct = '%1.0f%%', startangle = 140, pctdistance = 1.1, shadow = True)\n\n    # create a center circle for more aesthetics to make it better\n    gap = plt.Circle((0, 0), 0.5, fc = 'white')\n    fig = plt.gcf()\n    fig.gca().add_artist(gap)\n    \n    plt.axis('equal')\n    \n    cols = []\n    for index, row in rating_data.iterrows():\n        cols.append(index)\n    plt.legend(cols)\n    \n    plt.title('Donut Plot: Reviews \\n', loc='center')\n    plt.show()","30a2abf6":"show_donut_plot('Sentiment')","10dc2b0d":"import re\nimport spacy\nnlp = spacy.load('en')\n\ndef normalize(msg):\n    \n    msg = re.sub('[^A-Za-z]+', ' ', msg) #remove special character and intergers\n    doc = nlp(msg)\n    res=[]\n    for token in doc:\n        if(token.is_stop or token.is_punct or token.is_currency or token.is_space or len(token.text) <= 2): #word filteration\n            pass\n        else:\n            res.append(token.lemma_.lower())\n    return res","94e1fa0a":"df[\"News Headline\"] = df[\"News Headline\"].apply(normalize)","b3bc8add":"df.head()","431344bc":"words_collection = Counter([item for sublist in df['News Headline'] for item in sublist])\nfreq_word_df = pd.DataFrame(words_collection.most_common(20))\nfreq_word_df.columns = ['frequently_used_word','count']","e5f0d804":"fig = px.scatter(freq_word_df, x=\"frequently_used_word\", y=\"count\", color=\"count\", title = 'Frequently used words - Scatter plot')\nfig.show()","e190dc98":"df[\"News Headline\"] = df[\"News Headline\"].apply(lambda x : \" \".join(x))\ndf = df[[\"News Headline\", \"Sentiment\"]]\ndf[\"Sentiment\"] = le.fit_transform(df[\"Sentiment\"])\ndf.head()","b8da34f5":"!pip install transformers\n!pip install simpletransformers","f5d1e1be":"rename = {\"News Headline\": \"text\", \"Sentiment\": \"labels\"}\ndf.rename(columns = rename, inplace=True)","60b9469a":"train_x_y = df.sample(frac = 0.75, random_state = 42)\ntest_x_y = pd.concat([df, train_x_y]).drop_duplicates(keep=False)","9d86b91a":"print(train_x_y.shape)\nprint(test_x_y.shape)","ba371df0":"from simpletransformers.classification import ClassificationModel, ClassificationArgs\n\nmodel_args = ClassificationArgs()\nmodel_args.train_batch_size = 2\nmodel_args.gradient_accumulation_steps = 8\nmodel_args.learning_rate = 3e-5\nmodel_args.num_train_epochs = 1\n\nmodel_bert = ClassificationModel(\"bert\", \"bert-base-uncased\", num_labels=3, args=model_args, use_cuda=False)","11a9fb54":"model_bert.train_model(train_x_y)","c577ff2b":"pred_bert, out_bert = model_bert.predict(test_x_y['text'].values)\n\nacc_bert = accuracy_score(test_x_y['labels'].to_numpy(), pred_bert)\nf1_bert = f1_score(test_x_y['labels'].to_numpy(), pred_bert, average='micro')\n\nprint(\"Accuracy Score -\",acc_bert)\nprint(\"F1 Score - \", f1_bert)","ac4a73a4":"cm = confusion_matrix(pred_bert, test_x_y['labels'].to_numpy())\n\ngroup_counts = [\"{0:0.0f}\".format(value) for value in cm.flatten()]\ngroup_percentages = [\"{0:.2%}\".format(value) for value in cm.flatten()\/np.sum(cm)]\nlabels = [f\"{v1}\\n{v2}\" for v1, v2 in zip(group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(3,3)\nsns.heatmap(cm, annot=labels, fmt=\"\", cmap='Blues')\nplt.show()","db4ba3ce":"fig,ax=plt.subplots(figsize=(10,5))\nsns.regplot(x=pred_bert, y=test_x_y['labels'].to_numpy(),marker=\"*\")\nplt.show()","868aaa36":"## Cleaning the text column","fdc19391":"## Number of Characters","f92b4058":"## Feature correlation matrix","6d37bf00":"## Number of punctuation marks","68530e48":"## Number of words","21af2b65":"## Number of unique words","b757d6b1":"## Let's start with BERT","ee850fad":"## Number of stopwords","cc1182a9":"## Target Distribution"}}