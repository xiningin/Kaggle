{"cell_type":{"031ca7b8":"code","9ee12c26":"code","ba79960e":"code","8b6e0768":"code","f371e332":"code","0ad4978a":"code","1c9b3c7e":"code","6a31d083":"code","15edafd2":"code","11fe6d17":"code","cf1bb91f":"code","d9fd9130":"code","94145a9c":"code","3235ec91":"code","3568934f":"code","38e2c6ba":"code","5efd330b":"code","57077afa":"code","b8d136c5":"code","2da27625":"code","5328bc99":"code","45ee1d07":"markdown","b2bf2470":"markdown","7d1f006e":"markdown","8de9ac5a":"markdown","15697ff0":"markdown","be4e4602":"markdown"},"source":{"031ca7b8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9ee12c26":"import pandas as pd\nimport tensorflow as tf\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n%matplotlib inline","ba79960e":"train = pd.read_csv('..\/input\/rn-2019-diabetes\/diabetes_train.csv')\ntest = pd.read_csv('..\/input\/rn-2019-diabetes\/diabetes_test.csv')\n\nprint(f'Training shape: {train.shape}')\nprint(f'Testing shape: {test.shape}')","8b6e0768":"train.head()","f371e332":"train.tail()","0ad4978a":"test.head()","1c9b3c7e":"# Names of columns\nfor col in train.columns:\n    print(col)","6a31d083":"train.describe()","15edafd2":"# Making copy of train and dropping the column 'id'\ntrain_copy = train.copy(deep=True)\ntrain.drop('id',inplace=True,axis=1)","11fe6d17":"#Computing the correlation of attributes\ncorrelation = train.corr()\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(7,5))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(correlation, vmax=.3, center=0,cmap=cmap,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","cf1bb91f":"y = train['Outcome']\ntrain.drop('Outcome',inplace=True,axis=1)","d9fd9130":"xtrain, xval, ytrain, yval = train_test_split(train,y,random_state=1,train_size=0.7)\nprint(f'xtrain :{xtrain.shape}')\nprint(f'ytrain :{ytrain.shape}')\nprint(f'xval :{xval.shape}')\nprint(f'yval :{yval.shape}')","94145a9c":"from tensorflow.keras.layers import Dense,Dropout\n\nmodel = tf.keras.models.Sequential([\n    Dense(64,input_shape=[8],activation='relu'),\n    Dense(128,activation='relu'),\n    Dropout(0.3),\n    Dense(1,activation='sigmoid')\n])\n\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n\nmodel.summary()","3235ec91":"history = model.fit(xtrain,ytrain,epochs=1000,validation_data=(xval,yval),verbose=0)","3568934f":"f, axes = plt.subplots(2,1,figsize=(15,10))\nepochs = [i for i in range(len(history.history['accuracy']))]\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nsns.lineplot(x=epochs,y=acc,ax=axes[0])\nsns.lineplot(x=epochs,y=val_acc,ax=axes[1])","38e2c6ba":"ids = test['id']\ntest.drop('id',inplace=True,axis=1)","5efd330b":"preds = model.predict_classes(xtest)","57077afa":"preds = np.squeeze(preds)","b8d136c5":"submission_data = list(zip(ids,preds))","2da27625":"sub = pd.DataFrame(submission_data,columns=['id','Outcome'])","5328bc99":"sub.to_csv('diabetes_submission_1.csv')","45ee1d07":"### 2. Reading the data ","b2bf2470":"### 4. Splitting the Dataset ","7d1f006e":"### 5. Model Building ","8de9ac5a":"### 1. Importing the necessary dependancies","15697ff0":"### 3. EDA","be4e4602":"### 6. Prediction and submission file creation"}}