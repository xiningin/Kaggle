{"cell_type":{"d6d24e90":"code","7b26c760":"code","273f08a2":"code","bee1414b":"code","5f16ab31":"code","70095df8":"code","ed4c5b77":"code","380b2b04":"code","7e2f2f9d":"code","f3b7a680":"code","4871a318":"code","7e483b44":"code","59a0abeb":"code","b4c77ed6":"code","1b2a1076":"code","96ad0378":"code","7e5f28db":"code","543cb38a":"code","9e3d73d8":"code","b471e70f":"code","2a695c41":"code","6d154a5f":"code","d2182afe":"code","51f2e3fb":"code","0c4538d0":"code","d966c62a":"code","88027885":"code","515fb9e1":"code","8a569247":"code","08016b21":"markdown","431a11db":"markdown","ed793ea0":"markdown","3974adb5":"markdown","0c9f0d0d":"markdown","1a1a5bfa":"markdown","57afbc70":"markdown","ebf2c8fe":"markdown","d5161d10":"markdown","518d3b6c":"markdown","30bd969e":"markdown","27bdc94e":"markdown"},"source":{"d6d24e90":"# Datasets\n# \u4e8b\u524d\u306b\u4e0b\u8a18\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u30a2\u30c3\u30d7\u30ed\u30fc\u30c9\u3057\u3066\u304a\u304f\n#   faissgpu17\n#   pretrained-pytorch-models","7b26c760":"# Common","273f08a2":"# Images\nimport json\nfrom pathlib import Path\nimport csv\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom collections import defaultdict\nfrom tqdm import tqdm\nfrom matplotlib.pyplot import imshow\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport torch\nimport torchvision\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nfrom torchvision.datasets.utils import download_url\n\n!pip install --no-index --find-links ..\/input\/faissgpu17 -r ..\/input\/faissgpu17\/requirements.txt\nimport faiss","bee1414b":"# Texts\nimport time\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nfrom collections import defaultdict\nfrom PIL import Image\n\n!pip install --no-index --find-links ..\/input\/faissgpu17 -r ..\/input\/faissgpu17\/requirements.txt\nimport faiss","5f16ab31":"# Common\n# dir\nINPUT_DATA_PATH_DIR = \"\/kaggle\/input\/shopee-product-matching\/\"\nDATA_TYPE = \"test\" # train or test","70095df8":"# Images\n# dir\nMODEL_PATH_IMAGE = \"..\/input\/pretrained-pytorch-models\/resnet50-19c8e357.pth\"\n\n# cpu\/gpu\nUSE_GPU=True\nACCELERATOR=\"gpu\"\n\n# faiss\nFEATURE_IMAGES_DIM = 1000 # \u30d9\u30af\u30c8\u30eb\u306e\u6b21\u5143(dimension)","ed4c5b77":"# Texts\n# FEATURE_TEXTS_DIM = 25000\nFEATURE_TEXTS_DIM = 25","380b2b04":"test_df = pd.read_csv(INPUT_DATA_PATH_DIR + DATA_TYPE + '.csv')","7e2f2f9d":"# key: posting_id, value: matches\nmatches_defaultdict = defaultdict(set)","f3b7a680":"# load images\n# \u753b\u50cf\u30d5\u30a1\u30a4\u30eb\u306e\u4e00\u89a7\u3092\u53d6\u5f97\u3059\u308b\n# Path\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u751f\u6210\np = Path(INPUT_DATA_PATH_DIR + DATA_TYPE + \"_images\/\")\n\n# dir\u76f4\u4e0b\u306e\u30d5\u30a1\u30a4\u30eb\u3068\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u53d6\u5f97\n# Path.glob(pattern)\u306f\u30b8\u30a7\u30cd\u30ec\u30fc\u30bf\u3092\u8fd4\u3059\u3002\u7d50\u679c\u3092\u660e\u793a\u3059\u308b\u305f\u3081list\u5316\u3057\u3066\u3044\u308b\u304c\u3001\u666e\u6bb5\u306f\u4e0d\u8981\u3002\n# \u30d5\u30a1\u30a4\u30eb\u540d\u306e\u6761\u4ef6\u6307\u5b9a\ntest_images_path_list = list(p.glob(\"*.jpg\"))","4871a318":"# \u30c7\u30d0\u30a4\u30b9\u3092\u4f5c\u6210\u3059\u308b\ndef get_device(use_gpu):\n    if use_gpu and torch.cuda.is_available():\n        # \u3053\u308c\u3092\u6709\u52b9\u306b\u3057\u306a\u3044\u3068\u3001\u8a08\u7b97\u3057\u305f\u52fe\u914d\u304c\u6bce\u56de\u7570\u306a\u308a\u3001\u518d\u73fe\u6027\u304c\u62c5\u4fdd\u3067\u304d\u306a\u3044\u3002\n        torch.backends.cudnn.deterministic = True\n        return torch.device(\"cuda\")\n    else:\n        return torch.device(\"cpu\")\n\n\n# \u30c7\u30d0\u30a4\u30b9\u3092\u9078\u629e\u3059\u308b\u3002\ndevice = get_device(use_gpu=USE_GPU)","7e483b44":"# \u30e2\u30c7\u30eb\u3092\u4f5c\u6210\u3059\u308b\nmodel = torchvision.models.resnet50(pretrained=False)\nmodel.load_state_dict(torch.load(MODEL_PATH_IMAGE))\nif torch.cuda.is_available():\n    model.cuda()","59a0abeb":"# Transforms \u3092\u4f5c\u6210\u3059\u308b\n#. (256, 256) \u306b\u30ea\u30b5\u30a4\u30ba\u3059\u308b\n#. \u753b\u50cf\u306e\u4e2d\u5fc3\u306b\u5408\u308f\u305b\u3066\u3001(224, 224) \u3067\u5207\u308a\u629c\u304f\n#. RGB \u30c1\u30e3\u30f3\u30cd\u30eb\u3054\u3068\u306b\u5e73\u5747 (0.485, 0.456, 0.406)\u3001\u5206\u6563 (0.229, 0.224, 0.225) \u3067\u6a19\u6e96\u5316\u3059\u308b\ntransform = transforms.Compose(\n    [\n        transforms.Resize(256),  # (256, 256) \u3067\u5207\u308a\u629c\u304f\u3002\n        transforms.CenterCrop(224),  # \u753b\u50cf\u306e\u4e2d\u5fc3\u306b\u5408\u308f\u305b\u3066\u3001(224, 224) \u3067\u5207\u308a\u629c\u304f\n        transforms.ToTensor(),  # \u30c6\u30f3\u30bd\u30eb\u306b\u3059\u308b\u3002\n        transforms.Normalize(\n            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n        ),  # \u6a19\u6e96\u5316\u3059\u308b\u3002\n    ]\n)","b4c77ed6":"# \u753b\u50cf\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u7279\u5fb4\u91cf\u3092\u62bd\u51fa\u3059\u308b\nfeature_images = [] # index\u3068\u540c\u3058\u30c7\u30fc\u30bf\nfor test_images_path in tqdm(test_images_path_list):\n    # \u753b\u50cf\u3092\u8aad\u307f\u8fbc\u3080\uff081\u679a\uff09\n    img = Image.open(test_images_path)\n    inputs = transform(img)\n    inputs = inputs.unsqueeze(0).to(device)\n    # \u63a8\u8ad6\u3059\u308b\uff081\u679a\uff09\n    model.eval()\n    outputs = model(inputs).to('cpu').detach().numpy().copy()\n    feature_images.append(outputs[0])\n    # -> torch.Size([1, 1000]) -> numpy.ndarray","1b2a1076":"# faiss\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u4f5c\u6210\nfaiss_index = faiss.IndexFlatL2(FEATURE_IMAGES_DIM)","96ad0378":"# faiss\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u306b\u8ffd\u52a0\nfaiss_index.add(np.array(feature_images))","7e5f28db":"# \u8fd1\u508d\u63a2\u7d22\ndistance_similar_images, idx_similar_images = faiss_index.search(np.array(feature_images), 3)","543cb38a":"# idx\u756a\u53f7\u304b\u3089posting_id\u3092\u53d6\u5f97\u3057\u3066\u3001matches_defaultdict\u306b\u683c\u7d0d\nfor i in tqdm(idx_similar_images):\n    image_source = test_images_path_list[i[0]].name\n    id_source = test_df.query('image == @image_source')['posting_id'].iloc[-1]\n    # -> ex. 'test_3588702337'\n    \n    image_match = test_images_path_list[i[1]].name\n    id_match = test_df.query('image == @image_match')['posting_id'].iloc[-1]\n    # -> ex. 'test_3588702337 test_4015706929'\n    \n    matches_defaultdict[id_source].add(id_source)\n    matches_defaultdict[id_source].add(id_match)","9e3d73d8":"model = TfidfVectorizer(stop_words = 'english', binary = True, max_features = FEATURE_TEXTS_DIM)\nfeature_texts = model.fit_transform(test_df['title']).toarray()\nfeature_texts = np.array(feature_texts,dtype=np.float32)","b471e70f":"# faiss\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u4f5c\u6210\ndimension = len(feature_texts[0])\nnlist = min(100, len(feature_texts))\nquantiser = faiss.IndexFlatL2(dimension) \nfaiss_index = faiss.IndexIVFFlat(quantiser, dimension, nlist, faiss.METRIC_L2)","2a695c41":"# faiss\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u306e\u5b66\u7fd2\u30fb\u8ffd\u52a0\nfaiss_index.train(feature_texts)\nfaiss_index.add(feature_texts)","6d154a5f":"# \u8fd1\u508d\u63a2\u7d22\nfaiss_index.nprobe = 10\n\ns = time.time()\ndistance_similar_texts, idx_similar_texts = faiss_index.search(feature_texts, 3)\ne = time.time()\nprint(\"search time: {}\".format(e-s))","d2182afe":"test_text_list = list(test_df['title'])","51f2e3fb":"# idx\u756a\u53f7\u304b\u3089posting_id\u3092\u53d6\u5f97\u3057\u3066\u3001matches_defaultdict\u306b\u683c\u7d0d\nfor i in tqdm(idx_similar_texts):\n    text_source = test_text_list[i[0]]\n    id_source = test_df.query('title == @text_source')['posting_id'].iloc[-1]\n    # -> ex. 'test_3588702337'\n    \n    text_match = test_text_list[i[1]]\n    id_match = test_df.query('title == @text_match')['posting_id'].iloc[-1]\n    # -> ex. 'test_3588702337 test_4015706929'\n    \n    matches_defaultdict[id_source].add(id_source)\n    matches_defaultdict[id_source].add(id_match)","0c4538d0":"matches_dict = dict(matches_defaultdict)","d966c62a":"submit_list = []\nsubmit_list.append(\"posting_id,matches\")\nfor k, v in tqdm(matches_dict.items()):\n    v = \" \".join(list((v)))\n    submit_list.append(k + \",\" + v)","88027885":"with open('submission.csv', 'w') as f:\n    for d in submit_list:\n        f.write(\"%s\\n\" % d)","515fb9e1":"!cat submission.csv | head -n 10","8a569247":"# # \u753b\u50cf\u8868\u793a\n# # id -> \u753b\u50cf\u30d5\u30a1\u30a4\u30eb\u540d\n# for i in range(1, 10):\n#     id_list = submit_list[i].split(\",\")[1].split(\" \")\n#     for j in range(len(id_list)):\n#         posting_id = id_list[j]\n#         image = test_df.query('posting_id == @posting_id')['image'].iloc[-1]\n\n#         #\u753b\u50cf\u306e\u8aad\u307f\u8fbc\u307f\n#         im = Image.open(INPUT_DATA_PATH_DIR + \"train_images\/\" + image, 'r')\n\n#         #\u753b\u50cf\u3092array\u306b\u5909\u63db\n#         im_list = np.asarray(im)\n#         #\u8cbc\u308a\u4ed8\u3051\n#         plt.imshow(im_list)\n#         #\u8868\u793a\n#         print(image)\n#         plt.show()\n#     print(\"---\")\n","08016b21":"# 5. submit","431a11db":"# 2. conf","ed793ea0":"## 4-2. score similarity images","3974adb5":"## 4-4. score similarity texts","0c9f0d0d":"# 3. utils","1a1a5bfa":"# 1. requirements","57afbc70":"# memo","ebf2c8fe":"# 4. models","d5161d10":"## 4-5. score similarity all","518d3b6c":"# 6. Evaluation","30bd969e":"## 4-3. extract feature texts","27bdc94e":"## 4-1. extract feature images"}}