{"cell_type":{"3607e142":"code","7e9422c7":"code","7d49462b":"code","663f1d68":"code","3e4cb764":"code","78561b0b":"code","3ef7235b":"code","074f2377":"code","85c470c4":"code","23fa96bd":"code","8069dd70":"code","7fcb2e83":"code","c92d7570":"code","1b644b31":"code","036a0e49":"code","4c27a05e":"code","65a1e854":"code","1b716ecc":"code","b74c8aa2":"code","326c7fcd":"code","70c0d4c5":"code","9fdc05ef":"code","41ce9e1b":"code","b801ceb1":"code","15a974d1":"code","d6875af3":"code","a235fe57":"code","0a98d1c9":"code","c23feef8":"code","17b33f26":"code","b699d198":"code","ba502aef":"code","5ca30dce":"code","10685e48":"code","9dccf6be":"code","3a5ce950":"code","809a1e08":"code","6ef03ac2":"code","0c64a6aa":"code","f765d9f8":"code","e86d0ebd":"code","0aea6754":"code","1691cfed":"code","bd0a2447":"code","b744bd6c":"code","149138df":"code","d0959767":"code","97d22061":"markdown","514207f2":"markdown","7bfa0703":"markdown","cdb50783":"markdown","c8b54626":"markdown","0e8a78c2":"markdown","eb27a1ba":"markdown","b508ab7d":"markdown","e50f1db6":"markdown","e0079065":"markdown","b9b14dc3":"markdown","3f992c4c":"markdown","6767fa88":"markdown","5e9e672e":"markdown","f53c76eb":"markdown","568e3704":"markdown","9909b89f":"markdown","9441e159":"markdown","157aaf51":"markdown","51c3a90c":"markdown","2580306b":"markdown","33611cb4":"markdown"},"source":{"3607e142":"import pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# machine learning models\nfrom sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import (RandomForestClassifier, VotingClassifier, \n                              BaggingClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier)\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\n# DNN\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.optimizers import Adam\n\n# scikit utilities\nfrom sklearn.model_selection import train_test_split","7e9422c7":"# Load train and test datasets\ntrain_df= pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/titanic\/test.csv\")\nfull_data = [train_df, test_df]\n\ntrain_df.head(3)","7d49462b":"col_has_null = {\"train_df\":[], \"test_df\":[]}\n\nfor idx in range(0, 2):\n  df = full_data[idx]\n  for col in df:\n    if df[col].isnull().values.any():\n      if idx == 0: \n        col_has_null[\"train_df\"].append(col)\n      else: \n        col_has_null[\"test_df\"].append(col)\n        \ncol_has_null","663f1d68":"train_df[[\"PassengerId\", \"Survived\"]].groupby([\"PassengerId\"], as_index=False).mean()\ntrain_df.drop(columns=[\"PassengerId\"], inplace=True)","3e4cb764":"train_df[[\"Pclass\", \"Survived\"]].groupby([\"Pclass\"], as_index=False).mean().sort_values(by=\"Survived\", ascending=False)","78561b0b":"for df in full_data:\n  df[\"Title\"] = df[\"Name\"].apply(lambda x: x.split(\",\")[1].split(\".\")[0])\n  df.drop(columns=[\"Name\"], inplace=True)\n  \n#   small_title_group_df = pd.DataFrame(train_df[\"Title\"].value_counts()) # check small title group\n#   small_title_group_df.reset_index(inplace=True)\n#   small_title_group_df.rename(columns={\"index\":\"Title\", \"Title\":\"Count\"}, inplace=True)\n#   small_title_group_df = small_title_group_df[small_title_group_df[\"Count\"] < 10]\n#   small_title_group_df.reset_index(drop=True, inplace=True)\n#   print(small_title_group_df)\n  \n  df[\"Title\"] = df[\"Title\"].replace([\" Dr\", \" Rev\", \" Col\", \" Major\", \" the Countess\", \" Sir\", \" Capt\", \" Don\", \" Lady\", \" Jonkheer\", \" Dona\"], \" Rare\")\n  df[\"Title\"] = df[\"Title\"].replace(\" Mlle\", \" Miss\")\n  df[\"Title\"] = df[\"Title\"].replace(\" Ms\", \" Miss\")\n  df[\"Title\"] = df[\"Title\"].replace(\" Mme\", \" Mrs\")\n  df[\"Title\"] = df[\"Title\"].map({\" Mr\":1, \" Mrs\":2, \" Miss\":3, \" Master\":4, \" Rare\":5})\n\ntrain_df[[\"Title\", \"Survived\"]].groupby([\"Title\"], as_index=False).mean().sort_values(by=\"Survived\", ascending=False)","3ef7235b":"for df in full_data:\n  df[\"Sex\"] = df[\"Sex\"].map({\"female\": 1, \"male\": 0})\n\ntrain_df[[\"Sex\", \"Survived\"]].groupby([\"Sex\"], as_index=False).mean().sort_values(by=\"Survived\", ascending=False)","074f2377":"for df in full_data:\n  age_mean = df[\"Age\"].mean()\n  age_std = df[\"Age\"].std()\n  age_null_count = df[\"Age\"].isna().sum()\n  \n  random_null_list = np.random.randint(age_mean - age_std, age_mean + age_std, size=age_null_count)\n  \n  df[\"Age\"][np.isnan(df[\"Age\"])] = random_null_list\n  df[\"Age\"] = df[\"Age\"].astype(int)\n  \n  df.loc[df[\"Age\"] <= 16, \"Age\"] = 0\n  df.loc[(df[\"Age\"] > 16) & (df[\"Age\"] <= 32), \"Age\"] = 1\n  df.loc[(df[\"Age\"] > 32) & (df[\"Age\"] <= 48), \"Age\"] = 2\n  df.loc[(df[\"Age\"] > 48) & (df[\"Age\"] <= 64), \"Age\"] = 3\n  df.loc[(df[\"Age\"] > 64) & (df[\"Age\"] <= 80), \"Age\"] = 4\n  \n# train_df[\"AgeGroup\"] = pd.cut(train_df[\"Age\"], 5)\ntrain_df[[\"Age\", \"Survived\"]].groupby([\"Age\"], as_index=False).mean()","85c470c4":"for df in full_data:\n  df[\"FamilySize\"] = df[\"SibSp\"] + df[\"Parch\"] + 1\n  df[\"IsAlone\"] = (df[\"FamilySize\"] == 1).astype(int)\n  df.drop(columns=[\"SibSp\", \"Parch\", \"FamilySize\"], inplace=True)\n\n# train_df[[\"FamilySize\", \"Survived\"]].groupby([\"FamilySize\"], as_index=False).mean()\ntrain_df[[\"IsAlone\", \"Survived\"]].groupby([\"IsAlone\"], as_index=False).mean()","23fa96bd":"for df in full_data:\n  df.drop(columns=[\"Ticket\"], inplace=True)","8069dd70":"for df in full_data:\n  fare_median = train_df[\"Fare\"].median()\n  df[\"Fare\"] = df[\"Fare\"].fillna(fare_median)\n  \n  df.loc[df[\"Fare\"] <= 7.91, \"Fare\"] = 0\n  df.loc[(df[\"Fare\"] > 7.91) & (df[\"Fare\"] <= 14.454), \"Fare\"] = 1\n  df.loc[(df[\"Fare\"] > 14.454) & (df[\"Fare\"] <= 31.0), \"Fare\"] = 2\n  df.loc[df[\"Fare\"] > 31.0, \"Fare\"] = 3\n  \n  df[\"Fare\"] = df[\"Fare\"].astype(int)\n  \n# train_df[\"FareClass\"] = pd.qcut(train_df[\"Fare\"], 4)\n# train_df[[\"FareClass\", \"Survived\"]].groupby([\"FareClass\"], as_index=False).mean()\ntrain_df[[\"Fare\", \"Survived\"]].groupby([\"Fare\"], as_index=False).mean()","7fcb2e83":"for df in full_data:\n  df.drop(columns=[\"Cabin\"], inplace=True)\n  \ntrain_df.head()","c92d7570":"train_df[\"Embarked\"].value_counts()","1b644b31":"train_df[\"Embarked\"] = train_df[\"Embarked\"].fillna(\"S\")\n\nfor df in full_data:\n  df[\"Embarked\"] = df[\"Embarked\"].map({\"S\":0, \"C\":1, \"Q\":2})\n  \ntrain_df[[\"Embarked\", \"Survived\"]].groupby([\"Embarked\"], as_index=False).mean()","036a0e49":"train_df.isna().sum()","4c27a05e":"# Pclass\ntrain_df = pd.concat([train_df, pd.get_dummies(train_df[\"Pclass\"], prefix=\"Pclass\")], axis=1)\ntest_df = pd.concat([test_df, pd.get_dummies(test_df[\"Pclass\"], prefix=\"Pclass\")], axis=1)\n\n# Age\ntrain_df = pd.concat([train_df, pd.get_dummies(train_df[\"Age\"], prefix=\"Age\")], axis=1)\ntest_df = pd.concat([test_df, pd.get_dummies(test_df[\"Age\"], prefix=\"Age\")], axis=1)\n\n# Fare\ntrain_df = pd.concat([train_df, pd.get_dummies(train_df[\"Fare\"], prefix=\"Fare\")], axis=1)\ntest_df = pd.concat([test_df, pd.get_dummies(test_df[\"Fare\"], prefix=\"Fare\")], axis=1)\n\n# Embarked\ntrain_df = pd.concat([train_df, pd.get_dummies(train_df[\"Embarked\"], prefix=\"Embarked\")], axis=1)\ntest_df = pd.concat([test_df, pd.get_dummies(test_df[\"Embarked\"], prefix=\"Embarked\")], axis=1)\n\n# Title\ntrain_df = pd.concat([train_df, pd.get_dummies(train_df[\"Title\"], prefix=\"Title\")], axis=1)\ntest_df = pd.concat([test_df, pd.get_dummies(test_df[\"Title\"], prefix=\"Title\")], axis=1)","65a1e854":"train_df","1b716ecc":"fig, ax = plt.subplots(figsize=(10, 8))\nplt.title(\"Pearson Correlation of Features\", y=1.05, size=15, color=\"white\")\nsns.heatmap(train_df.iloc[:,:8].astype(float).corr(), linewidths=0.1, vmax=1.0, square=True, cmap=\"RdYlGn_r\", linecolor=\"white\", annot=True, ax=ax)\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)","b74c8aa2":"sns.pairplot(train_df.iloc[:,:8], hue=\"Survived\", palette=\"seismic\", size=1., diag_kws=dict(shade=True), plot_kws=dict(s=10))","326c7fcd":"grid = sns.FacetGrid(train_df, col=\"Survived\", row=\"Sex\", hue=\"Survived\", palette=\"Set1\")\ngrid.map(plt.hist, \"Age\", bins=9)","70c0d4c5":"grid = sns.FacetGrid(train_df, col=\"Survived\", row=\"Pclass\", hue=\"Survived\", palette=\"Set2\")\ngrid.map(plt.hist, \"Fare\", bins=8)","9fdc05ef":"grid = sns.FacetGrid(train_df, col=\"Survived\", row=\"IsAlone\", hue=\"Survived\", palette=\"Set1\")\ngrid.map(plt.hist, \"Title\", bins=9)","41ce9e1b":"grid = sns.FacetGrid(train_df, col=\"Survived\", hue=\"Survived\", palette=\"Set2\")\ngrid.map(plt.hist, \"Embarked\", bins=5)","b801ceb1":"# Delete unnecessary cols\ntrain_df.drop(columns=[\"Pclass\", \"Age\", \"Fare\", \"Embarked\", \"Title\"], inplace=True)\ntest_df.drop(columns=[\"Pclass\", \"Age\", \"Fare\", \"Embarked\", \"Title\"], inplace=True)","15a974d1":"x = train_df.iloc[:, 1:].copy()\ny = train_df[\"Survived\"].copy()\nx_train, x_val, y_train, y_val = train_test_split(x, y)\nx_test = test_df.iloc[:, 1:].copy()\n# x_train.shape, y_train.shape, x_val, y_val, x_test.shape","d6875af3":"# Data frame for comparing\nacc_df = pd.DataFrame(columns=[\"Model\", \"Acc\", \"Val_Acc\"])","a235fe57":"# Logistic Regression\nl_reg = LogisticRegression()\nl_reg.fit(x_train, y_train)\ny_pred = l_reg.predict(x_test)\nacc_log = round(l_reg.score(x_train, y_train) * 100, 2)\nval_acc_log = round(l_reg.score(x_val, y_val) * 100, 2)\nacc_df = acc_df.append(pd.Series([\"LogReg\", acc_log, val_acc_log], index=acc_df.columns), ignore_index=True)","0a98d1c9":"# Support Vector Machines\nsvc = SVC()\nsvc.fit(x_train, y_train)\ny_pred = svc.predict(x_test)\nacc_svc = round(svc.score(x_train, y_train) * 100, 2)\nval_acc_svc = round(svc.score(x_val, y_val) * 100, 2)\nacc_df = acc_df.append(pd.Series([\"SVM\", acc_svc, val_acc_svc], index=acc_df.columns), ignore_index=True)","c23feef8":"# KNN\nknn = KNeighborsClassifier()\nknn.fit(x_train, y_train)\ny_pred = knn.predict(x_test)\nacc_knn = round(knn.score(x_train, y_train) * 100, 2)\nval_acc_knn = round(knn.score(x_val, y_val) * 100, 2)\nacc_df = acc_df.append(pd.Series([\"KNN\", acc_knn, val_acc_knn], index=acc_df.columns), ignore_index=True)","17b33f26":"# Gaussian Naive Bayes\ngnb = GaussianNB()\ngnb.fit(x_train, y_train)\ny_pred = gnb.predict(x_test)\nacc_gnb = round(gnb.score(x_train, y_train) * 100, 2)\nval_acc_gnb = round(gnb.score(x_val, y_val) * 100, 2)\nacc_df = acc_df.append(pd.Series([\"GaussianNB\", acc_gnb, val_acc_gnb], index=acc_df.columns), ignore_index=True)","b699d198":"# Perceptron\nperceptron = Perceptron()\nperceptron.fit(x_train, y_train)\nY_pred = perceptron.predict(x_test)\nacc_perceptron = round(perceptron.score(x_train, y_train) * 100, 2)\nval_acc_perceptron = round(perceptron.score(x_val, y_val) * 100, 2)\nacc_df = acc_df.append(pd.Series([\"Perceptron\", acc_perceptron, val_acc_perceptron], index=acc_df.columns), ignore_index=True)","ba502aef":"# Linear SVC\nlinear_svc = LinearSVC()\nlinear_svc.fit(x_train, y_train)\nY_pred = linear_svc.predict(x_test)\nacc_linear_svc = round(linear_svc.score(x_train, y_train) * 100, 2)\nval_acc_linear_svc = round(linear_svc.score(x_val, y_val) * 100, 2)\nacc_df = acc_df.append(pd.Series([\"LinearSVC\", acc_linear_svc, val_acc_linear_svc], index=acc_df.columns), ignore_index=True)","5ca30dce":"# Stochastic Gradient Descent\nsgd = SGDClassifier()\nsgd.fit(x_train, y_train)\nY_pred = sgd.predict(x_test)\nacc_sgd = round(sgd.score(x_train, y_train) * 100, 2)\nval_acc_sgd = round(sgd.score(x_val, y_val) * 100, 2)\nacc_df = acc_df.append(pd.Series([\"SGD\", acc_sgd, val_acc_sgd], index=acc_df.columns), ignore_index=True)","10685e48":"# Decision Tree\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(x_train, y_train)\nY_pred = decision_tree.predict(x_test)\nacc_decision_tree = round(decision_tree.score(x_train, y_train) * 100, 2)\nval_acc_decision_tree = round(decision_tree.score(x_val, y_val) * 100, 2)\nacc_df = acc_df.append(pd.Series([\"DecisionTree\", acc_decision_tree, val_acc_decision_tree], index=acc_df.columns), ignore_index=True)","9dccf6be":"# Random Forest\nrandom_forest = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\nrandom_forest.fit(x_train, y_train)\ny_pred = random_forest.predict(x_test)\nacc_random_forest = round(random_forest.score(x_train, y_train) * 100, 2)\nval_acc_random_forest = round(random_forest.score(x_val, y_val) * 100, 2)\nacc_df = acc_df.append(pd.Series([\"RandomForest\", acc_random_forest, val_acc_random_forest], index=acc_df.columns), ignore_index=True)","3a5ce950":"# Voting(lr, knn, dt, rf)\nvoting = VotingClassifier(\n    estimators=[(\"lr\", LogisticRegression(solver=\"liblinear\")),\n                (\"knn\", KNeighborsClassifier()),\n                (\"dt\", DecisionTreeClassifier()),\n                (\"rf\", RandomForestClassifier(n_estimators=100))],\n                 voting=\"soft\")\nvoting.fit(x_train, y_train)\ny_pred = voting.predict(x_test)\nacc_voting = round(voting.score(x_train, y_train) * 100, 2)\nval_acc_voting = round(voting.score(x_val, y_val) * 100, 2)\nacc_df = acc_df.append(pd.Series([\"Voting\", acc_voting, val_acc_voting], index=acc_df.columns), ignore_index=True)","809a1e08":"# Bagging (Decision Tree x 500, random sample count: 100, n_jobs: cpu core count)\nbag = BaggingClassifier(\n    DecisionTreeClassifier(), n_estimators=500,\n    max_samples=100, bootstrap=True, n_jobs=-1\n)\nbag.fit(x_train, y_train)\ny_pred = bag.predict(x_test)\nacc_bag = round(bag.score(x_train, y_train) * 100, 2)\nval_acc_bag = round(bag.score(x_val, y_val) * 100, 2)\nacc_df = acc_df.append(pd.Series([\"Bagging\", acc_bag, val_acc_bag], index=acc_df.columns), ignore_index=True)","6ef03ac2":"# AdaBoost\nada = AdaBoostClassifier(\n    DecisionTreeClassifier(), n_estimators=500,\n    algorithm=\"SAMME.R\", learning_rate=0.01\n)\nada.fit(x_train, y_train)\ny_pred = ada.predict(x_test)\nacc_ada = round(ada.score(x_train, y_train) * 100, 2)\nval_acc_ada = round(ada.score(x_val, y_val) * 100, 2)\nacc_df = acc_df.append(pd.Series([\"AdaBoost\", acc_ada, val_acc_ada], index=acc_df.columns), ignore_index=True)","0c64a6aa":"# Gradient Boosting Classifier\ngbc = GradientBoostingClassifier(n_estimators=500, learning_rate=0.01)\ngbc.fit(x_train, y_train)\ny_pred = gbc.predict(x_test)\nacc_gbc = round(gbc.score(x_train, y_train) * 100, 2)\nval_acc_gbc = round(gbc.score(x_val, y_val) * 100, 2)\nacc_df = acc_df.append(pd.Series([\"GBC\", acc_gbc, val_acc_gbc], index=acc_df.columns), ignore_index=True)","f765d9f8":"# XGBoosting Classifier\nxgb = XGBClassifier(n_estimators=500, learning_rate=0.01)\nxgb.fit(x_train, y_train)\ny_pred = xgb.predict(x_test)\nacc_xgb = round(xgb.score(x_train, y_train) * 100, 2)\nval_acc_xgb = round(xgb.score(x_val, y_val) * 100, 2)\nacc_df = acc_df.append(pd.Series([\"XGB\", acc_xgb, val_acc_xgb], index=acc_df.columns), ignore_index=True)","e86d0ebd":"# LightGBM Classifier (faster working speed than xgboost but has a risk for overfitting)\nlgbm = LGBMClassifier(n_estimators=500, learning_rate=0.01, max_depth=2)\nlgbm.fit(x_train, y_train)\ny_pred = lgbm.predict(x_test)\nacc_lgbm = round(lgbm.score(x_train, y_train) * 100, 2)\nval_acc_lgbm = round(lgbm.score(x_val, y_val) * 100, 2)\nacc_df = acc_df.append(pd.Series([\"LGBM\", acc_lgbm, val_acc_lgbm], index=acc_df.columns), ignore_index=True)","0aea6754":"# DNN\nepochs = 2000\n\nmodel = Sequential()\nmodel.add(Dense(512, input_shape=(x_train.shape[1],), kernel_initializer=\"he_uniform\", activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.001)))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(128, kernel_initializer=\"he_uniform\", activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.001)))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(128, kernel_initializer=\"he_uniform\", activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.001)))\nmodel.add(Dense(1, activation=\"sigmoid\"))\n\nmodel.compile(loss=\"binary_crossentropy\", optimizer=Adam(), metrics=[\"accuracy\"])\nhist = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=epochs, batch_size=16, verbose=1)\ny_pred = (model.predict(x_test).flatten() > 0.5).astype(int)\nacc_df = acc_df.append(pd.Series([\"DNN\", round(np.max(hist.history[\"accuracy\"]) * 100, 2), round(np.max(hist.history[\"val_accuracy\"]) * 100, 2)], index=acc_df.columns), ignore_index=True)","1691cfed":"# summarize history for accuracy\nplt.plot(hist.history['accuracy'])\nplt.plot(hist.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(hist.history['loss'])\nplt.plot(hist.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","bd0a2447":"acc_df[\"Variance\"] = np.abs(acc_df[\"Acc\"] - acc_df[\"Val_Acc\"])","b744bd6c":"acc_df.sort_values(by=\"Acc\", ascending=False).reset_index(drop=True)","149138df":"acc_df.sort_values(by=\"Variance\", ascending=True).reset_index(drop=True)","d0959767":"submission_df = pd.DataFrame({\"PassengerId\": test_df[\"PassengerId\"], \"Survived\": y_pred})\nsubmission_df.to_csv(\"submission.csv\", index=False)","97d22061":"## 8. Fare\n\n\"Fare\" are continuous. so, it should be changed to discrete like \"Age\".\n\nI will handle null values and make fare groups.","514207f2":"## 1. PassengerId\n\n\"PassengerId\" does not have any correlation with the target feature \"Survived\".","7bfa0703":"## 4. Sex\n\nIt's strongly correlating with \"Survived\". It's divided into only two values female and male. We can know a survived rate of women is much higher than of men from this feature.\n\nTo apply it to our model, we need to convert it to numerics.\n* male => 0\n* female => 1","cdb50783":"## Findings\n\n* XGBoosting Classifier makes the best score(0.79) among models even though DNN has the highest accuracy.\n* DNN has a problem in terms of overtting.\n* Some of the models have low variance, but they are not effective because they are underfitting.\n\n## Future Works\n\n* Trying to extract new features from existing features to make train accuracy higher.\n* Applying diverse hyperparameters to models to solve overtting.\n","c8b54626":"## 6. SibSp + Parch => FamilySize\n\n* SibSp => Sibling, Spouse\n* Parch => Parent, Children\n* FmailySize = SibSp + Parch\n* IsAlone = 1 if FamilySize == 1 else 0\n\nI think they cannot be valuable independently. However, they can extract valuable feature like \"FamilySize\" and \"IsAlone\" from mixing each other.","0e8a78c2":"check columns of data sets that have null values.","eb27a1ba":"## Fare, Pclass, Survived\n\n`Pclass`\n\n* 1: 1st class\n* 2: 2nd class\n* 3: 3rd class\n\n`Fare`\n\n* 0: fare <= 7.91\n* 1: 7.91 < fare <= 14.454\n* 2: 14.454 < fare <= 31.0\n* 3: 31.0 < fare\n\n`Findings`\n\nAs much as having high economic status(high fare, high-level seat), people tended to survive.","b508ab7d":"## 2. Pclass\n\nPclass is an ordinal feature. As much as SES(socio-economic-status) is higher, people tended to be survived.\n\nBased on it, we can guess Pclass(SES) has a correlation with the feature \"Survived\".","e50f1db6":"## Age, Sex, Survived\n\n`Age`\n\n* 0: age <= 16\n* 1: 16 < age <= 32\n* 2: 32 < age <= 48\n* 3: 48 < age <= 64\n* 4: 64 < age <= 80\n\n`Sex`\n\n* 0: male\n* 1: female\n\n`Findings`\n\n* Young men are majority in group of people died.\n* Women survived a lot comparing to men.","e0079065":"## 3. Name\n\n\"Name\" has no correlation with \"Survived\" as it has so unique values. In other words, it does not have any pattern.\n\nHowever, we can extract \"title\" from it like Mr, Miss, Mrs, and so on. I think it's going to be useful information for us. That is because this new feature has a concept of group.\n\nHonestly, titles that have only few values(n < 10) have no meaning as data sets. so, we'll either merge them into other large groups or make them grouped as a new group \"Rare\".","b9b14dc3":"## 7. Ticket","3f992c4c":"## 10. Embarked\n\nThere are two missing values on \"Embarked\" column of the train data set.\n\nI will merge them into the majority group \"S\"(644 out of 889).","6767fa88":"## Title, IsAlone, Survived\n\n`Title`\n\n* 1: Mr\n* 2: Mrs\n* 3: Miss\n* 4: Master\n* 5: Rare\n\n`IsAlone`\n\n* 0: No\n* 1: Yes\n\n`Findings`\n\nPeople having title \"Mr\" and boarding alone overwhelmingly died.","5e9e672e":"# Models\n\n* Logistic Regression\n* Support Vector Machines\n* KNN\n* Naive Bayes\n* Perceptron\n* Linear SVC\n* Stochastic Gradient Descent\n* Decision Tree\n* Random Forest\n* Voting\n* Bagging\n* AdaBoost\n* Gradient Boosting Classifier\n* XGBoosting Classifier\n* DNN\n\nWe'll apply diverse machine learning models to our data sets, and then compare their accuracy to each other to find best models.","f53c76eb":"## 9. Cabin\n\nFeature \"Cabin\" has plenty of null values(687 out of 891).\n\nIt is not valuable as a data set. So, I will get rid of it on my table.","568e3704":"## Pairplot\n\nWe will visualize correlations among each of the features by using seaborn pairplot.","9909b89f":"# Data Analysis & Visualization","9441e159":"## Getting Dummies for One-Hot-Encoding","157aaf51":"## Embarked, Survived\n\n`Embarked`\n\n* 0: Southampton\n* 1: Cherbourg\n* 2: Queenstown\n\n`Findings`\n\nPeople boarding from Southampton died more than from other regions.","51c3a90c":"## 5. Age\n\nValues of \"Age\" are continuous. so, they need to be changed to discrete to be used on our model.\n\nTo make them discrete, we can make age-groups meaning of age-ranges.\n\n* group 1 : 0 <= age < 16\n* group 2 : 16 <= age < 32\n* group 3 : 32 <= age < 48\n* ...\n* group k : 64 <= age < 80\n\nAdditionally, \"Age\" includes null values. They should be handled to apply on our model by generating random values.","2580306b":"# Feature Engineering","33611cb4":"## Pearson Correlation Heatmap\n\nIt will provide you intuitions about correlations among features."}}