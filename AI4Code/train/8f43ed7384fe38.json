{"cell_type":{"f390b9ca":"code","a0a4ec33":"code","ecfe19d1":"code","87e86e73":"code","e53d4128":"code","bc17038e":"code","d5acd645":"code","8916d08c":"code","4f07acfd":"code","acf489b8":"code","47626209":"code","9d753019":"code","25fffc8c":"code","7095463b":"code","fc72147f":"code","add7a668":"code","62ff4916":"code","8f3b7d40":"code","f88aa51e":"code","e075a971":"code","61af2e90":"code","45ccee15":"code","0ec63c4c":"code","91cb4acf":"code","f64d4e2f":"code","6076083f":"code","00067bc3":"code","930ff061":"code","d793901b":"code","150a76da":"code","c980df8e":"code","83a4756c":"code","cb7c6d9f":"code","f832099f":"code","4a036aae":"markdown","fe2ea86a":"markdown","507c539d":"markdown","6ad82c41":"markdown","c2007c34":"markdown","4b253306":"markdown","0de64dbd":"markdown","e1a72cb8":"markdown","f1b44a1e":"markdown","773c6a60":"markdown","ca39f7f3":"markdown","d13cb200":"markdown","19de905b":"markdown","b4806bed":"markdown","027048de":"markdown","b334f0c8":"markdown","53a56f06":"markdown","8629f031":"markdown","1117e36c":"markdown","fd2db7f0":"markdown","7e796db6":"markdown","20be2fc7":"markdown","ee700dc7":"markdown","8f352022":"markdown","5b1204d0":"markdown","2b17d506":"markdown","b6f5869f":"markdown","96e6aac5":"markdown","0b09fd0c":"markdown","07330ad3":"markdown","1b15b6cb":"markdown","b9137802":"markdown","aa0d5f4e":"markdown","b7fd9d3f":"markdown","b95481d0":"markdown","1b8aa61e":"markdown","d3ba0d0f":"markdown","958dd30f":"markdown","bc829233":"markdown","02d20be4":"markdown","1a60b291":"markdown","f46198c5":"markdown"},"source":{"f390b9ca":"!pip install statsmodels  # modelos","a0a4ec33":"!pip install squarify  # TreeMap","ecfe19d1":"import numpy as np \nimport pandas as pd\nimport squarify\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\n\nfrom scipy import stats\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom datetime import datetime, timedelta\nfrom matplotlib import dates as mpl_dates\n\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.arima.model import ARIMA\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Activation,Flatten","87e86e73":"# Pandas config\npd.options.mode.chained_assignment = None \npd.set_option('display.max_columns', None) # Le indicamos a pandas que nos muestre todas las columnas\npd.options.display.float_format = \"{:,.2f}\".format  # Configuramos un formato m\u00e1s agradable a la vista","e53d4128":"# Cargamos el dataset\ndata = pd.read_csv(\".\/..\/input\/datos-clima-covid19-espaa\/salida_datos_clima_covid19.csv\", delimiter=\";\", encoding ='utf-8')","bc17038e":"data.head() # Nos muestra las primeras filas","d5acd645":"data.shape # Cuantas filas y columnas hay","8916d08c":"data.duplicated().sum()","4f07acfd":"pd.DataFrame((data.isna().sum() \/ len(data) * 100).values.reshape(1,-1), columns = data.columns)  # % de datos nulos en cada columna","acf489b8":"# Seleccionamos las columnas y mostramos un ejemplo\ndata = data[['fecha','casos','longitud','latitud','habitantes','provincia','CCAA','zona','altitud','tmed','tmin','prec','horatmin','horatmax','tmax','velmedia', 'racha','horaracha']]\ndata.head()","47626209":"for i in ['tmed','tmin','tmax','velmedia','prec','racha']:  #columnas que presentan problemas\n    data[i].replace(np.NaN, '100000.0', inplace=True) # Valores nulos se cambian por un valor espeficico temporal para poder transoformar\n    data[i].replace('Acum', '100000.0', inplace=True) # Lo mismo con valores string\n    data[i] = data[i].str.replace(',','.', regex=True) # Se cambian . por , para que pueda transforma a valor num\u00e9rico bien\n    data[i] = pd.to_numeric(data[i], downcast='float')  # se transforma a valor num\u00e9rico\n    data[i].replace(100000, np.NaN, inplace=True) # Se vuelve a poner el NaN donde estaba","9d753019":"#imputerKNN = KNNImputer(n_neighbors=7, weights=\"distance\") #inputacion mediante KNN\nimputerMean = SimpleImputer(missing_values=np.NaN, strategy='mean')\nfor i in ['tmed','tmin','tmax','velmedia','prec','racha']:\n    data[i] = imputerMean.fit_transform(data[i].values.reshape(-1,1))","25fffc8c":"imputerMstFrec = SimpleImputer(missing_values='nan', strategy='most_frequent')\n\ndata['horaracha'] = data['horaracha'].astype('str')\ndata['horaracha'] = data['horaracha'].replace('Varias','nan')\ndata['horaracha'] = imputerMstFrec.fit_transform(data['horaracha'].values.reshape(-1,1))[:,0]\n\ndata['horatmin'] = data['horatmin'].astype('str')\ndata['horatmin'] = data['horatmin'].replace('Varias','nan')\ndata['horatmin'] = imputerMstFrec.fit_transform(data['horatmin'].values.reshape(-1,1))[:,0]\n\ndata['horatmax'] = data['horatmax'].astype('str')\ndata['horatmax'] = data['horatmax'].replace('Varias','nan')\ndata['horatmax'] = imputerMstFrec.fit_transform(data['horatmax'].values.reshape(-1,1))[:,0]","7095463b":"pd.DataFrame((data.isna().sum() \/ len(data) * 100).values.reshape(1,-1), columns = data.columns) # Volvemos a ver el % de nulos","fc72147f":"medias = pd.DataFrame()\n\nfor i in ['casos','habitantes','altitud','tmed','tmin','tmax','velmedia','prec','racha']:\n    s = {'Media':data[i].mean(),\n         'Std': data[i].std(),\n         'Min': data[i].min(),\n         '25%':data[i].quantile(0.25),\n         '50%':data[i].quantile(0.5),\n         '75%':data[i].quantile(0.75),\n         'Max': data[i].max(),\n         'Rango': data[i].max() - data[i].min(),\n         'Skewness': stats.skew(data[i]),\n         'Mediana': data[i].median(),\n         'Moda': data[i].mode().values[0]\n         }\n    \n    medias[i]= pd.Series(s)\n\nmedias","add7a668":"f = plt.figure(figsize=(20,13))\n\nf.add_subplot(331)\nsns.histplot(x='tmin', data=data, bins=10, color = \"skyblue\").set(xlabel='Temperatura M\u00ednima')\n\nf.add_subplot(332)\nsns.histplot(x='tmed', data=data, bins=10, color = \"orange\").set(xlabel='Temperatura Media')\n\nf.add_subplot(333)\nsns.histplot(x='tmax', data=data, bins=10, color = \"red\").set(xlabel='Temperatura M\u00e1xima')\n\nf.add_subplot(334)\nsns.histplot(x='velmedia', data=data, bins=10).set(xlabel='Vel. media viento')\n\nf.add_subplot(335)\nsns.histplot(x='prec', data=data, bins=10).set(xlabel='Precipitaciones')\n\nf.add_subplot(336)\nsns.histplot(x='altitud', color='brown', data = data.drop_duplicates(['zona']), bins=10).set(xlabel='Altitud')\n\nf.add_subplot(337)\nsns.kdeplot(x='casos',color='green', data = data).set(xlabel='Casos')\n\nf.add_subplot(338)\nsns.histplot(x='racha', data = data, bins=10).set(xlabel='Racha viento')\n\nf.add_subplot(339)\nsns.kdeplot(x='habitantes',color='pink', data = data.drop_duplicates(['zona'])).set(xlabel='Habitantes')\nplt.show()\n","62ff4916":"plt.figure(figsize=(10, 8))\n\nmask_heatmap = np.triu(np.ones_like(data[['casos', 'habitantes', 'altitud','tmed','tmin','tmax','velmedia','prec','racha']].corr(), dtype=bool))\nsns.heatmap(data=data[['casos', 'habitantes', 'altitud','tmed','tmin','tmax','velmedia','prec','racha']].corr(), annot=True, linewidth=3, cmap='Greens', mask = mask_heatmap)\nplt.show()","8f3b7d40":"sns.pairplot(data=data[['casos', 'habitantes', 'altitud','tmed','tmin','tmax','velmedia','prec','racha']], diag_kind='hist')\nplt.show()","f88aa51e":"plt.figure(figsize=(20,15))\n\nfor i,c in enumerate(['casos', 'habitantes', 'altitud','tmed','tmin','tmax','velmedia','prec','racha']):\n    plt.subplot(431+i)\n    plt.title(c)\n    plt.boxplot(data[c], patch_artist=True)\n    plt.xticks([])\nplt.show()","e075a971":"maxi = data['casos'].max()\nd = data[data['casos']==maxi]\nprint(\"El d\u00eda en el que m\u00e1s casos se registrar\u00f3n fue el\", d['fecha'].values[0], \"con\", d['casos'].values[0], \"casos en\", d['zona'].values[0])","61af2e90":"frias = data.groupby('zona').mean()\nmasfrio = frias[frias['tmed'].max() == frias['tmed']]\ncalor = frias[frias['tmed'].min() == frias['tmed']]\nmasfrio.head()\n\nprint(\"La zona m\u00e1s c\u00e1lida ha sido:\", masfrio.index[0],\" con una media de temperatura de\",round(masfrio.tmed[0],2),\"\u00ba\")\n\nprint(\"La zona m\u00e1s fr\u00eda ha sido:\", calor.index[0],\" con una media de temperatura de\",round(calor.tmed[0],2),\"\u00ba\")","45ccee15":"topCasos = data.groupby('zona').sum('casos').sort_values('casos' ,ascending=False)['casos']\npd.DataFrame(topCasos.head(10))","0ec63c4c":"maxTemp = data['horatmax'].value_counts()\nminTemp = data['horatmin'].value_counts()\nracha = data['horaracha'].value_counts()\nprint(\"La hora a la que m\u00e1s se suele alcanza las m\u00e1ximas temeperaturas es\", maxTemp.idxmax(), \"alcanzandose un total de\",maxTemp.max(),\"veces\")\nprint(\"La hora a la que m\u00e1s se suele alcanza las minimas temeperaturas es\", minTemp.idxmax(), \"alcanzandose un total de\",minTemp.max(),\"veces\")\nprint(\"La hora a la que m\u00e1s se suele alcanzar las rachas m\u00e1s fuertes es\", racha.idxmax(), \"alcanzandose un total de\",racha.max(),\"veces\")","91cb4acf":"porCCAA = data.groupby('provincia')['casos'].sum().sort_values(ascending=False)\nplt.figure(figsize=(14,10))\nsquarify.plot(sizes=porCCAA.values, label=porCCAA.index[:9], alpha=.8)\nplt.axis('off')  # Ocultamos los valores de los ejes\nplt.show()","f64d4e2f":"#dataByPro = data.groupby([\"provincia\"]).mean().sort_values(\"tmed\", ascending=False)\n\naux = data.drop_duplicates(subset=['zona'])\n\ndataByHab = aux.groupby([\"provincia\"]).sum().sort_values(\"habitantes\",ascending=False)['habitantes']\n\ndataCont = data.groupby([\"provincia\"]).sum('casos').sort_values(\"habitantes\",ascending=False)['casos']\/dataByHab*100\ndataCont = pd.DataFrame(dataCont)","6076083f":"plt.figure(figsize=(20,6))\nplt.title(\"% casos respecto a la poblaci\u00f3n\")\nsns.barplot(x=dataCont.index , y = 0 ,data = dataCont, order = dataCont.index)\nplt.xticks(rotation=-75)\nplt.legend(loc=\"upper right\")\nplt.show()","00067bc3":"aux = data.drop_duplicates(subset=['zona']).sort_values('habitantes')\n\n# dividimos en 5 grupos\ng1 = aux[:81]\ng2 = aux[82:163]\ng3 = aux[164:244]\ng4 = aux[245:325]\ng5 = aux[326:408]\n\np1 = data[data['zona'].isin(g1['zona'])]\np1 = p1.groupby('zona').mean().sort_values('casos', ascending=False)\n\np2 = data[data['zona'].isin(g2['zona'])]\np2 = p2.groupby('zona').mean().sort_values('casos', ascending=False)\n\np3 = data[data['zona'].isin(g3['zona'])]\np3 = p3.groupby('zona').mean().sort_values('casos', ascending=False)\n\np4 = data[data['zona'].isin(g4['zona'])]\np4 = p4.groupby('zona').mean().sort_values('casos', ascending=False)\n\n\np5 = data[data['zona'].isin(g5['zona'])]\np5 = p5.groupby('zona').mean().sort_values('casos', ascending=False)\n\n\n\nf = plt.figure(figsize=(20,20))\n\nf.add_subplot(331)\nplt.title(\"Grupo 1\")\nmask_heatmap = np.triu(np.ones_like(p1[['casos', 'habitantes', 'altitud','tmed','tmin','tmax','velmedia','prec','racha']].corr(), dtype=bool))\nsns.heatmap(data=p1[['casos', 'habitantes', 'altitud','tmed','tmin','tmax','velmedia','prec','racha']].corr(), annot=True, linewidth=3, cmap='Blues', mask = mask_heatmap)\n\nf.add_subplot(332)\nplt.title(\"Grupo 2\")\nmask_heatmap = np.triu(np.ones_like(p2[['casos', 'habitantes', 'altitud','tmed','tmin','tmax','velmedia','prec','racha']].corr(), dtype=bool))\nsns.heatmap(data=p2[['casos', 'habitantes', 'altitud','tmed','tmin','tmax','velmedia','prec','racha']].corr(), annot=True, linewidth=3, cmap='Blues', mask = mask_heatmap)\n\nf.add_subplot(333)\nplt.title(\"Grupo 3\")\nmask_heatmap = np.triu(np.ones_like(p3[['casos', 'habitantes', 'altitud','tmed','tmin','tmax','velmedia','prec','racha']].corr(), dtype=bool))\nsns.heatmap(data=p3[['casos', 'habitantes', 'altitud','tmed','tmin','tmax','velmedia','prec','racha']].corr(), annot=True, linewidth=3, cmap='Blues', mask = mask_heatmap)\n\nf.add_subplot(334)\nplt.title(\"Grupo 4\")\nmask_heatmap = np.triu(np.ones_like(p4[['casos', 'habitantes', 'altitud','tmed','tmin','tmax','velmedia','prec','racha']].corr(), dtype=bool))\nsns.heatmap(data=p4[['casos', 'habitantes', 'altitud','tmed','tmin','tmax','velmedia','prec','racha']].corr(), annot=True, linewidth=3, cmap='Blues', mask = mask_heatmap)\n\n\nf.add_subplot(335)\nplt.title(\"Grupo 5\")\nmask_heatmap = np.triu(np.ones_like(p5[['casos', 'habitantes', 'altitud','tmed','tmin','tmax','velmedia','prec','racha']].corr(), dtype=bool))\nsns.heatmap(data=p5[['casos', 'habitantes', 'altitud','tmed','tmin','tmax','velmedia','prec','racha']].corr(), annot=True, linewidth=3, cmap='Blues', mask = mask_heatmap)\n\nplt.show()","930ff061":"data['fecha'] = pd.to_datetime(data['fecha'], format='%d\/%m\/%Y')","d793901b":"covid = data.groupby('fecha').sum('casos')\ncovid_media = covid.rolling(window=7).mean()\n\nplt.figure(figsize=(23,6))\nplt.title(\"Evoluci\u00f3n temporal de los casos\")\nplt.plot(covid['casos'], c='blue', label=\"Casos diarios\")\n\nplt.xlabel(\"Fecha\")\nplt.ylabel(\"Casos diarios\")\nplt.plot(covid_media['casos'], c='red', label=\"media movil\")\nplt.legend(loc=\"upper left\")\n#plt.xticks(np.arange(0,len(covid),15))\nplt.show()","150a76da":"sd = seasonal_decompose(covid['casos'],period=7)","c980df8e":"sd.trend.plot(figsize=(20,4))\nplt.title(\"Tendencia\")\nplt.show()\n\nsd.seasonal.plot(figsize=(20,4))\nplt.title(\"Seasonal\")\nplt.show()\n\nsd.resid.plot(figsize=(20,4))\nplt.title(\"Residual\")\nplt.show()","83a4756c":"minima = data.groupby(['fecha','CCAA'], as_index=False).agg({\"tmin\": \"mean\"})\nmaxima = data.groupby(['fecha','CCAA'], as_index=False).agg({\"tmax\": \"mean\"})\nmedia = data.groupby(['fecha','CCAA'], as_index=False).agg({\"tmed\": \"mean\"})\nplt.figure(figsize=(20,15))\n\nfor i,c in enumerate(data['CCAA'].unique()):\n    plt.subplot(321+i)\n    aux1 = minima[minima['CCAA']==c].rolling(window=7).mean()\n    aux2 = media[media['CCAA']==c].rolling(window=7).mean()\n    aux3 = maxima[maxima['CCAA']==c].rolling(window=7).mean()\n    plt.title(c)\n    plt.plot( minima[minima['CCAA']==c]['fecha'],aux1['tmin'], linestyle='solid', label=\"minima\", lw = 3, c='aqua')\n    plt.plot( media[media['CCAA']==c]['fecha'],aux2['tmed'],linestyle='solid', label=\"tmedia\", lw = 3, c='orange')\n    plt.plot( maxima[maxima['CCAA']==c]['fecha'],aux3['tmax'], linestyle='solid', label=\"tmax\", lw = 3, c='crimson')\n    plt.ylim(-1.5, 40.0)\n    #plt.xlim(['20-01-01', '21-01-30'])\n    plt.legend(loc=\"upper left\")\nplt.tight_layout()","cb7c6d9f":"poCom = data.groupby(['CCAA','fecha']).sum()\nuniquePlaces = data['CCAA'].unique()\nplt.figure(figsize=(20,8))\nfor p in uniquePlaces:\n    plt.plot(poCom.loc[p]['casos'].cumsum(), label=p)\n    #plt.xticks(np.arange(1,300,20))\nplt.legend(loc=\"upper left\")\nplt.ylabel(\"Casos\")\nplt.xlabel(\"Fecha\")\nplt.show()","f832099f":"plt.figure(figsize=(20,15))\nfor i,c in enumerate(uniquePlaces):\n    plt.subplot(321+i)\n    xe = poCom.loc[c]\n    xe.index = xe.index.to_period('D')\n    xe.sort_index(inplace=True)\n    mod = ARIMA(xe['casos'], order=(1,1,1))\n    model_fit = mod.fit()\n    lon = len(xe['casos'])\n    xe['pred'] = model_fit.predict(start=1, end=lon)\n    plt.title(c)\n    xe.index = xe.index.to_series().astype(str)\n    plt.plot(xe.casos, label=\"Casos reales\", color=\"blue\")\n    plt.plot(xe.pred, label=\"Casos predichos\", color=\"orange\")\n    plt.xticks(np.arange(1,len(xe),20))\n    plt.legend(loc=\"upper left\")\nplt.show()","4a036aae":"### **Predicci\u00f3n con modelo ARIMA**\nA continuaci\u00f3n vamos a probar el modelo para series temporales ARIMA y comprobaremos que resultados obtiene para cada comunidad.","fe2ea86a":"### \u00bfC\u00f3mo se han ido aumentando los casos?","507c539d":"Se ha obtenido una tabla resumen que trata de recoger la tendencia central y dispersi\u00f3n por cada atributo de inter\u00e9s.\n\nSobre la tendencia central nos dar\u00e1 diferentes medidas como son la media, mediana, moda y el *skewness*.\nY sobre la dispersi\u00f3n nos ofrecer\u00e1 la desviaci\u00f3n est\u00e1ndar, valores m\u00e1ximos y m\u00ednimos, el rango, los cuantiles.\n\nEsta tabla, nos da informaci\u00f3n muy \u00fatil dependiendo de nuestros objetivos. Se puede encontrar informaci\u00f3n tanto obvia como curiosa, por ejemplo, es obvio que la moda de las precipitaciones sea 0, ya que no llueve todos los d\u00edas. Pero es curioso que la media de poblaci\u00f3n en estas comunidades sea de 33.852,22 habitantes. Si analizamos la columna *casos*, que nos es de especial inter\u00e9s en este notebook, vemos que su media se sit\u00faa en 5.36 casos diarios y una moda de 0, vemos que contiene un valor de *skewness* s\u00faper sesgado hacia la derecha. Si hablamos de la dispersi\u00f3n encontramos una desviaci\u00f3n de 39.11, debida seguramente a los outliers que encontramos, ya que, si nos fijamos en los cuantiles, el 75% esta en 3 casos diarios.","6ad82c41":"De acuerdo, ya lo tenemos m\u00e1s \u201climpio\u201d, pero, sin embargo, \u00a1tenemos un peque\u00f1o problemilla! Algunos campos tienen valores nulos\u00a1 Esto nos puede suponer un problema para realizar algunas operaciones, as\u00ed que tenemos dos opciones, o eliminar las filas con nulos o aplicar imputaci\u00f3n para intentar rellenar esos valores faltantes. Ya que la eliminaci\u00f3n de filas en este caso, al ser datos temporales, puede ser eliminada informaci\u00f3n importante, por ello nos decantaremos por intentar rellenar los valores faltantes.\n\n\nEn el siguiente paso vamos a solucionar un peque\u00f1o problema, ya que pandas no ha considerado a algunos de los datos como num\u00e9ricos, por lo tanto, los tendremos que transformar.","c2007c34":"Ning\u00fan valor duplicado, perfecto. Ahora que sabemos con que datos vamos a tratar, una informaci\u00f3n interesante ser\u00eda saber el % datos nulos que tenemos en cada columna. Esto nos ayudar\u00e1 a la hora de seleccionar de seleccionar caracter\u00edsticas, ya que aquellas en las que existan muchos nulos no nos ser\u00e1n de utilidad.","4b253306":"> Una forma de responder a esta pregunta de forma visual y poder comparar con otras comunidades f\u00e1cilmente es usar un treemap.","0de64dbd":"Los resultados anteriores, nos hacen darnos cuenta que es normal que se haya obtenido siempre a Catalunya como comunidad m\u00e1s afectada y con m\u00e1s contagios. Y es qu\u00e9, es de la que existen m\u00e1s datos, junto con navarra.","e1a72cb8":"Ya hemos conseguido quitar los nulos de todas las columnas. Ahora vamos a explorar los datos y ver como est\u00e1n distribuidos.","f1b44a1e":"> \u00bfA qu\u00e9 hora sopla m\u00e1s el viento?\u00bfCuando se suelen alcanzar la m\u00e1xima temperatura?\u00bfY la m\u00ednima?","773c6a60":"El primer paso va a ser instalar las librer\u00edas necesarias, importarlas y cargar el dataset","ca39f7f3":"> Se puede observar la evoluci\u00f3n de los casos a lo largo del tiempo, con las 3 famosas olas que hemos sufrido. Ha esta serie temporal se le ha a\u00f1adido la media m\u00f3vil\n\nAhora vamos a analizar las componentes de esa serie temporal, para detectar tendencias, estacionalidad y los residuos.","d13cb200":"Como se muestra en el output de arriba, m\u00e1s de la mitad de las columnas contienen valores nulos, algunas en un bajo porcentaje y en otras ocasiones tienen un nivel bastante alto.\n\nDe estas columnas, no todas nos van a ser de inter\u00e9s, por lo tanto, vamos a seleccionar aquellas que puedan ser de mayor utilidad para nuestro objetivo. Al no ser ning\u00fan experto en la materia la selecci\u00f3n de las caracter\u00edsticas se har\u00e1 bas\u00e1ndose en algunas t\u00e9cnicas como la de la correlaci\u00f3n o la cantidad de nulos de un determinado atributo.\n\nDebido a la cantidad de nulos y la inconsistencia de algunas caracter\u00edsticas que mezclan que mezclan valores num\u00e9ricos y categ\u00f3ricos. Ignoraremos las columnas relacionadas con la presi\u00f3n atmosf\u00e9rica, ya que contiene un alto n\u00fameros de valores nulos, tambi\u00e9n la columna sol, ya que adem\u00e1s de contener casi la mitad de sus datos nulos y sin ser un experto en la materia, no parece a priori que tenga demasiada relaci\u00f3n con la cantidad de contagios de covid. Lo mismo se podr\u00eda interpretar con la direcci\u00f3n del viento.\n\nTambi\u00e9n se ha eliminado por motivos de simplificar el dataset, la latitud, longitud e indicativo de la estaci\u00f3n meteorol\u00f3gica donde se han recogido los datos. Esto se debe a que no se pretende utilizar esta informaci\u00f3n para nada.\n\nDe momento se mantendr\u00e1n las siguientes columnas:\n* fecha\n* casos\n* habitantes\n* provincia\n* CCAA\n* altitud\n* tmed : Temperatura media\n* tmin : Temperatura minima\n* tmax  : Temperatura m\u00e1xima\n* horatmin: Hora a la que se di\u00f3 la temperatura m\u00ednima\n* horatmax: Hora a la que se di\u00f3 la temperatura m\u00e1xima\n* velmedia: Velocidad media del viento\n* racha: Velocidad de racha de viento\n* horaracha: Hora de la racha de viento","19de905b":"### \u00bfQu\u00e9 % de poblaci\u00f3n ha sido contagiada en cada provincia?","b4806bed":"### Distribuci\u00f3n de los datos","027048de":"# Visualizaciones\nLlegados a este punto nos vamos a centrar en las visualizaciones. Desde treemaps, series temporales y alguna predicci\u00f3n. Y lo enfocaremos de una forma en el que la visualizaci\u00f3n va a tratar de responder a un pregunta planteada.","b334f0c8":"# Evoluci\u00f3n temporal \n","53a56f06":">Este gr\u00e1fico nos nuestra que existen fuertes correlaciones entre las variables de la temperatura, y entre las rachas y la velocidad media del viento, lo cual no es nada sorprendente, ya que es una correlaci\u00f3n con bastante sentido. En nuestra variable de inter\u00e9s *casos*, vemos como existe una correlaci\u00f3n positiva con el n\u00ba de habitantes, lo cu\u00e1l parece tener sentido, ya que, si hay m\u00e1s habitantes en una poblaci\u00f3n, ser\u00e1 m\u00e1s f\u00e1cil contagiar a otros. Estoy seguro de que podr\u00eda haber existido otras correlaciones, pero la naturaleza del dataset hace dif\u00edcil verlas, y con esto me refiero a qu\u00e9, los contagios tardan en ser notificados y una persona tarda en desarrollar los s\u00edntomas, lo que significa que el estado de una variable no refleja en una misma fila el n\u00famero de contagios que podr\u00eda estar ayudando a aumentar o reducir. Esto dificulta la b\u00fasqueda de correlaciones con este dataset.\n\nOtra forma de ver estas relaciones ser\u00eda mediante un pair plot.","8629f031":"### Revisemos las correlaciones\nEsta vez, vamos a volver a realizar una correlaci\u00f3n de los datos, pero de una forma un poco diferente. Vamos a dividir en dataset en 5 grupos, estos grupos dividir\u00e1n la informaci\u00f3n por zonas en funcion de sus habitantes, siendo el grupo 1, el grupo con menos habitantes por zona y el grupo 5, el que m\u00e1s tiene. Lo ideal hubiera sido usar un clustering, y dividir el dataset en grupos que contengan zonas con caracter\u00edsticas similares, pero debido a que no entraba dentro de los objetivos de esta pr\u00e1ctica han sido divididos simplemente por la cantidad de habitantes.\n","1117e36c":"## Describiendo los datos\n### Tendencia central y dispersi\u00f3n","fd2db7f0":"> Aqu\u00ed vemos como est\u00e1n distribuidos los datos gr\u00e1ficamente, adem\u00e1s se ha a\u00f1adido colores para que sea m\u00e1s f\u00e1cil identificar las variables que visualizamos. La distribuci\u00f3n de las temperaturas sigue una distribuci\u00f3n normal, como ya indicaba el valore de *skewness* de la anterior tabla. Las otras distribuciones est\u00e1n bastante sesgadas, la distribuci\u00f3n de la altitud, nos dice que mayoritariamente tiene valores bajos, lo que indica que las zonas de este dataset son zonas costeras. Existen dos variables especialmente sesgadas, como son habitantes, por los outliers producidos por ciudades importantes donde viven gran cantidad de gente. Y lo mismo pasa con casos, en los que han existido d\u00edas con gran n\u00famero de contagios.","7e796db6":"###\u00a0Correlaciones\n\nVamos a buscar si existen relaciones entre todo el dataset.","20be2fc7":"Veamos como va quedando el dataset:","ee700dc7":"Otra gr\u00e1fica interesante podr\u00eda ser como se han ido acumulando los casos por cada comunidad, no obstante, esta visualizaci\u00f3n queda un poco extra\u00f1a por la diferencia de datos temporales en cada comunidad.","8f352022":"### Outliers\nLos valores at\u00edpicos o tambi\u00e9n llamados *outliers*, merecen la pena ser detectados y estudiados, por sus posibles efectos en predicciones u obtenci\u00f3n de medidas. Por ello a continuaci\u00f3n se muestra una visualizaci\u00f3n para cada variable.","5b1204d0":"### Algunas preguntas...Y su respuesta\nA medida que vamos explorando el dataset, puede que nos hayan empezado a surgir preguntas y a la cu\u00e1l se pueda obtener la respuesta de forma f\u00e1cil. He aqu\u00ed algunos ejemplos.\n\n>\u00bfCu\u00e1l fue el d\u00eda con m\u00e1s casos?","2b17d506":"Mediante el pairplot se pueden observar las correlaciones de mediante gr\u00e1ficos.","b6f5869f":"La inmputaci\u00f3n de las horas en este dataset es algo m\u00e1s dif\u00edcil de tratar ya que tienen en algunas filas con datos en horas y otras en string.\nLa estrategia utilizada va a ser cambiar las que tienen un string o un valor nulo por el valor m\u00e1s frecuente de esta columna, esta decisi\u00f3n es un tanto dif\u00edcil ya que, para la columna de la hora de una racha de viento, no esta muy claro que sea la mejor opci\u00f3n, debido a que estas rachas podr\u00edan ser a cualquier hora. Sin embargo, para las horas de la temperatura m\u00e1xima y m\u00ednima si que tiene m\u00e1s sentido utilizar esta estrategia, ya que, por lo general, las horas m\u00e1s calurosas del d\u00eda suelen darse a mediod\u00eda, y las m\u00e1s fr\u00edas de madrugada. Sabiendo esto, usar el valor m\u00e1s frecuente puede ser una buena aproximaci\u00f3n para rellenar los valores nulos.","96e6aac5":"> Disponemos de 62269 filas y 27 columnas\n\nVamos a comprobar si existen valores duplicados","0b09fd0c":"Una vez pasados los datos a num\u00e9ricos vamos a proseguir con la imputaci\u00f3n, primero de las variables num\u00e9ricas. Para este caso, lo ideal hubiera sido realizar la imputaci\u00f3n mediante KNN, ya que tiene sentido que la temperatura, precipitaciones o las rachas de los d\u00edas \"vecinos\" sea similar. No obstante , nos surge un peque\u00f1o problema con esto, y es el coste computacional, tarda demasiado debido a la gran cantidad de filas de este dataset, por lo tanto, vamos a optar por una estrategia m\u00e1s sencilla pero menos eficaz como es rellenar con el valor medio de cada columna.","07330ad3":"> \u00bfC\u00faales son las zonas con m\u00e1s casos acumulados?","1b15b6cb":"> Vemos que hay comunidades para los que obtiene buenos resultados, como Madrid y otras como Navarra en la que no es muy efectiva, muy probablemente debido a la aparici\u00f3n de outliers.","b9137802":"> Analicemos los resultados, centr\u00e1ndonos en la variable de casos. Si observamos como es la correlaci\u00f3n de la poblaci\u00f3n con el n\u00ba de casos respecto a los grupos que hemos hecho, encontramos algo un poco sorprendente \u00a1El grupo 1 tiene la mayor correlaci\u00f3n de casos con habitantes! Quiz\u00e1 los habitantes de los pueblos peque\u00f1os se conf\u00eden m\u00e1s y tomen menos precauciones. Respecto a la altitud, no parece ser especialmente significativa en ninguno de los grupos. Las temperaturas tambi\u00e9n mantienen la misma correlaci\u00f3n m\u00e1s o menos en todos lo grupos. La velocidad media y las rachas de viento, si que parece que tenga cierto efecto sobre los casos y vemos que var\u00eda en funci\u00f3n del grupo, siendo los grupos con menos habitantes a los que m\u00e1s les afecta. Quiz\u00e1 al ser lugares m\u00e1s peque\u00f1os hay menos edificios y corra m\u00e1s el aire, permitiendo tener una mejor ventilaci\u00f3n y que afecte a la reducci\u00f3n de los casos.","aa0d5f4e":"> Para responder esta pregunta se ha utilizado un *barplot*, siendo otra forma visualmente muy sencilla de identificar la respuesta a la pregunta que hemos planteado. Para realizar esto, hemos tenido que agrupar los datos por provincia y sumar los casos, y posteriormente dividirlo entre la poblaci\u00f3n de la provincia.","b7fd9d3f":"## Visi\u00f3n general de los datos: *limpieza, imputaci\u00f3n, selecci\u00f3n\/extracci\u00f3n de caracter\u00edsticas*\n\nEl primer paso, una vez cargado el dataset, va a ser ver una peque\u00f1a muestra de lo que contiene, as\u00ed nos haremos una idea de con que datos vamos a trabajar. Y ya que estamos, comprobaremos tambi\u00e9n la cantidad de filas y columnas de las que disponemos.","b95481d0":"> \u00bfCuales han sido las zonas m\u00e1s fr\u00edas?\u00bfY m\u00e1s c\u00e1lidas?","1b8aa61e":"###\u00a0\u00bfEn qu\u00e9 provincias ha habido m\u00e1s casos?","d3ba0d0f":"# Covid19 vs Climatolog\u00eda en Espa\u00f1a\nEl an\u00e1lisis de datos tiene como objetivo dar respuesta a preguntas a las cuales no tenemos respuestas. Para ello, se exploran los datos en busca de esas respuestas e incluso nuevas que van apareciendo por el camino. En este notebook analizaremos un dataset que recoge los datos del covid en diversas comunidades aut\u00f3nomas espa\u00f1olas, y que adem\u00e1s incluye diversos datos climatol\u00f3gicos de esos lugares, como las temperaturas m\u00e1ximas y m\u00ednimas, las rachas de viento o las precipitaciones. Esto da cabida a poder realizar un interesante an\u00e1lisis exploratorio de los datos e intentar descubrir relaciones entre estos factores de la climatolog\u00eda y una posible influencia la expansi\u00f3n del covid, para responder a esta pregunta indagaremos en los datos, primero de una forma m\u00e1s general, y luego de una forma m\u00e1s espec\u00edfica centr\u00e1ndonos en aspectos que puedan dar con las respuestas que vamos buscando.\n\nFuente de los datos: https:\/\/wake.dlsi.ua.es\/datahub\/covid-detail.html\n\n**Nota:** Est\u00e9 notebook es una pr\u00e1ctica de la asignatura Miner\u00eda de datos, del m\u00e1ster en ciencia de datos de la Universidad de Alicante. Por tanto, su contenido se adec\u00faa a los objetivos de esta pr\u00e1ctica.","958dd30f":"> Los resultados arrojan la similitud en la forma que siguen las comunidades de Navarra y Catalunya o Euskadi y la Comunidad valenciana, Madrid, sin embargo, no se parece tanto. Tambi\u00e9n es debido a que no se dispone de la misma cantidad de datos en todas las comunidades.","bc829233":"# Conclusi\u00f3n\n\n> Hemos hecho una exploraci\u00f3n de los datos que nos ha dado una visi\u00f3n general de los mismos. Posteriormente, nos hemos centrado en responder algunas preguntas que nos hab\u00edamos planteado y otras que podr\u00edan ir surgiendo por el camino, ha habido otras hemos tenido dificultades para contestarlas, y otras muchas ni siquiera se plantearon. Cada dataset es un mundo por descubrir, conteniendo m\u00e1s informaci\u00f3n de la que puede parecer a simple vista. Nuestra pregunta principal \u00bfExiste una relaci\u00f3n de los casos de covid con la climatolog\u00eda? No ha podido ser claramente respuesta, por los problemas comentados anteiormente y la fiabilidad de los datos. No obstante, hemos podido responder a otras preguntas y visto las comunidades m\u00e1s afectadas. Tambi\u00e9n nos las hemos ingeniado para \"rellenar\" los datos que nos faltaban, incluso nos hemos atrevido a predecir mediante el modelo ARIMA. Esta claro que a\u00fan se podr\u00eda trabajar mucho m\u00e1s este dataset, pero por el momento ya hemos ido descubriendo alguno de sus secretos.","02d20be4":"### \u00bfC\u00f3mo han evolucionado las temperaturas a lo largo del a\u00f1o?\nOtra cosa que podemos hacer es analizar temporalmente como han variado las temperaturas durante el a\u00f1o. Vamos a verlo separandolo por comunidad aut\u00f3noma.","1a60b291":"###\u00a0Evoluci\u00f3n de los casos de covid con media movil","f46198c5":"## Carga de datos y configuraci\u00f3n"}}