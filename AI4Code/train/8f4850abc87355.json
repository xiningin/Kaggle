{"cell_type":{"8f8110bc":"code","7596fa69":"code","726f9dcf":"code","e46ed7a9":"code","a7906a90":"code","362947a9":"code","c6207d33":"code","4f3e4570":"code","c9e5c6e7":"code","d0b1567b":"code","b49adbd5":"code","9e2fee54":"code","9ff45126":"code","c8006a39":"code","55477ce3":"code","1e7f970f":"code","a067c8b0":"code","099dc307":"code","100c3f20":"code","2d0cf434":"code","2e6d3050":"code","7db0ce5f":"code","09235c4c":"code","891a129b":"code","bff78214":"code","4345ef57":"code","7fb2e24e":"code","c81fa4b1":"code","df95dd0d":"code","46bcf6af":"code","392233a6":"code","8339bf05":"code","08f3bb03":"code","9b9d81fa":"code","521ac48a":"code","80af06fe":"code","1e23b605":"code","80cb9dca":"code","dbd255c3":"code","0f77d5a2":"code","077bb316":"code","d7f14993":"code","c3ecaf99":"code","50597e36":"code","8c0bee23":"code","e4c532c2":"code","0cd9e356":"markdown","37344c60":"markdown","ee67574d":"markdown","9c940982":"markdown","107332b0":"markdown","d0fcf478":"markdown","bbe90b10":"markdown","04d5b98f":"markdown","682d37d5":"markdown","a1509901":"markdown","d6278949":"markdown","5b6ae012":"markdown","ff240da5":"markdown"},"source":{"8f8110bc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","7596fa69":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom nltk.corpus import stopwords\nfrom nltk.sentiment import SentimentAnalyzer\nfrom nltk.corpus import subjectivity\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.sentiment.util import *\nimport string\n%matplotlib inline\n","726f9dcf":"# Read in the data\ndf = pd.read_csv('..\/input\/reddit-wallstreetsbets-posts\/reddit_wsb.csv')","e46ed7a9":"df.info()","a7906a90":"df.head()","362947a9":"# Lets find out the lengths of the messages\ndf['length_title'] = df['title'].apply(len)\nfor i in range(0, len(df)):\n    df['body'][i] = str(df['body'][i])\ndf['length_body'] = df['body'].apply(len)","c6207d33":"print(\"The max length of title is {}\".format(df['length_title'].max()))\nprint(\"The min length of title is {}\".format(df['length_title'].min()))\nprint(\"The mode length of title is {}\".format(df['length_title'].mode()))\nprint(\"The max length of body is {}\".format(df['length_body'].max()))\nprint(\"The min length of body is {}\".format(df['length_body'].min()))\nprint(\"The mode length of body is {}\".format(df['length_body'].mode()))","4f3e4570":"plt.figure(figsize=(15,10))\nplt.title('Title Text Lengths > 100')\nsns.countplot(df[df['length_title'] > 100]['length_title'])","c9e5c6e7":"# Gets rid of punctuation\ndef text_clean(mess):\n    nopunct = [char for char in mess if char not in string.punctuation]\n    nopunct = ''.join(nopunct)\n    return nopunct","d0b1567b":"title_text = text_clean(df['title'])","b49adbd5":"body_text = text_clean(df['body'])","9e2fee54":"len(title_text)","9ff45126":"len(body_text)","c8006a39":"# Words to not include in Word Clouds\nmy_stopwords = set(STOPWORDS)\n# Additional phrases I don't want picked up in word clouds\nmy_stopwords.update(['https', 'http', '\\n', '\\t'])","55477ce3":"#Creating Title Word Cloud\ntitle_wc = WordCloud(stopwords = my_stopwords, background_color='white', collocations = False).generate(title_text)\nplt.figure(figsize=(12,10))\nplt.imshow(title_wc, interpolation='bilinear')\nplt.title('Most common words used in WSB Title', fontsize = 20)\nplt.axis('off')","1e7f970f":"body_wc = WordCloud(stopwords = my_stopwords, background_color='white', collocations = False).generate(body_text)\nplt.figure(figsize=(12,10))\nplt.imshow(body_wc, interpolation='bilinear')\nplt.title('Most common words used in WSB Body', fontsize = 20)\nplt.axis('off')","a067c8b0":"nltk.download('punkt')\nnltk.download('vader_lexicon')","099dc307":"# Sentiment Analyzer\nanalyzer = SentimentIntensityAnalyzer()","100c3f20":"# Will generate a score in the format: {'neg': 0.0, 'neu': 0.425, 'pos': 0.575, 'compound': 0.8877} \ndef sentiment_analyzer_scores(sentence):\n    score = analyzer.polarity_scores(sentence)\n    if score['compound'] > 0.05:\n        return str('positive')\n    elif score['compound'] < -0.05:\n        return str('negative')\n    else:\n        return str('neutral')","2d0cf434":"# Lets create a function to create a list of sentiment values \n\ndef sentiment_append(text):\n    moods = []\n    for mess in text:\n        mood = sentiment_analyzer_scores(mess)\n        moods.append(mood)\n    return moods\n    ","2e6d3050":"# Add title moods to data frame\ntitle_moods = sentiment_append(df['title'])\ndf['title_sentiment'] = title_moods","7db0ce5f":"# Lets find out how many of each sentiment there are in the titles text\n\ndf['title_sentiment'].value_counts()","09235c4c":"# Lets Visualize the Number of Each Sentiment\n\nplt.figure(figsize = (12,10))\nsns.countplot(df['title_sentiment'])\nplt.title(\"Number of Each Sentiment in Title\", fontsize = 20)\nplt.show()","891a129b":"# Sentiment Analyzer\nanalyzer = SentimentIntensityAnalyzer()","bff78214":"# Will generate a score in the format: {'neg': 0.0, 'neu': 0.425, 'pos': 0.575, 'compound': 0.8877} \ndef sentiment_analyzer_scores(sentence):\n    score = analyzer.polarity_scores(sentence)\n    if score['compound'] > 0.05:\n        return str('positive')\n    elif score['compound'] < -0.05:\n        return str('negative')\n    else:\n        return str('neutral')","4345ef57":"# Lets create a function to create a list of sentiment values \n\ndef sentiment_append(text):\n    moods = []\n    for mess in text:\n        mood = sentiment_analyzer_scores(mess)\n        moods.append(mood)\n    return moods\n    ","7fb2e24e":"# Add body  moods to data frame\nbody_moods = sentiment_append(df['body'])\ndf['body_sentiment'] = body_moods","c81fa4b1":"# Lets find out how many of each sentiment there are in the titles text\ndf['body_sentiment'].value_counts()","df95dd0d":"# Lets Visualize the Number of Each Sentiment\n\nplt.figure(figsize = (12,10))\nsns.countplot(df['body_sentiment'])\nplt.title(\"Number of Each Sentiment in Body\", fontsize = 20)\nplt.show()","46bcf6af":"# Sentiment Correlation\n\nplt.figure(figsize=(12,10))\nsns.heatmap(df.groupby('title_sentiment').corr(), cmap = 'viridis')\nplt.title(\"Title Correlation\")","392233a6":"plt.figure(figsize=(12,10))\nsns.heatmap(df.groupby('body_sentiment').corr(), cmap = 'viridis')\nplt.title(\"Body Correlation\")","8339bf05":"# Creating a positive title \npos_title_df = pd.DataFrame(columns=['positive_sentiment_text'])","08f3bb03":"# Extracting just the positive text titles \npos_list = []\nfor i in range(0, len(df)):\n    if df['title_sentiment'][i] == 'positive':\n        pos_list.append(df['title'][i])","9b9d81fa":"# Adding positive text messages to data frame\npos_title_df['positive_sentiment_text'] = pos_list","521ac48a":"# Creating Clean Text of Positive Title's\npos_title_text = text_clean(pos_title_df['positive_sentiment_text'])","80af06fe":"#Creating Positive Title Word Cloud\npos_title_wc = WordCloud(stopwords = my_stopwords, background_color='white', collocations = False).generate(pos_title_text)\nplt.figure(figsize=(12,10))\nplt.imshow(pos_title_wc, interpolation='bilinear')\nplt.title('Most common words used in Positive WSB Title', fontsize = 20)\nplt.axis('off')","1e23b605":"# Creating a Negative title \nneg_title_df = pd.DataFrame(columns=['neg_sentiment_text'])","80cb9dca":"# Extracting just the negative text titles \nneg_list = []\nfor i in range(0, len(df)):\n    if df['title_sentiment'][i] == 'negative':\n        neg_list.append(df['title'][i])","dbd255c3":"# Adding negative text messages to data frame\nneg_title_df['negative_sentiment_text'] = neg_list","0f77d5a2":"# Creating Clean Text of Negative Title's\nneg_title_text = text_clean(neg_title_df['negative_sentiment_text'])","077bb316":"#Creating Negative Title Word Cloud\nneg_title_wc = WordCloud(stopwords = my_stopwords, background_color='white', collocations = False).generate(neg_title_text)\nplt.figure(figsize=(12,10))\nplt.imshow(neg_title_wc, interpolation='bilinear')\nplt.title('Most common words used in Negative WSB Title', fontsize = 20)\nplt.axis('off')","d7f14993":"# Creating a Neutral title \nneutral_title_df = pd.DataFrame(columns=['neutral_sentiment_text'])","c3ecaf99":"# Extracting just the neutral text titles \nneu_list = []\nfor i in range(0, len(df)):\n    if df['title_sentiment'][i] == 'neutral':\n        neu_list.append(df['title'][i])","50597e36":"# Adding neutral text messages to data frame\nneutral_title_df['neutral_sentiment_text'] = neu_list","8c0bee23":"# Creating Clean Text of Neutral Title's\nneutral_title_text = text_clean(neutral_title_df['neutral_sentiment_text'])","e4c532c2":"#Creating Neutral Title Word Cloud\nneutral_title_wc = WordCloud(stopwords = my_stopwords, background_color='white', collocations = False).generate(neutral_title_text)\nplt.figure(figsize=(12,10))\nplt.imshow(neutral_title_wc, interpolation='bilinear')\nplt.title('Most common words used in Neutral WSB Title', fontsize = 20)\nplt.axis('off')","0cd9e356":"There is a much larger number of Neutral Body text messages thatn Positive and Negative text messages. A trend towards more neutral messages can be seen based on Body text messages and Title text messages. The majority of neutral values may come from nan values in the data. A more reliable distribution between Positive, Neutral, and Negative values can be made by replacing nan values or getting rid of them all together and comparing.","37344c60":"GME, AMC, and Robinhood all seem to be extremely common words amongst WSB Titles no matter the sentiment.","ee67574d":"# Positive Title Word Clouds ","9c940982":"# Body Text Sentiment Tracker","107332b0":"There is a much larger amount of Neutral titles than Positive and Negative titles","d0fcf478":"# Sentiment Correlation","bbe90b10":"# Negative Title Word Clouds","04d5b98f":"# Thank You!!!","682d37d5":"# Neutral Title Word Clouds ","a1509901":"# Generating Word Clouds","d6278949":"# Title Text Sentiment Tracker","5b6ae012":"# Let Me Know What You Think!","ff240da5":"# Text Cleaning"}}