{"cell_type":{"7b909932":"code","7cd918ce":"code","d4ca4be3":"code","7110b34e":"code","a5f3fea7":"code","085d4ac4":"code","26615d33":"code","aeb77907":"code","b753d39f":"code","d914b807":"code","2a24555f":"code","b76de075":"code","9c8ffb8f":"code","ee2e4abb":"code","c00abe50":"code","aed07c96":"code","c9507dc8":"code","94927b6b":"code","bed073b0":"code","3c72dbc8":"markdown","3affb843":"markdown","3fa5b689":"markdown","0ebe82eb":"markdown","0cc57c8f":"markdown","1080d142":"markdown"},"source":{"7b909932":"# Kaggle's setup code\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7cd918ce":"#Machine learning packages sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import linear_model","d4ca4be3":"#loads Google's historical data\ngoog_data = pd.read_csv(\"..\/input\/nasdaq-and-nyse-stocks-histories\/full_history\/GOOG.csv\")\n\n#loads Apple's historical data\nindex_symbol = \"AAPL\"\naapl_data = pd.read_csv(\"..\/input\/nasdaq-and-nyse-stocks-histories\/full_history\/\" + index_symbol + \".csv\")\n\n# Reverse order so most recent data must be predicted\naapl_data = aapl_data.sort_index(axis=0 ,ascending=False)\n\n# One way to turn a string into the datetime datatype\n# aapl_data['date'] = aapl_data[\"date\"].astype('datetime64')\n\n# This way is better because it deals with errors and allows you to specify the format\naapl_data[\"date\"] = pd.to_datetime(aapl_data[\"date\"], format = \"%Y-%m-%d\", errors = \"coerce\")\n\nprint(aapl_data.info()) #everything is numerical except for the date, which is now a datetime instead of a string\naapl_data.head(100)","7110b34e":"# Good to check, if there are Nan values I need to deal with them before using a ml model\naapl_data.isnull().sum()\naapl_data[\"date\"].describe(datetime_is_numeric=True)","a5f3fea7":"\n# Orginally I used pd.to_datetime(), but because the data type was changed above its already a datetime\ndates = aapl_data[\"date\"]\n\n# Changes size of graph: (width, height) | Must be called before plt.plot()\nplt.figure(figsize=(20,7))\n\n# Simple plot of apple's close price over its entire history\ngraph = plt.plot(dates, aapl_data[\"close\"], label = \"Apple Stock Close Price\")\n#graph = plt.plot(dates, aapl_data[\"open\"], label = \"Apple Stock Open Price\")\n\ngraph = plt.plot(pd.to_datetime(goog_data[\"date\"]), goog_data[\"close\"], label = \"Google Stock Close Price\")\n\nplt.legend(loc = 'upper left')\nplt.show()","085d4ac4":"# Date values are  datetime data, needs to be numerical for the ML Model\n# First I tried switching to ordinal data, but that doesn't allow the model to compare days of week, months of the year, etc.\n# Pandas datetime actually has a lot of functionaliy that will allow me to easily seperate the data into year, month, week, day, etc.\n\nfrom datetime import datetime as dt\n\n# The year as an int\naapl_data[\"date_year\"] = aapl_data[\"date\"].dt.year\n\n# Month values (1-12)\naapl_data[\"date_month\"] = aapl_data[\"date\"].dt.month\n\n# Regular Series.dt.week is deprecated, thats why this one is slightly different.  (1-53)\naapl_data[\"date_week\"] = aapl_data[\"date\"].dt.isocalendar().week\n\n# Day values for each month (1-31)\naapl_data[\"date_day\"] = aapl_data[\"date\"].dt.day\n\n# Day values from 1 to 366 (if it's a leap year)\naapl_data[\"date_day_of_year\"] = aapl_data[\"date\"].dt.dayofyear\n\n# Assigns days values of 0-6 for Monday-Sunday\naapl_data[\"date_day_of_week\"] = aapl_data[\"date\"].dt.dayofweek\n\n# we now have pure numerical data to feed into the ML algorithim!\nprint(aapl_data.info())\naapl_data.describe()","26615d33":"# This is the label we are trying to predict\naapl_y = aapl_data[\"close\"]\n\n\n# For now I will only use the opening price, year, and day of the year (1-366)\naapl_X = aapl_data.drop(columns=[\"close\", \"date\", \"high\", \"low\", \"adjclose\", \"volume\", \"date_month\", \"date_week\", \"date_day\", \"date_day_of_week\"])","aeb77907":"# Split the data\nX_train, X_test, y_train, y_test = train_test_split(aapl_X, aapl_y, test_size = .3, shuffle = False)\n\n# The features in the training data\nX_train.head()","b753d39f":"# Simple stats on the label in the training data (close price)\ny_train.describe()","d914b807":"#Intializes the model and fits it to the training data\nmodel = linear_model.Lasso(alpha=.1)\nmodel.fit(X_train, y_train)","2a24555f":"# Predicts using the test data\ny_pred = model.predict(X_test)","b76de075":"# Defines an evaluation function to display metrics on the model\n\nimport math\nfrom sklearn.metrics import mean_squared_error, r2_score\ndef eval_metrics(y_test, y_pred):\n    print(\"Mean Squared Error: %.3f\" % mean_squared_error(y_test, y_pred))\n    print(\"Root Mean Squared Error: %.3f\" % math.sqrt(mean_squared_error(y_test, y_pred)))\n    print(\"R Score: %.4f\" % math.sqrt(abs(r2_score(y_test, y_pred))))\n    print(\"R^2 Score: %.4f\" % r2_score(y_test, y_pred))","9c8ffb8f":"#Cool way to turn the year and day of year back into a datetime value for graphing\naapl_X_test_graph_date = pd.to_datetime(X_test['date_year'] * 1000 + X_test['date_day_of_year'], format='%Y%j')","ee2e4abb":"#Plot of the results\n\nplt.figure(figsize=(20,12))\n\n\ngraph = plt.plot(aapl_X_test_graph_date, y_test, label = \"Acutal Values\")\ngraph = plt.scatter(aapl_X_test_graph_date, y_pred, s=1, c= \"orange\", label = \"Predicted Values\")\nplt.legend()\n#plt.margins(0,0)\nplt.show()\neval_metrics(y_test,y_pred)","c00abe50":"# determine correlation between each variable\n# 'pearson' is the standard correlation coefficient method\ngoog_data.corr(method='pearson')\n\n# the correlation between close and open is 0.999902, which is insane.  Acutally all the stock price data has extremely high correlations\n","aed07c96":"plt.figure(figsize=(15,5))\n\n#Calculate r values for open and close stock data\naapl_r = math.sqrt(abs(r2_score(aapl_data[\"open\"], aapl_data[\"close\"])))\ngoog_r = math.sqrt(abs(r2_score(goog_data[\"open\"], goog_data[\"close\"])))\n\nprint(\"\\t\\tApple R Score: %.4f \\t\\t\\t\\t\\t\\tGoogle R Score: %.4f\" % (aapl_r, goog_r))\n\n#Plots for Apple and Google, they show the very strong correlation\n\nplt.subplot(1, 2, 1)\nplt.scatter(aapl_data[\"open\"], aapl_data[\"close\"], label=\"Apple open price vs close price\", s=1)\nplt.title(\"Apple Open vs. Close Price\")\nplt.xlabel('Open Price')\nplt.ylabel('Close Price')\n\nplt.subplot(1, 2,2)\nplt.scatter(goog_data[\"open\"], goog_data[\"close\"], color=\"orange\", label=\"Google open price vs close price\", s=1)\nplt.title(\"Google Open vs. Close Price\")\nplt.xlabel('Open Price')\nplt.ylabel('Close Price')\nplt.show()\n\n# I checked about 5 other stocks, they all have a really high corrleation between open price and close price","c9507dc8":"plt.figure(figsize=(20,7))\n\nplt.plot(dates, aapl_data['close'], linewidth=1)\nplt.scatter(dates, aapl_data['open'], s=1, c=\"orange\")\nplt.title(\"Open and Close Price plotted by date\")\nplt.show()","94927b6b":"# Runs the model again on about 250 days so we can see a closer view of the predictions\n\n# Split the data\n\nX_train, X_test, y_train, y_test = train_test_split(aapl_X[9300:9556], aapl_y[9300:9556], test_size = .3, shuffle = False)\n\nmodel = linear_model.Lasso(alpha=.1)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\n\n#Plot of the results\n\n\naapl_X_test_graph_date = pd.to_datetime(X_test['date_year'] * 1000 + X_test['date_day_of_year'], format='%Y%j')\n\nplt.figure(figsize=(20,8))\n\n\n\ngraph = plt.plot(aapl_X_test_graph_date, y_test, label = \"Actual Values\")\ngraph = plt.scatter(aapl_X_test_graph_date, y_pred, s=5, c= \"orange\", label = \"Predicted Values\")\nplt.legend()\n#plt.margins(0,0)\nplt.show()\neval_metrics(y_test,y_pred)\n","bed073b0":"# Runs the model again without any stock price as a feature\n\naapl_X = aapl_data.drop(columns=[\"close\", \"date\", \"high\", \"low\", \"adjclose\", \"open\"])\naapl_y = aapl_data[\"close\"]\n\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(aapl_X, aapl_y, test_size = .3, shuffle = True)\n\n\n# Train the model and generate predictions\nmodel = linear_model.Lasso(alpha=.1)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\n\n\n#Plot of the results\naapl_X_test_graph_date = pd.to_datetime(X_test['date_year'] * 1000 + X_test['date_day_of_year'], format='%Y%j')\n\nplt.figure(figsize=(15,8))\n\n\n\ngraph = plt.scatter(aapl_X_test_graph_date, y_test, label = \"Actual Values\", s=2)\ngraph = plt.scatter(aapl_X_test_graph_date, y_pred, s=2, c= \"orange\", label = \"Predicted Values\")\nplt.legend()\n#plt.margins(0,0)\nplt.show()\neval_metrics(y_test,y_pred)\n\n","3c72dbc8":"# What does this mean?\nThis model has an R score of 0.9992, which is a near perfect fit.  There is definitley an issue in the model or data that has led to these **insanely** accurate predictions.  I will have to spend time testing in the future to see why this has happened.  Below are a few ideas.\n\n1. Most likely, the model has overfit the data which could also be why the model performs worse in more variability (2018 vs 2008 in the graph above).\n2. The model is **heavily** reliant on the open price.  Because the open price and close price are almost always pretty close, the model can easily predict the close price with the open price.\n3. The model might have \"look ahead bias\".  This would mean the model has a feature which wouldn't be known at the time of prediction.\n\n\nI'm not sure exactly why the model performed so well, but it is very likely something is giving the false impression of predictive prowess.  I will have to see...","3affb843":"# More realistic model\nBy not including any stock price data, and only using volume and the date data we got a more realistic model.  The r value is still pretty high, which is a little strange, but the Root Mean Squared Error (RMSE) is 10x as large.  While the target value for the RSME changes based on the context, you generally want it very low, close to <1.\n\n## Next Steps\n1. I'm still skeptical about how the open and close price is highly correlated.  Mostly it seems to result in unrealisitcally good models, but I thought ML models would have to use the stock prices for grounded prediction.  It might just be R and RMSE are too broad for stocks where minute fluctuations have serious consqueneces.  Regardless, I will spend a little more time trying to learn about this.\n2. The last model (above) that didn't use any stock prices as a feature struggled to result in accurate predictions. The solution to this will probably hinge on two things: better data and a better ML model approach.\n\n    * Better data was the original goal of my Independent study (combining companies finnancial statments, news article sentiment, etc.).\n    * Better algorithims is something I didn't previously consider.  However the graph above shows how linear regression doesn't do a great job. I could explore both neural networks and a new approach I found called LSTM to improve results.  Alternativley, its possible to do more feature engineering to introduce non-linearity into a linear regression model.","3fa5b689":"# 9\/28 and 9\/30 Analysis\nAfter a closer look at the data, it appears the problem is related to the high r value...","0ebe82eb":"# Strong Correlation between features and the target label\n\n\nIt appears that the high correlation between the stock prices (close vs. open vs. high vs. low) is throwing things off.  I'm not exactly sure about the math\/theory behind it, but I will test the theory by removing stock price data from the features.\n\nBelow is a model without any stock price data as a feature.\n","0cc57c8f":"The hidden code is very similar to the original model.  Except I only use ~250 dates and plot predicted values as points.  This results in the \"zoomed in\" view for the graph below.","1080d142":"# Apple Stock Machine Learning Model\nIn this project, I built my first machine learning model using a simple linear regression model called Lasso.  I further explored the capabilities of pandas specifically the datetime library and plotting with matplotlib.  I also used sklearn for the machine learning functionality."}}