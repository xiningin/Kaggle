{"cell_type":{"6efcff6f":"code","738574ca":"code","bdfc5388":"code","285db287":"code","993ef4cc":"code","b2a5a15d":"code","59e41bd6":"code","b1e5b166":"code","91acf3b9":"code","5f17ef1f":"code","25c0e58a":"code","db960737":"code","727f3bb3":"code","a614ac2c":"code","47119648":"code","4c7961d7":"code","60d538ee":"code","e25c4dff":"code","b48bdf2a":"code","43c5e651":"code","6e0f3562":"code","5443d050":"code","4b65b233":"code","2dd877c8":"code","e97119a9":"code","66a8d29b":"code","5f23671e":"code","48e6e71a":"markdown","7c97f45c":"markdown","661cb2b9":"markdown","d8852cb2":"markdown","839936b6":"markdown","f4df7530":"markdown","b34b61a6":"markdown","42b59169":"markdown","5c0d79c2":"markdown","8717f654":"markdown"},"source":{"6efcff6f":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use(\"seaborn-whitegrid\")\nimport matplotlib.image as implt\nfrom PIL import Image \nimport seaborn as sns\nimport cv2 as cs2\nimport os\n\nimport warnings\nwarnings.filterwarnings('ignore')","738574ca":"## import Keras and its module for image processing and model building\nimport keras\nfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.layers import Activation, Dropout, Flatten, Dense, BatchNormalization","bdfc5388":"#copying the pretrained models to the cache directory\ncache_dir = os.path.expanduser(os.path.join('~', '.keras'))\nif not os.path.exists(cache_dir):\n    os.makedirs(cache_dir)\nmodels_dir = os.path.join(cache_dir, 'models')\nif not os.path.exists(models_dir):\n    os.makedirs(models_dir)\n\n#copy the Xception models\n!cp ..\/input\/keras-pretrained-models\/xception* ~\/.keras\/models\/\n#show\n!ls ~\/.keras\/models","285db287":"train_path = \"..\/input\/twoclass-weather-classification\/train\"\ntest_path = \"..\/input\/twoclass-weather-classification\/test\"\n\ntrain_cloudy = \"\/kaggle\/input\/twoclass-weather-classification\/train\/cloudy\"\ntrain_sunny = \"\/kaggle\/input\/twoclass-weather-classification\/train\/sunny\"\n\ntest_cloudy = \"\/kaggle\/input\/twoclass-weather-classification\/test\/cloudy\"\ntest_sunny = \"\/kaggle\/input\/twoclass-weather-classification\/test\/sunny\"","993ef4cc":"# VISUALIZATION\ncategory_names = os.listdir(train_path) # output: ['sunny', 'cloudy']\nnb_categories = len(category_names) # output: 2\ntrain_images = []\n\nfor category in category_names:\n    folder = train_path + \"\/\" + category\n    train_images.append(len(os.listdir(folder)))\n\nsns.barplot(y=category_names, x=train_images).set_title(\"Number Of Training Images Per Category\");","b2a5a15d":"img = load_img('..\/input\/twoclass-weather-classification\/train\/cloudy\/c0001.jpg')  # this is a PIL image\nx = img_to_array(img)  # this is a Numpy array \nprint('image shape: ', x.shape)\n\nprint('Train Cloudy Image')\nplt.imshow(img)\nplt.show()\n\n\nimg = load_img('..\/input\/twoclass-weather-classification\/train\/sunny\/s0001.jpg')  # this is a PIL image\nx = img_to_array(img)  # this is a Numpy array \nprint('Train Sunny Image')\nplt.imshow(img)\nplt.show()\n\n","59e41bd6":"img_size = 50\ncloudy_train = []\nsunny_train = []\nlabel = []\n\nfor i in os.listdir(train_cloudy): # all train cloudy images\n    if os.path.isfile(train_path + \"\/cloudy\/\" + i): # check image in file\n        cloudy = Image.open(train_path + \"\/cloudy\/\" + i).convert(\"L\") # converting grey scale \n        cloudy = cloudy.resize((img_size,img_size), Image.ANTIALIAS) # resizing to 50,50\n        cloudy = np.asarray(cloudy)\/255 # bit format\n        cloudy_train.append(cloudy)\n        label.append(1)\n        \nfor i in os.listdir(train_sunny): # all train sunny images\n    if os.path.isfile(train_path + \"\/sunny\/\" + i): # check image in file\n        sunny = Image.open(train_path + \"\/sunny\/\" + i).convert(\"L\") # converting grey scale \n        sunny = sunny.resize((img_size,img_size), Image.ANTIALIAS) # resizing to 50,50\n        sunny = np.asarray(sunny)\/255 # bit format\n        sunny_train.append(sunny)\n        label.append(0)","b1e5b166":"x_train = np.concatenate((cloudy_train,sunny_train),axis=0) # training dataset\nx_train_label = np.asarray(label) # label array containing 0 and 1\nx_train_label = x_train_label.reshape(x_train_label.shape[0],1)\n\nprint(\"cloudy:\",np.shape(cloudy_train) , \"sunny:\",np.shape(sunny_train))\nprint(\"train_dataset:\",np.shape(x_train), \"train_values:\",np.shape(x_train_label))","91acf3b9":"# Visualizing Training data\nprint(x_train_label[0])\nplt.imshow(cloudy_train[0])","5f17ef1f":"# Visualizing Training data\nprint(x_train_label[0])\nplt.imshow(sunny_train[0])","25c0e58a":"img_size = 50\ncloudy_test = []\nsunny_test = []\nlabel = []\n\n\nfor i in os.listdir(test_cloudy): # all train cloudy images\n    if os.path.isfile(test_path + \"\/cloudy\/\" + i): # check image in file\n        cloudy = Image.open(test_path + \"\/cloudy\/\" + i).convert(\"L\") # converting grey scale \n        cloudy = cloudy.resize((img_size,img_size), Image.ANTIALIAS) # resizing to 50,50\n        cloudy = np.asarray(cloudy)\/255 # bit format\n        cloudy_test.append(cloudy)\n        label.append(1)\n        \nfor i in os.listdir(test_sunny): # all train sunny images\n    if os.path.isfile(test_path + \"\/sunny\/\" + i): # check image in file\n        sunny = Image.open(test_path + \"\/sunny\/\" + i).convert(\"L\") # converting grey scale \n        sunny = sunny.resize((img_size,img_size), Image.ANTIALIAS) # resizing to 50,50\n        sunny = np.asarray(sunny)\/255 # bit format\n        sunny_test.append(sunny)\n        label.append(0)","db960737":"x_test = np.concatenate((cloudy_test,sunny_test),axis=0) # training dataset\nx_test_label = np.asarray(label) # label array containing 0 and 1\nx_test_label = x_test_label.reshape(x_test_label.shape[0],1)\n\nprint(\"cloudy:\",np.shape(cloudy_test) , \"sunny:\",np.shape(sunny_test))\nprint(\"test_dataset:\",np.shape(x_test), \"test_values:\",np.shape(x_test_label))","727f3bb3":"# Visualizing Training data\nprint(x_test_label[0])\nplt.imshow(cloudy_test[0])","a614ac2c":"# Visualizing Training data\nprint(x_test_label[0])\nplt.imshow(sunny_test[0])","47119648":"x = np.concatenate((x_train,x_test),axis=0) # count: train_data\n# x.shape: \n#   output \ny = np.concatenate((x_train_label,x_test_label),axis=0) # count: test_data\nx = x.reshape(x.shape[0],x.shape[1]*x.shape[2]) # flatten 3D image array to 2D, count: 50*50 = 2500\nprint(\"images:\",np.shape(x), \"labels:\",np.shape(y))","4c7961d7":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.15, random_state=42)\nnumber_of_train = X_train.shape[0]\nnumber_of_test = X_test.shape[0]\n\nprint(\"Train Number: \", number_of_train)\nprint(\"Test Number: \", number_of_test)","60d538ee":"x_train = X_train.T\nx_test = X_test.T\ny_train = y_train.T\ny_test = y_test.T\nprint(\"x train: \",x_train.shape)\nprint(\"x test: \",x_test.shape)\nprint(\"y train: \",y_train.shape)\nprint(\"y test: \",y_test.shape)","e25c4dff":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\nlogreg = LogisticRegression()\ntest_acc_logregsk = round(logreg.fit(x_train.T, y_train.T).score(x_test.T, y_test.T)* 100, 2)\ntrain_acc_logregsk = round(logreg.fit(x_train.T, y_train.T).score(x_train.T, y_train.T)* 100, 2)","b48bdf2a":"# with GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\n\ngrid = {\n    \"C\": np.logspace(-4, 4, 20),\n    \"penalty\": [\"l1\",\"l2\"]\n}\nlg=LogisticRegression(random_state=42)\nlog_reg_cv=GridSearchCV(lg,grid,cv=10,n_jobs=-1,verbose=2)\nlog_reg_cv.fit(x_train.T,y_train.T)\nprint(\"accuracy: \", log_reg_cv.best_score_)","43c5e651":"models = pd.DataFrame({\n    'Model': ['LR with sklearn','LR with GridSearchCV' ],\n    'Train Score': [train_acc_logregsk, \"-\"],\n    'Test Score': [test_acc_logregsk, log_reg_cv.best_score_*100]\n})\nmodels.sort_values(by='Test Score', ascending=False)","6e0f3562":"from sklearn.ensemble import BaggingClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC\n\n# We define the SVM model\nsvmcla = OneVsRestClassifier(BaggingClassifier(SVC(C=5,kernel='rbf',random_state=42, probability=True), \n                                               n_jobs=-1))\ntest_acc_svm = round(svmcla.fit(x_train.T, y_train.T).score(x_test.T, y_test.T)* 100, 2)\ntrain_acc_svm = round(svmcla.fit(x_train.T, y_train.T).score(x_train.T, y_train.T)* 100, 2)\n","5443d050":"model2 = pd.DataFrame({\n    'Model': ['SVM'],\n    'Train Score': [train_acc_svm],\n    'Test Score': [test_acc_svm*100]\n})\nmodel2.sort_values(by='Test Score', ascending=False)","4b65b233":"from sklearn.ensemble import RandomForestClassifier\n\n# We define the model\nrfcla = RandomForestClassifier(n_estimators=100,random_state=9,n_jobs=-1)\ntest_acc_rfcla = round(rfcla.fit(x_train.T, y_train.T).score(x_test.T, y_test.T)* 100, 2)\ntrain_acc_rfcla = round(rfcla.fit(x_train.T, y_train.T).score(x_train.T, y_train.T)* 100, 2)","2dd877c8":"model3 = pd.DataFrame({\n    'Model': ['Random Forest'],\n    'Train Score': [train_acc_rfcla],\n    'Test Score': [test_acc_rfcla*100]\n})\nmodel3.sort_values(by='Test Score', ascending=False)","e97119a9":"from sklearn.tree import DecisionTreeClassifier\n\n# We define the model\ndtcla =  DecisionTreeClassifier(random_state=9)\ntest_acc_dtcla = round(dtcla.fit(x_train.T, y_train.T).score(x_test.T, y_test.T)* 100, 2)\ntrain_acc_dtcla = round(dtcla.fit(x_train.T, y_train.T).score(x_train.T, y_train.T)* 100, 2)","66a8d29b":"model4 = pd.DataFrame({\n    'Model': ['Decision Tree'],\n    'Train Score': [train_acc_dtcla],\n    'Test Score': [test_acc_dtcla*100]\n})\nmodel4.sort_values(by='Test Score', ascending=False)","5f23671e":"model5 = pd.DataFrame({\n    'Model': ['train_acc_logregsk', 'train_acc_svm','train_acc_rfcla','Decision Tree'],\n    'Train Score': [train_acc_logregsk, train_acc_svm , train_acc_rfcla ,train_acc_dtcla],\n    'Test Score': [test_acc_logregsk, test_acc_svm , test_acc_rfcla ,test_acc_dtcla*100]\n})\nmodel5.sort_values(by='Test Score', ascending=False)","48e6e71a":"We combine image strings and tags and flatten 'x':","7c97f45c":"## 1. Logistic regression classification\n\nLogistic regression is a technique that can be applied to binary classification problems. This technique uses the logistic function or sigmoid function, which is an S-shaped curve that can assume any real value number and assign it to a value between 0 and 1, but never exactly in those limits. Thus, logistic regression models the probability of the default class (the probability that an input $(X)$ belongs to the default class $(Y=1)$) $(P(X)=P(Y=1|X))$. In order to make the prediction of the probability, the logistic function is used, which allows us to obtain the log-odds or the probit. Thus, the model is a linear combination of the inputs, but that this linear combination relates to the log-odds of the default class.\n\nStarted from make an instance of the model setting the default values. Specify the inverse of the regularization strength in 10. Trained the logistic regression model with the training data, and then applied such model to the test data.","661cb2b9":"Scaling down the train set and test set images:\n\n1. cloudy: (5000, 50, 50) sunny: (5000, 50, 50)\n1. train_dataset: (10000, 50, 50) train_values: (10000, 1)\n1. test_dataset: (253, 50, 50) test_values: (253, 1)\n1. label 1 for cloudy, and label 0 for sunny ","d8852cb2":"# Comparison of classification techniques","839936b6":"## 3. Random forest classification\n\nBased on the previous classification method, random forest is a supervised learning algorithm that creates a forest randomly. This forest, is a set of decision trees, most of the times trained with the bagging method. The essential idea of bagging is to average many noisy but approximately impartial models, and therefore reduce the variation. Each tree is constructed using the following algorithm:\n\n* Let $N$ be the number of test cases, $M$ is the number of variables in the classifier.\n* Let $m$ be the number of input variables to be used to determine the decision in a given node; $m<M$.\n* Choose a training set for this tree and use the rest of the test cases to estimate the error.\n* For each node of the tree, randomly choose $m$ variables on which to base the decision. Calculate the best partition of the training set from the $m$ variables.\n\nFor prediction a new case is pushed down the tree. Then it is assigned the label of the terminal node where it ends. This process is iterated by all the trees in the assembly, and the label that gets the most incidents is reported as the prediction. We define the number of trees in the forest in 100. ","f4df7530":"Next step, we need to determine the amount of data for train and test. You can modify test_size and see how it affects the accuracy. Let's split!","b34b61a6":"# Processing Dataset \nNow we need to modify images. The dataset contains different sizes of RGB color images. \n1. we should resize all the images, \n2. convert images to grayscale (only one dimension), while RBF image has three and helps you to avoid false classification and complexities.\n","42b59169":"## 2. SVM (Support Vector Machine) classification\n\nSVMs (Support Vector Machine) have shown a rapid proliferation during the last years. The learning problem setting for SVMs corresponds to a some unknown and nonlinear dependency (mapping, function) $y = f(x)$ between some high-dimensional input vector $x$ and scalar output $y$. It is noteworthy that there is no information on the joint probability functions, therefore, a free distribution learning must be carried out. The only information available is a training data set $D = {(x_i, y_i) \u2208 X\u00d7Y }, i = 1$, $l$, where $l$ stands for the number of the training data pairs and is therefore equal to the size of the training data set $D$, additionally, $y_i$ is denoted as $d_i$, where $d$ stands for a desired (target) value. Hence, SVMs belong to the supervised learning techniques.\n\nFrom the classification approach, the goal of SVM is to find a hyperplane in an N-dimensional space that clearly classifies the data points. Thus hyperplanes are decision boundaries that help classify the data points. Data points falling on either side of the hyperplane can be attributed to different classes.\n","5c0d79c2":"# 4. Decision tree classification\nA decision tree is a flowchart-like tree structure where an internal node represents feature, the branch represents a decision rule, and each leaf node represents the outcome. The decision tree analyzes a set of data to construct a set of rules or questions, which are used to predict a class, i.e., the goal of decision tree is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. In this sense the decision tree selects the best attribute using to divide the records, converting that attribute into a decision node and dividing the data set into smaller subsets, to finally start the construction of the tree repeating this process recursively.","8717f654":"Then we need to take transpose of all matrices. The purpose of it, quoted from here is: \"In python, it is often the case that transposing will enable you to have the data in a given shape that might make it easier to use whatever framework or algorithm\""}}