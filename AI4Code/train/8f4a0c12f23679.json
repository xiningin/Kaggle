{"cell_type":{"c0eeadea":"code","fffa28e9":"code","05e7e423":"code","0c6ec3e6":"code","0fbe1001":"code","1457a182":"code","3990455d":"code","c9fa792a":"code","f90eb64f":"code","31d1e799":"code","dba41b0c":"code","bba009a4":"code","e78eea88":"code","8296fac9":"code","4eec31bd":"code","3a975b72":"code","65c8da58":"code","480fa2c4":"code","21eaee13":"code","eacb34e6":"code","5cbf6ef7":"code","cbe8fa3d":"code","e5a280ca":"code","da84c425":"code","4439cae1":"code","e5eb756e":"code","32641e1b":"code","7edbb8b6":"code","66e6555e":"code","04ae6c3d":"code","c2ad905f":"code","7678d612":"code","1c114edc":"code","84c85da3":"code","322dba40":"code","30cef493":"code","7d214a73":"code","90e9c62d":"code","94c550c8":"code","8278726a":"code","ee9e5e5a":"code","3c11e352":"code","12339d8c":"code","36c96e1f":"markdown","b51de5cf":"markdown","1259ab18":"markdown","a224f581":"markdown","18c10bb2":"markdown","d0e258dd":"markdown","5e9a7c53":"markdown","ba47d0f0":"markdown","0283c5bd":"markdown","748a725d":"markdown","b9f5ab2e":"markdown","6de3be1a":"markdown","55c0e794":"markdown","f38de18c":"markdown","9baf7631":"markdown","1fa07111":"markdown","cf0f5551":"markdown","87b40fb7":"markdown","d970e76f":"markdown","c17f896b":"markdown","c277796c":"markdown","2d1da046":"markdown","163ade6a":"markdown","e31a62b4":"markdown","3be9dffb":"markdown","55a30c10":"markdown","d21bec87":"markdown","e0ce2149":"markdown","94043cec":"markdown","ad947644":"markdown","a5471e4c":"markdown","f96a3f82":"markdown","953fc419":"markdown","289e6690":"markdown","8553e886":"markdown","504f3029":"markdown","5cdedd6b":"markdown","c2a7e21e":"markdown","ad6d6d32":"markdown","57ade8b2":"markdown","a2d3e1b1":"markdown","3a42e98c":"markdown","b95a062b":"markdown"},"source":{"c0eeadea":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom colorama import Fore, Back, Style\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fffa28e9":"df = pd.read_csv('\/kaggle\/input\/factors-affecting-campus-placement\/Placement_Data_Full_Class.csv')","05e7e423":"df\ndf_copy = df.copy()","0c6ec3e6":"df.head()","0fbe1001":"df = df.set_index('sl_no')","1457a182":"df.info()","3990455d":"df.describe().transpose()","c9fa792a":"print(Fore.CYAN+'Total number of students',len(df['gender']))\nprint(Fore.CYAN+'Number of males:',len(df[df['gender']=='M']))\nprint(Fore.CYAN+'Number of females:',len(df[df['gender']=='F']))\nbox1 = sns.color_palette(['#0ead69','#003049'])\nsns.countplot(df['gender'],palette=box1)","f90eb64f":"figure, axes = plt.subplots(nrows=3, ncols=2,figsize=(8,10))\nswarmColor = sns.color_palette(['#da1e37','#0466c8'])\nsns.swarmplot(x='gender',y='ssc_p',data=df,ax=axes[0,0],palette=swarmColor)\nsns.swarmplot(x='gender',y='hsc_p',data=df,ax=axes[0,1],palette=swarmColor)\nsns.swarmplot(x='gender',y='degree_p',data=df,ax=axes[1,0],palette=swarmColor)\nsns.swarmplot(x='gender',y='etest_p',data=df,ax=axes[1,1],palette=swarmColor)\nsns.swarmplot(x='gender',y='mba_p',data=df,ax=axes[2,0],palette=swarmColor)\naxes[2,1].axis('off')\nfigure.tight_layout()","31d1e799":"figure, axes = plt.subplots(nrows=4, ncols=2,figsize=(10,12))\nbox2 = sns.color_palette(['#390099','#ff0054'])\nsns.countplot(hue='gender',x='ssc_b',data=df,ax=axes[0,0],palette=box2)\nsns.countplot(hue='gender',x='hsc_b',data=df,ax=axes[0,1],palette=box2)\nsns.countplot(hue='gender',x='degree_t',data=df,ax=axes[1,0],palette=box2)\nsns.countplot(hue='gender',x='workex',data=df,ax=axes[1,1],palette=box2)\nsns.countplot(hue='gender',x='specialisation',data=df,ax=axes[2,0],palette=box2)\nsns.countplot(hue='gender',x='status',data=df,ax=axes[2,1],palette=box2)\nsns.countplot(hue='gender',x='hsc_s',data=df,ax=axes[3,0],palette=box2)\naxes[3,1].axis('off')\nfigure.tight_layout()","dba41b0c":"print(Back.RED+'Remeber there are sum null values in  salary which we will cover in data processing.')\nplt.figure(figsize=(14,8))\nbox3 = sns.color_palette(['#00171f','#f7567c'])\nsns.boxenplot(x='gender',y='salary',data=df,palette=box3)","bba009a4":"forViolin = sns.color_palette(['#e29578','#ffddd2'])\nfigure, axes = plt.subplots(nrows=3, ncols=2,figsize=(12,14))\nsns.violinplot(x='ssc_b',y='salary',data=df,ax=axes[0,0],hue='gender',palette=forViolin)\nsns.violinplot(x='hsc_b',y='salary',data=df,ax=axes[0,1],hue='gender',palette=\"Paired\")\nsns.violinplot(x='hsc_s',y='salary',data=df,ax=axes[1,0],hue='gender',palette=forViolin)\nsns.violinplot(x='degree_t',y='salary',data=df,ax=axes[1,1],hue='gender',palette=\"Paired\")\nsns.violinplot(x='workex',y='salary',data=df,ax=axes[2,0],hue='gender',palette=forViolin)\nsns.violinplot(x='specialisation',y='salary',data=df,ax=axes[2,1],hue='gender',palette=\"Paired\")\nfigure.tight_layout()","e78eea88":"print(Fore.BLUE+'Observe the ssc percentage has good correlation with hsc and degree percentage.')\nprint(Fore.RED+'Observe the degree percentage has good correlation (not much) with MBA and hsc,ssc percentages.')\nprint(Fore.GREEN+'They actually make sense.Like wise observe every feature relation with other.')\nplt.figure(figsize=(12,6))\nsns.heatmap(df_copy.corr(),annot=True,cmap='RdPu')","8296fac9":"print(Back.BLUE+'Observe, the ssc percentage has linear relation with hsc and degree percentage which make sense.',Back.RESET)\nprint(Back.BLACK+'Also, the hsc percentage has linear relation with degree percentage which also make sense.',Back.RESET)\nprint(Back.CYAN+'Also, the degree percentage has linear relation with hsc and ssc percentage.',Back.RESET)\nprint(Back.MAGENTA+'But, the MBA percentage has linear relation with degree,ssc and hsc which make sense too.',Back.RESET)\nprint(Back.RED+'etest percentage have positive relation with all percentage features.',Back.RESET)\nsns.set_palette(sns.color_palette(['#660033']))\nsns.pairplot(df.drop(['salary'],axis=1),kind='reg',markers='+')","4eec31bd":"figure, axes = plt.subplots(nrows=3, ncols=2,figsize=(12,10))\nsns.distplot(df['ssc_p'], color=\"b\", ax=axes[0, 0])\nsns.distplot(df['hsc_p'], color=\"b\", ax=axes[0, 1])\nsns.distplot(df['degree_p'], color=\"r\", ax=axes[1, 0])\nsns.distplot(df['etest_p'], color=\"r\", ax=axes[1, 1])\nsns.distplot(df['mba_p'], color=\"g\", ax=axes[2, 0])\naxes[2,1].axis('off')\nfigure.tight_layout()\nprint(Fore.BLUE+'Notice ssc,hsc,degree and etest has more distribution between 60-70 percentage.More people scored between 60-70%.But its not same with MBA.')","3a975b72":"print('We have 67 Null values in salary.')\ndf.isnull().sum()","65c8da58":"df['status'].value_counts()","480fa2c4":"df = df.fillna(0)","21eaee13":"print('0 null values.')\ndf.isnull().sum()","eacb34e6":"print(df.select_dtypes('object').columns)","5cbf6ef7":"for col in df.select_dtypes('object').columns:\n    print(Fore.YELLOW+'-'*35,Fore.RESET)\n    print(Fore.RED+col+Fore.RESET)\n    print(Back.LIGHTYELLOW_EX+str(df[col].value_counts())+Back.RESET)","cbe8fa3d":"\n# NOTE: You can also pass drop_first=True into get_dummies function to drop first column of any two dummies generated\n# To reduce the size of dataframe. How ever its not recommended if you have more that 2 catogeries in one feature.\n# I will remove one column from the new columns which have only 2 catogeries we get after applying get_dummies  \n# which now indicates 1 for one catogery and 0 for another. I will leave the columns that have more that 2 catogeries.\n\n# Example: Consider gender, we have two catogeries, M and F\n# Before applying get_dummies: Gender\n#                                M\n#                                F\n#                                M\n# After applying get_dummies: Gender_M Gender_F\n#                                1        0\n#                                0        1\n#                                1        0\n# After applying get_dummies, i said we can drop any one column now, observe both the columns above.If we remove Gender_F\n# Then we can use Gender_M for both M and F as 1 mean 'Yes this is male' and 0 means 'No this isn't male' which is female.\n\na = pd.get_dummies(df['gender']).drop('M',axis=1).rename(columns={'F':'Gender'})\nb = pd.get_dummies(df['ssc_b']).drop('Others',axis=1).rename(columns={'Central':'SSC_b'})\nc = pd.get_dummies(df['hsc_b']).drop('Others',axis=1).rename(columns={'Central':'HSC_b'})\nd = pd.get_dummies(df['hsc_s'])\ne = pd.get_dummies(df['degree_t'])\nf = pd.get_dummies(df['workex']).drop('No',axis=1).rename(columns={'Yes':'WorkExp'})\ng = pd.get_dummies(df['specialisation']).drop('Mkt&Fin',axis=1).rename(columns={'Mkt&HR':'Mkt&HR and Mkt&Fin'})\nh = pd.get_dummies(df['status']).drop('Not Placed',axis=1).rename(columns={'Placed':'Status'})\n\ndf = df.drop(['gender','ssc_b','hsc_b','hsc_s','degree_t','workex','specialisation','status'],axis=1)\ndf = pd.concat([df,a,b,c,d,e,f,g,h],axis=1)","e5a280ca":"df","da84c425":"print(Back.RED+'Below are the features that are positively correlated with Placed column.Hence,these are the factor influenced a candidate in getting placed ')\ndf.corr()['Status'].sort_values(ascending=False)[1:7]","4439cae1":"print(Back.RED+'In 40th cell we can see the ssc_p,hsc_p,degree_p has highest correlation compared to others which make sense and yes percentage matters.',Back.RESET)","e5eb756e":"print(Back.RED+'Seems like Comm & Mgmt degree specialization is much demanded by corporate. ')\nfigure,axes = plt.subplots(figsize=(12,10),ncols=2,nrows=2)\nbox4 = sns.color_palette(['#e8505b','#f9d56e'])\nsns.swarmplot(y='salary',x='degree_t',data=df_copy,ax=axes[0,0],palette=box4)\nsns.countplot(hue='gender',x='degree_t',data=df_copy,ax=axes[0,1],palette=box4)\nsns.countplot(hue='specialisation',x='degree_t',data=df_copy,ax=axes[1,0],palette=box4)\nsns.countplot(hue='status',x='degree_t',data=df_copy,ax=axes[1,1],palette=box4)","32641e1b":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\nfrom sklearn.preprocessing import StandardScaler","7edbb8b6":"# Notice: we are not dropping the 'salary' column which is highly correlated.\nX = df.drop('Status',axis=1)\ny = df['Status']","66e6555e":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=101)","04ae6c3d":"model = LogisticRegression()","c2ad905f":"model.fit(X_train,y_train)","7678d612":"y_pred = model.predict(X_test)","1c114edc":"print(classification_report(y_test,y_pred))","84c85da3":"print(confusion_matrix(y_test,y_pred))","322dba40":"salary = [[0,0,0,0,0,19982,0,0,0,0,0,0,0,0,0,0,0]]\nprint(model.predict(salary)[0])\nprint('See we got 1 just by passing the salary column, 1 means Placed.')","30cef493":"X1 = df.drop(['Status','salary'],axis=1)\ny1 = df['Status']","7d214a73":"X_train,X_test,y_train,y_test = train_test_split(X1,y1,test_size=0.2,random_state=101)","90e9c62d":"model1 = LogisticRegression(max_iter=10000)","94c550c8":"model1.fit(X_train,y_train)","8278726a":"y_pred_1 = model1.predict(X_test)","ee9e5e5a":"print(classification_report(y_test,y_pred_1))","3c11e352":"print(confusion_matrix(y_test,y_pred_1))","12339d8c":"print(accuracy_score(y_test,y_pred_1))","36c96e1f":"# Campus Recruitment<br>\n**Academic and Employability Factors influencing placement**\n<h2>Beginner friendly steps<\/h2>\nThe dataset has very less features and entries.Which is good for beginner's to have hands on experience and to play with.We will cover the following steps in a neat understanding way (as possible).These are the steps involved in most of the data science problems (be more complex actually as the data increase and the kind of problem and task we are working on).\n<ol>\n<li><b>Step-1<\/b> Grabbing data, Importing required libraries, Walk around data.<\/li>\n<li><b>Step-2<\/b> Exploratory data analysis. Exploring the data visually and get insights of data, etc.<\/li>\n<li><b>Step-3<\/b> Data preprocessing. Working on missing values, Feature engineering etc.<\/li>\n<li><b>Step-4<\/b> Getting answers for the questions.<\/li>\n<li><b>Step-5<\/b> Making predictions. Given the required data predicting if he gets placed or not.<\/li>\n<\/ol>\n<h2>Questions we work on<\/h2>\n<ol>\n    <li>Which factor influenced a candidate in getting placed?<\/li>\n    <li>Does percentage matters for one to get placed?<\/li>\n    <li>Which degree specialization is much demanded by corporate?<\/li>\n<\/ol>\n<h2>Task i perform<\/h2>\nApart from the questions asked by <a href=\"https:\/\/www.kaggle.com\/benroshan\">Ben Roshan D<\/a> I also want to perform some perdiction tasks where we use the given data and try predict if a person will be placed or not given all the features present in this dataset.We use maximum out of this dataset for maximum prediction accuracy.","b51de5cf":"<h3>Gender wise percentage distribution of ssc,hsc,degree,mba,etest<\/h3>","1259ab18":"Preparing the training and testing data.","a224f581":"# Importing data into a variable df","18c10bb2":"Training the model","d0e258dd":"Let's check the unique value counts of all feature's objects.","5e9a7c53":"Classification report","ba47d0f0":"<h3>Here comes some feature engineering.<\/h3>","0283c5bd":"**Checking basic info of our data**<br>\nObserve the salary column.","748a725d":"**Let's check if we still have any null values left.**","b9f5ab2e":"Instantiating the model.","6de3be1a":"# Let's do some prediction tasks!","55c0e794":"**Noticed how the classification report is ? No loss or No wrong predictions.This is not an ideal model.In fact this model can work\neven if you pass just salary ignoring all the other 20+ columns.See below**","f38de18c":"**First things first,set the sl_no column as index or just drop it.Cause its just an index and not usefull in predictions or analysis.**","9baf7631":"If you remember,The salary column is the highly correlated column with the status column.","1fa07111":"1) Which factor influenced a candidate in getting placed?","cf0f5551":"3) Which degree specialization is much demanded by corporate? ","87b40fb7":"**Checking usefull information.**","d970e76f":"**That's it,Hope you learned something from this and is helpful.Please feel free to comment down if you have any query.Also let me know if i did any dumb mistakes.Also suggestions are welcome.Try the same with different classification models thank you and stay home stay safe. __\/\\__**","c17f896b":"# With highly correlated column","c277796c":"<h3>Gender wise ssc,hsc,degree,work experience,specialisation,status distribution<\/h3>","2d1da046":"<h3>Let's compare salary with the percentages student gained.Also with work experience and specialization.<\/h3>","163ade6a":"# Exploratory Data Analysis <br>\nLet's explore the data a bit.<br>\n<h3>Students gender distribution<\/h3><br>\nWe can see the number of males are more compare to females.","e31a62b4":"Predicting the test set and storing it within a variable.","3be9dffb":"# Importing required libraries.","55a30c10":"Let's import required libraries","d21bec87":"<h3>Who earns most ?<\/h3>","e0ce2149":"# **Let's try answering the questions now**","94043cec":"<h3>You might have heard somewhere someone saying that its good to drop the feature that are highly correlated with the\n    actual column we are going to predict.I want to show you what happens if you train the model with and without highly correlated column.<\/h3>","ad947644":"Let's grab the object data types from our dataframe.","a5471e4c":"2) Does percentage matters for one to get placed?","f96a3f82":"<h3>Let's check the correlation between all the available features.<\/h3><br>\nObserve the correlation features. We dont have the placement feature (placed or not) here because its not in either int or float data type.Further in data preprocessing we will do feature engineering then we do correlation again then we can seen if there is any change in correlation with placement feature.But here with the available raw unedited data we can see some (not much) correlation as printed below.","953fc419":"# With-out highly correlated column<br>\nLet's drop highly related column and then our model starts learning by finding patterns and make real predictions.","289e6690":"**Let's first check for null values and fill them.**","8553e886":"**Let's make sure if the number of Not Placed students is equal to number of Null values in salary.Because Who ever is placed will\nonly get paid and everyone else will not be paid.**","504f3029":"# Data Preprocessing","5cdedd6b":"Let's stop this here and start data preprocessing.","c2a7e21e":"**Checking head of data.**","ad6d6d32":"Confusion matrix","57ade8b2":"<h3>Lets plot a pairplot to check if there is any linear relation between percentages<\/h3>","a2d3e1b1":"**Okay number of null values in salary is equal to number of not placed student and because they are not placed we can fill the null values with 0 which make sense instead of removing 67 rows.In some situations we use mean values to fill the null values but it doesnt make sense here as all the not placed students should strongly have salary set to 0 because again they are not placed and hence will not get paid.**","3a42e98c":"<h3>Distribution of percentages in ssc,hsc,degree,etest,mba<\/h3>","b95a062b":"**We may observe that except hsc_s and degree_t all other colums have 2 catogeries in them.Let's use pandas get_dummies function\nto separate every individual catogeries and assign either 0 or 1 to them.**"}}