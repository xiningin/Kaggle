{"cell_type":{"42e6a7a8":"code","ad1c19a1":"code","b2f36a2e":"code","99d7b3af":"code","ba806e84":"code","c1ab9f80":"code","107455e6":"code","bf04f0f0":"code","62a1af7c":"code","8a0ae1eb":"code","078ac36a":"code","a208a38a":"code","c59b3f3c":"code","9d9e506b":"code","34263579":"code","59038482":"code","a1672f28":"code","153c31e1":"code","c10f2fc9":"markdown","e8a2e379":"markdown","d70eba74":"markdown","b30855a4":"markdown","2fad0a21":"markdown","d225ba3c":"markdown","dcde01fc":"markdown","d1e9be9b":"markdown","dbbb98e2":"markdown","540d4aac":"markdown","781bcdb0":"markdown"},"source":{"42e6a7a8":"!pip install missingpy\n!pip install imblearn","ad1c19a1":"import pandas as pd\nimport numpy as np\nimport os, shutil\nimport pprint\nfrom collections import Counter\nimport joblib\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import recall_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\n\nfrom xgboost import XGBClassifier\nfrom missingpy import MissForest\nfrom imblearn import over_sampling, under_sampling\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","b2f36a2e":"!pwd","99d7b3af":"DATA_PATH = '\/kaggle\/input\/novartis-data'\n\ntrain = pd.read_csv(os.path.join(DATA_PATH, 'Train.csv'))\ntest_df = pd.read_csv(os.path.join(DATA_PATH, 'Test.csv'))\nprint(\"We have {} rows and {} columns\".format(train.shape[0], train.shape[1]))","ba806e84":"\n# Target value distribution\ntarget_vc = train.MULTIPLE_OFFENSE.value_counts()\n\nsns.barplot(x = target_vc.index, y = target_vc.values)\nplt.show()","c1ab9f80":"train.isna().sum()","107455e6":"imputer = MissForest()\nimputed_data = imputer.fit_transform(train.drop(['INCIDENT_ID', 'DATE', 'MULTIPLE_OFFENSE'], axis=1))\ntrain['X_12'] = imputed_data[:, 11]\ndel imputed_data","bf04f0f0":"train.drop(['INCIDENT_ID', 'DATE'], axis=1, inplace=True)","62a1af7c":"# Function for balancing the data\n\ndef data_sampling(df, over_sampling_strategy, under_sampling_strategy, target_column='MULTIPLE_OFFENSE'):\n  #Over sampling and undersampling funcitons\n  over = over_sampling.SMOTE(sampling_strategy=over_sampling_strategy)\n  under = under_sampling.RandomUnderSampler(sampling_strategy=under_sampling_strategy)\n\n  over_sampled_data, _ = over.fit_resample(train.values, train[target_column].values)\n  sampled_data, _ = under.fit_resample(over_sampled_data, over_sampled_data[:, 15])\n  #Converting sampled data to pandas dataframe\n  sampled_df = pd.DataFrame(sampled_data)\n  sampled_df.columns = train.columns\n  return sampled_df","8a0ae1eb":"def create_folds(df, n_folds, target_column='MULTIPLE_OFFENSE'):\n  df['kFold'] = -1\n  kfold = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=18)\n  for fold, (train_idxs,val_idxs) in enumerate(kfold.split(X=df, y=df[target_column].values)):\n    df.loc[val_idxs, 'kFold'] = fold\n  return df","078ac36a":"def train_fn(df, n_folds, save_model=True, booster = 'gbtree', learning_rate = 0.1, max_depth = 3, subsample = 1, target_column='MULTIPLE_OFFENSE'):\n  all_recalls = []\n  if save_model:\n    if not os.path.exists('\/kaggle\/working\/models'):\n      os.mkdir('\/kaggle\/working\/models')\n    else:\n      shutil.rmtree('\/kaggle\/working\/models')\n      os.mkdir('\/kaggle\/working\/models')\n  for fold in range(n_folds):\n    train_df = df[df.kFold.isin(FOLD_MAPPING.get(fold))]\n    val_df = df[df.kFold == fold]\n\n    train_X = train_df.drop([target_column, 'kFold'], axis=1)\n    train_y = train_df[target_column].values\n\n    val_X = val_df.drop([target_column, 'kFold'], axis=1)\n    val_y = val_df[target_column].values\n\n    #Model\n    clf = XGBClassifier(booster=booster, learning_rate=learning_rate, max_depth=max_depth, subsample=subsample)\n    clf.fit(train_X, train_y)\n    predictions = clf.predict(val_X)\n    if save_model:\n      joblib.dump(clf, f\"models\/{algo}_{fold}.pkl\")\n      print(f\"Saved {algo}_{fold}.pkl\")\n    recall = recall_score(val_y, predictions)\n    all_recalls.append(recall)\n    # print(\"Recall score  for fold {} is {}\".format(fold, recall_score(val_y, predictions)))\n  return all_recalls, np.mean(all_recalls)","a208a38a":"def test(test_df):\n  test_idxs = test_df.INCIDENT_ID.values\n  imputer = MissForest()\n  imputed_data = imputer.fit_transform(test_df.drop(['INCIDENT_ID', 'DATE'], axis=1))\n  test_df['X_12'] = imputed_data[:, 11]\n  del imputed_data\n  test_df = test_df.drop(['INCIDENT_ID', 'DATE'], axis=1)\n  predictions = pd.DataFrame()\n  for fold in range(n_folds):\n    clf = joblib.load(f\"\/kaggle\/working\/models\/{algo}_{fold}.pkl\")\n    predictions[f\"pred_{fold}\"] = clf.predict(test_df)\n  final_predictions = predictions.mode(axis=1)[0].values\n  submission = pd.DataFrame({'INCIDENT_ID':test_idxs, 'MULTIPLE_OFFENSE':final_predictions})\n  return submission","c59b3f3c":"n_folds = 5\nFOLD_MAPPING = {\n    0 : [1,2,3,4],\n    1 : [0,2,3,4],\n    2 : [1,0,3,4],\n    3 : [1,2,0,4],\n    4 : [1,2,3,0],\n}\n\nparameters = {\n    'over_sampling_strategy' : np.arange(0.1, 0.6, 0.1),\n    'under_sampling_strategy' : np.arange(0.8, 0.4, -0.1),\n    'booster' : ['gbtree', 'dart'],\n    'learning_rate' : np.arange(0.1, 0.4, 0.1),\n    'max_depth' : np.arange(3, 8, 1),\n    'subsample' : np.arange(0.5, 1.25, 0.25)\n}","9d9e506b":"# scores = []\n\n# for o in parameters['over_sampling_strategy']:\n#   for u in parameters['under_sampling_strategy']:\n#       sampled_df = data_sampling(train, o, u)\n#       df = create_folds(sampled_df, n_folds)\n#       recall_arr, recall_mean = train_fn(df, n_folds, False, 'gbtree', 0.1, 6)\n#       scores.append((o, u, recall_arr, recall_mean))\n#       print(o, u, recall_mean)\n\n# scores = sorted(scores, key=lambda x : x[3], reverse=True)\n# scores[0:2]","34263579":"# o = 0.5\n# u = 0.5\n# scores2 = []\n# for b in parameters['booster']:\n#   for lr in parameters['learning_rate']:\n#     for d in parameters['max_depth']:\n#         sampled_df = data_sampling(train, o, u)\n#         df = create_folds(sampled_df, n_folds)\n#         recall_arr, recall_mean = train_fn(df, n_folds, False, b, lr, d)\n#         scores2.append((b, lr, d, recall_arr, recall_mean))\n\n# scores2 = sorted(scores2, key=lambda x : x[3], reverse=True)\n# scores2[0:2]","59038482":"o = 0.5\nu = 0.5\nalgo = 'xgb_gbtree'\nsampled_df = data_sampling(train, o, u)\ndf = create_folds(sampled_df, n_folds)\nrecall_arr, recall_mean = train_fn(df, n_folds, True, booster='gbtree', learning_rate=0.2, max_depth=4)\nprint(recall_mean)\nprint(recall_arr)","a1672f28":"os.listdir('\/kaggle\/working\/models')","153c31e1":"submission = test(test_df)\nsubmission.to_csv(f\"\/kaggle\/working\/submission.csv\", index=False)","c10f2fc9":"#### Filling missing values using random forest","e8a2e379":"#### Helper functions","d70eba74":"#### Target varaible distribution\u00b6\n","b30855a4":"Many real world classification problems like anomaly detection, churn prediction or fraud detection, the class distributions are always heavily skewed as the probability of occurance of an event(like fraud) is very low.\n\nBut most of the machine learning algorithms used for classification were designed around the assumption of an equal number of examples for each class.\n\nIn this notebook, I am using a dataset provided for a hackathon by Novartis.","2fad0a21":"#### Grid search to determine oversampling and undersampling ratios","d225ba3c":"#### Loading data","dcde01fc":"#### Missing values","d1e9be9b":"#### Prediction on test data","dbbb98e2":"#### Run","540d4aac":"#### Grid search to determine for boosting parameters","781bcdb0":"#### Training and validation"}}